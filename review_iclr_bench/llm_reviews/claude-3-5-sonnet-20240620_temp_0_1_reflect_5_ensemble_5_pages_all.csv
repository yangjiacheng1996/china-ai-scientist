paper_id,Summary,Questions,Limitations,Ethical Concerns,Soundness,Presentation,Contribution,Overall,Confidence,Strengths,Weaknesses,Originality,Quality,Clarity,Significance,Decision
tDirSp3pczB,"This paper presents novel theoretical upper and lower bounds for contrastive unsupervised representation learning (CURL), relating the contrastive loss to downstream classification performance. The bounds are shown to be tight in the negative sample size K and provide a theoretical explanation for why larger K improves performance empirically. The theory is validated through experiments on synthetic, vision, and language datasets, demonstrating better alignment with empirical observations compared to existing bounds.","['How sensitive are the bounds to deviations from uniform class priors?', 'Could the analysis be extended to explain the gap between supervised loss and accuracy?', 'How well do the bounds hold for more sophisticated downstream classifiers beyond the mean classifier?', 'Have you considered validating the results on other domains beyond vision and language?']","['Assumes uniform class priors which may not hold in practice', 'Does not fully explain gap between supervised loss and accuracy', 'Experiments limited to vision and language tasks', 'Analysis focuses only on mean classifier, not more sophisticated downstream models']",False,4,4,4,8,4,"['Novel and sharp theoretical bounds that are tighter than existing work', 'Provides theoretical justification for empirically observed benefits of larger negative sample sizes', 'Solid empirical validation on synthetic and real datasets', 'Advances theoretical understanding of CURL', 'Clear connection between theory and empirical observations']","['Analysis assumes uniform class priors, which may not always hold in practice', 'Does not fully explain the gap between supervised loss and accuracy', 'Experimental validation somewhat limited in scope (mainly vision and language tasks)', 'Focuses only on mean classifier for downstream evaluation']",4,4,4.0,4,Accept
YDud6vPh2V,"This paper introduces ξ-learning, a novel reinforcement learning algorithm that extends successor feature methods to work with general (non-linear) reward functions. The key contribution is learning cumulative discounted probabilities of successor features, allowing for transfer learning with arbitrary reward functions. Theoretical convergence guarantees are provided, and empirical results demonstrate improved performance over baselines on tasks with both linear and non-linear rewards.","['How does the computational complexity of ξ-learning compare to standard SF methods as the feature dimensionality increases?', 'How well does ξ-learning scale to more complex environments with high-dimensional state/action spaces?', 'Have you compared ξ-learning to other recent approaches for transfer with general rewards, like universal successor features?', 'Have you tested ξ-learning on any standard RL benchmark environments like Atari or MuJoCo tasks?', 'What are some potential real-world applications where ξ-learning could be particularly beneficial?']","['Empirical evaluation is limited to relatively simple environments', 'Scalability to high-dimensional spaces is not thoroughly demonstrated', 'Increased computational complexity compared to standard SF methods', 'Potential negative impacts not explicitly discussed, though likely minimal for this type of algorithmic work']",False,4,3,4,8,4,"['Novel theoretical framework (ξ-learning) that extends successor features to general rewards', 'Rigorous theoretical analysis including convergence proofs and transfer guarantees', 'Strong empirical results demonstrating improved performance over baselines', 'Handles both linear and non-linear reward functions', 'Addresses an important limitation of existing successor feature methods']","['Empirical evaluation is limited to relatively simple environments', 'Potential scalability issues for high-dimensional feature spaces', 'Computational complexity concerns, especially for larger state/action spaces', 'Limited comparison to other recent transfer learning approaches']",4,4,4.0,4,Accept
xZ6H7wydGl,"This paper presents a novel algorithm for learning stochastic differential equations (SDEs) using importance sampling with Brownian bridges. The method avoids SDE integrators, enabling parallelization and lower variance gradient estimates. Results show significant speedups and improved performance over integrator-based methods, especially in higher dimensions.","['How does the method perform on a wider range of real-world SDE learning tasks?', 'What are the theoretical convergence guarantees for the algorithm?', 'How sensitive is the method to the choice of hyperparameters?', 'Can the approach be extended to handle more general classes of SDEs?']","['Method is restricted to SDEs with transformable state-independent diffusion', 'Increased memory usage compared to adjoint methods', 'Limited evaluation on real-world applications', 'Some overconfidence in uncertainty quantification with GP posterior']",False,4,3,4,8,4,"['Novel algorithm avoiding limitations of SDE integrator-based methods', 'Theoretical justification provided', 'Significant improvements in computational efficiency and scalability', 'Lower gradient variance and faster training, especially in high dimensions', 'Enables use of variational Gaussian processes for drift function', 'Highly parallelizable approach', 'Comprehensive empirical evaluation showing advantages over existing methods']","['Limited evaluation on real-world datasets and tasks', 'Restricted to SDEs where diffusion term can be transformed to be state-independent', 'Potential increase in memory usage compared to adjoint methods', 'Limited theoretical analysis of convergence properties', 'Some aspects of the presentation could be clearer']",4,3,3.0,4,Accept
CNY9h3uyfiO,"This paper analyzes the effects of linear reward transformations, particularly reward shifting, in value-based deep reinforcement learning. The key insight is that reward shifting is equivalent to changing the initialization of the Q-function approximator. The authors demonstrate that positive reward shifting leads to conservative exploitation while negative shifting promotes exploration. Applications to offline RL, online continuous control, and curiosity-driven exploration are presented, showing improvements over baselines in various tasks.","['Can you provide more details on the equivalence between reward shifting and initialization for non-linear function approximators?', 'How do you determine appropriate shift constants for a given task?', 'How does the computational cost of your method compare to standard RL algorithms?', 'Can you elaborate on the limitations of your approach in more complex RL scenarios?']","['Limited theoretical analysis', 'Potential sensitivity to hyperparameter choices', 'Focus on standard benchmarks']",False,3,3,3,7,4,"['Novel theoretical insight connecting reward shifting to Q-function initialization', 'Broad applicability across offline and online RL settings', 'Empirical results demonstrating clear improvements over baselines', 'Simple technique that can be easily integrated into existing algorithms']","['Limited theoretical analysis and justification', 'Potential sensitivity to hyperparameters (shift constants)', 'Experiments focused on standard benchmarks rather than more complex domains', 'Some missing comparisons to relevant baselines']",3,3,3.0,4,Accept
fXHl76nO2AZ,"This paper introduces Gradient Importance Learning (GIL), a novel method for training neural networks to make predictions from incomplete data without imputation. GIL employs reinforcement learning to learn an importance matrix that weights gradients during backpropagation, enabling the model to leverage information in missingness patterns. The approach is evaluated on various datasets, including medical time series and images, and demonstrates superior performance compared to imputation-based methods, especially at high missing data rates.","['Can you provide more theoretical analysis on why/how GIL is able to leverage missingness patterns?', 'How does the computational complexity and training time of GIL compare to imputation-based methods?', 'Can you include additional ablation studies to analyze different components of the method?', 'How sensitive is the method to hyperparameter choices, especially for the RL component?']","['Currently limited to MLPs and LSTMs', 'Adds significant complexity which may limit practical adoption', 'Requires careful tuning of many hyperparameters', ""Theoretical understanding of method's effectiveness is limited""]",False,3,3,3,7,4,"['Novel approach to handling missing data without imputation', 'Theoretically well-founded with detailed derivations and proofs', 'Comprehensive empirical evaluation on multiple datasets', 'Outperforms existing methods, especially at high missing rates', 'Flexible framework that can incorporate other techniques']","['Limited theoretical analysis of why/how the method works', 'Adds significant complexity compared to standard imputation approaches', 'Computational complexity and training time not thoroughly discussed', 'Limited ablation studies', 'Some baseline comparisons missing']",4,3,4.0,3,Accept
bglU8l_Pq8Q,"This paper analyzes the capacity and generalization of dual-encoder (DE) models for neural ranking compared to cross-attention (CA) models. It provides theoretical results on DE expressivity, empirical analysis showing DE models overfit, and new distillation techniques (M3SE and softmax cross-entropy) to improve DE generalization. Experiments demonstrate the proposed methods can significantly close the performance gap between DE and CA models, even surpassing CA performance when combined with ColBERT scoring.","['How sensitive are the theoretical results to the simplifying assumptions made?', 'What are the computational efficiency tradeoffs between the proposed distillation techniques and standard DE/CA models?', 'How well do the proposed techniques generalize to other ranking datasets beyond MSMARCO?', 'Have you considered combining your distillation approach with other techniques like hard negative mining for retrieval?', 'Are there potential applications of your distillation techniques outside of information retrieval?']","['Focus is primarily on re-ranking rather than full retrieval', 'Experiments limited to moderate-sized models/datasets', 'May not generalize to all ranking scenarios or model architectures', 'Limited discussion of potential negative impacts or fairness considerations']",False,4,4,4,8,4,"['Novel theoretical analysis of DE model capacity', 'Empirical demonstration of overfitting as the key issue for DE models', 'Proposal of effective new distillation techniques to improve DE performance', 'Comprehensive experiments and analysis to validate claims', 'Clear writing and presentation of results', 'Addresses an important problem in information retrieval', 'Demonstrates improvements over existing methods, including ColBERT']","['Theoretical results rely on some simplifying assumptions', 'Proposed methods do not fully close gap with CA models in all cases', 'Focus is mainly on re-ranking rather than full retrieval', 'Limited analysis of computational efficiency tradeoffs']",3,4,4.0,4,Accept
cU8rknuhxc,"This paper introduces DISDAIN (Discriminator Disagreement Intrinsic Reward), a novel method to improve unsupervised skill discovery in reinforcement learning. DISDAIN addresses the problem of pessimistic exploration by using an ensemble of discriminators to estimate epistemic uncertainty and provide an exploration bonus. The method is evaluated on Four Rooms and 57 Atari games, showing significant improvements in the number of skills learned and state coverage compared to baselines.","['How does the computational complexity of DISDAIN compare to baseline methods?', 'How well would DISDAIN scale to environments with much larger state spaces?', 'Could you provide more analysis on how the additional learned skills translate to improved performance on downstream tasks?', 'Have you explored using other uncertainty estimation techniques besides ensembles?', 'Have you considered combining DISDAIN with other exploration methods or skill discovery techniques?']","['The method introduces additional computational complexity which may limit scalability', 'Evaluation focuses mainly on skill learning metrics rather than downstream task performance', 'The approach may not be directly applicable to continuous skill spaces without modification', 'Limited discussion of potential negative societal impacts', 'Performance in more complex or realistic environments beyond Atari games not explored']",False,3,4,3,7,4,"['Identifies and addresses an important problem in unsupervised skill discovery', 'Well-motivated approach with theoretical grounding', 'Strong empirical results showing consistent improvements over baselines', 'Clear writing and effective presentation', 'Thorough ablation studies and analyses']","['Adds computational overhead due to the ensemble of discriminators', 'Limited analysis of downstream utility of additional learned skills', 'Scalability to larger skill spaces or more complex environments not fully explored', 'Limited comparison to other recent exploration methods beyond RND']",4,3,4.0,4,Accept
HFPTzdwN39,"This paper introduces quantized reverse probing, a novel method for measuring the interpretability of self-supervised visual representations. The approach quantizes the representation space via clustering and then measures the mutual information between clusters and a wide range of semantic concepts. By using linear probes to predict cluster assignments from concept vectors, the method provides a single interpretability score. The authors evaluate numerous recent self-supervised learning approaches, offering insights into how these models encode various concepts beyond just object categories.","['How sensitive are the results to the choice of expert models used for concept labeling?', 'Could the method be extended to work with raw images instead of pre-extracted features to enable online evaluation?', 'How well does the approach generalize to other datasets beyond ImageNet?', 'Have you considered using non-linear probes, and how might this affect the results?', 'How does the choice of clustering algorithm affect the results?']","['Dependence on pre-trained expert models introduces some reliance on supervised data', 'Evaluation on more diverse datasets would strengthen generalizability claims', 'Computational cost of clustering large datasets could limit scalability', 'The quantization step introduces some information loss']",False,4,4,4,7,4,"['Novel and principled approach to evaluating interpretability of self-supervised representations', 'Rigorous information-theoretic foundation', 'Comprehensive evaluation of many state-of-the-art SSL methods', 'Use of diverse concept types beyond just object categories', 'Insightful qualitative analysis of concept-cluster relationships', 'Reveals interesting findings about clustering-based SSL methods']","['Reliance on pre-trained expert models for concept labeling', 'Results could be sensitive to choice of number of clusters K', 'Evaluation focused mainly on ImageNet dataset', 'K-means clustering may not capture all structure in representations', 'Linear probing may be too simple for complex concept relationships']",4,3,4.0,4,Accept
EJKLVMB_9T,"This paper introduces SplitRegex, a novel framework for synthesizing regular expressions from examples. It employs a neural network (NeuralSplitter) to split positive examples, synthesizes subregexes for each part, and combines them into a full regex. Experiments demonstrate significant improvements over existing methods across multiple datasets in terms of speed and accuracy.","['How does the approach scale to very long or complex regexes and large example sets?', 'Are there any theoretical guarantees on the quality or correctness of the synthesized regexes?', 'How does this method compare to recent neural approaches for regex synthesis?', 'How sensitive is the performance to the neural network architecture and hyperparameters?', 'How interpretable is the neural splitting process? Can we understand why certain splits are made?']","['May not work well for regexes with complex alternation/union structures', 'Performance depends on the quality of the underlying regex synthesizer', 'Limited evaluation on very complex regex features (e.g., backreferences, lookahead/lookbehind)', 'Relies on having a good set of positive and negative examples', 'May require a large training dataset for the NeuralSplitter to perform well']",False,3,3,3,7,4,"['Innovative divide-and-conquer approach using neural networks for regex synthesis', 'Substantial empirical improvements over baselines on multiple datasets', 'Comprehensive experiments with detailed ablation studies and analysis', 'Generalizable framework that can enhance existing regex synthesizers', 'Effectively combines neural networks with traditional regex synthesis algorithms']","['Limited theoretical analysis and guarantees', 'Potential scalability issues for very complex regexes or large example sets', 'Limited evaluation on regexes with complex alternation/union structures', 'Lack of comparison to some recent neural approaches for regex synthesis']",4,3,3.0,4,Accept
MWQCPYSJRN,"This paper introduces 'generative negative replay', a novel approach for continual learning where generated data from past experiences is used as negative examples when training on new classes. The method is shown to outperform standard generative replay on challenging benchmarks like CORe50 and ImageNet-1000, demonstrating robustness even when generated data quality degrades.","['How would the method perform on non-image domains like text or audio?', 'Is there a theoretical explanation for why negative replay works better than positive replay with generated data?', 'How does the computational/memory overhead compare to other replay methods?', 'Could the approach be extended to work without a generative model, e.g. using data augmentation?', 'What are potential real-world applications of this technique beyond academic research?']","['Only evaluated on image classification tasks', 'Relies on storing/training a generative model', 'Scalability to very long sequences of tasks not demonstrated', 'Potential negative societal impacts not thoroughly discussed, such as privacy concerns with storing generated data or potential biases in continual learning systems']",False,3,4,3,7,4,"['Novel and effective approach to generative replay in continual learning', 'Strong empirical validation on challenging large-scale benchmarks', 'Thorough experimental evaluation and insightful ablation studies', 'Robust performance even with low-quality generated data', 'Conceptually simple yet effective idea that improves over standard generative replay', 'Practical implications for improving continual learning systems in real-world applications']","['Limited theoretical analysis or justification', 'Evaluation focused only on image classification tasks', 'Lacks comparison to some very recent state-of-the-art continual learning methods', 'Some implementation details and hyperparameter choices not fully explained']",3,4,4.0,3,Accept
8CEJlHbKoP4,"This paper introduces METAGEN, a model that learns a 'metacognition' for object detection neural networks to identify and correct errors. METAGEN performs joint inference over world states and a metacognitive representation, conditioned on basic object principles. Evaluations on synthetic data show improvements over baseline object detection models.","['How well would this approach generalize to real-world data and more complex scenes?', 'What is the computational complexity and runtime of METAGEN compared to baseline approaches?', 'What types of errors does METAGEN tend to correct most effectively?']","['Authors acknowledge evaluation is limited to synthetic data', 'Potential challenges in scaling to more complex scenes not fully addressed', 'Reliance on camera pose information which may not always be available']",False,3,3,3,6,4,"['Novel approach combining cognitive science principles with machine learning for object detection', 'Demonstrates clear improvements over baselines across multiple object detection architectures', 'Learns metacognition without requiring ground truth labels', 'Potential for improving generalization to out-of-sample data', 'Well-motivated from cognitive science principles']","['Evaluation limited to synthetic data in constrained environments', 'Computational complexity and scalability not addressed', 'Comparison to baselines with fitted thresholds may give an unfair advantage to the proposed method', 'Limited analysis of what the learned metacognition actually captures', 'Lack of discussion on broader impacts and potential negative societal impacts']",4,3,3.0,3,Accept
WVX0NNVBBkV,"This paper proposes using synthetic data from generative models to improve adversarial robustness of neural networks. The authors provide theoretical analysis bounding robustness transfer between distributions, introduce a new metric (ARC) to measure distribution proximity, and demonstrate significant empirical improvements in both standard and certified adversarial robustness across multiple datasets and threat models. The work also includes insightful analysis of why certain generative models, particularly diffusion models, are more effective for this task.","['How does the computational cost of this approach compare to standard adversarial training, especially for large datasets?', 'How well does this approach generalize to domains where strong generative models are not available?', 'Have you explored using this technique for other tasks beyond image classification?', 'How sensitive is the performance to the ratio of real vs synthetic data used in training?', 'What potential negative societal impacts could arise from this approach, and how might they be mitigated?']","['The approach requires access to strong generative models, which may not be available for all domains', 'High computational cost may limit practical applicability, especially for large datasets', 'May not generalize equally well to all domains or tasks beyond image classification', 'Potential to amplify biases present in the original training data used for the generative models']",False,4,3,4,8,4,"['Strong theoretical foundation with proofs bounding robustness transfer', 'Novel ARC metric that effectively predicts robustness transfer', 'Significant empirical improvements over state-of-the-art in adversarial robustness', 'Comprehensive experiments on multiple datasets, architectures, and threat models', 'Insightful analysis of why diffusion models outperform GANs for this task', 'Improves both standard and certified robustness']","['High computational cost and complexity, especially for generating synthetic data', 'Reliance on strong generative models which may not be available for all domains', 'Limited exploration of domains beyond image classification', 'Some hyperparameter tuning required (e.g., ratio of real/synthetic data)', 'Limited discussion of broader impacts and potential negative consequences']",4,4,4.0,4,Accept
74x5BXs4bWD,"This paper introduces EDO-CS, a novel evolutionary diversity optimization algorithm for reinforcement learning. The method employs clustering-based selection to choose diverse, high-quality policies from an archive for reproduction and adaptively balances quality and diversity using multi-armed bandits. Experiments on various continuous control tasks, including deceptive and multi-modal environments, demonstrate that EDO-CS outperforms several baseline quality diversity (QD) algorithms.","['Can you provide any theoretical justification or analysis for why clustering-based selection is effective?', 'How does EDO-CS compare to state-of-the-art deep RL algorithms, not just other QD methods?', 'Have you considered applying EDO-CS to more complex, high-dimensional RL tasks beyond the standard benchmark environments?', 'What is the computational complexity of the clustering step, and how does it scale with the archive size?']","['Only tested on relatively simple continuous control tasks', 'May not scale well to very high-dimensional behavior spaces', 'Clustering step could become computationally expensive for very large archives', 'Lack of theoretical foundations may limit understanding of when/why the method works well']",False,3,4,3,7,4,"['Novel clustering-based selection mechanism for maintaining diversity', 'Adaptive balancing of quality and diversity using multi-armed bandits', 'Comprehensive experiments on various RL environments, including deceptive and multi-modal tasks', 'Thorough ablation studies examining different components', 'Clear explanation of the proposed method', 'Extensive empirical validation through ablation studies']","['Limited theoretical analysis or justification for the effectiveness of clustering-based selection', 'Lack of comparison to state-of-the-art deep RL algorithms', 'Potential scalability issues for very high-dimensional behavior spaces', 'Relatively incremental contribution on top of existing quality diversity algorithms']",3,3,4.0,4,Accept
4tOrvK-fFOR,"This paper introduces a novel learnable front-end called synperiodic filterbanks for sound source detection from raw audio waveforms. The key contributions are: (1) a synperiodic filterbank design that correlates filter length with frequency response for improved time-frequency resolution tradeoffs, (2) multi-scale processing in both time and frequency domains, and (3) a Transformer-like backbone with parallel branches for semantic and spatial learning. The approach demonstrates significant performance improvements over existing methods on direction-of-arrival and physical location estimation tasks.","['How does the computational complexity scale with longer input sequences?', 'Are there any theoretical guarantees on the properties of the learned filterbanks?', 'How well does the method generalize to other audio processing tasks beyond source detection?', 'Have you evaluated the approach on any real-world datasets beyond simulated data?', 'What are some potential real-world applications for this technology?']","['Computational complexity may limit applicability to real-time or low-resource settings', 'Potential overfitting to specific datasets/tasks evaluated', 'Lack of discussion on societal impacts or ethical considerations of the technology']",False,3,3,3,7,4,"['Novel synperiodic filterbank design with well-motivated theoretical foundations', 'Multi-scale processing addressing important challenges in audio processing', 'Strong empirical results outperforming previous methods on multiple tasks', 'Comprehensive ablation studies validating key components', 'End-to-end trainable approach working directly on raw waveforms', 'Thorough experimental validation including detailed ablation studies']","['Significantly higher computational complexity than baseline methods', 'Evaluation limited to two datasets/tasks, raising questions about broader applicability', 'Theoretical analysis of filterbank properties could be expanded', 'Lack of discussion on societal impacts and ethical considerations']",4,3,4.0,3,Accept
xFOyMwWPkz,"This paper introduces 'feature entropy', a novel method for quantitatively assessing the performance of individual units in convolutional neural networks using topological data analysis. The method characterizes spatial activation patterns and measures their consistency across samples. Extensive experiments on ImageNet models demonstrate its utility for analyzing units across layers, during training, and between models with different generalization abilities.","['What is the computational complexity of calculating feature entropy compared to simpler metrics?', 'How well does the method generalize to other types of neural network architectures beyond CNNs?', 'Can feature entropy be used to improve model training, pruning, or architecture search?', 'Can you provide more theoretical justification for why feature entropy should correlate with unit performance?', 'How might feature entropy be applied to enhance model interpretability in practical applications?']","['Method only evaluated on image classification tasks', 'Computational complexity may limit scalability to very large models', 'Theoretical foundations could be strengthened', 'Practical applications like pruning are not thoroughly explored']",False,3,3,3,7,4,"['Novel application of topological data analysis to CNN interpretability', 'Feature entropy shown to be more robust than magnitude-based metrics', 'Thorough empirical analysis on ImageNet models showing interesting trends', 'Method is invariant to activation rescaling', 'Provides insights into unit behavior across layers and during training', 'Able to discriminate between models with different generalization abilities']","['Limited theoretical justification for the relationship between feature entropy and unit/model performance', 'Evaluation focused only on image classification, generalization to other tasks/domains not shown', 'Computational complexity of the method not thoroughly discussed', 'Practical utility for tasks like network pruning or architecture search not fully demonstrated']",4,3,3.0,4,Accept
gPvB4pdu_Z,"This paper introduces a novel end-to-end training method called 'compositional training' for deep AUC maximization. The approach optimizes a compositional objective combining an AUC loss with a standard loss (e.g., cross-entropy), supported by an efficient stochastic optimization algorithm with theoretical convergence guarantees. Extensive experiments on imbalanced datasets, including medical image classification tasks, demonstrate significant improvements over existing methods.","['How does the proposed method compare to other recent approaches for handling imbalanced data in deep learning?', 'What are the main challenges in extending the compositional training approach to multi-class problems?', 'How does the method perform on non-image data, such as time series or text?']","['Increased computational complexity may limit scalability to very large datasets', 'Currently limited to binary classification', 'Potential difficulty in hyperparameter tuning', 'Theoretical assumptions may not always hold in practice', 'Generalization to other types of data or tasks remains to be explored']",False,4,4,4,8,4,"['Novel compositional formulation for end-to-end AUC maximization', 'Theoretically sound optimization algorithm with convergence guarantees', 'Strong empirical results on multiple datasets, particularly medical image tasks', 'Addresses an important problem in imbalanced classification', 'Well-written and technically rigorous']","['Increased computational complexity compared to simpler methods', 'Additional hyperparameters requiring tuning', 'Currently limited to binary classification', 'Primarily tested on image classification tasks']",4,4,4.0,4,Accept
Eot1M5o2Zy,"This paper introduces AestheticNet, a facial beauty prediction network achieving state-of-the-art performance, and focuses on methods to detect and reduce ethnic bias in such systems. The authors propose techniques including balanced training and a debiasing neural network to mitigate bias while maintaining prediction accuracy. Experiments are conducted on multiple datasets to demonstrate the effectiveness of these approaches.","['How do the proposed debiasing methods generalize to held-out test sets and other types of bias beyond ethnicity?', 'What are the potential negative societal impacts of deploying facial beauty prediction systems, even if debiased?', 'How do you justify the ethics of facial beauty prediction given its potential to reinforce harmful beauty standards?', 'Can you provide more details on the datasets used, including size, demographics, and annotation process?', 'Can you elaborate on the specific techniques used in your debiasing neural network and how they differ from existing approaches?']","['Focus primarily on ethnic bias, with limited exploration of other types of bias', 'Potential to reinforce harmful beauty standards even with debiased predictions', 'Unclear long-term impacts of widespread use of facial beauty prediction systems', 'Limited discussion of ethical considerations in data collection and annotation']",True,3,3,3,5,4,"['Addresses an important and timely issue of bias in AI systems', 'Achieves state-of-the-art performance in facial beauty prediction', 'Proposes novel methods for detecting and reducing bias', 'Conducts extensive experiments and analysis on multiple datasets', 'Attempts to tackle ethical considerations in AI systems']","['Insufficient discussion of ethical implications and potential negative societal impacts of facial beauty prediction systems', 'Lack of rigorous evaluation of debiasing methods, particularly on held-out datasets', 'Unclear generalizability of debiasing techniques beyond ethnic bias', 'Some claims and conclusions appear overstated given the evidence presented', 'Limited exploration of limitations and broader applicability of the proposed approaches']",3,2,2.0,3,Reject
rxF4IN3R2ml,"This paper introduces novel improvements to transformer architectures for multi-horizon time series forecasting, including learned positional encodings, horizon-specific attention, and a new decoder self-attention scheme. The proposed MQTransformer model shows significant accuracy improvements and reduced forecast volatility on both large-scale proprietary and public benchmark datasets.","['Can you provide more ablation analysis to isolate the impact of each proposed innovation?', 'How does the model perform on datasets from domains other than retail and electricity?', 'What are the limitations of the proposed positional encoding scheme?', 'How does the model handle missing data or irregular time series?']","['Evaluation on proprietary data limits reproducibility', 'Focused mainly on specific forecasting tasks, generalizability to other domains unclear', 'Computational requirements not thoroughly discussed', 'Potential negative impacts of improved demand forecasting not addressed', 'Possibility of overfitting to the specific datasets used in the study']",False,4,3,4,8,4,"['Novel and well-motivated architectural improvements to transformer models for forecasting', 'Strong empirical results on both proprietary and public datasets', 'Addresses both forecast accuracy and volatility reduction', 'Computationally efficient compared to prior transformer forecasting models', 'Thorough theoretical motivation and analysis', 'Comprehensive empirical evaluation across multiple datasets and metrics']","['Dense technical writing may limit accessibility', 'Limited ablation analysis to isolate impact of each proposed component', 'Results on proprietary dataset cannot be independently verified', 'Some aspects of the method could be explained more clearly']",4,4,3.0,4,Accept
jJOjjiZHy3h,"This paper introduces AdversarialAugment (AdA), a method for enhancing image corruption robustness by adversarially optimizing perturbations to pre-trained image-to-image model parameters. Combined with existing augmentation techniques and supported by theoretical analysis, AdA demonstrates state-of-the-art performance on corruption benchmarks while also improving adversarial robustness.","['How does the method perform on other types of data beyond image classification?', 'What are the limitations of using pre-trained image-to-image models for this approach?', 'How might this method be extended to handle more diverse types of corruptions or perturbations?']","['Increased computational requirements compared to simpler augmentation methods', 'Performance heavily relies on combination with other augmentation techniques', 'Requires access to suitable pre-trained image-to-image models', 'May not generalize well to non-image domains without significant modifications']",False,4,3,4,8,4,"['Innovative approach combining adversarial optimization with image-to-image models for data augmentation', 'Strong empirical results on corruption robustness benchmarks', 'Improves adversarial robustness in addition to corruption robustness', 'Provides theoretical analysis and guarantees', 'Effectively combines with existing augmentation techniques']","['Higher computational complexity compared to non-adversarial augmentation methods', 'Main performance gains come from combining with other techniques rather than AdA alone', 'Theoretical analysis relies on some strong assumptions', 'Clarity of presentation could be improved, especially regarding theoretical aspects']",4,4,3.0,4,Accept
Jjcv9MTqhcq,"This paper introduces LOOK, a novel supervised pre-training method based on Leave-One-Out K-Nearest-Neighbor classification. The key innovation is using a kNN classifier instead of a linear classifier during pre-training, which preserves more intra-class variation and semantic information. Extensive experiments demonstrate that LOOK outperforms existing supervised and self-supervised pre-training methods on various downstream tasks, particularly fine-grained classification.","['How does the computational cost of LOOK compare to standard supervised pre-training, especially for very large datasets?', 'Can you provide more theoretical insight or guarantees for the method?', 'How can the performance on detection and segmentation tasks be improved?', 'How sensitive is the method to key hyperparameters like k and queue size?', 'Are there potential applications of LOOK beyond computer vision tasks?']","['Only evaluated on image classification, detection, and segmentation tasks', 'Scalability to datasets much larger than ImageNet is not demonstrated', 'Introduces additional hyperparameters that need tuning', 'May not be as effective for datasets with less intra-class variation']",False,3,3,3,8,4,"['Novel approach to supervised pre-training that addresses limitations of existing methods', 'Comprehensive experiments and ablation studies on multiple downstream tasks', 'Improved performance over state-of-the-art supervised and self-supervised methods', 'Clear motivation and intuition for the method', 'Efficient implementation that scales to large datasets', 'Ability to preserve intra-class variation and semantic information']","['Potential scalability issues for datasets much larger than ImageNet', 'Limited theoretical analysis', 'Smaller performance gains on object detection and segmentation tasks', 'Sensitivity to hyperparameters', 'Only evaluated on image-related tasks']",3,3,3.0,3,Accept
9jsZiUgkCZP,"This paper introduces UVC, a unified framework for compressing Vision Transformer models. UVC integrates pruning, layer skipping, and knowledge distillation into a joint constrained optimization problem solved end-to-end. Experiments on ImageNet demonstrate that UVC can reduce FLOPs by 40-50% with minimal accuracy loss, outperforming recent ViT compression methods.","['How does the computational cost and training time of UVC compare to standard ViT training and simpler pruning approaches?', 'How well does UVC generalize to other vision tasks beyond image classification?', 'Are there any theoretical guarantees on the optimality or convergence of the proposed algorithm?', 'How sensitive is the method to hyperparameter choices?', 'What are the potential negative impacts of model compression on fairness or robustness?']","['Only evaluated on image classification, not other vision tasks', 'Focused mainly on FLOPs reduction, not model size or latency', 'May not generalize well to other architectures beyond ViT variants tested', 'Complex optimization may be challenging to implement in practice']",False,3,3,3,7,4,"['Novel unified formulation integrating multiple compression techniques', 'End-to-end optimization framework using primal-dual algorithm', 'Strong experimental results showing significant FLOP reductions with minimal accuracy loss', 'Comprehensive ablation studies and visualizations', 'Addresses an important problem in ViT efficiency', 'Extensive experiments on multiple ViT variants (DeiT, T2T-ViT)']","['Limited evaluation beyond ImageNet classification task', 'Lack of analysis on computational overhead and training time', 'Unclear generalization to other vision tasks or ViT variants', 'Some missing comparisons to recent baselines']",4,3,4.0,3,Accept
Ve0Wth3ptT_,"This paper introduces DEGREE, a novel method for explaining Graph Neural Networks (GNNs) based on decomposing the information flow through the model. The key contributions are: (1) A layer-wise decomposition scheme to track input contributions, (2) A subgraph agglomeration algorithm for identifying important structures, and (3) Theoretical motivation based on GNN information propagation. Experiments on synthetic and real datasets demonstrate DEGREE's effectiveness compared to baseline methods.","['How does DEGREE compare to very recent GNN explanation methods like GNN-LRP or SubgraphX?', 'How well does the method scale to larger, more complex graph datasets?', 'What are the main limitations or potential failure cases of DEGREE?']","['Current experiments are limited to relatively small graphs', 'Unclear generalizability to GNN architectures beyond GCN and GAT', 'Potential struggle with graphs that have very complex, long-range interactions']","While no major ethical concerns are apparent, the authors should discuss potential misuse of the explanation method, such as revealing sensitive information in graph structures.",3,3,4,7,4,"['Novel decomposition-based approach providing faithful explanations without perturbations', 'Theoretically grounded method analyzing actual GNN computation', 'Effective subgraph-level explanations via agglomeration algorithm', 'Strong performance on multiple datasets and tasks', 'Avoids issues associated with perturbation-based methods']","['Limited evaluation on large, complex real-world datasets', 'Lack of comparison to some recent GNN explanation methods', 'Insufficient discussion of limitations and potential negative impacts', 'Limited runtime efficiency analysis, especially for larger graphs']",4,3,3.0,4,Accept
aYSlxlHKEA,"This paper introduces Decentralized Model-based Policy Optimization (DMPO), a novel algorithm for multi-agent reinforcement learning in networked systems. Key contributions include localized models for predicting future states/rewards, extended value functions for k-hop neighbors, and theoretical analysis of performance bounds. Empirical results on traffic control tasks demonstrate improved sample efficiency compared to baselines.","['How does the method scale to larger numbers of agents?', ""How sensitive is DMPO's performance to the choice of k for the k-hop neighborhood?"", 'What are the key advantages of DMPO over existing model-based MARL approaches?', 'How does the localized model prediction accuracy affect overall performance?']","['Experiments limited to traffic control domains', 'Theoretical assumptions may not hold in all practical scenarios', 'Potential negative societal impacts not thoroughly discussed']",False,3,3,3,7,4,"['Novel combination of model-based RL and decentralized multi-agent learning', 'Rigorous theoretical analysis with performance bounds', 'Improved sample efficiency demonstrated in experiments', 'Addresses an important problem in scaling multi-agent RL to networked systems']","['Limited experimental evaluation, primarily on traffic control tasks', 'Scalability to very large networks not demonstrated', 'Some theoretical assumptions may limit real-world applicability', 'Lack of comparison to other model-based MARL approaches']",3,3,3.0,3,Accept
buSCIu6izBY,"This paper introduces Maximum Credit Assignment Occupancy (MaCAO), a novel reinforcement learning approach that interprets rewards as log-likelihoods of state occupancy and optimizes a variational lower bound balancing reward maximization and exploration. The method shows promising results on continuous control benchmarks, especially for sparse reward tasks.","['How sensitive is the method to hyperparameter choices, particularly beta and lambda?', 'What are the computational requirements compared to baselines?', 'How well does the method scale to higher-dimensional problems?', 'Can you provide more ablation studies to isolate the impact of different components?']","['Empirical evaluation limited to a small set of environments', 'Potential scalability issues not addressed', 'Lack of analysis on sample complexity or convergence guarantees', 'Computational overhead not thoroughly analyzed']",False,3,3,4,7,4,"['Novel theoretical framework combining reward maximization and exploration', 'Principled variational inference formulation', 'Strong empirical results, particularly on sparse reward tasks', 'Detailed mathematical derivations and proofs']","['Complex theoretical justification that may be difficult for some readers to follow', 'Limited empirical evaluation and lack of ablation studies', 'Potential sensitivity to hyperparameters', 'Inadequate discussion of limitations and potential negative impacts']",4,3,3.0,4,Accept
CC-BbehJKTe,"This paper introduces the concept of 'winning trees' in genetic programming for symbolic regression. It combines deterministic and random simplification techniques to reduce bloat and uses simplified subtrees from good solutions to seed new GP runs. Experiments on synthetic and real-world regression problems demonstrate improvements in solution quality, convergence speed, and reduced bloat, particularly for larger tree depths.","[""Can you provide a more formal definition of 'winning trees' and explain how they differ from traditional building blocks in GP?"", 'How might this approach be adapted for other GP domains beyond symbolic regression?', 'What is the computational overhead of the simplification process, and how does it scale with problem complexity?', 'How does your method compare to specific state-of-the-art bloat control techniques (e.g., parsimony pressure, multi-objective optimization) in terms of performance and solution quality?']","['The approach may not generalize well beyond symbolic regression problems without modification', 'Limited theoretical analysis of why winning trees are effective, which could hinder further development', 'Some arbitrary design choices (e.g., tree depth limits) that may impact results and require further justification', ""Potential for reduced diversity in solutions, which could impact the method's effectiveness on more complex problems""]",False,3,2,3,7,4,"[""Novel 'winning trees' concept shows promise for improving GP performance"", 'Extensive experimental evaluation on multiple benchmark problems', 'Demonstrates benefits in solution quality, convergence speed, and bloat reduction in many cases', 'Effective combination of multiple simplification techniques', 'Provides insights into the role of solution complexity and building blocks in GP']","[""Limited theoretical justification for the 'winning trees' concept"", 'Experiments focused primarily on symbolic regression, raising questions about generalizability', 'Some design choices and parameter selections lack clear justification', 'Writing and presentation could be improved for clarity in some sections', 'Limited comparison to other state-of-the-art GP bloat control or simplification methods']",3,3,3.0,3,Accept
AUszBTiYBB6,"This paper introduces Tmean, a new aggregation method for Byzantine-robust federated learning. Tmean works by trimming extreme gradient values before averaging. The authors provide theoretical analysis of Byzantine resilience and convergence properties, including proofs for convex optimization settings. They also present empirical evaluations comparing Tmean to other methods under various attack scenarios on MNIST and CIFAR-10 datasets.","['How would Tmean perform on more complex datasets and model architectures?', 'How sensitive is the performance to the choice of trimming parameter b?', 'What are the computational tradeoffs of Tmean compared to other aggregation methods?', 'How well does Tmean generalize to non-convex optimization problems like deep neural networks?', 'What are potential real-world applications where Tmean could be particularly beneficial?']","['Experiments limited to simple datasets (MNIST, CIFAR-10) and models', 'Analysis focused on convex optimization settings, which may not fully represent real-world deep learning scenarios', 'Potential increased computational cost not thoroughly analyzed', 'Sensitivity to trimming parameter b not extensively explored', 'Lack of discussion on how to choose the optimal trimming parameter in practice']",False,3,3,3,6,4,"['Novel aggregation method (Tmean) that improves robustness in federated learning', 'Solid theoretical analysis of Byzantine resilience and convergence properties', 'Empirical evaluations demonstrating advantages over existing methods in some attack scenarios', 'Addresses an important problem in federated learning (Byzantine robustness)']","['Limited novelty, as trimmed mean is a known statistical technique', 'Empirical evaluation limited to simple models/datasets (MNIST and CIFAR-10)', 'Does not consistently outperform all existing methods across all attack scenarios', 'Lacks discussion on limitations, potential negative impacts, and practical considerations (e.g., choosing the trimming parameter b)']",2,3,3.0,3,Accept
7gWSJrP3opB,"This paper introduces a unified theoretical framework for analyzing example selection schemes in stochastic gradient descent (SGD). The key contribution is an 'average gradient error' condition that enables proving convergence rates for various SGD variants. The paper recovers and improves existing rates for methods like shuffle once and random reshuffling, and proposes two new example selection methods: QMC-based data augmentation and a greedy selection algorithm. Theoretical analysis is supported by empirical results on image classification tasks.","['Could the empirical evaluation be extended to other domains beyond image classification?', 'What are the computational trade-offs of the greedy selection algorithm compared to simpler methods, especially for large datasets?', 'How well do the theoretical assumptions hold for typical deep learning tasks?', 'Are there potential extensions or generalizations of the theoretical framework to other optimization algorithms?']","['The theoretical framework may not fully capture all practical aspects of SGD in complex deep learning scenarios', 'Empirical evaluation is focused mainly on image classification tasks, limiting generalizability', 'Potential scalability issues of the greedy selection algorithm for very large datasets']",False,4,3,4,7,4,"['Novel and general theoretical framework unifying analysis of various SGD variants', 'Rigorous theoretical analysis with recovery and improvement of existing results', 'Introduction of two promising new example selection methods', 'Empirical results supporting theoretical claims']","['Empirical evaluation is somewhat limited in scope, focusing mainly on image classification', 'Highly technical content may limit accessibility to a broader audience', 'Scalability and computational cost of the greedy selection algorithm for large datasets not thoroughly discussed']",4,4,4.0,4,Accept
04pGUg0-pdZ,"This paper presents a consensus-based actor-critic algorithm for fully decentralized multi-agent reinforcement learning (MARL) in the average reward setting. The main contributions include: (1) A novel algorithm that doesn't require knowledge of joint actions, (2) The first finite-time convergence and sample complexity analysis for MARL with average reward, (3) Achieving sample complexity matching state-of-the-art single-agent RL algorithms, and (4) Improved communication efficiency compared to classical MARL methods. The paper provides rigorous theoretical analysis with detailed proofs and some supporting experimental results.","[""How does the algorithm's performance scale with increasing numbers of agents?"", 'Are there plans to evaluate the approach on more complex environments or real-world multi-agent problems?', 'Can the analysis be extended to non-linear function approximation?', 'How does the proposed algorithm compare empirically to other recent MARL approaches?', 'What are potential real-world applications of this work?']","['The analysis is limited to linear function approximation', 'Empirical evaluation is limited to simple synthetic experiments', 'The approach may not scale well to very large numbers of agents', 'Potential negative societal impacts are not discussed']",False,4,3,4,7,4,"['First finite-time convergence result for fully decentralized MARL in average reward setting', 'Sample complexity matches state-of-the-art single-agent RL algorithms', 'Improves communication efficiency over classical MARL approaches', 'Rigorous theoretical analysis with detailed proofs', 'Addresses a more practical MARL setting by not assuming joint action knowledge']","['Limited empirical evaluation with only simple synthetic experiments', 'Assumes linear function approximation, which may be restrictive for complex environments', 'May not scale well to very large numbers of agents', 'Lacks discussion of potential negative societal impacts', 'Does not compare empirically to other state-of-the-art MARL algorithms']",4,4,4.0,4,Accept
B2pZkS2urk_,"This paper introduces Evolutionary Plastic Recurrent Neural Networks (EPRNN), a biologically-inspired meta-learning framework that combines evolution strategies, plasticity rules, and recurrent neural networks. The approach is evaluated on sequence prediction and robot navigation tasks, demonstrating some advantages over baseline methods.","['How would this approach scale to more complex task domains?', 'Can you provide a more comprehensive comparison to state-of-the-art meta-learning methods like MAML?', 'What is the theoretical justification for why this combination of mechanisms is effective?', 'How does the computational efficiency compare to gradient-based meta-learning approaches?', 'What are potential real-world applications for this approach?']","['Evaluation limited to simple task domains', 'Unclear scalability to more complex problems', 'Computational efficiency may be a concern for larger models/tasks', 'Some biological plausibility claims not fully supported']",False,3,3,2,4,3,"['Interesting combination of evolutionary strategies, plasticity, and RNNs for meta-learning', 'Biologically-inspired approach with potential for new insights', 'Shows some improvements over baseline methods on evaluated tasks', 'Provides analysis of learned plasticity rules and network behavior']","['Evaluation limited to relatively simple tasks with unclear scalability', 'Insufficient comparison to state-of-the-art meta-learning methods', 'Some claims about biological plausibility not fully substantiated', 'Lack of analysis on computational efficiency and scalability', 'Writing and presentation need improvement in some areas', 'Lack of theoretical justification for the effectiveness of the proposed combination']",2,2,2.0,2,Reject
iMqTLyfwnOO,"This paper introduces the augmented sliced Wasserstein distance (ASWD), a novel variant of sliced Wasserstein distances. ASWD maps samples to higher-dimensional hypersurfaces using injective neural networks before applying random projections, allowing for more flexible nonlinear slicing of distributions. The authors provide theoretical proof that ASWD is a valid metric under certain conditions and demonstrate its superior performance over existing methods on tasks including generative modeling and sliced Wasserstein flows.","['How sensitive is the method to the choice of neural network architecture for the injective mapping?', 'Are there ways to reduce the computational cost while maintaining performance?', 'Can you provide any theoretical guarantees or intuition on the sample complexity and convergence rates of ASWD compared to other methods?', 'How well does ASWD scale to very high-dimensional data or large datasets with millions of samples?']","['The increased computational cost may limit applicability to very large-scale problems', 'The method introduces additional hyperparameters which may require careful tuning', 'Theoretical analysis is somewhat limited compared to simpler methods', 'Performance on a wider range of real-world datasets and tasks is not fully explored']",False,4,3,4,8,4,"['Novel and well-motivated approach to improve sliced Wasserstein distances', 'Strong theoretical contribution with rigorous mathematical formulation and proofs', 'Comprehensive empirical evaluation showing consistent improvements over baselines', 'Flexible framework that can be adapted to different applications', 'Addresses an important problem in computational optimal transport']","['Higher computational cost compared to simpler sliced Wasserstein distances', 'Introduces additional hyperparameters that may require careful tuning', 'Limited theoretical analysis of sample complexity and convergence rates', 'May be challenging to implement for practitioners not familiar with optimal transport theory']",4,4,3.0,4,Accept
yx_uIzoHJv,"This paper proposes incorporating topological similarity as an auxiliary loss to induce compositionality in emergent languages between neural agents. Extensive experiments analyze the effects across different input spaces, message structures, and agent architectures, demonstrating situational benefits to generalization, convergence speed, and language transmission.","['Can you provide stronger theoretical justification for why topological similarity should induce compositionality?', 'How does this method compare to other approaches for inducing compositionality in emergent languages?', 'How well does the method scale to more complex input spaces beyond attribute-value pairs?', 'Have you considered adaptive pressure that changes during training to potentially improve results?']","['Experiments limited to simple attribute-value input spaces', 'Lack of theoretical analysis of why the method works', 'Limited comparison to other compositionality induction techniques', 'Results may not generalize to more complex scenarios or real-world data']",False,3,3,2,5,4,"['Novel method for inducing compositionality without complex iterated learning', 'Comprehensive experiments across different parameters and configurations', 'Demonstrates improvements in generalization and convergence speed in many cases', 'Interesting analysis of language transmission to new learners', 'Thorough investigation of the relationship between compositionality and generalization']","['Limited theoretical justification for the topological similarity approach', 'Inconsistent improvements across different configurations', 'Experiments restricted to simple attribute-value input spaces', 'Lack of comparison to other compositionality induction methods', 'Some results lack statistical significance or have high variance']",3,2,3.0,3,Accept
4pijrj4H_B,This paper introduces fairness-aware data augmentation techniques for graph neural networks (GNNs). The authors provide a theoretical analysis of bias sources in GNN representations and develop adaptive augmentation methods for both node features and graph structure. Extensive experiments on real networks demonstrate improved fairness metrics while maintaining comparable utility on node classification and link prediction tasks.,"['How would the proposed techniques extend to multi-class or continuous sensitive attributes?', 'Can you provide more details on the hyperparameter tuning process and guidelines for selection?', 'Have you considered evaluating on a wider range of graph datasets and tasks?', 'What are the potential negative societal impacts or failure modes of the proposed methods?', 'What is the computational overhead of the proposed methods compared to standard GNN approaches?']","['Focus on binary sensitive attributes only', 'Experiments limited to a few datasets and tasks', 'Some hyperparameter sensitivity that requires tuning', 'Limited discussion of potential negative societal impacts', 'Potential increased computational complexity compared to standard GNN methods']",False,4,3,3,7,4,"['Novel theoretical analysis of sources of bias in GNN representations', 'Development of adaptive graph augmentation techniques for both node features and graph structure', 'Flexible methods compatible with various GNN frameworks', 'Extensive experiments showing improved fairness metrics while maintaining utility', 'Addresses an important problem in graph representation learning']","['Analysis and techniques limited to binary sensitive attributes', 'Evaluation focused on a limited set of datasets and tasks', 'Some hyperparameter sensitivity requiring tuning', 'Limited discussion of potential negative societal impacts']",3,4,4.0,4,Accept
vh-0sUt8HlG,"This paper introduces MobileViT, a lightweight vision transformer architecture for mobile devices. It combines convolutions and transformers in a novel way, applying transformers as convolutions on image patches. MobileViT outperforms both lightweight CNNs and other vision transformers on classification, detection, and segmentation tasks while being smaller and faster.","['Can you provide more theoretical justification for the MobileViT block design?', 'How does MobileViT compare to very recent efficient vision transformers like MobileFormer?', 'What are the key limitations of the approach?', 'Have you explored applying MobileViT to other vision tasks beyond classification/detection/segmentation?', 'Can you elaborate on potential negative societal impacts of this work?']","['The approach is focused only on 2D vision tasks currently', 'Potential negative impacts of more efficient mobile vision models are not thoroughly discussed', 'Trade-offs between accuracy and efficiency could be analyzed more deeply', 'Experiments are limited to standard vision benchmarks - real-world mobile use cases are not extensively evaluated']",False,3,3,3,7,4,"['Novel architecture design combining strengths of CNNs and transformers', 'Strong empirical results across multiple vision tasks (classification, detection, segmentation)', 'Smaller model size and faster inference than competing methods', 'Detailed ablation studies validating key components', 'Potential for significant impact on mobile vision applications', 'Introduction of a multi-scale training technique that improves efficiency and performance', 'Extensive experimental validation across various datasets and tasks']","['Lacks theoretical analysis or justification for design choices', 'Limited comparison to very recent vision transformer variants', 'Could benefit from more analysis of trade-offs and limitations', 'Improvements over MobileNetV3 are relatively modest on some benchmarks', 'Latency on mobile devices is still higher than MobileNets in some cases', 'Limited discussion of potential negative societal impacts']",4,3,3.0,4,Accept
0rjx6jy25R4,"This paper proposes a novel method called Classifier-Noise-Invariant Conditional GAN (CNI-CGAN) to jointly address positive-unlabeled (PU) classification and conditional generation using limited labeled data and extra unlabeled data. The approach uses a PU classifier to predict labels on unlabeled data, develops a noise-robust conditional GAN to learn from these potentially noisy labels, and then uses the generated samples to improve the PU classifier. The authors provide theoretical analysis and demonstrate empirical improvements over baselines on multiple datasets including MNIST, Fashion-MNIST, and CIFAR-10.","['How well does the method scale to more complex, real-world datasets beyond MNIST/CIFAR?', 'How sensitive is the method to the quality of the initial PU classifier and the choice of hyperparameters?', 'How does the approach compare to recent advances in semi-supervised learning?', 'What are the main failure cases or limitations of the method?', 'What are some potential real-world applications where this method could be particularly useful?']","['Evaluation limited to relatively simple image datasets', 'May not scale well to very high-dimensional or complex data', 'Potential privacy concerns with using extra unlabeled data not addressed', 'Computational requirements and scalability not thoroughly discussed', 'Challenges in applying this method to other domains (e.g., text, audio) not explored']",False,3,3,3,7,4,"['Novel combination of PU learning and conditional generation', 'Theoretical analysis providing guarantees for the method', 'Consistent empirical improvements over baselines across multiple datasets', 'Addresses an important problem of leveraging limited labeled data', 'Innovative approach to handling noisy labels in conditional generation']","['Evaluation limited to relatively simple datasets', 'Heavy reliance on the quality of the initial PU classifier', 'Missing comparisons to some recent state-of-the-art methods (e.g., semi-supervised learning approaches)', 'Limited discussion of limitations, failure cases, and potential negative impacts']",4,3,3.0,3,Accept
7B3IJMM1k_M,"This paper presents a novel method for converting artificial neural networks (ANNs) to spiking neural networks (SNNs) that achieves high accuracy with very low latency. The key contributions are: 1) A detailed analysis of conversion errors, including a new 'unevenness error'. 2) A new 'quantization clip-floor-shift' activation function to replace ReLU in the source ANN. 3) Theoretical proof that the expected conversion error is zero. 4) State-of-the-art results on image classification benchmarks, achieving high accuracy with very few time steps (e.g., 91% on CIFAR-10 with only 2 steps).","['How does the proposed method perform on non-CNN architectures, such as transformers or recurrent neural networks?', 'What is the computational overhead of training ANNs with the new activation function compared to standard ReLU networks?', 'How does the method scale to larger datasets and more complex tasks beyond image classification?', 'Can the authors provide insights on how to optimally choose the quantization steps L for different network architectures?']","['Method may not generalize to all ANN architectures', 'Energy efficiency claims require more thorough validation', 'Only evaluated on image classification tasks']",False,4,3,4,8,4,"['Novel and thorough analysis of ANN-SNN conversion errors', ""Innovative 'quantization clip-floor-shift' activation function with theoretical guarantees"", 'State-of-the-art results on standard datasets, especially at low latencies (2-32 time steps)', 'Comprehensive experiments and ablation studies', 'Potential for significant impact on practical SNN applications and neuromorphic computing']","['Limited comparison to directly-trained SNNs', 'Energy analysis is fairly basic and does not consider memory access costs', 'Evaluation focused only on image classification tasks', 'Some hyperparameters (e.g., quantization steps L) require tuning']",4,4,4.0,4,Accept
k7efTb0un9z,"This paper introduces Graph Network-based Scheduler (GNS), a reinforcement learning approach for learning optimal learning rate schedules. GNS represents neural networks as graphs and uses graph neural networks to encode network state, learning a policy to dynamically adjust learning rates. Experiments on image classification and NLP tasks demonstrate consistent improvements over baseline methods and good generalization across different architectures and datasets.","['How does the method perform on very long training runs (e.g., language model pretraining)?', 'Is there any theoretical motivation for the effectiveness of the graph representation?', 'How does the computational overhead of GNS compare to standard schedules for large models?', 'How sensitive is GNS to the choice of graph neural network architecture?']","['Computational overhead may be significant for very large models', 'Lack of theoretical guarantees', 'Not evaluated on extremely large-scale tasks', 'Still requires some hyperparameter tuning']",False,3,3,3,7,4,"['Novel use of graph neural networks to encode neural network state for learning rate scheduling', 'Strong empirical results on both image and language tasks', 'Demonstrates good generalization to different network architectures and datasets', 'Efficient reward collection procedure', 'Thorough ablation studies and analysis']","['Added complexity compared to standard learning rate schedules', 'Lack of theoretical analysis or guarantees', 'Limited evaluation on very long training runs (e.g., large-scale pretraining)', 'Some hyperparameters still require tuning']",4,3,3.0,4,Accept
hcoswsDHNAW,"This paper introduces Fast AdvProp, a method to accelerate adversarial propagation (AdvProp) training without sacrificing performance. Key ideas include decoupled training with a small portion of adversarial examples, reusing gradients, and techniques to stabilize training. Results demonstrate comparable or better performance to regular AdvProp with the same computational budget as standard training across multiple datasets and model sizes.","['Can you provide more theoretical justification for why decoupling clean and adversarial samples preserves AdvProp benefits?', 'How does Fast AdvProp compare to other efficient adversarial training methods?', 'Have you tested Fast AdvProp on more modern architectures like Vision Transformers?', 'How sensitive is the method to hyperparameter choices like the portion of adversarial examples?', 'Are there potential applications of Fast AdvProp beyond computer vision tasks?']","['Primarily evaluated on image classification and object detection tasks', 'May not generalize to all types of adversarial attacks or model architectures', 'Potential negative impact of increased energy consumption from more widespread use of adversarial training not thoroughly discussed']",False,3,3,3,7,4,"['Addresses an important practical issue (computational cost) of AdvProp', 'Achieves comparable or better performance than vanilla training with the same budget', 'Thorough empirical evaluation on multiple datasets and model sizes', 'Compatible with other data augmentation techniques', 'Demonstrates improved robustness on multiple benchmarks', 'Scales well to larger models', 'Clear ablation studies and analysis of the proposed method']","['Limited theoretical justification for some design choices', 'Improvements over standard training are sometimes small (e.g., 0.3% on ImageNet)', 'Evaluation limited primarily to image classification and object detection tasks', 'Tested mainly on ResNet architectures', 'Introduces additional complexity compared to standard training']",4,3,3.0,3,Accept
QKEkEFpKBBv,"This paper introduces Differentiable Nonparametric Belief Propagation (DNBP), a novel method combining graphical models with deep learning for articulated object tracking. DNBP replaces hand-crafted potential functions in belief propagation with learned neural networks, enabling end-to-end training while maintaining uncertainty estimation capabilities. The approach is evaluated on simulated articulated tracking tasks and real-world hand pose estimation, demonstrating competitive performance.","['How does the computational complexity of DNBP compare to traditional nonparametric belief propagation and other deep learning baselines?', 'Are there any theoretical guarantees on the convergence or optimality of the learned potentials?', 'How sensitive is the method to the choice of graph structure? Could this be learned as well?', 'How does DNBP perform on more complex articulated structures beyond those tested?', 'What other potential applications could benefit from the DNBP approach beyond articulated object tracking?']","['The method requires a predefined graph structure', 'Scalability to more complex articulated objects is not thoroughly explored', 'The approach may have higher computational requirements compared to pure deep learning methods', 'Evaluation limited to articulated tracking tasks', 'Requires labeled training data for supervised learning']",False,3,3,3,7,4,"['Novel combination of probabilistic graphical models and deep learning', 'End-to-end trainable approach for articulated object tracking', 'Ability to estimate uncertainty in predictions', 'Competitive performance on challenging articulated tracking tasks', 'Thorough empirical evaluation on both simulated and real-world datasets']","['Limited theoretical analysis or guarantees', 'Reliance on predefined graph structure', 'Computational complexity not thoroughly analyzed', 'Comparison to state-of-the-art methods could be expanded in some areas']",4,3,3.0,4,Accept
AsyICRrQ7Lp,"This paper introduces Bootstrapped Hindsight Experience Replay (BHER), an extension of HER that combines bootstrapped Q-learning with a novel counterintuitive prioritization scheme. BHER shows significant improvements over baselines on several goal-conditioned robotics tasks, demonstrating increased sample efficiency in sparse reward settings.","['Can you provide more theoretical justification for why prioritizing low-variance transitions is effective?', 'How does the computational cost of BHER compare to standard HER?', 'Have you compared BHER to very recent HER variants published in the last 1-2 years?', 'How sensitive is the method to the choice of key hyperparameters, such as the number of Q-value heads and temperature coefficient?', 'What potential impact could BHER have on real-world robotics applications?']","['The approach is only tested on relatively simple goal-conditioned environments', 'Computational overhead of using multiple Q-value heads is not thoroughly analyzed', 'May not generalize well to non-goal-conditioned RL tasks', 'Potential negative impacts of improved goal-conditioned RL not thoroughly discussed']",False,3,3,3,8,4,"['Novel combination of bootstrapped Q-learning with HER', 'Interesting counterintuitive prioritization scheme that prioritizes low-variance transitions', 'Strong empirical results across multiple goal-conditioned environments', 'Comprehensive ablation studies validating key components']","['Lack of theoretical justification for the counterintuitive prioritization scheme', 'Limited comparison to recent HER variants', 'Unclear analysis of computational overhead from using multiple Q-value heads', 'Some concerns about hyperparameter sensitivity']",4,3,3.0,4,Accept
lQI_mZjvBxj,"This paper introduces federated kernel regression to analyze knowledge distillation (KD) in model-agnostic federated learning. It reveals limitations of alternating KD, especially with data heterogeneity, and proposes new variants like averaged KD (AvgKD) and ensembled KD (EKD). Theoretical predictions are validated through experiments on neural networks.","['How does the proposed approach compare to other model-agnostic federated learning methods?', 'Can the EKD method be made more practical by reducing ensemble size?', 'How well do the results generalize to more complex datasets and models?', 'What are the privacy implications of the proposed methods?']","['Experiments limited to simple datasets (MNIST, CIFAR-10)', 'Proposed EKD method requires large ensembles which may be impractical', 'Analysis limited to two-agent case for most part', 'Privacy and security aspects not thoroughly addressed']",False,4,3,3,6,4,"['Novel theoretical framework (federated kernel regression) for analyzing KD in federated settings', 'Rigorous theoretical analysis revealing fundamental limitations of KD approaches', 'Proposal of new variants (AvgKD, EKD) to address limitations', 'Empirical validation on real neural networks matching theoretical predictions', 'Addresses an important problem in model-agnostic federated learning', 'Potential to significantly impact future research in model-agnostic federated learning']","['Limited experiments (only on MNIST and CIFAR-10)', 'Proposed EKD method requires large ensembles which may be impractical', 'No comparison to other model-agnostic federated learning approaches', 'Privacy and scalability concerns not fully addressed']",4,3,3.0,3,Accept
OWZVD-l-ZrC,"This paper introduces RUNE, a novel exploration method for preference-based reinforcement learning that uses uncertainty in the learned reward function as an intrinsic reward. The authors show improved sample and feedback efficiency on robotic manipulation tasks from Meta-World.","['Can you provide more theoretical justification for why reward uncertainty is an effective exploration signal in this context?', 'How well would RUNE generalize to other domains beyond robotic manipulation?', 'Have you considered comparing RUNE to more recent exploration methods in preference-based RL?']","['Experiments limited to the Meta-World benchmark', 'Lack of theoretical guarantees', 'Potential scalability issues in high-dimensional state/action spaces']",False,3,4,3,7,4,"['Novel exploration approach specifically designed for preference-based RL', 'Consistent empirical improvements over baselines across multiple tasks', 'Thorough evaluation with useful ablation studies', 'Clear writing and presentation of the method', 'Simple and efficient implementation without additional model architectures']","['Limited theoretical justification for the approach', 'Evaluation restricted to simulated robotic tasks from a single benchmark', 'Comparison limited to a small set of baseline methods', 'Potential scalability issues with the ensemble of reward models']",3,3,4.0,4,Accept
ghTlLwlBS-,"This paper proposes a Feudal Reinforcement Learning (FRL) framework for the 'reading to act' problem, consisting of a high-level manager that reasons over text to generate sub-goals, and a low-level worker that executes actions to achieve those sub-goals. Key innovations include a multi-hop reasoning manager and backwards sub-goal generation. The approach achieves near-perfect performance on the RTFM benchmark without curriculum learning, significantly outperforming previous methods, and shows generalization to the Messenger environment.","['How might this approach scale to more complex language inputs or environments?', 'How does the performance compare to other hierarchical RL methods beyond FRL?', 'Could the backwards sub-goal generation be applied to other RL problems beyond reading-to-act?', 'Have you explored end-to-end training of the full model?', 'What are some potential real-world applications for this approach?']","['Evaluation limited to only two relatively simple environments', 'May not generalize well to more complex language inputs or environments', 'Potential negative impact: Could enable automated systems to act on textual instructions without human oversight', 'Assumes perfect manager for worker training']",False,3,3,3,7,4,"['Novel hierarchical approach to address semantic mismatch between language and actions', 'Multi-hop reasoning manager with backwards sub-goal generation', 'Impressive empirical results on RTFM benchmark without curriculum learning', 'Generalizes to Messenger environment, showing flexibility', 'Thorough ablation studies validating key components', 'Effectively addresses the semantic mismatch problem in reading-to-act tasks']","['Limited evaluation on only two environments', 'Lack of comparison to other hierarchical RL approaches', 'Unclear how well it would scale to more complex language or environments', 'Some simplifying assumptions in training process']",4,3,4.0,3,Accept
aq6mqSkwApo,"This paper introduces Meta-OLE, a novel approach for few-shot image classification that imposes orthogonal low-rank geometry on the feature space and meta-learns an adaptive transformation for each task. The method demonstrates state-of-the-art performance on standard few-shot learning benchmarks.","['What is the computational complexity of the method compared to other approaches?', 'How well does the method generalize to other domains beyond image classification?', 'Are there any theoretical guarantees that can be provided for the approach?', 'How sensitive is the method to hyperparameter choices?', 'Can this approach scale to larger backbone networks and datasets?']","['Experiments limited to image classification tasks', 'Lack of theoretical analysis', 'Computational requirements not thoroughly analyzed', 'Potential sensitivity to hyperparameter choices', 'Scalability to larger models and datasets not demonstrated']",False,4,4,4,8,4,"['Novel geometric approach combining metric learning and meta-learning ideas', 'Strong empirical results across multiple standard benchmarks', 'Thorough ablation studies validating different components', 'Well-motivated method with clear geometric intuitions', 'Ability to leverage unlabeled query samples without extra network components', 'Efficient closed-form solution for final classification']","['Limited theoretical analysis or guarantees', 'Computational complexity not thoroughly discussed', 'Only image classification tasks considered', 'Potential sensitivity to hyperparameter choices', 'Scalability to larger backbone networks and datasets not demonstrated']",4,3,4.0,4,Accept
eMudnJsb1T5,"This paper introduces a new family of particle evolution samplers for constrained domains and non-Euclidean geometries, based on a novel class of mirrored Stein operators. The key contributions are three new algorithms: Mirrored SVGD (MSVGD), Stein Variational Mirror Descent (SVMD), and Stein Variational Natural Gradient (SVNG). The paper provides theoretical convergence guarantees and demonstrates empirical improvements over baselines on simplex-constrained problems, post-selection inference tasks, and large-scale Bayesian inference.","['How does the computational complexity of MSVGD, SVMD, and SVNG compare to existing methods?', 'Have the algorithms been tested on problems with more than 100 dimensions?', 'What are the key considerations for practitioners when choosing between MSVGD, SVMD, and SVNG?']","['Scalability to very high-dimensional problems may be limited', 'Empirical evaluation focused on a relatively narrow set of tasks', 'Potential negative impacts of more accurate Bayesian inference in sensitive domains not thoroughly discussed']",False,4,4,4,8,4,"['Novel theoretical development of mirrored Stein operators', 'Introduction of three new sampling algorithms for constrained domains', 'Strong theoretical foundations with convergence guarantees', 'Empirical improvements demonstrated on several important problems', 'Addresses limitations of existing methods for constrained domains and non-Euclidean geometries']","['Computational complexity of SVMD and SVNG may limit scalability to very high-dimensional problems', 'Empirical evaluation focused on a relatively narrow set of tasks', 'Implementation and tuning of algorithms may require significant expertise']",4,4,4.0,4,Accept
VNdFPD5wqjh,"This paper introduces Domain Generalizable Person Re-identification Without Demographics (DGWD-ReID) and proposes Unit DRO, a novel method to address this problem. Unit DRO reformulates KL-constrained distributionally robust optimization into an importance sampling approach that can work with overparameterized networks. Comprehensive experiments demonstrate that Unit DRO outperforms previous DG ReID methods on multiple benchmarks, even without using demographic information.","['Can you provide more theoretical justification for why Unit DRO works better than standard KL-DRO?', 'How does Unit DRO compare to other DRO formulations beyond Group DRO?', 'Could the method be applied to other domain generalization problems beyond ReID?', 'How sensitive is the method to hyperparameters like the queue size and step τ* schedule?', 'How does the method scale to larger datasets or more complex ReID scenarios?']","['The method introduces additional hyperparameters that may require tuning', 'Theoretical guarantees are limited', 'Only tested on ReID tasks, generalizability to other domains is unclear', 'Potential negative impacts on privacy if used for surveillance without proper ethical considerations', 'Interpreting the learned sample weights may be challenging']",False,3,3,4,7,4,"['Introduces a novel and practical problem setting (DGWD-ReID)', 'Proposes an innovative method (Unit DRO) that is well-motivated and effective', 'Strong empirical results, consistently outperforming state-of-the-art methods', 'Comprehensive analysis and ablation studies', 'Addresses an important limitation of previous work (reliance on demographics)']","['Theoretical justification for Unit DRO could be strengthened', 'Some baseline comparisons (e.g., to other DRO approaches) are missing', 'Writing and presentation could be improved in some sections', 'Experiments are limited to person re-ID tasks', 'Potential computational overhead of maintaining the weights queue']",4,3,3.0,4,Accept
_j4hwbj6Opj,"This paper introduces a meta-learning approach for 3D point cloud registration called 3D Meta-Registration. The key contributions include: (1) Formulating registration as a meta-learning problem, (2) Using a meta-learner to predict initialization for the registration network, (3) Incorporating a VAE to learn a distribution over registration tasks, and (4) Proposing a region-aware flow regression module. Experiments on multiple datasets demonstrate improved performance over previous methods, especially on unseen data.","['Can you provide more theoretical justification for why the meta-learning approach improves generalization?', 'How does the method compare to other meta-learning approaches like MAML?', 'What are the key failure cases or limitations of the approach?', 'How does computational efficiency compare to other methods?', 'Why was a VAE chosen for the task distribution estimator in the meta-learner?']","['The approach still requires a large amount of supervised training data', 'The complex model architecture may limit practical deployment', 'Performance on very large point clouds or partial data is not evaluated', ""The method's effectiveness on drastically different domains (e.g., medical imaging) is not explored""]",False,3,3,3,7,4,"['Novel application of meta-learning to 3D point cloud registration', 'Improved generalization to unseen data and categories', 'Comprehensive experiments on multiple datasets', 'Well-motivated technical approach with region-aware flow regression', 'Strong performance on unseen data, demonstrating good generalization']","['Limited theoretical analysis or justification for the meta-learning approach', 'Complexity of the overall model architecture', 'Explanation of the meta-learning formulation could be clearer', 'Limited discussion of failure cases and practical limitations']",4,3,3.0,3,Accept
iEvAf8i6JjO,"This paper introduces Trust Region Gradient Projection (TRGP), a novel approach for continual learning that facilitates forward knowledge transfer without forgetting. Key innovations include: (1) A 'trust region' to select old tasks most related to the new task, (2) Scaled weight projection to leverage knowledge from selected old tasks, and (3) Joint optimization of scaling matrices and model weights. Comprehensive experiments demonstrate significant improvements over state-of-the-art methods on multiple datasets and architectures.","['How could TRGP be extended to handle more complex real-world scenarios with dynamic task distributions?', 'What are the potential implications of TRGP for developing more general AI systems?', 'How might TRGP be integrated with other continual learning approaches for even better performance?', 'What are the computational trade-offs of TRGP in resource-constrained environments?', 'How does TRGP perform in scenarios with noisy or partially labeled data?']","['Increased computational complexity compared to simpler methods', 'Performance may depend on proper tuning of hyperparameters', 'Scalability to very large numbers of tasks is not demonstrated', 'Limited discussion of potential negative societal impacts', 'Potential challenges in adapting to real-world scenarios with concept drift or data distribution shifts']",False,4,4,4,8,4,"['Novel and well-motivated technical approach combining trust region and scaled weight projection ideas', 'Significant and consistent performance gains over state-of-the-art methods across multiple benchmarks', 'Comprehensive empirical evaluation on various datasets and architectures', 'Detailed ablation studies validating key components of the method', 'Addresses important limitations of existing continual learning approaches']","['Additional computational overhead compared to some baseline methods', 'Relies on hyperparameters like trust region threshold that may need tuning', 'Limited theoretical analysis or guarantees', 'Some technical details could be explained more clearly']",4,4,4.0,4,Accept
ptZfV8tJbpe,"This paper introduces a novel method for multi-label text classification using latent label encodings to implicitly model label correlations. The approach concatenates latent label embeddings with input text, processes them through BERT, and maps the latent encodings to actual labels. Experiments on two benchmark datasets demonstrate significant improvements over state-of-the-art baselines.","['Can you provide more details on how the latent label encodings are initialized and updated during training?', 'How does the method scale to datasets with a much larger number of labels?', 'What is the intuition behind why modeling label correlations implicitly works better than explicit modeling?', 'How sensitive is the method to the number of latent labels used, and how was the optimal number chosen?', 'How well might this approach generalize to other domains beyond text classification?']","['Evaluation limited to only two datasets', ""Lack of extensive discussion on the approach's limitations"", 'Potential negative impacts of the work are not addressed', 'Scalability to very large label spaces is not explored', 'Potential computational costs of the approach are not discussed']",False,3,3,3,7,4,"['Novel approach of using latent label encodings to implicitly model label correlations', 'Strong empirical results, outperforming state-of-the-art on major metrics', 'Thorough error analysis and ablation studies', 'Addresses limitations of previous explicit label correlation modeling approaches']","['Methodology description lacks clarity, especially regarding the latent label encoding process', 'Limited theoretical analysis or justification for the effectiveness of the approach', 'Experiments limited to only two datasets', 'Some writing and presentation issues that impact overall clarity']",4,3,3.0,3,Accept
wQStfB93RZZ,"This paper introduces novel multi-agent reinforcement learning algorithms for handling asynchronous macro-actions. The key contributions include formulating policy gradient methods for decentralized, centralized, and centralized training with decentralized execution paradigms that can handle macro-actions. A novel 'Independent Actor with Individual Centralized Critic' (Mac-IAICC) method is proposed to deal with asynchronous terminations. Empirical results on three domains demonstrate clear benefits over primitive action methods.","['Can you provide any theoretical analysis or convergence guarantees for the Mac-IAICC algorithm specifically?', 'Have you explored any approaches for automatically learning or adapting macro-actions during training?', 'How does the performance of Mac-IAICC compare to state-of-the-art hierarchical MARL methods like MAVEN or QMIX?', 'What is the computational complexity of Mac-IAICC compared to the other proposed methods, and how does it scale with the number of agents?']","['The approach requires predefined macro-actions, which may not always be available or optimal', 'Scalability to very large numbers of agents or more complex domains is not demonstrated', 'Evaluations are limited to cooperative settings, not considering competitive or mixed scenarios']",False,3,3,3,7,4,"['Addresses an important problem of incorporating macro-actions in multi-agent RL', 'Proposes novel algorithms for different training paradigms with macro-actions', 'Empirical results clearly demonstrate benefits over primitive action methods', 'Mac-IAICC method shows particular promise in performance', 'Clear explanations and derivations of the methods', 'Extensive empirical evaluation across multiple domains']","['Limited theoretical analysis or convergence guarantees', 'Macro-actions are predefined rather than learned', 'Comparison to other hierarchical/option-based multi-agent methods is limited', 'Evaluations limited to cooperative settings and relatively simple domains']",3,3,3.0,4,Accept
W9G_ImpHlQd,"This paper introduces ZO-AE-DS, a novel method for defending black-box ML models against adversarial attacks using only input-output queries. The approach combines denoised smoothing, zeroth-order optimization, and autoencoders to enable effective black-box defense. Experiments on image classification and reconstruction tasks demonstrate improved certified robustness compared to baseline methods.","['Can you provide theoretical analysis or guarantees for the ZO-AE-DS method?', 'How might the method be extended to other domains beyond image data?', 'What are potential strategies to improve scalability for very high-dimensional inputs?', 'How does the method perform on adversarial attacks beyond norm-bounded perturbations?', 'What are the key trade-offs between query complexity and defense performance?']","['Lack of theoretical guarantees on robustness', 'Potential scalability issues for very high-dimensional inputs', 'Primarily evaluated on image domains', 'Increased computational complexity compared to white-box methods']",False,3,3,4,7,4,"['Novel approach to the important problem of black-box adversarial defense', 'Clever integration of denoised smoothing, zeroth-order optimization, and autoencoders', 'Clear empirical improvements over baseline methods on multiple tasks', 'Addresses a significant problem with practical relevance in ML security', 'Well-motivated problem formulation and sound methodology']","['Lack of theoretical analysis or robustness guarantees', 'Limited experiments on high-dimensional datasets like ImageNet', 'Potential scalability issues for very high-dimensional inputs', 'Some performance degradation compared to white-box defense methods']",4,3,4.0,4,Accept
l431c_2eGO2,"This paper introduces Mix-MaxEnt, a simple yet effective approach to regularize neural networks for improved accuracy and uncertainty estimates. The method combines sample interpolation between different classes with an entropy maximization regularizer. Extensive experiments on CIFAR-10/100 demonstrate state-of-the-art results in accuracy, calibration, out-of-distribution detection, and robustness to domain shift compared to existing single model approaches.","['Can you provide theoretical insights into why maximizing entropy between classes leads to these improvements?', 'How sensitive is the method to the choice of hyperparameters like the beta distribution parameters?', 'Have you evaluated the approach on other domains beyond image classification?', 'How does the computational cost compare to methods like deep ensembles?', 'Are there potential applications of Mix-MaxEnt beyond improving uncertainty estimates?']","The main limitations are the lack of deeper theoretical understanding of the method's effectiveness, the empirical evaluation being restricted to image classification tasks, and the need for further investigation into scalability to larger datasets and more complex architectures. Generalizability to other domains remains to be explored.",False,3,3,4,7,3,"['Simple and effective approach to improve uncertainty estimation', 'State-of-the-art results on multiple metrics', 'Comprehensive empirical evaluation on standard datasets and architectures', 'Creates high entropy barriers between classes', 'Improves robustness to domain shift and corrupted data', 'Easy to implement with minimal changes to training']","[""Limited theoretical justification for the method's effectiveness"", 'Evaluation restricted to image classification tasks', 'Comparison to some recent baselines not fully comprehensive', 'Some hyperparameter tuning required for optimal performance']",4,3,3.0,4,Accept
z3Tf4kdOE5D,"This paper introduces FEDDISCRETE, a novel federated learning framework that employs a probabilistic discretization mechanism on the client side. The key contribution is its ability to simultaneously defend against multiple types of attacks, including availability poisoning, integrity backdoor, and inference attacks, while maintaining model utility. The authors provide theoretical analysis and empirical evaluation on image classification tasks to demonstrate its effectiveness.","['How does FEDDISCRETE perform on non-image datasets and more complex neural network architectures?', 'What is the computational overhead of the discretization mechanism for clients with limited resources?', 'How does the approach scale to very large models with millions of parameters?', 'Can the method be extended or combined with other techniques to defend against byzantine attacks?']","['May have reduced effectiveness in scenarios with a small number of clients', 'Potential trade-off between security and model accuracy', 'Limited evaluation beyond image classification tasks', 'Increased communication rounds compared to standard federated learning', 'Possible impact on convergence speed due to discretization']",False,3,3,3,7,4,"['Novel discretization-based approach for defending against multiple attack types simultaneously in federated learning', 'Comprehensive theoretical analysis of utility, robustness, and convergence', 'Empirical evaluation demonstrates effectiveness against various state-of-the-art attacks', 'Improves upon existing defense methods in terms of performance and communication efficiency', 'Provides a unified defense mechanism against diverse attack types']","['Limited evaluation on non-image datasets and complex tasks', 'Potential scalability issues with very large models or datasets with high dimensionality', 'Does not address byzantine attacks', 'May incur some utility loss compared to standard federated learning']",3,3,3.0,3,Accept
a34GrNaYEcS,"This paper introduces RP-DRO, a novel approach to distributionally robust optimization using parametric likelihood ratios. Key innovations include mini-batch normalization, KL penalty, and simultaneous gradient updates. The method demonstrates superior performance on subpopulation robustness across image and text classification benchmarks, consistently outperforming other DRO approaches.","['Can you provide a more detailed analysis of the computational overhead compared to standard training and other DRO methods?', 'Have you explored ways to reduce computational cost, such as parameter sharing between the model and adversary?', 'Is it possible to extend the approach to other types of tasks beyond classification?', 'Can you provide any theoretical insights or guarantees for RP-DRO?', 'How does RP-DRO perform on more diverse datasets or in real-world scenarios with more complex distribution shifts?']","['Increased computational cost may limit applicability to very large datasets', 'Lack of theoretical guarantees on convergence and robustness', 'Currently only evaluated on classification tasks', 'Performance on more diverse or real-world datasets not explored']",False,4,4,4,8,4,"['Novel parametric likelihood ratio formulation for DRO', 'Strong empirical results across multiple benchmarks', 'Effective mini-batch normalization strategy', 'Thorough experimental analysis and ablation studies', 'Clear writing and explanation of the technical approach', 'Advances state-of-the-art in robust machine learning', 'Comprehensive comparison with existing DRO methods']","['Increased computational cost due to training an adversary', 'Lack of theoretical guarantees or convergence analysis', 'Limited to classification tasks in current evaluation', 'Potential sensitivity to hyperparameters and adversary architecture']",3,4,4.0,4,Accept
izvwgBic9q,"This paper introduces UPFWI, an unsupervised approach to full-waveform inversion that connects a CNN and forward modeling in a loop. The method enables end-to-end learning from seismic data alone, without requiring paired velocity maps. Extensive experiments on a new large-scale OpenFWI dataset demonstrate the potential to match or outperform supervised methods when more unlabeled data is available. The paper also includes useful ablation studies on different loss terms.","['How does the computational cost and scalability of UPFWI compare to traditional iterative FWI methods and supervised deep learning approaches?', 'How well does the method generalize to real-world seismic data with noise and other artifacts?', 'What are the key limitations and failure cases of the approach, particularly for velocity maps with closely spaced layer velocities?', 'How does UPFWI compare to other unsupervised or self-supervised FWI methods in terms of performance and efficiency?', 'What potential impacts, both positive and negative, might this method have on the geophysics industry if widely adopted?']","['Only demonstrated on synthetic data, performance on real seismic data unclear', 'Computational cost and scalability not thoroughly analyzed', 'May struggle with velocity maps where adjacent layers have very close velocity values', 'Potential societal and industry impacts not thoroughly discussed']",False,3,3,3,7,4,"['Novel unsupervised approach connecting CNN and physics-based forward modeling', 'End-to-end learning from seismic data alone without paired velocity maps', 'Outperforms supervised methods when trained on sufficient unlabeled data', 'Introduction of a new large-scale OpenFWI dataset with more complex geological structures', 'Extensive experiments and ablation studies validating the approach', 'Use of perceptual loss improves overall quality of predicted velocity maps']","['Limited evaluation on real-world seismic data', 'Computational cost of forward modeling during training not discussed', 'Insufficient analysis of method limitations and failure cases', 'Lack of comparison to other unsupervised/self-supervised FWI methods', 'Potential negative impacts on the geophysics industry not analyzed']",4,3,3.0,4,Accept
7pZiaojaVGU,"This paper proves an equivalence between gradient attacks and data poisoning attacks in personalized federated learning under local PAC* learning conditions. It formalizes local PAC* learning, proves it holds for linear and logistic regression, and proposes a practical 'counter-gradient attack'. The theoretical results are supported by empirical experiments on MNIST and CIFAR-10.","['How well do the results generalize to non-convex loss functions and deep neural networks?', 'What are the most promising approaches for defending against the proposed counter-gradient attack?', 'How might the results extend to other federated learning settings beyond personalized learning?']","The paper acknowledges limitations in the empirical validation, particularly for neural networks where only the last layer is considered. The assumptions required for the theoretical results may not always hold in practice. The practical feasibility of some data poisoning attacks is not fully addressed, especially in terms of generating realistic poisoned data points.",True,4,4,4,8,4,"['Novel theoretical equivalence between gradient attacks and data poisoning', 'Rigorous proofs and comprehensive theoretical analysis', 'Introduction and formalization of local PAC* learning concept', 'Proposal of practical counter-gradient attack with empirical validation', 'Important implications for federated learning security']","['Assumptions for equivalence may not always hold in practice', 'Limited applicability to complex models like deep neural networks', 'Empirical results limited to relatively simple models/datasets', 'Limited discussion of potential defenses or mitigations']",4,4,4.0,4,Accept
30SXt3-vvnM,"This paper introduces a kernelized classification layer for deep neural networks that learns an optimal kernel function for nonlinear classification on embeddings. The method optimizes over all radial positive definite kernels on the unit sphere and shows consistent improvements over standard softmax classification across multiple tasks, especially for networks with limited capacity backbones.","['How does the method compare to other recent nonlinear classification approaches beyond standard softmax?', 'What is the additional computational cost during inference, especially for larger models or datasets?', 'How sensitive is the method to the choice of M (order of kernel approximation)?', 'Can you provide more analysis of the learned kernel functions and resulting decision boundaries?', 'Are there any cases where the kernelized layer underperforms compared to standard softmax?']","['The method adds some complexity and parameters, which may not be suitable for extremely resource-constrained settings', 'Potential limitations in scaling to very high-dimensional embeddings are not thoroughly explored', 'The theoretical analysis assumes radial kernels and L2-normalized embeddings, which may limit applicability in some domains', 'May not provide large gains for problems where embeddings are already well-separated']",False,4,3,4,7,5,"['Novel approach combining kernel methods with deep learning in an end-to-end trainable way', 'Strong theoretical foundations and analysis', 'Consistent empirical improvements across multiple tasks and settings', 'Minimal computational overhead compared to standard softmax', 'Complementary to existing model compression techniques', 'Improves model efficiency, especially for smaller backbone models']","['Limited comparison to other advanced nonlinear classification methods beyond standard softmax', 'Some lack of in-depth analysis on learned kernels and efficiency tradeoffs', 'Adds some complexity to the model', 'Some empirical design choices lack thorough justification']",4,3,3.0,4,Accept
WxuE_JWxjkW,This paper investigates factors influencing the expressivity of emergent languages in multi-agent communication games. The authors propose and test a hypothesis that expressivity is a trade-off between contextual complexity and unpredictability. They introduce a novel measure of expressivity based on generalization performance across games and conduct experiments to validate their hypothesis. The paper also identifies a 'message type collapse' issue and demonstrates how contrastive loss can help address it.,"['How might the findings generalize to more complex, real-world input domains?', 'Can you provide more theoretical justification for the expressivity trade-off?', 'How do the proposed expressivity measures compare to others in the literature?', 'What are the practical implications for developing more expressive emergent communication systems?', 'How do your findings relate to work on compositionality in emergent languages?']","['Experiments limited to a simple synthetic input space and specific game setup', 'Lack of thorough theoretical analysis to complement empirical results', 'Potential limited generalizability to more complex language tasks', 'Indirect measure of expressivity may not capture all aspects']",False,3,3,3,7,4,"['Novel hypothesis about expressivity as a trade-off between complexity and unpredictability', 'Well-designed experiments to test the hypothesis', 'New measure for expressivity based on generalization performance', ""Discovery and analysis of the 'message type collapse' problem"", 'Proposed solution to message type collapse using contrastive loss']","['Limited theoretical justification for the observed trade-off', 'Experiments use a relatively simple synthetic input space', 'Limited discussion of practical implications', 'Lack of comparison to other expressivity measures', 'Some aspects of presentation could be improved']",4,3,3.0,3,Accept
MvO2t0vbs4-,"This paper presents a comprehensive empirical study of committee-based models (ensembles and cascades) for improving the efficiency of deep neural networks. The key finding is that simple ensembles and cascades of existing pre-trained models can match or exceed state-of-the-art architectures while using significantly fewer FLOPs across various tasks and model types, including image/video classification and segmentation.","['Can you provide any theoretical insights into why cascades consistently outperform single models?', 'How do the proposed cascades compare to other dynamic inference approaches like conditional computation?', 'Have you explored more sophisticated ensemble/cascade methods beyond simple averaging and early exiting?']","['The analysis is primarily empirical without strong theoretical justifications', 'Potential negative impacts of increased model complexity and deployment challenges are not thoroughly discussed', 'The paper focuses mainly on computer vision tasks - generalization to other domains is not explored', 'Scalability of the approach to very large model pools or more complex tasks is not addressed']",False,4,4,4,8,4,"['Thorough and extensive experiments across multiple tasks and architectures', 'Significant efficiency gains demonstrated over state-of-the-art models (e.g., 5.4x speedup over EfficientNet-B7)', 'Results generalize well across different settings and model architectures', 'Practical insights on building and scaling cascades', 'Strong case made for using committee-based models as baselines in efficiency research', 'Immediate practical implications for improving model efficiency in real-world applications']","['Limited technical novelty - methods are simple combinations of existing techniques', 'Lack of theoretical analysis to complement empirical results', 'Some results (e.g., ensemble efficiency) are not entirely surprising']",2,4,4.0,4,Accept
IptBMO1AR5g,"This paper introduces Stochastic Estimators of Hessian Trace (SEHT), a novel regularization method for deep neural networks. SEHT penalizes the trace of the Hessian matrix to improve generalization, motivated by theoretical bounds. The authors develop efficient stochastic estimation techniques for the Hessian trace and provide a dynamical systems perspective. Experiments on image classification and language modeling tasks demonstrate SEHT's effectiveness compared to other regularization methods.","['Can you provide a more rigorous theoretical analysis of the generalization bound?', 'How does SEHT compare to other recent Hessian-based regularization techniques?', 'What is the sensitivity of SEHT to the choice of hyperparameters?', 'How well does SEHT scale to larger models and datasets?']","['Additional computational overhead compared to simpler regularization methods', 'Introduces new hyperparameters that require tuning', 'Potential scalability issues with very large models', 'Limited theoretical guarantees on performance improvements']",False,3,3,3,7,4,"['Novel regularization approach with solid theoretical motivation', 'Efficient stochastic algorithms for practical implementation', 'Consistent empirical improvements across multiple tasks and architectures', 'Interesting connection to dynamical systems theory', 'Combines well with other regularization techniques and data augmentation']","['Theoretical analysis and dynamical systems connection could be more rigorous', 'Missing comparisons to some recent advanced regularization techniques', 'Limited ablation studies on hyperparameter sensitivity', 'Empirical gains are modest in some cases', 'Potential limitations and negative impacts not thoroughly discussed']",3,3,3.0,3,Accept
NuzF7PHTKRw,"This paper introduces EAT-C (Environment-Adversarial Sub-Task Curriculum), a novel method for improving reinforcement learning on long-horizon tasks. EAT-C combines path planning, adversarial environment generation, and curriculum learning. It uses a path planner to decompose tasks into sub-tasks, an adversarial environment generator to create challenging environments, and trains these components jointly with the RL agent. Experiments on navigation and manipulation tasks demonstrate significant improvements over baseline methods in both efficiency and generalization.","['How well would this approach scale to more complex, high-dimensional tasks?', 'Is there potential for theoretical analysis of the convergence properties?', 'How sensitive is the method to hyperparameter choices?', 'How does the computational cost compare to baselines?', 'What are potential real-world applications for this method?']","['Current experiments focus on simulated environments', 'Potential challenges in scaling to very high-dimensional problems', 'Added complexity may increase implementation difficulty', 'Limited theoretical analysis of convergence properties']",False,3,3,3,7,4,"['Novel combination of curriculum learning, adversarial training, and planning for RL', 'Strong empirical results showing clear improvements over competitive baselines', 'Thorough ablation studies validating the importance of each component', 'Addresses important challenges of sparse rewards and generalization in RL', 'Clear explanations and visualizations of the method', 'Innovative mutual learning strategy between planning and RL components']","['Lack of theoretical analysis or convergence guarantees', 'Experiments limited to relatively simple environments', 'Added complexity from training multiple interacting components', 'Potential sensitivity to hyperparameters and training dynamics']",3,4,3.0,3,Accept
zuDmDfeoB_1,This paper provides a theoretical and empirical analysis of when and why Model-Agnostic Meta-Learning (MAML) outperforms standard non-adaptive learning (NAL) in few-shot learning scenarios. The key contributions are: (1) Analytical results in a linear regression setting showing MAML achieves gains over NAL when there is a discrepancy in task hardness and hard tasks have closely packed optimal solutions far from easy tasks. (2) Extension of these insights to two-layer neural networks. (3) Experimental validation on few-shot image classification tasks.,"['How do the theoretical insights change for deeper neural network architectures?', 'Can you provide specific criteria for determining when MAML is likely to outperform NAL in practice?', 'How does the computational complexity of MAML compare to NAL in light of your findings?']","The analysis is limited to simplified settings and may not fully generalize to more complex real-world scenarios. This potentially impacts the broader applicability of the results. The experiments are primarily on image classification tasks, which may not represent all meta-learning domains. The practical implications and computational trade-offs between MAML and NAL are not thoroughly discussed.",False,4,3,4,7,4,"[""Novel theoretical analysis providing insights into MAML's performance"", 'Rigorous mathematical derivations for linear case', 'Extension of insights from linear setting to neural networks', 'Empirical validation supporting theoretical claims', 'Clear writing and presentation of ideas']","['Analysis limited to simplified settings, may not fully generalize', 'Experiments primarily on image classification, could test on more domains', 'Limited discussion of practical implications and computational trade-offs']",4,4,4.0,4,Accept
4Muj-t_4o4,"This paper introduces a novel approach to online adaptation in reinforcement learning by learning a subspace of policies. The method learns multiple 'anchor' policies that can be combined through convex combinations to create a continuous space of policies. This allows for efficient adaptation to new environments at test time without requiring multiple training environments. The approach is evaluated on several RL benchmarks and generally outperforms baseline methods, especially on more complex tasks.","['Can you provide more theoretical justification or intuition for why learning a subspace of policies leads to better adaptation?', 'How well does this method scale to more complex RL tasks with high-dimensional state/action spaces?', 'Have you explored other ways to generate the subspace beyond linear interpolation between anchor policies?', 'How sensitive is the method to the choice of hyperparameters, especially the number of anchor policies and the beta parameter?', 'What is the computational overhead of your method compared to baselines during training and adaptation?']","['Only evaluated on simulated environments, not real-world tasks', 'Limited to scenarios with a single training environment family', 'May not scale well to very high-dimensional policy spaces', 'Theoretical understanding of why the method works well could be improved']",False,3,3,3,7,4,"['Novel and well-motivated approach to online adaptation in RL', 'Simple method that is easy to implement and does not require additional components', 'Comprehensive experimental evaluation across multiple environments', 'Generally outperforms baselines, especially on more complex tasks', 'Efficient adaptation at test time with minimal computational overhead']","['Limited theoretical justification for why the approach works', 'Gains over baselines are modest in some simpler environments', 'Potential scalability issues with more complex tasks', 'Introduces additional hyperparameters to tune']",4,3,3.0,4,Accept
m5EBN92vjN,"This paper proposes AASeg, a new network for real-time semantic segmentation that combines spatial attention, channel attention, and multi-scale context modules. The method achieves competitive results on Cityscapes, CamVid, and ADE20K datasets while maintaining high inference speed. The paper provides detailed ablation studies to analyze the impact of different components.","['Can you provide more implementation details to ensure reproducibility?', 'How does the method compare to very recent work (2021-2022) in real-time segmentation?', 'What is the theoretical justification for combining these particular attention mechanisms?', 'Have you explored using this architecture for other dense prediction tasks beyond semantic segmentation?', 'What are potential applications of this method outside of autonomous driving scenarios?']","['The paper does not adequately discuss limitations of the proposed method', 'Potential negative societal impacts of real-time semantic segmentation are not addressed', 'Computational requirements for training are not reported', 'The method is only evaluated on standard benchmarks, not real-world deployments']",False,3,3,2,6,4,"['Achieves a good balance between accuracy and speed on benchmark datasets', 'Provides detailed ablation studies on various components', 'Demonstrates competitive performance compared to many state-of-the-art methods', 'Novel combination of attention mechanisms and multi-scale context for semantic segmentation', 'Fast inference speed suitable for real-time applications']","['Limited novelty - individual components like attention modules are not new', 'Marginal improvements over state-of-the-art in most cases', 'Lacks sufficient implementation details for reproducibility', 'Missing comparisons to very recent work in real-time segmentation', 'Inadequate discussion of limitations and potential negative societal impacts']",3,3,3.0,3,Reject
mqIeP6qPvta,"This paper introduces FoveaTer, a novel foveated transformer model for image classification. The model incorporates biologically-inspired foveated processing and sequential fixations, using pooled image regions as input to a transformer. It predicts fixation locations based on attention and implements a dynamic stopping criterion. Experiments on ImageNet-100 and ImageNet demonstrate that FoveaTer can match the accuracy of full-resolution models with reduced computation, while also showing improved robustness to adversarial attacks.","['How does the model perform on datasets beyond ImageNet?', 'How do the learned fixation patterns compare to human eye movements?', 'How does the approach compare to other state-of-the-art foveated or attention-based models?', 'What are the accuracy-efficiency tradeoffs as the number of fixations is varied?', 'What potential applications could this approach have beyond image classification?']","['Evaluation limited to image classification tasks', 'Computational complexity may limit real-time applications', 'May not generalize well to very different image domains or tasks requiring fine-grained details across the whole image', 'Potential negative impacts on fairness if the model learns to allocate fewer fixations to certain classes or types of images', 'Challenges in extending the approach to larger image sizes or higher resolution inputs']",False,3,3,3,7,4,"['Novel integration of foveated vision principles with transformer architecture', 'Implementation of biologically-inspired dynamic resource allocation', 'Achieves computational efficiency while maintaining accuracy', 'Demonstrates improved robustness to adversarial attacks', 'Thorough ablation studies and analysis']","['Evaluation limited primarily to ImageNet datasets', 'Limited comparison to other foveated or attention-based models', 'Could benefit from more analysis of learned fixation patterns and dynamic stopping mechanism', 'Computational savings are significant but not dramatic']",4,3,3.0,4,Accept
JXSZuWSPH85,"This paper introduces SOLO-IRL, a novel inverse reinforcement learning method that estimates rewards using only expert trajectories. It combines classification-based IRL with adversarial one-class classification, eliminating the need for baseline trajectories. The method shows promising results compared to existing approaches, especially in environments with larger state spaces.","['How does the method perform on more complex image-based control tasks?', 'Are there ways to reduce or automate the tuning of noise parameters?', 'How does SOLO-IRL compare to more recent IRL approaches like AIRL?', 'What are the key limitations or failure cases of the approach?', 'How well does the method scale to environments with very high-dimensional state spaces?', 'What are potential real-world applications of this method?']","['Performance depends on tuning of noise parameters', 'Not evaluated on more complex image-based control tasks', 'Potential scalability issues for very high-dimensional state spaces', 'Limited comparison to recent IRL/imitation learning methods', ""Lack of in-depth theoretical analysis of the method's properties""]",False,3,3,3,7,4,"['Novel combination of classification-based IRL and adversarial one-class classification', 'Eliminates need for baseline trajectories, addressing a key limitation of existing IRL methods', 'Theoretically grounded approach building on the LMDP framework', 'Demonstrates good empirical performance across multiple environments', 'Clear explanation of methodology and theoretical background']","['Limited evaluation on more complex, high-dimensional, or image-based control tasks', 'Sensitivity to noise parameters that requires careful tuning', 'Lack of comparison to some recent IRL methods (e.g., AIRL)', 'Potential scalability issues for very high-dimensional state spaces', 'Limited discussion of limitations and failure cases']",4,3,3.0,4,Accept
D78Go4hVcxO,"This paper provides a comprehensive analysis of how Vision Transformers (ViTs) work compared to CNNs, challenging conventional understanding. Key findings include: 1) MSAs improve performance by flattening loss landscapes rather than capturing long-range dependencies, 2) MSAs act as low-pass filters while CNNs are high-pass filters, and 3) Multi-stage networks behave like series of small models. Based on these insights, the authors propose AlterNet, which alternates CNN and MSA blocks and outperforms both pure CNNs and ViTs.","['Can you provide more theoretical justification for why MSAs act as low-pass filters and flatten loss landscapes?', 'How well do the insights and proposed architecture generalize to other vision tasks beyond image classification?', 'Have you explored more sophisticated variants of the AlterNet architecture?', 'How does the computational efficiency of AlterNet compare to standard ViTs and CNNs?', 'Have you considered any potential negative societal impacts of this work?']","The authors adequately discuss limitations, noting that AlterNet has potential for future improvements and that some aspects like uncertainty calibration require further investigation. The focus on image classification may limit generalization to other vision tasks. Future work could explore AlterNet as a backbone for dense prediction tasks and investigate the impact of data augmentation on uncertainty calibration.",False,4,4,4,8,4,"['Novel insights that challenge conventional understanding of ViTs', 'Extensive empirical evidence through various analysis techniques', 'Proposal of AlterNet architecture demonstrating practical utility of insights', 'Thorough analysis across multiple datasets and model variants', 'Clear explanations and visualizations of complex concepts']","['Some claims could benefit from stronger theoretical justification', 'Limited exploration of AlterNet variations and other vision tasks', 'Proposed AlterNet architecture is a relatively simple modification', 'Limited discussion of computational costs and efficiency']",4,4,4.0,4,Accept
8QE3pwEVc8P,"This paper introduces Zero-Cost-PT, a novel approach for differentiable neural architecture search that combines zero-cost proxies with perturbation-based operation scoring. The method formalizes operation scoring in differentiable NAS, analyzes existing methods, proposes new zero-cost proxies, and develops an algorithm that significantly outperforms state-of-the-art methods in both search speed and accuracy across multiple benchmarks and search spaces.","['Can you provide more theoretical insights into why Zero-Cost-PT works well?', 'How well does the method generalize to search spaces beyond DARTS-like architectures?', 'How does Zero-Cost-PT perform on tasks other than image classification?', 'How sensitive is the method to hyperparameters like N and V?']","[""The method's generalization to all types of search spaces is not fully established"", 'Performance on very large or constrained (e.g., mobile-optimized) search spaces is not extensively evaluated', 'The approach does not consider multi-objective optimization (e.g., accuracy and efficiency)', 'Potential negative impact of automating architecture design on ML job roles and interpretability']",False,4,3,4,8,4,"['Novel combination of zero-cost proxies with perturbation-based scoring for differentiable NAS', 'Significant improvements in search speed (up to 40x faster) while maintaining or improving accuracy', 'Thorough empirical evaluation on multiple search spaces and datasets', 'Formalization and insightful analysis of operation scoring in differentiable NAS', 'Proposes new zero-cost proxies that work well for large search spaces']","['Limited theoretical justification for the effectiveness of zero-cost proxies', 'Evaluation primarily focuses on DARTS-like search spaces and image classification tasks', 'Some comparisons with other NAS approaches (e.g., evolutionary) are missing', 'Hyperparameter sensitivity of the method is not fully explored']",4,4,3.0,4,Accept
reFFte7mA0F,"This paper introduces Conditional Expectation based Value Decomposition (CEVD), a novel approach for solving the ride-pool matching problem at city scale. CEVD uses conditional probabilities to model dependencies between agents without an explicit coordination graph, allowing for scalability. Evaluated on NYC taxi data, the method shows consistent improvements of 3.8-9.76% over the previous state-of-the-art method, NeurADP.","['Can you provide any theoretical analysis or guarantees for the CEVD approach?', 'How does CEVD handle potential instability in the conditional probability estimates?', 'Have you considered the impact of different clustering methods on the performance of CEVD?', 'What is the computational overhead of CEVD compared to NeurADP?', ""How does CEVD's performance change with different numbers of vehicles or request densities?""]",The approach is only evaluated on one city's dataset. The scalability to even larger problem sizes or different urban layouts is not explored. The method's sensitivity to hyperparameters like the number of clusters is not fully analyzed. Potential negative impacts on driver wages or traffic congestion are not thoroughly discussed.,False,3,3,4,7,4,"['Novel and scalable approach to consider inter-agent dependencies', 'Significant empirical improvements over state-of-the-art on real-world data', 'Clear methodology and comprehensive experiments', 'Practical applicability to city-scale ride-pooling problems', 'Addresses known issues with value overestimation in previous approaches']","['Limited theoretical analysis or guarantees', 'Comparison to only one baseline method (NeurADP)', 'Experiments conducted on only one dataset (NYC taxi data)', 'Some aspects, like choice of clustering algorithm, not fully justified']",4,3,3.0,4,Accept
IsHQmuOqRAG,"This paper introduces OPPLE, a novel framework for unsupervised object-centric representation learning based on predicting future scenes. The model learns to extract object segmentations, 3D locations, and latent codes from single images without supervision, outperforming state-of-the-art baselines on a challenging dataset with complex textures.","['How well does the method generalize to real-world images and videos with more complex object interactions?', 'Can the approach be extended to handle non-rigid objects or more complex motions?', 'What is the computational complexity and training time compared to other methods?', 'How does the performance scale with the number of objects in the scene?']","['Only evaluated on synthetic data, which may not reflect real-world complexity', 'Current restriction to rigid objects and relatively simple motions', 'Potential scalability issues for scenes with many objects or complex backgrounds', 'Not yet tested on applications like robotic manipulation or autonomous driving']",False,3,3,4,7,4,"['Novel predictive learning approach for object-centric representation', 'Ability to infer 3D object locations and segmentations without supervision', 'Strong performance on a challenging dataset with complex textures', 'Learns meaningful latent object codes capturing shape/texture similarities', 'Generalizes to single frame inputs after training on videos', 'Well-motivated approach drawing inspiration from developmental psychology']","['Evaluation limited to synthetic datasets, lacking real-world data testing', 'Assumes rigid objects and simple motions, potentially limiting applicability', 'Lack of comparison to some recent state-of-the-art object-centric models', 'Complex model architecture and training procedure', 'Computational complexity not thoroughly analyzed']",4,3,3.0,4,Accept
fCSq8yrDkc,"This paper introduces DROT, a novel algorithm for solving large-scale optimal transport (OT) problems using Douglas-Rachford splitting. Key contributions include: 1) Solving the original OT problem directly without regularization, 2) Providing O(1/ε) iteration complexity and linear convergence guarantees, 3) Developing an efficient GPU implementation with cost comparable to Sinkhorn methods, and 4) Demonstrating empirical results showing faster and more reliable convergence to high-accuracy solutions compared to existing methods, particularly Sinkhorn algorithms.","['How does DROT perform on very large-scale problems (e.g., millions of points)?', 'Could you expand comparisons to include very recent OT solvers from the past 1-2 years?', 'How sensitive is the practical performance to the choice of ρ?', 'Are there any drawbacks or limitations of the method for certain types of OT problems?']","['Method is specific to OT problems, not applicable to general LPs', 'May require tuning of ρ parameter for optimal performance', 'Reliance on GPU for efficient implementation may limit applicability in some settings', 'Sparse solutions produced may not be desirable for all applications']",False,4,3,4,8,4,"['Solves the original OT problem directly, avoiding numerical issues associated with regularized approximations', 'Strong theoretical guarantees including O(1/ε) complexity and linear convergence rate', 'Efficient GPU implementation with low per-iteration cost', 'Empirical results consistently demonstrate clear benefits over state-of-the-art baselines, especially Sinkhorn methods', 'Thorough theoretical analysis and proofs provided', 'Open-source implementation available']","['Comparisons to very recent OT solvers beyond Sinkhorn methods are limited', 'Practical performance may be sensitive to parameter choices, particularly ρ', 'Technical nature of the approach may limit accessibility to non-experts', 'Some empirical results only in single precision - more double precision results could strengthen claims']",4,4,3.0,4,Accept
vfsRB5MImo9,"This paper introduces Continual Knowledge Learning (CKL), a new task for updating the world knowledge in language models. The authors create benchmark datasets to measure retention of old knowledge, updating of outdated knowledge, and acquisition of new knowledge. They propose a novel FUAR metric to measure trade-offs between forgetting and learning, adapt several continual learning methods as baselines, and conduct extensive experiments to analyze the unique challenges of CKL.","['How would the approaches scale to much larger datasets and models closer to full pretraining scale?', 'What are the long-term impacts of repeated knowledge updates over many iterations?', 'How might the CKL framework be adapted to other types of knowledge beyond factual knowledge?', 'What are the computational costs and practical considerations for implementing these methods in real-world scenarios?', 'Are there potential strategies to further mitigate forgetting in CKL beyond the methods explored in the paper?']","['Main experiments are limited to a single model architecture and size', 'Benchmark only covers factual knowledge, not other types', 'Long-term effects of multiple CKL updates are not explored', 'Potential negative impacts of continually updating LM knowledge are not thoroughly discussed']",False,4,3,4,7,4,"['Introduces an important new task (CKL) with clear real-world relevance', 'Creates valuable datasets and metrics for measuring different aspects of knowledge updating', 'Conducts comprehensive experiments comparing different CKL approaches', 'Provides useful analysis of factors affecting CKL performance and insights into challenges', 'Explores the effects of learning rate on the trade-off between maintaining old knowledge and acquiring new knowledge']","['Main experiments are limited in scale (primarily T5-large model, though GPT-2 is also explored)', 'Benchmark focuses mainly on factual knowledge, not other types', 'Proposed methods are mostly adaptations of existing techniques', 'Limited exploration of long-term effects and real-world applicability']",3,3,3.0,4,Accept
kSwqMH0zn1F,"This paper introduces PipeGCN, a novel method for efficient distributed training of Graph Convolutional Networks (GCNs). The key innovation is pipelining communication and computation across iterations to hide communication overhead. The authors provide theoretical convergence analysis for GCN training with stale features and gradients, and propose a smoothing technique to mitigate staleness effects. Extensive experiments demonstrate significant speedups (1.7-28.5x) over existing methods while maintaining accuracy across multiple large graph datasets. The method is also shown to scale effectively across multiple GPU servers.","['How well would the pipelining approach generalize to other types of graph neural networks beyond GCNs?', 'How does PipeGCN compare to very recent related work like Dorylus?', 'How well does the method scale to very large numbers of partitions (e.g., 100+)? Are there diminishing returns?', 'How sensitive is the method to the choice of smoothing decay rate?', 'What are some potential real-world applications where PipeGCN could have the most impact?']","['The method is specific to GCNs and may not directly apply to other graph neural network architectures', 'Performance gains may be dataset/architecture dependent', 'Increased implementation complexity compared to vanilla distributed training', 'Scalability to extremely large industry-scale graphs with billions of nodes not fully explored']",False,4,3,4,8,4,"['Novel pipelining approach that significantly improves GCN training efficiency', 'Thorough theoretical convergence analysis for GCN training with stale features and gradients', 'Significant empirical speedups (1.7-28.5x) demonstrated across multiple datasets', 'Maintains accuracy of full-graph training despite introducing staleness', 'Proposes smoothing technique to mitigate impact of staleness', 'Extensive experiments and ablations to validate the method']","['Method is specific to GCNs rather than more general graph neural networks', 'Limited comparison to very recent related work (e.g., Dorylus)', 'Scalability tested only up to 32 GPUs on a single large-scale dataset', 'Potential limitations for very large numbers of partitions not fully explored']",4,4,3.0,4,Accept
gFDFKC4gHL4,"This paper introduces MASA, an adaptive sampling algorithm for efficiently estimating performance shifts in ML prediction APIs over time. The authors provide theoretical guarantees for MASA and demonstrate its effectiveness through extensive experiments on real-world commercial APIs. The study reveals significant shifts in API performance and shows that MASA outperforms uniform sampling baselines in estimating these shifts.","['How can MASA be extended to non-classification APIs or more complex tasks?', 'What are the potential negative impacts or ways this system could be misused?', 'Can you provide more details on the practical considerations for deploying MASA in real-world applications?', 'How sensitive is MASA to the choice of hyperparameters and uncertainty scores?', 'How does MASA compare to other adaptive sampling techniques used in different domains?']","['Current focus is limited to classification tasks', 'Theoretical guarantees rely on some simplifying assumptions', 'May not scale well to very high-dimensional outputs', 'Requires access to labeled test data which may not always be available', 'Potential computational cost for very large datasets or frequent API checks']",False,3,3,3,7,4,"['Addresses an important and timely problem of monitoring ML API performance shifts', 'Proposes a novel algorithm (MASA) with theoretical guarantees', 'Extensive empirical evaluation on real-world APIs demonstrating clear benefits over baselines', 'Provides evidence of significant API shifts in commercial systems', 'Releases a large dataset to enable further research on API shifts']","['Evaluation focuses primarily on classification tasks with a small number of classes', 'Limited discussion of potential negative impacts and ethical concerns', 'Some gaps in theoretical analysis and practical implementation details', 'Generalizability to more complex API types is unclear']",4,3,3.0,3,Accept
d2TT6gK9qZn,"This paper presents a novel neural operator architecture for solving initial value problems (IVPs) in partial differential equations (PDEs). The method combines Padé approximation of exponential operators, implemented via recurrent neural networks, with multiwavelet transforms for spatial discretization. The approach is evaluated on synthetic PDE datasets (Korteweg-de Vries and Kuramoto-Sivashinsky equations) and COVID-19 forecasting, demonstrating improved performance and data efficiency compared to existing neural operator methods.","['Can you provide more theoretical justification for the superiority of the Padé approximation approach in this context?', 'How does the proposed method compare to state-of-the-art classical numerical techniques for solving PDEs?', ""Can you provide a more detailed analysis of the method's data efficiency across different problem types?"", 'How does the computational complexity of your method scale with problem size compared to existing approaches?', 'Have you investigated the stability of your method for long-time predictions in various PDE settings?']","['The approach may not generalize well to other types of PDEs not tested', 'Computational complexity could be high for large-scale problems', 'Potential instability issues for long-time predictions not thoroughly addressed', 'Currently limited to first-order time evolution problems']",False,3,3,4,7,4,"['Novel integration of Padé approximation, RNNs, and multiwavelets for neural operator learning', 'Theoretical analysis of gradient boundedness', 'Strong empirical results on both synthetic and real-world datasets', 'Improved data efficiency compared to existing neural operator approaches', 'Effective performance on real-world COVID-19 forecasting task']","[""Limited theoretical justification for the method's superiority over alternatives"", 'Experiments mostly on synthetic data with only one real-world application', 'Lack of comparison to classical numerical methods for solving PDEs', 'Some claims about data efficiency not thoroughly validated', 'Computational efficiency not comprehensively analyzed']",3,3,3.0,3,Accept
_PHymLIxuI,"This paper introduces CrossFormer, a novel vision transformer architecture that enables cross-scale attention through three key components: Cross-scale Embedding Layer (CEL), Long Short Distance Attention (LSDA), and Dynamic Position Bias (DPB). Extensive experiments demonstrate state-of-the-art performance on various vision tasks including image classification, object detection, instance segmentation, and semantic segmentation.","['Can you provide a more detailed analysis of the computational complexity of CEL and LSDA compared to standard attention mechanisms?', 'How does the cross-scale attention mechanism in CrossFormer differ from multi-scale approaches in CNNs?', 'Have you considered the potential limitations of CrossFormer for very small objects or fine-grained classification tasks?', ""How does the Dynamic Position Bias affect the model's ability to generalize to different image sizes?""]","['The paper does not explore the potential limitations of cross-scale attention for very small objects or fine-grained tasks', 'The computational overhead of CEL and LSDA is not thoroughly analyzed', 'The effectiveness of CrossFormer on datasets with significant scale variations is not explored', 'The paper does not discuss the potential challenges in deploying CrossFormer in real-world applications']",False,4,4,4,8,4,"['Novel cross-scale attention mechanism that addresses limitations of existing vision transformers', 'Strong empirical results across multiple vision tasks, particularly in dense prediction tasks', 'Comprehensive ablation studies validating the effectiveness of key components', 'Clear writing and detailed method description', 'Flexible architecture that can handle variable input sizes']","['Limited analysis of computational overhead and efficiency compared to simpler transformer models', 'Lack of theoretical analysis or justification for the effectiveness of cross-scale attention', 'Some comparisons to very recent related work (e.g., Swin Transformer v2) are missing', 'Limited discussion of potential negative societal impacts']",4,4,4.0,4,Accept
Xg47v73CDaj,"This paper introduces ParNet, a novel neural network architecture that achieves competitive performance on image classification and object detection tasks using only 12 layers. The key innovation is the use of parallel subnetworks instead of deep sequential layers. The authors demonstrate strong results on ImageNet, CIFAR, and COCO detection benchmarks, challenging the conventional wisdom about network depth.","['Can you provide theoretical insights or intuition for why parallel subnetworks achieve strong performance with low depth?', ""How does ParNet's training time compare to ResNets of similar accuracy?"", 'Have you explored applying this approach to other domains like NLP or very large scale models?', 'What are the memory requirements and energy efficiency trade-offs compared to deep sequential networks?', 'How might ParNet be adapted for edge computing or mobile device applications?']","['The approach requires more parameters and FLOPs than ResNets for similar performance', 'Lack of theoretical foundation for the effectiveness of the parallel design', 'Limited exploration of tasks beyond image classification and object detection', 'Potential challenges in scaling to very large (billion+ parameter) models not explored']",False,3,3,3,8,4,"['Highly original network architecture design that questions established norms about network depth', 'Impressive empirical results on major benchmarks (ImageNet, CIFAR, COCO) with only 12 layers', 'Thorough ablation studies and analysis of the proposed approach', 'Potential for efficient parallelization and reduced inference latency', 'Opens up new directions for efficient neural network design']","['Higher parameter count and FLOPs compared to ResNets of similar performance', 'Lack of theoretical analysis or justification for the effectiveness of the parallel design', 'Limited exploration of applications beyond image classification and object detection', 'Potential scalability issues for very large models not fully addressed', 'Practical speed advantages not conclusively demonstrated']",4,4,3.0,4,Accept
oxC2IBx8OuZ,"This paper introduces Continual Federated Learning (CFL), a framework addressing time-varying heterogeneity in federated learning. Key contributions include a new FL formulation for time-varying data, a unified CFL framework with different approximation techniques, convergence analysis, and empirical validation on various datasets.","['How does CFL perform on non-image data or in other domains like natural language processing?', 'What are the main challenges in scaling CFL to a very large number of clients?', 'How might CFL be extended to handle concept drift or evolving task distributions?', 'Can you discuss potential ways to further improve the privacy guarantees of CFL?']","['Potential increase in computational and communication overhead compared to standard FL', 'Privacy concerns due to storing information about past data distributions', 'May not be suitable for scenarios with extreme data heterogeneity or rapid distribution changes', 'Effectiveness on non-image data or other domains not thoroughly evaluated']",False,4,3,4,7,4,"['Novel formulation and framework for time-varying heterogeneity in FL', 'Rigorous theoretical analysis with convergence guarantees', 'Extensive empirical evaluation showing consistent improvements over baselines', 'Addresses an important and understudied problem in federated learning', 'Contributes to both theoretical understanding and practical implementation of FL']","['Some implementation details and theoretical aspects could be explained more clearly', 'Limited discussion on scalability and potential privacy implications', 'Experiments primarily focused on image classification tasks', 'Insufficient discussion of limitations and potential negative societal impacts']",4,3,3.0,4,Accept
ULfq0qR25dY,"This paper introduces the maximum n-times coverage problem, proves it is not submodular, and develops two algorithms (NTIMES-ILP and MARGINALGREEDY) to solve it. The authors apply these algorithms to design COVID-19 peptide vaccines, demonstrating superior predicted population coverage compared to existing designs.","['Can you provide any theoretical guarantees or bounds on the performance of NTIMES-ILP or MARGINALGREEDY?', 'Have you performed or planned any experimental validation of the designed vaccines?', 'How much of the performance gain over baselines is due to the n-times coverage formulation vs improved ML models for peptide immunogenicity?', 'How does the computational complexity of your algorithms scale with very large problem instances?', 'Are there potential applications of your n-times coverage formulation beyond vaccine design?']","['The approach relies on computational design and prediction - actual efficacy would need to be validated experimentally', 'The method depends on accurate prediction of peptide-HLA binding and immunogenicity, which may have inherent uncertainties', 'The computational complexity may limit scalability to extremely large problem instances']",False,3,4,4,7,4,"['Novel formulation of the n-times coverage problem with theoretical analysis', 'Development of two algorithms (NTIMES-ILP and MARGINALGREEDY) to solve the problem', 'Compelling application to COVID-19 vaccine design with clear improvements over baselines', 'Thorough evaluation on both synthetic data and real vaccine design task', 'High potential impact given the relevance to the current pandemic']","['Limited theoretical analysis - no approximation guarantees for the algorithms', 'Evaluation relies heavily on computational predictions without experimental validation', 'Unclear how much performance gain is from problem formulation vs better ML models', 'Limited exploration of scalability to very large problem instances', 'Lack of comparison to other advanced optimization techniques beyond greedy algorithms']",4,4,4.0,4,Accept
KTPuIsx4pmo,"This paper presents a meta-imitation learning approach that allows robots to learn from human video demonstrations without requiring robot demonstrations during training. The key components are: (1) An Action-awareness CycleGAN model that translates human videos to robot domain and learns action-aware latent representations, (2) A self-adaptive meta-imitation learning method that trains on the latent states and predicted actions, with an adaptive loss. The approach is evaluated on simulated and real-world manipulation tasks, showing comparable performance to baselines that use robot demonstrations.","['How sensitive is the method to the number of paired human-robot videos used?', 'How well would the approach scale to more complex, multi-stage manipulation tasks?', 'Could the A-CycleGAN be extended to work with no paired data at all?', 'How does computational efficiency compare to baselines like DAML?', 'What strategies could be employed to improve real-world performance?']","['The method still requires some paired human-robot videos', 'Evaluation limited to relatively simple manipulation tasks', 'Real-world performance lower than simulation results', 'May not generalize well to tasks with very different visual characteristics']",False,3,3,4,8,4,"['Novel approach eliminating need for robot demonstrations in meta-training', 'Achieves comparable performance to baselines using only human videos', 'Evaluation on both simulated and real-world tasks', 'Potential to significantly reduce data collection burden for imitation learning', 'Detailed ablation studies validate importance of key components']","['Still requires some paired human-robot videos', 'Performance not significantly better than baselines, just comparable', 'Limited to relatively simple manipulation tasks', 'Real-world performance lower than simulation results', 'Lacks thorough theoretical analysis or guarantees']",4,3,3.0,4,Accept
jJWK09skiNl,"This paper proposes a modified YOLOv5 architecture for generalized zero-shot object detection (gZSD) on the YCB Video dataset, introducing a new network, dataset splitting method, and attribute labeling scheme.","['Why was only recall evaluated and not precision or mAP?', 'How does the approach compare quantitatively to other zero-shot detection methods?', 'What is the impact of the attribute labeling scheme? How would learned attributes perform?', 'Can you provide ablation studies on key components of the architecture?', 'What is the computational efficiency of your approach compared to standard YOLOv5?']","['Evaluation is limited to only recall on one dataset split', 'Attribute labeling scheme is manually defined and may not scale', 'Approach relies on having similar seen objects to detect unseen ones', 'Only tested on a small number of unseen objects']",False,2,3,2,4,4,"['Novel application of YOLOv5 to zero-shot detection', 'Interesting use of attribute prediction for zero-shot generalization', 'New dataset splitting and labeling approaches for YCB Video', 'Addresses an important problem in flexible manufacturing and robotics']","['Limited evaluation - only recall is reported, not precision or mAP', 'Inconsistent performance on unseen objects, with poor results on some classes', 'Lack of comparison to other zero-shot detection methods', 'Absence of ablation studies to validate key components', 'Writing and presentation could be improved in several areas']",3,2,3.0,3,Reject
bUAdXW8wN6,"This paper introduces Domain Invariant Adversarial Learning (DIAL), a novel adversarial training method that enforces domain-invariant feature representations between natural and adversarial examples. DIAL consistently outperforms state-of-the-art adversarial training methods on standard benchmarks, improving both robustness and accuracy across various datasets and threat models.","['What is the computational overhead of DIAL compared to standard adversarial training?', 'Is there any theoretical insight into why enforcing domain invariance improves robustness?', 'How well does DIAL scale to larger datasets and model architectures?', 'Have you considered applying DIAL to tasks beyond image classification?', 'How might DIAL be extended to leverage unlabeled data in semi-supervised settings, as mentioned in the paper?']","['Method is primarily evaluated on image classification tasks', 'Potential increased computational cost for training', 'May not be effective against all types of unforeseen attacks or corruptions', 'Scalability to larger datasets like ImageNet not thoroughly explored', ""Theoretical understanding of the method's effectiveness is limited""]",False,4,4,4,8,4,"['Innovative approach combining domain adaptation with adversarial training', 'Comprehensive empirical evaluation demonstrating consistent improvements over SOTA', 'Thorough ablation studies and insightful visualizations of learned representations', 'Effective against both seen and unforeseen corruptions and attacks', 'Improves both robustness and standard accuracy', 'Introduces a novel F1-robust score for balanced evaluation of robustness and accuracy']","['Lacks theoretical analysis or guarantees', 'Limited discussion on computational overhead compared to standard adversarial training', 'Evaluation primarily focused on image classification tasks', 'Potential scalability issues with larger datasets not thoroughly addressed']",4,3,4.0,4,Accept
j-63FSNcO5a,"This paper introduces DisCo, a novel contrastive learning framework for discovering disentangled representations from pretrained generative models. Key contributions include a Δ-Contrastor module to model image variations, an entropy-based domination loss, and a hard negatives flipping strategy. The method demonstrates state-of-the-art performance on disentanglement metrics across various datasets and generative model types (GANs, VAEs, Flows).","['Can you provide more theoretical justification for why DisCo converges to disentangled representations?', 'How does the computational cost of DisCo scale with the size of the pretrained generative model?', 'How effective is DisCo on more complex real-world datasets beyond FFHQ?', 'How does the Δ-Contrastor compare to alternative approaches for modeling image variations?']","['Limited theoretical analysis of disentanglement guarantees', 'Scalability to very high-dimensional latent spaces not demonstrated', 'Reliance on availability of pretrained generative models']",True,3,3,3,7,4,"['Novel contrastive learning approach for disentanglement', 'Ability to work with different types of pretrained generative models', 'Strong empirical results on disentanglement benchmarks', 'Introduction of useful techniques like entropy-based loss and hard negative flipping', 'Comprehensive experiments and ablation studies']","['Limited theoretical justification for why the approach leads to disentanglement', 'Computational cost and scalability to very large models not thoroughly discussed', 'Potential negative societal impacts of latent space manipulation not adequately addressed', 'Experiments primarily focused on synthetic datasets, with limited evaluation on real-world data']",4,3,3.0,3,Accept
L1L2G43k14n,"This paper investigates the relationship between flatness of the loss landscape, Bayesian priors over functions, and generalization in deep neural networks. The key findings are that flatness measures correlate inconsistently with generalization across different optimizers, while the log of the Bayesian prior P(f) correlates robustly. The authors provide extensive empirical evidence to support their claims and argue that P(f) captures a more fundamental property related to the 'volume' of functions that explains generalization better than local flatness measures.","['Can you provide more theoretical justification for why log P(f) should correlate with generalization?', 'How well do these results hold up for larger state-of-the-art models and datasets?', 'Have you explored how these results might impact practical neural network training?', 'How computationally expensive is it to calculate P(f) compared to flatness measures?', 'How might these findings be applied to improve neural network architectures or training procedures?']","['Results are mostly on smaller datasets and models', 'Theoretical foundations could be strengthened', 'Practical implications for neural network training are not deeply explored', 'Computational cost of calculating P(f) may be prohibitive for very large models', 'Need for more diverse datasets to further validate the findings']",False,3,3,4,7,4,"['Extensive empirical results across multiple architectures, datasets, and optimizers', 'Challenges common assumptions about flatness and generalization', 'Proposes a new measure (log P(f)) that correlates more robustly with generalization', 'Thorough analysis of how flatness measures are affected by things like parameter rescaling and overtraining', 'Clear writing and presentation of results', 'Builds on and connects to previous work in the field effectively']","['Limited theoretical justification for why log P(f) should correlate with generalization', 'Most experiments are on relatively small/simple datasets and models', 'Some explanations are quite dense and could be clearer', 'Computational cost of calculating P(f) may limit practical applications']",4,3,3.0,4,Accept
yV4_fWe4nM,"This paper introduces a novel approach for incorporating group-level fairness constraints into deep clustering algorithms. The key contributions are: 1) Formulating fairness as an efficiently solvable integer linear programming (ILP) problem, 2) Integrating this fairness solver into a deep clustering framework, 3) Demonstrating strong empirical performance on both clustering quality and fairness metrics. The paper also presents extensions to flexible constraints, multi-state attributes, and predictive clustering.","['How sensitive is the method to the choice of hyperparameters, especially the fairness weight β?', 'How well does the approach scale to very large datasets with millions of samples?', 'Can you provide any theoretical guarantees on the fairness properties of the learned representations?', 'Have you explored using the approach without access to protected attributes during training?', 'What are some potential real-world applications where this fair deep clustering approach could be particularly beneficial?']","['Computational overhead may limit scalability to very large datasets', 'Requires protected attribute information during training', 'Limited theoretical analysis of fairness guarantees', 'Potential for misuse if users specify unfair constraints']",False,3,3,4,7,4,"['Novel formulation of fairness for deep clustering as an efficiently solvable ILP', 'Effective integration of fairness solver with deep learning in an end-to-end framework', 'Strong empirical results demonstrating improvements over baselines', 'Extensions to flexible constraints and multi-state attributes', 'Comprehensive experiments and analysis']","['Potential sensitivity to hyperparameter choices, especially fairness weight', 'Scalability concerns for very large datasets due to ILP solver overhead', 'Limited theoretical analysis or guarantees on fairness properties', 'Reliance on protected attribute information during training']",4,3,4.0,4,Accept
9rKTy4oZAQt,"This paper introduces a novel risk-sensitive reinforcement learning algorithm called Cumulative Prospect Proximal Policy Optimization (C3PO). The method optimizes distributional objectives based on the cumulative distribution function of episode rewards using policy gradients. The authors derive a sampling-based policy gradient estimate and apply variance reduction techniques to enable practical on-policy learning. Experiments on Safety Gym environments demonstrate consistent performance improvements over both unconstrained (PPO, TRPO) and constrained (Lagrangian) baselines.","['How well does the method generalize to other types of RL problems beyond Safety Gym?', 'What is the computational overhead compared to standard policy gradient methods?', 'Are there ways to automatically adapt or tune the risk sensitivity parameter during training?', 'Can you provide any theoretical guarantees on convergence?', 'How applicable is this method to other risk-sensitive domains outside of robotics?']","['Experiments limited to Safety Gym environments', 'May require careful tuning of risk-sensitivity parameters', 'Theoretical convergence properties not fully analyzed', 'Potential increased computational cost and complexity', ""Possible challenges in explaining the method's decision-making process to end-users""]",False,4,3,4,8,4,"['Novel approach to risk-sensitive RL using CDF-based objectives and policy gradients', 'Theoretically sound derivation of policy gradient estimators', 'Effective variance reduction techniques for practical implementation', 'Strong empirical results showing consistent improvements over baselines on Safety Gym tasks', 'General framework allowing different risk-sensitivity profiles', 'Clear writing and presentation', 'Contributes to safe exploration in reinforcement learning']","['Empirical evaluation limited to Safety Gym environments', 'Potential sensitivity to hyperparameters, including risk sensitivity parameter', 'Limited theoretical analysis of convergence properties', 'Added complexity compared to standard policy gradient methods', 'Potential difficulty in interpreting results of different risk profiles']",4,3,3.0,4,Accept
lY0-7bj0Vfz,"This paper introduces Memory Concept Attention (MoCA), a novel module for improving few-shot image generation inspired by neuroscience findings on complex feature detectors in the visual cortex. MoCA incorporates a prototype memory bank to accumulate diverse visual concepts over time, which are then used to modulate feature activations during generation via an attention mechanism. Experiments demonstrate consistent improvements over strong baselines like FastGAN and StyleGAN2 on several few-shot benchmarks, along with improved robustness to noise and interpretable learned concepts.","['Can you provide more theoretical justification for why accumulating prototypes in this way improves generation?', 'How does the computational cost and training time compare to the baselines, and is the improvement worth the added complexity?', 'Have you tested MoCA on larger datasets or higher resolution images? How does it scale?', 'How sensitive is the performance to hyperparameters like the number of prototype clusters?', 'Could the prototype memory be exploited for malicious purposes, such as creating more convincing deepfakes?']","['Evaluation is limited to mostly small/few-shot datasets', 'Computational overhead of MoCA is not clearly quantified', 'Performance on more complex, large-scale datasets is not evaluated', 'Potential for misuse in deepfake generation or creating misinformation', 'Ethical implications of improved image synthesis capabilities not fully explored']",False,3,3,3,7,4,"['Novel approach inspired by neuroscience findings', 'Consistent improvements over strong baselines on multiple datasets', 'Flexible integration with different GAN architectures', 'Good analysis of learned prototypes and concepts', 'Demonstration of improved robustness to noise']","['Limited theoretical analysis or justification', 'Relatively modest improvements on some datasets', 'Increased computational complexity not thoroughly analyzed', 'Limited evaluation on larger or more complex datasets', 'Potential negative societal impacts not thoroughly addressed']",3,3,3.0,4,Accept
R8sQPpGCv0,"This paper introduces Attention with Linear Biases (ALiBi), a novel position embedding method for transformer language models that enables efficient extrapolation to longer sequences at inference time. The authors demonstrate ALiBi's effectiveness across different model sizes and datasets, showing it performs similarly or better than existing methods even when not extrapolating, while being simpler and parameter-free. Extensive experiments and analysis reveal that ALiBi's gains come mainly from mitigating the 'early token curse' rather than utilizing longer contexts.","['Can you provide more theoretical justification for the specific set of slopes chosen for ALiBi?', 'How would ALiBi perform on other transformer tasks beyond language modeling, such as machine translation or text classification?', 'Have you explored ways to actually utilize longer contexts with ALiBi rather than just mitigating the early token curse?', 'How sensitive are the results to the specific slope values chosen? Is there a way to automatically determine optimal slopes for a given dataset/model?']","['The method may not actually be utilizing longer contexts, just mitigating issues with existing methods', 'Evaluation is limited mostly to language modeling tasks', 'Exact choice of slope values is not fully theoretically justified', 'Potential negative impacts of enabling more efficient large language model training are not thoroughly discussed', 'May lead to increased computational resources required for training larger language models']",False,4,4,4,7,4,"['Novel and simple position embedding method that enables extrapolation to longer sequences', 'Extensive experiments across different datasets and model sizes', 'Thorough analysis explaining why the method works', 'Computationally efficient and easy to implement', 'Potential for significant practical impact on transformer language models', 'Generalizes well across datasets without retuning']","['Limited theoretical justification for why ALiBi works and for the specific slope values chosen', ""Gains may be primarily from mitigating the 'early token curse' rather than truly utilizing longer contexts"", 'Evaluation focused mainly on language modeling tasks, with unclear generalization to other domains', 'Some results only marginally better than baselines for larger models']",4,4,4.0,3,Accept
EHaUTlm2eHg,"This paper introduces Policy Gradients Incorporating the Future (PGIF), a novel method allowing model-free RL algorithms to leverage information from future trajectories during training. The key components are: 1) A backwards RNN/transformer encoding future trajectory information into latent variables, 2) An information bottleneck via KL regularization to prevent over-reliance on future information, and 3) Auxiliary losses to ensure useful latent representations. PGIF is applied to various RL algorithms and demonstrates improved performance across online/offline RL, sparse rewards, and partially observable environments.","['What is the computational overhead of PGIF compared to baseline methods, especially for long trajectories?', 'How sensitive is the method to hyperparameter choices, particularly the KL penalty weight and choice of auxiliary losses?', 'Can the method be extended to provide theoretical guarantees on performance improvement?', ""Are there potential ways to mitigate the method's impact on exploration, especially in scenarios with suboptimal trajectories?""]","['Increased computational complexity may limit scalability to very large-scale problems', 'Potential to amplify dataset biases in offline RL settings', 'May negatively impact exploration in some scenarios by overfitting to observed trajectories', 'Lack of theoretical guarantees on performance improvements']",False,3,3,4,7,4,"['Novel and well-motivated approach to incorporating future information in model-free RL', 'Versatile method applicable to various RL algorithms', 'Strong empirical results across diverse RL settings', 'Addresses important challenges in RL (credit assignment, sparse rewards, partial observability)', 'Well-written with clear exposition of the method', 'Interesting connections to variational inference and information bottleneck principles']","['Adds significant computational overhead compared to baselines', 'Limited theoretical analysis or guarantees', 'Ablation studies could be more comprehensive', 'Potential to exacerbate biases in datasets, especially in offline RL', 'May hinder exploration by overfitting to suboptimal trajectories', 'Lack of comparison to other methods that incorporate future information (e.g., hindsight experience replay)']",4,3,4.0,4,Accept
Fj1Tpym9KxH,"This paper presents a novel analysis of smoothness in Domain Adversarial Training (DAT) and introduces Smooth Domain Adversarial Training (SDAT). The key finding is that smoothness benefits the task loss but is detrimental to the adversarial loss. SDAT applies smoothing only to the task loss, resulting in consistent improvements over DAT baselines across multiple datasets and tasks, including both image classification and object detection.","['How sensitive is SDAT to the choice of smoothness hyperparameter ρ?', 'What is the computational overhead of SDAT compared to standard DAT?', 'Have you explored applying SDAT to non-vision domains or non-adversarial domain adaptation methods?', 'How does SDAT compare to other recent advances in domain adaptation beyond the tested baselines?', 'Are there potential applications of SDAT outside of computer vision?']","['The method introduces an additional hyperparameter ρ that requires tuning', 'Experiments are primarily focused on computer vision tasks', 'Potential negative societal impacts are not thoroughly discussed', 'Lack of experiments on very large-scale datasets']",False,3,3,3,7,4,"['Novel and insightful analysis of smoothness effects in DAT', 'Theoretically grounded approach with supporting empirical validation', 'Consistent improvements across multiple datasets and domain adaptation methods', 'Applicability demonstrated for both classification and object detection tasks', 'Thorough experiments and ablation studies']","['Improvements are relatively modest (1-2% in many cases)', 'Approach is specific to adversarial domain adaptation methods', 'Limited analysis of computational overhead and hyperparameter sensitivity', 'Theoretical analysis could potentially be more rigorous']",3,3,3.0,4,Accept
k7-s5HSSPE5,"This paper analyzes factors affecting cross-lingual transfer in multilingual language models, focusing on representation invariance and class prior shifts. The authors propose a novel method called Importance-Weighted Domain Alignment (IWDA) to improve unsupervised cross-lingual transfer by aligning features while accounting for prior shifts. Extensive experiments across multiple NLP tasks and languages demonstrate IWDA's effectiveness, especially under large prior shifts. This work contributes significantly to understanding and improving cross-lingual transfer in multilingual models.","['Can you provide more theoretical justification for the importance weighting procedure in IWDA?', 'How sensitive is IWDA to the choice of hyperparameters? Was an ablation study performed?', 'In what scenarios do you expect IWDA to outperform or underperform compared to simpler SSL methods?', 'How well does IWDA perform on very low-resource languages not seen in pre-training?', 'Are there ways to make the optimization more stable to avoid underperforming baselines in some cases?', 'How might IWDA be applied to other cross-lingual tasks beyond those studied in the paper?']","The authors acknowledge some limitations, such as noise in the optimization procedure and potential for incorrect alignment. The method sometimes underperforms simpler baselines on standard benchmarks. Performance gains are smaller on larger models. The approach requires unlabeled target language data, which may not always be available. The computational complexity of IWDA may limit its applicability in some scenarios. There is limited discussion of potential negative societal impacts.",False,3,3,3,7,4,"['Comprehensive empirical analysis providing insights into cross-lingual transfer', 'Novel IWDA method addressing both feature alignment and prior shift correction', 'Extensive experiments across multiple tasks and languages', 'Demonstrates improvements over existing baselines in many cases', 'Valuable contributions to improving unsupervised cross-lingual transfer', 'Thorough analysis of the impact of class prior shifts on transfer performance']","['IWDA method is complex with several hyperparameters', 'Limited theoretical justification for some aspects of the method, particularly the importance weighting procedure', 'Performance gains over baselines are sometimes modest, especially on larger models', 'IWDA occasionally underperforms simpler baselines on standard benchmarks', 'Analysis limited to mBERT and XLM-R models']",3,3,3.0,3,Accept
SVwbKmEg7M,"This paper introduces a novel approach to unsupervised neural machine translation using only generative language models. The method consists of three steps: few-shot amplification, distillation, and backtranslation. By leveraging GPT-3's zero-shot translation capability, the authors achieve a new state-of-the-art in unsupervised translation on the WMT14 English-French benchmark, attaining a BLEU score of 42.1. This work has the potential to significantly impact the field of NMT by simplifying the unsupervised translation pipeline.","['How well does this approach generalize to language pairs very different from English-French?', 'What are the computational requirements for implementing this method?', 'How does the performance compare when using smaller, more accessible language models?', 'Have you explored potential biases in the translations produced by this method?', 'How does this approach perform on low-resource languages?']","['May not generalize well to low-resource or distant language pairs', 'Limited accessibility due to reliance on large pre-trained models', 'Potential biases from pre-trained models and web-crawled data', 'Environmental impact of using large language models', 'Evaluation limited to one high-resource language pair']","Potential biases in translations, environmental impact of large models, and limited accessibility for researchers without access to powerful computing resources",3,4,4,7,4,"['Innovative use of large pre-trained language models for unsupervised NMT', 'State-of-the-art results on WMT14 English-French benchmark', 'Novel formulation simplifying the unsupervised NMT pipeline', 'Thorough experimental analysis and ablation studies', 'Potential for broader application beyond translation tasks']","['Heavy reliance on very large language models like GPT-3', 'Limited evaluation on only one language pair (English-French)', 'Potential limitations in generalizability to low-resource languages', 'Lack of discussion on computational costs and environmental impact', 'Limited exploration of potential biases and ethical concerns']",4,3,4.0,4,Accept
nK7eZEURiJ4,"This paper provides a theoretical analysis of distributional reinforcement learning (RL), interpreting it as entropy-regularized maximum likelihood estimation and analyzing its stability, generalization, and acceleration properties. It also proposes a novel Sinkhorn distributional RL algorithm that interpolates between Wasserstein distance and maximum mean discrepancy (MMD). Experiments on Atari games demonstrate competitive performance.","['How sensitive are the theoretical results to the simplifying assumptions made?', 'How does the Sinkhorn algorithm compare to more recent distributional RL methods not included in the evaluation?', 'Can the authors provide results on more complex environments beyond Atari games?', 'What are the potential negative societal impacts of this work?', 'What is the computational complexity of the proposed Sinkhorn algorithm compared to existing methods?']","['Analysis makes simplifying assumptions that may limit real-world applicability', 'Empirical evaluation is limited primarily to Atari benchmark', 'No explicit discussion of potential negative societal impacts', 'Potential difficulty in scaling the approach to more complex environments']",False,3,3,3,7,4,"['Rigorous theoretical analysis of distributional RL algorithms', 'Novel interpretation of distributional RL as entropy-regularized MLE', 'Analysis of stability, generalization, and acceleration properties', 'Proposal of a new Sinkhorn distributional RL algorithm', 'Competitive performance on Atari benchmark']","['Theoretical analysis relies on simplifying assumptions that may limit real-world applicability', 'Empirical evaluation is limited primarily to Atari games', 'Dense technical content may reduce accessibility for some readers', 'Limited comparison to more recent state-of-the-art RL algorithms']",4,3,3.0,3,Accept
SvFQBlffMB,"This paper introduces Pseudo-KD, a novel method for learning instance-specific label smoothing regularization that unifies label smoothing and knowledge distillation. The approach is formulated as a two-stage optimization problem with a closed-form solution for the optimal smoothing distribution. Experiments on image classification and NLP tasks demonstrate competitive performance compared to knowledge distillation while being more computationally efficient.","['How does the method compare to recent self-distillation approaches?', 'Can you provide more analysis or visualization of the learned smoothing distributions?', 'How sensitive is the method to the choice of hyperparameters alpha and beta?', 'How well does the approach generalize to other architectures and tasks beyond classification?', 'What is the computational efficiency of Pseudo-KD compared to traditional knowledge distillation?']","['The method introduces additional hyperparameters that need tuning', 'Performance gains are modest in some cases, especially on NLP tasks', 'Theoretical results rely on some simplifying assumptions', 'May not scale as well to very large models/datasets']",False,3,3,3,8,4,"['Novel formulation that unifies and generalizes label smoothing and knowledge distillation', 'Theoretical analysis with closed-form solution for optimal smoothing', 'Efficient online implementation without needing a separate teacher model', 'Competitive empirical results on both image classification and NLP tasks', 'Clear connections to prior work and self-distillation techniques']","['Limited ablation studies and analysis of the proposed method', 'Lack of comparison to some recent self-distillation techniques', 'Improvements over baselines are modest in some cases', 'Introduction of new hyperparameters that require tuning', 'Theoretical analysis could benefit from more intuitive explanations']",4,3,3.0,4,Accept
uVXEKeqJbNa,"This paper introduces SANN (Stiffness-Aware Neural Network), a method for learning Hamiltonian dynamical systems from data. Key innovations include: (1) a stiffness-aware index for time interval classification, (2) different integration schemes for stiff vs non-stiff portions, and (3) stiff interval resampling. SANN shows significant improvements over state-of-the-art approaches on complex systems like the three-body problem and billiard model.","['How sensitive is the method to the choice of stiffness ratio?', 'Could the approach be extended to non-Hamiltonian systems?', 'How does the computational cost compare to existing methods?', 'Have you evaluated SANN on any real physical systems or experimental data?', 'Are there ways to improve robustness to noisy data?']","['Currently limited to Hamiltonian systems', 'Some sensitivity to hyperparameter choices', 'Not evaluated on real-world noisy data', 'Potential computational overhead from resampling and multi-scheme integration', 'May not generalize well to all types of Hamiltonian systems']",False,4,3,4,7,4,"['Novel approach to handling stiffness in learning Hamiltonian systems', 'Theoretically justified stiffness-aware index', 'Significant empirical improvements over strong baselines', 'Addresses an important challenge in learning complex dynamical systems', 'Thorough ablation studies and analysis']","['Limited to Hamiltonian systems, though potential for extension is noted', 'Some sensitivity to hyperparameters, particularly the stiffness ratio', 'Evaluation limited to simulated data and a few specific problems', 'Performance degrades on noisy data', 'Computational overhead not thoroughly analyzed']",4,3,4.0,4,Accept
sTNHCrIKDQc,"This paper introduces novel methods for clustering and two-sample testing of network-valued data based on graphon theory. The main contributions include: (1) A new graph distance measure derived from graphon estimation techniques, (2) Two clustering algorithms (DSC and SSDP) with theoretical consistency guarantees, (3) A two-sample test for graphs, and (4) Extensive empirical evaluation demonstrating state-of-the-art performance on both synthetic and real network data.","['How well do the proposed methods scale to very large graphs with millions of nodes?', 'Are the Lipschitz continuity assumptions likely to hold for most real-world networks?', 'What are some potential practical applications where these methods could have significant impact (e.g., in bioinformatics, social network analysis, or neuroscience)?', 'How sensitive are the results to the choice of n0 parameter in practice?', 'How does the proposed method compare to graph neural network approaches in terms of performance and computational efficiency?']","['Assumes certain smoothness properties of the underlying graphons', 'Computational complexity may be high for very large graphs', 'Limited exploration of hyperparameter sensitivity', 'Does not address directed or weighted graphs', 'May not be applicable to very sparse graphs']",False,4,3,4,8,4,"['Novel graph distance measure with strong theoretical foundation', 'Two new clustering algorithms with proven consistency guarantees', 'Comprehensive empirical evaluation showing superior performance', 'Unified framework addressing both clustering and hypothesis testing for networks', 'Theoretically grounded in graphon theory with detailed proofs', 'Addresses an important problem in network-valued data analysis']","['Assumptions like Lipschitz continuity may be restrictive for some real-world networks', 'Scalability to very large graphs (millions of nodes) not fully addressed', 'Limited discussion of practical implications and applications', 'Computational complexity not explicitly analyzed', 'Lack of comparison to graph neural network approaches']",4,4,4.0,4,Accept
yztpblfGkZ-,"This paper introduces BankGCN, a novel graph convolutional network that employs adaptive filter banks to process graph signals. The method decomposes multi-channel signals into subspaces, applies diverse learnable filters, and uses regularization to ensure filter diversity. Extensive experiments on node and graph classification tasks demonstrate consistent improvements over state-of-the-art baseline methods.","['How sensitive is the method to the choice of hyperparameters, particularly the number of subspaces and regularization strength?', 'What is the computational complexity and runtime compared to simpler GCN architectures?', 'How does the method perform on very large graph datasets (millions of nodes)?', 'Have you explored applications beyond node and graph classification, such as link prediction or graph generation?', 'Can you provide insights on how to interpret the learned filters in terms of their spectral characteristics?']","['The increased complexity may make the method more challenging to implement and tune in practice', 'Evaluation is primarily focused on node and graph classification tasks', 'Scalability to very large graphs is not explicitly demonstrated']",False,4,4,4,8,4,"['Innovative adaptive filter bank approach addressing limitations of existing GCNs', 'Strong theoretical foundation with thorough analysis', 'Comprehensive empirical evaluation showing consistent improvements over baselines', 'Parameter efficient design through subspace decomposition and filter sharing', 'Well-motivated approach to capture diverse frequency components in graph data']","['Introduces additional complexity and hyperparameters compared to simpler GCN models', 'Limited evaluation of computational efficiency and scalability to very large graphs', 'Potential difficulty in interpreting the learned filters']",4,4,4.0,4,Accept
FqKolXKrQGA,"This paper introduces NuGgeT, a novel transformer-based model for inferring network structure from observed actions in network games, without requiring knowledge of the underlying utility function. The approach unifies three common network games and demonstrates superior performance over existing methods on both synthetic and real-world datasets.","['Can you provide any theoretical guarantees on the quality or uniqueness of the inferred networks?', 'How does the method scale to much larger networks (e.g., 1000+ nodes)?', 'Have you considered extending the approach to dynamic games or networks?', 'How sensitive is the performance to the choice of hyperparameters?', 'What potential applications do you see for this approach in domains beyond economics and social sciences?']","['Only applicable to static games and undirected graphs currently', 'Lacks theoretical guarantees on performance or convergence', 'May not scale well to very large networks (not evaluated)', 'Requires training data from games with similar utility functions']",False,4,4,4,7,4,"['Novel utility-function agnostic approach to network game structure inference', 'Unified formulation of different network games', 'Strong empirical results on synthetic and real-world data', 'Comprehensive experiments and ablation studies', 'Potential for broad impact in economics and social sciences']","['Limited theoretical analysis and guarantees', 'Scalability to larger networks not thoroughly evaluated', 'Only considers static games and undirected graphs', 'No guarantee of uniqueness for inferred networks', 'Lack of comparison to some recent deep learning approaches for graph inference']",4,3,4.0,4,Accept
34mWBCWMxh9,"This paper introduces 'spatial smoothing', a method to ensemble neighboring feature map points in convolutional neural networks (CNNs) to improve Bayesian neural networks (BNNs). The authors provide theoretical analysis and extensive experiments demonstrating that spatial smoothing improves accuracy, uncertainty estimation, and robustness across multiple architectures and datasets for image classification and semantic segmentation tasks.","['How well does spatial smoothing generalize to non-vision tasks or non-CNN architectures?', 'Could the authors provide more insight into selecting optimal blur kernels for different architectures?', 'How does spatial smoothing compare to other recent techniques for improving BNN efficiency?', 'Are there any scenarios where spatial smoothing could potentially hurt performance?', 'What is the computational overhead of implementing spatial smoothing in practice?']","['The method is primarily evaluated on computer vision tasks and CNN architectures', 'Optimal blur kernel selection requires some tuning', 'Performance gains are less significant for very deep networks', 'Potential computational overhead of spatial smoothing not thoroughly analyzed', 'May be challenging to apply to non-vision tasks or non-CNN architectures']",False,4,3,3,7,4,"['Novel and simple technique that improves accuracy, uncertainty estimation, and robustness simultaneously', 'Thorough theoretical analysis explaining the method, supported by visualizations and experiments', 'Comprehensive empirical evaluation on multiple tasks, datasets, and architectures', 'Provides new insights into existing techniques like global average pooling', 'Improves efficiency of Bayesian neural networks, enabling use of smaller ensemble sizes']","['Currently limited to vision tasks and CNN architectures', 'Optimal blur kernel is model-dependent, requiring some tuning', 'Performance gains are smaller on very deep networks and those with built-in ensembling', 'Limited discussion of potential negative impacts or failure cases']",3,4,4.0,4,Accept
ZaVVVlcdaN,"This paper introduces FedChain, a novel federated learning framework that combines local update methods (e.g., FedAvg) with global update methods (e.g., SGD) to achieve faster convergence while leveraging client similarity. The authors provide a thorough theoretical analysis showing improved convergence rates for strongly convex, general convex, and PL condition settings, along with matching lower bounds demonstrating near-optimality in some cases. Empirical results on both convex and non-convex tasks support the theoretical gains.","['How sensitive is the performance to the choice of when to switch from local to global updates?', 'How well does FedChain perform on more complex, real-world federated datasets?', 'Can the authors provide more intuition on when/why FedChain outperforms existing methods?', 'Are there ways to automatically determine good hyperparameters for FedChain in practice?', 'What strategies do the authors suggest for closing the gap between theory and practice in the general convex case?']","The empirical evaluation is somewhat limited in scope and could benefit from a wider range of tasks and datasets. The sensitivity of the method to hyperparameters, particularly the switching point between local and global updates, needs further analysis. There is also a gap between theory and practice for the general convex case that should be addressed.",False,4,3,4,7,4,"['Novel algorithmic framework combining strengths of local and global update methods', 'Rigorous theoretical analysis showing improved convergence rates and near-optimality', 'Empirical results on convex and non-convex tasks validating the approach', 'Addresses an important problem in federated learning', 'Thorough comparison to existing federated optimization methods']","['Empirical gains, while consistent, are sometimes modest', 'Limited experimental evaluation on few datasets/tasks', 'Some strong assumptions in theoretical analysis that may not always hold in practice', 'Practical implementation details and sensitivity to hyperparameters not fully explored']",4,4,4.0,4,Accept
YJ1WzgMVsMt,"This paper introduces LOGO, a novel reinforcement learning algorithm designed for sparse reward settings that leverages offline demonstration data from sub-optimal policies. LOGO employs a two-step approach combining policy improvement via TRPO and policy guidance using offline data, both constrained by trust regions. The paper provides theoretical performance guarantees and demonstrates strong empirical results on MuJoCo benchmarks and real robot tasks.","['How sensitive is the algorithm to the choice of hyperparameters, especially trust region sizes and decay schedules?', 'How does LOGO compare to recent offline RL methods like BEAR, BCQ, or AWAC?', 'How well does the approach scale to higher-dimensional problems or longer horizon tasks?', 'How much demonstration data is typically needed for good performance?', 'How robust is LOGO to noisy or suboptimal demonstration data?']","['Requires access to offline demonstration data which may not always be available', 'Performance guarantees rely on assumptions that may not always hold in practice', 'May require careful tuning to work well on new problems', 'Computational complexity may be higher than standard RL algorithms', 'Potential challenges in extending the approach to more complex, high-dimensional tasks']",False,4,3,4,8,4,"['Novel algorithmic approach combining online RL with offline guidance', 'Solid theoretical analysis with performance guarantees', 'Strong empirical results on challenging sparse reward tasks, including MuJoCo and real robot experiments', 'Ability to handle incomplete state information in demonstrations', 'Addresses an important problem in RL (learning with sparse rewards)']","['Limited comparison to some recent offline RL methods', 'Potential sensitivity to hyperparameters and quality of offline data', 'Some strong assumptions in the theoretical analysis', 'Computational complexity not thoroughly analyzed', 'May be challenging to obtain suitable offline demonstration data in some real-world scenarios']",4,4,3.0,4,Accept
kG0AtPi6JI1,This paper introduces the problem of latent domain learning - learning from multiple visual domains without access to domain labels. It proposes a novel method called Sparse Latent Adaptation (SLA) that uses sparse gating to adaptively account for latent domains. Experiments on several benchmarks show SLA outperforms existing approaches and benefits related problems like fairness and long-tailed recognition.,"['Can you provide more theoretical justification or analysis for why SLA works well?', 'How does SLA compare to domain generalization methods that aim to learn domain-agnostic features?', 'How does the computational complexity of SLA compare to baseline methods, and how well does it scale to larger numbers of latent domains?', 'Can you provide more analysis of what the learned latent domains represent semantically?', 'Are there potential applications of SLA beyond computer vision tasks?']","['The method may not work as well on domains with very large differences', 'Increased computational complexity from sparse gating', 'Potential negative impact of amplifying biases in data or creating unfair advantages for certain subgroups', 'Scalability to very large numbers of latent domains is not explored', 'The learned latent domains may not correspond to meaningful semantic concepts']",False,3,3,3,7,4,"['Introduces an important and novel problem setting (latent domain learning)', 'Proposes a new method (SLA) with a novel sparse gating mechanism that shows promising results across multiple datasets', 'Comprehensive empirical evaluation on multiple benchmarks', 'Demonstrates broader applicability to fairness and long-tailed recognition', 'Clear writing and good paper structure']","['Limited theoretical justification for why SLA works well', 'Improvements on some benchmarks are modest', 'Limited comparison to some relevant baselines (e.g., domain generalization methods)', 'Lack of analysis on scalability to very large numbers of latent domains', 'Limited discussion of limitations and potential negative impacts']",4,4,3.0,3,Accept
o9DnX55PEAo,"This paper presents a method for distilling large pretrained language models into efficient embedding-based models. Key contributions include extending CMOW/CBOW hybrid embeddings with bidirectionality and per-token representations, comparing general vs task-specific distillation, and introducing a two-sequence encoding scheme. The approach shows competitive performance to ELMo on some GLUE tasks with significantly faster inference than DistilBERT.","['How does performance vary with embedding dimensionality?', 'Why does the approach work much better on some tasks (e.g. RTE) compared to others?', 'How does the method compare to other recent efficient Transformer variants?', 'What are the potential negative societal impacts of this work, including possible bias amplification?']","The authors acknowledge limitations in performance on linguistic acceptability tasks and the remaining gap to full BERT. They could expand discussion of potential negative impacts, including possible bias amplification. The trade-offs between embedding size and downstream performance were not fully explored.",False,3,3,3,7,4,"['Novel cross-architecture distillation into embedding models', 'Thorough empirical evaluation on GLUE benchmark', 'Significant efficiency gains (3x faster inference than DistilBERT)', 'Competitive performance to ELMo on some tasks', 'Useful ablation studies isolating impact of different components']","['Performance lags behind state-of-the-art distilled models on most tasks', 'Limited analysis of task-specific performance differences', 'Challenges with linguistic acceptability tasks', 'Insufficient exploration of embedding size vs performance trade-offs', 'Limited discussion of potential negative societal impacts']",3,3,3.0,3,Accept
68n2s9ZJWF8,"This paper introduces Implicit Q-Learning (IQL), a novel offline reinforcement learning algorithm that uses expectile regression to estimate an upper expectile of the state-value function. IQL avoids querying out-of-distribution actions during training while enabling multi-step dynamic programming. The method alternates between fitting this value function and backing it up into a Q-function, with final policy extraction via advantage-weighted behavioral cloning. IQL achieves state-of-the-art results on the D4RL benchmark, particularly excelling on challenging Ant Maze tasks, while being simple and computationally efficient. The method also demonstrates strong performance in online fine-tuning scenarios.","['How sensitive is the method to the choice of tau, and are there principled ways to select it?', 'How does IQL perform on datasets with very limited coverage or highly suboptimal data?', 'How well does the approach scale to high-dimensional, sparse reward tasks or real-world robotics applications?', 'Could the expectile regression approach be combined with other offline RL techniques?']","['May not perform as well on datasets with very limited coverage or sparse rewards', 'Theoretical analysis relies on some simplifying assumptions that may not hold in practice', 'Evaluation is primarily focused on standard benchmarks rather than real-world tasks']",False,4,4,4,9,4,"['Novel approach addressing key challenges in offline RL', 'Strong empirical results on D4RL benchmarks, especially Ant Maze tasks', 'Simple implementation and computational efficiency', 'Thorough theoretical analysis and ablation studies', 'Demonstrates good performance in both offline setting and online fine-tuning']","['Some hyperparameter sensitivity, particularly for tau', 'Theoretical analysis makes some simplifying assumptions', 'Comparison to some recent offline RL methods could be more comprehensive']",4,4,4.0,4,Accept
z7DAilcTx7,"This paper introduces 'Adversarial Transport', a novel theoretical framework that reformulates adversarial training as a distributional robustness optimization (DRO) problem using optimal transport theory. The authors derive closed-form solutions for the optimal adversarial distribution and propose a practical algorithm using Langevin Monte Carlo sampling. Experiments on MNIST and CIFAR-10 demonstrate improved robustness and significantly faster training compared to standard PGD adversarial training.","['How well does the method scale to larger datasets and model architectures?', 'How does the approach compare to other recent adversarial training methods beyond standard PGD?', 'What is the sensitivity of the method to key hyperparameters, especially λ?', 'Can you provide more intuition on why the Langevin sampling approach works well in practice?', 'What are the practical implications of using the ∞-Wasserstein distance in your formulation?']","['Evaluation limited to MNIST and CIFAR-10', 'Theoretical assumptions may not fully translate to practical performance', 'Potential negative impacts of improved adversarial attacks not extensively discussed', 'Computational complexity of the method not fully analyzed']",False,3,3,3,8,4,"['Novel theoretical framework connecting adversarial training, DRO, and optimal transport', 'Rigorous mathematical analysis with closed-form solutions', 'Practical algorithm showing improved robustness over PGD baseline', 'Significant speedup in training time (200x for MNIST, 8x for CIFAR-10)', 'Clear writing and explanation of complex theoretical concepts']","['Experimental evaluation limited to MNIST and CIFAR-10', 'Lack of comparison to more recent adversarial training methods', 'Potential scaling issues for larger datasets and model architectures', 'Some hyperparameters (e.g., λ) require careful tuning', 'Theoretical assumptions may not always hold in practice']",4,3,3.0,4,Accept
EcGGFkNTxdJ,"This paper introduces a theoretical framework for trust region learning in multi-agent reinforcement learning (MARL), along with two practical algorithms: HATRPO and HAPPO. The key contributions include a multi-agent advantage decomposition lemma, a sequential policy update scheme with monotonic improvement guarantees, and strong empirical results on challenging MARL benchmarks like Multi-Agent MuJoCo and SMAC.","['How does the computational complexity scale with the number of agents?', 'Have you considered extending the approach to competitive or mixed cooperative-competitive settings?', 'How sensitive are HATRPO and HAPPO to hyperparameter choices?', 'Could the sequential update scheme be parallelized or approximated for better scaling?', 'How well do the theoretical guarantees hold in practical settings with function approximation?']","['The approach is limited to fully cooperative settings', 'Computational complexity may limit scalability to very large numbers of agents', 'Some domain-specific hyperparameter tuning may be required', 'Potential negative impacts in adversarial multi-agent scenarios not explicitly discussed']",False,4,3,4,8,4,"['Novel theoretical framework with rigorous proofs and guarantees', 'Practical algorithms (HATRPO and HAPPO) that implement the theoretical ideas', 'Strong empirical results showing state-of-the-art performance on MARL benchmarks', 'Addresses limitations of previous approaches like parameter sharing', 'Clear connection between theory and practice']","['Algorithms may be computationally expensive for large numbers of agents', 'Only fully cooperative settings are considered', 'Some hyperparameter tuning may be required for different environments']",4,4,4.0,4,Accept
3pZTPQjeQDR,"This paper investigates how the size of subword vocabularies learned by Byte-Pair Encoding (BPE) affects memorization in Transformer models. Through experiments on random label memorization, membership inference, and training data recovery, the authors demonstrate that larger BPE vocabularies consistently lead to increased memorization across different architectures and tasks. They attribute this effect primarily to the reduction in sequence length that occurs with larger vocabularies. The findings have important implications for model design and hyperparameter tuning in NLP.","['How do these findings generalize to non-English languages and multilingual settings?', 'What additional experiments could provide more direct evidence for the sequence length hypothesis?', 'How might these results inform the design of more privacy-preserving NLP models?', 'What are the potential trade-offs between memorization and generalization in practical applications?']","['Experiments are limited to English language data', 'Findings may not generalize to all model architectures or tasks', 'Increased memorization could potentially lead to privacy risks if not carefully managed', 'Focus is primarily on memorization rather than overall model performance', 'The study does not explore potential interactions between BPE vocabulary size and other hyperparameters']",False,3,3,3,7,4,"['Comprehensive and well-designed experiments across multiple tasks and architectures', 'Careful controls and ablations to isolate effects and rule out alternative explanations', 'Novel insights into factors affecting memorization in Transformer models', 'Clear practical relevance for model design and hyperparameter tuning in NLP', 'Thorough investigation of potential explanations for the observed effect']","['Limited theoretical analysis to complement empirical results', 'Experiments focused on English language data only', 'Limited discussion of broader impacts and potential negative consequences', 'Explanation for observed effects could benefit from more direct evidence']",4,3,3.0,3,Accept
AJO2mBSTOHl,"This paper introduces Tractable Approximate Gaussian Inference (TAGI) for deep reinforcement learning, specifically adapting it for Q-learning. The method provides an analytically tractable Bayesian inference approach for neural network parameters without relying on gradient-based optimization. Experiments on Atari games and other RL benchmarks demonstrate competitive performance compared to standard backpropagation-based methods while requiring fewer hyperparameters.","['How does the computational complexity of TAGI compare theoretically and practically to backpropagation as network size increases?', 'What modifications would be needed to extend the method to actor-critic approaches or continuous action spaces?', 'Are there any theoretical guarantees on the quality of the approximations used in TAGI?', ""How sensitive is the method's performance to the choice of hyperparameters, particularly the standard deviation decay?""]","['Current implementation is slower than optimized gradient-based methods', 'Only applicable to Q-learning currently, not more advanced RL algorithms', 'Performance improvements are not consistent across all environments tested', 'Theoretical foundations could be strengthened']",False,3,3,3,6,4,"['Novel application of analytically tractable Bayesian inference to deep RL', 'Demonstrates scalability to complex environments like Atari games', 'Requires fewer hyperparameters than standard methods', 'Provides uncertainty estimates for improved exploration', 'Thorough empirical evaluation on standard benchmarks', 'Potential for improved exploration strategies due to uncertainty estimates']","['Performance does not consistently outperform baselines', 'Currently limited to Q-learning, not applicable to more advanced RL algorithms', 'Lower computational efficiency than optimized gradient-based implementations', 'Limited theoretical analysis of approximations made']",4,3,4.0,4,Accept
YevsQ05DEN7,"This paper analyzes dimensional collapse in contrastive self-supervised learning, providing theoretical explanations for two mechanisms causing this phenomenon: strong augmentation and implicit regularization. The authors propose DirectCLR, a method that optimizes representations directly without a projector, outperforming SimCLR with a linear projector on ImageNet.","['How well does the theoretical analysis extend to more complex nonlinear networks?', 'How does DirectCLR perform on other datasets and downstream tasks beyond ImageNet classification?', 'How does DirectCLR compare to other recent state-of-the-art self-supervised learning methods?', 'How sensitive is DirectCLR to hyperparameter choices, particularly the choice of d0?', 'What is the computational efficiency of DirectCLR compared to SimCLR?']","['Analysis is limited mainly to linear models and may not fully explain nonlinear networks', 'Experiments are conducted only on ImageNet, limiting the generalizability of results', 'Potential negative societal impacts are not thoroughly discussed']",False,3,4,3,7,4,"['Novel and thorough theoretical analysis of dimensional collapse in contrastive learning', 'Proposal of DirectCLR, a simple yet effective method that improves over SimCLR', 'Empirical results and ablation studies support the theoretical claims', 'Clear writing and presentation of complex concepts']","['Experimental validation limited primarily to ImageNet dataset', 'Theoretical analysis focused mostly on linear models, with limited extension to nonlinear networks', 'Modest improvement of DirectCLR over SimCLR (~1.6% accuracy gain)', 'Limited comparison to other recent state-of-the-art self-supervised methods']",3,4,4.0,3,Accept
6Q52pZ-Th7N,"This paper introduces Pseudo-Labeled Auto-Curriculum Learning (PLACL), a novel method for semi-supervised keypoint localization. The key contributions are: (1) using reinforcement learning to automatically learn a curriculum for selecting pseudo-labeled samples, (2) a cross-training strategy to mitigate confirmation bias, and (3) curriculum residual learning to improve search efficiency. Extensive experiments on multiple datasets demonstrate significant improvements over state-of-the-art SSL approaches.","['Can you provide a more detailed analysis of the computational overhead compared to simpler SSL approaches?', 'Have you explored applying PLACL to other vision tasks beyond keypoint localization?', 'Are there any theoretical insights or guarantees that can be provided for the RL-based curriculum learning approach?', 'How sensitive is the method to key hyperparameters, and how were they selected?', 'Could this method be extended to other semi-supervised learning tasks outside of computer vision?']","['The approach adds complexity compared to simpler SSL methods', 'Scalability to very large datasets may be challenging without approximations', 'Currently only demonstrated on keypoint localization tasks', 'Lack of theoretical guarantees on the optimality of learned curricula']",False,4,3,4,8,4,"['Highly original approach combining RL and curriculum learning for pseudo-label selection', 'Effective cross-training strategy to address confirmation bias', 'Significant performance improvements over SOTA methods across multiple datasets', 'Comprehensive experiments and ablation studies', 'Good generalization to different keypoint localization models and domains', 'Novel use of curriculum residual learning for improved search efficiency']","['Increased computational complexity due to RL-based curriculum search', 'Limited theoretical analysis or guarantees for the proposed method', 'Potential scalability challenges for very large datasets', 'Experiments focused primarily on keypoint localization tasks']",4,4,4.0,4,Accept
XJFGyJEBLuz,"This paper introduces Born Again neural Rankers (BAR), a novel approach applying knowledge distillation techniques to improve neural ranking models. Key contributions include a listwise distillation loss, teacher score transformation, and theoretical analysis. Experiments demonstrate that BAR outperforms state-of-the-art neural rankers on standard learning-to-rank benchmarks.","['How does BAR compare to other knowledge distillation approaches for ranking?', 'What is the computational overhead of BAR compared to training a single model?', 'How sensitive is the performance to hyperparameter choices, particularly the teacher score transformation?', 'Have you evaluated BAR on any industry/production ranking datasets or very large-scale ranking problems?', 'Have you explored iterative born-again training with multiple generations of students?']","['Experiments limited to three public datasets, may not generalize to real-world ranking tasks', 'Limited analysis of computational costs and scalability', 'Potential need for hyperparameter tuning in practice', 'Lack of discussion on potential negative societal impacts of improved ranking models']",False,3,3,3,8,4,"['Novel application of born-again networks to ranking problems', 'Strong empirical results, outperforming state-of-the-art neural rankers', 'Well-designed listwise distillation loss and teacher score transformation', 'Thorough ablation studies validating key components', 'Theoretical analysis attempting to explain the effectiveness of the approach']","['Evaluation limited to only 3 datasets', 'Limited discussion of computational costs', 'Theoretical analysis not fully convincing and makes some strong assumptions', 'Limited comparison to other knowledge distillation techniques for ranking']",4,3,3.0,4,Accept
u7UxOTefG2,"This paper challenges the common assumption that Bayesian neural networks (BNNs) are inherently good at out-of-distribution (OOD) detection. Using theoretical analysis and empirical evidence from both infinite-width (Gaussian Process) and finite-width BNNs, the authors demonstrate that common architectures like ReLU networks do not necessarily lead to good OOD detection. By leveraging the neural network Gaussian process (NNGP) correspondence, the paper provides insights into exact Bayesian inference in the infinite-width limit. The authors highlight the importance of function space priors induced by network architecture and weight priors, and discuss potential trade-offs between generalization and OOD detection capabilities.","['How do the findings extend to high-dimensional real-world problems like image classification?', 'Are there ways to modify BNN architectures or priors to improve OOD detection while maintaining good generalization?', 'How do the results compare to non-Bayesian approaches for uncertainty quantification and OOD detection?', 'What are the key practical takeaways for researchers and practitioners currently using BNNs for OOD detection?', 'How might these findings influence future research directions in Bayesian deep learning?']","['Results are limited to low-dimensional toy problems', 'Lack of theoretical guarantees or results for high-dimensional settings', 'Does not provide concrete solutions to improve BNNs for OOD detection', 'The trade-off between generalization and OOD detection may limit practical applicability']",False,3,4,4,7,4,"['Provides important critical analysis of widely held assumptions in Bayesian deep learning', 'Offers compelling theoretical arguments and empirical evidence to support claims', 'Uses NNGP correspondence to study exact Bayesian inference in the infinite-width limit', 'Demonstrates consistency between infinite-width and finite-width networks in OOD behavior', 'Well-structured argumentation with insightful visualizations', 'Opens up new research directions for improving OOD capabilities of BNNs', 'Discusses the important trade-off between generalization and OOD detection']","['Experiments are limited to low-dimensional toy problems', 'Lack of analysis on high-dimensional, real-world datasets', 'Does not propose concrete solutions to improve OOD detection in BNNs', 'Some findings may not directly generalize to more complex, high-dimensional settings', 'Limited discussion of immediate practical implications for BNN design in real-world applications']",4,4,4.0,4,Accept
30nbp1eV0dJ,"This paper establishes tight lower bounds for differentially private empirical risk minimization (DP-ERM) in both constrained and unconstrained settings. The main contributions are: (1) An Ω(√(p log(1/δ))/(nε)) lower bound for approximate DP-ERM, improving previous bounds by logarithmic factors. (2) An Ω(p/(nε)) lower bound for pure DP-ERM in the unconstrained case. The authors introduce novel technical tools, including modified fingerprinting codes and the use of ℓ1 and ℓ2 loss functions, to prove these bounds.","['Could the techniques introduced here be applied to other DP problems beyond ERM?', 'Are there implications of these lower bounds for the design of practical DP-ERM algorithms?', 'How might these results impact future research directions in private optimization?']","The results are purely theoretical and may not directly translate to practical performance. The paper focuses only on lower bounds and does not discuss matching upper bounds or algorithms. The highly technical nature of the work may limit its accessibility to a broader audience. Additionally, the paper does not provide empirical validation of the tightness of these bounds in practical scenarios.",False,4,3,4,8,4,"['Proves tight lower bounds for an important problem in private machine learning', 'Introduces novel technical tools that may be useful for other private learning problems', 'Improves on previous best known lower bounds', 'Provides a unified treatment of constrained and unconstrained settings', 'Rigorous theoretical analysis and proofs']","['Very technical presentation that may limit accessibility', 'Limited discussion of practical implications or algorithmic insights', 'Lack of experimental validation or empirical evidence']",4,4,3.0,4,Accept
AJAR-JgNw__,"This paper introduces DEPTS, a deep learning framework for periodic time series forecasting. Key contributions include a decoupled formulation separating local trends and global periodicity, an expansion module for complex dependencies, and a periodicity module for diverse periodic patterns. Experiments show consistent improvements over state-of-the-art baselines.","['Can you provide more theoretical justification for the decoupled formulation and expansion approach?', 'How does the computational complexity and scalability of DEPTS compare to simpler approaches?', 'Have you considered comparing to a wider range of baseline methods, including traditional statistical approaches for periodic time series?', 'How sensitive is the method to the choice of hyperparameters, especially the number of periods J?', 'Can the interpretability aspects be expanded to provide more insights into the learned periodic patterns?', 'In which application domains or types of periodic time series do you expect DEPTS to have the most significant impact?']","['The approach may not generalize well to non-periodic or weakly periodic time series', 'Added complexity may make the model more difficult to implement and tune compared to simpler approaches', 'Scalability to very large-scale forecasting problems is not addressed', 'The paper does not discuss potential negative societal impacts']",False,3,3,3,8,4,"['Novel deep learning architecture for periodic time series forecasting', 'Decoupled formulation allowing separate modeling of local trends and global periodicity', 'Expansion module to capture complex dependencies progressively', 'Periodicity module capable of learning diverse periodic patterns', 'Consistent and significant improvements over strong baselines on multiple datasets', 'Comprehensive experiments on both synthetic and real-world data', 'Ablation studies validating key components', 'Some interpretability capabilities']","['Limited theoretical justification for the decoupled formulation and expansion approach', 'Lack of comparison to a wider range of baselines, especially traditional statistical methods', 'Incomplete analysis of computational complexity and scalability', 'Interpretability aspects could be expanded further', 'Added complexity compared to simpler approaches']",3,3,3.0,4,Accept
Uxppuphg5ZL,"This paper introduces a novel constraint-based framework for learned physical simulation using graph neural networks (GNNs). Instead of directly predicting future states, it learns a scalar constraint function implemented as a GNN and uses gradient descent to find states that satisfy these constraints. The approach is evaluated on several challenging physical domains, demonstrating competitive or better performance compared to state-of-the-art forward GNN simulators. Key advantages include the ability to generalize to more solver iterations at test time, incorporate new hand-designed constraints, and potentially scale to larger systems than those seen during training.","['What is the trade-off between accuracy and computation time compared to forward models, especially for larger systems?', 'Can you provide more theoretical insight into why the constraint-based formulation provides useful inductive biases?', 'How well does the approach scale to much larger simulations with more complex dynamics?', 'Can you provide more analysis of what the learned constraint functions are actually capturing?']","['Computational cost may limit applicability to very large-scale or real-time simulations', 'Approach may require careful tuning of loss function and solver parameters', 'May not be suitable for all types of physical systems', 'Relies on differentiable physics which may not capture all real-world phenomena', 'Designing effective constraint functions for complex systems may be challenging']",False,3,3,4,7,4,"['Novel constraint-based formulation for learned simulation', 'Competitive or better performance compared to state-of-the-art baselines', 'Unique ability to generalize to more solver iterations at test time', 'Can incorporate new hand-designed constraints at test time', 'Thorough empirical evaluation on multiple challenging physical domains', 'Potential to bridge traditional constraint-based simulation with learning approaches', 'Demonstrates ability to generalize to larger systems than seen during training']","['Limited theoretical analysis and justification', 'Comparison to a relatively narrow set of baselines', 'Scalability to very large systems not thoroughly explored', 'Potential computational cost concerns, especially for larger systems', 'Limited analysis of what the learned constraint functions are actually capturing']",4,3,3.0,4,Accept
3PN4iyXBeF,"This paper introduces AmIGO (Amortized Implicit Gradient Optimization), a novel algorithm for bilevel optimization problems. The authors present a unified theoretical framework for analyzing AmIGO in both stochastic and deterministic settings, focusing on strongly convex inner problems. The analysis demonstrates that AmIGO matches the computational complexity of unbiased oracle methods in various scenarios, outperforming many existing approaches. The paper's key innovation lies in the use of warm-starting and amortized computation for gradient estimation, leading to improved efficiency. Empirical results on synthetic and real-world tasks, including hyperparameter optimization, support the theoretical claims and demonstrate practical benefits.","[""How sensitive is AmIGO's performance to the choice of warm-start parameters and inner loop iterations T and N?"", 'Is it possible to extend the analysis to non-strongly convex inner problems? What challenges might arise?', 'Can you provide a more detailed comparison between AmIGO and recent methods like MRBO/VRBO, particularly in terms of practical performance?', 'Are there any scenarios where the warm-start approach might be disadvantageous or lead to suboptimal results?']","The analysis is restricted to strongly convex inner problems, which may not apply to all real-world scenarios. The empirical evaluation, while supportive, could be more comprehensive, especially for large-scale problems. The authors acknowledge potential overfitting in one experiment, which warrants further investigation. The theoretical results are asymptotic, and practical performance for finite iterations could be further explored. Additionally, the sensitivity of the algorithm to parameter choices (T, N, and warm-start) is not fully addressed.",False,4,3,4,8,4,"['Provides a comprehensive theoretical analysis with improved complexity bounds', 'Presents a unified framework for both stochastic and deterministic cases', 'Introduces novel warm-starting and amortization techniques for enhanced efficiency', 'Matches the complexity of unbiased oracle methods in certain scenarios', 'Empirical results validate theoretical claims on both synthetic and real-world problems']","['Empirical evaluation is somewhat limited in scope', 'Analysis is restricted to strongly convex inner problems', 'Some comparisons to recent bilevel optimization methods (e.g., VRBO) are limited', 'Theoretical results are asymptotic, with less clarity on finite iteration performance']",3,4,3.0,4,Accept
Ih7LAeOYIb0,"This paper introduces the Iterative Memory Network (IMN), a novel framework for long sequential user behavior modeling in recommender systems. IMN employs iterative memory updates to efficiently model both target-sequence and intra-sequence dependencies with O(L) complexity. Comprehensive experiments on public and industrial datasets demonstrate consistent improvements over state-of-the-art baselines, and an online A/B test shows significant gains in an industrial deployment.","['Can you provide more theoretical justification for why the iterative memory approach works well?', 'How does IMN compare to recent efficient Transformer variants like Reformer or Linformer?', 'Can you provide more analysis on the learned attention patterns across iterations?', 'How sensitive is the model performance to the number of memory update iterations and other key hyperparameters?', 'Are there potential applications of IMN beyond e-commerce recommendations?']","['The approach may not generalize well to domains with very different sequential patterns or extremely sparse user behavior data', 'Computational requirements may still be high for very large-scale industrial systems or extremely long sequences', 'Potential privacy implications of maintaining long user behavior sequences']",True,3,3,4,7,4,"['Novel architecture combining iterative memory updates with attention for efficient sequential modeling', 'Achieves O(L) time and space complexity while modeling complex dependencies', 'Comprehensive empirical evaluation on multiple datasets showing consistent improvements', 'Successful industrial deployment with significant online gains (7.29% CTR improvement)', 'Thorough ablation studies and efficiency analyses', 'Detailed computational and memory efficiency analysis compared to baselines']","['Limited theoretical analysis and justification for the iterative memory approach', 'Comparison to some recent efficient Transformer variants is missing', 'Some technical details and explanations could be clearer', 'Limited analysis of learned representations and attention patterns']",3,3,4.0,4,Accept
7Bc2U-dLJ6N,"This paper introduces SGEM (Stochastic Gradient with Energy and Momentum), a novel optimization algorithm that combines energy stability from AEGD with momentum methods. The authors provide a thorough theoretical analysis of SGEM's convergence properties in both non-convex stochastic and online convex settings, along with empirical results on deep learning benchmarks showing competitive or improved performance compared to methods like AEGD and SGDM.","['How does the unconditional energy stability property of SGEM contribute to its performance in practice?', 'Can you provide more insight into the relationship between the energy variable and the convergence rate?', 'How does the theoretical analysis of SGEM extend to non-smooth optimization problems?', ""What are the implications of the lower bound on the energy variable for the algorithm's behavior?""]","['Experiments focused mainly on image classification tasks, limiting generalizability claims', 'Potential limitations or failure cases of SGEM not thoroughly explored', 'Practical behavior of the energy term not fully characterized across different tasks and architectures', 'Assumes bounded gradients which may not always hold in practice for deep networks']",False,3,3,4,7,4,"['Novel algorithm combining energy stability and momentum in a principled way', 'Rigorous theoretical analysis with detailed proofs and convergence guarantees', 'Empirical results demonstrating improved convergence and generalization on standard benchmarks', 'Clear exposition and well-structured paper', 'Unconditional energy stability property of SGEM']","['Empirical evaluation limited mainly to image classification tasks', 'Improvement over existing methods may be seen as incremental rather than transformative', 'Limited discussion of potential limitations or failure cases', 'Practical impact and broad applicability not fully demonstrated']",3,3,3.0,3,Accept
rTAclwH46Tb,"This paper introduces 'eigencurve', a novel learning rate scheduler for SGD on quadratic objectives that utilizes information about the Hessian's eigenvalue distribution. The authors provide theoretical analysis showing minimax optimal convergence rates for skewed Hessian spectrums, improving upon existing methods like step decay. They demonstrate empirical improvements over step decay schedules on image classification tasks and offer insights into the effectiveness of cosine decay schedules. The work also inspires two simple practical learning rate schedulers.","['How well does the approach generalize beyond quadratic objectives to general deep learning loss landscapes?', 'What is the computational overhead of estimating Hessian eigenvalue distributions for large models?', 'How sensitive is the performance to errors in estimating the Hessian spectrum?', 'Can you provide more extensive empirical evaluation on larger datasets and a wider range of tasks?']","['Analysis is limited to quadratic objectives, which may not fully represent complex deep learning landscapes', 'May have high computational cost for large models, potentially limiting practical applicability', 'Limited empirical evaluation on datasets beyond CIFAR-10, leaving questions about scalability', ""While theoretically sound, it's unclear how to best apply the method to general deep learning tasks""]",False,3,3,3,7,4,"['Rigorous theoretical analysis showing minimax optimal convergence rates for quadratic objectives', 'Novel use of Hessian eigenvalue distribution information for learning rate scheduling', 'Empirical improvements demonstrated on image classification tasks', 'Insights into the effectiveness of cosine decay schedules', 'Inspired two simple practical learning rate schedulers with potential broader applicability']","['Analysis limited to quadratic objectives, unclear generalization to non-convex problems', 'Potential computational overhead of estimating Hessian eigenvalue distributions', 'Limited empirical evaluation beyond CIFAR-10', 'Modest gains over strong baselines like cosine decay in some cases']",4,3,4.0,3,Accept
ZumkmSpY9G4,"This paper introduces a novel generative framework for online class-incremental learning to address the logits bias problem in softmax classifiers. The key components are: 1) A nearest-class-mean classifier for inference, 2) Training with a pair-based metric learning loss (Multi-Similarity loss), and 3) An auxiliary discriminative loss. Extensive experiments demonstrate significant improvements over state-of-the-art methods across multiple datasets and settings.","['How sensitive is the method to the choice of hyperparameters, particularly the weight of the auxiliary loss?', 'How does the computational complexity compare to existing methods, especially for larger numbers of classes?', 'Could the approach be extended to other continual learning settings beyond class-incremental?', 'How well does the method scale to larger datasets or longer sequences of tasks?', 'How might this approach be integrated with other continual learning techniques?']","['Evaluation limited to image classification tasks', 'Requires storing exemplars in memory which may not be feasible for all applications', 'Increased computational cost and hyperparameter tuning may limit applicability in some settings', 'Theoretical analysis for task-free scenario is limited']",False,4,4,4,8,4,"['Novel generative approach to avoid logits bias in online class-incremental learning', 'Strong theoretical foundation with analysis connecting MS loss to generative modeling', 'Consistent and significant performance improvements over SOTA baselines', 'Thorough experiments including new task-free datasets for more realistic evaluation', 'Addresses an important problem in continual learning']","['Increased complexity with multiple components and additional hyperparameters', 'Limited analysis of computational costs and time/memory tradeoffs', 'Theoretical analysis relies on some simplifying assumptions', 'Gains are more modest on simpler datasets like MNIST']",4,4,4.0,4,Accept
MkTPtnjeYTV,"This paper studies the memorization capacity of ReLU neural networks, proving that N data points satisfying a mild separability condition can be memorized using O(sqrt(N)) parameters, which is optimal up to logarithmic factors. The work provides generalizations for depth-bounded networks and analyzes bit complexity tradeoffs, significantly improving upon previous bounds and matching lower bounds from VC dimension theory.","['How do the results relate to generalization performance of neural networks?', 'Are there implications for more practical network architectures?', 'Can the techniques be extended to analyze generalization rather than just memorization?', 'Have you considered any empirical validation of these theoretical results?']","The paper focuses on memorization capacity rather than generalization. The network constructions use unrealistic architectures and large bit complexity weights, which may be impractical to implement. The assumptions of constant input dimension and separability of data points may limit applicability to some real-world settings. The results apply only to data satisfying certain separability conditions.",False,4,3,4,8,4,"['Novel main result improving significantly on previous bounds for neural network memorization capacity', 'Rigorous proofs and analysis with clever techniques using bit extraction', 'Generalizations to depth-bounded networks and bit complexity analysis', 'Matches lower bounds from VC dimension theory, showing optimality up to log factors', 'Improves understanding of fundamental capacity of neural networks', 'Potential to impact theoretical understanding of neural network capabilities']","['Focus on memorization rather than generalization or learning', 'Network constructions use unrealistic architectures with large bit complexity weights', 'Limited practical implications due to assumptions like arbitrary-precision weights', 'Lack of empirical validation or experiments']",4,4,4.0,4,Accept
hOaYDFpQk3g,"This paper introduces LightWaveS, a framework for multivariate time series classification that combines ROCKET, wavelet scattering, and distributed computing. It achieves comparable accuracy to state-of-the-art methods using fewer features, enabling faster inference on edge devices, and demonstrates good scalability.","['How sensitive is the method to the choice of wavelets and base kernels?', 'Have you explored using more than two levels in the wavelet scattering transform?', 'Can you provide more insight into why the method performs poorly on some datasets compared to ROCKET?', 'How does LightWaveS perform compared to other state-of-the-art MTSC methods beyond the ROCKET family?', 'How sensitive is the method to the choice of 500 features? Would adaptive feature selection improve results?']","['The method may not be suitable for applications requiring extremely high accuracy', 'Limited to 2 levels of wavelet scattering', 'The distributed nature of the framework may limit its applicability in some deployment scenarios', 'The channel reduction aspect might remove important information in some cases', 'The evaluation is limited to classification tasks; regression or forecasting are not considered']",False,3,3,3,8,4,"[""Achieves comparable accuracy to state-of-the-art methods while using only 2.5% of ROCKET's features"", 'Provides significant inference speedups (9x to 65x compared to ROCKET), especially relevant for edge deployment', 'Scales well with increasing number of channels and compute nodes', 'Novel combination of ideas from ROCKET, wavelet theory, and distributed computing', 'Distributes computation across multiple nodes for faster training', 'Can reduce input size by filtering out uninformative channels']","['Does not consistently outperform ROCKET in terms of accuracy', 'Limited exploration of different wavelet types and scattering network architectures', 'Theoretical justification for using arbitrary kernels instead of proper wavelets could be stronger', 'Comparison mainly limited to ROCKET family, with limited comparison to other state-of-the-art approaches', 'Some datasets show notable accuracy drops compared to ROCKET/MINIROCKET']",3,3,3.0,4,Accept
7MV6uLzOChW,"This paper introduces IPA (Inference in a Pretrained Artifact), a novel method for conditional image generation that efficiently leverages pretrained unconditional VAEs. The key innovation is training a 'partial encoder' to perform amortized inference over the latent variables of the pretrained VAE, conditioned on partial observations. The authors provide theoretical justification and demonstrate strong empirical performance on image completion tasks, particularly in terms of diversity and coverage of the true posterior. An application to Bayesian experimental design for medical imaging is also presented.","['How sensitive is the method to the choice and quality of the pretrained unconditional VAE?', 'Could the approach be extended to leverage other types of pretrained generative models beyond VAEs?', 'How does the method perform on other conditional generation tasks beyond image completion?', 'What are the computational requirements for inference at test time compared to GANs?']","['Reliance on pretrained unconditional VAEs limits applicability', 'May not be suitable for all conditional generation tasks like super-resolution', 'Theoretical results rely on assumptions that may not always hold in practice', 'Some image quality issues remain, particularly with bilateral symmetry in face completions']",False,4,4,3,7,4,"['Novel approach leveraging pretrained models in a principled way', 'Strong theoretical foundations with proofs', 'Extensive empirical evaluation showing competitive performance', 'Outperforms state-of-the-art GAN approaches on diversity metrics', 'Interesting application to Bayesian experimental design for medical imaging', 'Enables faster training compared to training conditional VAEs from scratch']","['Reliance on availability of pretrained unconditional VAEs', 'Image quality may still lag behind GANs in some cases', 'Limited evaluation on tasks beyond image completion', 'Some issues with sample quality, such as lack of bilateral symmetry in face completions']",4,3,4.0,3,Accept
vcUmUvQCloe,"This paper introduces joint Shapley values as a novel measure of feature importance for sets of features in machine learning models. It extends Shapley value axioms to feature sets, proves uniqueness of the solution, and demonstrates the method on both game theory examples and ML datasets. The approach provides different insights compared to existing interaction indices, particularly in capturing the joint contribution of feature sets.","['How does the computational complexity scale for high-dimensional problems?', 'Are there approximation methods that could improve scalability for large feature sets?', 'How do joint Shapley values compare to other feature importance methods on standard ML benchmarks?']","['Computational complexity may limit applicability to very high-dimensional problems', 'Requires careful selection of feature subsets to analyze as number of subsets grows exponentially', 'Interpretation may become challenging for large numbers of feature interactions']",False,4,3,3,7,4,"['Novel extension of Shapley values to feature sets with rigorous theoretical foundation', 'Detailed proofs and mathematical derivations supporting the uniqueness of the solution', 'Clear demonstration of insights different from existing interaction indices', 'Illustrative examples on both synthetic and real datasets (Boston housing and movie reviews)']","['Computational complexity may limit scalability to high-dimensional problems', 'Limited empirical evaluation on complex, real-world ML problems beyond the provided examples', 'Minimal comparison to other feature attribution methods beyond interaction indices']",4,3,3.0,3,Accept
fHeK814NOMO,This paper introduces a novel method for automatically adjusting the learning rate during gradient descent optimization of neural networks. The key innovation is treating the learning rate as a trainable parameter and deriving efficient update rules based on first and second-order gradients. The method is extended to use per-layer learning rates. Comprehensive experiments on image classification tasks demonstrate the effectiveness and robustness of the approach compared to other adaptive optimizers.,"['Can you provide any theoretical insights on the convergence properties of your method for non-convex settings?', 'How does the method perform on tasks beyond image classification, such as natural language processing or reinforcement learning?', 'Have you explored the possibility of combining your approach with momentum-based methods?', 'How does the choice of update frequency affect the performance and stability of the algorithm?']","['Lack of theoretical convergence guarantees for non-convex problems', 'Limited evaluation on very large-scale problems or long training runs', 'Some sensitivity to the update frequency hyperparameter']",False,3,3,3,7,4,"['Novel and well-motivated approach to learning rate adaptation', 'Thorough theoretical analysis with efficient gradient formulations', 'Comprehensive empirical evaluation across multiple datasets and architectures', 'Demonstrated robustness to hyperparameters like initial learning rate and batch size', 'Extension to per-layer learning rates', 'Clear writing and explanations']","['Lack of theoretical convergence guarantees for non-convex problems', 'Potential for getting stuck in suboptimal local minima', 'Limited evaluation beyond image classification tasks', 'Some sensitivity to hyperparameters like update frequency']",4,3,4.0,3,Accept
rqolQhuq6Hs,"This paper presents a novel theoretical analysis of stochastic gradient descent (SGD) dynamics, focusing on the noise structure and its implications for escape from local minima. Key contributions include deriving an approximation for SGD noise covariance, formulating SGD dynamics on a logarithmic loss landscape, obtaining a new escape rate formula, and demonstrating SGD's implicit preference for flat minima with low effective dimension. Experimental results support the main theoretical claims.","['How well does the analysis generalize to other loss functions beyond mean squared error?', 'Can you provide more extensive experimental validation on larger scale problems/networks?', 'What are the practical implications of your results for deep learning optimization?']","['Theory may not generalize well to all loss functions or neural network architectures', 'Experimental validation could be expanded to larger-scale problems', 'Practical implications for deep learning optimization not fully developed']",False,4,3,4,7,4,"['Novel theoretical framework for analyzing SGD dynamics using logarithmic landscape', 'Rigorous derivation of SGD noise covariance structure proportional to the loss function', 'New escape rate formula with interesting implications for optimization', ""Insights into SGD's implicit biases towards flat minima and low effective dimension"", 'Experimental validation of key theoretical claims']","['Analysis focused mainly on mean squared loss with limited discussion of other loss functions', 'Relies on several approximations which may limit generality', 'Somewhat limited experimental evaluation, especially on large-scale problems', 'Practical implications for deep learning not fully explored']",4,3,4.0,4,Accept
sBT5nxwt18Q,"This paper introduces Critical Classification Regions (CCRs) to augment explanation-by-example techniques for neural networks in image classification tasks. CCRs highlight important regions in both test images and nearest neighbor training examples that were critical for classification, uniquely connecting features used in predictions to where they were learned. The authors evaluate different CCR methods computationally and conduct a user study to assess the impact on human understanding.","['How might CCRs be extended beyond image classification tasks?', 'How does the CCR approach compare to other state-of-the-art XAI techniques like LIME or SHAP?', 'What are the key ethical considerations or potential negative impacts of using CCRs?', 'Could CCRs be integrated into the model training process rather than just post-hoc explanation?']","['Currently limited to image classification tasks and CNNs', 'User study shows improvements mainly for ambiguous cases', 'Potential to mislead users in ambiguous cases', 'Computational cost may be high for generating CCRs on large datasets']",False,3,3,3,7,4,"['Novel technical contribution with CCRs to enhance example-based explanations, connecting critical features to training data', 'Thorough computational experiments comparing different CCR methods', 'Well-designed and controlled user study to evaluate human understanding', 'Clear writing and presentation', 'Practical applicability to popular CNN architectures and datasets']","['Limited to image classification tasks', 'User study results show improvements mainly for ambiguous cases, with limited impact otherwise', 'Potential concerns about CCRs misleading users in ambiguous cases', 'Limited comparison to other state-of-the-art XAI techniques', 'Limited discussion of limitations and potential negative impacts']",3,3,3.0,3,Accept
49A1Y6tRhaq,"This paper proposes a novel approach to link emergent communication (EC) with natural language (NL) through corpus transfer. The authors demonstrate that pre-training on EC corpora can benefit downstream NL tasks like language modeling and image captioning, especially in low-resource settings. They also introduce a new translation-based metric for evaluating EC quality that correlates better with downstream performance than existing metrics.","['How would the approach generalize to other NLP tasks beyond language modeling and image captioning?', 'What specific properties of the EC corpora contribute most to the improvements in downstream tasks?', 'How do the learned EC languages relate to natural languages in terms of structure and semantics?']","['Experiments are limited to a small set of tasks', 'The emergent language game is still a fairly constrained setup', 'Lack of in-depth linguistic analysis of emergent languages']",False,3,3,3,7,4,"['Novel approach to bridging EC and NL through corpus transfer', 'Consistent improvements on downstream tasks, particularly in low-resource settings', 'Introduction of a new, more effective metric for evaluating EC quality', 'Thorough experimental analysis with ablation studies and baselines', 'Potential to open up new research directions in both EC and NLP']","['Improvements from EC pre-training are relatively modest in many cases', 'Analysis is limited primarily to language modeling and image captioning', 'Lack of in-depth qualitative analysis of EC properties beneficial for NL tasks', 'Some experimental details and hyperparameters not fully described']",4,3,4.0,3,Accept
9W2KnHqm_xN,"This paper introduces a novel spatiotemporal embedding framework called STE, inspired by the entorhinal-hippocampal system in the brain. The framework is applied to POI recommendation (STEP) and uses context graphs for unsupervised learning, a grid cell-inspired spatial encoder, and joint spatiotemporal modeling. Experiments demonstrate strong performance on POI recommendation and traffic flow prediction tasks, achieving state-of-the-art results without using user preference data.","['Can you provide more theoretical justification for the entorhinal-hippocampal inspired approach?', 'How does the computational complexity scale with the number of POIs?', 'How sensitive is the performance to the choice of hyperparameters like context window sizes?', 'Can you provide comparisons to recent graph-based POI recommendation methods like STP-UDGAT and STGCN?', 'Have you considered applying the method to other spatiotemporal tasks beyond POI recommendation and traffic flow prediction?', 'What potential negative societal impacts could arise from the widespread adoption of this technology?']","['The approach may not scale well to very large numbers of POIs or dense spatiotemporal data', 'Relies on having sufficient spatiotemporal context data to build graphs', 'May not be easily interpretable due to complex multi-component model', 'The evaluation is limited to two main tasks (POI recommendation and traffic flow prediction)', 'Privacy implications of location-based recommendations are not thoroughly explored']",False,3,3,4,7,4,"['Novel brain-inspired approach to spatiotemporal representation learning', 'Effective joint modeling of spatial and temporal information', 'Strong empirical results on POI recommendation tasks', 'Demonstration of generalizability through traffic flow prediction case study', 'Privacy-preserving method not requiring user preference data', 'Unsupervised learning approach enables use of sparse data', 'Includes ablation study to validate component contributions']","['Limited theoretical analysis or justification for the approach', 'Missing comparisons to some recent strong baselines', 'Complex model with many components', 'Limited discussion of computational efficiency and scalability']",4,4,3.0,4,Accept
p98WJxUC3Ca,"This paper presents a novel active learning approach for domain adaptation based on minimizing a localized discrepancy between domains. The authors provide theoretical analysis, develop a scalable K-medoids algorithm, and demonstrate competitive performance on domain adaptation tasks.","[""How does the method's performance change with varying degrees of domain shift?"", 'Can the approach be extended to handle multi-source domain adaptation scenarios?', 'How does the computational complexity scale with very large datasets (millions of samples)?']","['Theoretical assumptions may not hold for all practical scenarios', 'Empirical evaluation is limited to a few datasets', 'Scalability to very large datasets not demonstrated']",False,3,3,3,6,4,"['Strong theoretical foundation with novel localized discrepancy analysis', 'Practical and scalable K-medoids algorithm derived from theoretical insights', 'Competitive empirical results across multiple datasets and tasks', 'Good integration of theory and practice']","['Theoretical assumptions may limit practical applicability', 'Empirical evaluation is somewhat limited in scope (only 3 datasets)', 'Missing comparison to some recent active learning methods', 'Lacks deeper analysis of when/why the method outperforms others']",3,3,3.0,3,Accept
Gx6Tvlm-hWW,"This paper introduces a novel approach to conformal prediction that focuses on controlling false positives rather than just coverage. The authors develop a theoretical framework for trading off coverage for precision, with rigorous guarantees on false positive control. They present an algorithm using a DeepSets model to score candidate sets, maximizing true positive rate while constraining false positives. Empirical results on drug discovery, object detection, and named entity recognition tasks demonstrate the effectiveness of the approach compared to baselines.","['What is the computational overhead of the set function approach compared to standard conformal prediction?', 'How sensitive is the method to the choice of set function architecture?', 'Have you tested the method on any other types of classification tasks beyond the three presented?', 'How does the approach scale with very large label spaces?', 'Are there any notable failure cases or limitations observed in practice?', 'What are the most promising directions for future work or extensions of this approach?']","['Only evaluated on three classification tasks', 'May be computationally expensive for very large label spaces', 'Requires training an additional deep learning model', 'The method adds complexity which may limit practical adoption', 'May not be suitable for all applications, particularly those where coverage guarantees are critical']",False,4,4,4,7,4,"['Novel theoretical framework for false positive control in conformal prediction', 'Rigorous guarantees on false positive rates', 'Practical algorithm using set functions (DeepSets) to implement the approach', 'Empirical results on multiple real-world tasks showing improvements over baselines', 'Well-motivated problem with potential for real-world impact', 'Clear writing and thorough experimental details']","['Added complexity compared to standard conformal prediction', 'Computational cost of set function approach not thoroughly analyzed', 'Evaluation limited to only a few tasks', 'Empirical improvements over baselines are modest in some cases', 'Limited discussion of practical considerations and failure cases', 'Limited theoretical analysis of the set function approach']",4,3,4.0,4,Accept
lu_DAxnWsh,"This paper introduces a method for training Transformers on algorithmic tasks by providing supervision on intermediate computational steps. The authors prove theoretically that a 1-layer 1-head Transformer can compute any finite function given sufficient intermediate steps. Empirically, they demonstrate that this approach enables both a small Transformer and a frozen pretrained Transformer to learn binary addition, which they fail to learn without intermediate guidance.","['How well does this approach generalize to more complex algorithmic tasks beyond binary addition?', 'How does the performance compare to other methods for training neural networks on algorithmic tasks?', 'What are the challenges in scaling this approach to larger, more practical problems?', 'How can appropriate intermediate steps be determined for tasks where they are not as obvious as in binary addition?', 'Are there potential applications of this approach to non-algorithmic tasks that require step-by-step reasoning?']","['Empirical evaluation is limited to binary addition tasks', 'Scalability to more complex algorithms is not thoroughly explored', 'Providing intermediate steps may be impractical for some real-world applications', 'Potential negative societal impacts are not thoroughly discussed, though they seem minimal for this theoretical work', 'Determining appropriate intermediate steps for complex tasks may be challenging']",False,3,4,3,7,4,"['Strong theoretical foundation with a proof on the expressiveness of 1-layer 1-head Transformers', 'Clear motivation and well-articulated approach', 'Empirical evidence supporting the effectiveness of the method on binary addition tasks', 'Demonstration of the approach working on both small Transformers and frozen pretrained models']","['Limited experimental evaluation beyond binary addition', 'Lack of comparison to other approaches for algorithmic reasoning tasks', 'Insufficient discussion on scalability to more complex algorithms', 'Limited exploration of potential limitations and failure modes']",4,3,4.0,4,Accept
HfUyCRBeQc,"This paper introduces selective ensembles as a method to address inconsistency in deep model predictions and feature attributions across minor changes in training. The approach uses hypothesis testing on ensemble predictions to abstain when a consistent outcome cannot be achieved with high confidence. Theoretical guarantees are provided to bound the disagreement rate, and empirical results on several datasets demonstrate that selective ensembles can achieve consistent predictions with low abstention rates while also improving feature attribution consistency.","['How does the computational cost of selective ensembles compare to standard models or other approaches for improving consistency?', 'Are there ways to reduce abstention rates while maintaining consistency guarantees, especially for challenging datasets?', 'How well would this approach scale to larger models and more complex datasets?', 'What are the potential negative societal impacts of this approach, and how might they be mitigated?', 'How does this method compare to other approaches for improving model consistency or robustness?']","['The approach requires training multiple models, which may be computationally expensive for large models/datasets', 'High abstention rates in some cases may limit practical applicability', 'Evaluation is limited to relatively small/simple datasets and models', 'Potential for selective prediction to amplify existing accuracy disparities between groups', 'Further investigation needed on effectiveness across a wider range of tasks and model architectures']",False,3,3,3,7,4,"['Addresses an important practical issue of model inconsistency in deep learning', 'Provides theoretical guarantees on bounding prediction inconsistency', 'Comprehensive empirical evaluation on multiple datasets', 'Improves consistency of both predictions and feature attributions', 'Novel approach combining selective prediction with ensembles']","['Abstention rates can be high in some cases, potentially limiting practical applicability', 'Increased computational cost due to training multiple models', 'Evaluation limited to relatively small/simple datasets and models', 'Limited discussion of potential negative societal impacts and ethical considerations']",4,3,3.0,4,Accept
8e2vrVvvaeQ,"This paper reveals that perturbations from advanced indiscriminate data poisoning attacks are almost linearly separable. It demonstrates that synthetic linearly separable perturbations can be as effective as complex attacks, connecting this to shortcut learning. A defense using pre-trained feature extractors is proposed.","['Do the findings about linear separability of perturbations generalize beyond image classification tasks?', 'Is there a theoretical explanation for why the perturbations are linearly separable?', 'How would the proposed defense perform against white-box attacks that are aware of the pre-trained feature extractor?', 'How sensitive are the results to the choice of hyperparameters for generating synthetic perturbations?']","['The study focuses mainly on image classification tasks', 'The proposed defense may be vulnerable to white-box attacks', 'Limited theoretical analysis to explain the observed phenomena', 'Potential computational cost of the proposed defense using pre-trained feature extractors']",False,3,3,4,7,4,"['Novel insight into linear separability of perturbations in poisoning attacks', 'Demonstration that simple synthetic perturbations can be as effective as advanced attacks', ""Connection to shortcut learning, revealing deep models' vulnerability to imperceptible shortcuts"", 'Proposal and evaluation of a defense using pre-trained feature extractors', 'Thorough empirical validation across multiple datasets and attack methods', 'Potential impact on understanding and improving machine learning security']","['Limited theoretical analysis - mostly empirical results', 'Proposed defense may be vulnerable to white-box attacks', 'Focus is mainly on image classification tasks - unclear if findings generalize to other domains', 'Some experimental details and hyperparameters not fully specified']",4,3,3.0,4,Accept
O476oWmiNNp,"This paper presents a Fourier domain analysis of Vision Transformers (ViTs), showing that self-attention acts as a low-pass filter. The authors propose AttnScale and FeatScale techniques to rescale high-frequency components, improving deep ViT performance. Experiments demonstrate consistent improvements across multiple ViT variants on image classification tasks.","['How do AttnScale and FeatScale perform on vision tasks such as object detection or segmentation?', 'Can you provide a direct comparison of AttnScale and FeatScale to other recent techniques for improving deep ViTs, such as those mentioned in the related work?', 'What are the specific limitations of the theoretical assumptions, and how might they affect real-world application of the proposed techniques?', 'How sensitive are the improvements to the initial values and learning rates of the trainable parameters in AttnScale and FeatScale?']","['Primary focus on image classification limits generalizability to other vision tasks', 'Theoretical analysis relies on simplifying assumptions that may not hold in all practical scenarios', 'Improvements, while consistent, are incremental rather than revolutionary', 'Limited exploration of potential negative impacts or failure cases in real-world applications', 'Analysis does not extensively cover very deep ViT architectures (beyond 24 layers)']",False,4,3,4,8,4,"['Novel theoretical framework analyzing ViTs in the Fourier domain', 'Proposed techniques (AttnScale and FeatScale) are simple, efficient, and effective', 'Consistent empirical improvements across multiple ViT architectures', 'Thorough experiments and visualizations validating the theory and methods', 'Minimal computational overhead for the proposed techniques', 'Potential to influence future research on ViT architectures and analysis']","['Theoretical analysis makes some simplifying assumptions that may not fully capture ViT behavior', 'Performance improvements are moderate (typically 0.5-1% gains)', 'Analysis focused primarily on image classification, with limited exploration of other vision tasks', 'Limited comparison to some recent related work on improving deep ViTs']",4,4,3.0,4,Accept
JLbXkHkLCG6,This paper introduces two novel methods for imitation learning from pixel observations without expert actions: Pixel Sinkhorn Imitation Learning (P-SIL) and Pixel Discriminator Actor Critic (P-DAC). These methods extend optimal transport and adversarial approaches to visual inputs by leveraging RL encoder representations and data augmentation. Experiments on DeepMind Control Suite tasks show that the proposed methods outperform baselines and achieve expert-level performance. Comprehensive ablation studies analyze key components of the algorithms.,"['How well would these methods generalize to real-world robotic tasks with more complex visual inputs?', 'Can you provide any theoretical insights or guarantees for the proposed methods?', 'How do the computational requirements of P-SIL and P-DAC compare to baselines?', 'How sensitive are the methods to hyperparameter choices, especially the data augmentation strategy?', 'What are potential future extensions or applications of these methods beyond the tasks explored in this paper?']","['Methods are only evaluated on a limited set of simulated robotics tasks', 'Lack of theoretical analysis limits understanding of when/why methods will work', 'Potential negative impacts of imitation learning systems are not thoroughly discussed', 'Unclear how methods would scale to more complex visual scenes or real-world robotics']",False,3,4,3,7,4,"['Novel algorithmic ideas for visual imitation learning without expert actions', 'Strong empirical results on challenging continuous control tasks', 'Outperforms relevant baselines, including methods with privileged information', 'Thorough ablation studies providing insights into key components', 'Clear presentation and well-written paper', 'Provides code for reproducibility']","['Lacks theoretical analysis or guarantees', 'Limited diversity of tasks/domains evaluated (mainly DeepMind Control Suite)', 'Unclear scalability to more complex visual scenes or real-world robotics tasks', 'Limited discussion on broader impacts and potential negative consequences']",4,3,4.0,4,Accept
UTTrevGchy,"This paper introduces InfoMax Termination Critic (IMTC), a method for learning diverse and reusable options in reinforcement learning by maximizing mutual information between options and terminating states. Experiments in continuous control domains show IMTC learns diverse options that transfer well to new tasks, especially in complex manipulation scenarios.","[""How does IMTC's performance change with different numbers of options?"", 'Can you provide a more detailed analysis of the computational complexity of IMTC compared to baselines?', 'How does the method perform in environments with sparse or delayed rewards?']","['The paper does not discuss potential negative impacts of more efficient RL algorithms', ""The method's applicability to real-world robotics tasks is not explored""]",False,3,3,3,7,3,"['Novel theoretical contribution in deriving mutual information maximization for option learning', 'Tractable approximation and implementation compatible with deep RL methods', 'Clear empirical improvements over baselines, especially in complex manipulation tasks and transfer learning', 'Comprehensive ablation studies validating design choices', 'Clear exposition and derivations']","['Limited theoretical analysis of optimality, convergence, and guarantees', 'Experiments primarily focused on continuous control/MuJoCo domains', 'Some missing comparisons to relevant baselines (e.g., hierarchical RL methods)', 'Potential scalability issues in high-dimensional state spaces', 'Sensitivity to hyperparameters not thoroughly explored']",3,3,3.0,3,Accept
CgV7NVOgDJZ,"This paper introduces Guided-TTS, a novel text-to-speech synthesis approach using an unconditional diffusion model guided by a phoneme classifier. The key innovation is the ability to train on untranscribed speech data, potentially reducing the need for paired text-speech data for target speakers. Results demonstrate comparable performance to state-of-the-art TTS models that require transcribed data.","['How well would this approach generalize to non-English languages?', 'What are the computational requirements and inference speed compared to standard TTS models?', 'How robust is the model to different speaker accents or speaking styles not seen during training?', 'Can you provide more details on the naturalness of the synthesized speech beyond MOS scores?', 'How sensitive is the performance to the choice of phoneme classifier and guidance strength?']","['Evaluation limited mainly to English and LJSpeech dataset', 'Complex model architecture may limit practical adoption', 'Potential for misuse in generating fake speech not thoroughly discussed', 'May have higher computational costs during inference', 'Relies on a high-quality phoneme classifier']",False,4,4,4,7,4,"['Highly original approach enabling TTS without transcribed data for target speakers', 'Comparable performance to existing state-of-the-art TTS models', 'Thorough experimental evaluation and ablation studies', 'Potential to reduce data requirements for TTS in new languages/speakers', 'Novel application of classifier guidance to text-to-speech synthesis']","['Complex model architecture and training process', 'Limited evaluation metrics and focus mainly on LJSpeech dataset', 'Insufficient discussion of practical tradeoffs vs traditional TTS', 'Potential computational costs and inference speed concerns', 'Limited analysis of failure cases and limitations']",4,4,4.0,4,Accept
o_HsiMPYh_x,"This paper introduces Average Thresholded Confidence (ATC), a novel method for estimating the accuracy of machine learning models on out-of-distribution data using only labeled source data and unlabeled target data. ATC learns a threshold on the model's confidence using source validation data and estimates target accuracy as the fraction of unlabeled examples exceeding this threshold. The paper provides theoretical analysis, including impossibility results and conditions under which accuracy estimation is possible, along with extensive empirical evaluation across various datasets and distribution shifts. Results show that ATC outperforms previous methods by 2-4x in accuracy estimation.","['How sensitive is ATC to the choice of threshold on the source data?', 'Are there ways to extend ATC to better handle novel subpopulation shifts?', 'How well does ATC perform on other types of models beyond neural networks?', 'Can you provide more analysis of when/why ATC fails on certain datasets or shift types?', 'What are some potential real-world applications and impact of ATC?']","['Limited performance on novel subpopulation shifts', 'Theoretical analysis mostly on simplified models', 'May not work well for all types of distribution shifts, as noted by the authors', 'Requires access to a labeled validation set from the source distribution']",False,4,4,4,8,4,"['Novel and simple method (ATC) that significantly outperforms prior work', 'Thorough theoretical analysis including impossibility results and conditions for accuracy estimation', 'Extensive empirical evaluation on a wide range of datasets, models, and distribution shifts', 'Addresses an important practical problem in ML deployment', 'Clear writing and presentation']","['Method still has limitations on certain types of shifts (e.g. novel subpopulations)', 'Theoretical analysis is somewhat limited in scope and mostly on simplified models', 'Analysis of failure cases and limitations could be expanded']",3,4,4.0,4,Accept
LhObGCkxj4,"This paper presents a novel theoretical framework for optimizing deep neural networks, introducing a new formulation of the finite-sum problem and an algorithmic framework with O(1/ε^3) convergence to a global minimum under certain assumptions. The approach offers a fresh perspective on neural network optimization.","['How do your bounded style assumptions compare to typical smoothness and Lipschitz continuity assumptions in optimization literature?', 'Can you provide a complexity analysis comparing your method to SGD for reaching an ε-global minimum?', 'How does the performance of your method change with network depth and width?', 'What modifications would be needed to apply this framework to convolutional or recurrent neural networks?']","The paper acknowledges the lack of empirical validation and practical implementation. However, it does not adequately address the potential limitations of the strong assumptions made or discuss the practical implications of the worse theoretical convergence rate. Future work should focus on relaxing these assumptions and improving the convergence rate to be competitive with existing methods.",False,3,2,2,4,4,"['Novel reformulation of the deep learning optimization problem', 'Theoretical framework with global convergence guarantees', 'Detailed mathematical analysis and proofs', 'Potential to provide new insights into deep learning optimization']","['Strong and non-standard assumptions that may not hold in practice', 'Lack of empirical validation or experiments', 'Unclear practical applicability and scalability to real-world deep learning problems', 'Theoretical convergence rate is worse than existing methods', 'Limited comparison to current state-of-the-art optimization techniques']",3,2,2.0,3,Reject
0SiVrAfIxOe,"This paper presents a reinforcement learning approach for closed-loop control of additive manufacturing. Key contributions include an efficient numerical model for material deposition simulation, a RL framework designed for sim-to-real transfer, and demonstration of learned policies outperforming baselines on simulated and physical printing tasks.","['How well do you expect the approach to extend to multi-layer 3D printing?', 'What modifications would be needed to apply this method to different types of additive manufacturing processes?', 'How sensitive is the performance to the specific imaging setup used? Could simpler sensors potentially work?']","['Only demonstrated on single-layer printing so far', 'May require retraining for significantly different materials or printers', 'Relies on specialized imaging setup that may not be feasible for all printers', 'Simulation model makes some simplifying assumptions that may limit applicability']",False,3,3,4,7,4,"['Novel application of RL to closed-loop control for additive manufacturing', 'Efficient numerical simulation model that enables RL training', 'Successful sim-to-real transfer of learned policies to physical hardware', 'Thorough evaluation including ablation studies and baseline comparisons', 'Learned policies outperform baselines on both simulated and physical tasks', 'Potential for significant impact on additive manufacturing processes']","['Currently limited to single-layer printing', 'Generalization to different materials and more complex 3D geometries not fully explored', 'Reliance on specialized imaging hardware may limit broad applicability', 'Limited comparison to more advanced closed-loop control approaches']",4,3,3.0,4,Accept
Fza94Y8VS4a,"This paper analyzes the evolution of uncertainty in game learning dynamics using differential entropy. The authors prove linear entropy growth for MWU and OMWU algorithms in various game settings, establish connections to volume expansion, and demonstrate a 'grand escape' phenomenon. The analysis is extended to handle small payoff perturbations.","['How might this analysis be extended to more complex multi-agent learning algorithms?', 'What are the implications of these results for the convergence properties of learning in games?', 'Could this framework be applied to analyze uncertainty in other areas of machine learning?', ""How might the 'grand escape' phenomenon be mitigated in practical multi-agent systems?""]","['Analysis is limited to zero-sum, coordination, and certain one-population games', 'Focuses on MWU and OMWU algorithms', 'Lack of empirical validation', 'Asymptotic analysis rather than finite-time bounds']",False,4,3,4,8,4,"['Novel framework for analyzing uncertainty evolution using differential entropy', 'Rigorous theoretical analysis with complete proofs', ""Interesting results on linear entropy growth and 'grand escape' phenomenon"", 'Extends recent work on volume analysis', 'Covers multiple important learning algorithms and game types', 'Results are robust to small perturbations in game payoffs']","['Highly theoretical with limited discussion of practical implications', 'Focus primarily on MWU and OMWU algorithms', 'Lack of experimental validation or empirical results', 'Some results require technical conditions that may limit applicability']",4,4,3.0,4,Accept
aYAA-XHKyk,"This paper introduces Regrouping CPE (ReCPE), a novel method to improve class-prior estimation for positive-unlabeled learning when the irreducibility assumption is violated. ReCPE creates an auxiliary distribution that satisfies irreducibility and can be applied to existing CPE methods. Theoretical analysis and experiments on synthetic and real-world datasets demonstrate ReCPE's effectiveness in reducing estimation bias across multiple CPE approaches.","['How sensitive is the method to the choice of hyperparameter p? Is there a way to automatically select an optimal value?', 'What is the computational overhead of applying ReCPE compared to base CPE methods?', 'Are there cases where ReCPE could potentially degrade performance of base estimators?', ""How does the approximation of Pp' affect the theoretical guarantees?"", 'In which applications or domains might ReCPE be particularly beneficial?']","['Assumes access to positive and unlabeled samples, which may not always be available', 'Theoretical guarantees rely on some approximations and assume infinite samples', 'May not provide significant gains when irreducibility assumption already holds']",False,4,3,4,7,4,"['Addresses an important limitation in existing CPE methods', 'Proposes a general approach applicable to multiple CPE estimators', 'Provides thorough theoretical analysis of estimation bias', 'Comprehensive experiments on synthetic and real datasets showing consistent improvements', 'Clear writing and motivation']","['Introduces a new hyperparameter (p) that requires tuning', 'Uses approximations in practical implementation that may deviate from theory', 'Performance gains are modest in some cases', 'Limited analysis of computational complexity', 'Lack of comparison to other potential approaches for addressing irreducibility assumption violation']",4,3,4.0,4,Accept
2sDQwC_hmnM,"This paper introduces ZeroFL, a novel framework for accelerating on-device training in federated learning by leveraging high levels of sparsity (up to 95%). The authors analyze the challenges of applying sparsity in federated settings and propose techniques to mitigate accuracy degradation. Experiments on image and audio classification tasks demonstrate improved accuracy and reduced communication costs compared to baselines.","['How does the performance of ZeroFL vary with different sparsity levels?', 'What is the computational overhead of the local sparsification methods?', 'How does the choice of mask ratio affect the trade-off between accuracy and communication efficiency?', 'Can you provide more details on the implementation of the sparse operations on edge devices?']","['Potential trade-off between sparsity and model accuracy not fully explored', 'Computational cost of local sparsification not thoroughly analyzed', 'Possible challenges in implementing sparse operations on diverse edge devices', 'Impact on model fairness or robustness in federated settings not addressed']",False,3,3,3,7,4,"['Novel application of sparse training to federated learning', 'Comprehensive analysis of sparsity effects in federated settings', 'Improved accuracy and communication efficiency over baselines', 'Thorough empirical evaluation on multiple datasets', 'Addresses an important problem for efficient FL on edge devices']","['Evaluation limited to image and audio classification tasks', 'Lack of theoretical analysis or convergence guarantees', 'Unclear generalizability to other domains or larger models', 'Potential practical deployment challenges due to added complexity']",4,3,3.0,3,Accept
mF122BuAnnW,"This paper introduces localized randomized smoothing, a novel approach for certifying the collective robustness of multi-output classifiers. The method uses different non-i.i.d. smoothing distributions for different outputs to capture the soft locality of the model, derives base certificates for continuous and discrete data, and combines these into a stronger collective certificate using linear programming. Experiments on image segmentation and node classification demonstrate significant improvements over existing methods.","['How can the method be extended to automatically determine optimal smoothing distributions?', 'What is the scalability of the approach for very high-dimensional outputs?', 'Can this method be applied to other types of multi-output models beyond image segmentation and node classification?', 'Are there ways to improve the performance for adversarial additions, particularly in node classification?']","The authors acknowledge the main limitations, including the assumption of softly local models, the need for prior knowledge to choose smoothing distributions, and potential computational overhead. These limitations are adequately addressed in the paper and do not significantly detract from the overall contribution.",False,4,3,4,8,4,"['Novel and well-motivated approach for certifying collective robustness', 'Rigorous theoretical foundations with sound proofs', 'Comprehensive experiments showing clear improvements over baselines', 'Addresses an important problem in ML robustness', 'General framework applicable to different data types and perturbation models']","['Assumes soft locality of models, which may not always hold', 'Requires some prior knowledge to choose appropriate smoothing distributions', 'Potential computational costs for very large multi-output problems', 'Slightly weaker performance for adversarial additions compared to deletions']",4,4,3.0,4,Accept
fRnRsdc_nR7,"This paper introduces N-FGSM, a novel single-step adversarial training method that employs larger noise magnitudes and removes clipping constraints. Extensive experiments demonstrate that N-FGSM matches or exceeds state-of-the-art performance while being 3x faster than methods like GradAlign. The approach provides new insights into the role of noise and clipping in adversarial training.","['Can you provide theoretical intuition or analysis for why larger noise and no clipping improves robustness?', 'Are there any potential downsides to using such large perturbations during training?', 'How well does the method scale to larger models and datasets?', 'Is there a way to automatically determine the optimal noise magnitude for different datasets/epsilon values?']","The authors acknowledge the performance gap compared to multi-step PGD training, especially for larger perturbations. They also note that some hyperparameter tuning is required for best results on different datasets. The authors briefly discuss potential negative societal impacts of adversarial training research, but this discussion could be more comprehensive.",False,3,4,3,7,4,"['Novel insights on the role of noise magnitude and clipping in single-step adversarial training', 'Comprehensive empirical evaluation across multiple datasets, architectures, and threat models', 'Simple yet effective method that matches or outperforms more complex baselines', 'Significant speed improvement (3x) over previous state-of-the-art methods', 'Thorough ablation studies and analyses to justify design choices']","['Lack of theoretical justification for why the approach works better', 'Performance gap compared to multi-step PGD training, especially for larger perturbations', 'Limited analysis of potential downsides or failure modes', 'Some hyperparameter tuning still required for best performance', 'Lack of comparison to some recent single-step adversarial training methods']",3,3,4.0,3,Accept
mZsZy481_F,"This paper introduces FROB, a novel method for few-shot out-of-distribution (OoD) detection and classification. FROB generates boundary samples of the normal class distribution, imposes low confidence on these samples, and combines this with few-shot outlier exposure. The approach aims to improve robustness and performance for OoD detection in challenging few-shot settings. Extensive empirical evaluation is performed on multiple image datasets, demonstrating improved performance compared to baselines, especially as the number of few-shot samples decreases.","['Can you provide more theoretical justification for why generating boundary samples should improve OoD detection?', ""How does FROB's performance change when applied to non-image data?"", 'What is the computational overhead of FROB compared to simpler OoD detection methods?', 'How does FROB perform in extremely low-shot scenarios (e.g., 1-5 samples)?']","[""The method is only evaluated on image classification tasks - it's unclear how well it would generalize to other domains"", 'The approach may not be suitable for deployment in safety-critical systems without further analysis of robustness guarantees', 'The paper does not discuss potential limitations of the method in handling adversarial attacks', 'The scalability of the method to very large datasets or high-dimensional data is not addressed']",False,3,2,3,5,3,"['Novel combination of techniques for few-shot OoD detection', 'Extensive empirical evaluation on multiple datasets', 'Demonstrates improved robustness as number of few-shot samples decreases', 'Addresses an important and challenging problem in machine learning', 'Outperforms several baseline methods in experiments']","['Limited theoretical analysis or justification for the approach', 'Method is complex with multiple components, making it hard to determine which aspects are most critical', 'Some results are presented without sufficient context for interpretation', 'Comparison to very recent state-of-the-art methods is missing']",3,3,3.0,3,Reject
K47zHehHcRc,"This paper introduces 'interventional consistency' for autoencoders, proposes training methods and an 'explicit causal latent block' architecture to enforce this property. Experiments on MNIST and CelebA show some improvements in representation quality.","['Can you provide a more intuitive explanation of interventional consistency and its importance?', 'How does the approach compare to other recent work on causal representation learning?', 'What are the computational costs of the proposed training methods?', 'Can you provide more extensive quantitative comparisons to other disentanglement methods?']","['Experiments are limited to relatively simple image datasets (MNIST and CelebA)', 'Computational costs of the proposed methods are not analyzed', 'Potential negative societal impacts are not discussed']",False,2,2,2,5,3,"['Novel concept of interventional consistency connecting causality and representation learning', 'Proposed concrete training methods to enforce interventional consistency', ""Introduction of 'explicit causal latent block' architecture"", 'Some promising results on improving representation quality']","['Limited experimental validation - only MNIST and CelebA datasets', 'Insufficient comparisons to state-of-the-art disentanglement methods', 'Theoretical analysis could be strengthened', 'Practical benefits beyond improved robustness to latent space interventions could be clarified']",2,2,3.0,2,Accept
F0v5uBM-q5K,"This paper introduces PANN (Power-Aware Neural Network), a novel approach for significantly reducing power consumption in deep neural networks. Key contributions include switching to unsigned arithmetic, a new weight quantization method that removes multipliers, and flexible trade-offs between activation bit width and number of additions. Extensive theoretical analysis and empirical results demonstrate substantial improvements over existing quantization methods, especially at low bit-widths.","['How challenging would it be to implement PANN in existing hardware accelerators?', 'How does the approach perform on more complex network architectures like Transformers?', 'Have you evaluated PANN on tasks beyond image classification?', 'What are the potential impacts on model robustness and fairness?', 'What energy savings could be expected in real-world deployments of PANN?']","['Primarily evaluated on image classification tasks - generalizability to other domains is unclear', 'May require hardware changes to fully leverage the benefits', 'Potential negative impacts on inference speed for latency-sensitive applications', 'Limited analysis of effects on static power consumption and very large models', 'Further investigation needed on scalability for very large models']",False,4,3,4,8,4,"['Novel and well-motivated approach to reducing power consumption in DNNs', 'Thorough theoretical analysis and empirical evaluation', 'Significant improvements over existing methods, particularly at low bit-widths', 'Flexible method for traversing power-accuracy trade-offs', 'Applicable both post-training and during training']","['Requires architectural changes which may limit practical applicability', 'Potential increases in latency and memory footprint in some cases', 'Evaluation primarily focused on image classification tasks', 'Limited discussion of practical implementation considerations and potential negative impacts']",4,4,4.0,4,Accept
ChKNCDB0oYj,"This paper introduces a 'mistake-driven' training approach for image classification that uses GAN-based data augmentation to improve performance on the worst-performing classes. The method combines Progressive SpinalNet, FastGAN for augmentation, and Sharpness-Aware Minimization for optimization. Experiments on 5 datasets demonstrate improvements over baselines and some new state-of-the-art results.","['Can you provide a detailed comparison of computational costs between your method and standard data augmentation techniques?', 'How does the performance of your method compare to using standard data augmentation methods on the worst-performing classes?', 'Have you conducted sensitivity analyses on the number of worst-performing classes to augment?', 'What modifications would be necessary to apply this approach to tasks other than image classification?']","['Method may not scale well to very large datasets due to GAN training costs', 'Effectiveness varies significantly across datasets', 'Unclear if method would generalize well to other domains beyond image classification', 'Potential to amplify biases present in original dataset']",False,3,2,3,6,4,"['Novel idea of focusing augmentation on worst-performing classes', 'Comprehensive experiments on multiple datasets', 'Achieves new state-of-the-art results on some datasets', 'Efficient use of GAN-based augmentation for selected classes', 'Detailed ablation studies to analyze impact of different components']","['Limited comparison to simpler augmentation techniques', 'Computational cost of GAN training not thoroughly analyzed', 'Some datasets like CIFAR-10 see minimal improvement', 'Writing could be improved in some sections', 'Limited analysis of potential limitations or failure cases']",3,3,3.0,3,Accept
nhN-fqxmNGx,"This paper provides a theoretical comparison of six variable selection methods for high-dimensional linear regression, deriving explicit convergence rates and phase diagrams for Hamming errors under a rare/weak signal model with block-wise diagonal design. The authors characterize phase transitions in terms of signal strength and sparsity, offering insights into the relative performance of different approaches. Theoretical results are supported by simulations and connected to random design settings.","['How sensitive are the results to departures from the block-diagonal design assumption?', 'Can the analysis be extended to more general covariance structures or generalized linear models?', 'How do the theoretical results translate to finite-sample performance and real-world datasets?', 'What are the main practical implications of these findings for applied researchers?', 'Are there plans to release code to reproduce the experiments and compute phase diagrams for different settings?']","['Results may not generalize beyond the specific model assumptions (block-diagonal design, linear regression)', 'Lack of finite-sample guarantees limits immediate practical applicability', 'Limited discussion of computational complexity and practical implementation challenges', 'Absence of real data experiments', 'Insufficient guidance on tuning parameter selection in practice']",False,4,3,3,8,4,"['Novel theoretical analysis with explicit convergence rates and phase diagrams for multiple variable selection methods', 'Comprehensive comparison of methods in a unified framework', 'Rigorous theoretical results that align well with simulations', 'Insightful discussion connecting results to previous work and random design models', 'Thorough comparison across different correlation structures', 'Potential to guide future research on variable selection methods']","['Limited scope due to focus on block-diagonal designs', 'Lack of discussion on practical implications and real-world applications', 'Some theoretical derivations are complex and may be hard to follow', 'Limited empirical validation beyond simulations', 'Insufficient discussion of computational aspects and practical implementation']",3,4,3.0,4,Accept
mRc_t2b3l1-,"This paper analyzes the limiting dynamics of neural networks trained with stochastic gradient descent (SGD). The authors derive a continuous-time model for SGD as an underdamped Langevin equation, study it analytically for linear regression, and identify qualitative predictions that they verify empirically in deep neural networks. Key findings include a modified loss driving the dynamics, phase space oscillations, and anomalous diffusion behavior. The work combines theoretical analysis with empirical validation on large-scale deep learning problems.","['How sensitive are the results to the specific assumptions made in the theoretical analysis, particularly the quadratic loss approximation?', 'What are the practical implications of these theoretical insights for improving neural network training?', 'How well do the findings generalize across different neural network architectures and tasks beyond ResNets and ImageNet?']","['Theoretical analysis relies on simplifying assumptions that may not fully hold in practice', 'Empirical validation is primarily on image classification tasks with ResNet architectures', 'Long-term practical impact of the insights is uncertain']",False,4,3,4,7,4,"['Novel theoretical analysis of SGD dynamics using techniques from statistical physics', 'Rigorous mathematical derivations providing insights into optimization hyperparameters, gradient noise, and Hessian interactions', 'Empirical validation of key claims on large-scale deep learning problems', 'Combination of strong theory and practical empirics', 'Uncovering mechanisms driving anomalous diffusion in SGD limiting dynamics']","['Assumptions made for theoretical analysis may limit applicability to real neural networks', 'Practical implications of the theoretical insights could be elaborated on further', 'Focus primarily on limiting dynamics rather than early/mid training']",4,4,3.0,4,Accept
-3Qj7Jl6UP5,"This paper applies magnitude vectors from topology to image analysis, developing theoretical foundations and an efficient algorithm for large images. It demonstrates potential applications in edge detection and adversarial robustness.","['How does the edge detection performance compare quantitatively to state-of-the-art methods beyond the Canny detector?', 'Can you provide more extensive evaluations of adversarial robustness, including comparisons to other defense methods and a wider range of attacks?', 'Is it possible to strengthen the theoretical foundations with more rigorous proofs where analogies are currently used?', 'Have you explored applications of magnitude vectors to other computer vision tasks?', 'What are the potential real-world applications of this work in image processing or computer vision?']","['Limited evaluation on large-scale or diverse image datasets', 'Lack of theoretical guarantees for adversarial robustness', 'Computational complexity may still be high for very large images or real-time applications', 'Potential limitations in effectiveness for color images not fully explored']",False,3,3,3,7,4,"['Novel application of magnitude vectors to image analysis', 'Solid theoretical analysis and algorithm development', 'Efficient computation method for large images', 'Promising initial results in edge detection and adversarial robustness', 'Well-written and clearly structured paper']","['Limited scope of experiments, particularly in adversarial robustness', 'Lack of comparison to state-of-the-art methods in edge detection', 'Some theoretical arguments rely on analogies rather than rigorous proofs', 'Preliminary nature of some results']",3,3,3.0,4,Accept
alaQzRbCY9w,"This paper introduces a new stochastic optimization algorithm called Stochastic Model Building (SMB) for machine learning problems. SMB uses a model building step to adjust both step size and direction, and can handle different parameter groups separately. The authors provide theoretical convergence analysis for a variant (SMBi) and empirical results on image classification tasks showing competitive or better performance compared to methods like SGD, Adam, and SLS.","['Can you provide convergence analysis for the main SMB algorithm rather than just SMBi?', 'How does SMB perform on a wider range of tasks and model architectures?', 'Can you provide more details and analysis on the auto-scheduling approach?', 'How does SMB compare to other recent adaptive optimization methods beyond SGD, Adam, and SLS?', 'What is the runtime overhead of the model building step in practice?', 'How does SMB perform on highly non-convex optimization problems?']","['Convergence analysis limited to modified version', 'Empirical evaluation could be more comprehensive', 'Robustness claims need further validation', 'Lack of built-in learning rate adjustment', 'Potential computational overhead of model building step, especially for large-scale problems']",False,3,3,3,7,4,"['Novel approach combining model building with stochastic optimization', 'Ability to handle different parameter groups separately, suitable for deep learning', 'Theoretical convergence analysis provided (for SMBi)', 'Competitive empirical performance on some image classification tasks', 'Demonstrates robustness to learning rate choice, which is a significant practical advantage']","['Convergence analysis is for a modified version (SMBi) rather than the main proposed algorithm', 'Limited range of experiments - more tasks/models would strengthen empirical claims', 'Robustness claims need more thorough investigation', 'Lacks comparison to some recent adaptive optimization methods and stochastic quasi-Newton methods', 'Runtime overhead of model building step not thoroughly analyzed']",3,3,3.0,3,Accept
TKrlyiqKWB,"This paper introduces Concept Subspace Networks (CSNs), a novel neural network architecture that unifies fair and hierarchical classification. CSNs use sets of prototypes to define concept subspaces in the latent space and control relationships between these subspaces during training. The approach demonstrates competitive or superior performance on fair classification, hierarchical classification, and combined fair+hierarchical tasks across multiple datasets.","['How does the computational complexity and training time of CSNs compare to specialized approaches?', 'Can you provide more theoretical intuition or analysis on why controlling subspace alignment leads to the desired concept relationships?', 'How well does the approach scale to larger, more complex concept hierarchies or very high-dimensional spaces?', 'How sensitive are the results to hyperparameter choices, particularly the number of prototypes and alignment loss weights?', 'What other potential applications or domains could benefit from the CSN approach beyond the demonstrated tasks?']","['The paper lacks strong theoretical guarantees for the properties of the learned representations', 'Scalability to very high-dimensional problems or large numbers of concepts is not thoroughly explored', 'Potential negative societal impacts and ethical implications of the approach are not deeply analyzed', 'The approach assumes concepts can be represented in Euclidean space, which may not hold for all domains', 'Experiments are limited to specific datasets, and generalization to other domains may present challenges']",True,3,3,4,7,4,"['Novel and flexible architecture that unifies previously separate classification paradigms', 'Strong empirical results matching or exceeding state-of-the-art on fair and hierarchical tasks', 'Ability to combine fair and hierarchical classification in a single model', 'Potential for interpretability through prototype-based representations and visualizable concept subspaces', 'Comprehensive experiments across multiple datasets and tasks']","['Limited theoretical analysis and guarantees for the approach', 'Potential scalability issues for high-dimensional spaces or large numbers of concepts', 'Complexity of the model and training procedure compared to specialized approaches', 'Insufficient exploration of potential negative societal impacts and ethical considerations']",4,3,3.0,4,Accept
w8HXzn2FyKm,"This paper analyzes distributed linear stochastic approximation algorithms over time-varying directed graphs. The key contributions are: (1) Finite-time error bounds for consensus-based algorithms with stochastic interaction matrices, relaxing prior assumptions of bidirectional communication. (2) A push-sum based algorithm for achieving specific convex combinations of local equilibria with unidirectional communication, including finite-time analysis. (3) Novel proof techniques for handling stochastic matrices and push-sum dynamics, with direct applications to distributed reinforcement learning.","['Can the authors provide empirical results demonstrating the performance of the proposed algorithms compared to existing methods on standard RL benchmarks?', 'How restrictive is Assumption 6 (convergent absolute probability sequences) in practice? Are there common scenarios where it would not hold?', 'Can the analysis be extended to non-linear function approximation or asynchronous updates among agents?']","['Results are limited to linear function approximation', 'Lack of experimental validation makes it difficult to assess practical impact', 'Error bounds may not be easily computable or interpretable in practice', 'Assumes a Markovian noise model, which may not hold in all practical settings', 'Assumes uniformly strongly connected graphs, which may not always be the case in practice']",False,4,3,4,7,4,"['Relaxes assumptions in prior work to allow stochastic (not necessarily doubly stochastic) interaction matrices', 'Provides rigorous finite-time error bounds for both consensus-based and push-sum algorithms', 'Develops novel proof techniques to handle challenges of stochastic matrices and push-sum dynamics', 'Addresses important cases in distributed stochastic approximation and reinforcement learning', 'High technical depth and rigor in proofs and analysis']","['Lack of empirical evaluation or demonstration of practical impact', 'Finite-time bounds are complex with many constants that may be difficult to compute in practice', 'Highly theoretical nature may limit accessibility to practitioners', 'Analysis limited to linear function approximation']",4,4,3.0,4,Accept
e-JV6H8lwpl,"This paper presents a novel nonlinear system identification method using neural networks with a bottleneck structure, producing state estimators and predictors for model predictive control. The approach is theoretically justified, connects to subspace identification methods, and is demonstrated on both low and high-dimensional tasks.","['How well does the method scale to systems with very high-dimensional state spaces?', 'Can the approach be extended to partially observable systems?', 'How does the method compare to other recent deep learning approaches for system identification?', 'What are the computational requirements for real-time control applications?']","['Assumes full state observability', 'Only demonstrated on relatively simple dynamical systems', 'May struggle with very high-dimensional or highly nonlinear dynamics', 'Performance on systems with significant measurement noise not thoroughly explored']",False,3,4,3,7,4,"['Strong theoretical foundation linking bottleneck structure to system observability', 'Clear connections to established subspace identification methods', 'Demonstrates interpretability of learned models', 'Direct applicability to model predictive control', 'Validation on both low and high-dimensional examples', 'Clear writing and explanation of methods']","['Limited experimental validation on complex real-world systems', 'Reliance on strong assumptions for theoretical guarantees', 'Unclear scalability to very high-dimensional or highly nonlinear systems', 'Limited comparison with other state-of-the-art nonlinear system identification methods', 'Potential issues with identifiability and uniqueness of learned representations not fully addressed']",4,3,4.0,3,Accept
ZWykq5n4zx,"This paper presents improved high-probability generalization bounds for randomized learning algorithms with on-average uniform stability. The key technique is a confidence-boosting approach using subbagging. The authors derive refined first-moment generalization error bounds and use these to establish sharper high-probability bounds. The results are applied to stochastic gradient descent (SGD) to obtain tighter bounds, especially for decaying learning rates. For deterministic algorithms, the bounds improve on prior work by removing a log factor.","['How do the new bounds compare empirically to previous bounds on real datasets/problems?', 'What are the practical implications of these results for machine learning practitioners?', 'Could the confidence-boosting technique be extended to other types of learning algorithms?', 'How does the computational cost of the confidence-boosting technique compare to standard training?', 'Can this approach be extended to other notions of algorithmic stability beyond on-average uniform stability?']","['Results are purely theoretical with no empirical validation', 'Focus is narrow, limited to specific types of algorithms and stability notions', 'Practical impact of the tighter bounds is not clearly demonstrated', 'Confidence-boosting technique may increase computational cost', 'The highly technical nature of the work may limit its accessibility to a broader audience']",False,4,3,3,7,4,"['Novel theoretical results improving on important prior work', 'Rigorous technical analysis and proofs', 'Addresses an important open problem in generalization theory', 'Results apply to practically relevant algorithms like SGD', 'Improves bounds for both randomized and deterministic algorithms']","['Purely theoretical - no empirical validation', 'Limited discussion of practical implications', 'Highly technical presentation may be difficult for non-experts to follow', 'Some results only apply to subset of data, not full sample']",3,4,2.0,3,Accept
3mgYqlH60Uj,"This paper introduces a Normalized Cumulative Fatigue (NCF) model derived from biomechanical principles to improve reinforcement learning for locomotion tasks. The NCF model is used as a reward signal, replacing instantaneous torque-based rewards. Experiments across multiple locomotion environments demonstrate improvements in gait symmetry, periodicity, and relaxedness compared to baselines, without requiring motion capture data or manual tuning.","['Can you provide more theoretical justification for applying the NCF model to dynamic locomotion tasks, particularly in terms of its biological plausibility?', 'How well does the approach generalize to non-locomotion tasks or different character morphologies? Have you tested it on any other types of movements?', 'How sensitive are the results to key hyperparameters like the recovery coefficient? Is there a systematic way to determine optimal values for different tasks or characters?', 'How do the learned behaviors compare qualitatively to real human/animal locomotion? Are there any noticeable artifacts or unnatural movements?']","['Approach is only evaluated on simulated locomotion tasks', 'May not generalize well to non-locomotion tasks', 'Biological plausibility of NCF model for locomotion not fully validated', 'Potential negative impacts not thoroughly discussed', 'Possible increased computational cost compared to simpler reward functions']",False,3,3,3,7,4,"['Novel application of biomechanical fatigue modeling to RL for locomotion', 'Improved results over baselines across multiple environments and metrics', 'Does not require motion capture data or manual behavior specification, potentially enabling broader applicability', 'Thorough evaluation using multiple relevant metrics and environments', 'Computationally efficient compared to full muscle simulations']","['Limited theoretical analysis and justification for the NCF model', 'Experiments focused primarily on locomotion tasks', 'Improvements over baselines are modest in some cases', 'Lack of comparison to approaches using motion capture data']",3,3,3.0,3,Accept
z7p2V6KROOV,"This paper presents WILDS 2.0, a significant update to the WILDS benchmark for evaluating machine learning models on real-world distribution shifts. It adds 14.5M unlabeled examples to 8 of the 10 original WILDS datasets, expanding them by 3-13x. The authors systematically evaluate several unsupervised domain adaptation methods, including domain-invariant, self-training, and self-supervised approaches, across these expanded datasets. Results show mixed performance, with many methods not outperforming standard supervised training despite using additional unlabeled data. The paper provides valuable insights into the challenges of leveraging unlabeled data for improving robustness to real-world distribution shifts across diverse tasks and modalities.","['Could you elaborate on why certain methods failed on specific datasets?', 'Have you considered evaluating more recent unsupervised adaptation methods?', 'How representative are the datasets of real-world challenges in their respective domains?', 'Have you explored using the domain annotations provided in WILDS to develop multi-source/multi-target adaptation methods?']","['The benchmark may not capture all relevant aspects of real-world distribution shifts', 'Performance on these datasets may not generalize to other settings', 'The unlabeled data may introduce unintended biases or distribution shifts', 'The benchmark is computationally expensive, which may limit accessibility for some researchers']",False,4,4,4,7,4,"['Significant expansion of an important benchmark with highly relevant unlabeled data', 'Comprehensive and systematic evaluation of multiple unsupervised adaptation methods across diverse datasets', 'Technically sound with careful experimental design and hyperparameter tuning', 'Provides valuable insights into challenges of leveraging unlabeled data for real-world shifts', 'Open-sourcing of data and code to facilitate reproducibility and further research', 'Highlights important open problems and directions for future work']","['Limited comparison to very recent state-of-the-art methods', 'Could provide more in-depth analysis of why methods fail on certain datasets', 'Focus is on existing methods rather than proposing new ones', 'Some datasets may not be fully representative of their domains']",3,4,4.0,4,Accept
3Li0OPkhQU,"This paper presents a theoretical analysis of a semi-supervised algorithm for learning convolutional neural networks (CNNs) under the assumption that the distribution of important image patches has low-dimensional structure. The authors prove that their algorithm can efficiently learn a broad class of convolutional functions when the covering number of the patch distribution is small. They provide both upper and lower bounds on sample complexity, showing the dependence on covering number is tight. The paper includes some empirical validation on simple datasets to support the theoretical findings.","['How might the analysis be extended to more complex CNN architectures, such as those with skip connections or attention mechanisms?', 'What are potential approaches to relaxing the assumptions while still maintaining tractable analysis?', 'Could this theoretical framework be used to design new CNN architectures or training algorithms that are provably efficient?']","['The theoretical results do not directly apply to standard CNN training', 'Empirical evaluation is limited to simple datasets', 'Assumptions may not fully capture properties of natural image data', 'The gap between the analyzed semi-supervised algorithm and practical CNN training methods limits immediate applicability', 'The paper does not provide methods for estimating or bounding the patch covering number in practice']",False,3,3,3,7,4,"['Novel theoretical analysis connecting patch statistics to learnability of CNNs', 'Provides both upper and lower bounds on sample complexity', 'Analyzes a practically-motivated semi-supervised algorithm', 'Offers insights into what properties of data distributions make CNN learning tractable', 'Includes some empirical validation of theoretical results']","['Limited experimental validation, mostly on simple datasets', 'Assumptions may be too strong for complex real-world datasets', 'Analysis is for a specific semi-supervised algorithm, not standard CNN training', 'Practical implications of the theoretical results could be discussed more']",3,3,3.0,4,Accept
D1TYemnoRN,"This paper proposes a novel framework to connect optimization and generalization in machine learning by analyzing the length of optimization trajectories under gradient flow. The key component is the Uniform-LGI property, which allows deriving explicit length estimates for gradient flow paths and using these to obtain generalization bounds. The framework is applied to underdetermined linear regression, kernel regression, and overparameterized neural networks.","['How might the framework be extended to analyze discrete gradient descent or SGD?', 'Can the theoretical insights be translated into practical guidelines or algorithms?', 'Are there ways to empirically validate the tightness of the derived bounds on real-world datasets?', 'How sensitive are the results to violations of the assumptions, particularly Uniform-LGI?', 'Could this framework be applied to other optimization algorithms beyond gradient-based methods?']","['Framework currently limited to gradient flow algorithm', 'Assumes i.i.d. data which may not hold in practice', 'Theoretical results could benefit from more extensive empirical validation', 'Bounds may be loose in practice, limiting direct applicability', 'Potential computational complexity in applying the framework to large-scale problems']",False,4,3,4,8,4,"['Novel approach connecting optimization and generalization through trajectory length analysis', 'Introduction of Uniform-LGI property as a valuable analytical tool', 'Rigorous theoretical analysis with detailed proofs', 'Application to multiple important ML models (linear regression, kernel methods, neural networks)', 'Expands understanding of benign overfitting in high-dimensional settings', 'Provides new insights into why gradient-based methods generalize well']","['Limited empirical validation of theoretical results', 'Analysis focused on gradient flow rather than practical discrete optimization methods', 'Highly technical nature may limit accessibility for practitioners', 'Some assumptions (e.g., Uniform-LGI, i.i.d. data) may not always hold in practice']",4,4,4.0,4,Accept
CVfLvQq9gLo,"This paper introduces ARTEMIS, a novel approach for image retrieval using free-form text modifiers. The method combines Explicit Matching (EM) and Implicit Similarity (IS) modules, utilizing lightweight text-guided attention mechanisms. ARTEMIS achieves state-of-the-art results on multiple benchmarks (FashionIQ, Shoes, CIRR) while maintaining computational efficiency, as demonstrated through a comparative efficiency study.","['How might the approach be extended to better handle negations in the text modifiers?', 'Have you considered alternative evaluation metrics that may better capture fine-grained retrieval capabilities?', 'How does the performance of ARTEMIS scale with larger image/text encoders or on much larger datasets?', 'What are some potential real-world applications where ARTEMIS could be particularly useful?']","['Limited performance on complex language understanding (e.g., negations)', 'Reliance on pre-trained encoders may limit applicability to new domains', 'Current evaluation metrics may not fully capture model capabilities']",False,4,4,4,8,4,"['Innovative architecture combining explicit matching and implicit similarity modules', 'State-of-the-art performance on multiple benchmark datasets', 'Computationally efficient compared to more complex prior approaches', 'Thorough ablation studies and qualitative analyses', 'Effective use of lightweight attention mechanisms']","['Limited performance on negations in text queries', 'Reliance on pre-trained image and text encoders', 'Potential limitations of recall-based evaluation metrics', 'Some benchmark datasets use artificially generated text modifiers rather than natural language']",3,4,4.0,4,Accept
xRK8xgFuiu,"This paper introduces CDCF (Causal Discovery via Cholesky Factorization), a novel algorithm for learning DAG structures from observational data. The method uses Cholesky factorization of the covariance/precision matrix to recover the topological ordering of nodes, merging topology order learning and graph estimation into one step. The algorithm achieves significant improvements in both time and sample complexity compared to existing methods, with strong theoretical guarantees and state-of-the-art empirical performance on synthetic and real datasets.","['How might the CDCF algorithm be extended to handle nonlinear SEMs?', 'What are the implications of this work for causal discovery in large-scale systems?', 'How does the method perform when the equal variance assumption is violated?', 'Are there plans to release code/software implementing the CDCF algorithm?', 'How well does the method scale to very large graphs (millions of nodes), and what are the main computational bottlenecks?']","['The method is currently limited to linear SEMs, which may not capture all real-world causal relationships', 'Theoretical guarantees rely on specific assumptions that may not always hold in practice, potentially limiting applicability', 'Performance on complex, large-scale real-world datasets with potential nonlinear relationships is not extensively evaluated', 'The approach does not consider interventional or time-series data, which are important in many causal inference scenarios']",False,4,3,4,8,4,"['Novel algorithm with strong theoretical foundations', 'Significant improvements in time and sample complexity', 'Extensive experiments showing state-of-the-art performance on synthetic and real data', 'Clear exposition and thorough theoretical analysis', 'Potential for scaling to large graphs (up to 10,000 nodes)']","['Limited to linear structural equation models (SEMs)', 'Assumptions required for theoretical guarantees may be restrictive', 'Limited evaluation on complex real-world datasets', 'Lack of extension to nonlinear scenarios', 'Some results on real data are not conclusive']",4,4,4.0,4,Accept
T_uSMSAlgoy,"This paper introduces a tree-based decoder-centric (TDC) algorithm for efficiently identifying latent holes in VAEs for text generation. It provides a theoretical unification of previous hole indicators and presents an empirical analysis of how holes impact generation, their distribution, and information content. Experiments are conducted on multiple datasets and VAE models.","['How well would the TDC algorithm generalize to other domains beyond text generation?', 'What are the practical implications for improving VAE architectures based on these findings?', 'How does the computational efficiency of TDC compare quantitatively to previous methods?', 'Could the insights about latent holes be used to develop new regularization techniques for VAEs?', 'Are there any potential negative impacts of this work that should be considered?']","['Analysis limited to text generation VAEs', 'Does not explore how to use hole identification to actually improve VAE performance', 'Generalizability to non-text domains unclear', 'Computational complexity of TDC algorithm not thoroughly analyzed']",False,3,3,3,6,4,"['Novel and efficient TDC algorithm for identifying latent holes', 'Focus on decoder network, which is an underexplored perspective', 'Comprehensive experiments on multiple datasets and VAE models', 'Theoretical unification of previous hole indicators', 'Interesting empirical findings on hole distribution and impact on generation']","['Limited scope - focuses only on text generation VAEs', 'Practical implications and potential solutions not fully explored', 'Clarity of writing and presentation could be improved in some areas', 'Theoretical justification for focusing on decoder could be stronger']",3,3,3.0,3,Accept
SN2bkl9f69,"This paper introduces MSMT-GAN, a novel architecture for text-to-image synthesis with three key components: Multi-Tailed Word-level Initial Generation, Spatial Dynamic Memory module, and Iterative Multi-Headed refinement mechanism. Experiments demonstrate significant improvements over state-of-the-art methods on the CUB dataset and competitive performance on COCO.","['What are the computational costs of MSMT-GAN compared to prior methods?', 'Why are the improvements on COCO more modest than on CUB?', 'How does the model scale to larger datasets or higher resolutions?', 'Have the authors explored ways to reduce the model size or computational requirements?', 'What are potential real-world applications for this improved text-to-image synthesis model?']","['The approach may not generalize as well to more complex datasets beyond CUB and COCO', 'The increased model complexity and size could limit deployment on resource-constrained devices', 'High computational requirements may restrict use in real-time applications', ""The model's performance on datasets with more diverse or abstract concepts is not explored""]",False,3,3,4,7,4,"['Novel architecture with well-motivated components', 'Significant improvements on CUB dataset (21.58% lower FID, 4.24% higher R-precision)', ""Thorough ablation studies validating each component's effectiveness"", 'Competitive performance on COCO dataset', 'Qualitative results show improved image quality and text alignment']","['Complex model architecture may limit practical applicability', 'More modest improvements on COCO dataset compared to CUB', 'High computational requirements and training time', 'Limited discussion of potential negative societal impacts', 'Some implementation details and comparisons to very recent methods are missing']",4,3,4.0,4,Accept
6ya8C6sCiD,"This paper introduces a novel 'symbolic mapping' architecture for multi-agent language learning. The key contributions are: (1) Learning language in simple referential games before transferring to more complex dialog games, (2) Using a symbolic mapping component to associate inputs with relevant symbols, and (3) Exploring vocabulary expansion as environments become more complex. Experiments demonstrate benefits of this approach over baselines in terms of task performance, compositionality, and symmetry of learned languages.","['How does the approach compare to state-of-the-art methods in emergent communication?', 'Can you provide more theoretical justification for why symbolic mapping enables better language transfer?', 'How well does the approach scale to larger vocabularies and more complex environments?', 'Could you provide more analysis of the properties of the learned languages that enable better transfer?', 'What are potential real-world applications of this approach beyond the experimental settings described?']","['Experiments are limited to relatively simple environments and small vocabularies', 'The approach may not scale easily to much more complex, open-ended language tasks', 'Drawing strong conclusions about natural language evolution from these experiments requires caution']",False,3,2,3,7,4,"['Novel symbolic mapping architecture that enables language transfer across tasks', 'Interesting exploration of language evolution from simple to complex tasks', 'Empirical demonstration of benefits for compositionality and task transfer', 'Investigation of vocabulary expansion in increasingly complex environments']","['Experiments are limited in scale and complexity', 'Limited comparison to state-of-the-art approaches in emergent communication', 'Theoretical justification and analysis of why symbolic mapping works is limited', 'Some lack of clarity in explaining details of the approach']",3,3,3.0,3,Accept
1gEb_H1DEqZ,"This paper introduces PROPHET, a pre-trained language model enhanced with logical reasoning capabilities. The key innovations are: (1) using 'facts' as knowledge units, (2) constructing a 'logical graph' to capture relationships between facts, and (3) introducing new pre-training objectives focused on logical reasoning. Empirical results demonstrate consistent, though often modest, improvements over BERT baselines on various NLU tasks, particularly those requiring logical reasoning.","['How does the computational cost of PROPHET compare to standard BERT pre-training?', ""What is the impact of errors in fact extraction on the model's performance?"", 'Can you provide examples of specific logical reasoning tasks where PROPHET shows the most improvement?']",The approach relies on potentially error-prone NLP tools for fact extraction and logical graph construction. The scalability and computational costs of the method are not adequately discussed. The paper lacks thorough comparison to state-of-the-art models specifically designed for logical reasoning tasks.,False,3,3,3,6,4,"['Novel approach to enhancing logical reasoning in pre-trained language models', 'Well-motivated pre-training objectives targeting different aspects of logical reasoning', 'Consistent improvements over BERT baselines across multiple tasks', 'Detailed ablation studies and analyses']","['Improvements over BERT, while consistent, are often relatively modest (1-2 points)', 'Limited comparison to other recent models or approaches specifically designed for logical reasoning', 'Added complexity in pre-processing and pre-training, with insufficient discussion of computational costs', 'Reliance on potentially error-prone NLP tools for fact extraction and logical graph construction']",3,3,3.0,3,Reject
qyTBxTztIpQ,"This paper introduces CrowdPlay, a framework for crowdsourcing human gameplay data in reinforcement learning environments, particularly Atari games. The main contributions include a flexible crowdsourcing pipeline, a large-scale dataset of human Atari gameplay (over 250 hours), support for multimodal and multiagent data collection, analysis of incentive designs, and benchmark results for offline RL algorithms on this human data.","['How generalizable is the CrowdPlay framework to RL environments beyond Atari games?', 'What specific challenges does human data pose for offline RL algorithms compared to synthetic data?', 'Are there plans to expand the dataset or framework in future work, particularly for multi-human gameplay data?', 'What potential applications do you envision for the CrowdPlay framework outside of reinforcement learning research?']","['Dataset currently limited to Atari games', 'Potential biases in data due to participant demographics and incentive designs', 'Challenges in scaling up human-human multiplayer data collection']",False,3,4,3,7,4,"['Novel and flexible crowdsourcing pipeline for RL environments', 'Large-scale, diverse dataset of human Atari gameplay', 'Support for multimodal and multiagent data collection', 'Detailed analysis of incentive designs for data quality', 'Open-source code and dataset enhancing reproducibility', 'Benchmark results highlighting challenges for offline RL on human data']","['Limited evaluation beyond Atari games', 'Some imbalances in the collected dataset', 'Benchmark results are somewhat preliminary and expected', 'Limited depth in analysis of why offline RL algorithms struggle with human data']",3,3,4.0,4,Accept
hxitw01k_Ql,"This paper analyzes memory architectures for reinforcement learning in partially observable environments, focusing on a simple two-hypothesis testing problem. It compares random access memory (RAM) and a constrained 'Memento' memory, deriving optimal policies and performance bounds. Key findings include that both architectures can achieve exponential improvements in performance with memory size, and that the constrained Memento architecture leads to better learned policies when randomly initialized. The paper provides theoretical analysis, including a novel 'necklace' policy for Memento memory, and empirical results supporting the theoretical findings.","['How generalizable are the insights to other RL problems?', 'Could you expand the empirical evaluation to other environments?', 'How might these ideas apply to deep RL with recurrent networks?', 'What is the computational complexity of implementing the proposed memory architectures and policies?']","['Results may not generalize beyond simple two-hypothesis setting', 'Empirical evaluation is limited to a specific POMDP problem', 'Practical applicability to real-world RL problems with high-dimensional state spaces is unclear', 'Computational complexity of implementing the proposed methods in larger-scale problems is not addressed']",False,3,3,3,7,4,"['Rigorous theoretical analysis of memory architectures', 'Novel insights on performance of constrained memory', 'Interesting empirical findings supporting theoretical results', 'Potential insights for designing memory in RL agents']","['Very specific and simplified problem setting', 'Limited scope of empirical results', 'Writing and presentation could be improved in places', 'Unclear generalizability to more complex POMDPs']",3,3,3.0,4,Accept
RftryyYyjiG,"This paper proposes a method for extreme compression of pre-trained language models using tensor decomposition, particularly Tucker decomposition. The authors achieve up to 48x compression of BERT while maintaining comparable performance on GLUE tasks and improving inference speed. The method is complementary to other compression techniques like knowledge distillation.","['How well would this method scale to much larger models like GPT-3?', 'Can you provide more theoretical insight into why Tucker decomposition works so well for compressing Transformer layers?', 'How does this approach compare to other compression techniques like pruning or quantization in terms of compression ratio vs performance tradeoffs?', 'What are the limitations of the tensor decomposition approach? Are there cases where it would not work well?']","['Only evaluated on BERT and GLUE benchmark tasks', 'Limited experiments on larger models beyond BERT-base', 'Potential negative impacts of model compression (e.g., on fairness/bias) are not discussed', 'Theoretical understanding of why the approach works so well is limited']",False,4,3,4,8,4,"['Achieves impressive compression ratios (up to 48x) with minimal performance loss', 'Novel application of tensor decomposition for compressing large language models', 'Compressed models are faster for inference', 'Thorough experiments and analysis on GLUE benchmark', 'Method is complementary to other compression techniques']","['Evaluation limited to BERT, not tested on larger models like GPT-3', 'Limited theoretical analysis of why the method works so well', 'Potential limitations and negative impacts not thoroughly discussed', 'Clarity of technical presentation could be improved in some areas']",4,3,3.0,4,Accept
jNB6vfl_680,"This paper introduces Global Magnitude Pruning (GP) with Minimum Threshold (MT) as a simple yet effective neural network pruning technique. GP, a global magnitude-based pruning method, alone achieves state-of-the-art results on several benchmarks, while MT further improves performance by preserving a minimum number of weights per layer. Extensive experiments demonstrate SOTA results on ImageNet and other datasets across multiple architectures, outperforming more complex pruning approaches.","['Can you provide more theoretical justification for why GP works better than layer-wise pruning?', 'How sensitive is the method to the choice of MT value, and is there a way to automatically determine the optimal value?', 'Have you explored ways to jointly optimize for both parameter and FLOP reduction?', 'How does the method perform on non-vision tasks or other AI domains?', 'Have you tested the method on larger models or different modalities, such as language models or audio processing networks?']","['The method can result in higher FLOPs compared to some layer-wise pruning approaches', ""Limited theoretical understanding of the method's effectiveness"", 'Primarily tested on vision tasks, with less evidence for other domains', 'Manual tuning may be required for optimal MT value selection']",False,4,4,4,7,4,"['Simple and easy to implement pruning method, especially compared to more complex techniques', 'Achieves state-of-the-art results on multiple benchmarks including ImageNet', 'Extensive experiments and ablation studies across different architectures and datasets', ""One-shot pruning method that doesn't require iterative pruning"", 'Generalizes well across different network architectures and domains']","['Limited theoretical analysis or justification for why the method works so well, which could be an avenue for future work', 'Can incur higher FLOPs compared to some layer-wise pruning methods', 'Limited exploration of hyperparameter sensitivity, particularly for the MT value', 'Does not compare against some very recent SOTA pruning methods']",3,4,4.0,4,Accept
Ih0iJBSy4eq,"This paper introduces novel reinforcement learning algorithms for finding Stackelberg-Nash equilibria in general-sum Markov games with leader-controlled transitions. The authors propose optimistic and pessimistic variants of least-squares value iteration for online and offline settings, respectively. They provide strong theoretical guarantees, including sublinear regret bounds for the online algorithm and suboptimality bounds for the offline algorithm, which are near-optimal in some cases. The approach incorporates function approximation to handle large state spaces, advancing the state-of-the-art in multi-agent RL theory.","['How well do the theoretical results translate to empirical performance in practice?', 'What are the key challenges in implementing these algorithms for real-world multi-agent RL problems?', 'Can the leader-controlled transition assumption be relaxed while maintaining similar theoretical guarantees?', 'How computationally efficient is the matrix game oracle in practice for games with many followers?', 'Can the proposed methods be extended to non-linear function approximation schemes, such as neural networks?']","['Results are purely theoretical with no empirical validation', 'Assumes access to a computational oracle for multi-follower games', 'Limited to leader-controlled Markov games', 'Assumes linear function approximation which may not hold in all scenarios']",False,4,3,4,8,4,"['Novel algorithms for an important multi-agent RL problem', 'Strong theoretical results with near-optimal regret and sample complexity bounds', 'Handles both online and offline settings', 'Incorporates function approximation for large state spaces', 'Advances state-of-the-art in multi-agent RL theory', 'Addresses both leader and follower perspectives in Stackelberg games']","['Lack of empirical evaluation to validate theoretical results', 'Some assumptions (e.g., known rewards, leader-controlled transitions) may limit practical applicability', 'Computational complexity for multiple followers not fully addressed', 'Limited discussion of practical implementation challenges, including potential issues with the matrix game oracle and scalability to large-scale problems']",4,4,4.0,4,Accept
N0n_QyQ5lBF,"This paper introduces the task of unsupervised vision-language (VL) grammar induction, aiming to extract a shared hierarchical structure for both images and captions simultaneously. The authors propose a novel method called CLIORA, which extends DIORA to incorporate visual information through feature-level and score-level fusion. Empirical results show improvements over baselines on grammar induction and visual grounding tasks on MSCOCO and Flickr30k datasets. The paper also introduces a new evaluation metric (CCRR) to assess VL structures more directly.","['How does the computational complexity of CLIORA compare to baselines, and how well does it scale to longer sentences or more complex images?', 'What are some potential downstream applications that could benefit from the induced VL structures?', 'How sensitive is the method to the choice of pre-trained object detector and visual features?', 'Can you provide more qualitative examples and analysis of the learned VL structures?', 'How might this approach be extended to other languages or modalities beyond image-caption pairs?']","['The approach relies on pre-trained object detectors, limiting its applicability to domains without such models', 'Only evaluated on English captions - unclear how well it would generalize to other languages', 'Binary tree structure may be overly restrictive for some language constructs', 'Computational requirements may limit scalability to very large datasets']",False,3,3,3,6,4,"['Introduces a novel and challenging task of unsupervised VL grammar induction', 'Proposes a well-motivated method (CLIORA) that incorporates visual information in multiple ways', 'Demonstrates consistent improvements over strong baselines on multiple tasks', 'Introduces a new evaluation metric (CCRR) for directly assessing VL structures', 'Provides thorough empirical evaluation, including comprehensive ablation studies and error analysis']","['Relies on pre-trained object detectors, limiting its fully unsupervised nature', 'Improvements over baselines, while consistent, are relatively modest in some cases (1-3% on most metrics)', 'Limited discussion of broader implications and potential applications of the induced VL structures', 'Complexity of the approach may make it challenging for others to build upon', 'Limited qualitative analysis of the learned VL structures']",4,3,3.0,3,Accept
HiHWMiLP035,"This paper introduces E[2]CM, a novel early exit technique for neural networks based on class means at each layer. Unlike existing methods, E[2]CM does not require gradient-based training of internal classifiers, making it suitable for low-power devices and scenarios with limited training time. The method shows improved performance over state-of-the-art early exit approaches on image classification tasks, in terms of accuracy and/or computational efficiency. It can be applied to both supervised and unsupervised learning and can be combined with existing early exit methods.","['How could the method be extended to other types of neural network architectures or tasks?', 'Are there ways to further reduce the memory overhead for storing class means in very large networks?', 'Could the approach be combined with other dimensionality reduction techniques to improve efficiency?', 'How might the method perform in scenarios with highly imbalanced datasets or with out-of-distribution samples?', 'Are there potential applications of this technique beyond reducing computational cost, such as in interpretability or robustness?']","The method's performance is primarily demonstrated on image classification tasks, and its generalizability to other domains is not fully explored. The approach may have increased memory requirements for very large networks. The sensitivity of the method to dataset characteristics and choice of distance metric is not thoroughly analyzed.",False,3,3,3,7,4,"['Novel approach that does not require training additional classifiers', 'Applicable to both supervised and unsupervised learning', 'Can be combined with existing early exit methods', 'Improves accuracy and/or efficiency over state-of-the-art on several benchmarks', 'Well-suited for scenarios with limited training time/compute', 'Relatively simple to implement with low computational overhead']","['Limited theoretical analysis or justification for why the class means approach works well', 'Experiments mostly focused on image classification tasks', 'Some results show only modest gains over existing methods', 'Potential memory overhead for very large networks not thoroughly analyzed', 'Limited discussion of limitations and failure cases']",4,3,3.0,3,Accept
GQd7mXSPua,"This paper proposes a method for meta-learning covariance matrices to improve uncertainty estimation in few-shot learning. The key contributions are: 1) Meta-learning diagonal or low-rank covariance factors using a set encoder, 2) An energy-based inference procedure for improved OOD calibration, 3) Incorporating bi-Lipschitz regularization. Empirical evaluation on toy datasets and standard few-shot benchmarks shows improvements over baselines in calibration and OOD detection.","['How does the computational complexity scale with the number of classes and shots?', 'What is the relative importance/impact of each of the three main components?', 'How does the method compare to more recent meta-learning approaches for uncertainty estimation?', 'Are there scenarios where the added complexity may not be justified by the gains?', 'What are some potential real-world applications where this method could have significant impact?']","['The approach adds complexity which may limit practical applicability in some settings', 'Evaluation limited to few-shot learning benchmarks', 'Potential negative impacts are not thoroughly discussed, e.g. overconfidence in high-stakes applications if the method fails']",False,3,3,3,8,4,"['Novel approach to meta-learning covariance matrices for uncertainty estimation in few-shot learning', 'Well-motivated method addressing clear limitations of prior approaches', 'Comprehensive empirical evaluation showing improvements over relevant baselines', 'Good analysis and ablation studies of the proposed components', 'Clear improvements in OOD calibration and detection']","['Added complexity compared to simpler approaches, with limited discussion of practical implications', 'Limited theoretical analysis or guarantees for the proposed method', 'Missing comparisons to some recent meta-learning approaches for uncertainty estimation', 'Potential scalability issues for larger datasets/models']",3,3,4.0,4,Accept
vr39r4Rjt3z,"This paper introduces two architectural modifications for neural network classifiers to improve continual learning: Masked Highway Connections (MHC) and Layer-Wise Normalization (LWN). MHC selectively updates weights by masking less important activations, while LWN reduces distribution shifts in activations between tasks. Extensive experiments across multiple datasets and continual learning scenarios demonstrate consistent improvements over baselines and compatibility with existing continual learning techniques, without requiring additional memory or computational resources.","['How well do the proposed techniques generalize to other types of tasks beyond image classification?', 'Is there a theoretical justification for why LWN works better than batch normalization for continual learning?', 'How do MHC and LWN affect training time and inference speed compared to standard architectures?', 'How sensitive are the results to hyperparameter choices, particularly the masking threshold in MHC?', 'How do the proposed methods perform on more challenging continual learning scenarios, such as class-incremental learning with a large number of tasks?']","['Experiments are limited to image classification tasks', 'Theoretical analysis could be strengthened', 'Potential computational overhead not thoroughly analyzed', 'Hyperparameter sensitivity not fully explored']",False,3,3,3,7,4,"['Novel architectural modifications specifically designed for continual learning', 'Thorough empirical evaluation on multiple datasets and setups', 'Techniques are simple to implement and computationally efficient', 'Compatible with and improves existing continual learning methods', 'Addresses both weight-level and feature-level aspects of forgetting', 'Provides insights and visualizations to explain the effectiveness of the techniques', 'Does not require additional memory or computational resources']","['Limited theoretical analysis - relies mostly on empirical results', 'Experiments are focused only on image classification tasks', 'Comparison to some recent state-of-the-art continual learning methods is missing', 'Hyperparameter sensitivity, especially for MHC masking threshold, is not thoroughly explored']",3,3,3.0,3,Accept
EhYjZy6e1gJ,"This paper introduces PICO (Partial label learning with COntrastive label disambiguation), a novel framework for partial label learning that combines contrastive learning with prototype-based label disambiguation. The key idea is to use contrastively learned embedding prototypes to identify the true label from candidate sets. PICO alternates between using classifier output to select positive pairs for contrastive learning and using learned embeddings to update pseudo-labels via class prototypes. Empirically, PICO significantly outperforms state-of-the-art methods on multiple benchmark datasets, achieving results competitive with fully supervised learning in some cases. The authors provide theoretical justification by interpreting PICO from an expectation-maximization perspective.","['How does the computational cost and memory requirements of PICO compare to baseline methods?', 'Could the PICO approach be extended to other domains beyond image classification, such as text or audio?', ""How sensitive is PICO's performance to the choice of key hyperparameters like the queue size and momentum coefficients?"", 'Have you tested PICO on any real-world PLL datasets with naturally occurring label ambiguity?', 'Could this approach be extended to other weakly supervised learning problems like semi-supervised or noisy label learning?']","['Current evaluation limited to image classification tasks and synthetic PLL benchmarks', 'May require careful hyperparameter tuning in practice', 'Potential increased computational cost from contrastive learning component', 'Theoretical analysis makes some simplifying assumptions that may not always hold in practice', 'May require large batch sizes or memory banks for effective contrastive learning']",False,4,3,4,8,4,"['Novel combination of contrastive learning and prototype-based disambiguation for partial label learning', 'Strong empirical results significantly outperforming state-of-the-art on multiple datasets', 'Rigorous theoretical analysis connecting the approach to EM algorithms', 'Extensive ablation studies validating key components', 'Demonstrates effectiveness on challenging fine-grained classification tasks', 'Significant improvement in performance on fine-grained datasets like CUB-200']","['Evaluation limited to image classification tasks - could benefit from experiments on other domains', 'Some hyperparameters require careful tuning', 'Computational overhead of contrastive learning and maintaining prototypes not thoroughly discussed', 'Limited comparison to some recent PLL methods', 'Lack of experiments on real-world PLL datasets']",4,4,3.0,4,Accept
OCgCYv7KGZe,"This paper introduces Auto-Encoding Inverse Reinforcement Learning (AEIRL), a novel approach that employs an auto-encoder as the reward function for inverse reinforcement learning. The authors argue that this method provides more informative reward signals and demonstrates improved robustness to noisy demonstrations compared to discriminator-based approaches. Experiments on MuJoCo locomotion tasks show enhanced performance over state-of-the-art baselines, particularly when dealing with noisy expert data.","['Can you provide more theoretical justification for why auto-encoders work better as reward functions in this context?', 'How well does AEIRL generalize to other domains beyond locomotion tasks?', 'Have you tested AEIRL on any real-world robotics tasks?', 'How sensitive is AEIRL to the choice of auto-encoder architecture and hyperparameters?', 'Are there any specific scenarios or types of tasks where AEIRL might underperform compared to traditional methods?']","['Experiments limited to MuJoCo locomotion tasks, lacking diversity in evaluation domains', 'Theoretical understanding and analysis of the method could be improved', 'Potential computational overhead of using auto-encoders not thoroughly analyzed or compared to baselines']",False,3,3,3,7,4,"['Innovative use of auto-encoders as reward functions in IRL', 'Strong empirical results on MuJoCo tasks, especially with noisy demonstrations', 'Comprehensive experiments and ablation studies', 'Clear presentation of ideas and methodology', 'Significant contribution to robustness in IRL, particularly with noisy expert data']","['Limited theoretical analysis and justification for the approach', 'Experiments confined to MuJoCo locomotion tasks', 'Lack of real-world or diverse task evaluations', 'Some claims could benefit from more rigorous statistical testing']",4,3,3.0,3,Accept
LtKcMgGOeLt,"This paper analyzes Vision Transformers (ViTs) and MLP-Mixers from a loss landscape geometry perspective, revealing that they converge to extremely sharp local minima when trained from scratch on ImageNet. The authors propose using a sharpness-aware optimizer (SAM) to smooth the loss landscape, significantly improving accuracy and robustness across various tasks without pre-training or strong data augmentations. Comprehensive experiments demonstrate that this approach allows ViTs to outperform ResNets of similar size across supervised, adversarial, contrastive, and transfer learning settings.","['Can the authors provide any theoretical insights into why SAM is particularly effective for ViTs/MLP-Mixers?', 'Are there ways to reduce the computational overhead of SAM while maintaining its benefits?', 'How well does this approach scale to even larger models and datasets?', 'How does SAM interact with other techniques like knowledge distillation?']","['The increased computational cost of SAM may limit its applicability in large-scale training scenarios', 'Results are primarily focused on image classification, and generalization to other vision tasks is unclear', 'Diminishing returns on very large datasets suggest SAM may not benefit massive pre-training']",False,4,3,4,8,4,"['Novel and thorough analysis of ViTs and MLP-Mixers from a loss landscape perspective', 'Significant improvements in accuracy and robustness using SAM', 'Enables ViTs to outperform ResNets without pre-training or strong augmentations', 'Comprehensive experiments across multiple learning paradigms', 'Insightful analysis of intrinsic model changes resulting from SAM (e.g., sparser activations)']","['SAM increases training time by approximately 2x', 'Benefits of SAM diminish on larger datasets', 'Limited theoretical analysis of why SAM works particularly well for these architectures', 'Increased computational requirements may limit practical applicability in some settings']",4,4,3.0,4,Accept
zz9hXVhf40,"This paper presents a comprehensive empirical study of design choices in offline model-based reinforcement learning (MBRL), focusing on uncertainty estimation methods and key hyperparameters. The authors compare different uncertainty penalties, analyze their properties, and investigate interactions with other hyperparameters like model ensemble size and planning horizon. Through careful tuning, they demonstrate significant performance improvements over prior methods on standard benchmarks.","['How well do the insights generalize beyond continuous control to other RL domains?', 'Could the findings be used to develop theoretical guarantees or bounds for offline MBRL methods?', 'What are the practical implications of these findings for deploying offline MBRL in real-world applications?']","['The study is focused on continuous control benchmarks and may not generalize to all RL domains', 'The computational resources required for extensive hyperparameter tuning may be prohibitive for some practitioners', 'Improved performance comes at the cost of increased complexity in implementation and tuning']",False,4,4,4,8,4,"['Rigorous comparison of multiple uncertainty estimation methods', 'Novel evaluation protocols for assessing uncertainty calibration', 'Valuable insights into interactions between key hyperparameters and uncertainty estimation', 'Significant performance improvements through careful hyperparameter tuning', 'Extensive empirical results on standard benchmarks', 'Useful recommendations for practitioners on effective uncertainty estimation', 'Thorough analysis of the impact of model count on uncertainty estimation']","['Limited theoretical analysis to complement empirical findings', 'Focus primarily on continuous control tasks, potentially limiting generalizability', 'Some implementation details differ from original papers or are relegated to appendix', 'Computational cost of extensive hyperparameter tuning may be prohibitive in some settings']",3,4,4.0,4,Accept
bsycpMi00R1,"This paper introduces Natural Hidden Gradient (NHG) flows for hidden convex-concave games, including GANs. It provides global convergence guarantees for NHG flows, analyzes replicator dynamics in restricted settings, and offers experimental validation on toy problems. The main contributions are theoretical results on NHG flow convergence and characterization of replicator dynamics behavior.","['How well does the NHG approach scale to larger, more realistic GAN architectures?', 'Are the assumptions required for the theoretical results likely to hold in practical GAN settings?', 'How does the computational complexity of NHG compare to standard gradient methods?', 'Can the authors provide experimental results on standard GAN benchmarks like CIFAR-10 or ImageNet?']","['The experimental evaluation is limited to toy problems and small-scale GANs', 'Practical applicability to large-scale GAN training is not demonstrated', 'The approach may be computationally expensive due to matrix inversions', 'Theoretical results rely on certain assumptions that may not always hold in practice']",False,4,3,4,6,4,"['Strong theoretical results proving global convergence for NHG flows in hidden convex-concave games', 'Rigorous mathematical analysis and proofs', 'Potential implications for improving GAN training stability', 'Interesting analysis of replicator dynamics in restricted game settings', 'Addresses an important problem in GAN optimization']","['Limited experimental evaluation, especially on large-scale or real-world problems', 'Practical applicability and scalability of the approach is unclear', 'Some assumptions required for theoretical results may be hard to satisfy in practice', 'Computational complexity of NHG could limit scalability to large models']",4,4,3.0,3,Accept
WN2Sup7qLdw,"This paper introduces Multi-Resolution Continuous Normalizing Flows (MRCNF), a novel approach to generative modeling of images. The key contributions are a multi-resolution transformation that preserves likelihood, using CNFs at multiple resolutions in an autoregressive fashion, and achieving comparable or better bits-per-dimension performance on image datasets with significantly fewer parameters and less training time than previous approaches.","['How does the qualitative sample quality compare to state-of-the-art GAN/VAE models?', 'Are there any practical applications or downstream tasks where this method could be particularly useful?', 'How does the approach scale to even higher resolutions (e.g. 256x256 or 512x512)?', 'Have you explored using the multi-resolution approach with other types of normalizing flows beyond CNFs?', 'What potential improvements or future work do you envision to address the out-of-distribution detection limitations?']","['The method still struggles with out-of-distribution detection, a known issue for likelihood-based models', 'Visual quality of generated samples is not evaluated comprehensively', 'Performance gains are less significant on lower resolution datasets', 'Computational requirements for larger scale experiments may limit accessibility']",False,3,4,4,7,4,"['Novel multi-resolution formulation for CNFs that is well-motivated and theoretically sound', 'Achieves comparable performance with much fewer parameters and faster training', 'Provides theoretical justification for the multi-resolution transformation', 'Comprehensive empirical evaluation including ablation studies', 'Enables efficient modeling at higher image resolutions', 'Thorough analysis of out-of-distribution properties, which is crucial for likelihood-based models']","['Performance gains are more modest on lower resolution datasets', 'Out-of-distribution detection results are mixed compared to baselines', 'Qualitative sample quality is not extensively evaluated or compared to state-of-the-art GANs/VAEs', 'Limited discussion of potential applications or broader impact']",3,3,4.0,3,Accept
ExJ4lMbZcqa,"This paper introduces the novel task of audio-visual dereverberation, proposing VIDA, an end-to-end approach that leverages visual cues about room geometry and acoustics to enhance speech dereverberation. The authors create a large-scale dataset using acoustic simulations in 3D scanned environments. Comprehensive experiments on speech enhancement, recognition, and speaker verification tasks demonstrate significant improvements over audio-only baselines on both simulated and real data.","['How does the computational cost and inference time of VIDA compare to audio-only baselines?', 'How well does the approach generalize to different types of rooms/environments not seen during training?', 'What are the main failure cases or limitations of the current approach?', 'How sensitive is the method to changes in camera viewpoint or partial occlusions?', 'How might the approach be adapted for scenarios where full visual information is not available?']","['Performance on real data still lags behind simulated results', 'Evaluation focused primarily on English speech data', 'Potential privacy concerns with using visual data for audio enhancement', 'May not generalize to all types of environments or reverberation conditions']",False,4,3,4,8,4,"['Novel and important task formulation of audio-visual dereverberation', 'Technically sound approach leveraging both audio and visual information', 'Large-scale dataset creation enabling training and evaluation', 'Significant and consistent performance gains over audio-only baselines', 'Comprehensive evaluation across multiple downstream speech tasks', 'Demonstration of sim-to-real transfer', 'Thorough ablation studies and analysis']","['Performance on real data still has room for improvement', 'Limited discussion of computational requirements and efficiency', 'Could benefit from more analysis of failure cases or limitations', 'Approach requires visual access to environment, which may not always be available', 'Potential computational overhead of processing visual data']",4,4,4.0,4,Accept
xtZXWpXVbiK,"This paper introduces FORBES, a method for learning flexible belief states in POMDPs using normalizing flows. The key contributions are: (1) a novel flow-based representation of belief states, (2) theoretical analysis showing reduced approximation error, and (3) empirical evidence of improved performance on visual control tasks.","['How does the computational complexity of FORBES compare to baseline methods?', 'How sensitive is FORBES to the choice of normalizing flow architecture?', 'How well does FORBES generalize to non-visual POMDP domains?', ""What are the key factors that contribute to FORBES' performance improvements over Dreamer?""]","['Experiments limited to digit writing and DMControl benchmarks', 'Potential computational overhead of normalizing flows not thoroughly examined', 'May not generalize easily to more complex real-world visual domains', 'Potential difficulty in tuning the normalizing flow architecture']",False,3,3,3,7,4,"['Novel use of normalizing flows for belief state inference in POMDPs', 'Strong theoretical foundation with analysis of approximation error', 'Impressive empirical results on challenging visual control tasks', 'Detailed ablation studies validating the importance of flexible belief distributions']","['Limited comparison to other recent POMDP methods beyond Dreamer', 'Experiments focused mainly on visual control tasks', 'Computational complexity and efficiency not thoroughly analyzed', 'Some implementation details and hyperparameters not fully specified']",4,3,3.0,4,Accept
-29uFS4FiDZ,"This paper proposes a novel method to learn multi-sense word embeddings by distilling knowledge from BERT. Using a two-stage approach, it extracts sense information from BERT and transfers it to static embeddings. The method achieves state-of-the-art performance on word sense induction tasks while maintaining strong results on contextual similarity. It also demonstrates improved performance when applied to topic modeling.","['How sensitive is the method to the choice of number of senses per word?', 'What is the computational cost of training compared to baselines and using BERT directly?', 'Have you explored methods to automatically determine the optimal number of senses per word?', 'Are there other downstream tasks where these embeddings could be particularly beneficial?', 'Can this method be extended to other pre-trained language models beyond BERT?']",The method still requires manually specifying the number of senses per word. Evaluation is limited to a small set of tasks and datasets. The method may inherit biases from BERT. Potential negative impacts are not thoroughly discussed.,False,3,4,3,7,4,"['Novel approach using knowledge distillation from BERT for multi-sense embeddings', 'State-of-the-art results on word sense induction tasks', 'Comprehensive experiments and comparisons to baselines', 'Demonstration of improved performance in downstream topic modeling task', 'Well-written and technically sound']","['Still requires a fixed number of senses per word', 'Improvements over baselines are sometimes modest', 'Limited evaluation on downstream tasks beyond topic modeling', 'Computational cost of training not thoroughly discussed']",4,3,4.0,4,Accept
TNxKD3z_tPZ,"This paper proposes using persistent homology to analyze MLP neural network training by representing networks as simplicial complexes and computing distances between persistence diagrams of consecutive network states. The authors demonstrate empirically that the evolution of these distances correlates strongly with validation accuracy across different datasets and architectures, suggesting potential for monitoring training and estimating generalization without a validation set.","['Can you provide any intuition or theoretical justification for why this method correlates with validation accuracy?', 'Have you explored any ways to improve the computational efficiency to scale to larger models?', 'Is it possible to extend this approach to work with convolutional or other types of layers?', 'How does this compare to other methods for estimating generalization without validation data?', 'Could you develop a predictive model to actually estimate generalization from the topological measures?', 'Have you considered performing a time-series analysis to gain more insights on how the homological convergence and validation accuracy curves vary together?']","['Only applicable to MLPs currently', 'Computationally expensive, limiting applicability to large models', 'Lacks theoretical grounding', 'Not yet demonstrated to be practically superior to using a validation set', 'Potential for overfitting to specific datasets not fully addressed']",False,3,3,3,7,4,"['Novel application of topological data analysis to neural network training', 'Extensive experiments across multiple datasets and model configurations', 'Strong empirical results showing correlation with validation accuracy', 'Potential for monitoring training without a validation set', 'Opens up new directions for theoretical analysis of deep learning', 'Control experiments demonstrate consistency across different input orders']","['Lacks theoretical justification for why the method works', 'Computationally expensive, though potential for approximations exists', 'Currently limited to MLPs, not applicable to more complex architectures', 'No predictive model built, only correlations shown', 'Experiments limited to relatively small models and datasets']",4,3,3.0,3,Accept
AS0dhAKIYA0,"This paper introduces 'semantic role relation tables' as interpretable features for machine reading comprehension, specifically for identifying supporting facts in multi-hop QA. These tables, derived from semantic role labeling, are used as input to a CNN model and evaluated on the HotpotQA dataset.","['How does the performance compare to current state-of-the-art methods on HotpotQA?', 'Can you provide concrete examples demonstrating improved interpretability over baseline methods?', 'Have you considered evaluating on other machine reading comprehension datasets beyond HotpotQA?', 'Why was a simple CNN architecture chosen, and how might more advanced architectures affect results?']","['Depends on accuracy of upstream SRL system', 'May not scale well to very long documents due to table size']",False,2,2,2,5,4,"['Novel use of semantic role labeling for creating interpretable MRC features', 'Demonstrates reasonable performance with a simple model architecture', 'Shows stability with limited training data', 'Provides an interesting direction for improving interpretability in MRC']","['Limited evaluation - only tested on HotpotQA dataset', 'Performance does not exceed state-of-the-art methods', 'Unclear how much the semantic tables actually improve interpretability in practice', 'Simple model architecture (CNN) without justification or exploration of alternatives', 'Writing and presentation could be improved for clarity']",3,2,2.0,2,Reject
qTHBE7E9iej,"This paper introduces HeLMS, a novel method for learning transferable motor skills using a hierarchical latent mixture model. The approach learns a three-level skill hierarchy from offline data, capturing both discrete behavioral modes and continuous variation. The learned skills can be efficiently transferred to new tasks, objects, and vision-based policies via reinforcement learning. Extensive experiments demonstrate improved sample efficiency and performance compared to baselines across multiple transfer scenarios, particularly in sparse reward settings.","['How sensitive is the method to hyperparameter choices?', 'How well do you expect the approach to transfer to real-world robotic systems?', 'Have you considered comparisons to methods like DIAYN, DADS, or OPAL?', 'What are some potential real-world applications or domains where this method could be particularly useful?']","['Only demonstrated in simulation', 'Complexity may make practical implementation challenging', 'Potential difficulties in hyperparameter tuning']",False,4,3,4,8,4,"['Novel hierarchical model combining discrete and continuous latent variables', 'Effective techniques for transferring learned skills via RL', 'Strong empirical results showing benefits in sample efficiency and performance', 'Thorough ablation studies and analysis providing insight into the method', 'Demonstrates transfer to multiple scenarios: new tasks, objects, and modalities']","['Fairly complex method with many components and hyperparameters', 'Only demonstrated in simulation so far', 'May require careful tuning of hyperparameters', 'Lack of comparison to some relevant baselines like DIAYN or DADS']",4,4,4.0,4,Accept
vEIVxSN8Xhx,"This paper introduces Log-Polar Space Convolution (LPSC), a novel convolution method that increases receptive field size while maintaining a small parameter count by using a log-polar coordinate system. LPSC is implemented through log-polar pooling and conventional convolution. Experiments on image classification and segmentation tasks demonstrate modest but consistent improvements over baseline CNNs and other convolution methods.","['How can the efficiency of the LPSC implementation be improved to reduce runtime overhead?', 'What are the main factors that limit the performance gains of LPSC, especially for deeper networks like ResNet?', 'Have you explored combining LPSC with other advanced convolution methods?', 'Are there certain types of tasks or datasets where LPSC is particularly effective?', 'Have you considered potential applications of LPSC beyond computer vision tasks?']","['Current implementation is significantly slower than regular convolution, limiting practical applicability', 'Performance gains are modest and less consistent for deeper network architectures', 'The approach introduces additional hyperparameters that need to be tuned', 'Evaluation is limited to image classification and segmentation tasks']",False,3,3,3,7,4,"['Novel and well-motivated convolution design using log-polar coordinates', 'Increases receptive field size with fewer parameters', 'Thorough empirical evaluation across multiple tasks, datasets, and architectures', 'Comprehensive ablation studies and comparisons to other methods', 'Theoretically grounded approach with clear motivation', 'Clear explanation of how LPSC can be implemented using existing convolution operations']","['Performance gains are relatively modest in many cases (1-2% improvement)', 'Current implementation has significant runtime overhead compared to standard convolutions', 'Less effective for deeper networks like ResNet compared to shallower ones', 'Introduces additional hyperparameters that need tuning', 'Limited theoretical analysis of the properties of LPSC']",3,3,3.0,3,Accept
BduNVoPyXBK,"This paper introduces Feature Attending Recurrent Modules (FARM), a novel architecture for improving generalization in reinforcement learning. FARM uses multiple recurrent modules with feature attention and information sharing to learn distributed representations of perceptual schemas. Experiments on three diverse tasks (recalling novel object motion compositions, active 3D object perception, and memory retention in longer obstacle sequences) demonstrate FARM's superior performance compared to baselines like LSTM, AAA, and RIMs.","['Can you provide a theoretical justification for why feature attention in FARM enables better generalization than spatial attention?', ""How might FARM's performance scale to more complex, standard RL benchmarks?"", 'What additional analyses could you perform to strengthen your claims about the learned perceptual schemas?']","The authors acknowledge some limitations, such as the potential benefits of combining spatial and feature attention and the relative simplicity of the environments used. Future work should address scaling to more complex domains, potential challenges in real-world applications, and computational efficiency of the architecture. A more comprehensive discussion of potential negative societal impacts of more generalizable RL agents would also be valuable.",False,3,4,3,7,4,"['Novel architecture combining multiple useful components (recurrent modules, feature attention, information sharing)', 'Strong empirical results demonstrating generalization across diverse tasks', 'Thorough ablation studies and insightful analysis of learned representations', 'Clear writing and presentation', 'Addresses an important problem of generalization in RL', 'Contributes to understanding perceptual schemas in RL through empirical analysis']","['Lack of theoretical analysis or guarantees for the architecture', 'Limited comparison to very recent related work or state-of-the-art methods (e.g., transformers)', 'Experiments limited to relatively simple environments/tasks', 'Some claims about learned representations and perceptual schemas could benefit from more rigorous analysis']",3,3,4.0,3,Accept
AypVMhFfuc5,"This paper introduces FrugalMCT, a framework for efficiently selecting and combining multiple ML APIs for multi-label classification tasks under budget constraints. Key components include an accuracy predictor, online API selector with theoretical guarantees, and label combiner. Extensive experiments on real commercial APIs for tasks like image classification and named entity recognition demonstrate significant cost savings (60-98%) or accuracy improvements (up to 8%) compared to individual APIs and previous work.","['What are the key limitations or failure cases of the FrugalMCT approach?', 'How does FrugalMCT compare to other ensemble methods for multi-label classification?', 'Have you explored using more sophisticated ML models for the accuracy predictor and label combiner?', 'How sensitive is the performance to hyperparameter choices and the selection of the base API?', 'What are the potential negative societal impacts or ethical concerns with widespread adoption of this approach?']","['Approach may not work well for APIs with very different label spaces or output formats', 'Performance depends on the quality of the accuracy predictor', 'May increase latency by querying multiple APIs', 'Potential privacy risks from exposing data to multiple services', 'Only tested on a limited set of tasks and datasets', 'Assumes API costs and performance characteristics are static']",False,4,3,4,8,4,"['Novel end-to-end approach integrating API selection and output combination', 'Efficient online algorithm with strong theoretical foundations and performance guarantees', 'Comprehensive experiments showing significant improvements over baselines and single APIs across multiple tasks', 'Release of large dataset of API outputs for future research', 'Clear practical impact and relevance for ML API users', 'Principled approach backed by theoretical analysis']","['Limited discussion of limitations, failure cases, and potential negative societal impacts', 'Relatively simple accuracy predictor and label combiner components', 'Could benefit from more ablation studies and comparisons to other approaches', 'Some experimental details lacking (e.g., hyperparameter tuning process)', 'Limited testing on diverse or challenging datasets beyond those presented']",4,3,3.0,4,Accept
XWODe7ZLn8f,"This paper introduces C3-GAN, a novel GAN-based method for unsupervised fine-grained image clustering. The approach combines contrastive learning, scene decomposition, and a contrastive formulation of mutual information maximization. C3-GAN achieves state-of-the-art results on multiple fine-grained image datasets, demonstrating improvements in both clustering accuracy and image generation quality.","['How does the method perform on more diverse datasets beyond single-object images?', 'What are the main failure modes or limitations of the approach?', 'How does the computational complexity and training time compare to baseline methods?', 'Can the authors provide any theoretical justification for why the contrastive formulation works better than previous approaches?', 'Are there potential applications of this technique beyond image clustering, such as in other domains or tasks?']","['Current experiments limited to datasets with relatively few classes', 'Requires knowing the number of clusters in advance', 'May not scale well to very large numbers of fine-grained classes', 'Potential negative impacts of improved fine-grained image generation not fully explored']",False,4,4,4,7,4,"['Novel technical approach combining GANs, contrastive learning, and scene decomposition', 'State-of-the-art results on multiple fine-grained image datasets', 'Thorough ablation studies validating key components', 'Improved clustering and image generation quality compared to baselines', 'Clear writing and well-organized presentation']","['Limited discussion of limitations and failure cases', 'Lack of theoretical analysis or guarantees', 'Scalability to larger datasets and computational complexity not thoroughly addressed', 'Evaluation limited to specific image datasets']",4,3,4.0,4,Accept
Yr_1QZaRqmv,"This paper proposes a novel decision tree algorithm for solving Markov Decision Processes (MDPs) with large state spaces. The approach iteratively grows a decision tree to partition the state space, solving smaller aggregated MDPs using linear programming. The algorithm aims to find a compact representation of the optimal policy while maintaining computational tractability. The authors provide theoretical analysis, relating their method to state aggregation theory, and demonstrate it on simple examples like gridworld and cartpole.","['How does the method perform on more complex benchmark problems beyond gridworld and cartpole?', 'How does it compare to state-of-the-art deep RL methods in terms of performance and computational efficiency?', 'What are the limitations in terms of problem size/complexity where the method becomes intractable?', 'How sensitive is the method to the choice of initial partitioning?', 'What theoretical guarantees can be provided for the optimality or near-optimality of the resulting policy?']","['The method may not scale well to very high-dimensional state spaces', 'Assumes the optimal policy can be well-represented by a decision tree', 'Limited to problems with relatively small action spaces', 'Requires full model of MDP transition probabilities and rewards', 'The quality of the approximation may degrade for problems where the optimal policy is not well-represented by a tree structure']",False,3,3,2,5,4,"['Novel combination of decision trees and MDPs for state space partitioning', 'Potential for computational savings on certain problem types', 'Outputs interpretable policies as decision trees', 'Theoretical analysis of the approach and its relation to state aggregation']","['Very limited empirical evaluation (only 2 simple examples)', 'Lack of comparison to baseline methods or state-of-the-art approaches', 'Unclear scalability to more complex, high-dimensional problems', 'Assumptions about problem structure may be too restrictive']",4,2,3.0,2,Reject
XVPqLyNxSyh,"This paper presents a framework for discovering spurious and core features in deep learning image classifiers, focusing on ImageNet. Key contributions include a method to identify spurious vs core neural features with limited human supervision, the Salient ImageNet dataset, and analysis showing standard models' reliance on spurious features.","['How well does the methodology generalize to other domains beyond image classification?', 'Could the approach be extended to discover non-spatial spurious features?', 'How sensitive are the results to the specific choices in the robust model training?', 'What approaches do the authors suggest for mitigating reliance on the discovered spurious features?', 'How scalable is this method to larger datasets or more complex models?']","['The approach is limited to discovering spatial/visual spurious features', 'Reliance on human annotation introduces potential subjectivity', 'Analysis is focused mainly on ImageNet, generalizability to other datasets is unclear', 'The definitions of core and spurious features are somewhat informal', 'Potential computational cost of the approach for larger datasets or more complex models']",False,3,3,3,6,4,"['Novel and scalable methodology for identifying spurious features with minimal human supervision', 'Creation of the valuable Salient ImageNet dataset', 'Thorough experiments and analysis demonstrating issues with standard models', 'Potential impact on developing more reliable AI systems', 'Clear and well-structured presentation of the methodology and results']","[""Definition of 'core' vs 'spurious' features is somewhat subjective"", 'Limited scope - primarily applied to ImageNet classification', 'Reliance on robust model for interpretable features may limit applicability', 'Limited discussion of limitations and potential failure modes', 'Could elaborate more on practical impact and applicability of findings']",3,3,3.0,4,Accept
8la28hZOwug,"This paper introduces Prototypical Contrastive Predictive Coding (ProtoCPC), a novel method for knowledge distillation that combines prototypical methods with contrastive learning. ProtoCPC uses a probabilistic discrepancy between teacher and student features as the contrastive objective, enabling efficient implementation without large negative samples. The method is applied to supervised model compression, self-supervised model compression, and self-supervised learning via self-distillation, demonstrating strong performance across multiple benchmarks and model architectures.","['How sensitive is the method to the choice of number of prototypes and other hyperparameters?', 'Could this approach be extended to domains beyond image classification?', 'What are the computational trade-offs of ProtoCPC compared to existing methods like CRD?', 'How well does the method scale to larger models and datasets?', 'How might this method impact other areas of machine learning beyond computer vision?']","['Experiments limited to image classification tasks', 'Potential increased training complexity and memory usage compared to simpler distillation methods', 'May require careful tuning of additional hyperparameters', 'Theoretical analysis is limited and could be expanded']",False,3,3,3,7,4,"['Novel combination of prototypical and contrastive learning approaches', 'Strong empirical results across multiple tasks and datasets', 'Comprehensive experiments and ablation studies', 'Theoretical justification connecting the method to mutual information maximization', 'Efficient implementation avoiding the need for large negative samples', 'Effective on both convolutional networks and vision transformers']","['Limited theoretical analysis that could be expanded further', 'Experiments primarily focused on vision tasks, generalizability to other domains unclear', 'Some results only marginally better than baselines', 'May require careful hyperparameter tuning for optimal performance']",4,3,3.0,4,Accept
okmZ6-zU6Lz,"This paper proposes methods to estimate controllability in large-scale networked systems using coarse measurements. Two approaches are developed: a model order reduction method and a learning-based method. Theoretical error bounds are derived and validated through simulations, with potential applications in neuroscience.","['How sensitive are the results to violations of the stochastic block model assumptions?', 'What is the computational complexity of the proposed algorithms?', 'How do the approaches scale to very large networks (millions of nodes)?', 'Can the methods be extended to other controllability metrics beyond average controllability?']","['Results may not generalize beyond SBM network models', 'Scalability to very large networks is unclear', 'Only considers average controllability metric', 'Theoretical results are asymptotic - finite sample performance unclear']",False,3,3,3,7,3,"['Novel problem formulation connecting coarse measurements to fine-scale controllability', 'Rigorous theoretical analysis with probabilistic error bounds', 'Development of two complementary approaches (MOR and learning-based)', 'Validation through numerical simulations', 'Clear motivating applications in neuroscience']","['Assumptions and results may be limited to specific network models (SBM)', 'Practical applicability to very large real-world networks is unclear', 'Some theoretical results and proofs are dense and could use more intuitive explanations', 'Limited discussion of computational complexity']",4,3,3.0,4,Accept
DrZXuTGg2A-,"This paper presents novel theoretical results for differentially private stochastic convex optimization in the shuffle model. The main contributions include: (1) introducing sequentially and fully interactive variants of the shuffle model, (2) a new protocol for privately summing vectors with bounded L2 norm, and (3) new optimization algorithms with improved guarantees for various types of convex losses. The work provides rigorous theoretical analysis and proofs, improving upon prior results in local and shuffle models in some cases.","['How does the performance of your proposed algorithms compare empirically to existing baselines?', 'Are there ways to reduce the communication and runtime complexity of your algorithms?', 'How do you envision these protocols being implemented in practice given the high communication requirements?']","['Absence of empirical evaluation makes it difficult to assess practical performance', 'High communication and computational costs may limit applicability to large-scale problems', 'Potential challenges in implementing multi-round protocols in real-world distributed settings']",False,4,3,4,7,4,"['Novel theoretical contributions advancing shuffle private SCO', 'Introduction of new, well-motivated variants of the shuffle model', 'Rigorous proofs and detailed theoretical analysis', 'Improved utility guarantees over prior work for certain scenarios', 'Clear and well-organized presentation of technical content']","['Lack of empirical evaluation or comparison to existing baselines', 'Limited discussion of practical considerations and implementation challenges', 'High communication and runtime complexity for some proposed algorithms']",4,4,3.0,4,Accept
EMxu-dzvJk,"This paper introduces GRAND++, an extension of the GRAND model for graph neural networks that incorporates a source term in the diffusion process. The authors provide theoretical analysis linking GRAND++ to random walks on graphs and demonstrate empirically that it outperforms GRAND and other GNN models, particularly for deep architectures and low-labeling rate scenarios.","['How does GRAND++ compare to very recent GNN architectures like GraphGPS or DeeperGCN?', 'What is the computational overhead of GRAND++ compared to GRAND?', 'How well does GRAND++ scale to very large graphs (millions of nodes)?', 'Can you provide more details on hyperparameter tuning, especially for the source term?', 'How do the theoretical guarantees translate to practical performance improvements?']","['Experiments are limited to node classification tasks', 'Scalability to very large graphs is not demonstrated', 'The method may not scale well to very large graphs due to the need to solve ODEs', 'Performance gains may be less significant on graphs with different structural properties']",False,3,3,3,8,4,"['Novel theoretical analysis showing how the source term prevents over-smoothing', 'Comprehensive experiments on multiple datasets demonstrating improvements', 'Addresses important issues in GNNs (over-smoothing, performance with limited labels)', 'Clear theoretical motivation and analysis', 'Random walk interpretation provides good intuition']","['Limited comparison to very recent GNN architectures', 'Scalability to very large graphs not demonstrated', 'Computational complexity and efficiency not thoroughly analyzed', 'Theoretical analysis could be more directly connected to empirical results']",4,3,3.0,4,Accept
DIjCrlsu6Z,"This paper introduces a method for identifying directions orthogonal to a given classifier, formally defining orthogonality for non-linear classifiers and providing a simple construction method called 'classifier orthogonalization'. The approach is theoretically grounded with formal proofs and demonstrated on important ML tasks including controlled style transfer, domain adaptation with label shifts, and fairness. The paper provides both theoretical analysis and empirical results across multiple applications, showing improvements over existing methods in most cases.","[""How sensitive is the method to the quality of the 'full classifier', and how might this impact real-world applications?"", 'Are there specific types of classification problems where the orthogonal classifier construction fails or performs poorly?', 'How does the method compare to state-of-the-art approaches specifically in controlled style transfer tasks?', 'Can the fairness application be extended to more complex scenarios with multiple sensitive attributes?']","['Reliance on availability of a good full classifier', 'May not be applicable to all types of classification problems', 'Potential challenges in scaling to very high-dimensional data', 'Limited exploration of failure cases or limitations of the approach', 'The impact on real-world applications might be limited by the need for a full classifier']",False,3,3,4,8,4,"['Novel theoretical framework for orthogonal classifiers in non-linear cases', 'Simple and effective method for constructing orthogonal classifiers', 'Strong theoretical grounding with formal proofs', 'Demonstrated applicability to multiple important ML tasks', 'Empirical results show improvements over baselines in most cases']","[""Requires access to a 'full classifier' which may limit practical applicability"", 'Some empirical improvements are only modest over baselines', 'Limited discussion of limitations or failure cases', 'Fairness application could be more comprehensive']",4,3,3.0,4,Accept
89W18gW0-6o,"This paper introduces FOCAL++, an enhanced algorithm for context-based offline meta-reinforcement learning (COMRL). The key contributions include incorporating intra-task attention mechanisms and inter-task contrastive learning to improve task representation learning. The authors provide theoretical analysis and extensive empirical evaluations demonstrating significant improvements over the baseline FOCAL algorithm and other methods, particularly in scenarios with sparse rewards and distribution shift.","['How does the proposed attention mechanism specifically address the challenges of sparse rewards in COMRL?', 'What are the main computational tradeoffs of implementing FOCAL++ compared to FOCAL in real-world applications?', 'How does the matrix-form momentum contrast objective contribute to improved task representation learning?']","['Experiments limited to continuous control tasks', 'Potential computational overhead of attention mechanisms not fully addressed', 'Reliance on pre-collected offline data may limit applicability in some real-world scenarios', 'Scalability to larger, more complex task distributions not explored']",False,4,3,4,8,4,"['Novel combination of attention mechanisms and contrastive learning for offline meta-RL', 'Strong theoretical analysis justifying the proposed methods', 'Consistent and significant empirical improvements over FOCAL and other baselines', 'Demonstrated robustness to sparse rewards and distribution shift', 'Clear writing and organization, with good motivation and connection to prior work']","['Experiments primarily focused on continuous control tasks', 'Limited discussion of potential limitations and negative impacts', 'Could benefit from more extensive ablation studies and comparisons to a wider range of baselines', 'Computational complexity of attention mechanisms not thoroughly analyzed']",3,4,4.0,4,Accept
Z0XiFAb_WDr,"This paper introduces LARC, a dataset of natural language instructions for solving ARC (Abstraction and Reasoning Corpus) tasks. The authors collect these instructions through a communication game, analyze them linguistically as 'natural programs', and compare them to traditional computer programs. They also conduct preliminary program synthesis experiments using these natural language descriptions. The key contributions are the novel LARC dataset, linguistic analysis of natural instructions versus formal programs, and insights into challenges for AI systems in interpreting natural language procedures.","['Can you provide a more detailed analysis of why the program synthesis approaches failed on many tasks?', 'How does your approach compare to state-of-the-art methods for solving ARC tasks?', 'What specific technical improvements do you propose to address the challenges identified in executing natural language as programs?', 'Could you include more quantitative analysis to support your linguistic observations?', 'What are your plans for future work or extensions of this research?']","['The program synthesis results are quite limited in success rate', 'The dataset is specific to ARC tasks and may not generalize to other domains', 'Potential biases in the dataset collection process are not thoroughly addressed', 'The broader implications for AI systems are somewhat speculative', 'Scaling this approach to more complex tasks or domains may be challenging']",False,3,3,3,6,4,"['Novel LARC dataset of natural language instructions for abstract reasoning tasks', 'Interesting linguistic analysis comparing natural language instructions to formal programs', 'Creative data collection methodology using a communication game', 'Highlights important challenges in bridging natural and formal languages for procedures']","['Limited success in program synthesis experiments (only 22/183 test tasks solved)', 'Lack of thorough quantitative analysis to support qualitative observations', 'Absence of concrete solutions proposed for identified challenges', 'Limited comparison to state-of-the-art methods for solving ARC tasks']",4,3,3.0,4,Accept
PilZY3omXV2,"This paper introduces CoST, a contrastive learning framework for learning disentangled seasonal-trend representations in time series forecasting. CoST uses separate trend and seasonal feature disentanglers with contrastive losses in time and frequency domains. Experiments show significant improvements over state-of-the-art baselines on multiple datasets.","['Can you provide a more rigorous causal justification for the approach?', 'How does the computational complexity of CoST scale with time series length and dimensionality?', 'How does the method perform on time series without clear seasonal-trend structure?']","['The approach may struggle with very high-dimensional or extremely long time series', 'Performance on datasets with complex or irregular seasonal patterns is not extensively explored', 'The method introduces additional hyperparameters that may require careful tuning', 'The disentanglement approach may not be suitable for all types of time series data']",False,3,3,4,7,4,"['Highly original approach to learning disentangled seasonal-trend representations', 'Impressive empirical results, consistently outperforming SOTA baselines', 'Thorough ablation studies and visualizations validating the approach', 'Well-motivated from a causal perspective', 'Demonstrates robustness across different backbone encoders and downstream regressors']","['Increased computational complexity compared to simpler approaches', 'Theoretical justification for the causal perspective could be strengthened', 'Limited analysis on computational efficiency and scalability', 'Some relevant baselines (e.g., N-BEATS, Prophet) not included in comparisons', 'Limited discussion on limitations and potential negative impacts']",4,3,3.0,4,Accept
lkQ7meEa-qv,"This paper introduces Neural Acoustic Fields (NAFs), a neural network approach for modeling acoustic properties of 3D scenes. Key contributions include a continuous representation of acoustic fields, local geometric conditioning for generalization, cross-modal learning benefits with visual models, and applications like sound source localization. Experiments show improvements over baselines on simulated data.","['How well would the method generalize to real-world acoustic data?', 'What are the computational requirements and potential for real-time performance?', 'How does the approach compare to advanced acoustic modeling techniques from audio literature?']","['Evaluation limited to synthetic scenes from one dataset', 'Potential scalability issues for large or complex environments', 'Reliance on precomputed impulse responses for training', 'Limited to static scenes', 'Potential privacy concerns with acoustic modeling in real environments']",False,3,3,3,7,4,"['Novel neural representation for acoustic fields', 'Effective local geometric conditioning for generalization', 'Demonstration of cross-modal benefits for visual reconstruction', 'Promising applications like sound source localization', 'Comprehensive experiments showing improvements over baselines']","['Evaluation limited to simulated data from a single dataset', 'Lack of real-world acoustic testing and validation', 'Insufficient discussion of computational requirements', 'Limited exploration of complex sound localization scenarios', 'Modest cross-modal learning gains']",3,3,3.0,3,Accept
kj0_45Y4r9i,"This paper introduces Clustering by Discriminative Similarity (CDS), a novel approach that formulates clustering as unsupervised classification. It derives a discriminative similarity measure based on the generalization error bound of an unsupervised classifier. The authors provide theoretical foundations, connections to kernel density classification, and propose the CDSK algorithm, which shows promising empirical results on several datasets.","['How does the method scale to very large datasets with millions of samples?', 'Can you provide more intuitive explanations or visualizations of the key ideas behind CDS?', 'How does CDSK compare to recent state-of-the-art clustering methods, including deep clustering approaches?', 'What are the key practical advantages of this approach over existing methods?', 'What are some potential real-world applications where CDS might be particularly effective?']","['High computational complexity limits scalability to very large datasets', 'Empirical evaluation is limited in scope and dataset size', 'Theoretical results may be difficult for practitioners to apply', 'Lack of discussion on potential negative impacts or ethical considerations']",False,3,3,3,6,4,"['Novel theoretical framework connecting clustering to classification', 'Rigorous mathematical analysis and derivation of discriminative similarity', 'Good empirical performance compared to some baseline methods', 'Theoretical connections to kernel density classification', 'Strong theoretical foundations and mathematical rigor']","['Complex theoretical analysis that may be challenging for practitioners to implement', 'Limited empirical evaluation, especially on large-scale datasets', 'High computational complexity that may affect scalability', 'Lack of comparison to recent state-of-the-art clustering methods', 'Insufficient discussion of practical advantages over existing approaches']",3,3,3.0,3,Accept
-3yxxvDis3L,"This paper analyzes the convergence of stochastic gradient descent (SGD) algorithms over dependent data samples, proving improved sample complexity for SGD with subsampling and mini-batch SGD compared to standard SGD. The authors use a φ-mixing model to characterize data dependence and develop novel proof techniques for mini-batch updates with dependent samples.","['Can you provide experimental results on real-world datasets or problems to validate the theoretical findings?', 'What are the key practical takeaways or recommendations for practitioners based on your results?', 'How do the results generalize to non-convex optimization problems?', 'How sensitive are the theoretical bounds to the specific φ-mixing model used? Would the results generalize to other dependence models?', 'What are potential extensions or future directions for this work?']","['Analysis is limited to convex optimization problems', 'Experimental validation is limited in scope', 'Practical implications could be elaborated further', 'Only considers φ-mixing model of data dependence']",False,4,3,3,7,4,"['Rigorous theoretical analysis of SGD convergence for dependent data', 'Proves improved sample complexity bounds for subsampling and mini-batch SGD', 'Develops novel proof techniques for mini-batch SGD with dependent samples', 'Addresses an important problem, as dependent data is common in practice', 'Results provide insights for algorithm design with dependent data']","['Limited experimental validation, only on simple synthetic problems', 'Lack of clear practical implications or guidelines', 'Analysis is limited to convex optimization problems', 'Clarity of presentation could be improved in some sections']",3,4,3.0,4,Accept
BkIV7EOXkSs,"This paper analyzes the implicit regularization effects of Bregman Proximal Point Algorithm (BPPA) and Mirror Descent (MD) for binary classification with linearly separable data. The main results show these algorithms obtain solutions with non-trivial margin lower bounds dependent on the condition number of the distance generating function. The paper provides theoretical analyses, demonstrates bound tightness, extends results to mirror descent, and proposes aggressive stepsize schemes for faster convergence.","['How might the theoretical results extend to non-separable data?', 'Can the authors provide more extensive empirical evaluation on real-world datasets and larger neural networks?', 'What are the practical implications of the theoretical results for algorithm design in deep learning?', 'How might these results be extended to non-linear models?']","['Results are limited to linear classification with separable data', 'Empirical evaluation on real datasets is limited', 'Practical implications for deep learning not fully explored']",False,4,3,4,7,4,"['Novel theoretical results on implicit regularization of BPPA and MD', 'Rigorous analysis showing tight margin lower bounds', 'Extension of results to mirror descent, complementing prior work', 'Proposal of aggressive stepsize schemes for faster convergence', 'Some empirical validation on synthetic and real data']","['Limited empirical evaluation, especially on real datasets and larger neural networks', 'Theoretical results restricted to linear classification with separable data', 'Connection to practical deep learning applications not fully developed']",4,4,3.0,4,Accept
yhCp5RcZD7,"This paper introduces Implicit Displacement Fields (IDF), a novel neural representation for detailed 3D geometry. IDF decomposes a shape into a smooth base surface and a high-frequency displacement field, inspired by classic displacement mapping. The method demonstrates superior detail reconstruction compared to state-of-the-art baselines while using a compact model. The authors also show transferable IDFs for detail transfer applications.","['How sensitive is the method to the choice of frequency parameters?', 'How well does the method perform on noisy, real-world scanned data?', 'Is there a way to avoid the need for pre-aligned shapes in the detail transfer application?', 'Could the inference time be improved to make it more suitable for real-time applications?', 'How does the method handle very large displacements or topological changes between the base and detailed surface?']","['The method requires pre-aligned shapes for detail transfer', 'Only evaluated on clean, high-quality 3D models', 'Slower inference time compared to some baselines', 'Potential challenges with very large displacements or topological changes', 'Computational requirements for inference not thoroughly analyzed']",False,4,3,3,8,4,"['Novel and well-motivated technical approach for representing detailed geometry', 'Impressive results in reconstructing fine geometric details compared to baselines', 'Compact model size while achieving high-quality results', 'Theoretically grounded formulation with proofs', 'Enables interesting applications like detail transfer between shapes', 'Thorough empirical evaluation and ablation studies']","['Requires pre-aligned shapes for detail transfer, limiting practical use', 'Choice of some hyperparameters seems heuristic', 'Evaluation limited to clean, high-quality 3D models', 'Slower inference time compared to some baselines', 'Could benefit from more extensive ablation studies']",4,4,4.0,3,Accept
AkJyAE46GA,"This paper investigates whether pretrained models are better active learners, capable of selecting informative examples to resolve task ambiguity. Experiments on image and text datasets with spurious correlations or imbalanced classes show that finetuning pretrained models with uncertainty sampling achieves the same accuracy with up to 6x fewer labels compared to random sampling. The paper demonstrates that this ability emerges from pretraining and does not work with unpretrained models.","['Can you provide any theoretical insights into why pretraining enables more effective active learning?', 'How well does this approach generalize to other types of tasks beyond classification?', 'How does your method compare to more sophisticated active learning techniques?', 'Do you expect similar gains on more complex real-world datasets with task ambiguity?', 'What are some potential real-world applications where this approach could be particularly beneficial?']","['Requires human-in-the-loop labeling which may be impractical for some applications', 'Only tested on a limited set of datasets/tasks', 'May not work well on domains very different from pretraining data', 'Assumes relatively noise-free labeling process', 'Computational cost of using large pretrained models']",False,3,4,3,7,4,"['Novel framing of task ambiguity as a unified problem across different types of datasets', 'Thorough experiments demonstrating significant gains from active learning with pretrained models', 'Insightful analysis of how pretraining enables better active learning', 'Clear writing and presentation of results', 'Addresses an important problem in few-shot learning settings', 'Proposes a simple yet effective active learning approach using pretrained models']","['Limited theoretical analysis of why pretraining enables this capability', 'Approach requires human-in-the-loop labeling, which could be costly/slow', 'Evaluation limited to a relatively small set of datasets/tasks', 'May not generalize well to domains very different from pretraining data', 'Lack of comparison to more sophisticated active learning techniques']",3,3,4.0,4,Accept
mfwdY3U_9ea,"This paper introduces IGEOOD, a novel out-of-distribution (OOD) detection method for neural networks based on the Fisher-Rao distance. The approach provides a unified framework applicable to both softmax outputs and internal network features, demonstrating flexibility across black-box, grey-box, and white-box settings. Extensive experiments show competitive or superior performance compared to state-of-the-art methods on various benchmarks, particularly excelling in white-box scenarios.","['Can you provide a more detailed analysis of the computational complexity of IGEOOD compared to existing methods?', 'How well would the method generalize to domains beyond image classification?', 'Could you offer more intuition or theoretical justification for why the Fisher-Rao distance is particularly effective for OOD detection?', 'How sensitive is the method to hyperparameter choices, and is there a principled way to select them?']","['Current evaluation limited to image classification tasks', 'Added complexity may pose challenges for practical adoption', 'Performance gains are less significant in more restricted access scenarios', 'Reliance on Gaussian assumptions for latent features which may not always hold']",False,3,3,3,7,4,"['Innovative use of Fisher-Rao distance and information geometry for OOD detection', 'Unified framework applicable to both softmax outputs and latent features', 'Flexibility to work with different levels of model access', 'State-of-the-art performance, especially in white-box settings', 'Comprehensive empirical evaluation across multiple datasets and architectures', 'Does not require OOD samples for training']","['Added complexity and computational overhead compared to simpler methods', 'More modest performance gains in black-box and grey-box settings', 'Reliance on Gaussian assumptions for latent features', 'Limited to image classification tasks in current experiments', 'Lack of thorough computational complexity analysis']",3,3,3.0,3,Accept
6hTObFz_nB,"This paper introduces 'latent shielding', a novel approach for safe reinforcement learning that learns a safety-aware latent world model to shield unsafe actions without requiring a handcrafted environment model. The method shows improved safety and performance on benchmark environments.","['How well would this approach scale to more complex, high-dimensional environments?', 'Is it possible to provide any theoretical guarantees on the safety of the learned latent shield?', 'How sensitive is the performance to the design of the shield introduction schedule?']","['Loss of formal safety guarantees provided by symbolic shielding methods', 'Evaluation limited to two relatively simple benchmark environments', 'Unclear generalization to more complex domains', 'Potential need for careful tuning of shield introduction schedules']",False,3,3,3,7,4,"['Novel combination of world models and safety shielding', 'Addresses limitation of requiring handcrafted models in previous approaches', 'Demonstrates clear safety improvements over baselines', 'Introduces useful concepts like shield introduction schedules', 'Works on both discrete and continuous action spaces']","['Loses formal safety guarantees of symbolic shielding approaches', 'Limited evaluation on only two relatively simple environments', 'Unclear scalability to more complex domains', 'Some aspects of the method (e.g., shield schedules) seem heuristic']",4,3,3.0,4,Accept
0d1mLPC2q2,"This paper investigates the interplay between knowledge distillation (KD) and data augmentation (DA). The authors observe that KD can exploit DA more effectively than standard cross-entropy training, especially with longer training. They propose a framework based on 'input view diversity' to explain this phenomenon and develop new DA techniques specifically for KD (TLmixup, TLCutMix, TLCutMix+pick). Using these techniques with the original KD loss, they achieve state-of-the-art results on image classification benchmarks including CIFAR-100, Tiny ImageNet, and ImageNet.","[""Can you provide stronger theoretical justification for the 'input view diversity' framework?"", ""Why doesn't the data picking technique work as well on ImageNet? Are there ways to improve it for larger datasets?"", 'Have you explored applying these techniques to other domains beyond image classification?', 'How does the computational cost of the proposed methods compare to standard KD?', 'What are potential applications of this work beyond improving KD performance?']","['Proposed techniques may not scale as well to very large datasets', 'Experiments are limited to image classification tasks', 'Potential negative impacts (e.g., increased training time/compute requirements) not thoroughly discussed', 'The proposed techniques may increase overall training time']",False,4,3,4,8,4,"['Novel insights into the interaction between KD and DA', 'Development of effective new DA techniques tailored for KD', 'Strong empirical results across multiple datasets and architectures', 'Achieves SOTA results using just the original KD loss with new DA techniques', 'Opens up new research directions in KD', 'Proposed methods are complementary to existing advanced KD techniques']","[""Limited theoretical justification for the proposed methods and 'input view diversity' framework"", 'Data picking scheme is less effective on larger datasets like ImageNet', 'Analysis is focused only on image classification tasks', 'Some proposed techniques (e.g., data picking) add computational overhead']",4,4,4.0,4,Accept
e-IkMkna5uJ,"This paper introduces novel methodologies to measure spectral bias in modern image classification neural networks, specifically using label smoothing and linear interpolation techniques. The authors apply these methods to study how various training choices (e.g., model size, regularization, data augmentation) affect the frequency content of learned functions. Key findings include that larger models learn high frequencies more readily, many forms of regularization inhibit learning of high frequencies, and there's a relationship between function complexity and generalization.","['How well do these results generalize to other datasets beyond CIFAR-10?', 'Can the methodologies be extended to other domains beyond image classification?', 'What theoretical explanations might account for the observed relationship between spectral bias and generalization?', 'How might these insights be used in practice to improve model training?']","['Experiments limited to CIFAR-10 classification task', 'Potential computational costs of proposed measurement techniques', 'Lack of analysis on very large models or datasets', 'Potential negative impact of enabling more effective surveillance systems']",False,3,4,3,7,4,"['Novel experimental methodologies to measure spectral bias in practical settings', 'Comprehensive experiments examining effects of various training choices', 'Insightful findings connecting spectral bias to generalization and robustness', 'Clear and well-organized presentation of results', 'Good connection to prior theoretical work on spectral bias']","['Experiments limited primarily to CIFAR-10 dataset', 'Limited theoretical analysis to complement empirical findings', 'Some results are empirical observations without deeper explanations', 'Scope of models tested is somewhat limited']",4,3,4.0,4,Accept
Zk3TwMJNj7,"This paper analyzes the directional bias of stochastic gradient descent (SGD) versus gradient descent (GD) in kernel regression. It shows that SGD converges towards the largest eigenvalue of the kernel gram matrix, while GD converges towards the smallest. This bias is linked to better generalization for SGD. The paper provides theoretical bounds and empirical validation on synthetic data and a small neural network.","['How sensitive are the results to the diagonal dominance assumption on the gram matrix?', 'Can the analysis be extended to more general loss functions or other types of nonparametric regression?', 'How well do the theoretical results extend to more complex neural network architectures and larger datasets?', 'Are there practical guidelines that can be derived from this work for tuning SGD in real applications?']","['The theoretical results rely on specific assumptions that may not hold in all practical scenarios', 'The analysis is limited to kernel regression and may not fully generalize to deep neural networks', 'The experiments are limited in scale and may not fully demonstrate applicability to state-of-the-art deep learning', 'The paper does not extensively discuss potential negative societal impacts']",False,3,3,3,7,4,"['Novel theoretical insights into directional bias of SGD vs GD in kernel regression', 'Rigorous mathematical analysis with detailed proofs', 'Demonstrates connection between directional bias and generalization performance', 'Empirical validation on both synthetic data and neural networks', ""Potential implications for understanding SGD's success in deep learning""]","['Assumptions like diagonal dominance of the gram matrix may be restrictive', 'Analysis is limited to kernel regression, extension to deep networks is speculative', 'Empirical results, especially for neural networks, are somewhat limited in scope', 'Some mathematical notation and proofs are dense and could be more clearly explained']",3,3,3.0,4,Accept
rrWeE9ZDw_,"This paper presents a method for autonomously learning object-centric, lifted representations suitable for planning in reinforcement learning tasks. The approach learns PDDL representations including object types, predicates, and operators directly from low-level state observations and options. These learned representations can be transferred between tasks with similar object types. The method is demonstrated on increasingly complex domains including Blocks World, a 2D crafting task, and Minecraft.","['How sensitive is the approach to the amount of exploration data and hyperparameters?', 'Could the method be extended to learn object individuation rather than assuming it?', 'How does performance compare to end-to-end deep RL approaches on the Minecraft tasks?', 'Are there ways to detect or correct errors in the learned representations?', 'What are potential real-world applications where this approach could be particularly beneficial?']","['Assumes objects can be individuated in state representation', 'Requires substantial exploration data', 'Some errors in learned representations', 'May not scale to arbitrarily complex domains or large numbers of object types']",False,3,3,4,7,4,"['Novel approach for learning abstract, transferable representations', 'Learns representations directly from raw pixel inputs', 'Demonstrates scalability to complex domains like Minecraft', 'Enables use of off-the-shelf task planners', 'Improves sample efficiency when transferring to new tasks', 'Thorough empirical evaluation including ablation studies', 'Strong theoretical foundations based on prior work in symbolic planning']","['Assumes objects can be individuated in state representation', 'Requires substantial exploration data to learn initial representations', 'Some errors in learned representations as shown in failure cases', 'Scalability to even more complex domains is not fully established']",4,3,4.0,4,Accept
DIsWHvtU7lF,"This paper introduces FINN (Finite volume Neural Network), a novel physics-aware neural network architecture for modeling spatiotemporal advection-diffusion processes. FINN implements a modular, compositional approach to combine neural networks with numerical simulation methods, incorporating physical knowledge while learning unknown components of partial differential equations. The method demonstrates superior accuracy and generalization ability compared to pure machine learning and other physics-aware models on several PDE benchmarks, especially on out-of-distribution data. FINN also shows promise on real-world experimental data and allows for the extraction of interpretable physical relationships.","['How might the approach be extended to handle higher-order spatial derivatives?', 'What are the main challenges in scaling FINN to much larger datasets or higher-dimensional problems?', 'Are there ways to optimize the GPU implementation to improve runtime performance?', 'What other potential applications or domains could benefit from the FINN approach?']","['Limited to first and second order spatial derivatives', 'Scalability to very large datasets or higher dimensions not yet demonstrated', 'Current GPU implementation may not be fully optimized for performance']",False,4,3,4,8,4,"['Novel modular architecture that effectively combines physical knowledge with data-driven learning', 'Excellent generalization ability to different initial and boundary conditions', 'Superior performance on multiple PDE benchmarks, often by orders of magnitude', 'Ability to extract interpretable physical relationships', 'Successful application to both synthetic and real-world experimental data', 'Comprehensive experiments, comparisons, and ablation studies']","['Currently limited to first and second order spatial derivatives', 'Potential scalability challenges for very large datasets or higher-dimensional problems', 'GPU implementation could benefit from further optimization']",4,4,4.0,4,Accept
Ivku4TZgEly,"This paper identifies limitations in Integrated Gradients (IG) and other attribution methods, particularly 'attribution transfer' in non-additive regions. The authors propose Integrated Certainty Gradients (ICG), a novel method that integrates over a 'certainty' dimension added during training. Theoretical analysis and experiments on synthetic scenarios demonstrate ICG's ability to avoid failure cases of existing methods.","['How well does ICG perform on more complex real-world datasets and models?', 'What is the computational overhead of ICG compared to other methods?', 'How sensitive is ICG to the specific training procedure used for certainty awareness?', 'Are there ways to apply ICG to pre-trained models without the special training process?', 'How does ICG compare to other recent attribution methods not covered in the paper?', 'Are there potential applications of ICG beyond attribution tasks?']","['Evaluation is limited to synthetic scenarios and simple datasets', 'The ICG method requires special model training which may limit its practical applicability', 'Potential negative impacts of misattribution are not thoroughly discussed', 'Computational costs of the method are not thoroughly analyzed']",False,3,3,3,7,4,"[""Thorough theoretical analysis of IG's limitations, tracing issues to axiomatic foundations"", 'Proposes a novel attribution method (ICG) with sound theoretical motivation', 'Clearly demonstrates failure cases for existing methods on synthetic scenarios', 'Well-designed experiments to illustrate failure modes', 'Clear explanations and visualizations of key concepts']","['Limited evaluation on real-world datasets and complex models', 'ICG requires special training, potentially limiting its practical applicability', 'Lack of comparison to some recent attribution methods', 'Computational cost of ICG not thoroughly analyzed', ""Some claims about ICG's performance could benefit from more extensive empirical support""]",3,3,3.0,3,Accept
rdBuE6EigGl,"This paper introduces a 'dual' architecture for recurrent neural networks in language modeling tasks, adding a direct connection from input to output that bypasses the recurrent layer. The authors demonstrate consistent improvements over standard architectures across different models and datasets, achieving state-of-the-art results on the Penn Treebank dataset.","['Can you provide theoretical insights or intuition for why the dual connection improves performance?', 'How well does the dual architecture scale to much larger language models and datasets?', 'Have you considered applying this approach to other sequence modeling tasks beyond language modeling?', 'How does the dual architecture compare to transformer models on these tasks?', ""Are there any potential drawbacks or limitations to the dual architecture that you've observed?""]","['Experiments focused on moderate-sized language modeling datasets', 'No experiments on other sequence modeling tasks beyond language modeling', 'Potential increased computational cost not thoroughly analyzed']",False,3,4,3,7,4,"['Novel and simple architectural modification that consistently improves performance', 'Thorough experiments comparing dual vs non-dual architectures across different models and datasets', 'Achieves state-of-the-art results on Penn Treebank', 'Well-written and clearly explained', 'Comprehensive comparison to state-of-the-art methods and rigorous experimental setup']","['Limited theoretical analysis or intuition for why the dual connection improves performance', 'Experiments limited to word-level language modeling on moderate-sized datasets', 'No comparison to very large language models or transformers', 'Hyperparameter tuning process not fully described']",3,4,4.0,3,Accept
q2ZaVU6bEsT,"This paper introduces a new feature pyramid network for tiny object detection, combining a context augmentation module, feature refinement module, and a novel data augmentation technique. Experiments on PASCAL VOC demonstrate improvements over baseline methods for tiny object detection.","['How does the method perform on more recent and challenging datasets like COCO or Open Images?', 'What is the computational overhead of the proposed modules compared to the YOLO v3 baseline?', 'How does the method compare to recent tiny object detection approaches like FCOS or EfficientDet?', 'Have you investigated potential biases introduced by the copy-reduce-paste augmentation?']","['Evaluation limited to a single, relatively simple dataset (PASCAL VOC)', 'Lack of comparison to very recent state-of-the-art methods', 'No analysis of inference speed or model complexity trade-offs', 'Limited theoretical justification for the effectiveness of proposed modules']",False,3,2,3,6,4,"['Addresses the important and practical problem of tiny object detection', 'Proposes a novel combination of techniques (CAM, FRM, data augmentation)', 'Shows clear performance improvements on tiny object detection metrics', 'Includes ablation studies to validate different components', 'Provides visualizations to illustrate the effects of the proposed modules']","['Evaluation is limited to only the PASCAL VOC dataset', 'Comparisons are mostly to older baseline methods rather than recent state-of-the-art', 'Lack of analysis on computational cost and efficiency', 'Writing and presentation could be improved in places', 'Individual techniques are not particularly novel, though their combination is']",3,3,2.0,3,Accept
V7eSbSAz-O8,"This paper proposes a framework for benchmarking the robustness of machine learning models in classifying COVID-19 spike protein sequences. The authors introduce methods to perturb sequences mimicking sequencing errors, compare various ML and DL approaches, and evaluate their accuracy and robustness. Key findings suggest that deep learning approaches and k-mer representations are generally more robust to introduced errors.","['Can you provide more details on the deep learning architecture used?', 'How were hyperparameters selected for the various models?', 'What is the justification for the specific error rates chosen in the experiments?', 'How does the proposed method compare to existing robustness evaluation frameworks?']","['Evaluation is limited to a single dataset of SARS-CoV-2 spike sequences', 'Only considers a subset of possible ML/DL approaches', 'Limited exploration of different error types and rates', 'Limited theoretical analysis of why certain approaches are more robust']",False,3,2,3,5,4,"['Addresses an important and timely problem in COVID-19 sequence analysis', 'Proposes a novel benchmarking framework for evaluating model robustness', 'Compares multiple ML and DL approaches and representation methods', 'Large-scale evaluation on real COVID-19 sequence data', 'Demonstrates potential advantages of deep learning and k-mer representations']","['Methodology and experiments lack sufficient detail in some areas', 'Evaluation is somewhat limited, focusing mainly on accuracy/robustness metrics', 'Theoretical justification and analysis is relatively thin', 'Writing and presentation could be improved in several sections', 'Limited discussion of limitations and potential negative impacts', 'Lacks comparison to state-of-the-art methods in viral sequence classification']",3,2,2.0,3,Reject
GlN8MUkciwi,"This paper introduces a novel method for incorporating user comments into video-text retrieval models using a Context Adapter Module. The approach significantly improves representation learning and achieves state-of-the-art zero-shot retrieval results on standard benchmarks, even when comments are not available at test time. This work opens up new possibilities for leveraging auxiliary text data in multi-modal learning.","['How might the method perform on more diverse datasets beyond Reddit?', 'What safeguards could be put in place to prevent misuse of models trained on user data?', 'How does the computational cost scale with number of comments?', 'How does the method compare to other approaches that use comments?', 'Are there any legal/ethical concerns with using Reddit comments?', 'How well does the method generalize to other types of auxiliary text data?']","['Dataset biases from Reddit user demographics', 'Potential privacy concerns with using user comments', 'High computational requirements for training full video model', 'Method only tested on English language data', 'Scalability to very large comment volumes not explored']",True,4,4,4,7,4,"['Highly novel approach to leveraging noisy user comment data for video-text retrieval', 'Context Adapter Module effectively filters relevant information from comments', 'Improves video-text representations even when comments not available at test time', 'Achieves state-of-the-art zero-shot retrieval results on standard benchmarks', 'Thorough experiments and ablations validating design choices', 'Well-written paper with clear explanations and potential impact on future multi-modal research']","['Potential dataset biases from using Reddit data', 'Ethical concerns around using user data without explicit consent', 'Limited discussion of potential misuse of the technology', 'Computational cost of training full video model is high', 'Lack of comparison to other comment-based methods', 'Limited analysis of failure cases']",4,4,4.0,4,Accept
1-YP2squpa7,"This paper introduces message passing algorithms based on belief propagation (BP) for training deep neural networks. The authors adapt BP and related techniques to work with mini-batches on GPUs, introduce a 'Posterior-as-Prior' update scheme, and demonstrate competitive performance with SGD on image classification tasks. Potential advantages are shown for sparse networks and continual learning.","['Can you provide more theoretical analysis of the convergence properties?', 'How does the computational efficiency compare to SGD as network depth/width increases?', 'Have you tested on networks with continuous weights and activations?', 'Can the approach be extended to other architectures like CNNs or Transformers?', 'Is there a theoretical explanation for when/why the method outperforms SGD, especially in continual learning settings?', 'How does the method perform on more challenging datasets like ImageNet?']","['Currently limited to MLPs and relatively small datasets', 'Lack of theoretical analysis of convergence or optimality', 'Computational overhead not fully characterized', 'Hyperparameter tuning process is complex', 'Potential difficulty in scaling to very large datasets or models']",False,3,3,3,7,4,"['Highly original approach to neural network training, fundamentally different from SGD', 'Competitive performance with SGD on standard benchmarks', 'Potential advantages demonstrated for sparse networks and continual learning', 'Thorough empirical evaluation across different network architectures', ""Novel 'Posterior-as-Prior' scheme for mini-batch training""]","['Limited theoretical analysis or convergence guarantees', 'Currently restricted to MLPs with binary weights/activations', 'Computational efficiency compared to SGD is unclear, especially for larger networks', 'Complex hyperparameter tuning process', 'Limited exploration of very deep networks or more complex architectures']",4,3,4.0,3,Accept
hfU7Ka5cfrC,"This paper presents a novel algorithm for optimizing hyperparameters, including optimizer hyperparameters like learning rates and momentum, in a single training pass. The method extends previous work to handle a broader class of hyperparameters while maintaining computational efficiency comparable to standard training. The authors provide theoretical motivation and demonstrate empirical effectiveness on several datasets and model architectures, achieving competitive performance compared to more computationally expensive methods.","['How does the method compare to Bayesian optimization approaches in terms of final performance and computational cost?', 'What causes the high-dimensional version to sometimes underperform the scalar version?', 'How sensitive is the method to the choice of meta-hyperparameters like update interval and look-back distance?', 'Is there a risk of overfitting to the validation set, especially with many hyperparameters?']","['Relies on approximations that may impact optimality', 'Limited to continuous hyperparameters', 'May not be suitable for very long training runs due to memory constraints', 'Potential for overfitting to validation set with many hyperparameters']",False,3,3,3,7,4,"['Extends previous methods to optimize optimizer hyperparameters like learning rates and momentum', 'Maintains computational efficiency comparable to standard training', 'Provides theoretical motivation for the approach', 'Demonstrates empirical effectiveness on a range of datasets and architectures', 'Achieves competitive performance compared to more computationally expensive methods']","['Relies on approximations that may impact optimality', 'Limited comparison to other hyperparameter optimization approaches like Bayesian optimization', 'Some results show high-dimensional version underperforming scalar version', 'Potential for overfitting to validation set, especially with many hyperparameters']",3,3,3.0,3,Accept
dLTXoSIcrik,"This paper identifies a novel overfitting issue in importance sampling-based offline RL, where policies artificially avoid low-reward initial states. The authors propose POELA, a method that constrains the policy to select only actions observed in nearby states. Theoretical analysis and empirical results on a tumor simulator and ICU sepsis data demonstrate improved performance over baselines.","['How sensitive is the method to the choice of δ and distance metric for defining eligible actions?', 'How does POELA compare to more recent offline RL methods like CQL?', 'Can the method be extended to continuous action spaces?', 'What is the computational complexity of POELA compared to baseline methods, especially as the state space grows?']","['Only tested on two domains, both in healthcare', 'May not scale well to very high-dimensional state spaces due to the nearest neighbor computations', 'Requires tuning of hyperparameters', 'Theoretical guarantees rely on assumptions that may not hold in all practical settings']",False,3,4,4,7,4,"['Identifies an important and previously overlooked overfitting issue in offline RL', 'Proposes an elegant solution (POELA) with theoretical justification', 'Strong empirical results on both simulated and real-world healthcare data', 'Addresses non-Markovian settings, which are important for many applications', 'Clear writing and motivation']","['Empirical evaluation is somewhat limited (only two domains, both in healthcare)', 'Could benefit from more ablation studies', 'Some hyperparameters like δ require tuning']",4,3,4.0,4,Accept
-dzXGe2FyW6,"This paper introduces Equalized Robustness (ER), a new fairness concept aimed at achieving fair model performance across different groups under distribution shifts. The authors propose a metric ΔER to quantify ER and develop a Curvature Matching (CUMA) algorithm to optimize for both in-distribution fairness and ER. Experiments on multiple datasets demonstrate that CUMA achieves better robust fairness than existing methods without sacrificing in-distribution fairness or accuracy.","['Can you provide stronger theoretical justification for why curvature matching should lead to improved ER?', 'How does CUMA perform on other data modalities beyond tabular and image data (e.g., text)?', 'What is the computational overhead of CUMA compared to standard training?', 'How sensitive is CUMA to the choice of hyperparameters?', 'What are some potential real-world applications or impacts of the proposed method?']","['Experiments are limited to a few datasets and primarily focus on tabular and image data', 'Theoretical justification could be strengthened', 'Potential tradeoffs and computational costs of optimizing for ER are not thoroughly explored']",False,3,3,4,7,4,"['Introduces an important new fairness concept (ER) that considers robustness to distribution shifts', 'Proposes a novel metric (ΔER) and algorithm (CUMA) to optimize for ER', 'Comprehensive experiments showing improved robust fairness across multiple datasets', 'Addresses a significant gap in current fairness approaches']","['Limited theoretical justification for using curvature matching to achieve ER', 'Experiments are primarily limited to tabular and image datasets', 'Lack of analysis on computational overhead and hyperparameter sensitivity', 'Limited comparison to other robust fairness methods']",4,3,4.0,4,Accept
m8uJvVgwRci,"This paper introduces Weak Indirect Supervision (WIS), a novel paradigm for creating training labels using indirect supervision sources with mismatched label spaces. The authors propose a probabilistic model called PLRM to leverage these sources, provide theoretical analysis including generalization bounds and distinguishability conditions, and demonstrate empirical advantages over baselines on academic datasets and an industrial application.","['How does the approach scale to very large label spaces with thousands of labels?', 'In practice, how difficult is it to obtain the required label relations for a new domain?', 'How sensitive is the method to violations of the conditional independence assumption?', ""Could the approach be extended to leverage partial label spaces that overlap but don't fully match?"", 'How well does the method generalize to tasks other than classification?']","['Scalability to very large label spaces is not thoroughly addressed', 'Requires label relations to be provided, which may be non-trivial for some domains', 'Assumes conditional independence between features and labels given weak labels', 'Potential sensitivity to errors in provided label relations', 'Computational complexity could be an issue for very large label spaces']",False,3,4,4,8,4,"['Novel problem formulation (WIS) that extends weak supervision to leverage sources with mismatched label spaces', 'Sound theoretical analysis including generalization bounds and distinguishability conditions', 'Empirical results show consistent advantages over baselines on multiple datasets', 'Demonstrated applicability to a real-world industrial scenario', 'Theoretically-grounded probabilistic model (PLRM) with principled test for distinguishability of unseen labels']","['Scalability to very large label spaces is not thoroughly addressed', 'Experiments are somewhat limited in scope (mainly two academic datasets)', 'Requires label relations to be provided, which may be non-trivial in some domains', 'Some assumptions like conditional independence may not always hold in practice', 'Complexity of the method may pose challenges in certain applications']",4,3,4.0,3,Accept
ckZY7DGa7FQ,"This paper introduces Belief Fine-Tuning (BFT), a novel method for improving belief state modeling in partially observable environments by fine-tuning a pretrained belief model at inference time. BFT is evaluated on variants of the Hanabi card game, demonstrating improved belief accuracy and downstream search performance, including enabling approximate public belief search in large games where exact beliefs are intractable.","['How well does BFT generalize to domains beyond card games like Hanabi?', 'What is the computational overhead of BFT compared to using the pretrained belief model or exact belief computation?', 'How sensitive is BFT to hyperparameters like number of fine-tuning steps?', 'Could BFT be adapted to work in model-free settings without access to a dynamics model?', 'How does BFT compare to other approximate belief state methods on standard POMDP benchmarks?']","['Results are limited to variants of a single game (Hanabi)', 'Computational cost may limit applicability in real-time or resource-constrained settings', 'Lack of theoretical analysis or guarantees on performance improvement', 'May not scale well to very high-dimensional state/observation spaces', 'Requires access to a dynamics model for sampling']",False,3,3,4,8,4,"['Novel approach to improving belief state modeling through inference-time fine-tuning', 'Enables approximate public belief state search in large games where exact beliefs are intractable', 'Strong empirical results on Hanabi variants, showing improved belief accuracy and search performance', 'Well-motivated from an approximate dynamic programming perspective', 'Clear explanation and presentation of the method', 'Thorough ablation studies on various aspects of the method']","['Evaluation limited to Hanabi variants - generalizability to other domains unclear', 'Computational cost of fine-tuning at each time step could be prohibitive in many applications', 'Lack of theoretical analysis or guarantees', 'Limited comparison to other approximate belief state methods', 'Scalability to larger or more complex domains not thoroughly explored']",4,3,3.0,4,Accept
e6MWIbNeW1,"This paper introduces an inductive graph partitioning (IGP) framework to address the NP-hard challenge of graph partitioning across multiple associated graphs. Key contributions include: (1) a dual GNN structure for incorporating permutation-invariant label information, (2) an efficient feature extraction module for graphs with varying node counts, and (3) an inductive embedding scheme for handling varying cluster numbers. The method is trained offline on historical graphs and can be quickly applied to new graphs. Experimental results on synthetic and real datasets demonstrate that IGP achieves competitive or better quality compared to state-of-the-art methods while maintaining efficiency close to fast heuristic approaches.","['Can you provide any theoretical analysis or bounds on the performance of IGP?', 'How well does the method scale to very large graphs with millions of nodes?', 'Have you explored alternatives to k-means to further reduce the runtime?', 'How sensitive is the method to differences between training and test graph distributions?']","The method relies on having historical graphs from the same distribution as test graphs, which may limit its applicability in some scenarios. The runtime is still dominated by the k-means clustering step, which may hinder scalability to extremely large graphs. There is a lack of theoretical analysis providing quality guarantees for the partitioning results. The performance may degrade when applied to graphs that differ significantly from the training distribution.",False,3,3,4,7,4,"['Novel inductive approach to graph partitioning leveraging historical data', 'Dual GNN structure effectively incorporates permutation-invariant label information', 'Ability to handle graphs with variable numbers of nodes and clusters', 'Comprehensive experimental evaluation showing competitive quality and efficiency', 'Thorough ablation studies validating key components of the method']","['Lack of theoretical analysis or quality guarantees', 'Runtime still dominated by k-means clustering step, limiting scalability to very large graphs', 'Potential performance degradation when test graphs differ significantly from training graphs', 'Complex method with multiple components, which may affect reproducibility']",4,3,3.0,4,Accept
QNW1OrjynpT,"This paper introduces a novel paradigm to investigate short-term memory in transformer and LSTM language models. The authors test the models' ability to retrieve lists of nouns, varying factors like list length, semantic coherence, and intervening text. Key findings show that transformers demonstrate robust and precise memory retrieval, maintaining word identity and order over long distances, while LSTMs show limited, gist-like memory that decays quickly.","['How might these findings generalize to larger, more recent language models?', 'What are the implications of these memory differences for downstream language understanding tasks?', 'How do the memory retrieval capabilities compare to human short-term memory?', 'Could you provide more theoretical insight into why transformers exhibit such different memory properties compared to LSTMs?', 'How might your paradigm be extended to test retrieval of other types of linguistic information beyond noun lists?', 'What potential applications do you envision for this research in improving language model design or performance?']","['Study focused only on English language models', 'Limited to retrieval of noun lists, may not generalize to other linguistic phenomena', 'Does not provide mechanistic explanation for observed memory differences', 'Lack of direct comparison to human memory capabilities', 'Relatively small model sizes tested compared to state-of-the-art']",False,3,4,3,7,4,"['Well-designed and novel experimental paradigm for probing language model memory', 'Thorough evaluation across different model architectures, sizes, and training regimes', 'Clear and interpretable results showing significant differences between transformer and LSTM memory capabilities', 'Careful controls and ablations to isolate key factors', 'Clear writing and presentation of methods and findings']","['Limited to English language models and noun list recall, potentially limiting generalizability', 'Lack of testing on larger, more recent language models', 'Limited theoretical framing and explanation for observed memory differences', 'Insufficient discussion of broader implications for language understanding and downstream NLP tasks', 'Some concerns about the fairness of LSTM comparison due to architectural differences']",3,3,4.0,4,Accept
R332S76RjxS,"This paper presents a convergence analysis for training implicit neural networks with ReLU activations using gradient descent in the overparameterized regime. The main results show that with network width Ω(n^2), where n is the number of training samples, gradient descent converges linearly to a global minimum for both continuous and discrete time updates. The analysis addresses challenges specific to implicit networks, such as infinite depth and well-posedness of the equilibrium equation throughout training. The paper provides theoretical guarantees and supports them with experimental results on real datasets.","['Can the results be extended to multi-class classification or regression tasks?', 'Is it possible to relax the width requirement while maintaining convergence guarantees?', 'How sensitive are the results to the initialization scheme and the assumption of non-parallel data points?', 'Can similar guarantees be shown for stochastic gradient descent?']","['Results are limited to binary classification with square loss', 'Very large network widths may be impractical for real-world applications', 'Analysis does not cover more practical training scenarios like SGD', 'No analysis of generalization or sample complexity']",False,4,3,3,7,4,"['Novel theoretical results for implicit neural networks with ReLU activations', 'Rigorous mathematical analysis with detailed proofs', 'Extends convergence guarantees to networks with infinite depth', 'Proves well-posedness of equilibrium equation during training', 'Experimental results on real datasets support theoretical claims']","['Limited to binary classification tasks with square loss', 'Large overparameterization requirement (Ω(n^2) width) may limit practical applicability', 'Assumes random Gaussian initialization and non-parallel data points', 'Limited discussion of practical implications and generalization']",3,4,3.0,3,Accept
V3C8p78sDa,"This paper presents a large-scale empirical study on the limits of transfer learning in computer vision, analyzing over 4800 experiments across different model architectures, sizes, and training setups. Key findings include: 1) downstream task performance saturates as upstream performance improves, 2) saturation points vary across tasks, 3) model/data/compute size mainly impact downstream performance through upstream performance, 4) saturation correlates with usefulness of layer-wise features, and 5) upstream and downstream performance can sometimes be at odds.","['How did you ensure the robustness of your findings across different model architectures?', 'Can you elaborate on the methodology used to determine the saturation points for different tasks?', 'What specific implications do your findings have for the design of future transfer learning experiments?', 'How might the observed saturation behavior impact the development of more efficient transfer learning techniques?']","['Results limited to image classification tasks', 'Lack of theoretical analysis to explain empirical findings', 'Focused on supervised pretraining, may not apply to self-supervised approaches', 'Environmental impact of large-scale experiments not discussed', 'Limited exploration of potential solutions to overcome observed saturation']",False,4,3,4,8,4,"['Comprehensive large-scale study with over 4800 experiments', 'Careful analysis of multiple factors affecting transfer learning', 'Novel insights challenging common assumptions about scaling laws', 'Important implications for transfer learning research and practice', 'Rigorous empirical evidence supporting conclusions']","['Focused only on image classification tasks, limiting generalizability', 'Limited theoretical analysis to explain empirical observations', 'Does not propose solutions to overcome observed limitations']",4,4,4.0,4,Accept
EVqFdCB5PfV,"This paper introduces DOCHOPPER, a novel model for answering complex questions over long structured documents. It employs hierarchical attention to navigate document structure, updates queries in embedding space, and pre-computes document representations. The model achieves state-of-the-art results on multiple QA datasets while significantly improving inference speed.","['How does the model perform compared to flat attention baselines?', 'Can you provide visualizations of the learned attention patterns?', 'How sensitive is performance to the number of hops?', 'How does the model generalize to even longer documents (>10k tokens)?', 'What are the key limitations for different types of documents/questions?', 'Are there potential applications of DOCHOPPER beyond QA tasks?']","['May not generalize well to all types of structured data (e.g., tables)', 'Relies on well-structured documents with clear hierarchy', 'Performance on very long documents (>10k tokens) not evaluated', 'Potential to amplify biases in training data and pre-trained language models', 'Computational requirements for pre-computing document representations not fully discussed', 'Potential for misuse in generating misleading or biased answers if not properly constrained']",True,3,3,3,7,4,"['Novel approach combining hierarchical attention and efficient query updating', 'Strong empirical results on multiple challenging datasets', 'Significant gains in both accuracy and inference speed (3-10x faster than baselines)', 'Detailed ablation studies validating key components', 'Addresses an important problem of QA over long documents', 'Demonstrates versatility across different types of QA tasks']","['Underperforms on table-based QA (HybridQA dataset)', 'Limited analysis of learned attention patterns and navigation behavior', 'Missing comparisons to some strong baselines on certain datasets', 'Minimal discussion of limitations and ethical considerations', 'Some datasets are modified versions of existing benchmarks']",3,4,3.0,3,Accept
3ILxkQ7yElm,"This paper introduces a novel approach using implicit neural networks to learn continuous environment fields for navigation and trajectory planning. The method is applied to both 2D maze navigation and 3D human trajectory prediction in indoor scenes. It encodes reaching distances between points, allowing for efficient path planning and trajectory generation.","['How well does the method generalize to more complex, real-world 3D environments beyond the indoor scenes tested?', 'What is the computational complexity of the approach compared to baselines, especially for larger 3D scenes?', 'Could the approach be extended to generate human poses in addition to trajectories?', 'How sensitive is the performance to the choice of implicit network architecture?']","['Evaluation on 3D scenes and human trajectory prediction is somewhat limited', 'May not handle very dynamic environments well', 'Requires training separate implicit function per 3D scene, potentially limiting scalability', 'Does not model human pose generation, only fits given pose sequences to trajectories']",False,3,3,3,7,4,"['Novel use of implicit neural networks to learn continuous environment fields', 'Application to both 2D maze navigation and 3D human trajectory prediction tasks', 'Incorporation of affordances and physical constraints for realistic human trajectories', 'Competitive performance compared to relevant baselines', 'Efficient compared to traditional path planning algorithms, especially for larger environments']","['Limited evaluation on 3D human trajectory modeling and real-world 3D environments', 'Potential scalability issues for complex or large 3D environments', 'Lack of comparison to state-of-the-art 3D trajectory prediction methods', 'Reliance on pre-defined pose sequences for human trajectory alignment']",4,3,3.0,3,Accept
ccWaPGl9Hq,"This paper introduces a formal framework for deployment-efficient reinforcement learning (DE-RL), aiming to minimize the number of policy deployments while achieving near-optimal performance. The authors provide lower bounds on deployment complexity for linear MDPs and develop algorithms that nearly match these bounds for both deterministic and arbitrary policies. Key techniques include layer-by-layer exploration and novel covariance matrix estimation methods.","['How well might the proposed algorithms and bounds extend beyond linear MDPs to more general settings?', 'Are there plans for empirical evaluation to validate the theoretical results in practice?', 'How do the proposed algorithms compare computationally to standard RL methods?', 'Can you provide more intuition or examples of real-world scenarios where the reachability coefficient might be small or large?', 'What are some potential real-world applications where deployment-efficient RL would be particularly beneficial?']","['Results are limited to the linear MDP setting', 'Lack of empirical evaluation limits assessment of practical impact', 'Algorithms may be challenging to implement efficiently in practice', 'Reachability coefficient assumption may be restrictive in some settings']",False,4,3,4,8,4,"['Novel and important problem formulation for deployment-constrained RL', 'Rigorous theoretical analysis with tight lower and upper bounds', 'Development of algorithms that achieve near-optimal deployment efficiency', 'Introduction of novel techniques like layer-by-layer exploration and covariance estimation', 'Flexible framework that can be extended to other settings like safe RL']","['Focus limited to linear MDPs, potentially restricting applicability', 'Lack of empirical evaluation to demonstrate practical impact', 'Algorithms may be challenging to implement efficiently in practice', 'Dependence on reachability coefficient for arbitrary policy algorithm']",4,4,4.0,4,Accept
BlyXYc4wF2-,"This paper introduces MACPO and MAPPO-Lagrangian, two novel algorithms for safe multi-agent reinforcement learning with theoretical guarantees. The authors formulate the problem as a constrained Markov game and develop a new benchmark environment called Safe Multi-Agent MuJoCo. Empirical results show the proposed methods satisfy safety constraints while achieving competitive reward performance.","['How do MACPO and MAPPO-Lagrangian compare to state-of-the-art safe MARL algorithms in terms of performance and computational efficiency?', 'What is the largest number of agents the proposed methods have been tested on, and how does performance scale?', 'How do the proposed methods perform in more complex, real-world inspired environments beyond MuJoCo?', 'What is the sensitivity of the algorithms to different safety constraint thresholds?']","['Evaluation limited to MuJoCo environments', 'Scalability to large numbers of agents not thoroughly explored', 'Safety constraints are relatively simple (e.g., distance-based)', ""Limited comparison to other safe MARL methods beyond the authors' own""]",False,3,3,3,8,4,"['Novel formulation of safe MARL as a constrained Markov game', 'Development of two practical algorithms with theoretical guarantees', 'Introduction of a new benchmark environment (Safe Multi-Agent MuJoCo)', 'Strong empirical results showing satisfaction of safety constraints', 'Addresses an important and under-explored area in MARL']","['Limited comparison to other safe MARL methods', 'Evaluation primarily focused on MuJoCo environments', 'Lack of extensive exploration of scalability to larger numbers of agents', 'Sometimes underperforms unsafe baselines in terms of reward']",3,3,3.0,4,Accept
Dup_dDqkZC5,"This paper introduces Latent Variable Sequential Set Transformers (AutoBots), a novel architecture for modeling sequences of sets with applications to multi-agent trajectory prediction. Key innovations include a permutation-equivariant transformer-based encoder-decoder, discrete latent variables for capturing multimodal distributions, and learnable seed parameters for efficient one-shot prediction. The approach demonstrates strong performance on trajectory prediction benchmarks like nuScenes and Argoverse, while being computationally efficient.","['Can you provide more analysis of the learned latent space and how it captures different modes?', 'How sensitive is the approach to the number of latent modes?', 'How well does the approach scale to larger numbers of agents or longer sequences?', 'Are there any safety concerns or potential negative impacts when using this for autonomous driving applications?', 'How does the model perform on other datasets mentioned, such as TrajNet++ and Omniglot, compared to existing methods?']","['Approach assumes structured input of agent states, may not handle raw sensor data', 'Memory limitations may restrict number of agents that can be modeled', 'Does not address end-to-end learning from perception to prediction', 'Potential for model to learn and amplify biases in training data', 'Difficulty in interpreting model decisions, which is crucial for safety-critical applications']",False,4,3,4,7,4,"['Novel permutation-equivariant architecture for processing sequences of sets', 'Efficient one-shot trajectory generation using learnable seed parameters', 'Strong empirical results on nuScenes and other benchmarks', 'Thorough theoretical analysis of permutation equivariance', 'Demonstration of generality through application to other sequence-of-set tasks', 'Computational efficiency compared to prior work']","['Not state-of-the-art on all metrics and datasets (e.g., Argoverse)', 'Limited analysis of the learned latent space', 'Some ablation studies and comparisons to recent work could be more comprehensive', 'Limited discussion of limitations and potential negative societal impacts']",3,3,4.0,3,Accept
x4tkHYGpTdq,"This paper introduces DSEE (Dually Sparsity-Embedded Efficient Tuning), a novel framework for efficient fine-tuning of large pre-trained language models. DSEE combines sparsity-aware low-rank weight updates during fine-tuning with enforced sparsity in the final model weights. Experiments on BERT, GPT-2, and DeBERTa demonstrate that DSEE can achieve comparable performance to full fine-tuning while using <1% of trainable parameters and reducing inference FLOPs by ~35%.","['Can you provide more theoretical justification for why the sparsity-aware low-rank updates work well?', 'How might this approach generalize to other domains beyond NLP?', 'How sensitive is the method to hyperparameters like sparsity levels and low-rank dimensions?', 'Do you expect the approach to scale to even larger models like GPT-3? What challenges might arise?', 'Are there potential negative impacts on model performance or fairness due to the compression techniques used?']","['Evaluation limited to NLP tasks and models, potentially limiting generalizability', 'Lack of in-depth theoretical analysis to support the empirical findings', 'Minor performance drops observed in some cases compared to full fine-tuning', 'Potential scalability issues for extremely large models not fully addressed']",False,3,3,3,7,4,"['Novel combination of sparsity and low-rank techniques for both fine-tuning and model compression', 'Comprehensive experiments on multiple models and datasets', 'Significant reductions in trainable parameters and inference costs', 'Thorough ablation studies and analysis', 'Maintains competitive performance on downstream tasks', 'Jointly optimizes for parameter efficiency during fine-tuning and resource efficiency of the final model']","['Limited theoretical analysis or justification for the approach', 'Some results show minor performance drops compared to full fine-tuning', 'Experiments limited to NLP tasks and models', 'Missing comparisons to some other efficient fine-tuning methods']",4,3,3.0,3,Accept
Rupm2vTg1pe,"This paper introduces the Infinite Contextual Graph Markov Model (ICGMM), an extension of CGMM using Hierarchical Dirichlet Processes to automatically determine the number of latent states at each layer. The model is trained layer-wise in an unsupervised manner to learn graph representations. A faster approximation (ICGMMf) is also introduced to improve scalability. The model is evaluated on graph classification tasks, showing competitive performance with supervised methods.","['How does ICGMM compare to other unsupervised graph embedding methods?', 'Can the model be extended to incorporate edge features?', 'How does the method scale to very large graphs (millions of nodes)?', 'How does the model perform on node classification or link prediction tasks?']","['Scalability to very large graphs may be limited', 'Does not incorporate edge features', 'Evaluation limited to graph classification tasks', 'Approximate inference (ICGMMf) may impact model quality for larger graphs']",False,3,3,3,7,4,"['Novel combination of Bayesian nonparametrics and graph representation learning', 'Automatic selection of model complexity at each layer', 'Competitive performance with supervised methods on graph classification', 'Thorough theoretical development and empirical analysis', 'More compact graph embeddings compared to CGMM']","['Limited scalability for very large graphs', 'Evaluation focused only on graph classification tasks', 'Does not incorporate edge features', 'Limited comparison to other unsupervised graph embedding methods']",4,3,3.0,3,Accept
RB_2cor6d-w,"This paper introduces APSyn, a method for generating adversarial examples using sequences of small, localized patches. The approach formulates the attack as a program synthesis problem, using gradient estimation for discrete patch parameters and enumerative synthesis to find minimal patch sequences. Results show APSyn generates imperceptible attacks with low L0 distortion while maintaining similar L2 distortion and success rates across multiple datasets.","['Have you attempted to physically realize any of the generated attacks?', 'How does APSyn compare to other recent patch-based attacks in terms of imperceptibility and success rate?', 'Can you provide more concrete examples of how the generated programs improve explainability?', 'How does the computational cost and execution time scale with image size/complexity?', 'Could this approach be extended to generate universal adversarial patches or applied to other vision tasks?']","['Physical realizability not demonstrated, only hypothesized based on L0 distortion', 'Currently limited to image classification tasks', 'Computationally expensive, may not scale well to larger datasets or models', 'Potential to be used maliciously to create imperceptible physical attacks']",True,3,3,3,7,3,"['Novel formulation of adversarial attacks as a program synthesis problem', 'Clever optimization approach for discrete patch parameters', 'Achieves imperceptible attacks with very low L0 distortion', 'Thorough evaluation on multiple datasets', 'Potential for improved explainability and interpretability of attacks', 'Attacks appear to generalize better than standard methods']","['Physical realizability claimed but not demonstrated', 'Limited comparison to other patch-based attacks', 'Computationally expensive with long execution times', 'Potential negative impacts not thoroughly discussed', 'Scalability to larger/more complex images unclear']",3,3,3.0,3,Accept
0uZu36la_y4,"This paper introduces Class-Focused Online Learning (CFOL), a novel adversarial training method designed to improve worst-case performance across classes. CFOL uses an adaptive distribution over classes during training, framing it as a two-player game and employing online learning techniques. The authors provide theoretical convergence guarantees and demonstrate consistent empirical improvements in worst-case and tail class performance across multiple image classification datasets.","['How does CFOL perform on larger, more complex datasets beyond CIFAR and STL10?', 'Can the theoretical analysis be extended to relax some of the strong assumptions made?', 'How does CFOL compare to other methods specifically designed to improve worst-case performance in machine learning?', 'Are there any potential negative impacts or fairness implications of focusing on worst-case class performance?']","['Evaluation limited to image classification tasks', 'Potential trade-off between worst-case and average performance', 'Requires tuning of an additional hyperparameter', 'Theoretical guarantees rely on assumptions that may not always hold in practice']",False,3,3,3,7,4,"['Addresses an important issue in adversarial training (non-uniform performance across classes)', 'Proposes a novel and well-motivated approach with theoretical convergence guarantees', 'Demonstrates consistent empirical improvements across multiple datasets', 'Simple to implement and integrate into existing adversarial training pipelines', 'Comprehensive comparisons with relevant baselines']","['Evaluation limited to image classification tasks', 'Small decreases in average accuracy in some cases', 'Introduces an additional hyperparameter (adversarial step size) that requires tuning', 'Theoretical analysis makes some strong assumptions that may not fully hold in practice']",4,3,3.0,3,Accept
AAeMQz0x4nA,"This paper introduces ECAQ, a novel multi-agent reinforcement learning method that explicitly optimizes a credit assignment criterion among agents in addition to standard temporal difference learning. The approach demonstrates strong performance on challenging StarCraft II micromanagement tasks and provides interpretable credit assignment.","['How does ECAQ compare to very recent methods like QPLEX and WQMIX on the full suite of SMAC tasks?', 'Can you provide more concrete examples of how the learned credit assignment values are interpretable?', 'How well does ECAQ generalize to other multi-agent environments beyond StarCraft II?', 'How sensitive is performance to the hyperparameter eta that balances different loss terms?']","['Focus on fully cooperative settings may limit applicability to competitive or mixed scenarios', 'Use of deep neural networks limits interpretability of individual agent policies', 'May face ethical issues in scenarios requiring sacrificing certain agents for optimal team performance']",False,3,3,3,7,4,"['Novel formulation of explicit credit assignment for multi-agent Q-learning', 'Strong theoretical foundation with gradient-based solution for credit assignment', 'Impressive empirical results on challenging StarCraft II benchmarks', 'Thorough ablation studies demonstrating importance of key components', 'Clear explanation of methodology and motivation']","['Limited comparison to very recent SOTA MARL methods', 'Experiments focused mainly on StarCraft II environment', 'Some claims about interpretability could be better supported', 'Potential ethical concerns in scenarios requiring agent sacrifice']",3,3,4.0,3,Accept
PGGjnBiQ84G,"This paper introduces a novel method for learning surface parameterization of implicit neural representations, focusing on document image unwarping. The approach employs forward and backward MLPs to learn bijective mappings between 3D surface points and 2D texture coordinates, integrated with implicit differentiable rendering. The method achieves state-of-the-art results on document unwarping benchmarks and enables texture editing capabilities. It uses multi-view images as input and incorporates geometric constraints specific to document shapes.","['How might the approach be extended to handle more general 3D shapes beyond documents?', 'Are there ways to reduce the long per-scene training time?', 'How many input views are typically needed to get good results?', 'How well does the method work on real documents with more complex textures/patterns?', 'Could the method be adapted to work with fewer input views or even single images?']","['Current focus is only on document shapes', 'Long training time limits real-time applications', 'Requires multi-view images as input', 'Potential for misuse in document forgery/editing', 'May struggle with documents that have insufficient texture or complex deformations']",False,3,3,3,7,4,"['Novel approach for learning surface parameterization of implicit neural representations', 'Integration with implicit differentiable rendering for joint optimization of geometry and texture', 'State-of-the-art results on document unwarping benchmarks', 'Ability to perform texture editing as an additional application', 'Thorough experiments on both synthetic and real document data', 'Incorporation of domain-specific priors and constraints for document shapes']","['Long training time (18 hours per scene) limits real-time applications', 'Method is currently specific to document shapes, may not generalize easily to other objects', 'Requires multi-view images, which may not always be available', 'Limited discussion of broader applicability beyond document domain', 'Potential for misuse in document forgery/editing']",4,3,3.0,3,Accept
xDIvIqQ3DXD,"This paper provides a theoretical analysis of recurrent encoder-decoder architectures in the linear setting. Key contributions include: (1) A universal approximation theorem, (2) Demonstrating encoder-decoders generalize RNNs to time-inhomogeneous relationships, (3) Identifying a 'temporal product structure' suited for encoder-decoders, and (4) Deriving approximation rates based on model capacity and target properties.","[""How might the 'temporal product structure' finding guide the design of more efficient encoder-decoder architectures?"", 'Can you provide intuition on how the approximation rates might change in nonlinear settings?', 'How do your results on time-inhomogeneous relationships relate to specific real-world sequence modeling tasks?', 'How do the derived approximation rates compare to empirical performance on benchmark datasets?']","The analysis is limited to linear models, which may not fully capture the behavior of nonlinear architectures used in practice. Future work could explore extensions to nonlinear settings, connections to other sequence models like transformers, and more extensive empirical validation on real-world tasks. The potential negative societal impacts seem limited given the theoretical nature of the work.",False,4,3,3,7,4,"['Rigorous theoretical analysis of an important deep learning architecture', 'Novel insights into the approximation capabilities of encoder-decoders', ""Identification of the 'temporal product structure' as a key property"", 'Quantitative approximation rates relating model capacity to accuracy', 'Comparison to RNNs highlights advantages of encoder-decoders']","['Analysis restricted to the linear setting, limiting direct applicability to nonlinear models', 'Limited discussion of practical implications and empirical validation', 'Highly technical content may limit accessibility to broader ML audience']",4,4,3.0,4,Accept
NrB52z3eOTY,"This paper introduces KLoS, a novel uncertainty estimation method for evidential neural networks that measures the KL divergence between the model's predicted Dirichlet distribution and a class-wise prototype distribution. The authors also propose KLoSNet, an auxiliary network trained to predict refined KLoS values. Extensive experiments demonstrate that KLoSNet outperforms existing methods in simultaneously detecting misclassifications and out-of-distribution (OOD) samples across various datasets and architectures, without requiring OOD training data.","['Can you provide more theoretical insight into why KLoS outperforms other uncertainty measures?', 'What is the computational overhead of using KLoSNet compared to the baseline evidential model?', 'How well does the approach generalize to other types of data beyond image classification?', 'Have you explored performance on more realistic OOD scenarios beyond academic datasets?', 'What are some potential real-world applications where this method could be particularly beneficial?']","['Experiments limited to image classification tasks', 'Computational requirements not fully analyzed', 'May not generalize to all types of out-of-distribution data', 'Specific to evidential neural networks', ""Further theoretical analysis needed to fully understand method's advantages""]",False,4,4,4,8,4,"['Novel uncertainty measure combining first and second-order information', 'Does not require OOD training data, unlike many existing approaches', 'Strong theoretical motivation with connections to evidential training', 'Comprehensive experiments showing clear improvements over baselines', 'Effective on simultaneous detection of misclassifications and OOD samples', 'More robust to choice of OOD training data compared to existing methods']","['Limited theoretical analysis of why KLoS outperforms alternatives', 'Experiments focused mainly on image classification tasks', 'Additional computational overhead of KLoSNet not thoroughly analyzed', 'Performance on more challenging or real-world OOD scenarios not extensively explored']",4,4,4.0,4,Accept
rMbLORc8oS,"This paper introduces SemiRetro, a novel approach for retrosynthesis prediction that combines advantages of template-based and template-free methods. Key contributions include a directed relational graph attention network (DRGAT) for center identification, semi-templates for efficient synthon completion, and a unified framework. Evaluations on USPTO-50k demonstrate significant improvements over state-of-the-art methods in accuracy, scalability, and efficiency.","['Can you provide results on additional retrosynthesis datasets beyond USPTO-50k to demonstrate generalizability?', 'How does SemiRetro compare to recent methods like MEGAN and MHNreact in terms of efficiency and scalability?', 'Can you provide more detailed implementation specifics to aid reproducibility?', 'What are the main limitations or potential negative impacts of this approach in real-world applications?']","['Evaluation on only one dataset limits generalizability claims', 'Potential bias towards reaction types represented in USPTO-50k', 'Computational complexity and resource requirements not thoroughly analyzed', 'Potential impact on human expertise in retrosynthesis planning not discussed']",False,3,3,3,7,4,"['Novel semi-template approach reduces redundancy while preserving chemical knowledge', 'Significant performance improvements over state-of-the-art methods, especially for unknown reaction classes', 'Improved scalability and efficiency compared to full template methods', 'Introduction of DRGAT layer enhances center identification accuracy', 'Unified framework effectively combines center identification and synthon completion', 'Thorough experimental analysis and ablation studies']","['Evaluation limited to only one dataset (USPTO-50k)', 'Lack of comparison to some recent methods (e.g., MEGAN, MHNreact)', 'Some implementation details missing, affecting reproducibility', 'Limited discussion of potential limitations and negative societal impacts']",4,3,3.0,3,Accept
U-_89RnR8F,"This paper introduces Conceptual Counterfactual Explanations (CCE), a novel method for explaining mistakes made by image classification models using human-interpretable concepts. CCE combines concept activation vectors with counterfactual explanations to identify which high-level concepts, if added or removed, would correct a model's misclassification. The method is validated on controlled experiments with spurious correlations and real-world medical applications.","['How sensitive is the method to the quality and completeness of the concept bank?', 'How does CCE compare quantitatively to other explanation methods like LIME or SHAP on standard benchmarks?', 'Can CCE be extended to work with other types of data beyond images?', 'Have you conducted any user studies to evaluate how helpful domain experts find these explanations?', 'Could you provide more details on the optimization procedure and hyperparameter selection?']","['Requires a predefined concept bank, which may limit applicability in some domains', 'Evaluation on real-world applications, especially medical, could be more extensive', 'Potential for generating misleading explanations if concept bank is incomplete', 'Computational cost of generating explanations is not thoroughly discussed', 'Generalizability to more complex models and tasks is unclear']",False,3,3,3,7,4,"['Novel combination of concept-based and counterfactual explanations', 'Provides explanations in terms of high-level, interpretable concepts', 'Demonstrates ability to identify spurious correlations and biases', 'Validated on both controlled experiments and real-world medical applications', 'Does not require access to training data or model retraining']","['Relies on having a good concept bank, which may be challenging for some domains', 'Limited comparison to other explanation methods', 'Evaluation on medical applications is somewhat limited in scope', 'Lacks user studies to evaluate interpretability of explanations', 'Some technical details and hyperparameter choices need more discussion']",3,3,3.0,3,Accept
Qu_XudmGajz,"This paper introduces a novel approach for Variational Autoencoders (VAEs) by using a low-rank multivariate normal distribution for the observation space, replacing the typical pixel-wise independent distribution. This method captures spatial dependencies between pixels, enabling the generation of coherent samples and allowing for interactive editing capabilities. The approach demonstrates improvements over standard VAEs in terms of sample quality and FID scores on CELEBA and brain scan datasets.","['How does the method compare to more advanced VAE variants?', 'Can the stability issues be mitigated without the entropy constraint?', 'How does the computational cost scale with image size and rank?', 'How sensitive is the method to hyperparameters like the rank of the multivariate normal distribution?', 'What are potential applications of this method beyond image generation?']","['May not scale well to very high-dimensional or high-resolution data', 'Introduces additional computational overhead', 'Potential to amplify dataset biases in interactive editing', 'Evaluation limited to face and brain scan datasets']",False,3,3,3,7,4,"['Addresses a fundamental limitation of standard VAEs in a principled way', 'Provides clear theoretical motivation and implementation details', 'Demonstrates qualitative improvements in sample coherence and diversity', 'Enables novel capabilities like observation space interpolation and interactive editing', 'Shows quantitative improvements in FID scores over baseline VAEs']","['Introduces additional stability issues requiring careful mitigation', 'Evaluation limited to comparison with basic VAE baseline, not state-of-the-art models', 'Potential computational overhead not thoroughly analyzed', 'Some claims about latent space modeling not fully validated']",4,3,3.0,4,Accept
GrFix2vWsh4,"This paper provides a theoretical analysis of Cross-Entropy (CE) and Dice losses for semantic segmentation, revealing hidden label-marginal biases. The authors show that both losses decompose into label-marginal penalties and ground-truth matching terms. Based on this analysis, they propose a new loss combining CE with explicit L1 or KL regularization to control the label-marginal bias. Experiments on medical and natural image datasets demonstrate consistent improvements over existing methods.","['How sensitive is the proposed loss to the choice of hyperparameters?', 'How does the proposed method compare to other recent segmentation losses beyond CE and Dice variants?', 'Could the insights from this work be applied to other tasks beyond semantic segmentation?']","['The analysis is focused only on CE and Dice losses', 'Experiments are limited to two datasets, though they represent different domains', 'Potential increased computational cost of the proposed loss is not discussed']",False,4,3,4,8,4,"['Novel and insightful theoretical analysis of widely used segmentation losses', 'Principled formulation of a new loss to control label-marginal bias', 'Consistent empirical improvements on both medical and natural image datasets', 'Clear explanations for why Dice and CE perform differently across applications', 'Comprehensive experiments validating the theoretical insights']","['Improvements, especially on Cityscapes dataset, are relatively modest', 'Analysis is limited to CE and Dice losses, not considering other recent loss functions', 'Introduces an additional hyperparameter that needs tuning', 'Potential increased computational cost of the proposed loss is not discussed']",4,4,3.0,4,Accept
pzgENfIRBil,"This paper introduces Self-consistent Gradient-like Eigen Decomposition (SCGLED), a novel method for solving approximated Schrödinger equations using gradient-based eigendecomposition techniques from machine learning. SCGLED eliminates the need for domain-specific heuristics and demonstrates improved performance over traditional methods, particularly in initialization.","['How could the method be extended to other quantum chemistry theories beyond Hartree-Fock?', 'What strategies could be employed to improve the full solver performance to match or exceed advanced SCF techniques?', 'How sensitive is the method to the choice of hyperparameters, and are there ways to automatically tune them?', 'Could the approach be adapted to handle excited state calculations or open-shell systems?']","['Lack of theoretical guarantees may limit practical adoption', 'Performance on very large molecular systems not thoroughly evaluated', 'Currently limited to Hartree-Fock theory', 'Potential challenges in integrating the method into existing quantum chemistry software packages']",False,3,3,4,7,4,"['Novel application of machine learning techniques to quantum chemistry problems', 'Elimination of domain-specific heuristics for initialization', 'Strong empirical results showing improved performance over baselines', 'Potential for broader impact in scientific computing and computational chemistry']","['Lack of theoretical convergence guarantees', 'Limited evaluation beyond the W4-17 dataset', 'Full solver performance still lags behind advanced SCF acceleration techniques', 'Requires careful tuning of hyperparameters']",4,3,4.0,4,Accept
Mlwe37htstv,"This paper introduces two novel policy optimization algorithms for reinforcement learning: Wasserstein Policy Optimization (WPO) and Sinkhorn Policy Optimization (SPO). These algorithms use Wasserstein and Sinkhorn divergences, respectively, to define trust regions, which may lead to more robust policies compared to traditional KL-divergence based methods. The authors derive closed-form policy updates without restricting to parametric policy classes, provide theoretical guarantees including monotonic improvement for WPO, and demonstrate empirical improvements over TRPO and PPO on several benchmark tasks.","['How does the computational complexity of WPO/SPO scale with the size of the action space, and are there any techniques to mitigate potential issues?', 'Have you investigated the performance of WPO and SPO on tasks with high-dimensional continuous state and action spaces?', 'How sensitive are WPO and SPO to the choice of λ in more complex environments, and are there guidelines for selecting this parameter?', 'Is there potential to extend WPO and SPO to off-policy learning scenarios?']","['Evaluation limited to relatively simple benchmark tasks', 'Potential scalability issues for very large state/action spaces', 'Focused only on on-policy learning', 'Theoretical guarantees assume exact advantage estimation, which may not be realistic in practice', ""Further investigation needed on the algorithms' behavior in more complex, high-dimensional environments""]",False,3,3,4,7,4,"['Novel use of Wasserstein and Sinkhorn divergences for policy optimization, potentially leading to more robust policies', 'Rigorous theoretical analysis, including closed-form policy updates and monotonic improvement guarantees', 'Empirical results showing consistent improvements over TRPO and PPO', 'Comprehensive ablation studies and sensitivity analysis, providing insights into algorithm behavior']","['Limited evaluation on relatively simple environments', 'Potential computational complexity issues for large action spaces, particularly in computing optimal transport problems', 'Theoretical guarantees assume access to true advantage functions, which may not be realistic in practice', 'Sensitivity to hyperparameters not fully explored across a wide range of environments']",4,3,3.0,3,Accept
nKWjE4QF1hB,This paper introduces a novel approach to game solving by modifying the AlphaZero algorithm to prioritize solving speed rather than just winning. The authors propose a Proof Cost Network (PCN) that estimates the work required to solve game positions. Experiments on 15x15 Gomoku and 9x9 Killall-Go demonstrate that PCN outperforms standard AlphaZero networks when used with MCTS and DFPN solvers.,"['How does PCN compare to state-of-the-art specialized solving algorithms for these games?', 'Have you tested this approach on any other games beyond Gomoku and Killall-Go?', 'Can you provide more theoretical justification for why PCN should work better than AlphaZero for solving?', 'How might this method be adapted to solve games from a neutral standpoint rather than focusing on proving wins for a specific player?', 'What are the computational requirements of PCN compared to standard AlphaZero?']","['The approach is only tested on two specific games', 'The comparison is mainly against AlphaZero, not specialized solvers', 'The theoretical foundations could be strengthened', 'The method may not scale well to games with very large state spaces due to computational costs', 'Potential difficulty in applying to games with less clear-cut winning conditions']",False,3,3,3,7,4,"['Novel approach combining AlphaZero with game solving techniques', 'Clear improvement over AlphaZero baseline on solving tasks', 'Well-structured experiments on two different games', 'Potential broader impact beyond the specific games tested', 'Works with multiple search algorithms (MCTS and DFPN)']","['Limited to experiments on only two games', 'Lack of comparison to specialized solving algorithms', 'Theoretical justification could be stronger', 'Some implementation details are unclear']",4,3,3.0,4,Accept
H4EXaI6HR2,"This paper introduces a novel architecture called 'parametric network series' (PNS) for modeling value functions in power system optimization. The method uses a series of neural networks with time-dependent parametric weights to capture seasonal patterns. The approach is applied to and evaluated on the short-term optimization of Uruguay's power grid, demonstrating significant improvements in both solution quality and computation time compared to the current production system.","['How were the specific parametric functions for the network weights chosen?', 'What is the computational complexity of the proposed method compared to the classic approach?', 'How does the performance of the method change with different numbers of neural networks in the series?', ""Can you provide more details on the implementation of the 'sliding window' strategy mentioned in Section 1.3?""]","[""Evaluation limited to Uruguay's power system"", 'Lack of comparisons to state-of-the-art deep RL methods', 'Scalability to larger systems or longer time horizons not addressed']",False,3,2,3,7,4,"['Novel technical approach combining neural networks with explicit time-dependent parametric functions', 'Demonstrated on a real-world, complex power system optimization problem (Uruguay)', 'Significant improvements in both solution quality and computation time over current production system', 'Provides interpretability through learned time-dependent parameter functions', 'Addresses an important real-world problem in power systems']","['Limited empirical evaluation - only tested on one specific scenario and system', 'Lack of comparisons to other modern baseline methods (e.g., deep RL approaches)', 'Insufficient discussion of generalizability to other power systems or problem settings', 'Some important implementation details are unclear']",3,3,3.0,3,Accept
WQVouCWioh,"This paper introduces DARK, a novel framework for training deep generative models on synthetic protein sequences for de novo protein design. The key innovations include iterative training on synthetic optimized sequences, use of unconditional language models, and leveraging AlphaFold for structure prediction and refinement. By using synthetic data instead of natural sequences, DARK enables the generation of diverse sequences with confidently predicted stable structures, outperforming models trained on natural sequences.","['Have any of the designed sequences been experimentally validated?', 'How does DARK compare to state-of-the-art methods like RosettaDesign on standard benchmarks?', 'How well does the method scale to longer protein sequences and more complex design tasks?', 'How do you ensure the designs are not simply adversarial examples against AlphaFold?', 'What computational resources are required for training and using DARK models?']","['Current approach is limited to fixed-length sequences of 100 amino acids', 'Lack of experimental validation limits confidence in real-world applicability', 'Potential biases in the synthetic data generation process', 'Computational cost may limit accessibility for some researchers']",False,3,4,4,7,4,"['Highly original approach combining synthetic data generation and deep generative modeling', 'Clear improvements over baselines in generating diverse, stable designs', 'Thorough evaluation using multiple metrics, including AlphaFold predictions', 'Demonstration of practical applications in protein design tasks', ""Introduction of 'AlphaFold refinement' method"", 'Potential to enable development of new protein design methods']","['Lack of experimental validation of the designed proteins', 'Limited comparison to state-of-the-art protein design methods', 'Unclear scalability to larger proteins and more complex design tasks', 'Heavy reliance on computational predictions rather than experimental structures']",4,3,4.0,4,Accept
SgEhFeRyzEZ,"This paper provides a theoretical analysis of the Feedback Alignment (FA) algorithm for training deep linear neural networks. The main contributions include: (1) convergence guarantees and rates for FA in both continuous and discrete time settings, (2) analysis of implicit regularization phenomena, (3) proofs of convergence for discrete FA dynamics, and (4) proposed initialization schemes. The authors also present some experimental results comparing FA to gradient descent on a linear autoencoder task.","['How might the results extend to nonlinear networks?', 'What are the key practical implications of the theoretical findings?', 'Could you provide more intuition for why FA exhibits different regularization behaviors depending on initialization?', 'How do the convergence rates compare quantitatively to gradient descent?', 'Are there specific neural architectures or tasks where FA is likely to be particularly advantageous based on these results?']","['Analysis is limited to linear networks, which may not fully capture behavior in practical nonlinear settings', 'Empirical evaluation is limited in scope and does not test on real-world datasets or tasks', 'Potential negative impacts are not explicitly discussed, though this is less relevant for a theoretical paper']",False,4,3,3,7,4,"['Rigorous theoretical analysis with detailed proofs', 'Novel results on convergence and implicit regularization for FA', 'Analysis of both continuous and discrete dynamics', 'Convergence guarantees for arbitrarily deep linear networks', 'Proposed initialization schemes to avoid undesirable learning behaviors', 'Some experimental validation of theoretical results']","['Analysis limited to linear networks, reducing practical relevance', 'Empirical evaluation is fairly limited and does not include real-world datasets', 'Practical implications of theoretical results not extensively discussed', 'Could benefit from more intuitive explanations of key results']",3,4,3.0,3,Accept
WcZUevpX3H3,"This paper introduces FEDPNAS, a novel method for personalized neural architecture search in federated learning settings. Key contributions include a split architecture with shared base and personalized components, context-aware sampling of operations, and a personalized federated objective anticipating client fine-tuning. Empirical results demonstrate improvements over baselines on heterogeneous image classification tasks.","['How would the method scale to larger datasets and more clients in federated settings?', 'How does FEDPNAS compare to other state-of-the-art personalized federated learning methods?', 'What is the communication overhead of the approach in practical federated settings?']",The paper is limited by its focus on small-scale image classification tasks and lack of comparison to state-of-the-art personalized federated learning methods. Scalability to large numbers of clients and diverse task types remains unexplored.,False,3,3,3,6,4,"['Novel combination of ideas from federated learning, neural architecture search, and personalization', 'Well-motivated design addressing key challenges in federated personalization', 'Consistent empirical improvements over baselines on multiple datasets', 'Theoretical analysis providing justification for the approach', 'Demonstrates better transfer to unseen tasks']","['Limited experimental evaluation - only image classification on small datasets (MNIST, CIFAR-10)', 'Lack of comparison to state-of-the-art personalized federated learning methods', 'Scalability to larger federated settings with more clients not fully addressed', 'Increased computational complexity and potential communication overhead']",4,3,3.0,3,Accept
eSHBmLnD1s8,"This paper introduces Stochastic SubSampling (SSS), a novel two-stage method for selecting informative subsets of data for machine learning tasks. SSS formulates subset selection as a set-based problem, using a candidate selection stage followed by autoregressive subset selection. The method is evaluated on multiple tasks including image classification, reconstruction, and few-shot learning, demonstrating strong performance compared to baselines, especially at low sampling rates.","['How does the computational complexity scale for very large input sets?', 'Can you provide any theoretical guarantees for the optimality of the selected subset?', 'How well does SSS generalize to non-vision tasks and other domains?', 'How sensitive is the performance to hyperparameter choices in the two-stage selection process?', 'What are potential applications of SSS beyond the tasks demonstrated in the paper?']","['May not scale well to extremely large datasets', 'Performance on non-vision tasks and more complex real-world scenarios not fully explored', 'Potential for introducing or amplifying biases in the data through subsampling', 'Challenges in interpreting the selected subsets for complex tasks']",False,3,3,3,7,4,"['Novel set-based formulation allowing handling of multi-dimensional features', 'Effective two-stage selection process balancing efficiency and modeling of dependencies', 'Strong empirical results across multiple tasks and datasets', 'Ability to generalize to different subsampling rates with a single trained model', 'Comprehensive experiments and detailed ablation studies', 'Unified framework for both feature and instance selection tasks']","['Potential scalability issues for very large datasets due to computational complexity', 'Limited theoretical analysis or guarantees', 'Experiments primarily focus on image-based tasks', 'Added complexity may make implementation and tuning challenging']",4,3,3.0,3,Accept
1JN7MepVDFv,"This paper empirically investigates the relationship between multi-task learning and disentangled representations in neural networks. The authors conduct experiments on synthetic datasets to examine whether multi-task learning naturally encourages disentanglement and if disentangled representations improve multi-task performance. The study finds evidence that multi-task learning leads to more disentangled representations, but results on the benefits of disentanglement for multi-task performance are inconclusive.","['How might these results extend to real-world datasets and tasks?', 'Can you provide any theoretical justification for why multi-task learning should encourage disentanglement?', 'What are the key implications or takeaways from this work for researchers and practitioners?', 'What future work or extensions do you envision based on these findings?']","['Results are limited to synthetic datasets and may not generalize to real-world scenarios', 'Lack of conclusive results on practical benefits of disentanglement for multi-task learning', 'Absence of theoretical grounding for empirical findings', 'Experiments limited to a small number of tasks (10), which may affect generalizability']",False,3,3,2,4,4,"['Novel research question connecting multi-task learning and disentanglement', 'Careful experimental design using multiple datasets and metrics', 'Thorough empirical evaluation with ablation studies and visualizations', 'Clear presentation of results with informative visualizations']","['Experiments limited to synthetic datasets, unclear real-world applicability', 'Lack of theoretical analysis or justification for hypotheses', 'Inconclusive results on benefits of disentanglement for multi-task performance', 'Limited practical implications or clear takeaways for researchers and practitioners']",3,3,3.0,3,Reject
8rR8bIZnzMA,"This paper introduces Dynamic Graph Transformer (DGT), a novel Transformer-based approach for dynamic graph representation learning. Key contributions include: (1) A Transformer architecture with spatial-temporal encoding for dynamic graphs, (2) Two complementary self-supervised pre-training tasks with theoretical justification, (3) A temporal-union graph structure and sampling strategy for scalability, and (4) Strong empirical results on link prediction tasks across multiple datasets.","['How does the method scale to graphs with millions of nodes?', 'Can you provide more theoretical analysis on the expressiveness of the model compared to GNNs?', 'How does DGT perform on other graph learning tasks beyond link prediction?', 'What are potential real-world applications of DGT in domains other than those explored in the paper?', 'How does the computational complexity and runtime compare to baselines in practice?']","['Scalability to very large graphs not demonstrated', 'Evaluation focused primarily on link prediction task', 'Limited theoretical analysis of model capabilities', 'May not scale well to extremely dense graphs']",False,4,3,4,8,4,"['Novel application of Transformers to dynamic graphs with well-motivated design choices', 'Comprehensive empirical evaluation showing consistent improvements over SOTA baselines', 'Theoretically justified self-supervised pre-training approach', 'Thoughtful techniques to address scalability challenges', 'Thorough ablation studies validating different components of the model']","['Scalability to extremely large graphs (millions of nodes) not demonstrated', 'Evaluation primarily focused on link prediction task', 'Limited theoretical analysis of model capabilities/expressiveness', 'Complex model with many components may be difficult to implement/tune']",4,3,4.0,4,Accept
sOK-zS6WHB,"This paper introduces a novel method for fingerprinting generative models to enable responsible disclosure and attribution of generated content. The approach incorporates a fingerprint auto-encoder into the GAN architecture, allowing efficient generation of many distinct fingerprinted versions of a model. Comprehensive experiments demonstrate high fingerprint detection accuracy, minimal impact on generation quality, scalability, robustness, and effectiveness for practical applications like deepfake detection.","['What is the computational overhead of fingerprinting during inference?', 'How robust are the fingerprints to advanced adversarial removal techniques?', 'Can the method be extended to other types of generative models beyond GANs?', 'What are the practical challenges of deploying this system at scale and regulating its use?', 'Are there any legal or ethical implications of fingerprinting models that need to be considered?']","['May not be effective if model architecture is significantly modified', 'Potential for fingerprint collisions with a very large number of models', 'Requires cooperation from model creators to implement', 'May increase computational and storage costs for model deployment', 'Potential privacy concerns for model creators if fingerprints can be linked to individuals']",False,4,3,4,8,4,"['Novel and timely approach to an important problem in responsible AI', 'Technically sound method with strong experimental results', 'Achieves significant improvements in scalability and efficiency over prior work', 'Comprehensive evaluation demonstrating multiple desirable properties', 'Potential for real-world impact in areas like deepfake detection', 'Clear motivation and addressing a pressing need in AI ethics']","['Limited discussion of potential limitations or ways to circumvent fingerprinting', 'Computational overhead of fingerprinting during inference not clearly quantified', 'Generalizability to other types of generative models beyond GANs not fully explored', 'Some results focused on limited datasets (e.g., CelebA)']",4,4,4.0,4,Accept
7YDLgf9_zgm,"This paper introduces Recursive Gradient Optimization (RGO), a novel approach for continual learning that modifies gradients to minimize forgetting without data replay. Key components are an iteratively updated optimizer and a virtual Feature Encoding Layer. RGO achieves state-of-the-art results on several continual learning benchmarks, outperforming existing fixed-capacity methods and even single task learning in some cases.","['How does the method perform on more complex/realistic continual learning scenarios beyond standard benchmarks?', 'How sensitive is the performance to the choice of task descriptors and hyperparameters?', 'Have you evaluated RGO on any real-world continual learning applications, such as robotics or natural language processing tasks?', 'How well does the method scale to larger numbers of tasks (e.g. 100+) and very deep neural networks (e.g. Transformers)?']","['Relies on task descriptors which may not always be available', 'Assumes close vicinity of optimal parameters which may not always hold', 'Evaluation limited to standard benchmarks and relatively simple datasets/architectures', 'Potential scalability issues for very long task sequences not fully explored']",False,4,3,4,8,4,"['Novel and principled optimization formulation for continual learning', 'Strong theoretical foundation with proofs for key claims', 'Achieves state-of-the-art results on multiple benchmarks', 'Works with fixed network capacity, unlike expansion-based methods', 'Can be easily integrated into existing gradient-based training pipelines']","['Implementation complexity may pose challenges for practical adoption', 'Relies on task descriptors at test time, which may not always be available', 'Limited evaluation on more complex datasets/architectures beyond CIFAR100/ResNet18', 'Potential scalability issues for very large networks or long sequences of tasks']",4,4,3.0,4,Accept
WqoBaaPHS-,"This paper introduces top-label calibration as a new notion of multiclass calibration and proposes a multiclass-to-binary (M2B) reduction framework for achieving different types of multiclass calibration. The M2B framework is instantiated with histogram binning for top-label and class-wise calibration, and distribution-free calibration guarantees are proven. Empirical evaluations show the approach outperforms other methods on several calibration metrics.","['How does the approach scale to problems with hundreds or thousands of classes?', 'What is the computational complexity of the M2B + histogram binning method compared to other approaches?', 'How can the performance of top-label histogram binning be improved for datasets with many classes like CIFAR-100?', 'What are some specific applications or use cases where top-label calibration would be particularly beneficial compared to other calibration methods?']","['The approach may not scale well to extremely large numbers of classes', 'Computational requirements are not thoroughly analyzed', 'Performance depends on having sufficient calibration data for each class']",False,4,3,4,8,4,"['Novel concept of top-label calibration addressing limitations of existing approaches', 'Flexible M2B framework unifying different notions of multiclass calibration', 'Strong theoretical guarantees (distribution-free calibration bounds)', 'Impressive empirical results across multiple datasets and architectures', 'Thorough analysis and ablation studies', 'Comprehensive experimental evaluation on various datasets and deep network architectures']","['Performance of top-label histogram binning degrades on datasets with many classes (e.g., CIFAR-100)', 'Scalability to very large numbers of classes not thoroughly addressed', 'Limited discussion of computational complexity']",4,4,3.0,4,Accept
v-v1cpNNK_v,"This paper introduces NASI, a novel neural architecture search method that leverages the Neural Tangent Kernel (NTK) to estimate architecture performance at initialization without model training. By reformulating NAS as an optimization problem over the NTK trace norm and using efficient approximations, NASI achieves orders of magnitude faster search times while maintaining competitive performance on standard benchmarks like CIFAR and ImageNet. The method's label- and data-agnostic properties potentially enable more transferable architecture search.","['How does NASI perform on non-vision tasks, such as natural language processing or reinforcement learning?', 'What is the impact of network depth on the accuracy of the NTK approximations used in NASI?', 'How sensitive is NASI to the choice of hyperparameters, particularly the constraint ν and penalty coefficient μ?', 'Can you provide examples of scenarios where the label- and data-agnostic properties might not hold or might lead to suboptimal results?']","['Theoretical results rely on assumptions that may not always hold in practice', 'Current evaluation limited to image classification tasks', 'Potential scalability issues with extremely large search spaces', 'Increased computational complexity compared to simpler NAS approaches', 'Dependence on careful hyperparameter tuning']",False,3,3,4,8,4,"['Novel use of NTK theory to avoid training in NAS', 'Significant improvement in search efficiency (100x faster)', 'Competitive performance on standard benchmarks', 'Theoretical guarantees on label- and data-agnostic search', 'Sound theoretical analysis and derivations', 'Empirical validation of effectiveness across multiple search spaces and datasets']","['Relies on infinite width assumptions that may not hold in practice', 'Approximations used may impact search accuracy', 'Limited evaluation on more complex tasks/datasets beyond image classification', 'Gap between theoretical guarantees and practical finite-width networks']",4,3,3.0,4,Accept
rq1-7_lwisw,"This paper introduces the Object Concept Learning (OCL) task, which aims to advance object understanding beyond simple recognition by requiring models to infer object attributes, affordances, and their causal relationships. The authors create a large-scale dataset with 381 object categories, 114 attributes, and 170 affordances, along with dense annotations. They propose a baseline model called Object Concept Reasoning Network (OCRN) that incorporates causal reasoning principles.","['Can you provide more analysis on why performance is so low across all models, particularly for causal reasoning?', 'What specific steps were taken to mitigate potential biases in the dataset collection and annotation process?', ""How does OCRN's performance compare to human performance on the OCL task?"", 'Can you provide more detailed analysis of what causal relationships are actually captured by OCRN compared to baselines?']","['The overall low performance indicates the task may be too challenging for current models', 'The dataset may have inherent biases in the object categories, attributes, and affordances included', 'Causal annotations may have limitations or biases due to the complexity of real-world causality', 'Potential negative impacts of improved object understanding for surveillance or military applications are not thoroughly discussed']",False,3,3,3,6,4,"['Introduces an important new task (OCL) for deeper object understanding', 'Creates a valuable large-scale dataset with dense annotations for attributes, affordances, and causal relationships', 'Proposes a novel baseline model (OCRN) incorporating causal intervention and instantiation techniques', 'Evaluates on both the core OCL task and a downstream application (HOI detection)']","['Performance of the proposed OCRN model shows only modest improvements over baselines', 'Overall low performance across all models, especially on causal reasoning (TDE metric)', 'Limited analysis and ablations to understand model contributions and challenges', 'Evaluation methodology and metrics could be more rigorously justified']",3,3,3.0,3,Accept
HMJdXzbWKH,"This paper introduces Q-Rex and Q-RexDaRe, novel Q-learning algorithms that incorporate online target learning and reverse experience replay. The authors provide theoretical convergence guarantees and sample complexity bounds for these algorithms on linear MDPs and tabular MDPs. For tabular MDPs, Q-RexDaRe is shown to achieve near-optimal sample complexity. The paper focuses on theoretical analysis with limited empirical evaluation.","['How do Q-Rex and Q-RexDaRe perform empirically on standard RL benchmarks compared to other state-of-the-art algorithms?', 'What are the computational requirements of Q-Rex and Q-RexDaRe compared to standard Q-learning?', 'How sensitive are the algorithms to hyperparameter choices in practice?', 'Do the theoretical guarantees hold up in settings that violate some of the assumptions?']","['The algorithms are only analyzed for linear and tabular MDPs', 'Empirical evaluation is limited to a few toy problems', 'Practical challenges in implementing reverse experience replay are not discussed', 'Success of the proposed methods depends on carefully choosing parameters']",False,4,4,4,8,4,"['Novel algorithms (Q-Rex and Q-RexDaRe) with provable guarantees', 'Rigorous theoretical analysis with non-asymptotic bounds', 'Improved sample complexity over previous Q-learning methods', 'Addresses important open problems in reinforcement learning theory', 'Provides convergence guarantees for Q-learning with linear function approximation under standard assumptions', 'Potential to inspire new research directions in RL theory']","['Limited empirical evaluation', 'Practical applicability of the algorithms to complex problems is unclear', 'Assumptions may be restrictive for some real-world settings', 'Paper is very technical and may be difficult for practitioners to apply']",4,4,4.0,4,Accept
aPOpXlnV1T,"This paper identifies a problem with using negative log-likelihood (NLL) loss for training probabilistic neural networks, namely premature convergence to suboptimal fits. The authors analyze this issue and propose a new loss function called β-NLL that interpolates between NLL and MSE. Extensive experiments across various tasks and datasets demonstrate the effectiveness of β-NLL compared to standard NLL and other baselines.","['Is there a theoretical justification for why β=0.5 tends to work well in practice?', 'How sensitive is β-NLL to the choice of β? Is there a way to automatically select an optimal β?', 'Are there any cases where β-NLL performs worse than standard NLL? What are the limitations?', 'How does β-NLL compare to ensemble methods for uncertainty estimation?', 'Does the method extend to non-Gaussian likelihoods?', 'What are potential real-world applications where β-NLL could have significant impact?']","['The paper does not discuss potential negative societal impacts', 'Limited discussion of failure cases or limitations of β-NLL', 'The theoretical analysis could be expanded', 'Evaluation is limited to mostly small-scale tasks and datasets', 'Unclear how to optimally choose β', 'Potential increased computational cost due to hyperparameter tuning']",False,4,4,3,7,4,"['Identifies and analyzes an important issue in probabilistic deep learning', 'Provides insightful analysis of NLL loss shortcomings', 'Proposes a simple but effective solution (β-NLL) that is easy to implement', 'Thorough empirical evaluation on diverse tasks and datasets', 'Clear writing and insightful analysis', 'Practically useful contribution for improving probabilistic neural networks']","['Limited theoretical analysis and justification for β-NLL', 'Introduces a new hyperparameter (β) that requires tuning', 'Focused primarily on Gaussian distributions', 'Limited comparison to some uncertainty estimation methods (e.g., ensembles)', 'Some baselines achieve better likelihoods on certain tasks']",4,4,4.0,3,Accept
9Hrka5PA7LW,This paper introduces unsupervised continual learning (UCL) as an alternative to supervised continual learning (SCL). The authors demonstrate that UCL representations are more robust to catastrophic forgetting and generalize better to out-of-distribution tasks. They propose a new method called Lifelong Unsupervised Mixup (LUMP) that further improves UCL performance. Extensive experiments and analyses across multiple datasets show consistent improvements over state-of-the-art SCL methods.,"['How would the results scale to larger datasets like full ImageNet?', 'Can you provide more theoretical insight into why UCL is more robust to forgetting than SCL?', 'How does UCL compare to more recent supervised CL methods not included in the evaluation?', 'What is the computational cost of UCL compared to SCL methods?']","['Experiments are limited to relatively small-scale datasets', 'Theoretical understanding of UCL advantages is limited', 'Focused only on task-incremental setting, not class-incremental', 'Potential negative societal impacts not thoroughly discussed']",False,4,3,4,8,4,"['Novel application of unsupervised learning to continual learning', 'Comprehensive empirical evaluation showing UCL outperforms SCL on multiple datasets and metrics', 'Insightful analysis of learned representations using various visualization techniques', 'Proposal of LUMP method that further improves UCL performance', 'Strong performance in robustness, generalization, and few-shot learning scenarios']","['Experiments limited to relatively small datasets (CIFAR, Tiny ImageNet)', 'Limited theoretical analysis or explanation for why UCL outperforms SCL', 'Comparison to some recent supervised CL methods is missing', 'Computational cost of UCL methods not discussed', 'Potential negative societal impacts not thoroughly addressed']",4,4,3.0,4,Accept
b30Yre8MzuN,"This paper introduces NEUROSED, a novel neural approach for learning and predicting subgraph edit distance (SED) and graph edit distance (GED). Key contributions include a siamese GNN architecture that preserves theoretical properties, pair-independent embeddings for fast retrieval, and extensive experiments demonstrating superior performance over state-of-the-art baselines. The method shows significant improvements in both accuracy and efficiency on multiple real graph datasets.","['How does the approach scale to even larger graphs beyond those tested (e.g., billions of nodes)?', 'Are there ways to incorporate alignment information while maintaining efficiency?', 'How sensitive is the method to hyperparameter choices?', 'Could the approach be extended to other graph similarity measures?', 'Is it possible to reduce reliance on expensive ground truth computation for training?', 'Could transfer learning or pre-training techniques help address the issue with large unseen query graphs?']","['Performance degradation on very large unseen query graphs', 'Lack of alignment information', 'Potential sensitivity to hyperparameters (not thoroughly analyzed)', 'Limited to SED and GED measures currently', 'Scalability to very large graphs (billions of nodes) not fully explored', 'Reliance on expensive ground truth computation for training']",False,4,3,4,8,4,"['Novel formulation of learning SED and GED using neural networks', 'Carefully designed architecture preserving theoretical properties of SED/GED', 'Pair-independent embeddings enabling fast retrieval and inference', 'Comprehensive experiments showing significant improvements over state-of-the-art', 'Sound theoretical analysis and proofs', 'Addresses an important problem in graph similarity search', 'Scales to graphs with millions of nodes', 'Effective learning from low-volume training data']","['Performance degrades on very large query graphs not seen during training', 'Lack of alignment information (though justified)', 'Limited comparison to some recent GED learning approaches (e.g., GENN-A*)', 'Reliance on expensive ground truth SED/GED computation for training data', 'Limited analysis of the learned embeddings']",4,4,4.0,4,Accept
NH29920YEmj,"This paper introduces P3Mix, a novel positive-unlabeled (PU) learning method that employs a heuristic mixup technique to simultaneously achieve data augmentation and supervision correction. The key idea is to address the 'decision boundary deviation' phenomenon by mixing marginal pseudo-negative samples with positive samples near the decision boundary. Comprehensive experiments demonstrate that P3Mix consistently outperforms state-of-the-art PU learning methods on benchmark datasets.","['Can you provide more theoretical justification or analysis for why the heuristic mixup technique works?', 'Have you tested the method on non-image datasets or other domains?', 'How sensitive is the method to the choice of hyperparameters, and are there ways to reduce this sensitivity?', 'Could this approach be extended to multi-class classification or other weakly supervised learning settings?', 'Are there any potential negative societal impacts or ethical concerns related to this method?']","['Only evaluated on image classification tasks', 'Requires tuning of additional hyperparameters', 'Lack of theoretical guarantees', 'May not generalize well to other domains or task types', 'Computational complexity compared to existing approaches not discussed']",False,3,3,3,7,4,"['Novel heuristic mixup technique specifically designed for PU learning', 'Consistent and significant performance improvements over SOTA baselines', 'Thorough ablation studies and sensitivity analyses', 'Clear motivation and intuitive explanation of the approach', 'Comprehensive comparison with existing PU learning methods']","['Limited theoretical justification for the effectiveness of the heuristic mixup', 'Experiments are confined to image classification tasks', 'Introduction of additional hyperparameters that require tuning', 'Lack of theoretical guarantees']",4,3,3.0,3,Accept
MqEcDNQwOSA,"This paper introduces k-sub-embedding, a novel embedding compression technique for language models. The method reconstructs full embedding vectors from a smaller set of sub-embedding vectors using Cartesian products, achieving over 99% compression of embedding layers while maintaining reasonable performance on downstream tasks. The approach is evaluated on both English and multilingual benchmarks.","['How does this method compare to other embedding compression techniques like matrix factorization?', 'Is there a theoretical explanation for why reconstructing embeddings from sub-embeddings works so well?', 'How sensitive is the method to the choice of k (number of sub-embeddings)?', 'What is the computational overhead of reconstructing embeddings from sub-embeddings?', 'How does this approach affect model training time and convergence?', 'Could this technique be applied to other domains beyond language models, such as graph neural networks?']","['Primarily evaluated on BERT-style masked language models', 'May not work as well for languages with less subword sharing or specialized domain-specific embeddings', 'Potential negative impact on model interpretability and robustness not fully explored', 'Computational costs of sub-embedding allocation and reconstruction not thoroughly analyzed']",False,3,3,3,7,4,"['Novel and highly effective embedding compression technique', 'Impressive compression rates (>99%) with relatively small performance drops', 'Thorough evaluation on both English and multilingual benchmarks', 'Flexible method that can be easily integrated into existing models', 'Provides both random and clustering-based allocation strategies']","['Limited theoretical justification for why the method works so well', 'Lack of comparison to other embedding compression techniques', 'Some non-trivial performance drops, especially in multilingual settings', 'Potential impacts on model training, fine-tuning, and interpretability not fully explored', 'Methodology and evaluation could be more rigorous in some areas']",4,3,3.0,4,Accept
A05I5IvrdL-,"This paper provides a novel geometric and algebraic analysis of infinite-horizon POMDPs with memoryless stochastic policies. The key contributions include: (1) Demonstrating that state-action frequencies and expected rewards are rational functions of the policy, with degree related to partial observability. (2) Characterizing feasible state-action frequencies as a basic semialgebraic set with explicit polynomial constraints. (3) Reformulating POMDP optimization as a polynomial programming problem. (4) Using algebraic geometry to bound the number of critical points of the reward function.","['How might these results extend to policies with finite memory?', 'What are the most promising directions for developing new POMDP algorithms based on these results?', 'How well does the polynomial programming approach scale to larger POMDPs?', 'Could the geometric insights guide the design of new policy optimization algorithms?', 'Are there potential applications of this work in fields outside of reinforcement learning?']",The paper focuses on finite POMDPs with memoryless policies and does not address continuous spaces or large-scale problems. The practical impact on algorithm development and the sensitivity to technical assumptions remain to be fully explored.,False,4,3,4,8,4,"['Offers innovative theoretical insights into the mathematical structure of POMDPs', 'Presents rigorous analysis using techniques from algebraic geometry and optimization', 'Generalizes linear programming formulation of MDPs to POMDPs', 'Provides a new computational approach through polynomial programming formulation', 'Lays groundwork for developing improved POMDP optimization algorithms']","['Analysis is limited to memoryless policies and finite state/action spaces', 'Practical implications and scalability of the approach are not extensively demonstrated', 'High level of mathematical sophistication required for full comprehension', 'Some technical assumptions may limit applicability']",4,4,4.0,4,Accept
h-z_zqT2yJU,"This paper introduces Adaptive Temperature Knowledge Distillation (ATKD), a novel approach to address the performance degradation problem in knowledge distillation. ATKD adaptively adjusts temperatures to reduce the sharpness gap between teacher and student model outputs. The method is based on a new metric to quantify model output sharpness. Extensive experiments on CIFAR-100 and ImageNet demonstrate that ATKD outperforms state-of-the-art knowledge distillation methods, particularly when dealing with larger teacher models.","['What is the computational overhead of ATKD compared to vanilla KD?', 'How well does ATKD generalize to tasks beyond image classification?', 'Can you provide more theoretical analysis or guarantees for the optimality of the adaptive temperature approach?', 'How sensitive is ATKD to the choice of sharpness metric and other hyperparameters?', 'Are there any potential negative impacts or limitations of ATKD that were not addressed in the paper?']",The paper focuses primarily on image classification tasks and does not thoroughly analyze the computational cost of ATKD. The method's effectiveness on very large models or in different knowledge distillation settings remains unexplored.,False,3,3,4,8,4,"['Novel and well-motivated approach to mitigate performance degradation in knowledge distillation', 'Introduces a useful metric to quantify model output sharpness', 'Strong theoretical analysis and insights into the sharpness gap problem', 'Comprehensive experiments on CIFAR-100 and ImageNet showing significant improvements over baseline KD and SOTA methods', 'Effectively addresses the performance degradation problem with oversized teachers', ""Method is simple to implement and doesn't require validation set tuning""]","['Experiments are limited to image classification tasks', 'Computational overhead of ATKD is not clearly discussed or analyzed', 'Limited ablation studies on key components of the method', 'Some empirical claims could benefit from statistical significance testing']",4,3,4.0,4,Accept
Ctjb37IOldV,"This paper investigates how dropout helps neural networks find flatter minima during training, potentially improving generalization. The authors propose a 'variance principle' stating that dropout-induced noise has larger variance in sharper directions of the loss landscape. This principle is empirically validated through experiments on various datasets and network architectures, demonstrating that dropout indeed finds flatter minima and that the dropout noise satisfies the variance principle.","['Can you provide any theoretical justification for the variance principle?', 'How well do the findings generalize to larger, state-of-the-art models and datasets?', 'What are the practical implications of this work for designing or tuning dropout?', 'Can you clarify the relationship between the different flatness measures used (interval flatness, Hessian eigenvalues)?', 'What are potential future directions or extensions of this research?']","['Analysis is primarily empirical and lacks strong theoretical foundations', 'Experiments are limited to smaller datasets and networks, potentially limiting generalizability', 'The paper does not extensively discuss potential negative impacts or limitations of the approach', 'Lack of comparison with other regularization techniques']",False,3,3,3,6,4,"['Comprehensive empirical study across multiple datasets and model architectures', 'Novel perspective on understanding dropout through loss landscape geometry', 'Clear connection drawn between dropout noise structure and flatness of minima', 'Interesting parallels drawn between dropout and SGD behavior']","['Lack of theoretical analysis or proofs to support empirical findings', 'Experiments limited to relatively small datasets and networks', 'Limited discussion of practical implications', 'Some aspects of presentation and clarity could be improved']",4,3,3.0,3,Accept
dEwfxt14bca,"This paper introduces intra-episodic mode switching between exploration and exploitation in reinforcement learning. The approach allows agents to dynamically switch between explore and exploit modes during an episode, with various mechanisms proposed for switching. The method is evaluated on 7 Atari games, showing improvements over standard baselines.","['How would this approach perform on a broader set of environments beyond Atari?', 'Can you provide more theoretical intuition or justification for why intra-episodic exploration should work better than monolithic approaches?', 'How sensitive is the method to hyperparameter choices, and are there ways to reduce the number of hyperparameters or make them easier to tune?', 'How does the computational overhead compare to simpler exploration methods?', 'How does this method compare to recent intrinsic motivation or curiosity-driven exploration techniques?']","['Evaluation limited to Atari benchmark', 'Added complexity may make practical implementation more challenging', 'Theoretical understanding of benefits is limited', 'Potential for increased sample complexity', 'Potential negative societal impacts not thoroughly discussed']",False,3,3,3,7,4,"['Novel and well-motivated approach to exploration in RL', 'Thorough empirical evaluation on Atari benchmark', 'Comprehensive investigation of different design choices', 'Promising performance improvements over standard baselines', 'Produces diverse exploratory behaviors through mode switching']","['Limited theoretical justification for the approach', 'Evaluation restricted to Atari games, raising questions about broader applicability', 'Added complexity compared to simpler exploration methods', 'Performance gains not consistently large across all games', 'Limited comparison to more recent, advanced exploration techniques']",3,3,3.0,3,Accept
XHUxf5aRB3s,"This paper addresses non-stationarity in cooperative multi-agent reinforcement learning (MARL) using trust region methods. Key contributions include: (1) A 'δ-stationarity' definition to measure non-stationarity, (2) Theoretical analysis linking δ-stationarity to policy divergence and regret bounds, (3) A trust region decomposition network (TRD-Net), and (4) The MAMT algorithm. Empirical results show consistent improvements over baselines on multiple MARL tasks.","['How does the computational complexity of MAMT compare to baseline methods, especially for larger numbers of agents?', 'How well does the method extend to competitive multi-agent scenarios?', 'Have you compared to more recent MARL algorithms beyond those evaluated?', 'Are there any theoretical guarantees on the quality of the trust region decomposition by TRD-Net?']","['Method may have higher computational requirements than simpler approaches', 'Theoretical guarantees are asymptotic rather than finite sample', 'Empirical evaluation limited to cooperative tasks and relatively small-scale scenarios', 'Potential scalability issues with large numbers of agents', 'The linear regret bound is a limitation compared to sublinear bounds of some existing methods']",False,3,3,3,8,4,"['Novel theoretical framework (δ-stationarity) for analyzing non-stationarity in MARL', 'Solid theoretical analysis connecting δ-stationarity to policy divergence and regret bounds', 'Innovative trust region decomposition approach using graph neural networks', 'Consistent empirical improvements over baselines on multiple environments', 'Comprehensive experiments and ablation studies']","['Potentially higher computational complexity than simpler approaches', 'Limited analysis of scalability to large numbers of agents', 'Linear regret bound is weaker than sublinear bounds of some baselines', 'Lack of comparisons to some recent state-of-the-art MARL methods', 'Writing and presentation could be improved in some sections for clarity']",4,3,3.0,4,Accept
pgkwZxLW8b,"This paper introduces federated sampled softmax (FedSS), a novel method for efficient image representation learning in federated settings. FedSS enables clients to sample a subset of negative classes and optimize only the corresponding model parameters, approximating the full softmax objective while significantly reducing communication and computation costs. Experiments on multiple datasets demonstrate that FedSS achieves comparable performance to full softmax training while using only a fraction of the parameters on clients.","['Can you provide more theoretical analysis of the convergence properties and approximation quality of FedSS?', 'How does the method scale to even larger numbers of classes (e.g., millions)?', 'What are the privacy implications of the sampling approach, and how can they be mitigated?', 'How does FedSS compare to other efficient softmax approximations in non-federated settings?', 'Is there a principled way to set the number of sampled classes, and how sensitive is the performance to this parameter?']","['Experiments limited to datasets with up to ~21k classes', 'Lack of theoretical convergence analysis', 'Privacy implications not thoroughly explored', 'Hyperparameter sensitivity not extensively examined', 'Limited comparison to other efficient softmax approximations in non-federated settings']",False,3,3,3,7,4,"['Novel and significant contribution to federated learning for image representation', 'Effectively addresses the challenge of scaling federated learning to large numbers of classes', 'Demonstrates substantial reduction in communication and computation costs', 'Strong empirical results across multiple datasets and tasks', 'Thorough ablation studies validating key design choices']","['Limited theoretical analysis of convergence properties and approximation quality', 'Experiments conducted on a relatively small scale (largest dataset ~21k classes)', 'Privacy implications of the sampling procedure not thoroughly explored', 'Limited comparison to other efficient softmax approximations in non-federated settings']",4,3,3.0,4,Accept
s51gCxF70pq,"This paper introduces k-Step Latent (KSL), a novel representation learning method for improving sample efficiency in deep reinforcement learning from pixel observations. KSL enforces temporal consistency of representations through a self-supervised auxiliary task, predicting action-conditioned representations multiple steps into the future. The method achieves state-of-the-art results on the PlaNet benchmark suite, outperforming baselines like DrQ and RAD in both data efficiency and asymptotic performance. Extensive analysis demonstrates that KSL produces encoders that generalize better, with representations more strongly tied to reward, more invariant to perturbations, and temporally smoother.","['How does the computational cost of KSL compare to baselines in wall-clock time?', 'Could the method be extended to other RL domains beyond DMControl?', 'Is there a more principled way to handle the optimization of the two objectives?', 'How sensitive is the method to the choice of k and other hyperparameters?', 'What potential real-world applications could benefit from this improved sample efficiency?']","['Increased computational cost may limit applicability in some settings', 'Only evaluated on DMControl tasks, so generalization to other domains is unclear', 'Lack of theoretical guarantees or analysis', 'Potential difficulty in tuning the method due to its complexity']",False,4,3,4,8,4,"['Novel representation learning method leveraging temporal structure', 'Significant empirical improvements over strong baselines on PlaNet benchmark', 'Thorough analysis of learned representations', 'Comprehensive ablation studies and hyperparameter analysis', 'Clear and well-motivated method']","['Increased computational cost compared to simpler approaches', 'Limited evaluation beyond the specific benchmark suite used (DMControl)', 'Lack of theoretical analysis or justification', 'Some architectural choices (e.g., separate optimizers) could use more justification']",3,4,4.0,4,Accept
Rx9luEzcSoy,"This paper investigates the existence of sparse 'lottery ticket' subnetworks in deep neural networks used as image priors for inverse problems. The authors demonstrate that such subnetworks, termed 'lottery image priors' (LIPs), can be found in both untrained DNNs (for deep image prior) and pre-trained GANs (for compressed sensing). These sparse subnetworks match or exceed the performance of full networks on various tasks including denoising, inpainting, super-resolution, and compressed sensing. The paper also analyzes the transferability of these subnetworks across different images, tasks, and domains.","['Can you provide a theoretical explanation for why LIPs work effectively in untrained networks?', 'What specific computational or memory savings can be achieved in real-world applications of LIPs?', 'How do the results generalize to more diverse network architectures and larger datasets?', 'How does LIP performance compare to state-of-the-art compact image prior models or pruning techniques?']","['Lack of theoretical understanding for the effectiveness of sparse structures', 'High computational cost of finding lottery tickets limits practical efficiency gains', 'Potential limited generalizability beyond tested architectures and datasets']",False,3,3,3,7,4,"['Novel application of lottery ticket hypothesis to image priors and inverse problems', 'Comprehensive experimental validation across multiple settings and tasks', 'Interesting analysis of transferability and structure of found subnetworks', 'Potential for more efficient DNN-based image priors']","['Limited theoretical analysis or justification for why this approach works', 'Practical impact and computational benefits not clearly demonstrated', 'Experiments limited to specific architectures and datasets', 'Some claims about transferability could be more rigorously validated']",4,3,4.0,3,Accept
ab7lBP7Fb60,"This paper introduces FPFL, an algorithm designed to enforce group fairness in private federated learning. It adapts the modified method of differential multipliers to include fairness constraints and extends this approach to federated learning with differential privacy. The authors demonstrate empirically that FPFL can mitigate unfairness introduced by differential privacy, particularly for underrepresented groups, using experiments on the Adult and FEMNIST datasets.","['How would the approach scale to larger, more complex models and datasets?', 'Are there ways to reduce the required cohort size while maintaining performance?', 'How does FPFL compare to other recent approaches for fair federated learning?', 'Can the method be extended to other fairness definitions beyond group fairness?', 'What are some potential real-world applications where FPFL could be particularly beneficial?']","['Requires large cohort sizes (m=2000) for effective differential privacy implementation', 'May not scale well to very large models or datasets due to increased communication overhead', 'Limited to certain types of fairness constraints (e.g., group fairness)', 'Potential computational overhead compared to standard federated learning algorithms']",False,3,4,3,7,4,"['Addresses an important problem at the intersection of fairness, privacy, and federated learning', 'Provides theoretical justification for the approach', 'Demonstrates effectiveness on two datasets (Adult and FEMNIST)', 'Clear writing and presentation', 'Thorough comparison to baseline approaches', 'Comprehensive experimental analysis with detailed ablation studies']","['Requires larger cohort sizes to work well with differential privacy', 'Scalability to larger models and datasets is unclear', 'Experiments limited to only two datasets', 'May not be compatible with FederatedAveraging']",4,3,4.0,3,Accept
KeBPcg5E3X,"This paper introduces ContraLORD, a method that combines contrastive learning with latent optimization to learn disentangled representations. The approach uses a generator to learn initial embeddings via latent optimization, then applies content-level and residual-level contrastive losses across samples to further disentangle representations. Experiments on multiple datasets demonstrate improvements over prior methods on disentanglement metrics and linear classification tasks.","['Can you provide stronger theoretical justification for why combining contrastive learning with latent optimization improves disentanglement?', 'How does the method compare to very recent baselines published in the last year?', 'What are the computational requirements compared to prior methods like LORD and OverLORD?', 'How sensitive is the method to hyperparameter choices? Are there any failure cases?']","['Primarily evaluated on image datasets, generalization to other domains is unclear', 'Requires class supervision, limiting applicability to fully unsupervised settings', 'Computational cost of the method is not thoroughly discussed', 'May not scale well to more complex real-world datasets and tasks']",False,3,2,2,7,4,"['Novel combination of contrastive learning and latent optimization for disentanglement', 'Strong empirical results on multiple benchmarks, outperforming previous methods', 'Comprehensive ablation studies validating different components', 'Addresses limitations of prior work by learning disentanglement across samples']","['Incremental novelty over existing techniques like LORD and OverLORD', 'Lacks strong theoretical justification for improved disentanglement', 'Some experimental details and comparisons to very recent baselines are missing', 'Writing and organization could be improved in places', 'Limited analysis of limitations and potential failure cases']",3,3,2.0,3,Accept
neqU3HWDgE,"This paper introduces T D-VAE, a VAE architecture using a torus topology for the latent space to achieve unsupervised disentanglement. Inspired by quantum physics, the latent space is constructed as a tensor product of circles. Experiments show improvements over baseline VAE methods on several disentanglement metrics across multiple datasets.","['How does the method scale to latent spaces with many more dimensions (e.g. 100+)?', 'Have you tested the approach on any complex real-world, high-dimensional datasets?', ""Can you provide a more rigorous theoretical justification for the torus topology's benefits?"", 'How well does the method handle non-periodic latent factors?']","['May not scale well to very high-dimensional latent spaces', 'Primarily tested on relatively simple datasets, real-world applicability unclear', 'Theoretical justification could be more formal', 'Limited discussion of potential negative societal impacts']",False,3,3,3,7,4,"['Novel latent space representation using torus topology', 'Theoretical motivation from quantum physics concepts', 'Consistent improvements over baselines on multiple datasets and metrics', 'Introduction of new DC-score metric combining disentanglement and completeness', 'Thorough ablation studies and analysis']","['Potential scalability issues for high-dimensional latent spaces', 'Theoretical justification could be more rigorous', 'Limited testing on complex real-world datasets', 'Unclear handling of non-periodic latent factors', 'Limited discussion of potential negative societal impacts']",4,3,3.0,4,Accept
DYaFB19z1ig,"This paper introduces self-distribution distillation (S2D) and hierarchical distribution distillation (H2D), novel approaches for efficient uncertainty estimation in neural networks. S2D integrates ensemble training and distribution distillation into a single model, while H2D further distills S2D ensemble knowledge. Experiments on CIFAR-100 classification and out-of-distribution detection tasks demonstrate that these methods often outperform standard models and rival ensemble methods, while being computationally efficient.","['Can you provide more theoretical justification for why S2D and H2D work effectively?', 'How well do these methods generalize to domains beyond image classification?', 'How sensitive are the results to the choice of stochastic regularization technique and other hyperparameters?', 'How does the computational efficiency compare to other recent efficient ensemble methods for different model sizes and datasets?', 'How do S2D and H2D perform on more challenging datasets beyond CIFAR-100?']","['Evaluation limited to image classification tasks', 'Theoretical understanding of the methods could be improved', 'Computational trade-offs not fully explored for different scales', 'Potential negative societal impacts not thoroughly discussed', 'Scalability to larger datasets and model architectures not addressed']",False,3,3,4,7,4,"['Novel and efficient approach for uncertainty estimation', 'Comprehensive experimental evaluation across different architectures and tasks', 'Consistent improvements over baselines in many cases', 'Effective distillation of ensemble knowledge into single models', 'Minimal computational overhead compared to standard training', 'Potential for significant impact on real-world applications']","['Limited theoretical analysis or justification for the approach', 'Experiments focused primarily on image classification tasks', 'Some results show only marginal improvements over baselines', 'Potential sensitivity to hyperparameters not thoroughly explored']",4,3,4.0,4,Accept
8in_5gN9I0,"This paper introduces novel algorithms for triangle and four-cycle counting in graph streams, leveraging learned oracles to achieve improved space complexity. The work presents theoretical analysis showing enhanced space bounds in multiple streaming models, along with empirical results demonstrating practical improvements on real datasets.","['Can you provide more details on the implementation and computational overhead of the oracles/predictors in practice?', 'How do the algorithms perform with very noisy or adversarial predictors?', 'Is it possible to expand the empirical evaluation to include more diverse types of graph datasets or predictor models?']","['Theoretical results sometimes rely on strong assumptions about predictor accuracy', 'Empirical evaluation, while strong, could be more extensive in terms of dataset diversity', 'Trade-offs between predictor complexity, overhead, and algorithm performance could be discussed more thoroughly']",False,4,3,4,8,4,"['Innovative use of learned oracles in graph streaming algorithms', 'Theoretical improvements in space complexity for both adjacency list and arbitrary order streaming models', 'Strong empirical results showing benefits over state-of-the-art baselines on multiple real-world datasets', 'Comprehensive approach combining robust theoretical analysis with practical experimental evaluation']","['Implementation details of oracles and predictors could be more thoroughly explained', 'Empirical evaluation, while strong, could be expanded to cover more diverse datasets', 'Some theoretical results rely on assumptions about predictor accuracy that may not always hold in practice', 'Discussion of trade-offs between predictor complexity and algorithm performance could be expanded']",4,3,3.0,4,Accept
u6sUACr7feW,"This paper introduces DPP-TTS, a novel text-to-speech model that leverages conditional determinantal point processes (DPPs) to diversify prosodic features such as pitch and duration. The key components include a prosody diversifying module (PDM) trained with a conditional maximum induced cardinality objective, and the use of soft dynamic time warping to measure similarity between prosodic sequences. Experiments demonstrate improvements in prosodic diversity over baselines while maintaining naturalness.","['What is the computational overhead and inference speed compared to baselines?', 'How well does the method generalize to other languages or datasets?', 'How sensitive is the method to hyperparameter choices like the number of candidates?', 'Can you provide more examples demonstrating the perceptual differences in prosody?', 'What are some potential real-world applications or use cases for this technology?']","['Evaluated only on a single-speaker English dataset', 'Potential increased computational requirements compared to simpler models', 'Limited analysis of real-world impact and perceptual significance', 'May not capture long-range prosodic dependencies', 'Potential difficulty in interpreting or controlling the prosody generation process']",False,3,3,3,7,4,"['Novel application of DPPs to prosody modeling in TTS', 'Well-motivated approach with sound theoretical grounding', 'Demonstrates improvements in prosodic diversity metrics', 'Maintains naturalness while increasing expressiveness', 'Comprehensive experiments and analysis']","['Improvements in some metrics are relatively small', 'Slightly lower MOS naturalness scores compared to best baseline', 'Adds complexity and potential computational overhead', 'Limited evaluation on a single dataset (LJSpeech)', 'Some aspects of the model could be explained more clearly']",4,3,3.0,3,Accept
e8JI3SBZKa4,"This paper presents a novel approach to kernel similarity matching using Hebbian neural networks. The authors derive an upper bound on the kernel similarity matching objective that can be optimized using correlation-based Hebbian learning rules in a 1-layer recurrent neural network. Empirical results show the method's ability to learn useful nonlinear representations, with advantages over random feature approaches for lower-dimensional outputs.","[""How can the method's performance and stability be improved for higher-dimensional outputs?"", 'What techniques could be applied to enhance the convergence properties of the optimization procedure?', 'How does the computational efficiency compare to other kernel approximation methods as dimensionality increases?', 'Can the approach be extended to deeper architectures while maintaining biological plausibility?', 'What potential applications does this method have in neuroscience or cognitive science research?']","['The method struggles with higher-dimensional representations', 'Optimization process can be unstable and sensitive to hyperparameter choices', 'Lack of theoretical convergence guarantees', 'Evaluation is limited to relatively simple datasets']",False,3,3,3,7,4,"['Novel theoretical framework connecting kernel methods and Hebbian networks', 'Sound derivation of upper bound and learning rules', 'Biologically plausible learning rules and network architecture', 'Demonstrates ability to learn useful nonlinear representations', 'Competitive performance with other kernel approximation methods for lower dimensions', 'Emergence of sparse, template-like features without explicit constraints']","['Does not scale well to very high-dimensional outputs', 'Optimization is sensitive to hyperparameters and can be unstable', 'Lack of theoretical guarantees on convergence', 'Limited experimental evaluation on diverse datasets', 'Biological plausibility arguments could be more thoroughly justified']",3,3,3.0,3,Accept
inSTvgLk2YP,"This paper introduces MeshInversion, a novel approach for single-view 3D object reconstruction that leverages a pre-trained 3D GAN as prior knowledge. The method performs GAN inversion to reconstruct 3D shape and texture, and introduces new Chamfer-based losses to address challenges in 3D-to-2D projection. Results demonstrate improvements in perceptual quality and generalization to challenging cases, particularly for birds and cars datasets.","['How does the method generalize to object categories beyond birds and cars?', 'What is the computational cost and runtime of the test-time optimization process?', 'Is it possible to remove or reduce the dependency on test-time silhouettes and camera poses?', 'How can geometric accuracy be improved while maintaining the perceptual quality improvements?']","['Reliance on pre-trained 3D GANs for specific object categories', 'Potential computational cost at inference time', 'Limited evaluation on diverse datasets', 'May struggle with highly articulated or deformable objects due to fixed mesh topology']",False,3,3,3,7,4,"['Novel use of 3D GAN prior for single-view reconstruction', 'Innovative Chamfer-based losses to address misalignment and discretization', 'State-of-the-art results on perceptual metrics like FID', 'Impressive qualitative results, especially for texture and novel views', 'Good performance on challenging cases like occlusion and articulated shapes', 'Enables interesting applications like texture transfer between objects']","['Reliance on pre-trained GANs may limit applicability to new object categories', 'Geometric accuracy (IoU) not significantly improved over baselines', 'Limited evaluation on datasets beyond birds and cars', 'Optimization-based inference likely slower than feed-forward methods', 'Requires silhouette masks and camera poses during inference, potentially limiting real-world applicability']",4,3,3.0,4,Accept
0lSoIruExF,"This paper introduces methods to incorporate user-item similarity into neighborhood-based recommendation systems. The main contributions are: (1) novel techniques to estimate user preferences based on past interactions and item features, and (2) integration of user-item similarity into the baseline estimate of neighborhood-based models. Experiments on the MovieLens dataset demonstrate improved accuracy over existing methods, with 2-5% improvements in RMSE/MAE over standard baselines.","['How does the performance of the proposed methods vary across different types of items or users in the MovieLens dataset?', 'What is the time complexity of the proposed methods compared to the baselines, and how does this impact real-world deployment?', 'Have you considered comparing your approach to recent deep learning methods like Neural Collaborative Filtering?', 'How might the proposed user representation methods impact user privacy, and what safeguards could be implemented?']","['Evaluation on only one dataset (MovieLens) limits generalizability claims', 'Increased computational complexity may limit real-world applicability, especially for large-scale systems', 'Lack of theoretical analysis or justification for the approach', 'No discussion of potential privacy implications of using more detailed user representations', 'Absence of comparison to state-of-the-art deep learning approaches in recommendation systems']",False,3,3,3,6,4,"['Novel approach to estimate user interests and incorporate user-item similarity', 'Consistent empirical improvements over baseline neighborhood methods', 'Clear presentation and writing overall', 'Detailed experimental analysis and ablation studies']","['Evaluation limited to only one dataset (MovieLens)', 'Modest improvements (2-5%) may not justify added complexity for some applications', 'Lack of comparison to more recent state-of-the-art methods, particularly deep learning approaches', 'Limited analysis of computational complexity and scalability', 'Insufficient discussion of limitations and potential negative impacts']",3,3,3.0,3,Accept
4lLyoISm9M,"This paper introduces Range-Net, a neural network approach for computing low-rank SVD approximations of large matrices. The method achieves high accuracy matching conventional SVD up to machine precision, while requiring memory that scales only with the desired rank. Range-Net is deterministic, provides theoretical guarantees, and has a fully interpretable network architecture. Extensive experiments demonstrate significant improvements over randomized SVD methods, especially on datasets with slowly decaying singular values.","['How does the computational time complexity of Range-Net compare to randomized and deterministic SVD methods?', 'Have you tested the method on any extremely large real-world datasets (100GB+)?', 'How sensitive is the performance to the choice of neural network architecture and optimization algorithm?', 'Can theoretical guarantees on convergence rate or number of passes required be provided?']","['May not be suitable for extremely large datasets where multiple passes are infeasible', 'Computational complexity not explicitly compared to existing methods', 'Potential sensitivity to network hyperparameters', 'Limited testing on very large real-world datasets']",False,4,3,4,8,4,"['Achieves very high accuracy, matching conventional SVD up to machine precision', 'Low memory requirements that scale with desired rank, not full data size', 'Deterministic method with theoretical guarantees on optimality', 'Fully interpretable neural network architecture', 'Significant improvement over existing randomized SVD methods', 'Performs well on real datasets with slow singular value decay']","['Requires multiple passes over data, unlike single-pass randomized methods', 'Limited empirical evaluation on very large real-world datasets', 'Lack of runtime comparisons to other methods, including deterministic SVD approaches', 'Theoretical analysis and proofs could be more intuitive and rigorous']",4,4,3.0,4,Accept
g4nVdxU9RK,"This paper introduces ROEL (Rewardless Open-Ended Learning), a novel approach combining unsupervised reinforcement learning with open-ended learning to generate diverse skills without extrinsic rewards. ROEL utilizes the POET framework for environment mutation and DADS for skill discovery. The method is evaluated on the bipedal walker environment, showing some improvements in skill diversity and generalization compared to baselines.","['How would ROEL perform on more complex environments beyond bipedal walker?', 'How does ROEL compare to other recent open-ended learning approaches?', 'Can you provide a more quantitative evaluation of skill diversity and complexity?', 'What is the theoretical justification for why ROEL should outperform alternatives?', 'How scalable is ROEL to higher-dimensional state/action spaces?']","['Evaluation limited to bipedal walker environment', 'Computational complexity and scalability not thoroughly addressed', 'Potential negative impacts of open-ended learning not discussed', 'Further work needed to demonstrate generalizability to other domains']",False,2,3,2,5,4,"['Novel combination of unsupervised RL and open-ended learning', 'Interesting approach to generating diverse skills without extrinsic rewards', 'Some promising results on the bipedal walker environment', 'Clear explanation of methodology']","['Limited evaluation on only one environment (bipedal walker)', 'Lack of comparison to other open-ended learning approaches beyond POET and DADS', 'Some claims about open-endedness and increasing complexity not fully demonstrated', 'Limited theoretical analysis and justification']",4,2,3.0,2,Reject
6PlIkYUK9As,"This paper proposes a unified submodular optimization framework for selecting informative and diverse subsets of training data for deep learning models. The key contributions include: (1) A combined objective function incorporating uncertainty, diversity, and triple-clique interactions for improved diversity modeling, (2) Class and decision boundary balancing constraints using matroid intersections, (3) An efficient greedy algorithm with approximation guarantees, and (4) Empirical results demonstrating comparable performance to full datasets with 30-40% less data on image classification benchmarks.","['How does the computational complexity of the proposed method scale for very large datasets, and how does it compare to baseline methods?', 'Can you provide more insight into the scenarios or types of datasets where your method provides the largest gains over baselines?', 'How sensitive is the performance to the choice of hyperparameters, particularly the weights in the unified objective function?', 'Have you considered extending the approach to other deep learning tasks beyond image classification, such as object detection or segmentation?']","['Evaluation limited to image classification tasks, potentially limiting generalizability', 'Potential scalability challenges for extremely large datasets', 'May not provide significant gains on well-curated balanced datasets', 'Added complexity in implementation and hyperparameter tuning compared to simpler methods']",False,3,3,3,7,4,"['Novel unified submodular formulation combining multiple objectives', 'Elegant incorporation of class and boundary balancing constraints using matroid intersections', 'Theoretically grounded approach with approximation guarantees', 'Strong empirical results, especially on long-tailed datasets', 'Potential for significant efficiency gains in deep learning by reducing training data requirements and computational resources']","['Relatively modest improvements over baselines like k-center on standard datasets', 'Limited evaluation beyond image classification tasks', 'Computational complexity and scalability not thoroughly discussed', 'Added complexity and potential sensitivity to hyperparameter choices']",3,3,3.0,3,Accept
EFSctTwY4xn,"This paper introduces Adaptive Federated Meta-Learning (AFML), a novel framework for personalized federated learning designed to maintain good performance across various non-IID data scenarios. The key innovation is adaptively balancing information flow from the global model and local data during personalization. The authors provide theoretical analysis, including an 'adaptive trade-off theorem', and demonstrate AFML's effectiveness through experiments on CIFAR-100 and Shakespeare datasets, showing improved performance over existing methods in terms of accuracy, efficiency, and fairness.","['How does AFML compare to more recent personalized FL methods not included in the experiments?', 'Can you provide ablation studies to validate the importance of different components in AFML?', 'How well does AFML generalize to other datasets and tasks beyond image classification and language modeling?', 'What is the computational overhead of AFML compared to simpler methods?', 'How sensitive is the performance to hyperparameters like regularization coefficients?']","['Evaluation limited to only two datasets (CIFAR-100 and Shakespeare)', 'Potential scalability issues with very large numbers of clients', 'Increased computational complexity compared to simpler federated learning approaches', 'Privacy implications of the adaptive approach not thoroughly discussed']",False,3,2,3,7,4,"['Novel approach to personalized federated learning with adaptive balancing of global and local information', 'Strong theoretical foundation, including the adaptive trade-off theorem', 'Comprehensive experiments across different non-IID scenarios', 'Consistent performance improvements over state-of-the-art methods in accuracy, efficiency, and fairness', 'Addresses an important challenge in personalized federated learning']","['Limited experimental evaluation on only two datasets', 'Lack of ablation studies to validate the importance of different components', 'Some parts of the theoretical analysis and key concepts lack clarity', 'Incremental novelty over existing personalized FL methods', 'Limited discussion of limitations and potential negative impacts']",3,3,2.0,3,Accept
5o7lEUYRvM,This paper introduces a function-space variational inference approach for deep Bayesian classification using Dirichlet predictive priors. The method interprets stochastic neural classifiers as variational implicit processes and develops a novel Dirichlet KL estimator for function-space VI. Extensive experiments on image classification tasks demonstrate improvements in uncertainty quantification and adversarial robustness compared to weight-space approaches.,"['How does the computational complexity of the proposed method compare to standard weight-space VI approaches?', 'Are there ways to mitigate the underperformance on in-distribution data while maintaining OOD benefits?', 'How sensitive are the results to the choice of index set for the KL divergence?', 'Could the scaling issues for high-dimensional outputs be addressed more rigorously?', 'What are potential future directions for improving the method, particularly for high-dimensional classification tasks?']","['The method introduces additional computational complexity', 'Performance can degrade for high-dimensional label spaces without heuristic adjustments', 'Some trade-off between in-distribution performance and OOD robustness', 'Requires careful tuning of KL scaling for high-dimensional problems', 'Performance on very large-scale datasets (e.g., ImageNet) not demonstrated']",False,3,3,4,7,4,"['Novel combination of function-space VI and Dirichlet priors for classification', 'Theoretically well-grounded approach that unifies and extends prior work', 'Extensive empirical evaluation showing improved uncertainty quantification and robustness', 'Applicable to various Bayesian neural network architectures without changing model structure', 'Demonstrates improvements in OOD uncertainty quantification and adversarial robustness', 'Thorough ablation studies demonstrating the impact of different components']","['Added computational complexity and overhead of Dirichlet estimation procedure', 'Scaling issues with high-dimensional outputs requiring heuristic adjustments', 'Some underperformance on in-distribution data due to regularization from the prior', 'Limited exploration of different prior specifications', 'Potential difficulty in choosing appropriate index sets for real-world problems']",4,3,3.0,3,Accept
i2baoZMYZ3,"This paper introduces AQuaDem, a novel method for discretizing continuous action spaces in reinforcement learning using demonstrations. The approach learns a state-dependent mapping to discrete candidate actions by minimizing a reconstruction loss on demonstration data. AQuaDem is evaluated on challenging robotics tasks across three setups: RL with demonstrations, RL with play data, and imitation learning. Results consistently show improvements over baselines in sample efficiency and performance.","['How well does AQuaDem generalize to non-robotics domains?', 'How sensitive is the method to the quality and quantity of demonstrations?', 'Can theoretical guarantees be provided on the quality of the learned discretization?', 'How does the computational cost compare to continuous control baselines?', 'What are potential extensions or future work directions for AQuaDem?']","['May not capture full expressivity of continuous action space', 'Relies on availability of reasonable demonstration data', 'Potential scalability issues with very high-dimensional action spaces']",False,3,3,4,7,4,"['Novel and well-motivated approach to discretizing continuous action spaces', 'Strong empirical results across multiple robotics tasks and RL setups', 'Enables use of discrete-action RL algorithms for continuous control problems', 'Thorough analysis including visualizations and ablations', 'Addresses limitations of naive discretization approaches', 'Comprehensive experimental evaluation across multiple setups (RL with demonstrations, RL with play data, and imitation learning)']","['Limited theoretical analysis or guarantees', 'Relies on availability of demonstration data', 'Experiments focused primarily on robotics tasks', 'Potential loss of optimality compared to true continuous actions']",4,4,3.0,4,Accept
hjlXybdILM3,"This paper introduces SimpleBits, a novel method for simplifying neural network inputs by minimizing their encoding bit size using a pretrained generative model. The approach is applied to various scenarios including training, dataset condensation, and post-hoc analysis. Experiments on multiple datasets demonstrate that SimpleBits can effectively remove superfluous information while retaining task-relevant features, often maintaining good task performance.","['Can you provide stronger theoretical justification for using bits as a simplicity measure?', 'How does SimpleBits compare quantitatively to other input simplification or interpretability methods?', 'How sensitive are the results to the choice of generative model used?', 'Can you provide more quantitative evaluation of the post-hoc analysis results?']","['Method may not work well for very complex tasks or high-dimensional inputs', 'Reliance on pretrained generative models which may introduce biases', ""Potential to amplify dataset biases by removing 'irrelevant' information"", 'Computational cost of the method not thoroughly analyzed']",False,3,3,4,7,4,"['Novel approach to input simplification using generative model encoding size', 'Comprehensive evaluation across multiple tasks and datasets', 'Demonstrates ability to remove irrelevant information while preserving task performance', 'Provides interesting insights into model behavior and potential dataset biases', 'Potential applications for interpretability and dataset distillation']","['Lack of strong theoretical justification for using bit size as a simplicity measure', 'Limited comparisons to other input simplification or interpretability methods', 'Some results, especially in post-hoc analysis, are qualitative rather than quantitative', 'Potential negative impacts and limitations not thoroughly discussed']",4,3,3.0,4,Accept
pWBNOgdeURp,"This paper introduces Koopman operator theory-based pruning algorithms for neural networks, demonstrating equivalence with standard pruning methods and providing insights into early-stage pruning dynamics.","['How well does the Koopman pruning approach scale to larger networks and datasets like ImageNet?', 'What is the computational overhead of performing Koopman decomposition compared to standard pruning methods?', 'Are there scenarios where Koopman pruning provides significant practical advantages over simpler magnitude/gradient pruning?']","['Experiments are limited to small/medium scale networks and datasets', 'Computational cost of Koopman decomposition may limit scalability to very large models', 'Practical impact on state-of-the-art models not clearly demonstrated']",False,3,3,3,7,4,"['Novel theoretical approach using Koopman operator theory', 'Unification of magnitude and gradient-based pruning methods', 'New insights into pruning dynamics, especially early in training', 'Solid theoretical analysis and derivations', 'Use of ShrinkBench framework for reproducible experiments']","['Limited experimental validation on small datasets and networks', 'Unclear scalability to larger, state-of-the-art models', 'Potential high computational cost of Koopman decomposition', 'Limited demonstration of practical benefits over existing methods']",4,3,3.0,4,Accept
iLHOIDsPv1P,"This paper introduces the PAC-Bayes Information Bottleneck (PIB), a novel framework for analyzing neural network generalization. The key contributions include: 1) A new information bottleneck based on PAC-Bayes bounds, 2) An approximation for the intractable Information in Weights (IIW) measure, 3) A Bayesian inference algorithm using SGLD to sample from the optimal weight posterior, and 4) Empirical demonstrations that IIW can explain various neural network behaviors. The paper provides both theoretical foundations and extensive experiments to validate the approach.","['Can the theoretical approximations and assumptions made to derive the IIW measure be validated empirically for very deep networks?', 'How does the computational complexity and scalability of PIB compare to standard training for large models and datasets?', 'Can you provide more extensive comparisons to other recent generalization measures and information-theoretic frameworks?', 'Are there scenarios where IIW fails to explain generalization behavior or capture relevant aspects of neural network dynamics?']","['Focus primarily on supervised classification tasks and small to medium-sized image datasets', 'Scalability to very large models and datasets not demonstrated', 'Theoretical connections to other generalization frameworks could be expanded', 'Computational cost of estimating IIW may be prohibitive for very large networks']",False,3,3,3,7,4,"['Novel theoretical framework combining information bottleneck and PAC-Bayes bounds', 'Efficient approximation derived for the intractable IIW measure', 'Comprehensive empirical evaluation across multiple neural network settings', 'Provides new insights into neural network behavior through an information-theoretic lens', 'Consistent improvements over baselines when using PIB for training on several datasets']","['Theoretical approximations and assumptions could be examined more critically', 'Experiments limited to relatively small datasets (MNIST, CIFAR-10) and network architectures', 'Practical benefits of PIB training compared to standard regularization not fully demonstrated on large-scale problems', 'Limited comparisons to other recent information-theoretic approaches and generalization measures', 'Some claims about universality of the two-phase behavior may be overstated given the limited scope of experiments']",4,3,3.0,3,Accept
0q0REJNgtg,"This paper introduces Retrieval-Augmented Reinforcement Learning (R2A), a novel approach that enhances standard RL agents with a retrieval process to access and incorporate information from past experiences. The method uses attention mechanisms to select relevant past trajectories and states, integrating this information into the agent's value/policy estimates. Experiments on Atari games and multi-task environments demonstrate significant improvements over baseline methods.","['How does the computational complexity scale with the size of the experience dataset?', ""What kinds of information are typically being retrieved, and are there patterns in what's selected?"", 'How does this approach compare to other memory-augmented RL methods?', 'How sensitive is the method to hyperparameters like the number of retrieved trajectories/states?', 'What are potential real-world applications where this method could be particularly beneficial?']","['Increased computational complexity may limit applicability to some settings', 'Method may not scale well to extremely large datasets', 'Limited analysis of retrieved information makes it hard to interpret how the method works', 'Lack of theoretical analysis or performance guarantees']",False,3,3,3,7,4,"['Novel combination of parametric RL with non-parametric retrieval', 'Comprehensive empirical evaluation across different domains', 'Consistent improvements over strong baselines', 'Thorough ablation studies validating different components', 'Clear separation of retrieval process from main agent']","['Increased computational complexity compared to baseline RL algorithms', ""Limited analysis of what information is being retrieved and how it's used"", 'Potential scalability issues with very large datasets', 'Lack of theoretical analysis or guarantees']",4,3,3.0,4,Accept
kQ2SOflIOVC,"This paper introduces a novel few-shot learning approach for histology image classification, combining contrastive learning pre-training with latent augmentation. The method outperforms supervised pre-training baselines on cross-domain few-shot tasks using histology datasets. The authors provide an insightful analysis of contrastive learning's superior generalization compared to supervised learning for histology images.","['How would the method perform on larger, more diverse histology datasets?', 'Have you considered applying your method to other medical imaging modalities?', 'What specific clinical applications do you envision for this work?', 'How does your latent augmentation technique compare to more recent data augmentation methods in few-shot learning?']","The study focuses solely on histology images, limiting generalizability to other medical imaging modalities. The method relies on a large unlabeled dataset for pre-training, which may not be available in all clinical scenarios. The datasets used may not fully represent the diversity of real-world histology images, potentially introducing biases. The authors should expand their discussion of these limitations and potential negative societal impacts.",False,3,3,3,7,4,"['Addresses an important problem in medical AI', 'Novel application of contrastive learning and latent augmentation to histology images', 'Sets up realistic cross-domain few-shot tasks', 'Consistent improvements over baselines across multiple datasets and settings', 'Thorough empirical evaluation with careful ablation studies', 'Insightful analysis of contrastive vs supervised learning for histology images']","['Datasets used are relatively small compared to standard few-shot learning benchmarks', 'Limited comparison to other state-of-the-art few-shot learning methods', 'Some sections could be improved for clarity', 'Limited discussion of limitations and potential negative impacts']",3,3,3.0,3,Accept
gCmCiclZV6Q,"This paper presents a method for detecting offensive content in large image datasets using pre-trained vision-language models like CLIP. The authors demonstrate that CLIP models implicitly learn to detect offensive content during pre-training and can be fine-tuned on a small labeled dataset to improve this capability. They apply the method to detect various types of offensive content in ImageNet, going beyond previous manual curation efforts.","['How robust is the method to different cultural definitions of offensive content?', 'What are the false positive/negative rates on a held-out test set?', 'How might this method be integrated into practical dataset curation workflows?']","['Subjectivity in defining offensive content', 'Potential to encode biases present in pre-trained model or tuning data', 'May flag content inappropriately in some contexts', 'Could enable automated censorship if misused']","The automated flagging of content as 'offensive' raises concerns about cultural bias, potential misuse for censorship, and the need for human oversight in content moderation.",3,4,4,7,4,"['Novel approach leveraging pre-trained models for dataset curation', 'Demonstrates implicit learning of offensive content detection in CLIP', 'Method requires minimal additional training data', 'Comprehensive evaluation on ImageNet dataset', 'Potential for high impact in improving dataset quality', 'Scalable approach to assist human curators']","[""Definition of 'offensive' content is subjective and culturally dependent"", 'Potential biases in CLIP model or tuning dataset not fully explored', 'False positive/negative rates not thoroughly characterized', 'Ethical implications of automated content flagging need more discussion']",4,3,4.0,4,Accept
xS8AMYiEav3,"This paper introduces REASSURE, a novel methodology for repairing neural networks with ReLU activations. The key innovation is the synthesis of a patch network that corrects behavior in specific regions while preserving the original network's function elsewhere. The approach provides strong theoretical guarantees including soundness, completeness, minimality, and locality of repairs. Experimental results demonstrate significant improvements over existing methods in terms of localized repairs and minimal side effects.","['How well does the approach scale to very large networks (e.g., modern deep CNNs or transformers)?', 'Could the method be extended to networks with non-ReLU activations?', 'How sensitive are the results to the choice of hyperparameter γ, and how should it be set in practice?', 'Are there cases where the linear programming problems become intractable?', 'How might this method be integrated into existing ML development and deployment pipelines?']","['Currently only applicable to ReLU networks', 'Scalability to very large networks and high-dimensional inputs not fully explored', 'May introduce additional parameters and computational overhead', 'Theoretical assumptions (e.g., full rank weight matrices) may not always hold in practice', 'Potential need for careful hyperparameter tuning']",False,4,3,4,8,4,"['Novel approach leveraging the piecewise linear nature of ReLU networks', 'Strong theoretical foundations with guarantees on soundness, completeness, minimality, and locality', 'Significant empirical improvements over existing methods on multiple benchmarks', 'Ability to perform both point-wise and area repairs', 'Addresses key limitations of prior work on neural network repair']","['Currently limited to ReLU networks', 'Scalability to very large networks not fully demonstrated', 'Potential computational overhead for large networks', 'Some sections could benefit from improved clarity and more intuitive explanations']",4,4,3.5,4,Accept
OqHtVOo-zy,This paper introduces a novel method for learning with instance-dependent label noise by estimating the transition matrix between Bayes optimal labels and noisy labels using a neural network. The approach leverages 'distilled' examples with inferred Bayes optimal labels to train the transition matrix estimator. Extensive experiments on synthetic and real-world noisy datasets demonstrate strong performance compared to state-of-the-art methods.,"['How does the performance of the method vary with different choices of distillation threshold? Can you provide a more detailed sensitivity analysis?', 'Can you provide visualizations or quantitative metrics to demonstrate the quality of the estimated transition matrices compared to ground truth (if available)?', 'How does the training time and memory requirements of your method compare to other instance-dependent noise handling approaches, especially for large datasets?', 'Have you considered extending the approach to handle open-set label noise or multi-label classification problems?']","['Relies on the ability to collect good distilled examples, which may not always be possible in all noisy label scenarios', 'Added computational cost of training the transition matrix network, which may be significant for very large datasets', 'May not scale well to a very large number of classes due to the size of the transition matrix', 'Only tested on image classification tasks, generalization to other domains not demonstrated']",False,3,3,4,7,4,"['Highly original approach to modeling instance-dependent label noise', 'Strong theoretical justification for modeling Bayes optimal to noisy label transition', 'Robust empirical results across multiple datasets and noise levels', 'Comprehensive comparisons with relevant baseline methods', 'Addresses an important problem in weakly supervised learning']","['Potential bias introduced by the distillation process not fully analyzed', 'Sensitivity to hyperparameters, particularly the distillation threshold', 'Theoretical analysis could be more rigorous in some aspects', 'Limited ablation studies and analysis of different components', 'Computational complexity and scalability concerns not fully addressed']",4,3,4.0,4,Accept
oTQNAU_g_AZ,"This paper introduces DAIR (Disentangled Attention Intrinsic Regularization) to improve bimanual robot manipulation using reinforcement learning. DAIR uses an attention mechanism with intrinsic regularization to encourage robots to focus on different subtasks, addressing challenges of domination and conflict. Experiments on 5 bimanual tasks show improved performance, efficiency, and safety compared to baselines, as well as good generalization capabilities.","['How well would the DAIR approach transfer to real robot systems?', 'Does this method scale to scenarios with more than two robots?', 'How sensitive is the performance to the choice of hyperparameters, particularly the regularization weight λ?', 'Could DAIR be combined with other intrinsic motivation techniques for further improvements?']","['Evaluation limited to simulation environments', 'Scalability to many more objects or robots not thoroughly evaluated', 'Potential challenges in transferring to real-world robotic systems', 'May require careful tuning of regularization weight and other hyperparameters']",False,4,4,4,7,4,"['Novel and effective approach to address key challenges in bimanual manipulation', 'Comprehensive experiments on diverse and challenging tasks', 'Significant improvements in performance, efficiency, and safety over baselines', 'Impressive generalization to more objects than seen during training', 'Clear motivation and well-designed method leveraging attention mechanisms', 'Thorough ablation studies and analysis', 'Contributes to the development of safer robotics systems']","['Experiments limited to simulation environments, not tested on real robots', 'Limited exploration of scalability beyond two-robot scenarios', 'Somewhat limited theoretical analysis and comparison to recent related work', 'Potential sensitivity to hyperparameter choices not fully explored']",4,4,4.0,4,Accept
KSSfF5lMIAg,"This paper introduces novel model-agnostic interpretability methods for multiple instance learning (MIL), including independent-instance methods and a local surrogate method called MILLI. The authors evaluate these methods across multiple datasets and MIL model types, demonstrating improved performance over existing interpretability approaches in many cases. The methods aim to identify key instances within bags and determine what classes they support or refute.","['How do the proposed methods scale to very large MIL problems with many instances per bag?', 'Can you provide more theoretical justification or guarantees for the proposed methods, especially MILLI?', 'Have you considered extending these methods to other MIL settings like multi-label MIL or regression tasks?', 'What are some potential real-world applications where these interpretability methods could be particularly useful?']","['Evaluation limited to a few datasets, mostly synthetic or simple', 'Computational complexity and scalability concerns for large-scale MIL problems', 'Lack of human evaluation studies on interpretability outputs', 'Limited comparison to some existing MIL interpretability methods']",False,3,4,4,7,4,"['Novel model-agnostic interpretability methods for MIL, including MILLI', 'Comprehensive evaluation across multiple datasets and model types', 'Improved performance over existing techniques in many cases', 'General applicability to different MIL models', 'Well-structured and clearly presented research']","['Limited theoretical analysis or proofs', 'Experiments mostly on synthetic or simple datasets', 'Potential scalability issues with large bags or datasets', 'Limited discussion on computational complexity']",4,3,4.0,4,Accept
on54StZqGQ_,"This paper introduces 'degradation attacks' on certifiably robust neural networks. While certified defenses prevent adversarial examples that change the model's prediction, the authors show an attacker can force the model to reject valid inputs by perturbing them to cause robustness certification to fail. The paper provides theoretical analysis, presents algorithms to perform these attacks, and demonstrates empirically that state-of-the-art certified defenses are highly vulnerable. Some initial defenses are also proposed and evaluated.","['How do baseline rejection rates (without attacks) compare to the rejection rates under degradation attacks?', 'Have you investigated whether certain model architectures or training procedures make models more or less susceptible to degradation attacks?', 'How well do these results generalize beyond image classification tasks?', 'How do you envision practitioners should balance the tradeoff between robustness to standard adversarial attacks vs degradation attacks?', 'What potential mitigations could be implemented in real-world deployments to address degradation attacks?']","['The evaluation focuses primarily on image classification tasks', 'Proposed defenses against degradation attacks are not fully developed or evaluated', 'The computational cost of the proposed attacks is not thoroughly analyzed', 'Does not consider potential interactions with other types of robustness or attacks']",False,4,3,4,7,4,"['Introduces a novel and important attack concept (degradation attacks) that reveals a fundamental limitation in current certified defenses', 'Provides thorough theoretical analysis explaining why the attacks work', 'Presents concrete algorithms for performing degradation attacks', 'Strong empirical evaluation demonstrating vulnerability of state-of-the-art defenses', 'Clear writing and good paper organization']","['Evaluation is limited primarily to image classification tasks', 'Proposed defenses against degradation attacks are preliminary and not fully developed', 'Lack of comparison to baseline rejection rates without attacks', 'Could benefit from more analysis of why certain models/defenses are more vulnerable', 'Limited discussion on potential real-world implications of degradation attacks']",4,3,4.0,4,Accept
R3zqNwzAVsC,"This paper introduces the 'Ethical Module', a post-processing method to mitigate gender bias in face recognition models. It employs a shallow MLP trained with a fair von Mises-Fisher loss to transform embeddings from pre-trained models. The authors also propose new fairness metrics. Extensive experiments demonstrate the method's ability to reduce bias while maintaining overall performance across multiple datasets and models.","['How would the method extend to non-binary gender or other types of bias?', 'Can theoretical guarantees be provided for the fairness improvements?', 'How does this approach compare to other recent bias mitigation techniques for face recognition?', 'Is there a way to incorporate the fairness metrics directly into the training objective?', 'Could you elaborate on the geometric interpretation of the method and how it guides the choice of hyperparameters?']","['Only addresses binary gender bias, not intersectional effects or other protected attributes', 'Requires access to gender labels for training data', 'May not completely eliminate bias, only reduce it', 'Potential for slight reduction in overall performance when optimizing for fairness', 'Effectiveness may vary depending on the quality of the pre-trained model']",False,3,3,3,6,4,"['Novel and computationally efficient approach to bias mitigation', 'Introduction of new, relevant fairness metrics for face recognition', 'Extensive experimental evaluation on multiple datasets and models', 'Clear improvements in fairness metrics while maintaining good overall performance', 'Practical method that can leverage existing pre-trained models', 'Good interpretability due to the geometric nature of the von Mises-Fisher loss']","['Limited to binary gender bias, does not address intersectional effects', 'Lack of strong theoretical analysis or guarantees', 'Relies on potentially biased gender classifier for training data labels', 'Trade-off between fairness and overall performance not fully resolved']",4,3,3.0,3,Accept
BGvt0ghNgA,"This paper introduces Lipschitz-constrained Skill Discovery (LSD), a novel unsupervised skill discovery method that promotes learning more diverse and dynamic skills compared to previous mutual information-based approaches. LSD employs a Lipschitz-constrained state representation function to encourage larger state variations and enables zero-shot goal following. Experiments on MuJoCo environments show superior performance in skill diversity, state coverage, and downstream tasks.","['How sensitive is the method to the choice of Lipschitz constant, and how was the value of 1 determined?', 'Could the approach be extended to work with image-based observations, and what modifications would be necessary?', ""How does LSD's performance change as the dimensionality of the skill space increases beyond 5 dimensions?""]","['May not work well with pixel observations due to less meaningful Lipschitz constraints', 'Continuous LSD mainly discovers locomotion skills, potentially limiting skill diversity', 'Skills affected by reachable regions in state space, which may vary across environments', 'Potential scalability issues with very high-dimensional skill spaces']",False,4,4,4,8,4,"['Novel approach addressing limitations of previous MI-based methods', 'Enables zero-shot goal following without additional training', 'Strong empirical results on challenging MuJoCo environments', 'Thorough comparisons to baselines and ablation studies', 'Clear explanations and intuitions for the proposed method']","['Limited theoretical analysis or guarantees', 'Experiments focused mainly on locomotion tasks', 'Some hyperparameters and design choices lack full justification']",4,4,4.0,4,Accept
1XdUvpaTNlM,"This paper introduces Batch Whitening Channel Pruning (BWCP), a novel probabilistic method for pruning channels in convolutional neural networks. The key components are a batch whitening technique to adjust channel activation probabilities and a soft sampling procedure using Gumbel-Softmax. Experiments on CIFAR and ImageNet datasets demonstrate that BWCP outperforms several state-of-the-art pruning methods on image classification benchmarks.","['How does the computational overhead of BWCP compare to other pruning methods and potential inference speedups?', 'Are there ways to automatically set or optimize the regularization hyperparameters?', 'How well does BWCP generalize to other network architectures beyond CNNs and tasks beyond image classification?', 'Have you analyzed whether BWCP has any impact on the fairness/bias of the pruned models?']","['Only evaluated on image classification tasks', 'Added training complexity and hyperparameters to tune', 'Potential difficulty in scaling to very large models', 'Limited analysis of actual inference speedup on hardware']",False,3,3,3,7,4,"['Novel probabilistic formulation for channel pruning', 'Theoretically justified batch whitening technique', 'Strong empirical results across multiple datasets and architectures', 'Comprehensive ablation studies and analysis', 'End-to-end training without need for fine-tuning']","['Added computational overhead during training', 'Requires tuning of sparsity regularization hyperparameters', 'Limited analysis of potential limitations or failure cases', 'Evaluation focuses mainly on image classification tasks']",4,3,3.0,3,Accept
cpstx0xuvRY,"This paper presents an information-theoretic analysis of generalization error bounds for iterative semi-supervised learning algorithms, with a focus on pseudo-labeling approaches. The main theoretical results are derived for a binary Gaussian mixture model, demonstrating how generalization error evolves and converges over iterations. Experiments on real datasets (MNIST and CIFAR) validate the key theoretical insights.","['How well do the insights from the Gaussian mixture model generalize to more complex data distributions and models?', 'Could the analysis be extended to other SSL approaches beyond pseudo-labeling?', 'How do the derived bounds compare to existing generalization bounds for SSL in terms of tightness and practical implications?']","['Analysis is primarily limited to a simple Gaussian mixture model', 'Tightness of bounds in real-world scenarios is unclear', 'Practical implications for algorithm design are not extensively explored', 'Potential societal impacts of improved SSL techniques are not discussed']",False,3,3,3,6,4,"['Novel information-theoretic framework for analyzing generalization in iterative SSL', 'Rigorous theoretical analysis with detailed examination of a tractable GMM model', 'Empirical validation on real datasets that corroborates theoretical findings', 'Clear writing and presentation of complex theoretical concepts']","['Main theoretical analysis is limited to a relatively simple Gaussian mixture model', 'Derived bounds may be loose in practical scenarios', 'Empirical experiments are somewhat limited in scope']",3,3,3.0,3,Accept
Wm3EA5OlHsG,"This paper introduces Scene Transformer, a unified architecture for multi-agent trajectory prediction in autonomous driving scenarios. The model can perform both marginal (per-agent) and joint (scene-level) predictions using a single architecture. Key contributions include a scene-centric permutation equivariant model, factored attention across time and agents, and a masked modeling approach enabling conditional predictions. The paper reports state-of-the-art results on major benchmarks including Argoverse and Waymo Open Motion Dataset.","['What is the inference latency on a typical automotive compute platform?', 'How does performance degrade with reduced model size/compute?', 'Which components of the model contribute most to the performance gains?', 'How does the model compare to recent approaches like DenseTNT and LaneGCN?', 'How does the model perform in edge cases or rare scenarios?']","['Model complexity may limit real-world deployment and interpretability', 'Runtime analysis is limited, which is important for real-time applications', 'Focused on benchmark performance rather than real-world autonomous driving scenarios', 'Potential negative societal impacts of improved autonomous driving technology not thoroughly discussed', 'May require large amounts of training data']",False,3,3,4,7,4,"['Novel unified architecture for both marginal and joint predictions', 'State-of-the-art results on multiple benchmarks', 'Efficient factorized attention mechanism', 'Flexible masked modeling approach enabling conditional predictions', 'Extensive experiments and analysis on large-scale datasets']","['Complex model with many components - may be challenging to deploy and interpret', 'Limited evaluation of inference speed and real-time applicability', 'Missing comparison to some recent strong baselines', 'Limited discussion of societal impacts and ethics']",3,3,4.0,4,Accept
-uZp67PZ7p,"This paper introduces Context-aware Decentralized PPO (CD-PPO), a novel multi-agent reinforcement learning approach for inventory management with shared resources. CD-PPO models the problem as a shared-resource stochastic game, uses decentralized training with local simulators conditioned on learned context dynamics, and augments context data. Empirical results show strong performance and sample efficiency on environments with up to 100 agents.","['Can you provide stronger theoretical justification for the surrogate objective and approximations made?', 'How well would the approach generalize to other domains with shared resources beyond inventory management?', 'What are the key factors that determine when CD-PPO outperforms baselines like IPPO, especially in constrained environments?', 'How does CD-PPO compare to other recent MARL algorithms not included in the baselines, such as QMIX or mean field methods?', 'What are the computational requirements of CD-PPO compared to the baselines, especially for large-scale problems?']","['Evaluation limited to simulated inventory management problems', 'Performance degrades in very constrained environments', 'Theoretical foundations could be stronger', 'Scalability beyond 100 agents not demonstrated', 'May not generalize to more complex agent interactions beyond shared resources']",False,3,3,3,7,4,"['Novel formulation of shared-resource stochastic games', 'Decentralized training approach that scales well to many agents', 'Strong empirical results, especially in sample efficiency and scalability', 'Thorough experimental evaluation and ablation studies', 'Clear motivation and practical relevance for inventory management']","['Limited theoretical justification for surrogate objective', 'Evaluation restricted to inventory management domain', 'Performance issues in very constrained environments', 'Lack of comparisons to some relevant MARL approaches', 'Limited theoretical analysis and guarantees']",3,3,3.0,4,Accept
vgqS1vkkCbE,"This paper introduces Value Function Spaces (VFS), a novel approach for creating skill-centric state abstractions in hierarchical reinforcement learning. VFS uses the value functions of low-level skills to construct a compact state representation that captures affordances while ignoring irrelevant details. The authors demonstrate VFS's effectiveness in both model-free and model-based RL settings, outperforming other representation learning approaches on long-horizon maze navigation and robotic manipulation tasks. The paper also shows VFS enables good zero-shot generalization to new environments.","['How sensitive is the performance to the quality of the pre-trained skills?', 'Could VFS be extended to work with skills that are learned/improved online?', 'How well would VFS scale to very large numbers of skills?', 'Is there a way to provide theoretical guarantees on the properties of the VFS representation?', 'How does VFS compare to other affordance-based approaches in robotics?']","['Requires pre-trained skills and value functions', 'Only tested in simulated environments', 'May not scale well to very large skill sets', 'Lacks theoretical guarantees', 'Potential computational cost of computing value functions for all skills at each state']",False,3,4,3,7,4,"['Novel and intuitive approach to state abstraction based on skill affordances', 'Demonstrated effectiveness in both model-free and model-based RL settings', 'Strong empirical results on challenging long-horizon tasks', 'Ability to generalize zero-shot to new environments', 'Clear exposition and thorough experiments']","['Assumes access to pre-trained low-level skills and their value functions', 'Limited theoretical analysis or guarantees', 'Robotics experiments use simulated rather than real-world environments', 'Potential scalability issues with large numbers of skills']",4,3,4.0,4,Accept
2DJn3E7lXu,"This paper presents a comprehensive evaluation of 18 hardware metric prediction models for neural architecture search (NAS) using 10 datasets from two NAS benchmarks. The authors analyze predictor accuracy across different training set sizes and metrics, and use simulations to study how predictor errors impact architecture selection in NAS. Key findings include that MLP models generally perform best, and that verifying predictions on selected architectures can significantly improve results.","['How do the simulation results compare to real-world NAS scenarios? Have you validated the simulation approach against actual NAS runs?', 'Can you provide more insight into why MLP models consistently outperform other predictors across different datasets?', 'How might the findings change for larger, more complex search spaces beyond the benchmarks used in this study?', 'What are the computational trade-offs between predictor accuracy and training/inference time in practical NAS scenarios?']","['Simulation approach may not fully capture real-world NAS complexities', 'Results may not generalize beyond the specific benchmarks and metrics studied', 'Limited discussion of computational costs and practical trade-offs in predictor selection']",False,3,3,3,7,4,"['Extensive evaluation of multiple predictor models on diverse datasets', 'Novel simulation approach to analyze the impact of predictor errors on NAS outcomes', 'Practical recommendations for predictor selection and usage in NAS', 'Thorough empirical results and analysis', 'Good reproducibility with open-sourced code and data']","['Limited theoretical analysis or novel technical contributions', 'Simulation approach makes some simplifying assumptions that may not fully capture real-world scenarios', 'Lack of experiments on real NAS runs (only simulations)', 'Results may not generalize beyond the specific NAS benchmarks used']",3,3,3.0,3,Accept
aBAgwom5pTn,"This paper introduces DYHPO, a novel hyperparameter optimization method for deep learning. It combines a Gaussian Process with a deep kernel that incorporates learning curve information as a surrogate model, along with a multi-fidelity acquisition function. Extensive experiments across multiple benchmarks and tasks demonstrate significant improvements over state-of-the-art baselines.","['What is the computational overhead of fitting the deep kernel GP compared to simpler surrogate models?', 'How well would this method scale to very large models like transformers?', 'Are there any theoretical guarantees that can be provided for the proposed method?', ""How sensitive is the method's performance to the choices of GP kernel architecture and hyperparameters?"", 'In which domains or applications might DYHPO be particularly beneficial compared to existing methods?']","['Results are limited to relatively small-scale problems and may not generalize to larger deep learning models', 'Lack of theoretical analysis limits understanding of when/why the method works well', 'Potential computational costs not thoroughly analyzed', 'Introduces additional complexity compared to simpler approaches', 'No comparison to specialized NAS methods for neural architecture search experiments']",False,4,3,4,8,4,"['Novel technical approach combining deep kernel GPs with multi-fidelity optimization for HPO', 'Comprehensive empirical evaluation showing consistent improvements across diverse benchmarks', 'Addresses important limitations of existing methods', 'Thorough ablation studies validating key components', 'Clear explanation and motivation of the method']","['Lacks theoretical analysis or guarantees', 'Limited testing on very large models or datasets', 'Computational overhead of the deep kernel GP not thoroughly analyzed', 'Introduces additional hyperparameters for GP model and kernel']",4,4,3.0,4,Accept
SsPCtEY6yCl,"This paper proves several theoretical results demonstrating that highly expressive energy-based sequence models can have uncomputable and inapproximable partition functions. This leads to undecidability of model selection and parameter learning for these model families. The authors provide rigorous mathematical proofs, discuss implications for machine learning practice, and suggest some less expressive alternatives to avoid these issues.","['How do these theoretical results translate to practical limitations for current neural sequence models?', 'Are there empirical experiments that could demonstrate these issues on concrete models/datasets?', ""How do the proposed 'palliative alternatives' compare in expressiveness to state-of-the-art sequence models?"", 'What are the implications for current research directions in language modeling and text generation?']","['Results are purely theoretical and may not reflect issues in finite practical settings', 'Limited discussion of practical implications or solutions', 'Proposed alternatives may be too restrictive for many applications', 'Highly technical nature limits accessibility and potential impact']",False,4,2,3,7,4,"['Rigorous theoretical analysis with formal proofs', 'Novel and important results on fundamental limitations of expressive energy-based models', 'Clear connection between computability theory and machine learning', 'Thorough discussion of implications and alternatives']","['Highly theoretical with limited discussion of practical implications', 'Lack of empirical validation or concrete examples', 'Dense and technical writing may be challenging for some readers', 'Focuses mainly on worst-case scenarios that may not reflect typical use']",4,4,3.0,3,Accept
RxplU3vmBx,"This paper introduces Cost-Free Incremental Learning (CF-IL), a novel approach for incremental learning that uses a 'memory recovery paradigm' to synthesize examples of previously learned classes without storing old data or using auxiliary networks. The method aims to mitigate catastrophic forgetting and shows strong performance on standard benchmarks like CIFAR-10 and Tiny-ImageNet compared to state-of-the-art methods.","['Can you provide more theoretical justification for why the memory recovery paradigm is effective?', 'How does the method scale to larger datasets or longer task sequences?', 'How sensitive is the performance to the choice of hyperparameters, particularly the transfer set size?', 'Have you compared the method to very recent continual learning approaches not included in the current experiments?', 'Are there potential applications of CF-IL beyond image classification tasks?']","['May not scale well to very large datasets or long task sequences', 'Performance depends on tuning of some hyperparameters', 'Theoretical understanding and guarantees are limited', 'Evaluation is limited to image classification tasks']",False,3,3,4,7,4,"['Novel approach that eliminates the need for memory buffers or auxiliary networks', 'Strong empirical results on standard benchmarks, outperforming state-of-the-art methods', 'Addresses an important problem in continual learning', 'Applicable to both class-incremental and task-incremental settings', 'Detailed ablation studies and analysis provided']","['Limited theoretical justification for the effectiveness of the memory recovery paradigm', 'Evaluation limited to only two datasets (CIFAR-10 and Tiny-ImageNet)', 'Potential scalability issues for larger datasets or longer task sequences', 'Some aspects of the method, particularly the memory recovery process, could be explained more clearly', 'Introduces new hyperparameters that may require tuning']",4,3,3.0,4,Accept
PgNEYaIc81Q,"This paper introduces ComPhy, a novel benchmark for physical reasoning in videos, focusing on inferring hidden properties like mass and charge from object interactions. It also proposes a Compositional Physics Learner (CPL) as an oracle model baseline approach. The benchmark requires models to learn from few examples and reason about object dynamics in factual, counterfactual, and predictive scenarios.","['How might the approach be extended to handle more complex real-world scenes with additional physical properties?', 'What are the key challenges preventing the CPL model from approaching human-level performance?', 'Have you considered expanding the dataset to include other hidden properties beyond mass and charge?']","['Dataset is limited to relatively simple physical scenarios with a small number of objects', 'Focus is on inferring only mass and charge properties - other physical properties may be important', 'Current models still struggle with generalization to more complex scenes', 'Synthetic nature of dataset may not capture full complexity of real-world physical reasoning']",False,4,4,4,7,4,"['Novel and important benchmark for physical reasoning in videos', 'Few-shot learning setup that requires generalization from limited examples', 'Well-designed dataset with factual, counterfactual, and predictive questions', 'Focus on inferring hidden physical properties from interactions', 'Proposed CPL model provides a reasonable baseline approach', 'Results highlight significant room for improvement in physical reasoning capabilities', 'Comprehensive evaluation of various baseline models']","['Limited set of physical properties (only mass and charge)', 'Significant performance gap between CPL model and human performance', 'Limited generalization to more complex scenes with more objects', 'Synthetic nature of the dataset may not fully capture real-world physical reasoning']",4,3,4.0,4,Accept
qI4542Y2s1D,"This paper introduces FILM, a modular approach for embodied instruction following, achieving state-of-the-art performance on the ALFRED benchmark without requiring low-level instructions or expert trajectories. Key components include explicit language processing, semantic mapping, a semantic search policy, and deterministic planning.","['How well would FILM generalize to other embodied instruction following tasks beyond ALFRED?', 'Could the language processing module be extended to handle more open-ended instructions without templates?', 'How does the computational efficiency/runtime of FILM compare to end-to-end approaches?', 'Have you considered learning the low-level policy instead of using a deterministic one?', 'Have you tested FILM in any real-world robotic systems?', 'Are there any potential improvements to the semantic mapping module that could further enhance performance?']","['Evaluation limited to ALFRED benchmark and simulated environments', 'Reliance on task-specific templates may limit generalization', 'Performance still low for practical real-world use', 'May not handle arbitrary instructions beyond the 7 task types in ALFRED', 'Potential bias towards North American homes due to training data']",False,4,4,4,8,4,"['Novel modular architecture with explicit components for key sub-tasks', 'State-of-the-art performance on ALFRED benchmark without low-level instructions or expert trajectories', 'Effective semantic search policy for object localization and exploration', 'Thorough ablation studies and error analysis', 'Improved generalization to unseen environments', 'Good interpretability due to modular approach', 'Strong reproducibility efforts with code and pre-trained models provided']","['Reliance on hand-designed templates for language processing', 'Limited evaluation on only one benchmark (ALFRED)', 'Deterministic action policy rather than learned', 'Absolute performance still relatively low for practical real-world use', 'May not generalize well to tasks or environments beyond ALFRED']",4,4,3.0,4,Accept
uy602F8cTrh,"This paper introduces CausalDyna, a novel model-based reinforcement learning method that leverages structural causal models and counterfactual reasoning to enhance generalization in robotic manipulation tasks. The key innovation is the generation of simulated episodes with modified object properties, allowing the agent to practice with a broader range of scenarios than encountered in real training. Experiments on the CausalWorld benchmark demonstrate improved generalization to unseen object properties and better performance on rarely seen properties compared to MBPO and SAC baselines.","['How sensitive is the method to the accuracy of the world model?', 'How well would this approach scale to more complex multi-object manipulation scenarios?', 'Could this approach be combined with other techniques like meta-learning to further improve generalization?', 'Is there a way to provide theoretical guarantees on the generalization improvements?', 'What strategies could be employed to improve the accuracy of the world model for more complex environments?']","['Performance is limited by the quality of the learned world model', 'Only fully observable object properties are considered', 'Experiments are limited to relatively simple tasks', 'Potential negative impacts not explicitly discussed, but seem limited for this type of work', 'Lack of comparison to other generalization techniques in RL']",False,3,4,3,6,4,"['Novel combination of causal modeling, counterfactual reasoning, and model-based RL', 'Clear improvement in generalization to unseen object properties', 'Improved sample efficiency, especially in scenarios with unbalanced training distributions', 'Well-designed experiments and clear presentation of results', 'Addresses an important problem in robotic RL (generalization to new objects)']","['Relies heavily on having an accurate causal world model, which may be challenging for complex environments', 'Experiments limited to relatively simple manipulation tasks', 'Limited theoretical analysis or formal guarantees', 'Unclear scalability to more complex, multi-object manipulation scenarios']",4,3,4.0,4,Accept
iaqgio-pOv,"This paper introduces two novel methods for explaining black-box similarity models: a feature-based approach using Mahalanobis distance approximation, and an analogy-based method. These techniques address an important gap in explainable AI for similarity models. The proposed methods are evaluated on both text and tabular datasets through quantitative metrics and a user study, demonstrating their effectiveness and applicability across different domains.","['How do the proposed methods scale to larger datasets or higher-dimensional inputs?', 'What are the key limitations of the proposed methods?', 'How might the analogy-based method be adapted when a large dataset is not available?', 'How interpretable is the Mahalanobis distance explanation for non-expert users?']","['Feature-based method may not scale well to very high-dimensional data due to O(d^3) complexity of SDP', 'Explanations could potentially be misused to reverse-engineer proprietary models', 'User study results may not generalize to all types of users', 'Analogy-based method requires access to a large dataset', 'Explanations could potentially reveal private information about training data']",False,3,3,4,7,4,"['Addresses an important and under-explored problem in explainable AI', 'Proposes novel explanation methods, particularly the analogy-based approach', 'Provides theoretical analysis (submodularity proof for analogy method)', 'Conducts comprehensive evaluation including quantitative metrics and a user study', 'Demonstrates applicability on multiple datasets and domains']","['Limited discussion of limitations and potential negative impacts', 'User study has a relatively small sample size (41 participants)', ""Analogy-based method may require large datasets, which aren't always available"", 'Feature-based method using SDP might be computationally expensive for high-dimensional data', 'Limited comparison to very recent baseline methods']",4,3,3.0,3,Accept
JmU7lyDxTpc,"This paper analyzes epoch-wise double descent in neural networks using statistical physics tools. The authors introduce a linear teacher-student model exhibiting double descent, derive closed-form expressions for generalization dynamics, and explain the phenomenon through multi-scale feature learning. Results are validated through simulations and compared to deep neural network behavior.","['How do the insights from the linear model translate to nonlinear deep networks?', 'What specific training strategies could be derived from this theoretical understanding?', 'How might this analysis be extended to more complex loss landscapes?', 'What are the limitations of using statistical physics tools for neural network analysis?']","['Analysis limited to simplified linear models', 'May not fully capture complex neural network training dynamics', 'Practical implications of theoretical results not extensively explored', 'Statistical physics approach may have inherent limitations in modeling neural networks']",False,3,3,3,7,4,"['Novel application of statistical physics to neural network dynamics', 'Provides analytical insights into epoch-wise double descent', 'Derives closed-form expressions for generalization dynamics', 'Validates theoretical results through simulations', 'Introduces a simple model capturing key behaviors of complex networks']","['Analysis limited to linear models, generalization to nonlinear networks unclear', 'Experiments on real neural networks are limited', 'Broader implications and practical applications not extensively discussed', 'Mathematical derivations may be challenging for some readers']",3,3,3.0,4,Accept
ZC1s7bdR9bD,"This paper introduces a novel method for attributing uncertainty in Bayesian deep learning models using path integrals. The approach combines ideas from counterfactual explanations, variational autoencoders (VAEs), and integrated gradients. Key components include using a low-uncertainty counterfactual reference point, defining in-distribution paths in VAE latent space, and computing path integrals of uncertainty gradients. Experiments on image datasets demonstrate improved interpretability compared to baseline methods.","['How does the method generalize to non-image data types (e.g., text, graphs)?', 'What is the computational cost compared to existing approaches?', 'How sensitive are the results to the quality of the VAE used?', 'Have you evaluated the method on any regression tasks or higher-dimensional inputs?', 'Are there potential applications of this method beyond model interpretability?']","['Currently only demonstrated on image classification tasks', 'Effectiveness may depend on VAE quality', 'Computational requirements may limit scalability to very large models', 'Method requires differentiable models and uncertainty measures', 'Interpretation of results may be challenging for non-expert users']",False,3,3,3,7,4,"['Novel combination of techniques for uncertainty attribution in Bayesian deep learning', 'Theoretically grounded approach with desirable properties like completeness', 'Empirical results showing improved interpretability compared to baselines', 'Addresses an important problem in Bayesian deep learning interpretability', 'Clear presentation and well-written paper']","['Evaluation limited to image datasets, generalizability to other modalities unclear', 'Reliance on VAE quality for in-distribution paths', 'Computational complexity and scalability not thoroughly discussed', 'Added complexity compared to simpler attribution methods']",4,3,3.0,3,Accept
qnQN4yr6FJz,"This paper introduces vDIG, a novel method for discriminative learning with incomplete features using generative models. The key contributions are a new variational upper bound (EUBO) and a surrogate parameterization technique for stable optimization. Empirical results demonstrate improved performance over baseline methods on several datasets with missing data.","['How does the computational complexity and runtime of vDIG compare to baseline methods?', 'How well would this method scale to larger, high-dimensional datasets?', 'Have you explored ways to simplify the method to improve its practical applicability?', 'How does vDIG compare to recent deep learning methods for handling missing data?']","['Complexity of the method may limit practical adoption', 'Evaluation limited to small/medium datasets - scalability unclear', 'Potential computational overhead compared to simpler methods', 'Limited discussion of potential negative societal impacts']",False,3,3,3,7,4,"['Novel variational approximation (EUBO) for upper-bounding log integrals', 'Surrogate parameterization technique for stable optimization', 'Principled approach to handling missing data in ML models', 'Improved performance over existing methods on multiple datasets', 'Thorough theoretical analysis and justification', 'Demonstrated robustness to feature corruption']","['Complexity of the method may limit practical applicability', 'Experiments limited to small/medium datasets, raising scalability concerns', 'Limited discussion on computational complexity and runtime compared to baselines', 'Lack of comparison to some recent deep learning approaches for missing data']",3,3,4.0,4,Accept
HuaYQfggn5u,"This paper introduces FedBABU, a novel federated learning algorithm that only updates and aggregates the body (feature extractor) of the model during training while keeping the head (classifier) fixed. The authors demonstrate that this approach improves personalization performance compared to FedAvg and other personalized FL methods, especially under high data heterogeneity. Extensive experiments and analysis are provided to support the effectiveness of FedBABU across various datasets and settings.","['Can you provide theoretical analysis or convergence guarantees for FedBABU?', 'How does FedBABU perform on non-image domains like text or tabular data?', 'What is the computational overhead of FedBABU compared to FedAvg, including the fine-tuning step?', ""How sensitive is the performance to the choice of which layers are considered the 'body' vs 'head'?"", 'What are some potential real-world applications where FedBABU could have a significant impact?']","['Only evaluated on image classification tasks', 'Lack of theoretical analysis', 'Potential computational overhead not thoroughly discussed', 'May not generalize to all model architectures or domains', 'Long-term stability of the fixed head approach needs further investigation']",False,3,3,3,7,4,"['Novel and simple modification to federated averaging that consistently improves performance', 'Comprehensive empirical evaluation across different datasets, architectures, and FL settings', 'Insightful analysis explaining why updating only the body improves representation learning and personalization', 'Demonstrates applicability to other FL algorithms like FedProx', 'Well-written and clearly presented']","['Lacks theoretical analysis or convergence guarantees', 'Experiments limited to image classification tasks', 'Computational overhead of fine-tuning not thoroughly analyzed', 'May not generalize to all model architectures or domains']",4,3,3.0,4,Accept
YmONQIWli--,"This paper presents an efficient SDE solver for score-based generative models, using adaptive step sizes and extrapolation to accelerate the reverse diffusion process. The method achieves 2-10x faster generation compared to baselines while maintaining or improving sample quality, particularly for high-resolution images. It eliminates the need for manual step size tuning.","['Can the proposed method be extended to other domains beyond image generation?', 'How does the method compare theoretically to other adaptive SDE solvers?', 'Is there a principled way to set the relative tolerance parameter without tuning?', 'How does the approach perform on even higher resolution images (e.g., 1024x1024)?', 'Can this method be applied to other types of diffusion processes beyond VP and VE?']","['Only evaluated on image generation tasks', 'Potential for increased errors from score network approximation when taking many small steps', 'May not be optimal for all types of score-based models or diffusion processes', 'Limited theoretical guarantees provided']",False,3,3,3,7,4,"['Novel SDE solver design specifically addressing challenges in score-based generative models', 'Significant speedups (2-10x) while maintaining or improving sample quality', 'Comprehensive empirical evaluation on multiple datasets and model types', 'Removes need for manual step size/schedule tuning', 'Particularly strong results for high-resolution image generation']","[""Limited theoretical analysis of the proposed method's properties"", 'Experiments focused solely on image generation tasks', 'Still requires tuning of a relative tolerance parameter', 'Limited comparison to some very recent fast sampling techniques (e.g., DDIM)']",3,3,3.0,4,Accept
4jUmjIoTz2,"This paper introduces CDA^2, a novel collaboration framework for improving adversarial robustness using multiple sub-models. The approach allows sub-models to specialize on different vulnerability areas and collaborate to make predictions, outperforming standard ensemble methods. Theoretical analysis and empirical results on CIFAR-10 demonstrate improved robustness against both white-box and black-box attacks.","['How does the training time and computational overhead of CDA^2 compare to standard adversarial training and ensemble methods?', 'Could the approach be extended to other tasks beyond image classification?', 'How does CDA^2 compare to state-of-the-art single model defenses?', 'How sensitive is the performance to the number of sub-models and other hyperparameters?', 'How well does the approach scale to larger datasets and model architectures?']","['Increased computational cost compared to single models', 'Only evaluated on CIFAR-10 dataset with ResNet-20 architecture', 'Potential issues inherited from adversarial training like robust overfitting', 'May not be suitable for resource-constrained deployment scenarios']",False,3,3,3,7,4,"['Novel collaboration approach that improves upon ensembles for adversarial robustness', 'Theoretical analysis demonstrating the benefits of collaboration over ensembling', 'Comprehensive experiments showing improved robustness on white-box and black-box attacks', 'Addresses an important problem in adversarial machine learning', 'Well-motivated approach with clear intuition', 'Innovative dual-head structure of sub-models (label head and PPD head)']","['Additional computational overhead compared to single models', 'Limited experimental evaluation (only on CIFAR-10 with ResNet-20)', 'Lack of comparison to state-of-the-art single model defenses', 'Potential scalability issues with larger numbers of sub-models', 'Limited discussion of practical considerations for deployment']",3,4,3.0,3,Accept
q9zIvzRaU94,"This paper introduces State-Dependent Causal Inference (SDCI), a novel method for causal discovery in non-stationary time series data. SDCI models non-stationarity as conditional stationarity dependent on latent state variables. The method is implemented using deep probabilistic models and evaluated on synthetic datasets, showing improved performance over baseline methods. This work is significant as it addresses the challenge of causal discovery in more realistic, non-stationary scenarios.","['How well does the method scale to larger, more complex systems?', 'Have you evaluated the method on any real-world datasets?', 'Are there theoretical guarantees on identifiability of the causal structure?', 'How sensitive is the method to violations of the first-order Markov assumption?', 'What are some potential real-world applications where SDCI could be particularly useful?']","['Only evaluated on synthetic data', 'May not scale well to high-dimensional scenarios, which could limit applicability to complex real-world systems', 'Assumes first-order Markov property which may not hold for all real-world scenarios', 'Limited theoretical analysis']",False,3,3,3,6,4,"['Addresses an important problem of causal discovery in non-stationary data', 'Novel formulation incorporating latent state variables', 'Comprehensive empirical evaluation on synthetic datasets', 'Promising results showing improved performance over baselines', 'Demonstrates flexibility in handling different scenarios of state observability']","['Evaluation limited to synthetic datasets, no real-world data experiments', 'Limited comparison to other state-of-the-art methods', 'Computational scalability to larger systems not thoroughly analyzed', 'Theoretical analysis of identifiability conditions is limited']",3,3,3.0,3,Accept
JV4tkMi4xg,"This paper introduces NN+MILP, a novel framework for discrete black-box optimization that combines piecewise-linear neural networks as surrogate models with mixed-integer linear programming (MILP) for exact optimization of the acquisition function. The approach is evaluated on a range of unconstrained and constrained problems, including neural architecture search, demonstrating competitive or superior performance compared to strong baselines.","['How does the method scale to problems with thousands of variables?', 'Have you explored using more expressive surrogate models while maintaining MILP compatibility?', 'Can you provide any theoretical bounds on the optimality gap for the overall black-box optimization problem?', 'How sensitive is the performance to the choice of neural network architecture for the surrogate model?']","['The approach is limited to piecewise linear surrogate models', 'Scalability to very high-dimensional problems is not fully demonstrated', 'Lack of theoretical analysis or guarantees', 'Potential negative environmental impact due to computational resource requirements']",False,4,3,4,8,4,"['Novel combination of neural network surrogates and MILP for discrete optimization', 'Flexible framework capable of handling complex constraints', 'Strong empirical results on diverse benchmark problems', 'Practical MILP solve times for problems of reasonable scale', 'Exact optimization of acquisition function, avoiding heuristic approximations']","['Limited to piecewise linear surrogate models', 'Potential scalability issues for very high-dimensional problems', 'Lack of theoretical guarantees or in-depth analysis', 'Some baseline comparisons missing for certain problem types']",4,4,4.0,4,Accept
pC00NfsvnSK,"This paper introduces Generalized Similarity Functions (GSF), a method for improving zero-shot generalization in offline reinforcement learning using generalized value functions and contrastive learning. It is evaluated on a new offline Procgen benchmark, showing improvements over baselines.","['How sensitive is the method to the choice of number of quantile bins and cumulant function?', 'How does GSF perform on other offline RL benchmarks beyond Procgen?', 'Can you provide additional ablation studies isolating the impact of different components of GSF?', 'What is the computational overhead of GSF compared to baseline methods?']","['Evaluation limited to offline Procgen benchmark', 'Theoretical results provide limited guarantees', 'Potential negative impact of improved RL generalization not thoroughly discussed']",True,3,3,4,6,4,"['Novel approach combining GVFs, quantile binning, and contrastive learning for offline RL generalization', 'Introduction of a valuable new offline Procgen benchmark for evaluating generalization', 'Consistent empirical improvements over strong baselines', 'Provides theoretical analysis justifying the approach', 'Addresses an important problem (generalization in offline RL)']","['Empirical evaluation limited to Procgen environments', 'Lack of ablation studies on key components', 'Theoretical results not tightly connected to empirical approach', 'Some hyperparameters (e.g., number of quantile bins) require tuning']",4,3,3.0,4,Accept
LOz0xDpw4Y,"This paper presents a dynamic programming approach to optimize inference schedules for pre-trained denoising diffusion probabilistic models (DDPMs). The method finds likelihood-optimal schedules that achieve competitive performance with far fewer sampling steps (e.g., 32 vs 1000+) without requiring model retraining. Experiments on CIFAR-10 and ImageNet 64x64 demonstrate strong results, matching full model log-likelihoods with significantly reduced computational cost.","['How does the proposed method compare to very recent fast sampling approaches for diffusion models in terms of speed vs quality trade-offs?', 'Can you provide more analysis of why the discovered schedules are effective, beyond the empirical results?', 'Have you explored other objectives beyond ELBO that may better correlate with sample quality?', 'How does the computational cost of the dynamic programming optimization compare to the savings from fewer refinement steps?']","['The method optimizes for likelihood which may not directly translate to improved sample quality', 'Evaluation is limited to image generation tasks, applicability to other domains is not demonstrated', 'The approach may not generalize well to other types of generative models beyond DDPMs']",False,4,3,3,7,4,"['Novel formulation of inference schedule optimization as a dynamic programming problem', 'Significant improvements in sampling efficiency while maintaining good log-likelihoods', 'Method works on pre-trained models without retraining', 'Broad applicability across different DDPM variants and datasets', 'Strong theoretical foundation and analysis', 'Thorough empirical evaluation and ablation studies']","[""Focus mainly on log-likelihood optimization, which doesn't always correlate with perceptual quality or sample quality metrics like FID"", 'Limited comparison to very recent fast sampling methods for diffusion models', 'Some computational overhead for schedule optimization', 'Potential trade-offs between likelihood and perceptual quality not fully resolved']",4,3,3.0,3,Accept
DBOibe1ISzB,"This paper introduces Simulation Transformer (SiT), a novel Transformer-based approach for particle-based physics simulation. Key innovations include interaction tokens to model complex particle interactions, abstract tokens to capture material properties, and a selective attention mechanism. SiT demonstrates superior performance and generalization compared to existing GNN-based methods across multiple simulation environments.","['How does the computational complexity of SiT scale with larger numbers of particles?', 'Can you provide more theoretical justification for the interaction and abstract token designs?', 'How sensitive is the model performance to the choice of hyperparameters?', 'Have you explored applying SiT to other types of physical simulations beyond particles?', 'What are the potential real-world applications of this work?']","['The method is only evaluated on particle-based simulations, not other physics domains', 'May not scale well to very large simulations due to quadratic complexity', 'Abstract tokens may not capture all relevant material properties']",False,3,3,4,7,4,"['Novel application of Transformers to particle-based physics simulation', 'Introduction of interaction tokens and abstract tokens to improve modeling capabilities', 'Strong empirical results across multiple simulation environments', 'Impressive generalization to new configurations and scenarios', 'Thorough ablation studies validating key components']","['Limited theoretical analysis or justification for the model architecture', 'Potential scalability issues with larger particle systems', 'Lack of thorough computational efficiency comparisons with existing methods', 'Experiments limited to relatively small-scale simulations', 'No comparison to other Transformer-based approaches in physics simulation (if any exist)']",4,4,3.0,4,Accept
Tubzedlc4P,"This paper introduces a Riemannian geometric structure for point cloud data by interpreting point clouds as samples from probability distributions. The authors develop a mathematical framework based on statistical manifolds and information geometry, demonstrating its effectiveness on autoencoder applications for point cloud interpolation and representation learning.","['How does the computational complexity scale with the number of points and dimensions?', 'Are there ways to relax the fixed number of points assumption?', 'How does this method compare to other recent geometric/topological approaches to point clouds?']","['Fixed number of points assumption', 'Computational complexity may limit scalability', 'Limited evaluation on real-world downstream tasks']",False,3,3,3,7,4,"['Rigorous mathematical framework combining statistical manifolds, information geometry, and point cloud analysis', 'Novel theoretical approach with formal definitions and proofs', 'Demonstrates practical benefits on autoencoder applications', 'Improvements over baselines, especially for noisy data and semi-supervised settings', 'Extensive experiments on synthetic and standard benchmark datasets']","['Computational complexity and scalability concerns for large point clouds', 'Assumes fixed number of points per point cloud, limiting real-world applicability', 'Limited comparison to other recent geometric approaches for point clouds', 'Some mathematical details are dense and could use more intuitive explanations']",4,3,3.0,4,Accept
bTteFbU99ye,"This paper introduces a novel methodology for evaluating how neural language models estimate probabilities for rare sequences, using generative models trained on natural data as artificial languages with known probabilities. Key findings reveal that neural LMs systematically underestimate probabilities of rare sequences and overestimate probabilities of perturbed sequences. The paper also analyzes how factors like training data size and target distribution entropy affect this behavior.","['How well do the artificial languages match key statistical properties of natural language distributions?', 'Do the findings generalize to other types of language models beyond autoregressive models?', 'What are the implications of these findings for downstream applications of language models?', 'Are there ways to mitigate the underestimation problem through architectural changes or training procedures?', 'Are there any theoretical explanations for the observed underestimation and overestimation behaviors?']","['Artificial languages may not fully capture natural language properties', 'Findings are limited to autoregressive language models', 'Limited exploration of practical implications and potential solutions', 'Potential negative impacts not extensively discussed']",False,3,4,3,7,4,"['Novel evaluation methodology using artificial languages with known probabilities', 'Thorough empirical analysis across multiple model architectures and sizes', 'Important findings on LM behavior for rare and perturbed sequences', 'Careful ablation studies examining effects of data size, distribution entropy, etc.', 'Clear writing and presentation of results']","['Artificial languages may not fully capture properties of natural language distributions', 'Experiments only cover autoregressive LMs, not other architectures', 'Limited discussion of implications or potential solutions', 'Some aspects of methodology could be explained in more detail']",3,3,4.0,4,Accept
dIVrWHP9_1i,"This paper proposes G-Mixup, a graph data augmentation method that generates synthetic graphs by interpolating graphons of different graph classes. Theoretical analysis and experiments demonstrate improved performance and robustness of graph neural networks on graph classification tasks.","['How might G-Mixup be extended to handle very large graphs more efficiently?', 'What strategies could be employed to ensure synthetic graphs better preserve important properties of the original graphs?', 'How could G-Mixup be adapted for node or edge classification tasks?', 'Are there specific graph properties or dataset characteristics where G-Mixup is particularly effective or ineffective?']","['Current applicability limited to graph-level tasks', 'Computational complexity may be prohibitive for very large graphs', 'Potential loss of fine-grained structural information in synthetic graphs', 'Limited evaluation on diverse graph types (e.g., heterogeneous graphs)']",False,3,3,4,7,4,"['Novel approach to graph augmentation using graphon mixing', 'Strong theoretical foundation with analysis of graph property preservation', 'Comprehensive experiments showing consistent improvements across datasets and architectures', 'Broad applicability to different graph neural network architectures', 'Addresses limitations of existing graph augmentation methods']","['Potential scalability issues for very large graphs', 'Limited analysis of hyperparameter sensitivity', 'Synthetic graphs may not preserve all properties of original graphs', 'Moderate performance gains on some datasets']",4,3,3.0,4,Accept
FFGDKzLasUa,"This paper introduces StochLWTA-ML, a novel meta-learning approach that combines stochastic local winner-takes-all (LWTA) units with Bayesian inference in deep neural networks. The method achieves state-of-the-art results on few-shot image classification benchmarks while using significantly fewer parameters than existing approaches.","['Can the authors provide more theoretical insight into why stochastic LWTA units work well for meta-learning?', 'How well does this approach generalize to other types of tasks beyond image classification?', 'Is there a way to reduce the computational overhead at test time while maintaining accuracy?']","['Approach only evaluated on image classification tasks', 'Potential increased inference time due to sampling', 'Theoretical understanding of why the method works is limited']",False,4,4,4,8,4,"['Highly original combination of stochastic LWTA units and Bayesian inference for meta-learning', 'Strong empirical results on standard few-shot learning benchmarks', 'Significant reduction in model size (order of magnitude fewer parameters) while improving accuracy', 'Thorough ablation studies demonstrating the benefits of key components', 'Clear and well-presented exposition of the method and experimental setup']","['Limited theoretical analysis or justification for why the approach works better', 'Experiments focused only on image classification tasks', 'Potential computational overhead at test time due to sampling']",4,3,4.0,4,Accept
ahi2XSHpAUZ,"This paper introduces WeakM3D, a novel weakly supervised approach for monocular 3D object detection that uses 2D bounding boxes and LiDAR point clouds as weak supervision signals instead of 3D box labels. The method incorporates several innovative techniques, including geometric alignment loss, ray tracing loss, loss balancing, and learning disentanglement to address challenges in the weakly supervised setting. Experiments on the KITTI dataset demonstrate competitive performance compared to some fully supervised methods.","['How well does the method generalize to object categories beyond cars?', 'Could the approach be adapted to work without LiDAR data, using only 2D supervision?', 'How sensitive is the method to the quality of 2D detections used as input?', 'What are the main limitations preventing performance from matching state-of-the-art fully supervised methods?', 'What is the computational efficiency of the method, and is it suitable for real-time applications?']","['The method still requires LiDAR data for training, which may not always be available', 'Performance on small, far, or partially observed objects may be limited', 'Assumes objects can be well-approximated by cuboids with fixed dimensions, which may not hold for all object types', 'Generalization to other datasets and object categories is not thoroughly explored', 'Potential challenges in real-time applications due to computational requirements']",False,4,4,4,7,4,"['Novel weakly supervised approach that significantly reduces annotation costs', 'Well-designed techniques to address key challenges in weakly supervised 3D detection', 'Competitive performance compared to some fully supervised methods', 'Extensive experiments and ablation studies validating the proposed components', 'Potential for practical impact by reducing reliance on expensive 3D labels']","['Performance still lags behind state-of-the-art fully supervised methods', 'Limited evaluation on datasets beyond KITTI and object categories other than cars', 'Reliance on LiDAR data for weak supervision, which may not always be available', 'Some design choices, like freezing object dimensions, lack thorough justification', 'May struggle with small, distant, or heavily occluded objects']",4,4,4.0,4,Accept
EZNOb_uNpJk,"This paper introduces ClimateGAN, a novel approach for generating realistic flood images in street-level scenes to raise awareness about climate change impacts. The key contributions include: (1) Proposing a new task of flood generation in street images, (2) Creating a dataset of paired flooded/non-flooded images from a virtual world, (3) Developing the ClimateGAN model with a Masker to predict flood masks and a Painter to generate flood textures, and (4) Conducting a thorough evaluation methodology including human studies. Experiments demonstrate that ClimateGAN outperforms baselines in generating realistic flood images.","['How well does the model generalize to real-world flood scenarios not seen in training?', 'What are the potential risks or negative impacts of generating fake flood images?', 'How might the model be extended to generate floods at different water levels?', 'Have you evaluated the effectiveness of the generated images for actually raising climate change awareness?', 'What are the most promising directions for future work to improve or extend ClimateGAN?']","['Reliance on simulated data may limit real-world applicability', 'Potential to spread misinformation if fake flood images are misused', 'High computational cost and carbon footprint of training the model', 'Limited ability to control flood height/severity']",False,4,4,4,7,4,"['Novel and well-motivated task with potential societal impact on climate change awareness', 'Creation of a new dataset to enable the research', 'Well-designed model architecture combining multiple techniques', 'Extensive experiments and ablation studies', 'Comprehensive evaluation methodology including human studies', 'Clear writing and presentation', ""Responsible reporting of the model's carbon footprint""]","['Limited discussion of ethical considerations and potential negative impacts', 'Lack of comparison to very recent image translation methods', 'Evaluation mostly on simulated data, raising questions about real-world generalization', 'Some limitations in the current approach, such as fixed flood height', 'Computational cost and environmental impact of training the model not fully addressed']",4,3,4.0,4,Accept
FKp8-pIRo3y,"This paper introduces HinDRL, a novel method that combines demonstrations with hindsight experience replay to improve sample efficiency in long-horizon robotic manipulation tasks. The key innovation is using demonstration states to guide goal sampling for hindsight relabeling, which helps constrain the self-supervision process to task-relevant goals. HinDRL is evaluated on four challenging manipulation tasks, showing significant improvements over baselines, especially in low-demonstration regimes.","['How well would the method transfer to real-world robotic systems?', 'Could the approach be extended to work with raw visual observations instead of state-based inputs?', 'How does HinDRL compare to other recent demo-augmented RL methods like DAPG or XIRL?', 'Is there a principled way to determine the optimal number of hindsight samples per task?']","['Experiments limited to simulated environments', 'May require careful tuning of hyperparameters like number of hindsight samples', 'Potential challenges in scaling to very high-dimensional observation spaces', 'Performance may degrade with very low-quality demonstrations']",False,3,3,3,7,4,"['Novel combination of demonstrations and hindsight relabeling', 'Strong empirical results on complex, long-horizon robotic manipulation tasks', 'Improved sample efficiency and performance over baselines, especially with few demonstrations', 'Thorough ablation studies and analysis', 'Clear writing and sound technical approach']","['Evaluation limited to simulated environments, not tested on real robots', 'Some reliance on engineered state representations for best performance', 'Limited theoretical analysis or guarantees', 'Missing comparisons to some recent related work']",3,3,3.0,3,Accept
GugZ5DzzAu,"This paper extends the analysis of the MARINA algorithm for distributed nonconvex optimization to support correlated compressors. It introduces a new 'Hessian variance' quantity for refined analysis, proposes new 'PermK' compressors based on random permutations, and provides improved convergence bounds. The key contributions are theoretical, showing potential gains in communication complexity, especially when Hessian variance is small. The paper includes experimental validation on synthetic quadratic problems and MNIST autoencoder training.","['How well do the theoretical improvements translate to practical gains on large-scale machine learning problems beyond MNIST?', 'Can you provide examples of real-world scenarios where the Hessian variance is likely to be small enough to see significant gains from your method?', 'How robust are the improvements to violations of the theoretical assumptions in practical settings?', 'Could you provide guidelines for practitioners on how to implement and leverage the insights from this work in applied distributed optimization problems?']","The paper focuses primarily on theoretical analysis with limited large-scale empirical evaluation. The practical applicability of the method may be constrained by the assumptions made, particularly regarding Hessian variance. The high technical complexity may pose challenges for practitioners attempting to implement and apply the findings in real-world scenarios.",False,4,3,3,7,4,"['Rigorous theoretical analysis extending MARINA to correlated compressors', ""Introduction of 'Hessian variance' allows for more refined bounds"", ""Novel 'PermK' compressors with provably better communication complexity in some regimes"", 'Improved convergence bounds, especially for low Hessian variance cases', 'Experimental validation aligning with theoretical predictions on synthetic and MNIST problems']","['Limited experimental evaluation on large-scale, real-world problems', 'Practical impact and applicability of theoretical gains not fully demonstrated', 'Analysis is quite technical and may have limited accessibility to practitioners', 'Assumptions required for theoretical gains may not always hold in practice']",3,4,3.0,3,Accept
RQ428ZptQfU,"This paper introduces VaDeSC, a novel deep probabilistic approach for clustering survival data. It extends variational autoencoders with a Gaussian mixture prior to jointly model covariates and censored survival outcomes. The method is comprehensively evaluated on synthetic, semi-synthetic, and real datasets, demonstrating improved clustering performance over baselines while maintaining competitive survival prediction. An application to lung cancer CT imaging data showcases the discovery of clinically meaningful patient subgroups, highlighting the method's potential in precision medicine.","['How sensitive is the method to the choice of number of clusters?', 'How does the method scale to datasets with millions of samples or features?', 'Could the approach be extended to automatically determine the optimal number of clusters?', 'What additional clinical validation would be needed to assess real-world utility for the lung cancer application?', 'How does the method perform under different censoring rates?']","['Requires manual specification of number of clusters', 'Scalability to extremely large or high-dimensional datasets not demonstrated', 'Limited interpretability of latent space', 'Potential negative impacts if used for clinical decisions without proper validation', 'Performance under varying censoring rates not fully explored']",False,4,3,4,7,4,"['Novel probabilistic model combining deep generative models with survival analysis', 'Comprehensive experimental evaluation on multiple datasets', 'Outperforms baselines on clustering performance', 'Competitive survival prediction performance', 'Successful application to lung cancer CT imaging data', 'Discovers clinically meaningful and interpretable clusters']","['Requires specifying number of clusters a priori', 'Limited scalability to very large or high-dimensional datasets', 'Limited interpretability of learned latent representations', 'Some baseline comparisons (e.g., profile regression) are limited due to scalability issues', 'Potential sensitivity to hyperparameter choices']",4,4,3.0,3,Accept
5Qkd7-bZfI,This paper investigates the relationship between population size and language structure in emergent communication systems using neural agents. It addresses a discrepancy between sociolinguistic findings and neural emergent communication models regarding the effect of population size on language structure. The authors demonstrate that introducing heterogeneity in learning speeds can recover the sociolinguistic trend of larger populations leading to more structured languages. The paper provides insights into the importance of population dynamics and relative speaker-listener relationships in shaping emergent language properties.,"['How well do these results generalize to more complex communication tasks beyond the Lewis game?', 'Can you provide more theoretical justification for why heterogeneity in learning speeds leads to the observed effects?', 'Have you explored other forms of agent heterogeneity beyond learning speed? What other factors do you think could be important?', 'What are the implications of these findings for designing multi-agent language learning systems?', 'How do the results relate to theories of cultural evolution in linguistics?']","['Results are limited to a simple Lewis game setting and may not generalize to more complex linguistic scenarios', 'Focus is mainly on one type of heterogeneity (learning speed), other forms of diversity not explored in depth', 'Theoretical explanations for some of the observed effects are limited', 'Connections drawn to human language evolution are somewhat speculative']",False,3,3,3,7,4,"['Addresses an important discrepancy between sociolinguistics and computational models of emergent communication', 'Introduces a novel approach of heterogeneous populations in language emergence models', 'Well-designed experiments that isolate effects of population size and heterogeneity', 'Provides novel insights about the importance of population dynamics in language emergence', 'Clear connections drawn to sociolinguistic literature', 'Rigorous evaluation using multiple metrics of language quality', 'Opens up interesting directions for further research on population dynamics in language emergence']","['Experiments limited to a simple Lewis signaling game setup, raising questions about generalizability', 'Theoretical justification for why heterogeneity matters could be stronger', 'Focus mainly on learning speed heterogeneity - other types of agent diversity not explored in depth', 'Some results are empirical observations without deeper theoretical explanations', 'Limited discussion of broader implications and potential negative societal impacts']",4,3,3.0,3,Accept
0JzqUlIVVDd,"This paper proposes a novel domain adaptation method using KL divergence to align representations between source and target domains. The authors derive a generalization bound, demonstrate efficient optimization, and show improved performance over other marginal alignment techniques on several benchmarks.","['How does the method perform compared to state-of-the-art domain adaptation techniques?', 'What are the practical limitations of the theoretical assumptions?', 'How might this approach be extended to multi-source domain adaptation?']","['Applicability limited to single-source domain adaptation', 'Potential gap between theoretical assumptions and real-world datasets', 'Focus on marginal alignment may limit broader applicability']",False,3,3,3,7,4,"['Novel theoretical analysis using KL divergence for domain adaptation', 'Efficient optimization method avoiding adversarial training', 'Strong empirical results across multiple datasets', 'Theoretically grounded approach with sound justification']","['Only compares to marginal alignment techniques, not more advanced methods', 'Theoretical assumptions may not always hold in practice', 'Limited to single-source domain adaptation setting']",3,3,3.0,3,Accept
-6me0AsJVdu,"This paper introduces Mutation Validation (MV), a novel model validation technique that mutates training data labels to assess model fit without requiring a separate validation set. Extensive experiments are conducted to evaluate MV against traditional validation methods across multiple datasets, algorithms, and tasks. Results show MV outperforms cross-validation and test accuracy in model selection and hyperparameter tuning.","['How does MV perform on more diverse ML tasks beyond classification?', 'What are the practical considerations for integrating MV into real-world ML pipelines?', 'How does MV compare to other advanced validation techniques beyond basic cross-validation?', 'What theoretical guarantees can be provided for the effectiveness of MV?', 'How sensitive is MV to the choice of mutation strategy and degree?', 'How does MV perform on imbalanced datasets?']","['Evaluation limited to classification tasks', 'Theoretical analysis could be more rigorous', 'Practical deployment considerations not fully explored', 'Long-term stability and generalizability unclear', 'Computational cost for large datasets or complex models not thoroughly explored', 'Potential sensitivity to different types of data distributions not explicitly addressed']",False,3,3,4,7,4,"['Novel approach to model validation that does not require a separate validation set', 'Comprehensive empirical evaluation across diverse datasets and algorithms', 'Theoretical foundations provided to support the approach', 'MV shows improved effectiveness and stability over traditional methods in experiments', 'Potential to complement existing validation practices']","['As an exploratory study, more rigorous theoretical analysis may be needed', 'Broader experiments on more diverse tasks and model types would strengthen claims', 'Practical considerations for applying MV in real-world ML pipelines not fully addressed', 'Comparison to other advanced validation techniques beyond basic CV is limited', 'Long-term stability and generalizability of MV needs further investigation']",4,3,4.0,4,Accept
eIvzaLx6nKW,"This paper introduces Multi-Domain Self-Supervised Learning (MDSSL), a novel method for learning representations from multiple diverse datasets simultaneously. MDSSL employs a hierarchical loss function to encourage distinguishable representations across domains while maintaining good performance on individual domains. The method demonstrates improved performance over single-domain and multi-domain baselines on several benchmark datasets, better generalization to unseen domains, and can work without domain labels by incorporating clustering. MDSSL also offers significant resource efficiency compared to training separate models for each domain.","['How does the method scale to very large numbers of domains (e.g., 100+)? Can you provide computational complexity analysis?', 'Can you provide more theoretical justification for the effectiveness of the hierarchical loss function in MDSSL?', 'Have you tested MDSSL on tasks other than image classification or non-image modalities? If not, what challenges do you anticipate?', 'How sensitive is the performance to the choice of λ1 and λ2 hyperparameters? Can you provide guidelines for selecting these values?']","['Experiments are limited to relatively small datasets and image classification tasks', 'Theoretical understanding of why the method works is limited', 'Scalability to very large numbers of domains is not thoroughly analyzed', 'May require careful hyperparameter tuning in practice', 'Generalization to other types of data and tasks remains to be explored']",False,3,4,3,7,4,"['Novel approach to multi-domain self-supervised learning', 'Consistent performance improvements over baselines across multiple datasets', 'Better generalization to unseen domains', 'Ability to work without domain labels using clustering', 'More efficient in terms of compute and memory usage compared to training separate models', 'Comprehensive empirical evaluation and analysis']","['Limited theoretical analysis or justification for the proposed loss function', 'Evaluation primarily focused on image classification tasks', 'Potential scalability issues with very large numbers of domains', 'Some hyperparameters require tuning', 'Performance gains more modest on some dataset combinations']",3,3,4.0,3,Accept
xnYACQquaGV,"This paper introduces Neural-LinUCB, a novel algorithm for neural contextual bandits that combines deep neural networks for representation learning with UCB exploration only in the last linear layer. The authors provide theoretical analysis proving O(sqrt(T)) regret bounds under certain assumptions and demonstrate empirically that the algorithm can match or outperform existing methods while being more computationally efficient.","[""How sensitive is the algorithm's performance to the choice of neural network architecture?"", 'Can the approach be extended to more general reward structures beyond linear?', 'How does the algorithm scale to problems with very large action spaces?']","['Theoretical guarantees rely on strong assumptions that may not always hold in practice', 'Limited empirical evaluation on only 4 datasets', 'Potential negative societal impacts not adequately addressed']",False,3,3,4,7,4,"['Novel approach decoupling deep representation learning and shallow exploration', 'Rigorous theoretical analysis with sublinear regret bounds', 'Empirical results showing competitive or superior performance on real datasets', 'Improved computational efficiency compared to full neural UCB approaches', 'Addresses an important problem in contextual bandits']","['Theoretical assumptions may be too strong or unrealistic for practical settings', 'Limited empirical evaluation (only 4 datasets, few baselines)', 'Sensitivity to neural network architecture choices not thoroughly explored', 'Lack of discussion on limitations and potential negative societal impacts']",4,3,3.0,3,Accept
qQuzhbU3Gto,"This paper introduces a novel graph generative model based on nonnegative matrix factorization that can capture both homophily and heterophily in networks while providing interpretable community structure. The authors present theoretical results on the model's expressiveness and empirical evaluations on tasks such as link prediction and community detection, demonstrating competitive performance against existing methods.","['How does the model perform and scale on very large networks (millions of nodes)?', 'Can you provide a more in-depth analysis of the learned community structures, particularly in cases where heterophily is present?', 'How does the computational complexity of your model compare to existing methods, and what are the trade-offs in terms of expressiveness vs. efficiency?', 'Have you considered the potential implications of your model for privacy-preserving network analysis?']","['Experiments are limited to medium-sized networks', 'Scalability to very large graphs is not thoroughly addressed', 'Analysis of interpretability and learned structures could be more extensive', 'Potential negative societal impacts, such as privacy concerns in network analysis, are not discussed']",False,3,3,3,7,4,"['Novel approach addressing limitations in capturing heterophily', 'Strong theoretical results on model expressiveness', 'Competitive empirical performance on multiple tasks', 'Provides interpretable node embeddings and community structure', 'Effective optimization algorithm']","['Experiments are somewhat limited in scope and scale', 'Lack of thorough analysis on computational complexity and scalability', 'Missing comparisons to some relevant baselines, especially recent graph neural networks', 'Could benefit from more qualitative analysis of learned embeddings and communities']",4,3,3.0,3,Accept
SYuJXrXq8tw,"This paper proposes two sparse adversarial training methods, Robust Bird (static sparsity) and Flying Bird (dynamic sparsity), to improve robust generalization and computational efficiency in adversarial training. Extensive experiments across multiple datasets and architectures demonstrate that these methods can significantly reduce the robust generalization gap and training costs while maintaining or improving accuracy.","['Can you provide stronger theoretical justification for why sparsity improves robust generalization?', 'How do the proposed methods compare to very recent state-of-the-art adversarial training approaches?', 'Is there a more principled way to determine the adaptive sparsity in Flying Bird+?', 'Have you explored combining the proposed methods with other regularization techniques for adversarial training?', 'What potential applications do you see for these methods beyond image classification tasks?']","['The methods are only evaluated on image classification tasks', 'Potential negative impacts of reduced model capacity are not thoroughly discussed', 'Computational costs of finding initial sparse structures are not fully accounted for']",False,3,3,3,6,4,"['Novel approach of incorporating sparsity into adversarial training to improve robust generalization', 'Comprehensive empirical evaluation across multiple datasets and architectures', 'Significant improvements in robust generalization gap and computational efficiency', 'Well-designed methods for both static (Robust Bird) and dynamic (Flying Bird) sparsity', 'Interesting connection drawn between sparsity and robust generalization', 'Thorough ablation studies and informative visualizations to support the findings']","['Theoretical justification for why sparsity helps robust generalization could be stronger', 'Limited comparison to some recent state-of-the-art adversarial training methods', 'Flying Bird+ method with adaptive sparsity needs more explanation and justification', 'Some important results and ablations are relegated to the appendix']",4,3,3.0,3,Accept
O-r8LOR-CCA,"This paper introduces open-world semi-supervised learning (SSL), where unlabeled test data may contain both seen and novel classes. The authors propose ORCA, a method using an uncertainty adaptive margin mechanism to balance learning of seen and novel classes. Experiments show significant improvements over baselines.","['Can you provide theoretical justification or guarantees for the uncertainty adaptive margin approach?', 'How well does the method generalize to other types of data beyond images and single-cell data?', ""How does ORCA's computational complexity compare to the baselines, especially for large datasets or high numbers of classes?""]","['The method assumes a fixed number of novel classes, which may not always be known in practice', 'Evaluation is limited to image and single-cell datasets', 'Potential negative impacts of misclassifying novel classes not fully addressed']",False,3,4,4,7,4,"['Introduces a novel and important problem setting (open-world SSL)', 'Proposes an effective approach (ORCA) with an uncertainty adaptive margin mechanism', 'Demonstrates significant performance improvements over baselines', 'Provides comprehensive experiments and ablation studies', 'Clear writing and presentation']","['Lacks theoretical analysis or guarantees for the proposed method', 'Evaluation is mostly limited to image and single-cell datasets', 'Potential scalability issues for very large numbers of classes', 'Some concerns about the fairness of baseline adaptations']",4,3,4.0,4,Accept
9u5E8AFudRx,"This paper introduces Help Me Explore (HME), a novel social interaction protocol for integrating minimal social guidance into autotelic reinforcement learning agents, and GANGSTR, a graph-based autotelic agent architecture for manipulation tasks. GANGSTR uses Graph Neural Networks and a semantic graph to decompose complex goals into sequences of simpler sub-goals. The key innovation is allowing a social partner to suggest goals at the frontier of the agent's capabilities, which the agent then practices autonomously. Empirical results in a block stacking domain demonstrate that even a small amount of social guidance can significantly boost performance on complex manipulation tasks.","['How well would the approach generalize to other domains beyond block stacking?', 'How might the method perform with real human teachers instead of a simulated social partner?', 'What are the key challenges in scaling to higher-dimensional problems?', 'How does this compare to other recent social learning approaches in RL?', 'Can you elaborate on the internalization mechanism and its importance in the HME protocol?']","['Evaluation limited to a single simulated domain', 'Relies on simulated social partner rather than real human interaction', 'Potential scalability issues to more complex environments not fully addressed', 'Limited discussion of potential negative societal impacts, such as the ethical implications of developing highly teachable AI agents']",False,3,3,3,7,4,"['Novel combination of autotelic learning and social guidance', 'Graph-based agent architecture (GANGSTR) enables effective goal decomposition', 'Thorough empirical evaluation showing clear benefits over baselines', 'Demonstrates significant impact with minimal social interventions', 'Potential for developing more teachable robotic agents']","['Limited theoretical analysis or formal guarantees', 'Evaluation restricted to a single domain (block stacking)', 'Reliance on simulated social partner with perfect knowledge', 'Some design choices and hyperparameters not fully justified', 'Scalability to more complex domains not thoroughly explored']",4,3,3.0,3,Accept
EYCm0AFjaSS,"This paper introduces ZerO, a novel deterministic initialization method for residual networks using only zeros and ones. The approach involves adding extra skip connections and Hadamard transforms to enable training from this simple initialization. Empirical results on image classification tasks demonstrate that ZerO can match or exceed the performance of random initialization while improving reproducibility.","['How well does ZerO generalize to other architectures beyond ResNet?', 'Can you provide more theoretical insight into why ZerO initialization works so well?', 'How does ZerO perform on other tasks like object detection or segmentation?', 'What is the computational overhead of the Hadamard transforms in practice?', 'Given the observed weight distributions, could ZerO be particularly beneficial for network pruning applications?']","['Currently only demonstrated for ResNet-style architectures and image classification tasks', 'Lacks strong theoretical justification or guarantees', 'May not generalize well to other network architectures or domains', 'Potential increased memory usage and computational overhead due to extra skip connections and Hadamard transforms']",False,3,4,3,7,4,"['Highly original approach to neural network initialization', 'Strong empirical results on standard image classification benchmarks', 'Improves reproducibility by removing randomness in initialization', 'Enables training without batch normalization', 'Clear explanation of key ideas and motivation', 'Comprehensive ablation studies and experiments']","['Limited theoretical analysis or guarantees', 'Evaluation mostly restricted to ResNet architectures and image classification tasks', 'May not generalize well to other architectures or domains', 'Slight performance degradation in some cases compared to standard ResNet', 'Requires architectural modifications (extra skip connections, Hadamard transforms)']",4,4,4.0,3,Accept
MTsBazXmX00,"This paper presents a simplified target propagation (TP) method for training recurrent neural networks (RNNs), using regularized inverses and linearized propagation. The authors provide theoretical analysis comparing TP to gradient backpropagation (BP) and demonstrate empirically that TP outperforms BP, especially for long sequences. The paper also highlights the importance of regularization in TP.","['How does the proposed TP method compare to other recent TP variants and advanced optimizers like Adam?', 'Could this approach be extended to other neural network architectures beyond RNNs?', 'How sensitive is the method to hyperparameter choices, especially the regularization parameter?', 'How well does the method scale to larger, more complex network architectures and real-world tasks?']","['The approach is currently limited to RNNs', 'Computational complexity may be higher than BP for shorter sequences', 'Potential sensitivity to hyperparameter choices', 'Unclear generalizability to more complex models and problems']",False,3,4,3,7,3,"['Novel, simplified TP implementation grounded in theoretical principles', 'Clear theoretical analysis comparing TP to BP', 'Strong empirical results showing TP outperforms BP on several RNN tasks', 'Insights into the importance of regularization for TP', 'Clear exposition and well-written paper']","['Limited scope - focuses only on RNNs', 'Lack of comparison to other TP variants and advanced optimizers', 'Experiments focused mostly on synthetic/simple tasks', 'Some experimental details missing (e.g. hyperparameter tuning process)']",3,3,4.0,3,Accept
gjNcH0hj0LM,"This paper introduces TCLP, a novel label propagation framework for time-series active learning. TCLP leverages temporal coherence to propagate labels within estimated segments using a plateau model, addressing label sparsity issues. Extensive experiments across multiple datasets demonstrate significant improvements in classification accuracy with fewer labeled data points compared to existing methods.","['How does the method perform on time-series with very short segments or rapidly changing labels?', 'What is the computational complexity for very long time-series, and how does it scale?', 'Are there any notable failure cases or scenarios where TCLP performs worse than simpler methods?', 'How sensitive is the method to the choice of hyperparameters like temperature T and initial plateau width?', 'How well does TCLP generalize to different types of time-series data beyond those tested in the paper?']","['May not generalize well to all types of time-series data, especially those without clear temporal coherence', 'Computational scalability for very long time-series is addressed but could be a concern for extremely large datasets', 'Requires careful tuning of several hyperparameters, which could be challenging in practice']",False,4,3,4,8,4,"['Novel and intuitive approach for exploiting temporal coherence in time-series active learning', 'Significant empirical improvements over baselines across multiple datasets (up to 7.1x better accuracy)', 'Thorough evaluation with different query selection methods and ablation studies', 'Theoretical analysis providing justification for the plateau-based approach', 'Addresses an important problem of reducing labeling effort for time-series data', 'Demonstrates potential for substantial reduction in manual labeling requirements']","['May not perform as well on time-series with very short segments or rapidly changing labels', 'Limited discussion of potential failure cases or limitations', 'Computational complexity could be an issue for very long time-series, though this is partially addressed in the paper', 'Several hyperparameters that may require tuning']",4,4,3.0,4,Accept
wu5yYUutDGW,"This paper introduces BaSSL, a self-supervised learning framework for video scene segmentation. The key contributions are: (1) using dynamic time warping to discover pseudo-boundaries in unlabeled videos, (2) proposing boundary-aware pretext tasks to learn contextual representations, and (3) pre-training both a shot encoder and contextual relation network. Experiments demonstrate that BaSSL outperforms prior work on the MovieNet-SSeg benchmark.","['How well does the method generalize to other video scene segmentation datasets beyond MovieNet-SSeg? Have you tested on any other benchmarks?', 'What is the computational cost of the approach compared to previous methods? Can you provide training and inference times?', 'Have you considered incorporating other modalities like audio to improve performance, especially for visually similar scenes?', 'How does the method perform on scenes that are visually similar but semantically different? Can you provide examples of such cases?']","['Evaluation is limited primarily to one dataset (MovieNet-SSeg), which may not fully demonstrate generalization', 'Computational efficiency and requirements are not thoroughly analyzed, making it difficult to assess practical applicability', 'Method relies only on visual features, potentially limiting its effectiveness in scenarios where visual cues are insufficient', 'Potential bias from training data and generalization to other video domains (e.g., TV shows, documentaries) not fully explored']",False,4,3,4,8,4,"['Novel technical approach combining pseudo-boundary discovery and boundary-aware pretext tasks', 'Strong empirical results, significantly improving over prior state-of-the-art methods', 'Comprehensive ablation studies validating the different components', 'Well-motivated method addressing limitations of prior shot-level pre-training approaches', 'Potential for broader impact on self-supervised video representation learning']","['Evaluation limited primarily to one dataset (MovieNet-SSeg)', 'Lack of discussion on computational costs and efficiency', 'Potential struggles with visually similar but semantically different scenes', 'Reliance on visual features only, without considering other modalities']",4,4,3.0,4,Accept
qyzTEWWM0Pp,"This paper introduces MGVAE (Multiresolution Equivariant Graph Variational Autoencoders), a novel hierarchical generative model for learning and generating graphs in a multiresolution and equivariant manner. The model employs higher-order message passing to encode graphs while learning to partition them into clusters and coarsen them into a hierarchy of latent distributions. Key innovations include a multiresolution graph neural network (MGN) for graph clustering and coarsening, and a hierarchical VAE built on top of it. The method uses higher-order equivariant neural networks to preserve permutation equivariance. Extensive experiments demonstrate strong performance on molecular graph generation, general graph generation, and molecular property prediction tasks.","['How does the method scale to very large graphs (e.g., millions of nodes)?', 'What are the theoretical limitations of the model in terms of expressiveness?', 'How sensitive is performance to hyperparameter choices like number of levels and clustering algorithm?', 'Can you provide more insight into what graph structures/properties the model is learning at different resolutions?', 'What are potential applications of this model beyond the domains tested in the paper?']","['Scalability to very large graphs is not demonstrated', 'Limited theoretical analysis of model capabilities and limitations', 'Computational complexity may be high due to higher-order equivariant networks', 'Only tested on a limited set of domains (molecules, citation networks, synthetic graphs)']",False,4,3,4,8,4,"['Novel hierarchical and equivariant graph neural network architecture', 'Theoretically grounded approach leveraging equivariance properties', 'Strong empirical results across multiple graph generation and prediction benchmarks', 'Ability to generate graphs at multiple resolutions', 'Comprehensive experiments and ablation studies', 'Effective unsupervised learning of molecular representations']","['Complex model with many components may be difficult to implement or tune', 'Scalability to very large graphs is not clearly demonstrated', 'Limited theoretical analysis of model expressiveness and limitations', 'Computational complexity and runtime analysis is lacking']",4,4,4.0,4,Accept
ks_uMcTPyW4,"This paper proposes a reinforcement learning framework for active feature acquisition in partially observable environments. The key contributions are: (1) Formulating the problem as an AFA-POMDP, (2) Introducing a sequential VAE (Seq-PO-VAE) for representation learning from partial observations, (3) Demonstrating improved performance over baselines on two domains: a bouncing ball control task and a sepsis treatment simulator.","['Can you provide more details on how the Seq-PO-VAE architecture was designed?', 'How does the performance of your method change with different feature acquisition costs?', 'What are the computational requirements of your method compared to the baselines?', 'How does the method handle potential distribution shifts between the pre-training data and the online RL environment?']","['The approach is only tested on two relatively simple domains, limiting generalizability claims', ""The method's performance in high-dimensional state/action spaces is not explored"", 'The reliance on fully observed data for pre-training the VAE may limit applicability in some real-world scenarios', 'The paper lacks theoretical guarantees on the optimality or convergence of the proposed method']",False,3,3,3,7,4,"['Novel formulation of active feature acquisition in POMDPs as an AFA-POMDP', 'Sequential VAE (Seq-PO-VAE) for representation learning from partial observations is a key technical contribution', 'Empirical results show clear improvements over baselines on two domains', 'Addresses an important problem of cost-efficient information gathering in RL']","['Limited theoretical analysis and justification', 'Experiments only on two relatively simple domains - more varied domains would strengthen generalizability claims', 'Comparison to more sophisticated POMDP baselines could be valuable', 'Scalability to more complex, high-dimensional environments is not demonstrated']",3,3,3.0,3,Accept
qEGBB9YB31,"This paper introduces a novel approach to neural network interpretability by identifying salient network parameters responsible for misclassifications. The authors present a parameter-space saliency method and an input-space counterpart to visualize how image features interact with salient filters. The approach is validated through pruning and fine-tuning experiments, and case studies demonstrate its utility in uncovering model failures and dataset biases.","['How does the computational cost of this method compare to other interpretability approaches?', 'How well does this method generalize to other types of neural networks and tasks beyond image classification?', 'Could this approach be extended to analyze and improve model robustness?', 'How does the method perform on more complex datasets or real-world applications beyond ImageNet?', 'How can this method be integrated into the model debugging and improvement process?']","['The approach is mainly demonstrated on image classification tasks', 'Potential computational overhead for large models', 'May not generalize well to other neural network architectures', 'Relies on gradient information which may not always be available or meaningful']",False,4,4,4,7,4,"['Novel approach focusing on parameter-space saliency rather than just input-space', 'Thorough experimental validation of the proposed methods', 'Clear explanations and visualizations', 'Demonstrates practical utility for uncovering model failures and biases', 'Ability to link parameter and input space saliency', ""Comprehensive validation of the method's meaningfulness through various experiments""]","['Experiments limited to image classification tasks and CNNs', 'Computational cost not clearly discussed', 'Generalizability to other types of neural networks and tasks unclear', 'Comparison to other interpretability methods could be expanded']",4,3,4.0,3,Accept
eqaxDZg4MHw,"This paper presents a comprehensive empirical study on generalization in visual reinforcement learning using the Procgen benchmark. The authors investigate several methods for improving generalization, including data augmentation, domain invariance, and auxiliary tasks. Key findings include the importance of task-informed data augmentation, the non-necessity of strictly domain-invariant features, benefits of simple auxiliary tasks, and good transferability of visual features across levels.","['How well do the findings generalize beyond the Procgen benchmark to other RL domains?', 'Can you provide more theoretical insights to explain the empirical results?', 'How do you envision combining the different approaches (data augmentation, auxiliary tasks, etc.) into a unified method for improving generalization?', 'What are the implications of your findings for the design of future RL algorithms?']","['Results are limited to the Procgen benchmark and may not generalize to all RL domains', 'Lack of theoretical analysis to support empirical findings', 'The study focuses on policy generalization and does not address other important RL challenges', 'Limited discussion of potential negative societal impacts of the work', 'Hyperparameter sensitivity of the methods is not fully explored']",False,4,3,3,7,4,"['Comprehensive empirical analysis of multiple generalization techniques', 'Well-designed experiments with thorough evaluation', 'Novel insights that challenge some existing beliefs in the field', 'Clear presentation of results and findings', 'Valuable directions for future research']","['Limited to Procgen benchmark, generalizability of results unclear', 'Lack of theoretical analysis or formal framework', 'No novel algorithmic contributions', 'Some findings are intuitive and not particularly surprising', 'Limited discussion of broader impact and potential negative societal effects']",4,4,3.0,3,Accept
hyuacPZQFb0,"This paper introduces ADATIME, a systematic benchmarking framework for evaluating unsupervised domain adaptation (UDA) methods on time series data. The authors standardize the evaluation pipeline, adapt visual UDA methods to time series, explore realistic model selection approaches, and conduct extensive experiments across multiple datasets. Key contributions include standardizing backbone networks and datasets, comparing 10 UDA methods across 4 datasets and 20 cross-domain scenarios, and providing valuable insights on the performance of visual UDA methods on time series data.","['Could you include more time series-specific domain adaptation methods in the comparison?', 'Have you considered evaluating on larger-scale or more diverse time series datasets?', 'Could you provide any theoretical intuition for why visual UDA methods perform well on time series data?', 'Have you performed any statistical significance tests on the performance differences between methods?', 'What are the practical implications of your findings for researchers and practitioners working with time series data?']","['Results may not generalize to all types of time series data or application domains', 'Focus is mainly empirical without theoretical justifications', 'Limited to unsupervised domain adaptation (does not cover semi-supervised approaches)', 'Computational costs of different approaches not thoroughly analyzed']",False,4,3,3,7,4,"['Comprehensive and rigorous evaluation methodology with standardized pipeline', 'Adaptation of visual UDA methods to time series data', 'Exploration of realistic model selection approaches', 'Extensive experiments across multiple datasets and scenarios', 'Valuable insights and recommendations for future research']","['Limited number of time series-specific UDA baselines', 'Lack of theoretical analysis to complement empirical results', 'Experiments limited to a relatively small number of datasets', 'Some experimental details lacking (e.g., statistical significance tests)']",3,4,3.0,4,Accept
RjMtFbmETG,"This paper introduces 'resmax', a new soft-greedy operator for reinforcement learning. The authors provide theoretical analysis proving resmax is a non-expansion and ensures state-action space coverage. Empirical evaluations in both tabular and deep RL settings, including Atari games, demonstrate that resmax often performs comparably or better than epsilon-greedy and softmax baselines.","['How does the theoretical guarantee of non-expansion translate to practical performance improvements?', 'Have you considered extending resmax to continuous action spaces?', 'Can you provide more insight into the relationship between the exploration parameter η and the performance of resmax?', 'How does resmax compare to more advanced exploration techniques in terms of sample efficiency?']","['Empirical evaluation is somewhat limited to standard RL benchmarks', 'Lack of analysis on computational efficiency compared to simpler methods', 'Potential negative impacts of improved RL algorithms not thoroughly discussed', 'Possible increased sensitivity to hyperparameter tuning compared to simpler methods', 'May not be suitable for all types of RL problems, especially those with very large action spaces']",False,3,4,3,7,4,"['Strong theoretical analysis proving key properties of resmax', 'Comprehensive empirical evaluation across multiple environments', 'Addresses known limitations of existing soft-greedy operators', 'Clear writing and presentation']","['Empirical improvements over baselines are sometimes modest', 'Limited evaluation on very challenging exploration tasks', 'Lack of thorough analysis explaining why resmax outperforms in certain scenarios', 'Limited discussion of potential limitations or failure cases']",4,3,4.0,3,Accept
UgNQM-LcVpN,"This paper introduces the Modulating Fully Connected Layer (MFCL), a novel neural network layer inspired by biological neuromodulation, to improve model robustness to missing or low quality input data. The approach is evaluated on classification, regression, and imputation tasks, showing improvements over baselines, especially for non-random missingness patterns.","['Can you provide a more rigorous theoretical analysis of the MFCL approach, explaining why it works better for non-random missingness?', 'How does the MFCL compare to Bayesian neural networks and dropout techniques in terms of performance and computational efficiency?', 'Have you conducted experiments on large-scale, real-world datasets with natural missingness patterns? If so, what were the results?', 'What specific techniques do you employ to mitigate overfitting given the additional parameters in the modulation network?', 'Can you provide more details on the hyperparameter search and model selection process used in the experiments, including any cross-validation techniques?']","['Evaluation is limited mostly to artificial missingness scenarios', 'Lack of thorough theoretical analysis or mechanistic insights', 'Potential negative impacts and computational overhead of the approach are not thoroughly discussed', 'Experiments are limited to a relatively small number of datasets and tasks', 'Potential societal impacts of improved handling of missing data are not fully explored']",False,3,3,3,5,4,"['Novel architecture inspired by biological neuromodulation', 'Addresses an important problem of model robustness to missing/noisy data', 'Evaluation on multiple tasks (classification, regression, imputation)', 'Potential practical impact for handling real-world messy data', 'Shows promising results, especially for non-random missingness patterns', 'Clear writing and well-organized presentation of ideas']","['Limited experimental evaluation, especially on real-world datasets with natural missingness', 'Lack of theoretical analysis or justification for the approach', 'Missing comparisons to some state-of-the-art methods (e.g., Bayesian NNs, dropout)', 'Modest performance gains over baselines in many cases', 'Potential for overfitting due to additional parameters in the modulation network', 'Hyperparameter sensitivity and model selection process not thoroughly explored']",3,2,3.0,3,Reject
kcrIligNnl,"This paper introduces a novel method for molecular conformation generation that directly predicts 3D atomic coordinates instead of interatomic distances. The approach employs a VAE framework with a specialized rotation and translation invariant loss function and an iterative refinement architecture. Experiments on GEOM-QM9 and GEOM-Drugs datasets demonstrate state-of-the-art performance, especially for larger molecules, with significantly faster inference times compared to previous methods.","['Can you provide more theoretical analysis on why direct coordinate prediction works better than distance-based methods, especially for larger molecules?', 'How does the method handle highly flexible molecules or those with many rotatable bonds?', 'Have you explored potential ways to address the failure cases with rotatable rings?', 'How sensitive is the method to the choice of hyperparameters, particularly the number of refinement blocks?']","['Struggles with certain molecular structures like rotatable rings', 'Performance gains are less significant for smaller molecules', 'Potential scalability issues for very large biomolecules']",False,4,4,4,7,4,"['Innovative approach of directly predicting 3D coordinates instead of distances', 'Strong empirical results, particularly on larger molecules', 'Well-designed model architecture with iterative refinement', 'Rotation and translation invariant loss function', 'Significantly faster inference compared to previous methods', 'Thorough experiments including ablation studies and property prediction tasks']","['Limited theoretical analysis of why the method works better for larger molecules', 'Some issues with handling certain molecular structures (e.g., rotatable rings)', 'Inconsistent performance gains for smaller molecules', 'Limited discussion on ethical considerations and broader impacts']",4,3,4.0,4,Accept
Ub1BQTKiwqg,This paper presents a novel method for training sparse neural networks in a single cycle using soft thresholding and progressive sparsity increase. The approach achieves comparable or better accuracy than state-of-the-art pruning methods while significantly reducing training time.,"['How well does the method scale to larger models and datasets beyond ImageNet?', 'Is there any theoretical justification for why soft thresholding works better than hard thresholding?', 'How sensitive is the method to hyperparameters like the pruning schedule?', 'How does the final sparse architecture compare to those found by other pruning methods?', 'Can this method be effectively applied to other network architectures beyond ResNet and VGG?']","['Experiments are limited mostly to smaller models and datasets', 'Lack of theoretical analysis or guarantees', 'Potential sensitivity to hyperparameter choices', 'May not find optimal sparse architectures compared to more sophisticated pruning methods']",False,3,3,3,7,4,"['Novel and efficient method for training sparse networks in one cycle', 'Achieves comparable or better accuracy to state-of-the-art pruning methods', 'Significantly reduces training time and computational resources', 'Thorough ablation studies justifying design choices', 'Promising results on ImageNet with ResNet-50']","['Limited theoretical analysis or justification', 'Experiments mostly on smaller models/datasets (mainly CIFAR-10)', 'Some claims could be better supported with additional experiments', 'Limited comparison to very recent pruning methods']",3,3,3.0,4,Accept
BRFWxcZfAdC,"This paper introduces a theoretical framework for lossy compression with distribution shift, formulating it as entropy-constrained optimal transport. The authors provide rigorous mathematical analysis of rate-distortion tradeoffs with and without common randomness, derive architectural principles, analyze specific source examples, and validate their findings experimentally using deep learning for super-resolution and denoising tasks.","['How well would the approach scale to more complex, high-dimensional data?', 'What are the key practical implications or potential applications of this work?', 'How does the method compare quantitatively to other state-of-the-art approaches for joint compression and restoration?', 'What are potential extensions or future directions for this work?']","['Experimental evaluation focused on simple datasets and tasks', 'Theoretical results may not fully capture real-world complexity', 'Limited discussion of potential negative societal impacts']",False,4,3,4,7,4,"['Novel and significant problem formulation connecting compression, optimal transport, and distribution shift', 'Rigorous theoretical analysis with important results and proofs', 'Successful bridging of theory and practice through experimental validation', 'Derivation of practical architectural principles from theory', 'Analysis of important special cases (binary, uniform, Gaussian sources)']","['Experimental evaluation limited to relatively simple datasets', 'Some sections may be challenging for non-experts to follow', 'Limited discussion of broader implications and potential applications', 'Computational complexity not thoroughly analyzed', 'Lack of comparison to existing methods for joint compression and restoration']",4,3,2.0,3,Accept
_ixHFNR-FZ,"This paper presents a novel theoretical framework analyzing the relationship between adversarial robustness, domain transferability, and regularization in machine learning models. The authors prove that stronger regularization of feature extractors, rather than robustness itself, leads to improved domain transferability bounds. The work examines factors such as Jacobian norm, last layer norm, and data augmentation within this framework. Extensive experiments on image classification tasks validate the theoretical claims and provide counterexamples where robustness and transferability are negatively correlated, challenging previous assumptions in the field.","['How do the insights from this work apply to other transfer learning scenarios beyond domain adaptation?', 'What are the key practical implications of this work for designing transfer learning systems in industry?', 'How sensitive are the empirical results to choice of hyperparameters, datasets, and neural network architectures?', 'Could you provide more intuitive explanations for some of the more complex theoretical results, particularly for non-expert readers?']","['The theoretical framework makes some simplifying assumptions that may not hold in all real-world scenarios', 'Experiments are limited to image classification tasks and may not generalize to other domains or modalities', 'The paper does not thoroughly discuss potential negative impacts of improving domain transferability', 'Analysis focuses on sufficient conditions for transferability, not necessary ones']",False,4,3,4,8,4,"['Novel theoretical framework connecting regularization, robustness, and domain transferability', 'Rigorous mathematical analysis with detailed proofs in the appendix', 'Comprehensive experiments validating theoretical claims on CIFAR-10 and ImageNet datasets', 'Challenges existing assumptions about the relationship between robustness and transferability', 'Clear writing and organization of the main ideas']","['Theoretical sections are dense and may be difficult for non-experts to follow', 'Limited discussion of broader impacts and potential negative consequences of improved transferability', 'Experiments primarily focused on image classification tasks, limiting generalizability', 'Some empirical results could benefit from more in-depth analysis and interpretation', 'Limited discussion of practical implications for designing real-world transfer learning systems']",4,4,3.0,4,Accept
C4o-EEUx-6,"This paper introduces Flashlight, a novel open-source machine learning library designed to enable innovation in ML frameworks and systems. Key features include a modular architecture with customizable components, a small codebase for fast compilation, competitive performance on standard models, and inclusion of state-of-the-art models across multiple domains. Flashlight aims to accelerate research into ML systems by allowing easier modification of framework internals compared to large established libraries, potentially leading to breakthroughs in ML computation paradigms.","['Can you provide examples of novel ML systems research that has been enabled by Flashlight?', 'How does the learning curve compare to PyTorch/TensorFlow for ML practitioners?', 'Are there plans to add more production-ready features in the future?', 'How is backwards compatibility and stability managed given the highly modular design?', 'How does Flashlight compare to other research-focused ML frameworks like Jax?', 'How do you plan to balance maintaining simplicity with expanding features as the project grows?']","['The simplified design may lack features needed for some production use cases', 'Potential learning curve for researchers used to existing frameworks', 'May contribute to fragmentation of ML tools/ecosystem', 'Long-term maintenance and community growth plans are not discussed']",False,3,3,3,7,4,"['Highly modular and customizable architecture enables easy experimentation with framework internals', 'Lightweight codebase (27k lines) reduces complexity and enables faster iteration', 'Extremely fast compilation times, beneficial for rapid prototyping and research', 'Performance competitive with or exceeding PyTorch and TensorFlow on benchmarks', 'Includes state-of-the-art model implementations and benchmarks across vision, speech, and NLP domains', 'Focus on enabling ML systems research, not just ML applications', 'Open source implementation enhancing reproducibility and potential impact']","['Lacks some production-ready features of established frameworks', 'Smaller ecosystem and community compared to PyTorch/TensorFlow', 'May have a steeper learning curve for ML practitioners used to high-level APIs', 'Limited evaluation of novel ML systems research enabled by the framework', 'Performance evaluation is limited in scope and may not generalize to all model types or scales']",4,3,3.0,3,Accept
TBWA6PLJZQm,"This paper introduces two new benchmark datasets, CIFAR-10N and CIFAR-100N, which provide human-annotated noisy labels for the CIFAR-10 and CIFAR-100 training sets. The authors analyze the properties of these real-world noisy labels, showing they differ from synthetic noise models in being instance-dependent. They benchmark existing noisy label learning methods on the new datasets and study memorization effects, revealing important differences between human and synthetic noise.","['How well do you expect the findings to generalize to larger, more complex datasets beyond CIFAR?', 'Have you considered collecting similar noisy labels for other standard datasets like ImageNet?', 'How do the noise patterns compare to those in other real-world noisy label datasets like Clothing1M?', 'How do the authors envision these benchmarks being used by the community going forward?']","['The datasets are limited to CIFAR, so noise patterns may not generalize to other domains', 'The analysis is mostly empirical, lacking theoretical characterization of the noise', 'Potential negative impacts of releasing datasets with intentional label noise are not discussed']",False,4,4,4,8,4,"['Creates valuable new benchmark datasets with verifiable real-world noisy labels', 'Provides insightful analysis of human noise patterns compared to synthetic noise', 'Extensive benchmarking of existing methods reveals performance gaps on real noise', 'Study of memorization effects highlights differences between human and synthetic noise', 'Fills a gap by providing controlled real-world noise on standard datasets']","['Limited to CIFAR datasets, may not generalize to other domains/larger datasets', 'No major technical innovation beyond dataset creation and analysis', 'Analysis is mostly empirical rather than theoretical']",3,4,4.0,4,Accept
6u6N8WWwYSM,"This paper introduces ReCo, a pixel-level contrastive learning framework for improving semantic segmentation, particularly effective in low-label regimes. ReCo employs active sampling strategies to perform contrastive learning on a sparse set of hard negative pixels, and can be integrated with existing semi-supervised segmentation methods. The approach demonstrates consistent improvements across multiple datasets and experimental settings, with especially strong performance in very low-label scenarios.","['Can the authors provide more theoretical justification for why their approach works well?', 'How does the computational complexity of ReCo compare to existing methods?', 'How sensitive is the method to the choice of hyperparameters like number of queries/keys?', 'Could the method be extended to other dense prediction tasks beyond semantic segmentation?', 'Have the authors considered potential negative societal impacts of their work?']","['Performance gains are smaller in higher-label regimes', 'The approach adds some computational overhead during training', 'May require careful tuning of several hyperparameters']",False,4,4,4,7,4,"['Novel pixel-level contrastive learning approach for semantic segmentation', 'Effective active sampling strategies to reduce memory requirements and focus on informative pixels', 'Strong empirical results, especially in low-label regimes', 'Consistent improvements across multiple datasets and settings', 'Clear writing, thorough analysis, and comprehensive experiments']","['Limited theoretical analysis or justification for the approach', 'Improvements are more modest in higher-label regimes', 'Some complexity in the method and hyperparameter choices', 'Limited comparison to very recent concurrent works on contrastive learning for segmentation']",4,4,4.0,4,Accept
t5s-hd1bqLk,"This paper introduces a novel approach for conditioning neural networks using learned activation functions (LAs) instead of traditional concatenation or modulation methods. The key idea is to use conditioning vectors to dynamically modify activation functions in each layer of the network. The authors evaluate their approach on personalized sound enhancement and automatic speech recognition tasks, demonstrating competitive performance while reducing model size. The paper also provides analysis of the learned activations, offering insights into how the model adapts to different conditioning vectors.","['How well would this approach generalize to non-audio domains like computer vision or natural language processing?', 'What are the computational efficiency trade-offs of using learned activations compared to baseline methods?', 'Have you considered comparing the approach to more advanced conditioning methods beyond simple concatenation/modulation?']","['Evaluation limited to audio domain tasks', 'Robustness to noisy enrollment data not tested', 'Potential trade-offs between model size, latency, and quality not fully explored']",False,3,3,3,7,4,"['Novel and parameter-efficient approach to neural network conditioning', 'Comprehensive experiments on real-world speech tasks', 'Competitive performance while reducing model size', 'Thorough analysis including visualization of learned activations', 'Clear writing and presentation']","['Evaluation limited to audio domain tasks, generalizability to other domains unclear', 'Some results show mixed or modest improvements over baselines', 'Comparisons mainly to simple baseline methods, not state-of-the-art', 'Limited analysis of computational efficiency trade-offs']",4,3,4.0,3,Accept
1L0C5ROtFp,"This paper introduces the Filtered-CoPhy benchmark for counterfactual physics reasoning in pixel space and presents an unsupervised method (CoDy) for learning a hybrid latent representation combining 2D keypoints and appearance vectors. The approach demonstrates strong performance in forecasting counterfactuals for rigid body dynamics, outperforming baselines on simulated data and contributing to unsupervised learning in physics domains.","['How well does the method generalize to non-rigid or deformable objects?', 'What strategies could improve long-term stability of rigid structure predictions?', 'How computationally expensive is the method compared to baselines?', 'Have you considered self-supervised pretraining on unlabeled video data?', 'What potential applications do you envision for this approach beyond physics simulations?']","['Currently limited to rigid body dynamics', 'Performance degrades for very long-term predictions', 'Evaluation on real-world data is limited to a small dataset of wooden blocks', 'Computational requirements not clearly discussed']",False,4,4,4,8,4,"['Novel benchmark (Filtered-CoPhy) for counterfactual physics reasoning', 'Unsupervised learning of a hybrid latent representation', 'Strong empirical results compared to baselines', 'Thorough ablation studies and analysis']","['Limited evaluation on real-world data', 'Struggles with long-term prediction of rigid structures', 'Approach is tailored to rigid body dynamics - unclear generalizability', 'Some baselines not directly comparable']",4,4,4.0,4,Accept
ZFIT_sGjPJ,"This paper introduces data-dependent randomized smoothing for certified robustness of neural networks. The key contributions include optimizing smoothing parameters per input, implementing a memory-based certification scheme, and incorporating data-dependent smoothing in both certification and training. Experiments on CIFAR-10 and ImageNet demonstrate consistent improvements over state-of-the-art methods.","['How does the proposed method handle inputs that are significantly different from the training distribution?', 'Can you provide more insights into the trade-offs between certification accuracy and computational overhead?', 'Have you explored the potential of this approach in other domains beyond image classification?']",The memory-based certification adds computational overhead and may not scale well to very large datasets. The approach is currently limited to randomized smoothing methods and has been tested only on image classification tasks. Real-world deployment may face challenges in terms of latency and resource requirements.,False,4,3,3,7,4,"['Novel and well-motivated idea of data-dependent smoothing', 'Consistent empirical improvements over state-of-the-art baselines', 'General approach applicable to different smoothing distributions and norms', 'Thorough experiments and ablation studies on standard benchmarks']","['Increased computational cost and complexity due to memory-based certification', 'Potential scalability issues with very large datasets', ""Limited theoretical analysis of the approach's effectiveness""]",3,3,4.0,3,Accept
uEBrNNEfceE,"This paper proposes a safe online learning algorithm for linear quadratic regulators (LQR) with almost sure performance guarantees. Key components include a safe switching control strategy, parameter inference based on Markov parameter estimation, and theoretical analysis showing optimal convergence rates. The algorithm ensures 'bounded-cost safety' and achieves almost sure convergence in parameter estimation and control performance.","['How would the algorithm extend to partially observable or nonlinear systems?', 'What is the computational complexity compared to other state-of-the-art LQR algorithms?', 'How sensitive is the performance to the choice of switching threshold and other hyperparameters?', 'Have you considered simplifications to improve practical applicability?', 'What are some potential real-world applications for this algorithm?']","['Applicable only to linear systems with full state observation', 'Requires known initial stabilizing controller', 'Limited empirical evaluation', 'Potential for over-conservative behavior in safety-critical systems']",False,4,3,4,8,4,"['Novel safe switching control strategy with provable convergence', 'Almost sure convergence guarantees matching optimal rates', 'Thorough theoretical analysis and proofs', ""Introduction of practical 'bounded-cost safety' notion"", 'Advances state-of-the-art in online LQR learning']","['Limited empirical evaluation', 'Restricted to linear systems with full state observation', 'Assumes knowledge of an initial stabilizing controller', 'Algorithm and analysis are complex, potentially limiting practical applicability', 'Computational efficiency not thoroughly analyzed']",4,4,4.0,4,Accept
JGO8CvG5S9,"This paper presents novel theoretical results on universal approximation capabilities of transformer networks under exact constraint satisfaction. The key contributions include: (1) A constrained universal approximation theorem for probabilistic transformers, (2) A probabilistic reformulation of transformer attention to handle non-convex constraints, (3) Quantitative approximation rates that scale with the intrinsic dimension of the constraint set, and (4) Applications to classical transformers with convex constraints and Riemannian manifold-valued functions.","['Can you provide empirical results on real-world constrained learning tasks to validate the theoretical findings?', 'How can the proposed probabilistic transformer be efficiently implemented and trained in practice?', 'What are the computational trade-offs of using the probabilistic transformer vs classical transformers?', 'How might the theoretical results inform the design of practical constrained deep learning models?']","['The paper focuses primarily on theoretical results with limited empirical validation', 'The proposed probabilistic transformer architecture may be complex to implement in practice', 'Some assumptions required for the theorems may not hold in many real-world scenarios', 'The dense mathematical formulation may limit accessibility to a broader ML audience']",False,4,3,4,8,4,"['Novel and significant theoretical contributions to constrained universal approximation', 'Rigorous mathematical treatment with detailed proofs', 'Clever connections between approximation theory, optimal transport, and differential geometry', 'Handles both convex and non-convex constraints', 'Quantitative approximation rates provided', 'Applications to important special cases (classical transformers, Riemannian manifolds)']","['Limited empirical validation on real-world tasks', 'Lack of discussion on practical implications and implementability', 'Complexity of proposed architecture may limit practical applicability', 'Dense mathematical formulation may limit accessibility to broader ML audience']",4,4,2.0,4,Accept
IEsx-jwFk3g,"This paper introduces ReBraiD, a novel graph neural network model for analyzing dynamic brain imaging data. The model innovatively combines structural and functional connectivity data, incorporates techniques like adaptive adjacency matrix learning and multi-resolution inner cluster smoothing, and utilizes integrated gradients for interpretation. Experiments demonstrate superior performance over baselines on fMRI task classification and provide neuroscientifically meaningful insights.","['How does the model perform on other brain imaging datasets beyond CRASH?', 'What are the computational requirements and how does the model scale to larger datasets?', 'Can you provide a clearer ablation study isolating the impact of each novel model component?', 'How does the model compare to other state-of-the-art dynamic graph or time series models beyond basic GNNs?']","['Evaluation on only one dataset limits generalizability claims', 'Computational complexity and scalability not thoroughly discussed', 'Some neuroscientific interpretations may be dataset-specific']",False,4,3,4,8,4,"['Novel GNN architecture tailored for brain dynamics', 'Effective joint modeling of structural and functional connectivity', 'Innovative techniques to capture latent dynamics in brain networks', 'Use of integrated gradients for model interpretation', 'Comprehensive experiments and ablation studies', 'Strong empirical results outperforming baseline models', 'Neuroscientific interpretations align well with existing literature']","['Evaluation limited to one dataset (CRASH)', 'Limited discussion of computational complexity and scalability', 'Some neuroscientific claims could be more carefully qualified', 'Lack of comparison to some relevant baseline methods']",4,4,4.0,4,Accept
WLEx3Jo4QaB,"This paper introduces GCOND, a novel framework for graph condensation that reduces large graphs to small synthetic graphs while preserving GNN performance. The method uses gradient matching and parameterizes the condensed graph structure as a function of node features. Extensive experiments demonstrate that GCOND can achieve 95-99% of original accuracy while reducing graph size by over 99%, generalizing well across GNN architectures.","['Can you provide any theoretical insights or guarantees for the proposed method?', 'How does the computational complexity of GCOND scale with graph size? Is it practical for very large graphs with millions of nodes?', 'Have you explored applying this approach to other graph learning tasks beyond node classification?', 'How sensitive is the method to key hyperparameters, such as the condensation ratio?', 'What are some potential real-world applications or use cases for the condensed graphs beyond training efficiency?']","['The method is only evaluated on node classification tasks', 'Scalability to very large graphs is not thoroughly analyzed', 'Lacks theoretical guarantees', 'May not work as well for graphs where structure is more important than node features']",False,4,4,4,8,4,"['Novel and well-motivated approach to graph condensation', 'Effective gradient matching framework with clever graph parameterization', 'Impressive empirical results showing significant graph size reduction with minimal performance loss', 'Good generalization across different GNN architectures', 'Comprehensive experiments, ablation studies, and analysis']","['Lacks theoretical analysis or guarantees', 'Evaluation focused only on node classification tasks', 'Limited discussion of computational complexity and scalability to very large graphs', 'Some datasets show larger performance gaps compared to training on full data', 'Limited comparison to other graph reduction techniques like graph sparsification']",4,3,4.0,4,Accept
8Wdj6IJsSyJ,"This paper introduces a fully differentiable approach to model discovery, addressing limitations of existing methods that rely on non-differentiable operations. The authors integrate sparse Bayesian learning with physics-informed neural networks, enabling end-to-end training. The method is extended to physics-informed normalizing flows for density estimation. Empirical results demonstrate improved performance and robustness over non-differentiable methods on several PDE benchmark datasets.","['Can you provide a more rigorous theoretical analysis of the dynamic prior approach, including potential convergence guarantees?', 'How does the computational complexity of your method scale with the dimensionality of the PDE? Can you provide examples or analysis for 2D or 3D systems?', 'How does your method compare to other recent differentiable PDE discovery approaches, such as those using automatic differentiation or neural ODEs?', 'Can you provide a detailed computational complexity analysis of your method compared to existing approaches, especially for more complex systems?']","The authors acknowledge some limitations, such as the need for hyperparameter tuning. However, they could expand on discussing scalability to more complex systems, potential ways to discover models without specifying candidate terms, and the computational costs for larger-scale problems. The preliminary nature of the normalizing flow experiments is also a limitation that could be addressed in future work. Additionally, the authors could discuss potential limitations in applying this method to systems where the underlying equations are not well-approximated by the chosen library of candidate terms.",False,3,3,3,7,4,"['Novel fully differentiable model discovery algorithm', 'Integration of sparse Bayesian learning with PINNs', 'Extension to physics-informed normalizing flows', 'Clear performance improvements over existing methods on benchmark datasets', 'Connections drawn to multitask learning', 'Demonstrated robustness to high levels of noise in data']","['Limited theoretical analysis and justification for the dynamic prior, particularly regarding convergence guarantees', 'Experiments primarily focused on relatively simple 1D PDEs', 'Some ad hoc hyperparameter choices', 'Limited comparisons to recent differentiable model discovery approaches', 'Preliminary nature of normalizing flow experiments']",3,3,3.0,3,Accept
UJ9_wmscwk,"This paper introduces several graph neural network-based approaches for influence estimation and maximization in large networks. The main contributions are GLIE (a GNN for influence estimation), CELF-GLIE (an adaptation of CELF using GLIE), GRIM (a Q-learning approach using GLIE representations), and PUN (a submodular function based on GLIE representations). The methods are evaluated on synthetic and real-world networks, demonstrating competitive or superior performance in influence spread and computational efficiency compared to state-of-the-art methods.","['How do the proposed methods perform on even larger networks (e.g., 100M+ nodes)?', 'Can you provide more theoretical guarantees for the GLIE-based methods?', 'How robust are the methods to noise in the network structure or influence probabilities?', 'How do the methods compare to other recent learning-based approaches for influence maximization?', 'How can user profiles or contextual information be incorporated into the proposed methods?']","['Methods are evaluated only on certain types of graphs and datasets', 'Approach is specific to the Independent Cascade model', 'Potential for misuse in manipulating social networks, including targeted political advertising', 'Scalability to very large graphs (>10M nodes) not fully explored', 'Ethical concerns regarding the use of these methods for large-scale manipulation in social media']",True,3,3,4,7,4,"['Novel integration of GNNs with influence maximization techniques', 'Scalability to large graphs with millions of nodes', 'Comprehensive experiments on synthetic and real-world networks', 'Theoretical analysis and proofs for some proposed methods', 'PUN method achieves a good balance between influence spread and efficiency', 'Innovative use of adaptive full-feedback selection in PUN']","['Limited theoretical guarantees for GLIE and GRIM methods', 'High computational overhead for GLIE-CELF and GRIM approaches', 'Lack of experiments on very large networks (>10M nodes)', 'Insufficient discussion of potential negative societal impacts']",4,3,4.0,4,Accept
d2XZsOT-_U_,"This paper introduces a novel Transformer-based approach for predicting outcomes of pairwise competitions using learned embeddings from player match histories. The model recursively expands opponent histories to capture deeper context and is agnostic to player identity. Experiments on chess, hockey, and baseball datasets demonstrate significant improvements over traditional Elo/Glicko methods, especially for new players with limited history.","['Can you provide more detailed ablation studies to isolate the impact of different components?', 'How does the computational complexity compare to traditional methods like Elo or Glicko, especially for large-scale applications?', 'Is there a way to extract interpretable skill ratings from the learned embeddings?', 'Have you explored extending the approach to multi-player games or more complex domains?', 'What are potential applications of this approach beyond sports and games?']","['Less interpretable than scalar rating methods', 'May not generalize well to multi-player games without modification', 'Computational complexity could be an issue for very large-scale applications', 'Potential privacy concerns with storing detailed player histories']",False,4,3,4,8,4,"['Novel application of Transformers to learn history embeddings instead of scalar skills', 'Recursive expansion of opponent histories captures deeper context', 'Agnostic to player identity, focusing purely on match history', 'Significant empirical improvements over strong baselines on multiple real datasets', 'Fast adaptation to new players with limited history', 'Flexible framework that can incorporate additional contextual information']","['Limited ablation studies and analysis of what the model is learning', 'Lack of interpretability compared to scalar skill rating systems', 'Potential computational complexity issues for very large-scale applications', 'Limited theoretical analysis or guarantees', 'Evaluation primarily focused on 1v1 competitions']",4,3,4.0,4,Accept
HHpWuWayMo,"This paper introduces cMBA, a novel model-based adversarial attack framework for cooperative multi-agent reinforcement learning (MARL) with continuous action spaces. The key contributions include learning an environment dynamics model, using it to generate adversarial perturbations, and formulating victim agent selection as an optimization problem. Experiments on multi-agent MuJoCo tasks demonstrate the effectiveness of the approach compared to baselines.","['How does the computational complexity of cMBA scale with the number of agents?', 'What potential defense mechanisms could be effective against this type of attack?', 'How would the approach perform in black-box attack scenarios?', 'Can you provide more theoretical justification for the model-based attack approach?', 'Have you considered testing on any real-world robotic systems?']","['Focuses only on fully observable, simulated environments', 'Requires full access to trained MARL policy', 'Scalability to large numbers of agents not thoroughly analyzed', 'Potential negative impacts on real-world MARL systems not discussed in depth', 'Lack of theoretical analysis or guarantees', 'Ethical implications of developing attack methods not fully explored']",True,4,3,3,7,4,"['Novel model-based approach for attacking cooperative MARL systems with continuous actions', 'Addresses an important problem in MARL robustness', 'Comprehensive experiments on multiple environments showing clear improvements over baselines', 'Well-motivated victim selection formulation', 'Technically sound framework with clear explanations']","['Limited discussion of potential defenses or mitigation strategies', 'Lack of analysis on computational efficiency and scalability', 'Ethical implications and potential misuse not thoroughly addressed', 'Only tested on simulated environments, real-world applicability unclear', 'Some presentation issues noted (e.g., unclear figures, typos)']",3,3,3.0,3,Accept
WPI2vbkAl3Q,"This paper presents a theoretical framework for analyzing stochastic gradient descent (SGD) learning dynamics in linear models with structured features. The authors derive exact formulas for expected test error based on feature and task spectra, extend the analysis to non-Gaussian features, and examine the effects of batch size and learning rate. The theory is validated on random feature models and wide neural networks trained on real datasets like MNIST and CIFAR-10, providing insights into how data structure influences learning dynamics.","['How well do the insights extend to feature learning in deep networks beyond the lazy training regime?', 'Can the theory be extended to capture nonlinear feature learning in deep networks?', 'How sensitive are the results to deviations from the Gaussian feature assumption in practice?']","['Analysis is limited to linear models or linearized networks, but still provides valuable insights for deep learning', 'Gaussian feature assumptions may not hold in all real-world scenarios', 'Some results rely on asymptotic assumptions that may not apply for finite networks']",False,4,3,3,7,4,"['Rigorous theoretical analysis of SGD dynamics for structured features', 'Exact expressions derived for expected test error', 'Strong empirical validation on real datasets', 'Insights into optimal hyperparameter choices', 'Extension to practical case of fixed training sets', 'Illuminates the role of data structure in learning dynamics']","['Limited to linear models or linearized networks in lazy training regime', 'Relies on Gaussian approximations which may not always hold', 'May not fully capture nonlinear deep learning dynamics']",4,4,3.0,4,Accept
YZHES8wIdE,"This paper introduces the Generative Planning Method (GPM), a novel approach for temporally coordinated exploration in reinforcement learning. GPM generates multi-step action sequences using a learned plan generator, trains it by maximizing plan values, and employs a replanning mechanism to balance plan commitment and switching. The method demonstrates improved sample efficiency and performance on continuous control benchmarks and shows emerging interpretable planning behavior on an autonomous driving task.","['Can you provide more theoretical justification for why GPM should improve exploration?', 'How does the computational complexity of GPM compare to standard RL methods?', 'How sensitive is the performance to the choice of plan length and other hyperparameters?', 'How would GPM perform on more complex/realistic autonomous driving scenarios?', 'What other domains or applications could benefit from the GPM approach?']","['The approach is currently limited to continuous action spaces', 'Scalability to high-dimensional state/action spaces is not thoroughly explored', 'Performance gains may be task-dependent', 'Potential negative impacts of improved RL algorithms not thoroughly discussed']",False,3,3,3,7,4,"['Novel combination of planning and model-free RL concepts', 'Improved sample efficiency and performance on several benchmark tasks', 'Emergent interpretable planning behavior, especially in autonomous driving', 'Comprehensive empirical evaluation and ablation studies', 'Contributes to improved interpretability in reinforcement learning']","['Limited theoretical analysis or justification for the approach', 'Lack of comparison to some relevant baselines (e.g., state-of-the-art model-based methods)', 'Potential computational overhead and added complexity compared to standard RL algorithms', 'Some hyperparameters (e.g., plan length) require tuning']",3,4,3.0,3,Accept
o6dG7nVYDS,"This paper presents a novel theoretical analysis of domain generalization (DG) using Rademacher complexity bounds. The main contributions are: 1) A new generalization bound for DG performance, 2) Hypothesis that DG performance is governed by a bias-variance tradeoff similar to standard machine learning, 3) Empirical analysis explaining erratic DG method performance in terms of model complexity, and 4) Demonstration that domain-wise cross-validation is better for model selection in DG than instance-wise cross-validation. The work provides valuable theoretical grounding and practical insights for improving DG methods.","['How well do the insights from linear models extend to more complex neural architectures in practice?', 'Could the analysis be extended to provide tighter bounds for specific model architectures?', 'How might the insights from this work be translated into practical DG algorithms for deep neural networks?']","['Focus on simpler models may limit direct applicability to state-of-the-art deep learning', 'Does not propose a new DG method that outperforms existing approaches', 'Analysis assumes domains are i.i.d. samples from a distribution over domains, which may not always hold in practice']",False,4,3,4,8,4,"['Novel theoretical analysis of DG using Rademacher complexity', 'Clear empirical demonstration of complexity-performance tradeoff in DG', 'Insightful explanation of existing DG method performance variability', 'Identification of domain-wise cross-validation as superior for DG model selection', 'Thorough empirical validation on linear models and neural networks using DomainBed benchmark']","['Analysis primarily focused on linear models and fixed features', 'Limited demonstration of generalization to end-to-end deep learning', 'Does not propose a new state-of-the-art DG method', 'Evaluation on real-world DG benchmarks could be more extensive']",4,4,4.0,4,Accept
dtYnHcmQKeM,"This paper introduces Physics-Informed Neural Operators (PINO), a method that combines operator learning with physics-informed optimization for solving PDEs. PINO uses a pre-trained neural operator as an ansatz for solving specific PDE instances, demonstrating improved convergence and accuracy over previous methods like Physics-Informed Neural Networks (PINNs) and Fourier Neural Operators (FNOs). The approach shows impressive results on challenging problems like long temporal transient flows and chaotic Kolmogorov flows, and demonstrates potential for transfer learning across different Reynolds numbers.","['Can the authors provide more theoretical insight into why PINO has a better optimization landscape?', 'How does the computational efficiency of PINO compare to baseline methods across different problem scales?', 'To what extent can PINO be applied to other types of PDEs or non-Euclidean domains?', 'How sensitive is PINO to hyperparameter choices, especially in the test-time optimization phase?']","['The method is primarily demonstrated on fluid dynamics problems', 'Theoretical understanding of why the method works better than PINNs is lacking', 'Scalability to very high-dimensional PDEs not demonstrated', 'Potential negative societal impacts not thoroughly discussed']",False,3,3,4,8,4,"['Innovative combination of operator learning and physics-informed optimization', 'Strong empirical results on challenging PDE problems', 'Thorough experimental evaluation against baselines', 'Demonstration of transfer learning capabilities across different Reynolds numbers', 'Effective formulation for inverse problems with significant speedup']","[""Lack of theoretical analysis or guarantees for the method's performance"", 'Limited discussion on computational costs and efficiency compared to baselines', 'Restricted demonstration to certain types of PDEs, mainly fluid dynamics', 'Some important implementation details missing, such as hyperparameter sensitivity']",4,3,3.0,4,Accept
z8xVlqWwRrK,"This paper introduces EVaDE (Event-based Variational Distributions for Exploration), a set of variational distributions over reward functions for model-based reinforcement learning in object-based domains. The approach uses three types of noisy convolutional layers: noisy event interaction, noisy event weighting, and noisy event translation. These layers are incorporated into the SimPLe algorithm to approximate Posterior Sampling for Reinforcement Learning (PSRL). Experiments on 12 randomly selected Atari games show improved performance over baselines in a low-data regime (100K interactions).","['How well does the approach generalize to non-Atari object-based domains?', 'What is the computational overhead of adding the EVaDE layers compared to vanilla SimPLe?', 'How sensitive is the performance to the hyperparameters of the EVaDE layers?', 'Are there any patterns in the types of games where EVaDE performs better or worse?']","['The approach is currently limited to object-based domains', 'Only evaluated on a subset of Atari games', 'Potential increased computational cost', 'May require careful tuning of EVaDE layer hyperparameters']",False,4,3,4,8,4,"['Novel approach to exploration in model-based RL using variational distributions over reward functions', 'Theoretically well-grounded design with proofs of maintained expressivity when adding EVaDE layers', 'Significant empirical improvements over strong baselines on Atari games, achieving 44% higher mean human-normalized score than CURL', 'Detailed ablation studies validating the contribution of each proposed layer type', 'Well-motivated approach for object-based domains, leveraging domain knowledge for exploration']","['Evaluation limited to only Atari games, raising questions about generalizability to other object-based domains', 'Potential increased computational cost not thoroughly addressed or quantified', 'Added complexity in implementation and potential need for careful tuning of EVaDE layers', 'Limited discussion on the scalability of the approach to more complex environments']",4,4,3.0,4,Accept
ODnCiZujily,"This paper introduces DeepSplit, a novel operator splitting method for efficiently solving large-scale LP relaxations in neural network verification. The approach reformulates the verification problem to allow splitting into smaller subproblems, many with analytical solutions, and uses an ADMM-based algorithm for efficient GPU-accelerated solution. Extensive experiments demonstrate that DeepSplit can solve LP relaxations exactly for networks much larger than previously possible, including ResNet18, leading to significantly tighter bounds compared to fast linear-based methods across image classification and reinforcement learning tasks.","['Could the approach be extended to tighter relaxations like SDPs?', 'How does the method perform on verification properties other than L-infinity robustness?', 'Are there ways to further accelerate convergence for extremely large networks?', 'How sensitive is the method to the choice of ADMM parameters?', 'What are potential applications of this method beyond robustness certification?']","['Only considers LP relaxations, not tighter SDP relaxations', 'Primarily focuses on L-infinity robustness certification', 'Significant computation time still required for very large networks', 'Empirical evaluation limited to image classification and RL tasks', 'Lack of comparison to other state-of-the-art verification methods like branch-and-bound']",False,4,3,4,8,4,"['Novel operator splitting approach enabling exact solution of large-scale LP relaxations', 'Significant improvements in bound tightness compared to fast linear-based methods', 'Scales to verify networks an order of magnitude larger than previous work, including ResNet18', 'Efficient GPU implementation with notable speedup over commercial LP solvers', 'Comprehensive experiments across image classification and reinforcement learning tasks', 'Provides theoretical convergence guarantees']","['Limited to LP relaxations, not considering tighter SDP relaxations', 'Focuses primarily on L-infinity robustness certification', 'Runtimes are still quite long for very large networks', 'Some implementation details and parameter choices not fully justified']",4,4,4.0,4,Accept
morSrUyWG26,"This paper introduces AutoOED, an open-source platform for automated multi-objective Bayesian optimization (MOBO). Key contributions include a modular framework for implementing MOBO algorithms, a novel asynchronous optimization strategy called Believer-Penalizer (BP), an intuitive GUI for non-expert users, and capabilities for automating real-world experiments. The paper provides extensive empirical evaluations on benchmark problems and demonstrates competitive performance against other BO platforms.","['Can you provide more theoretical justification for the Believer-Penalizer strategy?', 'How does AutoOED compare to other asynchronous BO methods beyond those mentioned in the paper?', 'Can you demonstrate AutoOED on more complex real-world optimization problems?', 'How does the platform scale to higher-dimensional (e.g., 100+ dimensions) problems?', 'What is the computational complexity of the BP strategy compared to other asynchronous methods?']","['Scalability to very high-dimensional problems not thoroughly explored', 'GUI usability not extensively evaluated with diverse user groups', 'Potential computational overhead of BP strategy not discussed', 'The platform may not be suitable for all types of real-world problems']",False,3,3,3,7,4,"['Modular framework facilitating implementation and comparison of MOBO algorithms', 'Novel Believer-Penalizer strategy for asynchronous optimization with good empirical performance', 'Intuitive GUI improving accessibility for non-expert users', 'Capability to automate real-world hardware experiments', 'Extensive empirical evaluation on benchmark problems', 'Open-source implementation enhancing reproducibility and further research', 'Bridges the gap between MOBO research and practical applications']","['Limited theoretical justification for the Believer-Penalizer strategy', 'Real-world example (PID controller) is relatively simple', 'Comparison to other asynchronous BO methods is limited', 'Scalability to higher-dimensional problems not thoroughly explored']",3,3,3.0,3,Accept
-TSe5o7STVR,"LaMer is a novel text style transfer method that leverages large language models and self-supervision from mined parallel data. It combines scene graph alignment, MLE training, and imitation learning, demonstrating strong performance on sentiment, formality, and political stance transfer tasks. The paper also introduces a new dataset for political stance transfer.","['Can you provide a more detailed ablation study on the impact of the imitation learning step?', 'What specific safeguards do you propose to prevent misuse of the political stance transfer capability?', 'How did you validate the new political stance transfer dataset and ensure its representativeness?', 'Can you elaborate on potential negative societal impacts and propose concrete mitigation strategies?']","['May not generalize to more divergent styles or languages', 'Limited to English language only', 'No fact-checking or safety measures incorporated', 'Potential for generating misleading content']",True,3,3,4,6,4,"['Innovative combination of scene graph alignment and large language models for style transfer', 'Strong empirical results across multiple style transfer tasks', 'Introduction of a new political stance transfer dataset', 'Effective use of self-supervision from mined parallel data', 'Positive human evaluation results']","['Potential biases inherited from pretrained language models', 'Risk of misuse, particularly in political stance transfer', 'Limited ablation studies on individual components', 'Insufficient discussion of ethical considerations and mitigation strategies', 'Reliance on scene graph parser may limit applicability to certain domains']",4,3,3.0,3,Accept
i--G7mhB19P,"This paper presents a theoretical analysis of natural gradient descent (NGD) in deep linear networks, comparing its behavior to standard gradient descent (GD). Key findings include NGD's invariance to reparameterization, its convergence to ordinary least squares-like solutions in classification tasks, and its failure to find low-rank solutions in matrix completion. The paper provides rigorous mathematical proofs and supporting experimental evidence, demonstrating cases where NGD can generalize poorly compared to GD with appropriate architectures.","['How do the theoretical results for linear networks inform our understanding of NGD in nonlinear neural networks?', ""Can you elaborate on potential scenarios where NGD's inductive biases might be advantageous?"", 'How do your findings relate to popular approximate NGD methods like K-FAC?', 'What are the key hyperparameters affecting your experimental results, and how sensitive are the outcomes to these choices?']","['Results primarily based on linear models', 'Limited experimental validation on large-scale or real-world tasks', 'Does not propose solutions to identified issues with NGD']",False,3,3,3,7,4,"['Novel theoretical analysis of NGD behavior in linear models', 'Rigorous mathematical proofs for key claims', 'Experimental results supporting theoretical findings', 'Challenges common assumptions about NGD benefits', 'Develops efficient algorithms for exact NGD in studied models']","['Analysis primarily limited to linear networks', 'Experiments somewhat limited in scale and scope', 'Practical implications for deep learning not fully explored', 'Limited discussion of potential benefits of NGD']",4,3,3.0,3,Accept
zyrhwrd9EYs,"This paper introduces Mixed Confounded Missingness (MCM), a novel framework for handling missing data in treatment effect estimation. MCM distinguishes between missingness that determines treatment selection and missingness determined by treatment. The authors propose a 'selective imputation' approach based on MCM and demonstrate improved performance over baseline methods on synthetic data, supported by theoretical analysis.","['Can you provide results on real-world datasets to validate the approach?', 'How does the proposed method compare to other recent, more sophisticated approaches for handling missing data in causal inference?', 'What are the key practical recommendations for applying MCM and selective imputation in real-world scenarios?', 'How sensitive is the method to violations of the MCM assumptions?', 'How might the MCM framework and selective imputation approach be extended to non-binary treatments or time-varying settings?']","['Evaluation limited to synthetic data', 'May be challenging to implement in practice without knowledge of the data generation process', 'Assumes binary treatment, may not generalize to multiple treatments or time-varying settings', 'Computational complexity of selective imputation not thoroughly discussed']",False,3,3,3,6,4,"['Introduces a novel and important framework (MCM) for missing data in causal inference', 'Provides thorough theoretical justification for the proposed approach', 'Demonstrates consistent empirical improvements over baselines across different treatment effect estimation methods', 'Addresses an important problem in causal inference and treatment effect estimation']","['Empirical evaluation limited to synthetic data, lacking real-world validation', 'Clarity of presentation could be improved in some sections', 'Limited discussion of practical implications and comparisons to existing sophisticated approaches', 'Sensitivity analysis and discussion of limitations could be more comprehensive']",3,4,3.0,4,Accept
NZQ8aTScT1-,"This paper presents a theoretical framework for analyzing how different neural network architectures, particularly CNNs, affect the eigenstructure of their corresponding infinite-width kernels. The main contribution is an 'eigenspace restructuring theorem' demonstrating that convolutional architectures can efficiently learn a broader class of functions with various combinations of spatial range and frequency compared to MLPs. The theoretical results are supported by experiments showing good agreement with both kernel methods and finite-width networks.","['How well do the asymptotic results translate to practical finite-width networks?', 'What are the key limitations of the infinite-width approximations used in the theory?', 'Are there any implications or recommendations for neural architecture design based on these results?', 'How might this analysis extend to other architectures like transformers or graph neural networks?']","['Focuses on infinite-width networks which may not fully capture practical finite-width behavior', 'Analyzes relatively simple CNN architectures rather than more modern designs', 'Limited discussion of implications for CNN design in practice', 'Potential negative societal impacts are not thoroughly addressed', 'Generalization to real-world datasets and tasks is not extensively explored']",False,3,2,3,7,4,"['Provides novel theoretical insights into the inductive biases of different architectures', 'Rigorous mathematical analysis with precise characterizations', 'Experimental results align well with theoretical predictions for both kernel methods and finite-width networks', 'Contributes to understanding fundamental principles behind CNN architectures']","['Very technical presentation that could be improved for clarity', 'Limited discussion of practical implications and broader impact', 'Analysis is limited to the infinite-width kernel regime, which may not fully capture practical finite-width network behavior', 'Experiments are primarily on synthetic data, not real-world tasks']",3,3,2.0,3,Accept
IEKL-OihqX0,"This paper introduces ratio matching with gradient-guided importance sampling (RMwGGIS), a novel method for learning discrete energy-based models (EBMs). The approach uses the gradient of the energy function with respect to discrete inputs to guide importance sampling when estimating the ratio matching objective. Experiments on synthetic data, graph generation, and Ising models demonstrate significant improvements over baselines in both effectiveness and efficiency.","['How well does the method scale to very high-dimensional discrete data (e.g., 10,000+ dimensions)?', 'How sensitive is the method to the choice of number of importance samples s?', 'Can you provide more theoretical insight into why the biased version performs better?', 'How does the method compare to very recent alternative approaches for discrete EBMs?', 'What are potential applications of this method beyond the demonstrated tasks?']","['May not scale well to extremely high-dimensional data', 'Only tested on relatively simple datasets so far', 'Theoretical analysis relies on some approximations that may not always hold', 'Potential negative impacts include increased computational resource requirements and possible biases in learned models if not carefully implemented']",False,3,4,3,7,4,"['Novel technical approach combining ratio matching, importance sampling, and gradients for discrete EBMs', 'Strong theoretical justification and analysis', 'Comprehensive experiments showing clear improvements over baselines on multiple tasks', 'Significant efficiency gains in computation time and memory usage', 'Addresses an important problem in learning discrete EBMs', 'Thorough ablation studies and analysis of the proposed method']","['Limited evaluation on very high-dimensional or complex real-world datasets', 'Potential scalability issues for extremely high-dimensional data', 'Some reliance on approximations that may not always hold', 'Lack of comparison to some recent alternative approaches for discrete EBMs']",4,3,4.0,3,Accept
ZOcX-eybqoL,"This paper introduces SOPGOL, a framework for lifelong reinforcement learning using logical composition of skills. It extends previous work on Boolean algebra over tasks to stochastic and discounted settings. The key contributions include theoretical bounds on transfer performance and generalization, a method to determine when to reuse skills vs. learn new ones, and empirical validation on grid world and simple visual domains.","['How well does this approach scale to high-dimensional state/action spaces?', 'Could the framework be extended to handle more complex, non-binary reward structures?', 'How does performance compare to other recent lifelong/transfer RL methods on more complex benchmark tasks?', 'What are the key challenges in scaling this approach to more realistic domains?', 'What is the computational complexity of the approach in large task spaces?']","['Currently limited to tasks with binary goal rewards', 'Empirical evaluation only on relatively simple domains', 'Scalability to complex high-dimensional problems not demonstrated', 'Potential computational complexity in large task spaces not thoroughly analyzed']",False,3,4,4,8,4,"['Novel theoretical framework for lifelong RL using logical skill composition', 'Strong theoretical results bounding transfer performance and generalization', 'Clear writing and exposition of ideas', 'Demonstrates good empirical performance on evaluated tasks', 'Enables zero-shot solving of new tasks expressible as logical combinations of previous tasks']","['Empirical evaluation limited to relatively simple domains', 'Currently only considers tasks with binary goal rewards', 'Scalability to high-dimensional problems not clearly demonstrated', 'Limited comparison to other state-of-the-art lifelong/transfer RL methods']",4,3,4.0,4,Accept
hUr6K4D9f7P,"This paper introduces Weighted Truncated Adversarial Weight Perturbation (WT-AWP), a novel regularization technique for graph neural networks (GNNs). The authors identify and address limitations in existing adversarial weight perturbation methods when applied to GNNs, particularly a vanishing gradient issue. WT-AWP is shown to improve both clean accuracy and robustness across various GNN architectures, datasets, and attack scenarios. The paper provides theoretical analysis, including a new generalization bound for non-i.i.d. graph data, and extensive empirical evaluation.","['How sensitive are the results to the choice of hyperparameters λ and ρ?', 'Could the authors provide more insight on why WT-AWP is particularly effective for GNNs compared to other neural architectures?', 'How does WT-AWP perform on more diverse graph learning tasks beyond node classification?', 'What is the computational overhead of WT-AWP compared to standard training?', 'How does WT-AWP compare to other recent GNN robustness techniques not included in the evaluation?', 'Could WT-AWP be applied effectively to other types of neural networks beyond GNNs?']","['The method introduces additional hyperparameters that may require tuning for each dataset/model', 'Experiments are primarily limited to node classification tasks on standard benchmark datasets', 'Potential impacts on model fairness or introduction of biases are not explored', 'May not be as effective for very deep GNN architectures (most experiments use 2-layer GNNs)']",False,3,3,3,7,4,"['Novel WT-AWP technique that effectively addresses limitations of existing methods for GNNs', 'Theoretical analysis including a new generalization bound for non-i.i.d. graph data', 'Comprehensive empirical evaluation across multiple GNN architectures, datasets, and robustness measures', 'Consistent improvements in both clean accuracy and robustness to various attacks', 'Method is simple to implement with minimal computational overhead']","['Improvements, while consistent, are often relatively small in magnitude (1-2% in many cases)', 'Introduces additional hyperparameters that require tuning', 'Experiments are primarily focused on node classification tasks', 'Some experimental details are in the appendix rather than the main text', 'Comparison to some recent GNN robustness techniques is missing', 'Limited exploration of performance on deeper GNN architectures']",4,3,3.0,4,Accept
1xXvPrAshao,"This paper introduces MEME, a novel and innovative approach to multimodal VAEs that uses mutual supervision between modalities instead of explicit combinations. Key contributions include a novel formulation for handling partially observed data naturally, using a bidirectional objective for mutual supervision, and techniques for improved training. Evaluated on MNIST-SVHN and CUB datasets, MEME shows significant improvements over baselines, especially in partially observed data settings.","['How does the method scale to more than two modalities in practice?', 'Are there any theoretical guarantees on the performance or convergence of the method?', 'How does the method perform on more complex real-world multimodal datasets?', 'What are the computational requirements compared to existing methods?', 'What are the potential real-world applications or impact of this work?']","['Focus on bimodal data only', 'Potential scalability issues with more modalities', 'Lack of evaluation on more complex real-world datasets', 'No discussion of potential negative societal impacts']",False,3,3,3,7,4,"['Novel formulation using mutual supervision instead of explicit combinations', 'Ability to handle partially observed data naturally', 'Strong empirical results on standard datasets and metrics', 'Comprehensive ablation studies and analysis', 'Clear explanation of method and implementation details', ""Thorough empirical evaluation demonstrating the method's effectiveness""]","['Limited to bimodal data, though extension is discussed', 'Lack of theoretical analysis or guarantees', 'Potential scalability issues with more modalities', 'Limited evaluation on more complex real-world datasets']",4,3,3.0,3,Accept
IhkSFe9YqMy,"This paper introduces Experience Replay More (ERM), a novel experience replay mechanism for off-policy reinforcement learning. ERM classifies transitions as 'key' or 'non-key', stores them separately, and samples more recent and key transitions with higher probability using a linear distribution. The authors combine ERM with DDPG, TD3, and SAC algorithms and evaluate it on MuJoCo continuous control tasks, demonstrating consistent performance improvements.","['How does ERM compare to other experience replay methods, particularly Prioritized Experience Replay?', 'How sensitive are the results to key hyperparameters like sampling ratios?', 'How well does ERM generalize to other RL domains beyond MuJoCo?', 'Is there any theoretical justification for why ERM should improve learning?', 'What is the additional computational overhead of ERM compared to standard experience replay?', 'Could ERM be combined with other prioritization methods for potentially greater improvements?']","['Evaluation limited to a set of MuJoCo continuous control tasks', 'May not generalize well to other types of RL problems or more complex environments', 'Increased computational overhead from maintaining separate replay buffers', 'Additional hyperparameters that may require tuning', 'Potential negative societal impacts not thoroughly discussed']",False,3,3,3,6,3,"['Novel and intuitive approach to prioritizing certain types of transitions in experience replay', 'Clear explanation of the algorithm and implementation details', 'Consistent empirical improvements over baseline algorithms on benchmark tasks', 'Demonstrates compatibility with multiple off-policy algorithms (DDPG, TD3, SAC)', 'Particularly large gains for DDPG, bringing it closer to SAC performance']","['Limited theoretical justification for the approach', 'Lack of comparison to other experience replay methods, particularly Prioritized Experience Replay', 'Evaluation limited to MuJoCo tasks, raising questions about generalizability', 'Introduces additional hyperparameters that may require tuning', 'Improvements are modest in some cases, especially for state-of-the-art algorithms like SAC']",3,3,3.0,3,Accept
FpKgG31Z_i9,"This paper introduces 'learning rate grafting', a novel technique for transferring implicit learning rate schedules between optimizers. The authors demonstrate that grafting can transfer performance between adaptive and non-adaptive optimizers on large-scale vision and language models, and use it to discover effective learning rate corrections. Notably, they enable SGD to train BERT competitively for the first time, providing insights into the relative importance of adaptive preconditioning versus implicit schedules in popular optimizers.","['Can you provide more theoretical analysis or intuition for why grafting works?', 'How well do you expect grafting to generalize to other architectures or tasks not tested?', 'What are the practical considerations for implementing grafting in a new setting?', 'How computationally expensive is the grafting process compared to standard hyperparameter tuning?', 'Are there any potential negative consequences of using grafting in practice, such as overfitting to specific model architectures?']","['The grafting technique may not be easily applicable to all optimizers or models', 'The discovered schedules may not generalize well to other tasks or architectures', 'Potential computational overhead of running two optimizers in parallel', 'Lack of strong theoretical grounding']",False,4,3,4,7,4,"['Novel and insightful technique (grafting) for analyzing and transferring optimizer performance', 'Strong empirical results on large-scale vision and language models', 'Enables training BERT with SGD competitively for the first time', 'Provides useful insights for practitioners on optimizer behavior', 'Potential to simplify optimizer comparisons and reduce tuning costs', 'Could significantly impact future research on optimization algorithms']","['Limited theoretical justification for why grafting works, which may hinder its broader adoption and understanding', 'Unclear how broadly the insights generalize beyond tested models', 'Discovered schedules/corrections lack principled derivation', 'Some results (e.g. SGD training BERT) may be sensitive to hyperparameters']",4,3,3.0,4,Accept
eo1barn2Xmd,"This paper introduces SLIM-QN, a novel stochastic quasi-Newton optimization method for training large neural networks. Key innovations include incorporating momentum and adaptive damping into the Hessian approximation to enhance stability and convergence in stochastic settings. The authors provide theoretical convergence guarantees and demonstrate empirically faster convergence than SGD on large-scale problems like ImageNet classification, particularly for ResNet and Vision Transformer models.","['How does SLIM-QN perform on NLP tasks and other domains beyond computer vision?', 'Can you demonstrate the scaling of SLIM-QN to very large models with billions of parameters?', 'How does SLIM-QN compare to adaptive optimizers like Adam, especially on tasks where they typically outperform SGD?', 'What is the sensitivity of SLIM-QN to hyperparameter choices, particularly for the Hessian approximation components?']","['Experiments are limited in scope, focusing mostly on computer vision tasks', 'Scalability to very large models is not fully demonstrated', 'The method introduces additional hyperparameters that require tuning', 'Potential limitations in distributed training scenarios are not thoroughly explored']",False,4,3,3,7,4,"['Novel combination of techniques (BFGS updates, momentum, adaptive damping) addressing limitations of existing second-order methods in deep learning', 'Strong theoretical analysis with linear convergence guarantees', 'Impressive empirical results showing 1.5x faster convergence than SGD on ImageNet', 'Lower computational and memory costs compared to other second-order methods like KFAC', 'Detailed ablation studies validating the importance of key components', 'Extension to distributed training via block-wise SLIM-QN', 'Clear and well-organized presentation of the method and results']","['Limited experimental evaluation, primarily focused on computer vision tasks', 'Lack of comparisons to other advanced optimizers like Adam', 'Scalability to very large models (billions of parameters) not fully demonstrated', 'Introduction of additional hyperparameters that require tuning', 'Wall-clock time improvements could be more thoroughly analyzed']",3,4,3.0,4,Accept
dg79moSRqIo,"This paper introduces Discovery of Incremental Skills (DISk), a novel method for unsupervised skill discovery in reinforcement learning. Unlike previous approaches that learn all skills simultaneously, DISk learns skills incrementally one after another. This allows new skills to adapt to changes in the environment while preserving previously learned skills. Empirical results demonstrate improved performance over baselines like DIAYN and Off-DADS, especially in evolving environments, on metrics of skill quality and downstream task learning.","['How well does the method scale to learning very large numbers of skills (e.g. 100+)?', 'Could the approach be extended to use a single shared network while still maintaining the benefits of incremental learning?', 'How sensitive is the performance to the choice of hyperparameters?']","['The method is only evaluated on simulated environments, not real-world tasks', 'Scalability to very large numbers of skills is not demonstrated', 'No theoretical guarantees are provided on the optimality of the learned skills']",False,3,4,4,7,4,"['Novel incremental approach to skill discovery that addresses limitations of previous methods', 'Ability to adapt to changing environments without catastrophic forgetting', 'Comprehensive empirical evaluation on both static and evolving environments', 'Improved performance on skill quality metrics and downstream task learning', 'Thorough ablation studies validating key design choices']","['Limited theoretical analysis - results are mostly empirical', 'May not scale well to very large numbers of skills', 'Underperforms on simpler environments like HalfCheetah', 'Added complexity from using separate neural networks for each skill', 'Relies on hand-designed projection function for measuring state similarity']",4,3,4.0,4,Accept
DrCsriMQ1o,"This paper introduces a novel approach for generating counterfactual explanations using sum-product networks (SPNs), a type of tractable probabilistic model. The method employs a two-step gradient-based process: first perturbing the input to change its predicted class, then moving it to a higher density region. Empirical evaluations across multiple datasets, including complex real-world data, demonstrate that the approach generates plausible counterfactuals faster than optimization-based baselines, with improved density awareness.","['How sensitive is the method to the choice of epsilon values?', 'How well does the approach scale to very high-dimensional inputs?', 'Are there any theoretical guarantees on the properties of the generated counterfactuals?', 'How does the method perform on datasets with more complex decision boundaries?']","['The approach requires training an SPN model, which may be challenging for some datasets', 'Performance may degrade for very high-dimensional inputs', 'May not work well for datasets with complex, non-smooth decision boundaries', 'Limited analysis of potential negative societal impacts']",False,3,3,3,6,4,"['Novel use of tractable probabilistic models (SPNs) for counterfactual generation', 'Significantly faster computation times compared to optimization-based baselines', 'Explicit density awareness leading to more realistic counterfactuals', 'Effective on complex real-world datasets (e.g., CUB bird classification)', 'Clear and intuitive explanation of the approach', 'Comprehensive empirical evaluation on multiple datasets and metrics']","['Limited theoretical analysis or guarantees', 'Relatively small set of baselines compared, especially lacking very recent state-of-the-art methods', 'Success rate slightly lower than some baselines on certain datasets', 'Unclear scalability to very high-dimensional problems', 'Limited discussion of potential failure cases or limitations']",4,3,3.0,3,Accept
UseMOjWENv,"This paper introduces MIDI-DDSP, a hierarchical generative model for music synthesis that provides both realistic audio generation and detailed control for 13 different instruments. The model uses a 3-level hierarchy (notes, performance, synthesis) and introduces interpretable mid-level 'expression attributes' to enable fine-grained control of performance characteristics. Extensive evaluations, including listening tests, demonstrate improvements over baselines in audio quality and controllability.","['How might this approach be extended to polyphonic instruments and multi-instrument transcription?', 'Could the expression attributes be learned in an unsupervised way rather than hand-designed?', 'How well does the model generalize to instruments or musical styles not seen during training?', 'What is the computational cost and latency of the full pipeline, especially for real-time applications?']","['Limited to monophonic instruments currently', 'Relies on accurate pitch and note detection', 'May not capture all aspects of musical expression with the current attribute set', 'Potential for misuse in generating fake performances', 'Possible high computational requirements for real-time use']",False,4,3,4,8,4,"['Novel hierarchical approach combining realistic synthesis and detailed control', 'Introduction of interpretable mid-level expression attributes', 'Strong empirical results, especially in listener studies', 'Comprehensive evaluation including quantitative and qualitative results', 'Demonstrates both fine-grained control and fully automated generation', 'Ability to model multiple instruments in a single model', 'Can connect with other generative models for full-stack music generation']","['Currently limited to monophonic instruments', 'Relies on potentially error-prone pitch/note detection', 'Evaluation focused mainly on violin, less comprehensive for other instruments', 'Expression attributes are hand-designed rather than learned']",4,4,3.0,4,Accept
kiNEOCSEzt,This paper proposes a method to estimate and penalize unwanted preference shifts induced by recommender systems. The key contributions are: (1) A neural network approach to estimate future and counterfactual user preferences from historical data. (2) A framework to define 'safe shifts' in preferences and quantify deviations. (3) An approach to incorporate preference shift metrics into RL-based recommender system training. The work is significant as it addresses an important ethical challenge in AI systems that can influence user preferences over time.,"['How well would this approach scale to real-world recommender systems with millions of users and items?', 'What are the computational requirements for training and deploying this system at scale?', 'How sensitive is the method to violations of the assumptions about user behavior and preferences?', 'Have you considered any ways to validate aspects of the approach on real user data?', 'Are there potential negative consequences of penalizing certain preference shifts, and how might these be mitigated?']","['Evaluation limited to simulated data', 'Strong assumptions about user behavior may not hold in practice', 'Computational scalability not thoroughly addressed', 'Privacy implications of detailed preference modeling not discussed', ""May be challenging to define appropriate 'safe shifts' in complex domains""]",False,3,3,3,7,4,"['Addresses an important ethical issue in recommender systems', 'Novel technical approach combining preference estimation and RL', 'Principled framework for defining and penalizing undesirable preference shifts', 'Thorough theoretical justification and analysis', 'Promising initial empirical results in simulated environments', 'Potential to significantly impact the development of more ethical AI systems']","['Evaluation limited to simulated environments, lacking real-world validation which is crucial for assessing practical applicability', 'Strong assumptions about user behavior and preferences that may not hold in complex real-world scenarios', 'Unclear scalability to real-world, complex recommender systems', 'Limited discussion on computational requirements and practical challenges', 'Potential privacy implications not thoroughly addressed']",3,3,3.0,4,Accept
gLqnSGXVJ6l,This paper presents a reinforcement learning approach using an attention-based encoder-decoder neural network to solve the Vehicle Routing Problem with Time Windows (VRPTW). The method extends previous neural combinatorial optimization techniques to incorporate time window constraints. Experiments on small to medium-sized synthetic VRPTW instances (up to 50 nodes) demonstrate competitive performance compared to classical solvers like OR-Tools and LKH-3 in terms of solution quality and runtime.,"['How does the approach scale to larger problem sizes (e.g., 100+ nodes)?', 'What is the impact of different components, such as the attention mechanism, on overall performance?', 'How does the method compare to a wider range of VRPTW solvers, including specialized algorithms?', 'Have you evaluated the approach on any real-world VRPTW datasets?', 'What is the theoretical time/space complexity of the approach?', 'What are potential future extensions or improvements to this method?']","['Evaluation limited to small synthetic datasets', 'May not scale well to very large real-world routing problems', 'Runtime slower than classical methods for larger problems', 'Lacks theoretical analysis of complexity or optimality guarantees']",False,3,3,3,6,4,"['Successfully extends neural combinatorial optimization to handle VRPTW constraints', 'Demonstrates competitive performance on small problem instances compared to classical solvers', 'Provides a clear explanation of the methodology and model architecture', 'Addresses an important practical problem (VRPTW)', 'Offers an end-to-end learning framework for VRPTW', 'Clear and detailed explanation of the proposed methodology']","['Experiments limited to relatively small problem sizes (up to 50 nodes)', 'Lack of scalability analysis for larger problems (100+ nodes)', 'Limited comparison to other solvers, especially specialized VRPTW algorithms', 'Absence of evaluation on real-world datasets', 'Limited ablation studies or analysis of different model components']",3,3,3.0,3,Accept
5XmLzdslFNN,"This paper introduces a framework for lifelong reinforcement learning of compositional tasks using modular neural networks. It formulates the problem of lifelong compositional RL, proposes new evaluation domains, develops a modular lifelong RL algorithm, and uses batch RL to avoid catastrophic forgetting. The multi-stage learning process includes module selection, adaptation, and updating. Experiments on 2D grid world and robotic manipulation tasks demonstrate faster learning, effective knowledge transfer, and avoidance of catastrophic forgetting compared to baselines.","['How would the approach scale to problems requiring many more modules/tasks?', 'Could the discrete module search be replaced with a more efficient method?', 'How does the method compare to other recent compositional RL approaches beyond the baselines used?', 'Have you evaluated on any other domains beyond the two presented?', 'How might the approach be extended to handle more complex, hierarchical compositional structures?']","['Scalability with number of modules and tasks may be limited', 'Evaluation on only two relatively simple domains', 'Reliance on manually specified module structures', 'Potential negative impacts not thoroughly discussed', 'Potentially high computational cost, especially for robotic manipulation tasks']",False,3,4,3,7,4,"['Novel formulation of lifelong compositional RL problem', 'Development of modular lifelong RL algorithm that can discover compositional structures', 'Introduction of new compositional RL evaluation domains', 'Effective use of batch RL techniques to avoid catastrophic forgetting', 'Strong empirical results demonstrating faster learning, knowledge transfer, and retention', 'Thorough ablation studies validating key components']","['Scalability to very large numbers of modules/tasks may be limited', 'Evaluation limited to two relatively simple domains', 'Lack of comparison to some recent modular RL approaches', 'Reliance on manually designed module structures']",4,3,4.0,4,Accept
fEcbkaHqlur,"This paper introduces Functional Regularization (FR) as an alternative to target networks in Deep Q-Learning. FR decouples the stabilization role from target value estimation, allowing the use of up-to-date Q-values while adding explicit regularization. The approach is supported by theoretical analysis in the tabular case and empirical results on Four Rooms and Atari game environments.","['How does FR perform on a broader set of Atari games and other RL environments?', 'Can the theoretical convergence analysis be extended to function approximation?', 'How does FR compare to more recent DQN variants like Rainbow?', 'How sensitive is FR performance to the choice of regularization weight κ across different environments?']","['Evaluation limited to a subset of environments', 'Theoretical analysis limited to tabular case', 'Potential increased computational cost not thoroughly analyzed', 'May require tuning of additional hyperparameter κ']",False,3,3,3,7,4,"['Novel approach to stabilizing deep Q-learning that addresses limitations of target networks', 'Theoretical analysis provides insights into the relationship with target networks', 'Consistent empirical improvements over baselines on multiple environments', 'Conceptually simple modification that can be easily integrated into existing algorithms']","['Empirical evaluation limited to a subset of Atari games', 'Improvements over baselines are relatively modest in some cases', 'Limited comparison to recent DQN variants beyond Double DQN', 'Theoretical analysis limited to the tabular case']",3,3,4.0,4,Accept
RMv-5wMMrE3,"This paper introduces cell2state, a method for analyzing single-cell gene expression data with lineage information from genetic barcoding. It maps high-dimensional gene expression profiles to low-dimensional cell state vectors predictive of future cell dynamics. Theoretical analysis and empirical evaluation demonstrate advantages over baselines on tasks like cell fate prediction and clustering.","['How does cell2state compare to other recent single-cell trajectory inference methods?', 'What are the computational requirements and runtimes, especially for large datasets?', 'How sensitive is the method to hyperparameter choices?', 'Can you provide more rigorous validation of the biological interpretations?', 'How robust is the method to noise in gene expression data?']","['Method requires genetic barcoding data which limits broader applicability', 'Complexity may make it challenging for biologists to adopt', 'Performance on other types of single-cell data is not explored', 'Theoretical guarantees may not hold if underlying assumptions are violated']",False,3,3,3,7,4,"['Novel use of genetic barcoding data for trajectory analysis', 'Strong theoretical foundations with information-theoretic analysis', 'Improved performance on downstream tasks compared to baselines', 'Potential to reveal biologically meaningful insights about marker genes and pathways', 'Thorough evaluation on both real and simulated datasets']","['Complexity of method may limit accessibility to biologists', 'Limited comparison to other single-cell trajectory methods', 'Evaluation on only one real dataset', 'Some biological interpretations need further validation', 'Presentation could be improved for clarity']",4,3,3.0,3,Accept
DfMqlB0PXjM,"This paper introduces Hierarchical Divnoising (HDN), a novel unsupervised method for diversity denoising and artifact removal in images. HDN utilizes a hierarchical VAE architecture to learn an interpretable multi-scale representation. The method achieves state-of-the-art results on 12 denoising benchmarks and enables removal of structured artifacts in microscopy images without supervision. Importantly, HDN can generate diverse plausible denoised outputs, capturing uncertainty in the denoising process.","['How well does the artifact removal approach generalize to non-microscopy data?', 'What is the theoretical justification for the hierarchical VAE improving denoising performance?', 'How does the method compare to more recent state-of-the-art denoising approaches?', 'Can the computational efficiency be improved further?']","The authors adequately address limitations, noting that the artifact removal approach may not work when artifacts are at the same scale as true signals. The method's generalizability beyond microscopy data and its performance on natural images could be further improved. The computational cost is higher compared to some baseline methods. The theoretical foundations of the approach could be more thoroughly explored.",False,4,3,4,8,4,"['Novel hierarchical VAE architecture for unsupervised diversity denoising', 'State-of-the-art results on multiple denoising benchmarks', 'Interpretable approach for removing structured artifacts in microscopy images', 'Ability to generate diverse plausible denoised samples', 'Extensive experiments and comparisons to baselines', 'Practical utility demonstrated for microscopy applications']","['Limited theoretical analysis or justification for the approach', 'Artifact removal method may not generalize well beyond microscopy data', 'Higher computational cost compared to some baselines', 'Performance on natural images still has room for improvement']",4,4,3.0,4,Accept
R612wi_C-7w,"This paper introduces the Resetting Path Integrator (RPI), a novel recurrent neural network architecture for path integration and cognitive map formation. The RPI fuses visual and proprioceptive inputs using direct-inverse models of environment dynamics and a gating mechanism for occasional resetting based on visual input. Experiments in 2D grid world environments demonstrate the RPI's ability to perform path integration, develop allocentric representations, and handle ambiguous visual inputs, outperforming LSTM baselines in stability and interpretability.","['How would the RPI model perform in more complex 3D environments?', 'How does this approach compare to other recent cognitive mapping models from neuroscience and AI?', 'What are the implications of this work for understanding biological navigation and cognitive map formation?', 'How well does the learned model transfer to new environments or tasks?', 'What are potential real-world applications of this research in robotics or autonomous navigation?']","['Experiments are limited to simple 2D environments', 'May not scale well to more complex real-world navigation tasks', 'Biological plausibility and neuroscience connections could be expanded', 'Limited evaluation of transfer and generalization capabilities', 'Computational complexity of the model may be a concern for practical applications']",False,3,4,3,7,4,"['Novel RPI architecture effectively fuses visual and proprioceptive information', 'Demonstrates formation of stable cognitive map-like representations', 'Thorough analysis of internal representations and gating behavior', 'Effective comparison to LSTM baselines', 'Clear and well-presented experiments and results']","['Experiments limited to relatively simple 2D grid world environments', 'Lack of comparison to other recent cognitive mapping approaches beyond LSTMs', 'Limited discussion of biological plausibility and neuroscience connections', 'Some hyperparameters not optimized, potentially limiting performance comparisons']",4,3,4.0,4,Accept
Zq2G_VTV53T,"This paper introduces FastSHAP, a novel method for efficiently estimating Shapley values to explain machine learning model predictions. The key innovation is training an explainer model to output Shapley estimates in a single forward pass, using a custom loss function derived from the weighted least squares characterization of Shapley values. Extensive experiments on tabular and image data demonstrate that FastSHAP achieves orders of magnitude speedup over existing approaches while maintaining or improving accuracy, and outperforms gradient-based methods on image explanation tasks.","['How well does FastSHAP scale to inputs with much higher dimensionality (e.g. high resolution images)?', 'Are there any theoretical bounds on the approximation error of FastSHAP compared to true Shapley values?', 'How sensitive is the performance to the architecture and capacity of the explainer model?', 'How well does FastSHAP generalize to explaining models or data distributions not seen during training?', 'What is the trade-off between training time for the explainer model and inference speed gains?']","['Requires sufficient training data and model capacity to learn accurate Shapley estimates', 'May not be suitable for very low-data regimes or one-off predictions due to training overhead', 'Potential negative impact of providing explanations that could be used to game or attack models', 'Approach may not be suitable for applications requiring exact Shapley values due to approximation', 'Performance may degrade for out-of-distribution samples or models not seen during training']",False,4,4,4,8,4,"['Innovative approach to amortizing Shapley value computation', 'Strong theoretical foundation and analysis', 'Significant speedup over existing methods while maintaining accuracy', 'Comprehensive experiments on both tabular and image data', 'Outperforms gradient-based methods on image explanation tasks', 'Clear writing and thorough explanation of contributions']","['Requires training a separate explainer model, adding some complexity and overhead', 'Scalability to very high-dimensional inputs not clearly demonstrated', 'Limited theoretical analysis of convergence properties and error bounds', 'Performance may degrade for out-of-distribution samples', 'Potential computational cost of training the explainer model']",4,4,4.0,4,Accept
VyZRObZ19kt,"This paper proposes a framework for dynamically adjusting the error bound parameter ε in learned index structures based on local data characteristics. It provides theoretical analysis, develops an ε-learner module, and demonstrates improved space-time tradeoffs across multiple datasets and learned index methods.","['How does the computational overhead of the ε-learner impact index construction and query performance across different dataset sizes?', 'Can the theoretical analysis be extended to other learned index methods beyond MET?', 'How sensitive is the performance to the hyperparameters of the ε-learner module?', ""What is the method's performance on very large datasets or in distributed settings?"", 'Can this approach be generalized to other domains where adaptive error bounds might be beneficial?']","['Primarily focused on single-machine settings and may not scale well to very large datasets', 'Limited analysis of dynamic/update scenarios', 'Introduces additional hyperparameters that need tuning', 'Potential increased complexity for practitioners']",False,3,3,3,6,4,"['Novel approach of dynamically adjusting ε based on local data properties', 'Sound theoretical analysis, particularly for the MET algorithm', 'Consistent empirical improvements across various datasets and learned index methods', 'Pluggable framework applicable to multiple learned index structures', 'Thorough experimental evaluation and ablation studies']","['Theoretical analysis primarily focused on one specific method (MET)', 'Improvements, while consistent, are modest in some cases (7-25%)', 'Additional complexity and computational overhead introduced by ε-learner module', 'Limited applicability beyond learned index structures']",3,3,3.0,4,Accept
ZDYhm_o8MX,"This paper introduces Neural Manifold Clustering and Embedding (NMCE), a novel method for unsupervised clustering and representation learning of data on multiple manifolds. NMCE combines data augmentation to implement geometric constraints with the MCR2 objective for subspace learning, employing a multi-stage training procedure. The method demonstrates strong performance on several clustering benchmarks and image datasets, outperforming previous approaches.","['Could the multi-stage training procedure be simplified or made more principled?', 'How well does the method perform on more complex real-world datasets beyond the standard benchmarks?', 'How sensitive is the method to the choice of data augmentations and other hyperparameters?', 'Can you provide more analysis on the computational requirements and scalability of the method?']","['The multi-stage training procedure adds complexity and may limit practical applicability', 'Evaluation is limited to mostly standard benchmarks and image datasets', 'Scalability to very large datasets or high-dimensional data not thoroughly demonstrated', 'Potential societal impacts, both positive and negative, not thoroughly discussed']",False,3,3,3,7,4,"['Novel combination of data augmentation constraints and subspace learning for manifold clustering', 'Strong empirical results outperforming previous methods on multiple benchmarks', 'Good theoretical motivation and analysis', 'Insightful connections drawn to self-supervised learning', 'Detailed ablation studies and analysis of learned representations']","['Multi-stage training procedure seems complex and somewhat ad-hoc', 'Limited evaluation on more complex real-world datasets beyond standard benchmarks', 'Some sensitivity to hyperparameters', 'Computational cost concerns for larger models']",4,3,3.0,3,Accept
ziRLU3Y2PN_,"This paper introduces ALPHA, a new family of texture synthesis models based on wavelet transforms and generalized rectifier non-linearities, applicable to both gray-scale and color textures. The approach aims to bridge classical wavelet-based methods with modern CNN-based techniques, offering competitive synthesis quality with fewer statistics and improved interpretability. The authors explore various model configurations, analyzing trade-offs between synthesis quality, diversity, and model complexity.","['Can you provide more quantitative evaluation metrics to support the visual comparisons?', 'How does the computational efficiency of your model compare to CNN-based approaches?', 'Is it possible to further reduce the number of statistics while maintaining synthesis quality, especially for the color model?', 'How might the approach be extended to handle more structured or non-stationary textures?']","['The model struggles with highly structured or non-stationary textures', 'Potential memorization effects for non-stationary images', 'Evaluation relies heavily on qualitative visual assessment', 'The number of statistics, while reduced, is still relatively high for some variants']",False,3,3,4,7,4,"['Develops an innovative middle ground between classical wavelet models and modern CNN approaches', 'Achieves comparable visual quality to state-of-the-art CNN models with fewer statistics', 'Improves significantly upon classical wavelet models like Portilla & Simoncelli', 'Provides insights into quality-diversity trade-offs and model complexity', 'Thorough experimental comparisons with existing methods', 'Mathematically rigorous formulation with proofs', 'Comprehensive analysis of different model variants (ALPHAS, ALPHAI, ALPHAL)']","['Evaluation relies heavily on qualitative visual assessment', 'Limited quantitative evaluation metrics beyond VGG loss', 'Performance on highly structured or non-stationary textures is still limited', 'Some memorization effects observed for non-stationary images', 'The full color model still uses a large number of statistics', 'Reliance on specific wavelet families, which can impact synthesis quality']",3,3,3.0,3,Accept
Czsdv-S4-w9,"This paper introduces DIGAN, a novel video generation method combining implicit neural representations (INRs) and GANs. Key contributions include modeling videos as continuous functions using INRs, a generator that separates content and motion, and an efficient motion discriminator. The approach demonstrates significant improvements over state-of-the-art methods on standard benchmarks and enables new capabilities such as long video generation and temporal/spatial interpolation/extrapolation.","['How does the method scale to higher resolutions (e.g., 4K) or very long videos (e.g., hours) in terms of computational requirements?', 'How does the approach perform on more diverse, real-world video datasets?', 'Could this method be extended to handle variable-length videos or audio-visual generation?', 'How does the training time and stability compare to previous methods?', 'What are some potential real-world applications of this technology, and how might it impact existing industries?']","['Current evaluation is limited to videos of 128 frames or less', 'Method assumes fixed video length during training', 'Potential for misuse in generating fake or misleading videos', 'Computational requirements for training on very long videos are not fully explored']",True,4,3,4,8,4,"['Innovative use of INRs for video generation, allowing for efficient modeling of longer videos', 'Effective decomposition of content and motion in the generator', 'Efficient motion discriminator design using only pairs of frames', 'Substantial improvements over state-of-the-art on multiple benchmarks', 'Enables novel capabilities like long video generation and interpolation/extrapolation', 'Thorough empirical evaluation and ablation studies', 'Extensive ablation studies demonstrating the impact of each component']","['Scalability to very high resolutions or extremely long videos is not fully explored', 'Visual quality, while improved, may still have limitations on complex datasets', 'Method complexity may pose challenges for implementation and reproducibility', 'Limited comparison to very recent video generation approaches', 'Potential ethical concerns of advanced video generation not thoroughly addressed']",4,4,4.0,4,Accept
18Ys0-PzyPI,"This paper introduces ODITS, a novel reinforcement learning framework for ad hoc teamwork under partial observability. The key innovation lies in learning latent representations of teamwork situations, using an information-theoretic approach to infer proxy representations under partial observability, and conditioning the ad hoc agent's policy on these inferred representations. Experimental results demonstrate ODITS outperforms baseline approaches across multiple cooperative environments.","['How does ODITS handle highly dynamic environments where teamwork situations change rapidly?', 'Can you provide insights on the computational complexity of ODITS compared to baseline methods?', 'How might ODITS be extended to handle heterogeneous teams with varying capabilities?']","['Evaluation limited to small teams and relatively simple environments', 'Scalability to larger, more complex scenarios not demonstrated', 'Dependence on diversity of training teammates not fully characterized', 'Potential challenges in real-world applications with highly unpredictable teammates', 'Potential negative societal impacts not thoroughly discussed']",False,3,3,3,7,4,"['Novel approach that learns latent representations instead of using predefined teammate types', 'Addresses the challenging problem of ad hoc teamwork under partial observability', 'Strong empirical results, consistently outperforming baselines', 'Comprehensive ablation studies validating key components', 'Information-theoretic foundations for handling partial observability']","['Evaluation limited to relatively small teams (2-8 agents)', 'Unclear scalability to larger teams and more complex domains', 'Reliance on pre-trained RL policies for teammates, may not capture full diversity of human-like behavior', 'Limited theoretical analysis or guarantees']",4,3,3.0,4,Accept
0DcZxeWfOPt,"This paper introduces MEND (Model Editor Networks with Gradient Decomposition), a novel method for efficiently editing large pre-trained language models using small auxiliary networks that transform fine-tuning gradients. MEND leverages the low-rank structure of gradients to enable editing of very large models (10B+ parameters). Comprehensive experiments demonstrate MEND's superiority over existing methods across several editing tasks, particularly for large-scale models.","['How well does MEND generalize to domains beyond language models?', 'What are the computational requirements for editing 100B+ parameter models?', 'How might MEND be extended to mitigate overgeneralization issues?', 'Are there ways to further reduce the parameter count of the editing networks?', 'Are there any theoretical guarantees or analysis for the MEND method?']","['Potential for misuse in introducing backdoors or vulnerabilities', 'Overgeneralization issues not fully resolved', 'Evaluation limited to language models, generalizability to other domains unclear', 'Limited analysis of potential negative societal impacts of model editing capabilities']",False,4,3,4,7,4,"['Novel and scalable approach to editing very large language models (10B+ parameters)', 'Strong empirical results across multiple tasks and model sizes', 'Efficient parameterization leveraging gradient structure', 'Comprehensive experiments and comparisons to baselines', 'Addresses an important problem in deploying large language models']","['Limited discussion of potential negative impacts and ethical concerns', 'Some failure cases like overgeneralization are noted but not fully addressed', 'Experiments focused only on language models, generalizability to other domains unclear', 'Addition of auxiliary networks increases complexity', 'Lack of theoretical analysis or guarantees for the method']",4,3,4.0,4,Accept
mF5tmqUfdsw,"This paper introduces Zeroth-Order Actor-Critic (ZOAC), a novel reinforcement learning algorithm that combines zeroth-order optimization for policy improvement with a critic network for value estimation. Key innovations include timestep-wise parameter perturbation, first-order policy evaluation, and zeroth-order policy improvement. The authors provide theoretical variance bounds and extensive empirical results on continuous control tasks, demonstrating improved sample efficiency, performance, and robustness compared to both zeroth-order and first-order baselines.","['How does the performance of ZOAC scale with increasing dimensionality of the policy space?', 'Can the method be extended to handle partially observable environments or multi-agent settings?', 'How does the choice of critic network architecture affect the performance of ZOAC?', 'What are the trade-offs between the rollout length N and the number of random directions sampled?']","['Increased computational complexity compared to pure zeroth-order or first-order methods', 'Potential difficulty in tuning hyperparameters for optimal performance', 'May not be suitable for problems with very high-dimensional observation or action spaces', 'Limited theoretical guarantees on convergence or optimality']",False,3,3,3,7,4,"['Novel combination of zeroth-order optimization and actor-critic methods', 'Strong empirical results outperforming baselines on multiple benchmarks', 'Theoretical analysis including variance bounds', 'Improved sample efficiency over pure evolutionary methods', 'Enhanced robustness of learned policies', 'Wide applicability to different policy parameterizations']","['Limited theoretical analysis beyond variance bounds', 'Experiments focused only on continuous control benchmarks', 'Lack of comparison to some state-of-the-art methods (e.g., SAC, TD3)', 'Potential scalability issues for very high-dimensional problems']",4,3,4.0,3,Accept
4l5iO9eoh3f,"This paper presents a novel supervised learning approach for solving the capacitated vehicle routing problem (CVRP) with a fixed fleet size. The method utilizes a permutation-invariant neural network to construct complete tour plans for a pre-specified number of vehicles. Key contributions include: (1) competitive performance compared to state-of-the-art reinforcement learning methods, especially when considering fixed vehicle costs, (2) significantly faster training times, and (3) the ability to handle bounded fleet sizes. The approach demonstrates clear benefits in terms of training efficiency and fleet utilization compared to existing baselines.","['How sensitive is the approach to the quality of the OR-Tools solutions used for training?', 'How well does the method generalize to problem instances with very different distributions than the training data?', 'Could the approach be extended to handle heterogeneous fleets or time windows?', 'How does the method perform on even larger problem instances (e.g., 200+ customers)?']","['The method currently only handles homogeneous fleets and no time windows', 'Performance on larger problem sizes (>100 customers) is not evaluated', 'Reliance on OR-Tools for training data generation may limit applicability in some scenarios', 'Computational constraints may limit applicability to larger problem instances']",False,3,3,3,7,4,"['Novel supervised learning approach for CVRP with fixed fleet size', 'Addresses the practical constraint of fixed number of vehicles often ignored by ML methods', 'Much faster and easier to train compared to RL approaches', 'Competitive performance, especially when considering fixed vehicle costs', 'Ability to solve problems with bounded fleet sizes', 'Thorough experimental evaluation and ablation studies']","['Performance on pure route length minimization is not always better than baselines, especially for larger problem sizes', 'Reliance on OR-Tools for generating training data and post-processing', 'Limited evaluation on more realistic problem instances', 'May not generalize as well to very different problem distributions']",3,3,3.0,3,Accept
dEOeQgQTyvt,"This paper introduces SEAL (Structured Energy As Loss), a framework that uses structured energy networks as trainable, dynamic loss functions for feedforward networks in multi-label classification tasks. The key innovation is leveraging expressive energy networks to capture rich label relationships while maintaining efficient inference through a feedforward network. The dynamic aspect allows the energy function to adapt during training, focusing on regions most relevant to the current state of the feedforward model. The paper demonstrates consistent improvements over baselines on multiple feature-based and text datasets, and proposes a new NCE ranking loss that performs best.","['Can you provide more theoretical justification for why the SEAL framework is effective?', 'How does the computational cost and scalability of SEAL compare to baselines, especially for larger datasets?', 'Have you explored using SEAL for other structured prediction tasks beyond multi-label classification?', 'How sensitive is the method to hyperparameter choices?', 'Are there potential applications of SEAL in other domains or tasks beyond multi-label classification?']","['Only evaluated on multi-label classification tasks', 'Increased computational cost in training', 'May not scale easily to extremely large label spaces', 'Requires careful tuning of multiple hyperparameters']",False,3,3,3,7,4,"['Novel approach combining strengths of energy networks and feedforward networks', 'Consistent empirical improvements across multiple datasets', 'Thorough ablation studies and analysis', 'Effective on both feature-based and large text datasets', 'Introduction of dynamic learning for the energy loss', 'New NCE ranking loss that performs well', 'Comprehensive experimental evaluation on various datasets and model configurations']","['Limited theoretical analysis and justification for the effectiveness of the approach', 'Experiments focused only on multi-label classification', 'Increased computational cost during training compared to simpler approaches', 'Some clarity issues in presentation and motivation', 'Comparison to some recent multi-label classification baselines is missing']",4,3,2.0,3,Accept
4Ycr8oeCoIh,"This paper analyzes GAN pretraining and finetuning, focusing on StyleGAN2. The authors show that pretraining mainly improves model coverage rather than sample fidelity, explain the roles of pretrained generators and discriminators, and provide a method for selecting pretrained checkpoints. Extensive experiments demonstrate the effectiveness of ImageNet pretraining for GANs.","['How might these findings be extended to other GAN architectures or generative models?', 'What theoretical frameworks could be developed to support the empirical observations?', 'How could this work be applied to improve GAN performance in specific domains or applications?', 'What are the computational trade-offs of using pretrained GANs versus training from scratch?']","['Experiments are limited to StyleGAN2 architecture', 'Conclusions may not generalize to all types of generative models', 'Potential negative impacts of improved GAN transfer learning are not thoroughly discussed', 'Limited exploration of the impact on downstream tasks beyond image generation']",False,4,3,4,7,4,"['Thorough empirical analysis using StyleGAN2 on multiple datasets', 'Novel insights into the mechanisms of GAN pretraining and transfer learning', 'Practical method proposed for selecting pretrained checkpoints', 'Demonstrates the effectiveness of ImageNet pretraining for GANs', 'Well-written with clear figures and tables']","['Limited to StyleGAN2 architecture, potentially affecting generalizability', 'Lack of theoretical analysis to complement empirical results', 'Some conclusions remain somewhat speculative', 'Limited discussion on potential negative impacts or ethical concerns']",3,4,3.0,4,Accept
O5Wr-xX0U2y,"This paper introduces a novel deep reinforcement learning algorithm for solving risk-averse dynamic decision making problems using dynamic expectile risk measures. The approach extends the deep deterministic policy gradient (DDPG) algorithm to handle time-consistent coherent risk measures. The method is applied to option pricing and hedging problems in finance, demonstrating improved performance over static risk measure approaches.","['How does the proposed method compare to other dynamic risk-aware RL approaches in terms of computational efficiency?', 'What are the specific challenges in applying this method to real market data?', 'How does the performance of the algorithm degrade as the dimensionality of the problem increases?']","['Experiments focused only on simulated financial data', 'Scalability to higher-dimensional problems not thoroughly explored', 'Limited discussion of potential negative impacts of automated trading strategies']",False,3,3,3,8,4,"['Novel extension of DDPG to handle dynamic expectile risk measures', 'Clear motivation and relevance to financial applications', 'Empirical results show improved performance over static approaches', 'Addresses important problem of time-consistency in risk-averse RL', 'Provides both theoretical analysis and experimental results']","['Empirical evaluation limited to simulated financial data', 'Limited comparison to other dynamic risk-aware RL methods', 'Scalability to higher-dimensional problems not thoroughly explored']",4,3,3.0,3,Accept
tFgdrQbbaa,"This paper presents a theoretical analysis of generalization in continual learning scenarios for neural networks using the neural tangent kernel (NTK) framework. The authors derive analytical expressions for generalization error in sequential training settings, prove several key results including transitions between positive and negative transfer depending on task similarity, and introduce concepts of self-knowledge transfer and forgetting. The theoretical results are validated empirically on both synthetic data and real neural network experiments.","['How well do the theoretical results generalize beyond the NTK regime to practical neural networks?', 'Can the analysis be extended to more complex continual learning scenarios with unequal sample sizes or many diverse tasks?', 'What are the implications of these results for designing better continual learning algorithms?']","['Results are limited to the NTK regime, which may not fully capture dynamics of state-of-the-art neural networks', 'Empirical validation could be more extensive, particularly on challenging continual learning benchmarks', 'May not fully address practical continual learning scenarios with catastrophic forgetting between very different tasks']",False,4,3,4,7,4,"['Rigorous theoretical framework for analyzing generalization in continual learning', 'Novel analytical expressions derived for generalization error in sequential training', 'Reveals interesting phenomena like transitions between positive/negative transfer and self-knowledge effects', 'Provides theoretical guarantees on generalization behavior', 'Validates theoretical results empirically on synthetic and real neural network data', 'Contributes to deeper understanding of deep learning dynamics']","['Analysis is limited to the NTK regime, which may not fully capture practical neural network dynamics', 'Empirical validation on real networks is somewhat limited in scope', 'Highly technical content may limit accessibility to a broader audience', 'Limited exploration of practical implications for continual learning algorithms', 'Lack of comparison to other theoretical frameworks for continual learning']",4,4,4.0,4,Accept
JYQYysrNT3M,"This paper introduces Online-ReOpt, a novel algorithm for reinforcement learning with ex-post max-min fairness across multiple reward types. The key contributions include highlighting differences between ex-ante and ex-post fairness, developing the Online-ReOpt algorithm with near-optimal regret bounds, creating offline variants to reduce online computation, and providing both theoretical guarantees and empirical evaluation on a queueing network problem.","['How well does the approach scale to problems with larger state/action spaces?', 'What is the computational complexity of Online-ReOpt compared to baselines?', 'How sensitive is the performance to the choice of optimization oracle?', 'Have you considered evaluating on domains other than queueing networks?', 'Could the approach be extended to continuous action spaces?']","['Assumes access to a near-optimal optimization oracle which may not be available in practice', 'Empirical evaluation limited to one specific queueing network problem', 'Scalability to very large MDPs is unclear', 'Offline variants still require significant pre-computation']",False,3,3,3,7,3,"['Novel approach to ex-post max-min fairness in reinforcement learning', 'Theoretically sound algorithm design with provable regret bounds', 'Development of offline variants to improve practical applicability', 'Clear differentiation between ex-ante and ex-post fairness objectives', 'Empirical results demonstrating benefits over baselines on queueing problem']","['Reliance on near-optimal optimization oracle may limit applicability', 'Limited empirical evaluation (only one domain - queueing network)', 'Potential scalability issues for larger state/action spaces', 'Some parts of the theoretical analysis are dense and may be hard to follow']",4,3,3.0,3,Accept
2s4sNT11IcH,"This paper presents a theoretical analysis of differentially private (DP) deep learning convergence using neural tangent kernel (NTK) matrices. It demonstrates that noise addition does not affect convergence in continuous time, but gradient clipping does. The authors propose a new 'global clipping' method that improves convergence while maintaining privacy guarantees. Theoretical analysis and empirical results show improved loss convergence and model calibration compared to standard 'local clipping' across multiple tasks.","['How does the analysis extend to discrete time updates used in practice?', 'Can you provide more rigorous validation of the calibration improvements?', 'How does global clipping compare to other recent advances in DP optimization beyond standard DP-SGD?', 'How sensitive are the results to the choice of Z in global clipping?', 'What is the computational overhead of global clipping compared to local clipping for very large models?']","['Analysis is limited to continuous time gradient flow', 'Empirical evaluation could be expanded to more diverse and larger-scale tasks/models', 'Practical challenges of implementing global clipping are not fully addressed', 'Privacy-utility tradeoffs are not extensively explored', 'Potential scalability issues of global clipping for very large models']",False,4,4,4,7,4,"['Novel theoretical analysis of DP deep learning convergence using NTK theory', 'Proposed global clipping method shows clear empirical benefits while maintaining privacy', 'Analysis framework applies to general architectures, losses, and optimizers', 'Strong empirical results demonstrating improved loss convergence and model calibration', 'Clear writing and presentation of ideas']","['Analysis is limited to continuous time, missing discrete-time effects', 'Empirical evaluation could be more extensive, especially on larger datasets and models', 'Comparison to other recent DP methods could be more comprehensive', 'Some claims about calibration improvements need more rigorous validation']",3,3,4.0,4,Accept
6-lLt2zxbZR,"This paper evaluates the zero-shot performance of language models, particularly ALBERT, on commonsense reasoning tasks using pseudo-log-likelihood scoring. The authors claim ALBERT achieves state-of-the-art results on several benchmarks and shows robust performance across datasets, attributing this to its architecture promoting compositionality.","['Can you provide statistical significance tests for the improvements over previous results?', ""How do you justify the claims about compositionality without more direct analysis of ALBERT's representations?"", 'Can you provide more comprehensive comparisons to state-of-the-art fine-tuned models across all benchmarks?', 'How sensitive are the results to different preprocessing of the benchmark datasets?', 'Can you elaborate on the ethical considerations and potential negative impacts of this work?']","['Small sample size for initial dataset limits statistical power', 'Lack of rigorous statistical analysis', 'Speculative nature of explanations for model performance', 'Sensitivity of method to small text changes', 'High computational cost may limit practical applicability', 'Performance may not generalize well beyond binary choice tasks', 'Potential ethical concerns regarding the use of large language models and their environmental impact not adequately addressed']",True,2,2,3,4,4,"['Achieves competitive zero-shot results on several commonsense reasoning benchmarks', 'Demonstrates robustness of ALBERT across different datasets and perturbations', 'Provides analysis comparing different model architectures', 'Highlights issues around data quality and preprocessing in benchmarks']","['Initial experiments conducted on a very small dataset (20 examples)', 'Lack of rigorous statistical analysis and significance tests', ""Speculative explanations for ALBERT's performance without sufficient evidence"", 'Brittleness of the method to small text changes not fully addressed', 'Limited comparison to state-of-the-art fine-tuned models', 'Insufficient details on experimental setup and reproducibility', 'Inadequate addressing of ethical considerations and broader impacts']",2,2,2.0,3,Reject
cD0O_Sc-wNy,"This paper proposes a novel approach to continual learning by learning replay schedules using Monte Carlo Tree Search (MCTS). The authors demonstrate that their method, RS-MCTS, outperforms standard replay approaches on several benchmark datasets, particularly with small memory sizes. The paper shows that learning when to replay old tasks can be more efficient than fixed replay strategies.","['How does the MCTS approach scale to larger, more complex continual learning scenarios?', 'What are the computational requirements compared to standard approaches?', 'In which scenarios does learned scheduling provide the most benefit over heuristic approaches?', 'What are the practical implications of the tiny memory experiments for real-world applications?']","['Scalability of MCTS approach to larger problems not thoroughly addressed', 'Experiments limited to small benchmark datasets', 'Computational requirements not thoroughly analyzed', 'Practical applicability to real-world scenarios not fully explored']",False,3,3,3,7,4,"['Innovative approach to learning replay schedules in continual learning', 'Demonstrates significant performance improvements over baseline methods on benchmarks', 'Shows promising results with very small memory sizes (as low as 1 sample per class)', 'Provides insightful visualizations of learned schedules that resemble spaced repetition']","['Potential scalability issues with MCTS approach for larger problems', 'Experiments limited to relatively small benchmark datasets', 'Similar performance to heuristic scheduling in some cases', 'Limited discussion of practical implications and computational requirements']",4,3,3.0,3,Accept
dtt435G80Ng,"This paper introduces Centered Symmetric Quantization (CSQ), a novel quantization scheme for extremely low-bit neural networks. CSQ uses perfectly symmetrical quantization levels with uniform step size, improving representational capacity over conventional linear quantization (CLQ) at 2-3 bits. Theoretical analysis and experiments on ImageNet and CIFAR-10 demonstrate performance gains at these low bit-widths across various network architectures. The authors also propose an efficient hardware implementation method.","['How does CSQ compare to more recent quantization methods beyond LSQ?', 'Have you explored applying CSQ to activation quantization as well?', 'Could the principles of CSQ be extended to higher bit-widths in any way?', 'What is the hardware overhead of CSQ compared to CLQ implementations?', 'What is the potential impact of CSQ on energy efficiency or computational requirements in practical applications?']","['Benefits are limited to very low precisions (2-3 bits)', 'Not compared against some state-of-the-art quantization methods', 'Only weight quantization is explored, not activation quantization', 'Hardware analysis could be more comprehensive']",False,4,3,3,7,4,"['Novel quantization scheme (CSQ) that improves performance for 2-3 bit networks', 'Thorough theoretical analysis of representational capacity and quantization error', 'Strong experimental results on ImageNet and CIFAR-10 benchmarks', 'Proposed efficient hardware implementation method', 'Consistent gains over CLQ on standard benchmarks at 2-3 bits']","['Performance gains primarily limited to 2-3 bit precision', 'Limited comparison to other recent low-bit quantization methods', 'Only applied to weight quantization, not activations', 'Modest performance gains, though consistent across settings']",3,4,4.0,4,Accept
LtI14EpWKH,"This paper introduces tessellated 2D convolutional networks (TCNNs) as a defense against adversarial attacks for image classification. The approach partitions input images into non-overlapping tiles, processes each tile independently, and combines the results. Various tessellation strategies including regular grids, aperiodic patterns, and Mondrian-style tilings are explored. Experiments on FMNIST and CIFAR-10 datasets demonstrate some improvement in robustness against FGSM and PGD attacks compared to standard CNNs.","['How would this approach scale to larger, more complex models and datasets like ImageNet?', 'Can you provide a stronger theoretical justification for why tessellation improves robustness?', 'How does the method compare to state-of-the-art adversarial defenses on standard benchmarks?', 'What is the computational overhead of the tessellation approach compared to standard CNNs?', 'Have you explored combining this approach with other defense techniques like adversarial training?', 'How do different tessellation patterns affect the interpretability of the model?']","['Evaluation limited to simple architectures and datasets', 'Potential increased computational complexity not thoroughly analyzed', 'Trade-off between robustness and clean accuracy not fully explored', 'Effectiveness on more diverse and complex datasets unknown', ""Lack of theoretical guarantees for the method's robustness""]",False,3,3,2,5,4,"['Novel architectural approach to adversarial defense', 'Systematic exploration of different tessellation strategies', 'Shows some improvement in robustness over standard CNNs', 'Relatively simple modification to existing CNN architectures']","['Limited theoretical justification for the approach', 'Experiments restricted to simple CNN architectures and smaller datasets', 'Modest gains in robustness compared to some existing defenses', 'Lack of comparison to state-of-the-art adversarial defenses', 'Insufficient analysis of trade-offs between robustness and clean accuracy']",4,2,3.0,3,Reject
xspalMXAB0M,"This paper introduces a novel boosting approach for reinforcement learning in large state spaces, addressing the challenge of sample complexity independent of state space size. It proposes a method to aggregate weak learners into a stronger policy using a non-convex Frank-Wolfe method, providing theoretical guarantees for both episodic and continuous settings.","[""Can you provide empirical results demonstrating the method's performance on standard RL benchmarks?"", 'How realistic are the weak learning assumptions in practical RL settings?', 'Can you provide more intuition for the non-convex Frank-Wolfe method and why it works well here?', 'Are there ways to improve the sample complexity bounds or simplify the aggregation scheme?']","['Theoretical focus without empirical validation limits immediate practical impact', 'Assumptions on weak learners may restrict applicability in complex domains', 'Computational complexity of the algorithm is not thoroughly analyzed', ""The method's scalability to very high-dimensional state/action spaces is unclear""]",False,3,2,3,5,4,"['Novel combination of boosting and reinforcement learning for large state spaces', 'Rigorous theoretical analysis with sample complexity bounds independent of state space size', 'Handles both episodic and continuous reinforcement learning settings', 'Innovative non-linear policy aggregation scheme']","['Lacks experimental results to validate the approach empirically', 'Weak learning assumptions may be too strong for practical applications', 'Sample complexity bounds, while polynomial, may be too high for practical use', 'Presentation is highly technical, potentially limiting accessibility']",3,3,2.0,3,Accept
p7LSrQ3AADp,"This paper introduces a 9-dimensional framework for characterizing and comparing saliency methods used in interpreting ML models. The framework views saliency methods as abstractions rather than aiming for perfect faithfulness, and categorizes dimensions into methodology, sensitivity, and perceptibility. The authors demonstrate practical applications including creating saliency cards, guiding method selection for specific use cases, and identifying research opportunities. The framework is validated through a case study with radiologists.","['How can the framework dimensions be quantified to allow for more rigorous comparisons?', 'What additional empirical studies could be conducted to further validate the utility and generalizability of the framework?', 'How were the 9 dimensions chosen, and are there other important aspects that might have been overlooked?', 'How well does the framework generalize to saliency methods beyond those analyzed in the paper?', 'How might the framework evolve over time as new saliency methods are developed?']","['Framework may not capture all relevant aspects of saliency methods', 'Qualitative nature of dimensions makes precise comparisons challenging', 'Limited evaluation on real-world applications beyond the radiology case study', 'Potential for subjectivity in how methods are characterized along dimensions']",False,3,3,3,7,4,"['Novel framing of saliency methods as abstractions rather than faithful representations', 'Comprehensive 9-dimensional framework providing a nuanced way to compare methods', 'Demonstrates practical applications through saliency cards and radiologist case study', 'Identifies promising directions for future research in method development and evaluation']","['Limited quantitative evaluation of methods using the framework', 'Framework dimensions are somewhat qualitative and may be difficult to measure precisely', 'Evaluation is limited in scope, primarily based on interviews with 4 radiologists', 'Generalizability beyond examined methods and use cases is not fully established']",4,3,3.0,4,Accept
VFBjuF8HEp,"This paper introduces Differentiable Diffusion Sampler Search (DDSS), a method for optimizing fast samplers for pre-trained diffusion models. Key contributions include: (1) framing sampler design as a differentiable optimization problem, (2) proposing Generalized Gaussian Diffusion Models (GGDM), and (3) optimizing sampler parameters using a perceptual quality metric. DDSS achieves state-of-the-art sample quality for few-step sampling on CIFAR-10 and ImageNet 64x64.","['How does the computational cost of DDSS scale with model size and dataset complexity?', 'Could DDSS be adapted for conditional generation tasks, and what modifications would be needed?', 'How sensitive is the method to the choice of perceptual metric, and could alternative metrics be used?', 'What strategies could be employed to reduce the computational cost of the optimization procedure?']","['High computational cost may limit practical applicability', 'Evaluation limited to unconditional image generation on simple datasets', 'Potential negative impacts of faster synthetic media generation not thoroughly discussed', 'Reliance on pre-trained classifier for perceptual loss may limit generalizability', 'Lack of theoretical guarantees on convergence or optimality of the found samplers', 'Potential sensitivity to the choice of initial sampler parameters']",False,3,3,4,8,4,"['Novel approach to optimizing diffusion model samplers', 'Introduction of GGDM, a flexible family of samplers generalizing prior work', 'Significant empirical improvements over strong baselines for few sampling steps', 'Thorough ablation studies validating different components', 'Compatible with any pre-trained diffusion model without fine-tuning', 'Optimizes for perceptual quality rather than likelihood']","['Computationally expensive optimization procedure', 'Limited evaluation on higher resolution datasets or complex data', 'Potential overfitting concerns not fully addressed', 'Theoretical justification and guarantees could be stronger', 'Applicability to non-image data or conditional generation not explored']",4,3,3.0,4,Accept
jE_ipyh20rb,"This paper introduces FEDPROF, a novel federated learning algorithm that uses data representation profiling to selectively involve clients based on their data quality. The approach generates compact profiles of client data representations, compares them to a baseline, and adjusts client participation probabilities accordingly. Theoretical analysis provides convergence guarantees, and experiments on two tasks demonstrate significant improvements in convergence speed, accuracy, and energy efficiency compared to baselines.","['How does the approach perform on a wider range of tasks and more complex datasets?', 'What is the computational and communication overhead of profile generation/transmission?', 'Are there potential negative impacts on fairness from excluding some clients?', 'How sensitive is the method to the choice of hyperparameters like alpha?', 'How does FEDPROF compare to other recent adaptive client selection methods in federated learning?', 'How do the theoretical assumptions in the analysis impact the practical applicability of the method?']","['Experiments are limited to only two relatively simple datasets and tasks', 'Potential fairness and bias issues from preferential client selection are not thoroughly explored', 'Computational overhead of profiling is not clearly quantified', 'Privacy implications of sharing representation profiles could be analyzed further', 'Scalability to very large federated learning systems is not demonstrated']",False,3,3,3,7,4,"['Novel approach using representation profiling for client selection in federated learning', 'Theoretical analysis with convergence guarantees', 'Significant empirical improvements in convergence speed, accuracy, and energy efficiency', 'Privacy-preserving method for assessing client data quality', 'Addresses an important problem of heterogeneous client data quality in federated learning']","['Experiments limited to only two relatively simple datasets/tasks', 'Theoretical analysis relies on some strong assumptions that may not hold in practice', 'Potential fairness issues with preferential client selection not thoroughly addressed', 'Computational overhead of profiling not clearly quantified', 'Limited comparison to some recent state-of-the-art FL methods']",4,3,3.0,4,Accept
YLglAn-USkf,"This paper presents an empirical study on the zero-shot capabilities of BERT-family language models using prompting techniques. The authors propose simple strategies, particularly Multi-Null Prompting, which achieve strong performance on text classification tasks without labeled data or manual prompt engineering. The paper includes extensive experiments, ablation studies, and analysis of different components of the proposed methods. While showing promising results on text classification, the methods struggle with more complex NLU tasks.","['Can you provide more theoretical insight or justification for why Multi-Null Prompting is effective?', 'How do you think the proposed methods would scale to larger language models like GPT-3?', 'Have you tried applying the proposed techniques to a wider range of NLP tasks beyond text classification?', 'How do the proposed methods compare to few-shot learning approaches in terms of performance vs. data efficiency?', 'What do you see as the most promising future directions or applications for this work?']","['The study is primarily focused on text classification tasks, with limited exploration of other NLP domains', 'Proposed methods show limited effectiveness on more complex NLU tasks', 'Lack of theoretical analysis restricts understanding of the underlying mechanisms', 'No exploration of potential negative impacts or broader ethical considerations of the technology']",False,3,3,3,6,4,"['Comprehensive empirical study with extensive experiments and ablations', 'Proposes simple yet effective strategies for zero-shot learning with BERT models', 'Achieves strong results on text classification tasks without labeled data', 'Provides insights into the capabilities and limitations of zero-shot prompting with BERT', 'Honest discussion of limitations on more complex NLU tasks', ""Thorough exploration of the method's limitations, which is valuable for future research""]","['Limited theoretical analysis or explanation for why the proposed methods work', 'Experiments are mostly focused on text classification, with limited results on more complex NLU tasks', 'Limited novelty in the overall approach of zero-shot prompting', 'Writing and organization could be improved in some sections', 'Lack of comparison to very large language models like GPT-3']",2,3,3.0,3,Accept
mFpP0THYeaX,"This paper introduces GIFT, a novel method for unsupervised domain adaptation that creates virtual samples from intermediate distributions by interpolating representations between source and target domains. This approach enables gradual adaptation even when actual intermediate samples are unavailable. The method is evaluated on both synthetic and real distribution shifts, demonstrating improvements over iterative self-training baselines.","['Can you provide more theoretical analysis or guarantees for the GIFT method?', 'How does GIFT compare to other state-of-the-art domain adaptation methods beyond self-training baselines?', 'Have you explored applying GIFT to tasks beyond image classification?', 'How sensitive is GIFT to the choice of interpolation scheme and layers used for interpolation?', 'What is the computational overhead of GIFT compared to iterative self-training?', 'What are potential applications of GIFT in other domains or tasks?']","['Experiments are limited mainly to image classification tasks', 'Lack of theoretical guarantees', 'May not work well if source and target domains are very dissimilar', 'Assumes access to unlabeled target domain data', 'Potential sensitivity to hyperparameters like interpolation scheme', 'Difficulty in selecting appropriate intermediate layers for interpolation']",False,3,3,3,7,4,"['Novel approach to creating virtual intermediate samples via representation interpolation', 'Thorough empirical analysis on synthetic and real distribution shifts', 'Demonstrates how GIFT enables an implicit curriculum for adaptation', 'Consistent improvements over baselines across multiple datasets', 'Good theoretical motivation and connection to curriculum learning']","['Limited theoretical analysis or guarantees', 'Experiments focused mainly on image classification tasks', 'Comparisons mostly to self-training baselines rather than other state-of-the-art domain adaptation methods', 'Empirical gains are relatively modest in some cases', 'Potential computational overhead of the interpolation scheme']",4,3,3.0,4,Accept
q4HaTeMO--y,"This paper establishes a theoretical equivalence between deep equilibrium models (DEQs) and deep declarative networks (DDNs) with kernelized generalized linear models (kGLMs) as inner optimization problems. The authors derive closed-form expressions for DEQ parameters in terms of the kernel and use this to devise an initialization scheme for DEQs. Empirical results demonstrate that this initialization improves training stability and performance compared to random initialization across several tasks, particularly in image denoising.","['How well does the proposed initialization scheme scale to very large models and more complex tasks?', 'Are there ways to relax some of the theoretical assumptions while maintaining similar guarantees?', 'How does the computational cost of the initialization compare to training time for larger models/datasets?', 'Could the theoretical framework be extended to other types of implicit layers beyond DEQs?', 'What potential applications do you envision for this approach in domains other than image processing?']","['The empirical evaluation is primarily focused on image denoising tasks', 'Some theoretical assumptions may limit practical applicability in certain scenarios', 'The computational cost of the initialization for very large models is not fully addressed', 'Limited discussion of broader implications and potential negative impacts']",False,4,3,4,8,4,"['Novel and significant theoretical connections between DEQs, DDNs, and kGLMs', 'Rigorous mathematical analysis with clear assumptions and proofs', 'Derivation of a principled initialization scheme for DEQs based on theoretical results', 'Consistent empirical evidence showing improved training stability and performance', 'Provides new insights into the relationship between classical statistical models and modern deep learning architectures']","['Empirical evaluation somewhat limited in scope, focusing mainly on image denoising tasks', 'Some theoretical assumptions may be restrictive for certain practical applications', 'Limited discussion on scalability and computational costs for larger models', 'Broader implications and potential negative impacts not extensively discussed']",4,4,4.0,4,Accept
FPCMqjI0jXN,"This paper introduces Domino, a novel slice discovery method (SDM) that leverages cross-modal embeddings to identify systematic errors in machine learning models. The authors also propose a comprehensive evaluation framework for SDMs. Key contributions include: (1) A novel SDM using cross-modal embeddings and an error-aware mixture model, (2) A comprehensive evaluation framework for SDMs, and (3) The ability to generate natural language descriptions of discovered slices. Extensive experiments across multiple domains demonstrate Domino's improved performance over existing methods.","[""Can you provide a detailed analysis of Domino's computational complexity and runtime compared to existing SDMs?"", ""How does the quality of cross-modal embeddings affect Domino's performance, and are there minimum requirements for embedding quality?"", 'Have you explored any techniques to adapt Domino for domains where paired input-text data is scarce or unavailable?', ""How does Domino's performance scale to multi-class problems with a large number of classes?""]","[""The method's reliance on cross-modal embeddings may limit its applicability in some domains"", 'The complexity of the method may make it challenging to implement and tune in practice', 'The evaluation, while extensive, is limited to specific domains and may not generalize to all types of ML tasks', 'Performance depends on the quality of available cross-modal embeddings']",False,4,3,4,8,4,"['Introduces a comprehensive evaluation framework for SDMs, addressing a gap in the field', 'Proposes a novel SDM (Domino) that consistently outperforms existing methods', 'Ability to generate natural language descriptions of discovered slices', 'Thorough empirical evaluation across multiple domains and slice types', 'Clear improvement over previous methods in slice discovery performance', 'Comprehensive literature review and clear positioning within the field']","['Complexity of the method may limit practical adoption', 'Reliance on cross-modal embeddings or paired data may limit applicability in some domains', 'Performance improvements, while consistent, are not extremely large in magnitude', 'Limited discussion of computational costs and scalability']",4,4,4.0,4,Accept
5hLP5JY9S2d,"This paper analyzes open-set recognition (OSR) methods, demonstrating a strong correlation between closed-set and open-set performance. It shows that improving closed-set accuracy of a simple baseline using maximum logit scoring can match or outperform state-of-the-art OSR methods. The paper also proposes new 'Semantic Shift Benchmark' datasets that better isolate semantic novelty for OSR evaluation.","['Can you provide more theoretical analysis of why closed-set and open-set performance are correlated?', 'What are the implications of your findings for future OSR research directions?', 'How well do your findings generalize to other domains beyond image classification?', 'What specific challenges might arise when applying the Semantic Shift Benchmark to real-world OSR problems?']","['The study is primarily empirical, with limited theoretical analysis', 'The proposed benchmarks, while an improvement, may still have unexplored limitations', 'The paper does not extensively discuss potential negative societal impacts of the work', 'Findings are focused on image classification and may not directly generalize to other domains']",False,4,4,4,7,4,"['Extensive empirical analysis across multiple datasets, architectures, and methods', 'Important finding of correlation between closed-set and open-set performance', 'Proposal of new benchmarks that address limitations of existing OSR evaluations', 'Improvement of simple baselines to match or outperform state-of-the-art methods', 'Clear writing and well-structured presentation of results', 'Practical implications for simplifying and improving OSR research']","['Limited theoretical justification for the observed correlation', 'Focus on empirical results rather than proposing fundamentally new OSR methods', 'Potential limitations of the new benchmarks not fully explored', 'Limited discussion of broader implications and potential negative impacts']",3,4,4.0,4,Accept
JHXjK94yH-y,"This paper introduces Adversarial Surprise (AS), a novel unsupervised reinforcement learning method based on an adversarial game between two policies: an Explore policy maximizing surprise/entropy and a Control policy minimizing it. The authors provide theoretical analysis showing AS can achieve good state coverage in stochastic block MDPs, and present empirical results demonstrating strong performance compared to baselines on environments including Atari, VizDoom, and MiniGrid.","['How does the computational cost of AS compare to the baseline methods?', 'How sensitive is the method to hyperparameter choices, particularly the switching interval between policies?', 'How well would AS scale to more complex, high-dimensional continuous control tasks or real-world environments?', 'What are the key limitations or failure modes of AS?']","['Theoretical analysis makes strong assumptions that may not always hold in practice', 'Added complexity compared to simpler exploration methods may make it harder to implement and debug', 'Limited evaluation in real-world settings or very high-dimensional environments', 'Potential for learning unintended or unsafe behaviors not fully addressed', 'Possible increased computational cost due to maintaining two competing policies']",False,3,3,4,8,4,"['Highly original algorithm design addressing limitations of prior exploration methods', 'Strong theoretical analysis providing formal guarantees on state coverage', 'Comprehensive empirical evaluation demonstrating superior performance across multiple environments and baselines', 'Demonstration of emergent complex behaviors through the adversarial game', 'Effective combination of theoretical and empirical contributions']","['Complex algorithm that may be challenging to implement and tune', 'Theoretical analysis relies on assumptions that may not always hold in practice', 'Limited discussion of computational costs and scalability to more complex, real-world environments', 'Insufficient exploration of potential limitations and failure modes']",4,3,3.0,4,Accept
js62_xuLDDv,"This paper introduces finDML, a benchmark for fairness in deep metric learning (DML), shows bias propagation from DML embeddings to downstream tasks, and proposes PARADE to improve fairness by de-correlating embeddings from sensitive attributes. Extensive experiments are conducted across multiple datasets and DML techniques.","['How can finDML be extended to multi-class or continuous protected attributes?', 'Can PARADE be adapted to work without explicit knowledge of sensitive attributes?', 'How does PARADE compare to other fairness methods in DML or representation learning?', 'What are the long-term implications of using PARADE in real-world applications?']","['Focus on binary protected attributes and group fairness limits generalizability', 'Requiring sensitive attributes for PARADE may not be feasible in some applications', 'Potential negative impact of reducing overall model performance when optimizing for fairness', 'Limited exploration of potential negative impacts and ethical concerns', 'Computational complexity of PARADE not thoroughly discussed']",False,3,3,4,7,4,"['Novel fairness definitions and benchmark (finDML) for DML', 'Demonstrates important finding that bias propagates from embeddings to downstream tasks', 'Proposes new method (PARADE) to improve fairness', 'Extensive experiments on multiple datasets', 'Opens up new directions for fairness in representation learning']","['Only considers binary protected attributes and group fairness', 'PARADE requires knowledge of sensitive attributes during training', 'Limited exploration of fairness-performance tradeoffs', 'Improvements from PARADE are sometimes modest', 'Insufficient discussion of potential negative societal impacts']",4,3,3.0,4,Accept
rS9t6WH34p,"This paper introduces ObSuRF, a novel method for unsupervised 3D scene decomposition into objects represented as Neural Radiance Fields (NeRFs). The approach uses a slot-based encoder to infer object-centric latent representations from a single image, which are then used to condition NeRF decoders. A new training objective allows for efficient learning from RGB-D data without expensive ray marching. The method achieves state-of-the-art results on 2D image segmentation benchmarks and demonstrates impressive 3D scene decomposition on two new datasets.","['How well does the method generalize to real-world scenes with more complex geometry, lighting, and noisy depth measurements?', 'Have you explored using self-supervised depth estimation techniques to reduce reliance on RGB-D data?', 'How does the computational efficiency compare to other recent 3D scene decomposition methods, both in training and inference?', 'Could the approach be extended to handle dynamic scenes with moving objects?', 'What potential applications do you envision for this technology beyond computer vision tasks?']","['Current evaluation is limited to synthetic datasets, though this is appropriate for the scope of the paper', 'Performance on real-world scenes with complex lighting and materials is not evaluated', 'The method may struggle with highly cluttered scenes or objects significantly different from those in training', 'Requires depth information during training, which may not always be available in real-world scenarios']",False,4,4,4,8,4,"['Novel combination of object-centric representations with NeRFs for 3D scene decomposition', 'Efficient training method using RGB-D data that avoids expensive ray marching', 'State-of-the-art results on 2D object segmentation benchmarks', 'Impressive qualitative results on challenging 3D object segmentation tasks', 'Introduction of new 3D segmentation benchmarks', 'Thorough ablation studies and comparisons to baselines']","['Reliance on depth information for training may limit real-world applicability', 'Limited evaluation on real-world data with complex geometry and lighting', 'May struggle with highly cluttered scenes or objects significantly different from those in training', 'Computational requirements, while improved, may still be high for some applications']",4,4,4.0,4,Accept
VqzXzA9hjaX,This paper introduces 'optimizer amalgamation' to combine multiple optimization algorithms into a single stronger optimizer. The authors propose three amalgamation schemes and stability improvement techniques. Experiments show the amalgamated optimizer outperforms baselines across various tasks.,"['How well does the approach scale to very large models and datasets?', 'Can you provide any theoretical insights for the amalgamation process?', 'How does the computational cost of amalgamation compare to training individual optimizers?']","['Scalability to much larger models/datasets not fully demonstrated', 'Lacks theoretical analysis or guarantees', 'Computational cost of amalgamation process could be high', 'May not generalize well to drastically different optimization problems']",False,3,3,3,7,4,"['Novel and well-motivated problem formulation of optimizer amalgamation', 'Comprehensive experimental evaluation across multiple datasets and architectures', 'Consistent improvements shown over strong baselines', 'Exploration of different amalgamation schemes and stability techniques', 'Clear writing and presentation of ideas', 'Open source code release for reproducibility']","['Limited theoretical analysis or guarantees for the proposed methods', 'Scalability to very large models and datasets not fully demonstrated', 'Computational cost of the amalgamation process not thoroughly analyzed', 'Performance gains may diminish on larger models', 'Focused mainly on vision tasks, generalizability to other domains not explored']",3,3,3.0,3,Accept
vdbidlOkeF0,"This paper introduces SDRE (Scaled Density Ratio Estimator), a novel method for estimating density ratios between two distributions. SDRE uses a scaling distribution to rewrite the density ratio and estimates it using a multi-class classifier. The method is shown to outperform existing approaches, especially when distributions have both first-order and higher-order discrepancies. The paper provides strong theoretical justification based on scaled Bregman divergence and demonstrates the method's effectiveness through extensive empirical evaluation on various density ratio estimation tasks.","['Can you provide any theoretical bounds on the estimation accuracy of SDRE?', 'Is it possible to learn the scaling distribution m instead of manually specifying it?', 'How does the computational complexity of SDRE compare to baseline methods, especially for high-dimensional problems?', 'How well does SDRE perform on very high-dimensional (e.g. 1000+) or complex real-world datasets?']","['The method requires manually specifying the scaling distribution', 'No theoretical bounds are provided on estimation accuracy', 'Evaluation is limited to a specific set of tasks and datasets', 'Potential increased computational cost from using multiple scaling distributions']",False,4,4,4,8,4,"['Novel approach addressing limitations of existing density ratio estimators', 'Strong theoretical grounding using scaled Bregman divergence', 'Comprehensive empirical evaluation on synthetic and real datasets', 'Outperforms state-of-the-art baselines on challenging problems', 'Applicable to both model-based and model-free density ratio estimation', 'Clear exposition and thorough experimental design', 'Potential to improve representation learning methods']","['Lack of theoretical bounds on estimation accuracy', 'Scaling distribution is manually specified rather than learned', 'Limited evaluation on very high-dimensional or complex real-world datasets', 'Potential computational overhead from using multiple scaling distributions']",4,4,4.0,4,Accept
7AzOUBeajwl,This paper introduces a novel method for text style transfer in the presence of confounding factors. The approach uses invariant risk minimization to learn two classifiers: one for style and another for orthogonal characteristics. These classifiers are then used to guide a style transfer model. Experiments on sentiment transfer tasks with synthetic (punctuation) and real (product category) confounders demonstrate improvements over baselines.,"['How well would the approach generalize to other style transfer tasks beyond sentiment?', 'What are the main failure modes and how could they be addressed?', 'How computationally expensive is the proposed method compared to baselines?', 'How sensitive is the method to hyperparameter choices?', 'What are some potential real-world applications of this technique?']","['Primarily demonstrated on sentiment transfer tasks', 'May not generalize well to more subtle style differences or other types of confounders', 'Potential for misuse in generating fake reviews or misinformation not thoroughly addressed']",False,3,3,3,7,4,"['Novel problem formulation addressing style transfer with confounders', 'Clever use of invariant risk minimization for disentanglement', 'Solid empirical validation showing improvements over baselines', 'Clear writing and presentation', 'Addresses a practical challenge in real-world style transfer applications']","['Evaluation limited primarily to sentiment transfer tasks', 'Synthetic task with punctuation feels somewhat artificial', 'Limited scope of human evaluation', 'Insufficient analysis of failure cases and limitations', 'Brief treatment of ethical considerations and potential misuse']",4,3,3.0,3,Accept
nsjkNB2oKsQ,"This paper introduces Past-Invariant Delayed Reward MDPs (PI-DRMDPs) for off-policy reinforcement learning with delayed rewards. Key contributions include a novel Q-function formulation with theoretical guarantees, an HC-decomposition approximation scheme for improved training, and empirical results demonstrating superior performance on delayed reward tasks.","['How might the proposed method be extended to handle non-PI-DRMDPs?', 'What are the computational trade-offs of the HC-decomposition compared to standard Q-learning approaches?', 'How could this approach be applied to real-world delayed reward problems beyond simulated environments?']","['The approach is mainly focused on PI-DRMDPs and may not generalize well to all types of delayed reward problems', 'Lack of optimality guarantees for the full algorithm with HC-decomposition', 'Empirical evaluation is somewhat limited in scope', 'Potential scalability issues for very long delay periods or high-dimensional state spaces']",False,3,3,4,7,4,"['Novel theoretical framework (PI-DRMDPs) for delayed reward problems', 'New Q-function formulation with convergence guarantees', 'Practical HC-decomposition scheme that improves training efficiency', 'Strong empirical results showing improvements over baselines', 'Thorough theoretical analysis and proofs']","['Focus is mainly on PI-DRMDPs rather than general DRMDPs', 'Optimality guarantees only for a subclass of policies', 'Empirical evaluation limited to relatively simple control tasks', 'Limited discussion of potential negative societal impacts']",4,3,3.0,4,Accept
2hMEdc35xZ6,"This paper introduces DT-GAN, a novel GAN architecture designed for synthesizing diverse defect images to augment limited datasets in industrial inspection tasks. Key innovations include disentangling foreground defects from backgrounds, separating defect style and content, and enabling defect transfer across product types. Experiments on a real industrial dataset demonstrate significant improvements over baseline approaches in both image quality and downstream classification performance, with potential for practical impact in industrial settings.","['How well does the method generalize to other types of industrial inspection tasks or defects not seen during training?', 'Can you provide more theoretical justification for the key architectural choices?', 'How does the computational cost compare to baseline methods, both in training and inference?', 'Have you considered extending the approach to generate localized defects with precise spatial control?', 'Is this method suitable for real-time application in industrial inspection settings?']","['May not generalize well to defects or products very different from training data', 'Limited evaluation on diverse public datasets', 'Potential domain gap between synthetic and real images', 'Computational cost of training separate models for each product type']",False,3,3,3,7,4,"['Addresses an important real-world problem of data scarcity in industrial defect detection', 'Novel technical approach combining several ideas (FG/BG disentanglement, style-content separation, cross-domain transfer)', 'Clear improvements demonstrated over baseline GAN approaches, especially with limited training data', 'Extensive experiments and ablation studies on real industrial data', 'Practical applicability to industrial inspection tasks']","['Limited evaluation on public datasets, with less impressive results on MVTec', 'Potential issues with generalization to different types of defects or products', 'Innovations are mostly combinations of existing ideas rather than fundamentally new techniques', 'Limited comparison to other defect-specific synthesis methods', 'Computational cost of the more complex architecture not fully addressed']",4,3,3.0,4,Accept
uHv20yi8saL,"This paper presents a novel theoretical analysis of trust region methods for multi-agent reinforcement learning, focusing on IPPO and MAPPO algorithms. Key contributions include a monotonic improvement guarantee for decentralized policies under non-stationarity and justification for independent ratio clipping.","['How sensitive are the results to the assumption that advantage functions converge to a fixed point?', 'How well do the theoretical results and insights generalize to other types of MARL environments beyond SMAC?', 'Could the analysis be extended to partially cooperative or competitive settings?', 'What are the practical implications of this theory for designing new MARL algorithms?']","['Analysis is limited to fully cooperative settings and may not generalize to other MARL scenarios', 'Empirical evaluation is limited primarily to SMAC benchmark', 'Assumes convergence of advantage function which may not always hold in practice', 'Does not extensively compare to other MARL approaches beyond IPPO/MAPPO']",False,4,3,4,6,4,"['Novel theoretical analysis providing insight into successful MARL algorithms', 'Monotonic improvement guarantee for decentralized policies under non-stationarity', 'Theoretical justification for independent ratio clipping', 'Practical guidance on setting clipping ranges based on number of agents', 'Empirical results supporting key theoretical claims']","['Empirical evaluation limited primarily to SMAC environments', 'Assumes advantage functions converge to fixed point, which may not always hold', 'Limited comparison to other MARL algorithms beyond IPPO/MAPPO', 'Some parts of the theoretical analysis could be explained more clearly']",4,3,3.0,3,Accept
5FUq05QRc5b,"This paper provides a theoretical analysis of latent correlation-based multiview learning methods from an identifiability perspective. The key contributions are: 1) Proving that latent correlation maximization can extract shared components across views under a nonlinear mixture model, 2) Showing that with proper regularization, private components can be disentangled, 3) Providing finite sample analysis, which is rare in this area, and 4) Proposing a practical implementation with a novel minimax neural regularizer. Experiments on synthetic and real data validate the theoretical claims.","['How do the theoretical guarantees hold up under different levels of model misspecification?', 'Can the authors provide more insights on the practical advantages of their method compared to existing approaches?', 'How does the proposed minimax neural regularizer compare to other independence-promoting techniques in terms of effectiveness and computational efficiency?']","The authors discuss some limitations, such as assumptions in the generative model and practical implementation challenges. However, the empirical evaluation could be strengthened by testing on larger, more diverse datasets and providing more extensive comparisons with state-of-the-art methods. Additionally, a more detailed analysis of the method's sensitivity to hyperparameter choices would be beneficial. The authors could also further discuss potential negative impacts of disentangling private information in real-world applications.",False,4,4,4,8,4,"['Novel theoretical guarantees for shared component extraction in multiview learning', 'Rigorous proofs and finite sample analysis, which is rare for nonlinear mixture identifiability', 'Practical implementation with a new minimax neural regularizer', 'Experimental results validating theoretical claims on synthetic and real data', 'Provides valuable insights into widely-used representation learning methods']","['Practical impact and improvements over existing methods not clearly demonstrated', 'Assumptions in theoretical analysis may be strong for some real-world scenarios', 'Empirical evaluation could be strengthened with more extensive comparisons on larger datasets']",4,4,4.0,4,Accept
Py8WbvKH_wv,"This paper introduces DRIBO, a novel approach for learning robust representations in reinforcement learning using a multi-view information bottleneck objective. The method leverages data augmentation to create multiple views and learns predictive representations over time. Empirical results demonstrate that DRIBO outperforms baselines on visual control tasks with distractions and generalization benchmarks, particularly on the DeepMind Control Suite with distracting backgrounds.","['How can the computational requirements be reduced?', 'Are there ways to improve performance on the Procgen environments where gains were limited?', 'How sensitive is the method to hyperparameter choices, particularly the beta scheduler?', 'What are potential real-world applications where DRIBO could be particularly beneficial?']","['High computational requirements may limit accessibility', 'Added complexity may make practical implementation challenging', 'May not generalize equally well to all types of RL tasks/environments']",False,3,4,4,7,4,"['Novel technical approach combining multi-view information bottleneck with RL', 'Strong empirical results on challenging visual control and generalization benchmarks', 'Comprehensive experiments and ablation studies', 'Addresses an important problem of learning robust representations for RL', 'Clear writing and presentation', 'Provides theoretical analysis and proofs for the proposed method']","['Introduces additional complexity and hyperparameters compared to standard RL algorithms', 'Performance not uniformly better on all Procgen tasks', 'High computational requirements', 'Theoretical analysis could be further expanded']",4,4,4.0,4,Accept
R11xJsRjA-W,"This paper empirically investigates the connections between out-of-distribution (OOD) generalization, membership privacy, and learning of stable features in domain generalization methods. Through extensive experiments on multiple datasets, it finds that: 1) Better OOD generalization does not necessarily imply better membership privacy, 2) Learning stable features is more consistently correlated with better membership privacy, 3) Membership inference attack accuracy could potentially be used to evaluate stable feature learning in domain generalization methods.","['Can you provide theoretical intuition or analysis for why learning stable features leads to better membership privacy?', 'How might these results generalize beyond image classification tasks to other domains?', 'What are the potential limitations or downsides of using membership inference attack accuracy as a proxy for evaluating stable feature learning?', 'Could you elaborate on potential negative societal impacts or ethical concerns related to this work?', 'What are some potential future directions or extensions of this research?']","['Results may not generalize beyond image classification tasks', 'Lack of theoretical results to support empirical findings', 'Difficulty in directly measuring stable feature learning', 'Limited discussion of potential negative societal impacts or ethical concerns', 'Need for validation on larger, more diverse datasets']",False,4,4,4,7,4,"['Novel insights connecting OOD generalization, privacy, and stable feature learning', 'Extensive empirical evaluation on multiple datasets and state-of-the-art domain generalization methods', 'Results have useful implications for both privacy and domain generalization research', 'Clear and well-structured presentation of experiments and results', 'Proposes membership inference attacks as a new metric for evaluating domain generalization algorithms']","['Primarily empirical study without strong theoretical results or proofs', 'Experiments limited to image classification tasks, unclear generalizability to other domains', ""Some key metrics like 'stable feature learning' are difficult to measure directly"", 'Limited discussion of potential negative societal impacts and limitations']",4,3,4.0,4,Accept
5xEgrl_5FAJ,"This paper introduces BiBERT, a novel method for fully binarizing BERT models (1-bit weights, embeddings, and activations). Key contributions include a Bi-Attention structure to maximize information entropy in binarized attention, and a Direction-Matching Distillation scheme to improve optimization. BiBERT achieves state-of-the-art results for fully binarized BERT on GLUE tasks, with 56.3x FLOPs reduction and 31.2x model size reduction compared to full-precision BERT.","['How does BiBERT perform on NLP tasks beyond GLUE, such as question answering or text generation?', 'What are the actual speedups and memory savings on real hardware implementations?', ""How does BiBERT's performance compare to other recent BERT compression techniques like pruning?"", 'What are the potential limitations or drawbacks of full binarization for BERT in real-world applications?', 'What are some potential applications of BiBERT in resource-constrained environments, such as mobile devices or edge computing?']","['Evaluation scope limited to GLUE benchmark', 'Lack of experiments on actual hardware deployments', 'Potential negative impacts of model compression on fairness or robustness not thoroughly discussed', 'May not generalize well to all NLP tasks or domains', 'Potential challenges in training fully binarized models, such as convergence issues or increased training time']",False,3,3,4,7,4,"['Highly original approach to fully binarizing BERT, including activations', 'Strong theoretical analysis and justification for proposed techniques', 'Significant performance improvements over existing quantized BERT models', 'Impressive efficiency gains in FLOPs (56.3x) and model size (31.2x)', 'Extensive experiments and ablation studies on GLUE benchmark', 'Thorough ablation studies and analysis of the proposed methods']","['Evaluation limited to GLUE benchmark tasks', 'Lack of real-world hardware experiments to validate efficiency claims', 'Performance still lags behind full-precision models on some tasks', 'Limited discussion of potential negative impacts or limitations of the approach']",4,3,4.0,4,Accept
iGffRQ9jQpQ,"This paper introduces Self-interested Coalitional Learning (SCL), a novel semi-supervised learning framework that constructs a semi-cooperative game between the main SSL task and an auxiliary task of predicting label observability. The approach shows theoretical connections to loss reweighting and demonstrates empirical improvements across various SSL tasks including image classification, label propagation, and data imputation.","['How does the computational complexity of SCL compare to standard SSL methods?', 'Can you provide more details on how SCL handles noisy labels in practice?', 'How does the performance of SCL vary with different ratios of labeled to unlabeled data?', 'Are there any specific SSL scenarios where SCL might not be applicable or effective?']","['Experiments are limited to relatively small datasets', 'Computational complexity and scalability not thoroughly analyzed', 'Potential negative impacts of using soft labels not thoroughly discussed', 'May not be directly applicable to some SSL settings like semi-supervised semantic segmentation', 'Practical implementation may require careful tuning of the discriminator architecture']",False,3,3,3,7,4,"['Novel framework combining ideas from game theory and semi-supervised learning', 'Theoretical analysis connecting the method to loss reweighting on noisy labels', 'Consistent empirical improvements across multiple SSL tasks', 'Provides interpretable label confidence measures as a byproduct', 'Generally applicable and can be incorporated into existing SSL methods']","['Limited comparison to state-of-the-art SSL methods', 'Insufficient analysis of hyperparameter sensitivity, especially for alpha', 'Experimental evaluation could be more comprehensive (e.g., more datasets, larger scale)', 'Some claims about robustness and effectiveness are not fully substantiated', 'Added complexity compared to standard SSL approaches']",4,3,3.0,3,Accept
bVT5w39X0a,"This paper introduces multi-modal Relational Neural Process (mRNP), a novel approach for multi-modal learning with missing modalities that combines neural networks with stochastic processes. It uses a graph-based structure to model dependencies between samples across modalities and introduces a 'mixture-of-graphs' approach. The method is theoretically grounded as a valid stochastic process and demonstrates improved performance over existing methods on multi-modal classification tasks, particularly with missing data.","['How does the computational complexity scale with the number of samples and modalities?', 'How sensitive is performance to the choice of reference set size and selection?', 'Can the approach be extended to handle streaming data or more than two modalities?', 'How does training time compare to baselines?', 'Can you provide more ablations on key components of the model?', 'How does the method perform on more complex regression tasks beyond the toy examples provided?']","['Scalability to very large datasets not thoroughly explored', 'Limited evaluation on real-world datasets and tasks beyond classification', 'Potential sensitivity to reference set size and selection', 'Increased model complexity compared to simpler approaches']",False,3,3,4,7,4,"['Novel combination of ideas from neural processes, graph models, and multi-modal learning', 'Theoretical grounding with proofs of exchangeability and consistency', 'Ability to handle missing modalities and provide uncertainty estimates', 'Improved empirical performance over state-of-the-art baselines on real datasets', 'Interpretable latent representation through learned graph structure', 'Capability to encode inductive biases, as shown in regression experiments']","['Scalability to very large datasets is not thoroughly explored', 'Evaluation limited to a small number of datasets and primarily focused on classification tasks', 'Complexity of the model may make it challenging to implement and tune', 'Some ablation studies and comparisons to recent methods could be more comprehensive']",4,3,3.0,4,Accept
I2Hw58KHp8O,"This paper introduces CMLMC, a non-autoregressive machine translation model that addresses token distinguishability and training-inference mismatch issues in existing NAR models. Key contributions include architectural changes to improve token separation and a correction loss to align training with iterative refinement. CMLMC achieves state-of-the-art NAR performance on raw data, approaching autoregressive models on some datasets without relying on knowledge distillation.","['How does the increased inference time of CMLMC compare to autoregressive models and other NAR baselines?', 'Could the correction loss be applied to other NAR architectures beyond CMLM?', 'How does CMLMC perform on very long sequences compared to AR models?', 'Can you provide more theoretical justification for why the proposed architectural changes improve token distinguishability?', 'Have you explored ways to reduce the parameter count and inference time overhead?']","['Increased model size and complexity may limit applicability on resource-constrained devices', 'Performance still lags behind autoregressive models on some datasets', 'Approach is specific to CMLM architecture and may not generalize to all NAR models', 'Evaluation is limited to translation tasks - applicability to other sequence generation tasks is not explored', 'Potential challenges in scaling the approach to larger models or datasets']",False,4,4,4,7,4,"['Novel approach addressing key limitations of existing NAR models', 'Strong empirical results, especially on raw data without distillation', 'Comprehensive experiments and analysis across multiple datasets', 'Thorough ablation studies demonstrating the impact of each component', 'Clear motivation and problem identification', 'Approaches autoregressive performance on some datasets']","['Increased model size and inference time compared to CMLM baseline', 'Limited exploration of potential negative impacts or limitations', 'Some results still lag behind autoregressive models', 'Limited theoretical analysis or justification for the proposed changes']",4,4,4.0,4,Accept
bilHNPhT6-,"This paper proposes using multi-objective reinforcement learning (MORL) as a general framework for addressing challenges in RL, focusing on offline RL and finetuning. The authors introduce a new MORL algorithm called DiME that outperforms linear scalarization in multi-objective tasks and derive novel offline RL and finetuning algorithms based on DiME. Extensive empirical evaluation shows that these new algorithms achieve state-of-the-art performance on several benchmark tasks, particularly in offline RL.","['How might the MORL perspective be applied to other specific challenges in RL beyond offline RL and finetuning?', 'Can you provide a quantitative comparison of the computational costs between DiME and linear scalarization, especially when training policies for multiple trade-offs?', 'What modifications would be necessary to apply this approach to real-world robotic tasks with physical constraints?', 'Are there any potential negative impacts or limitations of the approach in terms of scalability or applicability to different types of RL problems?']","['Focus on only two case studies (offline RL and finetuning) - unclear how broadly applicable the approach is to other RL challenges', 'Computational cost of training multiple policies, which may limit scalability to larger problems', 'Lack of real-world evaluation in physical environments with additional constraints', 'Limited discussion of potential negative societal impacts or ethical considerations of the approach']",False,4,3,3,7,4,"['Novel perspective of using MORL to address fundamental RL challenges', 'Introduction of DiME, a new MORL algorithm with theoretical grounding', 'Derivation of new offline RL and finetuning algorithms based on DiME', 'Strong empirical results, outperforming SOTA in some cases, especially in offline RL', 'Thorough theoretical derivations and connections to existing work', 'Comprehensive experiments across multiple benchmarks and insightful ablations']","['Limited scope of application (focused on two case studies)', 'Potential computational costs of training multiple policies', 'Improvements over baselines are sometimes modest, particularly in finetuning', 'Lack of real-world validation beyond simulated environments', 'Limited discussion of potential negative impacts or limitations']",3,4,3.0,3,Accept
LQCUmLgFlR,"This paper analyzes optimal early stopping in overparameterized and underparameterized linear regression settings, proposing a new overparameterization model and deriving bounds on optimal stopping time. It demonstrates different behaviors in the two regimes, provides empirical validation on linear models and neural networks, and explores the relationship between early stopping and double descent.","['How well do the theoretical results extend to highly nonlinear neural networks?', 'Can you provide more intuition for why the optimal stopping time behaves differently in overparameterized vs underparameterized regimes?', 'How sensitive are the results to the specific noise model and data assumptions?', 'What are the key limitations of extending these results to non-linear neural networks?', 'How does the optimal stopping time vary with other hyperparameters not considered in the study?']","['Analysis is limited to linear models and may not fully capture nonlinear neural network dynamics', 'Empirical validation is primarily on relatively small-scale tasks and limited datasets', 'May not fully capture dynamics of very large neural networks', 'Limited discussion of potential negative impacts of early stopping', 'Assumptions in the theoretical model may not hold in all practical scenarios']",False,3,3,3,7,3,"['Novel theoretical analysis of early stopping in overparameterized settings', 'Explicit characterization and bounds on optimal stopping time', 'Demonstrates different behaviors in over vs. underparameterized regimes', 'Empirical validation on both linear models and neural networks', 'Connects theory to deep learning practice', 'Shows early stopping can mitigate double descent']","['Theoretical analysis is limited to linear models', 'Extension to non-linear neural networks is mostly empirical', 'Writing clarity and presentation could be improved in some sections', 'Could benefit from more extensive large-scale deep learning experiments', 'Limited discussion of practical implications and potential negative impacts']",3,3,3.0,3,Accept
rbFPSQHlllm,"This paper introduces AutoMO-Mixer, a novel approach for medical image diagnosis that combines the MLP-Mixer architecture with multi-objective optimization, Bayesian optimization for hyperparameter tuning, and evidential reasoning for model fusion. The method aims to balance sensitivity and specificity in medical diagnosis while reducing the number of parameters compared to CNNs. Experiments on two medical imaging datasets (brain tumor MRI and COVID-19 CT) show modest improvements over MLP-Mixer and CNN baselines.","['How does the computational cost and training time of AutoMO-Mixer compare to standard Mixer and CNN models?', 'Can you provide ablation studies to show the impact of each component (multi-objective optimization, Bayesian optimization, evidence reasoning)?', 'How does AutoMO-Mixer compare to other state-of-the-art methods in medical image diagnosis?', 'Have you tested the method on larger or more diverse medical imaging datasets?', 'How sensitive is the method to the choice of hyperparameters in the Bayesian optimization?']","['The approach is only evaluated on two datasets, which may limit generalizability claims', ""The paper doesn't discuss potential limitations or failure cases of the proposed method"", 'Computational requirements and scalability are not thoroughly addressed, which could be important for practical applications', 'The performance gains may not generalize to other medical imaging domains', 'The potential societal impact of improved medical image diagnosis is not thoroughly discussed']",False,3,2,3,6,4,"['Novel combination of MLP-Mixer with multi-objective optimization, Bayesian optimization, and evidential reasoning', 'Addresses the important issue of balancing sensitivity and specificity in medical diagnosis', 'Reduces number of parameters compared to CNNs while maintaining or improving performance', 'Consistent improvements over MLP-Mixer and CNN baselines on two datasets']","['Limited experimental evaluation with only two datasets', 'Relatively modest performance gains over baseline models (1-3% on most metrics)', 'Lack of comparison to state-of-the-art medical image diagnosis methods', 'Missing ablation studies to understand the contribution of each component', 'Insufficient analysis of computational costs, training time, and scalability', 'Some sections of the paper could be better organized and written more clearly']",2,3,3.0,2,Accept
ms7xJWbf8Ku,"This paper presents novel algorithms for efficiently packing sequences in NLP datasets to reduce padding and accelerate BERT pre-training. The authors analyze the BERT pre-training dataset, develop two new packing algorithms (SPFHP and NNLSHP), modify the BERT architecture to enable training on packed sequences, and demonstrate a 2x speedup in pre-training without loss of accuracy.","['How well would this approach generalize to other model architectures beyond BERT?', 'What is the impact on a wider range of downstream tasks beyond SQuAD?', 'How does the packing approach impact training dynamics or convergence behavior?', 'How does the 2x speedup compare to other BERT acceleration approaches like distillation or pruning?']","['The approach is specific to BERT and may not generalize to other architectures', 'The maximum speedup is limited by the amount of padding in the dataset', 'Potential impacts on training dynamics are not fully explored', 'Evaluation on downstream tasks is limited', 'May require hyperparameter tuning to match unpacked performance']",False,4,3,3,7,4,"['Novel and efficient packing algorithms that achieve near-optimal efficiency', 'Thorough analysis of padding in BERT pre-training data', 'Thoughtful modifications to BERT architecture to enable packed training', 'Impressive 2x speedup in pre-training without loss of accuracy', 'Potential for significant practical impact on NLP training efficiency', 'Scalability advantages over competing approaches', 'Comprehensive experimental evaluation']","['Approach is primarily focused on BERT, generalizability to other architectures not fully explored', 'May not be equally effective for all NLP tasks/datasets', 'Introduces some additional implementation complexity and hyperparameter tuning', 'Evaluation on downstream tasks could be more extensive']",4,4,3.0,4,Accept
nwKXyFvaUm,"This paper introduces DivFL, a novel client selection method for federated learning that uses submodular optimization to select diverse clients. The approach is theoretically grounded with convergence analysis and demonstrates empirical improvements in convergence speed, fairness, and learning efficiency across synthetic and real datasets.","['How does the method scale to very large numbers of clients (e.g., millions)?', 'What are the privacy implications of using client gradients for selection?', 'How does DivFL compare to other recent diversity-promoting client selection methods?', ""What are the computational/communication tradeoffs of the 'no overhead' variant in practical settings?""]","['May have high computational overhead for large numbers of clients', 'Privacy implications of gradient-based selection not thoroughly discussed', 'Performance on more complex, realistic federated learning tasks not demonstrated', 'Assumes access to client gradients which may not always be feasible in practice']",False,3,3,3,7,4,"['Novel formulation of client selection as a submodular optimization problem', 'Thorough theoretical analysis with convergence guarantees', 'Comprehensive experiments on synthetic and real datasets (FEMNIST, CelebA, Shakespeare)', 'Demonstrated improvements in convergence speed, accuracy, and fairness', 'Addresses important challenges in federated learning (client selection, communication efficiency)']","['Potential computational overhead for client selection not fully analyzed', 'Limited discussion of privacy implications', 'Scalability to very large numbers of clients not thoroughly addressed', 'Theoretical analysis relies on some strong assumptions (e.g., bounded heterogeneity)']",4,3,3.0,3,Accept
anbBFlX1tJ1,"This paper introduces Boosted Curriculum Reinforcement Learning (BCRL), a novel approach for curriculum-based value function approximation in RL. BCRL models the action-value function of each task as a sum of residuals learned on previous tasks, automatically allowing increased representational capacity as task complexity grows. The paper provides theoretical analysis of convergence and error bounds, and demonstrates empirical benefits over regular curriculum RL across multiple domains and algorithms.","['How does the computational complexity scale for very long task sequences?', 'How would BCRL perform on more complex high-dimensional tasks like Atari games?', 'How does BCRL compare to other recent curriculum RL methods not included in the evaluation?', 'Could the approach be extended to automatically generate curricula?', 'How sensitive is the method to the quality of the curriculum sequence?', 'How sensitive is the method to the order of tasks in the curriculum?']","['Increased computational complexity for long task sequences', 'May not scale well to very high-dimensional tasks', 'Limited evaluation on more complex domains', 'Performance depends on having a well-designed curriculum sequence']",False,4,3,4,8,4,"['Novel combination of boosting and curriculum RL concepts', 'Strong theoretical analysis with convergence guarantees and error bounds', 'Comprehensive empirical evaluation showing consistent improvements over baselines', 'Well-motivated approach addressing limitations of fixed function approximators in curriculum RL', 'Flexibility to choose appropriate function approximators for each task', 'Ability to leverage task-specific knowledge (e.g., using LSPI for LQ tasks)']","['Increased computational complexity for long task sequences', 'Limited evaluation on more complex high-dimensional tasks', 'Lack of comparison to some other recent curriculum RL methods', 'Potential scalability issues for very long task sequences or high-dimensional tasks']",4,4,3.0,4,Accept
JVWB8QRUOi-,"This paper introduces a novel homophilic learning framework to promote stable cooperation in sequential social dilemmas. The authors identify that existing incentive mechanisms can lead to unstable cooperation due to second-order social dilemmas. They propose encouraging agents with similar environmental behaviors to have similar incentivizing behaviors as a solution. The paper provides theoretical analysis for single-stage games and empirical results on sequential social dilemma tasks, demonstrating that the proposed method outperforms baselines in achieving stable cooperation.","['How well would the approach scale to much larger populations of agents?', ""Could the method be adapted to work with more limited observability of other agents' actions?"", 'Is it possible to extend the theoretical analysis to the sequential setting?', 'Have you considered any potential negative societal impacts of this work?', 'What are some potential real-world applications of this homophilic learning framework?']","['Theoretical analysis does not cover the main sequential case', ""Requires full observability of other agents' actions"", 'May have limited applicability in some real-world settings', 'Computational complexity could be high for systems with many agents']",False,3,3,3,7,4,"['Novel approach using homophily to address second-order social dilemmas in sequential settings', 'Strong empirical results showing clear improvements over baselines on multiple tasks', 'Good theoretical analysis and intuition for the single-stage case', 'Addresses an important problem in multi-agent cooperation', 'Clear explanation and motivation of key ideas']","['Limited theoretical analysis for the sequential case', ""Assumes full observability of other agents' actions, which may not always be available"", 'Potential scalability issues for large numbers of agents', 'Some practical limitations in applicability are not thoroughly discussed']",4,3,3.0,4,Accept
dEelotBE6e2,"This paper introduces a novel defense mechanism against backdoor attacks on neural networks. The approach combines weak learners in an ensemble to identify and remove poisoned training data, based on a theoretical framework of 'self-expanding sets' and compatibility assumptions between clean and poisoned data distributions. The method, which includes an Inverse Self-Paced Learning algorithm and a boosting framework, shows significant improvements over existing defenses in experiments on CIFAR-10.","['How does the approach scale to larger datasets and models?', 'Can the clean accuracy be improved further without compromising defense?', 'How sensitive is the method to the choice of hyperparameters and weak learners?', 'How does computational complexity compare to baseline defenses?', 'What are potential real-world applications of this defense mechanism?']","['Only evaluated on CIFAR-10 dataset', 'Slight drop in clean accuracy', 'Relies on certain assumptions about data distributions', 'Requires multiple training runs which increases computational cost']",False,4,3,4,9,4,"['Strong theoretical foundation with provable guarantees under certain assumptions', 'Novel algorithm (ISPL) for identifying self-expanding sets', 'Comprehensive experiments showing clear improvements over baselines', 'Well-motivated approach combining ideas from weak learning and boosting', 'Addresses an important problem in ML security', 'Thorough ablation studies demonstrating robustness to hyperparameters']","['Evaluation limited to CIFAR-10 dataset', 'Small drop in clean accuracy (2-3%) compared to undefended models', 'Introduces new hyperparameters that need tuning', 'Theoretical guarantees rely on somewhat strong assumptions']",4,4,3.0,4,Accept
FndDxSz3LxQ,"This paper introduces Learn Locally, Correct Globally (LLCG), a novel communication-efficient algorithm for distributed training of Graph Neural Networks (GNNs). LLCG reduces communication overhead by performing local training on subgraphs with periodic model averaging, combined with a global correction step on the server to account for cross-machine dependencies. Theoretical analysis and extensive experiments on real-world datasets demonstrate that LLCG can achieve accuracy comparable to methods that communicate full graph information, while significantly reducing communication costs.","['How would LLCG perform in a real distributed setting versus the simulated environment?', 'What are the privacy implications of the global correction step?', 'How does LLCG scale to graphs with billions of nodes?', 'How does LLCG perform on other GNN tasks beyond node classification?', 'How does the choice of graph partitioning method affect the performance of LLCG?']","['Evaluation limited to simulated distributed setting', 'Only semi-supervised node classification tasks considered', 'Scalability to very large graphs not demonstrated', 'Privacy considerations of server having access to full graph not fully explored', 'Potential computational overhead of the server correction step not explicitly discussed']",False,4,3,4,8,4,"['Novel algorithm addressing an important problem in distributed GNN training', 'Thorough theoretical analysis providing convergence guarantees', 'Comprehensive experiments across multiple datasets and GNN architectures', 'Significant reduction in communication costs while maintaining accuracy', 'Generalizes to different GNN architectures']","['Evaluation in simulated distributed setting rather than real multi-machine implementation', 'Limited to semi-supervised node classification tasks', 'Scalability to very large graphs (billions of nodes) not demonstrated', 'Limited discussion of privacy implications for the server correction step']",3,4,4.0,4,Accept
HRL6el2SBQ,"This paper proposes using intra-class mixup and angular margin to improve out-of-distribution (OoD) detection for neural networks. The key contributions are: 1) Demonstrating that intra-class mixup reduces angular spread of latent representations, 2) Showing this improves existing OoD detection methods, and 3) Supplementing OoD scores with angular margin information. Extensive experiments across multiple datasets and OoD detection techniques show consistent improvements over baselines.","['Can you provide stronger theoretical justification for the effectiveness of intra-class mixup and angular margin in OoD detection?', 'How does the method compare to more recent OoD detection techniques not evaluated in the paper?', 'Have you tested this approach on non-image domains like text or speech?', 'What is the computational overhead of intra-class mixup training compared to standard training?', 'How might this approach impact model robustness or vulnerability to adversarial examples?']","['Only evaluated on image classification tasks', 'Computational overhead of mixup training not analyzed', 'Limited comparison to other recent OoD detection methods', 'Potential impact on model robustness not explored', 'Lack of discussion on potential negative impacts of the approach']",False,3,3,3,7,4,"['Novel combination of intra-class mixup and angular margin for OoD detection', 'Comprehensive experimental evaluation across multiple datasets and techniques', 'Consistent and significant improvements over strong baselines', 'Method is simple to implement and broadly applicable', 'Good intuition and empirical justification for the proposed approach']","['Limited theoretical analysis, especially for angular margin contribution', 'Experiments restricted to image classification tasks', 'Comparison to some recent OoD detection techniques is missing', 'Computational overhead of mixup training not discussed', 'Some results show high variance across seeds/datasets', 'Lack of discussion on potential negative impacts or limitations of the approach']",3,3,3.0,3,Accept
t1QXzSGwr9,"This paper introduces a framework for classifying larger classical images using quantum neural networks. The key contributions are: (1) A novel encoding scheme to compress images into quantum states using fewer qubits, (2) New quantum neural network architectures (CRADL and CRAML) designed for these encoded states, and (3) Experimental results on MNIST digit classification for 8x8 and 16x16 images, showing comparable performance to classical neural networks in some cases.","['How does the method scale to larger image sizes beyond 16x16?', 'What are the theoretical limitations of the encoding scheme?', 'How does performance compare to state-of-the-art classical CNNs on more complex datasets?', 'Is there a regime where this quantum approach is expected to outperform classical methods?']","['Current approach limited to small black and white images', 'Only tested on simple MNIST digit classification task', 'No clear quantum advantage demonstrated', 'May require quantum hardware beyond near-term capabilities for larger images']",False,3,3,3,7,4,"['Novel quantum encoding scheme to compress classical images into quantum states', 'New quantum neural network architectures (CRADL and CRAML) designed for the encoded states', 'Enables classification of larger images than previous quantum approaches', 'Comparable performance to classical NNs for 8x8 MNIST images', 'Addresses an important challenge in quantum machine learning', 'Thorough explanation of quantum encoding process and network architecture']","['Results limited to small image sizes (maximum 16x16)', 'Performance degrades for 16x16 images compared to classical baseline', 'Limited experimental results (only MNIST dataset)', 'Lack of theoretical analysis or guarantees', 'No clear demonstration of quantum advantage over classical methods', 'High computational cost of simulating quantum systems on classical hardware']",4,3,3.0,4,Accept
e0uknAgETh,"This paper investigates adversarial attacks on spiking neural networks (SNNs) for event-based vision tasks. The authors adapt existing attack methods like SparseFool to work with discrete event data, demonstrate successful attacks on benchmark datasets, validate attacks on neuromorphic hardware, and explore universal adversarial patches as a more realistic attack scenario. The work provides novel insights into the vulnerability of SNNs and event-based vision systems to adversarial attacks.","['How might these attacks perform on more complex neuromorphic vision tasks?', 'Are there theoretical insights into what makes SNNs vulnerable to these attacks?', 'How were the hyperparameters for SparseFool chosen?', 'What other defense strategies besides adversarial training could be effective?', 'What are the potential real-world implications or applications of this research?']","['The work is limited to relatively simple vision tasks', 'Only a brief exploration of defenses is provided', 'The attacks may not generalize to more complex neuromorphic systems', 'Potential negative impacts on security of neuromorphic vision systems']",False,3,3,4,7,4,"['Novel application of adversarial attacks to SNNs and event-based vision', 'Thorough empirical evaluation on multiple datasets', 'Validation of attacks on actual neuromorphic hardware', 'Exploration of universal adversarial patches for more practical attacks', 'Clear writing and good analysis of results', 'Important contribution to understanding security implications for neuromorphic hardware']","['Limited exploration of defense methods', 'Relatively simple datasets/tasks used', 'Lack of theoretical analysis specific to SNN vulnerabilities', 'Some hyperparameters not fully justified']",4,4,3.0,4,Accept
8XM-AXMnAk_,"This paper introduces dynamicAL, a deep active learning method that selects samples to maximize training dynamics. The authors provide theoretical analysis connecting convergence speed, generalization, and training dynamics for wide neural networks, and demonstrate strong empirical performance across multiple datasets and architectures.","['How sensitive is the method to the size of the initial labeled set?', 'Can the theoretical results be extended to more realistic network widths?', 'How does dynamicAL compare to very recent active learning methods like BALD or BatchBALD?', 'How computationally expensive is dynamicAL compared to the baselines?', 'How does the method perform on different types of datasets (e.g., text, time series)?']","['Theoretical results limited to very wide networks', 'May not be as effective for very small or very large labeling budgets', 'Potential sensitivity to initial labeled set size and selection', 'Assumes access to large unlabeled pool']",False,4,3,4,8,4,"['Novel theoretically-motivated active learning criterion', 'Rigorous theoretical analysis connecting convergence, generalization, and training dynamics', 'Efficient approximation algorithm for scalability', 'Comprehensive empirical evaluation on multiple datasets and architectures', 'Consistent improvements over strong baselines', 'Both theoretical and practical contributions']","['Theoretical results rely on very wide network assumptions', 'Performance gains over baselines sometimes modest', 'Limited comparison to very recent state-of-the-art active learning methods', 'Assumes access to large unlabeled pool']",3,3,3.0,4,Accept
qZNw8Ao_BIC,"This paper investigates the robustness of Vision Transformers (ViTs) and proposes a novel patch-based negative augmentation technique to improve their out-of-distribution performance. The authors demonstrate that ViTs rely on non-robust features that survive patch-based transformations but are not semantically meaningful. By using these transformations as negative augmentations during training, the method regularizes the model away from using these non-robust features. Comprehensive experiments show consistent improvements in robustness across multiple benchmarks, even when combined with existing augmentation techniques and large-scale pretraining on datasets like ImageNet-21k.","['Can you provide more theoretical insight into why patch-based negative augmentation improves robustness?', 'How does this approach compare to other robustness techniques beyond data augmentation?', 'What is the computational overhead of using patch-based negative augmentation during training?', 'How well does this method generalize to other datasets and tasks beyond ImageNet classification?']","['The method is specific to ViT architectures', 'Experiments are limited to ImageNet-based datasets', 'Some hyperparameter tuning may be required', 'Increased computational resources likely needed for training']",False,4,4,4,8,4,"['Novel insights into ViT robustness and reliance on non-robust features', 'Innovative negative augmentation technique specific to ViT architecture', 'Comprehensive empirical evaluation across multiple robustness benchmarks', 'Demonstrates complementarity with existing augmentation methods', 'Improves robustness without sacrificing in-distribution accuracy', 'Effective even with large-scale pretraining datasets']","['Limited theoretical analysis of why the approach works', 'Experiments are limited to ImageNet-based datasets', 'Improvements, while consistent, are relatively modest in some cases', 'No comparison to robustness techniques beyond data augmentation']",4,4,4.0,4,Accept
RLtqs6pzj1-,"This paper introduces FreeTickets, a novel approach for efficient ensemble learning using sparse neural networks. The method leverages dynamic sparse training to generate diverse sparse subnetworks for ensembling, proposing two techniques: DST Ensemble and EDST Ensemble. Empirical results demonstrate improvements over state-of-the-art ensemble methods in terms of accuracy, robustness, uncertainty estimation, and efficiency, while using fewer parameters and computational resources. The work also provides insights into the diversity of sparse subnetworks and their impact on ensemble performance.","['How well does the approach scale to larger models and datasets?', 'Is there a theoretical justification for why dynamic sparse training leads to more diverse subnetworks?', 'How sensitive are the results to hyperparameters like sparsity levels and update intervals?', 'Could this approach be combined with other efficient ensemble techniques?', 'What are potential applications of this method beyond image classification tasks?']","['Experiments limited to image classification tasks', 'Dependence on hardware support for sparse networks to realize full efficiency gains', 'May require tuning of sparsity levels and other hyperparameters for new scenarios', 'Extended training time for large datasets']",False,3,3,4,8,4,"['Novel combination of sparse training and ensemble learning', 'Significant improvements in accuracy, robustness, and efficiency over SOTA methods', 'Thorough empirical evaluation and analysis, including diversity studies', 'Potential for large efficiency gains in both training and inference', 'Strong performance across multiple metrics (accuracy, uncertainty, OOD robustness)', 'Comprehensive ablation studies and analysis of factors affecting performance']","['Limited theoretical justification for the approach', 'Evaluation primarily focused on image classification tasks', 'Potential scalability issues with larger models and datasets', 'Practical speedups on real hardware not demonstrated', 'Some added complexity compared to standard ensembles']",4,3,3.0,4,Accept
kHNKTO2sYH,"This paper introduces CLSVAE, a novel semi-supervised VAE model for detecting and repairing systematic outliers in data. The key ideas include partitioning the latent space into clean and dirty subspaces, using a small labeled trusted set for supervision, and enforcing low mutual information between subspaces. Experiments on image datasets demonstrate CLSVAE's superior performance over baselines, especially with small labeled sets.","['How well would the method generalize to non-image data types like tabular or time series data?', 'What is the computational overhead compared to simpler baseline methods?', 'How sensitive is the performance to the choice of hyperparameters, especially the subspace dimensions?', 'Have you tested or considered applying this approach to any real-world datasets with natural systematic outliers?', 'How does the method scale to larger datasets, both in terms of computational resources and performance?']","The authors acknowledge limitations in generalizing beyond image data and using synthetic error types. They could further discuss potential negative impacts of automated data repair systems and the method's applicability to high-dimensional data. The reliance on a trusted set, while small, may still be challenging in some real-world scenarios where obtaining clean labels is difficult or expensive.",False,3,4,3,7,4,"['Novel model design well-suited for systematic outlier detection and repair', 'Effective use of small labeled trusted sets (often <2% of data)', 'Strong empirical results, particularly in repair quality', 'Comprehensive experimental evaluation on multiple datasets and metrics', 'Clear explanation and motivation of key ideas', 'Novel distance correlation penalty to encourage low mutual information between subspaces']","['Limited to image data, generalization to other data types unclear', 'Use of synthetic corruption process rather than real-world outliers', 'Lack of theoretical analysis or guarantees', 'Hyperparameter tuning process and computational complexity not fully addressed']",4,3,4.0,3,Accept
izj68lUcBpt,"This paper introduces Temporally-Adaptive Convolutions (TAdaConv) for video understanding. TAdaConv dynamically calibrates convolution weights for each frame based on its local and global temporal context. The method shows strong performance on action recognition and localization tasks, outperforming or matching state-of-the-art approaches across multiple datasets. It can be easily integrated into existing architectures as a drop-in replacement for standard convolutions.","['Can you provide more theoretical justification or intuition for why TAdaConv works well?', 'How does the computational overhead of TAdaConv compare to other temporal modeling approaches and recent transformer-based models?', 'Have you evaluated TAdaConv on other video understanding tasks beyond action recognition and localization?', 'Could TAdaConv be applied to other domains that involve sequential data, such as natural language processing or time series analysis?']","['Increased model complexity may limit applicability in resource-constrained settings', 'Performance on very long video sequences not evaluated', 'Potential negative impacts not thoroughly discussed', 'Challenges in real-time applications due to the adaptive nature of the convolutions']",False,3,3,3,7,4,"['Novel approach for temporal modeling in videos', 'Strong empirical results on multiple video datasets and tasks', 'Can be easily integrated into existing architectures', 'Comprehensive experiments and ablation studies']","['Limited theoretical analysis or justification', 'Increased model complexity and computational overhead not thoroughly discussed', 'Comparison to recent transformer-based video models is somewhat limited', 'Experiments mainly focused on action recognition and localization tasks']",4,3,3.0,3,Accept
dzZQEvQ6dRK,"This paper investigates the disentanglement properties of contrastive learning methods, focusing on BYOL. The key findings are that BYOL learns 'group disentangled' representations without explicit disentanglement objectives, achieves state-of-the-art performance on several disentanglement benchmarks, and demonstrates good disentanglement on real-world datasets like CelebA. The study also explores how implementation details such as normalization affect disentanglement.","['Can you provide more theoretical insight into why contrastive learning, particularly BYOL, leads to disentanglement?', ""How can the 'group disentanglement' property be more rigorously defined and quantified?"", 'Have you considered evaluating on more diverse real-world datasets beyond CelebA?', 'How do other contrastive learning methods like SimCLR or MoCo compare in terms of disentanglement?', 'Are there ways to leverage the group disentanglement property to improve downstream task performance?', 'Have you considered potential negative societal impacts of this work, such as unintended biases in the learned representations?']","['The study is primarily empirical with limited theoretical grounding', 'Evaluation on real-world data is limited mostly to the CelebA dataset', ""The concept of 'group disentanglement' could be more rigorously defined and analyzed"", 'Results are focused on BYOL and may not generalize to all contrastive methods']",False,3,3,3,8,4,"['Extensive empirical evaluation on multiple datasets and disentanglement metrics', 'Novel finding that contrastive learning can achieve strong disentanglement without explicit objectives', ""Introduction of the 'group disentanglement' concept"", 'State-of-the-art results on several disentanglement benchmarks', 'Evaluation on real-world datasets (CelebA)', 'Useful ablation studies on implementation details affecting disentanglement']","['Limited theoretical analysis and understanding of why contrastive learning leads to disentanglement', ""The 'group disentanglement' property needs more rigorous definition and analysis"", 'Real-world evaluation is limited primarily to the CelebA dataset', 'Lack of comparison with some recent disentanglement methods', 'Focus mainly on BYOL, with limited exploration of other contrastive learning methods']",4,4,3.0,3,Accept
bgAS1ZvveZ,"This paper introduces a method to improve value learning in reinforcement learning by using lower bounds on the optimal value function to enhance the Bellman value target. The authors provide a theoretical proof of convergence for the tabular case and demonstrate practical implementations using discounted episodic returns and goal-distance based returns. Extensive experiments show improved sample efficiency over baselines on a variety of tasks including Atari games, robotic manipulation, and simulated navigation.","['How might the method be extended to handle stochastic environments?', 'Can the theoretical analysis be expanded to cover the function approximation case?', 'How does the computational cost compare to related methods like n-step returns?', 'In what types of environments or tasks does this approach provide the largest benefits?', 'What potential impact could this method have on real-world RL applications?']","['Current applicability limited to deterministic environments', 'Theoretical guarantees only provided for the tabular case', 'Potential for overestimation in stochastic settings', 'Need for further investigation on more complex or real-world tasks']",False,4,4,4,8,4,"['Strong theoretical foundation with proof of convergence in tabular case', 'Simple and effective implementation requiring no additional hyperparameters', 'Consistent empirical improvements across diverse tasks and environments', 'Thorough experimental evaluation and comparisons to baselines', 'Clear exposition and motivation for the proposed method']","['Current implementation limited to deterministic environments', 'Lack of theoretical analysis for function approximation case', 'Potential for overestimation in stochastic settings not fully explored', 'Some comparisons to related methods (e.g., n-step returns) could be more comprehensive']",4,4,4.0,4,Accept
biyvmQe5jM,"This paper investigates the relationship between learning rate schedules and weight norm dynamics in deep learning. It proposes ABEL, an automatic scheduler based on weight norm bouncing, and demonstrates that complex schedules are only beneficial when weight norm exhibits 'bouncing' behavior. The study includes extensive experiments across vision, NLP, and RL tasks.","['How generalizable are the findings to other architectures or tasks not tested?', 'Can you provide more theoretical insight into why weight norm bouncing is beneficial?', 'How does ABEL compare to other adaptive learning rate methods like Adam?', 'How sensitive is ABEL to the choice of hyperparameters like the decay factor?']","The authors acknowledge the empirical nature of the work and lack of theoretical understanding. The practical impact may be most significant for vision tasks with L2 regularization, while the benefits for NLP and RL tasks appear more limited. More extensive comparisons to other automatic scheduling methods would be valuable.",False,3,3,3,7,4,"['Comprehensive empirical study across multiple domains (vision, NLP, RL)', 'Proposal of ABEL, a practical automatic scheduler that performs well without extensive tuning', 'Identification of an interesting connection between weight norm dynamics and schedule effectiveness', 'Thorough ablation studies and analysis', 'Potential to simplify learning rate scheduling in certain domains']","['Limited theoretical understanding of why bouncing weight norms benefit from complex schedules', 'Generalizability beyond tested architectures/tasks is unclear', 'Some claims could benefit from more rigorous statistical analysis', 'Practical impact may be limited as many already use well-tuned schedules']",4,3,3.0,3,Accept
nkaba3ND7B5,"This paper introduces a formalism and benchmark for autonomous reinforcement learning (ARL), which aims to develop RL algorithms that can learn with minimal human intervention. The key contributions are: (1) A formal framework for ARL, including definitions for deployment and continuing evaluation settings. (2) A new benchmark (EARL) with 6 diverse environments designed to test autonomous learning. (3) Experimental results showing that current RL and ARL algorithms struggle on these tasks compared to oracle RL with resets. (4) Analysis of exploration challenges and potential benefits of ARL.","['How might the insights from this work guide the development of new ARL algorithms?', 'Could you provide more analysis of why existing ARL methods struggle on some tasks?', 'How well do you expect the simulated benchmark tasks to transfer to real-world robotics scenarios?', 'Have you considered extending the benchmark to include non-robotics domains?', 'What are the computational requirements of ARL methods compared to standard RL?']","['Benchmark is limited to simulated environments, not tested in real-world settings', 'Does not propose new ARL algorithms', 'Limited analysis of negative societal impacts of ARL', 'Potential scalability issues of ARL methods to more complex tasks']",False,3,4,4,7,4,"['Addresses an important gap between typical RL benchmarks and real-world autonomous learning', 'Provides a well-designed formal framework and benchmark (EARL) for ARL research', 'Thorough experiments comparing multiple baselines and prior ARL methods', 'Insightful analysis of exploration challenges and potential benefits of ARL', 'Clear metrics and evaluation protocols to measure progress on ARL']","['Benchmark is limited to simulated environments, not tested in real-world settings', 'Limited analysis of why existing ARL methods struggle on some tasks', 'No novel algorithmic contributions', 'Analysis and insights could be expanded further', 'Lack of discussion on computational requirements for ARL methods']",4,3,4.0,4,Accept
OqlohL9sVO,"This paper introduces UGALR, a novel single-stage image retrieval method that unifies global and local feature learning. Key innovations include a Local Attention Learning Module (LALM), CNN-based learning of homography transformations, and intermediate supervision. The method achieves state-of-the-art results on standard benchmarks while being more efficient than existing approaches that use separate global and local features.","['Can you provide stronger theoretical justification for how CNNs learn homography transformations?', 'How does UGALR compare to very recent state-of-the-art approaches, such as DELG?', 'What are the potential limitations and negative societal impacts of this approach?', 'Can you provide a more detailed analysis of computational complexity and run-time compared to baseline methods?', 'How well does the method generalize to different types of images or retrieval tasks beyond the standard benchmarks?']","['Evaluation limited to standard benchmarks - real-world performance unclear', 'Potential negative impacts of more accurate image retrieval systems not thoroughly discussed', 'Method introduces significant complexity which may limit adoption', 'Approach may not generalize well to very different types of images or retrieval tasks']",False,3,3,3,8,4,"['Novel single-stage approach unifying global and local feature learning', 'State-of-the-art performance on standard image retrieval benchmarks', 'Improved efficiency in terms of memory usage and extraction speed', 'Comprehensive ablation studies validating key components', 'Innovative use of CNNs to implicitly learn homography transformations']","['Limited theoretical justification for CNN-based homography learning', 'Lack of comparison to very recent state-of-the-art methods', 'Insufficient discussion of potential limitations and negative societal impacts', 'Complexity of the method may affect reproducibility']",4,3,3.0,4,Accept
cLcLdwOfhoe,"This paper introduces FedLite, a communication-efficient federated learning framework designed for resource-constrained clients. It combines split learning with a novel vector quantization scheme to compress activations communicated between clients and servers. Additionally, it proposes a gradient correction technique to mitigate issues arising from quantization. Theoretical analysis and extensive experiments on standard federated learning benchmarks demonstrate significant communication reduction (up to 490x) with minimal accuracy loss.","['How does the method compare to other activation compression techniques like sparsification?', 'What are the privacy implications of the proposed quantization scheme and split learning approach?', 'How sensitive is the method to the choice of hyperparameters, particularly the number of clusters and the gradient correction parameter λ?', 'How well does the method generalize to other types of ML tasks beyond classification and to larger models and datasets?', 'What is the computational overhead of the quantization process on the client devices?']","['Privacy implications of the quantization scheme and split learning approach not thoroughly analyzed', 'Evaluation limited to standard federated learning benchmarks, not real resource-constrained scenarios', 'Method introduces additional hyperparameters that need tuning, which may be challenging in practice', 'Potential trade-off between compression and accuracy may be worse for more complex tasks or larger models']",False,4,3,4,8,4,"['Novel combination of split learning and vector quantization for efficient federated learning', 'Impressive empirical results showing large communication reduction with minimal accuracy loss', 'Addresses an important practical problem in federated learning for resource-constrained devices', 'Provides both theoretical convergence analysis and comprehensive empirical evaluation', 'Thorough ablation studies validating key components of the proposed method']","['Limited discussion and analysis of privacy implications', 'Evaluation primarily on standard benchmarks, not real resource-constrained scenarios', 'Introduction of additional hyperparameters that require tuning', 'Limited comparison to other compression methods beyond vanilla product quantization', 'Potential increased computational cost on clients not thoroughly analyzed']",4,4,4.0,4,Accept
FZoZ7a31GCW,"This paper introduces Draupnir, a deep generative model for ancestral sequence reconstruction that uses a tree-structured Ornstein-Uhlenbeck process as an informative prior for a variational autoencoder. The model explicitly represents the evolutionary process and is shown to perform competitively with state-of-the-art phylogenetic methods on both simulated and experimental datasets, while also capturing coevolution between sequence positions, which is a significant advancement in the field.","['How does the computational complexity of Draupnir compare to traditional phylogenetic methods?', 'Have you explored any strategies for improving scalability to larger datasets?', 'How might the approach be extended to handle multiple protein families simultaneously?', 'Could the model be extended to jointly infer the phylogenetic tree structure?', 'What biological insights can be gained from analyzing the learned latent representations?']","['Current limitation to single protein families', 'Potential scalability issues for very large datasets', 'Computational complexity may limit practical applicability', 'Requires a given phylogenetic tree']",False,3,3,3,7,4,"['Novel integration of tree-structured OU process as a prior in a deep generative model', 'Competitive performance on ancestral sequence reconstruction benchmarks', 'Ability to capture coevolution between sequence positions, unlike traditional methods', 'Comprehensive experiments on both simulated and experimental datasets', 'Thorough ablation studies demonstrating importance of model components', 'Clear exposition and detailed methods/appendix']","['Currently limited to single protein families', 'Potential scalability issues for very large datasets', 'Lack of comparison to other deep learning approaches for ancestral reconstruction', 'Computational complexity may be higher than traditional phylogenetic methods', 'Phylogenetic tree is assumed to be given, not learned']",4,3,3.0,3,Accept
8rCMq0yJMG,"This paper introduces STU-KD, a novel method for adapting compact models to new target domains on edge devices while preserving privacy and performance on the original source domain. Key components include a memory-efficient domain adaptation method (LRHT), collaborative knowledge distillation (Co-KD), and integration with secure aggregation. Comprehensive experiments demonstrate significant improvements over existing methods across multiple image classification datasets.","['How could the method be extended to other types of machine learning tasks beyond image classification?', 'What strategies could be employed to simplify the implementation or tuning process for practical deployment?', 'How might the approach be adapted for scenarios with extremely limited computational resources on edge devices?', 'Are there potential ways to strengthen the privacy guarantees of the method?']","['Complexity in practical implementation and tuning', 'Limited evaluation of computational efficiency and scalability', 'Privacy guarantees not rigorously proven']",False,3,3,4,8,4,"['Novel approach combining memory-efficient domain adaptation and collaborative knowledge distillation', 'Addresses an important problem in edge ML with privacy constraints', 'Comprehensive experiments showing significant improvements over state-of-the-art methods', 'Effective integration of multiple techniques (knowledge distillation, federated learning, etc.)', 'Thorough ablation studies and analysis']","['Method involves several components and hyperparameters, potentially complex to implement and tune', 'Limited analysis on computational efficiency, scalability, and latency', 'Privacy guarantees could be strengthened or more rigorously analyzed', 'Evaluation primarily limited to image classification tasks']",4,3,4.0,4,Accept
So6YAqnqgMj,"This paper introduces μ-EigenGame, an improved version of the EigenGame algorithm for eigendecomposition. The key contributions include an unbiased stochastic update allowing better parallelization, theoretical convergence guarantees, and strong empirical performance on large-scale tasks. The algorithm demonstrates clear improvements over the previous α-EigenGame method, enabling better scaling to massive datasets.","['Can you provide any insights or estimates on the finite-sample behavior of μ-EigenGame, even if not rigorous bounds?', 'How does μ-EigenGame compare computationally to other state-of-the-art eigenvector computation methods in terms of time and memory complexity?', ""Have you conducted any sensitivity analyses on the algorithm's performance with respect to learning rate and other key hyperparameters?""]","['The asymptotic nature of the convergence guarantee', 'Limited comparison to a broader set of baseline methods', 'Potential sensitivity to hyperparameter choices', 'The paper does not extensively discuss potential negative societal impacts', 'Computational costs of the algorithm are not explicitly analyzed']",False,4,3,4,7,4,"['Novel unbiased stochastic algorithm with theoretical convergence guarantees', 'Impressive empirical results on large-scale dimensionality reduction and graph clustering tasks', 'Enables better parallelization and scaling to massive datasets', 'Clear improvements over prior α-EigenGame method', 'Thorough theoretical analysis and extensive experiments']","['Theoretical analysis is asymptotic rather than finite-sample', 'Limited empirical comparisons beyond α-EigenGame', 'Potential sensitivity to hyperparameter choices']",3,4,4.0,4,Accept
Zca3NK3X8G,"This paper introduces WaveCorr, a novel deep reinforcement learning architecture for portfolio management. The key contributions are: (1) Introducing an 'asset permutation invariance' property for portfolio policy networks, (2) Designing a CNN-based architecture (WaveCorr) that satisfies this property while capturing cross-asset dependencies, and (3) Demonstrating significantly improved performance over state-of-the-art baselines on multiple datasets. Extensive experiments show 3-25% improvement in returns and up to 200% improvement in Sharpe ratio compared to baselines.","['How does the computational complexity and training time of WaveCorr compare to the baselines?', 'Have you considered comparing WaveCorr to any traditional portfolio optimization methods?', 'Are there any limitations or potential failure cases for the WaveCorr approach that you could discuss?', 'How sensitive is the performance to the choice of hyperparameters?', 'Have you tested the approach on other asset classes beyond stocks?']",The authors provide limited discussion on potential limitations of their approach and cases where it may not perform well. More extensive testing on different market regimes and asset classes could strengthen the results.,False,4,3,4,9,4,"['Novel theoretical contribution with the asset permutation invariance property', 'Well-designed neural network architecture (WaveCorr) that satisfies the proposed property', 'Impressive empirical results showing significant improvements over state-of-the-art baselines', 'Thorough experimental analysis including sensitivity studies and ablations', 'Clear and well-written presentation']","['Limited discussion of potential limitations and negative impacts of the approach', 'Lack of comparison to traditional portfolio optimization methods', 'Insufficient details on computational requirements and training time', 'Experiments limited to stock market data, not other asset classes']",4,3,4.0,4,Accept
YfFWrndRGQx,"This paper introduces a framework for multi-objective online convex optimization (MO-OCO) and proposes the Online Mirror Multiple Descent (OMMD) algorithm with two variants. The authors formulate the MO-OCO problem, define appropriate regret metrics, provide theoretical analysis with regret bounds, and evaluate the algorithm on both simulated and real-world datasets. The work makes significant contributions in formulating MO-OCO, designing gradient-based algorithms for this setting, and providing theoretical guarantees for dynamic regret.","['How does OMMD compare to other existing multi-objective online learning methods beyond linear scalarization?', 'What are the key practical applications where this approach could have significant impact?', 'How well does the method scale to high-dimensional problems or large numbers of objectives?', 'Are there plans to extend the analysis to multi-objective static regret in future work?', 'How sensitive is the performance of OMMD-II to the choice of the regularization parameter α?']","['Focus on dynamic regret with static regret left as future work', 'Empirical evaluation somewhat limited in scope and comparison to state-of-the-art methods', 'Scalability to high-dimensional problems or many objectives not thoroughly explored', 'Potential negative impacts and broader implications not extensively discussed']",False,4,3,4,7,4,"['Novel formulation of multi-objective online convex optimization (MO-OCO) framework', 'Proposal of Online Mirror Multiple Descent (OMMD) algorithm with two variants (OMMD-I and OMMD-II)', 'Rigorous theoretical analysis with non-trivial dynamic regret bounds', 'Empirical validation on synthetic, real-world classification, and multi-task learning datasets', 'Addresses an important and timely problem in machine learning and optimization']","['Analysis limited to dynamic regret, with static regret left as an open problem', 'Somewhat limited scope of experiments and comparisons to other multi-objective methods', 'Limited discussion on practical implications and broader impact of the proposed approach', 'Presentation and clarity could be improved in some technical sections']",4,3,3.0,3,Accept
Oh1r2wApbPv,"This paper introduces an Imagine-and-Verbalize (I&V) method for generative commonsense reasoning (GCSR) tasks. The approach incorporates an intermediate step of generating a scene knowledge graph (SKG) before verbalizing it into natural language. The imagination module is trained on diverse knowledge sources and demonstrates improved performance on GCSR tasks, particularly in low-resource settings.","['How does the computational complexity and runtime of I&V compare to end-to-end methods?', 'Can you provide a more comprehensive human evaluation comparing to baselines?', 'How does the quality of generated SKGs compare to silver-standard SKGs used for training?', 'Have you considered extending the approach to other NLP tasks beyond GCSR?', 'What are the potential future directions or extensions of the I&V method?']","['The approach adds complexity which may make it challenging to deploy in real-time applications', 'Performance gains are modest on some benchmarks', 'Reliance on the quality of silver-standard SKGs for training', 'May not generalize well to very abstract or specialized domains']",False,3,3,3,7,4,"['Novel approach using SKGs as an intermediate representation for GCSR', 'Effective utilization of diverse knowledge sources for training the imagination module', 'Consistent performance improvements over strong baselines', 'Improved performance in low-resource settings', 'Detailed ablation studies and analysis', 'Thorough experimental setup and comprehensive evaluation']","['Added complexity compared to end-to-end methods', 'Limited scope of human evaluation', 'Modest gains over state-of-the-art on some benchmarks', 'Lack of comprehensive analysis on the quality of generated SKGs', 'Limited discussion of limitations and potential negative impacts']",4,3,4.0,3,Accept
youe3QQepVB,"This paper introduces a novel framework called multi-task oriented generative modeling (MGM) for improving multi-task visual learning. The key innovation is using a generative model to synthesize images with weak labels that benefit multiple downstream tasks. The framework incorporates auxiliary components to effectively utilize the synthetic data. Experiments on standard benchmarks demonstrate consistent performance improvements over state-of-the-art multi-task learning approaches, particularly in low-data regimes.","['How does the computational cost and training time compare to standard multi-task learning?', 'How well would this approach generalize to a broader range of visual tasks or even tasks outside computer vision?', 'Could the visual quality of synthesized images be improved while maintaining task performance?', 'How would the approach scale to a much larger number of tasks (e.g., 10+)?', 'What are the potential negative societal impacts of using synthetic data in this context?']","['Experiments limited to a small set of visual tasks', 'Added complexity may limit practical applicability in some scenarios', 'Potential negative impacts of using synthetic data not thoroughly discussed', 'Unclear scalability to a much larger number of tasks']",False,3,3,3,7,4,"['Novel and original approach combining generative modeling with multi-task learning', 'Comprehensive experiments and ablation studies validating the approach', 'Consistent performance improvements over SOTA baselines across datasets and tasks', 'Flexible framework applicable to various multi-task scenarios', 'Particularly effective in low-data regimes']","['Increased model complexity and training time compared to standard multi-task learning', 'Limited exploration of scalability to a larger number of tasks', 'Synthesized images lack visual realism, which may limit applicability in some domains', 'Limited theoretical analysis or justification for the approach']",4,3,4.0,4,Accept
8Py-W8lSUgy,"This paper introduces MetaLink, a novel relational multi-task learning framework that leverages auxiliary task labels through a knowledge graph connecting data points and tasks. The approach reformulates multi-task learning as link prediction on this graph, using graph neural networks to model complex relationships. Extensive experiments on multiple benchmarks demonstrate significant improvements over state-of-the-art methods, with up to 27% performance gains.","['Can the authors provide any theoretical analysis or guarantees for the MetaLink framework?', 'How well does MetaLink generalize to multi-task problems beyond multi-label classification?', 'How does the method handle tasks that are uncorrelated or negatively correlated?', 'What are the computational costs and scalability of the method compared to standard multi-task learning approaches?', 'What are potential applications of MetaLink in domains beyond those tested in the paper?']","['May be less effective when tasks are not correlated', 'Empirical study limited mostly to multi-label learning scenarios', 'Potential for learning biased correlations if tasks are not carefully defined', 'Scalability to very large numbers of tasks not demonstrated']",False,3,3,4,7,4,"['Innovative problem formulation of relational multi-task learning', 'Novel use of knowledge graphs to connect data and tasks', 'Strong empirical results across multiple datasets and settings', 'Comprehensive experiments and ablation studies', 'Potential for broad impact in various domains', 'Flexibility to handle different multi-task learning settings']","['Limited theoretical analysis or guarantees', 'Experiments focused primarily on multi-label classification tasks', 'Potential limitations when tasks are not well-correlated', 'Some aspects of the methodology could be explained more clearly']",4,3,2.0,4,Accept
fVu3o-YUGQK,"This paper introduces Efficient self-supervised Vision Transformers (EsViT), a novel approach for visual representation learning. The key contributions are: (1) using multi-stage transformer architectures with sparse self-attention for improved efficiency, (2) identifying the loss of semantic correspondence learning in such architectures, and (3) proposing a new non-contrastive region-matching pretraining task to recover this ability. EsViT achieves state-of-the-art performance on ImageNet linear probing while being significantly more efficient than previous methods.","['How well does the approach generalize to non-transformer architectures?', 'Could the region matching task be adapted to improve performance on dense prediction tasks?', 'How sensitive is the method to hyperparameters of the region matching task and multi-stage architecture?', 'How does the method perform when pre-trained on much larger, web-scale datasets?', 'Could this approach be adapted for self-supervised learning in other domains, such as natural language processing or speech recognition?']","['The approach may not generalize as well to dense prediction tasks', 'Computational requirements may still be high for very large-scale pre-training', 'The method is specific to carefully designed transformer architectures, which may limit flexibility', 'Limited exploration of pre-training on larger, less curated datasets']",False,4,4,4,7,4,"['Novel identification and recovery of correspondence learning ability in self-supervised transformers', 'Significant improvements in efficiency and performance over previous methods', 'Comprehensive empirical evaluation across multiple benchmarks', 'Insightful analysis of learned correspondences and attention maps', 'Effective combination of multi-stage architectures and new pretraining task', 'Thorough ablation studies and analysis of the proposed method']","['Limited performance improvement on dense prediction tasks like object detection', 'Method is specific to transformer architectures', 'Some computational overhead from the region matching task', 'Initial exploration of pre-training on larger, less curated datasets with room for further investigation']",3,3,4.0,4,Accept
N0uJGWDw21d,"This paper introduces BINGO, a novel self-supervised distillation method that significantly improves the performance of small models. BINGO uses a 'bagging' strategy to group related instances based on feature similarity from a teacher model, and then distills knowledge by aggregating representations within these bags. The method achieves state-of-the-art results on ImageNet linear evaluation, semi-supervised learning, and various downstream tasks, particularly for smaller models like ResNet-18/34, addressing an important gap in self-supervised learning for resource-constrained scenarios.","['How does the method scale to larger student models beyond ResNet-34?', 'What is the additional computational cost compared to baseline methods in wall-clock time?', 'How sensitive is the performance to hyperparameters like bag size and bagging strategy?', 'Could the authors provide more theoretical intuition or analysis for why aggregating representations within bags is so effective?', 'Are there potential applications of this method beyond computer vision tasks?']","['Increased computational complexity may limit applicability in some scenarios', 'Performance on very large models and more challenging datasets not fully explored', 'Requires a pre-trained teacher model', 'Potential negative impacts of improved self-supervised learning are not discussed']",False,4,3,3,7,4,"['Novel approach combining instance grouping and distillation for self-supervised learning', 'Strong empirical results, significantly outperforming previous methods across multiple benchmarks', 'Comprehensive experiments and thorough ablation studies', 'Particularly effective for improving performance of small, resource-constrained models', 'Clear and well-motivated technical approach', 'Demonstrates effective knowledge transfer from large to small models in self-supervised setting']","['Increased computational overhead compared to some baseline methods', 'Limited theoretical analysis on why aggregating representations within bags is so effective', 'Gains may be less significant for larger student models', 'Evaluation focused mainly on ImageNet, with limited exploration of other datasets']",3,4,3.0,4,Accept
q79uMSC6ZBT,"This paper introduces GRAMMFORMER, a novel code completion model that generates code sketches with 'holes' for uncertain parts using a grammar-guided approach and reinforcement learning. The model is evaluated on Python and C# datasets, showing improvements over baselines on a new REGEXACC metric for measuring sketch quality.","['How does GRAMMFORMER compare to recent large language models fine-tuned for code completion?', 'What is the computational cost and inference speed of GRAMMFORMER compared to standard left-to-right models?', 'Have you conducted any user studies to evaluate if developers find the generated sketches useful in practice?', 'How sensitive are the results to the choice of grammar and hyperparameters in the reinforcement learning training?']","['The approach requires grammar specifications for each target language, which may limit broad applicability', 'Higher computational cost may make real-time code completion challenging', 'Lack of human evaluation makes it unclear how useful the generated sketches are in practice', 'The model does not incorporate semantic information beyond syntactic structure']",False,3,3,3,7,4,"['Novel grammar-guided approach to generating code sketches with holes', 'Introduction of REGEXACC metric for evaluating partial code completions', 'Thorough empirical evaluation on large Python and C# datasets', 'Outperforms baselines on REGEXACC metric', 'Qualitative examples demonstrate potential benefits of the approach']","['Limited comparison to state-of-the-art code completion models', 'Lack of human evaluation to validate practical usefulness of generated sketches', 'Potentially high computational cost compared to standard approaches', 'Improvements over baselines are sometimes modest', 'Evaluation focused mainly on the new REGEXACC metric']",3,3,3.0,3,Accept
-9ffJ9NQmal,"This paper introduces VICE (Variational Inference for Concept Embeddings), a novel method for learning interpretable object concept embeddings from human odd-one-out judgments. VICE employs variational inference with a spike-and-slab prior to learn sparse, non-negative embeddings with uncertainty estimates. The authors demonstrate that VICE performs comparably or better than the existing SPoSE method on various metrics, with notable improvements on smaller datasets.","['How does the computational complexity of VICE compare to SPoSE, especially for larger datasets?', 'Can you provide more quantitative evaluation of the interpretability of the learned dimensions, possibly through human labeling experiments?', 'How sensitive are the results to the choice of hyperparameters, particularly the FDR threshold?', 'Have you explored applying VICE to datasets from different domains or with different characteristics?', 'What are potential applications of VICE beyond cognitive science research?']","['The method has only been evaluated on a single dataset from one domain', 'Scalability to very large numbers of concepts or dimensions is not addressed', 'The interpretability of learned dimensions relies on subjective human judgment', 'Potential negative societal impacts of learning biased concept representations are not discussed', 'Interpreting learned dimensions may be challenging for non-experts']",False,3,3,3,7,4,"['Novel combination of variational inference and spike-and-slab prior for concept embedding', 'More principled approach to determining embedding dimensionality', 'Significantly improved performance on smaller datasets compared to SPoSE', 'Comprehensive empirical evaluation on multiple criteria', 'Potential impact on cognitive science research due to data efficiency']","['Limited evaluation of interpretability compared to original SPoSE work', 'Modest improvements over SPoSE on the full dataset', 'Some hyperparameters still require tuning (e.g., FDR tolerance)', 'Computational complexity and scalability not thoroughly addressed', 'Evaluation limited to a single dataset from one domain']",4,3,3.0,4,Accept
UvNXZgJAOAP,"This paper introduces a 'sharp attention' mechanism for sequence-to-sequence learning, aiming to achieve clearer alignment between input regions and output tokens. The key innovation is a 'sharpener' module with a localizer and encoder to produce more accurate target-specific representations. Experiments on handwritten digit and scene text recognition demonstrate consistent improvements over soft and hard attention baselines, along with enhanced interpretability.","[""How does the sharpener module's performance vary with different localizer and encoder architectures?"", 'Could the sharp attention mechanism be extended to other sequence-to-sequence tasks beyond text recognition?', 'What is the impact of the sharpener module on inference time and model complexity?']","['Evaluation limited to text recognition tasks', 'Modest performance gains on real-world datasets', 'Potential increased computational cost not thoroughly analyzed']",False,3,3,3,7,4,"['Novel and well-motivated approach to improve attention alignment and interpretability', 'Solid theoretical foundation derived from generalizing hard attention', 'Comprehensive experiments on synthetic and real-world datasets', 'Consistent performance improvements over baseline attention methods', 'Allows incorporation of external supervision to further boost performance', 'Improved interpretability through visualizations and entropy analysis']","['Limited comparison to recent attention variants beyond soft/hard attention', 'Performance gains on real-world scene text recognition tasks are modest', 'Evaluation is limited to text recognition tasks, raising questions about generalizability', 'Computational overhead of the sharpener module is not thoroughly analyzed']",4,3,3.0,3,Accept
2yITmG7YIFT,"This paper introduces HD-cos networks, which employ cosine activation functions and Hadamard-Diagonal structured weight matrices to create efficient neural networks for secure multi-party computation (MPC). Empirical evaluations on multiple datasets demonstrate competitive performance compared to standard architectures while significantly reducing computational costs in MPC settings.","['How does the proposed approach compare to other recent MPC-friendly neural network architectures?', 'Can the Hadamard-Diagonal structure be extended to more complex architectures like CNNs or Transformers?', 'What are the key trade-offs between model capacity and efficiency in the proposed approach?', 'What are the practical implications of this work for real-world privacy-preserving machine learning applications?']","['Evaluation is limited to feedforward networks and relatively simple datasets', 'Potential challenges in scaling to more complex problems are not thoroughly discussed', 'Trade-off between MPC efficiency and model performance not fully explored']",False,3,3,4,7,4,"['Novel combination of cosine activation and Hadamard-Diagonal matrices for MPC efficiency', 'Theoretical motivation linking cosine activation to kernel methods', 'Comprehensive empirical evaluation across multiple datasets', 'Addresses an important problem in privacy-preserving machine learning', 'Thorough ablation studies comparing different activation functions and matrix structures']","['Limited theoretical analysis, especially for Hadamard-Diagonal matrices', 'Experiments primarily focus on simple feedforward networks', 'Lack of direct comparison to state-of-the-art MPC neural network methods', 'Some results show minor performance degradation compared to non-MPC baselines']",3,3,3.0,3,Accept
kUGYDTJUcuc,"This paper proposes a unified visual attention model that combines top-down and bottom-up mechanisms for visual recognition tasks. The top-down component uses Q-learning to select regions of interest in an image pyramid, while the bottom-up component is based on a recurrent attention model (RAM) with additional entropy and context constraints. The method is evaluated on MNIST, CIFAR-10, and SVHN datasets, showing improvements over baselines.","['How does each component of the model (top-down, bottom-up, constraints) contribute to the overall performance?', 'Can you provide more theoretical analysis on the benefits of combining top-down and bottom-up attention?', 'How does the method compare to more recent state-of-the-art attention models in computer vision?', 'What is the computational complexity of the approach compared to baselines?', 'How sensitive is the method to key hyperparameters such as number of glimpses and patch sizes?']","['The approach may not scale well to very large images or complex scenes', 'Evaluation is limited to classification tasks, not exploring other vision problems', 'Computational complexity and efficiency are not thoroughly analyzed', 'The method relies on reinforcement learning, which can be unstable to train']",False,3,2,3,5,4,"['Novel combination of top-down and bottom-up attention mechanisms', 'Use of Q-learning for region selection is innovative', 'Addition of entropy and context constraints shows benefits', 'Improved performance over RAM and CNN baselines on multiple datasets', 'Some evidence of robustness to adversarial attacks', 'Comprehensive experimental evaluation on various datasets']","['Writing and presentation clarity could be improved in some areas', 'Limited comparison to state-of-the-art models on benchmark datasets', 'Lack of comprehensive ablation studies', 'Insufficient theoretical justification for the approach', 'Robustness claims need more rigorous evaluation']",3,3,3.0,3,Accept
F2r3wYar3Py,"This paper presents a novel approach to few-shot and one-shot learning inspired by human cognition, specifically focusing on image classification tasks. The authors introduce a 'distortable canvas' model to compute topological similarity between images, mimicking human-like transformation processes. Without pre-training, the model achieves competitive results on MNIST, EMNIST, and Omniglot benchmarks in extreme low-data regimes, outperforming more complex neural network approaches. The paper also demonstrates the model's potential for unsupervised learning and generating interpretable archetypes.","['How does the computational complexity scale with image size and complexity?', 'How might the approach be extended to handle more complex real-world images?', 'Can you provide more rigorous comparisons to state-of-the-art few-shot learning methods?', 'Is there any theoretical analysis or guarantees for the method?', 'How does the model perform on out-of-distribution samples?']","['Current limitation to abstract/symbolic images', 'Performance degrades with larger training sets', 'Scalability to larger, more complex datasets is unclear', 'Potential computational complexity for larger images', 'Lack of theoretical guarantees']",False,3,3,3,7,4,"['Novel and well-motivated approach inspired by human cognition and nativism', 'Strong performance on benchmarks in extreme low-data regimes without pre-training', 'Interpretable model with visualizable transformation flows', 'Potential for unsupervised learning and archetype generation', 'Addresses an important challenge in machine learning: learning from very limited data']","['Currently limited to abstract/symbolic images', 'Performance degrades with more training data compared to traditional methods', 'Scalability to more complex real-world tasks is unclear', 'Limited comparisons to some state-of-the-art few-shot learning methods', 'Computational efficiency and complexity not thoroughly analyzed']",4,3,3.0,3,Accept
36SHWj0Gp1,"This paper introduces GenTAL, a novel unsupervised Transformer-based approach for binary code similarity detection. The method combines a Transformer architecture with a skip-gram style reconstruction loss to create compact embeddings of assembly code instructions while preserving syntax. Experiments demonstrate superior performance compared to state-of-the-art baselines across multiple code similarity tasks, including cross-compiler, cross-optimization, and obfuscation scenarios.","['How well does the method generalize to other CPU architectures beyond x86?', 'What are the computational requirements for training and inference?', 'How does the approach handle very large constants or other out-of-vocabulary tokens?', 'Can you provide ablation studies to isolate the impact of key components?', 'What are the potential dual-use concerns or ethical implications of this work?', 'How robust is the model to different types of obfuscation techniques?']","['Only evaluated on x86 architecture, generalization to others unclear', 'May struggle with very large constants or other rare tokens', 'Performance degrades significantly on heavily obfuscated code', 'Potential for misuse in malware development not thoroughly discussed', 'Computational requirements and scalability to very large binaries not analyzed', 'Lack of comparison with some recent deep learning approaches in binary similarity detection']",False,3,3,3,8,4,"['Novel unsupervised approach combining Transformer with skip-gram for assembly code', 'Strong empirical results outperforming state-of-the-art baselines across multiple tasks', 'Addresses an important problem in cybersecurity and reverse engineering', 'Comprehensive experiments using a new dataset of verified block-level mappings', 'Potential for better generalization compared to supervised methods']","['Limited discussion of computational requirements and efficiency', 'Lack of ablation studies to isolate impact of different model components', 'Evaluation focused only on x86 architecture, unclear generalization to others', 'Limited discussion of limitations and potential negative societal impacts', 'Some results lack error bars or statistical significance testing']",4,3,3.0,4,Accept
n6Bc3YElODq,"This paper introduces Model-Based Opponent Modeling (MBOM), a novel approach for multi-agent reinforcement learning. MBOM uses an environment model to simulate opponent behavior through recursive imagination and combines multiple levels of opponent models using Bayesian mixing. Experiments demonstrate MBOM's superior performance against various opponent types in both competitive and cooperative tasks.","['How does the computational complexity scale with the number of agents and environment complexity?', 'Are there any theoretical guarantees on the performance or convergence of MBOM?', 'How would MBOM perform in more complex or partially observable environments?', 'Can you provide more details on the potential negative societal impacts of this approach?']","['May not scale well to many agents or complex environments', 'Relies on having access to an accurate environment model', 'Assumes access to opponent rewards in general-sum games', 'Increased computational cost compared to model-free methods', 'Potential for misuse in adversarial scenarios or privacy-sensitive applications']",False,3,3,3,7,4,"['Novel combination of model-based RL and opponent modeling', 'Flexible approach handling different opponent types without assumptions', 'Strong empirical results across multiple environments and opponent types', 'Applicability to both competitive and cooperative settings', 'Thorough empirical evaluation with ablation studies']","['Limited theoretical analysis or guarantees', 'Potential scalability issues with many agents or complex environments', 'High computational complexity of recursive rollouts', 'Limited discussion of limitations and potential negative impacts', 'Evaluation restricted to relatively simple environments']",4,3,3.0,3,Accept
Ti2i204vZON,"This paper analyzes representation learning methods for pixel-based reinforcement learning, focusing on settings with background distractors. It proposes a simple baseline using reward and transition prediction, analyzes failure modes of existing methods, and argues for finer benchmark categorization. Extensive empirical comparisons are provided across DMC and Atari benchmarks.","['How would the proposed benchmark categorization impact future research in pixel-based RL?', 'Can you provide stronger theoretical justification for the proposed baseline method?', 'How do the insights from this study apply to more complex RL tasks with sparse rewards?', 'What are the implications of your findings for real-world robotics applications?']","['Results limited to simulated environments (DMC and Atari)', 'Limited discussion of potential negative societal impacts', 'Computational requirements not thoroughly analyzed', 'May not generalize well to more complex or real-world tasks', 'Focus on dense reward tasks may limit applicability to sparse reward settings']",False,3,3,3,6,4,"['Thorough empirical analysis across multiple methods and environments', 'Insightful discussion on limitations of existing methods with distractors', 'Proposal of a simple yet competitive baseline approach', 'Well-justified call for more nuanced benchmark categorization', 'Extensive ablation studies and comparisons']","['Baseline method, while effective, is not entirely novel', 'Limited theoretical justification or analysis', 'Experiments mostly limited to simulated environments', 'Some reviewers consider the contribution incremental']",3,3,3.0,3,Accept
lvM693mon8q,"This paper introduces Compressed Vertical Federated Learning (C-VFL), a novel algorithm for communication-efficient training on vertically partitioned data. C-VFL combines embedding compression, local updates, and an arbitrary server model to significantly reduce communication costs. The authors provide thorough theoretical convergence analysis, examine common compression techniques, and demonstrate empirically that C-VFL can reduce communication by over 90% with minimal accuracy loss.","['How does C-VFL perform on larger, more complex datasets such as ImageNet or language models?', 'Can you provide guidelines for selecting the optimal compression technique for a given VFL scenario?', 'What modifications to C-VFL would be necessary to provide stronger privacy guarantees?', 'How does the performance of C-VFL change as the number of parties increases to hundreds or thousands?']","['Experiments are limited to two moderate-sized datasets, which may not fully represent real-world VFL scenarios', 'The assumption of bounded gradients may not hold for all deep learning models, potentially limiting applicability', 'While privacy is discussed, stronger guarantees or integration with differential privacy techniques could enhance the method', 'The scalability analysis focuses on a relatively small number of parties, leaving questions about performance in large-scale federated settings']",False,4,3,4,8,4,"['Novel algorithm (C-VFL) addressing an important problem in federated learning', 'Rigorous theoretical analysis with convergence guarantees', 'Comprehensive analysis of common compression techniques in the VFL setting', 'Significant empirical results showing over 90% communication reduction with minimal accuracy loss', 'Clear writing and organization']","['Limited experimental evaluation (only two datasets)', 'Some assumptions in theoretical analysis (e.g., bounded gradients) may be restrictive', 'Privacy analysis is somewhat limited', 'Scalability to larger datasets and more parties not thoroughly explored']",4,4,4.0,4,Accept
f4c4JtbHJ7B,"This paper introduces Pixab-CAM, a novel class activation mapping method that employs pixel-wise weights instead of channel-wise weights, excludes the activation tensor, and avoids gradients. The authors claim this approach addresses limitations of existing CAM methods. Extensive experiments demonstrate improved performance on various datasets and metrics, including newly proposed evaluation techniques using adversarial examples.","['Can you provide more theoretical justification for the pixel-wise weighting approach?', 'How does the computational efficiency of Pixab-CAM compare to other methods?', 'How well does the method generalize to tasks beyond image classification?', 'How do you validate that the proposed adversarial patch metrics are meaningful?', 'What are potential applications of this method outside of computer vision?']","['Evaluation limited to image classification tasks', 'Potential increased computational cost not thoroughly analyzed', 'Lack of user studies to validate explanation quality', 'Proposed metrics may not fully capture explanation quality', 'Limited comparison with non-CAM-based explanation methods']",False,3,3,3,7,4,"['Novel approach using pixel-wise weights instead of channel-wise weights', 'Addresses specific limitations of previous CAM methods', 'Consistent improvements over baseline methods across multiple datasets and metrics', 'Introduction of new evaluation techniques using adversarial examples', 'Comprehensive experimental evaluation']","['Limited theoretical justification for the pixel-wise ablation approach', 'Improvements over existing methods are often small in magnitude', 'Evaluation primarily focused on image classification tasks', 'Limited analysis of computational efficiency', 'Proposed adversarial metrics need more validation']",4,3,3.0,4,Accept
rGg-Qcyplgq,"This paper introduces Perturbed Quantile Regression (PQR), a novel exploration method for distributional reinforcement learning. PQR randomizes the risk criterion used for action selection, departing from fixed optimistic criteria. The authors provide theoretical analysis with convergence guarantees for their perturbed distributional Bellman optimality operator, and demonstrate empirical improvements over baselines like QR-DQN on several environments, including Atari games.","['How does PQR compare to more recent state-of-the-art RL exploration methods beyond distributional RL?', 'Can you provide more extensive ablation studies on the impact of different perturbation schemes and hyperparameters?', 'Have you considered running longer experiments (e.g., 200M frames) on Atari to compare more directly with some baselines?', 'How well does the theoretical analysis extend to the function approximation setting?']","['Empirical evaluation is somewhat limited in scope and duration', 'Computational complexity of the method is not thoroughly discussed', 'Applicability to continuous action spaces is not explored', 'Potential negative impacts of improved RL algorithms are not addressed', 'Broader impact of this work on real-world applications is not fully explored']",False,3,3,3,7,4,"['Novel theoretical framework (perturbed distributional Bellman operator) with convergence guarantees', 'Addresses an important issue in distributional RL (balancing exploration and exploitation)', 'Strong empirical results showing consistent improvements over baselines', 'Well-motivated problem and solution with clear intuition']","['Limited comparison to more recent state-of-the-art RL exploration methods', 'Empirical evaluation could be more extensive (e.g., longer training on Atari)', 'More ablation studies on hyperparameters and perturbation schemes would be beneficial', 'Theoretical results may not fully translate to function approximation settings']",4,3,3.0,4,Accept
bE239PSGIGZ,"This paper introduces Speech Synthesising based Attack (SSA), a novel method for generating audio adversarial examples to fool automatic speech recognition (ASR) systems. SSA synthesizes adversarial audio from scratch using a conditional variational autoencoder (CVAE), optimizing the latent space to generate audio that fools an ASR model while maintaining semantic content. Experiments demonstrate SSA outperforms existing audio attack methods in fooling a state-of-the-art ASR model (DeepSpeech).","['How well does the attack transfer to other state-of-the-art ASR models beyond DeepSpeech?', 'What are potential defenses against this type of audio-independent attack?', 'How can the audio quality of synthesized adversarial examples be improved while maintaining attack success?', 'What are the real-world implications and potential misuses of this technology?']","['Attack is currently limited to white-box scenarios with one specific ASR model', 'Synthesized adversarial audio has somewhat degraded quality compared to natural speech', 'Potential for misuse in generating malicious audio content', 'Ethical concerns around adversarial attacks on speech systems not fully explored']",The paper raises ethical concerns due to the potential misuse of the technology for generating malicious audio content and the limited discussion of these implications.,3,3,3,7,4,"['Highly original approach of synthesizing adversarial audio from scratch', 'Strong empirical results outperforming baselines across multiple datasets', 'Clever use of CVAE for controllable audio synthesis', 'Thorough experiments and analysis, including ablation studies', 'Potential to expose new vulnerabilities in ASR systems']","['Evaluation limited to only one ASR model (DeepSpeech)', 'Synthesized adversarial audio quality is somewhat degraded compared to natural speech', 'Limited discussion of ethical implications and potential misuse', 'Lack of analysis on transferability to other ASR models and potential defenses']",4,3,3.0,3,Accept
SHbhHHfePhP,"This paper introduces Graph Mechanics Networks (GMN), a novel approach for modeling constrained dynamical systems of interacting objects. The key contributions are: (1) using generalized coordinates and forward kinematics to implicitly encode geometric constraints, (2) developing equivariant neural network layers with theoretical guarantees on expressivity, and (3) combining these with graph neural networks. GMN demonstrates significant improvements over state-of-the-art baselines on both simulated and real-world datasets.","['How well does the method generalize to very complex systems with many interacting objects?', 'Are there ways to automatically learn the forward kinematics rather than requiring manual specification?', 'How sensitive is the performance to the choice of generalized coordinates?', 'Could the approach be extended to handle other types of constraints beyond distance constraints?', 'What are potential applications of this method in other scientific or engineering domains?']","['The method requires some domain expertise to set up for new systems', 'Scalability to very large systems is not thoroughly evaluated', 'May not fully capture all constraints in very complex systems']",False,4,3,4,8,4,"['Novel approach effectively handling geometric constraints in dynamical systems', 'Theoretically grounded method with proofs on expressivity', 'Strong empirical results on both simulated and real-world datasets', 'Thorough ablation studies and analyses', 'Good balance of theory and practice']","['Requires some domain knowledge to set up forward kinematics for different object types', 'Performance gains are more modest on some real-world datasets compared to simulated data', 'Limited evaluation on very complex or highly diverse real-world systems']",4,4,4.0,4,Accept
xaTensJtCP5,"This paper introduces 'Ab Initio' objective functions for optimizing neural MCMC proposal distributions. The approach constructs objectives by combining simpler functions with coefficients fit to reproduce known optimal results. The authors demonstrate the advantages of their method over existing approaches across various MCMC tasks, especially for expressive neural proposals.","['How well does the Ab Initio approach scale to higher-dimensional problems?', 'Can you provide more theoretical justification for the specific Ab Initio objective used?', 'How does the method compare to recent neural MCMC approaches like NeuTra-HMC or L2HMC?', 'What is the computational cost of optimizing the Ab Initio objective compared to existing approaches?']","['The specific Ab Initio objective proposed may not be universally optimal', 'Empirical evaluation is limited to relatively low-dimensional problems', 'Potential challenges in scaling to higher dimensions', 'Theoretical guarantees on optimality of resulting proposals not provided']",False,3,3,3,7,4,"['Novel approach to constructing MCMC objectives with theoretical motivation', 'Demonstrates clear advantages over existing objectives for expressive proposals', 'Thorough experimental evaluation across multiple MCMC tasks and datasets', 'Well-written and clearly motivated']","['Limited theoretical justification for the specific Ab Initio objective form', 'Experiments mostly focused on relatively low-dimensional problems', 'Limited comparison to state-of-the-art neural MCMC methods', 'Computational cost and scalability not thoroughly analyzed']",3,3,4.0,3,Accept
TXqemS7XEH,"This paper introduces 'Pseudo-to-Real', a novel training strategy for efficiently training extremely large language models with limited resources. The authors demonstrate this by training a 10 trillion parameter model in 10 days using 512 GPUs, along with a granular CPU offloading technique.","['How does the final 10T model compare to smaller models on standard language modeling benchmarks and downstream NLP tasks?', 'Can you provide more theoretical justification for why the Pseudo-to-Real approach works?', 'What is the environmental impact in terms of carbon emissions compared to training other large models?', 'How does this approach compare to other efficiency techniques like model distillation or progressive layer dropping?']","['Limited evaluation of downstream performance and generalization', 'Potential negative environmental impact of training very large models', 'Possible biases and fairness issues with large language models not thoroughly addressed', 'High computational requirements may limit accessibility for many researchers']",True,3,3,4,6,4,"['Novel and efficient training strategy for extreme-scale language models', 'Impressive demonstration of training a 10 trillion parameter model with limited resources', 'Potential for significant impact on large-scale model training', 'Granular CPU offloading technique for better memory utilization', 'Potential for greener AI by reducing resource requirements for large model training']","['Limited evaluation of downstream task performance, especially for the 10T model', 'Insufficient theoretical analysis and justification for the approach', 'Lack of thorough discussion on potential negative societal impacts and ethical considerations', 'Limited comparison to other efficient training methods']",4,3,3.0,4,Accept
iaxWbVx-CG_,"This paper introduces Hierarchical Cross Contrastive Learning (HCCL), a novel self-supervised learning method for visual representation learning. HCCL employs a hierarchical projection head and applies contrastive learning across different levels and views. The method demonstrates competitive performance on standard benchmarks and can be integrated into various SSL frameworks.","['Can you provide more theoretical justification or intuition for why cross-level contrastive learning improves performance?', 'How does HCCL compare to very recent state-of-the-art SSL methods like DINO or MoCo v3?', 'What is the computational overhead of HCCL compared to methods like SimCLR or SimSiam?', 'How sensitive is the method to the number of levels in the hierarchical projection head?', 'Are there potential applications of HCCL beyond computer vision tasks?']","['Experiments are mainly focused on vision tasks, unclear if it generalizes to other domains', 'Increased training complexity and compute requirements', 'Potential negative impacts of improved self-supervised learning are not discussed', 'May require more careful tuning of hyperparameters']",False,3,3,3,7,4,"['Novel approach combining hierarchical projection and cross-level contrastive learning', 'Comprehensive experiments and ablation studies on multiple tasks and datasets', 'Theoretical analysis of convergence provided', 'Competitive results without using negative samples or momentum encoders', 'Can be applied to different SSL frameworks like SimSiam and BYOL', 'Faster convergence compared to some existing methods', 'Thorough ablation studies demonstrating the importance of each component']","['Performance gains are relatively modest in some cases', 'Added complexity compared to simpler frameworks', 'Limited analysis of why cross-level learning is beneficial', 'Lack of comparison to some recent top-performing SSL methods', 'Limited analysis of computational overhead']",4,3,3.0,3,Accept
EMLJ_mTz_z,"This paper introduces a novel framework for predicting neural network performance by modeling training dynamics as evolving graphs. Key contributions include a new compact graph representation for convolutional layers, a method for extracting temporal graph signatures from early training epochs, and empirical results demonstrating high accuracy in predicting final NN performance after only 5-10 epochs across multiple architectures and datasets.","['How does the proposed method compare to other early stopping or performance prediction techniques in terms of accuracy and efficiency?', 'Could you provide more theoretical insight into what aspects of NN training the graph signatures are capturing?', 'How well does the approach generalize to more modern architectures like Transformers or to non-image domains like NLP?', 'How sensitive is the performance to the choice of graph features and summary statistics used?']","['Approach is primarily tested on CNNs and fully-connected networks for image classification tasks', 'Generalizability to other architectures and domains is unclear', 'Computational overhead of graph generation/analysis not fully quantified', 'Limited theoretical grounding for why the method works']",False,3,3,3,7,4,"['Novel graph-based approach to modeling NN training dynamics', ""New efficient 'rolled' graph representation for convolutional layers"", 'Strong empirical results showing accurate early prediction of NN performance', 'Evaluation on multiple standard architectures and datasets', 'Potential for significant practical impact on efficient architecture search', 'Contributes to interpretability of neural networks by providing insights into training dynamics']","['Limited theoretical analysis of why the approach works', 'Evaluation focused primarily on image classification tasks', 'Limited comparison to other early stopping / performance prediction methods', 'Some claims about efficiency could be better quantified']",4,3,3.0,4,Accept
kO-wQWwqnO,"This paper introduces L2BGAN, a GAN-based method for enhancing low-light images without paired supervision. The approach combines geometric and illumination consistency constraints, multiple discriminators for color, texture, and edges, and contextual loss. Extensive experiments demonstrate competitive results on benchmark datasets and improved performance on downstream tasks such as face detection in low-light conditions.","['How does the geometric consistency constraint specifically contribute to the enhancement process?', 'What is the computational cost of your method compared to existing approaches?', 'How does the method handle extreme low-light conditions or highly noisy images?', 'Can you elaborate on the choice of using multiple discriminators for color, texture, and edges?']","['Performance may degrade in extreme low-light conditions or highly noisy images', 'Computational complexity and efficiency not thoroughly analyzed', 'Potential for introducing artifacts in enhanced images, especially in challenging scenarios', 'Possible amplification of biases in downstream tasks not addressed']",False,3,3,3,7,4,"['Novel combination of techniques for unpaired low-light image enhancement', 'Comprehensive experiments on multiple benchmark datasets', 'Ablation studies justifying design choices', 'Demonstration of improved performance on real-world applications (e.g., face detection)', 'Competitive results compared to state-of-the-art methods']","['Complex method with many components', 'Limited analysis of failure cases and limitations', 'Lack of discussion on computational costs and efficiency', 'Insufficient consideration of ethical implications and potential negative societal impacts']",4,3,3.0,3,Accept
fJIrkNKGBNI,"This paper introduces PP-GNN, a graph neural network model that employs piecewise polynomial filters to adapt to different parts of the graph spectrum. The approach learns multiple adaptive polynomial filters acting on different subsets of eigenvalues, theoretically shown to better approximate optimal filters compared to single polynomials. Empirical results demonstrate state-of-the-art performance on several node classification benchmarks for both homophilic and heterophilic graphs.","['How does the method scale to very large graphs with millions of nodes?', 'Can you provide more detailed analysis of the learned filters on different datasets?', 'How sensitive is the performance to the number of polynomial pieces and eigenvalues/eigenvectors used?', 'Have you compared to very recent GNN models like GraphGPS or GLEM?', 'How does the training time compare to simpler GNN models as the graph size increases?']","['Computational cost and eigendecomposition may limit applicability to very large graphs', 'Added hyperparameters may make the method more difficult to tune', 'Not clear how well the method would generalize beyond node classification tasks', 'Theoretical results make some simplifying assumptions']",False,3,2,3,7,4,"['Novel piecewise polynomial approach with strong theoretical motivation', 'Comprehensive empirical evaluation showing consistent improvements over SOTA methods', 'Theoretical analysis providing justification for the approach', 'Adaptability to both homophilic and heterophilic graphs', 'Extensive experiments and ablation studies']","['Higher computational cost and complexity compared to simpler GNN models', 'Scalability to very large graphs (millions of nodes) not clearly demonstrated', 'Some baseline comparisons use older models rather than latest state-of-the-art', 'Potential sensitivity to hyperparameters and number of eigenvalues/eigenvectors used']",3,3,2.0,3,Accept
