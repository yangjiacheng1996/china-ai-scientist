## MULTI-AGENT CONSTRAINED POLICY OPTIMISATION

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Developing reinforcement learning algorithms that satisfy safety constraints is becoming increasingly important in real-world applications. In multi-agent reinforcement learning (MARL) settings, policy optimisation with safety awareness
is particularly challenging because each individual agent has to not only meet its
own safety constraints, but also consider those of others so that their joint behaviour can be guaranteed safe. Despite its importance, the problem of safe multiagent learning has not been rigorously studied; very few solutions have been proposed, nor a sharable testing environment or benchmarks. To fill these gaps, in
this work, we formulate the safe MARL problem as a constrained Markov game
and solve it with policy optimisation methods. Our solutions—Multi-Agent Con_strained Policy Optimisation (MACPO) and MAPPO-Lagrangian—leverage the_
theories from both constrained policy optimisation and multi-agent trust region
learning. Crucially, our methods enjoy theoretical guarantees of both monotonic
improvement in reward and satisfaction of safety constraints at every iteration. To
examine the effectiveness of our methods, we develop the benchmark suite of Safe
_Multi-Agent MuJoCo that involves a variety of MARL baselines. Experimental_
results justify that MACPO/MAPPO-Lagrangian can consistently satisfy safety
constraints, meanwhile achieving comparable performance to strong baselines.[1]

1 INTRODUCTION

In recent years, reinforcement learning (RL) techniques have achieved remarkable successes on a
variety of complex tasks (Silver et al., 2016; 2017; Vinyals et al., 2019). Powered by deep neural
networks, deep RL enables learning sophisticated behaviours. On the other hand, deploying neural networks turns the optimisation procedure from policy space to parameter space; this enables
gradient-based methods to be applied (Sutton et al., 1999; Lillicrap et al., 2015; Schulman et al.,
2017). For policy gradient methods, at every iteration, the parameters of a policy network are updated in the direction of the gradient that maximises return.

However, policies that are purely optimised for reward maximisation are rarely applicable to realworld problems. In many applications, an agent is often required not to visit certain states or take
certain actions, which are thought of as “unsafe” either for itself or for other elements in the background (Moldovan & Abbeel, 2012; Achiam et al., 2017). For instance, a robot carrying materials
in a warehouse should not damage its parts while delivering an item to a shelf, nor should a selfdriving car cross on the red light while rushing towards its destination (Shalev-Shwartz et al., 2016).
To tackle these issues, Safe RL (Moldovan & Abbeel, 2012; Garcıa & Fern´andez, 2015) is proposed,
aiming to develop algorithms that learn policies that satisfy safety constraints. Despite the additional
requirement of safety on solutions, algorithms with convergence guarantees have been proposed (Xu
et al., 2021; Wei et al., 2021).

Developing safe policies for multi-agent systems is a challenging task. Part of the difficulty comes
from solving multi-agent reinforcement learning (MARL) problems itself (Deng et al., 2021); more
importantly, tackling safety in MARL is hard because each individual agent has to not only consider
its own safety constraints, which already may conflict its reward maximisation, but also consider the
safety constraints of others so that their joint behaviour is guaranteed to be safe. As a result, there are
very few solutions that offer effective learning algorithms for safe MARL problems. In fact, many
of the existing methods focus on learning to cooperate (Foerster et al., 2018; Rashid et al., 2018;

1Paper homepage including Videos and Code: https://sites.google.com/view/macpo


-----

Yang & Wang, 2020). However, they often require certain structures on the solution; for example,
Rashid et al. (2018) and Yang et al. (2020) adopt greedy maximisation on the local component of
a monotonic joint value function, and Foerster et al. (2018) estimates the policy gradient based on
the counterfactual value from a joint critic. Therefore, it is unclear how to directly incorporate
safety constraints into these solution frameworks. Consequently, developing agents’ collaborations
towards reward maximisation under safety constraints remains an unsolved problem.

The goal of this paper is to increase practicality of MARL algorithms through endowing them with
safety awareness. For this purpose, we introduce a general framework to formulate safe MARL
problems, and solve them through multi-agent policy optimisation methods. Our solutions leverage
techniques from both constrained policy optimisation (Achiam et al., 2017) and multi-agent trust
region learning (Kuba et al., 2021a). The resulting algorithm attains properties of both monotonic
improvement guarantee and constraints satisfaction guarantee at every iteration during training. To
execute the optimisation objectives, we introduce two practical deep MARL algorithms: MACPO
and MAPPO-Lagrangian. As a side contribution, we also develop the first safe MARL benchmark within the MuJoCo environment, which include a variety of MARL baseline algorithms. We
evaluate MACPO and MAPPO-Lagrangian on a series of tasks, and results clearly confirm the effectiveness of our solutions both in terms of constraints satisfaction and reward maximisation. To
our best knowledge, MACPO and MAPPO-Lagrangian are the first safety-aware model-free MARL
algorithms that work effectively in the challenging MuJoCo tasks with safety constraints.

2 RELATED WORK

Considering safety in the development of AI is a long-standing topic (Amodei et al., 2016). When
it comes to safe reinforcement learning (Garcıa & Fern´andez, 2015), a commonly used framework
is Constrained Markov Decision Processes (CMDPs) (Altman, 1999). In a CMDP, at every step,
in addition to the reward, the environment emits costs associated with certain constraints. As a
result, the learning agent must try to satisfy those constraints while maximising the total reward.
In general, the cost from the environment can be thought of as a measure of safety. Under the
framework of CMDP, a safe policy is the one that explores the environment safely by keeping the
total costs under certain thresholds. To tackle the learning problem in CMDPs, Achiam et al. (2017)
introduced Constrained Policy Optimisation (CPO), which updates agent’s policy under the trust
region constraint (Schulman et al., 2015) to maximise surrogate return while obeying surrogate
cost constraints. However, solving a constrained optimisation at every iteration of CPO can be
cumbersome for implementation. An alternative solution is to apply primal-dual methods, giving
rise to methods like TRPO-Lagrangian and PPO-Lagrangian (Ray et al., 2019). Although these
methods achieve impressive performance in terms of safety, the performance in terms of reward is
poor (Ray et al., 2019). Another class of algorithms that solves CMDPs is by Chow et al. (2018;
2019); these algorithms leverage the theoretical property of the Lyapunov functions and propose
safe value iteration and policy gradient procedures. In contrast to CPO, Chow et al. (2018; 2019)
can work with off-policy methods; they also can be trained end-to-end with no need for line search.

Safe multi-agent learning is an emerging research domain. Despite its importance (Shalev-Shwartz
et al., 2016), there are few solutions that work with MARL in a model-free setting. The majority
of methods are designed for robotics learning. For example, the technique of barrier certificates
(Borrmann et al., 2015; Ames et al., 2016; Qin et al., 2020) or model predictive shielding (Zhang
et al., 2019) from control theory is used to model safety. These methods, however, are specifically
derived for robotics applications; they either are supervised learning based approaches, or require
specific assumptions on the state space and environment dynamics. Moreover, due to the lack of a
benchmark suite for safe MARL algorithms, the generalisation ability of those methods is unclear.
The most related work to ours is Safe Dec-PG (Lu et al., 2021) where they used the primal-dual
framework to find the saddle point between maximising reward and minimising cost. In particular,
they proposed a decentralised policy descent-ascent method through a consensus network. However, reaching a consensus equivalently imposes an extra constraint of parameter sharing among
neighbouring agents, which could yield suboptimal solutions (Kuba et al., 2021a). Furthermore,
multi-agent policy gradient methods can suffer from high variance (Kuba et al., 2021b). In contrast,
our methods employ trust region optimisation and do not assume any parameter sharing.

HATRPO (Kuba et al., 2021a) introduced the first multi-agent trust region method that enjoys
theoretically-justified monotonic improvement guarantee. Its key idea is to make agents follow a


-----

sequential policy update scheme so that the expected joint advantage will always be positive, thus
increasing reward. In this work, we show how to further develop this theory and derive a protocol
which, in addition to the monotonic improvement, also guarantees to satisfy the safety constraint at
every iteration during learning. The resulting algorithm (Algorithm 1) successfully attains theoretical guarantees of both monotonic improvement in reward and satisfaction of safety constraints.

3 PROBLEM FORMULATION: CONSTRAINED MARKOV GAME

We formulate the safe MARL problem as a constrained Markov game ⟨N _, S, A, p, 휌[0], 훾, 푅, C, c⟩._
Here, = 1, . . ., 푛 is the set of agents, is the state space, = _푖=1_
the agents’ action spaces, known as the joint action space, N { } S p : S × A A × S → ℝ[A][푖]is the probabilistic[is the product of]
transition function, 휌[0] is the initial state distribution, 훾 0, 1 is the discount factor, 푅 : ℝ
∈[ ) [Î][푛] S×A →
is the joint reward function, C = {퐶[푖]푗 [}][푖]1[∈N]≤ _푗_ ≤푚[푖] [is the set of sets of cost functions (every agent][ 푖] [has][ 푚][푖]

cost functions) of the form 퐶[푖]푗 [:][ S × A][푖] [→] [ℝ][, and finally the set of corresponding cost-constraining]
values is given by c = _푐[푖]푗_ [}][푖]1[∈N]푗 _푚[푖]_ [. At time step][ 푡][, the agents are in a state s][푡] [, and every agent][ 푖] [takes]
{ ≤ ≤

an action a[푖]푡 [according to its policy][ 휋][푖] [(][a][푖] [|][s][푡] [)][. Together with other agents’ actions, it gives a joint]
action a푡 = (a[1]푡 _[, . . .,][ a]푡[푛][)][ and the joint policy][ π][(][a][|][s][)][ =][ Î]푖[푛]=1_ _[휋][푖]_ [(][a][푖] [|][s][)][. The agents receive the reward]
_푅(s푡_ _, a푡_ ), meanwhile each agent 푖 pays the costs 퐶[푖]푗 [(][s][푡] _[,][ a][푖]푡_ [)][,][ ∀] _[푗]_ [=][ 1][, . . ., 푚][푖][. The environment then]
all agents share the same reward function, aiming to maximise the expected total reward oftransits to a new state s푡+1 ∼ p(·|s푡 _, a푡_ ). In this paper, we consider a fully-cooperative setting where

[∞]
_퐽_ (π) ≜ 피s0∼휌0,a0:∞∼π,s1:∞∼p _훾[푡]_ _푅(s푡_ _, a푡_ ) _,_

_푡=0_

h Õ i

meanwhile trying to satisfy every agent 푖’s safety constraints, written as

[∞]
_퐽[푖]푗_ [(][π][)][ ≜] [피]s0∼휌[0],a0:∞∼π,s1:∞∼p _훾[푡]퐶[푖]푗_ [(][s][푡] _[,][ a][푖]푡_ [)] ≤ _푐[푖]푗_ _[,]_ ∀ _푗_ = 1, . . ., 푚[푖]. (1)

_푡=0_

h Õ i

We define the state-action value and the state-value functions in terms of reward as

[∞]
_푄π_ _푠, a_ ≜ 피s1: p,a1: **_π_** _훾[푡]_ _푅_ s푡 _, a푡_ s0 = 푠, a0 = a _,_ and _푉π_ _푠_ ≜ 피a **_π_** _푄π_ _푠, a_ _._
( ) ∞∼ ∞∼ ( ) ( ) ∼ ( )

_푡=0_

h Õ i  

The joint policies π that satisfy the Inequality (1) are referred to as feasible. Notably, in the above
formulation, although the action a[푖]푡 [of agent][ 푖] [does not directly influence the costs][ {][퐶] _[푘]푗_ [(][s][푡] _[,][ a]푡[푘]_ [)}][푚]푗=[푘]1
of other agents 푘 ≠ _푖, the action a[푖]푡_ [will implicitly influence their total costs due to the dependence]
on the next state s푡 1 [2]. For the 푗 [th] cost function of agent 푖, we define the 푗 [th] state-action cost value
+
function and the state cost value function as

[∞]
_푄[푖]푗,π_ [(][푠, 푎][푖][)][ ≜] [피]a[−][푖]∼[π][−][푖] _,s1:∞∼p,a1:∞∼[π]_ _훾[푡]퐶[푖]푗_ [(][s][푡] _[,][ a][푖]푡_ [)] s0 = 푠, a[푖]0 [=][ 푎][푖][i],

_푡=0_

h Õ

[∞]
and, 푉 _푗,[푖]_ **_π_** [(][푠][)][ ≜] [피][a][∼][π][,][s]1: 1: _훾[푡]퐶[푖]푗_ [(][s][푡] _[,][ a][푖]푡_ [)] s0 = 푠 _,_ respectively.
∞[∼][p][,][a] ∞[∼][π]

_푡=0_

h Õ i

Notably, the cost value functions 푄[푖]푗,π [and][ 푉] _푗,[푖]_ **_π[, although similar to traditional][ 푄][π][ and][ 푉][π][, involve]_**
extra indices 푖 and 푗; the superscript 푖 denotes an agent, and the subscript 푗 denotes its 푗 [th] cost.

Throughout this work, we pay a close attention to the contribution to performance from different
subsets of agents, therefore, we introduce the following notations. We denote an arbitrary subset

2We believe that this formulation realistically describes multi-agent interactions in the real-world; an action
of an agent has an instantaneous effect on the system only locally, but the rest of agents may suffer from its
consequences at later stages. For example, consider a car that crosses on the red light, although other cars may
not be at risk of riding into pedestrians immediately, the induced traffic may cause hazards soon later.


-----

{푖1, . . ., 푖ℎ } of agents as 푖1:ℎ; we write −푖1:ℎ to refer to its complement. Given the agent subset 푖1:ℎ,
we define the multi-agent state-action value function:

_푄π[푖][1:][ℎ]_ **a[−][푖][1:][ℎ]** **_π[−][푖][1:][ℎ]_** _푄π_ s, a[푖][1:][ℎ] _, a[−][푖][1:][ℎ]_ _._

[(][푠,][ a][푖][1:][ℎ] [)][ ≜] [피] ∼ ( )

On top of it, the multi-agent advantage function [3] is defined as follows, 

_퐴π[푖][1:][ℎ]_ _푠, a 푗1:푘_ _, a푖1:ℎ_ ≜ _푄π푗1:푘_ _,푖1:ℎ_ _푠, a 푗1:푘_ _, a푖1:ℎ_ − _푄π푗1:푘_ _푠, a 푗1:푘_ _._ (2)

An interesting fact about the above multi-agent advantage function is that any advantage         _퐴π[푖][1:][ℎ]_ can
be written as a sum of sequentially-unfolding multi-agent advantages of individual agents, that is,
**Lemma 1 (Multi-Agent Advantage Decomposition, Kuba et al. (2021b)). For any state 푠** ∈S,
_subset of agents 푖1:ℎ_ ⊆N _, and joint action a[푖][1:][ℎ]_ _, the following identity holds_

_ℎ_

_퐴π[푖][1:][ℎ]_ _푠, a푖1:ℎ_ = _퐴π[푖]_ _[푗]_ _푠, a푖1: 푗−1, 푎푖_ _푗_ _._

_푗=1_

   Õ   

4 MULTI-AGENT CONSTRAINED POLICY OPTIMISATION

In this section, we first present a theoretically-justified safe multi-agent policy iteration procedure,
which leverages multi-agent trust region learning and constrained policy optimisation to solve constrained Markov games. Based on this, we propose two practical deep MARL algorithms, enabling
optimising neural-network based policies that satisfy safety constraints. Throughout this work, we
refer the symbols π and ¯π to be the “current” and the “new” joint policies, respectively.

4.1 MULTI-AGENT TRUST REGION LEARNING WITH CONSTRAINTS

Kuba et al. (2021a) introduced the first multi-agent trust region method—HATRPO—that enjoys
theoretically-justified monotonic improvement guarantee. Specifically, it relies on the multi-agent
advantage decomposition in Lemma 1, and the “surrogate” return that is given as follows.
**Definition 1. Let π be a joint policy, ¯π[푖][1:][ℎ][−][1]** _be some other joint policy of agents 푖1:ℎ−1, and ˆ휋[푖][ℎ]_ _be_
_a policy of agent 푖ℎ. Then we define_

_퐿π[푖][1:][ℎ]_ **_π¯[푖][1:][ℎ][−][1]_** _, ˆ휋[푖][ℎ]_ [] ≜ 피s∼휌π _,a푖1:ℎ−1_ ∼π¯ _[푖][1:][ℎ][−][1]_ _,a[푖ℎ]_ ∼ _휋ˆ_ _[푖ℎ]_ _퐴π[푖][ℎ]_ s, a푖1:ℎ−1, a푖ℎ _._
     

With the above definition, we can see that Lemma 1 allows for decomposing the joint surrogate
return 퐿π **_π¯_** ≜ 피s _휌π_ _,a_ **_π¯_** [[] _[퐴]π_ [(][s][,][ a][)]][ into a sum over surrogates of][ 퐿]π[푖][1:][ℎ] **_π[푖][1:][ℎ][−][1], ¯휋[푖][ℎ]_**, for ℎ =
1, . . ., 푛. This can be used to justify that if agents, with a joint policy( ) ∼ ∼ **_π, update their policies[(]_** **[¯]** )
by following a sequential update scheme, that is, if each agent in the subset 푖1:ℎ sequentially solves
the following optimisation problem:

_휋¯[푖][ℎ]_ = max **_π_** **_π¯[푖][1:][ℎ][−][1], ˆ휋[푖][ℎ]_** [] _휈퐷[max]KL_ _휋푖ℎ_ _, ˆ휋[푖][ℎ]_ [] _,_
_휋ˆ_ _[푖ℎ]_ _[퐿][푖][1:][ℎ]_ −
   

where 휈 = [4][훾] [max]1[푠,][a][ |]훾[퐴][π] [(][푠,][ a][)|] _,_ and 퐷[max]KL [(][휋][푖][ℎ] _[,][ ˆ]휋[푖][ℎ]_ ) ≜ max푠 _퐷KL(휋[푖][ℎ]_ (·|푠), ˆ휋[푖][ℎ] (·|푠)),

( − )[2]

then the resulting joint policy ¯π will surely improve the expected return, i.e., 퐽 (π¯) ≥ _퐽_ (π) (see the
proof in Kuba et al. (2021a, Lemma 2)). We know that due to the penalty term 퐷[max]KL _휋[푖][ℎ]_, the
new policy ¯휋[푖][ℎ] will stay close (w.r.t max-KL distance) to 휋[푖][ℎ] . [(][휋][푖][ℎ] _[,][ ˆ]_ )

For the safety constraints, we can extend Definition 1 to incorporate the “surrogate” cost, thus allowing us to study the cost functions in addition to the return.
**Definition 2. Let π be a joint policy, and ¯휋[푖]** _be some other policy of agent 푖. Then, for any of its_
_costs of index 푗_ ∈{1, . . ., 푚[푖] }, we define

_푖_
_퐿[푖]푗,π_ ¯휋[푖][] = 피s∼휌π _,a푖∼_ _휋¯_ _[푖]_ _퐴[푖]푗,π_ s, a _._

3We would like to highlight that these multi-agent functions of  h _푄 _ _[푖]π1:ℎ_ and[i] _퐴π[푖][1:][ℎ]_ [, although involve agents in]
superscripts, describe values w.r.t the reward rather than costs since they do not involve cost subscripts.


-----

By generalising the result about the surrogate return in Equation (1), we can derive how the expected
costs change when the agents update their policies. Specifically, we provide the following lemma.

**Lemma 2. Let π and ¯π be joint policies. Let 푖** ∈N be an agent, and 푗 ∈{1, . . ., 푚[푖] } be an index
_of one of its costs. The following inequality holds_


_퐽[푖]푗_ [(][ ¯]π _퐽[푖]푗_ [(][π][) +][ 퐿][푖]푗,π ¯휋[푖][] _휈[푖]_
) ≤ +
 


_푛_ _퐷[max]KL_ _휋ℎ, ¯휋[ℎ][],_ _where 휈[푖]푗_ [=] 4훾 max푠,푎푖 | _퐴[푖]푗,π_ [(][푠, 푎][푖][)|]

1 _훾_

Õℎ=1   ( − )[2]


See proof in Appendix A. The above lemma suggests that, as long as the distances between the
policies 휋[ℎ] and ¯휋[ℎ], ∀ℎ ∈N, are sufficiently small, then the change in the 푗 [th] cost of agent 푖, i.e.,
_퐽[푖]푗_ [(][ ¯]π) − _퐽[푖]푗_ [(][π][)][, is controlled by the surrogate][ 퐿][푖]푗,π [(][ ¯]휋[푖]). Importantly, this surrogate is independent of
other agents’ new policies. Hence, when the changes in policies of all agents are sufficiently small,
each agent 푖 can learn a better policy ¯휋[푖] by only considering its own surrogate return and surrogate
costs. To summarise, we provide the following algorithm that guarantees both safety constraints
satisfaction and monotonic performance improvement.

**Algorithm 1: Safe Multi-Agent Policy Iteration with Monotonic Improvement Property**

2:1: for Initialise a safe joint policy 푘 = 0, 1, . . . do **_π0 = (휋0[1][, . . ., 휋]0[푛][)][.]_**
3: Compute the advantage functions 퐴π푘 (푠, a) and 퐴[푖]푗,π푘 [(][푠, 푎][푖][)][, for all state-(joint)action pairs]

4: Compute(푠, a), agents 휈 = 푖4, and constraints훾 max푠,(a1− |훾퐴)π[2]푘 (푠,a) | 푗, and∈{1 휈, . . ., 푚[푖]푗 [=] 4훾[푖] }max. _푠,푎푖(1−|_ _퐴훾[푖]푗,)_ [2]π푘 [(][푠,푎][푖][) |], ∀푖 ∈N _, 푗_ = 1, . . ., 푚[푖].

5: Draw a permutaion 푖1:푛 of agents at random.

6: **for ℎ** = 1 : 푛 **do**

7: Compute the radius of the KL-constraint 훿[푖][ℎ] // see Appendix B for the setup of 훿[푖][ℎ] _._

8: Make an update 휋[푖]푘[ℎ]+1 [=][ arg max] _휋[푖]ℎ_ ∈Π푖ℎ _퐿π[푖][1:]푘[ℎ]_ _휋[푖]푘[1:]+[ℎ]1[−][1]_ _[, 휋][푖][ℎ]_ − _휈퐷[max]KL_ _휋[푖]푘[ℎ]_ _[, 휋][푖][ℎ]_,

where Π[푖][ℎ] is a subset of safe policies of agenth  _푖ℎ, given by_  i

Π[푖][ℎ] = _휋[푖][ℎ]_ Π[푖][ℎ] _퐷[max]_
∈ KL [(][휋][푖]푘[ℎ] _[, 휋][푖][ℎ]_ [) ≤] _[훿][푖][ℎ]_ _[,][ and]_
n _ℎ−1_

_퐽[푖][ℎ]_ _휈[푖][푙]_
_푗_ [(][π][푘] [) +][ 퐿][푖]푗,[ℎ]π푘 [(][휋][푖][ℎ] [) +][ 휈][푖]푗[ℎ] _[퐷][max]KL_ [(][휋][푖]푘[ℎ] _[, 휋][푖][ℎ]_ [) ≤] _[푐][푖]푗[ℎ]_ [−] _푙=1_ _푗_ _[퐷][max]KL_ [(][휋][푖]푘[푙] _[, 휋][푖][푙]_ [)][,][ ∀] _[푗]_ [=][ 1][, . . ., 푚][푖][ℎ]

Õ o


9: **end for**

10: end for

In the above algorithm, in addition to sequentially maximising agents’ surrogate returns, the agents
must assure that their surrogate costs stay below the corresponding safety thresholds. Meanwhile,
they have to constrain their policy search to small local neighbourhoods (w.r.t max-KL distance).
As such, Algorithm 1 demonstrates two desirable properties: reward performance improvement and
satisfaction of safety constraints, which we justify in the following theorem.

**Theorem 1. If a sequence of joint policies (π푘** )[∞]푘=0 _[is obtained from Algorithm][ 1][, then it has the]_
_monotonic improvement property, 퐽_ (π푘+1) ≥ _퐽_ (π푘 ), as well as it satisfies the safety constraints,
_퐽[푖]푗_ [(][π][푘] [) ≤] _[푐][푖]푗_ _[, for all][ 푘]_ [∈] [ℕ][, 푖] [∈N] _[, and][ 푗]_ [∈{][1][, . . ., 푚][푖] [}][.]

See proof in Appendix B. The above theorem assures that agents that follow Algorithm 1 will only
explore safe policies; meanwhile, every new policy will be guaranteed to result in performance
improvement. These two properties hold under the conditions that only restrictive policy updates
are made; this is due to the KL-penalty term in every agent’s objective (i.e., 휈퐷[max]KL _푘_ _[, 휋][푖][ℎ]_ [)][), as well]

_푖ℎ_ [(][휋][푖][ℎ]
as the constraints on cost surrogates (i.e., the conditions in Π ). In practice, it can be intractable to
evaluate 퐷KL _휋푖푘ℎ_ [(·|][푠][)][, 휋][푖][ℎ] [(·|][푠][)][][ at every state in order to compute][ 퐷][max]KL _푘_ _[, 휋][푖][ℎ]_ [)][. In the following]
subsections, we describe how we can approximate Algorithm 1 in the case of parameterised policies,[(][휋][푖][ℎ]
similar to TRPO/PPO implementations (  Schulman et al., 2015; 2017).


-----

4.2 MACPO: MULTI-AGENT CONSTRAINED POLICY OPTIMISATION

Here we focus on the practical settings where large state and action spaces prevent agents from designating policies 휋[푖] _푠_ for each state separately. To handle this, we parameterise each agent’s 휋[푖]휃 _[푖]_
(·| )
by a neural network 휃[푖]. Correspondingly, the joint policies πθ are parametrised by θ = (휃[1], . . ., 휃[푛]).

Let’s recall that at every iteration of Algorithm 1, every agent 푖ℎ maximises its surrogate return with
a KL-penalty, subject to surrogate cost constraint. Yet, direct computation of the max-KL constraint
is intractable in practical settings, as it would require computation of KL-divergence at every single
state. Instead, one can relax it by adopting a form of expected KL-constraint 퐷KL (휋[푖]푘[ℎ] _[, 휋][푖][ℎ]_ [) ≤] _[훿]_
where 퐷KL _휋[푖]푘[ℎ]_ _[, 휋][푖][ℎ]_ [)][ ≜] [피][s][∼][휌][π]푘 _퐷KL_ _휋[푖]푘[ℎ]_ [(·|][s][)][, 휋][푖][ℎ] [(·|][s][))] . Such an expectation can be approximated
( (
by stochastic sampling. As a result, the optimisation problem solved by agent 푖ℎ is written as
 

_휃[푖]푘[ℎ]+1_ [=][ arg max]휃 _[푖ℎ]_ 피s∼휌πθ푘 _,a푖1:ℎ−1_ ∼πθ푖1:[푖]푘[1:]ℎ[ℎ]−1 1[−][1] _,a[푖ℎ]_ ∼ _휋휃[푖ℎ][푖ℎ]_ _퐴π[푖][ℎ]θ푘_ s, a푖1:ℎ−1, a푖ℎ

+ h

  [i]

s.t. _퐽[푖]푗[ℎ]_ **_πθ푘_** + 피s∼휌πθ푘 _,a푖ℎ_ ∼ _휋휃푖ℎ푘[푖ℎ]_ _퐴[푖]푗,[ℎ]πθ푘_ s, a푖ℎ ≤ _푐[푖]푗[ℎ]_ _[,][ ∀]_ _[푗]_ [∈{][1][, . . ., 푚][푖][ℎ] [}][,]

h

and _퐷KL _ _휋푖푘ℎ[, 휋][푖][ℎ]_ [][ ≤] _[훿.]_   [i] (3)

We can further approximate Equation (  3) by Taylor expansion of the optimisation objective and cost
constraints up to the first order, and the KL-divergence up to the second order. Consequently, the
optimisation problem can be written as

_휃[푖]푘[ℎ]+1_ [=][ arg max]휃 _[푖ℎ]_ **_g푖ℎ_** _푇_ []휃[푖][ℎ] − _휃[푖]푘[ℎ]_
   _푇_ [] 

s.t. _푑[푖]푗[ℎ]_ [+][  ][b][푖]푗[ℎ] _휃[푖][ℎ]_ _휃[푖]푘[ℎ]_ 0, _푗_ = 1, . . ., 푚
− ≤

1  _푇_ 
and _휃[푖][ℎ]_ _휃[푖]푘[ℎ]_ **_H_** _[푖][ℎ]_ _휃[푖][ℎ]_ _휃[푖]푘[ℎ]_ _훿,_ (4)
2 − − ≤

   

where g[푖][ℎ] is the gradient of the objective of agent 푖ℎ in Equation (3), 푑[푖]푗 [=][ 퐽][푖]푗 [(][π][θ][푘] [) −] _[푐][푖]푗_ [, and]

**_H_** _[푖][ℎ]_ = ∇[2]휃 _[푖ℎ]_ _[퐷][KL]_ [(][휋][푖]휃[ℎ]푘[푖ℎ] _, 휋[푖][ℎ]_ ) _휃_ _[푖ℎ]_ =휃푘[푖ℎ] [is the Hessian of the average KL divergence of agent][ 푖][ℎ][, and][ b][푖]푗[ℎ]

is the gradient of agent of the 푗 [th] constraint of agent 푖ℎ.

Similar to Chow et al. (2017) and Achiam et al. (2017), one can take a primal-dual optimisation
approach to solve the linear quadratic optimisation in Equation (4). Specifically, the dual form can
be written as:

1
_휆[푖ℎ]_ ≥max0,v[푖ℎ] ⪰0 2−휆[푖][ℎ] (g[푖][ℎ] )[푇] (H _[푖][ℎ]_ )[−][1]g[푖][ℎ] − 2(r[푖][ℎ] )[푇] **v[푖][ℎ]** + (v[푖][ℎ] )[푇] **_S[푖][ℎ]_** [] + (v[푖][ℎ] )[푇] **_c[푖][ℎ]_** − _[휆][푖]2[ℎ]_ _[훿]_ _[,]_



where r[푖][ℎ] ≜ (g[푖][ℎ] )[푇] (H _[푖][ℎ]_ )[−][1]B[푖][ℎ] _, B[푖][ℎ]_ = **_b[푖]1[ℎ]_** _[, . . .,][ b][푖]푚[ℎ]_ and S[푖][ℎ] ≜ (B[푖][ℎ] )[푇] (H _[푖][ℎ]_ )[−][1]B[푖][ℎ] _._ (5)
h i

Given the solution to the dual form in Equation (5), i.e., 휆[푖][ℎ]
∗ [and][ v][푖]∗[ℎ] [, the solution to the primal]
problem in Equation (4) can thus be written by

_푖ℎ_ 1 _푖ℎ_ _푖ℎ_ _푖ℎ_

_휃[푖][ℎ]_ _푘_ [+][ 1] **_H_** − **_g_** **_B_** **v** _._
∗ [=][ 휃][푖][ℎ] _휆[푖][ℎ]_ − ∗

∗

     

In practice, we use backtracking line search starting at 1 _휆[푖][ℎ]_
/ ∗ [to choose the step size of the above]
update. Furthermore, we note that the optimisation step in Equation (4) is an approximation to the
original problem from Equation (3); therefore, it is possible that an infeasible policy 휋 _휃푘푖ℎ+1_ [will be]

generated. Fortunately, as the policy optimisation takes place in the trust region of 휋[푖][ℎ], the size of

update is small, and a feasible policy can be easily recovered. In particular, for problems with one휃푘[푖ℎ]
safety constraint, i.e., 푚[푖][ℎ] = 1, one can recover a feasible policy by applying a TRPO step on the
cost surrogate, written as

2훿 _푖ℎ_ 1 _푖ℎ_

_휃[푖]푘[ℎ]_ 1 [=][ 휃][푖]푘[ℎ] [−] _[훼]_ _[푗]_ **_H_** − **_b_** (6)
+ s **_b[푖][ℎ]_** _[푇]_ **_H_** _[푖][ℎ]_ **_b[푖][ℎ]_**

( )[−][1]
  


-----

where 훼 _[푗]_ is adjusted through backtracking line search. To put it together, we refer to this algorithm
as MACPO, and provide its pseudocode in Appendix D.

4.3 MAPPO-LAGRANGIAN

In addition to MACPO, one can use Lagrangian multipliers in place of optimisation with linear
and quadratic constraints to solve Equation (3). The Lagrangian method is simple to implement,
and it does not require computations of the Hessian H _[푖][ℎ]_ whose size grows quadratically with the
dimension of the parameter vector 휃[푖][ℎ] .

Before we proceed, let us briefly recall the optimisation procedure with a Lagrangian multiplier.
Suppose that our goal is to maximise a bounded real-valued function 푓 (푥) under a constraint 푔(푥);
max푥 _푓_ _푥_ _, s.t. 푔_ _푥_ 0. We can introduce a scalar variable 휆 and reformulate the optimisation by
( ) ( ) ≤
max min (7)
_푥_ _휆_ 0 _[푓]_ [(][푥][) −] _[휆푔][(][푥][)][.]_
≥

Suppose that 푥 satisfies 푔 _푥_ _> 0. This immediately implies that_ _휆푔_ _푥_, as 휆,
+ ( +) − ( +) →−∞ →+∞
and so Equation (7) equals for 푥 = 푥 . On the other hand, if 푥 satisfies 푔 _푥_ 0, we have
−∞ + − ( −) ≤
that _휆푔_ _푥_ 0, with equality only for 휆 = 0. In that case, the optimisation objective’s value
− ( −) ≥
equals 푓 _푥_ _>_ . Hence, the only candidate solutions to the problem are those 푥 that satisfy the
( −) −∞
constraint 푔(푥) ≤ 0, and the objective matches with 푓 (푥).

We can employ the above trick to the constrained optimisation problem from Equation (3) by subsuming the cost constraints into the optimisation objective with Lagrangian multipliers. As such,
agent 푖ℎ computes _휆[¯][푖]1:[ℎ]푚[푖ℎ]_ [and][ 휃][푖]푘[ℎ]+1 [to solve the following min-max optimisation problem]

_휆[푖ℎ]1:min푚[푖ℎ]_ [≥][0] max휃 _[푖ℎ]_ "피s∼휌πθ푘 _,a푖1:ℎ−1_ ∼πθ푖1:[푖]푘[1:]ℎ+[ℎ]−1 1[−][1] _,a[푖ℎ]_ ∼ _휋휃[푖ℎ][푖ℎ]_ h퐴π[푖][ℎ]θ푘 s, a푖1:ℎ−1, a푖ℎ

  [i]

_푚[푖ℎ]_

− _푢=1_ _휆[푖]푢[ℎ]_ 피s∼휌πθ푘 _,a푖ℎ_ ∼ _휋휃푖ℎ[푖ℎ]_ _퐴푢,[푖][ℎ]_ **_πθ푘_** s, a푖ℎ + 푑푢[푖][ℎ]  [#],

Õ h   [i]

s.t. 퐷KL _휋[푖]휃[ℎ]푘[푖ℎ]_ _, 휋[푖]휃[ℎ][푖ℎ]_ ≤ _훿._ (8)
 

Although the objective from Equation (8) is affine in the Lagrangian multipliers 휆[푖]푢[ℎ] [(][푢] [=][ 1][, . . ., 푚][푖][ℎ] [),]
which enables gradient-based optimisation solutions, computing the KL-divergence constraint still
complicates the overall process. To handle this, one can further simplify it by adopting the PPO-clip
objective (Schulman et al., 2017), which enables replacing the KL-divergence constraint with the
_clip operator, and update the policy parameter with first-order methods. We do so by defining_

_푚[푖ℎ]_

_퐴π[푖][ℎ]θ[,]_ _푘[(][휆][)]_ _푠, a푖1:ℎ−1_ _, 푎푖ℎ_ ≜ _퐴π푖ℎθ푘_ _푠, a푖1:ℎ−1, 푎푖ℎ_ − _휆[푖]푢[ℎ]_ _퐴푢,[푖][ℎ]_ **_πθ푘_** _푠, 푎푖ℎ_ + 푑푢푖ℎ _,_

_푢=1_

      Õ     

and rewriting the Equation (8) as

_휆[푖ℎ]1:min푚[푖ℎ]_ [≥][0] max휃 _[푖ℎ]_ [피][s][∼][휌][πθ]푘 _[,][a][푖][1:][ℎ][−][1]_ [∼][π]θ푖1:[푖]푘[1:]ℎ+[ℎ]−1 1[−][1] _,a[푖ℎ]_ ∼ _휋휃[푖ℎ][푖ℎ]_ h퐴π[푖][ℎ]θ[,] _푘[(][휆][)]_ s, a푖1:ℎ−1, a푖ℎ _,_

   [i]

s.t. 퐷KL _휋[푖]휃[ℎ]푘[푖ℎ]_ _, 휋[푖]휃[ℎ][푖ℎ]_ ≤ _훿._ (9)
 

The objective in Equation (9) takes a form of an expectation with quadratic constraint on the policy.
Up to the error of approximation of KL-constraint with the clip operator, it can be equivalently
transformed into an optimisation of a clipping objective. Finally, the objective takes the form of

_휋푖ℎ_

피s∼휌πθ푘 _,a푖1:ℎ−1_ ∼πθ푖1:[푖]푘[1:]ℎ+[ℎ]−1 1[−][1] _,a[푖ℎ]_ ∼ _휋휃[푖ℎ]푘[푖ℎ]_ "min _휋[푖]휃휃[ℎ]푘[푖ℎ][푖ℎ]_ [(]([a]a[푖][푖][ℎ][ℎ] [|]|[s]s[)]) _퐴π[푖][ℎ]θ[,]_ _푘[(][휆][)]_  s, a푖1:ℎ−1, a푖ℎ ,

_푖ℎ_
_휋_
clip _휃_ _[푖ℎ]_ [(][a][푖][ℎ] [|][s][)] _, 1_ _휖_ _퐴π[푖][ℎ]θ[,]_ _푘[(][휆][)]_ s, a푖1:ℎ−1, a푖ℎ _._ (10)
 _휋[푖]휃[ℎ]푘[푖ℎ]_ (a[푖][ℎ] |s) ±     !#


-----

(a) (b) (c)

Figure 1: Example tasks in Safe Multi-Agent MuJoCo Environment. (a): Safe 2x4-Ant, (b): Safe
4x2-Ant, (c): Safe 2x3-HalfCheetah. Body parts of different colours are controlled by different
agents. Agents jointly learn to manipulate the robot, while avoiding crashing into unsafe red areas.

The clip operator replaces the policy ratio with 1 − _휖, or 1 + 휖, depending on whether its value is_
below or above the threshold interval. As such, agent 푖ℎ can learn within its trust region by updating
_휃[푖][ℎ]_ to maximise Equation (10), while the Lagrangian multipliers are updated towards the direction
opposite to their gradients of Equation (8), which can be computed analytically. We refer to this
algorithm as MAPPO-Lagrangian, and give a detailed pseudocode of it in Appendix E.

5 EXPERIMENTS

Although MARL researchers have long had a variety of environments to test different algorithms,
such as StarCraftII (Samvelyan et al., 2019) and Multi-Agent MuJoCo (Peng et al., 2020), no public
safe MARL benchmark has been proposed; this impedes researchers from evaluating and benchmarking safety-aware multi-agent learning methods. As a key contribution of this paper, we introduce Safe Multi-Agent MuJoCo Benchmark, a safety-aware extension of the MuJoCo environment
that is designed for safe MARL research. We show example tasks in Figure 1, in our environment,
safety-aware agents have to learn not only skilful manipulations of a robot, but also to avoid crashing
into unsafe obstacles and positions. For more details of the setup, please refer to Appendix F.

We use Safe MAMuJoCo to examine if the MACPO/MAPPO-Lagrangian agents can satisfy
their safety constraints and cooperatively learn to achieve high rewards, compared to existing
MARL algorithms. Notably, our proposed methods adopt two different approaches for achieving
safety. MACPO reaches safety via hard constraints and backtracking line search, while MAPPOLagrangian maintains a rather soft safety awareness by performing gradient descents on the PPOclip objective. Figure 2 shows cost and reward performance comparisons between MACPO,
MAPPO-Lagrangian, MAPPO (Yu et al., 2021), IPPO (de Witt et al., 2020), and HAPPO (Kuba
et al., 2021a) algorithms on three challenging tasks. Figure 2 should be interpreted at three-folds;
each subfigure represents a different robot, within each subfigure, three task setups in terms of multiagent control are considered, for each task, we plot the cost curves (the lower the better) in the upper
row, and plot the reward curves (the higher the better) in the bottom row. Detailed hyperparameter
settings are described in Appendix H.

The experiments reveal that both MACPO and MAPPO-Lagrangian quickly learn to satisfy safety
constraints, and keep their explorations within the feasible policy space. This stands in contrast to
IPPO, MAPPO, and HAPPO which largely violate the constraints thus being unsafe. Furthermore,
our algorithms achieve comparable reward scores; both methods are often better than IPPO. In
general, the performance (in terms of reward) of MAPPO-Lagrangian is better than of MACPO;
moreover, MAPPO-Lagrangian outperforms the unconstrained MAPPO on challenging Ant tasks.
We note that on none of the tasks the reward of HAPPO was exceeded though it is unsafe.

6 CONCLUSION

In this paper, we tackled multi-agent policy optimisation problems with safety constraints. Central
to our findings is the safe multi-agent policy iteration procedure that attains theoretically-justified
monotonic improvement guarantee and constraints satisfaction guarantee at every iteration during
learning. Based on this, we proposed two practical algorithms: MACPO and MAPPO-Lagrangian.
To demonstrate their effectiveness, we introduced a new benchmark suite of Safe Multi-Agent Mu_JoCo and compared our methods against strong MARL baselines. Results show that both of our_
methods can significantly outperform existing state-of-the-art methods such as IPPO, MAPPO and
HAPPO in terms of safety, meanwhile maintaining comparable performance in terms of reward.


-----

ManyAgent Ant 2x3 ManyAgent Ant 3x2 ManyAgent Ant 6x1

600 600 600 HAPPOMAPPO

500 500 500 MACPO (ours)MAPPO-L (ours)

400 400 400 IPPO

300 300 300

200 200 200

100 100 100

Average Episode Cost Average Episode Cost Average Episode Cost

0 0 0

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

Environment steps 1e7 Environment steps 1e7 Environment steps 1e7

ManyAgent Ant 2x3 ManyAgent Ant 3x2 ManyAgent Ant 6x1

3000 3000

2500 2500 2500 HAPPOMAPPO

2000 2000 2000 MACPO (ours)MAPPO-L (ours)

1500 1500 1500 IPPO

1000 1000 1000

500 500 500

0

0 0

Average Episode Reward 500 Average Episode Reward 500 Average Episode Reward 500

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 1000 0.0 0.2 0.4 0.6 0.8 1.0

Environment steps 1e7 Environment steps 1e7 Environment steps 1e7


(a) Safe ManyAgent Ant: 2x3-Agent (left), 3x2-Agent (middle), 6x1-Agent (right)





Ant 2x4 Ant 4x2 Ant 2x4d

400350 400 350 HAPPOMAPPO

300 300 MACPO (ours)

250 300 250 MAPPO-L (ours)IPPO

200 200 200

150 150

100 100 100

Average Episode Cost 50 Average Episode Cost Average Episode Cost 50

0 0 0

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

Environment steps 1e7 Environment steps 1e7 Environment steps 1e7

Ant 2x4 Ant 4x2 Ant 2x4d

4000

3500 3500 3500 HAPPOMAPPO

30002500 30002500 30002500 MACPO (ours)MAPPO-L (ours)IPPO

2000 2000 2000

1500 1500 1500

1000 1000 1000

500 500 500

Average Episode Reward Average Episode Reward Average Episode Reward

0 0 0

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

Environment steps 1e7 Environment steps 1e7 Environment steps 1e7


(b) Safe Ant: 2x4-Agent (left), 4x2-Agent (middle), 2x4d-Agent (right)





HalfCheetah 2x3 HalfCheetah 3x2 HalfCheetah 6x1

700

600 HAPPO

600 600 MAPPO

500 500 500 MACPO (ours)MAPPO-L (ours)

400 400 400 IPPO

300 300 300

200 200 200

Average Episode Cost100 Average Episode Cost100 Average Episode Cost100

0 0 0

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

Environment steps 1e7 Environment steps 1e7 Environment steps 1e7

HalfCheetah 2x3 HalfCheetah 3x2 HalfCheetah 6x1

4000 4000 4000 HAPPOMAPPO

3000 MACPO (ours)

3000 3000 MAPPO-L (ours)IPPO

2000 2000 2000

1000 1000 1000

Average Episode Reward 0 Average Episode Reward 0 Average Episode Reward 0

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

Environment steps 1e7 Environment steps 1e7 Environment steps 1e7


(c) Safe HalfCheetah: 2x3-Agent (left), 3x2-Agent (middle), 6x1-Agent (right)

Figure 2: Performance comparisons on tasks of Safe ManyAgent Ant, Safe Ant, and Safe HalfCheetah in terms of cost (the first row) and reward (the second row). The safety constraint values are: 1
for ManyAgent Ant, 0.2 for Ant, and 5 for HalfCheetah. Our methods consistently achieve almost
zero costs, thus satisfying safe constraints, on all tasks. In terms of reward, our methods outperform
IPPO and MAPPO on some tasks but underperform HAPPO, which is also an unsafe algorithm.


-----

REFERENCES

Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In
_International Conference on Machine Learning, pp. 22–31. PMLR, 2017._

Eitan Altman. Constrained Markov decision processes, volume 7. CRC Press, 1999.

Aaron D Ames, Xiangru Xu, Jessy W Grizzle, and Paulo Tabuada. Control barrier function based
quadratic programs for safety critical systems. IEEE Transactions on Automatic Control, 62(8):
3861–3876, 2016.

Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man´e. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565, 2016.

Urs Borrmann, Li Wang, Aaron D Ames, and Magnus Egerstedt. Control barrier certificates for safe
swarm behavior. IFAC-PapersOnLine, 48(27):68–73, 2015.

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.

Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained reinforcement learning with percentile risk criteria. The Journal of Machine Learning Research, 18
(1):6070–6120, 2017.

Yinlam Chow, Ofir Nachum, Edgar Duenez-Guzman, and Mohammad Ghavamzadeh. A lyapunovbased approach to safe reinforcement learning. arXiv preprint arXiv:1805.07708, 2018.

Yinlam Chow, Ofir Nachum, Aleksandra Faust, Edgar Duenez-Guzman, and Mohammad
Ghavamzadeh. Lyapunov-based safe policy optimization for continuous control. arXiv preprint
_arXiv:1901.10031, 2019._

Christian Schroeder de Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip HS
Torr, Mingfei Sun, and Shimon Whiteson. Is independent learning all you need in the starcraft
multi-agent challenge? arXiv preprint arXiv:2011.09533, 2020.

Xiaotie Deng, Yuhao Li, David Henry Mguni, Jun Wang, and Yaodong Yang. On the complexity of computing markov perfect equilibrium in general-sum stochastic games. arXiv preprint
_arXiv:2109.01795, 2021._

Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In Proceedings of the AAAI Conference on Artificial
_Intelligence, volume 32, 2018._

Javier Garcıa and Fernando Fern´andez. A comprehensive survey on safe reinforcement learning.
_Journal of Machine Learning Research, 16(1):1437–1480, 2015._

Jakub Grudzien Kuba, Ruiqing Chen, Munning Wen, Ying Wen, Fanglei Sun, Jun Wang, and
Yaodong Yang. Trust region policy optimisation in multi-agent reinforcement learning. arXiv
_preprint arXiv:2109.11251, 2021a._

Jakub Grudzien Kuba, Muning Wen, Yaodong Yang, Linghui Meng, Shangding Gu, Haifeng Zhang,
David Henry Mguni, and Jun Wang. Settling the variance of multi-agent policy gradients. arXiv
_preprint arXiv:2108.08612, 2021b._

Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
_preprint arXiv:1509.02971, 2015._

Songtao Lu, Kaiqing Zhang, Tianyi Chen, Tamer Basar, and Lior Horesh. Decentralized policy
gradient descent ascent for safe multi-agent reinforcement learning. In Proceedings of the AAAI
_Conference on Artificial Intelligence, volume 35, pp. 8767–8775, 2021._

Teodor Mihai Moldovan and Pieter Abbeel. Safe exploration in markov decision processes. In Pro_ceedings of the 29th International Coference on International Conference on Machine Learning,_
pp. 1451–1458, 2012.


-----

Bei Peng, Tabish Rashid, Christian A Schroeder de Witt, Pierre-Alexandre Kamienny, Philip HS
Torr, Wendelin B¨ohmer, and Shimon Whiteson. Facmac: Factored multi-agent centralised policy
gradients. arXiv preprint arXiv:2003.06709, 2020.

David Pollard. Asymptopia: an exposition of statistical asymptotic theory. 2000. URL http://www.
_stat. yale. edu/pollard/Books/Asymptopia, 2000._

Zengyi Qin, Kaiqing Zhang, Yuxiao Chen, Jingkai Chen, and Chuchu Fan. Learning safe multi-agent
control with decentralized neural barrier certificates. In International Conference on Learning
_Representations, 2020._

Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and
Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning, pp. 4295–4304. PMLR, 2018.

Alex Ray, Joshua Achiam, and Dario Amodei. Benchmarking safe exploration in deep reinforcement
learning. arXiv preprint arXiv:1910.01708, 7, 2019.

Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas
Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson.
The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.

John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889–1897. PMLR,
2015.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement
learning for autonomous driving. arXiv preprint arXiv:1610.03295, 2016.

David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484–489, 2016.

David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. nature, 550(7676):354–359, 2017.

Richard S Sutton, David A McAllester, Satinder P Singh, Yishay Mansour, et al. Policy gradient
methods for reinforcement learning with function approximation. In NIPs, volume 99, pp. 1057–
1063. Citeseer, 1999.

Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha¨el Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster
level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350–354, 2019.

Honghao Wei, Xin Liu, and Lei Ying. A provably-efficient model-free algorithm for constrained
markov decision processes. arXiv preprint arXiv:2106.01577, 2021.

Tengyu Xu, Yingbin Liang, and Guanghui Lan. Crpo: A new approach for safe reinforcement
learning with convergence guarantee. In International Conference on Machine Learning, pp.
11480–11491. PMLR, 2021.

Yaodong Yang and Jun Wang. An overview of multi-agent reinforcement learning from game theoretical perspective. arXiv preprint arXiv:2011.00583, 2020.

Yaodong Yang, Ying Wen, Jun Wang, Liheng Chen, Kun Shao, David Mguni, and Weinan Zhang.
Multi-agent determinantal q-learning. In International Conference on Machine Learning, pp.
10757–10766. PMLR, 2020.

Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising
effectiveness of mappo in cooperative, multi-agent games. _arXiv preprint arXiv:2103.01955,_
2021.


-----

Moritz A. Zanger, Karam Daaboul, and J. Marius Z¨ollner. Safe continuous control with constrained
model-based policy optimization, 2021.

Wenbo Zhang, Osbert Bastani, and Vijay Kumar. Mamps: Safe multi-agent reinforcement learning
via model predictive shielding. arXiv preprint arXiv:1910.12639, 2019.


-----

# Appendices

**A Proofs of preliminary results** **14**

**B** **Auxiliary Results for Algorithm 1** **15**

**C Auxiliary Results for Implementation of MACPO** **17**

**D MACPO** **18**

**E** **MAPPO-Lagrangian** **19**

**F** **Safe Multi-Agent MuJoCo** **20**

**G Experiments in Safe Many-Agent Ant Environments** **21**

**H Details of Settings for Experiments** **22**


-----

A PROOFS OF PRELIMINARY RESULTS

**Lemma 1 (Multi-Agent Advantage Decomposition, Kuba et al. (2021b)). For any state 푠** ∈S,
_subset of agents 푖1:ℎ_ ⊆N _, and joint action a[푖][1:][ℎ]_ _, the following identity holds_

_ℎ_

_퐴π[푖][1:][ℎ]_ _푠, a푖1:ℎ_ = _퐴π[푖]_ _[푗]_ _푠, a푖1: 푗−1, 푎푖_ _푗_ _._

_푗=1_

   Õ   

_Proof. We write the multi-agent advantage as in its definition, and expand it in a telescoping sum._

_퐴π[푖][1:][ℎ]_ _푠, a푖1:ℎ_ = 푄푖π1:ℎ _푠, a푖1:ℎ_ _푉π_ _푠_
− ( )

_ℎ_

   = _푄 _ _[푖]π[1:][ 푗]_ _푠, a_ _푖1: 푗_ _푄π푖1: 푗−1_ _푠, a푖1: 푗−1_

−

_푗=1_

Õ h       [i]

_ℎ_

= _퐴π[푖]_ _[푗]_ _푠, a푖1: 푗−1, 푎푖_ _푗_ _._

_푗=1_

Õ   

□

**Lemma 2. Let π and ¯π be joint policies. Let 푖** ∈N be an agent, and 푗 ∈{1, . . ., 푚[푖] } be an index
_of one of its costs. The following inequality holds_


_퐽[푖]푗_ [(][ ¯]π _퐽[푖]푗_ [(][π][) +][ 퐿][푖]푗,π ¯휋[푖][] _휈[푖]_
) ≤ +
 


_푛_ _퐷[max]KL_ _휋ℎ, ¯휋[ℎ][],_ _where 휈[푖]푗_ [=] 4훾 max푠,푎푖 | _퐴[푖]푗,π_ [(][푠, 푎][푖][)|]

1 _훾_

Õℎ=1   ( − )[2]


_Proof. From the proof of Theorem 1 from Schulman et al. (2015) (in particular, equations (41)-(45)),_
applied to joint policies π and ¯π, we conclude that

_퐽[푖]푗_ [(]π[¯] _퐽[푖]푗_ [(][π][) +][ 피][s][∼][휌]π _[,][a][∼]π[¯]_ _퐴[푖]푗,π_ [(][s][,][ a][푖][)] 4훼[2]훾 max푠,푎푖 | _퐴[푖]푗,π_ [(][푠, 푎][푖][)|] _,_
) ≤ h i + (1 − _훾)[2]_

where _훼_ = 퐷[max]TV [(][π][,][ ¯]π) = max푠 _퐷TV (π(·|푠), ¯π(·|푠)) ._

Using the inequality 퐷TV _푝, 푞_ _퐷KL_ _푝, 푞_ (Pollard, 2000; Schulman et al., 2015), we obtain
( )[2] ≤ ( )

_퐽[푖]푗_ [(]π[¯] _퐽[푖]푗_ [(][π][) +][ 피][s][∼][휌]π _[,][a][∼]π[¯]_ _퐴[푖]푗,π_ [(][s][,][ a][푖][)] 4훾 max푠,푎푖 | _퐴[푖]푗,π_ [(][푠, 푎][푖][)|] _퐷[max]KL_ **_π_** _,_
) ≤ + 1 _훾_ [(][π][,][ ¯])
h i ( − )[2]

where _퐷[max]KL_ [(][π][,][ ¯]π) = max푠 _퐷KL (π(·|푠), ¯π(·|푠)) ._


Notice now that we have 피s _휌π_ _,a_ **_π¯_** _퐴[푖]푗,π_ [(][s][,][ a][푖][)] = 피s _휌π_ _,a푖_ _휋¯_ _[푖]_ _퐴[푖]푗,π_ [(][s][,][ a][푖][)], and
∼ ∼ ∼ ∼
h i _푛_ h i

_퐷[max]KL_ [(][π][,][ ¯]π) = max푠 _퐷KL (π(·|푠), ¯π(·|푠)) = max푠_ _푙=1_ _퐷KL_ _휋[푙]_ (·|푠), ¯휋[푙] (·|푠)

Õ   [!]

_푛_ _푛_

≤ max푠 _퐷KL_ _휋[푙]_ (·|푠), ¯휋[푙] (·|푠) = _퐷[max]KL_ _휋[푙], ¯휋[푙][]_ _._ (11)

_푙=1_ _푙=1_

Õ   Õ 


4훾 max푠,푎푖 _퐴[푖]푗,π_
Setting 휈[푖]푗 [=] (1−| _훾)_ [2] [(][푠,푎][푖][) |], we finally obtain

_퐽[푖]푗_ [(]π[¯] _퐽[푖]푗_ [(][π][) +][ 퐿][푖]푗,π ¯휋[푖][] _휈[푖]_
) ≤ +
 


_푛_

_퐷[max]KL_ _휋[푙], ¯휋[푙][]_
_푙=1_

Õ 


-----

B AUXILIARY RESULTS FOR ALGORITHM 1

**Remark 1. In Algorithm 1, we compute the size of KL constraint as**


_ℎ_ 1
_푐[푖]푗[푙]_ [−] _[퐽][푖]푗[푙]_ [(][π][푘] [) −] _[퐿][푖]푗,[푙]_ **_π푘_** [(][휋][푖]푘[푙] 1[) −] _[휈][푖]푗[푙]_ _푢=−1_ _[퐷][max]KL_ _푘_ _[, 휋][푢]푘_ 1[)]
min + [(][휋][푢] +
_푙_ ≤ℎ−1 1≤[min]푗 ≤푚[푙] _휈[푖]푗[푙]_ Í

_ℎ_ 1
_푐[푖]푗[푙]_ [−] _[퐽][푖]푗[푙]_ [(][π][푘] [) −] _[휈][푖]푗[푙]_ _푢=−1_ _[퐷][max]KL_ _푘_ _[, 휋][푢]푘_ 1[)]
min [(][휋][푢] +
_푙_ ≥ℎ+1 1≤[min]푗 ≤푚[푙] Í휈[푖]푗[푙] )


_훿[푖][ℎ]_ = min


_Note that 훿[푖][1]_ _(i.e., ℎ_ = 1) is guaranteed to be non-negative if π푘 _satisfies safety constraints; that is_
_because then 푐[푖]푗[푙]_ [≥] _[퐽][푖]푗[푙]_ [(][π][푘] [)][ for all][ 푙] _[and][ 푗][, and the set][ {][푙]_ [|][ 푙<ℎ][}][ is empty.]

_This formula for 훿[푖][ℎ]_ _, combined with Lemma 2, assures that the policies 휋[푖][ℎ]_ _within 훿[푖][ℎ]_ _max-KL_
_distance from 휋[푖]푘[ℎ]_ _[will not violate other agents’ safety constraints, as long as the base joint policy]_
**_π푘_** _did not violate them (which assures 훿[푖][1]_ 0). To see this, notice that for every 푙 = 1, . . ., ℎ 1,
≥ −
_and 푗_ = 1, . . ., 푚[푙],

_ℎ_ 1
_푐[푖]푗[푙]_ [−] _[퐽][푖]푗[푙]_ [(][π][푘] [) −] _[퐿][푖]푗,[푙]_ **_π푘_** [(][휋][푖]푘[푙] 1[) −] _[휈][푖]푗[푙]_ _푢=−1_ _[퐷][max]KL_ _푘_ _[, 휋][푢]푘_ 1[)]
_퐷[max]KL_ _푘_ _[, 휋][푖][ℎ]_ [) ≤] _[훿][푖][ℎ]_ [≤] + [(][휋][푢] + _,_

[(][휋][푖][ℎ] _휈[푖]푗[푙]_ Í


_ℎ−1_

_퐷[max]KL_ _푘_ _[, 휋][푢]푘_ 1[) +][ 휈][푖]푗[푙] _[퐷][max]KL_ _푘_ _[, 휋][푖][ℎ]_ [) ≤] _[푐][푖]푗[푙]_ _[.]_
_푢=1_ [(][휋][푢] + [(][휋][푖][ℎ]

Õ


_implies 퐽[푖]푗[푙]_ [(][π][푘] [) +][ 퐿][푖]푗,[푙] **_π푘_** [(][휋][푖]푘[푙] 1[) +][ 휈][푖]푗[푙]
+


_By Lemma 2, the left-hand side of the above inequality is an upper bound of 퐽[푖]푗[푙]_ [(][π]푘[푖][1:]+[ℎ]1[−][1] _[, 휋][푖][ℎ]_ _[,][ π]푘[−][푖][1:][ℎ]_ ),

_which implies that the update of agent 푖ℎ_ _doesn’t violate the constraint of 퐽[푖]푗[푙]_ _[. The fact that the]_

_constraints of 퐽[푖]푗[푙]_ _[for][ 푙]_ [≥] _[ℎ]_ [+][ 1][ are not violated, i.e.,]


_ℎ−1_

_퐷[max]KL_ _푘_ _[, 휋][푢]푘_ 1[) +][ 휈][푖]푗[푙] _[퐷][max]KL_ _푘_ _[, 휋][푖][ℎ]_ [) ≤] _[푐][푖]푗[푙]_ _[,]_ (12)
_푢=1_ [(][휋][푢] + [(][휋][푖][ℎ]

Õ


_퐽[푖]푗[푙]_ [(][π][푘] [) +][ 휈][푖]푗[푙]


_is analogous._

**Theorem 1. If a sequence of joint policies (π푘** )[∞]푘=0 _[is obtained from Algorithm][ 1][, then it has the]_
_monotonic improvement property, 퐽_ (π푘+1) ≥ _퐽_ (π푘 ), as well as it satisfies the safety constraints,
_퐽[푖]푗_ [(][π][푘] [) ≤] _[푐][푖]푗_ _[, for all][ 푘]_ [∈] [ℕ][, 푖] [∈N] _[, and][ 푗]_ [∈{][1][, . . ., 푚][푖] [}][.]

_Proof. Safety constraints are assured to be met by Remark 1._ It suffices to show the monotonic improvement property. Notice that at every iteration 푘 of Algorithm 1, 휋[푖]푘[ℎ] [∈] [Π]푖ℎ . Clearly
_퐷[max]_
KL _푘_ _[, 휋][푖]푘[ℎ]_ [)][ =][ 0][ ≤] _[훿][푖][ℎ]_ [. Moreover,]

[(][휋][푖][ℎ]


_ℎ−1_

_퐷[max]KL_ _푘_ _[, 휋][푖]푘[푙]_ 1[)][,]
_푙=1_ [(][휋][푖][푙] +

Õ


_퐽[푖]푗[ℎ]_ [(][π][푘] [) +][ 퐿][푖]푗,[ℎ]π푘 [(][휋][푖]푘[ℎ] [) +][ 휈][푖]푗[ℎ] _[퐷][max]KL_ [(][휋][푖]푘[ℎ] _[, 휋][푖]푘[ℎ]_ [)][ =][ 퐽][푖]푗[ℎ] [(][π][푘] [) ≤] _[푐][푖]푗[ℎ]_ [−] _[휈][푖]푗[ℎ]_


-----

where the inequality is guaranteed by updates of previous agents, as described in Remark 1 (Inequality 12). By Theorem 1 from Schulman et al. (2015), we have

_퐽_ (π푘+1) ≥ _퐽_ (π푘 ) + 피s∼휌π푘 _,a∼π푘+1_ _퐴π푘_ (s, a) − _휈퐷[max]KL_ [(][π][푘] _[,][ π][푘][+][1][)][,]_

which by Equation 11 is lower-bounded by 


≥ _퐽_ (π푘 ) + 피s∼휌π푘 _,a∼π푘+1_ _퐴π푘_ (s, a)


which by Lemma 1 equals


_휈퐷[max]KL_ _푘_ _[, 휋][푖]푘[ℎ]_ 1[)]
_ℎ=1_ [(][휋][푖][ℎ] +

Õ


_퐴π[푖][ℎ]푘_ [(][s][,][ a][푖][1:][ℎ][−][1][,][ a][푖][ℎ] [)]


_휈퐷[max]KL_ _푘_ _[, 휋][푖]푘[ℎ]_ 1[)]
_ℎ=1_ [(][휋][푖][ℎ] +

Õ


= 퐽 **_π푘_**
( ) +

= 퐽 **_π푘_**
( ) +


_ℎ=1_ 피s∼휌π푘 _,a푖1:ℎ_ ∼π푘푖1:+ℎ1

Õ


_퐿π[푖][1:]푘[ℎ]_ [(][π]푘[푖][1:][ℎ]1[−][1][, 휋][푖]푘[ℎ] 1[) −] _[휈퐷]KL[max]_ _푘_ _[, 휋][푖]푘[ℎ]_ 1[)] _,_ (13)
+ + [(][휋][푖][ℎ] +



_ℎ=1_


and as for every ℎ, 휋[푖]푘[ℎ] 1 [is the argmax, this is lower-bounded by]
+


_퐿π[푖][1:]푘[ℎ]_ [(][π]푘[푖][1:][ℎ]1[−][1][, 휋][푖]푘[ℎ] [) −] _[휈퐷]KL[max]_ _푘_ _[, 휋][푖]푘[ℎ]_ [)]
+ [(][휋][푖][ℎ]



_퐽_ **_π푘_**
≥ ( ) +

_ℎ=1_

Õ


which, as follows from Definition 1, equals


0 = 퐽 **_π푘_** _, which finishes the proof._
( )
_ℎ=1_

Õ


= **_π풌_**
J ( ) +


-----

C AUXILIARY RESULTS FOR IMPLEMENTATION OF MACPO

**Theorem 2. The solution to the following problem**

**_p_** = min
∗ _푥_ **_[g][푇]_** **_[x]_**

_s.t. b[푇]_ **_x + 푐_** ≤ 0

**_x[푇]_** **_Hx ≤_** _훿,_

_where g, b, x ∈_ ℝ[푛], 푐, 훿 ∈ ℝ, 훿> 0, H ∈ 핊[푛], and H ≻ 0. When there is at least one strictly feasible
_point, the optimal point x[∗]_ _satisfies:_

**_x[∗]_** = − _휆[1]_ **_H_** [−][1][ ]g[푇] + 푣∗b

∗


_where 휆_ _and 푣_ _are defined by_
∗ ∗

_휆_ _푐_ _푟_
_푣_ = ∗ −
∗ _푠_
  +

_휆∗_ = arg max휆≥0 ( _푓푓푏푎_ (휆휆) ≜ ≜ 21휆2  _푟 푞휆푠[2]_ [+][−][ 휆훿][푞][]+ _[휆]2_  _푐푠[2]_ [−] _[훿]_ − _[푟푐]푠_ _ifotherwise 휆푐_ − _푟> 0_

( ) − [1]

 

_where q = g[푇]_ **_H_** [−][1]g, r = g[푇] **_H_** [−][1]b, and s = b[푇] **_H_** [−][1]b.

_Furthermore, let Λ푎_ ≜ _휆_ _휆푐_ _푟> 0, 휆_ 0 _, and Λ푏_ ≜ _휆_ _휆푐_ _푟_ 0, 휆 0 _. The value of 휆_
{ | − ≥ } { | − ≤ ≥ } ∗
_satisfies_


_푞_ _푟_ [2] _푠_ _푞_

_휆_ _휆[푎]_ − / _, 휆[푏]_ (14)
∗ ∈ ∗ [≜] [Proj][ ©]s _훿_ _푐[2]_ _푠_ _[,][ Λ][푎][ª]_ ∗ [≜] [Proj] _훿_ _[,][ Λ][푏]_
 − / r 

­ ®

_where 휆_ = 휆[푎] _휆푎_  > 푓푏 _휆푏_ _and 휆_ = 휆 _otherwise, and Proj_ _푎, S_ _is the projection of a_
∗ ∗ _[if][ 푓][푎]_ ∗  ∗ « ∗ ∗ ¬ ( ) 
_pointvalue 푥 Projon to a set(푥, [푎, 푏 S]) =.  Note the projection of a point max_ (푎, min  (푏, 푥)). _푥_ ∈ ℝ _onto a convex segment of ℝ, [푎, 푏], has_

_Proof. See Achiam et al. (2017) (Appendix 10.2)._ □


-----

D MACPO

**Algorithm 2: MACPO**

1: Input: Stepsize 훼, batch size 퐵, number of: agents 푛, episodes 퐾, steps per episode 푇, possible
steps in line search 퐿.

2: Initialize: Actor networks {휃0[푖] _[,][ ∀][푖]_ [∈N}][, Global V-value network][ {][휙][0][}][, Individual][ 푉] _[푖][-cost]_
networks {휙[푖]푗,0[}][푖][=][1:][푛, 푗][=][1:][푚][, Replay buffer][ B]

3: for 푘 = 0, 1, . . ., 퐾 − 1 do
4: Collect a set of trajectories by running the joint policy πθ푘 = (휋[1]휃푘[1] _[, . . ., 휋][푛]휃푘[푛]_ [)][.]

5:6: Push transitionsSample a random minibatch of {(표[푖]푡 _[, 푎][푖]푡_ _[, 표][푖]푡+1[, 푟] 푀[푡]_ [)][,][ ∀]transitions from[푖] [∈N] _[, 푡]_ [∈] _[푇]_ [}][ into] B.[ B][.]

7: Compute advantage function _퐴[ˆ]_ (s, a) based on global V-value network with GAE.

8: Compute cost-advantage functions _퐴[ˆ][푖]푗_ [(][s][,][ a][푖][)][ based on individual][ 푉] _[푖][-cost critics with GAE.]_

9: Draw a random permutation of agents 푖1:푛.

10: Set 푀 _[푖][1]_ (s, a) = _퐴[ˆ]_ (s, a).

11: **for agent 푖ℎ** = 푖1, . . ., 푖푛 **do**

12: Estimate the gradient of the agent’s maximisation objective


**_gˆ_** _푘[푖][ℎ]_ [=][ 1]퐵


_푇_

_푡=1_ ∇휃푘푖ℎ [log][ 휋][푖]휃[ℎ]푘[푖ℎ]

Í


_푎[푖]푡[ℎ]_ [|][ 표][푖]푡[ℎ] _푀_ _[푖][1:][ℎ]_ _푠푡_ _, a푡_ .
( )



_푏=1_


_푘_

13: **for 푗** = 1, . . ., 푚[푖][ℎ] **do**

14: Estimate the gradient of the agent’s 푗 [th] cost


**_bˆ_** _[푖]푗[ℎ]_ [=][ 1]퐵


_푡=1_ ∇휃푘푖ℎ [log][ 휋][푖]휃[ℎ]푘[푖ℎ]

Í


_푎[푖]푡[ℎ]_ [|][표][푖]푡[ℎ] _퐴ˆ[푖]푗[ℎ]_ [(][푠][푡] _[, 푎][푖]푡[ℎ]_ [)][.]



_푏=1_

Í


_푘_

15: **end for**

16: Set **_B[ˆ]_** _[푖][ℎ]_ = **_bˆ_** _[푖]1[ℎ]_ _[, . . .,][ ˆ]b[푖]푚[ℎ]_ .

17: Compute **_H[ˆ]h_** _푘[푖][ℎ]_ [, the Hessian of the average KL-divergence]i


_퐵_ _푇_

1

_퐵푇_ _푏=1_ _푡=1_ _퐷KL_ _휋[푖]휃[ℎ]푘[푖ℎ]_ (·|표[푖]푡[ℎ] [)][, 휋][푖]휃[ℎ][푖ℎ] [(·|][표][푖]푡[ℎ] [)] .

 

18: Solve the dual (5) for 휆[푖][ℎ] Í Í
∗ [,][ v][푖]∗[ℎ] [.]
Use the conjugate gradient algorithm to compute the update direction

**_x[푖]푘[ℎ]_** [=][ (][ ˆ]H푘[푖][ℎ] [)][−][1][ ]g푘[푖][ℎ] [−] **_B[ˆ]_** _[푖][ℎ]_ **v[푖][ℎ]**,
∗

19: Update agent 푖ℎ’s policy by 

_휃[푖]푘[ℎ]+1_ [=][ 휃][푖]푘[ℎ] [+][ 훼]휆[푖ℎ]∗[푗] **_x[ˆ]_** _[푖]푘[ℎ]_ [,]

where 푗 ∈{0, 1, . . ., 퐿} is the smallest such 푗 which improves the sample loss, and
satisfies the sample constraints, found by the backtracking line search.

20: **if the approximate is not feasible then**

21: Use equation (6) to recover policy 휃[푖]푘[ℎ] 1 [from unfeasible points.]

22: **end if** +

23: Compute 푀 _[푖][1:][ℎ][+][1]_ s, a = _휋휃[푖ℎ]푘[푖ℎ]+1_ (a[푖ℎ] |o[푖ℎ] )
( ) _휋[푖ℎ]_ a[푖ℎ] o[푖ℎ] _[푀]_ _[푖][1:][ℎ]_ [(][s][푡] _[,][ a][푡]_ [)][. //][Unless][ ℎ] [=][ 푛][.]

_휃푘[푖ℎ]_ ( | )

24: **end for**

25: Update V-value network by following formula:

26: _휙푘+1 = arg min휙_ N[1] _푇1_ _푛N=1_ _푡푇=0_ _푉휙_ (푠푡 ) − _푅ˆ푡_ 2

27: end for Í Í   


-----

E MAPPO-LAGRANGIAN

**Algorithm 3: MAPPO-Lagrangian**

1: Input: Stepsizes 훼휃 _, 훼휆, batch size 퐵, number of: agents 푛, episodes 퐾, steps per episode 푇,_
discount factor 훾.

2: Initialize: Actor networks {휃0[푖] _[,][ ∀][푖]_ [∈N}][, Global V-value network][ {][휙][0][}][,]
V-cost networks _휙[푖]푗,0[}][푖]1[∈N]푗_ _푚[푖]_ [, Replay buffer][ B][.]
{ ≤ ≤

3: for 푘 = 0, 1, . . ., 퐾 − 1 do
4: Collect a set of trajectories by running the joint policy πθ푘 = (휋[1]휃푘[1] _[, . . ., 휋][푛]휃푘[푛]_ [)][.]

5:6: Push transitionsSample a random minibatch of {(표[푖]푡 _[, 푎][푖]푡_ _[, 표][푖]푡+1[, 푟] 퐵[푡]_ [)][,]transitions from[ ∀][푖] [∈N] _[, 푡]_ [∈] _[푇]_ [}][ into] B. [ B][.]

7: Compute advantage function _퐴[ˆ]_ (s, a) based on global V-value network with GAE.

8: Compute cost advantage functions _퐴[ˆ][푖]푗_ [(][s][,][ a][푖][)][ for all agents and costs,]
based on V-cost networks with GAE.

9: Draw a random permutation of agents 푖1:푛.

10: Set 푀 _[푖][1]_ (s, a) = _퐴[ˆ]_ (s, a).

11: **for agent 푖ℎ** = 푖1, . . ., 푖푛 **do**

12: Initialise a policy parameter 휃[푖][ℎ] = 휃[푖]푘[ℎ] [,]
and Lagrangian multipliers 휆[푖]푗[ℎ] [=][ 0][,][ ∀] _[푗]_ [=][ 1][, . . ., 푚][푖][ℎ] [.]

13: Make the Lagrangian modification step of objective construction

_푛_
_푀_ _[푖][ℎ]_ _[,]_ [(][휆][)] s푡 _, a푡_ = 푀 _[푖][ℎ]_ s푡 _, a푡_ _휆[푖]푗[ℎ]_ _퐴[ˆ][푖]푗[ℎ]_ [(][s][푡] _[,][ a][푖]푡[ℎ]_ [)][.]
( ) ( ) − _푗=1_

14: **for 푒** = 1, . . ., 푒PPO do Í

15: Differentiate the Lagrangian PPO-Clip objective
Δ휃 _푖ℎ_ =

_휃_ _푖ℎ_ _퐵[1]_ _퐵_ _푇_ min _휋휃[푖ℎ][푖ℎ]_ 푎푡[푖ℎ] [|][표]푡[푖ℎ]  푀 _[푖][ℎ]_ _[,]_ [(][휆][)] _푠푡_ _, a푡_ _, clip_ _휋휃[푖ℎ][푖ℎ]_ 푎푡[푖ℎ] [|][표]푡[푖ℎ] , 1 _휖[ª]_ _푀_ _[푖][ℎ]_ _[,]_ [(][휆][)] _푠푡_ _, a푡_
∇ _푏=1_ _푡=0_ _휋[푖ℎ]_ _푎푡[푖ℎ]_ [|][표]푡[푖ℎ] ( ) _휋[푖ℎ]_ _푎푡[푖ℎ]_ [|][표]푡[푖ℎ] ± ( )[ª]

_휃푘[푖ℎ]_ _휃푘[푖ℎ]_

16: Update temprorarily the actor paramatersÍ Í [©]­   [©]­   ® ®

« _휃[푖][ℎ]_ _휃[푖][ℎ]_ _훼휃_ Δ휃 _푖ℎ«._ ¬ ¬

← +

17: **for 푗** = 1, . . ., 푚[푖][ℎ] **do**

18: Approximate the constraint violation


_푑[푖]푗[ℎ]_ [=]


_푉ˆ_ _푗[푖][ℎ]_ [(][푠][푡] [) −] _[푐][푖]푗[ℎ]_ [.]
_푡=1_

Í


_퐵푇_


_푏=1_

Í


19: Differentiate the constraint


_휋휋휃휃[푖ℎ][푖ℎ]푘[푖ℎ][푖ℎ]_ ([(]푎[푎]푡푡[푖ℎ][푖ℎ] [|][|][표][표]푡푡[푖ℎ][푖ℎ] [)][)] _퐴ˆ[푖]푗[ℎ]_ [(][푠][푡] _[, 푎][푖]푡[ℎ]_ [)][ª]

®
¬


Δ휆[푖]푗[ℎ] [=][ −]퐵[1]


_푑[푖]푗[ℎ]_ [(][1][ −] _[훾][) +]_
_푏=1_

Í ©

­


_푡=0_

Í


20: **end for**

21: **for 푗** = 1, . . ., 푚[푖][ℎ] **do**

«

22: Update temporarily the Lagrangian multiplier


_휆[푖]푗[ℎ]_ [←] [ReLU] _휆[푖]푗[ℎ]_ [−] _[훼][휆][Δ][휆][푖]푗[ℎ]_ .

23: **end for**  

24: **end for**

25: Update the actor parameter 휃[푖]푘[ℎ] 1 [=][ 휃][푖][ℎ] [.]
+

26: Compute 푀 _[푖][ℎ][+][1]_ s, a = _휋휃[푖ℎ]푘[푖ℎ]+1_ (a[푖ℎ] |o[푖ℎ] )
( ) _휋[푖ℎ]_ a[푖ℎ] o[푖ℎ] _[푀]_ _[푖][ℎ]_ [(][s][,][ a][)][. //][Unless][ ℎ] [=][ 푛][.]

_휃푘[푖ℎ]_ ( | )

27: **end for**

28: Update V-value network (and V-cost networks analogously) by following formula:

29: _휙푘+1 = arg min휙_ _퐵푇1_ _푏퐵=1_ _푡푇=0_ _푉휙_ (푠푡 ) − _푅ˆ푡_ 2

30: end for Í Í   


-----

F SAFE MULTI-AGENT MUJOCO

Safe MAMuJoCo is an extension of MAMuJoCo (Peng et al., 2020). In particular, the background
environment, agents, physics simulator, and the reward function are preserved. However, as oppose
to its predecessor, Safe MAMuJoCo environments come with obstacles, like walls or bombs. Furthermore, with the increasing risk of an agent stumbling upon an obstacle, the environment emits
cost (Brockman et al., 2016). According to the scheme from Zanger et al. (2021), we characterise
the cost functions for each task below.

MANYAGENT ANT & ANT

The width of the corridor set by two walls is 9푚(ManyAgent Ant), The width of the corridor set by
three folding line walls with an angle of 30 degrees is 10푚(Ant). The environment emits the cost of
1 for an agent, if the distance between the robot and the wall is less than 1.8푚, or when the robot
topples over. This can be described as

c푡 = 10,, otherwisefor 0.2 ≤ . **_ztorso,푡+1 ≤_** 1.0 and **_xtorso,푡+1 −_** **_xwall_** 2 [≥] [1][.][8]


where ztorso,푡 1 is the robot’s torso’s 푧-coordinate, and xtorso,푡 1 is the robot’s torso’s 푥-coordinate,
+ +
at time 푡 + 1; xwall is the 푥-coordinate of the wall.

Figure 3: ManyAgent Ant 3x2 with a corridor and Ant 4x2 with three corridors.

HALFCHEETAH & COUPLE HALFCHEETAH

In these tasks, the agents move inside a corridor (which constraints their movement, but does not
induce costs). Together with them, there are bombs moving inside the corridor. If an agent finds
itself too close to a bomb, the distance between an agent and a bomb is less than 9푚, a cost of 1 will
be emitted.

c푡 = 10,, otherwisefor **_ytorso ._** _,푡+1 −_ **_yobstacle_** 2 [≥] [9]


where ytorso,푡 1 is the 푦-coordinate of the robot’s torso, and yobstacle is the 푦-coordinate of the
+
moving obstacle.

Figure 4: HalfCheetah 2x3 and Couple HalfCheetah 1P1.


-----

G EXPERIMENTS IN SAFE MANY-AGENT ANT ENVIRONMENTS

We provide additional results on the Safe Many-Agent ant tasks.


The width of the corridor is 12푚; its walls fold at the angle of 30 degrees. The environment emits
the cost of 1 for an agent, if the distance between the robot and the wall is less than 1.8푚, or when
the robot topples over. This can be described as

c푡 = 10,, otherwisefor 0.2 ≤ . **_ztorso,푡+1 ≤_** 1.0 and **_xtorso,푡+1 −_** **_xwall_** 2 [≥] [1][.][8]


where ztorso,푡 1 is the robot’s torso’s 푧-coordinate, and xtorso,푡 1 is the robot’s torso’s 푥-coordinate,
+ +
at time 푡 + 1; xwall is the 푥-coordinate of the wall.

Figure 5: Many-Agent Ant 3x2 with two folding line walls.





ManyAgent Ant 2x3 ManyAgent Ant 3x2 ManyAgent Ant 6x1

300 300 400 HAPPOMAPPO

250 250 300 MACPO (ours)MAPPO-L (ours)

200 200 IPPO

150 150 200

100 100

100

Average Episode Cost 50 Average Episode Cost 50 Average Episode Cost

0 0 0

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

Environment steps 1e7 Environment steps 1e7 Environment steps 1e7

ManyAgent Ant 2x3 ManyAgent Ant 3x2 ManyAgent Ant 6x1

5000 5000 5000 HAPPOMAPPO

4000 4000 4000 MACPO (ours)MAPPO-L (ours)

3000 3000 3000 IPPO

2000 2000 2000

1000 1000 1000

Average Episode Reward 0 Average Episode Reward 0 Average Episode Reward 0

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

Environment steps 1e7 Environment steps 1e7 Environment steps 1e7


Figure 6: Performance comparisons on tasks of Safe ManyAgent Ant in terms of cost (the first row)
and reward (the second row). The safety constraint values is set to 10. Our algorithms are the only
ones that learn the safety constraints, while achieving satisfying performance in terms of the reward.


-----

H DETAILS OF SETTINGS FOR EXPERIMENTS

In this section, we introduce the details of settings for our experiments.
The code is available at [https://github.com/Anonymous-ICLR2022/](https://github.com/Anonymous-ICLR2022/Multi-Agent-Constrained-Policy-Optimisation)
[Multi-Agent-Constrained-Policy-Optimisation](https://github.com/Anonymous-ICLR2022/Multi-Agent-Constrained-Policy-Optimisation)

|hyperparameters value|hyperparameters value|hyperparameters value|
|---|---|---|


|critic lr 5e-3 gamma 0.99 gain 0.01 std y coef 0.5 std x coef 1 activation ReLU|optimizer Adam optim eps 1e-5 hidden layer 1 actor network mlp eval episodes 32 hidden layer dim 64|num mini-batch 40 batch size 16000 training threads 4 rollout threads 16 episode length 1000 max grad norm 10|
|---|---|---|



Table 1: Common hyperparameters used for MAPPO-Lagrangian, MAPPO, HAPPO, IPPO, and
MACPO in the Safe Multi-Agent MuJoCo domain

|Algorithms|MAPPO-Lagrangian MAPPO HAPPO IPPO MACPO|
|---|---|


|actor lr ppo epoch kl-threshold ppo-clip Lagrangian coef Lagrangian lr fraction fraction coef|9e-5 9e-5 9e-5 9e-5 / 5 5 5 5 / / / / / 0.0065 0.2 0.2 0.2 0.2 / 0.78 / / / / 1e-3 / / / / / / / / 0.5 / / / / 0.27|
|---|---|



Table 2: Different hyperparameters used for MAPPO-Lagrangian, MAPPO, HAPPO, IPPO, and
MACPO in the Safe Multi-Agent MuJoCo domain.

|task value|task value|task value|
|---|---|---|


|Ant(2x4) 0.2 HalfCheetah(2x3) 5 ManyAgent Ant(2x3) 1|Ant(4x2) 0.2 HalfCheetah(3x2) 5 ManyAgent Ant(3x2) 1|Ant(2x4d) 0.2 HalfCheetah(6x1) 5 ManyAgent Ant(6x1) 1|
|---|---|---|



Table 3: Safety bound used for MACPO in the Safe Multi-Agent MuJoCo domain


-----

