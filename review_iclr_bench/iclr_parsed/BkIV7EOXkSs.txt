# IMPLICIT REGULARIZATION OF BREGMAN PROXIMAL POINT ALGORITHM AND MIRROR DESCENT ON SEPA## RABLE DATA

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Bregman proximal point algorithm (BPPA), as one of the centerpieces in the optimization toolbox, has been witnessing emerging applications. With simple and
easy to implement update rule, the algorithm bears several compelling intuitions
for empirical successes, yet rigorous justifications are still largely unexplored. We
study the computational properties of BPPA through classification tasks with separable data, and demonstrate provable algorithmic regularization effects associated
with BPPA. We show that BPPA attains non-trivial margin, which closely depends
on the condition number of the distance generating function inducing the Bregman
divergence. We further demonstrate that the dependence on the condition number
is tight for a class of problems, thus showing the importance of divergence in
affecting the quality of the obtained solutions. In addition, we extend our findings
to mirror descent (MD), for which we establish similar connections between the
margin and Bregman divergence. We demonstrate through a concrete example,
and show BPPA/MD converges in direction to the maximal margin solution with
respect to the Mahalanobis distance. Our theoretical findings are among the first
to demonstrate the benign learning properties BPPA/MD, and also provide strong
corroborations for a careful choice of divergence in the algorithmic design.

1 INTRODUCTION
The role of optimization algorithms has become arguably one of the most critical factors in the
empirical successes of training deep models. As the go-to choice for modern machine learning,
first-order algorithms, including (stochastic) gradient descent and their adaptive counterparts (Kingma
and Ba, 2014; Duchi et al., 2011), have received tremendous attention, with detailed investigations
dedicated to understanding the effect of batch size (Goyal et al., 2017; Smith et al., 2018; Keskar
et al., 2016), learning rate (Li et al., 2019; He et al., 2019; Lewkowycz et al., 2020), and momentum
(Sutskever et al., 2013; Smith, 2018) across a broad spectrum of applications.

Meanwhile, Bregman proximal point algorithm (Eckstein, 1993; Kiwiel, 1997) has been drawing
substantial interests. The resounding successes of this classical algorithm are particularly evident
for applications including knowledge distillation (Furlanello et al., 2018), mean-teacher learning
paradigm (Tarvainen and Valpola, 2017), few-shot learning (Zhou et al., 2019), policy optimization
(Green et al., 2019), and fine-tuning pre-trained models (Jiang et al., 2020), yielding competitive
performance compared to its first-order counterparts. In the general form, Bregman proximal point
algorithm updates parameters by minimizing a loss L(·), while regularizing the weighted distance to
the previous iterate measured by some divergence function D(·, ·),

_θt+1 = argminθ L(θ) + 1/(2ηt)D(θ, θt)._ (1.1)

Popular choices of divergence function used in practice include the squared ℓ2-norm distance
_DdivergenceLS(θ, θt) = DKL E(Dθ, θ ∥ftθ) =(x) E −DfKL (θt_ (x)f∥θ2[2]′ ([(Tarvainen and Valpola, 2017), and Kullback-Leibler based]x)∥fθ(x)) (Furlanello et al., 2018), where D denotes the data
distribution. Such a simple update is of great practical purposes, as it is easy to describe, and
admits simple implementation by adopting suitable off-the-shelf black-box optimization algorithms
(Solodov and Svaiter, 2000; Monteiro and Svaiter, 2010; Zaslavski, 2010). The updating form
also suggests plausible intuitions for its empirical successes, including iteratively constraining the
search space, alleviating aggressive updates, and preventing catastrophic forgetting (Schulman et al.,
2015; Li and Hoiem, 2017). However, none of the intuitions have been rigorously justified, and


-----

theoretical understandings for the empirical successes of Bregman proximal point algorithm remains
underexplored.
A first, and a natural question is whether Bregman proximal point algorithm benefits from the same
kind of mechanism that (stochastic) gradient descent (GD/SGD) enjoys for having the generalization
properties. In particular, in many important applications, GD/SGD is widely believed as the “the
algorithm that finds the right kind of solutions” for problems with non-unique solutions. Such a claim
is supported with numerous provable examples: GD/SGD converges to the minimum-norm solution
of under-determined linear systems (Gunasekar et al., 2018), converges to the max-margin solution
for separable data (Soudry et al., 2018; Nacson et al., 2019), aligns layers of deep linear networks (Ji
and Telgarsky, 2018), and converges to a generalizable solution for nonlinear networks (Brutzkus
et al., 2017; Allen-Zhu et al., 2018) in the presence of infinitely many overfitting solutions. Given the
emerging successes of Bregman proximal point algorithm, and the aforementioned evidences on its
first-order counterparts (e.g. GD/SGD) finding generalizable solutions, one would naturally wonder

_Does Bregman proximal point algorithm converge to a solution with favorable qualities?_

Another important question with great practical implications for Bregman proximal point algorithm
is how the divergence measure D(·, ·) affects the solution. Instead of directly applying the Euclidean
distance based divergence, it is widely observed that the successful application of Bregman proximal
point algorithm is contingent on the careful design of divergence measure, based on the task at
hand (Li and Hoiem, 2017; Hinton et al., 2015). Take the example of fine-tuning language model,
the symmetrized Kullback-Leibler based divergence evaluated on the predictions of the updated
model (i.e., θt+1) and previous model (i.e., θt) yields the state-of-the-art result (Jiang et al., 2020).
Identifying the underlying mechanism for the success or failure of a given divergence choice is not
only of theoretical interest, but also can significantly reduce human effort in searching/designing the
suitable divergence for a given task. As an important addition, one may also ask whether the impact of
divergence on the Bregman proximal point algorithm find natural counterparts in commonly adopted
first-order algorithms (e.g. mirror descent, (Nemirovski and Yudin, 1983)). In such cases, better
task-dependent algorithmic designs could be proposed in conjunction with the suitable divergence.
To this end, we raise our second question.

_How does divergence affect the qualities of the solution obtained by Bregman proximal point_
_algorithm (and other first-order algorithms)?_

In this paper, we initiate our study to address our previously proposed questions. We focus on
a non-trivial example of an under-determined system – training linear classifiers using separable
data. In particular, for exponential tail losses (e.g., exponential loss), the empirical loss function has
infimum zero that is asymptotically attainable at infinity along infinitely many directions. The natural
candidate for measuring the quality of the obtained classifier is its margin, i.e., the minimum distance
between the samples and the decision hyperplane. For such a problem, we summarize our theoretical
findings below as concrete answers to the previous questions.

_• We show that Bregman proximal point algorithm (BPPA) obtains a solution with non-trivial margin_
lower-bound. As a concrete demonstration, we tailor our main theorem for Mahalanobis distance,
and show that BPPA converges in direction to the maximal margin solution. We provide nonasymptotic analyses of the margin and empirical loss for constant stepsize BPPA, and propose a
more aggressive stepsize rule for a provable exponential speed-up.

_• We establish a dependence of such a margin lower-bound on the condition number of the distance_
generating function for defining the divergence. In addition, we provide a class of problems where
the margin lower-bound is tight, demonstrating that the Bregman divergence is crucial in affecting
the quality of the obtained solution.

_• We extend our findings to first-order algorithms. Specifically, we show that mirror descent_
(MD) enjoys the same previously mentioned margin properties. We also provide non-asymptotic
convergence analyses of the margin and empirical loss for constant stepsize MD, and its exponential
speed-up using a varying stepsize scheme. Our findings for MD strictly complement prior works
on under-determined regression problems (Gunasekar et al., 2018; Azizan and Hassibi, 2019).

**Notations. We denote [n] := {1, . . ., n}; sgn(z) = 1 if z ≥** 0 and −1 elsewhere. We use w.r.t in
short for “with respect to”. For any in Euclidean space R[d], we use = max _y_ 1 _, y_ to
_∥·∥_ _∥·∥∗_ _∥_ _∥≤_ _⟨·_ _⟩_
denote its dual norm. Note that we have (∥·∥∗)∗ = ∥·∥.


-----

2 PROBLEM SETUP

We study the binary classification on linearly separable data. Specifically, the dataset is S =
(xi, yi) _i=1_
_{exists a linear classifierfu(·) = sgn(}[n]_ _⟨[⊂]u, ·⟩[R])[d] achieves the perfect accuracy on the dataset, with[ × {][+1] u ∈[,][ −]R[1][}][d][, where]such that[ x][i] y[ is the feature vector, and]i ⟨u, xi⟩_ _> 0 for all i ∈[ y][i] y[[ is the label. In addition, there]ni] =. That is, the decision rule fu(xi) for all i ∈_ [n].

For each linear classifier fu( ) with perfect accuracy, we define its -norm margin as the minimum

_·_ _∥·∥∗_
distance in ∥·∥∗-norm from the feature vectors to the decision boundary Hu = {x : ⟨x, u⟩ = 0}. It is
well known that the -norm margin, denoted as γu, only depends on the direction of the classifier
_∥·∥∗_

and satisfies γu = mini∈[n] _xiyi,_ _∥uu∥_, where ∥·∥ is the dual norm of ∥·∥∗. The ∥·∥∗-norm margin

measures how well the data is separated by decision ruleD E _fu(_ ), measured in -norm, and is an

_·_ _∥·∥∗_
important measure on the generalizability and robustness of the decision rule. Given a norm ∥·∥∗ on
R[d], we define the optimal linear classifier with the maximum ∥·∥∗-margin below.
**Definition 2.1 (Maximum ∥·∥∗-norm Margin Classifier). Given a linearly separable dataset**
_{(xi, yi)}i∈[n], we define the maximum ∥·∥∗-norm margin classifier u∥·∥∗, and its associated maxi-_
_mum_ _-norm margin γ_ _as_
_∥·∥∗_ _∥·∥∗_

_u_ = argmax min _γ_ = max
_∥·∥∗_ _∥u∥≤1_ _i∈[n]_ _[⟨][u, y][i][x][i][⟩]_ _[,]_ _∥·∥∗_ _∥u∥≤1_ _i[min]∈[n]_ _[⟨][u, y][i][x][i][⟩]_ _[.]_

For a separable dataset, we consider finding the classifier by minimizing the empirical loss

_LS_ (θ) = _n[1]_ _ni=1_ _[ℓ]_ [(][⟨][θ, y][i][x][i][⟩][)][ .] (2.1)

Here we focus on the exponential loss ℓ(x) = exp( _x), and our analyses can be readily extended to_

P

_−_
other losses with tight exponential tail (e.g., logistic loss).

**Observation. One can readily verify that with a separable dataset S, the empirical loss has infimum 0**
but possesses no finite solution that attains the infimum. Thus any optimization algorithm minimizing
the loss L ( ) will observe the explosion on the norm of iterate.
_S_ _·_

It has been shown that various optimization algorithms, including (stochastic) gradient descent and
steepest descent, converge in direction to the maximum margin classifier in different norms (Soudry
et al., 2018; Nacson et al., 2019; Gunasekar et al., 2018; Ji and Telgarsky, 2019; 2021). Connections
between gradient descent and the regularization path of homotopy method have also been established
(Ji et al., 2020). A striking feature behind such phenomena is that there is no explicit regularization
in the loss function, and such effects have been termed as the implicit (algorithmic) regularization.

Up to date, most of the implicit regularization effects are attributed to (stochastic) gradient descent,
given their prevalence in applications. However, as Bregman proximal point algorithm (BPPA)
becomes increasingly popular in various domains, there exists considerable lack of understanding on
the computational properties of BPPA. In addition, practitioners often find the choice of divergence
function crucially important for the performance of BPPA (Jiang et al., 2020; Furlanello et al.,
2018). This empirical evidence thus calls for a detailed characterization on the connection between
computational properties and the divergence function of BPPA.

In what follows, we study the BPPA for solving problem (2.1) in detail. The BPPA (Algorithm 1) is an
adaptation of the vanilla proximal point algorithm (Rockafellar, 1976a;b) to non-euclidean geometry,
by using Bregman divergence as the divergence measure in (1.1). Specifically, given a distance
generating function w( ) that is convex and differentiable, we define the Bregman divergence Dw( _,_ )

_·_ _·_ _·_
associated with w( ) as Dw(θ, θ[′]) = w(θ) _w(θ[′])_ _w(θ[′]), θ_ _θ[′]_ _. Throughout our discussions,_

_·_ _−_ _−⟨∇_ _−_ _⟩_
we only impose the following mild assumption on Bregman divergence function Dw( _,_ ).

_·_ _·_

**Algorithm 1 Bregman Proximal Point Algorithm (BPPA)**


**Input: Distance generating function w(** ), stepsizes _η[t]_ _t_ 0, samples _xi, yi_ _i=1[.]_

_·_ _{_ _}_ _≥_ _{_ _}[n]_

**Initialize: θ[0]** _←_ 0.
**for t = 0, . . . do**

Update θt+1 = argmin _L_ (θ) + [1] _Dw(θ, θt)._ (2.2)

**end for** _θ_ _S_ 2ηt


-----

**Assumption 1. We assume that the distance generating function of Bregman divergence Dw(·, ·) is**
_Lw-smooth and µw-strongly convex w.r.t._ _-norm. That is,_
_∥·∥_
_µw_

2 2

_[∥][θ][ −]_ _[θ][′][∥][2][ ≤]_ _[w][(][θ][)][ −]_ _[w][(][θ][′][)][ −⟨∇][w][(][θ][′][)][, θ][ −]_ _[θ][′][⟩≤]_ _[L][w]_ _[∥][θ][ −]_ _[θ][′][∥][2][ .]_

3 ALGORITHMIC REGULARIZATION OF BPPA

We show BPPA achieves a -norm margin that is at least _µw/Lw-fraction of the maximal one._
_∥·∥∗_

**Theorem 3.1 (Constant Stepsize BPPA). Let D** = maxi [n] _xi_ _, where_ _denotes the dual_
_∥·∥∗_ _∈p_ _∥_ _∥∗_ _∥·∥∗_
_norm of ∥·∥. Then under Assumption 1, for any constant stepsize ηt = η > 0, the following hold._

_(1) We have limt→∞_ _LS_ (θt) = 0. Specifically, we have that LS (θt) diminishes at the following rate,

1 _γ_ _ηt_ _Lw log[2][  ]γ_ _ηt_
_LS_ (θt) ≤ _γ∥·∥∗_ _ηt_ [+][ L][w][ log]4γ[2]∥·∥[2][  ] _∗∥·∥[ηt]_ _∗_  = O _γ∥·∥[2]_ _∗_ _[ηt]∥·∥∗_  ! _._

_(2) We have that the margin is asymptotically lower bounded by_

_θt_ _µw_

lim _γ_ _,_ (3.1)
_t→∞_ _i[min]∈[n]_ _∥θt∥_ _[, y][i][x][i]_ _≥_ r _Lw_ _∥·∥∗_

_where γ_ _is defined in Definition 2.1. In addition, for any given ϵ > 0, there exists a t0 satisfying_
_∥·∥∗_

_t0 :=_ max _D∥·∥2_ _∗_ _, exp_ _D∥·∥2_ _∗_ _Lw_ 1 _,_
_O_  _ϵ[2]γ∥·∥[2]_ _∗_ _γ∥·∥[2]_ _∗_ _[ϵ][2]_ s _µw_ ! _γ∥·∥[2]_ _∗_ _[η]_ [!]

_such that for t_ _t0 number of iterations, we have_
_≥_ [e]
_θt_ _µw_

(1 _ϵ)_ _γ_ _,_ _i_ [n].

_θt_ _≥_ _−_ _Lw_ _∥·∥∗_ _∀_ _∈_

 _∥_ _∥_ _[, y][i][x][i]_ r

We highlight that (1) The choice of Bregman divergence in BPPA is flexible and can be data
_dependent. Properly chosen data-dependent divergence can adapt to data geometry much better than_
data-independent divergence, leading to better separation and margin. In Section 5 we demonstrate
how BPPA can benefit significantly from such an adaptivity of carefully designed data-dependent
divergence. (2) Our analysis on the convergence requires handling non-finite minimizers, which
implies divergence of iterate _θt_ . The optimization problem of our interest does not meet the
_∥_ _∥→∞_
standard assumptions in the classical analysis of BPPA in the literature, and requires a careful choice
of reference point in order to derive non-trivial convergence results. (3) Our result is closely related,
but should not be confused with the homotopy method in (Rosset et al., 2004), which can be viewed
as performing only one proximal step at the origin, with an extremely large stepsize. (4) Finally, our
result is a generalization of Telgarsky (2013); Gunasekar et al. (2018) to non-euclidean settings with
Bregman divergence. Working with Bregman divergence poses unique challenges, as it is previously
unclear how to relate the primal margin progress to the per-iteration progress over the dual space.

Theorem 3.1 shows that if the distance generating function w(·) is well-conditioned w.r.t. ∥·∥-norm,
then Bregman proximal point algorithm will output a solution with near optimal ∥·∥∗-norm margin.
As a concrete realization of Theorem 3.1, we consider the Mahalanobis distance ∥·∥A := _⟨·, A·⟩_

induced by a positive definite matrix A.

p

**Corollary 3.1. Let ∥·∥** = ∥·∥A for some positive definite matrix A. Under the same conditions as
_in Theorem 3.1, BPPA with distance generating function w(·) = ⟨·, A·⟩_ _converges to the maximum_
_-margin solution, where_ = _A−1_ _. Specifically, we have_
_∥·∥∗_ _∥·∥∗_ _∥·∥_

1 _γ_ _ηt_ _Lw log[2][  ]γ_ _ηt_
_LS_ (θt) ≤ _γ∥·∥∗_ _ηt_ [+][ L][w][ log]4γ[2]∥·∥[2][  ] _∗∥·∥[ηt]_ _∗_  = O _γ∥·∥[2]_ _∗_ _[ηt]∥·∥∗_  ! _._

_In addition, we have limt→∞_ mini∈[n] _∥θθtt∥_ _[, y][i][x][i]_ = γ∥·∥∗. Specifically, for any given ϵ > 0,

_there exists a t0 satisfying t0 :=_ maxD _ϵD[2]γ∥·∥∗[2][2]_ _,E exp_ _γD[2]_ _∥·∥∗[2]_ _γ[2]_ 1 _, such that for t_ _t0_
_O_ _≥_
  _∥·∥∗_  _∥·∥∗_ _[ϵ][2]_  _∥·∥∗_ _[η]_ 

_number of iterations, we have_
_θt[e]_

(1 _ϵ)γ_ _,_ _i_ [n].

_θt_ _≥_ _−_ _∥·∥∗_ _∀_ _∈_

_Finally, we have the direction convergence that_ _∥_ _∥_ _[, y][i][x][i]_ limt→∞ _∥θθtt∥_ [=][ u][∥·∥]∗ _[.]_


-----

Note that similar directional convergence results have been shown in Gunasekar et al. (2018) for
steepest descent w.r.t. ∥·∥A norm. The directional convergence of BPPA obtained here, however, is
not a simple corollary of known results, since existing analyses focus on first-order algorithms in
euclidean setting (e.g., GD/SGD, steepest descent). Such existing analyses do not simply extend to
non-first-order algorithms in non-euclidean setting, such as BPPA.

When the distance generating function w( ) is ill-conditioned w.r.t. -norm (i.e., _µw/Lw_ 1),

it might be tempting to suggest that the margin lower bound in· (3.1) is loose, and what really happens ∥·∥ _≪_
is limt→∞ mini∈[n] _∥θθtt∥_ _[, y][i][x][i]_ = γ∥·∥∗ . However, as we show in the following proposition, therep

exists a class of problems, where the lower bound inD E (3.1) is in fact a tight upper bound (up to a factor
of 2), demonstrating that the dependence on condition number of distance generating function w(·)
w.r.t. ∥·∥-norm is not a proof artifact.
**Proposition 3.1 (Tight Dependence on Condition Number). There exists a sequence of problems**
_{P_ [(][m][)]}m≥1, where each P [(][m][)] = _S_ [(][m][)], ∥·∥[(][m][)] _, w[(][m][)][]_ _denotes the dataset, the norm, and the_
_distance generating function of the m-th problem. For each m, the distance generating function_
_w[(][m][)](·) is µ[(]w[m][)][-strongly convex and][ L]w[(][m][)][-smooth w.r.t.][ ∥·∥][-norm. Then Bregman proximal point]_
_algorithm applied to each problem in_ _m_ 1 yields
_{P_ [(][m][)]} _≥_

_θt_ _µ[(]w[m][)]_

lim min _γ_ (m) 2 _,_ _m_ 1, (3.2)
_t→∞_ (x,y)∈S [(][m][)] _∥θt∥_ _[, yx]_ _∥·∥∗_ _≤_ s _L[(]w[m][)]_ _∀_ _≥_



_θt_ _µ[(]w[m][)]_
_In addition, for any m ≥_ 4, we have limt→∞ min(x,y)∈S (m) _∥θt∥_ _[, yx]_ _γ∥·∥(∗m)_ _≤_ 2 _L[(]w[m][)]_ _< 1._

r

_In fact,_ D E 

_θt_

lim min _γ_ (m) 0, _as m_ _._ (3.3)
_t→∞_ (x,y)∈S [(][m][)] _∥θt∥_ _[, yx]_ _∥·∥∗_ _→_ _→∞_



Combine Theorem 3.1, Corollary 3.1 and Proposition 3.1, we conclude that the margin of the obtained
solution by BPPA has non-trivial dependence on the condition number of the distance generating
function w( ). This observation provides a strong evidence that the Bregman divergence Dw( _,_ )

_·_ _·_ _·_
in BPPA is highly important to the quality of the obtained solution, and advocates a careful design
of Bregman divergence when using the BPPA. Our theoretical findings also aligns the empirical
evidences on the importance of divergence found in knowledge distillation and model fine-tuning
(Jiang et al., 2020; Furlanello et al., 2018).

We have shown that BPPA with constant stepsize achieves a margin that is at least _µw/Lw-fraction_

of the maximal one. Meanwhile, our complexity bound in Theorem 3.1 shows that to obtain such a

p

margin lower bound, it might take an exponential number of iterations. We next show by employing a
more aggressive stepsize scheme, we can attain the same margin lower bound in a polynomial number
of iterations, while speeding up the convergence of the empirical loss _L_ (θt) _t_ 0 drastically.
_{_ _S_ _}_ _≥_
**Theorem 3.2 (Varying Stepsize BPPA). Given any positive sequence** _αt_ _t_ 0, letting the stepsizes
_αt_ _{_ _}_ _≥_
_{ηt}t≥0 be ηt =_ _LS_ (θt) _[, then the following facts hold.]_

_(1) limt→∞_ _LS_ (θt) = 0. Specifically, for any t ≥ 0, we have LS (θt+1) ≤ _LS_ (θt)β(αt), where

2αβ[2]γ[2]
_β(α) = minβ∈(0,1) max_ _β, exp_ _−_ _Lw∥·∥∗_ _< 1._
   

_(2) Letting αt =_ _√t1+1_ _, we have limt→∞_ mini∈[n] _∥θθtt∥_ _[, y][i][x][i]_ _≥_ _Lµww_ _[γ][∥·∥]∗_ _[. In particular, for any]_

_ϵ ∈_ (0, [1]2 [)][, there exists a][ t][0][ satisfying] D E q

8[]

_Lw_

_t0 =_ _,_ (3.4)
_O_  _γ∥·∥∗_ _√µwϵ_ !

_such that in t ≥_ _t0 number of iterations, we have_ 
_θt_ _Lw_

(1 _ϵ)_ _γ_ _,_ _i_ [n].

_θt_ _≥_ _−_ s _µw_ _∥·∥∗_ _∀_ _∈_

 _∥_ _∥_ _[, y][i][x][i]_


-----




_γ[2]_
_Additionally, the convergence rate of {LS_ (θt)}t≥0 is given by LS (θt) = O exp _−_ _L∥·∥∗w_
 


We remark that (1) We do not optimize for the best polynomial dependence on 1/ϵ in the iteration
complexity (3.4), as our main goal is to show the exponential gap between the complexity presented
in Theorem 3.1 and here. We refer interested readers to Appendix B, where we show that we can
improve the polynomial dependence with more tailored analysis. (2) We also demonstrate that the
empirical loss converges almost exponentially faster with our choice of stepsizes. (3) We reiterate
that using the aggressive stepsizes does not change our established margin lower bound, and the
exact convergence to the maximum margin solution demonstrated in Corollary 3.1 still holds for this
scheme of stepsizes, which can be achieved with a polynomial number of iterations.

_• Inexact Implementation of BPPA. The proximal update (2.2) requires solving a non-trivial_
optimization problem, and there has been fruitful results of inexact implementation of BPPA in
optimization literature (Rockafellar, 1976b; Yang and Toh, 2021; Solodov and Svaiter, 2000; Monteiro
and Svaiter, 2010). Here based on the varying stepsize scheme proposed in Theorem 3.2, we discuss
the feasibility of a gradient descent based inexact BPPA that: (1) admits a simple implementation
and achieves polynomial complexity, (2) retains the margin properties of exact BPPA. Specifically, at
the t-th iteration, the gradient descent based inexact BPPA solves the proximal step
_θt+1 ≈_ argminθ φt(θ) := _n[1]_ _ni=1_ [exp (][−⟨][θ, y][i][x][i][⟩][) +] 21ηt _[D][w][(][θ,][ b]θt)_ (3.5)

up to a pre-specified accuracy δt with gradient descent. Our key observation comes from the factP
that when applying gradient descent tob _φt(_ ) with small enough stepsizes, the iterate would stay in a

_·_
region that has relative smoothness Mt and relative strong convexity µt bounded by

_Mt ≤_ _LS_ (θ[b]t) + _η[1]t_ [=][ L][S] [(]θ[b]t) 1 + _α1t_ _,_ _µt ≥_ _η1t_ [=][ L]α[(]θ[b]tt) _[,]_

both measured w.r.t. Bregman divergence Dh( _,) (Lu et al., 2018). Note that the first inequality_

_·_ _·_
follows by our choice of stepsize ηt in Theorem 3.2. Thus the effective condition number κt := Mt/µt
of φt(·) is bounded by κt = 1 + αt = O(1), which implies that the t-th proximal step requires

_O_ _κt log(_ _δ[1]t_ [)] = O log( _δ[1]t_ [)] number of gradient descent steps. Summing up across t0 iterations

(3.4), we need up to _O_ _tt0=1[log(][ 1]δt_ [)] gradient descent steps, which depends polynomially on t0

even if we choose an extremely high accuracy _δt =_ (exp( _t)) for each inexact proximal step (3.5)._

P _O_ _−_

4 ALGORITHMIC REGULARIZATION OF MIRROR DESCENT

Inspired by the results in the previous section, we further show that mirror descent (MD, Algorithm 2),
as a generalization of gradient descent to non-euclidean geometry, possesses similar connection between the margin and Bregman divergence. We remark that our results are the first to characterize the
algorithmic regularization effect of MD for classification tasks, while previous literature exclusively
focus on under-determined regression problems (Gunasekar et al., 2018; Azizan and Hassibi, 2019).

**Algorithm 2 Mirror Descent Algorithm (MD)**

**Input: Distance generating function w(** ), stepsizes _η[t]_ _t_ 0, samples _xi, yi_ _i=1[.]_

_·_ _{_ _}_ _≥_ _{_ _}[n]_

**Initialize: θ[0]** _←_ 0.
**for t = 0, . . . do**

Compute gradientUpdate θt+1 = argmin ∇LSθ( ⟨∇θt) =LS (n[1]θtP), θni=1 −[exp (]θt⟩ +[−⟨]21η[θ]t[t][D][, y][w][i][x][(][θ, θ][i][⟩][) (][t][)][−][.] _[y][i][x][i][)][.]_

**end for**


**Theorem 4.1 (Constant Stepsize MD). Let D∥·∥∗** = maxi∈[n] ∥xi∥∗, where ∥·∥∗ _denotes the dual_
_norm of ∥·∥, and D∥·∥2 = maxi∈[n] ∥xi∥2. Under Assumption 1, let µ2 be the strong convexityµ2_
_parameter of w(·) w.r.t. ∥·∥2-norm. Then for any constant stepsize ηt = η ≤_ 2D∥·∥2 _[, we have that]_

_(1) limt→∞_ _LS_ (θt) = 0. Specifically, we have that LS (θt) diminishes at the following rate,

1 _γ_ _ηt_ _Lw log[2][  ]γ_ _ηt_
_LS_ (θt) ≤ _γ∥·∥∗_ _ηt_ [+][ L][w][ log]4γ[2]∥·∥[2][  ] _∗∥·∥[ηt]_ _∗_  = O _γ∥·∥[2]_ _∗_ _[ηt]∥·∥∗_  ! _._


-----

_(2) We have that the margin is asymptotically lower bounded by_

_θt_ _µw_

lim _γ_ _._
_t→∞_ _i[min]∈[n]_ _∥θt∥_ _[, y][i][x][i]_ _≥_ r _Lw_ _∥·∥∗_

_In addition, for any ϵ > 0, there exists a t0 satisfying_

2 _[L][w][η]_ 1

_t0 =_ exp _∥·∥∗_ _[D][∥·∥]_ _,_ (4.1)
_O_   _γ[2]_ _w_ _[µ][3]2[/][2]ϵ[3][/][2][ log]_  _ϵ_ []

_∥·∥∗_ _[µ][1][/][2]_

_such that any t_ _t0, we have_   _[D][3][/][2]_ 
_≥_
_θt_ _µw_

(1 _ϵ)_ _γ_ _,_

_θt_ _≥_ _−_ _Lw_ _∥·∥∗_

 _∥_ _∥_ _[, y][i][x][i]_ r

Theorem 4.1 shows that mirror descent attains the same ∥·∥∗-norm margin lower bound as BPPA,
which is _µw/Lw-fraction of the maximal margin. Note that µ2 > 0 is a direct consequence of_

Assumption 1 and the equivalence of norm on finite-dimensional vector space.

p

Similar to Corollary 3.1, let ∥·∥ = ∥·∥A be the Mahalanobis distance, then MD equipped with
distance generating function w(·) = ⟨·, A·⟩ converges to the maximum ∥·∥∗-norm margin classifier.
**Corollary 4.1. Let ∥·∥** = ∥·∥A for some positive definite matrix A. Then under the same conditions as
_in Theorem 4.1, the MD with distance generating function w(·) = ⟨·, A·⟩_ _converges to the maximum_

_Lw log[2](γ_ _ηt)_
_∥·∥∗-margin solution, where ∥·∥∗_ = ∥·∥A−1 _. Specifically, we have LS_ (θt) = O _γ[2]_ _∥·∥∗_ _._
 _∥·∥∗_ _[ηt]_ 

_In addition, we have limt→∞_ mini∈[n] _∥θθtt∥_ _[, y][i][x][i]_ = γ∥·∥∗. Specifically, for any given ϵ > 0,

_DD[3][/][2]_ 2 _[L][w]E[η]_ 1
_there exists a t0 with t0 = O_ exp _γ[2]_ _∥·∥∗w[D][∥·∥]µ[3]2[/][2]ϵ[3][/][2][ log]_ _ϵ_ _, such that for t ≥_ _t0 number of_
  _∥·∥∗_ _[µ][1][/][2]_

_iterations, we have_   []
_θt_

(1 _ϵ)γ_ _,_ _i_ [n].

_θt_ _≥_ _−_ _∥·∥∗_ _∀_ _∈_

_Finally, we have direction convergence that_ _∥_ _∥_ _[, y][i][x][i]_ limt→∞ _∥θθtt∥_ [=][ u][∥·∥]∗ _[.]_

Note that Corollary 4.1 recovers the directional convergence of steepest descent in Gunasekar et al.
(2018) w.r.t _A, which coincides with MD with distance generating function w(_ ) = _, A_ .
_∥·∥_ _·_ _⟨·_ _·⟩_

Theorem 4.1 guarantees the near optimal ∥·∥∗-norm margin when the distance generating function
_w(·) is well-conditioned w.r.t. ∥·∥-norm. For cases when w(·) is ill-conditioned, we demonstrate that_
there exists a class of problem for which the margin lower bound is tight.
**Proposition 4.1. There exists a sequence of problems** _m_ 1 by the same construction as in
_{P_ [(][m][)]} _≥_
_Proposition 3.1, such that the margin lower bound in Theorem 4.1 is tight up to a non-trivial factor of_
2. Specifically, we have (3.2) and (3.3) also hold for MD.

Finally, we propose a more aggressive stepsize scheme for MD that achieves the same margin lower
bound. In addition, instead of requiring an exponential number of iterations (4.1) as constant stepsize
MD, such a stepsize scheme only needs a polynomial number of iterations, and achieves an almost
**Theorem 4.2exponential speedup for the empirical lossαt = min{** 2Dµ (Varying Stepsize MD)∥·∥2 2 _[,]_ _√t1+1_ _}. Then under the same conditions as in Theorem 4.1,. Let the stepsizes {LS_ (θt)}t≥0. _{ηt}t≥0 be given by ηt =_ _LSα(tθt)_ _[, where]_

_(1) We have limt→∞_ mini∈[n] _∥θθtt∥_ _[, y][i][x][i]_ _≥_ _Lµww_ _[γ][∥·∥]∗[. In addition, for any][ ϵ >][ 0][, there exists a]_

_t0 satisfying t0 =_ _γ_ _D∥·∥µ2D2[√]Lµwwϵ_ 4[] _,E such that for anyq_ _t_ _t0, we have_
_O_ _∥·∥∗_ _≥_
 

_θt_ _µw_

(1 _ϵ)_ _γ_ _,_ _i_ [n].

_θt_ _≥_ _−_ _Lw_ _∥·∥∗_ _∀_ _∈_

 _∥_ _∥_ _[, y][i][x][i]_ r

_(2) We have limt→∞_ _LS_ (θt) = 0. In addition, the convergence rate is given by

_γ[2]_

_LS_ (θt) = O exp _−_ _L∥·∥w∗_ _√t!!_ _._


-----

5 EXPERIMENTS

**Synthetic Data. We take S = {((−0.5, 1), +1), ((−0.5, −1), −1), ((−0.75, −1), −1), ((2, 1), +1)}.**
One can readily verify that the maximum ∥·∥2-norm margin classifier is u∥·∥2 = (0, 1). For both
BPPA and MD, we take the Bregman divergence as Dw(x, y) = ∥x − _y∥2[2][, which corresponds to]_
the vanilla proximal point algorithm and gradient descent algorithm. Note that both algorithms are
guaranteed to converge in direction towards u∥·∥2 = (0, 1), following Corollary 3.1 and 4.1.

|Col1|Col2|
|---|---|


(a) BPPA


(b) MD


_i2_ _i2_

)✓(Lt _kkh, u✓/✓ttk·k_ )✓(Lt _kkh, u✓/✓ttk·k_


Figure 1: BPPA and MD run on the simple data set S.

We take ηt = η = 1 for the constant stepsize BPPA/MD, and ηt = _L(θt)1[√]t+1_ [for the varying]

stepsize BPPA/MD, following the stepsize choices in Theorem 3.1, 3.2, 4.1 and 4.2. To implement
the proximal step in BPPA at the t-th iteration, we take 128 number of gradient descent steps with
stepsize 0.2ηt, following our discussion at the end of Section 3. We initialize all algorithms at the
origin and run 1200 iterations. From Figure 1, we can clearly observe that both BPPA and MD
converge in direction to the maximum 2-norm margin classifier u 2, which is consistent with our
_∥·∥_ _∥·∥_
theoretical findings. In addition, by adopting the varying stepsize scheme proposed in Theorem 3.2
and 4.2, both BPPA and MD converge exponentially faster than their constant stepsize counterparts.

**Data-dependent Bregman Divergence. We illustrate through an example on how properly chosen**
_data-dependent divergence can lead to much improved separation compared to data-independent_
divergence, even on simple linear models.

We have n labeled data {(xi, yi)}i[m]=1 [sampled from a mixture of sphere distribution:][ y][i][ ∼]
Bernoulli(1/2), xi ∼ Unif (Syiµ(r)), where Sz(r) denotes the sphere centered at z with radius
_r in R[d]. In addition, we also have m unlabeled data {xj}j[m]=1[, following the same distribution as]_
_{xi}i[n]=1[, with no labels given. Clearly, the maximum][ ∥·∥]2[-margin classifier for the mixture of sphere]_
distribution considered here is given by the linear classifiere _f_ _[∗](·) = sign(⟨·, µ⟩)._


|Divergence|Alignme|
|---|---|
|D(1)(, ) · ·|0.8703|
|D(2)(, ) · ·|0.8175|
|D(3)(, ) · ·|0.9754|


Divergence Alignment

_D[(1)](·, ·)_ 0.8703

_D[(2)](·, ·)_ 0.8175

_D[(3)](·, ·)_ 0.9754

Table 1: _∥θθ[T][T]∥2_ _[, µ]_ av
eraged over 8 runs.D E


Figure 2: BPPA with Bregman divergence D[(3)] (right) significantly

2 sgn(Unlabeled, x ) 2 sgn(Unlabeled, x ) 2 sgn(Unlabeled, x )

Positive Positive Positive

1 Negative 1 Negative 1 Negative

0 0 0

1 1 1

2 2 2

2 1 0 1 2 2 1 0 1 2 2 1 0 1 2

improves alignment with optimal classifier µ, compared to D[(1)] (left)
and D[(2)] (middle).


We choose n = d = 2, m = 100, r = 0.8, and generate µ ∼ Unif (S0(1)). We compare three types
(of Bregman divergence, given byθ − _θ[′])[⊤]Σ([b]_ _θ −_ _θ[′]), and D[(3)](θ, θ D[′]) = ([(1)](θ, θθ −[′]) =θ[′])[⊤] ∥Σ[b]θ[−] −[1](θθ[′] −∥2[2]_ _θ[(vanilla proximal point),][′]), where_ Σ = _m1_ _mj=1[ D][x][j][(2)][x]j[⊤][(][θ, θ][denotes][′][) =]_

the empirical covariance matrix. Note that D[(2)] and D[(3)] are data-dependent from their construction.
P
For each divergence function, we run BPPA with 8 independent runs, the results are reported in[b]
Figure 2 and Table 1. We make two important remarks on the empirical results:

_• Data-dependent divergence D[(3)]_ gives the best separation despite limited labeled data (in fact only
2!), much improved over data-independent squared ℓ2-distance D[(1)].

_• Not all data-dependent divergence helps, D[(2)]_ shows degradation compared to D[(1)].

We further remark that by utilizing Corollary 3.1, one can completely characterize the solution
obtained by BPPA for each of the divergence in closed form. Using such a characterization allows
one to corroborate the empirical phenomenon with our developed theories, deferred in Appendix A.


-----

**CIFAR-100. We demonstrate the potential of extending our theoretical findings for linear models**
to practical networks, using ResNet-18 (He et al., 2016), ShuffleNetV2 (Ma et al., 2018), MobileNetV2 (Sandler et al., 2018), with CIFAR-100 dataset (Krizhevsky et al., 2009). At each
iteration of BPPA, the updated model parameter θt+1 is given by solving the proximal step
_θt+1 = argminθ 1/n_ _i=1_ _[ℓ][(][f][θ][(][x][i][);][ y][i][) + 1][/][(2][η][t][)][D][(][θ][;][ θ][t][)][ for all][ t][ ≥]_ [0][, where][ D][ denotes di-]
vergence function, and θ0 is obtained by standard training with SGD. We consider inexact implementation of the proximal step, discussed in (3.5). Specifically, each proximal step is solved by

[P][n]
using SGD, with a batch size of 128, an initial learning rate of 0.1 which is subsequently divided
by 5 at the 60th, 120th, and 160th epoch. We consider two divergence functions widely used in
practice, defined byand DKL(θ, θ[′]) = 1 D/(2LSn()θ[′], θi=1) = 1[KL (]/(2[f][θ]n[′])[(][x][i][)]i[∥]=1[f][θ][∥][(][f][x][θ][i][))][(][x][ (Furlanello et al., 2018). For each of the diver-][i][)][ −] _[f][θ][′]_ [(][x][i][)][∥]2[2] [(Tarvainen and Valpola, 2017),]
gence, we run BPPA with 3 proximal steps, with the proximal stepsize[P][n] _ηt = η = 0.025 for DKL, and_
_ηt = η = 0.2 for DLS (ηt = 0.025 gives significantly worse performance). For standard training_

[P][n]
with SGD, we use a batch size of 128, an initial learning rate of 0.1 further divided by 5 at the 60th,
120th, and 160th epoch. The results are reported in Figure 3.





ResNet18


77.5

75.0

72.5

70.0

67.5


|Col1|SGD KL-Prox KL-Prox KL-Prox LS-Prox LS-Prox LS-Prox|
|---|---|


SGD
KL-Prox-1
KL-Prox-2
KL-Prox-3
LS-Prox-1
LS-Prox-2
LS-Prox-3

ResNet18 MobileNetV2 ShuffleNetV2

75 70 70

70 60 65

65 50 60

Test Accuracy60 SGD Test Accuracy SGD Test Accuracy55 SGD

55 KL-Prox-3 40 KL-Prox-3 50 KL-Prox-3

LS-Prox-3 LS-Prox-3 LS-Prox-3

50 30

50 100 150 200 50 100 150 200 50 100 150 200

Epoch Epoch Epoch

Figure 3: BPPA with divergences DKL and DLS on CIFAR-100 dataset. KL-Prox-k denotes learning
curve of the k-th proximal step with DKL; LS-Prox-k denotes learning curve of the k-th proximal
step with DLS.
One can clearly see from Figure 3: (1) Across different model architectures, BPPA with DKL
outperforms standard training with SGD; (2) BPPA with DLS yields negligible differences compared
to SGD. The qualitative difference of DKL and DLS strongly indicates that the divergence function
serves an important role in affecting the model performance learned by BPPA, which we view as an
important evidence showing broader applicability of our developed divergence-dependent margin
theories. In addition, the learned model with DKL improves gradually w.r.t the total number of
proximal steps. For ResNet-18, the accuracy increases from 75.83% (standard training) to 78.56%
after 3 proximal steps – an additional 1.4% improvement over Tf-KDself (see Table 2), which can be
viewed as BPPA with 1 proximal step. We view such findings as the evidence suggesting the scope of
algorithmic regularization associated with BPPA goes beyond simple linear models.


We make further remarks on the previously proposed
method in Yuan et al. (2019), named Teacher-free
Knowledge Distillation via self-training (Tf-KDself ),
which is equivalent to BPPA with 1 proximal steps, using DKL(θ, θ[′]) as the divergence function. Tf-KDself
was shown to improve over SGD for various network
architectures on CIFAR-100 and Tiny-ImageNet. We
include the reported results on CIFAR-100 therein in
Table 2 for completeness.

6 CONCLUSION AND FUTURE DIRECTION


|Model|SGD|Tf-KD self|
|---|---|---|
|MobileNetV2|68.38|70.96 (+2.58)|
|ShuffleNetV2|70.34|72.23 (+1.89)|
|ResNet18|75.87|77.10 (+1.23)|
|GoogLeNet|78.72|80.17 (+1.45)|
|DenseNet121|79.04|80.26 (+1.22)|


Model SGD Tf-KDself

MobileNetV2 68.38 70.96 (+2.58)

ShuffleNetV2 70.34 72.23 (+1.89)

ResNet18 75.87 77.10 (+1.23)

GoogLeNet 78.72 80.17 (+1.45)

DenseNet121 79.04 80.26 (+1.22)

Table 2: Comparison of Tf-KDself (2-step
BPPA) and SGD on CIFAR-100.


To conclude, we have shown that for binary classification task with linearly separable data, the
Bregman proximal point algorithm and mirror descent attain a ∥·∥∗-norm margin that is closely
related to the condition number of the distance generating function w.r.t. ∥·∥-norm. We list two
directions worthy of future investigations. (1) Our analyses exploit the fact that the Bregman
divergence is defined over the model parameters, while many popular data-dependent divergences
are defined over the model output (e.g. prediction confidence). Making this non-trivial extension to
data-dependent divergence can also help demystify the mechanism of the data-dependent divergence.
(2) Our current analyses focus on linear models, and the extension to nonlinear neural networks
requires more delicate definitions of margin and divergence. We leave this direction as our long-term
investigation plan.


-----

REFERENCES

ALLEN-ZHU, Z., LI, Y. and LIANG, Y. (2018). Learning and generalization in overparameterized
neural networks, going beyond two layers. arXiv preprint arXiv:1811.04918 .

AZIZAN, N. and HASSIBI, B. (2019). Stochastic gradient/mirror descent: Minimax optimality and
implicit regularization. In International Conference on Learning Representations.

BRUTZKUS, A., GLOBERSON, A., MALACH, E. and SHALEV-SHWARTZ, S. (2017). Sgd learns
over-parameterized networks that provably generalize on linearly separable data. arXiv preprint
_arXiv:1710.10174 ._

DEVLIN, J., CHANG, M.-W., LEE, K. and TOUTANOVA, K. (2018). Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 .

DUCHI, J., HAZAN, E. and SINGER, Y. (2011). Adaptive subgradient methods for online learning
and stochastic optimization. Journal of machine learning research 12.

ECKSTEIN, J. (1993). Nonlinear proximal point algorithms using bregman functions, with applications to convex programming. Mathematics of Operations Research 18 202–226.

FRENCH, R. M. (1999). Catastrophic forgetting in connectionist networks. Trends in cognitive
_sciences 3 128–135._

FURLANELLO, T., LIPTON, Z., TSCHANNEN, M., ITTI, L. and ANANDKUMAR, A. (2018). Born
again neural networks. In International Conference on Machine Learning. PMLR.

GOYAL, P., DOLLÁR, P., GIRSHICK, R., NOORDHUIS, P., WESOLOWSKI, L., KYROLA, A.,
TULLOCH, A., JIA, Y. and HE, K. (2017). Accurate, large minibatch sgd: Training imagenet in 1
hour. arXiv preprint arXiv:1706.02677 .

GREEN, S., VINEYARD, C. M. and KOÇ, C. K. (2019). Distillation strategies for proximal policy
optimization. arXiv preprint arXiv:1901.08128 .

GUNASEKAR, S., LEE, J., SOUDRY, D. and SREBRO, N. (2018). Characterizing implicit bias in
terms of optimization geometry. In International Conference on Machine Learning. PMLR.

HE, K., ZHANG, X., REN, S. and SUN, J. (2016). Deep residual learning for image recognition. In
_Proceedings of the IEEE conference on computer vision and pattern recognition._

HE, T., ZHANG, Z., ZHANG, H., ZHANG, Z., XIE, J. and LI, M. (2019). Bag of tricks for image
classification with convolutional neural networks. In Proceedings of the IEEE/CVF Conference on
_Computer Vision and Pattern Recognition._

HINTON, G., VINYALS, O. and DEAN, J. (2015). Distilling the knowledge in a neural network.
_arXiv preprint arXiv:1503.02531 ._

JI, Z., DUDÍK, M., SCHAPIRE, R. E. and TELGARSKY, M. (2020). Gradient descent follows the
regularization path for general losses. In Proceedings of Thirty Third Conference on Learning
_Theory (J. Abernethy and S. Agarwal, eds.), vol. 125 of Proceedings of Machine Learning Research._
PMLR.

JI, Z. and TELGARSKY, M. (2018). Gradient descent aligns the layers of deep linear networks. arXiv
_preprint arXiv:1810.02032 ._

JI, Z. and TELGARSKY, M. (2019). The implicit bias of gradient descent on nonseparable data. In
_Proceedings of the Thirty-Second Conference on Learning Theory (A. Beygelzimer and D. Hsu,_
eds.), vol. 99 of Proceedings of Machine Learning Research. PMLR, Phoenix, USA.

JI, Z. and TELGARSKY, M. (2021). Characterizing the implicit bias via a primal-dual analysis. In
_Algorithmic Learning Theory. PMLR._


-----

JIANG, H., HE, P., CHEN, W., LIU, X., GAO, J. and ZHAO, T. (2020). SMART: Robust and efficient
fine-tuning for pre-trained natural language models through principled regularized optimization.
In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.
Association for Computational Linguistics, Online.

KAKADE, S., SHALEV-SHWARTZ, S., TEWARI, A. ET AL. (2009). On the duality of strong convexity
and strong smoothness: Learning applications and matrix regularization. Unpublished Manuscript,
_http://ttic. uchicago. edu/shai/papers/KakadeShalevTewari09. pdf 2._

KESKAR, N. S., MUDIGERE, D., NOCEDAL, J., SMELYANSKIY, M. and TANG, P. T. P. (2016).
On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint
_arXiv:1609.04836 ._

KINGMA, D. P. and BA, J. (2014). Adam: A method for stochastic optimization. arXiv preprint
_arXiv:1412.6980 ._

KIRKPATRICK, J., PASCANU, R., RABINOWITZ, N., VENESS, J., DESJARDINS, G., RUSU, A. A.,
MILAN, K., QUAN, J., RAMALHO, T., GRABSKA-BARWINSKA, A. ET AL. (2017). Overcoming
catastrophic forgetting in neural networks. Proceedings of the national academy of sciences 114
3521–3526.

KIWIEL, K. C. (1997). Proximal minimization methods with generalized bregman functions. SIAM
_journal on control and optimization 35 1142–1168._

KRIZHEVSKY, A., HINTON, G. ET AL. (2009). Learning multiple layers of features from tiny images
.

LEWKOWYCZ, A., BAHRI, Y., DYER, E., SOHL-DICKSTEIN, J. and GUR-ARI, G. (2020). The large
learning rate phase of deep learning: the catapult mechanism. arXiv preprint arXiv:2003.02218 .

LI, Y., WEI, C. and MA, T. (2019). Towards explaining the regularization effect of initial large
learning rate in training neural networks. arXiv preprint arXiv:1907.04595 .

LI, Z. and HOIEM, D. (2017). Learning without forgetting. IEEE transactions on pattern analysis
_and machine intelligence 40 2935–2947._

LU, H., FREUND, R. M. and NESTEROV, Y. (2018). Relatively smooth convex optimization by
first-order methods, and applications. SIAM Journal on Optimization 28 333–354.

MA, N., ZHANG, X., ZHENG, H.-T. and SUN, J. (2018). Shufflenet v2: Practical guidelines for
efficient cnn architecture design. In Proceedings of the European conference on computer vision
_(ECCV)._

MCCLOSKEY, M. and COHEN, N. J. (1989). Catastrophic interference in connectionist networks:
The sequential learning problem. In Psychology of learning and motivation, vol. 24. Elsevier,
109–165.

MONTEIRO, R. D. and SVAITER, B. F. (2010). Convergence rate of inexact proximal point methods
with relative error criteria for convex optimization. submitted to SIAM Journal on Optimization .

NACSON, M. S., LEE, J., GUNASEKAR, S., SAVARESE, P. H. P., SREBRO, N. and SOUDRY, D.
(2019). Convergence of gradient descent on separable data. In Proceedings of the Twenty-Second
_International Conference on Artificial Intelligence and Statistics (K. Chaudhuri and M. Sugiyama,_
eds.), vol. 89 of Proceedings of Machine Learning Research. PMLR.

NEMIROVSKI, A. S. and YUDIN, D. B. (1983). Problem complexity and method efficiency in
optimization .

ROCKAFELLAR, R. T. (1976a). Augmented lagrangians and applications of the proximal point
algorithm in convex programming. Mathematics of operations research 1 97–116.

ROCKAFELLAR, R. T. (1976b). Monotone operators and the proximal point algorithm. SIAM journal
_on control and optimization 14 877–898._


-----

ROSSET, S., ZHU, J. and HASTIE, T. (2004). Boosting as a regularized path to a maximum margin
classifier. The Journal of Machine Learning Research 5 941–973.

SANDLER, M., HOWARD, A., ZHU, M., ZHMOGINOV, A. and CHEN, L.-C. (2018). Mobilenetv2:
Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer
_vision and pattern recognition._

SCHULMAN, J., LEVINE, S., ABBEEL, P., JORDAN, M. and MORITZ, P. (2015). Trust region policy
optimization. In International conference on machine learning. PMLR.

SMITH, L. N. (2018). A disciplined approach to neural network hyper-parameters: Part 1–learning
rate, batch size, momentum, and weight decay. arXiv preprint arXiv:1803.09820 .

SMITH, S. L., KINDERMANS, P.-J. and LE, Q. V. (2018). Don’t decay the learning rate, increase
the batch size. In International Conference on Learning Representations.

SOLODOV, M. V. and SVAITER, B. F. (2000). Error bounds for proximal point subproblems and
associated inexact proximal point algorithms. Mathematical programming 88 371–389.

SOUDRY, D., HOFFER, E. and SREBRO, N. (2018). The implicit bias of gradient descent on separable
data. In International Conference on Learning Representations.

SUTSKEVER, I., MARTENS, J., DAHL, G. and HINTON, G. (2013). On the importance of initialization and momentum in deep learning. In Proceedings of the 30th International Conference
_on Machine Learning (S. Dasgupta and D. McAllester, eds.), vol. 28 of Proceedings of Machine_
_Learning Research. PMLR, Atlanta, Georgia, USA._

TARVAINEN, A. and VALPOLA, H. (2017). Mean teachers are better role models: Weightaveraged consistency targets improve semi-supervised deep learning results. arXiv preprint
_arXiv:1703.01780 ._

TELGARSKY, M. (2013). Margins, shrinkage, and boosting. In International Conference on Machine
_Learning. PMLR._

YANG, L. and TOH, K.-C. (2021). Bregman proximal point algorithm revisited: a new inexact
version and its variant. arXiv preprint arXiv:2105.10370 .

YUAN, L., TAY, F. E., LI, G., WANG, T. and FENG, J. (2019). Revisit knowledge distillation: a
teacher-free framework .

ZASLAVSKI, A. J. (2010). Convergence of a proximal point method in the presence of computational
errors in hilbert spaces. SIAM Journal on Optimization 20 2413–2421.

ZHANG, T., WU, F., KATIYAR, A., WEINBERGER, K. Q. and ARTZI, Y. (2021). Revisiting
few-sample {bert} fine-tuning. In International Conference on Learning Representations.

ZHOU, P., YUAN, X., XU, H., YAN, S. and FENG, J. (2019). Efficient meta learning via minibatch proximal update. In Advances in Neural Information Processing Systems (H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox and R. Garnett, eds.), vol. 32. Curran
Associates, Inc.


-----

