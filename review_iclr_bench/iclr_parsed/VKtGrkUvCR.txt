# Only tails matter: Average-Case Universal- ity and Robustness in the Convex Regime

**Anonymous authors**
Paper under double-blind review

Abstract

Recent works have studied the average convergence properties of first-order
optimization methods on distributions of quadratic problems. The averagecase framework allows a more fine-grained and representative analysis of
convergence than usual worst-case results, in exchange for a more precise
hypothesis over the data generating process, namely assuming knowledge of
the expected spectral distribution (e.s.d) of the random matrix associated
with the problem. This work shows that a problem’s asymptotic average
complexity is determined by the concentration of eigenvalues near the edges
of the e.s.d. We argue that having a priori information on this concentration is a more grounded assumption than complete knowledge of the e.s.d.
basing our analysis on the approximate concentration is effectively a middle
ground between the coarseness of the worst-case scenario convergence and
the restrictive previous average-case analysis. We introduce the Generalized
Chebyshev method, asymptotically optimal under a hypothesis on this concentration, and globally optimal when the e.s.d. follows a Beta distribution.
We compare its performance to classical optimization algorithms, such as
Gradient Descent or Nesterov’s scheme, and we show that, asymptotically,
Nesterov’s method is universally nearly optimal in the average-case.

1 Introduction

The analysis of the average complexity of algorithms has a long story in computer science.
Average-case complexity, for instance, drives much of the decisions made in cryptography
(Bogdanov & Trevisan, 2006).

Despite their relevance, average-case analyses are difficult to extend to other algorithms,
partly because of the intrinsic issue of defining a typical distribution over problem instances.
Recently though, Pedregosa & Scieur (2020) derived a framework to systemically evaluate the
complexity of first-order methods when applied on distributions of quadratic minimization
problems. This is done by relating the average-case convergence rate to the expected spectral
_distribution (e.s.d) of the objective function’s Hessian, which is a well-studied object on_
random matrix theory. Having access to this object in practice is a much stronger hypothesis
when compared to the worst-case analysis that relies only on the values of the edges of this
distribution.

Paquette et al. (2020) extended the average-case framework by introducing a noisy generative
model for the problems. They further derived the average complexity of the Nesterov
Accelerated Method (Nesterov, 2003) on a particular distribution. They showed the strong
concentration of the metrics around a limiting value as dimensions go to infinity.

Scieur & Pedregosa (2020) showed that for a strongly convex problem with eigenvalues supported on a contiguous interval, the optimal average-case complexity converges asymptotically
to the one given by the Polyak Heavy Ball method (Polyak, 1964) in the worst-case.

1.1 Current limitations of the average-case analysis

When analyzing the state of the art of average-case methods on quadratics problems,
we observe significant limitations that we address in this paper. First, little is known


-----

about the convergence rate on convex problems. Also, optimal average-case algorithms
require an exact estimation of the e.s.d to guarantee an optimal convergence rate, their
convergence rate under inexact e.s.d. is not known. Finally, the non-smooth is also
discussed in (Pedregosa & Scieur, 2020), but with little details.

**Convex problems. The minimization of non-strongly convex problems is drastically slower**
than their strongly convex counterpart, as Gradient Descent presents worst-case convergence
in Θ( [1]t [)][ and Nesterov is][ Θ(][ 1]t[2][ )][. In the strongly convex case, both the worst-case and average-]

case are asymptotically equal. However, little is known on optimal average-case rates for
convex problems, as well as the average-case complexity of classical methods such as gradient
descent or Nesterov’s method, see (Paquette et al., 2020).

**Exact estimation of the e.s.d. In (Pedregosa & Scieur, 2020), the theoretical study of**
optimal algorithms in the average-case requires an exact estimation of the e.s.d. of the
problem class. Such estimation may be hard, nor impossible to obtain in practical scenarios.
Despite showing good performance when the e.s.d. is estimated with empirical quantities,
there are no theoretical guarantees on the performance of the method when the e.s.d. is
poorly estimated. There is therefore a need to analyze the algorithm’s performance under
different notions of uncertainty on the spectrum. This allows a practitioner to choose the
best algorithm for a practical problem, even with imperfect a priori information.

**Non-smooth. Pedregosa & Scieur (2020) briefly introduce average-case optimal rates on**
non-smooth problems, when the e.s.d. is the Laguerre distribution e[−][λ]. In this paper, we
extend the analysis to the generalized Laguerre distribution λ[α]e[−][λ], α > −1.

1.2 Contributions

Our main contribution is a fine-grained analysis of the average-case complexity on convex
quadratic problems: we show that a problem’s complexity depends on the concentration
of the eigenvalues of e.s.d. around the edges of their support. From this perspective, we
propose a family of optimal algorithms in the average-case, analyze their robustness,
and finally exhibit a universality result for Nesterov’s method. More precisely,

-  (Optimal algorithms). In Section 3, we propose the Generalized Chebyshev Method
(GCM, Algorithm 1), a family of algorithms whose parameters depend on the concentration
of the e.s.d. around the edges of their support. If the parameters of the GCM method
are set properly, the algorithm converges at an optimal average-case rate (Theorem 3 for
smooth problems, Theorem 6 for non-smooth problems), a rate that we show is faster
than worst-case optimal methods like Nesterov acceleration. We show these rates to be
representative of the practical performance of the algorithms in Fig. 6, and retrieve the
classical worst-case rates as limits of the average-case (see Table 1).

-  (Robustness). Developing an optimal algorithm requires the knowledge of the exact
e.s.d. However, in practical scenarios, we only have access to an approximation of the
e.s.d. In Theorem 2 in Section 4 we analyze the rate of GCM in the presence of such a
mismatch. We also analyze the optimal average-case rates of distributions representing
the smooth convex, non-smooth convex, and strongly convex settings and compare them
with the worst-case rates (Table 1).

-  (Universality). Finally, in Theorem 4, we analyze the asymptotic average-case convergence rate of Nesterov’s method. We show that its convergence rate is nearly optimal
(up to a logarithmic factor) under some natural assumptions over the data, namely a
concentration of eigenvalues around 0 similar to the Marchenko-Pastur measure. This
contributes to the theoretical understanding of the numerical efficiency of Nesterov’s
acceleration.

2 Average-Case Analysis

In this section, we recall the average-case analysis framework for random quadratic problems.
The main result is Theorem 1, which relates the expected error to the expected spectral


-----

**Regime** **Worst-case** **Average-Case**

Strongly conv. (1 − Θ([1]/[√]κ))[t] (1 − Θ([1]/[√]κ))[t]

Smooth conv. 1/t2 1/t2ξ+4


Convex 1/


1/tα+2


Table 1: Comparison between function value worstcase and average-case convergence. κ is the condition number in the smooth strongly convex case. In
the smooth convex case ξ > −1 is the concentration
of eigenvalues around 0 (see Assumption 1) and in
the non-smooth case we consider dµ ∝ _λ[α]e[−][λ]_


**Concentration around the**
**edges drive complexity**


Figure 1: Representation of different
spectra with different concentrations of
eigenvalues around the edges of the support. The average-case rates for nonstrongly problems are determined by
these concentrations


_distribution and the residual polynomial. The one-to-one correspondence between the residual_
polynomials and first-order methods applied to quadratics will allow us to pose the problem
of finding an optimal method as the best approximation problem in the space of polynomials.

We define a random quadratic problem:

**Problem 1. Let H ∈** R[d][×][d] _be a random symmetric positive-definite matrix independent_
_to x[⋆]_ _∈_ R[d], a random vector that is the solution to the problem. We define the random
_quadratic minimization problem as_


min _f_ (x) := [1] _._
**_x∈R[d]_** 2 [(][x][−][x][⋆][)][⊤][H][(][x][−][x][⋆][)]

n o


_We are interested on minimizing the expected errors E∥f_ (xt) − _f_ (x[⋆])∥, the expected function_value gap, and E||∇f_ (xt)||[2], the expected gradient norm, where xt is the t-th update of a
_first-order method starting from x0 and E is the expectation over the random variables H, x0_
_and x[⋆]._

The expectation we consider is over the problem and not over any randomness of the
algorithm.

In this paper, we consider the class of first-order methods (F.O.M’s) to minimize (OPT).
Methods in this class construct the iterates xt as

**_xt_** **_x0 + span_** _f_ (x0), . . ., _f_ (xt 1) _._ (1)
_∈_ _{∇_ _∇_ _−_ _}_

That is, xt belongs to the span of previous gradients. This class of algorithms includes
for instance gradient descent and momentum, but not quasi-Newton methods since the
preconditioner could allow the iterates to go outside of the span. Furthermore, we will only
consider oblivious methods, that is, methods in which the coefficients of the update are
known in advance and do not depend on previous updates. This leaves out some methods
such as conjugate gradient or methods with line-search.

**From First-Order Method to Polynomials.** There is an intimate link between firstorder methods and polynomials that simplifies the analysis of quadratic objectives. The
next proposition shows that, with this link, we can assign to each optimization method
a polynomial that determines its convergence. Following Fischer (1996), we will say a
polynomial Pt is residual if Pt(0) = 1.

**Proposition 1. (Hestenes et al., 1952) Let xt be generated by a first-order method. Then**
_there exists a residual polynomial Pt of degree t, that verifies_

**_xt_** **_x[⋆]_** = Pt(H)(x0 **_x[⋆]) ._** (2)
_−_ _−_


-----

**Remark 1. If the first-order method is further a momentum method, i.e.**

**_xt+1 = xt + ht_** _f_ (xt) + mt(xt **_xt_** 1).
_∇_ _−_ _−_

_We can determine the polynomials by the recurrence P0 = 1 and_


_Pt+1(λ) = Pt(λ) + htλPt(λ) + mt(Pt(λ) −_ _Pt−1(λ))._

_We note that while most popular F.O.M’s can be posed as a momentum method, the Nesterov_
_method cannot._

A convenient way to collect statistics on the spectrum of a matrix is through its empirical
_spectral distribution._

**Definition 1 (Expected spectral distribution (e.s.d)). . Let H be a random matrix with**
_eigenvalues_ _λ1, . . ., λd_ _. The empirical spectral distribution of H, called µH_ _, is the_
_{_ _}_
_probability measure_

_µH := [1]_ _di=1[δ][λ]i_ _[,]_ (3)

_d_

_where δλi is the Dirac delta, a distribution equal to zero everywhere except atP_ _λi and whose_
_integral over the entire real line is equal to one._

_Since H is random, the empirical spectral distribution µH is a random variable in the space_
_of measures. Its expectation over H is called the expected spectral distribution and we_
_denote it_
_µ := EH_ [µH ] . (4)

We can link the e.s.d. of H to the convergence of a first-order method on the distribution
of H. In the following we will consider x0 **_x[⋆]_** and H to be independent, with x0 **_x[⋆]_**
_−_ _−_
sampled isotropically.
**Theorem 1. Let xt be generated by a first-order method associated to the polynomial Pt,**
_Then we can write the convergence metrics at time stepthe measure µ the e.s.d. of H, and E[(x0 −_ **_x[⋆])(x0 −_** _tx as[⋆])[T]_ ] = R[2]I for some constant R.

E[∥xt − **_x[⋆]∥[2]] = R[2]_** _Pt[2][(][λ][)][dµ][(][λ][)][,]_ E[f (xt) − _f_ (x[⋆])] = R[2] _Pt[2][(][λ][)][λdµ][(][λ][)]_
Z Z (5)

_and_ E[||∇f (xt)||2[2][] =][ R][2] _Pt[2][(][λ][)][λ][2][dµ][(][λ][)][.]_
Z

This shows that polynomials are a powerful abstraction as they allow us to write all of our
convergence metrics within the same framework . For simplicity, we set R[2] = 1 and we will
refer directly to the polynomials associated to a given method. We will refer to objective l
as the one associated to the added λ[l] term, i.e. the function-value is objective l = 1.

This framework is linked to the field of orthogonal polynomials by the next proposition.
We construct an optimal method w.r.t. a given distribution through a family of orthogonal
polynomials associated to it.
**Proposition 2 ((Pedregosa & Scieur, 2020)). Let Pt[l]** _[be defined as]_


_Pt[l]_ [:= arg min]Pt(0)=1


_Pt[2][(][λ][)][λ][l][dν][(][λ][)][.]_ (6)


_Then (Pt[l][)][ is the family of residual orthogonal polynomials w.r.t. to][ λ][l][+1][dν][.]_

This theorem further implies that the optimal first-order method is a momentum method as
Favard’s theorem Marcellán & Álvarez-Nodarse (2001) tells us the orthogonal polynomials
w.r.t. a given distribution are related through a three term recurrence,

_Pt+1(λ) = atPt(λ) + btλPt(λ) + (1 −_ _at)Pt−1(λ)._ (7)

Following Remark 1, the optimal method is derived from this recurrence as

**_xt+1 = xt + (at_** 1)(xt **_xt_** 1) + bt _f_ (xt) . (8)
_−_ _−_ _−_ _∇_


-----

3 Methods

Being able to write the rates in terms of the expected spectral distribution ties the average-case
framework to the field of random matrix theory. Indeed, because of results from this field,
certain e.s.d’s are considered more natural than others. Indeed, it can be shown that the
same distribution arises when we take the gram matrix of random centered i.i.d. features
with variance σ[2]: the Marchenko Pastur distribution.
**Definition 2 (MP distribution). The Marchenko Pastur distribution associated with the**
_parameter r and with scale σ[2]_ _is given by_

1 (λ[+] _λ)(λ_ _λ[−])_
_dµMP (λ) =_ _−_ _−_ _,_ (9)

2πσ[2] _rλ_

p

_with λ[+]_ = σ[2](1 + _[√]r)[2], λ[−]_ = σ[2] max(0, (1 − _[√]r)[2])._

The Marchenko Pastur distribution µMP can be considered a natural first model for e.s.d’s
as it arises universally from matrices with i.i.d. entries,under mild low moment assumptions,
there is no specific distribution of the matrix to be considered. It can be seen as a model for
the white-noise in the data. When r = 1, i.e. n = d, we have dµMP _λ[−][1][/][2][√]λ[+]_ _λ._
_∝_ _−_

Pedregosa & Scieur (2020) first derived the optimal method w.r.t. µMP, and Paquette et al.

(2020) derived Nesterov’s rates under the distribution. As we are concerned with being
robust, a natural step is to consider the Beta weights.
**Definition 3. The (generalized) Beta weights with parameters τ, ξ and scale L are given by**
_the (non-normalized) pdf_
dµ(λ) = λ[ξ](L − _λ)[τ]_ _._ (10)

This family of distribution generalizes the MP distribution, and both have similar concentrations near 0 when ξ ≈−1/2.

The optimal method w.r.t. µ and objective l is associated to a shifted Jacobi polynomial
_P˜t[α,β]_ with β = ξ + l + 1, α = τ . When α = β = 1/2, we retrieve the Chebyshev Method
_−_
(Flanders & Shortley, 1950). As such, we name our proposed methods the Generalized
_Chebyshev Method (GCM)._

**Algorithm 1: GCM(α, β)**
**Inputs: Initial vector x0, function f**, smoothness parameter estimate L
**_xfor−1 t ← = 10, δ, . . ., T0 ←_** **do0**

2(β[2]+αβ+(2t+1)(α+β)+2t[2]+2)(2t+α+β+1)
_at_ 2(t+1)(t+α+β+1)(2t+α+β)
_←−_

_bt_ _L(t+1)(t+α+β+1)_

_γt ←_ [(2][t]([+]t(+1)([α]t+[+]α[β]t)([+1)(2]+tα++β)(2β[t][+]+1)(2t[α]+[+]α[β]+t[+2)]+βα+2)+β)

_δt ← ←−at+γ1tδt−1_

**_xt_** **_xt_** 1 + (δtat 1)(xt 1 **_xt_** 2) + δtbt _f_ (xt 1)
_←_ _−_ _−_ _−_ _−_ _−_ _∇_ _−_


We’ll consider the Nesterov’s method used in Paquette et al. (2020), which is defined by the
iterations:

**_xt+1 = yt −_** _L[1]_ _[∇][f]_ [(][y][t][)] (11)


_t_

(12)
_t + 3_ [(][x][t][+1][ −] **_[x][t][)]_**


**_yt+1 = xt+1 +_**


_x[α]e[−][x]_
We also consider the Laguerre method, which is optimal w.r.t. dµ(x) = Γ(α+1) [, taking][ α][ as]

a parameter. This method is proposed to optimize non-smooth functions.
Both these methods are generalizations of ones that have been proposed in Pedregosa &


-----

Scieur (2020). We show that Algorithm 1 corresponded to polynomials _P[˜]t[α,β]_ and derive the
Laguerre method in appendix B.

**Remark 2. The Generalized Chebyshev takes the largest eigenvalue L as a parameter, but**
_the rates we will show are robust to an overestimation of L._

4 Robust Average-Case Rates

We will state our assumption over the spectral distributions. It effectively allows us to
parametrize all of our distributions of interest in a way that characterizes the asymptotic
convergence, diving them into equivalence classes.
**Assumption 1. We will write ντ,ξ for a continuous distribution supported in (0, L] s.t.**
_ντ,ξ[′]_ [(][x][)][ >][ 0][ for][ x][ ∈] [[0][, L][]][,][ dν][τ,ξ][ = Θ(][λ][ξ][)][ near][ 0][ and][ dν][τ,ξ][ = Θ((][L][ −] _[λ][)][τ]_ [)][ near][ L][.]

Assumption 1 is quite nonrestrictive, in that, the spectral distribution of the Hessian for
any smooth convex problem can be identified with some τ, ξ in this class. It is a milder
assumption than (1) assuming complete knowledge of the spectrum of the Hessian or (2)
the specific distribution on the entries of your data. Moreover the assumption encompasses
the frequently used MP (e.g. Martin & Mahoney (2021); Pennington & Bahri (2017)) and
Uniform distributions We note there’s no need to consider eigenvalues situated at 0 as they
do not contribute to the optimization process.

The ξ works as a measure of how close we are to the worst-case scenario, as it approaches
_−1. Samples in finite dimension of distributions with high values of ξ will work as strongly_
convex functions in practice.

We show that ντ,ξ indeed behaves like an equivalence class when considering the asymptotics
of the convergence of the methods: only the concentrations near the edge matter. We do this
by singling out from each of these classes the beta distributions for which we can compute
the rates, then show the rates to be the same inside ντ,ξ.
**Theorem 2 (GCM average-case rates). A Generalized Chebyshev Method with parameters**
(α, β) applied to a problem with e.s.d. as in Assumption 1 has average-case rates


_t[−][1][−][2][β]_ _if α < τ + 1/2 and β < ξ + 3/2_
_t[−][2(][ξ][+2)]_ log t _if α = τ + 1/2 and β = ξ + 3/2_ _,_
_t[2(max][{][α][−][β][−][τ,][−][ξ][−][1][}−][1)]_ _if α > τ + 1/2 or β > ξ + 3/2_

(13)

_t[−][1][−][2][β]_ _if α < τ + 1/2 and β < ξ + 5/2_
_t[−][2(][ξ][+3)]_ log t _if α = τ + 1/2 and β = ξ + 5/2_ _,_
_t[2(max][{][α][−][β][−][τ,][−][ξ][−][2][}−][1)]_ _if α > τ + 1/2 or β > ξ + 5/2_

(14)


E[f (xt) _f_ (x[⋆])] _L_ _C1[α,β],ν_
_−_ _∼_ _·_

E[ _f_ (xt) 2[]][ ∼] _[L][2][ ·][ C]2[α,β],ν_
_||∇_ _||[2]_


_where Cν[α,β]_ _is a distribution dependent constant._

Theorem 2, which is illustrated by fig. 2 shows that overestimating β, and underestimating α
will still leave us with the optimal asymptotic rates, so a good rule of thumb for calibrating
the algorithm is to use high β and low α.

Theorem 3 shows that a proper choice of α, β can indeed make the Jacobi polynomial
asymptotically optimal w.r.t. to any ντ,ξ.
**Theorem 3 (Optimal Rates). Let ν follow Assumption 1. The optimal asymptotic average-**
_case rates for E[f_ (xt) − _f_ (x[⋆])] and E[||∇f (xt)||2[2][]][ are attained by the GCM with parameters]
(τ, ξ + 2) and (τ, ξ + 3), respectively, and read

E[f (xt) − _f_ (x[⋆])] = Θ(t[−][2(][ξ][+2)]), E[||∇f (xt)||2[2][] = Θ(][t][−][2(][ξ][+3)][)][.]

For the function value (l = 1), we find rates that approach t[−][2] as ξ →−1, showing the
worst-case as a limit (over the considered distribution) on the average-case.


-----

**Parameters (τ, ξ)**
**Method**

( [1]2 _[,][ 1]2_ [)] ( [1]2 _[,][ −]_ [1]2 [)]

GCM(α = 2[1] _[, β][ =][ 5]2_ [)] _t[−][5]_ _t[−][3]_

GCM(α = 2[1] _[, β][ =][ 3]2_ [)] _t[−][4]_ _t[−][3]_

Nesterov _t[−][4]_ _t[−][3]_ log t


Gradient Descent _t_ _−25_


_−23_


Figure 3 & Table 1: The figure illustrates the robustness of the Generalized Chebyshev Method
with parameters (α, β) for a fixed problem corresponding to the Marchenko-Pastur distribution
(τ = [1]2 _[, ξ][ =][ −]_ [1]2 [)][. The color represents the exponent][ a][ of the average-case rate][ O][(][t][a][)][ of the method]

for different values of α and β. The white star represents the optimal tuning and the blue area is the
set of parameters for which the method converges. Note we have a large of region that guarantee
the same optimal asymptotic rate. The table compares the asymptotic average-case rates for the
function-value for different methods with different (τ, ξ) values.

We remark that the above theorems imply that, at least asymptotically, the GCM is robust
for a suboptimal choice of parameter β up to 1/2 below the optimal choice and infinitely
above.

For completeness, we also derive worst-case rates for the GCM:
**Proposition 3 (GCM worst-case rates). Let f be a convex, L-smooth quadratic function.**
_Then, for the Generalized Chebyshev Method with parameters (α, β), we have worst-case rates_


_t[2(][α][−][β][)]_ _if_ _α > β −_ 1
_t[−][1][−][2][β],_ _if_ _α_ _β_ 1 _β_ 2 _,_ (15)
_≤_ _−_ _≤_ [1]

_t[−][2],_ _if_ _α_ _β_ 1 _β_ 2
_≤_ _−_ _≥_ [1]

_t[2(][α][−][β][)]_ _if_ _α > β −_ 2
_t[−][1][−][2][β],_ _if_ _α ≤_ _β −_ 2 _β ≤_ 3/2 _._ (16)
_t[−][4],_ _if_ _α ≤_ _β −_ 2 _β ≥_ 3/2


_f_ (xt) _f_ (x[⋆]) _C1L_
_−_ _≤_

_f_ (xt) _f_ (x[⋆]) _C2L[2]_
_||∇_ _−_ _|| ≤_


For a reasonable choice of α, β, i.e. β ≥ 2[1] [,][ α][ ≤] _[β][ −]_ [1][. the function value achieves the]

theoretical lower bound of t[−][2].

We now analyze the convergence of the Nesterov method. Nesterov (2003) has shown that
it matches up to a constant factor a lower bound on the worst-case complexity of non
strongly convex problems. A natural question is if this performance would translate to good
average-case rates. To do so, we will extend Paquette et al. (2020) proof for the Nesterov
method rates under the MP distribution.
**Theorem 4 (Nesterov average-case rates). Let ν as in Assumption 1. Then for the Nesterov**
_method, we have average-case rates_

_t[−][2(][ξ][+2)]_ _if ξ < −1/2_
E[f (xt) − _f_ (x[⋆])] ∼ _C1[′]_ _,ν_ _t[−][3]_ log t _if ξ = −1/2_ _,_ E[||∇f (xt)||2[2][]][ ∼] _[C]2[′]_ _,ν[t][−][(][ξ][+9][/][2)][.][ (17)]_
_t[−][(][ξ][+7][/][2)]_ _if ξ >_ 1/2
n
_−_

The difference between the asymptotic average-case rates of Nesterov and the optimal ones
are t[ξ][+][l][−][1][/][2], when ξ + l > 1/2, log t when ξ + l = 1/2 and 0 otherwise. This shows that
Nesterov is almost optimal when the concentrations near 0 are relatively high, i.e. low ξ.
**Theorem 5 (Gradient Descent average-case rates). Let ν as in Assumption 1. Then for**
_gradient descent_

E[f (xt) − _f_ (x[⋆])] = Θ(t[−][(][ξ][+2)]), E[||∇f (xt)||2[2][] = Θ(][t][−][(][ξ][+3)][)][.] (18)


-----

|Col1|CIFAR|Col3|
|---|---|---|
||||
||400 600 800||
||||
||||
||||
||||
||||
||||
||||


10[0] 10[1] 10[2]


|MNIST|Col2|
|---|---|
||600|
|||
|||
|||
|||


10[0] 10[1] 10[2]


10[2]

10[1]


10

10


10[0]

0

10
10


10[0]

0

10[4]

_||_

10[2]

_||∇_

10[0]


10[−]
10[−]
10[−]

10[−][11]
10[−][14]


10[−][2]


GCM( [1]2 _[,][ 3]2_ [)]

GCM( [1]2 _[,][ 5]2_ [)]

CG
GD
Nesterov


Figure 4: Above: Empirical spectrum for the covariance matrix of the features. Below:
Gradient norms throughout iterations. Left: CIFAR-10 Inception features Right: MNIST
features. Here we choose to compare gradient norms as the minimum function value is not
known. The properly tuned GCM achieves remarkable performance under these non-synthetic
spectrum’s.

Observe for the function value that the rate for Nesterov is t[−][2] and the rate for Gradient
Descent is t[−][1] when ξ →−1.

Lastly, we consider the optimal rates for a Gamma distribution.
**Theorem 6 (Laguerre method rates). Let α >** 1 and µα, α > 1 be a Gamma distribution,
_x[α]e[−][x]_ _−_ _−_
_i.e. dµα(x) =_ Γ(α+1) _[. The optimal rates are given by the Laguerre method of appropriate]_

_tuning and_
E[f (xt) − _f_ (x[⋆])] = Θ(t[−][(][α][+2)]). (19)

Note that this result does not have the same universality as the others because of the
non-compacity of the distribution’s support. These rates are contrasted to the worst-case
lower bound on the optimization of non-smooth functions by first-order methods, which gives


_f_ (xk) _f_ (x[⋆])
_−_ _≥_ _√[C]t_ _[.]_

These rates are not found when α →−1, indicating that the worst-case is especially
pessimistic in this scenario.
**Remark 3. All of the expected rates we state are almost deterministic on the high dimensional**
_setting as per the concentration results shown in Paquette et al. (2020)_


5 Experiments

We simulate the e.s.d’s in two ways. The Marchenko Pastur distribution, which we sample
by taking H = XX _[T]_ where X has i.i.d. gaussian samples. This enables us to simulate
(τ, ξ) values of (1/2, −1/2). Other values of (τ, ξ) are simulated by sampling Λ ∈ R[d] from the
corresponding Beta distribution and taking H = U diag(Λ)U _[T]_, where U is an independently
sampled orthonormal matrix.

We let x[⋆] = 0 and sample x0 from a centered Gaussian distribution, the dynamics are
the same as in the general case. In all experiments we use the problem’s instance largest
eigenvalue to calibrate each method (e.g. Gradient Descent’s stepsize is 1/L). Our theoretical
rates in Theorem 5 and Theorem 4 respectively for the Nesterov method and Gradient Descent
are precise under the approximate range −1 < ξ < 0 as we show in Figure 6. Distributions
with higher ξ need many samples otherwise they behave as strongly convex functions.

The same is not true for the Generalized Chebyshev Method. If β < β[⋆] or ξ is low the
empirical findings diverge from the theoretical. We believe this is due to numerical instability


-----

10[2]

10[−][1]

10[−][4]

10[−][7]


10[1]

10[−][1]

10[−][3]

10[−][5]

|Col1|Col2|Col3|
|---|---|---|
||||
||||
|GCM(1,3)|||
|2 2 GCM(1 2, 5 2) CG|||
|GD Nesterov|||


10[0] 10[1] 10[2]


10[0] 10[1] 10[2]


Figure 5: Rates for a synthetic problem, simulating the Marchenko Pastur distribution. Note
that both tunings of the GCM achieve performance in function value very close to the one of
Conjugate Gradient, which is optimal for every draw of the problem.

under these regimes as the metrics also have much larger variance than in the other regimes.
We’ve not been able though to pinpoint the exact source of this supposed instability. This is
shown in appendix D.

The GCM with β > β[⋆] performs corresponding to the theory, and it’s non-asymptotically
very close to the performance of β[⋆]. High values of β also perform very well on non-synthetic
data, suggesting in practice we should use these values.


Nesterov


Gradient Descent


10[0]

10 2


|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||= 0.99||||
||= 0.00 Empirical Theoretical Assym Regression Assym|p. p.|||


10[0] 10[1] 10[2] 10[3]

= 0.99

= 0.00

Empirical
Theoretical Assymp.
Regression Assymp.

Chebyshev


10[0] 10[1] 10[2] 10[3]

10[3]

10[0]

10 3

10 6

10 9


10[2]
10[0]


10

10

10

10


10 10

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|||||||
|= 0.50||||||
|= 0.00 Empirical||||||
|Theoretica Regressio|l Assymp. n Assymp.|||||

10[0] 10[1] 10[2] 10[3]

= 0.50

= 0.00

Empirical
Theoretical Assymp.
Regression Assymp.


Figure 6: Comparison between experiments run on synthetic Beta distribution and theoretical
asymptotic. Y-axis is the function value


6 Conclusion and further work

In this paper, we’ve established that the assymptotic convergence of first order methods
on quadratic problems in the convex regime depend on the concentration of the Hessian’s
eigenvalue near the edges of the spectrum’s support. We further contributed to the theoretic
understanding of the Nesterov’s method performance and established the contrast between
the worst-case and average-case in the main regimes considered in Optimization.


-----

Bibliography

Andrej Bogdanov and Luca Trevisan. Average-case complexity. arXiv preprint cs/0606037,
2006.

Bernd Fischer. Polynomial based iteration methods for symmetric linear systems. SIAM,
1996.

Donald A Flanders and George Shortley. Numerical determination of fundamental modes.
_Journal of Applied Physics, 21(12):1326–1332, 1950._

Magnus Rudolph Hestenes, Eduard Stiefel, et al. Methods of conjugate gradients for solving
_linear systems, volume 49. NBS Washington, DC, 1952._

Francisco Marcellán and Renato Álvarez-Nodarse. On the “favard theorem” and its extensions.
_Journal of computational and applied mathematics, 127(1-2):231–254, 2001._

Charles H Martin and Michael W Mahoney. Implicit self-regularization in deep neural
networks: Evidence from random matrix theory and implications for learning. Journal of
_Machine Learning Research, 22(165):1–73, 2021._

Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87.
Springer Science & Business Media, 2003.

Courtney Paquette, Bart van Merriënboer, Elliot Paquette, and Fabian Pedregosa. Halting
time is predictable for large models: A universality property and average-case analysis.
_arXiv preprint arXiv:2006.04299, 2020._

Fabian Pedregosa and Damien Scieur. Acceleration through spectral density estimation. In
_International Conference on Machine Learning, pp. 7553–7562. PMLR, 2020._

Jeffrey Pennington and Yasaman Bahri. Geometry of neural network loss surfaces via random
matrix theory. In International Conference on Machine Learning, pp. 2798–2806. PMLR,
2017.

[B.T. Polyak. Some methods of speeding up the convergence of iteration methods. USSR](https://doi.org/10.1016/0041-5553(64)90137-5)
_Computational Mathematics and Mathematical Physics, 04, 1964._

Damien Scieur and Fabian Pedregosa. Universal average-case optimality of polyak momentum.
_arXiv preprint arXiv:2002.04664, 2020._

Gabor Szegö. Orthogonal polynomials, vol. 23. In American Mathematical Society Colloquium
_Publications, 1975._

Walter Van Assche. Weak convergence of orthogonal polynomials. Indagationes Mathematicae,
6(1):7–23, 1995.


-----

A Proofs of Section 2

**Theorem 1. Let xt be generated by a first-order method associated to the polynomial Pt,**
_Then we can write the convergence metrics at time stepthe measure µ the e.s.d. of H, and E[(x0 −_ **_x[⋆])(x0 −_** _tx as[⋆])[T]_ ] = R[2]I for some constant R.

E[∥xt − **_x[⋆]∥[2]] = R[2]_** _Pt[2][(][λ][)][dµ][(][λ][)][,]_ E[f (xt) − _f_ (x[⋆])] = R[2] _Pt[2][(][λ][)][λdµ][(][λ][)]_
Z Z (5)

_and_ E[||∇f (xt)||2[2][] =][ R][2] _Pt[2][(][λ][)][λ][2][dµ][(][λ][)][.]_
Z

_Proof. We remark that by the definition of the expected spectral distribution µ of H, we_
have for continuous g

EH [g(tr(H))] = _g(λ) dµ(λ)_ (20)
Z

We know that xt **_x[⋆]_** = Pt(H)(x0 **_x[⋆]). We can write_** **_xt_** **_x[⋆]_** in terms of a trace and
use the independence of − **_H and x0 −_** **_x[⋆]_** to connect it to the e.s.d.: || _−_ _||[2]_
_−_

E||xt − **_x[⋆]||[2]_** = E[tr((x0 − **_x[⋆])[T]_** _Pt(H)[2](x0 −_ **_x[⋆]))]_** (21)

= EH,x0 **_x⋆_** [tr(Pt(H)[2](x0 **_x[⋆])(x0_** **_x[⋆])[T]_** ] (22)
_−_ _−_ _−_

= EH _Pt(H)[2]Ex0_ **_x⋆_** [(x0 **_x[⋆])(x0_** **_x[⋆])[T]_** ]) (23)
_−_ _−_ _−_

= R[2]EH [Pt(tr(H))[2]] = R[2] _Pt(λ)[2]_ dµ(λ)  (24)
Z

For the gradient and function value the reasoning is the same by noticing that


E[f (xt) − _f_ (x[⋆])] = E[tr((x0 − **_x[⋆])[T]_** _Pt(H)HPt(H)(x0 −_ **_x[⋆]))]_** (25)

= EH [(λPt)(tr(H))[2]], (26)

where λPt is also a polynomial. As ∇f (xt) = H(xt − **_x[⋆])._**

E||∇f (xt))||[2] = E[tr((x0 − **_x[⋆])[T]_** _Pt(H)H_ [2]Pt(H)(x0 − **_x[⋆]))]_** (27)

= EH [(λ[2]Pt)(tr(H))[2]] (28)


**Proposition 2 ((Pedregosa & Scieur, 2020)). Let Pt[l]** _[be defined as]_


_Pt[l]_ [:= arg min]Pt(0)=1


_Pt[2][(][λ][)][λ][l][dν][(][λ][)][.]_ (6)


_Then (Pt[l][)][ is the family of residual orthogonal polynomials w.r.t. to][ λ][l][+1][dν][.]_

_Proof. We differentiate the expression for the metrics w.r.t._ to the coefficients of the
polynomials:


_t_

_λ[l]Pt[2][(][λ][)][dµ][(][λ][)]_ = _λ[l][ d]_ _akλ[k]Pt(λ)_ dµ(λ) =
Z  Z dak _k=0_ !

X

= 2 _λ[l][+][k]Pt(λ)dµ(λ)_ = 0
_·_
Z 


_dak_


This means that Pt(λ) is orthogonal to any polynomial of degree t 1 w.r.t to the intern
product _., ._ _λl+1dµ_ _−_
_⟨_ _⟩_


-----

B GCM and Laguerre method derivation

We will first state two lemmas that allow us to construct the optimal polynomials. With
them in hand the procedure is trivial.

**Lemma 7. Let ( P[˜]t) be a family polynomials following**

_P˜t(λ) = (αt + βtλ) P[˜]t_ 1)λ) + γtP[˜]t 2(λ),
_−_ _−_

_with_ _P[˜]0 a constant polynomial and_ _P[˜]t ̸= 0, ∀t. Then_

_Pt(λ) = (at + btλ)Pt_ 1(λ) + (1 _at)Pt_ 2(λ) (29)
_−_ _−_ _−_

_is the recurrence for Pt(λ) = P[˜]t(λ)/P[˜]t(0). With:_

_at = δtαt_ (30)
_bt = δtβt_ (31)
_δt = (αt + γtδt_ 1) (δ0 = 0) (32)
_−_

The proof of this is presented in Pedregosa & Scieur (2020). Further, we know how to
compute the recurrence for the polynomials of a shifted distribution:

**Lemma 8. Let ( P[˜]t) be a family polynomials orthogonal w.r.t following**

_P˜t(λ) = (αt + βtλ) P[˜]t_ 1(λ) + γtP[˜]t 2(λ), (33)
_−_ _−_

_and define polynomials Pt s.t. :_
_Pt(m(λ)) = P[˜]t(λ),_

_with m(λ) = aλ + b a non singular affine transform. Then Pt follows a recurrence like in eq._
(33), with:

_αt[′]_ [=][ α][t] [+][ bβ][t] (34)

_βt[′]_ [=][ aβ][t] (35)

_γt[′]_ [=][ γ][t] (36)

The lemma is self-evident by considering eq. (33) with argument m[−][1](λ)

These results are enough to get the recurrence relation for the residual polynomial w.r.t
_x[β](L −_ _x)[α]. We begin by the standard Jacobi polynomials, which are orthogonal w.r.t_
(1 − _x)[α](1 + x)[β]_ and follow a recurrence according to αt, βt, γt below Szegö (1975):

_αt = [(2][n][ +][ α][ +][ β][)(2][n][ +][ α][ +][ β][ −]_ [1)] (37)

2n(n + α + β)


(α[2] _β[2])(2n + α + β_ 1)
_βt =_ _−_ _−_ (38)

2n(n + α + β)(2n + α + β − 2)

_γt =_ (39)

_[−][2(]2[n]n[ +](n[ α] +[ −] α[1)(] + β[n][ +])(2[ β]n[ −] +[1)(2] α +[n] β[ +][ α]2)[ +][ β][)]_

_−_

We then shift the distribution according to η(x), and then transform to the residual ones.
We slightly simplify these computations and use remark 1 to get Algorithm 1.

We know (Szegö, 1975) that the Laguerre polynomials L[α]t [, with usual normalization, follow]
the recurrence

2t + α 1
_L[α]t_ [(][λ][) =] _−_ _L[α]t_ 1[(][λ][) +][ t][ +][ α][ −] [1] _L[α]t_ 2[(][λ][)] (40)

_t_ _−_ [1]t [λ] _−_ _t_ _−_

 

As we don’t have to shift the domains, we have only to apply lemma 7 to get the Laguerre
method.Further, we can get a explicit expression for δt = _t+tα_ [, simplifying the expression.]


-----

**Algorithm 2: Laguerre(α)**
**Inputs: Initial vector x0, function f**
**_xfor−1 t ← = 10, . . ., T do_**

**_xt ←_** **_xt−1 +_** _t[t]+[−]α[1]_ [(][x][t][−][1][ −] **_[x][t][−][2][)][ −]_**


1

_t+α_ _[∇][f]_ [(][x][t][−][1][)]


C Proofs of section 3

In the following we will consider shifted versions of the spectral distributions. This shift is
written as an affine transform m(λ) : [0, L] → [−1, 1] because most results in the theory of
orthogonal polynomials are stated in terms of distributions supported in [−1, 1].
This can be seen as an additional layer of abstraction because the quantities evaluated with
the shifted distributions and polynomials are proportional, i.e. if Pt(x) = _P[˜]t(m(x)) and_
_µ[′](x) = ˜µ[′](m(x)):_

_Pt[2][(][x][)][µ][′][(][x][) d][x][ ∝]_ _P˜t[2][(][x][)˜]µ[′](x) dx_ (41)
Z Z

So all the asymptotics are the same. The Jacobi polynomials Pt[α,β] are those orthogonal w.r.t
_dµ(x) = (1_ _x)[α](1 + x)[β]. Most works use the normalization_ _P[˜]t[α,β](_ 1) = ( 1)[t][ ][t][+]t[β] . We
_−_ _−_ _−_
will write _P[˜]t[α,β]_ for this normalization and Pt[α,β] for the residual polynomials


**Theorem 2 (GCM average-case rates). A Generalized Chebyshev Method with parameters**
(α, β) applied to a problem with e.s.d. as in Assumption 1 has average-case rates


_t[−][1][−][2][β]_ _if α < τ + 1/2 and β < ξ + 3/2_
_t[−][2(][ξ][+2)]_ log t _if α = τ + 1/2 and β = ξ + 3/2_ _,_
_t[2(max][{][α][−][β][−][τ,][−][ξ][−][1][}−][1)]_ _if α > τ + 1/2 or β > ξ + 3/2_

(13)

_t[−][1][−][2][β]_ _if α < τ + 1/2 and β < ξ + 5/2_
_t[−][2(][ξ][+3)]_ log t _if α = τ + 1/2 and β = ξ + 5/2_ _,_
_t[2(max][{][α][−][β][−][τ,][−][ξ][−][2][}−][1)]_ _if α > τ + 1/2 or β > ξ + 5/2_

(14)


E[f (xt) _f_ (x[⋆])] _L_ _C1[α,β],ν_
_−_ _∼_ _·_

E[ _f_ (xt) 2[]][ ∼] _[L][2][ ·][ C]2[α,β],ν_
_||∇_ _||[2]_


_where Cν[α,β]_ _is a distribution dependent constant._

_Proof. We will prove that for any α and β, ξ, τ > −1, l > 0 and ν following Assumption 1,_
we have


_t[−][1][−][2][β]_ if α < τ + 1/2 and β < ξ + 1/2
_t[−][2(][ξ][+1)]_ log t if α = τ + 1/2 and β = ξ + 1/2
_t[2(max][{][α][−][β][−][τ,][−][ξ][}−][1)]_ if α > τ + 1/2 or β > ξ + 1/2


_Pt[α,β](x)[2]x[l]dντ,ξ−l(x) ∼_ _L[l]Cν[α,β]_


We will first show this result for the Beta weights, then show that distributions with the
same concentration behave similarly.
The normalization of _P[˜]t[α,β]_ is s.t.[ Szegö (1975) (4.3.3)]:

1

2[α][+][β][+1] Γ(n + α + 1)Γ(n + β + 1)

Z−1 _P˜t[α,β](x)(1−x)[α](1+x)[β]_ dx = 2n + α + β + 1 Γ(n + 1)Γ(n + α + β + 1) [= Θ(][t][−][1][)][ (42)]


Further, the residual polynomials are s.t. _Pt[α,β]_ = Θ(t[−][β]) _P[˜]t[α,β]_, from the definition of the
_|_ _|_ _|_ _|_
classical normalization.
We state the result (Exercise 91, Generalisation of 7.34.1) from Szegö (1975):


-----

**Lemma 9. We have**
1

0

Z


(1 − _x)[τ]_ _Pt[α,β](x)[2]dx ∼_ Θ(h[α]τ [)] (43)


_t[2(][α][−][τ]_ _[−][1)]_ _if α > τ + 1/2_
_t[−][1]_ log t _if α = τ + 1/2_ (44)
_t[−][1]_ _if α < τ + 1/2_


_h[α]τ_ [:=]


Noting that _P[˜]t[α,β](x) = (_ 1)[t][ ˜]Pt[β,α]( _x), we can write:_
_−_ _−_
1 1 1

_P˜t(x)[2](1_ _x)[τ]_ (1+x)[ξ]dx = Θ (1 _x)[τ]_ _P[˜]t[α,β](x)_ _dx_ +Θ (1 _x)[ξ]_ _P[˜]t[β,α](x)_ _dx_
1 _−_ 0 _−_ _|_ _|[2]_ 0 _−_ _|_ _|[2]_

Z− Z  Z 

(45)
We can then show our result for dντ,ξ−l(x) = x[ξ][−][l](L − _x)[α]_ by carefully considering each of
the cases on Lemma 9 and the maximum of each term in eq. 45, and an added t[−][2][β] from
the different normalization. With this, we have the wanted result for the Beta weights
It remains to show:
1 1

_P˜t[α,β](x)[2]_ dντ,ξ(x) = Θ (1 _x)[τ][ ˜]Pt[α,β](x)[2]dx_ (46)
0 0 _−_

Z Z 

And the rest follows from the same arguments. We do this with the help of this lemma
shown in Van Assche (1995) relating to the weak convergence of the orthogonal polynomials:

**Lemma 10. Let µ be a measure and (pt) it‘s family of orthonormal polynomials s.t. pt**
_follow the recurrence:_

_xpt(x) = atpt+1(x) + btpt(x) + at−1pt−1(x)_

_and at, bt converge respectively to a, b. Then for any f continuous and bounded:_

1

_f_ (x)

_f_ (x)p[2]t [(][x][)][dµ][(][x][)][ →] [1] (47)
Z _π_ Z−1 _√1 −_ _x[2][ dx]_

Let ϵ s.t.
_x_ 1 _ϵ_ dντ,ξ _A(1_ _x)[τ]_ _B(1_ _x)[τ]_ (48)

We observe that for 0 < x < ≥ 1 − − ⇒|ϵ, f (x) = −(1−xdν)[α] −τ,ξ(1+x)[β]| ≤[ is bounded.] −

We get from an application of 10, and the observation that _P[˜]t[α,β]_ = _tp[α,β]t_, with _t =_
_N_ _N_
Θ(t[−][1][/][2]):
1 1−ϵ 1

(1 _x)[τ][ ˜]Pt[α,β](x)[2]_ dx = (1 _x)[τ][ ˜]Pt[α,β](x)[2]_ dx + (1 _x)[τ][ ˜]Pt[α,β](x)[2]_ dx (49)

Z0 _−_ Z0 _−_ Z1−ϵ _−_ _⇒_

Θ(h[α]τ [)] Θ(t[−][1])

1

| {z } | {z }

Z1−ϵ(1 − _x)[τ][ ˜]Pt[α,β](x)[2]_ dx = Θ(h[α]τ [)] (50)

1 1−ϵ

_P˜t[α,β](x)[2]_ dντ,ξ(x) = _P˜t[α,β](x)[2]f_ (x)(1 _x)[α](1 + x)[β]_ dx)
0 0 _−_

Z Z

Θ(t[−][1])
| {z } (51)

1

+ Θ  (1 _x)[τ][ ˜]Pt[α,β](x)[2]_ dx

Z1−ϵ _−_ 
 Θ(h[α]τ [)] 
 
 

| {z }


-----

**Theorem 3 (Optimal Rates). Let ν follow Assumption 1. The optimal asymptotic average-**
_case rates for E[f_ (xt) − _f_ (x[⋆])] and E[||∇f (xt)||2[2][]][ are attained by the GCM with parameters]
(τ, ξ + 2) and (τ, ξ + 3), respectively, and read

E[f (xt) − _f_ (x[⋆])] = Θ(t[−][2(][ξ][+2)]), E[||∇f (xt)||2[2][] = Θ(][t][−][2(][ξ][+3)][)][.]

_Proof. We will prove that for τ, ξ > −1 If α = τ and β = ξ + l + 1 (i.e., are optimal), the_
rate of convergence reads

_l_

min _Pt[2][(][λ][)][λ][l][dν][(][λ][) = Θ]_ _P˜t[α,β](λ)[2](L_ _λ)[τ]_ _λ[ξ][+][l]_ dλ = Θ(t[−][2(][ξ][+][l][+1)]) (52)
_Pt(0)=1_ 0 _−_ !

Z Z

Showing the second equality is easy by considering theorem 2, and that is further the
minimum asymptotic rate for the Beta distribution µτ,ξ.
By setting p[ν]t [and][ P]t[ ν] [=] _p[ν]tp[(0)][ν]t_ [ the optimal orthonormal and residual and polynomials w.r.t.]

_ν we show that Pt[ν]_ [must have the same rate on][ µ][τ,ξ] [as it does on][ ν][, thus the optimal rate of]
_ν cannot be lower than the optimal rate of µτ,ξ. Indeed, setting ϵ1, ϵ2 as in eq. 48:_
1 1

_Pt[ν][(][x][)][2][dν][(][x][) =]_ Θ _Pt[ν][(][x][)][2][dµ][τ,ξ][(][x][)]_ (53)
1 _ϵ2_ 1 _ϵ2_

Z _−_  Z _−_ 

1+ϵ1 1+ϵ1
_−_ _−_

_Pt[ν][(][x][)][2][dν][(][x][) = Θ]_ _Pt[ν][(][x][)][2][dµ][τ,ξ][(][x][)]_ (54)
1 1

Z− Z− 

1−ϵ2 1−ϵ2 1

_Pt[ν][(][x][)][2][dν][(][x][) = Θ]_ _Pt[ν][(][x][)][2][dµ][τ,ξ][(][x][)]_ = Θ (55)
1+ϵ1 1+ϵ1 _p[ν]t_ [(][−][1)][2]

Z− Z−   

(56)

Where the first two equations come from the fact that ν = Θ(µτ,ξ) near 1 and 1 and the
_−_
third from lemma 10.
This effectively upper bounds the rates on ν because the rates of Pt[ν] [on][ µ][τ,ξ] [can’t be lower]
than −2(ξ + l + 1).

**Proposition 3 (GCM worst-case rates). Let f be a convex, L-smooth quadratic function.**
_Then, for the Generalized Chebyshev Method with parameters (α, β), we have worst-case rates_


_t[2(][α][−][β][)]_ _if_ _α > β −_ 1
_t[−][1][−][2][β],_ _if_ _α_ _β_ 1 _β_ 2 _,_ (15)
_≤_ _−_ _≤_ [1]

_t[−][2],_ _if_ _α_ _β_ 1 _β_ 2
_≤_ _−_ _≥_ [1]

_t[2(][α][−][β][)]_ _if_ _α > β −_ 2
_t[−][1][−][2][β],_ _if_ _α ≤_ _β −_ 2 _β ≤_ 3/2 _._ (16)
_t[−][4],_ _if_ _α ≤_ _β −_ 2 _β ≥_ 3/2


_f_ (xt) _f_ (x[⋆]) _C1L_
_−_ _≤_

_f_ (xt) _f_ (x[⋆]) _C2L[2]_
_||∇_ _−_ _|| ≤_


_Proof. rates] We will prove that: supx_ [0,L] x[l]Pt[α,β](x)[2] = O(L[l]t[v][(][α,β,l][)]). Where:
_∈_

2(α − _β)_ if _α > β −_ _l_

_v(α, β, l) =_ 1 2β, if _α_ _β_ _l_ _β_ _l_ 2

 _−_ _−_ _≤_ _−_ _≤_ _−_ [1]
 _−2l,_ if _α ≤_ _β −_ _l_ _β ≥_ _l −_ [1]2

From Szegö (1975), Theorem 7.32.2, if _θ <_ _[π]2_ [:]


(57)


_O(t[−][1][/][2])_ if _α <_ 2

_P˜t[α,β](cos θ) =_ _O(t[α])_ if _α_ 2 _[,][ 0][ ≤] −[θ][1][ ≤]_ _[ct][−][1]_ (58)

 _≥−_ [1]
 _θ[−][α][−][1][/][2]O(t[−][1][/][2])_ if _α_ 2 _[, θ > ct][−][1]_

_≥−_ [1]

We observe that, from the symmetry of the Jacobi polynomials:



sup _x[l]Pt[α,β](x)[2]_ = Θ max sup _Pt[α,β](x)[2], sup_ (1 _x)[l]Pt[β,α](x)[2]_ (59)
_x_ [0,L] (x [0,1] _x_ [0,1] _−_ )!
_∈_ _∈_ _∈_

The (1 − _x)[l]_ term, corresponds to (2 sin( _[θ]2_ [))][l][ in the variable][ θ][, which is][ O][(][θ][2][l][)][. The rest]

follows from carefully considering the expressions given by eq. 58.


-----

**Theorem 4 (Nesterov average-case rates). Let ν as in Assumption 1. Then for the Nesterov**
_method, we have average-case rates_

_t[−][2(][ξ][+2)]_ _if ξ < −1/2_
E[f (xt) − _f_ (x[⋆])] ∼ _C1[′]_ _,ν_ _t[−][3]_ log t _if ξ = −1/2_ _,_ E[||∇f (xt)||2[2][]][ ∼] _[C]2[′]_ _,ν[t][−][(][ξ][+9][/][2)][.][ (17)]_
_t[−][(][ξ][+7][/][2)]_ _if ξ >_ 1/2
n
_−_

_Proof. We will prove:_


1

0

Z


_t[−][2(][ξ][+1)]_ if 0 < ξ < 1/2
_Pt[Nes](λ)[2]λ[l]_ dντ,ξ−l ∼ _Cν[′]_ _t[−][3]_ log t if ξ = 1/2 (60)
_t[−][(][ξ][+5][/][2)]_ if ξ > 1/2
n


Paquette et al. (2020) has shown that the nesterov polynomials Pt are asymptotically, in t:


_Pt(λ)_
_∼_ [2][J][1]t√[(][t]


_√αλ)_

_e[−][αλt/][2]_ (61)
_αλ_


In the sense that:
1

_u[l]_ _Pt[2]˜[(][u][)][ −]_ [4][J]1[2][(][t][√][u][)] _e[−][ut]_ 4 dµMP (u) = O(t[−][(][l][+25][/][12))] (62)
0 _t[2]u_

Z  

The arguments can be easily used to show that such an integral is O(t[−][(][α][+][l][+31][/][12)]) when
evaluated wrt a general dµ s.t µ[′] = Θ(λ[α]) near 0.
We can thus consider our integral of interest substituting Pt[Nes] by it’s Bessel asymptotic
and dividing it into three regions, i.e. [0, 1] = [0, _[ϵ]t_ []][ ∪] [[][ ϵ]t _[,]_ _√ϵt_ []][ ∪] [[][ ϵ]√t _[,][ 1]][ corresponding to two]_

different regimes for the Bessel function. The first region will give us the asymptotic and the
others we will bound.
We consider first, for some ϵ > 0:
_√ϵt_ 1 [(][t][√][u][)]

_u[ξ][ 4][J]_ [2] _e[−][ut]_ du (63)
_ϵ_ _t[2]u_

Z


We note the asymptotic for J1[2][:]

_J1[2][(]√_


_tv + 2γ))_ (64)


_tv) ∼_


_tv_ [(1 + cos(2]


Doing the change of variable v = tu, and identifying the upper limit of the interval, which is
_ϵt[1][/][2]_ to ∞:

_√ϵt_ _u[ξ][ 4][J]1[2][(][t][√][u][)]_ _e[−][ut]_ du = Θ _t[−][2][−][ξ]_ _∞_ _v[ξ][−][1]J1[2][(]√tv)e[−][v]_ dv (65)
Z _ϵt_ _t[2]u_  Zϵ 

_∞_ 1
= Θ _t[−][2][−][ξ]_ _ϵ_ _v[ξ][−][1]_ _π√tv [e][−][v][ d][v]_ (66)
 Z 

_∞_ 1

= Θ  2 _[−][ξ]_ _ϵ_ _v[ξ][−]_ 2[3] _π√tv [e][−][v][ d][v]_ (67)

 Z 
 Γ(ξ− 2[1] _[,ϵ][)]_ 
[t][−] [5] 

Where the cosinus term goes to 0 from the Riemann-Lebesgue lemma and| {z Γ} is the incomplete
Gamma function.
The term corresponding to the interval [ϵt[−][1][/][2], 1] is exponentially small. Indeed, because of
the exponential e[−][ut] it is O(e[−][ϵ]√t). This shows that the integral concentrates in a region

that is closer and closer to 0 and that only the behaviour of the distribution near 0 matters.
We have for the [0, _[ϵ]t_ []][ region, doing the change of variables][ v][ =][ t][2][u][:]

_ϵt_ _u[ξ][ 4][J]1[2][(][t][√][u][)]_ _e[−][ut]_ du = Θ _t[−][2(][ξ][+1)]_ _tϵ_ _v[ξ][ J]1[2][(][√][v][)]_ _e[−]_ _[v]t dv_ (68)

0 _t[2]u_ 0 _v_

Z  Z 


-----

And the e _−tv_ is Θ(1). We have the following Bessel asymptotics:

_J1[2][(][√][v][)]_

_v_ 0 (69)

_v_ _∼_ 4[1] _[,]_ _→_

_J1[2][(][√][v][)]_

= O(v[−][3][/][2]), _v_ (70)
_v_ _→∞_

So we divide this integral aswell:

_tϵ_ _tϵ_
_t[−][2(][ξ][+1)]_ _v[ξ][ J]1[2][(][√][v][)]_ _e[−]_ _[v]t dv = Θ_ _t[−][2(][ξ][+1)]_ _v[ξ][−]_ 2[3] dv = Θ _Iξ(t)t[−][ξ][−]_ 2[5] (71)

1 _v_ _ϵ_

Z 1  Z   

_t[−][2(][ξ][+1)]_ _v[ξ][ J]1[2][(][√][v][)]_ _e[−]_ _[v]t dv = Θ_ _t[−][2(][ξ][+1)]_ _ϵ[1]v[ξ]_ dv = Θ _t[−][2(][ξ][+1)][]_ (72)

0 _v_ 0

Z  Z  

Where Iξ(t) = log t if ξ = 2[1] [and][ 1][ otherwise.]

The nesterov rate is then Iξ(t)t[−][ξ][−] 2[5] if ξ ≥ [1]2 [and][ t][−][2(][ξ][+1)][ if][ 0][ < ξ <][ 1]2

**Theorem 5 (Gradient Descent average-case rates). Let ν as in Assumption 1. Then for**
_gradient descent_


E[f (xt) − _f_ (x[⋆])] = Θ(t[−][(][ξ][+2)]), E[||∇f (xt)||2[2][] = Θ(][t][−][(][ξ][+3)][)][.] (18)

_Proof. Considering that Pt[GD](λ) = (1_ _L_ [)][t][ we will prove :]
_−_ _[λ]_

1

(1 _λ)[2][t]λ[l]_ dντ,ξ _l = Θ(t[−][(][ξ][+][l][+1)]_ (73)
0 _−_ _−_

Z

We know, for the Beta weights, that:
1

(1 _λ)[2][t][+][τ]_ _λ[ξ][+][l]_ dλ = [Γ(][l][ +][ ξ][ + 1)Γ(2][t][ +][ τ][ + 1)] = Θ(t[−][(][ξ][+][l][+1)]) (74)
0 _−_ Γ(2t + l + ξ + τ + 2)

Z

_ϵ_
We can indentify this asymptotic to the interval 0 [for any][ ϵ][ because:]
1
R

_ϵ_ (1 − _λ)[2][t][+][τ]_ _λ[ξ][+][l]_ dλ = O((1 − _ϵ)[2][t])_ (75)

Z

Then:
1

(1 _λ)[2][t]λ[l]_ dντ,ξ _l =_ ((1 _ϵ)[2][t])_ (76)
_ϵ_ _−_ _−_ _O_ _−_

Z _ϵ_

0[ϵ](1 _λ)[2][t]λ[l]_ dντ,ξ _l = Θ_ (1 _λ)[2][t][+][τ]_ _λ[ξ][+][l]_ dλ = Θ(t[−][(][ξ][+][l][+1)]) (77)
_−_ _−_ 0 _−_
Z Z 


**Theorem 6 (Laguerre method rates). Let α >** 1 and µα, α > 1 be a Gamma distribution,
_x[α]e[−][x]_ _−_ _−_
_i.e. dµα(x) =_ Γ(α+1) _[. The optimal rates are given by the Laguerre method of appropriate]_

_tuning and_
E[f (xt) − _f_ (x[⋆])] = Θ(t[−][(][α][+2)]). (19)

_Proof. Let L[α]t_ [be the Laguerre polynomials with the usual normalization][ Szegö][ (][1975][):]
_n + α_
_L[α]t_ [(][x][)][2][dµ][α][(][x][) =][ L]t[α][(0) =] (78)
_n_
Z  

We further now [Szegö (1975) (5.1.13)]]:

_t_

_L[α]t_ [(][x][) =][ L]t[α][+1](x) (79)
_k=0_

X


-----

Thus, letting Pt[α] [be the residual laguerre polynomial, we consider:]

_t + α + 2_ _−2_
E[f (xt) − _f_ (x[⋆])] = _Pt[α][+2](λ)[2]dµα+1(λ) =_ _t_ _L[α]t_ [+2]dµα+1(λ)
Z _t + α + 2_ _−2_ _t_   Z

= _t_ _L[α]k_ [+1](λ)dµα+1(λ)
  _k=0_ Z 

X

_t + α + 2_ _−2_ _t_ _k + α + 1_ _t + α + 2_ _−2_ _t + α + 2_
= =
_t_ _k_ _t_ _t_
  _k=0_     

X

_t + α + 2_ _−1_
= = Θ(t[−][(][α][+2)])
_t_
 


(80)


D Additional Experiments

Figure 7: Empirical vs Theoretical function-value performance for GCM(α[⋆], β[⋆]) . Red lines
are given by numerical integration, shades are minimum and maximum values under 10 runs
and the blue line is the mean

Figure 8: Empirical vs Theoretical function-value performance under Marchenko Pastur
distribution. Red lines are given by numerical integration,shades are minimum and maximum
values under 10 runs and the blue line is the mean

We note that in the regimes where the empirical average performance doesn’t match the
theoretical one, we can still find samples of problems who do match. This and the much
larger variance on the function-value, this discrepancy is due to numerical unstability in
these regimes.


-----

