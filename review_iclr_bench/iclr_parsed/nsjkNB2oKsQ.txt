# OFF-POLICY REINFORCEMENT LEARNING WITH DELAYED REWARDS

**Anonymous authors**
Paper under double-blind review

ABSTRACT

We study deep reinforcement learning (RL) algorithms with delayed rewards. In
many real-world tasks, instant rewards are often not readily accessible or even defined immediately after the agent performs actions. In this work, we first formally
define the environment with delayed rewards and discuss the challenges raised due
to the non-Markovian nature of such environments. Then, we introduce a general
off-policy RL framework with a new Q-function formulation that can handle the
delayed rewards with theoretical convergence guarantees. For practical tasks with
high dimensional state spaces, we further introduce the HC-decomposition rule
of the Q-function in our framework which naturally leads to an approximation
scheme that helps boost the training efficiency and stability. We finally conduct
extensive experiments to demonstrate the superior performance of our algorithms
over the existing work and their variants.

1 INTRODUCTION

Deep reinforcement learning (RL) aims at maximizing the cumulative reward of a MDP. To apply
RL algorithms, the reward has to be given at every state-action pair in general (i.e., r(s, a)). With
a good and high quality reward function, RL can achieve remarkable performance, e.g. AlphaGo
Zero for Go (Silver et al., 2017), DQN(Mnih et al., 2015) for Atari, SAC (Haarnoja et al., 2018b)
for robot control e.t.c. Recently, RL has been applied in many other real world settings beyond
games and locomotion control. This includes industrial process control (Hein et al., 2017), traffic
optimization (Gong et al., 2019; Lin et al., 2018), molecular design (Olivecrona et al., 2017) and
resource allocation (Xu et al., 2018). However, in many of these real-world scenarios, Markovian
and instant per-step rewards are hard or impossible to obtain or even clearly defined. In practice,
it becomes more reasonable to use a delayed reward of several consecutive steps as feedback. For
example, in traffic congestion reduction (Gong et al., 2019), the amount of decreased congestion
for a single traffic light switch is hard to define in practice while it is more adequate to use the
average routing time consumed for the vehicles as the feedback. The latter is a delayed reward
which can only be obtained after the vehicles have passed the congestion (long after a single switch).
In molecular design, only the molecule formed after the final operation can provide a meaningful
evaluation for the whole process (Olivecrona et al., 2017). In locomotion control where the feedback
is generated by the interaction with the large environment, it is usually the case that the frequency of
the rewards generated from the environment’s sensors is much lower than the frequency of the robot
control, thus the rewards are given only every so often.

Despite the importance and prevalence of the delayed reward setting, very few previous research in
RL has been focusing on problems with non-Markovian properties and delayed rewards. Besides,
current RL algorithms lack theoretical guarantee under non-Markovian rewards and perform unsatisfactorily in practice (Gangwani et al., 2020). Thus, in this paper, our goal is to better understand
the properties of RL with delayed rewards and introduce a new algorithm that can handle the delayed reward both theoretically and practically. A key to our approach lies in the definition of the
past-invariant delayed reward MDPs. Based on this definition, theoretical properties and algorithmic implications are discussed to motivate the design of a new practical algorithm, which explicitly
decompose the value function into components of both historical (H) and current (C) step information. We also propose a number of ways to approximate such HC decomposition, which can be
readily incorporated into existing off-policy learning algorithms. Experiments demonstrate that such
approximation can improve training efficiency and robustness when rewards are delayed.


-----

2 PROBLEM FORMULATION

In order to characterize the delayed and non-Markovian reward signals, we introduce the Delayed
Reward Markov Decision Process (DRMDP). In this work, we focus on DRMDP that satisfies the
_Past-Invariant (PI) condition, which is satisfied in many real-world settings with delayed rewards._

2.1 PAST-INVARIANT DELAYED REWARD MDPS

In DRMDP, the transition of the environment is still Markovian and the agent can observe and
interact with the environment instantly. However, rewards may be non-Markovian and delayed and
are observed only once every few steps. More specifically, the time steps are divided into consecutive
_signal intervals of random lengths, and the reward signal generated during a signal interval may_
depend on the state-action sequence and is observed only at the end of the interval. We formally
define DRMDP as follows.
**Definition 1 (DRMDP). A Delayed Reward Markov Decision Process M = (S, A, p, qn, r, γ) is**
_described by the following parameters._

_1. The state and action spaces are S and A respectively._
_2. The Markov transition function is p(s[′]|s, a) for each (s, a) ∈_ _S ×_ _A; the initial state distribution_
_is p(s0)._

_3. The signal interval length is distributed according to qn(_ ), i.e., for the i-th signal interval, its

_·_
_length ni is independently drawn from qn(·)._

_4. The reward function r defines the expected reward generated for each signal interval; suppose_
_τi = τt:t+ni = (s, a)t:t+ni = ((st, at), . . . (st+n_ 1, at+ni 1)) is the state-action sequence
_−_ _−_
_during the i-th signal interval of length ni, then the expected reward for this interval is r(τi)._

_5. The reward discount factor is γ._

In this work, we focus on the infinite-horizon DRMDP. We use (τ, n) = (τi, ni)i 1,2,3,... to
_∈{_ _}_
denote a trajectory, where τi is the state-action sequence during the i-th signal interval and ni is
the corresponding length. We also let ti be first time step of the i-th signal interval, i.e., ti =
_i−1_
_j=1_ _[n][j][. Note that the reward][ r][(][τ][i][)][ is revealed at time][ t][i][ +][ n][i][ −]_ [1 =][ t][i][+1][ −] [1][. We finally define]
the discounted cumulative reward of the trajectory (τ, n) by R(τ, n) := _i=1_ _[γ][t][i][+1][−][1][r][(][τ][i][)][. The]_
P
objective for DRMDP is to learn a policy π that maximized the expected discounted cumulative
reward (π) := E(τ,n) _π[R(τ, n)]._
_J_ _∼_ [P][∞]

**Augmented Policy Class. In DRMDP, the Markov policy class {π = π(at|st)} might not achieve**
satisfactory performance because of the non-Markovian nature of the reward function. Theoretically
speaking, we need a more general policy classsignal interval that t belongs to). A formal statement is included in Appendix B. Unfortunately, Πτ = {π = π(at|τti:t ◦ _st)} (i is the index of the_
policy optimization in such an exponentially large space Πτ is hard in practice, especially for highdimensional problems. Thus, as a trade-off, we consider the policy class Πs = {π = π(at|st, t −
_ti)_, which resembles the traditional Markov policy class, but is augmented with an extra parameter
_}_
indicting the relative index of the current time step in the signal interval. Moreover, we focus on the
PI-DRMDP problems, in which our algorithmic framework has theoretical guarantees.
**Definition 2 (PI-DRMDP). A Past-Invariant Delayed Reward Markov Decision Process is a DR-**
_MDP M = (S, A, p, qn, r, γ) whose reward function r satisfies the following Past-Invariant (PI)_
_condition: for any two trajectory segments τ1 and τ2 of the same length, and for any two equal-_
_length trajectory segmentsunder the transition dynamics τ1[′] p[and] for all[ τ][ ′]2_ _a, b[such that the concatenated trajectories] ∈{1, 2}, it holds that_ _[ τ][a]_ _[◦]_ _[τ][ ′]b_ _[are feasible]_

_r(τ1 ◦_ _τ1[′]_ [)][ > r][(][τ][1] _[◦]_ _[τ][ ′]2[)][ ⇐]⇒_ _r(τ2 ◦_ _τ1[′]_ [)][ > r][(][τ][2] _[◦]_ _[τ][ ′]2[)][.]_

Roughly speaking, in PI-DRMDP, the relative credit for different actions at each st only depends on
the experience in the future and is invariant of the past. This property may relieve the agent from
considering the past experience for decision making. A simple example of r with PI condition is
_r(τt:t+n) =_ _i=t_ _rˆ(si, ai), where ˆr is a per-step reward function. This kind of tasks is studied_
in many previous work (Zheng et al., 2018; Oh et al., 2018; Klissarov & Precup, 2020). We refer
this kind of reward functions as the sum-form.

[P][t][+][n][−][1]


-----

In the worst case, there exists some bizarre reward design in PI-DRMDPs that still requires the
optimal policy to take history information into consideration (see Appendix B). Despite the suboptimality issue, we will show that our algorithmic framework guarantees policy improvement in
Πs, which is sufficient to support the design of actor-critic methods. Consequently, we are able
to put forward a practical RL algorithm that can achieve the SOTA performance in delayed-reward
tasks. The overall contribution of our work is summarized in Table 1 and we leave the optimality of
the DRMDP problems as future works.

Table 1: Contribution of our algorithm in different problem settings. Notice that MDP is a degraded
case of DRMDP, which has signal interval length fixed as 1.

Policy Improvement Policy Optimality

MDP Yes Yes
PI-DRMDP Yes No
DRMDP No No

**General Reward Function. In Definition 1, we define the reward as a function of the state-action**
sequence of its signal interval. In general, we may allow reward functions with longer inputs which
overlap with the previous signal intervals, e.g., maximal overlapping of c steps, r(τti−c:ti+ni ). The
theoretical analysis in Section 3.1 and the empirical method in Section 3.2 can be directly extended
to this general reward function. We provide detailed discussions in Appendix B for the general
definition while we only consider c = 0 in the main text for the simplicity of the exposition.

2.2 OFF-POLICY RL IN PI-DRMDP

Deep Reinforcement Learning has achieved great success in solving high-dimensional MDPs problems, among which the off-policy actor-critic algorithms SAC (Haarnoja et al., 2018a) and TD3
(Fujimoto et al., 2018) are the most widely used ones. However, since the rewards are delayed and
non-Markovian in PI-DRMDP, directly applying these SOTA off-policy RL algorithms faces many
challenges that degrade the learning performance. In this subsection, we briefly discuss the problems
that arise in critic learning based on TD3 and similar problems also exist for SAC.

First, in PI-DRMDP, value evaluation with off-policy samples brings in off-policy bias. TD3 minimizes over φ w.r.t.

_Lφ = ED_ h(Rt + γQ[ˆ]φ(st+1, a[′]t+1[)][ −] _[Q][φ][(][s][t][, a][t][))][2][i]_ _,_ (1)

where (st, at, Rt, st+1) is sampled from the replay buffer D, Rt = r(τi) if t = ti+1 _−1 and i = i(t)_
is the index of the reward interval that t belongs to, and Rt = 0 otherwise. _Q[ˆ]φ represents the target_
value of the next state and a[′]t+1 [is sampled from the smoothed version of the policy][ π][. Furthermore,]
in practical implementation, samples in the replay buffer D are not sampled from a single behavior
policy β as in traditional off-policy RL (Sutton & Barto, 2018). Instead, the behavior policy changes
as the policy gets updated. Thus, we assume that samples are collected from a sequence of behavior
policies β = {βk}k[K]=1[.]

In the delayed reward setting, since the reward r(τi) depends on the trajectory of the whole signal
interval (i.e., τi) instead of a single step, the function Qφ(st, at) learned via Eq. (1) will also have
to depend on the trajectory in the signal interval upto time t (i.e., τti:t) rather than the single stateaction pair at time t. Since the samples are collected under a sequence of behavior polices β,
different behavior policy employed at state st may lead to different distribution over τti:t in Eq. (1).
Consequently, Qφ(st, at) will be affected by this discrepancy and may fail to assign the accurate
expected reward for the current policy. Please refer to Appendix B for detailed discussions.

Second, in addition to the issue resulted from off-policy samples, Qφ(st, at) learned in Eq. (1) with
on-policy samples in D may still be problematic. We formally state this problem via a simple sumform PI-DRMDP in Appendix B whose optimal policy is in Πs. In this example, the fix point of
Eq. (1) fails to assign the actual credit and thus misleads policy iteration even when the pre-update
policy is already the optimal. We refer this problem as the fixed point bias.

Last but not least, the critic learning via Eq. (1) would suffer a relative large variance. Since Rt
varies between 0 and r(τi), minimization of TD-error (Lφ) with a mini-batch of data has large


-----

variance. As a result, the approximated critic will be noisy and relative unstable. This will further
effect the policy gradients in Eq. (2).


_∇θJ(πθ) = ED_


_∇at_ _Qφ(st, at)_ _πθ(st)[∇][θ][π][θ][(][s][t][)]_ (2)



To sum up, directly applying SOTA off-policy algorithms in PI-DRMDP will suffer from multiple
problems (off-policy bias, fixed point bias, large traning noise, etc). Indeed, these problems result
in a severe decline in performance even in the simplest task when n = 5 (Gangwani et al., 2020).

3 METHOD

In this section, we first propose a novel definition of the Q-function (in contrast to the original Qfunction) and accordingly design a new off-policy RL algorithm for PI-DRMDP tasks. This method
has better theoretical guarantees in both critic learning and policy update. We then introduce a HC_decomposition framework for the proposed Q-function, which leads to easier optimization and better_
learning stability in practice.

3.1 THE NEW Q-FUNCTION AND ITS THEORETICAL GUARANTEES

Since the non-Markov rewards make the original definition of Q-function ambiguous, we instead
define the following new Q-function for PI-DRMDP tasks.

_∞_

(τti:t+1) := E(τ,n) _π_ _γ[t][j][+1][−][t][−][1]r(τj)_ _τti:t+1_ _,_ (3)
_Q[π]_ _∼_  

_j=i_

X
 

The new -function is defined over the trajectory segments τti:t+1, including all previous steps in
_Q_
_st’s signal interval. Besides, the expectation is taken over the distribution of the trajectory (τ, n) that_
is due to the randomness of the policy, the randomness of signal interval length and the transition
dynamics. Despite the seemingly complex definition of Q, we provide a few of its nice properties
that are useful for PI-DRMDP tasks as follows. The proofs are listed in Appendix B.

First, we consider the following objective function

_Lφ := ED_ (Rt + γQ[ˆ]φ(τtj :t+2) −Qφ(τti:t+1))[2][i] _,_ (4)
h

where τtj :t+2 = τti:t+1 (st+1, a[′]t+1[)][ (so that][ j][ =][ i][) if][ t][ is not the last step of][ τ][i][, and][ τ][t]j [:][t][+2] [=]
(sti+1 _, a[′]ti+1_ [)][ (so that][ j][ =] ◦[ i] [+1][) otherwise. Similarly to Eq. (1), in Eq. (4),][ (][τ][t]i[:][t][+1][, R][t][, s][t][+1][)][ is also]
sampled from D and a[′]t+1 [is sampled from][ π][ correspondently. We may view Eq. (4) as an extension]
of Eq. (1) for Q[π]. However, with these new definitions, we are able to prove the following fact [1].
**Fact 1. For any distribution D with non-zero measure for any τti:t+1,** (τti:t+1) is the unique
_Q[π]_
_fixed point of the MSE problem in Eq. (4). More specifically, when fixing_ _Q[ˆ]φ as the corresponding_
_Q[π], the solution of the MSE problem is still Q[π]._

Consequently, Q[π] can be found precisely via the minimization for tabular expressions. This helps
to solve the problems in critic learning for PI-DRMDP. We next introduce Fact 2 which states that
the order of Q[π] w.r.t. the actions at any state st is invariant to the choice of τti:t, thanks to the
PI condition.
**Fact 2.transition dynamics, For any τti:t, τπt[′]i:t** Π[and]s, we have that[ s][t][, a][t][, a]t[′] _[which both][ τ][t]i[:][t]_ _[◦]_ _[s][t]_ _[and][ τ][ ′]ti:t_ _[◦]_ _[s][t]_ _[are feasible under the]_
_∀_ _∈_

_Q[π](τti:t ◦_ (st, at)) > Q[π](τti:t ◦ (st, a[′]t[))][ ⇐]⇒Q[π](τt[′]i:t _[◦]_ [(][s][t][, a][t][))][ >][ Q][π][(][τ][ ′]ti:t _[◦]_ [(][s][t][, a]t[′] [))][.]

Fact 2 ensures that the policy iteration on πk(at _st, t_ _ti) with an arbitrary_ (τti:t (st, at)) results
_|_ _−_ _Q[π]_ _◦_
in the same πk+1 Πs. Consequently, we are able to prove the convergence theorem for the policy
iteration with the Q ∈-function.

1The definition of Qπ and Fact 1 also holds in DRMDP and for π ∈ Πτ .


-----

**Proposition 1. (Policy Improvement Theorem for PI-DRMDP)policy iteration w.r.t. Q[π][k]** _produces policy πk+1 ∈_ Πs such that ∀τti For any policy:t+1, it holds that πk ∈ Πs, the

(τti:t+1) (τti:t+1),
_Q[π][k][+1]_ _≥Q[π][k]_

_which implies that_ (πk+1) (πk).
_J_ _≥J_

Thus, the off-policy value evaluation with Eq. (4) and the policy iteration in Proposition 1 guarantee
that the objective function is properly optimized and the algorithm converges. Besides, for off-policy
actor critic algorithms in the continuous environment, the policy gradient _θ_ (πθ) is changed to
_∇_ _J_


_∇θJ (πθ) = ED_


_∇at_ _Q[π](τti:t ◦_ (st, at)) _πθ(st,t−ti)[∇][θ][π][θ][(][s][t][, t][ −]_ _[t][i][)]_ _,_ (5)


where (τti:t, st) is sampled from D. The full algorithm is summarized in Algorithm 1. As a straightforward implementation, we approximate (τti:t+1) with a GRU network (Cho et al., 2014) and
_Q[π]_
test it on the continuous PI-DRMDP tasks. As shown in Figure 2(a), our prototype algorithm
already outperforms the original counterparts (i.e., SAC) on many tasks.

3.2 THE HC-DECOMPOSITION FRAMEWORK

One challenge raised by the Q-function is that it takes a relatively long sequence of states and
actions as input. Directly approximating the Q-function via complex neural networks would suffer
from lower learning speed (e.g., recurrent network may suffer from vanishing or exploding gradients
(Goodfellow et al., 2016)) and computational inefficiency. Furthermore, the inaccurate estimation of
(τti:t+1) will result in inaccurate gradients in Eq. (5) which degrades the learning performance.
_Q[π]_

To improve the practical performance of our algorithm for high dimensional tasks, we propose the
HC-decomposition framework (abbreviation of History-Current) that decouples the approximation
task for the current step from the historical trajectory. More specifically, we introduce Hφ and Cφ
functions and require that

_φ(τti:t+1) = Hφ(τti:t) + Cφ(st, at),_ (6)
_Q_

Here, we use Cφ(st, at) to approximate the part of the contribution to Qφ made by the current step, and use Hφ(τti:t) to approximate the rest part that is due to the historical trajectory in the
signal interval. The key motivation is that, thanks to the Markov
transition dynamics, the current step (st, at) has more influence
than the past steps on the value of the future trajectories under
the given policy π. Therefore, in Eq. (6), we use Cφ to highlight
this part of the influence. In this way, we believe the architecture
is easier to optimize, especially for Cφ whose input is a single
state-action pair.


Moreover, we also find that the policy gradient will only depend on Cφ(st, at). Indeed, we calculate that the policy gradient
_θ_ [HC](πθ) equals to
_∇_ _J_


Figure 1: Policy gradient variance averaged over the training
process. All bars show the mean
and one standard deviation of 4
seeds.


ED


_∇at_ _Cφ(st, at)_ _πθ(st,t_ _ti)[∇][θ][π][θ][(][s][t][, t][ −]_ _[t][i][)]_ _,_ (7)
_−_ 


Comparing Eq. (7) with Eq. (5), we note that the gradient on the policy π under HC-decomposition
and the training becomes more efficient. In Figure 1, we visualize and compare the scale of thewill not be effected by τti:t. Thus, for a mini-batch update, our policy gradient has less variance
gradient variance of our HC-decomposition framework and the straightforward recurrent network
approximation of the Q function, w.r.t. the same policy and the same batch of samples. Clearly, the
result supports our claim that HC-decomposition results in less gradient variance.

Finally, to learn the Hφ and Cφ functions, we minimize the following objective function

_φ_ =ED[(Rt + γ( H[ˆ]φ(τti:t+1) + C[ˆ]φ(st+1, a[′]t+1[))][ −] [(][H][φ][(][τ][t]i[:][t][) +][ C][φ][(][s][t][, a][t][)))][2][] +][ λL][reg][(][H][φ][)][,]
_L[HC]_
(8)


-----

(a) Performance on High-Dimensional Tasks (b) Ablation on Regulation
Figure 2: (a): Performance of different implementations of HC-decomposition framework (Q-HC),
_Q-RNN and vanilla SAC. The task is the continuous sum-form PI-DRMDP task and qn is a uniform_
distribution between 15 and 20. The results show the mean and standard deviation of 7 seeds each.
**(b) Ablation on the Lreg. Each dashed line is the no-regularization version of the corresponding**
_Q-HC of the same color._

where the first term is the TD error in Eq. (4) and the second term is a regularizer on Hφ. We use
the regularization to stabilize the optimization process and prevent Hφ from taking away too much
information of the credit on the choice of at. In Section 4.1, we will compare several simple choices
of Hφ and Lreg. The search for other designs of Hφ and the regularization term in various settings
is left as future work.

4 EXPERIMENT

In this section, we first test and illustrate our algorithmic framework and HC-decomposition on highdimensional PI-DRMDP tasks to validate our claims in previous sections. Then, we compare our
algorithm with several previous baselines on sum-form delayed reward tasks. Our algorithm turns
out to be the SOTA algorithm which is most sample-efficient and stable. Finally, we demonstrate
our algorithm via an illustrative example.

4.1 DESIGN EVALUATION

We implement the following architectures of Hφ in our experiment. Our algorithm is based on SAC
(Haarnoja et al., 2018a).

**-HC-RNN. Hφ(τti:t) is approximated with a GRU network.**
_Q_

_Q-HC-Pairwise-K. Hφ(τti:t) is the sum of K+1 networks {c[k]φ[}][k][=0][,][1][,...K][ for different pairwise]_
terms as follows[2]. K = 1, 3 are included in the experiment.


_c[k]φ[(][τ][j][:][j][+1]_

_[◦]_ _[τ][j][+][k][:][j][+][k][+1][)][.]_
_j∈[Xti:t−k]_


_Hφ(τti:t) =_


_k∈[0:K+1]_


**-HC-Singleton. Hφ(τti:t) is the sum of single step terms.**
_Q_


_Hφ(τti:t) =_


_bφ(τj:j+1)._
_j∈X[ti:t]_


In terms of the representation power of these structures, (-RNN) is larger than (-Pairwise) larger than
(-Singleton). With a small representation power, critic’s learning will suffer from large projection
error. In terms of optimization, (-Singleton) is easier than (-Pairwise) and easier than (-RNN). As
discussed in Section 3.2, non-optimized critic will result in inaccurate and unstable policy gradients.
The practical design of Hφ is a trade-off between the efficiency of optimization and the scale of
projection error. For the regularizer in Eq. (8), we choose the following for all implementations.

_Lreg(Hφ) = ED_ (Hφ(τi) − _r(τi))[2][]_ _,_ (9)

2 0
Input for cφ [is thus a single state-action pair.] 


-----

where τi is sampled from the D. As discussed in Section 3.2, the history part Hφ(τti:t) should only
be used to infer the credit of τti:t within the signal interval τi. However, without the regularization,
the approximated Hφ may deprive too much information from Qφ than we have expected. Thus, in
a direct manner, we regularize Hφ(τi) to the same value of r(τi).

In Figure 2, we compare these algorithms on sum-form PI-DRMDP continuous control tasks based
on OpenAI Gym. Rewards are given once every n steps which is uniformly distributed from 15 to
20. The reward is the sum of the standard per-step reward in these tasks. Our method is compared
with vanilla SAC and Q-RNN whose Q[π] is approximated by a GRU network. The results verify our
claims made in Section 3 and the necessity of the design of our method.

1. Algorithms under the algorithmic framework (i.e., with prefix Q) outperform vanilla SAC. Our
framework approximates Q[π] while vanilla SAC cannot handle non-Markovian rewards.

2. Q[π] approximated with HC-decomposition architecture (i.e., with prefix Q-HC) is more sampleefficient and more stable than Q-RNN. Indeed, the HC architecture results in an easier optimization in critic learning.

3. Figure 2(b) shows the importance of regularization, especially when Hφ has complex form (i.e.,
_Q-HC-RNN) and thus is more likely to result in ill-optimized C-part._

**More Experiments. In Appendix C.1, we show that the empirical results are consistent in a variety**
of PI-DRMDP tasks and under different implementations. First, we test in sum-form tasks with
different interval length distributions (qn). As shown in Table 2, our -HC methods are scalable to
_Q_
sum-form tasks with a signal interval length of 60 environmental steps, which is quite a long period
in the Gym benchmark, and perform well in various random length tasks. Then, in non-sum-form
tasks with Max and Square reward functions, we find that Q-HCs still outperform Q-RNN and the
vanilla SAC consistently in all environments (Figure 5). This suggests that the advantage of HCdecomposition also holds in other PI reward function tasks. Moreover, in Figure 6, we also test
TD3-based Q-HC-Singleton and Q-HC-Pairwise-1. Comparing to TD3-based Q-RNN and vanilla
TD3 (Fujimoto et al., 2018), these two variants outperform the baselines uniformly.

To sum up, based on the empirical evidence, we conclude that our Q-HC is widely applicable in
PI-DRMDPs as a remedy to the unstable optimization of direct Q approximation. As Q-HC is an
implementation of the general algorithm in Section 3.1, it handles the non-Markovian rewards with
theoretical support. In future work, we will explore the boundary of HC-decomposition, and analyze
the effect of the projection error mathematically.

4.2 COMPARATIVE EVALUATION

Here, we compare our algorithm with previous algorithms in sum-form high-dimensional delayed
reward tasks. The following baselines are included in the comparison.

-  LIRPG (Zheng et al., 2018). It utilizes intrinsic rewards to make policy gradients more efficient.
The intrinsic reward is learnt by meta-gradient from the same objective function. We use the
same code provided by the paper.

-  RUDDER (Arjona-Medina et al., 2019). It decomposes the delayed and sparse reward to a
surrogate per-step reward via regression. Additionally, it utilizes on-policy correction to ensure
the same optimality w.r.t the original problem. We alter the code for the locomotion tasks.

-  SAC-IRCR (Gangwani et al., 2020). It utilizes off-policy data to provide a smoothed guidance
rewards for SAC. As mentioned in the paper, the performance heavily relies on the smoothing
policy. We implement a delayed reward version of IRCR in our setting.
For clarity, we only include Q-HC-Pairwise-1 and Q-HC-RNN in the comparison. As shown in
Figure 3, we find that LIRPG and RUDDER perform sluggishly in all tasks, due to their on-policy
nature (i.e., based on PPO (Schulman et al., 2017)). SAC-IRCR performs well only on some easy
tasks (e.g. Hopper-v2). Unfortunately, SAC-IRCR has a bad performance (e.g. Ant-v2, Walker2dv2, Reacher-v2) on other tasks. We suspect in these tasks, the smoothing technique results in a
erroneous guidance reward which biases the agent. Thus, SAC-IRCR is not a safe algorithm in solving delay reward tasks. In contrast, ours (as well as other implementations shown in Appendix C.3)
perform well on all tasks and surpass the baselines by a large margin. Most surprisingly, our algorithm can achieve the near optimal performance, i.e., comparing with Oracle SAC which is trained
on dense reward environment for 1M steps. Noticing that we use an environment in which rewards
are given only every 20 steps.


-----

Figure 3: Comparisons of Q-HC-Pairwise-1, Q-HC-RNN and baselines on 5 sum-form PI-DRMDP
tasks. The dashed line is the value of Oracle SAC. qn is fixed as 20 (Zheng et al., 2018; Oh et al.,
2018). The results show the mean and the standard deviation of 7 runs. All curves are further
smoothed equally for clarity. The last picture shows the relative performance w.r.t the Oracle SAC on
sum-form PI DRMDP tasks with general reward function (Appendix B). Each data point shows the
average of 5 tasks of each algorithm. X-axis refers to the overlapping steps of the reward function.

We also conduct experiments on tasks with general reward functions, in which our algorithmic
framework and the HC-decomposition can be naturally extended to (please refer to Appendix B and
Appendix A). In the General Experiment of Figure 3, we plot the relative average performance w.r.t
Oracle SAC on tasks whose general reward functions have different amount of overlapped steps (c =
5, 10). Clearly, our Q-HC is still the SOTA algorithm for every c. Thus, our method is applicable to
environments with general reward functions. Please refer to Appendix D for experiment details.

4.3 HISTORICAL INFORMATION

In addition, with a toy example, we explore what kind of information the H-component learns so
that the C-component makes suitable decisions. The toy example is a target-reaching task illustrated
in Figure 4 Left. The point agent is given delayed reward which roughly indicates its distance to
the target area. This mimics the low frequency feedback from the environment. Noticing that the
reward function is not in sum-form. Please refer to Appendix D for more details.

The learning curves are shown in Figure 4 Middle. Clearly, Q-HC-Singleton outperforms the baselines by a large margin. For better illustration, we visualize bφ(st, at) in -HC-Singleton on the
_Q_
grid in Figure 4 Right. The pattern highlights the line from the start point to the target area (i.e., the
optimal policy), suggesting that bφ has captured some meaningful patterns to boost training. We also
observe a similar pattern for HC-Singleton without regression (Appendix D). This is an interesting
discovery which shows that the H-component may also possess direct impact on the policy learning
instead of simply function as an approximator.

5 RELATED WORK

**Off-Policy RL: Off-policy deep reinforcement learning algorithms TD3 (Fujimoto et al., 2018)**
and SAC (Haarnoja et al., 2018a) are the most widely accepted actor-critic algorithms on the robot
locomotion benchmark (Duan et al., 2016) so far. Based on previous work (Degris et al., 2012; Silver
et al., 2014), Fujimoto et al. (2018) puts forward the clipped double Q-learning (CDQ) technique
to address overestimation in critic learning. Haarnoja et al. (2018a) also uses CDQ technique but
instead optimizes the maximum entropy objective. These two methods lead to a more robust policy.


-----

Figure 4: Left: 100 × 100 Point-Reach task with additional positional reward (indicated by area
color). Reward is given every 20 steps. **Middle: Learning curves of Q-HC-Singleton and several**
baselines. Dashed line represents Q-HC-Singleton without regularization. Y-axis shows the number
of steps needed for the point agent to reach the target area (cut after 500 steps). All curves represent
the mean of 10 seeds and are smoothed equally for clarity. Right: Heatmap visualization of bφ in
_Q-HC-Singleton._
In our setting, we observe that SAC-based algorithms slightly outperforms TD3-based algorithms
suggesting the benefit of maximum entropy in delayed reward tasks.

**Delayed or Episodic Reward RL: Developing RL algorithms for delayed reward or episodic reward**
(at the end of the trajectory) has become a popular research area recently. Some theoretical analysis
have been conducted on delayed rewards in MAB and MDP Zhou et al. (2018; 2019); H´eliou et al.
(2020) These tasks are known to be difficult for long-horizon credit assignment (Sutton, 1984). To
address this issue, Zheng et al. (2018) proposes to use intrinsic reward to boost the efficiency of
policy gradients. The intrinsic reward is learnt via meta-learning (Finn et al., 2020). Gangwani et al.
(2019) and Guo et al. (2018) use a discriminator to provide guidance reward for the policy. The
discriminator is trained jointly with the policy with binary classification loss for self-imitating. In
RUDDER (Arjona-Medina et al., 2019), it utilizes a recurrent network to predict a surrogate per-step
reward for guidance. To ensure the same optimality, the guidance is then corrected with ground truth
reward. Liu et al. (2019) extends the design and utilizes a Transformer (Vaswani et al., 2017) network
for better credit assignment on episodic reward tasks. Recently, Klissarov & Precup (2020) proposes
to use GCN (Kipf & Welling, 2016) network to learn a potential function for reward shaping (Ng
et al., 1999). Noticing that all these algorithms are based on PPO (Schulman et al., 2017) and thus
are on-policy algorithms while ours is an off-policy one.

Most recently, Gangwani et al. (2020) proposes to augment off-policy algorithms with a trajectoryspace smoothed reward in episodic reward setting. The design turns out to be effective in solving
the problem. However, as mentioned by the paper itself, this technique lacks theoretical guarantee
and heavily relies the choice of the smoothing policy. As shown in Section 4.2, in many cases, this
method becomes extremely spurious. Our algorithm is derived from a theoretical perspective and
performs well on all tasks.

6 CONCLUSION

In this paper, we model the sequential decision problem with delayed rewards as Past-Invariant
Delayed Reward MDPs. As previous off-policy RL algorithms suffer from multiple problems in PIDRMDP, we put forward a novel and general algorithmic framework to solve the PI-DRMDP problems that has theoretical guarantees in the tabular case. The framework relies on a novelly defined
_Q-value. However, in high dimensional tasks, it is hard to approximate the Q-value directly. To ad-_
dress this issue, we propose to use the HC-approximation framework for stable and efficient training
in practice. In the experiment, we compare different implementations of the HC framework. They
all perform well and robustly in continuous control PI-DRMDP locomotion tasks based on OpenAI
Gym. Besides, our method outperforms previous baselines on delayed reward tasks remarkably,
suggesting that our algorithm is a SOTA algorithm on these tasks so far.

In terms of future work, two research directions are worth exploring. One is to develop our algorithm
and the HC-approximation scheme to various real world settings mentioned in Section 1, possibly
with offline data (Levine et al., 2020). The other direction is to design efficient and advanced algorithms with theoretical guarantees for the general DRMDP tasks.


-----

REFERENCES

Mostafa Al-Emran. Hierarchical reinforcement learning: a survey. International journal of comput_ing and digital systems, 4(02), 2015._

Jose A. Arjona-Medina, Michael Gillhofer, Michael Widrich, Thomas Unterthiner, Johannes Brandstetter, and Sepp Hochreiter. Rudder: Return decomposition for delayed rewards. In Advances in
_Neural Information Processing Systems, volume 32, pp. 13566–13577. Curran Associates, Inc.,_
2019.

Kyunghyun Cho, Bart van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder–decoder
for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods
_in Natural Language Processing (EMNLP), pp. 1724–1734, Doha, Qatar, October 2014. Associ-_
ation for Computational Linguistics. doi: 10.3115/v1/D14-1179.

Thomas Degris, Martha White, and Richard S Sutton. Off-policy actor-critic. In Proceedings of the
_29th International Coference on International Conference on Machine Learning, pp. 179–186,_
2012.

Esther Derman, Gal Dalal, and Shie Mannor. Acting in delayed environments with non-stationary
markov policies. arXiv preprint arXiv:2101.11992, 2021.

Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford,
[John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. https:](https://github.com/openai/baselines)
[//github.com/openai/baselines, 2017.](https://github.com/openai/baselines)

Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeal. Benchmarking deep
reinforcement learning for continuous control. In International Conference on Machine Learning,
2016.

Chelsea Finn, Pieter Abbeal, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In 34th Conference on Neural Information Processing Systems, 2020.

Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actorcritic methods. In International Conference on Machine Learning, pp. 1587–1596, 2018.

Tanmay Gangwani, Qiang Liu, and Jian Peng. Learning self-imitating diverse policies. In Interna_tional Conference on Learning Representations, 2019._

Tanmay Gangwani, Yuan Zhou, and Jian Peng. Learning guidance rewards with trajectory-space
smoothing. In 34th Conference on Neural Information Processing Systems, 2020.

Yaobang Gong, Mohamed Abdel-Aty, Qing Cai, and Md Sharikur Rahman. Decentralized network
level adaptive signal control by multi-agent deep reinforcement learning. Transportation Research
_Interdisciplinary Perspectives, 1:100020, 2019._

Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT Press, 2016.

Brockman Greg, Cheung Vicki, Pettersson Ludwig, Schneider Jonas, Schulman John, Tang Jie, and
Zaremba Wojciech. Openai gym, 2016.

Yijie Guo, Junhyuk Oh, Satinder Singh, and Honglak Lee. Generative adversarial self-imitation
learning. arXiv preprint arXiv:1812.00950, 2018.

Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th
_International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning_
_Research, pp. 1861–1870, Stockholmsm¨assan, Stockholm Sweden, 10–15 Jul 2018a. PMLR._

Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018b.


-----

D. Hein, S. Depeweg, M. Tokic, S. Udluft, A. Hentschel, T. A. Runkler, and V. Sterzing. A benchmark environment motivated by industrial control problems. In 2017 IEEE Symposium Series on
_Computational Intelligence (SSCI), pp. 1–8, 2017. doi: 10.1109/SSCI.2017.8280935._

Am´elie H´eliou, Panayotis Mertikopoulos, and Zhengyuan Zhou. Gradient-free online learning in
continuous games with delayed rewards. In International Conference on Machine Learning, pp.
4172–4181. PMLR, 2020.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Computer Science,
2014.

Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. CoRR, abs/1609.02907, 2016.

Martin Klissarov and Doina Precup. Reward propagation using graph convolutional networks martin. In 34th Conference on Neural Information Processing Systems, 2020.

Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.

Kaixiang Lin, Renyu Zhao, Zhe Xu, and Jiayu Zhou. Efficient large-scale fleet management via
multi-agent deep reinforcement learning. In the 24th ACM SIGKDD International Conference,
2018.

Yang Liu, Yunan Luo, Yuanyi Zhong, Xi Chen, Qiang Liu, and Jian Peng. Sequence modeling of
temporal credit assignment for episodic reinforcement learning. arXiv preprint arXiv:1905.13420,
2019.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.

A. Ng, D. Harada, and S. Russell. Policy invariance under reward transformations: Theory and
application to reward shaping. In International Conference on Machine Learning, 1999.

Junhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee. Self-imitation learning. In Jennifer
Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine
_Learning, volume 80 of Proceedings of Machine Learning Research, pp. 3878–3887, Stock-_
holmsm¨assan, Stockholm Sweden, 10–15 Jul 2018. PMLR.

Marcus Olivecrona, Thomas Blaschke, Ola Engkvist, and Hongming Chen. Molecular de-novo
design through deep reinforcement learning. Journal of cheminformatics, 9(1):1–14, 2017.

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In Proceedings of the 31st International Conference on
_International Conference on Machine Learning-Volume 32, pp. I–387, 2014._

David Silver, Julian Schrittwieser, Karen Simonyan, Aj Antonoglou, Ioannis abd Huang, Arthur
Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel, and Demis Hassabis.
Mastering the game of go without human knowledge. Nature, 550, 2017.

Richard S Sutton. Between mdps and semi-mdps: Learning, planning, and representing knowledge
at multiple temporal scales. 1998.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.

Richard Stuart Sutton. Temporal credit assignment in reinforcement learning. PhD thesis, Department of Computer Science, University of Massachusetts at Amherst, 1984.


-----

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, N,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In 31th Conference on Neural
_Information Processing Systems, pp. 5998–6008, 2017._

Thomas J Walsh, Ali Nouri, Lihong Li, and Michael L Littman. Learning and planning in environments with delayed feedback. Autonomous Agents and Multi-Agent Systems, 18(1):83–105,
2009.

Zhe Xu, Zhixin Li, Qingwen Guan, Dingshui Zhang, and Jieping Ye. Large-scale order dispatch in
on-demand ride-hailing platforms: A learning and planning approach. In the 24th ACM SIGKDD
_International Conference, 2018._

Zeyu Zheng, Junhyuk Oh, and Satinder Singh. On learning intrinsic rewards for policy gradient
methods. In Advances in Neural Information Processing Systems, volume 31, pp. 4644–4654.
Curran Associates, Inc., 2018.

Zhengyuan Zhou, Panayotis Mertikopoulos, Nicholas Bambos, Peter Glynn, Yinyu Ye, Li-Jia Li,
and Li Fei-Fei. Distributed asynchronous optimization with unbounded delays: How slow can
you go? In International Conference on Machine Learning, pp. 5970–5979. PMLR, 2018.

Zhengyuan Zhou, Renyuan Xu, and Jose Blanchet. Learning in generalized linear contextual bandits
with stochastic delays. Advances in Neural Information Processing Systems, 32:5197–5208, 2019.


-----

A ALGORITHM

We summarize our algorithm for PI-DRMDP with General Reward Function as follows.

**Algorithm 1 Algorithm with General Reward Function**

1: Choose Qφ structure: Q-RNN, Q-HC-Pairwise-K, e.t.c
2: Initialize πθ, Qφ and target networks, D ←∅
3: for each env step do
4: _at_ _πθ(_ _st, t_ _ti)_

5: Take ∼ at and get·| _− st+1, Rt_

6: Add (st, at, Rt, st+1) to D

7: **for each gradient step do**

8: Sample a batch of (τti−c:t+1, Rt, st+1) from D

9: Update _φ via minimizing Eq. (8) (with τti_ _c:t) for_ -HC or Eq. (11) for -RNN
_Q_ _−_ _Q_ _Q_

10: Update πθ via Eq. (7) for Q-HC or Eq. (12) for Q-RNN

11: Update the target networks

12: **end for**

13: end for


B DISCUSSIONS AND FORMAL PROOFS

In this part, we provide the proofs for the statements in the main paper. In addition, we also add
detailed discussions on the issues mentioned above. To begin with, we restate the definitions with
general reward functions.

**Definition 3 (DRMDP with General Reward Function, DRMDP-c). A DRMDP-c Mc** =
(S, A, p, qn, rc, γ) is described by the following parameters.

_1. The state and action spaces are S and A respectively._
_2. The Markov transition function is p(s[′]|s, a) for each (s, a) ∈_ _S ×_ _A; the initial state distribution_
_is p(s0)._

_3. The signal interval length is distributed according to qn(_ ), i.e., for the i-th signal interval, its

_·_
_length ni is independently drawn from qn(·)._

_4. The general reward function rc defines the expected reward generated for each signal inter-_
_val with a overlap of maximal c steps with previous signal intervals; suppose τi = τt:t+ni =_
(s, a)t:t+ni = ((st, at), . . . (st+n 1, at+ni 1)) is the state-action sequence during the i-th sig_−_ _−_
_nal interval of length ni, then the expected reward for this interval is rc(τt−c:t+ni_ ). [3]

_5. The reward discount factor is γ._

The PI-DRMDP is also extended naturally as follows.

**Definition 4 (PI-DRMDP with General Reward Function, PI-DRMDP-c). A Past-Invariant**
_DRMDP-c is a DRMDP-c Mc = (S, A, p, qn, rc, γ) whose reward function rc satisfies the fol-_
_lowing Past-Invariant (PI) condition: for any two trajectory segments τ1 and τ2 of the same length_
_(no less than c), and for any two equal-length trajectory segments τ1[′]_ _[and][ τ][ ′]2_ _[such that the concate-]_
_thatnated trajectories τa ◦_ _τb[′]_ _[are feasible under the transition dynamics][ p][ for all][ a, b][ ∈{][1][,][ 2][}][, it holds]_

_r(τ1 ◦_ _τ1[′]_ [)][ > r][(][τ][1] _[◦]_ _[τ][ ′]2[)][ ⇐]⇒_ _r(τ2 ◦_ _τ1[′]_ [)][ > r][(][τ][2] _[◦]_ _[τ][ ′]2[)][.]_

**Remark 1. Definition 1 and Definition 2 are the special case (c = 0) of Definition 3 and Definition 4**
_respectively._

Clearly, we have the following Fact.

**Fact 3.** _c[′]_ _< c, DRMDP Mc′ is also a DRMDP Mc and PI-DRMDP Mc′ is also a PI-DRMDP Mc._
_∀_

3If t < c, τt _c:0 refers to some zero token paddings which indicates the beginning of the trajectory._
_−_


-----

Besides, under the general reward function, we extend the Q definition.


(τti _c:t+1) := E(τ,n)_ _π_ _γ[t][j][+1][−][t][−][1]r(τtj_ _c:tj+1_ ) _τti_ _c:t+1_ _._ (10)
_Q[π]_ _−_ _∼_  _−_ _−_ 

_j=i_

X
 

Correspondingly, Q[π](τti−c:t+1) is optimized via

_φ := ED_ (Rt + γ [ˆ]φ(τtj _c:t+2)_ _φ(τti_ _c:t+1))[2][i]_ _,_ (11)
_L_ _Q_ _−_ _−Q_ _−_

where τtj _c:t+2 = τti_ _c:t+1_ h(st+1, a[′]t+1[)][ (so that][ j][ =][ i][) if][ t][ is not the last step of][ τ][i][, and]
_τtj_ _c:t+2 =−_ _τti+1_ _c:ti+1−_ (sti ◦+1 _, a[′]ti+1_ [)][ (so that][ j][ =][ i][ + 1][) otherwise.]
_−_ _−_
_◦_

The general policy update is also extended to


_∇θJ (πθ) = ED_


_∇at_ _Q[π](τti−c:t ◦_ (st, at)) _πθ(st,t−ti)[∇][θ][π][θ][(][s][t][, t][ −]_ _[t][i][)]_ _._ (12)


**Without specification, the following proofs hold for ∀c.**

B.1 PROOF OF FACT 1

**Fact 4 (Fact 1 with General Reward Function). For any distribution D with non-zero measure**
_for any τti−c:t+1, Q[π](τti−c:t+1) is the unique fixed point of the MSE problem in Eq. (4). More_
_specifically, when fixing_ _Q[ˆ]φ as the corresponding Q[π], the solution of the MSE problem is still Q[π]._

_Proof. Though we state the fact for PI-DRMDP-c, we will prove it for general π ∈_ Πτ,c in DRMDPc setting. For brevity, we replace t + 1 with t in the proof. By definition in Eq. (10), we have
_Q[π](τti−c:t) = qn(ni ̸= t −_ _ti|ni ≥_ _t −_ _ti)_

_γ_ _st,at_ _p(st|st−1, at−1)π(at|τti−c:t ◦_ _st)Q[π](τti−c:t ◦_ (st, at))!
X

+ qn(ni = t − _ti|ni ≥_ _t −_ _ti)_


_st,at_ _p(st|st−1, at−1)π(at|τt−c:t ◦_ _st)Q[π](τt−c:t ◦_ (st, at))

X


_r(τti−c:t) + γ_


(13)


Clearly, (τti _c:t) is the solution of the following MSE problem. Noticing that we assume_ _φ has_
_Q[π]_ _−_ _Q_
tabular expression over τti−c:ts.

2

minφ _RtX−1,st_ _pD(τti−c:t, Rt−1, st)π(a[′]t[|][τ][t]j_ _[−][c][:][t]_ _[◦]_ _[s][t][)]_  Rt−1 + γQ[π](τtj _−c:t ◦_ (st, a[′]t[))][ −Q][φ][(][τ][t]i[−][c][:][t][)]

Here, τtj _c:t is defined similarly as in the main text and pD refers to the probability of the fragment_
_−_
of trajectory is sampled from the buffer D. We denote the objective function as Lφ(τti−c:t). As Lφ
in Eq. (11) is the sum of all Lφ(τti−c:t), Q[π](τti−c:t) is a fixed point of the MSE problem.

The uniqueness is proved similar to the standard MDPs setting. For any [ˆ]φ, we denote the optimal
_Q_
solution of Eq. (4) as Qφ(τti−c:t). Since we assume tabular expression and non-zero probability of
any pD(τti−c:t) Then, ∀τti−c:t, we have
_Qφ(τti−c:t) −Q[π](τti−c:t)_ _≤_ _γqn(ni ̸= t −_ _ti|ni ≥_ _t −_ _ti)_ _st,at_ _p(st|st−1, at−1)π(at|τti−c:t ◦_ _st)_
X

[ˆ]φ(τti _c:t+1)_ (τti _c:t+1)_ + γqn(ni = t _ti_ _ni_ _t_ _ti)_
_Q_ _−_ _−Q[π]_ _−_ _−_ _|_ _≥_ _−_

_st,at_ _p(st|st−1, at−1)π(at|τt−c:t ◦_ _st)_

X

_Q[ˆ]φ(τt−c:t ◦_ (st, at)) −Q[π](τt−c:t ◦ (st, at))

_≤_ _γ ∥_ _Q[ˆ]φ −Q[π]_ _∥∞_


-----

Last term denotes the infinite norm of the vector of residual value between and [ˆ]φ. Each entry
_Q[π]_ _Q_
corresponds to some τti _c:t feasible in the dynamic. Consequently, by the γ-concentration property,_
_−_
if Qφ = Q[ˆ]φ after the iteration (i.e., Qφ is a fixed point), then Qφ = Q[π]. This completes the proof
of uniqueness.

**Remark 2. Fact 1 holds as the special case of Fact 4 when c = 0.**

B.2 PROOF OF FACT 2

**Fact 5 (Fact 2 with General Reward Function). For any τti−c:t, τt[′]i−c:t** _[and][ s][t][, a][t][, a]t[′]_ _[which both]_
_τti−c:t ◦_ _st and τt[′]i−c:t_ _[◦]_ _[s][t]_ _[are feasible under the transition dynamics,][ ∀][π][ ∈]_ [Π][s][, we have that]

_Q[π](τti−c:t ◦_ (st, at)) > Q[π](τti−c:t ◦ (st, a[′]t[))][ ⇐]⇒Q[π](τt[′]i−c:t _[◦]_ [(][s][t][, a][t][))][ >][ Q][π][(][τ][ ′]ti−c:t _[◦]_ [(][s][t][, a]t[′] [))]

_Proof. Since π(at_ _st, t_ _ti)_ Πs, for any st, at, the distribution of the remaining trajectories
_|_ _−_ _∈_
_ρ[π](τt:T, nt:T_ _st, at, ti) is irrelevant with τti_ _c:t . Thus, we have_
_|_ _−_


_Q[π](τti−c:t ◦_ (st, at)) =


_ρ[π](τt:∞, nt:∞|st, at, ti)_
_τt:∞X,nt:∞_


_γ[t][j][+1][−][t][−][1]r(τtj_ _−c:tj+1_ )
_j=i_

X


_ti+1_ _γ[t][i][+1][−][t][−][1]qn(ni = ti+1 −_ _ti|ni ≥_ _t −_ _ti)_

X


_ρ[π](τt:ti+1_ _sti+1_ _st, at, ti)_
_τt:ti+1_ _◦_ _|_

X



_·_ r(τti−c:ti+1 ) + γ _ati+1_ _π(ati+1_ _|sti+1_ _, 0)Q[π](τti+1−c:ti+1 ◦_ (sti+1 _, ati+1_ ))

X
 

_qn(ni = ti+1_ _ti_ _ni_ _t_ _ti)ρ[π](τt:ti+1_ _st, at, ti)r(τti_ _c:ti+1_ )

_∝_ _ti+1,τt:ti+1_ _−_ _|_ _≥_ _−_ _|_ _−_
X

+ K(st, at, ti)

The last term denotes the residual part which is irrelevant with τti:t. Besides, under the PI condition,
the order ofamong a is also invariant of r(τti−c:ti+1 ) is invariant over τti−c:t. This completes the proof. τti−c:t. As a result, the order of Q[π](τti−c:t ◦ (st, a))

**Remark 3. Fact 2 holds as then special case of Fact 5 when c = 0.**

B.3 PROOF OF PROPOSITION 1

**Proposition 2iteration w.r.t.** (Proposition 1 with General Reward Function)(Eq. (10)) produces policy πk+1 Πs such that. For any policyτti _c:t+1 π, it holds thatk ∈_ Πs, the policy
_Q[π][k]_ _∈_ _∀_ _−_

_Q[π][k][+1]_ (τti−c:t+1) ≥Q[π][k] (τti−c:t+1),

_which implies that_ (πk+1) (πk).
_J_ _≥J_

_Proof. Under Fact 5, the new policy iteration is defined formally as following_

_πk+1(at|st, t −_ _ti) = arg maxat_ _Q[π][k]_ (τt−c:t ◦ (st, at))

where τt _c:t is any trajectory segments feasible for st under the dynamics. If there are multiple_
_−_
maximums, we can assign arbitrary probability among these ats. Following Eq. (13), we have


-----

_∀τti−c:t_

_Q[π][k]_ (τti−c:t) = qn(ni ̸= t − _ti|ni ≥_ _t −_ _ti)_

_γ_ _st,at_ _p(st|st−1, at−1)πk(at|st, t −_ _ti)Q[π][k]_ (τti−c:t ◦ (st, at))
X

_qn(ni = t −_ _ti|ni ≥_ _t −_ _ti)_


_st,at_ _p(st|st−1, at−1)πk(at|st, 0)Q[π][k]_ (τt−c:t ◦ (st, at))

X


_r(τti−c:t) + γ_


_qn(ni_ = t _ti_ _ni_ _t_ _ti)_
_≤_ _̸_ _−_ _|_ _≥_ _−_

_γ_ _st,at_ _p(st|st−1, at−1)πk+1(at|st, t −_ _ti)Q[π][k]_ (τti−c:t ◦ (st, at))
X

_qn(ni = t −_ _ti|ni ≥_ _t −_ _ti)_


_st,at_ _p(st|st−1, at−1)πk+1(at|st, 0)Q[π][k]_ (τt−c:t ◦ (st, at))

X


(14)


_r(τti−c:t) + γ_


Then, we can iteratively extend each Q[π][k] term in Eq. (14) and get ∀τti−c:t

_Q[π][k]_ (τti−c:t) ≤Q[π][k][+1] (τti−c:t)

Noticing that the inequality is strict if any step of the inequality in the iterations is strict. Furthermore, by definition, J (π) = _s0,a0_ _[p][(][s][0][)][π][(][a][0][|][s][0][,][ 0)][Q][π][(][s][0][, a][0][)][4][, we have]_

(πk) _p(s0)πk+1(a0_ _s0, 0)_ (s0, a0)
_J_ [P] ≤ _|_ _Q[π][k]_

_s0,a0_

X


_p(s0)πk+1(a0_ _s0, 0)_ (s0, a0)

_≤_ _|_ _Q[π][k][+1]_

_s0,a0_

X

(πk+1)
_≤J_


This completes the proof.


**Remark 4. Proposition 1 holds as the special case of Proposition 2 when c = 0.**

B.4 DISCUSSION OF OPTIMAL POLICIES

To begin with, we consider the general DRMDP-c. Similarly, we denote Πτ,c = _π(at_ _τti_ _c:t_ _st)_
as the extension of Πτ (c = 0). Then, we prove the following Fact. _{_ _|_ _−_ _◦_ _}_

**Fact 6. For any DRMDP-c, there exists an optimal policy π[∗]** _∈_ Πτ,c. However, there exists some
_DRMDP-0 (so as DRMDP-c_ _c) such that all of its optimal policies π[∗]_ _/_ Πs.
_∀_ _∈_

policy can only base on all the information it has experienced till stepIn general, any policy belongs to the class Πall = {π(at|τ0:t ◦ _st, n t. Besides, in DRMDP-c, the0:t)}. Namely, the agent’s_
expected discounted cumulative reward is


_ρ[π](τ, n)R(τ, n)._

_τ,n_

X


_J (π) =_


where ρ[π](τ, n) denotes the probability that trajectory (τ, n) is generated under policy π ∈ Πall. To
begin with, we first prove the following Lemma.

**Lemma 1. For any policy π ∈** Πall which is the optimal, i.e, maximize J (π), we consider two
_segments of trajectories (τ0:[a]_ _t[a]_ _[, n][a]0:t[a]_ [)][ and][ (][τ][ b]0:t[b] _[, n][b]0:t[b]_ [)][ which satisfies that][ τ][ a]t[a]i _t[b]i_ _[−][c][:][t][b][ and]_

_[−][c][:][t][a][ =][ τ][ b]_

_t[b]_ _t[a]. Besides, we also consider_ _t_ _t[b], t[′]_ = t _t[b]_ +t[a] _and τ0:[b]_ _t_ [=][ τ][ b]0:t[b][ ◦][τ][t][b][:][t][, n]0:[b] _t_ [=][ n][b]0:t[b][ ◦][n][t][b][:][t]
_≥_ _∀_ _≥_ _−_

4Here, we omit specifying the c-step padding.


-----

_and state s[(]t[′][)]_ _feasible in the dynamic, we switch the policy π to π[′]_ _w.r.t these two trajectories as_
_following_

_π[′](·|τ0:[b]_ _t_ _[◦]_ _[s][t][, n]0:[b]_ _t[) =][ π][(][·|][τ][ a]0:t[a][ ◦]_ _[τ]t[b]:t_ _[◦]_ _[s][t][′]_ _[, n]0:[a]_ _t[a][ ◦]_ _[n]t[b]:t[)][.]_

_Then, π[′]_ _∈_ Πall and J (π[′]) = J (π).


_Proof. Obviously, π[′]_ is well-defined and π[′] _∈_ Πall as the transition is Markovian. Noticing that

_J (π[′]) =_ _ρ[π][′]_ (τ, n)R(τ, n)

_τ,n_

X

= _ρ[π][′]_ (τ, n)R(τ, n) +ρ[π][′] (τ0:[b] _t[b]_ _[, n][b]0:t[b]_ [)]

(τ,n):(τ _[b]_
0:t[b]X[,n][b]0:t[b] [)][̸⊂][(][τ,n][)]

denoted by J _[π][′]_ (τ _[b]_
0:t[b] _[,n][b]0:t[b]_ [)]
| {z _ρ[π][′]_ (τ, n|τ0:[b] _t[b]_ _[, n][b]0:}t[b]_ [)][R][(][τ, n][)]

(τ,n):(τ _[b]_
0:t[b]X[,n][b]0:t[b] [)][⊂][(][τ,n][)]


= J _[π][′]_ (τ0:[b] _t[b]_ _[, n]0:[b]_ _t[b]_ [) +][ ρ][π][′] [(][τ][ b]0:t[b] _[, n]0:[b]_ _t[b]_ [)]


(τ,n):(τ _[b]_
0:t[b] _[,n][b]0:t[b]_ [)][⊂][(][τ,n][)]

_∞_

_γ[t][j][+1][−][1]r(τtj_ _−c:tj+1_ )
_j=1_

X




_ρ[π][′]_ (τ, n|τ0:[b] _t[b]_ _[, n]0:[b]_ _t[b]_ [)]


_i(t[b])−1_

_γ[t][j][+1][−][1]r(τtj_ _−c:tj+1_ )
_j=1_

X


= J _[π][′]_ (τ0:[b] _t[b]_ _[, n]0:[b]_ _t[b]_ [) +][ ρ][π][′] [(][τ][ b]0:t[b] _[, n]0:[b]_ _t[b]_ [)]


+ ρ[π][′] (τ0:[b] _t[b]_ _[, n]0:[b]_ _t[b]_ [)] _γ[t][b][−][1]Q[π][′]_ (τt[b][b]i _[−][c][:][t][b]_ [)] _._
 

We denote (τ0:t, n0:t) (τ, n) if the former is the trajectory prefix of the latter. is defined in
_⊂_ _Q[π][′]_
Eq. (3) except that π[′] _∈_ Πall in this case. By the definition of π[′] and the condition that π is optimal,
we have Q[π][′] (τt[b][b]i _[−][c][:][t][b]_ [) =][ Q][π][(][τ][ a]t[a]i _[−][c][:][t][a]_ [) =][ Q][π][(][τ][ b]t[b]i _[−][c][:][t][b]_ [)][ and][ J] _[π][′]_ [(][τ][ b]0:t[b] _[, n][b]0:t[b]_ [) =][ J] _[π][(][τ][ b]0:t[b]_ _[, n][b]0:t[b]_ [)][.]
Thus, we have


_i(t[b])−1_

_γ[t][j][+1][−][1]r(τtj_ _−c:tj+1_ )
_j=1_

X


_J (π[′]) = J_ _[π][′]_ (τ0:[b] _t[b]_ _[, n]0:[b]_ _t[b]_ [) +][ ρ][π][′] [(][τ][ b]0:t[b] _[, n]0:[b]_ _t[b]_ [)]


+ ρ[π][′] (τ0:[b] _t[b]_ _[, n]0:[b]_ _t[b]_ [)] _γ[t][b][−][1]Q[π](τt[b][b]i_ _[−][c][:][t][b]_ [)]



= J _[π](τ0:[b]_ _t[b]_ _[, n][b]0:t[b]_ [) +][ ρ][π][(][τ][ b]0:t[b] _[, n][b]0:t[b]_ [)]

_∞_

 _γ[t][j][+1][−][1]r(τtj_ _−c:tj+1_ )

_j=1_

X

= J (π). 


_ρ[π](τ, n|τ0:[b]_ _t[b]_ _[, n][b]0:t[b]_ [)]
(τ,n):(τ _[b]_
0:t[b]X[,n][b]0:t[b] [)][⊂][(][τ,n][)]


We refer it as a policy switch on π w.r.t (τ0:[b] _t[b]_ _[, n][b]0:t[b]_ [)][ and][ (][τ][ a]0:t[a] _[, n][a]0:t[a]_ [)][ to][ π][′][. Then, we prove Fact 6]
as follows.

_Proof of Fact 6.∀ta, tb ≤_ _t then For simplicity, we denote ∀(τ0:[a]_ _t[a]_ _[, n][a]0:t[a]_ [)][,][ (][τ][ b]0:t[b] _[, n][b]0:t Π[b]_ [)][ which][t]τ,c _[⊂]_ _[ τ][Π][ a]t[all][a]i_ _[−][as the policy space that if][c][:][t][a][ =][ τ][ b]t[b]i_ _[−][c][:][t][b]_ [,][ π][(][·|][τ][ a]0:t[a][ ◦][ π][s][ ∈][t][a] _[, n][Π][a]0:τ,c[t]_ _t[a]_ [) =][and]

_π(·|τ0:[b]_ _t[b][ ◦]_ _[s][t][b]_ _[, n][b]0:t[b]_ [)][ for all][ s][ feasible. Clearly,][ Π]τ,c[0] [= Π][all] [and][ Π]τ,c[∞] [= Π][τ,c][. Then, starting from]
some optimal policy π0 ∈ Π[0]τ,c[, we utilize the policy switch operation to shift it into][ Π][τ] [.]


-----

Suppose that πt−1 ∈ Πτ,c[t][−][1][, we consider the following two steps of operations.]

1. Step 1. ∀(τ0:t, n0:t) that ∃(τ0:[′] _t[′]_ _[, n][′]t[′]i[:][t][′]_ [)][ which][ t][′][ < t][ and][ τ][t][i][−][c][:][t][ =][ τ][ ′]t[′]i[−][c][:][t][′][ (randomly]
choose one if multiple segments exist), we conduct a policy switch on πt w.r.t (τ0:t, n0:t)
and (τ0:t′ _, n0:t′_ ). We denote the policy after all these policy switches as πt[′] 1[.]
_−_

2. Step 2. We denote Sπt−1 (˜τti−c:t) = {(τ0:t, n0:t)|τti−c:t = ˜τti−c:t and ∄(τ0:[′] _t[′]_ _[, n][′]0:t[′]_ [)][, t][′][ <]
_t which τt[′][′]i[−][c][:][t][′][ = ˜]τti−c:t}. For each Sπt−1_ (˜τti−c:t), we randomly choose some (τ0:[0] _t[, n][0]0:t[)][ ∈]_
_Sπt−1_ (˜τti−c:t) and conduct policy switch on πt[′]−1 [w.r.t all][ (][τ, n][)][ ∈] _[S][π]t−1_ [(˜]τti−c:t) and
(τ0:[0] _t[, n]0:[0]_ _t[)][. Finally, we get][ π][t][.]_
policy switches,Since πt−1 ∈ Π π[t]τ,c[−]t[1] is optimal if[, it is straightforward to check that] πt 1 is optimal. Consequently, by induction from the optimal[ π][t] _[∈]_ [Π]τ,c[t] [. Besides, as all operations are]
_−_
policy π0, we can prove that _π_ Πτ,c which is optimal.
_∃_ _∞_ _∈_

For the second statement, we consider the following DRMDP-0.

_A0_ _a0_

_b0, b1_

_B_ _C_

_A1_ _a1_


whose n = 2. a0, a1 denote two actions from A0, A1 to B respectively and b0, b1 are two actions
from B to C. The reward is defined as r(ai, bj) = i _j,_ (i, j) 0, 1 . The initial state distribution
_⊕_ _∀_ _∈{_ _}_
is p(A0) = p(A1) = 0.5. Clearly, for any optimal policy, the agent has to query for its action from
the initial state to B before deciding the action from B to C. This illustrates that π[∗] may not be in
Πs.

Consequently, learning optimal policies is relatively hard in DRMDP due to the large policy class
to search. In our work on PI-DRMDP, we only focus on the optimization in Πs. However, in some
bizarre cases, the optimal policy π[∗] Πs. For example, consider the following PI-DRMDP-0 where
_̸∈_
_n if fixed as 2._


_Ab_ _C+_

_a10_ _b+1_

_B_

_Al_ _a0.1_ _b−1_ _C_ _T_

_−_ _τ_


The initial state is uniformly chosen from _Ab, Al_ and C+, T are terminal states. The reward
_{_ _}_
function for the first interval is r(ai, bj) = i _j (Past-Invariant) and r(τ_ ) = 5. The optimal policy
_×_
at B should be π( _Ab/l, B) = b+1/_ 1 which is not in Πs.

_·|_ _−_

We have not covered the potential non-optimal issue in this work and leave developing general
algorithms for acquiring optimal policy for any PI-DRMDP-c or even DRMDP-c as future work. As
mentioned above, most real world reward functions still satisfy that the optimal policy belongs to
Πs. This makes our discussion in Πs still practically meaningful. Additionally, we put forward a
sub-class of PI-DRMDP-c which guarantees that there exists some optimal policy π[∗] Πs.
_∈_

**Definition 5 (Strong PI-DRMDP-c). A Strong Past-Invariant DRMDP-c is a PI-DRMDP-c whose**
_reward r satisfies a stronger condition than the Past-Invariant (PI) condition:_
_∀_ _trajectory segments τ1 and τ2 of the same length (no less than c), and for any two equal-length_
_transition dynamicstrajectory segments τ p1[′] for all[and][ τ] a, b[ ′]2_ _[such that the concatenated trajectories] ∈{1, 2}, it holds that_ _[ τ][a]_ _[◦]_ _[τ][ ′]b_ _[are feasible under the]_

_r(τ1 ◦_ _τ1[′]_ [)][ −] _[r][(][τ][1]_ _[◦]_ _[τ][ ′]2[) =][ r][(][τ][2]_ _[◦]_ _[τ][ ′]1[)][ −]_ _[r][(][τ][2]_ _[◦]_ _[τ][ ′]2[)][.]_


-----

Namely, the discrepancies are invariant of the past history τ1,2 in addition to only the order. A
straightforward example is the weighted linear sum of per-step rewards

_t+n−1_

_r(τt−c:t+n) =_ _i=Xt−c_ _wi−tr(si, ai),_ 0 ≤ _wi−t ≤_ 1

The weight mimics the scenario that the per-step reward for the i-t[th] step in the reward interval will
be lost with some probability wi−t. Furthermore, Strong PI-DRMDP-c satisfies

**Fact 7. For any Strong PI-DRMDP-c, there exists some optimal policy π[∗]** _∈_ Πs.

**Remark 5. Table 1 summarizes the case when c = 0.**

B.5 DISCUSSION OF OFF-POLICY BIAS

This following example illustrates the off-policy bias concretely. We consider the case for a sumform PI-DRMDP-0.

The signal interval length is fixed as n + 1 and any trajectory ends after one interval. The reward
function is in sum-form over r(s, a). When learning the critic (tabular expression) via Eq. (1), the
_Q-values at the last step n are_ [5]

_Qφ(sn, an) =_ _ρ[β](τ0:n_ _sn, an)r(τ1)_

_|_
_τ0:n_

X

_n−1_

= _ρ[β](τ0:n_ _sn, an)_ _r(τt:t+1)_ +r(sn, an) (15)

_τ0:n_ _|_ _t=0_ !

X X

_Q[β](sn,an)_

In Eq. (15), ρ[β](τ0:n|sn, an|) denotes the probability distribution off {z } _τ0:n collected under the policy_
sequence β conditioning on that τn:n+1 = (sn, an). Besides, we denote ρ[β][k] (τ0:n|sn) as the distribution under policy βk conditioning on sn at the last step. Obviously, ρ[β][k] is independent of last
step’s action an as π ∈ Πs. By Bayes rule, we have the following relation

_K_
_k=1_ _[ρ][β][k]_ [(][τ][0:][n][|][s][n][)][β][k][(][a][n][|][s][n][)]
_ρ[β](τ0:n_ _sn, an) =_ _K_ (16)
_|_
P _k=1_ _[β][k][(][a][n][|][s][n][)]_

P

In other word, ρ[β]( _sn, an) is a weighted sum of_ _ρ[β][k]_ ( _sn)_ . The weights are different between ans

_·|_ _{_ _·|_ _}_
since the behavior policies _βk_ are different. Consequently, _Q[β](sn, an) will also vary between ans_
_{_ _}_
and it is hard to be quantified in practice. Unfortunately, we may want Qφ(sn, an) _r(sn, an) for_
_∝_
unbiased credit assignment the actions at the last step. Thus, updating the policy with approximated

[f]
_Qφ(sn, an) will suffer from the off-policy bias term_ _Q[β](sn, an)._

B.6 DISCUSSION OF FIXED POINT BIAS [f]

Even with on-policy samples in D, critic learning via Eq. (1) can still be erroneous. We formally
state the observation via a toy sum-form PI-DRMDP-0.

_a0_ _C_ _c_ _C0_

_A_

_a1_

_D_ _D0_

_d_

_b_

_B_


5Without loss of generality, the discussion is simplified by assuming sn will only appear at the end of a
reward interval.


-----

where for state B, C, D, there is only one action. The agent only needs to decide among a0, a1 at
state A. n is fixed as 2 and the initial state distribution p0(A) = p0(B) = 0.5. The reward has
sum-form over the following per-step reward

_r(C, c) = r(D, d) = 0_
_r(A, a0) = 0.01, r(A, a1) = 1_
_r(B, b) = −1_

**Fact 8. For the above PI-DRMDP-0 and any initialization of the policy, value evaluation (Qφ(st, at)**
_in tabular form) via Eq. (1) with on-policy data and policy iteration w.r.t Qφ(st, at) will converge_
_to a sub-optimal policy with J (π) = −0.495γ. In contrast, our algorithm can achieve the optimal_
_policy with J (π) = 0._

_Proof. For any policy π, we denote p = π(a1_ _A). Then, if we minimize Eq. (1) with on-policy_
_|_
samples, we will have Qφ(C, c) = 0.01, Qφ(A, a0) = 0.01γ. Besides, as initial state distribution
are uniform, we have

_Qφ(D, d) = [1]_

2 [(][p][ −] [1)][, Q][φ][(][A, a][1][) = 1]2 _[γ][(][p][ −]_ [1)]

Since Qφ(A, a1) < Qφ(A, a0) for any p, policy iteration will converges to a sub-optimal policy that
_π(a0_ _A) = 1.The sub-optimal policy has_ (π) = 0.495γ. Noticing that this holds even if the
_|_ _J_ _−_
initial policy is already the optimal.

For our algorithm, we have

_Qφ(A, a1) = γQφ(A, a0, D, d) = γ_

For any γ > 0 and any p, a single policy iteration will have the optimal policy with but the optimal
has J (π[∗]) = 0.

B.7 DISCUSSION OF DRMDP DEFINITION

In this section, we compare DRMDPs with other related definitions about MDP delays. To be
specific, we provide brief discussions on the differences between DRMDP (delayed reward MDP)
with Delay MDP and Semi-MDP.

**Semi-MDP and Option Framework: In Semi-MDP, actions are allowed to execute variable length**
of time. In DRMDP, actions are executed and re-planned at each time step while rewards are delayed
for random length and are non-Marvian. Simply solving DRMDPs (or PI-DRMDPs) as Semi-MDPs
will result in serious sub-optimal problems.

The middle ground between Semi-MDP and MDP is the Option Framework (Sutton, 1998), in which
options are temporally extended action sequences and cen be learned, i.e., option policy, initial state
set, termination state set. Contrarily, in DRMDP, the length of the signal interval (comparing to the
length of the option execution time) is random and is determined by the environment. Furthermore,
in option framework, rewards are still provided once very step (Markovian).

Option framework is often used on problems with hierarchical structures (Al-Emran, 2015), in which
the whole policy is composed of a low-level option policy over actions and a high-level control policy
over options. However, DRMDPs are formulated for general sequential decision making problems.

**Delay MDP: Delay MDPs (Walsh et al., 2009; Derman et al., 2021) are used to characterize ob-**
servation delay and action delay. For example, the observation inputs to the controller (e.g., a
computer) is the observation of the remote agent (e.g., a drone) several milliseconds ago and the
action executed at the remote agent is the output of the control policy several steps ahead. We
have to emphasize that, in delay MDP, the rewards are still defined as per-step rewards and are coupled with the corresponding state-action pair, i.e., when the delay is a constant m, the transition
(st, at, rt, st+1)a observed at the remote agent, is also the transition the controller received at step
_t + m, i.e., (st+m, at+m, rt+m, st+m+1)c._

In DRMDP, there is no observation-delay or action-delay and there is no per-step rewards coupled
with each state-action pair.


-----

C ADDITIONAL RESULTS

C.1 DESIGN EVALUATION

Here, we show more experiments omitted in Section 4.1. To begin with, we compare different
HC-decomposition algorithms (Q-HC) with Q-RNN and vanilla SAC on several sum-form PIDRMDP tasks with different signal interval length distributions (qn). The results are consistent
with our discussions in Section 4.1, despite of the lengths and the distributions.

Table 2: Relative average performance in tasks with different signal interval length distribution qn
over 4 environments: Hopper-v2, HalfCheetah-v2, Walker2d-v2, Ant-v2. δ(x) refers to a fixed
interval length as x and U (a, b) refers a uniform distribution in range [a, b]. All results show the
mean of 6-7 seeds and N.A. refers to no result due to limited computation resources.

|q n|SAC -RNN -HC- -HC- -HC- -HC-RNN Q Q Q Q Q Singleton Pairwise-1 Pairwise-3|
|---|---|
|δ(10) U(10, 20) U(15, 20) δ(20) δ(40) δ(60)|0.62 0.61 0.91 0.89 0.92 0.98 0.48 0.52 0.95 0.84 0.95 0.96 0.48 0.61 0.89 0.99 0.95 0.93 0.51 0.53 0.91 0.93 0.98 0.99 0.30 0.38 0.90 0.94 0.85 0.87 0.17 N.A. 0.87 0.81 0.88 N.A.|


Then, we test the HC-decomposition on non sum-form PI-DRMDP taks based on OpenAI Gym.
Two non sum-form tasks are included: Max and Square. For Max,
_r(τt:t+n) = 10_ max
_×_ _i_ [t:t+n][{][r][(][s][i][, a][i][)][}]
_∈_

Here, r(si, ai) is the standard per-step reward in each task. For Square, we denote ravg(τt:t+n) =
1
_n_ _i_ [t:t+n] _[r][(][s][i][, a][i][)][ and the reward is defined as follows.]_

_∈_

P _ravg(τt:t+n)_ _ravg(τt:t+n)_ _< 1._

_r(τt:t+n) = 4 ×_ sign(ravg(τt:t+n)) _ravg[2]_ [(][τ][t][:][t][+][n][)] _|ravg(τt:t+n)|_ 1.
 _×_ _|_ _| ≥_
Noticing that these two tasks are still PI-DRMDP. The results are shown in Figure 5. Clearly, Q-HC
algorithms outperform Q-RNN and vanilla SAC on all tasks.

Figure 5: Experiments of non sum-form PI-DRMDP tasks. The first line shows the result of Max
form and the second line shows the result of Square form. The learning curves are mean and one
standard deviation of 6-7 seeds.

Moreover, we also implement TD3-based Q-HC-Singleton, Q-HC-Pairwise-1 and compare them
with the vanilla TD3 and TD3-based Q-RNN. The results still match our analysis of HCdecomposition in Section 3.2. However, we find that SAC-based variants consistently outperform


-----

TD3-based variants. We think that the entropy maximization regularization is useful in the delayed
reward environment, which prevents the policy getting trapped in local maximal points.

Figure 6: Learning curves of TD3-based Q-HC-Singleton, Q-HC-Pairwise-1, Q-RNN and vanilla
TD3. All curves show the mean and one standard deviation of 7 seeds. The result is consistent with
that of SAC-based methods.

C.2 ABLATION STUDY

In Figure 7, we show the ablation study on the regulation term Lreg on all 4 tasks. We observe
that in HalfCheetah-v2 and Ant-v2, the regulation plays an important role for the final performance
while the regulation is less necessary for the others. Besides, for Q-HC algorithms with complex
H-component architectures (i.e., Q-HC-RNN, Q-HC-Pairwise-3), the regulation is necessary while
for simple structure like Q-HC-Singleton, regulation is less helpful. We suspect that the simple
structure itself imposes implicit regulation during the learning.

Figure 7: More ablations on Lreg term. The task is a sum-form PI-DRMDP with n uniformly drawn
from 15 to 20. Dashed lines are the no regularization version of the algorithm Q-HC with the same
color. All curves show the mean and a standard deviation of 6-7 seeds.

Furthermore, we ablate the HC-decomposition in Q-HC-Singleton and Q-HC-Pairwise-1. Namely,
we implement Q-Singleton and Q-Pairwise-1 with the similar architecture but not with HCdecomposition. For Q-Singleton, we have


(τti:t+1) =
_Q_


_bφ(τj:j+1)_
_j∈[Xti:t+1]_


and for Q-Pairwise-1, we have

(τti:t+1) = _c[1]φ[(][τ][j][:][j][+1]_ _bφ(τj:j+1)_
_Q_ _[◦]_ _[τ][j][+1:][j][+2][) +]_

_j∈X[ti:t]_ _j∈[Xti:t+1]_

In Figure 8, we observe that these two ablations perform sluggishly and even worse than Q-RNN
(without any decomposition). This suggests the necessity of HC-decomposition for PI-DRMDP, i.e.,
the separated history part (H) and the current part (C), as the transition of the environment is still
Markovian. Without the separation, a simple decomposition fails to assign the adequate credit to the
last step action in τti:t+1, thus results in errorneous policy update.

C.3 COMPARATIVE EVALUATION

We show detailed results of our algorithms Q-HC and previous baselines on sum-form PIDRMDP tasks with general reward function. Please refer to Appendix B and Appendix A for


-----

Figure 8: Ablation of HC-decomposition in Q-HC-Singleton and Q-HC-Pairwise-1. The task is
a sum-form PI-DRMDP with n fixed as 20. Dashed lines are two ablations Q-Singleton and QPairwise-1. All curves show the mean and a standard deviation of 6 seeds.

detailed discussions. For environments with maximal c overlapping steps, the reward function is
formally defined as follows.


_rc(τti−c:ti+ni_ ) =


_r(si, ai)_
_i∈[ti−cX:ti+ni−c]_


where r(si, ai) is the standard per-step reward. If i < 0, then r(si, ai) = 0. In this definition,
the reward is delayed by c steps and thus the signal intervals are overlapped. In Figure 9, we show
the relative average performance (over Reach-v2, Hopper-v2, HalfCheetah-v2, Walker2d-v2, and
Ant-v2) of each algorithm w.r.t the Oracle SAC trained on the dense reward setting. Please refer to
Appendix D for the exact definition of this metric. The results demonstrate the superiority of our
algorithms over the baselines in the General Reward Function experiments. This supports our claim
that our algorithmic framework and the approximation method (HC) can be extended naturally to
the general definition, i.e., PI-DRMDP-c. Furthermore, we show the detailed learning curves of all
algorithms in tasks with different overlapping steps c in Figure 10.

Figure 9: Relative Average Performance of our algorithms and the baselines on PI-DRMDP-c tasks
with general reward functions. Each dot represents the relative performance of the algorithm over
the PI-DRMDP-c task (average over Reach-v2, Hopper-v2, HalfCheetah-v2, Walker2d-v2, Ant-v2).

D EXPERIMENT DETAILS

D.1 IMPLEMENTATION DETAILS OF OUR ALGORITHM

We re-implement SAC based on OpenAI baselines (Dhariwal et al., 2017). The corresponding
hyperparameters are shown in Table 3. We use a smaller batch size due to limited computation
power.

To convince that the superiority of our algorithms (Q-HC-RNN, Q-HC-Pairwise-K, Q-HCSingleton) results from the efficient approximation framework (Section 3.2) and the consistency


-----

Figure 10: Learning curves of our algorithms Q-HC and several previous baselines. The first line
refers to the task with c = 0. The second line refers to the task with c = 5. The third line refers to
the task with c = 10. All curves show the mean and one standard deviation of 6-7 seeds.

Table 3: Shared Hyperparameters with SAC

Hyperparameter SAC

_Qφ, πθ architecture_ 2 hidden-layer MLPs with 256 units each
non-linearity ReLU
batch size 128
discount factor γ 0.99
optimizer Adam (Kingma & Ba, 2014)
learning rate 3 × 10[−][4]

entropy target -|A|
target smoothing τ 0.005
replay buffer large enough for 1M samples
target update interval 1
gradient steps 1


with the theoretical analysis (Section 3.1), rather than from a better choice of hyper-parameters, we
did not tune any hyper-parameter shared with SAC (i.e., in Table 3).

The architecture of different Hφs are shown as follows. For the regularization coefficient λ, we
search λ in {0.0, 0.05, 0.5, 5.0} for each implementation of Q-HC. λ is fixed for all tasks including
_the toy example._

-  Q-HC-RNN. Each step’s input τt:t+1 is first passed through a fully connected network with 48
hidden units and then fed into the GRU with a 48-unit hidden state. Hφ(τti:t) is the output of
the GRU at the corresponding step. We use λ = 5.0.

-  Q-HC-Pairwise-1. Both c[0]φ [and][ c]φ[1] [are two-layer MLPs with 64 hidden units each.][ λ][ = 0][.][5][.]

-  Q-HC-Pairwise-3. All c[i]φ[, i][ = 0][,][ 1][,][ 2][,][ 3][ are two-layer MLP with 48 hidden units each.][ λ][ = 5][.][0][.]

-  Q-HC-Singleton. bφ is a two-layer MLPs with 64 hidden units each. λ = 0.05.
As suggested by the theoretical analysis, we augment the normalized t − _ti to the state input._


-----

All high dimensional experiments are based on OpenAI Gym (Greg et al., 2016) with MuJoCo200.
All experiments are trained on GeForce GTX 1080 Ti and Intel(R) Xeon(R) CPU E5-2630 v4 @
2.20GHz. Each single run can be completed within 36 hours.

D.2 IMPLEMENTATION DETAILS OF OTHER BASELINES

_Q-RNN. We use the same shared hyper-parameters as in Table 3. The only exception is the architec-_
ture of the GRU critic. Similar to Q-HC-RNN, each state-action pair is first passed through a fully
connected layer with 128 hidden units and ReLU activation. Then it is fed into the GRU network
with 128-unit hidden state. The _φ(τti:t+1) is the output of the GRU network at the correspond-_
_Q_
ing step. We choose the above architecture to ensure that the number of parameters is roughly the
same with our algorithm for fair comparison. The learning rate for the critic is still 3 × 10[−][4] after
fine-tuning.

**SAC-IRCR. Iterative Relative Credit Refinement (Gangwani et al., 2020) is implemented on**
episodic reward tasks. In the delayed reward setting, we replace the smoothing value (i.e., episode
return) with the reward of the reward interval. We find this performs better than using the episode
return.

D.3 DETAILS FOR TOY EXAMPLE

**Point Reach. As illustrated in Figure 4, the Point Reach PI-DRMDP task consists of a 100x100**
grid, the initial position at the bottom left corner, a 10x10 target area adjacent to the middle of the
right edge and a point agent. The observation for the point agent is its (x, y) coordinate only. The
action space is its moving speed (vx, vy) [ 1, 1][2] along the two directions. Since the agent can
_∈_ _−_
not observe the goal directly, it has to infer it from the reward signal. To relieve the agent from
heavy exploration (not our focus), the delayed rewards provide some additional information to the
agent as follows.

The grid is divided into 10 sub-areas of 10x100 along the x-axis. We denote the sub-areas (highlighted by one color each in Figure 4) as Si, i = [0 : 10] form left to right. In each sub-area Si, it
provides an extra reward and the reward r[i] indicates how far is the sub-area to the target (i.e., r[i] = i).
Clearly, the bigger the reward is, the closer the sub-area is to the target area. Besides, reward interval
length is fixed as 20 and the point agent is rewarded with the maximal r[i] it has visited in the interval
together with a bonus after reaching the target and a punishment for not reaching. Namely,


_r[i]_ **1(sj:j+1** _Si) + 10_ (1(reach target) 1)
_i=0_ _·_ _∈_ _·_ _−_

X


_r(τt:t+n) =_ max
_j∈[t:t+n]_


The task ends once the agent reach the target area and also terminates after 500 steps. Clearly. the
optimal policy is to go straightly from the initial point to the target area without hesitation. The
shortest path takes roughly 95 steps. The learning curves in Figure 4 show the mean and half of a
standard deviation of 10 seeds. All curves are smoothed equally for clarity.

**Heatmap. In Figure 4 right, the heatmap visualizes the value of bφ(st, at) on the whole grid. We**
select the bφs after the training has converged to the optimal policy. To be specific of the visualization, for each of the 10x10 cells on the grid, st in bφ(st, at) is selected as the center of the cell
and at is sampled from π(·|st, 0). Additionally, in Figure 11, we visualize bφ(st, at) similarly of
_Q-HC-Singleton without Lreg. Clearly, Figure 11 shows the similar pattern as Figure 4 right._

D.4 DETAILS FOR VARIANCE ESTIMATION

The experiment in Figure 1 is conducted in the following manner. First, we train a policy πθ with
_Q-HC-Singleton and collect all the samples along the training to the replay buffer D. Second, two_
additional critics are trained concurrently with samples uniformly sampled from D. One critic uses
the GRU architecture (i.e., Q-RNN) and the other uses Q-HC-Singleton structure. Most importantly,
since these two critics are not used for policy updates, there is no overestimation bias (Fujimoto et al.,
2018). Thus, instead of using the CDQ method Fujimoto et al. (2018), these two critics are trained
via the method in DDPG (Silver et al., 2014). With a mini-batch from D, we compute the gradients
on the policy’s parameters θ in Eq. (5) and Eq. (7) from the two extra critics respectively. Noticing
that these gradients are not used in the policy training.


-----

Figure 11: Visualization of bφ(st, at) of -HC-Singleton without regulation term.
_Q_

Then, we compute the sample variance of the gradient on each parameter θi in the policy network.
The statistics (y-axis in Figure 1) is the sum of variance over all parameters in the final layer (i.e., the
layer that outputs the actions). The statistics is further averaged over the whole training and scaled
uniformly for clarity.

D.5 DETAILS FOR RELATIVE AVERAGE PERFORMANCE

Here, we formally introduce the Relative Average Performance (RAP) metric in Table 2, Figure 9,
and Figure 3. The relative average performance (RAP) of algorithm A on task set E is defined
as the average of RAP over tasks in E. RAP of each task is the episodic return after 1M steps
training normalized by the episodic return of Oracle SAC at 1M steps (trained on dense reward
environment). The only exception is Reach-v2, in which the episodic returns are added by 50 to
before the normalization.


-----

