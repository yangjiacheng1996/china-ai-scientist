# GENERALISATION IN LIFELONG REINFORCEMENT LEARNING THROUGH LOGICAL COMPOSITION

**Geraud Nangue Tasse, Steven James & Benjamin Rosman**
School of Computer Science and Applied Mathematics
University of the Witwatersrand
Johannesburg, South Africa
geraudnt@gmail.com, {steven.james, benjamin.rosman1}@wits.ac.za

ABSTRACT

We leverage logical composition in reinforcement learning to create a framework
that enables an agent to autonomously determine whether a new task can be
immediately solved using its existing abilities, or whether a task-specific skill
should be learned. In the latter case, the proposed algorithm also enables the
agent to learn the new task faster by generating an estimate of the optimal policy.
Importantly, we provide two main theoretical results: we bound the performance
of the transferred policy on a new task, and we give bounds on the necessary and
sufficient number of tasks that need to be learned throughout an agent’s lifetime to
generalise over a distribution. We verify our approach in a series of experiments,
where we perform transfer learning both after learning a set of base tasks, and
after learning an arbitrary set of tasks. We also demonstrate that, as a side effect
of our transfer learning approach, an agent can produce an interpretable Boolean
expression of its understanding of the current task. Finally, we demonstrate our
approach in the full lifelong setting where an agent receives tasks from an unknown
distribution. Starting from scratch, an agent is able to quickly generalise over the
task distribution after learning only a few tasks, which are sub-logarithmic in the
size of the task space.

1 INTRODUCTION

Reinforcement learning (RL) is a framework that enables agents to learn desired behaviours by
maximising the rewards received through interaction with an environment (Sutton et al., 1998). While
RL has achieved recent success in several difficult, high-dimensional domains (Mnih et al., 2015;
Levine et al., 2016; Lillicrap et al., 2016; Silver et al., 2017), these methods require millions of samples
from the environment to learn optimal behaviours. This is ultimately a fatal flaw, since learning to
solve complex, real-world tasks from scratch for every task of interest is typically infeasible. Hence a
major challenge in RL is building general-purpose agents that are able to use existing knowledge to
quickly solve new tasks in the environment. The question of interest is then: after learning n tasks
sampled from some distribution, how can an agent transfer or leverage the skills learned from those n
tasks to improve its starting performance or learning speed in task n + 1?

This problem setting is formalised by lifelong RL (Thrun, 1996; Abel et al., 2018). One approach to
transfer in lifelong RL is composition (Todorov, 2009), which allows an agent to leverage its existing
skills to build complex, novel behaviours that can then be used to solve or speed up learning of a new
task (Todorov, 2009; Saxe et al., 2017; Haarnoja et al., 2018; van Niekerk et al., 2019; Hunt et al.,
2019; Peng et al., 2019). Recently, Nangue Tasse et al. (2020) proposed a framework for defining a
Boolean algebra over the space of tasks and their optimal value functions. This allowed for tasks and
value functions to be composed using the union, intersection and negation operators in a principled
manner to yield optimal skills zero-shot.

In this work, we propose a framework for lifelong RL that focuses not only on transfer between tasks
for faster RL, but also provides guarantees on the generalisation of an agent’s skills over an unknown
task distribution. We first extend the logical composition framework of Nangue Tasse et al. (2020) to
discounted and stochastic tasks. We provide theoretical bounds for our approach in stochastic settings,


-----

and also compare them to previous work in the discounted setting. We then show how our framework
leverages logical composition to tackle the lifelong RL problem. The framework enables agents to
iteratively solve tasks as they are given, while at the same time constructing a library of skills that
can be composed to obtain behaviours for solving future tasks faster, or even without further learning.

We empirically verify our framework in a series of experiments, where an agent is i) pretrained on
a set of base tasks provided by the Boolean algebra framework, and ii) when the pretrained tasks
are not base tasks. We show that agents here are able to achieve significant jumpstarts on new tasks.
Finally, we demonstrate our framework in the lifelong RL setting where an agent receives tasks from
an unknown (possibly non-stationary) distribution and must determine what skills to learn and add to
its library, and how to combine its current skills to solve new tasks. Results demonstrate that this
framework enables agents to quickly learn a set of skills, resulting in a combinatorial explosion in
their abilities. Consequently, even when tasks are sampled randomly from an unknown distribution, an
agent can leverage its existing skills to solve new tasks without further learning, thereby generalising
over task distributions.

2 BACKGROUND

We consider tasks modelled by Markov Decision Processes (MDPs). An MDP is defined by the
tuple (S, A, p, r, γ), where (i) S is the state space, (ii) A is the action space, (iii) p(s[′]|s, a) is a
Markov transition probability, (iv) r is the real-valued reward function bounded by [rMIN, rMAX], and
(v) γ ∈ [0, 1) is the discount factor. In this work, we focus on tasks where an agent is required to
reach a set of desirable goals in a goal space G ⊆S (a set of boundary states). Here, termination in
_G is modelled similarly to van Niekerk et al. (2019) by augmenting the state space with a virtual_
state, ω, such that p(ω|s, a) = 1 ∀(s, a) ∈ (G × A) and the rewards are zero after reaching ω. We
hence consider the set of tasks M such that the tasks are in the same environment—described by a
background MDP ( _,_ _, p, γ, r0)—and each task can be uniquely specified by a set of desirable and_
_S_ _A_
undesirable goals:

( _,_ _, p, γ, r0) :=_ ( _,_ _, p, γ, r)_ _a_ _, r(s, a) = r0(s, a)_ _s_ ;
_M_ _S_ _A_ _{_ _S_ _A_ _| ∀_ _∈A_ _∀_ _∈S \ G_
_r(g, a) = rg ∈{rMIN, rMAX} ∀g ∈G}_ (1)

The goal of the agent is to compute a Markov policy π from S to A that optimally solves a given task.
A given policy π is characterised by a value function V _[π](s) = Eπ [[P][∞]t=0_ _[γ][t][r][(][s][t][, a][t][)]][, specifying]_
the expected return obtained under π starting from state s. The optimal policy π[∗] is the policy that
obtains the greatest expected return at each state: V _[π][∗]_ (s) = V _[∗](s) = maxπ V_ _[π](s) for all s in S. A_
related quantity is the Q-value function, Q[π](s, a), which defines the expected return obtained by
executing a from s, and thereafter following π. Similarly, the optimal Q-value function is given by
_Q[∗](s, a) = maxπ Q[π](s, a) for all s in S and a in A._

2.1 LOGICAL COMPOSITION

Nangue Tasse et al. (2020) recently proposed the notion of a Boolean task algebra, which allows
an agent to perform logical operations—conjunction (∧), disjunction (∨) and negation (¬)—over
the space of tasks and value functions. While they only considered deterministic shortest path
tasks (γ = 1 with deterministic dynamics), we summarise their approach here and later extend it to
discounted stochastic tasks (Section 3.1).

To achieve zero-shot logical composition, Nangue Tasse et al. (2020) extend the standard rewards
and value functions used by an agent to define goal-oriented versions as follows:
**Definition 1. The extended reward function ¯r : S × G × A →** R is given by the mapping

(s, g, a) _r¯MIN_ _if g ̸= s and s ∈G_ (2)
_7→_ _r(s, a)_ _otherwise,_


_where ¯rMIN ≤_ min{rMIN, (rMIN − _rMAX)D}, and D is the diameter of the MDP (Jaksch et al., 2010)._

**Definition 2. The extended Q-value function** _Q[¯] : S × G × A →_ R is given by the mapping

(s, g, a) 7→ _r¯(s, g, a) + γ_ _p(s[′]|s, a) V[¯]_ _π[¯](s′, g),_ (3)

_sX[′]∈S_


-----

where _V[¯]_ _π[¯](s, g) = Eπ¯_ [[][P][∞]t=0 _r[¯](st, g, at)]._

By penalising the agent for achieving goals different from those it wanted to reach (r¯MIN if g ̸=
_s and s ∈G), the extended reward function has the effect of driving the agent to learn how_
to separately achieve all desirable goals. Importantly, the standard reward and value functions can be recovered from their extended versions by simply maximising over goals. As
such, the agent can also recover the task policy by maximising over both goals and actions:
_π(s)_ arg maxa maxg _Q[¯](s, g, a)._
_∈_ _∈A_ _∈G_

The logic operators over tasks and extended action-value functions are then defined as follows:
**Definition 3. Let M be a set of tasks with bounds MMIN** _, MMAX ∈M such that,_
_r_ _MAX_ (s, a) := max _r_ _MIN (s, a) := min_
_M_ _M_ _[r][M]_ [(][s, a][)] _M_ _M_ _[r][M]_ [(][s, a][)]
_∈M_ _∈M_

_Define the ¬, ∨, and ∧_ _operators over M as_

(M ) := ( _,_ _, p, r_ _M_ ), where r _M_ (s, a) := (r _MAX_ (s, a) + r _MIN (s, a))_ _rM_ (s, a)
_¬_ _S_ _A_ _¬_ _¬_ _M_ _M_ _−_

_∨(M1, M2) := (S, A, p, rM1∨M2_ ), where rM1∨M2 (s, a) := max{rM1 (s, a), rM2 (s, a)}

_∧(M1, M2) := (S, A, p, rM1∧M2_ ), where rM1∧M2 (s, a) := min{rM1 (s, a), rM2 (s, a)}


**Definition 4.bounds** _Q[¯][∗]MIN Let[,][ ¯]Q[∗]MAXQ[¯][∗]_ _be the set of optimal extended¯_ _which are respectively the optimalQ[¯]-value functions for tasks inQ[¯]-functions for the tasks M, with_
_∈_ _Q[∗]_
_MIN_ _,_ _MAX_ _. Define the_ _,_ _, and_ _operators over_ [¯] _as,_
_M_ _M_ _∈M_ _¬_ _∨_ _∧_ _Q[∗]_

_¬( Q[¯][∗])(s, g, a) :=_ ¯Q[∗]MIN [(][s, g, a][) + ¯]Q[∗]MAX [(][s, g, a][)] _−_ _Q[¯][∗](s, g, a)_
  

_∨( Q[¯][∗]1[,][ ¯]Q[∗]2[)(][s, g, a][) := max][{][ ¯]Q[∗]1[(][s, g, a][)][,][ ¯]Q[∗]2[(][s, g, a][)][}]_

_∧( Q[¯][∗]1[,][ ¯]Q[∗]2[)(][s, g, a][) := min][{]Q[ ¯][∗]1[(][s, g, a][)][,][ ¯]Q[∗]2[(][s, g, a][)][}]_

Using the definitions for the logical operations over M and _Q[¯][∗]_ given above, Nangue Tasse et al.
(2020) construct a Boolean algebra over tasks and extended value functions. Furthermore, by
leveraging the goal-oriented definition of extended value functions, they also show that M and _Q[¯][∗]_
are homomorphic. As a result, if a task can be expressed using the Boolean algebra, the optimal value
function for the task can immediately be computed. This enables agents to solve any new task that is
given as the logical combination of learned ones.

3 LIFELONG TRANSFER THROUGH COMPOSITION

In lifelong RL, an agent is presented with a series of tasks sampled from some distribution D. The
agent then needs to not only transfer knowledge learned from previous tasks to solve new but related
tasks quickly, but it also should not forget learned knowledge in the process. We formalise this
lifelong learning problem as follows:
**Definition 5. Let D be an unknown, possibly non-stationary, distribution over a set of tasks**
( _,_ _, p, γ, r0). The lifelong learning problem consists of the repetition of the following steps for_
_M_ _S_ _A_
_t ∈_ N:

_1. The agent is given a task Mt_ (t),
_∼D_

_2. The agent interacts with the MDP Mt until it is ϵ-optimal in M0, ..., Mt._


This formulation of lifelong RL is similar to that of Abel et al. (2018); the main difference is that we
do not assume that D is stationary, and we explicitly require an agent to retain learned skills.

As discussed in the introduction, one of the main goals in this setting is that of transfer (Taylor &
Stone, 2009). We add an important question to this setting: how many tasks should an agent learn


-----

during its lifetime in order to generalise over the task distribution? In other words, how many tasks
should it learn to be able to solve any new task immediately? While most approaches focus on the
goal of transfer, the question of the number of tasks is often neglected by simply assuming the case
where the agent has already learned n tasks (Abel et al., 2018; Barreto et al., 2018). Consider, for
example, a task space with only |G| = 40 goals. Then, given the combination of all possible goals,
the size of the task space is |M| = 2[|G|] _≈_ 10[12]. If D is a uniform distribution over |M|, then for
most transfer learning methods an agent will have to learn most of the tasks it is presented with, since
the probability of observing the same task will be approximately zero. This is clearly impractical for
a setting like RL, where learning methods often have a high sample complexity even with transfer
learning. It is also extremely memory inefficient, since the learned skills of most tasks must be stored.

3.1 EXTENDING THE BOOLEAN ALGEBRA FRAMEWORK

In this section, we show how logical composition can be leveraged to learn a subset of tasks
that is sufficient to generalise over the task distribution. Since the logical composition results of
Nangue Tasse et al. (2020) were only shown for deterministic shortest path tasks (where γ = 1), we
extend the framework to discounted and stochastic tasks M (Equation 1). To achieve this, we first
redefine the extended reward function (Definition 1) to use the simpler penalty ¯rMIN = rMIN. We
also redefine ¬ over _Q[¯][∗]_ as follows:

¯Q[∗]MAX [(][.][)] if _Q[¯][∗](.)_ _Q[∗]MIN_ [(][.][)][| ≤|][ ¯]Q[∗](.) _Q[∗]MAX_ [(][.][)][|]
( Q[¯][∗])(.) := _|_ _−_ [¯] _−_ [¯] _,_ (.) _._
_¬_ _Q¯[∗]MIN_ [(][.][)] otherwise, _∀_ _∈S × G × A_


The intuition behind this re-definition of the negation operator is as follows: since each goal is
either desirable or not, the optimal extended value function _Q[¯][∗](s, g, a) is either_ _Q[¯][∗]MAX_ [(][s, g, a][)]
or _Q[¯][∗]MIN_ [(][s, g, a][)][. Hence, if][ ¯]Q[∗](s, g, a) is closer to _Q[¯][∗]MIN_ [(][s, g, a][)][, then its negation should be]
_Q¯[∗]MAX_ [(][s, g, a][)][, and vice versa. For tasks in][ M][, this is equivalent to the previous definition of][ ¬]
for optimal _Q[¯]-value functions, but it will give us tight bounds when composing ϵ-optimal_ _Q[¯]-value_
functions (see Theorem 1).

We now show that the Boolean algebra and zero-shot composition results of Nangue Tasse et al.
(2020) also hold for tasks in M.

**Proposition 1. Let** _Q[¯][∗]_ _be the set of optimal_ _Q[¯]-value functions for tasks in M. Let A : M →_ _Q[¯][∗]_
_be any map from M to_ _Q[¯][∗]_ _such that A (M_ ) = Q[¯][∗]M _[for all][ M][ in][ M][. Then,]_

_(i)_ _and_ [¯] _respectively form a Boolean task algebra (_ _,_ _,_ _,_ _,_ _MAX_ _,_ _MIN_ ) and
_M_ _Q[∗]_ _M_ _∨_ _∧_ _¬_ _M_ _M_
_a Boolean extended value functions algebra ( Q[¯][∗], ∨, ∧, ¬,_ _Q[¯][∗]MAX_ _[,][ ¯]Q[∗]MIN_ [)][,]

_(ii) A is a homomorphism between M and_ _Q[¯][∗]._

We can now solve any new task in M zero-shot if we are given the correct Boolean expression that
informs the agent how to compose its optimal skills. This is essential for the following results.

3.2 TRANSFER BETWEEN TASKS

In this section, we leverage the logical composition results to address the following question of interest:
given an arbitrary set of learned tasks, can we transfer their skills to solve new tasks faster? As we
will show in Theorem 1, we answer this question in the affirmative. To achieve this, we first note that
each task M ∈M can be associated with a binary vector T ∈{0, 1}[|G|] which represents its set of
desirable goals, as illustrated by the tasks in Table 1. The approximation _T[˜] of this task representation_
can be learned just from task rewards (rM (s, a)) by simply computing _T[˜](s) = 1rM_ (s,a)=rMAX at
each terminal state s that the agent reaches. We can then use any generic method, such as the sum-of_products (SOP), to determine a candidate Boolean expression (BEXP ) in terms of the learned binary_
representations _T[˜]n = {T[˜]1, ...,_ _T[˜]n} of a set of past tasks_ _M[ˆ]_ = {M1, ..., Mn} ⊆M. An estimate of
the optimal _Q[¯]-value function of M can then be obtained by composing the learned_ _Q[¯]-value functions_
˜¯n [=][ {]Q[ ˜]¯[∗]1[, ...,][ ˜]Q¯[∗]n[}][ according to][ B][EXP] [. Theorem 1 shows the optimality of this process.][1]
_Q[∗]_

1See Appendix A for proofs of theorems and Appendix B for a brief description of the SOP method.


-----

**Theorem 1. Let M ∈M be a task with reward function r, binary representation T and optimal**
_extended action-value function_ _Q[¯][∗]. Given ϵ-approximations of the binary representations_ _T[˜]n =_
_T[˜]1, ...,_ _T[˜]n_ _and optimal_ _Q[¯]-functions_ ¯[˜]n [=][ {][ ˜]Q¯[∗]1[, ...,][ ˜]Q¯[∗]n[}][ for n tasks][ ˆ] = _M1, ..., Mn_ _, let_
_{_ _}_ _Q[∗]_ _M_ _{_ _} ⊆M_

_T_ = _EXP ( [˜]n) and_ _Q[¯]_ = _EXP ( ¯[˜]n[)][,]_
_F_ _B_ _T_ _F_ _B_ _Q[∗]_

_where BEXP is derived from_ _T˜n and_ _T˜ using a generic method F._ _Define π(s)_ _∈_
arg maxa _Q_ _where Q_ := maxg _Q[¯]_ (s, g, a). Then,
_∈A_ _F_ _F_ _∈G_ _F_


_(i)_ _Q[∗]_ _Q[π]_
_∥_ _−_ _∥∞_ _≤_


2

1−γ [((][1][T][ ̸][=][T][F] [+][ 1][r /]∈{rg}|G| [)][r]∆ [+][ ϵ][)][,]


_(ii) if the dynamics are deterministic,_

_∥Q[∗]_ _−_ _QF_ _∥∞_ _≤_ (1T ̸=TF )r∆ + ϵ,

maxwheres,g,a 1 is the indicator function,f (s, g, a) _h(s, g, a)_ _._ _rg(s, a) := ¯r(s, g, a), r∆_ := rMAX − _rMIN, and ∥f −_ _h∥∞_ :=
_|_ _−_ _|_

Theorem 1(i) states that if _Q[¯]_ is close to optimal, then acting greedily with respect to it is also close
_F_
to optimal. Interestingly, this is similar to the bound obtained by Barreto et al. (2018) (Proposition 1)
for transfer learning using generalised policy improvement (GPI), but stronger.[2] This is unsurprising,
since π(s) arg maxa maxg _Q[¯]_ (s, g, a) can be interpreted as generalised policy improvement
_∈_ _∈A_ _∈G_ _F_
on the set of goal policies of the extended value function _Q[¯]_ . Importantly, if the environment is
_F_
deterministic, then we obtain a strong bound on the composed value functions (Theorem 1(ii)). This
bound shows that transfer learning using logical composition is ϵ-optimal—that is, there is no loss
in optimality—when the new task is expressible as a logical combination of past ones. With the
exponential nature of logical combination, this gives agents a strong generalisation ability over the
task space—and hence over any task distribution—as we will show in Theorem 2.

3.3 GENERALISATION OVER A TASK DISTRIBUTION

We leverage Theorem 1 to design an algorithm that combines the SOP approach with goal-oriented
learning to achieve fast transfer in lifelong RL. Given an off-policy RL algorithm A, the agent
initializes its extended value function _Q¯[˜], the task binary vector_ _T[˜], and a goal buffer. At the beginning_
of each episode, the agent computes TSOP and QSOP for _T[˜] using the SOP method and its library of_
learned task vectors and extended Q-functions. It then acts using the behaviour policy (ϵ-greedy for
example) ofIf TSOP = T A[˜], the agent also updates with _Q[¯]SOP for the action-value function ifQ¯[˜] for each goal in the goal buffer using TSOP = T[˜], and_ _AQ[¯]SOP . Additionally, when ∨_ _Q¯[˜] otherwise.[3]_

_̸_
the agent reaches a terminal state s, it adds it to the goal buffer and updates _T[˜](s) using the reward it_
receives (T[˜](s) = 1rM (s,a)=rMAX ). Training stops when the agent has reached the desired level of
optimality (or after n episodes in practice), after which the agent adds the learned _T[˜] and_ _Q¯[˜] to its_
library if TSOP = T[˜]. The full algorithm is included in Appendix B. We refer to this algorithm as
SOPGOL (Sum Of Products with Goal-Oriented Learning ̸ ).

When G is finite, we show in Theorem 2 that SOPGOL generalises over any unknown task distribution
after learning only a number of tasks logarithmic in the size of the task space. The lower bound is
_⌈log|G|⌉, since this is the minimum number of tasks that span the task space, as can be seen in Table_
1 (top) for example. The upper bound is |G| because that is the dimensionality of the task binary
representations {0, 1}[|G|]. Since the number of tasks is |M| = 2[|G|], we have that the upper bound
_|G| = log|M| is logarithmic in the size of the task space._

2See Section 1.4 of the appendix for a detailed discussion of this with the simplification of the bound in
Proposition 1 (Barreto et al., 2018) to the same form as Theorem 1(i).
3Since ¯QSOP ∨ _Q¯[˜] = max{Q[¯]SOP,_ _Q¯[˜]}, it is equivalent to GPI and hence is guaranteed to be equal or_
more optimal than the individual value functions. Hence usingstraightforward way of leveraging _Q[¯]SOP to learn_ _Q¯[˜] faster._ _Q[¯]SOP ∨_ _Q¯[˜] in the behaviour policy gives a_


-----

**Theorem 2.(** _,_ _, p, γ, r Let0) D with finite be an unknown, possibly non-stationary, distribution over a set of tasks. Let A_ : ¯ _be any map from_ _to_ [¯] _such that_
_M_ _S_ _A_ _G_ _M →_ _Q[∗]_ _M_ _Q[∗]_
_A (M_ ) = Q[¯][∗]M _[for all][ M][ in][ M][. Let]_

_T˜t+1,_ _Q¯[˜]t[∗]+1_ [=][ SOPGOL][(][A][, M][t][,][ ˜]Tt, _Q¯[˜]t[∗][)][ where][ M][t]_ _[∼D][(][t][)][ and][ ˜]T0 = Q¯[˜]0[∗]_ [=][ ∅] _[∀][t][ ∈]_ [N][.]

_Then,_
log lim _where Nt :=_ [˜]t = ¯[˜]t
_⌈_ _|G|⌉≤_ _t→∞_ _[N][t][ ≤|G|]_ _|T_ _|_ _|Q[∗][|][.]_

Interestingly, Theorem 2 holds even in the case where a new task is expressible in terms of past tasks
(TSOP = T[˜]), but we wish to solve it to a higher degree of optimality than past tasks. In this case, we
can pretend TSOP = T[˜] and learn a new _Q[¯]-function to the desired degree of optimality. We can then_
add it to our library, and remove any other skill from our library (the least optimal for example). ̸

4 EXPERIMENTS

Goals

_Ta_ 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1

_Tb_ 0 1 1 0 0 1 1 0 0 1 1 0 0 1 1

_Tc_ 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1

_Td_ 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1

Goals


_T1_ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
_T2_ 0 0 1 0 1 1 1 0 1 1 0 0 1 0 0
_T3_ 1 1 0 1 0 0 0 1 0 1 1 0 0 1 1


Figure 1: PICKUPOBJ domain.
The red triangle represents the
agent.


Table 1: Binary representation for base (top) and test (bottom)
tasks. 0 or 1 corresponds to a goal reward of rMIN or rMAX.


4.1 TRANSFER AFTER PRETRAINING ON A SET OF TASKS

We consider the PICKUPOBJ domain from the MINIGRID environment (Chevalier-Boisvert et al.,
2018), illustrated by Figure 1, where an agent must navigate in a 2D room to pick up objects of
various shapes and colours from pixel observations.[4] This type of domain is prototypical in the
literature (Nangue Tasse et al., 2020; Barreto et al., 2020; van Niekerk et al., 2019; Abel et al., 2018),
because it allows for easy demonstration of transfer learning in many-goal tasks. In this domain,
there are |G| = 15 goals each corresponding to picking up objects of 3 possible types—box, ball,
key—and 5 possible colours—red, blue, green, purple, and yellow. Hence a set of log2 = 4
base tasks can be selected that can be used to solve all 2[|G|] = 32768 possible tasks under a Boolean ⌈ _|G|⌉_
composition of goals. The agent receives a reward of 2 when it picks up desired objects, and −0.1
otherwise. For all of our experiments in this section, we use deep Q-learning (Mnih et al., 2015)
as the RL method for SOPGOL and as the performance baseline. We also compare SOPGOL to
SOPGOL-transfer, or to SOPGOL-continual. SOPGOL-transfer refers to when no new skill is
learned and SOPGOL-continual refers to when a new skill is always learned using the SOP Q
estimate to speed up learning, even if the new task could be solved zero shot. Since SOPGOL
determines automatically which one to use, we compare whichever one it chooses with the other one
in each of our experiments.

We first demonstrate transfer learning after pretraining on a set of base tasks—a minimal set of tasks
that span the task space. This can be done if the set of goals is known upfront, by first assigning a
Boolean label to each goal in a table and then using the rows of the table as base tasks. These are
illustrated in Table 1 (top). Having learned the ϵ-optimal extended value functions for our base tasks,

4Further environment details are given in Appendix D.


-----

10


10


10


|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|||||||||
||SOP|GOL||||||
||SOP DQN|GOL conti|nual|||||
|||||||||
|||||||||
|||||||||


SOPGOL
SOPGOL continual
DQN

0.0 0.5 1.0 1.5 2.0 2.5 3.0

Episodes 1e4

Goals

0 1 2 3 4 5

Episodes 1e2


(a) M1


|Col1|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||||||||
||||||||
||||||||
||||||SOPGOL SOPGOL|continual|
||||||DQN||
||||||||


0.0 0.5 1.0 1.5 2.0 2.5

Episodes 1e4

Goals

0 1 2 3 4 5

Episodes 1e2


(b) M2

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|||||||||
||||SO|PGOL||||
||||SO DQ|PGOL co N|ntinual|||
|||||||||
|||||||||


3 4 5 6 7

Episodes 1e3


Episodes 1e2

(c) M3


Figure 2: Episodic returns (top) and learned binary representations (bottom) for test tasks M1, M2
and M3 after pretraining on the base set of tasks Ma, Mb, Mc and Md. The shaded regions on the
episodic returns indicate one standard deviation over 4 runs. The learned binary representations are
similarly averaged over 4 runs, and reported for the first 500 episodes. The initial drop in DQN
performance is as a result of the initial exploration phase where the exploration constant decays from
0.5 to 0.05. The Boolean expressions generated by SOPGOL during training for the respective test
tasks are:
_M1_ = _Ma_ _Mb_ _Mc_ _Md,_
_M2_ = (Ma ∧ _M ∧b_ _∧Md)_ (Ma _Mc_ _Md)_ ( _Ma_ _Mb_ _Mc_ _Md)_ ( _Ma_
_Mb ∧¬_ _Mc ∧¬Md), ∨_ _∧_ _∧_ _∨_ _¬_ _∧_ _∧¬_ _∧¬_ _∨_ _¬_ _∧_
_M3_ = (¬Ma _∧¬Mb_ _M ∧c)_ (Ma _Mb_ _Md)_ (Ma _Mc_ _Md)_ ( _Ma_ _Mb_ _Mc_
_Md ∧)_ ( ∧Ma _∨_ _Mb_ _∧¬Mc_ _∧¬Md)_ _∨(_ _Mb_ _∧Mc_ _∧_ _Md ∨)._ _¬_ _∧_ _∧¬_ _∧_
_¬_ _∨_ _¬_ _∧¬_ _∧¬_ _∧_ _∨_ _¬_ _∧_ _∧¬_

we can now leverage logical composition for transfer learning on test tasks. We consider the three test
tasks shown in Table 1 (bottom). For each, we run SOPGOL, SOPGOL-continual, and a standard
DQN. Figure 2 illustrates the results where, as predicted by our theoretical results in Section 3.2,
SOPGOL correctly determines that the current test tasks are solvable from the logical combinations
of the learned base tasks. Its performance from the start of training is hence the best.

Now that we have demonstrated how SOPGOL enables an agent to solve any new task in an
environment after training on base tasks, we consider the more practical case where new tasks are
not fully expressible as a Boolean expression of previously learned tasks. The agent in this case


10


10


10


|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|||||||||
||||||S|OPGOL||
||||||S D|OPGOL QN|transfer|
|||||||||
|||||||||


SOPGOL
SOPGOL transfer
DQN

0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75

Episodes 1e4

Goals

0 1 2 3 4 5

Episodes 1e2


(a) M1


|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
||||||||||
||||||||||
||||||||||
|||||||SO SO|PGOL PGOL t|ransfer|
|||||||DQ|N||


0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6

Episodes 1e4

Goals

0 1 2 3 4 5

Episodes 1e2


(b) M2

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|||||||||
|||||||||
|||||||||
||||||SO SO|PGOL PGOL tra|nsfer|
||||||DQ|N||


3 4 5 6

Episodes 1e3


2 3 4 5

Episodes 1e2

(c) M3


Figure 3: Episodic returns (top) and learned binary representations (bottom) for test tasks M1, M2
and M3 after pretraining on the non-base set of tasks,, and . The shaded regions on the episodic

,,

returns indicate one standard deviation over 4 runs. The learned binary representations are similarly
averaged over 4 runs, and reported for the first 500 episodes. The initial drop in DQN performance is
a result of the initial exploration phase where the exploration constant decays from 0.5 to 0.05. The
Boolean expressions generated by SOPGOL for the respective test tasks are:
_M1_ =,
_¬_ _∧¬_ _∧_ _∧¬_
_M2_ = ( ) ( ) ( ) ( ),
_∧¬_ _∧¬_ _∨_ _¬_ _∧_ _∧¬_ _∧_ _∨_ _¬_ _∧¬_ _∧_ _∧_ _∨_ _¬_ _∧¬_ _∧¬_
_Mf3_ = ( ) ( ) ( ).
_¬_ _∧¬_ _∧¬_ _∨_ _¬_ _∧¬_ _∨_ _¬_ _∧¬_ _∧¬_
f
f


-----

is pretrained on a set of tasks that do not span the task space, {, _,_ _, }, corresponding to the_
tasks of picking up green objects, blue objects, yellow objects, and keys. We then train the agent
with SOPGOL, SOPGOL-transfer, and a standard DQN on the same set of test tasks considered
previously (Table 1 (bottom)). The results in Figure 3 demonstrate how SOPGOL now chooses to
learn a task-specific skill after transfer, and hence outperforms SOPGOL-transfer since the test tasks
are not entirely expressible in terms of the pretrained ones. Consider Figure 3a, for example. The
test task is to pick up a yellow box, but the agent has only learned how to pick up red objects, blue
objects, yellow objects, and keys. It has not learned how to pick up boxes. However, we note from
the inferred Boolean expression (M[f]1) that the agent correctly identifies that the desired objects are, at
the very least, yellow. Without further improvements to this transferred policy (SOPGOL-transfer),
we can see that this approach outperforms DQN from the start. This is due to two main factors: (i) the
transferred policy navigates to objects more reliably, so takes fewer random actions; and (ii) although
the transferred policy does not have a complete understanding of which are the desirable objects, it at
least navigates to yellow objects, which are sometimes yellow boxes.

Finally, since SOPGOL is able to determine that the current task is not entirely expressible in terms
of its previous tasks (by checking whether TSOP = T[˜]), it is able to learn a new _Q[¯]-value function_
that improves on the transferred policy. Additionally, its returns are strictly higher than those of
SOPGOL-transfer because SOPGOL learns the new _Q[¯]-value function faster by using_ _Q[¯]SOP_ _Q¯ in_
the behaviour policy. _∨_ [˜]

4.2 LIFELONG TRANSFER


In this section, we consider the more general setting where the agent is not necessarily given pretrained
skills upfront, but is rather presented with tasks sampled from some unknown distribution. We revisit
the example given in Section 3, but now more concretely by using a stochastic Four Rooms domain
(Sutton et al., 1999), with a goal space of size |G| = 40 and a task space of size |M| = 2[|G|] _≈_ 10[12].
Complete environment details are given in Appendix C.

We demonstrate the ability of SOPGOL to generalise over task distributions by evaluating the
approach with the following distributions: (i) _sampled: the goals for each task are chosen uniformly_
_D_
at random over ; (ii) _best: the first_ log2 tasks are the base tasks, while the rest follow
_sampled. This distribution gives the agent the minimum number of tasks to learn and store, since G_ _D_ _⌈_ _|G|⌉_
_D_
the agent learns the base tasks first before being presented with any other task. (iii) _worst: the first_
_D_
_|G| tasks are each defined by a single goal that differs from the previous tasks, while the rest follow_
_sampled. This distribution forces the agent to learn and store the maximum number of tasks, since_
_D_
none of the |G| tasks can be expressed as a logical combination of the others. We use Q-learning
(Watkins, 1989) as the RL method for SOPGOL, and Q-learning with maxQ initialisation as a
baseline. This has been shown by previous work (Abel et al., 2018) to be a practical method of
initialising value functions with a theoretically optimal optimism criterion that speeds-up convergence
during training. Our results (Figure 4) show that SOPGOL enables a lifelong agent to quickly
generalise over an unknown task distribution. Interestingly, both graphs show that the convergence




2.5

2.0

1.5

1.0

0.5

0.0

|×101|Col2|
|---|---|
|Dbest Dworst||
|DSampled||
|maxQ||
|||
|||

|×104|Col2|Col3|
|---|---|---|
||Dbest||
||Dworst DSampl|ed|
||maxQ||
||||
||||
||||
||||


0 10 20 30 40 50

Tasks

(a) Number of policies learned and stored after
solving n tasks.


0 10 20 30 40 50

Tasks

(b) Number of samples required to learn ϵoptimal policies for each task.


Figure 4: Number of policies learned and samples required for the first 50 tasks of an agent’s lifetime
in the Four Rooms domain. The shaded regions represent standard deviations over 25 runs.


-----

speed during a randomly sampled task distribution Dsampled is very close to that of the best task
distribution _best. This suggests that there is room to make the bound in Theorem 2 even tighter by_
_D_
making some assumptions on the task distribution—an interesting avenue for future work.

5 RELATED WORK

There have been several approaches in recent years for tackling the problem of transfer in lifelong
RL. Most closely related is the line of work on concurrent skill composition (Todorov, 2009; Saxe
et al., 2017; Haarnoja et al., 2018; van Niekerk et al., 2019; Hunt et al., 2019). These methods
usually focus on multi-goal tasks, where they address the combinatorial amount of desirable goals by
composing learned skills to create new ones. Given a reward function that is well approximated by
a linear function, Barreto et al. (2020) propose a scheme for few-shot transfer in RL by combining
GPI and successor features (SF) (Barreto et al., 2017). In general, approaches based on GPI with SFs
(Barreto et al., 2021) are suitable for tasks defined by linear preferences over features (latent goal
states). Given the set of features for an environment, Alver & Precup (2022) shows that a base set of
successor features can be learned, which is sufficient to span the task space. While these approaches
also support tasks where goals are not terminal, the smallest number of successor features that must
be learned to span the task space is |G| (the upper-bound in Theorem 2). Our work is similar to these
approaches in that it can be interpreted as performing GPI with the logical composition of extended
value functions, which leads to stronger theoretical bounds than GPI with the linear composition of
successor features (see Appendix A.4). Finally, none of these works consider the lifelong RL setting
where an agent starts with no skill and receives tasks sampled from an unknown distribution (without
additional knowledge like base features or true task representations). In contrast, SOPGOL is able to
handle this setting with logarithmic bounds on the number of skills needed to generalise over the task
distribution (Theorem 2).

Other approaches like options (Sutton et al., 1999) and hierarchical RL (Barto & Mahadevan, 2003)
address the lifelong RL problem via temporal compositions. These methods are usually focused on
single-goal tasks, where they address the potentially long trajectories needed to reach a desired goal
by composing sub-goal skills sequentially (Levy et al., 2017; Bagaria & Konidaris, 2019). While they
do not consider the multi-goal setting, they can be used in conjunction with concurrent composition
to learn how to achieve a combinatorial amount of desirable long horizon goals. Finally, there are
also non-compositional approaches (Finn et al., 2017; Abel et al., 2018; Singh et al., 2021), which
usually aim to learn the policy for a new task faster by initializing the networks with some pre-training
procedure. These can be used in combination with SOPGOL to learn new skills faster.

6 CONCLUSION

In this work, we proposed an approach for efficient transfer learning in RL. Our framework, SOPGOL,
leverages the Boolean algebra framework of Nangue Tasse et al. (2020) to determine which skills
should be reused in a new task. We demonstrated that, if a new task is solvable using existing skills,
an agent is able to solve it with no further learning. However, even if this is not the case, an estimate
of the optimal value function can still be obtained to speed up training. This allows agents in a lifelong
learning setting to quickly generalise over any unknown (possibly non-stationary) task distribution.

The main limitation of this work is that it only consider tasks with binary goal rewards—where goals
are either desirable or not. Although this covers a vast number of many-goal tasks, combining our
framework with works on weighted composition (van Niekerk et al., 2019; Barreto et al., 2020) could
enable a similar level of generalisation over tasks with arbitrary goal rewards. Another exciting
avenue for future work would be to extend our transfer learning and generalisation results to include
temporal tasks by leveraging temporal composition approaches like options. Finally, we note that
just like previous work, we rely on the existence of an off-the-shelf RL method that is able to learn
goal-reaching tasks in a given environment. Since that is traditionally very sample inefficient, our
framework can be complemented with other transfer learning methods like MAXQINIT (Abel et al.,
2018) to speed up the learning of new skills (over and above the transfer learning and task space
generalisation shown here). Our approach is a step towards the goal of truly general, long-lived
agents, which are able to generalise both within tasks, as well as over the distribution of possible
tasks it may encounter.


-----

ACKNOWLEDGEMENTS

GNT is supported by an IBM PhD Fellowship. This research was supported, in part, by the National
Research Foundation (NRF) of South Africa under grant number 117808. The content is solely the
responsibility of the authors and does not necessarily represent the official views of the NRF.

The authors acknowledge the Centre for High Performance Computing (CHPC), South Africa, for
providing computational resources to this research project. Computations were also performed using
High Performance Computing Infrastructure provided by the Mathematical Sciences Support unit at
the University of the Witwatersrand.

REFERENCES

D. Abel, Y. Jinnai, S. Y. Guo, G. Konidaris, and M. Littman. Policy and value transfer in lifelong
reinforcement learning. In International Conference on Machine Learning, pp. 20–29, 2018.

S. Alver and D. Precup. Constructing a good behavior basis for transfer using generalized policy
[updates. In International Conference on Learning Representations, 2022. URL https://](https://openreview.net/forum?id=7IWGzQ6gZ1D)
[openreview.net/forum?id=7IWGzQ6gZ1D.](https://openreview.net/forum?id=7IWGzQ6gZ1D)

A. Bagaria and G. Konidaris. Option discovery using deep skill chaining. In International Conference
_on Learning Representations, 2019._

A. Barreto, W. Dabney, R. Munos, J. Hunt, T. Schaul, H. van Hasselt, and D. Silver. Successor
features for transfer in reinforcement learning. In Advances in Neural Information Processing
_Systems, pp. 4055–4065, 2017._

A. Barreto, D. Borsa, J. Quan, T. Schaul, D. Silver, M. Hessel, D. Mankowitz, A. Zidek, and
R. Munos. Transfer in deep reinforcement learning using successor features and generalised policy
improvement. In International Conference on Machine Learning, pp. 501–510. PMLR, 2018.

A. Barreto, S. Hou, D. Borsa, D. Silver, and D. Precup. Fast reinforcement learning with generalized
policy updates. Proceedings of the National Academy of Sciences, 117(48):30079–30087, 2020.

A. Barreto, D. Borsa, S. Hou, G. Comanici, E. Aygün, P. Hamel, D. Toyama, J. Hunt, S. Mourad,
D. Silver, et al. The option keyboard: Combining skills in reinforcement learning. arXiv preprint
_arXiv:2106.13105, 2021._

A. Barto and S. Mahadevan. Recent advances in hierarchical reinforcement learning. Discrete event
_dynamic systems, 13(1):41–77, 2003._

M. Chevalier-Boisvert, L. Willems, and S. Pal. Minimalistic gridworld environment for openai gym.
[https://github.com/maximecb/gym-minigrid, 2018.](https://github.com/maximecb/gym-minigrid)

C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep networks.
In International Conference on Machine Learning, pp. 1126–1135. PMLR, 2017.

T. Haarnoja, V. Pong, A. Zhou, M. Dalal, P. Abbeel, and S. Levine. Composable deep reinforcement
learning for robotic manipulation. In 2018 IEEE International Conference on Robotics and
_Automation, pp. 6244–6251, 2018._

J. Hunt, A. Barreto, T. Lillicrap, and N. Heess. Composing entropic policies using divergence
correction. In International Conference on Machine Learning, pp. 2911–2920, 2019.

T. Jaksch, R. Ortner, and P. Auer. Near-optimal regret bounds for reinforcement learning. Journal of
_Machine Learning Research, 11(Apr):1563–1600, 2010._

S. Levine, C. Finn, T. Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies. The
_Journal of Machine Learning Research, 17(1):1334–1373, 2016._

A. Levy, R. Platt, and K. Saenko. Hierarchical actor-critic. arXiv preprint arXiv:1712.00948, 12,
2017.


-----

T.. Lillicrap, J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous
control with deep reinforcement learning. In International Conference on Learning Representations,
2016.

V. Mnih, K. Kavukcuoglu, D. Silver, A. Rusu, J. Veness, M. Bellemare, A. Graves, M. Riedmiller,
A. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. Nature,
518(7540):529, 2015.

G. Nangue Tasse, S. James, and B. Rosman. A Boolean task algebra for reinforcement learning.
_Advances in Neural Information Processing Systems, 33, 2020._

X. Peng, M. Chang, G. Zhang, P. Abbeel, and S. Levine. MCP: Learning composable hierarchical
control with multiplicative compositional policies. In Advances in Neural Information Processing
_Systems, pp. 3686–3697, 2019._

A. Saxe, A. Earle, and B. Rosman. Hierarchy through composition with multitask LMDPs. Interna_tional Conference on Machine Learning, pp. 3017–3026, 2017._

D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,
M. Lai, A. Bolton, et al. Mastering the game of Go without human knowledge. Nature, 550(7676):
354, 2017.

A. Singh, H. Liu, G. Zhou, A. Yu, N. Rhinehart, and S. Levine. Parrot: Data-driven behavioral priors
for reinforcement learning. In International Conference on Learning Representations, 2021.

N. Subrahmanyam. Boolean vector spaces. Mathematische Zeitschrift, 83(5):422–433, 1964.

R. Sutton, A. Barto, et al. Introduction to reinforcement learning, volume 135. MIT press Cambridge,
1998.

R. Sutton, D. Precup, and S. Singh. Between MDPs and semi-MDPs: A framework for temporal
abstraction in reinforcement learning. Artificial Intelligence, 112(1-2):181–211, 1999.

M. Taylor and P. Stone. Transfer learning for reinforcement learning domains: a survey. Journal of
_Machine Learning Research, 10:1633–1685, 2009._

S. Thrun. Is learning the n-th thing any easier than learning the first? In Advances in neural
_information processing systems, pp. 640–646, 1996._

E. Todorov. Compositionality of optimal control laws. In Advances in Neural Information Processing
_Systems, pp. 1856–1864, 2009._

B. van Niekerk, S. James, A. Earle, and B. Rosman. Composing value functions in reinforcement
learning. In International Conference on Machine Learning, pp. 6401–6409, 2019.

C. Watkins. Learning from delayed rewards. PhD thesis, King’s College, Cambridge, 1989.


-----

A PROOFS OF THEORETICAL RESULTS

A.1 BOOLEAN ALGEBRA DEFINITION

**Definition 6. A Boolean algebra is a set B equipped with the binary operators ∨** _(disjunction) and ∧_
_(conjunction), and the unary operator ¬ (negation), which satisfies the following Boolean algebra_
_axioms for a, b, c in B:_

_(i) Idempotence: a ∧_ _a = a ∨_ _a = a._

_(ii) Commutativity: a ∧_ _b = b ∧_ _a and a ∨_ _b = b ∨_ _a._

_(iii) Associativity: a ∧_ (b ∧ _c) = (a ∧_ _b) ∧_ _c and a ∧_ (b ∨ _c) = (a ∨_ _b) ∨_ _c._

_(iv) Absorption: a ∧_ (a ∨ _b) = a ∨_ (a ∧ _b) = a._

_(v) Distributivity: a ∧_ (b ∨ _c) = (a ∧_ _b) ∨_ (a ∧ _c) and a ∨_ (b ∧ _c) = (a ∨_ _b) ∧_ (a ∨ _c)._

_(vi) Identity: there exists 0, 1 in B such that_

**0 ∧** _a = 0_
**0 ∨** _a = a_
**1 ∧** _a = a_
**1 ∨** _a = 1_

_(vii) Complements: for every a in B, there exists an element a[′]_ _in B such that a ∧_ _a[′]_ = 0 and
_a ∨_ _a[′]_ = 1.

A.2 PROOFS FOR PROPOSITION 2

**Lemma 1. Let M be a set of tasks. Then (M, ∨, ∧, ¬, MMAX** _, MMIN_ ) is a Boolean algebra.

_Proof. Let M1, M2_ . We show that _,_ _,_ satisfy the Boolean properties (i) – (vii).
_∈M_ _¬_ _∨_ _∧_

**(i)–(v): These easily follow from the fact that the min and max functions satisfy the idempotent,**
commutative, associative, absorption and distributive laws.

**(vi):Then for all Let rMMAX (s, a∧M1) and in S × A rM1 be the reward functions for,** _MMAX ∧_ _M1 and M1 respectively._

_rMMAX_ _∧M1_ (s, a) = minmin{rr0MAX(s, a, r)M, r10((s, as, a))},, ifotherwise. s ∈G
 _{_ _}_

= _rrM0(1s, a(s, a),),_ ifotherwise. s ∈G (rM1 (s, a) ∈{rMIN, rMAX} for s ∈G)


= rM1 (s, a).


Thusand M MMINMAX ∨ ∧MM1 =1 = M M1 . Hence1. Similarly MMIN MMAX and ∨ MMMAX1 = are the universal bounds of MMAX, MMIN ∧ _M1 = M MMIN._,

**(vii): Let rM1∧¬M1 be the reward function for M1 ∧¬M1. Then for all (s, a) in S × A,**

min _rM1_ (s, a), (rMAX + rMIN) _rM1_ (s, a) _,_ if s
_rM1∧¬M1_ (s, a) = min{r0(s, a), (r0(s, a) + r0(s, a −)) _r0(s, a})_ _,_ otherwise. ∈G
 _{_ _−_ _}_

_rMIN,_ if s and rM1 (s, a) = rMAX
_∈G_

= _rMAX,_ if s and rM1 (s, a) = rMIN

 _∈G_
r0(s, a), otherwise.

= rMMIN (s, a).

Thus M1 ∧¬M1 = MMIN, and similarly M1 ∨¬M1 = MMAX .


-----

**Lemma 2. Let** _Q¯[∗]_ _be the set of optimal_ _Q¯-value functions for tasks in M._ _Then_
( Q[¯][∗], ∨, ∧, ¬, _Q[¯][∗]MAX_ _[,][ ¯]Q[∗]MIN_ [)][ is a Boolean Algebra.]

_Proof. Let_ _Q[¯][∗]M1_ _[,][ ¯]Q[∗]M2_ be the optimal _Q[¯]-value functions for tasks M1, M2_ with reward
functions rM1 and rM2 . We show that[∈] _Q[¯][∗]_ _¬, ∨, ∧_ satisfy the Boolean properties (i) – (vii). ∈M

**(i)–(v): These follow directly from the properties of the min and max functions.**

**(vi): For all (s, g, a) in S × G × A,**

( Q[¯][∗]MAX _[∧]_ _Q[¯][∗]M1_ [)(][s, g, a][) = min][{][ ¯]Q[∗]MAX [(][s, g, a][)][,][ ¯]Q[∗]M1 [(][s, g, a][)][}]

min ¯Q[∗]MAX [(][s, g, a][)][,][ ¯]Q[∗]MAX [(][s, g, a][)][}][,] if rM1 (g, a[′]) = rMAX _a[′]_
= _{_ _∀_ _∈A_
min _Q[¯][∗]MAX_ [(][s, g, a][)][,][ ¯]Q[∗]MIN [(][s, g, a][)][}][,] otherwise.
 _{_

¯Q[∗]MAX [(][s, g, a][)][,] if rM1 (g, a) = rMAX _a[′]_
= _Q¯[∗]MIN_ [(][s, g, a][)][,] otherwise. _∀_ _∈A_


= Q[¯][∗]M1 [(][s, g, a][)] (since rM1 (g, a[′]) ∈{rMIN, rMAX} ∀a[′] _∈A)._


Similarly, _Q[¯][∗]MAX_ _[∨]_ _Q[¯][∗]M1_ [= ¯]Q[∗]MAX _[,][ ¯]Q[∗]MIN_ _[∧]_ _Q[¯][∗]M1_ [= ¯]Q[∗]MIN [, and][ ¯]Q[∗]MIN _[∨]_ _Q[¯][∗]M1_ [= ¯]Q[∗]M1 [.]

**(vii): For all (.) in S × G × A,**

( Q[¯][∗]M1 _[∧¬][ ¯]Q[∗]M1_ [)(][.][) = min][{]Q[ ¯][∗]M1 [(][.][)][,][ ¬][ ¯]Q[∗]M1 [(][.][)][}]

min _Q ¯[∗]MIN_ [(][.][)][,][ ¯]Q[∗]MAX [(][.][)][}] if _Q[¯][∗](.)_ _Q[∗]MIN_ [(][.][)][| ≤|]Q[ ¯][∗](.) _Q[∗]MAX_ [(][.][)][|]
= _{_ _|_ _−_ [¯] _−_ [¯]
min _Q[¯][∗]MAX_ [(][.][)][,][ ¯]Q[∗]MIN [(][.][)][}] otherwise,
 _{_

= Q[¯][∗]MIN [(][.][)][.]


Similarly, _Q[¯][∗]M1_ _[∨¬][ ¯]Q[∗]M1_ [= ¯]Q[∗]MAX [.]

**Lemma 3.for all M1 Let, M2** _Q[¯][∗]_ _be the set of optimal extended, we have (i)_ _Q[¯][∗]_ _M1_ = _Q[¯]Q[¯][∗]M-value functions for tasks in1[, (ii)][ ¯]Q[∗]M1_ _M2_ = _Q¯[∗]M1_ _MQ[∗]M._ 2[, and]Then

_(iii)_ _Q[¯][∗]M1_ _M2_ [= ¯] ∈MQ[∗]M1 _Q[∗]M2_ _[.]_ _¬_ _¬_ _∨_ _[∨]_ [¯]
_∧_

_[∧]_ [¯]

_Proof. Let M1, M2_ . Then for all (s, g, a) in,
_∈M_ _S × G × A_


**(i):**


_Q¯[∗]_ _M1_ [(][s, g, a][)]
_¬_

¯Q[∗]MAX [(][s, g, a][)][,] if r _M1_ (g, a[′]) = rMAX _a[′]_
= _Q¯[∗]MIN_ [(][s, g, a][)][,] otherwise.¬ _∀_ _∈A_


¯Q[∗]MAX [(][s, g, a][)][,] if rM1 (g, a[′]) = rMIN _a[′]_
= _Q¯[∗]MIN_ [(][s, g, a][)][,] otherwise. _∀_ _∈A_


¯Q[∗]MAX [(][s, g, a][)][,] if _Q[¯][∗]M1_ [(][s, g, a][) = ¯]Q[∗]MIN [(][s, g, a][)]
=
_Q¯[∗]MIN_ [(][s, g, a][)][,] otherwise.


¯Q[∗]MAX [(][s, g, a][)][,] if _Q[¯][∗]M1_ [(][s, g, a][)][ −] _Q[¯][∗]MIN_ [(][s, g, a][)][| ≤|]Q[ ¯][∗]M1 [(][s, g, a][)][ −] _Q[¯][∗]MAX_ [(][s, g, a][)][|]
= _|_
_Q¯[∗]MIN_ [(][s, g, a][)][,] otherwise.


= ¬Q[¯][∗]M1 [(][s, g, a][)][.]


-----

**(ii):**

_Q¯[∗]M1∨M2_ [(][s, g, a][) =] ¯QQ¯[∗]MIN[∗]MAX[(][(][s, g, a][s, g, a][)][)][,][,] otherwise.if rM1∨M2 (g, a[′]) = rMAX ∀a[′] _∈A_


¯Q[∗]MAX [(][s, g, a][)][,] if max _rM1_ (g, a[′]), rM2 (g, a[′]) = rMAX _a[′]_
= _Q¯[∗]MIN_ [(][s, g, a][)][,] otherwise.{ _}_ _∀_ _∈A_


¯Q[∗]MAX [(][s, g, a][)][,] if max _Q[¯][∗]M1_ [(][s, g, a][)][,][ ¯]Q[∗]M2 [(][s, g, a][)][}][ = ¯]Q[∗]MAX [(][s, g, a][)]
= _{_
_Q¯[∗]MIN_ [(][s, g, a][)][,] otherwise.


= max{Q[¯][∗]M1 [(][s, g, a][)][,][ ¯]Q[∗]M2 [(][s, g, a][)][}]

= ( Q[¯][∗]M1 _[∨]_ _Q[¯][∗]M2_ [)(][s, g, a][)][.]

**(iii): Follows similarly to (ii).**

**Proposition 2. Let** _Q[¯][∗]_ _be the set of optimal_ _Q[¯]-value functions for tasks in M. Let A : M →_ _Q[¯][∗]_
_be any map from M to_ _Q[¯][∗]_ _such that A (M_ ) = Q[¯][∗]M _[for all][ M][ in][ M][. Then,]_

_(i)_ _and_ [¯] _respectively form a Boolean task algebra (_ _,_ _,_ _,_ _,_ _MAX_ _,_ _MIN_ ) and
_M_ _Q[∗]_ _M_ _∨_ _∧_ _¬_ _M_ _M_
_a Boolean extended value functions algebra ( Q[¯][∗], ∨, ∧, ¬,_ _Q[¯][∗]MAX_ _[,][ ¯]Q[∗]MIN_ [)][,]

_(ii) A is a homomorphism between M and_ _Q[¯][∗]._

_Proof. (i): Follows from Lemma 1 and 2._

**(ii): Follows from Lemma 3.**

A.3 PROOFS FOR THEOREM 1

**Lemma 4. Let** _Q[¯][∗]_ _be the set of optimal_ _Q[¯]-value functions for tasks in M. Denote_ _Q¯[˜][∗]M_ _[as the]_
_ϵ-optimal_ _Q[¯]-value function for a task M ∈M such that_

_|Q[¯][∗]M_ [(][s, g, a][)][ −] _Q¯[˜][∗]M_ [(][s, g, a][)][| ≤] _[ϵ][ for all][ (][s, g, a][)][ ∈S × G × A][.]_

_Then for all M1, M2 in M and (s, g, a) in S × G × A,_

_(i)_ [ Q[¯][∗]M1 _[∨]_ _Q[¯][∗]M2_ [](][s, g, a][)][ −] [[ ˜]Q¯[∗]M1 _[∨]_ _Q¯[˜][∗]M2_ [](][s, g, a][)] _≤_ _ϵ_

_(ii)_ [ Q[¯][∗]M1 _[∧]_ _Q[¯][∗]M2_ [](][s, g, a][)][ −] [[ ˜]Q¯[∗]M1 _[∧]_ _Q¯[˜][∗]M2_ [](][s, g, a][)] _≤_ _ϵ_

_(iii)_ _¬Q[¯][∗]M1_ [(][s, g, a][)][ −¬][ ˜]Q¯[∗]M1 [(][s, g, a][)] _≤_ _ϵ_

_Proof._ **(i):**

[ Q[¯][∗]M1 _[∨]_ _Q[¯][∗]M2_ [](][s, g, a][)][ −] [[ ˜]Q¯[∗]M1 _[∨]_ _Q¯[˜][∗]M2_ [](][s, g, a][)]

= max _Q¯[∗]M_ [(][s, g, a][)][ −] max _Q˜¯[∗]M_ [(][s, g, a][)]
_M_ _∈{M1,M2}_ _M_ _∈{M1,M2}_

max _Q[¯][∗]M_ [(][s, g, a][)][ −] _Q¯[˜][∗]M_ [(][s, g, a][)]
_≤_ _M_ _M1,M2_
_∈{_ _}_

_≤_ _ϵ._


-----

**(ii):**

[ Q[¯][∗]M1 _[∧]_ _Q[¯][∗]M2_ [](][s, g, a][)][ −] [[ ˜]Q¯[∗]M1 _[∧]_ _Q¯[˜][∗]M2_ [](][s, g, a][)]

= min _Q¯[∗]M_ [(][s, g, a][)][ −] min _Q˜¯[∗]M_ [(][s, g, a][)]
_M_ _∈{M1,M2}_ _M_ _∈{M1,M2}_

min _Q[¯][∗]M_ [(][s, g, a][)][ −] _Q¯[˜][∗]M_ [(][s, g, a][)]
_≤_ _M_ _M1,M2_
_∈{_ _}_

_≤_ _ϵ._

**(iii):**
_¬Q[¯][∗]M1_ [(][s, g, a][)][ −¬][ ˜]Q¯[∗]M1 [(][s, g, a][)]

= _|Q[¯][∗]MAX_ [(][s, g, a][)][ −¬][ ˜]Q¯[∗]MIN [(][s, g, a][)][|][,] if _Q[¯][∗]M1_ [= ¯]Q[∗]MIN [(][s, g, a][)]

(|Q[¯][∗]MIN [(][s, g, a][)][ −¬][ ˜]Q¯[∗]MAX [(][s, g, a][)][|][,] otherwise.

= _|Q[¯][∗]MAX_ [(][s, g, a][)][ −] _Q¯[˜][∗]MAX_ [(][s, g, a][)][|][,] if _Q[¯][∗]M1_ [= ¯]Q[∗]MIN [(][s, g, a][)]

(|Q[¯][∗]MIN [(][s, g, a][)][ −] _Q¯[˜][∗]MIN_ [(][s, g, a][)][|][,] otherwise.

_≤_ _ϵ._


**Lemma 5. Let M ∈M be a task with reward function r, binary representation T and optimal**
_extended action-value function_ _Q[¯][∗]. Given ϵ-approximations of the binary representations_ _T[˜]n =_
_T[˜]1, ...,_ _T[˜]n_ _and optimal_ _Q[¯]-functions_ ¯[˜]n [=][ {][ ˜]Q¯[∗]1[, ...,][ ˜]Q¯[∗]n[}][ for n tasks][ ˆ] = _M1, ..., Mn_ _, let_
_{_ _}_ _Q[∗]_ _M_ _{_ _} ⊆M_

_T_ = _EXP ( [˜]n) and_ _Q[¯]_ = _EXP ( ¯[˜]n[)][,]_
_F_ _B_ _T_ _F_ _B_ _Q[∗]_

_where BEXP is derived from_ _T˜n and_ _T˜ using a generic method F._ _Define π(s)_ _∈_
arg maxa _Q_ _where Q_ := maxg _Q[¯]_ (s, g, a). Then,
_∈A_ _F_ _F_ _∈G_ _F_

_∥Q[¯][∗]_ _−_ _Q[¯]F_ _∥∞_ _≤_ (1T ̸=TF )r∆ + ϵ,

_whereh(s, g, a 1 is the indicator function,)|._ _r∆_ := rMAX − _rMIN, and ∥f −_ _h∥∞_ := maxs,g,a |f (s, g, a) −

_Proof._

_Q[¯][∗](s, g, a)_ _Q_ (s, g, a) = _Q[¯][∗](s, g, a)_ _Q[∗]_ _Q[∗]_ _Q_ (s, g, a)
_|_ _−_ [¯]F _|_ _|_ _−_ [¯]F [(][s, g, a][) + ¯]F [(][s, g, a][)][ −] [¯]F _|_

_Q[¯][∗](s, g, a)_ _Q[∗]_ _Q[∗]_ _Q_ (s, g, a)
_≤|_ _−_ [¯]F [(][s, g, a][)][|][ +][ |][ ¯]F [(][s, g, a][)][ −] [¯]F _|_

_Q[¯][∗](s, g, a)_ _Q[∗]_ (Using Lemma 4)
_≤|_ _−_ [¯]F [(][s, g, a][)][|][ +][ ϵ.]


If T = T, then _Q[¯][∗](s, g, a) = Q[¯][∗]_
_F_ _F_ [(][s, g, a][)][, and we are done. Let][ T][ ̸][=][ T][F] [. Without loss of generality,]
let _Q[¯][∗](s, g, a) = Q[¯][∗]MAX_ [(][s, g, a][)][ and][ ¯]Q[∗] _Q[∗]MIN_ [(][s, g, a][)][. Then,]
_F_ [(][s, g, a][) = ¯]

_Q[¯][∗](s, g, a)_ _Q[∗]_ _Q[∗]MAX_ [(][s, g, a][)][ −] _Q[¯][∗]MIN_ [(][s, g, a][)][|]
_|_ _−_ [¯]F [(][s, g, a][)][| ≤|][ ¯]
_r∆._
_≤_

**Lemma 6. Let Q[∗]** _and_ _Q[¯][∗]_ _be the optimal Q-value function and optimal extended Q-value function_
_respectively for a deterministic task in M. Then for all (s, a) in S × A, we have_

_Q[∗](s, a) = max_ _Q¯[∗](s, g, a)._
_g∈G_


-----

_Proof. We first note that_

max _r(s, g, a) =_
_g_ [¯]
_∈G_


max{rMIN, r(s, a)}, if s ∈G (4)
max otherwise. [=][ r][(][s, a][)][.]
_g_ _[r][(][s, a][)][,]_
_∈G_


Now define
_Q¯[∗]max[(][s, a][) := max]_ _Q¯[∗](s, g, a)._
_g∈G_


Then it follows that
_Q[¯][∗]max_ (s, a) = r(s, a) + γ
_T_
 


_p(s[′]_ _s, a) max_ _Q¯[∗]max[(][s][′][, a][′][)]_
_|_ _a[′]_
_sX[′]∈S_ _∈A_


_p(s[′]_ _s, a) max_
_|_ _a[′]_
_sX[′]∈S_ _∈A_

_p(s[′]_ _s, a) max_
_|_ _g_
_sX[′]∈S_ _∈G_


max _Q¯[∗](s[′], g, a[′])_
_g∈G_

max _Q¯[∗](s[′], g, a[′])_
_a[′]∈A_


= r(s, a) + γ

= r(s, a) + γ


_p(s[′]_ _s, a) max_ _Q¯[∗](s[′], g, a[′])_
_|_ _a[′]_
_sX[′]∈S_ _∈A_


(Since p is deterministic)


= r(s, a) + max
_g∈G_


_p(s[′]_ _s, a) max_ _Q¯[∗](s[′], g, a[′])_
_|_ _a[′]_
_sX[′]∈S_ _∈A_


(Using Equation 4)


= max _r(s, g, a) + max_
_g_ [¯] _g_
_∈G_ _∈G_


= max _r¯(s, g, a) + γ_ _p(s[′]_ _s, a) max_ _Q¯[∗](s[′], g, a[′])_ _,_
_g∈G_ " _sX[′]∈S_ _|_ _a[′]∈A_ #

since ¯r(s, g, a) = r0(s, a) _s /_ and p(s, a, ω) = 1 with _Q[¯][∗](ω, g, a[′]) = 0_ _s_ _._
_∀_ _∈G_ _∀_ _∈G_

= max _Q¯[∗](s, g, a)_
_g∈G_

= Q[¯][∗]max[(][s, a][)][.]


Hence _Q[¯][∗]max_ [is a fixed point of the Bellman optimality operator.]


If s ∈G, then


_Q¯[∗]max[(][s, a][) = max]_ _r(s, g, a) = r(s, a) = Q[∗](s, a)._
_g_ _[Q][∗][(][s, g, a][) = max]g_ [¯]
_∈G_ _∈G_


Since _Q[¯][∗]max_ [=][ Q][∗] [holds in][ G][ and][ ¯]Q[∗]max [is a fixed point of the Bellman operator, then][ ¯]Q[∗]max [=][ Q][∗]
holds everywhere.

**Theorem 1. Let M ∈M be a task with reward function r, binary representation T and optimal**
_extended action-value function_ _Q[¯][∗]. Given ϵ-approximations of the binary representations_ _T[˜]n =_
_T[˜]1, ...,_ _T[˜]n_ _and optimal_ _Q[¯]-functions_ ¯[˜]n [=][ {][ ˜]Q¯[∗]1[, ...,][ ˜]Q¯[∗]n[}][ for n tasks][ ˆ] = _M1, ..., Mn_ _, let_
_{_ _}_ _Q[∗]_ _M_ _{_ _} ⊆M_

_T_ = _EXP ( [˜]n) and_ _Q[¯]_ = _EXP ( ¯[˜]n[)][,]_
_F_ _B_ _T_ _F_ _B_ _Q[∗]_

_where BEXP is derived from_ _T˜n and_ _T˜ using a generic method F._ _Define π(s)_ _∈_
arg maxa _Q_ _where Q_ := maxg _Q[¯]_ (s, g, a). Then,
_∈A_ _F_ _F_ _∈G_ _F_


2

1−γ [((][1][T][ ̸][=][T][F] [+][ 1][r /]∈{rg}|G| [)][r]∆ [+][ ϵ][)][,]


_(i)_ _Q[∗]_ _Q[π]_
_∥_ _−_ _∥∞_ _≤_


_(ii) if the dynamics are deterministic,_

_∥Q[∗]_ _−_ _QF_ _∥∞_ _≤_ (1T ̸=TF )r∆ + ϵ,


-----

maxwheres,g,a 1 is the indicator function,f (s, g, a) _h(s, g, a)_ _._ _rg(s, a) := ¯r(s, g, a), r∆_ := rMAX − _rMIN, and ∥f −_ _h∥∞_ :=
_|_ _−_ _|_

_Proof. (i): We first note that each g in G can be thought of as defining an MDP Mg :=_
(S, A, p, rg, γ) with reward function rg(s, a) := ¯r(s, g, a), optimal policy πg[∗][(][s][) = ¯]π[∗](s, g)
and optimal Q-value function Q[π]g[∗] (s, a) = ¯Q[∗](s, g, a). Then this proof follows similarly to that
of Barreto et al. (2017) Theorem 2,

_Q[∗](s, a) −_ _Q[π](s, a)_

2
_Q[∗](s, a)_ _Q[π]g[∗]_ (s, a) + (Barreto et al. (2017) Theorem 1)
_≤_ _−_ 1 _γ_ [((][1][T][ ̸][=][T][F][ )][r][∆] [+][ ϵ][)]

_−_

2 2

(Barreto et al. (2017) Lemma 1)

_≤_ 1 − _γ_ [max]s,a _[|][r][(][s, a][)][ −]_ _[r][g][(][s, a][)][|][ +]_ 1 − _γ_ [((][1][T][ ̸][=][T][F][ )][r][∆] [+][ ϵ][)]

2 2
_≤_ 1 _γ_ [(][1][r][̸][=][r][g] [)][r][∆] [+] 1 _γ_ [((][1][T][ ̸][=][T][F][ )][r][∆] [+][ ϵ][)]

_−_ _−_

(Since rewards only differ in G where r(s, a), rg(s, a) ∈{rMIN, rMAX} for s ∈G)

2
_≤_ 1 _γ_ [((][1][T][ ̸][=][T][F][ +][ 1][r][̸][=][r][g] [)][r][∆] [+][ ϵ][)][.]

_−_

Hence,


2
_∥Q[∗]_ _−_ _Q[π]∥∞_ _≤_ 1 _γ_ [((][1][T][ ̸][=][T][F][ + min]g **1r≠** _rg_ )r∆ + ϵ)

_−_

2
_≤_ 1 _γ_ [((][1][T][ ̸][=][T][F][ +][ 1][r /]∈{rg}|G| [)][r]∆ [+][ ϵ][)]

_−_

(Since ming **1r≠** _rg = 0 only when r ∈{rg}|G| )._

_Q[∗](s, a)_ _Q_ (s, a) = max _Q¯[∗](s, g, a)_ max _Q¯_ (s, g, a) (Lemma 6)
_|_ _−_ _F_ _|_ _|_ _g_ _−_ _g_ _F_ _|_

max _Q[¯][∗](s, g, a)_ _Q_ (s, g, a)
_≤_ _g_ _|_ _−_ [¯]F _|_

_≤_ (1T ̸=TF )r∆ + ϵ. (Lemma 5)


**(ii):**


A.4 COMPARING THE BOUNDS OF THEOREM 1 WITH THAT OF GPI IN BARRETO ET AL.
(2018)

We first restate Proposition 1 (Barreto et al., 2018) here.

**Proposition 3 ((Barreto et al., 2018)). Let M** _and let Qπi_ _j[∗]_ _be the action value function of an_
_∈M_
_optimal policy of Mj ∈M when executed in Mi ∈M. Given approximations {Q[˜][π]i_ [1] _[, ...,][ ˜]Q[π]i_ _[n]_ _[}][ such]_
_that_ _Q[π]i_ _[j]_ _Q[π]i_ _[j]_
_|_ _−_ [˜] _[| ≤]_ _[ϵ][ for all][ s, a][ ∈S × A][, and][ j][ ∈{][1][, ..., n][}][, let]_

_π(s) ∈_ arg maxa maxj _Q˜[π]i_ _[j]_ [(][s, a][)][.]

_then,_

2
_Q[∗]_ _Q[π]_ _ri_ _rj_ + ϵ),
_∥_ _−_ _∥∞_ _≤_ 1 _γ_ [(][∥][r][ −] _[r][i][∥][∞]_ [+ min]j _∥_ _−_ _∥∞_

_−_

_where Q[∗]_ _is the optimal value function of M_ _, Q[π]_ _is the value function of π in M_ _, and_ _f_ _h_ :=
_∥_ _−_ _∥∞_
maxs,g,a _f_ (s, g, a) _h(s, g, a)_ _._
_|_ _−_ _|_


-----

We can simplify the bound in Proposition 3 as follows:


2
_Q[∗]_ _Q[π]_ _ri_ _rj_ + ϵ)
_∥_ _−_ _∥∞_ _≤_ 1 _γ_ [(][∥][r][ −] _[r][i][∥][∞]_ [+ min]j _∥_ _−_ _∥∞_

_−_

2

_ri_ _rj_ + ϵ)

_≤_ 1 _γ_ [((][1][r][̸][=][r][i] [)][r][∆] [+ min]j _∥_ _−_ _∥∞_

_−_

(Since rewards only differ in G where r(s, a), ri(s, a) ∈{rMIN, rMAX} for s ∈G)

2
_≤_ 1 _γ_ [((][1][r][̸][=][r][i] [)][r][∆] [+ (min]j **1ri≠** _rj_ )r∆ + ϵ)

_−_

2

_rj_ _n_ [)][r]∆ [+][ ϵ][)]

_≤_ 1 _γ_ [((][1][r][̸][=][r][i] [)][r][∆] [+ (][1][r][i][ /]∈{ _}_

_−_

(Since min **1ri=rj = 0 only when ri** _rj_ _n )_
_j_ _̸_ _∈{_ _}_


2

_rj_ _n_ [)][r]∆ [+][ ϵ][)][.]

_≤_ 1 _γ_ [((][1][r][̸][=][r][i][ +][ 1][r][i][ /]∈{ _}_

_−_

wherethat of Theorem 1(i) but weaker. This because: 1 is the indicator function, and r∆ := rMAX − _rMIN. We can see that this bound is similar to_

(i) The first term of this bound (1r≠ _ri_ ) requires that reward function of the current task (r) be
identical to that of a reference task (ri). In Barreto et al. (2018), ri is taken as the best linear
approximation of r. In contrast, the first term of Theorem 1(i) (1T ̸=TF ) only requires the
current task to be expressible as a Boolean composition of past tasks.

(ii) The second term of this bound (1ri /∈{rj _}n[) requires that the reference task (the best linear]_
approximation to the current task) is exactly one of the past tasks. In contrast, the second
term of Theorem 1(i) (1r /∈{rg}|G|[) only requires the current task to have a single desirable]
goal.

This suggests that we can can think of the logical composition approach as an efficient way of doing
GPI, one which leads to tight performance bounds on the transferred policy (Theorem 1(ii)).

A.5 PROOFS FOR THEOREM 2

**Theorem 2. Let D be an unknown distribution, possibly non-stationary, over a set of tasks**
( _,_ _, p, γ, r0). Let A_ : _be any map from_ _to_ [¯] _such that A (M_ ) = Q[¯][∗]M
_M_ _S_ _A_ _M →_ _Q[¯][∗]_ _M_ _Q[∗]_
_for all M in M. Let_

_T˜t+1,_ _Q¯[˜]t[∗]+1_ [=][ SOPGOL][(][A][, M][t][,][ ˜]Tt, _Q¯[˜]t[∗][)][ where][ M][t]_ _[∼D][(][t][)][ and][ ˜]T0 = Q¯[˜]0[∗]_ [=][ ∅] _[∀][t][ ∈]_ [N][.]

_Then,_
log lim _where Nt :=_ [˜]t = ¯[˜]t
_⌈_ _|G|⌉≤_ _t→∞_ _[N][t][ ≤|G|]_ _|T_ _|_ _|Q[∗][|][.]_

_Proof. Let_ _T[˜]t be the approximate binary representation of task Mt learned by SOPGOL. We first_
note that SOPGOL returns _T[˜]t ∪{T[˜]t} only if_ _T[˜]t is not in the span of_ _T[˜]t. That is,_

_T˜t+1 = T[˜]t ∪{T[˜]t} iff_ _T[˜]t ̸= BEXP ( T[˜]t) where BEXP = SOP_ ( T[˜]t, _T[˜]t)._

Hence, it is sufficient to show that the number, N, of linearly independent binary vectors, _T[˜] ∈_
_{0, 1}[|G|], that span the Boolean vector space (Subrahmanyam, 1964), GF_ (2)[|G|],[5] is bounded by

_⌈log |G|⌉≤_ _N ≤|G|._

This follows from the fact that ⌈log |G|⌉ is the size of a minimal set of generators for GF (2)[|G|] (as
can easily be seen with a Boolean table), and |G| is its dimensionality.

5GF(2) is the Galois field with two elements, ({0, 1}, +, .), where + := XOR and . := AND.


-----

B SUM OF PRODUCTS WITH GOAL ORIENTED LEARNING

Algorithm 1 shows the full pseudo-code for SOPGOL. Here, SOP ( T[˜], _T[˜]) is the classical sum_
of products method in Boolean logic. Given a list of binary vectors = [ T[˜]1, ..., _T[˜]n], a Boolean_
_T_
expression for a new binary vector _T[˜] is obtained as follows:_

1. Identify all rows of _T[˜] with a 1._

2. For each such row: Make a product (conjunction) of all the input variables and make the
negation of each variable with a 0 in this row.

3. Take the sum (disjunction) of all these product terms.

The output of SOP is a function BEXP that takes in |T |[˜] variables, and applies disjunctions, conjunctions and negations to them according to the Boolean expression obtained above.

**Algorithm 1: SOPGOL**
**Input :off-policy RL algorithm A,** /* e.g DQN */
task MDP M,
set of ϵ-optimal task binary representations _T[˜],_
set of ϵ-optimal _Q[¯]-value functions_ _Q¯[˜]._
Initialise _T[˜] : G →{0, 1}_
Initialise _Q¯[˜] : S × G × A →_ R according to A
Initialise goal buffer _G[˜] with terminal states observed from a random policy_
**while** _Q¯[˜] is not converged do_
Initialise state s from M
_EXP_ _SOP_ ( [˜], _T[˜])_
_B_ _←_ _T_
_TSOP,_ _Q[¯]SOP_ _EXP ( [˜] ),_ _EXP ( ¯[˜]_ )
_←B_ _T_ _B_ _Q[∗]_
_Q¯ ←_ _Q[¯]SOP if_ _T[˜] = TSOP else_ _Q¯[˜] ∨_ _Q[¯]SOP_

_g_ arg max max _Q¯(s, g[′], a)_
_←_ _g[′]∈G[˜]_  _a∈A_ 

**while s is not terminal do**
Select action a using the behaviour policy from A : a ← _π¯(s, g) /* e.g ϵ-greedy_
*[/]
Take action a, observe reward r and next state s[′] in M
**if** _T[˜] ̸= TSOP then_
**foreach g[′]** _∈_ _G[˜] do_
_r¯ ←_ _rMIN if g[′]_ ≠ _s ∈_ _G[˜] else r_
Update _Q¯[˜] with (s, g[′], a, ¯r, s[′]) according to A_
**end**
**if s is terminal then**
_T˜(s)_ **1r=rMAX**
_←_
_G ←˜_ _G ∪{[˜]_ _s}_
**else**
_s ←_ _s[′]_
**end**
**end**
**end**
_EXP_ _SOP_ ( [˜], _T[˜])_
_B_ _←_ _T_
_T˜,_ _Q ←¯[˜]_ ( T[˜], _Q¯[˜]) if_ _T[˜] = BEXP ( T[˜] ) else ( T ∪{[˜]_ _T[˜]},_ _Q ∪{¯[˜]_ _Q¯[˜]})_
**return** _T[˜],_ _Q¯[˜]_


-----

C FOUR ROOMS ENVIRONMENT

We use the Four Rooms domain (Sutton et al., 1999), where an agent must navigate in a grid world to
particular locations. The goal locations are placed along the sides of the walls and at the centre of
rooms (Figure 5). This gives a goal space of size |G| = 40 and a task space of size |M| = 2[|G|] _≈_ 10[12].
The agent can move in any of the four cardinal directions at each timestep, but colliding with a wall
leaves the agent in the same location. We add a 5th action for “stay” that the agent chooses to achieve
goals. A goal location only becomes terminal if the agent chooses to stay in it. All rewards are 0 at
non-terminal states, and 1 at the desirable goals. The transition dynamics are stochastic with a slip
probability (sp = 0.1). That is, with probability 1-sp the agent moves in the direction it chooses, and
with probability sp it moves in one of the other three chosen uniformly at random.

Figure 5: 40 goals Four Rooms domain with goals in green and the agent in red.


-----

D FUNCTION APPROXIMATION EXPERIMENT DETAILS

D.1 ENVIRONMENT

The PICKUPOBJ environment is fully observable, where each state observation is a 56 ∗ 56 ∗ 3 RGB
image (Figure 1). The agent has 7 actions it can take in this environment corresponding to: 1 - rotate
left, 2 - rotate right, 3 - move one step forward if there is no wall or object in front, 4 - pickup object
if there is an object in front and no object has been picked, 5 - drop the object in front if an object has
been picked and there is no wall or object in front, 6 - open the door in front if there is a closed-door
in front, and 7 - close the door in front if there is an opened door in front.

For each task, each episode starts with 1 desirable object and 4 other randomly chosen objects placed
randomly in the environment. The agent is also placed at a random position with a random orientation
at the start of each episode. The agent receives a reward of -0.1 at every timestep, and a reward
of 2 when it picks up a desirable object. The environment transitions to a terminal state once the
agent picks up any object and the agent observes the picked object. There are 15 types of objects
(illustrated in Table 1) resulting in 15 possible goal states. Hence, the dimension of the state space is
_|S| = 56 × 56 × 3, the goal space is |G| = 15, and the action space is |A| = 7._

D.2 NETWORK ARCHITECTURE AND HYPERPARAMETERS

In our function approximation experiments, we represent each extended value function _Q¯[˜][∗]_ with a list
of DQNs, such that the value function for each goal _Q[˜][∗]g[(][s, a][) := ˜]Q¯[∗](s, g, a) is approximated with_
_|G|_
a separate DQN. The DQNs used have the following architecture, with the CNN part being identical
to that used by Mnih et al. (2015):

1. Three convolutional layers:

(a) Layer 1 has 3 input channels, 32 output channels, a kernel size of 8 and a stride of 4.

(b) Layer 2 has 32 input channels, 64 output channels, a kernel size of 4 and a stride of 2.

(c) Layer 3 has 64 input channels, 64 output channels, a kernel size of 3 and a stride of 1.

2. Two fully-connected linear layers:

(a) Layer 1 has input size 3136 and output size 512 and uses a ReLU activation function.

(b) Layer 2 has input size 512 and output size 7 with no activation function.

We used the ADAM optimiser with batch size 256 and a learning rate of 10[−][3]. We started training
after 1000 steps of random exploration and updated the target Q-network every 1000 steps. Finally,
we used ϵ-greedy exploration, annealing ϵ from 0.5 to 0.05 over 100000 timesteps.

Finally, we used the same DQN architecture and training hyperparameters for the baseline in all
experiments.


-----

