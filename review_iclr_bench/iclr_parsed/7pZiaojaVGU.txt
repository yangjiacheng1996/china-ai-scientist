# AN EQUIVALENCE BETWEEN DATA POISONING AND BYZANTINE GRADIENT ATTACKS

**Anonymous authors**
Paper under double-blind review

ABSTRACT

To study the resilience of distributed learning, the “Byzantine” literature considers a strong threat model where workers can report arbitrary gradients to the parameter server. While this model helped generate several fundamental results, it
has however sometimes been considered unrealistic, when the workers are mostly
trustworthy machines. In this paper, we show a surprising equivalence between
this model and data poisoning, a threat considered much more realistic. More
specifically, we prove that any gradient attack can be reduced to data poisoning in
a personalized federated learning system that provides PAC guarantees (which we
show are both desirable and realistic in various personalized federated learning
contexts such as linear regression and classification). Maybe most importantly,
we derive a simple and practical attack that may be constructed against classical personalized federated learning models, and we show both theoretically and
empirically the effectiveness of this attack.

1 INTRODUCTION

Learning algorithms now typically leverage data generated by a large number of users (Smith et al.,
2013; Wang et al., 2019a;b) to often learn a common model that fits a large population (Konecn´y
et al., 2015), but also sometimes to construct a personalized model for each individual (Ricci et al.,
2011). Autocompletion (Lehmann & Buschek, 2021), conversational (Shum et al., 2018) and recommendation (Ie et al., 2019) algorithms are examples of such algorithms deployed at scale. To be
effective, besides huge amounts of (distributed) data (Brown et al., 2020; Fedus et al., 2021), these
algorithms require a high level of customization. This has motivated research into personalized
_federated learning (Fallah et al., 2020; Hanzely et al., 2020; Dinh et al., 2020)._

However, in applications such as content recommendation, activists, companies, and politicians have
strong incentives to promote certain views, products or ideologies (Hoang, 2020; Hoang et al., 2021).
Remember for instance that, on YouTube, two views out of three result from algorithmic recommendations (Solsman, 2018). Perhaps unsurprisingly, this has led to vast amounts of fabricated activities
to bias algorithms (Bradshaw & Howard, 2019; Neudert et al., 2019), like “fake reviews” (Wu et al.,
2020). The scale of this phenomenon is well illustrated by the case of Facebook which, in 2019
alone, reported the removal of around 6 billion fake accounts from its platform (Fung & Garcia,
2019). This is particularly concerning in the era of “stochastic parrots” (Bender et al., 2021): climate denialists are incentivized to pollute textual datasets with claims like “climate change is a
hoax”, as autocompletion, conversational and recommendation algorithms trained on such data will
more likely spread these views (McGuffie & Newhouse, 2020). This raises serious concerns about
the vulnerability of personalized federated learning to such misleading data. Data poisoning attacks
clearly constitute now a major machine learning security issue (Kumar et al., 2020).

Overall, in highly adversarial environments like social media, given the advent of deep fakes (Johnson & Diakopoulos, 2021), we should expect that most data are strategically crafted and labeled. In
this context, the authentication of the data provider seems critical. In particular, the safety of learning algorithms arguably demands that they be trained solely on cryptographically signed data, that
is, data that provably come from a known source. But even signed data cannot be wholeheartedly
trusted since users usually have preferences over what ought to be recommended to others. They
thus have incentives to behave strategically in order to promote certain views or products.


-----

To address data poisoning, the Byzantine learning literature usually considers that each federated
learning worker may behave arbitrarily (Blanchard et al., 2017; Mhamdi et al., 2018; El-Mhamdi
et al., 2021). Recall that at each iteration of the federated learning stochastic gradient descent, each
worker is given the updated model, and is asked to compute the gradient of the loss function with
respect to (a batch of) its local data. Byzantine learning usually assumes that a malicious (Byzantine)
worker may report any gradient; without having to justify whether such a gradient could have been
generated through data poisoning. In fact, the gradient attack threat model has sometimes been
claimed to be unrealistic in practical federated learning (Shejwalkar et al., 2021), especially when
the workers are machines owned by trusted entities (cross-silo FL (Kairouz et al., 2021)).

We prove in this paper an equivalence between gradients attacks and fabricated data injection, in a
general and desirable collaborative learning framework. Thereby, our paper provides the first practically compelling argument for the necessity to protect federated learning against gradient attacks.

**Contributions.** As a preamble of our main result, we formalize local PAC* learning[1] (Valiant,
1984) for personalized learning, and prove that a simple and general solution to personalized federated linear regression and classification is indeed locally PAC* learning. Our proof leverages a
new concept called gradient-PAC* learning, which is of independent interest. We prove that it is
sufficient to guarantee local PAC* learning, and that it is verified by basic learning algorithms, like
linear and logistic regression. This is an important and highly nontrivial contribution of this paper.

Our main contribution is then to prove that local PAC* learning in personalized federated learning
essentially implies an equivalence between data poisoning and gradient attacks. More precisely, we
show how any (converging) gradient attack can be turned into a data poisoning attack, with the same
resulting harm. Given how easy it generally is to create fake accounts on web platforms and to inject
data poisoning by generating fake activities, this result should arguably greatly increase the concerns
over the vulnerabilities of federated learning with user-generated data.

Finally, we propose a simple but very general strategic gradient attack, called the counter-gradient
_attack (CGA), which any participant to federated learning can deploy to bias the global model_
towards any target model that better suits their interest. We prove the effectiveness of this attack
under fairly general assumptions, which apply to many proposed personalized learning frameworks
including Hanzely et al. (2020); Dinh et al. (2020). We then show empirically how this gradient
attack can be turned into a devastating data poisoning attack, with remarkably few data.

**Related work.** Byzantine learning has provided both negative and positive results in Byzantine
resilience (Blanchard et al., 2017; Mhamdi et al., 2018; Baruch et al., 2019; Xie et al., 2019), some
of which apply almost straightforwardly to personalized federated learning (El-Mhamdi et al., 2020;
2021). Such results study the resilience against a minority of adversarial users. Our paper however focuses on a different kind of malicious users. Namely, like Suya et al. (2021), we study the
resilience against strategic users, who aim to bias the learned models towards a specific target model.

The study of the resilience against strategic users is part of the research on strategyproof learning.
Many special cases of strategyproofness have been tackled, including regression (Chen et al., 2018b;
Dekel et al., 2010; Perote & Perote-Pe˜na, 2004; Ben-Porat & Tennenholtz, 2017), classification
(Meir et al., 2012; Chen et al., 2020; Meir et al., 2011; Hardt et al., 2016), statistical estimation
(Cai et al., 2015), and clustering (Perote & Sevilla, 2003). However, none of these papers tackles a
general personalized federated learning scheme. Typically, for linear regression, Chen et al. (2018b)
and Perote & Perote-Pe˜na (2004) assume that each user can only provide a single data point. This
greatly restricts the users’ ability to contribute to the learning model. And while Dekel et al. (2010)
allows multiple contributions, they either require payments, which might not be possible (e.g., due to
ethical reasons), or they restrict the model to one dimension or a constant function in R[d]. Conversely,
Suya et al. (2021) show how to arbitrarily manipulate convex learning models through multiple data
injections, in the case where a single model is learned from all data at once.

A large literature has focused on data poisoning, with either a focus on backdoor (Dai et al., 2019;
Zhao et al., 2020; Severi et al., 2021; Truong et al., 2020; Schwarzschild et al., 2021) or triggerless
attacks (Biggio et al., 2012; Mu˜noz-Gonz´alez et al., 2017; Shafahi et al., 2018; Zhu et al., 2019;

1We omit complexity considerations for the sake of generality. We define PAC* to be PAC without such
considerations.


-----

Huang et al., 2020; Barreno et al., 2006; Aghakhani et al., 2021; Geiping et al., 2021). However,
most of this research analyzed data poisoning without signed data. One noteworthy exception is
Mahloujifar et al. (2019), whose universal attack amplifies the probability of a (bad) property.

Collaborative PAC learning was previously introduced by Blum et al. (2017), and then extensively
studied (Chen et al., 2018a; Nguyen & Zakynthinou, 2018), sometimes with the presence of Byzantine collaborating users (Qiao, 2018; Jain & Orlitsky, 2020; Konstantinov et al., 2020). We stress
however that this line of work assumes that all honest users have the same labeling function. In other
words, given any query, they agree on how the query should be answered. This is a very unrealistic
assumption in many critical applications, like content moderation or language processing. In fact, in
such applications, removing outliers can be argued to amount to ignoring minorities’ views, which
would be highly unethical. The very definition of PAC learning must then be adapted, which is what
we do in this paper (also, we adapt it to parameterized models).

**Structure of the paper.** The rest of the paper is organized as follows. Section 2 presents a general
model of personalized learning, formalizes local PAC* learning and defines a general federated
gradient descent algorithm. Section 3 proves the equivalence between data poisoning and gradient
attacks, under local PAC* learning. Section 4 proves the local PAC* learning properties for federated
linear regression and classification. Section 5 then describes a simple and general data poisoning
attack, whose effectiveness against ℓ[2]2 [is proved theoretically and empirically. Section 6 concludes.]

2 A GENERAL PERSONALIZED FEDERATED LEARNING FRAMEWORK

We consider a set [N ] = 1, . . ., N of users. Each user n [N ] has a local signed dataset _n, and_
_{_ _}_ _∈_ _D_
aims to learn a local model θn ∈ R[d]. Users may collaborate to improve their models. Personalized
learning must then input a tuple of users’ local datasets _D[⃗]_ ≜ (D1, . . ., DN ), and output a tuple of
local models _θ[⃗][∗]_ ≜ (θ1[∗][, . . ., θ]N[∗] [)][. Like many others, we assume that the users perform federated]
learning to do so, by leveraging the computation of a common global model ρ ∈ R[d]. Intuitively, the
global model is an aggregate of all users’ local models, which users can leverage to improve their
local models. The common global model will typically allow users with too few data to obtain an
effective local model, while it may be mostly discarded by users whose local datasets are large.

More formally, we consider a very general personalized learning framework which generalizes the
models proposed by Dinh et al. (2020) and Hanzely et al. (2020). Namely, we consider that the
personalized learning algorithm outputs a global minimum (ρ[∗], θ[⃗][∗]) of a global loss given by


LOSS(ρ, θ, [⃗] _D[⃗]_ ) ≜


(ρ, θn), (1)
_R_
_nX∈[N_ ]


_n(θn,_ _n) +_
_L_ _D_
_nX∈[N_ ]


where R is a regularization, typically with a minimum at θn = ρ. For instance, Hanzely et al. (2020)
and Dinh et al. (2020) define R(ρ, θn) ≜ _λ ∥ρ −_ _θn∥2[2][, which we shall call the][ ℓ]2[2]_ [regularization.]
But other regularizations may be considered, like the ℓ2 regularization (ρ, θn) ≜ _λ_ _ρ_ _θn_ 2, or
_R_ _∥_ _−_ _∥_

the smooth-ℓ2 regularization R(ρ, θn) ≜ _λ_ 1 + ∥ρ − _θn∥2[2][. Note that, for all such regularizations,]_

the limit λ essentially yields the classical non-personalized federated learning framework.q
_→∞_

2.1 LOCAL PAC* LEARNING

In this paper, we focus on personalized learning algorithms that provably recover a user n’s preferred
_model θn[†]_ [, if the user provides a large enough][ honest dataset][ D][n][, i.e. constructed with][ θ]n[†] [. Such]
honest datasets Dn could typically be obtained by repeatedly drawing random queries (or features),
and by using the user’s preferred model θn[†] [to provide (potentially noisy) answers (or labels). We]
refer to Section 4 for examples. The model recovery condition is then formalized as follows.

**Definition 1. A personalized learning algorithm is locally PAC* learning if, for any subset H ⊂** [N ]
_of honest users, any preferred models_ _θ[⃗][†], any ε, δ > 0, and any datasets_ _[⃗]_ _H from users n /_ _H,_
_D−_ _∈_
_there exists I such that, if all users h ∈_ _H provide honest datasets Dh with at least |Dh| ≥I data_

_points, then, with probability at least 1_ _δ, we have_ _θh[∗]_ _⃗_ _θh[†]_
_−_ D _−_ 2 _[≤]_ _[ε][ for all users][ h][ ∈]_ _[H][.]_


-----

Local PAC* learning is arguably a very desirable property. Indeed, it guarantees that any honest
active user will not be discouraged to participate in federated learning as they will eventually learn
their preferred model by providing more and more data. In Section 4, we will show how local PAC*
learning can be achieved in practice, by considering specific local loss functions _n._
_L_

2.2 FEDERATED GRADIENT DESCENT

While the computation of ρ[∗] and _θ[⃗][∗]_ could be done by a single machine, which first collects the
datasets _D[⃗]_ and then minimizes the global loss LOSS, modern machine learning deployments often
rather rely on federated (stochastic) gradient descent (or variants), with a central trusted parameter
server. In this setting, each user n keeps their data Dn locally. At each iteration t, the parameter
server sends the latest global model ρ[t] to the users. Each user n is then expected to update its local
model given the global modelis what we assume in the theory part, in the manner of Dinh et al. (2020)), or by making a (stochastic) ρ[t], either by solving θn[t] [≜] [arg min]θn _[L][n][(][θ][n][,][ D][n][)+]_ _[R][(][ρ][t][, θ][n][)][ (which]_
gradient step from the previous local model θn[t][−][1] (which is what is done in our experiments, in
the manner of Hanzely & Richt´arik (2021)). User n is then expected to report the gradient gn[t] [=]
_∇ρR(ρ[t], θn[t]_ [)][ to the parameter server. The parameter server then updates the global model, using a]
gradient step, i.e. it computes ρ[t][+1] ≜ _ρ[t]_ _−_ _ηt_ _n∈[N_ ] _[g]n[t]_ [. For simplicity, here, and since our goal]

is to show the vulnerability of personalized federated learning even in good conditions, we assume

P

that the network is synchronous and that no node can crash. Note also that our setting could be
generalized to fully decentralized collaborative learning, as was done by El-Mhamdi et al. (2021).

We assume that the users are only allowed to send plausible gradient gradients. More precisely, we
denote GRAD(ρ) ≜ _{∇ρR(ρ, θ) | θ ∈_ R[d]} the closure set of plausible (sub)gradients at ρ. If user n’s
gradient gn[t] [is not in the set G][RAD][(][ρ][t][)][, the parameter server can easily detect the malicious behavior]
and gn[t] [will be ignored by the parameter server at iteration][ t][. In the case of an][ ℓ]2[2] [regularization,]
where R(ρ, θ) = λ ∥ρ − _θ∥2[2][, we clearly have G][RAD][(][ρ][) =][ R][d][ for all][ ρ][ ∈]_ [R][d][. It can be easily]
shown that, for ℓ2 and smooth-ℓ2 regularizations, GRAD(ρ) is the closed ball B(0, λ).

Nevertheless, in this setting, a strategic user s ∈ [N ] can deviate from its expected behavior, to bias
the global model in their favor. We identify in particular three sorts of attacks.

**Data poisoning: Instead of collecting an honest dataset, s fabricates any strategically crafted**
dataset _s, and then performs all other operations as expected._
_D_

**Model attack: At each iteration t, s fixes θs[t]** [≜] _[θ]s[♠][, where][ θ]s[♠]_ [is any][ strategically crafted][ model.]
All other operations would then be executed as expected.

**Gradient attack: At each iteration t, s sends any (plausible) strategically crafted gradient gs[t][.]**

Gradient attacks are intuitively the most harmful attacks, as the strategic user can then adapt their
attack based on what they observe during the learning process. However, because of this, gradient
attacks are more likely to be flagged as suspicious behaviors. At the other end of the spectrum,
data poisoning is a much safer attack, as the strategic user can always report their entire dataset,
and prove that they rigorously performed the expected computations. In fact, data poisoning can be
executed, even if users directly provide the data to a (trusted) central authority, which is then tasked
to execute (federated) gradient descent. This is typically what is done to construct recommendation
algorithms on social medias, where users’ data are their online activities (what they view, like and
share). Crucially, especially in applications with no clear ground truth, such as content moderation
or language processing, the strategic user can always argue that their dataset is an “honest” dataset;
not a strategically crafted one. Ignoring the strategic user’s data on the basis that it is an “outlier”
may then be regarded as unethical, as it can be argued to amount to rejecting minorities’ viewpoints.

3 THE EQUIVALENCE BETWEEN DATA POISONING AND GRADIENT ATTACKS

We now present our main result. The threat model we considered is closely related to “modeltargeted attacks”, studied in Suya et al. (2021). Recall also that, in this theory part, at each iteration,
local models θn[t] [are fully optimized, given][ ρ][t][, in the manner of Dinh et al. (2020).]

**Theorem 1 (Equivalence between gradient attacks and data poisoning). Assume local PAC* learn-**
_ing, and ℓ[2]2_ _[or smooth-][ℓ][2]_ _[regularization. Assume also that][ L][n]_ _[is convex and][ L][-smooth for all users]_


-----

_n_ [N ], and that the learning rate ηt is constant and small enough. Consider any datasets _[⃗]_ _s_
_∈_ _D−_
_provided by users n_ = s. Then, for any target model θs[†]
_attack of strategic user ̸_ _s (i.e. gs[t]_ _[converges) such that][ ρ][t][ →][∈]_ [R][θ]s[†][d][, if and only if, for any][, there exists a converging gradient][ ε >][ 0][, there]
_exists a dataset_ _s such that_ _ρ[∗](_ _[⃗]_ ) _θs[†]_
_D_ _D_ _−_ 2 _[≤]_ _[ε][.]_

Note that smoothness is used as a sufficient condition to prove the convergence of federated gradient
descent. We now present our proof, which goes through the intermediary step of model attacks.

3.1 EQUIVALENCE BETWEEN DATA POISONING AND MODEL ATTACKS

In this section, we prove the equivalence between data poisoning and model attacks.

**Lemma 1 (Reduction from model attack to data poisoning). Consider any data** _D[⃗]_ _and user s ∈_ [N ].
_Assume the global loss with datasets_ _[⃗]_ _has a global minimum (ρ[∗], θ[⃗][∗]). Then (ρ[∗], θ[⃗][∗]_ _s[)][ is also a]_
_D_ _−_
_global minimum of the modified loss with datasets_ _D[⃗]_ _−s and strategic reporting θs[♠]_ [≜] _[θ]s[∗][(][ ⃗]D)._

_Proof. The proof is almost straightforward. It is given in Appendix B.1._

Now, intuitively, by virtue of local PAC* learning, strategic user s can essentially guarantee that the
personalized learning framework will be learningwill imply a biasing of the global model essentially identical to the one obtained through the model θs[∗] _[≈]_ _[θ]s[♠][. But, a priori, it may seem unclear if this]_
attack that imposes θs[∗] [=][ θ]s[♠][. In the sequel, we show that the answer is yes.]

**Lemma 2 (Reduction from data poisoning to model attack). Assume ℓ[2]2[,][ ℓ][2]** _[or smooth-][ℓ][2]_ _[regular-]_
_ization, and assume local PAC* learning. Consider any datasets D−s and any attack model θs[♠]_ _[such]_
_that the modified loss LOSSs has a unique minimum ρ[∗](θs[♠][, ⃗]_ _s), θ[⃗][∗]_ _s[(][θ]s[♠][, ⃗]_ _s). Then, for any_
_D−_ _−_ _D−_
_ε > 0, there exists a dataset Ds such that we have_
_ρ[∗](_ _[⃗]_ ) _ρ[∗](θs[♠][, ⃗]_ _s)_ _θn[∗]_ [(][ ⃗] ) _θn[∗]_ [(][θ]s[♠][, ⃗] _s)_ (2)
_D_ _−_ _D−_ 2 _[≤]_ _[ε][ and][ ∀][n][ ̸][=][ s,]_ _D_ _−_ _D−_ 2 _[≤]_ _[ε.]_

Our proof in fact holds for all continuous regularizations with (ρ, θ) as _ρ_ _θ_ 2 .
Moreover, note that the approximation guarantee holds for local models too. R _R_ _→∞_ _∥_ _−_ _∥_ _→∞_

_Sketch of proof. Given local PAC* learning, for a large dataset Ds constructed from θs[♠][, strategic]_
user s can guarantee θs[∗][(][ ⃗]D) ≈ _θs[♠][. By carefully bounding the effect of the approximation on the loss]_
using the Heine-Cantor theorem, we show that this implies ρ[∗]( _[⃗]_ ) _ρ[∗](θs[♠][, ⃗]_ _s) and θn[∗]_ [(][ ⃗] )
_D_ _≈_ _D−_ _D_ _≈_
_θn[∗]_ [(][θ]s[♠][, ⃗] _s) for all n_ = s too. The precise analysis, given in Appendix B.2, is nontrivial.
_D−_ _̸_

3.2 EQUIVALENCE BETWEEN MODEL ATTACKS AND GRADIENT ATTACKS

For the last few years, gradient attacks have been widely studied by the Byzantine learning literature.
Recently, Shejwalkar et al. (2021) argued that they are not a realistic threat model. Below, we prove
that, in our setting, they are actually as concerning as model attacks (and, thus, as data poisoning).
**Lemma 3 (Reduction from model attack to gradient attack). Assume that Ln is convex and L-**
_smooth for all nodes n_ [N ], and that we use ℓ[2]2 _[or smooth-][ℓ][2]_ _[regularization. If][ g]s[t]_ _[converges and]_
_∈_
_if ηt = η is a constant small enough, then ρ[t]_ _will converge too. Denote ρ[∞]_ _its limit. Then for any_
_ε > 0, there is θs[♠]_ _ρ[∞]_ _ρ[∗](θs[♠][, ⃗]_ _s)_

_[∈]_ [R][d][ such that] _−_ _D−_ 2

_[≤]_ _[ε][.]_

_Sketch of proof. Denote gs[∞]_ [the limit of][ g]s[t][. Gradient descent then behaves as though it was minimiz-]
ing the loss plus ρ[T] _gs[∞]_ [(and ignoring][ R][(][ρ, θ][s][)][). Essentially, classical gradient descent theory then]
guarantees the convergence of ρ[t] to ρ[∞], though the precise proof is nontrivial. Then, since GRAD
is closed and gs[∞] _∈_ GRAD, we can construct θs[♠] [which approximately yields the gradient][ g]s[∞][. The]
full proof (which also yields the necessary upper bound on η) is given in Appendix B.3.

Since any model attack can clearly be achieved by the corresponding honest gradient attack, we
conclude that model attacks and gradient attacks are essentially equivalent. In light of our previous
results, this implies that gradient attacks are essentially equivalent to data poisoning (Theorem 1).


-----

4 EXAMPLES OF LOCALLY PAC* LEARNING SYSTEMS

To the best of our knowledge, though similar to collaborative PAC learning (Blum et al., 2017),
local PAC* learnability is a new concept in the context of personalized federated learning. It is thus
important to show that it is not meaningless. To achieve this, in this section, we provide sufficient
conditions for a personalized learning model to be locally PAC* learnable. First, we construct local
losses Ln as sums of losses per input, i.e.

_Ln(θn, Dn) = ν ∥θn∥2[2]_ [+] _ℓ(θn, x),_ (3)

_xX∈Dn_

for some “loss per input” function ℓ and some weight ν > 0. Appendix C gives theoretical and
empirical arguments are provided for using such a sum (as opposed to an expectation). Remarkably,
for linear or logistic regression, given such a loss, local PAC* learning can then be guaranteed.
**Theorem 2 (Personalized least square linear regression is locally PAC* learning). Consider ℓ[2]2[,][ ℓ][2]**
_or smooth-ℓ2 regularization. Assume that, to generate a data xi, a user with preferred parameter_
_θ[†]_ _∈_ R[d] _first independently draws a random vector query Qi ∈_ R[d] _from a sub-Gaussian query_
_distribution_ _Q[˜], with parameter σQ and positive definite matrix Σ = E_ _QiQi[T]_ _. Assume that the_
_user labels_ _i with a real-valued answer_ _i =_ _i_ _[θ][†][ +][ ξ][i][, where][ ξ][i][ is a zero-mean sub-Gaussian]_
_Q_ _A_ _Q[T]_  
_random noise with parameter σξ_ _, independent from Qi and other data points. Finally, assume that_
_ℓ(θ, Qi, Ai) =_ [1]2 [(][θ][T][ Q][i][ −A][i][)][2][. Then the personalized learning algorithm is locally PAC* learning.]

**Theorem 3 (Personalized logistic regression is locally PAC*-learning). Consider ℓ[2]2[,][ ℓ][2]** _[or smooth-]_
_ℓ2 regularization. Assume that, to generate a data xi, a user with preferred parameter θ[†]_ _∈_ R[d]

_first independently draws a random vector query Qi ∈_ R[d] _from a query distribution_ _Q[˜], whose_
_support SUPP( Q[˜]) is bounded and spans the full vector space R[d]. Assume that the user then labels_
_Qi with answer Ai = 1 with probability σ(Qi[T]_ _[θ][†][)][, and labels it][ A][i][ =][ −][1][ otherwise, where][ σ][(][z][) =]_
(1+e[−][z])[−][1] _is the sigmoid logistic function. Finally, assume that ℓ(θ,_ _i,_ _i) =_ ln(σ( _iθ[T]_ _i))._
_Q_ _A_ _−_ _A_ _Q_
_Then the personalized learning algorithm is locally PAC* learning._


4.1 PROOF SKETCH

In this section, we outline the proofs of theorems 2 and 3, as they are interesting in their own sake.
The proofs both leverage the following notion, which intuitively means “robust PAC* learning”.
**Definition 2 (Gradient-PAC*). Denote E(D, θ[†], I, A, B, α) the event**

_∀θ ∈_ R[d], _θ −_ _θ[†][][T]_ _∇L (θ, D) ≥_ _AI min_ _θ −_ _θ[†]_ 2 _[,]_ _θ −_ _θ[†]_ 2 _−_ _BI_ _[α]_ _θ −_ _θ[†]_ 2 _[.]_ (4)

_The loss ℓ_ _is gradient-PAC* if, for any _ _> 0, there exist constantsn_ _A_ _, Bo_ _> 0 and α_ _< 1, such_
_K_ _K[2]_ _K_ _K_
_that for any preferred model θ[†]_ R[d] _with_ _θ[†]_ 2
_honestly collecting and labeling∈ I data points according to the preferred model[≤K][, assuming that the dataset] θ[ D][†], the probability[ is obtained by]_
_of the event_ ( _, θ[†],_ _, A_ _, B_ _, α_ ) goes to 1 as _._
_E_ _D_ _I_ _K_ _K_ _K_ _I →∞_

Intuitively, this definition asserts that, as we collect more data from an honest user, then, with high
probability, the gradient of the loss at any point θ too far from θ[†] will point away from θ[†]. In
particular, gradient descent is then essentially guaranteed to draw θ closer to θ[†]. The right-hand
side of equation 4 is subtly chosen to be strong enough to yield local PAC* learning guarantees, and
weak enough to be verified by linear and logistic regression, as proved by the following lemma.
**Lemma 4. Logistic and linear regression, defined in theorems 2 and 3, are gradient PAC* learning.**

_Sketch of proof. In the case of linear regression, remarkably, the discrepancy between the em-_
pirical and the expected loss functions depends only on a few key random variables, such as
min SP 1 _i_ _Ti_ and _ξi_ _i, which can be controlled by appropriate concentration bounds._

_I_ _Q_ _Q_ _Q_

Meanwhile, for logistic regression, for  _b_, we observe that (a _b)(σ(a)_ _σ(b))_

P _|_ _| ≤K_ _−_ _−_ _≥_

_c_ min( _a_ _b_ _,_ _a_ _b_ ). Essentially, this proves that gradient-PAC* would hold if the empiri-[P]
_K_ _|_ _−_ _|_ _|_ _−_ _|[2]_
cal loss was replaced by the expected loss (times I). The actual proofs, however, are nontrivial,
especially in the case of logistic regression, which leverages topological considerations to derive a
critical uniform concentration bound. The full proofs are given in appendices D.2 and D.3.


-----

Now, under very mild assumptions on the regularization R (not even convexity!), which are verified
by the ℓ[2]2[,][ ℓ][2] [and smooth-][ℓ][2] [regularizations, we prove that the gradient-PAC* learnability through][ ℓ]
suffices to guarantee that personalized learning will be locally PAC* learning.
**Lemma 5. Assume** _is (sub)differentiable and nonnegative, and_ (ρ, θ) _as_ _ρ_ _θ_ 2 _._
_If ℓ_ _is gradient-PAC* and nonnegative, then personalized learning is locally PAC*-learning. R_ _R_ _→∞_ _∥_ _−_ _∥_ _→∞_

_Sketch of proof. Once other users’ datasets are fixed, the learning of an honest user’s model has a_
fixed biased due to R. But as the user provides more data, by gradient-PAC*, the local loss becomes
predominant, which guarantees local PAC*-learning. Appendix E provides a full proof.

Combining the two lemmas clearly yields theorems 2 and 3 as special cases. Note that our result
actually applies to a more general set of regularizations and a more general set of local losses.

4.2 THE CASE OF NEURAL NETWORKS

Neural networks generally do not verify gradient PAC* learning. After all, because of symmetries
like neuron swapping, different values of the parameters compute the same neural network function.
Thus the notion of a “preferred model” θ[†] is arguably ill-defined for neural networks[2]. Nevertheless,
we may consider a strategic user who only aims to bias the parameters of the last layer. In particular,
assuming that all layers but the last one of a neural network are pretrained and fixed, then our theory
may apply to the parameters of the last layer, if it performs classification or regression.

5 A PRACTICAL DATA POISONING ATTACK

We now construct a practical data poisoning attack. We do so by introducing a new effective gradient
attack, and by then leveraging our equivalence to turn it into a data poisoning attack.

5.1 THE COUNTER-GRADIENT ATTACK

We now define a simple, general and practical gradient attack, which we call the counter-gradient
attack (CGA). Intuitively, this attack estimates the sum g[†][,t]s [of the gradients of other users based on]
_−_
its value at the previous iteration, which can be inferred from the way the global model ρ[t][−][1] was
updated into ρ[t]. More precisely, apart from initialization ˆg−[1] _s_ [≜] [0][, CGA makes the estimation]

_gˆ−[t]_ _s_ [≜] _[ρ][t][−]η[1]t[ −]−1_ _[ρ][t]_ _−_ _gs[t][−][1]_ = g−[†][,t]s[−][1]. (5)

Strategic user s then reports the plausible gradient that moves the global model closest to the user’s
target model θs[†][, assuming others report][ ˆ]g[t] _s[. In other words, at every iteration, CGA reports]_
_−_

_gs[t]_ _[∈]_ _g[arg min]GRAD(ρ)_ _ρ[t]_ _−_ _ηt(ˆg−[t]_ _s_ [+][ g][)][ −] _[θ]s[†]_ 2 _[.]_ (6)
_∈_

Note that, to perform this attack, user s only needs to know the previous and current learning rates
_ηt_ 1 and ηt, the previous and current global models ρ[t][−][1] and ρ[t], and their target model θs[†][.]
_−_

_s_
**Computation of CGA.** Define h[t]s [≜] _[g]s[t][−][1]_ + _[ρ][t]η[−]t[θ][†]_ _−_ _[ρ][t]η[−]t[1]−[−]1[ρ][t]_ . For convex sets GRAD(ρ[t]), it

is straightforward to see that CGA boils down to computing the orthogonal projection of h[t]s [on]
GRAD(ρ[t]). This yields very simple computations for ℓ[2]2[,][ ℓ][2] [and smooth-][ℓ][2] [regularizations.]

**Proposition 1. For ℓ[2]2** _[regularization,][ CGA][ reports][ g]s[t]_ [=][ h]s[t] _[. For][ ℓ][2]_ _[or smooth-][ℓ][2]_ _[regularization,]_
CGA reports gs[t] [=][ h]s[t] [min][ {][1][, λ/][ ∥][h]s[t] _[∥]2[}][.]_

_s_
_Proof. Equation (6) boils down to minimizing the distance between_ _[ρ][t]η[−]t[θ][†]_ _gˆ[t]_ _s_ [and G][RAD][(][ρ][)][,]

_−_ _−_
which is the (convex) ball B(0, λ). This minimum is the orthogonal projection on B(0, λ).

2Evidently, our definition could be easily modified to give importance to the computed function, rather than
to the parameters of the model.


-----

**Theoretical analysis.** We now prove that CGA is perfectly successful against ℓ[2]2 [regularization.]
To do so, we suppose that, at each iteration t and for each user n ̸= s, the local models θn are fully
optimized with respect to ρ[t], and the honest gradients of gn[†][,t] [are used for the gradient descent of][ ρ][.]

**Theorem 4. Consider ℓ[2]2** _[regularization. Assume that][ ℓ]_ _[is convex and][ L][ℓ][-smooth, and that][ η][t]_ [=][ η]
_is small enough. Then CGA is converging and optimal, as ρ[t]_ _→_ _θs[†][.]_

_Sketch of proof. The main challenge is to guarantee that the other users’ gradients gn[†][,t]_ for n = s
_̸_
remain sufficiently stable over time to guarantee convergence, which can be done by leveraging
_L-smoothness. The full proof, with the necessary upper-bound on η, is given in Appendix F._

The analysis of the convergence against smooth-ℓ2 is unfortunately significantly more challenging.
Here, we simply make a remark about what CGA yields in this case, if it converges.

**Proposition 2. If CGA against smooth-ℓ2 regularization converges for ηt = η, then it either**
_achieves perfect manipulation, or it is eventually partially honest, in the sense that the gradient_
_by CGA correctly points towards θs[†][.]_


_Proof. Denote P the projection onto the closed ball B(0, λ). If CGA converges, then, by Proposi-_

tion 1, P _gs[∞]_ [+][ ρ][∞]η[−][θ]s[†] = gs[∞][. This implies in particular that][ ρ][∞] _[−]_ _[θ]s[†]_ [and][ g]s[∞] [must be colinear.]

If perfect manipulation is not achieved (i.e.  _ρ[∞]_ ≠ _θs[†][), then we must have][ g]s[∞]_ [=][ λ] _ρ[∞]−θss[†]_ 2 .

_∥[ρ][∞][−][θ][†]∥_


**Empirical evaluation of CGA.** We deployed CGA to bias the
federated learning of MNIST. We consider a strategic user whose
target model is one that labels 0’s as 1’s, 1’s as 2’s, and so on,
until 9’s that are labeled as 0’s. In particular, this target model
has a nil accuracy. Figure 1 shows that such a user effectively
hacks the ℓ[2]2 [regularization against 10 honest users who each]
have 6,000 data points of MNIST, in the case where local models only undergo a single gradient step at each iteration, but fails
to hack the ℓ2 regularization. Further details on the experiment
are given in Appendix G. We also ran a similar successful attack
on the last layer of a deep neural network trained on cifar-10,
which is detailed in Appendix J. The Appendix also discusses
the extent to which the attack may be turned into data poisoning.


1.0

0.8

0.6

0.4

0.2

0.0

|Col1|Col2|
|---|---|
|||
|||
|||
|||
|||
|||
|||
|||
|||
|||
|||
|||
|||
||L2 squared|


25 50 75 100 125 150 175 200

L2 squared
L2

Epochs


Figure 1: Accuracy of the global
model under attack by CGA.


5.2 FROM GRADIENT ATTACK TO MODEL ATTACK AGAINST ℓ[2]2

We now show how to turn a gradient attack into model attack, against ℓ[2]2 [regularization. Assume]
that we found a gradient gs[∞] [such that][ ρ][∞] [=][ θ]s[†][. It is trivial to transform it into a model attack by]
setting θs[♠] [≜] _[θ]s[†]_ _[−]_ 2[1] _[g]s[∞][, as guaranteed by the following result, and as depicted by Figure 2.]_

Model attack Model attack


0.8

0.6

0.4

0.2

0.0

|Model attack|Col2|Col3|
|---|---|---|
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
|acc_glob|||


50 100 150 200 250

acc_glob

Epochs


Model attack

35

30

25

20 l2_dist

l2_norm

l2 norm 15 target_dist

10

5

0 50 100 150 200 250

Epochs

(a) Distance between the global model ρ[t] and
the target model θs[†] [(target dist).]


(b) Accuracy of ρ[t] according to θs[†] [(which rela-]
bels 0 → 1 → 2 → _... →_ 9 → 0).


Figure 2: Successful model attack against ℓ[2]2 [by combining CGA and Proposition 3.]


-----

**Proposition 3. Consider the ℓ[2]2** _[regularization. Suppose that][ g]s[t]_ _[→]_ _[g]s[∞]_ _and1_ _ρ[t]_ _→_ _θs[†][, with a]_
_constant learning rate ηt = η. Then, under the model attack θs[♠]_ [≜] _[θ]s[†]_ _[−]_ 2λ _[g]s[∞][, the gradient at]_

_ρ = θs[†]_ _[vanishes, i.e.][ ∇][ρ][L][OSS][(][θ]s[†][, ⃗]θ−[∗]_ _s[(][θ]s[†][, ⃗]D−s), θs[♠][,][ D][−][s][) = 0][.]_

_Proof. Given that the learning rate is constant, the convergence ρ[t]_ _→_ _θs[†]_ [implies that the sum of]
honest users’ gradients at ρ = θs[†] [must equal][ −][g]s[∞][. Therefore, to achieve][ ρ][∗] [=][ θ]s[†] [with a model]
attack, it suffices to send a modelρ = θs[†] [equals][ g]s[∞][. Since the gradient is] θs[♠] [such that the gradient of][ λ][(][θ]s[†] _[−]_ _[θ]s[♠][)][,][ θ]s[♠]_ [≜] _[θ]s[†]_ _[−][ λ]21λ_ _[g]ρs[∞] −[does the trick.]θs[♠]_ 2[2] [with respect to][ ρ][ at]

5.3 FROM MODEL ATTACK TO DATA POISONING AGAINST ℓ[2]2


**The case of linear regression.** In linear regression, any model attack can be turned into a single
_data poisoning attack, as proved by the following theorem whose proof is given in Appendix H._

**Theorem 5. Consider the ℓ[2]2** _[regularization and linear regression. For any data][ D][−][s]_ _[and any target]_
_value θs[†][, there is a datapoint][ (][Q][,][ A][)][ to be injected by user][ s][ such that][ ρ][∗][(][{][(][Q][,][ A][)][}][,][ D][−][s][) =][ θ]s[†][.]_

**The case of linear classification.** We now consider linear classification, with the case of MNIST.
By Theorem 2, any model attack can be turned into data poisoning, provided sufficiently many
(random) data points are labeled by the strategic user. However, this may require creating too many
data labelings, especially if the norm of θs[♠] [is large (which is the case if][ s][ is alone against many]
active users), as suggested by Theorem 3.

For efficient data poisoning, define the indifference affine subspace V ⊂ R[d] as the set of images
with equiprobable labels. Intuitively, labeling images close to V is very informative, as it informs us
directly about the separating hyperplanes. To generate images, we first draw random images, which
we then project orthogonally on V . We then add a small noise, before probabilistically labeling the
image with model θs[♠][. Note that this leads us to consider images not in][ [0][,][ 1]][d][. Figure 3 shows the]
effectiveness of the resulting data poisoning attack, with only 2,000 data points, as opposed to the
other nodes’ 6,000 data points. More details and explanations are provided in Appendix I.


Data attack

|Col1|Col2|Col3|
|---|---|---|
||||
||||
||||
||||
||||
||||
||||
||||
|acc_glob|||



50 100 150 200 250

acc_glob

Epochs


1.0

0.8

0.6

0.4

0.2

0.0


Data attack

35

30

25

20 l2_dist

l2_norm

l2 norm 15 target_dist

10

5

0 50 100 150 200 250

Epochs

(a) Distance between the global model ρ[t] and
the target model θs[†] [(target dist).]


(b) Accuracy of ρ[t] according to θs[†] [(which rela-]
bels 0 → 1 → 2 → _... →_ 9 → 0).


Figure 3: Successful data attack against ℓ[2]2 [by the efficient data generation scheme.]


6 CONCLUDING REMARKS

We showed in this paper that, unlike what has been argued by, e.g., Shejwalkar et al. (2021), the gradient attack threat is not unrealistic. For personalized federated learning with local PAC* guarantees,
gradient attacks are in fact just as realistic and harmful as strategic data reporting. More generally,
our work stresses how critical Byzantine learning research is. For instance, El-Mhamdi et al. (2021)
proved lower bounds on what any collaborative learning algorithm can guarantee in heterogeneous
environments, under Byzantine gradient attacks. Our work implies that, at least for certain personalized federated learning problems, these lower bounds also hold for data poisoning attacks, which
are known to be common for many high-risk applications. Arguably, a lot more security measures
are urgently needed to make large-scale learning algorithms safe.


-----

ETHICS STATEMENT

The safety of algorithms is arguably a prerequisite to their ethics. After all, an arbitrarily manipulable large-scale algorithm will unavoidably endanger the targets of the entities that successfully
design such algorithms. Typically, unsafe large-scale recommendation algorithms may be hacked by
health disinformation campaigns that aim to promote non-certified products, e.g., by falsely pretending that they cure COVID-19. Such algorithms must not be regarded as ethical, even if they were
designed with the best intentions. We believe that our work helps understand the vulnerabilities of
such algorithms, and will motivate further research in the ethics and security of machine learning.

REPRODUCIBILITY STATEMENT

All our experiments are run on the classical datasets MNIST and FashionMNIST. We provide all of
the source codes to reproduce the experiments:

-  The sum versus expectation experiments can be run by executing this file:
[https://www.dropbox.com/sh/qdgmz9air24nhyr/AAAtycEkxc_](https://www.dropbox.com/sh/qdgmz9air24nhyr/AAAtycEkxc_1hGbvU5YG18z4a?dl=0)
[1hGbvU5YG18z4a?dl=0](https://www.dropbox.com/sh/qdgmz9air24nhyr/AAAtycEkxc_1hGbvU5YG18z4a?dl=0)

-  The counter-gradient attack experiments can be run by executing this file:
[https://www.dropbox.com/sh/bycqkccgmk4muzn/](https://www.dropbox.com/sh/bycqkccgmk4muzn/AACRD1yeTglLSHEd1OOAzmVqa?dl=0)
[AACRD1yeTglLSHEd1OOAzmVqa?dl=0](https://www.dropbox.com/sh/bycqkccgmk4muzn/AACRD1yeTglLSHEd1OOAzmVqa?dl=0)

-  The data poisoning attack experiments can be run by executing this file:
[https://www.dropbox.com/sh/qodnl6ivzti8hch/](https://www.dropbox.com/sh/qodnl6ivzti8hch/AADgX4EYuSOotiMCAHyTIiGMa?dl=0)
[AADgX4EYuSOotiMCAHyTIiGMa?dl=0](https://www.dropbox.com/sh/qodnl6ivzti8hch/AADgX4EYuSOotiMCAHyTIiGMa?dl=0)

-  The cifar10 on VGG 13-BN experiments can be run by executing this file:
[https://www.dropbox.com/sh/5mw5c9dt25eiav7/AAC0jP3e2kuh_](https://www.dropbox.com/sh/5mw5c9dt25eiav7/AAC0jP3e2kuh_zvLudWbmDl-a?dl=0)
[zvLudWbmDl-a?dl=0](https://www.dropbox.com/sh/5mw5c9dt25eiav7/AAC0jP3e2kuh_zvLudWbmDl-a?dl=0)

The experiments are seeded and the CuDNN backend is configured in deterministic mode in order to
reduce the sources of non-determinism. We also turn of the benchmark mode. Executing the codes
will generate the figures and statistics of our main paper, and most of the figures of our Appendix.
Our other figures can be obtained by adjusting the hyperparameters of our codes.

The full description of the architecture and optimisation algorithm used is described in Appendix C.
The experimental setup details of each experiment are provided in the Appendix, along with additional results. The Appendix also contains the full proofs of our theorems.

REFERENCES

Hojjat Aghakhani, Dongyu Meng, Yu-Xiang Wang, Christopher Kruegel, and Giovanni Vigna.
Bullseye polytope: A scalable clean-label poisoning attack with improved transferability, 2021.

Marco Barreno, Blaine Nelson, Russell Sears, Anthony D. Joseph, and J. D. Tygar. Can machine
learning be secure? In Proceedings of the 2006 ACM Symposium on Information, Computer and
_Communications Security, ASIACCS ’06, pp. 16–25, New York, NY, USA, 2006. Association_
[for Computing Machinery. ISBN 1595932720. doi: 10.1145/1128817.1128824. URL https:](https://doi.org/10.1145/1128817.1128824)
[//doi.org/10.1145/1128817.1128824.](https://doi.org/10.1145/1128817.1128824)

Gilad Baruch, Moran Baruch, and Yoav Goldberg. A little is enough: Circumventing defenses for
distributed learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As[sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/](https://proceedings.neurips.cc/paper/2019/file/ec1c59141046cd1866bbbcdfb6ae31d4-Paper.pdf)
[ec1c59141046cd1866bbbcdfb6ae31d4-Paper.pdf.](https://proceedings.neurips.cc/paper/2019/file/ec1c59141046cd1866bbbcdfb6ae31d4-Paper.pdf)

Omer Ben-Porat and Moshe Tennenholtz. Best response regression. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 30. Curran Asso[ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/](https://proceedings.neurips.cc/paper/2017/file/1ce927f875864094e3906a4a0b5ece68-Paper.pdf)
[1ce927f875864094e3906a4a0b5ece68-Paper.pdf.](https://proceedings.neurips.cc/paper/2017/file/1ce927f875864094e3906a4a0b5ece68-Paper.pdf)


-----

Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the
dangers of stochastic parrots: Can language models be too big? In Madeleine Clare Elish,
William Isaac, and Richard S. Zemel (eds.), FAccT ’21: 2021 ACM Conference on Fairness,
_Accountability, and Transparency, Virtual Event / Toronto, Canada, March 3-10, 2021, pp. 610–_
[623. ACM, 2021. doi: 10.1145/3442188.3445922. URL https://doi.org/10.1145/](https://doi.org/10.1145/3442188.3445922)
[3442188.3445922.](https://doi.org/10.1145/3442188.3445922)

Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector machines. In Proceedings of the 29th International Conference on Machine Learning, ICML
_2012, Edinburgh, Scotland, UK, June 26 - July 1, 2012. icml.cc / Omnipress, 2012._ URL
[http://icml.cc/2012/papers/880.pdf.](http://icml.cc/2012/papers/880.pdf)

Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer. Machine
learning with adversaries: Byzantine tolerant gradient descent. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan,
and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30: _An-_
_nual Conference on Neural Information Processing Systems 2017, 4-9 December 2017,_
_Long Beach, CA, USA, pp. 119–129, 2017._ [URL http://papers.nips.cc/paper/](http://papers.nips.cc/paper/6617-machine-learning-with-adversaries-byzantine-tolerant-gradient-descent)
[6617-machine-learning-with-adversaries-byzantine-tolerant-gradient-descent.](http://papers.nips.cc/paper/6617-machine-learning-with-adversaries-byzantine-tolerant-gradient-descent)

Avrim Blum, Nika Haghtalab, Ariel D. Procaccia, and Mingda Qiao. Collaborative PAC learning. In
Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 30:
_Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long_
_Beach, CA, USA, pp. 2392–2401, 2017._ [URL https://proceedings.neurips.cc/](https://proceedings.neurips.cc/paper/2017/hash/186a157b2992e7daed3677ce8e9fe40f-Abstract.html)
[paper/2017/hash/186a157b2992e7daed3677ce8e9fe40f-Abstract.html.](https://proceedings.neurips.cc/paper/2017/hash/186a157b2992e7daed3677ce8e9fe40f-Abstract.html)

Samantha Bradshaw and Philip N Howard. The global disinformation order: 2019 global inventory
_of organised social media manipulation. Project on Computational Propaganda, 2019._

Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,
Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler,
Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot
learners. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan,
and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual
_Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,_
_[2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)_
[1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html)

Yang Cai, Constantinos Daskalakis, and Christos H. Papadimitriou. Optimum statistical estimation
with strategic data sources. In Peter Gr¨unwald, Elad Hazan, and Satyen Kale (eds.), Proceedings
_of The 28th Conference on Learning Theory, COLT 2015, Paris, France, July 3-6, 2015, vol-_
ume 40 of JMLR Workshop and Conference Proceedings, pp. 280–296. JMLR.org, 2015. URL
[http://proceedings.mlr.press/v40/Cai15.html.](http://proceedings.mlr.press/v40/Cai15.html)

Jiecao Chen, Qin Zhang, and Yuan Zhou. Tight bounds for collaborative PAC learning via multiplicative weights. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle,
Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett (eds.), Advances in Neu_ral Information Processing Systems 31:_ _Annual Conference on Neural Information Pro-_
_cessing Systems 2018,_ _NeurIPS 2018,_ _December 3-8,_ _2018,_ _Montr´eal,_ _Canada,_ pp.
[3602–3611, 2018a. URL https://proceedings.neurips.cc/paper/2018/hash/](https://proceedings.neurips.cc/paper/2018/hash/ed519dacc89b2bead3f453b0b05a4a8b-Abstract.html)
[ed519dacc89b2bead3f453b0b05a4a8b-Abstract.html.](https://proceedings.neurips.cc/paper/2018/hash/ed519dacc89b2bead3f453b0b05a4a8b-Abstract.html)

Yiling Chen, Chara Podimata, Ariel D. Procaccia, and Nisarg Shah. Strategyproof linear regression in high dimensions. In Proceedings of the 2018 ACM Conference on Economics and Com_putation, EC ’18, pp. 9–26, New York, NY, USA, 2018b. Association for Computing Machin-_
[ery. ISBN 9781450358293. doi: 10.1145/3219166.3219175. URL https://doi.org/10.](https://doi.org/10.1145/3219166.3219175)
[1145/3219166.3219175.](https://doi.org/10.1145/3219166.3219175)


-----

Yiling Chen, Yang Liu, and Chara Podimata. Learning strategy-aware linear classifiers.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances
_in Neural Information Processing Systems, volume 33, pp. 15265–15276. Curran Asso-_
[ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/](https://proceedings.neurips.cc/paper/2020/file/ae87a54e183c075c494c4d397d126a66-Paper.pdf)
[ae87a54e183c075c494c4d397d126a66-Paper.pdf.](https://proceedings.neurips.cc/paper/2020/file/ae87a54e183c075c494c4d397d126a66-Paper.pdf)

Jiazhu Dai, Chuanshuai Chen, and Yufeng Li. A backdoor attack against lstm-based text classification systems. IEEE Access, 7:138872–138878, 2019. doi: 10.1109/ACCESS.2019.2941376.
[URL https://doi.org/10.1109/ACCESS.2019.2941376.](https://doi.org/10.1109/ACCESS.2019.2941376)

Ofer Dekel, Felix Fischer, and Ariel D. Procaccia. Incentive compatible regression learning. Jour_nal of Computer and System Sciences, 76(8):759–777, 2010. ISSN 0022-0000. doi: https://doi._
org/10.1016/j.jcss.2010.03.003. [URL https://www.sciencedirect.com/science/](https://www.sciencedirect.com/science/article/pii/S0022000010000309)
[article/pii/S0022000010000309.](https://www.sciencedirect.com/science/article/pii/S0022000010000309)

Canh T. Dinh, Nguyen H. Tran, and Tuan Dung Nguyen. Personalized federated learning with
moreau envelopes. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual
_Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,_
_[2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/](https://proceedings.neurips.cc/paper/2020/hash/f4f1f13c8289ac1b1ee0ff176b56fc60-Abstract.html)_
[f4f1f13c8289ac1b1ee0ff176b56fc60-Abstract.html.](https://proceedings.neurips.cc/paper/2020/hash/f4f1f13c8289ac1b1ee0ff176b56fc60-Abstract.html)

El-Mahdi El-Mhamdi, Rachid Guerraoui, Arsany Guirguis, Lˆe Nguyˆen Hoang, and S´ebastien
Rouault. Genuinely distributed byzantine machine learning. In Yuval Emek and Christian
Cachin (eds.), PODC ’20: ACM Symposium on Principles of Distributed Computing, Virtual
_Event, Italy, August 3-7, 2020, pp. 355–364. ACM, 2020. doi: 10.1145/3382734.3405695. URL_
[https://doi.org/10.1145/3382734.3405695.](https://doi.org/10.1145/3382734.3405695)

El-Mahdi El-Mhamdi, Sadegh Farhadkhani, Rachid Guerraoui, Arsany Guirguis, Lˆe Nguyˆen Hoang,
and S´ebastien Rouault. Collaborative learning in the jungle (decentralized, byzantine, heterogeneous, asynchronous and nonconvex learning). In Advances in Neural Information Processing
_Systems 34: Annual Conference on Neural Information Processing Systems 2021, December 6-_
_14, 2021, 2021._

El-Mahdi El-Mhamdi, Rachid Guerraoui, and S´ebastien Rouault. Distributed momentum for
byzantine-resilient stochastic gradient descent. In 9th International Conference on Learning
_Representations, ICLR 2021, Vienna, Austria, May 4–8, 2021. OpenReview.net, 2021._ URL
[https://openreview.net/forum?id=H8UHdhWG6A3.](https://openreview.net/forum?id=H8UHdhWG6A3)

Alireza Fallah, Aryan Mokhtari, and Asuman E. Ozdaglar. Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach. In Hugo
Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien
Lin (eds.), Advances in Neural Information Processing Systems 33: _Annual Conference_
_on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,_
_virtual,_ 2020. [URL https://proceedings.neurips.cc/paper/2020/hash/](https://proceedings.neurips.cc/paper/2020/hash/24389bfe4fe2eba8bf9aa9203a44cdad-Abstract.html)
[24389bfe4fe2eba8bf9aa9203a44cdad-Abstract.html.](https://proceedings.neurips.cc/paper/2020/hash/24389bfe4fe2eba8bf9aa9203a44cdad-Abstract.html)

William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion param[eter models with simple and efficient sparsity. CoRR, abs/2101.03961, 2021. URL https:](https://arxiv.org/abs/2101.03961)
[//arxiv.org/abs/2101.03961.](https://arxiv.org/abs/2101.03961)

Brian Fung and Ahiza Garcia. Facebook has shut down 5.4 billion fake accounts this year. CNN
_Business, 2019._

Jonas Geiping, Liam H Fowl, W. Ronny Huang, Wojciech Czaja, Gavin Taylor, Michael Moeller,
and Tom Goldstein. Witches’ brew: Industrial scale data poisoning via gradient matching. In
_[International Conference on Learning Representations, 2021. URL https://openreview.](https://openreview.net/forum?id=01olnfLIbD)_
[net/forum?id=01olnfLIbD.](https://openreview.net/forum?id=01olnfLIbD)

Filip Hanzely and Peter Richt´arik. Federated learning of a mixture of global and local models, 2021.


-----

Filip Hanzely, Slavom´ır Hanzely, Samuel Horv´ath, and Peter Richt´arik. Lower bounds
and optimal algorithms for personalized federated learning. In Hugo Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.),
_Advances in Neural Information Processing Systems 33:_ _Annual Conference on Neu-_
_ral Information Processing Systems 2020,_ _NeurIPS 2020,_ _December 6-12,_ _2020,_ _vir-_
_tual,_ 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/](https://proceedings.neurips.cc/paper/2020/hash/187acf7982f3c169b3075132380986e4-Abstract.html)
[187acf7982f3c169b3075132380986e4-Abstract.html.](https://proceedings.neurips.cc/paper/2020/hash/187acf7982f3c169b3075132380986e4-Abstract.html)

Moritz Hardt, Nimrod Megiddo, Christos Papadimitriou, and Mary Wootters. Strategic classification. In Proceedings of the 2016 ACM Conference on Innovations in Theoretical Com_puter Science, ITCS ’16, pp. 111–122, New York, NY, USA, 2016. Association for Com-_
puting Machinery. ISBN 9781450340571. doi: 10.1145/2840728.2840730. [URL https:](https://doi.org/10.1145/2840728.2840730)
[//doi.org/10.1145/2840728.2840730.](https://doi.org/10.1145/2840728.2840730)

Lˆe Nguyˆen Hoang. Science communication desperately needs more aligned recommendation algorithms. Frontiers in Communication, 5:115, 2020.

Lˆe Nguyˆen Hoang, Louis Faucon, and El-Mahdi El-Mhamdi. Recommendation algorithms, a neglected opportunity for public health. Revue M´edecine et Philosophie, 4(2):16–24, 2021.

Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge University Press, 2 edition,
2012. doi: 10.1017/9781139020411.

W. Ronny Huang, Jonas Geiping, Liam Fowl, Gavin Taylor, and Tom Goldstein. Metapoison: Practical general-purpose clean-label data poisoning. In Hugo Larochelle,
Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.),
_Advances in Neural Information Processing Systems 33:_ _Annual Conference on Neu-_
_ral Information Processing Systems 2020,_ _NeurIPS 2020,_ _December 6-12,_ _2020,_ _vir-_
_tual,_ 2020. URL [https://proceedings.neurips.cc/paper/2020/hash/](https://proceedings.neurips.cc/paper/2020/hash/8ce6fc704072e351679ac97d4a985574-Abstract.html)
[8ce6fc704072e351679ac97d4a985574-Abstract.html.](https://proceedings.neurips.cc/paper/2020/hash/8ce6fc704072e351679ac97d4a985574-Abstract.html)

Eugene Ie, Vihan Jain, Jing Wang, Sanmit Narvekar, Ritesh Agarwal, Rui Wu, Heng-Tze Cheng,
Tushar Chandra, and Craig Boutilier. Slateq: A tractable decomposition for reinforcement
learning with recommendation sets. In Sarit Kraus (ed.), Proceedings of the Twenty-Eighth
_International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August_
_10-16, 2019, pp. 2592–2599. ijcai.org, 2019._ doi: 10.24963/ijcai.2019/360. [URL https:](https://doi.org/10.24963/ijcai.2019/360)
[//doi.org/10.24963/ijcai.2019/360.](https://doi.org/10.24963/ijcai.2019/360)

Ayush Jain and Alon Orlitsky. A general method for robust learning from batches. In
Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and HsuanTien Lin (eds.), Advances in Neural Information Processing Systems 33: _Annual Con-_
_ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,_
_[2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/](https://proceedings.neurips.cc/paper/2020/hash/f7a82ce7e16d9687e7cd9a9feb85d187-Abstract.html)_
[f7a82ce7e16d9687e7cd9a9feb85d187-Abstract.html.](https://proceedings.neurips.cc/paper/2020/hash/f7a82ce7e16d9687e7cd9a9feb85d187-Abstract.html)

Deborah G. Johnson and Nicholas Diakopoulos. What to do about deepfakes. Commun. ACM, 64
[(3):33–35, 2021. doi: 10.1145/3447255. URL https://doi.org/10.1145/3447255.](https://doi.org/10.1145/3447255)

Peter Kairouz, H. Brendan McMahan, Brendan Avent, Aur´elien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, Rafael G. L.
D’Oliveira, Hubert Eichner, Salim El Rouayheb, David Evans, Josh Gardner, Zachary Garrett,
Adri`a Gasc´on, Badih Ghazi, Phillip B. Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He,
Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi,
Mikhail Khodak, Jakub Koneˇcn´y, Aleksandra Korolova, Farinaz Koushanfar, Sanmi Koyejo,
Tancr`ede Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard Nock, Ayfer Ozg¨[¨] ur, Rasmus
Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, Ramesh Raskar, Dawn Song, Weikang Song,
Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tram`er, Praneeth Vepakomma,
Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao. Advances
and open problems in federated learning, 2021.

Jakub Konecn´y, Brendan McMahan, and Daniel Ramage. Federated optimization: Distributed op[timization beyond the datacenter. CoRR, abs/1511.03575, 2015. URL http://arxiv.org/](http://arxiv.org/abs/1511.03575)
[abs/1511.03575.](http://arxiv.org/abs/1511.03575)


-----

Nikola Konstantinov, Elias Frantar, Dan Alistarh, and Christoph Lampert. On the sample complexity
of adversarial multi-source PAC learning. In Proceedings of the 37th International Conference
_on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings_
_[of Machine Learning Research, pp. 5416–5425. PMLR, 2020. URL http://proceedings.](http://proceedings.mlr.press/v119/konstantinov20a.html)_
[mlr.press/v119/konstantinov20a.html.](http://proceedings.mlr.press/v119/konstantinov20a.html)

Ram Shankar Siva Kumar, Magnus Nystr¨om, John Lambert, Andrew Marshall, Mario Goertzel,
Andi Comissoneru, Matt Swann, and Sharon Xia. Adversarial machine learning-industry perspectives. In 2020 IEEE Security and Privacy Workshops, SP Workshops, San Francisco, CA,
_USA, May 21, 2020, pp. 69–75. IEEE, 2020._ doi: 10.1109/SPW50608.2020.00028. URL
[https://doi.org/10.1109/SPW50608.2020.00028.](https://doi.org/10.1109/SPW50608.2020.00028)

Florian Lehmann and Daniel Buschek. Examining autocompletion as a basic concept for interaction
[with generative AI. i-com, 19(3):251–264, 2021. doi: 10.1515/icom-2020-0025. URL https:](https://doi.org/10.1515/icom-2020-0025)
[//doi.org/10.1515/icom-2020-0025.](https://doi.org/10.1515/icom-2020-0025)

Saeed Mahloujifar, Mohammad Mahmoody, and Ameer Mohammed. Data poisoning attacks
in multi-party learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceed_ings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June_
_2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Re-_
_search, pp. 4274–4283. PMLR, 2019._ [URL http://proceedings.mlr.press/v97/](http://proceedings.mlr.press/v97/mahloujifar19a.html)
[mahloujifar19a.html.](http://proceedings.mlr.press/v97/mahloujifar19a.html)

Guangcan Mai, Kai Cao, Pong C. Yuen, and Anil K. Jain. On the reconstruction of face images
from deep face templates. _IEEE Trans. Pattern Anal. Mach. Intell., 41(5):1188–1202, 2019._
[doi: 10.1109/TPAMI.2018.2827389. URL https://doi.org/10.1109/TPAMI.2018.](https://doi.org/10.1109/TPAMI.2018.2827389)
[2827389.](https://doi.org/10.1109/TPAMI.2018.2827389)

Kris McGuffie and Alex Newhouse. The radicalization risks of GPT-3 and advanced neural language
[models. CoRR, abs/2009.06807, 2020. URL https://arxiv.org/abs/2009.06807.](https://arxiv.org/abs/2009.06807)

Reshef Meir, Shaull Almagor, Assaf Michaely, and Jeffrey S. Rosenschein. Tight bounds for strategyproof classification. In The 10th International Conference on Autonomous Agents and Multi_agent Systems - Volume 1, AAMAS ’11, pp. 319–326, Richland, SC, 2011. International Founda-_
tion for Autonomous Agents and Multiagent Systems. ISBN 0982657153.

Reshef Meir, Ariel D. Procaccia, and Jeffrey S. Rosenschein. Algorithms for strategyproof classification. Artificial Intelligence, 186:123–156, 2012. ISSN 0004-3702. doi: https://doi.org/10.1016/
[j.artint.2012.03.008. URL https://www.sciencedirect.com/science/article/](https://www.sciencedirect.com/science/article/pii/S000437021200029X)
[pii/S000437021200029X.](https://www.sciencedirect.com/science/article/pii/S000437021200029X)

El Mahdi El Mhamdi, Rachid Guerraoui, and S´ebastien Rouault. The hidden vulnerability of distributed learning in byzantium. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the
_35th International Conference on Machine Learning, ICML 2018, Stockholmsm¨assan, Stockholm,_
_Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp. 3518–_
[3527. PMLR, 2018. URL http://proceedings.mlr.press/v80/mhamdi18a.html.](http://proceedings.mlr.press/v80/mhamdi18a.html)

Luis Mu˜noz-Gonz´alez, Battista Biggio, Ambra Demontis, Andrea Paudice, Vasin Wongrassamee,
Emil C. Lupu, and Fabio Roli. Towards poisoning of deep learning algorithms with back-gradient
optimization. In Bhavani M. Thuraisingham, Battista Biggio, David Mandell Freeman, Brad
Miller, and Arunesh Sinha (eds.), Proceedings of the 10th ACM Workshop on Artificial Intelli_gence and Security, AISec@CCS 2017, Dallas, TX, USA, November 3, 2017, pp. 27–38. ACM,_
2017. doi: 10.1145/3128572.3140451. [URL https://doi.org/10.1145/3128572.](https://doi.org/10.1145/3128572.3140451)
[3140451.](https://doi.org/10.1145/3128572.3140451)

Lisa-Maria Neudert, Philip Howard, and Bence Kollanyi. Sourcing and automation of political news and information during three european elections. _Social Media+ Society, 5(3):_
2056305119863147, 2019.

Huy L. Nguyen and Lydia Zakynthinou. Improved algorithms for collaborative PAC learning. In
Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and


-----

Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Con_ference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,_
_[Montr´eal, Canada, pp. 7642–7650, 2018. URL https://proceedings.neurips.cc/](https://proceedings.neurips.cc/paper/2018/hash/3569df159ec477451530c4455b2a9e86-Abstract.html)_
[paper/2018/hash/3569df159ec477451530c4455b2a9e86-Abstract.html.](https://proceedings.neurips.cc/paper/2018/hash/3569df159ec477451530c4455b2a9e86-Abstract.html)

Javier Perote and Juan Perote-Pe˜na. Strategy-proof estimators for simple regression. _Math-_
_ematical Social Sciences, 47(2):153–176, 2004._ ISSN 0165-4896. doi: https://doi.org/10.
1016/S0165-4896(03)00085-4. [URL https://www.sciencedirect.com/science/](https://www.sciencedirect.com/science/article/pii/S0165489603000854)
[article/pii/S0165489603000854.](https://www.sciencedirect.com/science/article/pii/S0165489603000854)

Javier Perote and Olavide Sevilla. The impossibility of strategy-proof clustering. Economics Bul_letin, 2003._

[Huy Phan. huyvnphan/pytorch cifar10, January 2021. URL https://doi.org/10.5281/](https://doi.org/10.5281/zenodo.4431043)
[zenodo.4431043.](https://doi.org/10.5281/zenodo.4431043)

Mingda Qiao. Do outliers ruin collaboration? In Jennifer G. Dy and Andreas Krause (eds.), Proceed_ings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm¨assan,_
_Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Re-_
_search, pp. 4177–4184. PMLR, 2018._ [URL http://proceedings.mlr.press/v80/](http://proceedings.mlr.press/v80/qiao18a.html)
[qiao18a.html.](http://proceedings.mlr.press/v80/qiao18a.html)

Francesco Ricci, Lior Rokach, and Bracha Shapira. Introduction to recommender systems handbook. In Francesco Ricci, Lior Rokach, Bracha Shapira, and Paul B. Kantor (eds.), Recom_[mender Systems Handbook, pp. 1–35. Springer, 2011. doi: 10.1007/978-0-387-85820-3\ 1. URL](https://doi.org/10.1007/978-0-387-85820-3_1)_
[https://doi.org/10.1007/978-0-387-85820-3_1.](https://doi.org/10.1007/978-0-387-85820-3_1)

Avi Schwarzschild, Micah Goldblum, Arjun Gupta, John P Dickerson, and Tom Goldstein.
Just how toxic is data poisoning? a unified benchmark for backdoor and data poisoning attacks. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Con_ference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp._
[9389–9398. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/](https://proceedings.mlr.press/v139/schwarzschild21a.html)
[schwarzschild21a.html.](https://proceedings.mlr.press/v139/schwarzschild21a.html)

Giorgio Severi, Jim Meyer, Scott Coull, and Alina Oprea. Explanation-guided backdoor poisoning attacks against malware classifiers. In Michael Bailey and Rachel Greenstadt (eds.),
_30th USENIX Security Symposium, USENIX Security 2021, August 11-13, 2021, pp. 1487–_
1504. USENIX Association, 2021. [URL https://www.usenix.org/conference/](https://www.usenix.org/conference/usenixsecurity21/presentation/severi)
[usenixsecurity21/presentation/severi.](https://www.usenix.org/conference/usenixsecurity21/presentation/severi)

Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras,
and Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks.
In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi,
and Roman Garnett (eds.), Advances in Neural Information Processing Systems 31: Annual Con_ference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,_
_[Montr´eal, Canada, pp. 6106–6116, 2018. URL https://proceedings.neurips.cc/](https://proceedings.neurips.cc/paper/2018/hash/22722a343513ed45f14905eb07621686-Abstract.html)_
[paper/2018/hash/22722a343513ed45f14905eb07621686-Abstract.html.](https://proceedings.neurips.cc/paper/2018/hash/22722a343513ed45f14905eb07621686-Abstract.html)

Virat Shejwalkar, Amir Houmansadr, Peter Kairouz, and Daniel Ramage. Back to the drawing
board: A critical evaluation of poisoning attacks on federated learning. CoRR, abs/2108.10241,
[2021. URL https://arxiv.org/abs/2108.10241.](https://arxiv.org/abs/2108.10241)

Heung-Yeung Shum, Xiaodong He, and Di Li. From eliza to xiaoice: challenges and opportunities
with social chatbots. Frontiers Inf. Technol. Electron. Eng., 19(1):10–26, 2018. doi: 10.1631/
[FITEE.1700826. URL https://doi.org/10.1631/FITEE.1700826.](https://doi.org/10.1631/FITEE.1700826)

Jason R. Smith, Herve Saint-Amand, Magdalena Plamada, Philipp Koehn, Chris Callison-Burch,
and Adam Lopez. Dirt cheap web-scale parallel text from the common crawl. In Proceedings of
_the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013, 4-9 August_
_2013, Sofia, Bulgaria, Volume 1: Long Papers, pp. 1374–1383. The Association for Computer_
[Linguistics, 2013. URL https://www.aclweb.org/anthology/P13-1135/.](https://www.aclweb.org/anthology/P13-1135/)

Joan E. Solsman. Youtube’s ai is the puppet master over most of what you watch. CNET, 2018.


-----

Fnu Suya, Saeed Mahloujifar, Anshuman Suri, David Evans, and Yuan Tian. Model-targeted poisoning attacks with provable convergence. In Marina Meila and Tong Zhang (eds.), Proceedings
_of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual_
_Event, volume 139 of Proceedings of Machine Learning Research, pp. 10000–10010. PMLR,_
[2021. URL http://proceedings.mlr.press/v139/suya21a.html.](http://proceedings.mlr.press/v139/suya21a.html)

Loc Truong, Chace Jones, Brian Hutchinson, Andrew August, Brenda Praggastis, Robert
Jasper, Nicole Nichols, and Aaron Tuor. Systematic evaluation of backdoor data poisoning attacks on image classifiers. In 2020 IEEE/CVF Conference on Computer Vision and
_Pattern Recognition, CVPR Workshops 2020, Seattle, WA, USA, June 14-19, 2020, pp. 3422–_
3431. Computer Vision Foundation / IEEE, 2020. doi: 10.1109/CVPRW50498.2020.00402.
URL [https://openaccess.thecvf.com/content_CVPRW_2020/html/w47/](https://openaccess.thecvf.com/content_CVPRW_2020/html/w47/Truong_Systematic_Evaluation_of_Backdoor_Data_Poisoning_Attacks_on_Image_Classifiers_CVPRW_2020_paper.html)
[Truong_Systematic_Evaluation_of_Backdoor_Data_Poisoning_Attacks_](https://openaccess.thecvf.com/content_CVPRW_2020/html/w47/Truong_Systematic_Evaluation_of_Backdoor_Data_Poisoning_Attacks_on_Image_Classifiers_CVPRW_2020_paper.html)
[on_Image_Classifiers_CVPRW_2020_paper.html.](https://openaccess.thecvf.com/content_CVPRW_2020/html/w47/Truong_Systematic_Evaluation_of_Backdoor_Data_Poisoning_Attacks_on_Image_Classifiers_CVPRW_2020_paper.html)

Leslie G. Valiant. A theory of the learnable. Commun. ACM, 27(11):1134–1142, 1984. doi: 10.
[1145/1968.1972. URL https://doi.org/10.1145/1968.1972.](https://doi.org/10.1145/1968.1972)

Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018.

Martin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series
in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019. doi: 10.1017/
9781108627771.

Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill,
Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose
language understanding systems. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch´e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neu_ral Information Processing Systems 32:_ _Annual Conference on Neural Information Pro-_
_cessing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp._
[3261–3275, 2019a. URL https://proceedings.neurips.cc/paper/2019/hash/](https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html)
[4496bf24afe7fab6f046bf4923da8de6-Abstract.html.](https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html)

Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding. In
_7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA,_
_[May 6-9, 2019. OpenReview.net, 2019b. URL https://openreview.net/forum?id=](https://openreview.net/forum?id=rJ4km2R5t7)_
[rJ4km2R5t7.](https://openreview.net/forum?id=rJ4km2R5t7)

Shurun Wang, Shiqi Wang, Xinfeng Zhang, Shanshe Wang, Siwei Ma, and Wen Gao. Scalable
facial image compression with deep feature reconstruction. In 2019 IEEE International Confer_ence on Image Processing, ICIP 2019, Taipei, Taiwan, September 22-25, 2019, pp. 2691–2695._
[IEEE, 2019c. doi: 10.1109/ICIP.2019.8803255. URL https://doi.org/10.1109/ICIP.](https://doi.org/10.1109/ICIP.2019.8803255)
[2019.8803255.](https://doi.org/10.1109/ICIP.2019.8803255)

Yuanyuan Wu, Eric W. T. Ngai, Pengkun Wu, and Chong Wu. Fake online reviews: Literature
review, synthesis, and directions for future research. Decis. Support Syst., 132:113280, 2020. doi:
[10.1016/j.dss.2020.113280. URL https://doi.org/10.1016/j.dss.2020.113280.](https://doi.org/10.1016/j.dss.2020.113280)

Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta. Fall of empires: Breaking byzantine-tolerant
SGD by inner product manipulation. In Amir Globerson and Ricardo Silva (eds.), Proceedings of
_the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 2019, Tel Aviv, Israel,_
_July 22-25, 2019, volume 115 of Proceedings of Machine Learning Research, pp. 261–270. AUAI_
[Press, 2019. URL http://proceedings.mlr.press/v115/xie20a.html.](http://proceedings.mlr.press/v115/xie20a.html)

Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
David J. Fleet, Tom´as Pajdla, Bernt Schiele, and Tinne Tuytelaars (eds.), Computer Vision - ECCV
_2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part_
_I, volume 8689 of Lecture Notes in Computer Science, pp. 818–833. Springer, 2014. doi: 10.1007/_
[978-3-319-10590-1\ 53.](https://doi.org/10.1007/978-3-319-10590-1_53) [URL https://doi.org/10.1007/978-3-319-10590-1_](https://doi.org/10.1007/978-3-319-10590-1_53)
[53.](https://doi.org/10.1007/978-3-319-10590-1_53)


-----

Shihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey, Jingjing Chen, and Yu-Gang Jiang. Cleanlabel backdoor attacks on video recognition models. In 2020 IEEE/CVF Conference on Computer
_Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pp. 14431–_
14440. Computer Vision Foundation / IEEE, 2020. doi: 10.1109/CVPR42600.2020.01445.
URL [https://openaccess.thecvf.com/content_CVPR_2020/html/Zhao_](https://openaccess.thecvf.com/content_CVPR_2020/html/Zhao_Clean-Label_Backdoor_Attacks_on_Video_Recognition_Models_CVPR_2020_paper.html)
[Clean-Label_Backdoor_Attacks_on_Video_Recognition_Models_CVPR_](https://openaccess.thecvf.com/content_CVPR_2020/html/Zhao_Clean-Label_Backdoor_Attacks_on_Video_Recognition_Models_CVPR_2020_paper.html)
[2020_paper.html.](https://openaccess.thecvf.com/content_CVPR_2020/html/Zhao_Clean-Label_Backdoor_Attacks_on_Video_Recognition_Models_CVPR_2020_paper.html)

Chen Zhu, W. Ronny Huang, Hengduo Li, Gavin Taylor, Christoph Studer, and Tom Goldstein.
Transferable clean-label poisoning attacks on deep neural nets. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learn_ing, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of_
_[Machine Learning Research, pp. 7614–7623. PMLR, 2019. URL http://proceedings.](http://proceedings.mlr.press/v97/zhu19a.html)_
[mlr.press/v97/zhu19a.html.](http://proceedings.mlr.press/v97/zhu19a.html)


-----

# Appendices


A CONVEXITY LEMMAS

A.1 GENERAL LEMMAS

**Definition 3. We say that f : R[d]** _→_ R is locally strongly convex if, for any convex compact set
_C ⊂_ R[d], there exists µ > 0 such that f is µ-strongly convex on C, i.e. for any x, y ∈ _C and any_
_λ ∈_ [0, 1], we have

_f_ (λx + (1 _λ)y)_ _λf_ (x) + (1 _λ)f_ (y) 2 _[.]_ (7)
_−_ _≤_ _−_ _−_ _[µ]2_ _[λ][(1][ −]_ _[λ][)][ ∥][x][ −]_ _[y][∥][2]_

_It_ _is_ _well-known_ _that_ _if_ _f_ _is_ _differentiable,_ _this_ _condition_ _amounts_ _to_ _saying_ _that_
_∥∇to sayingf_ (x) −∇ ∇[2]ff((xy)) ⪰∥2 ≥µI for allµ ∥x − xy∥ ∈2 for allC. _x, y ∈_ _C. And if f is twice differentiable, then it amounts_
**Lemma 6. If f is locally strongly convex and g is convex, then f + g is locally strongly convex.**

_Proof. Indeed, (f + g)(λx + (1 −_ _λ)y) ≤_ _λf_ (x) + (1 − _λ)f_ (y) − _[µ]2_ _[λ][(1][ −]_ _[λ][)][ ∥][x][ −]_ _[y][∥]2[2]_ [+][ λg][(][x][) +]

(1 − _λ)g(y) = λ(f + g)(x) + (1 −_ _λ)(f + g)(y) −_ _[µ]2_ _[λ][(1][ −]_ _[λ][)][ ∥][x][ −]_ _[y][∥]2[2][.]_

**Definition 4. We say that f : R[d]** _→_ R is L-smooth if it is differentiable and if its gradient is
_L-Lipschitz continuous, i.e. for any x, y ∈_ R[d],

_∥∇f_ (x) −∇f (y)∥2 ≤ _L ∥x −_ _y∥2 ._ (8)

**Lemma 7. If f is Lf** _-smooth and g is Lg-smooth, then f + g is (Lf + Lg)-smooth._

_LProof.f ∥x − Indeed,y∥2 + ∥∇ Lg ∥(fx + − gy)(∥2x = () −∇Lf( +f + L gg))( ∥yx) −∥2 ≤∥∇y∥2 ._ _f_ (x) −∇f (y)∥2 + ∥∇g(x) −∇g(y)∥2 ≤

**Lemma 8. Suppose that f : R[d]** _× R[d][′]_ _7→_ R is locally strongly convex and L-smooth, and that, for
_any x ∈_ _X, where X ⊂_ R[d] _is a convex compact subset, the map y 7→_ _f_ (x, y) has a minimum y[∗](x).
_Note that local strong convexity guarantees the uniqueness of this minimum. Then, there exists K_
_such that the function y[∗]_ _is K-Lipschitz continuous on X._

_Proof. The existence and uniqueness of y[∗](x) hold by strong convexity. Fix x, x[′]. By optimality of_
_y[∗], we know that_ _yf_ (x, y[∗](x)) = _yf_ (x[′], y[∗](x[′])) = 0. We then have the following bounds
_∇_ _∇_

_µ ∥y[∗](x) −_ _y[∗](x[′])∥2 ≤∥∇yf_ (x, y[∗](x)) −∇yf (x, y[∗](x[′]))∥2 = ∥∇yf (x, y[∗](x[′]))∥2 (9)

= _yf_ (x, y[∗](x[′])) _yf_ (x[′], y[∗](x[′])) 2 (10)
_∥∇_ _−∇_ _∥_

_f_ (x, y[∗](x[′])) _f_ (x[′], y[∗](x[′])) 2 (11)
_≤∥∇_ _−∇_ _∥_

_≤_ _L ∥(x −_ _x[′], y[∗](x[′]) −_ _y[∗](x[′]))∥2 = L ∥x −_ _x[′]∥2,_ (12)

where we first used the local strong convexity assumption, then the fact that _yf_ (x, y[∗](x)) = 0,
_∇_
then the fact that _yf_ (x[′], y[∗](x[′])) = 0, and then the L-smooth assumption.
_∇_

**Lemma 9. Suppose that f : R[d]** _× R[d][′]_ _7→_ R is locally strongly convex and L-smooth, and that,
_for any x ∈_ _X, where X ⊂_ R[d] _is a convex compact subset, the map y 7→_ _f_ (x, y) has a minimum
_y[∗](x). Define g(x) ≜_ miny _Y f_ (x, y). Then g is convex and differentiable on X and _g(x) =_
_∈_ _∇_
_xf_ (x, y[∗](x)).
_∇_

_Proof. First we prove that g is convex. Let x1, x2 ∈_ R[d], and λ1, λ2 ∈ [0, 1] with λ1 + λ2 = 1. For
any y1, y2 ∈ R[d][′], we have

_g(λ1x1 + λ2x2) = min_ (13)
_y_ R[d][′][ f] [(][λ][1][x][1][ +][ λ][2][x][2][, y][)]
_∈_

_≤_ _f_ (λ1x1 + λ2x2, λ1y1 + λ2y2) (14)
_λ1f_ (x1, y1) + λ2f (x2, y2). (15)
_≤_


-----

Taking the infimum of the right-hand side over y1 and y2 yields g(λ1x1 + λ2x2) ≤ _λ1g(x1) +_
_λ2g(x2), which proves the convexity of g._

Now denote h(x) = ∇xf (x, y[∗](x)). We aim to show that ∇g(x) = h(x). Let ε ∈ R[d] small enough
so that x + ε ∈ _X. Now note that we have_

_g(x + ε) = min_ (16)
_y_ R[d][′][ f] [(][x][ +][ ε, y][)][ ≤] _[f]_ [(][x][ +][ ε, y][∗][(][x][))]
_∈_

= f (x, y[∗](x)) + ε[T] _xf_ (x, y[∗](x)) + o( _ε_ 2) (17)
_∇_ _∥_ _∥_

= g(x) + ε[T] _h(x) + o(_ _ε_ 2), (18)
_∥_ _∥_

which shows that h(x) is a superderivative of g at x. We now show that it is also a subderivative. To
do so, first note that its value at x + ε is approximately the same, i.e.

_h(x + ε)_ _h(x)_ 2 _xf_ (x + ε, y[∗](x + ε)) _xf_ (x, y[∗](x + ε)) 2
_∥_ _−_ _∥_ _≤∥∇_ _−∇_ _∥_
+ _xf_ (x, y[∗](x + ε)) _xf_ (x, y[∗](x)) 2 (19)
_∥∇_ _−∇_ _∥_

_L_ _ε_ 2 + L _y[∗](x + ε)_ _y[∗](x)_ 2 _L +_ _[L][2]_ _ε_ 2, (20)
_≤_ _∥_ _∥_ _∥_ _−_ _∥_ _≤_  _µ_  _∥_ _∥_


where we used the L-smoothness of f and Lemma 8. Now notice that

_g(x) = min_ (21)
_y_ R[d][′][ f] [(][x, y][)][ ≤] _[f]_ [(][x, y][∗][(][x][ +][ ε][)) =][ f] [((][x][ +][ ε][)][ −] _[ε, y][∗][(][x][ +][ ε][))]_
_∈_

= f (x + ε, y[∗](x + ε)) _ε[T]_ _xf_ (x + ε, y[∗](x + ε)) + o( _ε_ 2) (22)
_−_ _∇_ _∥_ _∥_

= g(x + ε) _ε[T]_ _h(x)_ _ε[T]_ (h(x + ε) _h(x)) + o(_ _ε_ 2), (23)
_−_ _−_ _−_ _∥_ _∥_

But we know that ∥h(x + ε) − _h(x)∥2 = O(∥ε∥2). Rearranging the terms then yields_

_g(x + ε)_ _g(x) + ε[T]_ _h(x)_ _o(_ _ε_ 2), (24)
_≥_ _−_ _∥_ _∥_

which shows that h(x) is also a subderivative. Therefore, we know that g(x + _ε) = g(x)+_ _ε[T]_ _h(x)+_
_o(_ _ε_ 2), which boils down to saying that g is differentiable in x _X, and that_ _g(x) = h(x)._
_∥_ _∥_ _∈_ _∇_

**Lemma 10. Suppose that f : X × R[d][′]** _→_ R is µ-strongly convex, where X ⊂ R[d] _is closed and_
_convex. Then g : X_ R, defined by g(x) = inf _y_ _Y f_ (x, y), is well-defined and µ-strongly convex
_→_ _∈_
_too._

_Proof. The function y 7→_ _f_ (x, y) is still strongly convex, which means that it is at least equal to
a quadratic approximation around 0, which is a function that goes to infinity in all directions as
_y_ 2 . This proves that the infimum must be reached within a compact set, which implies the
_∥_ _∥_ _→∞_
_λexistence of a minimum. Thus1, λ2 ≥_ 0 with λ1 + λ2 = 1, we have g is well-defined. Moreover, for any x1, x2 ∈ _X, y1, y2 ∈_ R[d][′], and

_g(λ1x1 + λ2x2) = inf_ (25)
_y_ _[f]_ [(][λ][1][x][1][ +][ λ][2][x][2][, y][)]

_≤_ _f_ (λ1x1 + λ2x2, λ1y1 + λ2y2) (26)

_≤_ _λ1f_ (x1, y1) + λ2f (x2, y2) − _[µ]2_ _[λ][1][λ][2][ ∥][(][x][1][ −]_ _[x][2][, y][1][ −]_ _[y][2][)][∥]2[2]_ (27)

_λ1f_ (x1, y1) + λ2f (x2, y2) 2 _[,]_ (28)
_≤_ _−_ _[µ]2_ _[λ][1][λ][2][ ∥][x][1][ −]_ _[x][2][∥][2]_

where we used the µ-strong convexity of f . Taking the infimum over y1, y2 implies the µ-strong
convexity of g.

A.2 APPLICATIONS TO LOSS

Now instead of proving our theorems for different cases separately, we make the following assumptions on the components of the global loss that encompasses both ℓ[2]2 [and smooth-][ℓ][2] [regularization,]
a well as linear regression and logistic regression.
**Assumption 1. Assume that ℓ** _is convex and Lℓ-smooth, and that R(ρ, θ) = R0(ρ −_ _θ), where_
0 : R[d] R is locally strongly convex (i.e. strongly convex on any convex compact set), L 0 _-_
_R_ _→_ _R_
_smooth and satisfy_ 0(z) = Ω( _z_ 2) as _z_ 2 _._
_R_ _∥_ _∥_ _∥_ _∥_ _→∞_


-----

**Lemma 11. Under Assumption 1, LOSS is locally strongly convex and L-smooth.**

_Proof. All terms of LOSS are L0-smooth, for an appropriate value of L0. By Lemma 7, their sum_
is thus also L-smooth, for an appropriate value of L. Now, given Lemma 6, to prove that LOSS is
locally strongly convex, it suffices to prove that ν _∥θn∥2[2]_ [+] _[R][0][(][ρ]_ _[−]_ _[θ][1][)][ is locally strongly convex.]_
Consider any convex compact set C ⊂ R[d][×][(1+][N] [)]. Since R0 is locally strongly convex, we know
that there exists µ > 0 such that 0 _µI. As a result,[P]_
_∇[2]R_ _⪰_

(ρ, θ[⃗])[T][  ]∇[2]LOSS (ρ, θ[⃗]) ≥ _ν_ _∥θn∥2[2]_ [+][ µ][ ∥][ρ][ −] _[θ][1][∥]2[2]_ (29)
 _nX∈[N_ ]

= ν ∥θ1∥2[2] [+][ µ][ ∥][ρ][∥]2[2] [+][ µ][ ∥][θ][1][∥]2[2] _[−]_ [2][µρ][T][ θ][1][ +][ ν] _nX=1̸_ _∥θn∥2[2]_ _[.]_ (30)


Now define α ≜ _ν+22µµ_ [. Clearly,][ 0][ < α <][ 1][. Moreover,][ 0][ ≤] _α[1]_ _[θ][1][ −]_ _[αρ]_ 2 [=] _α1[2][ ∥][θ][1][∥]2[2]_ [+]

1
_α[2]_ _∥ρ∥2[2]_ _[−]_ [2][ρ][T][ θ][1][. Therefore]q [ 2][ρ][T][ θ][1][ ≤] _[α][2][ ∥][ρ][∥]2[2]_ [+] _α[2][ ∥][θ][1][∥]2[2][, which thus implies]_ [2]

(ρ, θ[⃗])[T][  ]∇[2]LOSS (ρ, θ[⃗]) ≥ _ν + µ_ 1 − _α[−][2][]_ _∥θ1∥2[2]_ [+][ µ] 1 − _α[2][]_ _∥ρ∥2[2]_ [+][ ν] _∥θn∥2[2]_ (31)
       _nX=1̸_

2νµ _ν_ 2νµ 2
2 [+] 2 [+][ ν] _θn_ 2 (ρ, θ[⃗]) (32)

_≥_ _[ν]2_ _[∥][θ][1][∥][2]_ _ν + 2µ_ _[∥][ρ][∥][2]_ _nX=1̸_ _∥_ _∥[2]_ _[≥]_ [min]  2 _[,]_ _ν + 2µ_  2 _[,]_

which proves that ∇[2]LOSS ⪰ _κI, with κ > 0. This shows that LOSS is locally strongly convex._

**Lemma 12. Under Assumption 1, ρ 7→** _θ[⃗][∗](ρ, D[⃗]_ ) is Lipchitz continuous on any compact set.

_Proof. Define fn(ρ, θn) ≜_ _ν ∥θn∥2[2]_ [+][ P]x∈Dn _[ℓ][(][θ][n][, x][) +][ λ][ ∥][ρ][ −]_ _[θ][n][∥]2[2][. If][ ℓ]_ [is][ L][-smooth, then][ f][n]

is clearly ( _n_ _L + ν + λ)-smooth. Moreover, if ℓ_ is convex, then for any ρ, the function θn
_|D_ _|_ _7→_
_fn(ρ, θn) is at least ν-strongly convex. Thus Lemma 8 applies, which guarantees that ρ_ _θ[∗](ρ, [⃗]_ )
_7→_ _[⃗]_ _D_
is Lipchitz.

**Lemma 13. Under Assumption 1, ρ 7→** LOSS(ρ, θ[⃗][∗](ρ, D[⃗] ), D[⃗] ) is L-smooth and locally strongly
_convex._

_Proof. By Lemma 11, the global loss is known to be L-smooth, for some value of L and locally_
strongly convex. Denoting f : ρ 7→ LOSS(ρ, θ[⃗][∗](ρ, D[⃗] ), D[⃗] ), we then have

_f_ (ρ) _f_ (ρ[′]) 2 _ρLOSS(ρ, θ[⃗][∗](ρ, [⃗]_ ), [⃗] ) _ρLOSS(ρ[′], θ[⃗][∗](ρ[′], [⃗]_ ), [⃗] ) (33)
_∥∇_ _−∇_ _∥_ _≤_ _∇_ _D_ _D_ _−∇_ _D_ _D_ 2

_L_ (ρ, θ[⃗][∗](ρ, [⃗] )) (ρ[′], θ[⃗][∗](ρ[′], [⃗] )) (34)
_≤_ _D_ _−_ _D_ 2

_≤_ _L ∥ρ −_ _ρ[′]∥2,_ (35)
which proves that f is L-smooth.

For strong convexity, note that since the global loss function is locally strongly convex, for any compact convex set C, there exists µ such that LOSS(ρ, θ, [⃗] _[⃗]_ ) is µ-strongly convex on C = (C1, C2)
_D_ _⊂_
(R[d], R[N] _[×][d]), therefore, by Lemma 10, f_ (ρ) will also be µ-strongly convex on C1 which means that
_f_ (ρ) is locally strongly convex.

B PROOF OF THE EQUIVALENCE

B.1 PROOF OF THE REDUCTION FROM MODEL ATTACK TO DATA POISONING

_Proof of Lemma 1. We omit making the dependence of the optima on_ _D[⃗]_ explicit, and we consider
any other models ρ and _θ[⃗]−s. We have the following inequalities:_

LOSSs(ρ[∗], θ[⃗][∗] _s[, θ]s[♠][, ⃗]_ ) = LOSS(ρ[∗], θ[⃗][∗], [⃗] ) (θs[∗][,][ D][s][)] (36)
_−_ _D_ _D_ _−L_

LOSS(ρ, (θ[⃗] _s, θs[∗][)][, ⃗]_ ) (θs[∗][,][ D][s][) =][ L][OSS][s][(][ρ, ⃗]θ _s, θs[♠][, ⃗]_ ), (37)
_≤_ _−_ _D_ _−L_ _−_ _D_


-----

where we used the optimality of (ρ[∗], θ[⃗][∗]) in the second line, and where we repeatedly used the fact
that θs[∗] [=][ θ]s[♠][. This proves that][ (][ρ][∗][, ⃗]θ[∗] _s[)][ is a global minimum of the modified loss.]_
_−_

B.2 PROOF OF THE REDUCTION FROM DATA POISONING TO MODEL ATTACK

First, we define the following modified loss function:

LOSSs(ρ, θ[⃗] _s, θs[♠][, ⃗]_ _s) ≜_ LOSS(ρ, (θs[♠][, ⃗]θ _s), (_ _, [⃗]_ _s))_ (38)
_−_ _D−_ _−_ _∅_ _D−_

where _θ[⃗]_ _s and_ _[⃗]_ _s are variables and datasets for users n_ = s. We then define ρ[∗](θs[♠][, ⃗] _s) and_
_−_ _D−_ _̸_ _D−_
_θ⃗−[∗]_ _s[(][θ]s[♠][, ⃗]D−s) as a minimum of the modified loss function, and θs[∗][(][θ]s[♠][, ⃗]D−s) ≜_ _θs[♠][. We now prove]_
a slightly more general version of Lemma 2, which applies to a larger class of regularizations. It
also shows how to construct the strategic’s user data poisoning attack.
**Lemma 14 (Reduction from data poisoning to model attack). Assume local PAC* learning. Sup-**
_pose also that_ _is continuous and that_ (ρ, θ) _when_ _ρ_ _θ_ 2 _. Consider any_
_datasets D−s and any attack model R_ _θs[♠]_ _[such that the modified loss] R_ _→∞_ _∥_ [ L] −[OSS]∥ _[s] →∞[has a unique minimum]_
_ρ[∗](θs[♠][, ⃗]_ _s), θ[⃗][∗]_ _s[(][θ]s[♠][, ⃗]_ _s). Then, for any ε, δ > 0, there exists_ _such that if user s’s dataset_ _s_
_D−_ _−_ _D−_ _I_ _D_
_contains at least I inputs drawn from model θs[♠][, then, with probability at least][ 1][ −]_ _[δ][, we have]_
_ρ[∗](_ _[⃗]_ ) _ρ[∗](θs[♠][, ⃗]_ _s)_ _θn[∗]_ [(][ ⃗] ) _θn[∗]_ [(][θ]s[♠][, ⃗] _s)_ (39)
_D_ _−_ _D−_ 2 _[≤]_ _[ε][ and][ ∀][n][ ̸][=][ s,]_ _D_ _−_ _D−_ 2 _[≤]_ _[ε.]_

Clearly, ℓ[2]2[,][ ℓ][2] [and smooth-][ℓ][2] [are continuous regularizations, and verify][ R][(][ρ, θ][)][ →∞] [when]
_ρ_ _θ_ 2 . Moreover, setting δ ≜ 1/2 shows that the probability that the dataset _s satis-_
_∥fies the inequalities of Lemma 14 is positive. This implies in particular that there must be a dataset −_ _∥_ _→∞_ _D_
_Ds that satisfies these inequalities. ALl in all, this shows that Lemma 14 implies Lemma 2._

_Proof of Lemma 14. Let ε, δ > 0 and θs[♠]_ _s_ _[, ⃗]_ _s) and_ _θ[⃗][♠]_ ≜ _θ[⃗][∗](θs[♠][, ⃗]_ _s)_
the result of strategic user s’s model attack. We define the compact set[∈] [R][d][. Denote][ ρ][♠] [≜] _[ρ][∗][(][θ][♠]_ _D C− by_ _D−_

_C ≜_ _ρ, θ[⃗]−s_ _ρ −_ _ρ[♠]_ 2 _[≤]_ _[ε][ ∧∀][n][ ̸][=][ s,]_ _θn −_ _θn[♠]_ 2 _[≤]_ _[ε]_ (40)
n o

We define D ≜ R[d][×][N] _C the closure of the complement of C. Clearly, ρ[♠], θ[⃗][♠]s_ _[/]_ _D. We aim_
_−_ _−_ _∈_
to show that, when strategic user s reveals a large dataset Ds whose answers are provided using the
attack model θs[♠][, then the same holds for any global minimum of the global loss][ ρ][∗][(][ ⃗] ), θ[⃗][∗] _s[(][ ⃗]_ )
_D_ _−_ _D_ _∈_
_C. Note that, to prove this, it suffices to prove that the modified loss takes too large values, even_
when θs[♠] [is replaced by][ θ]s[∗][(][ ⃗]D).

Let us now formalize this. Denote L[♠] ≜ LOSSs(ρ[♠], θ[⃗][♠]s[, θ]s[♠][, ⃗] _s). We define_
_−_ _D−_

_η ≜_ inf LOSSs(ρ, θ[⃗] _s, θs[♠][, ⃗]_ _s)_ _L[♠]._ (41)
_ρ,θ[⃗]−s∈D_ _−_ _D−_ _−_

By a similar argument as that of Lemma 5, using the assumption R →∞ at infinity, we know that
the infimum is actually a minimum. Moreover, given that the minimum of the modified loss LOSSs
is unique, we know that the value of the loss function at this minimum is different from its value at
_ρ[♠], θ[⃗][♠]s[. As a result, we must have][ η >][ 0][.]_
_−_

Now, since the function R is differentiable, it must be continuous. By the Heine–Cantor theorem,
it is thus uniformly continuous on all compact sets. Thus, there must exist κ > 0 such that, for all
models θs satisfying _θs_ _θs[♠]_ 2
_−_ _[≤]_ _[κ][, we have]_
_R(θs, ρ[♠]) −R(θs[♠][, ρ][♠][)]_ _≤_ _η/3._ (42)

Now, Lemma 5 guarantees the existence of I such that, if user s provides a dataset Ds of least I
answers with the model θs[♠][, then with probability at least][ 1][ −] _[δ][, we will have]_ _θs[∗][(][ ⃗]_ ) _θs[♠]_

min(κ, ε). Under this event, we then have _D_ _−_ 2 _[≤]_

LOSSs _ρ[♠], θ[⃗][♠]s[, θ]s[∗][(][ ⃗]_ ), [⃗] _s_ _L[♠]_ + η/3. (43)
_−_ _D_ _D−_ _≤_
 


-----

Then


inf LOSSs(ρ, θ[⃗] _s, θs[∗][(][ ⃗]_ ), [⃗] _s)_ inf LOSSs(ρ, θ[⃗] _s, θs[♠][, ⃗]_ _s)_ _η/3_ (44)
_ρ,θ[⃗]−s∈D_ _−_ _D_ _D−_ _≥_ _ρ,θ[⃗]−s∈D_ _−_ _D−_ _−_

_≥_ _L[♠]_ + η − _η/3 ≥_ _L[♠]_ + 2η/3 (45)

_> LOSSs_ _ρ[♠], θ[⃗][♠]s[, θ]s[∗][(][ ⃗]_ ), [⃗] _s_ _._ (46)
_−_ _D_ _D−_
 

This shows that there is a high probability event under which the minimum of ρ, θ[⃗]−s _7→_
LOSSs _ρ, θ[⃗]_ _s, θs[∗][(][ ⃗]_ ), [⃗] _s_ cannot be reached in D. This is equivalent to what the theorem we
_−_ _D_ _D−_
needed to prove states. 

B.3 PROOF OF REDUCTION FROM MODEL ATTACK TO GRADIENT ATTACK

In this section, we prove a slightly more general result than Lemma 3. Namely, instead of working with specific regularizations, we consider a more general class of regularizations, identified by
Assumption 1.
**Lemma 15 (Reduction from model attack to gradient attack). Under Assumption 1, if gs[t]** _[converges]_
_and if ηt = η is a constant small enough, then ρ[t]_ _will converge too. Denote ρ[∞]_ _its limit. Then for_
_any ε > 0, there is θs[♠]_ _ρ[∞]_ _ρ[∗](θs[♠][, ⃗]_ _s)_

_[∈]_ [R][d][ such that] _−_ _D−_ 2

_[≤]_ _[ε][.]_

Note that since ℓ[2]2 [and smooth-][ℓ][2] [regularizations satisfy Assumption 1, Lemma 15 clearly implies]
Lemma 3. We now introduce the key objects of the proof of Lemma 15.

Denote gs[∞] [the limit of the attack gradients][ g]s[t][. We now define]


LOSS(ρ, θ, [⃗] _[⃗]_ ) _s(θs,_ _s)_ (ρ, θs) + ρ[T] _gs[∞]_ (47)
_D_ _−L_ _D_ _−R_
o


LOSS[1]s[(][ρ][)][ ≜] [inf]
_θ⃗−s_

= inf
_θ⃗−s_


= inf _n(θn,_ _n) +_ (ρ, θn) _s_ _[,]_ (48)
_θ⃗−s_ n=s _L_ _D_ _n=s_ _R_ 

X̸ X̸ 

and prove that ρ[t] will converge to the minimizer of L OSS[1]s[(][ρ][)][.]By Lemma 13, we show that[+][ ρ][T][ g][∞]
LOSS[1]s[(][ρ][)][ is both locally strongly convex and][ L][-smooth.]

vectors received from all users assuming the strategic userNow define ζs[t] [≜] _[g]s[t]_ _[−]_ _[g]s[∞][. We then know][ ζ]s[t]_ _[→]_ [0][ and][ ∇] s sends the vector[L][OSS]s[1][(][ρ][t][)][ is the sum of all gradient] gs[∞] in all iterations.
Thus, at iteration t of the optimization algorithm, we will take one step in the direction G[t] ≜
_∇LOSS[1]s[(][ρ][t][) +][ ζ]s[t][, i.e.,]_
_ρ[t][+1]_ = ρ[t] _ηtG[t]._ (49)
_−_

We now prove the following lemma that bounds the difference between the function value in two
successive iterations.
**Lemma 16. If LOSS[1]s[(][ρ][)][ is][ L][-smooth and][ η][t]**

_[≤]_ [1][/L][, we have]

LOSS[1]s[(][ρ][t][+1][)][ −] [L][OSS][1]s[(][ρ][t][)][ ≤−] _[η]2[t]_ _G[t]_ 2 [+][ η][t][ζ]s[t]T Gt. (50)

[2]

_Proof. Since LOSS[1]s_ [is][ L][-smooth, we have]

LOSS[1]s[(][ρ][t][+1][)][ ≤] [L][OSS][1]s[(][ρ][t][) + (][ρ][t][+1][ −] _[ρ][t][)][T][ ∇][L][OSS][1]s[(][ρ][t][) +][ L]2_ _ρ[t][+1]_ _−_ _ρ[t]_ 2 _[.]_ (51)

Now plugging ρ[t][+1] _ρ[t]_ = _ηtG[t]_ and LOSS[1]s[(][ρ][t][) =][ G][t][ −] _[ζ]s[t]_ [into the inequality implies][2]
_−_ _−_ _∇_

LOSS[1]s[(][ρ][t][+1][)][ −] [L][OSS][1]s[(][ρ][t][)][ ≤] _−ηtG[t][][T][  ]G[t]_ _−_ _ζs[t]_ + _[L]2_ _−ηtG[t]_ 2 (52)

_≤− _ _[η]2[t]_ _G[t]_ 2 [+][ η][t][ζ]s[t]T G _t,_ [2] (53)

where we used the fact ηt 1/L.

[2]

_≤_


-----

B.3.1 THE GLOBAL MODEL REMAINS BOUNDED

**Lemma 17. There is M such that, for all t, LOSS[1]s[(][ρ][t][)][ ≤]** _[M]_ _[.]_

_Proof. Consider the closed ball B(ρ[∗], 1) centered on ρ[∗]_ and of radius 1. By Lemma 13, we know
that LOSS[1]s [is locally strongly convex and thus there exists a][ µ][1] _[>][ 0][ such that L][OSS][1]s_ [is][ µ][1][-strongly]
convex on B(ρ[∗], 1). Now consider a point ρ1 on the boundary of B(ρ[∗], 1). By strong convexity we
have
LOSS[1]s[(][ρ][1][)] 2 _s[(][ρ][1][)][ ≥]_ _[µ][1]_ 2 [=][ µ][1][.] (54)
_∇_ _[≥]_ [(][ρ][1][ −] _[ρ][∗][)][T][ ∇][L][OSS][1]_ _[∥][ρ][1]_ _[−]_ _[ρ][∗][∥][2]_

Now similarly, by the convexity of L[2] OSS[1]s [on][ R][d][, for any][ ρ] _∈_ R[d] _−B(ρ[∗], 1), we have_
LOSS[1]s[(][ρ][1][)] 2 _s_ [after which (][t][ ≥] _[T][1][),]_
we have∇ _ζs[t][∥]2_ _[≥√]4_ _√µ[µ]1, and thus[1][. Now since]Gt[ ζ]2[t]_ _[→]_ [0][, there exists an iteration]LOSS[1]s[(][ρ][t][)] 2 _s[∥]2_ _[ T]4[1]√µ1. Thus, Lemma 16_

implies that for ∥ _t[≤]_ [1] _T1, if_ _ρ[t]_ _ρ[∗] ∥2_ _∥_ 1 ≥, then∇ _[−∥][ζ]_ _[t]_ _[≥]_ [3]
_≥_ _∥_ _−_ _∥_ _≥_

LOSS[1]s[(][ρ][t][+1][)][ −] [L][OSS][1]s[(][ρ][t][)][ ≤−] _[η]2_ _G[t]_ 2 [+][ ηζ]s[t]T Gt (55)

_≤−_ _[η]2_ _G[t]_ [2]2 [+][ η] _ζs[t]_ 2 _G[t]_ 2 (56)

_≤−_ _[η]2_ _G[t]_ [2]2 _G[t]_ 2 _ζs[t]_ 2 (57)

_[−]_ [2]

3  3 2 
_√µ1_ _√µ1_ _√µ1_ (58)

_≤−_ _[η]2_ 4 4 _−_ 4 _≤−_ [3]32[η] _[µ][1][ <][ 0][.]_

 

Thus, for _ρ[t]_ _ρ[∗]_ 2 1, the loss cannot increase at the next iteration.
_∥_ _−_ _∥_ _≥_

Now consider the case ∥ρ[t] _−_ _ρ[∗]∥2_ _< 1 for t ≥_ _T1._ The smoothness of LOSS[1]s [implies]
_∇LOSS[1]s[(][ρ][t][)]_ 2 _[< L][. Therefore,]_
_ρ[t][+1]_ _−_ _ρ[∗]_ 2 [=] _ρ[t]_ _−_ _η(∇LOSS[1]s[(][ρ][t][) +][ ζ]s[t][)][ −]_ _[ρ][∗]_ 2 (59)

_≤_ _ρ[t][+1]_ _−_ _ρ[∗]_ 2 [+][ η][(][L][ + 1]4 _√µ1) ≤_ 1 + η(L + 14 _√µ1)._ (60)

Now we define M1 ≜ maxρ∈B(ρ∗,1+η(L+ 14 _√µ1)) LOSS[1]s[(][ρ][)][, the maximum function value in the]_

closed ball _ρ[∗], 1 + η(L +_ 4[1] _√µ1)_ . Therefore, we have LOSS[1]s[(][ρ][t][+1][)][ ≤] _[M][1][. So far we proved]_
_B_

that for t _T1, in each iteration of gradient descent either the function value will not increase_
or it will be upper-bounded by ≥   _M1. This implies that for all_ _t, the function value LOSS[1]s[(][ρ][t][)][ is]_
upper-bounded by

_M ≜_ max max LOSS[1]s[(][ρ][t][)] _, M1_ _._ (61)
t≤T1 



This concludes the proof.

**Lemma 18. There is a compact set X such that, for all t, ρ[t]** _∈_ _X._

_Proof. Now since LOSS[1]s_ [is][ µ][1][-strongly convex in][ B][(][ρ][∗][,][ 1)][, for any point][ ρ][ ∈] [R][d][ such that]
_∥ρ −_ _ρ[t]∥2 = 1, we have_

LOSS[1]s[(][ρ][)][ ≥] [L][OSS][1]s[(][ρ][∗][) +][ µ][1] 2 [=][ L][OSS]s[1][(][ρ][∗][) +][ µ][1] (62)

2 _[∥][ρ][ −]_ _[ρ][∗][∥][2]_ 2 _[.]_

But now by the convexity of LOSS[1]s [in][ R][d][, for any][ ρ][ such that][ ∥][ρ][ −] _[ρ][∗][∥]2_ _[≥]_ [1][, we have]

_µ1_
LOSS[1]s[(][ρ][)][ ≥] [L][OSS][1]s[(][ρ][∗][) +][ ∥][ρ][ −] _[ρ][∗][∥]2_ 2 _[.]_ (63)

This implies that if _ρ[t]_ _ρ[∗]_ 2 > _µ21_ _M2_ LOSS[1]s[(][ρ][∗][)], then LOSS[1]s[(][ρ][t][)][ > M][2][. Therefore, we]

is a compact set.must have ∥ρ[t] _−_ _ρ[∗] ∥∥2 ≤−µ21_ _∥M2 −_ LOSS  [1]s −[(][ρ][∗][)], for all t ≥ 0. This describes a closed ball, which
  


-----

B.3.2 CONVERGENCE OF THE GLOBAL MODEL UNDER CONVERGING GRADIENT ATTACK

**Lemma 19. Suppose ut ≥** 0 verifies ut+1 ≤ _αut + δt, with δt →_ 0. Then ut → 0.

_Proof. We now show that for any ε > 0, there exists an iteration T_ (ε), such that for t ≥ _T_ (ε), we
have ut _ε. For this, note that by induction, we observe that, for all t_ 0,
_≤_ _≥_


_ut+1_ _u0α[t][+1]_ +
_≤_


_α[τ]_ _δt−τ_ _._ (64)
_τ_ =0

X


Since δt 0, there exists an iteration T2(ε) such that for all t _T2(ε), we have δt_ 2 .

Therefore, for → _t_ _T2(ε), we have_ _≥_ _≤_ _[ε][(1][−][α][)]_
_≥_


_t−T2(ε)_

_α[τ]_ _δt_ _τ +_
_−_
_τ_ =0

X


_ut+1_ _u0α[t][+1]_ +
_≤_


_α[τ]_ _δt−τ_ (65)
_τ_ =t−XT2(ε)+1


_t−T2(ε)_

_α[τ]_ +

_τ_ =0

X


_T2(ε)−1_

_α[t][−][s]δs_ (66)
_s=0_

X


_u0α[t][+1]_ + _[ε][(1][ −]_ _[α][)]_
_≤_ 2


_T2(ε)−1_ _∞_

_u0 +_ _α[−][s][−][1]δs_ _α[t][+1]_ + _[ε][(1][ −]_ _[α][)]_ _α[τ]_ _._ (67)

_≤_   2

_s=0_ _τ_ =0

X X

 

Denoting M0(ε) ≜ [P][T]s=0[2][(][ε][)][−][1] _α[−][s][−][1]δs, we then have_

_ut+1_ (u0 + M0(ε)) α[t][+1] + _[ε]_ (68)
_≤_ 2 _[.]_


ln
Therefore, for t ≥


2(uln0+ αM0(ε)), we have

_ut+1_ (69)
_≤_ 2 [ε] [+][ ε]2 [=][ ε.]


This proves that ut 0.
_→_

We first prove the first part of Lemma 3.

**Lemma 20. Under Assumption 1 and ηt = η ≤** 1/L, if gs[t] _[converges, then so does][ ρ][t][.]_

_Proof. Define X based on Lemma 18. Since LOSS[1]s_ [is locally strongly convex, there exists][ µ][2] _[>][ 0]_
such that LOSS[1]s [is][ µ][2][-strongly convex in a convex compact set][ X][ containing][ ρ][t][ for all][ t][ ≥] [0][. By]
the strong convexity of LOSS[1]s[(][ρ][)][, we have]

LOSS[1]s[(][ρ][t][)][ −] [L][OSS][1]s[(][ρ][∗][)][ ≤] [(][ρ][t][ −] _[ρ][∗][)][T][ ∇][L][OSS][1]s[(][ρ][t][)][ −]_ _[µ]2[2]_ _ρ[t]_ _−_ _ρ[∗]_ 2 (70)

= (ρ[t] _−_ _ρ[∗])[T][  ]G[t]_ _−_ _ζs[t]_ _−_ _[µ]2[2]_ _ρ[t]_ _−_ _ρ[∗]_ 2 _[.][2]_ (71)


[2]

Now, using the fact

(ρ[t] _ρ[∗])[T]_ _G[t]_ = [1] (72)
_−_ _η_ [(][ρ][t][ −] _[ρ][∗][)][T][ (][ρ][t][ −]_ _[ρ][t][+1][)]_


= [1] _ρ[t]_ _ρ[∗]_ _ρ[t]_ _ρ[t][+1]_ _ρ[t][+1]_ _ρ[∗]_

2η _−_ 2 [+] _−_ 2 _[−]_ _−_



= [1] _η[2]_ _G[t]_ [2] _ρ[t]_ _ρ[∗]_ [2] _ρ[t][+1]_ _ρ[∗]_

2η 2 [+] _−_ 2 _[−]_ _−_ 2

 

= _[η]_ _G[t]_ [2] _ρ[t]_ _ρ[∗]_ [2] _ρ[t][+1]_ _ρ[∗]_ [2] _,_

2 2 [+ 1]2η _−_ 2 _[−]_ _−_ 2

 

[2] [2] [2]


(73)

(74)

(75)


-----

we have
LOSS[1]s[(][ρ][t][)][ −] [L][OSS][1]s[(][ρ][∗][)][ ≤] (76)
_η_
2 _G[t]_ 2 [+ 1]2η _ρ[t]_ _−_ _ρ[∗]_ 2 _[−]_ _ρ[t][+1]_ _−_ _ρ[∗]_ 2 _−_ (ρ[t] _−_ _ρ[∗])[T]_ _ζs[t]_ _[−]_ _[µ]2[2]_ _ρ[t]_ _−_ _ρ[∗]_ 2 _[.]_ (77)

 

But now note that L[2] OSS[1]s[(][ρ][t][)][−][L][OSS][2] [1]s[(][ρ][∗][)][ ≥] [L][OSS][2][1]s[(][ρ][t][)][−][L][OSS][1]s[(][ρ][t][+1][)][. Thus, combining Equation][2]
(77) and Lemma 16 yields

_−_ _ηζs[t]T Gt ≤_ 21η _ρ[t]_ _−_ _ρ[∗]_ 2 _[−]_ _ρ[t][+1]_ _−_ _ρ[∗]_ 2 _−_ (ρ[t] _−_ _ρ[∗])[T]_ _ζs[t]_ _[−]_ _[µ]2[2]_ _ρ[t]_ _−_ _ρ[∗]_ 2 _[.]_ (78)

By rearranging the terms, we then have 

[2] [2] [2]

_ρ[t][+1]_ _ρ[∗]_ 2 _ρ[t]_ _ρ[∗]_ 2 _ρ[t][+1]_ _ρ[∗][][T]_ _ζs[t]_ (79)
_−_ _[≤]_ [(1][ −] _[µ][2][η][)]_ _−_ _[−]_ _[η]_ _−_

[2] _≤_ (1 − _µ2η)_ _ρ[t]_ _−_ _ρ[∗]_ [2]2 [+][ η]  ρ[t][+1] _−_ _ρ[∗]_ 2 _ζs[t]_ 2 _[.]_ (80)

Now note that η ≤ 1/L < 1/µ2 and thus 0 < 1 −[2]µ2η < 1. We now define two sequences
converges tout ≜ _∥ρ[t]_ _−_ 0ρ[∗]. By Equation (80), we have∥2 and δt = η ∥ζs[t][∥]2[. We already know that][ δ][t] _[→]_ [0][, and we want to show][ u][t] [also]
_u[2]t+1_ _[≤]_ [(1][ −] _[ηµ][2][)][u]t[2]_ [+][ δ][t][u][t][+1][,] (81)
which implies

2

_ut+1_ = u[2]t+1 [+][ δ]t[2] _t_ [+][ δ]t[2] (82)
_−_ _[δ]2[t]_ _[−]_ _[u][t][+1][δ][t]_ 4 4 _[,]_
  _[≤]_ [(1][ −] _[ηµ][2][)][u][2]_

and thus


_ut+1_ (1 _ηµ2)u[2]t_ [+][ δ]t[2]
_≤_ r _−_ 4 [+][ δ]2[t]

_[≤]_

Lemma 19 allows to conclude.


(1 _ηµ2)u[2]t_ [+][ δ][t] 1
_−_ 2 [+][ δ]2[t] _[≤]_  _−_ _[ηµ]2_ [2]


_ut + δt._ (83)


B.3.3 REDUCTION FROM MODEL ATTACK TO CONVERGING GRADIENT ATTACK

_Proof of Lemma 15. Lemma 20 already provided the convergence part of Lemma 3. We now move_
forward to proving the second part of the theorem, showing that for any ε > 0, there exists θs[♠]

_[∈]_ [R][d]

such that _ρ[∞]_ _ρ[∗](θs[♠][, ⃗]_ _s)_
_−_ _D−_ 2 _[≤]_ _[ε][. We define]_

LOSS[2]s[(][ρ, θ][s][)][ ≜] [inf] LOSS(ρ, θ, [⃗] _[⃗]_ ) _s(θs,_ _s)_ (84)
_θ⃗−s_ _D_ _−L_ _D_

n o

= LOSS[1]s[(][ρ][) +][ R][(][ρ, θ][s][)][ −] _[ρ][T][ g]s[∞][,]_ (85)
and ρ[∗](θs), its minimizer. Therefore, we have

_∇ρLOSS[2]s[(][ρ, θ][s][) =][ ∇][ρ][L][OSS][1]s[(][ρ][) +][ ∇][ρ][R][(][ρ, θ][s][)][ −]_ _[g]s[∞][.]_ (86)

By Lemma 13, we know that LOSS[2]s [is locally strongly convex. Therefore, there exists][ µ][3] _[>][ 0][ such]_
that LOSS[2]s[(][ρ][)][ is][ µ][3][-strongly convex in][ {][ρ][| ∥][ρ][ −] _[ρ][∗][(][θ][s][)][∥]2_
_∥ρ[∞]_ _−_ _ρ[∗](θs)∥2 > ε, we then have_ _[≤]_ [1][}][. Therefore, for any][ 0][ < ε <][ 1][, if]

_ε_ _ρLOSS[2]s[(][ρ][∞][, θ]s[♠][)]_ 2 _s[(][ρ][∞][, θ]s[♠][)]_ (87)
_∇_ _[≥]_ [(][ρ][∞] _[−]_ _[ρ][∗][(][θ][s][))][T][ ∇][ρ][L][OSS][2]_

_≥_ _µ3 ∥ρ[∞]_ _−_ _ρ[∗](θs)∥2[2]_ [=][ µ][3][ ≥] _[µ][3][ε][2][,]_ (88)

and thus _ρLOSS[2]s[(][ρ][∞][, θ]s[♠][)]_ 2
_∇_ _[≥]_ _[µ][3][ε][.]_

Now since gs[∞] GRAD(ρ[∞]) there exists θs[♠] _ρ_ (ρ[∞], θs[♠][)][ −] _[g]s[∞]_ 2 _µ23ε_

which yields _∈_ _[∈]_ [R][d][ such that][3] _∇_ _R_ _[≤]_
_∇ρLOSS[2]s[(][ρ][∞][, θ]s[♠][)]_ 2 [=] _∇ρLOSS[1]s[(][ρ][∞][) +][ ∇][ρ][R][(][ρ][∞][, θ]s[♠][)][ −]_ _[g]s[∞]_ 2 (89)

= _∇ρR(ρ[∞], θs[♠][)][ −]_ _[g]s[∞]_ 2 _[≤]_ _[µ]2[3][ε]_ _[,]_ (90)

which is a contradiction. Therefore, we must have _ρ[∞]_ _ρ[∗](θs[♠][, ⃗]_ _s)_
_−_ _D−_ 2

3In fact, if gs∞ [belongs to the interior of G][RAD][(][ρ][∞][)][, we can guarantee][ ∇]ρ[R][(][ρ][∞][≤][, θ][ε][.]s[♠][) =][ g]s[∞][.]


-----

C SUM OVER EXPECTATIONS

In this section, we provide both theoretical and empirical results to argue for using a sum-based local
loss over an expectation-based local loss.

C.1 THEORETICAL ARGUMENTS

Indeed, intuitively, if one considers an expectation Ex _n [ℓ(θn, x)] rather than a sum, as is done by_
_∼D_
Hanzely et al. (2020), Dinh et al. (2020) and El-Mhamdi et al. (2021), then the weight of an honest
active user’s local loss will not increase as a node provides more and more data, which will hinder
the ability of θn to fit the user’s local data. In fact, intuitively, using an expectation wrongly yields
the same influence to any two nodes, even when one (honest) node provides a much larger dataset
_Dn than the other, and should thus intuitively be regarded as “more reliable”._

There is another theoretical argument for using the sum rather than the expectation.
Namely, if the loss is regarded as a Bayesian negative log-posterior, given a prior
exp _−_ [P]n∈[N ] _[ν][ ∥][θ][n][∥]2_ _[−]_ [P]n∈[N ] _[R][(][ρ, θ][n][)]_ on the local and global models, then the term that

fits local data should equal the negative log-likelihood of the data, given the models  (ρ, θ[⃗]). Assuming that the distribution of each data point x ∈Dn is independent from all other data points, and
depends only on the local model θn, this negative log-likelihood yields a sum over data points; not
an expectation.

C.2 EMPIRICAL RESULTS


We also empirically compared the performances of sum as opposed to the expectation. To do so, we
constructed a setting where 10 “idle” users draw randomly 10 data points from the FashionMNIST
dataset, while one “active” user has all of the FashionMNIST dataset (60,000 data points). We
then learned local and global models, with R(ρ, θ) ≜ _λ ∥ρ −_ _θ∥2[2][,][ λ][ = 1][. We compared two]_
different classifiers to which we refer as a “linear model” and “2-layers neural network”, both using
_CrossEntropy loss. The linear model has (784+1)×10 parameters. The neural network has 2 layers_
of 784 parameters with bias, with ReLU activation in between, adding up to ((784 + 1) × 784 +
(784 + 1) × 10.

Note also that, in all our experiments, we did not consider any local regularization, i.e. we set ν ≜ 0.
All our experiments are seeded with seed 999.

C.2.1 NOISY FASHIONMNIST

To see a strong difference between sum and average, we made the FashionMNIST dataset harder
to learn, by randomly labeling 60% of the training set. Table 1 reports the accuracy of local and
global models in the different settings. Our results clearly and robustly indicate that the use of sums
outperforms the use of expectations.

|Col1|EL|ΣL|ENN|ΣNN|
|---|---|---|---|---|
|idle user’s model|0.52|0.80|0.55|0.79|
|active user’s model|0.58|0.80|0.56|0.79|
|global model|0.55|0.80|0.58|0.79|


EL ΣL ENN ΣNN

idle user’s model 0.52 0.80 0.55 0.79

active user’s model 0.58 0.80 0.56 0.79

global model 0.55 0.80 0.58 0.79


Table 1: Accuracy of trained models, depending on the use of expectation (denoted E) or sum (Σ),
and on the use of linear classifier (L) or a 2-layer neural net (NN ). Here, all users are honest and
an ℓ[2]2 [regularization is used, but there is a large heterogeneity in the amount of data per user.]

On each of the following plots, we display the top-1 accuracy on the MNIST test dataset (10 000
images) for the active user, for the global model and for one of the idle users (in Table 1, the mean
accuracy for idle users is reported), as we vary the value of λ. Intuitively, λ models how much we
want the local models to be similar.


-----

In the case of learning FashionMNIST, given that the data is i.i.d., larger values of λ are more
meaningful (though our experiments show that they may hinder convergence speed). However,
in settings where users have different data distributions, e.g. because the labels depend on users’
preferences, then smaller values of λ may be more relevant.

Note that the use of a common value of λ in both cases is slightly misleading, as using the sum
intuitively decreases the comparative weight of the regularization term. To reduce this effect, for
this experiment only, we divide the local losses by the average of the number of data points per
node for the sum version. This way, if the number of points is equal for all nodes, the two losses
will be exactly the same. What’s more, our experiments seem to robustly show that using the sum
consistently outperforms the expectation, for both a linear classifier and a 2-layer neural network,
for the problem of noisy FashionMNIST classification.


1.0

0.8


1.0

0.8


0.6

0.4

0.2

0.0


0.6

0.4

0.2

0.0

|Col1|Col2|Col3|
|---|---|---|
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
|||acc_big|
|||acc_small|
|||acc_glob|

|Col1|Col2|Col3|
|---|---|---|
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
|||acc_big|
|||acc_small|
|||acc_glob|


25 50 75 100 125 150 175 200

acc_big
acc_small
acc_glob

Epochs

(a) Using the average


25 50 75 100 125 150 175 200

acc_big
acc_small
acc_glob

Epochs

(b) Using the sum


Figure 4: Linear model on noisy FashionMNIST, for λ = 0.01.

1.0


1.0

0.8


0.8

0.6


0.6

0.4


0.4

0.2

0.0


0.2

0.0

|acc|_big|
|---|---|
|acc|_small|
|||
|acc|_glob|
|||
|||
|||
|||
|||
|||
|||
|||
|||
|||
|||
|||
|||
|||
|||

|ac|c_big|Col3|Col4|
|---|---|---|---|
|ac|c_small|||
|||||
|ac|c_glob|||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||


25 50 75 100 125 150 175 200

acc_big
acc_small
acc_glob

Epochs

(a) Using the average


25 50 75 100 125 150 175 200

acc_big
acc_small
acc_glob

Epochs

(b) Using the sum


Figure 5: 2-layer neural network on noisy FashionMNIST, for λ = 0.01.


-----

1.0

0.8

0.6

0.4


1.0

0.8

0.6

0.4


0.2

0.0


0.2

0.0

|Col1|Col2|acc_big|
|---|---|---|
|||acc_small|
||||
|||acc_glob|
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||

|ac|c_big|
|---|---|
|ac|c_small|
|||
|ac|c_glob|
|||
|||
|||
|||
|||
|||
|||
|||
|||
|||
|||
|||
|||


25 50 75 100 125 150 175 200

acc_big
acc_small
acc_glob

Epochs

(a) Using the average


25 50 75 100 125 150 175 200

acc_big
acc_small
acc_glob

Epochs

(b) Using the sum


Figure 6: Linear model on noisy FashionMNIST, for λ = 0.1.

1.0


1.0

0.8


0.8

0.6


0.6

0.4


0.4

0.2


0.2

0.0


0.0

|Col1|Col2|Col3|acc_big|
|---|---|---|---|
||||acc_small|
||||acc_glob|
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||

|ac|c_big|
|---|---|
|ac|c_small|
|ac|c_glob|
|||
|||
|||
|||
|||
|||
|||
|||
|||
|||
|||
|||
|||
|||
|||


25 50 75 100 125 150 175 200

acc_big
acc_small
acc_glob

Epochs

(a) Using the average


25 50 75 100 125 150 175 200

acc_big
acc_small
acc_glob

Epochs

(b) Using the sum


Figure 7: 2-layer neural network on noisy FashionMNIST, for λ = 0.1.

1.0


1.0

0.8


0.8

0.6


0.6

0.4


0.4

0.2

0.0


0.2

0.0

|Col1|Col2|acc_big|
|---|---|---|
|||acc_small|
||||
|||acc_glob|
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||

|ac|c_big|
|---|---|
|ac|c_small|
|||
|ac|c_glob|
|||
|||
|||
|||
|||
|||
|||
|||
|||
|||
|||
|||
|||


25 50 75 100 125 150 175 200

acc_big
acc_small
acc_glob

Epochs

(a) Using the average


25 50 75 100 125 150 175 200

acc_big
acc_small
acc_glob

Epochs

(b) Using the sum


Figure 8: Linear model on noisy FashionMNIST, for λ = 1.


-----

1.0

0.8

0.6

0.4


1.0

0.8

0.6

0.4


0.2

0.0


0.2

0.0

|Col1|Col2|acc_big|
|---|---|---|
|||acc_small|
||||
|||acc_glob|
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||

|ac|c_big|Col3|
|---|---|---|
|ac|c_small||
||||
|ac|c_glob||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||


25 50 75 100 125 150 175 200

acc_big
acc_small
acc_glob

Epochs

(a) Using the average


25 50 75 100 125 150 175 200

acc_big
acc_small
acc_glob

Epochs

(b) Using the sum


Figure 9: 2-layer neural network on noisy FashionMNIST, for λ = 1.

1.0


1.0

0.8


0.8

0.6


0.6

0.4


0.4

0.2


0.2

0.0


0.0

|Col1|Col2|acc_big|
|---|---|---|
|||acc_small|
||||
|||acc_glob|
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||

|Col1|Col2|acc_big|
|---|---|---|
|||acc_small|
||||
|||acc_glob|
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||


25 50 75 100 125 150 175 200

acc_big
acc_small
acc_glob

Epochs

(a) Using the average


25 50 75 100 125 150 175 200

acc_big
acc_small
acc_glob

Epochs

(b) Using the sum


Figure 10: Linear model on noisy FashionMNIST, for λ = 10.

1.0


1.0

0.8


0.8

0.6


0.6

0.4


0.4

0.2

0.0


0.2

0.0

|Col1|Col2|acc_big|
|---|---|---|
|||acc_small|
||||
|||acc_glob|
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||

|Col1|Col2|acc_big|
|---|---|---|
|||acc_small|
||||
|||acc_glob|
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||


25 50 75 100 125 150 175 200

acc_big
acc_small
acc_glob

Epochs

(a) Using the average


25 50 75 100 125 150 175 200

acc_big
acc_small
acc_glob

Epochs

(b) Using the sum


Figure 11: 2-layer neural network on noisy FashionMNIST, for λ = 10.


-----

1.0

0.8

0.6

0.4


1.0

0.8

0.6

0.4


0.2

0.0


0.2

0.0

|Col1|Col2|acc_big|
|---|---|---|
|||acc_small|
||||
|||acc_glob|
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||

|Col1|Col2|acc_big|
|---|---|---|
|||acc_small|
||||
|||acc_glob|
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||


25 50 75 100 125 150 175 200

acc_big
acc_small
acc_glob

Epochs

(a) Using the average


25 50 75 100 125 150 175 200

acc_big
acc_small
acc_glob

Epochs

(b) Using the sum


Figure 12: Linear model on noisy FashionMNIST, for λ = 100.

1.0


1.0

0.8


0.8

0.6


0.6

0.4


0.4

0.2

0.0


0.2

0.0

|Col1|Col2|Col3|acc_big|
|---|---|---|---|
||||acc_small|
|||||
||||acc_glob|
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||

|Col1|Col2|acc_big|
|---|---|---|
|||acc_small|
||||
|||acc_glob|
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||


25 50 75 100 125 150 175 200

acc_big
acc_small
acc_glob

Epochs

(a) Using the average


25 50 75 100 125 150 175 200

acc_big
acc_small
acc_glob

Epochs

(b) Using the sum


Figure 13: 2-layer neural network on noisy FashionMNIST, for λ = 100.

C.2.2 FASHIONMNIST WITHOUT NOISE


Recall that we introduced noise into FashionMNIST to make the problem harder to learn and observe a clear difference between the average and the sum. In this section, we present results of our
experiments when the noise is removed.


1.0

0.8


1.0

0.8


0.6

0.4


0.6

0.4


0.2

0.0


0.2

0.0

|Col1|Col2|acc_big|
|---|---|---|
|||acc_small|
||||
|||acc_glob|
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||

|Col1|Col2|Col3|
|---|---|---|
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
|||acc_big|
|||acc_small|
|||acc_glob|


200 400 600 800 1000

acc_big
acc_small
acc_glob

Epochs

(a) Using the average


200 400 600 800 1000

acc_big
acc_small
acc_glob

Epochs

(b) Using the sum


Figure 14: Linear model on FashionMNIST (without noise), for λ = 1.


-----

1.0

0.8

0.6

0.4


1.0

0.8

0.6

0.4


0.2

0.0


0.2

0.0

|acc_big|Col2|Col3|
|---|---|---|
|acc_small|||
||||
|acc_glob|||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||

|Col1|Col2|Col3|
|---|---|---|
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
|||acc_big|
|||acc_small|
|||acc_glob|


250 500 750 1000 1250 1500 1750 2000

acc_big
acc_small
acc_glob

Epochs

(a) Using the average


250 500 750 1000 1250 1500 1750 2000

acc_big
acc_small
acc_glob

Epochs

(b) Using the sum


Figure 15: 2-layer neural network on FashionMNIST (without noise), for λ = 1.

Even without noise, the difference between using the sum and using the expectation still seems
important. We acknowledge, however, that the plots suggest that even though we ran this experiment
for 10 times more (and 5 times more for the linear model) than other experiments, we might not have
reached convergence yet, and that the use of the expectation might still eventually gets closer to the
case of sum. We believe that the fact that the difference between sum and expectation in the absence
of noise is weak is due to the fact that the FashionMNIST dataset is sufficiently linearly separable.
Thus, we achieve a near-zero loss in both cases, which make the sum and the expectation close at
optimum.

Even in this case, however, we observed that the sum clearly outperforms the expectation especially,
in the first epochs. We argue that the reason for this is the following. By taking the average in local
losses, the weights of the data of idle nodes are essentially blown out of proportion. As a result, the
optimizer will very quickly fit these data. However, the signal from the data of the active node will
then be too weak, so that the optimizer has to first almost perfectly fit the idle nodes’ data before it
can catch the signal of the active node’s data and hence the average achieves weaker convergence
performances than the sum.


D LINEAR REGRESSION AND CLASSIFICATION ARE GRADIENT PAC*

Throughout this section, we use the following terminology.


**Definition 5. Consider a parameterized event E(I). We say that the event E occurs with high**
_probability if P [E(I)] →_ 1 as I →∞.

D.1 PRELIMINARIES


Define ∥Σ∥2 ≜ max∥x∥2=0̸ (∥Σx∥2 / ∥x∥2) the ℓ2 operator norm of the matrix Σ. For symmetric
matrices Σ, this is also the largest eigenvalue in absolute value.

**Theorem 6 (Covariance concentration, Theorem 6.5 in Wainwright (2019)). Denote Σ** =
E _QiQi[T]_ _, where Qi ∈_ R[d] _is from a σQ_ _-sub-Gaussian random distribution_ _Q[˜]_ _. Then, there are_
_universal constants_  _c1, c2 and c3 such that, for any set {Qi}i∈[I] of i.i.d. samples from_ _Q[˜]_ _, and any_

_δ > 0, the sample covariance_ Σ = [1] _i_ _Ti_ _[satisfies the bound]_

_I_ _Q_ _Q_
P

1 _d_

[b]

P Σ Σ + δ _c2 exp_ _c3_ min(δ, δ[2]) _._ (91)

" _σQ[2]_ _−_ 2 _[≥]_ _[c][1]_ r _I_ [+][ d]I ! # _≤_ _−_ _I_

  

**Theorem 7 (Weyl’s Theorem, Theorem 4.3.1 in Horn & Johnson (2012))[ˆ]** **. Let A and B be Hermi-**
_tian[4]_ _and let the respective eigenvalues of A and B and A + B be {λi(A)}i[d]=1[,][ {][λ][i][(][B][)][}][d]i=1[, and]_

4For real matrices, Hermitian is the same as symmetric.


-----

_{λi(A + B)}i[d]=1[, each increasingly ordered. Then]_

_λi(A + B) ≤_ _λi+j(A) + λd−j(B),_ _j = 0, 1, ..., d −_ _i,_ (92)

_and_
_λi+j(A) + λj+1(B)_ _λi(A + B),_ _j = 0, ..., i_ 1, (93)
_≤_ _−_
_for each i = 1, ..., d._
**Lemma 21. Consider two symmetric definite positive matrices S and Σ. Denote ρmin and λmin**
_their minimal eigenvalues. Then_ _ρmin_ _λmin_ _S_ Σ 2.
_|_ _−_ _| ≤∥_ _−_ _∥_

_Proof. This is a direct consequence of Theorem 7, for A = S, B = Σ −_ _S, i = 1, and j = 0._

**Corollary 1. There are universal constants c1, c2 and c3 such that, for any σ** _-sub-Gaussian vector_
_Q_
_distribution_ _Q ∈[˜]_ R[d] _and any δ > 0, the sample covariance_ Σ = _I[1]_ _QiQTi_ _[satisfies the bound]_

1 _d_ P

P min SP(Σ)[ˆ] min SP(Σ) _c1_ + δ[b] _c2 exp_ _c3_ min(δ, δ[2]) _,_ (94)

" _σQ[2]_ _−_ _≥_ r _I_ [+][ d]I ! # _≤_ _−_ _I_

  

_where min SP(Σ)[ˆ]_ _and min SP(Σ) are the minimal eigenvalues of_ Σ[ˆ] _and Σ._

_Proof. This follows from Theorem 6 and Lemma 21._

**Lemma 22. With high probability, min SP(Σ)[ˆ]** _≥_ min SP(Σ)/2.

_Proof. Denote λmin ≜_ min SP(Σ) and _λmin ≜_ min SP(Σ)[ˆ] . Since each Qi is drawn i.i.d. from a
_σ_ -sub-Gaussian, we can apply Corollary 1. Namely, there are constants c1, c2 and c3, such that for
_Q_
any δ > 0, we have

[b]

_d_

P _λmin_ _λmin_ _c1σ[2]_ + δσ[2] _c2 exp (_ _c3_ min _δ, δ[2]_ ). (95)

" _−_ _≥_ _Q_ r _I_ [+][ d]I ! _Q_ # _≤_ _−_ _I_



[b] _d_

We now set δ ≜ _λmin/(4σ[2]_ [+][ d]
_Q_ [)][ and we consider][ I][ large enough so that][ c][1] _I_ _I_ _≤_

_λmin/(4σQ[2]_ [)][. With high probability, we then have][ b]λmin ≥ _λmin/2._ q 

D.2 LINEAR REGRESSION IS GRADIENT-PAC*

In this section, we prove the first part of Lemma 4. Namely, we prove that linear regression is
gradient-PAC* learning.

D.2.1 LEMMAS FOR LINEAR REGRESSION

Before moving to the main proof that linear regression is gradient-PAC*, we first prove a few useful
lemmas. These lemmas will rest on the following well-known theorems.
**Theorem 8 (Lemma 2.7.7 in Vershynin (2018)). If X and Y are sub-Gaussian, then XY is sub-**
_exponential._
**Theorem 9 (Equation 2.18 in Wainwright (2019)). If X1, . . ., X** _are iid sub-exponential variables,_
_I_
_then there exist constants c4, c5 such that, for all I, we have_

_∀t ∈_ [0, c4], P [|X − E [X]| ≥ _tI] ≤_ 2 exp _−c5It[2][]_ _._ (96)

**Lemma 23. For all j ∈** [d], the random variables Xi ≜ _ξiQi[j] are iid, sub-exponential and have _
_zero mean._

_Proof. The fact that these variables are iid follows straightforwardly from the fact that the noises_
_ξi are iid, and the queries Qi are also iid. Moreover, both are sub-Gaussian, and by Theorem 8,_
the product of sub-Gaussian variables is sub-exponential. Finally, we have E [X] = E [ξQ[j]] =
E [ξ] E [Q[j]] = 0, using the independence of the noise and the query, and the fact that noises have
zero mean (E [ξ] = 0).


-----

_i∈I_ _[ξ][i][Q][i]_ 2

_[≤]_ _[B][I]_ [3][/][4][ with high probability.]


**Lemma 24. There exists B such that**


_Proof. By Lemma 23, the terms ξi_ _i[j] are iid, sub-exponential and have zero mean. Therefore, by_
_Q_
Theorem 9, there exist constants c4 and c5 such that for any coordinate j ∈ [d] of ξiQi and for all
0 _u_ _c4, we have_
_≤_ _≤_

P "Xi∈I _ξiQi[j]_ _[≥I][u]#_ _≤_ 2 exp (−c5Iu[2]). (97)

Plugging u = vI [(][−][1][/][4)] into the inequality for some small enough constant v, and using union bound
then yields


_v√d_ P
_≥I_ [(3][/][4)]  _≤_
2



_d yields the lemma._


_≥I_ [(3][/][4)]v


2d exp ( _c5_

 _≤_ _−_


_Iv[2])._ (98)


P _ξiQi_

Xi∈I



Defining B ≜ _v_


_ξi_ _i_
_Q_

Xi∈I


D.2.2 PROOF THAT LINEAR REGRESSION IS GRADIENT-PAC*

We now move on to proving that least square linear regression is gradient-PAC*.

_Proof of Theorem 2. Note that_ _θℓ(θ,_ _,_ ) = (θ[T] ) . Thus, on input i [ ], we have
_∇_ _Q_ _A_ _Q −A_ _Q_ _∈_ _I_

_θℓ(θ,_ _i,_ ( _i, θ[†])) =_ (θ _θ[†])[T]_ _i_ _i_ _ξi_ _i._ (99)
_∇_ _Q_ _A_ _Q_ _−_ _Q_ _Q_ _−_ _Q_

Moreover, we have   

(θ − _θ[†])[T]_ _∇θ_ _ν ∥θ∥2[2]_ = 2ν(θ − _θ[†])[T]_ _θ = 2ν_ _θ −_ _θ[†]_ 2 [+ 2][ν][(][θ][ −] _[θ][†][)][T][ θ][†][.]_ (100)
 

As a result, we have [2]

(θ _θ[†])[T]_ _θ_ (θ, ) = (101)
_−_ _∇_ _L_ _D_

_I(θ −_ _θ[†])[T][ b]Σ(θ −_ _θ[†]) −_ (θ − _θ[†])[T]_ Xi∈I _ξiQi!_ + 2ν _θ −_ _θ[†]_ 2 [+ 2][ν][(][θ][ −] _[θ][†][)][T][ θ][†][.]_ (102)

[2]

But now, with high probability, we have (θ−θ[†])[T][ b]Σ(θ−θ[†]) ≥ (λmin/2) _θ −_ _θ[†]_ 2 [(Lemma 22) and]

_i∈I_ _[ξ][i][Q][i]_ 2 _θ[†]_ 2
inequality, we have[≤] _[B][I]_ [(3][/][4)][ (Lemma 24). Using the fact that] _[≤K][ and the Cauchy-Schwarz][2]_

[P]

(θ − _θ[†])[T]_ _∇θL(θ, D) ≥_ ( _[λ][min]2_ _I + ν)_ _θ −_ _θ[†]_ 2 _[−]_ [(][B][I] [(3][/][4)][ + 2][ν][K][)] _θ −_ _θ[†]_ 2 _[.]_ (103)

Denoting AK ≜ _[λ][min]2_ and BK ≜ _B + 2νK and using the fact that[2]_ _I ≥_ 1, we then have

(θ − _θ[†])[T]_ _∇θL(θ, D) ≥_ _AKI_ _θ −_ _θ[†]_ 2 _[−]_ _[B][K][I]_ [(3][/][4)] _θ −_ _θ[†]_ 2 (104)

_≥_ _AKI min_ _θ −[2]_ _θ[†]_ 2 _[,]_ _θ −_ _θ[†]_ 2 _−_ _BKI_ [(3][/][4)] _θ −_ _θ[†]_ 2 _[,]_ (105)
n o

with high probability. This corresponds to saying Assumption 2 is satisfied for[2] _α = 3/4._

D.3 LOGISTIC REGRESSION

In this section, we now prove the second part of Lemma 4. Namely, we prove that logistic regression
is gradient-PAC* learning.


-----

D.3.1 LEMMAS ABOUT THE SIGMOID FUNCTION

We first prove two useful lemmas about the following logistic distance function.

**Definition 6. We define the logistic distance function by ∆(a, b) ≜** (a − _b) (σ(a) −_ _σ(b))._

**Lemma 25. If a, b ∈** R such that for some k > 0, |a| ≤ _k and |b| ≤_ _k, then there exists some_
_constant ck > 0 such that_
∆(a, b) _ck_ _a_ _b_ _._ (106)
_≥_ _|_ _−_ _|[2]_

_Proof. Note that the derivative of σ(z) is strictly positive, symmetric (σ[′](z) = σ[′](−z)) and mono-_
tonically decreasing for z ≥ 0. Therefore, for any z ∈ [−k, k], we know σ[′](z) ≥ _ck ≜_ _σ[′](k). Thus,_
by the mean value theorem, we have

_σ(a)_ _σ(b)_
_−_ _ck._ (107)

_a_ _b_ _≥_
_−_

Multiplying both sides by (a − _b)[2]_ then yields the lemma.

**Lemma 26. If b ∈** R, and |b| ≤ _k, for some k > 0, then there exists a constant dk, such that for any_
_a ∈_ R, we have
∆(a, b) _dk_ _a_ _b_ _dk_ (108)
_≥_ _|_ _−_ _| −_

_Proof. Assume |a −_ _b| ≥_ 1 and define dk ≜ _σ(k + 1) −_ _σ(k). If b ≥_ 0, since σ[′](z) is decreasing
for z 0, we have σ(b) _σ(b_ 1) _σ(b + 1)_ _σ(b)_ _dk, and by symmetry, a similar argument_
_≥_ _−_ _−_ _≥_ _−_ _≥_
holds for b ≤ 0. Thus, we have

_σ(a)_ _σ(b)_ min _σ(b)_ _σ(b_ 1), σ(b + 1) _σ(b)_ _dk._ (109)
_|_ _−_ _| ≥_ _{_ _−_ _−_ _−_ _} ≥_

Therefore,
(a _b) (σ(a)_ _σ(b))_ _dk_ _a_ _b_ _dk_ _a_ _b_ _dk._ (110)
_−_ _−_ _≥_ _|_ _−_ _| ≥_ _|_ _−_ _| −_

For the case of _a_ _b_ 1, we also have (a _b) (σ(a)_ _σ(b))_ 0 _dk_ _a_ _b_ _dk._
_|_ _−_ _| ≤_ _−_ _−_ _≥_ _≥_ _|_ _−_ _| −_

D.3.2 A UNIFORM LOWER BOUND

**Definition 7. Denote S[d][−][1]** ≜ **u ∈** R[d] _∥u∥2 = 1_ _the hypersphere in R[d]._

**Lemma 27. Assume SUPP( Q[˜]) spans R[d]. Then, for all u ∈** S[d][−][1], E _Q[T]_ **u** _> 0._
 []

_Proof.such that Let u u is colinear with ∈_ S[d][−][1]. We know that there existsαjQj. In particular, we then have Q1, . . ., Qd ∈ u[T]S[ P]UPPα( jQQ[˜])j and = _αα1, . . ., αj(Qj[T]_ **[u]d[)] ∈[ ̸][= 0]R[.]**
Therefore, there must be a query SUPP( [˜]) such that _>_
_Q∗_ _∈_ _Q_ _Q∗[T]_ **[u][ ̸][= 0][, which implies][ a][ ≜]** _Q∗[T]_ **[u]**
0 By continuity of the scalar product, there must then also exist[P] _ε > 0 such that, for any[P]_
_Q ∈_
( _, ε), we have_ **u** _a/2, where_ ( _, ε) is an Euclidean ball centered on_ and of radius
_B_ _Q∗_ _Q[T]_ _≥_ _B_ _Q∗_ _Q∗_
_ε._

But now, by definition of the support, we know that p ≜ P [ ( _, ε)] > 0. By the law of total_
_Q ∈B_ _Q∗_
expectation, we then have

E **u** = E **u** ( _, ε)_ P [ ( _, ε)]_
_Q[T]_ _Q[T]_ _Q ∈B_ _Q∗_ _Q ∈B_ _Q∗_
  + E **u** _/_ ( _, ε)_ P [ _/_ ( _, ε)]_ (111)

[] _Q[T]_ _Q_ _∈B_ _Q∗_ _Q_ _∈B_ _Q∗_

_ap/2 + 0 > 0,_ (112)

 

_≥_

which is the lemma.

**Lemma 28. Assume that, for all unit vectors u ∈** S[d][−][1], we have E _Q[T]_ **u** _> 0, and that SUPP( Q[˜])_
_is bounded by M_ _. Then there exists C > 0 such that, with high probability,_
_Q_  []

_∀u ∈_ S[d][−][1], _Qi[T]_ **[u]** _≥_ _CI._ (113)
Xi∈I


-----

_Proof. By continuity of the scalar product and the expectation operator, and by compactness of_
S[d][−][1], we know that
_C0 ≜_ **uinfR[d][ E]** _Q[T]_ **u** _> 0._ (114)
_∈_

Now define ε ≜ _C0/4MQ_ . Note that S[d][−][1] _⊂[S]u∈S[d][−][]_ [1][ B][(][u][, ε][)][. Thus we have a covering of]

the hypersphere by open sets. But since S[d][−][1] is compact, we know that we can extract a finite
covering. In other words, there exists a finite subset S S[d][−][1] such that S[d][−][1] **u** _S_
_⊂_ _⊂_ [S] _∈_ _[B][(][u][, ε][)][. Put]_

differently, for any v S[d][−][1], there exists u _S such that_ **u** **v** 2 _ε._
_∈_ _∈_ _∥_ _−_ _∥_ _≤_

Now consider u _S. Given that SUPP( [˜]) is bounded, we know that_ _i_ **[u]** [0, M ]. Moreover,
_∈_ _Q_ _Q[T]_ _∈_ _Q_
such variables _Qi[T]_ **[u]** are iid. By Hoeffding’s inequality, for any t > 0, we have

2 _t2_

P _i_ **[u]** E **u** 2 exp _−_ _I_ _._ (115)

Choosing t = C0/2 then yields"Xi∈I _Q[T]_ _−I_ Q[T] [] _[≥I][t]#_ _≤_  _MQ_ 

P _i_ **[u]** P _i_ **[u]θ** _θ[†]_ E **uθ** _θ†_ (116)

"Xi∈I _Q[T]_ _≤_ _[C]2[0][I]_ # _≤_ "Xi∈I _Q[T]C02−_ _−I_ Q[T] _−_ [] _[≥I][C]2_ [0] #

2 exp _−I_ _._ (117)
_≤_ 2M
 _Q_ 

Taking a union bound for u ∈ _S then guarantees_

_C02_

P **u** _S,_ _i_ **[u]** 1 2 _S_ exp _−I_ _,_ (118)

"∀ _∈_ _i_ _Q[T]_ _≥_ _[C]2[0][I]_ # _≥_ _−_ _|_ _|_  2MQ 

X∈I

which clearly goes to 1 as . Thus **u** _S,_ _i_ _i_ **[u]** 2 holds with high probability.
_I →∞_ _∀_ _∈_ _∈I_ _Q[T]_ _≥_ _[C][0][I]_

Now consider v S[d][−][1]. We know that there exists u _S such that_ **u** **v** 2 _ε. Then, we have_
_∈_ [P] _∈_ _∥_ _−_ _∥_ _≤_

_Qi[T]_ **[v]** = _Qi[T]_ **[u][ +][ Q]i[T]** [(][v][ −] **[u][)]** (119)

_iX∈[I]_ _iX∈[I]_

_i_ **[u]** _M_ **v** **u** 2 (120)

_≥_ _Q[T]_ _−I_ _Q ∥_ _−_ _∥_

_iX∈[I]_

_C0_
_M_ = _[C][0][I]_ (121)

_≥_ _[C]2[0][I]_ _−I_ _Q_ 4M 4 _[,]_

_Q_

which proves the lemma.

D.3.3 LOWER BOUND ON THE DISCREPANCY BETWEEN PREFERRED AND REPORTED
ANSWERS


**Lemma 29. Assume that** _Q[˜] has a bounded support, whose interior contains the origin. Suppose_
_also that_ _θ[†]_
2

_[≤K][. Then there exists][ A][K][ such that, with high probability, we have]_

∆(Qi[T] _[θ,][ Q]i[T]_ _[θ][†][)][ ≥]_ _[A][K][I][ min]_ _θ −_ _θ[†]_ 2 _[,]_ _θ −_ _θ[†]_ 2 _._ (122)
_i_ [ ]

X∈ _I_ n o

[2]

_Proof. Note that by Cauchy-Schwarz inequality we have_
_i_ _[θ][†]_ _i_ 2 _θ[†]_ 2 (123)
_Q[T]_ _≤∥Q_ _∥_ _[≤]_ _[M][Q]_ _[K][.]_

Thus, Lemma 26 implies the existence of a positive constant d, such that for all θ R[d], we have
_K_ _∈_

∆ _i_ _[θ,][ Q]i[T]_ _[θ][†][]_ _d_ _i_ _[θ][ −Q]i[T]_ _[θ][†]_ _d_ (124)
_Q[T]_ _≥_ _K_ _Q[T]_ _−_ _K_

Xi∈I   Xi∈I   

= −dKI + dK _θ −_ _θ[†]_ 2 _Qi[T]_ **[u]θ−θ[†]** _,_ (125)
Xi∈I


-----

where uθ−θ† ≜ (θ − _θ[†])/_ _θ −_ _θ[†]_ 2 [is the unit vector in the direction of][ θ][ −] _[θ][†][.]_

Now, by Lemma 28, we know that, with high probability, for all unit vectors u ∈ S[d][−][1], we have
_Qi[T]_ **[u]** _≥_ _CI. Thus, for I sufficiently large, for any θ ∈_ R[d], with high probability, we have
P

Xi∈I ∆(Qi[T] _[θ,][ Q]i[T]_ _[θ][†][)][ ≥]_ _[d][K][C]2[min]_ _I_ _θ −_ _θ[†]_ 2 _[−]_ _[d][K][I][.]_ (126)

Defining eK ≜ _[d][K][C]4[min]_, and fK ≜ _Cmin4_ [, for] _θ −_ _θ[†]_ 2 _[> f][K][, we then have]_

∆(Qi[T] _[θ,][ Q]i[T]_ _[θ][†][)][ ≥]_ _[e][K][I]_ _θ −_ _θ[†]_ 2 _[.]_ (127)

Xi∈I

We now focus on the case of _θ −_ _θ[†]_ 2 _[≤]_ _[f][K][. The triangle inequality yields][ ∥][θ][∥][2]_ _[≤]_ _θ −_ _θ[†]_ 2 [+]
_θ[†]i_ _[θ]2[†]_ _[≤]_ _[f][K][ +]M[ K][. By Cauchy-Schwarz inequality, we then have]g_ . Thus, by Lemma 25, we know there exists some constantQi[T] _[θ]_ _≤_ (fK + K c)M such thatQ ≜ _gK and_
_Q[T]_ _≤K_ _Q ≤_ _K_ _K_

_σ(_ _i_ _[θ][)][ −]_ _[σ][(][Q]i[T]_ _[θ][†][)]_ ( _i_ _[θ][ −Q]i[T]_ _[θ][†][)][ ≥]_ _c_ _i_ _[θ][ −Q]i[T]_ _[θ][†]_ (128)
_Q[T]_ _Q[T]_ _K_ _Q[T]_

Xi∈I    Xi∈I

[2]

= _c_ (θ _θ[†])[T]_ _i_ _i_ [(][θ][ −] _[θ][†][)]_ (129)

_K_ _−_ _Q_ _Q[T]_

Xi∈I


= c (θ _θ[†])[T]_
_K_ _−_


(θ − _θ[†])._ (130)


_i_ _i_
_Q_ _Q[T]_

Xi∈I


Since distribution _Q[˜]_ is bounded (and thus sub-Gaussian), by Theorem 6, with high probability, we
have

(θ − _θ[†])[T]_ Xi∈I _QiQi[T]_ ! (θ − _θ[†]) ≥_ _[λ][min]2_ _I_ _θ −_ _θ[†]_ 2 _[,]_ (131)

[2]

where λmin is the smallest eigenvalue of E _i_ _i_ . Thus, for _θ_ _θ[†]_ 2
_Q_ _Q[T]_ _−_ _[≤]_ _[f][K][, we have]_
 

_σ(Qi[T]_ _[θ][)][ −]_ _[σ][(][Q]i[T]_ _[θ][†][)]_ (Qi[T] _[θ][ −Q]i[T]_ _[θ][†][)][ ≥]_ _[λ][min]2_ _[c][K]_ _I_ _θ −_ _θ[†]_ 2 _[.]_ (132)

Xi∈I   

[2]

Combining this with (127), and defining AK ≜ min _λmin2_ _cK_ _, eK_, we then obtain the lemma.


D.3.4 PROOF THAT LOGISTIC REGRESSION IS GRADIENT-PAC*

Now we proceed with the proof that logistic regression is gradient-PAC*.

_Proof of Theorem 3. Note that σ(−z) = e[−][z]σ(z) = 1_ _−_ _σ(z) and σ[′](z) = e[−][z]σ[2](z). We then have_


_θℓ(θ,_ _,_ ) = = _e[−AQ][T][ θ]σ(_ _θ)_ (133)
_∇_ _Q_ _A_ _−_ _[σ][′][(]σ[AQ](_ _[T][ θ][)]θ[AQ])_ _−_ _AQ[T]_ _AQ_

_AQ[T]_

= −σ(−AQ[T] _θ)AQ =_ _σ(Q[T]_ _θ) −_ 1 [A = 1] _Q,_ (134)
  


-----

where 1 [A = 1] is the indicator function that outputs 1 if A = 1, and 0 otherwise. As a result,

(θ _θ[†])[T]_ _θ_ (θ, ) = (135)
_−_ _∇_ _L_ _D_


+ 2ν(θ − _θ[†])[T]_ _θ_ (136)


(θ − _θ[†])[T]_


_σ(_ _i_ _[θ][)][ −]_ [1][ [][A][i] [= 1]] _i_
_Q[T]_ _Q_



_i∈I_


= (θ _θ[†])[T]_ _σ(_ _i_ _[θ][)][ −]_ _[σ][(][Q]i[T]_ _[θ][†][) +][ σ][(][Q]i[T]_ _[θ][†][)][ −]_ [1][ [][A][i] [= 1]] _i_ (137)
_−_ Xi∈I   _Q[T]_  _Q_ !

+ 2ν _θ −_ _θ[†]_ 2 [+ 2][ν][(][θ][ −] _[θ][†][)][T][ θ][†]_ (138)

= ∆ _i_ _[θ,][2]_ _[ Q]i[T]_ _[θ][†][]_ + (θ _θ[†])[T]_ _σ(_ _i_ _[θ][†][)][ −]_ [1][ [][A][i] [= 1]] _i_ (139)

_iX∈[I]_  Q[T] _−_ Xi∈I   _Q[T]_  _Q_ !

+ 2ν _θ −_ _θ[†]_ 2 [+ 2][ν][(][θ][ −] _[θ][†][)][T][ θ][†][.]_ (140)

By Lemma 29, with high probability, we have

[2]

∆ _Qi[T]_ _[θ,][ Q]i[T]_ _[θ][†][]_ _≥_ _AKI min_ _θ −_ _θ[†]_ 2 _[,]_ _θ −_ _θ[†]_ 2 _._ (141)
_i_ [ ]

X∈ _I_   n o

[2]

To control the second term of (139), note that the random vectors Zi ≜ _σ(Qi[T]_ _[θ][†][)][ −]_ [1][ [][A][i][ = 1]] _Qi_
are iid with norm at most M . Moreover, since E [1 [ _i = 1]_ _i] = σ(_ _i_ _[θ][†][)][, by the tower rule,]_
_Q_ _A_ _|Q_   _Q[T]_ 
we have E [Zi] = E [E [Zi|Qi]] = 0. Therefore, by applying Hoeffding’s bound to every coordinate
of Zi, and then taking a union bound, for any B > 0, we have

P _Zi_ _B_ 2d exp _._ (142)

 _≥_ _I_ [3][/][4] _≤_ _−_ 2[B]dM[2][√I][2]

Xi∈I 2  _Q_ 

 

Applying now Cauchy-Schwarz inequality, with high probability, we have
Xi∈I  σ(Qi[T] _[θ][†][)][ −]_ [1][ [][A][i] [= 1]] _Qi!_ _[≤]_ _[B][I]_ [3][/][4] _θ −_ _θ[†]_ 2 _[.]_

Combining this with (132) and using[(][θ][ −] _[θ][†][)][T]_ _θ[†]_ 2

_[≤K][, we then have]_

(θ _θ[†])[T]_ _θ_ (θ, ) (143)
_−_ _∇_ _L_ _D_ [2]

_≥_ (AKI + ν) _θ −_ _θ[†]_ 2 _[,]_ _θ −_ _θ[†]_ 2 _−_ (BI [(3][/][4)] + 2νK) _θ −_ _θ[†]_ 2 (144)
n o

_≥_ _AKI_ _θ −_ _θ[†]_ 2 _[,]_ _θ −_ _θ[†]_ 2 _−_ _B[2]_ _KI_ [(3][/][4)] _θ −_ _θ[†]_ 2 _[,]_ (145)

where BK = B + 2νKn. This shows that Assumption[2]o 2 is satisfied for logistic loss for α = 3/4, and
_A_ and B as previously defined.
_K_ _K_

E PROOFS OF LOCAL PAC*-LEARNABILITY


Let us now prove Lemma 5. To do so, consider the preferred models _θ[⃗][†]_ and a subset H ⊂ [N ]
of honest users. Denote _[⃗]_ _H the datasets provided by users n_ [N ] _H. Each honest user_
_D−_ _∈_ _−_
_h ∈_ _H provides an honest dataset Dh of cardinality at least I ≥_ 1. Consider the bound KH ≜
maxh∈H _θh[†]_ 2 [on the parameter norm of honest active users][ h][ ∈] _[H][.]_

E.1 BOUNDS ON THE OPTIMA

Before proving the theorem, we prove a useful lemma that bounds the set of possible values for the
global model and honest local models.


-----

**Lemma 30. Assume that R and ℓ** _are nonnegative. For I large enough, if all honest active nodes_
_h ∈_ _H provide at least I data, then, with high probability,_ _θ[⃗]H[∗]_ _[must lie in a compact subset of][ R][d][×][H]_
_that does not depend on I._

_Proof. Denote L[0]_ ≜ LOSS(0, (θ[⃗]H[†] _[,][ 0][−][H]_ [)][,][ (][∅][, ⃗]D−H )). Essentially, we will show that, if _θ[⃗]H[∗]_ [is too]
far from _θ[⃗]H[†]_ [, then the loss will take values strictly larger than][ L][0][.]

Assumption 2 implies the existence of an event E that occurs with probability at least P0 ≜
_P_ (KH _, I)[|][H][|], under which, for any θh ∈_ R[d], we have

_T_ 2
_θh_ _θh[†]_ _h (θh)_ _AKH_ min _θh_ _θh[†]_ _θh_ _θh[†]_ _BKH_ _θh_ _θh[†]_
_−_ _∇L_ _≥_ _I_ _−_ 2 _[,]_ _−_ 2 _−_ _I_ _[α]_ _−_ 2 _[,][ (146)]_
   

which implies

**u[T]([θ]h[−][θ]h[†]** [)][∇L][h][ (][θ][h][)][ ≥] _[A][K][H]_ _[I][ min]_ 1, _θh −_ _θh[†]_ 2 _−_ _BKH_ _I_ _[α]._ (147)
n o

The fundamental theorem of calculus for line integrals then yieldsNote also that P0 → 1 as I →∞. We now integrate both sides over the line segment from θh[†] [to][ θ][h][.]

1

_Lh (θh) −Lh_ _θh[†]_ = _θh −_ _θh[†]_ 2 _t=0_ **u[T]([θ]h[−][θ]h[†]** [)][∇L] _θh[†]_ [+][ t][(][θ][h][ −] _[θ]h[†]_ [)] _dt_ (148)
 1 Z  

_θh_ _θh[†]_ _AKH_ min 1, t _θh_ _θh[†]_ _BKH_ _dt_ (149)
_≥_ _−_ 2 _t=0_ _I_ _−_ 2 _−_ _I_ _[α][]_

Z 1  n o

= _θh_ _θh[†]_ _AKH_ min 1, t _θh_ _θh[†]_ _dt_ _BKH_ _θh_ _θh[†]_ (150)
_−_ 2 _t=0_ _I_ _−_ 2 _−_ _I_ _[α]_ _−_ 2 _[.]_

Z  n o

Now, if _θh_ _θh[†]_
_−_ 2 _[>][ 2][, we then have]_

_h (θh)_ _h_ _θh[†]_ _AKH_ _I_ _BKH_ _θh_ _θh[†]_ (151)
_L_ _−L_ _≥_ 2 _−_ _I_ _[α]_ _−_ 2
   

_AKH_ 2BKH _._ (152)
_≥_ _I −_ _I_ _[α]_


Now for I > I1 ≜ max 2L[0]/AKH _, (4BKH_ _/AKH_ )
n


1

1 _α_
_−_, we have
o


_h (θh)_ _h_ _θh[†]_ _> L[0]._ (153)
_L_ _−L_
 

This implies that if _θh_ _θh[†]_
_−_ 2 _[>][ 2][ for any][ h][ ∈]_ _[H][, then we have]_

LOSS(0, (θ[⃗]H[†] _[,][ 0][−][H]_ [)][, ⃗] ) < LOSS(ρ, (θ[⃗]H _, θ[⃗]_ _H_ ), [⃗] ), (154)
_D_ _−_ _D_

regardless of ρ and θ−H . Therefore, we must have _θh[†]_ _h_ 2

bounded closed subset of R[d][×][H], which is thus compact. _[−]_ _[θ][∗]_ _[≤]_ [2][. Such inequalities describe a]

**Lemma 31. Assume that** (ρ, θ) _as_ _ρ_ _θ_ 2 _, and that_ _θh[†]_ _h_

_users h ∈_ _H. Then ρ[∗]_ _must lie in a compact subset of R_ _→∞_ _∥_ _−_ _∥_ _→∞ R[d]_ _that does not depend on[−]_ _[θ][∗]_ 2 _[≤] I.[2][ for all honest]_

_Proof. Consider an honest user h[′]. Given our assumption on R →∞, we know that there exists_
_DKH such that if ∥ρ −_ _θh[∗][′]_ _[∥]2_ _[≥]_ _[D][K]H_ [, then][ R][(][ρ, θ]h[∗][′] [)][ ≥] _[L][0][ + 1][. Thus any global optimum][ ρ][∗]_ [must]

satisfy _ρ[∗]_ _−_ _θh[†]_ _[′]_ 2 _[≤∥][ρ][∗]_ _[−]_ _[θ]h[∗][′]_ _[∥]2_ [+] _θh[∗][′][ −]_ _[θ]h[†]_ _[′]_ 2 _[≤]_ _[D][K][H][ + 2][.]_


-----

E.2 PROOF OF LEMMA 5

_Proof of Lemma 5. Fix ε, δ > 0. We want to show the existence of some value of I(ε, δ, D[⃗]_ _−H_ _, θ[⃗][†])_
that will guarantee (ε, δ)-locally PAC* learning for honest users.

By lemmas 30 and 31, we know that the set C of possible values for (ρ[∗], θ[⃗]H[∗] [)][ is compact. Now, we]
define
_EKH_ max (155)
≜ (ρ,θ)∈C _[∥∇][θ][R][(][ρ, θ][)][∥][2]_

the maximum of the norm of achievable gradients at the optimum. We know this maximum exists
since C is compact.

Using the optimality of (ρ[∗], θ[⃗][∗]), for all h ∈ _H, we have_

0 (θh[∗] _h[)][T][ ∇][θ]h_ [L][OSS][(][ρ][∗][, ⃗]θ[∗]) (156)
_∈_ _[−]_ _[θ][†]_

= (θh[∗] _h[)][T][ ∇L][h][(][θ]h[∗][) + (][θ]h[∗]_ _h[)][T][ ∇][θ]h_ _[R][(][ρ][∗][, θ]h[∗][)]_ (157)

_[−]_ _[θ][†]_ _[−]_ _[θ][†]_

(θh[∗] _h[)][T][ ∇L][h][(][θ]h[∗][)][ −]_ _θh[∗]_ _h_ _h[)][∥]2_ (158)
_≥_ _[−]_ _[θ][†]_ _[−]_ _[θ][†]_ 2 _[∥∇][θ][h]_ _[R][(][ρ][∗][, θ][∗]_

(θh[∗] _h[)][T][ ∇L][h][(][θ]h[∗][)][ −]_ _[E][K]H_ _θh[∗]_ _h_ (159)
_≥_ _[−]_ _[θ][†]_ _[−]_ _[θ][†]_ 2 _[.]_

We now apply assumption 2 for θ = θh[∗] [(for][ h][ ∈] _[H][). Thus, there exists some other event][ E]_ _[′][ with]_
probability at least P0, under which, for all h _H, we have_
_∈_

2
0 _AKH_ min _θh[∗]_ _h_ _θh[∗]_ _h_ _BKH_ _θh[∗]_ _h_ _θh[∗]_ _h_
_≥_ _I_ _[−]_ _[θ][†]_ 2 _[,]_ _[−]_ _[θ][†]_ 2 _−_ _I_ _[α]_ _[−]_ _[θ][†]_ 2 _[−]_ _[θ][†]_ 2 _[.][ (160)]_
  _[−]_ _[E][K][H]_

1
Now if I > I2 ≜ max 2EKH _/AKH_ _, (2BKH_ _/AKH_ ) 1−α this inequality cannot hold for

_θh[∗]_ _h_ n _θh[∗]_ _h_ o

_[−]_ _[θ][†]_ 2 _[≥]_ [1][. Therefore, for][ I][ >][ I]2[2][, we have] _[−]_ _[θ][†]_ 2 _[<][ 1][, and thus,]_

0 _AKH_ _θh[∗]_ _h_ _θh[∗]_ _h_ _θh[∗]_ _h_ (161)
_≥_ _I_ _[−]_ _[θ][†]_ 2 _[−]_ _[B][K][H]_ _[I]_ _[α]_ _[−]_ _[θ][†]_ 2 _[−]_ _[E][K][H]_ _[−]_ _[θ][†]_ 2

and thus,
_θh[∗]_ _h_ _._ (162)

_[−]_ _[θ][†]_ 2 _AKH_

_[≤]_ _[B][K][H]_ _[I]_ _[α][ +]I[ E][K][H]_

Now note that P [E ∧E _[′]] = 1 −_ P [¬E ∨¬E _[′]] ≥_ 1 − P [¬E] − P [¬E _[′]] = 2P0 −_ 1. It now suffices
to considerguaranteed by Assumption 2, and which guarantees I larger than I2 and large enough so that 2P P0 −(K1H ≥, I1)[|]−[H][|]δ) and so that≥ 1 − _δ/2 (whose existence isBKHA IKH[α]+ IEKH_ _≤_ _ε_

to obtain the theorem.

F CONVERGENCE OF CGA AGAINST ℓ[2]2

To write our proof, we define LOSS[ρ] _s_ [:][ R][d][ →] [R][ by]
_−_

LOSS[ρ] _s[(][ρ][)][ ≜]_ [inf] LOSS(ρ, θ, [⃗] _[⃗]_ ) _s(θs,_ _s)_ (ρ, θs) (163)
_−_ _θ⃗_ _D_ _−L_ _D_ _−R_

n o

= infθ⃗ _L(θn, Dn) + λ_ _∥ρ −_ _θn∥2[2]_ _[.]_ (164)

_nX≠_ _s_ _nX≠_ _s_

In other words, it is the loss when local models are optimized, and when the data of strategic user s
are removed.
**Lemma 32. Assuming ℓ[2]2** _[regularization and convex loss-per-input functions][ ℓ][, for any datasets][ ⃗]D,_
LOSS is strongly convex. As a result, so is LOSS[ρ] _s[.]_
_−_

_Proof. Note that the global loss can be written as a sum of convex function, and of ν_ _∥θn∥2[2]_ [+]
_∥ρ −_ _θ1∥2[2][. Using tricks similar to the proof of Lemma 11, we see that the loss is strongly convex.]_
The latter part of the lemma is then a straightforward application of Lemma 10. [P]


-----

We now move on to the proof of Theorem 4. Note that our statement of the proof was not fully
explicit, especially about the upper bound on the constant learning rate η. Here, we prove that it
holds for ηt = η 1/3L, where L is a constant such that LOSS[ρ] _s_ [is][ L][-smooth. The existence of][ L]
_≤_ _−_
is guaranteed by Lemma 13.

_Proof of Theorem 4. Note that by Lemma 9, LOSS[ρ]_ _s_ [is convex, differentiable and][ L][-smooth, and]
_−_
LOSS[ρ] _s[(][ρ][t][) =][ g][†][,t]s[. For][ ℓ]2[2]_ [regularization, we have G][RAD][(][ρ][) =][ R][d][ for all][ ρ][ ∈] [R][d][. Then the]
_∇_ _−_ _−_

_s_ _s_
minimum of equation 6 is zero, which is obtained when gs[t] [≜] _[ρ][t][−]η_ _[θ][†]_ _−gˆ−[t]_ _s_ [=][ g]s[t][−][1]+ _[ρ][t][−]η_ _[θ][†]_ + _[ρ][t][−]η[ρ][t][−][1]_ .

Note that

_ρ[t][+1]_ = ρ[t] _−_ _ηg−[†][,t]s_ _[−]_ _[ηg]s[t]_ (165)

= ρ[t] _−_ _ηg−[†][,t]s_ _[−]_ [(][ρ][t][ −] _[θ]s[†][) + (][ρ][t][−][1][ −]_ _[ρ][t][)][ −]_ _[ηg]s[t][−][1]_ (166)

= θs[†] _s_ [+][ g]s[t][−][1]) + η(g[†][,t]s[−][1] + gs[t][−][1]) (167)

_[−]_ _[η][t][(][g]−[†][,t]_ _−_

= θs[†] _[−]_ _[η][(][g]−[†][,t]s_ _[−]_ _[g]−[†][,t]s[−][1])._ (168)

Therefore, ρ[t][+1] _−_ _ρ[t]_ = η(g−[†][,t]s _[−]_ _[g]−[†][,t]s[−][1]) −_ _η(g−[†][,t]s[−][1]_ _−_ _g−[†][,t]s[−][2])._

Then, using the L-smoothness of LOSS[ρ] _s[, and denoting][ u][t]_ [≜] _ρ[t][+1]_ _ρ[t]_ 2[, we have][ u][t][+1][ ≤]
_−_ _−_
_Lηtut + Lηt−1ut−1. Now assume that η ≤_ 1/3L. Then ut+1 ≤ 3[1] [(][u][t][ +][ u][t][−][1][)][. We then know that]

_ut+2 ≤_ 3[1] [(][u][t][+1][ +][ u][t][)][ ≤] [1]3 [(][ 1]3 [(][u][t][ +][ u][t][−][1][) +][ u][t][) =][ 4]9 _[u][t][ +][ 1]9_ _[u][t][−][1][.]_

Now define97 [(][u][t][ +][ u][t][−] v[1][)]t ≜≤ _u79_ _t[v] +[t][.]_ _uBy induction, we know thatt−1._ We then have vt+2 ≤ vtut+2≤ +(7 u/t9)+1[(][t] ≤[−][1)][/][2]79max[u][t][ +] {[ 4]v90, v[u][t][−]1}[1][ ≤]≤

(√7/3)[t][  ](√7/3) max {v0, v1} . Thus, defining α ≜ _√7/3 < 1, there exists C > 0 such that_

_ut ≤_ _vt ≤_ _Cα[t]. This implies that_ _ρ[t][+1]_ _−_ _ρ[t]_ 2 _[≤]_ [P][ Cα][t][ <][ ∞][. Thus][ P][(][ρ][t][+1][ −] _[ρ][t][)][ con-]_
verges, which implies the convergence of ρ[t] to a limit ρ[∞]. By L-smoothness, we know that g−[†][,t]s
must converge too. Taking equation 168 to the limit then implies[P ] _ρ[∞]_ = θs[†][. This shows that the]
strategic user achieves precisely what they want with CGA. It is thus optimal.

G CGA ON MNIST

In this section, CGA is executed against 10 honest users, each one having 6,000 randomly and data
points of MNIST, drawn randomly and independently. CGA is run by a strategic user whose target
model θs[†] [labels 0’s as 1’s, 1’s as 2’s, and so on, until 9’s as 0’s. We learn][ θ]s[†] [by relabeling the]
MNIST training dataset and learning from the relabeled data. We use λ = 1, Adam optimizer and a
decreasing learning rate.


35

30

25

20 l2_dist

l2_norm

l2 norm 15 target_dist

10

5

0

0 25 50 75 100 125 150 175 200

Epochs


35

30

25

l2 norm 20

15

10

l2_dist

5 l2_norm

target_dist

0

0 25 50 75 100 125 150 175 200

Epochs


(a) Using ℓ[2]2


(b) Using ℓ2


Figure 16: Norm of global model, distance to initialisation and distance to target, under attack by
CGA. In particular, we see that the attack against ℓ[2]2 [is successful, as the distance between the global]
model and the target model goes to zero.


-----

H SINGLE DATA POISONING FOR LEAST SQUARE LINEAR REGRESSION

_Proof of Theorem 5. We define the minimized loss with respect to ρ and without strategic user s by_


LOSS[∗] _s[(][ρ, ⃗]_ _s) ≜_ min
_−_ _D−_ _θ⃗−s∈R[d][×][(][N]_ _[−][1)]_


_λ_ _θn_ _ρ_ 2
_nX≠_ _s_ _∥_ _−_ _∥[2]_


LOSS[∗]−s[(][ρ, ⃗]D−s) ≜ _θ⃗−s∈minR[d][×][(][N]_ _[−][1)]_ nX≠ _s_ _Ln(θn, Dn) +_ _nX≠_ _s_ _λ ∥θn −_ _ρ∥2[2]_ (169)

Now consider a subgradient g ∈∇ρLOSS[∗]−s[(][θ]s[†][, ⃗]D−s) of the minimized loss at θs[†][. For] _[.]_ _[ x][ ≜]_ _[−]2λ[g]_ [,]

then have −g ∈∇ _λ ∥x∥2[2]_ . We then define θs[♠] [≜] _[θ]s[†]_ _[−]_ _[x][.]_
 

0 = g − _g ∈∇ρLOSS[∗]−s[(][θ]s[†][, ⃗]D−s) + ∇ρ_ _λ_ _θs[♠]_ _[−]_ _[θ]s[†]_ 2 (170)
 

= ∇ρLOSSs(θs[†][, ⃗]θ−[∗] _s[(][θ]s[♠][, ⃗]D−s), θs[♠][, ⃗]D−s),_ [2] (171)

where LDs = {(QOSS, As is defined by (38). Now consider the data point)}, we then have ∇Ls(θs[♠][,][ D][s][) =][ g][, which implies] (Q, A) = (g, g[T] _θs[♠]_ _[−]_ [1)][. For]

_θs_ LOSS(θs[†][,][ (][θ]s[♠][, ⃗]θ[∗] _s[(][θ]s[♠][, ⃗]_ _s), [⃗]_ ) = 0. (172)
_∇_ _−_ _D−_ _D_

Combining it all together with the uniqueness of the solution then yields

arg min LOSS(ρ, θ, [⃗] _[⃗]_ ) = _θs[†][,]_ _θs[♠][, ⃗]θ[∗]_ _s[(][θ]s[♠][, ⃗]_ _s)_ _,_ (173)
(ρ,θ[⃗]) _D_ _−_ _D−_

n o   

which is what we wanted.


I DATA POISONING AGAINST LINEAR CLASSIFICATION

I.1 GENERATING EFFICIENT POISONING DATA


bias of the linear classifier). The indifference subspaceFor every label a ∈{1, . . ., 9}, we define ya ≜ _θa[♠]_ _[−]_ _[θ]0[♠] V[, and] is then the set of images[ c][a][ ≜]_ _[−][(][θ]a[♠]0_ _[−]_ _[θ]00[♠]_ [)][ (where] Q ∈[ θ]Ra[♠]0[d][is the]such
that Q[T] _ya = ca for all a ∈{1, . . ., 9}._

To project any image X ∈ R[d] on V, let us first construct an orthogonal basis of the vector space
orthogonal to V, using the Gram-Schmidt algorithm. Namely, we first define z1 ≜ _y1. Then, for_
any answer a ∈{1, . . ., 9}, we define

_zb_

_za ≜_ _ya_ _ya[T]_ _[z][b]_ _._ (174)
_−_ _b<a_ _zb_ 2
X _∥_ _∥[2]_

It is easy to check that for b < a, we have za[T] _[z][b]_ [= 0][. Moreover, if][ Q ∈] _[V][, then]_

(ya[T] _[z][b][)(][z]b[T]_ (ya[T] _[z][b][)(][z]b[T]_

_za[T]_ _a_ _[Q][)]_ = ca _[Q][)]_ _._ (175)

_[Q][ =][ y][T]_ _[Q −]_ _b<a_ _zb_ 2 _−_ _b<a_ _zb_ 2
X _∥_ _∥[2]_ X _∥_ _∥[2]_

By induction, we see that za[T]
as z1[T] 1 _[Q][ is a constant independent from][ Q][. Indeed, for]a_ _[ a][ = 1][, this is clear]_
_zsee that these constants can be computed byb[T]_ _[Q][ for][Q][ =][ b < a][ y][T]_ _[Q][. Moreover, denoting][ =][ c][1][. Moreover, for][ c][ a >]a[′]_ [the constant such that][ 1][, then, in the computation of][ z]a[T] _[Q][ =][ c]a[′][ z][for all][T]_ _[Q][,][ Q][ a][ always appear as][ ∈{][1][, . . .][ 9][}][, we]_

_ya[T]_ _[z][b]_

_c[′]a_ [=][ c][a] _c[′]b[.]_ (176)

_[−]_ _b<a_ _zb_ 2
X _∥_ _∥[2]_

Finally, we can simply perform repeated projection onto the hyperplanes where a is equally probable
as the answer 0. To do this, we first define the orthogonal projection P (X, y, c) of X ∈ R[d] on the
hyperplane x[T] _y = c, which is given by_


_c[′]a_ [=][ c][a]

_[−]_


_b<a_


_P_ (X, y, c) = X − (X _[T]_ _y −_ _c)_


_y_

_._ (177)
_y_ 2
_∥_ _∥[2]_


-----

It is straightforward to verify that P (X, y, c)[T] _y = c and that P_ (P (X, y, c), y, c) = P (X, y, c). We
then canonically define repeated projection by induction, as

_P_ (X, (y1, . . ., yk+1), (c1, . . ., ck+1)) ≜ _P_ (P (X, (y1, . . ., yk), (c1, . . ., ck)), yk+1, ck+1). (178)

Now consider any image X ∈ R[d]. Its projection can be obtained by setting

_Q ≜_ _P_ (X, (z1, . . ., z9), (c[′]1[, . . . c][′]9[)) +][ ξ.] (179)

Note that to avoid being exactly on the boundary, and thus retrieve information about the scales of
_θ[♠]_ and on which side of the boundary favors which label, we add a small noise ξ, to make sure Q
does not lie exactly on V (which would lead to multiple solutions for the learning), but small enough
so that the probabilities of the different label remain close to 0.1 (the equiprobable probability).

We acknowledge that images obtained this way may not be in [0, 1][d], like the images of the MNIST
dataset. In general, one could search for points Q ∈ _V ∩_ [0, 1][d]. Note that in theory, by Theorem 3
(or a generalization of it), labeling random images in [0, 1][d] should suffice. However, in the case
where V ∩ [0, 1][d] is empty (typically if no image in [0, 1] is argued by model θs[♠] [as realistically a 9),]
this procedure may require the labeling of significantly more images to be successful.

I.2 A BRIEF THEORY OF DATA POISONING FOR LINEAR CLASSIFICATION

Using the efficient poisoning data fabrication, we thus have a set of images ( _, p(_ )), where pa( )
_Q_ _Q_ _Q_
is the probability assigned to image Q and label a. This defines the following local loss for the
strategic node:
_s(θs,_ _s) =_ _pa(_ ) ln σa(θs, ), (180)
_L_ _D_ _Q_ _Q_

(Q,p(XQ))∈Ds _a∈{0X,1,...,9}_

where σa(θs, Q) = exp( exp(θsa[T]θsbT[Q][Q][+][+][θ][sa][θ][sb][0][)][0][)][ is the probability that image][ Q][ has label][ a][, according to the]
model θs. We acknowledge that such labelings of queries is unusual. Evidently, in practice, an imageP
may be labeled N times, and the number of labels Na it received can be set to be approximately
_Na_ _Npa(_ ).
_≈_ _Q_

It is noteworthy that the gradient of the loss function is then given by
_θs_ _θs[♠]_ _T_ _θs_ _s(θs,_ _s) =_ _σa(θs,_ ) _σa(θs[♠][,][ Q][)]_ _θsa_ _θsa[♠]_ _T_ +,
_−_ _∇_ _L_ _D_ _Q_ _−_ _−_ _Q_
   _Q∈DXs_ _a∈{0X,1,...,9}_      

(181)
where we defined Q[+] ≜ (1, Q) (which allows to factor in the bias of the model. This shows
that ∇θs _Ls(θs, Ds) points systematically away from θs[♠][, and thus that gradient descent will move]_
towards θs[♠][.]

In fact, if the set of images Q cover all dimensions (which occurs if there are Ω(d) images, which is
the case for 2,000 images, since d = 784), then gradient descent will always move the model in the
direction of θs[♠][, which will be the minimum. Moreover, by overweighting each data][ (][Q][, p][(][Q][))][ by]
a factor α (as though the image Q was labeled α times), we can guarantee gradient-PAC* learning,
which means that we will haveThis shows why data poisoning should work in theory, with relatively few data injections. θs[∗] _[≈]_ _[θ]s[♠][, even in the personalized federated learning framework.]_

Note that the number of other users does make learning harder. Indeed, the gradient of the regularizationother users grows, we should expect this distance to grow roughly proportionally to R(ρ, θs) at ρ = θs[†] [and][ θ][s] [=][ θ]s[♠] [is equal to][ 2][λ] _θs[†]_ _[−]_ _[θ]s[♠]_ 2[. As the number] N . In order to[ N][ −] [1][ of]
make strategic user s robustly learn θs[♠][, the norm of the gradient of the local loss][ L][s] [at][ θ]s[†] [must be]
vastly larger thandata injected in 2s) must also grow proportionally toλ _θs[†]_ _[−]_ _[θ]s[♠]_ 2[. This means that the value of] N . _[ α][ (or, equivalently, the number of]_
_D_

I.3 INITIALIZATION OF THE LEARNING ALGORITHM

The convergence to the optimum is slow. But given that the problem is convex, we focus here
mostly on showing that the minimum is indeed a poisoned model. To boost the convergence, we
initialize our learning algorithm at a point close to what we expect to be the minimum, by taking
this minimum and adding a Gaussian noise, and then we observe the convergence to this minimum.


-----

J CIFAR-10 ON VGG 13-BN EXPERIMENTS

We considered VGG 13-BN, which was pretrained on cifar-10 by Phan (2021). We now assume that
10 nodes are given part of the cifar-10 database, while a strategic node also joins to the personalized
federated gradient descent algorithm. The strategic node’s goal is to bias the global model towards
a target model, which misclassifies the cifar-10 data, by reclassifying 0 into 1, 1 into 2... and 9 into
0.


J.1 COUNTER-GRADIENT ATTACK

We first show the result of performing counter-gradient attack on the last layer of the neural network.
Essentially, images are now reduced to their vector embedding, and the last layer performs a simple
linear classification akin to the case of MNIST (see Appendix G).


|Gradient attack|Col2|Col3|
|---|---|---|
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
|acc_glob|||


200 400 600 800 1000

acc_glob

Epochs


1.0

0.8

0.6

0.4


0.2

0.0


Gradient attack

40

honest_dist

35 l2_norm

target_dist

30 attack_dist

25

20

l2 norm

15

10

5

0

0 200 400 600 800 1000

Epochs


(a) Accuracy according to attacker’s objective


(b) Distances


Figure 17: CGA on cifar-10.

J.2 RECONSTRUCTING A MODEL ATTACK


Reconstructing an attack model whose effect is equivalent to the counter-gradient attack is identical
to what was done in the case of MNIST (see Section 5.2).


Model attack

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|||||
|acc_glob||||



100 200 300 400 500

acc_glob

Epochs


Model attack

40

honest_dist

35 l2_norm

target_dist

30 attack_dist

25

20

l2 norm

15

10

5

0

0 100 200 300 400 500

Epochs

(b) Distances


1.0

0.8


0.6

0.4


0.2

0.0


(a) Accuracy according to attacker’s objective


Figure 18: Model attack on cifar-10.

J.3 RECONSTRUCTING DATA POISONING


This last step is however nontrivial. On one hand, we could simply use the attack model to label a
large number of random images. However, this solution would likely require a large sample com

-----

plexity. For a more efficient data poisoning, we can construct vector embeddings on the indifference
affine subspace V, as was done for MNIST in Section 5.3. This is what is shown below.


|Col1|Data attack|Col3|
|---|---|---|
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
||||
|acc_glob|||


200 400 600 800 1000 1200 1400

acc_glob

Epochs


1.0

0.8


0.6

0.4

0.2

0.0


Data attack

40

honest_dist

35 l2_norm

target_dist

30 attack_dist

25

20

l2 norm

15

10

5

0

0 200 400 600 800 1000 1200 1400

Epochs


(a) Accuracy according to attacker’s objective


(b) Distances


Figure 19: Data poisoning on cifar-10.

We acknowledge however that this does not quite correspond to data poisoning, as it requires reporting a vector embedding and its label, rather than an actual image and its label. The challenge
is then to reconstruct an image that has a given vector embedding. We note that, while this is not a
straightforward task in general, this has been shown to be at least somewhat possible for some neural
networks, especially when they are designed to be interpretable (Zeiler & Fergus, 2014; Wang et al.,
2019c; Mai et al., 2019).


-----

