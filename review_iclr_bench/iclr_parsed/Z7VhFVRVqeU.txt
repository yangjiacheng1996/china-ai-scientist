Under review as a conference paper at ICLR 2022
NEURAL BOOTSTRAPPING ATTENTION FOR NEURAL
PROCESSES
Anonymous authors
Paper under double-blind review
ABSTRACT
Neural Processes learn to ﬁt a broad class of stochastic processes with neural net-
works. Modeling functional uncertainty is an important aspect of learning stochastic
processes. Recently, Bootstrapping Neural Processes (B(A)NP) propose a bootstrap
method to capture the functional uncertainty which can replace the latent variable in
(Attentive) Neural Processes ((A)NP), thus overcoming the limitations of Gaussian
assumption on the latent variable. However, B(A)NP conduct bootstrapping in a
non-parallelizable and memory-inefﬁcient way and fail to capture diverse patterns
in the stochastic processes. Furthermore, we found that ANP and BANP both tend
to overﬁt in some cases. To resolve these problems, we propose an efﬁcient and
easy-to-implement approach, Neural Bootstrapping Attentive Neural Processes
(NeuBANP). NeuBANP learns to generate the bootstrap distribution of random func-
tions by injecting multiple random weights into the encoder and the loss function.
We evaluate our models in benchmark experiments including Bayesian optimization
and contextual multi-armed bandit. NeuBANP achieves the best performance in the
sequential decision-making tasks among NP methods, and this empirically shows
that our method greatly improves the quality of functional uncertainty modeling.
1
INTRODUCTION
Neural Processes (NP) (Garnelo et al., 2018b) deﬁne distributions over functions given a set of
observations, and are trained via a meta-learning framework so that it can adapt to new functions
rapidly. NP learns to model a wide range of stochastic processes and can estimate the uncertainty over
the predictions with less computational effort, compared to Gaussian Processes (GP) (Rasmussen,
2003). However, NP frequently suffers from a fundamental drawback of underﬁtting. As a remedy
of the underﬁtting issue, Attentive Neural Processes (ANP) (Kim et al., 2018) applies the attention
modules to the encoder network. Despite this modiﬁcation, a single Gaussian latent variable of (A)NP
has a limitation in inducing functional uncertainty (Louizos et al., 2019), a global uncertainty that
decides the distribution over the space of trajectories or functions.
Appropriate modeling of functional uncertainty in stochastic processes improves the predictive
performance and diversity in function realizations (Le et al., 2018), thus provides a principled way
to guide agents to ﬁnd optimal candidates in sequential decision-making problems. In these tasks, a
model needs to approximate a function and estimate uncertainty correctly to optimize a black-box
function whose analytic information is not given. Although GP is widely used for these tasks, these
are the promising area for the application of NP because GP is computationally expensive, and it can
be hard to choose an appropriate prior. Recently, Bootstrapping Neural Processes (B(A)NP) (Lee
et al., 2020) modify (A)NP to induce more robust uncertainty estimation by employing the residual
bootstrapping. However, B(A)NP underperforms in capturing a functional uncertainty because the
residual bootstrapping works in a homoscedastic way, removing the connection between the feature
and the label in its bootstrapped samples. The bootstrap strategy used in B(A)NP demands a higher
computational burden compared to (A)NP since it requires multiple computations of the encoder
network and additional heuristics, including the adaptation layer and the lower bound on the variance1.
Furthermore, ANP and BANP tend to overﬁt for a simple regression task rather than learning the
underlying heteroscedasticity. We observed that this problem is a by-product of the attention modules
1We colored the revised or added sentences in blue only for the rebuttal. This color will not appear in the
ﬁnal version of the manuscript.
1
Under review as a conference paper at ICLR 2022
used in both models (see Figure 1 and 5). This ﬁnding suggests that effective regularization of
attention modules is required to prevent overﬁtting. Additionally, as explained in Section 4, ANP and
BANP often tend to estimate homogeneous uncertainty regardless of whether the observation is given.
Fig. 1. Each plot shows predictions given by ANP, BANP, and the proposed method in a linear regression.
The ground-truth function is a simple linear function with heterogeneous variance: y = x + βϵ(x) where
ϵ(x) ∼N(0, σ2(x)) and σ(x) =
√
x2 + 10−5. See Appendix B.1 for more details.
To resolve the problems of previous NPs, we introduce a novel bootstrapping method for neural
processes, Neural Bootstrapping Attentive Neural Processes (NeuBANP). Motivated from the recent
work on efﬁcient bootstrapping of the neural network, Neural Bootstrapper (NeuBoots) (Shin et al.,
2021), we introduce bootstrapping of the attention in a computationally efﬁcient way by simple
modiﬁcation of the input of attention modules and the loss function, instead of memory-inefﬁcient
resampling and contrived heuristics employed in BANP. The simplicity and computational efﬁciency
of NeuBANP directly come from NeuBoots, but it does not guarantee the performance in modeling the
functional uncertainty since NeuBANP operates on the meta-learning framework. Thus, we modify the
method for training the model to learn the randomness present in the underlying function, allowing the
model to estimate random functions generated by any stochastic process. This modiﬁcation is simple
but gives a strong consistency between the bootstrapped samples and the representations from the
attention modules, unlike the residual bootstrapping used in BANP. Besides, our bootstrapping method,
which implements the concatenation and multiplication of random bootstrap weights, operates as
a regularizer on the attention networks, thus preventing overﬁtted predictions observed in previous
attention-based NP methods. NeuBANP is trained to generate a valid predictive distribution by utilizing
the uncertainty inherent in observations by bootstrapping instead of the uncertainty that depends
on the prior assumption on the latent variable as in (A)NP. This leads to the success in capturing
heteroscedasticity of the data and modeling functional uncertainty in stochastic processes. As a result,
NeuBANP achieves the best performance in sequential decision-making problems such as Bayesian
optimization and contextual multi-armed bandit. The experimental results demonstrate that our model
provides promising capabilities as an efﬁcient neural approximation of stochastic processes.
Contributions
We propose NeuBANP, an easy-to-implement and computationally efﬁcient method
for bootstrapping ANP. The proposed method has a novelty in learning a generator for bootstrapping
stochastic processes under a meta-learning framework. NeuBANP resolves overﬁtting problem of
attention modules and shows robust performance on heteroscedastic models without heuristics like
the extra adaptation layer in BANP. NeuBANP estimates functional uncertainty better than BANP and
achieves the best performance in stochastic optimization problems, including multi-dimensional
Bayesian optimization and contextual multi-armed bandit, compared to previous NP methods.
2
PRELIMINARIES
2.1
META-LEARNING FRAMEWORK OF NEURAL PROCESSES
Consider data D = (X, Y ) = {(xi, yi)}n
i=1 ⊂X ×Y, the pairs of inputs xi ∈X and outputs yi ∈Y.
Let P be a probability distribution over functions f ∈F; yi = f(xi) + ϵi where ϵi∼N(0, σ2
i ),
hence P determines the distribution of D. For disjoint index sets C and T satisfying C ∪T = [n],
deﬁne context DC = (XC, YC) = {(xc, yc)}c∈C and target DT = (XT , YT ) = {(xt, yt)}t∈T , so
that D = DC ∪DT . The task is to learn the neural processes pθ that ﬁts the stochastic processes
2
Under review as a conference paper at ICLR 2022
f ∼P given DC when C ∼Pn is a randomly chosen subset of [n], as follows:
θ⋆= argmin
θ
Ef∼P
h
EC∼Pn
h
−log pθ(Y |X, DC)
ii
.
(1)
This meta-learning framework allows pθ to learn diverse patterns in F via a prior distribution P;
hence NP can predict target points conditioned on contexts adaptively in the inference phase.
2.2
(BOOTSTRAPPING) ATTENTIVE NEURAL PROCESSES
ANP
ANP is a variant of NP equipped with attention modules in the encoder part. See Appendix
A for the detailed deﬁnition of attention operations. Let the context DC is given, and the model
aims to infer y for a given feature x ∈X. The encoder network of ANP maps (x, DC) into a pair of
representation vectors r = (z, h) as follows:
{sc}c∈C = SelfAttn(DC),
sC = mean({sc}c∈C),
z ∼N(z|µz(sC), σ2
z(sC))
(2)
{hc}c∈C = SelfAttn(DC),
h = CrossAttn(x, XC, {hc}c∈C)
(3)
Here, µz and σz are single linear layers that map sC to mean and standard deviation of the latent
variable z, respectively. In ANP, z and h refer to latent path and deterministic path, respectively. The
deterministic path models the overall skeleton of the encoder network, while the latent path models the
functional uncertainty using a stochastic global latent variable (Garnelo et al., 2018b; Kim et al., 2018).
Then the decoder network takes the representations r and target data x as inputs to predict µ(x, r)
and σ(x, r), the parameters of the conditional predictive distribution p(y|x, DC) = N(y|µ, σ2).
BANP
The global latent variable in ANP potentially limits the ﬂexibility in expressing functional
uncertainty. BANP proposes a method that utilizes paired bootstrapping and residual bootstrapping
to model stochasticity in a data-driven way. First, they resample pairs of (xc, yc) with replacement
to construct ˆ
D(b)
C
= ( ˆ
X(b)
C , ˆ
Y (b)
C
). Here b = 1, . . . , B implies the number of bootstrap samples. The
resampled context is encoded into the representation vector ˆ
r(b) = (ˆ
z(b), ˆ
h(b)) by replacing DC
in (2) and (3) by ˆ
D(b)
C . Then BANP conducts the residual bootstrapping to construct bootstrapped
contexts again ˜
D(b)
C
= ( ˜
X(b)
C , ˜
Y (b)
C
). By using DC and ˜
D(b)
C
in (2) and (3) separately, BANP gets the
representations of contexts (r, ˜
r(b)). Finally, BANP uses an adaptation layer to merge (r, ˜
r(b)) and
obtain µ(b), σ(b) through the decoder network (see Figure 2).
p(y|x, r, ˜
r(b)) = N(y|µ(b), (σ(b))2),
p(y|x, DC) ≈1
B
B
X
b=1
p(y|x, r, ˜
r(b)).
(4)
Due to the residual bootstrapping, BANP conducts the encoder computation three times and the
decoder computation twice for a single forward propagation. These additional calculations cause the
computational bottleneck in the training and inference (see Appendix B.6).
2.3
NEURAL BOOTSTRAPPER
Repetitions of training restrain the practical use of bootstrap procedures in deep neural networks due
to their high computational burden. To alleviate this, Shin et al. (2021) proposes Neural Bootstrapper
(NeuBoots). NeuBoots circumvents multiple training of networks by learning a bootstrap generator.
Random Weight Bootstrapping
Let P
c∈C ℓ(f(xc), yc) be the loss function of interest for a
neural network f. In standard bootstrap procedures, a bootstrapped neural network can be obtained
by minimizing the loss function weighted by a random bootstrap weight wC := {wc : c ∈C}:
L(wC, f, DC) =
X
c∈C
wcℓ(f(xc), yc).
(5)
According to the choice of the distribution on wC, various bootstrap procedures can be represented
under the form of (5); e.g., the paired bootstrap by wC ∼Multinomial(n; 1/n, . . . , 1/n) and the
Random Weight Bootstrapping (RWB) (Præstgaard & Wellner, 1993; Newton & Raftery, 1994)
by wC ∼|C| × Dirichlet(1, . . . , 1). NeuBoots utilizes RWB to avoid the data discard problem
which can occur in the standard bootstrapping. We then compute bootstrapped neural networks
{ b
f (b) : b = 1, . . . , B} via minimization of (5) for sampled w(1)
C , . . . , w(B)
C
.
3
Under review as a conference paper at ICLR 2022
Learning To Generate Bootstrap Distribution
The main idea of NeuBoots is to construct a single
generative network that models the bootstrapped neural networks with varying bootstrap weights in
(5). This formulation modiﬁes the backbone network in a form of f(x, wC) that inputs both feature x
and bootstrap weight wC. Shin et al. (2021) show that the minimizer of the following loss generates
valid bootstrap evaluations that match the results of the standard bootstrap procedure:
L(f, DC) = EwC∼|C|×Dirichlet(1,...,1) [L(wC, f(·, wC), DC))] ,
We call this weighted bootstrapping loss. Once this generator is trained via a single optimization
procedure, we can efﬁciently generate bootstrapped predictions by plugging random bootstrap weights
in the trained generator; i.e., for a feature of interest x, the trained generator inputs {w(b)
C }B
b=1 and
produces bootstrapped predictions ˆ
y(b) = ˆ
f(x, w(b)
C ) for b = 1, . . . , B.
3
NEURAL BOOTSTRAPPING ATTENTIVE NEURAL PROCESSES
We propose a novel class of NP, called Neural Bootstrapping Attentive Neural Processes (NeuBANP).
Aligned with the previous formulation of NP families, we can deﬁne our model pθ as:
pθ(Y |X, DC) =
ˆ
pϕ(Y |X, h, z)q(z|DC)dz =
ˆ
n
Y
i=1
pϕ(yi|xi, h, z)q(z|DC)dz
(6)
≈
n
Y
i=1
pϕ(yi|xi, h, z) where z ∼q(z|DC).
(7)
Here pϕ is the decoder and q denotes the posterior distribution. Since the above integral is intractable,
we approximate the predictive distribution by sampling z from the bootstrap distribution q(·|DC), in-
stead of Gaussian distribution as in (A)NP. Precisely, we train a generative encoder network gφ, which
outputs bootstrapped representation pairs (h, z) = {(h(b), z(b))}B
b=1 = gφ(X, DC, {w(b)
C }B
b=1).
Through the meta-learning framework (1), our model learns to generate bootstrapped predictions
for an arbitrarily given function f ∼P. Thus, our approach can be regarded as a learn-to-bootstrap
method for stochastic processes. Compared to the ﬁxed bootstrap method used in BANP, our learnable
bootstrap method can ﬁnd the best strategy to appropriately generate the random functions regarding
the given context and the general property of the underlying data generating process. This will lead
to the better modeling of functional uncertainty of the target stochastic processes. Also, note that
NeuBANP can obtain a number of bootstrapped predictions by simply plugging different bootstrap
weights into gφ, while BANP needs repetitive data resampling from scratch. Figure 2 shows the
difference between the forward computation paths of BANP and NeuBANP in detail.
3.1
NEURAL BOOTSTRAPPING CONTEXTS WITH ATTENTION MODULES
To train a generative network, which outputs the bootstrapped representations, we modify the encoder
in ANP to take both (x, DC) and bootstrap weight wC as inputs. We introduce the posterior and
the prediction paths in the encoder, analogous to NP’s latent and deterministic paths. NeuBANP is
designed to leverage RWB, which is theoretically proven as a valid bootstrap method. In detail, the
posterior and prediction path in NeuBANP take the random bootstrap weight as an auxiliary input.
This input provides enough randomness into the network so that our model can successfully capture
the functional uncertainty.
Posterior path
We tag each context (xc, yc) ∈DC with a bootstrap weight w(b)
c
to construct
bootstrapped contexts D(b)
C
:= {(xc, yc, w(b)
c )}c∈C. Then the posterior path receives D(b)
C
as input
and outputs a latent variable z(b). In detail, we apply self-attention to D(b)
C
and multiply the resultant
representation z(b)
C
by the bootstrap weight w(b)
C
before mean aggregation. This path connects the con-
texts with weighted bootstrapping loss during the training and allows the bootstrap weights to model
bootstrapped posterior distribution q by controlling the magnitude of each context representation.
Prediction path
The prediction path outputs a target-speciﬁc representation h(b) that is relevant
for the prediction. We apply self-attention to contexts DC and multiply the bootstrap weight w(b)
C
4
Under review as a conference paper at ICLR 2022
pairwise  
bootstrap
(a) BANP
Posterior path
Prediction path
Self
Attention
mean
Self
Attention
⊙
key
query
Bootstrapping
copy
MLP
MLP
value
(b) NeuBANP
⊙
Cross 
Attention
MLP
<latexit sha1_base64="17pdDSRH3KMr56xogZ6Egf3e0g8=">AC
gXicbVHLSgMxFE3Hd31VXboJFkFBykwRFdwUdOFSwT6gU0omc8cGM5khuaOWcT7Fr3GrH+DfmKldaOuFkM5ucm5J0EqhUHX/ao4C4tLyura9X
1jc2t7drObsckmebQ5olMdC9gBqRQ0EaBEnqpBhYHErB41Wpd59AG5GoexynMIjZgxKR4AwtNaydv/oxwxFnMr8qXqmPIgZjN3jB/FpowUcSsD
jyTnwZJmhOvGPqGxHTYa3uNtxJ0XngTUGdTOt2uFNp+mHCsxgUcsmM6XtuioOcaRcQlH1MwMp4/sAfoWKmaNDPLJhAU9tExIo0TbpZBO2N8dOY
uNGceBPVmOY2a1kvxXKxltImNFOq+GpnxuxhtGF4NcqDRDUPzHWpRJigktA6ah0MBRji1gXAs7HeUjphlH+w1/blfwjC9l0sU8PTFctSl7s5nOg
06z4Z01mnen9VZrmvcq2ScH5Ih45Jy0yA25JW3CyRt5Jx/k01lwjh3Xaf4cdSrTnj3yp5zLb4ByxZs=</latexit>|C| ⇥Dirichlet(1, . . ., 1) ⇠
<latexit sha1_base64="HEVtZzIPZCjr0h9GbVQ3xG14YE=">AC
WXicbVDLSsNAFJ3EV42PVrt0M1gEVyUpoi4LblxWsD6wpUymNzo4mYSZG20J/RK3+lHizhJs7CtFwYO59zHmROmUhj0/W/HXVvf2NyqbXs7u3v
79cbB4Z1JMs2hzxOZ6IeQGZBCQR8FSnhINbA4lHAfvl4V+v0baCMSdYvTFIYxe1YiEpyhpUaN+uMoH8QMXziT+dVsNmq0/LZfFl0FQVapKre6M
DpDMYJz2JQyCUz5inwUxzmTKPgEmbeIDOQMv7KnuHJQsViMO8dD6jJ5YZ0yjR9imkJft3ImexMdM4tJ2FSbOsFeS/WsFoExkr0lV1bIpzS94wuh
zmQqUZguJza1EmKSa0CI6OhQaOcmoB41rY31H+wjTjaONd2K7gHScIk8UDc7o07NmUg+VMV8Fdpx2ctzs3Z61ut8q7Ro7IMTklAbkgXJNeqRPO
MnIB/kX86P67g15u3uk410yQL5TZ/Adf8tuE=</latexit>YC
<latexit sha1_base64="7D4FGl/9EDzmXpfaHs9W+xSFrg=">AC
WXicbVDLSsNAFJ3EV42PVrt0M1gEVyUpoi4LblxWsFqwpUymNzo4mYSZG20J/RK3+lHizhJs7CtFwYO59zHmROmUhj0/W/H3djc2t6p7Xp7+we
H9cbR8YNJMs2hzxOZ6EHIDEihoI8CJQxSDSwOJTyGrzeF/vgG2ohE3eMshVHMnpWIBGdoqXGjPhjnw5jhC2cyv5nPx42W3/bLousgqECLVNUbHz
md4SThWQwKuWTGPAV+iqOcaRcwtwbZgZSxl/ZMzxZqFgMZpSXzuf0zDITGiXaPoW0ZP9O5Cw2ZhaHtrMwaVa1gvxXKxhtImNFuq5OTHFuxRtG16
NcqDRDUHxhLcokxYQWwdGJ0MBRzixgXAv7O8pfmGYcbxL2xW84xRhunxgQZeGPZtysJrpOnjotIPLdufuotXtVnXyAk5JeckIFekS25Jj/QJ
xn5IJ/ky/lxHbfmeotW16lmSp3OYv1gm24A=</latexit>XC
<latexit sha1_base64="pwgzH9v6pkIGnAeu8zPrKFGkeJw=">AC
WXicbVDLSsNAFJ3EV42PVrt0M1gEVyUpoi4LblwqtFqwpUymNzo4mYSZG20J/RK3+lHizhJs7CtFwYO59zHmROmUhj0/W/H3djc2t6p7Xp7+we
H9cbR8YNJMs2hzxOZ6EHIDEihoI8CJQxSDSwOJTyGrzeF/vgG2ohE9XCWwihmz0pEgjO01LhRH4zYczwhTOZ9+bzcaPlt/2y6DoIKtAiVd2Nj5
zOcJLwLAaFXDJjngI/xVHONAouYe4NMwMp46/sGZ4sVCwGM8pL53N6ZpkJjRJtn0Jasn8nchYbM4tD21mYNKtaQf6rFYw2kbEiXVcnpji34g2j61
EuVJohKL6wFmWSYkKL4OhEaOAoZxYwroX9HeUvTDONt6l7QrecYowXT6woEvDnk05WM10HTx02sFlu3N/0ep2q7xr5IScknMSkCvSJbfkjvQJ
xn5IJ/ky/lxHbfmeotW16lmSp3OYv9mC28Q=</latexit>XT
⊙
Elementwise product
<latexit sha1_base64="pwgzH9v6pkIGnAeu8zPrKFGkeJw=">ACWXicbVDLSsNAFJ3EV42PVrt0M1gEVyUpoi4LblwqtFqwpUymNzo4mYSZ
G20J/RK3+lHizhJs7CtFwYO59zHmROmUhj0/W/H3djc2t6p7Xp7+weH9cbR8YNJMs2hzxOZ6EHIDEihoI8CJQxSDSwOJTyGrzeF/vgG2ohE9XCWwihmz0pEgjO01LhRH4zYczwhTOZ9+bzcaPlt/2y6DoIKtAiVd2Nj5zOcJLwLAaFXDJjngI/xVHONAouYe4NMwMp46/sGZ4sVCwGM8pL53N6Zp
kJjRJtn0Jasn8nchYbM4tD21mYNKtaQf6rFYw2kbEiXVcnpji34g2j61EuVJohKL6wFmWSYkKL4OhEaOAoZxYwroX9HeUvTDONt6l7QrecYowXT6woEvDnk05WM10HTx02sFlu3N/0ep2q7xr5IScknMSkCvSJbfkjvQJxn5IJ/ky/lxHbfmeotW16lmSp3OYv9mC28Q=</latexit>XT
Self
Attention
query
MLP
value
mean
residual 
bootstrap
Adaptation 
Layer
Cross 
Attention
key
MLP
Self
Attention
MLP
<latexit sha1_base64="7D4FGl
/9EDzmXpfaHs9W+xSFrg=">ACWXicbVDLSsNAFJ3EV42PVrt0M1gE
VyUpoi4LblxWsFqwpUymNzo4mYSZG20J/RK3+lHizhJs7CtFwYO59z
HmROmUhj0/W/H3djc2t6p7Xp7+weH9cbR8YNJMs2hzxOZ6EHIDEihoI8
CJQxSDSwOJTyGrzeF/vgG2ohE3eMshVHMnpWIBGdoqXGjPhjnw5jhC2c
yv5nPx42W3/bLousgqECLVNUbHzmd4SThWQwKuWTGPAV+iqOcaRcwt
wbZgZSxl/ZMzxZqFgMZpSXzuf0zDITGiXaPoW0ZP9O5Cw2ZhaHtrMwaV
a1gvxXKxhtImNFuq5OTHFuxRtG16NcqDRDUHxhLcokxYQWwdGJ0MBRz
ixgXAv7O8pfmGYcbxL2xW84xRhunxgQZeGPZtysJrpOnjotIPLdufuo
tXtVnXyAk5JeckIFekS25Jj/QJxn5IJ/ky/lxHbfmeotW16lmSp
3OYv1gm24A=</latexit>XC
<latexit sha1_base64="HEVtZz
IPZCjr0h9GbVQ3xG14YE=">ACWXicbVDLSsNAFJ3EV42PVrt0M1gE
VyUpoi4LblxWsD6wpUymNzo4mYSZG20J/RK3+lHizhJs7CtFwYO59z
HmROmUhj0/W/HXVvf2NyqbXs7u3v79cbB4Z1JMs2hzxOZ6IeQGZBCQR8
FSnhINbA4lHAfvl4V+v0baCMSdYvTFIYxe1YiEpyhpUaN+uMoH8QMXzi
T+dVsNmq0/LZfFl0FQVapKre6MDpDMYJz2JQyCUz5inwUxzmTKPgEm
beIDOQMv7KnuHJQsViMO8dD6jJ5YZ0yjR9imkJft3ImexMdM4tJ2FSb
OsFeS/WsFoExkr0lV1bIpzS94wuhzmQqUZguJza1EmKSa0CI6OhQaOc
moB41rY31H+wjTjaONd2K7gHScIk8UDc7o07NmUg+VMV8Fdpx2ctzs3Z
61ut8q7Ro7IMTklAbkgXJNeqRPOMnIB/kX86P67g15u3uk410yQL
5TZ/Adf8tuE=</latexit>YC
<latexit sha1_base64="Ch1aMIJEpvcRZ5T9kms7+SNba4=">AC
X3icbVDLSsNAFJ3Gd321igtxM1gEVyUpoi4L3bhUsA8wpUymN3ZwMgkzN2oJ+Ri3+kUu/RMnbRa29cLA4Zz7OHOCRAqDrvtdcdbWNza3tnequ3v
7B4e1+lHPxKnm0OWxjPUgYAakUNBFgRIGiQYWBRL6wUun0PuvoI2I1SNOExhG7FmJUHCGlhrVTvwJw2yQjzI/YjhTGadPB/VGm7TnRVdBV4JGq
Ss+1G90vLHMU8jUMglM+bJcxMcZkyj4BLyqp8aSBh/Yc/wZKFiEZhNvOf0wvLjGkYa/sU0hn7dyJjkTHTKLCdhUmzrBXkv1rBaBMaK9JVdWyKc0
veMLwdZkIlKYLic2thKinGtIiPjoUGjnJqAeNa2N9RPmGacbQhL2xX8IbvCO+LB+b0zHDVpuwtZ7oKeq2md91sPVw12u0y721yRs7JfHIDWmTO
3JPuoSTjHyQT/JV+XG2nAOnNm91KuXMVko5/QXwIK5rQ=</latexit> ˆ
XC
<latexit sha1_base64="fjau3+1LebTs7CANaIqmfHP1vtI=">AC
X3icbVDLSsNAFJ3GV62vqrgQN4NFcFWSIuqy4MZlBdsqpTJ9MYOTiZh5kYtIR/jVr/IpX/ipM3Ctl4YOJxzH2dOkEh0HW/K87K6tr6RnWztrW
9s7tX3z/omTjVHLo8lrF+CJgBKR0UaCEh0QDiwIJ/eDlptD7r6CNiNU9ThIYROxZiVBwhpYa1o/8McPsMR9mfsRwzJnMbvJ8WG+4TXdadBl4JW
iQsjrD/UrLH8U8jUAhl8yYJ89NcJAxjYJLyGt+aiBh/IU9w5OFikVgBtnUf07PLDOiYaztU0in7N+JjEXGTKLAdhYmzaJWkP9qBaNaKxIl9WRKc
4teMPwepAJlaQIis+shamkGNMiPjoSGjKiQWMa2F/R/mYacbRhjy3XcEbviO8zx+Y0VPDNZuyt5jpMui1mt5ls3V30Wi3y7yr5IScknPikSvSJ
rekQ7qEk4x8kE/yVflxNpxdpz5rdSrlzCGZK+f4F8J2ua4=</latexit> ˆ
YC
<latexit sha1_base64="Ch1aMIJEpvcRZ5T9kms7+SNba4=">AC
X3icbVDLSsNAFJ3Gd321igtxM1gEVyUpoi4L3bhUsA8wpUymN3ZwMgkzN2oJ+Ri3+kUu/RMnbRa29cLA4Zz7OHOCRAqDrvtdcdbWNza3tnequ3v
7B4e1+lHPxKnm0OWxjPUgYAakUNBFgRIGiQYWBRL6wUun0PuvoI2I1SNOExhG7FmJUHCGlhrVTvwJw2yQjzI/YjhTGadPB/VGm7TnRVdBV4JGq
Ss+1G90vLHMU8jUMglM+bJcxMcZkyj4BLyqp8aSBh/Yc/wZKFiEZhNvOf0wvLjGkYa/sU0hn7dyJjkTHTKLCdhUmzrBXkv1rBaBMaK9JVdWyKc0
veMLwdZkIlKYLic2thKinGtIiPjoUGjnJqAeNa2N9RPmGacbQhL2xX8IbvCO+LB+b0zHDVpuwtZ7oKeq2md91sPVw12u0y721yRs7JfHIDWmTO
3JPuoSTjHyQT/JV+XG2nAOnNm91KuXMVko5/QXwIK5rQ=</latexit> ˆ
XC
<latexit sha1_base64="fjau3+1LebTs7CANaIqmfHP1vtI=">AC
X3icbVDLSsNAFJ3GV62vqrgQN4NFcFWSIuqy4MZlBdsqpTJ9MYOTiZh5kYtIR/jVr/IpX/ipM3Ctl4YOJxzH2dOkEh0HW/K87K6tr6RnWztrW
9s7tX3z/omTjVHLo8lrF+CJgBKR0UaCEh0QDiwIJ/eDlptD7r6CNiNU9ThIYROxZiVBwhpYa1o/8McPsMR9mfsRwzJnMbvJ8WG+4TXdadBl4JW
iQsjrD/UrLH8U8jUAhl8yYJ89NcJAxjYJLyGt+aiBh/IU9w5OFikVgBtnUf07PLDOiYaztU0in7N+JjEXGTKLAdhYmzaJWkP9qBaNaKxIl9WRKc
4teMPwepAJlaQIis+shamkGNMiPjoSGjKiQWMa2F/R/mYacbRhjy3XcEbviO8zx+Y0VPDNZuyt5jpMui1mt5ls3V30Wi3y7yr5IScknPikSvSJ
rekQ7qEk4x8kE/yVflxNpxdpz5rdSrlzCGZK+f4F8J2ua4=</latexit> ˆ
YC
Self 
Attention
MLP
Cross 
Attention
MLP
Self 
Attention
MLP
Self 
Attention
MLP
Cross 
Attention
MLP
MLP
<latexit sha1_base64="jX+blko4OsRnA7deWMoBNZzEntI=">ACYXicbVBNS8NAEN3G7/pV9eChl8UieCpJEfVY6MWjgrUFW8pmM7FLN5uwO9GWkF/jVX+QZ/+ImzYH2zqw8HhvZufN8xMpDLrud8XZ2Nza3tndq
+4fHB4d105On02cag5dHstY931mQAoFXRQoZ9oYJEvoedPOoXewNtRKyecJbAMGKvSoSCM7TUqHY+QCEDyPr5KBtEDMecyayT56Naw2686LrwCtBg5T1MDqptAZBzNMIFHLJjHnx3ASHGdMouIS8OkgNJIxP2Cu8WKhYBGaYzS/I6aVlAhrG2j6FdM7+nchYZMws8m1nYdKsagX5r1Yw2oTGinRdDUyxbsUbhnfDTKgkRVB8YS1MJcWYFgHSQGjgKGcWMK6FvY7yMdOMo4156XcF7zhFmC4vWNBzw1Wbsrea6Tp4bjW9m2br8brRbpd575I6uS
BXxCO3pE3uyQPpEk5y8kE+yVflx9lzas7potWplDNnZKmc+i+xNrqW</latexit> ˜
XC
<latexit sha1_base64="uLa7AyzMoiHFzZsh9ZeqCLvPbY=">ACYXicbVBNS8NAEN3Gr7Z+VT148LJYBE8lKaIeC714rGCtYkvZbCa6uNmE3Ym2hPwar/qDPtH3KQ92NaBhcd7Mztvnp9IYdB1vyvO2vrG5la1V
t/e2d3bxwc3ps41Rz6PJaxfvCZASkU9FGghIdEA4t8CQP/tVvogzfQRsTqDqcJjCL2rEQoOENLjRvHQxQygOwxH2fDiOELZzLr5vm40XRbl0FXhz0CTz6o0PKu1hEPM0AoVcMmOePDfBUcY0Ci4hrw9TAwnjr+wZnixULAIzysoLcnpmYCGsbZPIS3ZvxMZi4yZRr7tLEyaZa0g/9UKRpvQWJGuqoEp1i15w/B6lAmVpAiKz6yFqaQY0yJAGgNHOXUAsa1sNdR/sI042hjXvhdwTtOECaLC2Z0abhuU/aWM10F9+2Wd9lq3140O5153lVyQk
7JOfHIFemQG9IjfcJTj7IJ/mq/Dg1p+EczlqdynzmiCyUc/ILsyq6lw=</latexit> ˜
YC
<latexit sha1_base64="7D4FGl/9EDzmXpfaHs9W+xSFrg=">ACWXicbVDLSsNAFJ3EV42PVrt0M1gEVyUpoi4LblxWsFqwpUymNzo4mYSZG20J/RK3+lHizhJs7CtFwYO59zHmROmUhj0/W/H3djc2t6p7Xp7+
weH9cbR8YNJMs2hzxOZ6EHIDEihoI8CJQxSDSwOJTyGrzeF/vgG2ohE3eMshVHMnpWIBGdoqXGjPhjnw5jhC2cyv5nPx42W3/bLousgqECLVNUbHzmd4SThWQwKuWTGPAV+iqOcaRcwtwbZgZSxl/ZMzxZqFgMZpSXzuf0zDITGiXaPoW0ZP9O5Cw2ZhaHtrMwaVa1gvxXKxhtImNFuq5OTHFuxRtG16NcqDRDUHxhLcokxYQWwdGJ0MBRzixgXAv7O8pfmGYcbxL2xW84xRhunxgQZeGPZtysJrpOnjotIPLdufuotXtVnXyAk5JeckIFekS2
5Jj/QJxn5IJ/ky/lxHbfmeotW16lmSp3OYv1gm24A=</latexit>XC
<latexit sha1_base64="HEVtZzIPZCjr0h9GbVQ3xG14YE=">ACWXicbVDLSsNAFJ3EV42PVrt0M1gEVyUpoi4LblxWsD6wpUymNzo4mYSZG20J/RK3+lHizhJs7CtFwYO59zHmROmUhj0/W/HXVvf2NyqbXs7u
3v79cbB4Z1JMs2hzxOZ6IeQGZBCQR8FSnhINbA4lHAfvl4V+v0baCMSdYvTFIYxe1YiEpyhpUaN+uMoH8QMXziT+dVsNmq0/LZfFl0FQVapKre6MDpDMYJz2JQyCUz5inwUxzmTKPgEmbeIDOQMv7KnuHJQsViMO8dD6jJ5YZ0yjR9imkJft3ImexMdM4tJ2FSbOsFeS/WsFoExkr0lV1bIpzS94wuhzmQqUZguJza1EmKSa0CI6OhQaOcmoB41rY31H+wjTjaONd2K7gHScIk8UDc7o07NmUg+VMV8Fdpx2ctzs3Z61ut8q7Ro7IMTklAbkgX
JNeqRPOMnIB/kX86P67g15u3uk410yQL5TZ/Adf8tuE=</latexit>YC
<latexit sha1_base64="pwgzH9v6pkIGnAeu8zPrKFGkeJw=">ACWXicbVDLSsNAFJ3EV42PVrt0M1gEVyUpoi4LblwqtFqwpUymNzo4mYSZG20J/RK3+lHizhJs7CtFwYO59zHmROmUhj0/W/H3djc2t6p7Xp7+weH9
cbR8YNJMs2hzxOZ6EHIDEihoI8CJQxSDSwOJTyGrzeF/vgG2ohE9XCWwihmz0pEgjO01LhRH4zYczwhTOZ9+bzcaPlt/2y6DoIKtAiVd2Nj5zOcJLwLAaFXDJjngI/xVHONAouYe4NMwMp46/sGZ4sVCwGM8pL53N6ZpkJjRJtn0Jasn8nchYbM4tD21mYNKtaQf6rFYw2kbEiXVcnpji34g2j61EuVJohKL6wFmWSYkKL4OhEaOAoZxYwroX9HeUvTDONt6l7QrecYowXT6woEvDnk05WM10HTx02sFlu3N/0ep2q7xr5IScknMSkCvSJbfkjvQJxn5IJ/
ky/lxHbfmeotW16lmSp3OYv9mC28Q=</latexit>XT
<latexit sha1_base64="jX+blko4OsRnA7deWMoBNZzEntI=">ACYXicbVBNS8NAEN3G7/pV9eChl8UieCpJEfVY6MWjgrUFW8pmM7FLN5uwO9GWkF/jVX+QZ/+ImzYH2zqw8HhvZufN8xMpDLrud8XZ2Nza3tndq
+4fHB4d105On02cag5dHstY931mQAoFXRQoZ9oYJEvoedPOoXewNtRKyecJbAMGKvSoSCM7TUqHY+QCEDyPr5KBtEDMecyayT56Naw2686LrwCtBg5T1MDqptAZBzNMIFHLJjHnx3ASHGdMouIS8OkgNJIxP2Cu8WKhYBGaYzS/I6aVlAhrG2j6FdM7+nchYZMws8m1nYdKsagX5r1Yw2oTGinRdDUyxbsUbhnfDTKgkRVB8YS1MJcWYFgHSQGjgKGcWMK6FvY7yMdOMo4156XcF7zhFmC4vWNBzw1Wbsrea6Tp4bjW9m2br8brRbpd575I6uS
BXxCO3pE3uyQPpEk5y8kE+yVflx9lzas7potWplDNnZKmc+i+xNrqW</latexit> ˜
XC
<latexit sha1_base64="uLa7AyzMoiHFzZsh9ZeqCLvPbY=">ACYXicbVBNS8NAEN3Gr7Z+VT148LJYBE8lKaIeC714rGCtYkvZbCa6uNmE3Ym2hPwar/qDPtH3KQ92NaBhcd7Mztvnp9IYdB1vyvO2vrG5la1V
t/e2d3bxwc3ps41Rz6PJaxfvCZASkU9FGghIdEA4t8CQP/tVvogzfQRsTqDqcJjCL2rEQoOENLjRvHQxQygOwxH2fDiOELZzLr5vm40XRbl0FXhz0CTz6o0PKu1hEPM0AoVcMmOePDfBUcY0Ci4hrw9TAwnjr+wZnixULAIzysoLcnpmYCGsbZPIS3ZvxMZi4yZRr7tLEyaZa0g/9UKRpvQWJGuqoEp1i15w/B6lAmVpAiKz6yFqaQY0yJAGgNHOXUAsa1sNdR/sI042hjXvhdwTtOECaLC2Z0abhuU/aWM10F9+2Wd9lq3140O5153lVyQk
7JOfHIFemQG9IjfcJTj7IJ/mq/Dg1p+EczlqdynzmiCyUc/ILsyq6lw=</latexit> ˜
YC
<latexit sha1_base64="7D4FGl/9EDzmXpfaHs9W+xSFrg=">ACWXicbVDLSsNAFJ3EV42PVrt0M1gEVyUpoi4LblxWsFqwpUymNzo4mYSZG20J/RK3+lHizhJs7CtFwYO59zHmROmUhj0/W/H3djc2t6p7Xp7+
weH9cbR8YNJMs2hzxOZ6EHIDEihoI8CJQxSDSwOJTyGrzeF/vgG2ohE3eMshVHMnpWIBGdoqXGjPhjnw5jhC2cyv5nPx42W3/bLousgqECLVNUbHzmd4SThWQwKuWTGPAV+iqOcaRcwtwbZgZSxl/ZMzxZqFgMZpSXzuf0zDITGiXaPoW0ZP9O5Cw2ZhaHtrMwaVa1gvxXKxhtImNFuq5OTHFuxRtG16NcqDRDUHxhLcokxYQWwdGJ0MBRzixgXAv7O8pfmGYcbxL2xW84xRhunxgQZeGPZtysJrpOnjotIPLdufuotXtVnXyAk5JeckIFekS2
5Jj/QJxn5IJ/ky/lxHbfmeotW16lmSp3OYv1gm24A=</latexit>XC
<latexit sha1_base64="HEVtZzIPZCjr0h9GbVQ3xG14YE=">ACWXicbVDLSsNAFJ3EV42PVrt0M1gEVyUpoi4LblxWsD6wpUymNzo4mYSZG20J/RK3+lHizhJs7CtFwYO59zHmROmUhj0/W/HXVvf2NyqbXs7u
3v79cbB4Z1JMs2hzxOZ6IeQGZBCQR8FSnhINbA4lHAfvl4V+v0baCMSdYvTFIYxe1YiEpyhpUaN+uMoH8QMXziT+dVsNmq0/LZfFl0FQVapKre6MDpDMYJz2JQyCUz5inwUxzmTKPgEmbeIDOQMv7KnuHJQsViMO8dD6jJ5YZ0yjR9imkJft3ImexMdM4tJ2FSbOsFeS/WsFoExkr0lV1bIpzS94wuhzmQqUZguJza1EmKSa0CI6OhQaOcmoB41rY31H+wjTjaONd2K7gHScIk8UDc7o07NmUg+VMV8Fdpx2ctzs3Z61ut8q7Ro7IMTklAbkgX
JNeqRPOMnIB/kX86P67g15u3uk410yQL5TZ/Adf8tuE=</latexit>YC
<latexit sha1_base64="pwgzH9v6pkIGnAeu8zPrKFGkeJw=">ACWXicbVDLSsNAFJ3EV42PVrt0M1gEVyUpoi4LblwqtFqwpUymNzo4mYSZG20J/RK3+lHizhJs7CtFwYO59zHmROmUhj0/W/H3djc2t6p7Xp7+weH9
cbR8YNJMs2hzxOZ6EHIDEihoI8CJQxSDSwOJTyGrzeF/vgG2ohE9XCWwihmz0pEgjO01LhRH4zYczwhTOZ9+bzcaPlt/2y6DoIKtAiVd2Nj5zOcJLwLAaFXDJjngI/xVHONAouYe4NMwMp46/sGZ4sVCwGM8pL53N6ZpkJjRJtn0Jasn8nchYbM4tD21mYNKtaQf6rFYw2kbEiXVcnpji34g2j61EuVJohKL6wFmWSYkKL4OhEaOAoZxYwroX9HeUvTDONt6l7QrecYowXT6woEvDnk05WM10HTx02sFlu3N/0ep2q7xr5IScknMSkCvSJbfkjvQJxn5IJ/
ky/lxHbfmeotW16lmSp3OYv9mC28Q=</latexit>XT
Self 
Attention
<latexit sha1_base64="pwgzH9v6pkIGnAeu8zPrKFGkeJw=">ACWXicbVDLSsNAFJ3EV42PVrt0M1gEVyUpoi4LblwqtFqwpUymNzo4mYSZ
G20J/RK3+lHizhJs7CtFwYO59zHmROmUhj0/W/H3djc2t6p7Xp7+weH9cbR8YNJMs2hzxOZ6EHIDEihoI8CJQxSDSwOJTyGrzeF/vgG2ohE9XCWwihmz0pEgjO01LhRH4zYczwhTOZ9+bzcaPlt/2y6DoIKtAiVd2Nj5zOcJLwLAaFXDJjngI/xVHONAouYe4NMwMp46/sGZ4sVCwGM8pL53N6Zp
kJjRJtn0Jasn8nchYbM4tD21mYNKtaQf6rFYw2kbEiXVcnpji34g2j61EuVJohKL6wFmWSYkKL4OhEaOAoZxYwroX9HeUvTDONt6l7QrecYowXT6woEvDnk05WM10HTx02sFlu3N/0ep2q7xr5IScknMSkCvSJbfkjvQJxn5IJ/ky/lxHbfmeotW16lmSp3OYv9mC28Q=</latexit>XT
<latexit sha1_base64="XhlHKC/kEk/I8z4M4LkNtY9x+Sg=">ACX3icbVDLSsNAFJ3GV62vqrgQN4NFcFWSIuqy4MZlhdYHpTJ9MYOTiZh
5kZbQj7GrX6RS/ESdqFb0wcDjnPs6cIJHCoOt+V5yV1bX1jepmbWt7Z3evn9wb+JUc+jxWMb6MWAGpFDQ4ESHhMNLAokPASvN4X+8AbaiFh1cZJAP2IvSoSCM7TUoH7kjxhmT/kg8yOGI85k1s3zQb3hNt2y6DLwZqBZtUZ7Fda/jDmaQKuWTGPHtugv2MaRcQl7zUwMJ46/sBZ4tVCwC08
9K/zk9s8yQhrG2TyEt2b8TGYuMmUSB7SxMmkWtIP/VCkab0FiRLqtDU5xb8IbhdT8TKkRFJ9aC1NJMaZFfHQoNHCUEwsY18L+jvIR04yjDXlu4J3HCOM5w9M6dJwzabsLWa6DO5bTe+y2bq7aLTbs7yr5IScknPikSvSJrekQ3qEk4x8kE/yVflxNpxdpz5tdSqzmUMyV87xL+LNub8=</latexi
t> ˆ
YT
query
value
key
query
value
key
<latexit sha1_base64="HIXMoVwbeKOI+MbKpraOxcpkYlQ=">AC
WnicbVDLSgMxFE3HZ1sf9bFzEyCqzJTRF0WRHCpYB/QlpLJ3GlDM5khuVNbhv6JW/0nwY8xU7uwrQcCh3PuTU6On0h0HW/Cs7W9s7u3n6xVD4
4PDqunJy2TJxqDk0ey1h3fGZACgVNFCihk2hgkS+h7Y8fcr89AW1ErF5xlkA/YkMlQsEZWmlQqfQpoiYPSoeB6Dng0rVrbkL0E3iLUmVLPE8OC
nUe0HM0wgUcsmM6Xpugv2MaRcwrzUSw0kjI/ZELqWKhaB6WeL6HN6ZWAhrG2RyFdqH83MhYZM4t8OxkxHJl1Lxf/9XJFm9BYk26gcmfW8uG4X
0/EypJERT/jRamkmJM8+ZoIDRwlDNLGNfC/o7yEdOMo+135XYFbzjNe51vyovAJduyt97pJmnVa95trf5yU20ln3vkwtySa6JR+5IgzyRZ9Ikn
EzIO/kgn4Vvx3GKTvl31Cksd87ICpzHzVKt4s=</latexit>Encoder
<latexit sha1_base64="SCRbn0LCdynRVcTM6NU/sJbHNA=">ACWnicbVDLSgMxFE3HZ1sf9bFzEyCqzJTRF0WdOFSwT6gLSWTudOGZjJD
cqe2DP0Tt/pPgh9jpnZhWw8EDufcm5wcP5HCoOt+FZyt7Z3dvf1iqXxweHRcOTltmTjVHJo8lrHu+MyAFAqaKFBCJ9HAIl9C2x8/5H57AtqIWL3iLIF+xIZKhIztNKgUukhTBExewQeB6Dng0rVrbkL0E3iLUmVLPE8OCnUe0HM0wgUcsmM6Xpugv2MaRcwrzUSw0kjI/ZELqWKhaB6WeL6HN6Z
WAhrG2RyFdqH83MhYZM4t8OxkxHJl1Lxf/9XJFm9BYk26gcmfW8uG4X0/EypJERT/jRamkmJM8+ZoIDRwlDNLGNfC/o7yEdOMo+135XYFbzjNe51vyovAJduyt97pJmnVa95trf5yU20ln3vkwtySa6JR+5IgzyRZ9IknEzIO/kgn4Vvx3GKTvl31Cksd87ICpzHyIbt4E=</latexit>Decoder
<latexit sha1_base64="SCRbn0LCdynRVcTM6NU/sJbHNA=">ACWnicbVDLSgMxFE3HZ1sf9bFzEyCqzJTRF0WdOFSwT6gLSWTudOGZjJD
cqe2DP0Tt/pPgh9jpnZhWw8EDufcm5wcP5HCoOt+FZyt7Z3dvf1iqXxweHRcOTltmTjVHJo8lrHu+MyAFAqaKFBCJ9HAIl9C2x8/5H57AtqIWL3iLIF+xIZKhIztNKgUukhTBExewQeB6Dng0rVrbkL0E3iLUmVLPE8OCnUe0HM0wgUcsmM6Xpugv2MaRcwrzUSw0kjI/ZELqWKhaB6WeL6HN6Z
WAhrG2RyFdqH83MhYZM4t8OxkxHJl1Lxf/9XJFm9BYk26gcmfW8uG4X0/EypJERT/jRamkmJM8+ZoIDRwlDNLGNfC/o7yEdOMo+135XYFbzjNe51vyovAJduyt97pJmnVa95trf5yU20ln3vkwtySa6JR+5IgzyRZ9IknEzIO/kgn4Vvx3GKTvl31Cksd87ICpzHyIbt4E=</latexit>Decoder
<latexit sha1_base64="SCRbn0LCdynRVcTM6NU/sJbHNA=">ACWnicbVDLSgMxFE3HZ1sf9bFzEyCqzJTRF0WdOFSwT6gLSWTudOGZjJD
cqe2DP0Tt/pPgh9jpnZhWw8EDufcm5wcP5HCoOt+FZyt7Z3dvf1iqXxweHRcOTltmTjVHJo8lrHu+MyAFAqaKFBCJ9HAIl9C2x8/5H57AtqIWL3iLIF+xIZKhIztNKgUukhTBExewQeB6Dng0rVrbkL0E3iLUmVLPE8OCnUe0HM0wgUcsmM6Xpugv2MaRcwrzUSw0kjI/ZELqWKhaB6WeL6HN6Z
WAhrG2RyFdqH83MhYZM4t8OxkxHJl1Lxf/9XJFm9BYk26gcmfW8uG4X0/EypJERT/jRamkmJM8+ZoIDRwlDNLGNfC/o7yEdOMo+135XYFbzjNe51vyovAJduyt97pJmnVa95trf5yU20ln3vkwtySa6JR+5IgzyRZ9IknEzIO/kgn4Vvx3GKTvl31Cksd87ICpzHyIbt4E=</latexit>Decoder
<latexit sha1_base64="7D4FGl/9EDzmXpfaHs9W+xSFrg=">AC
WXicbVDLSsNAFJ3EV42PVrt0M1gEVyUpoi4LblxWsFqwpUymNzo4mYSZG20J/RK3+lHizhJs7CtFwYO59zHmROmUhj0/W/H3djc2t6p7Xp7+we
H9cbR8YNJMs2hzxOZ6EHIDEihoI8CJQxSDSwOJTyGrzeF/vgG2ohE3eMshVHMnpWIBGdoqXGjPhjnw5jhC2cyv5nPx42W3/bLousgqECLVNUbHz
md4SThWQwKuWTGPAV+iqOcaRcwtwbZgZSxl/ZMzxZqFgMZpSXzuf0zDITGiXaPoW0ZP9O5Cw2ZhaHtrMwaVa1gvxXKxhtImNFuq5OTHFuxRtG16
NcqDRDUHxhLcokxYQWwdGJ0MBRzixgXAv7O8pfmGYcbxL2xW84xRhunxgQZeGPZtysJrpOnjotIPLdufuotXtVnXyAk5JeckIFekS25Jj/QJ
xn5IJ/ky/lxHbfmeotW16lmSp3OYv1gm24A=</latexit>XC
<latexit sha1_base64="HIXMoVwbeKOI+MbKpraOxcpkYlQ=">ACWnicbVDLSgMxFE3HZ1sf9bFzEyCqzJTRF0WRHCpYB/QlpLJ3GlDM5kh
uVNbhv6JW/0nwY8xU7uwrQcCh3PuTU6On0h0HW/Cs7W9s7u3n6xVD4PDqunJy2TJxqDk0ey1h3fGZACgVNFCihk2hgkS+h7Y8fcr89AW1ErF5xlkA/YkMlQsEZWmlQqfQpoiYPSoeB6Dng0rVrbkL0E3iLUmVLPE8OCnUe0HM0wgUcsmM6Xpugv2MaRcwrzUSw0kjI/ZELqWKhaB6WeL6HN6Z
WAhrG2RyFdqH83MhYZM4t8OxkxHJl1Lxf/9XJFm9BYk26gcmfW8uG4X0/EypJERT/jRamkmJM8+ZoIDRwlDNLGNfC/o7yEdOMo+135XYFbzjNe51vyovAJduyt97pJmnVa95trf5yU20ln3vkwtySa6JR+5IgzyRZ9IknEzIO/kgn4Vvx3GKTvl31Cksd87ICpzHzVKt4s=</latexit>Encoder
<latexit sha1_base64="7D4FGl/9EDzmXpfaHs9W+xSFrg=">AC
WXicbVDLSsNAFJ3EV42PVrt0M1gEVyUpoi4LblxWsFqwpUymNzo4mYSZG20J/RK3+lHizhJs7CtFwYO59zHmROmUhj0/W/H3djc2t6p7Xp7+we
H9cbR8YNJMs2hzxOZ6EHIDEihoI8CJQxSDSwOJTyGrzeF/vgG2ohE3eMshVHMnpWIBGdoqXGjPhjnw5jhC2cyv5nPx42W3/bLousgqECLVNUbHz
md4SThWQwKuWTGPAV+iqOcaRcwtwbZgZSxl/ZMzxZqFgMZpSXzuf0zDITGiXaPoW0ZP9O5Cw2ZhaHtrMwaVa1gvxXKxhtImNFuq5OTHFuxRtG16
NcqDRDUHxhLcokxYQWwdGJ0MBRzixgXAv7O8pfmGYcbxL2xW84xRhunxgQZeGPZtysJrpOnjotIPLdufuotXtVnXyAk5JeckIFekS25Jj/QJ
xn5IJ/ky/lxHbfmeotW16lmSp3OYv1gm24A=</latexit>XC
<latexit sha1_base64="kFr+H3/CpjP2jARDg8w2wfSULSI=">ACXnicbVDLSsNAFJ3EV61W3UhuBksgquSFGXBTcuFWwr2FIm05t2cDIJ
MzdqDfkXt/pH7vwUJ20XtvXCwOGc+zhzgkQKg5737bhr6xubW6Xt8s5uZW+/WjvomDjVHNo8lrF+DJgBKRS0UaCEx0QDiwIJ3eD5ptC7L6CNiNUDThLoR2ykRCg4Q0sNqkc9I0YRG2S9iOGYM5k95PmgWvca3rToKvDnoE7mdTeoOc3eMOZpBAq5ZMY8+V6C/YxpFxCXu6lBhLGn9kInixULALTz6
b2c3pmSENY2fQjpl/05kLDJmEgW2szBplrWC/FcrG1CY0W6qg5NcW7JG4bX/UyoJEVQfGYtTCXFmBbp0aHQwFOLGBcC/s7ysdM424XtCl7xDeFt8cCMnhou25T95UxXQafZ8C8bzfuLeqs1z7tETsgpOSc+uSItckvuSJtw8k4+yCf5cn7cTbfi7s9aXWc+c0gWyj3+Bc8uTg=</latexi
t>σT
<latexit sha1_base64="IuL6JLWNMmuEcCxzOur0mPjBeDM=">ACW3icbVDLTgIxFC2D0RU0Lhy0hMXJEZYtQliRuXmPBKgJBOuQMNnc6k
vaOSCZ/iVr/Jhf9iB1gIeJMmJ+fcx+nxYykMu53zsnv7R8cFo6Kx6WT07Ny5bxjokRzaPNIRrnMwNSKGijQAm9WAMLfQldf/aU6d1X0EZEqoXzGIYhmygRCM7QUqNyZRAmo3QMpxyJtPWYjEqV92auy6C7w1qJ1NUeVXH0wjngSgkIumTF9z41xmDKNgktYFAeJgZjxGZtA30LFQjDdOl9QW
8sM6ZBpO1TSJfs34mUhcbMQ92ZibNtpaR/2oZo01grEh31bHJzm15w+BxmAoVJwiKr6wFiaQY0Sw6OhYaOMq5BYxrYX9H+ZRpxtEGvLFdwRu+I7xvHljRS8NFm7K3neku6NRr3n2t/nJXbTWeRfIFbkmt8QjD6RBnkmTtAknb+SDfJKv3I+Td4pOadXq5NYzF2SjnMtfAwa36w=</latexit>µT
<latexit sha1_base64="ePWJvSki1c98pR4/eTRLq96lMWo=">AC
fHicbVFNT9tAEN240FIKJbTHXqyGSkiIyEaU9ojaS49UIoAUW9F4MyYr9sPaHQPRyv+k1/Y/8WdQ1yaHhnSklZ7em915+6aopHCUJA+96MXa+st
XG6832xtv93p767cKa2HEfcSGOvCnAohcYRCZJ4VkEVUi8LG6+t/rlLVonjD6neYW5gmstSsGBAjXp97MZkM8U0Kwo/axpJv1BMky6ildBug
ADtqizyW7vPJsaXivUxCU4N06TinIPlgSX2GxmtcMK+A1c4zhADQpd7jvrTfwpMNO4NDYcTXH/nvDg3JurorQ2Xp0z7W/J82rqn8mnuhq5pQ86
dBZS1jMnGbQzwVFjnJeQDArQheYz4DC5xCWktTN7RPeE9Nat0N36JLgoViCByoxToqc+4ImrGae6z9hFtrALps/a7JSgh575ryMIOg6EuhUHa7
SF9nvoquDgap+Hyc/jwem3xUY2Af2ke2zlH1hp+wHO2Mjxtkt+8V+sz+9x2gvOogOn1qj3uLOe7ZU0clfS0rG9g=</latexit>ˆ
h
<latexit sha1_base64="YJCb+AbJ5y/ZlBxwc8zNahO5n8g=">AC
fHicbVFNb9NAEN0Y+kELbQpHLhahElJFZFdt4VjBhWORmrZSbEXjzbhZdT+s3XFpWPmf9Fr+E38GsXZzIA0jrfT03uzO2zdFJYWjJPndi549X1v
f2Hyxtf3y1c5uf+/1hTO15TjiRhp7VYBDKTSOSJDEq8oiqELiZXHztdUvb9E6YfQ5zSvMFVxrUQoOFKhJv5/NgHymgGZF6X82zaQ/SIZJV/EqSB
dgwBZ1NtnrnWdTw2uFmrgE58ZpUlHuwZLgEputrHZYAb+BaxwHqEGhy31nvYn3AzONS2PD0R37L83PCjn5qoIna1H91Rryf9p45rKz7kXuqoJNX
8cVNYyJhO3OcRTYZGTnAcA3IrgNeYzsMApLU0ReMPuiO8o2aV7sYv0UWhAhFEbpQCPfUZV0TNOM191j6ijVUgfdZ+twQl5Nx3DVnYTDUpTBIu
z2kT1NfBReHw/R4mHw/Gpx+Wxk71l79gHlrJP7JR9Y2dsxDi7Zfsgf3q/YneRwfRx8fWqLe484YtVXTyF2/sxwg=</latexit>ˆ
z
<latexit sha1_base64="NsGet1CJb1sBfGTPiJpWJOH9Dxg=">AC
inicbVFNT9tAEN24H1CgbWiPlSqLqFJPkY1aFcQFlR56pBIBpNiKxpsxWbEf1u4YSFe+9df0Cn+m/6ZrJ4eGdKSV3ryZ2Zl5U1RSOEqSP73oydN
nzc2X2xt7x89bq/+bcmdpyHEjb0swKEUGkckSOJlZRFUIfGiuD5p4xc3aJ0w+ozmFeYKrQoBQcK1KT/PpsB+UwBzYrS/2yaycLhIP1J8P
qDZJh0Fq+DdAkGbGmnk93eWTY1vFaoiUtwbpwmFeUeLAkusdnKaocV8Gu4wnGAGhS63HeLNPGHwEzj0tjwNMUd+2+FB+XcXBUhsx3SPY615P9i45
rKg9wLXdWEmi8albWMycStKvFUWOQk5wEAtyLMGvMZWOAUtFvpovGW7gjvqFmnu/YrdFGoQIQgN0qBnvqMK6JmnOY+az/RxqgdNauW4IScu67h
CxcNAzUqTBIuzukj1VfB+f7w/TzMPnxaXD8dXmRTfaO7bGPLGVf2DH7zk7ZiH2i/1m9+wh2on2o8PoaJEa9ZY1b9mKRd/+ApaDzF0=</latexi
t>ˆ
zC
<latexit sha1_base64="h3Sqk2O38zROeFx9LzGd7EMTUfw=">AC
inicbVFNT9tAEN24tFCgbShHpMoiqtRTZCOqtuoFlR4UokAUmxF482YrNgPa3dMiVa+9df02v6Z/hvWTg4NYaSV3ryZ2Zl5U1RSOEqSf73o2cb
zF5tbL7d3dl+9ftPfe3vpTG05jriRxl4X4FAKjSMSJPG6sgiqkHhV3J628as7tE4YfUHzCnMFN1qUgMFatJ/l82AfKaAZkXpZ0zWTgcpD8NXn
+QDJPO4nWQLsGALe18ste7yKaG1wo1cQnOjdOkotyDJcElNtZ7bACfgs3OA5Qg0KX+26RJn4fmGlcGhueprhj/6/woJybqyJktkO6x7GWfCo2rq
n8nHuhq5pQ80WjspYxmbhVJZ4Ki5zkPADgVoRZYz4DC5yCditdNP6ke8J7atbprv0KXRQqECHIjVKgpz7jiqgZp7nP2k+0sSonbXrlqCEnPsuI
QsXDQN1KgzS7g7pY9XweXRMP04TH4cD06+LS+yxQ7YIfvAUvaJnbAzds5GjLNf7Df7w/5Gu9FR9CX6ukiNesuafbZi0fcHcOXMSw=</latexi
t>ˆ
hC
<latexit sha1_base64="UuX046gL0Q9+bGwtHrnahwQny2o=">AChHicbVFNSxBEO2dGPMh2tyCXgZXAI5hGUmUZKTiF5yVHBV2BmWmt4abeyPobs
mujTjr/Gq/yf/Jj3jHlzXgobHe1VdVa+KSgpHSfKvF71aeb36Zu3t+rv3Hz5u9Dc/nTpTW4jbqSx5wU4lELjiARJPK8sgioknhVXh61+9hetE0af0KzCXMGFqXgQIGa9L9kCuiyKP1M/Ed5iD9YdNM+oNkmHQRL4N0DgZsHkeTzd5JNjW8VqiJS3BunCYV5R4sCS6xWc9qhxXwK7jAcYAaFLrcdys08dfATOPS2PA0
xR37tMKDcm6mipDZDumeay35kjauqfyde6GrmlDzx0ZlLWMycetHPBUWOclZAMCtCLPG/BIscAquLXTReE03hDfULNd+wW6KFQgsiNUqCnPuOKqBmnuc/aT7SxKjidteuWoISc+S4hC7cMA3UuDNLuDulz15fB6Y9hujtMjncG+wfzi6yxLbNvrGU/WL7A87YiPG2S27Y/fsIVqNvkc/o93H1Kg3r/nMFiLa+w82
smN</latexit>wC
<latexit sha1_base64="L9Nd836brixz3m+G/B8qvu5pMS4=">AChHicbVFNT9tAEN2YQikUGtpLpV6sRkg9oMiGInqULlwpBIBpNiKxpsx
rNgPa3dcSFfm1/Ta/p/+G9Ymh4Z0pJWe3pvZmXlTVFI4SpK/vWjlxeray/VXG5uvt7bf9HfeXjhTW4jbqSxVwU4lELjiARJvKosgiokXha3J61+QOtE0af06zCXMG1FqXgQIGa9N9nCuimKP3PZuI7zEH6k6aZ9AfJMOkiXgbpHAzYPM4mO73zbGp4rVATl+DcOE0qyj1YElxis5HVDivgt3CN4w
A1KHS571Zo4t3ATOPS2PA0xR37b4UH5dxMFSGzHdI91ryf9q4pvJL7oWuakLNnxqVtYzJxK0f8VRY5CRnAQC3Iswa8xuwCm4tBF4x3dE95Ts0x37RfolCBCI3SoGe+owromac5j5rP9HGquB01q5bghJy5ruELNwyDNS5MEi7O6TPXV8GF/vD9HCYfP8OP42v8g6+8A+sk8sZUfsmJ2yMzZi
nD2wX+w3+xOtRXvRQXT4lBr15jXv2EJEXx8BPTJkA=</latexit>zC
<latexit sha1_base64="oQ/9voCOB/UtKE1sVAFwhu4e32M=">AChHicbVFNT9tAEN24LeWjH6G9IHGxGiH1UEU2LWpPCJVLjyARQIqtaLwZ
kxX7Ye2OW6KV+TVc6f/pv+na5EAI6309N7MzsybopLCUZL860UvXr5ae72+sbn15u279/3tD+fO1JbjiBtp7GUBDqXQOCJBEi8ri6AKiRfF9XGrX/xG64TRZzSvMFdwpUpOFCgJv2dTAHNitLPmonvMAfpj5tm0h8kw6SLeBWkCzBgiziZbPfOsqnhtUJNXIJz4zSpKPdgSXCJzWZWO6yAX8MVjg
PUoNDlvluhifcCM41LY8PTFHfs4woPyrm5KkJmO6R7qrXkc9q4pvJH7oWuakLNHxqVtYzJxK0f8VRY5CTnAQC3Iswa8xlY4BRcW+qi8Q/dEN5Qs0p37ZfolCBCI3SoGe+owromac5j5rP9HGquB01q5bghJy7ruELNwyDNS5MEi7O6RPXV8F5/vD9GCYnH4bHP1cXGSd7bJP7DNL2Xd2xH6xEzZi
nN2yO3bP/kZr0Zfoa3TwkBr1FjUf2VJEh/8BF2jJfg=</latexit>hC
<latexit sha1_base64="UuX046gL0Q9+bGwtHrnahwQny2o=">AChHicbVFNSxBEO2dGPMh2tyCXgZXAI5hGUmUZKTiF5yVHBV2BmWmt4abeyPobs
mujTjr/Gq/yf/Jj3jHlzXgobHe1VdVa+KSgpHSfKvF71aeb36Zu3t+rv3Hz5u9Dc/nTpTW4jbqSx5wU4lELjiARJPK8sgioknhVXh61+9hetE0af0KzCXMGFqXgQIGa9L9kCuiyKP1M/Ed5iD9YdNM+oNkmHQRL4N0DgZsHkeTzd5JNjW8VqiJS3BunCYV5R4sCS6xWc9qhxXwK7jAcYAaFLrcdys08dfATOPS2PA0
xR37tMKDcm6mipDZDumeay35kjauqfyde6GrmlDzx0ZlLWMycetHPBUWOclZAMCtCLPG/BIscAquLXTReE03hDfULNd+wW6KFQgsiNUqCnPuOKqBmnuc/aT7SxKjidteuWoISc+S4hC7cMA3UuDNLuDulz15fB6Y9hujtMjncG+wfzi6yxLbNvrGU/WL7A87YiPG2S27Y/fsIVqNvkc/o93H1Kg3r/nMFiLa+w82
smN</latexit>wC
<latexit sha1_base64="UuX046gL0Q9+bGwtHrnahwQny2o=">AChHic
bVFNSxBEO2dGPMh2tyCXgZXAI5hGUmUZKTiF5yVHBV2BmWmt4abeyPobsmujTjr/Gq/yf/Jj3jHlzXgobHe1VdVa+KSgpHSfKvF71aeb36Zu3t+rv3Hz
5u9Dc/nTpTW4jbqSx5wU4lELjiARJPK8sgioknhVXh61+9hetE0af0KzCXMGFqXgQIGa9L9kCuiyKP1M/Ed5iD9YdNM+oNkmHQRL4N0DgZsHkeTzd5J
NjW8VqiJS3BunCYV5R4sCS6xWc9qhxXwK7jAcYAaFLrcdys08dfATOPS2PA0xR37tMKDcm6mipDZDumeay35kjauqfyde6GrmlDzx0ZlLWMycetHPBUWOcl
ZAMCtCLPG/BIscAquLXTReE03hDfULNd+wW6KFQgsiNUqCnPuOKqBmnuc/aT7SxKjidteuWoISc+S4hC7cMA3UuDNLuDulz15fB6Y9hujtMjncG+wfzi
6yxLbNvrGU/WL7A87YiPG2S27Y/fsIVqNvkc/o93H1Kg3r/nMFiLa+w82smN</latexit>wC
<latexit sha1_base64="V/to4Xj+3TNzswvT9Qw4Ot0xX0Q=">ACdHicbVFNT9tAEN24LVAoNJRje7BqReIU2YiqHF76REkAojYisabMazY
D2t3BKt/C96bf9X/0jPXZscGsJIKz29N7PzZqaspXCUpn8G0YuXrzY2t15v7zZ3Xs73H936UxjOU64kcZel+BQCo0TEiTxurYIqpR4Vd5/7fSr72idMPqCFjUWCm61qAQHCtRNroDuysrbdjZM0nHaR7wOsiVI2DLOZvuDi3xueKNQE5fg3DRLayo8WBJcYrudNw5r4Pdwi9MANSh0he8t/EoMP
O4MjY8TXHP/l/hQTm3UGXI7Cy6p1pHPqdNG6pOCi903RBq/tioamRMJu7mj+fCIie5CAC4FcFrzO/AqewpZUuGn/QA+EDtet036FLksViCByoxTouc+5ImqnWeHz7hNtrALp827cCpSQC98n5OF2wVC/hSRruztkT7e+Di6PxtmncXp+nJx+WV5ki71nH9khy9hndsq+sTM2YZxp9pP9Yr8Hf6MP
URKNHlOjwbLmgK1ENP4Ho63EAg=</latexit>r
<latexit sha1_base64="wFH28Sc5nNA3SyZsOX+sCz9nz0=">ACfnicbVFNT9wEPWmtKWUtgs9colYUXHpNqlalSMqlx5BYgFpE60mzgQs
/BHZk3ZXVv4KV/qX+m9wh6LCNZenpv7Hl+U9RSOEqSf4PoxcbLV6832y93X73/sNwZ/fCmcZynHAjb0qwKEUGickSOJVbRFUIfGyuD3p9MvfaJ0w+pwWNeYKrWoBAcK1Gy4m5GQJfpMAd0UlbdtOxuOknHSV7wO0iUYsWdznYG51lpeKNQE5fg3DRNaso9WBJcYruVNQ5r4LdwjdMANSh0ue
/Nt/FBYMq4MjYcTXHP/n/Dg3JuoYrQ2Xl0T7WOfE6bNlQd5V7ouiHU/HFQ1ciYTNwlEZfCIie5CAC4FcFrzG/AqeQ18oUjX9oTjindp3ux6/QRaECEURulAJd+owrona5j7rHtHGKpA+675bgRJy4fuGLGwxGOpTGKX9HtKnqa+Di6/j9Ps4Ofs2Ov653Mgm2P7JCl7Ac7Zr/YKZswzubsjt2z
vxGLPkWfoy+PrdFgecjW6no6AFB0cbq</latexit>˜
r
<latexit sha1_base64="8yh+4jA8HN9vnN2o+V7R3hAsMbI=">ACdHicbVFNT9tAEN2YtlD6Be2xPVi1kHqK7IoKjoheqRSAqixhcabMVmx
H9buGIhW/he9wv/ij3Bm7eTQkI60tN7MztvZspaCkdp+jCINl68fLW59Xr7zdt37z/s7H48daxHMfcSGPS3AohcYxCZJ4XlsEVUo8K69+dvrZNVonjB7RvMZCwaUWleBAgfqTK6BZWflZe7GTpMO0j3gdZEuQsGWcXOwORvnU8EahJi7BuUmW1lR4sCS4xHY7bxzWwK/gEicBalDoCt9buO9wE
zjytjwNMU9+2+FB+XcXJUhs7Ponmsd+T9t0lB1WHih64ZQ80WjqpExmbibP54Ki5zkPADgVgSvMZ+BU5hSytdN7QLeEtet036FLksViCByoxToqc+5ImonWeHz7hNtrALp827cCpSQc98n5OF2wVC/hSRruztkz7e+Dk6/D7Mfw/T3fnJ0vLzIFvMvrJvLGMH7Ij9YidszDjT7C+7Y/eDx+hL
lER7i9RosKz5xFYiGj4Bj13D+A=</latexit>h
<latexit sha1_base64="2rNHGD6p25LEQBGRfZfoWXhXps=">ACdHicbVFNT9tAEN24LVAoX+2xPVi1kDhFNgLRI2ovHKmUAGpsofFmDCv2
w9odF9KV/0Wv9H/1j/TctZNDQzrSk/vzey8mSlrKRyl6e9B9OLlq7X1jdebW2+2d3b39t9eOtNYjmNupLHXJTiUQuOYBEm8ri2CKiVelfdfOv3qO1onjB7RrMZCwa0WleBAgfqWK6C7svI/2pu9JB2mfcSrIFuAhC3i4mZ/MqnhjcKNXEJzk2ytKbCgyXBJbabeOwBn4PtzgJUINCV/jechsfBG
YaV8aGpynu2X8rPCjnZqoMmZ1F91zryP9pk4aqT4UXum4INZ83qhoZk4m7+eOpsMhJzgIAbkXwGvM7sMApbGmpi8YHeiR8pHaV7tsv0WpAhFEbpQCPfU5V0TtJCt83n2ijVUgfd6NW4EScub7hDzcLhjqt5BkbXeH7PnWV8Hl0TA7GaZfj5Oz4uLbLD37CM7ZBk7ZWfsnF2wMeNMs5/sif0a/Ik+
REl0ME+NBouad2wpouFfs+3ECg=</latexit>z
<latexit sha1_base64="oQ/9voCOB/UtKE1sVAFwhu4e32M=">AChHicbVFNT9tAEN24LeWjH6G9IHGxGiH1UEU2LWpPCJVLjyARQIqtaLwZkxX7Ye2OW6KV+TVc6f/pv+na5EAI6309N7MzsybopLCUZL860UvXr5ae72+
sbn15u279/3tD+fO1JbjiBtp7GUBDqXQOCJBEi8ri6AKiRfF9XGrX/xG64TRZzSvMFdwpUpOFCgJv2dTAHNitLPmonvMAfpj5tm0h8kw6SLeBWkCzBgiziZbPfOsqnhtUJNXIJz4zSpKPdgSXCJzWZWO6yAX8MVjgPUoNDlvluhifcCM41LY8PTFHfs4woPyrm5KkJmO6R7qrXkc9q4pvJH7oWuakLNHxqVtYzJxK0f8VRY5CTnAQC3Iswa8xlY4BRcW+qi8Q/dEN5Qs0p37ZfolCBCI3SoGe+owromac5j5rP9HGquB01q5bghJy7ruELNwyDNS5MEi7O6RPXV
8F5/vD9GCYnH4bHP1cXGSd7bJP7DNL2Xd2xH6xEzZinN2yO3bP/kZr0Zfoa3TwkBr1FjUf2VJEh/8BF2jJfg=</latexit>hC
<latexit sha1_base64="L9Nd836brixz3m+G/B8qvu5pMS4=">AChHicbVFNT9tAEN2YQikUGtpLpV6sRkg9oMiGInqULlwpBIBpNiKxpsxrNgPa3dcSFfm1/Ta/p/+G9Ymh4Z0pJWe3pvZmXlTVFI4SpK/vWjlxeray/VX
G5uvt7bf9HfeXjhTW4jbqSxVwU4lELjiARJvKosgiokXha3J61+QOtE0af06zCXMG1FqXgQIGa9N9nCuimKP3PZuI7zEH6k6aZ9AfJMOkiXgbpHAzYPM4mO73zbGp4rVATl+DcOE0qyj1YElxis5HVDivgt3CN4wA1KHS571Zo4t3ATOPS2PA0xR37b4UH5dxMFSGzHdI91ryf9q4pvJL7oWuakLNnxqVtYzJxK0f8VRY5CRnAQC3Iswa8xuwCm4tBF4x3dE95Ts0x37RfolCBCI3SoGe+owromac5j5rP9HGquB01q5bghJy5ruELNwyDNS5MEi7O6TPXV
8GF/vD9HCYfP8OP42v8g6+8A+sk8sZUfsmJ2yMzZinD2wX+w3+xOtRXvRQXT4lBr15jXv2EJEXx8BPTJkA=</latexit>zC
<latexit sha1_base64="YC6l5mTj/1OcogjnhMTR2d+I4=">ACjHicbVFNaxRBEO0dv2LUuNGjl8ZF8LTMSERBhGBAPEbIJoGdYanpqck26Y+hu0azNHP13jV/+K/sWd2D27WgoZXr6q6ql6VjZKe0vTPKLlz979B3sP
9x89fnLwdHz47Nzb1gmcCausuyzBo5IGZyRJ4WXjEHSp8K8PunjF9/QeWnNGa0aLDRcGVlLARSpxZjnJFWFIdAy7IOy65brB0BKpxEbzxJp+lgfBdkGzBhGztdHI7O8sqKVqMhocD7eZY2VARwJIXCbj9vPTYgruEK5xEa0OiLMKzS8VeRqXhtXyG+MD+WxFAe7/SZczsh/S3Yz35v9i8pfp9EaRpWkIj1o3qVnGyvNeFV9KhILWKAISTcVYuluBAUFRvq4vB73RDeEPdLj2036LUkciBoXVGkwVcqGJunlWhLz/xFino9J5v24NWqpVGBLyeNM40KDCJBvukN
1WfRecv5lmb6fp16PJ8afNRfbYC/aSvWYZe8eO2Rd2ymZMsB/sJ/vFficHyVHyIfm4Tk1Gm5rnbMuSz38BhcjNA=</latexit>˜
hC
<latexit sha1_base64="1uD/P0LYHMZx+NQAdlLT6rE84FM=">ACjHicbVFNaxRBEO0dv2LUuNGjl8ZF8LTMSIKCMGAeIyQTQI7w1LTU5M06Y+hu0azNnP13jV/+K/sWd2D27WgoZXr6q6ql6VjZKe0vTPKLlz979BzsP
dx89frL3dLz/7Mzb1gmcCausuyjBo5IGZyRJ4UXjEHSp8Ly8Pu7j51/ReWnNKS0bLDRcGlLARSpxZjnJFWFIdAV2UdvnfdYuUIUOE4euNJOk0H49sgW4MJW9vJYn90mldWtBoNCQXez7O0oSKAIykUdrt567EBcQ2XOI/QgEZfhGVjr+KTMVr6+IzxAf234oA2vulLmNmP6S/HevJ/8XmLdXviBN0xIasWpUt4qT5b0uvJIOBalBCcjLNycQUOBEX1NroY/EY3hDfUbdND+w26LHUkYlBYrcFUIReaqJtnRcj7T4x1Oiqd9+vWoKVahiEhjzeNAw0qTLhDt
lt1bfB2ZtpdjhNvxMj6uL7LDXrCX7DXL2Ft2xD6zEzZjgv1gP9kv9jvZSw6S98mHVWoyWtc8ZxuWfPoLq2bNRg=</latexit>˜
zC
<latexit sha1_base64="8yh+4jA8HN9vnN2o+V7R3hAsMbI=">ACdHicbVFNT9tAEN2YtlD6Be2xPVi1kHqK7IoKjoheqRSAqixhcabMVmxH9buGIhW/he9wv/ij3Bm7eTQkI60tN7MztvZspaCkdp+jCINl68fLW59Xr7
zdt37z/s7H48daxHMfcSGPS3AohcYxCZJ4XlsEVUo8K69+dvrZNVonjB7RvMZCwaUWleBAgfqTK6BZWflZe7GTpMO0j3gdZEuQsGWcXOwORvnU8EahJi7BuUmW1lR4sCS4xHY7bxzWwK/gEicBalDoCt9buO9wEzjytjwNMU9+2+FB+XcXJUhs7Ponmsd+T9t0lB1WHih64ZQ80WjqpExmbibP54Ki5zkPADgVgSvMZ+BU5hSytdN7QLeEtet036FLksViCByoxToqc+5ImonWeHz7hNtrALp827cCpSQc98n5OF2wVC/hSRruztkz7e+Dk6/D7Mfw/T3fn
J0vLzIFvMvrJvLGMH7Ij9YidszDjT7C+7Y/eDx+hLlER7i9RosKz5xFYiGj4Bj13D+A=</latexit>h
<latexit sha1_base64="2rNHGD6p25LEQBGRfZfoWXhXps=">ACdHicbVFNT9tAEN24LVAoX+2xPVi1kDhFNgLRI2ovHKmUAGpsofFmDCv2w9odF9KV/0Wv9H/1j/TctZNDQzrSk/vzey8mSlrKRyl6e9B9OLlq7X1jdeb
W2+2d3b39t9eOtNYjmNupLHXJTiUQuOYBEm8ri2CKiVelfdfOv3qO1onjB7RrMZCwa0WleBAgfqWK6C7svI/2pu9JB2mfcSrIFuAhC3i4mZ/MqnhjcKNXEJzk2ytKbCgyXBJbabeOwBn4PtzgJUINCV/jechsfBGYaV8aGpynu2X8rPCjnZqoMmZ1F91zryP9pk4aqT4UXum4INZ83qhoZk4m7+eOpsMhJzgIAbkXwGvM7sMApbGmpi8YHeiR8pHaV7tsv0WpAhFEbpQCPfU5V0TtJCt83n2ijVUgfd6NW4EScub7hDzcLhjqt5BkbXeH7PnWV8Hl0TA7GaZfj5
Oz4uLbLD37CM7ZBk7ZWfsnF2wMeNMs5/sif0a/Ik+REl0ME+NBouad2wpouFfs+3ECg=</latexit>z
<latexit sha1_base64="8PDJ3EBw2iShHPSHZLj8ghA8i9k=">ACfnicbVFNb9NAEJ2YQktpIS1HLhZRUS+kNgLRY0UvHIvUtJViK1qvx82q+2Htjkuilf8KV/hL/Ju3RxIw0grPb03u/P2TVFL4ShJ/g6iZ1vPX2zvNx9
tbf/+s3w4PDKmcZynHAjb0pmEMpNE5IkMSb2iJThcTr4u6806/v0Tph9CUta8wVu9WiEpxRoGbDw4yELNFnitG8qPy8bWfDUTJO+o3QboCI1jVxexgcJmVhjcKNXHJnJumSU25Z5YEl9juZo3DmvE7dovTADVT6HLfm2/jo8CUcWVsOJrinv3hmfKuaUqQmfn0T3VOvJ/2rSh6jT3QtcNoeaPg6pGxmTiLom4FBY5yWUAjFsRvMZ8zizjFPJam6LxJy0IF9Ru0v34NboVCyI1STJc+4qona5z7pHtLGKSZ91362YEnLp+4YsbDEY6lMYpf0e0qepb4KrT+
P0yzj58Xl09m21kR14B+/hGFL4CmfwHS5gAhwW8At+w58Iog/Rx+jksTUarO68hbWKTh8ALXfG4A=</latexit>˜
h
<latexit sha1_base64="y1PFDvzAgxMAxa0SPuApvaT6NE=">ACfnicbVFNT9wEJ0NtAX6tdBjL1FXVL10m6BW5YjaS49UYgFpE60cZwIW/ojsCezWyl/plf4l/g1O2APLdiRLT+NPc9viloKR0lyN4g2Np89f7G1vfPy
1es3b4e7e6fONJbjhBtp7HnBHEqhcUKCJ7XFpkqJ4Vz87/ewarRNGn9CixlyxCy0qwRkFajbcy0jIEn2mGF0Wlf/TtrPhKBknfcXrIF2CESzreLY7OMlKwxuFmrhkzk3TpKbcM0uCS2x3sZhzfgVu8BpgJopdLnvzbfxfmDKuDI2HE1xz6+4ZlybqGK0Nl5dE+1jvyfNm2oOsy90HVDqPnDoKqRMZm4SyIuhUVOchEA41YErzG/ZJZxCnmtTNF4Q3PCObXrdD9+hS4KFYgcqMU06XPuCJqp2nus+4Rbaxi0mfdyumhFz4viELWwyG+hRGab+H9Gnq6+D0YJ
x+Gye/v46Ofiw3sgXv4QN8ghS+wxH8gmOYAIc5/IVb+BdB9DH6H15aI0GyzvYKWiw3tSGcby</latexit>˜
z
Fig. 2. A single forward computation of (a) BANP and (b) NeuBANP. Note that the inference for each target point
requires B times of this forward computation.
element-wisely to compute another bootstrapped representation. Cross-attention module uses XC and
this bootstrapped representation as key-value pairs to which the target query x attends. Consequently,
reﬂecting the bootstrap weights shared with the posterior path, the prediction path models interactions
between the given context and the target input.
We summarize the above posterior and prediction paths as follows:
z(b)
C
= {z(b)
c }c∈C = SelfAttn(D(b)
C ),
z(b) = mean(z(b)
C
⊙w(b)
C )
(8)
hC = {hc}c∈C = SelfAttn(DC),
h(b) = CrossAttn(x, XC, hC ⊙w(b)
C )
(9)
where ⊙denotes element-wise multiplication. We concatenate random bootstrap weights to context
data, unlike NeuBoots, which only utilizes random weight multiplication in the ﬁnal layer. Concatena-
tion of weights and the contexts in the posterior path yields two effects. First, concatenating random
information in given contexts provides sufﬁcient randomness to the model, allowing it to cover a
wide range of function samples in a space where a true function is likely to exist. Second, we made
this modiﬁcation to maximize the use of random weights and provide bootstrapping information
to the model without resampling the data, enabling better uncertainty estimation. Additionally, by
multiplying the random weights to the representations from both paths, as in (8) and (9), the repre-
sentations are consistent with the weights and maximize bootstrapping effect. We think that these
multiplications also provide the effect of regularization, which mitigates the overﬁtting tendency
which ANP and BANP show in a simple regression experiment (see Figure 1).
Decoder and Bootsrapped Prediction
The decoder takes (z(b), h(b)) from the encoder and target
x ∈X as inputs to generate a prediction ˆ
y(b) = MLP(x, z(b), h(b)). We can generate bootstrap sam-
ples ˆ
y(1), ˆ
y(2), ..., ˆ
y(B) by plugging w(1)
C , w(2)
C , ..., w(B)
C
into the encoder gφ(x, DC, ·), respectively.
We estimate the predictive mean and standard deviation using bootstrap samples:
µ = 1
B
B
X
b=1
ˆ
y(b),
σ =
v
u
u
t
1
B −1
B
X
b=1
(ˆ
y(b) −µ)2
(10)
In the previous NP methods, the decoder directly outputs the parameters of the predictive distribution,
but the decoder of NeuBANP outputs the stochastic predictions {ˆ
y(b)}B
b=1. This design of output is
a distinction of our model from other NP methods and allows the nonparametric estimation. Due
to this structure, existing NPs set the lower bound of standard deviation for robust performance.
5
Under review as a conference paper at ICLR 2022
We found that the performance was susceptible to the lower bound value. However, NeuBANP
naturally obtains parameters of predictive distribution using bootstrap predictions and shows better
performance without such heuristics. It also has the advantage of calculating higher-order statistics
without changing the structure.
3.2
TRAINING
Weighted Bootstrapping Loss
We train NeuBANP with weighted loss similar to that of NeuBoots
as demonstrated in Section 2.3. Only context data has the corresponding bootstrap weight in our
setting, but the model still has to ﬁt target data. Thus we designed loss function as a sum of weighted
context loss Lcontext and non-weighted target loss Ltarget as follows:
Ltotal = 1
B
B
X
b=1
1
|C|
X
c∈C

−w(b)
c
log N(yc|µc, σ2
c)

|
{z
}
Lcontext
+ 1
|T |
X
t∈T

−log N(yt|µt, σ2
t )

|
{z
}
Ltarget
(11)
where µc, σc, µt, and σt are computed by (10) for context (xc, yc) ∈DC and target (xt, yt) ∈DT .
Averaging weighted context loss with multiple bootstrap weights improves the robustness of a
model by showing various bootstrap samples during training. We trained the model with negative
log-likelihood (NLL). One can replace the NLL with a different loss function, such as cross-entropy
according to the target tasks.
4
EXPERIMENTS
We conducted experiments to compare NeuBANP with the previous NP methods for regression
and sequential decision-making problems. For regression tasks, we conducted one-dimensional
(1D) regression experiments on random functions generated from GP prior and image completion
tasks as two-dimensional (2D) regression (see Appendix B.5 for image completion). For sequential
decision-making problems, we evaluated each method in Bayesian optimization (BO) and Contextual
Multi-Armed Bandit (CMAB).
4.1
NONPARAMETRIC REGRESSION AND UNCERTAINTY ESTIMATION
Settings
We followed the settings in Lee et al. (2020). To obtain meta-training datasets, we sampled
batches of random functions from GP prior with RBF kernel, and context and target points were
chosen randomly from each function. In addition, kernel parameters of GP were randomly sampled so
that the models could learn about various functions. NeuBANP was trained with 10 bootstrap samples,
and we conﬁrmed that it is robust to the number of samples. Please refer to Appendix B.2 for details.
Results
The numerical results are summarized in Table 2. ANP and BANP tend to estimate homo-
geneous uncertainties for all target points in a situation where context points are sufﬁciently given
(see Figure 3), because they place the heuristic lower bound on the variance. In the case of BANP,
homoscedasticity occurs even when the number of context is small. Another problem only occurs in
BANP, which is that it does not properly estimate functional uncertainty. Since the uncertainty about
the shape of the true function is high in the region where the context point is not given, models should
generate a wide range of function samples. However, we can see that BANP generates almost the same
functions, not like those of the other methods, so we argue that BANP is not an appropriate method
for modeling functional uncertainty. NeuBANP resolves these problems efﬁciently and achieves the
best performance except for target prediction in the Periodic kernel.
4.2
BAYESIAN OPTIMIZATION
Since NP can approximate a class of arbitrary functions, it can replace GP, the commonly used
surrogate model of BO. It is crucial to approximate the objective function from the given observations
using the surrogate model, but evaluating the acquisition function and determining the subsequent
samples are vital for efﬁcient exploration. We evaluated the proposed method on various black-box
functions, which may be unobserved in the meta-training step (see Algorithm 1).
6
Under review as a conference paper at ICLR 2022
Fig. 3. Comparison of ANP, BANP, and NeuBANP in 1D regression given 4 context points (top) and 20 context
points (bottom). Orange lines represent the ground-truth function. Blue lines are predictive mean given by each
model and shaded region denotes the standard deviation (amount of uncertainty). To visualize the quality of
functional uncertainty, we overlapped multiple shaded areas obtained with 30 sampled outputs for each input.
Settings
For 1D, we followed the same setting in Lee et al. (2020). We set objective functions gen-
erated from GP with RBF, Matérn 5/2, and Periodic kernels and applied the models trained in Section
4.1. Furthermore, we demonstrated the BO performance of NeuBANP for multi-dimensional settings
(2D and 3D). We set objective functions as various benchmark functions used in the optimization
literature (Kim, 2020; Kim & Choi, 2017). See Appendix B.3 for details. A simple regret measured
the performance of each model, and the mean performance over 100 experiments is reported for
reliable evaluations.
Fig. 4. Left: Multi-dimensional Bayesian optimization results on various benchmark functions with UCB as an
acquisition function. Bold lines represent the mean performance over 100 experiments. We indicate 20% of the
standard deviation. Right most: Time complexity of each model as the number of observations increases.
Results
NeuBANP outperformed the other methods in BO experiments (see Figure 4 and 8). Table
3 and 4 shows numerical results. For multi-dimensional BO, NeuBANP achieved the best results for
every function except the Hartmann-3D when using Upper Conﬁdence Bound (UCB) as an acquisition
function. For the Hartmann-3D, the performance of NeuBANP is statistically comparable to the best
method (ANP) when considering the standard deviation. For Goldsteinprice and Rastrigin functions,
GP records poor performance due to its numerical errors during the optimization procedure (see
Appendix B.3). The rightmost plots of Figure 4 show the time complexity according to the number
of observation points. NeuBANP has the fastest decreasing rate of regrets in terms of iterations
in both cases. We also conducted BO experiments using Expected Improvement (EI) to test the
performance of NeuBANP to be independent of the selection of acquisition functions (Figure 9).
Figure 10 demonstrates the better exploration strategy of NeuBANP compared to BANP. The second
7
Under review as a conference paper at ICLR 2022
Regret
Method
δ = 0.5
δ = 0.7
δ = 0.9
δ = 0.95
δ = 0.99
Cumulative
Uniform
100.00 ± 0.08
100.00 ± 0.09
100.00 ± 0.25
100.00 ± 0.37
100.00 ± 0.78
Neural Linear
0.95 ± 0.02
1.60 ± 0.03
4.65 ± 0.18
9.56 ± 0.36
49.63 ± 2.41
MAML
2.95 ± 0.12
3.11 ± 0.16
4.84 ± 0.22
7.01 ± 0.33
22.93 ± 1.57
NP
1.60 ± 0.06
1.75 ± 0.05
3.31 ± 0.10
5.71 ± 0.24
22.13 ± 1.23
ANP
2.17 ± 1.89
3.59 ± 8.03
5.63 ± 8.48
11.68 ± 8.97
24.75 ± 7.08
BANP
2.04 ± 1.52
2.34 ± 1.23
4.30 ± 0.77
6.76 ± 1.03
21.18 ± 1.69
NeuBANP
0.85 ± 0.22
1.02 ± 0.27
1.85 ± 0.56
3.04 ± 0.88
9.76 ± 1.93
Simple
Uniform
100.00 ± 0.45
100.00 ± 0.78
100.00 ± 1.18
100.00 ± 2.21
100.00 ± 4.21
Neural Linear
0.33 ± 0.04
0.79± 0.07
2.17 ± 0.14
4.08 ± 0.20
35.89 ± 2.98
MAML
2.49 ± 0.12
3.00 ± 0.35
4.75 ± 0.48
7.10 ± 0.77
22.89 ± 1.41
NP
1.04 ± 0.06
1.26 ± 0.21
2.90 ± 0.35
5.45 ± 0.47
21.45 ± 1.3
ANP
0.99 ± 1.68
1.50 ± 2.21
3.64 ± 4.71
6.32 ± 7.33
21.65 ± 1.72
BANP
1.22 ± 1.83
2.37 ± 3.04
3.27 ± 5.33
7.73 ± 12.16
20.63 ± 34.21
NeuBANP
0.86 ± 0.06
1.04 ± 0.08
1.88 ± 0.14
3.09 ± 0.23
9.96 ± 0.70
Table 1. Results of the wheel bandit problem according to the value of δ. Mean and standard deviation for
cumulative regret and simple regret over 50 runs are reported. Regrets are normalized to that of the uniform
policy.
and fourth columns show the explored points by BANP and NeuBANP, respectively. In most cases,
NeuBANP converged to the optimum faster than BANP. The third and ﬁfth column shows the contour
plots of acquisition functions of each method. Note that NeuBANP accurately infers the potential
area of the optimum compared to BANP. As explained above, NeuBANP can estimate heterogeneous
uncertainty, and thus it is able to handle well for the exploration-exploitation trade-off, which is
essential in sequential decision-making problems.
4.3
CONTEXTUAL MULTI-ARMED BANDIT
We conducted a CMAB experiment, the wheel bandit problem, as in Garnelo et al. (2018b) to show
that NeuBANP works efﬁciently as well as BO based on its performance of uncertainty estimation.
Settings
We followed the same environment in Garnelo et al. (2018b), but used UCB policy. For
more details, please refer to Appendix B.4. The parameter δ determines the environment of the
wheel bandit problem. As δ increases, high-reward observation becomes sparse, which makes the
problem more difﬁcult. We set the baselines of CMAB experiment to MAML (Finn et al., 2017), Neural
Linear (Riquelme et al., 2018) and NP. We measured cumulative regrets and simple regrets for 2,000
iterations to demonstrate the performance of each model.
Results
NeuBANP performed well for various δ values (see Table 1). Note that NeuBANP showed
better performance in challenging environments with sparse high rewards. NeuBANP showed the
ability to learn various reward distributions based on appropriate functional uncertainty modeling.
The result demonstrates that NeuBANP can utilize a small number of context information and make
accurate estimations. NeuBANP also has stable results as its small variance shows. On the other hand,
ANP has extremely high variance in the performance because it overﬁts to certain prediction and
failed to adapt to the various bandit environments.
5
RELATED WORK
Neural Processes
CNP (Garnelo et al., 2018a) uses a pair of encoder and decoder networks to
produce a predictive posterior distribution over functions. NP (Garnelo et al., 2018b) introduces a
global latent variable to embed functional uncertainty in the deterministic architecture of CNP and
predicts various outputs given the same context data. ANP (Kim et al., 2018) then enhances predictive
accuracy by replacing MLP modules in NP with the attention modules. BNP (Lee et al., 2020) proposes
the bootstrap method to model uncertainty in stochastic processes without the assumption of a single
latent variable on which previous methods rely. In addition to these works, there are many attempts
to use NPs in various tasks. Singh et al. (2019) tackles sequential stochastic processes, where the
8
Under review as a conference paper at ICLR 2022
dynamics of the given system changes as the time being. Leveraging time-variant context points,
Singh et al. (2019) models the underlying temporal 3D structures. Gordon et al. (2020) extends NP
families to contain translation equivariant functions, providing theoretical formulation to represent
translation-invariant functional representations. Louizos et al. (2019) do not assume explicit global
latent variables, instead supported by dependency graph among local latent variables, to encode
inductive bias for given data easier than NPs that use global latent variables.
Bootstrapping Neural Networks
Bootstrap method (Efron, 1987) is a reliable approach to esti-
mate predictive uncertainty (Lakshminarayanan et al., 2017; Osband et al., 2016). However, it is
computationally inefﬁcient to go through the feed-forward computation as much as the number of
bootstraps; hence, it discourages the practical application of bootstrap in neural networks. There have
been several works to circumvent this issue by approximating bootstrapped distribution. Amortized
bootstrap (Nalisnick & Smyth, 2017) approximates bootstrap distribution over model parameters
by using amortized inference and implicit models. Generative Bootstrap Sampler (Shin et al., 2020)
proposes a computational bootstrap procedure that constructs a generator function of bootstrap evalu-
ations for classical statistical models. Neural Bootstrapper (Shin et al., 2021) suggests a simple recipe
for generating bootstrapped predictive distributions of MLPs and convolutional neural networks.
Meta-Learning based Stochastic Optimization
NPs are trained with a meta-learning framework
to solve various tasks related to the data generation process through a single optimization. Santoro
et al. (2016); Chen et al. (2017) follow the same training procedure of NP. However, Santoro et al.
(2016) proposes an memory-augmented network for robust meta-learning, while Chen et al. (2017)
proposes the method to produce an algorithm for black-box optimization using recurrent networks.
Sharaf & Daumé III (2019) presents a meta-learning algorithm for learning a good exploration policy
in the contextual bandit. Ravi & Beatson (2019) also solves contextual bandits based on the Bayesian
framework by inferring a posterior on weights of neural networks. Galashov et al. (2019) introduces
a uniﬁed framework for applying NP to a wide range of sequential decision-making problems.
6
CONCLUSION
We have proposed NeuBANP, a novel bootstrap method for a family of NP to model functional
uncertainty appropriately. Instead of the standard bootstrap, NeuBANP learns to construct a generator
function that produces valid bootstrapped distribution without resampling which can be considered as
a learn-to-bootstrap method. NeuBANP successfully worked in a meta-learning framework, providing
diverse trajectories of underlying data-generating processes, consistent to any given context. In
addition, NeuBANP estimates the local uncertainty accurately, resolving overﬁtted prediction and
variance overestimation problems observed in both ANP and BANP. We replace the additional layer
and repetitive computations in BANP with the simple attachment of bootstrap weights to the model,
which leads to lower computations and smaller memory. NeuBANP shows superior performance to
previous NP methods in regression and stochastic optimization tasks, including multi-dimensional
setting, which has been the desired application of NP. However, due to the numerical instability of GP,
there is a limit in sampling high-dimensional stochastic processes for the meta-learning framework.
We suggest that this is a primary task for scalable applications of NP in stochastic optimization, as a
challenging research direction.
7
REPRODUCIBILITY
For reproducibility of experimental results, we provide a link to anonymous github that contains
our source code in Appendix B. The source code includes the implementation of our model, data
generation, and experiments. Additionally, the data generation steps are thoroughly explained in
Appendix B.
REFERENCES
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016.
9
Under review as a conference paper at ICLR 2022
Maximilian Balandat, Brian Karrer, Daniel R. Jiang, Samuel Daulton, Benjamin Letham, Andrew Gor-
don Wilson, and Eytan Bakshy. Botorch: A framework for efﬁcient Monte-Carlo bayesian opti-
mization. In Advances in Neural Information Processing Systems, 2020.
Yutian Chen, Matthew W. Hoffman, Sergio Gómez Colmenarejo, Misha Denil, Timothy P. Lillicrap,
Matt Botvinick, and Nando de Freitas. Learning to learn without gradient descent by gradient
descent. In International Conference on Machine Learning, pp. 748–756. PMLR, 2017.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and André van Schaik. Emnist: Extending MNIST
to handwritten letters. In International Joint Conference on Neural Networks, pp. 2921–2926.
IEEE, 2017.
Bradley Efron. Better bootstrap conﬁdence intervals. Journal of the American statistical Association,
82(397):171–185, 1987.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In International Conference on Machine Learning, pp. 1126–1135. PMLR, 2017.
Alexandre Galashov, Jonathan Schwarz, Hyunjik Kim, Marta Garnelo, David Saxton, Pushmeet
Kohli, S.M. Ali Eslami, and Yee Whye Teh. Meta-learning surrogate models for sequential decision
making. arXiv preprint arXiv:1903.11907, 2019.
Jacob Gardner, Geoff Pleiss, Kilian Q. Weinberger, David Bindel, and Andrew Gordon Wilson.
Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration. In Advances
in Neural Information Processing Systems, volume 31, pp. 7576–7586, 2018.
Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray
Shanahan, Yee Whye Teh, Danilo Rezende, and S.M. Ali Eslami. Conditional neural processes. In
International Conference on Machine Learning, pp. 1704–1713. PMLR, 2018a.
Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J Rezende, SM Eslami, and
Yee Whye Teh. Neural processes. arXiv preprint arXiv:1807.01622, 2018b.
Jonathan Gordon, Wessel P. Bruinsma, Andrew Y. K. Foong, James Requeima, Yann Dubois, and
Richard E. Turner. Convolutional conditional neural processes, 2020.
Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, Ali Eslami, Dan Rosenbaum, Oriol
Vinyals, and Yee Whye Teh. Attentive neural processes. In International Conference on Learning
Representations, 2018.
Jungtaek Kim.
Benchmark functions for bayesian optimization.
https://github.com/
jungtaekkim/bayeso-benchmarks, 2020.
Jungtaek Kim and Seungjin Choi. BayesO: A Bayesian optimization framework in Python. https:
//bayeso.org, 2017.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (Poster), 2015.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. In Advances in Neural Information Processing
Systems, 2017.
Tuan Anh Le, Hyunjik Kim, Marta Garnelo, Dan Rosenbaum, Jonathan Schwarz, and Yee Whye Teh.
Empirical evaluation of neural process objectives. In Advances in Neural Information Processing
Systems Workshop on Bayesian Deep Learning, 2018.
Juho Lee, Yoonho Lee, Jungtaek Kim, Eunho Yang, Sung Ju Hwang, and Yee Whye Teh. Bootstrap-
ping neural processes. In Advances in Neural Information Processing Systems, 2020.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
International Conference on Computer Vision, pp. 3730–3738, 2015.
Christos Louizos, Xiahan Shi, Klamer Schutte, and Max Welling. The functional neural process.
arXiv preprint arXiv:1906.08324, 2019.
10
Under review as a conference paper at ICLR 2022
Eric Nalisnick and Padhraic Smyth. The amortized bootstrap. In International Conference on
Machine Learning 2017 Workshop on Implicit Models, 2017.
Michael A. Newton and Adrian E. Raftery. Approximate Bayesian inference with the weighted
likelihood bootstrap. Journal of the Royal Statistical Society: Series B (Methodological), 56(1):
3–26, 1994.
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped DQN. arXiv preprint arXiv:1602.04621, 2016.
Jens Præstgaard and Jon A Wellner. Exchangeably weighted bootstraps of the general empirical
process. The Annals of Probability, pp. 2053–2086, 1993.
Carl Edward Rasmussen. Gaussian processes in machine learning. In Summer School on Machine
Learning, pp. 63–71. Springer, 2003.
Sachin Ravi and Alex Beatson. Amortized bayesian meta-learning. In International Conference on
Learning Representations, 2019.
Carlos Riquelme, George Tucker, and Jasper Snoek. Deep bayesian bandits showdown: An empirical
comparison of bayesian deep networks for thompson sampling. In International Conference on
Learning Representations, 2018.
Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-
learning with memory-augmented neural networks. In International Conference on Machine
Learning, pp. 1842–1850. PMLR, 2016.
Amr Sharaf and Hal Daumé III. Meta-learning for contextual bandit exploration. arXiv preprint
arXiv:1901.08159, 2019.
Minsuk Shin, Lu Wang, and Jun S Liu. Scalable uncertainty quantiﬁcation via generativebootstrap
sampler. arXiv preprint arXiv:2006.00767, 2020.
Minsuk Shin, Hyungjoo Cho, Hyun-seok Min, and Sungbin Lim. Neural bootstrapper. In Advances
in Neural Information Processing Systems, 2021.
Gautam Singh, Jaesik Yoon, Youngsung Son, and Sungjin Ahn. Sequential neural processes. In
Advances in Neural Information Processing Systems, volume 32, 2019.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information
Processing Systems, 2017.
A
MODEL
A.1
BASIC OPERATION
Muti-Layer Perceptron
MLP(di, dh, do, nl) denotes the multi-layer perceptron consisting of nl
linear transformations and ReLU activations between them. Let parameters di, dh, do the dimension
of input, hidden and output feature, repectively. We used same dh for every hidden layers. Lin(p, q)
denotes the linear transformation of input with feature dimension p into output with feature dimension
q.
MLP(di, dh,do, nl)(X) = Lin(dh, do) ◦(ReLU ◦Lin(dh, dh))nl−2 ◦ReLU ◦Lin(di, dh)(X)
(12)
Dot Product Attention
DotProdAttn (Vaswani et al., 2017) denotes the attention operation with
attention score based on cosine similarity. Dot product of query (Q) and key (K) tensors calculates
the similarity of each vectors in query tensor relative to each vectors in key tensor. Attention score is
calculated through softmax operation of normalized similiarity. Let dk the feature dimension of key
and query tensors, and dv that of value tensors.
DotProdAttn(Q, K, V ) = softmax
 QT K/
p
dk

V
(13)
11
Under review as a conference paper at ICLR 2022
Multi-Head Attention
MHA denotes multi-head attention with dot product attention. The input is
pre-processed with MLP and the output of attention is post-processed with layer normalization. For
simplicity, we omit parameters of each MLP layers. split(X, n) denotes splitting of tensor X with
respect to feature axis into a tuple of n tensors (X′
i)n
i=1 which have same dimension in feature axis.
[X1, X2, ..., Xn] denotes the concatenation of tensors X1, X2, ..., Xn with respect to feature axis.
LayerNorm denotes the layer normalization introduced in Ba et al. (2016). Let nhead the number of
heads in multi-head attention.
(Q′
i)nhead
i=1
= split(MLPqk(Q), nhead)
(14)
(K′
i)nhead
i=1
= split(MLPqk(K), nhead)
(15)
(V ′
i )nhead
i=1
= split(MLPv(V ), nhead)
(16)
MHA(Q, K, V ) = LayerNorm([(DotProdAttn(Q′
i, K′
i, V ′
i ))nhead
i=1
])
(17)
Self-Attention
We used self-attention to calculate efﬁcient representations of context DC =
(XC, YC). We deﬁne self-attention based on multi-head attention as follows:
D′
C = [XC, YC] = ([xc, yc])c∈C,
SelfAttn(DC) := MHA(D′
C, D′
C, D′
C)
(18)
Cross-Attention
We used cross-attention to calculate representation of context speciﬁc to target
feature x in interest, when original representations (hc)c∈C is given. We deﬁne cross-attention based
on multi-head attention as follows:
CrossAttn(x, XC, (hc)c∈C) := MHA(x, XC, [(hc)c∈C]).
(19)
A.2
ARCHITECTURE
For fair comparison, we used the same architecture of all models as in Lee et al. (2020). For NeuBANP,
we increased the input dimension of SelfAttn in the posterior path by one to take bootstrap weight as
additional input. Please refer to Lee et al. (2020) for detailed model architecture of ANP and BANP.
A.3
NON-ATTENTIVE CASE
As an ablation study, we consider the non-attentive case of NeuBANP, called Neural Bootstrapping
Neural Processes (NeuBNP). Like the architecture of NeuBANP is based on ANP, the architecture of
this model is based on NP, the non-attentive counterpart of ANP. With D(b)
C
= {(xc, yc, w(b)
c }c∈C as
deﬁned in 3, we applied the similar strategy of using random weights in the encoder as follows, and
trained with the same loss function:
z(b)
C
= {z(b)
c }c∈C = MLP(D(b)
C ,
z(b) = mean(z(b)
C
⊙w(b)
C )
(20)
h(b)
C
= {h(b)
c }c∈C = MLP(D(b)
C ,
h(b) = mean(h(b)
C
⊙w(b)
C )
(21)
(22)
To make the non-attentive counterpart of NeuBANP, we use random weights in both paths, re-
sulting in two latent variables z(b), h(b). They induce randomness into the decoder output ˆ
y(b) =
MLP(x, z(b), h(b)), and the predictive distribution is construct by (10). The results in Table 2 and
Table 3 shows that this model performs worse than NeuBANP, but outperforms BNP and NP, showing
the quality of functional uncertainty the model learns with the bootstrap.
B
EXPERIMENTS
Implementation2 of NPs other than NeuBANP was borrowed from the source code of Lee et al. (2020)3.
Regression and Bayesian optimization experiment was done in single GeForce RTX 2080 Ti GPU
with the memory of 11, 019 MiB. Multi-dimensional regression including image completion was
done in Tesla V100 GPU with the memory of 32, 480 MiB.
2https://anonymous.4open.science/r/neubanp_initial
3https://github.com/juho-lee/bnp, MIT License.
12
Under review as a conference paper at ICLR 2022
Fig. 5. Each plot shows predictions given by NP, BNP in a linear regression. The ground-truth function is
a simple linear function with heterogeneous variance: y = x + βϵ(x) where ϵ(x) ∼N(0, σ2(x)) and
σ(x) =
√
x2 + 10−5. We used the ofﬁcial code provided by Lee et al. (2020). Unlike the case of ANP and
BANP, considering that overﬁtting does not occur in NP and BNP, we can notice that attention modules are the
leading cause of overﬁtting.
B.1
SIMPLE LINEAR REGRESSION
We experimented using a linear function rather than a function sampled from the GP to analyze
how well the NP models work in a simple regression task. Additionally, we need to examine how
well the NP models predict uncertainty in the presence of heterogeneous noise in the data, and
we add ϵ(x) ∼N(0, σ2(x)), σ(x) =
√
x2 + 10−5 to the linear function. We multiplied the noise
by coefﬁcient β to follow the meta-learning framework, and the β value was uniformly randomly
sampled from [0.1, 1.0] during training. In this task, NP, BNP, and NeuBANP were able to estimate
the underlying true linear function and the uncertainty of heterogeneous noise. On the other hand, in
case of ANP and BANP, overﬁtting occurred and failed to predict the true function and its uncertainty
(see Figures 1 and 5). As explained in the main text, we can see that the overﬁtting phenomenon
occurs because of the attention mechanism that has appeared to solve the underﬁtting issue of the NP.
NeuBANP has the advantage of the attention mechanism and achieved better performance through
regularization using random weights. For (A)NP and B(A)NP, we used the ofﬁcial code provided by
Lee et al. (2020). The remaining training settings are the same as the 1D regression in Appendix B.2,
and we changed only the training iteration to 10, 000.
B.2
1D REGRESSION
Training
For all models, training dataset consists of randomly sampled context and target from
functions following GP with RBF kernel k(x, y) = s2 · exp(−||x −y||2/(2l2)). Parameters of
kernel are randomly sampled with s ∼Uniform(0.1, 1.0) and l ∼Uniform(0.1, 0.6). Feature values
(xi)i∈C∪T is chosen uniformly at random in [−2, 2]. The size of context and target are randomly
sampled with |C| ∼Uniform(3, 47) and |T | ∼Uniform(3, 50 −|C|). We trained all models for
100,000 iterations and used Adam optimizer (Kingma & Ba, 2015). For stable learning, we used the
cosine annealing scheduler with initial learning rate 5 × 10−4.
Results
Table 2 shows log-likelihood of NPs for various evaluation dataset sampled from GP
with RBF, Matérn 5/2, Periodic kernel. As in generation of training dataset, parameters of Matérn
5/2 kernel k(x, y) = s2(1 +
√
5||x −y||2/(3l2)) exp(−
√
5||x −y||/l) and Periodic kernel
k(x, y) = s2 exp(−2 sin2(π||x −y||2/p)/l2) was randomly sampled with s ∼Uniform(0.1, 1.0),
l ∼Uniform(0.1, 0.6) and p ∼Uniform(0.1, 0.5). For RBF and Matérn 5/2 dataset, NeuBANP
showed state-of-the-art performance both in ﬁtting context and predicting target. We added ﬁgures
showing the predictions of ANP, BANP, and NeuBANP for the Matérn 5/2 and Periodic kernel (see
Figure 6 and 7). In the case of the Matérn kernel, the two problems described in the main text
occurred identically for ANP and BANP (see Section 4.1). However, in the case of Periodic, we can
see that all models failed to approximate the true function correctly. This result came out because we
experimented with testing the models’ generalization performance when trained with the RBF kernel.
And the quantitative results in Table 2 show that the prediction performance of the attention-based
13
Under review as a conference paper at ICLR 2022
Method
RBF
Matérn 5/2
Periodic
context
target
context
target
context
target
CNP
1.17 ± 0.08
0.87 ± 0.36
1.06 ± 0.11
0.65 ± 0.39
-0.31 ± 0.41
-2.05 ± 1.17
NP
1.11 ± 0.09
0.78 ± 1.47
0.99 ± 0.11
0.56 ± 0.50
-0.28 ± 0.37
-1.73 ± 1.09
ANP
1.38 ± 0.00
1.08 ± 0.41
1.38 ± 0.00
0.94 ± 0.47
0.21 ± 0.76
-6.82 ± 2.83
BNP
1.20 ± 0.07
0.92 ± 0.34
1.09 ± 0.09
0.72 ± 0.35
-0.18 ± 037
-1.16 ± 0.56
BANP
1.38 ± 0.00
1.12 ± 0.33
1.38 ± 0.00
0.99 ± 0.38
0.28 ± 0.69
-5.69 ± 2.37
NeuBNP
1.54 ± 0.20
1.01 ± 0.55
1.22 ± 0.22
0.53 ± 0.59
-0.34 ± 0.45
-2.34 ± 1.95
NeuBANP
3.17 ± 0.28
1.38 ± 0.60
3.09 ± 0.29
1.13 ± 0.64
1.56 ± 0.73
-11.49 ± 8.08
Table 2. Log-Likelihood of NPs for 48,000 different evaluations of context and target.
model on the target data is poor. Among them, NeuBANP has the worst performance, and we think the
reason is the lower bound on the predicted variance set by ANP and BANP. Quantitative results show
similar predictions for all models, but ANP and BANP achieve numerically more robust performance
by setting a lower bound on the variance. We ﬁnd some cases with the jumps in the function values
which do not seem like a smooth function (See Figure 3, 6, and 7). This phenomena occurs in every
attentive models including ANP, BANP, and NeuBANP. It looks like a distorted prediction on particular
region, however, since our model predicts high variance in such region, this does not raise a problem
in predicting the global trend, as we can see in high average log likelihood in 1d regression.
Fig. 6. Comparison of ANP, BANP, and NeuBANP in 1D regression. Matérn 5/2 kernel case.
Fig. 7. Comparison of ANP, BANP, and NeuBANP in 1D regression. Periodic kernel case.
B.3
BAYESIAN OPTIMIZATION
One-dimensional case
Table 3 shows the performance of GP and various NPs for 1D Bayesian
optimization task. RBF, Matérn 5/2, and Periodic kernels are used to generate evaluation dataset.
14
Under review as a conference paper at ICLR 2022
Algorithm 1: Neural Process based Bayesian Optimization.
Input
:Target function f ⋆; Acquisition function U; Observed data D0 = {(x0, f ⋆(x0))};
Maximum evaluation step T.
1 Meta-train a neural process pθ on f ∼P(F).
2 for t = 1, . . . , T do
3
Find xt by optimizing acquisition function: xt = argmin
x∈X
U (pθ(y|x, Dt−1))
4
Evaluate f ⋆(xt) and update the observed data: Dt ←Dt−1 ∪{(xt, f ⋆(xt))}
Fig. 8. 1D Bayesian optimization results. Bold lines show the mean of simple regrets over 100 experiments. We
also report 10% of the standard deviation.
Fig. 9. Left: Multi-dimensional Bayesian optimization results on various benchmark functions with EI as an
acquisition function. Bold lines represent the mean performance over 100 experiments. We indicate 20% of the
standard deviation. Right most: Time complexity of each model as the number of observations increases.
Multi-dimensional case
In multi-dimensional BO experiments, we used GPyTorch 4 (Gardner
et al., 2018) for scalable GP regression, and BoTorch 5 (Balandat et al., 2020) for overall BO process
(e.g., optimization of acquisition functions). GP was set to the default setting of BoTorch. In detail,
GP model was parameterized with Matérn 5/2 kernel with ARD and constant mean function, and
prior distribution for hyperparameters was set as Gamma(3, 6) for length scale l and Gamma(2, 0.15)
for output scale s. For three-dimensional BO experiment in Figure 4, the overall time complexity of
ANP and BANP is almost the same. This result is seemingly in contrast with the fact that ANP takes a
shorter time for prediction than BANP. However, since the BO algorithm contains the optimization of
the acquisition function, the qualities of acquisition functions obtained by the model predictions may
affect the overall time complexity. We conjecture that BANP gives an acquisition function easier to
optimize than that of ANP so that the overall time complexities of both models are similar. Additionally,
we did not report the results of GP for the two functions. Speciﬁcally, we omitted the Goldstein-Price
4https://github.com/cornellius-gp/gpytorch, MIT License.
5https://github.com/pytorch/botorch, MIT License.
15
Under review as a conference paper at ICLR 2022
Fig. 10. First column: 2D objective functions Second & Fourth columns: Contour plots of functions and the
evaluated points during Bayesian optimization. Third & Fifth columns: UCB value at the last iteration.
Method
RBF
Matérn 5/2
Periodic
GP (RBF)
0.016 ± 0.052
0.048 ± 0.206
0.104 ± 0.242
CNP
0.072 ± 0.188
0.081 ± 0.198
0.096 ± 0.166
NP
0.154 ± 0.273
0.187 ± 0.303
0.083 ± 0.121
ANP
0.209 ± 0.364
0.223 ± 0.328
0.107 ± 0.142
BNP
0.109 ± 0.214
0.105 ± 0.188
0.071 ± 0.091
BANP
0.114 ± 0.216
0.136 ± 0.256
0.077 ± 0.11
NeuBNP
0.069 ± 0.169
0.125 ± 0.238
0.058 ± 0.069
NeuBANP
0.006 ± 0.011
0.011 ± 0.055
0.028 ± 0.035
Table 3. 1D Bayesian optimization results. Mean and standard deviations of simple regrets over 100 runs are
reported.
since the simple regret value of GP was too large (the performance was poor) compared to other
models and omitted the Rastrigin since the 46 errors occurred out of 100 experiments. An error may
occur when ill-conditioned data is given during the kernel training process of the GP, and the data
recommended by the UCB during the exploration (or exploitation) process seems to correspond to
this condition. If we report the performance ignoring the numerical error, the simple regret for the
Goldstein-Price was 52049.13, and the simple regret for the Rastrigin was 18.48. For the Rastrigin
function, GP with UCB is numerically unstable, but like EI, it achieved the best performance.
B.4
CONTEXTUAL MULTI-ARMED BANDIT
Setting
We followed wheel bandit setting introduced in Riquelme et al. (2018). At every step
t (< T), a two-dimensional point (xt, yt) inside the unit circle Runit = {(x, y) ∈R2 : x2 + y2 ≤1}
is given as a context ct. The algorithm chooses an action at ∈{1, 2, 3, 4, 5}. The stochastic reward
rt = r(at, ct) is sampled from the reward distribution. Let R1, R2, R3, R4, R5 ⊂Runit the disjoint
16
Under review as a conference paper at ICLR 2022
Dim
Target
GP
ANP
BNP
BANP
NeuBANP
2D
Ackley
2.84 ± 1.82
0.19 ± 0.53
1.82 ± 1.03
0.50 ± 1.03
0.08 ± 0.27
Dropwave
0.36 ± 0.17
0.22 ± 0.15
0.32 ± 0.18
0.28 ± 0.17
0.15 ± 0.10
Goldsteinprice
-
475.52 ± 469.15
2098.22 ± 1509.88
80.67 ± 65.85
30.33 ± 25.42
Michalewicz
0.67 ± 0.45
0.61 ± 0.40
1.00 ± 0.46
0.69 ± 0.38
0.45 ± 0.42
3D
Ackley
3.39 ± 1.25
5.36 ± 0.97
3.29 ± 1.11
4.30 ± 1.15
0.34 ± 0.26
Cosine
0.04 ± 0.24
0.10 ± 0.10
1.12 ± 0.57
0.25 ± 0.43
0.005 ± 0.003
Hartmann
0.42 ± 0.77
0.33 ± 0.50
1.94 ± 0.86
0.93 ± 0.98
0.39 ± 0.39
Rastrigin
-
54.94 ± 19.84
48.55 ± 14.34
38.36 ± 8.20
23.06 ± 19.77
Table 4. Multi-dimensional Bayesian optimization results. Mean and standard deviations of simple regrets over
100 runs are reported.
sets (regions) of unit circle as follows:
R1 = {(x, y) : x2 + y2 < δ}
(23)
R2 = {(x, y) : δ ≤x2 + y2 ≤1, x > 0, y > 0}
(24)
R3 = {(x, y) : δ ≤x2 + y2 ≤1, x < 0, y > 0}
(25)
R4 = {(x, y) : δ ≤x2 + y2 ≤1, x < 0, y < 0}
(26)
R5 = {(x, y) : δ ≤x2 + y2 ≤1, x > 0, y < 0}
(27)
where the constant δ determines the size of R1 relative to other regions. Each action results in rewards
following different distribution according to the region to which the given context belongs, where N
denotes the normal distribution.
r(1, c) ∼N(1.2, 0.012)
(28)
r(a, c) ∼
N(50, 0.012),
if c ∈Ra
N(1, 0.012),
otherwise
∀a ∈{2, 3, 4, 5}
(29)
Note that action {1} always produces a moderate reward, but the other actions {2, 3, 4, 5} sometimes
produce a very high reward when the context is sampled from the corresponding high-reward region.
Thus, learning different reward distributions for actions {2, 3, 4, 5} by apprehension of context
information is critical to bandit performance. As δ increases, the high-reward regions for each actions
become smaller. Then the model should learn from rare observation of such high reward, which
means the problem becomes more difﬁcult.
Training and Evaluation
When pre-training NeuBANP, as in Garnelo et al. (2018b), 8 training
batches of 512 contexts and 50 targets were generated from the environment with hyperparameter
randomly sampled; δ ∼Uniform(0, 1). We consider two-dimensional context point ci as feature
xi ∈R2 and ﬁve rewards for actions (r(1, ci), r(2, ci), r(3, ci), r(4, ci), r(5, ci)) as label yi ∈R5.
At evaluation, only rewards for chosen actions are observed by the model. Thus, we replace other
unobserved rewards with dummy values randomly sampled from N(0, 1), following the usual strategy.
B.5
IMAGE COMPLETION
Settings
We compared the baseline NPs and NeuBANP on image completion tasks. Following Lee
et al. (2020), we trained all models on EMNIST (Cohen et al., 2017) and CelebA (Liu et al., 2015)
which was resized to 32 × 32. For EMNIST, we used only 10 classes for training and reported
the evaluation results for both seen classes and unseen classes separately. NeuBANP was trained
with 10 samples, and the other baselines were trained with 4 samples. For evaluation, we used
50 samples for all methods. Similar to 1D regression experiment, we randomly select the pixels
of a given image as context/target, and the number of context/target were drawn randomly from
Uniform distribution. However, in this case, we increased the maximum number of given points;
i.e., |C| ∼Uniform(3, 197), |T | ∼Uniform(3, 200 −|C|). x values were rescaled to [−1, 1] and
the corresponding y values were rescaled to [−0.5, 0.5]. We trained all models for 200 epochs and
set a initial learning rate of 5 × 10−4 using the Adam optimizer (Kingma & Ba, 2015) with cosine
annealing scheduler for learning rate decay.
17
Under review as a conference paper at ICLR 2022
Fig. 11. Qualitative result of EMNIST image comple-
tion.
Method
Seen classes (0-9)
Unseen classes (10-46)
context
target
context
target
CNP
0.926 ± 0.007
0.751 ± 0.005
0.766 ± 0.009
0.498 ± 0.012
NP
0.948 ± 0.006
0.806 ± 0.005
0.808 ± 0.005
0.600 ± 0.009
ANP
1.383 ± 0.000
0.993 ± 0.005
1.383 ± 0.000
0.894 ± 0.004
BNP
1.004 ± 0.008
0.880 ± 0.005
0.883 ± 0.010
0.722 ± 0.006
BANP
1.383 ± 0.000
1.010 ± 0.006
1.382 ± 0.000
0.942 ± 0.005
NeuBANP
1.475 ± 0.345
1.337 ± 0.224
1.333 ± 0.516
1.119 ± 0.388
Table 5. Quantitative result of EMNIST image comple-
tion. Mean and standard deviationd of likelihoood over
5 experiments.
Fig. 12. Qualitative result of CelebA image comple-
tion.
context
target
CNP
2.975 ± 0.013
2.199 ± 0.003
NP
3.066 ± 0.011
2.492 ± 0.014
ANP
4.150 ± 0.000
2.731 ± 0.006
BNP
3.269 ± 0.008
2.788 ± 0.005
BANP
4.149 ± 0.000
3.129 ± 0.005
NeuBANP
13.946 ± 0.590
2.870 ± 0.021
Table 6. Quantitative result of CelebA results. Mean and
standard deviationd of likelihoood over 5 experiments.
Results
Figure 11 and 12 show the mean prediction and uncertainty estimation of ANP, BANP,
and NeuBANP for test images in unseen classes. For both datasets, though our model shows noisy
mean prediction due to the random weights, we can demonstrate the advantage of NeuBANP in
uncertainty estimation. NeuBANP estimated the uncertainty correctly in the area where the color of
the pixel changes and thus possesses signiﬁcant uncertainty. This leads to the overall improvement in
quantitative performance (see Table 5 and 6).
B.6
TIME COMPLEXITY
Settings
We measured the time complexity empirically according to the number of context points,
target points, and bootstrap samples. We ﬁxed the number of targets to 25 and adjusted the number
of contexts to 10, 20, 30, 40, and 50 to see how inference time varies with the number of contexts.
Conversely, to see the inference time according to the number of targets, we ﬁxed the number
of context points to 25. We ﬁxed the number of bootstrap samples to 50 as in the 1D regression
experiment. When conducting experiments with the number of bootstrap samples, the number of
context and target points was ﬁxed at 20 and 25. All experiments were conducted with a batch
containing 100 tasks.
Results
BANP places a remarkably high computational cost in that the approach of bootstrapping the
attention module is inefﬁcient, as demonstrated in Figure 13. The inference time becomes noticeably
longer as the number of context points increases. NeuBANP, on the other hand, learns to bootstrap
efﬁciently; therefore, its time complexity is comparable to that of BNP, which does not use the
attention module.
Method
Number of contexts
Number of targets
Number of bootstrap samples
10
20
30
40
50
10
20
30
40
50
10
50
100
BNP
1.797
1.977
2.222
2.546
2.886
1.830
1.882
1.965
2.057
2.156
1.532
1.639
1.950
BANP
3.512
4.405
5.345
6.509
7.793
4.439
4.626
4.834
4.926
5.117
3.189
3.699
4.775
NeuBANP
1.632
1.813
2.217
2.757
3.369
1.768
1.941
2.063
2.212
2.357
1.606
1.705
2.149
Table 7. Inference time measurement. Mean of inference time over 5 runs are reported.
18
Under review as a conference paper at ICLR 2022
Fig. 13. Inference time measurement.
19
