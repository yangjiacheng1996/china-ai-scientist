# CONTROLLING THE COMPLEXITY AND LIPSCHITZ CONSTANT IMPROVES POLYNOMIAL NETS

**Zhenyu Zhu,** **Fabian Latorre,** **Grigorios G Chrysos,** **Volkan Cevher**

EPFL, Switzerland
{[first name].[surname]}@epfl.ch

ABSTRACT

While the class of Polynomial Nets demonstrates comparable performance to neural
networks (NN), it currently has neither theoretical generalization characterization
nor robustness guarantees. To this end, we derive new complexity bounds for the
set of Coupled CP-Decomposition (CCP) and Nested Coupled CP-decomposition
(NCP) models of Polynomial Nets in terms of the ℓ∞-operator-norm and the ℓ2operator norm. In addition, we derive bounds on the Lipschitz constant for both
models to establish a theoretical certificate for their robustness. The theoretical
results enable us to propose a principled regularization scheme that we also evaluate
experimentally in six datasets and show that it improves the accuracy as well as
the robustness of the models to adversarial perturbations. We showcase how
this regularization can be combined with adversarial training, resulting in further
improvements.

1 INTRODUCTION

Recently, high-degree Polynomial Nets (PNs) have been demonstrating state-of-the-art performance
in a range of challenging tasks like image generation (Karras et al., 2019; Chrysos and Panagakis,
2020), image classification (Wang et al., 2018), reinforcement learning (Jayakumar et al., 2020),
non-euclidean representation learning (Chrysos et al., 2020) and sequence models (Su et al., 2020).
In particular, in public benchmarks like the Face verification on MegaFace task[1] (KemelmacherShlizerman et al., 2016), Polynomial Nets are currently the top performing model.

A major advantage of Polynomial Nets over traditional Neural Networks[2] is that they are compatible
with efficient Leveled Fully Homomorphic Encryption (LFHE) protocols (Brakerski et al., 2014).
Such protocols allow efficient computation on encrypted data, but they only support addition or
multiplication operations i.e., polynomials. This has prompted an effort to adapt neural networks by
replacing typical activation functions with polynomial approximations (Gilad-Bachrach et al., 2016;
Hesamifard et al., 2018). Polynomial Nets do not need any adaptation to work with LFHE.

Without doubt, these arguments motivate further investigation about the inner-workings of Polynomial
Nets. Surprisingly, little is known about the theoretical properties of such high-degree polynomial
expansions, despite their success. Previous work on PNs (Chrysos et al., 2020; Chrysos and Panagakis,
2020) have focused on developing the foundational structure of the models as well as their training,
but do not provide an analysis of their generalization ability or robustness to adversarial perturbations.

In contrast, such type of results are readily available for traditional feed-forward Deep Neural
Networks, in the form of high-probability generalization error bounds (Neyshabur et al., 2015;
Bartlett et al., 2017; Neyshabur et al., 2017; Golowich et al., 2018) or upper bounds on their Lipschitz
constant (Scaman and Virmaux, 2018; Fazlyab et al., 2019; Latorre et al., 2020). Despite their
similarity in the compositional structure, theoretical results for Deep Neural Networks[2] do not apply
to Polynomial Nets, as they are essentialy two non-overlapping classes of functions.

Why are such results important? First, they provide key theoretical quantities like the sample
complexity of a hypothesis class: how many samples are required to succeed at learning in the
PAC-framework. Second, they provide certified performance guarantees to adversarial perturbations

[1https://paperswithcode.com/sota/face-verification-on-megaface](https://paperswithcode.com/sota/face-verification-on-megaface)
2with non-polynomial activation functions.


-----

(Szegedy et al., 2014; Goodfellow et al., 2015) via a worst-case analysis c.f. Scaman and Virmaux
(2018). Most importantly, the bounds themselves provide a principled way to regularize the hypothesis
class and improve their accuracy or robustness.

For example, Generalization and Lipschitz constant bounds of Deep Neural Networks that depend
on the operator-norm of their weight matrices (Bartlett et al., 2017; Neyshabur et al., 2017) have
layed out the path for regularization schemes like spectral regularization (Yoshida and Miyato, 2017;
Miyato et al., 2018), Lipschitz-margin training (Tsuzuku et al., 2018) and Parseval Networks (Cisse
et al., 2017), to name a few.

Indeed, such schemes have been observed in practice to improve the performance of Deep Neural
Networks. Unfortunately, similar regularization schemes for Polynomial Nets do not exist due to
the lack of analogous bounds. Hence, it is possible that PNs are not yet being used to their fullest
potential. We believe that theoretical advances in their understanding might lead to more resilient and
accurate models. In this work, we aim to fill the gap in the theoretical understanding of PNs. We
summarize our main contributions as follows:

**Rademacher Complexity Bounds. We derive bounds on the Rademacher Complexity of the Cou-**
pled CP-decomposition model (CCP) and Nested Coupled CP-decomposition model (NCP) of PNs,
under the assumption of a unit ℓ -norm bound on the input (Theorems 1 and 3), a natural assumption
_∞_
in image-based applications. Analogous bounds for the ℓ2-norm are also provided (Appendices E.3
and F.3). Such bounds lead to the first known generalization error bounds for this class of models.

**Lipschitz constant Bounds. To complement our understanding of the CCP and NCP models, we**
derive upper bounds on their ℓ -Lipschitz constants (Theorems 2 and 4), which are directly related
_∞_
to their robustness against ℓ -bounded adversarial perturbations, and provide formal guarantees.
_∞_
Analogous results hold for any ℓp-norm (Appendices E.4 and F.4).

**Regularization schemes. We identify key quantities that simultaneously control both Rademacher**
Complexity and Lipschitz constant bounds that we previously derived, i.e., the operator norms of
the weight matrices in the Polynomial Nets. Hence, we propose to regularize the CCP and NCP
models by constraining such operator norms. In doing so, our theoretical results indicate that both the
generalization and the robustness to adversarial perturbations should improve. We propose a Projected
Stochastic Gradient Descent scheme (Algorithm 1), enjoying the same per-iteration complexity as
vanilla back-propagation in the ℓ -norm case, and a variant that augments the base algorithm with
_∞_
adversarial traning (Algorithm 2).

**Experiments. We conduct experimentation in five widely-used datasets on image recognition and on**
dataset in audio recognition. The experimentation illustrates how the aforementioned regularization
schemes impact the accuracy (and the robust accuracy) of both CCP and NCP models, outperforming
alternative schemes such as Jacobian regularization and the L2 weight decay. Indeed, for a grid of
regularization parameters we observe that there exists a sweet-spot for the regularization parameter
which not only increases the test-accuracy of the model, but also its resilience to adversarial perturbations. Larger values of the regularization parameter also allow a trade-off between accuracy and
robustness. The observation is consistent across all datasets and all adversarial attacks demonstrating
the efficacy of the proposed regularization scheme.

2 RADEMACHER COMPLEXITY AND LIPSCHITZ CONSTANT BOUNDS FOR
POLYNOMIAL NETS

**Notation.** The symbol ◦ denotes the Hadamard (element-wise) product, the symbol • is the facesplitting product, while the symbol ⋆ denotes a convolutional operator. Matrices are denoted by
uppercase letters e.g., V . Due to the space constraints, a detailed notation is deferred to Appendix C.

**Assumption on the input distribution.** Unless explicitly mentioned otherwise, we assume an
_ℓ_ -norm unit bound on the input data i.e., **_z_** 1 for any input z. This is the most common
_∞_ _∥_ _∥∞_ _≤_
assumption in image-domain applications in contemporary deep learning, i.e., each pixel takes values
in [ 1, 1] interval. Nevertheless, analogous results for ℓ2-norm unit bound assumptions are presented
_−_
in Appendices E.3, E.4, F.3 and F.4.

We now introduce the basic concepts that will be developed throughout the paper i.e., the Rademacher
Complexity of a class of functions (Bartlett and Mendelson, 2002) and the Lipschitz constant.


-----

Figure 1: Schematic of CCP model (left) and NCP model (right), where ◦ denotes the Hadamard
product. Blue boxes correspond to learnable parameters. Green and red boxes denote input and
output, respectively. Yellow boxes denote operations.

**Definition 1 (Empirical Rademacher Complexity). Let Z** = {z1, . . ., zn} ⊆ R[d] _and let_
_{σj : j = 1, . . ., n} be independent Rademacher random variables i.e., taking values uniformly_
_in {−1, +1}. Let F be a class of real-valued functions over R[d]. The Empirical Rademacher_
_complexity of F with respect to Z is defined as follows:_


_RZ(F) := Eσ_


sup
_f_ _∈F_


_σjf_ (zj)
_j=1_

X


**Definition 2 (Lipschitz constant). Given two normed spaces (** _,_ _) and (_ _,_ _), a function_
_X_ _∥· ∥X_ _Y_ _∥· ∥Y_
_f : X →Y is called Lipschitz continuous with Lipschitz constant K ≥_ 0 if for all x1, x2 in X _:_

_f_ (x1) _f_ (x2) _K_ _x1_ _x2_ _._
_∥_ _−_ _∥Y ≤_ _∥_ _−_ _∥X_

2.1 COUPLED CP-DECOMPOSITION OF POLYNOMIAL NETS (CCP MODEL)

The Coupled CP-Decomposition (CCP) model of PNs (Chrysos et al., 2020) leverages a coupled CP
Tensor decomposition (Kolda and Bader, 2009) to vastly reduce the parameters required to describe a
high-degree polynomial, and allows its computation in a compositional fashion, much similar to a
feed-forward pass through a traditional neural network. The CCP model was used in Chrysos and
Panagakis (2020) to construct a generative model. CCP can be succintly defined as follows:

_f_ (z) = C ◦i[k]=1 **_[U][i][z][,]_** (CCP)

whereare the learnable parameters, where z ∈ R[d] is the input data, f (z m) ∈ ∈RN[o] is the output of the model and is the hidden rank. In Fig. 1 we provide a schematic of Un ∈ R[m][×][d], C ∈ R[o][×][m]
the architecture, while in Appendix D.1 we include further details on the original CCP formulation
(and how to obtain our equivalent re-parametrization) for the interested reader.

In Theorem 1 we derive an upper bound on the complexity of CCP models with bounded ℓ -operator_∞_
norms of the face-splitting product of the weight matrices. Its proof can be found in Appendix E.1.
For a given CCP model, we derive an upper bound on its ℓ -Lipschitz constant in Theorem 2 and its
_∞_
proof is given in Appendix E.4.1.

**Theorem 1. Let Z =** **_z1, . . ., zn_** R[d] _and suppose that_ **_zj_** 1 for all j = 1, . . ., n. Let
_{_ _} ⊆_ _∥_ _∥∞_ _≤_

_FCCP[k]_ [:=] _f_ (z) = **_c, ◦i[k]=1[U][i][z]_** : ∥c∥1 ≤ _µ,_ _•i[k]=1[U][i]_ _∞_ _[≤]_ _[λ]_ _._

_The Empirical Rademacher Complexity of_ _CCPk (k-degree CCP polynomials) with respect to Z is_
_bounded as:_


2k log(d)


_RZ(FCCP[k]_ [)][ ≤] [2][µλ]


**Proof sketch of Theorem 1** We now describe the core steps of the proof. For the interested reader,
the complete and detailed proof steps are presented in Appendix E.1. First, Hölder’s inequality is
used to bound the Rademacher complexity as:


_RZ(FCCP[k]_ [) =][ E][ sup]f _∈FCCP[k]_



[σj◦i[k]=1[(][U][i][z][j][)]]
_j=1_

X



[σj◦i[k]=1[(][U][i][z][j][)]]
_j=1_

X


_≤_ E supf _∈FCCP[k]_


**_c,_**


_n_ _[∥][c][∥][1]_


(1)


-----

This shows why the factor _c_ 1 _µ appears in the final bound. Then, using the mixed product_
property (Slyusar, 1999) and its extension to repeated Hadamard products ( ∥ _∥_ _≤_ Lemma 7 in Appendix C.3),
we can rewrite the summation in the right-hand-side of (1) as follows:


_σj◦i[k]=1[(][U][i][z][j][) =]_
_j=1_

X


_j=1_ _σj •i[k]=1_ [(][U][i][)][ ∗]i[k]=1 [(][z][j][) =][ •]i[k]=1[(][U][i][)]

X


_j=1_ _σj ∗i[k]=1_ [(][z][j][)][ .]

X


This step can be seen as a linearization of the polynomial by lifting the problem to a higher dimensional
space. We use this fact and the definition of the operator norm to further bound the term inside
the ℓ -norm in the right-hand-side of (1). Such term is bounded as the product of the ℓ -operator
_∞_ _∞_
norm of •i[k]=1[(][U][i][)][, and the][ ℓ][∞][-norm of an expression involving the Rademacher variables][ σ][j][ and]
the vectors ∗i[k]=1[(][z][j][)][. Finally, an application of Massart’s Lemma (Shalev-Shwartz and Ben-David]

(2014), Lemma 26.8) leads to the final result.
**Theorem 2. The Lipschitz constant (with respect to the ℓ** _-norm) of the function defined in Eq. (CCP),_
_∞_
_restricted to the set_ **_z_** R[d] : **_z_** 1 _is bounded as:_
_{_ _∈_ _∥_ _∥∞_ _≤_ _}_


_Lip∞(f_ ) ≤ _k∥C∥∞_ _∥Ui∥∞_ _._

_i=1_

Y


2.2 NESTED COUPLED CP-DECOMPOSITION (NCP MODEL)

The Nested Coupled CP-Decomposition (NCP) model leverages a joint hierarchical decomposition,
which provided strong results in both generative and discriminative tasks in Chrysos et al. (2020). A
slight re-parametrization of the model (Appendix D.2) can be expressed with the following recursive
relation:

**_x1 = (A1z)_** (s1), **_xn = (Anz)_** (Snxn 1), _f_ (z) = Cxk, (NCP)
_◦_ _◦_ _−_

wherelearnable parameters. In z ∈ R[d] is the input vector and Fig. 1 we provide a schematic of the architecture. C ∈ R[o][×][m], An ∈ R[m][×][d], Sn ∈ R[m][×][m] and s1 ∈ R[m] are the

In Theorem 3 we derive an upper bound on the complexity of NCP models with bounded ℓ -operator_∞_
norm of a matrix function of its parameters. Its proof can be found in Appendix F.1. For a given NCP
model, we derive an upper bound on its ℓ -Lipschitz constant in Theorem 4 and its proof is given in
_∞_
Appendix F.4.1.
**Theorem 3. Let Z =** **_z1, . . ., zn_** R[d] _and suppose that_ **_zj_** 1 for all j = 1, . . ., n. Define
_{_ _} ⊆_ _∥_ _∥∞_ _≤_
_the matrix Φ(A1, S1, . . ., An, Sn) := (Ak •_ **_Sk)_** _i=1_ **_[I][ ⊗]_** **_[A][i][ •]_** **_[S][i][. Consider the class of functions:]_**

_FNCP[k]_ [:=][ {][f] [(][z][)][ as in][ (][NCP][)][ :][ ∥][C][∥][∞] _[≤][Q][µ,][k][ ∥][−][1][Φ(][A][1][,][ S][1][, . . .,][ A][k][,][ S][k][)][∥]∞_ _[≤]_ _[λ][}][,]_

_where C ∈_ R[1][×][m] _(single output), thus, we will write it as c, and the corresponding bound also_
_becomeswith respect to ∥c∥1 ≤ Z is bounded as:µ. The Empirical Rademacher Complexity of NCPk (k-degree NCP polynomials)_


2k log(d)


_RZ(FNCP[k]_ [)][ ≤] [2][µλ]


**Theorem 4. The Lipschitz constant (with respect to the ℓ** _-norm) of the function defined in Eq. (NCP),_
_∞_
_restricted to the set_ **_z_** R[d] : **_z_** 1 _is bounded as:_
_{_ _∈_ _∥_ _∥∞_ _≤_ _}_


_Lip_ (f ) _k_ **_C_**
_∞_ _≤_ _∥_ _∥∞_


(∥Ai∥∞∥Si∥∞) .
_i=1_

Y


3 ALGORITHMS

By constraining the quantities in the upper bounds on the Rademacher complexity (Theorems 1
and 3), we can regularize the empirical loss minimization objective (Mohri et al., 2018, Theorem 3.3).
Such method would prevent overfitting and can lead to an improved accuracy. However, one issue
with the quantities involved in Theorems 1 and 3, namely


_k−1_

**_I_** **_Ai_** **_Si_**
_i=1_ _⊗_ _•_

Y

[(][A][k][ •][ S][k][)]


_i=1[U][i]_

_•[k]_ _∞_ _[,]_


-----

is that projecting onto their level sets correspond to a difficult non-convex problem. Nevertheless, we
can control an upper bound that depends on the ℓ -operator norm of each weight matrix:
_∞_

**Lemma 1. It holds that** _i=1[U][i]_ _i=1_

_•[k]_ _∞_ _[≤]_ [Q][k] _[∥][U][i][∥][∞][.]_

**Lemma 2. It holds that** (Ak **_Sk)_** _i=1_ **_[I][ ⊗]_** **_[A][i][ •][ S][i]_** _i=1_
_•_ _∞_ _[≤]_ [Q][k] _[∥][A][i][∥][∞][∥][S][i][∥][∞][.]_

The proofs of Lemmas 1 and 2 can be found in[Q][k][−][1] Appendix E.2 and Appendix F.2. These results mean
that by constraining the operator norms of each weight matrix, we can control the overall complexity
of the CCP and NCP models.

Projecting a matrix onto an ℓ -operator norm ball is a simple task that can be achieved by projecting
_∞_
each row of the matrix onto an ℓ1-norm ball, for example, using the well-known algorithm from
Duchi et al. (2008). The final optimization objective for training a regularized CCP is the following:


_L(C, U1, . . ., Uk; xi, yi)_ subject to ∥C∥∞ _≤_ _µ, ∥Ui∥∞_ _≤_ _λ,_ (2)
_i=1_

X


min
**_C,U1,...,Uk_**


where (xi, yi)[n]i=1 [is the training dataset,][ L][ is the loss function (e.g., cross-entropy) and][ µ, λ][ are the]
regularization parameters. We notice that the constraints on the learnable parameters Ui and C have
the effect of simultaneously controlling the Rademacher Complexity and the Lipschitz constant of
the CCP model. For the NCP model, an analogous objective function is used.

To solve the optimization problem in Eq. (2) we will use a Projected Stochastic Gradient Descent
method Algorithm 1. We also propose a variant that combines Adversarial Training with the projection
step (Algorithm 2) with the goal of increasing robustness to adversarial examples.


**Algorithm 1: Projected SGD**
**Input: dataset Z, learning rate**
_{γt > 0}t[T]=0[ −][1][, iterations][ T]_ [,]
hyper-parameters R, f, Loss L.

**Output: model with parameters θ.**

Initialize θ.
**for t = 0 to T −** 1 do

Sample (x, y) from Z
**_θ = θ −_** _γt▽θL(θ; x, y)._
**if t mod f = 0 then**

**_θ =_**
_{[θ][:][∥][θ][∥]∞[≤][R]}[(][θ][)]_

[Q]


**Algorithm 2: Projected SGD + Adversarial Training**


**Input:{γt > dataset 0}t[T]=0[ −][1] Z[, iterations], learning rate[ T]** [,] **Input:and n dataset, hyper-parameters Z, learning rate R, f {,γ ϵt > and 0 α}t[T]=0, Loss[ −][1][, iterations] L** _[ T]_
hyper-parameters R, f, Loss L. **Output: model with parameters θ.**

**Output: model with parameters θ.** Initialize θ.

**forInitialize t = 0 θ to. T −** 1 do **forSample t = 0 to (x T, y −) from1 do** _Z_

Sampleθ = θ − (xγt, y▽)θ fromL(θ; x Z, y). **forx i[adv] = 0= to n −** 1 do _∞[≤][ϵ]_
**if t mod f = 0 then** _{[x][′]_ [:][||][x][′] _[−][x][||]_ _} [{][x][ +][ α][∇][x][L][(][θ][;][ x][, y][)][}]_

**_θ =_** **_θ = θ −_** _γt▽θL(θ; x[adv], y)_
_{[θ][:][∥][θ][∥]∞[≤][R]}[(][θ][)]_ **if t mod f[Q] = 0 then**

**_θ =_**

[Q] _{[θ][:][∥][θ][∥]∞[≤][R]}[(][θ][)]_

In Algorithms 1 and 2 the parameter f is set in practice to a positive value, so that the projection[Q]
(denoted by Π) is made only every few iterations. The variable θ represents the weight matrices of
the model, and the projection in the last line should be understood as applied independently for every
weight matrix. The regularization parameter R corresponds to the variables µ, λ in Eq. (2).

**Convolutional layers** Frequently, convolutions are employed in the literature, especially in the
image-domain. It is important to understand how our previous results extend to this case, and
how the proposed algorithms work in that case. Below, we show that the ℓ -operator norm of the
_∞_
convolutional layer (as a linear operator) is related to the ℓ -operator norm of the kernel after a
_∞_
reshaping operation. For simplicity, we consider only convolutions with zero padding.

We study the cases of 1D, 2D and 3D convolutions. For clarity, we mention below the result for the
3D convolution, since this is relevant to our experimental validation, and we defer the other two cases
to Appendix G.

**Theorem 5. Let A ∈** R[n][×][m][×][r] _be an input image and let K ∈_ R[h][×][h][×][r][×][o] _be a convolutional kernel_
_with o output channels. For simplicity assume that k ≤_ min(n, m) is odd. Denote by B = K ⋆ **_A the_**
_output of the convolutional layer. Let U ∈_ R[nmo][×][nmr] _be the matrix such that vec(B) = U_ _vec(A)_
_i.e., U is the matrix representation of the convolution. Let M_ (K) ∈ R[o][×][hhr] _be the matricization of_


-----

**_K, where each row contains the parameters of a single output channel of the convolution. It holds_**
_that: ∥U_ _∥∞_ = ∥M (K)∥∞.

Thus, we can control the ℓ -operator-norm of a convolutional layer during training by controlling
_∞_
that of the reshaping of the kernel, which is done with the same code as for fully connected layers. It
can be seen that when the padding is non-zero, the result still holds.

4 NUMERICAL EVIDENCE

The generalization properties and the robustness of PNs are numerically verified in this section.
We evaluate the robustness to three widely-used adversarial attacks in sec. 4.2. We assess whether
the compared regularization schemes can also help in the case of adversarial training in sec. 4.3.
Experiments with additional datasets, models (NCP models), adversarial attacks (APGDT, PGDT)
and layer-wise bound (instead of a single bound for all matrices) are conducted in Appendix H
due to the restricted space. The results exhibit a consistent behavior across different adversarial
attacks, different datasets and different models. Whenever the results differ, we explicitly mention the
differences in the main body below.

4.1 EXPERIMENTAL SETUP

The accuracy is reported as as the evaluation metric for every experiment, where a higher accuracy
translates to better performance.

**Datasets and Benchmark Models: We conduct experiments on the popular datasets of Fashion-**
MNIST (Xiao et al., 2017), E-MNIST (Cohen et al., 2017) and CIFAR-10 (Krizhevsky et al.,
2014). The first two datasets include grayscale images of resolution 28 × 28, while CIFAR-10
includes 60, 000 RGB images of resolution 32 × 32. Each image is annotated with one out of the
ten categories. We use two popular regularization methods from the literature for comparison, i.e.,
Jacobian regularization (Hoffman et al., 2019) and L2 regularization (weight decay).

**Models: We report results using the following three models: 1) a 4[th]-degree CCP model named**
"PN-4", 2) a 10[th]-degree CCP model referenced as "PN-10" and 3) a 4[th]-degree Convolutional CCP
model called "PN-Conv". In the PN-Conv, we have replaced all the Ui matrices with convolutional
kernels. None of the variants contains any activation functions.

**Hyper-parameters: Unless mentioned otherwise, all models are trained for 100 epochs with a batch**
size of 64. The initial value of the learning rate is 0.001. After the first 25 epochs, the learning rate is
multiplied by a factor of 0.2 every 50 epochs. The SGD is used to optimize all the models, while the
cross-entropy loss is used. In the experiments that include projection or adversarial training, the first
50 epochs are pre-training, i.e., training only with the cross-entropy loss. The projection is performed
every ten iterations.

**Adversarial Attack Settings: We utilize two widely used attacks: a) Fast Gradient Sign Method**
(FGSM) and b) Projected Gradient Descent (PGD). In FGSM the hyper-parameter ϵ represents the
step size of the adversarial attack. In PGD there is a triple of parameters (ϵtotal, niters, ϵiter), which
represent the maximum step size of the total adversarial attack, the number of steps to perform for a
single attack, and the step size of each adversarial attack step respectively. We consider the following
hyper-parameters for the attacks: a) FGSM with ϵ = 0.1, b) PGD with parameters (0.1, 20, 0.01), c)
PGD with parameters (0.3, 20, 0.03).

4.2 EVALUATION OF THE ROBUSTNESS OF PNS

In the next experiment, we assess the robustness of PNs under adversarial noise. That is, the method
is trained on the train set of the respective dataset and the evaluation is performed on the test set
perturbed by additive adversarial noise. That is, each image is individually perturbed based on the
respective adversarial attack. The proposed method implements Algorithm 1.

The quantitative results in both Fashion-MNIST and E-MNIST using PN-4, PN-10 and PN-Conv
under the three attacks are reported in Table 1. The column ‘No-proj’ exhibits the plain SGD training
(i.e., without regularization), while the remaining columns include the proposed regularization,
Jacobian and the L2 regularization respectively. The results without regularization exhibit a substantial
decrease in accuracy for stronger adversarial attacks. The proposed regularization outperforms all
methods consistently across different adversarial attacks. Interestingly, the stronger the adversarial


-----

Method No proj. Our method Jacobian _L2_

_Fashion-MNIST_

Clean 87.28 ± 0.18% 87.32 ± 0.14% 86.24 ± 0.14% 87.31 ± 0.13%
FGSM-0.1 12.92 2.74% 46.43 **0.95% 17.90** 6.51% 13.80 3.65%

PN-4 _±_ **_±_** _±_ _±_

PGD-(0.1, 20, 0.01) 5.64 ± 1.76% **49.58 ± 0.59% 12.23 ± 5.63%** 5.01 ± 2.44%
PGD-(0.3, 20, 0.03) 0.18 ± 0.16% **28.96 ± 2.31%** 1.27 ± 1.29% 0.28 ± 0.18%

Clean 88.48 ± 0.17% 88.72 ± 0.12% 88.12 ± 0.11% 88.46 ± 0.19%
FGSM-0.1 15.96 1.00% 44.71 **1.24% 19.52** 1.14% 16.51 2.33%

PN-10 _±_ **_±_** _±_ _±_

PGD-(0.1, 20, 0.01) 1.94 ± 0.82% **47.94 ± 2.29%** 5.44 ± 0.81% 2.16 ± 0.95%
PGD-(0.3, 20, 0.03) 0.02 ± 0.03% **30.51 ± 1.22%** 0.05 ± 0.02% 0.01 ± 0.02%

Clean 86.36 ± 0.21% 86.38 ± 0.26% 84.69 ± 0.44% 86.45 ± 0.21%
FGSM-0.1 10.80 1.82% 48.15 **1.23% 10.62** 0.77% 10.73 1.58%

PN-Conv _±_ **_±_** _±_ _±_

PGD-(0.1, 20, 0.01) 9.37 ± 1.00% **46.63 ± 3.68% 10.20 ± 0.32%** 8.96 ± 0.83%
PGD-(0.3, 20, 0.03) 1.75 ± 0.83% **28.94 ± 1.20%** 8.26 ± 1.05% 2.03 ± 0.99%

_E-MNIST_

Clean 84.27 ± 0.26% 84.34 ± 0.31% 81.99 ± 0.33% 84.22 ± 0.33%
FGSM-0.1 8.92 1.99% **27.56** **3.32% 14.96** 1.32% 8.18 3.48%

PN-4 _±_ **_±_** _±_ _±_

PGD-(0.1, 20, 0.01) 6.24 ± 1.43% **29.46 ± 2.73%** 6.75 ± 2.92% 5.93 ± 1.97%
PGD-(0.3, 20, 0.03) 1.22 ± 0.85% **19.07 ± 0.98%** 3.06 ± 0.53% 1.00 ± 0.76%

Clean 89.31 ± 0.09% 90.56 ± 0.10% 89.19 ± 0.07% 89.23 ± 0.13%
FGSM-0.1 15.56 1.16% 37.11 **2.81% 24.21** 1.89% 16.30 1.82%

PN-10 _±_ **_±_** _±_ _±_

PGD-(0.1, 20, 0.01) 2.63 ± 0.65% **37.89 ± 2.91%** 9.18 ± 1.09% 2.33 ± 0.43%
PGD-(0.3, 20, 0.03) 0.00 ± 0.00% **20.47 ± 0.96%** 0.11 ± 0.08% 0.02 ± 0.03%

Clean 91.49 ± 0.29% 91.57 ± 0.19% 90.38 ± 0.13% 91.41 ± 0.18%
FGSM-0.1 4.28 0.55% **35.39** **7.51%** 3.88 0.04% 4.13 0.41%

PN-Conv _±_ **_±_** _±_ _±_

PGD-(0.1, 20, 0.01) 3.98 ± 0.82% **33.75 ± 7.17%** 3.86 ± 0.01% 4.83 ± 0.87%
PGD-(0.3, 20, 0.03) 3.24 ± 0.76% **28.10 ± 3.27%** 3.84 ± 0.01% 2.76 ± 0.65%

Table 1: Comparison of regularization techniques on Fashion-MNIST (top) and E-MNIST (bottom).
In each dataset, the base networks are PN-4, i.e., a 4[th] degree polynomial, on the top four rows, PN-10,
i.e., a 10[th] degree polynomial, on the middle four rows and PN-Conv, i.e., a 4[th] degree polynomial
with convolutions, on the bottom four rows. Our projection method exhibits the best performance in
all three attacks, with the difference on accuracy to stronger attacks being substantial.

attack, the bigger the difference of the proposed regularization scheme with the alternatives of
Jacobian and L2 regularizations.

Next, we learn the networks with varying projection bounds. The results on Fashion-MNIST and
E-MNIST are visualized in Fig. 2, where the x-axis is plotted in log-scale. As a reference point, we
include the clean accuracy curves, i.e., when there is no adversarial noise. Projection bounds larger
than 2 (in the log-axis) leave the accuracy unchanged. As the bounds decrease, the results gradually
improve. This can be attributed to the constraints the projection bounds impose into the Ui matrices.

Similar observations can be made when evaluating the clean accuracy (i.e., no adversarial noise in
the test set). However, in the case of adversarial attacks a tighter bound performs better, i.e., the best
accuracy is exhibited in the region of 0 in the log-axis. The projection bounds can have a substantial
improvement on the performance, especially in the case of stronger adversarial attacks, i.e., PGD.
Notice that all in the aforementioned cases, the intermediate values of the projection bounds yield an
increased performance in terms of the test-accuracy and the adversarial perturbations.

Beyond the aforementioned datasets, we also validate the proposed method on CIFAR-10 dataset.
The results in Fig. 3 and Table 2 exhibit similar patterns as the aforementioned experiments. Although
the improvement is smaller than the case of Fashion-MNIST and E-MNIST, we can still obtain about
10% accuracy improvement under three different adversarial attacks.

4.3 ADVERSARIAL TRAINING (AT) ON PNS

Adversarial training has been used as a strong defence against adversarial attacks. In this experiment
we evaluate whether different regularization methods can work in conjunction with adversarial training
that is widely used as a defence method. Since multi-step adversarial attacks are computationally
intensive, we utilize the FGSM attack during training, while we evaluate the trained model in all three


-----

Clean FGSM_0.1 PGD_0.1_0.01_20 PGD_0.3_0.03_20


PN-4


PN-10

2

Log of Bound


PN-Conv


100

80


60

40

20


Log of Bound

PN-4


Log of Bound

PN-Conv


(a) Fashion-MNIST

PN-10


100

80


60

40


20


Log of Bound


Log of Bound


Log of Bound


(b) E-MNIST

Figure 2: Adversarial attacks during testing on (a) Fashion-MNIST (top), (b) E-MNIST (bottom)
with the x-axis is plotted in log-scale. Note that intermediate values of projection bounds yield the
highest accuracy. The patterns are consistent in both datasets and across adversarial attacks.



Clean FGSM_0.1 PGD_0.1_0.01_20 PGD_0.3_0.03_20


PN-Conv


100

80

60

40

20


Log of Bound

Figure 3: Adversarial attacks during testing on CIFAR-10.


-----

Model PN-Conv
Projection No-proj Our method Jacobian _L2_

Clean accuracy 65.09 ± 0.14% 65.22 ± 0.13% 64.43 ± 0.19% 65.11 ± 0.08%
FGSM-0.1 6.00 ± 0.53% **15.13 ± 0.81%** 3.34 ± 0.40% 1.27 ± 0.10%
PGD-(0.1, 20, 0.01) 7.08 ± 0.68% **15.17 ± 0.88%** 1.74 ± 0.14% 1.05 ± 0.05%
PGD-(0.3, 20, 0.03) 0.41 ± 0.09% **11.71 ± 1.11%** 0.04 ± 0.02% 0.51 ± 0.04%

Table 2: Evaluation of the robustness of PN models on CIFAR-10. Each line refers to a different
adversarial attack. The projection offers an improvement in the accuracy in each case; in PGD attacks
the projection improves the accuracy by a significant margin.

adversarial attacks. For this experiment we select PN-10 as the base model. The proposed model
implements Algorithm 2.

The accuracy is reported in Table 3 with Fashion-MNIST on the top and E-MNIST on the bottom. In
the FGSM attack, the difference of the compared methods is smaller, which is expected since similar
attack is used for the training. However, for stronger attacks the difference becomes pronounced
with the proposed regularization method outperforming both the Jacobian and the L2 regularization
methods.

AT Our method + AT Jacobian + AT _L2 + AT_
Method
Adversarial training (AT) with PN-10 on Fashion-MNIST

FGSM-0.1 65.33 ± 0.46% **65.64 ± 0.35%** 62.04 ± 0.22% 65.62 ± 0.15%
PGD-(0.1, 20, 0.01) 57.45 ± 0.35% **59.89 ± 0.22%** 57.42 ± 0.24% 57.40 ± 0.36%
PGD-(0.3, 20, 0.03) 24.46 ± 0.45% **39.79 ± 1.40%** 25.59 ± 0.20% 24.99 ± 0.57%

Adversarial training (AT) with PN-10 on E-MNIST

FGSM-0.1 78.30 ± 0.18% **78.61 ± 0.58%** 70.11 ± 0.18% 78.31 ± 0.32%
PGD-(0.1, 20, 0.01) 68.40 ± 0.32% **68.51 ± 0.19%** 64.61 ± 0.16% 68.41 ± 0.37%
PGD-(0.3, 20, 0.03) 35.58 ± 0.33% **42.22 ± 0.60%** 39.83 ± 0.24% 35.17 ± 0.46%


Table 3: Comparison of regularization techniques on (a) Fashion-MNIST (top) and (b) E-MNIST (bottom) along with adversarial training (AT). The base network is a PN-10, i.e., 10[th] degree polynomial.
Our projection method exhibits the best performance in all three attacks.

The limitations of the proposed work are threefold. Firstly, Theorem 1 relies on the ℓ -operator
_∞_
norm of the face-splitting product of the weight matrices, which in practice we relax in Lemma 1 for
performing the projection. In the future, we aim to study if it is feasible to compute the non-convex
projection onto the set of PNs with bounded ℓ -norm of the face-splitting product of the weight
_∞_
matrices. This would allow us to let go off the relaxation argument and directly optimize the original
tighter Rademacher Complexity bound (Theorem 1).

Secondly, the regularization effect of the projection differs across datasets and adversarial attacks, a
topic that is worth investigating in the future.

Thirdly, our bounds do not take into account the algorithm used, which corresponds to a variant of
the Stochastic Projected Gradient Descent, and hence any improved generalization properties due to
possible uniform stability (Bousquet and Elisseeff, 2002) of the algorithm or implicit regularization
properties (Neyshabur, 2017), do not play a role in our analysis.

5 CONCLUSION

In this work, we explore the generalization properties of the Coupled CP-decomposition (CCP) and
nested coupled CP-decomposition (NCP) models that belong in the class of Polynomial Nets (PNs).
We derive bounds for the Rademacher complexity and the Lipschitz constant of the CCP and the
NCP models. We utilize the computed bounds as a regularization during training. The regularization
terms have also a substantial effect on the robustness of the model, i.e., when adversarial noise is
added to the test set. A future direction of research is to obtain generalization bounds for this class of
functions using stability notions. Along with the recent empirical results on PNs, our derived bounds
can further explain the benefits and drawbacks of using PNs.


-----

ACKNOWLEDGEMENTS

We are thankful to Igor Krawczuk and Andreas Loukas for their comments on the paper. We are
also thankful to the reviewers for providing constructive feedback. Research was sponsored by the
Army Research Office and was accomplished under Grant Number W911NF-19-1-0404. This work is
funded (in part) through a PhD fellowship of the Swiss Data Science Center, a joint venture between
EPFL and ETH Zurich. This project has received funding from the European Research Council (ERC)
under the European Union’s Horizon 2020 research and innovation programme (grant agreement
number 725594 - time-data).

REFERENCES

P. Bartlett, D. J. Foster, and M. Telgarsky. Spectrally-normalized margin bounds for neural networks,
2017.

P. L. Bartlett and S. Mendelson. Rademacher and gaussian complexities: Risk bounds and structural
results. Journal of Machine Learning Research, 3(Nov):463–482, 2002.

O. Bousquet and A. Elisseeff. Stability and generalization. Journal of Machine Learning Research,
2:499–526, 2002.

Z. Brakerski, C. Gentry, and V. Vaikuntanathan. (leveled) fully homomorphic encryption without
bootstrapping. ACM Transactions on Computation Theory (TOCT), 6(3):1–36, 2014.

G. Chrysos, S. Moschoglou, G. Bouritsas, Y. Panagakis, J. Deng, and S. Zafeiriou. π−nets: Deep
polynomial neural networks. In Conference on Computer Vision and Pattern Recognition (CVPR),
2020.

G. G. Chrysos and Y. Panagakis. NAPS: Non-adversarial polynomial synthesis. Pattern Recognit.
_Lett., 140:318–324, 2020._

M. Cisse, P. Bojanowski, E. Grave, Y. Dauphin, and N. Usunier. Parseval networks: Improving
robustness to adversarial examples. In International Conference on Machine Learning, pages
854–863. PMLR, 2017.

T. Clanuwat, M. Bober-Irizar, A. Kitamoto, A. Lamb, K. Yamamoto, and D. Ha. Deep learning for
classical japanese literature. arXiv preprint arXiv:1812.01718, 2018.

G. Cohen, S. Afshar, J. Tapson, and A. van Schaik. Emnist: an extension of mnist to handwritten
letters. arXiv preprint arXiv:1908.06571, 2017.

F. Croce and M. Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse
parameter-free attacks. In Proceedings of the 37th International Conference on Machine Learning,
2020.

Z. Cvetkovski. Hölder’s Inequality, Minkowski’s Inequality and Their Variants, pages 95–105.
Springer Berlin Heidelberg, 2012. doi: 10.1007/978-3-642-23792-8_9.

J. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra. Efficient projections onto the l 1-ball for
learning in high dimensions. In Proceedings of the 25th international conference on Machine
_learning, pages 272–279, 2008._

J. Engel, C. Resnick, A. Roberts, S. Dieleman, D. Eck, K. Simonyan, and M. Norouzi. Neural audio
synthesis of musical notes with wavenet autoencoders, 2017.

M. Fazlyab, A. Robey, H. Hassani, M. Morari, and G. J. Pappas. Efficient and accurate estimation
of lipschitz constants for deep neural networks. In Advances in neural information processing
_systems (NeurIPS), 2019._

H. Federer. Geometric measure theory. Springer, 2014.


-----

R. Gilad-Bachrach, N. Dowlin, K. Laine, K. Lauter, M. Naehrig, and J. Wernsing. Cryptonets:
Applying neural networks to encrypted data with high throughput and accuracy. In M. F. Balcan
and K. Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine
_Learning, volume 48 of Proceedings of Machine Learning Research, pages 201–210, New York,_
New York, USA, 20–22 Jun 2016. PMLR.

N. Golowich, A. Rakhlin, and O. Shamir. Size-independent sample complexity of neural networks.
In COLT, 2018.

I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In
_International Conference on Learning Representations (ICLR), 2015._

E. Hesamifard, H. Takabi, M. Ghasemi, and R. N. Wright. Privacy-preserving machine learning as a
service. Proc. Priv. Enhancing Technol., 2018(3):123–142, 2018.

J. Hoffman, D. A. Roberts, and S. Yaida. Robust learning with jacobian regularization, 2019.

S. M. Jayakumar, W. M. Czarnecki, J. Menick, J. Schwarz, J. Rae, S. Osindero, Y. W. Teh, T. Harley,
and R. Pascanu. Multiplicative interactions and where to find them. In International Conference
_on Learning Representations (ICLR), 2020._

T. Karras, S. Laine, and T. Aila. A style-based generator architecture for generative adversarial
networks. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019.

I. Kemelmacher-Shlizerman, S. M. Seitz, D. Miller, and E. Brossard. The megaface benchmark: 1
million faces for recognition at scale. In Proceedings of the IEEE Conference on Computer Vision
_and Pattern Recognition, pages 4873–4882, 2016._

T. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM review, 51(3):455–500,
2009.

A. Krizhevsky, V. Nair, and G. Hinton. The cifar-10 dataset. online: http://www. cs. toronto.
_edu/kriz/cifar. html, 55, 2014._

F. Latorre, P. Rolland, and V. Cevher. Lipschitz constant estimation of neural networks via sparse
polynomial optimization. In International Conference on Learning Representations (ICLR), 2020.

Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. doi: 10.1109/5.726791.

F. Liao, M. Liang, Y. Dong, T. Pang, X. Hu, and J. Zhu. Defense against adversarial attacks using
high-level representation guided denoiser. In Proceedings of the IEEE Conference on Computer
_Vision and Pattern Recognition, 2018._

T. Lyche. Numerical Linear Algebra and Matrix Factorizations. Springer, Cham, 2020.

T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral normalization for generative adversarial
networks. In International Conference on Learning Representations, 2018.

M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of Machine Learning. Adaptive
Computation and Machine Learning. MIT Press, Cambridge, MA, 2 edition, 2018. ISBN 978-0262-03940-6.

B. Neyshabur. Implicit regularization in deep learning. arXiv preprint arXiv:1709.01953, 2017.

B. Neyshabur, R. Tomioka, and N. Srebro. Norm-based capacity control in neural networks. In
_Conference on Learning Theory, pages 1376–1401. PMLR, 2015._

B. Neyshabur, S. Bhojanapalli, and N. Srebro. A pac-bayesian approach to spectrally-normalized
margin bounds for neural networks. arXiv preprint arXiv:1707.09564, 2017.

C. R. Rao. Estimation of heteroscedastic variances in linear models. Journal of the American
_Statistical Association, 65(329):161–172, 1970. doi: 10.1080/01621459.1970.10481070._


-----

K. Scaman and A. Virmaux. Lipschitz regularity of deep neural networks: analysis and efficient
estimation. In Advances in neural information processing systems (NeurIPS), 2018.

S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning: From Theory to Algorithms.
Cambridge University Press, 2014. ISBN 1107057132.

V. Slyusar. A family of face products of matrices and its properties. Cybernetics and Systems Analysis,
35(3):379–384, 1999.

J. Su, W. Byeon, J. Kossaifi, F. Huang, J. Kautz, and A. Anandkumar. Convolutional tensor-train lstm
for spatio-temporal learning. Advances in neural information processing systems (NeurIPS), 2020.

C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing
properties of neural networks. In International Conference on Learning Representations (ICLR),
2014.

Y. Tsuzuku, I. Sato, and M. Sugiyama. Lipschitz-margin training: Scalable certification of perturbation invariance for deep neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems,
volume 31. Curran Associates, Inc., 2018.

X. Wang, R. Girshick, A. Gupta, and K. He. Non-local neural networks. In Conference on Computer
_Vision and Pattern Recognition (CVPR), pages 7794–7803, 2018._

H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine
learning algorithms. arXiv preprint arXiv:1708.07747, 2017.

C. Xie, Y. Wu, L. v. d. Maaten, A. L. Yuille, and K. He. Feature denoising for improving adversarial robustness. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
_Recognition, 2019._

Y. Yoshida and T. Miyato. Spectral Norm Regularization for Improving the Generalizability of Deep
Learning. arXiv e-prints, art. arXiv:1705.10941, May 2017.

H. Zhang, Y. Yu, J. Jiao, E. Xing, L. E. Ghaoui, and M. Jordan. Theoretically principled trade-off
between robustness and accuracy. In Proceedings of the 36th International Conference on Machine
_Learning, 2019._


-----

A APPENDIX INTRODUCTION

The Appendix is organized as follows:

-  The related work is summarized in Appendix B.

-  In Appendix C the notation and the core Lemmas from the literature are described.

-  Further details on the Polynomial Nets are provided in Appendix D.

-  The proofs on the CCP and the NCP models are added in Appendix E and Appendix F

respectively.



-  The extensions of the theorems for convolutional layers and their proofs are detailed in

Appendix G.

-  Additional experiments are included in Appendix H.

B RELATED WORK

**Rademacher Complexity: Known bounds for the class of polynomials are a consequence of more**
general result for kernel methods (Mohri et al., 2018, Theorem 6.12). Support Vector Machines
(SVMs) with a polynomial kernel of degree k effectively correspond to a general polynomial with the
same degree. In contrast, our bound is tailored to the parametric definition of the CCP and the NCP
models, which are a subset of the class of all polynomials. Hence, they are tighter than the general
kernel complexity bounds.

Bounds for the class of neural networks were stablished in (Bartlett et al., 2017; Neyshabur et al.,
2017), but they require a long and technical proof, and in particular it assumes an ℓ2-bound on the
input, which is incompatible with image-based applications. This bound also depend on the product of
spectral norms of each layer. In contrast, our bounds are more similar in spirit to the path-norm-based
complexity bounds (Neyshabur et al., 2015), as they depend on interactions between neurons at
different layers. This interaction precisely corresponds to the face-splitting product between weight
matrices that appears in Theorem 1.

**Lipschitz constant: A variety of methods have been proposed for estimating the Lipschitz constant**
of neural networks. For example, Scaman and Virmaux (2018) (SVD), Fazlyab et al. (2019) (Semidefinite programming) and Latorre et al. (2020) (Polynomial Optimization) are expensive optimization
methods to compute tighter bounds on such constant. These methods are unusable in our case as they
would require a non-trivial adaptation to work with Polynomial Nets. In contrast we find an upper
bound that applies to such family of models, and it can be controlled with efficient ℓ -operator-norm
_∞_
projections. However, our bounds might not be the tightest. Developing tighter methods to bound
and control the Lipschitz constant for Polynomial Nets is a promising direction of future research.

C BACKGROUND

Below, we develop a detailed notation in Appendix C.1, we include related definitions in Appendix C.2
and Lemmas required for our proofs in Appendix C.3. The goal of this section is to cover many of
the required information for following the proofs and the notation we follow in this work. Readers
familiar with the different matrix/vector products, e.g., Khatri-Rao or face-splitting product, and with
basic inequalities, e.g., Hölder’s inequality, can skip to the next section.

C.1 NOTATION

Different matrix products and their associated symbols are referenced in Table 4, while matrix
operations on a matrix A are defined on Table 5. Every matrix product, e.g., Hadamard product,
can be used in two ways: a) A ◦ **_B, which translates to Hadamard product of matrices A and B, b)_**
_i=1[A][i][ abbreviates the Hadamard products][ A][1][ ◦]_ **_[A][2][ ◦]_** _[. . .][ A][N]_ .
_◦[N]_
_N products_

The symbol x[j]i [refers to][ j][th][ element of vector]|[ x][i][.] {z }


-----

Table 4: Symbols for various matrix products. The precise definitions of the products are included in
Appendix C.2 for completion.

**Symbol** **Definition**

_◦_ Hadamard (element-wise) product.
_∗_ Column-wise Khatri–Rao product.

_•_ Face-splitting product.
_⊗_ Kronecker product.
_⋆_ Convolution.

Table 5: Operations and symbols on a matrix A.


**Symbol** **Definition**

_∥A∥∞_ _ℓ∞-operator-norm; corresponds to the maximum ℓ1-norm of its rows._
**_A[i]_** _i[th]_ row of A.
**_ai,j_** (i, j)[th] element of A.
**_Ai,j_** (i, j)[th] block of a block-matrix A
**_Ai_** The i-th matrix in a set of matrices **_A1,_** _, AN_ .
_{_ _· · ·_ _}_

C.2 DEFINITIONS

For thoroughness, we include the definitions of the core products that we use in this work. Specifically,
the definitions of the Hadamard product (Definition 3), Kronecker product (Definition 4), the KhatriRao product (Definition 5), column-wise Khatri-Rao product (Definition 6) and the face-splitting
product (Definition 7) are included.

**Definition 3 (Hadamard product). For two matrices A and B of the same dimension m × n, the**
_Hadamard product A ◦_ **_B is a matrix of the same dimension as the operands, with elements given by_**

(a ◦ _b)i,j = ai,jbi,j ._

**Definition 4 (Kronecker product). If A is an m × n matrix and B is a p × q matrix, then the**
_Kronecker product A ⊗_ **_B is the pm × qn block matrix, given as follows:_**

_a1,1B_ _a1,nB_

_· · ·_
_._ _._

**_A_** **_B =_**  _.._ _..._ _.._  _._
_⊗_

_am,1B_ _am,nB_

 _· · ·_ 
 

Example: the Kronecker product of the matrices A ∈ R[2][×][2] and B ∈ R[2][×][2] is computed below:


_a1,1b1,1_ _a1,1b1,2_ _a1,2b1,1_ _a1,2b1,2_

_a1,1_ _a1,2_ _b1,1_ _b1,2_ = _a1,1b2,1_ _a1,1b2,2_ _a1,2b2,1_ _a1,2b2,2_ _._
_a2,1_ _a2,2_ _⊗_ _b2,1_ _b2,2_ a2,1b1,1 _a2,1b1,2_ _a2,2b1,1_ _a2,2b1,2_
   

_a2,1b2,1_ _a2,1b2,2_ _a2,2b2,1_ _a2,2b2,2_

**_A_** **_B_**  

 

**_A⊗B_**

| {z } | {z }

**Definition 5 (Khatri–Rao product). The Khatri–Rao product is defined as:**

| {z }

**_A ∗_** **_B = (Ai,j ⊗_** _Bi,j)i,j,_

_in which theof A and B, assuming the number of row and column partitions of both matrices is equal. The size (i, j)-th block is the mipi × njqj sized Kronecker product of the corresponding blocks_
_of the product is then ([P]i_ _[m][i][p][i][)][ ×][ (][P]i_ _[n][j][q][j][)][.]_

Example: if A and B both are 2 × 2 partitioned matrices e.g.:



**_A =_** **_A1,1_** **_A1,2_**

**_A2,1_** **_A2,2_**




_,_






_a1,1_ _a1,2_ _a1,3_
_a2,1_ _a2,2_ _a2,3_

_a3,1_ _a3,2_ _a3,3_


-----

**_B =_** **_B1,1_** **_B1,2_**

**_B2,1_** **_B2,2_**



then we obtain the following:


_,_






_b1,1_ _b1,2_ _b1,3_

_b2,1_ _b2,2_ _b2,3_
_b3,1_ _b3,2_ _b3,3_




**_A ∗_** **_B =_** **_A21,,11 ⊗_** **_B21,,11_** **_A12,,22 ⊗_** **_B12,,22_** = _a23,,11b12,,11_ _a23,,22b12,,11_ _a23,,33b12,,22_ _a23,,33b12,,33_ _._

**_A1,1_** **_B1,1_** **_A1,2_** **_B1,2_**

**_A2,1 ⊗_** **_B2,1_** **_A2,2 ⊗_** **_B2,2_**
_⊗_ _⊗_

_a1,1b1,1_ _a1,2b1,1_ _a1,3b1,2_ _a1,3b1,3_
_a2,1b1,1_ _a2,2b1,1_ _a2,3b1,2_ _a2,3b1,3_

_a3,1b2,1_ _a3,2b2,1_ _a3,3b2,2_ _a3,3b2,3_
_a3,1b3,1_ _a3,2b3,1_ _a3,3b3,2_ _a3,3b3,3_


**Definition 6 (Column-wise Khatri–Rao product). A column-wise Kronecker product of two matrices**
_may also be called the Khatri–Rao product. This product assumes the partitions of the matrices are_
_their columns. In this case m1 = m, p1 = p, n = q and for each j: nj = pj = 1. The resulting_
_product is a mp × n matrix of which each column is the Kronecker product of the corresponding_
_columns of A and B._

Example: the Column-wise Khatri–Rao product of the matrices A ∈ R[2][×][3] and B ∈ R[3][×][3] is
computed below:


_a1,1b1,1_ _a1,2b1,2_ _a1,3b1,3_

aa12,,11 _aa12,,22_ _aa12,,33_ _∗_ "bbb123,,,111 _bbb123,,,222_ _bbb123,,,333#_ = aaaa1122,,,,1111bbbb2312,,,,1111 _aaaa1122,,,,2222bbbb2312,,,,2222_ _aaaa1122,,,,3333bbbb2312,,,,3333_ _._

**_A_**  

**_B_** a2,1b3,1 _a2,2b3,2_ _a2,3b3,3_

 

| {z } | {z }  **_A∗B_** 

From here on, all ∗ refer to Column-wise Khatri–Rao product.| {z }

**Definition 7 (Face-splitting product). The alternative concept of the matrix product, which uses**
_row-wise splitting of matrices with a given quantity of rows. This matrix operation was named the_
face-splitting product of matrices or the transposed Khatri–Rao product. This type of operation is
_based on row-by-row Kronecker products of two matrices._

Example: the Face-splitting product of the matrices A ∈ R[3][×][2] and B ∈ R[3][×][2] is computed below:


_a1,1_ _a1,2_ _b1,1_ _b1,2_
_a2,1_ _a3,2_ _b2,1_ _b2,2_
"a3,1 _a3,2#_ _•_ "b3,1 _b3,2_

**_A_** **_B_**
| {z } | {z

C.3 WELL-KNOWN LEMMAS


_a1,1b1,1_ _a1,2b1,1_ _a1,1b1,2_ _a1,2b1,2_
_a2,1b2,1_ _a2,2b2,1_ _a2,1b2,2_ _a2,2b2,2_
"a3,1b3,1 _a3,2b3,1_ _a3,1b3,2_ _a3,2b3,2_

**_A•B_**
| {z


In this section, we provide the details on the Lemmas required for our proofs along with their proofs
or the corresponding citations where the Lemmas can be found as well.

**Lemma 3. (Federer, 2014) Let g, h be two composable Lipschitz functions. Then g ◦** _h is also_
_Lipschitz with Lip(g ◦_ _h) ≤_ _Lip(g)Lip(h). Here and only here ◦_ _represents function composition._

**Lemma 4. (Federer, 2014) Let f : X ⊆** R[n] _→_ R[m] _be differentiable and Lipschitz continuous._
_Let Jf_ (x) denote its total derivative (Jacobian) at x. Then Lipp(f ) = supx _Jf_ (x) _p where_
_∈X ∥_ _∥_
_∥Jf_ (x)∥p is the induced operator norm on Jf (x).

**Lemma 5 (Hölder’s inequality). (Cvetkovski, 2012) Let (S, Σ, µ) be a measure space and let**
_p, q ∈_ [1, ∞] with _p[1]_ [+][ 1]q [= 1][. Then, for all measurable real-valued functions][ f][ and][ g][ on S, it]

_holds that:_
_∥fg∥1 ≤∥f_ _∥p ∥g∥q ._


-----

**Lemma 6 (Mixed Product Property 1). (Slyusar, 1999) The following holds:**

(A1B1) (A2B2) = (A1 **_A2)(B1_** **_B2) ._**
_◦_ _•_ _∗_

**Lemma 7 (Mixed Product Property 2). The following holds:**

_◦i[N]=1[(][A][i][B][i][) =][ •][N]i=1[(][A][i][)][ ∗][N]i=1_ [(][B][i][)][ .]

_Proof. We prove this lemma by induction on N_ .

Base case (N = 1): A1B1 = A1B1.

Inductive step: Assume that the induction hypothesis holds for a particular k, i.e., the case N = k
holds. That can be expressed as:

_◦i[k]=1[(][A][i][B][i][) =][ •][k]i=1[(][A][i][)][ ∗][k]i=1_ [(][B][i][)][ .] (3)

Then we will prove that it holds for N = k + 1:


_◦i[k]=1[+1]_ [(][A][i][B][i][)]

= [◦i[k]=1[(][A][i][B][i][)]][ ◦] [(][A][k][+1][B][k][+1][)]

= [•i[k]=1[(][A][i][)][ ∗]i[k]=1 [(][B][i][)]][ ◦] [(][A][k][+1][B][k][+1][)] use inductive hypothesis (Eq. (3))

= [•i[k]=1[(][A][i][)][ •][ A][k][+1][][][∗]i[k]=1[(][B][i][)][ ∗] **_[B][k][+1][]]_** Lemma 6 [Mixed product property 1]

= •i[k]=1[+1][(][A][i][)][ ∗]i[k]=1[+1] [(][B][i][)][ .]

That is, the case N = k + 1 also holds true, establishing the inductive step.


**Lemma 8 (Massart Lemma. Lemma 26.8 in Shalev-Shwartz and Ben-David (2014)). Let**

_N_ _A_

_={a1, · · ·, aN_ _} be a finite set of vectors in R[m]. Define ¯a =_ _N[1]_ _i=1_ **_[a][i][. Then:]_**

_√2 log NP_
( ) max **_a_** _._
_R_ _A_ _≤_ **_a_** _A_ _∥_ _m_
_∈_ _[∥][a][ −]_ **[¯]**

**Definition 8 (Consistency of a matrix norm). A matrix norm is called consistent on C[n,n], if**

_∥AB∥≤∥A∥∥B∥_ _._

_holds for A, B ∈_ C[n,n].
**Lemma 9 (Consistency of the operator norm). (Lyche, 2020) The operator norm is consistent if the**
_vector norm ∥·∥α is defined for all m ∈_ N and ∥·∥β = ∥·∥α

_Proof._


**_AB_** = max _∥ABx∥α_ = max _∥ABx∥α_ _∥Bx∥α_
_∥_ _∥_ **_Bx=0̸_** _∥x∥α_ **_Bx=0̸_** _∥Bx∥α_ _∥x∥α_

max _∥Ay∥α_ max _∥Bx∥α_ = **_A_** **_B_** _._
_≤_ **_y=0̸_** _∥y∥α_ **_x=0̸_** _∥x∥α_ _∥_ _∥∥_ _∥_

**Lemma 10. (Rao, 1970)**
(AC) ∗ (BD) = (A ⊗ **_B)(C ∗_** **_D) ._**

D DETAILS ON POLYNOMIAL NETWORKS

In this section, we provide further details on the two most prominent parametrizations proposed in
Chrysos et al. (2020). This re-parametrization creates equivalent models, but enables us to absorb
the bias terms into the input terms. Firstly, we provide the re-parametrization of the CCP model in
Appendix D.1 and then we create the re-parametrization of the NCP model in Appendix D.2.


-----

D.1 RE-PARAMETRIZATION OF CCP MODEL

The Coupled CP-Decomposition (CCP) model of PNs (Chrysos et al., 2020) leverages a coupled CP
Tensor decomposition. A k-degree CCP model f (ζ) can be succinctly described by the following
recursive relations:
**_y1 = V1ζ,_** **_yn = (Vnζ)_** **_yn_** 1 + yn 1, _f_ (ζ) = Qyk + β, (4)
_◦_ _−_ _−_
whereQ ∈ R ζ[o] ∈[×][µ] Rand[δ] is the input data with β ∈ R[o] are the learnable parameters, where δ ∈ N, f (ζ) ∈ R[o] is the output of the model and µ ∈ N is the hidden rank. In order to Vn ∈ R[µ][×][δ],
simplify the bias terms in the model, we will introduce a minor re-parametrization in Lemma 11 that
we will use to present our results in the subsequent sections.
**Lemma 11. Let z = [ζ[⊤], 1][⊤]** _∈_ R[d], xn = [yn[⊤][,][ 1]][⊤] _[∈]_ [R][m][,][ C][ = [][Q][,][ β][]][ ∈] [R][o][×][m][, d][ =][ δ][ + 1][, m][ =]
_µ + 1. Define:_

**_U1 =_** **_V1_** **0** R[m][×][d], **_Ui =_** **_Vi_** **1** R[m][×][d] (i > 1) .
**0[⊤]** 1 _∈_ **0[⊤]** 1 _∈_
   

_where the boldface numbers 0 and 1 denote all-zeros and all-ones column vectors of appropriate_
_size, respectively. The CCP model in Eq. (4) can be rewritten as f_ (z) = C ◦i[k]=1 **_[U][i][z][, which is the]_**
_one used in Eq. (CCP)._

As a reminder before providing the proof, the core symbols for this proof are summarized in Table 6.

Table 6: Core symbols in the proof of Lemma 11.


**Symbol** **Dimensions** **Definition**

_◦_ -  Hadamard (element-wise) product.
**_ζ_** R[δ] Input of the polynomial expansion.
_f_ (ζ) R[o] Output of the polynomial expansion.
_k_ N Degree of polynomial expansion.
_m_ N Hidden rank of the expansion.
**_Vn_** R[µ][×][δ] Learnable matrices of the expansion.
**_Q_** R[o][×][µ] Learnable matrix of the expansion.
**_β_** R[o] Bias of the expansion.

**_z_** R[d] Re-parametrization of the input.
**_C_** R[o][×][m] **_C = (Q, β)._**


_Proof. By definition, we have:_

**_x1 = [y1[⊤][,][ 1]][⊤]_** [=] **_y11_**


**_xn = [yn[⊤][,][ 1]][⊤]_** [=] **_y1n_**



**_V1_**
**0[⊤]**



 


**_V1_**
**0[⊤]**



**_V1ζ_**



[ζ[⊤], 1][⊤] = U1z .


(Vnζ) **_yn_** 1 + yn 1
_◦_ _−_ _−_
1


**_Vnζ + 1_**


**_yn−1_**


= **_Vn_**
**0[⊤]**


Hence, it holds that:


 


**_Vn_**
**0[⊤]**



**_yn−1_**



[ζ[⊤], 1][⊤] _◦_ [yn[⊤]−1[,][ 1]][⊤] [=][ U][n][z][ ◦] **_[x][n][−][1]_** _[.]_


_f_ (z) = Qyk + β = (Q, β) **_yk_**
1



= Cxk


= C Ukz ◦ **_xk−1_**
= C Ukz ◦ (Uk−1z) ◦ **_xk−2_**
= · · ·
= C Ukz ◦ (Uk−1z) ◦· · · ◦ (U2z) ◦ **_x1_**
= C Ukz ◦ (Uk−1z) ◦· · · ◦ (U2z) ◦ (U1z)

= C ◦i[k]=1 [(][U][i][z][)][ .]


-----

D.2 REPARAMETRIZATION OF THE NCP MODEL

The nested coupled CP decomposition (NCP) model of PNs (Chrysos et al., 2020) leverages a joint
hierarchical decomposition. A k-degree NCP model f (ζ) is expressed with the following recursive
relations:

**_y1 = (V1ζ)_** (b1), **_yn = (Vnζ)_** (Unyn 1 + bn), _f_ (ζ) = Qyk + β . (5)
_◦_ _◦_ _−_

wherebis the hidden rank. In order to simplify the bias terms in the model, we will introduce a minorn ∈ **_ζR ∈[µ], UR[δ]n ∈is the input data withR[µ][×][µ], Q ∈_** R[o][×] δ[µ] ∈andN β, f ∈(ζ) ∈R[o]Rare the learnable parameters, where[o] is the output of the model and Vn ∈ µR ∈[µ][×]N[δ],
re-parametrization in Lemma 12 that we will use to present our results in the subsequent sections.

**Lemma 12. Let z = [ζ[⊤], 1][⊤]** _∈_ R[d], xn = [yn[⊤][,][ 1]][⊤] _[∈]_ [R][m][,][ C][ = [][Q][,][ β][]][ ∈] [R][o][×][m][, d][ =][ δ][ + 1][, m][ =]
_µ + 1. Let:_


**_s1 = [b[⊤]1_** _[,][ 1]][⊤]_ _[∈]_ [R][m][,] **_Si =_** **_Ui_** **_bi_**
**0[⊤]** 1



R[m][×][m](i > 1), **_Ai =_** **_Vi_**
_∈_ **0[⊤]**



_∈_ R[m][×][d] _._


_where the boldface numbers 0 and 1 denote all-zeros and all-ones column vectors of appropriate_
_size, respectively. The NCP model in Eq. (5) can be rewritten as_

**_x1 = (A1z)_** (s1), **_xn = (Anz)_** (Snxn 1), _f_ (z) = Cxk . (6)
_◦_ _◦_ _−_

_In the aforementioned Eq. (6), we have written Sn even for n = 1, when s1 is technically a vector,_
_but this is done for convenience only and does not change the end result._

E RESULT OF THE CCP MODEL

E.1 PROOF OF THEOREM 1: RADEMACHER COMPLEXITY BOUND OF CCP UNDER ℓ NORM
_∞_

To facilitate the proof below, we include the related symbols in Table 7. Below, to avoid cluttering
the notation, we consider that the expectation is over σ and omit the brackets as well.

Table 7: Core symbols for proof of Theorem 1.

Symbol Dimensions Definition

_◦_ -  Hadamard (element-wise) product.

_•_ -  Face-splitting product.
_∗_ -  Column-wise Khatri–Rao product.
**_z_** R[d] Input of the polynomial expansion.
_f_ (z) R Output of the polynomial expansion.
_k_ N Degree of polynomial expansion.
_m_ N Hidden rank of the expansion.
**_Ui_** R[m][×][d] Learnable matrices.
**_c_** R[1][×][m] Learnable matrix.

_µ_ R _∥c∥1 ≤_ _µ._
_λ_ R _•i[k]=1[(][U][i][)]_ _∞_ _[≤]_ _[λ][.]_


-----

_Proof._

_RZ(FCCP[k]_ [) =][ E][ sup]f _∈FCCP[k]_

= E sup
_f_ _∈FCCP[k]_

= E sup
_f_ _∈FCCP[k]_

_≤_ E supf _∈FCCP[k]_

= E sup
_f_ _∈FCCP[k]_

= E sup
_f_ _∈FCCP[k]_

_≤_ E supf _∈FCCP[k]_


_σjf_ (zj)
_j=1_

X


_σj_ **_c, ◦i[k]=1[(][U][i][z][j][)]_**

_j=1_

X  

_n_

*c, _j=1[σj◦i[k]=1[(][U][i][z][j][)]]_

X



[σj◦i[k]=1[(][U][i][z][j][)]]
_j=1_

X _∞_

_n_

_j=1[σj •i[k]=1_ [(][U][i][)][ ∗]i[k]=1 [(][z][j][)]]

X


Lemma 5 [Hölder’s inequality]

Lemma 7 [Mixed product property]

(7)


_n_

_[∥][c][∥][1]_

1
_n_

_[∥][c][∥][1]_

1
_n_

_[∥][c][∥][1]_

1
_n_

_[∥][c][∥][1]_



_•i[k]=1[(][U][i][)]_


_j=1[σj ∗i[k]=1_ [(][z][j][)]]

X



_•i[k]=1[(][U][i][)]_


_j=1[σj ∗i[k]=1_ [(][z][j][)]]

X


_≤_ _[µλ]n_ [E]


_j=1[σj ∗i[k]=1_ [(][z][j][)]]

X


Next, we compute the bound of E _j=1[[][σ][j][ ∗]i[k]=1_ [(][z][j][)]]
_∞[.]_

Let Zj = _i=1[(][z][j][)][ ∈]_ [R][d][k] [. For each][P][n][ l][ ∈] [[][d][k][]][, let][ v][l][ = (][Z]1[l] _[, . . .,][ Z]n[l]_ [)][ ∈] [R][n][. Note that][ ∥][v][l][∥]2
_√n maxj ∗Z[k]_ _j_ . Let V = **_v1, . . ., vdk_** . Then, it is true that: _[≤]_
_∥_ _∥∞_ _{_ _}_


_d[k]_
= E max
_l=1_


_j=1[σj ∗i[k]=1_ [(][z][j][)]]

X


= E


= nR(V ) . (8)


_σjZj_
_j=1_

X


_σj(vl)j_
_j=1_

X


Using Lemma 8 [Massart Lemma] we have that:

(V ) 2 max **_Zj_**
_R_ _≤_ _j_ _∥_ _∥∞_


2 log (d[k])/n . (9)


-----

Then, it holds that:

_RZ(FCCP[k]_ [) =][ E][ sup]f _∈FCCP[k]_


_σjf_ (zj)
_j=1_

X


_≤_ _[µλ]n_ [E]


_j=1[σj ∗i[k]=1_ [(][z][j][)]]

X


Eq. (7)


= _[µλ]n [n][R][(][V][ )]_ Eq. (8)

2µλ max **_Zj_** 2 log(d[k])/n Eq. (9)
_≤_ _j_ _∥_ _∥∞_

q

= 2µλ max _i=1[(][z][j][)]_ 2 log (d[k])/n
_j_ _∗[k]_ _∞_

q

2µλ(max **_zj_** )[k] 2 log (d[k])/n
_≤_ _j_ _∥_ _∥∞_
q


(10)


_≤_ 2µλ


2k log (d)/n .


E.2 PROOF OF LEMMA 1

Table 8: Core symbols in the proof of Lemma 1.

**Symbol** **Dimensions** **Definition**

_⊗_ -  Kronecker product.

_•_ -  Face-splitting product.
**_z_** R[d] Input of the polynomial expansion.
_f_ (z) R Output of the polynomial expansion.
_k_ N Degree of polynomial expansion.
_m_ N Hidden rank of the expansion.
**_Ui_** R[m][×][d] Learnable matrices.
**_Ui[j]_** R[d] _j[th]_ row of Ui.

_λi_ R **_Ui_** _λi for i = 1, 2, . . ., k._
_∥_ _∥∞_ _≤_


_Proof._
_m_

_•i[k]=1[(][U][i][)]_ _∞_ [=] maxj=1 [•i[k]=1[(][U][i][)]][j] 1

_m_
= maxj=1 _⊗i[k]=1[[][U][ j]i_ []] 1 Definition of Face-splitting product

_k_

= maxm **_Ui[j]_** Multiplicativity of absolute value
_j=1_ 1

"i=1 #
Y

_k_

maxm **_Ui[j]_**

_≤_ _j=1_ 1

_i=1_  

Y

_k_

= **_Ui_** _._

_∥_ _∥∞_
_i=1_

Y

E.3 RADEMACHER COMPLEXITY BOUND UNDER ℓ2 NORM

**Theorem 6. Let Z =** **_z1, . . ., zn_** R[d] _and suppose that_ **_zj_** 1 for all j = 1, . . ., n. Let
_{_ _} ⊆_ _∥_ _∥∞_ _≤_
_CCP_ [:=] _f_ (z) = **_c,_** _i=1[U][i][z]_ : **_c_** 2 _µ,_ _i=1[U][i]_ 2 _._
_F_ _[k]_ _◦[k]_ _∥_ _∥_ _≤_ _•[k]_ _[≤]_ _[λ]_



-----

_The Empirical Rademacher Complexity of CCPk (k-degree CCP polynomials) with respect to Z is_
_bounded as:_

_Z(_ _CCP[)][ ≤]_ _[µλ]_
_R_ _F_ _[k]_ _√n ._

To facilitate the proof below, we include the related symbols in Table 9. Below, to avoid cluttering
the notation, we consider that the expectation is over σ and omit the brackets as well.

Table 9: Core symbols for proof of Theorem 6.

**Symbol** **Dimensions** **Definition**

_◦_ -  Hadamard (element-wise) product.

_•_ -  Face-splitting product.
_∗_ -  Column-wise Khatri–Rao product.
**_z_** R[d] Input of the polynomial expansion.
_f_ (z) R Output of the polynomial expansion.
_k_ N Degree of polynomial expansion.
_m_ N Hidden rank of the expansion.
**_Ui_** R[m][×][d] Learnable matrices.
**_c_** R[1][×][m] Learnable matrix.

_µ_ R _∥c∥2 ≤_ _µ._
_λ_ R _i=1[(][U][i][)]_ 2

_•[k]_ _[≤]_ _[λ][.]_


_Proof._

_RZ(FCCP[k]_ [) =][ E][ sup]f _∈FCCP[k]_

= E sup
_f_ _∈FCCP[k]_

= E sup
_f_ _∈FCCP[k]_

_≤_ E supf _∈FCCP[k]_

= E sup
_f_ _∈FCCP[k]_

= E sup
_f_ _∈FCCP[k]_

_≤_ E supf _∈FCCP[k]_


_σjf_ (zj)
_j=1_

X


_σj_ **_c, ◦i[k]=1[(][U][i][z][j][)]_**

_j=1_

X  

_n_

*c, _j=1[σj◦i[k]=1[(][U][i][z][j][)]]_

X



[σj◦i[k]=1[(][U][i][z][j][)]]
_j=1_

X 2

_n_

_j=1[σj •i[k]=1_ [(][U][i][)][ ∗]i[k]=1 [(][z][j][)]]

X


Lemma 5 [Hölder’s inequality]

Lemma 7 [Mixed product property]


_n_

_[∥][c][∥][2]_

1
_n_

_[∥][c][∥][2]_

1
_n_

_[∥][c][∥][2]_

1
_n_

_[∥][c][∥][2]_



_•i[k]=1[(][U][i][)]_


_j=1[σj ∗i[k]=1_ [(][z][j][)]]

X



_•i[k]=1[(][U][i][)]_ 2 _[.]_


_j=1[σj ∗i[k]=1_ [(][z][j][)]]

X


-----

_j=1[σj ∗i[k]=1_ [(][z][j][)]]

X


= E

u
u
u
t

vE

_≤_ u

u
u
t

= E

v
u
u
t


_j=1[σj ∗i[k]=1_ [(][z][j][)]]

X

_n_

_j=1[σj ∗i[k]=1_ [(][z][j][)]]

X


Jensen’s inequality



[σsσj _∗i[k]=1[(][z][s][)][,][ ∗][k]i=1[(][z][j][)]_
_s,j_

X


(11)



[ _∗i[k]=1[(][z][j][)]_ 2[]]
_j=1_

X

[2]


_∥zj∥2[2][)]_
_i=1_

Y


_j=1_

t

_≤_ _[√]n ._


So:



_•i[k]=1[(][U][i][)]_


_RZ(FCCP[k]_ [)][ ≤] [E][ sup]f _∈FCCP[k]_


_j=1[σj ∗i[k]=1_ [(][z][j][)]]

X


_n_

_[∥][c][∥][2]_


_≤_ supf _∈FCCPk_ _[∥][c]√[∥][2]n_ _•i[k]=1[(][U][i][)]_

_≤_ _√[µλ]n ._


E.4 LIPSCHITZ CONSTANT BOUND OF THE CCP MODEL

We will first prove a more general result about the ℓp-Lipschitz constant of the CCP model.
**Theorem 7. The Lipschitz constant (with respect to the ℓp-norm) of the function defined in Eq. (CCP),**
_restricted to the set {z ∈_ R[d] : ∥z∥p ≤ 1} is bounded as:


_Lipp(f_ ) _k_ **_C_** _p_
_≤_ _∥_ _∥_


_∥Ui∥p ._
_i=1_

Y


_Proof. Let g(x) = Cx and h(z) = ◦i[k]=1[(][U][i][z][)][. Then it holds that][ f]_ [(][z][) =][ g][(][h][(][z][))][. By][ Lemma 3][,]
we have: Lipp(f ) Lipp(g)Lipp(h). We will compute an upper bound of each function individually.
_≤_

Let us first consider the function g(x) = Cx. By Lemma 4, because g is a linear map represented by
a matrix C, its Jacobian is Jg(x) = C. So:

Lipp(g) = ∥C∥p := **_xsupp=1_** _∥Cx∥p ._
_∥_ _∥_

where ∥C∥p is the operator norm on matrices induced by the vector p-norm.

Now, let us consider the function h(z) = ◦i[k]=1[U][i][z][. Its Jacobian is given by:]


_dh_
_dz_ [=]


diag( _j=iUjz)Ui ._
_◦_ _̸_
_i=1_

X


-----

Using Lemma 4 we have:

Lipp(h) sup
_≤_ **_z:∥z∥p≤1_**

_≤_ **_z:∥supz∥p≤1_**

_≤_ **_z:∥supz∥p≤1_**

_≤_ **_z:∥supz∥p≤1_**

_≤_ **_z:∥supz∥p≤1_**

_≤_ **_z:∥supz∥p≤1_**

_≤_ **_z:∥supz∥p≤1_**



[diag(◦j≠ _i(Ujz))Ui]_
_i=1_ _p_

X

_k_

_∥diag(◦j≠_ _i(Ujz))Ui∥p_ Triangle inequality
_i=1_

X

_k_

_i=1_ _∥diag(◦j≠_ _i(Ujz))∥p ∥Ui∥p_ Lemma 9 [consistency]

X

_k_

_i=1_ _∥◦j≠_ _i(Ujz)∥p ∥Ui∥p_

X


( **_Ujz_** _p)_ **_Ui_** _p_
_∥_ _∥_ _∥_ _∥_

Yj≠ _i_

( **_Uj_** _p_ **_z_** _p)_ **_Ui_** _p_

Yj≠ _i_ _∥_ _∥_ _∥_ _∥_ _∥_ _∥_

_k_

( **_Uj_** _p)_
_∥_ _∥_
_j=1_

Y


_i=1_

_k_

_i=1_

X

_k_

_i=1_

X


= k


_∥Uj∥p ._
_j=1_

Y


So:

E.4.1 PROOF OF THEOREM 2


_Lipp(_ _L)_ _Lipp(g)Lipp(h)_
_F_ _≤_


_k_ **_C_** _p_
_≤_ _∥_ _∥_


_∥Ui∥p ._
_i=1_

Y


_Proof. This is a particular case of Theorem 7 when p = ∞._


-----

F RESULT OF THE NCP MODEL

F.1 PROOF OF THEOREM 3: RADEMACHER COMPLEXITY OF NCP UNDER ℓ NORM
_∞_

_Proof._


_RZ(FNCP[k]_ [) =][ E][ sup]f _∈FNCP[k]_

= E sup
_f_ _∈FNCP[k]_

= E sup
_f_ _∈FNCP[k]_

_≤_ E supf _∈FNCP[k]_

= E sup
_f_ _∈FNCP[k]_

= E sup
_f_ _∈FNCP[k]_

= E sup
_f_ _∈FNCP[k]_


_σjf_ (zj)
_j=1_

X

_n_

(σj **_c, xk(zj)_** )
_j=1_ _⟨_ _⟩_

X


**_c,_**



[σjxk(zj)]
_j=1_

X


Lemma 5 [Hölder’s inequality]

Lemma 7 [Mixed product property]

(12)


_n_

_[∥][c][∥][1]_

1

_n_

_[∥][c][∥][1]_

1

_n_

_[∥][c][∥][1]_

1

_n_

_[∥][c][∥][1]_



[σjxk(zj)]
_j=1_

X _∞_

_n_

[σj((Akzj) ◦ (Skxk−1(zj)))]
_j=1_

X _∞_

_n_

_j=1[σj((Ak • Sk)(zj ∗_ **_xk−1(zj)))]_**

X


(Ak **_Sk)_**
_•_


_j=1[σj(zj ∗_ **_xk−1(zj))]_**

X


Now, because of the recursive definition of the Eq. (NCP), we obtain:


_j=1_ _σj(zj ∗_ **_xk−1(zj)) =_**

X


_j=1_ _σj(zj ∗_ (Ak−1zj) ◦ (Sk−1xk−2(zj)))

X

_n_

_j=1_ _σj(zj ∗_ ((Ak−1 • Sk−1)(zj ∗ **_xk−2(zj))))_** Lemma 7

X

_n_

_j=1_ _σj(I ⊗_ (Ak−1 • Sk−1))(zj ∗ (zj ∗ **_xk−2(zj)))_** Lemma 10

X


= I ⊗ (Ak−1 • Sk−1)

recursively applying this argument we have:


_k−1)_ _j=1[σj(zj ∗_ (zj ∗ **_xk−2(zj))) ._**

X

(13)

_k−1_ _n_

_i=1_ **_I ⊗_** **_Ai • Si!_** _j=1_ _σj ∗i[k]=1_ [(][z][j][)][ .] (14)

Y X


_j=1_ _σj(zj ∗_ **_xk−1(zj)) =_**

X


Combining the two previous equations (Eqs. (13) and (14)) inside Eq. (12) we finally obtain


-----

_k−1_

**_I_** **_Ai_** **_Si_**
_i=1_ _⊗_ _•_

Y

_k−1_

**_I_** **_Ai_** **_Si_**
_i=1_ _⊗_ _•_

Y


_RZ(FNCP[k]_ [)][ ≤] [E][ sup]f _∈FNCP[k]_

_≤_ E supf _∈FNCP[k]_


_j=1_ _σj ∗i[k]=1_ [(][z][j][)]

X _∞_

_n_

_j=1_ _σj ∗i[k]=1_ [(][z][j][)]

X


_n_

_[∥][c][∥][1]_

1
_n_

_[∥][c][∥][1]_


(Ak **_Sk)_**
_•_


_n_

NCP

_[∥][c][∥][1]_

_n_

[(][A][k][ •][ S][k][)]

_j=1_ _σj ∗i[k]=1_ [(][z][j][)]

X


_≤_ _[µλ]n_ [E]


= _[µλ]n [n][R][(][V][ ) =][ µλ][R][(][V][ )][ .]_ Eq. (8) .

following the same arguments as in Eq. (10) it follows that:


2k log (d)


_RZ(FNCP[k]_ [)][ ≤] [2][µλ]


F.2 PROOF OF LEMMA 2

_Proof._


_k−1_

**_I_** **_Ai_** **_Si_**
_i=1_ _⊗_ _•_

Y


_k−1_

**_I_** **_Ai_** **_Si_** Lemma 9 [consistent]
_i=1_ _∥_ _⊗_ _•_ _∥∞_

Y



[(][A][k][ •][ S][k][)]


**_Ak_** **_Sk_**
_≤∥_ _•_ _∥∞_


**_Ai_** **_Si_**
_i=1_ _∥_ _•_ _∥∞_

Y

_k_

_∥Ai∥∞∥Si∥∞_ Lemma 1 .
_i=1_

Y


F.3 RADEMACHER COMPLEXITY UNDER ℓ2 NORM

**Theorem 8. Let Z =** **_z1, . . ., zn_** R[d] _and suppose that_ **_zj_** 1 for all j = 1, . . ., n. Define
_{_ _} ⊆_ _∥_ _∥∞_ _≤_
_the matrix Φ(A1, S1, . . ., An, Sn) := (Ak •_ **_Sk)_** _i=1_ **_[I][ ⊗]_** **_[A][i][ •]_** **_[S][i][. Consider the class of functions:]_**

[Q][k][−][1]

_NCP_ [:=][ {][f] [(][z][)][ as in][ (][NCP][)][ :][ ∥][C][∥][2] 2
_F_ _[k]_ _[≤]_ _[µ,][ ∥][Φ(][A][1][,][ S][1][, . . .,][ A][k][,][ S][k][)][∥]_ _[≤]_ _[λ][}][,]_

_where C ∈_ R[1][×][m] _(single output case). The Empirical Rademacher Complexity of NCPk (k-degree_
_NCP polynomials) with respect to Z is bounded as:_

_Z(_ _NCP[)][ ≤]_ _[µλ]_
_R_ _F_ _[k]_ _√n ._


-----

_Proof._

_RZ(FNCP[k]_ [) =][ E][ sup]f _∈FNCP[k]_

= E sup
_f_ _∈FNCP[k]_

= E sup
_f_ _∈FNCP[k]_

_≤_ E supf _∈FNCP[k]_

= E sup
_f_ _∈FNCP[k]_

= E sup
_f_ _∈FNCP[k]_

= E sup
_f_ _∈FNCP[k]_

= E sup
_f_ _∈FNCP[k]_

_≤_ E supf _∈FNCP[k]_


_σjf_ (zj)
_j=1_

X

_n_

(σj **_c, xk(zj)_** )
_j=1_ _⟨_ _⟩_

X


**_c,_**



[σjxk(zj)]
_j=1_

X


Lemma 5 [Hölder’s inequality]

Lemma 7 [Mixed product property]


_n_

_[∥][c][∥][2]_

1

_n_

_[∥][c][∥][2]_

1

_n_

_[∥][c][∥][2]_

1

_n_

_[∥][c][∥][2]_

1

_n_

_[∥][c][∥][2]_

1

_n_

_[∥][c][∥][2]_



[σjxk(zj)]
_j=1_

X 2

_n_

[σj((Akzj) ◦ (Skxk−1(zj)))]
_j=1_

X 2

_n_

_j=1[σj((Ak • Sk)(zj ∗_ **_xk−1(zj)))]_**

X


(Ak **_Sk)_**
_•_

(Ak **_Sk)_**
_•_


_j=1[σj(zj ∗_ **_xk−1(zj))]_**

X


_k−1_

**_I_** **_Ai_** **_Si_**
_i=1_ _⊗_ _•_

Y

_k−1_

**_I_** **_Ai_** **_Si_**
_i=1_ _⊗_ _•_

Y


_j=1_ _σj ∗i[k]=1_ [(][z][j][)]]

X 2

_n_

2 _j=1_ _σj ∗i[k]=1_ [(][z][j][)]]

X


Eq. (14)


_n_

NCP

_[∥][c][∥][2]_

_n_

[(][A][k][ •][ S][k][)]

_j=1_ _σj ∗i[k]=1_ [(][z][j][)]

X


_≤_ _[µλ]n_ [E]


_≤_ _√[µλ]n ._ Eq. (11) .

F.4 LIPSCHITZ CONSTANT BOUND OF THE NCP MODEL

**Theorem 9. Let FL be the class of functions defined as**

_L :=_ **_x1 = (A1z)_** (S1), xn = (Anz) (Snxn 1), f (z) = Cxk :
_F_ _◦_ _◦_ _−_


**_C_** _p_ _µ,_ **_Ai_** _p_ _λi,_ **_Si_** _p_ _ρi,_ **_z_** _p_ 1 _._
_∥_ _∥_ _≤_ _∥_ _∥_ _≤_ _∥_ _∥_ _≤_ _∥_ _∥_ _≤_


_The Lipschitz Constant of_ _(k-degree NCP polynomial) under ℓp norm restrictions is bounded as:_
_FL_


_Lipp(_ _L)_ _kµ_
_F_ _≤_


(λiρi) .
_i=1_

Y


_Proof. Let g(x) = Cx, h(z) = (Anz) ◦_ (Snxn−1(z)). Then it holds that f (z) = g(h(z)).


-----

By Lemma 3, we have: Lip(f ) ≤ Lip(g)Lip(h). This enables us to compute an upper bound of each
function (i.e., g, h) individually.

Let us first consider the function g(x) = Cx. By Lemma 4, because g is a linear map represented by
a matrix C, its Jacobian is Jg(x) = C. So:

Lipp(g) = **_C_** _p :=_ sup **_Cx_** _p =_ _σmax(C)_ if p = 2
_∥_ _∥_ **_x_** _p=1_ _∥_ _∥_ (maxi _j_ **_C(i,j)_** if p = _._
_∥_ _∥_ _∞_

P

where ∥C∥p is the operator norm on matrices induced by the vector p-norm, and σmax(C) is the
largest singular value of C.

Now, let us consider the function xn(z) = h(z) = (Anz) ◦ (Snxn−1(z)). Its Jacobian is given by:

_Jxn = diag(Anz)SnJxn−1 + diag(Snxn−1)An,_ _Jx1 = diag(S1)A1 ._

_Lipp(h) =_ sup _Jxn_ _p_
**_z:∥z∥p≤1_** _∥_ _∥_

= **_z:∥supz∥p≤1_** _∥_ diag(Anz)SnJxn−1 + diag(Snxn−1)An∥p

sup diag(Anz)SnJxn 1 _p +_ diag(Snxn 1)An _p_ Triangle inequality
_≤_ **_z:∥z∥p≤1_** _∥_ _−_ _∥_ _∥_ _−_ _∥_

sup diag(Anz) _p_ **_Sn_** _p_ _Jxn_ 1 _p +_ diag(Snxn 1) _p_ **_An_** _p_ Lemma 9 [consistent]
_≤_ **_z:∥z∥p≤1_** _∥_ _∥_ _∥_ _∥_ _∥_ _−_ _∥_ _∥_ _−_ _∥_ _∥_ _∥_

sup **_Anz_** _p_ **_Sn_** _p_ _Jxn_ 1 _p +_ **_Snxn_** 1 _p_ **_An_** _p_
_≤_ **_z:∥z∥p≤1_** _∥_ _∥_ _∥_ _∥_ _∥_ _−_ _∥_ _∥_ _−_ _∥_ _∥_ _∥_

sup **_An_** _p_ **_z_** _p_ **_Sn_** _p_ _Jxn_ 1 _p +_ **_Sn_** _p_ **_xn_** 1 _p_ **_An_** _p_
_≤_ **_z:∥z∥p≤1_** _∥_ _∥_ _∥_ _∥_ _∥_ _∥_ _∥_ _−_ _∥_ _∥_ _∥_ _∥_ _−_ _∥_ _∥_ _∥_

= sup **_An_** _p_ **_z_** _p_ **_Sn_** _p_ _Jxn_ 1 _p +_ **_Sn_** _p_ (An 1z) (Sn 1xn 2) _p_ **_An_** _p_
**_z:∥z∥p≤1_** _∥_ _∥_ _∥_ _∥_ _∥_ _∥_ _∥_ _−_ _∥_ _∥_ _∥_ _∥_ _−_ _◦_ _−_ _−_ _∥_ _∥_ _∥_

sup **_An_** _p_ **_z_** _p_ **_Sn_** _p_ _Jxn_ 1 _p +_ **_Sn_** _p_ **_An_** 1z _p_ **_Sn_** 1xn 2 _p_ **_An_** _p_
_≤_ **_z:∥z∥p≤1_** _∥_ _∥_ _∥_ _∥_ _∥_ _∥_ _∥_ _−_ _∥_ _∥_ _∥_ _∥_ _−_ _∥_ _∥_ _−_ _−_ _∥_ _∥_ _∥_

sup **_An_** _p_ **_z_** _p_ **_Sn_** _p_ _Jxn_ 1 _p +_ **_Sn_** _p_ **_An_** 1 _p_ **_z_** _p_ **_Sn_** 1 _p_ **_xn_** 2 _p_ **_An_** _p_
_≤_ **_z:∥z∥p≤1_** _∥_ _∥_ _∥_ _∥_ _∥_ _∥_ _∥_ _−_ _∥_ _∥_ _∥_ _∥_ _−_ _∥_ _∥_ _∥_ _∥_ _−_ _∥_ _∥_ _−_ _∥_ _∥_ _∥_

= sup **_An_** _p_ **_z_** _p_ **_Sn_** _p(_ _Jxn_ 1 _p +_ **_An_** 1 _p_ **_Sn_** 1 _p_ **_xn_** 2 _p)_
**_z:∥z∥p≤1_** _∥_ _∥_ _∥_ _∥_ _∥_ _∥_ _∥_ _−_ _∥_ _∥_ _−_ _∥_ _∥_ _−_ _∥_ _∥_ _−_ _∥_

_n−1_

_≤_ **_z:∥supz∥p≤1_** _∥An∥p∥z∥p∥Sn∥p(∥Jxn−1_ _∥p +_ _i=1_ (∥Si∥p∥Ai∥p)∥z∥p[n][−][2]) .

Y

Then we proof the result by induction.

Inductive hypothesis:


sup _Jxn_ _p_ _n_
**_z:∥z∥p≤1_** _∥_ _∥_ _≤_


( **_Si_** _p_ **_Ai_** _p) ._
_∥_ _∥_ _∥_ _∥_
_i=1_

Y


Case k = 1:


_Lipp(h) =_ sup _Jx1_ _p_
**_z:∥z∥p≤1_** _∥_ _∥_

= diag(S1)A1 _p_
_∥_ _∥_
diag(S1) _p_ **_A1_** _p_
_≤∥_ _∥_ _∥_ _∥_
_≤∥S1∥p∥A1∥p ._


-----

Case k = n:

_Lipp(h) =_ sup _Jxn_ _p_
**_z:∥z∥p≤1_** _∥_ _∥_

sup **_An_** _p_ **_z_** _p_ **_Sn_** _p(_ _Jxn_ 1 _p +_
_≤_ **_z:∥z∥p≤1_** _∥_ _∥_ _∥_ _∥_ _∥_ _∥_ _∥_ _−_ _∥_


_n−1_

( **_Si_** _p_ **_Ai_** _p)_ **_z_** _p_
_∥_ _∥_ _∥_ _∥_ _∥_ _∥[n][−][2]_
_i=1_

Y


_n−1_

( **_Si_** _p_ **_Ai_** _p) +_
_∥_ _∥_ _∥_ _∥_
_i=1_

Y


_n−1_

( **_Si_** _p_ **_Ai_** _p)_ **_z_** _p_
_∥_ _∥_ _∥_ _∥_ _∥_ _∥[n][−][2]_
_i=1_

Y


sup **_An_** _p_ **_z_** _p_ **_Sn_** _p((n_ 1)
**_z:∥z∥p≤1_** _∥_ _∥_ _∥_ _∥_ _∥_ _∥_ _−_


_≤_ _n_


( **_Si_** _p_ **_Ai_** _p) ._
_∥_ _∥_ _∥_ _∥_
_i=1_

Y

_Lipp(_ _L)_ _Lipp(g)Lipp(h)_
_F_ _≤_


So:


_k_ **_C_** _p_
_≤_ _∥_ _∥_


( **_Si_** _p_ **_Ai_** _p) ._
_∥_ _∥_ _∥_ _∥_
_i=1_

Y


F.4.1 PROOF OF THEOREM 4

_Proof. This is particular case of Theorem 9 with p = ∞._

G RELATIONSHIP BETWEEN A CONVOLUTIONAL LAYER AND A FULLY
CONNECTED LAYER

In this section we discuss various cases of input/output types depending on the dimensionality of the
input tensor and the output tensor. We also provide the proof of Theorem 5.
**Theorem 10. Let A ∈** R[n]. Let K ∈ R[h] _be a 1-D convolutional kernel. For simplicity, we assume_
_h is odd and h ≤_ _n. Let B ∈_ R[n], B = K ⋆ **_A be the output of the convolution. Let U be the_**
_convolutional operator i.e., the linear operator (matrix) U ∈_ R[n][×][n] _such that B = K ⋆_ **_A = UA._**
_It holds that_ **_U_** = **_K_** 1.
_∥_ _∥∞_ _∥_ _∥_

**Theorem 11. Let A ∈** R[n][×][m], and let K ∈ R[h][×][h] _be a 2-D convolutional kernel. For simplicity_
_assume h is odd number and h ≤_ min(n, m). Let B ∈ R[n][×][m], B = K ⋆ **_A be the output of the_**
_convolution. Let U be the convolutional operator i.e., the linear operator (matrix) U ∈_ R[nm][×][nm]
_such that vec(B) = U_ _vec(A). It holds that_ **_U_** = _vec(K)_ 1.
_∥_ _∥∞_ _∥_ _∥_

G.1 PROOF OF THEOREM 10

_Proof. From, B = K ⋆_ **_A = UA we can obtain the following:_**


_u1,1_ _u1,2_ _u1,n_

_· · ·_
_u2,1_ _u2,2_ _u2,n_

_· · ·_

 . . .

.. .. ... ..

un,1 _un,2_ _un,n_
 _· · ·_


We observe that:




 [(][A][1][,][ · · ·][,][ A][n][)][⊤] [= (][K][1][,][ · · ·][,][ K][h][)][⊤] _[⋆]_ [(][A][1][,][ · · ·][,][ A][n][)][⊤] _[.]_


_h+1_


+j−i if _i_ _j_ 2
_|_ _−_ _| ≤_ _[h][−][1]_

if _i_ _j_ _>_ _[h][−]2_ [1]
_|_ _−_ _|_


_ui,j =_


-----

Then, it holds that:

_n_
**_U_** = max
_∥_ _∥∞_ _i=1_

G.2 PROOF OF THEOREM 11


**_K[j]_** = ∥K∥1 .


_ui,j_ max
_|_ _| ≤_ _i=1_
_j=1_

X


_j=1_


_Proof. We partition U into n × n partition matrices of shape m × m. Then the (i, j)[th]_ partition
matrix U(i,j) describes the relationship between the B[i] and the A[j]. So, U(i,j) is also similar to the
Toeplitz matrix in the previous result.


**_M h+12_** +j−i if |i − _j| ≤_ _[h][−]2_ [1]

**0** if _i_ _j_ _>_ _[h][−]2_ [1]
_|_ _−_ _|_

_k(i, h+12_ +l−s) if |s − _l| ≤_ _[h][−]2_ [1]

0 if _s_ _l_ _>_ _[h][−]2_ [1]
_|_ _−_ _|_


**_U(i,j) =_**

Meanwhile, the matrix M satisfies:

_mi(s,l) =_

Then, we have the following:


_n×m_ _n_

_ui,j_ max
_|_ _| ≤_ _i=1_
_j=1_

X


**_K[j]_**
1 [=]


**_K[i]_**
1 [=][ ∥][vec][(][K][)][∥][1][ .]


**_U_** = maxn×m
_∥_ _∥∞_ _i=1_


_n_
**_Ui,j_** max
_∥_ _∥∞_ _≤_ _i=1_
_j=1_

X


_j=1_


_i=1_


In addition, by h ≤ _n, h ≤_ _m we have:_
**_U_** _h+12_ +M ( _[h][−]2_ [1]

Then, it holds that ∥U _∥∞_ = _i=1_ **_K[i]_** 1[.]

G.3 PROOF OF THEOREM 5[P][h]


**_K[i]_** 1 _[.]_ (15)


1 [=]


_i=1_


_Proof. We partition U into o × r partition matrices of shape nm × nm. Then the (i, j)[th]_ partition
of the matrix U(i,j) describes the relationship between the i[th] channel of B and the j[th] channel of
**_A. Then, the following holds:_** **_U(i,j)_** _i=1_ **_Kij[i]_** 1[, where][ K][ij][ means the two-dimensional]
_∞_ [=][ P][h]
tensor obtained by the third dimension of K takes j and the fourth dimension of K takes i.

_n×m×r_

**_U_** = maxi[n]=1[×][m][×][o] _u(i,j)_
_∥_ _∥∞_

_j=1_

X

= maxo−1 maxn×m _r−1_ _n×m_ _u(i+nml,j+nms)_
_l=0_ _i=1_

_s=0_ _j=1_

X X

maxo−1 _r−1(maxn×m_ _n×m_ _u(i+nml,j+nms)_ )
_≤_ _l=0_ _i=1_

_s=0_ _j=1_

X X

= maxo−1 _r−1_ **_U(l+1,s+1)_**
_l=0_ _s=0_ _∞_

Xr

_o_
= max **_U(l,s)_**
_l=1_ _s=1_ _∞_

X

_r_ _h_

_o_
= maxl=1 **_Kls[i]_** 1

_s=1_ _i=1_

X X

_o_
max **_K[ˆ]_** _[l]_
_≤_ _l=1_ 1

= **_K[ˆ]_**
_∞_ _[.]_


-----

th

Similar to Eq. (15): for every nm rows, we choose _[k][+1]2_ row. Then its 1-norm is equal to this nm

rows of the **_K[ˆ]_** ’s ∞-norm. So the equation holds.

H AUXILIARY NUMERICAL EVIDENCE

A number of additional experiments are conducted in this section. Unless explicitly mentioned
otherwise, the experimental setup remains similar to the one in the main paper. The following
experiments are conducted below:

1. The difference between the theoretical and the algorithmic bound and their evolution during
training is studied in Appendix H.1.

2. An ablation study on the hidden size is conducted in Appendix H.2.

3. An ablation study is conducted on the effect of adversarial steps in Appendix H.3.

4. We evaluate the effect of the proposed projection into the testset performance in Appendix H.4.

5. We conduct experiments on four new datasets, i.e., MNIST, K-MNIST, E-MNIST-BY,
NSYNTH in Appendix H.5. These experiments are conducted in addition to the datasets
already presented in the main paper.

6. In Appendix H.6 experiments on three additional adversarial attacks, i.e., FGSM-0.01,
APGDT and TPGD, are performed.

7. We conduct an experiment using the NCP model in Appendix H.7.

8. The layer-wise bound (instead of a single bound for all matrices) is explored in Appendix H.8.

9. The comparison with adversarial defense methods is conducted in Appendix H.9.

H.1 THEORETICAL AND ALGORITHMIC BOUND

As mentioned in Section 3, projecting the quantity θ = _i=1[U][i]_

_•[k]_ _∞_ [onto their level set corresponds]
to a difficult non-convex problem. Given that we have an upper bound

_θ =_ _i=1[U][i]_ _i=1[∥][U][i][∥][∞]_ [=:][ γ .]

_•[k]_ _∞_ _[≤]_ [Π][k]

we want to understand in practice how tight is this bound. In Fig. 4 we compute the ratio _[γ]θ_ [for PN-4.]

In Fig. 5 the ratio is illustrated for randomly initialized matrices (i.e., untrained networks).

H.2 ABLATION STUDY ON THE HIDDEN SIZE

Initially, we explore the effect of the hidden rank of PN-4 and PN-10 on Fashion-MNIST. Fig. 6
exhibits the accuracy on both the training and the test-set for both models. We observe that PN-10
has a better accuracy on the training set, however the accuracy on the test set is the same in the two
models. We also note that increasing the hidden rank improves the accuracy on the training set, but
not on the test set.

H.3 ABLATION STUDY ON THE EFFECT OF ADVERSARIAL STEPS

Our next experiment scrutinizes the effect of the number of adversarial steps on the robust accuracy.
We consider in all cases a projection bound of 1, which provides the best empirical results. We
vary the number of adversarial steps and report the accuracy in Fig. 7. The results exhibit a similar
performance both in terms of the dataset (i.e., Fashion-MNIST and K-MNIST) and in terms of
the network (PN-4 and PN-Conv). Notice that when the adversarial attack has more than 10 steps
the performance does not vary significantly from the performance at 10 steps, indicating that the
projection bound is effective for stronger adversarial attacks.


-----

PN-4 Fashion-MNIST


PN-4 Fashion-MNIST


1.12

1.10

1.08

1.06

1.04

1.02

1.00

1.200

1.175

1.150

1.125

1.100

1.075

1.050

1.025

1.000


58000

56000

54000

52000

50000

48000

46000

44000

42000

52000

50000

48000

46000

44000

42000


0 2 4 6 8 10 12

Log of bound for each matrix in regularization

(a)


10 20 30 40 50

Epochs

(b)


PN-4 K-MNIST


PN-4 K-MNIST


0 2 4 6 8 10 12

Log of bound for each matrix in regularization

(c)


10 20 30 40 50

Epochs

(d)


Figure 4: Visualization of the difference between the bound results on Fashion-MNIST (top row) and

_k_

on K-MNIST (bottom row). Specifically, in (a) and (c) we visualize the ratio _[γ]θ_ [=] Qi=1i=1[∥][U][U][i][i][∥][∥][∞] for

_∥[•][k]_ _∞_

different log bound values for PN-4. In (b), (d) the exact values of the two bounds are computed over
the course of the unregularized training. Notice that there is a gap between the two bounds, however
importantly the two bounds are increasing at the same rate, while their ratio is close to 1.


PN-10


hidden rank = 16


1.9

1.8

1.7

1.6

1.5

1.4

1.3

1.2


1.6

1.5

1.4

1.3

1.2

1.1

1.0


10 12 14


Log of hidden rank

(a)


Depth

(b)


_k_

Figure 5: Visualization of the ratio _i=1_ _[∥][U][i][∥][∞]_ in a randomly initialized network (i.e., using normal

Q _i=1[U][i][∥]_

_∥[•][k]_ _∞_

distribution random matrices). Specifically, in (a) we visualize the ratio for different log hidden rank
values for PN-10. In (b) we visualize the ratio for different depth values for hidden rank = 16. Neither
of two plots contain any regularization.


-----

100

90

80

70

60

50


PNN-10-Test
PNN-10-Train
PNN-4-Test
PNN-4-Train


Log of Hidden size

Figure 6: Accuracy of PN-4 and PN-10 when the hidden rank varies (plotted in log-scale).



PN-4 PN-Conv


Ablation study on the effect of adversarial steps

5 10 15 20 25 30

PGD attack steps


Ablation study on the effect of adversarial steps

5 10 15 20 25 30

PGD attack steps


70

65

60

55

50

45

40


70

65

60

55

50

45

40

35

30


(a) Fashion-MNIST


(b) K-MNIST


Figure 7: Ablation study on the effect of adversarial steps in Fashion-MNIST and K-MNIST. All
methods are run by considering a projection bound of 1.


-----

H.4 EVALUATION OF THE ACCURACY OF PNS

In this experiment, we evaluate the accuracy of PNs. We consider three networks, i.e., PN-4, PN-10
and PN-Conv, and train them under varying projection bounds using Algorithm 1. Each model is
evaluated on the test set of (a) Fashion-MNIST and (b) E-MNIST.

The accuracy of each method is reported in Fig. 8, where the x-axis is plotted in log-scale (natural
logarithm). The accuracy is better for bounds larger than 2 (in the log-axis) when compared to tighter
bounds (i.e., values less than 0). Very tight bounds stifle the ability of the network to learn, which
explains the decreased accuracy. Interestingly, PN-4 reaches similar accuracy to PN-10 and PN-Conv
in Fashion-MNIST as the bound increases, while in E-MNIST it cannot reach the same performance
as the bound increases. The best bounds for all three models are observed in the intermediate values,
i.e., in the region of 1 in the log-axis for PN-4 and PN-10.



PN-4 PN-10 PN-Conv


Fashion-MNIST

2 4

Log of Bound


E-MNIST

2 4

Log of Bound


100

80


60

40

20


Figure 8: Accuracy of PN-4, PN-10 and PN-Conv under varying projection bounds (x-axis in
log-scale) learned on (a) Fashion-MNIST, (b) E-MNIST. Notice that the performance increases for
intermediate values, while it deteriorates when the bound is very tight.

We scrutinize further the projection bounds by training the same models only with cross-entropy loss
(i.e., no bound regularization). In Table 10, we include the accuracy of the three networks with and
without projection. Note that projection consistently improves the accuracy, particularly in the case
of larger networks, i.e., PN-10.


PN-4 PN-10 PN-Conv
Method

_Fashion-MNIST_

No projection 87.28 ± 0.18% 88.48 ± 0.17% 86.36 ± 0.21%
Projection 87.32 ± 0.14% 88.72 ± 0.12% 86.38 ± 0.26%

_E-MNIST_

No projection 84.27 ± 0.26% 89.31 ± 0.09% 91.49 ± 0.29%
Projection 84.34 ± 0.31% 90.56 ± 0.10% 91.57 ± 0.19%

Table 10: The accuracy of different PN models on Fashion-MNIST (top) and E-MNIST (bottom)
when trained only with SGD (first row) and when trained with projection (last row).


-----

H.5 EXPERIMENTAL RESULTS ON ADDITIONAL DATASETS

To validate even further we experiment with additional datasets. We describe the datasets below
and then present the robust accuracy in each case. The experimental setup remains the same as in
Section 4.2 in the main paper. As a reminder, we are evaluating the robustness of the different models
under adversarial noise.

**Dataset details: There are six datasets used in this work:**

1. Fashion-MNIST (Xiao et al., 2017) includes grayscale images of clothing. The training set
consists of 60, 000 examples, and the test set of 10, 000 examples. The resolution of each
image is 28 × 28, with each image belonging to one of the 10 classes.

2. E-MNIST (Cohen et al., 2017) includes handwritten character and digit images with a
training set of 124, 800 examples, and a test set of 20, 800 examples. The resolution of each
image is 28 × 28. E-MNIST includes 26 classes. We also use the variant EMNIST-BY that
includes 62 classes with 697, 932 examples for training and 116, 323 examples for testing.

3. K-MNIST (Clanuwat et al., 2018) depicts grayscale images of Hiragana characters with a
training set of 60, 000examples, and a test set of 10, 000 examples. The resolution of each
image is 28 × 28. K-MNIST has 10 classes.

4. MNIST (Lecun et al., 1998) includes handwritten digits images. MNIST has a training set
of 60, 000 examples, and a test set of 10, 000 examples. The resolution of each image is
28 × 28.

5. CIFAR-10 (Krizhevsky et al., 2014) depicts images of natural scenes. CIFAR-10 has a
training set of 50, 000 examples, and a test set of 10, 000 examples. The resolution of each
RGB image is 32 × 32.

6. NSYNTH (Engel et al., 2017) is an audio dataset containing 305, 979 musical notes, each
with a unique pitch, timbre, and envelope.


We provide a visualization[3] of indicative samples from MNIST, Fashion-MNIST, K-MNIST and
E-MNIST in Fig. 9.

We originally train PN-4, PN-10 and PN-Conv without projection bounds. The results are reported
in Table 11 (columns titled ‘No proj’) for MNIST and K-MNIST, Table 13 (columns titled ‘No
proj’) for E-MNIST-BY and Table 14 (columns titled ‘No proj’) for NSYNTH. Next, we consider
the performance under varying projection bounds; the accuracy in each case is depicted in Fig. 10
for K-MNIST, MNIST and E-MNIST-BY and Fig. 11 for NSYNTH. The figures (and the tables)
depict the same patterns that emerged in the two main experiments, i.e., the performance can be vastly
improved for intermediate values of the projection bound. Similarly, we validate the performance
when using adversarial training. The results in Table 12 demonstrate the benefits of using projection
bounds even in the case of adversarial training.

H.6 EXPERIMENTAL RESULTS OF MORE TYPES OF ATTACKS

To further verify the results of the main paper, we conduct experiments with three additional adversarial attacks: a) FGSM with ϵ = 0.01, b) Projected Gradient Descent in Trades (TPGD) (Zhang et al.,
2019), c) Targeted Auto-Projected Gradient Descent (APGDT) (Croce and Hein, 2020). In TPGD
and APGDT, we use the default parameters for a one-step attack.

The quantitative results are reported in Table 15 for four datasets and the curves of Fashion-MNIST
and E-MNIST are visualized in Fig. 12 and the curves of K-MNIST and MNIST are visualized in
Fig. 13. The results in both cases remain similar to the attacks in the main paper, i.e., the proposed
projection improves the performance consistently across attacks, types of networks and adversarial
attacks.

H.7 EXPERIMENTAL RESULTS IN NCP MODEL

To complement, the results of the CCP model, we conduct an experiment using the NCP model. That
is, we use a 4[th] degree polynomial expansion, called NCP-4, for our experiment. We conduct an

[3The samples were found in https://www.tensorflow.org/datasets/catalog.](https://www.tensorflow.org/datasets/catalog)


-----

(a) MNIST (b) Fashion-MNIST

(c) K-MNIST (d) E-MNIST

Figure 9: Samples from the datasets used for the numerical evidence. Below each image, the class
name and the class number are denoted.


-----

Clean FGSM_0.1 PGD_0.1_0.01_20 PGD_0.3_0.03_20


PN-4


PN-10

2

Log of Bound


PN-Conv


100

80


60

40

20


Log of Bound

PN-4


Log of Bound

PN-Conv


(a) K-MNIST

PN-10


100

80


60

40


20

0

100


Log of Bound

PN-4


Log of Bound


Log of Bound

PN-Conv


(b) MNIST

PN-10


80

60

40


20


Log of Bound


Log of Bound


Log of Bound


(c) E-MNIST-BY

Figure 10: Adversarial attacks during testing on (a) K-MNIST (top), (b) MNIST (middle), (c)
E-MNIST-BY (bottom) with the x-axis is plotted in log-scale. Note that intermediate values of
projection bounds yield the highest accuracy. The patterns are consistent in all datasets and across
adversarial attacks.


-----

Clean FGSM_0.1 PGD_0.1_0.01_20 PGD_0.3_0.03_20

PN-4


100

80


60

40


20


Log of Bound

Figure 11: Adversarial attacks during testing on NSYNTH.


-----

Clean FGSM0.01 APGDT TPGD


PN-4


PN-10

2

Log of Bound


PN-Conv


100

80

60


40

20


Log of Bound

PN-4


Log of Bound

PN-Conv


(a) Fashion-MNIST

PN-10


100

80

60

40


20


Log of Bound


Log of Bound


Log of Bound


(b) E-MNIST

Figure 12: Three new adversarial attacks during testing on (a) Fashion-MNIST (top), (b) E-MNIST
(bottom ) with the x-axis is plotted in log-scale. Note that intermediate values of projection bounds
yield the highest accuracy. The patterns are consistent in both datasets and across adversarial attacks.


-----

Clean FGSM0.01 APGDT TPGD


PN-4


PN-10

2

Log of Bound


PN-Conv


100

80

60


40

20


Log of Bound

PN-4


Log of Bound

PN-Conv


(a) K-MNIST

PN-10


100

80

60

40


20


Log of Bound


Log of Bound


Log of Bound


(b) MNIST

Figure 13: Three new adversarial attacks during testing on (a) K-MNIST (top), (b) MNIST (bottom)
with the x-axis is plotted in log-scale. Note that intermediate values of projection bounds yield the
highest accuracy. The patterns are consistent in both datasets and across adversarial attacks.


-----

Method No proj. Our method Jacobian _L2_

_K-MNIST_

Clean 84.04 ± 0.30% **84.23 ± 0.30%** 83.16 ± 0.31% 84.18 ± 0.44%
FGSM-0.1 18.86 2.61% **35.84** **1.67%** 22.61 1.30% 22.05 2.76%

PN-4 _±_ **_±_** _±_ _±_

PGD-(0.1, 20, 0.01) 11.20 ± 4.27% **40.26 ± 1.36%** 16.00 ± 5.63% 10.93 ± 2.42%
PGD-(0.3, 20, 0.03) 1.94 ± 1.11% **24.75 ± 1.32%** 4.46 ± 2.59% 2.70 ± 1.26%

Clean 87.90 ± 0.24% **88.80 ± 0.19%** 88.73 ± 0.16% 87.93 ± 0.18%
FGSM-0.1 24.52 1.44% **41.83** **2.00%** 26.90 1.02% 26.62 1.59%

PN-10 _±_ **_±_** _±_ _±_

PGD-(0.1, 20, 0.01) 7.54 ± 0.79% **39.55 ± 0.64%** 11.50 ± 1.35% 5.09 ± 0.68%
PGD-(0.3, 20, 0.03) 0.05 ± 0.04% **25.24 ± 0.93%** 1.24 ± 0.64% 0.19 ± 0.12%

Clean 88.41 ± 0.37% 88.48 ± 0.42% 86.57 ± 0.46% 88.56 ± 0.62%
FGSM-0.1 13.34 2.01% **47.75** **2.03%** 14.16 3.05% 12.43 2.58%

PN-Conv _±_ **_±_** _±_ _±_

PGD-(0.1, 20, 0.01) 10.81 ± 1.25% **45.68 ± 3.11%** 12.05 ± 0.82% 11.05 ± 0.85%
PGD-(0.3, 20, 0.03) 6.91 ± 2.04% **31.68 ± 1.43%** 7.54 ± 1.39% 6.28 ± 2.37%

_MNIST_

Clean 96.52 ± 0.13% **96.62 ± 0.17%** 95.88 ± 0.16% 96.44 ± 0.18%
FGSM-0.1 20.96 5.16% **64.09** **2.41%** 33.59 8.46% 26.07 5.64%

PN-4 _±_ **_±_** _±_ _±_

PGD-(0.1, 20, 0.01) 14.23 ± 5.39% **66.05 ± 7.06%** 20.83 ± 5.64% 16.06 ± 5.84%
PGD-(0.3, 20, 0.03) 2.59 ± 2.01% **51.47 ± 3.17%** 4.92 ± 1.18% 4.26 ± 2.44%

Clean 97.46 ± 0.11% **97.63 ± 0.06%** 97.36 ± 0.05% 97.53 ± 0.10%
FGSM-0.1 30.12 4.58% **70.02** **1.28%** 40.22 2.31% 28.77 2.41%

PN-10 _±_ **_±_** _±_ _±_

PGD-(0.1, 20, 0.01) 9.70 ± 2.11% **73.57 ± 1.17%** 18.74 ± 5.39% 10.91 ± 2.32%
PGD-(0.3, 20, 0.03) 0.47 ± 0.53% **55.36 ± 2.32%** 2.49 ± 1.46% 0.44 ± 0.29%

Clean 98.32 ± 0.12% **98.40 ± 0.12%** 97.88 ± 0.12% 98.32 ± 0.11%
FGSM-0.1 18.98 2.99% **67.50** **6.22%** 27.02 9.88% 23.77 5.58%

PN-Conv _±_ **_±_** _±_ _±_

PGD-(0.1, 20, 0.01) 12.57 ± 2.81% 72.85 ± 12.23% 13.96 ± 2.57% 13.84 ± 3.18%
PGD-(0.3, 20, 0.03) 10.57 ± 4.08% **55.56 ± 8.48%** 10.22 ± 0.52% 9.10 ± 3.62%

Table 11: Comparison of regularization techniques on K-MNIST (top) and MNIST (bottom). In each
dataset, the base networks are PN-4, i.e., a 4[th] degree polynomial, on the top four rows, PN-10, i.e., a
10[th] degree polynomial, on the middle four rows and PN-Conv, i.e., a 4[th] degree polynomial with
convolutions, on the bottom four rows. Our projection method exhibits the best performance in all
three attacks, with the difference on accuracy to stronger attacks being substantial.

AT Our method + AT Jacobian + AT _L2 + AT_
Method
Adversarial training (AT) with PN-10 on K-MNIST

FGSM-0.1 70.93 ± 0.46% **71.14 ± 0.30%** 64.48 ± 0.51% 70.90 ± 0.57%
PGD-(0.1, 20, 0.01) 60.94 ± 0.71% 61.20 ± 0.39% 57.89 ± 0.31% **61.47 ± 0.44%**
PGD-(0.3, 20, 0.03) 30.77 ± 0.26% **33.07 ± 0.58%** 29.96 ± 0.21% 30.35 ± 0.42%

Adversarial training (AT) with PN-10 on MNIST

FGSM-0.1 91.89 ± 0.30% 91.94 ± 0.17% 87.85 ± 0.27% **92.22 ± 0.30%**
PGD-(0.1, 20, 0.01) 87.36 ± 0.29% **87.38 ± 0.37%** 84.96 ± 0.25% 87.26 ± 0.49%
PGD-(0.3, 20, 0.03) 61.96 ± 0.92% **63.96 ± 1.02%** 62.24 ± 0.24% 62.44 ± 0.76%


Table 12: Comparison of regularization techniques on (a) K-MNIST (top) and (b) MNIST (bottom)
along with adversarial training (AT). The base network is a PN-10, i.e., 10[th] degree polynomial. Our
projection method exhibits the best performance in all three attacks.

experiment in the K-MNIST dataset and present the result with varying bound in Fig. 14. Notice that
the patterns remain similar to the CCP model, i.e., intermediate values of the projection bound can
increase the performance significantly.

H.8 LAYER-WISE BOUND

To assess the flexibility of the proposed method, we assess the performance of the layer-wise bound. In
the previous sections, we have considered using a single bound for all the matrices, i.e., ∥Ui∥∞ _≤_ _λ,_
because the projection for a single matrix has efficient projection algorithms. However, Lemma 1
enables each matrix Ui to have a different bound λi. We assess the performance of having different
bounds for each matrix Ui.


-----

PN-4, PN-10 and PN-Conv on E-MNIST-BY
Method

No proj. Our method

Clean 80.18 ± 0.19% **80.26 ± 0.17%**
FGSM-0.1 3.65 ± 0.76% **16.58 ± 3.87%**
PGD-(0.1, 20, 0.01) 4.57 ± 1.98% **19.77 ± 4.42%**
PGD-(0.3, 20, 0.03) 0.59 ± 0.40% **10.13 ± 2.08%**

Clean 84.17 ± 0.06% **85.32 ± 0.04%**
FGSM-0.1 11.67 ± 1.21% **32.37 ± 2.58%**
PGD-(0.1, 20, 0.01) 2.48 ± 0.66% **31.22 ± 2.32%**
PGD-(0.3, 20, 0.03) 0.03 ± 0.05% **13.74 ± 0.77%**

Clean 85.92 ± 0.08% **86.03 ± 0.08%**
FGSM-0.1 0.65 ± 0.17% **29.07 ± 2.72%**
PGD-(0.1, 20, 0.01) 1.57 ± 1.40% **31.06 ± 4.70%**
PGD-(0.3, 20, 0.03) 0.33 ± 0.06% **23.93 ± 6.32%**


PN-4

PN-10

PN-Conv


Table 13: Comparison of regularization techniques on E-MNIST-BY. The base network are PN-4,
i.e., 4[th] degree polynomial, on the top four rows, PN-10, i.e., 10[th] degree polynomial, on the middle
four rows and PN-Conv, i.e., a 4[th] degree polynomial with convolution, on the bottom four rows. Our
projection method exhibits the best performance in all three attacks, with the difference on accuracy
to stronger attacks being substantial.

Model PN-4
Projection No-proj Proj

Clean accuracy 80.25 ± 0.27% **80.33 ± 0.26%**
FGSM-0.1 0.91 ± 0.14% **22.25 ± 0.04%**
PGD-(0.1, 20, 0.01) 0.31 ± 0.11% **22.27 ± 0.00%**
PGD-(0.3, 20, 0.03) 0.46 ± 0.29% **20.38 ± 2.30%**

Table 14: Evaluation of the robustness of PN models on NSYNTH. Each line refers to a different
adversarial attack. The projection offers an improvement in the accuracy in each case; in PGD attacks
projection improves the accuracy by a remarkable margin.


-----

Clean FGSM_0.1 PGD_0.1_0.01_20 PGD_0.3_0.03_20

### K-MNIST


100

80


60

40


20


Log of Bound

Figure 14: Experimental result of K-MNIST in NCP model.


-----

No proj. Our method
Method

_Fashion-MNIST_

FGSM-0.01 26.49 ± 3.13% **58.09 ± 1.63%**
PN-4 APGDT 16.59 ± 4.35% **50.83 ± 1.55%**
TPGD 26.88 ± 6.78% **59.03 ± 1.45%**

FGSM-0.01 18.59 ± 1.82% **60.56 ± 1.06%**
PN-10 APGDT 8.76 ± 1.14% **51.93 ± 1.91%**
TPGD 14.53 ± 1.49% **63.33 ± 0.51%**

FGSM-0.01 15.30 ± 3.10% **55.90 ± 2.60%**
PN-Conv APGDT 11.88 ± 1.33% **53.49 ± 0.72%**
TPGD 14.50 ± 1.59% **58.72 ± 1.87%**

_E-MNIST_

FGSM-0.01 13.40 ± 5.16% **32.83 ± 2.08%**
PN-4 APGDT 9.33 ± 4.00% **26.38 ± 2.70%**
TPGD 17.40 ± 3.11% **34.68 ± 1.92%**

FGSM-0.01 14.47 ± 1.80% **48.28 ± 3.06%**
PN-10 APGDT 10.13 ± 0.93% **41.72 ± 4.05%**
TPGD 13.97 ± 0.88% **47.44 ± 3.62%**

FGSM-0.01 4.71 ± 1.10% **39.37 ± 5.43%**
PN-Conv APGDT 3.58 ± 0.66% **30.43 ± 4.87%**
TPGD 4.08 ± 0.33% **35.85 ± 10.20%**

_K-MNIST_

FGSM-0.01 23.31 ± 5.34% **43.74 ± 5.97%**
PN-4 APGDT 17.02 ± 6.97% **39.43 ± 1.89%**
TPGD 23.45 ± 7.67% **48.46 ± 3.84%**

FGSM-0.01 26.87 ± 2.14% **50.99 ± 3.52%**
PN-10 APGDT 16.23 ± 1.32% **41.46 ± 3.85%**
TPGD 22.63 ± 0.99% **49.91 ± 1.37%**

FGSM-0.01 12.31 ± 2.03% **52.58 ± 6.80%**
PN-Conv APGDT 13.47 ± 2.19% **42.94 ± 1.68%**
TPGD 14.25 ± 2.51% **48.19 ± 3.02%**

_MNIST_

FGSM-0.01 34.14 ± 7.63% **73.95 ± 5.18%**
PN-4 APGDT 29.88 ± 9.47% **71.26 ± 4.88%**
TPGD 27.01 ± 9.77% **76.88 ± 1.98%**

FGSM-0.01 32.34 ± 4.67% **78.83 ± 1.63%**
PN-10 APGDT 19.55 ± 1.72% **75.22 ± 2.05%**
TPGD 28.11 ± 3.87% **79.74 ± 2.07%**

FGSM-0.01 22.73 ± 3.10% **69.83 ± 8.91%**
PN-Conv APGDT 17.95 ± 3.39% **64.94 ± 8.96%**
TPGD 21.82 ± 3.07% 66.47 ± 11.83%

Table 15: Evaluation of the robustness of PN models on four datasets with three new types of attacks.
Each line refers to a different adversarial attack. The projection offers an improvement in the accuracy
in each case.

We experiment on PN-4 that we set a different projection bound for each matrix Ui. Specifically,
we use five different candidate values for each λi and then perform the grid search on the FashionMNIST FGSM-0.01 attack. The results on Fashion-MNIST in Table 16 exhibit how the layer-wise
bounds outperform the previously used single bound[4]. The best performing values for PN-4 are
_λ1 = 1.5, λ2 = 2, λ3 = 1.5, λ4 = 2, µ = 0.8. The values of λi in the first few layers are larger,_
while the value in the output matrix C is tighter.

To scrutinize the results even further, we evaluate whether the best performing λi can improve the
performance in different datasets and the FGSM-0.1 attack. In both cases, the best performing λi can
improve the performance of the single bound.

4The single bound is mentioned as ‘Our method’ in the previous tables. In this experiment both ‘single
bound’ and ‘layer-wise bound’ are proposed.


-----

Method No proj. Jacobian _L2_ Single bound Layer-wise bound

_Fashion-MNIST_

FGSM-0.01 26.49 ± 3.13% 39.88 ± 4.59% 24.36 ± 1.95% 58.09 ± 1.63% **63.95 ± 1.26%**
FGSM-0.1 12.92 ± 2.74% 17.90 ± 6.51% 13.80 ± 3.65% 46.43 ± 0.95% **55.14 ± 3.65%**

_K-MNIST_

FGSM-0.01 23.31 ± 5.34% 25.46 ± 3.51% 27.85 ± 7.62% 43.74 ± 5.97% **49.61 ± 1.44%**
FGSM-0.1 18.86 ± 2.61% 22.61 ± 1.30% 22.05 ± 2.76% 35.84 ± 1.67% **47.54 ± 3.74%**

_MNIST_

FGSM-0.01 34.14 ± 7.63% 32.78 ± 6.94% 29.31 ± 3.95% 73.95 ± 5.18% **79.23 ± 3.65%**
FGSM-0.1 20.96 ± 5.16% 33.59 ± 8.46% 26.07 ± 5.64% 64.09 ± 2.41% **74.97 ± 5.60%**

Table 16: Evaluation of our layer-wise bound versus our single bound. To avoid confusion with
previous results, note that ’single bound’ corresponds to ’Our method’ in the rest of the tables in this
work. The different λi values are optimized on Fashion-MNIST FGSM-0.01 attack. Then, the same
_λi values are used for training the rest of the methods. The proposed layer-wise bound outperforms_
the single bound by a large margin, improving even further by baseline regularization schemes.

No proj. Single bound Layer-wise bound Gaussian denoising Median denoising Guided denoising
Method

_Fashion-MNIST_

FGSM-0.01FGSM-0.1 2612..4992 ± ± 3 2..13% 5874% 46..0943 ± ± 1 0..63%95% **6355..9514 ± ± 1 3..2665%%** 1814..8014 ± ± 3 2..08%77% 1914..6802 ± ± 3 1..20%95% 2922..6994 ± ± 5 5..37%65%

Table 17: Comparison of the proposed method against adversarial defense methods on feature
denoising (Xie et al., 2019) and guided denoising (Liao et al., 2018). Notice that the single bound (cf.
Appendix H.8 for details) already outperforms the proposed defense methods, while the layer-wise
bounds further improves upon our single bound case.

H.9 ADVERSARIAL DEFENSE METHOD

One frequent method used against adversarial perturbations are the so called adversarial defense
methods. We assess the performance of adversarial defense methods on the PNs when compared with
the proposed method.

We experiment on PN-4 in Fashion-MNIST. We chose three different methods: gaussian denoising,
median denoising and guided denoising (Liao et al., 2018). Gaussian denoising and median denoising
are the methods of using gaussian filter and median filter for feature denoising (Xie et al., 2019).
The results in Table 17 show that in both attacks our method performs favourably to the adversarial
defense methods.


-----

