# THE CONVEX GEOMETRY OF BACKPROPAGATION: NEURAL NETWORK GRADIENT FLOWS CONVERGE TO EXTREME POINTS OF THE DUAL CONVEX PROGRAM


**Yifei Wang**
Department of Electrical Engineering
Stanford University
Stanford, CA 94305, USA
wangyf18@stanford.edu


**Mert Pilanci**
Department of Electrical Engineering
Stanford University
Stanford, CA 94305, USA
pilanci@stanford.edu

ABSTRACT


We study non-convex subgradient flows for training two-layer ReLU neural networks from a convex geometry and duality perspective. We characterize the implicit
bias of unregularized non-convex gradient flow as convex regularization of an equivalent convex model. We then show that the limit points of non-convex subgradient
flows can be identified via primal-dual correspondence in this convex optimization
problem. Moreover, we derive a sufficient condition on the dual variables which
ensures that the stationary points of the non-convex objective are the KKT points
of the convex objective, thus proving convergence of non-convex gradient flows
to the global optimum. For a class of regular training data distributions such as
orthogonal separable data, we show that this sufficient condition holds. Therefore,
non-convex gradient flows converge to optimal solutions of a convex optimization
problem. We present numerical results verifying the predictions of our theory for
non-convex subgradient descent.

1 INTRODUCTION

Neural networks (NNs) exhibit remarkable empirical performance in various machine learning tasks.
However, a full characterization of the optimization and generalization properties of NNs is far from
complete. Non-linear operations inherent to the structure of NNs, over-parameterization and the
associated highly nonconvex training problem makes their theoretical analysis quite challenging.

In over-parameterized models such as NNs, one natural question arises: Which particular solution does
gradient descent/gradient flow find in unregularized NN training problems? Suppose that X ∈ R[N] _[×][d]_
is the training data matrix and y ∈{1, −1}[N] is the label vector. For linear classification problems
such as logistic regression, it is known that gradient descent (GD) exhibits implicit regularization
properties, see, e.g., (Soudry et al., 2018; Gunasekar et al., 2018). To be precise, under certain
assumptions, GD converges to the following solution which maximizes the margin:

1
arg min 2[,][ s.t.][ y][n][w][T][ x][n] (1)
**w∈R[d]** 2 _[∥][w][∥][2]_ _[≥]_ [1][, n][ ∈] [[][N] []][.]

Here we denote [N ] = {1, . . ., N _}. Recently, there are several results on the implicit regularization_
of the (stochastic) gradient descent method for NNs. In (Lyu & Li, 2019), for the multi-layer
homogeneous network with exponential or cross-entropy loss, with separable training data, it is
shown that the gradient flow (GF) and GD finds a stationary point of the following non-convex
max-margin problem:


1
2[,][ s.t.][ y][n][f] [(][θ][;][ x][n][)][ ≥] [1][, n][ ∈] [[][N] []][,] (2)
2 _[∥][θ][∥][2]_


arg min


where f (θ; x) represents the output of the neural network with parameter θ given input x. In (Phuong
& Lampert, 2021), by further assuming the orthogonal separability of the training data, it is shown
that all neurons converge to one of the two max-margin classifiers. One corresponds to the data with
positive labels, while the other corresponds to the data with negative labels.


-----

However, as the max-margin problem of the neural network (2) is a non-convex optimization problem,
the existing results only guarantee that it is a stationary point of (2), which can be a local minimizer
or even a saddle point. In other words, the global optimality is not guaranteed.

In a different line of work (Pilanci & Ergen, 2020; Ergen & Pilanci, 2020; 2021b), exact convex
optimization formulations of two and three-layer ReLU NNs are developed, which have global
optimality guarantees in polynomial-time when the data has a polynomial number of hyperplane
arrangements, e.g., in any fixed dimension or with convolutional networks of fixed filter size. The
convex optimization framework was extended to vector output networks (Sahiner et al., 2021b),
quantized networks (Bartan & Pilanci, 2021b), autoencoders (Sahiner et al., 2021c; Gupta et al.,
2021), networks with polynomial activation functions (Bartan & Pilanci, 2021a), networks with batch
normalization (Ergen et al., 2021), univariate deep ReLU networks, deep linear networks (Ergen &
Pilanci, 2021c) and Generative Adversarial Networks (Sahiner et al., 2021a).

In this work, we first derive an equivalent convex program corresponding to the maximal margin
problem (2). We then consider non-convex subgradient flow for unregularized logistic loss. We show
that the limit points of non-convex subgradient flow can be identified via primal-dual correspondence
in the convex optimization problem. We then present a sufficient condition on the dual variable to
ensure that all stationary points of the non-convex max-margin problem are KKT points of the convex
max-margin problem. For certain regular datasets including orthogonal separable data, we show that
this sufficient condition on the dual variable holds, thus implies the convergence of gradient flow
on the unregularized problem to the global optimum of the non-convex maximalo margin problem
(2). Consequently, this enables us to fully characterize the implicit regularization of unregularized
gradient flow or gradient descent as convex regularization applied to a convex model.

1.1 RELATED WORK

There are several works studying the property of two-layer ReLU networks trained by gradient
descent/gradient flow dynamics. The following papers study the gradient descent like dynamics in
training two-layer ReLU networks for regression problems. Ma et al. (2020) show that for two-layer
ReLU networks, only a group of a few activated neurons dominate the dynamics of gradient descent.
In (Mei et al., 2018), the limiting dynamics of stochastic gradient descent (SGD) is captured by
the distributional dynamics from a mean-field perspective and they utlize this to prove a general
convergence result for noisy SGD. Li et al. (2020) focus on the case where the weights of the second
layer are non-negative and they show that the over-parameterized neural network can learn the
ground-truth network in polynomial time with polynomial samples. In (Zhou et al., 2021), it is shown
that mildly over-parameterized student network can learn the teacher network and all student neurons
converge to one of the teacher neurons.

Beyond (Lyu & Li, 2019) and (Phuong & Lampert, 2021), the following papers study the classification
problems. In (Chizat & Bach, 2018), under certain assumptions on the training problem, with overparameterized model, the gradient flow can converge to the global optimum of the training problem.
For linear separable data, utilizing the hinge loss for classification, Wang et al. (2019) introduce a
perturbed stochastic gradient method and show that it can attain the global optimum of the training
problem. Similarly, for linear separable data, Yang et al. (2021) introduce a modified loss based on
the hinge loss to enable (stochastic) gradient descent find the global minimum of the training problem,
which is also globally optimal for the training problem with the hinge loss.

1.2 PROBLEM SETTING

We focus on two-layer neural networks with ReLU activation, i.e., f (θ, X) = (XW1)+w2, where
**Wthis neural network is homogeneous, i.e., for any scalar1 ∈** R[d][×][m], w2 ∈ R[m] and θ = (W1, w2) represents the parameter. Due to the ReLU activation, c > 0, we have f (cθ; X) = c[2]f (θ; X). The
training problem is given by


_ℓ(ynf_ (θ; xn)), (3)
_n=1_

X


min


where ℓ(q) : R → R+ is the loss function. We focus on the logistic, i.e, cross-entropy loss, i.e.,
_ℓ(q) = log(1 + exp(−q))._


-----

Then, we briefly review gradient descent and gradient flow. The gradient descent takes the update rule

**_θ(t + 1) = θ(t) −_** _η(t)g(t),_

where g(t) ∈ _∂[◦]L(θ(t)) and ∂[◦]_ represents the Clarke’s subdifferential.

The gradient flow can be viewed as the gradient descent with infinitesimal step size. The trajectory
of the parameter θ during training is an arc θ : [0, + ) Θ, where Θ = **_θ = (W1, w2)_** **W1**
R[d][×][m], W2 ∈ R[m]}. More precisely, the gradient flow is given by the differential inclusion∞ _→_ _{_ _|_ _∈_
_d_

for t 0, almost everywhere.
_dt_ **_[θ][(][t][)][ ∈−][∂][◦][L][(][θ][(][t][))][,]_** _≥_

2 MAIN RESULTS


In this section, we present our main results and defer the detailed analysis to the following sections.
Consider the more general multi-class version of the problem with K classes. Suppose that ¯y ∈ [K][N]
is the label vector. Let Y = (yn,k)n [N ],k [K] R[N] _[×][K]_ be the encoded label matrix such that
_∈_ _∈_ _∈_

1, if ¯yn = k,
_yn,k =_
1, otherwise.
 _−_

Similarly, we consider the following two-layer vector-output neural networks with ReLU activation:

_f1(θ1, X)_ (XW1[(1)][)][+][w]2[(1)]
. .

_F_ (Θ, X) =  ..  =  ..  _,_

fK(θK, X) (XW1[(][K][)])+w2[(][K][)]
   

where we write Θ = (θ1, . . ., θK). For k = 1, . . ., K, we have θk = (W1[(][k][)][,][ w]2[(][k][)][)][ where]
**W1[(][k][)]** R[N] _[×][m]_ and w2[(][k][)] R[m]. One can view each of the K outputs of F (Θ, X) as the output of a
_∈_ _∈_
two-layer scalar-output neural network. Consider the following training problem:


_ℓ(yn,kfk(θk, xn))._ (4)
_n=1_

X


min


_k=1_


According to (Lyu & Li, 2019), the gradient flow and the gradient descent finds a stationary point of
the following non-convex max-margin problem:


1
2[,][ s.t.][ y][n,k][f] [(][θ][k][;][ x][n][)][ ≥] [1][, n][ ∈] [[][N] []][, k][ ∈] [[][K][]][.] (5)
2 _[∥][θ][k][∥][2]_


arg min
**Θ**

_k=1_

X


Denote the set of all possible hyperplane arrangement as

_P = {diag(I(Xw ≥_ 0))|w ∈ R[d]}, (6)

and let p = . We can also write = **D1, . . ., Dp** . From (Cover, 1965), we have an upper
_|P|_ _r_ _P_ _{_ _}_

bound p 2r _e(Nr−1)_ where r = rank(X). We first reformulate (5) as convex optimization.
_≤_

**Proposition 1 The non-convex problem**  (5) is equivalent to the following convex program


(∥uj,k∥2 + ∥u[′]j,k[∥][2][)][,]

_k=1_ _j=1_

X X


min


(7)


_j=1_ **DjX(uj,k −** **u[′]j,k[)][ ≥]** **[1][,]**

X


_s.t. diag(yk)_


(2Dj _I)Xuj,k_ 0, (2Dj _I)Xu[′]j,k_
_−_ _≥_ _−_ _[≥]_ [0][, j][ ∈] [[][p][]][, k][ ∈] [[][K][]][.]

_where yk is the k-th column of Y. The dual problem of (7) is given by_

max tr(Λ[T] **Y),**

(8)
_s.t. diag(yk)λk_ 0, max _k_ [(][X][T][ w][)][+][| ≤] [1][, k][ ∈] [[][K][]][.]
_⪰_ _∥w∥2≤1_ _[|][λ][T]_

_where λk is the k-th column of Λ._


-----

We present the detailed derivation of the convex formulation (7) and its dual problem (8) in the
appendix. Givendefine the cosine angle between u ∈ R[d], we define u and D v( byu) = cos diag ∠(u,( vI() =Xu >∥u∥u 0))2[T]∥vv∥. For two vectors2 [.] **u, v ∈** R[d], we

2.1 OUR CONTRIBUTIONS

The following theorem illustrate that for neurons satisfying sign(yk[T] [(][Xw]1[(][k],i[)][)][+][) =][ sign][(][w]2[(][k],i[)][)][ at]

initialization, w1[(][k],i[)] [align to the direction of][ ±][X][T][ D][(][w]1[(][k],i[)][)][y][k][ at a certain time][ T] [, depending on]

**sign(w2[(][k],i[)]k,+** [)][ at initialization. In Section 2.3, we show that these are dual extreme points of (7).]

**Theorem 1 Consider the K-class classification training problem (4) for any dataset. Suppose that**
_the neural network is scaled at initialization such that_ **w1[(][k],i[)]** 2,i
_∥_ _[∥][2][ =][ |][w][(][k][)][|][ for][ i][ ∈]_ [[][m][]][ and][ k][ ∈] [[][K][]][.]

_Assume that at initialization, for k ∈_ [K], there exists neurons (w1[(][k],i[)]k _[,][ w]2[(][k],i[)]k_ [)][such that]

**sign(yk[T]** [(][Xw]1[(][k],i[)]k [)][+][) =][ sign][(][w]2[(][k],i[)]k [) =][ s,] (9)

_where s ∈{1, −1}. Consider the subgradient flow applied to the non-convex problem (4). Let_
_δ ∈_ (0, 1). Suppose that the initialization is sufficiently close to the origin. Then, for k ∈ [K], there
_exist T = T_ (δ, k) such that

cos ∠ **w1[(][k],i[)]k** [(][T] [)][, s][X][T][ D][(][w]1[(][k],i[)]k [(][T] [))][y][k] _≥_ 1 − _δ._
 

Next, we impose conditions on the dataset to prove a stronger global convergence results on the flow.
We say that the dataset (X, ¯y) is orthogonal separable among multiple classes if for all n, n[′] _∈_ [N ],

**x[T]n** **[x][n][′][ >][ 0][,][ if][ ¯]yn = ¯yn′** _,_

**x[T]n** **[x][n][′][ ≤]** [0][,][ if][ ¯]yn ̸= ¯yn′ _._

For orthogonal separable dataset among multiple classes, the subgradient flow for the non-convex
problem (4) can find the global optimum of (5) up to a scaling constant.

**Theorem 2 Suppose that (X, ¯y) ∈** R[N] _[×][d]_ _× [K][N]_ _is orthogonal separable among multiple classes._
_Consider the non-convex subgradient flow applied to the non-convex problem (4). Suppose that the_
_initialization is sufficiently close to the origin and scaled as in Theorem 1. Then, the non-convex_
_subgradient flow converges to the global optimum of the convex program (7) and hence the non-convex_
_objective (5) up to scaling._

Therefore, the above result characterizes the implicit regularization of unregularized gradient flow as
_convex regularization, i.e., group ℓ1 norm, in the convex formulation (7). It is remarkable that group_
sparsity is enforced by small initialization magnitude with no explicit form of regularization.

2.2 CONVEX GEOMETRY OF NEURAL GRADIENT FLOW

Suppose that λ ∈ R[N] . Here we provide an interesting geometric interpretation behind the formula

cos ∠(u, X[T] **D(u)λ) > 1 −** _δ._

which describes a dual extreme point to which hidden neurons approach to as predicted by
Theorem 1. We now explain the geometric intuition behind this result. Consider an ellipsoid
**Xu :** **u** 2 1 . A positive extreme point of this ellipsoid along the direction λ is defined by
_{arg max ∥u : ∥∥u ≤∥2≤1 λ}_ _[T]_ **Xu, which is given by the formula** _∥XX[T][T]λλ∥2_ [. Next, we consider the rectified]

ellipsoid set Q := {(Xu)+ : ∥u∥2 ≤ 1} introduced in (Ergen & Pilanci, 2021a) and shown in
Figure 1. The constraintthe absolute polar set of Q max, which appears as a constraint in the convex programu:∥u∥2≤1 |λ[T] (Xu)+| ≤ 1 on λ is equivalent to λ (8) ∈Q and is defined as[∗]. Here Q[∗] is
the following convex set
= **_λ : max_** (10)
_Q[∗]_ _{_ **z**
_∈Q_ _[|][λ][T][ z][| ≤]_ [1][}][.]


-----

An extreme point of this non-convex body along the direction λ is given by the solution of the
problem

max max (11)
**u :** **u** 2 1 **_[λ][T][ (][Xu][)][+][ = max]Dj_** **u :** **u** 2 1,(2Dj _I)Xu_ 0 **_[λ][T][ D][j][Xu][.]_**
_∥_ _∥_ _≤_ _∈P_ _∥_ _∥_ _≤_ _−_ _≥_

Here, (λ, u) are primal-dual pairs as they appear in the convex dual program (8). First, note that a
stationary point of gradient flow on the objective in (11) is given by the identity cu _∂u[◦][λ][T][ (][Xu][)][+]_
_∈_
where c is a constant. In particular, by picking the zero as the subgradient of (x[T]n **[u][)][+]** [when][ x]n[T] **[u][ = 0][,]**

**X[T]** **D(u)λ** _Nn=1_ _[λ][n][x][n][I][(][u][T][ x][n][ >][ 0)]_
**u =** = _._ (12)

**X[T]** **D(u)λ** 2
_∥_ _∥_ _∥_ [P]Pn[N]=1 _[λ][n][x][n][I][(][u][T][ x][n][ >][ 0)][∥][2]_

Note that the formula cos ∠(u, X[T] **D(u)λ) > 1** _−_ _δ appearing in Theorem 1 shows that gradient flow_
reaches the extreme points of projected ellipsoids **DjXu :** **u** 2 1 in the direction of λ = yk,
where Dj corresponds to a valid hyperplane arrangement. This interesting phenomenon is { _∥_ _∥_ _≤_ _}_
depicted in Figures 3 and 4. The one-dimensional spikes in Figures 1 and 3 are projected ellipsoids. ∈P
Detailed setup for Figure 1 to 4 and additional experiments can be found in Appendix F.


2.0

optimal

y[T]u = 0

maximal

1.5 minimal

optimal (Xw1,[*] i[)][ +]

1.0

0.5

0.0

0.5

1.0

1.0 0.5 0.0 0.5 1.0 1.5 2.0

Figure 1: Rectified Ellipsoid Q := {(Xu)+ :
**u** 2 1 and its extreme points (spikes).
_∥_ _∥_ _≤_ _}_

2.00

optimal

y[T]u = 0

1.75

trained (Xw1, i) +

1.50 optimal (Xw1,[*] i[)][ +]

1.25

1.00

0.75

0.50

0.25

0.00

0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00


Figure 3: Trajectories of (X ˆw1,i)+ along the
training dynamics of gradient descent.


2.0

y

1.5 { : u : |maxu|2 1[|] T(Xu) + | 1}

{ : diag(y) 0}

optimal

1.0

0.5

0.0

0.5

1.0

1.5

2.0

2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0

Figure 2: Convex absolute polar set Q[∗] of
the Rectified Ellipsoid (purple) and other dual
constraints (grey).


1.0

cone boundary

0.5 1-th neuron

2-th neuron
3-th neuron
4-th neuron
5-th neuron

0.0

6-th neuron
7-th neuron
8-th neuron
9-th neuron

0.5 10-th neuron

optimal neuron

1.0

1.0 0.5 0.0 0.5 1.0


Figure 4: Trajectories of ˆw1,i = _∥ww11,i,i∥2_

along the training dynamics of gradient descent.


Figure 5: Two-layer ReLU network gradient descent dynamics on an orthogonal separable dataset.
**wˆ** 1,i = _∥ww11,i,i∥2_ [is the normalized vector of the][ i][-th hidden neuron in the first layer.]


-----

3 CONVEX MAX-MARGIN PROBLEM

In this section, we consider the equivalent convex model of the max-margin problem and its optimality
conditions. We primarily focus on the binary classification problem for simplicity, which are later
extended to the multi-class case. We can reformulate the nonconvex max-margin problem (2) as

min [1] _F_ [+][ ∥][w][2][∥]2[2][)][,][ s.t.][ Y][(][XW][1][)][+][w][2] (13)

2 [(][∥][W][1][∥][2] _[≥]_ **[1][,]**

where Y = diag(y). This is a nonconvex optimization problem due to the ReLU activation and
the two-layer structure of neural network. Analogous to the convex formulation introduced in
(Pilanci & Ergen, 2020) for regularized training problem of neural network, we can provide a convex
optimization formulation of (13) and derive the dual problem.
**Proposition 2 The problem (13) is equivalent to**


_Pcvx[∗]_ [= min]


(∥uj∥2 + ∥u[′]j[∥][2][)][,]
_j=1_

X


(14)


_s.t. Y_


**DjX(u[′]j**
_j=1_ _[−]_ **[u][j][)][ ≥]** **[1][,]**

X


(2Dj _I)Xuj_ 0, (2Dj _I)Xu[′]j_
_−_ _≥_ _−_ _[≥]_ [0][,][ ∀][j][ ∈] [[][p][]][.]

_The dual problem of (14) is given by_

_D[∗]_ = max **y[T]** **_λ s.t. Yλ_** 0, max (15)
**_λ_** _⪰_ **u:** **u** 2 1
_∥_ _∥_ _≤_ _[|][λ][T][ (][X][T][ u][)][+][| ≤]_ [1][.]

The following proposition gives a characterization of the KKT point of the non-convex max-margin
problem (2). The definition of B-subdifferential can be found in Appendix A.
**Proposition 3 Let (W1, w2, λ) be a KKT point of the non-convex max-margin problem (2) (in terms**
_of B-subdifferential). Suppose that w2,i_ = 0 for certain i [m]. Then, there exists a diagonal matrix
**Dˆ** _i ∈_ R[N] _[×][N]_ _satisfying_ _̸_ _∈_
( D[ˆ] _i)n = 1, for x[T]n_ **[w][1][,i]** _[>][ 0][,]_


( D[ˆ] _i)n_ 0, 1 _, for x[T]n_ **[w][1][,i]** [= 0][,]
_∈{_ _}_

( D[ˆ] _i)n = 0, for x[T]n_ **[w][1][,i]** _[<][ 0][.]_
_such that_
**w1,i**

= X[T][ ˆ]Diλ, **X[T][ ˆ]Diλ** 2 = 1.
_w2,i_ _∥_ _∥_

Based on the characterization of the KKT point of the non-convex max-margin problem (2), we
provide an equivalent condition to ensure that it is also the KKT point of the convex max-margin
problem (14).
**Theorem 3 The KKT point of the non-convex max-margin problem (13) (in terms of B-subdifferential)**
_corresponds to a KKT point of the convex max-margin problem (14) if λ is dual feasible, i.e.,_

max (16)
**u:∥u∥2≤1** _[|][λ][T][ (][Xu][)][+][| ≤]_ [1][.]

_This condition is equivalent to for all Dj_ _, the dual variable λ satisfies that_
_∈P_

max (17)
_∥u∥2≤1,(2Dj_ _−I)Xu≥0_ _[|][λ][T][ D][j][Xu][| ≤]_ [1][.]


3.1 DUAL FEASIBILITY OF THE DUAL VARIABLE

A natural question arises: is it possible to examine whether λ is feasible in the dual problem? We say
the dataset (X, y) is orthogonal separable if for all n, n[′] _∈_ [N ],

**x[T]n** **[x][n][′][ >][ 0][,][ if][ y][n]** [=][ y][n][′] _[,]_

**x[T]n** **[x][n][′][ ≤]** [0][,][ if][ y][n]

_[̸][=][ y][n][′]_ _[.]_


-----

For orthogonal separable data, as long as the induced diagonal matrices in Proposition 3 cover the
positive part and the negative part of the labels, the KKT point of the non-convex max-margin problem
(2) is the KKT point of the convex max-margin problem (14).
**Proposition 4 Suppose that (X, y) is orthogonal separable. Suppose that the KKT point of the non-**
_convex problem include two neurons (w1,i+_ _, w2,i+_ ) and (w1,i− _, w2,i−_ ) such that the corresponding
_diagonal matrices_ **D[ˆ]** _i+ and_ **D[ˆ]** _i−_ _defined in Proposition 3 satisfy that_

**Dˆ** _i+_ **diag(I(y = 1)),** **Dˆ** _i_ **diag(I(y =** 1)).
_≥_ _−_ _≥_ _−_

_Then, the dual variable λ is dual feasible, i.e., satisfying (16)._

The spike-free matrices discussed in (Ergen & Pilanci, 2021a) also makes examining the dual
feasibility of λ easier. The definition of spike-free matrices can be found in Appendix A
**Proposition 5 Suppose that X is spike-free. Suppose that the KKT point of the non-convex problem**
_include two neurons (w1,i+_ _, w2,i+_ ) and (w1,i− _, w2,i−_ ) such that the corresponding diagonal matrices
**Dˆ** _i+ and_ **D[ˆ]** _i−_ _defined in Proposition 3 satisfy that_

**Dˆ** _i+_ **diag(I(y = 1)),** **Dˆ** _i_ **diag(I(y =** 1)).
_≥_ _−_ _≥_ _−_

_Then, the dual variable λ is dual feasible, i.e., satisfying (16)._

**Remark 1 For the spike-free data, the constraint on the dual problem is equivalent to**


max or equivalently
**Xu≥0,∥u∥2≤1** _[|][λ][T][ Xu][| ≤]_ [1][,]

max min
**Xu** 0, **u** 2 1 **_[λ][T][ Y][+][Xu][ ≤]_** [1][,] **Xu** 0 **_[λ][T][ Y][−][Xu][ ≥−][1][.]_**
_≥_ _∥_ _∥_ _≤_ _≥_

4 SUB-GRADIENT FLOW DYNAMICS OF LOGISTIC LOSS

In this section, we consider the following sub-gradient flow of the logistic loss (3)

_∂_

_λ˜n(t)xn(t)_ _,_

_∂t_ **[w][1][,i][(][t][) =][w][2][,i][(][t][)]**  

_n:(w1,i(t))[T]_ **xn>0**

X

_N_  

_∂_

_λ˜n(t)((w1,i(t))[T]_ **xn(t))+.**

_∂t_ _[w][2][,i][(][t][) =]_

_n=1_

X


(18)


where the n-th entry of **_λ(t) ∈_** R[N] is defined

[e] _λ˜n = −ynℓ[′](qn),_ _qn = yn(x[T]n_ **[W][1][)][+][w][2][.]** (19)

For simplicity, we omit the term (t). For instance, we write w1,i = w1,i(t). To be specific,
when w1[T],i[x][n][ = 0][, we select][ 0][ as the subgradient of][ w][2][,i][(][w]1[T],i[x][n][)][+][ with respect to][ w][1][,i][. Denote]
**_σi = sign(Xui). For σ ∈{1, −1, 0}[N]_**, we define

**g(σ,** **_λ) =_** _λ˜nxn._ (20)

_n:σn>0_

X

For simplicity, we also write [e]

**g(u,** **_λ) := g(sign(Xu), λ) =_** _λ˜nxn._ (21)

_n:w1[T],i[x][n][>][0]_

X

[e]

Then, we can rewrite sub-gradient flow of the logistic loss (3) as follows:

_∂_ _∂_

**_λ),_** _i,1[g][(][u][,][ e]λ)._ (22)
_∂t_ **[w][i,][1][ =][ w][2][,i][g][(][u][,][ e]** _∂t_ _[w][i,][2][ =][ w][T]_

Assume that the neural network is scaled at initialization, i.e., ∥w1,i(0)∥2[2] [=][ w]2[2],i[(0)][ for][ i][ ∈] [[][m][]][.]
Then, the neural network is scaled for t ≥ 0.


-----

**Lemma 1 Suppose that ∥w1,i(0)∥2 = |w2,i(0)| > 0 for i ∈** [m]. Then, for any t > 0, we have
_∥w1,i(t)∥2 = |w2,i(t)| > 0._

According to Lemma 1, for all t 0, sign(w2,i(t)) = sign(w2,i(0)). Therefore, we can simply
_≥_
write si = si(t) = sign(w2,i(t)). As the neural network is scaled for t ≥ 0, it is interesting to study
the dynamics of w1,i in the polar coordinate. We write w1,i(t) = e[r][i][(][t][)]ui(t), where ∥ui(t)∥2 = 1.
The gradient flow in terms of polar coordinate writes

_∂_ _∂_

_i_ **[g][(][u][i][,][ e]λ),** **g(ui,** **_λ)_** **u[T]i** **[g][(][u][i][,][ e]λ)** **ui** _._ (23)
_∂t_ _[r][i][ =][ s][i][u][T]_ _∂t_ **[u][i][ =][ s][i]** _−_

   

Let xmax = maxi [n] **xi** 2. Define gmin to be [e]
_∈_ _∥_ _∥_

_gmin = minσ∈Q_ _[∥][g][(][σ][,][ y][/][4)][∥][2][,][ s.t.][ g][(][σ][,][ y][/][4)][ ̸][= 0][,]_ where we denote (24)

_Q = {σ ∈{1, 0, −1}[N]_ _|σ = sign(Xw), w ∈_ R[d]}. (25)

As the set 1, 1, 0 is finite, we note that gmin > 0. We note that when maxn [N ] _qn_ 0,
we have **_λ Q ⊆{ ≈_** **y4** [. The following lemma shows that with initializations sufficiently close to] − _}[N]_ _∈_ _|_ _| ≈[ 0][,]_

**g(u(t),** **_λ(t))_** **g(u(t), y/4)** 2 and _dt[d]_ **[g][(][u][(][t][)][,][ e]λ(t))**
_∥_ _−_ _∥_ 2 [can be very small.]

[e]

**Lemma 2 Suppose that T > 0 and δ > 0. Suppose that (u(t), r(t)) follows the gradient flow (23)**

[e]

_with s = 1 and the initialization u(0) = u0 and r(0) = r0. Suppose that r0 is sufficiently small._
_Then, the following two statements hold._

-  For all t _T_ _, we have_ **g(u(t),** **_λ(t))_** **g(u(t), y/4)** 2 8 _._
_≤_ _∥_ _−_ _∥_ _≤_ _[g][min][δ]_

-  Fordt[d] _t[g][(] ≤[u][(][t][)][,]T[ e]λ such that(t))_ 2 **signmin16** _[δ]_ _[.]_ (Xu[e] (s)) is constant in a small neighbor of t, we have

_[≤]_ _[g][2]_

Based on the above lemma on the property of g(u(t), **_λ(t)), we introduce the following lemma_**
to upper-bound the time such that cos ∠(u(t), g(u(t), **_λ(t))) approaches 1 −_** _δ or sign(Xu(t))_
changes. [e]

**Lemma 3 Let δ ∈** (0, 1).Suppose that u0 satisfies that ∥u[e]0∥2 = 1 and **_λ(0)[T]_** (Xu0)+ > 0. Suppose
_that (u(t), r(t)) follows the gradient flow (23) with s = 1 and the initialization u(0) = u0 and_

_r(0) = r0. Let v(t) =_ **g(u(t),λ[e](t))** [e]

_∥g(u(t),λ[e](t))∥2_ _[. We write][ v][0][ =][ v][(0)][,][ σ][0][ =][ σ][(0)][ and][ g][0][ =][ ∥][g][(][σ][0][,][ y][/][4)][∥][2][.]_
_Denote_

1 1 _δ/8 + 1_ _δ_ 1 _δ/8 + v0[T]_ **[u][0]**
_T_ _[∗]_ = 2g0 1 _δ/8_ log p1 − _δ/8_ 1 + − δ _−_ log p1 − _δ/8_ **v0[T]** **[u][0]** ! _._ (26)

_−_ _−_ _−_ _−_ _−_

_For c ∈_ (0, 1 − _δ], definep_ p p

_T_ [shift](c) = 2g0 11 _δ/8_ log p11 − _δ/δ/8 +8_ _cc_ _−_ log p11 − _δ/δ/8 +8_ **vv00[T][T]** **[u][u][0][0]** ! (27)

_−_ _−_ _−_ _−_ _−_

_Suppose that r0 is sufficiently small such that the statements in Lemma 2 holds forp_ p p _T = T_ _[∗]. Then, at_
_least one of the following event happens_

-  There exists a time T such that we have sign(Xu(t)) = sign(Xu0) for t [0, T ) and
_∈_
1sign − _δ(, then the timeXu(t)) ̸= sign T satisfies that(Xu0). Let u T1 ≤ =T u[shift](T_ )( andv1[T] **[u] v[1][)][.]1[ Otherwise, there exists a time] = limt→T −0 v(t). If u[T]1** **[v][1][ T][≤][ ′]**
_satisfying T_ _T_ _, such that we have sign(Xu(t)) = sign(Xu0) for t_ [0, T ] and

_[′]_ _≤_ _[∗]_ _∈_ _[′]_
**u(T** _[′])[T]_ **v(T** _[′]) ≥_ 1 − _δ._

-  There exists a time T _T_ _, such that we have sign(Xu(t)) = sign(Xu0) for t_ [0, T ]
_≤_ _[∗]_ _∈_
_and u(T_ )[T] **v(T** ) ≥ 1 − _δ._


-----

**Corollary 1 Suppose that there exists a time T** _such that we have sign(Xu(t))_ =
**sign(Xu0) for t** _∈_ [0, T ) and sign(Xu(t)) ≠ **sign(Xu0).** _If T_ _>_ _T_ [shift](v1[T] **[u][1][)]** =

_g0[√]11−δ/8_ log _√√11−−δ/δ/88+−vv11[T][T]_ **[u][u][1][1][ −]** [log] _√√11−−δ/δ/88+−vv00[T][T]_ **[u][u][0][0]** _, then, we have u[T]1_ _[v][1]_ _[>][ 1][ −]_ _[δ][.]_

 

**Proposition 6 Consider the sub-gradient flow (23) with s = 1 and the initialization u(0) = u0 and**

**vr(0) =(t) = r0g. Here at initilization the neuron(u(t),λ[e](t))** **u0 satisfies that ∥u0∥2 = 1 and y[T]** (Xu0)+ > 0. Let

_∥g(u(t),λ[e](t))∥2_ _[. For any][ δ >][ 0][, for sufficiently small][ r][0][, there exists a time][ T][ =][ O][(log(][δ][−][1][))]_

_such that u(T_ )[T] **v(T** ) ≥ 1 − _δ and cos ∠(u(T_ ), g(u(T ), y)) ≥ 1 − _δ._

**Remark 2 The statement of proposition is similar to Lemma 4 in (Maennel et al., 2018). However,**
their proof contains a problem because they did not consider the change of sign(Xw) along the
gradient flow. Our proof in Appendix D.4 corrects this error.

We next study the properties of orthogonal separable datasets. DenoteThe following lemma give a sufficient condition on w to satisfy the condition in Proposition 4. B = {w ∈ R[d] : ∥w∥2 ≤ 1}.
**Lemma 4 Assume that (X, y) is orthogonal separable. Suppose that w ∈B is a local maximizer of**
**wysuch that[T] ∈B(Xw is a local minimizer of) y+ inn = B − and1.** (Xw)+ ̸= 0 y[T]. Then,(Xw)+ ⟨ inw, B xn and⟩ _> ( 0Xw for n)+ ∈ ̸= 0[N. Then,] such that ⟨w, y xnn = 1⟩_ _> 0. Suppose that for n ∈_ [N ]

We show an equivalent condition of u ∈B being the local maximizer/minimizer of y[T] (Xu)+ in B.
**Proposition 7 Assume that (X, y) is orthogonal separable. Then, u ∈B is a local maximizer of**
**y[T]** (Xu)+ in B is equivalent to cos ∠(u, g(u, y)) = 1. Similarly, u ∈B is a local minimizer of
**y[T]** (Xu)+ in B is equivalent to cos ∠(u, g(u, y)) = −1.

Based on Proposition 4 and 7, we present the main theorem.
**Theorem 4 Suppose that the dataset is orthogonal separable and θ(t) follows the gradient flow.**
_For almost all initializations which are sufficiently close to zero, the limiting point ofSuppose that the neural network is scaled at initialization, i.e., ∥w1,i(0)∥2 = |w2,i(0)∥| for allθθ((tt))∥2_ _[is] i ∈∥θθ[[∗]m[∗]∥2].[,]_

_where θ[∗]_ _is a global minimizer of the max-margin problem (2)._

We present a sketch of the proof. According to Proposition 6, for initialization sufficiently close to zero,
there exist two neurons and time T+, T _> 0 such that cos ∠(w1,i+_ (T+), g(w1,i+ (T+), y)) 1 _δ_
_−_ _≥_ _−_
and cos ∠(w1,i− (T−), g(w1,i− (T−), y)) ≤−(1 − _δ). This implies that w1,i+_ (T+) and w1,i− (T+)
are sufficiently close to certain stationary points of gradient flow maximizing/minimizing y[T] (Xu+)
over B, i.e., {u ∈B| cos(u, g(u, y)) = ±1}. As the dataset is orthogonal separable, from Proposition
7 and Lemma 4, the induced masking matrices _D[ˆ]_ _i+_ (T+) and _D[ˆ]_ _i−_ (T−) by w1,i+ (T+)/w1,i− (T−)
in Proposition 3 satisfy that _D[ˆ]_ _i+_ (T+) **diag(I(y = 1)) and** _D[ˆ]_ _i_ (T ) **diag(I(y =** 1)).
_≥_ _−_ _−_ _≥_ _−_
According to Lemma 3 in (Phuong & Lampert, 2021), for t ≥ max{T+, T−}, we also have _D[ˆ]_ _i+_ (t) ≥
**diag(I(y = 1)) and** _D[ˆ]_ _i_ (t) **diag(I(y =** 1)). According to Theorem 3 and Proposition 4, the
_−_ _≥_ _−_
KKT point of the non-convex problem (2) that gradient flow converges to corresponds to the KKT
point of the convex problem (14).

5 CONCLUSION

We provide a convex formulation of the non-convex max-margin problem for two-layer ReLU neural
networks and uncover a primal-dual extreme point relation between non-convex subgradient flow.
Under the assumptions on the training data, we show that flows converge to KKT points of the convex
max-margin problem, hence a global optimum of the non-convex objective.

6 ACKNOWLEDGEMENTS

This work was partially supported by the National Science Foundation under grants ECCS-2037304,
DMS-2134248, and US Army Research Office.


-----

REFERENCES

Burak Bartan and Mert Pilanci. Neural spectrahedra and semidefinite lifts: Global convex optimization of polynomial activation neural networks in fully polynomial-time. arXiv preprint
_arXiv:2101.02429, 2021a._

Burak Bartan and Mert Pilanci. Training quantized neural networks to global optimality via semidefinite programming. International Conference on Machine Learning (ICML), 2021, 2021b.

Lénaïc Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized
models using optimal transport. Advances in Neural Information Processing Systems, 31:3036–
3046, 2018.

Thomas M Cover. Geometrical and statistical properties of systems of linear inequalities with
applications in pattern recognition. IEEE transactions on electronic computers, (3):326–334, 1965.

Tolga Ergen and Mert Pilanci. Implicit convex regularizers of cnn architectures: Convex optimization
of two-and three-layer networks in polynomial time. International Conference on Learning
_Representations (ICLR), 2021, 2020._

Tolga Ergen and Mert Pilanci. Convex geometry and duality of over-parameterized neural networks.
_Journal of Machine Learning Research, 22(212):1–63, 2021a._

Tolga Ergen and Mert Pilanci. Global optimality beyond two layers: Training deep relu networks
via convex programs. In International Conference on Machine Learning, pp. 2993–3003. PMLR,
2021b.

Tolga Ergen and Mert Pilanci. Revealing the structure of deep neural networks via convex duality. In
_International Conference on Machine Learning, pp. 3004–3014. PMLR, 2021c._

Tolga Ergen, Arda Sahiner, Batu Ozturkler, John Pauly, Morteza Mardani, and Mert Pilanci. Demystifying batch normalization in relu networks: Equivalent convex optimization models and implicit
regularization. arXiv preprint arXiv:2103.01499, 2021.

Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in
terms of optimization geometry. In International Conference on Machine Learning, pp. 1832–1841.
PMLR, 2018.

Vikul Gupta, Burak Bartan, Tolga Ergen, and Mert Pilanci. Exact and relaxed convex formulations
for shallow neural autoregressive models. In International Conference on Acoustics, Speech, and
_Signal Processing, 2021._

Yuanzhi Li, Tengyu Ma, and Hongyang R Zhang. Learning over-parametrized two-layer neural
networks beyond ntk. In Conference on Learning Theory, pp. 2613–2682. PMLR, 2020.

Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks.
_arXiv preprint arXiv:1906.05890, 2019._

Chao Ma, Lei Wu, and Weinan E. The quenching-activation behavior of the gradient descent dynamics
for two-layer neural network models. arXiv preprint arXiv:2006.14450, 2020.

Hartmut Maennel, Olivier Bousquet, and Sylvain Gelly. Gradient descent quantizes relu network
features. arXiv preprint arXiv:1803.08367, 2018.

Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of twolayer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665–E7671,
2018.

Mary Phuong and Christoph H Lampert. The inductive bias of relu networks on orthogonally
separable data. 2021.

Mert Pilanci and Tolga Ergen. Neural networks are convex regularizers: Exact polynomial-time
convex optimization formulations for two-layer networks. pp. 7695–7705, 2020.


-----

Arda Sahiner, Tolga Ergen, Batu Ozturkler, Burak Bartan, John Pauly, Morteza Mardani, and Mert
Pilanci. Hidden convexity of wasserstein gans: Interpretable generative models with closed-form
solutions. arXiv preprint arXiv:2107.05680, 2021a.

Arda Sahiner, Tolga Ergen, John Pauly, and Mert Pilanci. Vector-output relu neural network problems
are copositive programs: Convex analysis of two layer networks and polynomial-time algorithms.
_International Conference on Learning Representations (ICLR), 2021b._

Arda Sahiner, Morteza Mardani, Batu Ozturkler, Mert Pilanci, and John Pauly. Convex regularization
behind neural reconstruction. International Conference on Learning Representations (ICLR),
2021c.

Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):
2822–2878, 2018.

Gang Wang, Georgios B Giannakis, and Jie Chen. Learning relu networks on linearly separable
data: Algorithm, optimality, and generalization. IEEE Transactions on Signal Processing, 67(9):
2357–2370, 2019.

Qiuling Yang, Alireza Sadeghi, Gang Wang, and Jian Sun. Learning two-layer relu networks is nearly
as easy as learning linear classifiers on separable data. IEEE Transactions on Signal Processing,
69:4416–4427, 2021.

Mo Zhou, Rong Ge, and Chi Jin. A local convergence theory for mildly over-parameterized two-layer
neural network. arXiv preprint arXiv:2102.02410, 2021.


-----

A DEFINITIONS AND NOTIONS

We introduce several useful definitions and notions which will be utilized in the proof.

A.1 DEFINITIONS

**Definition 1 Let O ⊂** R[n] be an open set and let F : O → R be locally Lipschitz continuous at
_x ∈O. Let DF be the differentiable points of F in O. The B-subdifferential of F at x is defined by_

_∂[B]F_ (x) := lim _._ (28)
k→∞ _[F][ ′][(][x][k][)][|][x][k][ ∈]_ _[D][F][, x][k][ →]_ _[x]_

The set ∂[◦]F (x) = co(∂BF (x)) is called Clarke’s subdifferential, where co denotes the convex hull.

**Definition 2there exists** A matrixz 2 1 such that A is spike-free if and only if the following conditions hold: for all ∥u∥2 ≤ 1,
_∥_ _∥_ _≤_ (Au)+ = Az. (29)

This is equivalent to say that

max (30)
**u:∥u∥2≤1,(I−XX[T]** )(Xu)+=0 _[∥][X][†][(][Xu][)][+][∥][2][ ≤]_ [1][.]

A.2 NOTIONS

We use the following letters for indexing.

-  The index n is for the n-th data sample xn.

-  We use the index i to represent the i-th neuron-pair (w1,i, w2,i).

-  The index j is for the j-th masking matrix Di .
_∈P_

B PROOFS IN SECTION 3

B.1 PROOF FOR PROPOSITION 2

Consider the following loss function [˜]l : R[N] _× R[N]_ _→_ R ∪{+∞}

_ℓ˜(z, y) =_ 0, _ynzn ≥_ 1, ∀n ∈ [N ], (31)
+ _,_ otherwise.
 _∞_

For a given y ∈{1, −1}[N], _ℓ[˜](z, y) is a convex loss function of z. The non-convex max-margin is_
equivalent to

min [˜]l ((XW1)+w2, y) + [1] **W1** _F_ [+][ ∥][w][2][∥]2[2] _._ (32)

2 _∥_ _∥[2]_

According to Appendix A.13 in (Pilanci & Ergen, 2020), the problem (32) is equivalent to  


+ [1]


min [˜]l


**DiX(u[′]i**
_i=1_ _[−]_ **[u][i][)][,][ y]**

X


_∥W1∥F[2]_ [+][ ∥][w][2][∥]2[2]


(33)


s.t. (2Di _I)Xui_ 0, (2Di _I)Xu[′]i_
_−_ _≥_ _−_ _[≥]_ [0][,][ ∀][i][ ∈] [[][p][]][.]

This is equivalent to (14). For fixed y ∈{1, −1}[N], the Fenchel conjugate function of _ℓ[˜](z, y) with_
respect to z can be computed by


_l[∗](λ[ˆ], y) = max_ _ℓ(z, y)_
**z** R[N][ z][T][ x][ −] [˜]
_∈_

= max **_λ, s.t. diag(y)z_** **1,**
**z** R[N][ z][T][ ˆ] _≥_
_∈_

**y[T][ ˆ]λ,** **diag(y)λ[ˆ]** 0
= _≤_
+ _,_ otherwise.
 _∞_


(34)


-----

According to Theorem 6 in (Pilanci & Ergen, 2020), the dual problem of (14) writes


max _l[∗](λ, y), s.t._ max (35)
_−[˜]_ **u:** **u** 2 1
_∥_ _∥_ _≤_ _[|][λ][T][ (][Xu][)][+][| ≤]_ [1][,]


which is equivalent to


max **y[T]** **_λ, s.t. diag(y)λ_** 0, max (36)
_−_ _≤_ **u:** **u** 2 1
_∥_ _∥_ _≤_ _[|][λ][T][ (][Xu][)][+][| ≤]_ [1][.]

By taking λ = −λ[ˆ], we derive (15). This completes the proof.

B.2 PROOF FOR PROPOSITION 3

For the non-convex max-margin problem (13), consider the Lagrange function


_L(W1, w2, λ) = [1]_ _F_ [+][ ∥][w][2][∥]2[2][)][ −] [(][Y][λ][)][T][ (][Y][(][XW][1][)][+][w][2]

2 [(][∥][W][1][∥][2] _[−]_ **[1][)]**

where Yλ ⪰ 0. The KKT point of the non-convex max-margin problem (13) (in terms of Bsubdifferential) satisfies
0 ∈ _∂W[B]_ 1 _[L][(][W][1][,][ w][2][,][ λ][)][,]_

**w2 −** (XW1)[T]+[λ][ = 0][,] (37)

_λn(yn(x[T]n_ **[W][1][)][+][w][2]**

_[−]_ [1) = 0][.]

The KKT condition on the i-th column of W1 is equivalent to


_w2,iλnxngn,i,_ (38)
_n=1_

X


**w1,i =**


where gn,i ∈ _∂[B](z)+|z=xTn_ **[w][1][,i]** [. In other words, we have]

= I(x[T]n **[w][1][,i]** if x[T]n **[w][1][,i]**

_gn,i_ _[≥]_ [0)][,] _[̸][= 0][,]_ (39)

( 0, 1 _,_ if x[T]n **[w][1][,i]** [= 0][.]

_∈{_ _}_

Let **D[ˆ]** _i = diag([g1,i, . . ., gN,i]). Then, we can write that_


**w1,i =**


_λngn,ixnw2,i_
_n=1_

X


(40)


=w2,iX[T][ ˆ]Diλ.

_gn,ix[T]n_ **[w][1][,i]** [= 0][.] (41)


From the definition of gn,i, we have

Therefore, we can compute that


_w2,i =(Xw1,i)[T]+[λ]_

_N_

= I(x[T]n **[w][1][,i]** _n_ **[w][1][,i][λ][n]**

_n=1_ _[≥]_ [0)][x][T]

X

_N_

= _gn,ix[T]n_ **[w][1][,i][λ][n]**

_n=1_

X

=w1[T],i[X][T][ ˆ]Diλ.


(42)


In summary, we have
**w1,i = w2,iX[T][ ˆ]Diλ,** _w2,i = w1[T],i[X][T][ ˆ]Diλ._ (43)
Suppose that w2,i = 0. This implies that
_̸_
**w1,i**

= X[T][ ˆ]Diλ, **X[T][ ˆ]Diλ** 2 = 1. (44)
_w2,i_ _∥_ _∥_

This completes the proof.


-----

B.3 PROOF FOR THEOREM 3

PROOF We can write the Lagrange function for the convex max-margin problem (14) as
_L({uj}j[p]=1[,][ {][u]j[′]_ _[}][p]j=1[,][ λ][,][ {][z][j][}][p]j=1[,][ {][z]i[′]_ _[}][p]j=1[)]_


( **uj** 2 + **u[′]j[∥][2][) +][ λ][T][ diag][(][y][)]** **1** **diag(y)**
_∥_ _∥_ _∥_  _−_
_j=1_

X

_p_ 

(z[T]j [(2][D][j] [+ (][z][′]j[)][T][ (2][D][j] _j[)]_

_−_ _j=1_ _[−]_ _[I][)][Xu][j]_ _[−]_ _[I][)][Xu][′]_

X


_j=1_ **DjX(uj −** **u[′]j[)]**

X


(45)


=λ[T] **y +**


_j=1(u[′]j[)][T][ (][X][T][ D][j][λ][ −]_ **[X][T][ (2][D][j]** _[−]_ _[I][)][z]j[′]_ [)]

X


(∥uj∥2 + ∥u[′]j[∥][2][) +]
_j=1_

X


+ (uj)[T] ( **X[T]** **Djλ** **X[T]** (2Dj _I)zj)._

_j=1_ _−_ _−_ _−_

X

where zj, z[′]j _j_
The KKT point shall satisfy the following KKT conditions:[∈] [R][N][ satisfies that][ z][j][ ≥] [0][,][ z][′] _[≥]_ [0][ for][ j][ ∈] [[][p][]][ and][ λ][ ∈] [R][N][ satisfies that][ diag][(][y][)][λ][ ≥] [0][.]
_−X[T]_ **Djλ + X[T]** (2Dj − _I)z[′]j_ _[∈]_ _[∂][u]j[′]_ _[∥][u]j[′]_ _[∥][2][,]_

**X[T]** **Djλ + X[T]** (2Dj _I)zj_ _∂uj_ **uj** 2,
_−_ _∈_ _∥_ _∥_


(Dj)nx[T]n [(][u][j] _j[)][ −]_ _[y][n]_ = 0,
_j=1_ _[−]_ **[u][′]** 

X



_zj,n(2(Dj)n,n_ 1)x[T]n **[u][j]** [= 0][,]
_−_

_zj,n[′]_ [(2(][D][j][)][n,n] _[−]_ [1)][x]n[T] **[u]j[′]** [= 0][.]


(46)


_λn_


Let (W1, w2, λ) be the KKT point of the non-convex problem (2) and λ satisfies (17). Let **D[ˆ]** _i be_
the diagonal matrix defined in Proposition 3 with respect to w1,i and denote _P[¯] = {D[ˆ]_ _i|i ∈_ [m]}.
Without the loss of generality, we may assume that {D[¯] _i}i[m]=1_ [are different. (Otherwise, we can merge]
two neurons w1,i1 and w1,i2 with **D[¯]** _i1 = D[¯]_ _i2 together.)_

Suppose thatuj = −w1,iw D2,ij and ∈ **zP[ˆ]j, i.e., = 0, the following identities hold. Dj = D[ˆ]** _i for certain i ∈_ [m]. By letting u[′]j [=][ w][1][,i][w][2][,i][,][ z]j[′] [= 0][,]

**X[T]** **Djλ + X[T]** (2Di _I)z[′]j_ [=][ X][T][ ˆ]Diλ = **[w][1][,i]** = **u[′]i** (47)
_−_ _w2,i_ **u[′]i[∥]** _[.]_

_∥_

**ui**

**X[T]** **Djλ + X[T]** (2Di _I)zj =_ **X[T][ ˆ]Diλ =** **[w][1][,i]** = (48)
_−_ _−_ _−_ _w2,i_ **ui**

_∥_ _∥_ _[.]_

Therefore, for index j satisfying Di _P_, the first two KKT conditions in (46) hold.
_∈_ [ˆ]

For Dj /∈ _P[ˆ], we can let uj = u[′]j_ [= 0][. As][ λ][ satisfies (17), we have]

max (49)
_∥u∥2≤1,(2Dj_ _−I)Xu≥0_ _[|][λ][T][ D][j][Xu][| ≤]_ [1][.]

According to Lemma 4 in (Pilanci & Ergen, 2020), this implies that there exist zj, z[′]j

_[≥]_ [0][ such that]

**X[T]** **Djλ + Z[T]** (2Dj _I)z[′]j[∥≤]_ [1][,][ ∥][X][T][ D][j][λ][ +][ Z][T][ (2][D][j] (50)
_∥−_ _−_ _[−]_ _[I][)][z][j][∥≤]_ [1][.]
Therefore, the first two KKT conditions in (46) hold.

From our choice of uj, zj, u[′]j[,][ z][j][, the last two KKT conditions in (46) hold. We also note that]


**DjX(u[′]j**
_j=1_ _[−]_ **[u][j][) =]**

X


(Xw1,i)+w2,i. (51)
_i=1_

X


As (W1, w2, λ) is the KKT point of the non-convex problem, the third KKT condition in (46) holds.
This completes the proof.


-----

C PROOFS IN SECTION 3.1

In this section, we present several proofs for propositions in Section 3.1.

C.1 PROOF FOR PROPOSITION 4

We start with two lemmas.
**Lemma 5 Suppose that u0 = X[T][ ˆ]D0λ and ∥u0∥2 ≤** 1. For any masking matrix Dj ∈P such that
(Dj − **D[ˆ]** 0)I(λ > 0) = 0, we have

max (52)
(2Dj _I)Xu_ 0, **u** 2 1 **_[λ][T][ D][j][Xu][ ≤]_** [1][.]
_−_ _≥_ _∥_ _∥_ _≤_

PROOF According to Lemma 4 in (Pilanci & Ergen, 2020), the constraint (52) is equivalent to that
there exist zj ∈ R[N] such that zj ≥ 0 and
**X[T]** **Djλ + X[T]** (2Dj _I)zj_ 1. (53)
_∥_ _−_ _∥≤_

Consider the index n ∈ [N ] such that (Dj − **D[ˆ]** 0)nn ̸= 0. As (Dj − **D[ˆ]** 0)I(λ > 0) = 0, we have
_λn ≤_ 0. We let (zj)n = −λn. If ( D[ˆ] 0)nn = 0, then we have (Dj)nn = 1 and

(Dj **D0)nnλnxn = λnxn =** **x[T]n** [(2(][D][j][)][nn] (54)
_−_ [ˆ] _−_ _[−]_ [1)(][z][j][)][n][.]

If ( D[ˆ] 0)nn = 1, then we have (Dj)nn = 0 and

(Dj **D0)nnλnxn =** _λnxn =_ **x[T]n** [(2(][D][j][)][nn] (55)
_−_ [ˆ] _−_ _−_ _[−]_ [1)(][z][j][)][n][.]
For other index n ∈ [N ], we simply let (zj)n = 0. Then, we have

(Dj **D0)nnλnxn = 0 =** **x[T]n** [(2(][D][j][)][nn] (56)
_−_ [ˆ] _−_ _[−]_ [1)(][z][j][)][n][.]
Based on our choice of zj, we have zj 0 and for n [N ]
_≥_ _∈_

(Dj **D0)nnλnxn =** **x[T]n** [(2(][D][j][)][nn] (57)
_−_ [ˆ] _−_ _[−]_ [1)(][z][j][)][n][.]
This implies that
**X[T]** (Dj **D0)λ =** **X[T]** (2Dj _I)zj._ (58)
_−_ [ˆ] _−_ _−_
Hence, we have
**X[T]** **Djλ + X[T]** (2Dj − _I)zj = X[T][ ˆ]D0λ = u0._ (59)
Therefore, **X[T]** **Djλ + X[T]** (2Dj _I)zj_ 2 1.
_∥_ _−_ _∥_ _≤_

**Lemma 6 Suppose that the data is orthogonal separable and Yλ ≥** 0. Suppose that u0 = X[T][ ˆ]D0λ
_andu0 ∥2u0∥12 ≤. Therefore,1. For any masking matrix (52) holds._ **Dj such that** **D[ˆ]** 0 − **Dj ≥** 0, we have ∥X[T] **Djλ∥2 ≤**
_∥_ _∥_ _≤_

PROOF We note that u0 = X[T] ( D[ˆ] 0 − **Dj)λ + XDjλ. Denote a = X[T]** ( D[ˆ] 0 − **Dj)λ and b =**
**X[T]** **Djλ. We note that**

_T_

**a[T]** **b =** _λnxn_ _λn′_ **xn′** _._ (60)

  [] 

_n:( D[ˆ]_ 0)nn=1X,(Dj )n,n=0 _n[′]:( D[ˆ]_ 0)n′ _n′X=0,(Dj_ )n′ _n′_ =0

As diag(y)λ ≥ 0, λn has the same signature with _yn. Therefore, from the orthogonal separability_
of the data, we have
_λnλn′_ **x[T]n** **[x][T]n[′][ ≥]** [0][.] (61)
This immediately implies that a[T] **b ≥** 0. Therefore,
1 ≥∥u0∥2[2] [=][ ∥][a][ +][ b][∥]2[2] [=][ ∥][a][∥]2[2] [+][ ∥][b][∥]2[2] [+ 2][a][T][ b][ ≥∥][a][∥][2][.] (62)
This completes the proof.

Based on Lemma 5 and Lemma 6, we present the proof for Proposition 3. Let u+ = **ww21,i,i++** [. From the]

proof of Proposition 3, we note that ∥u+∥2 = 1. For any masking matrix Dj ∈P, let **D[˜]** = D[ˆ] _i+_ **Dj.**
As **D[ˆ]** _i+_ **D, according to Lemma 6, we have**
_≥_ [˜]

_∥X[T][ ˜]Dλ∥2 ≤∥X[T][ ˆ]Di+_ **_λ∥2 = ∥u+∥2 ≤_** 1. (63)

AsFrom Lemma 5, we note that Yλ ≥ 0 and **D[ˆ]** _i+ ≥_ **diag( λI(y satisfies = 1)), we have (52). Similarly, we can show that (Dj −D[˜]** )I(λ > 0) = Dj(I − −λD[ˆ] also satisfiesi+ )I(λ > 0) = 0 (52)..
This completes the proof.


-----

C.2 PROOF FOR PROPOSITION 5

PROOF Note that Yλ 0. Let Y+ = diag(I(y = 1)) and Y = diag(I(y = 1)). We claim
_≥_ _−_ _−_
that
max (64)
**u** 1 **_[λ][T][ (][Xu][)][+][ = max]u_** 1 **_[λ][T][ Y][+][(][Xu][)][+][.]_**
_∥_ _∥≤_ _∥_ _∥≤_

Firstly, we note that


**_λ[T]_** (Xu)+ =


(λn)+(x[T]n **[u][)][+]** [=][ λ][T][ Y][+][(][Xu][)][+][.] (65)
_n=1_

X


_λn(x[T]n_ **[u][)][+]**
_n=1_ _[≤]_

X


This implies that max **u** 1 λ[T] (Xu)+ max **u** 1 λ[T] **Y+(Xu)+.**
_∥_ _∥≤_ _≤_ _∥_ _∥≤_

On the other hand, suppose that u arg max **u** 1 λ[T] **Y+(Xu)+. As X is spike-free, there exists z**
_∈_ _∥_ _∥≤_
such that **z** 2 1 and Xz = (Xu)+. Therefore, we have
_∥_ _∥_ _≤_

**_λ[T]_** **Y+(Xu)+ = λ[T]** **Y+Xz = λ[T]** **Xz = λ[T]** (Xz)+. (66)

This implies that max **u** 1 λ[T] (Xu)+ max **u** 1 λ[T] **Y+(Xu)+.**
_∥_ _∥≤_ _≥_ _∥_ _∥≤_

For any Dj with Dj **Y+. We note that**
_∈P_ _≥_

**_λ[T]_** (Xu)+ **_λ[T]_** **Dj(Xu)+** **_λ[T]_** **Y+(Xu)+.** (67)
_≤_ _≤_

Combining with (65), this implies that max **u** 1 λ[T] (Xu)+ = max **u** 1 λ[T] **D(Xu)+.**
_∥_ _∥≤_ _∥_ _∥≤_

Let us go back to the original problem. Let u+ = **ww21,i,i++** [. We note that][ (][Xw][+][) = ˆ]Di+ **Xw+ =**
**Dˆ** _i+_ **XX[T][ ˆ]Di+** **_λ. Therefore, we have_**

**_λ[T]_** (Xw+) = λ[T][ ˆ]Di+ **XX[T][ ˆ]Di+** **_λ = ∥X[T][ ˆ]Di+_** **_λ∥2[2]_** [=][ ∥][u][+][∥]2[2] [= 1][.] (68)

Thus, for any ∥u∥2 ≤ 1, suppose that (Xu)+ = Xz, where ∥z∥2 ≤ 1. Then, we have

**_λ[T][ ˆ]Di+_** (Xu)+ = λ[T][ ˆ]Di+ **Xz ≤∥z∥2 ≤** 1. (69)

Therefore, max **u** 1 λ[T] (Xu)+ = max **u** 1 λ[T] **D+(Xu)+** 1. Similarly, we have
_∥_ _∥≤_ _∥_ _∥≤_ _≤_

min
**u** 1 **_[λ][T][ (][Xu][)][+][ = min]u_** 1 **_[λ][T][ D][−][(][Xu][)][+][ ≥−][1][.]_**
_∥_ _∥≤_ _∥_ _∥≤_

This completes the proof.

D PROOFS IN SECTION 4

D.1 PROOF FOR LEMMA 1

PROOF According to the sub-gradient flow (22), we can compute that


_∂_

**w1,i** 2 2,i = 2w1[T],i _w2,ig(u,_ **_λ)_** 2w2,iw1[T],i[g][(][u][,][ e]λ) = 0. (70)
_∂t_ _∥_ _∥[2]_ _[−]_ _[w][2]_ _−_
    

Let T0 = sup{T _|∥w1,i(t)∥2 = |w2,i(t)| > 0, ∀i ∈_ [n][e], t ∈ [0, T )}. For t ∈ [0, T0), as the neural
network is scaled, it is sufficient study the dynamics of w1,i in the polar coordinate. Let us write
**w1,i(t) = e[r][i][(][t][)]ui(t), where ∥ui(t)∥2 = 1. Then, in terms of polar coordinate, the projected gradient**
flow follows
_∂_

_i_ **[g][(][u][i][,][ e]λ),**
_∂t_ _[r][i][ =][sign][(][w][2][,i][)][u][T]_

(71)

_∂_

**g(ui,** **_λ)_** **u[T]i** **[g][(][u][i][,][ e]λ)** **ui** _._
_∂t_ **[u][i][ =][sign][(][w][2][,i][)]** _−_

Without the loss of generality, we may assume that _w2,i(0)_ = 0 for i  [m]. Denote

[e] _̸_ _∈_

_xmax = max_ (72)
_i∈[n]_ _[∥][x][i][∥][2][.]_


-----

From the definition of **_λ, we have_** **_λ_** 1/4. Therefore, we have
_∥[e]∥∞_ _≤_

[e]∂ **_λ)_** 2 _λ˜jxj_

_∂t_ _[r][i]_ _[≤∥][g][(][u][i][,][ e]_ _∥_ _≤_ _j:xX[T]j_ **[u][>][0]** 2 _≤_ _[nx]2[max]_

Therefore, for finite t > 0, we have

_ri(t)_ _ri(0)_ _,_
_≥_ _−_ _[nx][max]4_ _[t]_

which implies that |w2,i(t)| > 0. This implies that T0 = ∞.

D.2 PROOF OF LEMMA 2

PROOF As we have ∥w1,i∥2 = |w2,i|, for n ∈ [N ], we can compute that

_|qn| =|(x[T]n_ **[W][1][)][+][w][2][|]**

_m_

_≤_ _|(x[T]n_ **[w][1][,i][)][+][w][2][,i][|]**

_i=1_

Xm

= _∥w1,i∥2|(x[T]n_ **[w][1][,i][)][+][|]**

_i=1_

Xm

_≤_ _∥w1,i∥2|x[T]n_ **[w][1][,i][|]**

_i=1_

X


(73)

(74)

(75)


_∥w1,i∥2[2][.]_
_i=1_

X


**xn** 2
_≤∥_ _∥_


Note that _λ[˜]n = −ynℓ[′](qn) and_ _[y]4[n]_ [=][ −][y][n][ℓ][′][(0)][. As][ ℓ][′][ is][ 1]4 [-Lipschitz continuous, we have]

_m_

_λn_ _yn/4_ **w1,i** 2[.] (76)
_−_ _≤_ [1]4 _[|][q][n][| ≤∥][x]4[n][∥][2]_ _i=1_ _∥_ _∥[2]_

X

For any ˆσ, as _λ[˜]n_ [0, 1[˜]/4] for n [N ], we have
_∈Q_ _∈_ _∈_

**g(ˆσ,** **_λ)_** **g(ˆσ, y/4)** 2
_∥_ [e] _−(λ[˜]n_ _yn/4)∥xn_

_≤_ _−_

_n:(ˆσ)n>0_

X


_λk_ _λk_ **xk** 2
_|_ _−_ [˜] _| ∥_ _∥_
_k:(ˆσ)k>0_

X


(77)


**xk** 2
_∥_ _∥[2]_


**w1,j** 2
_∥_ _∥[2]_
_j=1_

X


_k=1_


=c1 _∥w1,i∥2[2][,]_

_i=1_

X


where c1 = [1]4 _[∥][X][∥]F[2]_ _[>][ 0][ is a constant. Therefore, we can bound][ ∥][g][(ˆ]σ,_ **_λ(t))∥_** by

_m_ _m_

**g(ˆσ,** **_λ(t))_** 2 **g(ˆσ, y/4)** 2 + c1 **w1,i(t)** 2 [e][+][ c][1] _e[2][r][i][(][t][)],_ (78)
_∥_ _∥_ _≤∥_ _∥_ _i=1_ _∥_ _∥[2]_ _[≤]_ _[d][max]_ _i=1_

X X

where we let [e]
_gmax = max_ (79)
**_σ∈Q_** _[∥][g][(][σ,][ y][/][4)][∥][2][.]_


-----

Let r(t) = maxi [m] ri(t). We note that
_∈_

_∂_

(80)
_∂t_ _[r][(][t][)][ ≤]_ _[d][max][ +][ c][1][ne][2][r][(][t][)][ ≤]_ _[c][2][(1 +][ e][2][r][(][t][)][)][,]_

where c2 = max{nc1, dmax} > 0 is a constant. If we start with r(0) ≪ 0, then, r(t) cannot grow
much faster than c2t. Let ˜r(t) satisfy the following ODE:

_∂_ _r(t) = c2(1 + e[2˜]r(t))._ (81)

_∂t_ [˜]

The solution is given by

_r˜a(t) = c2(t_ _a)_ (82)
_−_ _−_ 2 [1] [log(1][ −] _[e][2][c][2][(][t][−][a][)][)][,]_


where a > 0 is a parameter depending on the initialization. For any initial r(0), we have a unique a
satisfying ˜ra(0) = r(0). Therefore, we have r(t) _r˜a(t) and_
_≤_

**g(σ,** **_λ(t))_** **g(σ, y/4)** 2 _c1ne[2˜]ra(t)._ (83)
_∥_ _−_ _∥_ _≤_

According to the bound (83), by choosing a sufficiently small r0, (which leads to a sufficiently small
_a), such that_ [e]

_e[2˜]ra(T )_ min _dminδ_ _,_ _d[2]min[δ]_ _._ (84)
_≤_ 16c1 4n[2]x[3]max
 

Therefore, for t ≤ _T_, we have

**g(ˆσ,** **_λ(t))_** **g(ˆσ, y/4)** 2 _c1ne[2˜]ra(t)_ _c1ne2˜ra(T )_ _._ (85)
_∥_ _−_ _∥_ _≤_ _≤_ _≤_ _[d][min]8_ _[δ]_

Hence, we have

[e]

**g(σ(t),** **_λ(t))_** **g(σ(t), y/4)** 2 _c1ne[2˜]ra(t)_ _c1ne2˜ra(T )_ _._ (86)
_∥_ _−_ _∥_ _≤_ _≤_ _≤_ _[d][min]8_ _[δ]_

We can compute that

[e] _d_

_λ˜i =_ _yil[(2)](qi)_ _[d]_ (87)
_dt_ _−_ _dt_ _[q][i][.]_

As l[(2)](q) ∈ (0, 1/4], we can compute that


_w2,j_ **xi** 2
_|_ _|∥_ _∥_
_j=1_

X


_dt_ _[q][i]_


_dt_ **[w][1][,j]**


**w1,j** 2 **xi** 2
_∥_ _∥_ _∥_ _∥_
_j=1_

X


_dt_ _[w][2][,j]_


(88)


_≤_ _[n]4_


_m_

**w1,j** 2[x]max[2] [+][ n]
_∥_ _∥[2]_ 4
_j=1_

X


_w2[2],j[x]max[2]_
_j=1_

X


max
_≤_ _[nx]2[2]_


_e[2][r][(][t][)]._


Therefore, we have
_d_ _λ˜i_ [=][ |][ℓ][′′][(][q][i][)][|] _d_ _d_ max _e[2][r][(][t][)]._ (89)

_dt_ _dt_ _[q][i]_ 4 _dt_ _[q][i]_ 8

_[≤]_ [1] _[≤]_ _[nx][2]_

Suppose that sign(Xu(s)) = σ(t) holds for s in a small neighbor of t. Then, we have


_d_

**_λ(t))_**
_dt_ **[g][(][u][(][t][)][,][ e]**

This completes the proof.


_d_
= **_λ(t))_**

_dt_ **[g][(][σ][(][t][)][,][ e]** 2

_n_

**xi** 2 _d_ _λ˜i_ max

_≤_ _i=1_ _∥_ _∥_ _dt_ _[≤]_ _[n][2][x]8[3]_

X

max _e[2˜]ra(T )_ min[δ] _._
_≤_ _[n][2][x]8[3]_ _≤_ _[d][2]16_


_e[2][r][(][t][)]_


(90)


-----

D.3 PROOF OF LEMMA 3

PROOF Let T0 = sup{T _|sign(Xu(t)) = sign(Xu0), ∀t ∈_ [0, T )}. We analyze the dynamics of
**u(t) in the interval [0, min** _T0, T_ ]. For t min _T0, T_, as the statements in Lemma 2 hold, we
_{_ _[∗]}_ _≤_ _{_ _[∗]}_
can compute that

_d_

_dt_ **[v][(][t][)][T][ u][(][t][)]**

_T_

**g(σ0,** **_λ(t))_**

= _[d]_ **u(t)**

_dt_ **g(σ0,** **_λ(t))_** 2 !

_∥_ _∥_

[e] _T_

**g(σ0,** **_λ(t))_** _d_ 1 _d_

= [e] **_λ(t))_**

**g(σ0,** **_λ(t))_** 2 ! _dt_ **[u][(][t][) +][ u][(][t][)][T]** **g(σ0,** **_λ(t))_** 2 _dt_ **[g][(][σ][0][,][ e]**
_∥_ _∥_ _∥_ _∥_

[e]

**_λ(t))[T d]dt_** **[g][(][σ][0][,][ e]λ(t))**
**u(t)[T]** **g[e](σ0,** **_λ(t))_** **[g][(][σ][0][,][ e]** [e] (91)
_−_ **g(σ0,** **_λ(t))_** 2

_∥_ _∥[3]_

2 _d_

**g(σ0,** **_λ(t))[T][ d][e]_** **_λ(t))_**
_≥_ _dt_ **[u][t][ −]** _gmin_ _dt_ **[g][(][σ][e][0][,][ e]** 2

**g(σ0,[e]λ(t))** 2 1 (v(t)[T] **u(t))[2][]**
_≥∥_ _∥_   _−_ _−_ _[g][min]8_ _[δ]_

_g0 (1_ [e]δ/8) 1 (v(t)[T] **u(t))[2][]**
_≥g0_ 1 − _δ/4_   ( −v(t)[T] **u(t))[2][]** _._ _−_ _[g][min]8_ _[δ]_

_≥_ _−_ _−_

Here we utilize that  g0 ≥ _gmin, where gmin is defined in (24). Let z(t) satisfies the ODE_

_dz(t)_

= (1 _δ/4_ _z(t)[2])g0,_ (92)
_dt_ _−_ _−_

with initialization z(0) = v0[T] **[u][0][. Then, we note that]**


2 1 _δ/8_

_z(t) =_ 1 − _δ/8 −_ 1 + c3 exp(2pg0 −t/( 1 _δ/8))_ _,_ (93)
p _−_

_√1_ _δ/8_ _−1_ p
where c3 = **v0[T]−[u][0]** _−_ 1 . We can compute that
 

_z(T3) = 1_ _δ._ (94)
_−_

According to the comparison theorem, for t min _T0, T3_, we have
_≤_ _{_ _}_

**v(t)[T]** **u(t) ≥** _z(t)._ (95)

We first consider the case where T0 = ∞. As T0 = ∞, we have

**v(T** _[∗])[T]_ **u(T** _[∗]) ≥_ _z(T_ _[∗]) = 1 −_ _δ._ (96)

Therefore, the second event holds for T ≤ _T_ _[∗]._

Otherwise, we have T0 < . Recall that u1 = u(T0) and v1 = limt _T0 v(t). Let T1 =_
_∞_ _↑_
sup _T_ **v(t)[T]** **u(t) < v1[T]** **[u][1][,][ ∀][t][ ∈]** [[0][, T] [)][}][ and][ T][2] [= sup][{][T] _[|][v][(][t][)][T][ u][(][t][)][ <][ 1][ −]_ _[δ,][ ∀][t][ ∈]_ [[0][, T] [)][}][.]
_{_ _|_
If T2 _T0, for t_ [0, T2], we have
_≤_ _∈_

_d_

1 _δ/4_ (1 _δ)[2][]_ _g0 > 0._ (97)
_dt_ **[v][(][t][)][T][ u][(][t][)][ ≥]** _−_ _−_ _−_


Therefore, v(t)[T] **u(t) monotonically increases in [0, T2]. As v(t)[T]** **u(t)** _z(t) for t_ [0, T0], we
_≥_ _∈_
have that z(T2) **v(T2)[T]** **u(T2) = 1** _δ = z(T3). Hence, we have T2_ _T_ . Therefore, the second
condition of the first event holds at ≤ _T = −_ _T2._ _≤_ _[∗]_

Then, we consider the case where T2 _T0. For t_ _T0, we have v(t)[T]_ **u(t)** 1 _δ. This implies that_
**v1[T]** **[u][1]** _≥_ _≤_ _[< T][0][, as][ T][0]_ _≤_ _−_

_[≤]_ [1][ −] _[δ][. Apparently, we have][ T][1]_ _[≤]_ _[T][0][. If][ T][1]_ _[≤]_ _[T][2][, for][ t][ ∈]_ [[0][, T][0][]][, the inequality]


-----

(97) holds. This implies that limt _T0_ 0 v(t)[T] **u(T0) > v(T1)[T]** **u(T1) = limt** _T0_ 0 v(t)[T] **u(T0),**
_→_ _−_ _→_ _−_
which leads to a contradiction. Therefore, we have T0 = T1. We note that

_z(T_ [shift](u[T]1 **[v][1][)) =][ u]1[T]** **[v][1][.]** (98)

As u(t)[T] **g(u(t),** **_λ(t))_** _z(t) for t_ [0, T0], we have that z(T1) **u[T]1** **[v][1]**
_T0 = T1 ≤_ _T_ [shift](u[T]1 **[v] ≥[1][)][. This completes the proof.] ∈** _≤_ _[≤]_ _[z][(][T][4][)][. Hence, we have]_

[e]

D.4 PROOF OF PROPOSITION 6

We first introduce a lemma.

**Lemma 7 Let a, b ∈** R[d] _and 0 < δ < c. Suppose that ∥a −_ **b∥2 ≤** _δ and ∥a∥2 ≥_ _c. Then, we have_
**a** **b**

(99)

**a** 2 _−_ **b** 2 2 _≤_ [2]c [δ] _[.]_
_∥_ _∥_ _∥_ _∥_

PROOF As δ < c, we have ∥b∥2 > ∥a∥2 −∥a − _b∥2 ≥_ _c −_ _δ > 0. We first note that_

_δ_

_∥a∥2[−][1]_ _−∥b∥2[−][1]_ = _[|∥][a][∥]a[2][ −∥]2_ **b[b]2[∥][2][|]** _≤_ _c_ **b** 2 _._ (100)

_∥_ _∥_ _∥_ _∥_ _∥_ _∥_

Therefore, we can compute that


**a** 2
_∥_ _∥_


**b** 2
_∥_ _∥_


(101)


2

_[∥][b][∥]_


_≤_ **a** 2 _−_ **a** 2

_∥_ _∥_ _∥_ _∥_

_≤_ _[δ]c_ [+][ δ]c [= 2]c [δ] _[.]_

This completes the proof.

Then we present the proof of Proposition 6.


**a** 2
_∥_ _∥_


**b** 2
_∥_ _∥_


PROOF As y[T] (Xu0)+ > 0, with sufficiently small initialization and sufficiently small δ > 0, we
also havetime T such thatλ(0)[T] ( uXu(T0))[T]+v ≥(Ty) ≥[T] (Xu1 −0)[3]4+[δ][ by contradiction. Denote]/4 −∥X∥2∥λ[e](0) − **y/4∥[ v]2 >[0][ =] 0[ v]. We prove that there exists a[(0)][. For all possible values]**

of **g(u, y/4)** 2, we can arrange them from the smallest to the largest by g(1) < g(2) < _< g(p)._
_∥_ [e] _∥_ _· · ·_

Let Ti = 2[√]1−1δ/8g(i) log _√√11−−δ/δ/8+18−1+−δ/δ/22_ _[−]_ [log] _√√11−−δ/δ/88+−gg(([−][−]ii))[1][1][v][v]00[T][T]_ **[u][u][0][0]**  and T = _i=1_ _[T][i][. Suppose]_

that r0 is sufficiently small such that statements in Lemma 2 holds for T . According to Lemma 3,
we can find 0 = t0 < t1 < . . . such that for i = 1, . . ., sign(Xu(t)) is constant on[P][p] [ti 1, ti) and
_−_
**sign(Xu(ti** 1)) = sign(Xu(ti)). We write ui = u(ti), gi = **g(u(ti), y/4)** 2,
_−_ _̸_ _∥_ _∥_

**gi[−]** [= lim]t _ti_ **[g][(][u][(][t][)][,][ e]λ(t)),** **gi = g(u(ti),** **_λ(t)),_** (102)
_↑_

**vi[−]** [=] _∥ggi[−]i[−][∥][2][ and][ v][i][ =]_ _∥ggii∥2_ [. We note that][ g]i[−] [=][ g]i[+]−1[. According to Lemma 3, we have][e]

_ti −_ _ti−1 ≤_ 2 1 −1δ/8gi−1 log p11 − − _δ/δ/8 + (8 −_ (vvii[−][−][)][)][T][T][ u][ u][i][i] _−_ log p11 − − _δ/δ/8 +8 − vvii[T][T]−−11[u][u][i][i][−][−][1][1]_ !

p 1 log p1 − _δ/8 + (vi[−][)][T][ u][i]_ log p1 − _δ/8 + vi[T]−1[u][i][−][1]_ _._

_≤_ 2 1 _δ/8gmin_ p1 _δ/8_ (vi[−][)][T][ u][i] _−_ p1 _δ/8_ **vi[T]** 1[u][i][−][1] !

_−_ _−_ _−_ _−_ _−_ _−_

(103)

p p p

Here we utilize that gi 1 _gmin, where gmin is defined in (24). This implies that_
_−_ _≥_

1 _δ/8 + (vi[−][)][T][ u][i]_ 1 _δ/8gmin(ti_ _ti_ 1) 1 _δ/8 + vi[T]_ 1[u][i][−][1]

p1 − _δ/8_ (vi[−][)][T][ u][i] _≥_ _e[2][√]_ _−_ _−_ _−_ p1 − _δ/8_ **vi[T]−1[u][i][−][1]** _._ (104)

_−_ _−_ _−_ _−_ _−_

p p


-----

We can show that for t satisfying t 1 log _√1−δ/8+1−δ/2_ min[v]0[T] **[u][0]** and
_≥_ 2[√]1−δ/8gmin  _√1−δ/8−1+δ/2_ _[−]_ [log][ 1+]1−[g]gmin[−][−][1][1] **[v]0[T]** **[u][0]** 

_t ≤_ _T_, we have ∥g(u(t), λ)∥2 > gmin. According to Lemma 3, as gi ≥ _gmin, we have_

1 _δ/8 + gmin[−][1]_ [(][g]i[−][)][T][ u][i] 1 _δ/8gmin(ti_ _ti_ 1) 1 _δ/8 + gmin[−][1]_ **[g]i[T]** 1[u][i][−][1]

p1 − _δ/8_ _gmin[−][1]_ [(][g]i[−][)][T][ u][i] _≥_ _e[2][√]_ _−_ _−_ _−_ p1 − _δ/8_ _gmin[−][1]_ **[g]i[T]−1[u][i][−][1]** _._ (105)

_−_ _−_ _−_ _−_ _−_

This implies thatp p


1 _δ/8 + gmin[−][1]_ [(][g]i[−][)][T][ u][i] 1 _δ/8gminti_
_−_ _e[2][√]_ _−_

p1 _δ/8_ _gmin[−][1]_ [(][g]i[−][)][T][ u][i] _≥_

_−_ _−_

or equivalently, for anyp _t > 0, we have_


1 _δ/8 + gmin[−][1]_ **[v]0[T]** **[u][0]**
_−_ _,_ (106)

1 _δ/8_ _gmin[−][1]_ **[v]0[T]** **[u][0]**
_−_ _−_


p11 − _δ/δ/8 +8_ _ggmin[−]min[−][1][1][(][g][g][(][(][u][u][(][(][t][t][))][)][,][ e][,]λ[ e]λ((t))t))[T][T]uu((t)t)_ _≥_ _e[2][√]1−δ/8gmint_ p11 − _δ/δ/8 +8_ _ggminmin[−][−][1][1]_ **[v][v]00[T][T]** **[u][u][0][0]** _._ (107)

_−_ _−_ _−_ _−_

Here we utilize thatp **g(u(t), λ(t))[T]** **u(t) is continuous w.r.t.p** _t._ Therefore, for t _≥_

2g1min log [2][−]δ _[δ]_ _−_ log 1[1+]−[g]gminmin[−][−][1][1] **[v][v]00[T][T]** **[u][u][0][0]**, we have

 

1 + gmin[−][1] [(][g][(][u][(][t][)][,][ e]λ(t)))[T] **u(t)** 1 _δ/8 + 1_ _δ/2_

1 _gmin[−][1]_ **[g][(][u][(][t][)][,][ e]λ(t))[T]** **u(t)** _≥_ p1 − _δ/8_ 1 + − δ/2 _._ (108)
_−_ _−_ _−_

This implies that p
_gmin[−][1]_ [(][g][(][u][(][t][)][,][ e]λ(t)))[T] **u(t) ≥** 1 − _δ/2._ (109)

If ∥g(u(t), λ)∥2 = gmin, as the statements in Lemma 2 hold, we can compute that

**g(u(t), λ)** **g(u(t),** **_λ(t))_** 2 = _[δ]_ (110)
_∥_ _−_ _∥_ _≤_ _[g][min]4_ _[δ]_ 4 _[∥][g][(][u][(][t][)][,][ λ][)][∥][2][,]_

which implies that

[e]

**g(u(t),** **_λ(t))_** 2 (1 + δ/4) **g(u(t), λ)** 2. (111)
_∥_ _∥_ _≤_ _∥_ _∥_

Therefore, we have

[e] **_λ(t)))[T]_** **u(t)**

**v(t)[T]** **u(t) =** [(][g][(][u][(][t][)][,][ e]

**g(u(t),** **_λ(t))_** 2
_∥_ _∥_

1 (g(u(t), **_λ(t)))[T]_** **u(t)** (112)
_≥_ 1 + δ/4 [e] _gmin_

[e]

_≥_ 1 +[1][ −] δ/[δ/]4[2] 4 _[δ.]_

_[≥]_ [1][ −] [3]

This leads to a contradiction.

Analogously, we can show that for t ≥ [P]j[i] =1 _[T][i][, we have][ ∥][g][(][u][(][t][)][,][ y][/][4)][∥][2][ > g][(][i][)][. Thus, by taking]_
_t ≥_ [P]i[p]=1 _[T][i][, we have][ ∥][g][(][u][(][t][)][,][ y][/][4)][∥][2][ > g][(][p][)][ =][ g][max][. However, from the definition of][ g][max][,]_
we have **g(u(t), y/4)** 2 _gmax. This leads to a contradiction. Therefore, there exists a time_
_T =_ _i ∥=1_ _[T][i][ =][ O][(log][ δ]∥_ _[−] ≤[1][)][ such that][ v][(][T]_ [)][T][ u][(][T] [)][ ≥] [1][ −] [3]4 _[δ][.]_

We note that **g(u(T** ), y/4) 2 _gmin. As the statements in Lemma (2) hold, we have_

[P][p] _∥_ _∥_ _≥_

**g(u(T** ), y/4) **g(u(T** ), **_λ(T_** )) 2 (113)
_∥_ _−_ _∥_ _≤_ _[δg]8[min]_

According to Lemma 7, we have

[e]

**g(u(T** ), y/4) **g(u(T** ), **_λ(T_** )) **_λ(T_** )) 2

_∥_

_∥g(u(T_ ), y/4)∥2 _−_ _∥g(u(T_ ), **_λ(T_** ))∥2 2 _≤_ [2][∥][g][(][u][(][T] [)][,][ y][/][4)]g[ −]min[g][(][u][(][T] [)][,][ e] _≤_ 4[δ] _[.]_

[e] (114)

[e]


-----

This implies that

Hence, we have


**g(u(T** ), y/4)
**u(T** )[T]

**g(u(T** ), y/4) 2
_∥_ _∥_

**g(u(T** ), λ) **g(u(T** ), **_λ(T_** ))

**u(T** )[T] **v(T** )
_≥_ _−_ **g(u(T** ), λ) 2 _−_ **g(u(T** ), **_λ(T_** )) 2

_∥_ _∥_ _∥_ _∥_

[e]

_≥1 −_ _δ._

[e]


(115)


**g(u(T** ), y) **g(u(T** ), y/4)
cos ∠(u(T ), g(u(T ), y)) = u(T )[T] = u(T )[T] 1 _δ._

**g(u(T** ), y) 2 **g(u(T** ), y/4) 2 _≥_ _−_
_∥_ _∥_ _∥_ _∥_

This completes the proof.

D.5 PROOF OF LEMMA 4


PROOF This is proved in Lemma 2 in (Phuong & Lampert, 2021). Here we provide an alternative
proof. It is sufficient to prove for the case of local maximizer. Suppose that w is a local maximizer of
**y[T]** (Xw)+ in B. We first consider the case where y[T] (Xw)+ > 0.

If there exists n [N ] such that **w, xn** 0 and yn = 1. Consider v = xn/ **xn** 2 and let
**w+ϵv** _∈_ _⟨_ _⟩≤_ _∥_ _∥_
**wϵ =** _∥w+ϵv∥2_ [, where][ ϵ >][ 0][. For index][ n][′][ ∈] [[][N] []][ such that][ y][n][′][ = 1][, as the dataset is orthogonal]

separable, we have x[T]n[′] **[x][n][ >][ 0][ and]**

_ϵ_
**x[T]n[′]** [(][w][ +][ ϵ][v][) =][ x]n[T][′] **[w][ +]** **x[T]n[′]** **[x][n]** _[>][ x][T]n[′]_ **[w][.]** (116)

**xn** 2
_∥_ _∥_

This implies that (x[T]n[′] **[w][ϵ][)][+][ ≥]** [(][x][T]n[′] **[w][)][+][. For][ y][n][′][ =][ −][1][, as the data is orthogonal separable, we note]**
that x[T]n[′] **[x][n][ ≤]** [0][ and]
_ϵ_
**x[T]n[′]** [(][w][ +][ ϵ][v][) =][ x]n[T][′] **[w][ +]** **x[T]n[′]** **[x][n]** _n[′]_ **[w][.]** (117)

**xn** 2 _[≤]_ **[x][T]**
_∥_ _∥_

This implies that (x[T]j **[w][ϵ][)][+][ ≤]** [(][x]j[T] **[w][)][+][. In summary, we have]**


**y[T]** (X(w + ϵv))+ =


_yn(x[T]j_ **[w][)][+]** [=][ y][T][ (][Xw][)][+] _[>][ 0]_ (118)
_n=1_

X


_yn(x[T]j_ [(][w][ +][ ϵ][v][))][+]
_n=1_ _[≥]_

X


If ⟨w, xn⟩ _< 0, then w[T]_ **v < 0. This implies that with sufficiently small ϵ, we have ∥w + ϵv∥2 <**
_∥w∥2 = 1. Therefore,_

1
**y[T]** (Xwϵ))+ = **y[T]** (X(w + ϵv))+ > y[T] (X(w + ϵv))+ **y[T]** (Xw)+, (119)

**w + ϵv** 2 _≥_
_∥_ _∥_

which leads to a contradiction. If **w, xn** = 0, we note that
_⟨_ _⟩_

(x[T]n [(][w][ +][ ϵ][v][))][+] [=][ ϵ >][ (][x][T]n **[w][)][+][.]** (120)

This implies that
**y[T]** (X(w + ϵv))+ ≥ **y[T]** (Xw)+ + ϵ. (121)
We also note that ∥w + ϵv∥2 = _√1 + ϵ[2]_ = 1 + O(ϵ[2]). Therefore, with sufficiently small ϵ, we have

**y[T]** (Xwϵ)+ _> y[T]_ (Xw)+. (122)
_≥_ **[y][T][ (]√[Xw]1 +[)] ϵ[+][2][ +][ ϵ]**

We then consider the case where y[T] (Xw)+ < 0. Apparently, we can make y[T] (Xw)+ larger by
replacing w by (1 − _ϵ)w, where ϵ ∈_ (0, 1), which leads to a contradiction.

Finally, we consider the case where y[T] (Xw)+ = 0. This implies that

(x[T]j **[w][)][+]** [=] (x[T]j **[w][)][+][.]** (123)
_n:Xyn=1_ _n:yXn=−1_

As (Xw)+ = 0, this implies that there exists at least for one index n [N ] such that yn = 1 and
**x[T]n** **[w][ >][ 0][. Let] ̸** **[ v][ =][ x][n][/][∥][x][n][∥][2][. We note that]** **w+1ϵv** 2 **[y][T][ (][X][(][w][ +][ ϵ][v][))] ∈[+][ >][ 0][ for][ ϵ >][ 0][. This leads]**

_∥_ _∥_
to a contradiction.


-----

D.6 PROOF OF PROPOSITION 7

It is sufficient to consider the case of the local maximizer. Denote Q = {σ ∈{−1, 0, 1}[N] _|diag(σ) ∈_
_P}is open if. For σ σ, σn_ _[′]= 0∈Q for, we say n_ [N σ] ⊆. Defineσ[′] if for all index n ∈ [N ] with σn ̸= 0, σn[′] [=][ σ][n][. We say][ σ][ ∈Q]
_̸_ _∈_

_Sσ = {u|sign(Xu) = σ}._ (124)

We start with the two lemmas.

**Lemma 8 Let λ ∈** R[N] _. Suppose that u0 satisfies that u0 =_ _∥gg((uu00,,λλ))∥2_ _[. Let][ σ][ =][ sign][(][u][0][)][. Then,]_

**v ∈B2 is a local maximizer of λ[T]** (Xu)+ in B2 if for any open σ[′] _satisfying σ ⊆_ **_σ[′], we have_**
_∥g(σ, y)∥2 = ∥g(σ[′], y)∥2._

PROOF Suppose that σ is open. Then, Sσ is an open set. In a small neighbor around u0 =
**g(u0,λ)** **g(σ,λ)**

_∥g(u0,λ)∥2_ [=] _∥g(σ,λ)∥2_ [,][ λ][T][ (][Xu][)][+][ =][ u][T][ g][(][σ][,][ λ][)][ is a linear function of][ u][. The Riemannian]

gradient of u[T] **g(σ, λ) at v is zero. This implies that v locally maximizes λ[T]** (Xu)+.

Suppose that there exists at least one zero in σ. Consider any v satisfying u[T]0 **[v][ = 0][. Let][ ϵ >][ 0]**
_∈B_ **u+sv**
be a small constant such that for any s ∈ (0, ϵ], u0 + sv ∈ _Sσ[′] where σ ⊆_ _σ[′]. Let us =_ _√1+s[2][ .]_

Suppose thatσ ⊆ **_σ[′], we construct ∥g(σ[′′], λ) σ∥2[′′] ≤∥by σg([′′]iσ,[=] λ[ −])∥2[1] for all open[ for][ n][ ∈]_** [[][N] σ[]][ such that][′′] satisfying[ σ] σn[′] _⊆[= 0]σ[ and][′′]. For any[ σ]n[′′]_ [=] σ[ σ][′] _n[′]with[for]_
_n_ [N ] such that σn[′] [= 0][. We note that][ ∥][g][(][σ][′′][,][ λ][)][∥][2]
_∈_ _[≥∥][g][(][σ][′][,][ λ][)][∥][2][. Thus,][ ∥][g][(][σ][′][,][ λ][)][∥][2]_ _[≤]_
**g(σ[′′], λ)** 2 **g(σ, λ)** 2. As **_λ[T]_** (Xus)+ = **g(σ[′], λ)[T]** **us** **g(σ[′], λ)** 2, we have
_∥_ _∥_ _≤∥_ _∥_ _|_ _|_ _|_ _| ≤∥_ _∥_
**_λ[T]_** (Xus)+ **g(σ[′], λ)** 2 **g(σ, λ)** 2. Therefore, u is a local maximizer of λ[T] (Xu)+.
_|_ _| ≤∥_ _∥_ _≤∥_ _∥_

_Suppose thatLemma 9 Suppose that the dataset is orthogonal separable. Let u0 satisfies that u0 =_ _∥gg((uu00,,λλ))∥2_ _[. Then, for any] λ ∈[ σ][′]R[ satisfying][N]_ _satisfy that[ σ][ ⊆] diag[σ][′][, we have](y)λ ≥_ 0.

_∥g(σ[′], λ)∥2 = ∥g(σ, λ)∥2._

PROOF If there exists n ∈ [N ] such that σn = 1 and yn = −1, as the data is orthogonal separable,
we note that


**x[T]n** **[g][(][σ][,][ λ][) =][ x]n[T]** _λn′_ **xn′** = yn(ynxn)[T] (λn′ _yn′_ )yn′ **xn′**

  

_n[′]:σn′_ _>0_ _n[′]:σn′_ _>0_

X X
  

which contradicts with sign(x[T]n **[g][(][σ][,][ λ][)) =][ sign][(][x][T]n** **[u][0][) =][ σ][n]** [= 1][.]


**x[T]n** **[g][(][σ][,][ λ][) =][ x]n[T]**


0, (125)

 _≤_


_λn′_ **xn′**
_n[′]:σn′_ _>0_

X


Suppose that there exists n ∈ [N ] such that σn and yn = 1. Then, as the dataset is orthogonal
separable, then, for index n1 ∈ [N ] such that σn1 = 0, we note that yn1 ̸= 1. Otherwise,

**x[T]n1** **[g][(][σ][,][ λ][) =][ x]n[T]1** _λn2_ **xn2** = x[T]n1 (λn2 _yn2_ )yn2 **xn2** _> 0,_ (126)

   

_n2:σn2_ _>0_ _n2:σn2_ _>0_

X X
   

which contradicts with sign(x[T]n1 **[g][(][σ][,][ λ][)) =][ sign][(][x]n[T]1** **[u][0][) =][ σ][n]1** [= 0][. This also implies that the]
index set {n ∈ [N ]|σn > 0} include all data with yn = 1.

If there exists σ[′] such that σ ⊆ **_σ[′]_** and ∥g(σ[′], λ)∥2 > ∥g(σ, λ)∥2. Then, there exists at least one
indexyn = − n ∈1 and[N ] such that σn ≤ 0 and σn[′] [= 1][. However, from the previous derivation, we note that]


= x[T]n






_< 0,_ (127)






**x[T]n** **[g][(][σ][′][,][ λ][) =][ x][T]n**


_λn1_ **xn1**
_j:σ[′]n1_ _[>][0]_

X


(λn1 _yn1_ )yn1 **xn1**
_n1:σ[′]n1_ _[>][0]_

X


which contradicts with σn[′] [= 1][.]


By combining Lemma 8 and 9, we complete the proof.


-----

D.7 PROOF OF THEOREM 4

PROOF For almost all initialization, we can find two neurons such that sign(w2,i+ ) =
**sign(y[T]** (Xw1,i+ )+) = 1 and sign(w2,i− ) = **sign(y[T]** (Xw1,i− )+) = _−1 at initializa-_
tion. By choosing a sufficiently small δ _> 0 in Proposition 6, there exist two neurons_
**w1,i+** _, w1,i−_ and times T+, T− _> 0 such that cos ∠(w1,i+_ (T+), g(w1,i+ (T+), y)) > 1 − _δ and_
cos ∠(w1,i+ (T+), g(w1,i+ (T+), y)) < −(1 − _δ). This implies that w1,i+_ (T+) and w1,i− (T+) are
sufficiently close to certain stationary points of gradient flow maximizing/minimizing y[T] (Xu+)
over B, i.e., {u ∈B| cos(u, g(u, y)) = ±1}. As the dataset is orthogonal separable, according to
Lemma 4 and Proposition 7, the corresponding diagonal matrices **D[ˆ]** _i+_ (T+) and **D[ˆ]** _i−_ (T−) satisfy
that **D[ˆ]** _i+_ (T+) **diag(I(y = 1)) and** **D[ˆ]** _i_ (T ) **diag(I(y =** 1)). According to Lemma 3 in
_≥_ _−_ _−_ _≥_ _−_
(Phuong & Lampert, 2021), we have **D[ˆ]** _i+_ (t) **diag(I(y = 1)) and Di** (t) **diag(I(y =** 1))
_≥_ _−_ _≥_ _−_
hold for t ≥ max{T+, T−}.

With t →∞, according to Proposition 4, the dual variable λ in the KKT point of the non-convex
max-margin problem (13) is dual feasible, i.e., λ satisfies (16). Suppose that θ[∗] is a limiting point
of _∥θθ((tt))∥2_ _t_ 0 [and][ λ][∗] [is the corresponding dual variable. From Theorem 1, we note that the pair]

_≥_

(θ[∗]n, λ[∗]) corresponds to the KKT point of the convex max-margin problem (14).o


E PROOFS OF MAIN RESULTS ON MULTI-CLASS CLASSIFICATION

E.1 PROOF OF PROPOSITION 1

The neural network training problem (4) can be separated into K subproblems. Each of these
subproblems corresponds to the neural network training problem (3) for binary classification. For
each subproblem, by applying Proposition 2, we complete the proof.

E.2 PROOF OF THEOREM 1

We note that the neural network training problem (4) can be separated into K subproblems. Each of
these subproblems corresponds to the neural network training problem (3) for binary classification.
By applying Proposition 6 with to each subproblem with y = yk, we complete the proof.

E.3 PROOF OF THEOREM 2

Similarly, the corresponding non-convex max-margin problem (5) and the convex max-margin
problem (7) can be separated into K subproblems. Each of these subproblems corresponds to the nonconvex max-margin problem (2) and the convex max-margin problem (14) for binary classification.
By applying Theorem 4 to each subproblem with y = yk, we complete the proof.

F NUMERICAL EXPERIMENT

F.1 DETAILS ON FIGURE 5

We provide the experiment setting in Figure 1 and 5 as follows. The dataset is given by X =
1.65 0.47 1
_−_ R[2][×][2] and y = R[2]. Here we have N = 2 and d = 2. We note that this
0.47 1.35 _∈_ 1 _∈_
−  − 

dataset is orthogonal separable but not spike-free. We plot the ellipsoid set and the rectified ellipsoid
set in Figure 6.


-----

2 2

1 1

0 0

1 1

2 2

2 1 0 1 2 2 1 0 1 2


Figure 6: The ellipsoid set and the rectified ellipsoid set. Orthogonal separable dataset.

We enumerate all possible hyperplane arrangements in the set P and solve the convex max-margin
problem (14) via CVXPY to obtain the following non-zero neurons


0.58
**u1,3 =** _,_
−0.16

We note that the dual problem (15) is equivalent to

max λ[T] **y,**


0.23
**w1[′]** _,2_ [=] _−0.66_



(128)

(129)


s.t. **X[T]** **Djλ** **X[T]** (2Dj _I)zj,+_ 2 1, _j_ [p],
_∥_ _−_ _−_ _∥_ _≤_ _∀_ _∈_

_∥−_ **X[T]** **Djλ −** **X[T]** (2Dj − _I)zj,−∥2 ≤_ 1, ∀j ∈ [p],
**zj,+ ≥** 0, zj,− _≥_ 0, ∀j ∈ [p], diag(y)λ ≥ 0.


The above problem is a second-order cone program (SOCP) and can be solved via standard convex
optimization frameworks such as CVX and CVXPY. We solve (129) to obtain the optimal dual
variable λ. For the geometry of the dual problem, as the dataset is orthogonal separable, the set
**_λ : max_** **u** 2 1 **_λ[T]_** (Xu)+ 1 reduces to **_λ : max_** **u** 2 1 **_λ[T]_** (Xu[∗]1[)][+][| ≤] [1][,][ λ][T][ (][Xu][∗]2[)][+][| ≤]
_{_ _∥_ _∥_ _≤_ _|_ _| ≤_ _}_ _{_ _∥_ _∥_ _≤_ _|_
1}, where u[∗]1[,][ u][∗]2 [correspond to two vectors at the spikes of the rectified ellipsoid set. We draw the]
setsFigure 2. {λ : max∥u∥2≤1 |λ[T] (Xu)+| ≤ 1}, {λ :, the optimal dual variable λ and the direction of y in

For eachconstraints Dju ∈Pj 2, we solve for the vector1 and (2Dj _I)Xuj_ **u0. We plot the rectified ellipsoid setj which maximize/minimize λ[T]** **D(jXuXu)j+ with theu** 2
1, vectors ∥ uj, neurons in the optimal solution to∥ _≤_ _−_ _≥_ (14) scaled to unit ℓ2-norm and the direction of { _|∥_ _∥_ _≤ λ_
_}_
in Figure 1. We note that each neuron u[∗]j [in the optimal solution from][ (14)][ (scaled to unit][ ℓ][2][-norm)]
maximize/minimize the corresponding λ[T] **DjXuj given (2Dj** _I)Xu[∗]j_
_−_ _[≥]_ [0][.]

Then, we consider a two-layer ReLU network with m = 10 neurons and apply the gradient descent
method to train on the logistic loss (3). Let ˆw1,i = _∥ww11,i,i∥2_ [for][ i][ ∈] [[][m][]][. We plot][ ˆ]w1,i and (X ˆw1,i)+

at iteration {10[l]|l = 0, . . ., 4} along with neurons in the optimal solution to (14) scaled to unit
_ℓ2-norm in Figure 5. Certain neurons do not move, while the activated neurons trained by gradient_
descent tend to converge to the direction of the neurons in the optimal solution to (14).

We repeat the training on the logistic loss (3) with the gradient descent method several times and we
plot the trajectories in Figure 7.


-----

|Col1|cone boundary 1-th neuron 2-th neuron 3-th neuron 4-th neuron 5-th neuron 6-th neuron 7-th neuron 8-th neuron 9-th neuron 10-th neuron optimal neuron|
|---|---|

|Col1|cone boundary 1-th neuron 2-th neuron 3-th neuron 4-th neuron 5-th neuron 6-th neuron 7-th neuron 8-th neuron 9-th neuron 10-th neuron optimal neuron|
|---|---|

|Col1|cone boundary 1-th neuron 2-th neuron 3-th neuron 4-th neuron 5-th neuron 6-th neuron 7-th neuron 8-th neuron 9-th neuron 10-th neuron optimal neuron|
|---|---|

|1.0 0.5 0.0 0.5 1.0 1.0 0. 1.0 0.5 0.0 0.5 1.0 1.0 0. 1.0 0.5 0.0 0.5 1.0 1.0 0. 1.0 0.5 0.0 0.5 1.0 1.0 0.|Col2|cone boundary 1-th neuron 2-th neuron 3-th neuron 4-th neuron 5-th neuron 6-th neuron 7-th neuron 8-th neuron 9-th neuron 10-th neuron optimal neuron|
|---|---|---|


2.00

optimal

y[T]u = 0

1.75

trained (Xw1, i) +

1.50 optimal (Xw1,[*] i[)][ +]

1.25

1.00

0.75

0.50

0.25

0.00

0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00

2.00

optimal

y[T]u = 0

1.75

trained (Xw1, i) +

1.50 optimal (Xw1,[*] i[)][ +]

1.25

1.00

0.75

0.50

0.25

0.00

0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00

2.00

optimal

y[T]u = 0

1.75

trained (Xw1, i) +

1.50 optimal (Xw1,[*] i[)][ +]

1.25

1.00

0.75

0.50

0.25

0.00

0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00

2.00

optimal

y[T]u = 0

1.75

trained (Xw1, i) +

1.50 optimal (Xw1,[*] i[)][ +]

1.25

1.00

0.75

0.50

0.25

0.00

0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00


Figure 7: Multiple independent random initializations of gradient descent trajectories on the same
h l bl d


-----

F.2 EXPERIMENT ON SPIKE-FREE DATASET

1.65 0.47
We repeat the previous numerical experiment on a non-spike-free dataset: X =
0.47 1.35



R[2][×][2] and y =


_∈_ R[2]. Similarly, we plot the ellipsoid set and the rectified set in Figure 8.


2 2

1 1

0 0

1 1

2 2

2 1 0 1 2 2 1 0 1 2


Figure 8: The ellipsoid set and the rectified ellipsoid set for a non-spike-free dataset.

We enumerate all possible hyperplane arrangements in the set P and solve the convex max-margin
problem (14) via CVXPY to obtain the following non-zero neuron


0.43
**u1,4 =**
0.59



(130)


We plot the rectified ellipsoid set (Xu)+ **u** 2 1, vectors uj, neurons in the optimal solution to
(14) scaled to unit ℓ2-norm and the direction of { _|∥_ _∥_ _≤ λ in Figure 9. We also plot}_ ˆw1,i and (X ˆw1,i)+ at
iteration 10[l] _l = 0, . . ., 4_ along with neurons in the optimal solution to (14) scaled to unit ℓ2-norm
_{_ _|_ _}_
in Figure 10.

2.0

1.5

1.0

0.5

0.0

optimal

y[T]u = 0

0.5 maximal

minimal

optimal (Xw1,[*] i[)][ +]

1.0

1.0 0.5 0.0 0.5 1.0 1.5 2.0


Figure 9: Recitified Ellipsoidal set and corresponding extreme points for a non-spike-free dataset.


-----

|cone b 1-th n 2-th n 3-th n 4-th n 5-th n 6-th n 7-th n 8-th n 9-th n 10-th optim|oundary euron euron euron euron euron euron euron euron euron neuron al neuron|
|---|---|

|cone b 1-th n 2-th n 3-th n 4-th n 5-th n 6-th n 7-th n 8-th n 9-th n 10-th optim|oundary euron euron euron euron euron euron euron euron euron neuron al neuron|
|---|---|

|1.0 cone b 0.5 1-th n 2-th n 3-th n 4-th n 5-th n 0.0 6-th n 7-th n 8-th n 9-th n 0.5 10-th optim 1.0 1.0 0.5 0.0 1.0 cone b 0.5 1-th n 2-th n 3-th n 4-th n 5-th n 0.0 6-th n 7-th n 8-th n 9-th n 0.5 10-th optim 1.0 1.0 0.5 0.0 1.0 cone b 0.5 1-th n 2-th n 3-th n 4-th n 5-th n 0.0 6-th n 7-th n 8-th n 9-th n 0.5 10-th optim 1.0 1.0 0.5 0.0|cone b 1-th n 2-th n 3-th n 4-th n 5-th n 6-th n 7-th n 8-th n 9-th n 10-th optim|oundary euron euron euron euron euron euron euron euron euron neuron al neuron|
|---|---|---|


2.00

optimal

y[T]u = 0

1.75

trained (Xw1, i) +

1.50 optimal (Xw1,[*] i[)][ +]

1.25

1.00

0.75

0.50

0.25

0.00

0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00

2.00

optimal

y[T]u = 0

1.75

trained (Xw1, i) +

1.50 optimal (Xw1,[*] i[)][ +]

1.25

1.00

0.75

0.50

0.25

0.00

0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00

2.00

optimal

y[T]u = 0

1.75

trained (Xw1, i) +

1.50 optimal (Xw1,[*] i[)][ +]

1.25

1.00

0.75

0.50

0.25

0.00

0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00


Figure 10: Multiple independent random initializations of gradient descent trajectories on the same
non-spike-free dataset. Note that the optimal extreme point (star), which is the uniquely optimal single
neuron is on the boundary of the main two-dimensional ellipsoid and not on the one-dimensional
spikes (projected ellipsoids). Also note that some neurons are stuck at spurious stationary points.


-----

