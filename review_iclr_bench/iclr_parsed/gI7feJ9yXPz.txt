# HIGH PROBABILITY GENERALIZATION BOUNDS WITH FAST RATES FOR MINIMAX PROBLEMS

**Shaojie Li[1,2], Yong Liu[1,2,][∗]**

1Gaoling School of Artificial Intelligence, Renmin University of China, Beijing, China
2Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing, China
2020000277@ruc.edu.cn, liuyonggsai@ruc.edu.cn

ABSTRACT

Minimax problems are receiving an increasing amount of attention in a wide range
of applications in machine learning (ML), for instance, reinforcement learning, robust optimization, adversarial learning, and distributed computing, to mention but
a few. Current studies focus on the fundamental understanding of general minimax
problems with an emphasis on convergence behavior. As a comparison, there is
far less work to study the generalization performance. Additionally, existing generalization bounds are almost all derived in expectation, and the high probability
bounds are all presented in the slow order O(1/[√]n), where n is the sample size.
In this paper, we provide improved generalization analyses and obtain sharper
high probability generalization bounds for most existing generalization measures
of minimax problems. We then use the improved learning bounds to establish
high probability generalization bounds with fast rates for classical empirical saddle point (ESP) solution and several popular gradient-based optimization algorithms, including gradient descent ascent (GDA), stochastic gradient descent ascent (SGDA), proximal point method (PPM), extra-gradient (EG), and optimistic
gradient descent ascent (OGDA). In summary, we provide a systematical analysis
of sharper generalization bounds of minimax problems.

1 INTRODUCTION

Minimax learning problems have achieved great success over a broad range of learning tasks in machine learning, with examples including reinforcement learning (Du et al., 2017; Dai et al., 2018),
robust optimization (Chen et al., 2017; Namkoong & Duchi, 2017), adversarial learning (Goodfellow et al., 2014), distributed computing (Razaviyayn et al., 2020; Shamma, 2008; Mateos et al.,
2010), and AUC maximization (Lei & Ying, 2021b), to just name a few. This framework is formulated as a zero-sum game characterized as two groups of decision variables, one for minimization
and one for maximization. The coupling of the two groups of variables makes analysis of minimax
problems more complex than the standard statistical learning theory setting, with only one minimization operator (Liu et al., 2021b; Yin et al., 2020; Li & Liu, 2021; Li et al., 2018; Liu et al.,
2020; Li & Liu, 2021). Researchers have designed various optimization algorithms, for instance,
gradient descent ascent (GDA), stochastic gradient descent ascent (SGDA), proximal point method
(PPM), extra-gradient (EG), and optimistic gradient descent ascent (OGDA), to solve the minimax
optimization problem (Farnia & Ozdaglar, 2021). Current theoretical research in ML literature is
mainly devoted to the convergence rate and optimality of these minimax optimization algorithms in
different setting, such as convex-concave settings (Nemirovski et al., 2008), nonconvex-concave setting (Rafique et al., 2018), strongly convex-strongly-concave setting (Balamurugan & Bach, 2016),
and nonconvex-nonconcave setting (Liu et al., 2021a; Yang et al., 2020). In contrast, there is far
less work on the generalization performance analysis, which is an important measure to indicate the
performance of the learned model based on training samples when generalized to the test data.

To the best of our knowledge, there is only three work on the generalization bounds of minimax optimization algorithms (Zhang et al., 2021a; Farnia & Ozdaglar, 2021; Lei et al., 2021). Among them,

_∗Corresponding Author._


-----

(Zhang et al., 2021a) studies the generalization bounds for ESP solution to minimax problems, (Farnia & Ozdaglar, 2021) analyzes the generalization properties of several gradient-based optimization
algorithms: GDA, SGDA, GDmax and PPM, and (Lei et al., 2021) provides a systematical generalization analysis of SGDA. However, in the above-mentioned papers, almost all generalization
bounds are derived in expectation. Only two high probability bounds exist, proposed in (Lei et al.,
2021). Unfortunately, they are of the slow order O (1/[√]n).

It is known that the high probability bound is beneficial to understand the robustness of optimization
algorithms (Bousquet et al., 2020; Klochkov & Zhivotovskiy, 2021) and is much more challenging
to be derived (Bousquet et al., 2020; Lei et al., 2021; Lv et al., 2021). In this paper, our goal is
to provide the sharper high probability generalization bounds for minimax learning problems. We
leverage the lens of algorithmic stability, which is also served as an important tool in (Zhang et al.,
2021a; Farnia & Ozdaglar, 2021; Lei et al., 2021). Our contributions are summarized below.

1. In view of the coupling construction between the minimization variable and the maximization
variable, minimax learning problems have many generalization measures (Lei et al., 2021; Farnia
& Ozdaglar, 2021; Zhang et al., 2021a). In this paper, we provide improved stability analyses for
almost all existing generalization measures, based on which we establish sharper high probability
generalization bounds. These developed learning bounds can be employed to derive generalization
bounds with fast rates for stable minimax learning algorithms.

2. The generalization performance of the ESP solution and gradient-based optimization algorithms
stands a central place in the learning theory of minimax problems (Lei et al., 2021). In this paper, we
develop high probability generalization bounds with fast rates for ESP solution and several popular
gradient-based optimization algorithms: GDA, SGDA, PPM, EG, and OGDA. Overall, we provide
a systematical analysis of sharper generalization bounds for minimax learning problems.

2 RELATED WORK

**Algorithmic stability. Algorithmic stability is a fundamental concept in learning theory (Bousquet**
& Elisseeff, 2002), which has a deep connection with learnability (Rakhlin et al., 2005; ShalevShwartz & Ben-David, 2014; Shalev-Shwartz et al., 2010). A training algorithm is stable if small
changes in the training set lead to small differences in the output predictions of the trained model.
Different algorithmic stability measures have been developed, including uniform stability (Bousquet
& Elisseeff, 2002; Feldman & Vondrak, 2018; 2019; Klochkov & Zhivotovskiy, 2021; Hardt et al.,
2016; Lei et al., 2020), uniform argument stability (Liu et al., 2017; Bassily et al., 2020), hypothesis
stability (Bousquet & Elisseeff, 2002; Charles & Papailiopoulos, 2018), hypothesis set stability (Foster et al., 2019), on average stability (Shalev-Shwartz et al., 2010; Lei & Ying, 2020; Kuzborskij &
Lampert, 2018; Zhang et al., 2021b; Lei & Ying, 2021a), locally elastic stability (Deng et al., 2021),
collective stability (London et al., 2016), and PAC-Bayesian stability (Li et al., 2020). These stability measures have been extensively studied in the generalization analysis of the standard statistical
learning theory setting (Chen et al., 2018; Zhang et al., 2021b). Several stability measures have also
been extended to minimax learning problems, for instance, weak stability, argument stability, and
uniform stability (Farnia & Ozdaglar, 2021; Zhang et al., 2021a; Lei et al., 2021). In related work
(Farnia & Ozdaglar, 2021; Zhang et al., 2021a; Lei et al., 2021), they mostly focus on the expectation form of these stability measures since they are to derive bounds in expectation. In this paper,
we will focus on the last two measures, which are often used when establishing high probability
generalization bounds (Feldman & Vondrak, 2018; 2019; Klochkov & Zhivotovskiy, 2021).

**Convergence analysis. Convergence analysis has been widely studied in different settings, includ-**
ing convex-concave learning (Nemirovski, 2005; Nedic & Ozdaglar, 2009; Mokhtari et al., 2020;
Cherukuri et al., 2017; Mokhtari et al., 2019; Balamurugan & Bach, 2016; Hsieh et al., 2019; Yan
et al., 2020; Lin et al., 2020b; Wang & Li, 2020; Yoon & Ryu, 2021), nonconvex-concave learning
(Rafique et al., 2018; Kong & Monteiro, 2019; Luo et al., 2020; Grnarova et al., 2017; Thekumparampil et al., 2019; Lu et al., 2020; Namkoong & Duchi, 2016; Sanjabi et al., 2018; Nouiehed et al.,
2019; Lin et al., 2020a; Sinha et al., 2017; Chen et al., 2021), and nonconvex-nonconcave learning
(Heusel et al., 2017; Balduzzi et al., 2018; Daskalakis & Panageas, 2018; Mertikopoulos et al., 2019;
Loizou et al., 2020; Yang et al., 2020; Liu et al., 2021a; Lin et al., 2018; Diakonikolas et al., 2021;
Wang et al., 2020; Loizou et al., 2021; Fiez & Ratliff, 2021). There are so many studies on convergence. Thus, considering the length limit, the references listed here are not complete. Please refer


-----

to the related references concerning the above work. We investigate the generalization performance
of minimax problems instead of the convergence behavior. Note that the convergence analysis also
plays an essential role in this paper, formalized as strong PD empirical risk (please refer to Definition 1), which is defined on the function value difference and referred to as optimization error or
primal-dual gap in some convergence literature (Lei et al., 2021; Nemirovski, 2005; Mokhtari et al.,
2019; 2020).

3 PRELIMINARIES

Let X and Y be two parameter spaces in R[d]. Let P be a probability measure defined on a sample
space Z. We define f : X ×Y ×Z 7→ R and consider the following minimax optimization problem

min (1)
**x** [max]y _[F]_ [(][x][,][ y][) :=][ E][z][∼][P][[][f] [(][x][,][ y][;][ z][)]][.]
_∈X_ _∈Y_

The above minimax objective represents an expectation of a cost function f (x, y; z) for minimization variable x, maximization variable y and data variable z. Unfortunately, we typically are not
available to the underlying distribution P. In practice, F is approximated by the corresponding empirical risk. Let S = {z1, ..., zn} be a dataset whose samples are independent drawn according to P,
the empirical risk is defined as


_FS(x, y) = [1]_


_f_ (x, y; zi). (2)
_i=1_

X


Let the output of a (randomized) algorithm A on a dataset S be A(S) := (Ax(S), Ay(S)) .
_∈X ×Y_
Since A(S) is just an empirical approximated solution of the true minimax optimization problem,
we are interested in studying how well A(S) generalizes to the unseen data. As claimed in (Farnia
& Ozdaglar, 2021; Lei et al., 2021), the coupling between the minimization variable and the maximization variable in (1) makes minimax problems have many different generalization performance
measures. These measures are collected in (Lei et al., 2021). For better readability, we use their
symbols. Let E be the expectation with respect to (w.r.t.) the randomness of algorithm A and the
dataset S. These generalization measures are listed below.
**Definition 1. (Lei et al., 2021) There are four groups of generalization measures.**

_1 (Primal Measures.)_ _The primal population risk of a model x is defined as R(x)_ =
supy _F_ (x, y), _and the corresponding primal empirical risk is defined as RS(x)_ =
_∈Y_
supy _FS(x, y). Then, when using empirical risk RS(x) to bound R(x), we call this error of_
_∈Y_
_the model x the primal generalization error. While using optimal inf_ **x** _R(x) to bound R(x), we_
_∈X_
_call this error of the model x the excess primal population risk._

_2 (Plain Measure.) When using FS(x, y) to bound F_ (x, y), we call the this error of a model (x, y)
_the plain generalization error._

_3 (Strong Measures.) The strong primal-dual (PD) population risk of a model (x, y) is defined as_

(x, y) = sup _F_ (x, y[′]) inf
_△[s]_ **y[′]∈Y** _−_ **x[′]∈X** _[F]_ [(][x][′][,][ y][)][,]

_and the corresponding strong PD empirical risk is defined as_

_S[(][x][,][ y][) = sup]_ _FS(x, y[′])_ inf
_△[s]_ **y[′]∈Y** _−_ **x[′]∈X** _[F][S][(][x][′][,][ y][)][.]_

_Then, the strong PD generalization error △[s](x, y) −△S[s]_ [(][x][,][ y][)][ of the model][ (][x][,][ y][)][ is defined as]
sup _F_ (x, y[′]) sup _FS(x, y[′])_ + inf _._
**y[′]∈Y** _−_ **y[′]∈Y** **x[′]∈X** _[F][S][(][x][′][,][ y][)][ −]_ **x[inf][′]∈X** _[F]_ [(][x][′][,][ y][)]
   

_4 (Weak Measures.) The weak PD population risk of a (randomized) model (x, y) is defined as_


(x, y) = sup E[F (x, y[′])] inf
_△[w]_ **y[′]∈Y** _−_ **x[′]∈X** [E][[][F] [(][x][′][,][ y][)]][,]

_and the corresponding weak PD empirical risk is defined as_

_S_ [(][x][,][ y][) = sup] E[FS(x, y[′])] inf
_△[w]_ **y[′]∈Y** _−_ **x[′]∈X** [E][[][F][S][(][x][′][,][ y][)]][.]


-----

_Then, the weak PD generalization error △[w](x, y) −△S[w][(][x][,][ y][)][ of the model][ (][x][,][ y][)][ is defined as]_
sup E[F (x, y[′])] sup E[FS(x, y[′])] + inf _._
**y[′]∈Y** _−_ **y[′]∈Y** **x[′]∈X** [E][[][F][S][(][x][′][,][ y][)]][ −] **x[inf][′]∈X** [E][[][F] [(][x][′][,][ y][)]]
   

**Remark 1. We provide some discussions for the four groups of measures. (1. Primal Measures:)**
In the context of GANs, the primal population risk R(x) represents a divergence measure between
the learned and true distributions, and in the context of adversarial training it represents the learner’s
risk under adversarial perturbations (Farnia & Ozdaglar, 2021). One would be interested in the relationship between R(x) and its corresponding empirical risk RS(x), and the relationship between
_R(x) and its infimum inf_ **x′∈X R(x[′]). (2. Plain Measure:) This generalization measure is a direct**
extension of the standard generalization error in the minimization optimization. (3. Strong Measures:) △S[s] [(][x][,][ y][)][ is referred to as the primal-dual gap in the optimization literature.][ △][s][(][x][,][ y][)][ is]
the primal-dual gap of the population risk. △[s](x, y) −△S[s] [(][x][,][ y][)][ studies the difference between]
the population primal-dual gap and its empirical counterpart. (4. Weak Measures:) The difference
between the strong and weak measures is that weak measures take the expectation over the randomness of the dataset and the algorithm, for instance, supy′∈Y F (x, y[′]) − inf **x′∈X F** (x[′], y) in the
strong measures and supy′∈Y E[F (x, y[′])] − inf **x′∈X E[F** (x[′], y)] in the weak measures. Therefore,
the upper bounds of weak measures hold in expectation, while the upper bounds of strong measures
hold uniformly for any dataset.

Denote the Lp norm of a random variable Z as ∥Z∥p = (EZ|Z|[p])[1][/p]. Let ∥·∥ be the Euclidean norm
and ⟨·, ·⟩ be the inner product. A differentiable function g : W 7→ R is called µ-strongly-convex in
**w if the following inequality holds for every w1, w2:**

_g(w1)_ _g(w2)_ _g(w2), w1_ **w2** + _[µ]_
_−_ _≥⟨∇_ _−_ _⟩_ 2 _[∥][w][1][ −]_ **[w][1][∥][2][,]**

where ∇ is the gradient operator. We say g is µ-strongly-concave if −g is µ-strongly-convex.
**Definition 2. Let g : X × Y 7→** R. Assume that X and Y are convex feasible sets. Then

_1. g is µ-strongly-convex-strongly-concave (µ-SC-SC) if g(·, y) is µ-strongly-convex for any y ∈Y_
_and g(x, ·) is µ-strongly-concave for any x ∈X_ _._

_2. g is convex-concave (C-C) if g is 0-SC-SC._

We then introduce the definition of algorithmic stability this paper used. Algorithmic stability plays
an important role in studying the generalization behavior of a learning algorithm. Intuitively, an
algorithm A : ( _,_ ) is said to be stable if the output model (Ax(S), Ay(S)) is insensitive
_Z_ _[n]_ _7→_ _X_ _Y_
to perturbations. Let S[′] be a neighboring dataset that differs at most one single example to S.
**Definition 3 (Algorithmic Stability). Let A be a learning algorithm and ϵ > 0.**

_1. We say A is ϵ-uniformly-stable if for any training datasets S, S[′]_ _∈Z_ _[n]_ _we have_
sup
_z_ [[][f] [(][A][x][(][S][)][, A][y][(][S][);][ z][)][ −] _[f]_ [(][A][x][(][S][′][)][, A][y][(][S][′][);][ z][)]][ ≤] _[ϵ.]_

_2. We say A is ϵ-argument-stable if for any training datasets S, S[′]_ _∈Z_ _[n]_ _we have_
_Ax(S)_ _Ax(S[′])_ + _Ay(S)_ _Ay(S[′])_ _ϵ._
_∥_ _−_ _∥_ _∥_ _−_ _∥≤_

From Definition 3, one can see that the uniform stability measures the sensitivity of the function
values, while the argument stability measures the sensitivity of the arguments.

We finally introduce two standard assumptions in minimax problems. Assumption 1 implies f is
Lipschitz continuous w.r.t. both x and y, while Assumption 2 implies f is smooth w.r.t. (x, y).
**Assumption 1 (Lipschitz continuity). Let L > 0. Assume that for any x ∈X** _, y ∈Y and z ∈Z,_
_f_ (x, y; z) satisfies
**xf** (x, y; z) _L_ _and_ **yf** (x, y; z) _L._
_∥∇_ _∥≤_ _∥∇_ _∥≤_

_fAssumption 2(x, y; z) satisfies (Smoothness). Let β > 0. Assume that for any x1, x2 ∈X_ _, y1, y2 ∈Y and z ∈Z,_

**xf** (x1, y1; z) **xf** (x2, y2; z) **x1** **x2**
_∇_ _−∇_ _−_ _[.]_
**yf** (x1, y1; z) **yf** (x2, y2; z) **y1** **y2**
∇ _−∇_  _[≤]_ _[β]_  _−_ 

Under Assumption 1, the argument stability implies the uniform stability. Therefore, the argument
stability is the main stability measure that we will focus on.


-----

4 MAIN RESULTS

In this section, we provide sharper high probability bounds for the generalization measures of Definition 1, shown as follows.

**Theorem 1. Let A be a learning algorithm and ϵ > 0. Suppose |f** (x, y; z)| ≤ _M for some M > 0_
_and x ∈X_ _, y ∈Y, z ∈Z. Fixed any η > 0. There exists an absolute positive constant C._

_(a.) If the algorithm A is ϵ-uniformly stable, then for any δ > 0, with probability at least 1 −_ _δ,_

_M_

_F_ (Ax(S), Ay(S)) (1 + η)FS(Ax(S), Ay(S)) + C [1 +][ η] _._
_≤_ _η_ _n_ [log(1][/δ][) +][ ϵ][ log][2][ n][ log 1]δ

 

_(b.) Assume that for all x, the function y 7→_ _F_ (x, y) is µ-strongly-concave. If the algorithm A is
_ϵ-argument stable and Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ,_

_M_ _β_

_R(Ax(S))_ (1 + η)RS(Ax(S)) + C [1 +][ η] _Lϵ log2 n log [1]_ _._
_≤_ _η_ _n_ [log 1]δ [+] _µ_ [+ 1] _δ_

   

_(c.) Assume that for all x and y, the function F_ (x, y) is µ-SC-SC. If the algorithm A is ϵ-argument
_stable and Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ,_

_△[s]_ (Ax(S), Ay(S)) ≤△S[s] [(][A][x][(][S][)][, A][y][(][S][)) +]L2(1 +[ η][E][S] η[△]) _S[s]_ [(][A][x][(][S][)][, A][y][(][S][))] 1

+ C(1 + η) + _[M]_ 1 + _[β]_ _ϵL log2 n_ log _._

_nµη_ _n_ [+] _µ_ _δ_

     

_(d.) Assume that for all x and y, the function F_ (x, y) is µ-SC-SC. If the algorithm A is ϵ-argument
_stable and Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ,_

_△[s]_ (Ax(S), Ay(S)) −△S[s] [(][A][x][(][S][)][, A][y][(][S][))]L[ ≤]2(1 +[η][E][S] η[△]) _S[s]_ [(][A][x][(][S][)][, A][y][(][S][))] 1

+ C(1 + η) + _[M]_ 1 + _[β]_ _ϵL log2 n_ log _._

_nµη_ _n_ [+] _µ_ _δ_

     


_(e.) Assume that for all x, the function y 7→_ _F_ (x, y) is µ-strongly-concave. If the algorithm A is
_ϵ-argument stable and Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ,_

_R(Ax(S))_ (1 + η) inf
_≤_ **x** _[R][(][x][)]_
_∈X_

_M_ _β_

+ C [2 +][ η] _Lϵ log2 n log [1]_ _S[(][A][x][(][S][)][, A][y][(][S][))]_ _._

_η_ _n_ [log 1]δ [+] _µ_ [+ 1] _δ_ [+][ △][s]

   


According to Definition 1 and Jensen’s inequality, we know that △[w](x, y) ≤ E[△[s](x, y)] and
_△S[w][(][x][,][ y][)][ ≤]_ [E][[][△]S[s] [(][x][,][ y][)]][. By this connection, we have][ △][w][(][x][,][ y][)][ −△][w]S [(][x][,][ y][)][ ≤] [E][[][△][s][(][x][,][ y][)] +]
_|E[△S[s]_ [(][x][,][ y][)]][|][. We therefore obtain the following results for][ △][w][(][x][,][ y][)][ and][ △][w][(][x][,][ y][)][ −△][w]S [(][x][,][ y][)][.]

**Corollary 1. Suppose the same conditions as Theorem 1 hold.**

_(f.) If the assumptions of Part (c) in Theorem 1 hold, then with probability at least 1 −_ _δ,_

_△[w]_ (Ax(S), Ay(S)) ≤ (1 + η)E △S[s] [(][A][x][(][S][)][, A][y][(][S][))]

_L2(1 + η)_ 1
+ C(1 + η) + _[M]_ 1 + _[β]_ _ϵL log2 n_ log _._

_nµη_ _n_ [+] _µ_ _δ_

     

_(g.) If the assumptions of Part (d) in Theorem 1 hold, then with probability at least 1 −_ _δ,_

_△[w]_ (Ax(S), Ay(S)) −△S[w][(][A][x][(][S][)][, A][y][(][S][))][ ≤|][E][ △]S[s] [(][A][x][(][S][)][, A][y][(][S][))][|]

_L2(1 + η)_ 1
+ (1 + η)E _S_ [(][A][x][(][S][)][, A][y][(][S][)) +][ C][(1 +][ η][)] + _[M]_ 1 + _[β]_ _ϵL log2 n_ log _._
_△[s]_ _nµη_ _n_ [+] _µ_ _δ_
     


-----

**Remark 2. In Theorem 1, we have established a quantitative connection between the generalization**
measures and the stability measures. The complete proof of Theorem 1 is provided in Appendix A.

Part (a) provides the relationship between the uniform stability and the plain generalization error of (Ax(S), Ay(S)). If the uniform stability of algorithm A is of fast order (1/n), then
_O_
_F_ (Ax(S), Ay(S)) is bounded by (1 + η) _FS(Ax(S), Ay(S)) +_ [log][ n][ log(1]nη _[/δ][)]_ . Usually for a
_O_

well-trained model (Ax(S), Ay(S)) over the training set, the empirical risk FS(Ax(S), Ay(S)) is

  

small or even zero (Lever et al., 2013; Yang et al., 2019; Cortes et al., 2021). If the empirical risk is
of orderbe of fast order O(1/n), then we can choose a proper constant for log n log(1n _/δ)_ . It is (1/n) when we hide the logarithmic term. In the related η and the plain generalization error will
_O_ _O_

work, (Lei et al., 2021) also establish the plain generalization error bound under the same assumptions, but their bound is of slow order   _O_ _ϵ log n log(1/δ) + Mn[−]_ [1]2 log(1/δ) . Even if they get

a sharper bound for stability measure ϵ, the influence of _n[−]_ 2[1] log(1/δ) can not disappear. By

  _O_ 

comparison, we have completely removed the (1/[√]n) term. Thus, our plain generalization error[p]
_O_   
bound enables the fast (1/n) rate when the empirical risk is small.[p]
_O_

Part (b) provides the connection between the argument stability and the primal generalization error.
Similar to the analysis of Part (a), if both the argument stability of algorithm A and RS(Ax(S))
are of order O(1/n), then the primal generalization error implies a fast O 1/n rate. Considering that we assume the function f is well-behaved, i.e., Lipschitz continuity, smoothness, and the
strong-concavity of its population risk F, and that RS(Ax(S)) is data-dependent, thus it is reason-  
able to assume RS(Ax(S)) is small for a well-trained model Ax(S) (Lever et al., 2013; Yang et al.,
2019; Cortes et al., 2021). In (Lei et al., 2021), they also establish a bound for primal generalization error under the same assumptions. However, their bound is O _Lβµ[−][1]ϵlogn log(1/δ) +_
_Mn[−]_ 2[1] log(1/δ), limited to the (1/[√]n) order. In contrast, we successfully removed the

_O_  
(1/[√]n) term, which makes the fast rate possible. (Farnia & Ozdaglar, 2021) studies the expected
_O_ 
primal generalization error, i.e., bounding[p] ES,A[R(Ax(S))] by ES,A[RS(Ax(S))]. They establish
the connection between the stability measure and the expected error under the same assumptions
as Part (b), which is then used to derive generalization bounds for (S)GDA, (S)GDmax, and PPM
algorithms. By comparison, our bound is derived in high probability.

Part (c) provides the relationship between the argument stability and the strong PD population risk.
If both the argument stability of algorithm A and the strong PD empirical risk are all of the order
_O(1/n), the strong PD population risk will be of the fast order O(1/n). Note that in our proof for_
the gradient-based optimization algorithms, the strong PD empirical risk mainly has a dependence
on the iterative number T (see Lemma 8 of GDA, Lemma 11 of SGDA, etc.). To obtain sharper
generalization bounds, we require T to be associated with n, such as T = O(n[2]) for GDA, the
strong PD empirical risk finally has a dependence on the sample size n. To our best knowledge,
this is the first high probability strong PD population risk bound. The expected version of this
risk is studied for the ESP solution in (Zhang et al., 2021a). However, the discussion there does
not establish the connection between stability and generalization. Their analysis is restricted to the
specific ESP problem. Under the same assumptions, they provide the upper bound of order O (1/n).
Compared with their result, our result is presented in high probability. Additionally, our strong PD
population risk bound is applicable for any stable minimax optimization algorithms.

Part (d) provides the connection between the argument stability and the strong PD generalization
error. Similarly, if both the argument stability of algorithm A and the strong PD empirical risk are
all of O(1/n) order, the strong PD generalization error will be of the fast order O(1/n). Although
Part (c) and Part (d) have a similar upper bound, they are different generalization measures (Lei
et al., 2021). To our best knowledge, this is also the first high probability strong PD generalization
error bound. The expected version of this generalization error is studied in (Lei et al., 2021), that
is ES,A [△[s](Ax(S), Ay(S)) −△S[s] [(][A][x][(][S][)][, A][y][(][S][))]][. Under the same assumptions, their expected]
strong PD generalization error is bounded by (1 + β/µ)L√2ϵ, which can also be used to obtain

_O (1/n) order rate when ϵ is of order O(1/n). However, this bound is provided for the expected_
error, while our bound is high probabilistic and holds uniformly for any dataset.

Part (e) provides the relationship between the argument stability and the excess primal population
risk. Similar to the analysis of Part (a) and Part (b), if the argument stability of algorithm A, the
strong PD empirical risk, and inf **x** _R(x) are all of the order_ (1/n), the excess primal population
_∈X_ _O_


-----

risk will also be of the fast order O(1/n). Meanwhile, in the minimization learning problems,
assuming the optimal population risk F _[∗]_ is small or even zero, i.e., F _[∗]_ _≤O(1/n), can be found_
in (Lei & Ying, 2021a; Zhang et al., 2017; Zhang & Zhou, 2019; Srebro et al., 2010; Lei & Ying,
2020). Note that the optimal population risk F _[∗]_ = O(1/n) just to show that the improved bound can
be got under low noise conditions. F _[∗]_ should be independent of n. Similar to the assumption on F _[∗]_
and considering that we assume the function f is well-behaved, it will also be reasonable to assume
inf **x** _R(x) is small. High probability excess primal population risk bound is also studied for_
_∈X_
SGDA in (Lei et al., 2021). Their bound, however, is of slow order O (β/µ)n[−] 2[1] log n log[2](1/δ)

and is restricted to SGDA. By comparison, our result in Part (e) enables (1/n) bounds for stable

  _O_ 

minimax learning algorithms since we have completely removed the O(1/[√]n) term.

We discuss a noteworthy difference between (Farnia & Ozdaglar, 2021; Lei et al., 2021) and ours.
In Part (a), Part (b), and Part (e), we study the upper bounds of F (Ax(S), Ay(S)), R(Ax(S)) (w.r.t.
_RS(Ax(S))), and R(Ax(S)) (w.r.t. inf_ **x** _R(x)), respectively, while (Lei et al., 2021; Farnia &_
_∈X_
Ozdaglar, 2021) study the upper bounds of F (Ax(S), Ay(S)) _FS(Ax(S), Ay(S)), R(Ax(S))_
_−_ _−_
_RS(Ax(S)), and R(Ax(S))_ inf **x** _R(x) (or their expected forms). One of our motivations to_
_−_ _∈X_
study such forms is that, in practice, we are often directly interested in the true risk, i.e., how the
learned models behave on the testing data, such as F (Ax(S), Ay(S)), instead of the error between
the true risk and empirical risk. Note that in the above comparison between Theorem 1 and the
results in (Lei et al., 2021; Farnia & Ozdaglar, 2021), we all take the right side of the generalization
bound inequalities to compare, which is fair since our bounds can be written as F (Ax(S), Ay(S))
_−_
_FS(Ax(S), Ay(S)) ≤_ _ηFS(Ax(S), Ay(S)) + C_ [1+]η _[η]_ [(][ M]n [log(1][/δ][) +][ ϵ][ log][2][ n][ log][ 1]δ [)][, etc.]

**Remark 3. From Remark 2, one can see that compared with (Zhang et al., 2021a; Farnia &**
Ozdaglar, 2021; Lei et al., 2021), we have established sharper high probability generalization
bounds. In the applications of Section 5, we will establish O(1/n) order bounds for two terms
in Theorem 1: stability measures and strong PD empirical risk. Hence, the strong PD population
risk and the strong PD generalization error will be of the fast order O(1/n) when applying Theorem
1 to these applications. These bounds are clearly of order O(1/n) and sharper than the results in
(Zhang et al., 2021a; Farnia & Ozdaglar, 2021; Lei et al., 2021). For the plain generalization error, the primal generalization error, and the excess primal population risk, to obtain O(1/n) order
bounds for these applications, we need to assume the extra corresponding terms F (Ax(S), Ay(S)),
_R(Ax(S)), and inf_ **x** _R(x) are of order_ (1/n), respectively. The clear motivation is that in
_∈X_ _O_
practice, learning algorithms achieve a small or even zero empirical risk, as discussed in Remark 2.

**Remark 4. This remark discusses η in Part (a), Part (b), and Part (e). (1:) When establishing sharper**
generalization error bound (i.e., Pf _Pnf_ ), the existence of η is common in the standard statistical
_−_
learning theory. Specifically, in the uniform localized convergence theory, the generalization error
bound in (Bartlett et al., 2005) is of the form Pf _η_ _η_ 1 _[P][n][f][ +][ O][(][ηr][∗]_ [+][ η][ log(1]n _[/δ][)]_ ) with η >
_≤_ _−_

1 (see Theorem 3.3 and Theorem 4.1). In the PAC-Bayesian theory, the generalization bounds
in (Catoni, 2007) (see Theorem 1.2.6), (Lever et al., 2013) (see Theorem 6), (Yang et al., 2019)
(see Proposition 3.1 and Theorem 4.3), etc., also have η. For instance, the Catoni’s bound is of
the form PQ 1 1e[−][η] _ηPnQ +_ ( _[KL][(][Q][∥][P rior]n_ [)+log(1][/δ][)] ) with η > 0 (Catoni, 2007). In the
_≤_ _−_ _O_

algorithmic stability theory, Theorem 1.2 in (Klochkov & Zhivotovskiy, 2021) is of the form Pf

   _≤_

(1 +bounds (Cortes et al., 2021), they also imply a multiplier η)Pnf + [1+]η _[η]_ _[O][((][ϵ][ log][ n][ +][ 1]n_ [) log(][ 1]δ [))][ with][ η >][ 0] η[. In the recent Cortes’s deviation margin]. The above bounds can be transformed

into the form of empirical risk multiplied by 1 + η, similar to our results. It is discussed in (Lever
et al., 2013; Yang et al., 2019; Cortes et al., 2021; Bartlett et al., 2005; Klochkov & Zhivotovskiy,
2021) that this type of generalization error bound can obtain a fast rate when the empirical risk
is small. Note that Part (e) also involves generalization error bounds due to the decomposition,
see (31). The above generalization error analysis thus holds for Part (e). (2:) Furthermore, in

1

(10), we show that F (Ax(S), Ay(S)) _FS(Ax(S), Ay(S))_ ( _[MF][ (][A][x][(][S][)][,A]n[y][(][S][)) log(1][/δ][)]_ ) 2 +
_−_ _≤O_

_ϵ log(_ [1]δ [)], where M means that |f (x, y; z)| ≤ _M, ∀x, y, z. Using the elementary inequality _ _√ab ≤_

_ηa +_ _η[1]_ _[b][ for any][ a, b, η >][ 0][ and by some rearrangements, the form of Part (a) appears. This is]_



the reason why η exists. The corresponding bound in (Lei et al., 2021) is F (Ax(S), Ay(S))
_−_
_FS(Ax(S), Ay(S)) ≤O_ _ϵ log n log(_ [1]δ [) +][ Mn][−] [1]2 log[1][/][2]( [1]δ [)] . Focusing on the dominated term,

it is clear that F (Ax(S), Ay(S)) _M since F_ (Ax(S), Ay(S)) is data-dependent, which implies

  _≪_ 

that our plain generalization error bound is sharper. Similar analysis holds for Part (b) and Part (e).


-----

|Reference|Algorithm|Assumption|Generalization Measure|Learning Bound|
|---|---|---|---|---|
|Zhang|ESP|SC-SC, Lip|Weak PD Risk|(1/n) O|
|||SC-SC, Lip, S|(E.) Strong PD Risk|(1/n) O|
||R-ESP|C-C, Lip|Weak PD Risk|√ (1/ n) O|
|Farnia|SGDA|SC-SC, Lip, S|(E.) Primal generalization|(1/n) O|
||SGDmax|SC-SC, Lip, S|(E.) Primal generalization|(1/n) O|
||GDA|SC-SC, Lip, S|(E.) Primal generalization|(1/n) O|
||GDmax|SC-SC, Lip, S|(E.) Primal generalization|(1/n) O|
||PPM|SC-SC, Lip, S|(E.) Primal generalization|(1/n) O|
||PPM|C-C, Lip, S|(E.) Primal generalization|√ (1/ n) O|
||SGDA|Lip, S|(E.) Primal generalization|O Tβcβ +c /n 1|
||SGDmax|NC-SC, Lip, S|(E.) Primal generalization|O T(k( +k 1+ 01 L)β βc /n +1|
|Lei|SGDA|C-C, Lip|Weak PD Risk|√ (1/ n) O|
|||C-C, Lip, S|Weak PD Risk|√ (1/ n) O|
|||SC-SC, Lip|Weak PD Risk|√ ( log n/n) O|
|||SC-SC, Lip, S|Weak PD Risk|(log n/n) O|
|||C-SC, Lip, S|(E.) Excess Primal Risk|√ (1/ n) O|
|||C-SC, Lip, S|(H.P.) Excess Primal Risk|√ (log n/ n) O|
|||C-C, Lip|(H.P.) Plain Generalization|√ (log n/ n) O|
|||WC-WC, Lip|Weak PD Generalization|O T2c2 µc +µ /n22 cc µµ ++ 31  3|
|||V-WC-WC, Lip, S|Weak PD Generalization| 1/√ n O|
||AGDA|NC-SC, PL, Lip, S|(E.) Excess Primal Risk|O n−2c cβ β+ +1  1|
|Ours|ESP|SC-SC, Lip, LN|Plain Generalization|(log n/n) O|
|||SC-SC, Lip, S, LN|Primal Generalization|(log n/n) O|
|||SC-SC, Lip, S|Strong PD Risk|(log n/n) O|
|||SC-SC, Lip, S|Strong PD Generalization|(log n/n) O|
|||SC-SC, Lip, S, LN|Excess Primal Risk|(log n/n) O|
||GDA|SC-SC, Lip, LN|Plain Generalization|((log n)3/2/n) O|
|||SC-SC, Lip, S, LN|Primal Generalization|((log n)3/2/n) O|
|||SC-SC, Lip, S|Strong PD Risk|((log n)3/2/n) O|
|||SC-SC, Lip, S|Strong PD Generalization|((log n)3/2/n) O|
|||SC-SC, Lip, S, LN|Excess Primal Risk|((log n)3/2/n) O|
||SGDA|SC-SC, Lip, LN|Plain Generalization|(log n/n) O|
|||SC-SC, Lip, S, LN|Primal Generalization|(log n/n) O|
|||SC-SC, Lip, S|Strong PD Risk|(log n/n) O|
|||SC-SC, Lip, S|Strong PD Generalization|(log n/n) O|
|||SC-SC, Lip, S, LN|Excess Primal Risk|(log n/n) O|
||PPM|SC-SC, Lip, S, LN|Plain Generalization|(log n/n) O|
|||SC-SC, Lip, S, LN|Primal Generalization|(log n/n) O|
|||SC-SC, Lip, S|Strong PD Risk|(log n/n) O|
|||SC-SC, Lip, S|Strong PD Generalization|(log n/n) O|
|||SC-SC, Lip, S, LN|Excess Primal Risk|(log n/n) O|
||EG|SC-SC, Lip, S, LN|Plain Generalization|(log n/n) O|
|||SC-SC, Lip, S, LN|Primal Generalization|(log n/n) O|
|||SC-SC, Lip, S|Strong PD Risk|(log n/n) O|
|||SC-SC, Lip, S|Strong PD Generalization|(log n/n) O|
|||SC-SC, Lip, S, LN|Excess Primal Risk|(log n/n) O|
||OGDA|SC-SC, Lip, S, LN|Plain Generalization|(log n/n) O|
|||SC-SC, Lip, S, LN|Primal Generalization|(log n/n) O|
|||SC-SC, Lip, S|Strong PD Risk|(log n/n) O|
|||SC-SC, Lip, S|Strong PD Generalization|(log n/n) O|
|||SC-SC, Lip, S, LN|Excess Primal Risk|(log n/n) O|


Reference Algorithm Assumption Generalization Measure Learning Bound

SC-SC, Lip Weak PD Risk (1/n)
ESP _O_
Zhang SC-SC, Lip, S (E.) Strong PD Risk _O(1/n)_

R-ESP C-C, Lip Weak PD Risk _O(1/[√]n)_

SGDA SC-SC, Lip, S (E.) Primal generalization _O(1/n)_

SGDmax SC-SC, Lip, S (E.) Primal generalization _O(1/n)_

GDA SC-SC, Lip, S (E.) Primal generalization _O(1/n)_

GDmax SC-SC, Lip, S (E.) Primal generalization (1/n)

Farnia _O_

PPM SC-SC, Lip, S (E.) Primal generalization _O(1/n)_

PPM C-C, Lip, S (E.) Primal generalization (1/[√]n)
_O_ _βc_

SGDA Lip, S (E.) Primal generalization _T_ _βc+1 /n_
_O_ (k+1)βc

SGDmax NC-SC, Lip, S (E.) Primal generalization _T_ (k+10Lβ+1 /n

  

_O_

C-C, Lip Weak PD Risk (1/[√]n)

  _O_ 

C-C, Lip, S Weak PD Risk _O(1/[√]n)_

SC-SC, Lip Weak PD Risk _O([√]log n/n)_

SC-SC, Lip, S Weak PD Risk _O(log n/n)_

SGDA C-SC, Lip, S (E.) Excess Primal Risk (1/[√]n)
Lei _O_

C-SC, Lip, S (H.P.) Excess Primal Risk _O(log n/[√]n)_

C-C, Lip (H.P.) Plain Generalization (log n/[√]n)
_O_ 2cµ 2cµ+1

2cµ+3 2cµ+3

WC-WC, Lip Weak PD Generalization _O_ _T_ _/n_

V-WC-WC, Lip, S Weak PD Generalization 1/[√]n

  _O_

AGDA NC-SC, PL, Lip, S (E.) Excess Primal Risk _n[−]_ 2[cβ]cβ[+1]+1 []
_O_   

SC-SC, Lip, LN Plain Generalization _O (log n/n)_

SC-SC, Lip, S, LN Primal Generalization (log n/n)[]
_O_

ESP SC-SC, Lip, S Strong PD Risk _O(log n/n)_

SC-SC, Lip, S Strong PD Generalization _O(log n/n)_

SC-SC, Lip, S, LN Excess Primal Risk _O(log n/n)_

SC-SC, Lip, LN Plain Generalization _O((log n)[3][/][2]/n)_

SC-SC, Lip, S, LN Primal Generalization _O((log n)[3][/][2]/n)_

GDA SC-SC, Lip, S Strong PD Risk _O((log n)[3][/][2]/n)_

SC-SC, Lip, S Strong PD Generalization _O((log n)[3][/][2]/n)_

SC-SC, Lip, S, LN Excess Primal Risk _O((log n)[3][/][2]/n)_

SC-SC, Lip, LN Plain Generalization _O(log n/n)_

SC-SC, Lip, S, LN Primal Generalization _O(log n/n)_

SGDA SC-SC, Lip, S Strong PD Risk _O(log n/n)_

SC-SC, Lip, S Strong PD Generalization _O(log n/n)_

Ours SC-SC, Lip, S, LN Excess Primal Risk _O(log n/n)_

SC-SC, Lip, S, LN Plain Generalization _O(log n/n)_

SC-SC, Lip, S, LN Primal Generalization _O(log n/n)_

PPM SC-SC, Lip, S Strong PD Risk _O(log n/n)_

SC-SC, Lip, S Strong PD Generalization _O(log n/n)_

SC-SC, Lip, S, LN Excess Primal Risk _O(log n/n)_

SC-SC, Lip, S, LN Plain Generalization _O(log n/n)_

SC-SC, Lip, S, LN Primal Generalization _O(log n/n)_

EG SC-SC, Lip, S Strong PD Risk _O(log n/n)_

SC-SC, Lip, S Strong PD Generalization _O(log n/n)_

SC-SC, Lip, S, LN Excess Primal Risk _O(log n/n)_

SC-SC, Lip, S, LN Plain Generalization _O(log n/n)_

SC-SC, Lip, S, LN Primal Generalization _O(log n/n)_

OGDA SC-SC, Lip, S Strong PD Risk _O(log n/n)_

SC-SC, Lip, S Strong PD Generalization _O(log n/n)_

SC-SC, Lip, S, LN Excess Primal Risk _O(log n/n)_

Table 1: Summary of Results. Here, “Zhang” means reference (Zhang et al., 2021a), “Farnia” means
reference (Farnia & Ozdaglar, 2021), and “Lei” means reference (Lei et al., 2021). The bounds are
established by choosing an optimal iterate number T . “LN” means the low noise condition, see
Section 5.1. Other auxiliary descriptions of Table 1 are shown in Appendix H.


-----

In summary, the above analyses from two different perspectives support our claim that Part (a), Part
(b), and Part (e) provide sharper high probability generalization bounds.
**Remark 5. Different measures quantify different degrees of the generalization error. Thus, deriv-**
ing bounds of different generalization measures requires different assumptions (Lei et al., 2021;
Zhang et al., 2021a; Farnia & Ozdaglar, 2021). Strong measures require stronger assumptions
compared with the weak (Lei et al., 2021). For instance, for the term supy _F_ (Ax(S), y) in
_∈Y_
(Ax(S), Ay(S)), one has to consider the fact that for different Ax(S), y is different, which
_△[s]_
makes the proof more challenging. While in (Ax(S), Ay(S)) and (Ax(S), Ay(S))
_△[w]_ _△[w]_ _−_
_△S[w][(][A][x][(][S][)][, A][y][(][S][))][, both the supremum over][ A][x][(][S][)][ and][ A][y][(][S][)][ are outside the expectation op-]_
erator, thus one does not need to consider the coupling between Ax(S) and y. The upper bounds
shown in Corollary 1 directly derived from Theorem 1 are sub-optimal since (Ax(S), Ay(S))
_△[w]_
and △[w](Ax(S), Ay(S)) −△S[w][(][A][x][(][S][)][, A][y][(][S][))][ are pretty weak generalization measures (Lei et al.,]
2021). We list Corollary 1 here to suggest that, when Theorem 1 is established, the fast order O(1/n)
is easy to be achieved for △[w](Ax(S), Ay(S)) and △[w](Ax(S), Ay(S)) −△S[w][(][A][x][(][S][)][, A][y][(][S][))][. On]
the other hand, the two weak generalization measures in (Zhang et al., 2021a) is studied for the
specific ESP solution, while Corollary 1 is applicable for any stable minimax learning algorithms, it
thus may be useful in some applications.

5 APPLICATIONS

We now apply Theorem 1 to the ESP solution and several gradient-based optimization algorithms:
GDA, SGDA, PPM, EG, and OGDA. Considering the length limit, we postpone the introductions
and theorems of these applications to the Appendix. Here, we list the generalization bounds of these
optimization algorithms in Table 1.

5.1 DESCRIPTIONS OF TABLE 1

Table 1 gives almost all existing generalization bounds in minimax learning. In Table 1, “LN” means
the low noise conditions, i.e., the corresponding F (Ax(S), Ay(S)), R(Ax(S)), or inf **x** _R(x) of_
_∈X_
these applications is of the order O(1/n). For instance, for the ESP solution (ˆx[∗]S[,][ ˆ]yS[∗] [)][, we assume]
_F_ (ˆx[∗]S[,][ ˆ]yS[∗] [)][,][ R][(ˆ]x[∗]S[)][, and][ inf] **[x][∈X][ R][(][x][)][ are of the order][ O][(1][/n][)][ for the plain generalization error, the]**
primal generalization error, and the excess primal population risk, respectively. For other learning
algorithms, please refer to the Remarks in the Appendix. In Table 1, we compare our results with
(Zhang et al., 2021a; Farnia & Ozdaglar, 2021; Lei et al., 2021) in the way described in the last
paragraph of Remark 2. (E.) denotes that the bound is derived in expectation, while (H.P.) denotes
high probability. Since our results are all established with high probability, we thus omit (H.P.) for
brevity. The descriptions of other notations are shown in Appendix H.

In Table 1, (Zhang et al., 2021a) and (Farnia & Ozdaglar, 2021) focus on the expected generalization
measures. We improve the learning bounds of the ESP solution in (Zhang et al., 2021a) to high probability guarantees. Compared with (Farnia & Ozdaglar, 2021), we have provided high probability
primal generalization error bounds for GDA, SGDA, and PPM. Additionally, we also study other
generalization measures. (Lei et al., 2021) focus on SGDA and mainly provide guarantees for weak
generalization measures, i.e., weak PD risk and weak PD generalization error. In contrast, we have
developed bounds for strong PD risk and strong PD generalization error. Note that the two type of
bounds don’t require the “LN” condition. Moreover, although (Lei et al., 2021) provides two high
probability bounds, however, in slow order. Note that in addition to the classical GDA, SGDA, and
PPM, we also provide sharper high probability bounds for EG and OGDA in that their widespread
use in training GANs (Mokhtari et al., 2019; Daskalakis et al., 2017; Liang & Stokes, 2019).

6 CONCLUSION

In this paper, we provide a systematical analysis of sharper generalization bounds for minimax
problems. We first establish sharper high probability bounds for almost all existing generalization
measures via algorithmic stability and then apply these bounds to several important applications. We
believe that our research can provide in-depth insights into minimax learning problems. For future
work, it would be important to relax the assumptions in this paper. Also, it would be interesting to
investigate how well other theoretical tools perform on the generalization of minimax problems.


-----

ACKNOWLEDGMENTS

We appreciate all the anonymous reviewers for their invaluable and constructive comments.
This work is supported in part by the National Natural Science Foundation of China (No.
62076234, No.61703396, No. 62106257), Beijing Outstanding Young Scientist Program
NO.BJJWZYJH012019100020098, Intelligent Social Governance Platform, Major Innovation &
Planning Interdisciplinary Platform for the ”Double-First Class” initiative, Renmin University of
China, China Unicom Innovation Ecological Cooperation Plan, Public Computing Cloud of Renmin
University of China, Beijing Natural Science Foundation (No. 4222029).

REFERENCES

Palaniappan Balamurugan and Francis Bach. Stochastic variance reduction methods for saddle-point
problems. In Advances in Neural Information Processing Systems, 2016.

David Balduzzi, S´ebastien Racani`ere, James Martens, Jakob N. Foerster, Karl Tuyls, and Thore
Graepel. The mechanics of n-player differentiable games. In International Conference on Ma_chine Learning, 2018._

Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. Annals
_of Statistics, 33(4):1497–1537, 2005._

Raef Bassily, Vitaly Feldman, Crist´obal Guzm´an, and Kunal Talwar. Stability of stochastic gradient
descent on nonsmooth convex losses. In Advances in Neural Information Processing Systems,
2020.

St´ephane Boucheron, G´abor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymp_totic theory of independence. Oxford university press, 2013._

Olivier Bousquet and Andr´e Elisseeff. Stability and generalization. Journal of Machine Learning
_Research, 2(3):499–526, 2002._

Olivier Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy. Sharper bounds for uniformly stable
algorithms. In Conference On Learning Theory, 2020.

Olivier Catoni. PAC-BAYESIAN SUPERVISED CLASSIFICATION: The Thermodynamics of Statis_tical Learning. 2007._

Zachary Charles and Dimitris Papailiopoulos. Stability and generalization of learning algorithms
that converge to global optima. In International Conference on Machine Learning, 2018.

Robert S. Chen, Brendan Lucier, Yaron Singer, and Vasilis Syrgkanis. Robust optimization for
non-convex objectives. In Advances in Neural Information Processing Systems, 2017.

Yuansi Chen, Chi Jin, and Bin Yu. Stability and convergence trade-off of iterative optimization
algorithms. arXiv preprint arXiv:1804.01619, 2018.

Ziyi Chen, Yi Zhou, Tengyu Xu, and Yingbin Liang. Proximal gradient descent-ascent: Variable
convergence under kłgeometry. In International Conference on Learning Representations, 2021.

Ashish Cherukuri, Bahman Gharesifard, and Jorge Cortes. Saddle-point dynamics: conditions for
asymptotic stability of saddle points. Siam Journal on Control and Optimization, 55(1):486–511,
2017.

Corinna Cortes, Mehryar Mohri, and Ananda Theertha Suresh. Relative deviation margin bounds.
In International Conference on Machine Learning, pp. 2122–2131, 2021.

Bo Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and Le Song. Sbeed:
Convergent reinforcement learning with nonlinear function approximation. In International Con_ference on Machine Learning, 2018._

Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient descent in
min-max optimization. In Advances in Neural Information Processing Systems, 2018.


-----

Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with
optimism. In International Conference on Learning Representations, 2017.

Zhun Deng, Hangfeng He, and Weijie Su. Toward better generalization bounds with locally elastic
stability. In International Conference on Machine Learning, 2021.

Jelena Diakonikolas, Constantinos Daskalakis, and Michael I. Jordan. Efficient methods for structured nonconvex-nonconcave min-max optimization. In International Conference on Artificial
_Intelligence and Statistics, 2021._

Simon S. Du, Jianshu Chen, Lihong Li, Lin Xiao, and Dengyong Zhou. Stochastic variance reduction methods for policy evaluation. In International Conference on Machine Learning, 2017.

Farzan Farnia and Asuman Ozdaglar. Train simultaneously, generalize better: Stability of gradientbased minimax learners. In International Conference on Machine Learning, 2021.

Vitaly Feldman and Jan Vondrak. Generalization bounds for uniformly stable algorithms. In Ad_vances in Neural Information Processing Systems, 2018._

Vitaly Feldman and Jan Vondrak. High probability generalization bounds for uniformly stable algorithms with nearly optimal rate. In Conference on Learning Theory, 2019.

Tanner Fiez and Lillian J Ratliff. Local convergence analysis of gradient descent ascent with finite
timescale separation. In International Conference on Learning Representations, 2021.

Dylan J. Foster, Spencer Greenberg, Satyen Kale, Haipeng Luo, Mehryar Mohri, and Karthik Sridharan. Hypothesis set stability and generalization. In Advances in Neural Information Processing
_Systems, 2019._

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor_mation Processing Systems, 2014._

Paulina Grnarova, Kfir Y. Levy, Aur´elien Lucchi, Thomas Hofmann, and Andreas Krause. An online
learning approach to generative adversarial networks. In International Conference on Learning
_Representations, 2017._

Moritz Hardt, Benjamin Recht, and Yoram Singer. Train faster, generalize better: stability of
stochastic gradient descent. In International Conference on Machine Learning, 2016.

Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
_in Neural Information Processing Systems, 2017._

Yu-Guan Hsieh, Franck Iutzeler, J´erˆome Malick, and Panayotis Mertikopoulos. On the convergence
of single-call stochastic extra-gradient methods. In Advances in Neural Information Processing
_Systems, 2019._

Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximalgradient methods under the polyak-łojasiewicz condition. In Joint European Conference on Ma_chine Learning and Knowledge Discovery in Databases, pp. 795–811, 2016._

Yegor Klochkov and Nikita Zhivotovskiy. Stability and deviation optimal risk bounds with convergence rate O(1/n). Advances in Neural Information Processing Systems, 2021.

Weiwei Kong and Renato D. C. Monteiro. An accelerated inexact proximal point method for solving
nonconvex-concave min-max problems. arXiv preprint arXiv:1905.13433, 2019.

G. M. Korpelevich. The extragradient method for finding saddle points and other problems. Mate_con, 12:747–756, 1976._

Ilja Kuzborskij and Christoph Lampert. Data-dependent stability of stochastic gradient descent. In
_International Conference on Machine Learning, 2018._


-----

Yunwen Lei and Yiming Ying. Fine-grained analysis of stability and generalization for stochastic
gradient descent. In International Conference on Machine Learning, 2020.

Yunwen Lei and Yiming Ying. Sharper generalization bounds for learning with gradient-dominated
objective functions. In International Conference on Learning Representations, 2021a.

Yunwen Lei and Yiming Ying. Stochastic proximal auc maximization. Journal of Machine Learning
_Research, 22(61):1–45, 2021b._

Yunwen Lei, Antoine Ledent, and Marius Kloft. Sharper generalization bounds for pairwise learning. In Advances in Neural Information Processing Systems, 2020.

Yunwen Lei, Zhenhuan Yang, Tianbao Yang, and Yiming Ying. Stability and generalization of
stochastic gradient methods for minimax problems. In International Conference on Machine
_Learning, 2021._

Guy Lever, Franc¸ois Laviolette, and John Shawe-Taylor. Tighter pac-bayes bounds through
distribution-dependent priors. Theoretical Computer Science, 473:4–28, 2013.

Jian Li, Yong Liu, Rong Yin, Hua Zhang, Lizhong Ding, and Weiping Wang. Multi-class learning:
From theory to algorithm. In Advances in Neural Information Processing Systems, pp. 1586–
1595, 2018.

Jian Li, Xuanyuan Luo, and Mingda Qiao. On generalization error bounds of noisy gradient methods
for non-convex learning. In International Conference on Learning Representations, 2020.

Shaojie Li and Yong Liu. Sharper generalization bounds for clustering. In International Conference
_on Machine Learning, pp. 6392–6402, 2021._

Shaojie Li and Yong Liu. Towards sharper generalization bounds for structured prediction. In
_Advances in Neural Information Processing Systems, 2021._

Tengyuan Liang and James Stokes. Interaction matters: A note on non-asymptotic local convergence
of generative adversarial networks. In International Conference on Artificial Intelligence and
_Statistics, 2019._

Qihang Lin, Mingrui Liu, Hassan Rafique, and Tianbao Yang. Solving weakly-convex-weaklyconcave saddle-point problems as weakly-monotone variational inequality. 2018.

Tianyi Lin, Chi Jin, and Michael Jordan. On gradient descent ascent for nonconvex-concave minimax problems. In International Conference on Machine Learning, 2020a.

Tianyi Lin, Chi Jin, and Michael I. Jordan. Near-optimal algorithms for minimax optimization. In
_Conference on Learning Theory, 2020b._

Mingrui Liu, Hassan Rafique, Qihang Lin, and Tianbao Yang. First-order convergence theory for
weakly-convex-weakly-concave min-max problems. Journal of Machine Learning Research, 22
(169):1–34, 2021a.

Tongliang Liu, G´abor Lugosi, Gergely Neu, and Dacheng Tao. Algorithmic stability and hypothesis
complexity. In International Conference on Machine Learning, 2017.

Yong Liu, Shizhong Liao, Shali Jiang, Lizhong Ding, Hailun Lin, and Weiping Wang. Fast crossvalidation for kernel-based algorithms. _IEEE Transactions on Pattern Analysis and Machine_
_Intelligence, 42(5):1083–1096, 2020._

Yong Liu, Jiankun Liu, and Shuqiang Wang. Effective distributed learning with random features:
Improved bounds and algorithms. In International Conference on Learning Representations,
2021b.

Nicolas Loizou, Hugo Berard, Alexia Jolicoeur-Martineau, Pascal Vincent, Simon Lacoste-Julien,
and Ioannis Mitliagkas. Stochastic hamiltonian gradient methods for smooth games. In Interna_tional Conference on Machine Learning, 2020._


-----

Nicolas Loizou, Hugo Berard, Gauthier Gidel, Ioannis Mitliagkas, and Simon Lacoste-Julien.
Stochastic gradient descent-ascent and consensus optimization for smooth games: Convergence
analysis under expected co-coercivity. arXiv preprint arXiv:2107.00052, 2021.

Ben London, Bert Huang, and Lise Getoor. Stability and generalization in structured prediction.
_Journal of Machine Learning Research, 17(221):7808–7859, 2016._

Songtao Lu, Ioannis Tsaknakis, Mingyi Hong, and Yongxin Chen. Hybrid block successive approximation for one-sided non-convex min-max problems: Algorithms and applications. IEEE
_Transactions on Signal Processing, 68:3676–3691, 2020._

Luo Luo, Haishan Ye, Zhichao Huang, and Tong Zhang. Stochastic recursive gradient descent
ascent for stochastic nonconvex-strongly-concave minimax problems. In Advances in Neural
_Information Processing Systems, 2020._

Shaogao Lv, Junhui Wang, Jiankun Liu, and Yong Liu. Improved learning rates of a functional lassotype svm with sparse multi-kernel representation. In Advances in Neural Information Processing
_Systems, 2021._

Gonzalo Mateos, Juan Andr´es Bazerque, and Georgios B Giannakis. Distributed sparse linear regression. IEEE Transactions on Signal Processing, 58(10):5262–5276, 2010.

Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chandrasekhar, and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going
the extra (gradient) mile. In International Conference on Learning Representations, 2019.

Aryan Mokhtari, Asuman E. Ozdaglar, and Sarath Pattathil. Proximal point approximations achieving a convergence rate of O(1/k) for smooth convex-concave saddle point problems: Optimistic
gradient and extra-gradient methods. 2019.

Aryan Mokhtari, Asuman E. Ozdaglar, and Sarath Pattathil. A unified analysis of extra-gradient and
optimistic gradient methods for saddle point problems: Proximal point approach. In International
_Conference on Artificial Intelligence and Statistics, 2020._

Hongseok Namkoong and John C. Duchi. Stochastic gradient methods for distributionally robust
optimization with f-divergences. In Advances in Neural Information Processing Systems, 2016.

Hongseok Namkoong and John C. Duchi. Variance-based regularization with convex objectives. In
_Advances in Neural Information Processing Systems, 2017._

Angelia Nedic and Asuman E. Ozdaglar. Subgradient methods for saddle-point problems. Journal
_of Optimization Theory and Applications, 142(1):205–228, 2009._

A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to
stochastic programming. Siam Journal on Optimization, 19(4):1574–1609, 2008.

Arkadi Nemirovski. Prox-method with rate of convergence O(1/t) for variational inequalities
with lipschitz continuous monotone operators and smooth convex-concave saddle point problems.
_Siam Journal on Optimization, 15(1):229–251, 2005._

Maher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason D. Lee, and Meisam Razaviyayn. Solving
a class of non-convex min-max games using iterative first order methods. In Advances in Neural
_Information Processing Systems, 2019._

L. D. Popov. A modification of the arrow-hurwicz method for search of saddle points. Mathematical
_Notes, 28(5):845–848, 1980._

Hassan Rafique, Mingrui Liu, Qihang Lin, and Tianbao Yang. Non-convex min-max optimization:
Provable algorithms and applications in machine learning. arXiv: Optimization and Control,
2018.

Alexander Rakhlin, Sayan Mukherjee, and Tomaso Poggio. Stability results in learning theory.
_Analysis and Applications, 3(04):397–417, 2005._


-----

Meisam Razaviyayn, Tianjian Huang, Songtao Lu, Maher Nouiehed, Maziar Sanjabi, and Mingyi
Hong. Non-convex min-max optimization: Applications, challenges, and recent theoretical advances. arXiv preprint arXiv:2006.08141, 2020.

R. Tyrrell Rockafellar. Monotone operators and the proximal point algorithm. Siam Journal on
_Control and Optimization, 14(5):877–898, 1976._

Maziar Sanjabi, Jimmy Ba, Meisam Razaviyayn, and Jason D. Lee. On the convergence and robustness of training gans with regularized optimal transport. In Advances in Neural Information
_Processing Systems, 2018._

Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo_rithms. Cambridge university press, 2014._

Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability
and uniform convergence. Journal of Machine Learning Research, 11(90):2635–2670, 2010.

Jeff Shamma. Cooperative Control of Distributed Multi-Agent Systems. 2008.

Aman Sinha, Hongseok Namkoong, and John C. Duchi. Certifying some distributional robustness
with principled adversarial training. In International Conference on Learning Representations,
2017.

Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Optimistic rates for learning with a smooth
loss. arXiv preprint arXiv:1009.3896, 2010.

Pierre Tarres and Yuan Yao. Online learning as stochastic approximation of regularization paths:
Optimality and almost-sure convergence. IEEE Transactions on Information Theory, 60(9):5716–
5735, 2014.

Kiran Koshy Thekumparampil, Prateek Jain, Praneeth Netrapalli, and Sewoong Oh. Efficient algorithms for smooth minimax optimization. In Advances in Neural Information Processing Systems,
2019.

Yuanhao Wang and Jian Li. Improved algorithms for convex-concave minimax optimization. In
_Advances in Neural Information Processing Systems, 2020._

Yuanhao Wang, Guodong Zhang, and Jimmy Ba. On solving minimax optimization locally: A
follow-the-ridge approach. In International Conference on Learning Representations, 2020.

Yan Yan, Yi Xu, Qihang Lin, Wei Liu, and Tianbao Yang. Optimal epoch stochastic gradient descent ascent methods for min-max optimization. In Advances in Neural Information Processing
_Systems, 2020._

Jun Yang, Shengyang Sun, and Daniel M. Roy. Fast-rate pac-bayes generalization bounds via shifted
rademacher processes. In Advances in Neural Information Processing Systems, volume 32, pp.
10802–10812, 2019.

Junchi Yang, Negar Kiyavash, and Niao He. Global convergence and variance reduction for a class
of nonconvex-nonconcave minimax problems. In Advances in Neural Information Processing
_Systems, 2020._

Rong Yin, Yong Liu, Weiping Wang, and Dan Meng. Sketch kernel ridge regression using circulant
matrix: Algorithm and theory. IEEE Transactions on Neural Networks, 31(9):3512–3524, 2020.

TaeHo Yoon and Ernest K. Ryu. Accelerated algorithms for smooth convex-concave minimax problems with O(1/k[2]) rate on squared gradient norm. arXiv preprint arXiv:2102.07922, 2021.

Junyu Zhang, Mingyi Hong, Mengdi Wang, and Shuzhong Zhang. Generalization bounds for
stochastic saddle point problems. In International Conference on Artificial Intelligence and Statis_tics, 2021a._

Lijun Zhang and Zhi-Hua Zhou. Stochastic approximation of smooth and strongly convex functions:
Beyond the O(1/t) convergence rate. In Conference on Learning Theory, pp. 3160–3179, 2019.


-----

Lijun Zhang, Tianbao Yang, and Rong Jin. Empirical risk minimization for stochastic convex optimization: O(1/n)- and O(1/n[2])-type of risk bounds. In Conference on Learning Theory, pp.
1954–1979, 2017.

Yikai Zhang, Wenjia Zhang, Sammy Bald, Vamsi Pingali, Chao Chen, and Mayank Goswami. Stability of sgd: Tightness analysis and improved bounds. arXiv preprint arXiv:2102.05274, 2021b.

A PROOF OF THEOREM 1

We now provide proofs of Theorem 1. For better readability, we restate Theorem 1 below.
**Theorem 2. Let A be a learning algorithm and ϵ > 0. Suppose |f** (x, y; z)| ≤ _M for some M > 0_
_and x ∈X_ _, y ∈Y, z ∈Z. Fixed any η > 0. There exists an absolute positive constant C._

_(a.) If the algorithm A is ϵ-uniformly stable, then for any δ > 0, with probability at least 1 −_ _δ,_

_M_

_F_ (Ax(S), Ay(S)) (1 + η)FS(Ax(S), Ay(S)) + C [1 +][ η] _._
_≤_ _η_ _n_ [log(1][/δ][) +][ ϵ][ log][2][ n][ log 1]δ

 

_(b.) Assume that for all x, the function y 7→_ _F_ (x, y) is µ-strongly-concave. If the algorithm A is
_ϵ-argument stable and Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ,_

_M_ _β_

_R(Ax(S))_ (1 + η)RS(Ax(S)) + C [1 +][ η] _Lϵ log2 n log [1]_ _._
_≤_ _η_ _n_ [log 1]δ [+] _µ_ [+ 1] _δ_

   

_(c.) Assume that for all x and y, the function F_ (x, y) is µ-SC-SC. If the algorithm A is ϵ-argument
_stable and Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ,_

_△[s]_ (Ax(S), Ay(S)) ≤△S[s] [(][A][x][(][S][)][, A][y][(][S][)) +]L2(1 +[ η][E][S] η[△]) _S[s]_ [(][A][x][(][S][)][, A][y][(][S][))] 1

+ C(1 + η) + _[M]_ 1 + _[β]_ _ϵL log2 n_ log _._

_nµη_ _n_ [+] _µ_ _δ_

     

_(d.) Assume that for all x and y, the function F_ (x, y) is µ-SC-SC. If the algorithm A is ϵ-argument
_stable and Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ,_

_△[s]_ (Ax(S), Ay(S)) −△S[s] [(][A][x][(][S][)][, A][y][(][S][))]L[ ≤]2(1 +[η][E][S] η[△]) _S[s]_ [(][A][x][(][S][)][, A][y][(][S][))] 1

+ C(1 + η) + _[M]_ 1 + _[β]_ _ϵL log2 n_ log _._

_nµη_ _n_ [+] _µ_ _δ_

     

_(e.) Assume that for all x, the function y 7→_ _F_ (x, y) is µ-strongly-concave. If the algorithm A is
_ϵ-argument stable and Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ,_

_R(Ax(S))_ (1 + η) inf
_≤_ **x** _[R][(][x][)]_
_∈X_

_M_ _β_

+ C [2 +][ η] _Lϵ log2 n log [1]_ _S[(][A][x][(][S][)][, A][y][(][S][))]_ _._

_η_ _n_ [log 1]δ [+] _µ_ [+ 1] _δ_ [+][ △][s]

   

**Remark 6. To prove sharper high probability bounds than (Lei et al., 2021), the concentration**
inequality for a summation of weakly-dependent random variables proposed in (Bousquet et al.,
2020) (Lemma 2 in Appendix A) plays a key role in our analysis. However, the direct use of this
inequality will inevitably lead to a slow order bound since it contains a sampling error of slow order
_O(1/[√]n). We exploit the proof techniques of the recent breakthrough work (Klochkov & Zhiv-_
otovskiy, 2021) to make new constructions of gi(S) so that the parameter M in Lemma 2 is 0.
However, the proof techniques of (Klochkov & Zhivotovskiy, 2021) can not be directly extended
to minimax problems. The coupling construction between the minimization variables and the maximization variables makes the proofs of minimax problems more difficult than the minimization
problem studied by (Klochkov & Zhivotovskiy, 2021). We must proceed with novel decompositions for the generalization measures. Note that different decompositions are required for different
generalization measures. A pretty technical decomposition is exploited in the proof of Part (c).


-----

Moreover, the proof of minimax problems needs refined analyses due to the minimax structure. For
instance, in proving the primal generalization error, we need to quantify the fact that for different
_Ax(S), the optimal y is different in R(Ax(S)). And the analysis of excess primal population risk in_
Part (e) is different from the excess risk analysis in (Klochkov & Zhivotovskiy, 2021). The reason
is that the supremum operator in inf **x** _R(x) makes the Bernstein condition used in (Klochkov_
_∈X_
& Zhivotovskiy, 2021) not applicable for excess primal population risk. Additionally, (Klochkov
& Zhivotovskiy, 2021) only study ERM and GD, while we study more optimization algorithms:
SGDA, PPM, EG, and OGDA.
**Remark 7. According to Definition 1 and Jensen’s inequality, we know that △S[w][(][x][,][ y][)][ ≤]**
E[△S[s] [(][x][,][ y][)]][. Meanwhile, since we will provide the strong PD empirical risk bounds for several]
important optimization algorithms in Section 5, it thus implies that we also establish bounds with
fast rates for △S[w][(][x][,][ y][)][.]

To begin the proof of Theorem 1, we first introduce some key lemmas on concentration inequalities.
The first lemma translates a moment bound into a high probability bound.
**Lemma 1. (Bousquet et al., 2020) Let Z be a random variable with**

_Z_ _p_ _pa + pb_
_∥_ _∥_ _≤_ _[√]_

_for some a, b > 0 and for any p ≥_ 2. Then for any δ ∈ (0, 1) we have, with probability at least
1 − _δ,_

_e_ _e_

_Z_ _e_ _a_ log + b log _,_
_|_ _| ≤_ _δ_ _δ_
r
    

_where e is the base of the natural logarithm._

The second lemma establishes a concentration inequality for a summation of weakly-dependent
random variables.
**Lemma 2. (Bousquet et al., 2020) Let S = {z1, ..., zn} be a set of independent random variables**
_each taking values in_ _and M > 0. Denote [n] as the set_ 1, ..., n _. Define S_ _zi_ _be set_
_Z_ _{_ _}_ _\{_ _}_
_z1, ..., zi_ 1, zi+1, ..., zn _. Let g1, ..., gn be some functions gi :_ R such that the following
_{_ _−_ _}_ _Z_ _[n]_ _7→_
_inequalities hold for any i ∈_ [n],

-  ES _zi_ [gi(S)] _M almost surely (a.s.),_

_\{_ _}_ _≤_

-  Ezi [gi(S)] = 0 a.s.,



-  for any j [n] with j = i, and zj[′′]
_∈_ _̸_ _[∈Z]_
_gi(S)_ _gi(z1, ..., zj_ 1, zj[′′][, z][j][+1][, ..., z][n][)] _β._
_−_ _−_ _≤_

_Then, for any p ≥_ 2

_n_ _gi(S)_ _√2pnβ_ log2 n + 4M _[√]pn._

_p_ _⌈_ _⌉_
_i=1_

_[≤]_ [12]

X

The following definition and lemma give the concentration inequality for non-negative weakly selfbounded functions.
**Definition 4. (Weakly Self-Bounded Function) Assume that a, b > 0. A function f : Z** _[n]_ _7→_ [0, +∞)
_is said to be (a, b)-weakly self-bounded if there exist functions fi : Z_ _[n][−][1]_ _7→_ [0, +∞) that satisfies
_for all Z_ _[n]_ _∈Z_ _[n],_

_n_

(f (Z _[n])_ _fi(Z_ _[n]))[2]_ _af_ (Z _[n]) + b._
_−_ _≤_
_i=1_

X

**Lemma 3. (Klochkov & Zhivotovskiy, 2021) Suppose that z1, ..., zn are independent random vari-**
_ables and the function f : Z_ _[n]_ _7→_ [0, +∞) is (a, b)-weakly self-bounded and the corresponding
_function fi satisfy fi(Z_ _[n]) ≥_ _f_ (Z _[n]) for i = 1, ..., n and any Z_ _[n]_ _∈Z_ _[n]. Then, for any t > 0,_

_t[2]_
_Pr(Ef_ (z1, ..., zn) _f_ (z1, ..., zn) + t) exp _._
_≥_ _≤_ _−_ 2aEf (z1, ..., zn) + 2b
 


-----

The following lemma is the classical Bernstein concentration inequality.
**Lemma 4. (Boucheron et al., 2013) Let z1, ..., zn be i.i.d. random variables and assume that E[zi] =**
_µ. Suppose_ _zi_ _c for any i. Then for any δ_ (0, 1), with probability at least 1 _δ,_
_|_ _| ≤_ _∈_ _−_

_n_

2σ[2] log(1/δ)

[1] _zi_ _µ_ + [2][c][ log(1][/δ][)] _,_

_n_ _i=1_ _−_ _≤_ r _n_ 3n

X

_where σ[2]_ _is the variance of zi._

A.1 PROOF OF PART (A)

We first prove the plain generalization error bound.

_Proof. Let S_ = _z1, ..., zn_ be a set of independent random variables each taking values
_{_ _}_
in Z and S[′] = _{z1[′]_ _[, ..., z]n[′]_ _[}][ be its independent copy.]_ For any i _∈_ [n], define S[(][i][)] =
_z1, ..., zi_ 1, zi[′][, z][i][+1][, ..., z][n][}][ be a dataset by replacing the][ i][-th sample in][ S][ with another i.i.d. sam-]
_{_ _−_
ple zi[′][. We first have the following decomposition]

_nF_ (Ax(S), Ay(S)) _nFS(Ax(S), Ay(S))_
_−_

_n_

= EZ[f (Ax(S), Ay(S); Z) − Ezi[′] [[][f] [(][A][x][(][S][(][i][)][)][, A][y][(][S][(][i][)][);][ Z][)]]]

_i=1_

Xn

+ Ezi[′] [[][E][Z][[][f] [(][A][x][(][S][(][i][)][)][, A][y][(][S][(][i][)][);][ Z][)]][ −] _[f]_ [(][A][x][(][S][(][i][)][)][, A][y][(][S][(][i][)][);][ z][i][)]]

_i=1_

X


Ezi[′] [[][f] [(][A][x][(][S][(][i][)][)][, A][y][(][S][(][i][)][);][ z][i][)]][ −]
_i=1_

X


_f_ (Ax(S), Ay(S); zi).
_i=1_

X


According to the definition of uniform stability (Part 1 of Definition 3), we have


_nF_ (Ax(S), Ay(S)) _nFS(Ax(S), Ay(S))_ 2nϵ +
_−_ _≤_


_gi(S),_
_i=1_

X


where we have introduced _gi(S)_ = Ezi[′] [[][E][Z][[][f] [(][A][x][(][S][(][i][)][)][, A][y][(][S][(][i][)][);][ Z][)]] _−_
_f_ (Ax(S[(][i][)]), Ay(S[(][i][)]); zi)]. Thus, by a rearrangement, we have

_n_

_nF_ (Ax(S), Ay(S)) _nFS(Ax(S), Ay(S))_ _gi(S)_ 2nϵ. (3)
_−_ _−_ _≤_

_i=1_

X

Then, for any i = 1, ..., n, we define hi(S) = gi(S) ES _zi_ [gi(S)]. It is easy to verify that
_−_ _\{_ _}_
ES _zi_ [hi(S)] = 0 and Ezi [hi(S)] = Ezi [gi(S)] Ezi ES _zi_ [gi(S)] = 0 0 = 0. Also, for any
_\{_ _}_ _−_ _\{_ _}_ _−_
_j_ [n] with j = i, and zj[′′]
_∈_ _̸_ _[∈Z][, we have the following inequality]_
_hi(S)_ _hi(z1, ..., zj_ 1, zj[′′][, z][j][+1][, ..., z][n][)] _gi(S)_ _gi(z1, ..., zj_ 1, zj[′′][, z][j][+1][, ..., z][n][)]
_−_ _−_ _≤_ _−_ _−_

+ ES _zi_ [gi(S)] ES _zi_ [gi(z1, ..., zj 1, zj[′′][, z][j][+1][, ..., z][n][)]] _._
_\{_ _}_ _−_ _\{_ _}_ _−_

For the first term _gi(S)_ _gi(z1, ..., zj_ 1, zj[′′][, z][j][+1][, ..., z][n][)][|][,] it can be bounded by 2ϵ
_|_ _−_ _−_
according to the definition of uniform stability. Similar result holds for the second term ES _zi_ [gi(S)] ES _zi_ [gi(z1, ..., zj 1, zj[′′][, z][j][+1][, ..., z][n][)]] according to the defini_\{_ _}_ _−_ _\{_ _}_ _−_
tion of uniform stability. By a combination of the above analysis, we get _hi(S)_
_|_ _−_
_hi(z1, ..., zj_ 1, zj[′′][, z][j][+1][, ..., z][n][)][| ≤] [4][ϵ][.]
_−_

We thus have verified that the three conditions in Lemma 2 are satisfied for hi(S). There will hold
the following result for any p ≥ 2


2ϵpn⌈log2 n⌉. (4)


_hi(S)_
_p_
_i=1_

_[≤]_ [48]

X


-----

Furthermore, we can derive that

_nF_ (Ax(S), Ay(S)) _nFS(Ax(S), Ay(S))_
_−_ _−_

=nF (Ax(S), Ay(S)) _nFS(Ax(S), Ay(S))_
_−_ _−_


_gi(S) +_
_i=1_

X


_hi(S)_
_i=1_

X


ES _zi_ [gi(S)]
_\{_ _}_
_i=1_

X


=nF (Ax(S), Ay(S)) _nFS(Ax(S), Ay(S))_ _nES′_ _F_ (Ax(S[′]), Ay(S[′]))
_−_ _−_

+ nES[′] _FS(Ax(S[′]), Ay(S[′]))_

Due to the i.i.d. property between S and S[′], we know that ES′ _F_ (Ax(S[′]), Ay(S[′])) =
ESF (Ax(S), Ay(S)).

Thus, combined (3) with (4), we have

_nF_ (Ax(S), Ay(S)) _nFS(Ax(S), Ay(S))_ _nESF_ (Ax(S), Ay(S)) + nES′ _FS(Ax(S[′]), Ay(S[′]))_ _p_
_∥_ _−_ _−_ _∥_


_nF_ (Ax(S), Ay(S)) _nFS(Ax(S), Ay(S))_ _gi(S)_
_−_ _−_ _p_

_i=1_

_n_ X

_gi(S)_ _nESF_ (Ax(S), Ay(S)) + nES′ _FS(Ax(S[′]), Ay(S[′]))_
_−_
_i=1_

X


= _nF_ (Ax(S), Ay(S)) _nFS(Ax(S), Ay(S))_
_−_ _−_

_≤_ 2nϵ + 48√2ϵpn⌈log2 n⌉

_≤_ 50√2ϵpn⌈log2 n⌉.


_gi(S)_
_p_ [+]
_i=1_

X


_hi(S)_
_i=1_

X


According to Lemma 1, for any δ ∈ (0, 1), with probability at least 1 − _δ/3, we have_

_F_ (Ax(S), Ay(S)) _FS(Ax(S), Ay(S))_
_−_

_≤|ES′_ _FS(Ax(S[′]), Ay(S[′])) −_ ESF (Ax(S), Ay(S))| + 50√2eϵ⌈log2 n⌉ log(3e/δ). (5)

We now begin to bound the term ES′ _FS(Ax(S[′]), Ay(S[′]))_ ESF (Ax(S), Ay(S)). There holds
_−_
that ESES′ _FS(Ax(S[′]), Ay(S[′])) = ESF_ (Ax(S), Ay(S)). We first consider the variance of
ES′ _f_ (Ax(S[′]), Ay(S[′]); zi). By the Jensen’s inequality, we have

Ezi [(ES′ _f_ (Ax(S[′]), Ay(S[′]); zi))[2]] Ezi ES′ [(f (Ax(S[′]), Ay(S[′]); zi))[2]]
_≤_

= EZES′ [(f (Ax(S[′]), Ay(S[′]); Z))[2]]

= EZES[(f (Ax(S), Ay(S); Z))[2]].


Then, by the Bernstein inequality in Lemma 4, we obtain the following inequality with probability
at least 1 − _δ/3,_

ES′ _FS(Ax(S[′]), Ay(S[′]))_ ESF (Ax(S), Ay(S)) (6)
_|_ _−_ _|_

2EZES[(f (Ax(S), Ay(S); Z))[2]] log(3/δ) + [2][M][ log(3][/δ][)] _._

_≤_ _n_ 3n

r

Combined (5) with (6), we finally obtain that with probability at least 1 − 2δ/3,

_F_ (Ax(S), Ay(S)) _FS(Ax(S), Ay(S))_
_−_

2EZES[(f (Ax(S), Ay(S); Z))[2]] log(3/δ) + [2][M][ log(3][/δ][)] + 50√2eϵ log2 n log(3e/δ).

_≤_ _n_ 3n _⌈_ _⌉_

r

(7)

In the following, we define q = _q(z1, ..., zn)_ = EZ[(f (Ax(S), Ay(S); Z))[2]] and qi =
_qi(z1, ..., zn) = supzi_ _q(z1, ..., zn)._ So there holds qi _q for any i = 1, .., n and any_
_∈Z_ _≥_


-----

_z1, ..., zn_ . Also, there holds that
_{_ _} ∈Zn_ _[n]_

(q _qi)[2]_
_−_
_i=1_

X


_n_

2

= _i=1_ EZ[(f (Ax(S), Ay(S); Z))[2]] − _zsupi∈Z_ EZ[(f (Ax(S), Ay(S); Z))[2]]

Xn  

2

_≤_ _i=1_ _ϵ[2][]EZ_ _f_ (Ax(S), Ay(S); Z) + supzi∈Z _f_ (Ax(S), Ay(S); Z)

X h i

_≤nϵ[2]_ (2EZ[(f (Ax(S), Ay(S); Z))] + ϵ)[2]

_≤8nϵ[2]q + 2nϵ[4],_ (8)
where the first inequality follows from the Jensen’s inequality and the definition of uniform stability,
and where the second inequality also follows from the definition of uniform stability.

From (8), we know that q is (8nϵ[2], 2nϵ[4]) weakly self-bounded. Thus, by Lemma 3, we obtain that
with probability at least 1 − _δ/3,_

ESEZ[(f (Ax(S), Ay(S); Z))[2]] − EZ[(f (Ax(S), Ay(S); Z))[2]]

_≤_ (16nϵ[2]ESEZ[(f (Ax(S), Ay(S); Z))[2]] + 4nϵ[4]) log(3/δ)
q

= ESEZ[(f (Ax(S), Ay(S); Z))[2]] + [1] 16nϵ[2] log(3/δ)

4 _[ϵ][2]_

r 

ESEZ[(f (Ax(S), Ay(S); Z))[2]] + [1] + 8nϵ[2] log(3/δ),

_≤_ 2[1] 4 _[ϵ][2][]_

where the last inequality follows from that _√ab_ 2 for all a, b > 0.

_≤_ _[a][+][b]_

Since EZ[(f (Ax(S), Ay(S); Z))[2]] ≤ _MF_ (Ax(S), Ay(S)), we have

ESEZ[(f (Ax(S), Ay(S); Z))[2]] 2MF (Ax(S), Ay(S)) (9)
_−_ _≤_ [1]4 _[ϵ][2][ + 16][nϵ][2][ log(3][/δ][)][.]_

Substituting (9) into (7), we finally obtain that with probability at least 1 − _δ,_
_F_ (Ax(S), Ay(S)) _FS(Ax(S), Ay(S))_
_−_

(4MF (Ax(S), Ay(S)) + [1]2 _[ϵ][2][ + 32][nϵ][2][ log(3][/δ][)) log(3][/δ][)]_

s


+ [2][M][ log(3][/δ][)] + 50√2eϵ log2 n log(3e/δ). (10)

3n _⌈_ _⌉_

According to inequalities _√ab ≤_ _ηa +_ _η[1]_ _[b][ and]_ _√a + b ≤_ _[√]a +_ _√b for any a, b, η > 0, we have the_

following inequality with probability at least 1 − _δ_
_F_ (Ax(S), Ay(S)) _FS(Ax(S), Ay(S))_
_−_


( [1]2 _[ϵ][2][ + 32][nϵ][2][ log(3][/δ][)) log(3][/δ][)]_


_η_

1 + η [F] [(][A][x][(][S][)][, A][y][(][S][)) + 1 +]η _[ η]_


4M log(3/δ)


+ [2][M][ log(3][/δ][)] + 50√2eϵ log2 n log(3e/δ),

3n _⌈_ _⌉_

which implies that
_F_ (Ax(S), Ay(S)) (1 + η)FS(Ax(S), Ay(S))
_−_


( [1]2 _[ϵ][2][ + 32][nϵ][2][ log(3][/δ][)) log(3][/δ][)]_


+ [1 +][ η]


4M log(3/δ)


_≤(1 + η)_


+ [2][M][ log(3][/δ][)] + 50√2eϵ log2 n log(3e/δ)

3n _⌈_ _⌉_

_M_

_C_ [1 +][ η] _,_
_≤_ _η_ _n_ [log(1][/δ][) +][ ϵ][ log][2][ n][ log(1][/δ][)]

 


-----

where C is an absolute constant. The proof is complete.

A.2 PROOF OF PART (B)

We then prove the primal generalization error bound. Before presenting the proof, we first introduce
a lemma that quantifies the sensitivity of the optimal y and x w.r.t the perturbation of x and y
respectively.

**Lemma 5. (Zhang et al., 2021a) Let f : X × Y 7→** R. Assume that f is µ-strongly-convex-strongly_concave. Suppose that for any x, x[′]_ _∈X and y, y[′]_ _∈Y we have_

**yf** (x, y) **yf** (x[′], y) _β_ **x** **x[′]** _and_ **xf** (x, y) **xf** (x, y[′]) _β_ **y** **y[′]** _._
_∥∇_ _−∇_ _∥≤_ _∥_ _−_ _∥_ _∥∇_ _−∇_ _∥≤_ _∥_ _−_ _∥_

_Define x[∗](y) = arg minx_ _h(x, y) and y[∗](x) = arg maxy_ _h(x, y) for any y and x respec-_
_∈X_ _∈Y_
_tively. Then, for any x, x[′]_ _∈X and y, y[′]_ _∈Y there holds that_

**y[∗](x)** **y[∗](x[′])** _and_ **x[∗](y)** **x[∗](y[′])**
_∥_ _−_ _∥≤_ _[β]µ_ _[∥][x][ −]_ **[x][′][∥]** _∥_ _−_ _∥≤_ _[β]µ_ _[∥][y][ −]_ **[y][′][∥][.]**


The proof of Part (b) shares similar proof techniques with Part (a), but requires a novel decomposition and several important changes. For instance, Lemma 5 should be needed to quantify the fact
that for different Ax(S), the optimal y is different in R(Ax(S)).

_Proof. Let S_ = _z1, ..., zn_ be a set of independent random variables each taking values
_{_ _}_
in Z and S[′] = _{z1[′]_ _[, ..., z]n[′]_ _[}][ be its independent copy.]_ For any i _∈_ [n], define S[(][i][)] =
_z1, ..., zi_ 1, zi[′][, z][i][+1][, ..., z][n][}][ be a dataset by replacing the][ i][-th sample in][ S][ with another i.i.d. sam-]
_{_ _−_
ple zi[′][. Denote][ y]S[∗] [= arg max][y][∈Y][ F] [(][A][x][(][S][)][,][ y][)][ and][ ˆ]yS[∗] [= arg max][y][∈Y][ F][S][(][A][x][(][S][)][,][ y][)][. We have]
the following decomposition

_nR(Ax(S))_ _nRS(Ax(S))_
_−_
=nF (Ax(S), yS[∗] [)][ −] _[nF][S][(][A][x][(][S][)][,][ ˆ]yS[∗]_ [)]

_n_

= EZ[f (Ax(S), yS[∗] [;][ Z][)][ −] [E][z]i[′] [[][f] [(][A][x][(][S][(][i][)][)][,][ y]S[∗] [(][i][)] [;][ Z][)]]]

_i=1_

X _n_

+ Ezi[′] [[][E][Z][[][f] [(][A][x][(][S][(][i][)][)][,][ y]S[∗] [(][i][)] [;][ Z][)]][ −] _[f]_ [(][A][x][(][S][(][i][)][)][,][ y]S[∗] [(][i][)] [;][ z][i][)]]

_i=1_

X


Ezi[′] [[][f] [(][A][x][(][S][(][i][)][)][,][ y]S[∗] [(][i][)] [;][ z][i][)]][ −]
_i=1_

X


_f_ (Ax(S), ˆyS[∗] [;][ z][i][)][.] (11)
_i=1_

X


Firstly, we have

_f_ (Ax(S), yS[∗] [;][ Z][)][ −] _[f]_ [(][A][x][(][S][(][i][)][)][,][ y]S[∗] [(][i][)] [;][ Z][)]

=f (Ax(S), yS[∗] [;][ Z][)][ −] _[f]_ [(][A][x][(][S][)][,][ y]S[∗] [(][i][)] [;][ Z][) +][ f] [(][A][x][(][S][)][,][ y]S[∗] [(][i][)] [;][ Z][)][ −] _[f]_ [(][A][x][(][S][(][i][)][)][,][ y]S[∗] [(][i][)] [;][ Z][)]

_≤L∥yS[∗]_ _[−]_ **[y]S[∗]** [(][i][)] _[∥]_ [+][ L][∥][A][x][(][S][)][ −] _[A][x][(][S][(][i][)][)][∥]_

1 + _[β]_ _L_ _Ax(S)_ _Ax(S[(][i][)])_
_≤_ _µ_ _∥_ _−_ _∥_
 

_≤_ 1 + _[β]_ _Lϵ,_ (12)
 


where the second inequality follows from Lemma 5 with the fact that F is smooth and µ-SC-SC.


-----

Secondly, we have

_n_

Ezi[′] [[][f] [(][A][x][(][S][(][i][)][)][,][ y]S[∗] [(][i][)] [;][ z][i][)]]
_i=1_

Xn

= Ezi[′] [[][f] [(][A][x][(][S][(][i][)][)][,][ y]S[∗] [(][i][)] [;][ z][i][)][ −] _[f]_ [(][A][x][(][S][)][,][ y]S[∗] [;][ z][i][) +][ f] [(][A][x][(][S][)][,][ y]S[∗] [;][ z][i][)]]

_i=1_

X


1 + _[β]_

_µ_

_Lnϵ +_


Ezi[′]
_i=1_

X

1 + _[β]_



_L_ _Ax(S)_ _Ax(S[(][i][)])_ +
_∥_ _−_ _∥_

_n_

_f_ (Ax(S), yS[∗] [;][ z][i][)][,]
_i=1_

X


_f_ (Ax(S), yS[∗] [;][ z][i][)]
_i=1_

X


where the first and the last inequalities follow from (12). Substituting the above two results into
(11), we obtain that

_nF_ (Ax(S), yS[∗] [)][ −] _[nF][S][(][A][x][(][S][)][,][ ˆ]yS[∗]_ [)]

_n_ _n_ _n_

2 1 + _[β]_ _Lnϵ +_ _f_ (Ax(S), yS[∗] [;][ z][i][) +] _gi(S)_ _f_ (Ax(S), ˆyS[∗] [;][ z][i][)]
_≤_ _µ_ _−_

_i=1_ _i=1_ _i=1_

  Xn X X

2 1 + _[β]_ _Lnϵ +_ _gi(S),_
_≤_ _µ_

_i=1_

  X

whereni=1 _[f]_ [(][A]the[x][(][S]last[)][,][ ˆ]yS[∗] [;][ z]inequality[i][)][ ≤] [0][ and that we have introduced]follows from the _[ g]facts[i][(][S][) =]that[ E][z]i[′]_ [[][E][Z][[]ni[f]=1[(][A][f][x][(][(][A][S][x][(][(][i][)][S][)][)][,][ y][,][ y]S[∗]S[∗][(][i][;][)][ z][;][ Z][i][)][)]] _[−]−_
P
_fP(Ax(S[(][i][)]), yS[∗]_ [(][i][)] [;][ z][i][)]][.]

Now we get


_n_

_gi(S)_ 2 1 + _[β]_
_≤_ _µ_
_i=1_

X 


_nF_ (Ax(S), yS[∗] [)][ −] _[nF][S][(][A][x][(][S][)][,][ ˆ]yS[∗]_ [)][ −]


_Lnϵ._ (13)


For any i = 1, ..., n, we define hi(S) = gi(S) ES _zi_ [gi(S)].
_−_ _\{_ _}_

We also get that ES _zi_ [hi(S)] = 0 and Ezi [hi(S)] = Ezi [gi(S)] Ezi ES _zi_ [gi(S)] = 0 0 = 0.
_\{_ _}_ _−_ _\{_ _}_ _−_
Moreover, for any j [n] with j = i, and zj[′′]
_∈_ _̸_ _[∈Z][, we have the following inequality]_

_hi(S)_ _hi(z1, ..., zj_ 1, zj[′′][, z][j][+1][, ..., z][n][)][| ≤|][g][i][(][S][)][ −] _[g][i][(][z][1][, ..., z][j][−][1][, z]j[′′][, z][j][+1][, ..., z][n][)][|]_
_|_ _−_ _−_

+ ES _zi_ [gi(S)] ES _zi_ [gi(z1, ..., zj 1, zj[′′][, z][j][+1][, ..., z][n][)]][|][.]
_|_ _\{_ _}_ _−_ _\{_ _}_ _−_


Denote Sj[(][i][)] as the set collected by replacing the j-th element of S[(][i][)] with zj[′′][. For the first term]
_gi(S)_ _gi(z1, ..., zj_ 1, zj[′′][, z][j][+1][, ..., z][n][)][|][, we have]
_|_ _−_ _−_

_gi(S)_ _gi(z1, ..., zj_ 1, zj[′′][, z][j][+1][, ..., z][n][)]
_−_ _−_

= Ezi[′] EZ[f (Ax(S[(][i][)]), yS[∗] [(][i][)] [;][ Z][)]][ −] _[f]_ [(][A][x][(][S][(][i][)][)][,][ y]S[∗] [(][i][)] [;][ z][i][)]

h i

_−_ Ezi[′] EZ[f (Ax(Sj[(][i][)][)][,][ y]S[∗] _j[(][i][)]_ [;][ Z][)]][ −] _[f]_ [(][A][x][(][S]j[(][i][)][)][,][ y]S[∗] _j[(][i][)]_ [;][ z][i][)]

h i


EZ[f (Ax(S[(][i][)]), yS[∗] [(][i][)] [;][ Z][)]][ −] _[f]_ [(][A][x][(][S]j[(][i][)][)][,][ y]S[∗] _j[(][i][)]_ [;][ Z][)]


Ezi[′] E

h

+ Ezi[′]


_f_ (Ax(S[(][i][)]), yS[∗] [(][i][)] [;][ z][i][)][ −] _[f]_ [(][A][x][(][S]j[(][i][)][)][,][ y]S[∗] _j[(][i][)]_ [;][ z][i][)] _._ (14)
i


-----

Furthermore, for any z, we have the following result which can help to bound the above inequality.
_f_ (Ax(S[(][i][)]), yS[∗] [(][i][)] [;][ z][)][ −] _[f]_ [(][A][x][(][S]j[(][i][)][)][,][ y]S[∗] _j[(][i][)]_ [;][ z][)]

_≤_ _f_ (Ax(S[(][i][)]), yS[∗] [(][i][)] [;][ z][)][ −] _[f]_ [(][A][x][(][S][(][i][)][)][,][ y]S[∗] _j[(][i][)]_ [;][ z][)] + _f_ (Ax(S[(][i][)]), yS[∗] _j[(][i][)]_ [;][ z][)][ −] _[f]_ [(][A][x][(][S]j[(][i][)][)][,][ y]S[∗] _j[(][i][)]_ [;][ z][)]

_≤L_ **yS[∗]** [(][i][)][ −] **[y]S[∗]** _j[(][i][)]_ + L _Ax(S[(][i][)]) −_ _Ax(Sj[(][i][)][)]_

_β_

_L_ _Ax(S[(][i][)])_ _Ax(Sj[(][i][)][)]_

_≤_ _µ_ [+ 1] _−_
 

_β_

_Lϵ,_ (15)

_≤_ _µ_ [+ 1]
 

where the third inequality follows from Lemma 5. Thus, we can bound the first term by 2 _βµ_ [+1] _Lϵ._

By a similar analysis, the second term ES _zi_ [gi(S)] ES _zi_ [gi(z1, ..., zj 1, zj[′′][, z][j][+1] _[, ..., z][n][)]][|]_
_|_ _\{_ _}_ _−_ _\{_ _}_ _−_

can also be bounded by 2 _βµ_ [+ 1] _Lϵ._
 

We now have verified that the three conditions in Lemma 2 are satisfied for hi(S). We obtain that
for any p ≥ 2, there holds

_n_

_hi(S)_ _√2pn_ _β_ _Lϵ_ log2 n _._ (16)
_p_ _µ_ [+ 1] _⌈_ _⌉_
_i=1_

X _[≤]_ [48]  

Thus, combined (13) with (16), we derive that

_nF_ (Ax(S), yS[∗] [)][ −] _[nF][S][(][A][x][(][S][)][,][ ˆ]yS[∗]_ [)][ −] _[n][E][S][′]_ _[F]_ [(][A][x][(][S][′][)][,][ y]S[∗] _[′]_ [) +][ n][E][S][′] _[F][S][(][A][x][(][S][′][)][,][ y]S[∗]_ _[′]_ [)][∥]p
_∥_


= _nF_ (Ax(S), yS[∗] [)][ −] _gi(S) +_ _gi(S)_ ES _zi_ [gi(S)]

_−_ _\{_ _}_ _p_

_i=1_ _i=1_ _i=1_

X X _n_ X _n_

_nF_ (Ax(S), yS[∗] [)][ −] _[nF][S][(][A][x][(][S][)][,][ ˆ]yS[∗]_ [)][ −] _gi(S)_ _hi(S)_
_≤_ _p_ [+] _p_

_i=1_ _i=1_

X X

2 1 + _[β]_ _Lnϵ + 48√2pn_ _β_ _Lϵ_ log2 n
_≤_ _µ_ _µ_ [+ 1] _⌈_ _⌉_
   

50√2 1 + _[β]_ _ϵLpn_ log2 n _._ (17)
_≤_ _µ_ _⌈_ _⌉_

 

Then, according to Lemma 1, for any δ ∈ (0, 1), with probability at least 1 − _δ/3, we have_

_F_ (Ax(S), yS[∗] [)][ −] _[F][S][(][A][x][(][S][)][,][ ˆ]yS[∗]_ [)]

ES′ _F_ (Ax(S[′]), yS[∗] _[′]_ [)][ −] [E][S][′] _[F][S][(][A][x][(][S][′][)][,][ y]S[∗]_ _[′]_ [)][|][ + 50]√2ϵeL 1 + _[β]_ log2 n log(3e/δ). (18)
_≤|_ _µ_ _⌈_ _⌉_

 

With a similar analysis to the proof of Part (a), we now begin to bound the variance of
ES′ _f_ (Ax(S[′]), yS[∗] _[′]_ [;][ z][i][)][.]

Ezi [(ES′ _f_ (Ax(S[′]), yS[∗] _[′]_ [;][ z][i][))][2][]][ ≤] [E][z]i [E][S][′] [[(][f] [(][A][x][(][S][′][)][,][ y]S[∗] _[′]_ [;][ z][i][))][2][]]

= EZES′ [(f (Ax(S[′]), yS[∗] _[′]_ [;][ Z][))][2][]]

= EZES[(f (Ax(S), yS[∗] [;][ Z][))][2][]][.]

There holds that ESES′ _FS(Ax(S[′]), yS[∗]_ _[′]_ [) =][ E][S][′] _[F]_ [(][A][x][(][S][′][)][,][ y]S[∗] _[′]_ [)][. Then, by the Bernstein inequality]
in Lemma 4, we obtain that with probability at least 1 − _δ/3,_

_|ES′_ _F_ (Ax(S[′]), yS[∗] _[′]_ [)][ −] [E][S][′] _[F][S][(][A][x][(][S][′][)][,][ y]S[∗]_ _[′]_ [)][|]

2EZES[(f (Ax(S), yS[∗] [;][ Z][))][2][] log(3][/δ][)] + [2][M][ log(3][/δ][)] _._ (19)

_≤_ _n_ 3n

r


-----

Combined (18) with (19), we finally obtain that with probability at least 1 − 2δ/3,

_F_ (Ax(S), yS[∗] [)][ −] _[F][S][(][A][x][(][S][)][,][ ˆ]yS[∗]_ [)]

3 3
2EZES[(f (Ax(S), yS[∗] [;][ Z][))][2][] log] _δ_ _δ_ 3e

+ [2][M][ log] + 50√2ϵeL _[β][ +][ µ]_ log2 n log _._

_≤s_ _n_ 3n _µ_ _⌈_ _⌉_ _δ_

     

 (20)

In the following, we define q = q(z1, ..., zn) = EZ[(f (Ax(S), yS[∗] [;][ Z][))][2][]][ and][ q][i][ =][ q][i][(][z][1][, ..., z][n][) =]
supzi _q(z1, ..., zn). So there holds qi_ _q for any i = 1, .., n and any_ _z1, ..., zn_ . Also,
there holds that∈Z _≥_ _{_ _} ∈Z_ _[n]_

_n_

(q _qi)[2]_
_−_
_i=1_

Xn 2

= EZ[(f (Ax(S), yS[∗] [;][ Z][))][2][]][ −] [sup] EZ[(f (Ax(S), yS[∗] [;][ Z][))][2][]]

_i=1_  _zi∈Z_ 

Xn

_≤_ Xi=1βEZh 2zsupi∈Z(f (Ax(S), yS[∗] [;][ Z][))][2][ −] [(][f] [(][A][x][(][S]β[)][,][ y]S[∗] [;][ Z][))][2]2[i][2]

_n_ _L[2]ϵ[2]_ 2EZ[f (Ax(S), yS[∗] [;][ Z][)] +] _Lϵ_
_≤_ _µ_ [+ 1] _µ_ [+ 1]
 _β_  2  _β_ 4   

8n _L[2]ϵ[2]q + 2n_ _L[4]ϵ[4],_
_≤_ _µ_ [+ 1] _µ_ [+ 1]
   

where the first inequality follows from the Jensen’s inequality and the second inequality follows from

2 4
a similar analysis to (12) or (15). Now, we know that q is 8n _βµ_ [+ 1] _L[2]ϵ[2], 2n_ _βµ_ [+ 1] _L[4]ϵ[4][]-_
weakly self-bounded. Thus, by Lemma 3, we obtain the following inequality with probability at    
least 1 − _δ/3,_


ESEZ[(f (Ax(S), yS[∗] [;][ Z][))][2][]][ −] [E][Z][[(][f] [(][A][x][(][S][)][,][ y]S[∗] [;][ Z][))][2][]]

_β_ 2 _β_ 4
16n _L[2]ϵ[2]ESEZ[(f_ (Ax(S), yS[∗] [;][ Z][))][2][] + 4][n] _L[4]ϵ[4]_ log(3/δ)

_≤_ _µ_ [+ 1] _µ_ [+ 1]

s     

_β_ 2 _β_ 2

= ESEZ[(f (Ax(S), yS[∗] [;][ Z][))][2][] + 1] _L[2]ϵ[2]_ 16n _L[2]ϵ[2]_ log(3/δ)

4 _µ_ [+ 1] _µ_ [+ 1]

s     

_β_ 2 _β_ 2

ESEZ[(f (Ax(S), yS[∗] [;][ Z][))][2][] + 1] _L[2]ϵ[2][]_ + 8n _L[2]ϵ[2]_ log(3/δ),

_≤_ 2[1] 4 _µ_ [+ 1] _µ_ [+ 1]

    

where the last inequality follows from _√ab_ 2 for all a, b > 0.

_≤_ _[a][+][b]_

Since EZ[(f (Ax(S), yS[∗] [;][ Z][))][2][]][ ≤] _[MF]_ [(][A][x][(][S][)][,][ y]S[∗] [)][, we have]

ESEZ[(f (Ax(S), yS[∗] [;][ Z][))][2][]][ −] [2][MF] [(][A][x][(][S][)][,][ y]S[∗] [)]

_β_ 2 _β_ 2

_L[2]ϵ[2]_ + 16n _L[2]ϵ[2]_ log(3/δ). (21)

_≤_ [1]4 _µ_ [+ 1] _µ_ [+ 1]

   

Plugging (21) into (20), we finally obtain that with probability at least 1 − _δ,_

_F_ (Ax(S), yS[∗] [)][ −] _[F][S][(][A][x][(][S][)][,][ ˆ]yS[∗]_ [)]

2 2

4MF (Ax(S), yS[∗] [) +][ 1]2 _βµ_ [+ 1] _L[2]ϵ[2]_ + 32n _βµ_ [+ 1] _L[2]ϵ[2]_ log(3/δ) log(3/δ)

v

_≤u_    _n_   

u
t


+ [2][M][ log(3][/δ][)] + 50

3n


2ϵeL _[β][ +][ µ]_


_⌈log2 n⌉_ log(3e/δ). (22)


-----

By the elementary inequalities _√ab ≤_ _ηa +_ _η[1]_ _[b][ and]_ _√a + b ≤_ _[√]a +_ _√b for any a, b, η > 0, we_

have the following inequality with probability at least 1 − _δ_

_M_ _β_

_F_ (Ax(S), yS[∗] [)][ −] [(1 +][ η][)][F][S][(][A][x][(][S][)][,][ ˆ]yS[∗] [)][ ≤] _[C][ 1 +][ η]_ _Lϵ log2 n log [1]_ _,_

_η_ _n_ [log 1]δ [+] _µ_ [+ 1] _δ_

   

where C is an absolute constant. The proof is complete.

A.3 PROOF OF PART (C)

We then prove the strong PD population risk bound. To begin, we introduce the following concentration inequality, which is a moment version of the Bernstein inequality.
**Lemma 6. (Boucheron et al., 2013) If z1, ..., zn are i.i.d., zero mean and |zi| ≤** _M almost surely._
_Then, for any p ≥_ 2,


Ezi[2] _p + 4pM._
_i=1_

X 


_zi_
_p_
_i=1_

_[≤]_ [6]

X


_Proof. Let S_ = _z1, ..., zn_ be a set of independent random variables each taking values
_{_ _}_
in Z and S[′] = _{z1[′]_ _[, ..., z]n[′]_ _[}][ be its independent copy.]_ For any i _∈_ [n], define S[(][i][)] =
_z1, ..., zi_ 1, zi[′][, z][i][+1][, ..., z][n][}][ be a dataset by replacing the][ i][-th sample in][ S][ with another i.i.d.]
_{_ _−_
sample zi[′][. Denote][ y]S[∗] [= arg max][y][∈Y][ F] [(][A][x][(][S][)][,][ y][)][,][ ˆ]yS[∗] [= arg max][y][∈Y][ F][S][(][A][x][(][S][)][,][ y][)][,][ x]S[∗] [=]
arg minx∈X F (x, Ay(S)) and ˆx[∗]S [= arg min][x][∈X][ F][S][(][x][, A][y][(][S][))][.]

The proof of Part (c) requires a pretty technical error decomposition, i.e.,
(Ax(S), Ay(S))
_△[s]_

=F (Ax(S), yS[∗] [)][ −] [inf]
**x[′]** _[F]_ [(][x][′][, A][y][(][S][))]
_∈X_

=F (Ax(S), yS[∗] [)][ −] _[F][S][(][A][x][(][S][)][,][ ˆ]yS[∗]_ [) +][ E][S][′] _[F][S][(][A][x][(][S][′][)][,][ y]S[∗]_ _[′]_ [)][ −] [E][S][F] [(][A][x][(][S][)][,][ y]S[∗] [)]

+ ESF (x[∗]S[, A][y][(][S][))][ −] [E][S][′] _[F][S][(][x]S[∗]_ _[′]_ _[, A][y][(][S][′][)) +][ F][S][(ˆ]x[∗]S[, A][y][(][S][))][ −]_ [inf]
**x[′]** _[F]_ [(][x][′][, A][y][(][S][))]
_∈X_

_−_ ES′ _FS(Ax(S[′]), yS[∗]_ _[′]_ [) +][ E][S][F] [(][A][x][(][S][)][,][ y]S[∗] [) +][ E][S][′] _[F][S][(][x]S[∗]_ _[′]_ _[, A][y][(][S][′][))][ −]_ [E][S][F] [(][x]S[∗] _[, A][y][(][S][))]_
+ FS(Ax(S), ˆyS[∗] [)][ −] _[F][S][(ˆ]x[∗]S[, A][y][(][S][))][.]_
Let’s first consider the term F (Ax(S), yS[∗] [)][ −] _[F][S][(][A][x][(][S][)][,][ ˆ]yS[∗]_ [) +][ E][S][′] _[F][S][(][A][x][(][S][′][)][,][ y]S[∗]_ _[′]_ [)][ −]
ESF (Ax(S), yS[∗] [)][. It can be then decomposed into]


_F_ (Ax(S), yS[∗] [)][ −] _[F][S][(][A][x][(][S][)][,][ ˆ]yS[∗]_ [) + 1]

_n_


EZ[f (Ax(S[(][i][)]), yS[∗] [(][i][)] [;][ Z][)]][ −] _[f]_ [(][A][x][(][S][(][i][)][)][,][ y]S[∗] [(][i][)] [;][ z][i][)]


Ezi[′]
_i=1_

X


_−_ _n[1]_


Ezi[′]
_i=1_

X


EZ[f (Ax(S[(][i][)]), yS[∗] [(][i][)] [;][ Z][)]][ −] _[f]_ [(][A][x][(][S][(][i][)][)][,][ y]S[∗] [(][i][)] [;][ z][i][)] + ES′ _FS(Ax(S[′]), yS[∗]_ _[′]_ [)]
i


_−_ ESF (Ax(S), yS[∗] [)][,]
which can be bounded by the proof techniques in the proof of Part (b). According to (17), we know
that
_F_ (Ax(S), yS[∗] [)][ −] _[F][S][(][A][x][(][S][)][,][ ˆ]yS[∗]_ [) +][ E][S][′] _[F][S][(][A][x][(][S][′][)][,][ y]S[∗]_ _[′]_ [)][ −] [E][S][F] [(][A][x][(][S][)][,][ y]S[∗] [)][∥]p
_∥_

50√2 1 + _[β]_ _ϵLp_ log2 n _._ (23)
_≤_ _µ_ _⌈_ _⌉_

 

For the second term ESF (x[∗]S[, A][y][(][S][))] _−_ ES′ _FS(x[∗]S[′]_ _[, A][y][(][S][′][))]_ + _FS(ˆx[∗]S[, A][y][(][S][))]_ _−_
inf **x[′]** _F_ (x[′], Ay(S)), we have the following decomposition
_∈X_
_nFS(ˆx[∗]S[, A][y][(][S][))][ −]_ _[n][ inf]_
**x[′]** _[F]_ [(][x][′][, A][y][(][S][))]
_∈X_


=nFS(ˆx[∗]S[, A][y][(][S][))][ −]


EZ _f_ (x[∗]S[, A][y][(][S][)][, Z][)][ −] [E][z]i[′] [[][f] [(][x]S[∗] [(][i][)] _[, A][y][(][S][(][i][)][);][ Z][)]]_
_i=1_

X h


_f_ (x[∗]S[(][i][)] _[, A][y][(][S][(][i][)][);][ z][i][)][ −]_ [E][Z][[][f] [(][x][∗]S[(][i][)] _[, A][y][(][S][(][i][)][);][ Z][)]]_


_f_ (x[∗]S[(][i][)] _[, A][y][(][S][(][i][)][);][ z][i][)]_


Ezi[′]
_i=1_

X


Ezi[′]
_i=1_

X


-----

It is clear that


_f_ (x[∗]S[(][i][)] _[, A][y][(][S][(][i][)][);][ z][i][)]_
i

_f_ (x[∗]S[(][i][)] _[, A][y][(][S][(][i][)][);][ z][i][)][ −]_ _[f]_ [(][x]S[∗] _[, A][y][(][S][);][ z][i][) +][ f]_ [(][x][∗]S[, A][y][(][S][);][ z][i][)]


Ezi[′]
_i=1_

Xn

Ezi[′]
_i=1_

X


_≥nFS(ˆx[∗]S[, A][y][(][S][)) +]_


Ezi[′]
_i=1_

X


_f_ (x[∗]S[(][i][)] _[, A][y][(][S][(][i][)][);][ z][i][)][ −]_ _[f]_ [(][x]S[∗] _[, A][y][(][S][);][ z][i][)]_ _._ (24)
i


Denote gi(S) = Ezi[′] _f_ (x[∗]S[(][i][)] _[, A][y][(][S][(][i][)][);][ z][i][)][ −]_ [E][Z][[][f] [(][x][∗]S[(][i][)] _[, A][y][(][S][(][i][)][);][ Z][)]]_ . By (24), we now get
 


_nFS(ˆx[∗]S[, A][y][(][S][))][ −]_ _[n][ inf]_
**x[′]** _[F]_ [(][x][′][, A][y][(][S][))][ −]
_∈X_


_gi(S)_
_i=1_

X


_≤−_ EZ _f_ (x[∗]S[, A][y][(][S][);][ Z][)][ −] [E][z]i[′] [[][f] [(][x]S[∗] [(][i][)] _[, A][y][(][S][(][i][)][);][ Z][)]]_

_i=1_

X h


Ezi[′]
_i=1_

X


_f_ (x[∗]S[(][i][)] _[, A][y][(][S][(][i][)][);][ z][i][)][ −]_ _[f]_ [(][x]S[∗] _[, A][y][(][S][);][ z][i][)]_


Furthermore, according to Lemma 5, we have
_f_ (x[∗]S[, A][y][(][S][);][ Z][)][ −] _[f]_ [(][x][∗]S[(][i][)] _[, A][y][(][S][(][i][)][);][ Z][)]_ 1 + _[β]_ _L_ _Ay(S)_ _Ay(S[(][i][)])_ _._
_≤_ _µ_ _∥_ _−_ _∥_
 

Similarly,
_f_ (x[∗]S[(][i][)] _[, A][y][(][S][(][i][)][);][ z][i][)][ −]_ _[f]_ [(][x]S[∗] _[, A][y][(][S][);][ z][i][)]_ 1 + _[β]_ _L_ _Ay(S)_ _Ay(S[(][i][)])_ _._
_≤_ _µ_ _∥_ _−_ _∥_
 

Thus, we obtain

_n_

_nFS(ˆx[∗]S[, A][y][(][S][))][ −]_ _[n][ inf]_ _gi(S)_
**x[′]** _[F]_ [(][x][′][, A][y][(][S][))][ −]
_∈X_ _i=1_

_n_ X

2 1 + _[β]_ _L_ _Ay(S[(][i][)])_ _Ay(S)_ 2n 1 + _[β]_ _Lϵ,_ (25)

_≤_ _µ_ _∥_ _−_ _∥≤_ _µ_

_i=1_

X    

where the last inequality follows from the definition of argument stability (Part 2 of Definition 3).


Furthermore, we define hi(S) = gi(S) ES _zi_ [gi(S)]. For hi(S), We have ES _zi_ [hi(S)] = 0
_−_ _\{_ _}_ _\{_ _}_
and Ezi [hi(S)] = Ezi [gi(S)] Ezi ES _zi_ [gi(S)] = 0 0 = 0. Moreover, for any j [n] with
_−_ _\{_ _}_ _−_ _∈_
_j_ = i, and zj[′′]
_̸_ _[∈Z][, we get]_

_hi(S)_ _hi(z1, ..., zj_ 1, zj[′′][, z][j][+1][, ..., z][n][)][| ≤] [2] 1 + _[β]_ _Lϵ,_
_|_ _−_ _−_ _µ_
 

where this inequality follows from a similar analysis to (14) and (15) of the proof of Part (b). We
thus can obtain that for any p ≥ 2, there holds

_n_

_hi(S)_ _√2pn_ _β_ _Lϵ_ log2 n _._ (26)
_p_ _µ_ [+ 1] _⌈_ _⌉_
_i=1_

X _[≤]_ [48]  

Combined (25) with (26), we finally get the bound of the second term:

ESF (x[∗]S[, A][y][(][S][))][ −] [E][S][′] _[F][S][(][x][∗]S[′]_ _[, A][y][(][S][′][)) +][ F][S][(ˆ]x[∗]S[, A][y][(][S][))][ −]_ [inf]
**x[′]** _[F]_ [(][x][′][, A][y][(][S][))] _p_
_∈X_

_n_ _n_

_FS(ˆx[∗]S[, A][y][(][S][))][ −]_ [inf] _gi(S)_ [1] _hi(S)_
_≤_ **x[′]** _[F]_ [(][x][′][, A][y][(][S][))][ −] _n[1]_ _p_ [+] _n_ _p_
_∈X_ _i=1_ _i=1_

X X

50√2 1 + _[β]_ _ϵLp_ log2 n _._ (27)
_≤_ _µ_ _⌈_ _⌉_

 


-----

We then consider the third term −ES′ _FS(Ax(S[′]), yS[∗]_ _[′]_ [)+][E][S][F] [(][A][x][(][S][)][,][ y]S[∗] [)+][E][S][′] _[F][S][(][x][∗]S[′]_ _[, A][y][(][S][′][))][−]_
ESF (x[∗]S[, A][y][(][S][))][.] It is clear that ES[ES′ _FS(x[∗]S[′]_ _[, A][y][(][S][′][))]]_ = ESF (x[∗]S[, A][y][(][S][))][ and]
ES[ES′ _FS(Ax(S[′]), yS[∗]_ _[′]_ [)] =][ E][S][F] [(][A][x][(][S][)][,][ y]S[∗] [)][.]

Moreover, we having the following important property due to the strong convexity and strong concavity of F,
E (ES′ _f_ (x[∗]S[′] _[, A][y][(][S][′][);][ z][i][)][ −]_ [E][S][′] _[f]_ [(][A][x][(][S][′][)][,][ y]S[∗] _[′]_ [;][ z][i][))][2][]

EE _S′_ _L[2](_ **x[∗]S[′][ −]** _[A][x][(][S][′][)][∥]_ [+][ ∥][A][y][(][S][′][)][ −] **[y]S[∗]** _[′]_ _[∥][)][2][]_
_≤_ _∥_

_≤2L[2]EES′_ [∥x[∗]S[′][ −] _[A][x][(][S][′][)][∥][2][ +][ ∥][A][y][(][S][′][)][ −]_ **[y]S[∗]** _[′]_ _[∥][2][]]_

[]

_≤4L[2]µ[−][1]EES[′]_ [F (Ax(S[′]), yS[∗] _[′]_ [)][ −] _[F]_ [(][x][∗]S[′] _[, A][x][(][S][′][))]]_

=4L[2]µ[−][1]ES′ [F (Ax(S[′]), yS[∗] _[′]_ [)][ −] _[F]_ [(][x][∗]S[′] _[, A][x][(][S][′][))]][,]_ (28)
where the first inequality follows from Jensen’s inequality and the Lipschitz continuity of f (Assumption 1), the second inequality follows from that (a + b)[2] _≤_ 2a[2] + 2b[2] and the third inequality
follows from the property of strong convexity and strong concavity of F and the optimality condition, derived as follows,
_F_ (Ax(S[′]), yS[∗] _[′]_ [)][ −] _[F]_ [(][x][∗]S[′] _[, A][x][(][S][′][))]_

=F (Ax(S[′]), yS[∗] _[′]_ [)][ −] _[F]_ [(][x]S[∗] _[′]_ _[,][ y]S[∗]_ _[′]_ [) +][ F] [(][x]S[∗] _[′]_ _[,][ y]S[∗]_ _[′]_ [)][ −] _[F]_ [(][x]S[∗] _[′]_ _[, A][x][(][S][′][))]_

_Ax(S[′])_ **x[∗]S[′]** _[∥][2][ +][ ∥][y]S[∗]_ _[′][ −]_ _[A][x][(][S][′][)][∥]_ _._

_≥_ _[µ]2_ _∥_ _−_

h i

It is clear that E[(Z − EZ)[2]] ≤ E[Z [2]]. Therefore, by the variance bound in (28) and applying the moment Bernstein inequality in Lemma 6 to the sum of independent random variables
_−ES′_ _f_ (Ax(S[′]), yS[∗] _[′]_ [;][ z][i][)+][E][S][′] _[f]_ [(][x]S[∗] _[′]_ _[, A][y][(][S][′][);][ z][i][)+][E][S][F]_ [(][A][x][(][S][)][,][ y]S[∗] [)][−][E][S][F] [(][x][∗]S[, A][y][(][S][))][, we have]
the following inequality for alln _p ≥_ 2,

[1] ES′ _f_ (Ax(S[′]), yS[∗] _[′]_ [;][ z][i][) +][ E][S][′] _[f]_ [(][x]S[∗] _[′]_ _[, A][y][(][S][′][);][ z][i][) +][ E][S][F]_ [(][A][x][(][S][)][,][ y]S[∗] [)][ −] [E][S][F] [(][x]S[∗] _[, A][y][(][S][))]_

_n_ _−_

_i=1_

X


ES′ [F (Ax(S[′]), yS[∗] _[′]_ [)][ −] _[F]_ [(][x]S[∗] _[′]_ _[, A][x][(][S][′][))]4][L][2][p]_ + [16][pM]

_nµ_ _n_


_≤_ 6


From Definition 1, we know that the last term FS(Ax(S), ˆyS[∗] [)][ −] _[F][S][(ˆ]x[∗]S[, A][y][(][S][))][ is actually the]_
strong PD empirical risk △S[s] [(][A][x][(][S][)][, A][y][(][S][))][.]

Based on the above analysis, we have derived that for each p ≥ 2,
_F_ (Ax(S), yS[∗] [)][ −] [inf] _S[(][A][x][(][S][)][, A][y][(][S][))]_
**x[′]** _[F]_ [(][x][′][, A][y][(][S][))][ −△][s] _p_
_∈X_

12 ES′ [F (Ax(S[′]), yS[∗] _[′]_ [)][ −] _[F]_ [(][x][∗]S[′] _[, A][x][(][S][′][))]]_ _[pL][2]_ + 100√2 1 + _[β]_ _ϵLp_ log2 n
_≤_ s _nµ_ [+ 16]n[pM] _µ_ _⌈_ _⌉_

 


12pL[2] + [16][pM]

_nµ_ _n_


_η_

_S[′]_ [)][ −] _[F]_ [(][x][∗]S[′] _[, A][x][(][S][′][))] + 1 +][ η]_
1 + η [E][S][′] [[][F] [(][A][x][(][S][′][)][,][ y][∗] _η_


1 + _[β]_


_ϵLp⌈log2 n⌉,_ (29)


+ 100


where the last inequality holds since for any a, b, η > 0, _√ab ≤_ _ηa +_ _η[b]_ [.]

Taking p = 2 and using the Cauchy-Schwarz inequality, we obtain that


ES _F_ (Ax(S), yS[∗] [)][ −] [inf] _S[(][A][x][(][S][)][, A][y][(][S][))]_
**x[′]** _[F]_ [(][x][′][, A][y][(][S][))][ −△][s]
_∈X_
h i

_F_ (Ax(S), yS[∗] [)][ −] [inf] _S[(][A][x][(][S][)][, A][y][(][S][))][∥][2]_
_≤∥_ **x[′]** _[F]_ [(][x][′][, A][y][(][S][))][ −△][s]
_∈X_

_η_ _S[′]_ [)][ −] _[F]_ [(][x][∗]S[′] _[, A][x][(][S][′][))] + 1 +][ η]_ 24L[2] + [32][M]
_≤_ 1 + η [E][S][′] [[][F] [(][A][x][(][S][′][)][,][ y][∗] _η_ _nµ_ _n_


1 + _[β]_


+ 200


_ϵL⌈log2 n⌉._


-----

Since ES′ [F (Ax(S[′]), yS[∗] _[′]_ [)][ −] _[F]_ [(][x][∗]S[′] _[, A][x][(][S][′][))] =][ E][S][[][F]_ [(][A][x][(][S][)][,][ y]S[∗] [)][ −] _[F]_ [(][x][∗]S[, A][x][(][S][))]][, we finally]
get


ES[F (Ax(S), yS[∗] [)][ −] [inf]
**x[′]** _[F]_ [(][x][′][, A][y][(][S][))]]
_∈X_

(1 + η) ES _S_ [(][A][x][(][S][)][, A][y][(][S][)) + 24][L][2][(1 +][ η][)] + [32][M]
_≤_ _△[s]_ _nµη_ _n_



1 + _[β]_


+ 200


_ϵL⌈log2 n⌉_


Plugging this inequality into (29), we thus have that for each p ≥ 2,
_F_ (Ax(S), yS[∗] [)][ −] [inf] _S[(][A][x][(][S][)][, A][y][(][S][))]_
**x[′]** _[F]_ [(][x][′][, A][y][(][S][))][ −△][s]
_∈X_

_η_ ES _S_ [(][A][x][(][S][)][, A][y][(][S][)) + 24][L][2][(1 +][ η][)] + [32][M] + 200√2
_≤_ _△[s]_ _nµη_ _n_



1 + _[β]_


_ϵL⌈log2 n⌉_


+ [12][pL][2][(1 +][ η][)] + [16][pM]

_nµη_ _n_


1 + _[β]_


+ 100


_ϵLp⌈log2 n⌉._


According to Lemma 1, for any δ > 0, with probability at least 1 − _δ, there holds that_

_F_ (Ax(S), yS[∗] [)][ −] [inf] _S[(][A][x][(][S][)][, A][y][(][S][)) +][ η][E][S]_ _S_ [(][A][x][(][S][)][, A][y][(][S][))]
**x[′]∈X** _[F]_ [(][x][′][, A][y][(][S][))][ ≤△][s] _[△][s]_

_L2(1 + η)_ 1
+ C(1 + η) + _[M]_ 1 + _[β]_ _ϵL log2 n_ log _,_

_nµη_ _n_ [+] _µ_ _δ_

     

where C > 0 is an absolute constant. The proof is complete.


(30)


A.4 PROOF OF PART (D)

We now prove the strong PD generalization error bound.

_Proof. From (30) in the proof of Part (c), we know that for any δ > 0, with probability at least 1_ _−_ _δ,_
there holds that


_△[s]_ (Ax(S), Ay(S)) −△S[s] [(][A][x][(][S][)][, A][y][(][S][))]

=F (Ax(S), yS[∗] [)][ −] [inf] _S[(][A][x][(][S][)][, A][y][(][S][))]_
**x[′]** _[F]_ [(][x][′][, A][y][(][S][))][ −△][s]
_∈X_

_L2(1 + η)_
_ηES_ _S_ [(][A][x][(][S][)][, A][y][(][S][)) +][ C][(1 +][ η][)] + _[M]_ 1 + _[β]_
_≤_ _△[s]_ _nµη_ _n_ [+] _µ_
 

Therefore, the proof is complete.


1
_ϵL log2 n_ log

_δ_

 


A.5 PROOF OF PART (E)

We finally prove the excess primal population risk bound.

_Proof. Denote x[∗]_ = arg minx∈X R(x) and yS[∗] [= arg max][y][∈Y][ F] [(][A][x][(][S][)][,][ y][)][. Firstly, we have the]
following decomposition

_R(Ax(S))_ inf
_−_ **x[′]** _[R][(][x][′][) =][ R][(][A][x][(][S][))][ −]_ _[R][S][(][A][x][(][S][)) +][ R][S][(][A][x][(][S][))][ −]_ _[F][S][(][x][∗][, A][y][(][S][))]_
_∈X_

+ FS(x[∗], Ay(S)) _F_ (x[∗], Ay(S)) + F (x[∗], Ay(S)) _R(x[∗])._ (31)
_−_ _−_

Consider the first term R(Ax(S)) _RS(Ax(S)). From (22) of the Part (b), we know that with_
_−_
probability at least 1 − _δ,_


_R(Ax(S))_ _RS(Ax(S))_ + 50
_−_ _≤_ [2][M][ log(3]3n _[/δ][)]_


2ϵeL _[β][ +][ µ]_


_⌈log2 n⌉_ log(3e/δ)


2 2
_βµ_ [+ 1] _L[2]ϵ[2]_ + 32n _βµ_ [+ 1] _L[2]ϵ[2]_ log(3/δ) log(3/δ)
 _n_   


4MF (Ax(S), yS[∗] [) +][ 1]2


-----

For the second term RS(Ax(S)) _FS(x[∗], Ay(S)), we have RS(Ax(S))_ _FS(x[∗], Ay(S))_
_−_ _−_ _≤_
_RS(Ax(S))_ inf **x′** _FS(x[′], Ay(S)) =_ _S[(][A][x][(][S][)][, A][y][(][S][))][.]_
_−_ _∈Y_ _△[s]_

Note that under Assumption 1, the argument stability implies the uniform stability. Therefore, for
the third term FS(x[∗], Ay(S)) _F_ (x[∗], Ay(S)), from (10) of Part (a), we know that with probability
_−_
at least 1 − _δ_


_FS(x[∗], Ay(S))_ _F_ (x[∗], Ay(S)) + 50
_−_ _≤_ [2][M][ log(3]3n _[/δ][)]_


2eϵ⌈log2 n⌉ log(3e/δ)


(4MF (x[∗], Ay(S)) + [1]2 _[ϵ][2][ + 32][nϵ][2][ log(3][/δ][)) log(3][/δ][)]_

+ _._

s _n_

It is clear that F (x[∗], Ay(S)) _R(x[∗])_ 0.
_−_ _≤_

Since F (x[∗], Ay(S)) ≤ supy′∈Y F (x[∗], y[′]) = R(x[∗]) = inf **x′∈X R(x), based on the above results,**
we have the following inequality with probability at least 1 − 2δ


_R(Ax(S))_ inf
_−_ **x[′]** _[R][(][x][)]_
_∈X_

4MF (Ax(S), yS[∗] [) +][ 1]2

v
u 


2 2
_βµ_ [+ 1] _L[2]ϵ[2]_ + 32n _βµ_ [+ 1] _L[2]ϵ[2]_ log(3/δ) log(3/δ)
 _n_   


(4M inf **x′∈X R(x) +** [1]2 _[ϵ][2][ + 32][nϵ][2][ log(3][/δ][)) log(3][/δ][)]_


+ [4][M][ log(3][/δ][)]

3n


2ϵeL _[β][ +][ µ]_


_⌈log2 n⌉_ log(3e/δ) + △S[s] [(][A][x][(][S][)][, A][y][(][S][)) + 50]


2eϵ⌈log2 n⌉ log(3e/δ)


+ 50


2 2
_βµ_ [+ 1] _L[2]ϵ[2]_ + 32n _βµ_ [+ 1] _L[2]ϵ[2]_ log(3/δ) log(3/δ)
  _n_  


_η_

_S[)]_
1 + η [F] [(][A][x][(][S][)][,][ y][∗]


( [1]2 _[ϵ][2][ + 32][nϵ][2][ log(3][/δ][)) log(3][/δ][)]_


+ [1 +][ η]


4M log(3/δ) + [4][M][ log(3][/δ][)]

_n_ 3n


_η_ 4M log(3/δ)
+

1 + η **x[inf][′]∈X** _[R][(][x][) + 1 +]η_ _[ η]_ _n_

+ 50√2ϵeL _[β][ +][ µ]_ log2 n log(3e/δ) + _S[(][A][x][(][S][)][, A][y][(][S][)) + 50]_

_µ_ _⌈_ _⌉_ _△[s]_


2eϵ⌈log2 n⌉ log(3e/δ),


where the last inequality follows from the elementary inequalities _√ab ≤_ _ηa +_ _η[1]_ _[b][ and]_ _√a + b ≤_

_√a +_ _√b for any a, b > 0. Therefore, by a rearrangement, we have the following inequality with_

probability at least 1 − _δ_


_R(Ax(S))_ (1 + 2η) inf
_−_ **x[′]** _[R][(][x][)]_
_∈X_

_M_ _β_

_C_ [1 +][ η] _Lϵ log2 n log [1]_ _S[(][A][x][(][S][)][, A][y][(][S][))]_
_≤_ _η_ _n_ [log 1]δ [+] _µ_ [+ 1] _δ_ [+][ △][s]

  


that is


_R(Ax(S))_ (1 + η) inf
_−_ **x[′]** _[R][(][x][)]_
_∈X_

_M_ _β_

_C_ [2 +][ η] _Lϵ log2 n log [1]_ _S[(][A][x][(][S][)][, A][y][(][S][))]_
_≤_ _η_ _n_ [log 1]δ [+] _µ_ [+ 1] _δ_ [+][ △][s]

  

where C is an absolute constant. The proof is complete.

Till here, the proof of Theorem 1 is complete.


-----

B EMPIRICAL SADDLE POINT

Empirical saddle point (ESP) problem refers to problem (2), which is also known as sample average
approximation (SAA) (Zhang et al., 2021a). We denote (ˆx[∗]S[,][ ˆ]yS[∗] [)][ as the ESP solution to (2), which]
is analogy to the ERM in stochastic optimization (Shalev-Shwartz et al., 2010). We first provide the
main theorem of the ESP solution, as shown below.

**Theorem 3. Assume for all z, the function (x, y) 7→** _f_ (x, y; z) is µ-SC-SC. Suppose |f (x, y; z)| ≤
_M for some M > 0 and x ∈X_ _, y ∈Y, z ∈Z. Denote Ax(S) = ˆx[∗]S_ _[and][ A][y][(][S][) = ˆ]yS[∗]_ _[for]_
(ˆx[∗]S[,][ ˆ]yS[∗] [)][. Fixed any][ η >][ 0][. There exists an absolute positive constant][ C][.]

_(a) If Assumption 1 holds, then for any δ > 0, with probability at least 1 −_ _δ, we have_


_M_

_n_ [log(1][/δ][) + 4]nµ[L] [log][2][ n][ log(1][/δ][)]




_F_ (ˆx[∗]S[,][ ˆ]yS[∗] [)][ ≤] [(1 +][ η][)][F][S][(ˆ]x[∗]S[,][ ˆ]yS[∗] [) +][ C][ 1 +][ η]

_η_


_(b) If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ, we have_


_M_ _β_ 4L2

_n_ [log 1]δ [+] _µ_ [+ 1] _nµ_ [log][2][ n][ log 1]δ

  


_R(ˆx[∗]S[)][ ≤]_ [(1 +][ η][)][R][S][(ˆ]x[∗]S[) +][ C][ 1 +][ η]

_η_


_(c) If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ, we have_


_L2(1 + η)_
(ˆx[∗]S[,][ ˆ]yS[∗] [)][ ≤] _[C][(1 +][ η][)]_ + _[M]_ 1 + _[β]_
_△[s]_ _nµη_ _n_ [+] _µ_
 


4L2 1

log
_nµ_ [log][2][ n] _δ_

  


_(d) If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ, we have_


_△[s]_ (ˆx[∗]S[,][ ˆ]yS[∗] [)][ −△]S[s] [(ˆ]x[∗]S[,][ ˆ]yS[∗] [)]

_L2(1 + η)_
_C(1 + η)_ + _[M]_ 1 + _[β]_
_≤_ _nµη_ _n_ [+] _µ_
 


4L2 1

log
_nµ_ [log][2][ n] _δ_

  


_(e) If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ, we have_


_M_ _β_ 4L2

_n_ [log 1]δ [+] _µ_ [+ 1] _nµ_ [log][2][ n][ log 1]δ

  


_R(ˆx[∗]S[)][ ≤]_ [(1 +][ η][) inf]
**x∈X** _[R][(][x][) +][ C][ 2 +]η_ _[ η]_


_Proof. To prove Theorem 3, we should derive the strong PD empirical risk bound and the stability_
bound of (ˆx[∗]S[,][ ˆ]yS[∗] [)][. It is easy to verify that][ △][s]S[(ˆ]x[∗]S[,][ ˆ]yS[∗] [) = 0][ (Zhang et al., 2021a). We then]
investigate the stability bound of (ˆx[∗]S[,][ ˆ]yS[∗] [)][.]

Let S = _z1, ..., zn_ be a set of independent random variables each taking values in . For any
_{_ _}_ _Z_
_i_ [n], define S[(][i][)] = _z1, ..., zi_ 1, zi[′][, z][i][+1][, ..., z][n][}][ be a dataset by replacing the][ i][-th sample in]
_∈_ _{_ _−_
_S with another i.i.d. sample zi[′][. We define][ F]S[(][i][)][ be the empirical risk on dataset][ S][(][i][)][ and define]_
(ˆx[∗]S[(][i][)] _[,][ ˆ]yS[∗]_ [(][i][)] [)][ be the ESP solution on dataset][ S][(][i][)][.]


-----

Then we have

_FS(ˆx[∗]S[(][i][)]_ _[,][ ˆ]yS[∗]_ [)][ −] _[F][S][(ˆ]x[∗]S[,][ ˆ]yS[∗]_ [(][i][)] [)]

_n_

= [1] _f_ (ˆx[∗]S[(][i][)] _[,][ ˆ]yS[∗]_ [;][ z][j][)][ −] _[f]_ [(ˆ]x[∗]S[,][ ˆ]yS[∗] [(][i][)] [;][ z][j][)]

_n_

_j=1_

X  n 

= [1] (f (ˆx[∗]S[(][i][)] _[,][ ˆ]yS[∗]_ [;][ z][j][)][ −] _[f]_ [(ˆ]x[∗]S[,][ ˆ]yS[∗] [(][i][)] [;][ z][j][)) +][ f] [(ˆ]x[∗]S[(][i][)] _[,][ ˆ]yS[∗]_ [;][ z]i[′][)][ −] _[f]_ [(ˆ]x[∗]S[,][ ˆ]yS[∗] [(][i][)] [;][ z]i[′][)]

_n_

_j=1,j=i_

 X̸ 

+ [1] _f_ (ˆx[∗]S[(][i][)] _[,][ ˆ]yS[∗]_ [;][ z][i][)][ −] _[f]_ [(ˆ]x[∗]S[,][ ˆ]yS[∗] [(][i][)] [;][ z][i][)] _f_ (ˆx[∗]S[(][i][)] _[,][ ˆ]yS[∗]_ [;][ z]i[′][)][ −] _[f]_ [(ˆ]x[∗]S[,][ ˆ]yS[∗] [(][i][)] [;][ z]i[′][)]

_n_ _−_ _n[1]_

=FS(i) (ˆ x[∗]S[(][i][)] _[,][ ˆ]yS[∗]_ [)][ −] _[F]S[(][i][)]_ [(ˆ]x[∗]S[,][ ˆ]yS[∗] [(][i][)] [)]    

+ [1] _f_ (ˆx[∗]S[(][i][)] _[,][ ˆ]yS[∗]_ [;][ z][i][)][ −] _[f]_ [(ˆ]x[∗]S[,][ ˆ]yS[∗] [;][ z][i][) +][ f] [(ˆ]x[∗]S[,][ ˆ]yS[∗] [;][ z][i][)][ −] _[f]_ [(ˆ]x[∗]S[,][ ˆ]yS[∗] [(][i][)] [;][ z][i][)]

_n_
  

_f_ (ˆx[∗]S[(][i][)] _[,][ ˆ]yS[∗]_ [;][ z]i[′][)][ −] _[f]_ [(ˆ]x[∗]S[,][ ˆ]yS[∗] [;][ z]i[′][) +][ f] [(ˆ]x[∗]S[,][ ˆ]yS[∗] [;][ z]i[′][)][ −] _[f]_ [(ˆ]x[∗]S[,][ ˆ]yS[∗] [(][i][)] [;][ z]i[′][)]

_−_ _n[1]_   

_FS(i)_ (ˆx[∗]S[(][i][)] _[,][ ˆ]yS[∗]_ [)][ −] _[F]S[(][i][)]_ [(ˆ]x[∗]S[,][ ˆ]yS[∗] [(][i][)] [) + 2][L] **x[∗]S[(][i][)][ −]** **x[ˆ][∗]S[∥]** [+][ ∥]y[ˆ]S[∗] **yS[∗]** [(][i][)] _[∥][)]_
_≤_ _n_ [(][∥][ˆ] _[−]_ [ˆ]

=FS(i) (ˆx[∗]S[(][i][)] _[,][ ˆ]yS[∗]_ [)][ −] _[F]S[(][i][)]_ [(ˆ]x[∗]S[(][i][)] _[,][ ˆ]yS[∗]_ [(][i][)] [) +][ F][S][(][i][)] [(ˆ]x[∗]S[(][i][)] _[,][ ˆ]yS[∗]_ [(][i][)] [)][ −] _[F][S][(][i][)]_ [(ˆ]x[∗]S[,][ ˆ]yS[∗] [(][i][)] [)]

+ [2][L] **x[∗]S[(][i][)][ −]** **x[ˆ][∗]S[∥]** [+][ ∥]y[ˆ]S[∗] **yS[∗]** [(][i][)] _[∥][)]_

_n_ [(][∥][ˆ] _[−]_ [ˆ]

**x[∗]S[(][i][)][ −]** **x[ˆ][∗]S[∥][2][ −]** _[µ]_ **yS[∗]** **yS[∗]** [(][i][)] _[∥][2][ + 2][L]_ **x[∗]S[(][i][)][ −]** **x[ˆ][∗]S[∥]** [+][ ∥]y[ˆ]S[∗] **yS[∗]** [(][i][)] _[∥][)][,]_

_≤−_ _[µ]2_ _[∥][ˆ]_ 2 _[∥][ˆ]_ _[−]_ [ˆ] _n_ [(][∥][ˆ] _[−]_ [ˆ]

where the first inequality follows from the Lipschitz continuous assumption, and where the second
inequality follows from the facts that the µ-SC-SC property of FS(i) and (ˆx[∗]S[(][i][)] _[,][ ˆ]yS[∗]_ [(][i][)] [)][ is the ESP]
solution of FS(i) .

Similarly, according to the µ-SC-SC property of FS, we have


_FS(ˆx[∗]S[(][i][)]_ _[,][ ˆ]yS[∗]_ [)][ −] _[F][S][(ˆ]x[∗]S[,][ ˆ]yS[∗]_ [(][i][)] [)]
=FS(ˆx[∗]S[(][i][)] _[,][ ˆ]yS[∗]_ [)][ −] _[F][S][(ˆ]x[∗]S[,][ ˆ]yS[∗]_ [) +][ F][S][(ˆ]x[∗]S[,][ ˆ]yS[∗] [)][ −] _[F][S][(ˆ]x[∗]S[,][ ˆ]yS[∗]_ [(][i][)] [)]

**x[∗]S[(][i][)][ −]** **x[ˆ][∗]S[∥][2][ +][ µ]** **yS[∗]** **yS[∗]** [(][i][)] _[∥][2]_ (32)

_≥_ _[µ]2_ _[∥][ˆ]_ 2 _[∥][ˆ]_ _[−]_ [ˆ]

Based on the above results, we have

_µ∥xˆ[∗]S[(][i][)][ −]_ **x[ˆ][∗]S[∥][2][ +][ µ][∥]y[ˆ]S[∗]** _[−]_ **y[ˆ]S[∗]** [(][i][)] _[∥][2]_

**x[∗]S[(][i][)][ −]** **x[ˆ][∗]S[∥]** [+][ ∥]y[ˆ]S[∗] **yS[∗]** [(][i][)] _[∥][)]_

_≤_ [2]n[L] [(][∥][ˆ] _[−]_ [ˆ]

_√2_ **xˆ[∗]S[(][i][)][ −]** **x[ˆ][∗]S[∥][2][ +][ ∥]y[ˆ]S[∗]** **yS[∗]** [(][i][)] _[∥][2][,]_

_≤_ [2]n[L] _∥_ _[−]_ [ˆ]

q

where the last inequality uses the Caucy-Schwarz inequality. Therefore, we have


_∥xˆ[∗]S[(][i][)][ −]_ **x[ˆ][∗]S[∥]** [+][ ∥]y[ˆ]S[∗] _[−]_ **y[ˆ]S[∗]** [(][i][)] _[∥]_

_≤_ 2(∥xˆ[∗]S[(][i][)][ −] **x[ˆ][∗]S[∥][2][ +][ ∥]y[ˆ]S[∗]** _[−]_ **y[ˆ]S[∗]** [(][i][)] _[∥][2][)]_
q

(33)

_≤_ _nµ[4][L]_ _[.]_

Now, plugging this stability bound into Theorem 1, we obtain generalization bounds of the ESP
solution. The proof of Theorem 3 is complete.

**Remark 8. When conditions in Theorem 3 hold, we obtain that (a) If Assumption 1 holds and**
_FS(ˆx[∗]S[,][ ˆ]yS[∗]_ [) =][ O] _n1_, then for any δ > 0, with probability at least 1 _δ, the plain generalization_
_−_

error of (ˆx[∗]S[,][ ˆ]yS[∗] [)][ is of the order]  _[ O]_ logn2 n log(1/δ) . (b) If Assumptions 1 and 2 hold and RS(ˆx[∗]S[) =]

_n1_, then for any δ > 0, with probability at least  1 _δ, the primal generalization error of_
_O_ _−_
 


-----

(ˆx[∗]S[,][ ˆ]yS[∗] [)][ is of the order][ O] logn2 n log(1/δ) . (c) If Assumptions 1 and 2 hold, then for any δ > 0,

with probability at least 1 _δ, the strong PD population risk and the strong PD generalization_
error of (ˆx[∗]S[,][ ˆ]yS[∗] [)][ are all of the order] − _[ O]_ logn2 n log(1/δ) . (d) If Assumptions 1 and 2 hold and

inf **x∈X R(x) = O** _n1_, then for any δ > 0, with probability at least 1 − _δ, the excess primal_

population risk of (ˆx[∗]S[,][ ˆ]yS[∗] [)][ is of the order][ O] logn2 n log(1/δ) .

**Remark 9. (Zhang et al., 2021a) also studies the generalization bound of the ESP solution. They** 
provide O(1/n) order bounds for weak PD population risk and expected strong PD population
risk. Their proofs also show that the expected strong PD population risk is more difficult to analyze than the former. They have to consider the fact that different ˆx[∗]S [corresponds to different]
**y, as discussed in Remark 5. Moreover, the expectation operator in expected strong PD popula-**
tion risk also relaxes the difficulty of proof. Specifically, define S[(][i][)] be a dataset by replacing the
_i-th sample in S with another i.i.d. sample zi[′]_ [and][ y][∗][(][x][) = arg max][y][∈Y][ F] [(][x][,][ y][)][, there holds]
the following important propertyn1 _ni=1_ [E] _f_ (ˆx[∗]S[(][i][)] _[,][ y][∗][(ˆ]x[∗]S[(][i][)]_ [);][ z][i][)] Ebecausesupy∈Y (ˆx F[∗]S[,][ y](ˆx[∗][∗]S[(ˆ][,]x[ y][∗]S[)][))] [ and]= [ (ˆ]n1x[∗]SP[(][i][)]ni=1[,][ y][∗][E][(ˆ]xF[∗]S[(](ˆ[i][)]x[))][∗]S[ are identically dis-][(][i][)] _[,][ y][∗][(ˆ]x[∗]S[(][i][)]_ [))] =
tributed and the independence between zi and S[(][i][)]. On the contrary, when there is no expectation
P  
operator, we do not have this property and the proof is much more challenging.

C GRADIENT DESCENT ASCENT

We need some notations to state results on GDA. Specifically, assume the initial point satisfies
**x1 = 0 and y1 = 0. Let {ηt} be a sequence of positive step sizes. At the t-th iteration, GDA**
updates

**xt+1 = xt −** _ηt∇xFS(xt, yt),_
**yt+1 = yt + ηt∇yFS(xt, yt).** (34)

We denote the average of iterates by

_T_ _T_
**x¯T =** _t=1_ **[x][t]** and **y¯T =** _t=1_ **[y][t]** _._ (35)

_T_ _T_

P P

Here, we first provide an important lemma to connect the argument stability with the strong PD
empirical risk, which will also be used in the remaining applications.

**Lemma 7. For any i** [n], define S[(][i][)] = _z1, ..., zi_ 1, zi[′][, z][i][+1][, ..., z][n][}][. Let][ (][x][t][,][ y][t][)][ be the output]
_∈_ _{_ _−_
_produced by FS on dataset S in running a minimax learning algorithm. Let (x[i]t[,][ y]t[i][)][ be the corre-]_
_sponding output produced by FS(i) on dataset S[(][i][)], where FS(i) is empirical risk on dataset S[(][i][)]._
_Suppose Assumption 1 holds. Assume for all z, the function (x, y) 7→_ _f_ (x, y; z) is µ-SC-SC. For
_any S[(][i][)]_ _and S, we have_

1

**x[i]t** _t_ _S[(][x][t][,][ y][t][)][.]_
_∥_ _[−]_ **[x][t][∥]** [+][ ∥][y][i] _[−]_ **[y][t][∥≤]** _nµ[4][L]_ [+ 4]r _µ_ q△[s]

_Proof. Define (ˆx[∗]S[(][i][)]_ _[,][ ˆ]yS[∗]_ [(][i][)] [)][ be the ESP solution on dataset][ S][(][i][)][ and][ (ˆ]x[∗]S[,][ ˆ]yS[∗] [)][ be the ESP solution]
on dataset S. To prove the stability bound, we consider


**x[i]t** _t_
_∥_ _[−]_ **[x][t][∥]** [+][ ∥][y][i] _[−]_ **[y][t][∥]**

=∥x[i]t _[−]_ **x[ˆ][∗]S[(][i][)][ + ˆ]x[∗]S[(][i][)][ −]** **x[ˆ][∗]S** [+ ˆ]x[∗]S _[−]_ **[x][t][∥]** [+][ ∥][y]t[i] _[−]_ **y[ˆ]S[∗]** [(][i][)][ + ˆ]yS[∗] [(][i][)][ −] **y[ˆ]S[∗]** [+ ˆ]yS[∗] _[−]_ **[y][t][∥]**

_≤∥x[i]t_ _[−]_ **x[ˆ][∗]S[(][i][)]** _[∥]_ [+][ ∥]x[ˆ][∗]S[(][i][)][ −] **x[ˆ][∗]S[∥]** [+][ ∥]x[ˆ][∗]S _[−]_ **[x][t][∥]** [+][ ∥][y]t[i] _[−]_ **y[ˆ]S[∗]** [(][i][)] _[∥]_ [+][ ∥]y[ˆ]S[∗] [(][i][)][ −] **y[ˆ]S[∗]** _[∥]_ [+][ ∥]y[ˆ]S[∗] _[−]_ **[y][t][∥]**

_t_ **x[∗]S[(][i][)]** _[∥]_ [+][ ∥]x[ˆ][∗]S _t_ **yS[∗]** [(][i][)] _[∥]_ [+][ ∥]y[ˆ]S[∗]

_≤_ _nµ[4][L]_ [+][ ∥][x][i] _[−]_ [ˆ] _[−]_ **[x][t][∥]** [+][ ∥][y][i] _[−]_ [ˆ] _[−]_ **[y][t][∥]**

_√2_ **x[i]t** **x[∗]S[(][i][)]** _[∥][2][ +][ ∥][y]t[i]_ **yS[∗]** [(][i][)] _[∥][2][ +]_ _√2_ **yˆS[∗]** **x[∗]S**

_≤_ _nµ[4][L]_ [+] _∥_ _[−]_ [ˆ] _[−]_ [ˆ] _∥_ _[−]_ **[y][t][∥][2][ +][ ∥][ˆ]** _[−]_ **[x][t][∥][2]**

q q

4 4

_FS(i)_ (x[i]t[,][ ˆ]yS[∗] [(][i][)] [)][ −] _[F][S][(][i][)]_ [(ˆ]x[∗]S[(][i][)] _[,][ y]t[i][) +]_ _FS(xt, ˆyS[∗]_ [)][ −] _[F][S][(ˆ]x[∗]S[,][ y][t][)][,]_

_≤_ _nµ[4][L]_ [+] _µ_ _µ_

r q r q


-----

where the second inequality uses the result in (33), the third inequality uses the Caucy-Schwarz
inequality, and the last inequality uses the strong convexity and strong concavity of FS(i) and
_FS and the optimality condition (please refer to (32)). As will see in the rest paper, we bound_
_FS(i)_ (x[i]t[,][ ˆ]yS[∗] [(][i][)] [)][ −] _[F][S][(][i][)]_ [(ˆ]x[∗]S[(][i][)] _[,][ ˆ]yS[∗]_ [(][i][)] [)][ and][ F][S][(][x][t][,][ ˆ]yS[∗] [)][ −] _[F][S][(ˆ]x[∗]S[,][ y][t][)][ with the same upper bound]_
since they are all strong PD empirical risk. Thus, for brevity, we derive the following inequality


**x[i]t** _t_
_∥_ _[−]_ **[x][t][∥]** [+][ ∥][y][i] _[−]_ **[y][t][∥]**

1

_FS(xt, ˆyS[∗]_ [)][ −] _[F][S][(ˆ]x[∗]S[,][ y][t][)]_

_≤_ _nµ[4][L]_ [+ 4] _µ_

r q

1

_S[(][x][t][,][ y][t][)][.]_

_≤_ _nµ[4][L]_ [+ 4] _µ_ _△[s]_

r q


The proof is complete.


**Remark 10. Lemma 7 provides the connection between the stability bound and the strong PD em-**
pirical risk. The subscript t here represents not only the output of an iterative optimization algorithm,
but any output of the empirical risk of any minimax learning algorithm.
**Remark 11. In studying the stability bound of gradient-based optimization algorithms, a popular**
approach is to use the property of smoothness to establish the nonexpansiveness of gradient mapping,
proposed in the seminal work (Hardt et al., 2016). (Farnia & Ozdaglar, 2021; Lei et al., 2021) extend
this approach to the minimax problems and use it to analyze the stability bound of SGDA, GDA,
PPM, etc. However, their stability bounds are often derived in expectation. In (Lei et al., 2021),
the authors also use the Chernoff bounds of Bernoulli variables to establish high probability stability
bounds when they are to derive high probability generalization bounds. Unfortunately, these stability
bounds are often of slow order O(1/[√]n). To derive sharper stability bounds, we established Lemma
7.

The following lemma shows the strong PD empirical risk of GDA.
**Lemma 8. Suppose Assumption 1 holds and FS(** _,_ ) be µ-SC-SC with µ > 0. Let **xt, yt** _be the_
1 _·_ _·_ _{_ _}_
_sequence produced by (34) with ηt =_ _µ(t+t0)_ _[. Assume][ t][0][ ≥]_ [0][. Suppose][ sup][x][∈X][ ∥][x][∥≤] _[R][X][ and]_

supy **y** _RY . Then for (¯xT, ¯yT ) in (35) we have_
_∈Y ∥_ _∥≤_

_X_ [+][ R]Y[2] [)]
sup _FS(¯xT, y)_ inf **yT )** + _[L][2][ log(][eT]_ [)] _._
**y∈Y** _−_ **x∈X** _[F][S][(][x][,][ ¯]_ _≤_ _[µt][0][(][R][2]T_ _µT_

_If t0 = 0, then_

sup _FS(¯xT, y)_ inf **yT )** _._
**y∈Y** _−_ **x∈X** _[F][S][(][x][,][ ¯]_ _≤_ _[L][2][ log(]µT_ _[eT]_ [)]

_Proof. Firstly, we have_

**xt+1** **x** = **xt** _ηt_ **xFS(xt, yt)** **x**
_∥_ _−_ _∥[2]_ _∥_ _−_ _∇_ _−_ _∥[2]_

= **xt** **x** + ηt[2]
_∥_ _−_ _∥[2]_ _[∥∇][x][F][S][(][x][t][,][ y][t][)][∥][2][ + 2][η][t][⟨][x][ −]_ **[x][t][,][ ∇][x][F][S][(][x][t][,][ y][t][)][⟩]**

_≤∥xt −_ **x∥[2]** + ηt[2][L][2][ + 2][η][t][⟨][x][ −] **[x][t][,][ ∇][x][F][S][(][x][t][,][ y][t][)][⟩][,]**

where the first inequality holds because of Assumption 1. By the strong convexity of FS( _, yt), we_

_·_
have

2ηt(FS(xt, yt) − _FS(x, yt)) ≤_ (1 − _ηtµ)∥xt −_ **x∥[2]** _−∥xt+1 −_ **x∥[2]** + ηt[2][L][2][.]

Since ηt = _µ(t+1_ _t0)_ [, we further get]


2
_L[2]._



**xt** **x** **xt+1** **x** +
_∥_ _−_ _∥[2]_ _−∥_ _−_ _∥[2]_


1
_µ(t + t0)_ [(][F][S][(][x][t][,][ y][t][)][ −] _[F][S][(][x][,][ y][t][))][ ≤]_ _−_


Multiplying both sides by t + t0, we have


(t + t0)


_µ(t + t0)_


2 _L[2]_

_µ_ [(][F][S][(][x][t][,][ y][t][)][ −] _[F][S][(][x][,][ y][t][))][ ≤]_ [(][t][ +][ t][0][ −] [1)][∥][x][t][ −] **[x][∥][2][ −]** [(][t][ +][ t][0][)][∥][x][t][+1][ −] **[x][∥][2][ +]** _µ[2](t + t0)_ _[.]_


-----

Since x1 = 0 and _t=1_ _[t][−][1][ ≤]_ [log(][eT] [)][, by taking a summation of the above inequality from][ t][ = 1]
to T, we obtain

[P][T] _T_

(FS(xt, yt) _FS(x, yt))_ _X_ [+][ L][2][ log(][eT] [)] _._
_−_ _≤_ _[µ]2_ _[t][0][R][2]_ 2µ
_t=1_

X

From the concavity of FS(x, ) we get
_·_


_T_

(FS(xt, yt) _FS(x, ¯yT ))_ _X_ [+][ L][2][ log(][eT] [)]
_−_ _≤_ _[µ]2_ _[t][0][R][2]_ 2µ
_t=1_

X

Since this inequality holds for any x, we get


_T_

(FS(xt, yt) inf **yT ))** _X_ [+][ L][2][ log(][eT] [)]
_−_ **x** _[F][S][(][x][,][ ¯]_ _≤_ _[µ]2_ _[t][0][R][2]_ 2µ
_t=1_ _∈X_

X


This implies that


_T_

1 (FS(xt, yt) inf **yT ))** _X_ + _[L][2][ log(][eT]_ [)]

_T_ _−_ **x** _[F][S][(][x][,][ ¯]_ _≤_ _[µt]2[0]T[R][2]_ 2µT

_t=1_ _∈X_

X

In a similar way, we have the following inequality


_T_

(FS(xt, yt)) _Y_ + _[L][2][ log(][eT]_ [)]
_≤_ _[µt]2[0]T[R][2]_ 2µT
_t=1_

X


sup _FS(¯xT, y)_
**y∈Y** _−_ _T[1]_


Combined the above two inequalities together we get

_X_ [+][ R]Y[2] [)]
sup _FS(¯xT, y)_ inf **yT )**
**y∈Y** _−_ **x∈X** _[F][S][(][x][,][ ¯]_ _≤_ _[µt][0][(][R][2]T_


+ _[L][2][ log(][eT]_ [)]

_µT_


For optimization algorithm GDA, substituting the strong PD empirical risk bound of (¯xT, ¯yT ) into
Lemma 7, we get the following stability bound,


1

_∥x¯[i]T_ _[−]_ **x[¯]T ∥** + ∥y¯T[i] _[−]_ **y[¯]T ∥≤** _nµ[4][L]_ [+ 4]r _µ_

1

_≤_ _nµ[4][L]_ [+ 4] _µ_

r


_△S[s]_ [(¯]xT, ¯yT )

_µt0(RX[2]_ [+][ R]Y[2] [)]


+ _[L][2][ log(][eT]_ [)] _._ (36)

_µT_


Furthermore, for any x ∈X, y ∈Y and z ∈Z,

_f_ (x, y; z) − _f_ (0, 0; z) ≤ _L∥x −_ 0∥ + L∥y − 0∥≤ _L(RY + RY ),_

which implies that

_f_ (x, y; z) ≤ _zsup∈Z_ _f_ (0, 0; z) + L(RY + RY ). (37)

Till here, plugging (37), the stability bound in (36) and the strong PD empirical risk bound in Lemma
8 into Theorem 1, we obtain generalization bounds of GDA.

We now write the main theorem of GDA.

**Theorem 4.RX and sup Assume for ally** **y** _RY z . Let, the functionxt, yt_ ( be produced by (34) withx, y) 7→ _f_ (x, y; z) is µ-SC-SC. Suppose ηt = _µ(t+1_ _t0)_ _[. Assume] supx∈X[ t] ∥[0][ ≥]x∥≤[0][.]_
_∈Y ∥_ _∥≤_ _{_ _}_

_Denote Ax(S) = ¯xT and Ay(S) = ¯yT for (¯xT, ¯yT ) in (35). Let M = supz_ _f_ (0, 0; z)+L(RX +
_∈Z_
_RY ). Fixed any η > 0. There exists an absolute positive constant C._


-----

_(a) If Assumption 1 holds, then for any δ > 0, with probability at least 1 −_ _δ, we have_

_F_ (¯xT, ¯yT ) ≤ (1 + η)FS(¯xT, ¯yT )

_M_ 4L 1 _µt0(RX[2]_ [+][ R]Y[2] [)]

+ C [1 +][ η] + _[L][2][ log(][eT]_ [)] log2 n log(1/δ) _._

_η_ _n_ [log(1][/δ][) +] _nµ_ [+ 4] _µ_ s _T_ _µT_

r

   

_(b) If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ, we have_

_R(¯xT )_ (1 + η)RS(¯xT ) + C [1 +][ η]
_≤_ _η_

_M log 1δ_ 4L 1 _µt0(RX[2]_ [+][ R]Y[2] [)]

+ _[β][ +][ µ]_ _L_ + _[L][2][ log(][eT]_ [)] log2 n log [1] _._

_×_ _n_ _µ_ _nµ_ [+ 4] _µ_ s _T_ _µT_ _δ_

r

   

_(c) If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ, we have_

_µt0(RX2_ [+][ R]Y[2] [)]
(¯xT, ¯yT ) (1 + η) + _[L][2][ log(][eT]_ [)] + C(1 + η)
_△[s]_ _≤_ _T_ _µT_
 

_L2(1 + η)_ 4L 1 _µt0(RX[2]_ [+][ R]Y[2] [)] 1

+ _[M]_ + _[L][2][ log(][eT]_ [)] _L log2 n_ log _._

_×_ _nµη_ _n_ [+] _[β][ +]µ[ µ]_ _nµ_ [+4] _µ_ s _T_ _µT_ _δ_

r

     

_(d) If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ, we have_

_µt0(RX2_ [+][ R]Y[2] [)]
(¯xT, ¯yT ) _S[(¯]xT, ¯yT )_ _η_ + _[L][2][ log(][eT]_ [)] + C(1 + η)
_△[s]_ _−△[s]_ _≤_ _T_ _µT_
 

_L2(1 + η)_ 4L 1 _µt0(RX[2]_ [+][ R]Y[2] [)] 1

+ _[M]_ + _[L][2][ log(][eT]_ [)] _L log2 n_ log _._

_×_ _nµη_ _n_ [+] _[β][ +]µ[ µ]_ _nµ_ [+4] _µ_ s _T_ _µT_ _δ_

r

     

_(e) If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ, we have_

_µt0(RX2_ [+][ R]Y[2] [)]

_R(¯xT )_ (1 + η) inf + _[L][2][ log(][eT]_ [)]
_≤_ **x** _[R][(][x][) +][ C][ 2 +]η_ _[ η]_ _T_ _µT_
_∈X_

 

_M_ _β_ 4L 1 _µt0(RX[2]_ [+][ R]Y[2] [)]

+C [2 +][ η] _L_ + _[L][2][ log(][eT]_ [)] log2 n log [1] _._

_η_ _n_ [log 1]δ [+] _µ_ [+1] _nµ_ [+4] _µ_ s _T_ _µT_ _δ_

r

     

**Remark 12. When conditions in Theorem 4 hold, we obtain that (a) If Assumption 1 holds and**
_FS(¯xT, ¯yT ) =_ _n1_, then for any δ > 0, with probability at least 1 _δ, the plain generalization_
_O_ _−_
  1 log T

error of (¯xT, ¯yT ) of GDA is of the order O _n_ [+] _T_ log2 n log(1/δ) . (b) If Assumptions

1 and 2 hold and RS(¯xT ) = _n1_, then for any _δ >q 0, with probability at least_  1 _δ, the primal_
_O_ _−_
  1 log T

generalization error of (¯xT, ¯yT ) of GDA is of the order O _n_ [+] _T_ log2 n log(1/δ) . (c)

If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least q  1 _δ, the strong PD_
_−_
population risk and the strong PD generalization error of (¯xT, ¯yT ) of GDA are all of the order

_O_ _n1_ [+] logT T log2 n log(1/δ) . (d) If Assumptions 1 and 2 hold and inf **x∈X R(x) = O** _n1_,

then for any q δ > 0, with probability at least 1 − _δ, the excess primal population risk of (¯xT, ¯yT )_

of GDA is of the order O _n1_ [+] logT T log2 n log(1/δ) . For the above bounds, we can take

_T =_ (n[2]) gradient evaluations to get bound of the order q   log[3]n[/][2] _n_ log(1/δ) .
_O_ _O_
 

D STOCHASTIC GRADIENT DESCENT ASCENT

We need some notations to state results on SGDA. Specifically, assume the initial point satisfies
**x1 = 0 and y1 = 0. Let {ηt} be a sequence of positive step sizes. At the t-th iteration, SGDA**


-----

first randomly select an index it form the uniform distribution over [n] := {1, ..., n} and then do the
update

**xt+1 = xt −** _ηt∇xf_ (xt, yt, zit ),
**yt+1 = yt + ηt∇yf** (xt, yt, zit ). (38)

We denote the average of iterates by


_T_
**x¯T =** _t=1_ **[x][t]**

_T_

P


_T_
and **y¯T =** _t=1_ **[y][t]**

_T_

P


(39)


Let’s first introduce two concentration inequalities for martingales, which are required in deriving
the strong PD empirical risk bound of SGDA.

**Lemma 9. (Boucheron et al., 2013) Let z1, ..., zn be a sequence of random variables such that**
_zk may depend the previous variables z1, ..., zk_ 1 for all k = 1, ..., n. Consider a sequence of
_−_
_functionalsprobability at least ξk(z1, ..., z 1 −kδ), k = 1, ..., n. Assume |ξk −_ Ezk [ξk]| ≤ _bk for each k. Let δ ∈_ (0, 1). With

_n_ _n_ _n_ [1]

_ξk_ Ezk [ξk] 2 _b[2]k_ [log 1] 2 .
_k=1_ _−_ _k=1_ _≤_ _k=1_ _δ_

X X  X 

**Lemma 10. (Tarres & Yao, 2014) Let {ξk}k∈N be a martingale difference sequence in R[d]. Suppose**
_that almost surely ∥ξk∥≤_ _D and_ _k=1_ [E][[][∥][ξ][k][∥][2][|][ξ][1][, ..., ξ][k][−][1][]][ ≤] _[σ]t[2][. Then, for any][ 0][ < δ <][ 1][, the]_
_following inequality holds with probability at least 1 −_ _δ_

[P][t] _j_

max _ξk_ 2 _D_ log [2]
1 _j_ _t_ _≤_ 3 [+][ σ][t] _δ [.]_
_≤_ _≤_ _k=1_

X  

The following lemma shows the strong PD empirical risk bound of SGDA.

**Lemma 11. Suppose Assumption 1 holds and FS(** _,_ ) be µ-SC-SC with µ > 0. Let **xt, yt** _be the_
1 _·_ _·_ _{_ _}_
_sequence produced by (38) with ηt =_ _µ(t+t0)_ _[. Assume][ t][0][ ≥]_ [0][. Suppose][ sup][x][∈X][ ∥][x][∥≤] _[R][X][ and]_

supy **y** _RY . Let δ > 0. Then for (¯xT, ¯yT ) in (39), with probability at least 1_ _δ we have_
_∈Y ∥_ _∥≤_ _−_

_X_ [+][ R]Y[2] [)]
sup _FS(¯xT, y)_ inf **yT )** + _[L][2][ log(][eT]_ [)]
**y∈Y** _−_ **x∈X** _[F][S][(][x][,][ ¯]_ _≤_ [2][µt][0][(][R][2]T _µT_

1

+ [2(][R][X][ +][ R][Y][ )] 2L _√T_ log [6] 2 _._

_T_ 3 [+ 2][L] _δ_ [+ 2][L][(][R][X][ +][ R][Y][ )(2]T _[T][ log(6][/δ][))]_

 

_If t0 = 0, then with probability at least 1 −_ _δ we have_

sup _FS(¯xT, y)_ inf **yT )** + [2(][R][X][ +][ R][Y][ )] 2L _√T_ log [6]
**y∈Y** _−_ **x∈X** _[F][S][(][x][,][ ¯]_ _≤_ _[L][2][ log(]µT_ _[eT]_ [)] _T_ 3 [+ 2][L] _δ_

1 

+ [2][L][(][R][X][ +][ R][Y][ )(2][T][ log(6][/δ][))] 2 _._

_T_

_Proof. This proof follows from (Lei et al., 2021). Firstly, we have_


**xt+1** **x** = **xt** _ηt_ **xf** (xt, yt; zit ) **x**
_∥_ _−_ _∥[2]_ _∥_ _−_ _∇_ _−_ _∥[2]_

= **xt** **x** + ηt[2] _t_ [)][∥][2][ + 2][η][t][⟨][x][ −] **[x][t][,][ ∇][x][f]** [(][x][t][,][ y][t][;][ z][i]t [)][⟩]
_∥_ _−_ _∥[2]_ _[∥∇][x][f]_ [(][x][t][,][ y][t][;][ z][i]

**xt** **x** + ηt[2][L][2][ + 2][η][t][⟨][x][ −] **[x][t][,][ ∇][x][f]** [(][x][t][,][ y][t][;][ z][i]t [)][ −∇][x][F][S][(][x][t][,][ y][t][)][⟩] [+ 2][η][t][⟨][x][ −] **[x][t][,][ ∇][x][F][S][(][x][t][,][ y][t][)][⟩][,]**
_≤∥_ _−_ _∥[2]_

where the first inequality holds because of Assumption 1. By the strong convexity of FS( _, yt), we_

_·_
have

2ηt(FS(xt, yt) − _FS(x, yt)) ≤_ (1 − _ηtµ)∥xt −_ **x∥[2]** _−∥xt+1 −_ **x∥[2]** + ηt[2][L][2]

+2ηt **x** **xt,** **xf** (xt, yt; zit ) **xFS(xt, yt)** _._
_⟨_ _−_ _∇_ _−∇_ _⟩_


-----

Since ηt =


1

_µ(t+t0)_ [, we further get]


2 1

1 **xt** **x** **xt+1** **x**
_µ(t + t0)_ [(][F][S][(][x][t][,][ y][t][)][ −] _[F][S][(][x][,][ y][t][))][ ≤]_ _−_ (t + t0) _∥_ _−_ _∥[2]_ _−∥_ _−_ _∥[2]_

1 2 2  
+ _L[2]_ +

_µ(t + t0)_ _µ(t + t0)_ _[⟨][x][ −]_ **[x][t][,][ ∇][x][f]** [(][x][t][,][ y][t][;][ z][i][t] [)][ −∇][x][F][S][(][x][t][,][ y][t][)][⟩][.]

 

Multiplying both sides by t + t0, we have

2 _L[2]_

_µ_ [(][F][S][(][x][t][,][ y][t][)][ −] _[F][S][(][x][,][ y][t][))][ ≤]_ [(][t][ +][ t][0][ −] [1)][∥][x][t][ −] **[x][∥][2][ −]** [(][t][ +][ t][0][)][∥][x][t][+1][ −] **[x][∥][2][ +]** _µ[2](t + t0)_


+ [2]

_µ_ _[⟨][x][ −]_ **[x][t][,][ ∇][x][f]** [(][x][t][,][ y][t][;][ z][i][t] [)][ −∇][x][F][S][(][x][t][,][ y][t][)][⟩][.]

Since x1 = 0 and _t=1_ _[t][−][1][ ≤]_ [log(][eT] [)][, by taking a summation of the above inequality from][ t][ = 1]
to T, we obtain

_T_

[P][T]

(FS(xt, yt) _FS(x, yt))_ _X_ [+][ L][2][ log(][eT] [)]
_−_ _≤_ _[µ]2_ _[t][0][R][2]_ 2µ
_t=1_

X

_T_ _T_

+ **x,** **xf** (xt, yt; zit ) **xFS(xt, yt)** + **xt,** **xFS(xt, yt)** **xf** (xt, yt; zit ) _._

_⟨_ _∇_ _−∇_ _⟩_ _⟨_ _∇_ _−∇_ _⟩_
_t=1_ _t=1_

X X

From the concavity of FS(x, ) we get
_·_

_T_

(FS(xt, yt) _FS(x, ¯yT ))_ _X_ [+][ L][2][ log(][eT] [)]
_−_ _≤_ _[µ]2_ _[t][0][R][2]_ 2µ
_t=1_

X


+ **x,** **xf** (xt, yt; zit ) **xFS(xt, yt)** +

_⟨_ _∇_ _−∇_ _⟩_
_t=1_

X

Since this inequality holds for any x, we get


**xt,** **xFS(xt, yt)** **xf** (xt, yt; zit ) _._
_⟨_ _∇_ _−∇_ _⟩_
_t=1_

X


_T_

(FS(xt, yt) inf **yT ))** _X_ [+][ L][2][ log(][eT] [)]
_−_ **x** _[F][S][(][x][,][ ¯]_ _≤_ _[µ]2_ _[t][0][R][2]_ 2µ
_t=1_ _∈X_

X


+ sup **x,** **xf** (xt, yt; zit ) **xFS(xt, yt)** + **xt,** **xFS(xt, yt)** **xf** (xt, yt; zit ) _._

**x** _⟨_ _∇_ _−∇_ _⟩_ _⟨_ _∇_ _−∇_ _⟩_
_t=1_ _∈X_ _t=1_

X X

By Schwarz’s inequality, we have


_T_

(FS(xt, yt) inf **yT ))** _X_ [+][ L][2][ log(][eT] [)]
_−_ **x** _[F][S][(][x][,][ ¯]_ _≤_ _[µ]2_ _[t][0][R][2]_ 2µ
_t=1_ _∈X_

X


+ RX


**xf** (xt, yt; zit ) **xFS(xt, yt)**
_∇_ _−∇_
_t=1_

X


**xt,** **xFS(xt, yt)** **xf** (xt, yt; zit ) _._
_⟨_ _∇_ _−∇_ _⟩_
_t=1_

X


Denote ξt = _⟨xt, ∇xFS(xt, yt) −∇xf_ (xt, yt; zit )⟩. Since Eit [⟨xt, ∇xFS(xt, yt) −
**xf** (xt, yt; zit ) ] = 0, so _ξt_ _t = 1, ..., T_ is a martingale difference sequence. By Schwarz’s
_∇_ _⟩_ _{_ _|_ _}_
inequality and Assumption 1, we know that **xt,** **xFS(xt, yt)** **xf** (xt, yt; zit ) 2LRX .
_|⟨_ _∇_ _−∇_ _⟩| ≤_
Then, according to Lemma 9, we have the following inequality with probability at least 1 − _δ/6_

_T_

1
**xt,** **xFS(xt, yt)** **xf** (xt, yt; zit ) 2LRX (2T log(6/δ)) 2 .
_⟨_ _∇_ _−∇_ _⟩≤_
_t=1_

X

Define ξt[′] [=][ ∇][x][f] [(][x][t][,][ y][t][;][ z][i]t [)][ −∇][x][F][S][(][x][t][,][ y][t][)][. Then we get][ ∥][ξ]t[′][∥≤] [2][L][ and]

_T_

E[ _ξt[′][∥][2][|][ξ]1[′]_ _[, ..., ξ]t[′]_ 1[]][ ≤] [4][TL][2][.]
_∥_ _−_
_t=1_

X


-----

Applying Lemma 10 to the martingale difference sequence {ξt[′][}][, we have the following inequality]
with probability at least 1 − _δ/3_

_T_

_ξt[′]_ 2 2L _√T_ log [6]
_≤_ 3 [+ 2][L] _δ [.]_
_t=1_

X  

That is, with probability at least 1 − _δ/3_

_T_

**xf** (xt, yt; zit ) **xFS(xt, yt)** 2 2L _√T_ log [6]
_∇_ _−∇_ _≤_ 3 [+ 2][L] _δ [.]_
_t=1_

X  

Combined with the above results, we finally have the following inequality with probability at least
1 − _δ/2_

_T_

1 (FS(xt, yt) inf **yT ))** _X_ + _[L][2][ log(][eT]_ [)]

_T_ _−_ **x** _[F][S][(][x][,][ ¯]_ _≤_ _[µt]2[0]T[R][2]_ 2µT

_t=1_ _∈X_

X 1

+ [2][R][X] 2L _√T_ log [6] 2 _._

_T_ 3 [+ 2][L] _δ_ [+ 2][LR][X] [(2][T]T[ log(6][/δ][))]

 

In a similar way, we have the following inequality with probability at least 1 − _δ/2_

_T_

sup _FS(¯xT, y)_ (FS(xt, yt)) _Y_ + _[L][2][ log(][eT]_ [)]
**y** _−_ _T[1]_ _≤_ _[µt]2[0]T[R][2]_ 2µT
_∈Y_ _t=1_

X 1

+ [2][R][Y] 2L _√T_ log [6] 2 _._

_T_ 3 [+ 2][L] _δ_ [+ 2][LR][Y][ (2][T][ log(6]T _[/δ][))]_

 

Combined the above two inequalities together we get the result with probability at least 1 − _δ_

_X_ [+][ R]Y[2] [)]
sup _FS(¯xT, y)_ inf **yT )** + _[L][2][ log(][eT]_ [)]
**y∈Y** _−_ **x∈X** _[F][S][(][x][,][ ¯]_ _≤_ _[µt][0][(][R][2]T_ _µT_

1

+ [2(][R][X][ +][ R][Y][ )] 2L _√T_ log [6] 2 _._

_T_ 3 [+ 2][L] _δ_ [+ 2][L][(][R][X][ +][ R][Y][ )(2]T _[T][ log(6][/δ][))]_

 


Denote E = _[µt][0][(][R]X[2]T_ [+][R]Y[2] [)] + _[L][2][ log(]µT_ _[eT][ )]_ + [2(][R][X] [+][R][Y][ )(][ 2]3[L]T[+2][L]√T ) log _δ[6]_ + [2][L][(][R][X] [+][R][Y][ )(2]T _[T][ log(6][/δ][))]_ 21 .

Now, plugging Lemma 11 to Lemma 7, we know that the argument stability bound of SGDA is

1 1

**x¯[i]T** **xT** + **y¯T[i]** **yT** _S[(¯]xT, ¯yT )_ 4 _√E + [4][L]_ (40)
_∥_ _[−]_ [¯] _∥_ _∥_ _[−]_ [¯] _∥≤_ _nµ[4][L]_ [+ 4]r _µ_ q△[s] _≤_ r _µ_ _nµ_ _[.]_

Furthermore, for any x ∈X, y ∈Y and z ∈Z,

_f_ (x, y; z) ≤ _f_ (0, 0; z) + L∥x − 0∥ + L∥y − 0∥≤ _zsup∈Z_ _f_ (0, 0; z) + L(RX + RY ), (41)

Note that since SGDA is a randomized algorithm, thus we need the following variant of Theorem 1.
**Theorem 5. Let A be a randomized learning algorithm and ϵ > 0. Suppose |f** (x, y; z)| ≤ _M for_
_some M > 0 and x ∈X_ _, y ∈Y, z ∈Z. Fixed any η > 0. There exists an absolute positive_
_constant C._

_(a.) If A has ϵ-uniform stability with probability at least 1 −_ _δ[′]_ _for some δ[′]_ _∈_ (0, 1) over the
_randomness of A, i.e.,_

_PrA_ sup _ϵ,_
_z_ [[][f] [(][A][x][(][S][)][, A][y][(][S][);][ z][)][ −] _[f]_ [(][A][x][(][S][′][)][, A][y][(][S][′][);][ z][)]] _≤_
 

_And if the randomness of A is independent of the training set S. Then for any δ > 0, with probability_
_at least 1 −_ _δ[′]_ _−_ _δ,_

_M_

_F_ (Ax(S), Ay(S)) (1 + η)FS(Ax(S), Ay(S)) + C [1 +][ η] _._
_≤_ _η_ _n_ [log(1][/δ][) +][ ϵ][ log][2][ n][ log 1]δ

 


-----

_(b.) Assume that for all x, the function y 7→_ _F_ (x, y) is µ-strongly-concave. Suppose Assumptions
_1 and 2 hold. If the algorithm A is ϵ-argument stable with probability at least 1 −_ _δ[′]_ _for some_
_δ[′]_ _∈_ (0, 1) over the randomness of A, i.e.,

_PrA_ _Ax(S)_ _Ax(S[′])_ + _Ay(S)_ _Ay(S[′])_ _ϵ._
_∥_ _−_ _∥_ _∥_ _−_ _∥_ _≤_
 

_And if the randomness of A is independent of the training set S. Then for any δ > 0, with probability_
_at least 1 −_ _δ[′]_ _−_ _δ,_

_M_ _β_

_R(Ax(S))_ (1 + η)RS(Ax(S)) + C [1 +][ η] _Lϵ log2 n log [1]_ _._
_≤_ _η_ _n_ [log 1]δ [+] _µ_ [+ 1] _δ_

   

_(c.) Assume that for all x and y, the function F_ (x, y) is µ-SC-SC. Suppose Assumptions 1 and 2
_hold. If the algorithm A is ϵ-argument stable with probability at least 1 −_ _δ[′]_ _for some δ[′]_ _∈_ (0, 1)
_over the randomness of A, i.e.,_

_PrA_ _Ax(S)_ _Ax(S[′])_ + _Ay(S)_ _Ay(S[′])_ _ϵ._
_∥_ _−_ _∥_ _∥_ _−_ _∥_ _≤_
 

_And if the randomness of A is independent of the training set S. Then for any δ > 0, with probability_
_at least 1 −_ _δ[′]_ _−_ _δ,_

_△[s]_ (Ax(S), Ay(S)) ≤△S[s] [(][A][x][(][S][)][, A][y][(][S][)) +]L2(1 +[ η][E][S] η[△]) _S[s]_ [(][A][x][(][S][)][, A][y][(][S][))] 1

+ C(1 + η) + _[M]_ 1 + _[β]_ _ϵL log2 n_ log _._

_nµη_ _n_ [+] _µ_ _δ_

     

_(d.) Assume that for all x and y, the function F_ (x, y) is µ-SC-SC. Suppose Assumptions 1 and 2
_hold. If the algorithm A is ϵ-argument stable with probability at least 1 −_ _δ[′]_ _for some δ[′]_ _∈_ (0, 1)
_over the randomness of A, i.e.,_

_PrA_ _Ax(S)_ _Ax(S[′])_ + _Ay(S)_ _Ay(S[′])_ _ϵ._
_∥_ _−_ _∥_ _∥_ _−_ _∥_ _≤_
 

_And if the randomness of A is independent of the training set S. Then for any δ > 0, with probability_
_at least 1 −_ _δ[′]_ _−_ _δ,_

_△[s]_ (Ax(S), Ay(S)) −△S[s] [(][A][x][(][S][)][, A][y][(][S][))]L[ ≤]2(1 +[η][E][S] η[△]) _S[s]_ [(][A][x][(][S][)][, A][y][(][S][))] 1

+ C(1 + η) + _[M]_ 1 + _[β]_ _ϵL log2 n_ log _._

_nµη_ _n_ [+] _µ_ _δ_

     

_(e.) Assume that for all x, the function y 7→_ _F_ (x, y) is µ-strongly-concave. Suppose Assumptions
_1 and 2 hold. If the algorithm A is ϵ-argument stable with probability at least 1 −_ _δ[′]_ _for some_
_δ[′]_ _∈_ (0, 1) over the randomness of A, i.e.,

_PrA_ _Ax(S)_ _Ax(S[′])_ + _Ay(S)_ _Ay(S[′])_ _ϵ._
_∥_ _−_ _∥_ _∥_ _−_ _∥_ _≤_
 

_And if the randomness of A is independent of the training set S. Then for any δ > 0, with probability_
_at least 1 −_ _δ[′]_ _−_ _δ,_

_R(Ax(S))_ (1 + η) inf
_≤_ **x** _[R][(][x][)]_
_∈X_

_M_ _β_

+ C [2 +][ η] _Lϵ log2 n log [1]_ _S[(][A][x][(][S][)][, A][y][(][S][))]_ _._

_η_ _n_ [log 1]δ [+] _µ_ [+ 1] _δ_ [+][ △][s]

   


Therefore, plugging (41), the stability bound in (40) and the strong PD empirical risk bound in
Lemma 11 into Theorem 5, we obtain generalization bounds of SGDA. Now, we write the main
theorem of SGDA as follows.
**Theorem 6.RX and sup Assume for ally** **y** _RY z . Let, the functionxt, yt_ ( be produced by (38) withx, y) 7→ _f_ (x, y; z) is µ-SC-SC. Suppose ηt = 1/(µ(t + sup t0x))∈X. Assume ∥x∥≤
_t0 ≥_ 0. Denote∈Y ∥ Ax∥≤(S) = ¯xT and { _Ay(}S) = ¯yT for (¯xT, ¯yT ) in (39). Fixed any η > 0. Let_


-----

+ _[L][2][ log(]µT_ _[eT][ )]_ + [2(][R][X]T[+][R][Y][ )]


_M = supz∈Z f_ (0, 0; z) + L(RX + RY ). Let E = _[µt][0][(][R]X[2]T_ [+][R]Y[2] [)]


2L

3 [+]


2 1

2L√T log [6]δ [+][ 2][L][(][R][X] [+][R][Y][ )(2]T _[T][ log(6][/δ][))]_ _and B =_ _nµ[4][L]_ [+4] _µ_ _√E. There exists an absolute positive_

_constant C._ q

_(a) If Assumption 1 holds, then for any δ > 0, with probability at least 1 −_ 2δ, we have

_M_

_F_ (¯xT, ¯yT ) (1 + η)FS(¯xT, ¯yT ) + C [1 +][ η] _._
_≤_ _η_ _n_ [log(1][/δ][) +][ B][ log][2][ n][ log(1][/δ][)]

 


_(b) If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ 2δ, we have

_M_ _β_

_R(¯xT )_ (1 + η)RS(¯xT ) + C [1 +][ η] _LB log2 n log [1]_ _._
_≤_ _η_ _n_ [log 1]δ [+] _µ_ [+ 1] _δ_

   

_(c) If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ 2δ, we have

_L2(1 + η)_ 1
(¯xT, ¯yT ) (1 + η)E + C(1 + η) + _[M]_ 1 + _[β]_ _BL log2 n_ log
_△[s]_ _≤_ _nµη_ _n_ [+] _µ_ _δ_
     

_(d) If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ 2δ, we have

_△[s]_ (¯xT, ¯yT ) −△S[s] [(¯]xT, ¯yT )

_L2(1 + η)_ 1
_ηE + C(1 + η)_ + _[M]_ 1 + _[β]_ _BL log2 n_ log
_≤_ _nµη_ _n_ [+] _µ_ _δ_
    


_(e) If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ 2δ, we have

_M_ _β_

_R(¯xT )_ (1 + η) inf _LB log2 n log [1]_ _._
_≤_ **x** _[R][(][x][) +][ C][ 2 +]η_ _[ η]_ _n_ [log 1]δ [+] _µ_ [+ 1] _δ_ [+][ E]
_∈X_

   

**Remark 13. When conditions in Theorem 6 hold, we obtain that (a) If Assumption 1 holds and**
_FS(¯xT, ¯yT ) =_ _n1_, then for any δ > 0, with probability at least 1 _δ, the plain gen-_
_O_ _−_
  1 log(1/δ)

eralization error of (¯xT, ¯yT ) of SGDA is of the order O _n_ [+] _T_ 12 log2 n log(1/δ) .

(b) If Assumptions 1 and 2 hold and RS(¯xT ) = _n1_ , then for anyq _δ >_ 0, with prob-
_O_

ability at leastO _n1_ [+] log(1T 112/δ −) _δlog, the primal generalization error of2 n log(1/δ)_ . (c) If Assumptions 1 and 2 hold, then for any (¯ **xT, ¯yT ) of SGDA is of the order δ > 0,**

with probability at least q  1 _δ, the strong PD population risk and strong PD generalization error of_
(¯xT, ¯yT ) of SGDA are all of the order − _O_ _n1_ [+] log(1T 12/δ) log2 n log(1/δ) . (d) If Assumptions 1

and 2 hold and inf **x∈X R(x) = O** _n1_, then for any q _δ > 0, with probability at least_  1 _−_ _δ, the excess_
  1 log(1/δ)

primal population risk of (¯xT, ¯yT ) of SGDA is of the order O _n_ [+] _T_ 12 log2 n log(1/δ) .

For the above bounds, we can take T = (n[4]) stochastic gradient evaluations to get bound of the q  

order logn n log 32 (1/δ) . _O_
_O_
 

E PROXIMAL POINT METHOD

One of the classical algorithms studied for solving the minimax problem is the Proximal Point
method (Rockafellar, 1976). We denote the t-th iterate of PPM as (xt, yt). The averaged iterate is
defined as


**x¯T = [1]**


_T_

**xt** and **y¯T = [1]**

_T_

_t=1_

X


**yt.** (42)
_t=1_

X


-----

Given stepsize parameter ν, the PPM generates the iterate **xt+1, yt+1** by
_{_ _}_

arg min arg max _FS(x, y) + [1]_ _._ (43)
**x∈X** **y∈Y**  2ν _[∥][x][ −]_ **[x][t][∥−]** 2[1]ν _[∥][y][ −]_ **[y][t][∥]**

**xt+1, yt+1** is the unique solution since the objective function of problem (43) is strongly convex
_{_ _}_
in x and strongly concave in y. From the discussion in (Mokhtari et al., 2019), the update of PPM
can be written as

**xt+1 = xt −** _ν∇xFS(xt+1, yt+1),_
**yt+1 = yt + ν∇yFS(xt+1, yt+1),** (44)

Assume that the initial point satisfies x0 = 0 and y0 = 0.

We now begin to prove the strong PD empirical risk. Firstly, two lemmas are introduced.

**Lemma 12. (Nemirovski, 2005) Define vector v = [x, y] ∈** R[2][d] _and the operator P : R[2][d]_ _7→_ R[2][d]
_as_

_P_ (v) = [ **xFS(x, y);** **yFS(x, y)].** (45)
_∇_ _−∇_

_Consider (¯xT, ¯yT ) in (42). Suppose the ESP solution exists. Assume the function FS(x, y) is_
_continuously differentiable in x and y. Assume that FS(x, y) is a convex function of x for any y_
_and is a concave function of y for any x. Then for any v = [x, y] ∈_ R[2][d], we have


_FS(¯xT, y)_ _FS(x, ¯yT )_
_−_ _≤_ _T[1]_


_P_ (vt)[T] (vt **v).**
_t=1_ _−_

X


**Lemma 13. (Mokhtari et al., 2019) Consider the sequence of iterates {vt} ∈** R[2][d] _generated by the_
_following update_

**vt+1 = vt −** _νP_ (vt+1),

_where P is a monotone and Lipschitz continuous operator, and ν is a positive constant. Then for_
_any v ∈_ R[2][d] _and for each t ≥_ 1 we have

_P_ (vt+1)[T] (vt+1 **v) = [1]**
_−_ 2ν _[∥][v][t][ −]_ **[v][∥][2][ −]** 2[1]ν _[∥][v][t][+1][ −]_ **[v][∥][2][ −]** 2[1]ν _[∥][v][t][+1][ −]_ **[v][t][∥][2][.]**

The following lemma is the strong PD empirical risk bound of PPM.

**Lemma 14. Let {xt, yt} be the iterates generated by PPM in (44). Assume ν is a positive constant.**
_Suppose the ESP solution exists. Assume that FS(x, y) is a convex function of x for any y and_
_is a concave function of y for any x. Suppose supx_ **x** _RX and supy_ **y** _RY . If_
_Assumption 2 holds, then for all T ≥_ 1, we have _∈X ∥_ _∥≤_ _∈Y ∥_ _∥≤_

_X_ [+][ R]Y[2]
sup _FS(¯xT, y)_ inf **yT )** _._
**y∈Y** _−_ **x∈X** _[F][S][(][x][,][ ¯]_ _≤_ _[R][2]2νT_

_Proof. The update of the PPM in (44) can be written as_

**vt+1 = vt −** _νP_ (vt+1).

According to Lemma 1 in (Mokhtari et al., 2019), if FS(x, y) is convex-concave and Assumption 2
holds, then P (v) defined in (45) is monotone and Lipschitz continuous. According to Lemma 13,
we have

_P_ (vt+1)[T] (vt+1 **v) = [1]**
_−_ 2ν _[∥][v][t][ −]_ **[v][∥][2][ −]** 2[1]ν _[∥][v][t][+1][ −]_ **[v][∥][2][ −]** 2[1]ν _[∥][v][t][+1][ −]_ **[v][t][∥][2][.]**

Taking a summation of the above inequality from t = 0 to T − 1, we obtain


_T −1_

_P_ (vt+1)[T] (vt+1 **v)** (46)
_t=0_ _−_ _≤_ 2[1]ν 2ν

_[∥][v][0][ −]_ **[v][∥][2][ −]** [1] _[∥][v][T][ −]_ **[v][∥][2][.]**

X


-----

According to (46), we know that

_T −1_

_P_ (vt+1)[T] (vt+1 **v)**
_t=0_ _−_ _≤_ 2[1]ν

_[∥][v][0][ −]_ **[v][∥][2]**

X

=

_[∥][x][0][ −]_ **[x][∥][2][ +]2ν[ ∥][y][0][ −]** **[y][∥][2]**

Combined this result with Lemma 12, we can write


_FS(¯xT, y)_ _FS(x, ¯yT )_
_−_ _≤_ _[∥][x][0][ −]_ **[x][∥][2]2[ +]νT[ ∥][y][0][ −]** **[y][∥][2]**

_X_ [+][ R]Y[2]
sup _FS(¯xT, y)_ inf **yT )** _._
**y∈Y** _−_ **x∈X** _[F][S][(][x][,][ ¯]_ _≤_ _[R][2]2νT_


which implies that

The proof is complete.


Combined Lemma 7 and Lemma 14, we know that the argument stability bound of PPM is

1

**x¯[i]T** **xT** + **y¯T[i]** **yT** _S[(¯]xT, ¯yT )_
_∥_ _[−]_ [¯] _∥_ _∥_ _[−]_ [¯] _∥≤_ _nµ[4][L]_ [+ 4]r _µ_ q△[s]

1 _RX[2]_ [+][ R]Y[2]

_._ (47)

_≤_ _nµ[4][L]_ [+ 4] _µ_ 2νT

r r

Furthermore, for any x ∈X, y ∈Y and z ∈Z,

_f_ (x, y; z) ≤ _f_ (0, 0; z) + L∥x − 0∥ + L∥y − 0∥≤ _zsup∈Z_ _f_ (0, 0; z) + L(RX + RY ), (48)

Therefore, plugging (48), the stability bound in (47) and the strong PD empirical risk bound in
Lemma 14 into Theorem 1, we obtain generalization bounds of PPM, shown as below.
**Theorem 7.RX and sup Assume for ally** **y** _R zY, the function . Let_ **xt, y (tx, be produced by (44). Assume y) 7→** _f_ (x, y; z) is µ-SC-SC. Suppose ν is a positive con- supx∈X ∥x∥≤
_stant. Denote∈Y A ∥x(S∥≤) = ¯xT and A {y(S) = ¯}_ **yT for (¯xT, ¯yT ) in (42). Fixed any η > 0. Let**

_M = supz_ _f_ (0, 0; z) + L(RX + RY ). Let E = _[R]X[2]2νT[+][R]Y[2]_ _and B =_ _nµ[4][L]_ [+ 4] _µ1_ _√E. There exists_
_∈Z_

_an absolute positive constant C._ q

_(a) If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ, we have_

_M_

_F_ (¯xT, ¯yT ) (1 + η)FS(¯xT, ¯yT ) + C [1 +][ η] _._
_≤_ _η_ _n_ [log(1][/δ][) +][ B][ log][2][ n][ log(1][/δ][)]

 

_(b) If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ, we have_

_M_ _β_

_R(¯xT )_ (1 + η)RS(¯xT ) + C [1 +][ η] _LB log2 n log [1]_ _._
_≤_ _η_ _n_ [log 1]δ [+] _µ_ [+ 1] _δ_

   

_(c) If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ, we have_

_L2(1 + η)_ 1
(¯xT, ¯yT ) (1 + η)E + C(1 + η) + _[M]_ 1 + _[β]_ _BL log2 n_ log _._
_△[s]_ _≤_ _nµη_ _n_ [+] _µ_ _δ_
     

_(d) If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ, we have_

_△[s]_ (¯xT, ¯yT ) −△S[s] [(¯]xT, ¯yT )

_L2(1 + η)_ 1
_ηE + C(1 + η)_ + _[M]_ 1 + _[β]_ _BL log2 n_ log _._
_≤_ _nµη_ _n_ [+] _µ_ _δ_
     

_(e) If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ, we have_

_M_ _β_

_R(¯xT )_ (1 + η) inf _LB log2 n log [1]_ _._
_−_ **x** _[R][(][x][)][ ≤]_ _[C][ 2 +]η_ _[ η]_ _n_ [log 1]δ [+] _µ_ [+ 1] _δ_ [+][ E]
_∈X_

   


-----

**Remark 14. When conditions in Theorem 7 hold, we obtain that (a) If Assumption 1 and 2 hold and**
_FS(¯xT, ¯yT ) =_ _n1_, then for any δ > 0, with probability at least 1 _δ, the plain generalization_
_O_ _−_
  1 1

error of (¯xT, ¯yT ) of PPM is of the order O _n_ [+] _T_ log2 n log(1/δ) . (b) If Assumptions

1 and 2 hold and RS(¯xT ) = _n1_, then for any _δ >q_  0, with probability at least 1 _δ, the_
_O_ _−_
  1 1

primal generalization error of (¯xT, ¯yT ) of PPM is of the order O _n_ [+] _T_ log2 n log(1/δ) .

(c) If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least q  1 _δ, the strong_
_−_
PD population risk and strong PD generalization error of (¯xT, ¯yT ) of PPM are all of the order

_O_ _n1_ [+] _T1_ log2 n log(1/δ) . (d) If Assumptions 1 and 2 hold and inf **x∈X R(x) = O** _n1_,

then for any q δ > 0, with probability at least 1 − _δ, the excess primal population risk of (¯xT, ¯yT ) of_

PPM is of the order _n1_ [+] _T1_ log2 n log(1/δ) . For the above bounds, we can take T = (n[2])
_O_ _O_

gradient evaluations to get bound of the order q  logn n log(1/δ) .
_O_
 

F EXTRAGRADIENT METHOD


EG is a classical algorithm for solving minimax problems introduced by (Korpelevich, 1976). We
now introduce some notations. Followed (Mokhtari et al., 2019), we consider the following update
of EG: given stepsize parameter ν, we first compute a set of mid-point iterates {xt+ 12 _[,][ y][t][+][ 1]2_ _[}]_

**xt+ 12** [=][ x][t][ −] _[ν][∇][x][F][S][(][x][t][,][ y][t][)][,]_

**yt+ 12** [=][ y][t][ +][ ν][∇][y][F][S][(][x][t][,][ y][t][)][,] (49)


we then compute the next iterates **xt+1, yt+1**
_{_ _}_

**xt+1 = xt −** _ν∇xFS(xt+ 12_ _[,][ y][t][+][ 1]2_ [)][,]

**yt+1 = yt + ν∇yFS(xt+ 12** _[,][ y][t][+][ 1]2_ [)][.] (50)


Consider the averaged iterate

**x¯T = [1]**


_T_

**xt** and **y¯T = [1]**

_T_

_t=1_

X


**yt.** (51)
_t=1_

X


Assume that the initial point satisfies x0 = x 1/2 and y0 = y 1/2.
_−_ _−_

We now show the strong PD empirical risk bound for (¯xT, ¯yT ) of EG.

**Lemma 15. (Mokhtari et al., 2019) Let** **xt, yt** _,_ **xt+1/2, yt+1/2** _be the iterates generated by the_
_{_ _}_ _{_ _}_
_EG updates in (49) and (50). Assume that the initial point satisfies x0 = x_ 1/2 and y0 = y 1/2.
_−_ _−_
_Suppose the ESP solution (ˆx[∗], ˆy[∗]) exists. Assume that FS(x, y) is a convex function of x for any_
**y and is a concave function of y for any x. If Assumption 2 holds and the stepsize ν satisfies the**
_condition ν =_ 2cβ _[for any][ c][ ∈]_ [(0][,][ 1)][, then:]

_(a) the iterates_ **xt, yt** _,_ **xt+1/2, yt+1/2** _stay within the compact convex set_
_{_ _}_ _{_ _}_


_D :=_ (x, y)|∥x − **xˆ[∗]∥[2]** + ∥y − **yˆ[∗]∥[2]** _≤_ 2 +
 

_(b) for all T ≥_ 1, we have


( **x0** **xˆ[∗]** + **y0** **yˆ[∗]** )
_∥_ _−_ _∥[2]_ _∥_ _−_ _∥[2]_


1 − 4ν[2]β[2]


(52)


sup _FS(¯xT, y)_ inf **yT )** 2β(16 + 2(133−c[2]) [)(][∥][x][0][ −] **x[ˆ][∗]∥[2]** + ∥y0 − **yˆ[∗]∥[2])**
**y:(¯xT,y)∈D** _−_ **x:(x,y¯T )∈D** _[F][S][(][x][,][ ¯]_ _≤_ _T_


-----

Combined Lemma 7 and Lemma 15, we know that the argument stability bound of EG is

1

**x¯[i]T** **xT** + **y¯T[i]** **yT** _S[(¯]xT, ¯yT )_
_∥_ _[−]_ [¯] _∥_ _∥_ _[−]_ [¯] _∥≤_ _nµ[4][L]_ [+ 4]r _µ_ q△[s]

1 2β(16 + 2(133−c[2]) [)(][∥][x][0][ −] **x[ˆ][∗]∥[2]** + ∥y0 − **yˆ[∗]∥[2])** _._

_≤_ _nµ[4][L]_ [+ 4] _µ_ s _T_

r

(53)

Furthermore, for any x ∈X, y ∈Y and z ∈Z,

_f_ (x, y; z) ≤ _f_ (ˆx[∗], ˆy[∗]; z) + L∥x − **xˆ[∗]∥** + L∥y − **yˆ[∗]∥**

_≤_ _zsup∈Z_ _f_ (ˆx[∗], ˆy[∗]; z) + _√2L_ _∥x −_ **xˆ[∗]∥[2]** + ∥y − **yˆ[∗]∥[2]**

p

4
sup _f_ (ˆx[∗], ˆy[∗]; z) + L 4 + ( **x0** **xˆ[∗]** + **y0** **yˆ[∗]** ), (54)
_≤_ _z_ 1 4ν[2]β[2] _∥_ _−_ _∥[2]_ _∥_ _−_ _∥[2]_
_∈Z_ r _−_ 

where the first inequality follows from Assumption 1, the second inequality follows from CaucySchwarz inequality and the last inequality follows from (52). Now, plugging (54), the stability
bound in (53) and the strong PD empirical risk bound in Lemma 15 into Theorem 1, we obtain
generalization bounds of EG. The main theorem is shown below.
**Theorem 8. Assume for all z, the function (x, y) 7→** _f_ (x, y; z) is µ-SC-SC. Let {xt, yt} and
**xt+1/2, yt+1/2** _be the iterates produced by (49)-(50). Assume the stepsize ν satisfies the con-_
_{dition ν =_ 2cβ _}[for any][ c][ ∈]_ [(0][,][ 1)][. Denote][ A][x][(][S][) = ¯]xT and Ay(S) = ¯yT for (¯xT, ¯yT ) in

_(51). Denote the ESP solution as (ˆx[∗], ˆy[∗]). Consider the compact convex set in (52). Fixed any_

_η > 0. Let M = supz∈Z f_ (ˆx[∗], ˆy[∗]; z) + L 4 + 1−44ν[2]β[2] (∥x0 − **xˆ[∗]∥[2]** + ∥y0 − **yˆ[∗]∥[2]). Let**

_B =_ 2β(16+ 2(133−c[2] ) [)(][∥][x]T[0][−]x[ˆ][∗]∥[2]+∥y0−yˆ[∗]∥[2]) _and Er =_ _nµ[4][L]_ [+ 4] _µ1√B. There exists an absolute posi-_

_tive constant C._ q

_(a) If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ, we have_

_M_

_F_ (¯xT, ¯yT ) (1 + η)FS(¯xT, ¯yT ) + C [1 +][ η] _._
_≤_ _η_ _n_ [log(1][/δ][) +][ B][ log][2][ n][ log(1][/δ][)]

 

_(b) If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ, we have_

_M_ _β_

_R(¯xT )_ (1 + η)RS(¯xT ) + C [1 +][ η] _LB log2 n log [1]_ _._
_≤_ _η_ _n_ [log 1]δ [+] _µ_ [+ 1] _δ_

   

_(c) If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ, we have_

_L2(1 + η)_ 1
(¯xT, ¯yT ) (1 + η)E + C(1 + η) + _[M]_ 1 + _[β]_ _BL log2 n_ log _._
_△[s]_ _≤_ _nµη_ _n_ [+] _µ_ _δ_
     


_(d) If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ, we have_

_△[s]_ (¯xT, ¯yT ) −△S[s] [(¯]xT, ¯yT )

_L2(1 + η)_ 1
_ηE + C(1 + η)_ + _[M]_ 1 + _[β]_ _BL log2 n_ log _._
_≤_ _nµη_ _n_ [+] _µ_ _δ_
     

_(e) If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ, we have_

_M_ _β_

_R(¯xT )_ (1 + η) inf _LB log2 n log [1]_ _._
_≤_ **x** _[R][(][x][) +][ C][ 2 +]η_ _[ η]_ _n_ [log 1]δ [+] _µ_ [+ 1] _δ_ [+][ E]
_∈X_

   

**Remark 15. When conditions in Theorem 8 hold, we obtain that (a) If Assumptions 1 and 2 hold and**
_FS(¯xT, ¯yT ) =_ _n1_, then for any δ > 0, with probability at least 1 _δ, the plain generalization_
_O_ _−_
 


-----

error of (¯xT, ¯yT ) of EG is of the order O _n1_ [+] _T1_ log2 n log(1/δ) . (b) If Assumptions 1

and 2 hold and RS(¯xT ) = _n1_, then for any _δ >q 0, with probability at least_  1 _δ, the primal_
_O_ _−_
  1 1

generalization error of (¯xT, ¯yT ) of EG is of the order O _n_ [+] _T_ log2 n log(1/δ) . (c) If

Assumptions 1 and 2 hold, then for any δ > 0, with probability at least q  1 _δ, the strong PD_
population risk and the strong PD generalization error of (¯xT, ¯yT ) of EG are all of the order − _O_ _n1_ [+]

1 1 

_T_ log2 n log(1/δ) . (d) If Assumptions 1 and 2 hold and inf **x∈X R(x) = O** _n_, then for

qany δ > 0, with probability at least 1 − _δ, the excess primal population risk of (¯xT, ¯yT ) of EG_

is O _n1_ [+] _T1_ log2 n log(1/δ) . For the above bounds, we can take T = O(n[2]) gradient

evaluations to get bound of the order q   logn n log(1/δ) .
_O_
 

G OPTIMISTIC GRADIENT DESCENT ASCENT

OGDA is introduced by Popov (1980), as a variant of the EG method. We introduce some notations
to state the result of OGDA. Given a stepsize parameter ν > 0, OGDA do the following update for
each t ≥ 0

**xt+1 = xt** 2ν **xFS(xt, yt) + ν** **xFS(xt** 1, yt 1),
_−_ _∇_ _∇_ _−_ _−_
**yt+1 = yt + 2ν** **yFS(xt, yt)** _ν_ **yFS(xt** 1, yt 1). (55)
_∇_ _−_ _∇_ _−_ _−_

Assume that the initial point satisfies x0 = x 1 and y0 = y 1. Consider the averaged iterate
_−_ _−_


**x¯T = [1]**


_T_

**xt** and **y¯T = [1]**

_T_

_t=1_

X


**yt.** (56)
_t=1_

X


We first provide a lemma on the strong PD empirical risk of OGDA.
**Lemma 16. (Mokhtari et al., 2019) Let {xt, yt} be the iterates generated by the OGDA updates in**
_(55). Assume that the initial point satisfies x0 = x_ 1 and y0 = y 1. Suppose the ESP solution
_−_ _−_
(ˆx[∗], ˆy[∗]) exists. Assume that FS(x, y) is a convex function of x for any y and is a concave function
_of y for any x. If Assumption 2 holds and the stepsize ν satisfies 0 < ν ≤_ 41β _[, then:]_

_(a) the iterates_ **xt, yt** _stay within the compact convex set_
_{_ _}_

_D :=_ (x, y) **x** **xˆ[∗]** + **y** **yˆ[∗]** 2( **x0** **xˆ[∗]** + **y0** **yˆ[∗]** ) _._ (57)
_|∥_ _−_ _∥[2]_ _∥_ _−_ _∥[2]_ _≤_ _∥_ _−_ _∥[2]_ _∥_ _−_ _∥[2]_



_(b) for all T ≥_ 1, we have

sup _FS(¯xT, y)_ inf **yT )**
**y:(¯xT,y)∈D** _−_ **x:(x,y¯T )∈D** _[F][S][(][x][,][ ¯]_ _≤_ [(16][β][ +]


21ν [)(][∥][x][0][ −] **x[ˆ][∗]∥[2]** + ∥y0 − **yˆ[∗]∥[2])**


Combined Lemma 7 and Lemma 16, we know that the argument stability bound of OGDA is

1

**x¯[i]T** **xT** + **y¯T[i]** **yT** _S[(¯]xT, ¯yT )_
_∥_ _[−]_ [¯] _∥_ _∥_ _[−]_ [¯] _∥≤_ _nµ[4][L]_ [+ 4]r _µ_ q△[s]

1 (16β + 21ν [)(][∥][x][0][ −] **x[ˆ][∗]∥[2]** + ∥y0 − **yˆ[∗]∥[2])** _._

_≤_ _nµ[4][L]_ [+ 4] _µ_ s _T_

r

Moreover, similar to (54), we have


(58)


_f_ (x, y; z) ≤ _f_ (ˆx[∗], ˆy[∗]; z) + L∥x − **xˆ[∗]∥** + L∥y − **yˆ[∗]∥**

sup _f_ (ˆx[∗], ˆy[∗]; z) + 2L **x0** **xˆ[∗]** + **y0** **yˆ[∗]** _._ (59)
_≤_ _z_ _∥_ _−_ _∥[2]_ _∥_ _−_ _∥[2]_
_∈Z_
p

Therefore, plugging (59), the stability bound in (58) and the strong PD empirical risk bound in
Lemma 16 into Theorem 1, we obtain generalization bounds of OGDA.


-----

**Theorem 9. Assume for all z, the function (x, y)** _f_ (x, y; z) is µ-SC-SC. Let **xt, yt** _be pro-_
_7→_ 1 _{_ _}_
_duced by (55). Assume the stepsize parameter ν satisfies 0 < ν ≤_ 4β _[. Denote][ A][x][(][S][) = ¯]xT and_

_Ay(S) = ¯yT for (¯xT, ¯yT ) in (56). Denote the ESP solution as (ˆx[∗], ˆy[∗]). Consider the compact con-_
_vex set in (57). Fixed any η > 0. Let M = supz_ _f_ (ˆx[∗], ˆy[∗]; z) + 2L **x0** **xˆ[∗]** + **y0** **yˆ[∗]** _._

_Define B =_ (16β+ 2[1]ν [)(][∥][x][0][−]Tx[ˆ][∗]∥[2]+∥y0−yˆ[∗]∥[2]) _and∈Z E =_ _nµ4L_ [+ 4] _µ1_ _√pB∥. There exists an absolute −_ _∥[2]_ _∥_ _−_ _∥[2]_

_positive constant C._ q

_(a) If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ, we have_

_M_

_F_ (¯xT, ¯yT ) (1 + η)FS(¯xT, ¯yT ) + C [1 +][ η] _._
_≤_ _η_ _n_ [log(1][/δ][) +][ B][ log][2][ n][ log(1][/δ][)]

 

_(b) If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ, we have_

_M_ _β_

_R(¯xT )_ (1 + η)RS(¯xT ) + C [1 +][ η] _LB log2 n log [1]_ _._
_≤_ _η_ _n_ [log 1]δ [+] _µ_ [+ 1] _δ_

   

_(c) If Assumptions 1 and 2 hold, then for anyL δ >2(1 + 0, with probability at least η)_ 1 − _δ, we have1_

(¯xT, ¯yT ) (1 + η)E + C(1 + η) + _[M]_ 1 + _[β]_ _BL log2 n_ log _._
_△[s]_ _≤_ _nµη_ _n_ [+] _µ_ _δ_
     

_(d) If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ, we have_

_△[s]_ (¯xT, ¯yT ) −△S[s] [(¯]xT, ¯yT )

_L2(1 + η)_ 1
_ηE + C(1 + η)_ + _[M]_ 1 + _[β]_ _BL log2 n_ log _._
_≤_ _nµη_ _n_ [+] _µ_ _δ_
     

_(e) If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least 1 −_ _δ, we have_

_M_ _β_

_R(¯xT )_ (1 + η) inf _LB log2 n log [1]_ _._
_≤_ **x** _[R][(][x][) +][ C][ 2 +]η_ _[ η]_ _n_ [log 1]δ [+] _µ_ [+ 1] _δ_ [+][ E]
_∈X_

   

**Remark 16. When conditions in Theorem 9 hold, we obtain that (a) If Assumptions 1 and 2 hold and**
_FS(¯xT, ¯yT ) =_ _n1_, then for any δ > 0, with probability at least 1 _δ, the plain generalization_
_O_ _−_
  1 1

error of (¯xT, ¯yT ) of OGDA is of the order O _n_ [+] _T_ log2 n log(1/δ) . (b) If Assumptions 1

and 2 hold and RS(¯xT ) = _n1_, then for any δ >q 0, with probability at least  1 _δ, the primal_
_O_ _−_
  1 1

generalization error of (¯xT, ¯yT ) of OGDA is of the order O _n_ [+] _T_ log2 n log(1/δ) . (c)

If Assumptions 1 and 2 hold, then for any δ > 0, with probability at least q  1 _δ, the strong PD_
_−_
population risk and the strong PD generalization error of (¯xT, ¯yT ) of OGDA are all of the order

_O_ _n1_ [+] _T1_ log2 n log(1/δ) . (d) If Assumptions 1 and 2 hold and inf **x∈X R(x) = O** _n1_,

then for any q δ > 0, with probability at least 1 _δ, the excess primal population risk of OGDA is of_ 
_−_

the order O _n1_ [+] _T1_ log2 n log(1/δ) . For the above bounds, we can take T = O(n[2]) gradient

evaluations to get bound of the order q  logn n log(1/δ) .
_O_
 

H AUXILIARY DESCRIPTIONS OF TABLE 1


In Table 1, Lip means Lipschitz continuity and S means smoothness. (R)-ESP means the
(regularized)-empirical risk saddle point (Zhang et al., 2021a). C-SC means convex-µ-stronglyconcave, and NC-SC means nonconvex-µ-strongly-concave. A function f (x, y) is called
nonconvex-strongly-concave if f (x, ·) is strongly-concave for every x. Moreover, a function f (x, y)
is µ-weakly-convex-weakly-concave (WC-WC) if f + _[µ]2_ **x** + **y** is convex-concave. V-WC
_∥_ _∥[2]_ _∥_ _∥[2][]_
WC is a variant of WC-WC, please refer to (Lei et al., 2021). PL means the two-sided PL condition,
 
which relaxes the convex-concavity requirement of the objective function (Yang et al., 2020) and is
usually used to guarantee the linear convergence rate (Karimi et al., 2016; Yang et al., 2020). AGDA
algorithm is variant of GDA with alternating updates of the primal-dual variables. c is a parameter
in the step size, β is a parameter in Assumption 2 and k := β/µ.


-----

I NUMERICAL EXPERIMENTS

In this section, we report preliminary experimental results to verify our theoretical results by performing numerical experiments on the simulated data. We study how the generalization error would
behave along the number of samples. To this aim, we consider an isotropic Gaussian data vector
**Z ∼N** (0, Id×d) with zero mean and identity covariance. We will draw n independent samples
from the underlying Gaussian distribution to form a training dataset S = **z1, ..., zn** . We set the
_{_ _}_
dimension d of Z as 50. Similar to the strongly-convex-strong-concave case of (Farnia & Ozdaglar,
2021), we consider the following minimax objective function

_f_ (x, y; z) = x[T] (z **y) +** _[µ]_
_−_ 2 [(][∥][x][∥][2][ −∥][y][∥][2][)][.]

In the experiments, we set µ = 1 and constrain optimization variables x and y to satisfy ∥x∥, ∥y∥≤
100 which we enforced by projection. For this minimax objective function, one can verify that

_F_ (x, y) − _FS(x, y) = x[T]_ (E[Z] − ES[Z]); _R(x) −_ _RS(x) = x[T]_ (E[Z] − ES[Z]),

where E[Z] = 0 since the mean of the underlying Gaussian distribution is 0, and where ES[Z] =
1 _n_
_n_ _i=1_ _[z][i][. For brevity, we call][ |][x][T][ (][E][[][Z][]][ −]_ [E][S][[][Z][])][|][ the “generalization error”.]

We apply the above experimental settings to validate the theoretical results of GDA, SGDA, EG,P
and OGDA. We evaluate the generalization error |x[T] (E[Z] − ES[Z])| and apply these algorithms
to S. For GDA and SGDA, we consider the stepsize parameter as 1/t. We iterate GDA with n[2]
times and SGDA with n[4] times. The generalization error of GDA and SGDA with different sizes of
training data are reported in Figure 1. And for EG and OGDA, we select the stepsize parameter as
0.003. We run EG and OGDA n[2] times. Similarly, the generalization error of EG and OGDA with
different sizes of training data are given in Figure 2. From the two figures, we can see that the line of
best fit for the generalization error is [log]n[0][3][.][/][98][2][ n] for GDA, _n[log][0][.][98][ n][ for SGDA,][ log]n[0][0][.][.][99][98][ n]_ for EG, and _n[log][1][.][02][ n]_

for OGDA. These results match the predictive rates of the plain generalization error and the primal
generalization error in Table 1, i.e., [log][3]n[/][2][ n] for GDA, [log]n[ n] for SGDA, [log]n[ n] for EG, and [log]n[ n] for

OGDA, which verifies our theoretical findings.

Figure 1: |x[T] (E[Z] − ES[Z])| versus the number of samples on GDA (left) and SGDA (right).

Figure 2: |x[T] (E[Z] − ES[Z])| versus the number of samples on EG (left) and OGDA (right).


-----

