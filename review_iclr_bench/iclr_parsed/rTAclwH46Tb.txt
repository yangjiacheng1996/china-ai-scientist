# EIGENCURVE: OPTIMAL LEARNING RATE SCHEDULE
## FOR SGD ON QUADRATIC OBJECTIVES WITH SKEWED
# HESSIAN SPECTRUMS

**Rui Pan[1][∗], Haishan Ye[2][ ∗†], Tong Zhang[1][ ∗‡]**

1The Hong Kong University of Science and Technology
2Xi’an Jiaotong University
rpan@connect.ust.hk, yehaishan@xjtu.edu.cn

ABSTRACT

Learning rate schedulers have been widely adopted in training deep neural networks. Despite their practical importance, there is a discrepancy between its practice and its theoretical analysis. For instance, it is not known what schedules
of SGD achieve best convergence, even for simple problems such as optimizing
quadratic objectives. In this paper, we propose Eigencurve, the first family of
learning rate schedules that can achieve minimax optimal convergence rates (up
to a constant) for SGD on quadratic objectives when the eigenvalue distribution
of the underlying Hessian matrix is skewed. The condition is quite common in
practice. Experimental results show that Eigencurve can significantly outperform
step decay in image classification tasks on CIFAR-10, especially when the number of epochs is small. Moreover, the theory inspires two simple learning rate
schedulers for practical applications that can approximate eigencurve. For some
problems, the optimal shape of the proposed schedulers resembles that of cosine
decay, which sheds light to the success of cosine decay for such situations. For
other situations, the proposed schedulers are superior to cosine decay.

1 INTRODUCTION

Many machine learning models can be represented as the following optimization problem:


min
_w_ _[f]_ [(][w][)][ ≜] _n[1]_


_fi(w),_ (1.1)
_i=1_

X


such as logistic regression, deep neural networks. To solve the above problem, stochastic gradient
descent (SGD) (Robbins & Monro, 1951) has been widely adopted due to its computation efficiency
in large-scale learning problems (Bottou & Bousquet, 2008), especially for training deep neural
networks.

Given the popularity of SGD in this field, different learning rate schedules have been proposed to
further improve its convergence rates. Among them, the most famous and widely used ones are
inverse time decay, step decay (Goffin, 1977), and cosine scheduler (Loshchilov & Hutter, 2017).
The learning rates generated by the inverse time decay scheduler depends on the current iteration
number inversely. Such a scheduling strategy comes from the theory of SGD on strongly convex
functions, and is extended to non-convex objectives like neural networks while still achieving good
performance. Step decay scheduler keeps the learning rate piecewise constant and decreases it by
a factor after a given amount of epochs. It is theoretically proved in Ge et al. (2019) that when
the objective is quadratic, step decay scheduler outperforms inverse time decay. Empirical results
are also provided in the same work to demonstrate the better convergence property of step decay
in training neural networks when compared with inverse time decay. However, even step decay

_∗Equal contribution._
_†Corresponding author is Haishan Ye._
_‡Jointly with Google Research._


-----

is proved to be near optimal on quadratic objectives, it is not truly optimal. There still exists a
log T gap away from the minimax optimal convergence rate, which turns out to be non-trivial in a
wide range of settings and may greatly impact step decay’s empirical performance. Cosine decay
scheduler (Loshchilov & Hutter, 2017) generates cosine-like learning rates in the range [0, T ], with
_T being the maximum iteration. It is a heuristic scheduling strategy which relies on the observation_
that good performance in practice can be achieved via slowly decreasing the learning rate in the
beginning and “refining” the solution in the end with a very small learning rate. Its convergence
property on smooth non-convex functions has been shown in Li et al. (2021), but the provided
bound is still not tight enough to explain its success in practice.

Except cosine decay scheduler, all aforementioned learning rate schedulers have (or will have) a
tight convergence bound on quadratic objectives. In fact, studying their convergence property on
quadratic objective functions is quite important for understanding their behaviors in general nonconvex problems. Recent studies in Neural Tangent Kernel (NTK) (Arora et al., 2019; Jacot et al.,
2020) suggest that when neural networks are sufficiently wide, the gradient descent dynamic of
neural networks can be approximated by NTK. In particular, when the loss function is least-square
loss, neural network’s inference is equivalent to kernel ridge regression with respect to the NTK
in expectation. In other words, for regression tasks, the non-convex objective in neural networks
resembles quadratic objectives when the network is wide enough.

The existence of log T gap in step decay’s convergence upper bound, which will be proven to be tight
in a wide range of settings, implies that there is still room for improvement in theory. Meanwhile, the
existence of cosine decay scheduler, which has no strong theoretical convergence guarantees but possesses good empirical performance in certain tasks, suggests that its convergence rate may depend on
some specific properties of the objective determined by the network and dataset in practice. Hence
it is natural to ask what those key properties may be, and whether it is possible to find theoreticallyoptimal schedulers whose empirical performance are comparable to cosine decay if those properties
are available. In this paper, we offer an answer to these questions. We first derive a novel eigenvaluedistribution-based learning rate scheduler called eigencurve for quadratic functions. Combining
with eigenvalue distributions of different types of networks, new neural-network-based learning rate
schedulers can be generated based on our proposed paradigm, which achieve better convergence
properties than step decay in Ge et al. (2019). Specifically, eigencurve closes the log T gap in
step decay and reaches minimax optimal convergence rates if the Hessian spectrum is skewed. We
summarize the main contributions of this paper as follows.

1. To the best of our knowledge, this is the first work that incorporates the eigenvalue distribution of objective function’s Hessian matrix into designing learning rate schedulers. Accordingly, based on the eigenvalue distribution of the Hessian, we propose a novel eigenvalue
distribution based learning rate scheduler named eigencurve.

2. Theoretically, eigencurve can achieve optimal convergence rate (up to a constant) for
SGD on quadratic objectives when the eigenvalue distribution of the Hessian is skewed.
Furthermore, even when the Hessian is not skewed, eigencurve can still achieve no
worse convergence rate than the step decay schedule in Ge et al. (2019), whose convergence
rate are proven to be sub-optimal in a wide range of settings.

3. Empirically, on image classification tasks, eigencurve achieves optimal convergence
rate for several models on CIFAR-10 and ImageNet if the loss can be approximated by
quadratic objectives. Moreover, it obtains much better performance than step decay on
CIFAR-10, especially when the number of epochs is small.

4. Intuitively, our learning rate scheduler sheds light on the theoretical property of cosine
decay and provides a perspective of understanding the reason why it can achieve good
performance on image classification tasks. The same idea has been used to inspire and
discover several simple families of schedules that works in practice.

**Problem Setup** For the theoretical analysis and the aim to derive our eigenvalue-dependent learning rate schedulers, we mainly focus on the quadratic function, that is,

min (1.2)
_w_ _[f]_ [(][w][)][ ≜] [E][ξ][ [][f] [(][w, ξ][)]][,][ where][ f] [(][w, ξ][) = 1]2 _[w][⊤][H][(][ξ][)][w][ −]_ _[b][(][ξ][)][⊤][w,]_

where ξ denotes the data sample. Hence, the Hessian of f (w) is
_H = Eξ [H(ξ)] ._ (1.3)


-----

Letting us denote b = Eξ[b(ξ)], we can obtain the optima of problem (1.2)

_w_ = H _[−][1]b._ (1.4)
_∗_

Given an initial iterate w0 and the learning rate sequence {ηt}, the stochastic gradient update is

_wt+1 = wt −_ _ηt∇f_ (wt, ξ) = wt − _ηt(H(ξ)wt −_ _b(ξ))._ (1.5)

We denote that

_nt = Hwt −_ _b −_ (H(ξ)wt − _b(ξ)),_ _µ ≜_ _λmin(H),_ _L ≜_ _λmax(H),_ and, _κ ≜_ _L/µ. (1.6)_

In this paper, we assume that

Eξ _ntn[⊤]t_ _σ[2]H._ (1.7)
_⪯_

The reason for this assumption is presented in Appendix G.5. 

**Related Work** In convergence analysis, one key property that separates Table 1: Convergence rate of SGD with common schedSGD from vanilla gradient descent is ulers on quadratic objectives.
that in SGD, noise in gradients dominates. In gradient descent (GD), constant learning rate can achieve linear Scheduler Convergence rate of SGD in

quadratic objectives

convergence (c[T] ) with 0 < c < 1 for
_O_ Constant Not guaranteed to converge
strongly convex objectives, i.e. obtain
_dσ[2]_

ing f (w[(][t][)]) − _f_ (w[∗]) ≤ _ϵ in O(log(_ [1]ϵ [))] Inverse Time Decay Θ  _T_ _· κ_

iterations. However, in SGD, f (w[(][t][)]) Θ _dσT[2]_ _· log T_
cannot even be guaranteed to converge Step Decay (Ge et al. (2019); Wu et al. (2021); 
to f (w[∗]) due to the existence of gra- This work - Theorem 4)
dient noise (Bottou et al., 2018).tuitively, this noise leads to a varianceproportional to the learning rate size, soIn- Eigencurve _O_  _dσOT[2]_ dσTwith skewed Hessian spectrums,[2] _· log κ_ in worst case

(This work - Theorem 1, Corollary 2, 3) 

constant learning rate will always introduce a Ω(ηt) = Ω(η0) gap when compared with the convergence rate of GD.
Fortunately, inverse time decay scheduler solves the problem by decaying the learning rate inversely
proportional to the iteration number t, which achieves O( _T[1]_ [)][ convergence rate for strongly convex]

objectives, specifically, ( _[dσ]T_ [2] _κ). However, this is sub-optimal since the minimax optimal rate_
_O_ _·_

for SGD is O( _[dσ]T_ [2] [)][ (Ge et al., 2019; Jain et al., 2018). Moreover, in practice,][ κ][ can be very big]

for large neural networks, which makes inverse time decay scheduler undesirable for those models.
This is when step decay (Goffin, 1977) comes to play. Empirically, it is widely adopted in tasks
such as image classification and serves as a baseline for a lot of models. Theoretically, it has been
proven that step decay can achieve nearly optimal convergence rate ( _[dσ]T_ [2]

vex least square regression (Ge et al., 2019). A tighter set of instance-dependent bounds in a recent O _[·][ log][ T]_ [)][ for strongly con-]
work (Wu et al., 2021), which is carried out independently from ours, also proves its near optimality. Nevertheless, step decay is not always the best choice for image classification tasks. In practice,
cosine decay (Loshchilov & Hutter, 2017) can achieve comparable or even better performance, but
the reason behind this superior performance is still unknown in theory (Gotmare et al., 2018). All
the aforementioned results are summarized in Table 1, along with our results in this paper. It is
worth mentioning that the minimax optimal rate O( _[dσ]T_ [2] [)][ can be achieved by iterate averaging meth-]

ods (Jain et al., 2018; Bach & Moulines, 2013; D´efossez & Bach, 2015; Frostig et al., 2015; Jain
et al., 2016; Neu & Rosasco, 2018), but it is not a common practice to use them in training deep
neural networks, so only the final iterate (Shamir, 2012) behavior of SGD is analyzed in this paper,
i.e. the point right after the last iteration.

**Paper organization: Section 2 describes the motivation of our eigencurve scheduler. Section 3**
presents the exact form and convergence rate of the proposed eigencurve scheduler, along with
the lower bound of step decay. Section 4 shows the experimental results. Section 5 discusses the
discovery and limitation of eigencurve and Section 6 gives our conclusion.


-----

2 MOTIVATION

In this section, we will give the main motivation and intuition of our eigencurve learning rate
scheduler. We first give the scheduling strategy to achieve the optimal convergence rate in the case
that the Hessian is diagonal. Then, we show that the inverse time learning rate is sub-optimal in
most cases. Comparing these two scheduling methods brings up the reason why we should design
eigenvalue distribution dependent learning rate scheduler.

Letting H be a diagonal matrix diag(λ1, λ2, . . ., λd) and reformulating Eqn. (1.5), we have
_wt+1 −_ _w∗_ =wt − _w∗_ _−_ _ηt(H(ξ)wt −_ _b(ξ))_
=wt _w_ _ηt(Hwt_ _b) + ηt (Hwt_ _b_ (H(ξ)wt _b(ξ)))_
_−_ _∗_ _−_ _−_ _−_ _−_ _−_
=wt _w_ _ηt(Hwt_ _b_ (Hw _b)) + ηt (Hwt_ _b_ (H(ξ)wt _b(ξ)))_
_−_ _∗_ _−_ _−_ _−_ _∗_ _−_ _−_ _−_ _−_
= (I − _ηtH) (wt −_ _w∗) + ηtnt._

It follows,

E _λj (wt+1,j −_ _w∗,j)[2][i][ E][[][n]=[t][]=0]λj_ (1 − _ηtλj)[2]E_ (wt,j − _w∗,j)[2][]_ + ηt[2][E][ ∥][[][n][t][]][j][∥][2][o] (2.1)
h (1.7) n 

(1 _ηtλj)[2]_ _λjE_ (wt,j _w_ _,j)[2][]_ + ηt[2][λ]j[2][σ][2][.]
_≤_ _−_ _·_ _−_ _∗_

Since H is diagonal, we can set step size scheduling for each dimension separately. Letting us
choose step size ηt coordinately with the step size ηt,j = _λj_ (t1+1) [being optimal for the][ j][-th coordi-]

nate, then we have the following proposition.
**Proposition 1. Assume that H is diagonal matrix with eigenvalues λ1** _λ2_ _. . ., λd_ 0 and
_Eqn. (1.7) holds. If we set step size ηt,j =_ _λj_ (t1+1) _[, it holds that]_ _≥_ _≥_ _≥_

_d_ _d_

_j=1_ _[λ][j][(][w][1][,j][ −]_ _[w][∗][,j][)][2]_ _t_

2E [f (wt+1) _f_ (w )] = E _λj (wt+1,j_ _w_ _,j)[2]_ +
_−_ _∗_ j=1 _−_ _∗_  _≤_ P (t + 1)[2] (t + 1)[2][ ·][dσ][2][.]

X
  (2.2)

The leading equality here is proved in Appendix G.1, with the followed inequality proved in Appendix E. From Eqn. (2.2), we can observe that choosing proper step sizes coordinately can achieve
the optimal convergence rate (Ge et al., 2019; Jain et al., 2018). Instead, if the widely used inverse
time scheduler ηt = 1/(L + µt) is chosen, we can show that only a sub-optimal convergence rate
can be obtained, especially when λj’s vary from each other.
**Proposition 2. If we set the inverse time step size ηt =** (L+1µt) _[, then we have]_


_λj (wt+1,j_ _w_ _,j)[2]_
_j=1_ _−_ _∗_

X


2E [f (wt+1) _f_ (w )] = E
_−_ _∗_


(2.3)


_L + µ_

_L + µt_




2



_L + µ_ 2 _d_ _d_ _λ[2]j_ 1 _λ[2]j_ _[σ][2]_
_≤_  _L + µt_  Xj=1 _λj(w1,j −_ _w∗,j)[2]_ + Xj=1 2λj − _µ_ _[·]_ _L + µt_ _[·][ σ][2][ +]_ (L + µt)[2] ! _._

**Remark 1. Eqn. (2.2)** _shows that if one can choose step size coordinate-wise with step size_ _ηt,j =_
1

_λj_ (t+1) _[, then SGD can achieve a convergence rate]_ _d_

E [f (wT +1) _f_ (w )] _._ (2.4)
_−_ _∗_ _≤O_ _T_
 _[·][ σ][2]_

_which matches the lower bound (Ge et al., 2019; Jain et al., 2018). In contrast, replacing L = λ1_
_and µ = λd in Proposition 2, we can obtain that the convergence rate of SGD being_

_d_

_λj_

E [f (wT +1) _f_ (w )] _σ[2]_ _._ (2.5)
_−_ _∗_ _≤O_  _T_ _λd_ _·_ 

_j=1_

X

_Since it holds that λj_ _λd, the convergence rate in Eqn._ [1] (2.4) is better than the one in Eqn. (2.5),
_especially when the eigenvalues of the Hessian ( ≥_ _H matrix) decay rapidly. In fact, the upper bound_
_in Eqn. (2.5) is tight for the inverse time decay scheduling, as proven in Ge et al. (2019)._


-----

**Main Intuition** The diagonal case H = diag(λ1, λ2, . . ., λd) provides an important intuition for
designing eigenvalue dependent learning rate scheduling. In fact, for general non-diagonal H, letting
_H = U_ ΛU _[⊤]_ be the spectral decomposition of the Hessian and setting w[′] = U _[⊤]w, then the Hessian_
becomes a diagonal matrix from perspective of updating w[′], with the variance of the stochastic
gradient being unchanged since U is a unitary matrix. This is also the core idea of Newton’s method
and many second-order methods (Huang et al., 2020). However, given our focus in this paper being
learning rate schedulers only, we move the relevant discussion of their relationship to Appendix H.

Proposition 1 and 2 imply that a good learning rate scheduler should decrease the error of each coordinate. The inverse time decay scheduler is only optimal for the coordinate related to the smallest
eigenvalue. That’s the reason why it is sub-optimal overall. Thus, we should reduce the learning
rate gradually such that we can run a optimal learning rate associated to λj to sufficiently drop the
error of j-th coordinate. Furthermore, given the total iteration T and the eigenvalue distribution of
_the Hessian, we should allocate the running time for each optimal learning rate associated to λj._

3 EIGENVALUE DEPENDENT STEP SCHEDULING

Just as discussed in Section 2, to obtain better convergence rate for SGD, we should consider Hessian’s eigenvalue distribution and schedule the learning rate based on
the distribution. In this section, we propose a novel learning rate scheduler for this task, which can be regarded as
piecewise inverse time decay (see Figure 1). The method
is very simple, we group eigenvalues according to their
value and denote si to be the number of eigenvalues lie in
the range Ri = [µ · 2[i], µ · 2[i][+1]), that is, Figure 1: Eigencurve : piecewise in
verse time decay scheduling.

_si = #λj ∈_ [µ · 2[i], µ · 2[i][+1]). (3.1)

Then, there are at most Imax = log2 κ such ranges. By the inverse time decay theory, the optimal
learning rate associated to eigenvalues in the range Ri should be

1
_ηt =_ _, with 0 = t0 < t1 < t2 <_ _< tImax = T._ (3.2)
_O_ 2[i][−][1]µ (t _ti)_ _· · ·_
 _·_ _−_ 

Our scheduling strategy is to run the optimal learning rate for eigenvalues in each Ri for a period to
sufficiently decrease the error associated to eigenvalues in _i._
_R_

To make the step size sequence {ηt}t[T]=1 [monotonely decreasing, we define the step sizes as]


1
_ηt =_ if _t_ [ti 1, ti) (3.3)

_L + µ_ _j=1_ [∆][j][2][j][−][1][ + 2][i][−][1][µ][(][t][ −] _[t][i][−][1][)]_ _∈_ _−_

[P][i][−][1]

0 = t0 < t1 < t2 < _< tImax = T, ∆i = ti_ _ti_ 1, and Imax = log2 κ. (3.4)
_· · ·_ _−_ _−_


where


To make the total error, that is the sum of error associated with _i, to be small, we should allocate_
_R_
∆i according to si 1’s. Intuitively, a large portion of eigenvalues lying in the range _i should_
_−_ _R_
allocate a large portion of iterations. Specifically, we propose to allocate ∆i as follows:

_√si_ 1
∆i = _Imax_ _−1_ _T, with si = #λj_ [µ 2[i], µ 2[i][+1]). (3.5)
_i[′]=0_ _−_ _√si′_ _·_ _∈_ _·_ _·_

In the rest of this section, we will show that the step size scheduling according to Eqn. (3.3) andP
(3.5) can achieve better convergence rate than the one in Ge et al. (2019) when si is non-uniformly
distributed. In fact, a better ∆i allocation can be calculated using numerical optimization.

3.1 THEORETICAL ANALYSIS

**Lemma 1. Let objective function f** (x) be quadratic and Assumption (1.7) hold. Running SGD for
_T_ _-steps starting from w0 and a learning rate sequence {ηt}t[T]=1_ _[defined in Eqn.][ (3.3)][, the final iterate]_


-----

_wT +1 satisfies_


E [f (wT +1) − _f_ (w∗)] ≤(f (w0) − _f_ (w∗)) · ∆[κ][2][2]1 + [15]2 _[·][ σ][2][µ]_ _Imax˜i=0−1_ _L + µ_ 2[˜]i+1ij+1=1s˜i[∆][j][2][j][−][1][ .]

X

(3.6)

[P][˜]

Since the bias term is a high order term, the variance term in Eqn. (3.6) dominates the error for
_wT +1. For simplicity, instead of using numerical methods to find the optimal_ ∆i, we propose to
_{_ _}_
use ∆i defined in Eqn. (3.5). The value of ∆i is linear to square root of the number of eigenvalues
lying in the range [µ 2[i][−][1], µ 2[i]). Using such ∆i, eigencurve has the following convergence
_·_ _·_
property.
**Theorem 1. Let objective function f** (x) be quadratic and Assumption (1.7) hold. Running SGD for
_T_ _-steps starting from w0, a learning rate sequence_ _ηt_ _t=1_ _[defined in Eqn.][ (3.3)][ and][ ∆][i]_ _[defined in]_
_{_ _}[T]_
_Eqn. (3.5), the final iterate wT +1 satisfies_

2 2
_κ[2]_ _Ii=0max−1_ _√si_ 15 _Ii=0max−1_ _√si_

_·_
E [f (wT +1) _f_ (w )] (f (w0) _f_ (w )) + _σ[2]._
_−_ _∗_ _≤_ _−_ _∗_ _·_ Ps0T [2]  P _T_  _·_

Please refer to Appendix D, F and G for the full proof of Lemma 1 and Theorem 1. The variance

term 15( _Ii=0maxT−1_ _√si)2_ _σ[2]_ in above theorem shows that when si’s vary largely from each other,
P _·_ _d_

then the variance can be close to _T_ which matches the lower bound (Ge et al., 2019). For

example, letting Imax = 100, s0 = 0 O  .99[·]d[ σ] and[2][] _si =_ [0]99[.][01] _[d][, we can obtain that]_

2
99i=0 _√si_

_σ[2]_ = (√0.99 + 99 0.01/99)[2] _[d]_

P _T_  _·_ _×_ _·_ _T_ _T_

_[·][ σ][2][ <][ 4][d]_ _[·][ σ][2][.]_

p

We can observe that if the variance of si’s is large, the
variance term in Theorem 1 can be close to dσ[2]/T .
More generally, as rigorously stated in Corollary 2,
eigencurve achieves minimax optimal convergence
rate if the Hessian spectrum satisfies an extra assumption
of “power law”: the density of eigenvalue λ is exponentially decaying with increasing value of λ in log scale, i.e.
ln(λ). This assumption comes from the observation of
estimated Hessian spectrums in practice (see Figure 2),
which will be further illustrated in Section 4.1.
**Corollary 2. Given the same setting as in Theorem 1,**
_when Hessian H’s eigenvalue distribution p(λ) satisfies_

Figure 2: Power law observed in

_“power law”, i.e._

ResNet-18 on ImageNet, both eigen
_µ_ _α_ value (x-axis) and density (y-axis) are

_p(λ) = [1]_

plotted in log scale.

_Z_ _Z_ _λ_

_[·][ exp(][−][α][(ln(][λ][)][ −]_ [ln(][µ][))) = 1] _[·]_  (3.7)

_L_
_for some α > 1, where Z =_ _µ_ [(][µ/λ][)][α][dλ][, there exists a]
_constant C(α) which only depends on α, such that the final iterate wT +1 satisfies_
R

E [f (wT +1) _f_ (w )] (f (w0) _f_ (w )) _[κ][2]_ _C(α)._
_−_ _∗_ _≤_ _−_ _∗_ _·_ _T_ [2][ +][ dσ]T [2] _·_
 

Please refer to Appendix G.3 for the proof. As for the worst-case guarantee, it is easy to check that
only when si’s are equal to each other, that is, si = d/Imax = d/ log2(κ), the variance term reaches
its maximum.
**Corollary 3. Given the same setting as in Theorem 1, when si = d/ log2(κ) for all 0** _i_
_≤_ _≤_
_Imax −_ 1, the variance term in Theorem 1 reaches its maximum and wT +1 satisfies

E [f (wT +1) _f_ (w )] (f (w0) _f_ (w )) _[κ][2][ log][2][ κ]_ + [15][d][ ·][ log][ κ] _σ[2]._
_−_ _∗_ _≤_ _−_ _∗_ _·_ _T_ [2] _T_


-----

**Remark 2. When si’s vary from each other, our eigenvalue dependent learning rate scheduler can**
_achieve faster convergence rate than eigenvalue independent scheduler such as step decay which_
_suffers from an extra log(T_ ) term (Ge et al., 2019). Only when si’s are equal to each other, Corol_lary 3 shows that the bound of variance matches to lower bound up to log κ which is same to the_
_one in Proposition 3 of Ge et al. (2019)._

Furthermore, we show that this log T gap between step decay and eigencurve certainly exists
for problem instances of skewed Hessian spectrums. For simplicity, we only discuss the case where
_H is diagonal._
**Theorem 4. Let objective function f** (x) be quadratic. We run SGD for T _-steps starting from w0 and_
_a step decay learning rate sequence_ _ηt_ _t=1_ _[defined in Algorithm 1 of Ge et al. (2019) with][ η][1]_
_{_ _}[T]_ _[≤]_
1/L. As long as (1) H is diagonal, (2) The equality in Assumption (1.7) holds, i.e. Eξ _ntn[⊤]t_ =
_σ[2]H and (3) λj (w0,j_ _w_ _,j)[2]_ = 0 for _j = 1, 2, . . ., d, the final iterate wT +1 satisfies,_
_−_ _∗_ _̸_ _∀_  

_dσ2_
E [f (wT +1) _f_ (w )] = Ω log T
_−_ _∗_ _T_ _·_
 

The proof is provided in Appendix G.4. Removing this extra log T term may not seem to be a big
deal in theory, but experimental results suggest the opposite.

4 EXPERIMENTS

To demonstrate eigencurve ’s practical value, empirical experiments are conducted on the task
of image classification [1]. Two well-known dataset are used: CIFAR-10 (Krizhevsky et al., 2009)
and ImageNet (Deng et al., 2009). For full experimental results on more datasets, please refer to
Appendix A.

4.1 HESSIAN SPECTRUM’S SKEWNESS IN PRACTICE

According to estimated[2] eigenvalue distributions of Hessian on CIFAR-10 and ImageNet, as shown
in Figure 3, it can be observed that all of them are highly skewed and share a similar tendency: A
_large portion of small eigenvalues and a tiny portion of large eigenvalues. This phenomenon has also_
been observed and explained by other researchers in the past(Sagun et al., 2017; Arjevani & Field,
2020). On top of that, when we plot both eigenvalues and density in log scale, the “power law” arises.
Therefore, if the loss surface can be approximated by quadratic objectives, then eigencurve has
already achieved optimal convergence rate for those practical settings. The exact values of the extra
constant terms are presented in Appendix A.2.

4.2 IMAGE CLASSIFICATION ON CIFAR-10 WITH EI G E N C U R V E SCHEDULING

This optimality in theory induces eigencurve ’s superior performance in practice, which is
demonstrated in Table 2 and Figure 4. The full set of figures are available in Appendix A.8. All models are trained with stochastic gradient descent (SGD), no momentum, batch size 128 and weight
decay wd = 0.0005. For full details of the experiment setup, please refer to Appendix B.

4.3 INSPIRED PRACTICAL SCHEDULES WITH SIMPLE FORMS

By simplifying the form of eigencurve and capturing some of its key properties, two simple and
practical schedules are proposed: Elastic Step Decay and Cosine-power Decay, whose empirical
performance are better than or at least comparable to cosine decay. Due to page limit, we leave all
the experimental results in Appendix A.5, A.6, A.7.

Elastic Step Decay: ηt = η0/2[k], if t ∈ (1 − _r[k])T, (1 −_ _r[k][+1])T_ _α_ (4.1)

1 _t_

 

Cosine-power Decay: ηt = ηmin + (η0 _ηmin)_ _π))_ (4.2)
_−_ 2 [(1 + cos(] _tmax_
 

[1Code: https://github.com/opensource12345678/why_cosine_works/tree/main](https://github.com/opensource12345678/why_cosine_works/tree/main)
2Please refer to Appendix B.2 for details of the estimation and preprocessing procedure.


-----

10[0]

10[0] 10[0]

10 1

10 2 10 2 10 2

10 3

10 4 10 4 10 4

Density (Log Scale) Density (Log Scale) Density (Log Scale) 10 5

10 6 10 6 10 6

10 7

0 25 50 75 100 125 150 175 0 50 100 150 200 250 0 200 400 600 800 1000 1200

Eigenvlaue Eigenvlaue Eigenvlaue


Figure 3: The estimated eigenvalue distribution of Hessian for ResNet-18 on CIFAR-10, GoogLeNet
on CIFAR-10 and ResNet-18 on ImageNet respectively. Notice that the density here is all shown
in log scale. First row: original scale for eigenvalues. Second row: log scale for preprocessed
eigenvalues.

Table 2: CIFAR-10: training losses and test accuracy of different schedules. Step Decay denotes the
scheduler proposed in Ge et al. (2019) and General Step Decay means the same type of scheduler
with searched interval numbers and decay rates. “*” before a number means at least one occurrence
of loss explosion among all 5 trial experiments.

#Epoch Schedule ResNet-18 GoogLeNet VGG16
Loss Acc(%) Loss Acc(%) Loss Acc(%)


Inverse Time Decay 1.58±0.02 79.45±1.00 2.61±0.00 86.54±0.94 2.26±0.00 84.47±0.74
Step Decay 1.82±0.04 73.77±1.48 2.59±0.02 87.04±0.48 2.42±0.45 82.98±0.27
General Step Decay 1.52±0.02 81.99±0.35 1.93±0.03 88.32±1.32 2.14±0.42 86.79±0.36
EigencurveCosine Decay 1.421.36±±0.010.01 **85.6284.23±±0.070.28** 1.941.33±±0.000.00 90.5690.65±±0.150.31 **1.872.03±±0.000.00** 87.9988.73±±0.130.11

Inverse Time Decay 0.73±0.00 90.82±0.43 0.62±0.02 92.05±0.69 1.32±0.62 *76.24±13.77
Step Decay 0.26±0.01 91.39±1.03 0.28±0.00 92.83±0.15 0.59±0.00 91.37±0.20
General Step Decay 0.17±0.00 93.97±0.21 0.13±0.00 94.18±0.18 0.20±0.00 *92.36±0.46
EigencurveCosine Decay 0.170.14±±0.000.00 94.0494.05±±0.210.18 0.120.12±±0.000.00 94.6294.75±±0.110.15 0.200.18±±0.000.00 **93.1792.88±±0.050.24**


=10

=100


Figure 4: Example: CIFAR-10 results for ResNet-18, with #Epoch = 100. Left: training losses.
Right: test accuracy. For full figures of this experiment, please refer to Appendix A.8.


-----

5 DISCUSSION

**Cosine Decay and Eigencurve** For ResNet-18 on CIFAR-10 dataset, eigencurve scheduler
presents an extremely similar learning rate curve to cosine decay, especially when the number of
training epochs is set to 100, as shown in Figure 5. This directly links cosine decay to our theory:
the empirically superior performance of cosine decay is very likely to stem from the utilization of the
“skewness” among Hessian matrix’s eigenvalues. For other situations, especially when the number
of iterations is small, as shown in Table 2, eigencurve presents a better performance than cosine
decay .

Figure 5: Eigencurve ’s learning rate curve generated by the estimated eigenvalue distribution for
ResNet-18 on CIFAR-18 after training 50/100/200 epochs. The cosine decay’s learning rate curve
(green) is also provided for comparison.

**Sensitiveness to Hessian’s Eigenvalue Distributions** One limitation of eigencurve is that it
requires a precomputed eigenvalue distribution of objective functions’s Hessian matrix, which can
be time-consuming for large models. This issue can be overcome by reusing the estimated eigenvalue distribution from similar settings. Further experiments on CIFAR-10 suggest the effectiveness of this approach. Please refer to Appendix A.3 for more details. This evidence suggests that
eigencurve’s performance is not very sensitive to estimated eigenvalue distributions.

**Relationship with Numerically Near-optimal Schedulers** In Zhang et al. (2019), a dynamic
programming algorithm was proposed to find almost optimal schedulers if the exact loss of the
quadratic objective is accessible. While it is certainly the case, eigencurve still possesses several
additional advantages over this type of approaches. First, eigencurve can be used to find simpleformed schedulers. Compared with schedulers numerically computed by dynamic programming,
eigencurve provides an analytic framework, so it is able to bypass the Hessian spectrum estimation process if some useful assumptions of the Hessian spectrum can be obtained, such as ”power
law”. Second, eigencurve has a clear theoretical convergence guarantee. Dynamic programming
can find almost optimal schedulers, but the convergence property of the computed scheduler is still
unclear. Our work fills this gap.

6 CONCLUSION

In this paper, a novel learning rate schedule named eigencurve is proposed, which utilizes the
“skewness” of objective’s Hessian matrix’s eigenvalue distribution and reaches minimax optimal
convergence rates for SGD on quadratic objectives with skewed Hessian spectrums. This condition
of skewed Hessian spectrums is observed and indeed satisfied in practical settings of image classification. Theoretically, eigencurve achieves no worse convergence guarantee than step decay for
quadratic functions and reaches minimax optimal convergence rate (up to a constant) with skewed
Hessian spectrums, e.g. under “power law”. Empirically, experimental results on CIFAR-10 show
that eigencurve significantly outperforms step decay, especially when the number of epochs is
small. The idea of eigencurve offers a possible explanation for cosine decay’s effectiveness in
practice and inspires two practical families of schedules with simple forms.


-----

ACKNOWLEDGEMENT

This work is supported by GRF 16201320. Rui Pan acknowledges support from the Hong Kong PhD
Fellowship Scheme (HKPFS). The work of Haishan Ye was supported in part by National Natural
Science Foundation of China under Grant No. 12101491.

REFERENCES

Yossi Arjevani and Michael Field. Analytic characterization of the hessian in shallow relu models:
A tale of symmetry, 2020.

Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net, 2019.

Francis Bach and Eric Moulines. Non-strongly-convex smooth stochastic approximation with convergence rate o (1/n). arXiv preprint arXiv:1306.2119, 2013.

Aleksandar Botev, Hippolyt Ritter, and David Barber. Practical Gauss-Newton optimisation for
deep learning. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
_Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research,_
[pp. 557–565. PMLR, 06–11 Aug 2017. URL https://proceedings.mlr.press/v70/](https://proceedings.mlr.press/v70/botev17a.html)
[botev17a.html.](https://proceedings.mlr.press/v70/botev17a.html)

L´eon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In J. Platt, D. Koller,
Y. Singer, and S. Roweis (eds.), Advances in Neural Information Processing Systems, volume 20. Curran Associates, Inc., 2008. [URL https://proceedings.neurips.cc/](https://proceedings.neurips.cc/paper/2007/file/0d3180d672e08b4c5312dcdafdf6ef36-Paper.pdf)
[paper/2007/file/0d3180d672e08b4c5312dcdafdf6ef36-Paper.pdf.](https://proceedings.neurips.cc/paper/2007/file/0d3180d672e08b4c5312dcdafdf6ef36-Paper.pdf)

L´eon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning, 2018.

Richard H Byrd, Samantha L Hansen, Jorge Nocedal, and Yoram Singer. A stochastic quasi-newton
method for large-scale optimization. SIAM Journal on Optimization, 26(2):1008–1031, 2016.

Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM trans_actions on intelligent systems and technology (TIST), 2(3):1–27, 2011._

Alexandre D´efossez and Francis Bach. Averaged least-mean-squares: Bias-variance trade-offs and
optimal sampling distributions. In Artificial Intelligence and Statistics, pp. 205–213. PMLR,
2015.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255. Ieee, 2009.

[Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.](http://archive.ics.uci.edu/ml)
[ics.uci.edu/ml.](http://archive.ics.uci.edu/ml)

Murat A Erdogdu and Andrea Montanari. Convergence rates of sub-sampled newton methods. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 28. Curran Asso[ciates, Inc., 2015. URL https://proceedings.neurips.cc/paper/2015/file/](https://proceedings.neurips.cc/paper/2015/file/404dcc91b2aeaa7caa47487d1483e48a-Paper.pdf)
[404dcc91b2aeaa7caa47487d1483e48a-Paper.pdf.](https://proceedings.neurips.cc/paper/2015/file/404dcc91b2aeaa7caa47487d1483e48a-Paper.pdf)

Roy Frostig, Rong Ge, Sham M Kakade, and Aaron Sidford. Competing with the empirical risk
minimizer in a single pass. In Conference on learning theory, pp. 728–763. PMLR, 2015.

Rong Ge, Sham M Kakade, Rahul Kidambi, and Praneeth Netrapalli. The step decay schedule: A
near optimal, geometrically decaying learning rate procedure for least squares. In Advances in
_Neural Information Processing Systems, pp. 14977–14988, 2019._

Jean-Louis Goffin. On convergence rates of subgradient optimization methods. Mathematical pro_gramming, 13(1):329–347, 1977._


-----

Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. A closer look at
deep learning heuristics: Learning rate restarts, warmup and distillation, 2018.

Priya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour, 2018.

Roger Grosse and James Martens. A kronecker-factored approximate fisher matrix for convolution layers. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The
_33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine_
_Learning Research, pp. 573–582, New York, New York, USA, 20–22 Jun 2016. PMLR. URL_
[https://proceedings.mlr.press/v48/grosse16.html.](https://proceedings.mlr.press/v48/grosse16.html)

Sepp Hochreiter and J¨urgen Schmidhuber. Long Short-Term Memory. Neural Computation, 9
[(8):1735–1780, 11 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https:](https://doi.org/10.1162/neco.1997.9.8.1735)
[//doi.org/10.1162/neco.1997.9.8.1735.](https://doi.org/10.1162/neco.1997.9.8.1735)

Xunpeng Huang, Xianfeng Liang, Zhengyang Liu, Lei Li, Yue Yu, and Yitan Li. Span: A stochastic
projected approximate newton method. In Proceedings of the AAAI Conference on Artificial
_Intelligence, volume 34, pp. 1520–1527, 2020._

Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and generalization in neural networks, 2020.

Prateek Jain, Sham M Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Parallelizing stochastic approximation through mini-batching and tail-averaging. _arXiv preprint_
_arXiv:1610.03774, 2016._

Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Accelerating stochastic gradient descent for least squares regression, 2018.

Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.

Xiaoyu Li, Zhenxun Zhuang, and Francesco Orabona. A second look at exponential and cosine step
sizes: Simplicity, adaptivity, and performance, 2021.

Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with warm restarts. In 5th
_International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,_
_2017, Conference Track Proceedings, 2017._

Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330, 1993. URL
[https://aclanthology.org/J93-2004.](https://aclanthology.org/J93-2004)

Gergely Neu and Lorenzo Rosasco. Iterate averaging as regularization for stochastic gradient descent. In Conference On Learning Theory, pp. 3222–3242. PMLR, 2018.

Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathemati_cal statistics, pp. 400–407, 1951._

Levent Sagun, Leon Bottou, and Yann LeCun. Eigenvalues of the hessian in deep learning: Singularity and beyond, 2017.

Nicol N Schraudolph. Fast curvature matrix-vector products for second-order gradient descent.
_Neural computation, 14(7):1723–1738, 2002._

Ohad Shamir. Is averaging needed for strongly convex stochastic gradient descent. Open problem
_presented at COLT, 2012._

Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes, 2012.


-----

Jingfeng Wu, Difan Zou, Vladimir Braverman, Quanquan Gu, and Sham M Kakade. Last iterate
risk bounds of sgd with decaying stepsize for overparameterized linear regression. arXiv preprint
_arXiv:2110.06198, 2021._

Minghan Yang, Dong Xu, Hongyu Chen, Zaiwen Wen, and Mengyun Chen. Enhance curvature
information by structured stochastic quasi-newton methods. In Proceedings of the IEEE/CVF
_Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10654–10663, June 2021._

Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael Mahoney. Pyhessian: Neural networks
through the lens of the hessian, 2020.

Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization,
2015.

Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E. Dahl,
Christopher J. Shallue, and Roger Grosse. Which algorithmic choices matter at which batch
sizes? insights from a noisy quadratic model, 2019.


-----

A MORE EXPERIMENTAL RESULTS

A.1 RIDGE REGRESSION

We compare different types of schedulings on ridge regression

_f_ (w) = [1] 2 [+][ α][||][w][||]2[2][.]

_n_ _[||][Xw][ −]_ _[Y][ ||][2]_

This experiment is only an empirical proof of our theory. In fact, the optima of ridge regression has
a closed form and can be directly computed with

_w_ = _X_ _[⊤]X + nαI_ _−1 X_ _T Y._
_∗_
  

Thus the optimal training loss f (w ) can be calculated accordingly. In all experiments, we use the
_∗_
loss gap f (w[T] ) _f_ (w ) as our performance metric.
_−_ _∗_

[Experiments are conducted on a4a datasets (Chang & Lin, 2011; Dua & Graff, 2017) (https://](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#a4a/)
[www.csie.ntu.edu.tw/˜cjlin/libsvmtools/datasets/binary.html#a4a/),](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#a4a/)
which contains 4, 781 samples and 123 features. This dataset is chosen majorly because it has a
moderate number of samples and features, which enables us to compute the exact Hessian matrix
_H = 2(X_ _[⊤]X/n + αI) and its corresponding eigenvalue distribution in acceptable time and space_
consumption.

In all of our experiments, we set α = 10[−][3]. The model is optimized via SGD without momentum,
with batch size 1, initial learning rate η0 0.1, 0.06, 0.03, 0.02, 0.01, 0.006, 0.003, 0.002, 0.001,
0.0006, 0.0003, 0.0002, 0.0001 and learning rate of last iteration ∈{ _ηmin_ 0.1, 0.01, 0.001, 0.0001,
0.00001, 0, “UNRESTRICTED”} _}. Here “UNRESTRICTED” denotes the case where ∈{_ _ηmin is not_
set, which is useful for eigencurve, who can decide the learning rate curve without setting ηmin.
Given η0 and ηmin, we adjust all schedulers as follows. For inverse time decay ηt = η0/(1 + γη0t)
and exponential decay ηt = γ[t]η0, the hyperparameter γ is computed accordingly based on η0 and
_ηmin. For cosine decay, η0 and ηmin is directly used, with no restart adopted. For eigencurve,_
the learning rate curve is linearly scaled to match the given ηmin.

In addition, for eigencurve, we use the eigenvalue distribution of the Hessian matrix, which is
directly computed via eigenvalue decomposition, as shown in Figure 6.

Figure 6: The eigenvalue distribution of Hessian for ridge regression on a4a. Left: original scale for
eigenvalues. Right: log scale for eigenvalues. Notice that the density here is shown in log scale.

All experimental results demonstrate that eigencurve can obtain similar or better training losses
when compared with other schedulers, as shown in Table 3.


-----

Table 3: Ridge regression: training loss gaps of different schedules over 5 trials.


Training loss - optimal training loss: f (w[T] ) _f_ (w )
_−_ _∗_

Schedule #Epoch = 1 #Epoch = 5

Constant 0.014963±0.001369 0.004787±0.000175
Inverse Time Decay 0.007284±0.000190 0.002098±0.000160
Exponetial Decay 0.008351±0.000360 0.000931±0.000100
Cosine Decay 0.007767±0.000006 0.001167±0.000142
Eigencurve **0.006977±0.000197** **0.000676±0.000069**

Schedule #Epoch = 25 #Epoch = 250

Constant 0.001351±0.000179 0.000122±0.000009
Inverse Time Decay 0.000637±0.000143 0.000011±0.000001
Exponetial Decay 0.000048±0.000007 **0.000000±0.000000**
Cosine Decay 0.000054±0.000005 **0.000000±0.000000**
Eigencurve **0.000045±0.000008** **0.000000±0.000000**

A.2 EXACT VALUE OF THE EXTRA TERM ON CIFAR-10 EXPERIMENTS

In Section 4.1, we have already given the qualitative evidence that shows eigencurve ’s optimality for practical settings on CIFAR-10. Here we strengthen this argument by providing the
quantitative evidence as well. The exact value of the extra term is presented in Table 4, where we
assume CIFAR-10 has batch size 128, number of epochs 200 and weight decay 5 × 10[−][4], while
ImageNet has batch size 256, number of epochs 90 and weight decay 10[−][4].

Table 4: Convergence rate of SGD with common schedulers given the estimated eigenvalue distribution of Hessian, assuming the objective is quadratic.

Value of the extra term


Convergence
rate of SGD in
quadratic
functions


CIFAR-10 CIFAR-10 CIFAR-10 ImageNet
+ ResNet18 + GoogLeNet + VGG16 + ResNet18

3.39 × 10[5] 4.92 × 10[5] 6.50 × 10[5] 6.80 × 10[6]

16.25 16.25 16.25 18.78

**8.15** **5.97** **7.12** **12.61**


Scheduler


Inverse Time _dσ[2]_
Decay Θ _T_




_· κ_


_dσ[2]_
Step Decay Θ _T_



_dσT_ _· log T_

_iI=0max_ _−1_ _√si_ 2
P _d_ 


_dσ[2]_


Eigencurve


where Imax = log2 κ,
_si = #λj ∈_ [µ · 2[i], µ · 2[i][+1])

_dσ[2]_
Minimax optimal rate Ω _T_ 1 1 1 1
 

It is worth noticing that the extra term’s value of eigencurve is independent from the number of
iterations T, since the value ([P]i[I]=0[max][−][1] _√si)2/d only depends on the Hessian spectrum. So basically_
eigencurve has already achieved the minimax optimal rate (up to a constant) for models and
datasets listed in Table 4, if the loss landscape around the optima can be approximated by quadratic
functions. For full details of the estimation process, please refer to Appendix B.

A.3 REUSING EIGENCURVE FOR DIFFERENT MODELS ON CIFAR-10

For image classification tasks on CIFAR-10, we check the performance of reusing ResNet-18’s
eigenvalue distribution for other models. As shown in Table 5, experimental results demonstrate that
Hessian’s eigenvalue distribution of Resnet-18 on CIFAR-10 can be applied to GoogLeNet/VGG16
and still achieves good peformance. Here the experiment settings are exactly the same as Section 4.2
in main paper.


-----

Table 5: CIFAR-10: training losses and test accuracy of different schedules over 5 trials. Here
all eigencurve schedules are generated based on ResNet-18’s Hessian spectrums. “*” before a
number means at least one occurrence of loss explosion among all 5 trial experiments.

#Epoch Schedule GoogLeNet VGG16
Loss Acc(%) Loss Acc(%)


Inverse Time Decay 2.61±0.00 86.54±0.94 2.26±0.00 84.47±0.74
Step Decay 2.59±0.02 87.04±0.48 2.42±0.45 82.98±0.27
General Step Decay 1.93±0.03 88.32±1.32 2.14±0.42 86.79±0.36
Cosine Decay 1.94±0.00 90.56±0.31 2.03±0.00 87.99±0.13
Eigencurve (transferred) **1.65±0.00** **91.17±0.20** **1.89±0.00** **88.17±0.32**

Inverse Time Decay 0.62±0.02 92.05±0.69 1.32±0.62 *76.24±13.77
Step Decay 0.28±0.00 92.83±0.15 0.59±0.00 91.37±0.20
General Step Decay 0.13±0.00 94.18±0.18 0.20±0.00 *92.36±0.46
Cosine Decay 0.12±0.00 94.62±0.11 0.20±0.00 **93.17±0.05**
Eigencurve (transferred) **0.11±0.00** **94.81±0.19** **0.20±0.00** **93.17±0.09**


=10

=100


A.4 COMPARISON WITH EXPONENTIAL MOVING AVERAGE ON CIFAR-10

Besides learning rate schedules, Exponential Moving Averaging (EMA) method


(1 _α)[t][−][k]wk_ _wt = αwt + (1_ _α)wt_ 1
_−_ _⇐⇒_ _−_ _−_
_k=0_

X


_wt = α_


is another competitive practical method that is commonly adopted in training neural networks with
SGD. Thus, it is natural to ask whether eigencurve can beat this method as well. The answer
is yes. In Table 6, we present additional experimental results on CIFAR-10 to compare the performance of eigencurve and exponential moving averaging. It can be observed that there is a large
performance gap between those two methods.

Table 6: CIFAR-10: training losses and test accuracy of Exponential Moving Average (EMA) and
eigencurve with #Epoch = 100 over 5 trials. For EMA, we search its constant learning rate
remain the same as Section 4.2.ηt = η0 ∈{1.0, 0.6, 0.3, 0.2, 0.1} and decay α ∈{0.9, 0.95, 0.99, 0.995, 0.999}. Other settings

Method/Schedule ResNet-18 GoogLeNet VGG16
Loss Acc(%) Loss Acc(%) Loss Acc(%)

EMA 0.30±0.01 90.09±0.82 0.33±0.01 93.42±0.26 0.49±0.00 91.87±0.82
Eigencurve **0.14±0.00** **94.05±0.18** **0.12±0.00** **94.75±0.15** **0.18±0.00** **92.88±0.24**

A.5 IMAGENET CLASSIFICATION WITH ELASTIC STEP DECAY

One key observation in CIFAR-10 experiments is the existence of “power law” shown in Figure 3.
Also, notice that in the form of eigencurve, specifically Eqn. (3.5), iteration interval length ∆i
is proportional to the square root of eigenvalue density si in range [µ _·_ 2[i], µ _·_ 2[i][+1]). Combining those
two facts together, it suggests the length of “learning rate interval” should have lengths exponentially
decreasing.

Based on this idea, Elastic Step Decay (ESD) is proposed, which has the following form,

_ηt = η0/2[k]_, if t ∈ (1 − _r[k])T, (1 −_ _r[k][+1])T_
 

Compared to general step decay with adjustable interval lengths, elastic step decay does not require
manual adjustment for the length of each interval. Instead, they are all controlled by one hyperparameter r ∈ (0, 1), which decides the “shrinking speed” of interval lengths. Experiments on


-----

CIFAR-10, CIFAR-100 and ImageNet demonstrate its superiority in practice, as shown in Table 7,
Table 8.

For experiments on CIFAR-10/CIFAR-100, we adopt the same settings as eigencurve, except
we only use common step decay with three same-length intervals + decay factor 10.

Table 7: Elastic Step Decay on CIFAR-10/CIFAR-100: test accuracy(%) of different schedules over
5 trials. “*” before a number means at least one occurrence of loss explosion among all 5 trial
experiments.

#Epoch Schedule ResNet-18 GoogLeNet VGG16
CIFAR-10 CIFAR-100 CIFAR-10 CIFAR-100 CIFAR-10 CIFAR-100


Inverse Time Decay 79.45±1.00 48.73±1.66 86.54±0.94 57.90±1.27 84.47±0.74 50.04±0.83
Step Decay 79.67±0.74 54.54±0.26 88.37±0.13 63.05±0.35 85.18±0.06 45.86±0.31
Cosine Decay 84.23±0.07 61.26±1.11 90.56±0.31 69.09±0.27 87.99±0.13 55.42±0.28
ESD **85.38±0.38** **64.17±0.57** **91.23±0.33** **70.46±0.41** **88.67±0.21** **57.23±0.39**

Inverse Time Decay 90.82±0.43 69.82±0.37 92.05±0.69 73.54±0.28 *76.24±13.77 67.70±0.49
Step Decay 93.68±0.07 73.13±0.12 94.13±0.32 76.80±0.16 92.62±0.15 70.02±0.41
Cosine Decay 94.04±0.21 74.65±0.41 94.62±0.11 78.13±0.54 93.17±0.05 72.47±0.28
ESD **94.06±0.11** **74.76±0.33** **94.65±0.11** **78.23±0.20** **93.25±0.12** **72.50±0.26**


=10

=100


For experiments on ImageNet, we use ResNet-50 trained via SGD without momentum, batch size
256 and weight decay wd = 10[−][4]. Since no momentum is used, the initial learning rate is set to
_η0 = 1.0 instead of η0 = 0.1. Two step decay baselines are adopted. “Step Decay [30-60]” is the_
common choice that decays the learning rate 10 folds at the end of epoch 30 and epoch 60. “Step
Decay [30-60-80]” is another popular choice for the ImageNet setting (Goyal et al., 2018), which
further decays learning rate 10 folds at epoch 80. For cosine decay scheduler, the hyperparameter
_ηmin is set to be 0. As for the dataset, we use the common ILSVRC 2012 dataset, which contains_
1000 classes, around 1.2M images for training and 50,000 images for validation. For all experiments,
we search r ∈{1/2, 1/√2} for ESD, with other hyperparameter search and selection process being

the same as eigencurve .

Table 8: Elastic Step Decay on ImageNet-1k: Losses and validation accuracy of different schedulings for ResNet-50 with #Epoch=90 over 3 trials.

Top-1 validation Top-5 validation
Schedule Training loss
acc(%) acc(%)


Step Decay [30-60] 1.4726±0.0057 75.55±0.13 92.63±0.08
Step Decay [30-60-80] 1.4738±0.0080 76.05±0.33 92.83±0.15
Cosine Decay 1.4697±0.0049 76.57±0.07 93.25±0.05
ESD (r = 1/√2) **1.4317±0.0027** **76.79±0.10** **93.31±0.05**


#Epoch=90


A.6 LANGUAGE MODELING WITH ELASTIC STEP DECAY

More experiments on language modeling are conducted to further demonstrate Elastic Step Decay’s
superiority over other schedulers.

For all experiments, we follow almost the same setting in Zaremba et al. (2015), where a large
regularized LSTM recurrent neural network (Hochreiter & Schmidhuber, 1997) is trained on Penn
Treebank (Marcus et al., 1993) for language modeling task. The Penn Treebank dataset has a training
set of 929k words, a validation set of 73k words and a test set of 82k words. SGD without momentum
is adopted for training, with batch size 20 and 35 unrolling steps in LSTM.

Other details are exactly the same, except for the number of training epochs. In Zaremba et al.
(2015), it uses 55 epochs to train the large regularized LSTM, which is changed to 30 epochs in our
setting, since we found that the model starts overfitting after 30 epochs. We conducted hyperparameter search for all schedules, as shown in Table 9.


-----

Table 9: Hyperparameter search for schedulers.

Scheduler Form Hyperparameter choices

_η0_ 10[0], 10[−][1], 10[−][2], 10[−][3],

Inverse Time Decay _ηt =_ 1+λη·0η0 _·t_ _∈{_ and set λ, so that _}_

_ηmin ∈{10[−][2], 10[−][3], 10[−][4], 10[−][5], 10[−][6]}_

_η0 = 1,_

General Step Decay if t ∈ηt =[k, k η0 + 1) · γ[k] ·, _K[T]_ _K ∈{3, 4,γ 5 ∈{, ⌊log[1]2 T[,][ 1] ⌋}5_ _[,]_ _,10 ⌊1_ log[}] _T ⌋_ + 1},

Cosine Decay _ηt = ηmin +_ 2[1] [(][η][0][ −] _[η][min][)]_  1 + cos   tπT  _ηη0min ∈{ ∈{10[0]10, 10[−][2][−], 10[1], 10[−][3][−], 10[2], 10[−][4][−], 0[3]}},_

_ηt = η0/2[k],_ _η0 = 1,_
Elastic Step Decay
if t ∈ (1 − _r[k])T, (1 −_ _r[k][+1])T_ _r ∈{2[−][1], 2[−][1][/][2], 2[−][1][/][3], 2[−][1][/][5], 2[−][1][/][20]},_
h 


_η10.η150_ _[k]_ for first 14 epochsfor epoch k + 14 _η0 = 1_


Baseline _ηt =_


Experimental results show that Elastic Step Decay significantly outperforms other schedulers, as
shown in Table 10.

Table 10: Scheduler performance on LSTM + Penn Treebank over 5 trials.

Scheduler Validation perplexity Test perplexity

Inverse Time Decay 114.9±1.1 112.7±1.1
General Step Decay 82.4±0.1 79.1±0.2
Baseline (Zaremba et al., 2015) 82.2 78.4
Cosine Decay 82.4±0.4 78.5±0.4
Elastic Step Decay **81.1±0.2** **77.4±0.3**


-----

A.7 IMAGE CLASSIFICATION ON IMAGENET WITH COSINE-POWER SCHEDULING

Another key observation in CIFAR-10 experiments is that eigencurve ’s learning rate curve
shape changes in a fixed tendency: more “concave” learning rate curves for less training epochs,
which inspire the cosine-power schedule in following form.

_α_

1 _t_
Cosine-power : ηt = ηmin + (η0 _ηmin)_ _π))_
_−_ 2 [(1 + cos(] _tmax_
 

Results in Table 11 show the schedulings’ performance with α = 0.5/1/2, which are denoted as
_√Cosine/Cosine/Cosine[2]_ respectively. Notice that the best scheduler gradually moves from small α

to larger α when the number of epochs increases. For #epoch=270, since the number of epochs is
large enough to make model converge, it is reasonable that the accuracy gap between all schedulers
is small.

For experiments on ImageNet, we use ResNet-18 trained via SGD without momentum, batch size
256 and weight decay wd = 10[−][4]. Since no momentum is used, the initial learning rate is set to
_η0 = 1.0 instead of η0 = 0.1. The hyperparameters ηmin is set to be 0 for all cosine-power scheduler._
As for the dataset, we use the common ILSVRC 2012 dataset, which contains 1000 classes, around
1.2M images for training and 50,000 images for validation.

Table 11: Cosine-power Decay on ImageNet: training losses and validation accuracy (%) of different
schedulings for ResNet-18 over 3 trials. Settings #Epoch≥ 90 only have 1 trial due to constraints of
resource and time.

Training Top-1 Top-5
#Epoch Schedule
loss validation acc (%) validation acc (%)

_√Cosine_ **5.4085±0.0080** **30.01±0.21** **55.26±0.33**

1 Cosine 5.4330±0.0106 26.43±0.31 50.85±0.43

Cosine[2] 5.4939±0.0157 21.81±0.21 44.53±0.09

_√Cosine_ 2.9515±0.0057 **57.27±0.15** **80.71±0.12**

5 Cosine **2.8389±0.0061** 55.67±0.08 79.46±0.16

Cosine[2] 2.9160±0.0099 52.75±0.20 77.11±0.08


_√Cosine_ 2.1739±0.0046 67.56±0.03 87.82±0.09

**Cosine** **2.0402±0.0031** **67.97±0.10** **88.12±0.03**
Cosine[2] 2.0525±0.0032 67.41±0.05 87.70±0.10

_√Cosine_ 1.9056 69.85 89.46

**Cosine** 1.7676 **70.46** **89.75**
Cosine[2] **1.7403** 70.42 89.69

_√Cosine_ 1.7178 71.37 90.31

Cosine 1.5756 **71.93** 90.33

**Cosine[2]** **1.5250** 71.69 **90.37**


30

90

270


Figure 7: Learning rate curve of three cosine-power schedulers. Top: original scale; Bottom: log
scale.


-----

A.8 FULL FIGURES FOR EIGENCURVE EXPERIMENTS IN SECTION 4.2

Please refer to Figure 8, 9, 10, 11, 12 and 13.

Figure 8: CIFAR-10 results for ResNet-18, with #Epoch = 10. Left: training losses. Right: test
accuracy.

Figure 9: CIFAR-10 results for GoogLeNet, with #Epoch = 10. Left: training losses. Right: test
accuracy.

Figure 10: CIFAR-10 results for VGG16, with #Epoch = 10. Left: training losses. Right: test
accuracy.


-----

Figure 11: CIFAR-10 results for ResNet-18, with #Epoch = 100. Left: training losses. Right: test
accuracy.

Figure 12: CIFAR-10 results for GoogLeNet, with #Epoch = 100. Left: training losses. Right: test
accuracy.

Figure 13: CIFAR-10 results for VGG16, with #Epoch = 100. Left: training losses. Right: test
accuracy.

B DETAILED EXPERIMENTAL SETTINGS FOR IMAGE CLASSIFICATION ON
CIFAR-10/CIFAR-100

B.1 BASIC SETTINGS

As mentioned in the main paper, all models are trained with stochastic gradient descent (SGD),
no momentum, batch size 128 and weight decay wd = 0.0005. Furthermore, we perform a grid
search to choose the best hyperparameters of all schedulers, with a validation set created from 5, 000


-----

samples in the training set, i.e. one-tenth of the training set. The remaining 45, 000 samples are then
used for training the model. After obtaining hyperparameters with the best validation accuracy, we
train the model again with the full training set and test the trained model on test set, where 5 trials
of experiments are conducted. The mean and standard deviation of the test results are reported.

Here the grid search explores hyperparameters η0 1.0, 0.6, 0.3, 0.2, 0.1 and ηmin
_∈_ _{_ _}_ _∈_
_{0.01, 0.001, 0.0001, 0, “UNRESTRICTED”}, where η0 denotes the initial learning rate and ηmin_
stands for the learning rate of last iteration. “UNRESTRICTED” denotes the case where ηmin is
not set, which is useful for eigencurve, who can decide the learning rate curve without setting
_ηmin. Given η0 and ηmin, we adjust all schedulers as follows. For inverse time decay, the hy-_
perparameter γ is computed accordingly based on η0 and ηmin. For cosine decay, η0 and ηmin is
directly used, with no restart adopted. For general step decay, we search the interval number in
_{3, 4, 5, ⌊log T_ _⌋, ⌊log T_ _⌋_ + 1} and decay factor in {2, 5, 10}. For step decay proposed in Ge et al.
(2019), the interval number is fixed to be ⌊log T _⌋, along with a decay factor 2. For eigencurve,_
two major modifications are made to make it more suitable for practical settings:

1/L **_η0_**
_ηt =_ 1 + _κ[1]_ _ij−=11_ [∆][j][2][j][−][1][ +][ 2][i]κ[−][1] [(][t][ −] _[t][i][−][1][)]_ = 1 + _κ[1]_ _ij−=11_ [∆][j][β][j][−][1][ +][ β][i]κ[−][1] [(][t][ −] _[t][i][−][1][)]_ _._

P P

Here we change 1/L to η0 so that it is possible to adjust the initial learning rate of eigencurve.
We also change the fixed constant 2 to a general constant β > 1, which is aimed at making the
learning rate curve smoother. The learning rate curve of eigencurve is then linearly scaled to
match the given ηmin.

Notice that the learning rate η0 can be larger than 1/L, while the loss still does not explode. There
are several explanations for this phenomenon. First, in basic non-smooth analysis of GD and SGD
with inverse time decay scheduler, the learning rate can be larger than 1/L if the gradient norm is
bounded (Shamir & Zhang, 2012). Second, deep learning has a non-convex loss landscape, especially when the parameter is far away from the optima. Hence it is common to use larger learning
rate at first. As long as the loss does not explode, it is okay. So we still include large learning rate
_η0 in our grid search process._

B.2 SETTINGS FOR EIGENCURVE

In addition, for our eigencurve scheduler, we use PyHessian (Yao et al., 2020) to generate Hessian matrix’s eigenvalue distribution for all models. The whole process consists of three phases,
which are illustrated as follows.

**1) Training the model** Almost all CNN models on CIFAR-10 have non-convex objectives, thus
the Hessian’s eigenvalue distributions are different for different parameters. Normally, we want the
this distribution to reflect the overall tendency of most parts of the training process. According to
the phenomenon demonstrated in Appendix E, figure A.11-A.17 of Yao et al. (2020), the eigenvalue
distribution of ResNet’s Hessian presents similar tendency after training 30 epochs, which suggests
that the Hessian’s eigenvalue distribution can be used after sufficient training.

In all CIFAR-10 experiments, we use the Hessian’s eigenvalue distribution of models after training 180 epochs. Since the goal here is to sufficiently train the model, not to obtain good performance, common baseline settings are adopted for training. For all models used for eigenvalue
distribution estimation, we adopt SGD with momentum = 0.9, batch size 128, weight decay
_wd = 0.0005 and initial learning rate 0.1. On top of that, we use step decay, which decays the_
learning rate by a factor of 10 at epoch 80 and 120. All of them are default settings of the PyHessian
[code (https://github.com/amirgholami/PyHessian/blob/master/training.](https://github.com/amirgholami/PyHessian/blob/master/training.py)
[py, commit: f4c3f77).](https://github.com/amirgholami/PyHessian/blob/master/training.py)

ImageNet adopts a similar setting, with training epochs being 90, SGD with momentum = 0.9,
batch size 256, weight decay wd = 0.0001, inital learning rate 0.1 and step decay schedule decays
learning rate by a factor of 10 at epoch 30 and 60.

**2) Estimating Hessian matrix’s eigenvalue distribution for the trained model** After obtaining
the checkpoint of a sufficiently trained model, we then run PyHessian to estimate the Hessian’s


-----

eigenvalue distribution for that checkpoint. The goal here is to obtain the Hessian’s eigenvalue
distribution with sufficient precision. To be more specific, the length of intervals around each estimated eigenvalue. PyHessian estimates the eigenvalue spectral density (ESD) of a model’s Hessian,
in other words, the output is a list of eigenvalue intervals, along with the density of each interval,
where the whole density adds up to 1. Precision means the interval length here.

It is natural that the estimation precision is related to the complexity of the PyHessian algorithm, e.g.
the better precision it yields, the more time and space it consumes. More specifically, the algorithm
has a time complexity of O(Nn[2]v[d][)][ and space complexity][ O][(][Bd][ +][ n][v][d][)][, where][ d][ is the number of]
model parameters, N is the number of samples used for estimating the ESD, B is the batch size and
_nv is the iteration number of Stochastic Lanczos Quadrature used in PyHessian, which controls the_
estimation precision (see Algorithm 1 of Yao et al. (2020)).

In our experiments, we use nv = 5000 for ResNet-18 and nv = 3000 for GoogLeNet/VGG16,
which gives an eigenvalue distribution estimation with precision around 10[−][5] to 10[−][4]. N and B are
both set to 200 due to GPU memory constraint, i.e. we use one mini-batch to estimate the eigenvalue
distribution. It turns out that this one-batch estimation is good enough and yields similar results to
full dataset settings shown in Yao et al. (2020).

However, space complexity is still a bottleneck here. Due to the large number of nv and space
complexity (Bd + nvd) of PyHessian, the value of d cannot be very large. In practice, with a
_O_
NVIDIA GeForce 2080 Ti GPU, which has around 11GB memory, the maximum acceptable parameter number d is around 200K − 400K. This implies that the model has to be compressed. In
our experiments, we reduce the number of channels by a factor of C for all models. For ResNet-18,
_C = 16. For GoogLeNet, C = 4. For VGG16, C = 8. Notice that those compressed models are_
only used for eigenvalue distribution estimation. In experiments of comparing different scheduling,
we still use the original model with no compression.

One may refer to [https://github.com/opensource12345678/why_cosine_](https://github.com/opensource12345678/why_cosine_works/tree/main/eigenvalue_distribution)
[works/tree/main/eigenvalue_distribution for generated eigenvalue distributions.](https://github.com/opensource12345678/why_cosine_works/tree/main/eigenvalue_distribution)

**3) Generating eigencurve scheduler with the estimated eigenvalue distribution** After obtaining the eigenvalue distribution, we do a preprocessing before plug it into our eigencurve
scheduler.

First, we notice that there are negative eigenvalues in the final distribution. Theoretically, if the
parameter is right at the optimal point, no negative eigenvalues should exist for Hessian matrix.
Thus we conjecture that those negative eigenvalues are caused by the fact that the model is closed to
optima w, but not exactly at that point. Furthermore, the estimation precision loss can be another
_∗_
cause. In fact, most of those negative eigenvalues are small, e.g. 98.6% of those negative eigenvalues
lie in [−0.1, 0), and can be generally ignored without much loss. In our case, we set them to their
absolute values.

Second, for a given weight decay value wd, we need to take the implicit L2 regularization into
account, since it affects the Hessian matrix as well. Therefore, for all eigenvalues after the first step,
we add wd to them.

After preprocessing, we plug the eigenvalue distribution into our eigencurve scheduler and generates the exact form of eigencurve.

1/L **_η0_**
_ηt =_ 1 + _κ[1]_ _ij−=11_ [∆][j][2][j][−][1][ +][ 2][i]κ[−][1] [(][t][ −] _[t][i][−][1][)]_ = 1 + _κ[1]_ _ij−=11_ [∆][j][β][j][−][1][ +][ β][i]κ[−][1] [(][t][ −] _[t][i][−][1][)]_

P P


For experiments with 100 epochs, we set β = 1.000005, so that the learning rate curve is much
smoother. For experiments with 10 epochs, we set β = 2.0. In our experiments, β serves as a fixed
constant, not hyperparameters. So no hyperparameter search is conducted on β. One can do that in
practice though, if computation resource allows.


-----

B.3 COMPUTE RESOURCE AND IMPLEMENTATION DETAILS

All the code for results in main paper can be found in [https://github.com/](https://github.com/opensource12345678/why_cosine_works/tree/main)
[opensource12345678/why_cosine_works/tree/main, which is released under the](https://github.com/opensource12345678/why_cosine_works/tree/main)
MIT license.

All experiments on CIFAR-10/CIFAR-100 are conducted on a single NVIDIA GeForce 2080
Ti GPU, where ResNet-18/GoogLeNet/VGG16 takes around 20mins/90mins/40mins to train 100
epochs, respectively. High-precision eigenvalue distribution estimation, e.g. nv 3000, requires
around 1-2 days to complete, but this is no longer necessary given the released results. ≥

The ResNet-18 model is implemented in Tensorflow 2.0. We use tensorflow-gpu 2.3.1 in our code.
The GoogLeNet and VGG16 model is implemented in Pytorch, specifically, 1.7.0+cu101.

B.4 LICENSE OF PYHESSIAN

According to [https://github.com/amirgholami/PyHessian/blob/master/](https://github.com/amirgholami/PyHessian/blob/master/LICENSE)
[LICENSE, PyHessian (Yao et al., 2020) is released under the MIT License.](https://github.com/amirgholami/PyHessian/blob/master/LICENSE)

C DETAILED EXPERIMENTAL SETTINGS FOR IMAGE CLASSIFICATION ON
IMAGENET

[One may refer to https://www.image-net.org/download for specific terms of access for](https://www.image-net.org/download)
[ImageNet. The dataset can be downloaded from https://image-net.org/challenges/](https://image-net.org/challenges/LSVRC/2012/2012-downloads.php)
[LSVRC/2012/2012-downloads.php, with training set being “Training images (Task 1 & 2)”](https://image-net.org/challenges/LSVRC/2012/2012-downloads.php)
and validation set being “Validation images (all tasks)”. Notice that registration and verification of
institute is required for successful download.

ResNet-18 experiments on ImageNet are conducted on two NVIDIA GeForce 2080 Ti GPUs with
data parallelism, while ResNet-50 experiments are conducted on 4 GPUs in a similar fashion. Both
models take around 2 days to train 90 epochs, about 20mins-30mins per epoch. Those ResNet
models on ImageNet are implemented in Pytorch, specifically, 1.7.0+cu101.

D IMPORTANT PROPOSITIONS AND LEMMAS

**Proposition 3. Letting f** (x) be a monotonically increasing function in the range [t0, _t[˜]], then it holds_
_that_

_t˜−1_ ˜t

_k=t0_ _f_ (k) ≤ Zt0 _f_ (x) dx. (D.1)

X


_If f_ (x) is monotonically decreasing in the range [t0, _t[˜]], then it holds that_

˜t _t˜−1_ _t˜_ ˜t

_f_ (x) dx _f_ (k) _f_ (k) _f_ (t0) + _f_ (x) dx. (D.2)

Zt0 _≤_ _k=t0_ _≤_ _k=t0_ _≤_ Zt0

X X

**Lemma 2. Function f** (x) = exp(−αx)x[2] _with 0 < α and x ∈_ (0, 1] is monotone decreasing in
_the range x ∈_ ( _α[2]_ _[,][ +][∞][)][ and monotone increasing in the range][ x][ ∈]_ [[0][,][ 2]α []][.]

_Proof. We can obtain the derivative of f_ (x) as

_∇f_ (x) = x exp(−αx)(2 − _αx)._

Thus, it holds that ∇f (x) ≥ 0 when x ∈ [0, _α[2]_ []][. This implies that][ f] [(][x][)][ is monotone increasing when]

_x ∈_ [0, _α[2]_ []][. Similarly, we can obtain that][ f] [(][x][)][ is monotone decreasing when][ x][ ∈] [(][ 2]α _[,][ +][∞][)][.]_

**Lemma 3. It holds that**
exp(−αx)x dx = −α[−][1](x exp(−αx) + α[−][1] exp(−αx)). (D.3)
Z


-----

_Proof._

exp(−αx)x dx = − _α[−][1]_ _x d exp(−αx) = −α[−][1](x exp(−αx) −_ exp(−αx)dx)
Z Z Z

= − _α[−][1](x exp(−αx) + α[−][1]_ exp(−αx)).

E PROOF OF SECTION 2

_Proof of Proposition 1. By iteratively applying Eqn. (2.1), we can obtain that_


_λj (wt+1,j_ _w_ _,j)[2][i][ (2.1)]_
_−_ _∗_ _≤_


_i=1(1 −_ _ηi,jλj)[2]_ _· λj(w1,j −_ _w∗,j)[2]_

Y


+ λ[2]j _[σ][2][ ·]_

_k=1_

X


(1 _ηi,jλj)[2]ηk,j[2]_
_−_
_i=k+1_

Y


= _[λ][j][(][w][1][,j][ −]_ _[w][∗][,j][)][2]_ + σ[2]

(t + 1)[2] _·_


(k + 1)[2]

(t + 1)[2][ ·]


(k + 1)[2]


_k=1_


= _[λ][j][(][w][1][,j][ −]_ _[w][∗][,j][)][2]_

(t + 1)[2]

Summing up each coordinate, we can obtain the result.


_t_

(t + 1)[2][ ·][ σ][2][.]


_Proof of Proposition 2. Let us denote σj[2]_ [=][ λ]j[2][σ][2][. By Eqn. (2.1), we can obtain that]

E _λj (wt+1,j_ _w_ _,j)[2][i]_
_−_ _∗_
h _t_

Π[t]i=1[(1][ −] _[η][i,j][λ][j][)][2][ ·][ λ][j][(][w][1][,j]_ Πi=k(1 _ηi,jλj)ηk,j[2]_ _[σ]j[2]_
_≤_ _[−]_ _[w][∗][,j][)][2][ +]_ _k=1_ _−_

X



_· λj(w1,j −_ _w∗,j)[2]_ + _k=1_ exp

X


_ηk,j[2]_ _[σ]j[2]_

!

1

_L + µi_


_≤_ exp

= exp


2 _ηi,jλj_
_−_

_i=1_

X


_−2_


_ηi,jλj_

_i=k_

X


1 1

= exp 2λj _λj(w1,j_ _w_ _,j)[2]_ + exp 2λj _ηk,j[2]_ _[σ]j[2]_

_−_ _i=1_ _L + µi_ ! _·_ _−_ _∗_ _k=1_ _−_ _i=k_ _L + µi_ !

X X X

_≤_ exp  2µλj [ln]  LL + + µt µ  _· λj(w1,j −_ _w∗,j)[2]_ + _k=1t_ exp  2µλj [ln]  _LL + + µk µt_  _·_ (L +σ µkj[2] )[2]

[2][λj]µ _t_ X 2λjµ

_L + µ_ (L + µk) _[−][2]_
=  _L + µt_  _· λj(w1,j −_ _w∗,j)[2]_ + _k=1_ (L + µt) 2λjµ  _· σj[2]_

X

[2][λj] 2λj

_L + µ_ _µ_ _σj[2]_ (L + µt) _µ_ _[−][1]_ _σj[2]_
_≤_ _L + µt_ _· λj(w1,j −_ _w∗,j)[2]_ + 2λj _µ_ 2λjµ + (L + µt)[2]
  _−_ (L + µt)

[2][λj]

= _L + µ_ _µ_ _λj(w1,j_ _w_ _,j)[2]_ + 1 1 _j_ [+] _σj[2]_

_L + µt_ _·_ _−_ _∗_ 2λj _µ_ _L + µt_ (L + µt)[2][ .]

  _−_ _[·]_ _[·][ σ][2]_

The third inequality is because function F (x) = 1/(L + µx) is monotone decreasing in the range

[1, ∞), and it holds that

_t_ _t_

1 1 _L + µt_

_._

_i=k_ _L + µi_ _[≥]_ Zi=k _L + µi [di][ = 1]µ_ [ln]  _L + µk_ 

X


2λj
_−_


-----

The last inequality is because function F (x) = (L + µx)[2][λ][j] _[/µ][−][2]_ is monotone increasing in the
range [0, ∞), and it holds that

_t_ 2λj _t_ 2λj 2λj

_k=1(L + µk)_ _µ_ _[−][2]_ _≤_ Zk=1(L + µk) _µ_ _[−][2]dk + (L + µt)_ _µ_ _[−][2]_

X


2λj

_µ_ _[−][1]_ _−_ (L + µ)


2λj

_µ_ _[−][1]_ + (L + µt)



2λj

_µ_

_[−][2]_


1

2λj _·_ (L + µt) _µ_ _[−][1]_ _−_ (
_µ_ _µ_ 
1 _[−]_ [1] 2λj

_µ_ _[−][1]_ + (L + µt)

2λj _µ_ [(][L][ +][ µt][)]
_−_


2λj

_µ_ _[−][2]._


By µ ≤ _λj, σj[2]_ [=][ λ]j[2][σ][2][, and summing up from][ i][ = 1][ to][ d][, we can obtain the result.]

F PRELIMINARIES

**Lemma 4. Let objective function f** (x) be quadratic. Running SGD for T _-steps starting from w0_
_and a learning rate sequence_ _ηt_ _t=1[, the final iterate][ w][T][ +1]_ _[satisfies]_
_{_ _}[T]_


E (wT +1 _w_ )[⊤]H(wT +1 _w_ )
_−_ _∗_ _−_ _∗_

=E (w0 _w_ )[⊤] _PT . . . P0HP0 . . . P_ _T_ (w0 _w_ )
_−_ _∗_ _·_ _·_ _−_ _∗_

_T_



+ _τ_ =0 E _ητ[2][n]τ[⊤]_ _[·][ P][T]_ _[. . . P][τ]_ [+1][HP][τ] [+1] _[. . . P][T]_ _[·][ n][τ]_ _,_

X  

_where Pt = I −_ _ηtH._

_Proof. Reformulating Eqn. (1.5), we have_


(F.1)


_wt+1 −_ _w∗_ =wt − _w∗_ _−_ _ηt(H(ξ)wt −_ _b(ξ))_
=wt _w_ _ηt(Hwt_ _b) + ηt (Hwt_ _b_ (H(ξ)wt _b(ξ)))_
_−_ _∗_ _−_ _−_ _−_ _−_ _−_
=wt _w_ _ηt(Hwt_ _b_ (Hw _b)) + ηt (Hwt_ _b_ (H(ξ)wt _b(ξ)))_
_−_ _∗_ _−_ _−_ _−_ _∗_ _−_ _−_ _−_ _−_
= (I − _ηtH) (wt −_ _w∗) + ηtnt_
=Pt(wt − _w∗) + ηtnt._

Thus, we can obtain that


_Pt . . . Pτ_ +1ητ _nτ_ _._ (F.2)
_τ_ =0

X


_wt+1_ _w_ = Pt . . . P0(w0 _w_ ) +
_−_ _∗_ _−_ _∗_


We can decompose above stochastic process associated with SGD’s update into two simpler processes as follows:

_wt[b]+1_ _[−]_ _[w][∗]_ [=][ P][t][(][w]t[b] _[−]_ _[w][∗][)][,]_ and _wt[v]+1_ _[−]_ _[w][∗]_ [=][ P][t][(][w]t[v] _[−]_ _[w][∗][) +][ η][t][n][t][,][ with][ w]0[v]_ [=][ w][∗][,] (F.3)

which entails that

_wt[b]+1_ _[. . . P][0][(][w]0[b]_ _[. . . P][t][(][w]0[b]_ (F.4)

_[−]_ _[w][∗]_ [=][ P]t[t] _[−]_ _[w][∗][) =][ P][0]t_ _[−]_ _[w][∗][)]_


_wt[v]+1_

_[−]_ _[w][∗]_ [=]


_Pτ_ +1 . . . Ptητ _nτ_ (F.5)
_τ_ =0

X


_Pt . . . Pτ_ +1ητ _nτ =_
_τ_ =0

X


(F.2)
_⇒_ _wt+1 −_ _w∗_ = _wt[b]+1_ _[−]_ _[w][∗]_ + _wt[v]+1_ _[−]_ _[w][∗]_ (F.6)

where the last equality in Eqn. (F.4) and Eqn. (F.5) is because the commutative property      _PtPt′ =_
(I − _ηtH)(I −_ _ηt′_ _H) = (I −_ _ηt′_ _H)(I −_ _ηtH) = Pt′_ _Pt holds for ∀t, t[′]._


-----

Thus, we have

E (wT +1 _w_ )[⊤]H(wT +1 _w_ )
_−_ _∗_ _−_ _∗_

(F.6)
= E (wT[b] +1 _T +1_  _T +1_ _T +1_

_[−]_ _[w][∗][)][⊤][H][(][w][b]_ _[−]_ _[w][∗][) + 2(][w][v]_ _[−]_ _[w][∗][)][⊤][H][(][w][b]_ _[−]_ _[w][∗][)]_

+( _wT[v] +1_ _T +1_

_[−]_ _[w][∗][)][⊤][H][(][w][v]_ _[−]_ _[w][∗][)]_

(F.4)
= E (w0[b] _[. . . P][0][HP][0]_ _[. . . P]_ _[T]_ [(][w]0[b] _T +1_ _[. . . P][T]_ [(][w]0[b]

_[−]_ _[w][∗][)][⊤][P][T]_ _[−]_ _[w][∗][) + 2(][w][v]_ _[−]_ _[w][∗][)][⊤][HP][0]_ _[−]_ _[w][∗][)]_

+( _wT[v] +1_ _T +1_

_[−]_ _[w][∗][)][⊤][H][(][w][v]_ _[−]_ _[w][∗][)]_

(F.5)
= E (w0[b] _[. . . P][0][HP][0]_ _[. . . P]_ _[T]_ [(][w]0[b]

_[−]_ _[w][∗][)][⊤][P][T]_ _[−]_ _[w][∗][)]_
 _T_ _⊤_

+ 2 _PT . . . Pτ_ +1ητ _nτ_ _HP0 . . . PT (w0[b]_

_τ_ =0 ! _[−]_ _[w][∗][)]_

X

_T_ _⊤_ _T_

+ _PT . . . Pτ_ +1ητ _nτ_ _H_ _PT . . . Pτ_ +1ητ _nτ_

_τ_ =0 ! _τ_ =0 ![]

X X




E[nτ ]=0
= E (w0[b] _[. . . P][0][HP][0]_ _[. . . P][T]_ [(][w]0[b]

_[−]_ _[w][∗][)][⊤][P][T]_ _[−]_ _[w][∗][)]_



_ητ_ _ητ ′_ _n[⊤]τ_ _[P][T]_ _[. . . P][τ]_ [+1] _[. . . P][T]_ _[n][τ][ ′]_
_τ_ =0,τ _[′]=0_ _·_ _[·][ H][ ·][ P][τ][ ′][+1]_

X


+ E


=E (w0[b] _[. . . P][0][HP][0]_ _[. . . P][T]_ [(][w]0[b]

_T_ _[−]_ _[w][∗][)][⊤][P][T]_ _[−]_ _[w][∗][)]_

 

+ _τ_ =0 E _ητ[2][n]τ[⊤]_ _[·][ P][T]_ _[. . . P][τ]_ [+1][HP][τ] [+1] _[. . . P][T]_ _[·][ n][τ]_ _,_

X  

where the last equality is because when τ and τ _[′]_ are different, it holds that

E[n[⊤]τ _[. . . P][τ]_ [+1][HP][τ] [+1] _[. . . P][T]_

_[·][ P][T]_ _[·][ n][τ][ ′]_ [] = 0]

due to independence between nτ and nτ ′ .

**Lemma 5. Given the assumption that Eξ** _ntn[⊤]t_ _σ[2]H, then the variance term satisfies that_
_⪯_
 


(1 _ηiλj)[2]_ _,_ (F.7)
_−_
_i=k+1_

Y


E _ητ[2][n][⊤]τ_ _[. . . P][τ]_ [+1][HP][τ] [+1] _[. . . P][T]_ _σ[2]_
_τ_ =0 _[·][ P][T]_ _[·][ n][τ]_ _≤_

X  

_where Pt = I −_ _ηtH._

1
_Proof. Denote Aτ ≜_ _PT . . . Pτ_ +1H 2, then


_λ[2]j_
_j=1_

X


_ηk[2]_
_k=0_

X


1 1 1
_A[⊤]τ_ [=] _PT . . . Pτ_ +1H 2 _⊤_ = _H_ 2 _⊤_ _Pτ[⊤]+1_ _[. . . P]T[ ⊤]_ [=][ H] 2 Pτ +1 . . . PT, (F.8)
   

1
where the second equality is entailed by the fact that H 2, Pτ +1, . . ., PT are symmetric matrices.


-----

Therefore, we have,

_T_

E _ητ[2][n][⊤]τ_ _[. . . P][τ]_ [+1][HP][τ] [+1] _[. . . P][T]_
_τ_ =0 _[·][ P][T]_ _[·][ n][τ]_

X 


_T_ _T_ _T_

E _ητ[2][n][⊤]τ_ _[A][τ]_ _[A]τ[⊤][n][τ]_ = _ητ[2][E]_ tr _n[⊤]τ_ _[A][τ]_ _[A]τ[⊤][n][τ]_ = _ητ[2][E]_ tr _A[⊤]τ_ _[n][τ]_ _[n]τ[⊤][A][τ]_
_τ_ =0 _τ_ =0 _τ_ =0

XT   X _T_     X    

_ητ[2][tr]_ E _A[⊤]τ_ _[n][τ]_ _[n]τ[⊤][A][τ]_ = _ητ[2][tr]_ _A[⊤]τ_ [E] _nτ_ _n[⊤]τ_ _Aτ_
_τ_ =0 _τ_ =0

X     X     


(F.8)


_≤σ[2]_ _·_

=σ[2] _·_


_ητ[2][tr]_ _A[⊤]τ_ _[HA][τ]_ = σ[2]

_·_
_τ_ =0

X   


_ητ[2][tr]_ _Aτ_ _A[⊤]τ_ _[H]_
_τ_ =0

X  


_ητ[2][tr (][P][T]_ _[. . . P][τ]_ [+1][HP][τ] [+1] _[. . . P][T]_ _[H][)]_
_τ_ =0

X


(1 _ηiλj)[2]_ _,_
_−_
_i=k+1_

Y


=σ[2]


_λ[2]j_
_j=1_

X


_ηk[2]_
_k=0_

X


where the third and sixth equality come from the cyclic property of trace, while the first inequality
is because of the condition Eξ _ntn[⊤]t_ _σ[2]H, where_
_⪯_

_∀x,_ _x[⊤]E[nτ_ _n[⊤]τ_ []][x][ ≤] _[σ][2][x][⊤][Hx]_

_⇒_ _∀z,_ _z[⊤]A[⊤]τ_ [E][[][n][τ] _[n]τ[⊤][]][A][τ]_ _[z][ = (][A][τ]_ _[z][)][⊤][E][[][n][τ]_ _[n]τ[⊤][](][A][τ]_ _[z][)][ ≤]_ _[σ][2][(][A][τ]_ _[z][)][⊤][H][(][A][τ]_ _[z][) =][ σ][2][z][⊤][A]τ[⊤][HA][τ]_ _[z]_

_⇒_ _A[⊤]τ_ [E][[][n][τ] _[n]τ[⊤][]][A][τ]_ _[⪯]_ _[σ][2][A]τ[⊤][HA][τ]_
_⇒_ tr _A[⊤]τ_ [E][[][n][τ] _[n]τ[⊤][]][A][τ]_ _≤_ _σ[2]tr_ _A[⊤]τ_ _[HA][τ]_ _._
     


**Lemma 6. Letting λ˜j** _[be the smallest positive eigenvalue of][ H][, then the bias term satisfies that]_

E (w0 _w_ )[⊤] _PT . . . P0HP0 . . . PT_ (w0 _w_ )
_−_ _∗_ _·_ _·_ _−_ _∗_
 


_≤(w0 −_ _w∗)[⊤]H(w0 −_ _w∗) · exp_


(F.9)


2λ˜j
_−_


_ηk_

_k=0_

X


_Proof. Letting H = U_ ΛU _[⊤]_ be the spectral decomposition of H and uj be j-th column of U, we
can obtain that

E (w0 _w_ )[⊤] _PT . . . P0HP0 . . . PT_ (w0 _w_ )
_−_ _∗_ _·_ _·_ _−_ _∗_
 


_λj_ (u[⊤]j [(][w][0]
_j=1_ _·_ _[−]_ _[w][∗][))][2][ ·]_

X


(1 _ηkλj)[2]_
_−_
_k=0_

Y


_λj_ (u[⊤]j [(][w][0]
_j=1_ _·_ _[−]_ _[w][∗][))][2][ ·][ exp]_

X


2λj
_−_


_ηk_

_k=0_

X


Since λ˜j [is the smallest positive eigenvalue of][ H][, it holds that]


_λj_ (u[⊤]j [(][w][0]
_j=1_ _·_ _[−]_ _[w][∗][))][2][ ·][ exp]_

X


_λj_ (u[⊤]j [(][w][0]

_≤_ _j=1_ _·_ _[−]_ _[w][∗][))][2][ ·][ exp]_

X

=(w0 − _w∗)[⊤]H(w0 −_ _w∗) · exp_


2λj
_−_


_ηk_

_k=0_

X


2λ˜j
_−_

2λ˜j
_−_


_ηk_

_k=0_

X

_T_

_ηk_

_k=0_

X


-----

G PROOF OF THEOREMS

**Lemma 7. Let learning rate ηt is defined in Eqn. (3.3). Assuming k ∈** [ti′−1, ti′ ] with 1 ≤ _i[′]_ _≤_ [˜]i ≤
_T_ _, the sequence {ηt}t[T]=0_ _[satisfies that]_


_t˜i+1[−][1]_

_ηt_
_t=k_ _≥_

X

_where αi is defined as_


˜i+1

_i=i[′]+1_

X


1

2[i][−][1]µ [ln][ α]αi−[i] 1

_αi ≜_ _L + µ_


1 _αi′_
+ (G.1)

2[i][′][−][1]µ [ln] _αi′−1 + 2[i][′][−][1]µ(k −_ _ti′−1)_ _[,]_

_i_

∆j2[j][−][1] = [1] _._ (G.2)
_j=1_ _ηti_

X


_Proof. First, we divide learning rates into two groups: those who are guaranteed to cover a full_
interval and those who may not.


_t˜i+1[−][1]_ _t˜i+1[−][1]_

_ηt =_ _ηt +_
_t=k_ _t=ti′_

X X


_ti′_ _−1_

_ηt =_
_t=k_

X


_ti′_ _−1_

_ηt_

_t=k_

X


˜i+1

_i=i[′]+1_

X


_ti−1_

_ηt +_
_t=Xti−1_


Furthermore, because ηt is monotonically decreasing with respect to t, by Proposition 3, we have


˜i+1 _ti_ _ti′_

_ηtdt +_ _ηtdt_

_i=i[′]+1_ Zti−1 Zk

X

˜i+1 _ti_ 1

_dt_

_i=i[′]+1_ Zti−1 _L + µ_ _j=1_ [∆][j][2][j][−][1][ + 2][i][−][1][µ][(][t][ −] _[t][i][−][1][)]_

X

_ti′_ 1
+ [P][i][−][1] _dt_
Zk _L + µ_ _j=1_ [∆][j][2][j][−][1][ + 2][i][′][−][1][µ][(][t][ −] _[t][i][′][−][1][)]_

˜i+1 1 _L + µ_ _j=1_ [∆][j][2][j][−][1][ + 2][i][−][1][µ][(][t][i][ −] _[t][i][−][1][)]_

[P][i][′][−][1]

_i=i[′]+1_ 2[i][−][1]µ [ln] _L + µ_ _j=1_ [∆][j][2][j][−][1]

X [P][i][−][1]

1 _L + µ_ _j=1_ [∆][j][2][j][−][1][ + 2][i][′][−][1][µ][(][t][i][′][ −] _[t][i][′][−][1][)]_
+ [P][i][−][1]

2[i][′][−][1]µ [ln] _L + µ_ _j=1_ [∆][j][2][j][−][1][ + 2][i][′][−][1][µ][(][k][ −] _[t][i][′][−][1][)]_

[P][i][′][−][1]

˜i+1 1 _L + µ_ _j=1_ [∆][j][2][j][−][1]

[P][i][′][−][1]

_i=i[′]+1_ 2[i][−][1]µ [ln] _L + µ_ _j=1_ [∆][j][2][j][−][1]

X [P][i]

1 _L + µ_ _j=1_ [∆][j][2][j][−][1]
+ [P][i][−][1]

2[i][′][−][1]µ [ln] _L + µ_ _j=1_ [∆][j][2][j][−][1][ + 2][i][′][−][1][µ][(][k][ −] _[t][i][′][−][1][)]_

[P][i][′]

˜i+1

1 1 _αi′_

[P]+[i][′][−][1]

_i=i[′]+1_ 2[i][−][1]µ [ln][ α]αi−[i] 1 2[i][′][−][1]µ [ln] _αi′−1 + 2[i][′][−][1]µ(k −_ _ti′−1)_ _[.]_

X


_t˜i+1[−][1]_

(D.2)
_ηt_
_≥_
_t=k_

X


(3.3)


**Lemma 8. Letting sequence {αi} be defined in Eqn. (G.2), given 1 ≤** [˜]i, it holds that

˜i+1 ˜i+1 2˜i−j+2 [] _ti_ 1

 _ααj−j_ 1 _αi[−][2]˜i−i+2_ _−_ (αi−1 + 2[i][−][1]µ(k − _ti−1))[2]˜i−i+2−2_

Xi=1 _j=Yi+1_   _k=Xti−1_

 ˜i+1 ˜i+1  2˜i−j+2 []

_≤2 ·_ 2[˜]i+11 _µ_ _·_ Xi=1 j=Yi+1  _ααj−j_ 1  αi[2]˜i−i+2−1 _−_ _αi[2]−˜i−1i+2−1_ _αi[−][2]˜i−i+2_ _._

 


(G.3)


-----

_Proof. Notice that g(k) := (αi_ 1 + 2[i][−][1]µ(k _ti_ 1))[2]˜i−i+2−2 is a monotonically increasing func_−_ _−_ _−_
tion, we have,


˜i+1 ˜i+1 2˜i−j+2 [] _ti_ 1

 _ααj−j_ 1 _αi[−][2]˜i−i+2_ _−_ (αi−1 + 2[i][−][1]µ(k − _ti−1))[2]˜i−i+2−2_

Xi=1 _j=Yi+1_   _k=Xti−1_

(D.1)≤ ˜ii+1=1 j=˜i+1i+1  _ααj−j_ 1 2˜i−j+2 [] _αi[−][2]˜i−i+2_ Z tti−i 1 (αi−1 + 2[i][−][1]µ(t − _ti−1))[2]˜i−i+2−2dt_

X Y

˜i+1  ˜i+1 2˜i−j+2 []

=  _ααj−j_ 1 _αi[−][2]˜i−i+2_ ((2˜i−i+2 − 1) · 2i−1µ)−1 _αi[2]˜i−i+2−1_ _−_ _αi[2]−˜i−1i+2−1_

_i=1_ _j=i+1_  

X Y

˜i+1  ˜i+1 2˜i−j+2 [] []

= Xi=1 j=Yi+1  _ααj−j_ 1  _αi[−][2]˜i−i+2_ (2[˜]i+1 −12i−1)µ αi[2]˜i−i+2−1 _−_ _αi[2]−˜i−1i+2−1_

˜i+1  ˜i+1 2˜i−j+2 []

_≤_ Xi=1 j=Yi+1  _ααj−j_ 1  _αi[−][2]˜i−i+2_ (2[˜]i+1 −1 2[˜]i)µ αi[2]˜i−i+2−1 _−_ _αi[2]−˜i−1i+2−1_

˜i+1  ˜i+1 2˜i−j+2 []

= Xi=1 j=Yi+1  _ααj−j_ 1  _αi[−][2]˜i−i+2_ 2[˜]i+11 _µ_ 1 −1 2[1] αi[2]˜i−i+2−1 _−_ _αi[2]−˜i−1i+2−1_

 ˜i+1 ˜i+1  2˜i−j+2 []

=2 · 2[˜]i+11 _µ_ Xi=1 j=Yi+1  _ααj−j_ 1  _αi[−][2]˜i−i+2_ αi[2]˜i−i+2−1 _−_ _αi[2]−˜i−1i+2−1_

˜i+1  ˜i+1 2˜i−j+2 []

=2 · 2[˜]i+11 _µ_ Xi=1 j=Yi+1  _ααj−j_ 1  αi[2]˜i−i+2−1 _−_ _αi[2]−˜i−1i+2−1_ _αi[−][2]˜i−i+2_ _._

 


**Lemma 9. Letting {αi} be a positive sequence, given 1 ≤** [˜]i, it holds that

˜i+1 ˜i+1 2˜i−j+2 []

_αj−1_ _αi[2]˜i−i+2−1_ _αi[2]˜i−1i+2−1_ _αi[−][2]˜i−i+2_ _α˜i[−]+1[1]_ _[.]_ (G.4)

 _αj_ _−_ _−_ _≤_

Xi=1 _j=Yi+1_    

 

_Proof. First, we have_


˜i _j+2_ []
2 _−_

_ααj−j_ 1 _αi[2]˜i−i+2−1_ _−_ _αi[2]−˜i−1i+2−1_ _αi[−][2]˜i−i+2_
   

 _ααj−j_ 1 2˜i−j+2 [] αi[−][1] _−_ _[α]αi[2]−˜ii[2]−[˜]1i−i+2i+2−1_ 

 _ααj−j_ 1 2˜i−j+2 [] αi[−][1] _−_ ˜ii+1=1 j=˜i+1i+1 ααj−j 1 2˜i−j+2 []  _αi−˜ii[2]−[˜]1i−i+2i+2−1_ 

X Y

j=˜i+1i+1  _ααj−j_ 1 2˜i−j+2 [] _αi[−][1]−_ ˜ii+1=1 j=˜i+1i+1  _ααj−j_ 1 2˜i−j[α]+2[2] []  _αi−˜ii[2]−[˜]1i−i+2i+2−1_

Y X Y

     _[α][2]_


˜i+1

_i=1_

X

˜i+1

_i=1_

X

˜i+1

_i=1_

X


˜i+1

_j=i+1_

Y

˜i+1

_j=i+1_

Y

˜i+1

_j=i+1_

Y


=α˜i[−]+1[1] [+]


_._






_i=1_


-----

˜i _j+2_ [#]

˜ij+1=i+1 _ααj−j_ 1 2 _−_ _αi[−][1]_ as follows
 
Q

˜i+1 2˜i−j+2 [] 2˜i−i+1

_ααj−j_ 1 _ααi+1i_ _αi[−][1]_ (G.5)

_j=i+2_    

Y

_j=˜i+1i+2_  _ααj−j_ 1 2˜i−j+2 [] _ααi[2]˜ii[2]−+1[˜]i−i+1i+1−1_ ! (G.6)

Y

j=˜ii+1[′′]+1  _ααj−j_ 1 2˜i−j+2 []  _αi[′′]˜ii[2]−−[′′][˜]i−i1[′′]i+2[′′]_ +2−1  (G.7)

Y

j=˜i+1i+1  _ααj−j_ 1 2˜i−j+2 [] αi[α]−˜ii[2]−[2][˜]1i−i+2i+2−1  _._ (G.8)

Y

  _[α][2]_ 


Furthermore, we reformulate the term _ii=1_

˜i ˜i+1 2˜i−j+2 [] [P][˜]˜i

 _ααj−j_ 1 _αi[−][1]_ =

_i=1_ _j=i+1_   _i=1_

X Y X

  ˜i

=

_i=1_

X

˜i+1

_i[′′]=i+1_
=

_i[′′]=2_

X

˜i+1

_i=i[′′]_
=

_i=2_

X

Combining above results, we can obtain that


˜i+1 ˜i+1 2˜i−j+2 []

 _ααj−j_ 1 _αi[2]˜i−i+2−1_ _−_ _αi[2]−˜i−1i+2−1_ _αi[−][2]˜i−i+2_

Xi=1 _j=Yi+1_    

=α˜i[−]+1[1] [+] ˜ii+1=2 j=˜i+1i+1  _ααj−j_ 1 2˜i−j+2 []  _αi−˜ii[2]−[˜]1i−i+2i+2−1_ 

X Y

_−_ ˜ii+1=1 j=˜i+1i+1  _ααj−j_ 1 2˜i−j+2 []  αi−˜ii[2]−[˜]1i−i[α]+2i[2]+2−1  

X Y

=α˜i[−]+1[1] _[−]˜ji+1=2_  _ααj−j_ 1 2˜i−j+2 [] αα[α]0[2]˜i1[2][2]+1[˜]i+1−1 ! 

Y

 

_≤α˜i[−]+1[1]_ _[.]_

**Lemma 10. Letting us denote vt+1,j ≜** [P]k[t] =0 _[η]k[2]_ _ti=k+1_ [(1][ −] _[η][i][λ][j][)][2][ with][ η][i][ defined in Eqn.][ (3.3)][,]_
_for 1 ≤_ _t ≤_ _t[′], it holds that_ Q

_vt′,j_ max(vt,j, ηt/λj). (G.9)
_≤_

_Proof. If vt+1,j_ max(vt,j, ηt/λj) holds for _t_ 1, then it naturally follows that
_≤_ _∀_ _≥_


_vt′,j ≤_ max _vt′−1,j, [η][t]λ[′][−]j_ [1]



_≤_ max _vt′−2,j, [η][t]λ[′][−]j_ [2] _, [η][t]λ[′][−]j_ [1]



_≤_ _. . ._

max _vt,j, [η][t]_ _, . . . [η][t][′][−][2]_ _, [η][t][′][−][1]_
_≤_ _λj_ _λj_ _λj_



= max _vt,j, [η][t]_

_λj_

 

where the last equality is entailed by the fact that t ≤ _t[′]_ and ηt defined in Eqn. (3.3) is monotonically
decreasing. We then prove vt+1,j max(vt,j, ηt/λj) holds for _t_ 1.
_≤_ _∀_ _≥_


-----

For ∀t ≥ 1, we have


_ηk[2]_
_k=0_

X


(1 _ηiλj)[2]_
_−_
_i=k+1_

Y


_vt+1,j =_


_t−1_

_ηk[2]_
_k=0_

X


=ηt[2] [+]


(1 _ηiλj)[2]_
_−_
_i=k+1_

Y


(G.10)


_t−1_ _t−1_
=ηt[2] [+ (1][ −] _[η][t][λ][j][)][2]_ _ηk[2]_ (1 − _ηiλj)[2]_

_k=0_ _i=k+1_

X Y


=ηt[2] [+ (1][ −] _[η][t][λ][j][)][2][v][t,j]_

1) If vt+1,j _vt,j, then it naturally follows vt+1,j_ max(vt,j, ηt/λj).
_≤_ _≤_

2) If vt+1,j > vt,j, denote a ≜ (1 _ηtλj)[2], b ≜_ _ηt[2][, we have][ v][t][+1][,j]_ [=][ av][t,j] [+][ b][, where][ a][ ∈] [[0][,][ 1)]
_−_
and b ≥ 0. It follows,

_vt+1,j > vt,j_
_⇒_ _avt,j + b > vt,j_


_vt,j <_


1 − _a_


_vt+1,j = avt,j + b < a ·_


1 _a_ [+][ b][ =]
_−_


1 − _a_


Therefore,

_vt+1,j <_


_b_ _ηt[2]_ _ηt[2]_ max _vt,j, [η][t]_

1 _a_ [=] 1 (1 _ηtλj)[2][ <]_ 1 (1 _ηtλj) [=][ η]λ[t]j_ _≤_ _λj_
_−_ _−_ _−_ _−_ _−_ 


where the second inequality is entailed by the fact that 1 _ηtλj_ [0, 1).
_−_ _∈_

**Lemma 11.v˜i+1,j** _[has the following property] Letting vt,j be defined as Lemma 10 and index_ [˜]i satisfy λj ∈ [µ · 2[˜]i, µ · 2[˜]i+1), then

_vt˜i+1[,j][ ≤]_ [15][ ·] _ηt˜i+1_ _._ (G.11)

_λj_

_Proof. By the fact that (1 −_ _x) ≤_ exp(−x), we have


_ηk[2][.]_

_ηk[2][.]_ (G.12)






_vt+1,j_
_≤_


exp

_k=0_

X


_−2_


_ηt[′]_ _λj_
_t[′]=k+1_

X


Setting t = t˜i+1

_[−]_ [1][ in above equation, we have]


_t˜i+1[−][1]_

exp

_k=0_

X


_t˜i+1[−][1]_

_ηt′_ _λj_
_t[′]=k+1_

X


2

−


_vt˜i+1[,j][ ≤]_


-----

Now we bound the variance term. First, we have


_t˜i+1[−][1]_

exp

_k=0_

X


_t˜i+1[−][1]_

_ηtλj_

_t=k+1_

X


_t˜i+1[−][1]_

exp

_k=0_

X

_t˜i+1[−][1]_

exp

_k=0_

X


_t˜i+1[−][1]_

_ηtλj_

_t=k_

X

_t˜i+1[−][1]_

_ηtλj_

_t=k_

X


2

−


_ηk[2]_ [=]






2

−


_λj_ exp(2ηkλj)ηk[2]





2λj

_λj_ exp _L_ _ηk[2]_

 

_t˜i+1[−][1]_

_ηtλj_ _ηk[2][,]_



_t=k_

X




2

−

_t_

_t˜i+1[−][1]_

exp

_k=0_

X


exp(2) exp 2 _ηtλj_ _ηk[2][,]_
_≤_ _·_ − 

_k=0_ _t=k_

X X

where the first inequality is because ηk 1/L. Hence, we can obtain 

_t˜i+1[−][1]_ _≤_ _t˜i+1[−][1]_

exp 2 _ηtλj_ _ηk[2]_

− 

_k=0_ _t=k+1_

X X

_t˜i+1[−][1]_ _t˜i+1[−][1]_

exp(2) exp 2 _ηtλj_ _ηk[2]_
_≤_ _·_ − 

_k=0_ _t=k_

X X

˜i+1 _ti−1_  _t˜i+1[−][1]_ 

= exp(2) exp 2 _ηtλj_ _ηk[2][.]_
_·_ − 

Xi=1 _k=Xti−1_ Xt=k

 i _i+1_

Furthermore, combining with Eqn. (G.1) and the condition λj [µ 2[˜], µ 2[˜] ), we can obtain
_∈_ _·_ _·_


_t˜i+1[−][1]_

_ηtλj_

_t=k_

X


_t˜i+1[−][1]_

_ηtµ_ 2
_·_
_t=k_

X


˜i+1

_i=1_

X

˜i+1

_i=1_

X

˜i+1

_i=1_

X

˜i+1

_i=1_

X

˜i+1

_i=1_

X

˜i+1

_i=1_

X


_ti−1_

exp
_k=Xti−1_

_ti−1_

exp
_k=Xti−1_


˜i+1

_i=1_

X


_ti−1_

exp
_k=Xti−1_


2

−


_ηtλj_ _ηk[2]_ exp 2 _ηtµ_ 2˜i _ηk[2]_

Xt=k  _[≤]_ Xi=1 _k=Xti−1_ − Xt=k _·_ 

˜i+1   

2˜i−j+1 ln _[α][j]_ 2 2˜i−i+1 ln _αi_
_j=i+1_ _αj−1_ _−_ _·_ _αi−1 + 2[i][−][1]µ(k −_ _ti−1)_

X


(G.1)
_≤_

=

=

=

(3.3)
=

(G.2)


Xi=1 _k=Xti−1_ exp −2 _j=Xi+1_ 2˜i−j+1 ln _α[α]j−[j]_ 1 _−_ 2 · 2˜i−i+1 ln _αi−1 + 2[i][−]α[1]µi_ (k − _ti−1)_  _ηk[2]_

˜i+1 _ti−1_ exp  ˜i+1 2˜i−j+2 ln _[α]α[j][−]j_ [1] + 2˜i−i+2 ln _[α][i][−][1][ + 2][i][−]α[1][µ]i_ [(][k][ −] _[t][i][−][1][)]_  _ηk[2]_ 
Xi=1 _k=Xti−1_ _j=Xi+1_

˜i+1 _ti−1_ ˜i+1 _αj_ 1 2˜i−j+2 [] _αi_ 1 + 2i−1µ(k _ti_ 1) 2˜i−i+2 

 _α−j_ _·_ _−_ _αi_ _−_ _−_ _ηk[2]_

Xi=1 _k=Xti−1_ _j=Yi+1_    

˜i+1 ˜i+1  _αj_ 1 2˜i−j+2 [] _ti−1_ _αi_ 1 + 2i−1µ(k _ti_ 1) 2˜i−i+2

 _α−j_ _−_ _αi_ _−_ _−_ _ηk[2]_

Xi=1 _j=Yi+1_   _k=Xti−1_  

˜i+1  ˜i+1 2˜i−j+2 []

_αj−1_

 _αj_

_i=1_ _j=i+1_  

X Y

_ti−1_ _αi_ 1 + 2i−1µ(k  _ti_ 1) 2˜i−i+2 _i−1_ _−2_

_·_ _−_ _αi_ _−_ _−_ _L + µ_ ∆j2[j][−][1] + 2[i][−][1]µ(k − _ti−1)_

_k=Xti−1_   [] Xj=1

˜i+1 ˜i+1 2˜i−j+2 []  

_αj−1_

 _αj_

_i=1_ _j=i+1_  

X Y

 


2˜i−i+2
_αi_ 1 + 2[i][−][1]µ(k _ti_ 1) _−2_
_−_ _−_ _−_
   


_αi_ 1 + 2i−1µ(k _ti_ 1)
_−_ _−_ _−_

_αi_




_ti−1_

_k=Xti−1_


-----

˜i+1 ˜i+1 2˜i−j+2 []

=  _ααj−j_ 1 _αi[−][2]˜i−i+2_

_i=1_ _j=i+1_  

X Y

_ti1_ 
_−_ (αi 1 + 2[i][−][1]µ(k _ti_ 1))[2]˜i−i+2 _αi_ 1 + 2[i][−][1]µ(k _ti_ 1) _−2_

_·_ _−_ _−_ _−_ _−_ _−_ _−_

˜i+1k=Xti−˜1i+1 2˜i−j+2 [] _t  i_ 1 

=  _ααj−j_ 1 _αi[−][2]˜i−i+2_ _−_ (αi−1 + 2[i][−][1]µ(k − _ti−1))[2]˜i−i+2−2_

Xi=1 _j=Yi+1_   _k=Xti−1_

 ˜i+1 ˜i+1  2˜i−j+2 []

(G.3)≤ 2 · 2[˜]i+11 _µ_ _·_ Xi=1 j=Yi+1  _ααj−j_ 1  αi[2]˜i−i+2−1 _−_ _αi[2]−˜i−1i+2−1_ _αi[−][2]˜i−i+2_

(G.4)≤ 2 · 2[˜]i+11 _µ_ _· α˜i[−]+1[1]_ _ηtλ˜i+1j_ _,_ 

_[≤]_ [2][ ·]


_αj−1_

_αj_




˜i+1

_i=1_

X


˜i+1

_j=i+1_

Y


where the last inequality is because of the condition λj [µ 2[˜]i, µ 2[˜]i+1) and the definition of αi.
_∈_ _·_ _·_

Therefore, we have

_vt˜i+1[,j][ ≤]_ [2 exp(2)][ ·] _ηt˜i+1_ 15 _ηt˜i+1_ _._

_λj_ _≤_ _·_ _λj_


**Lemma 12. Let objective function f** (x) be quadratic and Assumption (1.7) hold. Running SGD
_for T_ _-steps starting from w0 and a learning rate sequence {ηt}t[T]=1_ _[defined in Eqn.][ (3.3)][, the final]_
_iterate wT +1 satisfies_


(wT +1 − _w∗)[⊤]H(wT +1 −_ _w∗)_ _≤(w0 −_ _w∗)[⊤]H(w0 −_ _w∗) · exp_



_−2µ_


_ηk_

_k=0_

X


+ 15σ[2]µ _Imax−1_ 2[˜]i+1s˜i

˜i=0 _L + µ_ _ij+1=1_ [∆][j][2][j][−][1][ .]

X

_Proof. The target of this lemma is to obtain the explicit form to bound the variance term. By the[P][˜]_
definition of vt+1,j in Lemma 10, we can obtain that


_Imax−1_

˜i=0

X


+ 15σ[2]µ


_τ_ =0 E _ητ[2][n]τ[⊤]_ _[·][ P][T]_ _[. . . P][τ]_ [+1][HP][τ] [+1] _[. . . P][T]_ _[·][ n][τ]_

X 


(F.7)
_≤_ _σ[2]_

(G.9)
_≤_ _σ[2]_

(G.11)
_≤_ _σ[2]_


_λ[2]j_
_j=1_ _[·][ v][T][ +1][,j]_

X

_d_ _ηt˜i+1[+1]_

_λ[2]j_ _vt˜i+1[+1][,j][,]_
_j=1_ _[·][ max]_  _λj_

X


_d_ _ηt˜i+1[+1]_ _ηt˜i+1[+1]_

_j=1_ _λ[2]j_ _[·][ max]_ 15 · _λj_ _,_ _λj_

X


_j=1d_ _λj · ηt˜i+1_ _[≤]_ [15][σ][2][µ] _Imax˜i=0−1_ 2˜i+1s˜i _[·][ η][t]˜i+1_ _[,]_

X X


=15σ[2]


_j=1_ _λj · ηt˜i+1[+1][ ≤]_ [15][σ][2]

X


By Eqn. (3.3), we havewhere the last inequality is because λj ∈ [2[˜]iµ, 2[˜]i+1µ) and there are s˜i [such][ λ][j][’s lie in this range.]


1

(G.13)
_L + µ_ [P][˜]ij+1=1 [∆][j][2][j][−][1][ .]


_ηt˜i+1_ [=]


-----

Therefore, we have

_T_ _Imax−1_

E _ητ[2][n][⊤]τ_ _[. . . P][τ]_ [+1][HP][τ] [+1] _[. . . P][T]_ 15σ[2]µ
_τ_ =0 _[·][ P][T]_ _[·][ n][τ]_ _≤_ ˜i=0

X   X

Combining with Lemma 4 and Lemma 6, we can obtain that


2[˜]i+1s˜i (G.14)

_L + µ_ [P][˜]ij+1=1 [∆][j][2][j][−][1][ .]


(wT +1 − _w∗)[⊤]H(wT +1 −_ _w∗)_ _≤(w0 −_ _w∗)[⊤]H(w0 −_ _w∗) · exp_



_−2µ_


_ηk_

_k=0_

X


_Imax−1_

˜i=0

X


2[˜]i+1s˜i

_L + µ_ [P][˜]ij+1=1 [∆][j][2][j][−][1][ .]


+ 15σ[2]µ


**Lemma 13. For ∀t ≥** 0, the learning rate sequence {ηt}t[T]=1 _[defined in Eqn.][ (3.3)][ satisfies]_


1

(G.15)
_L + µt_


_ηt_
_≤_


_Proof. For ∀t ≥_ 0, there ∃i ≥ 1, where t ∈ [ti−1, ti). Given the form defined in Eqn. (3.3), we
have,


_ηt =_

_≤_

(3.4)


_L + µ_ _j=1_ [∆][j][2][j][−][1][ + 2][i][−][1][µ][(][t][ −] _[t][i][−][1][)]_

1

[P][i][−][1]

_L + µ_ _j=1_ [∆][j][ +][ µ][(][t][ −] _[t][i][−][1][)]_

1

[P][i][−][1]

_L + µ_ _j=1[(][t][j][ −]_ _[t][j][−][1][) +][ µ][(][t][ −]_ _[t][i][−][1][)]_

1

_L + µ([P]ti−[i][−]1 −[1]_ _t0) + µ(t −_ _ti−1)_


_L + µ(t_ _t0)_
_−_

1
=

_L + µt_

**Lemma 14. Let objective function f** (x) be quadratic and Assumption (1.7) hold. Running SGD
_for T_ _-steps starting from w0 and a learning rate sequence {ηt}t[T]=1_ _[defined in Eqn.][ (3.3)][, the final]_
_iterate wT +1 satisfies_

E (wT +1 _w_ )[⊤]H(wT +1 _w_ ) (w0 _w_ )[⊤]H(w0 _w_ ) _[κ][2]_
_−_ _∗_ _−_ _∗_ _≤_ _−_ _∗_ _−_ _∗_ _·_ ∆[2]1
 


_Imax−1_

˜i=0

X


+ 15σ[2]µ _Imax−1_ 2[˜]i+1s˜i

˜i=0 _L + µ_ _ij+1=1_ [∆][j][2][j][−][1][ .]

X

[P][˜]

_Proof. The target of this lemma is to obtain the explicit form to bound the bias term._


+ 15σ[2]µ


-----

First, by Eqn. (G.1) and the condition λj [µ 2[˜]i, µ 2[˜]i+1), we have
_∈_ _·_ _·_


_t˜i+1[−][1]_

_ηkλj_

_k=0_

X


˜i+1

_i=1_

X


1

_λj_

2[i][−][1]µ [ln][ α]αi−[i] 1


exp 2 _ηkλj_ exp 2
_≤_ −  _≤_ − 2[i]

_k=0_ _i=1_

X X

 ˜i+1  ˜i+1

exp 2˜i−i+2 ln _[α][i]_ = _αi−1_
_≤_ − _αi_ 1  _αi_

_i=1_ _−_ _i=1_ 

X Y

˜i+1  2 2 

_≤_ _i=1_  _ααi−i_ 1  =  αα˜i+11  = L[2] _· ηt[2]˜i+1_

Y


2λj
_−_


exp


_ηk_

_k=0_

X


2˜i−i+2



(G.16)

2
= _[κ][2]_

∆[2]1



(G.17)


For λj = µ, since µ ∈ [µ · 2[˜]i, µ · 2[˜]i+1) for ˜i = 0, it follows,

_T_ (G.15) _L_ 2 (3.4)

exp _−2µ_ _k=0_ _ηk!_ _≤_ _L[2]_ _· ηt[2]1_ _≤_  _L + µt1_  =

X

Combining with Lemma 12, we obtain that,


2
_L_
_≤_ _µ∆1_
 


_L + µ∆1_


(wT +1 − _w∗)[⊤]H(wT +1 −_ _w∗)_ _≤(w0 −_ _w∗)[⊤]H(w0 −_ _w∗) ·_ ∆[κ][2][2]1



_Imax−1_

˜i=0

X


2[˜]i+1s˜i

_L + µ_ [P][˜]ij+1=1 [∆][j][2][j][−][1][ .]


+ 15σ[2]µ


G.1 PROOF OF LEMMA 1

**Lemma 1. Let objective function f** (x) be quadratic and Assumption (1.7) hold. Running SGD for
_T_ _-steps starting from w0 and a learning rate sequence {ηt}t[T]=1_ _[defined in Eqn.][ (3.3)][, the final iterate]_
_wT +1 satisfies_


_Imax−1_

˜i=0

X


2[˜]i+1s˜i

_L + µ_ [P][˜]ij+1=1 [∆][j][2][j][−][1][ .]


E [f (wT +1) _f_ (w )] (f (w0) _f_ (w )) _[κ][2]_ + [15]
_−_ _∗_ _≤_ _−_ _∗_ _·_ ∆[2]1 2

_[·][ σ][2][µ]_


(3.6)


_Proof. For ∀t ≥_ 0, we have

(1.2) 1 1
_f_ (wt) − _f_ (w∗) = E 2 _[w]t[⊤][H][(][ξ][)][w][t]_ _[−]_ _[b][(][ξ][)][⊤][w][t]_ _−_ E 2 _[w]∗[⊤][H][(][ξ][)][w][∗]_ _[−]_ _[b][(][ξ][)][⊤][w][∗]_
   

1 1
= 2 _[w]t[⊤][E][[][H][(][ξ][)]][w][t]_ _[−]_ [E][[][b][(][ξ][)]][⊤][w][t] _−_ 2 _[w]∗[⊤][E][[][H][(][ξ][)]][w][∗]_ _[−]_ [E][[][b][(][ξ][)]][⊤][w][∗]
  

1 1
= 2 _[w]t[⊤][Hw][t]_ _[−]_ _[b][⊤][w][t]_ _−_ 2 _[w]∗[⊤][Hw][∗]_ _[−]_ _[b][⊤][w][∗]_
   

(1.4) 1 1
= _t_ _[Hw][t]_ _H_ _[⊤][][−][1]_ _b_ _b[⊤]H_ _[−][1]b_

2 _[w][⊤]_ _[−]_ _[b][⊤][w][t]_ _−_ 2 _[b][⊤]_ [ ] _−_

   

1 1
= _t_ _[Hw][t]_

2 _[w][⊤]_ _[−]_ _[b][⊤][w][t]_ _−_ 2 _[b][⊤][H]_ _[−][1][b][ −]_ _[b][⊤][H]_ _[−][1][b]_

   

= [1] _t_ _[Hw][t]_ [+ 1]

2 _[w][⊤]_ _[−]_ _[b][⊤][w][t]_ 2 _[b][⊤][H]_ _[−][1][b]_

= [1] _t_ _[Hw][t]_

2 _[w][⊤]_ _[−]_ 2[1] _[b][⊤][w][t][ −]_ [1]2 _[b][⊤][w][t][ + 1]2_ _[b][⊤][H]_ _[−][1][b]_


-----

= [1] _t_ _[Hw][t]_ _t_ _[b][ −]_ [1]

2 _[w][⊤]_ _[−]_ 2[1] _[w][⊤]_ 2 _[b][⊤][w][t][ + 1]2_ _[b][⊤][H]_ _[−][1][b]_

= [1] _t_ _[Hw][t]_ _t_ _[Hw][∗]_ _[−]_ [1] [+ 1]

2 _[w][⊤]_ _[−]_ 2[1] _[w][⊤]_ 2 _[w]∗[⊤][Hw][t]_ 2 _[w]∗[⊤][Hw][∗]_

= [1]

2 [(][w][t][ −] _[w][∗][)][⊤][H][(][w][t][ −]_ _[w][∗][)][,]_

where the 5th equality is entailed by the fact that H _[⊤]_ = H is a symmetric matrix, and the 9th
equality uses both H _[⊤]_ = H and Eqn 1.4.

Combine the above result with Lemma 14, we obtain that


_Imax−1_

˜i=0

X


2[˜]i+1s˜i

_L + µ_ [P][˜]ij+1=1 [∆][j][2][j][−][1][ .]


E [f (wT +1) _f_ (w )] (f (w0) _f_ (w )) _[κ][2]_ + [15]
_−_ _∗_ _≤_ _−_ _∗_ _·_ ∆[2]1 2

_[·][ σ][2][µ]_


G.2 PROOF OF THEOREM 1

**Theorem 1. Let objective function f** (x) be quadratic and Assumption (1.7) hold. Running SGD for
_T_ _-steps starting from w0, a learning rate sequence_ _ηt_ _t=1_ _[defined in Eqn.][ (3.3)][ and][ ∆][i]_ _[defined in]_
_{_ _}[T]_
_Eqn. (3.5), the final iterate wT +1 satisfies_


2 2
_κ[2]_ _Ii=0max−1_ _√si_ 15 _Ii=0max−1_ _√si_

_·_
E [f (wT +1) _f_ (w )] (f (w0) _f_ (w )) +
_−_ _∗_ _≤_ _−_ _∗_ _·_ Ps0T [2]  P _T_ 

_Proof. We have_



_· σ[2]._

_· σ[2]._


_Imax−1_

˜i=0

X


_Imax−1_

˜i=0

X


maxX˜i=0−1 _µ22[˜]i[˜]i+1∆˜is+1˜i_ (3.5)= 2 _ImaxX˜i=0−1_ _Ii=0max√s−s˜i1˜i√si_ _T_

_Imax−1_ _Imax−1_ 2 P _Ii=0max−1_ _√ ·si_ 2

_i=0_ _√si ·_ ˜i=0 _√s˜i_ [=] P _T_ 

X X


2[˜]i+1s˜i

_L + µ_ [P][˜]ij+1=1 [∆][j][2][j][−][1][ <µ][ ·]

= [2]

_T_

_[·]_


_µ ·_


Combining with Lemma 1 and the definition of ∆1, we can obtain that

2 2
_κ[2]_ _Ii=0max−1_ _√si_ 15 _Ii=0max−1_ _√si_

_·_
E [f (wT +1) _f_ (w )] (f (w0) _f_ (w )) +
_−_ _∗_ _≤_ _−_ _∗_ _·_ Ps0T [2]  P _T_ 

G.3 PROOF OF COROLLARY 2


**Corollary 2. Given the same setting as in Theorem 1, when Hessian H’s eigenvalue distribution**
_p(λ) satisfies “power law”, i.e._

_µ_ _α_

_p(λ) = [1]_ (3.7)

_Z_ _Z_ _λ_

_[·][ exp(][−][α][(ln(][λ][)][ −]_ [ln(][µ][))) = 1] _[·]_  

_L_
_for some α > 1, where Z =_ _µ_ [(][µ/λ][)][α][dλ][, there exists a constant][ C][(][α][)][ which only depends on][ α][,]
_such that the final iterate wT +1 satisfies_
R


E [f (wT +1) _f_ (w )] (f (w0) _f_ (w )) _[κ][2]_
_−_ _∗_ _≤_ _−_ _∗_ _·_ _T_ [2][ +][ dσ]T [2]




_· C(α)._


-----

_Proof. According to Theorem 1,_

E [f (wT +1) _f_ (w )]
_−_ _∗_

2 2
_κ[2]_ _Ii=0max−1_ _√si_ 15 _Ii=0max−1_ _√si_

_·_
(f (w0) _f_ (w )) + _σ[2]_
_≤_ _−_ _∗_ _·_ Ps0T [2]  P _T_  _·_

2 2
_Ii=0max−1_ _√si_ 15 _Ii=0max−1_ _√si_

=(f (w0) _f_ (w )) _[κ][2]_ + _[dσ][2]_
_−_ _∗_ _·_ _T_ [2][ ·] P _s0_  _T_ _·_ P _d_ 


2 2
The key terms here are C1 ≜ _Ii=0max−1_ _√si_ _/s0 and C2 ≜_ 15 _Ii=0max−1_ _√si_ _/d. As long as_

we can bound both terms with a constantP _C(α), the corollary will be directly proved.P_ 

1) If κ < 2, then there is only one interval with s0 = d. By setting C(α) = max(C1, C2) = 15, this
completes the proof.

2) If κ ≥ 2, then bounding C1 and C2 be done by computing the value of si under power law. For
all interval i except the last interval, we have,


(3.1) _µ·2[i][+1]_
= #λj [µ 2[i], µ 2[i][+1]) = _p(λ)dλ_
_∈_ _·_ _·_ _µ_ 2[i]
Z _·_

(3.7) _µ·2[i][+1]_ 1 _µ_ _α_ _µ·2[i][+1]_
= _dλ = [1]_ _λ[−][α]dλ_
Zµ·2 L[i] _µZ_ _α[·]_  _λ_ −1 _Z_ _µ[·][ µ]·2[α][i][ ·][+1]Zµ·2[i]_ _L_ _−1_ _µ·2[i][+1]_

= _dλ_ _µ[α]_ _λ[−][α]dλ =_ _λ[−][α]dλ_ _λ[−][α]dλ_

_µ_ _λ_ ! _·_ _·_ _µ_ 2[i] _µ_ ! _·_ _µ_ 2[i]

Z   Z _·_ Z Z _·_

_λ1−α_ _L_ _−1_ _λ1−α_ _µ·2[i][+1]_ _L_ _−1_ _µ·2[i][+1]_
= = _λ[1][−][α]_ _λ[1][−][α]_

1 _α_ _µ_ _·_ 1 _α_ _µ_ 2[i] _µ_ _·_ _µ_ 2[i]

 _−_   _−_ _·_     _·_ 

= _L[1][−][α]_ _µ[1][−][α][][−][1]_ _µ[1][−][α]_ 2[i][+1][][1][−][α] _µ[1][−][α]_ 2[i][][1][−][α][]
_−_ _·_ _·_ _−_ _·_
  2[i][+1][][1][−][α] µ[1][−][α] 2 [i][][1][−][α] 2[i][+1][][1][−][α]  2[i][][1][−][α]

= _[µ][1][−][α][ ·]_ _−_ _·_ = _−_

_L[1][−][α]_ _µ[1][−][α]_ _κ[1][−][α]_ 1

  _−_     _− _

=2[i][(1][−][α][)] [2][1][−][α][ −] [1]

_·_ _κ[1][−][α]_ 1

_−_


_si_


Therefore, we have


_si = d_ 2[i][(1][−][α][)] [2][1][−][α][ −] [1] (G.18)
_·_ _·_ _κ[1][−][α]_ 1 [=][ d][ ·][ 2]κ[1][1][−][−][α][α][ −] [1]1

_−_ _−_ _[·][ 2][i][(1][−][α][)]_


holds for all interval i except the last interval i[′] = Imax − 1 = log2 κ − 1 > 0. This last interval
may not completely covers [µ · 2[i][′] _, µ · 2[i][′][+1]) due to the boundary truncated by L, but we still have_

_si′_ _d_ [2][1][−][α][ −] [1]
_≤_ _·_ _κ[1][−][α]_ 1

_−_ _[·][ 2][i][′][(1][−][α][)]_


-----

It follows,

2

_Imax−1_

_√si_

_i=0_ !

X

Thus,


2 2

_Imax−1_ _Imax−1_

2[i][(1][−][α][)] = d [2][1][−][α][ −] [1] 2[i][(1][−][α][)][/][2]

Xi=0 p ! _·_ _κ[1][−][α]_ _−_ 1 _[·]_ Xi=0 !

_Imax_ 1 _i(α_ 1)/2[!][2]
_−_ 1 _−_

2

_i=0_  

X

_∞_ 1 _i(α−1)/2[!][2]_ _∞_ 1 _i[!][2]_

= d [2][1][−][α][ −] [1]

2 _·_ _κ[1][−][α]_ 1 2[(][α][−][1)][/][2]

_i=0_   _−_ _[·]_ _i=0_  

X 2 X 2

1 1

1 − 2[(][α][−]1[1)][/][2] ! = d · _κ[2][1][1][−][−][α][α][ −]−_ [1]1 _[·]_  1 − 2[(1][−][α][)][/][2]  _._


_d_ [2][1][−][α][ −] [1]
_≤_ _·_ _κ[1][−][α]_ 1

_−_ _[·]_

=d [2][1][−][α][ −] [1]
_·_ _κ[1][−][α]_ 1

_−_ _[·]_

_d_ [2][1][−][α][ −] [1]
_≤_ _·_ _κ[1][−][α]_ 1

_−_ _[·]_

=d [2][1][−][α][ −] [1]
_·_ _κ[1][−][α]_ 1

_−_ _[·]_


2 2
_Ii=0max−1_ _√si_ _d_ _κ[2][1][1][−][−][α][α][−][1]1_ 1 2[(1]1[−][α][)][/][2] 1

_C1 =_ _·_ _−_ _[·]_ _−_ =

P _s0_  _≤2_ _d ·_ _κ[2][1][1][−][−][α][α][−]−[1]1_ _[·][ 2][0(1][−][α][)]_   1 − 2[(1][−][α][)][/][2]

15 _Ii=0max−1_ _√si_ 1 2
_C2 =_
P _d_  _≤_ [15]d _κ[1][−][α]_ 1 1 2[(1][−][α][)][/][2]

_[·][ d][ ·][ 2][1][−][α][ −]−_ [1] _[·]_  _−_ 

=15 · 1[1] −[ −]   1 1κ2 αα−−11 _[·]_  1 − 2[(1]1[−][α][)][/][2] 2

2

  1

15 _._
_≤_ _·_ 1 2[(1][−][α][)][/][2]
 _−_ 


2



Here the last inequality for C2 is entailed by κ ≥ 2 and α > 1.

2

By setting C(α) = max(C1, C2) = 15 1 2[(1]1[−][α][)][/][2], we obtain
_·_ _−_
 


E [f (wT +1) _f_ (w )]
_−_ _∗_

2
_Ii=0max−1_ _√si_

(f (w0) _f_ (w )) _[κ][2]_ + _[dσ][2]_
_≤_ _−_ _∗_ _·_ _T_ [2][ ·] P _s0_  _T_

=(f (w0) − _f_ (w∗)) · _T[κ][2][2][ ·][ C][1][ +][ dσ]T_ [2] _· C2_

_≤_ (f (w0) − _f_ (w∗)) · _T[κ][2][2][ +][ dσ]T_ [2] _· C(α)._
 


2
15 _Ii=0max−1_ _√si_
P _d_ 


G.4 PROOF OF THEOREM 4

**Theorem 4. Let objective function f** (x) be quadratic. We run SGD for T _-steps starting from w0 and_
_a step decay learning rate sequence_ _ηt_ _t=1_ _[defined in Algorithm 1 of Ge et al. (2019) with][ η][1]_
_{_ _}[T]_ _[≤]_
1/L. As long as (1) H is diagonal, (2) The equality in Assumption (1.7) holds, i.e. Eξ _ntn[⊤]t_ =
_σ[2]H and (3) λj (w0,j_ _w_ _,j)[2]_ = 0 for _j = 1, 2, . . ., d, the final iterate wT +1 satisfies,_
_−_ _∗_ _̸_ _∀_  

_dσ2_
E [f (wT +1) _f_ (w )] = Ω log T
_−_ _∗_ _T_ _·_
 


-----

_Proof. The lower bound here is an asymptotic bound. Specifically, we require_


_σ[2]_

2[16], 16, [1]

256 minj λj (w0,j _w_ _,j)[2]_

_[·]_ _−_ _∗_


(G.19)


log T

_[≥]_ [max]


In Ge et al. (2019), step decay has following learning rate sequence:


_T_ _T_

_ηt =_ _[η][1]_ if t 1 + _,_ (G.20)

2[ℓ] _∈_ log T log T
 _[·][ ℓ,]_ _[·][ (][ℓ]_ [+ 1)]

where ℓ = 0, 1, . . ., log T − 1. Notice that the index start from t = 1 instead of t = 0. For
consistency with our framework, we set η0 = 0, which produces the exact same step decay scheduler
while only adding one extra iteration, thus does not affect the overall asymptotic bound.

We first translate the general notations to diagonal cases so that the idea of the proof can be clearer.

Since f (x) is quadratic, according to the proof of Lemma 1 in Appendix G.1,


_f_ (wT +1) − _f_ (w∗) = [1]2 [(][w][T][ +1][ −] _[w][∗][)][⊤][H][(][w][T][ +1][ −]_ _[w][∗][)][.]_

Furthermore, according to Lemma 4, where Pt = I − _ηtH,_

E (wT +1 _w_ )[⊤]H(wT +1 _w_ )
_−_ _∗_ _−_ _∗_

=E (w0 _w_ )[⊤] _PT . . . P0HP0 . . . P_ _T_ (w0 _w_ )
_−_ _∗_ _·_ _·_ _−_ _∗_
 


_τ_ =0 E _ητ[2][n]τ[⊤]_ _[·][ P][T]_ _[. . . P][τ]_ [+1][HP][τ] [+1] _[. . . P][T]_ _[·][ n][τ]_

X 


(1 _ηkλj)[2]_ +
_−_
_k=0_

Y

_T_

(1 _ηkλj)[2]_ +
_−_
_k=0_

Y


_λj (w0,j_ _w_ _,j)[2]_
_j=1_ _−_ _∗_

X

_d_

_λj (w0,j_ _w_ _,j)[2]_
_j=1_ _−_ _∗_

X

_d_

_λj (w0,j_ _w_ _,j)[2]_
_j=1_ _−_ _∗_

X


_ητ[2]_
_τ_ =0 _j=1_

X X

_T_ _d_

_ητ[2]_
_τ_ =0 _j=1_

X X


_λj(1_ _ηkλj)[2]E_ _n[2]τ,j_
_−_
_k=τ_ +1

Y 

_T_

_λj(1_ _ηkλj)[2]_ _λjσ[2]_
_−_ _·_
_k=τ_ +1

Y


(1 _ηkλj)[2]_ + σ[2]
_−_
_k=0_

Y


_λ[2]j_
_j=1_

X


_ητ[2]_ (1 _ηkλj)[2]._

_−_

_τ_ =0 _k=τ_ +1

X Y


Here the second equality is entailed by the fact that H and Pt are diagonal, and the third equality
comes from Eξ _nTtn[⊤]t_ = σ[2]H. Thus, by denoting bj ≜ _λj (w0,j −_ _w∗,j)[2][ Q][T]k=0[(1][ −]_ _[η][k][λ][j][)][2][ and]_
_vj ≜_ [P]τ[T]=0 _[η]τ[2]_  _k=τ_ +1 [(1][ −] _[η][k][λ][j][)][2][, we have,]_
Q

E [f (wT +1 _f_ (w )] = [1] (wT +1 _w_ )[⊤]H(wT +1 _w_ )
_−_ _∗_ 2 [E] _−_ _∗_ _−_ _∗_

 _d_ _d_  (G.21)

= [1] _bj_ + _σ[2]_ _λ[2]j_ _[v][j]_ _._

2    

_j=1_ _j=1_

X X

   

To proceed the analysis, we divide all eigenvalues _λj_ into two groups:
_{_ _}_

= _j_ _,_ = _j_ _,_ (G.22)
_A_ 8η1T _B_ 8η1T
   

_Bwhere group are those small eigenvalues that the bias term A are those large eigenvalues that the variance term[λ][j][ >][ log][ T]_ _bj will finally dominate. Rigorously speaking,[λ][j][ ≤] vj[log] will finally dominate, and group[ T]_

**a) For ∀j ∈A:**

Step decay’s bottleneck in variance term actually occurs at the first interval ℓ that satisfies

8T
2[ℓ] _λjη1_ (G.23)
_≥_ _·_ log T


-----

We first show that interval ℓ is well-defined for any dimension j ∈A. Since j ∈A, it follows from
the definition of A in Eqn. (G.22),

8T

_λj >_ [log][ T] = _λjη1_

8η1T _⇒_ _·_ log T [>][ 1 = 2][0]

On the other hand, since we assume T/ log T ≥ 2[16] in Eqn. (G.19), which implies T ≥ 2[16] _⇒_
log T ≥ 16, it follows

8T
_λjη1_
_·_ log T _[≤]_ _[λ][j][η][1][ ·][ T]2_ _[≤]_ _[λ]L[j]_ _[·][ T]2_ _[≤]_ _[T]2 [= 2][log][ T][ −][1][,]_

where the second inequality comes from η1 1/L in assumption (1), and the third inequality is
entailed by λj _L given the definition of L in Eqn. (1.6). ≤_
_≤_

As a result, we have


8T
_λjη1_ 2[0], 2[log][ T][ −][1][]
_·_ log T _[∈]_

 


thus


8T
2[ℓ] _λjη1_
_≥_ _·_ log T

will guaranteed be satisified for some interval ℓ = 1, . . ., log T − 1. Since interval ℓ is the first
interval satisifies Eqn. (G.23), we also have


8T
2[ℓ][−][1] _< λjη1_ = 2[ℓ] _< λjη1_ [16][T] (G.24)
_·_ log T _⇒_ _·_ log T

Back to our analysis for the lower bound, by focusing on the variance produced by interval ℓ only,
we have,


(ℓ+1)· logT T

_ητ[2]_

_τ_ =ℓ·XlogT T [+1]


_ητ[2]_ (1 _ηkλj)[2]_
_τX=0_ _k=Yτ_ +1 _−_ _≥_ _τ_ =ℓ·XlogT T [+1]

(ℓ+1)· logT T _T_

_ητ[2]_ (1 _ηkλj)[2]_ =

_τ_ =ℓ·XlogT T [+1] _k=ℓ·YlogT T_ [+1] _−_


(1 _ηkλj)[2]_
_−_
_k=τ_ +1

Y


_vj =_

_≥_

=

(G.24)


(ℓ+1)· logT T


_η1_

2[ℓ]




2



(1 _ηkλj)[2]_

_T_ _−_

Ylog T [+1]


_τ_ =ℓ·


_T_

log T [+1]


_k=ℓ·_


_T_ _η1_

log T 2[ℓ]

_[·]_ 


2

_k=ℓ_

 _·_

_η1_


_η1_

(1 _ηkλj)[2]_

2[ℓ]  _k=ℓ·YlogT T_ [+1] _−_

2 _T_

_η1_

_λjη1 ·_ log[16][T] T ! _k=ℓ·YlogT T_ [+1](1 − _ηkλj)[2]_


log T

_[·]_


= [1]

256 _T_

_[·][ log][ T]_

_≥_ 256[1] _T_

_[·][ log][ T]_



[1]

_·_ _λ[2]j_

[1]

_·_ _λ[2]j_


(1 _ηkλj)[2]_

_T_ _−_

Ylog T [+1]


_k=ℓ·_


1

256 _T_

_[·][ log][ T]_



[1]

_·_ _λ[2]j_


1 −




1 2λj
_−_




2ηkλj

_k=ℓ·XlogT T_ [+1]


=

T

log T


_ηk_

_T_

Xlog T [+1]


_k=ℓ·_


(i+1)· logT T

_ηk_

_k=i·XlogT T_ [+1]


log T −1

_i=ℓ_

X


= [1]

256 _T_

_[·][ log][ T]_



[1]

_·_ _λ[2]j_


1 2λj
_−_




-----

(i+1)· logT T


log T −1

1 2λj



_−_

_i=ℓ_

X


 log T 1

_−_

1 2λj
_−_

_i=ℓ_

X


= [1]

256 _T_

_[·][ log][ T]_

= [1]

256 _T_

_[·][ log][ T]_

_≥_ 256[1] _T_

_[·][ log][ T]_

(G.23) 1
_≥_ 256 _[·][ log]T[ T]_

= [1]

256 _T_

_[·][ log][ T]_

= [1]

512 _T_

_[·][ log][ T]_



[1]

_·_ _λ[2]j_

[1]

_·_ _λ[2]j_


_η1_

2[i]


_k=i·_ logT T [+1]


_T_

log T 2[i]

_[·][ η][1]_



[1] 1 2λj

_·_ _λ[2]j_ _·_ _−_ _·_




_T_ _η1_

log T 2[ℓ][−][1]

_[·]_ 

_T_ _η1_

log T _λjη1_ log8T T

_[·]_ _·_



[1]

_·_ _λ[2]j_ _·_

[1] [1]

_·_ _λ[2]j_ _·_ 2


1 4λj
_−_ _·_



[1]

_·_ _λ[2]j_


Here the first inequality is obtained by focusing variance generated in interval ℓ only. The second
inequality utilizes τ _ℓ_ _T/ log T_ . The fourth inequality is entailed by (1 _a1)(1_ _a2) =_
_≥_ _·_ _−_ _−_
1extend this inequality for more terms − _a1 −_ _a2 + a1a2 ≥_ 1 − _a1 −_ _a2 for ∀i=1a1[(1], a[ −]2 ∈[a][i][0[)][ ≥], 1][1], where by mathematical induction, we can[ −]_ [P][n]i=1 _[a][i][ as long as][ P]i[n]=1_ _[a][i][ ≤]_ [1][. The]
fifth inequality comes from _i=ℓ_ 1/2[i] _≤_ [P]i[∞]=ℓ [1][/][2][i][ = 1][/][2][ℓ][−][1][.]

[Q][n]

**b) For ∀j ∈B:** [P][log][ T][ −][1]

Step decay’s bottleneck will occur in the bias term. Since j ∈B, it follows from the definition of B
in Eqn.(G.22),


_λj_ = _η1λj_
_≤_ [log]8η1[ T]T _⇒_ _≤_ [log]8T [ T] _[,]_

_T_

(1 _ηkλj)[2]_
_−_
_k=0_

Y


we have

_bj =λj (w0,j_ _w_ _,j)[2]_
_−_ _∗_


_λj (w0,j_ _w_ _,j)[2]_
_≥_ _−_ _∗_ _·_

=λj (w0,j _w_ _,j)[2]_
_−_ _∗_ _·_

=λj (w0,j _w_ _,j)[2]_
_−_ _∗_ _·_

=λj (w0,j _w_ _,j)[2]_
_−_ _∗_ _·_


= λj (w0,j _w_ _,j)[2]_
_−_ _∗_ _·_


1 −

1



_−_



1



_−_




2ηkλj

_k=0_

X


1 −


2ηkλj

_k=1_

X


(i+1)· logT T

2ηkλj

_k=i·XlogT T_ [+1]


log T −1

_i=0_

X

log T −1

_i=0_

X


(i+1)· logT T


_η1λj_
2[i][−][1]


_k=i·_ logT T [+1]


log T −1

_i=0_

X


1 _η1λj_
_−_


log T

_[·]_


2[i][−][1]


_λj (w0,j_ _w_ _,j)[2]_ 1 4η1λj
_≥_ _−_ _∗_ _·_ _−_ _·_



log T


_λj (w0,j_ _w_ _,j)[2]_ 1 4 [log][ T]
_≥_ _−_ _∗_ _·_ _−_ _·_ 8T



log T


=λj (w0,j _w_ _,j)[2]_ [1]
_−_ _∗_ _·_ 2 _[,]_

forwhere the first inequality is caused by ∀a1, a2 ∈ [0, 1] and applying mathematical induction for (1 − _a1)(1 −_ _a2) = 1 − {aa1n −} to obtaina2 + a1a2 ≥i=11[(1] −[ −]a1[a] −[i][)][ ≥]a2_

[Q][n]


-----

1 − [P]i[n]=1 _[a][i][ as long as][ P]i[n]=1_ _[a][i][ ≤]_ [1][.] The second equality is because η0 = 0. The second inequality comes from _i=0_ 1/2[i][−][1] _≤_ [P]i[∞]=0 [1][/][2][i][−][1][ = 4][. The last inequality follows]
_η1λj_ log T/(8T ).
_≤_

[P][log][ T][ −][1]

From assumption (3), we know λj (w0,j _w_ _,j)[2]_ _> 0. Furthermore, as we require_
_−_ _∗_

_T_ 1 _σ[2]_

log T 256 minj λj (w0,j _w_ _,j)[2]_

_[≥]_ _[·]_ _−_ _∗_

in Eqn. (G.19),


1

512 _T_

_[·][ σ][2]_ _[·][ log][ T.]_


_bj_ _λj (w0,j_ _w_ _,j)[2]_ [1] _λj (w0,j_ _w_ _,j)[2]_ [1]
_≥_ _−_ _∗_ _·_ 2 _[≥]_ [min]j _−_ _∗_ _·_ 2 _[≥]_

In sum, we have obtained


1 [1]

512 _T_ _·_ _λ[2]j_

_[·][ log][ T]_

1

512 _T_

_[·][ σ][2]_ _[·][ log][ T]_


_j_ _,_ _vj_
_∀_ _∈A_ _≥_

_j_ _,_ _bj_
_∀_ _∈B_ _≥_

By combining with Eqn. (G.21), we have


E [f (wT +1 _f_ (w )] = [1]
_−_ _∗_ 2

_≥_ 2[1]



[1] _bj_ + _σ[2]_ _λ[2]j_ _[v][j]_

2    

_j=1_ _j=1_

X X

   

_bj_ + _σ[2][ X]_ _λ[2]j_ _[v][j]_

_≥_ 2[1]    

_j∈B_ _j∈A_

[X]1    1

_≥|B| ·_  1024 _[·][ σ]T[2]_ _[·][ log][ T]_  + _jX∈A_ _σ[2]_ _· λ[2]j_ _[·]_ 1024 _[·][ log]T[ T]_

1 1
= +
_|B| ·_ 1024 _T_ _|A| ·_ 1024 _T_
 _[·][ σ][2]_ _[·][ log][ T]_   _[·][ σ][2]_ _[·][ log][ T]_

1
= ( + )
_|A|_ _|B|_ _·_ 1024 _T_
 _[·][ σ][2]_ _[·][ log][ T]_ 

1
=d
_·_ 1024dσ2 _[·][ σ]T[2]_ _[·][ log][ T]_

=Ω log T _,_

_T_ _·_

 


+







[1]

_·_ _λ[2]j_


where the first inequality is because both the bias and variance terms are non-negative, given bj =
_λj (w0,j −_ _w∗,j)[2][ Q][T]k=0[(1][ −]_ _[η][k][λ][j][)][2][ ≥]_ [0][ and][ v][j][ =][ P][T]τ =0 _[η]τ[2]_ _Tk=τ_ +1[(1][ −] _[η][k][λ][j][)][2][ ≥]_ [0][.]
Q

**Remark 3. The requirement T/ log T** 1/256 _σ[2]/_ minj λj (w0,j _w_ _,j)[2][]_ _and assumption_
_≥_ _·_ _−_ _∗_

_λj (w0,j_ _w_ _,j)[2]_ = 0 for _j = 1, 2, . . ., d can be replaced with_ _T/ log T > 1/(8η1µ), since in_
_that case − j ∈A∗_ _holds for̸_ _∀ ∀j = 1, 2, . . ., d and B = ∅. In particular, if η1 = 1/L, this requirement_
_on T becomes T/ log T ≥_ _κ/8._

G.5 THE REASON OF USING ASSUMPTION (1.7)

In all of our analysis, we employ assumption (1.7)

Eξ _ntn[⊤]t_ _σ[2]H_ where nt = Hwt _b_ (H(ξ)wt _b(ξ))_
_⪯_ _−_ _−_ _−_
 


-----

which is the same as the one in Appendix C, Theorem 13 of Ge et al. (2019). This key theorem is
the major difference between our work and Ge et al. (2019), which directly entails its main theorem
by instantiating σ with specific values in its assumptions.

On the other hand, it is possible to use the assumptions in Ge et al. (2019); Bach & Moulines (2013);
Jain et al. (2016) instead of our assumption (1.7) for least square regression:

min where f (w) ≜ [1] (y _w[⊤]x)[2][]_ (G.25)
_w_ _[f]_ [(][w][)] 2 [E][(][x,y][)][∼D] _−_

_y = w∗[⊤][x][ +][ ϵ][ with][ ϵ][ satisfying][ E](x,y)∼D_ _ϵ[2]xx[⊤][]_ _⪯_ _σ[2]H for ∀(x, y) ∼D_ (G.26)

E _x_ _xx[⊤][]_ _R[2]H_  (G.27)
_||_ _||[2]_ _⪯_


By combining our Lemma 1 and assumption (1.7) with Lemma 5, Lemma 8 and Lemma 9 in Ge
et al. (2019), one can obtain similar results in this paper with their assumptions. For simplicity, we
just use assumption (1.7) here.

H RELATIONSHIP WITH (STOCHASTIC) NEWTON’S METHOD

Our motivation in Proposition 1 shares a similar idea with (stochastic) Newton’s method on quadratic
objectives

_wt+1 =wt −_ _ηtH_ _[−][1]∇f_ (wt, ξ),

where the parameters are also updated coordinately in the “rotated space”, i.e. given H = U ΛU _[⊤]_
and w[′] = U _[⊤]w. In particular, when the Hessian H is diagonal and ηt = 1/(t + 1), the update_
formula is exactly the same as the one for Proposition 1.

Despite of this similarity, our method differ from Newton method’s and its practical variants in several aspects. First of all, our method focuses on learning rate schedulers and is a first-order method.
This property is especially salient when we consider eigencurve’s derivatives in Section 4.3:
only hyperparameter search is needed, just like other common learning rate schedulers. In addition,
most second-order methods, e.g. Schraudolph (2002); Erdogdu & Montanari (2015); Grosse &
Martens (2016); Byrd et al. (2016); Botev et al. (2017); Huang et al. (2020); Yang et al. (2021), approximates the Hessian matrix or the Hessian inverse and exploits the curvature information, while
eigencurve only utilizes the rough estimation of the Hessian spectrum. On top of that, this estimation is only an one-time effect and can be even further removed for similar models. These key
differences highlight eigencurve’s advantages over most second-order methods in practice.


-----

