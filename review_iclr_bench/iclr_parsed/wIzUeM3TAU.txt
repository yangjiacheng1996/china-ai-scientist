# EXPRESSIVENESS AND APPROXIMATION PROPERTIES
## OF GRAPH NEURAL NETWORKS

**Floris Geerts**
Department of Computer Science, University of Antwerp, Belgium
floris.geerts@uantwerpen.be

**Juan L. Reutter**
School of Engineering, Pontificia Universidad Cat´olica de Chile, Chile & IMFD, Chile
jreutter@ing.puc.cl

ABSTRACT

Characterizing the separation power of graph neural networks (GNNs) provides
an understanding of their limitations for graph learning tasks. Results regarding
separation power are, however, usually geared at specific GNN architectures, and
tools for understanding arbitrary GNN architectures are generally lacking. We
provide an elegant way to easily obtain bounds on the separation power of GNNs
in terms of the Weisfeiler-Leman (WL) tests, which have become the yardstick to
measure the separation power of GNNs. The crux is to view GNNs as expressions
in a procedural tensor language describing the computations in the layers of the
GNNs. Then, by a simple analysis of the obtained expressions, in terms of the
number of indexes and the nesting depth of summations, bounds on the separation power in terms of the WL-tests readily follow. We use tensor language to
define Higher-Order Message-Passing Neural Networks (or k-MPNNs), a natural
extension of MPNNs. Furthermore, the tensor language point of view allows for
the derivation of universality results for classes of GNNs in a natural way. Our
approach provides a toolbox with which GNN architecture designers can analyze
the separation power of their GNNs, without needing to know the intricacies of
the WL-tests. We also provide insights in what is needed to boost the separation
power of GNNs.

1 INTRODUCTION

Graph Neural Networks (GNNs) (Merkwirth & Lengauer, 2005; Scarselli et al., 2009) cover many
popular deep learning methods for graph learning tasks (see Hamilton (2020) for a recent overview).
These methods typically compute vector embeddings of vertices or graphs by relying on the underlying adjacency information. Invariance (for graph embeddings) and equivariance (for vertex embeddings) of GNNs ensure that these methods are oblivious to the precise representation of the graphs.

**Separation power.** Our primary focus is on the separation power of GNN architectures, i.e., on
their ability to separate vertices or graphs by means of the computed embeddings. It has become
standard to characterize GNN architectures in terms of the separation power of graph algorithms
such as color refinement (CR) and k-dimensional Weisfeiler-Leman tests (k-WL), as initiated in Xu
et al. (2019) and Morris et al. (2019). Unfortunately, understanding the separation power of any
given GNN architecture requires complex proofs, geared at the specifics of the architecture. We
provide a tensor language-based technique to analyze the separation power of general GNNs.

**Tensor languages.** Matrix query languages (Brijder et al., 2019; Geerts et al., 2021b) are defined
to assess the expressive power of linear algebra. Balcilar et al. (2021a) observe that, by casting
various GNNs into the MATLANG (Brijder et al., 2019) matrix query language, one can use existing
separation results (Geerts, 2021) to obtain upper bounds on the separation power of GNNs in terms
of 1-WL and 2-WL. In this paper, we considerably extend this approach by defining, and studying, a
new general-purpose tensor language specifically designed for modeling GNNs. As in Balcilar et al.
(2021a), our focus on tensor languages allows us to obtain new insights about GNN architectures.


-----

First, since tensor languages can only define invariant and equivariant graph functions, any GNN that
can be cast in our tensor language inherits these desired properties. More importantly, the separation
power of our tensor language is as closely related to CR and k-WL as GNNs are. Loosely speaking,
_if tensor language expressions use k + 1 indices, then their separation power is bounded by k-WL._
_Furthermore, if the maximum nesting of summations in the expression is t, then t rounds of k-WL_
_are needed to obtain an upper bound on the separation power. A similar connection is obtained for_
CR and a fragment of tensor language that we call “guarded” tensor language.

We thus reduce problem of assessing the separation power of any specific GNN architecture to
the problem of specifying it in our tensor language, analyzing the number of indices used and
counting their summation depth. This is usually much easier than dealing with intricacies of CR and
_k-WL, as casting GNNs in our tensor language is often as simple as writing down their layer-based_
definition. We believe that this provides a nice toolbox for GNN designers to assess the separation
power of their architecture. We use this toolbox to recover known results about the separation
power of specific GNN architectures such as GINs (Xu et al., 2019), GCNs (Kipf & Welling, 2017),
Folklore GNNs (Maron et al., 2019b), k-GNNs (Morris et al., 2019), and several others. We also
derive new results: we answer an open problem posed by Maron et al. (2019a) by showing that
the separation power of Invariant Graph Networks (k-IGNs), introduced by Maron et al. (2019b), is
bounded by (k − 1)-WL. In addition, we revisit the analysis by Balcilar et al. (2021b) of ChebNet
(Defferrard et al., 2016), and show that CayleyNet (Levie et al., 2019) is bounded by 2-WL.

When writing down GNNs in our tensor language, the less indices needed, the stronger the bounds
in terms of k-WL we obtain. After all, (k − 1)-WL is known to be strictly less separating than
_k-WL (Otto, 2017). Thus, it is important to minimize the number of indices used in tensor language_
expressions. We connect this number to the notion of treewidth: expressions of treewidth k can be
translated into expressions using k + 1 indices. This corresponds to optimizing expressions, as done
in many areas in machine learning, by reordering the summations (a.k.a. variable elimination).

**Approximation and universality.** We also consider the ability of GNNs to approximate general
invariant or equivariant graph functions. Once more, instead of focusing on specific architectures,
we use our tensor languages to obtain general approximation results, which naturally translate to
universality results for GNNs. We show: (k + 1)-index tensor language expressions suffice to approximate any (invariant/equivariant) graph function whose separating power is bounded by k-WL,
and we can further refine this by comparing the number of rounds in k-WL with the summation depth
of the expressions. These results provide a finer picture than the one obtained by Azizian & Lelarge
(2021). Furthermore, focusing on “guarded” tensor expressions yields a similar universality result
for CR, a result that, to our knowledge, was not known before. We also provide the link between
approximation results for tensor expressions and GNNs, enabling us to transfer our insights into
universality properties of GNNs. As an example, we show that k-IGNs can approximate any graph
function that is less separating than (k−1)-WL. This case was left open in Azizian & Lelarge (2021).

In summary, we draw new and interesting connections between tensor languages, GNN architectures
and classic graph algorithms. We provide a general recipe to bound the separation power of GNNs,
optimize them, and understand their approximation power. We show the usefulness of our method
by recovering several recent results, as well as new results, some of them left open in previous work.

**Related work.** Separation power has been studied for specific classes of GNNs (Morris et al.,

2019; Xu et al., 2019; Maron et al., 2019b; Chen et al., 2019; Morris et al., 2020; Azizian & Lelarge,
2021). A first general result concerns the bounds in terms of CR and 1-WL of Message-Passing
Neural Networks (Gilmer et al., 2017; Morris et al., 2019; Xu et al., 2019). Balcilar et al. (2021a)
use the MATLANG matrix query language to obtain upper bounds on the separation power of various
GNNs. MATLANG can only be used to obtain bounds up to 2-WL and is limited to matrices. Our
tensor language is more general and flexible and allows for reasoning over the number of indices,
treewidth, and summation depth of expressions. These are all crucial for our main results. The tensor
language introduced resembles sum-MATLANG (Geerts et al., 2021b), but with the added ability to
represent tensors. Neither separation power nor guarded fragments were considered in Geerts et al.
(2021b). See Section A in the supplementary material for more details. For universality, Azizian
& Lelarge (2021) is closest in spirit. Our approach provides an elegant way to recover and extend
their results. Azizian & Lelarge (2021) describe how their work (and hence also ours) encompasses
previous works (Keriven & Peyr´e, 2019; Maron et al., 2019c; Chen et al., 2019). Our results use
connections between k-WL and logics (Immerman & Lander, 1990; Cai et al., 1992), and CR and


-----

guarded logics (Barcel´o et al., 2020). The optimization of algebraic computations and the use of
treewidth relates to the approaches by Aji & McEliece (2000) and Abo Khamis et al. (2016).

2 BACKGROUND

We denote sets by {} and multisets by {{}}. For n ∈ N, n > 0, [n] := {1, . . ., n}. Vectors
are denoted by v, w, . . ., matrices by A, B, . . ., and tensors by S, T, . . .. Furthermore, vi is the i-th
entry of vector v, Aij is the (i, j)-th entry of matrix A and Si denotes the i = (i1, . . ., ik)-th entry of
a tensor S. If certain dimensions are unspecified, then this is denoted by a “:”. For example, Ai: and
**_A:j denote the i-th row and j-th column of matrix A, respectively. Similarly for slices of tensors._**

We consider undirected simple graphs G = (VG, EG, colG) equipped with a vertex-labelling colG :
_Vof sizeVGG → with nR [ and let[ℓ]n. We assume that graphs have size]. For a vertex Gs be the set of pairs v ∈_ _VG, NG ((vG,) := v) { with nu, so ∈ GV VG ∈GG | consists of vu and ∈ vE ∈G} nV. We letG[s] vertices and we often identify[. Note that] G be the set of all graphs[ G][ =][ G][0][.]_

The color refinement algorithm (CR) (Morgan, 1965) iteratively computes vertex labellings based
on neighboring vertices, as follows. For a graph G and vertex v _VG, cr[(][0][)](G, v) := colG(v)._
_∈_
Then, for t 0, cr[(][t][+1][)](G, v) := (cr[(][t][)](G, v), cr[(][t][)](G, u) _u_ _NG(v)_ ). We collect all vertex
_≥_ _{{_ _|_ _∈_ _}}_
labels to obtain a label for the entire graph by defining gcr[(][t][)](G) := cr[(][t][)](G, v) _v_ _VG_ . The
_{{_ _|_ _∈_ _}}_
_k-dimensional Weisfeiler-Leman algorithm (k-WL) (Cai et al., 1992) iteratively computes labellings_
of k-tuples of vertices. For a k-tuple v, its atomic type in G, denoted by atpk(G, v), is a vector in
R[2][(][k]2[)][+][kℓ]. The first _k2_ entries are 0/1-values encoding the equality type of v, i.e., whether vi = vj
for 1 ≤ _i < j ≤_ _k . The second_ _k2_ entries are 0/1-values encoding adjacency information, i.e.,
whether vivj _EG for 1_ _i < j_ _k. The last kℓ_ real-valued entries correspond to colG(vi) R[ℓ]
_∈_ _≤_ _≤ _  _∈_

for 1 ≤ _i ≤_ _k. Initially, for a graph G and v ∈_ _VG[k][,][ k][-][WL][ assigns the label][ wl][(]k[0][)][(][G,][ v][) :=]_

atpk(G, v). For t ≥ 0, k-WL revises the label according to wl[(]k[t][+1][)](G, v) := (wl[(]k[t][)][(][G,][ v][)][, M] [)][ with]

_M :=_ atpk+1(G, vu), wl[(]k[t][)][(][G,][ v][[][u/][1])][, . . .,][ wl]k[(][t][)][(][G,][ v][[][u/k][])] _| u ∈_ _VG_, where v[u/i] :=

(v1, . . ., v i−1, u, vi+1, . . ., vk). We use k-WL to assign labels to vertices and graphs by defining:
vwl[(]k[t][)][(][G, v][) :=][ wl]k[(][t][)][(][G,][ (][v, . . ., v][))][, for vertex-labellings, and][ gwl]k[(][t][)] [:=][ {]{wl[(]k[t][)][(][G,][ v][)][ |][ v][ ∈] _[V][ k]G[}]},_

for graph-labellings. We use cr[(][∞][)], gcr[(][∞][)], vwl[(]k[∞][)], and gwl[(]k[∞][)] to denote the stable labellings pro
duced by the corresponding algorithm over an arbitrary number of rounds. Our version of 1-WL
differs from CR in that 1-WL also uses information from non-adjacent vertices; this distinction only
matters for vertex embeddings (Grohe, 2021). We use the “folklore” k-WL of Cai et al. (1992),
except Cai et al. use 1-WL to refer to CR. While equivalent to “oblivious” (k + 1)-WL (Grohe,
2021), used in some other works on GNNs, care is needed when comparing to our work.

Let G be a graph with VG = [n] and let σ be a permutation of [n]. We denote by σ ⋆G the
isomorphic copy of G obtained by applying the permutation σ. Similarly, for v ∈ _VG[k][,][ σ ⋆]_ **_[v]_**
_invariantis the permuted version of if f_ (G) = f (σ ⋆G v). Let for any permutation F be some feature space. A function π. More generally, f : G fs → : GF0 is → equivariantF is called if
_f_ (σ ⋆G, σ ⋆ **_v) = f_** (G, v) for any permutation σ. The functions cr[(][t][)] : G1 → F and vwl[(]k[t][)] [:][ G][1][ →] [F]

are equivariant, whereas gcr[(][t][)] : G0 → F and gwl[(]k[t][)] [:][ G][0][ →] [F][ are invariant, for any][ t][ ≥] [0][ and][ k][ ≥] [1][.]

3 SPECIFYING GNNS

Many GNNs use linear algebra computations on vectors, matrices or tensors, interleaved with the
application of activation functions or MLPs. To understand the separation power of GNNs, we
introduce a specification language, TL, for tensor language, that allows us to specify any algebraic
computation in a procedural way by explicitly stating how each entry is to be computed. We gauge
the separation power of GNNs by specifying them as TL expressions, and syntactically analyzing the
components of such TL expressions. This technique gives rise to Higher-Order Message-Passing
_Neural Networks (or k-MPNNs), a natural extension of MPNNs (Gilmer et al., 2017). For simplicity,_
we present TL using summation aggregation only but arbitrary aggregation functions on multisets
of real values can be used as well (Section C.5 in the supplementary material).


-----

To introduce TL, consider a typical layer in a GNN of the form F _[′]_ = σ(A·F ·W ), where A ∈ R[n][×][n]
is an adjacency matrix,vertex i, σ is a non-linear activation function, and F ∈ R[n][×][ℓ] are vertex features such that W ∈ R[ℓ][×][ℓ] is a weight matrix. By exposing the Fi: ∈ R[ℓ] is the feature vector of
indices in the matrices and vectors we can equivalently write: for i ∈ [n] and s ∈ [ℓ]:

_Fis[′]_ [:=][ σ] _j∈[n]_ _[A][ij][ ·]_ _t∈[ℓ]_ _[W][ts][ ·][ F][jt]_ _._

In TL, we do not work with specific matrices or indices ranging overP  P [] [n], but focus instead on
expressions applicable to any matrix. We use index variables x1 and x2 instead of i and j, replace
_Aij with a placeholder E(x1, x2) and Fjt with placeholders Pt(x2), for t ∈_ [ℓ]. We then represent
the above computation in TL by ℓ expressions ψs(x1), one for each feature column, as follows:

_ψs(x1) = σ_ _x2_ _[E][(][x][1][, x][2][)][ ·]_ _t_ [ℓ] _[W][ts][ ·][ P][t][(][x][2][)]_ _._

_∈_

These are pure syntactical expressions.P To give them a semantics, we assign to P [] _E a matrix_
**_Avariable ∈_** R[n] x[×][n]2 under the summation range over, to Pt column vectors F:t ∈ R[n][×] 1[1], for, 2, . . ., n t ∈ [ℓ, the], and to TL expression x1 an index ψ is ∈(i)[ evaluates ton]. By letting the Fis[′] [.]
As such, F _[′]_ = σ(A · F · W ) can be represented as a specific instance of the above TL expressions.
Throughout the paper we reason about expressions in TL rather than specific instances thereof.
Importantly, by showing that certain properties hold for expressions in TL, these properties are
inherited by all of its instances. We use TL to enable a theoretical analysis of the separating power
of GNNs; It is not intended as a practical programming language for GNNs.

**Syntax.** We first give the syntax of TL expressions. We have a binary predicate E, to represent
adjacency matrices, and unary vertex predicates Ps, s [ℓ], to represent column vectors encoding
_∈_
the ℓ-dimensional vertex labels. In addition, we have a (possibly infinite) set Ω of functions, such as
activation functions or MLPs. Then, TL(Ω) expressions are defined by the following grammar:

_φ := 1x op y | E(x, y) | Ps(x) | φ · φ | φ + φ | a · φ | f_ (φ, . . ., φ) | _x_ _[φ]_

where op ∈{=, ̸=}, x, y are index variables that specify entries in tensors, s ∈ [ℓ], a ∈ R, and
_f_ Ω. Summation aggregation is captured by _x_ _[φ][.][1][ We sometimes make explicit which functions][P]_
_∈_

are used in expressions in TL(Ω) by writing TL(f1, f2, . . .) for f1, f2, . . . in Ω. For example, the
expressions ψs(x1) described earlier are in TL(σ).

[P]

The set of free index variables of an expression φ, denoted by free(φ), determines the order of
the tensor represented by φ. It is defined inductively: free(1x op y) = free(E(x, y)) := _x, y_,
_{_ _}_
freefree((fP(sφ(x1)) =, . . ., φ {xp})) :=, free ∪(φi∈1 ·[pφ]free2) =(φ freei), and(φ1 free + _φ(2[P]) :=x_ _[φ] free[1][) :=](φ[ free]1)_ _∪[(]free[φ][1][)]([ \ {]φ2)[x], free[}][. We sometimes explic-](a_ _·_ _φ1) := free(φ1),_

itly write the free indices. In our example expressions ψs(x1), x1 is the free index variable.

An important class of expressions are those that only use index variables _x1, . . ., xk_ . We denote
_{_ _}_
by TLk(Ω) the k-index variable fragment of TL(Ω). The expressions ψs(x1) are in TL2(σ).

**Semantics.** We next define the semantics of expressions in TL(Ω). Let G = (VG, EG, colG) be a
vertex-labelled graph. We start by defining the interpretation [[·, ν]]G of the predicates E, Ps and the
(dis)equality predicates, relative to G and a valuation ν assigning a vertex to each index variable:

[[E[[1(xx, y op y), ν, ν]]]]GG := if := if ν ν((xx)) opν(y ν) ∈(y)E thenG then 1 else 1 else 0. 0 [[Ps(x), ν]]G := colG(ν(x))s ∈ R

In other words, E is interpreted as the adjacency matrix of G and the Ps’s interpret the vertexlabelling colG. Furthermore, we lift interpretations to arbitrary expressions in TL(Ω), as follows:

[[[[[P]φ1x · φ[φ]2[1], ν[, ν]][]]]]GG := := [[φ1v∈, νVG]]G[[][ ·φ [1[, νφ2[, νx 7→]]G _v]]]G_ [[φ[[1a+ · φφ21, ν, ν]]]]GG := [ := a[φ · [1[, νφ1]], νG + []]G [φ2, ν]]G

[[f (φ1, . . ., φp), ν]]G := f ([[φ1, ν]]G, . . ., [[φp, ν]]G)

where, ν[x _v] is the valuation[P]_ _ν but which now maps the index x to the vertex v_ _VG. For_
_7→_ _∈_
simplicity, we identify valuations with their images. For example, [[φ(x), v]]G denotes [[φ(x), x 7→
_v]]G. To illustrate the semantics, for each v ∈_ _VG, our example expressions satisfy [[ψs, v]]G = Fvs[′]_
for F _[′]_ = σ(A · F · W ) when A is the adjacency matrix of G and F represents the vertex labels.

1We can replace _x_ _[φ][ by a more general aggregation construct][ aggr]x[F]_ [(][φ][)][ for arbitrary functions][ F][ that]

assign a real value to multisets of real values. We refer to the supplementary material (Section C.5) for details.

[P]


-----

_k-MPNNs._ Consider a function f : _Gs_ _→_ R[ℓ] : (G, v) _7→_ _f_ (G, v) _∈_ R[ℓ] for some
_ℓ_ _∈_ N. We say that the function f can be represented in TL(Ω) if there exists ℓ expressions
_φ1(x1, . . ., xs), . . ., φℓ(x1, . . ., xs) in TL(Ω) such that for each graph G and each s-tuple v ∈_ _VG[s][:]_

_f_ (G, v) = [[φ1, v]]G, . . ., [[φℓ, v]]G _._

Of particular interest are kth-order MPNN  _s (or k-MPNNs) which refers to the class of functions_
that can be represented inGNN is a k-MPNN if its corresponding functions are TLk+1(Ω). We can regard GNN k-MPNNs as functionss. For example, we can interpret f : Gs → R[ℓ]. Hence, a
thuseachF _[′]_ = f s σ belongs to ∈(A[ℓ ·], F [[ψ ·s W 1, v-]MPNN] )G as a function = Fs and our examplevs[′] [with][ ψ] f :[s] _G[∈]_ 1[TL] →[2] GNN[(]R[σ][)][ℓ][. Hence,]such that is a 1-[ f]MPNN f[(][G, v](G, v.[) = ([]) := F[ψ1v[′], v:[. We have seen that for]]]G, . . ., [[ψℓ, v]]G) and

**TL represents equivariant or invariant functions.** We make a simple observation which follows
from the type of operators allowed in expressions in TL(Ω).
**Proposition 3.1. Any function f : Gs →** R[ℓ] represented in TL(Ω) is equivariant (invariant if s = 0).

An immediate consequence is that when a GNN is a k-MPNN, it is automatically invariant or equivariant, depending on whether graph or vertex tuple embeddings are considered.

4 SEPARATION POWER OF TENSOR LANGUAGES

Our first main results concern the characterization of the separation power of tensor languages in
terms of the color refinement and k-dimensional Weisfeiler-Leman algorithms. We provide a finegrained characterization by taking the number of rounds of these algorithms into account. This will
allow for measuring the separation power of classes of GNNs in terms of their number of layers.

4.1 SEPARATION POWER

We define the separation power of graph functions in terms of an equivalence relation, based on the
definition from Azizian & Lelarge (2021), hereby first focusing on their ability to separate vertices.[2]

**Definition 1. Let F be a set of functions f : G1 →** R[ℓ][f] . The equivalence relation ρ1(F) is defined
by F on G1 as follows: (G, v), (H, w) _∈_ _ρ1(F) ⇐⇒∀f ∈F, f_ (G, v) = f (H, w).

In other words, when (( G, v), (H, w))  _ρ1(_ ), no function in can separate v in G from w in
_∈_ _F_ _F_
_H. For example, we can view cr[(][t][)]_ and vwl[(]k[t][)] [as functions from][ G][1][ to some][ R][ℓ][. As such][ ρ][1][(][cr][(][t][)][)]

and ρ1(vwl[(]k[t][)][)][ measure the separation power of these algorithms. The following strict inclusions are]

known: for all k ≥ 1, ρ1(vwl[(]k[t]+1[)] [)][ ⊂] _[ρ][1][(][vwl][(]k[t][)][)][ and][ ρ][1][(][vwl]1[(][t][)][)][ ⊂]_ _[ρ][1][(][cr][(][t][)][)][ (][Otto][,][ 2017][;][ Grohe][,][ 2021][).]_

It is also known that more rounds (t) increase the separation power of these algorithms (F¨urer, 2001).

For a fragment of TL(Ω) expressions, we define ρ1( ) as the equivalence relation associated with
_L_ _L_
all functionsexpressions in f TL : G(Ω)1 → with one free index variable resulting in vertex embeddings.R[ℓ][f] that can be represented in L. By definition, we here thus consider

4.2 MAIN RESULTS

We first provide a link between k-WL and tensor language expressions using k + 1 index variables:
**Theorem 4.1. For each k** 1 and any collection Ω _of functions, ρ1_ vwl[(]k[∞][)] = ρ1 TLk+1(Ω) _._
_≥_

This theorem gives us new insights: if we wish to understand how a new  GNN architecture compares  
against the k-WL algorithms, all we need to do is to show that such an architecture can be represented
in TLk+1(Ω), i.e., is a k-MPNN, an arguably much easier endeavor. As an example of how to use
this result, it is well known that triangles can be detected by 2-WL but not by 1-WL. Thus, in order
to design GNNs that can detect triangles, layer definitions in TL3 rather than TL2 should be used.

We can do much more, relating the rounds of k-WL to the notion of summation depth of TL(Ω)
expressions. We also present present similar results for functions computing graph embeddings.

2We differ slightly from Azizian & Lelarge (2021) in that they only define equivalence relations on graphs.


-----

The summation depth sd(φ) of a TL(Ω) expression φ measures the nesting depth of the summations

_x_ [in the expression. It is defined inductively:][ sd][(][1][x][ op][ y][) =][ sd][(][E][(][x, y][)) =][ sd][(][P][s][(][x][)) := 0][,]
sdPmaxsummation depth one. We write(φ{1sd · φ(φ2i) =)|i ∈ sd[(pφ]}1 +, and φ2 sd) :=([P] max TLx _[φ]([1]kt{[) :=])+1sd[(Ω)](φ[ sd]1[ for the class of expressions in])[(],[φ] sd[1][) + 1](φ2)}[. For example, expressions], sd(a · φ1) := sd(φ[ TL]1), sd[k][+1]([ ψ]f[(Ω)]([s]φ[(][x]1[ of summation][1], . . ., φ[)][ above have]p)) :=_

depth at most t, and use k-MPNN[(][t][)] for the corresponding class of k-MPNNs. We can now refine
Theorem 4.1, taking into account the number of rounds used in k-WL.


**Theorem 4.2. For all t** 0, k 1 and any collection Ω _of functions, ρ1_ vwl[(]k[t][)]
_≥_ _≥_
 


= ρ1 TL(kt)+1[(Ω)]
 


**Guarded TL and color refinement.** As noted by Barcel´o et al. (2020), the separation power of
vertex embeddings of simple GNNs, which propagate information only through neighboring vertices, is usually weaker than that of 1-WL. For these types of architectures, Barcel´o et al. (2020)
provide a relation with the weaker color refinement algorithm, but only in the special case of first_order classifiers. We can recover and extend this result in our general setting, with a guarded version_
of TL which, as we will show, has the same separation power as color refinement.

The guarded fragment GTL(Ω) of TL2(Ω) is inspired by the use of adjacency matrices in simple
GNNs. In GTL(Ω) only equality predicates 1xi=xi (constant 1) and 1xi≠ _xi (constant 0) are allowed,_
addition and multiplication require the component expressions to have the same (single) free index,
and summation must occur in a guarded form _xj_ _E(xi, xj)_ _φ(xj)_, for i, j [2]. Guardedness

_·_ _∈_
means that summation only happens over neighbors. In GTL(Ω), all expressions have a single free
  
variable and thus only functions from 1 can be represented. Our example expressions[P] _ψs(x1) are_
(t) _G_
guarded. The fragment GTL (Ω) consists of expressions in GTL(Ω) of summation depth at most t.
We denote by MPNNs and MPNNs[(][t][)] the corresponding “guarded” classes of 1-MPNNs.[3]

**Theorem 4.3. For all t ≥** 0 and any collection Ω _of functions: ρ1_ cr[(][t][)][] = ρ1 GTL(t)(Ω) _._

As an application of this theorem, to detect the existence of paths of length  _t, the number of guarded _ 
layers in GNNs should account for a representation in GTL(Ω) of summation depth of at least t. We
recall that ρ1(vwl[(]1[t][)][)][ ⊂] _[ρ][1][(][cr][(][t][)][)][ which, combined with our previous results, implies that][ TL](2t)[(Ω)]_

(resp., 1-MPNNs) is strictly more separating than GTL(t)(Ω) (resp., MPNNs).

**Graph embeddings.** We next establish connections between the graph versions of k-WL and CR,
and TL expressions without free index variables. To this aim, we use ρ0( ), for a set of functions
_F_ _F_
_f : G →_ R[ℓ][f], as the equivalence relation over G defined in analogy to ρ1: (G, H) ∈ _ρ0(F) ⇐⇒_
_∀f ∈F, f_ (G) = f (H). We thus consider separation power on the graph level. For example, we
can consider ρ0(gcr[(][t][)]) and ρ0(gwl[(]k[t][)][)][ for any][ t][ ≥] [0][ and][ k][ ≥] [1][. Also here,][ ρ][0][(][gwl]k[(][t]+1[)] [)][ ⊂] _[ρ][0][(][gwl][(]k[t][)][)]_

but different from vertex embeddings, ρ0(gcr[(][t][)]) = ρ0(gwl[(]1[t][)][)][ (][Grohe][,][ 2021][). We define][ ρ][0][(][L][)][ for a]

fragment L of TL(Ω) by considering expressions without free index variables.

The connection between the number of index variables in expressions and k-WL remains to hold.
Apart from k = 1, no clean relationship exists between summation depth and rounds, however.[4]

**Theorem 4.4. For all t ≥** 0, k ≥ 1 and any collection Ω _of functions, we have that:_

_(1)_ _ρ0_ gcr[(][t][)][] = ρ0 TL(2t+1)(Ω) = ρ0 gwl[(]1[t][)] _(2)_ _ρ0_ gwl[(]k[∞][)] = ρ0 TLk+1(Ω) _._
             

Intuitively, in (1) the increase in summation depth by one is incurred by the additional aggregation
needed to collect all vertex labels computed by gwl[(]1[t][)][.]

**Optimality of number of indices.** Our results so far tell that graph functions represented in
TLk+1(Ω) are at most as separating as k-WL. What is left unaddressed is whether all k + 1 index variables are needed for the graph functions under consideration. It may well be, for example,
that there exists an equivalent expression using less index variables. This would imply a stronger
upper bound on the separation power by ℓ-WL for ℓ< k. We next identify a large class of TL(Ω)
expressions, those of treewidth k, for which the number of index variables can be reduced to k + 1.

3For the connection to classical MPNNs (Gilmer et al., 2017), see Section H in the supplementary material.
4Indeed, the best one can obtain for general tensor logic expressions is ρ0 TL[(]k[t]+1[+][k][)][(Ω)] _⊆_ _ρ0_ gwl[(]k[t][)] _⊆_

_ρ0_ TL[(]k[t]+1[+1][)][(Ω)] . This follows from Cai et al. (1992) and connections to finite variable logics.     
  


-----

**Proposition 4.5. Expressions in TL(Ω) of treewidth k are equivalent to expressions in TLk+1(Ω).**

Treewidth is defined in the supplementary material (Section G) and a treewidth of k implies that the
computation of tensor language expressions can be decomposed, by reordering summations, such
that each local computation requires at most k + 1 indices (see also Aji & McEliece (2000)). As a
simple example, consider θ(x1) = _x2_ _x3_ _[E][(][x][1][, x][2][)]_ _[·]_ _[E][(][x][2][, x][3][)][ in][ TL](32)_ [such that][ [][θ, v]]G counts

the number of paths of length two starting from v. This expression has a treewidth of one. And

P (2)

indeed, it is equivalent to the expression[P] _θ[˜](x1) =_ _x2_ _[E][(][x][1][, x][2][)]_ _[·]_ _x1_ _[E][(][x][2][, x][1][)]_ in TL2 [(and in]

fact in GTL(2)). As a consequence, no more vertices can be separated by θ(x1) than by cr(2), rather
than vwl[2]2 [as the original expression in][ TL](32) [suggests.][P]  P 

**On the impact of functions.** All separation results for TL(Ω) and fragments thereof hold irregardless of the chosen functions in Ω, including when no functions are present at all. Function
applications hence do not add expressive power. While this may seem counter-intuitive, it is due to
the presence of summation and multiplication in TL that are enough to separate graphs or vertices.

5 CONSEQUENCES FOR GNNS

We next interpret the general results on the separation power from Section 4 in the context of GNNs.

**_1. The separation power of any vertex embedding GNN architecture which is an MPNN[(][t][)]_** **_is_**
**_bounded by the power of t rounds of color refinement._**

We consider the Graph Isomorphism Networks (GINs) (Xu et al., 2019) and show that these are
MPNNs. To do so, we represent them in GTL(Ω). Let gin be such a network; it updates vertex
embeddings as follows. Initially, gin[(0)] : G1 → R[ℓ][0] : (G, v) 7→ **_Fv[(][0]:_** [)] [:=][ col][G][(][v][)][ ∈] [R][ℓ][0] [. For layer]

_t > 0, gin[(][t][)]_ : G1 → R[ℓ][t] is given by: (G, v) 7→ **_Fv[(][t]:[)]_** [:=][ mlp][(][t][)][ ]Fv[(][t]:[−][1][)], _u∈NG(v)_ **_[F][ (]u[t]:[−][1][)]_**, with

**_F_** [(][t][)] _∈_ R[n][×][ℓ][t] and mlp[(][t][)] = (mlp[(]1[t][)][, . . .,][ mlp]ℓ[(][t]t[)][) :][ R][2][ℓ][t][−][1][ →] [R][ℓ][t][ is an][ MLP][. We denote by](0) [ GIN][(][t][)]

the class of GINs consisting t layers. Clearly, gin[(][0][)] can be represented in GTL[P] by considering the
expressions φ[(]i[0][)][(][x][1][) :=][ P][i][(][x][1][)][ for each][ i][ ∈] [[][ℓ][0][]][. To represent][ gin][(][t][)][, assume that we have][ ℓ][t][−][1]

expressions φ[(]i[t][−][1][)](x1) in GTL(t−1)(Ω) representing gin(t−1). That is, we have [[φ[(]i[t][−][1][)], v]]G = Fvi[(][t][−][1][)]

for each vertex v and i ∈ [ℓt−1]. Then gin[(][t][)] is represented by ℓt expressions φ[(]i[t][)][(][x][1][)][ defined as:]

mlp[(]i[t][)] _φ[(]1[t][−][1][)](x1), . . ., φ[(]ℓ[t]t[−]−[1]1[)][(][x][1][)][,][ P]x2_ _[E][(][x][1][, x][2][)][ ·][ φ]1[(][t][−][1][)](x2), . . .,_ _x2_ _[E][(][x][1][, x][2][)][ ·][ φ]ℓ[(][t]t[−]−[1]1[)][(][x][2][)]_ _,_

which are now expressions in GTL(t)(Ω) where Ω consists of MLPs. We have [[φ[(]i[t][)][, v][]]]G = Fv,i[(][t][)] [for]

[P]

each v ∈ _VG and i ∈_ [ℓt], as desired. Hence, Theorem 4.3 tells that t-layered GINs cannot be more
separating than t rounds of color refinement, in accordance with known results (Xu et al., 2019; Morris et al., 2019). We thus simply cast GINs in GTL(Ω) to obtain an upper bound on their separation
power. In the supplementary material (Section D) we give similar analyses for GraphSage GNNs
with various aggregation functions (Hamilton et al., 2017), GCNs (Kipf & Welling, 2017), simplified GCNs (SGCs) (Wu et al., 2019), Principled Neighborbood Aggregation (PNAs) (Corso et al.,
2020), and revisit the analysis of ChebNet (Defferrard et al., 2016) given in Balcilar et al. (2021a).

**_2. The separation power of any vertex embedding GNN architecture which is an k-MPNN[(][t][)]_** **_is_**
**_bounded by the power of t rounds of k-WL._**

For k = 1, we consider extended Graph Isomorphism Networks (eGINs) (Barcel´o et al., 2020). For
anis defined by egin ∈ eGIN (G, v, egin) 7→[(][0][)] :F Gv[(][t]1:[)] →[:=][ mlp]R[ℓ][0] [(]is defined as for[t][)][ ]Fv[(][t]:[−][1][)], _u∈N GING(v)s, but for layer[F][ (]u[t]:[−][1][)],_ _u∈V t >G_ **_[F][ (]u 0[t]:[−], egin[1][)]_**, where[(][t][)] : G mlp1 →[(][t]R[)] _[ℓ]is[t]_

now an MLP from R[3][ℓ][t][−][1] _→_ R[ℓ][t] . The difference with GINs is the use of _u∈VG_ **_[F][ (]u[t]:[−][1][)]_** which

corresponds to the unguarded summation _x1[P][φ][(][t][−][1][)][(][x][1][)][. This implies that][P]_ [ TL][ rather than][ GTL]

needs to be used. In a similar way as for GINs, we can represent eGIN layers in[P] TL(2t)[(Ω)][. That is,]

each eGIN[(][t][)] is an 1-MPNN[(][t][)]. Theorem 4.2[P] tells that t rounds of 1-WL bound the separation power
of t-layered extended GINs, conform to Barcel´o et al. (2020). More generally, any GNN looking to
go beyond CR must use non-guarded aggregations.

For k ≥ 2, it is straightforward to show that t-layered “folklore” GNNs (k-FGNNs) (Maron et al.,
2019b) are k-MPNN[(][t][)] and thus, by Theorem 4.2, t rounds of k-WL bound their separation power.


-----

One merely needs to cast the layer definitions in TL(Ω) and observe that k+1 indices and summation
depth t are needed. We thus refine and recover the k-WL bound for k-FGNNs by Azizian & Lelarge
(2021). We also show that the separation power of (k +1)-Invariant Graph Networks ((k +1)-IGNs)
(Maron et al., 2019b) are bounded by k-WL, albeit with an increase in the required rounds.
**Theorem 5.1. For any k ≥** 1, the separation power of a t-layered (k + 1)-IGNs is bounded by the
_separation power of tk rounds of k-WL._

We hereby answer open problem 1 in Maron et al. (2019a). The case k = 1 was solved in Chen
et al. (2020) by analyzing properties of 1-WL. By contrast, Theorem 4.2 shows that one can focus on
expressing (k +1)-IGNs in TLk+1(Ω) and analyzing the summation depth of expressions. The proof
of Theorem 5.1 requires non-trivial manipulations of tensor language expressions; it is a simplified
proof of Geerts (2020). The additional rounds (tk) are needed because (k + 1)-IGNs aggregate
information in one layer that becomes accessible to k-WL in k rounds. We defer detail to Section E
in the supplementary material, where we also identify a simple class of t-layered (k + 1)-IGNs that
are as powerful as (k + 1)-IGNs but whose separation power is bounded by t rounds of k-WL.

We also consider “augmented” GNNs, which are combined with a preprocessing step in which
higher-order graph information is computed. In the supplementary material (Section D.3) we show
how TL encodes the preprocessing step, and how this leads to separation bounds in terms of k-WL,
where k depends on the treewidth of the graph information used. Finally, our approach can also be
used to show that the spectral CayleyNets (Levie et al., 2019) are bounded in separation power by
2-WL. This result complements the spectral analysis of CayleyNets given in Balcilar et al. (2021b).

**_3. The separation power of any graph embedding GNN architecture which is a k-MPNN is_**
**_bounded by the power of k-WL._**

Graph embedding methods are commonly obtained from vertex (tuple) embeddings methods by
including a readout layer in which all vertex (tuple) embeddings are aggregated. For example,
mlp([P]v _V_ [egin][(][t][)][(][G, v][))][ is a typical readout layer for][ eGIN][s . Since][ egin][(][t][)][ can be represented in]

TL(2t)[(Ω)][, the readout layer can be represented in]∈ [ TL](2t+1)(Ω), using an extra summation. So they are

1-MPNNs. Hence, their separation power is bounded by gwl[(]1[t][)][, in accordance with Theorem][ 4.4][.]

This holds more generally. If vertex embedding methods are k-MPNNs, then so are their graph
versions, which are then bounded by gwl[(]k[∞][)] by our Theorem 4.4.

**_4. To go beyond the separation power of k-WL, it is necessary to use GNNs whose layers are_**
**_represented by expressions of treewidth > k._**

Hence, to design expressive GNNs one needs to define the layers such that treewidth of the resulting
TL expressions is large enough. For example, to go beyond 1-WL, TL3 representable linear algebra
operations should be used. Treewidth also sheds light on the open problem from Maron et al. (2019a)
where it was asked whether polynomial layers (in A) increase the separation power. Indeed, consider
a layer of the form σ(A[3]·F ·W ), which raises the adjacency matrix A to the power three. Translated
in TL(Ω), layer expressions resemble _x2_ _x3_ _x4_ _[E][(][x][1][, x][2][)][·][E][(][x][2][, x][3][)][·][E][(][x][3][, x][4][)][, of treewidth]_

one. Proposition 4.5 tells that the layer is bounded byP P wl[(]1[3][)] [(and in fact by][ cr][(][3][)][) in separation power.]

If instead, the layer is of the form σ(C[P]F **_W ) where Cij holds the number of cliques containing the_**

_·_ _·_
edge ij. Then, in TL(Ω) we get expressions containing _x2_ _x3_ _[E][(][x][1][, x][2][)]_ _[·]_ _[E][(][x][1][, x][3][)]_ _[·]_ _[E][(][x][2][, x][3][)][.]_

The variables form a 3-clique resulting in expressions of treewidth two. As a consequence, the

P

separation power will be bounded by wl[(]2[2][)][. These examples show that it is not the number of multi-][P]

plications (in both cases two) that gives power, it is how variables are connected to each other.

6 FUNCTION APPROXIMATION

We next provide characterizations of functions that can be approximated by TL expressions, when
interpreted as functions. We recover and extend results from Azizian & Lelarge (2021) by taking
the number of layers of GNNs into account. We also provide new results related to color refinement.

6.1 GENERAL TL APPROXIMATION RESULTS

We assume that Gs is a compact space by requiring that vertex labels come from a compact set K ⊆
R[ℓ][0] . Let F be a set of functions f : Gs → R[ℓ][f] and define its closure F as all functions h from Gs for


-----

which there exists a sequence f1, f2, . . . ∈F such that limi→∞ supG,v∥fi(G, v) − _h(G, v)∥_ = 0
for some norm ∥.∥. We assume F to satisfy two properties. First, F is concatenation-closed: if
(ff11 :(G, G vs →), f2(RG,[p] **_vand)) is also in f2 : Gs → F. Second,R[q]_** are in F F is, then function-closed g := (f1, for a fixed, f2) : Gs → ℓ _∈RN[p]: for any[+][q]_ : (G, f v ∈F) 7→
such thatFor such F f :, we let Gs → FRℓ _[p]be the subset of functions in, also g ◦_ _f : Gs →_ R[ℓ] is in F F from for any continuous function Gs to R[ℓ]. Our next result is based on a g : R[p] _→_ R[ℓ].
generalized Stone-Weierstrass Theorem (Timofte, 2005), also used in Azizian & Lelarge (2021).
**Theorem 6.1. For any ℓ, and any set F of functions, concatenation and function closed for ℓ, we**
_have: Fℓ_ = {f : Gs → R[ℓ] _| ρs(F) ⊆_ _ρs(f_ )}.

This result gives us insight on which functions can be approximated by, for example, a set F of
functions originating from a class of GNNs. In this case, _ℓ_ represent all functions approximated by
_F_
instances of such a class and Theorem 6.1 tells us that this set corresponds precisely to the set of all
functions that are equally or less separating than the GNNs in this class. If, in addition, _ℓ_ is more
_F_
separating that CR or k-WL, then we can say more. Let alg ∈{cr[(][t][)], gcr[(][t][)], vwl[(]k[t][)][,][ gwl]k[(][∞][)]}.

**Corollary 6.2.R[ℓ]** _| ρ(alg) ⊆_ _ρ Under the assumptions of Theorem(f_ )}. _6.1 and if ρ(Fℓ) = ρ(alg), then Fℓ_ = {f : Gs →

The properties of being concatenation and function-closed are satisfied for sets of functions representable in our tensor languages, if Ω contains all continuous functions g : R[p] _→_ R[ℓ], for any p, or
alternatively, all MLPs (by Lemma 32 in Azizian & Lelarge (2021)). Together with our results in
Section 4, the corollary implies that MPNNs[(][t][)], 1-MPNNs[(][t][)], k-MPNNs[(][t][)] or k-MPNNs can approximate all functions with equal or less separation power than cr[(][t][)], gcr[(][t][)], vwl[(]k[t][)] [or][ gwl]k[(][∞][)], respectively.

Prop. 3.1 also tells that the closure consists of invariant (s = 0) and equivariant ( s > 0) functions.

6.2 CONSEQUENCES FOR GNNS


**_All our results combined provide a recipe to guarantee that a given function can be approxi-_**
**_mated by GNN architectures. Indeed, suppose that your class of GNNs is an MPNN[(][t][)]_** (respectively,
1-MPNN[(][t][)], k-MPNN[(][t][)] or k-MPNN, for some k ≥ 1). Then, since most classes of GNNs are
concatenation-closed and allow the application of arbitrary MLPs, this implies that your GNNs can
only approximate functions f that are no more separating than cr[(][t][)] (respectively, gcr[(][t][)], vwl[(]k[t][)] [or]

gwl[(]k[∞][)]). To guarantee that that these functions can indeed be approximated, one additionally has to

show that your class of GNNs matches the corresponding labeling algorithm in separation power.

For example, GNNs in GIN[(]ℓ[t][)] [are][ MPNN][s][(][t][)][, and thus][ GIN][(]ℓ[t][)] [contains any function][ f][ :][ G][1][ →] [R][ℓ]

satisfying ρ1(cr[(][t][)]) ⊆ _ρ1(f_ ). Similarly, eGIN[(]ℓ[t][)][s are][ 1][-][MPNN][s][(][t][)][, so][ eGIN][(]ℓ[t][)] [contains any function]

satisfying ρ1(wl[(]1[t][)][)][ ⊆] _[ρ][1][(][f]_ [)][; and when extended with a readout layer, their closures consist of]

functions f : G0 → R[ℓ] satisfying ρ0(gcr[(][t][)]) = ρ0(vwl[(]1[t][)][)][ ⊆] _[ρ][0][(][f]_ [)][. Finally,][ k][-][FGNN]ℓ[(][t][)][s are][ k][-]

MPNNs[(][t][)], so k-FGNN[(]ℓ[t][)] [consist of functions][ f][ such that][ ρ][1][(][vwl][(]k[t][)][)][ ⊆] _[ρ][1][(][f]_ [)][. We thus recover and]

extend results by Azizian & Lelarge (2021) by including layer information (t) and by treating color
refinement separately from 1-WL for vertex embeddings. Furthermore, Theorem 5.1 implies that
(k + 1)-IGNℓ consists of functions f satisfying ρ1(vwl[(]k[∞][)]) _ρ1(f_ ) and ρ0(gwl[(]k[∞][)]) _ρ0(f_ ), a

_⊆_ _⊆_

case left open in Azizian & Lelarge (2021).

These results follow from Corollary 6.2, that the respective classes of GNNs can simulate CR or
_k-WL on either graphs with discrete (Xu et al., 2019; Barcel´o et al., 2020) or continuous labels_
(Maron et al., 2019b), and that they are k-MPNNs of the appropriate form.

7 CONCLUSION

Connecting GNNs and tensor languages allows us to use our analysis of tensor languages to understand the separation and approximation power of GNNs. The number of indices and summation
depth needed to represent the layers in GNNs determine their separation power in terms of color
refinement and Weisfeiler-Leman tests. The framework of k-MPNNs provides a handy toolbox
to understand existing and new GNN architectures, and we demonstrate this by recovering several
results about the power of GNNs presented recently in the literature, as well as proving new results.


-----

8 AKNOWLEDGEMENTS & DISCLOSURE FUNDING

This work is partially funded by ANID–Millennium Science Initiative Program–Code ICN17 002,
Chile.

ETHICS STATEMENT

The results in this paper do not include misleading claims; their correctness is theoretically verified.
Related work is accurately represented.

REFERENCES

Mahmoud Abo Khamis, Hung Q. Ngo, and Atri Rudra. FAQ: Questions Asked Frequently. In
_Proceedings of the 35th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database_
_[Systems, PODS, pp. 13–28. ACM, 2016. URL https://doi.org/10.1145/2902251.](https://doi.org/10.1145/2902251.2902280)_
[2902280. 3, 39, 40, 41](https://doi.org/10.1145/2902251.2902280)

Srinivas M. Aji and Robert J. McEliece. The generalized distributive law. IEEE Transactions on In_[formation Theory, 46(2):325–343, 2000. URL https://doi.org/10.1109/18.825794.](https://doi.org/10.1109/18.825794)_
3, 7

Waiss Azizian and Marc Lelarge. Expressive power of invariant and equivariant graph neural networks. In Proceedings of the 9th International Conference on Learning Representations, ICLR,
[2021. URL https://openreview.net/forum?id=lxHgXYN4bwl. 2, 5, 8, 9, 28, 37,](https://openreview.net/forum?id=lxHgXYN4bwl)
38

Muhammet Balcilar, Pierre H´eroux, Benoit Ga¨uz`ere, Pascal Vasseur, S´ebastien Adam, and Paul
Honeine. Breaking the limits of message passing graph neural networks. In Proceedings of
_the 38th International Conference on Machine Learning, volume 139 of Proceedings of Ma-_
_[chine Learning Research, pp. 599–608. PMLR, 2021a. URL http://proceedings.mlr.](http://proceedings.mlr.press/v139/balcilar21a.html)_
[press/v139/balcilar21a.html. 1, 2, 7, 14, 30, 31](http://proceedings.mlr.press/v139/balcilar21a.html)

Muhammet Balcilar, Guillaume Renton, Pierre H´eroux, Benoit Ga¨uz`ere, S´ebastien Adam, and Paul
Honeine. Analyzing the expressive power of graph neural networks in a spectral perspective.
In Proceedings of the 9th International Conference on Learning Representations, ICLR, 2021b.
[URL https://openreview.net/forum?id=-qh0M9XWxnv. 2, 8, 30, 31](https://openreview.net/forum?id=-qh0M9XWxnv)

Pablo Barcel´o, Egor V Kostylev, Mikael Monet, Jorge P´erez, Juan Reutter, and Juan Pablo Silva.
The logical expressiveness of graph neural networks. In Proceedings of the 8th International
_[Conference on Learning Representations, ICLR, 2020. URL https://openreview.net/](https://openreview.net/forum?id=r1lZ7AEKvB)_
[forum?id=r1lZ7AEKvB. 3, 6, 7, 9, 18, 38](https://openreview.net/forum?id=r1lZ7AEKvB)

Pablo Barcel´o, Floris Geerts, Juan L. Reutter, and Maksimilian Ryschkov. Graph neural networks with local graph parameters. In Advances in Neural Information Processing Systems,
volume 34, 2021. [URL https://proceedings.neurips.cc/paper/2021/hash/](https://proceedings.neurips.cc/paper/2021/hash/d4d8d1ac7e00e9105775a6b660dd3cbb-Abstract.html)
[d4d8d1ac7e00e9105775a6b660dd3cbb-Abstract.html. 28, 29](https://proceedings.neurips.cc/paper/2021/hash/d4d8d1ac7e00e9105775a6b660dd3cbb-Abstract.html)

Cristian Bodnar, Fabrizio Frasca, Yuguang Wang, Nina Otter, Guido F. Mont´ufar, Pietro Li´o,
and Michael M. Bronstein. Weisfeiler and Lehman go topological: Message passing simplicial networks. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 1026–1037. PMLR, 2021. URL
[http://proceedings.mlr.press/v139/bodnar21a.html. 29, 30](http://proceedings.mlr.press/v139/bodnar21a.html)

Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M. Bronstein. Improving graph
neural network expressivity via subgraph isomorphism counting. In Graph Representation Learn_ing and Beyond (GRL+) Workshop at the 37 th International Conference on Machine Learning,_
[2020. URL https://arxiv.org/abs/2006.09252. 29](https://arxiv.org/abs/2006.09252)

Robert Brijder, Floris Geerts, Jan Van den Bussche, and Timmy Weerwag. On the expressive power
[of query languages for matrices. ACM TODS, 44(4):15:1–15:31, 2019. URL https://doi.](https://doi.org/10.1145/3331445)
[org/10.1145/3331445. 1, 14, 30](https://doi.org/10.1145/3331445)


-----

Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. In Proceedings of the 2nd International Conference on
_Learning Representations, ICLR, 2014._ [URL https://openreview.net/forum?id=](https://openreview.net/forum?id=DQNsQf-UsoDBa)
[DQNsQf-UsoDBa. 30](https://openreview.net/forum?id=DQNsQf-UsoDBa)

Jin-yi Cai, Martin F¨urer, and Neil Immerman. An optimal lower bound on the number of variables
[for graph identifications. Comb., 12(4):389–410, 1992. URL https://doi.org/10.1007/](https://doi.org/10.1007/BF01305232)
[BF01305232. 2, 3, 6](https://doi.org/10.1007/BF01305232)

Zhengdao Chen, Soledad Villar, Lei Chen, and Joan Bruna. On the equivalence between graph
isomorphism testing and function approximation with GNNs. In Advances in Neural Informa_[tion Processing Systems, volume 32, 2019. URL https://proceedings.neurips.cc/](https://proceedings.neurips.cc/paper/2019/file/71ee911dd06428a96c143a0b135041a4-Paper.pdf)_
[paper/2019/file/71ee911dd06428a96c143a0b135041a4-Paper.pdf. 2, 28](https://proceedings.neurips.cc/paper/2019/file/71ee911dd06428a96c143a0b135041a4-Paper.pdf)

Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. Can graph neural networks
count substructures? In Advances in Neural Information Processing Systems, volume 33, 2020. [URL https://proceedings.neurips.cc/paper/2020/file/](https://proceedings.neurips.cc/paper/2020/file/75877cb75154206c4e65e76b88a12712-Paper.pdf)
[75877cb75154206c4e65e76b88a12712-Paper.pdf. 8, 32](https://proceedings.neurips.cc/paper/2020/file/75877cb75154206c4e65e76b88a12712-Paper.pdf)

Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Li`o, and Petar Veliˇckovi´c. Principal neighbourhood aggregation for graph nets. In Advances in Neural Information Processing
_[Systems, volume 33, 2020. URL https://proceedings.neurips.cc/paper/2020/](https://proceedings.neurips.cc/paper/2020/file/99cad265a1768cc2dd013f0e740300ae-Paper.pdf)_
[file/99cad265a1768cc2dd013f0e740300ae-Paper.pdf. 7, 26, 27](https://proceedings.neurips.cc/paper/2020/file/99cad265a1768cc2dd013f0e740300ae-Paper.pdf)

L. Csanky. Fast parallel matrix inversion algorithms. SIAM J. Comput., 5(4):618–623, 1976. URL

[https://doi.org/10.1137/0205040. 31](https://doi.org/10.1137/0205040)

Radu Curticapean, Holger Dell, and D´aniel Marx. Homomorphisms are a good basis for counting
small subgraphs. In Proceedings of the 49th Symposium on Theory of Computing, STOC, pp.
[210––223, 2017. URL http://dx.doi.org/10.1145/3055399.3055502. 29](http://dx.doi.org/10.1145/3055399.3055502)

Clemens Damke, Vitalik Melnikov, and Eyke H¨ullermeier. A novel higher-order weisfeiler-lehman
graph convolution. In Proceedings of The 12th Asian Conference on Machine Learning, ACML,
volume 129 of Proceedings of Machine Learning Research, pp. 49–64. PMLR, 2020. URL

[http://proceedings.mlr.press/v129/damke20a.html. 28](http://proceedings.mlr.press/v129/damke20a.html)

Micha¨el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks
on graphs with fast localized spectral filtering. In Advances in Neural Information Processing
_[Systems, volume 30, 2016. URL https://proceedings.neurips.cc/paper/2016/](https://proceedings.neurips.cc/paper/2016/file/04df4d434d481c5bb723be1b6df1ee65-Paper.pdf)_
[file/04df4d434d481c5bb723be1b6df1ee65-Paper.pdf. 2, 7, 30](https://proceedings.neurips.cc/paper/2016/file/04df4d434d481c5bb723be1b6df1ee65-Paper.pdf)

Martin F¨urer. Weisfeiler-Lehman refinement requires at least a linear number of iterations. In
_Proceedings of the 28th International Colloqium on Automata, Languages and Programming,_
_ICALP, volume 2076 of Lecture Notes in Computer Science, pp. 322–333. Springer, 2001. URL_
[https://doi.org/10.1007/3-540-48224-5_27. 5](https://doi.org/10.1007/3-540-48224-5_27)

Floris Geerts. The expressive power of kth-order invariant graph networks. CoRR, abs/2007.12035,
[2020. URL https://arxiv.org/abs/2007.12035. 8](https://arxiv.org/abs/2007.12035)

Floris Geerts. On the expressive power of linear algebra on graphs. Theory Comput. Syst., 65(1):
[179–239, 2021. URL https://doi.org/10.1007/s00224-020-09990-9. 1, 14](https://doi.org/10.1007/s00224-020-09990-9)

Floris Geerts, Filip Mazowiecki, and Guillermo A. P´erez. Let’s agree to degree: Comparing graph
convolutional networks in the message-passing framework. In Proceedings of the 38th Interna_tional Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Re-_
_[search, pp. 3640–3649. PMLR, 2021a. URL http://proceedings.mlr.press/v139/](http://proceedings.mlr.press/v139/geerts21a.html)_
[geerts21a.html. 26](http://proceedings.mlr.press/v139/geerts21a.html)

Floris Geerts, Thomas Mu˜noz, Cristian Riveros, and Domagoj Vrgoc. Expressive power of linear
algebra query languages. In Proceedings of the 40th ACM SIGMOD-SIGACT-SIGAI Symposium
_[on Principles of Database Systems, PODS, pp. 342–354. ACM, 2021b. URL https://doi.](https://doi.org/10.1145/3452021.3458314)_
[org/10.1145/3452021.3458314. 1, 2, 14](https://doi.org/10.1145/3452021.3458314)


-----

Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
_[Machine Learning, volume 70, pp. 1263–1272, 2017. URL http://proceedings.mlr.](http://proceedings.mlr.press/v70/gilmer17a/gilmer17a.pdf)_
[press/v70/gilmer17a/gilmer17a.pdf. 2, 3, 6, 42, 43](http://proceedings.mlr.press/v70/gilmer17a/gilmer17a.pdf)

Martin Grohe. The logic of graph neural networks. In Proceedings of the 36th Annual ACM/IEEE
_[Symposium on Logic in Computer Science, LICS, pp. 1–17. IEEE, 2021. URL https://doi.](https://doi.org/10.1109/LICS52264.2021.9470677)_
[org/10.1109/LICS52264.2021.9470677. 3, 5, 6, 17](https://doi.org/10.1109/LICS52264.2021.9470677)

William L. Hamilton. Graph representation learning. _Synthesis Lectures on Artificial Intelli-_
_gence and Machine Learning, 14(3):1–159, 2020._ [URL https://doi.org/10.2200/](https://doi.org/10.2200/S01045ED1V01Y202009AIM046)
[S01045ED1V01Y202009AIM046. 1](https://doi.org/10.2200/S01045ED1V01Y202009AIM046)

William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, volume 30, 2017. [URL https://proceedings.neurips.cc/paper/2017/file/](https://proceedings.neurips.cc/paper/2017/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf)
[5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf. 7, 25](https://proceedings.neurips.cc/paper/2017/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf)

David K. Hammond, Pierre Vandergheynst, and R´emi Gribonval. Wavelets on graphs via spectral graph theory. _Applied and Computational Harmonic Analysis, 30(2):129–150, 2011._
ISSN 1063-5203. doi: https://doi.org/10.1016/j.acha.2010.04.005. [URL https://www.](https://www.sciencedirect.com/science/article/pii/S1063520310000552)
[sciencedirect.com/science/article/pii/S1063520310000552. 30](https://www.sciencedirect.com/science/article/pii/S1063520310000552)

Neil Immerman and Eric Lander. Describing graphs: A first-order approach to graph canonization. In Complexity Theory Retrospective: In Honor of Juris Hartmanis on the Occasion
_[of His Sixtieth Birthday, pp. 59–81. Springer, 1990. URL https://doi.org/10.1007/](https://doi.org/10.1007/978-1-4612-4478-3_5)_
[978-1-4612-4478-3_5. 2](https://doi.org/10.1007/978-1-4612-4478-3_5)

Nicolas Keriven and Gabriel Peyr´e. Universal invariant and equivariant graph neural
networks. In Advances in Neural Information Processing Systems, volume 32, pp.
[7092–7101, 2019. URL https://proceedings.neurips.cc/paper/2019/file/](https://proceedings.neurips.cc/paper/2019/file/ea9268cb43f55d1d12380fb6ea5bf572-Paper.pdf)
[ea9268cb43f55d1d12380fb6ea5bf572-Paper.pdf. 2](https://proceedings.neurips.cc/paper/2019/file/ea9268cb43f55d1d12380fb6ea5bf572-Paper.pdf)

Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In Proceedings of the 5th International Conference on Learning Representations, ICLR,
[2017. URL https://openreview.net/pdf?id=SJU4ayYgl. 2, 7, 25](https://openreview.net/pdf?id=SJU4ayYgl)

Ron Levie, Federico Monti, Xavier Bresson, and Michael M. Bronstein. Cayleynets: Graph convolutional neural networks with complex rational spectral filters. IEEE Trans. Signal Process., 67
[(1):97–109, 2019. URL https://doi.org/10.1109/TSP.2018.2879624. 2, 8, 30, 31](https://doi.org/10.1109/TSP.2018.2879624)

Haggai Maron, Heli Ben-Hamu, and Yaron Lipman. Open problems: Approximation power of
invariant graph networks. In NeurIPS 2019 Graph Representation Learning Workshop, 2019a.
[URL https://grlearning.github.io/papers/31.pdf. 2, 8, 32](https://grlearning.github.io/papers/31.pdf)

Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph networks. In Advances in Neural Information Processing Systems, volume 32, 2019b. [URL https://proceedings.neurips.cc/paper/2019/file/](https://proceedings.neurips.cc/paper/2019/file/bb04af0f7ecaee4aae62035497da1387-Paper.pdf)
[bb04af0f7ecaee4aae62035497da1387-Paper.pdf. 2, 7, 8, 9, 27, 28, 32, 35, 36,](https://proceedings.neurips.cc/paper/2019/file/bb04af0f7ecaee4aae62035497da1387-Paper.pdf)
37, 38, 39

Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph
networks. In Proceedings of the 7th International Conference on Learning Representations,
[ICLR, 2019c. URL https://openreview.net/forum?id=Syx72jC9tm. 2, 32, 35](https://openreview.net/forum?id=Syx72jC9tm)

Christian Merkwirth and Thomas Lengauer. Automatic generation of complementary descriptors
[with molecular graph networks. J. Chem. Inf. Model., 45(5):1159–1168, 2005. URL https:](https://doi.org/10.1021/ci049613b)
[//doi.org/10.1021/ci049613b. 1](https://doi.org/10.1021/ci049613b)

H. L. Morgan. The generation of a unique machine description for chemical structures-a technique
developed at chemical abstracts service. _Journal of Chemical Documentation, 5(2):107–113,_
[1965. URL https://doi.org/10.1021/c160017a018. 3](https://doi.org/10.1021/c160017a018)


-----

Christopher Morris, Martin Ritzert, Matthias Fey, William L. Hamilton, Jan Eric Lenssen, Gaurav
Rattan, and Martin Grohe. Weisfeiler and Leman go neural: Higher-order graph neural networks.
In Proceedings of the 33rd AAAI Conference on Artificial Intelligence, pp. 4602–4609, 2019.
[URL https://doi.org/10.1609/aaai.v33i01.33014602. 1, 2, 7, 25, 28](https://doi.org/10.1609/aaai.v33i01.33014602)

Christopher Morris, Gaurav Rattan, and Petra Mutzel. Weisfeiler and Leman go sparse: Towards
scalable higher-order graph embeddings. In Advances in Neural Information Processing Systems,
[volume 33, 2020. URL https://proceedings.neurips.cc//paper/2020/file/](https://proceedings.neurips.cc//paper/2020/file/f81dee42585b3814de199b2e88757f5c-Paper.pdf)
[f81dee42585b3814de199b2e88757f5c-Paper.pdf. 2, 28](https://proceedings.neurips.cc//paper/2020/file/f81dee42585b3814de199b2e88757f5c-Paper.pdf)

Martin Otto. Bounded Variable Logics and Counting: A Study in Finite Models, volume 9 of Lecture
_[Notes in Logic. Cambridge University Press, 2017. URL https://doi.org/10.1017/](https://doi.org/10.1017/9781316716878)_
[9781316716878. 2, 5, 18](https://doi.org/10.1017/9781316716878)

Martin Otto. Graded modal logic and counting bisimulation. _ArXiv, 2019._ [URL https://](https://arxiv.org/abs/1910.00039)
[arxiv.org/abs/1910.00039. 18](https://arxiv.org/abs/1910.00039)

Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
The graph neural network model. _IEEE Trans. Neural Networks, 20(1):61–80, 2009._ URL

[https://doi.org/10.1109/TNN.2008.2005605. 1](https://doi.org/10.1109/TNN.2008.2005605)

Vlad Timofte. Stone–Weierstrass theorems revisited. Journal of Approximation Theory, 136(1):
[45–59, 2005. URL https://doi.org/10.1016/j.jat.2005.05.004. 9, 36](https://doi.org/10.1016/j.jat.2005.05.004)

Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua
Bengio. Graph attention networks. In Proceedings of the 6th International Conference on
_Learning Representations, ICLR, 2018._ [URL https://openreview.net/forum?id=](https://openreview.net/forum?id=rJXMpikCZ)
[rJXMpikCZ. 27](https://openreview.net/forum?id=rJXMpikCZ)

Felix Wu, Amauri H. Souza Jr., Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Q. Weinberger.
Simplifying graph convolutional networks. In Proceedings of the 36th International Confer_ence on Machine Learning, ICML, volume 97 of Proceedings of Machine Learning Research,_
[pp. 6861–6871. PMLR, 2019. URL http://proceedings.mlr.press/v97/wu19e.](http://proceedings.mlr.press/v97/wu19e.html)
[html. 7, 26](http://proceedings.mlr.press/v97/wu19e.html)

Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In Proceedings of the 7th International Conference on Learning Representations,
[ICLR, 2019. URL https://openreview.net/forum?id=ryGs6iA5Km. 1, 2, 7, 9, 23,](https://openreview.net/forum?id=ryGs6iA5Km)
25, 38

Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov,
and Alexander J Smola. Deep sets. In Advances in Neural Information Processing Systems,
volume 30, 2017. [URL https://proceedings.neurips.cc/paper/2017/file/](https://proceedings.neurips.cc/paper/2017/file/f22e4747da1aa27e363d86d40ff442fe-Paper.pdf)
[f22e4747da1aa27e363d86d40ff442fe-Paper.pdf. 23](https://proceedings.neurips.cc/paper/2017/file/f22e4747da1aa27e363d86d40ff442fe-Paper.pdf)


-----

SUPPLEMENTARY MATERIAL

A RELATED WORK CNT’D

We provide additional details on how the tensor language TL(Ω) considered in this paper relates
to recent work on other matrix query languages. Closest to TL(Ω) is the matrix query language
sum-MATLANG (Geerts et al., 2021b) whose syntax is close to that of TL(Ω). There are, however,
key differences. First, although sum-MATLANG uses index variables (called vector variables), they
all must occur under a summation. In other words, the concept of free index variables is missing,
which implies that no general tensors can be represented. In TL(Ω), we can represent arbitrary tensors and the presence of free index variables is crucial to define vertex, or more generally, k-tuple
embeddings in the context of GNNs. Furthermore, no notion of summation depth was introduced for
sum-MATLANG. In TL(Ω), the summation depth is crucial to assess the separation power in terms
of the number of rounds of color refinement and k-WL. And in fact, the separation power of sumMATLANG was not considered before, and neither are finite variable fragments of sum-MATLANG
and connections to color refinement and k-WL studied before. Finally, no other aggregation functions were considered for sum-MATLANG. We detail in Section C.5 that TL(Ω) can be gracefully
extended to TL(Ω, Θ) for some arbitrary set Θ of aggregation functions.

Connections to 1-WL and 2-WL and the separation power of another matrix query language,
MATLANG (Brijder et al., 2019) were established in Geerts (2021). Yet, the design of MATLANG is
completely different in spirit than that of TL(Ω). Indeed, MATLANG does not have index variables
or explicit summation aggregation. Instead, it only supports matrix multiplication, matrix transposition, function applications, and turning a vector into a diagonal matrix. As such, MATLANG can
be shown to be included in TL3(Ω). Similarly as for sum-MATLANG, MATLANG cannot represent
general tensors, has no (free) index variables and summation depth is not considered (in view of the
absence of an explicit summation).

We also emphasize that neither for MATLANG nor for sum-MATLANG a guarded fragment was
considered. The guarded fragment is crucial to make connections to color refinement (Theorem 4.3).
Furthermore, the analysis in terms of the number of index variables, summation depth and treewidth
(Theorems 4.1,4.2 and Proposition 4.5), were not considered before in the matrix query language
literature. For none of these matrix query languages, approximation results were considered (Section 6.1).

Matrix query languages are used to assess the expressive power of linear algebra. Balcilar et al.
(2021a) use MATLANG and the above mentioned connections to 1-WL and 2-WL, to assess the
separation power of GNNs. More specifically, similar to our work, they show that several GNN
architectures can be represented in MATLANG, or fragments thereof. As a consequence, bounds on
their separation power easily follow. Furthermore, Balcilar et al. (2021a) propose new architectures
inspired by special operators in MATLANG. The use of TL(Ω) can thus been seen as a continuation
of their approach. We note, however, that TL(Ω) is more general than MATLANG (which is included
in TL3(Ω)), allows to represent more complex linear algebra computations by means summation
(or other) aggregation, and finally, provides insights in the number of iterations needed for color
refinement and k-WL. The connection between the number of variables (or treewidth) and k-WL is
not present in the work by Balcilar et al. (2021a), neither is the notion of guarded fragment, needed
to connect to color refinement. We believe that it is precisely these latter two insights that make the
tensor language approach valuable for any GNN designer who wishes to upper bound their GNN
architecture.

B DETAILS OF SECTION 3

B.1 PROOF OF PROPOSITION 3.1

Let G = (V, E, col) be a graph and let σ be a permutation of V . As usual, we define
_σ ⋆G = (V_ _[σ], E[σ], col[σ]) as the graph with vertex set V_ _[σ]_ := V, edge set vw ∈ _E[σ]_ if and only
if σ[−][1](v)σ[−][1](w) ∈ _E, and col[σ](v) := col(σ[−][1](v)). We need to show that for any expression φ(x)_
in TL(Ω) either [[φ, σ ⋆ **_v]]σ⋆G = [[φ, v]]G, or when φ has no free index variables, [[φ]]σ⋆G = [[φ]]G._**
We verify this by a simple induction on the structure of expressions in TL(Ω).


-----

-  If φ(xi, xj) = 1xi op xj, then for a valuation ν mapping xi to vi and xj to vj in V :

[[1xi op xj _, ν]]G = 1vi op vj = 1σ(vi) op σ(vj_ ) = [[1xi op xj _, σ ⋆ν]]σ⋆G,_

where we used that σ is a permutation.

-  If φ(xi) = Pℓ(xi), then for a valuation µ mapping xi to vi in V :

[[Pℓ, µ]]G = (col(vi))ℓ = (col[σ](σ(vi))ℓ = [[Pℓ, σ ⋆ν]]σ⋆G,

where we used the definition of col[σ].

-  Similarly, if φ(xi, xj) = E(xi, xj), then for a valuation ν assigning xi to vi and xj to vj:

[[φ, ν]]G = 1vivj _E = 1σ(vi)σ(vj_ ) _Eσ = [[φ, σ ⋆ν]]σ⋆G,_
_∈_ _∈_

where we used the definition of E[σ].

-  If φ(x) = φ1(x1) _φ2(x2), then for a valuation ν from x to V :_
_·_

[[φ, ν]]G = [[φ1, ν]]G · [[φ2, ν]]G = [[φ1, σ ⋆ν]]σ⋆G · [[φ2, σ ⋆ν]]σ⋆G = [[φ, σ ⋆ν]]σ⋆G,

where we used the induction hypothesis for φ1 and φ2. The cases φ(x) = φ1(x1) + φ2(x2) and
_φ(x) = a_ _φ1(x) are dealt with in a similar way._
_·_

-  If φ(x) = f (φ1(x1), . . ., φp(xp)), then



[[φ, ν]]G = f ([[φ1, ν]]G, . . ., [[φp, ν]]G)
= f ([[φ1, σ ⋆ν]]σ⋆G, . . ., [[φp, σ ⋆ν]]σ⋆G)
= [[φ, σ ⋆ν]]σ⋆G,

where we used again the induction hypothesis for φ1, . . ., φp.

-  Finally, if φ(x) = _y_ _[φ][1][(][x][, y][)][ then for a valuation][ ν][ of][ x][ to][ V][ :]_

[[φ, ν]]G = [[φ1, ν[y _v]]]G =_ [[φ1, σ ⋆ν[y _v]]]σ⋆G_

[P] _7→_ _7→_

_vX∈V_ _vX∈V_


= [[φ1, σ ⋆ν[y 7→ _v]]]σ⋆G = [[φ, σ ⋆ν]]σ⋆G,_

_vX∈V_ _[σ]_

where we used the induction hypothesis for φ1 and that V _[σ]_ = V because σ is a permutation.

We remark that when φ does not contain free index variables, then [[φ, ν]]G = [[φ]]G for any valuation ν, from which invariance follows from the previous arguments. This concludes the proof of
Proposition 3.1.

C DETAILS OF SECTION 4

In the following sections we prove Theorem 4.1, 4.2, 4.3 and 4.4. More specifically, we start by
showing these results in the setting that TL(Ω) only supports summation aggregation ([P]x _[e][) and in]_

which the vertex-labellings in graphs take values in {0, 1}[ℓ]. In this context, we introduce classical
logics in Section C.1 and recall and extend connections between the separation power of these logics
and the separation power of color refinement and k-WL in Section C.2. We connect TL(Ω) and
logics in Section C.3, to finally obtain the desired proofs in Section C.4. We then show how these
results can be generalized in the presence of general aggregation operators in Section C.5, and to the
setting where vertex-labellings take values in R[ℓ] in Section C.6.

C.1 CLASSICAL LOGICS

In what follows, we consider graphsdefining the k-variable fragment C[k] Gof first-order logic with counting quantifiers, followed by the = (VG, EG, colG) with colG : VG →{0, 1}[ℓ]. We start by
definition of the guarded fragment GC of C[2]. Formulae φ in C[k] are defined over the set _x1, . . ., xk_
_{_ _}_
of variables and are formed by the following grammar:

_φ := (xi = xj) | E(xi, xj) | Ps(xi) | ¬φ | φ ∧_ _φ | ∃[≥][m]xi φ,_


-----

where i, j ∈ [k], E is a binary predicate, Ps for s ∈ [ℓ] are unary predicates for some ℓ _∈_ N, and
_m ∈_ N. The semantics of formulae in C[k] is defined in terms of interpretations relative to a given
graph G and a (partial) valuation µ : _x1, . . ., xk_ _VG. Such an interpretation maps formulae,_
_{_ _} →_
graphs and valuations to Boolean values B := {⊥, ⊤}, in a similar way as we did for tensor language
expressions.

More precisely, given a graph G = (VG, EG, colG) and partial valuation µ : _x1, . . ., xk_ _VG,_
_{_ _} →_
we define [[φ, µ]][B]G

_[∈]_ [B][ for valuations defined on the free variables in][ φ][. That is, we define:]

[[xi = xj, µ]][B]G [:= if][ µ][(][x][i][) =][ µ][(][x][j][)][ then][ ⊤] [else][ ⊥][;]

[[E(xi, xj), µ]][B]G [:= if][ µ][(][x][i][)][µ][(][x][j][)][ ∈] _[E][G]_ [then][ ⊤] [else][ ⊥][;]

[[Ps(xi), µ]][B]G [:= if][ col][G][(][µ][(][x][i][))][s] [= 1][ then][ ⊤] [else][ ⊥][;]

[[¬φ, µ]][B]G [:=][ ¬][[][φ, µ]][B]G[;]

[[φ1 ∧ _φ2, µ]][B]G_ [:= [][φ1, µ]][B]G _[∧]_ [[][φ2, µ]][B]G[;]

[[∃[≥][m]xi φ1, µ]][B]G [:=][ if][ |{][v][ ∈] _[V][G]_ _[|][ [][φ, µ[xi 7→_ _v]]][B]G_ [=][ ⊤}| ≥] _[m][ then][ ⊤]_ [else][ ⊥][.]

In the last expression, µ[xi 7→ _v] denotes the valuation µ modified such that it maps xi to vertex v._

We will also need the guarded fragment GC of C[2] in which we only allow equality conditions of the
form xi = xi, component expressions of conjunction and disjunction should have the same single
free variable, and counting quantifiers can only occur in guarded form: _x2(E(x1, x2)_ _φ(x2))_
_∃[≥][m]_ _∧_
or _x1(E(x2, x1)_ _φ(x1)). The semantics of formulae in GC is inherited from formulae in C[2]._
_∃[≥][m]_ _∧_

Finally, we will also consider C[k] _ω[, that is, the logic][ C][k][ extended with][ infinitary disjunctions][ and]_
_∞_
_conjunctions. More precisely, we add to the grammar of formulae the following constructs:_

_φα and_ _φα_
_α_∈A_ _α^∈A_

where the index set A can be arbitrary, even containing uncountably many indices. We define
GC _ω in the same way by relaxing the finite variable conditions. The semantics is, as expected:_
_∞_

[[[W]α _A_ _[φ][α][, µ][]]][B]G_ [=][ ⊤] [if for at least one][ α][ ∈] _[A][,][ [][φα, µ]][B]G_ [=][ ⊤][, and][ [][[V]α _A_ _[φ][α][, µ][]]][B]G_ [=][ ⊤] [if for all]
_∈_ _∈_
_α ∈_ _A, [[φα, µ]][B]G_ [=][ ⊤][.]

We define the free variables of formulae just as for TL, and similarly, quantifier rank is defined as
summation depth (only existential quantifications increase the quantifier rank). For any of the above
logics L we define L[(][t][)] as the set of formulae in L of quantifier rank at most t.

To capture the separation power of logics, we define ρ1 as the equivalence relation on 1
_L[(][t][)][]_ _G_
defined by
 
(G, v), (H, w) _∈_ _ρ1_ _L[(][t][)][]_ _⇐⇒∀φ(x) ∈L[(][t][)]_ : [[φ, µv]][B]G [= [][φ, µw]][B]H _[,]_

where µv is any valuation such that     µ(x) = v, and likewise for w. The relation ρ0 is defined in
a similar way, except that now the relation is only over pairs of graphs, and the characterization is
over all formulae with no free variables (also called sentences). Finally, we also use, and define,
the relation ρs, which relates pairs from _s: consisting of a graph and an s-tuple of vertices. The_
_G_
relation is defined as
(G, v), (H, w) _∈_ _ρs_ _L[(][t][)][]_ _⇐⇒∀φ(x) ∈L[(][t][)]_ : [[φ, µv]][B]G [= [][φ, µw]][B]H _[,]_

where x consist of  _s free variables and_   _µv is a valuation assigning the i-th variable of x to the i-th_
value of v, for any i ∈ [s].

C.2 CHARACTERIZATION OF SEPARATION POWER OF LOGICS

We first connect the separation power of the color refinement and k-dimensional Weisfeiler-Leman
algorithms to the separation power of the logics we just introduced. Although most of these connections are known, we present them in a bit of a more fine-grained way. That is, we connect the
number of rounds used in the algorithms to the quantifier rank of formulae in the above logics.
**Proposition C.1. For any t ≥** 0, we have the following identities:


-----

_(1) ρ1_ cr[(][t][)][] = ρ1 GC[(][t][)][] _and ρ0_ gcr[(][t][)][] = ρ0 gwl[(]1[t][)] = ρ0 C[2][,][(][t][+1)][];

_(2) For  k_ 1, ρ1  vwl[(]k[t][)] = ρ1 C [k][+1][,][(][t][)][] _and_     
_≥_
  ρ0 C[k] [+1][,][(][t][+][k][)][] _ρ0_ gwl[(]k[t][)] _ρ0_ C[k][+1][,][(][t][+1)][].

_⊆_ _⊆_
      

_As a consequence, ρ0_ gwl[(]k[∞][)] = ρ0 C[k][+1][].
    

_Proof. For (1), the identity ρ1_ cr[(][t][)][] = ρ1 GC[(][t][)][] is known and can be found, for example, in
Theorem V.10 in Grohe (2021). The identity ρ0 gcr[(][t][)][] = ρ0 gwl[(]1[t][)] can be found in Proposition
   

V.4 in Grohe (2021). The identity ρ0 gwl[(]1[t][)] = ρ0 C[2][,][(][t][+1)][] is a consequence of the inclusion

    

shown in (2) for k = 1.

    

For (2), we use that ρk wl[(]k[t][)] = ρk C[k][+1][,][(][t][)][], see e.g., Theorem V.8 in Grohe (2021). We ar
gue that this identity holds for ρ1 vwl[(]k[t][)] = ρ1 C[k][+1][,][(][t][)][]. Indeed, suppose that (G, v) and (H, w)

    

are not in ρ1 C[k][+1][,][(][t][)][]. Let φ(x1) be a formula in C[k][+1][,][(][t][)] such that [[φ, v]][B]G [φ, w]][B]H [. Con-]

    

_[̸][= []_

[sider the formula[φ[+], (w, . . ., w  )]][B]H φ[, and hence][+](x1, . . ., x[ (]k[G,]) =[ (][v, . . ., v] φ(x1)[))] ∧[ and][V]i[k][ (]=1[H,][(][x][ (][1][ =][w, . . ., w][ x][i][)][.][))][ are not in]Then, [[φ[+],[ ρ] (v, . . ., v[k] C[k][+1][,])][(][t]][)][B]G[] ei-[̸][=]
ther. This implies that wl[(]k[t][)][(][G,][ (][v, . . ., v][))][ ̸][=][ wl]k[(][t][)][(][H,][ (][w, . . ., w][))][, and thus, by definition,] 

vwl[(]k[t][)][(][G, v][)][ ̸][=][ vwl]k[(][t][)][(][H, w][)][. In other words,][ (][G, v][)][ and][ (][H, w][)][ are not in][ ρ][1] vwl[(]k[t][)], from which

the inclusion ρ1 vwl[(]k[t][)] _ρ1_ C[k][+1][,][(][t][)][] follows. Conversely, if (G, v) and (H, w) are not in

_⊆_   
_ρ1_ vwl[(]k[t][)], then wl  [(]k[t][)][(][G,][ (][v, . . ., v]  [))][ ̸][=][ wl]k[(][t][)][(][H,][ (][w, . . ., w][))][. As a consequence,][ (][G,][ (][v, . . ., v][))][ and]

(H, (w, . . ., w)) are not in ρk C[k][+1][,][(][t][)][] either. Let φ(x1, . . ., xk) be a formula in C[k][+1][,][(][t][)] such that

  

[[φ, (v, . . ., v)]][B]G [φ, (w, . . ., w)]][B]H [. Then it is readily shown that we can convert][ φ][(][x][1][, . . ., x][k][)]
 
into a formulanot in ρ1 C[k][+1] φ[,][(][t][−][)][̸][= [][](. Hence, we also have the inclusionx1) in C[k][+1][,][(][t][)] such that [[φ[−], v]][B]G _[̸][= [] ρ[1φ[−]vwl, w[(]k[t]]][)][B]H_ [, and thus]ρ1 C[k][+1][ (][G, v][,][(][t][)][], form which the[)][ and][ (][H, w][)][ are]

_⊇_
first identity in (2) follows.

      

It remains to show ρ0 C[k][+1][,][(][t][+][k][)][] _ρ0_ gwl[(]k[t][)] _ρ0_ C[k][+1][,][(][t][+1)][]. Clearly, if (G, H) is not in
_⊆_ _⊆_

_ρ0_ gwl[(]k[t][)] then the multisets of labels  wl[(]k[t][)] [(][G,][ v][)][ and][ wl] k[(][t][)][(][H,][ w][)][ differ. It is known that with each]

label  _c one can associate a formula_ _φ[c]_ in C[k][+1][,][(][t][)] such that [[φ[c], v]][B]G [=][ ⊤] [if and only if][ wl]k[(][t][)][(][G,][ v][) =]

_c. So, if the multisets are different, there must be a c that occurs more often in one multiset than_
in the other one. This can be detected by a fomulae of the form (x1, . . ., xk)φ[c](x1, . . ., xk)
_∃[=][m]_
which is satisfied if there are m tuples v with label c. It is now easily verified that the latter formula
can be converted into a formula in C[k][+1][,][(][t][+][k][)]. Hence, the inclusion ρ0 C[k][+1][,][(][t][+][k][)][] _ρ0_ gwl[(]k[t][)]
_⊆_

follows.

    

For ρ0 gwl[(]k[t][)] _ρ0_ C[k][+1][,][(][t][+1)][], we show that if (G, H) is in ρ0 gwl[(]k[t][)], then this implies that

_⊆_

[[φ, µ]][B]G  [= [][φ, µ ]][B]H [for all formulae in]  [ C][k][+1][,][(][t][+1)][ and any valuation][ µ]  [ (notice that] _[ µ][ is superfluous]_
in this definition when formulas have no free variables). Assume that (G, H) is in ρ0(gwl[(]k[t][)][)][. Since]

any formula of quantifier rank t+1 is a Boolean combination of formulas of less rank or a formula of
the form φ = ∃[≥][m]xi ψ where ψ is of quantifier rank t, without loss of generality consider a formula
of the latter form, and assume for the sake of contradiction that [[φ, µ]][B]G [=][ ⊤] [but][ [][φ, µ]][B]H [=][ ⊥][.]
Since [[φ, µ]][B]G [=][ ⊤][, there must be at least][ m][ elements satisfying][ ψ][. More precisely, let][ v][1][, . . ., v][p]
in G be all vertices in G such that for each valuation µ[x 7→ _vi] it holds that [[ψ, µ[x 7→_ _vi]][B]G_ [=][ ⊤][.]
As mentioned, it must be that p is at least m. Using again the fact that ρk wl[(]k[t][)] = ρk C[k][+1][,][(][t][)][], we

infer that the color wl[(]k[t][−][1)](G, (vi, . . ., vi)) is the same, for each such vi.    

Now since gwl[(]k[t][−][1)](G) = gwl[(]k[t][−][1)](H), it is not difficult to see that there must be exactly p vertices w1, . . ., wp in H such that wl[(]k[t][−][1)](G, (vi, . . ., vi)) = wl[(]k[t][−][1)](H, (wi, . . ., wi)). Otherwise, it
would simply not be the case that the aggregation step of the colors, assigned by k-WL is the same
in G and H. By the connection to logic, we again know that for valuation µ[x _wi] it holds that_
_7→_

[[ψ, µ[x 7→ _wi]][B]H_ [=][ ⊤][. It then follows that][ [][φ, µ]][B]H [=][ ⊤] [for any valuation][ µ][, which was to be]
shown.

Finally, we remark that ρ0 gwl[(]k[∞][)] = ρ0 C[k][+1][] follows from the preceding inclusions in (2).
    


-----

Before moving to tensor languages, where we will use infinitary logics to simulate expressions in
TLk(Ω) and GTL(Ω), we recall that, when considering the separation power of logics, we can freely
move between the logics and their infinitary counterparts:

**Theorem C.2. The following identities hold for any t ≥** 0, k ≥ 2 and s ≥ 0:

_(1) ρ1_ GC[(]∞[t][)] _ω_ = ρ1 GC[(][t][)][];

_(2) ρs C[k,]∞[(]ω[t][)]_ = ρs C [k,][(][t][)][].
    

_Proof. For identity (1), notice that we only need to prove that ρ1_ GC[(][t][)][] _⊆_ _ρ1_ GC[(]∞[t][)] _ω_, the other

direction follows directly from the definition. We point out the well-known fact that two tuples
(G, v) and (H, w) belong to ρ1 GC[(][t][)][] if and only if the unravelling  of G rooted at  _v up to depth_ _t is_
isomorphic to the unravelling of H rooted at w up to root t. Here the unravelling is the infinite tree
 
whose root is the root node, and whose children are the neighbors of the root node (see e.g. Barcel´o
et al. (2020); Otto (2019). Now for the connection with infinitary logic. Assume that the unravellings
of G rooted at v and of H rooted at w up to level t are isomorphic, but assume for the sake of
contradiction that there is a formula φ(x) in GC[(][t][)] _ω_ [such that][ [][φ, µv]][B]G [φ, µw]][B]H [, where][ µ][v][ and]

_∞_
_µw are any valuation mapping variable x to v and w, respectively. Now since[̸][= []_ _G and H are finite_
graphs, one can construct, from formulaNotice that this is in contradiction with our assumption that unravellings where isomorphic and ϕ, a formula ϕ[′] in GC[(][t][)] such that [[ψ, µv]][B]G _[̸][= [][ψ, µw]][B]H_ [.]
therefore indistinguishable by formulae in GC[(][t][)]. To construct ψ, consider an infinitary disjunction

_a_ _A_ _[α][a][. Since][ G][ and][ H][ have a finite number of vertices, and the formulae have a finite number]_
_∈_
of variables, the number of different valuations from the variables to the vertices in G or H is also
finite. Thus, one can replace any extra copy ofW _αa, αa′ such that their value is the same in G and H._
The final result is a finite disjunction, and the truth value over G and H is equivalent to the original
infinitary disjunction.

For identity (2) we refer to Corollary 2.4 in Otto (2017).

C.3 FROM TL(Ω) TO C[k]∞ω [AND][ GC][∞][ω]

We are now finally ready to make the connection between expressions in TL(Ω) and the infinitary
logics introduced earlier.

**Proposition C.3. For any expression φ(x) in TLk(Ω) and c ∈** R, there exists an expression ˜φ[c](x)
_in C[k]_ _ω_ _[such that][ [][φ, v]]G = c if and only if [[ ˜φ[c], v]][B]G_ [=][ ⊤] _[for any graph][ G][ = (][V][G][, E][G][,][ col][G][)][ in][ G]_
_∞_
_and v_ _VG[k][. Furthermore, if][ φ][(][x][)][ ∈]_ [GTL][(Ω)][ then][ ˜]φ[c] GC _ω. Finally, if φ has summation depth_
_∈_ _∈_ _∞_
_t then ˜φ[c]_ _has quantifier rank t._

_Proof. We define ˜φ[c]_ inductively on the structure of expressions in TLk(Ω).

-  φ(xi, xj) := 1xi op xj . Assume first that op is “=”. We distinguish between (a) i = j and
_̸_
(b) i = j. For case (a), if c = 1, then we define ˜φ[1](xi, xj) := (xi = xj), if c = 0, then we
define ˜φ[0](xi, xj) := ¬(xi = xj), and if c ̸= 0, 1, then we define ˜φ[c](xi, xj) := xi ̸= xi.
For case (b), if c = 1, then we define ˜φ[1](xi, xj) := (xi = xi), and for any c ̸= 1, we
define ˜φ[c](xi, xj) := ¬(xi = xi). The case when op is “≠ ” is treated analogously.

-  φ(xi) := Pℓ(xi). If c = 1, then we define ˜φ[1](xi) := Pℓ(xi), if c = 0, then we define
_φ˜[0](xi) := ¬Pj(xi). For all other c, we define ˜φ[c](xi, xj) := ¬(xi = xi)._

-  φ(xi, xj) := E(xi, xj). If c = 1, then we define ˜φ[1](xi, xj) := E(xi, xj), if c = 0, then
we define ˜φ[0](xi, xj) := ¬E(xi, xj). For all other c, we define ˜φ[c](xi, xj) := ¬(xi = xi).

-  φ[[φ :=1, v φ]]G1 + = φ c12 and. We observe that [[φ2, v]]G = c [2[ andφ, v c]]G = = c c1 + if and only if there are c2. Hence, it suffices to define c1, c2 ∈ R such that


_φ˜[c]_ :=


_φ˜[c]1[1]_ _[∧]_ _φ[˜][c]2[2]_ _[,]_


_cc=1,cc12+∈cR2_


-----

where ˜φ[c]1[1] [and][ ˜]φ[c]2[2] [are the expressions such that][ [][φ1, v]]G = c1 if and only if [[ ˜φ[c]1[1] _[,][ v][]]][B]G_ [=]
_⊤_ and [[φ2, v]]G = c2 if and only if [[ ˜φ[c]2[2] _[,][ v][]]][B]G_ [=][ ⊤][, which exist by induction.]

-  φonly if there areHence, it suffices to define := φ1 · φ2. This is case is analogous to the previous one. Indeed, c1, c2 ∈ R such that [[φ1, v]]G = c1 and [[φ2, v]]G = [ c[φ,2 and v]]G c = = c c if and1 · c2.
_φ˜[c]_ := _cc1=,c_c21∈cR2_ _φ˜[c]1[1]_ _[∧]_ _φ[˜][c]2[2]_ _[.]_

_·_

-  φ := a · φ1. This is case is again dealt with in a similar way. Indeed, [[φ, v]]G = c if and
only if there is a c1 ∈ R such that [[φ1, v]]G = c1 and c = a · c1. Hence, it suffices to define

_φ˜[c]_ := _φ˜[c]1[1]_ _[.]_

_cc=_1a∈Rc1_

_·_

-  φ := f (φ1, . . ., φp) with f : R[p] _→_ R. We observe that [[φ, v]]G = c if and only if there
aresuffices to define c1, . . ., cp ∈ R such that c = f (c1, . . ., cp) and [[φi, v]]G = ci for i ∈ [p]. Hence, it
_φ˜[c]_ := _c=c1f,...,c(c_1,...,cp∈Rp)_ _φ˜[c]1[1]_ _[∧· · · ∧]_ _φ[˜][c]p[p]_ _[.]_

-  φ := _xi_ _[φ][1][. We observe that][ [][φ, µ]]G = c implies that we can partition VG into ℓ_ parts

_V1, . . ., Vℓ, of sizes m1, . . ., mℓ, respectively, such that [[φ1, µ[xi →_ _v]]]G = ci for each_
_v ∈_ _Vi[P], and such that all ci’s are pairwise distinct and c =_ _i=1_ _[c][i][ ·][ m][i][. It now suffices to]_
consider the following formula

_ℓ_ [P][ℓ] _ℓ_

_φ˜[c]_ := _ℓ,mc11,...,c,...,m_ℓ∈ℓR∈N_ _i^=1_ _∃[=][m][i]_ _xi ˜φ[c]1[i]_ _[∧∀][x][i]_ _i_=1_ _φ˜[c]1[i]_ _[,]_

_c=[P][ℓ]i=1_ _[m][i][c][i]_

where ∃[=][m][i] _xi ψ is shorthand notation for ∃[≥][m][i]_ _xi ψ ∧¬∃[≥][m][i][+1]xi ψ, and ∀xi ψ denotes_
_xi_ _ψ._
_¬∃[≥][1]_ _¬_

This concludes the construction of ˜φ[c]. We observe that we only introduce a quantifiers when φ =

_xi_ _[φ][1][ and hence if we assume by induction that summation depth and quantifier rank are in sync,]_
then ifPsummation depth φ1 has summation depth t, and as can be seen from the definition of t _−_ 1 and thus ˜φ[c]1 [has quantifier rank] ˜φ[c], this formula has quantifier rank[ t] _[−]_ [1][ for any][ c][ ∈] [R][, then][ φ][ has] t,
as desired.

It remains to verify the claim about guarded expressions. This is again verified by induction. The
only case requiring some attention is φ(x1) := _x2_ _[E][(][x][1][, x][2][)][ ∧]_ _[φ][1][(][x][2][)][ for which we can define]_

_ℓ_

_φ˜[c]_ := _∃[=][m]x2E(x1, x[P]2) ∧_ _∃[=][m][i]_ _x2 E(x1, x2) ∧_ _φ˜[c]1[i]_ [(][x][2][)][,]

_ℓ,mc11,...,c,...,m_ℓ∈ℓR∈N_ _i^=1_
_c=[P][ℓ]i=1_ _[m][i][c][i]_
_m=[P][ℓ]i=1_ _[m][i]_

which is a formula in GC again only adding one to the quantifier rank of the formulae ˜φ[c]1 [for][ c][ ∈]
R. So also here, we have the one-to-one correspondence between summation depth and quantifier
rank.


C.4 PROOF OF THEOREM 4.1, 4.2, 4.3 AND 4.4

**Proposition C.4. We have the following inclusions: For any t ≥** 0 and any collection Ω _of functions:_

-  ρ1 cr[(][t][)][] _ρ1_ GTL[(][t][)](Ω) _;_
_⊆_
    


-----

-  ρ1 vwl[(]k[t][)]
 

-  ρ0 gwl[(]k[t][)]
 


_⊆_ _ρ1_ TL[(]k[t]+1[)] [(Ω)] _; and_
  

_⊆_ _ρ0_ TL[(]k[t]+1[+1)][(Ω)] _._
  


_Proof. We first show the second bullet by contraposition. That is, we show that if (G, v) and (H, w)_
are not in ρ1 TL[(]k[t]+1[)] [(Ω)], then neither are they in ρ1 vwl[(]k[t][)] . Indeed, suppose that there exists an

expression φ(x1) in TL[(]k[t]+1[)] [(Ω)][ such that][ [][φ, v]]G = c = c[′] = [[φ, w]]H . From Proposition C.3 we

     ̸ 

know that there exists a formula ˜φ[c] in C[k][+1]ω _[,][(][t][)]_ such that [[ ˜φ[c], v]][B]G [=][ ⊤] [and][ [][ ˜φ[c], w]][B]H [=][ ⊥][. Hence,]

_∞_
(G, v) and (H, w) do no belong to ρ1 C[k]∞[+1]ω _[,][(][t][)]_ . Theorem C.2 implies that (G, v) and (H, w) also

do not belong to ρ1 C[k][+1][,][(][t][)][]. Finally, Proposition C.1 implies that (G, v) and (H, w) do not belong

  

to ρ1(vwl[(]k[t][)], as desired. The third bullet is shown in precisely the same, but using the identities for
 

_ρ0 rather than_ _ρ1, and gwl[(]k[t][)]_ [rather than][ vwl]k[(][t][)][.]

Also the first bullet is shown in the same way, using the connection between GTL[(][t][)](Ω), GC[2][,][(]ω[t][)][,]

_∞_
GC[(][t][)] and cr[(][t][)], as given by Proposition C.1, Theorem C.2, and Proposition C.3.

We next show that our tensor languages are also more separating than the color refinement and
_k-dimensional Weisfeiler-Leman algorithms._

**Proposition C.5. We have the following inclusions: For any t ≥** 0 and any collection Ω _of functions:_

-  ρ1 GTL[(][t][)](Ω) _ρ1_ cr[(][t][)][];
_⊆_

-  ρ1 TL[(]k[t]+1[)] [(Ω)] _ρ1 vwl[(]k[t][)]_ _; and_

_⊆_

     

-  ρ0 TLk[(][t]+1[+][k][)][(Ω)] _⊆_ _ρ0_ gwl[(]k[t][)] _._
     


_Proof. For any of these inclusions to hold, for any Ω, we need to show the inclusion without the_
use of any functions. We again use the connections between the color refinement and k-dimensional
Weisfeiler-Leman algorithms and finite variable logics as stated in Proposition C.1. More precisely,
we show for any formula φ(x) ∈ C[k,][(][t][)] there exists an expression ˆφ(x) ∈ TL[(]k[t][)] [such that for any]

graph G in G, [[φ, v]][B]G [=][ ⊤] [implies][ [][ ˆφ, v]]G = 1 and [[φ, v]][B]G [=][ ⊥] [implies][ [][ ˆφ, v]]G = 0. By
appropriately selecting k and t and by observing that when φ(x) ∈ GC then ˆφ(x) ∈ GTL, the
inclusions follow.

The construction of ˆφ(x) is by induction on the structure of formulae in C[k].

-  φ := (xi = xj). Then, we define ˆφ := 1xi=xj .

-  φ := Pℓ(xi). Then, we define ˆφ := Pℓ(xi).

-  φ := E(xi, xj). Then, we define ˆφ := E(xi, xj).

-  φ := _φ1. Then, we define ˆφ := 1xi=xi_ _φˆ1._
_¬_ _−_

-  φ := φ1 _φ2. Then, we define ˆφ := ˆφ1_ ˆφ2.
_∧_ _·_

-  φ := ∃[≥][m]xi φ1. Consider a polynomial p(x) := _j_ _[a][j][x][j][ such that][ p][(][x][) = 0][ for][ x][ ∈]_

_{0, 1, . . ., m −_ 1} and p(x) = 1 for x ∈{m, m + 1, . . ., nj _}. Such a polynomial exists by_
interpolation. Then, we define ˆφ := _j_ _[a][j]_ _xi_ _φ[ˆ]1[P]._

 P 

We remark that we here crucially rely on the assumption that[P] contains graphs of fixed size n
_G_
and that TLk is closed under linear combinations and product. Clearly, if φ ∈ GC, then the above
translations results in an expression ˆφ ∈ GTL(Ω). Furthermore, the quantifier rank of φ is in oneto-one correspondence to the summation depth of ˆφ.

We can now apply Proposition C.1. That is, if (G, v) and (H, w) are not in ρ1 cr[(][t][)][] then by Proposition C.1, there exists a formula φ(x) in GC[(][t][)] such that [[φ, v]][B]G [=][ ⊤̸][= [][φ, w ]][B]H [=][ ⊥][. We have just]
shown when we consider ˜φ, in GTL[(][t][)], also [[ ˜φ, v]]G ̸= [[ ˜φ, w]]H holds. Hence, (G, v) and (H, w)


-----

are not in ρ1 GTL[(][t][)](Ω) either, for any Ω. Hence, ρ1 GTL[(][t][)](Ω) _ρ1_ cr[(][t][)][] holds. The other
_⊆_
bullets are shown in the same way, again by relying on Proposition C.1 and using that we can move
       
from vwl[(]k[t][)] [and][ gwl]k[(][t][)] [to logical formulae, and to expressions in][ TL]k[(][t]+1[)] [and][ TL]k[(][t]+1[+][k][)][, respectively, to]

separate (G, v) from (H, w) or G from H, respectively.

Theorems 4.1, 4.2, 4.3 and 4.4 now follow directly from Propositions C.4 and C.5.

C.5 OTHER AGGREGATION FUNCTIONS

As is mentioned in the main paper, our upper bound results on the separation power of tensor languages (and hence also of GNNs represented in those languages) generalize easily when other aggregation functions than summation are used in TL expressions.

To clarify what we understand by an aggregation function, let us first recall the semantics of
summation aggregation. Let φ := _xi_ _[φ][1][, where][ P]xi_ [represents summation aggregation, let]

_G = (VG, EG, colG) be a graph, and let ν be a valuation assigning index variables to vertices in VG._
The semantics is then given by:

[P]

[[[P]xi _[φ][1][, ν][]]]G :=_ _v_ _VG_ [[][φ1, ν[xi _v]]]G,_

_∈_ _7→_

as explained in Section 3. Semantically, we can alternatively view _xi_ _[φ][1][ as a function which takes]_

the sum of the elements in the following multiset of real values:[P]

[[φ1, ν[xi _v]]]G_ _v_ _VG_ _.[P]_
_{{_ _7→_ _|_ _∈_ _}}_

One can now consider, more generally, an aggregation function F as a function which assigns to
any multiset of values in R a single real value. For example, F could be max, min, mean, . . .. Let
Θ be such a collection of aggregation functions. We next incorporate general aggregation function
in tensor language.

First, we extend the syntax of expressions in TL(Ω) by generalizing the construct _xi_ _[φ][ in the]_

grammar of TL(Ω) expression. More precisely, we define TL(Ω, Θ) as the class of expressions,
formed just like tensor language expressions, but in which two additional constructs, unconditional

[P]

and conditional aggregation, are allowed. For an aggregation function F we define:

aggr[F]xj [(][φ][)] and aggr[F]xj _φ(xj)_ _E(xi, xj)_ _,_
_|_

where in the latter construct (conditional aggregation) the expression  _φ(xj) represents a TL(Ω, Θ)_
expression whose only free variable is xj. The intuition behind these constructs is that unconditional aggregation aggr[F]xj [(][φ][)][ allows for aggregating, using aggregate function][ F] [, over the values of]
_φ where xj ranges unconditionally over all vertices in the graph. In contrast, for conditional aggre-_
gation aggr[F]xj _φ(xj)_ _E(xi, xj)_, aggregation by F of the values of φ(xj) is conditioned on the
_|_
neighbors of the vertex assigned to xi. That is, the vertices for xj range only among the neighbors
  
of the vertex assigned to xi.

More specifically, the semantics of the aggregation constructs is defined as follows:

[[aggr[F]xj [(][φ][)][, ν][]]]G := F ({{[[φ, ν[xj 7→ _v]]]G | v ∈_ _VG}}) ∈_ R.

[[aggr[F]xj _φ(xj)_ _E(xi, xj)_ _, ν]]G := F (_ [[φ, ν[xj _v]]]G_ _v_ _VG, (ν(xi), v)_ _EG_ ) R.
_|_ _{{_ _7→_ _|_ _∈_ _∈_ _}}_ _∈_

We remark that we can also consider aggregations functions   _F over multisets of values in R[ℓ]_ for
some ℓ _∈_ N. This requires extending the syntax with aggr[F]xj [(][φ][1][, . . ., φ][ℓ][)][ for unconditional ag-]
gregation and with aggr[F]xj _φ1(xj), . . ., φℓ(xj)_ _E(xi, xj)_ for conditional aggregation. The se_|_
mantics is as expected: F ( (([[φ1, ν[xj _v]]]G, . . ., [[φℓ, ν[xj_ _v]]]G)_ _v_ _VG_ ) R and
_F_ ({{(([[φ1, ν[xj 7→ _v]]]G, . . ., {{_ [[φℓ, ν[xj 7→ 7→v]]]G) | v ∈ _VG, (ν(xi 7→), v) ∈_ _EG |}}) ∈ ∈_ R. _}}_ _∈_

The need for considering conditional and unconditional aggregation separately is due to the use of
arbitrary aggregation functions. Indeed, suppose that one uses an aggregation function F for which
0 ∈ R is a neutral value. That is, for any multiset X of real values, the equality F (X) = F (X _⊎{0})_
holds. For example, the summation aggregation function satisfies this property. We then observe:

[[aggr[F]xj _φ(xj)_ _E(xi, xj)_ _, ν]]G = F_ ( [[φ, ν[xj _v]]]_ _v_ _VG, (ν(xi), v)_ _EG_
_|_ _{{_ _7→_ _|_ _∈_ _∈_ _}}_
   


-----

= F ( [[φ _E(xi, xj), ν[xj_ _v]]]_ _v_ _VG_
_{{_ _·_ _7→_ _|_ _∈_ _}}_

= [[aggr[F]xj [(][φ][(][x][j][)][ ·][ E][(][x][i][, x][j][))][, ν][]]]G. 

In other words, unconditional aggregation can simulate conditional aggregation. In contrast, when
0 is not a neutral value of the aggregation function F, conditional and unconditional aggregation
behave differently. Indeed, in such cases aggr[F]xj _φ(xj) | E(xi, xj)_ and aggr[F]xj [(][φ][(][x][j][)][ ·][ E][(][x][i][, x][j][))]
may evaluate to different values, as illustrated in the following example.
  

As aggregation function F we take the average avg(X) := _X1_ _x_ _X_ _[x][ for multisets][ X][ of real]_

_|_ _|_ _∈_

values. We remark that 0’s in X contribute to the size of X and hence 0 is not a neutral element of

P

avg. Now, let us consider the expressions

_φ1(xi) := aggr[avg]xj_ [(][1][x]j [=][x]j _xj_ [(][1][x]j [=][x]j

_[·][ E][(][x][i][, x][j][))][ and][ φ][2][(][x][i][) :=][ aggr][avg]_ _[|][ E][(][x][i][, x][j][))][.]_

Let ν be such that ν(xi) = v. Then, [[φ1, ν]]G results in applying the average to the multiset {{1w=w _·_
_E(v, w)_ _w_ _VG_ which includes the value 1 for every w _NG(v) and a 0 for every non-_
_|_ _∈_ _}}_ _∈_
neighbor w ̸∈ _NG(v). In other words, [[φ1, ν]]G results in |NG(v)|/|VG|. In contrast, [[φ2, ν]]G_
results in applying the average to the multiset **1w=w** _w_ _VG, (v, w)_ _EG_ . In other words,
this multiset only contains the value 1 for each {{ w _|NG( ∈v), ignoring any information about the ∈_ _}}_
_∈_
non-neighbors of v. In other words, [[φ2, ν]]G results in |NG(v)|/|NG(v)| = 1. Hence, conditional
and unconditional aggregation behave differently for the average aggregation function.

This said, one could alternative use a more general variant of conditional aggregation of the form
aggr[F]xj [(][φ][|][ψ][)][ with as semantics][ [][aggr[F]xj [(][φ][|][ψ][)][, ν][]]]G := F _{{[[φ, ν[xj →_ _v]]]G | v ∈_ _VG, [[ψ, ν[xj →_
_v]]G_ = 0 where one creates a multiset only for those valuations ν[xj _v] for which the condition_
_ψ evaluates to a non-zero value. This general form of aggregation includes conditional aggregation, ̸_ _}}_   _→_
by replacing _ψ with E(xi, xj) and restricting φ, and unconditional aggregation, by replacing ψ with_
the constant functionnot discuss this general form of aggregation further. 1, e.g., 1xj =xj . In order not to overload the syntax of TL expressions, we will

The notion of free index variables for expressions in TL(Ω, Θ) is defined as before, where now
free(aggr[F]xj [(][φ][)) :=][ free][(][φ][)][ \ {][x][j][}][, and where][ free][(][aggr]x[F]j _φ(xj)_ _E(xi, xj)) :=_ _xi_ (recall
_|_ _{_ _}_
that free(φ(xj)) = _xj_ in conditional aggregation). Moreover, summation depth is replaced by
_{_ _}_  
the notion of aggregation depth, agd(φ), defined in the same way as summation depth except that
agd(aggr[F]xj [(][φ][)) :=][ agd][(][φ][) + 1][ and][ agd][(][aggr]x[F]j [(][φ][(][x][j][)][ |][ E][(][x][i][, x][j][)) :=][ agd][(][φ][) + 1][. Similarly,]
the fragments TLk(Ω, Θ) and its aggregation depth restricted fragment TL[(]k[t][)][(Ω][,][ Θ)][ are defined as]

before, using aggregation depth rather than summation depth.

For the guarded fragment, GTL(Ω, Θ), expressions are now restricted such that aggregations must
occur only in the form aggr[F]xj [(][φ][(][x][j][)][ |][ E][(][x][i][, x][j][))][, for][ i, j][ ∈] [[2]][. In other words, aggregation only]
happens on multisets of values obtained from neighboring vertices.

We now argue that our upper bound results on the separation power remain valid for the extension
TL(Ω, Θ) of TL(Ω) with arbitrary aggregation functions Θ.

**Proposition C.6. We have the following inclusions: For any t ≥** 0, any collection Ω _of functions_
_and any collection Θ of aggregation functions:_

-  ρ1 cr[(][t][)][] _ρ1_ GTL[(][t][)](Ω, Θ) _;_
_⊆_
    

-  ρ1 vwl[(]k[t][)] _⊆_ _ρ1_ TL[(]k[t]+1[)] [(Ω][,][ Θ)] _; and_
     



-  ρ0 gwl[(]k[t][)]
 


_⊆_ _ρ0_ TL[(]k[t]+1[+1)][(Ω][,][ Θ)]
 


_Proof. It suffices to show that Proposition C.3 also holds for expressions in the fragments of_
TL(Ω, Θ) considered. In particular, we only need to revise the case of summation aggregation
(that is, φ := _xi_ _[φ][1][) in the proof of Proposition][ C.3][. Indeed, let us consider the more general case]_

when one of the two aggregating functions are used.

[P]


-----

-  φ := aggr[F]xi [(][φ][1][)][. We then define]

_φ˜[c]_ :=
_ _


_φ˜[c]_ := _xi ˜φ[c]1[s]_

_ℓ_∈N_ (m1,...,m_ _ℓ)∈N[ℓ]_ (c,c1,...,cℓ)∈C_(m1,...,mℓ,F ) _s^=1_ _∃[=][m][s]_ _[∧∀][x][i]_

where C(m1, . . ., mℓ, F ) now consists of all (c, c1, . . ., cℓ) ∈ R[ℓ][+1] such that


_φ˜[c]1[s]_ _[,]_
_s=1_

_


_ℓ∈N_


_c = F_ _c1, . . ., c1, . . ., cℓ, . . ., cℓ_
_{{_
 _m1 times_ _mℓ_ times
| {z } | {z }

-  φ := aggr[F]xi [(][φ][1][(][x][i][)][ |][ E][(][x][j][, x][i][))][. We then define]


_φ˜[c]_ :=


_∃[=][m]xi E(xj, xi) ∧_
(c,c1,...,cℓ)∈C_(m1,...,mℓ,F )

_ℓ_

_∃[=][m][s]_ _xi E(xj, xi) ∧_ _φ˜[c]1[s]_ [(][x][i][)]
_s=1_

^


_ℓ∈N_


(m1,...,mℓ)∈N[ℓ]


where C(m1, . . ., mℓ, F ) again consists of all (c, c1, . . ., cℓ) ∈ R[ℓ][+1] such that

_ℓ_

_c = F_ _c1, . . ., c1, . . ., cℓ, . . ., cℓ_ and m = _ms._
_{{_ _}}_

_s=1_

 _m1 times_ _mℓ_ times  X
| {z } | {z }

It is readily verified that [[aggr[F]xi [(][φ][)][,][ v][]]]G = _c iff [[ ˜φ[c], v]][B]G_ = _⊤, and [[aggr[F]xi_ [(][φ][(][x][i][)] _|_
_E(xj, xi)), v]]G = c iff [[ ˜φ[c], v]][B]G_ [=][ ⊤][, as desired.]

For the guarded case, we note that the expression ˜φ[c] above yields a guarded expression as long
conditional aggregation is used of the form aggr[F]xi [(][φ][(][x][i][)][ |][ E][(][x][j][, x][i][))][ with][ i, j][ ∈] [[2]][, so we can]
reuse the argument in the proof of Proposition C.3 for the guarded case.


We will illustrate later on (Section D) that this generalization allows for assessing the separation
power of GNNs that use a variety of aggregation functions.

The choice of supported aggregation functions has, of course, an impact on the ability of TL(Ω, Θ)
to match color refinement or the k-WL procedures in separation power. The same holds for GNNs,
as shown by Xu et al. (2019). And indeed, the proof of Proposition C.5 relies on the presence
of summation aggregation. We note that most lower bounds on the separation power of GNNs in
terms of color refinement or the k-WL procedures assume summation aggregation since summation
suffices to construct injective sum-decomposable functions on multisets (Xu et al., 2019; Zaheer
et al., 2017), which are used to simulate color refinement and k-WL. A more in-depth analysis of
lower bounding GNNs with less expressive aggregation functions, possibly using weaker versions
of color refinement and k-WL is left as future work.

C.6 GENERALIZATION TO GRAPHS WITH REAL-VALUED VERTEX LABELS

We next consider the more general setting in whichvertices in a graph can carry real-valued vectors. We remark that no changes to neither the syntax colG : VG → R[ℓ] for some ℓ _∈_ N. That is,
nor the semantics of TL expressions are needed, yet note that [[Ps(x), ν]]G := colG(ν)s is now an
element in R rather than 0 or 1, for each s ∈ [ℓ].

A first observation is that the color refinement and k-WL procedures treat each real value as a
separate label. That is, two values that differ only by any small ϵ > 0, are considered different. The
proofs of Theorem 4.1, 4.2, 4.3 and 4.4 rely on connections between color refinement and k-WL
and the finite variable logics GC and C[k][+1], respectively. In the discrete context, the unary predicates
_Ps(x) used in the logical formulas indicate which label vertices have. That is, [[Ps, v]][B]G_ [=][ ⊤] [iff]
colG(v)s = 1. To accommodate for real values in the context of separation power, these logics now
need to be able to differentiate between different labels, that is, different real numbers. We therefore


-----

extend the unary predicates allowed in formulas. More precisely, for each dimension s ∈ [ℓ], we
now have uncountably many predicates of the form Ps,r, one for each r ∈ R. In any formula in GC
or C[k][+1] only a finite number of such predicates may occur. The Boolean semantics of these new
predicates is as expected:

[[Ps,r(x), ν]][B]G [:= if][ col][G][(][µ][(][x][i][))][s] [=][ r][ then][ ⊤] [else][ ⊥][.]
In other words, in our logics, we can now detect which real-valued labels vertices have. Although,
in general, the introduction of infinite predicates may cause problems, we here consider a specific
setting in which the vertices in a graph have a unique label. This is commonly assumed in graph
learning. Given this, it is easily verified that all results in Section C.2 carry over, where all logics
involved now use the unary predicates Ps,r with s ∈ [ℓ] and r ∈ R.

The connection between TL and logics also carries over. First, for Proposition C.3 we now need to
connect TL expressions, that use a finite number of predicates Ps, for s [ℓ], with the extended
_∈_
logics having uncountably many predicates Ps,r, for s ∈ [ℓ] and r ∈ R, at their disposal. It suffices
to reconsider the case φ(xi) = Ps(xi) in the proof of Proposition C.3. More precisely, [[Ps(xi), ν]]G
can now be an arbitrary value c ∈ R. We now simply define ˜φ[c](xi) := Ps,c(xi). By definition

[[Ps(xi), ν]]G = c if and only if [[Ps,c(xi), ν]][B]G [=][ ⊤][, as desired.]

The proof for the extended version of proposition C.5 now needs a slightly different strategy, where
we build the relevant TL formula after we construct the contrapositive of the Proposition. Let us
first show how to construct a TL formula that is equivalent to a logical formula on any graph using
only labels in a specific (finite) set R of real numbers.

In other words, given a set R of real values, we show that for any formula φ(x) ∈ C[k,][(][t][)] using unary
predicates Ps,r such that r ∈ _R, we can construct the desired ˆφ. As mentioned, we only need to_
reconsider the case φ(xi) := Ps,r(xi). We define


(Ps(xi) _r[′]1xi=xi_ ).
_−_
_r[′]∈R,rY≠_ _r[′]_


_φˆ :=_


_r[′]_ _R,r=r[′][ r][ −]_ _[r][′]_
_∈_ _̸_


Then, [[ ˆφ, ν]]G evaluates to

_r[′]∈R,r≠_ _r[′]_ [(][r][ −] _[r][′][)]_ 1 [[Ps,r, ν]] = ⊤

Qr[′]∈R,r≠ _r[′]_ [(][r][ −] _[r][′][) =]_ 0 [[Ps,r, ν]] = ⊥ _[.]_

Indeed, if [[Ps,r, ν]] =, then colG(v)s = r and hence [[Ps, v]]G = r, resulting in the same nomi_⊤_ Q
nator and denominator in the above fraction. If [[Ps,r, ν]] = ⊥, then colG(v)s = r[′] for some value
_r[′]_ _∈_ _R with r ̸= r[′]. In this case, the nominator in the above fraction becomes zero. We remark that_
this revised construction still results in a guarded TL expression, when the input logical formula is
guarded as well.

Coming back to the proof of the extended version of Proposition C.5, let us show the proof for the
the fact that ρ1 GTL[(][t][)](Ω) _ρ1_ cr[(][t][)][], the other two items being analogous. Assume that there is
_⊆_
a pair (G, v) and (H, w) which is not in ρ1 cr[(][t][)][]. Then, by Proposition C.1, applied on graphs with
    
real-valued labels, there exists a formula φ (x) in GC[(][t][)] such that [[φ, v]][B]G [=][ ⊤̸][= [][φ, w]][B]H [=][ ⊥][.]
We remark that φ(x) uses finitely many Ps,r predicates. Let R be the set of real values used in
both G and H (and φ(x)). We note that R is finite. We invoke the construction sketched above,
and obtain a formula ˆφ in GTL[(][t][)] such that [[ ˜φ, v]]G = [[ ˜φ, w]]H . Hence, (G, v) and (H, w) is not in
_̸_
_ρ1_ GTL[(][t][)](Ω) either, for any Ω, which was to be shown.
  

D DETAILS OF SECTION 5

We here provide some additional details on the encoding of layers of GNNs in our tensor languages,
and how, as a consequence of our results from Section 4, one obtains a bound on their separation
power. This section showcases that it is relatively straightforward to represent GNNs in our tensor
languages. Indeed, often, a direct translation of the layers, as defined in the literature, suffices.

D.1 COLOR REFINEMENT

We start with GNN architectures related to color refinement, or in other words, architectures which
can be represented in our guarded tensor language.


-----

**GraphSage.** We first consider a “basic” GNN, that is, an instance of GraphSage (Hamilton et al.,

2017) in which sum aggregation is used. The initial features are given by F [(][0][)] = (f1[(][0][)][, . . .,][ f][ (]d[0]0[)][)]

where fi[(][0][)] R[n][×][1] is a hot-one encoding of the ith vertex label in G. We can represent the ini
tial embedding easily in∈ GTL(0), without the use of any summation. Indeed, it suffices to define
_φcan be represented by simple expressions in[(]i[0][)][(][x][1][) :=][ P][i][(][x][1][)][ for][ i][ ∈]_ [[][d][0][]][. We have][ F][ (]vj GTL[0][)] [= []([0φ).[(]j[0][)][, v][]]]G for j ∈ [d0], and thus the initial features

Assume now, by induction, that we can also represent the features computed by a basic GNN in
layer t − 1. That is, let F [(][t][−][1][)] _∈_ R[n][×][d][t][−][1] be those features and for each i ∈ [dt−1] let φ[(]i[t][−][1][)](x1)

be expressions in GTL[(][t][−][1][)](σ) representing them. We assume that, for each i ∈ [dt−1], Fvi[(][t][−][1][)] =

[[φ[(]i[t][−][1][)], v]]G. We remark that we assume that a summation depth of t 1 is needed for layer t 1.

_−_ _−_

Then, in layer t, a basic GNN computes the next features as

**_F_** [(][t][)] := σ **_F_** [(][t][−][1][)] _· V_ [(][t][)] + A · F [(][t][−][1][)] _· W_ [(][t][)] + B[(][t][)][] _,_

where A ∈ R[n][×][n] is the adjacency matrix of  _G, V_ [(][t][)] and W [(][t][)] are weight matrices in R[d][t][−][1][×][d][t],
**_B[(][t][)]_** _∈_ R[n][×][d][t] is a (constant) bias matrix consist of n copies of b[(][t][)] _∈_ R[d][t], and σ is some activation
function. We can simply use the following expressions φ[(]j[t][)][(][x][1][)][, for][ j][ ∈] [[][d][t][]][:]

_σ_ _Vij[(][t][)]_ _i_ (x1) + _E(x1, x2)_ _Wij[(][t][)]_ _i_ (x2) + b[(]j[t][)] 1[=][x]1 _._

[d]Xi[t]=1[−][1] _[·][ φ][(][t][−][1][)]_  Xx2  _·_ [d]Xi[t]=1[−][1] _[·][ φ][(][t][−][1][)]_ [] _[·][ 1][x]_ 

 

Here, Wij[(][t][)][,][ V][ (]ij[t][)] [and][ b]j[(][t][)] [are real values corresponding the weight matrices and bias vector in layer]

_t. These are expressions in GTL[(][t][)](σ) since the additional summation is guarded, and combined_
with the summation depth of t 1 of φ[(]i[t][−][1][)], this results in a summation depth of t for layer t.
_−_

Furthermore, Fvi[(][t][)] [= [][φ[(]i[t][)][, v][]]]G, as desired. If we denote by bGNN[(][t][)] the class of t-layered basic

GNNs, then our results imply

_ρ1_ cr[(][t][)][] _ρ1(_ GTL[(][t][)](Ω) _ρ1_ bGNN[(][t][)][],
_⊆_ _⊆_

and thus the separation power of basic  GNN s is bounded by the separation power of color refinement.  
We thus recover known results by Xu et al. (2019) and Morris et al. (2019).

Furthermore, if one uses a readout layer in basic GNNs to obtain a graph embedding, one typically
applies a functionplaces over all vertices of the graph. This corresponds to an expression in ro : R[d][t] _→_ R[d][t] in the form of ro _v∈VG_ **_[F][ (]v[t][)]_**, in which aggregation takes TL(2t+1)(σ, ro): φj :=
 P 

roj _x1_ _[φ]j[(][t][−][1][)](x1)_, where roj is the projection of the readout function on the jthe coordinate. We

note that this is indeed not a guarded expression anymore, and thus our results tell that

 P 

_ρ0_ gcr[(][t][)][] _ρ0(TL[(]2[t][+1][)](Ω)_ _ρ0_ bGNN[(][t][)] + readout _._
_⊆_ _⊆_
     

More generally, GraphSage allows for the use of general aggregation functions F on the multiset of
features of neighboring vertices. To cast the corresponding layers in TL(Ω), we need to consider the
extension TL(Ω, Θ) with an appropriate set Θ of aggregation functions, as described in Section C.5.
In this way, we can represent layer t by means of the following expressions φ[(]j[t][)][(][x][1][)][, for][ j][ ∈] [[][d][t][]][.]

_dt−1_

_σ_ _Vij[(][t][)]_ _i_ (x1) + _Wij[(][t][)]_ _x2_ _φ[(]i[t][−][1][)](x2)_ _E(x1, x2)_ + b[(]j[t][)] 1[=][x]1 _,_

 _i=1_ _[·][ φ][(][t][−][1][)]_ _i=1_ _[·][ aggr][F]_ _|_ _[·][ 1][x]_ 

[d]X[t][−][1]  X  
 

which is now an expression in GTL[(][t][)]({σ}, Θ) and hence the bound in terms of t iterations of color
refinement carries over by Proposition C.6. Here, Θ simply consists of the aggregation functions
used in the layers in GraphSage.

**GCNs.** _Graph Convolution Networks (GCNs) (Kipf & Welling, 2017) operate alike basic GNNs_
except that a normalized Laplacian D[−][1][/][2](I + A)D[−][1][/][2] is used to aggregate features, instead
of the adjacency matrix A. Here, D[−][1][/][2] is the diagonal matrix consisting of reciprocal of the
square root of the vertex degrees in G plus 1. The initial embedding F [(][0][)] is just as before. We


_Vij[(][t][)]_ _i_
_i=1_ _[·][ φ][(][t][−][1][)]_

[d]X[t][−][1]


_dt−1_

_Wij[(][t][)]_ _x2_ _φ[(]i[t][−][1][)]_
_i=1_ _[·][ aggr][F]_

X 


(x2) _E(x1, x2)_ + b[(]j[t][)] 1[=][x]1
_|_ _[·][ 1][x]_



-----

use again dt to denote the number of features in layer t. In layer t > 0, a GCN computes F [(][t][)] :=
_σ(D[−][1][/][2](I + A)D[−][1][/][2]_ **_F_** [(][t][−][1][)]W [(][t][)] + B[(][t][)]). If, in addition to the activation function σ we add
1 _·_ 1
the function _√x+1 : R →_ R : x 7→ _√x+1 to Ω, we can represent the GCN layer, as follows. For_

_j_ [dt], we define the GTL(t+1)(σ, _√x1+1_ ) expressions
_∈_

_φ[(]j[t][)][(][x][1][) :=][ σ]_ _f1/[√]x+1_ _x2_ _E(x1, x2)_ _·_ _i=1_ _Wij[(][t][)]_ _[·][ φ]i[(][t][−][1][)](x1)_ _· f1/[√]x+1_ _x2_ _E(x1, x2)_

 [X]  [d]X[t][−][1]   [X] 

+f1/[√]x+1 _E(x1, x2)_ _E(x1, x2)_ _f1/[√]x+1_ _E(x2, x1)_ _Wij[(][t][)]_ _i_ (x2) _,_

_x2_ _·_ _x2_ _·_ _x1_ _·_ _i=1_ _[·][φ][(][t][−][1][)]_

 [X]  X  [X]  [d]X[t][−][1] [!]

where we omitted the bias vector for simplicity. We again observe that only guarded summations are
needed. However, we remark that in every layer we now add two the overall summation depth, since
we need an extra summation to compute the degrees. In other words, a t-layered GCN correspond
to expressions in GTL(2t)(σ, _√x1+1_ ). If we denote by GCN[(][t][)] the class of t-layered GCNs, then our
results imply
_ρ1_ cr[(2][t][)][] _ρ1_ GTL(2t)(Ω) _ρ1_ GCN[(][t][)][].
_⊆_ _⊆_
We remark that another representation can be provided, in which the degree computation is factored
      
out (Geerts et al., 2021a), resulting in a better upper bound ρ1 cr[(][t][+1)][] _ρ1_ GCN[(][t][)][]. In a similar
_⊆_
way as for basic GNNs, we also have ρ0 gcr[(][t][+1)][] _ρ0_ GCN[(][t][)] + readout .
_⊆_    
    

**SGCs.** As an other example, we consider a variation of Simple Graph Convolutions (SGCs) (Wu
et al., 2019), which use powers the adjacency matrix and only apply a non-linear activation function
at the end. That is, F := σ(A[p] _·_ **_F_** [(][0][)] _·_ **_W ) for some p ∈_** N and W ∈ R[d][0][×][d][1] . We remark that SGCs
actually use powers of the normalized Laplacian, that is, F := σ (D[−][1][/][2](I + AG)D[−][1][/][2]))[p]

_·_
**_F_** [(][0][)] **_W_** but this only incurs an additional summation depth as for GCNs. We focus here on our
simpler version. It should be clear that we can represent the architecture in·   TL(pp+1) [(Ω)][ by means of]


the expressions:

_p_ _[d][0]_

_φ[(]j[t][)][(][x][1][) :=][ σ]_ _E(xk, xk+1)_ _Wij_ _φ[(]i[0][)][(][x][p][+1][)]_ _,_

 _x2_ _· · ·_ _xp+1_ _k=1_ _·_ _i=1_ _·_ 

X Y X 

for j [d1]. A naive application of our results would imply an upper bound on their separation[X] 
_∈_
power by p-WL. We can, however, use Proposition 4.5. Indeed, it is readily verified that these
expressions have a treewidth of one, because the variables form a path. And indeed, when for
example, p = 3, we can equivalently write φ[(]j[t][)][(][x][1][)][ as]

_[d][0]_

_σ_ _x2_ _E(x1, x2) ·_ _x1_ _E(x2, x1) ·_ _x2_ _E(x1, x2) ·_ _i=1_ _Wij · φ[(]i[0][)][(][x][2][)]_ _,_

X X X  X [!]

by reordering the summations and reusing index variables. This holds for arbitrary p. We thus obtain
guarded expressions in GTL(p)(σ) and our results tell that t-layered SGCs are bounded by cr(p) for
vertex embeddings, and by gcr[(][p][)] for SGCs + readout.

**Principal Neighbourhood Aggregation.** Our next example is a GNN in which different aggregation functions are used: Principal Neighborhood Aggregation (PNA) is an architecture proposed by
Corso et al. (2020) in which aggregation over neighboring vertices is done by means of mean, stdv,
max and min, and this in parallel. In addition, after aggregation, three different scalers are applied.
Scalers are diagonal matrices whose diagonal entries are a function of the vertex degrees. Given the
features for each vertex v computed in layer t 1, that is, Fv[(][t]:[−][1][)] R[1][×][ℓ], a PNA computes v’s
_−_ _∈_

new features in layer t in the following way (see layer definition (8) in (Corso et al., 2020)). First,
vectors G[(]v[t]:[)]

_[∈]_ [R][1][×][4][ℓ] [are computed such that]

mean mlpj(Fw[(][t]:[−][1][)]) _w_ _NG(v)_ for 1 _j_ _ℓ_
_{{_ _|_ _∈_ _}}_ _≤_ _≤_

stdv mlpj(Fw[(][t]:[−][1][)]) _w_ _NG(v)_ for ℓ + 1 _j_ 2ℓ

_G[(]vj[t][)]_ [=]  _{ {_ _|_ _∈_ _}}_  _≤_ _≤_

max  {{mlpj(Fw[(][t]:[−][1][)]) | w ∈ _NG(v)}}_ for 2ℓ + 1 ≤ _j ≤_ 3ℓ

min mlpj(Fw[(][t]:[−][1][)]) _w_ _NG(v)_ for 3ℓ + 1 _j_ 4ℓ,

 {{ _|_ _∈_ _}}_  _≤_ _≤_

   


-----

where mlpj : R[ℓ] _→_ R is the projection of an MLP mlp : R[ℓ] _→_ R[ℓ] on the jth coordinate. Then,
three different scalers are applied. The first scaler is simply the identity, the second two scalers s1
and s2 depend on the vertex degrees. As such, vectors Hv[(][t]:[)] _[∈]_ [R][12][ℓ] [are constructed as follows:]

_Hvj[(][t][)]_ for 1 _j_ 4ℓ

_≤_ _≤_

_Hvj[(][t][)]_ [=] s1(degG(v)) _Hvj[(][t][)]_ for 4ℓ + 1 _j_ 8ℓ

_·_ _≤_ _≤_

s2(degG(v)) _Hvj[(][t][)]_ for 8ℓ + 1 _j_ 12ℓ,

_·_ _≤_ _≤_

where s1 and s2 are functions from R → R (see (Corso et al., 2020) for details). Finally, the new
vertex embedding is obtained as
**_Fv[(][t]:[)]_** [=][ mlp][′][(][H]v[(][t]:[)][)]

for some MLP mlp[′] : R[12][ℓ] _→_ R[ℓ]. The above layer definition translates naturally into expressions
in TL(Ω, Θ), the extension of TL(Ω) with aggregate functions (Section C.5). Indeed, suppose that
for each j [ℓ] we have TL(Ω, Θ) expressions φ[(]j[t][−][1][)](x1) such that [[φ[(]j[t][−][1][)], v]]G = Fvj[(][t][−][1][)] for any
_∈_

vertex v. Then, G[(]vj[t][)] [simply corresponds to the guarded expressions]

_ψj[(][t][)][(][x][1][) :=][ aggr]x[mean]2_ (mlpj(φ[(]1[t][−][1][)](x2), . . ., φ[(]ℓ[t][−][1][)](x2)) | E(x1, x2)),

for 1 ≤ _j ≤_ _ℓ, and similarly for the other components of G[(]v[t]:[)]_ [using the respective aggregation]

functions, stdv, max and min. Then, Hvj[(][t][)] [corresponds to]

_ψj[(][t][)][(][x][1][)]_ for 1 ≤ _j ≤_ 4ℓ

_ξj[(][t][)][(][x][1][) =]_ s1(aggr[sum]x2 [(][1][x]2[=][x]2 _j_ [(][x][1][)] for 4ℓ + 1 _j_ 8ℓ

s2(aggr[sum]x2 [(][1][x]2[=][x]2 _[|][ E][(][x][1][, x][2][)))][ ·][ ψ]j[(][t][)][(][x][1][)]_ for 8ℓ + 1 ≤ _j ≤_ 12ℓ,

_[|][ E][(][x][1][, x][2][)))][ ·][ ψ][(][t][)]_ _≤_ _≤_

where we use summation aggregation to compute the degree information used in the functions in
the scalers s1 and s2. And finally,

_φ[(]j[t][)]_ [:=][ mlp]j[′] [(][ξ]1[(][t][)][(][x][1][)][, . . ., ξ]12[(][t][)]ℓ[(][x][1][))]


represents Fvj[(][t][)][. We see that all expressions only use two index variables and aggregation is applied]

in a guarded way. Furthermore, in each layer, the aggregation depth increases with one. As such,
a t-layered PNA can be represented in GTL[(][t][)](Ω, Θ), where Ω consists of the MLPs and functions
used in scalers, and Θ consists of sum (for computing vertex degrees), and mean, stdv, max and
min. Proposition C.6 then implies a bound on the separation power by cr[(][t][)].

**Other example.** In the same way, one can also easily analyze GATs (Velickovic et al., 2018) and
show that these can be represented in GTL(Ω) as well, and thus bounds by color refinement can be
obtained.

D.2 _k-DIMENSIONAL WEISFEILER-LEMAN TESTS_

We next discuss architectures related to the k-dimensional Weisfeiler-Leman algorithms. For k = 1,
we discussed the extended GINs in the main paper. We here focus on arbitrary k ≥ 2.

**Folklore GNNs.** We first consider the “Folklore” GNNs or k-FGNNs for short (Maron et al.,

2019b). For k ≥ 2, k-FGNNs computes a tensors. In particular, the initial tensor F[(][0][)] encodes
atpk(G, v) for each v ∈ _VG[k][. We can represent this tensor by the following][ k][2][(][ℓ]_ [+ 2)][ expressions]
in TL[(]k[0][)][:]

**1xr=xs** _Pj(xr)_ for j [ℓ]
_·_ _∈_

_φ[(]r,s,j[0][)]_ [(][x][1][, . . ., x][k][) :=] _E(xr, xs)_ for j = ℓ + 1 _,_


1xr=xs for j = ℓ + 2

for r, s [k] and j [ℓ + 2]. We note: [[φ[(]r,s,j[0][)] _[,][ (][v][1][, . . ., v][k][)]]]G = Fv[(][0]1[)],...,vk,r,s,j_ [for all][ (][r, s, j][)][ ∈]
_∈_ _∈_

[k][2] _× [ℓ_ + 2], as desired. We let τ0 := [k][2] _× [ℓ_ + 2] and set d0 = k[2] _× (ℓ_ + 2).

Then, in layer t, a k-FGNN computes a tensor


**F[(]v[t]1[)]** _,...,vk,_ [:=][ mlp]0[(][t][)]

_•_


**F[(]v[t]1[−],...,v[1][)]** _k,•[,]_


mlp[(]s[t][)][(][F]v[(][t]1[−],...,v[1][)] _s_ 1,w,vs+1,...,vk,

_−_ _•[)]_

_s=1_

Y


_w∈VG_


-----

where mlp[(]s[t][)] [:][ R][d][t][−][1][ →] [R][d]t[′], for s ∈ [k], and and mlp[(]0[t][)] [:][ R][d][t][−][1][×][d]t[′] → R[d][t] are MLPs. We here use

to denote combinations of indices in τd for F[(][t][)] and in τd 1 for F[(][t][−][1][)].

_•_ _−_

Let F[(][t][−][1][)] _∈_ R[n][k][×][d][t][−][1] be the tensor computed by an k-FGNN in layer t − 1. Assume
that for each tuple of elements j in τdt−1 we have an expression φ[(]j[t][−][1][)](x1, . . ., xk) satisfying

[[φ[(]j[t][−][1][)], (v1, . . ., vk)]]G = Fv[(][t]1[−],...,v[1][)] _k,j_ [and such that it is an expression in][ TL](kt−+11)[(Ω)][. That is, we]

need k + 1 index variables and a summation depth of t − 1 to represent layer t − 1.

Then, for layer t, for each j _τdt_, it suffices to consider the expression
_∈_

_φ[(]j[t][)][(][x][1][, . . ., x][k][) :=][ mlp]0[(][t],[)]j_ _φ[(]i[t][−][1)](x1, . . ., xk)_ **_i∈τdt−1_** _[,]_

  _k_ 

_xk+1_ _s=1_ mlp[(]s,[t][)]j (φ[(]i[t][−][1][)](x1, . . ., xs−1, xk+1, xs+1, . . ., xk) **_i∈τdt−1_** _,_

X Y    

where mlp[(]o,[t][)]j [and][ mlp]s,[(][t][)]j [are the projections of the][ MLP][s on the][ j][-coordinates. We remark that]

we need k + 1 index variables, and one extra summation is needed. We thus obtain expressions in
TL(kt)+1[(Ω)][ for the][ t][th layer, as desired. We remark that the expressions are simple translations of the]

defining layer definitions. Also, in this case, Ω consists of all MLPs. When a k-FGNN is used for
vertex embeddings, we now simply add to each expression a factor _s=1_ **[1][x]1[=][x]s** [. As an immediate]
consequence of our results, if we denote by k-FGNN[(][t][)] the class of t-layered k-FGNNs, then for
vertex embeddings:
_ρ1_ vwl[(]k[t][)] _⊆_ _ρ1_ TL(kt)+1[(Ω)] _⊆_ _ρ1_ _k-FGNN[Q][k][(][t][)][]_

in accordance with the known results from Azizian & Lelarge (2021). When used for graph embed
       

dings, an aggregation layer over all k-tuples of vertices is added, followed by the application of an
MLP. This results in expressions with no free index variables, and of summation depth t + k, where
the increase with k stems from the aggregation process over all k-tuples. In view of our results, for
graph embeddings:
_ρ0_ gwl[(]k[∞][)] _ρ0_ TLk+1(Ω) _ρ0_ _k-FGNN_

_⊆_ _⊆_

in accordance again with Azizian & Lelarge (2021). We here emphasize that the upper bounds in

        

terms of k-WL are obtained without the need to know how k-WL works. Indeed, one can really just
focus on casting layers in the right tensor language!

We remark that Azizian & Lelarge (2021) define vertex embedding k-FGNNs in a different way.
Indeed, for a vertex v, its embedding is obtained by aggregating of all (k _−1) tuples in the remaining_
coordinates of the tensors. They define vwlk accordingly. From the tensor language point of view,
this corresponds to the addition of k − 1 to the summation depth. Our results indicate that we loose
the connection between rounds and layers, as in Azizian & Lelarge (2021). This is the reason why
we defined vertex embedding k-FGNNs in a different way and can ensure a correspondence between
rounds and layers for vertex embeddings.

**Other higher-order examples.** It is readily verified that t-layered k-GNNs (Morris et al., 2019)
can be represented in TL(kt)+1[(Ω)][, recovering the known upper bound by][ vwl][(]k[t][)] [(Morris et al., 2019).]

It is an equally easy exercise to show that 2-WL-convolutions (Damke et al., 2020) and Ring-GNNs
(Chen et al., 2019) are bounded by 2-WL, by simply writing their layers in TL3(Ω). The invariant
graph networks (k-IGNs) (Maron et al., 2019b) will be treated in Section E, as their representation
in TLk+1(Ω) requires some work.

D.3 AUGMENTED GNNS

Higher-order GNN architectures such as k-GNNs, k-FGNNs and k-IGNs, incur a substantial cost
in terms of memory and computation (Morris et al., 2020). Some recent proposals infuse more
efficient GNNs with higher-order information by means of some pre-processing step. We next show
that the tensor language approach also enables to obtain upper bounds on the separation power of
such “augmented” GNNs.

We first consider F-MPNNs (Barcel´o et al., 2021) in which the initial vertex features are augmented
with homomorphism counts of rooted graph patterns. More precisely, let P _[r]_ be a connected rooted


-----

graph (with root vertex r), and consider a graph G = (VG, EG, colG) and vertex v _VG. Then,_
_∈_
hom(P _[r], G[v]) denotes the number of homomorphism from P to G, mapping r to v. We recall that_
a homomorphism is an edge-preserving mapping between vertex sets. Given a collection F =
_{P1[r][, . . ., P]ℓ[ r][}][ of rooted patterns, an][ F][-][MPNN][ runs an][ MPNN][ on the augmented initial vertex]_
features:
**_F˜v[(][0]:_** [)] [:= (][F][ (]v[0]: [)][,][ hom][(][P]1[ r][, G][v][)][, . . .,][ hom][(][P]ℓ[ r][, G][v][))][.]

Now, take any GNN architecture that can be cast in GTL(Ω) or TL2(Ω) and assume, for simplicity
of exposition, that a t-layer GNN corresponds to expressions in GTL[(][t][)](Ω) or TL[(]2[t][)][(Ω)][. In order]

to analyze the impact of the augmented features, one only needs to revise the expressions φ[(]j[0][)][(][x][1][)]

that represent the initial features. In the absence of graph patterns, φ[(]j[0][)][(][x][1][) :=][ P][j][(][x][1][)][, as we have]

seen before. By contrast, to represent **_F[˜]vj[(][0][)]_** [we need to cast the computation of][ hom][(][P]i[ r][, G][v][)][ in][ TL][.]

Assume that the graph pattern Pi consists of p vertices and let us identify the vertex set with [p].
Furthermore, without of loss generality, we assume that vertex “1” is the root vertex in Pi. To obtain
hom(Pi[r][, G][v][)][ we need to create an indicator function for the graph pattern][ P][i][ and then count how]
many times this indicator value is equal to one in G. The indicator function for Pi is simply given
by the expression _uv∈EPi_ _[E][(][x][u][, x][v][)][. Then, counting just pours down to summation over all index]_

variables except the one for the root vertex. More precisely, if we define

[Q]

_φPi_ (x1) := _E(xu, xv),_

_· · ·_

Xx2 Xxp _uvY∈EPi_

then [[φPi _, v]]G = hom(Pi[r][, G][v][)][. This encoding results in an expression in][ TL][p][. However, it is]_
well-known that we can equivalently write φPi (x1) as an expression ˜φPi (x1) in TLk+1 where k is
the treewidth of the graph Pi. As such, our results imply that -MPNNs are bounded in separation
_F_
power by k-WL where k is the maximal treewidth of graphs in F. We thus recover the known upper
bound as given in Barcel´o et al. (2021) using our tensor language approach.

Another example of augmented GNN architectures are the Graph Substructure Networks (GSNs)
(Bouritsas et al., 2020). By contrast to F-MPNNs, subgraph isomorphism counts rather than homomorphism counts are used to augment the initial features. At the core of a GSN thus lies the
computation of sub(P _[r], G[v]), the number of subgraphs H in G isomorphic to P (and such that the_
isomorphisms map r to v). In a similar way as for homomorphisms counts, we can either directly
cast the computation of sub(P _[r], G[v]) in TL resulting again in the use of p index variables. A possible_
reduction in terms of index variables, however, can be obtained by relying on the result (Theorem
1.1.) by Curticapean et al. (2017) in which it shown that sub(P _[r], G[v]) can be computed in terms_
of homomorphism counts of graph patterns derived from P _[r]. More precisely, Curticapean et al._
(2017) define spasm(P _[r]) as the set of graphs consisting of all possible homomorphic images of_
_P_ _[r]. It is then readily verified that if the maximal treewidth of the graphs in spasm(P_ _[r]) is k, then_
sub(P _[r], G[v]) can be cast as an expression in TLk+1. Hence, GSNs using a pattern collection_ can
_F_
be represented in TLk+1, where k is the maximal treewidth of graphs in any of the spams of patterns
in F, and thus are bounded in separation power k-WL in accordance to the results by Barcel´o et al.

(2021).

As a final example, we consider the recently introduced Message Passing Simplicial Networks
(MPSNs) (Bodnar et al., 2021). In a nutshell, MPSNs are run on simplicial complexes of graphs
instead of on the original graphs. We sketch how our tensor language approach can be used to assess
the separation power of MPSNs on clique complexes. We use the simplified version of MPSNs
which have the same expressive power as the full version of MPSNs (Theorem 6 in Bodnar et al.
(2021)).

We recall some definitions. Let Cliques(G) denote the set of all cliques in G. Given two cliques
_c and c[′]_ in Cliques(G), define c ≺ _c[′]_ if c ⊂ _c[′]_ and there exists no c[′′] in Cliques(G), such that
_c ⊂_ _c[′′]_ _⊂_ _c[′]. We define Boundary(c, G) := {c[′]_ _∈_ Cliques(G) | c[′] _≺_ _c} and Upper(c, G) := {c[′]_ _∈_
Cliques(G) | ∃c[′′] _∈_ Cliques(G), c[′] _≺_ _c[′′]_ and c ≺ _c[′′]}._

For each c in Cliques(G) we have an initial feature vector Fc[(]:[0][)] R[1][×][ℓ]. Bodnar et al. (2021)

_∈_
initialize all initial features with the same value. Then, in layer t, for each c ∈ Cliques(G), features
are updated as follows:

**_G[(]c[t]:[)]_** [=][ F][B][(][{]{mlpB(Fc[(]:[t][−][1][)], Fc[(][′][t]:[−][1][)]) | c[′] _∈_ Boundary(G, c)}})


-----

**_Hc[(][t]:[)]_** [=][ F][U] [(][{] mlpU (Fc[(]:[t][−][1][)], Fc[(][′][t]:[−][1][)], Fc[(][t][−]c[′][1]:[)][)][ |][ c][′][ ∈] [Upper][(][G, c][)][}] )

_{_ _∪_ _}_

**_Fc[(]:[t][)]_** [=][ mlp][(][F][ (]c:[t][−][1][)], G[(]c[t]:[)][,][ H]c[(][t]:[)][)][,]

where FB and FU are aggregation functions and mlpB, mlpU and mlp are MLPs. With some effort,
one can represent these computations by expressions in TLp(Ω, Θ) where p is largest clique in G. As
such, the separation power of clique-complex MPSNs on graphs of clique size at most p is bounded
by p − 1-WL. And indeed, Bodnar et al. (2021) consider Rook’s 4 × 4 graph, which contains a
4-clique, and the Shirkhande graph, which does not contain a 4-clique. As such, the analysis above
implies that clique-complex MPSNs are bounded by 2-WL on the Shrikhande graph, and by 3-WL
on Rook’s graph, consistent with the observation in Bodnar et al. (2021). A more detailed analysis
of MPSNs in terms of summation depth and for other simplicial complexes is left as future work.

This illustrates again that our approach can be used to assess the separation power of a variety
of GNN architectures in terms of k-WL, by simply writing them as tensor language expressions.
Furthermore, bounds in terms of k-WL can be used for augmented GNNs which form a more efficient
way of incorporating higher-order graph structural information than higher-order GNNs.

D.4 SPECTRAL GNNS

In general, spectral GNNs are defined in terms of eigenvectors and eigenvalues of the (normalized) graph Laplacian (Bruna et al., 2014; Defferrard et al., 2016; Levie et al., 2019; Balcilar et al.,
2021b)). The diagonalization of the graph Laplacian is, however, avoided in practice, due to its
excessive cost. Instead, by relying on approximation results in spectral graph analysis (Hammond
et al., 2011), the layers of practical spectral GNNs are defined in term propagation matrices consisting of functions, which operate directly on the graph Laplacian. This viewpoint allows for a spectral
analysis of spectral and “spatial” GNNs in a uniform way, as shown by Balcilar et al. (2021b). In
this section, we consider two specific instances of spectral GNNs: ChebNet (Defferrard et al., 2016)
and CayleyNet (Levie et al., 2019), and assess their separation power in terms of tensor logic. Our
general results then provide bounds on their separation power in terms color refinement and 2-WL,
respectively.

**Chebnet.** The separation power of ChebNet (Defferrard et al., 2016) was already analyzed in

Balcilar et al. (2021a) by representing them in the MATLANG matrix query language (Brijder et al.,
2019). It was shown (Theorem 2 (Balcilar et al., 2021a)) that it is only the maximal eigenvalue λmax
of the graph Laplacian used in the layers of ChebNet that may result in the separation power of
ChebNet to go beyond 1-WL. We here revisit and refine this result by showing that, when ignoring
the use of λmax, the separation power of Chebnet is bounded already by color refinement (which,
as mentioned in Section 2, is weaker than 1-WL for vertex embeddings). In a nutshell, the layers
of a ChebNet are defined in terms of Chebyshev polynomials of the normalized Laplacian Lnorm =
**_I −_** **_D[−][1][/][2]_** _· A · D[−][1][/][2]_ and these polynomials can be easily represented in GTL(Ω). One can
alternatively use the graph Laplacian L = D _−_ **_A in a ChebNet, which allows for a similar analysis._**
The distinction between the choice of Lnorm and L only shows in the needed summation depth (in as
in similar way as for the GCNs described earlier). We only consider the normalized Laplacian here.

More precisely, following Balcilar et al. (2021a;b), in layer t, vertex embeddings are updated in a
ChebNet according to:


**_C_** [(][s][)] _· F_ [(][t][−][1)] _· W_ [(][t][−][1][,s][)]
_s=1_

X


**_F_** [(][t][)] := σ


with


**_C_** [(1)] := I, C [(2)] =


2

**_Lnorm_** **_I, C_** [(][s][)] = 2C [(2)] **_C_** [(][s][−][1)] **_C_** [(][s][−][2)], for s 3,
_λmax_ _−_ _·_ _−_ _≥_


and where λmax denotes the maximum eigenvalue of Lnorm. We next use a similar analysis as
in Balcilar et al. (2021a). That is, we ignore for the moment the maximal eigenvalue λmax and
redefine C [(2)] as cLnorm − **_I for some constant c. We thus see that each C_** [(][s][)] is a polynomial of
the form ps(c, Lnorm) := _i=0_ _[a]i[(][s][)][(][c][)][ ·][ (][L][norm][)][i][ with scalar functions][ a]i[(][s][)]_ : R → R and where we
interpret (Lnorm)[0] = I. To upper bound the separation power using our tensor language approach,

[P][q][s]


-----

we can thus shift our attention entirely to representing (Lnorm)[i] _· F_ [(][t][−][1)] _· W_ [(][t][−][1][,s][)] for powers
_i_ N. Furthermore, since (Lnorm)[i] is again a polynomial of the form qi(D[−][1][/][2] **_A_** **_D[−][1][/][2]) :=_**
_∈ri_ _·_ _·_
_j=0_ _[b][ij][ ·][ (][D][−][1][/][2][ ·][ A][ ·][ D][−][1][/][2][)][j][, we can further narrow down the problem to represent]_
P (D[−][1][/][2] _· A · D[−][1][/][2])[j]_ _· F_ [(][t][−][1)] _· W_ [(][t][−][1][,s][)]

in GTL(Ω), for powers j ∈ N. And indeed, combining our analysis for GCNs and SGCs results in
expressions in GTL(Ω). As an example let us consider (D[−][1][/][2] _·_ **_A_** _·_ **_D[−][1][/][2])[2]_** _·_ **_F_** [(][t][−][1)] _·_ **_W_** [(][t][−][1)], that
is we use a power of two. It then suffices to define, for each output dimension j, the expressions:

_ψj[2][(][x][1][) =][ f]1/[√]x_ _x2_ _[E][(][x][1][, x][2][)]_ _·_ _x2_ _E(x1, x2) · f1/x_ _x1_ [(][E][(][x][2][, x][1][)] _·_
 P _x1_ _E(x2, x1) · f[P]1/[√]x([P]x2_ _[E][(][x][1][, x][2][))] P[ ·]_ _di=1t−1_ _[W][ (]ij[t][−][1)]φ[(]i[t][−][1)](x1)_ _,_

P   P [!]

where the φ[(]i[t][−][1)](x1) are expressions representing layer t 1. It is then readily verified that we can
use ψj[2][(][x][1][)][ to cast layer][ t][ of a][ ChebNet][ in][ GTL][(Ω)][ with][ Ω] −[consisting of][ f]1/[√]x [:][ R][ →] [R][ :][ x][ 7→] _√1x_,
_f1/x : R →_ R : x 7→ _x[1]_ [, and the used activation function][ σ][. We thus recover (and slightly refine)]

Theorem 2 in Balcilar et al. (2021a):
**Corollary D.1. On graphs sharing the same λmax values, the separation power of ChebNet is**
_bounded by color refinement, both for graph and vertex embeddings._

A more fine-grained analysis of the expressions is needed when interested in bounding the summation depth and thus of the number of rounds needed for color refinement. Moreover, as shown
by Balcilar et al. (2021a), when graphs have non-regular components with different λmax values,
ChebNet can distinguish them, whilst 1-WL cannot. To our knowledge, λmax cannot be computed
in TLk(Ω) for any k. This implies that it not clear whether an upper bound on the separation power
can be obtained for ChebNet taking λmax into account. It is an interesting open question whether
there are two graphs G and H which cannot be distinguished by k-WL but can be distinguished
based on λmax. A positive answer would imply that the computation of λmax is beyond reach for
TL(Ω) and other techniques are needed.

**CayleyNet.** We next show how the separation power of CayleyNet (Levie et al., 2019) can be
analyzed. To our knowledge, this analysis is new. We show that the separation power of CayleyNet
is bounded by 2-WL. Following Levie et al. (2019) and Balcilar et al. (2021b), in each layer t, a
CayleyNet updates features as follows:


**_C_** [(][s][)] _· F_ [(][t][−][1)]W [(][t][−][1][,s][)]
_s=1_

X


**_F_** [(][t][)] := σ


with

_hL_ _ıI_ _s_ _hL_ _ıI_ _s_

**_C_** [(1)] := I, C [(2][s][)] := Re _−_ _, C_ [(2][s][+1)] := Re _ı_ _−_ _,_

_hL + ıI_ _hL + ıI_

  !   !

where h is a constant, ı is the imaginary unit, and Re : C → C maps a complex number to its
real part. We immediately observe that a CayleyNet requires the use of complex numbers and
matrix inversion. So far, we considered real numbers only, but when our separation results are
concerned, the choice between real or complex numbers is insignificant. In fact, only the proof
of Proposition C.3 requires a minor modification when working on complex numbers: the infinite
disjunctions used in the proof now need to range over complex numbers. For matrix inversion, when
dealing with separation power, one can use different expressions in TL(Ω) for computing the matrix
inverse, depending on the input size. And indeed, it is well-known (see e.g., Csanky (1976)) that
based on the characteristic polynomial of A, A[−][1] for any matrix A R[n][×][n] can be computed as a

_n_ 1 _∈_

polynomial _[−]cn[1]_ _i=1−_ _[c][i][A][n][−][1][−][i][ if][ c][n][ ̸][= 0][ and where each coefficient][ c][i][ is a polynomial in][ tr][(][A][j][)][,]_

for various j. Here, tr( ) is the trace of a matrix. As a consequence, layers in CayleyNet can be
P _·_
viewed as polynomials in hL − _ıI with coefficients polynomials in tr((hL −_ _ıI)[j]). One now needs_
three index variables to represent the trace computations tr((hL _ıI)[j]). Indeed, let φ0(x1, x2) be_
_−_


-----

the TL2 expression representing hL − _ıI. Then, for example, (hL −_ _ıI)[j]_ can be computed in TL3
using
_φj(x1, x2) :=_ _φ0(x1, x3) · φj−1(x3, x2)_

_x3_

X

and hence tr((hL _ıI)[j]) is represented by_ _x1_ _x2_ _[φ][j][(][x][1][, x][2][)][·][1][x]1[=][x]2_ _[.][. In other words, we obtain]_
_−_

expressions inThis implies that each layer in TL3. The polynomials in CayleyNet h can be represented, on graphs of fixed size, byL −PıI can be represented in TL2 just as for ChebNet TL3(Ω).

[P]

expressions, where Ω includes the activation function σ and the function Re. This suffices to use
our general results and conclude that CayleyNets are bounded in separation power by 2-WL. An
interesting question is to find graphs that can be separated by a CayleyNet but not by 1-WL. We
leave this as an open problem.

E PROOF OF THEOREM 5.1


We here consider another higher-order GNN proposal: the invariant graph networks or k-IGNs of
Maron et al. (2019b). By contrast to k-FGNNs, k-IGNs are linear architectures. If we denote by
_k-IGN[(][t][)]_ the class of t layered k-IGNs, then following inclusions are known (Maron et al., 2019b)

_ρ1_ _k-IGN[(][t][)][]_ _ρ1_ vwl[(]k[t][)] 1 and ρ0 _k-IGN_ _ρ0_ gwl[(]k[∞][)]1 _._
_⊆_ _−_ _⊆_ _−_

The reverse inclusions were posed as open problems in Maron et al. (2019a) and were shown to          
hold by Chen et al. (2020) for k = 2, by means of an extensive case analysis and by relying on
properties of 1-WL. In this section, we show that the separation power of k-IGNs is bounded by that
of (k − 1)-WL, for arbitrary k ≥ 2. Theorem 4.2 tells that we can entirely shift our attention to
showing that the layers of k-IGNs can be represented in TLk(Ω). In other words, we only need to
show that k index variables are needed for the layers. As we will see below, this requires a bit of
work since a naive representation of the layers of k-IGNs use 2k index variables. Nevertheless, we
show that this can be reduced to k index variables only.

By inspecting the expressions needed to represent the layers of k-IGNs in TLk(Ω), we obtain that a
_t layer k-IGN[(][t][)]_ require expressions of summation depth of tk. In other words, the correspondence
between layers and summation depth is precisely in sync. This implies, by Theorem 4.2:

_ρ1_ _k-IGN_ = ρ1 vwl[(]k[∞]−[)]1 _,_

where we ignore the number of layers. We similarly obtain that      _ρ0_ _k-IGN_ = ρ0 gwl[(]k[∞]−[)]1, hereby

answering the open problem posed in Maron et al. (2019a). Finally, we observe that the k-IGNs used
in Maron et al. (2019b) to show the inclusion ρ1 _k-IGN[(][t][)][]_ _ρ1_ vwl  [(]k[t][)] 1 are of very simple form.  
_⊆_ _−_

By defining a simple class of k-IGNs, denoted by k-GINs, we obtain

    

_ρ1_ _k-GIN[(][t][)][]_ = ρ1 vwl[(]k[t]−[)] 1 _,_

hereby recovering the layer/round connections.    

We start with the following lemma:


**Lemma E.1. For any k ≥** 2, a t layer k-IGNs can be represented in TL(ktk)[(Ω)][.]

Before proving this lemma, we recall k-IGNs. These are architectures that consist of linear equivariant layers. Such linear layers allow for an explicit description. Indeed, following Maron et al.
(2019c), let _ℓ_ be the equality pattern equivalence relation on [n][ℓ] such that for a, b [n][ℓ], a _ℓ_ **_b_**
_∼_ _∈_ _∼_
if and only if ai = aj _bi = bj for all j_ [ℓ]. We denote by [n][ℓ]/ _ℓ_ the equivalence classes
_⇔_ _∈_ _∼_
induced by ∼ℓ. Let us denote by F[(][t][−][1][)] _∈_ R[n][k][×][d][t][−][1] the tensor computed by an k-IGN in layer t _−_ 1.
Then, in layer t, a new tensor in R[n][k][×][d][t] is computed, as follows. For j ∈ [dt] and v1, . . ., vk ∈ [n][k]:


_Fv[(][t]1[)],...,vk,j_ [:=][ σ]


_cγ,i,jFw[(][t]1[−],...,w[1][)]_ _k,i_ [+]
_i∈X[dt−1]_


(1)


**1(v,w)∈γ**
**_wX∈[n][k]_**


**1v∈µbµ,j**


_γ∈[n][2][k]/∼2k_


_µ∈[n][k]/∼k_


for activation function σ, constants cγ,i,j and bµ,j in R and where 1(v,w) _γ and 1v_ _µ are indicator_
_∈_ _∈_
functions for the 2k-tuple (v, w) to be in the equivalence class γ ∈ [n][2][k]/∼2k [and the][ k][-tuple][ v][ to]


-----

be in classd0 = 2(k2[) +] µ[ kℓ] ∈ [where][n][k]/∼[ ℓ]k [. As initial tensor][is the number of initial vertex labels, just as for][ F][(][0][)][ one defines][ F][ (]v[0]1[)],...,vk,j [:=][ k][-][ atp][FGNN]k[(][G,][s.] **_[ v][)][ ∈]_** [R][d][0] _[,][ with]_

We remark that the need for having a summation depth of tk in the expressions in TLk(Ω), or
equivalently for requiring tk rounds of (k − 1)-WL, can intuitively be explained that each layer of a
_k-IGN aggregates more information from “neighbouring” k-tuples than (k_ _−_ 1)-WL does. Indeed, in
each layer, an k-IGN can use previous tuple embeddings of all possible k-tuples. In a single round
of (k − 1)-WL only previous tuple embeddings from specific sets of k-tuples are used. It is only
after an additional k − 1 rounds, that k-WL gets to the information about arbitrary k-tuples, whereas
this information is available in a k-IGN in one layer directly.

_Proof of Lemma E.1. We have seen how F[(][0][)]_ can be represented in TLk(Ω) when dealing with
_k-FGNNs. We assume now that also the t_ 1th layer F[(][t][−][1][)] can be represented by dt 1 expressions in TL(k(t−1)k)(Ω) and show that the same holds for the − _tth layer._ _−_

We first represent F[(][t][)] in TL2k(Ω), based on the explicit description given earlier. The expressions
use index variables x1, . . ., xk and y1, . . ., yk. More specifically, for j ∈ [dt] we consider the
expressions:


_dt−1_

_cγ,i,j_
_i=1_

X


_φ[(]j[t][)][(][x][1][, . . ., x][k][) =][ σ]_


_γ∈[n][2][k]/∼2k_


_ψγ(x1, . . ., xk, y1, . . ., yk)_ _φ[(]i[t][−][1][)](y1, . . ., yk)_
_·_
_yk_

X



_· · ·_
_y1_

X


+ _bµ,j_ _ψµ(x1, . . ., xk)_ _,_ (2)

_·_ 

_µ∈[Xn][k]/∼k_



where ψµ(x1, . . ., xk) is a product of expressions of the form 1xi op xj encoding the equality pattern
_µ, and similarly, ψγ(x1, . . ., xk, y1, . . ., yk) is a product of expressions of the form 1xi op xj_, 1yi op yj
and 1xi op yj encoding the equality pattern γ. These expressions are indicator functions for the their
corresponding equality patterns. That is,

1 if (v, w) _γ_ 1 if v _µ_

[[ψγ, (v, w)]]G = _∈_ [[ψµ, v]]G = _∈_
0 otherwise 0 otherwise
 


We remark that in the expressions φ[(]j[t][)] [we have two kinds of summations: those ranging over a fixed]

number of elements (over equality patterns, feature dimension), and those ranging over the index
variables y1, . . ., yk. The latter are the only ones contributing the summation depth. The former are
just concise representations of a long summation over a fixed number of expressions.

We now only need to show that we can equivalently write φ[(]j[t][)][(][x][1][, . . ., x][k][)][ as expressions in][ TL][k][(Ω)][,]

that is, using only indices x1, . . ., xk. As such, we can already ignore the term _µ∈[n][k]/∼k_ _[b][µ,j][ ·]_

_ψµ(x1, . . ., xk) since this is already in TLk(Ω). Furthermore, this expressions does not affect the_
summation depth. [P]

Furthermore, as just mentioned, we can expand expression φ[(]j[t][)] [into linear combinations of other]

simpler expressions. As such, it suffices to show that k index variables suffice for each expression
of the form:

_· · ·_ _ψγ(x1, . . ., xk, y1, . . ., yk) · φ[(]i[t][)][(][y][1][, . . ., y][k][)][,]_ (3)
_y1_ _yk_

X X

obtained by fixing µ and i in expression (2). To reduce the number of variables, as a first step we
eliminate any disequality using the inclusion-exclusion principle. More precisely, we observe that
_ψγ(x, y) can be written as:_


**1xi=xj**
(i,jY)∈I _·_


**1xi≠** _xj_
(i,jY)∈I[¯] _·_


**1yi=yj**
(i,jY)∈J _·_


**1yi≠** _yj_
(i,jY)∈J[¯]


**1xi=yj**
(i,jY)∈K _·_


**1xi≠** _yj_
(i,jY)∈K[¯]


-----

= ( 1)[|][A][|][+][|][B][|][+][|][C][|] **1xi=xj** **1yi=yj** **1xi=yj** _,_ (4)

_AX⊆I[¯]_ _BX⊆J[¯]_ _CX⊆K[¯]_ _−_ (i,jY)∈I∪A (i,j)Y∈J∪B _·_ (i,j)Y∈J∪C

for some setsK¯ = [k][2]\K. Here we use that I, J and K of pairs of indices in 1xi≠ _xj = 1−1xi [=kxj][2], 1, and whereyi≠_ _yj = 1−I[¯]1 = [yi=ykj] and[2]_ _\ I 1,xJi[¯]≠_ = [yj = 1k][2]−\1 Jy andi=yj
and use the inclusion-exclusion principle to obtain a polynomial in equality conditions only.

In view of expression (4), we can push the summations over y1, . . ., yk in expression (3) to the
subexpressions that actually use y1, . . ., yk. That is, we can rewrite expression (3) into the equivalent
expression:


(−1)[|][A][|][+][|][B][|][+][|][C][|] _·_
_CX⊆K[¯]_


**1xi=xj**
(i,jY)∈I∪A


_A⊆I[¯]_


_B⊆J[¯]_


**1yi=yj**

_·_  _y1_ _· · ·_ Xyk (i,j)Y∈J∪B _·_

[X]

By fixing A, B and C, it now suffices to argue that

**1xi=xj** **1yi=yj**
(i,jY)∈I∪A _·_  _y1_ _· · ·_ Xyk (i,j)Y∈J∪B _·_

[X]

can be equivalently expressed in TLk(Ω).


**1xi=yj** _φ[(]i[t][−][1][)]_
(i,j)Y∈K∪C _·_

**1xi=yj** _φ[(]i[t][−][1][)]_
(i,j)Y∈K∪C _·_


(y1, . . ., yk) _._ (5)





(y1, . . ., yk) _,_ (6)






Since our aim is to reduced the number of index variables from 2k to k, it is important to known
which variables are the same. In expression (6), some equalities that hold between the variables may
not be explicitly mentioned. For this reason, we expand I ∪ _A, J ∪_ _B and K ∪_ _C with their implied_
equalities. That is, 1xi=xj is added to I ∪ _A, if for any (v, w) such that_


**1xi=xj** **1yi=yj**
(i,jY)∈I∪A _·_ (i,j)Y∈J∪B _·_


**1xi=yj** _, (v, w)]]G = 1 ⇒_ [[1xi=xj _, v]]G = 1_
(i,j)Y∈K∪C


holds. Similar implied equalities 1yi=yj and 1xi=yj are added to J ∪ _B and K ∪_ _C, respectively._
let us denoted by I _[′], J_ _[′]_ and K _[′]. It should be clear that we can add these implied equalities to_
expression (6) without changing its semantics. In other words, expression (6) can be equivalently
represented by

**1xi=xj** **1yi=yj** **1xi=yj** _φi[(][(][t][−][1][)](y1, . . ., yk)_ _,_ (7)
(i,jY)∈I _[′]_ _·_  _y1_ _· · ·_ Xyk (i,jY)∈J _[′]_ _·_ (i,jY)∈K[′] _·_ 

[X] 

There now two types of index variables among the y1, . . ., yk: those that are equal to some xi, and
those that are not. Now suppose that (j, j[′]) _J_ _[′], and thus yj = yj′_, and that also (i, j) _K_ _[′], and_
_∈_ _∈_
thus xi = yj. Since we included the implied equalities, we also have (i, j[′]) _K_ _[′], and thus xi = yj′_ .
_∈_
There is no reason to keep (j, j[′]) ∈ _J_ _[′]_ as it is implied by (i, j) and (i, j[′]) ∈ _K_ _[′]. We can thus safely_
remove all pairs (j, j[′]) from J _[′]_ such that (i, j) ∈ _K_ _[′]_ (and thus also (i, j[′]) ∈ _K_ _[′]). We denote by J_ _[′′]_
be the reduced set of pairs of indices obtained from J _[′]_ in this way. We have that expression (7) can
be equivalently written as

**1xi=xj** **1xi=yj** **1yi=yj** _φ[(]i[t][−][1][)](y1, . . ., yk)_ _,_ (8)
(i,jY)∈I _[′]_ _·_  _y1_ _· · ·_ Xyk (i,jY)∈K[′] _·_ (i,jY)∈J _[′′]_ _·_ 

[X] 

where we also switched the order of equalities in J _[′′]_ and K _[′]. Our construction of J_ _[′′]_ and K _[′]_ ensures
that none of the variables yj with j belonging to a pair in J _[′′]_ is equal to some xi.

By contrast, the variable yj occurring in (i, j) ∈ _K_ _[′]_ are equal to xi. We observe, however, that
also certain equalities among the variables _x1, . . ., xk_ hold, as represented by the pairs in I _[′]. let_
_{_ _}_
_I_ _[′](i) := {i[′]_ _| (i, i[′]) ∈_ _I_ _[′]} and define ˆı as a unique representative element in I_ _[′](i). For example,_
one can take [ˆ]i to be smallest index in I _[′](i). We use this representative index (and corresponding_
_x-variable) to simplify K_ _[′]. More precisely, we replace each pair (i, j) ∈_ _K_ _[′]_ with the pair (ˆı, j). In


-----

terms of variables, we replace xi = yj with xˆi [=][ y][j][. Let][ K] _[′′][ be the set][ K]_ _[′′][ modified in that way.]_
Expression (8) can thus be equivalently written as


**1xi=xj** **1xˆı[=][y]j**
(i,jY)∈I _[′]_ _·_  _y1_ _· · ·_ Xyk (ˆı,jY)∈K[′′] _[·]_

[X]

where the free index variables of the subexpression


**1yi=yj** _φ[(]i[t][−][1][)](y1, . . ., yk)_ _,_ (9)
(i,jY)∈J _[′′]_ _·_ 




**1yi=yj** _φ[(]i[t][−][1][)](y1, . . ., yk)_ (10)
(i,jY)∈J _[′′]_ _·_



_· · ·_
_y1_

X


**1xˆı[=][y]j**
(ˆı,jY)∈K[′′] _[·]_


_yk_


are precisely the index variables xˆı [for][ (ˆ]ı, j) ∈ _K_ _[′′]. Recall that our aim is to reduce the variables_
from 2k to k. We are now finally ready to do this. More specifically, we consider a bijection
_β :_ _y1, . . ., yk_ _x1, . . ., xk_ in which ensure that for each ˆı there is a j such that ([ˆ]i, j) _K_ _[′′]_
_{_ _} →{_ _}_ _∈_
_βand(y βj) =(yj x) =ˆı_ [holds. After all, they only contribute for a given] xˆı[. Furthermore, among the summations][ P]y1 _[· · ·][ x]ˆı[ P][value. Let]yk_ [we can ignore those for which][ Y][ be those indices in][ [][k][]]
such that β(yj) ̸= xˆı [for some][ ˆ]ı. Then, we can equivalently write expression (9) as


**1xi=xj**
(i,jY)∈I _[′]_ _·_


**1xˆı[=][β][(][y]j** [)][ ·]

_β(yXi),i∈Y_ (ˆı,jY)∈K[′]


**1β(yi)=β(yj** )
(i,jY)∈J _[′′]_

_β(φ[(]i[t][−][1][)](y1, . . ., yk))_

_·_


(11)


where β(φ[(]i[t][−][1][)](y1, . . ., yk)) denotes the expression obtained by renaming of variables y1, . . ., yj in

_φ[(]i[t][−][1)](y1, . . ., yk) into x-variables according to β. This is our desired expression in TLk(Ω). If we_
analyze the summation depth of this expression, we have by induction that the summation depth of
_φ[(]i[t][−][1][)]_ is at most (t 1)k. In the above expression, we are increasing the summation depth with at

_−_
most |Y |. The largest size of Y is k, which occurs when none of the y-variables are equal to any
of the x-variables. As a consequence, we obtained an expression of summation depth at most tk, as
desired.

As a consequence, when using k-IGNs[(][t][)] for vertex embeddings, using (G, v) → **F[(]v,...,v,[t][)]** : [one simply]

pads the layer expression with _i∈[k]_ **[1][x]1[=][x]i** [which does not affect the number of variables or]

summation depth. When using k-IGNs[(][t][)] of graph embeddings, an additional invariant layer is added
to obtain an embedding from G [Q]R[d][t] . Such invariant layers have a similar (simpler) representation
_→_
as given in equation 1 (Maron et al., 2019c), and allow for a similar analysis. One can verify
that expressions in TL(k(t+1)k)(Ω) are needed when such an invariant layer is added to previous t

layers. Based on this, Theorem 4.2, Lemma E.1 and Theorem 1 in Maron et al. (2019b), imply that
_ρ1_ _k-IGN_ = ρ1 vwl[(]k[∞]−[)]1 and ρ0 _k-IGN_ = ρ0 gwl[(]k[∞]−[)]1 hold.
           

_k-dimensional GINs._ We can recover a layer-based characterization for k-IGNs that compute vertex embeddings by considering a special subset of k-IGNs. Indeed, the k-IGNs used in Maron et al.
(2019b) to show ρ1(wl[(]k[t][)] 1[)][ ⊆] _[ρ][1][(][k][-][IGN][(][t][)][)][ are of a very special form. We extract the essence of]_

_−_
these special k-IGNs in the form of k-dimensional GINs. That is, we define the class k-GINs to
consist of layers defined as follows. The initial layers are just as for k-IGNs. Then, for t ≥ 1:


**F[(]v[t]1[)]** _,...,vk,:_ [:=][ mlp]0[(][t][)]


**F[(]v[t]1[−],...,v[1][)]** _k,:[,]_


mlp[(]1[t][)][(][F]u,v[(][t][−]2[1],...,v[)] _k,:[)][,]_ mlp[(]1[t][)][(][F]v[(][t]1[−],u,...,v[1][)] _k,:[)]_
_uX∈VG_ _uX∈VG_

_, . . .,_ mlp[(]1[t][)][(][F]v[(][t]1[−],v[1]2[)] _,...,vk_ 1,w,:[))]

_−_

_uX∈VG_


where Fv[(][t]1[−],v[1]2[)],...,vk,: 1 : R[d][t][−][1] R[b][t] and mlp[(]1[t][)] : R[d][t][−][1][+][kb][t] R[d][t] are MLPs. It is

_[∈]_ [R][d][t][−][1] [,][ mlp][(][t][)] _→_ (t) _→_
now an easy exercise to show that k-GIN[(][t][)] can be represented in TLk [(Ω)][ (remark that the summa-]

tions used increase the summation depth with one only in each layer). Combined with Theorem 4.2
and by inspecting the proof of Theorem 1 in Maron et al. (2019b), we obtain:


-----

**Proposition E.2. For any k ≥** 2 and any t ≥ 0: ρ1(k-GIN[(][t][)]) = ρ1(vwl[(]k[t]−[)] 1[)][.]

We can define the invariant version of k-IGNs by adding a simple readout layer of the form

mlp(F[(]v[t]1[)] _,...,vk,:[)][,]_
_v1,...,vXk∈VG_


as is used in Maron et al. (2019b). We obtain, ρ0(k-GIN) = ρ0(gwl[(]k[∞][)]1[)][, by simply rephrasing the]

_−_
readout layer in TLk(Ω).

F DETAILS OF SECTION 6

Let C(Gs, R[ℓ]) be the class of all continuous functions from Gs to R[ℓ]. We always assume that Gs
forms a compact space. For example, when vertices are labeled with values in {0, 1}[ℓ][0], Gs is a
finite set which we equip with the discrete topology. When vertices carry labels in R[ℓ][0] we assume
that these labels come from a compact set K ⊂ R[ℓ][0] . In this case, one can represent graphs in Gs
by elements in (R[ℓ][0] )[2] and the topology used is the one induced by some metric ∥.∥ on the reals.
Similarly, we equip R[ℓ] with the topology induced by some metric ∥.∥.

Consider F ⊆C(Gs, R[ℓ]) and define F as the closure of F in C(Gs, R[ℓ]) under the usual topology
induced by f 7→ supG,v∥f (G, v)∥. In other words, a continuous function h : Gs → R[ℓ] is in F if
there exists a sequence of functions f1, f2, . . . ∈F such that limi→∞ supG,v∥fi(G, v)−h(G, v)∥ =
0. The following theorem provides a characterization of the closure of a set of functions. We state it
here modified to our setting.
**Theorem F.1 ((Timofte, 2005)). Let F ⊆C(Gs, R[ℓ]) such that there exists a set S ⊆C(Gs, R)**
_satisfying S · F ⊆F and ρ(S) ⊆_ _ρ(F). Then,_

_F :=_ _f ∈C(Gs, R[ℓ])_ _ρ(F_ ) ⊆ _ρ(f_ ), ∀(G, v) ∈Gs, f (G, v) ∈F(G, v) _,_

_where F(G, v) := {h(G, v) | h ∈F} ⊆_ R[ℓ]. We can equivalently replace ρ(F) by ρ(S) in the
_expression for F._

We will use this theorem to show Theorem 6.1 in the setting that F consists of functions that can
be represented in TL(Ω), and more generally, sets of functions that satisfy two conditions, stated
below. We more generally allowdepend on f . We will require F to satisfy the following two conditions: F to consist of functions f : Gs → R[ℓ][f], where the ℓf ∈ N may

**concatenation-closed:R[p][+][q]** : (G, v) If 7→ f(f1 :1( GG,s v →), fR2([p]G,and v)) f is also in2 : Gs → FR.[q] are in F, then g := (f1, f2) : Gs →

**function-closed:in F for any continuous function For a fixed ℓ** _∈_ N, for any h ∈C f ∈F(R such that[p], R[ℓ]). _f : Gs →_ R[p], also h ◦ _f : Gs →_ R[ℓ] is

We denote by Fℓ be the subset of F of functions from Gs to R[ℓ].
**Theorem 6.1. For any ℓ, and any set F of functions, concatenation and function closed for ℓ, we**
_have: Fℓ_ = {f : Gs → R[ℓ] _| ρs(F) ⊆_ _ρs(f_ )}.

_Proof. The proof consist of (i) verifying the existence of a set S as mentioned Theorem F.1; and_
of (ii) eliminating the pointwise convergence condition “ (G, v) _s, f_ (G, v) _ℓ(G, v) in the_
_∀_ _∈G_ _∈F_
closure characterization in Theorem F.1.

For showing (ii) we argue that Fℓ(G, v) = R[ℓ] such that the conditions f (G, v) ∈Fℓ(G, v) is
automatically satisfied for anythe constant functionsclosed for+ :for a R ∈[2][ℓ] R→ ℓ, then, so isR[ℓ] : ( s Fax ◦ℓ,. Hence, y bf) ∈Fi : 7→ R[ℓ]x bℓ→ + fiand thus := ∈C yR[ℓ] g. For:i( x ◦Gs 7→ Ff f, ∈F Rℓ andb[ℓ]is closed under scalar multiplication. Finally, consider)i with. Indeed, take an arbitraryℓ gas well. Furthermore, if in b Fi ∈ℓ, hR = ([ℓ] the if, gth basis vector. Since) ∈F s f sincea ∈G : R[ℓ]k F →→ is concatenation-RR[ℓ] F[ℓ] :and consider x is function- 7→ _a × x,_
closed. As a consequence, the functionunder addition. All combined, this shows that + ◦ _h : Gℓs →is closed under taking linear combinations andR[ℓ]_ is in Fℓ, showing that Fℓ is also closed
_F_
since the basis vectors of R[ℓ] can be attained, Fℓ(G, v) := R[ℓ], as desired.


-----

For (i), we show the existence of a set S ⊆C(Gs, R) such that S · Fℓ _⊆Fℓ_ and ρs(S) ⊆ _ρs(Fℓ)_
hold. Similarly as in Azizian & Lelarge (2021), we define

_S :=_ _f ∈C(Gs, R)_ (f, f, . . ., f ) _∈Fℓ_ _._
_ℓ_ times


We remark that for s and f _ℓ, s_ _f :_ _s_ |R[ℓ] : ({zG, v)} _s(G, v)_ _f_ (G, v), with being
pointwise multiplication, is also in ∈S _∈F_ _ℓ. Indeed, ·_ _G_ _s → f =_ (s, f 7→) with (s, f ⊙) the concatenation of ⊙ _s_
_F_ _·_ _⊙◦_
and f and ⊙ : R[2][ℓ] _→_ R[ℓ] : (x, y) → **_x ⊙_** **_y being pointwise multiplication._**

It remains to verify ρs( ) _ρs(_ _ℓ). Assume that (G, v) and (H, w) are not in ρs(_ _ℓ). By_
_S_ _⊆_ _F_ _F_
definition, this implies the existence of a function _f[ˆ]_ _ℓ_ such that _f[ˆ](G, v) = a_ = b = f[ˆ](H, w)
_∈F_ _̸_
with a, b ∈ R[ℓ]. We argue that (G, v) and (H, w) are also not in ρs(S) either. Indeed, Proposition 1
in Maron et al. (2019b) implies that there exists natural numbers ααα = (α1, . . ., αℓ) ∈ N[ℓ] such
that the mapping hααα [:][ R][ℓ] _[→]_ [R][ :][ x][ →] [Q][ℓ]i=1 _[x]i[α][i]_ satisfies hααα[(][a][) =][ a][ ̸][=][ b][ =][ h]ααα[(][b][)][, with]
_a, b ∈_ R. Since F (and thus also Fℓ) is function-closed, hααα _[◦]_ _[f][ ∈F]ℓ_ [for any][ f][ ∈F]ℓ[. In particular,]
_g := hααα_ _[◦]_ _f[ˆ] ∈Fℓ_ and concatenation-closure implies that (g, . . ., g) : Gs → R[ℓ] is in Fℓ too.
Hence, g ∈S, by definition. It now suffices to observe that g(G, v) = hααα[( ˆ]f (G, v)) = a ̸= b =
_hααα[( ˆ]f_ (H, w)) = g(H, w), and thus (G, v) and (H, w) are not in ρs( ), as desired.
_S_

When we know more about ρs( _ℓ) we can say a bit more._ In the following, we let alg
_F_ _∈_
_{cr[(][t][)], gcr[(][t][)], vwl[(]k[t][)][,][ gwl]k[(][∞][)]} and only consider the setting where s is either 0 (invariant graph func-_

tions) or s = 1 (equivariant graph/vertex functions).
**Corollary 6.2.R[ℓ]** _| ρ(alg) ⊆_ _ρ Under the assumptions of Theorem(f_ )}. _6.1 and if ρ(Fℓ) = ρ(alg), then Fℓ_ = {f : Gs →

_Proof. This is just a mere restatement of Theorem 6.1 in which ρs(_ _ℓ) in the condition ρs(_ _ℓ)_
_F_ _F_ _⊆_
_ρs(f_ ) is replaced by ρs(alg), where s = 1 for alg cr[(][t][)], vwl[(]k[t][)]

gwl[(]k[∞][)] . _∈{_ _[}][ and][ s][ = 0][ for][ alg][ ∈{][gcr][(][t][)][,]_

_}_

To relate all this to functions representable by tensor languages, we make the following observations.
First, if we consider to be the set of all functions that can be represented in GTL(t)(Ω), TL(2t+1)(Ω),

TL(kt)+1[(Ω)][ or][ TL][(Ω)] F[, then][ F][ will be automatically concatenation and function-closed, provided that]

Ω consists of all functions in _p_

_[C][(][R][p][,][ R][ℓ][)][. Hence, Theorem][ 6.1][ applies. Furthermore, our results](t)_
from Section 4 tell us that for all t 0, and k 1, ρ1 cr[(][t][)][] = ρ1 GTL (Ω), ρ0 gcr[(][t][)][] =
(t+1) _≥_ _≥(t)_
_ρ0_ TL2 (Ω) = ρ0 gwl[(]1[t][)],[S] ρ1 ewl[(]k[t][)] = ρ1 TLk+1[(Ω)], and ρ0 TLk+1(Ω) = ρ0 gwl[(]k[∞][)] . As
      

a consequence, Corollary 6.2 applies as well. We thus easily obtain the following characterizations:

                 

**Proposition F.2. For any t ≥** 0 and k ≥ 1:

-  If F consists of all functions representable in GTL(t)(Ω), then Fℓ = {f : G1 → Rℓ _|_
_ρ1_ cr[(][t][)][] _ρ1(f_ ) _;_
_⊆_ _}_

_(t)_

-  If F  _consists of all functions representable in TLk+1[(Ω)][, then][ F][ℓ]_ [=][ {][f][ :][ G][1][ →] [R][ℓ] _[|]_

_ρ1_ vwl[(]k[t][)] _ρ1(f_ ) _;_

_⊆_ _}_

-  If   _consists of all functions representable in_ TL(2t+1)(Ω), then _ℓ_ = _f :_ 0 R[ℓ]
_F_ _F_ _{_ _G_ _→_ _|_

_ρ0_ gwl[(]1[t][)] _ρ0(f_ ) _; and finally,_

_⊆_ _}_

  



-  If F consists of all functions representable in TLk+1(Ω), then Fℓ = {f : G0 → R[ℓ] _|_
_ρ0_ gwl[(]k[∞][)] _ρ0(f_ ) _,_

_⊆_ _}_

  

_provided that Ω_ _consists of all functions in_ _p_

_[C][(][R][p][,][ R][ℓ][)][.]_

In fact, Lemma 32 in Azizian & Lelarge (2021) implies that we can equivalently populate Ω with

[S]

all MLPs instead of all continuous functions. We can thus use MLPs and continuous functions
interchangeably when considering the closure of functions.


-----

At this point, we want to make a comparison with the results and techniques in Azizian & Lelarge
(2021). Our proof strategy is very similar and is also based on Theorem F.1. The key distinguishing
feature is that we consider functionsas great advantage that no separate proofs are needed to deal with invariant or equivariant functions. f : Gs → R[ℓ][f] instead of functions from graphs alone. This has
Equivariance incurs quite some complexity in the setting considered in Azizian & Lelarge (2021).
A second major difference is that, by considering functions representable in tensor languages, and
based on our results from Section 4, we obtain a more fine-grained characterization. Indeed, we
obtain characterizations in terms of the number of rounds used in CR and k-WL. In Azizian &
Lelarge (2021), t is always set to ∞, that is, an unbounded number of rounds is considered. Further1more, when it concerns functions-WL is considered in Azizian & Lelarge (2021). Finally, another difference is that we define the f : G1 → R[ℓ][f], we recall that CR is different from 1-WL. Only
equivariant version vwlk in a different way than is done in Azizian & Lelarge (2021), because in this
way, a tighter connection to logics and tensor languages can be made. In fact, if we were to use the
equivariant version of k-WL from Azizian & Lelarge (2021), then we necessarily have to consider
an unbounded number of rounds (similarly as in our gwlk case).

We conclude this section by providing a little more details about the consequences of the above
results for GNNs. As we already mentioned in Section 6.2, many common GNN architectures are
concatenation and function-closed (using MLPs instead of continuous functions). This holds, for
example, for the classes GIN[(]ℓ[t][)][,][ eGIN][(]ℓ[t][)][,][ k][-][FGNN][(]ℓ[t][)] [and][ k][-][GIN][(]ℓ[t][)] [and][ k][-][IGN][(][t][)][, as described in Sec-]

tion 5 and further detailed in Section E and D. Here, the subscript ℓ refers to the dimension of the
embedding space.

We now consider a function f that is not more separating than cr[(][t][)] (respectively, gcr[(][t][)], vwl[(]k[t][)] [or]

gwl[(]k[∞][)], for some k 1), and want to know whether f can be approximated by a class of GNNs.

_≥_
Proposition F.2 tells that such f can be approximated by a class of GNNs as long as these are at least
as separating as GTL[(][t][)] (respectively, TL[(]2[t][+1][)], TL[(]k[t]+1[)] [or][ TL]k[(][∞]+1[)] [). This, in turn, amounts showing]

that the GNNs can be represented in the corresponding tensor language fragment, and that they can
match the corresponding labeling algorithm in separation power. We illustrate this for the GNN
architectures mentioned above.

-  In Section 5 we showed that GIN[(]ℓ[t][)] [can be represented in][ GTL][(][t][)][(Ω)][. Theorem][ 4.3][ then]

implies that ρ1(cr[(][t][)]) ⊆ _ρ1(GIN[(]ℓ[t][)][)][. Furthermore, Xu et al. (2019) showed that][ ρ][1][(][GIN][(]ℓ[t][)][)][ ⊆]_

_ρ1(cr[(][t][)]). As a consequence, ρ1(GIN[(]ℓ[t][)][) =][ ρ][1][(][cr][(][t][)][)][. We note that the lower bound for][ GIN][s]_

only holds when graphs carry discrete labels. The same restriction is imposed in Azizian
& Lelarge (2021).

-  In Section 5 we showed that eGIN[(]ℓ[t][)] [can be represented in][ TL]2[(][t][)][(Ω)][. Theorem][ 4.2][ then]

implies that ρ1(vwl[(]1[t][)][)][ ⊆] _[ρ][1][(][eGIN]ℓ[(][t][)][)][. Furthermore, Barcel´]o et al. (2020) showed that_

_ρ1(eGIN[(]ℓ[t][)][)][ ⊆]_ _[ρ][1][(][vwl]1[(][t][)][)][. As a consequence,][ ρ][1][(][eGIN]ℓ[(][t][)][) =][ ρ][1][(][vwl]1[(][t][)][)][. Again, the lower]_

bound is only valid when graphs carry discrete labels.

-  In Section 5 we mentioned (see details in Section D) that k-FGNN[(]ℓ[t][)] [can be represented]

in TL[(]k[t]+1[)] [(Ω)][.] Theorem 4.2 then implies that ρ1(vwl[(]k[t][)][)][ ⊆] _[ρ][1][(][k][-][FGNN]ℓ[(][t][)][)][.]_ Further
more, Maron et al. (2019b) showed that ρ1(k-FGNN[(]ℓ[t][)][)][ ⊆] _[ρ][1][(][vwl][(]k[t][)][)][. As a consequence,]_

_ρ1(k-FGNN[(]ℓ[t][)][)][ ⊆]_ _[ρ][1][(][vwl][(]k[t][)][)][. Similarly,][ ρ][1][((][k][ + 1)][-][GIN]ℓ[(][t][)][) =][ ρ][1][(][vwl][(]k[t][)][)][ for the special]_

class of (k + 1)-IGNs described in Section E. No restrictions are in place for the lower
bounds and hence real-valued vertex-labelled graphs can be considered.

-  When GIN[(]ℓ[t][)] [or][ eGIN][(]ℓ[t][)] [are extended with a readout layer, we showed in Section][ 5][ that]

these can be represented in TL(2t+1)(Ω). Theorem 4.4 and the results by Xu et al. (2019) and

Barcel´o et al. (2020) then imply that ρ0(vwl[(]1[t][)][)][ and][ ρ][0][(][gcr][(][t][)][)][ coincide with the separation]

power of these architectures with a readout layer. Here again, discrete labels need to be
considered.

-  Similarly, when k-FGNN or (k +1)-IGNs are used for graph embeddings, we can represent
these in TLk+1(Ω) resulting again that their separation power coincides with that of gwl[(]k[∞][)].

No restrictions are again in place on the vertex labels.


-----

So for all these architectures, Corollary 6.2 applies and we can characterize the closures of these
architectures in terms of functions that not more separating than their corresponding versions of cr
or k-WL, as described in the main paper. In summary,
**Proposition F.3. For any t ≥** 0:

GIN[(]ℓ[t][)] [=][ {][f][ :][ G][1][ →] [R][ℓ] _[|][ ρ][1][(][cr][(][t][)][)][ ⊆]_ _[ρ][1][(][f]_ [)][}][ =][ GTL][(][t][)][(Ω)][ℓ]

eGIN[(]ℓ[t][)] [=][ {][f][ :][ G][1][ →] [R][ℓ] _[|][ ρ][1][(][vwl]1[(][t][)][)][ ⊆]_ _[ρ][1][(][f]_ [)][}][ =][ TL]2[(][t][)][(Ω)][ℓ]

_and when extended with a readout layer:_


GIN[(]ℓ[t][)] [=][ eGIN][(]ℓ[t][)] [=][ {][f][ :][ G][0][ →] [R][ℓ] _[|][ ρ][0][(][gwl]1[(][t][)][)][ ⊆]_ _[ρ][0][(][f]_ [)][}][ =][ TL]2[(][t][+1][)]

_Furthermore, for any k ≥_ 1


(Ω)ℓ.


_k-FGNN[(]ℓ[t][)]_ [=][ k][-][GIN][(]ℓ[t][)] [=][ {][f][ :][ G][1][ →] [R][ℓ] _[|][ ρ][1][(][vwl][(]k[t][)][)][ ⊆]_ _[ρ][1][(][f]_ [)][}][ =][ TL]k[(][t]+1[)] [(Ω)][ℓ]

(k + 1)-IGNℓ = _f :_ 1 R[ℓ] _ρ1(vwl[(]k[∞][)])_ _ρ1(f_ ) = TLk+1(Ω)ℓ
_{_ _G_ _→_ _|_ _⊆_ _}_

_and when converted into graph embeddings:_

_k-FGNNℓ_ = k-GINℓ = (k + 1)-IGNℓ = _f :_ 0 R[ℓ] _ρ0(gwl[(]k[∞][)])_ _ρ0(f_ ) = TLk+1(Ω)ℓ,
_{_ _G_ _→_ _|_ _⊆_ _}_

_where the closures of the tensor languages are interpreted as the closure of the graph or graph/vertex_
_functions that they can represent. For results involving GINs or eGINs, the graphs considered should_
_have discretely labeled vertices._

As a side note, we remark that in order to simulate CR on graphs with real-valued labels, one can
use a GNN architecture of the form Fv[(][t]:[)] [=] **_Fv[(][t]:[−][1][)],_** _u_ _NG(v)_ [mlp][(][F][ (]u[t]:[−][1][)]), which translates in

_∈_

GTL[(][t][)](Ω) as expressions of the form

  [P] 

_φ[(]j[t][)][(][x][1][) :=]_ _φ[(]j[t][−][1][)](x1)_ 1 ≤ _j ≤_ _dt−1_

( _x2_ _[E][(][x][1][, x][2][)][ ·][ mlp]j_ _φ[(]1[t][−][1][)](x1), . . ., φ[(]d[t]t[−][1][)](x1)_ _dt−1 < j ≤_ _dt._

The upper bound in terms ofP CR follows from our main results. To show that   CR can be simulated,
it suffices to observe that one can approximate the function used in Proposition 1 in Maron et al.
(2019b) to injectively encode multisets of real vectors by means of MLPs. As such, a continuous
version of the first bullet in the previous proposition can be obtained.

G DETAILS ON TREEWIDTH AND PROPOSITION 4.5

As an extension of our main results in Section 4, we enrich the class of tensor language expressions
for which connections to k-WL exist. More precisely, instead of requiring expressions to belong to
TLk+1(Ω), that is to only use k + 1 index variables, we investigate when expressions in TL(Ω) are
_semantically equivalent to an expression using k + 1 variables. Proposition 4.5 identifies a large_
class of such expressions, those of treewidth k. As a consequence, even when representing GNN
architectures may require more than k + 1 index variables, sometimes this number can be reduced.
As a consequence of our results, this implies that their separation power is in fact upper bounded by
_ℓ-WL for a smaller ℓ< k. Stated otherwise, to boost the separation power of GNNs, the treewidth_
of the expressions representing the layers of the GNNs must have large treewidth.

We next introduce some concepts related to treewidth. We here closely follow the exposition given
in Abo Khamis et al. (2016) for introducing treewidth by means variable elimination sequences of
hypergraphs.

In this section, we restrict ourselves to summation aggregation.

G.1 ELIMINATION SEQUENCES

We first define elimination sequences for hypergraphs. Later on, we show how to associate such
hypergraphs to expressions in tensor languages, allowing us to define elimination sequences for
tensor language expressions.


-----

With a multi-hypergraph H = (V, E) we simply mean a multiset E of subsets of vertices V. An
_elimination hypergraph sequences is a vertex ordering σ = v1, . . ., vn of the vertices of H. With_
such a sequence σ, we can associate for j = n, n−1, n−2, . . ., 1 a sequence of n multi-hypergraphs
_n[,][ H]n[σ]_ 1[, . . .,][ H]1[σ] [as follows. We define]
_H[σ]_ _−_

_Hn := (Vn, En) := H_
_∂(vn) :=_ _F_ _n_ _vn_ _F_
_{_ _∈E_ _|_ _∈_ _}_

_Un :=_ _F._

_F ∈[∂(vn)_

and for j = n − 1, n − 2, . . ., 1 :

_Vj := {v1, . . ., vj}_
_Ej := (Ej+1 \ ∂(vj+1)) ∪{Uj+1 \ {vj+1}}_
_∂(vj) :=_ _F_ _j_ _vj_ _F_
_{_ _∈E_ _|_ _∈_ _}_

_Uj :=_ _F._

_F ∈[∂(vj_ )

in whichThe induced width H has some distinguished vertices. As we will see shortly, these distinguished vertices on H by σ is defined as maxi∈[n] |Ui| − 1. We further consider the setting
correspond to the free index variables of tensor language expressions. Without loss of generality,
we assume that the distinguished vertices are v1, v2, . . ., vf . When such distinguished vertices are
present, an elimination sequence is just as before, except that the distinguished vertices come first
in the sequence. If v1, . . ., vf are the distinguished vertices, then we define the induced width of the
sequence astinguished vertices, and then augment it with the induced width of the sequence, starting from f + maxf +1≤i≤n |Ui \ {v1, . . ., vf _}| −_ 1. In other words, we count the number of dis- vf +1
to to vn, hereby ignoring the distinguished variables in the Ui’s. One could, more generally, also try
to reduce the number of free index variables but we assume that this number is fixed, similarly as
how GNNs operate.

G.2 CONJUNCTIVE TL EXPRESSIONS AND TREEWIDTH

We start by considering a special form of TL expressions, which we refer to as conjunctive TL
expressions, in analogy to conjunctive queries in database research and logic. A conjunctive TL
expression is of the form
_φ(x) =_ _ψ(x, y)._

**_y_**

X

where x denote the free index variables, y contains all index variables under the scope of a summation, and finally, ψ(x, y) is a product of base predicates in TL. That is, ψ(x, y) is a product of
_E(zi, zj) and Pℓ(zi) with zi, zj variables in x or y. With such a conjunctive TL expression, one can_
associate a multi-hypergraph in a canonical way (Abo Khamis et al., 2016). More precisely, given a
conjunctive TL expression φ(x) we define Hφ as:

-  Vφ consist of all index variables in x and y;

-  Eφ: for each atomic base predicate τ in ψ we have an edge Fτ containing the indices
occurring in the predicate; and

-  the vertices corresponding to the free index variables x form the distinguishing set of vertices.

We now define an elimination sequence for φ as an elimination sequence for Hφ taking the distinguished vertices into account. The following observation ties elimination sequences of φ to the
number of variables needed to express φ.
**Proposition G.1. Let φ(x) be a conjunctive TL expression for which an elimination sequence of**
_induced with k_ 1 exists. Then φ(x) is equivalent to an expression ˜φ(x) in TLk.
_−_

_Proof. We show this by induction on the number of vertices in Hφ which are not distinguished. For_
the base case, all vertices are distinguished and hence φ(x) does not contain any summation and is
an expression in TLk itself.


-----

Suppose that in Hφ there are p undistinguished vertices. That is,


_φ(x) =_



_· · ·_
_y1_

X


_ψ(x, y)._

_yp_

X


By assumption, we have an elimination sequence of the undistinguished vertices. Assume that yp is
first in this ordering. Let us write


_φ(x) =_



_· · ·_
_y1_

X

_· · ·_
_y1_

X


_ψ(x, y)_

_yp_

X

_ψ1(x, y_ _yp)_
_\_ _·_
_yXp−1_


_ψ2(x, y)_
_yp_

X


containingwhere ψ1 is the product of predicates corresponding to the edges yp, and ψ2 is the product of all predicates corresponding to the edges F ∈Eφ \ ∂(yp F), that is, those not ∈ _∂(yp), that is,_
those containing the predicate yp. Note that, because of the induced width of k − 1, _yp_ _[ψ][2][(][x][,][ y][)]_

contains all indices in Up which is of size ≤ _k. We now replace the previous expression with another_
expression [P]


_φ[′](x) =_


_ψ1(x, y_ _yp)_ _Rp(x, y)_

_· · ·_ _\_ _·_

Xy1 _yXp−1_


Whereverified that Rp is regarded as anφ′ is the hypergraph |Up| − 1p-ary predicate over the indices in1 corresponding to the variable ordering Up \ yp. It is now easily σ. We note that
_H_ _H_ _−_
this is a hypergraph over p − 1 undistinguished vertices. We can apply the induction hypothesis and
replace φ[′](x) with its equivalent expression ˜φ[′](x) in TLk. To obtain the expression ˜φ(x) of φ(x),
it now remains to replace the new predicate Rp with its defining expression. We note again that Rp
contains at most k 1 indices, so it will occur in ˜φ[′](x) in the form Rp(x, z) where **_z_** _k_ 1.
_−_ _|_ _| ≤_ _−_
In other words, one of the variables in z is not used, say zs, and we can simply replace Rp(x, z) by
_zs_ _[ψ][x][(][x][,][ z][, z][s][)][.]_
P

As a consequence, one way of showing that a conjunctive expression φ(x) in TL is equivalently
expressible in TLk, is to find an elimination sequence of induced width k 1. This in turn is
_−_
equivalent to Hφ having a treewidth of k − 1, as is shown, e.g., in Abo Khamis et al. (2016).
As usual, we define the treewidth of a conjunctive expression φ(x) in TL as the treewidth of its
associated hypergraph _φ._
_H_

We recall the definition of treewidth (modified to our setting): A tree decomposition T =
(VT, ET, ξT ) of Hφ with ξT : VT → 2[V] is such that

-  For any F ∈E, there is a t ∈ _VT such that F ⊆_ _ξT (t); and_

-  For any v ∈V corresponding to a non-distinguished index variable, the set {t | t ∈ _VT, v ∈_
_ξ(t)} is not empty and forms a connected sub-tree of T_ .

The width of a tree decomposition T is given by maxt _VT_ _ξT (t)_ 1. Now the treewidth of _φ,_
tw(H) is the minimum width of any of its tree decompositions. We denote by∈ _|_ _| −_ tw(φ) the treewidth of H
_φ. Again, similar modifications are used when distinguished vertices are in place. Referring again_
_H_
to Abo Khamis et al. (2016), tw(φ) = k − 1 is equivalent to having a variable elimination sequence
for φ of an induced width of k − 1. Hence, combining this observation with Proposition G.1 results
in:
**Corollary G.2. Let φ(x) be a conjunctive TL expression of treewidth k−1. Then φ(x) is equivalent**
_to an expression ˜φ(x) in TLk._

That is, we have established Proposition 4.5 for conjunctive TL expressions. We next lift this to
arbitrary TL(Ω) expressions.

G.3 ARBITRARY TL(Ω) EXPRESSIONS

First, we observe that any expression in TL can be written as a linear combination of conjunctive
expressions. This readily follows from the linearity of the operations in TL and that equality and


-----

inequality predicates can be eliminated. More specifically, we may assume that φ(x) in TL is of the
form

_aαψα(x, y),_
_αX∈A_

with A finite set of indices and aα ∈ R, and ψα(x, y) conjunctive TL expressions. We now define

tw(φ) := max tw(ψα) _α_ _A_
_{_ _|_ _∈_ _}_

for expressions in TL. To deal with expressions in TL(Ω) that may contain function application,
we define tw(φ) as the maximum treewidth of the expressions: (i) φnofun(x) TL obtained by
_∈_
replacing each top-level function application f (φ1, . . ., φp) by a new predicate Rf with free indices
free(φ1) ∪· · · ∪ free(φp); and (ii) all expressions φ1, . . ., φp occurring in a top-level function application f (φ1, . . ., φp) in φ. We note that these expression either have no function applications
(as in (i)) or have function applications of lower nesting depth (in φ, as in (ii)). In other words,
applying this definition recursively, we end up with expressions with no function applications, for
which treewidth was already defined. With this notion of treewidth at hand, Proposition 4.5 now
readily follows.

H HIGHER-ORDER MPNNS

We conclude the supplementary material by elaborating on k-MPNNs and by relating them to classical MPNNs (Gilmer et al., 2017). As underlying tensor language we use TLk+1(Ω, Θ) which
includes arbitrary functions (Ω) and aggregation functions (Θ), as defined in Section C.5.

_ℓWe recall from Sectionnotion of being represented is defined in terms of the existence of∈_ N that can be represented in 3 that k- TLMPNNk+1(Ωs refer to the class of embeddings, Θ). When considering an embedding ℓ expressions in f : G fs → : TL GRs →k[ℓ]+1for some(ΩR[ℓ],, the Θ),
which together provide each of the ℓ components of the embedding in R[ℓ]. We remark, however,
that we can alternatively include concatenation in tensor language. As such, we can concatenate
represented in tensor language, we can then simply define it by requiring the existence of a singleℓ separate expressions into a single expression. As a positive side effect, for f : Gs → R[ℓ] to be
expression, rather than ℓ separate ones. This results in a slightly more succinct way of reasoning
about k-MPNNs.

In order to reason about k-MPNNs as a class of embeddings, we can obtain an equivalent definition
for the class of k-MPNNs by inductively stating how new embeddings are computed out of old
embeddings. Let X = _x1, . . ., xk+1_ be a set of k + 1 distinct variables. In the following, v
_{_ _}_
denotes a tuple of vertices that have at least as many components as the highest index of variables
used in expressions. Intuitively, variable xj refers to the jth component in v. We also denote the
image of a graph G and tuple v by an expression φ, i.e., the semantics of φ given G and v, as
_φ(G, v) rather than by [[φ, v]]G. We further simply refer to embeddings rather than expressions._

We first define “atomic” k-MPNN embeddings which extract basic information from the graph G
and the given tuple v of vertices.

-  Label embeddings(colG(vi))s, are k-MPNN of the forms; _φ(xi) := Ps(xi), with xi ∈_ _X, and defined by φ(G, v) :=_

-  Edge embeddings of the form φ(xi, xj) := E(xi, xj), with xi, xj ∈ _X, and defined by_

_φ(G, v) :=_ 1 if vivj ∈ _EG_
0 otherwise,


are k-MPNNs; and



-  (Dis-)equality embeddings of the form φ(xi, xj) := 1xi op xj, with xi, xj _X, and_
defined by _∈_

1 if vi op vj
_φ(G, v) :=_
0 otherwise,


are k-MPNNs.


-----

We next inductively define new k-MPNNs from “old” k-MPNNs. That is, given k-MPNNs
_φ1(x1), . . ., φℓ(xℓ), the following are also k-MPNNs:_

-  Function applications of the form φ(x) := f (φ1(x1), . . ., φℓ(xℓ) are k-MPNNs, where
**_x = x1_** **_xℓ, and defined by_**
_∪· · · ∪_

_φ(G, v) := f (φ1(G, v_ **_x1_** ), . . ., φℓ(G, v **_xℓ_** )) .
_|_ _|_

Here, if φi(G, v|xi ) ∈ R[d][i], then f : R[d][1] _× · · · × R[d][ℓ]_ _→_ R[d] for some d ∈ N. That
is, φ generates an embedding in R[d]. We remark that our function applications include
concatenation.



-  Unconditional aggregations of the form φ(x) := agg[F]xj [(][φ][1][(][x][, x][j][))][ are][ k][-][MPNN][s, where]
_xj_ _X and xj_ **_x, and defined by_**
_∈_ _̸∈_

_φ(G, v) := F_ _{{φ1(G, v1, . . ., vj−1, w, vj+1, . . ., vk) | w ∈_ _VG}}_ _._

Here, if φ1 generates an embedding in  R[d][1], then F is an aggregation function assigning to
multisets of vectors in R[d][1] a vector in R[d], for some d ∈ N. So, φ generates an embedding
in R[d].

-  Conditional aggregations of the form φ(xi) := agg[F]xj [(][φ][1][(][x][i][, x][j][)][|][E][(][x][i][, x][j][))][ are][ k][-]
MPNNs, with xi, xj _X, and defined by_
_∈_

_φ(G, v) := F_ _φ1(G, vi, w)_ _w_ _NG(vi)_ _._
_{{_ _|_ _∈_ _}}_

As before, if φ1 generates an embedding in  R[d][1], then F is an aggregation function assigning
to multisets of vectors in R[d][1] a vector in R[d], for some d ∈ N. So again, φ generates an
embedding in R[d].

As defined in the main paper, we also consider the subclass k-MPNNs[(][t][)] by only considering kMPNNs defined in terms of expressions of aggregation depth at most t. Our main results, phrased
in terms of k-MPNNs are:

_ρ1(vwl[(]k[t][)][) =][ ρ][1][(][k][-][MPNN][s][(][t][)][)][ and][ ρ][0][(][gwl]k[) =][ ρ][0][(][k][-][MPNN][s][)][.]_

Hence, if the embeddings computed by GNNs are k-MPNNs, one obtains an upper bound on the
separation power in terms of k-WL.

The classical MPNNs (Gilmer et al., 2017) are subclass of 1-MPNNs in which no unconditional
aggregation can be used and furthermore, function applications require input embeddings with the
same single variable (x1 or x2), and only 1xi=xi and 1xi≠ _xi are allowed. In other words, they_
correspond to guarded tensor language expressions (Section 4.2). We denote this class of 1-MPNNs
by MPNNs and by MPNNs[(][t][)] when restrictions on aggregation depth are in place. And indeed, the
classical way of describing MPNNs as

_φ[(][0][)](x1) = (P1(x1), . . ., Pℓ(x1))_

_φ[(][t][)](x1) = f_ [(][t][)][]φ[(][t][−][1][)](x1), aggr[F]x2[(][t][)] _φ[(][t][−][1][)](x1), φ[(][t][−][1][)](x2)_ _E(xi, xj)_

_|_

correspond to 1-MPNNs that satisfy the above mentioned restrictions. Without readouts,  [] MPNNs
compute vertex embeddings and hence, our results imply

_ρ1(cr[(][t][)]) = ρ1(MPNNs[(][t][)])._

Furthermore, MPNNs with a readout function fall into the category of 1-MPNNs:

_φ := aggr[readout]x1_ (φ[(][t][)](x1))

where unconditional aggregation is used. Hence,


_ρ0(gcr[(][t][)]) = ρ0(gwl[(]1[t][)][) =][ ρ][0][(1][-][MPNN][s][(][t][+1][)][)][.]_

We thus see that k-MPNNs gracefully extend MPNNs and can be used for obtaining upper bounds
on the seperation power of classes of GNNs.


-----

