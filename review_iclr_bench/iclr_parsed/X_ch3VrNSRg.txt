# EE-NET: EXPLOITATION-EXPLORATION NEURAL NETWORKS IN CONTEXTUAL BANDITS

**Yikun Ban, Yuchen Yan, Arindam Banerjee, Jingrui He**
University of Illinois at Urbana-Champaign
{yikunb2, yucheny5, arindamb, jingrui}@illinois.edu

ABSTRACT

In this paper, we propose a novel neural exploration strategy in contextual bandits,
EE-Net, distinct from the standard UCB-based and TS-based approaches. Contextual multi-armed bandits have been studied for decades with various applications.
To solve the exploitation-exploration tradeoff in bandits, there are three main techniques: epsilon-greedy, Thompson Sampling (TS), and Upper Confidence Bound
(UCB). In recent literature, linear contextual bandits have adopted ridge regression
to estimate the reward function and combine it with TS or UCB strategies for
exploration. However, this line of works explicitly assumes the reward is based on
a linear function of arm vectors, which may not be true in real-world datasets. To
overcome this challenge, a series of neural bandit algorithms have been proposed,
where a neural network is used to learn the underlying reward function and TS or
UCB are adapted for exploration. Instead of calculating a large-deviation based
statistical bound for exploration like previous methods, we propose "EE-Net", a
novel neural-based exploration strategy. In addition to using a neural network
(Exploitation network) to learn the reward function, EE-Net uses another neural
network (Exploration network) to adaptively learn potential gains compared to the
currently estimated reward for exploration. Then, a decision-maker is constructed
to combine the outputs from the Exploitation and Exploration networks. We prove
that EE-Net can achieve O([√]T log T ) regret and show that EE-Net outperforms
existing linear and neural contextual bandit baselines on real-world datasets.

1 INTRODUCTION

The stochastic contextual multi-armed bandit (MAB) (Lattimore and Szepesvári, 2020) has been
studied for decades in machine learning community to solve sequential decision making, with
applications in online advertising (Li et al., 2010), personal recommendation (Wu et al., 2016; Ban
and He, 2021b), etc. In the standard contextual bandit setting, a set of n arms are presented to a
learner in each round, where each arm is represented by a context vector. Then by certain strategy,
the learner selects and plays one arm, receiving a reward. The goal of this problem is to maximize
the cumulative rewards of T rounds.

MAB algorithms have principled approaches to address the trade-off between Exploitation and
Exploration (EE), as the collected data from past rounds should be exploited to get good rewards but
also under-explored arms need to be explored with the hope of getting even better rewards. The most
widely-used approaches for EE trade-off can be classified into three main techniques: Epsilon-greedy
(Langford and Zhang, 2008), Thompson Sampling (TS) (Thompson, 1933), and Upper Confidence
Bound (UCB) (Auer, 2002; Ban and He, 2020).

Linear bandits (Li et al., 2010; Dani et al., 2008; Abbasi-Yadkori et al., 2011), where the reward is
assumed to be a linear function with respect to arm vectors, have been well studied and succeeded
both empirically and theoretically. Given an arm, ridge regression is usually adapted to estimate its
reward based on collected data from past rounds. UCB-based algorithms (Li et al., 2010; Chu et al.,
2011; Wu et al., 2016; Ban and He, 2021b) calculate an upper bound for the confidence ellipsoid
of estimated reward and determine the arm according to the sum of estimated reward and UCB.
TS-based algorithms (Agrawal and Goyal, 2013; Abeille and Lazaric, 2017) formulate each arm as
a posterior distribution where mean is the estimated reward and choose the one with the maximal


-----

**Gradient**


**Arm**


&![(!; (][!][)]

ℎ(!)


ℎ(!)


**Estimated Reward**

!"[(∇!]![; ']["][)]


**Expected Reward**

!"[(∇!]![; ']["][)]


(["]
([!]

! ∇!""[($; &]["][)]

!! !"

**Predict** **Predict**

r r −! [(%; '][!][)]


**Reward** **Potential Gain**

Exploitation Network !! Exploration Network !"


**Estimated Reward** &![(!; (][!][)] **Expected Reward**


**Case 1: Upward Exploration** **Case 2: Downward Exploration**

Adaptive Exploitation-Exploration: !! [+ !]"


Figure 1: Left figure: Structure of EE-Net. In the right figure, Case 1: "Upward" exploration should
be made when the learner underestimates the reward; Case 2: "Downward" exploration should
be chosen when the learner overestimates the reward. EE-Net has the ability to adaptively make
exploration according to different cases. In contrast, UCB-based strategy will always make upward
exploration, and TS-based strategy will randomly choose upward or downward exploration.

sampled reward. However, the linear assumption regarding the reward may not be true in real-world
applications (Valko et al., 2013).

To learn non-linear reward functions, recent works have utilized deep neural networks to learn the
underlying reward functions, thanks to its powerful representation ability. Considering the past
selected arms and received rewards as training samples, a neural network f1 is built for exploitation.
Zhou et al. (2020) computes a gradient-based upper confidence bound with respect to f1 and uses
UCB strategy to select arms. Zhang et al. (2021) formulates each arm as a normal distribution where
the mean is f1 and deviation is calculated based on gradient of f1, and then uses the TS strategy to
choose arms. Both Zhou et al. (2020) and Zhang et al. (2021) achieve the near-optimal regret bound
of O(√T log T ).

In this paper, we propose a novel neural exploration strategy, named "EE-Net". Similar to other
neural bandits, EE-Net has another exploitation network f1 to estimate rewards for each arm. The
crucial difference from existing works is that EE-Net has an exploration network f2 to predict the
potential gain for each arm compared to current reward estimate. The input to the exploration network
is the gradient of f1 and the ground-truth is residual difference between the true received reward
and the estimated reward from f1. The strategy is inspired by recent advances in the neural UCB
strategies (Zhou et al., 2020; Ban et al., 2021). Finally, a decision-maker f3 is constructed to select
arms. f3 has two modes: linear or nonlinear. In linear mode, f3 is a linear combination of f1 and f2,
inspired by the UCB strategy. In the nonlinear mode, f3 is formulated as a neural network with input
(f1, f2) and the goal is to learn the probability of being an optimal arm for each arm. Figure 1 depicts
the workflow of EE-Net and its advantages for exploration compared to UCB or TS-based methods
(see more details in Appendix D). To sum up, the contributions of this paper can be summarized as
follows:

1. We propose a novel neural exploration strategy, EE-Net, where another neural network is
assigned to learn the potential gain compared to the current reward estimate.

2. Under standard assumptions of over-parameterized neural networks, we prove that EE-Net
can achieve the regret upper bound of O([√]T log T ), which improves a multiplicative factor
of log T and is independent of either input or effective dimension, compared to existing

_[√]_
state-of-the-art neural bandit algorithms.

3. We conduct extensive experiments on four real-world datasets, showing that EE-Net outperforms baselines including linear and neural versions of ϵ-greedy, TS, and UCB.

Next, we discuss the problem definition in Sec.3, elaborate on the proposed EE-Net in Sec.4, and
present our theoretical analysis in Sec.5. In the end, we provide the empirical evaluation (Sec.6) and
conclusion.


-----

2 RELATED WORK

**Constrained Contextual bandits. The common constrain placed on the reward function is the linear**
assumption, usually calculated by ridge regression (Li et al., 2010; Abbasi-Yadkori et al., 2011; Valko
et al., 2013; Dani et al., 2008). The linear UCB-based bandit algorithms (Abbasi-Yadkori et al.,
2011; Li et al., 2016) and the linear Thompson Sampling (Agrawal and Goyal, 2013; Abeille and
Lazaric, 2017) can achieve successful performance and the near-optimal regret bound of _O[˜](√T_ ). To

break the linear assumption, Filippi et al. (2010) generalizes the reward function to a composition of
linear and non-linear functions and then adopt a UCB-based algorithm to deal with it; Bubeck et al.
(2011) imposes the Lipschitz property on reward metric space and constructs a hierarchical optimistic
optimization to make selections; Valko et al. (2013) embeds the reward function into Reproducing
Kernel Hilbert Space and proposes the kernelized TS/UCB bandit algorithms.

**Neural Bandits. To learn non-linear reward functions, deep neural networks have been adapted to**
bandits with various variants. Riquelme et al. (2018); Lu and Van Roy (2017) build L-layer DNN to
learn the arm embeddings and apply Thompson Sampling on the last layer for exploration. Zhou et al.
(2020) first introduces a provable neural-based contextual bandit algorithm with a UCB exploration
strategy and then Zhang et al. (2021) extends the neural network to Thompson sampling framework.
Their regret analysis is built on recent advances on the convergence theory in over-parameterized
neural networks(Du et al., 2019; Allen-Zhu et al., 2019) and utilizes Neural Tangent Kernel (Jacot
et al., 2018; Arora et al., 2019) to construct connections with linear contextual bandits (AbbasiYadkori et al., 2011). Ban and He (2021a) further adopts convolutional neural networks with UCB
exploration aiming for visual-aware applications. Xu et al. (2020) performs UCB-based exploration
on the last layer of neural networks to reduce the computational cost brought by gradient-based UCB.
Different from the above existing works, EE-Net keeps the powerful representation ability of neural
networks to learn reward function and first assigns another neural network to determine exploration.

3 PROBLEM DEFINITION

We consider the standard contextual multi-armed bandit with the known number of rounds T (Zhou
et al., 2020; Zhang et al., 2021). In each round t ∈ [T ], where the sequence [T ] = [1, 2, . . ., T ],
the learner is presented with n arms, Xt = {xt,1, . . ., xt,n}, in which each arm is represented by a
feature vectorgenerated by the function: xt,i ∈ R[d] for each i ∈ [n]. After playing one arm xt,i, its reward rt,i is assumed to be
_rt,i = h(xt,i) + ηt,i,_ (3.1)
where the unknown reward function h(xt,i) can be either linear or non-linear and the noise ηt,i is
drawn from certain distribution with expectation E[ηt,i] = 0. Following many existing works (Zhou
et al., 2020; Ban et al., 2021; Zhang et al., 2021), we consider bounded rewards, rt,i [a, b]. For the
brevity, we denote the selected arm in round t by xt and the reward received in t by ∈ rt. The pseudo
_regret of T rounds is defined as:_

_T_

**RT = E** (rt[∗] _,_ (3.2)

"t=1 _[−]_ _[r][t][)]#_
X

where E[rt[∗] _i_ [n] _[h][(][x][t,i][)][ is the maximal expected reward in the round][ t][. The goal of this]_
_∈_
problem is to minimize[|][ X][t][] = max] RT by certain selection strategy.

**Notation. We denote by {xi}i[t]=1** [the sequence][ (][x][1][, . . .,][ x][t][)][. We use][ ∥][v][∥][2][ to denote the Euclidean]
norm for a vector v, and ∥W∥2 and ∥W∥F to denote the spectral and Frobenius norm for a matrix
**W. We use ⟨·, ·⟩** to denote the standard inner product between two vectors or two matrices. We may
use ▽θ1t _[f][1][(][x][t,i][)][ or][ ▽][θ]t[1]_ _[f][1][ to represent the gradient][ ▽][θ]t[1]_ _[f][1][(][x][t,i][;][ θ]t[1][)][ for brevity. We use][ {][x][τ]_ _[, r][τ]_ _[}][t]τ_ =1
to represent the collected data up to round t.

4 PROPOSED METHOD: EE-NET

EE-Net is composed of three components. The first component is the exploitation network, f1( ; θ[1]),

_·_
which focuses on learning the unknown reward function h based on the data collected in past rounds.


-----

|Input|Network|Label|
|---|---|---|


|x t { τ}τ=1|f (; θ1) (Exploitation) 1 ·|r t { τ}τ=1|
|---|---|---|


|{▽ θ1 τ−1f 1(x τ; θ1 τ−1) }t τ=1|f (; θ2) (Exploration) 2 ·| r f (x ; θ1 ) t τ − 1 τ τ−1 τ|
|---|---|---|


|{(f 1(x τ; θ1 τ−1), f 2(▽ τ−1f 1; θ2 τ−1)) }t θ1 τ=1|f (; θ3) (Decision- 3 · maker with non-linear function)|p t { τ}τ=1|
|---|---|---|


The second component is the exploration network, f2( ; θ[2]), which focuses on characterizing the level

_·_
of exploration needed for each arm in the present round. The third component is the decision-maker,
_f3, which focuses on suitably combining the outputs of the exploitation and exploration networks_
leading to the arm selection.

**1) Exploitation Net. The exploitation net f1 is a neural network which learns the mapping from**
arms to rewards. In round t, denote the network by f1( ; θ[1]t 1[)][, where the superscript of][ θ][1]t 1 [is the]

_·_ _−_ _−_
index of network and the subscript represents the round where the parameters of f1 finished the last
update. Given an arm xt,i, i [n], f1(xt,i; θ[1]t 1[)][ is considered the "exploitation score" for][ x][t,i][. By]
_∈_ _−_
some criterion, after playing arm xt, we receive a reward rt. Therefore, we can conduct gradient
descent to update θ[1] based on the collected training samples {xτ _, rτ_ _}τ[t]_ =1 [and denote the updated]
parameters by θ[1]t [.]

**2) Exploration Net. Our exploration strategy is inspired by existing UCB-based neural bandits**
(Zhou et al., 2020; Ban et al., 2021). Based on the Lemma 5.2 in (Ban et al., 2021), given an arm xt,i,
with probability at least 1 − _δ, we have the following UCB form:_

_h(xt,i)_ _f1(xt,i; θ[1]t_ 1[)][| ≤] [Ψ(][▽]θ[1]t 1 _[f][1][(][x][t,i][;][ θ]t[1]_ 1[))][,] (4.1)
_|_ _−_ _−_ _−_ _−_

where h is defined in Eq. (3.1) and Ψ is an upper confidence bound represented by a function with
respect to the gradient ▽θ1t 1 _[f][1][ (see more details and discussions in Appendix D). Then we have the]_
_−_
following definition.

**Definition 4.1. In round t, given an arm xt,i, we define h(xt,i)** _f1(xt,i; θ[1]t_ 1[)][ as the "][expected]
_−_ _−_
potential gain" for xt,i and rt,i _f1(xt,i; θ[1]t_ 1[)][ as the "][potential gain][" for][ x][t,i][.]
_−_ _−_

Let yt,i = rt,i _f1(xt,i; θ[1]t_ 1[)][. When][ y][t,i] _[>][ 0][, the arm][ x][t,i]_ [has positive potential gain compared to]
_−_ _−_
the estimated reward f1(xt,i; θ[1]t 1[)][. A large positive][ y][t,i] [makes the arm more suitable for exploration,]
_−_
whereas a small (or negative) yt,i makes the arm unsuitable for exploration. Recall that traditional
approaches such as UCB intend to estimate such potential gain yt,i using standard tools, e.g., Markov
inequality, Hoeffding bounds, etc., from large deviation bounds.

Instead of calculating a large-deviation based statistical bound for yt,i, we use a neural network
_f2(_ ; θ[2]) to represent Ψ, where the input is ▽θ1t 1 _[f][1][(][x][t,i][)][ and the ground truth is][ r][t,i][ −]_ _[f][1][(][x][t,i][;][ θ]t[1]_ 1[)][.]

_·_ _−_ _−_
Adopting gradient ▽θ1t 1 _[f][1][(][x][t,i][)][ as the input also is due to the fact that it incorporates two aspects of]_
_−_
information: the feature of the arm and the discriminative information of f1.

Moreover, in the upper bound of NeuralUCB or the variance of NeuralTS, there is a recursive term
**At−1 = I +** _τ_ =1 _τ_ _−1_ _[f][1][(][x][τ]_ [)][∇][θ][1]τ _−1_ _[f][1][(][x][τ]_ [)][⊤] [which is a function of past gradients up to][ (][t][ −] [1)]

_[∇][θ][1]_
and incorporates relevant historical information. On the contrary, in EE-Net, the recursive term
which depends on past gradients is[P][t][−][1] **_θ[2]t−1_** [in the exploration network][ f][2] [because we have conducted]
gradient descent for θ[2]t 1 [based on][ {∇]θ[1]τ 1 _[f][1][(][x][τ]_ [)][}][t]τ[−]=1[1] [. Therefore, this form][ θ]t[2] 1 [is similar to][ A][t][−][1]
_−_ _−_ _−_
in neuralUCB/TS, but EE-net does not (need to) make a specific assumption about the functional
form of past gradients, and is also more memory-efficient.

To sum up, in round t, we consider f2(▽θ1t 1 _[f][1][(][x][t,i][);][ θ]t[2]_ 1[)][ as the "exploration score" of][ x][t,i][,]
_−_ _−_

because it indicates the potential gain of xt,i compared to our current exploitation score f1(xt,i; θ[1]t 1[)][.]
_−_
Therefore, after receiving the reward rt, we can use gradient descent to update θ[2] based on collected
training samples ▽θ1τ 1 _[f][1][(][x][τ]_ [)][, r][τ][ −] _[f][1][(][x][τ]_ [;][ θ]τ[1] 1[}][t]τ =1[. We also provide other two heuristic forms]
_{_ _−_ _−_

for f2’s ground-truth label: _rt,i_ _f1(xt,i; θ[1]t_ 1[)][|][ and][ ReLU][(][r][t,i] _t_ 1[))][. We compare]
them in an ablation study in Appendix B. | _−_ _−_ _[−]_ _[f][1][(][x][t,i][;][ θ][1]−_


-----

**Algorithm 1 EE-Net**
**Input: f1, f2, f3, T (number of rounds), η1 (learning rate for f1), η2 (learning rate for f2), η3**
(learning rate for f3), K1 (number of iterations for f1), K2 (number of iterations for f2), K3
(number of iterations for f3), φ (normalization operator)

1 2 3
1: Initialize θ[1]0[,][ θ][2]0[,][ θ][3]0[;][ b]θ0 [=][ θ]0[1][,][ b]θ0 [=][ θ]0[2][,][ b]θ0 [=][ θ]0[3]

2: for t = 1, 2, . . ., T do
3: Observe n arms **xt,1, . . ., xt,n**
_{_ _}_

4: **for each i ∈** [n] do

5: Compute f1(xt,i; θ[1]t 1[)][, f][2][(][φ][(][▽]θ[1]t 1 _[f][1][(][x][t,i][));][ θ]t[2]_ 1[)][, f][3][((][f][1][, f][2][);][ θ][3]t 1[)]
_−_ _−_ _−_ _−_

6: **end for**

7: **xt = arg maxxt,i,i∈[n] f3** _f1(xt,i; θ[1]t−1[)][, f][2][(][φ][(][▽]θ[1]t−1_ _[f][1][(][x][t,i][));][ θ]t[2]−1[);][ θ][3]t−1_

8: Play xt and observe reward rt 

9: **_θ[1]t_** _[,][ θ][2]t_ _[,][ θ][3]t_ [= G][RADIENT][D][ESCENT][(][θ][0][,][ {][x][τ] _[}]τ[t]_ =1[,][ {][r][τ] _[}][t]τ_ =1[)]

10: end for
11:

13:12: procedure1 = [1]2 GRADIENTtτ =1 _f1(DxτESCENT; θ[1])_ (rθτ0,2 {xτ _}τ[t]_ =1[,][ {][r][τ] _[}][t]τ_ =1[)]
_L_ _−_

14: **_θ[1][,][(0)]_** = θ[1]0

P   

15: **for k ∈{1, . . ., K1} do**

16: **_θ[1][,][(][k][)]_** = θ[1][,][(][k][−][1)] _η1▽θ1,(k−1)_ 1
_−_ _L_

17: **end for**

1
18: **_θt_** [=][ θ][1][,][(][K][1][)]

2
_t_

19: 2 = [1]2 _τ_ =1 _f2(φ(▽θ1τ_ 1 _[f][1][(][x][τ]_ [));][ θ][2][)][ −] [(][r][τ][ −] _[f][1][(][x][τ]_ [;][ θ]τ[1] 1[))]
_Lb_ _−_ _−_

20: **_θ[2][,][(0)]_** =P θ[2]0  

21: **for k ∈{1, . . ., K2} do**

22: **_θ[2][,][(][k][)]_** = θ[2][,][(][k][−][1)] _η2▽θ2,(k−1)_ 2
_−_ _L_

23: **end for**

2
24: **_θt_** [=][ θ][2][,][(][K][2][)]

25: Determine label pt

26: b 3 = _t_ _ti=1_ _pt log f3((f1, f2); θ[3]) + (1_ _pt) log(1_ _f3((f1, f2); θ[3]))_
_L_ _−_ [1] _−_ _−_

27: **_θ[3][,][(0)]_** = θ[3]0

P  

28: **for k ∈{1, . . ., K3} do**

29: **_θ[3][,][(][k][)]_** = θ[3][,][(][k][−][1)] _η3▽θ3,(k−1)_ 3
_−_ _L_

30: **end for**

3
31: **_θt_** [=][ θ][3][,][(][K][3][)]

1 2 1 2 1 2
32: Randomly choose (θ[1]t [,][ θ][2]t [) uniformly from][ {][(]θ[b]0[,][ b]θ0[)][,][ (]θ[b]1[,][ b]θ1[)][, . . .,][ (]θ[b]t _[,][ b]θt_ [)][}]

b 3 3 3

33: Randomly choose θ[3]t [uniformly from][ {]θ[b]0[,][ b]θ1[, . . .,][ b]θt _[}]_

34: **Return θ[1]t** [,][ θ][2]t [,][ θ][3]t

35: end procedure


**3) Decision-maker. In round t, given an arm xt,i, i ∈** [n], with the computed exploitation score
_f1(xt,i; θ[1]t_ 1[)][ and exploration score][ f][2][(][▽]θ[1]t 1 _[f][1][;][ θ]t[2]_ 1[)][, we use a function][ f][3] _f1, f2; θ[3][]_ to trade
_−_ _−_ _−_
off between exploitation and exploration and compute the final score for xt,i. The selection criterion
 
is defined as

**xt = arg** **xt,imax,i** [n] _[f][3]_ _f1(xt,i; θ[1]t−1[)][, f][2]_ ▽θ1t−1 _[f][1][(][x][t,i][);][ θ]t[2]−1_ ; θ[3]t−1 _._
_∈_
   

Note that f3 can be either linear or non-linear functions. We provide the following two forms.

(1) Linear function. f3 can be formulated as a linear function with respect to f1 and f2 :

_f3(f1, f2; θ[3]) = w1f1(xt,i; θ[1]) + w2f2(▽θ1_ _f1; θ[2])_


-----

where w1, w2 are two weights preset by the learner. When w1 = w2 = 1, f3 can be thought of as
UCB-type policy, where the estimated reward f1 and potential gain f2 are simply added together. In
experiments, we report its empirical performance in ablation study (Appendix B).

(2) Non-linear function. f3 also can be formulated as a neural network to learn the mapping from
(f1, f2) to the optimal arm. We transform the bandit problem into a binary classification problem.
Given an arm xt,i, we define pt,i as the probability of being the optimal arm for xt,i in round t. For
brevity, we denote by pt the probability of being the optimal arm for the selected arm xt in round t.
According to different reward distributions, we have different approaches to determine pt.

1. Binary reward. ∀t ∈ [T ], suppose rt is a binary variable over a, b(a < b), it is straightforward to set: pt = 1.0 if rt = b; pt = 0.0, otherwise.

2. Continuous reward. ∀t ∈ [T ], suppose rt is a continuous variable over the range [a, b], we
provide two ways to determine pt. (1) pt can be directly set as _[r]b[t][−]a[a]_ [. (2) The learner can set]

_−_
a threshold θ, (a < θ < b). Then pt = 1.0 if rt > θ; pt = 0.0, otherwise.


_t_
Therefore, with the collected training samples _f1(xτ_ ; θ[1]τ 1[)][, f][2][(][▽]θ[1]τ 1 _[f][1][;][ θ]τ[2]_ 1[)] _, pτ_
_−_ _−_ _−_ _τ_ =1 [in]

round t, we can conduct gradient descent to update parameters ofn _f3(_ ; θ[3]).  o

_·_

Table 1 details the working structure of EE-Net. Algorithm 1 depicts the workflow of EE-Net, where
the input of f2 is normalized, i.e., φ(▽θ1t 1 _[f][1][(][x][t,i][))][. Algorithm 1 provides a version of gradient]_
_−_

descent (GD) to update EE-Net, where drawing (θ[1]t _[,][ θ][2]t_ [)][ uniformly from their stored historical]
parameters is for the sake of analysis. One can easily extend EE-Net to stochastic GD to update the
parameters incrementally.

**Remark 4.1 (Network structure). The networks f1, f2, f3 can be different structures according**
to different applications. For example, in the vision tasks, f1 can be set up as convolutional layers
(LeCun et al., 1995). For the exploration network f2, the input ▽θ1 _f1 may have exploding dimensions_
when the exploitation network f1 becomes wide and deep, which may cause huge computation cost
for f2. To address this challenge, we can apply dimensionality reduction techniques to obtain lowdimensional vectors of ▽θ1 _f1. In the experiments, we use Roweis and Saul (2000) to acquire a_
10-dimensional vector for ▽θ1 _f1 and achieve the best performance among all baselines._

**Remark 4.2 (Exploration direction). EE-Net has the ability to determine exploration direction.**
Given an arm xt,i, when the estimation f1(xt,i) is lower than the expected reward h(xt,i), the learner
should make the "upward" exploration, i.e., increase the chance of xt,i being explored; When f1(xt,i)
is higher than h(xt,i), the learner should do the "downward" exploration, i.e., decrease the chance
of xt,i being explored. EE-Net uses the neural network f2 to learn h(xt,i) − _f1(xt,i) (which has_
positive and negative scores) and has the ability to determine the exploration direction. In contrast,
NeuralUCB will always make "upward" exploration and NeuralTS will randomly choose between
"upward" exploration and "downward" exploration (see selection criteria in Table 2 and more details
in Appendix D).

**Remark 4.3 (Space complexity). NeuralUCB and NeuralTS have to maintain the gradient outer**
product matrix (e.g., At = _τ_ =1 [▽][θ][1] _[f][1][(][x][τ]_ [;][ θ]τ[1] [)][▽]θ[1] _[f][1][(][x][τ]_ [;][ θ][1]τ [)][⊤] _[∈]_ [R][p][×][p][) and, for][ θ][1][ ∈] [R][p][, have]
a space complexity of O(p[2]) to store the outer product. On the contrary, EE-Net does not have this
matrix and only regards ▽θ1[P]f1[t] as the input of f2. Thus, EE-Net reduces the space complexity from
_O(p[2]) to O(p)._

5 REGRET ANALYSIS

In this section, we provide the regret analysis of EE-Net when f3 is set as the linear function
_f3 = f1_ +f2, which can be thought of as the UCB-type trade-off between exploitation and exploration.
For the sake of simplicity, we conduct the regret analysis on some unknown but fixed data distribution
. In each round t, n samples (xt,1, rt,1), (xt,2, rt,2), . . ., (xt,n, rt,n) are drawn i.i.d. from .
_D_ _{_ _}_ _D_
This is standard distribution assumption in over-parameterized neural networks (Cao and Gu, 2019).
Then, for the analysis, we have the following assumption, which is a standard input assumption in
neural bandits and over-parameterized neural networks(Zhou et al., 2020; Allen-Zhu et al., 2019).


-----

**Assumption 5.1every pair xt,i, x (t′ρ,i-Separability)′, t[′]** [T ], i[′] **. For any[k], and t ( ∈t, i[)T= (], i ∈t[′], i[n[′])],, ∥xxt,it,i∥2 = 1xt′,i, and′** 2 > ρ rt,i, and suppose there ∈ [0, 1]. Then, for
exists an operator such that∈ _∥φ(·)∥∈2 = 1 and ∥φ(▽ ̸_ **_θ1_** _f1(xt,i ∥)) −_ _−φ(▽θ1_ _f1∥(xt′,i′_ ))∥2 ≥ _ρ_

For example, the operator can be designed as φ(▽θ1 _f1(xt,i)) = (_ _√2∥▽▽θθ11ff11(x(xt,it,i))∥2_ _[,][ x]√[t,i]2_ [)][. The]

analysis will focus on over-parameterized neural networks (Jacot et al., 2018; Du et al., 2019; AllenZhu et al., 2019). Given an input x ∈ R[d], without loss of generality, we define the fully-connected
network f with depth L ≥ 2 and width m:

_f_ (x; θ) = WLσ(WL 1σ(WL 2 . . . σ(W1x))) (5.1)
_−_ _−_

**Wwhere[L]** _∈ σR is the ReLU activation function,[1][×][m], and θ = [vec(W1)[⊺], vec(W W2)[⊺], . . .,1 ∈_ vecR[m]([×]W[d], WL)[⊺]l] ∈[⊺]. R[m][×][m], for 2 ≤ _l ≤_ _L −_ 1,

_Initialization. For any l ∈_ [L − 1], each entry of Wl is drawn from the normal distribution N (0, _m[2]_ [)]

and WL is drawn from the normal distribution N (0, _m[1]_ [)][. Note that EE-Net at most has three networks]

_f1, f2, f3. We define them following the definition of f for brevity, although they may have different_
depth or width. Then, we have the following theorem for EE-Net. Recall that η1, η2 are the learning
rates for f1, f2; K1 is the number of iterations of gradient descent for f1 in each round; and K2 is the
number of iterations for f2.
**Theorem 1. Let f1, f2 follow the setting of f (Eq. (5.1) ) with the same width m and depth L. Let**
_L1, L2 be loss functions defined in Algorithm 1. Set f3 as f3 = f1 + f2. For any δ ∈_ (0, 1), ϵ ∈
(0, O( _T[1]_ [)]][, ρ][ ∈] [(0][,][ O][(][ 1]L [)]][, suppose]

_√log(T n/δ)_
_m ≥_ Ω[e] _poly(T, n, L, ρ[−][1]) · log(1/δ) · e_ ) _,_
 _T_ [5] _ρ_ 

_η1 = η2 = min_ Θ _√2δ[2]m_ _, Θ_ _poly(T, n, L)_ _m_ _,_ (5.2)
    _·_ 

_poly(T, n, L)_
_K1 = K2 = Θ_ log _ϵ[−][1][]_ _._

_ρδ[2]_ _·_



 

_Then, with probability at least 1 −_ _δ over the initialization, the pseudo regret of EE-Net in T rounds_
_satisfies_


2 log

_[O][(][Tn]δ_ [)]


(5.3)


**RT** (1) + (2
_≤O_


_T −_ 1)3


2O(L) + O


(2


_T −_ 1)


**Comparison with existing works. Under the similar assumptions in over-parameterized neural**
networks, the regret bounds complexity of NeuralUCB (Zhou et al., 2020) and NeuralTS (Zhang
et al., 2021) both are

**RT** _dT˜_ log T _d˜ log T_ _, and_ _d[˜] = [log][ det][(][I][ +][ H][/λ][)]_
_≤O_ _· O_ log(1 + Tn/λ)
q  q 

where H is the neural tangent kernel matrix (NTK) (Jacot et al., 2018; Arora et al., 2019) and λ is a
regularization parameter. Similarly, in linear contextual bandits, Abbasi-Yadkori et al. (2011) achieve
_O(d√T log T_ ) and Li et al. (2017) achieve O(√dT log T ).

**Remark 5.1. Compared to NeuralUCB/TS, EE-Net roughly improves by a multiplicative factor**
of log T, because our proof of EE-Net is directly built on recent advances in convergence theory

_[√]_
(Allen-Zhu et al., 2019) and generalization bound (Cao and Gu, 2019) of over-parameterized neural
networks. Instead, the analysis for NeuralUCB/TS contains three parts of approximation error by
calculating the distances between the expected reward and ridge regression, ridge regression and
NTK, and NTK and network function.
**Remark 5.2. The regret bound of EE-Net does not have the effective dimension** _d[˜] or input dimension_
_d._ _d[˜] or d may cause significant error, when the determinant of H is extremely large or d > T_ .

The proof of Theorem 1 is in Appendix C and mainly based on the following generalization bound.
The bound results from an online-to-batch conversion while using convergence guarantees of deep
learning optimization.


-----

**Lemma 5.1. For any δ ∈** (0, 1), ϵ ∈ (0, 1), ρ ∈ (0, O( _L[1]_ [))][, suppose][ m, η][1][, η][2][, K][1][, K][2][ satisfy the]

_conditions in Eq. (5.2) and (xτ,i, rτ,i)_ _,_ _τ_ [t], i [n]. Let
_∼D_ _∀_ _∈_ _∈_


_f2_ _φ(▽θ1t_ 1 _[f][1][(][x][t,i][;][ θ]t[1]_ 1[));][ θ][2]t 1 + f1(xt,i; θ[1]t 1[)]
_−_ _−_ _−_ _−_
 


**xt = arg** max
**xt,i,i∈[n]**


_and rt is the corresponding reward, given (xt,i, rt,i), i ∈_ [n]. Then, with probability at least (1 − _δ)_
_over the random of the initialization, it holds that_


_f2_ _φ(▽θ1t_ 1 _[f][1][(][x][t,i][;][ θ]t[1]_ 1[));][ θ][2]t 1 _rt_ _f1(xt; θ[1]t_ 1[)] **xτ** _, rτ_ _τ_ =1
_−_ _−_ _−_ _−_ _−_ _−_ _| {_ _}[t][−][1]_
    

2ϵ 3L 2 log( (tn/δ))

+ (1 + 2ξ) _O_ _,_

_≤_ r _t_ [+][ O]  _√2t_  r _t_


(xt,i,rt,i),i∈[n]


(5.4)


1 2
_where the expectation is also taken over (θ[1]t_ 1[,][ θ][2]t 1[)][ that are uniformly drawn from][ (]θ[b]τ _[,][ b]θτ_ [)][, τ][ ∈]
_−_ _−_

[t − 1].

**Remark 5.3. Lemma 5.1 provides a fixed** _O[˜](_ _√[1]t_ [)][-rate generalization bound for exploitation-]

exploration networks f1, f2 in contrast with the relative bound w.r.t. the Neural Tangent Random
Feature (NTRF) benchmark (Cao and Gu, 2019). We achieve this by working in the regression rather
than classification setting and utilizing the convergence guarantees for square loss (Allen-Zhu et al.,
2019). Note that the bound in Lemma 5.1 holds in the setting of bounded (possibly random) rewards
_r ∈_ [0, 1] instead of a fixed function in the conventional classification setting.

6 EXPERIMENTS

In this section, we evaluate EE-Net on four real-world datasets comparing with strong state-of-the-art
baselines. We first present the setup of experiments, then show regret comparison and report ablation

and unknown user parameter and then applies the ridge regression and un upper confidence

study. Codes are available at .

We use four real-world datasets: Mnist, Yelp, Movielens, and Disin, the details and settings of
which are attached in Appendix A.

Figure 2: Regret comparison on Movielens and Yelp (mean of 10 runs with standard deviation
(shadow)). With the same exploitation network f1, EE-Net outperforms all baselines.
**Baselines. To comprehensively evaluate EE-Net, we choose 3 neural-based bandit algorithms, one**
linear and one kernelized bandit algorithms.

1. LinUCB (Li et al., 2010) explicitly assumes the reward is a linear function of arm vector

bound to determine selected arm.

2. KernelUCB (Valko et al., 2013) adopts a predefined kernel matrix on the reward space
combined with a UCB-based exploration strategy.

3. Neural-Epsilon adapts the epsilon-greedy exploration strategy on exploitation network f1.
I.e., with probability 1 _ϵ, the arm is selected by xt = arg maxi_ [n] f1(xt,i; θ[1]) and with
_−_ _∈_
probability ϵ, the arm is chosen randomly.

1https://github.com/banyikun/EE-Net-ICLR-2022


-----

Figure 3: Regret comparison on Mnist and Disin (mean of 10 runs with standard deviation (shadow)).
With the same exploitation network f1, EE-Net outperforms all baselines.

4. NeuralUCB (Zhou et al., 2020) uses the exploitation network f1 to learn the reward function
coming with an UCB-based exploration strategy.

5. NeuralTS (Zhang et al., 2021) adopts the exploitation network f1 to learn the reward function
coming with an Thompson Sampling exploration strategy.

Note that we do not report results of LinTS and KernelTS in experiments, because of the limited
space in figures, but LinTS and KernelTS have been significantly outperformed by NeuralTS (Zhang
et al., 2021).

**Setup for EE-Net. To compare fairly, for all the neural-based methods including EE-Net, the**
exploitation network f1 is built by a 2-layer fully-connected network with 100 width. For the
exploration network f2, we use a 2-layer fully-connected network with 100 width as well. For the
decision maker f3, by comprehensively evaluate both linear and nonlinear functions, we found that
the most effective approach is combining them together, which we call " hybrid decision maker". In
detail, for rounds t ≤ 500, f3 is set as f3 = f2 + f1, and for t > 500, f3 is set as a neural network
with two 20-width fully-connected layers. Setting f3 in this way is because the linear decision maker
can maintain stable performance in each running (robustness) and the non-linear decision maker can
further improve the performance (see details in Appendix B). The hybrid decision maker can combine
these two advantages together. The configurations of all methods are attached in Appendix A.

**Results. Figure 2 and Figure 3 show the regret comparison on these four datasets. EE-Net consistently**
outperforms all baselines across all datasets. For LinUCB and KernelUCN, the simple linear reward
function or predefined kernel cannot properly formulate ground-truth reward function existed in
real-world datasets. In particular, on Mnist and Disin datasets, the correlations between rewards
and arm feature vectors are not linear or some simple mappings. Thus, LinUCB and KernelUCB
barely exploit the past collected data samples and fail to select correct arms. For neural-based
bandit algorithms, the exploration probability of Neural-Epsilon is fixed and difficult to be adjustable.
Thus it is usually hard to make effective exploration. To make exploration, NeuralUCB statistically
calculates a gradient-based upper confidence bound and NeuralTS draws each arm’s predicted reward
from a normal distribution where the standard deviation is computed by gradient. However, the
confidence bound or standard deviation they calculated only consider the worst cases and thus
may not be able represent the actual potential of each arm, and they cannot make "upward" and
"downward" exploration properly. Instead, EE-Net uses a neural network f2 to learn each arm’s
potential by neural network’s powerful representation ability. Therefore, EE-Net can outperform
these two state-of-the-art bandit algorithms. Note that NeuralUCB/TS does need two parameters to
tune UCB/TS according to different scenarios while EE-Net only needs to set up a neural network
and automatically learns it.

**Ablation Study. In Appendix B, we conduct ablation study regarding the label function y of f2 and**
the different setting of f3.
7 CONCLUSION
In this paper, we propose a novel exploration strategy, EE-Net. In addition to a neural network that
exploits collected data in past rounds, EE-Net has another neural network to learn the potential gain
compared to current estimation for exploration. Then, a decision maker is built to make selections
to further trade off between exploitation and exploration. We demonstrate that EE-Net outperforms
NeuralUCB and NeuralTS both theoretically and empirically, becoming the new state-of-the-art
exploration policy.


-----

**Acknowledgements: We are grateful to Shiliang Zuo and Yunzhe Qi for the valuable discussions**
in the revisions of EE-Net. This research work is supported by National Science Foundation
under Awards No. IIS-1947203, IIS-2002540, IIS-2137468, IIS-1908104, OAC-1934634, and DBI2021898, and a grant from C3.ai. The views and conclusions are those of the authors and should not
be interpreted as representing the official policies of the funding agencies or the government.

REFERENCES

Y. Abbasi-Yadkori, D. Pál, and C. Szepesvári. Improved algorithms for linear stochastic bandits. In
_Advances in Neural Information Processing Systems, pages 2312–2320, 2011._

M. Abeille and A. Lazaric. Linear thompson sampling revisited. In Artificial Intelligence and
_Statistics, pages 176–184. PMLR, 2017._

S. Agrawal and N. Goyal. Thompson sampling for contextual bandits with linear payoffs. In
_International Conference on Machine Learning, pages 127–135. PMLR, 2013._

H. Ahmed, I. Traore, and S. Saad. Detecting opinion spams and fake news using text classification.
_Security and Privacy, 1(1):e9, 2018._

Z. Allen-Zhu, Y. Li, and Z. Song. A convergence theory for deep learning via over-parameterization.
In International Conference on Machine Learning, pages 242–252. PMLR, 2019.

S. Arora, S. S. Du, W. Hu, Z. Li, R. R. Salakhutdinov, and R. Wang. On exact computation with
an infinitely wide neural net. In Advances in Neural Information Processing Systems, pages
8141–8150, 2019.

P. Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine
_Learning Research, 3(Nov):397–422, 2002._

Y. Ban and J. He. Generic outlier detection in multi-armed bandit. In Proceedings of the 26th ACM
_SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 913–923,_
2020.

Y. Ban and J. He. Convolutional neural bandit: Provable algorithm for visual-aware advertising.
_arXiv preprint arXiv:2107.07438, 2021a._

Y. Ban and J. He. Local clustering in contextual multi-armed bandits. In Proceedings of the Web
_Conference 2021, pages 2335–2346, 2021b._

Y. Ban, J. He, and C. B. Cook. Multi-facet contextual bandits: A neural network perspective. In
_The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event,_
_Singapore, August 14-18, 2021, pages 35–45, 2021._

S. Bubeck, R. Munos, G. Stoltz, and C. Szepesvári. X-armed bandits. Journal of Machine Learning
_Research, 12(5), 2011._

Y. Cao and Q. Gu. Generalization bounds of stochastic gradient descent for wide and deep neural
networks. Advances in Neural Information Processing Systems, 32:10836–10846, 2019.

N. Cesa-Bianchi, A. Conconi, and C. Gentile. On the generalization ability of on-line learning
algorithms. Advances in neural information processing systems, 14, 2001.

E. Chlebus. An approximate formula for a partial sum of the divergent p-series. Applied Mathematics
_Letters, 22(5):732–737, 2009._

W. Chu, L. Li, L. Reyzin, and R. Schapire. Contextual bandits with linear payoff functions. In
_Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,_
pages 208–214, 2011.

V. Dani, T. P. Hayes, and S. M. Kakade. Stochastic linear optimization under bandit feedback. 2008.

S. Du, J. Lee, H. Li, L. Wang, and X. Zhai. Gradient descent finds global minima of deep neural
networks. In International Conference on Machine Learning, pages 1675–1685. PMLR, 2019.


-----

S. Filippi, O. Cappe, A. Garivier, and C. Szepesvári. Parametric bandits: The generalized linear case.
In Advances in Neural Information Processing Systems, pages 586–594, 2010.

D. Fu and J. He. SDG: A simplified and dynamic graph neural network. In SIGIR ’21: The 44th
_International ACM SIGIR Conference on Research and Development in Information Retrieval,_
_Virtual Event, Canada, July 11-15, 2021, pages 2273–2277. ACM, 2021._

F. M. Harper and J. A. Konstan. The movielens datasets: History and context. Acm transactions on
_interactive intelligent systems (tiis), 5(4):1–19, 2015._

A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in neural
networks. In Advances in neural information processing systems, pages 8571–8580, 2018.

J. Langford and T. Zhang. The epoch-greedy algorithm for multi-armed bandits with side information.
In Advances in neural information processing systems, pages 817–824, 2008.

T. Lattimore and C. Szepesvári. Bandit algorithms. Cambridge University Press, 2020.

Y. LeCun, Y. Bengio, et al. Convolutional networks for images, speech, and time series. The handbook
_of brain theory and neural networks, 3361(10):1995, 1995._

Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

L. Li, W. Chu, J. Langford, and R. E. Schapire. A contextual-bandit approach to personalized news
article recommendation. In Proceedings of the 19th international conference on World wide web,
pages 661–670, 2010.

L. Li, Y. Lu, and D. Zhou. Provably optimal algorithms for generalized linear contextual bandits. In
_International Conference on Machine Learning, pages 2071–2080. PMLR, 2017._

S. Li, A. Karatzoglou, and C. Gentile. Collaborative filtering bandits. In Proceedings of the 39th
_International ACM SIGIR conference on Research and Development in Information Retrieval,_
pages 539–548, 2016.

X. Lu and B. Van Roy. Ensemble sampling. arXiv preprint arXiv:1705.07347, 2017.

C. Riquelme, G. Tucker, and J. Snoek. Deep bayesian bandits showdown: An empirical comparison
of bayesian deep networks for thompson sampling. arXiv preprint arXiv:1802.09127, 2018.

S. T. Roweis and L. K. Saul. Nonlinear dimensionality reduction by locally linear embedding. science,
290(5500):2323–2326, 2000.

W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of the
evidence of two samples. Biometrika, 25(3/4):285–294, 1933.

M. Valko, N. Korda, R. Munos, I. Flaounas, and N. Cristianini. Finite-time analysis of kernelised
contextual bandits. arXiv preprint arXiv:1309.6869, 2013.

Q. Wu, H. Wang, Q. Gu, and H. Wang. Contextual bandits in a collaborative environment. In
_Proceedings of the 39th International ACM SIGIR conference on Research and Development in_
_Information Retrieval, pages 529–538, 2016._

P. Xu, Z. Wen, H. Zhao, and Q. Gu. Neural contextual bandits with deep representation and shallow
exploration. arXiv preprint arXiv:2012.01780, 2020.

W. Zhang, D. Zhou, L. Li, and Q. Gu. Neural thompson sampling. In International Conference on
_Learning Representations, 2021._

D. Zhou, L. Li, and Q. Gu. Neural contextual bandits with ucb-based exploration. In International
_Conference on Machine Learning, pages 11492–11502. PMLR, 2020._


-----

A DATASETS AND SETUP

**MNIST dataset. MNIST is a well-known image dataset (LeCun et al., 1998) for the 10-class**
classification problem. Following the evaluation setting of existing works (Valko et al., 2013; Zhou
et al., 2020; Zhang et al., 2021), we transform this classification problem into bandit problem.
Consider an image x ∈ R[d], we aim to classify it from 10 classes. First, in each round, the image
**x is transformed into 10 arms and presented to the learner, represented by 10 vectors in sequence**
**x1 = (x, 0, . . ., 0), x2 = (0, x, . . ., 0), . . ., x10 = (0, 0, . . ., x) ∈** R[10][d]. The reward is defined as 1
if the index of selected arm matches the index of x’s ground-truth class; Otherwise, the reward is 0.

**Yelp[2]** **and Movielens (Harper and Konstan, 2015) datasets. Yelp is a dataset released in the Yelp**
dataset challenge, which consists of 4.7 million rating entries for 1.57 × 10[5] restaurants by 1.18
million users. MovieLens is a dataset consisting of 25 million ratings between 1.6 × 10[5] users
and 6 × 10[4] movies. We build the rating matrix by choosing the top 2000 users and top 10000
restaurants(movies) and use singular-value decomposition (SVD) to extract a 10-dimension feature
vector for each user and restaurant(movie). In these two datasets, the bandit algorithm is to choose the
restaurants(movies) with bad ratings. We generate the reward by using the restaurant(movie)’s gained
stars scored by the users. In each rating record, if the user scores a restaurant(movie) less than 2 stars
(5 stars totally), its reward is 1; Otherwise, its reward is 0. In each round, we set 10 arms as follows:
we randomly choose one with reward 1 and randomly pick the other 9 restaurants(movies) with 0
rewards; then, the representation of each arm is the concatenation of corresponding user feature
vector and restaurant(movie) feature vector.

**Disin (Ahmed et al., 2018) dataset. Disin is a fake news dataset on kaggle[3]** including 12600 fake
news articles and 12600 truthful news articles, where each article is represented by the text. To
transform the text into vectors, we use the approach (Fu and He, 2021) to represent each article by a
300-dimension vector. Similarly, we form a 10-arm pool in each round, where 9 real news and 1 fake
news are randomly selected. If the fake news is selected, the reward is 1; Otherwise, the reward is 0.

**Configurations. For LinUCB, following (Li et al., 2010), we do a grid search for the exploration**
constant α over (0.01, 0.1, 1) which is to tune the scale of UCB. For KernelUCB (Valko et al., 2013),
we use the radial basis function kernel and stop adding contexts after 1000 rounds, following (Valko
et al., 2013; Zhou et al., 2020). For the regularization parameter λ and exploration parameter ν in
KernelUCB, we do the grid search for λ over (0.1, 1, 10) and for ν over (0.01, 0.1, 1). For NeuralUCB
and NeuralTS, following setting of (Zhou et al., 2020; Zhang et al., 2021), we use the exploiation
network f1 and conduct the grid search for the exploration parameter ν over (0.001, 0.01, 0.1, 1) and
for the regularization parameter λ over (0.01, 0.1, 1). For NeuralEpsilon, we use the same neural
network f1 and do the grid search for the exploration probability ϵ over (0.01, 0.1, 0.2). For the
neural bandits NeuralUCB/TS, following their setting, as they have expensive computation cost to
store and compute the whole gradient matrix, we use a diagonal matrix to make approximation. For
all neural networks, we conduct the grid search for learning rate over (0.01, 0.001, 0.0005, 0.0001).
For all grid-searched parameters, we choose the best of them for the comparison and report the
averaged results of 10 runs for all methods.

**Create exploration samples for f2. When the selected arm is not optimal in a round, the optimal**
arm must exist among the remaining arms, and thus the exploration consideration should be added
to the remaining arms. Based on this fact, we create additional samples for the exploration network
_f2 in practice. For example, in setting of binary reward, e.g., 0 or 1 reward, if the received reward_
_rcrt = 0(0 while select, 1) usually is a small constant. This measure can further improve the performance of EE-Net xt, we add new train samples for f2, (xt,i, cr) for each i ∈_ [i] ∩ **xt,i ̸= xt, where**
in our experiments. ∈

B ABLATION STUDY

In this section, we conduct ablation study regarding the label function y for exploration network f2
and seting of decision maker f3 on two representative datasets Movielens and Mnist.

[2https://www.yelp.com/dataset](https://www.yelp.com/dataset)
3https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset


-----

Figure 4: Ablation study on label function y for f2. EE-Net denotes y1 = r − _f1, EE-Net-abs_
denotes y2 = |r − _f1|, and EE-Net-ReLU denotes y3 = ReLU(r −_ _f1). EE-Net shows the best_
performance on these two datasets.

Figure 5: Ablation study on decision maker f3. EE-Net-Lin denotes f3 = f1 + f2, EE-Net-NoLin
denote the nonlinear one where f3 is a neural network (2 layer, width 20), EE-Net denotes the hybrid
one where f3 = f1 + f2 if t ≤ 500 and f3 is the neural network if t > 500. EE-Net has the most
stable and best performance.


**Label function y. In this paper, we use y1 = r −** _f1 to measure the potential gain of an arm, as the_
label of f2. Moreover, we provide other two intuitive form y2 = |r − _f1| and y3 = ReLU_ (r − _f1)._
Figure 4 shows the regret with different y, where "EE-Net" denotes our method with default y1,
"EE-Net-abs" represents the one with y2 and "EE-Net-ReLU" is with y3. On Movielens and Mnist
datasets, EE-Net slightly outperforms EE-Net-abs and EE-Net-ReLU. In fact, y1 can effectively
represent the positive potential gain and negative potential gain, such that f2 intends to score the
arm with positive gain higher and score the arm with negative gain lower. However, y2 treats the
positive/negative potential gain evenly, weakening the discriminative ability. y3 can recognize the
positive gain while neglecting the difference of negative gain. Therefore, y1 usually is the most
effective one for empirical performance.

**Setting of f3. f3 can be set as an either linear function or non-linear function. In the experiment, we**
test the simple linear function f3 = f1 + f2, denoted by "EE-Net-Lin", and a non-linear function
represented by a 2-layer 20-width fully-connected neural network, denoted by "EE-Net-NoLin". For
the default hybrid setting, denoted by "EE-Net", when rounds t ≤ 500, f3 = f1 + f2; Otherwise, f3
is the neural network. Figure 5 reports the regret with these three different modes. EE-Net achieves
the best performance with small standard deviation. In contrast, EE-Net-NoLin obtains the worst
performance and largest standard deviation. However, notice that EE-Net-NoLin can achieve the
best performance in certain running (the green shallow) but it is erratic. Because in the begin phase,
without enough training samples, EE-Net-NoLin strongly relies on the quality of collected samples.
With appropriate training samples, gradient descent can lead f3 to global optimum. On the other hand,
with misleading training samples, gradient descent can deviate f3 from global optimum. Therefore,
EE-Net-NoLin shows very unstable performance. In contrast, EE-Net-Lin is inspired by the UCB
strategy, i.e., the exploitation plus the exploration, exhibiting stable performance. To combine their


-----

advantages together, we propose the hybrid approach, EE-Net, achieving the best performance with
strong stability.

C PROOF OF THEOREM 1

In this section, we provide the proof of Theorem 1 and related lemmas.

_Proof. For brevity, for the selected arm xt in round t, let h(xt) be its expected reward_
and x[∗]t = arg maxxt,i,i [n] h(xt,i) be the optimal arm in round t. Let f3(x; θt 1) =
_∈_ _−_

_f2_ _φ(▽θ1t_ 1 _[f][1][(][x][;][ θ]t[1]_ 1[));][ θ][2]t 1 + f1(x; θ[1]t 1[)][.]
_−_ _−_ _−_ _−_
 

Note that (xt,i, rt,i), for each i [n]. Then, the expected regret of round t is given by
_∼D_ _∈_
_Rt = Ext,i,i_ [n][h(x[∗]t [)][ −] _[h][(][x][t][)]]_
_∈_

= Ext,i,i [n][h(x[∗]t [)][ −] _[f][3][(][x][t][) +][ f][3][(][x][t][)][ −]_ _[h][(][x][t][)]]_
_∈_

Ext,i,i [n][h(x[∗]t [)][ −] _[f][3][(][x]t[∗][) +][ f][3][(][x][t][)][ −]_ _[f][3][(][x][t][)]_ +f3(xt) _h(xt)]_
_≤_ _∈_ _−_
_I1_

= Ext,i,i∈[n][|h(x[∗]t [)][ −] _[f][3][(][x]t[∗][) +]{z[ f][3][(][x][t][)][ −]_ _[h][(][x][t][)]]}_

= Ext,i,i [n][h(x[∗]t [)][ −] _[f][3][(][x]t[∗][;][ θ][t][−][1][) +][ f][3][(][x][t][;][ θ][t][−][1][)][ −]_ _[h][(][x][t][)]]_
_∈_

(a)
= Ext,i,i [n][h(x[∗]t [)][ −] _[f][3][(][x]t[∗][;][ θ]t[∗]_ 1[) +][ f][3][(][x]t[∗][;][ θ]t[∗] 1[)][ −] _[f][3][(][x]t[∗][;][ θ][t][−][1][) +][ f][3][(][x][t][;][ θ][t][−][1][)][ −]_ _[h][(][x][t][)]]_
_∈_ _−_ _−_

= Ext,i,i [n][h(x[∗]t [)][ −] _[f][3][(][x]t[∗][;][ θ]t[∗]_ 1[)][|][] +][ E]xt,i,i [n][[][f][3][(][x][∗]t [;][ θ]t[∗] 1[)][ −] _[f][3][(][x]t[∗][;][ θ][t][−][1][)]]_
_∈_ _−_ _∈_ _−_

+ Ext,i,i [n][f3(xt; θt 1) _h(xt)]_
_∈_ _−_ _−_

_≤_ (xt,i,rt,iE),i∈[n] _f2_ _φ(▽θ1t−,∗1_ _[f][1][(][x]t[∗][;][ θ]t[1]−[,][∗]1[));][ θ][2]t−[,][∗]1_ _−_ _rt[∗]_ _[−]_ _[f][1][(][x]t[∗][;][ θ]t[1]−[,][∗]1[)]_

h    i

_I2_

+| Ext,i,i∈[n] _f2_ _φ(▽θ1t−,∗1_ _[f][1][(][x]t[∗][;][ θ]t[1]−[,][∗]{z1[));][ θ][2]t−[,][∗]1_ _−_ _f2_ _φ(▽θ1t−1_ _[f][1][(][x]t[∗][;][ θ]t[1]−1}[));][ θ][2]t−1_
h    i

_I3_

+ E| **xt,i,i** [n] _f1(x[∗]t_ [;][ θ]t[1][,][∗]1[)][ −] _[f][1][(][x]t[∗][;][ θ]t[1]_ 1[)] {z }
_∈_ _−_ _−_
h i

_I4_

+ |(xt,i,rt,iE),i∈[n] _f2_ _φ{z(▽θ1t−1_ _[f][1][(][x][t][;][ θ]t[1]−1[));]}[ θ][2]t−1_ _−_ _rt −_ _f1(xt; θ[1]t−1[)]_

h      i

_I5_
(C.1)
| {z }
where I1 is because f3(xt) = maxi [n] f3(xt,i) and f3(xt) _f3(x[∗]t_ [)][ ≥] [0][ and][ (][a][)][ introduces the]
_∈_ _−_
additional parameters θ[∗]t 1 [= (][θ]t[1][,][∗]1[,][ θ]t[2][,][∗]1[)][ which will be suitably chosen.]
_−_ _−_ _−_

Because, for each i [n], (xt,i, rt,i), applying Lemma C.1 and Corollary C.1, with probability
_∈_ _∼D_
at least (1 _δ) over the randomness of initialization, for I2, I5, we have_
_−_

2ϵ 3L 2 log( (tn)/δ)

_I2, I5_ + (1 + 2ξ) _O_ _,_ (C.2)
_≤_ r _t_ [+][ O]  _√2t_  r _t_

where

_t3nL log m_ _t[4]nL[2]_ log[11][/][6] _m_
_ξ =_ (1) + + (C.3)
_O_ _O_  _ρ[√]m_  _O_ _ρ[4][/][3]m[1][/][6]_ !

and we apply the union bound over round τ, ∀τ ∈ [t] to make Lemma C.1 and Corollary C.1 hold for
each round τ, τ ∈ [t].

For I3, I4, based on Lemma C.2, with probability at least 1 _δ, we have_
_−_

_tL[3]_ log[5][/][6] _m_ _Lt3_ _t[4]L[2]_ log[11][/][6] _m_

_I3, I4_ 1 + + := ξ1. (C.4)
_≤_ _O_ _ρ[1][/][3]m[1][/][6]_ !! _O_  _ρ[√]m_ [log][ m] _O_ _ρ[4][/][3]m[1][/][6]_ !


-----

To sum up, with probability at least 1 − _δ, we have_

2ϵ 3L

_Rt_ 2 + (1 + 2ξ)
_≤_ r _t_ [+][ O]  _√2t_ 

Then expected regret of T rounds is computed by


2 log(O(tn)/δ)


(C.5)

+O(1)


+ ξ1


**RT =**


_Rt_
_t=1_

X


2ϵ 3L

_t_ [+][ O] _√2t_



2 log(O(tn)/δ)


_≤_ 2


+ (1 + 2ξ)


+ ξ1


_t=1_


_≤_ (2 _T −_ 1)2√2ϵ + (2 _T −_ 1)3√2O(L) + 2(1 + 2ξ)(2 _T −_ 1) 2 log(O(Tn)/δ) +O(1)

_I2_ p

= (2| _√T_ 1)(2√2ϵ + 3√2 (L)) + 2(1 + 2{zξ)(2√T 1) 2 log( (Tn)/δ) + (1)}

_−_ _O_ _−_ _O_ _O_

(C.6)
1 _T_ 1 p
where I2 is because _t=1_ _√t_ _[≤]_ 1 _√t_ _[dx][ + 1 = 2]√T −_ 1 (Chlebus, 2009) and the bound of ξ1 is

due to the choice of m, i.e., sinceR ξ1 = (1/m[1][/][6]) and m Ω(poly(T )), m can be chosen so that
_O_ _≥_ [e]
_Tξ1 =_ _O(T/m[1][/][6]) ≤O[P][T]_ (1).

[e]

Then, when[e] _ϵ ≤_ 1/T, we have

**RT** (1) + (2√T 1)3√2 (L) + 2(1 + 2ξ)(2√T 1) 2 log( (Tn)/δ). (C.7)
_≤O_ _−_ _O_ _−_ _O_

As the choice of m, we have ξ (1). Therefore, we have p
_≤O_


_≤_ (2


_T −_ 1)2


2ϵ + (2


_T −_ 1)3


2 log(O(Tn)/δ) _._ (C.8)



**RT** (1) + (2
_≤O_

The proof is completed.


_T −_ 1)3


2O(L) + O (2



_T −_ 1)


**Lemma C.1. [Lemma 5.1 restated] For any δ, ϵ** (0, 1), ρ (0, ( _L[1]_ [))][, suppose][ m, η][1][, η][2][, K][1][, K][2]
_∈_ _∈_ _O_

_satisfy the conditions in Eq. (5.2) and (xτ,i, rτ,i)_ _,_ _τ_ [t], i [n]. Let
_∼D_ _∀_ _∈_ _∈_


_f2_ _φ(▽θ1t_ 1 _[f][1][(][x][t,i][;][ θ]t[1]_ 1[));][ θ][2]t 1 + f1(xt,i; θ[1]t 1[)]
_−_ _−_ _−_ _−_
 


**xt = arg** max
**xt,i,i∈[n]**


_and rt is the corresponding reward, given (xt,i, rt,i), i ∈_ [n]. Then, with probability at least (1 − _δ)_
_over the random of the initialization, it holds that_


_f2_ _φ(▽θ1t_ 1 _[f][1][(][x][t][;][ θ]t[1]_ 1[));][ θ][2]t 1 _rt_ _f1(xt; θ[1]t_ 1[)] **xτ** _, rτ_ _τ_ =1
_−_ _−_ _−_ _−_ _−_ _−_ _| {_ _}[t][−][1]_
h     

2ϵ 3L 2 log( (tn/δ))

+ (1 + 2ξ) _O_ _,_

_≤_ r _t_ [+][ O]  _√2t_  r _t_


(xt,i,rt,i),i∈[n]


(C.9)


1 2
_where the expectation is also taken over (θ[1]t_ 1[,][ θ][2]t 1[)][ that are uniformly drawn from][ (]θ[b]τ _[,][ b]θτ_ [)][, τ][ ∈]
_−_ _−_

[t − 1].

_Proof. In this proof, we consider the collected data of up to round t −_ 1, {xτ _, rτ_ _}τ[t][−]=1[1]_ [, as the training]
dataset and then obtain a generalization bound for it, inspired by Cao and Gu (2019).

For convenience, we use x := xt,i, r := rt,i, noting that the same analysis holds for each i [n].
_∈_
Consider the exploration network f2, applying Lemma C.3. With probability at least 1 _δ, for any_
_−_
_τ ∈_ [t], we have

1 2
_f2_ _φ(▽θ1τ_ _[f][1][(][x][;][ b]θτ_ [));][ b]θτ _≤_ _ξ._ (C.10)
 
b

Similarly, applying Lemma C.3 again, with probability at least 1 − _δ, for any t ∈_ [T ], we have

1
_|f1(x;_ **_θ[b]τ_** [)][| ≤] _[ξ]_ (C.11)


-----

Because for any r _r,_ _r_ 1, with Eq. (C.10) and (C.11), applying union bound, with probability
_∼D_ _|_ _| ≤_
at least (1 − 2δ) over the random initialization, we have

1 2 1
_f2_ _φ(▽θ1τ_ _[f][1][(][x][;][ b]θτ_ [));][ b]θτ _−_ (r − _f1(x;_ **_θτ_** [))] _≤_ 1 + 2ξ. (C.12)
 
b

Noting that (C.12) is for x = xτ,i for a specific τ ∈ [t], i[b] ∈ [n]. By union bound,2 (C.12)

holds ∀τ ∈ [t], i ∈1 [n]2 with probability at least (1 − _ntδ). For brevity, let f2(x;_ **_θτ_** [)][ represent]

_f2_ _φ(▽θ1τ_ _[f][1][(][x][;][ b]θτ_ [));][ b]θτ .

[b]

  1 2
b

Recall that, for each τ ∈ [t − 1], **_θτ_** [and][ b]θτ [are the parameters training on][ {][x][τ][ ′] _[, r][τ][ ′]_ _[}]τ[τ]_ _[′]=1_ [according]
to Algorithm 1. In round τ [t], let xτ = arg maxxτ,i,i [n][f1(xτ,i; θ[1]τ 1[) +][ f][2][(][x][τ,i][;][ θ][2]τ 1[)]][,][ given]
_∈_ _∈_ _−_ _−_
(xτ,i, rτ,i) _, i_ [n]. Let rτ be the corresponding reward. Let[b] (x[′]τ,i[, r]τ,i[′] [)][ ∼D][, i][ ∈] [[][n][]][ be shadow]
_∼D_ _∈_
samples from the same distribution and let x[′]τ [= arg max]x[′]τ,i[,i][∈][[][n][]][[][f][1][(][x]τ,i[′] [;][ θ]τ[1] 1[) +][ f][2][(][x][′]τ,i[;][ θ]τ[2] 1[)]][,]
_−_ _−_
with rτ[′] [being the corresponding reward. Then, we define]

2 1

_Vτ :=_ E _f2(x[′]τ_ [;][ b]θτ 1[)][ −] _rτ[′]_ _τ_ [;][ b]θτ 1[)]
(x[′]τ,i[,r]τ,i[′] [)][,i][∈][[][n][]] _−_ _[−]_ _[f][1][(][x][′]_ _−_

h 2  1 i (C.13)

_f2(xτ_ ; **_θτ_** 1[)][ −] _rτ_ _f1(xτ_ ; **_θτ_** 1[)] _._
_−_ _−_ _−_ _−_
 

[b] [b]

Then, as (xτ,i, rτ,i) _, i_ [n], based on the definition of (xτ _, rτ_ ), we have
_∼D_ _∈_


2 1

E[Vτ _|Fτ_ _−1] =_ (x[′]τ,i[,r]τ,i[′]E [)][,i][∈][[][n][]] _f2_ **x[′]τ** [;][ b]θτ _−1_ _−_ _rτ[′]_ _[−]_ _[f][1][(][x]τ[′]_ [;][ b]θτ _−1[)]_ _| Fτ_ _−1_

h  2   1  i

E _f2_ **xτ** ; **_θτ_** 1 _rτ_ _f1(xτ_ ; **_θτ_** 1[)] **Fτ** 1
_−_ (xτ,i,rτ,i),i∈[n] _−_ _−_ _−_ _−_ _|_ _−_

h    

= 0,

[b] [b]

where Fτ _−1 denotes the σ-algebra generated by the history Hτ_ _−1 = {xτ ′_ _, rτ ′_ _}τ[τ]_ _[′][−]=1[1]_ [.]

Moreover, we have


(C.14)


_t_ _t_

2 1

_Vτ = [1]_ E _f2(x[′]τ_ [;][ b]θτ 1[)][ −] _rτ[′]_ _τ_ [;][ b]θτ 1[)]
_τ_ =1 _t_ _τ_ =1 (x[′]τ,i[,r]τ,i[′] [)][,i][∈][[][n][]] _−_ _[−]_ _[f][1][(][x][′]_ _−_

X X h 

_t_

2 1
_f2_ **xτ** ; **_θτ_** 1 _rτ_ _f1(xτ_ ; **_θτ_** 1[)]

_−_ [1]t _τ_ =1 _−_ _−_ _−_ _−_

X    

[b] [b]


Since {Vτ _}τ[t]_ =1 [is a martingale difference sequence, inspired by Lemma 1 in (Cesa-Bianchi et al.,]
2001), applying the Hoeffding-Azuma inequality, with probability at least 1 − 3δ, we have


_t_

_Vτ_
_τ_ =1 _−_ [1]t

X


2 log(1/δ)

_t_

2 log(1/δ)


E[Vτ _|Fτ_ ] _> (1 + 2ξ)_
_τ_ =1

X _I2_

_I1_

| {z }

_t_

{z1 }

P _Vτ > (1 + 2ξ)_

_t_

" _τ_ =1

X


_≤_ _δ_

_≤_ _δ,_


(C.15)


where I1 = 0 according to (C.14) and I2 is because of (C.12).


-----

1 2
According to Algorithm 1, (θ[1]t 1[,][ θ][2]t 1[)][ is uniformly drawn from][ {][(]θ[b]τ _[,][ b]θτ_ [)][}][t]τ[−]=0[1] [. Thus, with proba-]
_−_ _−_
bility 1 − 3δ, we have

E E _f2_ **x[′]t[;][ θ][2]t** 1 _rt[′]_ _t[;][ θ][1]t_ 1[)]
(x[′]t,i[,r]t,i[′] [)][,i][∈][[][n][]] (θ[1]t−1[,][θ][2]t−1[)] _−_ _−_ _[−]_ _[f][1][(][x][′]_ _−_

_t_       

= [1]t _τX=1_ E(x[′]τ,i[,r]τ,i[′] [)][,i][∈][[][n][]] hf2 x[′]τ [;][ b]θ2τ _−1_ _−_ rτ[′] _[−]_ _[f][1][(][x]τ[′]_ [;][ b]θ1τ _−1[][)]i_ (C.16)

_t_

2 1 2 log(1/δ)

_≤_ [1]t _τ_ =1 _f2_ **xτ** ; **_θτ_** _−1_ _−_ _rτ −_ _f1(xτ_ ; **_θτ_** _−1[)]_ +(1 + 2ξ)r _t_ _._

X    

[b] _I3_ [b]

| {z }

2 2 2 _t[3]_
For I3, according to Lemma C.6, for any **_θ_** satisfying ∥θ _−θ0[∥][2]_ _[≤O][(]_ _ρ[√]m_ [log][ m][)][, with probability]

1 − _δ, we have_

_t_ [e] e

1 2 1

_f2(xτ_ ; **_θτ_** 1[)][ −] _rτ_ _f1(xτ_ ; **_θτ_** 1[)]

_t_ _τ_ =1 _−_ _−_ _−_

X  

_t_ [b] 2 [b] 1 3L (C.17)

_f2(xτ_ ; **_θ_** ) _rτ_ _f1(xτ_ ; **_θτ_** 1[)] + _._

_≤_ [1]t _τX=1_ _−_  _−_ _−_  _O_  _√2t_ 

[e] _I4_ [b]

| {z 2 } 2 2 _t[3]_

For I4, according to Lemma C.4 (1), these exists **_θ_** satisfying ∥θ _−_ **_θ0[∥][2]_** _[≤O][(]_ _ρ[√]m_ [log][ m][)][, with]

probability 1 − _δ, such that_

_t_ [e] e

1 2 1

_f2(xτ_ ; **_θ_** ) _rτ_ _f1(xτ_ ; **_θτ_** 1[)]

_t_ _τ_ =1 _−_ _−_ _−_

X  

_t_ [e] 2 [b] 1 2

_√t_ _f2(xτ_ ; **_θ_** ) _rτ_ _f1(xτ_ ; **_θτ_** 1[)]

_≤_ [1]t vuτ =1 _−_ _−_ _−_ (C.18)

uX   
u
u [e] _I5_ [b]
u
t

| {z }

_≤_ _√[1]t_ 2ϵ _,_

_I5_
r
|{z} 2

where21 _tτ_ =1 I5 follows by a direct application of Lemma C.4 (1) by defining the lossf2(xτ ; **_θ2) −_** _rτ −_ _f1(xτ_ ; **_θ1τ_** _−1[)]_ 2 _≤_ _ϵ._ _L(θ[e]_ ) =

  

Combining Eq.(C.16), Eq.(C.17) and Eq.(C.18), with probabilityP (1 5δ) we have

[e] [b] _−_


E _f2_ **xt; θ[2]t** 1 _rt_ _f1(xt; θ[1]t_ 1[)] **xτ** _, rτ_ _τ_ =1
(xt,i,rt,i),i∈[n] _−_ _−_ _−_ _−_ _|{_ _}[t][−][1]_

      

2ϵ 3L 2 log(1/δ)

_≤_ r _t_ [+][ O]  _√2t_  + (1 + 2ξ)r _t_ _._


(C.19)


1 2
where the expectation over (θ[1]t 1[,][ θ][2]t 1[)][ that is uniformly drawn from][ {][(]θ[b]τ _[,][ b]θτ_ [)][}][t]τ[−]=0[1] [.]
_−_ _−_

Then, applying union bound to t, n and rescaling the δ complete the proof.

**Corollary C.1. For any δ, ϵ ∈** (0, 1), ρ ∈ (0, O( _L[1]_ [))][, suppose][ m, η][1][, η][2][, K][1][, K][2][ satisfy the condi-]

_tions in Eq. (5.2) and (xτ,i, rτ,i)_ _,_ _τ_ [t], i [n]. For any τ [t], let
_∼D_ _∀_ _∈_ _∈_ _∈_

**x[∗]τ** [= arg] max
**xτ,i,i** [n] [[][h][(][x][τ,i][)]][,]
_∈_


-----

_and rτ[∗]_ _[is the corresponding reward, given][ (][x][τ,i][, r][τ,i][)][, i][ ∈]_ [[][n][]][. Then, with probability at least][ (1][ −] _[δ][)]_
_t[3]_
_over the random of the initialization, there existt[3]_ **_θt[1]−[,][∗]1[,][ θ][2]t−[,][∗]1[,][ s.t.,][ ∥][θ]t[1]−[,][∗]1_** _[−]_ **_[θ]0[1][∥][2]_** _[≤O][(]_ _ρ[√]m_ [log][ m][)]

_and ∥θ[2]t−[,][∗]1_ _[−]_ **_[θ]0[2][∥][2]_** _[≤O][(]_ _ρ[√]m_ [log][ m][)][, such that]

(xt,i,rt,iE),i∈[n] _f2_ _φ(▽θ1t−,∗1_ _[f][1][(][x]t[∗][;][ θ]t[1]−[,][∗]1[));][ θ]t[2]−[,][∗]1_ _−_ _rt[∗]_ _[−]_ _[f][1][(][x]t[∗][;][ θ]t[1]−[,][∗]1[)]_ _| {x[∗]τ_ _[, r]τ[∗][}][t]τ[−]=1[1]_

h     i

2ϵ 3L 2 log( (tn/δ))

+ (1 + 2ξ) _O_ _,_

_≤_ r _t_ [+][ O]  _√2t_  r _t_

(C.20)
1,∗ 2,∗
_where the expectation is also taken over (θt[1][,][∗]1[,][ θ]t[2][,][∗]1[)][ that are uniformly drawn from][ (]θ[b]τ_ _[,][ b]θτ_ [)][, τ][ ∈]
_−_ _−_

[t − 1].

_Proof.brevity, let This a direct corollary of Lemma C.1, given the optimal historical pairs f2(x;_ **_θτ2,∗[)][ represent][ f][2]_** _φ(▽θ1τ,∗_ _[f][1][(][x][;][ b]θ1τ,∗[));][ b]θ2τ,∗_ . _{x[∗]τ_ _[, r]τ[∗][}][t]τ[−]=1[1]_ [. For]

 1, 2, 

_∗b_ _∗_
Suppose that, for each[b] _τ ∈_ [t − 1], **_θτ_** and **_θτ_** are the parameters training on {x[∗]τ _[′]_ _[, r]τ[∗][′]_ _[}]τ[τ]_ _[′]=1_
according to Algorithm 1. Note that these pairs1,∗ 2,∗ _{x[∗]τ_ _[′]_ _[, r]τ[∗][′]_ _[}]τ[τ]_ _[′]=1_ [are unknown to the algorithm we run,]

and the parameters (θ[b]τ _[,][ b]θτ_ [)][ are not estimated. However, for the analysis, it is sufficient to show][b] [b]
that there exist such parameters so that the conditional expectation of the error can be bounded.

In round τ [t], let x[∗]τ [= arg max]xτ,ii [n][[][h][(][x][τ,i][)]][,][ given][ (][x][τ,i][, r][τ,i][)][ ∼D][, i][ ∈] [[][n][]][. Let][ r]τ[∗] [be the]
_∈_ _∈_
corresponding reward. Let (x[′]τ,i[, r]τ,i[′] [)][ ∼D][, i][ ∈] [[][n][]][ be shadow samples from the same distribution]
and let x[′∗]τ [= arg max]x[′]τ,i[,i][∈][[][n][]][ h][(][x]τ,i[′] [)][, with][ r]τ[′∗] [being the corresponding reward. Then, we define]

2,∗ 1,∗

_Vτ :=_ E _f2(x[′∗]τ_ [;][ b]θτ 1[)][ −] _rτ[′∗]_ _τ_ [;][ b]θτ 1[)]
(x[′]t,i[,r]t,i[′] [)][,i][∈][[][n][]] _−_ _[−]_ _[f][1][(][x][′∗]_ _−_

h 2,∗  1,∗ i (C.21)

_f2(x[∗]τ_ [;][ b]θτ 1[)][ −] _rτ[∗]_ _τ_ [;][ b]θτ 1[)] _._
_−_ _−_ _[−]_ _[f][1][(][x][∗]_ _−_
 

Then, as (xτ,i, rτ,i) _, i_ [n], we have
_∼D_ _∈_


2,∗ 1,∗

E[Vτ **Fτ** 1] = E _f2_ **x[′∗]τ** [;][ b]θτ 1 _rτ[′∗]_ _τ_ [;][ b]θτ 1[)] **Fτ** 1
_|_ _−_ (x[′]τ,i[,r]τ,i[′] [)][,i][∈][[][n][]] _−_ _−_ _[−]_ _[f][1][(][x][′∗]_ _−_ _|_ _−_

h  2,∗  1,∗ 

E _f2_ **x[∗]τ** [;][ b]θτ 1 _rτ[∗]_ _τ_ [;][ b]θτ 1[)] **Fτ** 1
_−_ (xτ,i,rτ,i),i∈[n] _−_ _−_ _[−]_ _[f][1][(][x][∗]_ _−_ _|_ _−_

h    

= 0,


(C.22)


where Fτ _−1 denotes the σ-algebra generated by the history {x[∗]τ_ _[′]_ _[, r]τ[∗][′]_ _[}]τ[τ]_ _[−][′]=1[1]_ [.]

Therefore, {Vτ _}τ[t]_ =1 [is a martingale difference sequence. Similarly, applying the Hoeffding-Azuma]
inequality to Vτ, with probability 1 3δ, we have
_−_


E E _f2_ **x[′∗]t** [;][ θ]t[2][,][∗]1 _rt[′∗]_ _t_ [;][ θ]t[1][,][∗]1[)]
(x[′]t,i[,r]t,i[′] [)][,i][∈][[][n][]] (θ[1]t _[,][∗]1[,][θ][2]t_ _[,][∗]1[)]_ _−_ _−_ _[−]_ _[f][1][(][x][′∗]_ _−_
_−_ _−_

h    i

_t_

2,∗ 1,∗

= [1]t _τ_ =1 E(x[′]τ,i[,r]τ,i[′] [)][,i][∈][[][n][]] _f2_ **x[′∗]τ** [;][ b]θτ _−1_ _−_ _rτ[′∗]_ _[−]_ _[f][1][(][x]τ[′∗][;][ b]θτ_ _−1[)]_

X h    i

_t_

2,∗ 1,∗ 2 log(1/δ)
_f2_ **x[∗]τ** [;][ b]θτ 1 _rτ[∗]_ _τ_ [;][ b]θτ 1[)] +(1 + 2ξ)

_≤_ [1]t _τ_ =1 _−_ _−_ _[−]_ _[f][1][(][x][∗]_ _−_ r _t_

X    

_I3_
| {z }


(C.23)


-----

2,∗ 2,∗ 2 _t[3]_
For I3, according to Lemma C.6, for any **_θ_** satisfying ∥θ _−_ **_θ0[∥][2]_** _[≤O][(]_ _ρ[√]m_ [log][ m][)][, with]

probability 1 − _δ, we have_

_t_ [e] e

1 2,∗ 1,∗

_f2(x[∗]τ_ [;][ b]θτ 1[)][ −] _rτ[∗]_ _τ_ [;][ b]θτ 1[)]

_t_ _τ_ =1 _−_ _[−]_ _[f][1][(][x][∗]_ _−_

X  

_t_ 2,∗ 1,∗ 3L (C.24)

_f2(x[∗]τ_ [;][ e]θ ) _rτ[∗]_ _τ_ [;][ b]θτ 1[)] + _._

_≤_ [1]t _τX=1_ _−_  _[−]_ _[f][1][(][x][∗]_ _−_  _O_  _√2t_ 

_I4_
| {z 2,∗ } 2,∗ 2 _t[3]_

For I4, according to Lemma C.4 (1), these exists **_θ_** satisfying ∥θ _−_ **_θ0[∥][2]_** _[≤O][(]_ _ρ[√]m_ [log][ m][)][,]

with probability 1 − _δ, such that_

_t_ [e] e

1 2,∗ 1,∗

_f2(x[∗]τ_ [;][ e]θ ) _rτ[∗]_ _τ_ [;][ b]θτ 1[)]

_t_ _τ_ =1 _−_ _[−]_ _[f][1][(][x][∗]_ _−_

X  

_t_

_√t_ _f2(x[∗]τ_ [;][ e]θ2,∗) _rτ[∗]_ _τ_ [;][ b]θ1τ,∗1[)] 2

_≤_ [1]t vuτ =1 _−_ _[−]_ _[f][1][(][x][∗]_ _−_ (C.25)

uX   
u
u _I5_
u
t

| {z }

_≤_ _√[1]t_ 2ϵ _,_

_I5_
r
|{z} 2,∗

where1 _t_ _I5 follows by a direct application of Lemma C.4 (1) by defining the loss2,∗_ 1,∗ 2 _L(θ[e]_ ) =

2 _τ_ =1 _f2(x[∗]τ_ [;][ e]θ ) − _rτ[∗]_ _[−]_ _[f][1][(][x]τ[∗][;][ b]θτ_ _−1[)]_ _≤_ _ϵ. Combining above inequalities, with proba-_

bility (1  5δ) we have  

P _−_

E _f2_ **x[∗]t** [;][ θ]t[2][,][∗]1 _rt[∗]_ _t_ [;][ θ]t[1][,][∗]1[)] **x[∗]τ** _[, r]τ[∗][}][t]τ[−]=1[1]_
(xt,i,rt,i),i∈[n] _−_ _−_ _[−]_ _[f][1][(][x][∗]_ _−_ _|{_

2ϵ h  3L   2 log(1/δ)  i (C.26)

_≤_ r _t_ [+][ O]  _√2t_  + (1 + 2ξ)r _t_ _._

Then, applying union bound to t, n and rescaling the δ complete the proof.


**Lemma C.2. Given δ, ϵ ∈** (0, 1), ρ ∈ (0, O( _L[1]_ [))][, suppose][ m, η][1][, η][2][, K][1][, K][2][ satisfy the conditions]

_in Eq. (5.2). Then, with probability at least 1 −_ _δ, in each round t ∈_ [T ], for any ∥x∥2 = 1, we have


(1) _f1(x; θt[1][,][∗]1[)][ −]_ _[f][1][(][x][;][ θ]t[1]_ 1[)][|]
_|_ _−_ _−_

_tL[3]_ log[5][/][6] _m_

1 +

_≤_ _O_ _ρ[1][/][3]m[1][/][6]_ !!


3
_Lt_

+
_ρ[√]m_ [log][ m] _O_

 


_t[4]L[2]_ log[11][/][6] _m_

_ρ[4][/][3]m[1][/][6]_


(C.27)

(C.28)


(2) _f2_ _φ(▽θ1t−,∗1_ _[f][1][(][x][;][ θ]t[1]−[,][∗]1[));][ θ][2]t−[,][∗]1_ _−_ _f2_ _φ(▽θ1t−1_ _[f][1][(][x][;][ θ]t[1]−1[));][ θ][2]t−1_
  3  

_tL[3]_ log[5][/][6] _m_ _Lt_ _t[4]L[2]_ log[11][/][6] _m_

1 + +

_≤_ _O_ _ρ[1][/][3]m[1][/][6]_ !! _O_  _ρ[√]m_ [log][ m] _O_ _ρ[4][/][3]m[1][/][6]_


(3) ▽θ1t 1 _[f][1][(][x][;][ θ]t[1]_ 1[)][∥][2][,][ ∥][▽]θ[2]t 1 _[f][2]_ _φ(▽θ1t_ 1 _[f][1][(][x][;][ θ]t[1]_ 1[));][ θ][2]t 1 2
_∥_ _−_ _−_ _−_ _−_ _−_ _−_ _∥_
  (C.29)

_tL[3]_ log[5][/][6] _m_

1 + (L) .

_≤_ _O_ _ρ[1][/][3]m[1][/][6]_ !! _O_


-----

1
_t[3]_
_Proof. According to Lemma C.4 (2),t[3]_ _∥θ[b]τ_ _−1_ _[−]_ **_[θ]0[1][∥][2]_** _[≤O][(]_ _ρ[√]m_ [log][ m][)][,][ ∀][τ][ ∈] [[][t][]][. Thus, we have]

_∥θ[1]t−1_ _[−]_ **_[θ]0[1][∥][2]_** _[≤O][(]_ _ρ[√]m_ [log][ m][)][.]

First, based on Triangle inequality, for any ∥x∥2 = 1, we have

▽θ1t 1 _[f][1][(][x][;][ θ]t[1]_ 1[)][∥][2] **_θ[1]0_** _[f][1][(][x][;][ θ]0[1][)][∥][2]_ [+][ ∥][▽]θ[1]t 1 _[f][1][(][x][;][ θ]t[1]_ 1[)][ −] [▽]θ[1]0 _[f][1][(][x][i][;][ θ]0[1][)][∥][2]_
_∥_ _−_ _−_ _[≤∥][▽]_ _−_ _−_

_tL[3]_ log[5][/][6] _m_ (C.30)

1 + (L)

_≤_ _O_ _ρ[1][/][3]m[1][/][6]_ !! _O_

where the last inequality is because of Lemma C.4 (3) and Lemma C.7.


_t[3]_
Applying Lemma C.5 (1), for any x ∼D, ∥x∥2 = 1 and ∥θ[1]t−[,][∗]1 _[−]_ **_[θ]t[1]−1[∥≤O][(]_** _ρ[√]m_ [log][ m][) =][ w][, we]

have


_f1(x; θt[1][,][∗]1[)][ −]_ _[f][1][(][x][;][ θ]t[1]_ 1[)][|]
_|_ _−_ _−_

▽θ1t 1 _[f][1][(][x][i][;][ θ]t[1]_ 1[)][,][ θ]t[1][,][∗]1 _t_ 1[⟩|][ +][ O][(][L][2][p]m log(m)) **_θ[1]t_** _[,][∗]1_ _t_ 1[∥][2][w][1][/][3]
_≤|⟨_ _−_ _−_ _−_ _[−]_ **_[θ][1]−_** _∥_ _−_ _[−]_ **_[θ][1]−_**

▽θ1t 1 _[f][1][(][x][i][;][ θ]t[1]_ 1[)][∥][2][∥][θ][1]t _[,][∗]1_ _t_ 1[∥][2] [+][ O][(][L][2][p]m log(m)) **_θ[1]t_** _[,][∗]1_ _t_ 1[∥][2][w][1][/][3]
_≤∥_ _−_ _−_ _−_ _[−]_ **_[θ][1]−_** _∥_ _−_ _[−]_ **_[θ][1]−_**

_tL[3]_ log[5][/][6] _m_ _t[4]L[2]_ log[11][/][6] _m_

1 + ( _[Lt][3]_

_≤_ _O_ _ρ[1][/][3]m[1][/][6]_ !! _O_ _ρ[√]m_ [log][ m][) +][ O] _ρ[4][/][3]m[1][/][6]_ !

Similarly, we can use the same way to prove the lemmas for f2.


(C.31)


**Lemma C.3. Let f** (·; **_θt) follow the stochastic gradient descent of f1 or f2 in Algorithm 1. Suppose_**
_m, η1, η2 satisfy the conditions in Eq. (5.2). With probability at least 1 −_ _δ, for any x with ∥x∥2 = 1_
_and t_ [T ], it holds that
_∈_ [b]

_t3nL log m_ _t[4]nL[2]_ log[11][/][6] _m_
_f_ (x; **_θt)_** (1) + + _._
_|_ _| ≤O_ _O_  _ρ[√]m_  _O_ _ρ[4][/][3]m[1][/][6]_ !

[b]

_Proof. Considering an inequality |a −_ _b| ≤_ _c, we have |a| ≤|b| + c. Let θ0 be randomly initialized._
Then applying Lemma C.5 (1), for any x ∼D, ∥x∥2 = 1 and ∥θ[b]t − **_θ0∥≤_** _w, we have_

_|f_ (x; **_θt)| ≤|f_** (x; θ0)| + |⟨▽θ0 _f_ (xi; θ0), **_θt −_** **_θ0⟩| + O(L[2][p]m log(m))∥θ[b]t −_** **_θ0∥2w[1][/][3]_**

[b] _≤O(1)_ + ∥▽θ0 _f_ (xi; θ0)∥2∥θ[b]t −[b] **_θ0∥2_** +O(L[2][p]m log(m))∥θ[b]t − **_θ0∥2w[1][/][3]_**

_I0_ _I1_

4/3

| {z } | {zt[3] } _t[3]_

(1) + (L) + _L[2][p]m log(m)_
_≤O_ _O_ _· O_ _ρ[√]m_ [log][ m] _O_ _· O_ _ρ[√]m_ [log][ m]
     

_I2_ _I3_
| _t3L log{z m_ _t}[4]L[2]|log[11][/][6]_ _m_ {z }

= (1) + +
_O_ _O_  _ρ[√]m_  _O_ _ρ[4][/][3]m[1][/][6]_ !

(C.32)
where: I0 is based on the Lemma C.4 (3); I1 is an application of Cauchy–Schwarz inequality; I2 is
according to Lemma C.4 (2) and (3) in which **_θt can be considered as one step gradient descent; I3 is_**
due to Lemma C.4 (2).

Then, the proof is completed. [b]

**Lemma C.4. Given a constant 0 < ϵ < 1, suppose m satisfies the conditions in Eq. (5.2), the**
_learning rate η = Ω(_ _poly(t,n,Lρ_ )m [)][, the number of iterations][ K][ = Ω(][ poly][(]ρ[t,n,L][2] [)] log ϵ[−][1]). Then, with

_·_
_probability at least 1_ _δ, starting from random initialization θ0,_
_−_

_(1) (Theorem 1 in (Allen-Zhu et al., 2019)) In round{xτ_ _, rτ_ _}i[t]=τ_ _[, the loss function is defined as:][ L][(][θ][) =] t ∈21_ [Ttτ]=1, given the collected data[(][f] [(][x][τ] [;][ θ][)][ −] _[r][τ]_ [)][2][. Then,]
P


-----

_t[3]_
_there exists_ **_θ satisfying ∥θ[e] −_** **_θ0∥2_** _≤O_ _ρ[√]m_ [log][ m] _, such that L(θ[e]) ≤_ _ϵ in_

_K = Ω(_ _[poly][(]ρ[t,n,L][2]_ [)] log ϵ[−][1]) iterations;  

[e] _·_

_(2) (Theorem 1 in (Allen-Zhu et al., 2019)) For any k_ [K], it holds uniformly that **_θ[(]t[k][)]_**
_∈_ _∥_ _−_

_t[3]_
**_θ0∥2 ≤O_** _ρ[√]m_ [log][ m] _;_
 


_(3) Following the initialization, given ∥x∥2 = 1, it holds that_

_∥▽θ0_ _f_ (x; θ0)∥2 ≤O(L), _|f_ (x; θ0)| ≤O(1)

_where θ[(]t[k][)]_ _represents the parameters of f after k_ [K] iterations of gradient descent in round t.
_∈_


_Proof. Note that the output dimension d in (Allen-Zhu et al., 2019) is removed because the output_
of network function in this paper always is a scalar. For (1) and (2), the only different setting from
(Allen-Zhu et al., 2019) is that the initialization of last layer WL ∼N (0, _m[2]_ [)][ in this paper while]

**W(Allen-Zhu et al., 2019) still holds forL ∼N** (0, _d[1]_ [)][ in (Allen-Zhu et al., 2019). Because] WL: with probability at least[ d][ = 1][ and][ m > d] 1 exp ([ here, the upper bound in]Ω(m/L)), **WL** _F_

_−_ _−_ _∥_ _∥_ _≤_

_m/d. Therefore, (1) and (2) still hold for the initialization of this paper._

For (3), based on Lemma 7.1 in Allen-Zhu et al. (2019), we havep _f_ (x; θ0) (1). Denote by D
_|_ _| ≤O_
the ReLU function. For any l ∈ [L],

▽Wl _f_ (x; θ0) _F_ **WLDWL** 1 _DWl+1_ _F_ _DWl+1_ **x** _F_ (√L)
_∥_ _∥_ _≤∥_ _−_ _· · ·_ _∥_ _· ∥_ _· · ·_ _∥_ _≤O_

where the inequality is according to Lemma 7.2 in Allen-Zhu et al. (2019). Therefore, we have
_∥▽θ0_ _f_ (x; θ0)∥2 ≤O(L).

**Lemma C.5 (Lemma 4.1, (Cao and Gu, 2019)). For any δ ∈** (0, 1), if w satisfies

_O(m[−][3][/][2]L[−][3][/][2][log(tnL[2]/δ)][3][/][2]) ≤_ _w ≤O(L[−][6][log m][−][3][/][2]),_

_then, with probability at least 1 −_ _δ over randomness of θ0, for any t ∈_ [T ], ∥x∥2 = 1, and θ, θ[′]

_satisfying_ **_θ_** **_θ0_** 2 _w and_ **_θ[′]_** **_θ0_** 2 _w, it holds uniformly that_
_∥_ _−_ _∥_ _≤_ _∥_ _−_ _∥_ _≤_

_|f_ (xi; θ) − _f_ (xi; θ[′]) −⟨▽θ[′] _f_ (xi; θ[′]), θ − **_θ[′]⟩| ≤O(w[1][/][3]L[2][p]m log(m))∥θ −_** **_θ[′]∥2._** (C.33)

**Lemma C.6. For any δ > 0, suppose**

_√log 1/δ_
_m >_ _O[˜]_ _poly(T, n, ρ[−][1], L, log(1/δ) · e_ ) _._
  2

_t[5]_
_Then, with probability at least 1 −_ _δ, setting η2 = Θ(_ _δ[2][√]2m_ [)][ for algorithm 1, for any][ e]θ _satisfying_

2 2 _t[3]_
_∥θ[e]_ _−_ **_θ0[∥][2]_** _[≤O][(]_ _ρ[√]m_ [log][ m][)][, it holds that]

_t_

1 2 1

_τ_ =1 _f2_ _φ(▽θ1τ_ _−1_ _[f][1][(][x][τ]_ [;][ b]θτ _−1[));][ b]θτ_ _−1_ _−_ _rτ −_ _f1(xτ_ ; **_θτ_** _−1[)]_

_t_ X  b   

1 2[] 1 [b]3L√t

_≤_ _τX=1_ _f2_ φ(▽θb1τ _−1_ _[f][1][(][x][τ]_ [;][ b]θτ _−1[));][ e]θ_ _−_ rτ − _f1(xτ_ ; **_θτ_** _−1[)] + O_  _√2_ 

[b]

_Proof. This is a direct application of Lemma 4.3 in (Cao and Gu, 2019) by setting R =_ _[t]ρ[3]_ [log][ m, ϵ][ =]

2

_√LR2νt_ [, and][ ν][ =][ ν][′][R][2][, where][ ν][′][ is some small enough absolute constant. We set][ L][τ] [(]θ[b]τ _−1[) =]_

2 1
_f2(▽θ1τ_ _−1_ _[f][1][;][ b]θτ_ _−1[)][ −]_ _rτ −_ _f1(xτ_ ; **_θτ_** _−1[)]_ . Based on Lemma C.4 (2), for any τ ∈ [t], we have
 
b 2 2 2 2 _t[3]_

[b]

_∥θ[b]τ_ _[−]_ **_θ[b]τ_** _−1[∥][2]_ _[≤∥]θ[b]τ_ _[−]_ **_[θ][0][∥][2]_** [+][ ∥][θ][0] _[−]_ **_θ[b]τ_** _−1[∥][2]_ _[≤O][(]_ _ρ[√]m_ [log][ m][)][.]


-----

Table 2: Selection Criterion Comparison (xt: selected arm in round t).

|Methods|Selection Criterion|
|---|---|


|Neural Epsilon-greedy|With probability 1 ϵ, x = arg max f 1(x t,i; θ1); − t xt,i,i∈[n] Otherwise, select x randomly. t|
|---|---|



EE-Net (Our approach) _∀i ∈_ [n], compute f1(x t,i; θ[1]), f2 ▽θ1 _f1(xt,i; θ[1]); θ[2][]_ (Ex
|NeuralTS (Zhang et al., 2021)|For x t,i, ∀i ∈[n], draw rˆ from N(f 1(x t,i; θ1), σ t,i2). Then, t,i select x t,ˆi, ˆi = arg max rˆ t,i. i∈[n]|
|---|---|


|NeuralUCB (Zhou et al., 2020)|x = arg max xt,i,i∈[n] f 1(x t,i; θ1) + UCB t,i. t|
|---|---|


|EE-Net (Our approach)|∀i ∈ [n], compute f 1(x t,i; θ1), f 2  ▽ θ1f 1(x t,i; θ1); θ2(Ex- ploration Net). Then x = arg max f 3(f 1, f 2; θ3). t xt,ii∈[n]|
|---|---|



Then, according to Lemma 4.3 in (Cao and Gu, 2019), then, for any **_θ2 satisfying_** **_θ2_** **_θ20[∥][2]_**
_t[3]_ _∥_ _−_ _[≤]_
(
_O_ _ρ[√]m_ [log][ m][)][, there exist a small enough absolute constant][ ν][′][, such that]

[e] e

_t_ _t_


2
_Lτ_ (θ[e] ) + 3tϵ. (C.34)
_τ_ =1

X


2
_Lτ_ (θ[b]τ 1[)][ ≤]
_−_
_τ_ =1

X

Then, replacing ϵ completes the proof.


**Lemma C.7 (Theorem 5, Allen-Zhu et al. (2019)). For any δ ∈** (0, 1), if w satisfies that

_O(m[−][3][/][2]L[−][3][/][2]_ max{log[−][3][/][2] _m, log[3][/][2](Tn/δ)}) ≤_ _w ≤O(L[−][9][/][2]_ log[−][3] _m),_ (C.35)

_then, with probability at least 1_ _δ, for all_ **_θ_** **_θ0_** 2 _w, we have_
_−_ _∥_ _−_ _∥_ _≤_

_∥▽θf_ (x; θ) − ▽θ0 _f_ (x; θ0)∥2 ≤O( log mw[1][/][3]L[3])∥▽θ0 _f_ (x; θ0)∥2. (C.36)
p

D MOTIVATION OF EXPLORATION NETWORK

In this section, we list one gradient-based UCB from existing works (Ban et al., 2021; Zhou et al.,
2020), which motivates our design of exploration network f2. Let g(xt; θt) = ▽θt _f_ (xt; θt).

**Lemma D.1. (Lemma 5.2 in (Ban et al., 2021)). Given a set of context vectors {xt}t[T]=1** _[and the]_
_corresponding rewards_ _rt_ _t=1_ _[,][ E][(][r][t][) =][ h][(][x][t][)][ for any][ x][t]_ _t=1[. Let][ f]_ [(][x][t][;][ θ][)][ be the][ L][-layers]
_fully-connected neural network where the width is {_ _}[T]_ _m, the learning rate is[∈{][x][t][}][T]_ _η, the number of iterations_
_of gradient descent is K. Then, there exist positive constants C1, C2, S, such that if_

_√log 1/δ_ 1
_m ≥_ _poly(T, n, L, log(1/δ) · d · e_ ), η = O(TmL + mλ)− _, K ≥_ _O(TL/λ),_

_then, with probability at least 1_ _δ, for any xt_ **xt** _t=1[, we have the following upper confidence]_
_bound:_ _−_ _∈{_ _}[T]_ e
_h(xt)_ _f_ (xt; θt) _γ1_ _g(xt; θt)/[√]m_ **A−t** 1 + γ2 + γ1γ3 + γ4, (D.1)
_|_ _−_ _| ≤_ _∥_ _∥_

_where_

_γ1(m, L) = (λ + t_ (L)) ((1 _ηmλ)[J/][2][p]t/λ) + 1_
_O_ _·_ _−_

det(A′t[)]

_γ2(m, L, δ) = ∥g(xt; θ0)/[√]m∥A′−t_ 1 _·_ slog  det(λI)  _−_ 2 log δ + λ[1][/][2]S!

_γ3(m, L) = C2m[−][1][/][6][p]log mt[1][/][6]λ[−][7][/][6]L[7][/][2], γ4(m, L) = C1m[−][1][/][6][p]log mt[2][/][3]λ[−][2][/][3]L[3]_

_t_ _t_

**At = λI +** _g(xt; θt)g(xt; θt)[⊺]/m, A[′]t_ [=][ λ][I][ +] _g(xt; θ0)g(xt; θ0)[⊺]/m._

_i=1_ _i=1_

X X

Note that g(xt; θ0) is the gradient at initialization, which can be initialized as constants. Therefore,
the above UCB can be represented as the following form for exploitation network f1: _h(xt,i)_
_|_ _−_
_f1(xt,i; θ[1]t_ [)][| ≤] [Ψ(][g][(][x][t][;][ θ][t][))][.]


-----

Table 3: Exploration Direction Comparison.

|Methods|"Upward" Exploration|"Downward" Exploration|
|---|---|---|


|NeuralUCB|√|×|
|---|---|---|


|NeuralTS|Randomly|Randomly|
|---|---|---|


|EE-Net|√|√|
|---|---|---|



**EE-Net has smaller approximation error. Given an arm x, let f1(x) be the estimated reward and**
_h(x) be the expected reward. The exploration network f2 in EE-Net is to learn h(x) −_ _f1(x), i.e.,_
the residual between expected reward and estimated reward, which is the ultimate goal of making
exploration. There are advantages of using a network f2 to learn h(x) − _f1(x) in EE-Net, compared_
to giving a statistical upper bound for it such as NeuralUCB, (Ban et al., 2021), and NeuralTS (in
NeuralTS, the variance ν can be thought of as the upper bound). For EE-Net, the approximation error
for h(x) _f1(x) is caused by the genenalization error of the neural network (Lemma B.1. in the_
_−_
manuscript). In contrast, for NeuralUCB, (Ban et al., 2021), and NeuralTS, the approximation error
for h(x) _f1(x) includes three parts. The first part is caused by ridge regression. The second part_

1
_−_
of the approximation error is caused by the distance between ridge regression and Neural Tangent
Kernel (NTK). The third part of the approximation error is caused by the distance between NTK and
the network function. Because they use the upper bound to make selections, the errors inherently
exist in their algorithms. By reducing the three parts of the approximation errors to only the neural
network convergence error, EE-Net achieves tighter regret bound compared to them (improving by
roughly log T ).

_[√]_

ℎ(𝑥",$ ) 𝑓![(𝑥]",$ [; 𝜃][!][)]

Gap Gap

𝑓![(𝑥]",$ [; 𝜃][!][)] ℎ(𝑥",$ )

Case 1: Upward Exploration Case 2: Downward Exploration

Figure 6: Two types of exploration: Upward exploration and Downward exploration. f1 is the
exploitation network (estimated reward) and h is the expected reward.

**EE-Net has the ability to determine exploration direction. The two types of exploration are**


described by Figure 6. When the estimated reward is larger than the expected reward, i.e., h(x) −
_f1(x) < 0, we need to do the ‘downward exploration’, i.e., lowering the exploration score of x to_
reduce its chance of being explored; when h(x) _f1(x) > 0, we should do the ‘upward exploration’,_
_−_
i.e., raising the exploration score of x to increase its chance of being explored. For EE-Net, f2 is
to learn h(x) _f1(x). When h(x)_ _f1(x) > 0, f2(x) will also be positive to make the upward_
_−_ _−_
exploration. When h(x) _f1(x) < 0, f2(x) will be negative to make the downward exploration. In_
_−_
contrast, NeuralUCB will always choose upward exploration, i.e., f1(x) + UCB(x) where UCB(x)
is always positive. In particular, when h(x) _f1(x) < 0, NeuralUCB will further amplify the mistake._
_−_
NeuralTS will randomly choose upward or downward exploration for all cases, because it draws a
sampled reward from a normal distribution where the mean is f1(x) and the variance ν is the upper
bound.


-----

