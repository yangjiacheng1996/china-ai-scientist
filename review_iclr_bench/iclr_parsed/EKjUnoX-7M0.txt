# A NEW LOOK AT FAIRNESS IN STOCHASTIC MULTI## ARMED BANDIT PROBLEMS

**Anonymous authors**
Paper under double-blind review

ABSTRACT

We study an important variant of the stochastic multi-armed bandit (MAB) problem, which takes fairness into consideration. Instead of directly maximizing cumulative expected reward, we need to balance between the total reward and fairness level. In this paper, we present a new insight in MAB with fairness and
formulate the problem in the penalization framework, where rigorous penalized
regret can be well defined and more sophisticated regret analysis is possible. Under such a framework, we propose a hard-threshold UCB-like algorithm, which
enjoys many merits including asymptotic fairness, nearly optimal regret, better
tradeoff between reward and fairness. Both gap-dependent and gap-independent
upper bounds have been established. Lower bounds are also given to illustrate the
tightness of our theoretical analysis. Numerous experimental results corroborate
the theory and show the superiority of our method over other existing methods.

1 INTRODUCTION

The multi-armed bandit (MAB) problem is a classical framework for sequential decision-making in
uncertain environments. Starting with the seminal work of Robbins (1952), over the years, a significant body of work has been developed to address both theoretical aspects and practical applications
of this problem. In a traditional stochastic multi-armed bandit (MAB) problem (Lai & Robbins,
1985; Auer et al., 2002; Vermorel & Mohri, 2005; Bubeck & Cesa-Bianchi, 2012), a learner has access to K arms and pulling arm k generates a stochastic reward for the principal from an unknown
distribution Fk with an unknown expected reward µk. If the mean rewards were known as prior information, the learner could just repeatedly pull the best arm given by k[∗] = arg maxk µk. However,
the learner has no such knowledge of the reward of each arm. Hence, one should use some learning
algorithm π which operates in rounds, pulls arm πt 1, . . ., K in round t, observes the stochastic
reward generated from reward distribution Fπt, and uses that information to learn the best arm over ∈{ _}_
time. The performance of learning algorithm π is evaluated based on its cumulative regret over time
horizon T, defined as


Eµπt _._ (1)
_t=1_

X


_R¯π(T_ ) = µk∗ _T_
_−_


To achieve the minimum regret, a good learner should make a balance between exploration (pulling
different arms to get more information of reward distribution of each arm) and exploitation (pulling
the arm currently believed to have the highest reward).

In addition to the above classical MAB problem, many variations of the MAB framework have been
extensively studied in the literature recently. Various papers study MAB problems with additional
constraints which include bandits with knapsack constraints (Badanidiyuru et al., 2013), bandits
with budget constraints (Xia et al., 2015), sleeping bandits (Kleinberg et al., 2010; Chatterjee et al.,
2017), etc. Except these, there is a huge research interest in fairness within machine learning field.
Fairness has been a hot topic of many recent application tasks, including classification (Zafar et al.,
2017a;b; Agarwal et al., 2018; Roh et al., 2021), regression (Berk et al., 2017; Rezaei et al., 2019),
recommendation (Celis et al., 2017; Singh & Joachims, 2018; Beutel et al., 2019; Wang et al., 2021),
resource allocation (Baruah et al., 1996; Talebi & Proutiere, 2018; Li et al., 2020), Markov decision
process (Khan & Goodridge, 2019), etc. There are two popular definitions of fairness in the MAB
literature. 1). The fairness is introduced into the bandit learning framework by saying that it is unfair


-----

to preferentially choose one arm over another if the chosen arm has lower expected reward than the
unchosen arm (Joseph et al., 2016). In other words, the learning algorithm cannot favor low-reward
arms. 2). The fairness is introduced such that the algorithm needs to ensure that uniformly (i.e.,
at the end of every round) each arm is pulled at least a pre-specified fraction of times (Patil et al.,
2020). In other words, it imposes an additional constraint to prevent the algorithm from playing
low-reward arms too few times.

In this paper, we adopt a new perspective, e.g., in addition to maximizing the cumulative expected
reward, it also allows the user to specify how “hard” or how “soft” the fairness requirement on each
arm is. In this view, it is not always easy even to formulate the problem and to introduce an appropriate notion of regret. We thus propose a new formulation of fairness MAB by introducing penalty
term Ak max(τkT −Nk(T ), 0), where Ak, τk are the penalty rate and fairness fraction for arm k and
_Nk(T_ ) is the number of times pulling arm k. Hence it gives penalization when the algorithm fails
to meet the fairness constraint and penalty term is proportional to the gap between pulling number
and its required level. To solve this regularized MAB problem, we also propose a hard-threshold
upper confidence bound (UCB) algorithm. It is similar to the classical UCB algorithm but adds an
additional term to encourage the learner to favor those arms whose pulling numbers are below the
required level at each round. The advantage of our approach is that it allows the user to distinguish,
if desired, between arms for which is more important to sample an arm with required frequency and
those arms for which it is less important to do so.

To the best of our knowledge, there is no work on mathematical framework of fairness MAB with
regularization term in the literature. In this paper, we provide a relatively complete theory for the
fairness MAB. We rigorously formalize the penalized regret which can be used for evaluating the
performance of learning algorithm under fairness constraints. On theoretical side, the hard-threshold
UCB algorithm is proved to achieve asymptotic fairness when a large penalty rate is chosen. The algorithm is shown to obtain O(log T ) regret when the sub-optimality gap is assumed to be fixed. Additionally, the characterization of fluctuation of non-fairness level, max1 _t_ _T max(τkt_ _Nk(t), 0)_
_≤_ _≤_ _−_
is also given. Its magnitude is also shown to be O(log T ). Moreover, we establish a sub-optimal
gap-free regret bound of proposed method and provide insights on how hard-threshold based UCB
index works. We also point out that the analysis of proposed hard-threshold UCB algorithm is much
harder than the classical UCB due to the existence of interventions between different sub-optimal
arms. On numerical side, the experimental results confirm our theory and show that the performance of the proposed algorithm is better than other popular methods. Our method achieves a better
trade-off between reward and fairness.

**Notations. For real number x, (x)+ stands for max{0, x}; ⌊x⌋** is the largest integer smaller or equal
to x. For integer n, we use [n] to represent the set {1, . . ., n}. We say a = O(b); a = Ω(b) if there
exists a constant C such that a ≤ _Cb; a ≥_ _b/C. The symbols E and P(·) denote generic expectation_
and probability under a probability measure that may be determined from the context. We let π be a
generic policy / learning algorithm.

2 ACHIEVING FAIRNESS VIA PENALIZATION

Consider a stochastic multi-armed bandit problem with K arms and unknown expected rewards
_µ1, . . ., µK associated with these arms. The notion of fairness we introduce consists of proportions_
andτk ≥ N0k,π, k( = 1t) to denote the number of times that arm, . . ., K with τ1 + · · · + τK < 1. We use k has been pulled by time T ∈{1, 2, . . ., } to denote the time horizon t [T ] using policy
_∈_
_π. For notational simplicity, we may write Nk,π(t) as Nk(t). It is desired to pull arm k at least at_
the uniform rate of τk, k = 1, . . ., K. In other words, the learner should obey the constraint that
_Nk(t)_ _τkt for any t_ [T ]. Thus a good policy aims to solve the following optimization problem,
_≥_ _∈_


_µkNk,π(T_ ), subject to Nk,π(t) _τkt for all k and t._ (2)
_≥_


arg max


Instead of directly working with such a constrained bandit problem, we consider a penalization
problem. That is, one gets penalized if the arm is not pulled sufficiently often. To reflect this, we
introduce the following design problem. Let Sπ(T ) be the sum of the rewards obtained by time
_t under policy π, i.e., Sπ(T_ ) = _t=1_ _[r][π][t]_ [where][ π][t][ is the arm index chosen by policy][ π][ at time]

[P][T]


-----

_t ∈_ [T ] and rπt is the corresponding reward. Then the penalized total reward is defined as


+[,] (3)


_Spen,π(T_ ) = Sπ(T )
_−_


_Ak_ _τkT_ _Nk,π(T_ )
_−_
_k=1_

X  


where A1, . . ., AK are known nonnegative penalty rates. Our goal is to design a learning algorithm
to make the expectation of Spen,π(T ) as large as possible. By taking the expectation, we have


E[Spen,π(T )] =


_µkE[Nk,π(t)] −_
_k=1_

X


_AkE[_ _τkT −_ _Nk,π(T_ )
_k=1_

X  


+[]][,] (4)


which is the penalized reward achieved by policy π and we would like to maximize it over π. Now
we are ready to introduce the penalized regret function, which is the core for the regret analysis.

To derive the new regret, we first note that maximizing E[Spen,π(T )] is the same as minimizing the
following loss function,


_L(T_ ) = µ[∗]T − E[Spen,π(T )] =


∆kE[Nk(t)] + AkE[ _τkT −_ _Nk(T_ )
 


+[]] _,_ (5)
i


_k=1_


where we denote
_µ[∗]_ = max
_k=1,...,K_ _[µ][k][,][ ∆][k][ =][ µ][∗]_ _[−]_ _[µ][k][, k][ = 1][, . . ., K.]_

In order to find the minimum possible value of L(T ), let us understand what a prophet (who knows
the expected rewards µ1, . . ., µK) would do. Clearly, a prophet (who, in addition, is not constrained
by integer value) would solve the following optimization problem,


∆kxk + AkE _τkT −_ _xk_
 


subject to


min
_x1,...,xK_


_k=1_ _xk = T, xk ≥_ 0, k = 1, . . ., K,

X


_k=1_


and pull arm k for xk times (k = 1, . . ., K). By denoting yk = xk/T, k ∈ [K], we transform this
problem into


subject to


_k=1_ _yk = 1, yk ≥_ 0, k = 1, . . ., K. (6)

X


∆kyk + Ak _τk −_ _yk_
 


min
_y1,...,yK_


_k=1_


We will solve the problem (6) by finding y1, . . ., yK that satisfy the constraints and that minimize
simultaneously each term in the objective function. It is not hard to observe the following facts.

1. For A ≥ 0, function y 7→ _A(τ −_ _y)+ achieves its minimum value of 0 for y ≥_ _τ_ .
2. For A ≥ ∆ _> 0, function y 7→_ ∆y + A(τ − _y)+ achieves its minimum of ∆τ at y = τ_ .
3. For ∆ _> A ≥_ 0, function y 7→ ∆y + A(τ − _y)+ achieves its minimum of Aτ at y = 0._

As a result, we introduce the following three sets

opt = _k_ [K] : µk = µ[∗] _,_ cr = _k_ [K] : Ak ∆k > 0 _,_ non cr = _k_ [K] : ∆k > Ak
_A_ _∈_ _A_ _∈_ _≥_ _A_ _−_ _∈_

where opt consists of all optimal arms, cr consists of sub-optimal arms with penalty rate larger
_A_ _A_
than (or equal to) the sub-optimal gap and non cr includes sub-optimal arms with penalty rate
_A_ _−_
smaller than the sub-optimal gap. Therefore an optimal solution to the problem (6) can be constructed as follows. Let k[∗] be an arbitrary arm in opt, and choose
_A_

1 − [P]j∈Acr∪(Aopt\{k[∗]}) _[τ][j][,]_ _k = k[∗],_

_yk =_ _τk,_ _k_ cr ( opt _k[∗]_ ), (7)

 0, _k ∈Anon ∪_ crA. _\ {_ _}_
 _∈A_ _−_




Therefore, a prophet would choose (modulo rounding) in (5)


1 _j_ cr ( opt _k[∗]_ ) _[τ][j]_ _T,_ _k = k[∗],_
_−_ [P] _∈A_ _∪_ _A_ _\{_ _}_

_τkT,_  _k_ cr ( opt _k[∗]_ ), (8)
0, _k ∈A ∈Anon ∪−crA,_ _\ {_ _}_


_Nk(T_ ) =


-----

leading to the following optimal value of L(T ),


_L[∗](T_ ) =


_T._ (9)

(10)


∆kτkT +
_kX∈Acr_


_AkτkT =_
_k∈AXnon−cr_


min(∆k, Ak)τk

_k=1_

X


Given an arbitrary algorithm π, we can therefore define the penalized regret as


_Rπ(T_ ) = Lπ(T ) _L[∗](T_ ) =
_−_


_AkE_ _τkT −_ _Nk,π(T_ )
_k∈AXopt_  


∆kE _Nk,π(T_ ) − _τkT_ + AkE _τkT −_ _Nk,π(T_ )
    


_k∈Acr_


+

_[−]_ _[τ][k][T]_

 i


∆kENk,π(T ) + Ak E _τkT −_ _Nk,π(T_ )
  


_k∈Anon−cr_


3 A HARD-THRESHOLD UCB ALGORITHM

We now introduce a UCB-like algorithm which aims to achieve the minimum penalized regret described in the previous section. We assume that all rewards take values in the interval [0, 1]. We
denote by Xn[(][k][)] the reward obtained after pulling arm k for the nth time, k [K], n = 1, 2, . . .. Let
_∈_

1 _Nn(k)_
_mˆ_ _k(n) =_ _Nn(k)_ _Xi[(][k][)], k = 1, . . ., K, n = 1, 2, . . ._ (11)

_i=1_

X

and introduce the following index: for k = 1, . . ., K, n = 1, 2, . . . set

2 log n

_ik(n) = ˆmk(n_ 1) + Ak1 _Nk(n_ 1) < τkn + (12)
_−_ _−_ s _Nk(n_ 1) _[.]_

_−_

  

The algorithm proceeds as follows. It starts by pulling each arm once. Then at each subsequent step,
we pull an arm with the highest value of the index ik(n). In equation 12, there is an additional term
_Ak1(Nk(n_ 1) < τkn) compared with classical UCB algorithm. It takes the hard threshold form.
_−_
Once the number of times that arm k has been pulled before time n is less than the fairness level
(τkn) at round n, penalty rate Ak will be added to the UCB index. In other words, the proposed algorithm favors those arms which does not meet the fairness requirement. The detailed implementation
is given in Algorithm 1.

4 THEORETICAL ANALYSIS OF THE HARD-THRESHOLD UCB ALGORITHM

In this section, we present theoretical results for the hard-threshold UCB algorithm introduced in
Section 3. Throughout this section, we need to introduce additional notation and concepts. We say
_τk = Ω(1)[˜]_ if it is a positive constant which is independent of T . We assume that there exists a
positive constant c0 such that _k_ _[τ][k][ ≤]_ [1][ −] _[c][0][ and][ T][ is much larger than][ K][. The penalty rates][ A][k][’s]_
are assumed to be known fixed constants. The expected reward µk (k ∈ [K]) is assumed between 0
and 1. Hence sub-optimality gap ∆k is between 0 and 1 as well.

[P]

**Asymptotic Fairness. Given the large penalty rates, the proposed algorithm can guarantee the**
asymptotic fairness for any arm k ∈ [K]. In other words, the algorithm can guarantee that the
number of times that arm k has been pulled up to time T is at least _τkT_ with high probability.
_⌊_ _⌋_

**Theorem 1 If Ak** ∆k min 32 logτkT T _, 1_ _and τk = Ω(1)[˜]_ _for all k, we have Nk(T_ ) _τkT_
_−_ _≥_ _{_ _}_ _≥⌊_ _⌋_

_for any k with probability going to 1 asq_ _T_ _._
_→∞_

Theorem 1 tells us that the proposed algorithm treats every arm fairly when the penalty rate dominates the sub-optimality gap.


-----

**Algorithm 1 Hard-Threshold UCB Algorithm.**

1: Input. Number of arms K, fairness proportions τk’s, penalty rates Ak’s, time horizon T .
2: Output. Cumulative reward, the number of times that each arm is played (Nk(T ), k ∈
_{1, . . ., K}.)_

3: Initialization.
For each k ∈{1, . . ., K}, we set initial count Nk = 0 and arm-specific cumulative reward
_Rk = 0._

4: while n ≤ _T do_
5: If n ≤ _K, we choose kn = n._

6: If n > K, we choose kn = arg maxk ik(n).

7: We observe reward rn. We update count Nkn = Nkn +1 and update reward Rkn = Rkn +rn.


8: We update hard-threshold index for each arm k ∈{1, . . ., K} by calculating


2 log n

_Nk_


_ik(n + 1) = Rk/Nk + Ak1(Nk < τk(n + 1)) +_


9: Increase time index n by 1.

10: end while
11: Return vector (Nk).

4.1 REGRET ANALYSIS: UPPER BOUNDS
In this section, we provide upper bounds on the penalized regret defined in equation 10 under two
scenarios. (1) We establish the gap-dependent bound when the sub-optimality ∆k’s are fixed constants. (2) We prove the gap-independent bound when ∆k’s vary within the interval [0, 1].

**Theorem 2holds for any arm (Gap-dependent Upper bound.) Assume that k ∈Aopt ∪Acr and ∆k −** _Ak ≥_ 8K Ac log[2]0[T]k − T ∆holds for anyk ≥ _ca (ca is a positive constant) k ∈Anon-cr. We then_

_have the following results._ q


_For any k ∈Aopt ∪Acr, it holds_


E[(τkT − _Nk(T_ ))+] = O(1).


_For any k ∈Acr, it holds_


E[Nk(T )] max _, τkT_ + O(1).
_≤_ _{_ [8 log]∆[2]k[ T] _}_


_For any arm kj ∈Anon-cr, it holds_

8 log T
E[Nk(T )] max min + O(1).
_≤_ _{_ _{_ (∆k _Ak)[2][, τ][k][T]_ _[}][,][ 8 log]∆[2]k[ T]_ _}_

_−_

_Therefore, we have_


( [8 log][ T] _τkT_ )+ +

∆k _−_

_kX∈Acr_


max min [8 log][ T] _, (∆k_ _Ak)τkT_ _,_ [8 log][ T] + O(K). (13)
_k_ _non-cr_ _{_ _{_ ∆k _Ak_ _−_ _}_ ∆k _}_
_∈AX_ _−_


_Rπ(T_ )
_≤_


Theorem 2 tells us that the number of times that each arm k in critical set Acr is played is at least
around fairness requirement τkT when the the penalty rate is larger than the sub-optimality gap by
some constant. On the other hand, for each arm k in non-critical set Anon-cr, it could be played less
than fairness requirement when sub-optimality gap substantially dominates the penalty rate. The
total penalized regret has order of log T and is hence nearly not improvable. In addition, when
_Ak_ 0 and it degenerates to the classical settings, then all arms become non-critical arms and our
_≡_ 8 log T
bound reduces to O([P]k ∆k [)][ which matches the existing result (Auer et al., 2002).]

**Maximal Inequality. In Theorem 2 above we have shown that E[(τkT −** _Nk(T_ ))+] = O(1) for any
_kfor the ∈Aopt non-fairness level ∪Acr under mild conditions on, (τkt_ _Nk(t))+ ∆, tk’s. In the result below, we derive a maximal inequality[T_ ].
_−_ _∈_


-----

**Theorem 3 Order the K arms in such a way that**

_Ak1 + µk1 ≥_ _. . . ≥_ _Akj + µkj ≥_ _. . . ≥_ _AkK + µkK_ _._

_Then for any arm kj_ _opt_ _cr, we have_
_∈A_ _∪A_

E[ max
1 _t_ _T_ [(][τ][k][j] _[t][ −]_ _[N][k][j]_ [(][t][))][+][]][ ≤] _[a][j][ log][ T][ +][ O][(1)][,]_
_≤_ _≤_

_where the coefficient aj is defined as_

_j_ _d−1_ 1 _K_ 1

_aj = 8_ (j _d + 1)_

_d=1_ _−_ _m=1_ (µkd + Akd _µkm)[2][ +]_ _m=d+1_ (µkd + Akd _µkm_ _Akm_ )[2]

X X _−_ X _−_ _−_


(14)


Theorem 3 nearly guarantees the ANY-ROUND fairness for all arms k ∈ [K] up to a O(log T )
difference.

**Gap-independent Upper bound. We now switch to establishing a gap-independent upper bound. It**
relies on the following observations. The key challenge is how to bound the term E[(τkT −Nk(T ))+]
for k ∈Aopt ∪Acr.


(Observation 1) If Ak ∆k
_−_

4τkT [2][/][3](2 log T )[1][/][2].


2 log T

_T_ [2][/][3][, then][ (][A][k][ −] [∆][k][)][E][[(][τ][k][T][ −] _[N][k][(][T]_ [))][+][]]


**Lemma 1 (Observation 2) If arm k satisfies that Ak −** ∆k ≥ 4

_have E[(τkT −_ _Nk(T_ ))+] = O(τkKT [2][/][3]).

Based on above observations, we have the following regret bound.

**Theorem 4 When τk = Ω(1)[˜]** _, it holds that_


2 logT [2][/] T[3][ and][ τ][k][ = ˜]Ω(1), then we


_√τk) + 8_


(1 _τmin)KT log T + AmaxKT_ [2][/][3](2 log T )[1][/][2], (15)
_−_


_Rπ(T_ ) 8
_≤_


_T log T_ (


_where τmin = mink τk and Amax = maxk Ak._

The first term in (15) is for Ak(E(τkT −Nk(T ))+) with k ∈Anon-cr. The second term gives a bound
for ∆kE(Nk(T ) − _τkT_ ) for k ∈ [K]. The third term in (15) is for bounding AkE(τkT − _Nk(T_ ))+
for k ∈Aopt ∪Acr.

4.2 REGRET ANALYSIS: LOWER BOUNDS

**Gap-dependent Lower Bound.** In this part, we first show that the bound given in inequality (13)
is tight. To see this, the results are stated in the following theorems.

**Theorem 5 There exists a bandit setting for which the regret of proposed algorithm has the follow-**
_ing lower bound,_


log T

_._ (16)
∆k _Ak_
_−_


_Rπ(T_ )
_≥_


_k∈Anon-cr,τk>0_


**Theorem 6 There exists a bandit setting for which the regret of proposed algorithm has the follow-**
_ing lower bound,_


∆k( [log][ T] _τkT_ ). (17)
_kX∈Acr_ ∆[2]k _−_


_Rπ(T_ )
_≥_


Theorem 5 says that the term log T/(∆k _Ak) is nearly optimal up a multiplicative constant 8 for_
_−_
any arm in the non-critical set. Similarly, Theorem 6 tells us that ( [8 log]∆k[ T] _τkT_ )+ is also nearly

_−_


-----

optimal for arms in the critical set. Therefore, Theorem 2 gives a relatively sharp gap-dependent
upper bound. It is almost impossible to improve the regret bound analysis for our proposed hardthreshold UCB algorithm in the instance-dependent scenario.

**Gap-independent Lower Bound.** We also obtain a gap-independent lower bound as follows.

**Theorem 7 Let K > 1 and T be a large integer. Penalty rates A1, A2, . . ., AK are fixed positive**

_policyconstants. Assume that the fairness parameters π, there exists a mean vector µ = (µ1, . . ., µ τ1, . . ., τK) such thatK ∈_ [0, 1] with _k_ _[τ][k][ <][ 1][. Then, for any]_

[P]

_Rπ(T_ ) _C(1_ 2 max _τk)_ (K 1)T,
_≥_ _−_ _k_ _−_
p

_where C is a universal constant which does not depend on Ak, τk’s._

By comparing Theorems 4 and 7, we can see that there is a substantial gap. This is because term
_AkE[(τkT −_ _Nk(T_ ))+] is very hard to handle. This term can be trivially lower bounded below by
zero for any algorithm. However, this term is proved to be O(T [2][/][3]) (ignoring log T factor) by our
current techniques under the proposed algorithm. Whether we can improve the gap-independent
upper bound to be O(T [1][/][2]) is an open question in the future work.

5 COMMENTS ON THE HARD-THRESHOLD UCB ALGORITHM

**On hard threshold. In the proposed algorithm, we use a hard-threshold term Ak1(Nk(n −** 1) <
_τkn) in constructing a UCB-like index ik(n). A natural question is whether we can use a soft-_
threshold index by defining


˜ik(n) = ˆmk(n 1) + Ak max(τkn − _Nk(n −_ 1), 0)
_−_ _τkn_


2 log n

_Nk(n_ 1) [?]
_−_


The answer is negative in the sense that [˜]ik(n) becomes a continuous function of Nk and does not
have a jump point at the critical value τkn. Hence it does not give sufficient penalization to those
arms k which are below the fairness proportion τk. Hence, a soft-threshold UCB-like index fails to
guarantee the asymptotic fairness and nearly-optimal penalized regret.

**When τk is not constant. In our theoretical analysis, we only consider the case that τk = Ω(1)[˜]** for
ease of presentation. The current results could also apply when threshold τk is dependent on time
horizon T with τk(T ) = 1/T _[b](0 < b < 1)._

**Technical Challenges. Since the index ik(n) is a discontinuous function of Nk, this brings addi-**
tional difficulties in analyzing the regret bound. The most distinguished feature from the classical
regret analysis is that we cannot analyze term Nk(T ) separately for each sub-optimal gap k. In fact,
the optimal arm (arg maxk µk) is fixed for all rounds in the classical setting. In contrast, the “optimal arm” (arg maxk µk + Ak1{Nk < τkn}) varies as the algorithm progresses in our framework.
Due to such interventions among different arms, term (τkT − _Nk(T_ ))+ should be treated carefully.

**Connections to LASSO problems. We would like to point out that our current framework shares**
similarities with LASSO problem (Tibshirani, 1996; Zhao & Yu, 2006; Zou, 2006) in linear regression models. Both of them introduces the penalization terms to enforce the solution to obey fairness
constraints / sparsity to some degree. In our penalized MAB framework, whether an arm k is played
at least τkT times or not depends on the penalty rate Ak and the sub-optimality gap ∆k. Similarly,
in the LASSO framework, whether a coefficient is to be estimated as zero depends on the penalty
parameter and its true coefficient value.

**Comparison with Baselines. We compare the proposed methods with related existing methods.**

_Learning with Fairness Guarantee (LFG, Li et al. (2019)). It is implemented via following steps._

-  For each round n, we compute the index for each arm, [¯]ik(n) = min _m ˆ_ _k(n_ 1) +
_{_ _−_

2 log n

_Nk(n−1)_ _[,][ 1][}][ and compute queue length for each arm,][ Q][k][(][n][) = max][{][Q][k][(][n][ −]_ [1) +]
_τqk_ **1** arm k is pulled _, 0_ .
_−_ _{_ _}_ _}_


-----

-  The learner plays the arm which maximizes Qk(n) + η0wkik(n) and receive the corresponding reward, where η0 is the tuning parameter and wk is the known weight. Without
loss of generality, we assume wk 1 by treating each arm equally when we have no
additional information. _≡_

_Fair-Learn (Flearn, Patil et al. (2020)). Its main procedure is given as below._

-  For each round n, we compute set (n), (n) := _k : τk(n_ 1) _Nk(n_ 1) > α,
_A_ _A_ _{_ _−_ _−_ _−_ _}_
which contains those arms which are not fair at round n at level.

-  If (n) =, we play arm which maximizes τk(n 1) _Nk(n_ 1). Otherwise, we play
_A_ _̸_ _∅_ _−_ _−_ _−_

arm which maximizes ˆmk(n 1) + _N2 logk(n_ _n1)_ [.]
_−_ _−_
q


Fair-learn method can enforce each arm k should be played at proportion level τk only when α = 0.
LFG method fails to guarantee the asymptotic fairness when η0 > 0. Neither of these methods can
well balance between total rewards and fairness constraint as our method does.

6 EXPERIMENT RESULTS

A In this experimental setting, we examine the relationship between number of times that noncritical arm k has been pulled at T (=20000) rounds and the inverse gap 1/(∆k _Ak)[2]. In_
particular, we construct the following three parameter settings (τk 1/20). _−_
_≡_
Case 1: K = 9; µ = (0.9, 0.8, 0.7, 0.6, 0.6, 0.4, 0.3, 0.2, 0.1); Ak 0.45.
_≡_
Case 2: K = 9; µ = (0.95, 0.8, 0.7, 0.6, 0.6, 0.4, 0.3, 0.2, 0.1); Ak 0.41.
_≡_
Case 3: K = 9; µ = (0.9, 0.8, 0.7, 0.6, 0.6, 0.425, 0.4, 0.375, 0.35); A ≡ 0.45.

B Similarly, we examine the relationship between number of times that critical arm k has been
pulled at T (=20000) rounds and the inverse gap 1/∆[2]k[. We set][ τ][k][ ≡] [1][/][20][,][ A][k][ ≡] [0][.][45][.]
Case 1: K = 9; µ = (0.9, 0.86, 0.84, 0.82, 0.6, 0.4, 0.3, 0.2, 0.1).
Case 2: K = 9; µ = (0.95, 0.85, 0.84, 0.83, 0.82, 0.4, 0.3, 0.2, 0.1).
Case 3: K = 9; µ = (0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1).

C We investigate the relationship between cumulative penalized regret and total time horizon (T ) under three algorithms (proposed method, LFG, and Flearn). The parameters
are constructed as follows. The number of arms (K) is set to be 5 or 20. The total
time horizon (T ) varies from 500 to 16000. The fairness proportion τk of each arm is
set to be τk = τ/K with τ ∈{0.2, 0.4, 0.8}. The penalty rate Ak is constructed as
generated betweenAk ≡ (maxk µk − [0min, 1]k. The reward distribution of each arm is a Gaussian distribution, µk)/2. Each entry of the mean reward vector (µk) is randomly
e.g., N (µk, _K1[2][ )][. For Flearn algorithm, we take tuning parameter][ α][ = 0][. For LFG algo-]_

rithm, we take η0 = _√T_ . Each case is replicated for 50 times.


From Figure 1, we can see that the pulling number Nk(T ) is proportional to 1/(∆k _Ak)[2]_ for
_k ∈Anon-cr when Nk(T_ ) does not reach fairness level τkT . We also see that Nk(T ) is proportional −
to 1/∆[2]k [for][ k][ ∈A][cr][ when the pulling number is larger than fairness level][ τ][k][T] [. These phenomena]
match the results in Theorem 2. From Figure 2, we observe that the proposed method achieves
smaller penalized regret compared with LFG and Flearn. This confirms that our method is indeed a
good learning algorithm under penalization framework.

In the appendix, we also study the paths of unfairness level ((τkT _Nk(T_ ))+) when tuning parame_−_
ter varies and investigate the relationship between total expected reward ([P][T]t=1 _[µ][π]t_ [) and unfairness]
level ([P]k [K][(][τ][k][T][ −] _[N][k][(][T]_ [))][+][) for three algorithms. From Figure 3 (See Appendix A), the paths]

_∈_
of unfairness level show different behaviors under three algorithms. For our method, with scale
parameter decreasing, each arm becomes unfair one by one. By contrast, all arms under both Flearn
and LFG methods suddenly become unfair once scale parameter decreases from 1. This suggests
that our method has sparsity feature as LASSO does, e.g., making arms with small sub-optimality
gap fair. From Figure 4 (See Appendix A), we can tell that the proposed method always achieves
the highest reward given the same unfairness level under different parameter settings. This gives
evidence that hard-threshold UCB algorithm makes better balance between total reward and fairness
constraints compared with other competing methods.


-----

Figure 1: Upper row: Nk(T ) vs 1/(∆k _Ak)[2]_ for arm k non-cr. Bottom row: Nk(T ) vs 1/∆[2]k
for arm k ∈Acr. In all plots, the blue horizontal line stands for fairness level − _∈A_ _τkT_ .

Figure 2: Penalized Regret (Rπ(T )) vs Different Time Horizon (T ) under different settings.

7 CONCLUSION

In this paper, we provide a new framework of fairness MAB problem by introducing regularization
terms. The advantage of our new approach is that it allows the user to distinguish between arms for
which is more important to sample an arm with required frequency level and arms for which it is
less important to do so. A hard-threshold UCB algorithm is proposed and is shown to have good
performance under this framework. Unlike other existing algorithms, the proposed algorithm not
only achieves the asymptotic fairness but also handles well in balance between reward and fairness
constraints. A relatively complete theory, including both gap-dependent / independent bounds, has
been established. The new theoretical results contribute to the fairness in machine learning field and
bring better insights in how to play smartly in the exploitation and exploration games.


-----

REFERENCES

Alekh Agarwal, Alina Beygelzimer, Miroslav Dud´ık, John Langford, and Hanna Wallach. A reductions approach to fair classification. In International Conference on Machine Learning, pp.
60–69. PMLR, 2018.

Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
problem. Machine learning, 47(2):235–256, 2002.

Ashwinkumar Badanidiyuru, Robert Kleinberg, and Aleksandrs Slivkins. Bandits with knapsacks.
In 2013 IEEE 54th Annual Symposium on Foundations of Computer Science, pp. 207–216. IEEE,
2013.

Sanjoy K Baruah, Neil K Cohen, C Greg Plaxton, and Donald A Varvel. Proportionate progress: A
notion of fairness in resource allocation. Algorithmica, 15(6):600–625, 1996.

Richard Berk, Hoda Heidari, Shahin Jabbari, Matthew Joseph, Michael Kearns, Jamie Morgenstern, Seth Neel, and Aaron Roth. A convex framework for fair regression. _arXiv preprint_
_arXiv:1706.02409, 2017._

Alex Beutel, Jilin Chen, Tulsee Doshi, Hai Qian, Li Wei, Yi Wu, Lukasz Heldt, Zhe Zhao, Lichan
Hong, Ed H Chi, et al. Fairness in recommendation ranking through pairwise comparisons. In
_Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &_
_Data Mining, pp. 2212–2220, 2019._

S´ebastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multiarmed bandit problems. arXiv preprint arXiv:1204.5721, 2012.

L Elisa Celis, Damian Straszak, and Nisheeth K Vishnoi. Ranking with fairness constraints. arXiv
_preprint arXiv:1704.06840, 2017._

Aritra Chatterjee, Ganesh Ghalme, Shweta Jain, Rohit Vaish, and Y Narahari. Analysis of thompson
sampling for stochastic sleeping bandits. In UAI, 2017.

Matthew Joseph, Michael Kearns, Jamie H Morgenstern, and Aaron Roth. Fairness in learning:
Classic and contextual bandits. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc.,
2016.

Koffka Khan and Wayne Goodridge. S-mdp: Streaming with markov decision processes. IEEE
_Transactions on Multimedia, 21(8):2012–2025, 2019._

Robert Kleinberg, Alexandru Niculescu-Mizil, and Yogeshwer Sharma. Regret bounds for sleeping
experts and bandits. Machine learning, 80(2):245–272, 2010.

Tze Leung Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances
_in applied mathematics, 6(1):4–22, 1985._

Fengjiao Li, Jia Liu, and Bo Ji. Combinatorial sleeping bandits with fairness constraints. IEEE
_Transactions on Network Science and Engineering, 7(3):1799–1813, 2019._

Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. Fair resource allocation in federated
learning. In 8th International Conference on Learning Representations, 2020.

Vishakha Patil, Ganesh Ghalme, Vineet Nair, and Y Narahari. Achieving fairness in the stochastic
multi-armed bandit problem. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 34, pp. 5379–5386, 2020.

Ashkan Rezaei, Rizal Fathony, Omid Memarrast, and Brian D Ziebart. Fair logistic regression: An
adversarial perspective. 2019.

Herbert Robbins. Some aspects of the sequential design of experiments. Bulletin of the American
_Mathematical Society, 58(5):527–535, 1952._


-----

Yuji Roh, Kangwook Lee, Steven Euijong Whang, and Changho Suh. Fairbatch: Batch selection for
model fairness. In 9th International Conference on Learning Representations, 2021.

Ashudeep Singh and Thorsten Joachims. Fairness of exposure in rankings. In Proceedings of
_the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp._
2219–2228, 2018.

Mohammad Sadegh Talebi and Alexandre Proutiere. Learning proportionally fair allocations with
low regret. Proceedings of the ACM on Measurement and Analysis of Computing Systems, 2(2):
1–31, 2018.

Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
_Society: Series B (Methodological), 58(1):267–288, 1996._

Joannes Vermorel and Mehryar Mohri. Multi-armed bandit algorithms and empirical evaluation. In
_European conference on machine learning, pp. 437–448. Springer, 2005._

Lequn Wang, Yiwei Bai, Wen Sun, and Thorsten Joachims. Fairness of exposure in stochastic
bandits. In Proceedings of the 38th International Conference on Machine Learning, volume 139,
pp. 10686–10696. PMLR, 2021.

Yingce Xia, Haifang Li, Tao Qin, Nenghai Yu, and Tie-Yan Liu. Thompson sampling for budgeted
multi-armed bandits. In Twenty-Fourth International Joint Conference on Artificial Intelligence,
2015.

Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fairness beyond disparate treatment & disparate impact: Learning classification without disparate
mistreatment. In Proceedings of the 26th international conference on world wide web, pp. 1171–
1180, 2017a.

Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P Gummadi. Fairness constraints: Mechanisms for fair classification. In Artificial Intelligence and Statistics, pp.
962–970. PMLR, 2017b.

Peng Zhao and Bin Yu. On model selection consistency of lasso. The Journal of Machine Learning
_Research, 7:2541–2563, 2006._

Hui Zou. The adaptive lasso and its oracle properties. Journal of the American statistical associa_tion, 101(476):1418–1429, 2006._


-----

APPENDICES

In this appendix, the first section is dedicated for experimental results of Experiments D and E. In
the rest, we collect all technical proofs. Specifically, the proofs of gap-dependent upper and lower
bounds are given in Section B and C. The proofs of gap independent upper and lower bounds are
given in Section E and F, respectively. The proof of E[max1 _t_ _T (τkt_ _Nk(t))]+ is given in Section_
_≤_ _≤_ _−_
D.

A THE PLOTS FOR EXPERIMENTS D AND E

D We investigate the path of unfairness level ((τkT _Nk(T_ ))+) of each arm when the tuning
_−_
parameter varies. The parameters of two settings are constructed as follows.
Setting 1: K = 8, T = 10000; (µ1, . . ., µ8) = (0.9, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1); τ1 =
_. . . = τ8 =_ 21K [. The reward distribution of each arm is a Gaussian distribution, e.g.,]

_N_ (µk, _K1[2][ )][.]_

Setting 2: K = 8, T = 10000; (µ1, . . ., µ8) = (0.95, 0.7, 0.65, 0.6, 0.2, 0.15, 0.1, 0.05);
_τ1 = . . . = τ4 = 0.8_ _K[1]_ [and][ τ][5][ =][ . . .][ =][ τ][8][ = 0][.][4][τ][1][. Again, the reward distribution of each]

arm is a Gaussian distribution, e.g., N (µk, _K1[2][ )][.]_

The penalty rates Ak _η, where we call η is the scale parameter which takes value between_
0 and 1. For Flearn algorithm, the tuning parameter ≡ _α = (1_ _η)τ1T with η varying from_
_−_
0 to 1. For LFG algorithm, the tuning parameter η0 = (1 − _η)T with η ∈_ (0, 1].
When scale parameter η → 1, three algorithms will prefer to exploit the arm with highest
reward and pay less attention to the fairness. On the other hand, η → 0, three algorithms
tend to treat the fairness as the priority. (Due to space limit, the results are in Appendix A)

E We investigate the relationship between total expected reward ([P][T]t=1 _[µ][π]t_ [) and unfairness]
level ([P]k [K][(][τ][k][T][ −] _[N][k][(][T]_ [))][+][) for three algorithms. The parameters are given as follows.]

_∈_
We set K 5, 20 and τk _τ/K with τ_ 0, 20.5 . Each element in mean reward
vector (µk ∈{) is generated between 0 and 1. Moreover, we generate the reward from three} _≡_ _∈{_ _}_
different distributions, (1) Gaussian N (µk, _K1[2][ )][, (2) Beta][ Beta][(][µ][k][,][ 1][ −]_ _[µ][k][)][, (3) Bernoulli]_

_Bern(1, µk)._


Figure 3: Unfairness path ((τkT _Nk(T_ ))+, k [K]) for three algorithms under two settings
_−_ _∈_
described in Experiment D. (Upper row is for Setting 1 and bottom row is for Setting 2.) For
sub-optimal arms, the proposed method can guarantee the fairness with a wider range of tuning
parameter. By contrast, Flearn and LFG can break the fairness easily.


-----

Figure 4: Total regret vs unfairness level for three algorithms under different settings described in
Experiment E. (The first row is for 5 arms with required fraction of times τ = 0.2; The second row
is for 5 arms with required fraction of times τ = 0.5; The third row is for 20 arms with required
fraction of times τ = 0.2; The fourth row is for 20 arms with required fraction of times τ = 0.5.
The first column is for Gaussian reward distribution; the second column is for Beta distribution; and
the third column is for Bernoulli distribution.) Given the fixed unfairness level, the proposed method
can have larger total reward than other two methods consistently over all experimental settings.


-----

**Highlight of the proof. The technical challenge lies in handling the term (τkT −** _Nk(T_ ))+. (1)
Our main task in proving upper bound is to show that, for any k ∈Acr, (τkT − _Nk(T_ ))+ is O(1)
for in gap-dependent setting and it is _O[˜](T_ [2][/][3]) for gap-independent setting. Unlike the classical
UCB algorithm analysis, we cannot bound Nk(T ) separately for each arm k. Instead, we need to
study the relationship between any pair of critical arms and the relationship between critical arm
and non-critical arm. A key step is to find a stopping time n1 such that any arm k ∈Acr satisfies
_Nk(n1) ≥_ _τkn1. Therefore, between rounds n1 and T_, the behavior of (τkT − _Nk(T_ ))+ can
be well controlled. (2) In proving maximal inequality, we need to order K arms according to the
values of µk + Ak. Then the bound of max1 _t_ _T (τkt_ _Nk(T_ ))+ can be obtained by a recursive
_≤_ _≤_ _−_
formula (see equation 30) starting from k = k1 to k = kK, where k1 := arg max{µk + Ak} and
_kK := arg min{µk + Ak}._

B PROOF OF GAP-DEPENDENT UPPER BOUNDS

**Proof of Theorem 1 Its proof is essentially same as the proof of first part in Theorem 2 by treating**
the non-critical set Anon-cr empty.

**Proof of Theorem 2 We first prove the first part: E[(τkT −Nk(T** ))+] = O(1) for any k ∈Aopt∪Acr.

Suppose at time n that a critical arm k is played less than τkn. We can prove that the algorithm
pulls critical arm k[′] at time n such that Nk[′] (n) ≥ 8 log T/c[2]a [and][ N][k][′] [(][n][)][ > τ][k][n][ with vanishing]
probability. This is because


2 log n
_Nk(n)_

_[≤]_ _[m][k][′]_ [(][n][) +]


2 log n

_Nk′_ (n) [)]


P(Ak + mk(n) +


2 log n

_Nk′_ (n) [) + 2]n[2]

2 log n

s _Nk′_ (n) [) + 2]n[2]


P(Ak + µk ≤ +µ[′]k [+ 2]s

P(Ak ∆k ∆k′ + 2
_−_ _≤−_


= 2/n[2]. (18)

By the same reason, the algorithm pulls non-critical arm k[′′] at time n when Nk′′ (n) 8 log T/c[2]a
_≥_
with vanishing probability.

(Observation 3) In other words, it holds with high probability that once a critical arm k is played
with proportion less than required level τk’s, it must be pulled in next round when all other arms is
played with proportion greater than level τk’s and is played more than 8 log T/c[2]a [times.]

(Observation 4) It also holds with high probability that once a non-critical arm is played more than
8 log T/c[2]a[, it can be only played when all critical arms are played with frequency more than the]
required level τk’s.

Moreover, we can show that Nk′ (n) ≥ 8 log T/c[2]a [at time][ n][ =][ c][0][T/][2][ for each critical arm][ k][′][. If]
not, note thatHence, for any critical arm 8 log T/c[2]a _[≤] k[τ][k][′]_ _[′]_ _[c]can be played at most[0][T/][4][, then][ N][ ′]k[(][n][)][ < τ] maxk[ ′]_ _[n][ for any]{τk[′]_ _c0T/[ n]4[ ∈{⌈], 8 log[c][0] T/c[T/][4][2]a[⌉][}][, . . .,][ times between][ ⌊][c][0][T/][2][⌋}][.]_
rounds c0T/2 and c0T ; every non-critical arm k[′′] can be played at most 8 log T/c[2]a [times. Then, we]
must have
_c0T/2 −_ _c0T/4 ≤_ _τkc0T/4 +_ 8 log T/c[2]a[.]

_k_ _k_

X X

However, the above inequality fails to hold when T/ log T ≥ 16c[2]0[K/c]a[2][. This leads to the contra-]
diction. Thus, we have Nk′ (n) ≥ 8 log T/c[2]a [for any critical arm][ k][′][ at time][ n][ =][ c][0][T/][2][.]

Actually, this further gives us that we must have Nk′ (n) _τk′_ _n_ for all very critical arms at some
_≥⌊_ _⌋_
time n [c0T/2, T ]. To see this, we observe the fact that for any arm _k[¯], it will be played with prob-_
_∈_ 2
ability less than _T_ [2][ at time][ n][ once][ N]k[¯][(][n][)][ ≥] [max][{][τ][k][n,][ 8 log][ T/c][2]a[}][ and one critical arm][ k][′][ is played]

less than τk′ _n. (In other words, this tells us that once arm_ _k[¯] has been played max{τk¯[n,][ 8 log][ T/c][2]a[}]_
times, then it can only be played at time when all very critical arms k[′]s have been played for τk′ _n_
times or ⌊τk¯[n][⌋] [jumps by one with probability greater than][ 1][ −] [2][/T][ 2][.)]


-----

Let n1( _c0T/2) be the first ever time such that Nk′_ (n1) _τk′_ _n1_ for all critical arms k[′]s. By
_≥_ _≥⌊_ _⌋_
straightforward calculation, it gives that n1 must be bounded by


8 log T/c[2]a [+ (] _τk)T_
_k:non-critical_ _k:critical_

X X


_n1_ _c0T/2 +_
_≤_


with probability greater than 1 − 2K/T . That is, n1 is well defined between c0T and T . At time n1,
we have all critical arms k[′] such that Nk′ (n1) _τk′_ _n1._
_≥_

Moreover, we consider the first time n2(> n1) such that every non-critical arm k[′′] has been played

for at least 8 log T/c[2]a [times when][ ∆][k][′′][ ≤] _ca_ log(16 logc0T/ T2) (If ca > 4, it automatically holds for

anyalgorithm will choose non-critical arm ∆k′′ ). We claim that n2 ≤ _n1 + k c0[′′]T/whenq2. This is because, between rounds Nk′_ (n) _τk′_ (n) for all critical arms n1 and k n[′]2s and, the
_≥_
_Nk[′′]_ (n) ≤ log(c0T/2)/2∆[2]k[′′] [. To see this, we know that]


2 log n

_Nk′_ (n)

_[≥]_ _[m][k][′′]_ [(][n][) +]


2 log n

_Nk′′_ (n) [)]


P(mk[′] (n) +


2 log n

_Nk′_ (n)

_[≥]_ _[µ][k][′′][ +]_


log n

_Nk′′_ (n) [)]


P(µ[′]k [+ 2]

P(µ[′]k [+ 2]


2 log T log(c0T/2)

P(µ[′]k [+ 2] )

s _τk[′]_ _c0T/2_ s _Nk[′′]_ (n)

_[≥]_ _[µ][k][′′][ +]_

2/c0T. (19)


That is, index of arm k[′′] is larger than k[′] with high probability.

In other words, for each round between n1 and n2, each critical arm k[′] can be only
pulled at most τk′ (n2 _n1) before every non-critical arm k[′′]_ has been played for
min 8 log T/c[2]a[,][ log(][c][0][T/] −[2)][/][2∆][2]k[′′] _[}][. Additionally, each non-critical arm][ k][′′][ can be only played]_
_{_
for at most 8 log T/(∆k _Ak)[2]_ with high-probability. Therefore, it must hold that
_−_


8 log T/(∆k _Ak)[2]._
_−_


_n2_ _n1_ (
_−_ _≤_


_τk)(n2_ _n1) +_
_−_


However, the above inequality fails to hold when n2 _n1_ _c0T/2 under assumption that ∆k_ _Ak_

8Kc log[2]0[T] _T_ . This validates the claim n2 _n1 + c0T/−2._ _≥_ _−_ _≥_

_≤_

q

Starting from time n2, by the observations 3 and 4, it can be seen that the maximum values of
(τk′ _n −_ _Nk′_ (n))+ for any critical arm k[′] is always bounded by 1 with probability 1 − 2K/T (n ∈

[n2, T ]). This completes the proof of the first part.

For the second part, we need to prove E[Nk(T )] ≤ max{ [8 log]∆[2]k[ T] _[, τ][k][T]_ _[}][ +][ O][(1)][ for][ k][ ∈A][cr][.]_


When [8 log]∆[2]k[ T] _> τkT_, we can calculate the probability

P(arm k is pulled at round n + 1 _Nk(n)_
_|_ _≥_ [8 log]∆[2]k[ T]

_≤_ P(ik(n + 1) ≥ _ik[∗]_ (n + 1))


2 log(n + 1)

_mˆ_ _k∗_ (n + 1) +
_Nk(n)_ _≥_


2 log(n + 1)

_Nk(n)_


P( ˆmk(n + 1) +


1/n[2] 1/(8 log T/∆k)[2] 1/(τkT )[2]. (20)
_≤_ _≤_


-----

When [8 log]∆[2]k[ T] _≤_ _τkT_, we can similarly calculate the probability

P(arm k is pulled at round n + 1 |Nk(n) ≥ _τkT_ )
P(ik(n + 1) _ik∗_ (n + 1))
_≤_ _≥_


2 log(n + 1)

_mˆ_ _k∗_ (n + 1) +
_Nk(n)_ _≥_


2 log(n + 1)

_Nk(n)_


P( ˆmk(n + 1) +


1/n[2] 1/(τkT )[2]. (21)
_≤_ _≤_

Hence we can easily obtain that E[Nk(T )] ≤ max{ [8 log]∆[2]k[ T] _[, τ][k][T]_ _[}][ +][ O][(1)][ by union bound.]_

For the third part that E[Nk(T )] min (∆8 logk _A Tk)[2][, τ][k][T]_ _[}][ +][ O][(1)][ (][k][j][ ∈A][non-cr][), it follows from the]_
_≤_ _{_ _−_

fact that we can treat µk + Ak as new expected reward for arm k ∈Anon-cr. Thus the corresponding
sub-optimality gap is ∆k _Ak. The result follow by using standard technique in the classical UCB_
algorithm. Hence we omit the details here. −

Finally, by combining three parts and straightforward calculation, we obtain the desired gapdependent upper bounds. This concludes the proof.

C PROOF OF GAP-DEPENDENT LOWER BOUNDS

**Proof of Theorem 5.** We consider the following setting, where arm 1 is the optimal arm with a
deterministic rewardrate Ak = A for all k ∆ ∈and arms[K] with k ∆, (> Ak ≥. Assuming that2) are sub-optimal arms with reward zero. Let penalty(∆8 log−A T)[2][ ≤] _[τ][k][T/][2][, we construct a lower]_

bound as follows.

We claim that each arm k 2 will be played at least n1 := (∆logA T)[2][ times. If there exists an arm][ k][0]
_≥_ _−_

has not been played for n1 times, we then consider the time index na = T/2+1+(K −2) (∆[8 log]−A[ T])[2][ +]

_n1. At this time, we have that arm 1 is the arm with largest index since that for each sub-optimal_
arm k ̸= k0, its index will never exceeds ∆ once it has been played (∆8 log−A T)[2][ times. According to]

assumption that arm k0 has been played less than n1 times, thus arm 1 is the arm with largest index
at time na.


However, the index of arm 1 at time na is never larger than


2 logT/2 T + ∆. The index of arm k0 at


2 log(n1T/2) . It gives


time na is always larger than A +


2 log T

+ ∆ _< A +_
_T/2_


2 log(T/2)

_n1_ _≤_ _ik0_ (na), (22)


_i1(na)_
_≤_


which leads to the contradiction of the mechanism of the proposed algorithm. Hence, we have that
each sub-optimal should have been played for at least (∆logA T)[2][ times.]

_−_

**Proof of Theorem 6. We consider the another setting, where where arm 1 is the optimal arm with**
deterministic reward ∆1 + ∆2, arm k’s (k ∈Acr) are sub-optimal arms with reward being ∆1 and
arm k’s (k _non_ _cr) ar sub-optimal arms with reward being ∆2. Let penalty rate Ak = A2 for_
_∈A_ _−_
all k _cr with ∆2 < A2 and penalty rate Ak = A1 for all k_ _non_ _cr with ∆1 > A1. Assume_
that _∈Ak∈Anon−cr_ (∆8 log1−A T1)[2][ +][ P]k∈Acr 8 log∆[2]2 T _< T/2 and τkT ∈A ≤_ [log]∆[2]2[ T]−for k ∈Acr, we then have

the following lower bound.

[P]

We claim that for each arm k ∈Acr will be played for at least n2 := [log]∆[2]2[ T] [times. If not, there will]

be at least one arm k1 _cr has been played for less than n2 times. We consider the time stamp,_
_∈A_ 8 log T 8 log T
_nb = T/2+1+_ [P]k∈Anon−cr (∆1−A1)[2][ +] [P]k∈Acr;k≠ _k1_ ∆[2]2 + _n2. At this time, we have that arm_

1 is the arm with the largest index since that for each arm inthan ∆1 +∆2 once it has been played for (∆8 log1−A T1)[2][ times. For each arm] Anon−cr, its index is always smaller[ k][ ∈A][cr][ (][k][ ̸][=][ k][1][), its index]

is also smaller than ∆1 +∆2 once it has been played for [8 log]∆[2]2[ T] times. According to assumption that

arm k1 has been played less than n2 times, thus arm 1 is the arm with largest index at time nb.


-----

However, on other hand, the index of arm 1 at time nb is never larger than


2 log T

_T/2_ [+∆][1][ +∆][2][. The]


2 log(n2T/2) . It leads to


index of arm k1 is not smaller than ∆1 +


2 log T

+ ∆1 + ∆2 ∆1 +
_T/2_ _≤_


2 log(T/2)

_i2(nb),_
_n2_ _≤_


_i1(nb)_
_≤_


this contradicts with arm 1 is arm with largest index at time nb. Hence, any arm in Acr should be
played at least [log]∆[2]2[ T] [times.]

D PROOF OF MAXIMAL INEQUALITY (PROOF OF THEOREM 3)


We can order K arms according to the sums µk + Ak’s. Specifically, let the order k1, k2, . . ., kK be
defined by
_µk1 + Ak1 > µk2 + Ak2 > · · · > µkK + AkK_ _._ (23)
For simplicity we assume no ties in equation 23. We also assume that Ak > ∆k for all k ∈
opt cr.
_A_ _∪A_

We now aim to bound expectations of the E maxt∈[T ] _τkt −_ _Nk(t)_ + [for][ k][ ∈A][opt][ ∪A][cr][. We will]

use the ordering of the arms k1, k2, . . ., kK defined in equation 23. Take any arbitrary t [T ] and

   _∈_

let kj opt cr,
_∈A_ _∪A_
_m[(]t[j][)]_ = sup _n = 1, . . ., t : τkj_ _n_ _Nkj_ (n) _._ (24)
_≤_

Suppose for a moment that m[(]t[j][)] _< t. We have_
_τkj_ _t_ _Nkj_ (t) + (25)
_−_ _[≤]_ _[τ][k][j]_

+ τkj # _n = m[(]t[j][)]_ + 1, . . ., t : τkd _n > Nkd_ (n 1) for some d = 1, . . ., j 1
_−_ _−_

+τkj #n = m[(]t[j][)] + 1, . . ., t : τkd _n_ _Nkd_ (n 1) for all d = 1, . . ., j 1,
_≤_ _−_ _−_
 arm kj not pulled at time n

(1 _τkj_ )# _n = m[(]t[j][)]_ + 1, . . ., t : τkd _n_ _Nkd_ (n 1) for all d = 1, . . ., j 1,
_−_ _−_ _≤_ _−_ _−_
 arm kj pulled at time n


=τkj + τkj # _n = m[(]t[j][)]_ + 1, . . ., t : τkd _n > Nkd_ (n − 1) for some d = 1, . . ., j − 1

(1 _τkj_ )#n = m[(]t[j][)] + 1, . . ., t : τkd _n_ _Nkd_ (n 1) for all d = 1, . . ., j 1
_−_ _−_ _≤_ _−_ _−_

+# _n = m[(]t[j][)]_ + 1, . . ., t : τkd _n_ _Nkd_ (n 1) for all d = 1, . . ., j 1,
_≤_ _−_ _−_
 arm kj not pulled at time n

=τkj + # _n = m[(]t[j][)]_ + 1, . . ., t : τkd _n > Nkd_ (n − 1) for some d = 1, . . ., j − 1

+# _n = m_ [(]t[j][)] + 1, . . ., t : τkd _n_ _Nkd_ (n 1) for all d = 1, . . ., j 1,
_≤_ _−_ _−_
 arm kj not pulled at time n

_−(1 −_ _τkj_ )(t − _m[(]t[j][)][)][.]_

The final bound is, clearly, also valid in the case m[(]t[j][)] = t.

Next,


# _n = m[(]t[j][)]_ + 1, . . ., t : τkd _n > Nkd_ (n 1) for some d = 1, . . ., j 1 (26)
_−_ _−_

_j−1_ 

= # _n = m[(]t[j][)]_ + 1, . . ., t : τkd _n > Nkd_ (n 1), τkm _n_ _Nkm_ (n 1), m = 1, . . ., d 1 _._

_−_ _≤_ _−_ _−_
_d=1_

X 

For d = 1, . . ., j − 1 denote

_m[(]t[j,d][)]_ = sup _n = m[(]t[j][)][, . . ., t][ :][ τ][k]d_ _[n > N][k]d_ [(][n][ −] [1)] _._ (27)



-----

Suppose, for a moment, that m[(]t[j,d][)] _> m[(]t[j][)][. Then]_

0 < τkd _m[(]t[j,d][)]_ _Nkd_ _m[(]t[j,d][)]_ 1 = τkd _m[(]t[j][)]_ _Nkd_ _m[(]t[j][)]_ 1
_−_ _−_ _−_ _−_

+τkd  m[(]t[j,d][)] _−_ _m[(]t[j][)]_ _−_ # _n = m[(]t[j][)]_ + 1  _, . . ., m[(]t[j,d][)]_ : arm kd pulled
 


_−(1 −_ _τkd_ )# _n = m[(]t[j][)]_ + 1, . . ., m[(]t[j,d][)] : arm kd pulled

=τkd _m[(]t[j][)]_ _Nkd_ _m[(]t[j][)]_ 1 + τkd _m[(]t[j,d][)]_ _m[(]t[j][)]_
_−_ _−_ _−_

_−#_ _n = m[(]t[j][)]_ + 1  _, . . ., m[(]t[j,d][)]_ : arm  _kd pulled_ _._ 
We conclude that

# _n = mt[(][j][)]_ + 1, . . ., m[(]t[j,d][)] : arm kd pulled



+ [+][ τ][k][d] _m[(]t[j,d][)]_ _m[(]t[j][)]_
_−_
 


_≤_ _n=1max,...,t_ _τkd_ _n −_ _Nkd_ (n)
 


Therefore,

# _n = m[(]t[j][)]_ + 1, . . ., t : τkd _n > Nkd_ (n 1), τkmn _Nkm(n_ 1), m = 1, . . ., d 1
_−_ _≤_ _−_ _−_

=#n = m[(]t[j][)] + 1, . . ., m[(]t[j,d][)] : τkd _n > Nkd_ (n 1), τkmn _Nkm_ (n 1), m = 1, . . ., d 1
_−_ _≤_ _−_ _−_

_≤#n = m[(]t[j][)]_ + 1, . . ., m[(]t[j,d][)] : arm kd pulled

+#n = m[(]t[j][)] + 1, . . ., t : τkd _n > Nkd_ (n 1), τkmn _Nkm(n_ 1), m = 1, . . ., d 1,
_−_ _≤_ _−_ _−_
 arm kd not pulled


+ [+][ τ][k][d] _t_ _m[(]t[j][)]_
_−_
 


_≤_ _n=1max,...,t_ _τkd_ _n −_ _Nkd_ (n)
 


+# _n = m[(]t[j][)]_ + 1, . . ., t : τkd _n > Nkd_ (n 1), τkmn _Nkm(n_ 1), m = 1, . . ., d 1,
_−_ _≤_ _−_ _−_
 arm kd not pulled

and the final bound is clearly valid even if m[(]T[j,d][)] = m[(]T[j][)][. Substituting this bound into equation 26]
we obtain
# _n = m[(]t[j][)]_ + 1, . . ., t : τkd _n > Nkd_ (n 1) for some d = 1, . . ., j 1
_−_ _−_
 _[j][−][1]_ _j−1_

_≤_ _t −_ _m[(]t[j][)]_ _τkd +_ _t[′]=1max,...,t_ _τkd_ _t[′]_ _−_ _Nkd_ (t[′]) +

_d=1_ _d=1_

 j−1  X X   

+ # _n = m[(]t[j][)]_ + 1, . . ., t : τkd _n > Nkd_ (n 1), τkm _n_ _Nkm_ (n 1), m = 1, . . ., d 1,

_−_ _≤_ _−_ _−_
_d=1_

X 


arm kd not pulled
Substituting this bound into equation 25 gives us
_τkj_ _t_ _Nkj_ (t) + (28)
 j−1 _−_  _[≤]_ _[τ][k][j]_ _j_

+ _d=1_ _t[′]=1max,...,t_ _τkd_ _t[′]_ _−_ _Nkd_ (t[′]) + _[−]_ _t −_ _m[(]t[j][)]_ 1 − _d=1_ _τkd_ !

Xj       X

+ # _n = m[(]t[j][)]_ + 1, . . ., t : τkd _n > Nkd_ (n 1),

_−_
_d=1_

X 


_τkmn ≤_ _Nkm_ (n − 1), m = 1, . . ., d − 1, arm kd not pulled

_j−1_

_t[′]=1max,...,t_ _τkd_ _t[′]_ _−_ _Nkd_ (t[′]) +
_d=1_

X   


_≤τkj +_


# _n = 1, . . ., t : τkd_ _n > Nkd_ (n 1),
_−_
_d=1_

X 

_τkmn ≤_ _Nkm_ (n − 1), m = 1, . . ., d − 1, arm kd not pulled


-----

Taking the maximum over t on both sides of above inequality, we then have


_j−1_

_t=1max,...,T_ _τkd_ _t −_ _Nkd_ (t)
_d=1_

X  


(29)

(30)


_t=1max,...,T_ _τkj_ _t −_ _Nkj_ (t)
 


+

_[≤]_ _[τ][k][j][ +]_


# _n = 1, . . ., T : τkd_ _n > Nkd_ (n 1),
_−_
_d=1_

X 


_τkm_ _n ≤_ _Nkm(n −_ 1), m = 1, . . ., d − 1, arm kd not pulled


Therefore, we arrive at


E _t=1max,...,T_ _τkj_ _t −_ _Nkj_ (t) + (30)
 

_j_  1 
_−_

_≤τkj + E_ _d=1_ _t=1max,...,T_ _τkd_ _t −_ _Nkd_ (t) +!

_j_ _TX_   

+ E **1** _τkd_ _n > Nkd_ (n − 1),

_d=1_ n=1

X X  

_τkm_ _n ≤_ _Nkm(n −_ 1), m = 1, . . ., d − 1, arm kd not pulled _._
[]

We will prove that for kd opt cr
_∈A_ _∪A_

_T_
E **1** _τkd_ _n > Nkd_ (n − 1), (31)
n=1
X  

_τkmn ≤_ _Nkm(n −_ 1), m = 1, . . ., d − 1, arm kd not pulled

_≤bd log T + O(1)_ []

forfor d bd = 1 > 0, . . ., j that we will compute. It is elementary that − 1. Therefore, it will follow from equation 31, equation 30 and a simple inductive kj ∈Aopt ∪Acr implies kd ∈Aopt ∪Acr
argument that for any kj opt cr,
_∈A_ _∪A_


_≤_ _aj log T + O(1)_ (32)


_j_

_bd,_

_d=1_

X


_t=1max,...,T_ _τkj_ _t −_ _Nkj_ (t)
 


with a1 = b1 and for j > 1,

which means that

We now prove equation 31. We have


_j−1_

_ad +_
_d=1_

X


_aj =_


_aj =_ (j − _d + 1)bd._ (33)

_d=1_

X


_T_

_Eπ_ **1** _τkd_ _n > Nkd_ (n − 1), τkmn ≤ _Nkm_ (n − 1), m = 1, . . ., d − 1, arm kd not pulled

n=1

_d−1_ X _T _ []

_Eπ_ **1** _τkd_ _n > Nkd_ (n − 1), τkm _n ≤_ _Nkm_ (n − 1), arm km is pulled at time n
_m=1_ n=1

XK X T   []

_Eπ_ **1** _τkd_ _n > Nkd_ (n − 1), arm km is pulled at time n _._
_m=d+1_ n=1

X X   []


-----

Observe that a “no-tie” assumption imposed at the beginning of the section implies that

_µkd + Akd > µ_ _µkm._
_∗_ _≥_

Therefore, we can use once again the usual UCB-type argument to see that for any m = 1, . . ., d−1,
for any B > 0,


_T_

**1** _τkd_ _n > Nkd_ (n − 1), τkmn ≤ _Nkm(n −_ 1), arm km is pulled at time n

n=1
X   _T_ []


_Eπ_


_≤B log T +_

_≤B log T +_

_≤B log T +_


_Pπ_ _Nkm(n_ 1) > B log T,
_−_
_n=1_

X  

_τkd_ _n > Nkd_ (n − 1), τkm _n ≤_ _Nkm_ (n − 1), arm km is pulled at time n

_T_

_Pπ_ _Nkm(n_ 1) > B log T,
_−_
_n=1_

X  

_τkd_ _n > Nkd_ (n 1), τkm _n_ _Nkm_ (n 1), ikm(n) _ikd_ (n)
_−_ _≤_ _−_ _≥_



2 log n

_Nkm(n_ 1)
_−_


_Nkm_ (n 1) > B log T, ˆmkm (n 1) +
_−_ _−_


_Pπ_
_n=1_

X


2 log n

_Nkd_ (n 1)
_−_


_≥_ _mˆ_ _kd_ (n − 1) + Akd +

8

(µkd + Akd − _µkm_ )[2][,]


By carefully choosing


_B =_


we obtain the bound

_T_

_Eπ_ **1** _τkd_ _n > Nkd_ (n − 1), τkmn ≤ _Nkm(n −_ 1), arm km is pulled at time n (34)

n=1
X   []

8
_≤_ (µkd + Akd − _µkm)[2][ log][ T][ +][ O][(1)][,]_

_m = 1, . . ., d −_ 1. The same argument shows that for every m = d + 1, . . ., K,

_T_

_Eπ_ **1** _τkd_ _n > Nkd_ (n − 1), arm km is pulled at time n (35)

n=1
X   []

8


_≤_ (µkd + Akd − _µkm −_ _Akm_ )[2][ log][ T][ +][ O][(1)][.]

Now equation 34 and equation 35 imply equation 31 with


_d−1_

_m=1_

X


8 8

_bd =_ (36)

_m=1_ (µkd + Akd _µkm_ )[2][ +] _m=d+1_ (µkd + Akd _µkm_ _Akm)[2][ .]_

X _−_ X _−_ _−_

Now it follows from equation 36 and equation 33 that for every j such that kj opt cr,
_∈A_ _∪A_

_j_ _d−1_ 1 _K_ 1

_aj = 8_ (j _d + 1)_ _._

_d=1_ _−_ _m=1_ (µkd + Akd _µkm)[2][ +]_ _m=d+1_ (µkd + Akd _µkm_ _Akm_ )[2] !

X X _−_ X _−_ _−_

(37)
We conclude by equation 32 that every j such that kj opt cr,
_∈A_ _∪A_

_Eπ_ _τkj_ _T_ _Nkj_ (T ) + (38)
_−_ _[≤]_ _[a][j][ log][ T][ +][ O][(1)][,]_

with aj given in equation 37.   

_AjRemark1k ̸j=2 + j2 µ ∈. In the proof, we assume that there is no tie, i.e.,kj2[ for someK]. This assumption is not restrictive since the probability that event “ j1 ̸= j2 ∈_ [K].” is zero when we pick penalty rates Akj1 + µkj A1 ̸k=’s uniformly randomly. Akj2 +Ak µj1k +j2 for any µkj1 ̸=


_bd =_


-----

E PROOF OF GAP-INDEPENDENT UPPER BOUNDS

**Proof of Lemma 1 We first prove that the algorithm pulls arm k[′]** with Ak′ ∆k′ 2 2 logT [2][/] T[3][ at]
_−_ _≤_

time n when Nk′ (n) _T_ [2][/][3] and Nk(n) < τkn with vanishing probability. This is becauseq
_≥_


2 log n
_Nk(n)_

_[≤]_ _[A][k][′][ +][ m][k][′]_ [(][n][) +]


2 log n

_Nk′_ (n) [)]


P(Ak + mk(n) +


2 log n

_Nk′_ (n) [) + 2]n[2]

2 log n

s _Nk′_ (n) [) + 2]n[2]


P(Ak + µk ≤ _A[′]k_ [+][ µ]k[′] [+ 2]s

P(Ak ∆k _A[′]k_
_−_ _≤_ _[−]_ [∆][k][′][ + 2]


= 2/n[2]. (39)

Next, we say arm k is a very critical arm if arm k satisfies Ak ∆k 2 2 logT [2][/] T[3][ . Otherwise][ k]
_−_ _≥_

is a non-very critical arm. In other words, each non-very critical arm can be only played at mostq
_O(T_ [2][/][3]) times with high probability.

Furthermore, we can show that Nk′ (n) _T_ [2][/][3] at time n = c0T/2 for each very critical arm k[′].
_≥_
If not, note that T [2][/][3] _≤_ _τk′_ _c[2]0[T/][4][, then][ N][ ′]k[(][n][)][ < τ]k[ ′]_ _[n][ for any][ n][ ∈{⌈][c]0[2][T/][4][⌉][, . . .,][ ⌊][c][0][T/][2][⌋}][.]_
Hence, for any arm k[′′] can be played at most max{τk[′′] _c0T/2, T_ [2][/][3]} times between rounds c[2]0[T/][4]
and c0T/2. Then, we must have


_c0T/2 −_ _c[2]0[T/][4][ ≤]_


_T_ [2][/][3].


_τkc0T/2 +_


However, the above inequality fails to hold when T ≥ (4K/c[2]0[)][3][. This leads to the contradiction.]
Thus, we have Nk′ (n) _T_ [2][/][3] for any very critical arm k[′] at time n = c0T/2.
_≥_

This further gives us that we must have Nk′ (n) _τk′_ _n_ for all very critical arms at some time
_≥⌊_ _⌋_
_n_ [c0T, T ]. To prove this, we observe the fact that for any arm _k[¯], it will be played with probability_
less than ∈ _T2[2][ at time][ n][ once][ N]k[¯][(][n][)][ ≥]_ [max][{][τ][k][n, T][ 2][/][3][}][ and one critical arm][ k][′][ is played less than]

_τk′_ _n. (In other words, this tells us that once arm_ _k[¯] has been played max_ _τk¯[n, T][ 2][/][3][}][ times, then it]_
_{_
can only be played at time when all very critical arms k[′]s have been played for τk[′] _n times or ⌊τk¯[n][⌋]_
jumps by one with probability greater than 1 − 2/n[2].)

Let n1( _c0T/2) be the first ever time such that Nk′_ (n1) _τk′_ _n1_ . By straightforward calcula_≥_ _≥⌊_ _⌋_
tion, it gives that n1 must be bounded by


_T_ [2][/][3] + (
_k[′′]_ :non-very critical

X


_n1_ _c0T/2 +_
_≤_


_τk′_ )T (c0 +
_≤_
_k[′]_

X


_τk′_ )T
_k[′]_

X


with probability greater than 1 − 2K/T .

That is, n1 is well defined between c0T/2 and T . At time n1, we have all very critical arms k[′]
such that Nk′ (n1) _τk′_ _n1. Therefore, starting from time n1, the maximum difference between any_
_≥_
non-fairness level (τk′ _n_ _Nk′_ (n))+’s with k[′] in the set of very-critical arms is always bounded by
_−_
1 with probability 1 2K/T for all n [n1, T ].
_−_ _∈_

Lastly, suppose n2 be the last time that arm k is above fairness level. We know at time n = n2, each
very critical arm k[′] is played for at least τk′ _n2_ 1. by previous argument. Then in the remaining
_T −_ _n2 rounds, we know that each very critical arm is played at most −_ _τk′_ _T −_ _τk′_ _n2 + 1. Then we_
must have
_T_ _n2_ ( _τk′_ )(T _n2) + K +_ _T_ [2][/][3],
_−_ _≤_ _k[′]:very critical_ _−_ _k:non-very critical_
X X

which implies T _n2_ (KT [2][/][3] + K)/c0. This finally implies that Nk(T ) _Nk(n2)_ _τkT_
_−_ _≤_ _≥_ _≥_ _−_
_τk(KT_ [2][/][3] + K)/c0 − 1 with probability at least 1 − 2K/T . That is, E[(τkT − _Nk(T_ ))+] =
_τk(KT_ [2][/][3] + K)/c0 + 1 = O(τkKT [2][/][3]).


-----

We prove the gap-independent upper bound (Theorem 4) by considering the following situations.


**Situation 1.a For arm k ∈Anon-cr and ∆k ≤** 4


log T


, the regret on arm k is upper bounded by


(∆k _Ak)(τkT_ _Nk(T_ )) (40)
_−_ _−_

if 0 _Nk(T_ ) _τkT_ ; or bounded by
_≤_ _≤_

∆k(Nk(T ) _τkT_ ) + (∆k _Ak)τkT_ (41)
_−_ _−_

if Nk(T ) _τkT_ .
_≥_


**Situation 1.b For arm k ∈Anon-cr and ∆k > 4**


log T



-  if ∆k − _Ak > 4_

-  if ∆k _Ak_ 4
_−_ _≤_


log T/τkT the regret on arm k is upper bounded by

8 log T
(∆k _Ak)(_ (42)
_−_ (∆k _Ak)[2][ +][ O][(1))][.]_

_−_

log T/τkT the regret on arm k is upper bounded by


(∆k _Ak)τkT + ∆k[(_ [8 log][ T] _τkT_ )+ + O(1)]. (43)
_−_ ∆[2]k _−_

In other words, for any arm k ∈Anon-cr, its regret is always bounded by


log T


log T


(Nk(T ) _τkT_ )+. (44)
_−_


_τkT + 4_


_τkT log T + 4_


**Situation 2 We then split set Aopt ∪Acr into two subsets, Acr, large and Acr,small, where**


2 log T

_T_ [2][/][3][ }]

2 log T

_T_ [2][/][3][ }][.]


_Acr, large := {k : Ak −_ ∆k > 4

_Acr, small := {k : Ak −_ ∆k ≤ 4


and


For arm k ∈Acr,large, we have E[(τkT − _Nk(T_ ))+] = O(τkKT [2][/][3]) by Lemma 1. The regret on
arm k is then bounded by

∆kE[Nk(T ) − _τkT_ ] + O(AkτkKT [2][/][3])

∆k min _τkT, Nk(T_ ) _τkT_ + O(AkτkKT [2][/][3]). (45)
_≤_ _{_ [8 log]∆[2]k[ T] _−_ _−_ _}_


For arm k ∈Acr,small, the regret on arm k is then bounded by

(Ak ∆k)(τkT _Nk(T_ )) 4τkT [2][/][3](log T )[1][/][2] (46)
_−_ _−_ _≤_

if 0 _Nk(T_ ) _τkT_, or
_≤_ _≤_


∆k min _τkT + O(1), Nk(T_ ) _τkT_ (47)
_{_ [8 log]∆[2]k[ T] _−_ _−_ _}_

if Nk(T ) _τkT_ .
_≥_

In summary, for any arm k ∈Aopt ∪Acr,

∆k min _τkT, Nk(T_ ) _τkT_ + O(max _AkτkKT_ [2][/][3], 4τkT [2][/][3](log T )[1][/][2] ). (48)
_{_ [8 log]∆[2]k[ T] _−_ _−_ _}_ _{_ _}_


-----

Combining above situations, the total regret is upper bounded by


log T

max 8 _τkT log T + 4_ (Nk(T ) _τkT_ )+
_{_ _T_ _−_ _}_
_k∈AXnon-cr_ p r

+ ∆k min _τkT, Nk(T_ ) _τkT_ + O(max _AkτkKT_ [2][/][3], 4τkT [2][/][3](log T )[1][/][2] )

_k∈AXopt∪Acr_ _{_ [8 log]∆[2]k[ T] _−_ _−_ _}_ _{_ _}_


_√τk) + AmaxKT 2/3(log T_ )1/2( _τk) + 4_

_k∈AXcr∪Aopt_


log T


_T log T_ (

_k∈AXnon-cr_


(Nk(T ) _τkT_ )+
_−_
_k∈AXnon-cr_


8(Nk(T ) − _τkT_ )+ log T

_√τk) + AmaxKT 2/3(log T_ )1/2 + 4


_k∈Aopt∪Acr_

_T log T_ (

p


log T


(1 _τmin)T +_
_−_


8 log T


_KT_ (1 _τmin)_
_−_

(49)


where 49 uses the fact that _k_ cr _opt_ _[τ][k][ ≤]_ [P]k _[τ][k][ ≤]_ [1][;][ P]k non-cr[(][N][k][(][T] [)][ −] _[τ][k][T]_ [)][+][ ≤] _[T]_ [(1][ −]

_∈A_ _∪A_ _∈A_

_τmin) and_

[P]


(Nk(T ) _τkT_ )+
_−_ _≤_


(Nk(T ) _τkT_ )+
_−_ _≤_


(Nk(T ) _τkT_ )+
_−_ _≤_


_KT_ (1 _τmin)_
_−_


_k∈Aopt∪Acr_


by Jenson’s inequality.

F PROOF OF GAP-INDEPENDENT LOWER BOUNDS

Consider a K-arm setting with µ2 = µ3 = . . . = µK = 0, µ1 = ∆ (0 < ∆ _< 1/2),_
_A1, A2, . . ., AK > 0, ∆_ _< Ak for k ∈_ [K], τ1, τ2, . . ., τK ∈ [0, 1].

Since _k=2_ _[N][k][(][T]_ [)] _≤_ _T_, then it holds Eπ[Nk1 (T )] _≤_ _T/(K −_ 1) with k1 =
arg mink>1 Eπ[Nk(T )] for any policy π. We then construct another K-arm setting with µk1 = 2∆
and all other parameters remain the same.

[P][T]

For policy π, the regret of the first setting is


_R1,π(T_ ) ≥ _AE[(τ1T −_ _N1(T_ ))+] + {∆E[Nk1 (T ) − _τk1_ _T_ ] + AE(τk1 _T −_ _Nk1_ (T ))+}

and the regret of the second setting is

_R2,π(T_ ) ≥ _AE[(τk1_ _T −_ _Nk1_ (T ))+] + {∆E[N1(T ) − _τ1T_ ] + AE(τ1T − _N1(T_ ))+}

If N1(T ) < (1 + τ1 _τk1_ )T/2, then R1,π(T ) ∆ 1−τ12−τk1 _T_ . While N1(T ) > (1 + τ1 _τk1_ )/2,

then R2,π(T ) ∆ 1 −−τ12−τk1 _T_ . In other words, for policy ≥ _π,_ _−_
_≥_


1
worst regret
_≥_ 2 [(][R][1][,π][(][T] [) +][ R][2][,π][(][T] [))]

1

P(N1(T ) < [1 +][ τ][1][ −] _[τ][k][1]_ ) + ∆T [1][ −] _[τ][1][ −]_ _[τ][k][1]_ P(N1(T ) ))

_≥_ 2 [(∆][T][ 1][ −] _[τ][1]2[ −]_ _[τ][k][1]_ 2 2 _≥_ [1 +][ τ][1]2[ −] _[τ][k][1]_

(1 − _τ1 −_ _τk1_ )∆T exp _KL(P1_ _P2)_ (50)
_≥_ 8 _{−_ _∥_ _}_

(1 − _τ1 −_ _τk1_ )∆T exp _CT_ ∆[2]/(K 1) _,_ (51)
_≥_ 8 _{−_ _−_ _}_


where P1 and P2 are two probability distributions under two settings associated with policy
_π; 50 follows from the Bretagnolle–Huber inequality. Inequality 51 holds since KL-divergence_
_KL(P1_ _P2)_ _CT_ ∆[2]/(K 1) for many probability distributions. (E.g. C = 1/2 if the reward of
_∥_ _≤_ _−_
each arm follows Gaussian distribution with variance 1.)


-----

_KCT−1_ [, we have]

q

(1 _τ1_ maxk=1 τk)∆T
worst regret _−_ _−_ _̸_
_≥_


Taking ∆=


exp{−CT ∆[2]/(K − 1)}


(1 2 maxk τk) (K 1)T/C
_−_ _−_ _,_ (52)
_≥_ 8e
p

where e = exp{1}. This completes the proof of Theorem 7.


-----

