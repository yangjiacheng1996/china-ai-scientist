# NEURAL CONTEXTUAL BANDITS WITH DEEP REPRE## SENTATION AND SHALLOW EXPLORATION


**Pan Xu**
California Institute of Technology
panxu@caltech.edu

**Quanquan Gu**
University of California, Los Angeles
qgu@cs.ucla.edu


**Zheng Wen**
DeepMind
zhengwen@google.com

ABSTRACT


**Handong Zhao**
Adobe Research
hazhao@adobe.com


We study neural contextual bandits, a general class of contextual bandits, where
each context-action pair is associated with a raw feature vector, but the specific reward generating function is unknown. We propose a novel learning algorithm that
transforms the raw feature vector using the last hidden layer of a deep ReLU neural network (deep representation learning), and uses an upper confidence bound
(UCB) approach to explore in the last linear layer (shallow exploration). We prove
that under standard assumptions, our proposed algorithm achieves _O(√T_ ) finite
time regret, where T is the learning time horizon. Compared with existing neural
contextual bandit algorithms, our approach is computationally much more effi
[e]

cient since it only needs to explore in the last layer of the deep neural network.

1 INTRODUCTION

Multi-armed bandits (MAB) (Auer et al., 2002; Audibert et al., 2009; Lattimore & Szepesv´ari, 2020)
are a class of online decision-making problems where an agent needs to learn to maximize its expected cumulative reward by repeatedly interacting with a partially known environment. Following
a bandit algorithm (also called a strategy or policy), in each round, the agent adaptively chooses
an arm, and then receives a reward associated with that arm. Since only the reward of the chosen
arm will be observed (bandit information feedback), a good bandit algorithm has to deal with the
exploration-exploitation dilemma: trade-off between pulling the best arm based on existing knowledge/history data (exploitation) and trying the arms that have not been fully explored (exploration).

In many real-world applications, the agent will also be able to access detailed contexts associated
with the arms. For example, when a company wants to choose an advertisement to present to a user,
the recommendation will be much more accurate if the company takes into consideration the contents, specifications, and other features of the advertisements in the arm set as well as the profile of
the user. To encode the contextual information, contextual bandit models and algorithms have been
developed, and widely studied both in theory and in practice (Dani et al., 2008; Rusmevichientong &
Tsitsiklis, 2010; Li et al., 2010; Chu et al., 2011; Abbasi-Yadkori et al., 2011). Most existing contextual bandit algorithms assume that the expected reward of an arm at a context is a linear function in
a known context-action feature vector, which leads to many useful algorithms such as LinUCB (Chu
et al., 2011), OFUL (Abbasi-Yadkori et al., 2011), etc. The representation power of the linear model
can be limited in applications such as marketing, social networking, clinical studies, etc., where the
rewards are usually counts or binary variables. The linear contextual bandit problem has also been
extended to richer classes of parametric bandits such as the generalized linear bandits (Filippi et al.,
2010; Li et al., 2017) and kernelised bandits (Valko et al., 2013; Chowdhury & Gopalan, 2017).

With the prevalence of deep neural networks (DNNs) and their phenomenal performances in many
machine learning tasks (LeCun et al., 2015; Goodfellow et al., 2016), there has emerged a line
of work that employs DNNs to increase the representation power of contextual bandit algorithms
(Allesiardo et al., 2014; Riquelme et al., 2018; Collier & Llorens, 2018; Zahavy & Mannor, 2019;


-----

Zhou et al., 2020; Deshmukh et al., 2020; Zhang et al., 2020). The problems they solve are usually
referred to as neural contextual bandits. For example, Zhou et al. (2020) developed the NeuralUCB
algorithm, which can be viewed as a natural extension of LinUCB (Chu et al., 2011; Abbasi-Yadkori
et al., 2011), where they use the output of a deep neural network with the feature vector as input
to approximate the reward. Zhang et al. (2020) adapted neural networks in Thompson Sampling
(Thompson, 1933; Chapelle & Li, 2011; Russo et al., 2018) for both exploration and exploitation
and proposed NeuralTS . For a fixed time horizon T, it has been proved that both NeuralUCB
and NeuralTS achieve a O(d√T ) regret bound, where _d is the effective dimension of a neural_

tangent kernel matrix which can potentially scale with O(TK) for K-armed bandits. This high
complexity is mainly due to that the exploration is performed over the entire huge neural network

[e] [e]

parameter space, which is inefficient and even infeasible when the number of neurons is large. A
more realistic and efficient way of learning neural contextual bandits may be to just explore different
arms using the last layer as the exploration parameter. More specifically, Riquelme et al. (2018)
provided an extensive empirical study of benchmark algorithms for contextual-bandits through the
lens of Thompson Sampling, which suggests decoupling representation learning and uncertainty
estimation improves performance.

In this paper, we show that the decoupling of representation learning and the exploration can be
theoretically validated. We study a new neural contextual bandit algorithm, which learns a mapping to transform the raw features associated with each context-action pair using a deep neural
network (deep representation), and then performs an upper confidence bound (UCB)-type exploration over the linear output layer of the network (shallow exploration). We prove a sublinear regret
of the proposed algorithm by exploiting the UCB exploration techniques in linear contextual bandits (Abbasi-Yadkori et al., 2011) and the analysis of deep overparameterized neural networks using
neural tangent kernels (Jacot et al., 2018). Our theory confirms the empirically observed effectiveness of decoupling the deep representation learning and the UCB exploration in contextual bandits
(Riquelme et al., 2018; Zahavy & Mannor, 2019).

**Contributions we summarize the main contributions of this paper as follows.**

_• We propose a contextual bandit algorithm, Neural-LinUCB, for solving a general class of con-_
textual bandit problems without knowing the specific reward generating function. The proposed
algorithm learns a deep representation to transform the raw feature vectors and performs UCBtype exploration in the last layer of the neural network, which we refer to as deep representation
and shallow exploration. Compared with LinUCB (Li et al., 2010; Chu et al., 2011) and neural
bandits such as NeuralUCB (Zhou et al., 2020) and NeuralTS (Zhang et al., 2020), our algorithm enjoys the best of two worlds: strong expressiveness due to the deep representation and
computational efficiency due to the shallow exploration.

_• Despite the usage of a DNN as the feature mapping, we prove a_ _O(√T_ ) regret for the proposed

Neural-LinUCB algorithm, which matches the regret bound of linear contextual bandits (Chu
et al., 2011; Abbasi-Yadkori et al., 2011). To the best of our knowledge, this is the first work

[e]

that theoretically shows the convergence of bandits algorithms under the scheme of deep representation and shallow exploration. It is notable that a similar scheme called Neural-Linear was
proposed by Riquelme et al. (2018) for Thompson sampling algorithms, and they empirically
showed that decoupling representation learning and uncertainty estimation improves the performance. Our work confirms this observation from a theoretical perspective.

_• We conduct experiments on contextual bandit problems based on real-world datasets, demon-_
strating a better performance and computational efficiency of Neural-LinUCB over LinUCB and
existing neural bandits algorithms such as NeuralUCB, which well aligns with our theory.

1.1 ADDITIONAL RELATED WORK

There is a line of related work to ours on the recent advance in the optimization and generalization
analysis of deep neural networks. In particular, Jacot et al. (2018) first introduced the neural tangent
kernel (NTK) to characterize the training dynamics of network outputs in the infinite width limit.
From the notion of NTK, a fruitful line of research emerged and showed that loss functions of deep
neural networks trained by (stochastic) gradient descent can converge to the global minimum (Du
et al., 2019b; Allen-Zhu et al., 2019b; Du et al., 2019a; Zou et al., 2018; Zou & Gu, 2019). The
generalization bounds for overparameterized deep neural networks are also established in Arora
et al. (2019a;b); Allen-Zhu et al. (2019a); Cao & Gu (2019a;b). Recently, the NTK based analysis


-----

is also extended to the study of sequential decision problems including bandits (Zhou et al., 2020;
Zhang et al., 2020), and reinforcement learning algorithms (Cai et al., 2019; Liu et al., 2019; Wang
et al., 2020; Xu & Gu, 2020).

Our algorithm is also different from Langford & Zhang (2008); Agarwal et al. (2014) which reduce
the bandit problem to supervised learning. Moreover, their algorithms need to access an oracle that
returns the optimal policy in a policy class given a sequence of context and reward vectors, whose
regret depends on the VC-dimension of the policy class.

**Notation We use [k] to denote a set {1, . . ., k}, k ∈** N[+]. ∥x∥2 = _√x[⊤]x is the Euclidean norm_

of a vector x ∈ R[d]. For a matrix W ∈ R[m][×][n], we denote by ∥W∥2 and ∥W∥F its operator norm
and Frobenius norm respectively. For a semi-definite matrix A ∈ R[d][×][d] and a vector x ∈ R[d], we
denote the Mahalanobis norm as ∥x∥A = _√x[⊤]Ax. Throughout this paper, we reserve the notations_

_{Ci}i=0,1,... to represent absolute positive constants that are independent of problem parameters_
such as dimension, sample size, iteration number, step size, network length and so on. The specific
values of {Ci}i=0,1,... can be different in different context. For a parameter of interest T and a
function f (T ), we use notations such as O(f (T )) and Ω(f (T )) to hide constant factors and _O(f_ (T ))
to hide constant and logarithmic dependence of T .

[e]

2 PRELIMINARIES

In this section, we provide the background of contextual bandits and deep neural networks.

2.1 LINEAR CONTEXTUAL BANDITS


A contextual bandit is characterized by a tuple (S, A, r), where S is the context (state) space, A is
the arm (action) space, and r encodes the unknown reward generating function at all context-arm
pairs. A learning agent, who knows S and A but does not know the true reward r (values bounded
in (0, 1) for simplicity), needs to interact with the contextual bandit for T rounds. At each round
_t = 1, . . ., T_, the agent first observes a context st chosen by the environment; then it needs to
adaptively select an arm at based on its past observations; finally it receives a reward ∈S
_∈A_

_rt(xs,at_ ) = r(xs,at ) + ξt, (2.1)

wherenoise with zero mean. The agent’s objective is to maximize its expected total reward over these xs,a ∈ R[d] is a known feature vector for context-arm pairb (s, a) ∈S × A, and ξt is a random T
rounds, which is equivalent to minimizing the pseudo regret (Audibert et al., 2009):

_T_
_RT = E_ _r(xst,a[∗]t_ [)][ −] _r[b](xst,at_ ) _,_ (2.2)
 _t=1_
X   []

where a[∗]t _a_ _t[,a][) =][ E][[]r[b](xst,ab_ )] . To simplify the exposition, we use xt,a to denote
**xst,a since it only depends on the round index[∈]** [argmax] _∈A[{][r][(][x][s]_ _t in most bandit problems, and we assume}_ _A = [K]._

In linear contextual bandits, the reward function in (2.1) is assumed to have a linear structure
_r(xs,a) = x[⊤]s,a[θ][∗]_ [for some unknown weight vector][ θ][∗] _[∈]_ [R][d][. One provably sample efficient al-]
gorithm for linear contextual bandits is Linear Upper Confidence Bound (LinUCB) (Chu et al.,
2011) or Optimism in the Face of Uncertainty Linear bandit algorithm (OFUL) (Abbasi-Yadkori
et al., 2011). Specifically, at each round t, LinUCB chooses the action at = argmaxa∈[K]{x[⊤]t,a[θ][t] [+]
is a matrix defined based on the historical context-arm pairs, andαt∥xt,a∥A−t 1 _[}][, where][ θ][t][ is a point estimate of][ θ][∗][,][ A][t][ =][ λ][I][ +][ P] αi[t]t=1 > 0[x][i,a] is a tuning parameter thati_ **[x][⊤]i,ai** [with some][ λ >][ 0]
controls the exploration rate in LinUCB.

2.2 DEEP NEURAL NETWORKS


In this paper, we use f (x) to denote a neural network with input data x ∈ R[d]. Let L be the number
of hidden layers andm1 = . . . = mL 1 = W ml ∈ andR m[m][l]0[×] =[m] m[l][−][1]Lbe the weight matrices in the = d. Then a L-hidden layer neural network is defined as l-th layer, where l = 1, . . ., L,
_−_

_f_ (x) = _[√]mθ[∗⊤]σL(WLσL−1(WL−1 · · · σ1(W1x) · · · )),_ (2.3)


-----

where σl is an activation function and θ[∗] _∈_ R[d] is the weight of the output layer. To simplify the
presentation, we will assume σ1 = σ2 = . . . = σL = σ is the ReLU activation function, i.e.,
_σ(x) = max{0, x} for x ∈_ R. We denote w = (vec(W1)[⊤], . . ., vec(WL)[⊤])[⊤], which is the
concatenation of the vectorized weight parameters of all hidden layers of the neural network. We
also write f (x; θ[∗], w) = f (x) in order to explicitly specify the weight parameters of neural network
_f_ . It is easy to show that the dimension p of vector w satisfies p = (L − 2)m[2] + 2md. To simplify
the notation, we define φ(x; w) as the output of the L-th hidden layer of neural network f .

**_φ(x; w) =_** _[√]mσ(WLσ(WL−1 · · · σ(W1x) · · · ))._ (2.4)

Note that φ(x; w) itself can also be viewed as a neural network with vector-valued outputs.

3 DEEP REPRESENTATION AND SHALLOW EXPLORATION

3.1 HIGH-LEVEL IDEA OF THE PROPOSED ALGORITHM

The linear parametric form in linear contextual bandits might produce biased estimates of the reward
due to the lack of representation power (Snoek et al., 2015; Riquelme et al., 2018). In contrast, it
is well known that deep neural networks are powerful enough to approximate an arbitrary function
(Cybenko, 1989). Therefore, a natural extension of linear contextual bandits is to use a deep neural
network to approximate the reward generating function r(·). Nonetheless, DNNs usually have a prohibitively large dimension for weight parameters, which makes the exploration in neural networks
based UCB algorithm inefficient (Kveton et al., 2020; Zhou et al., 2020).

In this work, we study a neural contextual bandit algorithm, where the hidden layers of a deep neural
network are used to represent the features and the exploration is only performed in the last layer of
the neural network. In particular, for any arm feature vector x, we use ⟨θ[∗], φ(x; w)⟩ to approximate
the unknown reward function r(x), where φ(x; w) defined as in (2.4) is a neural network with
weight w, and θ[∗] is a unknown weight parameter. Note that we can also view ⟨θ[∗], φ(x; w)⟩ as a
neural network with φ(x; w) being the output of the last hidden layer and θ[∗] the weight parameter
of the last (linear) layer. Different from existing neural bandit algorithms, we only add a UCB bonus
term involving the last layer instead of all the weight parameter of this large neural network.

This decoupling of the representation and the exploration achieves the best of both worlds: efficient
exploration of shallow (linear) models and high expressive power of deep models. In what follows,
we will describe a neural contextual bandit algorithm that uses the output of the last hidden layer of
a neural network to transform the raw feature vectors (deep representation) and performs UCB-type
exploration in the last layer of the neural network (shallow exploration). Since the exploration is
performed only in the last linear layer, we call this procedure Neural-LinUCB, which is displayed
in Algorithm 1.

3.2 DETAILED IMPLEMENTATION OF DEEP REPRESENTATION AND SHALLOW EXPLORATION

Now we describe the details of Algorithm 1. In round t, the agent receives an action set with raw
features Xt = {xt,1, . . ., xt,K}. Then the agent chooses an arm at that maximizes the following
upper confidence bound:


_⟨φ(xt,k; wt−1), θt−1⟩_ + αt∥φ(xt,k; wt−1)∥A−t−11


_at = argmax_
_k∈[K]_


(3.1)


where θt 1 is a point estimate of the unknown weight in the last layer, φ(x; w) is defined as in (2.4),
_−_
**wt** 1 is an estimate of all the weight parameters in the hidden layers of the neural network, αt > 0
_−_
is the algorithmic parameter controlling the exploration, and At defined as follows.


**_φ(xi,ai_** ; wi−1)φ(xi,ai ; wi−1)[⊤], (3.2)
_i=1_

X


**At = λI +**


and λ > 0. After pulling arm at, the agent will observe a noisy reward _rt :=_ _r(xt,at_ ) = r(xt,k)+ξt,
where ξt is an independent ν-subGaussian random noise for some ν > 0 and r(·) is an unknown
reward function. In this paper, we will interchangeably use notation _rt to denote the reward received_
b b
at the t-th step and an equivalent notation _r(x) to express its dependence on the feature vector x._
b
b


-----

Upon receiving the reward _rt, the agent updates its estimate θt of the output layer weight by using_
the same ℓ[2]-regularized least-squares estimate in linear contextual bandits (Abbasi-Yadkori et al.,
2011). In particular, we have b θt = A[−]t [1][b][t][, where][ b][t] [=][ P][t]i=1 _riφ(xi,ai_ ; wi−1).

To save the computation, the neural network φ( ; wt) will be updated once every H steps. Therefore,

_·_
we have w(q 1)H+1 = . . . = wqH for q = 1, 2, . . .. We call the time steps[b] (q 1)H +1, . . ., qH
_−_ _{_ _−_ _}_
an epoch with length H. At time step t = Hq, we will retrain the neural network based on all the
historical data via Algorithm 2, which minimizes the following empirical loss function:

_qH_

2

_q(w) =_ **_θi[⊤][φ][(][x][i,a]i_** [;][ w][)][ −] _r[b]i_ _._ (3.3)
_L_

_i=1_

X   

In practice, one can further save computational cost by only feeding data **xi,ai** _,_ _ri, θi_ _i=(q_ 1)H+1
_{_ _}[qH]_ _−_
from the q-th epoch into Algorithm 2 to update the parameter wt, which does not hurt the performance since the historical information has been encoded into the estimate of θi. In this paper, we will b
perform the following gradient descent step wq[(][s][)] = wq[(][s][−][1)] _ηq_ **w** _q(w[(][s][−][1)]), for s = 1, . . ., n,_
_−_ _∇_ _L_
where wq[(0)] = w[(0)] is chosen as the same random initialization point. We will discuss more about
the initial point w[(0)] in the next paragraph. Then Algorithm 2 outputs wq[(][n][)] and we set it as the
updated weight parameter wHq+1 in Algorithm 1. In the next round, the agent will receive another
action set Xt+1 with raw feature vectors and repeat the above steps to choose the sub-optimal arm
and update estimation for contextual parameters.

**Initialization: Recall that w is the collection of all hidden layer weight parameters of the neural net-**
work. We will follow the same initialization scheme as used in Zhou et al. (2020), where each entry
of the weight matrices follows some Gaussian distribution. Specifically, for any l ∈{1, . . ., L − 1},

**W** **0**
we set Wl =, where each entry of W follows distribution N (0, 4/m) independently; for
**0** **W**
 

**WL, we set it as [V** **V], where each entry of V follows distribution N** (0, 2/m) independently.
_−_

**Comparison with LinUCB and NeuralUCB: Compared with linear contextual bandits in Sec-**
tion 2.1, Algorithm 1 has a distinct feature that it learns a deep neural network to obtain a deep
representation of the raw data vectors and then performs UCB exploration. This deep representation allows our algorithm to characterize more intrinsic and latent information about the raw data
**xt,k** _t_ [T ],k [K] R[d]. However, the increased complexity of the feature mapping φ( ; w) also
introduces great hardness in training. For instance, a recent work by{ _}_ _∈_ _∈_ _⊂_ Zhou et al. (2020) also stud-·
ied the neural contextual bandit problem, but different from (3.1), their algorithm (NeuralUCB)
performs the UCB exploration on the entire network parameter space, which is Rp[e]+d, where
except thatof a matrixpe = m + md Z Zt is defined based on the gradient of the network instead of the output of the last hiddent + ( ∈ RL[(] −p[e]+d1))×m(pe[2]+. Note that ind), which is defined in a similar way to the matrix Zhou et al. (2020), they need to compute the inverse At in our paper
layer as in (3.2). In sharp contrast, At in our paper is only of size d × d and thus is much more
efficient and practical in implementation, which will be seen from our experiments in later sections.

We note that there is also a similar algorithm to our Neural-LinUCB presented in Deshmukh et al.
(2020), where they studied the self-supervised learning loss in contextual bandits with neural network representation for computer vision problems. However, no regret analysis has been provided.
When the feature mapping φ(·; w) is an identity function, the problem reduces to linear contextual
bandits where we directly use xt as the feature vector. In this case, it is easy to see that Algorithm 1
reduces to LinUCB (Chu et al., 2011) since we do not need to learn the representation parameter w.

**Comparison with Neural-Linear: The high-level idea of decoupling the representation and explo-**
ration in our algorithm is also similar to that of the Neural-Linear algorithm (Riquelme et al., 2018;
Zahavy & Mannor, 2019), which trains a deep neural network to learn a representation of the raw
feature vectors, and then uses a Bayesian linear regression to estimate the uncertainty in the bandit
problem. However, these two algorithms are significantly different since Neural-Linear (Riquelme
et al., 2018) is a Thompson sampling based algorithm that uses posterior sampling to estimate the
weight parameter θ[∗] via Bayesian linear regression, whereas Neural-LinUCB adopts upper confidence bound based techniques to estimate the weight θ[∗]. Nevertheless, both algorithms share the
same idea of deep representation and shallow exploration, and we view our Neural-LinUCB algorithm as one instantiation of the Neural-Linear scheme.


-----

**Algorithm 1 Deep Representation and Shallow Exploration (Neural-LinUCB)**

1: Input: regularization parameter λ > 0, number of total steps T, episode length H, exploration
parameters _αt > 0_ _t_ [T ]
_{_ _}_ _∈_

2: Initialization: A0 = λI, b0 = 0; entries of θ0 follow N (0, 1/d), and w[(0)] is initialized as
described in Section 3; q = 1; w0 = w[(0)]

3: for t = 1, . . ., T do
4: receive feature vectors **xt,1, . . ., xt,K**
_{_ _}_

5: choose arm at = argmaxk∈[K] θt[⊤]−1[φ][(][x][t,k][;][ w][t][−][1][) +][α][t][∥][φ][(][x][t,k][;][ w][t][−][1][)][∥]A[−]t−[1]1 [, and obtain]
reward _rt_

6: update At and bt as follows:
**At = At** 1 + φ(xt,at ; wt 1)φ(xt,at ; wt 1)[⊤], **bt = bt** 1 + _rtφ(xt,at_ ; wt 1),

b _−_ _−_ _−_ _−_ _−_

7: update θt = A[−]t [1][b][t]

8: **if mod(t, H) = 0 then** b

9: **wt** output of Algorithm 2

10: _q = ← q + 1_

11: **else**

12: **wt = wt** 1
_−_

13: **end if**

14: end for
15: Output wT


**Algorithm 2 Update Weight Parameters with Gradient Descent**

1: Input: initial point wq[(0)] = w[(0)], maximum iteration number n, step size ηq, and loss function
defined in (3.3).

2: for s = 1, . . ., n do
3: **wq[(][s][)]** = wq[(][s][−][1)] _ηq_ **w** _q(wq[(][s][−][1)])._
_−_ _∇_ _L_

4: end for
5: Output wq[(][n][)]


4 MAIN RESULTS

To analyze the regret bound of Algorithm 1, we first lay down some important assumptions on the
neural contextual bandit model.
**Assumption 4.1. For all i ≥** 1 and k ∈ [K], we assume that ∥xi,k∥2 = 1 and its entries satisfy

[xi,k]j = [xi,k]j+d/2.

The assumption that ∥xi,k∥2 = 1 is not essential and is only imposed for simplicity, which is also
used in Zou & Gu (2019); Zhou et al. (2020). The condition on the entries of xi,k is also mild
since otherwise we could always construct x[′]i,k [= [][x]i,k[⊤] _[,][ x]i,k[⊤]_ []][⊤][/]√2 to replace it. An implication of

Assumption 4.1 is that the initialization scheme in Algorithm 1 results in φ(xi,k; w[(0)]) = 0 for all
_i ∈_ [T ] and k ∈ [K].

We assume the following stability condition on the spectral norm of the neural network gradient:

**Assumption 4.2. There is a constant ℓLip > 0 such that it holds** _∂[∂]w[φ]_ [(][x][;][ w][0][)][ −] _∂[∂]w[φ]_ [(][x][′][;][ w][0][)] 2

_ℓLip_ **x** **x[′]** 2 for all x, x[′] **xi,k** _i_ [T ],k [K]. _[≤]_
_∥_ _−_ _∥_ _∈{_ _}_ _∈_ _∈_

The inequality in Assumption 4.2 resembles the Lipschitz condition on the gradient of the neural
network. However, it is essentially different from the smoothness condition since here the gradient
is taken with respect to the neural network weights while the Lipschitz condition is imposed on the
feature parameter x. Similar conditions are widely made in nonconvex optimization (Wang et al.,
2014; Balakrishnan et al., 2017; Xu et al., 2017), in the name of first-order stability, which is essential
to derive the convergence of alternating optimization algorithms. Furthermore, Assumption 4.2 is
only required on the TK training data points and a specific weight parameter w0. Therefore, the
condition will hold if the raw feature data lie in a certain subspace of R[d]. We provided some further
discussions in the supplementary material about this assumption for interested readers.


-----

In order to analyze the regret bound of Algorithm 1, we need to characterize the properties of the
deep neural network in (2.3) that is used to represent the feature vectors. Following a recent line of
research (Jacot et al., 2018; Cao & Gu, 2019a; Arora et al., 2019b; Zhou et al., 2020), we define the
covariance between two data point x, y ∈ R[d] as follows.

**Σ[(0)](x, y) = Σ[(0)](x, y) = x[⊤]y,**

**Σ[l][−][1](x, x)** **Σ[l][−][1](x, y)**

eΛ[(][l][)](x, y) = _,_

**Σ[l][−][1](y, x)** **Σ[l][−][1](y, y)**
 

**Σ[(][l][)](x, y) = 2E(u,v)∼N** (0,Λ(l−1)(x,y))[σ(u)σ(v)],

**Σ[(][l][)](x, y) = 2Σ[e]** [(][l][−][1)](x, y)Eu,v[ ˙σ(u) ˙σ(v)] + Σ[(][l][)](x, y), (4.1)

where (u, v) _N_ (0,e Λ[(][l][−][1)](x, y)), and ˙σ( ) is the derivative of activation function. We denote the
_∼_ _·_
neural tangent kernel (NTK) matrix H R[T K][×][T K] based on all feature vectors **xt,k** _t_ [T ],k [K].
_∈_ _{_ _}_ _∈_ _∈_
Renumbering **xt,k** _t_ [T ],k [K] as **xi** _i=1,...,T K_, then each entry Hij is defined as
_{_ _}_ _∈_ _∈_ _{_ _}_

**Hij = [1]** **Σ[(][L][)](xi, xj) + Σ[(][L][)](xi, xj)** _,_ (4.2)

2


for all i, j [TK]. Based on the above definition, we impose the following assumption on e **H.**
_∈_

**Assumption 4.3. The neural tangent kernel defined in (4.2) is positive definite, i.e., λmin(H) ≥** _λ0_
for some constant λ0 > 0.

Assumption 4.3 essentially requires the neural tangent kernel matrix H to be non-singular, which is
a mild condition and also imposed in other related work (Du et al., 2019a; Arora et al., 2019b; Cao
& Gu, 2019a; Zhou et al., 2020). Moreover, it is shown that Assumption 4.3 can be easily derived
from Assumption 4.1 for two-layer ReLU networks (Oymak & Soltanolkotabi, 2020; Zou & Gu,
2019). Therefore, Assumption 4.3 is mild or even negligible given the non-degeneration assumption
on the feature vectors. Also note that matrix H is only defined based on layers l = 1, . . ., L of the
neural network, and does not depend on the output layer θ. It is easy to extend the definition of H
to the NTK matrix defined on all layers including the output layer θ, which would also be positive
definite by Assumption 4.3 and the recursion in (4.2).

Before we present the regret analysis of the neural contextual bandit, we need to modify the regret
defined in (2.2) to account for the randomness of the neural network initialization. For a fixed time
horizon T, we define the regret of Algorithm 1 as follows.

_T_
_RT = E_ _r(xt,a[∗]t_ [)][ −] _r[b](xt,at_ ) _|w[(0)]_ _,_ (4.3)
 _t=1_ 
X   

where the expectation is taken over the randomness of the reward noise. Note thatb _RT defined_
in (4.3) is still a random variable since the initialization of Algorithm 2 is randomly generated.

Now we are going to present the regret bound of the proposed algorithm.

positive constantTheorem 4.4. Suppose Assumptions M > 0. For any δ ∈ 4.1(0, 1), 4.2, let us choose and 4.3 hold. Assume that αt in Neural-LinUCB as ∥θ[∗]∥2 ≤ _M for some_


_d log(1 + t log(HK)/λ) + log(1/δ)_ + λ[1][/][2]M.


_αt = ν_


We choose the step size ηq of Algorithm 2 as ηq ≤ _C0_ _d[2]mnT_ [5][.][5]L[6] log(TK/δ) _−1 and the width_
of the neural network satisfies m = poly(L, d, 1/δ, H, log(TK/δ)). With probability at least 1 _δ_
   _−_
over the randomness of the initialization of the neural network, it holds that

_RT_ _C1αT_ _Td log_ 1 + _[TG][2]_ + _C2ℓLipL[3]d[5][/][2]T_ log m log( [1]δ [) log(][ T K]δ [)][∥][r][ −] [e]r∥H−1 _,_
_≤_ r _λd_ q _m[1][/][6]_

 

where constants {Ci}i=0,1,2 are independent of the problem, r = (r(x1), r(x2), . . ., r(xT K))[⊤] _∈_
R[T K] and **r = (f** (x1; θ0, w0), . . ., f (xT K; θT 1, wT 1))[⊤] R[T K], and **r** **A =** _√r[⊤]Ar._
e _−_ _−_ _∈_ _∥_ _∥_


-----

**Remark 4.5. Theorem 4.4 shows that the regret of Algorithm 1 can be bounded by two parts: the**
first part is of order _O(√T_ ), which resembles the regret bound of linear contextual bandits (Abbasi
Yadkori et al., 2011); the second part is of order _O(m[−][1][/][6]T_ (r − **r)[⊤]H[−][1](r −** **r)), which de-**

pends on the estimation error of the neural network[e] _f for the reward generating function r and the_

p

neural tangent kernel H.

[e] e e

It is worth noting that our theoretical analysis depends on the reward structure assumption that
_r(·) = ⟨θ∗, ψ(·)⟩. However, the linear structure between θ∗_ and ψ(·) is not essential. As long as
the deep representation of the feature vector and the uncertainty weight parameter can be decoupled,
Algorithm 1 can be easily extended to settings with milder assumptions on the reward structure such
as generalized linear models (Sarkar, 1991; Filippi et al., 2010; Li et al., 2017; Kveton et al., 2020).
For more general bandit models where no assumption is imposed to the reward generating function,
it is still unclear whether the decoupled deep representation and shallow exploration would work
especially in cases a thorough exploration may be needed.

Based on the result in Theorem 4.4, we can easily verify the following conclusion:

**Corollary 4.6. Under the same conditions of Theorem 4.4, if we choose a sufficiently overpa-**
rameterized neural network mapping φ(·) such that m ≥ _T_ [3], then the regret of Algorithm 1 is
_RT =_ _O(√T_ (r − **r)[⊤]H[−][1](r −** **r)).**

**Remark 4.7. For the ease of presentation, let us denotep** := **r** **r** **H−1** . If we have = O(1), the
_E_ _∥_ _−_ _∥_ _E_
total regret in Theorem[e] e 4.4 becomese _O(√T_ ) which matches the regret of linear contextual bandits

(Abbasi-Yadkori et al., 2011). We remark that there is a similar assumption ine Zhou et al. (2020)
where they assume that r[⊤]H[−][1]r can be upper bounded by a constant. They show that this term

[e]

can be bounded by the RKHS norm of r if it belongs to the RKHS induced by the neural tangent
kernel (Arora et al., 2019a;b; Lee et al., 2019). In addition, E here is the difference between the true
reward function and the neural network function, which can also be small if the deep neural network
function well approximates the reward generating function r(·).

5 EXPERIMENTS

In this section, we provide empirical evaluations of Neural-LinUCB on real-world datasets. As we
have discussed in Section 3, Neural-LinUCB could be viewed as an instantiation of the NeuralLinear scheme studied in Riquelme et al. (2018) except that we use the UCB exploration instead
of the posterior sampling exploration therein. Note that there has been an extensive comparison
(Riquelme et al., 2018) of the Neural-Linear methods with many other baselines such as greedy
algorithms, Variational Inference, Expectation-Propagation, Bayesian Non-parametrics and so on.
Therefore, we do not seek a thorough empirical comparison of Neural-LinUCB with all existing
bandits algorithms. In this experiment, we only aim to validate the advantages of our algorithm over
the following baselines: (1) Neural-Linear (Riquelme et al., 2018); (2) LinUCB (Chu et al., 2011),
which does not have a deep representation of the feature vectors; (3) NeuralUCB (Zhou et al., 2020),
and (4) NeuralTS (Zhang et al., 2020) which perform UCB/TS exploration on all the parameters of
the neural network. All numerical experiments were run on a workstation with Intel(R) Xeon(R)
CPU E5-2637 v4 @ 3.50GHz.

**Datasets: we evaluate the performances of all algorithms on bandit problems created from real-**
world data. Specifically, following the experimental setting in Zhou et al. (2020),we use datasets
_(Shuttle) Statlog, Magic and Covertype from UCI machine learning repository (Dua & Graff, 2017),_
and the MINST dataset from LeCun et al. (1998). The details of these datasets are presented in Table
1. In Table 1, each instance represents a feature vector x ∈ R[d] that is associated with one of the K
arms, and dimension d is the number of attributes in each instance.

**Implementations: for LinUCB, we follow the setting in Li et al. (2010) to use disjoint models**
for different arms. For neural network based algorithms, we use a ReLU neural network defined
as in (2.3) with L = 2 and m = 100 for the UCI datasets (Statlog, Magic, Covertype). Thus the
_mneural network weights are = 100, and d is the dimension of features in the corresponding task. Since the problem size of W1 ∈_ R[m][×][d], W2 ∈ R[k][×][m], and θ ∈ R[k] respectively, where k = 100,
the MNIST dataset is larger, inspired by Hinton & Salakhutdinov (2006), we use a deeper NN and
set L = 3, k = 100 and m = 100, with weights W1 ∈ R[m][×][d], W2 ∈ R[m][×][m], W3 ∈ R[k][×][m],


-----

Table 1: Specifications of datasets from the UCI machine learning repository and the MNIST dataset
used in this paper.

_Statlog_ _Magic_ _Covertype_ _MNIST_


Number of attributes 9 11 54 784
Number of arms 7 2 7 10
Number of instances 58,000 19,020 581,012 60,000


500

400

300

200

100


8000

7000

6000

5000

4000

3000

2000

1000


|Col1|LinUCB NeuralTS|
|---|---|
||NeuralUCB|
||NeuralLine Neural-Lin|
|||
|||


3000 6000 9000 12000 15000

round


3000

2500

2000

1500

1000

500


4000

3000

2000

1000

|Lin Ne|UCB uralTS|Col3|Col4|
|---|---|---|---|
|Ne|uralUCB|||
|Ne Ne|uralLinear ural-LinUCB|||
|||||
|||||

|LinUCB NeuralTS|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
|NeuralUC|B||||
|NeuralLin Neural-Lin|ear UCB||||
||||||
||||||
||||||

|LinUCB NeuralTS|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
|NeuralUCB|||||
|NeuralLinear Neural-LinUCB|||||
||||||
||||||
||||||


LinUCB
NeuralTS
NeuralUCB
NeuralLinear
Neural-LinUCB


LinUCB
NeuralTS
NeuralUCB
NeuralLinear
Neural-LinUCB


LinUCB
NeuralTS
NeuralUCB
NeuralLinear
Neural-LinUCB


3000 6000 9000 12000 15000

round

(b) Magic


3000 6000 9000 12000 15000

round

(c) Covertype


3000 6000 9000 12000 15000

round

(d) MNIST


(a) Statlog


Figure 1: The cumulative regrets of LinUCB, NeuralUCB, Neural-Linear and Neural-LinUCB over
15, 000 rounds. Experiments are averaged over 10 repetitions.

and θ ∈ R[k]. We set the time horizon T = 15, 000, which is the total number of rounds for each
algorithm on each dataset. We use stochastic gradient decent to optimize the network weights, with
a step size ηq =1e-5 and maximum iteration number n = 1, 000. To speed up the training process,
the network parameter w is updated every H = 100 rounds starting from round 2000. We also apply
early stopping when the loss difference of two consecutive iterations is smaller than a threshold of
1e-6. We set λ = 1 and αt = 0.02 for all algorithms, t ∈ [T ]. For NeuralUCB and NeuralTS, since
it is computationally unaffordable to perform the original UCB exploration as displayed in Zhou
et al.papers with its diagonal matrix. (2020), we follow their experimental setting to replace the matrix Zt ∈ R[(][d][+]p[e])×(d+p) in their
e

**Results: we plot the cumulative regret of all algorithms versus round in Figures 1(a), 1(b) and 1(c)**
for UCI datasets and in Figure 1(d) for MNIST. The results are reported based on the average of 10
repetitions over different random shuffles of the datasets. It can be seen that algorithms based on
neural network representations (NeuralUCB, NeuralTS, Neural-Linear and Neural-LinUCB) consistently outperform the linear contextual bandit method LinUCB, which shows that linear models may
lack representation power and find biased estimates for the underlying reward generating function.
Furthermore, our proposed Neural-LinUCB achieves a comparable regret with NeuralUCB in all experiments despite the fact that our algorithm only explores in the output layer of the neural network,
which is more computationally efficient as we will show in the sequel.The results in our experiment
are well aligned with our theory that deep representation and shallow exploration are sufficient to
guarantee a good performance of neural contextual bandit algorithms, which is also consistent with
the findings in existing literature (Riquelme et al., 2018) that decoupling the representation learning
and uncertainty estimation improves the performance.

We also conducted experiments to study the effects of different widths of deep neural networks on
the regret performance and to show the computational efficiency of Neural-LinUCB compared with
existing neural bandit algorithms. Due to the space limit, we defer the results to Appendix A.

6 CONCLUSIONS


In this paper, we propose a new neural contextual bandit algorithm called Neural-LinUCB, which
uses the hidden layers of a ReLU neural network as a deep representation of the raw feature vectors
and performs UCB type exploration on the last layer of the neural network. By incorporating techniques in liner contextual bandits and neural tangent kernels, we prove that the proposed algorithm
achieves a sublinear regret when the width of the network is sufficiently large. This is the first regret
analysis of neural contextual bandit algorithms with deep representation and shallow exploration,
which have been observed in practice to work well on many benchmark bandit problems (Riquelme
et al., 2018). We also conducted experiments on real-world datasets to demonstrate the advantage
of the proposed algorithm over LinUCB and existing neural contextual bandit algorithms.


-----

ACKNOWLEDGEMENTS

We thank the anonymous reviewers for their helpful comments. PX and QG are partially supported
by the National Science Foundation CAREER Award 1906169 and IIS-1904183. The views and
conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.

REFERENCES

Yasin Abbasi-Yadkori, D´avid P´al, and Csaba Szepesv´ari. Improved algorithms for linear stochastic
bandits. In Advances in Neural Information Processing Systems, pp. 2312–2320, 2011.

Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming
the monster: A fast and simple algorithm for contextual bandits. In International Conference on
_Machine Learning, pp. 1638–1646, 2014._

Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural networks, going beyond two layers. In Advances in neural information processing
_systems, pp. 6155–6166, 2019a._

Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via overparameterization. In International Conference on Machine Learning, pp. 242–252, 2019b.

Robin Allesiardo, Rapha¨el F´eraud, and Djallel Bouneffouf. A neural networks committee for the
contextual bandit problem. In International Conference on Neural Information Processing, pp.
374–381. Springer, 2014.

Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In International
_Conference on Machine Learning, pp. 322–332, 2019a._

Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang.
On exact computation with an infinitely wide neural net. In Advances in Neural Information
_Processing Systems, pp. 8139–8148, 2019b._

Jean-Yves Audibert, R´emi Munos, and Csaba Szepesv´ari. Exploration–exploitation tradeoff using
variance estimates in multi-armed bandits. Theoretical Computer Science, 410(19):1876–1902,
2009.

Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
problem. Machine learning, 47(2-3):235–256, 2002.

Sivaraman Balakrishnan, Martin J Wainwright, Bin Yu, et al. Statistical guarantees for the em
algorithm: From population to sample-based analysis. The Annals of Statistics, 45(1):77–120,
2017.

Qi Cai, Zhuoran Yang, Jason D Lee, and Zhaoran Wang. Neural temporal-difference learning converges to global optima. In Advances in Neural Information Processing Systems, 2019.

Yuan Cao and Quanquan Gu. A generalization theory of gradient descent for learning overparameterized deep relu networks. arXiv preprint arXiv:1902.01384, 2019a.

Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and
deep neural networks. In Advances in Neural Information Processing Systems, pp. 10835–10845,
2019b.

Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In Advances in
_neural information processing systems, pp. 2249–2257, 2011._

Sayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. In International
_Conference on Machine Learning, pp. 844–853, 2017._


-----

Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff functions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and
_Statistics, pp. 208–214, 2011._

Mark Collier and Hector Urdiales Llorens. Deep contextual multi-armed bandits. arXiv preprint
_arXiv:1807.09809, 2018._

George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of control,
_signals and systems, 2(4):303–314, 1989._

Varsha Dani, Thomas P Hayes, and Sham M Kakade. Stochastic linear optimization under bandit
feedback. In Conference on Learning Theory, 2008.

Aniket Anand Deshmukh, Abhimanu Kumar, Levi Boyles, Denis Charles, Eren Manavoglu,
and Urun Dogan. Self-supervised contextual bandits in computer vision. _arXiv preprint_
_arXiv:2003.08485, 2020._

Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675–
1685, 2019a.

Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
[2019b. URL https://openreview.net/forum?id=S1eK3i09YQ.](https://openreview.net/forum?id=S1eK3i09YQ)

[Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL http://archive.](http://archive.ics.uci.edu/ml)
[ics.uci.edu/ml.](http://archive.ics.uci.edu/ml)

Sarah Filippi, Olivier Cappe, Aur´elien Garivier, and Csaba Szepesv´ari. Parametric bandits: The
generalized linear case. In Advances in Neural Information Processing Systems, pp. 586–594,
2010.

Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.

Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural
networks. science, 313(5786):504–507, 2006.

Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in neural information processing systems, pp. 8571–
8580, 2018.

Branislav Kveton, Manzil Zaheer, Csaba Szepesvari, Lihong Li, Mohammad Ghavamzadeh, and
Craig Boutilier. Randomized exploration in generalized linear bandits. In International Confer_ence on Artificial Intelligence and Statistics, pp. 2066–2076, 2020._

John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side
information. In Advances in neural information processing systems, pp. 817–824, 2008.

Tor Lattimore and Csaba Szepesv´ari. Bandit algorithms. Cambridge University Press, 2020.

Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436–444,
2015.

Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha SohlDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in neural information processing systems, pp. 8570–8581,
2019.

Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to
personalized news article recommendation. In Proceedings of the 19th international conference
_on World wide web, pp. 661–670, 2010._


-----

Lihong Li, Yu Lu, and Dengyong Zhou. Provably optimal algorithms for generalized linear contextual bandits. In International Conference on Machine Learning, pp. 2071–2080, 2017.

Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural trust region/proximal policy optimization attains globally optimal policy. In Advances in Neural Information Processing Systems, pp.
10564–10575, 2019.

Samet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: global convergence guarantees for training shallow neural networks. IEEE Journal on Selected Areas in
_Information Theory, 2020._

Carlos Riquelme, George Tucker, and Jasper Snoek. Deep bayesian bandits showdown: An empirical comparison of bayesian deep networks for thompson sampling. In International Confer_[ence on Learning Representations, 2018. URL https://openreview.net/forum?id=](https://openreview.net/forum?id=SyYe6k-CW)_
[SyYe6k-CW.](https://openreview.net/forum?id=SyYe6k-CW)

Paat Rusmevichientong and John N Tsitsiklis. Linearly parameterized bandits. Mathematics of
_Operations Research, 35(2):395–411, 2010._

Daniel J. Russo, Benjamin Van Roy, Abbas Kazerouni, Ian Osband, and Zheng Wen. A tutorial on
thompson sampling. Foundations and Trends R⃝ _in Machine Learning, 11(1):1–96, 2018. ISSN_
1935-8237.

Jyotirmoy Sarkar. One-armed bandit problems with covariates. The Annals of Statistics, pp. 1978–
2002, 1991.

Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish, Narayanan Sundaram,
Mostofa Patwary, Mr Prabhat, and Ryan Adams. Scalable bayesian optimization using deep
neural networks. In International conference on machine learning, pp. 2171–2180, 2015.

William R Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrika, 25(3/4):285–294, 1933.

Michal Valko, Nathan Korda, R´emi Munos, Ilias Flaounas, and Nello Cristianini. Finite-time analysis of kernelised contextual bandits. In Proceedings of the Twenty-Ninth Conference on Uncer_tainty in Artificial Intelligence, pp. 654–663, 2013._

Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods: Global
optimality and rates of convergence. In International Conference on Learning Representations,
[2020. URL https://openreview.net/forum?id=BJgQfkSYDS.](https://openreview.net/forum?id=BJgQfkSYDS)

Zhaoran Wang, Han Liu, and Tong Zhang. Optimal computational and statistical rates of convergence for sparse nonconvex learning problems. Annals of statistics, 42(6):2164, 2014.

Pan Xu and Quanquan Gu. A finite-time analysis of q-learning with neural network function approximation. In International Conference on Machine Learning, 2020.

Pan Xu, Jian Ma, and Quanquan Gu. Speeding up latent variable gaussian graphical model estimation via nonconvex optimization. In Advances in Neural Information Processing Systems, pp.
1933–1944, 2017.

Tom Zahavy and Shie Mannor. Deep neural linear bandits: Overcoming catastrophic forgetting
through likelihood matching. arXiv preprint arXiv:1901.08612, 2019.

Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In International Conference on Learning Rep_[resentations, 2017. URL https://openreview.net/forum?id=Sy8gdB9xx.](https://openreview.net/forum?id=Sy8gdB9xx)_

Weitong Zhang, Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural thompson sampling. arXiv
_preprint arXiv:2010.00827, 2020._

Dongruo Zhou, Lihong Li, and Quanquan Gu. Neural contextual bandits with ucb-based exploration.
In International Conference on Machine Learning, 2020.


-----

Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural
networks. In Advances in Neural Information Processing Systems, pp. 2053–2062, 2019.

Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018.


-----

A ADDITIONAL EXPERIMENTAL RESULTS

In this section, we provide more experimental results that are omitted in Section 5 due to space limit.

A.1 COMPUTATIONAL EFFICIENCY OF NEURAL-LINUCB

Throughout the experiments, our Neural-LinUCB algorithm is much more computationally efficient
than NeuralUCB since we only perform the UCB exploration on the last layer of the neural network,
where the dimension is much lower. In specific, on the Statlog dataset, it takes on average 1.11
seconds for NeuralUCB to finish 100 rounds (one epoch in Algorithm 1) and achieve the regret in
Figure 1(a), while it only takes 0.58 seconds for Neural-LinUCB to finish 100 rounds and achieve
the comparable or even better regret in Figure 1(a). On the Magic dataset, the average runtimes
for 100 rounds of NeuralUCB and Neural-LinUCB are 1.32 seconds and 0.81 seconds respectively.
On the Covertype dataset, the runtimes of NeuralUCB and Neural-LinUCB are 1.02 seconds and
0.66 seconds respectively. And on the MNIST dataset, the average runtimes for 100 rounds of
NeuralUCB and Neural-LinUCB are 4.67 seconds and 1.29 seconds respectively. For practical
applications in the real-world with larger problem sizes, we believe that the improvement of our
algorithm in terms of the computational efficiency will be more pronounced.

As we discussed in Section 5 and in the above paragraph as well, the computational efficiency of
Neural-LinUCB mainly stems from the design of shallow exploration. This is because in UCB based
bandit algorithms we need to compute the inverse of matrix A at every time step for arm selection
(Line 5 of Algorithm 1). Due to the large width of the neural network used in practice, the arm
selection operation could be rather time consuming. However, the neural network weight can be
updated periodically (i.e., in our paper it is only updated every H steps). To validate our analysis
on computational efficiency, we further studied the time profiling of the experiments conducted on
MNIST to compared our proposed algorithm with NeuralUCB in more details.

Table 2: Profiling experiment on MNIST for running 100 rounds: runtime (seconds) for different
algorithms on arm selection and network weight update.

Operations NeuralUCB Neural-LinUCB
Arm selection (Line 5 in Algorithm 1) 3.60 0.28
Network weight update (Line 9 in Algorithm 1) 0.96 0.92

In particular, the setting is the same as that in Section 5 for MNIST experiments. We record the
time cost of the most expensive two subroutines: (1) the operation of arm selection (Line 5 in
Algorithm 1); and (2) the operation of updating the neural network weights (Line 9 in Algorithm
1), for H = 100 rounds. The time cost is presented in Table 2. For Neural-LinUCB, the arm
selection operation takes about 0.28 seconds (this is 21.71% of the total time cost by the algorithm
in these H = 100 rounds), among which the matrix inverse step only takes 0.17 seconds. For
NeuralUCB, the arm selection operation takes 3.60 seconds (this is 77.19% of the total time time
cost by NeuralUCB for H = 100 rounds). Therefore, the operation of arm selection in NeuralUCB
is much (almost 13 times) more time consuming than that in Neural-LinUCB. Moreover, since
the UCB matrix Zt in NeuralUCB is defined as ∇f (x; w)∇f (x; w)[⊤], it needs to compute the
gradients via back-propagation (0.93 seconds) and compute the matrix inverse (1.54 seconds), while
our Neural-LinUCB algorithm only needs to compute the matrix inverse of a small matrix (0.17
seconds). To summarize, our method is much more computationally efficient.

A.2 IMPACT OF LARGE WIDTHS

Note that the requirement of width m in our Theorem 4.4 is extremely high. On one hand, our theory may be too conservative since the current understanding of deep learning is still very limited in
the field. We believe our work is a good starting point towards understanding the behavior of deep
bandits algorithms. On the other hand, we would also like to investigate the impact of mild overparameterization on the regret performance of Neural-LinUCB in practice. Therefore, we conducted
additional experiments on the Statlog dataset with wider neural networks. In particular, the neural


-----

500

400


300

200


100

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|
|---|---|---|---|---|---|---|---|---|---|
||||Neural- Neural-|LinUC LinUC|B (100) B (100|0)||||
||||Neural- Neural- Neural-|LinUC LinUC LinUC|B (100 B (500 B (100|00) 00) 00, 100|00)|||
||||Neural-|LinUC|B (100|00, 100|00, 100|00)||
|||||||||||
|||||||||||
|||||||||||


Neural-LinUCB (100)
Neural-LinUCB (1000)
Neural-LinUCB (10000)
Neural-LinUCB (50000)
Neural-LinUCB (10000, 10000)
Neural-LinUCB (10000, 10000, 10000)


2000 4000 6000 8000 10000 12000 14000

round


Figure 2: Performance of Neural-LinUCB with different widths on Statlog dataset.

network parameters are listed as follows


**W1 ∈** R[m][×][d], W2 ∈ R[m][×][m], . . ., WL ∈ R[k][×][m], θ ∈ R[k],
where L is the depth, k = 100, d is the feature dimensions, and m is the width. We conducted
experiments for the following settings: (1) L = 2, m = 100, and thus the hidden layer width is
(100); (2) L = 2, m = 1000, and thus the hidden layer width is (1000); (3) L = 2, m = 10000, and
thus the hidden layer width is (10000); (4) L = 2, and the hidden layer width is (50000); (5) L = 3,
_m = 10000, and thus the hidden layer width is (10000, 10000); and (6) L = 4, m = 10000, and thus_
the hidden layer width is (10000, 10000, 10000). The results are plotted in Figure A.2. We observe
that the performance of our Neural-LinUCB algorithm is not negatively impacted by the width of
the neural network. In fact, Figure A.2 shows improved performance of Neural-LinUCB when the
total number of hidden nodes increases. This is consistent to the observations in Zhang et al. (2017)
that an overparameterized neural network trained by gradient descent does not necessarily lead to
overfitting and also aligns with our Theorem 4.4 that the regret bound of Neural-LinUCB decreases
as the width m increases.

B MORE DISCUSSIONS ON ASSUMPTION 4.2


In this section, we are going to show that Assumption 4.2 could be satisfied as long as the feature
vectors {x} lie in a begin subspace of R[d]. Let us start with the case that φ : R[d] _→_ R[m] is a
two-layer ReLU neural network with vector output. In particular, we define φ(x; w) as follows
_σφ is the ReLU activation function applied elementwise. We use(x; w) = σ(W2σ(W1x)), where w = (vec(W1), vec(W2)) u[⊤][⊤]i, W[to denote the]1 ∈_ R[m][×][d][ i], W[-th row of]2 ∈ R[d][ W][×][m][1], and[ and]
obtainwherethus W v1j = ( ∈ Ru[m]1, . . .,is the u jm-th row of)[⊤], where W ui2 ∈, ∀jR ∈[d], ∀[di] ∈. Let us denote[m]. Similarly, we have h as the vector W2 σ = ((Wv11x, . . .,). We thus vd)[⊤],

1{u[⊤]1 **[x][ ≥]** [0][}][u]1[⊤][x] 1{v1[⊤][h][ ≥] [0][}][v]1[⊤][h]
. .

**h =**  ..  _,_ **_φ(x; w) =_**  ..  _._

1 **u[⊤]m[x][ ≥]** [0][}][u][⊤]m[x] 1 **vd[⊤][h][ ≥]** [0][}][v]d[⊤][h]

 _{_   _{_ 

We use φl(x; w) to denote the _l-th entry of vector_ **_φ(x; w), for any_** _l_ [d]. Then it holds that
_∈_
_∂φl(x; w)_

1 **[h][ ≥]** [0][}] _v1[1]_ [1][{][u][⊤]1 **[x][ ≥]** [0][}][x][⊤][, . . ., v]1[m] [1][{][u][⊤]m[x][ ≥] [0][}][x][⊤][],
_∂vec(W1) [=][ 1][{][v][⊤]_


for all l ∈ [d], where v1[i] [is the][ i][-th element in][ v][1][,][ i][ ∈] [[][m][]][. This further implies that]

_∂φ(x; w)_ 1{v1[⊤][h][ ≥] [0][}] _v1[1]_ [1][{][u][⊤]1 **[x][ ≥]** [0][}].[x][⊤][, . . ., v]1[m] [1][{][u][⊤]m[x][ ≥] [0][}][x][⊤][]

_∂vec(W1) [=]_ 1 **vd[⊤][h][ ≥]** [0][}] vd[1] [1][{][u][⊤]1 **[x][ ≥]** [0][}]..[x][⊤][, . . ., v]d[m] [1][{][u][⊤]m[x][ ≥] [0][}][x][⊤][] _∈_ R[d][×][md].

 _{_ 
 

 


-----

Similarly, we can compute the gradient of φ(x; w) with respect to W2. In particular, we have

_∂φ1(x; w)_ 1 **[h][ ≥]** [0][}] 1 **u[⊤]1** **[x][ ≥]** [0][}][u]1[⊤][x][, . . .,][ 1][{][u]m[⊤] **[x][ ≥]** [0][}][u][⊤]m[x] _,_

_∂vec(v1) [=][ 1][{][v][⊤]_ _{_
 


_∂φ1(x; w)_

_j_ = 1.
_∂vec(vj) [= [0][, . . .,][ 0]][,]_ _̸_

Therefore, the gradient of φ(x; w) with respect to W2 is


_∂φ∂vec1((xv;w1))_ _. . ._ **0[⊤]**

...
**0[⊤]** _. . ._ _∂φ∂vecd((xv;wd))_


_∂φ(x; w)_

_∂vec(W2) [=]_




 _[∈]_ [R][d][×][md][.]


Lastly, we have


_∂φ∂(xw; w)_ = _∂∂vecφ((xW;w1))_ _∂∂vecφ((xW;w2))_ _∈_ R[d][×][(][md][+][md][)].

h i

Therefore, for any two feature vectors x and x[′] from {xi,k}i∈[T ],k∈[K], if many nodes in the initial
neural network φ(x; w0) are activated or deactivated at the same time for both x and x[′], then the
spectral norm of the matrix _[∂][φ][(]∂[x]w[;][w][0][)]_ _∂w_ would satisfy the condition in Assumption 4.2. A

_−_ _[∂][φ][(][x][′][;][w][0][)]_

more thorough study of this stability condition is out of the scope of this paper, though it would be
an interesting open direction in the theory of deep neural networks.

C PROOF OF THE MAIN RESULTS

In this section, we provide the proof of the regret bound for Neural-LinUCB. Recall that in neural
contextual bandits, we do not assume a specific formulation of the underlying reward generating
function r(·). Instead, we use deep neural networks defined in Section 2.2 to approximate r(·). We
will first show that the reward generating function r(·) can be approximated by the local linearization
of the overparameterized neural network near the initialization weight w[(0)]. In particular, we denote
the gradient of φ(x; w) with respect to w by g(x; w), namely,

**g(x; w) =** **wφ(x; w),** (C.1)
_∇_

which is a matrix in R[d][×][p]. We define φj(x; w) to be the j-th entry of vector φ(x; w), for any
_j ∈_ [d]. Then, we can prove the following lemma.

**Lemma C.1. Suppose Assumptions 4.3 hold. Then there exists w[∗]** _∈_ R[p] such that ∥w[∗] _−w[(0)]∥2 ≤_
1/[√]m (r − **r)[⊤]H[−][1](r −** **r) and it holds that**
p _r(xt,k) = θ[∗⊤]φ(xt,k; wt_ 1) + θ0[⊤][g][(][x][t,k][;][ w][(0)][)] **w[∗]** **w[(0)][],**

e e _−_ _−_

for all k ∈ [K] and t = 1, . . ., T .  

Lemma C.1 implies that the reward generating function r( ) at points **xi,k** _i_ [T ],k [K] can be ap
_·_ _{_ _}_ _∈_ _∈_
proximated by a linear function around the initial point w[(0)]. Note that a similar lemma is also
proved in Zhou et al. (2020) for NeuralUCB.

The next lemma shows the upper bounds of the output of the neural network φ and its gradient.
**Lemma C.2. Suppose Assumptions 4.1 and 4.3 hold. For any round index t ∈** [T ], suppose it is
in the q-th epoch of Algorithm 2, i.e., t = (q − 1)H + i for some i ∈ [H]. If the step size ηq in
Algorithm 2 satisfies

_C0_
_η_
_≤_ _d[2]mnT_ [5][.][5]L[6] log(TK/δ) _[,]_

and the width of the neural network satisfies

_m ≥_ max{L log(TK/δ), dL[2] log(m/δ), δ[−][6]H [18]L[16] log[3](TK)}, (C.2)


-----

then, with probability at least 1 − _δ we have_

_δ[3][/][2]_
**wt** **w[(0)]** 2
_∥_ _−_ _∥_ _≤_ _m[1][/][2]Tn[9][/][2]L[6]_ log[3](m) _[,]_


**g(xt,k; w[(0)])** _F_ _C1√dLm,_
_∥_ _∥_ _≤_

**_φ(x; wt)_** 2 _d log(n) log(TK/δ),_
_∥_ _∥_ _≤_

for all t ∈ [T ], k ∈ [K], where the neural networkp **_φ is defined in (2.4) and its gradient is defined_**
in (C.1).

The next lemma shows that the neural network φ(x; w) is close to a linear function in terms of the
weight w parameter around a small neighborhood of the initialization point w[(0)].

**Lemma C.3 (Theorems 5 in Cao & Gu (2019b)). Let w, w[′]** be in the neighborhood of w0, i.e.,
**w, w[′]** _∈_ B(w0, ω) for some ω > 0. Consider the neural network defined in (2.4), if the width m
and the radius ω of the neighborhood satisfy

_m ≥_ _C0 max{dL[2]_ log(m/δ), ω[−][4][/][3]L[−][8][/][3] log(TK) log(m/(ωδ))},

_ω_ _C1L[−][5](log m)[−][3][/][2],_
_≤_

then for all x ∈{xt,k}t∈[T ],k∈[K], with probability at least 1 − _δ it holds that_

_φj(x; w)_ _φj(x; w)_ _C2ω[4][/][3]L[3]d[−][1][/][2][p]m log m,_
_|_ _−_ [b] _| ≤_

where _φj(x; w) is the linearization of φj(x; w) at w[′]_ defined as follow:

_φj(x; w) = φj(x; w[′]) +_ **wφj(x; w[′]), w** **w[′]** _._ (C.3)

Similar results on the local linearization of an overparameterized neural network are also presented[b] b _⟨∇_ _−_ _⟩_
in Allen-Zhu et al. (2019b); Cao & Gu (2019b).

For the output layer θ[∗], we perform a UCB type exploration and thus we need to characterize the
uncertainty of the estimation. The next lemma shows the confidence bound of the estimate θt in
Algorithm 1.

**Lemma C.4. Suppose Assumption and 4.3 hold. For any δ ∈** (0, 1), with probability at least 1 − _δ,_
the distance between the estimated weight vector θt by Algorithm 1 and θ[∗] can be bounded as
follows:

_t_

_t_ **_φ(xs,as_** ; ws−1)θ0[⊤][g][(][x][s,a]s [;][ w][(0)][)(][w][∗] _[−]_ **[w][(0)][)]**

_s=1_ **At**

X

**_[θ][t]ν[ −]_** **_[θ]2[∗]_** _d[−] log(1 +[A][−][1]_ _t(log HK)/λ) + log 1/δ_ + λ[1][/][2]M,

_≤_
q

for any t ∈ [T ].   


Note that the confidence bound in Lemma C.4 is different from the standard result for linear contextual bandits in Abbasi-Yadkori et al. (2011). The additional term on the left hand side of the
confidence bound is due to the bias caused by the representation learning using a deep neural network. To deal with this extra term, we need the following technical lemma.

and some constantsLemma C.5. Assume that λ, G > A 0. Lett = λ {Iζ +t}t=1s,...=1 be a real-value sequence such that[φ][s][φ]s[⊤][, where][ φ][t] _[∈]_ [R][d][ and][ ∥][φ][t][∥][2] |[≤]ζt| ≤[G][ for all]U for some[ t][ ≥] [1]
constant U > 0. Then we have

[P][t]

_t_

_t_ **_φsζs_** 2Ud, _t = 1, 2, . . ._

_s=1_ 2 _≤_ _∀_

X

**[A][−][1]**

The next lemma provides some standard bounds on the feature matrix At, which is a combination
of Lemma 10 and Lemma 11 in Abbasi-Yadkori et al. (2011).


-----

**Lemma C.6. Let {xt}t[∞]=1** [be a sequence in][ R][d][ and][ λ >][ 0][.] Suppose ∥xt∥2 ≤ _G and λ ≥_
max{1, G[2]} for some G > 0. Let At = λI + _s=1_ **[x][t][x]t[⊤][. Then we have]**

_T_

[P][t]

det(At) ≤ (λ + tG[2]/d)[d], and _t=1_ _∥xt∥A[2]_ _[−]t−[1]1_ _[≤]_ [2 log det(]det([A]λI[T])[ )] _[≤]_ [2][d][ log(1 +][ TG][2][/][(][λd][))][.]

X

Now we are ready to prove the regret bound of Algorithm 1.

_Proof of Theorem 4.4. For a time horizon T_, without loss of generality, we assume T = QH for
some epoch number Q. By the definition of regret in (4.3), we have


_RT = E_ (r(xt,a[∗]t [)][ −] _r[b](xt,at_ )) = E (r(xqH+i,a[∗]qH+i [)][ −] _r[b](xqH+i,aqH+i_ )) _._
 _t=1_   _q=1_ _i=1_ 
X X X

b b

Note that for the simplicity of presentation, we omit the conditional expectation notation of w[(0)] in
the rest of the proof when the context is clear. In the second equation, we rewrite the time index
_t = qH + i as the i-th iteration in the q-th epoch._

By the definition in (2.1), we have E[r(xt,k)|xt,k] = r(xt,k) for all t ∈ [T ] and k ∈ _K. Based on the_
linearization of reward generating function, we can decompose the instaneous regret into different
parts and upper bound them individually. In particular, by Lemma C.1, there exists a vector w[∗] R[p]
b _∈_
such that we can write the expectation of the reward generating function as a linear function. Then
it holds that

_r(xt,a[∗]t_ [)][ −] _[r][(][x][t,a][t]_ [) =][ θ]0[⊤] **g** **xt,a[∗]t** [;][ w][(0)][] **g** **xt,at** ; w[(0)][ ]w[∗] **w[(0)][]**
_−_ _−_

+ θ[∗⊤] []φ **xt,a[∗]t** [;][ w][t][−][1]  − **_φ_** **xt,at** ; wt−1

= θ0[⊤] **g** **xt,a ** _[∗]t_ [;][ w][(0)][] **g** **xt,a t** ; w[(0)][ ]w[∗] **w[(0)][]**
_−_ _−_

+ θt[⊤]− 1 **_φ_** **xt,a[∗]t** [;][ w][t][−][1]  − **_φ_** **xt,at** ; wt−1

_−_ (θt−1 − θ[∗])[⊤][]φ **xt,a** _[∗]t_ [;][ w][t] [−][1] _−_ **_φ_** **xt,at** ; wt−1 _._ (C.4)

The first term in (C.4) can be easily bounded using the first order stability in Assumption      4.2 and the
distance between w[∗] and w[(0)] in Lemma C.1. The second term in (C.4) is related to the optimistic
rule of choosing arms in Line 5 of Algorithm 1, which can be bounded using the same technique for
LinUCB (Abbasi-Yadkori et al., 2011). For the last term in (C.4), we need to prove that the estimate
of weight parameter θt 1 lies in a confidence ball centered at θ[∗]. For the ease of notation, we define
_−_


**Mt = A[−]t** [1]


**_φ(xs,as_** ; ws−1)θ0[⊤][g][(][x][s,a]s [;][ w][(0)][)(][w][∗] _[−]_ **[w][(0)][)][.]** (C.5)
_s=1_

X


Then the second term in (C.4) can be bounded in the following way:

_−_ (θt−1 − **_θ[∗])[⊤][]φ_** **xt,a[∗]t** [;][ w][t][−][1] _−_ **_φ_** **xt,at** ; wt−1

= − **_θt−1 −_** **_θ[∗]_** _−_ **M ** _t−1_ _⊤φ_ **xt,a[∗]t** [;][ w]  _[t][−][1]_ + **_θt−1 −_** **_θ[∗]_** _−_ **Mt−1** _⊤φ_ **xt,at** ; wt−1
 − **M[⊤]t−1** **_φ_** **xt,a[∗]t** [;][ w][t][−][1]  − **_φ_** **xt,at** ; w _t−1 _    

_≤∥θt−1 −_ **_θ[∗]_** _− Mt−1∥At−1 · ∥φ( xt,a[∗]t_ [;][ w][t][−][1][)][∥]A[−]t−[1]1
+ ∥θt−1 − **_θ[∗]_** _−_ **Mt−1∥At−1 · ∥φ(xt,at** ; wt−1)∥A−t−11
+ **M[⊤]t−1** **_φ_** **xt,a[∗]t** [;][ w][t][−][1] _−_ **_φ_** **xt,at** ; wt−1 2
_≤_ _αt∥φ(xt,a[∗]t_ [;][ w][t] [−][1][)][∥]A[−]t−[1]1 [+][ α] _[t][∥][φ][(] [x][t,a][t]_ [;][ w][t][−][1][)][∥][A]t[−]−[1]1
+ ∥Mt−1∥2 · ∥φ **xt,a[∗]t** [;][ w][t][−][1] _−_ **_φ_** **xt,at** ; wt−1 _∥2._ (C.6)
     


-----

where the last inequality is due to Lemma C.4 and the choice of αt. Plugging (C.6) back into (C.4)
yields

_r(xt,a[∗]t_ [)][ −] _[r][(][x][t,a][t]_ [)][ ≤] _[α][t][∥][φ][(][x][t,a][t]_ [;][ w][t][−][1][)][∥]A[−]t−[1]1 _t_ [;][ w][t][−][1][)][∥]A[−]t−[1]1

_[−]_ _[α][t][∥][φ][(][x][t,a][∗]_
+ αt∥φ(xt,a[∗]t [;][ w][t][−][1][)][∥]A[−]t−[1]1 [+][ α][t][∥][φ][(][x][t,a][t] [;][ w][t][−][1][)][∥][A]t[−]−[1]1
+ ∥Mt−1∥2 · ∥φ **xt,a[∗]t** [;][ w][t][−][1] _−_ **_φ_** **xt,at** ; wt−1 _∥2_

+ **_θ0_** 2 **g(xt,a [∗]t** [;][ w][(0)][)][ −] **[g][(][x][t,a][t] [;][ w][(0)][)][∥][F][ · ∥][w][∗]** _[−]_ **[w][(0)][∥][2]**
_∥_ _∥_ _· ∥_
_≤_ 2αt∥φ(xt,at ; wt−1)∥A−t−11 [+][ ∥][M][t][−][1][∥][2][ · ∥][φ] **xt,a[∗]t** [;][ w][t][−][1] _−_ **_φ_** **xt,at** ; wt−1 _∥2_

+ ℓLip **_θ0_** 2 **xt,a[∗]t**      (C.7)
_∥_ _∥_ _· ∥_ _[−]_ **[x][t,a][t]** _[∥][2][ · ∥][w][∗]_ _[−]_ **[w][(0)][∥][2][,]**

where in the first inequality we used the definition of upper confidence bound in Algorithm 1 and
the second inequality is due to Assumption 4.2. Recall the linearization of φj in Lemma C.3, we
have


**_φ(x; wt−1) = φ(x; w0) + g(x; w0)(wt−1 −_** **w0).**

Note that by the initialization, we haveb **_φ(x; w0) = 0 for any x_** R[d]. Thus, it holds that
_∈_

**_φ_** **xt,a[∗]t** [;][ w][t][−][1] _−_ **_φ_** **xt,at** ; wt−1

=  φ **xt,a[∗]t** [;][ w][t][−][1] _− _ **_φ_** **xt,a[∗]t** [;][ w][0] + φ **xt,at** ; w0 _−_ **_φ_** **xt,at** ; wt−1

= φ xt,a[∗]t [;][ w][t][−][1] _−_ **_φ[b] xt,a[∗]t_** [;][ w][t][−][1] + g  (xt,a[∗]t [;][ w][0][)(][w] [t][−][1][ −] **[w][0][)]** 
 + φ **xt,at** ; wt 1   **_φ_** **xt,at** ; wt 1 **g(xt,at** ; w0)(wt 1 **w0),** (C.8)

_−_ _−_ [b] _−_ _−_ _−_ _−_

which immediately implies that     
**_φ_** **xt,a[∗]t** [;][ w][t][−][1] _−_ **_φ_** **xt,at** ; wt−1 2
_≤_   **_φ_** **xt,a[∗]t** [;][ w][t][−][1] _− _ **_φ[b]_** **xt,a[∗]t** [;][ w][t][−][1] 2 [+] **_φ_** **xt,at** ; wt−1 _−_ **_φ[b]_** **xt,at** ; wt−1 2
+  **g(xt,a[∗]t** [;][ w] [0][)][ −] [g][(][x][t,a][t] [;][ w][0][)](wt−1 −  **w0)** 2    
_C0ω[4][/][3]L[3]d[1][/][2][p]m log m + ℓLip_ **xt,a** _[∗]t_ (C.9)
_≤_ [ ] _∥_ _[−]_ **[x][t,a][t]** _[∥][2][∥][w][t][−][1][ −]_ **[w][(0)][∥][2][,]**

where the second inequality is due to Lemma C.3 and Assumption 4.2. Therefore, the instaneous
regret can be further upper bounded as follows.

_r(xt,a[∗]t_ [)][ −] _[r][(][x][t,a][t]_ [)]

_≤_ 2αt∥φ(xt,at ; wt−1)∥A−t−11 [+][ ℓ][Lip][∥][θ][0][∥][2][ · ∥][x][t,a]t[∗] _[−]_ **[x][t,a][t]** _[∥][2][ · ∥][w][∗]_ _[−]_ **[w][(0)][∥][2]**

+ **Mt** 1 2 _C0ω[4][/][3]L[3]d[1][/][2][p]m log m + ℓLip_ **xt,a[∗]t** _._ (C.10)
_∥_ _−_ _∥_ _·_ _∥_ _[−]_ **[x][t,a][t]** _[∥][2][∥][w][t][−][1][ −]_ **[w][(0)][∥][2]**

By Assumption 4.1 we have  **xt,a[∗]t** 
_∥_ _[−]_ **[x][t,a][t]** _[∥][2][ ≤]_ [2][. By Lemma][ C.1][ and Lemma][ C.2][, we have]


**w[∗]** **w[(0)]** 2 1/m(r **r)[⊤]H[−][1](r** **r),**
_∥_ _−_ _∥_ _≤_ _−_ _−_
q (C.11)

_δ[3][/][2]_
**wt** **w[(0)]** 2 e e
_∥_ _−_ _∥_ _≤_ _m[1][/][2]Tn[9][/][2]L[6]_ log[3](m) _[.]_

In addition, since the entries of θ0 are i.i.d. generated from N (0, 1/d), we have ∥θ0∥2 ≤
2(2 + _d[−][1]_ log(1/δ)) with probability at least 1 − _δ for any δ > 0. By Lemma C.2, we have_

**g(xt,at** ; w[(0)]) _F_ _C1√dm. Therefore,_
_∥_ p _∥_ _≤_


**_θ0[⊤][g][(][x][s,a]s_** [;][ w][(0)][)(][w][∗] _[−]_ **[w][(0)][)]** _C2d_ log(1/δ)(r **r)[⊤]H[−][1](r** **r).**
_≤_ _−_ _−_
q

Then, by the definition of Mt in (C.5) and Lemma C.5, we have

e e


_∥Mt−1∥2 ≤_ _C3d[2]_


log(1/δ)(r − er)[⊤]H[−][1](r − er). (C.12)


-----

Substituting (C.12) and the above results on **xt,at** **xt,a[∗]t**
**w[(0)]∥2 back into (C.10) further yields** _∥_ _−_ _[∥][2][,][ ∥][θ][0][∥][2][,][ ∥][w][∗]_ _[−]_ **[w][(0)][∥][2][ and][ ∥][w][t][−][1][ −]**

_r(xt,a[∗]t_ [)][ −] _[r][(][x][t,a][t]_ [)]

_≤_ 2αt∥φ(xt,at ; wt−1)∥A−t−11 [+][ C][4][ℓ][Lip][m][−][1][/][2] log(1/δ)(r − **r)[⊤]H[−][1](r −** **r)**
q

2ℓLipδ[3][/][2]

+ _C0ω[4][/][3]L[3]d[1][/][2][p]m log m +_ e _C3d[2]_ log(1e/δ)(r **r)[⊤]H[−][1](r** **r).**

_m[1][/][2]Tn[9][/][2]L[6]_ log[3](m) _−_ _−_

  q

Note that we have ω = O(m[−][1][/][2] **r** **r** **H−1** ) by Lemma C.1. Therefore, the regret of thee e
_∥_ _−_ _∥_
Neural-LinUCB is

_Q_ _H_ e

_RT ≤_ vuQH maxt∈[T ] _[α]t[2]_ _q=1_ _i=1_ _∥φ(xi,ai_ ; wqH+i)∥A[2] _[−]i_ [1] + C4ℓLipm[−][1][/][2]T log(1/δ)∥r − **r∥H−1**

u X X p
t _C0TL3d1/2√log m_ **r** **r** **H[−][1]** 2ℓLipδ[3][/][2] e

+ _∥_ _−_ _∥[4][/][3]_ + _C3d[2][p]log(1/δ)_ **r** **r** **H−1**

_m[1][/][6]_ _m[1][/][2]n[9][/][2]L[6]_ log[3](m) _∥_ _−_ _∥_

 
e

_C5_ _Td log(1 + TG[2]/(λd))_ _ν_ _d log(1 + T_ (log TK)/λ) + log 1/δ + λ[1][/][2]M e
_≤_

+p C6ℓLipL[3]d[5][/][2]m[−][1][/][6]T  logp m log(1/δ) log(TK/δ) **r** **r** **H−1** _,_ 

_∥_ _−_ _∥_

where the first inequality is due to Cauchy’s inequality, the second inequality comes from the upperp
bound of αt in Lemma C.4 and Lemma C.6. {Cj}j=0,...,6 are absolute constants that are independente
of problem parameters.


_Proof of Corollary 4.6. It directly follows the result in Theorem 4.4._

D PROOF OF TECHNICAL LEMMAS

In this section, we provide the proof of technical lemmas used in the regret analysis of Algorithm 1.

D.1 PROOF OF LEMMA C.1

Before we prove the lemma, we first present some notations and a supporting lemma for simplification. Let β = (θ[⊤], w[⊤])[⊤] _∈_ R[d][+][p] be the concatenation of the exploration parameter and the hidden
layer parameter of the neural network f (x; β) = θ[⊤]φ(x; w). Note that for any input data vector
**x ∈** R[d], we have

_∂_ **_φ(x; w)[⊤], θ[⊤]_** _[∂]_ _⊤_ = **_φ(x; w)[⊤], θ[⊤]g(x; w)_** _⊤,_ (D.1)

_∂β_ _[f]_ [(][x][;][ β][) =] _∂w_ **_[φ][(][x][;][ w][)]_**
 

  

where g(x; w) is the partial gradient of φ(x; w) with respect to w defined in (C.1), which is a
matrix in R[d][×][p]. Similar to (4.2), we define HL+1 to be the neural tangent kernel matrix based on all
_L + 1 layers of the neural network f_ (x; β). Note that by the definition of H in (4.2), we must have
**HL+1 = H + B for some positive definite matrix B ∈** R[T K][×][T K]. The following lemma shows that
the NTK matrix is close to the matrix defined based on the gradients of the neural network on TK
data points.
**Lemma D.1 (Theorem 3.1 in Arora et al. (2019b)). Let ϵ > 0 and δ ∈** (0, 1). Suppose the activation
function in (2.3) is ReLU, i.e., σl(x) = max(0, x), and the width of the neural network satisfies

_L14_ _L_
_m_ **Ω** _._ (D.2)
_≥_ _ϵ[4][ log]_ _δ_
  

Then for any x, x[′] _∈_ R[d] with ∥x∥2 = ∥x[′]∥2 = 1, with probability at least 1−δ over the randomness
of the initialization of the network weight w it holds that


1 _∂f_ (β, x) 1 _∂f_ (β, x[′])
_√m_ _∂β_ _,_ _√m_ _∂β_


**HL+1(x, x[′])**
_−_ _[≤]_ _[ϵ.]_


-----

Note that in the above lemma, there is a factor 1/[√]m before the gradient. This is due to the additional _m factor in the definition of the neural network in (2.3), which ensures the value of the_

_[√]_
neural network function evaluated at the initialization is of the order O(1).

_Proof of Lemma C.1. Recall that we renumbered the feature vectors_ **xt,k** _t_ [T ],k [K] for all arms
_{_ _}_ _∈_ _∈_
from round 1 to round T as **xi** _i=1,...,T K_ . By concatenating the gradients at different inputs and
_{_ _}_
the gradient in (D.1), we define Ψ ∈ R[T K][×][(][d][+][p][)] as follows.

**_φ(x1; w[(0)])[⊤]_** **_θ0[⊤][g][(][x][1][;][ w][(0)][)]_**

_∂_ . .

_∂β_ **_[θ][⊤][φ][(][x][1][;][ w][)]_**  .. .. 

1 . 1
**Ψ =** _√m_  ..  = _√m_  **_φ(xi; w[(0)])[⊤]_** **_θ0[⊤][g][(][x][i][;][ w][(0)][)]_**  _._

_∂_  .. .. 

 _∂β_ **_[θ][⊤][φ][(][x][T K][;][ w][)]_**  . . 
  φ(xT K; w[(0)])[⊤] **_θ0[⊤][g][(][x][T K][;][ w][(0)][)]_**

 
 

By Applying Lemma D.1, we know with probability at least 1 − _δ it holds that_

_|⟨Ψj∗, Ψl∗⟩−_ **HL+1(xj, xl)| ≤** _ϵ_

for any ϵ > 0 as long as the width m satisfies the condition in (D.2). By applying union bound over
all data points **x1, . . ., xt, . . ., xT K**, we further have
_{_ _}_

**ΨΨ[⊤]** **HL+1** _F_ _TKϵ._
_∥_ _−_ _∥_ _≤_

Note that H is the neural tangent kernel (NTK) matrix defined in (4.2) and HL+1 is the NTK matrix
defined based on all L + 1 layers. By Assumption 4.3, H has a minimum eigenvalue λ0 > 0,
which is defined based on the first L layers of f . Furthermore, by the definition of NTK matrix
in (4.2), we know that HL+1 = H + B for some semi-positive definite matrix B. Therefore, the
NTK matrix HL+1 defined based on all L + 1 layers is also positive definite and its minimum
eigenvalue is lower bounded by λ0. Let ϵ = λ0/(2TK). By triangle equality we have ΨΨ[⊤]
_≻_
which means thatHL+1 −∥ΨΨ[⊤] **Ψ−** is semi-definite positive and thus rankHL+1∥2I ≻ **HL+1 −∥ΨΨ[⊤]** _−_ **HL+1(Ψ∥F) = I ≻ TKH sinceL+1 − m > TKλ0/2I ≻.** 1/2HL+1,

We assume that Ψ can be decomposed as Ψ = PDQ[⊤], where P ∈ R[T K][×][T K] is the eigenvectors
of ΨΨ[⊤] and thus PP[⊤] = IT K, D ∈ R[T K][×][T K] is a diagonal matrix with the square root of
eigenvalues of ΨΨ[⊤], and Q[⊤] _∈_ R[T K][×][(][d][+][p][)] is the eigenvectors of Ψ[⊤]Ψ and thus Q[⊤]Q = IT K.
We useBy definition, we have Q1 ∈ R[d][×][T K] and Q2 ∈ R[p][×][T K] to denote the two blocks of Q such that Q[⊤] = [Q[⊤]1 _[,][ Q]2[⊤][]][.]_


**Q[⊤]Q = [Q[⊤]1** _[,][ Q]2[⊤][]]_ **QQ12**



= Q[⊤]1 **[Q][1]** [+][ Q][⊤]2 **[Q][2]** [=][ I][T K][.]


Note that the minimum singular value ofTK > d. Therefore, it must hold that rank Q(Q1 ∈2) =R TK[d][×][T K] and thusis zero since Q[⊤]2 **[Q] d[2]** is a fixed number and[is positive definite. Let]
**r = (r(x1), . . ., r(xi), . . ., r(xT K))[⊤]** _∈_ R[T K] denote the vector of all possible rewards. We further
define G ∈ R[T Kd][×][p] and Φ ∈ R[T Kd] as follows

**g(x1; w[(0)])** **_φ(x1,1; w0)_**
. .
. .

1  .   . 
**G =** _√m_ gg(x(xT Ki; w...; w[(0)][(0)]) ) _,_ **Φ =** φφ(x(xT,Kt,k...;; w wtT− −1)1) _._ (D.3)

   

and Θ, Θ0 ∈ R[T K][×][T Kd] as follows

**_θ[∗⊤]_** **_θ0[⊤]_**

 ...   ... 

**Θ[∗]** = **_θ[∗⊤]_** _,_ **Θ0 =** **_θ0[⊤]_** _,_ (D.4)

   
   
 ...   ... 
   
 **_θ[∗⊤]_**  **_θ0[⊤]_**
   
   


-----

It can be verified that Ψ = PD[Q[⊤]1 _[,][ Q]2[⊤][]][ and][ PDQ]2[⊤]_ [=][ Θ][0][G][. Note that we have][ Q]2[⊤][Q][2] [is]
positive definite by Assumption 4.3, which corresponds to the neural tangent kernel matrix defined
on the first L layers. Then we can define w[∗] as follows

**w[∗]** = w[(0)] + 1/[√]mQ2(Q[⊤]2 **[Q][2][)][−][1][D][−][1][P][⊤][(][r][ −]** **[Θ][∗][Φ][)][.]** (D.5)

We can verify that

**Θ[∗]Φ +** _mPDQ[⊤]2_ [(][w][∗] _[−]_ **[w][(0)][) =][ r][.]**

_[√]_

On the other hand, we have

_∥w[∗]_ _−_ **w[(0)]∥2[2]** _[≤]_ [1][/m][(][r][ −] **[Θ][∗][Φ][)][⊤][PD][−][1][(][Q]2[⊤][Q][2][)][−][1][D][−][1][P][⊤][(][r][ −]** **[Θ][∗][Φ][)]**

_≤_ 1/m(r − **Θ[∗]Φ)[⊤]H[−][1](r −** **Θ[∗]Φ),**

which completes the proof.


D.2 PROOF OF LEMMA C.2

Note that we can view the output of the last hidden layer φ(x; w) defined in (2.4) as a vector-output
neural network with weight parameter w. The following lemma shows that the output of the neural
network φ is bounded at the initialization.
**Lemma D.2 (Lemma 4.4 in Cao & Gu (2019b)). Let δ ∈** (0, 1), and the width of the neural
network satisfy m _C0L log(TKL/δ). Then for all t_ [T ], k [K] and j [d], we have
_≥_ _∈_ _∈_ _∈_
_φj(xt,k; w[(0)])_ _C1_ log(TK/δ) with probability at least 1 _δ, where w[(0)]_ is the initialization
_|_ _| ≤_ _−_

of the neural network.

p

In addition, in a smaller neighborhood of the initialization, the gradient of the neural network φ is
uniformly bounded.
**Lemma D.3 (Lemma B.3 in Cao & Gu (2019b)). Let ω ≤** _C0L[−][6](log m)[−][3]_ and w ∈ B(w0, ω).
Then for all t ∈ [T ], k ∈ [K] and j ∈ [d], the gradient of the neural network φ defined in (2.4)
satisfies **wφj(xt,k; w)** 2 _C1√Lm with probability at least 1_ _TKL[2]_ exp( _C2mω[2][/][3]L)._
_∥∇_ _∥_ _≤_ _−_ _−_

The next lemma provides an upper bound on the gradient of the squared loss function defined in
(3.3). Note that our definition of the loss function is slightly different from that in Allen-Zhu et al.
(2019b) due to the output layer θi and thus there is an additional term on the upper bound of ∥θi∥2
for all i ∈ [T ].

**Lemma D.4 (Theorem 3 in Allen-Zhu et al. (2019b)). Let ω ≤** _C0δ[3][/][2]/(T_ [9][/][2]L[6] log[3] _m). For all_
**w ∈** B(w[(0)], ω), with probability at least 1 − exp(−C1mω[2][/][3]L) over the randomness of w[(0)], it
holds that

_∥∇L(w)∥2[2]_ _[≤]_ _[C][2][Tm][L][(][w][) sup]d_ _[i][=1][,...,H][ ∥][θ][i][∥]2[2]_ _._

_Proof of Lemma C.2. Fix the epoch number q and we omit it in the subscripts in the rest of the_
proof when no confusion arises. Recall that w[(][s][)] is the s-th iterate in Algorithm 2. Let δ > 0 be any
constant. Let ω be defined as follows.

_ω = δ[3][/][2]m[−][1][/][2]T_ _[−][9][/][2]L[−][6]_ log[−][3](m). (D.6)

We will prove by induction that with probability at least 1 − _δ the following statement holds for all_
_s = 0, 1, . . ., n_


log(TK/δ)

_h + 1_ _,_ for ∀j ∈ [d]; and ∥wq[(][s][)] _−_ **w[(0)]∥≤** _ω._ (D.7)


_φj(x; w[(][s][)])_ _C0_
_≤_


_h=0_


First note that (D.7) holds trivially when s = 0 due to Lemma D.2. Now we assume that (D.7) holds
for all j = 0, . . ., s. The loss function in (3.3) can be bounded as follows.


_qH_

(θi[⊤][φ][(][x][i][;][ w][(][j][)][)][ −] _r[b]i)[2]_ _≤_
_i=1_

X


_qH_

_i=1_ 2(∥θi∥2[2] _[· ∥][φ][(][x][i][;][ w][(][j][)][)][∥]2[2]_ [+ 1)][.]

X


_L(w[(][j][)]) =_


-----

By the update rule of θt, we have


_t_ _−1_ _t_

**_θt_** 2 = _λI +_ **_φ(xi; wi_** 1)φ(xi; wi 1)[⊤] **_φ(xi; wi_** 1)r 2d, (D.8)
_∥_ _∥_  _i=1_ _−_ _−_  _i=1_ _−_ 2 _≤_

X X

where the inequality is due to Lemma C.5, which combined with (D.7) immediately impliesb


2
_C1Td[3]_ log(TK/δ) log[2] _n._ (D.9)
_≤_



(w[(][j][)]) _C1Td[3]_ log(TK/δ)
_L_ _≤_


_h + 1_


_h=0_


Substituting (D.8) and (D.9) into the inequality in Lemma D.4, we also have
**w[(][j][)][]2** _dTm_ (w[(][j][)]) _C3d[2]T log(n)_ _m log(TK/δ)._ (D.10)
_∇L_ _[≤]_ _[C][2]_ _L_ _≤_
q

Now we consider w  [(][s][+1)]. By triangle inequality we have p


**w[(][s][+1)]** **w[(0)]**
2
_−_ _[≤]_


_s_

**w[(][j][+1)]** **w[(][j][)]**
2
_−_

_j=0_

X

_s_

_η_ **w[(][j][)][]**
2
_∇L_
_j=0_

Xs  

_ηd[2]T log(n)_ _m log(TK/δ),_ (D.11)

_j=0_

X p


where the last inequality is due to (D.10). If we choose the step size ηq in the q-th epoch such that


_,_ (D.12)
_m log(TK/δ)_


_η ≤_


_d[2]Tn log(n)_


then we have **wq[(][s][+1)]** **w[(0)]** 2 _ω. Note that the choice of m, ω satisfies the condition in_
Lemma C.3. Thus we know ∥ _−_ _φj(∥x; ≤ w) is almost linear in w, which leads to_

_φj(x; w[(][s][+1)])_ _φj(x; w[(][s][)]) +_ _φj(x; w[(][s][)]), w[(][s][+1)]_ **w[(][s][)]** + C5ω[4][/][3]L[3]d[−][1][/][2][p]m log m
_|_ _| ≤|_ _⟨∇_ _−_ _⟩|_

_s_

_C_ log(TK/δ) + η√dm (w[(][s][)]) 2 + 2C5ω[4][/][3]L[3]d[−][1][/][2][p]m log m

_≤_ _h + 1_ _∥∇L_ _∥_

_h=0_ p

Xs

_C0_ log(TK/δ)

+ C3η√dm _CT_ [2]d[4]m log(TK/δ) log n

_≤_ _h + 1_

_h=0_ p

X p

+ 2C5ω[4][/][3]L[3]d[−][1][/][2][p]m log m

_s_

_C0_ log(TK/δ) _√dm_

= + _[ω]_ + 2C5ω[4][/][3]L[3]d[−][1][/][2][p]m log m, (D.13)

_h + 1_ _n_

_h=0_ p

X

where in the second inequality we used the induction hypothesis (D.7), Cauchy-Schwarz inequality
and Lemma D.3, and the third inequality is due to (D.10). Note that the definition of ω in (D.6)
ensures that ω√dm < 1/2 and ω[4][/][3]L[3]d[−][1][/][2][√]m log m ≤ _m[−][1][/][6]T_ _[−][6]L[−][5]d[−][1][/][2][√]log m ≤_ 1/n as

long as m ≥ _n[6]. Plugging these two upper bounds back into (D.13) finishes the proof of (D.7)._

Note that for any t ∈ [T ], we have wt = wq[(][n][)] for some q = 1, 2, . . .. Since we have wt ∈ B(w, ω),
the gradient g(x; w[(0)]) can be directly bounded by Lemma D.3, which implies **g(x; w[(0)])** _F_
_C6√dLm. Applying (D.7) with s = n, we have the following bound of the neural network function ∥_ _∥_ _≤_

**_φ(x; wq[(][n][)]) = φ(x; wt) for all t in the q-th epoch_**

**_φ(x; wt)_** 2 _C0_ _d log(n) log(TK/δ),_
_∥_ _∥_ _≤_

which completes the proof. In this proof, {Cj >p 0}j=0,...,6 are constants independent of problem
parameters.


-----

D.3 PROOF OF LEMMA C.4

The following lemma characterizes the concentration property of self-normalized martingales.
**Lemma D.5 (Theorem 1 in Abbasi-Yadkori et al. (2011)). Let {ξ}t[∞]=1** [be a real-valued stochastic]
process and **xt** _t=1_ [be a stochastic process in][ R][d][. Let][ F][t] [=][ σ][(][x][1][, . . .,][ x][t][+1][, ξ][ −] [1][, . . ., ξ][t][)][ be a]
_{_ _}[∞]_
_σ-algebra such that xt and ξt are Ft−1-measurable. Let At = λI +_ _s=1_ **[x][s][x]s[⊤]** [for some constant]
_λ > 0 and St =_ _s=1_ _[ξ][s][x][i][. If we assume][ ξ][t][ is][ ν][-subGaussian conditional on][ F][t][−][1][, then for any]_
_η_ (0, 1), with probability at least 1 _δ, we have_ [P][t]
_∈_ _−_

[P][t] det(At)1/2 det(λI)−1/2

_∥St∥A[2]_ _[−]t_ [1] _≤_ 2ν[2] log _δ_ _._
 

_Proof of Lemma C.4. Let Φt = [φ(x1,a1_ ; w0), . . ., φ(xt,at ; wt 1)] R[d][×][t] be the collection of
_−_ _∈_
feature vectors of the chosen arms up to time t and **rt = (r1, . . .,** _rt)[⊤]_ be the concatenation of all
received rewards. According to Algorithm 1, we have At = λI + ΦtΦ[⊤]t [and thus]

**_θt = A[−]t_** [1][b][t] [= (][λ][I][ +] b[ Φ][t][Φ][⊤]t b[)][−][1][Φ][t][b]r bt.

By Lemma C.1, the underlying reward generating function rt = r(xt,at ) = E[r(xt,at )|xt,at ] can be
rewritten as

_rt = ⟨θ[∗], φ(xt,at_ ; wt−1)⟩ + θ0[⊤][g][(][x][t,a]t [;][ w][(0)][)(][w][∗] _[−]_ **[w][(0)]b[)][.]**

By the definition of the reward in (2.1) we have _rt = rt + ξt. Therefore, it holds that_

_t_

**_θt = A[−]t_** [1][Φ][t][Φ]t[⊤][θ][∗] [+][ A]t[−][1] **_φ(xs,as_** ; w bs−1)(θ0[⊤][g][(][x][s,a]s [;][ w][(0)][)(][w][∗] _[−]_ **[w][(0)][) +][ ξ][s][)]**

_s=1_

X


= θ[∗] _−_ _λA[−]t_ [1][θ][∗] [+][ A]t[−][1]


**_φ(xs,as_** ; ws−1)(θ0[⊤][g][(][x][s,a]s [;][ w][(0)][)(][w][∗] _[−]_ **[w][(0)][) +][ ξ][s][)][.]**
_s=1_

X


Note that At is positive definite as long as λ > 0. Therefore ∥· ∥At and ∥· ∥At are well defined
norms. Then for any δ ∈ (0, 1) by triangle inequality we have

**_θt_** **_θ[∗]_** **A[−]t** [1][Φ][t][Θ][t][G][t][(][w][∗] _[−]_ **[w][(0)][)][∥][A]t** **A[−]t** [1] + **Φtξt** **A−t** 1
_∥_ _−_ _−_ _[≤]_ _[λ][∥][θ][∗][∥]_ _∥_ _∥_

det(At)1/2 det(λI)−1/2

_ν_ 2 log + λ[1][/][2]M
_≤_ s _δ_

 

holds with probability at least 1 − _δ, where in the last inequality we used Lemma D.5 and the fact_
that **_θ[∗]_** **A−t** 1 _λ[−][1][/][2]_ **_θ[∗]_** 2 _λ[−][1][/][2]M by Lemma C.1. Plugging the definition of Φt, Θt and Gt_
_∥_ _∥_ _≤_ _∥_ _∥_ _≤_
and apply Lemma C.6, we further have

_t_

_t_ **_φ(xs,as_** ; ws−1)θ0[⊤][g][(][x][s,a]s [;][ w][(0)][)(][w][∗] _[−]_ **[w][(0)][)]**

_s=1_ **At**

X

**_[θ][t]ν[ −]_** **_[θ]2[∗]_** _d[−] log(1 +[A][−][1]_ _t(log HK)/λ) + log 1/δ_ + λ[1][/][2]M,

_≤_
q

where we used the fact that  _φ(x; w)_ 2 _C[√]d log HK by Lemma_ C.2.
_∥_ _∥_ _≤_

D.4 PROOF OF LEMMA C.5


We now prove the technical lemma that upper bounds ∥A[−]t [1] _ts=1_ **_[φ][s][ζ][s][∥][2][.]_**
P

_Proof of Lemma C.5. We first construct auxiliary vectors_ **_φt_** _∈_ R[d][+1] and matrices Bt _∈_
R[(][d][+1)][×][(][d][+1)] for all t = 1, . . . in the following way:

**_φt =_** _G[−][1]φt_ _,_ **Bt =** [e]A[−]t [1] **0d** _,_ (D.14)

1 _G[−][2]_ **_φt_** 2 **0[⊤]d** 0

 _−_ _∥_ _∥[2]_  
e p


-----

where 0d ∈ R[d] is an all-zero vector. Then by definition we immediately havet _t_
_t_ **_φsζs_** = **_φsζs_** _._ (D.15)

_s=1_ 2 _s=1_ 2

X X

For all s = 1, 2 . . ., let {βs,j}j[d]=1[+1][A][be the coefficients of the decomposition of][−][1] **[B][t]** e _[ U][ −][1][ζ][s][ e]φs on the natural_
basis. Specifically, let {e1, . . ., ed+1} be the natural basis of R[d][+1] such that the entries of ej are all
zero except the j-th entry which equals 1. Then we have

_d_

_U_ _[−][1]ζsφs =_ _βs,jej,_ _∀s = 1, 2, . . ._ (D.16)

_j=1_

X

We can conclude that _βs,j_ 1 since[e] _ζs_ _U and_ **_φs_** 2 1. Moreover, it is easy to verify that
_|_ _| ≤_ _|_ _| ≤_ _∥_ [e] _∥_ _≤_
_∥φ[e]t∥2 = 1 for all t ≥_ 1. Therefore, we have

_t_ _t_

**_φsζs_** = **_φsφ[⊤]s_** **_φsζs_**
_s=1_ 2 _s=1_ 2

X X

**[B][t]** e **[B][t]** _t_ e [e] [e] _d_

= **_φsφ[⊤]s_** _[U]_ _βs,jej_

_s=1_ _j=1_ 2

X X

**[B][t]** _d_ e [e]t

= U **Bt** **_φsφ[⊤]s_** _[β][s,j][e][j]_

_j=1_ _s=1_ 2

X X

_d_ _t_ e [e]

_U_ **_φsφ[⊤]s_** _[β][s,j]_
_≤_ _j=1_ _s=1_ 2

X X

_d_ **[B][t]** _t_ e [e]

= U _t_ **_φsφ[⊤]s_** _[β][s,j]_ _,_ (D.17)

_j=1_ _s=1_ 2

X X

where the inequality is due to triangle inequality and the last equation is due to the definition of[A][−][1] **_φt_**
and Bt in (D.14). For each j = 1, . . ., d + 1, we have

_t_

[e]
_t_ **_φsφ[⊤]s_** _[β][s,j]_ = _t_ **_φsφ[⊤]s_** _[β][s,j]_ [+][ A][−]t [1] **_φsφ[⊤]s_** _[β][s,j]_

Xs=1 2 _s∈[tX]:βs,j_ _≥0_ _s∈[tX]:βs,j_ _<0_ 2

**[A][−][1]** **[A][−][1]**

_t_ **_φsφ[⊤]s_** _[β][s,j]_ + _t_ **_φsφ[⊤]s_** [(][−][β][s,j][)] _._
_≤_ _s∈[tX]:βs,j_ _≥0_ 2 _s∈[tX]:βs,j_ _<0_ 2

(D.18)

**[A][−][1]** **[A][−][1]**

Since we have _βs,j_ 1, it immediately implies
_|_ _| ≤_


**_φsφ[⊤]s_**
_s=1_ _[≻]_

X

_t_

**_φsφ[⊤]s_**
_s=1_ _[≻]_

X


**_φsφ[⊤]s_** _[β][s,j][,]_
_s∈[tX]:βs,j_ _≥0_


**At = λI +**

**At = λI +**


**At = λI +** Xs=1 **_φsφ[⊤]s_** _[≻]_ _s∈[tX]:βs,j_ _<0_ **_φsφ[⊤]s_** [(][−][β][s,j][)][.]

Further by the fact that **A[−][1]B** 2 1 for any A **B** 0, combining the above results with (D.18)
yields _∥_ _∥_ _≤_ _≻_ _⪰_

_t_

_t_ **_φsφ[⊤]s_** _[β][s,j]_ 2.

_s=1_ 2 _≤_

X

Finally, substituting the above results into (D.17) and (D.15) we have

**[A][−][1]** _t_

_t_ **_φsζs_** 2Ud,

_s=1_ 2 _≤_

X

which completes the proof.

**[A][−][1]**


-----

