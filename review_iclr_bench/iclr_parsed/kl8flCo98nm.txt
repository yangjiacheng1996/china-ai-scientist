# LEARNING DISTRIBUTIONS GENERATED BY SINGLE- LAYER RELU NETWORKS IN THE PRESENCE OF AR## BITRARY OUTLIERS

**Anonymous authors**
Paper under double-blind review

ABSTRACT

We consider a set of data samples such that a constant fraction of the samples
are arbitrary outliers and the rest are the output samples of a single-layer neural
network (NN) with rectified linear unit (ReLU) activation. The goal of this paper
is to estimate the parameters (weight matrix and bias vector) of the NN assuming
the bias vector to be non-negative. Our proposed method is a two-step algorithm.
We first estimate the norms of the rows of the weight matrix and the bias vector
using the gradient descent algorithm. Here, we also incorporate either the median
or the trimmed mean based filters to mitigate the effect of the arbitrary outliers.
Next, we estimate the angles between any two row vectors of the weight matrix.
Combining the estimates of the norms and the angles, we obtain the final estimate
of the weight matrix. Our main contribution is the sample complexity for robust
estimation rather than the algorithm itself. Here, we prove that Ω( _ϵp[1][4][ log][ d]δ_ [)][ sam-]

ples are sufficient for our algorithm to estimate the NN parameters within an error
of ϵ with probability 1 − _δ when the probability of a sample being uncorrupted is_
_p and the problem dimension is d. Our theoretical and simulation results provide_
insights on how the estimation of the NN parameters depends on the probability of
a sample being uncorrupted, the number of samples, and the problem dimension.


1 INTRODUCTION

A fundamental challenge in machine learning and statistics is to estimate a high dimensional distribution given a set of observed data samples from the distribution. One solution technique is the
deep generative method that models the unknown distribution as the output distribution of a neural
network (NN) when the input of the NN is drawn from a known distribution like standard Gaussian
which is motivated from the case of image generation by generative adversarial networks (GANs)
(Goodfellow et al. (2014); Arjovsky et al. (2017); Radford et al. (2015); Kingma & Welling (2013);
Van Oord et al. (2016)). Then, the unknown distribution is estimated by learning the NN parameters
from the data samples. Several approaches like GANs (Goodfellow et al. (2014); Arjovsky et al.
(2017); Radford et al. (2015)), variational autoencoders (Kingma & Welling (2013)), and autoregressive models (Van Oord et al. (2016)) have been proposed to train the NN. However, they do not
provide theoretical guarantees for the parameter learning. So, we address the open problem of computing the sample complexity for the parameter estimation of a single-layer NN with rectified linear
unit (ReLU) activation using a corrupted dataset in the unsupervised learning framework. Here, the
corrupted dataset refers to the model wherein the output samples consists of a fraction of arbitrary
outliers introduced by an adversary (Byzantine). Motivated by the deep generative models, we consider the unsupervised learning framework, i.e., we assume the knowledge of the output samples
but, the corresponding inputs are unknown and drawn from the standard Gaussian distribution.

We start with a brief review of the related literature. The NN parameter estimation has been considered under both supervised and unsupervised learning frameworks. The estimation rely on the
stochastic gradient descent (SGD)-based algorithms (Goel et al. (2018); Allen-Zhu et al. (2019);
Chen et al. (2020); Oymak (2019); Mazumdar & Rawat (2018); Wu et al. (2019); Lei et al. (2020));
or the gradient descent (GD)-based approach (Cao & Gu (2019); Du et al. (2019)). A major shortcoming of these works is that they assume that the available samples are not corrupted by noise
or outliers. Several works in the literature have addressed the problem of corrupted samples in the


-----

context of supervised learning. The existing works have considered the learning of the parameters
of a NN with ReLU activation using both SGD-based (Bakshi et al. (2019); Goel et al. (2019);
Mukherjee & Muthukumar (2020)) and GD-based algorithms(Zhang et al. (2019); Frei et al. (2020);
Vempala & Wilmes (2019)). However, to the best of our knowledge, the estimation of a NN in the
presence of noise or outliers in the unsupervised learning framework has not been studied in the
literature.

Our setting of unsupervised parameter estimation is also related to the area of robust statistics as
described below. We recall that the goal of our paper is to estimate the parameters of a NN using corrupted output samples assuming that the input distribution is Gaussian. Mathematically, this
problem is equivalent to estimating the parameters of a truncated Gaussian distribution (see Section 2 for details). The area of robust statistics also deals with the estimation of high dimensional
distributions like Gaussian, Gaussian product, and Gaussian mixture distributions where a constant
fraction of the samples were corrupted by noise (Huber (2004); Hampel et al. (2011); Lai et al.
(2016); Diakonikolas et al. (2019); Diakonikolas & Kane (2019); Kane (2021)). However, in this
paper, we consider parameter estimation from truncated Gaussian samples in the adversarial setting
which has not been studied in the literature.

We present our major contributions as the following.

-  Parameter Estimation (Sections 2 and 3): We estimate the parameters (weight matrix and bias
vector) of a single-layer NN with ReLU in the unsupervised learning framework where each
output sample can potentially be an arbitrary outlier with a fixed probability. We propose an
estimation algorithm that has two steps: (i) we estimate the row norms of the weight matrix and
the bias vector using GD along with either the median or trimmed mean-based filters to mitigate
the effect of arbitrary outliers, and (ii) we estimate the angles between any two row vectors of
the weight matrix using a simple geometric result (Williamson & Shmoys, 2011, Lemma 6.7).

-  Theoretical guarantees (Section 4): We show that the proposed algorithm requires Ω( _ϵp[1][4][ log][ d]δ_ [)]

samples to estimate the network parameters within an error of ϵ with probability 1 − _δ when the_
probability of a sample being uncorrupted is p. Here, ϵ > 0 represents the estimation error.



-  Empirical validation (Section 5): We evaluate the performance of our algorithm empirically in
terms of its relation to the probability of a sample being uncorrupted, number of samples, and
dimension (see Figs. 1 and 2). We see that our proposed filtering schemes ensure robustness
to the arbitrary outliers. Also, the performance of our algorithm improves with the increase in
the probability of a sample being uncorrupted and the number of samples, as expected. Further,
we note that the estimation error increases slowly as the dimension grows for a fixed number of
samples in the presence of arbitrary outliers. These observations from the empirical results are
consistent with our theoretical results.

To summarize, in this paper, we estimate the parameters of a single-layer NN with ReLU activation
in the presence of arbitrary outliers. The results obtained in this paper provide insights to generalize
to a multi-layer NN with various activation functions and obtain generalization bounds.

2 LEARNING A NEURAL NETWORK MODEL USING CORRUPTED OUTPUT
SAMPLES

We consider a single-layer ReLU NN. Let the weight matrix of the NN be denoted by W ∈ R[d][×][k]
and the bias vector by b ∈ R[d]. The input to the NN is denoted by the latent variable z ∈ R[k].
We assume that the variable z is drawn from the standard Gaussian distribution. The Gaussianity
assumption is for mathematical tractability, and it is motivated by a popular generative approach to
estimate a high-dimensional distribution from observed samples in the case of image generation by
GANs Goodfellow et al. (2014); Radford et al. (2015); Arjovsky et al. (2017). Thus, the output of
the network is the random vector x ∈ R[d] given by

**x = ReLU(Wz + b), where z** (0, Ik), (1)
_∼N_

where N (0, Ik) denotes the Gaussian distribution with mean 0 and covariance matrix Ik which is
the identity matrix of size k _×k. Let the distribution of the random vector x be denoted by D(W, b)._
Then, our goal is to estimate the unknown parameters W and b of the distribution D(W, b) using


-----

the knowledge of n output samples x1, x2, . . ., xn. Here, we assume that a fraction of the samples
are arbitrary outliers. The distribution of the observed samples are modeled using the Huber’s p_contamination model_ [1] defined as the following.
**Definition 2.1 (Huber’s p-Contamination Model Huber (1964)). The observed samples are said to**
be following Huber’s contamination model if any given sample is drawn from the true distribution
_D and an arbitrary distribution G with probability p, and 1 −_ _p, respectively. In other words, the_
observed samples are drawn from the mixture distribution, _p(W, b) given by_
_D_

_p(W, b) = p_ (W, b) + (1 _p)_ _._ (2)
_D_ _D_ _−_ _G_

Note that if b has large negative values, then most of the output samples are zeros due to the ReLU
operation. Further, if W is a zero matrix, then any negative coordinate of b cannot be identified as
it is reset to zero after the ReLU operation. Further, in Wu et al. (2019), the authors have shown that
if b ∈ R[d] then exponentially large number of samples are required to estimate the bias. This holds
true in our case as in the presence of arbitrary outliers if b has large negative values, then we would
require larger number of samples to estimate the bias vector b compared to that in Wu et al. (2019).
Hence, we also assume b to be non-negative.

Our estimation problem is challenging due to two reasons: 1) ReLU operation is not invertible, and
therefore, estimation of W and b utilizing maximum likelihood of the probability density function
of the random vector x is intractable; 2) the distribution of the corrupted samples is unknown. We
tackle these issues using a new formulation combining the GD algorithm and a filtering technique.
Note that the GD algorithm is similar to that in Wu et al. (2019) wherein the authors use the SGD
algorithm. The proposed algorithm is presented in the next section.

3 ESTIMATION OF PARAMETERS OF SINGLE-LAYER NN

To design our estimation algorithm, we first note that the weight matrix W may not be identifiable
from the distribution D(W, b). This is because of the fact that for any matrices W1 and W2 if
**W1W1[T]** [=][ W][2][W]2[T] [then there exists a unitary matrix][ Q][ such that][ W][2] [=][ W][1][Q][. As][ z][ ∼N] [(][0][,][ I][k][)][,]
we have Qz (0, Ik). Hence, for any vector b, we have (W1, b) = (W2, b) (Wu et al.
_∼N_ _D_ _D_
(2019)). Since our goal is to learn the distribution, learning either W1 or W2 is sufficient. In
short, we focus on the learnability of the underlying distribution and not the learnability of the NN
parameters. Therefore, our proposed algorithm estimates WW[T] _∈_ R[d][×][k] and b ∈ R[d] from the
observed samples from _p(W, b)._
_D_

Next, we note that (i, j)-th entry **WW[T][]** (i, j) of the symmetric matrix WW[T] is ∥W(i, :
) 2 **W(j, :)** 2 cos(θij), where θij = θji is the angle between vectors W(i, :) and W(j, :) which
_∥_ _∥_ _∥_  
are the i-th and j-th rows of matrix W, respectively. Also, note that the ℓp norm of a vector is
given by **x** _p = ([P]i_
_∥_ _∥_ _[|][x][(][i][)][|][p][)][1][/p][. Thus, we can construct the matrix][ WW][T][ using the row norms]_

_{∥W(i, :)∥2}i[d]=1_ [and angles][ {][θ][ij][}]i,j[d] =1[.]

Our estimation algorithm consists of two steps: 1) estimation of row norms of W and bias vector b;
and 2) estimation of angles between the row vectors of W.

_iTo estimate-th coordinate can be written as ∥W(i, :)∥2 ∈_ R and b(i) ∈ R, for i ∈ [d] = {1, 2, . . ., d}, suppose x ∼D(W, b). Its

**x(i) = ReLU(W(i, :)[T]z + b(i)),** (3)

where W(i, :)[T]z + b(i) ∼N (b(i), ∥W(i, :)∥2[2][)][. Thus, to compute][ b][(][i][)][ and][ ∥][W][(][i,][ :)][∥][2]2[, it is]
enough to consider the i-th entries of the observed samples. Also, in (3), the ReLU operator sets
**x(i) to zero if W(i, :)[T]z + b(i) is negative. Consequently, we estimate b(i) and ∥W(i, :)∥2[2]** [using]
the positive entries of the observed samples. Hence, estimation of b(i) and ∥W(i, :)∥2 is equivalent
to estimating the parameters of a one-dimensional normal distribution using positive samples, i.e.,
the samples that belong to the set R>0 := {x(i) ∈ R : x(i) > 0}. We use µ[∗] = b(i) and σ[∗][2] =
_∥W(i, :)∥2 to denote the parameters of this one-dimensional normal distribution. The estimation of_
_µ[∗]_ and σ[∗][2] using corrupted samples is discussed next.

1Our algorithm and its analysis follow even if the outliers are not sampled from the same arbitrary distributions G. However, we need to assume that all the true samples come from the same distribution D.


-----

**Algorithm 1: Learning one-layer ReLU**
NN with outliers

**1 forInput: i ∈ Samples[d] do** **x1, . . ., xn ∈** R[d]

**2** _>0_ **xm, m** [n] : xm(i) > 0

**3** **vXˆ = ProjGD ←{** ( _> ∈0)_ _}_
_X_

**4** **Σˆ** _i,i_ 1/vˆ(1)

**5** **bˆ(i) ← ←** max{0, ˆv(2)/vˆ(1)}

**6 for i < j ∈** [d] do

**7** Compute _θ[ˆ]ij using (11)_

**8** **Σˆ** (i, j) **Σˆ** (i, i) Σ[ˆ] (j, j) cos(θ[ˆ]ij)
_←_

**9** **Σˆ** (j, i) ← **Σq[ˆ]** (i, j)

**Output:** **Σ[ˆ]** _∈_ R[d][×][d], **b[ˆ] ∈** R[d]


**Algorithm 2: ProjGD**
**Input: Samples X>0, Parameters T**,γt, and nb

**1 Compute {xj(i)}j[n]=1[b]** [from][ X][>][0][ using (7)]

**2 ˜gx** filter **xj(i)** _j=1_
_←_ _{_ _}[n][b]_

**3 v0** [0 0] 

**4 for ← t = 1, 2, . . ., T do**

**5** _µ ←_ **vt−1(2)/vt−1(1)**

**6** _σ[2]_ _←_ 1/vt−1(1) T

**7** **g˜z** (˘σ[2] + ˘µ[2])/2 ˘µ using (5) and (6)
_←_ _−_

**8** **g˜t** **g˜x + ˜gz**

**109** **vv˜tt ← ←vPt(˜−v1t −) using (10)γt−1g˜t−1** 
_←_

**Output: vT ∈** R[2]


We determine the parameters of the univariate normal distribution using maximum likelihood estimation (Daskalakis et al. (2018)) given by arg minv ℓ(v) where ℓ(v) is the expected negative
log-likelihood with respect to x(i) and v = [1/σ[2]; µ/σ[2]] ∈ R[2]. This optimization problem can
be solved using a learning algorithm like GD or SGD. SGD utilizes only one sample for the gradient computation which introduces large variance due to the stochasticity and low reliability in the
presence of arbitrary outliers. Thus, we use GD that utilizes all the samples for the gradient.

To derive our algorithm based on GD, we compute the gradient (Daskalakis et al. (2018)), ∇ℓ(v) =
Ex,z[g(i)] where

T T
**g(i) = gx(i) + gz(i) =** **x[2](i)/2** **x(i)** + **z[2](i)/2** **z(i)** _._ (4)
_−_ _−_

Here, x(i) (µ[∗], σ[∗][2]; R>0) and z(i) (µ, σ[2]; R>0). Also, _µ[∗]_ and σ[∗][2] are the parameters
_∼N_ _∼N_
of the true distribution, and µ and σ[2] are functions of v. In (4), Ez[gz(i)] admits a closed form
T
expression (Johnson et al. (1995)), ˜gz = Ez[gz(i)] = _−(˘σ[2]_ + ˘µ[2])/2 _µ˘_ . Here, we define

_φ(_ _µ/σ)_ 
_µ˘ = E[z(i)_ 0 < z(i) < ] = µ + _−_ (5)
_|_ _∞_ 1 Φ( _µ/σ)_ _[σ,]_

_−_ _−_

2[!]

_φ(_ _µ/σ)_ _φ(_ _µ/σ)_

_σ˘[2]_ = Var(z(i) 0 < z(i) < ) = σ[2] 1 _−_ _−_ _,_ (6)
_|_ _∞_ _−_ _σ[µ]_ 1 Φ( _µ/σ)_ 1 Φ( _µ/σ)_

_−_ _−_ _[−]_  _−_ _−_ 

where φ(·) and Φ(·) denote the probability density function and the cumulative distribution function
of the standard normal distribution, respectively. We also need Ex[gx(i)] to compute Ex,z[g(i)].
Since the value of Ex[gx(i)] is not known, we use the sample mean to estimate Ex[gx(i)]. For this,
all the positive samples are partitioned into nb batches. Let Sj be the j-th batch. We then compute
the vector xj(i) ∈ R[2] as follows:

1
**xj(i) =** **x(i)** _Sj_ **[x][(][i][)][2][/][2]** **x(i)** _Sj_ **[x][(][i][)][][T][ .]** (7)

_Sj_ _∈_ _−_ [P] _∈_
_|_ _|_
P

We then combine the nb vectors {xj(i)}j[n]=1[b] [to estimate][ E][x][,][z][[][g][(][i][)]][. Recall that the observed samples]
are from the mixture distribution _p(W, b). Therefore, to compute the expectation with respect to_
_D_
the true distribution D(W, b), a filter is applied on this set of nb vectors to mitigate the effect of the
arbitrary outliers which is a common tool in the robust statistics literature and obtain Ex[gx(i)]. We
consider two filters: median and trimmed mean.

_Median: The median based filter possesses the following robustness property. Typically, a median_
is the value separating the higher half from the lower half of a given set of points. If more than half
of a given set of points are in [−M, M ] for some M > 0, then their median must be in [−M, M ].
Thus, the median of {xj(i)}j[n]=1[b] [is the vector computed either from all the true samples or outliers]
whose magnitudes are comparable to the true samples.

_Trimmed Mean: The trimmed mean removes the vectors among the set of nb vectors with relatively_
large and small values and computes the estimate of Ex[gx(i)] as the mean of the remaining vectors.


-----

Here, we use the parameter β to indicate the number of vectors to be discarded. This technique
prunes the vectors xj(i), j [nb] that are computed from the batches with outliers having relatively
_∈_
high or low magnitudes. Therefore, we obtain the gradient estimate as

_nb_ T

**g˜(i) = ˜gx(i) + ˜gz(i) = filter** **xj(i)** + (˘σ[2] + ˘µ[2])/2 _µ˘_ _,_ (8)

_j=1_

n o   

where filter is either median or trimmed mean, and ˘µ and ˘σ[2] are given in (5) and (6), respectively.

Having computed the gradient estimate, we next present the proposed GD algorithm. In the t-th
iteration, the algorithm performs the following three steps: The first step is Gradient Computation
_Stepsecond step is where the gradient Update Step ˜gt where the gradient computed in the previous step is used to perform the ∈_ R[2] is computed from the observed positive samples using (8). The
GD update:
**v˜t = vt** 1 _γt_ 1g˜t 1, (9)
_−_ _−_ _−_ _−_
where γt 1 > 0 is the diminishing step size. The last step is Projection Step where the objective
_−_
function l(v) is a strongly convex function of v, if v belongs to a bounded region. To control the
strong-convexity of the objective function, we project the update vector v into the domain Dr =
**v ∈** R[2] : 1/r ≤ **v(1) ≤** _r, |v(2)| ≤_ _r_ . Thus, the projection is
 _P_ (v) = [min max **v(1), 1/r** _, r_ min max **v(2),** _r_ _, r_ ] . (10)

_{_ _{_ _}_ _}_ _{_ _{_ _−_ _}_ _}_

The GD algorithm is run for T iterations where T is the algorithm parameter. The overall algorithm
is summarized in Algorithm 2. This completes the first step of our algorithm that uses the GD
algorithm to obtain the estimate of the bias vector and the row norms of the weight matrix.

Finally, using the estimates **b[ˆ] and** **Σ[ˆ]** obtained using the GD algorithm, we estimate _θ[ˆ]ij similar to Wu_
et al. (2019) using (Williamson & Shmoys, 2011, Lemma 6.7). Specifically, we have


1(xm(i) > **b[ˆ](i))1(xm(j) >** **b[ˆ](j))**
_m=1_

X


(11)


_θˆij = π −_ 2π


where 1(·) denotes the indicator function and **b[ˆ] is the estimate of the bias vector computed in the**
previous step using the projected GD algorithm.

The overall algorithm is given in Algorithm 1 where **Σ[ˆ]** is the estimate of WW[T]. Here, Steps 1-5
estimate the row norms of W and b, and Steps 6-9 estimate the angles between any two rows of W.

Our algorithm is similar to that in Wu et al. (2019) except for the filter which is also a standard
technique in robust statistics. However, our algorithm is different from the algorithm in Wu et al.
(2019) in the following aspects. We do not rescale the unknown parameters (see Step 1 of Algorithm
2 in Wu et al. (2019)). We employ median or trimmed mean based filter to compute the full gradient
and utilize the GD algorithm instead of SGD used in Wu et al. (2019). We note that our main
contribution is the analysis rather than the algorithm which is presented in the next section.

4 THEORETICAL GUARANTEES FOR PARAMETER ESTIMATION

In this section, we provide our main result which characterizes the sample complexity for robust
estimation of NN parameters. We address the general problem of learning the distribution generated
by a NN in the presence of arbitrary outliers (p ≤ 1). Hence, the proof techniques which have been
devised for only the special case of p = 1 in Wu et al. (2019), are not applicable to our setup due
to the presence of corrupted samples. Therefore, we develop novel proof techniques for performing
the parameter estimation in the presence of arbitrary outliers. To arrive at the main result, we first
present the error bounds for the two steps of Algorithm 1, which are described in Sec. 3.

The following proposition provides the estimation error bounds for the first step, i.e., the estimation
of the bias vector b and the norms of the row vectors of WW[T]. The proof relies on the fact that
the objective function is η-strongly-convex and L-smooth, and we show that the parameters η, L are
functions of r. Using the properties of the objective function, we the bound on the error between the
true gradient ∇l(v) and the estimated gradient ˜gt computed from the output samples with arbitrary
outliers, obtained in terms of parameter ϵ. This bound leads to the following result that shows that


-----

we can estimate b(i) within an additive error of Ξ∥W(i, :)∥ and Σ(i, i) within an additive error of
Ξ∥W(i, :)∥[2] where Ξ is a function of the above parameters.

**vProposition 4.1.[∗]∥** = V and choose the step size Suppose we initialize the projected gradient step (Algorithm 2) such that γt = γ and the number of iterations as T _. Then, there exists ∥v0 −_
_γ > 0 such that for any δ, ϵ ∈_ (0, 1), the output ( Σ[ˆ] (i, i), **b[ˆ](i)) of the projected GD step of our**
_algorithm with median-based filter for any i ∈_ [d] satisfies

_|Σ[ˆ]_ (i, i) −∥W(i, :)∥[2]| ≤ Ξ∥W(i, :)∥[2] _and |b[ˆ](i) −_ **b(i)| ≤** Ξ∥W(i, :)∥, (12)

_with probability at least 1 −_ _δ if the number of data samplesT_ _n ≥_ _p[2](pΨϵ1−1/2)[2][ log][ 1]δ_ _[. Here, the]_

_error bound is Ξ = V_ 1 _η+LL_ + [2][ϵ][(]Lη[L][+][η][)] _with η, L > 0 being constants that depend only on_
_−_

_the parameter r. Also, p is the probability of a sample being uncorrupted and_ Ψϵ [1/2p, 1] is such
_∈_
_that O(ϵ[3])_ Ψϵ _O([√]ϵ) with ϵ > O_ _p[−][1][/][3][]_ _where the order constants depend only on r._
_≤_ _≤_

The two terms in the error bound Ξ in Proposition 4.1 can be interpreted as the following. The 

_T_

first term V 1 _η+LL_ captures the convergence error due to the GD algorithm. For any upper
_−_

bound on the convergence error  Ω _> 0, Algorithm 1 runs in time (log Ω_ log V )/ log _η+ηL_ . This
_−_

relation shows that smaller convergence errors require more iterations. The second term [2][ϵ][(]Lη[L][+][η][)]

captures the error due to the gradient computed using the observed samples. Two factors contribute
to this error: 1) difference between the (true) sample mean and the true mean, and 2) the arbitrary
outliers. Also, the difference between the sample mean and true mean decreases with the number of
samples. We notice that the sample complexity depends on Ψϵ which is the probability of a sample
being uncorrupted deviating from its mean and variance by ϵ and as Ψϵ _O([√]ϵ). Consequently,_
for smaller error ϵ, we need a large number of samples n = Ω( _ϵp[1][4][ log] ≤[ d]δ_ [)][. We also note that the]

lower bound on ϵ decreases with p.

The following proposition bounds the estimation error in the second step, i.e., the estimation of the
angle between any two row vectors W(i, :) and W(j, :) (where i ̸= j ∈ [d]). The result shows that
we can estimate θij within an additive error of Ξ.

**Proposition 4.2. Assume that (12) holds. Then, for a fixed pair of i ̸= j ∈** [d] and δ, ϵ ∈ (0, 1), the
_estimate_ _θ[ˆ]ij of Algorithm 1 that uses a median-based filter satisfies_

cos θ[ˆ]ij cos θij _<_ _√2πp(pΨϵ_ 1/2) + 3π(1 _p) + 2π(2_ _p)Ξ,_ (13)
_|_ _−_ _|_ _−_ _−_ _−_

_with probability at least 1 −_ _δ if the number of data samplesT_ _n ≥_ _p[2](pΨϵ1−1/2)[2][ log][ 1]δ_ _[. Here, the]_

_error bound is Ξ = V_ 1 _η+LL_ + [2][ϵ][(]Lη[L][+][η][)] _with η, L > 0 being constants that depend only on_
_−_

_the parameter r. Also, p is the probability of a sample being uncorrupted and_ Ψϵ [1/2p, 1] is such
_∈_
_that O(ϵ[3])_ Ψϵ _O([√]ϵ) with ϵ > O_ _p[−][1][/][3][]_ _where the order constants depend only on r._
_≤_ _≤_

Combining Propositions 4.1 and 4.2, we next present the sample complexity needed to achieve a 
small parameter estimation error. Specifically, the diagonal entries, Σ(i, i), for i ∈ [d] are obtained
from Proposition 4.1. Further, the off-diagonal entries of Σ are obtained by combining the Propositions 4.1 and 4.2 to obtain the final result as follows:

**Theorem 4.3. Suppose we initialize the projected gradient step (Algorithm 2 with median-based**
_filter) such that ∥v0 −_ **v[∗]∥** = V and choose the step size γt = γ and the number of iterations as T _._
_Then, there exists γ > 0 such that for δ, ϵ ∈_ (0, 1), the output of Algorithm 1, ( Σ[ˆ] _,_ **b[ˆ]) satisfies**

**Σ** **WW[T]** _F_ _√2πp(pΨϵ_ 1/2) + π(2 _p)(3 + 2Ξ)_ **W** _F_ (14)
_∥_ [ˆ] _−_ _∥_ _≤_ _−_ _−_ _∥_ _∥[2]_
 

_∥b[ˆ] −_ **b∥≤** Ξ∥W∥F, (15)

_with probability at least 1 −_ _δ if the number of data samples n = Ω(_ _ϵp[1][4][ log][ d]δ_ [)][. Here,][ p][ is the]

_probability of a sample being uncorrupted and Ψϵ_ [1/2p, 1] is such that O(ϵ[3]) Ψϵ _O([√]ϵ)_
_∈_ _≤_ _≤_
_with ϵ > O_ _p[−][1][/][3][]_ _where the order constants depend only on r._
 


-----

**Table 1: Runtime of various schemes when p = 0.95, n = 20000, and d = 5. The table indicates that SGD**
schemes are faster than their GD counterparts.

|Scheme|Oracle|Without Filter|With Median|With Trimmed Mean|
|---|---|---|---|---|
|GD|16.95 s|17.73 s|34.44 s|60.78 s|
|SGD|1.24 s|1.60 s|2.11 s|3.62 s|


**Scheme** **Oracle** **Without Filter** **With Median** **With Trimmed Mean**

**GD** 16.95 s 17.73 s 34.44 s 60.78 s

**SGD** 1.24 s 1.60 s 2.11 s 3.62 s


We note that in Propositions 4.1 and 4.2, and Theorem 4.3, η, L and the other order constants depend
on the parameter r > 0. Recall that the parameter r is required for the convexity region Dr and we
can choose r to be a large constant. Thus, r is a relatively insensitive parameter which does not
require any hand-tuning.

As the probability of a sample being uncorrupted p reduces, we observe that the error bound in (14)
becomes larger as expected. Also, as the probability of a sample being uncorrupted decreases, the
number of samples n required for the error bound increases. Moreover, the obtained error bounds
increase with the problem dimension for a fixed number of samples.

Our results are also comparable with the existing error bounds for ReLU parameter estimation
without outliers from (Wu et al., 2019, Theorem 1). Specifically, if there are no outliers, for any
_ζ, δ ∈_ (0, 1), the number of samples n = O( _ζ[1][2][ log][ d]δ_ [)][ are sufficient for the SGD algorithm in Wu]

et al. (2019) to achieve the following error bounds with probability at least 1 − 2δ,

_∥Σ[ˆ]_ _−_ **WW[T]∥F ≤** _ζ∥W∥F[2]_ [and][ ∥]b[ˆ] − **b∥≤** _ζ∥W∥F,_ (16)

where ζ indicates the parameter estimation error. Comparing the above bounds with (14) and (15),
we observe that our bounds are larger due to the presence of arbitrary outliers. In particular, Ξ

_T_

consists of two error terms. The first term V 1 _η+LL_ _< 1 in our error bounds results from the_
_−_

convergence of the GD algorithm and is similar to _ζ < 1 in (16). However, the second error term_
2ϵ(LηL+η) quantifies the error due to the gradient estimated from the output samples with arbitrary

outliers and is not present in (16). Additionally, in (14), the extra term 3π(2 − _p) further increases_
the bound compared to the error bound in (16). Therefore, the error ζ in (16) can not be directly
compared to the error in (14) and (15). Further, our sample complexity depends on the probability
of a sample being uncorrupted p and the second error term ϵ whereas the complexity in (Wu et al.,
2019, Theorem 1) depends on the square of the convergence error term ζ in their sample complexity.
Also, when there is no outlier (p = 1), the two sample complexities do not match.

5 SIMULATION RESULTS


In this section, we provide numerical results to verify the performance of our algorithm. In our
setup, the columns of W are chosen as the left singular vectors of random matrices from the standard
Gaussian distribution. For b, we use a random vector from the standard normal distribution whose
negative values are replaced with zeros. The mixture of samples are generated such that a sample
comes from (W, b) with probability p and from = (5, 1) with probability 1 _p. The hyper-_
_D_ _G_ _N_ 1 _−_
parameters in our proposed algorithm are set as r = 3 and γt = 0.1t [. We use the batch-splitting]

approach to compute the gradient. This approach induces randomization. As the output samples
consist of a fraction of arbitrary outliers, the standard gradient descent algorithm without any filter
would perform poorly as shown in our results. Also, we choose the total number of GD and SGD
iterations T to be _>0_ _/100 where_ _>0_ is the number of positive output samples. From our
_|X_ _|_ _|X_ _|_
experiments, we observe that the errors first decrease with the number of iterations and then after a
certain number of iterations, the errors flatten. Based on this observation, we chose the number of
iterations (see Appendix A.4). Note that we discard the zero entries and only consider only the set
of positive samples X>0 for the number of iterations as they do not convey any information about
the row norms of W and bias vector b.

We compute two error metrics from the estimated parameters and the ground truth, ∥Σ[ˆ] _−_
**WW[T]∥F /∥W∥F[2]** [and][ ∥]b[ˆ] _−_ **b∥2/∥W∥F . Also, we compare our algorithm with two other schemes:**
oracle scheme (estimation using the true samples only), and scheme without filter.

The results are shown in Figs. 1 and 2, and Table 1, and the observations from them are as follows.


-----

**1.5**

**0.6**

**1**

**0.4**

**0.5**

**Error in bias vector** **Error in bias vector** **0.2**

**0** **0**

**0.8** **0.85** **0.9** **0.95** **1** **5** **10** **15** **20**
**Probability of true samples (p)** **Dimension (d)**

**2** **2**

**10[0]**

**1.5** **1.5**

**1** **1**

**10[-1]**

**Error in weights** **0.5** **0.5**

**Error in weight matrix** **Error in Weight Matrix**

**0** **10[-2]** **0**

**0.8** **0.85** **0.9** **0.95** **1** **0** **0.5** **1** **1.5** **2** **5** **10** **15** **20**
**Probability of true samples (p)** **No.of samples (n)** **10[5]** **Dimension (d)**


**(a) d = 5 and n = 20000.**


**(b) p = 0.95 and d = 5.**


**(c) p = 0.95 and n = 20000.**


|Col1|GD w00/o Filte 0.1r|GD with Median GD with Trimmed Mean 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9|Oracle GD 1|
|---|---|---|---|


**GD w/o Filter** **GD with Median** **GD with Trimmed Mean**

**Figure 1: Comparison of the different GD schemes as a function of p (first column), n (second column), and**
_d (third column). The figures indicate that utilizing the filters improves the performance of GD algorithm for_
mixture of samples.


**3** **0.4**

**2.5** **0.6**

**0.3**

**2**

**1.5** **0.2** **0.4**

**1**

**Error in bias vector** **Error in bias vector** **0.1** **Error in bias vector** **0.2**

**0.5**

**0** **0** **0**

**0.8** **0.85** **0.9** **0.95** **1** **0** **0.5** **1** **1.5** **2** **5** **10** **15** **20**
**Probability of true samples (p)** **No.of samples (n)** **10[5]** **Dimension (d)**

**3.5** **10[0]** **1**

**3**

**0.8**

**2.5**

**2** **0.6**

**1.5** **10[-1]** **0.4**

**Error in weights** **1**

**0.5** **Error in weight matrix** **Error in weight matrix** **0.2**

**00.8** **0.85** **0.9** **0.95** **1** **0** **0.5** **1** **1.5** **2** **05** **10** **15** **20**

**Probability of true samples (p)** **No.of samples (n)** **10[5]** **Dimension (d)**


**(a) d = 5 and n = 20000.**


**(b) p = 0.95 and d = 5.**


**(c) p = 0.95 and n = 20000.**


**SGD with Median0.5100** 0.1 0.2 0.3GD with Median0.4 0.5 0.6 **Oracle SGD0.7** 0.8 0.9 **Oracle GD1**

**Figure 2: Comparison of GD and SGD schemes as a function of p (first column), n (second column), and d**
(third column). We infer that SGD is more sensitive to corrupted samples compared to GD.

_Effect of filters: From Fig. 2, we observe that the GD schemes perform better than the corresponding_
SGD schemes. Further, the performance is improved when we use median based filter along with
GD or SGD (for instance, as seen by comparing the performances of GD without Filter and GD with
Median in Fig. 1). Thus, the filter ensures that the effect of the outliers is reduced, and the curves are
closer to the Oracle GD scheme. Finally, from Fig. 1, we also infer that the median-based approach
performs slightly better than the trimmed mean-based approach.

_Dependence on probability of a sample being uncorrupted p: The variation in the error estimation as_
a function of p is shown in Figs. 1a and 2a. The performance of the proposed schemes improve with
_p, as expected. This trend is because as p increases, the fraction of outliers in the observed samples_


-----

decreases, which leads to better performance. Note that the oracle schemes assume the knowledge
of true samples. Thus, their performance does not change with p. On the contrary, the performance
of the schemes without filters and SGD schemes with filter do not have a monotonic behavior with
_p. This observation shows that these schemes are not able to handle the outliers effectively. Further,_
all the schemes converge to the corresponding oracle schemes when p = 1.

_Dependence on number of samples n: Figs. 1b and 2b show how the estimation error of different_
schemes varies with n. We observe that the parameter estimation errors computed using Oracle
SGD and Oracle GD schemes decrease as the number of samples increases. This trend is because
the oracle schemes consider only the true samples. Also, the Oracle GD scheme performs better
than the Oracle SGD scheme. Further, the estimation errors computed using our proposed schemes
with median and trimmed mean based filters perform better than GD without filter. Hence, the filters
mitigate the effect of arbitrary outliers and having more samples reduces the estimation errors.

_Dependence on dimension d: In Figs. 1c and 2c, we plot the error in different schemes as a function_
of the dimension d. We note that due to the presence of arbitrary outliers, the errors increase as
the dimension increases for GD without filter. Further, we observe a performance improvement
using our proposed schemes with median and trimmed mean-based filters compared to the other
benchmarking schemes, supporting our previous conclusions that the filters mitigate the effect of
outliers. However, we observe a slow increase in the parameter estimation errors using our proposed
schemes with median and trimmed mean based filters as we increase the dimension for fixed number
of samples. We observe that the errors when using the oracle schemes also increase slowly as the
dimension is increased (Wu et al. (2019)).

_Comparison of runtimes: From Table 1, we observe that the SGD schemes run faster than their GD_
counterparts. This is because the SGD utilizes only one sample for the gradient whereas GD utilizes
all the samples for the gradient. Further, the usage of median and trimmed mean based filters along
with SGD or GD increases their computation times. However, we reiterate that using the GD scheme
with filters offers the best performance in terms of error (see Fig. 2). Further, the runtimes of the
trimmed mean-based schemes are significantly higher than those of the median-based schemes.

To summarize, from our results, we conclude that GD schemes offer better performance than the
SGD schemes. Also, we deduce that compared to the trimmed mean-based scheme, the medianbased filter is computationally more efficient while offering similar or lower estimation error. Moreover, the median-based scheme is parameter free (the trimmed mean depends on the parameter β)
and enjoys strong theoretical guarantees (see Theorem 4.3). Overall, the median-based GD algorithm is the most effective approach to estimate the NN parameters in the presence of the outliers
and performs slightly better than trimmed mean-based GD algorithm. Further, we observe that GD
schemes with filters perform better than Oracle SGD scheme.

6 CONCLUSION

In this paper, we proposed an algorithm for estimation of the parameters of a single-layer ReLU
neural network from the truncated Gaussian samples where each sample was assumed to be an
arbitrary outlier with a fixed probability. Our only assumption was that the bias vector was nonnegative. We derived a GD-based algorithm for parameter estimation and incorporated filters to
handle the outliers. We analyzed the sample complexity of the proposed algorithm in terms of the
parameter estimation error. The efficacy of our approach was also demonstrated using numerical
experiments. Removing the non-negativity assumption on the bias vector is a promising direction
for future work. Also, extending our results to multi-layer neural networks is also an interesting
problem to consider in the future.

REFERENCES

Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via overparameterization. In International Conference on Machine Learning, pp. 242–252. PMLR, 2019.

Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein generative adversarial networks.
In International Conference on Machine Learning, pp. 214–223. PMLR, 2017.


-----

Ainesh Bakshi, Rajesh Jayaram, and David P Woodruff. Learning two layer rectified neural networks
in polynomial time. In Conference on Learning Theory, pp. 195–268. PMLR, 2019.

Yuan Cao and Quanquan Gu. Tight sample complexity of learning one-hidden-layer convolutional
neural networks. arXiv preprint arXiv:1911.05059, 2019.

Sitan Chen, Adam R Klivans, and Raghu Meka. Learning deep ReLU networks is fixed-parameter
tractable. arXiv preprint arXiv:2009.13512, 2020.

Constantinos Daskalakis, Themis Gouleakis, Chistos Tzamos, and Manolis Zampetakis. Efficient
statistics, in high dimensions, from truncated samples. In 2018 IEEE 59th Annual Symposium on
_Foundations of Computer Science (FOCS), pp. 639–649. IEEE, 2018._

Ilias Diakonikolas and Daniel M Kane. Recent advances in algorithmic high-dimensional robust
statistics. arXiv preprint arXiv:1911.05911, 2019.

Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Ankur Moitra, and Alistair Stewart.
Robust estimators in high-dimensions without the computational intractability. SIAM Journal on
_Computing, 48(2):742–864, 2019._

Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675–
1685. PMLR, 2019.

Spencer Frei, Yuan Cao, and Quanquan Gu. Agnostic learning of a single neuron with gradient
descent. arXiv preprint arXiv:2005.14426, 2020.

Surbhi Goel, Adam Klivans, and Raghu Meka. Learning one convolutional layer with overlapping
patches. In International Conference on Machine Learning, pp. 1783–1791. PMLR, 2018.

Surbhi Goel, Sushrut Karmalkar, and Adam Klivans. Time/accuracy tradeoffs for learning a ReLU
with respect to gaussian marginals. arXiv preprint arXiv:1911.01462, 2019.

Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint
_arXiv:1406.2661, 2014._

Frank R Hampel, Elvezio M Ronchetti, Peter J Rousseeuw, and Werner A Stahel. Robust statistics:
_The approach based on influence functions, volume 196. John Wiley & Sons, 2011._

Peter J. Huber. Robust Estimation of a Location Parameter. The Annals of Mathematical Statistics,
35(1):73 – 101, 1964.

Peter J Huber. Robust statistics, volume 523. John Wiley & Sons, 2004.

Norman L Johnson, Samuel Kotz, and Narayanaswamy Balakrishnan. Continuous univariate distri_butions, volume 1, volume 289. John wiley & sons, 1995._

Daniel M Kane. Robust learning of mixtures of Gaussians. In Proceedings of the 2021 ACM-SIAM
_Symposium on Discrete Algorithms (SODA), pp. 1246–1258. SIAM, 2021._

Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. _arXiv preprint_
_arXiv:1312.6114, 2013._

Kevin A. Lai, Anup B. Rao, and Santosh Vempala. Agnostic estimation of mean and covariance. In
_2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS), pp. 665–674,_
2016. doi: 10.1109/FOCS.2016.76.

Qi Lei, Jason Lee, Alex Dimakis, and Constantinos Daskalakis. SGD learns one-layer networks in
WGANs. In International Conference on Machine Learning, pp. 5799–5808. PMLR, 2020.

Arya Mazumdar and Ankit Singh Rawat. Representation learning and recovery in the ReLU model.
_arXiv preprint arXiv:1803.04304, 2018._


-----

Anirbit Mukherjee and Ramchandran Muthukumar. Guarantees on learning depth-2 neural networks
under a data-poisoning attack. arXiv preprint arXiv:2005.01699, 2020.

Samet Oymak. Stochastic gradient descent learns state equations with nonlinear activations. In
_Conference on Learning Theory, pp. 2551–2579. PMLR, 2019._

Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.

Aaron Van Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In
_International Conference on Machine Learning, pp. 1747–1756. PMLR, 2016._

Santosh Vempala and John Wilmes. Gradient descent for one-hidden-layer neural networks: Polynomial convergence and SQ lower bounds. In Conference on Learning Theory, pp. 3115–3117.
PMLR, 2019.

David P Williamson and David B Shmoys. The design of approximation algorithms. Cambridge
university press, 2011.

Shanshan Wu, Alexandros G Dimakis, and Sujay Sanghavi. Learning distributions generated by
one-layer ReLU networks. Advances in Neural Information Processing Systems, 32:8107–8117,
2019.

Xiao Zhang, Yaodong Yu, Lingxiao Wang, and Quanquan Gu. Learning one-hidden-layer ReLU
networks via gradient descent. In The 22nd International Conference on Artificial Intelligence
_and Statistics, pp. 1524–1534. PMLR, 2019._


-----

