# FLOW-BASED RECURRENT BELIEF STATE LEARNING
## FOR POMDPS

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Partially Observable Markov Decision Process (POMDP) provides a principled
and generic framework to model real world sequential decision making processes
but yet remains unsolved, especially for high dimensional continuous space and
unknown models. The main challenge lies in how to accurately obtain the belief state, which is the probability distribution over the unobservable environment
states given historical information. Accurately calculating this belief state is a
precondition for obtaining an optimal policy of POMDPs. Recent advances in
deep learning techniques show great potential to learn good belief states, but they
assume the belief states follow certain types of simple distributions such as diagonal Gaussian, which imposes strong restrictions to precisely capture the real
belief states. In this paper, we introduce the FlOw-based Recurrent BElief State
model (FORBES), which incorporates normalizing flows into the variational inference to learn general continuous belief states for POMDPs. Furthermore, we
show that the learned belief states can be plugged into downstream RL algorithms
to improve performance. In experiments, we show that our methods successfully
capture the complex belief states that enable multi-modal predictions as well as
high quality reconstructions, and results on challenging visual-motor control tasks
show that our method achieves superior performance and sample efficiency.

1 INTRODUCTION

Partially Observable Markov Decision Process (POMDP) ( Astr¨[˚] om, 1965) provides a principled and
generic framework to model real world sequential decision making processes. Unlike Markov Decision Process (MDP), the observations of a POMDP are generally non-Markovian. Therefore, to
make optimal decisions, the agent needs to consider all historical information, which is usually
intractable. One effective solution is to obtain the belief state. The belief state is defined as the
probability distribution of the unobservable environment state conditioned on the past observations
and actions (Kaelbling et al., 1998). Such belief state accurately summarizes the history. Traditional
methods of calculating belief states (Smallwood & Sondik, 1973; Sondik, 1971; Kaelbling et al.,
1998) assume finite discrete space with a known model. In many real world problems, however, the
underlying model remains unknown, and the state space is large and even continuous.

With the recent advances of deep learning technologies, a branch of works have been proposed to
learn the belief states of POMDPs with unknown model and continuous state space (Krishnan et al.,
2015; Gregor et al., 2019; Lee et al., 2020; Hafner et al., 2019b;a; 2021). These works solve the belief state learning problem by sequentially maximizing the observation probability at each timestep
using the variational inference and achieve the state-of-the-art performance on many visual-motor
control tasks (Hafner et al., 2019a; Zhu et al., 2020; Okada et al., 2020; Ma et al., 2020a). However,
they still cannot capture general belief states due to the intractability of complex distributions in
high-dimensional continuous space and instead approximate the belief states with diagonal Gaussians. This approximation imposes strong restrictions and is problematic. As shown in Figure 1,
the blue area denotes the unobservable state space of the POMDP. Given the past information τ,
the agent maintains a prior distribution of the state s, denoted as p(s|τ ) (the distribution in white).
Each colored distribution corresponds to the belief state after receiving a different new observation
_o, denoted as the posterior distribution q(s|τ, o). Consider an example of the true beliefs as shown in_
Figure 1(b), with their Gaussian approximations shown in Figure 1(a). The approximation error of
Gaussian distributions will easily result in problems of intersecting belief which leads to a mixed-up


-----

state (e.g., the white triangle), and empty belief, which leads to a meaningless state (e.g., the grey triangle). This also explains the poor reconstruction problems in interactive environments observed by
Okada & Taniguchi (2021). Furthermore, as mentioned in Hafner et al. (2021), the Gaussian approximation of belief states also makes it difficult to predict multi-modal future behaviours. Therefore,
it is preferable to relax the Gaussian assumptions and use a more flexible family of distributions to
learn accurate belief states as shown in Figure 1(b).

Figure 1: Difference between (a) spherical Gaussian belief states and (b) true belief states (Better
viewed in color).

In this paper, we propose a new method called FlOw-based Recurrent BElief State model
(FORBES) that is able to learn general continuous belief states for POMDPs. FORBES incorporates Normalizing Flows (Tabak & Turner, 2013; Rezende & Mohamed, 2015; Dinh et al., 2017)
into the variational inference step to construct flexible belief states. In experiments, we show that
FORBES allows the agent to maintain flexible belief states, which result in multi-modal and precise
predictions as well as higher quality reconstructions. We also demonstrate the results combining
FORBES with downstream RL algorithms on challenging visual-motor control tasks (DeepMind
Control Suite, Tassa et al. (2018)). The results show the efficacy of FORBES in terms of improving
both performance and sample efficiency.

Our contributions can be summarized as follows:

-  We propose FORBES, the first flow-based belief state learning algorithm that is capable of
learning general continuous belief states for POMDPs.

-  We propose a POMDP RL framework based on FORBES for visual-motor control tasks,
which uses the learned belief states from FORBES as the inputs to the downstream RL
algorithms.

-  Empirically, we show that FORBES allows the agent to learn flexible belief states that
enable multi-modal predictions as well as high quality reconstructions and help improve
both performance and sample efficiency for challenging visual-motor control tasks.

2 PRELIMINARIES

2.1 PARTIALLY OBSERVABLE MARKOV DECISION PROCESS

Formally, a Partially Observable Markov Decision Process (POMDP) is
a 7-tuple (S, A, T, R, Ω, O, γ), where S is a set of states, A is a set of actions, T is a set of conditional transition probabilities between states, R
is the reward function, Ω is a set of observations, O is a set of conditional
observation probabilities, and γ is the discount factor.


At each timestepagent takes an actionto state st with probability t − a1t, the state of the environment is−1 ∈A T (s, which causes the environment to transitt _st_ 1, at 1). The agent then receives st−1 ∈S. The
an observation ot Ω which depends on the new state of the environ- | _−_ _−_
_rmentt_ 1 equal to st with probability R(s ∈t 1). The agent’s goal is to maximize the the expected O (ot | st). Finally, the agent receives a reward
_−_ _−_


Figure 2: The PGM of
POMDP


-----

sum of discounted rewards E [[P][∞]t=0 _[γ][t][r][t][]][.][ Such a POMDP model can also be described as a proba-]_
bilistic graphical model (PGM) as shown in Figure 2. After having taken action at 1 and observing
_−_
_ot, an agent needs to update its belief state, which is defined as the probability distribution of the_
environment state conditioned on all historical information:


_b(st) = p(st_ _τt, ot)_ (1)
_|_


where τt = _o1, a1, . . ., ot_ 1, at 1 .
_{_ _−_ _−_ _}_

2.2 NORMALIZING FLOW


Instead of using the Gaussian family to approximate the prior and posterior belief distributions, we
believe it is more desirable to use a family of distributions that is highly flexible and preferably
flexible enough to describe all possible true belief states. Therefore, we use Normalizing Flows
(Tabak & Turner, 2013; Rezende & Mohamed, 2015) to parameterize those distributions.

Rather than directly parameterizing statistics of the distribution itself, Normalizing Flows model the
transformations, or the “flow” progress, needed to derive such a distribution. More specifically, it
describes a sequence of invertible mappings that gradually transform a relatively simple probability
density to a more flexible and complex one.

Let fθ : R[D] _→_ R[D] to be an invertible and differentiable mapping in state space parameterized by θ.
Given a random variable x ∈ R[D] with probability distribution p(x), we can derive the probability
of the transformed random variable z = fθ(x) by applying the change of variable formula:

_p(z) = p(x)_ _θ_ (2)

_∂z_

log p(z) = log p(x[det]) _[ ∂f]log[ −][1]_ (3)
_−_ _∂z_

To construct a highly flexible family of distributions, we can propagate the random variable at be
[det][ ∂f][θ]

ginning z0 through a sequence of K mappings and get zK = fθK _fθK−1_ _fθ1_ (z0) with the
probability _◦_ _◦· · · ◦_

_K_

log pK(zK) = log p(z0) log (4)
_−_ _∂zk_ 1

_k=1_ _−_

X

Given a relatively simple distribution of z0, say, Gaussian distribution, by iteratively applying the

[det][ ∂f][θ][k]

transformations, the flow is capable of representing a highly complex distribution with the probability that remains tractable. The parameters θ1, . . ., θK determine the transformations of the flow.

An effective transformation that is widely accepted is affine coupling layer (Dinh et al., 2017;
Kingma & Dhariwal, 2018; Kingma et al., 2017). Given the input x ∈ R[D], let s and t stand
for scale and translation functions which are usually parameterized by neural networks, where
_s, t : R[k]_ _→_ R[D][−][k], k < D. The output, y, can be viewed as a concatenation of its first k dimensions y1:k and the remaining part yk+1:D:

**y1:k = x1:k,** **yk+1:D = xk+1:D ⊙** exp(s(x1:k)) + t(x1:k) (5)

where ⊙ denotes the element-wise product (see details about affine coupling layer in Appendix A.1).

3 FLOW-BASED RECURRENT BELIEF STATE LEARNING

3.1 FLOW-BASED RECURRENT BELIEF STATE MODEL

We propose the FlOw-based Recurrent BElief State model (FORBES) which learns general continuous belief states via normalizing flows under the variational inference framework. Specifically,
the FORBES model consists of components needed to construct the PGM of POMDP as shown in
Figure 2:
State transition model : _p(st|st−1, at−1)_
Observation model : _p(ot_ _st)_ (6)
_|_
Reward model : _p(rt_ _st)_
_|_


-----

In addition, we have a belief inference model q(st _τt, ot) to approximate the true posterior distri-_
_|_
bution p(st _τt, ot), where τt =_ _o1, a1, . . ., ot_ 1, at 1 is the past information. The above com_|_ _{_ _−_ _−_ _}_
ponents of FORBES can be optimized jointly by maximizing the Evidence Lower BOund (ELBO)
(Jordan et al., 1999) or more generally the variational information bottleneck (Tishby et al., 2000;
Alemi et al., 2016):

log p(o1:T, r1:T |a1:T )

_T_

_≥_ Eq(s1:T |o1:T,a1:T −1) " _t=1(ln p(ot|st) + ln p(rt|st) −_ _DKL(q(st|st−1, at−1, ot)∥p(st|st−1, at−1)))#_

X


= Eq(st|st−1,at−1,ot)(ln p(ot|st) + ln p(rt|st))−

_t=1_

X  _._

Eq(st−1|st−2,at−2,ot−1)(DKL(q(st|st−1, at−1, ot)∥p(st|st−1, at−1))) = JModel
(7)

here we use the factorization of q(s1:T |o1:T, a1:T −1) = _t_ _[q][(][s][t][|][s][t][−][1][, a][t][−][1][, o][t][)][.]_

Detailed derivations can be found in Appendix.A.9. In practice, the state transition model, observation model, reward model, and belief inference model can be represented by stochastic deep neural[Q]
networks parameterized by ψ:
_pψ(st|st−1, at−1),_ _pψ(ot|st),_ _pψ(rt|st),_ _qψ(st|τt, ot)_ (8)
where their outputs usually follow simple distributions such as diagonal Gaussians. The parameterized belief inference model qψ(st _τt, ot) acts as an encoder that encodes the historical information_
_|_
using a combination of convolutional neural networks and recurrent neural networks. Note that q
below is obtained through the factorization of q(s1:T |o1:T, a1:T −1) = _t_ _[q][(][s][t][|][s][t][−][1][, a][t][−][1][, o][t][)][.]_

In FORBES we provide special treatments for the belief inference model and the state transition
model to represent more complex and flexible posterior and prior distributions. As shown in Fig-[Q]
ure 3(a), the input images o1:t and actions a1:t 1 are encoded with qψ(st _τt, ot) (the blue and the_
_−_ _|_
red path). Then our final inferred belief is obtained by propagating qψ(st _τt, ot) through a set of_
_|_
normalizing flow mappings denotedqψ,θ(st _τt, ot). For convenience, we denote fθK ◦· · · ◦ q0 =f qθ1ψ to get a representative posterior distribution and qK = qψ,θ. On the other hand, o1:t_ 1
_|_ _−_
and a1:t 2 are encoded with qψ(st 1 _τt_ 1, ot 1) (the blue path), then the state transition model is
_−_ _−_ _|_ _−_ _−_
used to obtain the prior guess of the state pψ(st _τt) = Eqψ(st_ 1 _τt_ 1,ot 1) [pψ(st _st_ 1, at 1)] (the
_|_ _−_ _|_ _−_ _−_ _|_ _−_ _−_
green path). Then our final prior is obtained by propagating pψ(st _τt) through another set of nor-_
_|_
malizing flow mappings denotedFor convenience, we denote p0 = f pωψK and ◦· · ·◦ pKf =ω1 to get a representative prior distribution pψ,ω. Then as shown in Figure 3(b), we can sample pψ,ω(st|τt).
sampled initial state, we can use the state transition model to predict the future statesthe initial state st (the yellow and purple triangles) from the belief states qK(st | τt ˆ, ost+t)h. For each given the
future actions at:t+h−1, and then use the observation model to reconstruct the observations ˆot+h,
where h is the prediction horizon.

With the above settings, we can substitute the density probability inside the KL-divergence term in
Equation 7 with Normalizing Flow:
_DKL(qK(st_ _τt, ot)_ _pK(st_ _st_ 1, at 1)) = EqK (s1:t _τt,ot) [log qK(st_ _τt, ot)_ log pK(st _τt)]_
_|_ _∥_ _|_ _−_ _−_ _|_ _|_ _−_ _|_

_K_

= EqK (s1:t _τt,ot)_ log q0(st _τt, ot)_ log _∂fθk_ _∂fωk_
_|_ " _|_ _−_ _k=1_ _∂st,k−1_ _[−]_ [log][ p][0][(][s][t][|][τ][t][) + log] _∂st,k−1_ #

X

(9)
where pK(st _st_ 1, at 1) = pK(st _τt) given the sampled[det]_ _st_ 1 from qK(s1:t _τt, o[det]t). st,k is the_
state variable | st transformed by− _−_ _k layers of normalizing flows, and |_ _−_ _st,0 = st._ _|_

To further demonstrate the properties of FORBES, we provide the following theorems.


**Theorem 1 The approximation error of the log-likelihood when maximizing the JModel (the derived**
_ELBO) defined in Equation 7 is:_

_T_

log p(o1:T, r1:T |a1:T ) −JModel = EqK (s1:T |τT,oT ) "t=1 _DKL(q(st|τt, ot)∥p(st | τt, ot))#_ (10)

X

_where p(st_ _τt, ot) denotes the true belief states._
_|_


-----

(a) Belief state inference (b) Predictions beginning from different samples

Figure 3: The algorithm framework of FORBES. Figure 3a shows how to calculate prior and posterior belief distribution given previous information. The blue arrows bring in historical observations
and actions, and the green path shows the evolution of prior belief distribution. The red path takes
an additional ot and shows the evolution of posterior belief distribution. Figure 3b shows prediction
of future trajectories given the future actions.

Detailed proofs can be found in Appendix.A.9. Theorem 1 suggests that, when the learning algorithm maximizes the JModel (the derived ELBO), then the DKL terms in the right-hand side are
minimized, which indicate the KL-divergence between the learned belief states q(st _τt, ot) and the_
_|_
true belief states p(st _τt, ot). Clearly, if p(st_ _τt, ot) is a complex distribution and q(st_ _τt, ot)_
is chosen from a restricted distribution class such as diagonal Gaussian, then when the algorithm | _|_ _|_
maximizes the JModel (the derived ELBO), there will still be a potentially large KL-divergence between the learned and the true belief states. Therefore, naturally there raises the problem that is
normalizing flow a universal distributional approximator that is capable of accurately representing
arbitrarily complex belief states, so the KL-divergence terms in the right-hand side of Equation (10)
can be minimized to approach zero? The answer is yes for a wide range of normalizing flows.
To be specific, Teshima et al. (2020) provides theoretical results for the family of the flow used in
FORBES.

In fact, there always exists a diffeomorphism that can turn one well-behaved distribution to another.
Besides the aforementioned affine coupling flow, many works show the distributional universality
of other flows (Kong & Chaudhuri, 2020; Huang et al., 2018). Ideally, the universal approximation
property of the flow model qK(st _τt, ot) allows us to approximate the true posterior p(st_ _τt, ot)_
with arbitrary accuracy. Thus, compared to previous methods, FORBES helps close the gap between | _|_
the log-likelihood and the ELBO to obtain a more accurate belief state. Though we usually cannot
achieve the ideal zero KL-divergence in practice, our method can get a smaller approximation error,
equally a higher ELBO than previous works. We verify this statement in section 4.1.

3.2 POMDP RL FRAMEWORK BASED ON FORBES

To show the advantage of the belief states inferred by the FORBES model compared to the existing
belief inference method in visual-motor control tasks, we design a flow-based belief reinforcement
learning algorithm for learning the optimal policy in POMDPs. The algorithm follows an actor-critic
framework. The critic estimates the accumulated future rewards, and the actor chooses actions to
maximize the estimated cumulated rewards. Both the actor and critic operate on top of the samples
of learned belief states, and thus benefit from the accurate representations learned by the FORBES
model. Note that this is an approximation of the true value on belief, which avoids the intractable
integration through observation model.

The critic vξ (sτ ) aims to predict the discounted sum of future rewards that the actor can achieve

given an initial state st, known as the state value E _tτ+=∞t_ _[γ][τ]_ _[−][t][r][τ]_, where ξ denote the parameters
of the critic network and H is the prediction horizon. We leverage temporal-difference to learn thisP 
value, where the critic is trained towards a value target that is constructed from the intermediate
reward and the critic output for the next step’s state. In order to trade-off the bias and the variance
of the state value estimation, we use the more general TD(λ) target (Sutton & Barto, 2018), which
is a weighted average of n-step returns for different horizons and is defined as follows:


-----

_Vτ[λ]_ = ˆ. _rτ + ˆγτ_ (1vξ −(st+λ)Hv)ξ(sτ +1) + λV λτ +1 ifif _τ < tτ = t + + H, H._ (11)



To better utilize the flexibility belief states from FORBES, we run the sampling method multiple
times to capture the diverse predictions. Specifically, we sample N states from the belief state given
by FORBES and then rollout trajectories of future states and rewards using the state transition model
and the reward model. Finally, we train the critic to regress the TD(λ) target return using a mean
squared error loss:

_JCritic(ξ) =_ _si,0_ _qK_ _,aτEqφ,si,τ_ _pψ_ _Ni=1_ _tτ+=Ht_ 21 _vξ(si,τ_ ) − sg(Vi,τ[λ] [)] 2[i]. (12)
_∼_ _∼_ _∼_

h P P   

that maximize the prediction of long-term future rewards made by the critic and is trained directlywhere sg(·) is the stop gradient operation. The actor aτ ∼ _qφ (aτ | sτ_ ) aims to output actions
by backpropagating the value gradients through the sequence of sampled states and actions, i.e.,
maximize:

_N_ _t+H_


V[λ]i,τ
_τ_ =t

X


(13)


Actor(φ) = E
_J_ _si,0_ _qK_ _,aτ_ _qφ,si,τ_ _pψ_
_∼_ _∼_ _∼_


_i=1_


We jointly optimize the model loss JModel with respect to the model parameters ψ, θ and ω, the
critic loss JCritic with respect to the critic parameters ξ and the actor JActor loss with respect to
the actor parameters φ using the Adam optimizer with different learning rates:

min FORBES = α0 Critc(ξ) _α1_ Actor(φ) _α2_ Model(ψ, θ, ω) (14)
_ψ,ξ,φ,θ,ω_ _J_ _J_ _−_ _J_ _−_ _J_

where α0, α1, α2 are coefficients for different components, and we summarize the whole framework
of optimizing in Algorithm 1.

**Algorithm 1 FORBES Algorithm**

**Input: buffer B, imagination horizon H, interacting step T** **, batch size B, batch length L,**
**number of trajetories N** **.**

Draw B data sequences {(ot, at, rt)}t[k]=[+]k[L] [from][ B]

Infer belief state qK(st _τt, ot)._
_|_
**for i = 1, . . ., N do**

Rollout imaginary trajectories {(si,τ _, ai,τ_ )}τ[t][+]=[H]t [with belief transition model.]

**end for**
For each si,τ, predict rewards pψ(ri,τ _si,τ_ ) and values vφ(si,τ ) _▷_ _Calculate imaginary returns_
_|_

Update θ, ω, ξ, φ, ψ using Equation (7), (9), (12), (13) and (14) ▷ _Optimize the model, critic and_
_actor_


4 EXPERIMENTS

Our experiments evaluate FORBES on two image-based tasks. We first demonstrate the belief learning capacity on a digit writing task in Section 4.1, and show that FORBES captures beliefs that allow
for multi-modal yet precise long-term predictions as well as higher ELBO. For large-scale experiments, we test the proposed POMDP RL framework based on FORBES in Section 4.2. The results
of multiple challenging visual-motor control tasks from DeepMind Control Suite (Tassa et al., 2018)
show that FORBES outperforms baselines in terms of performance and sample efficiency. In Section 4.3, we further provide ablation studies of the multiple imagined trajectories technique used in
our method.

4.1 DIGIT WRITING TASKS

In this experiment, we validate the capacity of FORBES by modelling the partially observable sequence with visual inputs. We adopt the MNIST Sequence Dataset (D. De Jong, 2016) that consists
of sequences of handwriting MNIST digit stokes. This problem can be viewed as a special case


-----

Figure 4: Predictions on sequential MNIST of two models.

of POMDP, whose action space is Ø and rewards remain 0. Such a problem setting separates the
belief learning and policy optimizing problem and allows us to concentrate on the former one in
this section. We convert the digit stroke to a sequence of images of size 28 × 28 to simulate the
writing process. At time step t, the agent can observe ot that has already written t pixels, and
we train the agent maximizing JModel in Equation 7 except for the reward reconstruction term.

As shown in Figure 4, we randomly select three digits as examples
(see Appendix A.10 for more results) and show the inputs as well
as the prediction outputs of our model and the RSSM (Hafner et al., 724.0
2019b) baseline, which is the previous state-of-the-art method for
learning continuous belief states of POMDPs. The leftmost column
is the ground truth of the fully written digits. During the testing, we 725.0
feed the initial 15 frames _o1, o2,_ _, o15_ to the model, and the 725.5
_{_ _· · ·_ _}_
columns in grey exhibit a part of the inputs. Then we sample several FORBES
states from the inferred belief state and rollout via the learned state RSSM
transition model (Equation (6)) for 15 steps and show the recon- 0.5 1.0 1.5 2.0 2.5
struction results of the predictions. As shown in the blue and green Training Steps (M)
columns on the right of Figure 4, though RSSM can also predict the

724.0

724.5

725.0

ELBO

725.5

FORBES

726.0

RSSM

0.5 1.0 1.5 2.0 2.5

Training Steps (M)

future strokes in general, the reconstructions are relatively blurred Figure 5: ELBO on digit writand mix different digits up. It also fails to give diverse predictions. ing
However, FORBES can make precise yet diverse predictions. Each
prediction is clear and distinct from other digits. Given the beginning of the digit 7, FORBES successfully predicts both 7 and 3 since they have a similar beginning. The results can be partially
explained via the mixed-up belief and the empty belief as shown in Figure 1, which support the
claim that FORBES can better capture the complex belief states.

We also provide the quantitative results in Figure 5, which is the ELBO on test digits sequence
set that is never seen during training. The results show that FORBES can achieve a tighter ELBO,
which verifies the theoretical results in 3.1. Details of the network can be found in Appendix A.2.


4.2 VISUAL-MOTOR CONTROL TASKS

We experimentally evaluate the performance of FORBES on Reinforcement Learning on a variety
of visual-motor control tasks from the DeepMind Control Suite (Tassa et al., 2018), illustrated in
Figure 6. Across all the tasks, the observations are 64 _×_ 64 _×_ 3 images. These environments provide
different challenges. The Cartpole-Swingup task requires a long planning horizon and memorizing
the state of the cart when it is out of view; Finger-Spinning includes contact dynamics between the
finger and the object; Cheetah-Run exhibits high-dimensional state and action spaces; the WalkerWalk and Walker-Run are challenging because the robot has to learn to first stand up and then walk;
Hopper Stand is based on a single-legged robot, which is sensitive to the reaction force on the ground
and thus needs more accurate control. As for baselines, we include the scores for A3C Mnih et al.
(2016) with state inputs (1e9 steps), D4PG Barth-Maron et al. (2018) (1e9 steps), PlaNet (Hafner
et al., 2019b) (1e6 steps) and Dreamer Hafner et al. (2019a) with pixel inputs. All the scores of
baselines are aligned with the ones reported in Hafner et al. (2019a) (see details in Appendix A.3).
The details of the implementations can be found in Appendix A.2.

Our experiments empirically show that FORBES achieves superior performance and sample efficiency on challenging visual-motor control tasks. As illustrated in Figure 6, FORBES achieves


-----

cartpole_swingup


cheetah_run

100 200 300 400 500

walker_run


finger_spin


1000

800

600

400

200


700

600

500

400

300

200

100

0

600

500

400

300

200

100


800

600

400

200

800

600

400

200


1000

800

600

400

200


100 200 300 400 500

|100 200 300 400 500 hopper_stand|Col2|
|---|---|
|||
|||
|||


Environment Steps (K)


0 100 200 300 400 500

|Col1|Col2|
|---|---|
|||
|||
|||


Environment Steps (K)


0 100 200 300 400 500

|0 100 200 300 400 500 walker_walk|Col2|
|---|---|
|||
|||
|||


Environment Steps (K)


FORBES D4PG (1e9 steps) PlaNet (1e6 steps)
Dreamer A3C (1e9 steps, proprio)


Figure 6: Performance on DeepMind Control Suite. The shaded areas show the standard deviation
across 3 seeds. FORBES achieves better performance and sample efficiency in various challenging
tasks.

higher scores than Dreamer (Hafner et al., 2019a) in most of the tasks and achieves better performance than PlaNet (Hafner et al., 2019b) within much fewer environment steps. We provide some
insights into the results. As shown in Section 4.1, baselines with Gaussian assumptions may suffer
from the mixed-up belief and empty belief issues, while FORBES can better capture the general belief states. Furthermore, multiple imagined trajectories can better utilize the diversity in the rollout.
Therefore, the inner coherency within the model components allows the agent a better performance.
We further discuss the role of multiple imagined trajectories in the next section.

4.3 ABLATION STUDY


cartpole_swingup cheetah_run 1000 finger_spin

700

800 600 800

600 500 600

400

400 300 400

Episode Return 200 200

200 100

0 0

100 200 300 400 500 100 200 300 400 500 0 100 200 300 400 500

hopper_stand walker_run walker_walk

1000

600

800 500 800

600 400 600

400 300 400

200

Episode Return200 200

100

0 0 0

0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500

Environment Steps (K) Environment Steps (K) Environment Steps (K)

FORBES Dreamer Dreamer + Multiple Imagined Trajectories


Figure 7: Comparison of the performance between FORBES and Dreamer with multiple imagined
trajectories.

In order to verify that the outperformance of FORBES is not simply due to increasing the number
of imagined trajectories, we conducted an ablation study in this section. We compare FORBES
with the “Dreamer + multiple imagined trajectories” baseline by increasing the number of imagined
trajectories in Dreamer to the same as in FORBES. As shown in Figure 7, no consistent and obvious
gain can be observed after increasing the number of trajectories to Dreamer. The agent gains slight
improvements in two environments and suffers from slight performance loss on other tasks. This
result indicates that increasing the number of imagined trajectories may only be effective when the
agent can make diverse predictions as in FORBES. The Gaussian assumptions lead to the lack of
trajectory diversity, so that increasing the number of imagined trajectories will not effectively help.


-----

5 RELATED WORK

**POMDP: POMDP solving approaches can be divided into two categories based on whether their**
state, action and observation spaces are discrete or continuous. Discrete space POMDP solvers, in
general, either approximate the value function using point-based methods (Kurniawati et al., 2008;
Shani et al., 2013) or using Monte-Carlo sampling in the belief space (Silver & Veness, 2010; Kurniawati & Yadav, 2016) to make the POMDP problem tractable. Continuous space POMDP solvers
often approximate the belief states as a distribution with few parameters (typically Gaussian) and
solve the problem analytically either using gradients (Van Den Berg et al., 2012; Indelman et al.,
2015) or using random sampling in the belief space (Agha-Mohammadi et al., 2014; Hollinger &
Sukhatme, 2014). However, the classical POMDP methods mentioned above are all based on an
accurately known dynamic model, which is a restricted assumption in many real world tasks.

**MBRL for visual-motor control:** Recent researches in model-based reinforcement learning
(MBRL) for visual-motor control provides promising methods to solve POMDPs with highdimensional continuous space and unknown models since visual-motor control tasks can be naturally
modelled as POMDP problems. Learning effective latent dynamics models to solve challenging
visual-motor control problems is becoming feasible through advances in deep generative modeling
and latent variable models (Krishnan et al., 2015; Karl et al., 2016; Doerr et al., 2018; Buesing et al.,
2018; Ha & Schmidhuber, 2018; Hafner et al., 2019b;a). Among which, the recurrent state-space
model (RSSM) based methods (Hafner et al., 2019b;a) provide a principled way to learn continuous
latent belief states for POMDPs by variational inference and learns behaviours based on the belief
states using model-based reinforcement learning, which achieves high performance on visual-motor
control tasks. However, they assume the belief states obey diagonal Gaussian distributions. Such
assumptions impose strong restrictions to belief inference and lead to limitations in practice, including mode collapse, posterior collapse and object vanishing in reconstruction(Bowman et al., 2016;
Salimans et al., 2015; Okada & Taniguchi, 2020). A few works propose particle filter based methods
that use samples to approximate the belief states (Ma et al., 2020b; Igl et al., 2018). However, they
suffer from insufficient sample efficiency and performance.

**Normalizing Flows: Normalizing Flows (NF) are a family of generative models which produce**
tractable distributions with analytical density. For a transformation f : R[D] _→_ **R[D], the compu-**
tational time cost of the log determinant is O(D[3]). Thus most previous works choose to make the
computation more tractable. Rezende & Mohamed (2015); van den Berg et al. (2019) propose to use
restricted functional form of f . Another choice is to force the Jacobian of f to be lower triangular
by using an autoregressive model (Kingma et al., 2016; Papamakarios et al., 2018). These models
usually excel at density estimation, but the inverse computation can be time-consuming. Dinh et al.
(2015; 2017); Kingma & Dhariwal (2018) propose the coupling method to make the Jacobian triangular and ensure the forward and inverse can be computed with a single pass. The applications of
NF include image generation (Ho et al., 2019; Kingma & Dhariwal, 2018), video generation (Kumar
et al., 2019) and reinforcement learning (Mazoure et al., 2020; Ward et al., 2019; Touati et al., 2020).

6 CONCLUSION

General continuous belief states inference is a crucial yet challenging problem in high-dimensional
Partially Observable Markov Decision Process (POMDP) problems. In this paper, we propose the
**FlOw-based Recurrent BElief State model (FORBES) that can learn general continuous belief states**
by incorporating normalizing flows into the variational inference framework and then effectively utilize the learned belief states in downstream RL tasks. We show that theoretically, our method can
accurately learn the true belief states and we verify the effectiveness of our method in terms of both
the quality of learned belief states and the final performance of our extended POMDP RL framework on two visual input environments. The digit writing tasks demonstrate that our method can
learn general belief states that enable precise and multi-modal predictions and high-quality reconstructions. General belief inference plays a vital role in solving the POMDP, and our method paves
a way towards it. In the future, we will explore further approaches to improve the accuracy of belief
states inference and information seeking, such as combining contrastive learning and using advanced
network architectures such as transformers to build normalizing flows.


-----

7 REPRODUCIBILITY

We implement FORBES based on the Pytorch version of Dreamer, provided by
[“https://github.com/yusukeurakami/dreamer-pytorch”](https://github.com/yusukeurakami/dreamer-pytorch) and our code will
[be released after the author notification in “https://anonymous.4open.science/r/](https://anonymous.4open.science/r/Implementation-of-FORBES-E45B)
[Implementation-of-FORBES-E45B”. We describe the implementation details as well as](https://anonymous.4open.science/r/Implementation-of-FORBES-E45B)
the hyperparameters in Appendix A.2. We train our FORBES model on NVIDIA Tesla V100 with
Pytorch 1.5.0.

REFERENCES

Ali-Akbar Agha-Mohammadi, Suman Chakravorty, and Nancy M Amato. Firm: Sampling-based
feedback motion-planning under motion uncertainty and imperfect measurements. The Interna_tional Journal of Robotics Research, 33(2):268–304, 2014._

Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information
bottleneck. arXiv preprint arXiv:1612.00410, 2016.

Gabriel Barth-Maron, Matthew W. Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva TB,
Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. Distributed distributional deterministic
policy gradients, 2018.

Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Bengio. Generating sentences from a continuous space. CONLL, 2016.

Lars Buesing, Theophane Weber, S´ebastien Racaniere, SM Eslami, Danilo Rezende, David P Reichert, Fabio Viola, Frederic Besse, Karol Gregor, Demis Hassabis, et al. Learning and querying
fast generative models for reinforcement learning. arXiv preprint arXiv:1802.03006, 2018.

Kyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.

[Edwin D. De Jong. The mnist sequence dataset. https://edwin-de-jong.github.io/](https://edwin-de-jong.github.io/blog/mnist-sequence-data/)
[blog/mnist-sequence-data/, 2016. Accessed: 2019-07-07.](https://edwin-de-jong.github.io/blog/mnist-sequence-data/)

Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components estimation. In ICLR, 2015.

Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp, 2017.

Andreas Doerr, Christian Daniel, Martin Schiegg, Nguyen-Tuong Duy, Stefan Schaal, Marc Toussaint, and Trimpe Sebastian. Probabilistic recurrent state-space models. In International Confer_ence on Machine Learning, pp. 1280–1289. PMLR, 2018._

Karol Gregor, George Papamakarios, Frederic Besse, Lars Buesing, and Theophane Weber. Temporal difference variational auto-encoder, 2019.

David Ha and J¨urgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018.

Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning
behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019a.

Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels, 2019b.

Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models, 2021.

Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving flowbased generative models with variational dequantization and architecture design. In International
_Conference on Machine Learning, pp. 2722–2730. PMLR, 2019._


-----

Geoffrey A Hollinger and Gaurav S Sukhatme. Sampling-based robotic information gathering algorithms. The International Journal of Robotics Research, 33(9):1271–1287, 2014.

Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron Courville. Neural autoregressive
flows. In ICML, 2018.

Maximilian Igl, Luisa Zintgraf, Tuan Anh Le, Frank Wood, and Shimon Whiteson. Deep variational
reinforcement learning for pomdps, 2018.

Vadim Indelman, Luca Carlone, and Frank Dellaert. Planning in the continuous domain: A generalized belief space approach for autonomous navigation in unknown environments. The Interna_tional Journal of Robotics Research, 34(7):849–882, 2015._

Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction
to variational methods for graphical models. Machine learning, 37(2):183–233, 1999.

Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in
partially observable stochastic domains. Artificial intelligence, 1998.

Maximilian Karl, Maximilian Soelch, Justin Bayer, and Patrick Van der Smagt. Deep variational bayes filters: Unsupervised learning of state space models from raw data. arXiv preprint
_arXiv:1605.06432, 2016._

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
_arXiv:1412.6980, 2014._

Diederik P. Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling.
Improving variational inference with inverse autoregressive flow, 2017.

Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In
_NeurIPS, 2018._

Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved variational inference with inverse autoregressive flow. In NIPS, 2016.

Zhifeng Kong and Kamalika Chaudhuri. The expressive power of a class of normalizing flow models, 2020.

Rahul G Krishnan, Uri Shalit, and David Sontag. Deep kalman filters. _arXiv preprint_
_arXiv:1511.05121, 2015._

Manoj Kumar, Mohammad Babaeizadeh, Dumitru Erhan, Chelsea Finn, Sergey Levine, Laurent
Dinh, and Durk Kingma. Videoflow: A flow-based generative model for video. arXiv preprint
_arXiv:1903.01434, 2(5), 2019._

Hanna Kurniawati and Vinay Yadav. An online pomdp solver for uncertainty planning in dynamic
environment. In Robotics Research, pp. 611–629. Springer, 2016.

Hanna Kurniawati, David Hsu, and Wee Sun Lee. Sarsop: Efficient point-based pomdp planning
by approximating optimally reachable belief spaces. In Robotics: Science and systems, volume
2008. Citeseer, 2008.

Alex X. Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic:
Deep reinforcement learning with a latent variable model, 2020.

Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
_preprint arXiv:1509.02971, 2015._

Xiao Ma, Siwei Chen, David Hsu, and Wee Sun Lee. Contrastive variational reinforcement learning
for complex observations, 2020a.

Xiao Ma, Peter Karkus, David Hsu, and Wee Sun Lee. Particle filter recurrent neural networks. In
_Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pp. 5101–5108, 2020b._


-----

Bogdan Mazoure, Thang Doan, Audrey Durand, Joelle Pineau, and R Devon Hjelm. Leveraging
exploration in off-policy algorithms via normalizing flows. In Conference on Robot Learning, pp.
430–444. PMLR, 2020.

Volodymyr Mnih, Adri`a Puigdom`enech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning, 2016.

Masashi Okada and Tadahiro Taniguchi. Dreaming: Model-based reinforcement learning by latent
imagination without reconstruction. arXiv preprint arXiv:2007.14535, 2020.

Masashi Okada and Tadahiro Taniguchi. Dreaming: Model-based reinforcement learning by latent
imagination without reconstruction, 2021.

Masashi Okada, Norio Kosaka, and Tadahiro Taniguchi. Planet of the bayesians: Reconsidering and
improving deep planning network by incorporating bayesian inference, 2020.

George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for density
estimation, 2018.

Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. In
_ICML, 2015._

Tim Salimans, Diederik Kingma, and Max Welling. Markov chain monte carlo and variational
inference: Bridging the gap. In International Conference on Machine Learning, pp. 1218–1226.
PMLR, 2015.

Guy Shani, Joelle Pineau, and Robert Kaplow. A survey of point-based pomdp solvers. Autonomous
_Agents and Multi-Agent Systems, 27(1):1–51, 2013._

David Silver and Joel Veness. Monte-carlo planning in large pomdps. Neural Information Processing
Systems, 2010.

Richard D Smallwood and Edward J Sondik. The optimal control of partially observable markov
processes over a finite horizon. Operations research, 21(5):1071–1088, 1973.

Edward Jay Sondik. The optimal control of partially observable Markov processes. Stanford University, 1971.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.

Esteban G Tabak and Cristina V Turner. A family of nonparametric density estimation algorithms.
_Communications on Pure and Applied Mathematics, 66(2):145–164, 2013._

Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, Timothy Lillicrap, and Martin Riedmiller. Deepmind control suite, 2018.

Takeshi Teshima, Isao Ishikawa, Koichi Tojo, Kenta Oono, Masahiro Ikeda, and Masashi Sugiyama.
Coupling-based invertible neural networks are universal diffeomorphism approximators, 2020.

Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv
_preprint physics/0004057, 2000._

Ahmed Touati, Harsh Satija, Joshua Romoff, Joelle Pineau, and Pascal Vincent. Randomized value
functions via multiplicative normalizing flows. In Uncertainty in Artificial Intelligence, pp. 422–
432. PMLR, 2020.

Jur Van Den Berg, Sachin Patil, and Ron Alterovitz. Motion planning under uncertainty using
iterative local optimization in belief space. The International Journal of Robotics Research, 31
(11):1263–1278, 2012.

Rianne van den Berg, Leonard Hasenclever, Jakub M. Tomczak, and Max Welling. Sylvester normalizing flows for variational inference, 2019.


-----

Patrick Nadeem Ward, Ariella Smofsky, and Avishek Joey Bose. Improving exploration in softactor-critic with normalizing flows policies. arXiv preprint arXiv:1906.02771, 2019.

Guangxiang Zhu, Minghao Zhang, Honglak Lee, and Chongjie Zhang. Bridging imagination and
reality for model-based deep reinforcement learning, 2020.

K.J Astr¨[˚] om. Optimal control of markov processes with incomplete state information. _Journal_
_of Mathematical Analysis and Applications, 10(1):174–205, 1965._ ISSN 0022-247X. doi:
https://doi.org/10.1016/0022-247X(65)90154-X. [URL https://www.sciencedirect.](https://www.sciencedirect.com/science/article/pii/0022247X6590154X)
[com/science/article/pii/0022247X6590154X.](https://www.sciencedirect.com/science/article/pii/0022247X6590154X)

A APPENDIX

A.1 DETAILS OF AFFINE COUPLING LAYER FOR NORMALIZING FLOW

In this section, we will introduce the details about the affine coupling layer (Dinh et al., 2017).

In the forward function, we split the input x ∈ R[D] into two parts according to the dimension:
**x = [x1:k, xk+1:D]. Then, we let the first part x1:k stay identical, so that the first k dimensions in**
the output y ∈ R[D] is y1:k = x1:k. After that, we use the identical part as the inputs to determine the
transform parameters. In our case, we define two neural network s, t : R[k] _→_ R[D][−][k], which stand
for scale and translation functions. They receive x1:k as inputs and output the affine parameters. As
in (Dinh et al., 2017), the second part can be derived by:

**yk+1:D = xk+1:D ⊙** exp(s(x1:k)) + t(x1k ) (15)

Finally, the output, y is the concatenation of the two parts: y = [y1:k, yk+1:D].

The affine coupling layer is an expressive transformation with easily-computed forward and reverse
passes. The Jacobian of affine coupling layer is a triangular matrix, and its log determinant can also
be efficiently computed.

A.2 HYPER PARAMETERS AND IMPLEMENTATION DETAILS

**Network Architecture** We use the convolutional and deconvolutional networks that are similar
to Dreamer(Hafner et al., 2019a), a GRU (Cho et al., 2014) with 200 units in the dynamics model,
and implement all other functions as two fully connected layers of size 200 with ReLU activations.
Base distributions in latent space are 30-dimensional diagonal Gaussians with predicted mean and
standard deviation. As for the parameters network, we use a residual network composed of one fully
connected layer, one residual block, and one fully connected layer. The residual network receives
**xa and c as input. The input is first concatenated with the context and passed into the network. The**
residual block passes the input through two fully connected layers and returns the sum of the input
and the output. Finally the last layer outputs the parameters and we use 5 layers of affine coupling
flows with a LU layer between them.

In our case, we use samples from the belief distribution as the inputs to the actor and value function
as an approximation to the actor and value function with belief distribution as input. Calculating
_V (b) needs to integrate through both the observation model and state transition model. Our approx-_
imation makes an assumption like in Qmdp, to avoid integrating through the observation model.

We use a GRU as the recurrent neural network to summary to temporal information. We assume
an initial state s0 to be a zero vector. After taking action at, we concatenate at with the previous
state st and pass it through a small MLP to get yt = f (st, at), and use it as the input to the GRU:
_ht+1, zt+1 = GRU_ (ht, yt). We pass zt+1 through an MLP to get the base prior belief distribution
_p0 (mean and variance) and then we sample from p0 and pass it through a sequence of Normalizing_
Flow to get a sample from pK. For the posterior distribution, we first use a CNN as encoder to
encode the observation ot into the feature xt, and then concatenate zt+1 and xt and pass them
through an MLP to get the base posterior belief distribution q0 and a sequence of Normalizing Flow.
Similarly, we finally get a sample st+1 from qK.


-----

**Training Details** We basically adopt the same data buffer updating strategy as in Dreamer Hafner
et al. (2019a). First, we use a small amount of S seed episodes (S = 5 in DMC experiments) with
random actions to collect data. After that, we train the model for C update steps (C = 100 in DMC
experiment) and conduct one additional episode to collect data with small Gaussian exploration
noise added to the action. Algorithm 1 shows one update step in C update steps. After C update
steps, we conduct one additional episode to collect data (this is not shown in Algorithm 1). When
the agent interacts with the environment, we record the observations, actions, and rewards of the
whole trajectory ((ot, at, rt)[T]t=1[) and add it to data buffer][ B][.]

**Hyperparameters** For DMControl tasks, we pre-process images by reducing the bit depth to 5
bits and draw batches of 50 sequences of length 50 to train the FORBES model, value model,
and action model models using Adam (Kingma & Ba, 2014) with learning rates α0 = 5 × 10[−][4],
_α1 = 8 × 10[−][5], α2 = 8 × 10[−][5], respectively and scale down gradient norms that exceed 100. We_
clip the KL regularizers in JModel below 3.0 free nats as in Dreamer and PlaNet. The imagination
horizon is H = 15 and the same trajectories are used to update both action and value models. We
compute the TD-λ targets with γ = 0.99 and λ = 0.95. As for multiple imagined trajectories, we
choose N = 4 across all environments.

For digit writing experiments in Section 4.1, we decrease the GRU hidden size to be 20, let the base
distributions be a 2-dimensional diagonal Gaussian and only use 3 layers of affine coupling flows.
For the image processing, we simply divide the raw pixels by 255 and subtract 0.5 to make the inputs
lie in [−0.5, 0.5].

A.3 EXTENDED INFORMATION OF BASELINES

For model-free baselines, we compare with D4PG (Barth-Maron et al., 2018), a distributed extension
of DDPG, and A3C (Mnih et al., 2016), the distributed actor-critic approach. D4PG is an improved
variant of DDPG (Lillicrap et al., 2015) that uses distributed collection, distributional Q-learning,
multi-step returns, and prioritized replay. We include the scores for D4PG with pixel inputs and A3C
(Mnih et al., 2016) with vector-wise state inputs from DMCcontrol. For model-based baselines, we
use PlaNet (Hafner et al., 2019b) and Dreamer (Hafner et al., 2019a), two state-of-the-art modelbased RL. PlaNet (Hafner et al., 2019b) selects actions via online planning without an action model
and drastically improves over D4PG and A3C in data efficiency. Dreamer (Hafner et al., 2019a)
further improve the data efficiency by generating imaginary rollouts in the latent space.


-----

A.4 AN ABLATION STUDY ON THE NUMBER OF IMAGINED TRAJECTORIES

Cartpole Swingup Cheetah Run Finger Spin

800 1000

800 700

800

600

600 500 600

400

400 300 400

Episode Return 200 200

200 100

0 0

100 200 300 400 500 100 200 300 400 500 0 100 200 300 400 500

Hopper Stand 700 Walker Run 1000 Walker Walk

800 600

800

500

600

400 600

400 300

400

200

Episode Return200

200

100

0 0 0

0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500

Environment Steps (K) Environment Steps (K) Environment Steps (K)

N=1 N=2 N=4


Figure 8: An ablation study on the effect of different N on DMC environments.

To show the effect of N, we adjust the number of imagined trajectories on some DMC environments.
We choose N = 1, 2, 4 and run 500K environment steps. We run N = 1, 2 with 3 different seeds,
and N = 4 with 5 different seeds (we use the main DMC experiment results, where N = 4 here).
The result shows that, in Finger Spin, the performance gain caused by multiple imagined trajectories
is obvious. In finger spin, there are two objects and their interactions may result in complex locomotion patterns. When the environmental locomotion pattern itself is complex and flexible enough
to incorporate diverse possibilities, then using FORBES allows the agent to make diverse predictions and using the multiple imagined trajectories technique will further exploit the advantages of
FORBES. However, not all environments can show the advantages of multiple imaginations. In
other environments, where there’s only one agent and its behavior is relatively unimodal, a larger N
does not effectively improve the performance, and different N results in similar performances.


-----

A.5 EXTENDED RESULTS ON DMC

|0.0 0.2 0.4 0.6 0.8 1.0 Walker Walk|Col2|
|---|---|
|||
|||
|||


Cartpole Swingup Cheetah Run Finger Spin

900 800 1000

800 700

800

700 600

600 500 600

500 400

400 300 400

Episode Return300 200 200

200 100

100 0 0

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

Hopper Stand Walker Run Walker Walk

1000

700

800 600 800

600 500

600

400

400 300 400

200

Episode Return200 200

100

0 0 0

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

Environment Steps (M) Environment Steps (M) Environment Steps (M)

FORBES D4PG (1e9 steps) PlaNet (1e6 steps)
Dreamer A3C (1e9 steps, proprio)


Figure 9: The training curve on DMC environment for 1M environment steps.

We run our algorithm for 1M environment steps and show the curve in Figure 9. We choose 1M
environment steps because most of the curves have converged in most of the environments. FORBES
achieves higher scores than Dreamer in most of the tasks.


-----

A.6 AN ABLATION STUDY ON THE MODEL PARAMETERS



Cheetah Run Finger Spin

700 800

600

600

500

400 400

300

200 200

Episode Return

100

0

0

100 200 300 400 500 0 100 200 300 400 500

Environment Steps (K) Environment Steps (K)

Dreamer Dreamer+1L Dreamer+2L


Figure 10: An ablation study on the effect of adding parameters to Dreamer on two DMC environments.

In this section, we show that having a flexible belief state distribution is the key to improving performance, rather than introducing more parameters. Having more parameters do not necessarily mean
better performance. Increasing parameters may also make it difficult to converge and negatively
affect the sample efficiency.

We add an ablation study that adds more parameters to Dreamer to test the effectiveness of having
more parameters. We add 1, 2 hidden layer(s) to all the MLP in RSSM, and the result is shown in
Figure 10. The results show that simply adding parameters cannot improve the performance.


-----

A.7 COMPARISON OF ELBO ON FORBES AND RSSM ON DMC




Cheetah Run Walker Walk

0 10

2 15

4

20

6

25

8

30

0 50 100 150 200 250 0 50 100 150 200 250

Dreamer FORBES


Figure 11: The ELBO of FORBES and RSSM.

We provide the ELBO in DMC environments and FORBES in Figure 11, and FORBES has higher
ELBO.


-----

A.8 EVIDENCE LOWER BOUND DERIVATIONS

The variational bound for latent dynamics models _p (o1:T, s1:T | a1:T )_ =

_t_ _[p][(][s][t][|][s][t][−][1][, a][t][−][1][)][p][(][o][t][|][s][t][)][ and a variational posterior][ q][ (][s][1:][T][ |][ o][1:][T][, a][1:][T][ ) =][ Q]t_ _[q][ (][s][t][ |][ o][≤][t][, a][<t][)]_

follows from importance weighting and Jensen’s inequality as shown,

Q

_T_

log p (o1:T | a1:T ) = log Ep(s1:T |a1:T ) "t=1 _p (ot | st)#_

Y

_T_

= log Eq(s1:T |o1:T,a1:T ) "t=1 _p (ot | st) p (st | st−1, at−1) /q (st | o≤t, a<t)#_

Y

_T_

_≥_ Eq(s1:T |o1:T,a1:T ) "t=1 log p (ot | st) + log p (st | st−1, at−1) − log q (st | o≤t, a<t)

X

(16)


-----

A.9 PROOFS OF THEOREM

**Theorem 1:The approximation error of the lower bound is**

_T_

log p(o1:T, r1:T |a1:T ) −JModel = EqK (s1:T |τT,oT ) "t=1 _DKL(q(st|τt, ot)∥p(st | τt, ot))_

X


where p(st _τt, ot) is the true posterior._
_|_

**Proof:**
_DKL(q(st | τt, ot)∥p (st | st−1, at−1, ot)) | a1:T_

_q(st_ _τt, ot)_
= _q(st_ _τt, ot) log_ _|_
_|_ _p (st_ _st_ 1, at 1, ot) [d][s][t]
Z _|_ _−_ _−_

_q(st_ _τt, ot)_
= _q(st | τt, ot) log_ _p(st|st−1 |,at−1)p(ot|st)_ dst
Z _p(ot_ _a1:T )_

_|_

= _q(st | τt, ot) log q(st | τt, ot)dst +_ _q(st | τt, ot) log p(ot | a1:T )dst_
Z Z

_−_ _q(st | τt, ot) log[p(st | st−1, at−1)p(ot | st)]dst_
Z

= log p(ot _a1:T ) +_ _q(st_ _τt, ot) log q(st_ _τt, ot)dst_ _q(st_ _τt, ot) log[p(st_ _st_ 1, at 1)p(ot _st)]dst_
_|_ _|_ _|_ _−_ _|_ _|_ _−_ _−_ _|_
Z Z

= log p(ot _a1:T ) +_ _q(st_ _τt, ot) log q(st_ _τt, ot)dst_ _q(st_ _τt, ot) log p(st_ _st_ 1, at 1)dst
_|_ _|_ _|_ _−_ _|_ _|_ _−_ _−_
Z Z

_q(st_ _τt, ot) log p(ot_ _st)dst_
_−_ _|_ _|_
Z


_q(st_ _τt, ot)_
= log p(ot _a1:T ) +_ _q(st_ _τt, ot) log_ _|_ _q(st_ _τt, ot) log p(ot_ _st)dst_
_|_ _|_ _p(st_ _st_ 1, at 1) [d][s][t][ −] _|_ _|_
Z _|_ _−_ _−_ Z

= log p(ot _a1:T ) + DKL (q (st_ _τt, ot)_ _p (st_ _st_ 1, at 1, ot)) Eq(s1:t _τt,ot)[log p (ot_ _st)]_
_|_ _|_ _∥_ _|_ _−_ _−_ _−_ _|_ _|_
(17)

For a sequence from time 1 to T, we have


_DKL (q (st_ _τt, ot)_ _p (st_ _st_ 1, at 1, ot))
_t_ _|_ _∥_ _|_ _−_ _−_

X

_T_

= log p(o1:T _a1:T )_ Eq(s1:t _τt,ot)_ (log p(ot _st)_ _DKL(q(st_ _τt, ot)_ _p(st_ _st_ 1, at 1)))
_|_ _−_ _|_ "t=1 _|_ _−_ _|_ _∥_ _|_ _−_ _−_ #

X

(18)


Then we can derive the Theorem 1 with equation 18:
log p(o1:T, r1:T | a1:T )

=EqK (s1:T |τT,oT ) _t_ _DKL (q (st | τt, ot) ∥p (st | st−1, at−1, ot))#_

"X

_T_

+ Eq(s1:T |τT,oT ) "t=1(log p(ot|st) + log p(rt|st) − _DKL(q(st|τt, ot)∥p(st|st−1, at−1)))_

X

=EqK (s1:T |τT,oT ) _t_ _DKL (q (st | τt, ot) ∥p (st | st−1, at−1, ot))#_ + JModel

"X

=EqK (s1:T |τT,oT ) _t_ _DKL (q (st | τt, ot) ∥p (st | τt, ot))#_ + JModel

"X

where p(st _st_ 1, at 1, ot) = p(st _τt, ot) given the sampled st_ 1 from q(s1:t _τt, ot)._
_|_ _−_ _−_ _|_ _−_ _|_


(19)


-----

A.10 MORE RESULTS ON DIGIT WRITING EXPERIMENTS

In this section, we show more results of the predictions on the digit writing experiment in Figure 12.

Figure 12: Additional prediction results on sequential MNIST of two models.


-----

A.11 RECONSTRUCTIONS OF THE VISUAL CONTROL TASKS

(a) Cartpole Swing Up

(b) Cheetah Run

(c) Finger Spin

(d) Hopper Stand

(e) Walker Run

(f) Walker Walk

Figure 13: The reconstruction results on of FORBES six environments from DeepMind Control
Suite(Tassa et al., 2018).

In this section, we show the reconstructions of the visual control tasks during the evaluating phase.


-----

For each environment, we use 10 frames. The left one is the original picture for each frame, and the
right one is the reconstruction picture. The following results in Figure 13 show that FORBES can
make high-quality reconstructions. The corresponding videos can be found in the supplementary
material.


-----

