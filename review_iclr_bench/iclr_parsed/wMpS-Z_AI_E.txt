# A THEORETICAL ANALYSIS ON FEATURE LEARNING IN NEURAL NETWORKS: EMERGENCE FROM INPUTS AND ADVANTAGE OVER FIXED FEATURES

**Zhenmei Shi[*], Junyi Wei[*], Yingyu Liang**
University of Wisconsin-Madison
zhmeishi@cs.wisc.edu,jwei53@wisc.edu,yliang@cs.wisc.edu

ABSTRACT

An important characteristic of neural networks is their ability to learn representations of the input data with effective features for prediction, which is believed
to be a key factor to their superior empirical performance. To better understand
the source and benefit of feature learning in neural networks, we consider learning problems motivated by practical data, where the labels are determined by a
set of class relevant patterns and the inputs are generated from these along with
some background patterns. We prove that neural networks trained by gradient
descent can succeed on these problems. The success relies on the emergence and
improvement of effective features, which are learned among exponentially many
candidates efficiently by exploiting the data (in particular, the structure of the
input distribution). In contrast, no linear models on data-independent features of
polynomial sizes can learn to as good errors. Furthermore, if the specific input
structure is removed, then no polynomial algorithm in the Statistical Query model
can learn even weakly. These results provide theoretical evidence showing that feature learning in neural networks depends strongly on the input structure and leads
to the superior performance. Our preliminary experimental results on synthetic and
real data also provide positive support.

1 INTRODUCTION

Various empirical studies have shown that an important characteristic of neural networks is their
feature learning ability, i.e., to learn a feature mapping for the inputs which allow accurate prediction
(e.g., Zeiler & Fergus (2014); Girshick et al. (2014); Zhang et al. (2019); Manning et al. (2020)). This
is widely believed to be a key factor to their remarkable success in many applications, in particular, an
advantage over traditional machine learning methods. To understand their success, it is then crucial
to understand the source and benefit of feature learning in neural networks. Empirical observations
show that networks can learn neurons that correspond to different semantic patterns in the inputs
(e.g., eyes, bird shapes, tires, etc. in images (Zeiler & Fergus, 2014; Girshick et al., 2014)). Moreover,
recent progress (e.g., Caron et al. (2018); Chen et al. (2020b); He et al. (2020); Jing & Tian (2020))
shows that one can even learn a feature mapping using only unlabeled inputs and then learn an
accurate predictor (usually a linear function) on it using labeled data. This further demonstrates
the feature learning ability of neural networks and that these input distributions contain important
information for learning useful features. These empirical observations strongly suggest that the
structure of the input distribution is crucial for feature learning and feature learning is crucial for
the strong performance. However, it is largely unclear how practical training methods (gradient
descent or its variants) learn important patterns from the inputs and whether this is necessary for
obtaining the superior performance, since the empirical studies do not exclude the possibility that
some other training methods can achieve similar performance without feature learning or with feature
learning that does not exploit the input structure. Rigorous theoretical investigations are thus needed
for answering these fundamental questions: How can effective features emerge from inputs in the
_training dynamics of gradient descent? Is learning features from inputs necessary for the superior_
_performance?_

*Equal contribution


-----

Compared to the abundant empirical evidence, the theoretical understanding still remains largely
open. One line of work (e.g. Jacot et al. (2018); Li & Liang (2018); Du et al. (2019); Allen-Zhu et al.
(2019); Zou et al. (2020); Chizat et al. (2019) and many others) shows in certain regime, sufficiently
overparameterized networks are approximately linear models, i.e., a linear function on the Neural
Tangent Kernel (NTK). This falls into the traditional approach of linear models on fixed features,
which also includes random features (Rahimi & Recht, 2008) and other kernel methods (Kamath et al.,
2020). The kernel viewpoint thus does not explain feature learning in networks nor the advantage over
fixed features. A recent line of work (e.g. Daniely & Malach (2020); Bai & Lee (2019); Ghorbani
et al. (2020); Yehudai & Shamir (2019); Allen-Zhu & Li (2019; 2020a); Li et al. (2020); Malach
et al. (2021) and others) shows examples where networks provably enjoy advantages over fixed
features, under different settings and assumptions. While providing insightful results separating
the two approaches, most studies have not investigated if the input structure is crucial for feature
learning and thus the advantage. Also, most studies have not analyzed how gradient descent can
learn important input patterns as effective features, or rely on strong assumptions like models or data
atypical in practice (e.g., special networks, Gaussian data, etc).

Towards a more thorough understanding, we propose to analyze learning problems motivated by
practical data, where the labels are determined by a set of class relevant patterns and the inputs are
generated from these along with some background patterns. We use comparison for our study: (1) by
comparing network learning approaches with fixed feature approaches on these problems, we analyze
the emergence of effective features and demonstrate feature learning leads to the advantage over fixed
features; (2) by comparing these problems to those with the input structure removed, we demonstrate
that the input structure is crucial for feature learning and prediction performance.

More precisely, we obtain the following results. We first prove that two-layer networks trained by
gradient descent can efficiently learn to small errors on these problems, and then prove that no linear
models on fixed features of polynomial sizes can learn to as good errors. These two results thus
establish the provable advantage of networks and implies that feature learning leads to this advantage.
More importantly, our analysis reveals the dynamics of feature learning: the network first learns
a rough approximation of the effective features, then improves them to get a set of good features,
and finally learns an accurate classifier on these features. Notably, the improvement of the effective
features in the second stage is needed for obtaining the provable advantage. The analysis also reveals
the emergence and improvement of the effective features are by exploiting the data, and in particular,
they rely on the input structure. To formalize this, we further prove the third result: if the specific
input structure is removed and replaced by a uniform distribution, then no polynomial algorithm can
even weakly learn in the Statistical Query (SQ) learning model, not to mention the advantage over
fixed features. Since SQ learning includes essentially all known algorithms (in particular, mini-batch
stochastic gradient descent used in practice), this implies that feature learning depends strongly on
the input structure. Finally, we perform simulations on synthetic data to verify our results. We also
perform experiments on real data and observe similar phenomena, which show that our analysis
provides useful insights for the practical network learning.

Our analysis then provides theoretical support for the following principle: feature learning in neural
_networks depends strongly on the input structure and leads to the superior performance. In particular,_
our results make it explicit that learning features from the input structure is crucial for the superior
performance. This suggests that input-distribution-free analysis (e.g., traditional PAC learning) may
not be able to explain the practical success, and advocates an emphasis of the input structure in
the analysis. While these results are for our proposed problem setting and network learning in
practice can be more complicated, the insights obtained match existing empirical observations and
are supported by our experiments. The compelling evidence hopefully can attract more attention to
further studies on modeling the input structure and analyzing feature learning.

2 RELATED WORK

This section provides an overview while more technical discussions can be found in Appendix A.

**Neural Tangent Kernel (NTK) and Linearization of Neural Networks. One line of work (e.g. Jacot**
et al. (2018); Li & Liang (2018); Matthews et al. (2018); Lee et al. (2019); Novak et al. (2019);
Yang (2019); Du et al. (2019); Allen-Zhu et al. (2019); Zou et al. (2020); Ji & Telgarsky (2019); Cao
et al. (2020); Geiger et al. (2020); Chizat et al. (2019) and more) explains the success of sufficiently


-----

over-parameterized neural network by connecting them to linear methods like NTK. Though their
approaches are different, they all base on the observation that when the network is sufficiently large,
the weights stay close to the initialization during the training, and training is similar to solving a kernel
method problem. This is typically referred to as the NTK regime, or lazy training, or linearization.
However, networks used in practice are usually not large enough to enter this regime, and the weights
are frequently observed to traverse away from the initialization. Furthermore, in this regime, network
learning is essentially the traditional approach of linear methods over fixed features, which cannot
establish or explain feature learning and the advantage of network learning.

**Advantage of Neural Networks over Linear Models on Fixed Features. Since the superior**
network learning results via gradient descent are not well explained by the NTK view, a recent line
of work has turned to learning settings where neural networks provably have advantage over linear
models on fixed features (e.g. Daniely & Malach (2020); Refinetti et al. (2021); Malach et al. (2021);
Dou & Liang (2020); Bai & Lee (2019); Ghorbani et al. (2020); Allen-Zhu & Li (2019); see the
great summary in Malach et al. (2021)). While formally establishing the advantage, they have not
thoroughly answered the two fundamental questions this work focuses on; in particular, most existing
work has not studied whether the input structure is a crucial factor for feature learning and thus the
advantage, and/or has not considered how the features are learned in more practical training scenarios.
For example, Ghorbani et al. (2020) show the advantage of networks in approximation power and
Dou & Liang (2020) show their statistical advantage, but they do not consider the learning dynamics
(i.e., how the training method obtains the good network). Allen-Zhu & Li (2019) prove the advantage
of the networks for PAC learning with labels given by a depth-2 ResNet and Allen-Zhu & Li (2020a)
prove for Gaussian inputs with labels given by a multiple-layer network, while neither considers
the influence of the input structure on feature learning or the advantage. Daniely & Malach (2020)
prove the advantage of the networks for learning sparse parities on specific input distributions that
help gradient descent learn effective features for prediction, and Malach et al. (2021) consider similar
learning problems but with specifically designed differentiable models, while our work analyzes data
distributions and models closer to those in practice and also explicitly focuses on whether the input
structure is needed for the learning. There are also other theoretical studies on feature learning in
networks (e.g. Yehudai & Ohad (2020); Zhou et al. (2021); Diakonikolas et al. (2020); Frei et al.
(2020)), which however do not directly relate feature learning to the input structure or the advantage
of network learning.

3 PROBLEM SETUP

To motivate our setup, consider images with various kinds of patterns like lines and rectangles. Some
patterns are relevant for the labels (e.g., rectangles for distinguishing indoor or outdoor images),
while the others are not. If the image contains a sufficient number of the former, then we are confident
that the image belongs to a certain class. Dictionary learning or sparse coding is a classic model
of such data (e.g., Olshausen & Field (1997); Vinje & Gallant (2000); Blei et al. (2003)). We thus
model the patterns as a dictionary, generate a hidden vector indicating the presence of the patterns,
and generate the input and label from this vector.

Let X = R[d] be the input space, and Y = {±1} be the label space. Suppose M ∈ R[d][×][D] is an
unknown dictionary with D columns that can be regarded as patterns. For simplicity, assume M is
orthonormal. Let _ϕ[˜] ∈{0, 1}[D]_ be a hidden vector that indicates the presence of each pattern. Let
_A ⊆_ [D] be a subset of size k corresponding to the class relevant patterns. Then the input is generated
by M _ϕ[˜], and the label can be any binary function on the number of class relevant patterns. More_
precisely, let P ⊆ [k]. Given A and P, we first sample _ϕ[˜] from a distribution D ˜ϕ[, and then generate]_
the input ˜x and the class label y from _ϕ[˜]:_

_ϕ˜_ ˜ϕ[,] _x˜ = M_ _ϕ,[˜]_ _y =_ +1, if _i∈A_ _ϕ[˜]i ∈_ _P,_ (1)
_∼D_ 1, otherwise.
−

[P]

**Learning with Input Structure. We allow quite general D ˜ϕ** [with the following assumptions:]

**(A0) The class probabilities are balanced: Pr[[P]i** _A_ _ϕ[˜]i_ _P_ ] = 1/2.

**(A1) The patterns in A are correlated with the labels with the same correlation: for any∈** _∈_ _i ∈_ _A,_
_γ = E[yϕ[˜]i] −_ E[y]E[ϕ[˜]i] > 0.


-----

**(A2) Each pattern outside A is identically distributed and independent of all other patterns. Let**
_po := Pr[ϕ[˜]i = 1] and without loss of generality assume po ≤_ 1/2.

Let D(A, P, D ˜ϕ[)][ denote the distribution on][ (˜]x, y) for some A, P, and D ˜ϕ[. Given parameters][ Ξ =]
(d, D, k, γ, po), the family FΞ of distributions include all D(A, P, D ˜ϕ[)][ with][ A][ ⊆] [[][D][]][,][ P][ ⊆] [[][k][]][, and]
_D ˜ϕ_ [satisfying the above assumptions. The labeling function includes some interesting special cases:]

_Example 1. Suppose P = {i ∈_ [k] : i > k/2} for some threshold, i.e., we will set the label y = +1
when more than a half of the relevant patterns are presented in the input.

_Example 2. Suppose k is odd, and let P = {i ∈_ [k] : i is odd}, i.e., the labels are given by the parity
function on _ϕ[˜]j(j_ _A). This is useful to prove our lower bounds via the properties of parities._
_∈_

Appendix F presents results for more general settings (e.g., incoherent dictionary, unbalanced classes,
etc.). On the other hand, our problem setup does not include some important data models. In
particular, one would like to model hierarchical representations often observed in practical data and
believed to be important for deep learning. We leave such more general cases for future work.

**Learning Without Input Structure. For comparison, we also consider learning problems without**
input structure. The data are generated as above but with different distributions D ˜ϕ[:]

**(A1’) The patterns are uniform over {0, 1}[D]: for any i ∈** [D], Pr[ϕ[˜]i = 1] = 1/2 independently.

Given parameters Ξ0 = (d, D, k), the family FΞ0 of distributions without input structure is the set of
all the distributions with A ⊆ [D], P ⊆ [k] and D ˜ϕ [satisfying the above assumptions.]

3.1 NEURAL NETWORK LEARNING

**Networks. We consider training a two-layer network via gradient descent on the data distribution:**

2m


_aiσ(_ _wi, x_ + bi) (2)
_⟨_ _⟩_
_i=1_

X


_g(x) =_


where(ReLU) activation function. Let wi ∈ R[d], bi, ai ∈ R, and θ = σ( {zw) = min(1i, bi, ai}i[2]=1[m], max([denote all the parameters, and let superscript]z, 0)) is the truncated rectified linear unit[ (][t][)]
denote the time step, e.g., g[(][t][)] denote the network at time step t with θ[(][t][)] = {wi[(][t][)][, b][(]i[t][)][, a][(]i[t][)][}][.]

**Loss Function. Similar to typical practice, we will normalize the data for learning: first compute**
_x = (˜x −_ E[˜x])/σ˜ where ˜σ[2] = E _i=1[(˜]xi −_ E[˜xi])[2] is the variance of the data, and then train on
(x, y). This is equivalent to setting ϕ = (ϕ[˜] − E[ϕ[˜]])/σ˜ and generating x = Mϕ. For (˜x, y) from D
and the normalized (x, y), we will simply say[P][d] (x, y) .
_∼D_

For the training, we consider the hinge-loss ℓ(y, ˆy) = max{1 − _yy,ˆ_ 0}. We will inject some noise ξ
to the neurons for the convenience of the analysis. (This can be viewed as using a smoothed version
of the activation ˜σ(z) = Eξσ(z + ξ) similar to those in existing studies like Allen-Zhu & Li (2020b);
Malach et al. (2021). See Section 5 for more explanations.) Formally, the loss is:


2m

_aiEξ[σ(⟨wi, x⟩_ + bi + ξi)] (3)
_i=1_

X


_L_ (g; σξ) = E(x,y)[ℓ(y, g(x; ξ))], where g(x; ξ) =
_D_


where ξ ∼N (0, σξ[2][I][m][×][m][)][ are independent Gaussian noise. Let][ L][D][(][g][)][ denote the typical hinge-loss]
without noise. We also consider ℓ2 regularization: R(g; λa, λw) = _i=1_ _[λ][a][|][a][i][|][2][ +][ λ][w][∥][w][i][∥]2[2]_ [with]
regularization coefficients λa, λw.

[P][2][m]

**Training Process. We first perform an unbiased initialization: for every i** [m], initialize wi[(0)]
_∈_ _∼_
(0, σw[2] _[I][d][×][d][)][ with][ σ][w]_ [= 1][/k][,][ b][(0)]i 0, σb[2] with σb = 1/k[2], a[(0)]i (0, σa[2][)][ with][ σ][a] [=]
_N_ _∼N_ _∼N_
_σ˜[2]/(γk[2]), and then set wm[(0)]+i_ [=][ w]i[(0)], b[(0)]m+i [=][ b]i[(0)][,][ a]m[(0)]+i [=][ −][a]i[(0)][. We then do gradient updates:]

_θ[(][t][)]_ = θ[(][t][−][1)] _−_ _η[(][t][)]∇θ_ _LD(g[(][t][−][1)]; σξ[(][t][)][) +][ R][(][g][(][t][−][1)][;][ λ]a[(][t][)][, λ][(]w[t][)][)]_ _, for t = 1, 2, . . ., T,_ (4)
 

for some choice of the hyperparameters η[(][t][)], λ[(]a[t][)][, λ]w[(][t][)][,][ σ]ξ[(][t][)][, and][ T] [.]


-----

4 MAIN RESULTS

**Provable Guarantee for Neural Networks. The network learning has the following guarantee:**

**Theorem 1. For any δ, ϵ** _∈_ (0, 1), if k = Ω log[2] (D/(δγ)) _, po_ = Ω(k[2]/D), and
max Ω(k[12]/ϵ[3][/][2]), D _m_ _poly(D), then with properly set hyperparameters, for any_ Ξ,
_{_ _} ≤_ _≤_    _D ∈F_
_with probability at least 1_ _δ, there exists t_ [T ] such that Pr[sign(g[(][t][)](x)) = y] _L_ (g[(][t][)]) _ϵ._
_−_ _∈_ _̸_ _≤_ _D_ _≤_

The theorem shows that for a wide range of the background pattern probability po and the number of
class relevant patterns k, the network trained by gradient descent can obtain a small classification
error. More importantly, the analysis shows the success comes from feature learning. In the early
stages, the network learns and improves the neuron weights such that on the features (i.e., the neurons’
outputs) there is an accurate classifier; afterwards it learns such a classifier. The next section will
provide a detailed discussion on the feature learning.

**Lower Bound for Fixed Features. Empirical observations and Theorem 1 do not exclude the**
possibility that some methods without feature learning can achieve similar performance. We thus
prove a lower bound for the fixed feature approach, i.e., linear models on data-independent features.

**Theorem 2. Suppose Ψ is a data-independent feature mapping of dimension N with bounded features,**
_i.e., Ψ : X →_ [−1, 1][N] _. For B > 0, the family of linear models on Ψ with bounded norm B is_
_HB = {Ξ such that allh(˜x) : h(˜x) = h_ _⟨Ψ(˜xB) have hinge-loss at least, w⟩, ∥w∥2 ≤_ _B}. If 3 < k po_ _≤1_ _D/√1622NB[k] and. k is odd, then there exists_

_D ∈F_ _∈H_ _−_
 

So using fixed features independent of the data cannot get loss nontrivially smaller than po unless
with exponentially large models. In contrast, viewing the neurons σ( _wi, x_ + bi) as learned features,
_⟨_ _⟩_
network learning can achieve any loss ϵ ∈ (0, 1) with models of polynomial sizes. We emphasize
the lower bound is because the feature map Ψ is independent of the data. Indeed, there exists a
small linear model on a small dimensional feature map allowing 0 loss for each data distribution
in our problem set FΞ (Lemma 5). However, this feature map Ψ[∗] is different for different data
distribution in Ξ, i.e., depends on the data. On the other hand, the feature map Ψ in the lower
_F_
bound is data-independent, i.e., fixed before seeing the data. For Ψ to work simultaneously for all
distributions in Ξ, it needs to have exponential dimensions. Intuitively, it needs a large number
_F_
of features, so that there are some features to approximate each Ψ[∗]i [. There are exponentially many]
data distributions in FΞ, and thus exponentially many data-dependent features Ψ[∗]i [, which requires]
Ψ to have an exponentially large dimension. Network learning updates the hidden neurons using
the data and can learn to move the features to the right positions to approximate the ground-truth
data-dependent features Ψ[∗], so it does not need an exponentially large dimension feature map.

The theorem directly applies to linear models on fixed finite-dimensional feature maps, e.g., linear
models on the input or random feature approaches (Rahimi & Recht, 2008). It also implies lower
bounds to infinite dimensional feature maps (e.g., some kernels) that can be approximated by feature
maps of polynomial dimensions. For example, Claim 1 in Rahimi & Recht (2008) implies that a
function f using shift-invariant kernels (e.g., RBF) can be approximated by a model ⟨Ψ(˜x), w⟩ with
the dimension N and weight norm B bounded by polynomials of the related parameters of f like its
RKHS norm and the input dimension. Then our theorem implies some related parameter of f needs
to be exponential in k for f to get nontrivial loss, formalized in Corollary 3. Kamath et al. (2020) has
more discussions on approximating kernels with finite dimensional maps.

**Corollary 3. For any function f using a shift-invariant kernel K with RKHS norm bounded by L, or**
_f_ (x) = _i_ _[α][i][K][(][z][i][, x][)][ for some data points][ z][i][ and][ ||][α][||][2][ ≤]_ _[L][. If][ 3][ < k][ ≤]_ _[D/][16][ and][ k][ is odd, then]_

_there exists_ Ξ such that f has hinge-loss at least po 1 2[k] poly(1d,L) _[.]_
_D ∈F_ _−_ [poly][(][d,L][)] _−_

[P]

 

**Lower Bound for Without Input Structure. Existing results do not exclude the possibility that**
some learning methods without exploiting the input structure can achieve strong performance. To
show the necessity of the input structure, we consider learning FΞ0 with input structure removed. We
obtain a lower bound for such learning problems in the classic Statistical Query (SQ) model (Kearns,
1998). In this model, the algorithm can only receive information about the data through statistical
queries. A statistical query is specified by some polynomially-computable property predicate Q of
labeled instances and a tolerance parameter τ ∈ [0, 1]. For a query (Q, τ ), the algorithm receives


-----

a responsesimulated using the average of roughlyP[ˆ]Q ∈ [PQ − _τ, PQ + τ_ ], where O(1/τ P[2]Q) random data samples with high probability. The SQ = Pr[Q(x, y) is true]. Notice that a query can be
model captures almost all common learning algorithms (except Gaussian elimination) including the
commonly used mini-batch SGD, and thus is suitable for our purpose.

**Theorem 4.error less than For any algorithm in the Statistical Query model that can learn over[1]2** (Dk1[)]3 _[, either the number of queries or][ 1][/τ][ must be at least][ 1] F2_ ΞDk0 to classification1/3.

_[−]_

  

The theorem shows that without the input structure, polynomial algorithms in the SQ model cannot
get a classification error nontrivially smaller than random guessing. The comparison to the result for
with input structure then shows that the input structure is crucial for network learning, in particular,
for achieving the advantage over fixed feature models.

5 PROOF SKETCHES

Here we provide the sketch of our analysis, focusing on the key intuition and discussing some
interesting implications. The complete proofs are included in Appendix B-D.

5.1 PROVABLE GUARANTEES OF NEURAL NETWORKS

**Overall Intuition. We first show that there is a two-layer network that can represent the target**
labeling function, whose neurons can be viewed as the “ground-truth” features to be learned. We
then show that after the first gradient step, the hidden neurons of the trained network become close
to the ground-truth: their weights contain large components along the class relevant patterns but
small along the background patterns. We further show that in the second gradient step, these features
get improved: the “signal-noise” ratio between the components for class relevant patterns and those
for the background ones becomes larger, giving a set of good features. Finally, we show that the
remaining steps learn an accurate classifier on these features.

**Existence of A Good Network. We show that there is a two-layer network that can fit the labels.**

**Lemma 5. For any D ∈FΞ, there exists a network g[∗](x) =** _i=1_ _[a]i[∗][σ][(][⟨][w]i[∗][, x][⟩][+][b]i[∗][)][ with][ y][ =][ g][∗][(][x][)]_
_for any (x, y)_ _. Furthermore, the number of neurons n = 3(k + 1),_ _a[∗]i_
_|b[∗]i_ _[| ≤]_ [1][/][2][,][ w]i[∗] ∼D[= ˜]σ _j∈A_ _[M][j][/][(4][k][)][, and][ |⟨][w]i[∗][, x][⟩]_ [+][ b]i[∗][| ≤] [P][1][ for any][n] _[ i][ ∈]_ [[][n] |[]][ and][| ≤][ (][x, y][32][k,][)][ ∼D][ 1][/][(32][.] _[k][)][ ≤]_

In particular, the weights of the neurons are proportional to[P] _j∈A_ _[M][j][, the sum of the class relevant]_

patterns. We thus focus on analyzing how the network learns such neuron weights.

**Feature Emergence in the First Gradient Step. The gradient for[P]** _wi (ignoring the noise) is:_


_∂L_ (g)
_D_ = _aiE(x,y)_ _yI[yg(x)_ 1]σ[′][ _wi, x_ + bi]x = _aiE(x,y)_ _yxσ[′][_ _wi, x_ + bi]

_∂wi_ _−_ _∼D {_ _≤_ _⟨_ _⟩_ _}_ _−_ _∼D {_ _⟨_ _⟩_ _}_

where the last step is due to g(x) = 0 by the unbiased initialization. Let qj = ⟨Mj, wi⟩ denote the
component along the direction of the pattern Mj. Then the component of the gradient on Mj is:

_∂_
_Mj,_ _L_ (g) = _aiE_ _yϕjσ[′][_ _wi, x_ + bi] = _aiE_ _ϕℓqℓ_ + bi

_∂wi_ _D_ _−_ _{_ _⟨_ _⟩_ _}_ _−_   

  _ℓ∈[D]_ 

 [X] 

The key intuition is that with the randomness of ϕℓ (and potentially that of the injected noise[yϕ][j][σ][′]  ξ), the[.]
random variable under σ[′] is not significantly affected by a small subset of ϕℓqℓ. For example, for
class relevant patterns j ∈ _A, let I[D] := σ[′][ hP]ℓ∈[D]_ _[ϕ][ℓ][q][ℓ]_ [+][ b][i] and I−A := σ[′][ hP]ℓ̸∈A _[ϕ][ℓ][q][ℓ]_ [+][ b][i] .

We have I[D] I _A and thus:_ i i
_−_
_≈_
_∂_
_Mj,_ _L_ (g) E _yϕjI[D]_ E _yϕjI_ _A_ = E _yϕj_ E[I _A] =_ _[γ]_

_∂wi_ _D_ _∝_ _≈_ _{_ _−_ _}_ _{_ _}_ _−_ _σ˜_ [E][[][I][−][A][]]

 



since y only depends on ϕj(j _A). Then the gradient has a nontrivial component along the pattern._
_∈_
Similarly, for background patterns j ̸∈ _A, the component of the gradient along Mj is close to 0._


-----

**Lemma 6 (Informal). Assume po, k as in Theorem 1 and σξ[(1)]** _< 1/k, then with high probability_

_∂_ _D_

_∂wi_ _[L][D][(][g][(0)][;][ σ]ξ[(1)][) =][ −][a]i[(0)]_ _j=1_ _[M][j][T][j][ where for a small][ ϵ][e][:]_

-  if j ∈ _A, then |Tj −Pβγ/σ˜| ≤_ _O(ϵe/σ˜) with β ∈_ [Ω(1), 1];

-  if j ̸∈ _A, then |Tj| ≤_ _O(σϕ[2][ϵ][e]σ[˜])._

By setting λ[(1)]w = 1/(2η[(1)]), we have wi[(1)] = η[(1)]a[(0)]i _Dj=1_ _[M][j][T][j][ ≈]_ _[η][(1)][a]i[(0)]_ _βγσ˜_ _j_ _A_ _[M][j][. For]_

_∈_
small po, e.g., po = O[˜](k[2]/D), these neurons can already allow accurate prediction. However,

P P

for such small po, we cannot show a provable advantage of networks over fixed features. On the
other hand, for larger po meaning a significant number of background patterns in the input, the
approximation error terms Tj(j _A) together can overwhelm the signals Tj(j_ _A) and lead to bad_
_̸∈_ _∈_
prediction, even though each term is small. Fortunately, we will show that the second gradient step
can improve the weights by decreasing the ratio between Tj(j _A) and Tj(j_ _A)._
_̸∈_ _∈_

**Feature Improvement in the Second Gradient Step. We note that by setting a small η[(1)], after the**
update we still have yg(x; ξ) < 1 for most (x, y) ∼D and thus the gradient in the second step is:

_∂_

_L_ (g; σξ) _aiE(x,y)_ _yxEξσ[′][_ _wi, x_ + bi + ξi] _._
_∂wi_ _D_ _≈−_ _∼D {_ _⟨_ _⟩_ _}_


We can then follow the intuition for the first step again. For j ∈ _A, the component ⟨Mj,_ _∂w∂_ _i_ _[L][D][(][g][)][⟩]_

is roughly proportional to _σ[γ]˜_ [E][[][I][−][A,ξ][]][ where][ I][−][A,ξ][ :=][ σ][′][ hP]ℓ̸∈A _[ϕ][ℓ][q][ℓ]_ [+][ b][i][ +][ ξ][i] . While ϕℓqℓ may

not have large enough variance, the injected noise ξi makes sure that a nontrivial amount of datai
activate the neuron.[1] Then I _A,ξ_ = 0, leading to a nontrivial component along Mj, similar to
the first step. On the other hand, for− _̸_ _j ̸∈_ _A, the approximation error term Tj depends on how_

well σ[′][ hP]ℓ̸∈A,ℓ≠ _j_ _[ϕ][ℓ][q][ℓ]_ [+][ b][i][ +][ ξ][i] approximates σ[′][ hP]ℓ∈[D] _[ϕ][ℓ][q][ℓ]_ [+][ b][i][ +][ ξ][i] . Since the qℓ’s (the

weight’s component along Mℓ) in the second step are small compared to those in the first step, wei i
can then get a small error term Tj. So the ratio between Tj(j _A) over Tj(j_ _A) improves after_
_̸∈_ _∈_
the second step, giving better features allowing accurate prediction.

**Classifier Learning Stage. Given the learned features, we are then ready to show the remaining**
gradient steps can learn accurate classifiers. Intuitively, with small hyperparameter values (η[(][t][)] =
_k[2]_ _k[3]_

_T m[1][/][3][, λ]a[(][t][)]_ = λ[(]w[t][)] _[≤]_ _σm˜_ [1][/][3][, σ]ξ[(][t][)] = 0 for 2 < t ≤ _T = m[4][/][3]), the first layer’s weights do not_

change too much and thus the learning is similar to convex learning using the learned features.
Formally, our proof uses the online convex optimization technique in Daniely & Malach (2020).

5.2 LOWER BOUNDS


The lower bounds are based on the following observation: our problem setup is general enough to
include learning sparse parity functions. Consider an odd k, and let P = {i ∈ [k] : i is odd}. Then y
is given by ΠA(z) := _j∈A_ _[z][j][ for][ z][j][ = 2˜]ϕj −_ 1, i.e., the parity function on zj(j ∈ _A). Then known_

results for learning parity functions can be applied to prove our lower bounds.

**Lower Bound for Fixed Features.[Q]** We show that FΞ contains learning problems that consist of a
mixture of two distributions with weights po and 1 − _po respectively, where in the first distribution_
_DA[(1)][,][ ˜]x is given by the uniform distribution over_ _ϕ[˜] and the label y is given by the parity function on A._
On such DA[(1)][, Daniely & Malach (2020) shows that exponentially large models over fixed features is]
needed to get nontrivial loss. Intuitively, there are exponentially many labeling functions ΠA that are
uncorrelated (i.e., “orthogonal” to each other): E[ΠA1 ΠA2 ] = 0 for any A1 and A2. Note that the
best approximation of ΠA by a fixed set of features Ψi’s is its projection on the linear span of the
features. Then with polynomial-size models, there always exists some ΠA far from the linear span.

1Equivalently, the network uses ˜σ(z) = Eξσ(z + ξ), a Gaussian smoothed version of σ, and the smoothing
allows z slightly outside the activated region of σ to generate gradient for the learning. Empirically it is not
needed since typically sufficient data can activate the neurons. One potential reason is that the data have their
own noise to achieve a similar effect (a remote analog being noisy gradients can help the optimization). Further
analysis on such an effect is left for future work.


-----

_Remark. It is instructive to compare to network learning, which finds the effective weights_ _j∈A_ _[M][j]_

among the exponentially many candidates corresponding to different A’s. This can be done efficiently
by exploiting the data since the gradient is roughly proportional to E _yx_ = _j_ _A_ _[M][j][. The network][P]_
_{_ _}_ _∈_

then learns data-dependent features on which polynomial size linear models can achieve small loss.

[P]

**Lower Bound for Learning without Input Structure. Clearly, FΞ0 contains the distributions DA[(1)]**
described above. The lower bound then follows from classic SQ learning results (Blum et al., 1994).

_Remark. The SQ lower bound analysis does not apply to FΞ, because in FΞ the input distribution_
is related the labeling function. This allows networks to learn with polynomial time/sample. While
both the labeling function and the input distribution affect the learning, few existing studies explicitly
point out the importance of the input structure. We thus emphasize the input structure is crucial for
networks to learn effective features and achieve superior performance.

6 EXPERIMENTS

Our experiments mainly focus on feature learning and the effect of the input structure. We first
perform simulations on our learning problems to (1) verify our main theorems on the benefit of feature
learning and the effect of input structure; (2) verify our analysis of feature learning in networks. We
then check if our insights carry over to real data: (3) whether similar feature learning is presented in
real network/data; (4) whether damaging the input structure lowers the performance. The results are
consistent with our analysis and provide positive support for the theory. Below we present part of the
results and include the complete experimental details and results in Appendix E.

**Simulation: Verification of the Main Results. We**
generate data according to our problem setup, with
_d = 500, D = 100, k = 5, po = 1/2, a randomly_
sampled A, and labels given by the parity function.
We then train a two-layer network with m = 300
following our learning process, and for comparison,
we also use two fixed feature methods (the NTK and
random feature methods based on the same network).
Finally, we also use these three methods on the data
distribution with the input structure removed (i.e.,
_FΞ0 in Theorem 4)._

Figure 1 shows that the results are consistent with
our results. Network learning gets high test accuracy Figure 1: Test accuracy on simulated data
while the two fixed feature methods get significantly with or without input structure.
lower accuracy. Furthermore, when the input structure is removed, all three methods get test accuracy similar to random guessing.

**Simulation: Feature Learning in Networks. We compute the cosine similarities between the**
weights wi’s and visualize them by Multidimensional Scaling. (Recall that our analysis is on the
_directions of the weights without considering their scaling, and thus it is important to choose cosine_
similarity rather than say the typical Euclidean distance.) Figure 2 shows that the results are as
predicted by our analysis. After the first gradient step, some weights begin to cluster around the
ground-truth _j_ _A_ _[M][j][ (or][ −]_ [P]j _A_ _[M][j][ due to the][ a][i][ in the gradient update which can be positive]_

_∈_ _∈_

or negative). After the second step, the weights get improved and well-aligned with the ground-truth
(with cosine similarities > 0.99). Furthermore, if a classifier is trained on the features after the first

[P]

step, the test accuracy is about 52%; if the same is done after the second step, the test accuracy is
about 100%. This demonstrates while some effective features emerge in the first step, they need to be
improved in the second step to get accurate prediction.

**Real Data: Feature Learning in Networks. We perform experiments on MNIST (LeCun et al.,**
1998; Deng, 2012), CIFAR10 (Krizhevsky, 2012), and SVHN (Netzer et al., 2011). On MNIST,
we train a two-layer network with m = 50 on the subset with labels 0/1 and visualize the neurons’
weights as in the simulation. Figure 3 shows a similar feature learning phenomenon: effective features
emerge after a few steps and then get improved to form two clusters. Similar results are observed on
other datasets. These suggest the insights obtained in our analysis are also applicable to the real data.


-----

Figure 2: Visualization of the weights wi’s after initialization/one gradient step/two steps in network
learning on the synthetic data. The red star denotes the ground-truth _j_ _A_ _[M][j][; the orange star is]_

_∈_
_−_ [P]j∈A _[M][j][. The red/orange dots are the weights closest to the red/orange star, respectively.]_

[P]

Figure 3: Visualization of the neurons’ weights in a two-layer network trained on the subset of
MNIST data with label 0/1. The weights gradually form two clusters.

(a) (b) (c)

Figure 4: Test accuracy at different steps for an equal mixture of Gaussian inputs with data: (a)
MNIST, (b) CIFAR10, (c) SVHN.

**Real Data: The Effect of Input Structure. Since we cannot directly manipulate the input distribution**
of real data, we perform controlled experiments by injecting different inputs. For labeled dataset
_L and injected input U, we first train a teacher network fitting L, then use the teacher network to_
give labels on a mixture of inputs from L and U, and finally train a student network on this new
dataset M consisting of the mixed inputs and the teacher network’s labels. Checking the student’
performance on different parts of M and comparing to those by directly training the student on the
original data L can reveal the impact of changing the input structure. We use MNIST, CIFAR10, or
SVHN as L, and use Gaussian or images in Tiny ImageNet (Le & Yang, 2015) as U. The networks
for MNIST are two-layer with m = 9, and those for CIFAR10/SVHN are ResNet-18 convolutional
neural networks (He et al., 2016).

Figure 4 shows the results on an equal mixture of data and Gaussian. It presents the test accuracy
of the student on the original data part, the Gaussian part, and the whole mixture. For example, on
CIFAR10, the network learns well over the CIFAR10 part (with accuracy similar to directly training
on the original data) but learns slower with worse accuracy on the Gaussian part. Furthermore, the
accuracy on the whole mixture is lower than that of training on the original CIFAR10. This shows
that the input structure indeed has a significant impact on the learning. While MNIST+Gaussian
shows a less significant trend (possibly because the tasks are simpler), the other datasets show similar
significant trends as CIFAR10+Gaussian (the results using Tiny ImageNet are in the appendix).


-----

7 ETHICS STATEMENT

Our paper is mostly theoretical in nature and thus we foresee no immediate negative ethical impact.
We are of the opinion that our theoretical framework may lead to better understanding and inspire
development of improved network learning methods, which may have a positive impact in practice. In
addition to the theoretical machine learning community, we perceive that our conceptual message that
the input structure is crucial for the network learning’s performance can be beneficial to engineeringinclined machine learning researchers.

8 REPRODUCIBILITY STATEMENT

For theoretical results in the Section 4, a complete proof is provided in the Appendix B-D. The
theoretical results and complete proofs for a setting more general than that in the main text are
provided in the Appendix F. For experiments in the Section 6, complete details and experimental
results are provided in the Appendix Section E. The source code with explanations and comments is
provided in the supplementary material.

9 ACKNOWLEDGEMENT

The work is partially supported by Air Force Grant FA9550-18-1-0166, the National Science Foundation (NSF) Grants 2008559-IIS and CCF-2046710.

REFERENCES

Zeyuan Allen-Zhu and Yuanzhi Li. What can resnet learn efficiently, going beyond kernels? In
_Advances in Neural Information Processing Systems, 2019._

Zeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs deep
learning. arXiv preprint arXiv:2001.04413, 2020a.

Zeyuan Allen-Zhu and Yuanzhi Li. Feature purification: How adversarial training performs robust
deep learning. arXiv preprint arXiv:2005.10190, 2020b.

Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and
self-distillation in deep learning. arXiv preprint arXiv:2012.09816, 2020c.

Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via overparameterization. In International Conference on Machine Learning, 2019.

Yu Bai and Jason D Lee. Beyond linearization: On quadratic and higher-order approximation of wide
neural networks. In International Conference on Learning Representations, 2019.

Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning
practice and the classical bias–variance trade-off. Proceedings of the National Academy of Sciences,
116(32):15849–15854, 2019.

David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet allocation. the Journal of
_machine Learning research, 3:993–1022, 2003._

Avrim Blum, Merrick Furst, Jeffrey Jackson, Michael Kearns, Yishay Mansour, and Steven Rudich.
Weakly learning dnf and characterizing statistical query learning using fourier analysis. In Pro_ceedings of the twenty-sixth annual ACM symposium on Theory of computing, pp. 253–262,_
1994.

Yuan Cao, Zhiying Fang, Yue Wu, Ding-Xuan Zhou, and Quanquan Gu. Towards understanding the
spectral bias of deep learning, 2020.

Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In European Conference on Computer Vision, 2018.


-----

Minshuo Chen, Yu Bai, Jason D Lee, Tuo Zhao, Huan Wang, Caiming Xiong, and Richard Socher.
Towards understanding hierarchical learning: Benefits of neural representations. arXiv preprint
_arXiv:2006.13436, 2020a._

Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International Conference on Machine Learning,
2020b.

Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
In Advances in Neural Information Processing Systems, 2019.

Amit Daniely and Eran Malach. Learning parities with neural networks. Advances in Neural
_Information Processing Systems, 2020._

Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE Signal
_Processing Magazine, 29(6):141–142, 2012._

Ilias Diakonikolas, Surbhi Goel, Sushrut Karmalkar, Adam R Klivans, and Mahdi Soltanolkotabi.
Approximation schemes for relu regression. In Conference on Learning Theory, 2020.

Xialiang Dou and Tengyuan Liang. Training neural networks as learning data-adaptive kernels: Provable representation and approximation benefits. Journal of the American Statistical Association,
2020.

Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, 2019.

Cong Fang, Hanze Dong, and Tong Zhang. Over parameterized two-level neural networks can learn
near optimal feature representations, 2019.

Cong Fang, Hanze Dong, and Tong Zhang. Mathematical models of overparameterized neural
networks. Proceedings of the IEEE, 109(5):683–703, 2021.

Spencer Frei, Yuan Cao, and Quanquan Gu. Agnostic learning of a single neuron with gradient
descent. In Advances in Neural Information Processing Systems, 2020.

Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart. Disentangling feature and lazy
training in deep neural networks. Journal of Statistical Mechanics: Theory and Experiment, 2020
(11):113301, 2020.

Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural
networks outperform kernel methods? In Advances in Neural Information Processing Systems,
2020.

Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate
object detection and semantic segmentation. In Computer Vision and Pattern Recognition, 2014.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Computer Vision and Pattern Recognition, 2016.

Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Computer Vision and Pattern Recognition, 2020.

Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, 2018.

Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve
arbitrarily small test error with shallow relu networks. In International Conference on Learning
_Representations, 2019._

Longlong Jing and Yingli Tian. Self-supervised visual feature learning with deep neural networks: A
survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.

Pritish Kamath, Omar Montasser, and Nathan Srebro. Approximate is good enough: Probabilistic
variants of dimensional and margin complexity. In Conference on Learning Theory, 2020.


-----

Michael Kearns. Efficient noise-tolerant learning from statistical queries. Journal of the ACM, 1998.

Frederic Koehler and Andrej Risteski. The comparative power of relu networks and polynomial
kernels in the presence of sparse latent structure. In International Conference on Learning
_Representations, 2018._

Alex Krizhevsky. Learning multiple layers of features from tiny images. University of Toronto, 2012.

Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 2015.

Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha SohlDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. Advances in neural information processing systems, 2019.

Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems, 2018.

Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial large
learning rate in training neural networks. Advances in Neural Information Processing Systems,
2019.

Yuanzhi Li, Tengyu Ma, and Hongyang R Zhang. Learning over-parametrized two-layer neural
networks beyond ntk. In Conference on Learning Theory, 2020.

Eran Malach, Pritish Kamath, Emmanuel Abbe, and Nathan Srebro. Quantifying the benefit of using
differentiable learning over tangent kernels. arXiv preprint arXiv:2103.01210, 2021.

Christopher D Manning, Kevin Clark, John Hewitt, Urvashi Khandelwal, and Omer Levy. Emergent
linguistic structure in artificial neural networks trained by self-supervision. Proceedings of the
_National Academy of Sciences, 117(48):30046–30054, 2020._

Alexander G de G Matthews, Mark Rowland, Jiri Hron, Richard E Turner, and Zoubin Ghahramani.
Gaussian process behaviour in wide deep neural networks. In International Conference on Learning
_Representations, 2018._

Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. In International Conference on Learning
_Representations, 2020._

Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. 2011.

Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington,
and Jascha Sohl-Dickstein. Bayesian convolutional neural networks with many channels are
gaussian processes. In International Conference on Learning Representations, 2019.

B. Olshausen and D. Field. Sparse coding with an overcomplete basis set: A strategy employed by
v1? Vision Research, 37:3311–3325, 1997.

Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in
_Neural Information Processing Systems, 2008._

Maria Refinetti, Sebastian Goldt, Florent Krzakala, and Lenka Zdeborová. Classifying highdimensional gaussian mixtures: Where kernel methods fail and neural networks succeed, 2021.

William E Vinje and Jack L Gallant. Sparse coding and decorrelation in primary visual cortex during
natural vision. Science, 287(5456):1273–1276, 2000.

Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Itay Golan,
Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In
_Conference on Learning Theory, 2020._


-----

Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019.

Gilad Yehudai and Shamir Ohad. Learning a single neuron with gradient methods. In Conference on
_Learning Theory, 2020._

Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding
neural networks. Advances in Neural Information Processing Systems, 2019.

Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
_European Conference on Computer Vision, 2014._

Chiyuan Zhang, Samy Bengio, and Yoram Singer. Are all layers created equal? arXiv preprint
_arXiv:1902.01996, 2019._

Mo Zhou, Rong Ge, and Chi Jin. A local convergence theory for mildly over-parameterized two-layer
neural network. In Conference on Learning Theory, 2021.

Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes overparameterized deep ReLU networks. Machine Learning, 109(3):467–492, 2020. ISSN 1573-0565.


-----

# Appendix

Section A presents more technical discussion on related work. Section B-D provides the complete
proofs for our results in the main text. Section E provides the complete details and experimental
results for our experiments.

Finally, Section F provides the theoretical results and complete proofs for a setting more general than
that in the main text, allowing incoherent dictionaries, unbalanced classes, and Gaussian noise in the
data.

CONTENTS

**1** **Introduction** **1**

**2** **Related Work** **2**

**3** **Problem Setup** **3**

3.1 Neural Network Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4

**4** **Main Results** **5**

**5** **Proof Sketches** **6**

5.1 Provable Guarantees of Neural Networks . . . . . . . . . . . . . . . . . . . . . . . 6

5.2 Lower Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7

**6** **Experiments** **8**

**7** **Ethics Statement** **10**

**8** **Reproducibility Statement** **10**

**9** **Acknowledgement** **10**

**A More Technical Discussion on Related Work** **16**

**B** **Complete Proofs for Provable Guarantees of Neural Networks** **19**

B.1 Existence of A Good Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19

B.2 Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

B.3 Some Auxiliary Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

B.4 Feature Emergence: First Gradient Step . . . . . . . . . . . . . . . . . . . . . . . 23

B.5 Feature Improvement: Second Gradient Step . . . . . . . . . . . . . . . . . . . . . 27

B.6 Classifier Learning Stage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

B.7 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

**C Lower Bound for Linear Models on Fixed Feature Mappings** **38**

**D Lower Bound for Learning without Input Structure** **39**


-----

**Complete Experimental Results** **40**

E.1 Simulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

E.1.1 Parity Labeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

E.1.2 Interval Labeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

E.2 More Simulation Result in Various Settings . . . . . . . . . . . . . . . . . . . . . 43

E.2.1 Varying Input Data Dimension . . . . . . . . . . . . . . . . . . . . . . . . 43

E.2.2 Varying Class Imbalance Ratio . . . . . . . . . . . . . . . . . . . . . . . . 44

E.2.3 Varying Sample Size . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45

E.3 Experiments on More Data Generation Models . . . . . . . . . . . . . . . . . . . 46

E.3.1 Hidden Representation Labeling . . . . . . . . . . . . . . . . . . . . . . . 46

E.3.2 Two-layer Networks on Mixture of Gaussians . . . . . . . . . . . . . . . . 47

E.4 Real Data: Feature Learning in Networks . . . . . . . . . . . . . . . . . . . . . . 47

E.4.1 CNNs on Binary Cifar10: Feature Learning in Networks . . . . . . . . . . 49

E.5 Real Data: The Effect of Input Structure . . . . . . . . . . . . . . . . . . . . . . . 52

E.5.1 Experimental Methodology . . . . . . . . . . . . . . . . . . . . . . . . . 52

E.5.2 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53

E.5.3 Larger Network on MNIST for Checking The Effect of Input Structure . . 55

E.5.4 Empirical Verification of Our Method . . . . . . . . . . . . . . . . . . . . 55

**Provable Guarantees for Neural Networks in A More General Setting** **57**

F.1 Problem Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57

F.1.1 Neural Network Learning . . . . . . . . . . . . . . . . . . . . . . . . . . 57

F.2 Main Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58

F.3 Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58

F.4 Existence of A Good Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59

F.5 Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60

F.6 Some Auxiliary Lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61

F.7 Feature Emergence: First Gradient Step . . . . . . . . . . . . . . . . . . . . . . . 62

F.8 Feature Improvement: Second Gradient Step . . . . . . . . . . . . . . . . . . . . . 68

F.9 Classifier Learning Stage and Main Theorem . . . . . . . . . . . . . . . . . . . . 81


-----

A MORE TECHNICAL DISCUSSION ON RELATED WORK

**Advantage of Neural Networks over Linear Models on Fixed Features. A recent line of work has**
turned to show learning settings where network learning provably has advantage over linear models
on fixed features; see the nice summary in Malach et al. (2021). Here we highlight the results and
focuses of the existing related studies and discuss the differences from ours.

Yehudai & Shamir (2019) shows that the random feature method fails to learn even a single ReLU
neuron on Gaussian inputs unless its size is exponentially large in dimension. This points out the
limitation of the random feature method (belonging to the fixed feature approach) but does not
consider feature learning in networks.

Some studies show that a single ReLU neuron can be learnt by gradient descent (Yehudai & Ohad,
2020; Diakonikolas et al., 2020; Frei et al., 2020). The analysis typically involves feature learning.
However, their focus is different: they do not show the advantage over fixed feature methods and do
not consider the effect of the input structures.

Zhou et al. (2021) shows that in a special teacher-student setting, the student network will do exact
local convergence in a surprising way that all student neurons will converge to one of the teacher
neurons. The work does not consider the effect of the input structure nor the advantage over fixed
features.

Dou & Liang (2020) explains the advantage of network learning by constructing adaptive Reproducing
Kernel Hilbert Space (RKHS) indexed by the training process of the neural network, and shows that
adaptive RKHS benefits from a smaller function space containing the residue comparing to RKHS.
The work shows the statistical advantage of networks over data-independent kernels, but does not
consider the optimization for learning the network.

Ghorbani et al. (2020) considers data generated from a hidden vector with two subsets of variables,
each uniformly distributed in a high-dimensional sphere (with a different radius), while the label is
determined by only the first subset of variables. It shows the existence of good neural networks that
can overcome the curse of dimensionality by representing the best low-dimensional hidden structure.
However, it studies the approximation power of neural networks rather than the learning, i.e., it does
not show how to learn the good network.

Fang et al. (2019) argues that in the infinite width limit, a two-layer neural network will learn a nearly
optimal feature representation in the distribution sense, thanks to the convexity of the limit problem.
It is unclear how this result helps to understand the feature learning procedure for practical networks,
which is usually a non-convex process.

Chen et al. (2020a) considers a fixed, randomly initialized neural network as a representation function
fed into another trainable network which is the quadratic Taylor model of a wide two-layer network.
It shows that learning over the random representation can achieve improved sample complexities
compared to learning over the raw data. However, the representation considered is not learned, which
is different from our focus on feature learning.

Allen-Zhu & Li (2020a) considers Gaussian inputs with labels given by a multiple-layer network with
quadratic activations and skip connections (with the assumption of information gap on the weights),
and studies training a deep network with quadratic activation. It shows that the trained network can
learn proper representations and obtain small errors while no polynomial fixed feature methods can.
On the other hand, it does not focus on the influence of input structure on feature learning: note that
its input distribution contains no information about the “ground-truth” features in the target network.
It also points out that the learned features get improved during training: higher-level layers will help
lower-level layers to improve by backpropagating correction signals. Our analysis also shows feature
improvement which however is by signals from the input distribution.

Allen-Zhu & Li (2019) considers PAC learning with labels given by a depth-2 ResNet, and studies
training an overparameterized depth-2 ResNet (using uniform inputs over Boolean hypercube as an
example). It shows the trained network can obtain small errors while no polynomial kernel methods
can obtain as good errors. Similar to Allen-Zhu & Li (2020a), it does not focus on the influence of
input structure on feature learning or the advantage of networks.


-----

Allen-Zhu & Li (2020c) studies how ensemble of deep learning models can improve test accuracy
and how the ensemble can be distilled into a single model. It develops a theory which assumes the
data has multi-view structure and shows that the ensemble of independently trained networks can
provably improve test accuracy and the ensemble can also be provably distilled into a single model.
The analysis also relies on showing that the data structure can help the ensemble and the distillation.
On the other hand, their focus is on ensembles and is quite different from ours: the analysis is on
showing the multi-view input structure allows the ensembles of networks to improve over single
ones and ensembles of fixed feature mappings do not have improvement. While our focus is on
supervisedly learning one single network that outperforms the fixed feature approaches.

Daniely & Malach (2020) considers the task of learning sparse parities with two-layer networks, and
the analysis suggests that the ability to learn the label-correlated features also seems to be critical
towards the success of neural networks, although the authors did not explore much in this direction.
Malach et al. (2021) also considers similar learning problems but with specifically designed models
for the problems. The learning problems considered in Daniely & Malach (2020); Malach et al. (2021)
have input distributions that leak information about the target labeling function, which is similar
to our setting, and their analysis also shows that the first gradient descent can learn a set of good
features and later steps can learn an accurate classifier on top. Our work is inspired by their studies,
while there are some important differences. First, their focuses are different from ours. Daniely
& Malach (2020) focuses on showing neural networks can learn targets (i.e., k-parity functions)
that are inherently non-linear. Our analysis generalizes to more general distributions, including
practically motivated ones. Malach et al. (2021) focuses on strong separations between learning with
gradient descent on differentiable models (including typical neural networks) and learning using the
corresponding tangent kernels. The analysis is on specific differentiable models, while our work is
on two-layer neural networks similar to practical ones. Second, our analysis relies on the feature
improvement in the second gradient step. This is not an artifact of the analysis but comes from our
problem setup. While in Daniely & Malach (2020) the data distribution allows some neurons to be
sufficiently good after the first gradient step and needs no feature improvement, our setup is more
general where the data distribution may not have a similar strong benign effect and thus needs feature
improvement in the second gradient step.

Most related to our work is Daniely & Malach (2020). Therefore, we provide a detailed discussion to
highlight the connections and differences.

1. Our problem setting is more general than that in Daniely & Malach (2020). To see this,
let our dictionary be the identity matrix, the set P to be the odd numbers (i.e., the labeling
function is a sparse parity). Furthermore, let the distribution of the hidden representation be
an equal mixture of the following two:

(a) 1: Uniform distribution over the hypercube.
_D_

(b) D2: Irrelevant patterns _ϕ[˜]j(j ̸∈_ _A) have appearance probability p0 = 1/2. And the_
distribution of relevant patterns _ϕ[˜]j(j_ _A) is: all 0’s with probability 1/2, and all 1’s_
_∈_
with probability 1/2.

Then our problem setting reduces to their setting (up to scaling/translation of _ϕ[˜]j’s). On the_
other hand, in general our setting allows for more choices for the labeling, the dictionary,
and the distributions over _ϕ[˜]._


2. Upper bound: Because of the more general setting, our upper bound proof requires technical
_novelty. Recall that in their work, the input distribution is essentially a mixture of D1 and_
_D2 above. In D2, the relevant patterns_ _ϕ[˜]j(j ∈_ _A) have the specific structure of all 0’s or_
all 1’s with probability 1/2. This allows to show that neurons with weight w satisfying

_j_ _A_ _[w][j][ = 0][ will have good gradients: small components from irrelevant patterns (their]_
_∈_
Lemma 7) and large components from relevant patterns (their Lemma 8). However, in

P

our setting, the relevant patterns do not have this specific structure, and thus their proof
technique is not applicable (or can be applied only when we have an exponentially large
number of hidden neurons so that some hit the good positions at random initialization). What
we showed is that the gradient has some correlation with the good feature direction. So after
the first gradient step, the neuron weights are not good yet but are in a better position for
further improvement (in particular, their setting corresponds to p0 = 1/2 which means large
noise in the weights after the first step; see discussion after our Lemma 6 in Section 5). Then


-----

the latter gradient steps are able to improve the weights to better “signal-to-noise-ratio”. In
summary, our proof does not rely on their specific input structure or an exponentially large
number of hidden neurons for hitting some good positions. The key is that the good feature
will emerge with the help of the input structure, and once in a better position, the neurons’
weights can be improved to the desired quality.

3. Lower bound: On the other hand, our lower bound is proved by a reduction to the lower
bound results in Daniely & Malach (2020). They have shown that D1 above can lead to large
errors for fixed feature models of polynomial size. Our proof is essentially constructing
a mixture of D1 and D2 with mixture weights p0 and (1 − _p0), and applying their lower_
bound for 1. See our proof in Appendix C.
_D_

4. Conceptually, our work belongs to the same line of research as Daniely & Malach (2020), to
analyze how feature learning leads to the superior performance of networks. While their
analysis also relies on feature learning from good gradients induced by input structure,
their focus is more on separating network learning and fixed feature models and has not
explicitly explored the impact of input structures (while we agree that such an explicit study
will not be difficult in their setting). More importantly, their input distribution is specific
and atypical in practice, which allows a specific type of feature learning (as explained in
the above discussion on upper bounds). Our work thus considers a more general setting
that is motivated by practical problems. Our results then bring theoretical insights closer
for explaining the feature learning in practice and provide some positive evidence for the
importance of analysis under proper models of the input distributions.

**Sparse Coding and Subspace Data Models. To analyze neural networks’ performance, various**
data models have been considered. A practical way to model the underlying structure of data is by
assuming that a set of hidden variables exists and the input data is a high dimensional projection of
the hidden vector (possibly with noise). Along this line, the classic sparse coding model has been
used in existing works for analyzing networks. Koehler & Risteski (2018) considers such a data
distribution where the label is given by a linear function on the hidden sparse vector, but studies the
approximation power of networks and classic polynomial methods rather than the learning. Allen-Zhu
& Li (2020b) considers similar data distributions, but studies the performance of networks under
adversarial perturbations. Another type of related data models assumes that the label is determined
by a subset of hidden variables. Ghorbani et al. (2020) considers a hidden vector with two subsets of
variables, each uniformly distributed in a high-dimensional sphere (with a different radius), while
the label is determined by only the first subset of variables. However, Ghorbani et al. (2020) studies
the approximation power of neural networks rather than the learning. Compared to these studies,
our work assumes the input is given by a dictionary multiplied with a hidden vector (not necessarily
sparse) while the label is determined by a subset of the hidden vector, as motivated by pattern
recognition applications in practice. Furthermore, we focus on the learning ability of networks instead
of approximation.


-----

B COMPLETE PROOFS FOR PROVABLE GUARANTEES OF NEURAL NETWORKS

We first make a few remarks about the proof.

_Remark. The analysis can be carried out for more gradient steps following similar intuition, while we_
analyze two steps for simplicity.

_Remark. Readers may notice that the network can be overparameterized. With sufficient overpa-_
rameterization and proper initialization and step sizes, network learning becomes approximately
NTK. However, here our learning scheme allows going beyond this kernel regime: we use aggressive
gradient updates λ[(]w[t][)] [= 1][/][(2][η][(][t][)][)][ in the first two steps, completely forgetting the old weights to]
learn effective features. Using proper initialization and aggressive updates early to escape the kernel
regime has been studied in existing work (e.g., Woodworth et al. (2020); Li et al. (2019)). Our result
thus adds another concrete example.

**Notations.** For a vector v and an index set I, let vI denote the vector containing the entries of v
indexed by I, and v _I denote the vector containing the entries of v with indices outside I._
_−_

By initialization, wi[(0)] for i [m] are i.i.d. copies of the same random variable w[(0)] (0, σw[2] _[I][d][×][d][)][;]_
_∈_ _∼N_
similar for a[(0)] and b[(0)]. Let qℓ := ⟨w[(0)], Mℓ⟩, then ⟨w[(0)], x⟩ = ⟨ϕ, q⟩. Similarly, define qi,ℓ[(][t][)] [:=]

_⟨wi[(][t][)][, M][ℓ][⟩][. Let][ σ]ϕ[2]_ [:=][ p][o][(1][ −] _[p][o][)][/]σ[˜][2]_ denote the variance of ϕℓ for ℓ _̸∈_ _A._

We also define the following sets to denote typical initialization. For a fixed δ ∈ (0, 1), define


_w[(][D][ −]_ _[k][)]_
_w_ R[d] : qℓ = _w, Mℓ_ _,_ _[σ][2]_
_∈_ _⟨_ _⟩_ 2 _≤_

max _qℓ_ _σw_
_ℓ_ _|_ _| ≤_


_w[(][D][ −]_ _[k][)]_
_qℓ[2]_ _[≤]_ [3][σ][2]

2

_ℓX̸∈A_

2 log(Dm/δ) _,_

)


_w(δ) :=_
_G_


(5)


_Ga(δ) := {a ∈_ R : |a| ≤ _σa_

_Gb(δ) := {b ∈_ R : |b| ≤ _σb_

B.1 EXISTENCE OF A GOOD NETWORK


2 log(m/δ)}. (6)

2 log(m/δ)}. (7)


we first show that there exists a network that can fit the data distribution.

**Lemma 7. For some s, a, b ∈** R with a, b ≥ 0, define a function δs,a,b : R → R as

_δs,a,b(z) = aσr(z_ _s + b)_ 2aσr(z _s) + aσr(z_ _s_ _b)._ (8)
_−_ _−_ _−_ _−_ _−_

_where σr(z) = max_ _z, 0_ _is the ReLU activation function. Then_
_{_ _}_


0 _when z ≤_ _s −_ _b,_
_a(z −_ _s) + ab_ _when s −_ _b ≤_ _z ≤_ _s,_
_−a(z −_ _s) + ab_ _when s ≤_ _z ≤_ _s + b,_
0 _when s + b ≤_ _z._


(9)


_δs,a,b(z) =_


_That is, δs,a,b(z) linearly interpolates between (s_ _b, 0), (s, ab), (s + b, 0) when z_ [s _b, s + b],_
_−_ _∈_ _−_
_and is 0 elsewhere._

_Proof of Lemma 7. This can be simply verified for the four cases of the value of z._

**Lemma 8 (Restatement of Lemma 5). For any** Ξ, there exists a network g[∗](x) =
_n_ _D ∈F_
_i=1_ _[a]i[∗][σ][(][⟨][w]i[∗][, x][⟩]_ [+][ b]i[∗][)][ with][ y][ =][ g][∗][(][x][)][ for any][ (][x, y][)][ ∼D][. Furthermore, the number of neurons]
_nP = 3(k + 1), |a[∗]i_ _[| ≤]_ [32][k,][ 1][/][(32][k][)][ ≤|][b]i[∗][| ≤] [1][/][2][,][ w]i[∗] [= ˜]σ _j∈A_ _[M][j][/][(4][k][)][, and][ |⟨][w]i[∗][, x][⟩]_ [+][ b]i[∗][| ≤] [1]

_for any i ∈_ [n] and (x, y) ∼D.

[P]


-----

_Proof of Lemma 5. Let w = ˜σ_ _j∈A_ _[M][j][ and let][ µ][ =][ P]j∈A_ [E][[˜]ϕj]. We have

_w, x_ = ˜σ _Mj, Mϕ_ = ˜σ _ϕj =_ _ϕ˜j_ _µ._ (10)
_⟨_ _⟩_ [P] _jX∈A⟨_ _⟩_ _jX∈A_ _jX∈A_ _−_


Then by Lemma 7,

_g1[∗][(][x][) :=]_ _δp_ _µ,2,1/2(_ _w, x_ )

_−_ _⟨_ _⟩_ _−_
_pX∈P_


_δp−µ,2,1/2(⟨w, x⟩)_ (11)
_p̸∈P,X0≤p≤k_


_δp,2,1/2(_ _w, x_ + µ) (12)
_⟨_ _⟩_
_p̸∈P,X0≤p≤k_


_δp,2,1/2(_ _w, x_ + µ)
_⟨_ _⟩_ _−_
_pX∈P_


= _δp,2,1/2_ _ϕ˜j_ _δp,2,1/2_ _ϕ˜j_ (13)

  _−_  

_pX∈P_ _j∈A_ _p̸∈P,X0≤p≤k_ _j∈A_

= y [X]  [X]  (14)

for any (x, y) ∼D. Similarly,


_g2[∗][(][x][) :=]_ _δp_ _µ+1/4,4,1/2(_ _w, x_ )

_−_ _⟨_ _⟩_ _−_
_pX∈P_


_δp−µ+1/4,4,1/2(⟨w, x⟩)_ (15)
_p̸∈P,X0≤p≤k_


_δp+1/4,4,1/2(_ _w, x_ + µ) (16)
_⟨_ _⟩_
_p̸∈P,X0≤p≤k_


_δp+1/4,4,1/2(_ _w, x_ + µ)
_⟨_ _⟩_ _−_
_pX∈P_


= _δp+1/4,4,1/2_ _ϕ˜j_ _δp+1/4,4,1/2_ _ϕ˜j_ (17)

  _−_  

_pX∈P_ _j∈A_ _p̸∈P,X0≤p≤k_ _j∈A_

= y [X]  [X]  (18)

for any (x, y) ∼D. Note that the bias terms in g1[∗] [and][ g]2[∗] [have distance at least][ 1][/][4][, then at least]
one of them satisfies that all its bias terms have absolute value ≥ 1/8. Pick that one and denote it as
_g(x) =_ _i=1_ _[a][i][σ][r][(][⟨][w][i][, x][⟩]_ [+][ b][i][)][. By the positive homogeneity of][ σ][r][, we have]

_n_

[P][n] _g(x) =_ 4kaiσr( _wi, x_ _/(4k) + bi/(4k))._ (19)

_⟨_ _⟩_
_i=1_

X


Since for any (x, y), _wi, x_ _/(4k) + bi/(4k)_ 1, then
_∼D_ _|⟨_ _⟩_ _| ≤_


4kaiσ( _wi, x_ _/(4k) + bi/(4k))_ (20)
_⟨_ _⟩_
_i=1_

X


_g(x) =_


where σ is the truncated ReLU. Now we can set a[∗]i [= 4][ka][i][, w]i[∗] [=][ w][i][/][(4][k][)][, b]i[∗] [=][ b][i][/][(4][k][)][, to get our]
final g[∗].

B.2 INITIALIZATION

We first show that with high probability, the initial weights are in typical positions.

**Lemma 9. For any δ ∈** (0, 1), with probability at least 1 − _δ −_ 2 exp (−Θ(D − _k)) over w[(0)],_


_σw[2]_ [(][D][ −] _[k][)][/][2][ ≤]_


_qℓ[2]_ _[≤]_ [3][σ]w[2] [(][D][ −] _[k][)][/][2][,]_
_ℓX̸∈A_


max _qℓ_ _σw_
_ℓ_ _|_ _| ≤_

_With probability at least 1 −_ _δ over b[(0)],_

_b[(0)]_ _σb_
_|_ _| ≤_

_With probability at least 1 −_ _δ over a[(0)],_

_a[(0)]_ _σa_
_|_ _| ≤_


2 log(D/δ).

p

2 log(1/δ).

2 log(1/δ).


-----

_Proof of Lemma 9. From q ∼N_ (0, σw[2] _[I][d][×][d][)][, we have:]_



-  With probability 1 _δ/2, maxℓ_ _qℓ_
_≥_ _−_ _|_ _| ≤_


2σw[2] [log][ D]δ [, and]


_S_ _σw[2]_

-  For any subset S [D], with probability 1 2 exp ( Θ( _S_ )), _qS_ 2 _|_ _|2_
_⊆_ _≥_ _−_ _−_ _|_ _|_ _∥_ _∥[2]_ _[∈]_


Similar for b[(0)] and a[(0)]. The lemma then follows.

**Lemma 10. We have:**


_w_
_,_ [3][|][S]2[|][σ][2]



-  With probability 1 _δ_ 2m exp( Θ(D _k)) over wi[(0)]’s, for all i_ [2m], wi[(0)] _w(δ)._
_≥_ _−_ _−_ _−_ _−_ _∈_ _∈G_

-  With probability ≥ 1 − _δ over b[(0)]i_ _[’s, for all][ i][ ∈]_ [[2][m][]][,][ b]i[(0)] _∈Gb(δ)._

-  With probability ≥ 1 − _δ over a[(0)]i_ _[’s, for all][ i][ ∈]_ [[2][m][]][,][ a]i[(0)] _∈Ga(δ)._

_Proof of Lemma 10. This follows from Lemma 9 by union bound._

The following lemma about the typical wi[(0)]’s will be useful for later analysis.

**Lemma 11. Fix δ** (0, 1). For any wi[(0)] _w(δ), we have_
_∈_ _∈G_

Pr _ϕℓqi,ℓ[(0)]_ _[≥]_ [Θ] (D _k)σϕ[2][σ]w[2]_ = Θ(1) (21)
_ϕ_  _−_  _−_ _[O][(log][3][/][2][(][Dm/δ][))]_

_ℓ_ _A_ (D _k)σϕ[2]σ[˜][2][ .]_
_̸∈_ q  _−_

[X]  q

_Consequently, when po = Ω(k[2]/D) and k = Ω(log[2](Dm/δ)),_


_ϕℓqi,ℓ[(0)]_ _[≥]_ [Θ (][σ][w][)] = Θ(1) (22)

  _−_ _[O]k[1][(1)][/][4][ .]_

_ℓ̸∈A_

[X] 


Pr


_Proof of Lemma 11. Note that for ℓ_ _̸∈_ _A, E[ϕℓ] = 0, E[ϕ[2]ℓ_ [] =][ σ]ϕ[2][, and][ E][[][|][ϕ][ℓ][|][3][] = Θ(][σ]ϕ[2][/]σ[˜]). Then
the statement follows from Berry-Esseen Theorem.

B.3 SOME AUXILIARY LEMMAS

The expression of the gradients will be used frequently.
**Lemma 12.**

_∂_

_L_ (g; σξ) = _aiE(x,y)_ _yI[yg(x; ξ)_ 1]Eξi I[ _wi, x_ + bi + ξi (0, 1)]x _,_ (23)
_∂wi_ _D_ _−_ _∼D {_ _≤_ _⟨_ _⟩_ _∈_ _}_

_∂_

_L_ (g; σξ) = _aiE(x,y)_ _yI[yg(x; ξ)_ 1]Eξi I[ _wi, x_ + bi (0, 1)] _,_ (24)
_∂bi_ _D_ _−_ _∼D {_ _≤_ _⟨_ _⟩_ _∈_ _}_

_∂_

_L_ (g; σξ) = E(x,y) _yI[yg(x; ξ)_ 1]Eξi _σ(_ _wi, x_ + bi + ξi) _._ (25)
_∂ai_ _D_ _−_ _∼D {_ _≤_ _⟨_ _⟩_ _}_

_Proof of Lemma 12. It follows from straightforward calculation._

We now show that a small subset of the entries in ϕ, q does not affect the probability distribution of
_⟨ϕ, q⟩_ much.
**Lemma 13. Suppose ν ∼N** (0, σ[2]). For any B ⊇ _A and any b:_
_ϕ−[Pr]B_ _,ν_ _ϕ−PrB_ _,ν_ (26)

_[{⟨][ϕ, q][⟩]_ [+][ ν][ ≥] _[b][} −]_ _[{⟨][ϕ][−][B][, q][−][B][⟩]_ [+][ ν][ ≥] _[b][}]_

_ϕB, qB_ _σ[3]_ + σϕ[2][∥][q][−][B][∥]3[3][/]σ[˜]

_O_ _|⟨_ _⟩|_ _._ (27)
_≤_ (σϕ[2][∥][q][−][B][∥]2[2] [+][ σ][2][)][1][/][2][ +] (σ[2] + σϕ[2][∥][q][−][B][∥]2[2][)][3][/][2] !


-----

_Similarly,_
_ϕ[Pr]−B_ _ϕ−B_ (28)

_[{⟨][ϕ, q][⟩≥]_ _[b][} −]_ [Pr] _[{⟨][ϕ][−][B][, q][−][B][⟩≥]_ _[b][}]_

_O_ _|⟨ϕB, qB⟩|_ + _∥q−B∥3[3]_ _._ (29)
_≤_  _σϕ∥q−B∥2_ _σσ˜_ _ϕ∥q−B∥2[3]_ 

_Proof of Lemma 13. Note that for ℓ_ _̸∈_ _A, E[ϕℓ] = 0, E[ϕ[2]ℓ_ [] =][ σ]ϕ[2][, and][ E][[][|][ϕ][ℓ][|][3][] = Θ(][σ]ϕ[2][/]σ[˜]). Let
_t =_ _ϕB, qB_ . Then by the Berry-Esseen Theorem,
_|⟨_ _⟩|_
_ϕ[Pr]−B_ _ϕ−B_ (30)

_[{⟨][ϕ, q][⟩]_ [+][ ν][ ≥] _[b][} −]_ [Pr] _[{⟨][ϕ][−][B][, q][−][B][⟩]_ [+][ ν][ ≥] _[b][}]_

Pr (31)
_≤_ _ϕ−B_

_[{⟨][ϕ][−][B][, q][−][B][⟩]_ [+][ ν][ ∈] [[][−][t][ +][ b, t][ +][ b][]][}]

2t _O(σ[3]_ + σϕ[2][∥][q][−][B][∥]3[3][/]σ[˜])

(32)

_≤_ (σϕ[2][∥][q][−][B][∥]2[2] [+][ σ][2][)][1][/][2][ +] (σ[2] + σϕ[2][∥][q][−][B][∥]2[2][)][3][/][2][ .]

The second statement follows from a similar argument.

We also have the following auxiliary lemma for later calculations.
**Lemma 14.**


EϕA {y} = 0, (33)
EϕA {|y|} = 1, (34)

Eϕj {|ϕj|} = 2σϕ[2]σ,[˜] _for j ̸∈_ _A,_ (35)

EϕA _yϕj_ = _[γ]_ (36)
_{_ _}_ _σ ˜_ _[,]_

EϕA _yϕj_ (37)
_{|_ _|} ≤_ _σ ˜[1]_ _[,][ for all][ j][ ∈]_ [[][D][]][.]


_Proof of Lemma 14._


EϕA {y} =


= _v∈{±X1}_ EϕA {y|y = v} Pr[y = v] (38)

= [1] EϕA _y_ _y = v_ (39)

2 _v∈{±X1}_ _{_ _|_ _}_

= 0. (40)

= _v∈{±X1}_ EϕA {|y| |y = v} Pr[y = v] (41)

= [1] EϕA _y_ _y = v_ (42)

2 _v∈{±X1}_ _{|_ _| |_ _}_

= 1. (43)


EϕA {|y|} =


Eϕj _ϕj_ =
_{|_ _|}_ _[| −]_ _[p][o][|][(1][ −]_ _[p][o]σ˜[) +][ |][1][ −]_ _[p][o][|][p][o]_

_ϕ˜j_ E[ϕ[˜]j]

EϕA _yϕj_ = EϕA _y_ _−_
_{_ _}_ ( _σ˜_ )


= 2σϕ[2]σ.[˜] (44)

(45)


= [1] _yϕ[˜]j_ _yE[ϕ[˜]j]_ (46)

_σ˜_ [E][ϕ][A] _−_
n o

= _[γ]_ (47)

_σ ˜_ _[.]_

EϕA {|yϕj|} = EϕA {|ϕj|} (48)

(49)

_≤_ _σ [1]˜_ _[.]_


-----

B.4 FEATURE EMERGENCE: FIRST GRADIENT STEP

We will show that w.h.p. over the initialization, after the first gradient step, there are neurons that
represent good features.

We begin with analyzing the gradients.

**Lemma 15 (Full version of Lemma 6). Fix δ** (0, 1) and suppose wi[(0)] _w(δ), b[(0)]i_ _b(δ) for_
_∈_ _∈G_ _∈G_
_all i ∈_ [2m]. Let

_ϵe :=_ _[k][ log][1][/][2][(][Dm/δ][) + log][3][/][2][(][Dm/δ][)]_ _._

_σϕ[2]σ[˜][2](D −_ _k)_

q

_If po = Ω(k[2]/D), k = Ω(log[2](Dm/δ)), and σξ[(1)]_ _< 1/k, then_


_∂_

_∂wi_ _LD(g[(0)]; σξ[(1)][) =][ −][a]i[(0)]_


_MjTj_ (50)
_j=1_

X


_where Tj satisfies:_

-  if j ∈ _A, then |Tj −_ _βγ/σ˜| ≤_ _O(ϵe/σ˜), where β ∈_ [Ω(1), 1] and depends only on wi[(0)], b[(0)]i _[;]_

-  if j ̸∈ _A, then |Tj| ≤_ _O(σϕ[2][ϵ][e]σ[˜])._

_Proof of Lemma 15. Consider one neuron index i and omit the subscript i in the parameters. Since_
the unbiased initialization leads to g[(0)](x; ξ[(1)]) = 0, we have

_∂_

_ξ_ [)] (51)
_∂w_ _[L][D][(][g][(0)][;][ σ][(1)]_

= _a[(0)]E(x,y)_ _yI[yg[(0)](x; ξ[(1)])_ 1]Eξ(1)I[ _w[(0)], x_ + b[(0)] + ξ[(1)] (0, 1)]x (52)
_−_ _∼D_ _≤_ _⟨_ _⟩_ _∈_
n o

= _a[(0)]E(x,y)_ _,ξ(1)_ _yI[_ _w[(0)], x_ + b[(0)] + ξ[(1)] (0, 1)]x (53)
_−_ _∼D_ _⟨_ _⟩_ _∈_

_D_ n o
= _a[(0)]_ _Mj E(x,y)_ _,ξ(1)_ _yI[_ _w[(0)], x_ + b[(0)] + ξ[(1)] (0, 1)]ϕj _._ (54)
_−_ _∼D_ _⟨_ _⟩_ _∈_

_j=1_

X n :=Tj o

First, consider j ∈ _A._ | {z }

_Tj = E(x,y)_ _,ξ(1)_ _yI[_ _w[(0)], x_ + b[(0)] + ξ[(1)] (0, 1)]ϕj (55)
_∼D_ _⟨_ _⟩_ _∈_
n o

= EϕA,ξ(1) _yϕj Prϕ−A_ _⟨ϕ, q⟩_ + b[(0)] + ξ[(1)] _∈_ (0, 1) _._ (56)
 h i[]

Let

_Ia := Pr_ _ϕ, q_ + b[(0)] + ξ[(1)] (0, 1) _,_ (57)
_ϕ−A_ _⟨_ _⟩_ _∈_

h i

_Ia[′]_ [:= Pr] _ϕ_ _A, q_ _A_ + b[(0)] + ξ[(1)] (0, 1) _._ (58)
_ϕ−A_ _⟨_ _−_ _−_ _⟩_ _∈_

h i

We have

_|Eξ(1)(Ia −_ _Ia[′]_ [)][|] (59)

_≤_ Eξ(1) _ϕ[Pr]−A_ _⟨ϕ, q⟩_ + b[(0)] + ξ[(1)] _≥_ 0 _−_ _ϕPr−A_ _⟨ϕ−A, q−A⟩_ + b[(0)] + ξ[(1)] _≥_ 0 (60)

h i h i

+ _ϕ−APr,ξ[(1)]_ _⟨ϕ, q⟩_ + b[(0)] + ξ[(1)] _≥_ 1 + _ϕ−APr,ξ[(1)]_ _⟨ϕ−A, q−A⟩_ + b[(0)] + ξ[(1)] _≥_ 1 _._ (61)

h i h i


-----

Then by Lemma 13,

_ϕ[Pr]−A_ _⟨ϕ, q⟩_ + b[(0)] + ξ[(1)] _≥_ 0 _−_ _ϕPr−A_ _⟨ϕ−A, q−A⟩_ + b[(0)] + ξ[(1)] _≥_ 0 [=][ O][(][ϵ][e][)][.] (62)

h i h i

Note that _ℓ_ _A_ [Var][(][ϕ][ℓ][q][ℓ][) = Θ(][σ]ϕ[2][σ]w[2] [(][D][ −] _[k][)) = Θ(][σ]w[2]_ [)][, and][ |][ϕ][ℓ][| ≤] _σ˜1_ [,][ max][ℓ] _[|][q][ℓ][| ≤]_

_̸∈_
_σw_ 2 log(Dm/δ). Applying Bernstein’s inequality for bounded distributions, we have:

[P]

p Pr (63)

_ϕ−A_ [[][⟨][ϕ][−][A][, q][−][A][⟩≥] [1][/][4] = exp(][−][Ω(][k][)) =][ O][(][ϵ][e][)][.]


We also have:

Therefore,


_b[(0)]_ + ξ[(1)] _≥_ 1/4 = exp(−Ω(k)) = O(ϵe). (64)
h i

_⟨ϕ, q⟩_ + b[(0)] + ξ[(1)] _≥_ 1 = exp(−Ω(k)) = O(ϵe) (65)
i


Pr
_ξ[(1)]_

Pr
_ϕ−A,ξ[(1)]_


where the last step follows from the assumption on σw and k. A similar argument gives:


_⟨ϕ−A, q−A⟩_ + b[(0)] + ξ[(1)] _≥_ 1 = exp(−Ω(k)) = O(ϵe). (66)
i


Pr
_ϕ−A,ξ[(1)]_


Then we have
_Tj_ EϕA,ξ(1) _yϕjIa[′]_ _[}]_ (67)
_−_ _{_

_≤_ EϕA _|yϕj|_ Eξ(1)(Ia − _Ia[′]_ [)] (68)

_O(ϵe)EϕA_ _yϕj_ (69)
_≤_ _{|_ _|}_
_≤_ _O(ϵe/σ˜)_ (70)

where the last step is from Lemma 14. Furthermore,

EϕA,ξ(1) _yϕjIa[′]_ _[}]_ (71)
_{_

= EϕA {yϕj} Eξ(1)[Ia[′] []] (72)

= EϕA _yϕj_ Pr _ϕ_ _A, q_ _A_ + b[(0)] + ξ[(1)] (0, 1) (73)
_{_ _}_ _ϕ−A,ξ[(1)]_ _⟨_ _−_ _−_ _⟩_ _∈_

h i


By Lemma 11, the assumption on po, and (63), we have


_⟨ϕ−A, q−A⟩_ + b[(0)] _∈_ (0, 1/2) _≥_ Ω(1) − _O(1/k[1][/][4]),_ (74)
h i

_ξ[(1)]_ _∈_ (0, 1/2) = 1/2 − exp(−Ω(k)), (75)
i


Pr
_ϕ−A_

Pr
_ξ[(1)]_


This leads to


_⟨ϕ−A, q−A⟩_ + b[(0)] + ξ[(1)] _∈_ (0, 1) _≥_ Ω(1). (76)
i


_β := Eξ(1)[Ia[′]_ [] =] Pr
_ϕ−A,ξ[(1)]_


By Lemma 14, EϕA {yϕj} = γ/σ˜. Therefore,

_|Tj −_ _βγ/σ˜| ≤_ _O(ϵe/σ˜)._ (77)

Now, consider j ̸∈ _A. Let B denote A ∪{j}._

_Tj = E(x,y)_ _,ξ(1)_ _yϕjI_ _ϕ, q_ + b[(0)] + ξ[(1)] (0, 1) (78)
_∼D_ _⟨_ _⟩_ _∈_
n h io

= EϕB Eϕ−B _,ξ(1)_ _yϕjI_ _⟨ϕ, q⟩_ + b[(0)] + ξ[(1)] _∈_ (0, 1) (79)
n h io

= EϕB _,ξ(1)_ _yϕj Prϕ−B_ _⟨ϕ, q⟩_ + b[(0)] + ξ[(1)] _∈_ (0, 1) _._ (80)
 h i[]


-----

Let

_Ib := Pr_ _ϕ, q_ + b[(0)] + ξ[(1)] (0, 1) _,_ (81)
_ϕ−B_ _⟨_ _⟩_ _∈_

h i

_Ib[′]_ [:= Pr] _ϕ_ _B, q_ _B_ + b[(0)] + ξ[(1)] (0, 1) _._ (82)
_ϕ−B_ _⟨_ _−_ _−_ _⟩_ _∈_

h i

Similar as above, we have |Eξ(1)(Ib − _Ib[′][)][| ≤]_ _[O][(][ϵ][e][)][ by Lemma 13. Then by Lemma 14,]_
_Tj_ EϕB _,ξ(1)_ _yϕjIb[′][}]_ (83)
_−_ _{_

_≤_ EϕB _|yϕj||Eξ(1)(Ib −_ _Ib[′][)][|]_ (84)

_O(ϵe)EϕA_ _y_ Eϕj _ϕj_ (85)
_≤_ _{|_ _|}_ _{|_ _|}_

_≤_ _O(ϵe) × O(σϕ[2]σ[˜])_ (86)

= O(σϕ[2][ϵ][e]σ[˜]). (87)
Furthermore,
EϕB _,ξ(1)_ _yϕjIb[′][}][ =][ E][ϕ]A_ _j_ _ξ[(1)][[][I]b[′][] = 0][.]_ (88)
_{_ _[{][y][}][ E][ϕ]_ _[{][ϕ][j][}][ E]_
Therefore,
_|Tj| ≤_ _O(σϕ[2][ϵ][e]σ[˜])._ (89)


**Lemma 16. Under the same assumptions as in Lemma 15,**
_∂_

_∂bi_ _LD(g[(0)]; σξ[(1)][) =][ −][a]i[(0)][T][b]_ (90)

_where |Tb| ≤_ _O(ϵe)._

_Proof of Lemma 16. Consider one neuron index i and omit the subscript i in the parameters. Since_
the unbiased initialization leads to g[(0)](x; ξ[(1)]) = 0, we have
_∂_

_ξ_ [)] (91)
_∂b_ _[L][D][(][g][(0)][;][ σ][(1)]_

= _a[(0)]E(x,y)_ _yI[yg[(0)](x; ξ)_ 1]Eξ(1)I[ _w[(0)], x_ + b[(0)] + ξ[(1)] (0, 1)] (92)
_−_ _∼D_ _≤_ _⟨_ _⟩_ _∈_
n o

= _a[(0)]_ E(x,y) _,ξ(1)_ _yI[_ _w[(0)], x_ + b[(0)] + ξ[(1)] (0, 1)] _._ (93)
_−_ _∼D_ _⟨_ _⟩_ _∈_
n o

:=Tb

Similar to the proof in Lemma 6,

| {z }

_ϕ[Pr]A[[][⟨][ϕ, q][⟩]_ [+][ b][(0)][ +][ ξ][(1)][ ∈] [(0][,][ 1)]][ −] _ϕ[Pr]A[[][⟨][ϕ][−][A][, q][−][A][⟩]_ [+][ b][(0)][ +][ ξ][(1)][ ∈] [(0][,][ 1)]] [=][ O][(][ϵ][e][)][.] (94)
_−_ _−_

Then

_A[,ξ][(1)]_ _y Pr_ (95)
_ϕ_ _A[[][⟨][ϕ][−][A][, q][−][A][⟩]_ [+][ b][(0)][ +][ ξ][(1)][ ∈] [(0][,][ 1)]]
_−_
 

= E[T]ϕ[b][ −]A,ξ(1)[E][ϕ] _|y|_ _ϕ[Pr]−A[[][⟨][ϕ, q][⟩]_ [+][ b][(0)][ +][ ξ][(1)][ ∈] [(0][,][ 1)]][ −] _ϕ[Pr]−A[[][⟨][ϕ][−][A][, q][−][A][⟩]_ [+][ b][(0)][ +][ ξ][(1)][ ∈] [(0][,][ 1)]]
 

(96)

_≤_ _O(ϵe)EϕA {|y|}_ (97)
_≤_ _O(ϵe)._ (98)
Also,

EϕA,ξ(1) _y Prϕ_ _A[[][⟨][ϕ][−][A][, q][−][A][⟩]_ [+][ b][(0)][ +][ ξ][(1)][ ∈] [(0][,][ 1)]] (99)
_−_
 

= EϕA _y_ Pr (100)
_{_ _}_ _ϕ−A,ξ[(1)][[][⟨][ϕ][−][A][, q][−][A][⟩]_ [+][ b][(0)][ +][ ξ][(1)][ ∈] [(0][,][ 1)]]

= 0. (101)
Therefore, |Tb| ≤ _O(ϵe)._


-----

**Lemma 17. We have**


_∂_

_∂ai_ _LD(g[(0)]; σξ[(1)][) =][ −][T][a]_ (102)


_where |Ta| ≤_ _O(maxℓ_ _qi,ℓ[(0)][)][. So if][ w]i[(0)]_ _∈G(δ), |Ta| ≤_ _O(σw_


log(Dm/δ)).


_Proof of Lemma 17. Consider one neuron index i and omit the subscript i in the parameters. Since_
the unbiased initialization leads to g[(0)](x; ξ[(1)]) = 0, we have

_∂_

_ξ_ [)] (103)
_∂a_ _[L][D][(][g][(0)][;][ σ][(1)]_

= E(x,y) _yI[yg[(0)](x; ξ[(1)])_ 1]Eξ(1)σ( _w[(0)], x_ + b[(0)] + ξ[(1)]) (104)
_−_ _∼D_ _≤_ _⟨_ _⟩_
n o

= E(x,y) _,ξ(1)_ _yσ(_ _w[(0)], x_ + b[(0)] + ξ[(1)]) _._ (105)
_−_ _∼D_ _⟨_ _⟩_
n o

:=Ta

Let ϕ[′]A [be an independent copy of]| _[ ϕ][A][,][ ϕ][′][ be the vector obtained by replacing in]{z_ } _[ ϕ][ the entries][ ϕ][A][ with]_
_ϕ[′]A[, and let][ x][′][ =][ Mϕ][′][ and its label is][ y][′][. Then]_

_|Ta| =_ EϕA _yEϕ−A,ξ(1)σ(⟨w[(0)], x⟩_ + b[(0)] + ξ[(1)]) (106)
n o

_≤_ [1]2 Eϕ−A,ξ(1)σ(⟨w[(0)], x⟩ + b[(0)] + ξ[(1)])|y = 1 (107)

n o

_−_ E[E]ϕ[ϕ]A[A] Eϕ−A,ξ(1)σ(⟨w[(0)], x⟩ + b[(0)] + ξ[(1)])|y = −1 (108)
n o


_≤_ [1]2 Eϕ−A,ξ(1)σ(⟨w[(0)], x⟩ + b[(0)] + ξ[(1)])|y = 1 (109)

n o

_−_ E[E]ϕ[ϕ][′]A[A] Eϕ−A,ξ(1)σ(⟨w[(0)], x[′]⟩ + b[(0)] + ξ[(1)])|y[′] = −1 (110)

n o

_[.]_


Since σ is 1-Lipschitz,

_|Ta| ≤_ 2[1] [E][ϕ][A][,ϕ]A[′] Eϕ−A _⟨w[(0)], x⟩−⟨w[(0)], x[′]⟩_ _|y = 1, y[′]_ = −1 (111)

n o

_≤_ [1]2 [E][ϕ][−][A] EϕA _⟨w[(0)], x⟩_ _|y = 1_ + Eϕ[′]A _⟨w[(0)], x[′]⟩_ _|y[′]_ = −1 (112)

 n o n o

= Eϕ _A,ϕA_ _w[(0)], x_ (113)
_−_ _⟨_ _⟩_

= Ex _⟨w[(0)], x⟩_ (114)

_≤_ Ex⟨w[(0)], x⟩[2] (115)
q


_≤_ maxℓ _qi,ℓ[(0)]vuuEx_ ℓ∈[D] _ϕ[2]ℓ_ [+] _j≠_ _ℓX:j,ℓ∈A_ _|ϕjϕℓ|_ (116)

u
t  [X] 

max _qi,ℓ[(0)]_ Ex (1 + O(1)) (117)
_≤_ _ℓ_
p

= Θ(max _qi,ℓ[(0)][)][.]_ (118)
_ℓ_

With the bounds on the gradient, we now summarize the results for the weights after the first gradient
step.


-----

**Lemma 18. Set**
_λ[(1)]w_ [= 1][/][(2][η][(1)][)][, λ]a[(1)] = λ[(1)]b = 0, σξ[(1)] = 1/k[3][/][2].

_Fix δ_ (0, 1) and suppose wi[(0)] _w(δ), b[(0)]i_ _b(δ) for all i_ [2m]. If po = Ω(k[2]/D),
_∈_ _∈G_ _∈G_ _∈_
_k = Ω(log[2](Dm/δ)), then for all i ∈_ [m], wi[(1)] = _ℓ=1_ _[q]i,ℓ[(1)][M][ℓ]_ _[satisfying]_

-  if ℓ _∈_ _A, then |qi,ℓ[(1)]_ _[−]_ _[η][(1)][a]i[(0)][βγ/]σ[˜]| ≤_ _O_ [P]|η[D][(1)]aσ˜[(0)]i _|ϵe_ _, where β ∈_ [Ω(1), 1] and depends
 


_only on wi[(0)], b[(0)]i_ _[;]_

-  if ℓ _̸∈_ _A, then |qi,ℓ[(1)][| ≤]_ _[O]_ _σϕ[2][|][η][(1)][a]i[(0)][|][ϵ][e]σ[˜]_ _;_
 

-  b[(1)]i = b[(0)]i + η[(1)]a[(0)]i _[T][b][ where][ |][T][b][|][ =][ O][ (][ϵ][e][)][;]_


_and_



-  a[(1)]i = a[(0)]i + η[(1)]Ta where _Ta_ = O(σw
_|_ _|_


log(Dm/δ)).


_Proof of Lemma 18. This follows from Lemma 10 and Lemma 15-17._

B.5 FEATURE IMPROVEMENT: SECOND GRADIENT STEP

We first show that with properly set η[(1)], for most x, |g[(1)](x; σξ[(2)][)][|][ <][ 1][ and thus][ yg][(1)][(][x][;][ σ]ξ[(2)][)][ <][ 1][.]

**Lemma 19. Fix δ** (0, 1) and suppose wi[(0)] _w(δ), b[(0)]i_ _b(δ), a[(0)]i_ _a(δ) for all i_ [2m].

_If po = Ω(k[2]/D), ∈ k = Ω(log[2](Dm/δ)), σa_ _∈Gσ˜[2]/(γk[2]), η∈G[(1)]_ = O _kmσ∈Gγ_ _a_ _, and σξ[(2)] ∈_ 1/k,
_≤_ _≤_

_then with probability ≥_ 1 − exp(−Θ(k)) over (x, y), we have yg[(1)](x; σξ[(2)][)][ <][ 1][.][ Furthermore,]

_for any i ∈_ [2m], _⟨wi[(1)], x⟩_ = _⟨qi[(1)], ϕ⟩_ = O(η[(1)]σ/γ˜ ), _⟨(qi[(1)])−A, ϕ−A⟩_ = O(η[(1)]σ/γ˜ ), and
_b[(1)]i_ _b[(1)]m+i_ = O( _η[(1)]a[(0)]i_
_−_ _|_ _[|][ϵ][e][)][.]_

_Proof of Lemma 19. Note that wi[(0)]_ = wm[(0)]+i[,][ b][(0)]i = b[(0)]m+i[, and][ a][(0)]i = −a[(0)]m+i[. Then the gradient]
for wi is the negation of that for wm+i, the gradient for bi is the negation of that for bm+i, and the
gradient for ai is the same as that for am+i. With probability ≥ 1 _−_ exp(−Θ(max{2po(D − _k), k})),_
among all j ̸∈ _A, we have that at most 2po(D −_ _k) + k of ϕj are (1 −_ _po)/σ˜, while the others are_
_−po/σ˜. For data points with ϕ satisfying this, we have:_

_g[(1)](x; σξ[(2)][)]_ (119)

2m

= _a[(1)]i_ [E]ξ[(2)][σ][(][⟨][w]i[(1)], x⟩ + b[(1)]i + ξi[(2)]) (120)

_i=1_

X


_a[(1)]i_ [E]ξ[(2)][σ][(][⟨][w]i[(1)], x⟩ + b[(1)]i + ξi[(2)]) + a[(1)]m+i[E]ξ[(2)][σ][(][⟨][w]m[(1)]+i[, x][⟩] [+][ b][(1)]m+i [+][ ξ]m[(2)]+i[)] [(121)]


_a[(1)]i_ [E]ξ[(2)][σ][(][⟨][w]i[(1)], x⟩ + b[(1)]i + ξi[(2)]) + a[(1)]m+i[E]ξ[(2)][σ][(][⟨][w]i[(1)], x⟩ + b[(1)]i + ξi[(2)]) (122)


_−a[(1)]m+i[E]ξ[(2)][σ][(][⟨][w]i[(1)], x⟩_ + b[(1)]i + ξi[(2)]) + a[(1)]m+i[E]ξ[(2)][σ][(][⟨][w]m[(1)]+i[, x][⟩] [+][ b][(1)]m+i [+][ ξ]i[(2)]) _[.]_



_i=1_

_m_

_i=1_

X

_m_

_i=1_

X


(123)


-----

Then we have

_m_

_g[(1)](x; σξ[(2)][)]_ _≤_ 2η[(1)]TaEξ(2)σ(⟨wi[(1)], x⟩ + b[(1)]i + ξi[(2)]) (124)

_i=1_

Xm

+ _a[(1)]m+i_ _wi[(1)]_ _wm[(1)]+i[, x][⟩]_ + _b[(1)]i_ _b[(1)]m+i_ (125)

_⟨_ _−_ _−_

_i=1_

Xm  

2η[(1)]Ta _wi[(1)], x_ + b[(1)]i + Eξ(2) _ξi[(2)]_ (126)

_≤_ _⟨_ _⟩_

_i=1_

Xm  

+ _a[(1)]m+i_ _wi[(1)]_ _wm[(1)]+i[, x][⟩]_ + _b[(1)]i_ _b[(1)]m+i_ _._ (127)

_⟨_ _−_ _−_

_i=1_

X  

We have _Ta_ = O(σw log(Dm/δ)), and
_|_ _|_
p

_⟨wi[(1)], x⟩_ _≤_ _O(|η[(1)]a[(0)]i_ _[|][) (][βγ/]σ[˜] + ϵe/σ˜)_ _σ[k]˜_ (128)

+ O( _η[(1)]a[(0)]i_ _ϕ[ϵ][e]σ[˜]) ((2po(D_ _k) + k)(1_ _po)/σ˜ + poD/σ˜)_ (129)
_|_ _[|][σ][2]_ _−_ _−_

_O(_ _η[(1)]a[(0)]i_ _kγ/σ˜[2]_ + ϵek/σ˜[2] + (k + poD)σϕ[2][ϵ][e] (130)
_≤_ _|_ _[|][)]_

_O(η[(1)](1 + po σ˜)/γ)._  (131)
_≤_
_b[(1)]i_ _b[(0)]i_ + _η[(1)]a[(0)]i_ _[T][b]_ (132)
_≤_

log(m/δ) _ϵe_

_≤_ _k[2]_ + _η[(1)]a[(0)]i_ _σ˜_ _._ (133)

p

Eξ(2) _ξi[(2)]_ _≤_ _O(σξ[(2)][)][.]_ (134)

_a[(1)]m+i[| ≤|][a][(0)]i_ _i_ log(Dm/δ)). (135)
_|_ _[|][ +][ |][η][(1)][T][a][| ≤|][a][(0)][|][ +][ O][(][η][(1)][σ][w]_

_wi[(1)]_ _wm[(1)]+i[, x][⟩]_ = 2 _wi[(1)], x_ = O(η[(1)](1 + poσ˜)/γ). p (136)
_⟨_ _−_ _⟨_ _⟩_
_b[(1)]i_ _−_ _b[(1)]m+i_ = 2|η[(1)]a[(0)]i _[T][b][|][ =][ O][(][|][η][(1)][a]i[(0)][|][ϵ][e][)][.]_ (137)

Then we have

_η[(1)]_ log(m/δ) _ϵe_

_g[(1)](x; σξ[(2)][)]_ _O_ _mη[(1)]σw_ log(Dm/δ) + + _η[(1)]a[(0)]i_ + σξ[(2)]
_≤_ _γ_ p _k[2]_ _σ˜_ !
 p 

(138)

_η(1)_ _ϵe_

+ O _m(|a[(0)]i_ _[|][ +][ η][(1)][σ][w]_ log(Dm/δ)) _γ_ + _η[(1)]a[(0)]i_ _σ˜_ (139)
 p  [](1) 

log(Dm/δ) _η_ _ϵe_
= O _mη[(1)]σw_ _k_ + m|a[(0)]i _[|]_ _γ_ + _η[(1)]a[(0)]i_ _σ˜_ (140)
  

log(Dm/δ)
= O _mη[(1)]σw_ _k_ + m|a[(0)]i _[|]_ _[η]γ[(1)]_ + mσaη[(1)][ k]γ (141)
 

_< 1._ (142)

Then _yg[(1)](x; σξ[(2)][)]_ _< 1. Finally, the statement on_ _⟨(qi[(1)])−A, ϕ−A⟩_ follows from a similar

calculation on _wi[(1)], x_ = _qi[(1)], ϕ_ .
_⟨_ _⟩_ _⟨_ _⟩_

We are now ready to analyze the gradients in the second gradient step.

**Lemma 20. Fix δ** (0, 1) and suppose wi[(0)] _w(δ), b[(0)]i_ _b(δ), a[(0)]i_ _a(δ) for all i_ [2m].
_∈_ _∈G_ _∈G_ _∈G_ _∈_

_η[(1)]_ _a[(0)]i_ _k(γ+ϵe)_
_Let ϵe2 := O_ _|_ _|_ + exp( Θ(k)). If k = Ω(log[2](Dm/δ)) and k = O(D), σa

_σ˜[2]σξ[(2)]_ _−_ _≤_

 


-----

_σ˜[2]/(γk[2]), η[(1)]_ = O

_where Tj satisfies:_


_, and σξ[(2)]_ = 1/k[3][/][2], then

_∂_

_∂wi_ _LD(g[(1)]; σξ[(2)][) =][ −][a]i[(1)]_


_kmσa_


_MjTj_ (143)
_j=1_

X



-  if j ∈ _A, then |Tj −_ _βγ/σ˜| ≤_ _O(ϵe2/σ˜ + η[(1)]/σξ[(2)]_ + η[(1)]|a[(0)]i _[|][ϵ][e][/][(˜]σσξ[(2)][))][, where][ β][ ∈]_

[Ω(1), 1] and depends only on wi[(0)], b[(0)]i _[;]_



-  if j ̸∈ _A, then |Tj| ≤_ _σ[1]˜_ [exp(][−][Θ(][k][)) +][ O][(][σ]ϕ[2][ϵ][e2]σ[˜]).

_Proof of Lemma 20. Consider one neuron index i and omit the subscript i in the parameters. By_
Lemma 19, Pr[yg[(1)](x; ξ[(2)]) > 1] ≤ exp(−Θ(k)). Let Ix = I[yg[(1)](x; ξ[(2)]) ≤ 1].

_∂_

_ξ_ [)] (144)
_∂w_ _[L][D][(][g][(1)][;][ σ][(2)]_

= _a[(1)]E(x,y)_ _yIxEξ(2)I[_ _w[(1)], x_ + b[(1)] + ξ[(2)] (0, 1)]x (145)
_−_ _∼D_ _⟨_ _⟩_ _∈_

_D_ n o
= _a[(1)]_ _Mj E(x,y)_ _,ξ(2)_ _yIxI[_ _w[(1)], x_ + b[(1)] + ξ[(2)] (0, 1)]ϕj _._ (146)
_−_ _∼D_ _⟨_ _⟩_ _∈_

_j=1_

X n :=Tj o

Let Tj1 := E(x,y) _,ξ(2)_ _yI[|w[(1)], x_ + b[(1)] + ξ[(2)] (0{z, 1)]ϕj . We have }
_∼D_ _⟨_ _⟩_ _∈_

_Tj_ _Tj1_ (147)
_|_ _−_ _|_

= E(x,y) _,ξ(2)_ _y(1_ Ix)I[ _w[(1)], x_ + b[(1)] + ξ[(2)] (0, 1)]ϕj (148)
_∼D_ _−_ _⟨_ _⟩_ _∈_
n o

(149)

_≤_ _σ˜[1]_ [E][(][x,y][)][∼D][,ξ][(2)][ |][1][ −] [I][x][|]

(150)

_≤_ _σ˜[1]_ [exp(][−][Θ(][k][))][.]


So it is sufficient to bound Tj1. For simplicity, we use q as a shorthand for qi[(1)].

First, consider j ∈ _A._

_Tj1 = E(x,y)_ _,ξ(2)_ _yI[_ _w[(1)], x_ + b[(1)] + ξ[(2)] (0, 1)]ϕj (151)
_∼D_ _⟨_ _⟩_ _∈_
n o

= EϕA _yϕj_ Pr _ϕ, q_ + b[(1)] + ξ[(2)] (0, 1) _._ (152)

 _ϕ−A,ξ[(2)]_ h⟨ _⟩_ _∈_ i[]

Let


_⟨ϕ, q⟩_ + b[(1)] + ξ[(2)] _∈_ (0, 1) _,_ (153)
i

_⟨ϕ−A, q−A⟩_ + b[(1)] + ξ[(2)] _∈_ (0, 1) _._ (154)
i


_Ia := Pr_
_ξ[(2)]_

_Ia[′]_ [:= Pr]
_ξ[(2)]_


_i_ _k(γ+ϵe)_
By the property of the Gaussian ξ[(2)], that _ϕA, qA_ = O( _[η][(1)][|][a][(0)]σ˜|[2]_ ), and that _ϕ, q_ =
_|⟨_ _⟩|_ _|⟨_ _⟩|_

_|⟨wi[(1)], x⟩| = O(η[(1)]/γ) < O(1/k) and |⟨ϕ−A, q−A⟩| = O(η[(1)]/γ) < O(1/k), we have_


_ϕ, q_ + b[(1)] + ξ[(2)] 0 Pr
_⟨_ _⟩_ _≥_ _−_ _ξ[(2)]_
h i

_ϕ, q_ + b[(1)] + ξ[(2)] 1 + Pr
_⟨_ _⟩_ _≥_ _ξ[(2)]_
i


_⟨ϕ−A, q−A⟩_ + b[(1)] + ξ[(2)] _≥_ 0 (155)
h i

_⟨ϕ−A, q−A⟩_ + b[(1)] + ξ[(2)] _≥_ 1 (156)
i


_Ia_ _Ia[′]_ _[| ≤]_
_|_ _−_ _ξ[(2)]_

+ Pr
_ξ[(2)][Pr]_

= O


_η[(1)]_ _a[(0)]i_
_|_ _[|][k][(][γ][ +][ ϵ][e][)]_

_σ˜[2]σξ[(2)]_


+ exp(−Θ(k)) = O(ϵe2). (157)


-----

This leads to
_Tj1_ EϕA,ϕ−A _yϕjIa[′]_ _[}]_ (158)
_−_ _{_

_≤_ EϕA _|yϕj|_ Eϕ−A (Ia − _Ia[′]_ [)] (159)

_O(ϵe2)EϕA_ _yϕj_ (160)
_≤_ _{|_ _|}_
_≤_ _O(ϵe2/σ˜)_ (161)

where the last step is from Lemma 14. Furthermore,

EϕA,ϕ−A _yϕjIa[′]_ _[}]_ (162)
_{_

= EϕA {yϕj} Eϕ−A [Ia[′] []] (163)

= EϕA _yϕj_ Pr _ϕ_ _A, q_ _A_ + b[(1)] + ξ[(2)] (0, 1) _._ (164)
_{_ _}_ _ϕ−A,ξ[(2)]_ _⟨_ _−_ _−_ _⟩_ _∈_

h i


By Lemma 19, we haveproperty of ξ[(2)], _|⟨ϕ−A, q−A⟩| ≤_ _O(η[(1)]σ/γ˜_ ). Also, |b[(1)] _−_ _b[(0)]| ≤_ _O(η[(1)]|a[(0)]i_ _[|][ϵ][e][)][. By the]_

_ξ[(2)]_ _⟨ϕ−A, q−A⟩_ + b[(1)] + ξ[(2)] _∈_ (0, 1) _−_ _ξPr[(2)]_ _b[(0)]_ + ξ[(2)] _∈_ (0, 1) (165)

h i h i

_O(η[(1)]σ/˜_ (γσξ[(2)][)) +][ O][(][η][(1)][|][a]i[(0)] _ξ_ [)][.] (166)
_≤_ [Pr] _[|][ϵ][e][/σ][(2)]_

On the other hand,


_b[(0)]_ + ξ[(2)] (0, 1) = Pr
_∈_ _ξ[(2)]_
i


_ξ[(2)]_ _∈_ (−b[(0)], 1 − _b[(0)])_ (167)
i


_β :=_ Pr
_ϕ−A,ξ[(2)]_


= Ω(1) (168)


and β only depends on b[(0)]. By Lemma 14, EϕA {yϕj} = γ/σ˜. Therefore,

_|Tj1 −_ _βγ/σ˜| ≤_ _O(ϵe2/σ˜) + O(η[(1)]/σξ[(2)][) +][ O][(][η][(1)][|][a]i[(0)][|][ϵ][e][/][(˜]σσξ[(2)][))][.]_ (169)

Now, consider j ̸∈ _A. Let B denote A ∪{j}._

_Tj1 = E(x,y)_ _,ξ(2)_ _yϕjI_ _ϕ, q_ + b[(1)] + ξ[(2)] (0, 1) (170)
_∼D_ _⟨_ _⟩_ _∈_
n h io

= EϕB Eϕ−B _,ξ(2)_ _yϕjI_ _⟨ϕ, q⟩_ + b[(1)] + ξ[(2)] _∈_ (0, 1) (171)
n h io

= EϕB _yϕj_ Pr _ϕ, q_ + b[(1)] + ξ[(2)] (0, 1) _._ (172)

 _ϕ−B_ _,ξ[(2)]_ h⟨ _⟩_ _∈_ i[]

Let


_⟨ϕ, q⟩_ + b[(1)] + ξ[(2)] _∈_ (0, 1) _,_ (173)
i

_⟨ϕ−B, q−B⟩_ + b[(1)] + ξ[(2)] _∈_ (0, 1) _._ (174)
i


_Ib := Pr_
_ξ[(2)]_

_Ib[′]_ [:= Pr]
_ξ[(2)]_


Similar as above, we have |Ib − _Ib[′][| ≤]_ _[ϵ][e2][. Then by Lemma 14,]_
_Tj1_ EϕB _,ϕ−B_ _yϕjIb[′][}]_ (175)
_−_ _{_

_≤_ EϕB _|yϕj||Eϕ−B_ (Ib − _Ib[′][)][|]_ (176)

_O(ϵe2)Eϕj_ _ϕj_ (177)
_≤_ _{|_ _|}_

_≤_ _O(ϵe) × O(σϕ[2]σ[˜])_ (178)

= O(σϕ[2][ϵ][e2]σ[˜]). (179)


Furthermore,

Therefore,


EϕB _,ϕ−B_ _yϕjIb[′][}][ =][ E][ϕ]A_ _j_ _−B_ [[][I]b[′][] = 0][.] (180)
_{_ _[{][y][}][ E][ϕ]_ _[{][ϕ][j][}][ E][ϕ]_

_|Tj1| ≤_ _O(σϕ[2][ϵ][e2]σ[˜])._ (181)


-----

**Lemma 21. Under the same assumptions as in Lemma 20,**

_∂_

_ξ_ [) =][ −][a]i[(1)][T][b] (182)
_∂b_ _[L][D][(][g][(1)][;][ σ][(2)]_

_where |Tb| ≤_ exp(−Ω(k)) + O(ϵe2).

_Proof of Lemma 21. Consider one neuron index i and omit the subscript i in the parameters. By_
Lemma 19, Pr[yg[(1)](x; ξ[(2)]) > 1] ≤ exp(−Ω(k)). Let Ix = I[yg[(1)](x; ξ[(2)]) ≤ 1].

_∂_

_ξ_ [)] (183)
_∂b_ _[L][D][(][g][(1)][;][ σ][(2)]_

= _a[(1)]_ E(x,y) _yIxEξ(2)I[_ _w[(1)], x_ + b[(1)] + ξ[(2)] (0, 1)] _._ (184)
_−_ _∼D_ _⟨_ _⟩_ _∈_
n o

:=Tb

Let Tb1 := E(x,y) _,ξ(2)_ |yI[ _w[(1)], x_ + b[(1)] + ξ[(2)] {z (0, 1)] . We have }
_∼D_ _⟨_ _⟩_ _∈_

_Tb_ Tb1 (185)
_|_ _−_ _|_

= E(x,y) _,ξ(2)_ _y(1_ Ix)I[ _w[(1)], x_ + b[(1)] + ξ[(2)] (0, 1)] (186)
_∼D_ _−_ _⟨_ _⟩_ _∈_

E(x,y) _,ξ(2)_ 1n Ix o (187)
_≤_ _∼D_ _|_ _−_ _|_

_≤_ exp(−Ω(k)). (188)


So it is sufficient to bound Tb1. For simplicity, we use q as a shorthand for qi[(1)]


_Tb1 = E(x,y)_ _,ξ(2)_ _yI_ _ϕ, q_ + b[(1)] + ξ[(2)] (0, 1) (189)
_∼D_ _⟨_ _⟩_ _∈_
n h io

= EϕA Eϕ−A,ξ(2) _yI_ _⟨ϕ, q⟩_ + b[(1)] + ξ[(2)] _∈_ (0, 1) (190)
n h io

= EϕA _y_ Pr _ϕ, q_ + b[(1)] + ξ[(2)] (0, 1) _._ (191)

 _ϕ−A,ξ[(2)]_ h⟨ _⟩_ _∈_ i[]


Let


_⟨ϕ, q⟩_ + b[(1)] + ξ[(2)] _∈_ (0, 1) _,_ (192)
i

_⟨ϕ−A, q−A⟩_ + b[(1)] + ξ[(2)] _∈_ (0, 1) _._ (193)
i


_Ib := Pr_
_ξ[(2)]_

_Ib[′]_ [:= Pr]
_ξ[(2)]_


Similar as in Lemma 20, we have |Ib − _Ib[′][| ≤]_ _[ϵ][e2][. Then by Lemma 14,]_
_Tb1_ EϕA,ϕ−A _yIb[′][}]_ (194)
_−_ _{_

_≤_ EϕA _|Eϕ−A_ (Ib − _Ib[′][)][|]_ (195)

_O(ϵe2)._ (196)
_≤_

Furthermore,


EϕA,ϕ−A _yIb[′][}][ =][ E][ϕ]A_ _−A_ [[][I]b[′][] = 0][.] (197)
_{_ _[{][y][}][ E][ϕ]_

Therefore, |Tb1| ≤ _O(ϵe2) and the statement follows._

**Lemma 22. Under the same assumptions as in Lemma 20,**

_∂_

_∂ai_ _LD(g[(1)]; σξ[(2)][) =][ −][T][a]_ (198)

_where_ _Ta_ = O(η[(1)]σ/γ˜ ) + exp( Ω(k))poly(Dm).
_|_ _|_ _−_


-----

_Proof of Lemma 22. Consider one neuron index i and omit the subscript i in the parameters. By_
Lemma 19, Pr[yg[(1)](x; ξ[(2)]) > 1] ≤ exp(−Ω(k)). Let Ix = I[yg[(1)](x; ξ[(2)]) ≤ 1].
_∂_

_ξ_ [)] (199)
_∂a_ _[L][D][(][g][(1)][;][ σ][(2)]_

= E(x,y) _yIxEξ(2)σ(_ _w[(1)], x_ + b[(1)] + ξ[(2)]) _._ (200)
_−_ _∼D_ _⟨_ _⟩_
n o

:=Ta

Let Ta1 := E(x,y) _yEξ(2)|σ(_ _w[(1)], x_ + b[(1)] + ξ{z[(2)]) . We have }
_∼D_ _⟨_ _⟩_
 _Ta_ _Ta1_ (201)

_|_ _−_ _|_

= E(x,y) _y(1_ Ix)Eξ(2)σ( _w[(1)], x_ + b[(1)] + ξ[(2)]) (202)
_∼D_ _−_ _⟨_ _⟩_

E(x,y) _,ξn(2)_ 1 Ix o (203)
_≤_ _∼D_ _|_ _−_ _|_

_≤_ exp(−Ω(k)). (204)

So it is sufficient to bound Ta1. For simplicity, we use q as a shorthand for qi[(1)].

Let ϕ[′]A [be an independent copy of][ ϕ][A][,][ ϕ][′][ be the vector obtained by replacing in][ ϕ][ the entries][ ϕ][A][ with]
_ϕ[′]A[, and let][ x][′][ =][ Mϕ][′][ and its label is][ y][′][. Then]_

_|Ta1| :=_ EϕA _yEϕ−A,ξ(2)σ(⟨w[(1)], x⟩_ + b[(1)] + ξ[(2)]) (205)
n o

_≤_ 2[1] Eϕ−A,ξ(2)σ(⟨w[(1)], x⟩ + b[(1)] + ξ[(1)])|y = 1 (206)

n o

_−_ E[E]ϕ[ϕ]A[A] Eϕ−A,ξ(2)σ(⟨w[(1)], x⟩ + b[(1)] + ξ[(2)])|y = −1 (207)
n o


_≤_ 2[1] Eϕ−A,ξ(2)σ(⟨w[(1)], x⟩ + b[(1)] + ξ[(2)])|y = 1 (208)

n o

_−_ E[E]ϕ[ϕ][′]A[A] Eϕ−A,ξ(2)σ(⟨w[(1)], x[′]⟩ + b[(1)] + ξ[(2)])|y[′] = −1 (209)

n o


_≤_ 2[1] [E][ϕ][A][,ϕ]A[′] Eϕ−A _⟨w[(1)], x⟩−⟨w[(1)], x[′]⟩_ _|y = 1, y[′]_ = −1 (210)

n o

_≤_ 2[1] [E][ϕ][−][A] EϕA _⟨w[(1)], x⟩_ _|y = 1_ + Eϕ[′]A _⟨w[(1)], x[′]⟩_ _|y[′]_ = −1 (211)

 n o n o

Eϕ _A,ϕA_ _w[(1)], x_ (212)
_≤_ _−_ _⟨_ _⟩_

= Ex _⟨w[(1)], x⟩_ (213)

= O(η[(1)]σ/γ˜ ) + exp( Ω(k)) _D_ _q[(1)]_ _ϕ_ (214)
_−_ _×_ _× ∥_ _∥∞∥_ _∥∞_

= O(η[(1)]σ/γ˜ ) + exp( Ω(k)) _[D][|][η][(1)][a][(0)][|][(][γ][ +][ ϵ][e][)]_ (215)
_−_ _σ˜[2]_


= O(η[(1)]σ/γ˜ ) + exp(−Ω(k))poly(Dm) (216)

where the fourth step follows from that σ is 1-Lipschitz, the third to the last step from Lemma 19,
and the second to the last step from Lemma 18.

With the above lemmas about the gradients, we are now ready to show that at the end of the second
step, we get a good set of features for accurate prediction.
**Lemma 23. Set**

_σ_
_η[(1)]_ = _[γ][2]_ [˜] _a_ = 0, λ[(1)]w [= 1][/][(2][η][(1)][)][, σ]ξ[(1)] = 1/k[3][/][2], (217)

_km[3][, λ][(1)]_

_η[(2)]_ = 1, λ[(2)]a = λ[(2)]w [= 1][/][(2][η][(2)][)][, σ]ξ[(2)] = 1/k[3][/][2]. (218)


-----

_Fix δ_ (0, O(1/k[3])). If po = Ω(k[2]/D), k = Ω log[2][ ] _Dmδγ_ _, and m_ max Ω(k[4]), D _,_
_∈_ _≥_ _{_ _}_

_then with probability at least 1_ _δ over the initialization, there exist_  ˜ai’s such that ˜g(x) :=
_andfor allP2i=1m ∥a˜ ia[˜]∥i2[2]σ([=][2⟨w[ O]mi[(2)]][(].[k], x[9][/m]⟩+[)]b[. Finally,][(2)]i_ [)][ satisfies][ ∥] −[a][ L][(2)][D][∥][(˜][∞]g) = 0[=][ O]. Furthermore,km1 [2] _, ∥wi[(2)] ∥∥2a˜ =∥0 = O O(˜σ/k(m/k), and), ∥ |a˜b∥[(2)]i∞_ _[|]=[ =] O[ O](k[(1][5]/m[/k][2])[)],_

_∈_   

_Proof of Lemma 23. By Lemma 5, there exists a network g[∗](x) =_ _ℓ=1_ _a[∗]ℓ_ _[σ][(][⟨][w]ℓ[∗][, x][⟩]_ [+][ b][∗]ℓ [)]
_σsatisfying˜_ _j∈A_ _[M] g[j][∗][/]([(4]x)[k] for all[)][, and][ |⟨] ([w]x, yi[∗][, x]) ∼D[⟩]_ [+][ b]i[∗]. Furthermore,[| ≤] [1][ for any][ i] |[ ∈]a[∗]i[[][n][| ≤][]][ and][32][ (][k,][x, y][ 1][/][)][(32][ ∼D][P][k][3(][)][ ≤|][k][. Now we fix an][+1)] _[b]i[∗][| ≤]_ [1][/][2][,][ w][ ℓ][, and]i[∗] [=]

show that with high probability there is a neuron in g[(2)] that can approximate the ℓ-th neuron in g[∗].

[P]

By Lemma 10, with probability 1 2δ over wi[(0)]’s, they are all in _w(δ); with probability 1_ _δ over_
_−_ _G_ _−_
_a[(0)]i_ [’s, they are all in][ G][a][(][δ][)][; with probability][ 1][ −] _[δ][ over][ b]i[(0)][’s, they are all in][ G][b][(][δ][)][. Under these]_
events, by Lemma 18, Lemma 20 and 21, for any neuron i ∈ [2m], we have


_wi[(2)]_ = a[(1)]i


_MjTj,_ (219)
_j=1_

X


_b[(2)]i_ = b[(1)]i + a[(1)]i _[T][b][.]_ (220)

-  if j ∈ _A, then |Tj −_ _βγ/σ˜| ≤_ _ϵw1 := O(ϵe2/σ˜ + η[(1)]/σξ[(2)]_ + η[(1)]|a[(0)]i _[|][ϵ][e][/][(˜]σσξ[(2)][))][, where]_

_β ∈_ [Ω(1), 1] and depends only on wi[(0)], b[(0)]i [;]


where



-  if j ̸∈ _A, then |Tj| ≤_ _ϵw2 :=_ _σ[1]˜_ [exp(][−][Θ(][k][)) +][ O][(][σ]ϕ[2][ϵ][e2]σ[˜]).

-  |Tb| ≤ _ϵb :=_ _σ[1]˜_ [exp(][−][Θ(][k][)) +][ O][(][ϵ][e2][)][.]

Given the initialization, with probability Ω(1) over b[(0)]i [, we have]


1
_|b[(0)]i_ _[| ∈]_ 2k[2][,][ 2]k[2]



_, sign(b[(0)]i_ [) = sign(][b]ℓ[∗][)][.] (221)


Finally, since [4][k]b[(0)]i[|][b]ℓ[∗][|]σ˜[βγ][2][ ∈] [[Ω(][k][2][γ/]σ[˜][2]), O(k[3]γ/σ˜[2])] and depends only on wi[(0)], b[(0)]i [, we have that for]

_|_ _|_

_ϵa = Θ(1/k[2]), with probability Ω(ϵa) > δ over a[(0)]i_ [,]
4k _b[∗]ℓ_ _[|][βγ]_ ˜σ[2]

_|b[(0)]i|_ _[|]σ[˜][2][ a]i[(0)]_ _−_ 1 _[≤]_ _[ϵ][a][,]_ _|a[(0)]i_ _[|][ =][ O]_  _k[2]γ_  _._ (222)

Let na = ϵam/4. For the given value of m, by (219)-(222) we have with probability ≥ 1 − 5δ over
the initialization, for each ℓ there is a different set of neurons Iℓ _⊆_ [m] with |Iℓ| = na and such that
for each iℓ _Iℓ,_
_∈_

1
_b[(0)]iℓ_ _[| ∈]_ _,_ sign(b[(0)]iℓ [) = sign(][b]ℓ[∗][)][,] (223)
_|_ 2k[2][,][ 2]k[2]
 

4k _b[∗]ℓ_ _[|][βγ]_ ˜σ[2]
_|_ _iℓ_ _[−]_ [1] _a[(0)]iℓ_ _[|][ =][ O]_ _._ (224)

_|b[(0)]iℓ_ _[|]σ[˜][2][ a][(0)]_ _[≤]_ _[ϵ][a][,]_ _|_  _k[2]γ_ 


-----

We also have

_iℓ_ _[βγ]_
_wi[(2)]ℓ_ _[, x][⟩−]_ _[a][(0)]_
_⟨_ _σ˜_

_iℓ_ _[βγ]_
_wi[(2)]ℓ_ _[, x][⟩−]_ _[a][(1)]_

_≤_ _⟨_ _σ˜_


(225)

(226)

(227)

(228)

(229)


_Mj, x_
_⟨_ _⟩_
_jX∈A_

_Mj, x_
_⟨_ _⟩_
_jX∈A_


_a[(1)]iℓ_ _[βγ]_


_iℓ_ _[βγ]_
_Mj, x_
_⟨_ _⟩−_ _[a][(0)]σ˜_
_jX∈A_


_Mj, x_
_⟨_ _⟩_
_jX∈A_


_D_

_iℓ_ _[βγ]_
_Tjϕj_
_j=1_ _−_ _[a][(1)]σ˜_

X


_βγ_


_iℓ_ _[βγ]_ _βγ_

= _a[(1)]iℓ_ Xj=1 _Tjϕj −_ _[a]_ _σ˜_ _jX∈A_ _ϕj_ + _a[(1)]iℓ_ _[−]_ _[a]i[(0)]ℓ_ _σ˜_ _jX∈A_ _ϕj_ (227)

_D_

_βγ_

= _a[(1)]iℓ_ Xj=1 _Tjϕj −_ _[βγ]σ˜_ _jX∈A_ _ϕj_ + _a[(1)]iℓ_ _[−]_ _[a]i[(0)]ℓ_ _σ˜_ _jX∈A_ _ϕj_ _._ (228)

We have _a[(1)]iℓ_ _[−]_ _[a]i[(0)]ℓ_ = O(η[(1)]σw log(Dm/δ)), and

p

_D_

_Tjϕj_ _ϕj_ (Tj + _Tjϕj_ (229)

Xj=1 _−_ _[βγ]σ˜_ _jX∈A_ _≤_ _jX∈A_ _−_ _[βγ]σ˜_ [)][ϕ][j] _jX̸∈A_

_O(kϵw1/σ˜) + O(Dϵw2/σ˜) =: ϵϕ._ (230)
_≤_

For the given values of parameters, we have


_a[(1)]iℓ_


_γ_
_ϵe2 = O_ _,_

_m[2]_

 _kγ_

_ϵw1 = O_ _,_

_m[2]σ˜_ [+][ γϵ]km[e][2]

 

_γ_
_ϵw2 = O_ _,_

_m[2]σ˜_

 γ 

_ϵb = O_ _,_

_m[2]_

 _k2γ_

_ϵϕ = O_

_m[2]σ˜[2][ +][ γϵ]m[2][e]σ˜_ [+]




(231)

(232)

(233)

(234)

(235)

(236)


_mσ˜[2]_


Therefore,

_iℓ_ _[βγ]_
_wi[(2)]ℓ_ _[, x][⟩−]_ _[a][(0)]_
_⟨_ _σ˜_


_Mj, x_
_⟨_ _⟩_
_jX∈A_


_a[(1)]iℓ_ _ϵϕ +_ _a[(1)]iℓ_ _[−]_ _[a]i[(0)]ℓ_ _[kγ]_ (237)
_≤_ _σ˜[2]_

2

˜σ[2] _k_ _γ_ _γ_
_O_ log(Dm/δ) (238)
_≤_ _k[2]γ_ [+][ η][(1)][σ][w] _m[2]σ˜[2][ +][ γϵ]m[2][e]σ˜_ [+] _mσ˜[2]_
   

p _kγ_

+ O _η[(1)]σw_ log(Dm/δ) (239)

_σ˜[2]_

1 p 
_O_ _._ (240)
_≤_ _m_
 

We also have by Lemma 18 and 21:

1 1
_b[(2)]iℓ_ _[−]_ _[b]i[(0)]ℓ_ _[| ≤]_ _[O]_ _η[(1)]_ _a[(0)]iℓ_ _[|][ϵ][e][ +][ |][a]i[(1)]ℓ_ _[|]_ _O_ _._ (241)
_|_ _|_ _σ˜_ [exp(][−][Θ(][k][)) +][ ϵ][e2] _≤_ _m_
    


-----

Now, construct ˜a such that ˜aiℓ = [2][a]ℓ[∗][|][b][∗]ℓ _[|]_ _ai = 0 elsewhere. Then_

_b[(0)]_
_|_ _iℓ_ _[|][n][a][ for each][ ℓ]_ [and each][ i][ℓ] _[∈]_ _[I][ℓ][, and][ ˜]_

_|g˜(x) −_ 2g[∗](x)| (242)

3(k+1) 3(k+1)

= _a˜iℓ_ _σ_ _wi[(2)]ℓ_ _[, x][⟩]_ [+][ b]i[(2)]ℓ 2a[∗]ℓ _[σ][ (][⟨][w]ℓ[∗][, x][⟩]_ [+][ b][∗]ℓ [)] (243)

_⟨_ _−_

_ℓ=1_ _iℓ_ _Iℓ_ _ℓ=1_

X X∈   X

3(k+1) 3(k+1)

= Xℓ=1 _iXℓ∈Iℓ_ _|2ba[(0)]iℓ[∗]ℓ_ _[|][|][b][n][∗]ℓ[a][|]_ _σ_ ⟨wi[(2)]ℓ _[, x][⟩]_ [+][ b]i[(2)]ℓ  _−_ Xℓ=1 _iXℓ∈Iℓ_ _|2ba[(0)]iℓ[∗]ℓ_ _[|][|][b][n][∗]ℓ[a][|]_ _σ_ _|b|b[(0)]iℓ[∗]ℓ_ _[| ⟨][|]_ _[w]ℓ[∗][, x][⟩]_ [+][ b][(0)]iℓ !

(244)

3(k+1)

1 _ℓ_ _[|][b][∗]ℓ_ _[|]_ _ℓ_ _[|][b][∗]ℓ_ _[|]_ _iℓ_ _[βγ]_

_≤_ _ℓ=1_ _iℓ_ _Iℓ_ _na_  _b[(0)]iℓ_ _[|]_ _σ_ _⟨wi[(2)]ℓ_ _[, x][⟩]_ [+][ b]i[(2)]ℓ _−_ [2][a]b[∗][(0)]iℓ _[|]_ _σ_  _σ˜_ _j_ _A⟨Mj, x⟩_ + b[(0)]iℓ 

X X∈ _|_   _|_ X∈

 [2][a][∗]  _[a][(0)]_ (245)

3(k+1)

+ Xℓ=1 _iXℓ∈Iℓ_ _n1a_  _|bℓ[(0)]iℓ[|][b][∗]ℓ[|]_ _[|]_ _σ_  _iℓσ˜[βγ]_ _jX∈A⟨Mj, x⟩_ + b[(0)]iℓ  _−_ [2]|[a]bℓ[∗][(0)]iℓ[|][b][∗]ℓ[|] _[|]_ _σ_ _|b|b[(0)]iℓ[∗]ℓ_ _[| ⟨][|]_ _[w]ℓ[∗][, x][⟩]_ [+][ b][(0)]iℓ

 [2][a][∗]  _[a][(0)]_  (246)

2a[∗]ℓ _[|][b][∗]ℓ_ _[|]_ 1
3(k + 1) max _O_ + (247)
_≤_ _ℓ_ _b[(0)]iℓ_ _[|]_  _m_ 

_|_

3(k + 1) max 2a[∗]ℓ _[|][b][∗]ℓ_ _[|]_ _σ˜|b[(0)]iℓ_ _[|]_ 4ka[(0)]iℓ _[βγ][|][b]ℓ[∗][|]_ 1 _k_ (248)
_ℓ_ _|b[(0)]iℓ_ _[|]_ 4k|b[∗]ℓ _[|]_ _σ˜[2]|b[(0)]iℓ_ _[|]_ _−_ _σ˜_

_k4_
= O (249)

_m_ [+][ k][2][ϵ][a]

 

_≤_ 1. (250)

Here the second equation follows from that σ is positive-homogeneous in [0, 1], |⟨wℓ[∗][, x][⟩] [+][ b][∗]ℓ _[| ≤]_ [1][,]
_|b[(0)]iℓ_ _[|][/][|][b]ℓ[∗][| ≤]_ [1][. This guarantees][ y]g[˜](x) ≥ 1. Changing the scaling of δ leads to the statement.

Finally, the bounds on ˜a follow from the above calculation. The bound on ∥a[(2)]∥2 follows from
Lemma 22, and those on _wi[(2)]_ 2 and _b[(2)]i_ _i_ and
_∥_ _∥_ _∥_ _[∥][2][ follow from (219)(220) and the bounds on][ a][(1)]_
_b[(1)]i_ in Lemma 18.

B.6 CLASSIFIER LEARNING STAGE

Once we have a set of good features, we are now ready to prove that the later steps will learn an
accurate classifier. The intuition is that the first layer’s weights do not change much and the second
layer’s weights get updated till achieving good accuracy. In particular, we will employ the online
optimization technique from Daniely & Malach (2020).

We begin by showing that the first layer’s weights do not change too much.

**Lemma 24. Assume the same conditions as in Lemma 23. Suppose for t > 2, λ[(]a[t][)]** = λ[(]w[t][)] [=][ λ][,]
_η[(][t][)]_ = η for some λ, η (0, 1), and σξ[(][t][)] = 0. Then for any t > 2 and i [2m],
_∈_ _∈_


_|bb[(0)]iℓ[∗]ℓ_ _[| ⟨][|]_ _[w]ℓ[∗][, x][⟩]_ [+][ b][(0)]iℓ ![]

_|_

(246) 


3(k+1)

_ℓ=1_

X


_iℓ_ _[βγ]_

 _σ˜_

 _[a][(0)]_


_a[(]i[t][)]_
_|_ _[| ≤]_ _[ηt][ +][ O]_


_tηλσ˜_
_∥wi[(][t][)]_ _−_ _wi[(2)]∥2 ≤_ _O_ _k_


_tηλ_
_|b[(]i[t][)]_ _−_ _b[(2)]i_ _[| ≤]_ _[O]_ _k[2]_
 


(251)

(252)

(253)


_km[2]_

+ η[2]t[2] + O


+ η[2]t[2] + O



_km[2]_

_t_

_km[2]_


-----

_Proof of Lemma 24. First, we bound the size of_ _a[(]i[t][)]_
_|_ _[|][:]_

_|a[(]i[t][)][|][ =]_ _i_ _−_ _η ∂a[∂]_ _i_ _LD(g[(][t][−][1)])_ (254)

(1 2ηλ)a[(]i[t][−][1)] _ηE(x,y)_ _yI[yg[(][t][−][1)](x)_ 1]σ( _wi[(][t][−][1)], x_ + b[(]i[t][−][1)]) (255)
_≤_ [(1] −[ −] [2][ηλ][)][a][(][t][−][1)] _−_ _∼D_ _≤_ _⟨_ _⟩_
n o

_a[(]i[t][−][1)]_ + η (256)
_≤|_ _|_


which leads to

where _a[(2)]i_
_|_ _[|][ =][ O]_


_a[(]i[t][)]_ _i_ (257)
_|_ _[| ≤]_ _[ηt][ +][ |][a][(2)][|]_

_km1_ [2] . We are now to bound the change of wi[(][t][)] and b[(]i[t][)][.]



_wi[(][t][)]_ _wi[(2)]_ 2 (258)
_∥_ _−_ _∥_

_i_ _−_ _η ∂w[∂]_ _i_ _LD(g[(][t][−][1)]) −_ _wi[(2)]_ 2 (259)

[(1][ −] [2][ηλ][)][w]i[(][t][−][1)] (260)

+[(1] ηa[ −][(]i[t][−][2][ηλ][1)]E[)]([w]x,y[(][t][−])∼D[1)] _yI[yg[(][t][−][1)](x) ≤_ 1]I[⟨wi[(][t][−][1)], x⟩ + b[(]i[t][−][1)] _∈_ (0, 1)]x _−_ _wi[(2)]_ (261)
n o


(1 2ηλ)wi[(][t][−][1)] _wi[(2)]_ (262)
_≤_ _−_ _−_ 2

+ η _a[(]i[t][−][1)]E(x,y)∼D_ _yI[yg[(][t][−][1)](x) ≤_ 1]I[⟨wi[(][t][−][1)], x⟩ + b[(]i[t][−][1)] _∈_ (0, 1)]x 2 (263)
n o

(1 2ηλ) _wi[(][t][−][1)]_ _wi[(2)]_ _wi[(2)]_ _a[(]i[t][−][1)]_ (264)
_≤_ _−_ _−_ 2 [+ 2][ηλ] 2 [+][ η]

leading to

_wi[(][t][)]_ _wi[(2)]_ 2 2tηλ _wi[(2)]_ _i_ (265)
_∥_ _−_ _∥_ _≤_ 2 [+][ η][2][t][2][ +][ t][|][a][(2)][|][.]

Note that _wi[(2)]_ 2 = O(˜σ/k).
_∥_ _∥_

_|b[(]i[t][)]_ _−_ _b[(2)]i_ _[|][ =]_ _i_ _−_ _η ∂b[∂]_ _i_ _LD(g[(][t][−][1)]) −_ _b[(2)]i_ (266)

_b[(]i[t][−][1)]_ _b[(2)]i_ (267)
_≤_ _[b][(][t][−][1)]_ _−_

+ η _a[(]i[t][−][1)]E(x,y)∼D_ _yI[yg[(][t][−][1)](x) ≤_ 1]I[⟨wi[(][t][−][1)], x⟩ + b[(]i[t][−][1)] _∈_ (0, 1)] (268)
n o

_b[(]i[t][−][1)]_ _b[(2)]i_ + η _a[(]i[t][−][1)]_ (269)
_≤_ _−_

leading to

_b[(]i[t][)]_ _b[(2)]i_ _i_ (270)
_|_ _−_ _[| ≤]_ _[η][2][t][2][ +][ t][|][a][(2)][|][.]_


Note that _b[(2)]i_ = O(1/k[2]).

**Lemma 25. Assume the same conditions as in Lemma 24. Let ga˜[(][t][)][(][x][) =][ P]i[m]=1** _a[˜]iσ(⟨wi[(][t][)][, x][⟩]_ [+][ b][(]i[t][)][)][.]
_Then_

_tηλσ˜_ _t_

_ℓ(ga˜[(][t][)][(][x][)][, y][)][ −]_ _[ℓ][(][g]a˜[(2)][(][x][)][, y][)][| ≤∥]a[˜]_ 2 _a˜_ 0 _O_ + η[2]t[2] + O _._ (271)
_|_ _∥_ _∥_ _∥_ _k_ _km[2]_

    

p


-----

_Proof of Lemma 25. It follows from that_

_|ℓ(ga˜[(][t][)][(][x][)][, y][)][ −]_ _[ℓ][(][g]a˜[(2)][(][x][))][|]_ (272)

_≤|ga˜[(][t][)][(][x][)][ −]_ _[g]a˜[(2)][(][x][)][|]_ (273)

_a˜_ 2 _a˜_ 0 max _σ(_ _wi[(][t][)][, x][⟩]_ [+][ b]i[(][t][)][)][ −] _[σ][(][⟨][w]i[(2)], x_ + b[(2)]i [)] (274)
_≤∥_ _∥_ _∥_ _∥_ _i_ [2m] _⟨_ _⟩_

_∈_

p

_a˜_ 2 _a˜_ 0 max _wi[(][t][)]_ _wi[(2)], x_ + _b[(]i[t][)]_ _b[(2)]i_ _._ (275)
_≤∥_ _∥_ _∥_ _∥_ _i_ [2m] _⟨_ _−_ _⟩_ _−_

_∈_

p  

and Lemma 24.

B.7 PROOF OF THEOREM 1

Based on the above lemmas, following the same argument as in the proof of Theorem 2 in Daniely &
Malach (2020), we get our main theorem.
**Theorem 26 (Full version of Theorem 1). Set**


_σ[2]_
_η[(1)]_ = _[γ][2]_ [˜] _a_ = 0, λ[(1)]w [= 1][/][(2][η][(1)][)][, σ]ξ[(1)] = 1/k[2], (276)

_km[3][, λ][(1)]_

_η[(2)]_ = 1, λ[(2)]a = λ[(2)]w [= 1][/][(2][η][(2)][)][, σ]ξ[(2)] = 1/k[2], (277)

_k[2]_ _k[3]_
_η[(][t][)]_ = η = _a_ = λ[(]w[t][)] [=][ λ][ ≤] _ξ_ = 0, for 2 < t _T._ (278)

_Tm[1][/][3][, λ][(][t][)]_ _σm˜_ [1][/][3][, σ][(][t][)] _≤_

_For any δ_ (0, 1), if po = Ω(k[2]/D), k = Ω log[2][ ] _δγD_ _, max_ Ω(k[4]), D _m_ _poly(D), then_
_∈_ _{_ _} ≤_ _≤_

_we have for any_ Ξ, with probability at least 1 _δ, there exists t_ [T ] such that
_D ∈F_ _−_ _∈_

8
_k_
Pr[sign(g[(][t][)](x)) = y] _L_ (g[(][t][)]) = O _._ (279)
_̸_ _≤_ _D_ _m[2][/][3][ +][ k]m[3][T][2][ +][ k][2][m]T_ [2][/][3]
 

_Consequently, for any ϵ ∈_ (0, 1), if T = m[4][/][3], and max{Ω(k[12]/ϵ[3][/][2]), D} ≤ _m ≤_ _poly(D), then_

Pr[sign(g[(][t][)](x)) = y] _L_ (g[(][t][)]) _ϵ._ (280)
_̸_ _≤_ _D_ _≤_

_Proof of Theorem 1. Consider_ _L[˜]_ (g[(][t][)]) = E[ℓ(g[(][t][)], y)] + λ[(]a[t][)] 2[. Note that the gradient update]
_D_ _[∥][a][(][t][)][∥][2]_
using _L[˜]_ (g[(][t][)]) is the same as the update in our learning algorithm. Then by Theorem 27, Lemma 23,
_D_
and Lemma 25,


_Tηλσ˜_

_k_




_T_
+ η[2]T [2] + O

_km[2]_




(281)



_T_

_L˜_ (g[(][t][)]) _a∥2[2]_
_D_ _≤_ _[∥][˜]2_
_t=3_

X


+ _a˜_ 2
_∥_ _∥_


_a˜_ 0
_∥_ _∥_


_a_ 2
+ _∥[2]_ _√m + ηm_ (282)

_[∥]2[˜]ηT_ [+][ ∥][a][(2)][∥][2]

_k9_ _k[9]_
_O_ _._ (283)
_≤_ _m_ [+][ k][4][η][2][T][ 2][ +][ k]m[3][T][2][ +] _ηTm_ [+][ ηm]
 

8
_k_
_O_ _._ (284)
_≤_ _m[2][/][3][ +][ k]m[3][T][2][ +][ k][2][m]T_ [2][/][3]
 

The statement follows from that 0-1 classification error is bounded by the hinge-loss.

**Theorem 27 (Theorem 13 in Daniely & Malach (2020)). Fix some η, and let f1, . . ., fT be some**
_sequence of convex functions. Fix someevery θ[∗]_ _the following holds:_ _θ1, and assume we update θt+1 = θt −_ _η∇ft(θt). Then for_


_T_

_ft(θt)_
_≤_ _T[1]_
_t=1_

X


_T_

_ft(θt)_ 2 + η [1]
_∥∇_ _∥_ _T_
_t=1_

X


_ft(θ[∗]) +_
_t=1_

X


1

2 [+][ ∥][θ][1][∥][2]
2ηT

_[∥][θ][∗][∥][2]_


_∥∇ft(θt)∥2[2][.]_
_t=1_

X

(285)


-----

C LOWER BOUND FOR LINEAR MODELS ON FIXED FEATURE MAPPINGS

**Theorem 28 (Restatement of Theorem 2). Suppose Ψ is a data-independent feature mapping of**
_dimension N with bounded features, i.e., Ψ : X →_ [−1, 1][N] _. Define for B > 0:_

_HB = {h(˜x) : h(˜x) = ⟨Ψ(˜x), w⟩, ∥w∥2 ≤_ _B}._ (286)

_Then, ifat least p 3o < k1_ _≤√D/22NB[k]16 and._ _k is odd, then there exists D ∈FΞ such that all h ∈HB have hinge-loss_

_−_
 

_Proof of Theorem 2. We first show that FΞ contains some distributions that are essentially sparse_
parity learning problems, and then we invoke the lower bound result from existing work for such
problems.

Consider D defined as follows.

-  Let P = _i_ [k] : i is odd . That is, if there are odd numbers of 1’s in _ϕ[˜]A, then y = +1._
_{_ _∈_ _}_

-  Let _ϕ˜_ be a distribution where all entries _ϕ[˜]j are i.i.d. with Pr[ϕ[˜]j = 0] = Pr[ϕ[˜]j = 1] =_
_D[(0)]_

1/2. Let be the distribution over (˜x, y) induced by _ϕ˜_ and the above P .
_D[(0)]_ _D[(0)]_

-  Let _ϕ˜_ be a distribution where all entries _ϕ[˜]j for j_ _A are i.i.d. with Pr[ϕ[˜]j = 1] =_
_D[(1)]_ _̸∈_

_po/(2 −_ 2po), while Pr[ϕ[˜]A = (0, 0, . . ., 0)] = Pr[ϕ[˜]A = (1, 1, . . ., 1)] = 1/2. Let D[(1)] be
the distribution over (˜x, y) induced by _ϕ˜_ and the above P .
_D[(1)]_

-  Let _A_ = po + (1 _po)_ .
_D[mix]_ _D[(0)]_ _−_ _D[(1)]_


It can be verified that such distributions are included in FΞ for γ = Θ(1).

Assume for contradiction that for allsmaller than po 1 _√22NB[k]_ . Then for all the distributions D ∈FΞ, there exists hA[∗] _∈Hdefined above, we haveB such that h = ⟨Ψ, w[∗]⟩_ loss

_−_ _D[mix]_
 


2NB

_._ (287)
2[k]


E (0)[ℓ(h[∗](˜x), y)] < 1
_D_ _−_


Now let Dz be a distribution over z ∈{−1, +1}[D] with i.i.d. entries zj and Pr[zj = −1] = Pr[zj =
+1] = 1/2. Let fA(z) = _j_ _A_ _[z][j][ be the][ k][-sparse parity functions. Let][ Ψ][′][(][z][) = Ψ(][M]_ [(][z][ + 1)][/][2)][.]

_∈_
Then we have h[′](z) = ⟨Ψ[′](z), w[∗]⟩ such that for all A,

[Q]

_√2NB_

E _z_ [ℓ(h[′](z), fA(z))] < 1 _._ (288)
_D_ _−_ 2[k]

This is contradictory to Theorem 29.

The following theorem is implicit in the proof in Theorem 1 in Daniely & Malach (2020).


**Theorem 29. For a subset A ⊆** [D] of size k, let the distribution DA over (z, y) defined as follows:
_z is uniform over {±1}[D]_ _and y =_ _i∈A_ _[z][i][. Fix some][ Ψ :][ {±][1][}][D][ →]_ [[][−][1][,][ +1]][N] _[, and define:]_

Ψ [=][ {][z][ →⟨][Ψ(][z][)][, w][⟩] [:][ ∥][w][∥][2]
_H[B]_ [Q] _[≤]_ _[B][}][.]_

_If k is odd and k ≤_ _D/16, then there exists some A such that_


2NB

2[k]


min E _A_ [ℓ(h(z), y)] 1
_h_ Ψ _D_ _≥_ _−_
_∈H[B]_


We now prove the corollary.


-----

**Corollary 30 (Restatement of Corollary 3). For any function f using a shift-invariant kernel K**
_with RKHS norm bounded by L, or f_ (x) = _i_ _[α][i][K][(][z][i][, x][)][ for some data points][ z][i][ and][ ||][α][||][2][ ≤]_ _[L][.]_

_Ifpo 3(1 < k ≤2[k]_ _D/)16 andpoly(1 kd,L is odd, then there exists)_ _[.]_ [P] _D ∈FΞ such that f have hinge-loss at least_
_−_ [poly][(][d,L][)] _−_

_Proof. By Claim 1 in Rahimi & Recht (2008), for any ν > 0, there exists N = poly(d, 1/ν)_
Fourier features Ψj that can approximate the shift-invariant kernel up to error ν. For any ϵ > 0,
consider _i_ _[α][i][⟨][Ψ(][z][i][)][,][ Φ(][x][)][⟩]_ [=][ ⟨][P]i _[α][i][Ψ(][z][i][)][,][ Ψ(][x][)][⟩][. Let][ w][ =][ P]i_ _[α][i][Ψ(][z][i][)][ and let][ ν][ =][ O][(][ ϵ]L_ [)][,]

then ⟨Ψ(x), w⟩ approximates f (x) upto error ϵ and N = poly(d, L, 1/ϵ) and the norm of w bounded
by B = poly(d, L, 1/ϵ). The reasoning is the same for f in the RKHS form, replacing sum with

[P]

integral. By Theorem 2, Ψ(x), w has hinge-loss at least p0(1 _√22NB[k]_ ). Thus, the function f has

loss at least p0(1 _⟨2[k]_ ) _⟩_ _ϵ. Choose ϵ =_ poly(1d,L) [, we get the bound.] −
_−_ [poly][(][d,L,][1][/ϵ][)] _−_

D LOWER BOUND FOR LEARNING WITHOUT INPUT STRUCTURE

First recall the Statistical Query model (Kearns, 1998). In this model, the learning algorithm can
only receive information about the data through statistical queries. A statistical query is specified
by some property predicate Q of labeled instances, and a tolerance parameter τ ∈ [0, 1]. When
the algorithm asks a statistical queryPQ = Pr[Q(x, y) is true]. Q is also required to be polynomially computable, i.e., for any (Q, τ ), it receives a response _P[ˆ]Q ∈_ [PQ − _τ, PQ + τ_ ], where (x, y)
_Q(x, y) can be computed in polynomial time. Notice that a statistical query can be simulated by_
empirical average of a large random sample of data of size roughly O(1/τ [2]) to assure the tolerance τ
with high probability.

Blum et al. (1994) introduces the notion of Statistical Query dimension, which is convenient for our
purpose.
**Definition 31 (Definition 2 in Blum et al. (1994)). For concept class C and distribution D, the**
_statistical query dimension SQ-DIM(C, D) is the largest number d such that C contains d concepts_
_c1, . . ., cd that are nearly pairwise uncorrelated: specifically, for all i ̸= j,_

Pr (289)
_|_ _x_ _x_
_∼D[[][c][i][(][x][) =][ c][j][(][x][)]][ −]_ [Pr]∼D[[][c][i][(][x][)][ ̸][=][ c][j][(][x][)]][| ≤] [1][/d][3][.]

**Theorem 32 (Theorem 12 in Blum et al. (1994)). In order to learn C to error less than 1/2 −** 1/d[3]
_in the Statistical Query model, where d = SQ-DIM(C, D), either the number of queries or 1/τ must_
_be at least_ [1]2 _[d][1][/][3][.]_

We now use the above tools to prove our lower bound.

**Theorem 33 (Restatement of Theorem 4). For any algorithm in the Statistical Query model that can**
_learn over_ Ξ0 to error less than [1]2 _D1_ 3 _[, either the number of queries or][ 1][/τ][ must be at least]_
_F_ _[−]_ ( _k_ [)]

12 _Dk_ 1/3.
  

_Proof of Theorem 4. Consider the following concept class and marginal distribution:_

-  Let D be the distribution over ˜x, given by ˜x = M _ϕ[˜] and_ _ϕ[˜]j are i.i.d. with Pr[ϕ[˜]j = 0] =_
Pr[ϕ[˜]j = 1] = 1/2.



-  Let C be the class of functions y = gA(ϕ[˜]) = I[[P]j[(1][ −] _ϕ[˜]j) is odd] for different A ⊆_ [D].

The distributions over (˜x, y) induced by (C, ) are a subset of Ξ0 . It is then sufficient to show that
SQ-DIM(C, ) _Dk_ . _D_ _F_
_D_ _≥_

It is easy to see that  C are essentially the sparse parity functions: if zj = 2ϕ[˜]j 1, then gA(ϕ[˜]) =
_−D_

_j_ _A_ _[z][j][. This then implies that the][ g][A][’s are uncorrelated, so SQ-DIM][(][C,][ D][)][ ≥]_ _k_ .
_∈_

Q   


-----

E COMPLETE EXPERIMENTAL RESULTS

Our experiments mainly focus on feature learning and the effect of the input structure. We first
perform simulations on our learning problems to (1) verify our main theorems on the benefit of
feature learning and the effect of input structure (2) verify our analysis of feature learning in networks.
We then check if our insights carry over to real data: (3) whether similar feature learning is presented
in real network/data; (4) whether damaging the input structure lowers the performance. The results
are consistent with our analysis and provide positive support for the theory.

The experiments were ran 5 times with different random seeds, and the average results (accuracy) are
reported. The standard deviations of the results are smaller than 0.5% and thus we do not present
them for clarity. The hardware specifications are 4 Intel(R) Core(TM) i7-7700HQ CPU @ 2.80GHz,
16 GB RAM, and one NVIDIA GPU GTX1080.

E.1 SIMULATION

We train a two-layer network following our learning process. We use two fixed feature methods: the
NTK (Fang et al., 2021) and random feature (RF) methods based on the same network and random
initialization as the network learning. More precisely, in the NTK method, we randomly initialize the
network and take its NTK and learn a classifier on it. In the RF method, we freeze the first layer of
the network, and train the second layer (on the random features given by the frozen neurons). The
training step number is the same as that in network learning. We also test these three methods on the
data distribution with input structure removed (i.e., FΞ0 in Theorem 4). For comparison, we take
the representation of our two-layer network at step one/step two, named One Step/Two Step (fix the
weight of the 1st layer after the first step/second step to train the weight of the second layer), and
train the best classifiers on top of them.

Recall that our analysis is on the directions of the weights without considering their scaling, and thus
it is important to choose cosine similarity rather than the typical ℓ2 distance. Thus, we use metric Cos
Similarity max{i∈[2m]} cos(wi, _j∈A_ _[M][j][)][ in our tables, and use Multidimensional Scaling to plot]_

the weights distribution. The simulation dataset size is 50000. During training, the batch size is 1000,
while for the first two steps we use the approximate full gradient (batch size is 50000). Each step is

[P]

corresponding to one weights update.

E.1.1 PARITY LABELING

**Setting. We generate data according to the parity function data distributions used in our proof of**
the lower bound for fixed features (Theorem 2), with d = 500, D = 100, k = 5, po = 1/2, with a
randomly sampled A. More precisely, we consider D defined as follows.

-  Let P = _i_ [k] : i is odd . That is, if there are odd numbers of 1’s in _ϕ[˜]A, then y = +1._
_{_ _∈_ _}_

-  Let _ϕ˜_ be a distribution where all entries _ϕ[˜]j are i.i.d. with Pr[ϕ[˜]j = 0] = Pr[ϕ[˜]j = 1] =_
_D[(0)]_

1/2. Let be the distribution over (˜x, y) induced by _ϕ˜_ and the above P .
_D[(0)]_ _D[(0)]_

-  Let _ϕ˜_ be a distribution where all entries _ϕ[˜]j for j_ _A are i.i.d. with Pr[ϕ[˜]j = 1] =_
_D[(1)]_ _̸∈_

_po/(2 −_ 2po), while Pr[ϕ[˜]A = (0, 0, . . ., 0)] = Pr[ϕ[˜]A = (1, 1, . . ., 1)] = 1/2. Let D[(1)] be
the distribution over (˜x, y) induced by _ϕ˜_ and the above P .
_D[(1)]_

-  Let _A_ = po + (1 _po)_ .
_D[mix]_ _D[(0)]_ _−_ _D[(1)]_

The network and the training follow Section 3, where the network size is m = 300 and the training
time T = 600 steps.

**Verification of the Main Results. Figure 5 shows that the results are consistent with our analysis.**
Network learning gets high test accuracy while the two fixed feature methods get significantly lower
accuracy. Furthermore, when the input structure is removed, all three methods get test accuracy
similar to random guessing.


-----

Figure 5: Test accuracy on simulated data under parity labeling with or without input structure.


Figure 6: Visualization of the weights wi’s after initialization/one gradient step/two gradient steps in
network learning under parity labeling. The red star denotes the ground-truth _j_ _A_ _[M][j][; the orange]_

_∈_
star is − [P]j∈A _[M][j][. The red dots are the weights closest to the red star after two steps; the orange]_

ones are for the orange star. [P]

|Model|Network NTK RF One Step Two Step Network w/o structure|
|---|---|
|Train Acc (%) Test Acc (%) Cos Similarity|100.0 84.0 74.7 51.3 100.0 100.0 100.0 86.4 76.0 52.2 100.0 52.0 0.997 NA 0.114 0.848 0.997 0.253|


Table 1: Parity labeling results in six methods. The cosine similarity is computed between the
ground-truth _j_ _A_ _[M][j][ and the closest neuron weight.]_

_∈_

[P]

**Feature Learning in Networks. Figure 6 shows that the results are as predicted by our analysis.**
After the first gradient step, some weights begin to cluster around the ground-truth _j_ _A_ _[M][j][ (or]_

_∈_
_−_ [P]j∈A _[M][j][ due to we have][ a][i][ in the gradient update which can be positive or negative). After the]_
second step the weights get improved and well-aligned with the ground-truth (with cosine similarity[P]
_> 0.99)._

Table 1 shows the results for different methods. Recall that the Cos Similarity metric is
max{i∈[2m]} cos(wi, _j∈A_ _[M][j][)][, which reports the cosine value of the closest one. One Step refers]_
to the method where we take the neurons after one gradient step, freeze their weights, and train a
classifier on top; similar for Two Step. One Step gets test accuracy about 52%, while Two Step gets

[P]
accuracy about 100%. This demonstrates that while some effective feature emerge in the first step,
they need to be improved in the second step for accurate prediction. NTK, random feature, One Step
all failed, while Network and Two Step can achieve 100% test accuracy. Network w/o structure refers
to training the network on data without the input structure. It overfits the training dataset with 52%
test accuracy.


-----

E.1.2 INTERVAL LABELING

Figure 7: Test accuracy on simulated data under interval labeling with or without input structure.

Figure 8: Visualization of the weights wi’s after initialization/one gradient step/two gradient steps
in network learning under interval labeling. The red star denotes the ground-truth _j_ _A_ _[M][j][; the]_
_∈_
orange star is − [P]j∈A _[M][j][. The red dots are the weights closest to the red star after two steps; the]_

orange ones are for the orange star. [P]

|Model|Network NTK RF One Step Two Step Network w/o structure|
|---|---|
|Train Acc (%) Test Acc (%) Cos Similarity|100.0 100.0 76.4 44.1 100.0 100.0 100.0 100.0 73.2 41.0 100.0 100.0 1.00 NA 0.153 0.901 0.994 0.965|



Table 2: Interval labeling results in six methods.

**Setting. We also tried interval function, where y = 1 if** _i_ _A_ _ϕ[˜]i is in the range [t1, t2] with t1 = 20_

_∈_
and t2 = 30, otherwise y = −1. We use d = 500, D = 100, k = 30. The _ϕ[˜]i’s are independent, and_
Pr[ϕ[˜]i = 1] = 2/3 for any i ∈ _A, and Pr[ϕ[˜]i = 1] = 1[P]/2 otherwise. When the input structure is_
removed, we set Pr[ϕ[˜]i = 1] = 1/2 for all i’s.

The network and training again follows Section 3 with a network size m = 100 and the training time
_T = 200 steps._

**Verification of the Main Results. Figure 7 shows that network learning learns the fastest, NTK**
learns slower but reaches similar test accuracy, while random feature can only reach a decent but
lower accuracy. This is because for such simpler labeling functions, fixed feature methods can still
achieve good performance (note that the lower bound does not hold for such a case), while the
performance depends on what fixed features to use.

Furthermore, when the input structure is removed, the methods still get similar (or only slightly
worse) performance as with input structure. This shows that when the labeling function is simple, the


-----

help of the input structure for learning may not be needed. In the experiments on real data, we will
show that when the input structure is changed, it indeed leads to lower performance which suggests
that the labeling function in practice is typically more complicated than this interval labeling setting,
and the help of the input structure is significant for learning.

**Feature Learning in Networks. Figure 8 shows the phenomenon of feature learning similar to that in**
the parity labeling setting. Table 2 shows the test accuracy of six different methods. Random feature
and One Step failed, while Network, NTK and Two Step succeed showing that interval labeling
setting is a simpler case than parity labeling setting.

E.2 MORE SIMULATION RESULT IN VARIOUS SETTINGS

We show the robustness of our simulation results by studying the learning behaviors in a variety of
settings including different sample size, input data dimension and class imbalance. We reuse the
same setting as the simulation in the main text (details in E.1.1), vary different parameters, and report
the accuracy, the cosine similarities between the learned weights, and the visualization of the neuron
weights.

E.2.1 VARYING INPUT DATA DIMENSION

In the simulation experiments in the main text, the input data dimension d is 500. Here we change
the input data dimension to 100 and 2000. All other configurations follow E.1.1.

**Verification of the Main Results. Figure 9 shows that our claim is robust under different input**
data dimensions. The performance of network learning is superior over NTK and random feature
approaches on inputs with structure, and on inputs without structure, all three methods fail.

(a) d = 100 (b) d = 2000

Figure 9: Test accuracy on simulated data under different input data dimensions.

|d = 100|Network NTK RF One Step Two Step Network w/o structure|
|---|---|
|Train Acc 100.0 83.1 78.9 53.0 100.0 100.0 Test Acc 100.0 81.5 78.3 51.1 100.0 51.0 Cos Similarity 1.000 NA 0.354 0.967 1.000 0.331||


|d = 100 Train Acc Test Acc Cos Similarity|Network NTK RF One Step Two Step Network w/o structure 100.0 83.1 78.9 53.0 100.0 100.0 100.0 81.5 78.3 51.1 100.0 51.0 1.000 NA 0.354 0.967 1.000 0.331|
|---|---|
|d = 2000|Network NTK RF One Step Two Step Network w/o structure|
|Train Acc Test Acc Cos Similarity|100.0 75.6 80.0 50.22 100.0 100.0 100.0 75.4 77.0 50.01 100.0 52.5 0.998 NA 0.056 0.560 0.998 0.309|



Table 3: Results of six methods for different input data dimensions. The cosine similarity is computed
between the ground-truth _j_ _A_ _[M][j][ and the closest neuron weight.]_

_∈_

**Feature Learning in Networks.[P]** Figure 10 visualizes the neuron weights. It shows similar results to
that in E.1.1: the weights gets updated to to the effective feature in the first two steps, forming clusters.


-----

Figure 10: Visualization of the weights wi’s in early steps under different input data dimensions.
Upper row: input data dimension d = 100; lower row: d = 2000.

Table 3 shows some quantitative results. In particular, the average cosine similarities between neuron
weights and the effective features after two steps are close to 1, showing that they match the effective
features.

E.2.2 VARYING CLASS IMBALANCE RATIO

The experiments in the main text has 25000 training samples for each class. Here we keep the total
sample size 50000 but use different class imbalance ratios, which is the class −1 sample size divide
by the total sample size.

**Verification of the Main Results. Figure 11 shows that our claim is robust under different class**
imbalance ratios. The results are similar to those for balanced classes, except that NTK becomes less
stable.

(a) Negative class ratio = 0.8 (b) Negative class ratio = 0.9

Figure 11: Test accuracy on simulated data under different negative class ratios.

**Feature Learning in Networks. Figure 12 visualizes the neurons’ weights. Again, the observation**
is similar to that for balanced classes. Table 4 shows some quantitative results which are also similar
to those for balanced classes.


-----

Figure 12: Visualization of the weights wi’s in early steps under different class imbalance ratios.
Upper row: negative class ratio 0.8; lower row: 0.9.

|ratio = 0.8|Network NTK RF One Step Two Step Network w/o structure|
|---|---|
|Train Acc 100.0 62.9 72.7 78.3 100.0 100.0 Test Acc 100.0 82.7 70.4 75.7 100.0 61.7 Cos Similarity 0.999 NA 0.293 0.950 0.999 0.218||


|ratio = 0.8 Train Acc Test Acc Cos Similarity|Network NTK RF One Step Two Step Network w/o structure 100.0 62.9 72.7 78.3 100.0 100.0 100.0 82.7 70.4 75.7 100.0 61.7 0.999 NA 0.293 0.950 0.999 0.218|
|---|---|
|ratio = 0.9|Network NTK RF One Step Two Step Network w/o structure|
|Train Acc Test Acc Cos Similarity|100.0 84.0 73.6 92.3 100.0 100.0 100.0 81.7 72.4 89.2 100.0 71.8 0.997 NA 0.296 0.956 0.997 0.286|



Table 4: Results of six methods under different negative class ratios.

E.2.3 VARYING SAMPLE SIZE

Here we change the sample size 50000 in Section E.1.1 to be 25000 and 10000. For sample size
25000, we observe similar results. For sample size 10000, we observe over-fitting (test accuracy
much lower than train accuracy). Therefore, for sample size 10000 we reduces the size of the network
(i.e., number of hidden neurons) from m = 300 to m = 50.

**Verification of the Main Results. Figure 13 shows that our claim is robust under different sample**
sizes. In particular, the network learning still outperforms the NTK and random feature approaches
on structured inputs.

|n = 25000|Network NTK RF One Step Two Step Network w/o structure|
|---|---|
|Train Acc 100.0 84.0 78.6 50.6 100.0 100 Test Acc 100.0 84.1 74.7 50.0 100.0 50.2 Cos Similarity 0.997 NA 0.105 0.851 0.997 0.230||


|n = 25000 Train Acc Test Acc Cos Similarity|Network NTK RF One Step Two Step Network w/o structure 100.0 84.0 78.6 50.6 100.0 100 100.0 84.1 74.7 50.0 100.0 50.2 0.997 NA 0.105 0.851 0.997 0.230|
|---|---|
|n = 10000|Network NTK RF One Step Two Step Network w/o structure|
|Train Acc Test Acc Cos Similarity|100.0 73.9 71.6 50.7 100.0 100.0 100.0 75.0 74.3 50.3 100.0 52.2 0.995 NA 0.096 0.974 0.994 0.176|



Table 5: Results of six methods for different sample size.

**Feature Learning in Networks. Figure 14 and Table 5 show that the phenomenon of feature learning**
for different samples is similar to that in E.1.1.


-----

(a) n = 25000 (b) n = 10000

Figure 13: Test accuracy on simulated data under different sample sizes n.

Figure 14: Visualization of the weights wi’s in early steps under different sample sizes. Upper row:
sample size 25000; lower row: 10000.

E.3 EXPERIMENTS ON MORE DATA GENERATION MODELS

In this section we consider some additional data distributions and run the simulation experiments,
in particular, focusing on the feature learning phenomenon. Note that our analysis is for the setting
where the input distributions have structure revealing some information about the labeling function.
(More precisely, the labeling function is specified by A and P, while the input distribution also
depends on them.) We therefore consider two other data generation mechanisms where the labeling
function also has connections to the input distributions.

E.3.1 HIDDEN REPRESENTATION LABELING

Here we consider the following data model: first uniformly at random select _ϕ[˜]A from a set of binary_
vectors, and assign label 1 to some and -1 to others; sample irrelevant patterns _ϕ[˜]_ _A uniformly at_
_−_
random; generate the input x = M _ϕ[˜]. We randomly select 50 binary vectors for each label, with_
_d = 500, D = 250, k = 50, po = 1/2._

This is a generalization of the distribution D[(1)], a component in the distribution of our simulation
experiments (see the proof of Theorem 2 for details). Recall the definition of D[(1)]: _ϕ[˜]A is uniform_
on only two values [+1, . . ., +1] and [0, . . ., 0], and uniform over irrelevant patterns; the value


-----

[+1, . . ., +1] corresponds to one class and [0, . . ., 0] correspond to another class. Our data model
here generalizes D[(1)] to more than 2 values.

The visualization is shown in Figure 15. We can observe similar feature learning phenomena, and the
neuron weights are updated to form clusters.

Figure 15: Visualization of the weights wi’s after initialization/one gradient step/two gradient steps
in network learning under hidden representation labeling.

E.3.2 TWO-LAYER NETWORKS ON MIXTURE OF GAUSSIANS

To further support our intuition of feature learning, we run experiments on mixture of Gaussians.

**Data.** Let X = R[d] be the input space, and Y = {±1} be the label space. Suppose M ∈ R[d][×][k] is
an dictionary with k orthonormal columns. Let εi, i = 1, . . ., k be i.i.d symmetric Bernoulli random
variables, and g ∼N (0, σr[2] _kd_ [I][d][)][. Then we generate the input][ x][ and class label][ y][ by:]


_εi_ (290)
_i=1_

Y


_x =_


_εiM:i + g,_ _y =_
_i=1_

X


In this case, 2[k] Gaussian clusters will be created. The centers of the Gaussian clusters _i=1_
lie on the vertices of a hyper cube, and the label of each Gaussian cluster is determined by the parity[±][M][:][i]
function on the vertices of the hyper cube.

[P][k]

Note that the labeling function is roughly equivalent to a network: y = _i=1_ _[a][i][ReLU][(][⟨][c][i][, x][⟩][)][ where]_
_ci’s are the Gaussian centers, and ai_ 1 for Gaussian components with label 1 and ai 1 for
those with label -1. _∝_ _∝−_

[P][n]

**Setting. We then train a two-layer network with m = 800 hidden neurons on data sets generated as**
above with different chosen k’s and d’s. The training follows typical practice (not the hyperparameters
in our analysis). In this setting, we expect the neural network to learn the effective features: the
directions of Gaussian cluster centers.

**Result.** We run experiments with different settings. The parameters are shown in Table 6. From
Figure 16 we can see that some neurons learn the directions of Gaussian centers, and each Gaussian
center is covered by some neurons, which matches our expectation.

|Parameters|d k Number of Clusters σ r|
|---|---|
|Experiment 1 Experiment 2 Experiment 3|100 4 16 1 25 4 16 0.7 100 5 32 1|



Table 6: Gaussian mixture setting.

E.4 REAL DATA: FEATURE LEARNING IN NETWORKS

We take the subset of MNIST (Deng, 2012) with labels 0/1, CIFAR10 (Krizhevsky, 2012) with labels
airplane/automobile and SVHN (Netzer et al., 2011) with labels 0/1, and train a two-layer network


-----

(a) Experiment 1 with epoch 0/50/80

(b) Experiment 2 with epoch 0/30/50

(c) Experiment 3 with epoch 0/50/80

Figure 16: Visualization of the weights wi’s (blue dots) and Gaussian centers (red for positive labeled
clusters and orange for negative labeled clusters).

Figure 17: Visualization of the neurons’ weights in a two-layer network trained on the subset of
MNIST data with label 0/1. The weights gradually form two clusters.

with m = 50. We use traditional weight initialization method (random Gaussian) and training method
(SGD with momentum = 0.95 without regularization) in this section, for our purpose of investigating
the training dynamics in practice.

Then we visualize the neurons’ weights following the same method in the simulation. Figure 17,
Figure 18 and Figure 19 show a similar feature learning phenomenon: effective features emerge after
a few steps, and then get improved to form clusters. This shows the insights obtained on our learning
problems are also applicable to the real data.


-----

Figure 18: Visualization of the neurons’ weights in a two-layer network trained on the subset of
CIFAR10 data with label airplane/automobile. The weights gradually form two clusters.

Figure 19: Visualization of the neurons’ weights in a two-layer network trained on the subset of
SVHN data with label 0/1. The weights gradually form four clusters.

|Col1|cos(v, v¯) cos(v, v¯) cos(v, v¯) cos(v, v ) cos(v, v ) cos(v, v ) 1 2 3 1 2 1 3 2 3|
|---|---|
|ResNet(128) ResNet(256)|0.9727 0.8655 0.6549 0.7454 0.5083 0.6533 0.8646 0.9665 0.9121 0.7087 0.6919 0.9135|



Table 7: Cosine similarities between the gradients in the early steps. We choose the neuron weight closest to the average weight of the green cluster at the end of the training (in Figure 20 for ResNet(128)
and Figure 21 for ResNet(256)). We record the gradients of the first 30 steps and divide them to
three trunks of 10 steps evenly and sequentially. For the three trunks, we get the average gradients
_v1, v2, v3. We calculate their cosine similarities to their average ¯v = (v1 + v2 + v3)/3 and those_
between them.

E.4.1 CNNS ON BINARY CIFAR10: FEATURE LEARNING IN NETWORKS

**Setting. We use ResNet(m), which is a ResNet-18 convolutional neural network (He et al., 2016)**
with m filters in the first residual block. It is obtained by scaling the number of filters in each block
proportionally from the standard ResNet-18 network which is ResNet(64). We use ResNet(128)
and ResNet(256) in this experiment. We train our model on Binary CIFAR10 (Krizhevsky, 2012)
with labels airplane/automobile for 20 epochs. The final test accuracy of ResNet(128) is 95.75%
and that of ResNet(256) is 93.8%.

**Results.** Figure 20 visualizes the filters’ weights of different residual blocks in ResNet(128) at
Epoch 0, 3, and 20, and Figure 21 shows those in ResNet(256). They show that feature learning
happens in the early stage, and show that there are some clusters of weights (e.g., the red and green
points). These colored points are selected at Epoch 20. We first visualize the weights at Epoch 20,
and then hand pick the points that roughly form two clusters (i.e., the points in the same cluster are
close to each other while those in different clusters are far away). We assign red and green colors to
the two clusters at Epoch 20, and then assign these weights with the same color in Epoch 0 and 3.
Finally, we compute the cosine similarities and show that the hand picked points are indeed roughly
clusters in the high-dimension.

In particular, we have the following three observations.


-----

(a) Residual block 1: (Green: 0.6152, Red: 0.6973, Two Centers: -0.7245)

(b) Residual block 2: (Green: 0.5528, Red: 0.6000, Two Centers: -0.7509)

(c) Residual block 3: (Green: 0.4260, Red: 0.5006, Two Centers: -0.5099)

(d) Residual block 4: (Green: 0.5584, Red: 0.5697, Two Centers: -0.9074)

Figure 20: Visualization of the normalized convolution weights in all Residual block of ResNet(128)
trained on the subset of CIFAR10 data with labels airplane/automobile. We show the weights after
0/3/20 epochs in network learning. The weights gradually form two clusters in all Residual blocks.
We also report average cosine similarity between the green/red points in the clusters to their centers
and cosine similarity between two cluster centers as (Green, Red, Two Centers).

First, we can see that the filter weights change significantly during the early stage of the training,
indicating feature learning happens in the early stage: the change between Epoch 0 and Epoch 3 is
much more significant than that between Epoch 3 and Epoch 20.

Second, we can also verify that the feature learning is guided by the gradients: the gradients of a
filter in the early gradient steps point to similar directions (and thus the updated filter will learn this
direction). More precisely, for a selected filter, we average the gradients every 10 gradient steps (so to
reduce the variance due to mini-batch), and get v1, v2 and v3 for the first 30 steps and compute their
cosine similarities and those to their average. Table 7 shows the results. In general the similarities


-----

(a) Residual block 1: (Green: 0.7065, Red: 0.6551, Two Centers: -0.7875)

(b) Residual block 2: (Green: 0.6599, Red: 0.5299, Two Centers: -0.9004)

(c) Residual block 3: (Green: 0.5193, Red: 0.6267, Two Centers: -0.9258)

(d) Residual block 4: (Green: 0.7386, Red: 0.6839, Two Centers: -0.9740)

Figure 21: Visualization of the normalized convolution weights in all Residual block of ResNet(256)
trained on the subset of CIFAR10 data with labels airplane/automobile. We show the weights after
0/3/20 epochs in network learning. The weights gradually form two clusters in all Residual blocks.
We also report average cosine similarity between the green/red points in the clusters to their centers
and cosine similarity between two cluster centers as (Green, Red, Two Centers).

are high indicating they point to similar directions. (Note that a similarity of 0.6 is regarded as very
significant as the filters are in a high dimension of 3 × 3 × 1024 = 9216).

Third, we also observe some clustering effect of the filter weights, though not as significant as in our
simulations. For example, in the red and green clusters in Figure 20(a) for the first residual block, the
average cosine similarity for filter weights in the red cluster is about 0.62 and that for the green is
about 0.7, while the cosine similarity between the two clusters’ centers is about -0.72. This shows
significant similarities within the cluster while difference between clusters.


-----

Note that the clustering is less significant than our simulation experiments. This is because practical
data have more patterns (i.e., effective feature directions) to be learned than our synthetic data, and
also the practical network is not as overparameterized as in our simulation. Then filters are likely
to learn different patterns (or their mixtures) without forming significant clusters. The results of
ResNet(256) show more significant clustering than ResNet(128), which supports our explanation.
On the other hand, we emphasize that the key insight of our analysis is that the gradient guides
the learning of effective features in the early stage of training (rather than the clustering), which is
verified as discussed above.

E.5 REAL DATA: THE EFFECT OF INPUT STRUCTURE

To study the influence of the input structure, we propose to keep the labeling function unchanged,
vary the input distributions, and exam the change of the loss surface and the training dynamics. We
first describe the detailed experimental methodology, which allows us to generate data with similar
labeling function but different input distributions. Then we perform experiments on the generated
datasets to investigate the change of the learning due to the change in the input distributions, and
present the experimental results. Finally, we also perform experiments to verify the intuition behind
our experimental method.

E.5.1 EXPERIMENTAL METHODOLOGY

We consider the following experimental method. Given an original dataset L = {(xi, yi)}i[n]=1 [(e.g.,]
CIFAR10) and an unlabeled dataset U = {x˜i}i[m]=1 [from a proposed distribution][ P][U][ (e.g., Gaussians),]
first extend the labeling function of L to U, giving synthetic labels ˜yi to ˜xi. Then train a neural
network on the union of and the synthetic data = (˜xi, ˜yi) _i=1[. By investigating the new]_
_L_ _LU_ _{_ _}[m]_
training dynamics, in particular the difference on the original part and the synthetic part, we
_L_ _LU_
can see the effect of the input structure. The original dataset should be from real-world data, since
one of our goals is to compare them with synthetic data, and identify the properties of real-world data
important for the success of learning.

A natural idea is to first learn a powerful network f (x) (called the teacher) on L to approximate
the true labeling function, then apply f on U to generate synthetic labels, and finally train another
network (called the student) on the synthetic data and original data. However, we found that naïvely
implementation of this idea fails miserably: the support of L and U can be typically different, and the
powerful network learned over L can have entirely different behavior on U. Therefore, we need to
control the size of the teacher f so that the labeling on U has similar complexity as that on L. For
our purpose, we can define the complexity of the labeling on L as the minimum size of the teacher
achieving an approximation error ϵ for a chosen ϵ, if the ground-truth data distribution of L is known.
However, given only limited data, we cannot faithfully estimate the needed size of the teacher, and
need to take into account the variance introduced by the finite data.

Our key idea is to use the U-shaped curve of the bias-variance trade-off and select the size of the
teacher at the minimum of the U-shaped curve. Since recent works (Belkin et al., 2019; Nakkiran
et al., 2020) show that neural networks can have a double descent curve for the error v.s. model
complexity, we thus plot the double descent curve, and find the minimum in the classical regime
(corresponding to the traditional U-shape curve).

Our method is designed based on the following two reasons. First, on the U-shaped curve, the
complexity of the network is still roughly controlled by that of the number of parameters. The
local minimum of the U-shaped curve is a good measurement of the complexity of the data. If the
ground-truth is much more complicated than the teacher, then increasing the teacher’s size leads to a
significant decrease in the approximation error (bias) compared to a small increase in the variance,
that is, we will be on the left-hand side of the U-shaped. In contrast, on the right-hand side of the
U-shaped, increasing the teacher’s size leads to a small decrease in the bias compared to a significant
increase in the variance. That is, the complexity of the ground-truth is comparable to or lower than
the teacher. So the local minimum approximates the complexity of the ground-truth labeling function.

Second, the local minimum point is chosen to get the best approximation of the true labels. This
helps to maintain the labeling from the real-world data and thus helps our investigation on the input,
since too drastic change in the labeling can affect the training.


-----

We note that the method is not perfect. First, the teacher at the local minimum of U-shape may not
have very high accuracy, especially on more complicated data. To alleviate this, we also use the
teacher to give synthetic labels yi[′] [to][ x][i][ in][ L][, and train the student network on][ L][′][ =][ {][(][x][i][, y]i[′][)][}][n]i=1[.]
Though this introduces some differences from the original labels, it is acceptable for our purpose of
studying the inputs. Furthermore, ensuring the consistency of the labels on the original input in L and
_U is important in our experiments. Second, the measurement is an approximation due to variance._
Since only limited labeled data is available, it’s important and necessary to calibrate the measurement
w.r.t. the level of variance on the given dataset.

**Method Description. Algorithm 1 presents the details. For a fixed network architecture for the**
teacher f, it first varies the network size and plots the double descent curve. Then it selects the
local minimum in the classic regime of U-shape and trains the teacher with the corresponding size.
In practice, we observed that the teacher might have unbalanced probabilities for different classes
on U if its training does not take into account U. Therefore, we propose the following heuristic
regularization using x ∈U, where λ is a regularization weight, and f (x) is the probabilities over
classes given by the teacher:
_R(x) = R1(x) + λR2(x)_ (291)

_i_ _[f]_ [(][x][)][j] _i_ _[f]_ [(][x][)][j]

_R1(x) =_ ln (292)

_m_ _m_

_j_ P P 

X

_R2(x) =_ (f (x)j ln(f (x)j)). (293)
_−_ _m[1]_

_i_ _j_

X X

Here, R1(x) guarantees that each kind of label has the same average probability to be generated, and
_R2(x) pushes the probability away from uniform to avoid the case that the class probabilities for_
each data point converge to uniform.

**Algorithm 1 Learning the teacher network to generate synthetic labels for studying the effect of the**
input structure

**Input: teacher architecture f**, labeled dataset L = {(xi, yi)}i[n]=1[, unlabeled dataset][ U][ =][ {]x[˜]i}i[m]=1[.]

Let i to be the size of f, fi to be the teacher of size i.
**for i = 1 to n do**

Train fi on L and let li denote the test loss

**end for**
Plot li v.s. i, identify the classical regime, and the size it corresponding to the local minimum in
classical regime.
Train fit on L with a regularizer R(x) on U defined in (291).
**Output: fit**


E.5.2 EXPERIMENTAL RESULTS

**Network models. Here we use one-hidden-layer fully-connected networks with m hidden units and**
quadratic activation functions. The network is denoted as FC(m). We use ResNet(m), which is a
ResNet-18 convolutional neural network (He et al., 2016) with m filters in the first residual block. It
is obtained by scaling the number of filters in each block proportionally from the standard ResNet-18
network which is ResNet(64).

**Datasets. We use MNIST (Deng, 2012), CIFAR10 (Krizhevsky, 2012) and SVHN (Netzer et al.,**
2011) as L, and use Gaussian and images in Tiny ImageNet (Le & Yang, 2015) as U. We generate
the mixture data, where the fraction of the unlabeled data is denoted as α.

**Setup. We first use Algorithm 1 on the labeled data L and the unlabeled data U to get a synthetic**
labeling function (the teacher network) and then use it to give synthetic labels on a mixture of inputs
from L and U. For MNIST, the teacher network learned is FC(9), where the number of the hidden
units is determined by Algorithm 1. See empirical verification in Figure 26. For CIFAR10 and SVHN,
the teacher networks are ResNet(5) and ResNet(2), respectively, as determined by our method.
The student network for MNIST is FC(9), and those for CIFAR10 and SVHN are ResNet(9) and
ResNet(8), respectively. Finally, we train the student networks on these new datasets with perturbed
input distributions.


-----

(a) (b) (c)

Figure 22: Test accuracy at different steps for an equal mixture α = 0.5 of Gaussian inputs with data:
(a) MNIST, (b) CIFAR10, (c) SVHN.

(a) (b)

Figure 23: Test accuracy at different steps for an equal mixture α = 0.5 of Tiny ImageNet inputs
with data: (a) CIFAR10, (b) SVHN.

(a) α = 0.25 (b) α = 0.50 (c) α = 0.75

Figure 24: Test accuracy at different steps for varying mixture α of Gaussian inputs with CIFAR10.

Figure 22 shows the results on an equal mixture of data and Gaussian. It presents the test accuracy
of the student on the original data part, the Gaussian part, and the whole mixture. For example,
for CIFAR10, the test accuracy on the whole mixture is lower than that of training on the original
CIFAR10, showing that the input structure indeed has a significant impact on the learning. Furthermore, the network learns well over the CIFAR10 part (with accuracy similar to that on the
original data) but learns slower with worse accuracy on the Gaussian part. This suggests that the
CIFAR10 input structure is still helping the network to learn effective features. While the results on
MNIST+Gaussian do not show a significant trend (possibly because the tasks there are simpler), the
results on SVHN+Gaussian show similar significant trends as CIFAR10+Gaussian.

Figure 24 shows the results when we vary the fraction of the Gaussian data α. We observe that the
test accuracy curve on the original part and that on the synthetic part have roughly the same trend for
different α as before, further verifying our insights.


-----

Figure 23 shows the results when mixed with Tiny ImageNet data instead of Gaussians. It shows a
similar trend, while the performance on the Tiny ImageNet part is higher than that on the Gaussian
part. This suggests that compared to Gaussians, the Tiny ImageNet data has helpful input structures,
though not as helpful as that on the original data for learning the particular labeling.

E.5.3 LARGER NETWORK ON MNIST FOR CHECKING THE EFFECT OF INPUT STRUCTURE

Here we perform the experiment on MNIST as in E.5.2, but for a network with m = 50 hidden
neurons rather than m = 9. Figure 25 shows similar results as those for m = 9: the learning on the
MNIST input part is faster and better than that on the Gaussian input part. The separation between
the two is actually more significant than that for m = 9. This then also supports our insight about the
effect of input structures.

Figure 25: Test accuracy at different steps for an equal mixture α = 0.5 of Gaussian inputs with
MNIST, where m = 50.

E.5.4 EMPIRICAL VERIFICATION OF OUR METHOD

(a) Teacher’s double descent curve (b) Student’s curve when Teacher=FC(9)

(c) Student’s curve when Teacher=FC(50) (d) Student’s curve when Teacher=FC(500)

Figure 26: Double descent curves of the students trained on data with synthetic labels (Loss v.s.
Parameter number).


-----

We also perform experiments to verify the intuition behind our methodology, i.e., the method gives a
synthetic labeling function with roughly the same complexity on the original inputs and the injected
inputs. We first use our method on MNIST and samples (of the same size as MNIST) from a Gaussian
to get the teacher FC(9); the double descent curve is in Figure 26(a). Then we train students on
the Gaussian data with synthetic labels from the teacher, and plot the double descent curve for the
students in Figure 26(b). The local minimums of the two U-shapes are roughly the same, matching
our reasoning. Then we also train larger teachers and plot the double descent curve for students on
Gaussian data. Figure 26(c) Teacher size 50. Figure 26(d) Teacher size 500. The local minimum of
the U-shape becomes larger when the teacher gets larger, again matching our reasoning.


-----

F PROVABLE GUARANTEES FOR NEURAL NETWORKS IN A MORE GENERAL
SETTING

This section provides the analysis in a more general setting. We first describe the learning problems,
and then provide the proofs following similar intuitions as for the simpler settings in the main text.

F.1 PROBLEM SETUP

Let X = R[d] be the input space, and Y = {±1} be the label space. Suppose M ∈ R[d][×][D] is a
dictionary with D elements, where each element Mj can be regarded as a pattern. We assume quite
general incoherent dictionary:

**(D) M is µ-incoherent, i.e., the columns of M are unit vectors, and for any i ̸= j, |⟨Mi, Mj⟩| ≤**
_µ/√d._

Note that the setting in the main text corresponds to µ = 0.

Let _ϕ[˜] ∈{0, 1}[D]_ be a hidden vector that indicates the presence of each pattern, and D ˜ϕ [a distribution]
for _ϕ[˜]. Let A ⊆_ [D] be a subset of size k corresponding to the class relevant patterns. Let P ⊆ [k].
We first sample _ϕ[˜] from D ˜ϕ[, and then generate the input][ ˜]x and the class label y from_ _ϕ, A, P[˜]_ by:

+1, if _i_ _A_ _ϕ[˜]i_ _P,_
_x˜ = M_ _ϕ[˜] + ζ,_ _y =_ _∈_ _∈_ (294)

1, otherwise

−

[P]

where the Gaussian noise ζ ∼N (0, σζ[2][I][d][×][d][)][ is independent from][ ˜]ϕ. Note that the setting in the main
text corresponds to σζ = 0.

We allow general D ˜ϕ [with the following assumptions:]


**(A1) The patterns in A are correlated with the labels: for any i ∈** _A, for v ∈{±1} let γv =_
E[yϕ[˜]i _y = v], then γ := (γ+1 + γ_ 1)/2 > 0.
_|_ _−_

**(A2) The patterns outside A are independent of the patterns in A.**

Note that we allow imbalanced classes. Let pmin := min(Pr[y = −1], Pr[y = +1]). If the classes
are balanced, then the assumption (A1) implies the assumption (A1) in the main text, so the setting
here is more general. (A2) is also more general, in particular, allowing dependence between irrelevant
patterns and non-identical distributions for them.

Let D(A, P, D ˜ϕ[)][ denote the distribution on][ (˜]x, y) corresponding to some A, P, and D ˜ϕ[. Given]
parameters Ξ = (d, D, k, γ, po, µ, σζ), the family FΞ of distributions for learning is the set of all
_D(A, P, D ˜ϕ[)][ with][ A][ ⊆]_ [[][D][]][,][ P][ ⊆] [[][k][]][, and][ D][ ˜]ϕ [satisfying the above assumptions.]

One special case is the mixture of two Gaussians.

_Example. Suppose M has one single column v, and y = +1 if_ _ϕ[˜] = 1 and y = −1 otherwise. Then_
the data distribution is simply a mixture of two Gaussians: ˜x ∼ _[v]2_ [+][ N] [(][y][ v]2 _[, σ]ζ[2][I][d][×][d][)][.]_

F.1.1 NEURAL NETWORK LEARNING


Again, we will normalize the data for learning: we first computedi=1[(˜]xi E[˜xi])[2] = _j_ [D] [Var][(˜]ϕj) + dσζ[2] [is the variance of the data, and then train on] x = (˜x − E[˜x])/σ˜ where ˜[ (]σ[x, y][2] :=[)][.]
_−_ _∈_

This is equivalent to settingP _ϕ = (ϕ[˜] −_ E[ϕ[˜]])/σ˜ and generating x = Mϕ + ζ/σζ. For (˜x, y) from D
and the normalized (x, y[P]), we will simply say (x, y) .
_∼D_

The learning will be the same as that in the main text, except the following. We will use a small
_σw[2]_ [= ˜]σ[2]/poly(Dm). And we will use a weighted loss to handle the imbalanced classes in the first
two steps for feature learning, and then use the unweighted loss in the remaining steps. Formally, the
weighted loss is:
_L[α]_ (x,y)[[][α][y][ℓ][(][y, g][(][x][;][ ξ][))]][,] (295)
_D[(][g][;][ σ][ξ][) =][ E]_

where the class weights αv = 2 Pr[1y=v] [for][ v][ ∈{±][1][}][.]


-----

F.2 MAIN RESULT

In this setting, we have the following theorem:

**Theorem 34. Set**

_σ_
_η[(1)]_ = _[γ][2][p][min]_ [˜] _, λ[(1)]a_ = 0, λ[(1)]w [= 1][/][(2][η][(1)][)][, σ]ξ[(1)] = 1/k[3][/][2], (296)

_km[3]_

_η[(2)]_ = 1, λ[(2)]a = λ[(2)]w [= 1][/][(2][η][(2)][)][, σ]ξ[(2)] = 1/k[3][/][2], (297)


_k[2]_ _k[3]_
_η[(][t][)]_ = η = _a_ = λ[(]w[t][)] [=][ λ][ ≤] _ξ_ = 0, for 2 < t _T._ (298)

_Tm[1][/][3][, λ][(][t][)]_ _σm˜_ [1][/][3][, σ][(][t][)] _≤_

_Dmd_

_For any δ_ (0, O(1/k[3])), if µ _O(√d/D), σζ_ _O(min_ 1/σ,˜ ˜σ/√d ), k = Ω log[2][ ] _δγpmin_ _,_
_∈_ _≤_ _≤_ _{_ _}_

_m_ max Ω(k[4]), D, d _, then we have for any_ Ξ, with probability at least 1 _δ, there exists_
_≥_ _{_ _}_ _D ∈F_ _−_
_t ∈_ [T ] such that

8
_k_
Pr[sign(g[(][t][)](x)) = y] _L_ (g[(][t][)]) = O _._ (299)
_̸_ _≤_ _D_ _m[2][/][3][ +][ k]m[3][T][2][ +][ k][2][m]T_ [2][/][3]
 

_Consequently, for any ϵ ∈_ (0, 1), if T = m[4][/][3], and m ≥ max{Ω(k[12]/ϵ[3][/][2]), D}, then

Pr[sign(g[(][t][)](x)) = y] _L_ (g[(][t][)]) _ϵ._ (300)
_̸_ _≤_ _D_ _≤_

The rest of the section is devoted to the proof of this theorem.

F.3 NOTATIONS

Recall some notations that we will use throughout the analysis.

For a vector v and an index set I, let vI denote the vector containing the entries of v indexed by I,
and v _I denote the vector containing the entries of v with indices outside I._
_−_

Let ρ := M _[⊤]M_ . Then we have ρjj = 1 for any j, and |ρjℓ| ≤ _µ/√d for any j ̸= ℓ._

By initialization, wi[(0)] for i [m] are i.i.d. copies of the same random variable w[(0)] (0, σw[2] _[I][d][×][d][)][;]_
_∈_ _∼N_
similar for a[(0)] and b[(0)]. Let σϕj[2] [:=][ p][o]j[(1][ −] _[p][o]j[)][/]σ[˜][2]_ denote the variance of ϕℓ for ℓ _̸∈_ _A, where_

_poj = Pr[ϕ[˜]j = 1]. Let po be the value such that with probability 1 −_ exp(−Ω(k)), _j̸∈A_ _ϕ[˜]j ≤_

_pprobability.o(D −_ _k) for some po ∈_ [0, 1]. That is, po is an upper bound on the density of _ϕ[˜][P]j with high_

Let qℓ := ⟨w[(0)], Mℓ⟩. Similarly, define qi,ℓ[(][t][)] [:=][ ⟨][w]i[(][t][)][, M][ℓ][⟩][.]

We also define the following sets to denote typical initialization. For a fixed δ ∈ (0, 1), define


_w[d]_ _w[d]_
_w ∈_ R[d] : qℓ = ⟨w, Mℓ⟩, _[σ][2]2_ _≤∥w[(0)]∥2[2]_ _[≤]_ [3][σ]2[2]


(301)

(302)


_w(δ) :=_
_G_


_σw[2]_ [(][D][ −] _[k][)]_

2 _≤_

max _qℓ_ _σw_
_ℓ_ _|_ _| ≤_


_w[(][D][ −]_ _[k][)]_
_qℓ[2]_ _[≤]_ [3][σ][2]

2

_ℓX̸∈A_

2 log(Dm/δ) _,_

)


_Ga(δ) := {a ∈_ R : |a| ≤ _σa_

_Gb(δ) := {b ∈_ R : |b| ≤ _σb_


2 log(m/δ)}. (303)

2 log(m/δ)}. (304)


-----

F.4 EXISTENCE OF A GOOD NETWORK

We first show that there exists a network that can fit the data distribution.

**Lemma 35. Suppose** _√kµd_ _poσ˜D_ 161 _[.]_ _For any_ Ξ, there exists a network g[∗](x) =

_n_ _≤_ _D ∈F_
_i=1_ _[a]i[∗][σ][(][⟨][w]i[∗][, x][⟩]_ [+][ b]i[∗][)][ which satisfies]
P

1

Pr Ω _._
(x,y)∼D[[][yg][∗][(][x][)][ ≤] [1]][ ≤] [exp(][−][Ω(][k][)) + exp] _−_ _σζ[2][(][k][ +][ k][2][µ/]√d)_ !!

_Furthermore, the number of neuronsσ˜_ _j_ _A_ _[M][j][/][(8][k][)][, and][ |⟨][w]i[∗][, x][⟩]_ [+][ b]i[∗] _n = 3(k + 1), |a[∗]i_ _[| ≤]_ [64][k,][ 1][/][(64][k][)][ ≤|][b]i[∗][| ≤] [1][/][4][,][ w]i[∗] [=]

_∈_ _[| ≤]_ [1][ for any][ i][ ∈] [[][n][]][ and][ (][x, y][)][ ∼D][.]

_Consequently, if furthermore we have kµ/√d < 1 and σζ < 1/k, then_

[P]


Pr
(x,y)∼D[[][yg][∗][(][x][)][ ≤] [1]][ ≤] [exp(][−][Ω(][k][))][.]

_Proof of Lemma 35. Let w = ˜σ_ _j∈A_ _[M][j][ and let][ u][ =][ P]j∈A_ [E][[˜]ϕj]. We have

_w, x_ = ˜[P]σ _Mj, Mϕ_ + _w, ζ/σ˜_ (305)
_⟨_ _⟩_ _⟨_ _⟩_ _⟨_ _⟩_

_jX∈A_

= _ϕj +_ _ρjℓϕℓ_ + ⟨w, ζ/σ˜⟩ (306)

_jX∈A_ _j∈XA,ℓ≠_ _j_

= _ϕ˜j_ _u +_ _ρjℓϕℓ_ + _w, ζ/σ˜_ _._ (307)

_jX∈A_ _−_ _j∈XA,ℓ≠_ _j_ _⟨_ _⟩_

:=ϵx
| {z }

With probability ≥ 1 − exp(−Ω(k)), among all j ̸∈ _A, we have that at most po(D −_ _k) of ϕj are_
(1 − _po)/σ˜, while the others are −po/σ˜, and thus_


_≤_ _√[kµ]d_


_poD_


(308)

_≤_ 16[1] _[.]_


_ρjℓϕℓ_
_j∈XA,ℓ≠_ _j_


Furthermore, ⟨w, ζ⟩∼N (0, σζ[2][∥][w][∥]2[2][)][ and][ ∥][w][∥][2]2 _[≤]_ _σ[˜][2](k + k[2]µ/_


_d), we have_


!!


(309)

(310)


Pr[|⟨w, ζ/σ˜⟩| ≤ 1/16] ≥ 1 − exp

_≥_ 1 − exp


_−Θ_

_−Θ_


_σζ[2][∥][w][∥]2[2][/]σ[˜][2]_


!!


_σζ[2][(][k][ +][ k][2][µ/]_


_d)_


For good data points with ϕ and ζ satisfying the above, we have _ϵx_ 1/8. By Lemma 7,
_|_ _| ≤_


_g1[∗][(][x][) :=]_ _δp_ _µ,4,1/2(_ _w, x_ )

_−_ _⟨_ _⟩_ _−_
_pX∈P_


_δp−µ,4,1/2(⟨w, x⟩)_ (311)
_p̸∈P,X0≤p≤k_


_δp,4,1/2(_ _w, x_ + u) (312)
_⟨_ _⟩_
_p̸∈P,X0≤p≤k_


_δp,4,1/2(_ _w, x_ + u)
_⟨_ _⟩_ _−_
_pX∈P_


_ϕ˜j + ϵx_

  _−_

_j∈A_

[X] 


_ϕ˜j + ϵx_ _._ (313)

 

_j∈A_

[X] 


_δp,4,1/2_
_pX∈P_


_δp,4,1/2_
_p̸∈P,X0≤p≤k_


-----

Then for good data points, we have yg1[∗][(][x][)][ ≥] [1][. Similarly,]


_g2[∗][(][x][) :=]_ _δp_ _µ+1/4,8,1/2(_ _w, x_ )

_−_ _⟨_ _⟩_ _−_
_pX∈P_


_δp−µ+1/4,8,1/2(⟨w, x⟩)_ (314)
_p̸∈P,X0≤p≤k_


_δp+1/4,8,1/2(_ _w, x_ + u) (315)
_⟨_ _⟩_
_p̸∈P,X0≤p≤k_


_δp+1/4,8,1/2(_ _w, x_ + u)
_⟨_ _⟩_ _−_
_pX∈P_


= _δp+1/4,8,1/2_ _ϕ˜j + ϵx_ _δp+1/4,8,1/2_ _ϕ˜j + ϵx_ _._ (316)

  _−_  

_pX∈P_ _j∈A_ _p̸∈P,X0≤p≤k_ _j∈A_

[X]  [X] 

Then for good data points, we have yg2[∗][(][x][)][ ≥] [1][.]

Note that the bias terms in g1[∗] [and][ g]2[∗] [have distance at least][ 1][/][4][, then at least one of them]
satisfies that all its bias terms have absolute value ≥ 1/8. Pick that one and denote it as
_g(x) =_ _i=1_ _[a][i][σ][r][(][⟨][w][i][, x][⟩]_ [+][ b][i][)][. By the positive homogeneity of][ σ][r][, we have]

_n_

[P][n] _g(x) =_ 8kaiσr( _wi, x_ _/(8k) + bi/(8k))._ (317)

_⟨_ _⟩_
_i=1_

X

Since for any good data points, _wi, x_ _/(8k) + bi/(8k)_ 1, then
_|⟨_ _⟩_ _| ≤_


8kaiσ( _wi, x_ _/(8k) + bi/(8k))_ (318)
_⟨_ _⟩_
_i=1_

X


_g(x) =_


where σ is the truncated ReLU. Now we can set a[∗]i [= 8][ka][i][, w]i[∗] [=][ w][i][/][(8][k][)][, b]i[∗] [=][ b][i][/][(8][k][)][, to get our]
final g[∗].

F.5 INITIALIZATION

We first show that with high probability, the initial weights are in typical positions.

**Lemma 36. Suppose Dµ/√d ≤** 1/16. For any δ ∈ (0, 1), with probability at least 1 − _δ −_

2 exp (−Θ(D − _k)) over w[(0)],_

_σw[2]_ _[d/][2][ ≤∥][w][(0)][∥]2[2]_ _[≤]_ [3][σ]w[2] _[d/][2][,]_

_σw[2]_ [(][D][ −] _[k][)][/][2][ ≤]_ _qℓ[2]_ _[≤]_ [3][σ]w[2] [(][D][ −] _[k][)][/][2][,]_

_ℓX̸∈A_


max _qℓ_ _σw_
_ℓ_ _|_ _| ≤_

_With probability at least 1 −_ _δ over b[(0)],_

_b[(0)]_ _σb_
_|_ _| ≤_

_With probability at least 1 −_ _δ over a[(0)],_

_a[(0)]_ _σa_
_|_ _| ≤_


2 log(D/δ).

p

2 log(1/δ).

2 log(1/δ).


_Proof of Lemma 36. The bound on ∥w[(0)]∥2[2]_ [follows from the property of Gaussians.]

Note that q = M _[⊤]w[(0)]_ _∼N_ (0, σw[2] _[ρ][)][ for the matrix][ ρ][ =][ M][ ⊤][M]_ [. We have with probability][ ≥] [1][−][δ/][2][,]

maxℓ _|qℓ| ≤_ 2σw[2] [log][ D]δ [.]
q

For any subset S ⊆ [D], let ρS denote the submatrix of ρ containing the rows and columns
indexed by S. Then qS = M _[⊤]w[(0)]_ _∼N_ (0, σw[2] _[ρ][S][)][.]_ By diagonalizing ρS and then applying Bernstein’s inequality, we have with probability 1 2 exp ( Θ( _S_ _/_ _ρ_ 2), _qS_ 2
_≥_ _−_ _−_ _|_ _|_ _∥_ _∥_ _∥_ _∥[2]_ _[∈]_
(∥ρS∥F[2] _[−]_ _[|][S]4[|]_ [)][σ]w[2] _[,][ (][∥][ρ][S][∥][2]F_ [+][ |][S]4[|] [)][σ]w[2] . By Gershgorin circle theorem, we have
 


_ρ_ 2 1 + ( _S_ 1)µ/
_∥_ _∥_ _≤_ _|_ _| −_


_d ≤_ 17/16.


-----

Similarly, we have

3 15
4 _[|][S][| ≤]_ 16


The bounds on q then follow.


2
17
_|S| ≤∥ρS∥F[2]_ _[≤]_ 16
 


2
_S_
_|_ _| ≤_ 4[5] _[|][S][|][.]_



The bounds on b[(0)] and a[(0)] follow from the property of Gaussians.


**Lemma 37. Suppose Dµ/**


_d ≤_ 1/16. We have:



-  With probability 1 _δ_ 2m exp( Θ(D _k)) over wi[(0)]’s, for all i_ [2m], wi[(0)] _w(δ)._
_≥_ _−_ _−_ _−_ _−_ _∈_ _∈G_

-  With probability ≥ 1 − _δ over b[(0)]i_ _[’s, for all][ i][ ∈]_ [[2][m][]][,][ b]i[(0)] _∈Gb(δ)._

-  With probability ≥ 1 − _δ over a[(0)]i_ _[’s, for all][ i][ ∈]_ [[2][m][]][,][ a]i[(0)] _∈Ga(δ)._

_Proof of Lemma 37. This follows from Lemma 36 by union bound._

F.6 SOME AUXILIARY LEMMAS

The expression of the gradients will be used frequently.

**Lemma 38.**

_∂_

_∂wi_ _L[α]D[(][g][;][ σ][ξ][) =][ −][a][i][E](x,y)∼D_ _[{][α][y][y][I][[][yg][(][x][;][ ξ][)][ ≤]_ [1]][E][ξ]i [I][[][⟨][w][i][, x][⟩] [+][ b][i] [+][ ξ][i] _[∈]_ [(0][,][ 1)]][x][}][,] (319)

_∂_

_∂bi_ _L[α]D[(][g][;][ σ][ξ][) =][ −][a][i][E](x,y)∼D_ _[{][α][y][y][I][[][yg][(][x][;][ ξ][)][ ≤]_ [1]][E][ξ]i [I][[][⟨][w][i][, x][⟩] [+][ b][i] _[∈]_ [(0][,][ 1)]][}][,] (320)

_∂_

_L[α]_ (x,y) _i_ _[σ][(][⟨][w][i][, x][⟩]_ [+][ b][i] [+][ ξ][i][)][}][ .] (321)
_∂ai_ _D[(][g][;][ σ][ξ][) =][ −][E]_ _∼D_ _[{][α][y][y][I][[][yg][(][x][;][ ξ][)][ ≤]_ [1]][E][ξ]

_Proof of Lemma 38. It follows from straightforward calculation._

We also have the following auxiliary lemma for later calculations.

**Lemma 39.**

EϕA {αyy} = 0, (322)
EϕA {|αyy|} = 1, (323)

Eϕj {|ϕj|} = 2σϕj[2] _σ,[˜]_ _for j ̸∈_ _A,_ (324)

EϕA _αyyϕj_ = _[γ]_ (325)
_{_ _}_ _σ ˜_ _[,][ for][ j][ ∈]_ _[A,]_

EϕA _αyyϕj_ (326)
_{|_ _|} ≤_ _σ ˜[1]_ _[,][ for all][ j][ ∈]_ [[][D][]][.]


-----

_Proof of Lemma 39._


EϕA {αyy} =


= _v∈{±X1}_ EϕA {αyy|y = v} Pr[y = v] (327)

= [1] EϕA _y_ _y = v_ (328)

2 _v∈{±X1}_ _{_ _|_ _}_

= 0. (329)

= _v∈{±X1}_ EϕA {|αyy| |y = v} Pr[y = v] (330)

= [1] EϕA _y_ _y = v_ (331)

2 _v∈{±X1}_ _{|_ _| |_ _}_

= 1. (332)


EϕA {|αyy|} =


Eϕj _ϕj_ = = 2σϕj[2] _σ.[˜]_ (333)
_{|_ _|}_ _[| −]_ _[p][o][j][|][(1][ −]_ _[p][o][j]σ˜[) +][ |][1][ −]_ _[p][o][j][|][p][o][j]_

EϕA {αyyϕj} = _v∈{±X1}_ EϕA {αyyϕj| y = v} Pr[y = v] (334)

= [1] EϕA _yϕj_ _y = v_ (335)

2 _v∈{±X1}_ _{_ _|_ _}_


_ϕ˜j −_ E[ϕ[˜]j]


= [1]


(336)


_j_ _j_

= EϕA _y_ _−_ (336)

2 _σ˜_

_v∈{±X1}_ ( )

= [1] _[y][ =][ v]_ (337)

2˜σ [(][γ][+1][ +][ γ][−][1][) =][ γ]σ ˜ _[.]_


EϕA {|αyyϕj|} = _v∈{±X1}_ EϕA {|αvyϕj| |y = v} Pr[y = v] (338)

EϕA _yϕj_ _y = v_ (339)

_≤_ 2[1] _v∈{±X1}_ _{|_ _| |_ _}_

EϕA _yϕj_ _y = v_ (340)

_≤_ 2[1] _v∈{±X1}_ _{|_ _| |_ _}_

(341)

_≤_ _σ ˜[1]_ _[.]_


F.7 FEATURE EMERGENCE: FIRST GRADIENT STEP

We will show that w.h.p. over the initialization, after the first gradient step, there are neurons that
represent good features.

We begin with analyzing the gradients.

**Lemma 40. Fix δ** (0, 1) and suppose wi[(0)] _w(δ), b[(0)]i_ _b(δ) for all i_ [2m]. Let
_∈_ _∈G_ _∈G_ _∈_

2 log(D/δ) _√dσξσw_ 2 log(D/δ)

_ϵe :=_ _[Dσ][w]_ + _, ϵν := ϵe._
pσ˜[2]σξ[(1)] _σσ˜pξ[(1)]_

_If σζ[2][σ]w[2]_ _[d/]σ[˜][2]_ = O(1/k), po = Ω(k[2]/D), k = Ω(log[2](Dmd/δ)), and σξ[(1)] = O(1/k), then


_∂_

_L[α]_ _ξ_ [) =][ −][a]i[(0)]
_∂wi_ _D[(][g][(0)][;][ σ][(1)]_


(342)


_MjTj + ν_
_j=1_

X


_where Tj satisfies:_


-----

-  if j ∈ _A, then |Tj −_ _βγ/σ˜| ≤_ _O(ϵe/σ˜), where β ∈_ [Ω(1), 1] and depends only on wi[(0)], b[(0)]i _[;]_

-  if j ̸∈ _A, then |Tj| ≤_ _O(σϕj[2]_ _[ϵ][e]σ[˜]);_

_σζ[√]log(k)_

-  |νj| ≤ _O_ _σ˜_ _ϵν_ + _[σ]σ˜[ζ]_ _[d]_ _[e][−][Θ(][k][)][.]_
 


_Proof of Lemma 40. Consider one neuron index i and omit the subscript i in the parameters. Since_
the unbiased initialization leads to g[(0)](x; ξ[(1)]) = 0, we have
_∂_

_ξ_ [)] (343)
_∂w_ _[L]D[α]_ [(][g][(0)][;][ σ][(1)]

= _a[(0)]E(x,y)_ _αyyI[yg[(0)](x; ξ[(1)])_ 1]Eξ(1)I[ _w[(0)], x_ + b[(0)] + ξ[(1)] (0, 1)]x (344)
_−_ _∼D_ _≤_ _⟨_ _⟩_ _∈_
n o

= _a[(0)]E(x,y)_ _,ξ(1)_ _αyyI[_ _w[(0)], x_ + b[(0)] + ξ[(1)] (0, 1)]x (345)
_−_ _∼D_ _⟨_ _⟩_ _∈_

_D_ n o
= _a[(0)]_ _Mj E(x,y)_ _,ξ(1)_ _αyyϕjI[_ _w[(0)], x_ + b[(0)] + ξ[(1)] (0, 1)] (346)
_−_ _∼D_ _⟨_ _⟩_ _∈_

_j=1_

X n :=Tj o

_a[(0)]_ E(x,y) | _,ξ(1)_ _αyyζ_ I[ _w[(0)], x_ + b{z[(0)] + ξ[(1)] (0, 1)] } (347)
_−_ _∼D_ _σ˜_ _⟨_ _⟩_ _∈_
 

:=ν

First, consider j _A._

| _∈_ {z }

_Tj = E(x,y)_ _,ξ(1)_ _αyyϕjI[_ _w[(0)], x_ + b[(0)] + ξ[(1)] (0, 1)] (348)
_∼D_ _⟨_ _⟩_ _∈_
n o

= EϕA,ζ _αyyϕj_ Pr _ϕ, q_ + ι + b[(0)] + ξ[(1)] (0, 1) _._ (349)

 _ϕ−A,ξ[(1)]_ h⟨ _⟩_ _∈_ i[]

where ι := ⟨w[(0)], ζ/σ˜⟩.

Let

_Ia := Pr_ _ϕ, q_ + ι + b[(0)] + ξ[(1)] (0, 1) _,_ (350)
_ξ[(1)]_ _⟨_ _⟩_ _∈_

h i

_Ia[′]_ [:= Pr] _ϕ_ _A, q_ _A_ + ι + b[(0)] + ξ[(1)] (0, 1) _._ (351)
_ξ[(1)]_ _⟨_ _−_ _−_ _⟩_ _∈_

h i

Note that _ϕA, qA_ = O( _kσw[√]2 log(σ˜[2]_ _D/δ)_ ), and that _ι_ = _wi[(0)], ζ/σ˜_ = O( _√dσξσw[√]σ˜2 log(D/δ)_ ),
_|⟨_ _⟩|_ _|_ _|_ _|⟨_ _⟩|_

_Dσw[√]2 log(D/δ)_
and that |⟨ϕ, q⟩|, |⟨ϕ−A, q−A⟩| are O( _σ˜[2]_ ). When σw is sufficiently small, by the prop
erty of the Gaussian ξ[(1)], we have
_|Ia −_ _Ia[′]_ _[|]_ (352)

_≤_ _ξ[(1)]_ _⟨ϕ, q⟩_ + ι + b[(0)] + ξ[(1)] _≥_ 0 _−_ _ξPr[(1)]_ _⟨ϕ−A, q−A⟩_ + ι + b[(0)] + ξ[(1)] _≥_ 0 (353)

h i h i

+ Pr _ϕ, q_ + ι + b[(0)] + ξ[(1)] 1 + Pr _ϕ_ _A, q_ _A_ + ι + b[(0)] + ξ[(1)] 1 (354)
_ξ[(1)][Pr]_ _⟨_ _⟩_ _≥_ _ξ[(1)]_ _⟨_ _−_ _−_ _⟩_ _≥_

h i h i

= O(ϵe). (355)


In summary,
_|Eζ,ϕ−A_ (Ia − _Ia[′]_ [)][|][ =][ O][(][ϵ][e][)][.] (356)
Then we have
_Tj_ EϕA,ζ,ϕ−A _αyyϕjIa[′]_ _[}]_ (357)
_−_ _{_

_≤_ EϕA _|αyyϕj|_ Eζ,ϕ−A (Ia − _Ia[′]_ [)] (358)

_O(ϵe)EϕA_ _αyyϕj_ (359)
_≤_ _{|_ _|}_
_≤_ _O(ϵe/σ˜)_ (360)


-----

where the last step is from Lemma 39. Furthermore,

EϕA,ζ,ϕ−A _αyyϕjIa[′]_ _[}]_ (361)
_{_

= EϕA {αyyϕj} Eζ,ϕ−A [Ia[′] []] (362)

= EϕA _αyyϕj_ Pr _ϕ_ _A, q_ _A_ + ι + b[(0)] + ξ[(1)] (0, 1) (363)
_{_ _}_ _ϕ−A,ζ,ϕ−A_ _⟨_ _−_ _−_ _⟩_ _∈_

h i


When σw is sufficiently small, we have


_⟨ϕ−A, q−A⟩_ + b[(0)] _∈_ (0, 1/2) _≥_ Ω(1), (364)
i

_ι + ξ[(1)]_ _∈_ (0, 1/2) = 1/2 − exp(−Ω(k)), (365)
h i


Pr
_ϕ−A_

Pr
_ζ,ξ[(1)]_


This leads to

_β := Eζ,ϕ−A_ [Ia[′] [] =] _ϕ−APr,ζ,ξ[(1)]_


_⟨ϕ−A, q−A⟩_ + ι + b[(0)] + ξ[(1)] _∈_ (0, 1) _≥_ Ω(1). (366)
i


By Lemma 39, EϕA {αyyϕj} = γ/σ˜. Therefore,

_|Tj −_ _βγ/σ˜| ≤_ _O(ϵe/σ˜)._ (367)

Now, consider j ̸∈ _A. Let B denote A ∪{j}._

_Tj = E(x,y)_ _,ζ,ξ(1)_ _αyyϕjI_ _ϕ, q_ + ι + b[(0)] + ξ[(1)] (0, 1) (368)
_∼D_ _⟨_ _⟩_ _∈_
n h io

= EϕB Eϕ−B _,ζ,ξ(1)_ _αyyϕjI_ _⟨ϕ, q⟩_ + ι + b[(0)] + ξ[(1)] _∈_ (0, 1) (369)
n h io

= EϕB _,ζ_ _αyyϕj_ Pr _ϕ, q_ + ι + b[(0)] + ξ[(1)] (0, 1) _._ (370)

 _ϕ−B_ _,ξ[(1)]_ h⟨ _⟩_ _∈_ i[]

Let


_⟨ϕ, q⟩_ + ι + b[(0)] + ξ[(1)] _∈_ (0, 1) _,_ (371)
i

_⟨ϕ−B, q−B⟩_ + ι + b[(0)] + ξ[(1)] _∈_ (0, 1) _._ (372)
i


_Ib := Pr_
_ξ[(1)]_

_Ib[′]_ [:= Pr]
_ξ[(1)]_


Similar as above, we have |Eζ,ξ(1)(Ib − _Ib[′][)][| ≤]_ _[O][(][ϵ][e][)][. Then by Lemma 39,]_
_Tj_ EϕB _,ζ,ϕ−B_ _αyyϕjIb[′][}]_ (373)
_−_ _{_

_≤_ EϕB _|αyyϕj||Eζ,ϕ−B_ (Ib − _Ib[′][)][|]_ (374)

_O(ϵe)EϕA_ _αyy_ Eϕj _ϕj_ (375)
_≤_ _{|_ _|}_ _{|_ _|}_

_≤_ _O(ϵe) × 1 × O(σϕj[2]_ _σ[˜])_ (376)

= O(σ[2] _σ)._ (377)
_ϕj[ϵ][e]_ [˜]


Furthermore,

Therefore,


EϕB _,ζ,ϕ−B_ _αyyϕjIb[′][}][ =][ E][ϕ]A_ _j_ _−B_ [[][I]b[′][] = 0][.] (378)
_{_ _[{][α][y][y][}][ E][ϕ]_ _[{][ϕ][j][}][ E][ζ,ϕ]_

_|Tj| ≤_ _O(σϕ[2][ϵ][e]σ[˜])._ (379)


Finally, consider νj.

_αyyζj_
_νj = E(x,y)_ _,ξ(1)_ I[ _w[(0)], x_ + b[(0)] + ξ[(1)] (0, 1)] (380)
_∼D_ _σ˜_ _⟨_ _⟩_ _∈_
 

_αyyζj_
= EϕA,ϕ−A,ζ,ξ(1) _σ˜_ I[⟨ϕ, q⟩ + ιj + ι−j + b[(0)] + ξ[(1)] _∈_ (0, 1)] (381)
 

_αyyζj_

= EϕA,ζ Pr (382)

 _σ˜_ _ϕ−A,ξ[(1)][[][⟨][ϕ, q][⟩]_ [+][ ι][j][ +][ ι][−][j][ +][ b][(0)][ +][ ξ][(1)][ ∈] [(0][,][ 1)]]


-----

where ιj := wj[(0)][ζ][j][/]σ[˜] and ι−j := ⟨w[(0)], ζ/σ˜⟩− _ιj._

With probability ≥ 1 − _d exp(−Θ(k)) over ζ, for any j, |ζj| ≤_ _O(σζ_ log(k)). Let Gζ denote this

event.

p

Let


_ϕ, q_ + ιj + ι _j + b[(0)]_ + ξ[(1)] (0, 1) _,_ (383)
_⟨_ _⟩_ _−_ _∈_
i

_ϕ, q_ + ι _j + b[(0)]_ + ξ[(1)] (0, 1) _._ (384)
_⟨_ _⟩_ _−_ _∈_
i


_Ij := Pr_
_ξ[(1)]_

_Ij[′]_ [:= Pr]
_ξ[(1)]_


Similar as above, we have |Eζ[Ij − _Ij[′]_ _[|G][ζ][]][| ≤]_ _[O][(][ϵ][ν][)][. Then]_

_|Eζ,ϕ−A_ (Ij − _Ij[′]_ [)][| ≤|][E][ζ,ϕ]−A [[(][I][j] _[−]_ _[I]j[′]_ [)][|G][ζ][]][|][ + Pr[][−G][ζ][]] (385)

_≤_ _O(ϵν + d exp(−Θ(k)))._ (386)

_αyyζj_

_σ˜_ _Ij[′]_ (387)

 

_αyyζj_

= _[ν][j][ −]_ [E][ϕ][A][,ζ,ϕ][−][A] (Ij _Ij[′]_ [)] (388)

_σ˜_ _−_

 

_αyyζj_ _αyyζj_

[E][ϕ][A][,ζ,ϕ][−][A] (Ij _Ij[′]_ [)][|G][ζ] [+] (Ij _Ij[′]_ [)][| −G][ζ] [Pr[][−G][ζ][]][.][ (389)]

_≤_ _σ˜_ _−_ _σ˜_ _−_

   

The first term is bounded by

[E][ϕ][A][,ζ,ϕ][−][A] [E][ϕ][A][,ζ,ϕ][−][A]

_αyyζj_

(Ij _Ij[′]_ [)][|G][ζ] (390)
_σ˜_ _−_

 

_αyyσζ_ log(k)

_≤_ E[E]ϕ[ϕ]A[A][,ζ,ϕ]( _[−][A]_ pσ˜ _|Eζ,ϕ−A_ [Ib − _Ib[′][|G][ζ][]][|])_ (391)


_≤_ _O(ϵν)EϕA {|αyy|}_ _[σ][ζ]_


log(k)
(392)


_O(ϵν)_ 1 _[σ][ζ]_
_≤_ _×_ _×_


log(k)
(393)
_σ˜_

_._ (394)

!


_σζ_


log(k)
_ϵν_


= O


The second term is bounded by

_αyyζj_

(Ij _Ij[′]_ [)][| −G][ζ] [Pr[][−G][ζ][]] (395)
_σ˜_ _−_

 

_αyyζj_

[E][ϕ][A][,ζ,ϕ][−][A] (Ij _Ij[′]_ [)][| −G][ζ] (396)

_≤_  _σ˜_ _−_  _[×][ de][−][Θ(][k][)]_

EϕA Eζ _ζj_ _ζ_ _de[−][Θ(][k][)]_ (397)
_≤_ [E][ϕ][A][,ζ,ϕ]σ˜[−][A] × _{|_ _|| −G_ _} ×_

(398)

_≤_ _[σ]σ˜[ζ]_ _[×][ de][α][y][−][y]_ [Θ(][k][)]

(399)

_≤_ _[σ]σ ˜[ζ][d]_ _[e][−][Θ(][k][)][.]_

Furthermore,

_αyyζj_ _ζj_

EϕA,ζ,ϕ−A _σ˜_ _Ij[′]_ = EϕA {αyy} Eζj _σ˜_ Eζ−j [Ij[′] [] = 0][.] (400)

   

Therefore,

_σζ_ log(k)

_νj_ _O_ _ϵν_ + _[σ][ζ][d]_ (401)
_|_ _| ≤_ pσ˜ ! _σ ˜_ _[e][−][Θ(][k][)][.]_


-----

**Lemma 41. Under the same assumptions as in Lemma 40,**

_∂_

_L[α]_ _ξ_ [) =][ −][a]i[(0)][T][b] (402)
_∂bi_ _D[(][g][(0)][;][ σ][(1)]_

_where |Tb| ≤_ _O(ϵe)._

_Proof of Lemma 41. Consider one neuron index i and omit the subscript i in the parameters. Since_
the unbiased initialization leads to g[(0)](x; ξ[(1)]) = 0, we have

_∂_

_ξ_ [)] (403)
_∂b_ _[L]D[α]_ [(][g][(0)][;][ σ][(1)]

= _a[(0)]E(x,y)_ _αyyI[yg[(0)](x; ξ[(1)])_ 1]Eξ(1)I[ _w[(0)], x_ + b[(0)] + ξ[(1)] (0, 1)] (404)
_−_ _∼D_ _≤_ _⟨_ _⟩_ _∈_
n o

= _a[(0)]E(x,y)_ _,ξ(1)_ _αyyI[_ _w[(0)], x_ + b[(0)] + ξ[(1)] (0, 1)] (405)
_−_ _∼D_ _⟨_ _⟩_ _∈_
n o

= −a[(0)] EϕA,ζ,ξ(1) _αyy Prϕ−A_ _⟨ϕ, q⟩_ + ι + b[(0)] + ξ[(1)] _∈_ (0, 1) _._ (406)
 h i[]

:=Tb
| {z }

where ι := ⟨w[(0)], ζ/σ˜⟩. Similar to the proof in Lemma 40,

Pr (407)
_ϕ−A,ξ[(1)][[][⟨][ϕ, q][⟩]_ [+][ ι][ +][ b][(0)][ +][ ξ][(1)][ ∈] [(0][,][ 1)]]

[E][ζ] _−_ _ϕ−APr,ξ[(1)][[][⟨][ϕ][−][A][, q][−][A][⟩]_ [+][ ι][ +][ b][(0)][ +][ ξ][(1)][ ∈] [(0][,][ 1)]]! [=][ O][(][ϵ][e][)][.] (408)

Then

_αyy_ Pr (409)
 _ϕ−A,ξ[(1)][[][⟨][ϕ][−][A][, q][−][A][⟩]_ [+][ ι][ +][ b][(0)][ +][ ξ][(1)][ ∈] [(0][,][ 1)]]

= E[T]ϕ[b][ −]A,ζ[E][ϕ][A]α[,ζ]yy Pr (410)

( _|_ _|_ _ϕ−A,ξ[(1)][[][⟨][ϕ, q][⟩]_ [+][ ι][ +][ b][(0)][ +][ ξ][(1)][ ∈] [(0][,][ 1)]]


Pr
_ϕ−A,ξ[(1)][[][⟨][ϕ][−][A][, q][−][A][⟩]_ [+][ ι][ +][ b][(0)][ +][ ξ][(1)][ ∈] [(0][,][ 1)]]


(411)


_≤_ _O(ϵe)EϕA {|αyy|}_ (412)
_≤_ _O(ϵe)._ (413)


Also,


EϕA,ζ _αyy_ Pr (414)

 _ϕ−A,ξ[(1)][[][⟨][ϕ][−][A][, q][−][A][⟩]_ [+][ ι][ +][ b][(0)][ +][ ξ][(1)][ ∈] [(0][,][ 1)]]

= EϕA _αyy_ Pr (415)
_{_ _}_ _ϕ−A,ζ,ξ[(1)][[][⟨][ϕ][−][A][, q][−][A][⟩]_ [+][ ι][ +][ b][(0)][ +][ ξ][(1)][ ∈] [(0][,][ 1)]]

= 0. (416)

Therefore, |Tb| ≤ _O(ϵe)._

**Lemma 42. We have**

_∂_

_∂ai_ _L[α]D[(][g][(0)][;][ σ]ξ[(1)][) =][ −][T][a]_ (417)


_where |Ta| ≤_ _O(maxℓ_ _qi,ℓ[(0)][)][. So if][ w]i[(0)]_ _∈G(δ), |Ta| ≤_ _O(σw_


log(Dm/δ)).


-----

_Proof of Lemma 42. Consider one neuron index i and omit the subscript i in the parameters. Since_
the unbiased initialization leads to g[(0)](x; ξ[(1)]) = 0, we have

_∂_

_ξ_ [)] (418)
_∂a_ _[L]D[α]_ [(][g][(0)][;][ σ][(1)]

= E(x,y) _αyyI[yg[(0)](x; ξ[(1)])_ 1]Eξ(1)σ( _w[(0)], x_ + b[(0)] + ξ[(1)]) (419)
_−_ _∼D_ _≤_ _⟨_ _⟩_
n o

= E(x,y) _,ξ(1)_ _αyyσ(_ _w[(0)], x_ + b[(0)] + ξ[(1)]) _._ (420)
_−_ _∼D_ _⟨_ _⟩_
n o

:=Ta

Let ϕ[′]A [be an independent copy of]| _[ ϕ][A][,][ ϕ][′][ be the vector obtained by replacing in]{z_ } _[ ϕ][ the entries][ ϕ][A][ with]_
_ϕ[′]A[, and let][ x][′][ =][ Mϕ][′][ +][ ζ/]σ[˜] and its label is y[′]. Then_

_|Ta| =_ EϕA _αyyEϕ−A,ζ,ξ(1)σ(⟨w[(0)], x⟩_ + b[(0)] + ξ[(1)]) (421)
n o

_≤_ 2[1] Eϕ−A,ζ,ξ(1)σ(⟨w[(0)], x⟩ + b[(0)] + ξ[(1)])|y = 1 (422)

n o

_−_ E[E]ϕ[ϕ]A[A] Eϕ−A,ζ,ξ(1)σ(⟨w[(0)], x⟩ + b[(0)] + ξ[(1)])|y = −1 (423)
n o


_≤_ 2[1] Eϕ−A,ζ,ξ(1)σ(⟨w[(0)], x⟩ + b[(0)] + ξ[(1)])|y = 1 (424)

n o

_−_ E[E]ϕ[ϕ][′]A[A] Eϕ−A,ζ,ξ(1)σ(⟨w[(0)], x[′]⟩ + b[(0)] + ξ[(1)])|y[′] = −1 (425)

n o

_[.]_


Since σ is 1-Lipschitz,


_|Ta| ≤_ 2[1] [E][ϕ][A][,ϕ]A[′] Eϕ−A _⟨w[(0)], Mϕ⟩−⟨w[(0)], Mϕ[′]⟩_ _|y = 1, y[′]_ = −1 (426)

n o

_≤_ [1]2 [E][ϕ][−][A] EϕA _⟨w[(0)], Mϕ⟩_ _|y = 1_ + Eϕ[′]A _⟨w[(0)], Mϕ[′]⟩_ _|y[′]_ = −1 (427)

 n o n o

_≤_ maxℓ _qi,ℓ[(0)]vuuEϕ_ ℓ∈[D] _ϕ[2]ℓ_ [+] _j≠_ _ℓX:j,ℓ∈A_ _|ϕjϕℓ|_ (428)

u
t  [X] 

max _qi,ℓ[(0)]_ Eϕ (1 + O(1)) (429)
_≤_ _ℓ_
q


= Θ(max _qi,ℓ[(0)][)][.]_ (430)
_ℓ_

With the bounds on the gradient, we now summarize the results for the weights after the first gradient
step.
**Lemma 43. Set**
_λ[(1)]w_ [= 1][/][(2][η][(1)][)][, λ]a[(1)] = λ[(1)]b = 0, σξ[(1)] = 1/k[3][/][2].

_Fix δ_ (0, 1) and suppose wi[(0)] _w(δ), b[(0)]i_ _b(δ) for all i_ [2m]. If k = Ω(log[2](Dm/δ)),
_∈_ _∈G_ _∈G_ _∈_
_then for all i ∈_ [m], wi[(1)] = _ℓ=1_ _[q]i,ℓ[(1)][M][ℓ]_ [+][ υ][ satisfying]

-  if ℓ _∈_ _A, then |qi,ℓ[(1)]_ _[−][P][η][D][(1)][a]i[(0)][βγ/]σ[˜]| ≤_ _O_ _|η[(1)]aσ˜[(0)]i_ _|ϵe_ _, where β ∈_ [Ω(1), 1] and depends
 

_only on wi[(0)], b[(0)]i_ _[;]_

-  if ℓ _̸∈_ _A, then |qi,ℓ[(1)][| ≤]_ _[O]_ _|η[(1)]a[(0)]i_ _[|][σ]ϕℓ[2]_ _[ϵ][e]σ[˜]_ _;_
 


-----

_σζ[√]log(k)_

-  |υj| ≤ _O_ _|η[(1)]a[(0)]i_ _[|]_ _σ˜_ _ϵν +_ _[σ]σ˜[ζ]_ _[d]_ _[e][−][Θ(][k][)]_
  

-  b[(1)]i = b[(0)]i + η[(1)]a[(0)]i _[T][b][ where][ |][T][b][|][ =][ O][ (][ϵ][e][)][;]_


_and_



-  a[(1)]i = a[(0)]i + η[(1)]Ta where _Ta_ = O(σw
_|_ _|_


log(Dm/δ)).


_Proof of Lemma 43. This follows from Lemma 37 and Lemma 40-42._

F.8 FEATURE IMPROVEMENT: SECOND GRADIENT STEP

We first show that with properly set η[(1)], for most x, |g[(1)](x; σξ[(2)][)][|][ <][ 1][ and thus][ yg][(1)][(][x][;][ σ]ξ[(2)][)][ <][ 1][.]

**Lemma 44. Fix δ** (0, 1) and suppose wi[(0)] _w(δ), b[(0)]i_ _b(δ), a[(0)]i_ _a(δ) for all i_ [2m].
_∈_ _∈G_ _∈G_ _∈G_ _∈_
_Ifη[(1)] Dµ/= O√d ≤kmσγ1a/σ˜16,, and σζσ˜ = σξ[(2)] O(1)1, σ/kζ[2], then with probability[d/]σ[˜][2]_ = O(1), k = Ω(log1 [2](Dm/δ(d + D))) exp(, σa ≤ Ω(σ˜[2]k/))(γk over[2]),

_≤_ _≥_ _−_ _−_

(x, y), we have _yg[(1)]_ (x; σξ[(2)][)][ <][ 1][.][ Furthermore, for any][ i][ ∈] [[2][m][]][,] _⟨wi[(1)], ζ/σ˜⟩_ = O(η[(1)]σ/γ˜ ),
_⟨qi[(1)], ϕ⟩_ = O(η[(1)]σ/γ˜ ), and _⟨(qi[(1)])−A, ϕ−A⟩_ = O(η[(1)]σ/γ˜ ), and for any j ∈ [d], ℓ _∈_ [D],

_ζj_ _O(σζ_ log(k)) and _ζ, Dℓ_ _O(σζ_ log(k)).
_|_ _| ≤_ _|⟨_ _⟩| ≤_
p p

_Proof of Lemma 44. Note that wi[(0)]_ = wm[(0)]+i[,][ b][(0)]i = b[(0)]m+i[, and][ a][(0)]i = a[(0)]m+i[. Then the gradient for]
_wm+i is the negation of that for wm+i, the gradient for bm+i is the negation of that for bm+i, and the_
gradient for am+i is the same as that for am+i.
_g[(1)](x; σξ[(2)][)]_ (431)

2m

= _a[(1)]i_ [E]ξ[(2)][σ][(][⟨][w]i[(1)], x⟩ + b[(1)]i + ξi[(2)]) (432)

_i=1_

X


_a[(1)]i_ [E]ξ[(2)][σ][(][⟨][w]i[(1)], x⟩ + b[(1)]i + ξi[(2)]) + a[(1)]m+i[E]ξ[(2)][σ][(][⟨][w]m[(1)]+i[, x][⟩] [+][ b][(1)]m+i [+][ ξ]m[(2)]+i[)] [(433)]


_a[(1)]i_ [E]ξ[(2)][σ][(][⟨][w]i[(1)], x⟩ + b[(1)]i + ξi[(2)]) + a[(1)]m+i[E]ξ[(2)][σ][(][⟨][w]i[(1)], x⟩ + b[(1)]i + ξi[(2)]) (434)


_−a[(1)]m+i[E]ξ[(2)][σ][(][⟨][w]i[(1)], x⟩_ + b[(1)]i + ξi[(2)]) + a[(1)]m+i[E]ξ[(2)][σ][(][⟨][w]m[(1)]+i[, x][⟩] [+][ b][(1)]m+i [+][ ξ]i[(2)]) _[.]_



_i=1_

_m_

_i=1_

X

_m_

_i=1_

X


(435)

2η[(1)]TaEξ(2)σ( _wi[(1)], x_ + b[(1)]i + ξi[(2)]) (436)
_⟨_ _⟩_

_a[(1)]m+i_ _wi[(1)]_ _wm[(1)]+i[, x][⟩]_ + _b[(1)]i_ _b[(1)]m+i_ (437)
_⟨_ _−_ _−_
 

2η[(1)]Ta _wi[(1)], x_ + b[(1)]i + Eξ(2) _ξi[(2)]_ (438)
_⟨_ _⟩_
 

_a[(1)]m+i_ _wi[(1)]_ _wm[(1)]+i[, x][⟩]_ + _b[(1)]i_ _b[(1)]m+i_ _._ (439)
_⟨_ _−_ _−_
 


Then we have

_g[(1)](x; σξ[(2)][)]_


_i=1_

_m_

_i=1_

Xm

_i=1_

Xm

_i=1_

X


-----

With probability ≥ 1 − exp(−Ω(k)), among all j ̸∈ _A, we have that at most po(D −_ _k) of ϕj are_
(1 _poj)/σ˜, while the others are_ _poj/σ˜. With probability_ 1 (d + D) exp( Ω(k)) over ζ,
_−_ _−_ _≥_ _−_ _−_
for any j, _ζj_ _O(σζ_ log(k)) and _ζ, Dℓ_ _O(σζ_ log(k)). For data points with ϕ and ζ
_|_ _| ≤_ _|⟨_ _⟩| ≤_

satisfying these, we have:

p p


_wi[(1)], x_ _O(η[(1)]/γ)(1 + ˜σ + ˜σ/_
_⟨_ _⟩_ _≤_


**Claim 1.**


_k)._


_Proof of Claim 1._

_D_ _D_

_⟨wi[(1)], x⟩_ = _⟨_ _qi,ℓ[(1)][M][ℓ]_ [+][ υ,] _ϕjMj + ζ/σ˜⟩_

_ℓ=1_ _j=1_

X X

_D_ _D_ _D_

_≤_ _⟨_ _qi,ℓ[(1)][M][ℓ][,]_ _ϕjMj⟩_ + _qi,ℓ[(1)][M][ℓ][, ζ/]σ[˜]⟩_

_ℓ=1_ _j=1_ _ℓ=1_

X X X

_[⟨]_


(440)

+ |⟨υ, ζ/σ˜⟩| .

(441)


_⟨υ,_


_ϕjMj_
_⟩_
_j=1_

X


For each term above we bound as follows. Note that when σw is sufficiently small, ϵe =
_O(k log[1][/][2](Dm/δ)/√D). Let_

_B1 := βγ/σ˜ + ϵe/σ,˜_ (442)


_B2 := σϕ[2][ϵ][e]σ[˜] = O(ϵe/_


_D),_ (443)


_C1 =_ _[k]_ (444)

_σ ˜_ _[,]_

_C2 := poD/σ˜ = O(D/σ˜)._ (445)


Then

_a[(0)]i_ (446)
_|_ _[|][B][1][C][1][ =][ O][(log(][Dm/δ][)][/k][ + log(][Dm/δ][)][ϵ][e][/][(][γk][)) =][ O][(1][/γ][)][,]_

_a[(0)]i_ _σ/γ),_ (447)
_|_ _[|][B][2][C][2][ =][ O][(˜]_

_a[(0)]i_ _√D/γ),_ (448)
_|_ _[|][B][1][C][2][ =][ O][(][D/k][ +]_

_a[(0)]i_ _√k)) = O(1/γ),_ (449)
_|_ _[|][B][2][C][1][ =][ O][(][ϵ][e][/][(][γ]_

Then by the assumption on µ,

_D_ _D_

_⟨_ _qi,ℓ[(1)][M][ℓ][,]_ _ϕjMj⟩_ (450)

_ℓ=1_ _j=1_

X X


_⟨qi,ℓ[(1)][M][ℓ][, M][ℓ][ϕ][ℓ][⟩]_
_ℓX∈A_


_⟨qi,ℓ[(1)][M][ℓ][, M][ℓ][ϕ][ℓ][⟩]_
_ℓX̸∈A_


_⟨qi,ℓ[(1)][M][ℓ][, M][j][ϕ][j][⟩]_

Xℓ≠ _j_


(451)


_O(_ _η[(1)]a[(0)]i_ _B1C1 + B2C2 +_ _[µ]_
_≤_ _|_ _[|][)]_ _√d_


_O(_ _η[(1)]a[(0)]i_ _B1C1 + B2C2 +_ _[kµ]_
_≤_ _|_ _[|][)]_ _√d_



(kB1(C1 + C2) + DB2(C1 + C2)) (452)



(453)


_B1C2 + B2C1_


_≤_ _O(η[(1)])(1/γ + ˜σ/γ + 1/γ + 1/γ)_ (454)

_≤_ _O(η[(1)]/γ)(1 + ˜σ)._ (455)


-----

By the assumption on σζ,

_D_

_qi,ℓ[(1)][M][ℓ][, ζ/]σ[˜]⟩_ (456)
_ℓ=1_

X

log(k)

_≤_ _O[⟨]_ (|η[(1)]a[(0)]i _[|][)(][kB][1][ +][ DB][2][)]_ _[σ][ζ]pσ˜_ (457)

log(k) _σ˜[2][p]log(Dm/δ)_

_O(η[(1)])(kB1 + DB2)_ _[σ][ζ]_ (458)
_≤_ _σ˜_ _γk[2]_
p

1
_O(η[(1)])_ _σζ log(Dm/δ)_ + σζσ˜ [log(][k][) log(][Dm/δ][)] (459)
_≤_ _k_ [+][ ϵ]γk[e] _γk_
   

_≤_ _O(η[(1)]/γ)._ (460)


_σζ log[2](Dm/δ)_
Also note that _νj_ _O_ _σ˜√D_
_|_ _| ≤_



. Then by the assumption on σζ,


(461)

_× (C1 + C2)_ (462)


_⟨υ,_


_ϕjMj_
_⟩_
_j=1_

X


_O(_ _η[(1)]a[(0)]i_ _√d_ _O_ _σζ log2(Dm/δ)_ (C1 + C2) (462)
_≤_ _|_ _[|][)][ ×]_ _×_ _σ˜√D_ _×_

 

_≤_ _O(|η[(1)]/γ)._ (463)


_O(_ _η[(1)]a[(0)]i_
_≤_ _|_ _[|][)][ ×]_


Finally, we have

We also have:


_υj_ _ζj/σ˜_ (464)
_|_ _||_ _|_
_j=1_

X


_|⟨υ, ζ/σ˜⟩| ≤_


_O(_ _η[(1)]a[(0)]i_
_≤_ _|_ _[|][)][ ×][ d][ ×][ σ][ζ][ log]σ˜[2]√[(][Dm/δ]D_ [)]

_σ_
_O(η[(1)]/γ) [˜]_ _._
_≤_ _√k_


log(k)


(465)

(466)


_Ta_ = O(σw log(Dm/δ)) (467)
_|_ _|_

_b[(1)]i_ _b[(0)]i_ +p _η[(1)]a[(0)]i_ _[T][b]_ (468)
_≤_

log(m/δ)

+ _η[(1)]a[(0)]i_ _[ϵ][e]_ _._ (469)

_≤_ _k[2]_

p

Eξ(2) _ξi[(2)]_ _≤_ _O(σξ[(2)][)][.]_ (470)

_a[(1)]m+i[| ≤|][a][(0)]i_ _i_ log(Dm/δ)). (471)
_|_ _[|][ +][ |][η][(1)][T][a][| ≤|][a][(0)][|][ +][ O][(][η][(1)][σ][w]_

_wi[(1)]_ _wm[(1)]+i[, x][⟩]_ = 2 _wi[(1)], x_ = O(η[(1)]σ/γ˜ ). p (472)
_⟨_ _−_ _⟨_ _⟩_
_b[(1)]i_ _−_ _b[(1)]m+i_ = 2|η[(1)]a[(0)]i _[T][b][|][ =][ O][(][|][η][(1)][a]i[(0)][|][ϵ][e][)][.]_ (473)


-----

Then we have
_g[(1)](x; σξ[(2)][)]_ _≤_ _O_ _mη[(1)]σw_



_η[(1)]σ˜_
log(Dm/δ)



_η[(1)]σ˜_ log(m/δ)

_O_ _mη[(1)]σw_ log(Dm/δ) + + _η[(1)]a[(0)]i_ _[ϵ][e]_ + σξ[(2)]
_≤_  p  _γ_ p _k[2]_ !

(474)

_η(1)σ˜_

+ O _m(_ _a[(0)]i_ log(Dm/δ)) + _η[(1)]a[(0)]i_ _[ϵ][e]_ (475)
_|_ _[|][ +][ η][(1)][σ][w]_ _γ_
 p  [](1) 

log(Dm/δ) _η_ _σ˜_
= O _mη[(1)]σw_ + m _a[(0)]i_ + _η[(1)]a[(0)]i_ _[ϵ][e]_ (476)

_k_ _|_ _[|]_ _γ_

  

log(Dm/δ) _σ_ _σ_
= O _mη[(1)]σw_ + m _a[(0)]i_ + m _a[(0)]i_ (477)

_k_ _|_ _[|]_ _[η][(1)]γ_ [˜] _|_ _[|]_ _[η]γ[(1)]√k[˜]_

 

_< 1._ (478)


Then _yg[(1)](x; σξ[(2)][)]_ _< 1. Finally, the statement on_ _⟨(qi[(1)])−A, ϕ−A⟩_ follows from a similar

calculation on _wi[(1)], x_ = _qi[(1)], ϕ_ .
_⟨_ _⟩_ _⟨_ _⟩_

We are now ready to analyze the gradients in the second gradient step.

**Lemma 45. Fix δ** (0, 1) and suppose wi[(0)] _w(δ), b[(0)]i_ _b(δ), a[(0)]i_ _a(δ) for all i_ [2m].
_∈_ _∈G_ _∈G_ _∈G_ _∈_

_Let ϵe2 := O_ _η[(1)]|aσ˜[(0)]i[2]σ|ξ[(2)]k(γ+ϵe)_ + exp(−Θ(k)). If Dµ/√d ≤ 1/16, σζσ˜ = O(1), σζ[2][d/]σ[˜][2] = O(1),
 


_k = Ω(log[2](Dm/δ)), σa_ _σ˜[2]/(γk[2]), η[(1)]_ = O
_≤_


_kmσγ_ _aσ˜_ _, and σξ[(2)]_ = 1/k[3][/][2], then



_∂_

_∂wi_ _LD(g[(1)]; σξ[(2)][) =][ −][a]i[(1)]_


(479)


_MjTj + ν_
_j=1_

X


_where Tj satisfies:_

-  if j ∈ _A, then |Tj −_ _βγ/σ˜| ≤_ _O(ϵe2/σ˜ + η[(1)]/σξ[(2)]_ + η[(1)]|a[(0)]i _[|][ϵ][e][/][(˜]σσξ[(2)][))][, where][ β][ ∈]_

[Ω(1), 1] and depends only on wi[(0)], b[(0)]i _[;]_



-  if j ̸∈ _A, then |Tj| ≤_ _σ[1]˜_ [exp(][−][Ω(][k][)) +][ O][(][σ]ϕ[2]σϵ[˜] _e2);_



-  _νj_ _O_ _η[(1)]σζ_
_|_ _| ≤_ _γσξ[(2)]_



+ exp(−Ω(k)).


_Proof of Lemma 45. Consider one neuron index i and omit the subscript i in the parameters._
By Lemma 44, with probability at least 1 − (d + D) exp(−Ω(k)) = 1 − exp(−Ω(k)) over

(x, y), yg[(1)](x; ξ[(2)]) > 1 and furthermore, for any i [2m], _wi[(1)], ζ/σ˜_ = O(η[(1)]σ/γ˜ ),
_∈_ _⟨_ _⟩_
_⟨qi[(1)], ϕ⟩_ = O(η[(1)]σ/γ˜ ), and _⟨(qi[(1)])−A, ϕ−A⟩_ = O(η[(1)]σ/γ˜ ), and for any j ∈ [d], ℓ _∈_ [D],

_|ζj| ≤_ _O(σζ_ log(k)) and |⟨ζ, Dℓ⟩| ≤ _O(σζ_ log(k)). Let Ix be the indicator of this event.

_∂_

p _ξ_ [)] p (480)

_∂w_ _[L]D[α]_ [(][g][(1)][;][ σ][(2)]

= _a[(1)]E(x,y)_ _αyyIxEξ(2)I[_ _w[(1)], x_ + b[(1)] + ξ[(2)] (0, 1)]x (481)
_−_ _∼D_ _⟨_ _⟩_ _∈_
n o


= _a[(1)]_ _Mj E(x,y)_ _,ξ(2)_ _αyyIxI[_ _w[(1)], x_ + b[(1)] + ξ[(2)] (0, 1)]ϕj
_−_ _∼D_ _⟨_ _⟩_ _∈_

_j=1_

X n :=Tj

_a[(1)]_ E(x,y) | _,ξ(2)_ _αyyζ_ IxI[ _w[(1)], x_ +{z b[(1)] + ξ[(2)] (0, 1)] _._
_−_ _∼D_ _σ˜_ _⟨_ _⟩_ _∈_
 

:=ν
| {z }


(482)

(483)


-----

Let Tj1 := E(x,y) _,ξ(2)_ _αyyI[_ _w[(1)], x_ + b[(1)] + ξ[(2)] (0, 1)]ϕj . We have
_∼D_ _⟨_ _⟩_ _∈_

_Tj_ _Tj1_ (484)
_|_ _−_ _|_

= E(x,y) _,ξ(2)_ _αyy(1_ Ix)I[ _w[(1)], x_ + b[(1)] + ξ[(2)] (0, 1)]ϕj (485)
_∼D_ _−_ _⟨_ _⟩_ _∈_
n o

(486)

_≤_ _σ˜[1]_ [exp(][−][Ω(][k][))][.]


_αyyζ_
Similarly, let ν[′] := E(x,y)∼D,ξ(2) _σ˜_ [I][[][⟨][w][(1)][, x][⟩] [+][ b][(1)][ +][ ξ][(2)][ ∈] [(0][,][ 1)]] . We have
n o

_|ν −_ _ν[′]|_ (487)

_αyyζ_
= (1 Ix)I[ _w[(1)], x_ + b[(1)] + ξ[(2)] (0, 1)] (488)

_σ˜_ _−_ _⟨_ _⟩_ _∈_

 

(489)

_≤_ _[σ]σ[E]˜[ζ][(][x,y][exp(][)][∼D][−][,ξ][Ω(][(2)][k][))][.]_


So it is sufficient to bound Tj1 and ν[′]. For simplicity, we use q as a shorthand for qi[(1)].

First, consider j ∈ _A._

_Tj1 = E(x,y)_ _,ξ(2)_ _αyyI[_ _w[(1)], x_ + b[(1)] + ξ[(2)] (0, 1)]ϕj (490)
_∼D_ _⟨_ _⟩_ _∈_
n o

= EϕA _αyyϕj_ Pr _ϕ, q_ + ι + b[(1)] + ξ[(2)] (0, 1) (491)

 _ϕ−A,ξ[(2)]_ h⟨ _⟩_ _∈_ i[]

where ι := ⟨w[(1)], ζ/σ˜⟩. Let


_⟨ϕ, q⟩_ + ι + b[(1)] + ξ[(2)] _∈_ (0, 1) _,_ (492)
i

_⟨ϕ−A, q−A⟩_ + ι + b[(1)] + ξ[(2)] _∈_ (0, 1) _._ (493)
i


_Ia := Pr_
_ξ[(2)]_

_Ia[′]_ [:= Pr]
_ξ[(2)]_


_i_ _k(γ+ϵe)_
By the property of the Gaussian ξ[(2)], that _ϕA, qA_ = O( _[η][(1)][|][a][(0)]σ˜|[2]_ ), and that _ι_ =
_|⟨_ _⟩|_ _|_ _|_

_|⟨wi[(1)], ζ/σ˜⟩|, |⟨ϕ, q⟩|, |⟨ϕ−A, q−A⟩| are all O(η[(1)]σ/γ˜_ ) < O(1/k), we have

_|Ia −_ _Ia[′]_ _[|]_ (494)

_≤_ _ξ[(2)]_ _⟨ϕ, q⟩_ + ι + b[(1)] + ξ[(2)] _≥_ 0 _−_ _ξPr[(2)]_ _⟨ϕ−A, q−A⟩_ + ι + b[(1)] + ξ[(2)] _≥_ 0 (495)

h i h i

+ Pr _ϕ, q_ + ι + b[(1)] + ξ[(2)] 1 + Pr _ϕ_ _A, q_ _A_ + ι + b[(1)] + ξ[(2)] 1 (496)
_ξ[(2)][Pr]_ _⟨_ _⟩_ _≥_ _ξ[(2)]_ _⟨_ _−_ _−_ _⟩_ _≥_

h i h i


_η[(1)]_ _a[(0)]i_
_|_ _[|][k][(][γ][ +][ ϵ][e][)]_

_σ˜[2]σξ[(2)]_


+ exp(−Θ(k)) = O(ϵe2). (497)


= O


This leads to
_Tj1_ EϕA,ϕ−A _αyyϕjIa[′]_ _[}]_ (498)
_−_ _{_

_≤_ EϕA _|αyyϕj|_ Eϕ−A (Ia − _Ia[′]_ [)] (499)

_O(ϵe2)EϕA_ _αyyϕj_ (500)
_≤_ _{|_ _|}_
_≤_ _O(ϵe2/σ˜)_ (501)

where the last step is from Lemma 39. Furthermore,

EϕA,ϕ−A _αyyϕjIa[′]_ _[}]_ (502)
_{_

= EϕA {αyyϕj} Eϕ−A [Ia[′] []] (503)

= EϕA _αyyϕj_ Pr _ϕ_ _A, q_ _A_ + ι + b[(1)] + ξ[(2)] (0, 1) _._ (504)
_{_ _}_ _ϕ−A,ξ[(2)]_ _⟨_ _−_ _−_ _⟩_ _∈_

h i


-----

By Lemma 19, we havethe property of ξ[(2)], _|⟨ϕ−A, q−A⟩_ + ι| ≤ _O(η[(1)]σ/γ˜_ ). Also, |b[(1)] _−_ _b[(0)]| ≤_ _O(η[(1)]|a[(0)]i_ _[|][ϵ][e][)][. By]_

_ξ[(2)]_ _⟨ϕ−A, q−A⟩_ + ι + b[(1)] + ξ[(2)] _∈_ (0, 1) _−_ _ξPr[(2)]_ _b[(0)]_ + ξ[(2)] _∈_ (0, 1) (505)

h i h i

_O(η[(1)]σ/˜_ (γσξ[(2)][)) +][ O][(][η][(1)][|][a]i[(0)] _ξ_ [)][.] (506)
_≤_ [Pr] _[|][ϵ][e][/σ][(2)]_

On the other hand,


_b[(0)]_ + ξ[(2)] (0, 1) = Pr
_∈_ _ξ[(2)]_
i


_ξ[(2)]_ _∈_ (−b[(0)], 1 − _b[(0)])_ (507)
i


_β :=_ Pr
_ϕ−A,ξ[(2)]_


= Ω(1) (508)

and β only depends on b[(0)]. By Lemma 39, EϕA {αyyϕj} = γ/σ˜. Therefore,

_|Tj1 −_ _βγ/σ˜| ≤_ _O(ϵe2/σ˜) + O(η[(1)]/σξ[(2)][) +][ O][(][η][(1)][|][a]i[(0)][|][ϵ][e][/][(˜]σσξ[(2)][))][.]_ (509)

Now, consider j ̸∈ _A. Let B denote A ∪{j}._

_Tj1 = E(x,y)_ _,ξ(2)_ _αyyϕjI_ _ϕ, q_ + ι + b[(1)] + ξ[(2)] (0, 1) (510)
_∼D_ _⟨_ _⟩_ _∈_
n h io

= EϕB Eϕ−B _,ζ,ξ(2)_ _αyyϕjI_ _⟨ϕ, q⟩_ + ι + b[(1)] + ξ[(2)] _∈_ (0, 1) (511)
n h io

= EϕB _αyyϕj_ Pr _ϕ, q_ + ι + b[(1)] + ξ[(2)] (0, 1) _._ (512)

 _ϕ−B_ _,ζ,ξ[(2)]_ h⟨ _⟩_ _∈_ i[]

Let


_⟨ϕ, q⟩_ + ι + b[(1)] + ξ[(2)] _∈_ (0, 1) _,_ (513)
i

_⟨ϕ−B, q−B⟩_ + ι + b[(1)] + ξ[(2)] _∈_ (0, 1) _._ (514)
i


_Ib := Pr_
_ξ[(2)]_

_Ib[′]_ [:= Pr]
_ξ[(2)]_


Similar as above, we have |Ib − _Ib[′][| ≤]_ _[ϵ][e2][. Then by Lemma 39,]_
_Tj1_ EϕB _,ϕ−B_ _,ζ_ _αyyϕjIb[′][}]_ (515)
_−_ _{_

_≤_ EϕB _|αyyϕj||Eϕ−B_ _,ζ(Ib −_ _Ib[′][)][|]_ (516)

_O(ϵe2)EϕA_ _αyy_ Eϕj _ϕj_ (517)
_≤_ _{|_ _|}_ _{|_ _|}_

_≤_ _O(ϵe2) × O(σϕ[2]σ[˜])_ (518)

= O(σϕ[2]σϵ[˜] e2). (519)


Furthermore,

Therefore,


EϕB _,ϕ−B_ _,ζ_ _αyyϕjIb[′][}][ =][ E][ϕ]A_ _j_ _−B_ [[][I]b[′][] = 0][.] (520)
_{_ _[{][α][y][y][}][ E][ϕ]_ _[{][ϕ][j][}][ E][ϕ]_

_|Tj1| ≤_ _O(σϕ[2]σϵ[˜]_ e2). (521)


Finally, consider νj[′] [.]

_αyyζj_
_νj[′]_ [=][ E](x,y) _,ξ[(2)]_ I[ _w[(1)], x_ + b[(1)] + ξ[(2)] (0, 1)] (522)
_∼D_ _σ˜_ _⟨_ _⟩_ _∈_
 

_αyyζj_
= EϕA,ϕ−A,ζ,ξ(2) _σ˜_ I[⟨ϕ, q⟩ + ι + b[(1)] + ξ[(2)] _∈_ (0, 1)] (523)
 

_αyyζj_

= EϕA,ϕ _A,ζ_ Pr (524)
_−_ _σ˜_ _ξ[(2)][[][⟨][ϕ, q][⟩]_ [+][ ι][ +][ b][(1)][ +][ ξ][(2)][ ∈] [(0][,][ 1)]]

 


-----

Let


_⟨ϕ, q⟩_ + ι + b[(0)] + ξ[(1)] _∈_ (0, 1) _,_ (525)
i

_⟨ϕ, q⟩_ + b[(0)] + ξ[(1)] _∈_ (0, 1) _._ (526)
i


_Ij := Pr_
_ξ[(2)]_

_Ij[′]_ [:= Pr]
_ξ[(2)]_


Since |ι| ≤ _O(η[(1)]σ/γ˜_ ), we have |Ij − _Ij[′]_ _[| ≤]_ _[O][(][η][(1)]σ/[˜]_ (γσξ[(2)][))][. Then]

_αyyζj_

_j_ _[−]_ [E][ϕ]A[,ϕ]−A[,ζ] _σ˜_ _Ij[′]_ (527)

 

_αyyζj_

= _[ν][′]_ (Ij _Ij[′]_ [)] (528)

_σ˜_ _−_

 

_αyyζj_

_O[E]([ϕ]η[A][(1)][,ϕ][−]σ/˜[A][,ζ](γσξ[(2)][))][E][ϕ][A][,ϕ][−][A][,ζ]_ (529)
_≤_ _σ˜_

_ζj_

_≤_ _O(η[(1)]σ/˜_ (γσξ[(2)][))][E][ϕ][A] _[|][α][y][y][|][ E][ζ]_ _σ˜_ (530)

_O(η[(1)]σ/˜_ (γσξ[(2)][))][ ×][ 1][ ×][ σ][ζ] (531)
_≤_ _σ˜_


_≤_ _O(η[(1)]σζ/(γσξ[(2)][))][.]_ (532)


Furthermore,

Therefore,


_αyyζj_

EϕA,ϕ _A,ζ_
_−_ _σ˜_




_αyy_
= EϕA,ϕ−A _σ ˜_ _[I]j[′]_ Eζ {ζj} = 0. (533)
n o


_Ij[′]_


_η[(1)]σζ_

_γσξ[(2)]_


+ exp(−Ω(k)). (534)


_νj_ _O_
_|_ _| ≤_


**Lemma 46. Under the same assumptions as in Lemma 45,**
_∂_

_ξ_ [) =][ −][a]i[(1)][T][b] (535)
_∂b_ _[L][D][(][g][(1)][;][ σ][(2)]_

_where |Tb| ≤_ exp(−Ω(k)) + O(ϵe2).

_Proof of Lemma 46. Consider one neuron index i and omit the subscript i in the parameters. By_
Lemma 44, Pr[yg[(1)](x; ξ[(2)]) > 1] ≤ exp(−Ω(k)). Let Ix = I[yg[(1)](x; ξ[(2)]) ≤ 1].
_∂_

_ξ_ [)] (536)
_∂b_ _[L]D[α]_ [(][g][(1)][;][ σ][(2)]

= _a[(1)]_ E(x,y) _αyyIxEξ(2)I[_ _w[(1)], x_ + b[(1)] + ξ[(2)] (0, 1)] _._ (537)
_−_ _∼D_ _⟨_ _⟩_ _∈_
n o

:=Tb

Let Tb1 := E(x,y) _,ξ(2)_ | _αyyI[_ _w[(1)], x_ + b[(1)] + ξ[(2)]{z (0, 1)] . We have }
_∼D_ _⟨_ _⟩_ _∈_

_Tb_ _Tb1_ (538)
_|_ _−_ _|_

= E(x,y) _,ξ(2)_ _αyy(1_ Ix)I[ _w[(1)], x_ + b[(1)] + ξ[(2)] (0, 1)] (539)
_∼D_ _−_ _⟨_ _⟩_ _∈_

_≤_ exp(−Ω(k)). n o (540)

So it is sufficient to bound Tb1. For simplicity, we use q as a shorthand for qi[(1)].

_Tb1 = E(x,y)_ _,ξ(2)_ _αyyI_ _ϕ, q_ + ι + b[(1)] + ξ[(2)] (0, 1) (541)
_∼D_ _⟨_ _⟩_ _∈_
n h io

= EϕA Eϕ−A,ξ(2) _αyyI_ _⟨ϕ, q⟩_ + ι + b[(1)] + ξ[(2)] _∈_ (0, 1) (542)
n h io

= EϕA _αyy_ Pr _ϕ, q_ + ι + b[(1)] + ξ[(2)] (0, 1) _._ (543)

 _ϕ−A,ξ[(2)]_ h⟨ _⟩_ _∈_ i[]


-----

Let


_⟨ϕ, q⟩_ + ι + b[(1)] + ξ[(2)] _∈_ (0, 1) _,_ (544)
i

_⟨ϕ−A, q−A⟩_ + ι + b[(1)] + ξ[(2)] _∈_ (0, 1) _._ (545)
i


_Ib := Pr_
_ξ[(2)]_

_Ib[′]_ [:= Pr]
_ξ[(2)]_


Similar as in Lemma 20, we have |Ib − _Ib[′][| ≤]_ _[ϵ][e2][. Then by Lemma 39,]_

_Tb1_ EϕA,ϕ−A _αyyIb[′][}]_ (546)
_−_ _{_

= EϕA,ϕ−A _αyy(Ib_ _Ib[′][)][}]_ (547)
_{_ _−_

= O(ϵe2)EϕA,ϕ _A_ _αyy_ (548)
_−_ _|_ _|_

_≤_ _O(ϵe2)._ (549)


Furthermore,

EϕA,ϕ−A _αyyIb[′][}][ =][ E][ϕ]A_ _−A_ [[][I]b[′][] = 0][.] (550)
_{_ _[{][α][y][y][}][ E][ϕ]_

Therefore, |Tb1| ≤ _O(ϵe2) and the statement follows._

**Lemma 47. Under the same assumptions as in Lemma 45,**

_∂_

_∂ai_ _LD(g[(1)]; σξ[(2)][) =][ −][T][a]_ (551)


_where_ _Ta_ = O _γpη[(1)]minσ˜_
_|_ _|_



+ exp( Ω(k))poly _pdDmin_
_−_



_Proof of Lemma 47. Consider one neuron index i and omit the subscript i in the parameters. By_
Lemma 44, Pr[yg[(1)](x; ξ[(2)]) > 1] ≤ exp(−Θ(k)). Let Ix = I[yg[(1)](x; ξ[(2)]) ≤ 1].

_∂_

_ξ_ [)] (552)
_∂a_ _[L]D[α]_ [(][g][(1)][;][ σ][(2)]


= E(x,y) _αyyIxEξ(2)σ(_ _w[(1)], x_ + b[(1)] + ξ[(2)])
_−_ _∼D_ _⟨_ _⟩_
n

:=Ta
| {z

Let Ta1 := E(x,y) _αyyEξ(2)σ(_ _w[(1)], x_ + b[(1)] + ξ[(2)]) . We have
_∼D_ _⟨_ _⟩_



(553)


_Ta_ _Ta1_ (554)
_|_ _−_ _|_

= E(x,y) _αyy(1_ Ix)Eξ(2)σ( _w[(1)], x_ + b[(1)] + ξ[(2)]) (555)
_∼D_ _−_ _⟨_ _⟩_

_≤_ exp(−Ω(kn)). o (556)

So it is sufficient to bound Ta1. For simplicity, we use q as a shorthand for qi[(1)].


-----

Let ϕ[′]A [be an independent copy of][ ϕ][A][,][ ϕ][′][ be the vector obtained by replacing in][ ϕ][ the entries][ ϕ][A][ with]
_ϕ[′]A[, and let][ x][′][ =][ Mϕ][′][ +][ ζ][ and its label is][ y][′][. Then]_

_|Ta1| :=_ EϕA _αyyEϕ−A,ζ,ξ(2)σ(⟨w[(1)], x⟩_ + b[(1)] + ξ[(2)]) (557)
n o

_≤_ 2[1] Eϕ−A,ζ,ξ(2)σ(⟨w[(1)], x⟩ + b[(1)] + ξ[(1)])|y = 1 (558)

n o

_−_ E[E]ϕ[ϕ]A[A] Eϕ−A,ζ,ξ(2)σ(⟨w[(1)], x⟩ + b[(1)] + ξ[(2)])|y = −1 (559)
n o


_≤_ 2[1] Eϕ−A,ζ,ξ(2)σ(⟨w[(1)], x⟩ + b[(1)] + ξ[(2)])|y = 1 (560)

n o

_−_ E[E]ϕ[ϕ][′]A[A] Eϕ−A,ζ,ξ(2)σ(⟨w[(1)], x[′]⟩ + b[(1)] + ξ[(2)])|y[′] = −1 (561)

n o


_≤_ 2[1] [E][ϕ][A][,ϕ]A[′] Eϕ−A _⟨w[(1)], x⟩−⟨w[(1)], x[′]⟩_ _|y = 1, y[′]_ = −1 (562)

n o

_≤_ [1]2 [E][ϕ][−][A] EϕA _⟨w[(1)], Mϕ⟩_ _|y = 1_ + Eϕ[′]A _⟨w[(1)], Mϕ[′]⟩_ _|y[′]_ = −1 (563)

 n o n o

Eϕ _A,ϕA_ _αy_ _w[(1)], Mϕ_ (564)
_≤_ _−_ _⟨_ _⟩_

= Eϕ _αy⟨w[(1)], Mϕ⟩_ (565)

_√dD_

= O(η[(1)]σ/γ˜ ) + exp( Ω(k)) _w[(1)]_ (566)
_−_ _×_ _σ˜_ _× ∥_ _∥∞_

(1)
_η_ _σ˜_
= O + exp( Ω(k))poly(dD/pmin) (567)

_γpmin_ _−_

 

where the fourth step follows from that σ is 1-Lipschitz, and the second to the last line from Lemma 44
and that _w[(1)], Mϕ_ _w[(1)]_ _d_ _Mϕ_ 2[.]
_⟨_ _⟩_ _≤∥_ _∥∞_ _∥_ _∥[2]_
p

With the above lemmas about the gradients, we are now ready to show that at the end of the second
step, we get a good set of features for accurate prediction.
**Lemma 48. Set**

_σ_
_η[(1)]_ = _[γ][2][p][min]_ [˜] _, λ[(1)]a_ = 0, λ[(1)]w [= 1][/][(2][η][(1)][)][, σ]ξ[(1)] = 1/k[3][/][2], (568)

_km[3]_

_η[(2)]_ = 1, λ[(2)]a = λ[(2)]w [= 1][/][(2][η][(2)][)][, σ]ξ[(2)] = 1/k[3][/][2]. (569)

_Dmd_

_Fix δ_ (0, O(1/k[3])). If Dµ/√d 1/16, σζσ˜ = O(1), σζ[2][d/]σ[˜][2] = O(1), k = Ω log[2][ ] _δγpmin_ _,_
_∈_ _≤_

_and m_ max Ω(k[4]), D, d _, then with probability at least 1_ _δ over the initialization, there exist_ 
_≥_ _{_ _}_ _−_
_a∥˜a˜i’s such that∥0 = O(m/k ˜g(x),) := ∥a˜∥∞_ _i==1 Oa[˜]i(σk([5]⟨/mwi[(2)]), and, x⟩_ + ∥ ba˜[(2)]i _∥2[2][)][ satisfies][=][ O][(][k][9][ L][/m][D][(˜][)]g[. Finally,]) ≤_ exp([ ∥]−[a]Ω([(2)]k[∥]))[∞]. Furthermore,[=][ O] _km1_ [2] _,_

_wi[(2)]_ 2 = O(˜σ/k), and[P]b[2][(2)]i[m]   
_∥_ _∥_ _|_ _[|][ =][ O][(1][/k][2][)][ for all][ i][ ∈]_ [[2][m][]][.]

_Proof of Lemma 48. By Lemma 35, there exists a network g[∗](x) =_ _ℓ=1_ _a[∗]ℓ_ _[σ][(][⟨][w]ℓ[∗][, x][⟩]_ [+][ b][∗]ℓ [)]
satisfying
Pr
(x,y)∼D[[][yg][∗][(][x][)][ ≤] [1]][ ≤] [exp(][−][Ω(][k][))][.] [P][3(][k][+1)]

_σFurthermore, the number of neurons˜_ _j_ _A_ _[M][j][/][(8][k][)][, and][ |⟨][w]i[∗][, x][⟩]_ [+][ b]i[∗] _n = 3(k + 1), |a[∗]i_ _[| ≤]_ [64][k,][ 1][/][(64][k][)][ ≤|][b]i[∗][| ≤] [1][/][4][,][ w]i[∗] [=]

_∈_ _[| ≤]_ [1][ for any][ i][ ∈] [[][n][]][ and][ (][x, y][)][ ∼D][. Now we fix an][ ℓ][, and]
show that with high probability there is a neuron in g[(2)] that can approximate the ℓ-th neuron in g[∗].

[P]

With probability ≥ 1 − exp(−Ω(max{2po(D − _k), k})), among all j ̸∈_ _A, we have that at most_
2po(D − _k) + k of ϕj are (1 −_ _po)/σ˜, while the others are −po/σ˜. With probability ≥_ 1 − (d +


-----

_D) exp(_ Ω(k)) over ζ, for any j, _ζj_ _O(σζ_ log(k)) and _ζ, Dℓ_ _O(σζ_ log(k)). Below
_−_ _|_ _| ≤_ _|⟨_ _⟩| ≤_

we consider data points with ϕ and ζ satisfying these.

p p

By Lemma 37, with probability 1 2δ over wi[(0)]’s, they are all in _w(δ); with probability 1_ _δ over_
_−_ _G_ _−_
_a[(0)]i_ [’s, they are all in][ G][a][(][δ][)][; with probability][ 1][ −] _[δ][ over][ b]i[(0)][’s, they are all in][ G][b][(][δ][)][. Under these]_
events, by Lemma 43, Lemma 45 and 46, for any neuron i ∈ [2m], we have

_D_

_wi[(2)]_ = a[(1)]i _MjTj + ν_ _,_ (570)

 

_j=1_

X
 

_b[(2)]i_ = b[(1)]i + a[(1)]i _[T][b][.]_ (571)
where

-  if j ∈ _A, then |Tj −_ _βγ/σ˜| ≤_ _ϵw1 := O(ϵe2/σ˜ + η[(1)]/σξ[(2)]_ + η[(1)]|a[(0)]i _[|][ϵ][e][/][(˜]σσξ[(2)][))][, where]_

_β ∈_ [Ω(1), 1] and depends only on wi[(0)], b[(0)]i [;]



-  if j ̸∈ _A, then |Tj| ≤_ _ϵw2 :=_ _σ[1]˜_ [exp(][−][Ω(][k][)) +][ O][(][σ]ϕ[2]σϵ[˜] e2);



-  _νj_ _ϵν := O_ _η[(1)]σζ_
_|_ _| ≤_ _γσξ[(2)]_



+ exp(−Ω(k)).



-  |Tb| ≤ _ϵb := exp(−Ω(k)) + O(ϵe2)._

Given the initialization, with probability Ω(1) over b[(0)]i [, we have]

1
_b[(0)]i_ _, sign(b[(0)]i_ [) = sign(][b]ℓ[∗][)][.] (572)
_|_ _[| ∈]_ 2k[2][,][ 2]k[2]
 

Finally, since [8][k]b[(0)]i[|][b]ℓ[∗][|]σ˜[βγ][2][ ∈] [[Ω(][k][2][γ/]σ[˜][2]), O(k[3]γ/σ˜[2])] and depends only on wi[(0)], b[(0)]i [, we have that for]

_|_ _|_

_ϵa = Θ(1/k[2]), with probability Ω(ϵa) > δ over a[(0)]i_ [,]
8k _b[∗]ℓ_ _[|][βγ]_ ˜σ[2]

_|b[(0)]i|_ _[|]σ[˜][2][ a]i[(0)]_ _−_ 1 _[≤]_ _[ϵ][a][,]_ _|a[(0)]i_ _[|][ =][ O]_  _k[2]γ_  _._ (573)

Let na = ϵam/4. For the given value of m, by (570)-(573) we have with probability ≥ 1 − 5δ over
the initialization, for each ℓ there is a different set of neurons Iℓ _⊆_ [m] with |Iℓ| = na and such that
for each iℓ _Iℓ,_
_∈_

1
_b[(0)]iℓ_ _[| ∈]_ _,_ sign(b[(0)]iℓ [) = sign(][b]ℓ[∗][)][,] (574)
_|_ 2k[2][,][ 2]k[2]
 

8k _b[∗]ℓ_ _[|][βγ]_ ˜σ[2]
_|_ _iℓ_ _[−]_ [1] _a[(0)]iℓ_ _[|][ =][ O]_ _._ (575)

_|b[(0)]iℓ_ _[|]σ[˜][2][ a][(0)]_ _[≤]_ _[ϵ][a][,]_ _|_  _k[2]γ_ 

Now, construct ˜a such that ˜aiℓ = [2][a]ℓ[∗][|][b][∗]ℓ _[|]_ _ai = 0 elsewhere. To show_

_b[(0)]_
_|_ _iℓ_ _[|][n][a][ for each][ ℓ]_ [and each][ i][ℓ] _[∈]_ _[I][ℓ][, and][ ˜]_

that it gives accurate predictions, we first consider bounding some error terms.


For the given values of parameters, we have

_γ_
_ϵe2 = O_ _,_

_m[2]_

 _kγ_

_ϵw1 = O_

_m[2]σ˜_ [+][ γϵ]km[e][2]



_γ_
_ϵw2 = O_ _,_

_m[2]σ˜_

 _γk_ 

_ϵν = O_ _,_

_m[3]_

 

_γ_
_ϵb = O_ _._

_m[2]_

 


(576)

(577)

(578)

(579)

(580)


-----

We also have the following useful claims.

_k_

**Claim 2.** _ℓ∈A_ _[|⟨][M][ℓ][, x][⟩| ≤]_ _[O]_ _σ˜_ _._

[P]   

_Proof of Claim 2._


_Mℓ, x_ (581)
_|⟨_ _⟩|_
_ℓX∈A_


_≤_ |ϕj| + _Mℓ[⊤][M][j][ϕ][j]_ +

_ℓX∈A_ Xj≠ _ℓ_



_k_ _µ_
_≤_ _O_ _σ˜_ + O _kD_ _√dσ˜_ + O
   

_k_
_O_ _._
_≤_ _σ˜_
 


_Mℓ[⊤][ζ/]σ[˜]_ 



_k [σ][ζ]_ log(k)
p


(582)

(583)

(584)

(585)

(586)


**Claim 3.**

_Proof of Claim 3._

_⟨wi[(2)]ℓ_ _[, x][⟩]_ =

_≤_

Then

_iℓ_ _[βγ]_
_wi[(2)]ℓ_ _[, x][⟩−]_ _[a][(0)]_
_⟨_ _σ˜_

_iℓ_ _[βγ]_
_wi[(2)]ℓ_ _[, x][⟩−]_ _[a][(1)]_

_≤_ _⟨_ _σ˜_

_iℓ_ _[βγ]_
_wi[(2)]ℓ_ _[, x][⟩−]_ _[a][(1)]_

_≤_ _⟨_ _σ˜_


1
_O_
_≤_ _m_



_iℓ_ _[βγ]_
_wi[(2)]ℓ_ _[, x][⟩−]_ _[a][(0)]_
_⟨_ _σ˜_


_Mj, x_
_⟨_ _⟩_
_jX∈A_


_a[(1)]iℓ_ _[T][ℓ][M][ℓ]_ [+][ υ, x][⟩]
_ℓ=1_

X

_[⟨]_

_a[(1)]iℓ_ _[T][ℓ][M][ℓ][, x][⟩]_ [+]
_ℓX∈A_

_[⟨]_

_Mj, x_
_⟨_ _⟩_
_jX∈A_


_⟨_ _a[(1)]iℓ_ _[T][ℓ][M][ℓ][, x][⟩]_

_ℓX̸∈A_


+ |⟨υ, x⟩| . (587)

(588)


_a[(1)]iℓ_ _[βγ]_


_iℓ_ _[βγ]_
_Mj, x_
_⟨_ _⟩−_ _[a][(0)]σ˜_
_jX∈A_


(589)

(590)

(591)


_Mj, x_
_⟨_ _⟩_
_jX∈A_

_Mj, x_
_⟨_ _⟩_
_jX∈A_


_Mj, x_
_⟨_ _⟩_
_jX∈A_


_βγ_


_a[(1)]iℓ_ _[−]_ _[a]i[(0)]ℓ_


_Mj, x_
_⟨_ _⟩_
_jX∈A_


The first term is

_iℓ_ _[βγ]_
_wi[(2)]ℓ_ _[, x][⟩−]_ _[a][(1)]_
_⟨_ _σ˜_


_Mj, x_
_⟨_ _⟩_
_jX∈A_


_Tℓ_
_−_ _[βγ]σ˜_


_a[(1)]iℓ_


+ _ν, x_ _._ (592)
_|⟨_ _⟩|_


_Mℓ, x_
_⟩_


_TℓMℓ, x_
_⟩_
_ℓX̸∈A_


_ℓ∈A_


-----

By Claim 2,

_TℓMℓ, x_

_⟨_ _⟩_

_ℓX̸∈A_

Then by (576)-(580),


_Tℓ_
_−_ _[βγ]σ˜_


_Mℓ, x_ (593)
 _⟩_ _[≤]_ _ℓX∈A_ _σ˜_ _[|⟨][M][ℓ][, x][⟩|]_

_ϵw1_ _[T][ℓ]_ _[−]M[βγ]ℓ, x_ (594)
_≤_ _|⟨_ _⟩|_

_ℓX∈A_

_kϵw1_
_O_ _._ (595)
_≤_ _σ˜_
 

_TℓMℓ[⊤][M][j][ϕ][j]_ + _TℓMℓ[⊤][ζ/]σ[˜]_ (596)
_ℓ̸∈XA,j≠_ _ℓ_ _ℓX̸∈A_

_µ_ _σζ_ log(k)
+ O _D[2]ϵw2_ + O _Dϵw2_ (597)
 _√dσ˜_  pσ˜ !

_._ (598)


_ℓ∈A_


_Tℓϕj_

_≤_

_ℓX̸∈A_

_Dϵw2_
_O_
_≤_ _σ˜_


_Dϵw2_
_O_
_≤_ _σ˜_



+ |⟨ν, ζ/σ˜⟩| (599)


_|⟨ν, x⟩| ≤_


_⟨ν,_


_Mℓϕℓ_
_⟩_
_ℓX∈[D]_


_ϕℓ_ _ν, Mℓ_ + _ν, ζ/σ˜_ (600)

_≤_ _|_ _⟨_ _⟩|_ _|⟨_ _⟩|_

_ℓX∈[D]_

_poD + k_ _O(σζ_ log(k))
_O_ _ϵν√d + dϵν_ (601)
_≤_ _σ˜_ _σ˜_
  p

_ϵν√d_

_O_ _poD +_ _√dϵν_ log(k) (602)
_≤_ _σ˜_

!  p 

_ϵν√d_

_O_ _poD + ˜σ_ log(k) (603)
_≤_ _σ˜_

!  p 

_poDϵν√d_

_O_ _._ (604)
_≤_ _σ˜_ !


2
_k_ _γ_
= O

_m[2]σ˜[2][ +][ γϵ]m[2][e]σ˜_ [+]




_kϵw1_


+ _[Dϵ][w][2]_


+ _[p][o][Dϵ][ν]_


_γ_ _γk_

_mσ˜[2][ +]_ _m[3][/][2]σ˜_


(605)


_ϵϕ :=_


We have _a[(1)]iℓ_ _[−]_ _[a]i[(0)]ℓ_ = O(η[(1)]σw log(Dm/δ)). So the first term is bounded by

p

_iℓ_ _[βγ]_
_⟨wi[(2)]ℓ_ _[, x][⟩−]_ _[a][(0)]σ˜_ _⟨Mj, x⟩_ _≤_ _a[(1)]iℓ_ _ϵϕ_ (606)

_jX∈A_

2

˜σ[2] _k_ _γ_ _γ_ _γk_ 1
_O_ log(Dm/δ) _O_ _. (607)_
_≤_ _k[2]γ_ [+][ η][(1)][σ][w] _m[2]σ˜[2][ +][ γϵ]m[2][e]σ˜_ [+] _mσ˜[2][ +]_ _m[3][/][2]σ˜_ _≤_ _m_
     

p

By Claim 2, the second term is bounded by


3
_γ_
_O_
_≤_ _m[3]_



_kη[(1)]σw_


_βγ_


log(Dm/δ)γ
_σ˜[2]_


_βγ_ _kη_ _σw_

_a[(1)]iℓ_ _[−]_ _[a]i[(0)]ℓ_ _σ˜_ _⟨Mj, x⟩_ _≤_ _O_ _σ˜[2]_

_j_ _A_ p

X∈

Combining the bounds on the two terms leads to the claim.


(608)


-----

**Claim 4.**

_|b[(2)]iℓ_ _[−]_ _[b]i[(0)]ℓ_ _[| ≤]_ _[O]_

_Proof of Claim 4. By Lemma 43 and 46:_


(609)


_k[2]m_


_|b[(2)]iℓ_ _[−]_ _[b]i[(0)]ℓ_ _[| ≤|][b]i[(2)]ℓ_ _[−]_ _[b]i[(1)]ℓ_ _[|][ +][ |][b]i[(1)]ℓ_ _[−]_ _[b]i[(0)]ℓ_ _[|]_ (610)

_≤_ _O_ _η[(1)]|a[(0)]iℓ_ _[|][ϵ][e][ +][ |][a]i[(1)]ℓ_ _[|][ (exp(][−][Ω(][k][)) +][ ϵ][e2][)]_ (611)
 _γ_ 1 1 

_O_ _O_ _._ (612)
_≤_ _km[2][ +]_ _k[2]m_ _≤_ _k[2]m_
   


We are now ready to show ˜g is close to 2g[∗].

_|g˜(x) −_ 2g[∗](x)| (613)

3(k+1) 3(k+1)

= _a˜iℓ_ _σ_ _wi[(2)]ℓ_ _[, x][⟩]_ [+][ b]i[(2)]ℓ 2a[∗]ℓ _[σ][ (][⟨][w]ℓ[∗][, x][⟩]_ [+][ b]ℓ[∗][)] (614)

_⟨_ _−_

_ℓ=1_ _iℓ_ _Iℓ_ _ℓ=1_

X X∈   X

3(k+1) 3(k+1)

= Xℓ=1 _iXℓ∈Iℓ_ _|2ba[(0)]iℓ[∗]ℓ_ _[|][|][b][n][∗]ℓ[a][|]_ _σ_ ⟨wi[(2)]ℓ _[, x][⟩]_ [+][ b]i[(2)]ℓ  _−_ Xℓ=1 _iXℓ∈Iℓ_ _|2ba[(0)]iℓ[∗]ℓ_ _[|][|][b][n][∗]ℓ[a][|]_ _σ_ _|b|b[(0)]iℓ[∗]ℓ_ _[| ⟨][|]_ _[w]ℓ[∗][, x][⟩]_ [+][ b][(0)]iℓ !

(615)

3(k+1)

1 _ℓ_ _[|][b][∗]ℓ_ _[|]_ _ℓ_ _[|][b][∗]ℓ_ _[|]_ _iℓ_ _[βγ]_

_≤_ _ℓ=1_ _iℓ_ _Iℓ_ _na_  _b[(0)]iℓ_ _[|]_ _σ_ _⟨wi[(2)]ℓ_ _[, x][⟩]_ [+][ b]i[(2)]ℓ _−_ [2][a]b[∗][(0)]iℓ _[|]_ _σ_  _σ˜_ _j_ _A⟨Mj, x⟩_ + b[(0)]iℓ 

X X∈ _|_   _|_ X∈

 [2][a][∗]  _[a][(0)]_ (616)


_|bb[(0)]iℓ[∗]ℓ_ _[| ⟨][|]_ _[w]ℓ[∗][, x][⟩]_ [+][ b][(0)]iℓ ![]

_|_

(617) 


3(k+1)

_ℓ=1_

X


_iℓ_ _[βγ]_

 _σ˜_

 _[a][(0)]_


_ℓ_ _[|][b][∗]ℓ_ _[|]_

 _|b[(0)]iℓ_ _[|]_

 [2][a][∗]


_ℓ_ _[|][b][∗]ℓ_ _[|]_

_Mj, x_ + b[(0)]iℓ
_j_ _A⟨_ _⟩_  _−_ [2][a]b[∗][(0)]iℓ _[|]_

X∈ _|_




_na_


_iℓ∈Iℓ_


Here the second equation follows from that σ is positive-homogeneous in [0, 1], |⟨wℓ[∗][, x][⟩] [+][ b][∗]ℓ _[| ≤]_ [1][,]
_|b[(0)]iℓ_ _[|][/][|][b]ℓ[∗][| ≤]_ [1][.]

By Claim 3 and 4, the first term is bounded by:


2a[∗]ℓ _[|][b][∗]ℓ_ _[|]_ _iℓ_ _[βγ]_
3(k + 1) max _wi[(2)]ℓ_ _[, x][⟩−]_ _[a][(0)]_
_ℓ_ _|b[(0)]iℓ_ _[|]_  _⟨_ _σ˜_

2a[∗]ℓ _[|][b][∗]ℓ_ _[|]_ 1
3(k + 1) max _O_
_≤_ _ℓ_ _b[(0)]iℓ_ _[|]_  _m_ 

_|_

_k4_
_O_ _._
_≤_ _m_
 


+ |b[(2)]iℓ _[−]_ _[b]i[(0)]ℓ_ _[|]_


(618)

(619)

(620)


_Mj, x_
_⟨_ _⟩_
_jX∈A_


-----

By Claim 2, the second term is bounded by:


_a[(0)]iℓ_ _[βγ]_ _iℓ_ _[|]_

_Mj, x_ _ℓ_ _[, x][⟩]_

_σ˜_ _jX∈A⟨_ _⟩−_ _[|][b]|b[(0)][∗]ℓ_ _[| ⟨][w][∗]_

_a[(0)]iℓ_ _[βγ]_ _iℓ_ _[|]σ[˜]_

_Mj, x_ _Mj, x_

_σ˜_ _jX∈A⟨_ _⟩−_ _[|]8[b]k[(0)]|b[∗]ℓ_ _[|]_ _jX∈A⟨_ _⟩_


2a[∗]ℓ _[|][b][∗]ℓ_ _[|]_
3(k + 1) max
_ℓ_ _|b[(0)]iℓ_ _[|]_

2a[∗]ℓ _[|][b][∗]ℓ_ _[|]_
3(k + 1) max
_≤_ _ℓ_ _|b[(0)]iℓ_ _[|]_


(621)

(622)

(623)


3(k + 1) max 2a[∗]ℓ _[|][b][∗]ℓ_ _[|]_ _σ˜|b[(0)]iℓ_ _[|]_ 8kaiℓ _βγ|b[∗]ℓ_ _[|]_ 1 _Mj, x_ (623)
_≤_ _ℓ_ _|b[(0)]iℓ_ _[|]_ 8k|b[∗]ℓ _[|]_ _σ˜[2]|b[(0)]iℓ_ _[|]_ _−_ _jX∈A⟨_ _⟩_

3(k + 1) max _O(a[∗]ℓ_ _[ϵ][a][)]_ (624)
_≤_ _ℓ_

_O_ _k[2]ϵa_ _._ (625)
_≤_
  


Then


_k4_
_g˜(x)_ 2g[∗](x) = O 1. (626)
_|_ _−_ _|_ _m_ [+][ k][2][ϵ][a] _≤_
 

This guarantees yg˜(x) ≥ 1. Changing the scaling of δ leads to the statement.

Finally, the bounds on ˜a follow from the above calculation. The bound on ∥a[(2)]∥2 follows from
Lemma 47, and those on _wi[(2)]_ 2 and _b[(2)]i_ _i_ and
_∥_ _∥_ _∥_ _[∥][2][ follow from (570)(571) and the bounds on][ a][(1)]_
_b[(1)]i_ in Lemma 43.

F.9 CLASSIFIER LEARNING STAGE AND MAIN THEOREM

Once we have a good set of features in Lemma 48, we can follow exactly the same argument as in
Section B.6 and B.7 for the simplified setting, and arrive at the main theorem for the general setting:

**Theorem 49 (Restatement of Theorem 34). Set**


_σ_
_η[(1)]_ = _[γ][2][p][min]_ [˜] _, λ[(1)]a_ = 0, λ[(1)]w [= 1][/][(2][η][(1)][)][, σ]ξ[(1)] = 1/k[3][/][2], (627)

_km[3]_

_η[(2)]_ = 1, λ[(2)]a = λ[(2)]w [= 1][/][(2][η][(2)][)][, σ]ξ[(2)] = 1/k[3][/][2], (628)

_k[2]_ _k[3]_
_η[(][t][)]_ = η = _a_ = λ[(]w[t][)] [=][ λ][ ≤] _ξ_ = 0, for 2 < t _T._ (629)

_Tm[1][/][3][, λ][(][t][)]_ _σm˜_ [1][/][3][, σ][(][t][)] _≤_

_Dmd_

_For any δ_ (0, O(1/k[3])), if µ _O(√d/D), σζ_ _O(min_ 1/σ,˜ ˜σ/√d ), k = Ω log[2][ ] _δγpmin_ _,_
_∈_ _≤_ _≤_ _{_ _}_

_m_ max Ω(k[4]), D, d _, then we have for any_ Ξ, with probability at least 1 _δ, there exists_
_≥_ _{_ _}_ _D ∈F_ _−_
_t ∈_ [T ] such that

8
_k_
Pr[sign(g[(][t][)](x)) = y] _L_ (g[(][t][)]) = O _._ (630)
_̸_ _≤_ _D_ _m[2][/][3][ +][ k]m[3][T][2][ +][ k][2][m]T_ [2][/][3]
 

_Consequently, for any ϵ ∈_ (0, 1), if T = m[4][/][3], and m ≥ max{Ω(k[12]/ϵ[3][/][2]), D}, then

Pr[sign(g[(][t][)](x)) = y] _L_ (g[(][t][)]) _ϵ._ (631)
_̸_ _≤_ _D_ _≤_


-----

