# ON LEARNING TO SOLVE CARDINALITY CON## STRAINED COMBINATORIAL OPTIMIZATION IN ONE# SHOT: A RE-PARAMETERIZATION APPROACH VIA GUMBEL-SINKHORN-TOPK

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Cardinality constrained combinatorial optimization requires selecting an optimal
subset of k elements, and it will be appealing to design data-driven algorithms
that perform TopK selection over a probability distribution predicted by a neural network. However, the existing differentiable TopK operator suffers from an
unbounded gap between the soft prediction and the discrete solution, leading to inaccurate estimation of the combinatorial objective score. In this paper, we present
a self-supervised learning pipeline for cardinality constrained combinatorial optimization, which incorporates with Gumbel-Sinkhorn-TopK (GS-TopK) for neardiscrete TopK predictions and the re-parameterization trick resolving the nondifferentiable challenge. Theoretically, we characterize a bounded gap between
the Maximum-A-Posteriori (MAP) inference and our proposed method, resolving
the divergence issue in the previous differentiable TopK operator and also providing a more accurate estimation of the objective score given a provable tightened
bound to the discrete decision variables. Experiments on max covering and discrete clustering problems show that under comparative time budget, our method
outperforms state-of-the-art Gurobi solver and the novel one-shot learning method
Erdos Goes Neural.

1 INTRODUCTION

In this paper, we aim to solve a family of cardinality constrained combinatorial optimization: given
the cardinality budget k, our aim is to find the optimal set with size k which minimizes the objective
function f (x). Denote **x** 0 _k as the cardinality constraint, we have:_
_||_ _||_ _≤_

min _s.t._ **x** 0 _k_ (1)
**x** _[f]_ [(][x][)] _||_ _||_ _≤_

Extensive applications of these problems can be found in machine learning and operations research,
for example, discovering coresets for dataset summarization and continual learning (Borsos et al.,
2020), discovering the most influential seed users in social networks (Chen et al., 2021), and planning facility locations in operation research (Liu, 2009). Although there has been a success in
discovering the important submodularity (Fujishige, 1991) for a family of these problems with a
bounded approximation ratio of greedy algorithms, the ubiquitous NP-hard challenge is always motivating us to develop improved algorithms with better efficiency-accuracy trade-off. The recent
success of machine learning has inspired researchers to develop learning-based algorithms for combinatorial optimizations (Vinyals et al., 2015; Dai et al., 2016; Karalias & Loukas, 2020; Wang et al.,
2021b), and this paper falls in line with these works.

According to the taxonomy by a recent survey (Peng et al., 2021), existing machine learning methods for combinatorial optimization can be mainly categorized into two groups: multi-step methods (Vinyals et al., 2015; Dai et al., 2016; Lu et al., 2019; Wang et al., 2021a) and one-shot methods (Wang et al., 2019; Karalias & Loukas, 2020; Li et al., 2019). The former line of works usually
formulates the combinatorial optimization solving procedure as a Markov Decision Process (MDP)
and refers to novel reinforcement learning methods (Mnih et al., 2013; Schulman et al., 2017) to
tackle the MDP. Perhaps the largest drawback of such a multi-step pipeline is that the training and


-----

inference procedures can be inefficient. Based on some previous efforts (Wang et al., 2019; Li
et al., 2019), there is a recent seminal work (Karalias & Loukas, 2020) aiming to develop a one-shot
pipeline for general combinatorial problems. Although being a fast and general pipeline, Karalias
& Loukas (2020) do not encode constraint in the model prediction and thus the estimation of the
combinatorial objective score may be inaccurate.

In this paper, we aim to facilitate the one-shot learning pipeline with cardinality constraints, such
that the constraints can be softly encoded by the output of a neural network. Regarding the input to
TopK as a probability distribution, the maximum-a-posterior (MAP) inference is done by sorting all
probabilities and selecting the K-largest ones, however, this process is non-differentiable. Though
appealing, it is non-trivial to encode the constraint in a differentiable manner with bounded gap w.r.t.
the MAP inference and the bound of the existing SOFT-TopK algorithm (Xie et al., 2020) diverges if
the k-th and (k +1)-th probabilities are equal. Unfortunately, such a dilemma is possible because the
probabilities are outputs from a neural network. Besides, concerning the discrete decision variables
in combinatorial optimization, it is also non-trivial to build an accurate estimation of the objective
score with neural network output (which has to be continuous in order to preserve the gradient).

To tackle the non-differentiable challenge and the ill-estimated objective score, we refer to the seminal reparameterization technique with Gumbel noise, which can mimic the sampling procedure
from the continuous probability distributions to near-discrete decision variables, while still preserving the gradient. Such a technique has been found successful for Top-1 (Jang et al., 2017),
permutation (Mena et al., 2018), and sorting (Grover et al., 2019) distributions, and in this paper, we
improve the SOFT-TopK operator (Xie et al., 2020) by introducing the Gumbel re-parameterization
trick, and we theoretically prove that its gap can be bounded with the existence of the Gumbel noise,
in contrast to the gap of SOFT-TopK (Xie et al., 2020) that may turn diverged.

To this end, we present Gumbel-Sinkhorn-TopK (GS-TopK) to facilitate the one-shot selfsupervised learning pipeline (Karalias & Loukas, 2020) with constrained neural network output.
The proposed GS-TopK operator improves the SOFT-TopK (Xie et al., 2020), where we theoretically characterize a bounded gap by introducing non-zero noise factors. Besides, we show how to
integrate the proposed GS-TopK method into the combinatorial optimization pipeline, with an accurate estimation of the objective score, formulating a self-supervised learning pipeline following
(Karalias & Loukas, 2020). The contributions of this paper are summarized as follows:

**1) We propose a self-supervised learning approach to solve cardinality constrained combinatorial**
optimizations in one shot. The cardinality constraints are encoded in the model output via our proposed GS-TopK, and our pipeline is differentiable via the re-parameterization trick.
**2) We theoretically characterize a bounded gap between the MAP inference and the approximate**
solution by Gumbel-Sinkhorn-TopK without requiring the large gap of k-th and (k +1)-th probabilities, which improves the result of the state-of-the-art SOFT-TopK algorithm (Xie et al., 2020). This
property is crucial for an accurate estimation of the combinatorial objective score in learning.
**3) Experiments on max covering and discrete clustering problems show that under a comparative or**
smaller inference time budget, our method surpasses the state-of-the-art one-shot learning methods
e.g. (Karalias & Loukas, 2020), and also surpasses the state-of-the-art Gurobi and SCIP solvers.

2 RELATED WORK

2.1 LEARNING OF COMBINATORIAL OPTIMIZATION
Recently it is a trending topic of developing machine learning based combinatorial optimization
methods to achieve a better trade-off between time and accuracy by modern data-driven approaches.
Here we follow the taxonomy by Peng et al. (2021) and discuss some representative works.

**Multi-step methods predict the solution by a multi-step decision process. Since the fundamental**
work (Vinyals et al., 2015) that proposes an encoding-decoding architecture, researchers have explored this general pipeline to various real-world combinatorial optimization problems. The most
popular framework follows the reinforcement learning pipeline introduced by Khalil et al. (2017),
where the state is encoded by a graph neural network, and multi-step actions are taken to sequentially
construct the solution. Applications have been explored on job scheduling (Zhang et al., 2020), computer resource allocation (Mao et al., 2019), quadratic assignment programming (Liu et al., 2020),
bin-packing (Duan et al., 2019), just to name a few. The advantage of multi-step methods is that they
can handle combinatorial constraints that are non-trivial to be encoded in a differentiable manner, by


-----

adding constraints on the feasible actions in the multi-step roll-out procedure. However, the training
of reinforcement learning may be inefficient and vulnerable, and there are also some recent efforts
on developing one-shot combinatorial optimization learning methods.

**One-shot methods predict the solution by a single forward pass. Despite the challenges in encoding**
general constraints, one-shot learning methods have been successful on certain problems where the
constraint can be encoded by the neural network. For example, in graph matching (Wang et al., 2019;
2021b) the constraint can be encoded by Sinkhorn algorithm (Sinkhorn & Rangarajan, 1964), and
also for graph similarity regression problems (Bai et al., 2019; Li et al., 2019) that are unconstrained.
The seminal work (Karalias & Loukas, 2020) aims to develop a general pipeline for one-shot learning of combinatorial optimization, by encoding the violation of the constraint as part of its objective
score. However, our experimental results show that compared to Karalias & Loukas (2020), it will
be more appealing if we can explicitly encode the constraint in the neural network output.

2.2 OPTIMAL TRANSPORT AND GUMBEL RE-PARAMETERIZATION
**Optimal Transport. The optimal transport (OT) is a linear programming problem that aims to**
find the optimal transportation plan with minimal cost. Though the development of the Sinkhorn
algorithm can be date back to 1960s (Sinkhorn & Rangarajan, 1964), it has drawn increasing attention from the machine learning community since the pioneering work (Cuturi, 2013) which introduces the entropic-regularization for the optimal transport problem. Since the Sinkhorn algorithm (Sinkhorn & Rangarajan, 1964) incorporates row- and column-wise normalization which is
differentiable, OT can be incorporated within any end-to-end deep learning pipelines. Thus, OT has
been found successful for deep generative models (Arjovsky et al., 2017; Patrini et al., 2019), solving jigsaw puzzles (Santa Cruz et al., 2018), and deep graph matching (Fey et al., 2020; Yu et al.,
2020). In complement to the recent line of differentiable sorting papers (Petersen et al., 2021; Cuturi
et al., 2019), Xie et al. (2020) develop the differentiable TopK algorithm namely SOFT-TopK, by
formulating the TopK problem as an OT problem. However, the theoretical gap between the prediction of SOFT-TopK and the result obtained by sorting diverges if the k-th and (k + 1)-th largest
items are equal. In this paper, we prove that this gap can be bounded by introducing Gumbel noise.

**Gumbel Re-parameterization.** The Gumbel distribution with the re-parameterization trick is
shown effective to mimic the sampling procedure from a given distribution in a differentiable manner. Jang et al. (2017); Maddison et al. (2017) take the initiative to develop Gumbel-Softmax,
where one-hot vectors are sampled from a softmax probability distribution. The non-differentiable
sampling procedure turned to be differentiable by the namely re-parameterization trick (Kingma
& Welling, 2013). Inspired by Jang et al. (2017), Mena et al. (2018) propose a Gumbel-Sinkhorn
method to learn the latent permutations, which is relevant to the GS-TopK method developed in
this paper. Besides, Paulus et al. (2020) develop a general framework for generating combinatorial
distributions via random distributions. However, these existing methods do not study the application in combinatorial optimization learning, and the gap between the MAP inference and the result
after Gumbel re-parameterization is not well characterized in Mena et al. (2018). In this paper, we
explore the application of the re-parameterization trick with Gumbel noise in combinatorial optimization, also we theoretically characterize the gap between the MAP inference and the proposed
Gumbel method.

3 THE PROPOSED GUMBEL-SINKHORN-TOPK

3.1 PRELIMINARY: SOFT-TOPK ALGORITHM
The TopK problems are ubiquitous in machine learning, yet the MAP inference approach (performing sorting at the time cost of O(m log m) and selecting the first k items) is unfriendly for current
gradient-based deep neural networks. To introduce differentiable TopK for machine learning, Xie
et al. (2020) formulate the TopK problem as an optimal transport (OT) problem. Specifically, for
selecting the k largest items from the probability vector s of size m (k < m), the OT problem refers
to moving k items to one destination (selected), and the other m − _k elements to another destination_
(not selected). Thus, the marginal distributions of the OT are:


_m_ _k_
**r =** _−_
_k_



2 destinations (2)



**c = [1** 1 _..._ 1]

_m items_

| {z }


-----

**Algorithm 1: Gumbel-Sinkhorn-TopK (GS-TopK) for Solving Cardinality Constrained**
**Combinatorial Optimization**
**Input: List s with m items; TopK size k; Sinkhorn factor τ** ; noise factor σ; sample size #G.

**1 for a ∈{1, 2, ..., #G} do**

**2** for all si, **si = si −** _σ log(−_ log(ui)), where ui is sampled from (0, 1) uniform distribution;

**s1** min(s) _..._ **sm** min(s) _m_ _k_
**3** **D =** max( e − **s)** **s1** _..._ max( −s) **sm** ; c = [1 1 _..._ 1]; r = _−k_ ;
 _−_ _−_  _m items_  
e e

**4** **Te** _a = exp(−D[e]_ _/τ_ )e; e | {z }

**5** **while not converged do**

**6** e **Dr = diag(T[e]** _a1_ **r);** **Ta =** **D[−]r** [1]Ta;
_⊘_

**7** **Dc = diag(T[e]** _[⊤]a_ **[1][ ⊘]** **[c][);]** **Ta =** **TaD[−]c** [1][;]

e e [e] e

**Output:e A list of transport matricese** [T[e] 1,[e]T2[e], ..., **T#G].**

We can design the destinations to be the min/max values of s, such that the TopK items are moved
to max(s), and the other items are moved to[e] min([e]s). Then, the distance matrix of OT is given as:


**D =** max(s1 − min(s) **ss)1** max(s2 − min(s) **ss)2** _......_ max(sm −smin() **ssm)**
 _−_ _−_ _−_

While the OT is formulated as an integer linear programming problem:


(3)


min _s.t._ **T** 0, 1 _, T1 = r, T[⊤]1 = c,_ (4)
**T** [tr(][T][⊤][D][)] _∈{_ _}[m][×][2]_

where T is the transportation plan which also corresponds to the TopK solution of the problem, and
**1 is a column vector whose all elements are 1s. The optimal solution T[∗]** should be equivalent to
the solution by firstly sorting all items and then selecting the TopK items. It is also equivalent to
the MAP inference by regarding s1, ..., sm as probabilities. Following the differentiable Sinkhorn
algorithm for OT (Cuturi, 2013; Sinkhorn & Rangarajan, 1964), the binary constraint on T is relaxed
to continuous values [0, 1], and Eq. (4) is modified with entropic regularization:

min _s.t._ **T[τ]** [0, 1][m][×][2], T[τ] **1 = r, T[τ]** _[⊤]1 = c,_ (5)
**T[τ][ tr(][T][τ]** _[⊤][D][) +][ τh][(][T][τ]_ [)] _∈_

where h(T[τ] ) = _i,j_ **[T]ij[τ]** [log][ T]ij[τ] [is the entropic regularizer (][Cuturi][,][ 2013][). Given any real-valued]

matrix D, Eq. (5) is solved by firstly enforcing the regularization factor τ : T[τ] = exp(−D/τ ). Then
**T[τ]** is row- and column-wise normalized alternatively:[P]

**Dr = diag(T[τ]** **1 ⊘** **r),** **T[τ]** = D[−]r [1][T][τ] _[,]_ **Dc = diag(T[τ]** _[⊤]1 ⊘_ **c),** **T[τ]** = T[τ] **D[−]c** [1][,] (6)

where ⊘ means element-wise division. We denote T[τ] _[∗]_ as the converged solution, which is the
optimal solution to Eq. (5).

**Theorem 2 of (Xie et al., 2020). Denote xk, xk+1 as the k-th (k + 1)-th largest items, we have**


2mτ log 2
**T[∗]** **T[τ]** _[∗]_ _F_ (7)
_||_ _−_ _||_ _≤_ _xk_ _xk+1_

_|_ _−_ _|_ _[,]_

_xwhere the gap is controlled byk+1, which is unavoidable because τ if |x xk −k, xxkk+1+1 are outputs by a neural network. It motivates us to| > 0. However, the above bound diverges if xk =_
develop our improved version Gumbel-Sinkhorn-TopK by introducing the Gumbel noise.

3.2 GUMBEL-SINKHORN-TOPK (GS-TOPK) ALGORITHM

In this section, we present our proposed Gumbel-Sinkhorn-TopK (GS-TopK) as summarized in
Alg. 1 and we theoretically characterize the gap between the MAP inference and the prediction
by GS-TopK. As the re-parameterization trick (Jang et al., 2017), instead of sampling from a distribution that is non-differentiable, we add random variables to probabilities predicted by neural
networks. The Gumbel distribution is characterized as:

_gσ(u) =_ _σ log(_ log(u)), (8)
_−_ _−_


-----

where σ controls the variance and u is from (0, 1) uniform distribution. We can update s and D as:

**si = si + gσ(ui)** (9)

**De** = max(s1 − min(s) **ss)1** max(s2 − min(s) **ss)2** _......_ max(sm −smin() **ssm)** _._ (10)
 _−_ _−_ _−_ 

Again we formulate the integer linear programming version of the OT with Gumbel noise:e e e e

e e e

min **D)** _s.t._ **T[σ]** 0, 1 _, T[σ]1 = r, T[σ][⊤]1 = c,_ (11)
**T[σ][ tr(][T][σ][⊤]** [e] _∈{_ _}[m][×][2]_

where the optimal solution to Eq. (11) is denoted as T[σ][∗]. Without loss of generality, in the following
we sort s1, s2, s3, ..., sm in descending order, into X = x1, x2, x3, ..., xm, and xk, xk+1 are the k-th
and (k + 1)-th largest items, respectively. We characterize the gap between T[∗] and T[σ][∗] under the
expectation over Gumbel distribution.

Now we present the following two lemmas to help establish our theoretical results, in terms of a
bounded gap between the MAP inference and the approximate solution by Gumbel-Sinkhorn-TopK,
**Lemma 1 (Bias by the Gumbel Noise). For the integer linear programming solutions with and**
_without Gumbel noises, we have_


4m

1 + exp _[|][x][k][−]σ[x][k][+1][|]_


Eu [||T[∗] _−_ **T[σ][∗]||F ] ≤**


(12)


_Proof sketch: The gap between T[∗], T[σ][∗]_ is derived by counting the expected number of times that,
after adding the Gumbel noise, an element may become larger than xk or smaller than xk+1. For
the top k elements, the probability that it will become smaller than xk+1 is upper bounded by
1/(1 + exp _[|][x][k][−]σ[x][k][+1][|]_ ). For the last (m _−_ _k) elements, the probability that it will become larger than_

_xk is also upper bounded by 1/(1 + exp_ _[|][x][k][−]σ[x][k][+1][|]_ ). Since ||T[∗] _−_ **T[σ][∗]||F[2]** [changes at most by 4 if]

an item becomes larger/smaller than xk/xk+1, and we have m items, then we can proof the upper
bound of this gap. The detailed proof is referred to Appendix A.1.

To make the integer linear programming problem feasible for gradient-based deep learning methods,
we also relax the integer constraint and add the entropic regularization term:

minT tr(T[e] _[⊤]D[e]_ ) + h(T[e] ) _s.t._ **T ∈** [0, 1][m][×][2], **T1 = r,** **T[⊤]1 = c,** (13)

which is solved by Sinkhorn algorithm one **D: firstlye** **T = exp(** **D[e]** _/τ_ ), then[e]
_−_ [e]

**Dr = diag(T1[e]** _⊘_ **r),** **T =** **D[−]r[e][1]T,** **Dc[e] = diag(T[e]** _[⊤]1 ⊘_ **c),** **T =** **TD[−]c** [1][.] (14)

Here we denote the optimal solution to Eq. (13) as **T[∗]. We theoretically characterize the gap between**

e e [e] e e e [e] [e]

**T[σ][∗]** and **T[∗], which corresponds to the optimal solutions of Eq. (11) and Eq. (13), respectively.**

**Lemma 2 (Gap to a Discrete Solution). Under probability[e]** (1 − _ϵ), we have_

[e]

Eu _||T[σ][∗]_ _−_ **T[e]** _[∗]||F_ _≤_ (log 2)mτ Ω(xi, xj, σ, ϵ), (15)

_i=j_

h i X̸

_where_


Ω(xi, xj, σ, ϵ) = 2σ log _σ −_ _[|][x]log(1[i][−][x][j]−[|][+2]ϵ)_ _[σ]_ + |xi − _xj|_ _π2_ [+ arctan][ x][i]2[−]σ[x][j]

(1 _ϵ)((xi_ _xj)[2]_ + 4σ[2])(1 + exp _[x][i][−]σ[x][k]_ )(1 + exp _[x][k][+1]σ[−][x][j]_
_−_ _−_


(16)


_Proof sketch: The gap between T[σ][∗]_ and **T[∗]** is a generalization from the Theorem 2 of

Xie et al. (2020). We denote xπk _, xπk+1 as the k-th and (k + 1)-th largest items af-_
ter disturbed by the Gumbel noise, and our aim becomes to prove the upper bound

[e]

_gofσ E(uπuk_ )1/(|gxσπ(ku +πk g+1σ)( can be bounded byuπk ) − _xπk+1 −_ _gσ( fuπ(ky+1) = 1)|)_, where the probability density function of/(y[2] + 4). Thus we can compute the upper
 _−_ 
bound under probability (1 − _ϵ) by integration. The detailed proof is referred to Appendix A.2._


-----

Graph Modeling

of the Problem

Gumbel noiseGumbel noise


Add Gumbel

Noise


Graph Neural

Network


input graph graph with probabilities

Objective score

|SOFT-TopK (k=2)|Col2|
|---|---|


probabilitiesprobabilities


probabilities with Gumbel noiseprobabilities with Gumbel noise TopK solution

Forward pass

Backward pass

Figure 2: Our proposed self-supervised learning pipeline for combinatorial optimization. The probability of selecting each element is predicted by a graph neural network, and an accurate estimation
of the combinatorial objective score is achieved via near-discrete TopK solution by our GS-TopK.

**Theorem 1. For the proposed Gumbel TopK solver, denote T[∗]** _as the solution by deterministic TopK_
_algorithm,_ **T[∗]** _as the solution of Gumbel-Sinkhorn-TopK. With probability (1 −_ _ϵ), we have_

4m

E[e]u **T[∗]** **T[∗]** + (log 2)mτ Ω(xi, xj, σ, ϵ), (17)
h _−_ [e] _F_ i _≤s_ 1 + exp _[|][x][k][−]σ[x][k][+1][|]_ Xi≠ _j_

_where σ is the noise factor and τ is the temperature of Sinkhorn algorithm. m is the number of_
_candidates to be selected. Ω(xi, xj, σ, ϵ) is defined in Eq. (16)._

_Proof: Based on Lemma 1 and Lemma 2, Theorem 1 is proved by triangle inequality:_


Eu **T[∗]** **T[∗]** Eu [ **T[∗]** **T[σ][∗]** _F ] + Eu_ **T[σ][∗]** **T[∗]** _F_
_−_ [e] _F_ _≤_ _||_ _−_ _||_ _||_ _−_ [e] _||_
h i h i

**Remarks. As derived above, the gap between**
**T[∗]** and **T is composed of two parts: the bias** 2.00
by the Gumbel noise (in Lemma 1) and the gap
to a discrete solution (in Lemma 2). Interest- 1.75

[e]
ingly, the first term becomes smaller given a 1.50
smaller σ, and the second term is smaller given 1.25
a larger σ or a smaller τ . See the toy example
in Fig. 1, a larger σ can tighten the gap of GS- GS-TopK ( = 0.050)
TopK to a discrete solution, which is welcomed 0.75 GS-TopK ( = 0.010)GS-TopK ( = 0.005)
in our combinatorial optimization learning sce- Gap to Discrete Solution 0.50 GS-TopK ( = 0.001)
nario where **T[∗]** is used to compute the objective 0.25 SOFT-TopK ( = 0.010)
score in a self-supervised learning pipeline. SOFT-TopK ( = 0.005)

If _xk_ _xk+1[e]_ _> 0, with σ_ 0[+], Eq. (17) 10 3 10 2 10 1
degenerates to the bound derived by ( | _−_ _|_ _→_ Xie et al., (log scale)

Figure 1: A toy example to explain Lemma 2: se
2.00

1.75

1.50

1.25

1.00

GS-TopK ( = 0.050)

0.75 GS-TopK ( = 0.010)

GS-TopK ( = 0.005)

Gap to Discrete Solution 0.50 GS-TopK ( = 0.001)

SOFT-TopK ( = 0.050)

0.25 SOFT-TopK ( = 0.010)

SOFT-TopK ( = 0.005)

0.00 SOFT-TopK ( = 0.001)

10 3 10 2 10 1

(log scale)

2020) and only differs by a constant factor (see

lect top-3 items from [1.0, 0.8, 0.601, 0.6, 0.4, 0.2]

Appendix A.3 for details):

and we compare GS-TopK and SOFT-TopK con
(π log 2)mτ cerning the gap to a discrete solution w.r.t. differlim **T[∗]** **T[∗]** _F_ ent τ, σ configurations. Here the gap to discrete
_σ→0[+][ E][u]_ h|| _−_ [e] _||_ i _≤_ (1 − _ϵ)|xk_ _−xk+1|_ solutions is tighten by a larger σ and a smaller

_τ for GS-TopK, compared to SOFT-TopK whose_

It makes a strong assumption that _xk_ _xk+1_ _>_
_|_ _−_ _|_ gap is larger and can only be controlled by τ .
0, and the bound diverges if xk = xk+1. Unfortunately, xk, xk+1 are predictions by a neural network, whereby such an assumption may not be
satisfied. In comparison, given σ > 0, our conclusion in Eq. (17) is bounded for any xk, xk+1.

3.3 APPLICATION TO SELF-SUPERVISED COMBINATORIAL OPTIMIZATION LEARNING
In the following, we discuss the application of GS-TopK to self-supervised combinatorial optimization learning, by taking the max covering problem as an example. We consider the max covering
problem with m sets and n objects. Each set may cover any number of objects, and each object is


-----

**Algorithm 2: Gumbel-Sinkhorn-TopK Learning for Solving the Max Covering Problem**
**Input: bipartite adjacency A; values v; learning rate α; GS-TopK parameters k, τ, σ, #G.**

**1 if Training then**

**2** Randomly initialize neural network weights θ;

**3 if Inference then**

**4** Load a pretrained neural network weights θ; Jbest = 0;


**5 while not converged do**

**6** **s = GraphSageθ(A); [T[e]** 1, **T2, ...,** **T#G] = GS-TopK(s, k, τ, σ, #G);**

**7** for all i, _Ji = min(T[e]_ _i[2, :]A, 1) · v; J = mean([J1,_ _J2, ...,_ _J#G]);_

**8** **if Training then** [e] [e]

**9** update[e] θ with respect to the gradient _[∂J]∂θ_ [and learning rate][e] [e] [e][ α][ by gradient ascent;]

**10** **if Inference then**

**11** update s with respect to the gradient _[∂J]∂s_ [and learning rate][ α][ by gradient ascent;]

**12** for all i, _Ji = TopK(T[e]_ _i[2, :])A · v; Jbest = max([J1,_ _J2, ...,_ _J#G], Jbest);_

**13 if Homotopy Inference then**

**14** Shrink the values of[e] _τ, σ and jump to line 5;_ [e] [e] [e]

**Output: Learned network weights θ (if training)/The best objective Jbest (if inference).**

associated with a value. We aim to find k sets (where k < m) such that the covered objects have the
maximum sum of values. The general illustration of our pipeline can be found in Fig. 2.

Firstly, we build a bipartite graph whose two disjoint sets of vertices are the sets and the objects.
An edge is defined if an object is covered by a set. Denote v ∈ R[n] as the value of each object, and
**A ∈{0, 1}[m][×][n]** as the adjacency matrix, the problem is formulated as


_s.t._ **x** 0, 1 _,_ **x** 0 _k_ (18)
_∈{_ _}[m]_ _||_ _||_ _≤_


max
**x**

_j=1_

X


**xiAij**
_i=1_

X


**vj**

_·_


we exploit three layers of GraphSage (where I(x) is an indicator I(x)i = 1 ifHamilton et al. xi ≥ 1 else I, 2017(x)i = 0) followed by a fully-connected layer. To encode the bipartite graph,
with sigmoid to predict the probability of selecting each set, which is denoted as s ∈ [0, 1][m]. The
probabilities s are fed into the GS-TopK algorithm, which outputs a batch of near-discrete transportation matrices [T[e] 1, **T2, ...,** **T#G], whose second row is regarded as a continuous approximation**
of the decision variables. The objective value is estimated as:

_Ji = min([e]_ **T[e][e]i[2, :]A, 1) · v,** _J = mean([J1,_ _J2, ...,_ _J#G])_ (19)

For the discrete clustering problem, we aim to selecte _k objects from the full set of[e]_ [e] [e] _m objects, by_
minimizing the sum of distance for each object between itself and its nearest selected object. It
differs from the k-means clustering by adding the “discrete” constraint that cluster centers have to
be a subset of objects, which is also known as facility location problem in operations research (Liu,
2009). Denote ∆ _∈_ R[+][m][×][m] as the distance matrix for each pair of objects, we formulate the
discrete clustering problem as follows (please refer to Appendix B for details):


min min( **∆i,j** **xi = 1** ) _s.t._ **x** 0, 1 _,_ _x_ 0 _k_ (20)
**x** _j=1_ _{_ _| ∀_ _}_ _∈{_ _}[m]_ _||_ _||_ _≤_

X


As illustrated in Alg. 2, during training, the Adam optimizer (Kingma & Ba, 2014) is applied for
learning the neural network weights. During inference, the neural network prediction is regarded
as initialization, and we again optimize the distribution w.r.t. the combinatorial objective score by
Adam, during which a search procedure is also performed among all Gumbel samples.

**Homotopy GS-TopK. Based on our Theorem 1, the gap between T[∗]** and **T[∗]** can be tightened
by shrinking τ and σ, which motivates us to develop a homotopy variant (Xiao & Zhang, 2012;
Xu et al., 2016) of GS-TopK with gradually decreased τ, σ, such that **T[∗]** and[e]J can provide more
accurate estimation during inference. See line 13 in Alg. 2 for details.

[e] [e]


-----

20.0

TopK

17.5 EGN

15.0

12.5

10.0

objective score 7.5

5.0

2.5

0.0

0 250 500 750 1000 1250 1500 1750

# iterations


20.0

TopK

17.5 SOFT-TopK

15.0

12.5

10.0

objective score 7.5

5.0

2.5

0.0

0 250 500 750 1000 1250 1500 1750

# iterations


20.0

TopK

17.5 Homotopy GS-TopK

15.0

12.5

10.0

objective score 7.5

5.0

2.5

0.0

0 250 500 750 1000 1250 1500 1750

# iterations


(a) Erdos Goes Neural (b) SOFT-TopK w/ sampling (c) Homotopy GS-TopK

Figure 3: In discrete clustering, comparison of the estimated objective score (red) and the real one
(blue). Our GS-TopK achieves the best accurate estimation of the objective score.

4 EXPERIMENTS AND DISCUSSION

4.1 PROTOCOLS AND BASELINES

We apply the proposed GS-TopK algorithm to learning two representative problems of cardinality
constrained NP-hard combinatorial optimization: i) max covering and ii) discrete clustering problem. For each problem, both a smaller-sized case and a larger-sized case are considered, and we build
separate training/testing datasets for learning methods. In this paper, we referred to randomly generated datasets following previous machine learning for combinatorial optimization papers (Khalil
et al., 2017; Kool et al., 2019) concerning the limited large-scale open-source dataset. For the max
covering problem, our data distribution follows the distribution in ORLIB[1]. For the discrete clustering problem, the data points are uniformly generated in a 2D plane.

We compare the following representative and diverse baselines with our proposed method:

**Classic Algorithms. Classic algorithms are well-developed strong baselines with theoretical guar-**
antees. In our experiment, we consider the greedy algorithm as the baseline for the max covering
problem, which has the worst-case approximation ratio of (1 − 1/e) due to the submodularity of
the max covering problem (Fujishige, 1991). For the discrete clustering problem, we resort to the
classic kmeans (Macqueen, 1967) and kmeans++ (Arthur & Vassilvitskii, 2007) with improved initialization techniques. These algorithms are fast but effective, even surpassing commercial solvers
with only 1/1000 inference time on certain test cases.

**General Purpose Solvers. We also compare two general-purpose solvers Gurobi 9.0 (under educa-**
tional license) (Gurobi Optimization, LLC, 2021) and SCIP 7.0 (Gamrath et al., 2020). Gurobi is
the state-of-the-art commercial solver for general combinatorial optimization tasks, and it is known
heavily engineered by various speedup techniques. SCIP is the state-of-the-art open-source solver,
which is also regarded as a baseline. We formulate the combinatorial problems as integer programming and then call the solver to solve the problems, with a limited time budget. Our implementation
of the solvers is with the Google ORTools API [2].

**One-shot Learning Baselines. We compare with the general Erdos Goes Neural (EGN) (Karalias &**
Loukas, 2020) framework that models each decision variable as a Bernoulli distribution and encodes
the constraint as part of the learning objective. Since the authors of (Karalias & Loukas, 2020) do not
consider cardinality constrained problems in their experiment, we re-implement and tune the EGN
model based on their official implementation[3]. We also compare with SOFT-TopK (Xie et al., 2020),
which is theoretically characterized as a special case of our proposed GS-TopK when σ = 0. Also
to ensure the effectiveness of the re-parameterization trick, we facilitate SOFT-TopK with Gumbel
sampling during inference. All one-shot learning methods are with the same model structure for a
fair comparison and are trained and tested on separate datasets. Besides, we empirically find the
learning process of one-shot methods converges within tens of minutes. Since the reinforcement
learning methods (Khalil et al., 2017; Chen & Tian, 2019) require significantly more training time,
we only include one-shot methods for a fair comparison.

[1http://people.brunel.ac.uk/˜mastjjb/jeb/orlib/scpinfo.html](http://people.brunel.ac.uk/~mastjjb/jeb/orlib/scpinfo.html)
[2https://developers.google.com/optimization](https://developers.google.com/optimization)
[3https://github.com/Stalence/erdos_neu](https://github.com/Stalence/erdos_neu)


-----

Table 1: Objective score ↑ and inference time (in seconds) ↓ comparison of the max covering problem, including mean and standard deviation. Under cardinality constraint k, the problem is to select
from m sets to cover a fraction of n objects. Gurobi solver fails to return the optimal solution within
24 hours, thus reported as out-of-time. Our GS-TopK outperforms all competing methods, with
comparable or smaller time costs. The performance is further improved by Homotopy GS-TopK.

**k=50, m=500, n=1000** **k=100, m=1000, n=2000**
EGN/SOTF-TopK/ours are one-shot solvers **objective ↑** **time ↓** **(sec)** **objective ↑** **time ↓** **(sec)**

**greedy** 44312.8±818.4 **0.024±0.000** 88698.9±1217.5 **0.089±0.001**

**SCIP 7.0 (faster)** 43034.7±869.2 30.058±0.017 86269.9±1256.3 59.916±1.752
**SCIP 7.0 (slower)** 43497.4±875.6 100.136±0.097 86269.9±1256.3 120.105±0.498
**Gurobi 9.0 (faster)** 43261.6±856.1 30.078±0.014 85992.4±1643.7 60.168±0.042
**Gurobi 9.0 (slower)** 43937.2±791.5 100.171±0.085 86862.1±1630.5 120.277±0.139

**Gurobi 9.0 (optimal)** OOT OOT OOT OOT

**EGN (efficient) (Karalias & Loukas, 2020)** 36423.7±1128.4 0.244±0.107 70336.7±1676.9 0.525±0.229
**EGN (accurate) (Karalias & Loukas, 2020)** 36927.3±1163.0 40.542±4.056 71487.5±1552.8 93.670±8.797
**SOFT-TopK (Xie et al., 2020)** 41959.9±803.4 48.605±5.783 83211.7±1322.3 45.252±0.713
**SOFT-TopK (w/ sampling)** 39684.8±675.6 44.960±4.390 77285.2±1123.9 59.488±0.143
**GS-TopK (ours)** 44710.3±770.9 32.839±3.227 89264.8±1232.1 60.685±0.045
**Homotopy GS-TopK (ours)** **44718.2±745.2** 47.627±4.247 **89294.3±1211.2** 89.764±0.128

Table 2: Objective score ↓, optimal gap ↓ and inference time (in seconds) ↓ comparison of the
discrete clustering problem, with mean and standard deviation. The problem is to select k clustering
centers from m data points. GS-TopK and Homotopy GS-TopK surpasses all competing methods
with less time compared to general-purpose solvers and other one-shot methods.

**k=30, m=500** **k=50, m=800**
EGN/SOTF-TopK/ours are one-shot solvers **objective ↓** **optimal gap ↓** **time ↓** **(sec)** **objective ↓** **optimal gap ↓** **time ↓** **(sec)**

**kmeans++ (Arthur & Vassilvitskiikmeans (Macqueen,, 1967 2007))** 3.0202.854±±0.2000.166 0.2140.169±±0.0490.043 0.0530.042±±0.0740.007 2.6932.905±±0.1590.101 0.2330.174±±0.0420.027 0.5040.085±±0.2340.164

**Gurobi 9.0 (optimal)Gurobi 9.0 (slower)Gurobi 9.0 (faster)SCIP 7.0 (slower)SCIP 7.0 (faster)** 4.6413.3652.3654.4702.453±±±±±1.8801.9180.3410.1420.063 0.3770.3480.2900.0330.000±±±±±0.2880.2950.0720.0420.000 71.738118.06880.582125.589314.798±±±±±13.1920.86148.0550.606116.858 5.2583.3642.2215.4503.532±±±±±0.6741.0180.3580.2680.041 0.5870.5520.3650.3350.000±±±±±0.0460.1460.0650.0550.000 218.426243.919116.446214.360648.213±±±±±57.53054.1185.0873.785194.486

**EGN (accurate) (EGN (efficient) (SOFT-TopK (Homotopy GS-TopK (ours)SOFT-TopK (w/ sampling)Karalias & LoukasKaralias & LoukasGS-TopK (ours)Xie et al.,,, 2020 2020 2020)))** 3.0322.7952.7192.7172.4202.418±±±±±±0.1950.1400.1360.1180.0720.076 **0.0220.2170.1520.1290.1290.023±±±±±±0.0480.0350.0370.0270.0090.010** 0.830123.55997.147129.85676.534103.742±±±0.308±±±6.5656.32112.2787.2564.778 2.8152.5932.6002.2832.2732.865±±±±±±0.1380.1240.1040.0860.0500.047 **0.0230.2230.2090.1450.1430.027±±±±±±0.0360.0340.0290.0260.0080.007** 0.988191.091120.835166.132120.689158.400±0.140±±±±±13.1417.2988.7012.4053.498

4.2 EXPERIMENTAL RESULTS

Table 1 and Table 2 report results on max covering and discrete clustering problems, respectively.
An interesting finding is that the classic algorithms are efficient but also very effective. However, if
we can afford a higher time budget, there seems no universal approach to improve the results of the
classic algorithms. In comparison, Gurobi and SCIP solvers can trade-off time for better accuracy,
but they are less effective than our learning methods when considering the objective scores under
the same time budget. Among one-shot learning methods, Erdos Goes Neural (Karalias & Loukas,
2020) is the only method that does not encode the constraint in the model output, and the experimental results suggest that it is appealing if we can encode the constraint in the model output. Directly
applying Gumbel sampling to SOFT-TopK seems either an effective improvement, and satisfying
results are achieved when exploiting the Gumbel re-parameterization trick and enabling gradientbased optimization over the combinatorial objective score. We further validate the effectiveness of
Homotopy GS-TopK, where the objective scores are improved at the cost of more inference time.

**Further Discussions. Our insight for the effectiveness of GS-TopK over other state-of-the-art one-**
shot learning methods (Karalias & Loukas, 2020; Xie et al., 2020) is that it can provide a more
accurate estimation of the combinatorial objective score with discrete decision variables. Fig. 3
shows that our GS-TopK estimates a lower bound of the true objective score at the beginning 200
iterations while providing an accurate estimation afterward. Besides, the Homotopy version of our
algorithm can gradually reduce the variance and help converge to a better solution.


-----

REPRODUCIBILITY STATEMENT

The following efforts are made to ensure the reproducibility of this paper:

_• We provide a complete proof about our Lemmas and Theorems in the Appendix, see Ap-_
pendix A;

_• We discuss the hyper-parameter configurations and the experiment setup in Sec 4.1;_

_• We provide the implementation details on max covering problem in Sec. 3.3, and the im-_
plementation details on discrete problem problem in Appendix B;



_• We discuss the way of generating random dataset in Sec 4.1;_

_• The code will be made publicly available once this paper is accepted._

REFERENCES

Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein GAN. _arXiv e-prints, art._
arXiv:1701.07875, January 2017.

David Arthur and Sergei Vassilvitskii. K-means++: The advantages of careful seeding. In Proceed_ings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’07, pp._
1027–1035, USA, 2007. ISBN 9780898716245.

Yunsheng Bai, Hao Ding, Song Bian, Ting Chen, Yizhou Sun, and Wei Wang. Simgnn: A neural network approach to fast graph similarity computation. In Proceedings of the Twelfth ACM
_International Conference on Web Search and Data Mining, pp. 384–392, 2019._

Zal´an Borsos, Mojm´ır Mutn´y, and Andreas Krause. Coresets via bilevel optimization for continual
learning and streaming. Neural Info. Process. Systems, 2020.

Wei Chen, Xiaoming Sun, Jialin Zhang, and Zhijie Zhang. Network inference and influence maximization from samples. In Int. Conf. Mach. Learn., July 2021.

Xinyun Chen and Yuandong Tian. Learning to perform local rewriting for combinatorial optimization. Neural Info. Process. Systems, 32:6281–6292, 2019.

Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. _Neural Info._
_Process. Systems, pp. 2292–2300, 2013._

Marco Cuturi, Olivier Teboul, and Jean-Philippe Vert. Differentiable ranking and sorting using
optimal transport. In Advances in Neural Information Processing Systems, volume 32, pp. 6858–
6868, 2019.

Hanjun Dai, Bo Dai, and Le Song. Discriminative embeddings of latent variable models for structured data. In Int. Conf. Mach. Learn., pp. 2702–2711. PMLR, 2016.

Lu Duan, Haoyuan Hu, Yu Qian, Yu Gong, Xiaodong Zhang, Jiangwen Wei, and Yinghui Xu. A
multi-task selected learning approach for solving 3d flexible bin packing problem. In Proceedings
_of the 18th International Conference on Autonomous Agents and MultiAgent Systems, pp. 1386–_
1394, 2019.

Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In
_ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019._

Matthias Fey, Jan Eric Lenssen, Frank Weichert, and Heinrich M¨uller. SplineCNN: Fast geometric
deep learning with continuous b-spline kernels. In Comput. Vis. Pattern Recog., pp. 869–877,
2018.

Matthias Fey, Jan E Lenssen, Christopher Morris, Jonathan Masci, and Nils M Kriege. Deep graph
matching consensus. In Int. Conf. Learn. Rep., 2020.

Satoru Fujishige. Submodular Functions and Optimization. Elsevier, 1991.


-----

Gerald Gamrath, Daniel Anderson, Ksenia Bestuzheva, Wei-Kun Chen, Leon Eifler, Maxime Gasse,
Patrick Gemander, Ambros Gleixner, Leona Gottwald, Katrin Halbig, Gregor Hendel, Christopher Hojny, Thorsten Koch, Pierre Le Bodic, Stephen J. Maher, Frederic Matter, Matthias Miltenberger, Erik M¨uhmer, Benjamin M¨uller, Marc E. Pfetsch, Franziska Schl¨osser, Felipe Serrano,
Yuji Shinano, Christine Tawfik, Stefan Vigerske, Fabian Wegscheider, Dieter Weninger, and Jakob
Witzig. The SCIP Optimization Suite 7.0. Technical report, Optimization Online, March 2020.
[URL http://www.optimization-online.org/DB_HTML/2020/03/7705.html.](http://www.optimization-online.org/DB_HTML/2020/03/7705.html)

Aditya Grover, Eric Wang, Aaron Zweig, and Stefano Ermon. Stochastic optimization of sorting
networks via continuous relaxations. In Int. Conf. Learn. Rep., 2019.

[Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2021. URL https://www.](https://www.gurobi.com)
[gurobi.com.](https://www.gurobi.com)

William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large
graphs. Neural Info. Process. Systems, 2017.

Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In
_Int. Conf. Learn. Rep., 2017._

Nikolaos Karalias and Andreas Loukas. Erdos goes neural: an unsupervised learning framework for
combinatorial optimization on graphs. In Neural Info. Process. Systems, 2020.

Elias Khalil, Hanjun Dai, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial optimization algorithms over graphs. In Neural Info. Process. Systems, pp. 6351–6361, 2017.

Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Int. Conf. Learn.
_Rep., Dec 2014._

Diederik P Kingma and Max Welling. Auto-Encoding Variational Bayes. _arXiv e-prints, art._
arXiv:1312.6114, December 2013.

Wouter Kool, Herke van Hoof, and Max Welling. Attention, learn to solve routing problems! In Int.
_Conf. Learn. Rep., pp. 1–25, 2019._

Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and Pushmeet Kohli. Graph matching networks for learning the similarity of graph structured objects. In International Conference on
_Machine Learning, pp. 3835–3845, 2019._

Baoding Liu. Facility Location Problem, pp. 157–165. Springer Berlin Heidelberg, Berlin, Heidelberg, 2009. ISBN 978-3-540-89484-1. doi: 10.1007/978-3-540-89484-1 11.

Chang Liu, Runzhong Wang, Zetian Jiang, and Junchi Yan. Deep reinforcement learning of graph
matching. arXiv preprint arXiv:2012.08950, 2020.

Hao Lu, Xingwen Zhang, and Shuang Yang. A learning-based iterative method for solving vehicle
routing problems. In Int. Conf. Learn. Rep., 2019.

J. Macqueen. Some methods for classification and analysis of multivariate observations. In In 5-th
_Berkeley Symposium on Mathematical Statistics and Probability, pp. 281–297, 1967._

Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. Int. Conf. Learn. Rep., 2017.

Hongzi Mao, Malte Schwarzkopf, Shaileshh Bojja Venkatakrishnan, Zili Meng, and Mohammad
Alizadeh. Learning scheduling algorithms for data processing clusters. In Proceedings of the
_ACM Special Interest Group on Data Communication, pp. 270–288, 2019._

G. Mena, D. Belanger, S. Linderman, and J. Snoek. Learning latent permutations with gumbelsinkhorn networks. Int. Conf. Learn. Rep., 2018.

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. _arXiv preprint_
_arXiv:1312.5602, 2013._


-----

Giorgio Patrini, Rianne van den Berg, Patrick Forr´e, Marcello Carioni, Samarth Bhargav, Max
Welling, Tim Genewein, and Frank Nielsen. Sinkhorn autoencoders. In Amir Globerson and
Ricardo Silva (eds.), Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial In_telligence, UAI 2019, pp. 253, 2019._

Max B Paulus, Dami Choi, Daniel Tarlow, Andreas Krause, and Chris J Maddison. Gradient estimation with stochastic softmax tricks. Neural Info. Process. Systems, 2020.

Yun Peng, Byron Choi, and Jianliang Xu. Graph learning for combinatorial optimization: A survey
of state-of-the-art. Data Science and Engineering, 6(2):119–141, 2021. ISSN 2364-1185, 23641541. doi: 10.1007/s41019-021-00155-3.

Felix Petersen, Christian Borgelt, Hilde Kuehne, and Oliver Deussen. Differentiable sorting networks for scalable sorting and ranking supervision. In International Conference on Machine
_Learning (ICML), 2021._

R. Santa Cruz, B. Fernando, A. Cherian, and S. Gould. Visual permutation learning. Trans. Pattern
_Anal. Mach. Intell., 41(12):3100–3114, 2018._

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

R. Sinkhorn and A. Rangarajan. A relationship between arbitrary positive matrices and doubly
stochastic matrices. Ann. Math. Statistics, 1964.

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks. In Neural Info. Process.
_Systems, pp. 2692–2700, 2015._

R. Wang, J. Yan, and X. Yang. Learning combinatorial embedding networks for deep graph matching. In Int. Conf. Comput. Vis., pp. 3056–3065, 2019.

Runzhong Wang, Zhigang Hua, Gan Liu, Jiayi Zhang, Junchi Yan, Feng Qi, Shuang Yang, Jun Zhou,
and Xiaokang Yang. A bi-level framework for learning to solve combinatorial optimization on
graphs. In Neural Info. Process. Systems, 2021a.

Runzhong Wang, Junchi Yan, and Xiaokang Yang. Neural graph matching network: Learning
lawler’s quadratic assignment problem with extension to hypergraph and multiple-graph matching. Trans. Pattern Anal. Mach. Intell., 2021b.

Lin Xiao and Tong Zhang. A proximal-gradient homotopy method for the sparse least-squares
problem, 2012.

Yujia Xie, Hanjun Dai, Minshuo Chen, Bo Dai, Tuo Zhao, Hongyuan Zha, Wei Wei, and Tomas
Pfister. Differentiable top-k with optimal transport. In Neural Info. Process. Systems, volume 33,
pp. 20520–20531. Curran Associates, Inc., 2020.

Yi Xu, Yan Yan, Qihang Lin, and Tianbao Yang. Homotopy smoothing for non-smooth problems
with lower complexity than o(1/ϵ). Neural Info. Process. Systems, 2016.

Tianshu Yu, Runzhong Wang, Junchi Yan, and Baoxin Li. Learning deep graph matching with
channel-independent embedding and hungarian attention. In Int. Conf. Learn. Rep., 2020.

Cong Zhang, Wen Song, Zhiguang Cao, Jie Zhang, Puay Siew Tan, and Xu Chi. Learning to dispatch
for job shop scheduling via deep reinforcement learning. Neural Info. Process. Systems, 33, 2020.


-----

A PROOF OF THEOREMS

Before starting the detailed proof of the lemmas and theorems, firstly we recall the notations used in
this paper:

_• T[∗]_ = TopK(D) is the optimal solution of the integer linear programming form of the OT
problem Eq. (4), which is equivalent to the solution by firstly sorting all items and then
select the TopK items;

_• T[τ]_ _[∗]_ = Sinkhorn(D) is the optimal solution of the entropic regularized form of the OT
problem Eq. (5) solved by Sinkhorn algorithm. It is also the output by SOFT-TopK (Xie
et al., 2020);

_• T[σ][∗]_ = TopK(D) is the optimal solution of the integer linear programming form of the OT
problem after disturbed by the Gumbel noise Eq. (11), which is equivalent to the solution
by firstly adding the Gumbel noise, then sorting all items and finally select the TopK items;

[e]

_•_ **T[∗]** = Sinkhorn(D) is the optimal solution of the entropic regularized form of the OT
problem after disturbed by the Gumbel noise Eq. (13) solved by Sinkhorn algorithm. It is
also the output by our proposed GS-TopK.

[e] [e]

**Lemma 3. Given real numbers xi, xj, and ui, uj are from i.i.d. (0, 1) uniform distribution. After**
_Gumbel perturbation, the probability that xi + gσ(ui) > xj + gσ(uj) is:_


(21)


_P_ (xi + gσ(ui) > xj + gσ(uj)) =


1 + exp _σ_
_−_ _[x][i][−][x][j]_


_Proof. Since gσ(ui) = −σ log(−_ log(ui)), P (xi + gσ(ui) > xj + gσ(uj)) is equivalent to the
probability that the following inequality holds:


_xi_ _σ log(_ log(ui)) > xj _σ log(_ log(uj)) (22)
_−_ _−_ _−_ _−_

_xi −_ _xj > σ log(−_ log(ui)) − _σ log(−_ log(uj)) (23)

_xi_ _xj_ log(ui)
_−_ _> log_ (24)

_σ_ log(uj)
 

_xi_ _−xj_

_e_ _σ_ _>_ [log(][u][i][)] (25)

log(uj)


And we have


Since uj (0, 1), log(uj) < 0. Then we have
_∈_

_xi_ _−xj_
log(uj) < log(ui)e[−] _σ_

log (uj) < log _uexpi_ _−_ _xi_ _−σxj_
 

_uj < uexpi_ _−_ _xi_ _−σxj_

Since ui, uj are i.i.d. uniform distributions, the probability when the above formula holds is


(26)

(27)

(28)

(29)

(30)


_uexpi_ _−_ _xi_ _−σxj_

0

Z


1

0

Z


1
_duj dui =_

0

Z


exp − _xi_ _−σxj_


_dui =_


1 + exp _σ_
_−_ _[x][i][−][x][j]_


Thus the probability that xi + gσ(ui) > xj + gσ(uj) after Gumbel perturbation is:


_P_ (xi + gσ(ui) > xj + gσ(uj)) =


1 + exp _σ_
_−_ _[x][i][−][x][j]_


-----

A.1 PROOF OF LEMMA 1

_Proof of Lemma 1. If one of the top-k items becomes smaller than xk+1 after distorted by the Gum-_
bel noise, then ||T[∗] _−_ **T[σ][∗]||F increases at most by 2. Without loss of generality, here we assume xi**
as one of the top-k items i.e. i ≤ _k. According to Lemma 3, the probability that xi becomes smaller_
than xk+1 after perturbation is


(31)


_P_ (xi + gσ(ui) < xk+1 + gσ(uk+1)) =


1 + exp _σ_
_−_ _[x][k][+1][−][x][i]_


Similarly, if one of the last-(m − _k) items becomes larger than xk after distorted by the Gumbel_
noise, then ||T[∗] _−_ **T[σ][∗]||F also increases at most by 2. Without loss of generality, here we assume**
_xj as one of the last-(m −_ _k) items i.e. j ≥_ _k + 1. According to Lemma 3, the probability that xj_
becomes larger than xk after perturbation is


(32)

(33)


_P_ (xj + gσ(uj) > xk + gσ(uk)) =


1 + exp _σ_
_−_ _[x][j]_ _[−][x][k]_


To compute the expectation of ||T[∗] _−_ **T[σ][∗]||, we sum all probabilities and we have**


Eu **T[∗]** **T[σ][∗]** _F_
_||_ _−_ _||[2]_
h


1 + exp _σ_
_−_ _[x][k][+1][−][x][i]_


1 + exp _σ_
_−_ _[x][j]_ _[−][x][k]_


_i=1_


_j=k+1_


where the constant 4 appears because ||T[∗] _−_ **T[σ][∗]||F[2]** [will change at most by 4 if one more item]
crosses the boundary. Since for all i ≤ _k and j ≥_ _k + 1, we have_

_xk+1_ _xi_ _xk+1_ _xk_ (34)
_−_ _≤_ _−_
_xj_ _xk_ _xk+1_ _xk_ (35)
_−_ _≤_ _−_

Thus we have


Eq. (33) ≤

_i=1_

X

_m_

=

_i=1_

X


(36)

(37)

(38)

(39)


1 + exp _σ_
_−_ _[x][k][+1][−][x][k]_

4

1 + exp _σ_
_−_ _[x][k][+1][−][x][k]_

4m


1 + exp _σ_
_−_ _[x][k][+1][−][x][k]_


_j=k+1_


1 + exp _[|][x][k][−]σ[x][k][+1][|]_


Eu **T[∗]** **T[σ][∗]** _F_, we have
_||_ _−_ _||[2]_
h i


Since Eu [||T[∗] _−_ **T[σ][∗]||F ] ≤**


4m

1 + exp _[|][x][k][−]σ[x][k][+1][|]_


Eu [||T[∗] _−_ **T[σ][∗]||F ] ≤**

A.2 PROOF OF LEMMA 2


_Proof of Lemma 2. By disturbing X with i.i.d. Gumbel noise, we have_

_X_ _[′]_ = x1 + gσ(u1), x2 + gσ(u2), x3 + gσ(u3), ..., xm + gσ(um) (40)

where gσ(u) = _σ log(_ log(u)) is the Gumbel noise modulated by noise factor σ, and
_−_ _−_
_u1, u2, u3, ..., um are i.i.d. uniform distribution. We define π as the permutation of sorting X_ _[′]_ in
descending order, i.e. xπ1 + gσ(uπ1 ), xπ2 + gσ(uπ2 ), xπ3 + gσ(uπ3 ), ..., xπm + gσ(uπm) are in
descending order.


-----

Recall Theorem 2 in (Xie et al., 2020), for x1, x2, x3, ..., xm we have

2mτ log 2
**T[∗]** **T[τ]** _[∗]_ _F_ (41)
_||_ _−_ _||_ _≤_ _xk_ _xk+1_

_|_ _−_ _|_

By substituting X with X _[′]_ and taking the expected value, we have

2mτ log 2

Eu **T[σ][∗]** **T[∗]** _F_ Eu (42)
_||_ _−_ [e] _||_ _≤_ _xπk + gσ(uπk_ ) _xπk+1_ _gσ(uπk+1_ )
h i  _|_ _−_ _−_ _|_ 

Based on Lemma 3, the probability that πk = i, πk+1 = j is

1 _k−1_ 1 _m_ 1
_P_ (πk = i, πk+1 = j) = 1 + exp _σ_ _π_ _a=1_ 1 + exp _σ_ _b=k+2_ 1 + exp _xj_ _−σxπb_ !

_−_ _[x][i][−][x][j]_ X∀ Y _−_ _[x][πa]_ _[−][x][i]_ Y _−_

(43)

where the first term denotes xi + gσ(ui) > xj + gσ(uj), the second term denotes all conditions that
there are (k − 1) items larger than xi + gσ(ui) and the rest items are smaller than xj + gσ(uj).


In the following we derive the upper bound of Eu


. We denote


_u_

_|[x]πk_ [+][g]σ[(][u]πk [)][−][x]πk+1 _[−][g]σ[(][u]πk+1_ [)]|

_Ai,j as_
_ui, uj ∈Ai,j,_ _s.t._ _xi + gσ(ui) −_ _xj −_ _gσ(uj) > ϵ_ (44)

where ϵ is a sufficiently small number. Then we have


1

Eu

" _xπk + gσ(uπk_ ) _xπk+1_ _gσ(uπk+1_ ) #
_−_ _−_

1

= _P_ (πk = i, πk+1 = j) Eui,uj _i,j_ (45)
Xi≠ _j_ _∈A_  _|xi + gσ(ui) −_ _xj −_ _gσ(uj)|_ 

1 _k−1_ 1 _m_ 1

= _i=j_ 1 + exp _σ_ _π_ _a=1_ 1 + exp _σ_ _b=k+2_ 1 + exp _xj_ _−σxπb_ !
X̸ _−_ _[x][i][−][x][j]_ X∀ Y _−_ _[x][πa]_ _[−][x][i]_ Y _−_

1

Eui,uj _i,j_ (46)
_∈A_ _xi + gσ(ui)_ _xj_ _gσ(uj)_

 _|_ _−_ _−_ _|_ 

1 _k−1_ 1 _m_ 1

= _i=j_ 1 + exp _σ_ _π_ _a=1_ 1 + exp _σ_ _b=k+2_ 1 + exp _xj_ _−σxπb_ !
X̸ _−_ _[x][i][−][x][j]_ X∀ Y _−_ _[x][πa]_ _[−][x][i]_ Y _−_

1

Eui,uj _i,j_ (47)
_∈A_ _xi_ _σ log(_ log(ui)) _xj + σ log(_ log(uj))

 _|_ _−_ _−_ _−_ _−_ _|_ 

_k−1_ 1 _m_ 1

= _i=j_ _f_ (xi − _xj, σ, z)_ _π_ _a=1_ 1 + exp _σ_ _b=k+2_ 1 + exp _xj_ _−σxπb_ !! (48)
X̸ X∀ Y _−_ _[x][πa]_ _[−][x][i]_ Y _−_

We denote f (δ, σ, z) as:

1 1
_f (δ, σ, z) =_ Eui,uj (49)

1 + exp − _σ[δ]_  _|δ −_ _σ log(−_ log(ui)) + σ log(− log(uj))| 

s.t. _δ_ _σ log(_ log(ui)) + σ log( log(uj)) > z > 0 (50)
_−_ _−_ _−_

For the probability terms in Eq. (48), for all permutations π, there must exist πa, πb, such that


(51)

(52)


1 + exp _σ_
_−_ _[x][πa]_ _[−][x][i]_

1

1 + exp _xj_ _−σxπb_
_−_


1 + exp _σ_
_−_ _[x][k][−][x][i]_

1

1 + exp _σ_
_−_ _[x][j]_ _[−][x][k][+1]_


-----

Thus we have

Eq. (48) ≤


(53)

(54)

(55)

(56)

(57)


_f_ (xi _xj, σ, z)_
_−_ 1 + exp _σ_

_−_ _[x][k][−][x][i]_


1 + exp _σ_
_−_ _[x][j]_ _[−][x][k][+1]_


_i≠_ _j_

Xi≠ _j_


_f_ (xi _xj, σ, z)_
_−_

(1 + exp _[x][i][−]σ[x][k]_ )(1 + exp _[x][k][+1]σ[−][x][j]_


By transforming Eq. (21) in Lemma 3, and substituting xj − _xi by y, we have_


Eq. (21) _P_ (gσ(ui) _gσ(uj) > xj_ _xi) =_
_⇒_ _−_ _−_


1 + exp _σ_
_−_ _[x][i][−][x][j]_


_P_ (gσ(ui) _gσ(uj) > y) =_
_⇒_ _−_


1 + exp _σ[y]_


_P_ (gσ(ui) _gσ(uj) < y) = 1_
_⇒_ _−_ _−_


1 + exp _[y]_


1 + exp _σ_
_−_ _[y]_


where the right side is the form of the cumulative distribution function (CDF) of standard Logistic
distribution by setting σ = 1:


CDF(y) =


1

(58)
1 + exp (−y)


Thus log( log(ui))+log( log(uj)) is equivalent to the Logistic distribution whose probability
_−_ _−_ _−_
density function (PDF) is


PDF(y) = _[d][CDF][(][y][)]_

_dy_


1

(59)
exp (−y) + exp y + 2


and in this proof we exploit an upper bound of PDF(y):


PDF(y) =


1

(60)
_y[2]_ + 4


exp (−y) + exp y + 2 _[≤]_


Based on the Logistic distribution, we can replace _σ log(_ log(ui)) + σ log( log(uj)) by σy
_−_ _−_ _−_
where y is from the Logistic distribution. Thus we can derive the upper bound of f (δ, σ, z) as


-----

follows

1 1
_∞_

_f (δ, σ, z) =_ 1 _−δ/σ+z_ _δ+σy_ exp (−1y)+exp y+2 _[dy]_ (61)

1 + exp _σ_ _·_ R _∞δ/σ+z_ exp ( _y)+exp y+2_ _[dy]_
_−_ _[δ]_ _−_ _−_

1 1

= 1 _∞−Rδ/σ+z_ _δ+σy_ exp (1−y)+exp y+2 _[dy]_ (62)

1 + exp _σ_ _·_ R 1 1+exp (δ/σ _z)_
_−_ _[δ]_ _−_ _−_

1 1
_∞_

= 1 _−δ/σ+z_ _δ+σy_ exp (−y)+exp y+2 _[dy]_ (63)

exp (δ/σ _z)_

1 + exp _σ_ _·_ R 1+exp (δ/σ− _z)_
_−_ _[δ]_ _−_

1 1
_∞_

= 1 _−δ/σ+z_ _δ+σy_ exp (1 _−y)+exp y+2_ _[dy]_ (64)

1 + exp _σ_ _·_ R 1+exp ( _δ/σ+z)_
_−_ _[δ]_ _−_

= [1 + exp (][−] _σ[δ]_ [+][ z][)] _∞_ 1 1 (65)

1 + exp − _σ[δ]_ Z−δ/σ+z _δ + σy_ exp (−y) + exp y + 2 _[dy]_

_σ_ [+][ z][)] _∞_ 1 1

(66)

_≤_ [1 + exp (]1 + exp[−] −[δ] _σ[δ]_ Z−δ/σ+z _δ + σy_ _y[2]_ + 4 _[dy]_

2σ log (zσ _δ)[2]_ + 4σ[2][] 2δ arctan _z−2δ/σ_ 4σ log z + πδ

_σ_ [+][ z][)] _−_ _−_ _−_

= [1 + exp (][−] _[δ]_

1 + exp _σ_ _·_   4δ[2] + 16σ[2]  
_−_ _[δ]_

(67)

2σ log (zσ + _δ_ )[2] + 4σ[2][] 2δ arctan _z−2δ/σ_ 4σ log z + πδ

_σ_ [+][ z][)] _|_ _|_ _−_ _−_

_≤_ [1 + exp (]1 + exp[−] _[δ]_ _σ_ _·_   4δ[2] + 16σ[2]  

_−_ _[δ]_

(68)

2σ log (zσ + _δ_ )[2] + 4σ[2][] 2δ arctan _z−2δ/σ_ 2σ log z[2] + πδ

_σ_ [+][ z][)] _|_ _|_ _−_ _−_

= [1 + exp (][−] _[δ]_

1 + exp _σ_ _·_   4δ[2] + 16σ[2]  
_−_ _[δ]_

(69)


2σ log (zσ+|δz|[2])[2]+4σ[2] 2δ arctan _z−2δ/σ_ + πδ

_−_

(70)

 4δ[2] + 16σ[2]  

2σ log (zσ+|zδ|[2]+2σ)[2] 2δ arctan _z−2δ/σ_ + πδ

_−_

(71)

 4δ[2] + 16σ[2]  


= [1 + exp (][−] _σ[δ]_ [+][ z][)]

1 + exp _σ_
_−_ _[δ]_

_σ_ [+][ z][)]

_≤_ [1 + exp (]1 + exp[−] _[δ]_ _σ_

_−_ _[δ]_

= [1 + exp (][−] _σ[δ]_ [+][ z][)]

1 + exp _σ_
_−_ _[δ]_

= [1 + exp (][−] _σ[δ]_ [+][ z][)]

1 + exp _σ_
_−_ _[δ]_

_σ_ [+][ z][)]

_≤_ [1 + exp (]1 + exp[−] _[δ]_ _σ_

_−_ _[δ]_

_σ_ [+][ z][)]

_≤_ [1 + exp (]1 + exp[−] _[δ]_ _σ_

_−_ _[δ]_

= [1 + exp (][−] _σ[δ]_ [+][ z][)]

1 + exp _σ_
_−_ _[δ]_


4σ log _zσ+|zδ|+2σ_


4σ log _zσ+|zδ|+2σ_


4σ log _zσ+|zδ|+2σ_


4σ log _zσ+|zδ|+2σ_



2δ arctan _z−2δ/σ_ + πδ
_−_

(72)
4δ[2] + 16σ[2]  

+ δ _π_ 2 arctan _z−2δ/σ_
_−_

(73)

4δ[2] + 16 _σ[2]_  

+ _δ_ _π_ 2 arctan _z−2δ/σ_
_|_ _|_ _−_

(74)

4δ[2] + 16 _σ[2]_  


4σ log _zσ+|zδ|+2σ_ + _δ_ _π_ 2 arctan 2σ

_|_ _|_ _−_ _−_ _[δ]_

(75)

 4δ[2] + 16  _σ[2]_   

_zσ+_ _δ_ +2σ _δ_
4σ log _|z|_ + _δ_ _π + 2 arctan_ 2σ

_|_ _|_

(76)

 4δ[2] + 16 σ[2]   


-----

where Eq. (66) is because exp ( _y)+exp1_ _y+2_ _y[2]1+4_ [, and Eq. (][74][) is because][ π] _[−][2 arctan(][ z][−]2[δ/σ]_ )

_−_ _[≤]_ _≥_

0. With probability (1 − _ϵ), we have_

_z = log [1 +][ ϵ][ exp][ δ]σ_ log (1 _ϵ)_ (77)

1 _ϵ_ _≥−_ _−_
_−_

1 + exp (− _σ[δ]_ [+][ z][)] = 1 (78)

1 + exp _σ_ 1 _ϵ_
_−_ _[δ]_ _−_

Thus


_zσ+_ _δ_ +2σ _δ_
4σ log _|z|_ + _δ_ _π + 2 arctan_ 2σ

_|_ _|_

(79)

 4δ[2] + 16 σ[2]   

_δ_ +2σ _δ_
4σ log _σ_ log(1| _|_ _ϵ)_ + _δ_ _π + 2 arctan_ 2σ
_−_ _−_ _|_ _|_

(80)

 4δ[2] + 16σ [2]   


_f_ (δ, σ, z) ≤ Eq. (76) =

_≤_

Thus we have


1 − _ϵ_

1

1 − _ϵ_


4σ log _σ_ log(1 _ϵ)_ + _xi_ _xj_ _π + 2 arctan_ _xi2−σxj_
_−_ _[|][x][i][−][x][j]−[|][+2][σ]_ _|_ _−_ _|_

(1 _ϵ)(4(_ _xi_ _xj)[2]_ + 16σ[2])(1 + exp [x][i][−]σ[x][k] )(1 + exp _[x][k][+1]σ[−][x][j]_
_−_ _−_


Eq. (54) ≤


(81)


_i≠_ _j_


In conclusion, with the probability (1 − _ϵ), we have_

(2 log 2)mτ 4σ log _σ_ log(1 _ϵ)_ + _xi_ _xj_ _π + 2 arctan_ _[x][i]2[−]σ[x][j]_

Eu **T[σ][∗]** **T[∗]** _F_ _−_ _[|][x][i][−][x][j]−[|][+2][σ]_ _|_ _−_ _|_
_||_ _−_ [e] _||_ _≤_ _i=j_ (1 _ϵ)(4(_ _xi_ xj)[2] + 16σ[2])(1 + exp _[x][i][−]σ[x][k]_ )(1 + exp _[x][k][+1]σ[−][x][j]_ ) 
h i X̸ _−_ _−_

(82)

= (log 2)mτ 2σ log _σ −_ _[|][x]log(1[i][−][x][j]−[|][+2]ϵ)_ _[σ]_ + |xi − _xj|_ _π2_ [+ arctan][ x][i]2[−]σ[x][j]

_i=j_ (1 _ϵ)((xi_ xj)[2] + 4σ[2])(1 + exp _[x][i][−]σ[x][k]_ )(1 + exp _[x][k][+1]σ[−][x][j]_ ) 

X̸ _−_ _−_

(83)


Ω(xi, xj, σ, ϵ) (84)

Xi≠ _j_


=(log 2)mτ

And we denote Ω(xi, xj, σ, ϵ) as


Ω(xi, xj, σ, ϵ) = 2σ log _σ −_ _[|][x]log(1[i][−][x][j]−[|][+2]ϵ)_ _[σ]_ + |xi − _xj|_ _π2_ [+ arctan][ x][i]2[−]σ[x][j]

(1 _ϵ)((xi_ _xj)[2]_ + 4σ[2])(1 + exp _[x][i][−]σ[x][k]_ )(1 + exp _[x][k][+1]σ[−][x][j]_
_−_ _−_


(85)


Finally, Theorem 1 is proved by triangle inequality by jointly considering Lemma 1 and Lemma 2.

A.3 REMARKS W.R.T. SOFT-TOPK

In the following, we add some remarks about the relationship between our conclusion of GS-TopK
and the conclusion derived by the authors of SOFT-TopK (Xie et al., 2020): the SOFT-TopK is a
special case of our proposed algorithm when we set σ = 0. We have the following conclusion:
**Theorem 2. Assume the values of xk, xk+1 are unique[4], under probability (1 −** _ϵ), we have_

(π log 2)mτ
lim **T[∗]** **T[∗]** _F_ (86)

4For the ease of a compact proof, we make this assumption that the values ofσ→0[+][ E][u] h|| _−_ [e] _||_ i _≤_ (1 − _ϵ)|xk −_ _xk+1 x|_ _k, xk+1 are unique. If there_
are duplicate values of xk, xk+1, the bound only differs by a constant multipler therefore does not affect our
conclusion: SOFT-TopK (Xie et al., 2020) is a special case of our proposed approach when σ = 0.


-----

**Condition 1**

**Condition 2**

**Condition 3**


xi xk xk+1 xj

xj xk xk+1 xi


xi xj xk xk+1

**Condition 4**

xk xk+1 xi xj

Figure 4: Four conditions considered in our proof. It is worth noting that xi, xj must not lie between
_xk, xk+1, because we define xk, xk+1 as two adjacent items in the original sorted list._

which differs from the conclusion in (Xie et al., 2020) by only a constant factor.

_Proof. Since σ →_ 0+, the first term becomes 0. For the second term, we discuss four conditions as
shown in Fig. 4, except for the following condition: xi = xk, xj = xk+1.

_xCondition 1i −_ _xk > 0 or. If x xk+1i ≥ −xxkj, x >j 0 ≤. Then we havexk+1 (equalities do not hold at the same time), we have at least_

1
lim = 0 (87)
_σ→0[+]_ (1 + exp _[x][i][−]σ[x][k]_ )(1 + exp _[x][k][+1]σ[−][x][j]_ )

lim (88)
_⇒_ _σ_ 0[+][ Ω(][x][i][, x][j][, σ, ϵ][) = 0]
_→_

**Condition 2. For any case that xi < xj, we have xi −** _xj < 0, thus_

lim = (89)
_σ_ 0[+][ arctan][ x][i][ −]σ _[x][j]_ _−_ _[π]2_
_→_

_π_
lim = 0 (90)
_⇒_ _σ_ 0[+] 2 [+ arctan][ x][i][ −]σ _[x][j]_
_→_

lim (91)
_⇒_ _σ_ 0[+][ Ω(][x][i][, x][j][, σ, ϵ][) = 0]
_→_


**Condition 3we have** . If xi ≥ _xj ≥_ _xk (equalities do not hold at the same time), we have xi −_ _xk > 0. Then_


1
lim = 0 (92)
_σ→0[+]_ 1 + exp _[x][i][−]σ[x][k]_

lim (93)
_⇒_ _σ_ 0[+][ Ω(][x][i][, x][j][, σ, ϵ][) = 0]
_→_

Then we haveCondition 4. If xk+1 ≥ _xi ≥_ _xj (equalities do not hold at the same time), we have xk+1 −_ _xj > 0._


1
lim = 0 (94)
_σ→0[+]_ 1 + exp _[x][k][+1]σ[−][x][j]_

lim (95)
_⇒_ _σ_ 0[+][ Ω(][x][i][, x][j][, σ, ϵ][) = 0]
_→_


-----

Therefore, ifσ → 0[+]. Thus we have the following conclusion by only considering xi ̸= xk and xj ̸= xk+1, the second term Ω(xi, xj x, σ, ϵi =) x degenerates to 0 whenk, xj = xk+1:

lim **T[∗]** **T[∗]** _F_ (log 2)mτ _|xk −_ _xk+1|_ _π2_ [+ arctan][ x][k][−]2[x]σ[k][+1] (96)
_σ→0[+][ E][u]_ h|| _−_ [e] _||_ i _≤_  (1 − _ϵ)(xk −_ _xk+1)[2]_ 

(π log 2)mτ

(97)

_≤_ (1 _ϵ)_ _xk_ _xk+1_

_−_ _|_ _−_ _|_

B IMPLEMENTATION DETAILS ON DISCRETE CLUSTERING PROBLEM


For the discrete clustering problem, recall that we aim to select a subset of objects as cluster centers.
It differs from the k-means clustering problem whose cluster centers may not have to be a subset
of the objects, and the constraint for discrete clustering is more welcomed for real-world facility
location planning applications. Denote ∆ _∈_ R[+][m][×][m] as the distance matrix for each pair of objects.

Following our implementation of the max covering problem, we can also derive a GPU-friendly
formulation of the discrete clustering problem using PyTorch syntax:

min _s.t._ **x** 0, 1 _,_ _x_ 0 _k_ (98)
**x** [sum(min(][∆][[][x][,][ :]][, dim][ = 0))] _∈{_ _}[m]_ _||_ _||_ _≤_

We also model the discrete clustering problem as a graph and encode the problem structure by a
graph neural network. Specifically, for points that lie on the 2D plane, we define there is an edge
between two points if their distance is smaller than a predefined threshold. Then the geometric
information is encoded by 3-layer SplineCNN (Fey et al., 2018) which was found successful for
geometric feature learning. Similar to the max covering problem, we rewrite the GPU-friendly
form of the objective score in a differentiable manner, being estimated by the output of GS-TopK
algorithm:

_Ji = sum(softmax(−T_ **∆[x, :], dim = 0, keepdim = True) ⊙** **∆[x, :])** (99)

_J = mean([J1,_ _J2, ...,_ _J#G])_

e

where we use a softmax operator with temperature T to approximate the min operator in a differentiable way. ⊙ means the tensor-product operator in PyTorch. The GS-TopK learning algorithm is[e] [e] [e]
summarized in Alg. 3.

C EXPERIMENT SETUP

Our algorithms are implemented by PyTorch and the graph neural network modules are based
on (Fey & Lenssen, 2019). In our paper, we optimize the hyperparameters by greedy search
on a small subset of problem instances (∼5) and set the best configuration of hyperparameters for both GS-TopK, SOFT-TopK. For the max covering problem, we empirically set
learning rate = 0.1, τ = 0.05, σ = 0.15 for GS-TopK, τ = 0.05 for SOFT-TopK, and (τ, σ) =
(0.05, 0.15), (0.04, 0.10), (0.03, 0.05) for Homotopy GS-TopK. For the discrete clustering problem,
we set learning rate = 0.1, τ = 0.05, σ = 0.25 for GS-TopK, τ = 0.05 for SOFT-TopK, and we set
(τ, σ) = (0.05, 0.25), (0.04, 0.15), (0.03, 0.05) for Homotopy GS-TopK. We set #G = 1000 for
max covering, #G = 500 for discrete clustering. All experiments are done on a workstation with
i7-9700K@3.60GHz, 16GB memory, and 2080Ti GPU.

D ABLATION STUDY ON HYPERPARAMETERS

Firstly, we want to add some remarks about the selection of hyperparameters:

_• #G (number of Gumbel samples): #G affects how many samples are taken during train-_
ing and inference for GS-TopK. A larger #G (i.e. more samples) will be more appealing,


-----

**Algorithm 3: Gumbel-Sinkhorn-TopK Learning for Solving the Discrete Clustering Prob-**
**lem**
**Input: the distance matrix ∆; learning rate α; softmax temperature T** ; GS-TopK parameters
_k, τ, σ, #G._

**1 if Training then**

**2** Randomly initialize neural network weights θ;

**3 if Inference then**

**4** Load a pretrained neural network weights θ; Jbest = +∞;

**5 while not converged do**

**6** **s = SplineCNNθ(∆); [T[e]** 1, **T2, ...,** **T#G] = GS-TopK(s, k, τ, σ, #G);**

**7** for all i,
_Ji = sum(softmax(_ _T_ **∆[[e]T[e]** _i[2, :][e], :], dim = 0, keepdim = True)_ **∆[T[e]** _i[2, :], :]);_
_−_ _⊙_

**8** _J = mean([J1,_ _J2, ...,_ _J#G]);_

**9** **if Traininge** **then**

**10** update θ with respect to the gradient[e] [e] [e] _[∂J]∂θ_ [and learning rate][ α][ by gradient descend;]

**11** **if Inference then**

**12** update s with respect to the gradient _[∂J]∂s_ [and learning rate][ α][ by gradient descend;]

**13** for all i, _Ji = TopK(T[e]_ _i[2, :])A · v; Jbest = min([J1,_ _J2, ...,_ _J#G], Jbest);_

**14 if Homotopy Inference then**

**15** Decline the values of[e] _τ, σ and jump to line 5;_ [e] [e] [e]


**Output: Learned network weights θ (if training)/The best objective Jbest (if inference).**

because GS-TopK will have a more accurate estimation of the objective score, and it will
have a higher probability of discovering better solutions. However, #G cannot be arbitrarily large because we are running GS-TopK in parallel on GPU, and the GPU has limited
memory. Therefore, in experiments, we set a large enough #G (e.g. #G = 1000) and
ensure that it can fit into the GPU memory of our workstation (2080Ti, 11G).

_• τ (entropic regularization factor of Sinkhorn): Theoretically, τ controls the gap of the_
continuous Sinkhorn solution to the discrete solution, and a smaller τ will lead to a tightened gap. This property is validated by our theoretical findings in Lemma 2. Unfortunately,
_τ cannot be arbitrarily small, because a smaller τ requires more Sinkhorn iterations to con-_
verge. Therefore, given a fixed number of Sinkhorn iterations (100) to ensure the efficiency
of our algorithm, we need trial-and-error to discover the suitable τ for both SOFT-TopK
and GS-TopK. The grid search results below show that our selection of τ fairly balances
the performances of both SOFT-TopK and GS-TopK.

_• σ (Gumbel noise factor): As derived in Theorem 1, we need to balance between the two_
gaps (directly find the theoretically optimalLemma 1 and Lemma 2) by a suitable σ given σ τ. Since. In the experiments, we firstly determine a |xk − _xk+1| is unknown, we cannot_
_τ_, and then find a suitable σ by greedy search on a small subset (∼5) of problem instances.

We conduct an ablation study about the sensitivity of hyperparameters by performing an extensive grid search near the configuration used in our max covering experiments (τ = 0.05, σ =
0.15, #G = 1000). We choose the k=50, m=500, n=1000 max covering problem, and we have the
following results for GS-TopK (ours) and SOFT-TopK (Xie et al., 2020) (higher is better):

|3: Ablation stud|dy result of GS-TopK with #G =|
|---|---|
|τ = σ =|0.01 0.05 0.1|
|0.1 0.15 0.2|42513.4 44759.2 45039.5 41456.5 44713.2 44837.2 41264.3 44638.1 44748.2|


-----

|4: Ablation stu|udy result of GS-TopK with #G =|
|---|---|
|τ = σ =|0.01 0.05 0.1|
|0.1 0.15 0.2|42511.6 44754.6 45037.6 41421.4 44705.8 44841.5 41235.9 44651.5 44748.6|


Table 5: Ablation study result of SOFT-TopK.

_τ =_ 0.001 0.005 0.01 0.05 0.1

objective score 35956.6 42013.3 **42520.8** 41004.3 40721.2


Under the configuration used in our paper, both SOFT-TopK and GS-TopK have relatively good
results. By grid search, we also discover better hyperparameters for both GS-TopK and SOFTTopK, but it is worth noting that SOFT-TopK is still inferior to GS-TopK with the best-discovered
hyperparameters. Our grid search result shows that our GS-TopK is not very sensitive to σ if we
have τ = 0.05 or 0.1, and the result of τ = 0.01 is inferior because the Sinkhorn algorithm may
not converge. The results of #G = 1000 are all better than #G = 800, suggesting that a larger
#G is appealing if we have enough GPU memory. It is also discovered that SOFT-TopK seems to
be able to accept a smaller value of τ compared to GS-TopK, probably because adding the Gumbel
noise will increase the divergence of elements thus performs in a sense similar to decreasing τ when
considering the convergence of Sinkhorn.


-----

