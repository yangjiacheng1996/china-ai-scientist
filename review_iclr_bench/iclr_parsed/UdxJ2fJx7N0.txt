# MINIMAX OPTIMIZATION WITH SMOOTH ALGORITH## MIC ADVERSARIES


**Tanner Fiez[∗], Lillian J. Ratliff**
University of Washington, Seattle
{fiezt, ratliffl}@uw.edu

**Praneeth Netrapalli**
Google Research, India
pnetrapalli@google.com


**Chi Jin**
Princeton University
chij@princeton.edu


ABSTRACT

This paper considers minimax optimization minx maxy f (x, y) in the challenging setting where f can be both nonconvex in x and nonconcave in y. Though
such optimization problems arise in many machine learning paradigms including
training generative adversarial networks (GANs) and adversarially robust models,
from a theoretical point of view, two fundamental issues remain: (i) the absence of
simple and efficiently computable optimality notions, and (ii) cyclic or diverging
behavior of existing algorithms. This paper proposes a new theoretical framework
for nonconvex-nonconcave minimax optimization that addresses both of the above
issues. The starting point of this paper is the observation that, under a computational budget, the max-player can not fully maximize f (x, ·) since nonconcave
maximization is NP-hard in general. So, we propose a new framework, and a corresponding algorithm, for the min-player to play against smooth algorithms deployed
by the adversary (i.e., the max-player) instead of against full maximization. Our
algorithm is guaranteed to make monotonic progress (thus having no limit cycles or
diverging behavior), and to find an appropriate “stationary point” in a polynomial
number of iterations. Our framework covers practically relevant settings where the
smooth algorithms deployed by the adversary are multi-step stochastic gradient
ascent, and its accelerated version. We further present experimental results that
confirm our theoretical findings and demonstrate the effectiveness of the proposed
approach in practice on simple, conceptual settings.

1 INTRODUCTION

This paper considers minimax optimization minx maxy f (x, y) in the context of two-player zero-sum
games, where the min-player (controlling x) tries to minimize objective f assuming a worst-case
opponent (controlling y) that acts so as to maximize it. Minimax optimization naturally arises in
a variety of important machine learning paradigms, with the most prominent examples being the
training of generative adversarial networks (GANs) (Goodfellow et al., 2014) and adversarially robust
models (Madry et al., 2018). These applications commonly engage deep neural networks with various
techniques such as convolution, recurrent layers, and batch normalization. As a result, the objective
function f is highly nonconvex in x and nonconcave in y.

Theoretically, minimax optimization has been extensively studied starting from the seminal work of
von Neumann (Neumann, 1928), with many efficient algorithms proposed for solving it (Robinson,
1951; Korpelevich, 1976; Nemirovski, 2004). A majority of these classical results have been focused
on convex-concave functions, and heavily rely on the minimax theorem, i.e., minx maxy f (x, y) =
maxy minx f (x, y), which no longer holds beyond the convex-concave setting. Recent line of
works (Lin et al., 2020a; Nouiehed et al., 2019; Thekumparampil et al., 2019; Lin et al., 2020b;
Ostrovskii et al., 2020) address the nonconvex-concave setting where f is nonconvex in x but concave

_∗Currently with Amazon._


-----

in y by proposing meaningful optimality notions and designing algorithms to find such points. A key
property heavily exploited in this setting is that the inner maximization over y given a fixed x can be
computed efficiently, which unfortunately does not extend to the nonconvex-nonconcave setting.

Consequently, nonconvex-nonconcave optimization remains challenging, and two of the most fundamental theoretical issues are still unresolved: (i) what is an appropriate notion of optimality that
can be computed efficiently? and (ii) can we design algorithms that do not suffer from cycling or
diverging behavior? Practitioners often use simple and popular algorithms such as gradient descent
ascent (GDA) and other variants for solving these challenging optimization problems. While these
algorithms seem to perform well in terms of producing high quality images in GANs and robust
models in adversarial training, they are also highly unstable, particularly in training GANs. Indeed
the instability of GDA and other empirically popular methods is not surprising since they are known
to not converge even in very simple settings (Daskalakis & Panageas, 2018a; Bailey et al., 2020).
This current state of affairs strongly motivates the need to develop a strong theoretical foundation for
nonconvex-nonconcave minimax optimization and to design better algorithms for solving them.

**This work considers the challenging nonconvex-nonconcave setting. Our framework sprouts from**
the practical consideration that under a computational budget, the max-player cannot fully maximize
_f_ (x, ·) since nonconcave maximization is NP-hard in general. In fact, in both the settings of GAN
and adversarial training, in practice, the max-player employs simple gradient based algorithms such
as gradient ascent, run for a few steps – on the order of 5 for GAN training and 40 for adversarial
training (see, e.g., Arjovsky et al. 2017; Madry et al. 2018) – to estimate argmaxy f (x, y). To
capture this aspect, we assume that the max-player has a toolkit of multiple (potentially randomized)
algorithms A1, A2, · · ·, Ak in an attempt to solve the maximization problem given fixed x, and picks
the best solution among these algorithms. This motivates us to study the surrogate of the minimax
optimization problem as

minx maxi∈[k] f (x, Ai(x)) = minx maxλ∈∆k _ki=1_ _[λ][i][f]_ [(][x,][ A][i][(][x][))][,] (1)

where ∆k denotes the k-dimensional simplex, and Ai(x) denotes the output of algorithmP _Ai for a_
given x. When both the objective function f and the algorithms {Ai}i[k]=1 [are smooth (defined formally]
in Section 3), we can show that (1) becomes a smooth nonconvex-concave minimax optimization
problem, where recent advances can be leveraged in solving such problems.

In particular, given the smooth algorithms deployed by the adversary (i.e. the max-player), this
paper proposes two algorithms for solving problems in (1). The first algorithm is based on stochastic
gradient descent (SGD), which is guaranteed to find an appropriate notion of “ϵ-approximate stationary
point” in O(ϵ[−][4]) gradient computations. The second algorithm is based on proximal algorithm, in
the case of deterministic adversarial algorithms {Ai}i[k]=1[, this algorithm has an improved gradient]
complexity O(ϵ[−][3]) or _O[˜](poly(k)/ϵ[2]) depending on the choice of subroutine within the algorithm._
All our algorithms are guaranteed to make monotonic progress, thus having no limit cycles.

Our second set of results show that, many popular algorithms deployed by the adversary such as
multi-step stochastic gradient ascent, and multi-step stochastic Nesterov’s accelerated gradient ascent
are in fact smooth. Therefore, our framework readily applies to those settings in practice.

We also present complementing experimental results showing that our theoretical framework and
algorithm succeed on simple, conceptual examples of GAN and adversarial training. While more
work is needed to scale our approach to large scale benchmarks in GAN and adversarial training,
our experimental results serve as a proof of concept demonstration that our algorithm converges to
desirable points in practice, at least on simple conceptual examples.

2 RELATED WORK

Due to lack of space, we focus our discussion here on directly related works and present a more
detailed overview of related work in Appendix A. Many existing works that propose local optimality
notions in nonconvex-nonconcave minimax optimization suffer from nonexistence of such points
in general (Ratliff et al., 2013; Ratliff et al., 2016; Jin et al., 2020; Fiez et al., 2020; Zhang et al.,
2020a; Farnia & Ozdaglar, 2020). To the best of our knowledge, the only works that propose a
relaxed local optimality notion that is shown to exist and be computable in polynomial time is due
to Keswani et al. (2020); Mangoubi & Vishnoi (2021). The aforementioned works are similar to this


-----

paper in the sense that the min-player faces the max-player with computational restrictions, but are
different from ours in terms of the model of the max-player and the algorithms to solve the problem.
Concretely, Keswani et al. (2020); Mangoubi & Vishnoi (2021) consider the stationary points of
a “smoothed" greedy-max function, which is computed by maximizing along a locally ascending
path when fixing the min-player. In contrast, our paper considers the stationary points of a “max"
function computed by particular smooth algorithms given their initializations, which is a more faithful
surrogate to the objective that the min-player wishes to minimize. Depending on the choice of smooth
algorithms and their initializations, the optimal points of Keswani et al. (2020); Mangoubi & Vishnoi
(2021) and the optimal points in our paper can be very different, making them incomparable.

3 PRELIMINARIES

In this section, we present problem formulation and preliminaries. We consider function f satisfying
**Assumption 1. We denote w = (x, y), and assume f : R[d][1]** _× R[d][2]_ _→_ R is:

_(a) B-bounded i.e.,_ _f_ (w) _B,_ _(b) G-Lipschitz i.e.,_ _f_ (w1) _f_ (w2) _G_ _w1_ _w2_ _,_
_|_ _| ≤_ _|_ _−_ _| ≤_ _∥_ _−_ _∥_

_(c) L-gradient Lipschitz i.e.,_ _f_ (w1) _f_ (w2) _L_ _w1_ _w2_ _,_
_∥∇_ _−∇_ _∥≤_ _∥_ _−_ _∥_

_(d) ρ-Hessian Lipschitz i.e.,_ _f_ (w1) _f_ (w2) _ρ_ _w1_ _w2_ _._
_∥∇[2]_ _−∇[2]_ _∥≤_ _∥_ _−_ _∥_

_where ∥·∥_ _denotes Euclidean norm for vectors and operator norm for matrices._

We aim to solve minx∈Rd1 maxy∈Rd2 f (x, y). Since maxy∈Rd2 f (x, y) involves non-concave maximization and hence is NP-hard in the worst case, we intend to play against algorithm(s) that y-player
uses to compute her strategy. Concretely, given x ∈ R[d][1], we assume that the y-player chooses her
(potentially random) strategy _yz(x) =_ _i∗(x)(x, zi∗(x)), where we use shorthand z := (z1,_ _, zk),_
_A_ _· · ·_
as i[∗](x) = argmaxi [k] f (x, _i(x, zi)), where_ 1, _,_ _k are k deterministic algorithms that take_
_∈_ _A_ _A_ _· · ·_ _A_
as inputrandomized algorithms e.g., x and a random seed A z b could be stochastic gradient ascent oni ∈ R[ℓ], where zi are all independent. Note that the framework captures f (x, ·), with initialization, minibatching etc. determined by the random seed z. This also incorporates running the same algorithm
multiple times, with different seeds and then choosing the best strategy. We now reformulate the
minimax objective function to:

minx∈Rd1 g(x) where _g(x) := Ez [f_ (x, _yz(x))] ._ (2)
For general algorithms _i, the functions f_ (x, _i(x, zi)) need not be continuous even when f_
_A_ _A_
satisfies Assumption 1. However, if the algorithms Ai are smooth as defined below, the functions b
_f_ (x, _i(x, zi)) behave much more nicely._
_A_
**Definition 1 (Algorithm Smoothness). A randomized algorithm A : R[d][1]** _× R[ℓ]_ _→_ R[d][2] _is:_

_(a) G-Lipschitz, if_ (x1, z) (x2, z) _G_ _x1_ _x2_ _for any z._
_∥A_ _−A_ _∥≤_ _∥_ _−_ _∥_
_(b) L-gradient Lipschitz, if_ _D_ (x1, z) _D_ (x2, z) _L_ _x1_ _x2_ _for any z._
_∥_ _A_ _−_ _A_ _∥≤_ _∥_ _−_ _∥_

Here DA(x, z) ∈ R[d][1] _× R[d][2]_ is the Jacobian of the function A(·, z) for a fixed z. The following
lemma tells us that f (x, A(x, z)) behaves nicely whenever A is a Lipschitz and gradient Lipschitz
algorithm. For deterministic algorithms, we also use the shortened notation A(x) and DA(x).
**Lemma 1. Suppose A is G[′]-Lipschitz and L[′]-gradient Lipschitz and f satisfies Assumption 1. Then,**
_for a fixed z, function f_ (·, A(·, z)) is G(1 + G[′])-Lipschitz and L(1 + G[′])[2] + GL[′]-gradient Lipschitz.

While g(x) defined in (2) is not necessarily gradient Lipschitz, it can be shown to be weakly-convex
as defined below. Note that an L-gradient Lipschitz function is L-weakly convex.
**Definition 2. A function g : R[d][1]** _→_ R is L-weakly convex if ∀ _x, there exists a vector ux satisfying:_

_g(x[′])_ _g(x) +_ _ux, x[′]_ _x_ 2 (3)
_≥_ _⟨_ _−_ _⟩−_ _[L]_ _[∥][x][′][ −]_ _[x][∥][2][ ∀]_ _[x][′][.]_

_Any vector ux satisfying this property is called the subgradient of g at x and is denoted by ∇g(x)._

An important property of weakly convex function is that the maximum over a finite number of weakly
convex function is still a weakly convex function.
**Lemma 2. Given L-weakly convex functions g1, · · ·, gk : R[d]** _→_ R, the maximum function g(·) :=
maxi [k] gi( ) is also L-weakly convex and the set of subgradients of g( ) at x is given by:
_∈_ _·_ _·_

_∂g(x) =_ _j_ _S(x)_ _[λ][j][∇][g][j][(][x][) :][ λ][j][ ≥]_ [0][,][ P]j _S(x)_ _[λ][j][ = 1][}][,][ where][ S][(][x][) := argmax]i_ [k] _[g][i][(][x][)][.]_
_{[P]_ _∈_ _∈_ _∈_


-----

Thus, under Assumption 1 and the assumptions Ai are all G[′]-Lipschitz and L[′]-gradient Lipschitz,
then g(·) defined in (2) is L(1 + G[′])[2] + GL[′]-weakly convex. The usual optimality notion for
weakly-convex functions is approximate first order stationary points (Davis & Drusvyatskiy, 2018).

**Approximate first-order stationary point for weakly convex functions: In order to define approxi-**
mate stationary points, we also need the notion of Moreau envelope.
**Definition 3. The Moreau envelope of a function g : R[d][1]** _→_ R and parameter λ is:

_gλ(x)_ = minx′∈Rd1 g(x[′]) + (2λ)[−][1] _∥x −_ _x[′]∥[2]_ _._ (4)

The following lemma provides useful properties of the Moreau envelope.
**Lemma 3. For an L-weakly convex function g : R[d][1]** _→_ R and λ < 1/L, we have:

_(a) The minimizer ˆxλ(x) = arg minx′∈Rd1 g(x[′])+(2λ)[−][1]∥x_ _−_ _x[′]∥[2]_ _is unique and g(ˆxλ(x)) ≤_
_gλ(x) ≤_ _g(x). Furthermore, arg minx g(x) = arg minx gλ(x)._

_(b) gλ is λ[−][1](1 + (1 −_ _λL)[−][1])-smooth and thus differentiable, and_

_(c) minu∈∂g(ˆxλ(x))_ _[∥][u][∥≤]_ _[λ][−][1][∥]x[ˆ]λ(x) −_ _x∥_ = ∥∇gλ(x)∥.

First order stationary points (FOSP) of a non-smooth nonconvex function are well-defined, i.e., x[∗] is
a FOSP of a function g(x) if, 0 ∈ _∂f_ (x[∗]). However, unlike smooth functions, it is nontrivial to define
an approximate FOSP. For example, if we define an ε-FOSP as the point x with minu∈∂g(x) ∥u∥≤ _ε,_
where ∂g(x) denotes the subgradients of g at x, there may never exist such a point for sufficiently
small ε, unless x is exactly a FOSP. In contrast, by using above properties of the Moreau envelope of
a weakly convex function, it’s approximate FOSP can be defined as (Davis & Drusvyatskiy, 2018):
**Definition 4. Given an L-weakly convex function g, we say that x[∗]** _is an ε-first order stationary_
_point (ε-FOSP) if, ∥∇g1/2L(x[∗])∥≤_ _ε, where g1/2L is the Moreau envelope with parameter 1/2L._

Using Lemma 3, we can show that for any ε-FOSP x[∗], there exists ˆx such that ∥xˆ − _x[∗]∥≤_ _ε/2L and_
minsmaller thanu∈∂g(ˆx) _[∥] ε[u]. Other notions of FOSP proposed recently such as in Nouiehed et al. (2019) can be[∥≤]_ _[ε][. In other words, an][ ε][-FOSP is][ O][(][ε][)][ close to a point][ ˆ]x which has a subgradient_
shown to be a strict generalization of the above definition.

4 MAIN RESULTS

In this section, we present our main results. Assuming that the adversary employs Lipschitz and
gradient-Lipschitz algorithms (Assumption 2), Section 4.1 shows how to compute (stochastic)
subgradients of g(·) (defined in (2)) efficiently. Section 4.2 further shows that stochastic subgradient
descent (SGD) on g(·) can find an ϵ-FOSP in O _ϵ[−][4][]_ iterations while for the deterministic setting,
where the adversary uses only deterministic algorithms, Section 4.3 provides a proximal algorithm
that can find an ϵ-FOSP faster than SGD. For convenience, we denote  _gz,i(x) := f_ (x, _i(x, zi)) and_
_A_
recall g(x) := Ez maxi [k] gz,i(x) . For deterministic _i, we drop z and just use gi(x)._
_∈_ _A_
 

4.1 COMPUTING STOCHASTIC SUBGRADIENTS OF g(x)

In this section, we give a characterization of subgradients of g(x) and show how to compute stochastic
subgradients efficiently under the following assumption.
**Assumption 2. Algorithms Ai in (2) are G[′]-Lipschitz and L[′]-gradient Lipschitz as per Definition 1.**

Under Assumptions 1 and 2, Lemma 1 tells us that gz,i(x) is a G(1+ _G[′])-Lipschitz and L(1+_ _G[′])[2]_ +
_GL[′]-gradient Lipschitz function for every i ∈_ [k] with

_gz,i(x) =_ _xf_ (x, _i(x, zi)) + D_ _i(x, zi)_ _yf_ (x, _i(x, zi)),_ (5)
_∇_ _∇_ _A_ _A_ _· ∇_ _A_

where we recall that DAi(x, zi) ∈ R[d][1][×][d][2] is the Jacobian matrix of Ai(·, zi) : R[d][1] _→_ R[d][2] at x and
_xf_ (x, _i(x, zi)) denotes the partial derivative of f with respect to the first variable at (x,_ _i(x, zi))._
_∇_ _A_ _A_
While there is no known general recipe for computing D _i(x, zi) for an arbitrary algorithm_ _i, most_
_A_ _A_
algorithms used in practice such as stochastic gradient ascent (SGA), stochastic Nesterov accelerated
gradient (SNAG), ADAM, admit efficient ways of computing these derivatives e.g., higher package


-----

**Algorithm 1: Stochastic subgradient descent (SGD)**
**Input: initial point x0, step size η**

**1 for s = 0, 1, . . ., S do**

**2** Sample z1, · · ·, zk and compute _∇g(xs) according to eq. (6)._

**3** _xs+1_ _xs_ _η_ _g(xs)._
_←_ _−_ _∇_

**4 return ¯x ←** _xs, where s is uniformly sampled from[b]_ _{0, · · ·, S}._

[b]

in PyTorch (Grefenstette et al., 2019). For concreteness, we obtain expression for gradients of SGA
and SNAG in Section 5 but the principle behind the derivation holds much more broadly and can
be extended to most algorithms used in practice (Grefenstette et al., 2019). In practice, the cost of
computing _gz,i(x) in (5) is at most twice the cost of evaluating gz,i(x)—it consists a forward pass_
_∇_
for evaluating gz,i(x) and a backward pass for evaluating its gradient (Grefenstette et al., 2019).

Lemma 2 shows that g(x) := Ez maxi [k] gz,i(x) is a weakly convex function and a stochastic
_∈_
subgradient of g( ) can be computed by generating a random sample of z1, _, zk as:_

_·_   _· · ·_

_g(x) =_ _λj_ _gz,i(x) for any λ_ ∆k, where S(x) := argmax _gz,i(x)_ (6)
_∇_ _∇_ _∈_ _i_ [k]

_j_ _S(x)_ _∈_
_∈X_

b

Here ∆k is the k-dimensional probability simplex. It can be seen that Ez[ ∇[ˆ] _g(x)] ∈_ _∂g(x). Further-_
more, if all Ai are deterministic algorithms, then the above is indeed a subgradient of g.

4.2 CONVERGENCE RATE OF SGD

The SGD algorithm to solve (2) is given in Algorithm 1. The following theorem shows that Algorithm 1 finds an ϵ-FOSP of g(·) in S = O _ϵ[−][4][]_ iterations. Since each iteration of Algorithm 1
requires computing the gradient of gz,i(x) for each i [k] at a single point xs, this leads to a total of
  _∈_
_S = O_ _ϵ[−][4][]_ gradient computations for each gz,i(x).

**Theorem 1. ** _Under Assumptions 1 and 2, if S_ 16BL[b]G[2]ϵ[−][4] _and learning rate η_ =
_≥_
(2B/[L[b]G[2](S + 1)])[1][/][2] _then output of Algorithm 1 satisfies E[∥∇g1/2L[(¯]x)∥[2]] ≤_ _ϵ[2], where_ _L :=_

_L(1 + G[′])[2]_ + GL[′] _and_ _G := G(1 + G[′])._ [b]

b

[b] [b]

Theorem 1 claims that the expected norm squared of the Moreau envelope gradient of the output

[b]

point satisfies the condition for being an ϵ-FOSP. This, together with Markov’s inequality, implies
that at least half of x0, · · ·, xS are 2ϵ-FOSPs, so that the probability of outputting a 2ϵ-FOSP is at
least 0.5. Proposition 1 in Appendix C shows how to use an efficient postprocessing mechanism to
output a 2ϵ-FOSP with high probability. Theorem 1 essentially follows from the results of (Davis &
Drusvyatskiy, 2018), where the key insight is that, in expectation, the SGD procedure in Algorithm 1
almost monotonically decreases the Moreau envelope evaluated at xs i.e., E _g1/2L[(][x][s][)]_ is almost
monotonically decreasing. This shows that Algorithm 1 makes (almost) monotonic progress in ah i
b
precise sense and hence does not have limit cycles. In contrast, none of the other existing algorithms
for nonconvex-nonconcave minimax optimization enjoy such a guarantee. Note that this claim includes extra-gradient and optimistic gradient methods that have been touted as empirically mitigating
cycling behavior; see Appendix F for examples demonstrating nonconvergence of these methods.


4.3 PROXIMAL ALGORITHM WITH FASTER CONVERGENCE FOR DETERMINISTIC ALGORITHMS

While the rate achieved by SGD is the best known for weakly-convex optimization with a stochastic
subgradient oracle, faster algorithms exist for functions which can be written as maximum over a finite
_number of smooth functions with access to exact subgradients of these component functions. These_
conditions are satisfied when Ai are all deterministic and satisfy Assumption 2. A pseudocode of
such a fast proximal algorithm, inspired by Thekumparampil et al. (2019, Algorithm 3), is presented
in Algorithm 2. However, in contrast to the results of Thekumparampil et al. (2019), the following
theorem provides two alternate ways of implementing Step 3 of Algorithm 2, resulting in two different
(and incomparable) convergence rates.


-----

**Algorithm 2: Proximal algorithm**


**Input: initial point x0, target accuracy ϵ, smoothness parameter** _L_

**1** _ϵ ←_ _ϵ[2]/(64L[b])_

**2 for t = 0, 1, . . ., S do** [b]

**3** Find xs+1 such that

b

maxi [k] gi(xs+1) + _L_ _xs_ _xs+1_ minx maxi [k] gi(x) + _L_ _xs_ _x_ + _ϵ/4_
_∈_ _∥_ _−_ _∥[2]_ _≤_ _∈_ _∥_ _−_ _∥[2][]_

**4** **if maxi** [k] gi(xs+1) + _L_ _xs_ _xs+1_ maxi [k] gi(xs) 3ϵ/4 then

**5** **return∈** _xs_ [b] _∥_ _−_ _∥[2]_ _≥_ _∈_ _−_ [b] b

[b] b

**Theorem 2. Under Assumptions 1 and 2, if** _L := L(1 + G[′])[2]_ + GL[′] + kG(1 + G[′]) and S ≥
200LBϵ[b] _[−][2]_ _then Algorithm 2 returns xs satisfying ∥∇g1/2L[(][x][s][)][∥≤]_ _[ϵ][. Depending on whether we]_
_use (Thekumparampil et al., 2019, Algorithm 1) or cutting plane method (Lee et al., 2015) for[b]_
_solving Step 3 of Algorithm 2, the total number of gradient computations of eachb_ _gi is_ _O_ _Lϵ[2][3]B_ _or_

_O_ _LBϵ[2][ ·][ poly(][k][) log]_ _Lϵ_ _respectively._  b 

[e]

b b
 

Ignoring the parameters L, G, L[′] and G[′], the above theorem tells us that Algorithm 2 outputs an
_ϵ-FOSP using O_ _kϵ[−][3][]_ or O poly(k)ϵ[−][2] log [1]ϵ gradient queries to each gi depending on whether

Thekumparampil et al. (2019, Algorithm 3) or cutting plane method (Lee et al., 2015) was used
for implementing Step  3. While the proximal algorithm itself works even when   _Ai are randomized_
algorithms, there are no known algorithms that can implement Step (3) with fewer than O _ϵ[−][2][]_

stochastic gradient queries. Hence, this does not improve upon the O _ϵ[−][4][]_ guarantee for Algorithm 1

 

whendecrease the value Ai are randomized algorithms. The proof of Theorem 2 shows that the iterates g(xs), guaranteeing that there are no limit cycles for Algorithm 2 as well.  _xs monotonically_


5 SMOOTHNESS OF POPULAR ALGORITHMS

In this section, we show that two popular algorithms—namely, T -step stochastic gradient ascent
(SGA) and T -step stochastic Nesterov’s accelerated gradient ascent (SNAG)—are both Lipschitz and
gradient-Lipschitz satisfying Assumption 2 and hence are captured by our results in Section 4. Consider the setting f (x, y) = _n[1]_ _j_ [n] _[f][j][(][x, y][)][. Let][ z][ be a random seed that captures the randomness]_

_∈_
in the initial point as well as minibatch order in SGA and SNAG. We first provide the smoothness

P

results on T -step SGA for different assumptions on the shape of the function f and for T -step SNAG.
After giving these results, we make remarks interpreting their significance and the implications.

_T_ **-step SGA: For a given x and random seed z, the T** -step SGA update is given by: yt+1 =
_yt + η∇yfσ(t)(x, yt), where σ : [T_ ] → [N ] is a sample selection function and η is the stepsize.
Observe that with the same randomness z, the initial point does not depend on x i.e., y0(x) = y0(x[′]),
so Dy0 = 0. The following theorems provide the Lipschitz and gradient Lipschitz constants of yT (x)
(as generated by T -step SGA) for the general nonconvex-nonconcave setting as well as the settings in
which the function f is nonconvex-concave and nonconvex-strongly concave.
**Theorem 3 (General Case). Suppose for all j ∈** [n], fj satisfies Assumption 1. Then, for any fixed
_randomness z, T_ _-step SGA is (1 + ηL)[T]_ _-Lipschitz and 4(ρ/L) · (1 + ηL)[2][T]_ _-gradient Lipschitz._

**Theorem 4 (Concave Case). Suppose for all j ∈** [n], fj satisfies Assumption 1 and fj(x, ·) is concave
_for any x. Then, for any fixed randomness z, T_ _-step SGA is ηLT_ _-Lipschitz and (ρ/L) · (1 + ηLT_ )[3]_gradient Lipschitz._

**Theorem 5 (Strongly-concave Case). Suppose for all j ∈** [n], fj satisfies Assumption 1 and fj(x, ·)
_is α-strongly concave for any x. Then, for any fixed randomness z, T_ _-step SGA is κ-Lipschitz and_
4(ρ/L) · κ[3]-gradient Lipschitz, where κ = L/α is the condition number.

_T_ **-step SNAG: For a given random seed z, the T** -step SNAG update is given by:

_y˜t = yt + (1_ _θ)(yt_ _yt_ 1) and _yt+1 = ˜yt + η_ _yfσ(t)(x, ˜yt),_
_−_ _−_ _−_ _∇_ [b]


-----

where η is the stepsize, θ ∈ [0, 1] is the momentum parameter. The output of the algorithm is given
by A(x, z) = yT (x). Furthermore, we have the following guarantee.
**Theorem 6 (General Case). Suppose for all j ∈** [n], fj satisfies Assumption 1. Then, for any fixed
_seed z, T_ _-step SNAG is T_ (1+ _ηL/θ)[T]_ _-Lipschitz and 50(ρ/L)_ _·_ _T_ [3](1+ _ηL/θ)[2][T]_ _-gradient Lipschitz._

**Remarks on the Impact of the Smoothness Results: The Lipschitz and gradient Lipschitz param-**
eters for T -step SGA and T -step SNAG in the setting where f is nonconvex-nonconcave are all
exponential in T, the duration of the algorithm. In general, this seems unavoidable in the worst case
for the above algorithms and also seems to be the case for most of the other popular algorithms such
as ADAM, RMSProp, etc. In contrast, in the nonconvex-concave and nonconvex-strongly concave
settings, our results show that the smoothness parameters of T -step SGA are no longer exponential in
_T_ . In particular, in the nonconvex-concave the Lipschitz parameter is linear in T and the gradient
Lipschitz parameter is polynomial in T while in the nonconvex-strongly concave, the analogous
smoothness parameters are no longer dependent on T . We conjecture this is also the case for T -step
SNAG, though the proof appears quite tedious and hence, we opted to leave that for future work. For
problems of practical importance, however, we believe that the smoothness parameters are rarely
exponential in T . Our experimental results confirm this intuition (see Figure 2). Furthermore, note
that the function g(x) (2), and its stationary points, vary as one varies T . While in adversarial training,
_T is essentially determined by the power of adversary against which we wish the model to be robust,_
in GAN training, T is actually an algorithmic knob that can be tuned to improve the final generator
performance. Indeed, larger T does not necessarily lead to better generators and small values of T
are of vital interest; as is pointed out in the original paper (Goodfellow et al., 2014), fully optimizing
the discriminator can result in overfitting and too strong of a discriminator can limit the ability of
the generator to learn. Finally, while we prove the Lipschitz and gradient Lipschitz properties only
for SGA and SNAG, we believe that the same techniques could be used to prove similar results for
popular algorithms such as ADAM, RMSProp, etc. However, there are other algorithms, particularly
those involving projection that are not gradient Lipschitz (see Proposition 3 in Appendix E.5).

6 EMPIRICAL RESULTS

This section presents empirical results evaluating our SGD algorithm (Algorithm 1) for generative
adversarial networks (Goodfellow et al., 2014) and adversarial training (Madry et al., 2018). Our
results demonstrate that our framework results in stable monotonic improvement during training and
converges to desirable solutions in both GAN and adversarial training problems. While optimization
through the algorithm of the adversary increases the complexity of each update by a factor of two, our
results demonstrate that it also leads to faster convergence. Finally, we show the gradient norms do
not grow exponentially in the number of gradient ascent steps T taken by the adversary in practice.

**Generative Adversarial Networks. Generative adversarial network formulations can be charac-**
terized by the following minimax problem (Nagarajan & Kolter, 2017; Mescheder et al., 2018):

minθ maxω f (θ, ω) = Ex∼pX [ℓ(Dω(x))] + Ez∼pZ [ℓ(−Dω(Gθ(z)))]. (7)
In this formulation Gθ : Z →X is the generator network parameterized by θ that maps from the
latent space Z to the input space X, Dω : X → R is discriminator network parameterized by ω that
maps from the input space to real-valued logits, and p and p are the distributions over the input
_X_ _X_ _Z_
and latent spaces. The loss function defines the objective where ℓ(w) = − log(1 + exp(−w)) gives
the original “saturating” generative adversarial networks formulation (Goodfellow et al., 2014).

**Dirac–GAN. The Dirac-GAN (Mescheder et al., 2018) is a simple and common baseline for eval-**
uating the efficacy of generative adversarial network training methods. In this problem, the generator distribution Gθ(z) = δθ is a Dirac distribution concentrated at θ, the discriminator network
_Dω(x) =_ _ωx is linear, and the real data distribution p_ = δ0 is a Dirac distribution concentrated at
_−_ _X_
zero. The resulting objective after evaluating (7) with the loss ℓ(w) = − log(1 + exp(−w)) is

minθ maxω f (θ, ω) = ℓ(θω) + ℓ(0) = − log(1 + e[−][θω]) − log(2).

To mimic the real data distribution, the generator parameter θ should converge to θ[∗] = 0. Notably,
simultaneous and alternating gradient descent-ascent are known to cycle and fail to converge on
this problem (see Figure 1). We consider an instantiation of our framework where the discriminator


-----

Figure 2: **Adversarial** **training:**
_∥∇f_ (θ, A(θ))∥ as a function of number
of steps T taken by gradient ascent (GA)
algorithm A evaluated at multiple points
in the training procedure. The plot shows
that, in practice, the Lipschitz parameter of
GA does not grow exponentially in T .


Figure 1: Dirac-GAN: Generator parameters while
training using simultaneous and alternating gradient
descent-ascent (left), and our framework (right) with
& without optimizing through the discriminator. Under our framework, training is stable and converges to
correct distribution. Further, differentiating through
the discriminator results in faster convergence.


(a) Real Data (b) 10k (c) 40k (d) 70k (e) 100k (f) 130k (g) 150k

Figure 3: Mixture of Gaussians: Generated distribution at various steps during course of training.
We see that training is stable and results in monotonic progress towards the true distribution.

samples an initialization uniformly between [−0.1, 0.1] and performs T = 10 steps of gradient ascent
between each generator update. The learning rates for both the generator and the discriminator
are η = 0.01. We present the results in Figure 1. Notably, the generator parameter monotonically
converges to the optimal θ[∗] = 0 and matches the real data distribution using our training method. We
also show the performance when the generator descends using partial gradient _θf_ (x, (θ)) instead
_∇_ _A_
of the total gradient ∇f (θ, A(θ)) in Algorithm 1. This method is able to converge to the optimal
generator distribution but at a slower rate. Together, this example highlights that our method fixes
the usual cycling problem by reinitializing the discriminator and also that optimizing through the
discriminator algorithm is key to fast convergence. Additional results are given in Appendix G.

**Mixture of Gaussians. We now demonstrate that the insights we developed from the Dirac-GAN**
(stability and monotonic improvement) carry over to the more complex problem of learning a 2dimensional mixture of Gaussians. This is a common example and a number of papers (see, e.g., Metz
et al. 2017; Mescheder et al. 2017; Balduzzi et al. 2018) show that standard training methods using
simultaneous or alternating gradient descent-ascent can fail. The setup for the problem is as follows.
The real data distribution consists of 2-dimensional Gaussian distributions with means given by
_µ = [sin(φ), cos(φ)] for φ ∈{kπ/4}k[7]=0_ [and each with covariance][ σ][2][I][ where][ σ][2][ = 0][.][05][. For]
training, the real data x ∈ R[2] is drawn at random from the set of Gaussian distributions and the
latent data z ∈ R[16] is drawn from a standard normal distribution with batch sizes of 512. The
network for the generator and discriminator contain two and one hidden layers respectively, each of
which contain 32 neurons and ReLU activation functions. We consider the objective from (7) with
_ℓ(w) = −_ log(1 + exp(−w)) which corresponds to the “saturating” generative adversarial networks
formulation (Goodfellow et al., 2014). This objective is known to be difficult to train since with
typical training methods the generator gradients saturate early in training.

We show results using our framework in Figure 3 where the discriminator performs T = 15 steps of
gradient ascent and the initialization between each generator step is obtained by the default network
initialization in Pytorch. The generator and discriminator learning rates are both fixed to be η = 0.5.
We see that our method has stable improvement during the course of training and recovers close to
the real data distribution. We demonstrate in Appendix G that this result is robust by presenting the


-----

Figure 4: Adversarial training: Test accuracy during course of training where the attack used during
training is gradient ascent (GA) with learning rate (LR) of 4 and number of steps (Steps) of 10 but
evaluated against attacks with different Steps and LR. These plots show that training with a single
attack gives more robustness to even other attacks with different parameters or algorithms of similar
computational power. This empirical insight is complementary to our theoretical results that extend to
the setting of training against multiple smooth algorithms. Finally, using total gradient ∇f (θ, A(θ))
yields better robustness compared to using partial gradient _θf_ (θ, (θ)).
_∇_ _A_

final output of 10 runs of the procedure. Notably, the training algorithm recovers all the modes of the
distribution in each run. We also show results using Adam for the discriminator in Appendix G.

**Adversarial Training. Given a data distribution D over pairs of examples x ∈** R[d] and labels y ∈ [k],
parameters θ of a neural network, a set S ⊂ R[d] of allowable adversarial perturbations, and a loss
function ℓ(·, ·, ·) dependent on the network parameters and the data, adversarial training amounts to
considering a minmax optimization problem of the form minθ E(x,y)∼D[maxδ∈S ℓ(θ, x + δ, y)]. In
practice (Madry et al., 2018), the inner maximization problem maxδ _ℓ(θ, x + δ, y) is solved using_
_∈S_
_projected gradient ascent. However, as described in Section 5, this is not a smooth algorithm and does_
not fit our framework. So, we use gradient ascent, without projection, for the inner maximization.

We run an adversarial training experiment with the MNIST dataset, a convolutional neural network,
and the cross entropy loss function. We compare Algorithm 1 with usual adversarial training (Madry
et al., 2018) which descends _θf_ (θ, (θ)) instead of _f_ (θ, (θ)), and a baseline of standard
_∇_ _A_ _∇_ _A_
training without adversarial training. For each algorithm, we train for 100 passes over the training set
using a batch size of 50. The minimization procedure has a fixed learning rate of η1 = 0.0001 and the
maximization procedure runs for T = 10 steps with a fixed learning rate of η2 = 4. We evaluate the
test classification accuracy during the course of training against gradient ascent or Adam optimization
adversarial attacks. The results are presented in Figure 4 where the mean accuracies are reported
over 5 runs and the shaded regions show one standard deviation around the means. We observe
that the adversarial training procedure gives a significant boost in robustness compared to standard
training. Moreover, consistent with the previous experiments, our algorithm which uses total gradient
outperforms standard adversarial training which uses only partial gradient. We present results against
more attacks in Appendix G. As suggested in Section 5, we also find that in practice, the gradient
norms ∥∇f (θ, A(θ))∥ do not grow exponentially in the number of gradient ascent steps T in the
adversary algorithm A (see Figure 2). For further details and additional results see Appendix G.

7 CONCLUSION

Motivated by the facts that (i) nonconcave maximization is NP-hard in the worst case and (ii) in
practice, simple gradient based algorithms are used for maximization, this paper presents a new
framework for solving nonconvex-nonconcave minimax optimization problems. Assuming that the
min player has knowledge of the smooth algorithms being used by max player, we propose new
efficient algorithms and verify that these algorithms find desirable solutions in practice on smallscale generative adversarial network and adversarial training problems. Furthermore, the framework
proposed in this paper extends naturally to two player non-zero sum games as well. An important
future direction of work is to scale our algorithm to large scale, state of the art GAN benchmarks.
The key challenge here is that reinitializing the discriminator parameters necessitates running several
steps of gradient ascent for discriminator for every generator update. One approach to overcome this
could be to use pretrained networks for initializing the discriminator.


-----

REFERENCES

Leonard Adolphs, Hadi Daneshmand, Aurelien Lucchi, and Thomas Hofmann. Local saddle point
optimization: A curvature exploitation approach. In International Conference on Artificial Intelli_gence and Statistics, pp. 486–495, 2019._

Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks.
In International Conference on Machine Learning, pp. 214–223, 2017.

Kendall Atkinson, Weimin Han, and David E Stewart. Numerical solution of ordinary differential
_equations, volume 108. John Wiley & Sons, 2011._

James P Bailey, Gauthier Gidel, and Georgios Piliouras. Finite regret and cycles with fixed step-size
via alternating gradient descent-ascent. In Conference on Learning Theory, pp. 391–407. PMLR,
2020.

David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, Karl Tuyls, and Thore Graepel.
The mechanics of n-player differentiable games. In International Conference on Machine Learning,
pp. 354–363, 2018.

Tamer Ba¸sar and Geert Jan Olsder. Dynamic noncooperative game theory. SIAM, 1998.

Dimitri P Bertsekas. Convex optimization theory. Athena Scientific Belmont, 2009.

Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim vSrndi´c, Pavel Laskov,
Giorgio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In
_European conference on machine learning and knowledge discovery in databases, pp. 387–402,_
2013.

Benjamin Chasnov, Tanner Fiez, and Lillian J Ratliff. Opponent anticipation via conjectural variations.
In Smooth Games Optimization and Machine Learning Workshop at NeurIPS 2020: Bridging
_Game Theory and Deep Learning, 2020._

Earl A Coddington and Norman Levinson. Theory of ordinary differential equations. Tata McGrawHill Education, 1955.

Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient descent in
min-max optimization. In Proceedings of the 32nd International Conference on Neural Information
_Processing Systems, pp. 9256–9266, 2018a._

Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient descent in
min-max optimization. In Advances in Neural Information Processing Systems, pp. 9236–9246,
2018b.

Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with
optimism. In International Conference on Learning Representations (ICLR 2018), 2018.

Constantinos Daskalakis, Stratis Skoulakis, and Manolis Zampetakis. The complexity of constrained
min-max optimization. In ACM Symposium on Theory of Computing, 2021.

Damek Davis and Dmitriy Drusvyatskiy. Stochastic subgradient method converges at the rate
_o(k[−][1][/][4]) on weakly convex functions. arXiv preprint arXiv:1802.02988, 2018._

Tristan Deleu, Tobias Würfl, Mandana Samiei, Joseph Paul Cohen, and Yoshua Bengio. Torchmeta:
A meta-learning library for pytorch. arXiv preprint arXiv:1909.06576, 2019.

Farzan Farnia and Asuman Ozdaglar. Do gans always have nash equilibria? In International
_Conference on Machine Learning, pp. 3029–3039, 2020._

Tanner Fiez and Lillian Ratliff. Local convergence analysis of gradient descent ascent with finite
timescale separation. In International Conference on Learning Representations, 2021.

Tanner Fiez, Benjamin Chasnov, and Lillian Ratliff. Implicit learning dynamics in stackelberg games:
Equilibria characterization, convergence analysis, and empirical study. In International Conference
_on Machine Learning, pp. 3133–3144, 2020._


-----

Jakob Foerster, Richard Y Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor Mordatch. Learning with opponent-learning awareness. In International Conference on Autonomous
_Agents and MultiAgent Systems, pp. 122–130, 2018._

Yoav Freund, Michael Kearns, Yishay Mansour, Dana Ron, Ronitt Rubinfeld, and Robert E Schapire.
Efficient algorithms for learning to play repeated games against computationally bounded adversaries. In Proceedings of IEEE 36th Annual Foundations of Computer Science, pp. 332–341. IEEE,
1995.

Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial networks. In Advances in Neural
_Information Processing Systems, 2014._

Edward Grefenstette, Brandon Amos, Denis Yarats, Phu Mon Htut, Artem Molchanov, Franziska
Meier, Douwe Kiela, Kyunghyun Cho, and Soumith Chintala. Generalized inner loop meta-learning.
_arXiv preprint arXiv:1910.01727, 2019._

Joseph Y Halpern, Rafael Pass, and Lior Seeman. Decision theory with resource-bounded agents.
_Topics in cognitive science, 6(2):245–257, 2014._

Philip Hartman. Ordinary Differential Equations. Society for Industrial and Applied Mathematics,
second edition, 2002. doi: 10.1137/1.9780898719222.

Nicholas JA Harvey, Christopher Liaw, and Sikander Randhawa. Simple and optimal high-probability
bounds for strongly-convex stochastic gradient descent. arXiv preprint arXiv:1909.00843, 2019.

Ya-Ping Hsieh, Panayotis Mertikopoulos, and Volkan Cevher. The limits of min-max optimization
algorithms: Convergence to spurious non-critical sets. arXiv preprint arXiv:2006.09065, 2020.

Chi Jin, Praneeth Netrapalli, and Michael Jordan. What is local optimality in nonconvex-nonconcave
minimax optimization? In International Conference on Machine Learning, pp. 4880–4889, 2020.

Sham Kakade, Shai Shalev-Shwartz, Ambuj Tewari, et al. On the duality of strong convexity and
strong smoothness: Learning applications and matrix regularization. Unpublished Manuscript,
_http://ttic. uchicago. edu/shai/papers/KakadeShalevTewari09. pdf, 2(1), 2009._

Vijay Keswani, Oren Mangoubi, Sushant Sachdeva, and Nisheeth K Vishnoi. Gans with first-order
greedy discriminators. arXiv preprint arXiv:2006.12376, 2020.

Weiwei Kong and Renato DC Monteiro. An accelerated inexact proximal point method for solving
nonconvex-concave min-max problems. arXiv preprint arXiv:1905.13433, 2019.

Galina M Korpelevich. The extragradient method for finding saddle points and other problems.
_Matecon, 12:747–756, 1976._

Alexey Kurakin, Ian J Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In
_International Conference on Learning Representations, 2017._

Yin Tat Lee, Aaron Sidford, and Sam Chiu-wai Wong. A faster cutting plane method and its
implications for combinatorial and convex optimization. In 2015 IEEE 56th Annual Symposium on
_Foundations of Computer Science, pp. 1049–1065. IEEE, 2015._

Qi Lei, Sai Ganesh Nagarajan, Ioannis Panageas, and xiao wang. Last iterate convergence in no-regret
learning: constrained min-max optimization for convex-concave landscapes. In International
_Conference on Artificial Intelligence and Statistics, pp. 1441–1449, 2021._

Alistair Letcher. On the impossibility of global convergence in multi-loss optimization. In Interna_tional Conference on Learning Representations, 2021._

Alistair Letcher, Jakob Foerster, David Balduzzi, Tim Rocktäschel, and Shimon Whiteson. Stable opponent shaping in differentiable games. In International Conference on Learning Representations,
2019.

Haochuan Li, Yi Tian, Jingzhao Zhang, and Ali Jadbabaie. Complexity lower bounds for nonconvexstrongly-concave min-max optimization. arXiv preprint arXiv:2104.08708, 2021.


-----

Tianyi Lin, Chi Jin, and Michael Jordan. On gradient descent ascent for nonconvex-concave minimax
problems. In International Conference on Machine Learning, pp. 6083–6093, 2020a.

Tianyi Lin, Chi Jin, Michael Jordan, et al. Near-optimal algorithms for minimax optimization. In
_Conference on Learning Theory, pp. 2738–2779, 2020b._

Songtao Lu, Ioannis Tsaknakis, Mingyi Hong, and Yongxin Chen. Hybrid block successive approximation for one-sided non-convex min-max problems: algorithms and applications. IEEE
_Transactions on Signal Processing, 2020._

Luo Luo, Haishan Ye, Zhichao Huang, and Tong Zhang. Stochastic recursive gradient descent ascent
for stochastic nonconvex-strongly-concave minimax problems. Advances in Neural Information
_Processing Systems, 33, 2020._

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
_Learning Representations, 2018._

Oren Mangoubi and Nisheeth K Vishnoi. Greedy adversarial equilibrium: An efficient alternative
to nonconvex-nonconcave min-max optimization. In ACM Symposium on Theory of Computing,
2021.

Eric Mazumdar, Lillian J Ratliff, and S Shankar Sastry. On gradient-based learning in continuous
games. SIAM Journal on Mathematics of Data Science, 2(1):103–131, 2020.

Eric V Mazumdar, Michael I Jordan, and S Shankar Sastry. On finding local nash equilibria (and
only local nash equilibria) in zero-sum games. arXiv preprint arXiv:1901.00838, 2019.

Lars Mescheder, Sebastian Nowozin, and Andreas Geiger. The numerics of gans. In Advances in
_Neural Information Processing Systems, pp. 1823–1833, 2017._

Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do
actually converge? In International Conference on Machine Learning, pp. 3481–3490, 2018.

Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-Dickstein. Unrolled generative adversarial
networks. In International Conference on Learning Representations, 2017.

Vaishnavh Nagarajan and J Zico Kolter. Gradient descent gan optimization is locally stable. In
_Advances in Neural Information Processing Systems, pp. 5591–5600, 2017._

Arkadi Nemirovski. Prox-method with rate of convergence o (1/t) for variational inequalities with
lipschitz continuous monotone operators and smooth convex-concave saddle point problems. SIAM
_Journal on Optimization, 15(1):229–251, 2004._

J v Neumann. Zur theorie der gesellschaftsspiele. Mathematische annalen, 100(1):295–320, 1928.

Maher Nouiehed, Maziar Sanjabi, Tianjian Huang, Jason D Lee, and Meisam Razaviyayn. Solving a
class of non-convex min-max games using iterative first order methods. In Advances in Neural
_Information Processing Systems, pp. 14934–14942, 2019._

Dmitrii M Ostrovskii, Andrew Lowy, and Meisam Razaviyayn. Efficient search of first-order nash
equilibria in nonconvex-concave smooth min-max problems. arXiv preprint arXiv:2002.07919,
2020.

Hassan Rafique, Mingrui Liu, Qihang Lin, and Tianbao Yang. Weakly-convex–concave min–max
optimization: provable algorithms and applications in machine learning. Optimization Methods
_and Software, pp. 1–35, 2021._

L. J. Ratliff, S. A. Burden, and S. S. Sastry. Characterization and computation of local Nash equilibria
in continuous games. In Allerton Conference on Communication, Control, and Computing, pp.
917–924, 2013.

Lillian J Ratliff, Samuel A Burden, and S Shankar Sastry. On the characterization of local Nash
equilibria in continuous games. IEEE Transactions on Automatic Control, 61(8):2301–2307, 2016.


-----

Julia Robinson. An iterative method of solving a game. Annals of mathematics, pp. 296–301, 1951.

Tuomas W Sandhlom and Victor RT Lesser. Coalitions among computationally bounded agents.
_Artificial intelligence, 94(1-2):99–137, 1997._

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning
_Representations, 2014._

Kiran K Thekumparampil, Prateek Jain, Praneeth Netrapalli, and Sewoong Oh. Efficient algorithms
for smooth minimax optimization. Advances in Neural Information Processing Systems, 32:
12680–12691, 2019.

Yuanhao Wang, Guodong Zhang, and Jimmy Ba. On solving minimax optimization locally: A
follow-the-ridge approach. In International Conference on Learning Representations, 2020.

Chongjie Zhang and Victor Lesser. Multi-agent learning with policy prediction. In AAAI Conference
_on Artificial Intelligence, 2010._

Guojun Zhang, Pascal Poupart, and Yaoliang Yu. Optimality and stability in non-convex-non-concave
min-max optimization. arXiv preprint arXiv:2002.11875, 2020a.

Guojun Zhang, Kaiwen Wu, Pascal Poupart, and Yaoliang Yu. Newton-type methods for minimax
optimization. arXiv preprint arXiv:2006.14592, 2020b.

Siqi Zhang, Junchi Yang, Cristóbal Guzmán, Negar Kiyavash, and Niao He. The complexity of
nonconvex-strongly-concave minimax optimization. arXiv preprint arXiv:2103.15888, 2021.

Renbo Zhao. A primal dual smoothing framework for max-structured nonconvex optimization. arXiv
_preprint arXiv:2003.04375, 2020._


-----

A DETAILED RELATED WORK

**Nonconvex-Nonconcave Zero-Sum Games. The existing work on nonconvex-nonconcave zero-**
sum games has generally focused on (1) defining and characterizing local equilibrium solution
concepts and (2) analyzing the local stability and convergence behavior of gradient-based learning
algorithms around fixed points of the dynamics. The concentration on local analysis stems from
the inherent challenges that arise in nonconvex-nonconcave zero-sum games from both a dynamical
systems perspective and a computational perspective. In particular, it is know that broad classes of
gradient-based learning dynamics can admit limit cycles and other non-trivial periodic orbits that
are antithetical to any type of global convergence guarantee in this class of games (Hsieh et al.,
2020; Letcher, 2021). Moreover, on constrained domains, it has been shown that finding even a local
equilibrium is computationally intractable (Daskalakis et al., 2021).

A number of local equilibrium notions for nonconvex-nonconcave zero-sum games now exist with
characterizations in terms of gradient-based conditions relevant to gradient-based learning. This
includes the local Nash (Ratliff et al., 2013; Ratliff et al., 2016) and local minmax (Stackelberg) (Jin
et al., 2020; Fiez et al., 2020) equilibrium concepts, which both amount to local refinements and
characterizations of historically standard game-theoretic equilibrium notions. In terms of provable
guarantees, algorithms incorporating higher-order gradient information have been proposed and
analyzed that guarantee local convergence to only local Nash equilibria (Adolphs et al., 2019;
Mazumdar et al., 2019) or local convergence to only local minmax equilibria (Wang et al., 2020;
Zhang et al., 2020b; Fiez et al., 2020) in nonconvex-nonconcave zero-sum games. Beyond the
local Nash and minmax equilibrium, notions including the proximal equilibrium concept (Farnia &
Ozdaglar, 2020), which is a class between the set of local Nash and local minmax equilibria, and the
local robust equilibrium concept (Zhang et al., 2020a), which includes both local minmax and local
maxmin equilibria, have been proposed and studied. It is worth noting that a shortcoming of each of
the local equilibrium notions is that may fail to exist on unconstrained domains.

Significant attention has been given to the local stability and convergence of simultaneous gradient
descent-ascent in nonconvex-nonconcave zero-sum games. This stems from the fact that it is the
natural analogue of learning dynamics for zero-sum game optimization to gradient descent for
function optimization. Moreover, simultaneous gradient descent-ascent is know to often perform
reasonably well empirically and is ubiquitous in a number of applications such as in training generative
adversarial networks and adversarial learning. However, it has been shown that while local Nash are
guaranteed to be stable equilibria of simultaneous gradient descent-ascent (Mazumdar et al., 2020;
Daskalakis & Panageas, 2018b; Jin et al., 2020), local minmax may not be unless there is sufficient
timescale separation between the minimizing and maximizing players (Jin et al., 2020; Fiez & Ratliff,
2021). Specific to generative adversarial networks, it has been shown that simultaneous gradient
descent-ascent locally converges to local equilibria under certain assumptions on the generator
network and the data distribution (Nagarajan & Kolter, 2017; Mescheder et al., 2018). Later in
this section we discuss in further detail learning dynamics studied previously in games which bear
resemblance to that which we consider in this paper depending on the model of the maximizing
player.

The challenges of nonconvex-nonconcave zero-sum games we have highlighted limit the types of
provable guarantees that can be obtained and consequently motivate tractable relaxations including to
nonconvex-concave zero-sum games and the general framework we formulate in this work. Before
moving on, we mention that from a related perspective, a line of recent work (Keswani et al., 2020;
Mangoubi & Vishnoi, 2021) in nonconvex-nonconcave zero-sum games proposes relaxed equilibrium
notions that are shown to be computable in polynomial time and are guaranteed to exist. At a high
level, the equilibria correspond to a joint strategy at which the maximizing player is at an approximate
local maximum of the cost function and the minimizing player is at an approximate local minimum
of a smoothed and relaxed best-response function of the maximizing player. The aforementioned
works are similar to this paper in the sense that the minimizing player faces a maximizing player
with computational restrictions, but diverge in terms of the model of the maximizing player and the
algorithms for solving the problem.

**Nonconvex-Concave Zero-Sum Games. The past few years has witnessed a significant amount of**
work on gradient-based dynamics in nonconvex-concave zero-sum games. The focus of existing work
on nonconvex-concave zero-sum games has key distinctions from that in nonconvex-nonconcave


-----

zero-sum games. Generally, the work on nonconvex-concave zero-sum games has analyzed dynamics
on constrained domains, where typically the strategy space of the maximizing player is constrained
to a closed convex set and occasionally the minimizing player also faces a constraint. In contrast,
nonconvex-nonconcave zero-sum games have generally been analyzed on unconstrained domains.
Moreover, instead of focusing on computing notions of game-theoretic equilibrium as is typical in
nonconvex-nonconcave zero-sum games, the body of work on nonconvex-concave zero-sum games
has focused on achieving stationarity of the game cost function f (·, ·) or the best-response function
Φ(·) = maxy f (·, y).

The structure present in nonconvex-concave zero-sum games has been shown to simplify the problem
compared to nonconvex-nonconcave zero-sum games so that global finite-time convergence guarantees are achievable. Thus, work in this direction has focused on improving the rates of convergence
in terms of the gradient complexity to find ϵ–approximate stationary points of f (·, ·) or Φ(·), both
with deterministic and stochastic gradients. Guarantees on the former notion of stationarity can be
translated to guarantees on the latter notion of stationarity with extra computational cost (Lin et al.,
2020a).

For the the class of nonconvex-strongly-concave zero-sum games, a series of works design algorithms
that are shown to obtain ϵ–approximate stationary points of the functions f (·, ·) or Φ(·) with a
gradient complexity of _O(ϵ[−][2]) in terms of ϵ in the deterministic setting (Jin et al., 2020; Rafique_
et al., 2021; Lu et al., 2020; Lin et al., 2020a;b). In the deterministic nonconvex-strongly concave
problem, the notions of stationarity are equivalent in terms of the dependence on ϵ up to a logarithmic

[e]
dependence (Lin et al., 2020a). Lower bounds for this problem have also been established (Zhang
et al., 2021; Li et al., 2021). In the stochastic nonconvex-strongly-concave problem, existing work
has developed algorithms that are shown to obtain ϵ–approximate stationary points of the function
Φ(·) in gradient complexities of _O(ϵ[−][4]) (Rafique et al., 2021; Jin et al., 2020; Lin et al., 2020a) and_
_O(ϵ[−][3]) (Luo et al., 2020) in terms of ϵ dependence._

[e]

In the deterministic nonconvex-concave problem, a number of algorithms with provable guarantees

e

to ϵ–approximate stationary points of the function f (·, ·) have been shown with gradient complexities
of _O(ϵ[−][4]) (Lu et al., 2020),_ _O(ϵ[−][3][.][5]) (Nouiehed et al., 2019), and_ _O(ϵ[−][2][.][5]) (Ostrovskii et al., 2020;_
Lin et al., 2020b). Similarly, in this class of problems, there exist results on algorithms that guarantee
convergence to an ϵ–approximate stationary points of the function Φ( ) with gradient complexities

[e] [e] [e] _·_
_O(ϵ[−][6]) (Rafique et al., 2021; Jin et al., 2020; Lin et al., 2020a) and_ _O(ϵ[−][3]) (Thekumparampil_
et al., 2019; Zhao, 2020; Kong & Monteiro, 2019; Lin et al., 2020b). Finally, existing results in the
stochastic setting for achieving ane _ϵ–approximate stationary point of Φ(·) show gradient complexities[e]_
of _O(ϵ[−][6]) (Rafique et al., 2021) and_ _O(ϵ[−][8]) (Lin et al., 2020a)._

In this work, we build on the developments for nonconvex-concave problems to obtain our results.

[e] [e]

**Gradient-Based Learning with Opponent Modeling. A number of gradient-based learning schemes**
have been derived is various classes of games based on the following idea: if a player knows how the
opponents in a game are optimizing their cost functions, then it is natural to account for this behavior
in the players own optimization procedure. The simultaneous gradient descent learning dynamics
can be viewed as the simplest instantiation of this perspective, where each player is optimizing their
own cost function assuming that all other players in the game will remain fixed. In general, the more
sophisticated existing learning dynamics based on opponent modeling assume the opponents are
doing gradient descent on their cost function and this prediction is incorporated into the objective
being optimized in place of the current strategies of opponents. A key conceptual distinction between
this approach and our work is that in existing opponent modeling methods the dynamics of the players
are always updated simultaneously whereas the procedure we consider is sequential in nature with
the opponent initializing again at each interaction. Moreover, the types of guarantees we prove are
distinct compared to existing work in this realm.

In this modern literature, gradient-based learning with opponent modeling dates back to the work
of Zhang & Lesser (2010). They study simple two-player, two-action, general-sum matrix games,
and analyze a set of learning dynamics called iterated descent descent with policy prediction (IGAPP) and show asymptotic convergence to a Nash equilibrium. In this set of learning dynamics,
each player assumes the other player is doing gradient descent and this prediction is used in the
objective. In particular, each player i has a choice variable x[i] and a cost function fi(x[i], x[−][i]) that after


-----

incorporating the prediction becomes fi(x[i]t[, x]t[−][i] _γ_ _if_ _i(x[i]t[, x]t[−][i][))][. To optimize the objective,]_
_−_ _∇−_ _−_
each player takes a first-order Taylor expansion of their cost function to give the augmented objective

_fi(x[i]t[, x]t[−][i]_ _γ_ _if_ _i(x[i]t[, x]t[−][i][))][ ≈]_ _[f][i][(][x]t[i][, x]t[−][i][)][ −]_ _[γ][∇][−][i][f][i][(][x]i[t][, x][t]_ _i[)][⊤][∇][−][i][f][−][i][(][x][i]t[, x]t[−][i][)][.]_
_−_ _∇−_ _−_ _−_

Each player in the game simultaneously follows the gradient of their augmented objective which is
given by
_ifi(x[i]t[, x]t[−][i][)][ −]_ _[γ][∇][−][i,i][f][i][(][x]i[t][, x][t]_ _i[)][⊤][∇][−][i][f][−][i][(][x][i]t[, x]t[−][i][)][.]_
_∇_ _−_
This gradient computation is derived based on the fact that the assumed update of the other player
_if_ _i(x[i]t[, x]t[−][i][)][ does not depend on the optimization variable.]_
_∇−_ _−_

Similar ideas have recently been revisited in more general nonconvex multiplayer games (Foerster
et al., 2018; Letcher et al., 2019). In learning with opponent learning awareness (LOLA) (Foerster
et al., 2018), players again assume the other players are doing gradient descent and take their objective
to be fi(x[i]t[, x]t[−][i] _γ_ _if_ _i(x[i]t[, x]t[−][i][))][. To derive the learning rule, an augmented objective is again]_
_−_ _∇−_ _−_
formed by computing a first-order Taylor expansion, but now the term _if_ _i(x[i]t[, x]t[−][i][)][ in the]_
_∇−_ _−_
augmented objective is treated as dependent on the optimization variable so that the gradient of the
augmented objective is given by

_ifi(x[i]t[, x]t[−][i][)][ −]_ _[γ][∇][−][i,i][f][i][(][x]i[t][, x][t]_ _i[)][⊤][∇][−][i][f][−][i][(][x]t[i][, x]t[−][i][)][ −]_ _[γ][∇][−][i,i][f][−][i][(][x]t[i][, x]t[−][i][)][⊤][∇][−][i][f][i][(][x]i[t][, x][t]_ _i[)][.]_
_∇_ _−_ _−_

Finally, to arrive at the final gradient update for each player, the middle term in the equation above is
removed and each player takes steps along the gradient update

_ifi(x[i]t[, x]t[−][i][)][ −]_ _[γ][∇][−][i,i][f][−][i][(][x]t[i][, x]t[−][i][)][⊤][∇][−][i][f][i][(][x]i[t][, x][t]_ _i[)][.]_
_∇_ _−_

While no convergence results are given for LOLA, a follow-up work shows local convergence guarantees to stable fixed points for IGA-PP and learning dynamics called stable opponent shaping (SOS)
that interpolate between IGA-PP and LOLA (Letcher et al., 2019). A related work derives learning
dynamic based on the idea that the opponent selects a best-response to the chosen strategy (Fiez
et al., 2020). The resulting learning dynamics can be viewed as LOLA with the opponent selecting a
Newton learning rate. For nonconvex-nonconcave zero-sum games, local convergence guarantees to
only local Stackelberg equilibrium are given in for this set of learning dynamics (Fiez et al., 2020). It
is worth remarking that gradient-based learning with opponent modeling is historically rooted in the
general framework of consistent conjectural variations (see, e.g., Ba¸sar & Olsder 1998, Chapter 4.6),
a concept that is now being explored again and is closely related to the previously mentioned learning
dynamics (Chasnov et al., 2020).

Perhaps the closest work on gradient-based learning with opponent modeling to this paper is that
of unrolled generative adversarial networks (Metz et al., 2017). In unrolled generative adversarial
networks, the generator simulates the discriminator doing a fixed number of gradient steps from
the current parameter configurations of the generator and discriminator. The resulting discriminator
parameters are then used in place of the current discriminator parameters in the generator objective.
The generator then updates following the gradient of this objective, optimizing through the rolled out
discriminator update by computing the total derivative. Simultaneously with the generator update, the
discriminator updates its parameters by performing a gradient step on its objective. In our framework,
for generative adversarial networks when the discriminator is modeled as performing T -steps of
gradient ascent, the procedure we propose is similar but an important difference is that when the
generator simulates the discriminator unrolling procedure the discriminator parameters are initialized
from scratch and there is no explicit discriminator being trained simultaneously with the generator.

**Extragradient methods: Starting from the seminal work of Korpelevich (1976), extragradient**
methods have been studied extensively for solving minimax optimization problems. While such
methods have been shown to obtain faster convergence rates (Nemirovski, 2004) and final iterate
convergence (Daskalakis et al., 2018; Lei et al., 2021) for convex-concave minimax optimization,
there is no result showing that it converges in general nonconvex-nonconcave settings. In fact, in
Appendix F we present simple examples demonstrating that the extragradient method as well as
another variant called optimistic gradient descent ascent, do not converge for general nonconvexnonconcave minimax optimization.

**Games with computationally bounded adversaries: There are also a few works in the game theory**
literature which consider resource/computationally bounded agents. For example Freund et al. (1995)


-----

considers repeated games between resource bounded agents, (Sandhlom & Lesser, 1997) considers
coalition formation between resource bounded agents in cooperative games and Halpern et al. (2014)
shows that resource constraints in otherwise rational players might lead to some commonly observed
human behaviors while making decisions. However, the settings, models of limited computation and
the focus of results considered in all of these prior works are distinct from those of this paper.

**Stability of algorithms in numerical analysis: To our knowledge such results on the smoothness**
of the classes of algorithms we study—i.e., gradient-based updates such as SGA and SNAG—with
respect to problem parameters (e.g., in this case, x) have not been shown in the machine learning
and optimization literature. This being said, in the study of dynamical systems—more specifically
differential equations—the concept of continuity (and Lipschitzness) with respect to parameters and
initial data has been studied using a variational approach wherein the continuity of the solution of
the differential equation is shown to be continuous with respect to variations in the parameters or
initial data by appealing to nonlinear variation of parameters results such as the Bellman-Grownwall
inequality or Alekseev’s theorem (see classical references on differential equations such as Coddington
& Levinson (1955, Chapter 2) or Hartman (2002, Chapter IV.2)).

In numerical methods, such results on the “smoothness” or continuity of the differential equation with
respect to initial data or problem parameters are used to understand stability of particular numerical
methods (see, e.g., Atkinson et al. 2011, Chapter 1.2). In particular, a initial value problem is only
considered well-posed if there is continuous dependence on initial data. For instance, the simple
scalar differential equation

_y˙(t) = −y(t) + 1, 0 ≤_ _t ≤_ _T, y(0) = 1_

has solution y(t) ≡ 1, yet the perturbed problem,

_y˙ϵ(t) =_ _yϵ(t) + 1, 0_ _t_ _T, yϵ(0) = 1 + ϵ,_
_−_ _≤_ _≤_

has solution yϵ(t) = 1 + ϵe[−][t] so that


_y(t)_ _yϵ(t)_ _ϵ_ _, 0_ _t_ _T._
_|_ _−_ _| ≤|_ _|_ _≤_ _≤_

If the maximum error ∥yϵ−y∥∞ is (much) larger than ϵ then the initial value problem is ill-conditioned
and any typical attempt to numerically solve such a problem will lead to large errors in the computed
solution. In short, the stability properties of a numerical method (i.e., discretization of the differential
equation) are fundamentally connected to the continuity (smoothness) with respect to initial data.

Observe that methods such as gradient ascent can be viewed as a discretization of an differential
equation:
_y˙(t) = ∇yf_ (x, y(t)) −→ _yk+1 = yk + η∇yf_ (x, yk).

As such, the techniques for showing continuity of the solution of a differential equation with respect
to initial data or other problem parameters (e.g., in this case x) can be adopted to show smoothness
of the T -step solution of the discretized update. Our approach to showing smoothness, on the other
hand, leverages the recursive nature of the discrete time updates defining the classes of algorithms
we study. This approach simplifies the analysis by directly going after the smoothness parameters
using the update versus solving the difference (or differential) equation for yT (x) and then finding
the smoothness parameters which is the method typically used in numerical analysis of differential
equations. An interesting direction of future research is to more formally connect the stability analysis
from numerical analysis of differential equations to robustness of adversarial learning to initial data
and even variations in problem parameters.

B PROOF OF RESULTS IN SECTION 3

_Proof of Lemma 1. For any fixed z, we note that A(·, z) is a deterministic algorithm. Consequently,_
it suffices to prove the lemma for a deterministic algorithm A(·). By chain rule, the derivative of
_f_ (x, A(x)) is given by:

_f_ (x, (x)) = _xf_ (x, (x)) + D (x) _yf_ (x, (x)), (8)
_∇_ _A_ _∇_ _A_ _A_ _· ∇_ _A_

where DA(x) ∈ R[d][1][×][d][2] is the derivative of A(·) : R[d][1] _→_ R[d][2] at x and ∇xf (x, A(x)) and
_yf_ (x, (x)) denote the partial derivatives of f with respect to the first and second variables
_∇_ _A_


-----

respectively at (x, A(x)). An easy computation shows that

_f_ (x, (x)) _xf_ (x, (x)) + _D_ (x) _yf_ (x, (x))
_∥∇_ _A_ _∥≤∥∇_ _A_ _∥_ _∥_ _A_ _∥· ∥∇_ _A_ _∥_

_≤_ _G + G[′]_ _· G = (1 + G[′])G._

This shows that f (x, A(x)) is (1 + G[′])G-Lipschitz. Similarly, we have:

_f_ (x1, (x1)) _f_ (x2, (x2))
_∥∇_ _A_ _−∇_ _A_ _∥_
_xf_ (x1, (x1)) _xf_ (x2, (x2)) + _D_ (x1) _yf_ (x1, (x1)) _D_ (x2) _yf_ (x2, (x2)) _._
_≤∥∇_ _A_ _−∇_ _A_ _∥_ _∥_ _A_ _∇_ _A_ _−_ _A_ _∇_ _A_ _∥_

For the first term, we have:


_xf_ (x1, (x1)) _xf_ (x2, (x2))
_∥∇_ _A_ _−∇_ _A_ _∥_
_xf_ (x1, (x1)) _xf_ (x2, (x1)) + _xf_ (x2, (x1)) _xf_ (x2, (x2))
_≤∥∇_ _A_ _−∇_ _A_ _∥_ _∥∇_ _A_ _−∇_ _A_ _∥_

_L (_ _x1_ _x2_ + (x1) (x2) ) _L (1 + G[′])_ _x1_ _x2_ _._
_≤_ _∥_ _−_ _∥_ _∥A_ _−A_ _∥_ _≤_ _∥_ _−_ _∥_

Similarly, for the second term we have:

_D_ (x1) _yf_ (x1, (x1)) _D_ (x2) _yf_ (x2, (x2))
_∥_ _A_ _∇_ _A_ _−_ _A_ _∇_ _A_ _∥_
_D_ (x1) _yf_ (x1, (x1)) _yf_ (x2, (x2)) + _yf_ (x2, (x2)) _D_ (x2) _D_ (x1)
_≤∥_ _A_ _∥∥∇_ _A_ _−∇_ _A_ _∥_ _∥∇_ _A_ _∥∥_ _A_ _−_ _A_ _∥_

(LG[′](1 + G[′]) + GL[′]) _x1_ _x2_ _._
_≤_ _∥_ _−_ _∥_

This proves the lemma.


_Proof of Lemma 2. Given any x and y, and any λ such that λj_ 0 and _j_ _S(x)_ _[λ][j][ = 1][, we have:]_
_≥_ _∈_

_g(y) = max_ _λjgj(y)_ _λj_ _gj(x) +_ _gj(x), y[P]_ _x_
_j_ [k] _[g][j][(][y][)][ ≥]_ _≥_ _⟨∇_ _−_ _⟩−_ 2[1]L _[∥][x][ −]_ _[y][∥][2]_
_∈_ _j_ _S(x)_ _j_ _S(x)_  

_∈X_ _∈X_

= g(x) + _λj_ _gj(x), y_ _x_
_⟨_ _∇_ _−_ _⟩−_ 2[1]L _[∥][x][ −]_ _[y][∥][2][,]_

_j∈XS(x)_

proving the lemma.


_Proof of Lemma 3. We re-write fλ(x) as minimum value of a (_ _λ[1]_

as g is L-weakly convex (Definition 2) and 21λ _[∥][x][ −]_ _[x][′][∥][2][ is differentiable and][−]_ _[L][)][-strong convex function][ 1]λ_ [-strongly convex,][ φ][λ,x][,]


_φλ,x(x[′]) = g(x[′]) + [1]_ _._ (9)

2λ _[∥][x][ −]_ _[x][′][∥][2]_



_gλ(x) = min_
_x[′]∈R[d][1]_


Then first part of (a) follows trivially by the strong convexity. For the second part notice the following,

min
_x_ _[g][λ][(][x][) = min]x_ [min]x[′][ g][(][x][′][) + 1]2λ _[∥][x][ −]_ _[x][′][∥][2]_


= min
_x[′][ min]x_ _[g][(][x][′][) + 1]2λ_ _[∥][x][ −]_ _[x][′][∥][2]_

= min
_x[′][ g][(][x][′][)]_

Furthermore, we will show that arg minx gλ(x) = arg minx g(x). Consider x[∗] _∈_ argminx g(x).
We have that for any x,

_gλ(x) = min_
_x[′][ g][(][x][′][) + 1]2λ_ _[∥][x][ −]_ _[x][′][∥][2][ ≥]_ _[g][(][x][∗][)][ ≥]_ _[g][λ][(][x][∗][)][.]_

This shows that x[∗] _∈_ argminx gλ(x) and hence argminx g(x) ⊆ argminx gλ(x). Similarly, for any
_x[∗]_ _∈_ argminx gλ(x), we have:

_gλ(x[∗]) = g(ˆxλ(x[∗])) + [1]_ _xλ(x[∗])_ _x[∗]_ _g(ˆxλ(x[∗]))_ _gλ(ˆxλ(x[∗]))._

2λ _[∥][ˆ]_ _−_ _∥[2]_ _≥_ _≥_


-----

**Algorithm 3: Estimating Moreau envelope’s gradient for postprocessing**
**Input: point x, stochastic subgradient oracle for function g, error ϵ, failure probability δ**

**1 Find** **x such that g(x) + L∥x −** **x∥[2]** _≤_ minx′ g(x[′]) + L∥x − _x[′]∥[2][]_ + 4[ϵ]L[2] [using (Harvey et al.,]

2019, Algorithm 1).

**2 return 2L(x** **x).**  

e _−_ e e e

Since x[∗] _∈_ argminx gλ(x), the above inequality is infact an equality and hence ˆxλ(x[∗]) = x[∗]. So,
_gλ(x[∗]) = g(x[∗]) and hence x[∗]_ _∈_ argminx g(x) implying that argminx gλ(x) ⊆ argminx g(x).
Thus arg minx gλ(x) = arg minx g(x). For (b) we can re-write the Moreau envelope gλ as,

_gλ(x) = min_
_x[′][ g][(][x][′][) + 1]2λ_ _[∥][x][ −]_ _[x][′][∥][2]_


= )

_[∥]2[x]λ[∥][2]_ _−_ _λ[1]_ [max]x[′][ (][x][T][ x][′][ −] _[λg][(][x][′][)][ −∥][x]2[′][∥][2]_

_∗_

= _λg(_ ) + (x) (10)

_[∥]2[x]λ[∥][2]_ _−_ _λ[1]_ _·_ _[∥· ∥]2_ [2]

 

where (·)[∗] is the Fenchel conjugation operator. Since L < 1/λ, using L-weak convexity of g, it is
easy to see that1 _λg(x[′])+_ _[∥][x]2[′][∥][2]_ is (1 _−_ _λL)-strongly convex, therefore its Fenchel conjugate would be_

(1 _λL)_ [-smooth (Kakade et al., 2009, Theorem 6). This, along with][ 1]λ [-smoothness of first quadratic]

term implies that− _gλ(x) is_ 1λ [+] _λ(1_ 1λL) -smooth, and thus differentiable.

_−_

For (c) we proceed as follows. Since  ˆxλ(x) = argminx′ φλ,x(x[′]), by first order KKT optimality
conditions, we have that
_x_ _xˆλ(x)_ _λ∂g(x)._ (11)
_−_ _∈_
Further, from proof of part (a) we have that φλ,x(x[′]) (1 _λL)-strongly-convex in x[′]_ and it is
_−_
quadratic (and thus convex) in x. Danskin’s theorem (Bertsekas, 2009, Section 6.11) thus implies
that for gλ(x) = minx′∈Rd1 φλ,x(x[′]), we have that ∇gλ(x) = (x − _xˆλ(x))/λ. Letting ˆuλ(x) :=_
_λ[−][1](x_ _xˆλ(x))_ _∂g(x), we have that:_
_−_ _∈_

min _uλ(x)_ = λ[−][1] _x_ _xˆλ(x)_ = _gλ(x)_ _._
_u_ _∂g(ˆxλ(x))_ _∥_ _∥_ _−_ _∥_ _∥∇_ _∥_
_∈_ _[∥][u][∥≤∥][ˆ]_

C PROOFS OF RESULTS IN SECTION 4.2


In order to prove convergence of this algorithm, we first recall the following result from (Davis &
Drusvyatskiy, 2018).
**Theorem 7and Ez1,···,z (Corollary 2.2 from (Davis & Drusvyatskiy, 2018))k** _∥∇[b]_ _g(x)∥[2][i]_ _≤_ _G[2]. Then, the output ¯x of Algorithm 1 with stepsize. Suppose g(·) is L-weakly convex, η =_ _√Sγ+1_
_satisfies:_ h

_g 1_ + LG[2]γ[2]

2L [(][x][0][)][ −] [min][x][ g][(][x][)]

E _∥∇g 12L_ [(¯]x)∥[2][i] _≤_ 2 ·  _γ√S + 1_  _._
h

_Proof of Theorem 1. Lemmas 1 and 2 tell us that g(x) is_ _L-weakly convex and for any choice of z,_
the stochastic subgradient _∇g(x) is bounded in norm by_ _G. Consequently, Theorem 7 with the stated_
choice of S proves Theorem 1. [b]

[b] [b]

**Proposition 1. Given a point x, an L-weakly convex function g and a G-norm bounded and a**
_stochastic subgradient oracle to g (i.e., given any point x[′], which returns a stochastic vector u such_
_that E[u]_ _∂g(x[′]) and_ _u_ _G), with probability at least 1_ _δ, Algorithm 3 returns a vector u_

_satisfying ∈u_ _gλ(x) ∥_ _∥≤ϵ with at most O_ _G2 logϵ[2]_ 1δ _queries to the stochastic subgradient oracle −_
_∥_ _−∇_ _∥≤_

_of g._  


-----

_Proof of Proposition 1. Let Φ 1_

2L [(][x][′][, x][) :=][ g][(][x][′][) +][ L][∥][x][ −] _[x][′][∥][2][. Recall the notation of Lemma 3]_
_x 1_

2L [(][x][) := argmin][x][′][ Φ][ 1]2L [(][x][′][, x][)][ and][ g][λ][(][x][) = min][x][′][ Φ][ 1]2L [(][x][′][, x][)][. The proof of Lemma 3 tells us that]

_x_ _x 1_

_∇b_ _g 12L_ [(][x][) =] _−bλ2L_ [(][x][)] and also that ∥x 12L [(][x][)][ −] _[x][∥≤]_ 2GL [. Since][ Φ][ 1]2L [(][·][, x][)][ is a][ L][-strongly convex]

_G_

and G Lipschitz function in the domainbnx G[′] :2 ∥ logx 1 12δL [(][x][)][ −] _[x][∥≤]_ 2L o, (Harvey et al., 2019, Theorem

3.1) tells us that we can use SGD with O _Lϵ_ stochastic gradient oracle queries to implement

b

Step 1 of Algorithm 3 with success probability at leastexpression gives us a stochastic gradient oracle complexity of e  1 − Oδ, where G2 logϵ[2] 1ϵδ :=. 4ϵL[2] [. Simplifying this]
e

 

D PROOFS OF RESULTS IN SECTION 4.3

_Proof of Theorem 2. Letting h(x, λ)_ := _i∈[k]_ _[λ][i][g][i][(][x][)][,]_ we note that ∥∇xxh(x, λ)∥ =

_∥[P]i∈[k]_ _[λ][i][∇][2][g][i][(][x][)][∥≤]_ _[L][(1 +][ G][′][)][2][ +][ GL][′][, where we used Lemma 1 and the fact that]P_ [ P]i _[|][λ][i][| ≤]_ [1][.]

On the other hand, _xλh(x, λ) =_ _i_ [k]
_∥∇_ _∥[P]_ _∈_ _[∇][g][i][(][x][)][∥∥≤]_ _[kG][(1 +][ G][′][)][, where we again used]_

Lemma 1. Since _λλh(x, λ) = 0, we can conclude that h is an_ _L-gradient Lipschitz function_
_∇_
with _L := L(1 + G[′])[2]_ + GL[′] + kG(1 + G[′]). Consequently, g(x) = maxλ _S h(x, λ), where_
_∈_

_S :=_ _λ ∈_ R[k] : λi ≥ 0, _i∈[k]_ _[λ][i][ = 1]_, is _L-weakly convex and the Moreau envelope[b]_ _g 12L_ [is well]

defined.[b]n o [b]

[P] [b]

Denote _g(x, xs) := maxi_ [k] gi(x) + _L_ _x_ _xs_ . We now divide the analysis of each of iteration
_∈_ _∥_ _−_ _∥[2]_
of Algorithm 2 into two cases.

**Case I, bg(xs+1, xs)** maxi [k] gi([b]xs) 3ϵ/4: Since maxi [k] gi(xs+1) _g(xs+1, xs)_
_≤_ _∈_ _−_ _∈_ _≤_ _≤_
maxi [k] gi(xs) 3ϵ/4, we see that in this case g(xs) decreases monotonically by at least 3ϵ/4
_∈_ _−_
in each iteration. Since by Assumption 1, g is bounded by B in magnitude, and the termination
b b b
condition in Step 4 guarantees monotonic decrease in every iteration, there can only be at most
b b
2B/(3ϵ/4) = 8B/ϵ such iterations in Case I.

**Case II,** _g(xs+1, xs)_ maxi [k] gi(xs) 3ϵ/4: In this case, we claim that xs is an ϵ-FOSP of
_g = maxb_ _i_ [k] gi(x)b. To see this, we first note that ≥ _∈_ _−_
_∈_

_g(xs)_ b3ϵ/4 _g(xs+1, xs)_ (min _Lb_ _x_ _xs_ ) + _ϵ/4_ = _g(xs) < min_ _g(x; xs) +_ _ϵ._
_−_ _≤_ _≤_ _x_ _[g][(][x][) +][ b]∥_ _−_ _∥[2]_ _⇒_ _x_

(12)

b b b b

Let x[∗]s [:= arg min][x] _g(x; xk). Since g is_ _L-gradient Lipschitz, we note that_ _g(_ ; xs) is [b]L-strongly

_·_
convex. We now use this to prove that xs is close to x[∗]s[:]

_L_ [b] [b] (a) b [b]2ϵ
_g(x[∗]s[;][ x]s[) +]_ _s[∥][2][ ≤]_ _g[b](xs; xs) = f_ (xs) _<_ _g(x[∗]k[;][ x]s[) +][ b]ϵ =_ _xs_ _x[∗]s[∥]_ _[<]_ (13)

2 _⇒∥_ _−_ s _L_

_[∥][x][s][ −]_ _[x][∗]_

b b

whereb (a) uses (12). Now consider any _x, such that 4 bϵ/L_ _x_ _xs_ . Then,

_≤∥_ _−_ _∥_ b

(a)
_g(x) + L_ _x_ _xs_ = max _x) + L_ _x_ _xs_ p= _g(x; xs)_ = _g(x[∗]s[;][ x][s][) +][ L]_ _x_ _x[∗]s[∥][2]_
_∥_ _−_ _∥[2]_ _i_ [k] _[g][i][(][b]_ b _∥_ _−_ _∥[2]_ b b 2 _−_
_∈_ _[∥][b]_

(b) _L_

b b b b b b

_f_ (xs) _ϵ +_ _x_ _xs_ _xs_ _x[∗]s[∥][)][2 (][c][)]_ _f_ (xs) + _ϵ,_ (14)
_≥_ _−_ 2 [(][∥][b] − _∥−∥_ _−_ _≥_

where (a) uses uses _L-strong convexity ofbg(_ ; xs) at its minimizer x[∗]s[,][ (][b][)][ uses][ (12)][, and][ (][b][)][ and][ (][c][)]

b _·_ b

use triangle inequality, (13) and 4 _ϵ/L[b]_ _x_ _xs_ .

[b] _≤∥ b_ _−_ _∥_

q

Now consider the Moreau envelope, g 1

b 2L [(][x][) = min]b _[x][′][ φ][ 1]2L_ _[,x][(][x][′][)][ where][ φ][ 1]2L_ _[,x][(][x][′][) =][ g][(][x][′][) +][ L][∥][x][ −]_

_x[′]_ . Then, we can see that φ 1 [b] [b] [b] _ϵ/L[b]_
_∥[2]_ 2L _[,x][s]_ [(][x][′][)][ achieves its minimum in the ball][ {][x][′][ | ∥][x][′][ −] _[x][s][∥≤]_ [4] _}_

by (14) and Lemma 3(a). Then, with Lemma 3(b,c) and[b] _ϵ =_ _ε[2]_ q

64 _L_ [, we get that,]

b

_∥∇g 12L_ [(][x][s][)][∥≤] [(2]L[b])∥xs − _xˆ 12L_ [(] b[x][s][)][∥] [= 8][b] _Lϵ = ε,_ (15)

[b] [b] p

bb


-----

i.e., xs is an ε-FOSP of g.

By combining the above two cases, we establish that [8]3[B]ϵ [“outer” iterations ensure convergence to a]

_ε-FOSP._
b

We now compute the gradient call complexity of each of these “outer" iterations, where we have
two options for implementing Step 3 of Algorithm 2. Note that this step corresponds to solving
minx maxλ _S h(x, λ) up to an accuracy of_ _ϵ/4._
_∈_

**Option I, (Thekumparampil et al., 2019, Algorithm 2): Since the minimax optimization problem here**
is _L-strongly-convex–concave and 2L[b]-gradient Lipschitz, (Thekumparampil et al., 2019, Theorem 1) b_
tells us that this requires at most m gradient calls for each gi where,

[b]

6(2L[b])[2] _ϵ_ _L_

= _O_ _m_ (16)

_Lm[2]_ _≤_ 4 [b] [=][ ε]2[8][2]L[b] _⇒_ _ε_ _≤_

 b 

Therefore the number of gradient computations required for each iteration of inner problem is
_O bLϵ_ [log][2][ ] 1ε .

**Option II, Cutting plane method (Lee et al., 2015): Let us consider u(λ) := minx h(x, λ) +**
_L_ _x_ _xs_, which is a _L-Lipschitz, concave function of λ. (Lee et al., 2015) tells us that we can use_
_∥_ _−_ _∥[2]_

cutting plane algorithms to obtainqueries tob _u and poly(k log[b]_ _Lϵ_ [)][ computation. The gradient of]λ satisfying u(λ[b]) ≥ maxλ∈[ u]S[ is given by] u(λ)−ϵ using[ ∇][u] O[(][λ][) =]k log[ ∇][λ][k][h]eϵL[b][(][x][∗]gradient[(][λ][)][, λ][)][,]

[b] e

where x[∗](λ) := argminx h(bex, λ) + _L∥x −_ _xs∥[2]. Since h(x, λ) +_ _L∥x −_ _xs∥[2]_ is a 3L[b]-smooth and
_L-strongly convex function in x, x[∗](λ) can be computed up to_ _ϵ error using gradient descent in_

_L_
_O_ log _ϵ_ iterations. If we choose[b]ϵ = _ϵ[2]/poly(k,_ _L/µ), then Proposition 2 tells us that[b]_ _x[∗](λ[b])_

satisfies the requirements of Step (3) of Algorithm 2 and the total number of gradient calls to eachb  b  e _gi_
is at moste _O_ poly(k) log _Lϵ_ in each outer iteration of Algorithm 2. e b [b]
b
 


**Proposition 2. Suppose h : R[d][1]** _× U →_ R be such that h(·, λ) is µ-strongly convex for every λ ∈U _,_
_h(x, ·) is concave for every x ∈_ R[d][1] _and h is L-gradient Lipschitz. Let_ _λ be such that minx h(x,_ _λ) ≥_
maxλ minx h(x, λ) − _ϵ and let x[∗](λ[b]) := argminx h(x,_ _λ). Then, we have maxλ h(x[∗](λ[b]), λ) ≤_

_L_
minof _x._ maxλ h(x, λ) + c  _µ_ _[·][ ϵ][ +][ LD]√µ[U] · [√]ϵ, where DU = max[b]_ _λ1,λ2[b]∈U ∥λ1 −_ _λ2∥_ _is the diameter[b]_
_U_

_Proof of Proposition 2. From the hypothesis, we have:_


_ϵ_ _h(x[∗], λ[∗])_ _h(x[∗](λ[b]),_ _λ)_ _h(x[∗],_ _λ)_ _h(x[∗](λ[b]),_ _λ)_ _λ)_ _,_
_≥_ _−_ [b] _≥_ [b] _−_ [b] _≥_ _[µ]2_ _[∥][x][∗]_ _[−]_ _[x][∗][(][b]_ _∥[2]_

where (x[∗], λ[∗]) is the Nash equilibrium and the second step follows from the fact that λ[∗] =
argmaxλ h(x[∗], λ) and the third step follows from the fact that x[∗](λ[b]) = argminx h(x, _λ). Con-_
sequently, we have that ∥x[∗] _−_ _x[∗](λ[b])∥≤_ 2ϵ/µ. Let _λ[¯] := argmaxλ h(x[∗](λ[b]), λ). We now have_

[b]
p


-----

that:
max _h(x[∗](λ[b]), λ)_ max min _λ),_ _λ[¯])_ _h(x[∗], λ[∗])_
_λ_ _−_ _λ_ _x_ _[h][(][x, λ][) =][ h][(][x][∗][(][b]_ _−_

= h(x[∗](λ[b]), _λ[¯]) −_ _h(x[∗](λ[b]), λ[∗]) + h(x[∗](λ[b]), λ[∗]) −_ _h(x[∗], λ[∗])_

(ζ1)
_λh(x[∗](λ[b]), λ[∗]),_ _λ[¯]_ _λ[∗]_ + _[L]_ _λ)_ _x[∗]_

(≤⟨∇ζ2) _−_ _⟩_ 2 _[∥][x][∗][(][b]_ _−_ _∥[2]_
_λh(x[∗](λ[b]), λ[∗])_ _λ_ _λ[∗]_ + _[Lϵ]_
_≤∥∇_ _∥∥[¯] −_ _∥_ _µ_

(ζ3)
_λh(x[∗], λ[∗])_ + L _x[∗](λ[b])_ _x[∗]_ + _[Lϵ]_
_≤_ _∥∇_ _∥_ _∥_ _−_ _∥_ _DU_ _µ_
 

_√2ϵ_

+ _[Lϵ]_

_≤_ _[L][D]√[U]µ_ _µ [,]_

where (ζ1) follows from the fact that h(x[∗](λ[b]), ·) is concave and x[∗] = argminx h(x, λ[∗]), (ζ2) follows
from the bound _x[∗]_ _x[∗](λ[b])_ 2ϵ/µ, (ζ3) follows from the L-gradient Lipschitz property of h,
_∥_ _−_ _∥≤_

and the last step follows from the fact that _λh(x[∗], λ[∗]) = 0. This proves the proposition._

p _∇_

E PROOFS OF RESULTS IN SECTION 5

In this appendix, we present the proofs for the lemmas in Section 5. Recall the SGA update:
_yt+1 = yt + η∇yfσ(t)(x, yt)._ (17)

Therefore, the Jacobian of the T -step SGA update is given by
_Dyt+1 =_ _I + η∇yyfσ(t)(x, yt)_ _Dyt + η∇yxfσ(t)(x, yt), with A(x, z) = yT (x)._ (18)
  

E.1 PROOF OF THEOREM 3

**Theorem 3 (General Case). Suppose for all j ∈** [n], fj satisfies Assumption 1. Then, for any fixed
_randomness z, T_ _-step SGA is (1 + ηL)[T]_ _-Lipschitz and 4(ρ/L) · (1 + ηL)[2][T]_ _-gradient Lipschitz._

_Proof. Lipschitz of yt(x). We first show the Lipschitz claim. We have the following bound on the_
Jacobian of the update equation given in (18):
_∥Dyt+1(x)∥≤∥(I + η∇yy[2]_ _[f]_ [(][x, y][t][(][x][)))][Dy][t][(][x][)][∥] [+][ η][∥∇]yx[2] _[f]_ [(][x, y][t][(][x][))][∥]

(1 + ηL) _Dyt(x)_ + ηL.
_≤_ _∥_ _∥_
Since Dy0(x) = 0, the above recursion implies that


_t−1_

(1 + ηL)[τ] _≤_ (1 + ηL)[t].
_τ_ =0

X


_Dyt(x)_ _ηL_
_∥_ _∥≤_


**Gradient-Lipschitz of yt(x). Next, we show the claimed gradient Lipschitz constant. As above,**
using the update equation in (18), we have the following bound on the Jacobian:
_Dyt+1(x1)_ _Dyt+1(x2)_
_∥_ _−_ _∥_

_≤∥(I + η∇yy[2]_ _[f]_ [(][x][1][, y][t][(][x][1][)))(][Dy][t][(][x][1][)][ −] _[Dy][t][(][x][2][))][∥]_ [+][ η][∥∇][2]yx[f] [(][x][1][, y][t][(][x][1][))][ −∇][2]yx[f] [(][x][2][, y][t][(][x][2][))][∥]

+ η∥[∇yy[2] _[f]_ [(][x][1][, y][t][(][x][1][))][ −∇][2]yy[f] [(][x][2][, y][t][(][x][2][))]][Dy][t][(][x][2][)][∥]

(1 + ηL) _Dyt(x1)_ _Dyt(x2)_ + ηρ(1 + _Dyt(x2)_ )( _x1_ _x2_ + _yt(x1)_ _yt(x2)_ )
_≤_ _∥_ _−_ _∥_ _∥_ _∥_ _∥_ _−_ _∥_ _∥_ _−_ _∥_

(1 + ηL) _Dyt(x1)_ _Dyt(x2)_ + 4ηρ(1 + ηL)[2][t] _x1_ _x2_ _._
_≤_ _∥_ _−_ _∥_ _∥_ _−_ _∥_
The above recursion implies the claimed Lipschitz constant. Indeed,


_t−1_

(1 + ηL)[t][+][τ] _[−][1]_ _x1_ _x2_ 4(ρ/L) (1 + ηL)[2][t] _x1_ _x2_ _._
_τ_ =0 _∥_ _−_ _∥≤_ _·_ _∥_ _−_ _∥_

X


_Dyt(x1)_ _Dyt(x2)_ 4ηρ
_∥_ _−_ _∥≤_


-----

E.2 PROOF OF THEOREM 4

**Theorem 4 (Concave Case). Suppose for all j ∈** [n], fj satisfies Assumption 1 and fj(x, ·) is concave
_for any x. Then, for any fixed randomness z, T_ _-step SGA is ηLT_ _-Lipschitz and (ρ/L) · (1 + ηLT_ )[3]_gradient Lipschitz._

_Proof. Lipschitz of yt(x). We first show the Lipschitz claim. Using the update equation in (18), we_
have the following bound on the Jacobian:

_∥Dyt+1(x)∥≤∥(I + η∇yy[2]_ _[f]_ [(][x, y][t][(][x][)))][Dy][t][(][x][)][∥] [+][ η][∥∇][2]yx[f] [(][x, y][t][(][x][))][∥]

_Dyt(x)_ + ηL.
_≤∥_ _∥_

Since Dy0(x) = 0, the above recursive implies that _Dyt(x)_ _ηLt._
_∥_ _∥≤_

**Gradient-Lipschitz of yt(x). Next, we show the claimed gradient Lipschitz constant. Using the**
update equation in (18):, we have the following bound on the Jacobian:

_Dyt+1(x1)_ _Dyt+1(x2)_
_∥_ _−_ _∥_

_≤∥(I + η∇yy[2]_ _[f]_ [(][x][1][, y][t][(][x][1][)))(][Dy][t][(][x][1][)][ −] _[Dy][t][(][x][2][))][∥]_ [+][ η][∥∇][2]yx[f] [(][x][1][, y][t][(][x][1][))][ −∇][2]yx[f] [(][x][2][, y][t][(][x][2][))][∥]

+ η∥[∇yy[2] _[f]_ [(][x][1][, y][t][(][x][1][))][ −∇]yy[2] _[f]_ [(][x][2][, y][t][(][x][2][))]][Dy][t][(][x][2][)][∥]

_Dyt(x1)_ _Dyt(x2)_ + ηρ(1 + _Dyt(x2)_ )( _x1_ _x2_ + _yt(x1)_ _yt(x2)_ )
_≤∥_ _−_ _∥_ _∥_ _∥_ _∥_ _−_ _∥_ _∥_ _−_ _∥_

_Dyt(x1)_ _Dyt(x2)_ + ηρ(1 + ηLt)[2] _x1_ _x2_ _._
_≤∥_ _−_ _∥_ _∥_ _−_ _∥_

This recursion implies the following gradient Lipschitz constant:


_t−1_

(1 + ηLτ )[2] _x1_ _x2_ (ρ/L) (1 + ηLt)[3] _x1_ _x2_ _._
_τ_ =0 _∥_ _−_ _∥≤_ _·_ _∥_ _−_ _∥_

X


_Dyt(x1)_ _Dyt(x2)_ _ηρ_
_∥_ _−_ _∥≤_

E.3 PROOF OF THEOREM 5


**Theorem 5 (Strongly-concave Case). Suppose for all j ∈** [n], fj satisfies Assumption 1 and fj(x, ·)
_is α-strongly concave for any x. Then, for any fixed randomness z, T_ _-step SGA is κ-Lipschitz and_
4(ρ/L) · κ[3]-gradient Lipschitz, where κ = L/α is the condition number.

_Proof. Denote the condition number κ = L/α._

**Lipschitz of yt(x). We first show the claimed Lipschitz constant. Using the update equation in (18),**
we have that

_∥Dyt+1(x)∥≤∥(I + η∇yy[2]_ _[f]_ [(][x, y][t][(][x][)))][Dy][t][(][x][)][∥] [+][ η][∥∇][2]yx[f] [(][x, y][t][(][x][))][∥]

(1 _ηα)_ _Dyt(x)_ + ηL.
_≤_ _−_ _∥_ _∥_

Since Dy0(x) = 0, the above recursion gives the following bound:


_t−1_

(1 − _ηα)[τ]_ _≤_ _κ._
_τ_ =0

X


_Dyt(x)_ _ηL_
_∥_ _∥≤_


**Gradient-Lipschitz of yt(x). Next we show the claimed gradient Lipschitz constant. Again, using**
the update equation, we have that

_Dyt+1(x1)_ _Dyt+1(x2)_
_∥_ _−_ _∥_

_≤∥(I + η∇yy[2]_ _[f]_ [(][x][1][, y][t][(][x][1][)))(][Dy][t][(][x][1][)][ −] _[Dy][t][(][x][2][))][∥]_ [+][ η][∥∇][2]yx[f] [(][x][1][, y][t][(][x][1][))][ −∇][2]yx[f] [(][x][2][, y][t][(][x][2][))][∥]

+ η∥[∇yy[2] _[f]_ [(][x][1][, y][t][(][x][1][))][ −∇][2]yy[f] [(][x][2][, y][t][(][x][2][))]][Dy][t][(][x][2][)][∥]

(1 _ηα)_ _Dyt(x1)_ _Dyt(x2)_ + ηρ(1 + _Dyt(x2)_ )( _x1_ _x2_ + _yt(x1)_ _yt(x2)_ )
_≤_ _−_ _∥_ _−_ _∥_ _∥_ _∥_ _∥_ _−_ _∥_ _∥_ _−_ _∥_

(1 _ηα)_ _Dyt(x1)_ _Dyt(x2)_ + 4ηρκ[2] _x1_ _x2_ _._
_≤_ _−_ _∥_ _−_ _∥_ _∥_ _−_ _∥_


-----

This recursion implies that

_t−1_
_Dyt(x1)_ _Dyt(x2)_ 4ηρκ[2] (1 _ηα)[τ]_ _x1_ _x2_ 4(ρ/L) _κ[3]_ _x1_ _x2_ _._
_∥_ _−_ _∥≤_ _τ_ =0 _−_ _∥_ _−_ _∥≤_ _·_ _∥_ _−_ _∥_
X

E.4 PROOF OF THEOREM 6

**Theorem 6 (General Case). Suppose for all j ∈** [n], fj satisfies Assumption 1. Then, for any fixed
_seed z, T_ _-step SNAG is T_ (1 + ηL/θ)[T] _-Lipschitz and 50(ρ/L) · T_ [3](1 + ηL/θ)[2][T] _-gradient Lipschitz._

_Proof. Recall the SNAG update_
_y˜t = yt + (1_ _θ)(yt_ _yt_ 1) (19)
_−_ _−_ _−_
_yt+1 = ˜yt + η∇yfσ(t)(x, ˜yt)._ (20)
Observe that the update equation for T -step SNAG implies that
_Dy˜t = Dyt + (1_ _θ)(Dyt_ _Dyt_ 1)
_−_ _−_ _−_ (21)
_Dyt+1 = (I + η∇yyfσ(t)(x, ˜yt))Dy˜t + η∇yxfσ(t)(x, ˜yt)_

**Lipschitz of yt(x), vt(x). We first show the claimed Lipschitz constant. By the update equations in**
(21), we have that
_Dyt+1 = (I + η_ _yyfσ(t)(x, ˜yt))(Dyt + (1_ _θ)(Dyt_ _Dyt_ 1)) + η _yxfσ(t)(x, ˜yt)._
_∇_ _−_ _−_ _−_ _∇_

Denote δt = _Dyt_ _Dyt_ 1, and note that Dy0 = Dy 1 = 0 so that δ0 = 0. By the equation
above, we have that ∥ _−_ _−_ _∥_ _−_
_δt+1 ≤ηL∥Dyt∥_ + (1 + ηL)(1 − _θ)δt + ηL_


_≤ηL_


_δτ + (1 + ηL)(1 −_ _θ)δt + ηL._
_τ_ =1

X


In the following, we use induction to prove that
_δt_ (1 + ηL/θ)[t]. (22)
_≤_
It is easy to verify that this is true for the base case δ0 = 0 ≤ 1. Suppose the claim is true for all
_τ ≤_ _t, then we have that_


(1 + ηL/θ)[τ] + (1 + ηL)(1 − _θ)(1 + ηL/θ)[t]_ + ηL
_τ_ =1

X

_t_

(1 + ηL/θ)[τ] + (1 − _θ)(1 + ηL/θ)[t][+1]_
_τ_ =0

X


_δt+1_ _ηL_
_≤_

_≤ηL_


=θ[(1 + ηL/θ)[t][+1] _−_ 1] + (1 − _θ)(1 + ηL/θ)[t][+1]_ _≤_ (1 + ηL/θ)[t][+1].
This proves the induction claim. Therefore, by (22), we have the following two bounds:


_Dyt(x)_ _δτ_ _t(1 + ηL/θ)[t],_
_∥_ _∥≤_ _τ_ =1 _≤_

X


_∥Dy˜t(x)∥≤(2 −_ _θ)∥Dyt(x)∥_ + (1 − _θ)∥Dyt−1(x)∥≤_ 3t(1 + ηL/θ)[t].

**Gradient-Lipschitz of yt(x). Next, we show the claimed gradient Lipschitz constant. For any fixed**
_x1, x2, denote wt = Dyt(x1) −_ _Dyt(x2), we have_
_wt+1 =(I + η_ _yyfσ(t)(x1, ˜yt(x1)))(wt + (1_ _θ)(wt_ _wt_ 1))
_∇_ _−_ _−_ _−_

+ η( _yxfσ(t)(x1, ˜yt(x1))_ _yxfσ(t)(x2, ˜yt(x2)))_
_∇_ _−∇_
_T1_
+ η|(∇yyfσ(t)(x1, ˜yt(x1)){z −∇yyfσ(t)(x2, ˜yt(x2)))(} _Dyt(x2) + (1 −_ _θ)(Dyt(x2) −_ _Dyt−1(x2)))_
_T2_
| {z }


-----

We note that we can upper bound the last two terms above as follows:

_T1 + T2 ≤ηρ(∥x1 −_ _x2∥_ + ∥y˜t(x1) − _y˜t(x2)∥)_
+ ηρ(∥x1 − _x2∥_ + ∥y˜t(x1) − _y˜t(x2)∥)(2∥Dyt(x2)∥_ + ∥Dyt−1(x2)∥)

24ηρt[2](1 + ηL/θ)[2][t] _x1_ _x2_ _._
_≤_ _∥_ _−_ _∥_

Therefore, let ζt = _wt_ _wt_ 1, and ∆= _x1_ _x2_, we have the following:
_∥_ _−_ _−_ _∥_ _∥_ _−_ _∥_

_ζt+1 ≤ηL∥wt∥_ + (1 + ηL)(1 − _θ)ζt + 24ηρt[2](1 + ηL/θ)[2][t]∆_


_ζτ + (1 + ηL)(1 −_ _θ)ζt + 24ηρt[2](1 + ηL/θ)[2][t]∆._
_τ_ =1

X


_≤ηL_


In the following, we use induction to prove that

_ζt_ 50(ρ/L) _t[2](1 + ηL/θ)[2][t]∆:= ψ(t)._ (23)
_≤_ _·_

It is easy to verify that this is true for the base case ζ0 = 0. Suppose the claim is true for all τ ≤ _t,_
then we have that


_τ_ [2](1 + ηL/θ)[2][τ] ∆+ (1 + ηL)(1 − _θ)ψ(t) + 24ηρt[2](1 + ηL/θ)[2][t]∆_
_τ_ =1

X


_ζt+1_ 50ηρ
_≤_


_θ(ρ/L)_ [50t[2][ (1 +][ ηL/θ][)][2(][t][+1)][ −] [1] + 24(ηL/θ)t[2](1 + ηL/θ)[2][t]]∆+ (1 _θ)ψ(t + 1)_
_≤_ _·_ (1 + ηL/θ)[2] 1 _−_

_−_

_≤θ(ρ/L) · [25t[2](1 + ηL/θ)[2(][t][+1)]_ + 24t[2](1 + ηL/θ)[2(][t][+1)]]∆+ (1 − _θ)ψ(t + 1)_

_≤θ(ρ/L) · [50t[2](1 + ηL/θ)[2(][t][+1)]]∆+ (1 −_ _θ)ψ(t + 1) ≤_ _ψ(t + 1)._

This proves the induction claim. Therefore, by (23), we have that


_ζτ_ 50(ρ/L)t[3](1 + ηL/θ)[2][t] _x1_ _x2_ _._
_τ_ =1 _≤_ _∥_ _−_ _∥_

X


_Dyt(x1)_ _Dyt(x2)_ = _wt_
_∥_ _−_ _∥_ _∥_ _∥≤_


E.5 PROJECTED GRADIENT ASCENT IS NOT GRADIENT-LIPSCHITZ

**Proposition 3. Consider f** (x, y) = xy for (x, y) ∈X × Y, where X = [0, 10] and Y = [0, 1].
1-step projected gradient ascent given by:

_y1(x) =_ (y0 + η _yf_ (x, y0)) _y0 = 0,_
_PY_ _∇_

_where η > 1/10 is not a gradient-Lipschitz algorithm._

_Proof. We see that y1(x) = min(1, ηx) and f_ (x, y1(x)) = x min(1, ηx). For η < 1/10, we see that
_f_ (x, y1(x)) is not gradient-Lipschitz at x = 1/η (0, 10).
_∈_

F ON NONCONVERGENCE OF EXTRAGRADIENT METHOD AND OPTIMISTIC
GRADIENT DESCENT ASCENT (OGDA)

In this section, we show that there exist nonconvex-nonconcave problems where the extragradient
method and OGDA get trapped in limit cycles. While the example we construct is a quadratic function
that is not bounded over all of the space, it will be clear from the proof that the same results hold for
any function that matches the constructed quadratic locally around origin.


Consider the following two dimensional quadratic function parametrized by a, b ∈ R:

_a_ _b_ _x_

_f_ (x, y) = [1] _y)_

2 _[ax][2][ +][ bxy][ −]_ [1]2 _[ay][2][ = 1]2 [(][x]_ _b_ _a_ _y_

 _−_   


(24)


-----

_a_ _b_
Denote z := (x, y), Φ := and define:
−b _a_

_F_ (z) := _∇xf_ (x, y) = _a_ _b_ _x_ := Φ _x_ _,_
_yf_ (x, y) _b_ _a_ _y_ _·_ _y_
−∇  −     

Φ

Let the eigenvalues of matrix Φ be λ1, λ2. We know they satify the characteristic equation (λ

| {z }

_−_
_a)[2]_ + b[2] = 0. By Vieta’s formulas, we know that λ1 + λ2 = 2a, λ1λ2 = a[2] + b[2]. This justifies the
following proposition.
**Proposition 4. For any complex number c ∈** C, there exist a, b ∈ R s.t. the eigenvalues of Φ are c
_and ¯c. Here ¯c is the complex conjugate of c._

_Proof. Let c = p + qi for some p, q ∈_ R, we can choose a = (c + ¯c)/2 = p, b = ±√cc¯ − _a[2]_ = ±q.

It is easy to check such matrix Φ has eigenvalues c and ¯c.

F.1 EXTRAGRADIENT METHODS


The extragradient algorithm (Korpelevich, 1976) uses the following update:

_zt+1/2 =zt −_ _ηF_ (zt)

_zt+1 =zt −_ _ηF_ (zt+1/2)

For the case of quadratic function specified by (24), this update is equivalent to


_zt+1 = (I −_ _ηΦ(I −_ _ηΦ))zt = (I −_ _ηΦ + η[2]Φ[2])zt_
Clearly, the eigenvalues of matrix (I −ηΦ+η[2]Φ[2]) are simply (1−ηλ1 +η[2]λ[2]1[)][ and][ (1][−][ηλ][2] [+][η][2][λ][2]2[)][.]

**Proposition 5. For the extragradient algorithm with any fixed learning rate η, there exists a quadratic**
_function, and appropriate initial points such that the extragradient algorithm is trapped in a limit_
_cycle._

_Proof. For any given θ_ (0, 2π), we can simply pick λ1 **C to be the solution of equation**
1 − _ηλ1 + η[2]λ[2]1_ [=][ e][iθ][. This is a quadratic equation of] ∈ _[ λ][1][, which always has nonzero complex roots.] ∈_
For λ2 = λ[¯]1, we verify that 1 − _ηλ2 + η[2]λ[2]2_ [=][ e][−][iθ][. Finally, by Proposition 4, we know that there]
exists a choice of a, b such that, the two eigenvalues of (I − _ηΦ + η[2]Φ[2]) are precisely e[iθ]_ and e[−][iθ].

If the initial point _x0_ can be decomposed as _x0_ = α1z1 + α2z2, where z1 and z2 are the two
_y0_ _y0_
   

eigenvectors of Φ corresponding to eigenvalues λ1 and λ2 respectively, we see that the extra gradient

algorithm follows the trajectory _xt_ = α1e[iθt]z1 + α2e[−][iθt]z2, thereby getting trapped in a limit
_yt_
 

cycle.

F.2 OPTIMISTIC GRADIENT DESCENT ASCENT (OGDA)

The OGDA algorithm (Daskalakis & Panageas, 2018a) has the following update:


_zt+1 = zt_ 2ηF (zt) + ηF (zt 1).
_−_ _−_

(Daskalakis & Panageas, 2018a)[Section 4.1] presents an example where OGDA is empirically
not shown to converge for some initialization points. Here, we present an example for which
nonconvergence of OGDA is shown rigorously. We can rewrite this update as
_zt+1_ = _I −_ 2ηΦ _ηΦ_ _zt_
 _zt_   _I_ 0  zt−1


_I_ 2ηΦ _ηΦ_
There are four eigenvalues for the matrix _−_
_I_ 0



. Two of them µ1, µ2 are eigenval

ues of the matrix 1 − 2ηλ1 _ηλ1_ while the other two µ3, µ4 are eigenvalues of the matrix
1 0
 

1 − 2ηλ2 _ηλ2_, where λ1 and λ2 are the eigenvalues of Φ.
1 0
 


-----

**Proposition 6. For the OGDA algorithm with any fixed learning rate η, there exists a quadratic**
_function, and a choice of initial points such that the iterates are trapped in a limit cycle._

_Proof. We will fist show that there is a choice for a and b so that the resulting eigenvalues µ1,_


_µ2 of_ 1 − 2ηλ1 _ηλ1_
1 0



and µ3, µ4 of 1 − 2ηλ2 _ηλ2_
1 0



_µ2 of_ 1 − 2ηλ1 _ηλ1_ and µ3, µ4 of 1 − 2ηλ2 _ηλ2_ satisfy _µ1_ = _µ3_ = 1 and _µ2_ =
1 0 1 0 _|_ _|_ _|_ _|_ _|_ _|_
   

_µ4_ _< 1. Let us first fix µ1 = e[iθ]_ and choose ηλ1 = _[µ][1]2[(]µ[µ]1[1][−]1[1)]_ so that we have µ1 is a root of
_|_ _|_ _−_

_µ[2]_ _−_ (1 − 2ηλ1)µ + ηλ1 = 0. We note that µ2 is the other root of the above equation. We have that:

(cos θ 1)[2] + sin[2] _θ_

_ηλ1_ = _−_
_|_ _|_ _[|]2[µ]µ[1]1[ −]_ [1]1[|] [=] s (2 cos θ 1)[2] + sin[2] _θ_ _[.]_

_|_ _−_ _|_ _−_

Therefore, for any θ (0, π/4], we have _ηλ1_ _< 1. That is_ _µ1µ2_ _< 1, since_ _µ1_ = 1, we have
_∈_ _|_ _|_ _|_ _|_ _|_ _|_

_µ2_ _< 1. By symmetry, we know by choosing λ2 = λ[¯]1, the two eigenvalues of_ 1 − 2ηλ2 _ηλ2_
_|_ _|_ 1 0
 

are µ3 = ¯µ1 and µ4 = ¯µ2. In sum, we have proved that for any θ ∈ (0, π/4], and any learning rate η,

_I_ 2ηΦ _ηΦ_
there exists a choice of a, b such that the four eigenvalues of _−_ are e[±][iθ], c[′], _c[¯][′]_
_I_ 0
 

where |c[′]| = |c[¯][′]| < 1. If the initial points _zz10_ have the decomposition _i=1_ _[α][i]z[¯]i, where_
 

_z¯i is the eigenvector corresponding to eigenvalue µi, then the iterates zt and zt_ 1 are given by
_−_
_ztzt_ 1 = _i=1_ _[α][i][µ]i[t]z[¯]i. As t →∞,_ _ztzt_ 1 converges to α1e[iθt]z¯1 + α3e[−][iθt][P]z[4]¯3, proving that
 _−_   _−_ 

OGDA iterates get trapped in a limit cycle and neither converge to a single point nor escape to
infinity. [P][4]


(cos θ − 1)[2] + sin[2] _θ_

(2 cos θ 1)[2] + sin[2] _θ_ _[.]_
_−_


G ADDITIONAL EXPERIMENTS AND DETAILS

In this appendix section, we provide additional experimental results and details.

**Dirac-GAN. In the results presented in Section 6 for this problem, the discriminator sampled its**
initialization uniformly from the interval [−0.1, 0.1] and performed T = 10 steps of gradient ascent.
For the results given in Figure 5, we allow the discriminator to sample uniformly from the interval

[−0.5, 1] and consider the discriminator performing T = 100 (Figure 5b) and T = 1000 (Figure 5c)
gradient ascent steps. The rest of the experimental setup is equivalent to that described in Section 6.

For the result presented in Figure 5b, we see that with this distribution of initializations for the
discriminator and T = 100 gradient ascent steps, the generator is not able to converge to the
optimal parameter of θ[∗] = 0 to recreate the underlying data distribution using our algorithm which
descends _f_ (θ, (θ)) or the algorithm that descends _θf_ (θ, (θ)). However, we see that our
_∇_ _A_ _∇_ _A_
algorithm converges significantly closer to the optimal parameter configuration. Furthermore, we still
observe stability and convergence from our training method, whereas standard training methods using
simultaneous or alternating gradient descent-ascent always cycle. This example highlights that the
optimization through the algorithm of the adversary is important not only for the rate of convergence,
but it also influences what the training method converges to and gives improved results in this regard.

Finally, in the result presented in Figure 5b, we see that with this distribution of initializations for the
discriminator and T = 1000 gradient ascent steps, the generator is able to converge to the optimal
parameter of θ[∗] = 0 to recreate the underlying data distribution using our algorithm which descends
_f_ (θ, (θ)) or the algorithm that descends _θf_ (θ, (θ)). Thus, while with T = 100 we did
_∇_ _A_ _∇_ _A_
not observe convergence to the optimal generator parameter, with a stronger adversary we do see
convergence to the optimal generator parameter. This behavior can be explained by the fact that when
the discriminator is able to perform enough gradient ascent steps to nearly converge, the gradients
_f_ (θ, (θ)) and _θf_ (θ, (θ)) are nearly equivalent.
_∇_ _A_ _∇_ _A_

We remark that we repeated the experiments 5 times with different random seeds and show the mean
generator parameters during the training with a window around the mean of a standard deviation. The
results were very similar between runs so the window around the mean is not visible.


-----

(b) (c)

Figure 5: Dirac-GAN: Generator parameters while training using our framework with and without
optimizing through the discriminator where between each generator update the discriminator samples
an initial parameter choice uniformly at random from the interval [−0.5, 1] and then performs
_T = 100 (Figure 5b) and T = 1000 (Figure 5c) steps of gradient ascent._

(b) (c) (d) (e) (f)

(a) Real Data

(g) (h) (i) (j) (k)

Figure 6: Mixture of Gaussians: Figure 6a shows the real data distribution and Figures 6b–6k show
the final generated distributions after 150k generator updates from 10 separate runs of the training
procedure described in Section 6 using gradient ascent for the discriminator. Each run recovers a
generator distribution closely resembling the underlying data distribution.

**Mixture of Gaussians. We noted in Section 6 that we repeated our experiment training a generative**
adversarial network to learn a mixture of Gaussians 10 times and observed that for each run of the
experiment our training algorithm recovered all modes of the distribution. We now show those results
in Figure 6. In particular, in Figure 6a we show the real data distribution and in Figures 6b–6k we
show the final generated distribution from 10 separate runs of the training procedure after 150k
generator updates. Notably, we observe that each run of the training algorithm is able to generate
a distribution that closely resembles the underlying data distribution, showing the stability and
robustness of our training method.

We also performed an experiment on the mixture of Gaussian problem in which the discriminator
algorithm was the Adam optimization procedure with parameters (β1, β2) = (0.99, 0.999) and
learning rate η2 = 0.004 and the generator learning rate was η1 = 0.05. The rest of the experimental
setup remained the same. We ran this experiment 10 times and observed that for 7 out of the 10 runs
of the final generated distribution was reasonably close to the real data distribution, while for 3 out of
the 10 runs the generator did not learn the proper distribution. This is to say that we found the training
algorithm was not as stable when the discriminator used Adam versus normal gradient ascent. The
final generated distribution from the 7 of 10 runs with reasonable distributions are shown in Figure 7.


-----

(b) (c) (d)

(e) (f) (g)


(h)


(a) Real Data


Figure 7: Mixture of Gaussians: Figure 7a shows the real data distribution and Figures 7b–7h show
the final generated distributions after 150k generator updates from the 7 out of 10 separate runs
of the training procedure using Adam optimization for the discriminator that produced reasonable
distributions.

**Adversarial Training. We now provide some further background on the adversarial training ex-**
periment and additional results. It is now well-documented that the effectiveness of deep learning
classification models can be vulnerable to adversarial attacks that perturb the input data (see, e.g.,
Biggio et al. 2013; Szegedy et al. 2014; Kurakin et al. 2017; Madry et al. 2018). A common approach
toward remedying this vulnerability is by training the classification model against adversarial perturbations. Recall from Section 6 that given a data distribution D over pairs of examples x ∈ R[d] and labels
_y ∈_ [k], parameters θ of a neural network, a set S ⊂ R[d] of allowable adversarial perturbations, and a
loss function ℓ(·, ·, ·) dependent on the network parameters and the data, adversarial training amounts
to considering a minmax optimization problem of the form minθ E(x,y)∼D[maxδ∈S ℓ(θ, x + δ, y)].

A typical approach to solving this problem is an alternating optimization approach (Madry et al.,
2018). In particular, each time a batch of data is drawn from the distribution, T -steps of projected
gradient ascent are performed by ascending along the sign of the gradient of the loss function with
respect to the data and projecting back onto the set of allowable perturbations, then the parameters
of the neural network are updated by descending along the gradient of the loss function with the
perturbed examples. The experimental setup we consider is analogous but the inner maximization
loop performs T -steps of regular gradient ascent (not using the sign of the gradient and without
projections).

For the adversarial training experiment considered in Section 6, we also evaluated the trained models
against various other attacks. Recall that the models were training using T = 10 steps of gradient
ascent in the inner optimization loop with a learning rate of η2 = 4. To begin, we evaluated the
trained models against gradient ascent attacks with a fixed learning rate of η2 = 4 and a number of
steps T ∈{5, 10, 20, 40}. We also evaluated the trained models against gradient ascent attacks with
a fixed budget of Tη2 = 40 and various choices of T and η2. These results are presented in Figure 9.
Finally, we evaluated the trained models against attacks using the Adam optimization method with a
fixed budget of Tη2 = 0.04 and various choices of T and η2. These results are presented in Figure 10.
Notably, we see that our algorithm outperforms the baselines and similar conclusions can be drawn
as from the experiments for adversarial training presented in Section 6. The additional experiments
highlight that our method of adversarial training is robust against attacks that the algorithm did not
use in training when of comparable computational power and also that it improves robustness against
attacks of greater computational power than used during training.

In Section 6, we showed the results of evaluating the gradient norms ∥∇f (θ, A(θ))∥ as a function
of the number of gradient ascent steps T in the adversary algorithm A and observed that it grows
much slower than exponentially. Here we provide more details on the setup. We took a run of our
algorithm trained with the setup described in Section 6 and retrieved the models that were saved after
25, 50, 75, and 100 training epochs. For each model, we then sampled 100 minibatches of data and


-----

Figure 8: Adversarial Training: Test accuracy during the course of training against gradient ascent
attacks with a fixed learning rate of η2 = 4 and the number of steps T ∈{5, 10, 20, 40}.

Figure 9: Adversarial Training: Test accuracy during the course of training against gradient ascent
attacks with a fixed attack budget of Tη2 = 40 where T is the number of attack steps and η2 is the
learning rate (LR).

Figure 10: Adversarial Training: Test accuracy during the course of training against Adam optimization attacks with a fixed attack budget of Tη2 = 0.04 where T is the number of attack steps and
_η2 is the learning rate (LR)._

for each minibatch performed T ∈{20, 30, 40, 50, 60, 70, 80, 90, 100} steps of gradient ascent with
learning rate η2 = 4 (the learning rate from training) and then computed the norm of the gradient
_∇f_ (θ, A(θ)) where A corresponds to the gradient ascent procedure with the given number of steps
and learning rate. In Figure 2, which is reproduced here in Figure 11a, the mean of the norm of the
gradients over the sampled minibatches are shown with the shaded window indicating a standard
deviation around the mean. We also repeated this procedure using η2 = 1 and show the results in
Figure 11b from which similar conclusions can be drawn.


-----

(a) (b)

Figure 11: Adversarial Training: ∥∇f (θ, A(θ))∥ as a function of the number of steps T taken by
the gradient ascent algorithm A evaluated at multiple points in the training procedure. Figure 11a
corresponds to using η2 = 4 in the gradient ascent procedure and Figure 11b corresponds to using
_η2 = 1 in the gradient ascent procedure._

Layer Type Shape

Convolution + ReLU 5 × 5 × 20

Max Pooling 2 × 2

Convolution + ReLU 5 × 5 × 20

Max Pooling 2 × 2

Fully Connected + ReLU 800

Fully Connected + ReLU 500

Softmax 10


Table 1: Convolutional neural network model for the adversarial training experiments.

Finally, we provide details on the convolutional neural network model for the adversarial training
experiments. In particular, this model is exactly the same as considered in (Nouiehed et al., 2019)
and we reproduce it in Table 1.

**Experimental Details. For the experiments with neural network models we used two Nvidia GeForce**
GTX 1080 Ti GPU and the PyTorch higher library(Deleu et al., 2019) to compute ∇f (θ, A(θ)). In
total, running all the experiments in the paper takes about half of a day with this computational setup.
The code for the experiments is included in the supplementary material with instructions on how to
run. We built our code for the adversarial training based off code openly provided by the authors
of (Nouiehed et al., 2019).


-----

