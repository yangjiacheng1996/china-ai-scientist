# On the Uncomputability of Partition Functions In Energy-Based Sequence Models

**Chu-Cheng Lin & Arya D. McCarthy**
Center for Language and Speech Processing, Johns Hopkins University

Abstract

In this paper, we argue that energy-based sequence models backed by expressive
parametric families can result in uncomputable and inapproximable partition functions.
Among other things, this makes model selection — and therefore learning model
parameters — not only difficult, but generally undecidable. The reason is that there are
no good deterministic or randomized estimators of partition functions. Specifically,
we exhibit a pathological example where under common assumptions, no useful
importance sampling estimators of the partition function can guarantee to have variance
bounded below a rational number. As alternatives, we consider sequence model families
whose partition functions are computable (if they exist), but at the cost of reduced
expressiveness. Our theoretical results suggest that statistical procedures with asymptotic
guarantees and sheer (but finite) amounts of compute are not the only things that make
sequence modeling work; computability concerns must not be neglected as we consider
more expressive model parametrizations.

1 Introduction

Modeling discrete sequences is central to natural language processing and bioinformatics. Many common
parametric sequence models ˜푝 are (or can be cast as) energy-based (LeCun et al., 2006): they yield a weight
_푝˜(풙) for any given string 풙. Although energy-based models (EBMs) of sequences need not represent_
probability distributions, they have been proposed as such to counter the inexpressivity of autoregressive
sequence models (Bakhtin et al., 2021; Lin et al., 2021, §4.1). EBMs with finite partition functions
principle of probabilistic inference (푍 _푝˜_ [≜] [Í]풙 _푝[˜](풙) ∈_ R>0 define distributionsGhahramani 푝(풙) ≜,푝 2015[˜] (풙)/푍).푝˜ 1[over strings, which are often queried under the]

Energy-based sequence models are often parametrized as neural models. Each parameter vector 휽 in
some parametric neural network family 횯 R[푑] identifies a parametric model ˜푝휽, which then defines a
⊆
parametric distribution over strings 푝휽, assuming 푍 _푝˜휽_ [exists (][Chen et al.][,][ 2018][). Contemporary neural]
networks have been shown to be very powerful. In particular, some popular parametric neural sequence
model families, such as RNNs (Siegelmann & Sontag, 1995) and Transformers (Bhattamishra et al.,
2020; Pérez et al., 2021), have been formally shown to recognize recursively enumerable languages:
given any Turing machine 푀, there is a parameter vector 휽 ∈ 횯 such that the parametrized sequence
model 푁휽 recognizes the same language as 푀 does. In other words, these sequence model families are
**Turing-complete.**

It is therefore intuitive that energy-based sequence models, backed by such powerful neural networks, form
expressive families of string distributions (§3.1): for any decision algorithm 푀, there is a parameter vector
_휽_ _푀_ in a Turing-complete parametric family 횯, such that 푝휽 _푀_ exists, and 푝휽 _푀_ _풙_ is high if and only if 푀
( )
accepts 풙. It would seem assuring to work with such expressive family of sequence distributions, 횯:
assuming the true string probabilities indeed can be computed in polytime, 횯 is well-specified. Moreover,
one may assume that given we can sample from 푝휽 _푀_, we would be able to use consistent estimators to find
_휽_ [′] 횯, where 휽 [′] _휽_ _푀_ .
∈ ≈

Unfortunately, we find that with such an expressive distribution family 횯, whether the identifiability
assumption holds — required for most consistent estimators — itself is undecidable (Turing, 1937).

1Many popular energy-based sequence models compute normalized probabilities directly (i.e., ˜푝(풙) = 푝(풙))
(Jelinek, 1980; Katz, 1987; Bengio et al., 2003; Brown et al., 2020, inter alia). This makes both training and querying
with string prefixes much easier, at the cost of expressiveness (Lin et al., 2021).


-----

Moreover, model selection on any held-out data is also undecidable. Even worse, we show that there exists
a vector 휽 [′] ∈ 횯, such that as long as a parametric family 횯[′] ⊆ 횯 contains 휽 [′], model selection will not be
possible for 횯[′] either — even when 횯[′] itself is not necessarily expressive (e.g., 횯[′] can be fixed-size
Transformer EBMs, which cannot parametrize all EBMs that require more parameters). We construct one
such ‘pathological’ distribution 푝휽[′] as a Transformer EBM.

These negative results stem from the uncomputability and inapproximability of 푍. A main technical
contribution of this paper is that there is no algorithm (either deterministic or randomized) that can
approximate 푍 well. An immediate consequence is that sampling-based statistical procedures are not useful
in this scenario, either, as long as they terminate in finite time. However, we will see that for less expressive
model families, such uncomputability issues do not arise. Our negative results are summarized in Table A.1.
To ensure that model selection is still possible (such that we can compare different parameter vectors with
confidence),2 we have no choice but to resort to less expressive string distributions.

The paper is structured as follows. In §2 we review definitions and known results of weighted languages,
sequence model families, and formalize weighted Turing machines, EC-complete parametric families, and
computable estimators. In §§3–5 we describe our main technical results: there exist pathological EBM
sequence models that have uncomputable partition functions, which cannot be approximated well under
randomized estimators, and do not have asymptotic estimators that have any good guarantees. In §6 we that
argue our negative results make model selection impossible for expressive model families, and discuss why
common estimation methods fail. Finally, we discuss three ‘palliative’ parametrization choices in §7, which
all guarantee computable partition functions, at the cost of expressiveness.

2 Background

2.1 Energy-based sequence models

Energy-based models (EBMs) of sequences (LeCun et al., 2006; Rosenfeld et al., 2001; Sandbank, 2008;
Huang et al., 2018; Bakhtin et al., 2021) are a family of discrete sequence models. Given a string 풙 ∈ _푉_ [∗]
over a finite vocabularyprobability 푝(풙) = _푝[˜]_ (풙 푉)/[Í], an EBM ˜풙 _푝[˜]_ (풙). In this work, we focus on EBMs whose weight is efficiently computable,푝 computes string weight ˜푝(풙), but not the (normalized) string
_i.e., polytime in the length of 풙. Previous work (Bakhtin et al., 2021; Lin et al., 2021) showed that EBMs_
define expressive distributions; but this requires normalization. While EBMs are often intractable to
normalize (e.g., the Ising model), the infinite domain of finite strings opens the door to uncomputable
probabilities, which invite the impossibility of model selection and comparison.

2.2 Weighted languages

EBMs give weights to strings. Here we formally characterize these weighted strings as a weighted
**weighted languagelanguage. An unweighted language ˜푝** is a function ˜푝 퐿: 푉⊆[∗]푉→[∗] is a set of finite stringsR≥0.3 In this work, we discuss Boolean languages, such 풙 over a finite vocabulary 푉. A
that 푉 = B ≜ {0, 1}. We also focus on weighted languages where distributions over strings exist. Following
Lin et al.also called the (2021 partition function), we say such weighted languages of ˜푝.4 We can then normalize ˜푝 are normalizable ˜푝 into a: 푍 _푝˜ distribution[≜]_ [Í]풙 ∈B[∗] _푝[˜](풙 푝) ∈overR> B0.[∗] 푍such푝˜_ [is]
that 푝(풙) = ˜푝(풙)/푍 _푝˜_ [, and thereby][ Í]풙 ∈B[∗] _[푝][(][풙][)][ =][ 1.]_

The efficiently computable (weighted) languages (EC; Lin et al., 2021) are those weighted languages
_푝˜, where a string’s weight (which must be non-negative) is a polytime function of the string. Weighted_
languages defined by most EBM sequence models fall into this class, since they score every string in finite
time (and usually in polytime); and the scores ultimately are computed by some algorithm.

Intuitively, string weights of an EC language can be obtained as a side product from a weighted Turing
**machine that recognizes 풙** (in polytime). However, Lin et al. (2021) did not precisely describe how a
Turing machine maps input string 풙 to its (rational) weight ˜푝(풙) in finite time. In this work, such a
construction (out of many possible ones) is necessary, as we need to show that this string weighting can be

2Of course, model selection (and computing 푍 _푝˜_ [) are not always needed — for example, simply deciding whether]
_푝˜(풙431With a slight abuse of notation, we also define supportWe use the convention that) > ˜푝(풙2) for two given strings 푍_ _푝˜_ _풙[is the partition function of ˜]{1,2}. In such case the uncomputability issues we discuss are not a concern.( ˜푝) ≜푝, and{풙_ : ˜ 푍푝푞(˜풙[of ˜]) >푞 0, etc. The subscript is omitted in}.
unambiguous cases.


-----

done by parametric sequence model families (see Appendix B).

2.3 Locally normalized distributions

A popular subclass of weighted languages is locally normalized weighted languages (LN), where
conditional local distributions given prefix ˆ풙: 푝(· | ˆ풙) can be computed in finite time. Since they
automatically define a distribution over strings, we use the term locally normalized distributions
interchangeably. If 푝(· | ˆ풙) can be computed in polynomial time, we call such distributions efficiently
**locally normalized distributions (ELN) — this is the weighted language class of most autoregressive**
models, just as its superset EC is the weighted language class of most energy-based sequence models.

Locally normalized distributions over sequences have ˜푝(풙) = 푝(풙), and 푍 = **_풙_** ∈B[∗] _푝[˜](풙) = 1. They are_
**consistent: the probability that a string is infinitely long under such distributions is zero. Equivalently,**
given any 휖> 0, we can approximate 푍 with a finite sum of string weights:

[Í]

**Proposition 1 (Consistency of LN distributions (Booth & Thompson, 1973; Chen et al., 2018)). Let**
_푝_ ∈ LN be a locally normalized distribution over strings. All strings of a given length or longer have their
_probabilities bounded. That is, for all positive real numbers 휖, there is a length 푛_ _at which all strings 풙_ _of_
_length at least 푛_ _have 푝(풙) < 휖._

The consistency property of locally normalized distributions implies that they have an exact sampling
procedure that almost surely terminates in finite time. Therefore, they (and in particular ELN distributions)
are an attractive choice when we need to sample from the distribution they define, e.g., in sampling-based
parameter estimation procedures.

2.4 EC-complete parametric families

We have introduced energy-based sequence models (§2.1) and their characterization as weighted languages
(§2.2). However, we usually do not work with weighted Turing machines directly in machine learning. Here
the most common models of computation are (neural) sequence model families, such as RNNs and
Transformers. While these computational models can appear quite dissimilar to state-machine-based models
of computation (e.g., Turing machines), they have been shown to possess the same computation power
(Siegelmann & Sontag, 1995; Pérez et al., 2021). That is, they are Turing-complete.

Just as we extend the definition of Turing machines to weighted Turing machines, we likewise formalize
_weighted sequence model families. We thus introduce EC-complete parametric families as a sequence_
model counterpart of the weighted language class EC. At a high level, a parametric family 횯 is EC-complete
if given any ˜푝 ∈ EC (as a description of a weighted Turing machine), we can construct a parameter vector
_휽_ ∈ 횯 ⊆ Q[∗] which defines ˜푝’s corresponding sequence model. The model produces an output embedding
which we then decide in polytime to be either a ‘halting embedding’ or not. If it is, we can then extract the
weight ˜푝(풙). (A rigorous exposition is in Appendix C.)

We show that with a modification to positional embeddings, the family of arbitrary precision, hard attention
Transformers defined in Pérez et al. (2021) is EC-complete:5

**Theorem 1. The class of one-encoder-layer, four-decoder-layer Transformer networks with positional**
_encodings (푛,_ [1]/푛, [1]/푛[2], 2[푛]) is EC-complete.

_Proof sketch. We extend the construction from Pérez et al. (2021), adding an additional layer to accumulate_
the string weight over time steps. □

2.5 Estimators

A main result of this work is that partition functions in an EC-complete family are uncomputable. Moreover,
randomness does not help estimation; and correct asymptotic estimators are not useful. We define these
estimators here in order to discuss the power of different estimators concretely.

Let 횯 be a parametric family. The function 푓 : 횯 → Q is an exact estimator if there exists a weighted
deterministic Turing machine that takes 휽 as input and, upon halting, outputs 푓 (휽) ∈ Q in finite time.

5All proofs omitted from the main text are included in Appendix D. Proof sketches for major theorems are in the
main text.


-----

Many estimation procedures are anytime algorithms: they do not have a predetermined runtime, and they
can be stopped at any time before completion to produce a valid output. Output quality of an anytime
algorithm may improve with increased runtime. Moreover, many of these algorithms have asymptotic
guarantees, in that their outputs are optimal in some sense (e.g., consistency) in the limit. We capture these
algorithms with asymptotic estimators: a function 푓 (휽) is an asymptotic estimator if there exists a
weighted Turing machine that takes both 휽 and an index 푖 as input, then outputs a value 푓 _푝,푖˜_ [∈] [Q][ in finite]
time, such that the outputs converge toward 푓 (휽) as 푖 increases toward infinity. (An example is _푍[ˆ]asym_
introduced at §5.)

We now extend both exact and asymptotic estimators to the stochastic case, where we compute the estimates
using randomized algorithms instead of deterministic ones. As is conventional for randomized algorithms,
we assume the use of probabilistic Turing machines. These have access to an infinitely long random tape.
The tape describes an infinite binary sequence from tossing a fair two-sided coin infinitely many times. We
call the random tape distribution 푝 _휏.6 We define a randomized exact estimator f as a weighted Turing_
machine with two input tapes — the random tape 휏 ∈ B[N], and an input tape that has 휽 — and outputs
_푓_ (휽, 휏) in finite time. Likewise, we say f휽,푖 is a randomized asymptotic estimator if there exists a
function 푓 (휽) ∈ R and a weighted Turing machine that takes (휽, 푖), 휏 on two input tapes, so that for all
random Boolean tapes 휏 ∈ B[N], we converge with lim푖→∞ _푓휽,푖,휏_ = 푓 (휽). Many Monte Carlo estimators
can be seen as randomized asymptotic estimators, including rejection sampling and importance sampling
estimators.

3 Expressiveness and uncomputability: pathological EBMs

3.1 Expressive sequence distributions

To illustrate the uncomputability issues of energy-based models, we construct families of string distributions
that are expressive: they require the full power of an EC-complete sequence model family.

We define G푘 = { ˜푝 _푀,푘_ : 푘 ∈ Q>0} to be a set of weighted languages, where ˜푝 _푀,푘_ is parametrized by
deterministic Turing machine 푀 that takes an empty string as input (‘input-free’). Let 퐿 _푀_ B[∗] be a
⊆
prefix-free Boolean language of computation sequences of a Turing machine — that is, encodings of the
trace of the Turing machine 푀’s operations. We define

1 3 [|][풙] [|+][1] _푘_ if 풙 _퐿_ _푀_, and 풙 encodes a valid accepting

_푝˜푀,푘_ (풙) = 1/3 [|][풙] [|+][1] + trace ofotherwise.∈ _푀._ (1)
 /

The weight of any string 풙, where _풙_ = 푛, under ˜푝 _푀,푘_ can be computed in time 푂 [ ]poly _푛_, by verifying

 | | ( )[]

whether 풙 is an accepting execution trace of 푀, from the initial state to an halting state. That is, ˜푝 ∈ EC
(§2.2). We also know that for any (deterministic) machine 푀, the language’s partition function 푍 _푝˜_ _푀,푘_
(exists, and it must equal either 1 or 1푍 _푝˜_ _푀,푘_ [=][ 1). Therefore, each ˜]푝 ∈G +푘 _푘defines a string distribution., since there is either one halting trace (7_ _푍_ _푝˜_ _푀,푘_ [=][ 1][ +][ 푘][), or none]

Since for all 푘 ∈ Q>0, G푘 ⊂ EC, all weighted languages ˜푝 _푀,푘_ have an equivalent parameter vector 휽 _푀,푘_
in any EC-complete family 횯. Also, since each _푘_ is a bĳection between the set of all input-free Turing
G
machines and a subset of 횯, it follows that there is no exact estimator (§2.5) of the partition function of any
EC-complete family (e.g., Transformer EBMs (Theorem 1)), by a reduction from Halt:8
**Theorem 2. Let 횯** _be a parametric EC-complete sequence model family. And let 횯푘_ ⊂ 횯 _be bĳective_
_with G푘_ _. There is no 푘_ ∈ Q>0 for which there exists an exact estimator _푍[ˆ]_ _푘_ _that takes as input 휽_ ∈ 횯푘 _as_
_input, and computes_ _푍[ˆ]_ _푘_ (휽) = 푍 _푝˜휽_ _[in finite time.]_

_Proof sketch. 횯푘_ contains parametric vectors for all input-free Turing machines 푀. For any of these

6Formally speaking, we define a probability space Ω, _, P_ where Ω = B[N] is our sample space, = _퐴풃_ :
( A ) A {
_퐴풃_ is the set of all sequences Ω that have prefix 풃 B[∗] is our event space, and P _퐴_ = 2[−][푛] where 푛 is the
∈ ∈ } ( )
length of the longest shared prefix of 퐴, is our probability function (Stroock, 2014).
7Our construction of expressive sequence distributions is inspired by a weighted language construction in Lin et al.

(2021). See Appendix E for further discussion.
8Speaking loosely, Halt is the task of recognizing (deciding) whether a given program on an ideal computer will
properly terminate in finite time or not (Sipser, 2013; Arora & Barak, 2006).


-----

weighted languages, knowing the exact value of 푍 _푝˜_ _푀,푘_ [∈{][1][, 푘] [+][ 1][}][ is enough to decide whether][ 푀] [halts]
(푀 halts iff 푍 _푝˜_ _푀, 푗_ [=][ 푘] [+][ 1). As Halt is undecidable for input-free Turing machines,][ ˆ]푍푘 cannot exist. □

3.2 An EBM whose partition function is uncomputable

Theorem 2 states there is no estimator that ‘works’ for a subset of parameter vectors. While every _푘_ is
G
much smaller than its superset EC, _푘_ is still (countably) infinite. Here under the assumption that ZFC is
G
consistent, we construct one weighted language _푏[˜]_ 1 (for simplicity; it holds for arbitrary 푘), where 푍푏˜
∈G
is uncomputable:

**Theorem 3. Assuming ZFC axioms and that assuming they is consistent, there exists an EC weighted**
_language_ _푏[˜]_ ∈G1 such that (a) 푍푏˜ [=][ 푐] [∈{][1][,][ 2][}][ exists, but (b) there is no proof of][ 푍]푏[˜] [=][ 푐][.]

_Proof sketch. We construct_ _푏[˜]_ such that 푍푏˜ [=][ 2 iff ZFC is inconsistent. If there were a proof][ 푍]푏[˜] [=][ 1 then]
we proved ZFC is consistent, which is impossible under Gödel’s 2nd incompleteness theorem (Jech, 1994).
If there were a proof 푍푏˜ [=][ 2, then we proved ZFC is not consistent, which violated our assumption.] □

The existence of _푏[˜]_ suggests that if there is an algorithm that approximates 푍 _푝˜_ [,][ and][ produces a witness of]
its approximation quality, then this algorithm will not work on any set of parameter vectors that can
parametrize _푏[˜]_ — even when this algorithm may work for some subsets of 횯. This is useful in allowing us to
show negative results regarding finite subsets of 횯. Appendix F gives one such example.

4 No randomized algorithm can estimate 푍 accurately

We’ve shown by Theorem 2 that for an EC-complete family, there are no exact estimators than can get 푍
perfectly right. In this section, we show that no randomized exact estimator for this is unbiased. Further,
there isn’t even an estimator whose bias is within some factor 휖, regardless of the variance’s magnitude.

**Lemma 1. Let 횯** _be an EC-complete parametric family. There is no multiplicative factor 휖_ ∈ Q>1 for
_which every 휽_ ∈ 횯 _can have its partition function approximated with_ **Z[ˆ]** _휖_ (휽) within a factor of 휖 _— with_
_probability greater than_ [2]/3. That is, we cannot have

_푃_ _휖_ _푍_ _푝˜휽_ [≤] **Z[ˆ]** _휖_ _휽_ _휖푍_ _푝˜휽_ _>_ [2] 3. (2)
([1]/ ) ( ) ≤ /
 

_Proof sketch. Because_ **Z[ˆ]** _휖_ computes an estimate in finite time, it can only use finitely many distinct
random tape segments. We therefore derandomize **Z[ˆ]** _휖_ by enumerating finitely many ‘random’ tapes, and
can always return correct answer, given the [2]/3 success probability assumption, and would be able to decide
Halt, making use of distributions in _휖_ 2. □
G

Taken together, Theorem 2 and Lemma 1 state that no exact estimator 푍[ˆ] — whether randomized or
deterministic — can approximate 푍 with good confidence. In Theorem 4 below, we will make an even
stronger claim: regardless of the dispersion magnitude, it is impossible to bound the mean of random exact
estimators of 푍 to within any (computable) multiplicative factor. This is because the mean of **Z[ˆ]** _휖_ can be
computed in finite time, by derandomizing **Z[ˆ]** _휖_ similarly to our proof of Lemma 1:

**Theorem 4.that there exists a randomized exact estimator Let 횯** _be an EC-complete parametric family. There is no multiplicative boundZ[ˆ]_ _휖_ _that guarantees_ [1]/휖 ≤ [E][ [][ ˆ]Z휖 (휽)]/푍푝˜휽 [≤] 휖 _[휖]∈[, for every]Q>1 such_
_휽_ 횯 _where ˜푝휽_ _is normalizable._
∈

_Proof sketch. We derandomize the expectation of_ **Z[ˆ]** _휖_ . □


-----

5 Common asymptotic estimators do not give useful guarantees

Let’s now recap the progress we’ve made so far. We’ve shown that no (deterministic) exact estimator can get
_푍_ exactly right in general (Theorem 2), lest it need to solve Halt. Further, no randomized exact estimator
can approximate it within any given relative tolerance, with good confidence (Theorem 4).

But what about asymptotic estimators? We do know there are correct asymptotic estimators of 푍. For
example, consider the following asymptotic estimator _푍[ˆ]_ (휽) backed by a weighted Turing machine that
takes 휽 ∈ 횯 and 푖 ∈ N as inputs, and returns 푓휽,푖 ≜ [Í]풙:풙 ∈B[∗], |풙 |≤푖 _푝[˜]휽_ (풙). We have lim푖→∞ _푍[ˆ]휽,푖_ = 푍 _푝˜휽_ [,]
so _푍[ˆ]_ is asymptotically correct. However, _푍[ˆ]asym does not have a convergence rate guarantee: for any 푖_ ∈ N,
∥푍[ˆ]휽,푖 − _푍_ _푝˜휽_ [∥] [is uncomputable. We also do not know how much can we improve our estimator when we]
increment 푖. As Corollary 2 suggests, we likely cannot have such a guarantee.

In this section, we formalize this intuition for two popular asymptotic estimators: rejection and importance
sampling methods (with other asymptotic estimators left as future work). Specifically, we show that any
parametric family that is able to parametrize _푏[˜]_ from §3.2 cannot have provably useful locally normalized
distributions (§2.3) as proposal distributions.

5.1 Rejection sampling estimator of 푍 cannot be guaranteed to be possible.

Rejection sampling (Owen, 2013) is a common exact sampling method, applicable even when we cannot
sample from an unnormalized distribution ˜푝. We instead sample from an easy-to-sample distribution 푞,
then stochastically reject samples, to ensure the probability that a sample 풙 is kept is proportional to ˜푝(풙).

For rejection sampling to work, the candidate 푞’s support must contain the target ˜푝’s entire support, so that
all true points can be sampled. We also need some finite constant 푐 so that 푐푞 envelops ˜푝:
∃푐 ∈ R>0 such that ∀풙 ∈ B[∗], ( ˜푝(풙)/푞(풙)) ≤ _푐._
We will show that for certain EBMs, one cannot formally guarantee the existence of an eligible 푞 ∈ LN.
**Theorem 5. Using ZFC as our axiom set and assuming they are consistent, then there exists a normalizable**
EC weighted language ˜푝, where there does not exist a consistent locally normalized proposal distribution
_푞_ ∈ LN, and 푐푞 ∈ Q>0, such that it can be proven ∀풙 ∈ B[∗], _푝[˜]_ (풙)/푞 (풙) < 푐푞.

_Proof sketch. Let ˜푝_ = _푏[˜]_ . If there were such 푞 and 푐푞, we could in finite time prove 푍 _푝˜_ [=][ 1 or][ 푍] _푝˜_ [=][ 2,]
which contradicts Theorem 3. □

Theorem 5 implies that there is no way of ensuring rejection sampling works, not only for any EC-complete
families, but also for any parametric family that can parametrize 푏[˜].

5.2 Importance sampling estimator of 푍 cannot be guaranteed to be effective.

Similar to the case of rejection sampling, one cannot guarantee an importance-sampling estimator of 푍 to be
‘good’ — in this case, we mean that there cannot be a proof that the importance sampling variance is finite.

We first formalize importance sampling estimators of 푍 as randomized asymptotic estimators (§2.5). Let

_푁_

**Zˆ** **_휽[푞],푁_** [=][ 1]푁 _푝푞˜휽_ (풙풙[(][(][푛][푛][)][)] )

_푛=1_

Õ ( )

be an 푁-sample importance sampling estimator of 푍 _푝˜휽_ [under][ 푞][, so all][ 풙] [(][푛][)][ are samples from][ 푞] [∈] [LN][.]

We generally want to minimize the variance of **Z[ˆ]** **_휽[푞],푁_** [under][ 푞][:][ Var][푞] **Zˆ** **_휽[푞],푁_** (Owen & Zhou, 2000). And

we certainly do not want Var푞 **Zˆ** **_휽[푞],푁_** = . Unfortunately, for certain EBMs, we cannot guarantee there is 
∞
a good locally normalized proposal distribution that has finite variance: 
**Theorem 6. Let 횯** _be an EC-complete parametric family. Assuming ZFC axioms and assuming they are_
_consistent, there exists 휽_ ∈ 횯 _where there does not exist a consistent locally normalized “proposal”_
_distribution 푞_ LN such that it can be proven Var푞 **Zˆ** **_휽[푞],푁_** _< 푐_ ≠ _, where 푐_ Q>0.
∈ ∞ ∈
 

_Proof sketch. Proven in a manner similar to the proof of Theorem 5._ □


-----

6 Uncomputable 푍 causes parameter estimation problems

Theorems 2 and 3 state that it is generally impossible to estimate partition functions in an expressive
parametric family, such as certain subsets of an EC-complete family. Here we show how parameter
estimation is made difficult as well: parameter identifiability is formally undecidable for an EC-complete
family. Model selection is not possible, either, despite attempts to circumvent this (Table 1).

6.1 Parameter identifiability under EC-complete families is undecidable

Consistency of many estimators relies upon the condition of parameter identifiability (Lehmann & Casella,
2006) — two different parameter vectors should define different string distributions. But we in general
cannot ascertain whether this condition holds, even for finite subsets of an EC-complete family:

**Theorem 7.and decides whether Let 횯** _be an ˜푝휽1 and EC-complete family. There is no algorithm that takes ˜푝휽2 are the same weighted language._ _휽_ 1 ∈ 횯, 휽 2 ∈ 횯 _as input,_

6.2 Model selection is generally impossible for EC-complete sequence model families

Theorem 7 suggests that consistency guarantees of common parameter estimators do not hold, when
used to estimate vectors from 횯. Nonetheless, such ‘off-label’ use of consistent parameter estimators
in an expressive parametric family is quite common — one usually just selects the best model 휽 [∗]
among finitely many candidates (say {휽 1 . . . 휽 _푁_ }) that achieves highest held-out likelihood: 휽 [∗] =
arg max휽푛:1 _푛_ _푁_ **_풙_** _[푝]휽푛_ [(][풙][)][, where][ D][ is a finite set of strings.]
≤ ≤ ∈D

However, Theorem 8Î implies that exact log-likelihood-based model selection is generally impossible for
EC-complete sequence model families:

**Theorem 8. For any ˜푝** ∈ EC and for any EC-complete family 횯, there is no algorithm Better ˜푝 _[that]_
_takes two parameter vectors 휽_ 1 횯, 휽 2 횯, and returns YES if KL _푝_ _푝휽1_ KL _푝_ _푝휽2_ _, and NO_
_otherwise, where 푝, 푝휽1, 푝휽2 are string distributions defined by ∈_ ∈ ˜푝, ˜푝휽(1, ˜||푝휽2 respectively.) ≥ ( || )

7 Palliative alternatives

What is a practitioner to do, given that this class of models is unlearnable in general? We emphasize that a
model family does not have to be EC-complete to suffer from model selection problems (§6.2) — for
example, model selection is impossible for fixed-size Transformer networks with large enough 푑’s either, by
an extension to Theorem 8 (see Theorem 11 in Appendix G).9 10 In other words, to ensure the problem of
uncomputability does not haunt us, the best we can do is to cripple ˜푝 so severely that uncomputability is
impossible.

We identify three palliative choices that restrict the family of EBMs. Each cripples the model ˜푝 in its own
way, affording computability at the cost of expressiveness. The choice of triage infuses the model with an
inductive bias; we must tailor our models based on our prior beliefs about the problem domain.

**Restricting support( ˜푝) to be finite.** If ˜푝 assigns non-zero weights to only finitely many strings, then
_푍_ _푝˜_ [is a finite sum of rational numbers, and is also rational. Here, sampling-based inference and estimation]
methods return to their usefulness. One way to ensure support( ˜푝) is finite is by upper-bounding the
maximum string length (e.g., Bakhtin et al. (2021)).

The finite-support restriction imposes an obvious limitation that it cannot handle long strings. Moreover,
while 푍 _푝˜_ [is computable when][ support][(][ ˜]푝) is finite, this quantity can still be practically inapproximable,
assuming that ˜푝 is expressive (e.g., ˜푝 is an EC weighted language that has finite support), except for very
short strings. Let 푛 be the longest string under ˜푝 to have non-zero weight. Assuming NP ⊈ P/poly,
no randomized estimator of 푍 _푝˜_ [that is a good approximation can have a guaranteed][ 푂] [(][poly][(][푛][))][ time]
complexity (Chandrasekaran et al., 2008). However, if ˜푝 has limited expressiveness (e.g., when ˜푝 is an Ising
model where a string weight is the sum of pairwise weights), then FPRAS algorithms for approximating 푍 _푝˜_

9In addition to model selection issues, it may also be difficult to acquire unbiased gradients of log 푍:
∇(10logLimiting ourselves to small 푍) ≜ [1]/푍 ∇푍, which are needed for MLE-based training. 푑’s to avoid uncomputability issues may not be practical; we leave finding the largest 푑
that provably does not involve uncomputability problems — if it is even possible — as future work.


-----

may exist when ˜푝 describe a low-degree (≤ 2) graph (Jerrum & Sinclair, 1993; Luby & Vigoda, 1999).
However, for high-degree graphs (≥ 3) it can be shown no FPRAS algorithm for approximating 푍 _푝˜_ [exists,]
assuming RP ≠ NP (Galanis et al., 2016).

**Autoregressive parametrization of ˜푝.** An alternative choice is to confine ourselves to autoregressive
models, i.e., locally normalized string distributions (§2.3). Under an autoregressive model 푝, 푍 _푝_ = 1 by
definition. We also note that any (unnormalized) distribution ˜푝 obtained by removing probability mass from
_푝_ will have a computable partition function, as long as ˜푝 ∈ EC:

**Theorem 9. Let 푝** _be any LN weighted language. Any ˜푝_ ∈ EC where ∀풙 ∈ _푉_ [∗], ˜푝(풙) ≤ _푝(풙) has a_
_computable 푍_ _푝˜_ _[.]_

_Proof sketch. We construct an algorithm that approximates 푍_ _푝˜_ [to any arbitrary error level in finite time,]
exploiting the consistency property of 푝 (Proposition 1). □

Theorem 9 implies that conditionalization operations on 푝, which remove strings from the support of 푝 to
get weighted language ˜푝, result in a computable 푍 _푝˜_ [(as long as we can decide which strings are removed);]
and such a ˜푝 is therefore not subjected to the limitations of Theorem 4.

Unlike the ‘finite support’ fix, an autoregressively parametrized (or subsequently conditionalized) ˜푝 can
have an infinite support. A conditionalized ˜푝 can have an intractable (but computable) partition function,
and they are still subject to the expressive limitations imposed on LN languages: namely there is an EC
language whose string weight rankings cannot be honored by any such conditionalized ˜푝 (Lin et al., 2021).

_푝˜_ **as low-treewidth factor graph grammars.** Finally, we may limit ourselves to weighted languages
defined by low-treewidth factor graph grammars (Chiang & Riley, 2020). Factor graph grammars generalize
factor graphs, which cover many classes of graphical sequence models, such as 푛-gram, HMM, and
whole-sentence language models (Jelinek, 1980; Kuhn et al., 1994; Rosenfeld et al., 2001), linear CRF
models (Lafferty et al., 2001), and weighted FSAs in general (Dreyer & Eisner, 2009): a factor graph
grammar describes a (possibly infinite) set of factor graphs, generated from a hyperedge replacement graph
grammar.

Assuming that an FGG 퐺 contains at most one 푛-observed-node factor graph for all 푛 ∈ N, it then defines a
weighted language ˜푝퐺 _풙_ = _휓_ Ψ **_풙_** _[휓][, where][ factor][ 휓]_ [is a positive function of nodes. The treewidth of]
( ) ∈ | |
an FGG 푊 _퐺_ is the maximum number of nodes any 휓 can be a function of, and Ψ **_풙_** is the set of all
( ) | |
factors of string length _풙_ .

[Î]
| |

If 푍 _푝˜퐺_ [∈] [R][ exists, it can be computed exactly by an algorithm, in time exponential in][ 푊] [(][퐺][)][ following]

Chiang & Riley (2020). Exact computation of 푍 _푝˜퐺_ [may be manageable as long as][ 푊] [(][퐺][)][ is small, which]
would allow us to exactly compute held-out data likelihood, and also train with a (marginalized) MLE
objective function. However, limiting 푊 (퐺) directly limits the expressiveness of ˜푝.

8 Related work

Turing completeness of formal languages and associated uncomputability issues emerge repeatedly
in computer science. For example, Turing completeness may emerge as an unwanted side effect in
programming languages, since it implies undecidability. One of the best known examples is the Turing
completeness of the C++ grammar (Veldhuizen, 2003; Haberman, 2013), which makes both parsing and
compiling C++ programs undecidable. Similar problems exist for Java (Grigore, 2016) and Haskell with
(unrestricted) instance declarations (Wansbrough, 1998). Another example is the (possibly inadvertently
introduced) Turing completeness of the page fault handling mechanism on modern computer systems,
which raises security concerns in the context of trusted computing (Bangert et al., 2013).

Our work is not the first to discuss the consequences of computability in machine learning: assuming
we can acquire training data from an oracle, under a supervised learning setting, recognition of an
undecidable language is PAC-learnable (Lathrop, 1996). Agarwal et al. (2020) extended the definition of
PAC learnability (Valiant, 1984) to computable learners. By contrast, we are focused on the computability
of EBMs for sequences, such as a language model as a component of a larger system for automatic
speech recognition. Designing an appropriate, efficient loss functional is a challenge that several prior
works have compared. With the plethora of learning strategies for EBMs, it is untenable to point out the


-----

Energy-based
(i.e., globally
normalized)


Infinite
language
support


Scoring function
has unbounded
treewidth Consistency


Technique

Noise-contrastive estimation (Ma & Collins (2018);
used in Lin et al. (2021); Bakhtin et al. (2021))

MLE with variable elimination in factor graph
grammars (Chiang & Riley (2020); used in Eisner
(2001); Finkel et al. (2008), inter alia)

MLE with autoregressive parametrization (Mikolov
et al., 2010)

Contrastive divergence (Hinton, 2002)

Contrastive estimation (Smith & Eisner, 2005)


Table 1: Deficiencies of some common alternatives to overly expressive EBMs.

deficiency in each. Table 1 gives a handful of examples; none share the four properties we desire: 1. Global
normalization (without which the model would be in LN) 2. Support over infinite languages (of finite
strings) 3. Unbounded treewidth in the function assigning weights to strings 4. Estimator consistency (i.e.,
asymptotic guarantee to recover the true parameters).

Lin et al. (2021) noted autoregressive factors of EC languages can be uncomputable (see also Theorem 10).
They also noted that a weighted language can have an uncomputable partition function (presumably
resulting from the sum over infinitely many string weights). But they did not dwell on the question whether
such a weighted language could lie within the EC class, much less providing a constructive proof (see also
Appendix E). Instead, they emphasized that under the assumption that oracular access to trained parameter
strings is possible, arbitrarily good approximations of the (possibly uncomputable) partition function can be
memorized in the (autoregressive) model parameters. There is an interesting constrast between the stances
of Lin et al. (2021) and our work: Lin et al. (2021) saw the uncomputability of 푍 as a trivial issue from the
model capacity viewpoint, since good approximations take few bits in the parameters to store. On the other
hand, we see that the uncomputability problem can be a parameter estimation disaster — there will be no
guarantee of good approximations can be found in finite time at all.

9 Conclusion and future work

Energy-based models are posed as an efficient tool for decision problems, circumventing probabilities and
expensive normalization (LeCun et al., 2006). Extending this vision to generic sequence models, though,
can involve complexity/computability problems that are difficult, or even impossible. We’ve shown that as
energy-based sequence models become more powerful, the partition function becomes uncomputable —
even when we are restricted to polytime-computable weighted languages. Exact estimators, even if
randomized, cannot have accuracy guarantees. Popular asymptotic estimators on the other hand are not
useful either. Furthermore, model selection is generally impossible, even if we limit ourselves to fixed-size
sequence model families.

This paper continues a discussion started by Lin et al. (2021), who posture energy-based models as a more
powerful alternative to autoregressive sequence models. Autoregressive sequence models, after all, have
wide adoption and empirical successes (Radford et al., 2019; Brown et al., 2020). By contrast, more general
neural energy-based sequence models have not caught on. Why not? We give unlearnability — due to
_uncomputability — as a possible explanation: unless we give up the ability to learn parameters from data,_
we likely cannot use the full expressiveness afforded by powerful neural networks. Just like the model
capacity problems brought up by Lin et al. (2021), this result is independent of the amount of training data.

We emphasize that our results do not invalidate the findings of Lin et al. (2021): regardless of the actual
neural parametrization, autoregressive models can never capture certain distributions that energy-based
models can. Instead, one of our main messages is that we may not be able to find those EBM parameters in
_finite time, if we do not know what the parameters are. Of course, if we know the task perfectly well_
and can in fact manually assign the model parameters, we will not need to learn from data at all. The
middle ground — when we have some prior knowledge about the task, but cannot really design the
parameter vectors — is an interesting direction for future work: the three palliative alternatives outlined
in §7 do not take task-specific information into account at all. Can we do better than that, without suffering
uncomputability problems?


-----

Acknowledgments

[We thank the four reviewers for their comments, especially Reviewer NAcm for shortening the proof of](https://openreview.net/forum?id=SsPCtEY6yCl&noteId=0m2vSGF4L9)
Theorem 4. Additionally, we thank Alexandra DeLucia, Matthew Francis-Landau, Chin-Fu Liu, Suzanna
Sia, Neha Verma, and Chenghao Yang (sorted alphabetically) for discussions that improved the presentation;
and Jason Eisner, discussions with whom motivated the original exploration of this work.

References

Sushant Agarwal, Nivasini Ananthakrishnan, Shai Ben-David, Tosca Lechner, and Ruth Urner. On
learnability wih computable learners. In Aryeh Kontorovich and Gergely Neu (eds.), Proceedings of the
_31st International Conference on Algorithmic Learning Theory, volume 117 of Proceedings of Machine_
_[Learning Research, pp. 48–60. PMLR, 08 Feb–11 Feb 2020. URL https://proceedings.mlr.](https://proceedings.mlr.press/v117/agarwal20b.html)_
```
 press/v117/agarwal20b.html.

```
S. Arora and B. Barak. Computational Complexity: A Modern Approach. Cambridge University Press,
[2006. ISBN 978-0-521-42426-4. URL https://theory.cs.princeton.edu/complexity/](https://theory.cs.princeton.edu/complexity/book.pdf)
```
 book.pdf.

```
Anton Bakhtin, Yuntian Deng, Sam Gross, Myle Ott, Marc’Aurelio Ranzato, and Arthur Szlam. Residual
[energy-based models for text generation. JMLR, 22(40):1–41, 2021. URL http://jmlr.org/](http://jmlr.org/papers/v22/20-326.html)
```
 papers/v22/20-326.html.

```
Julian Bangert, S. Bratus, Rebecca Shapiro, and Sean W. Smith. The page-fault weird machine: Lessons in
instruction-less computation. In WOOT, 2013.

Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic
language model. J. Mach. Learn. Res., 3(null):1137–1155, mar 2003. ISSN 1532-4435. URL
```
 https://www.jmlr.org/papers/v3/bengio03a.html.

```
Satwik Bhattamishra, Arkil Patel, and Navin Goyal. On the computational power of Transformers and its
implications in sequence modeling. In Proceedings of the 24th Conference on Computational Natural
_Language Learning, pp. 455–475, Online, November 2020. Association for Computational Linguistics._
[doi: 10.18653/v1/2020.conll-1.37. URL https://aclanthology.org/2020.conll-1.37.](https://aclanthology.org/2020.conll-1.37)

T.L. Booth and R.A. Thompson. Applying probability measures to abstract languages. IEEE Transactions
_on Computers, C-22(5):442–450, 1973. doi: 10.1109/T-C.1973.223746._

Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan,
and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877–1901.
[Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)
```
 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.

```
Venkat Chandrasekaran, Nathan Srebro, and Prahladh Harsha. Complexity of inference in graphical models.
[In UAI, 2008. URL https://dl.acm.org/doi/10.5555/3023476.3023485.](https://dl.acm.org/doi/10.5555/3023476.3023485)

Yining Chen, Sorcha Gilroy, Andreas Maletti, Jonathan May, and Kevin Knight. Recurrent neural networks
as weighted language recognizers. In Proceedings of the 2018 Conference of the North American
_Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1_
_(Long Papers), pp. 2261–2271, New Orleans, Louisiana, June 2018. Association for Computational_
[Linguistics. doi: 10.18653/v1/N18-1205. URL https://aclanthology.org/N18-1205.](https://aclanthology.org/N18-1205)

David Chiang and Darcey Riley. Factor graph grammars. In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
[6648–6658. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/](https://proceedings.neurips.cc/paper/2020/file/49ca03822497d26a3943d5084ed59130-Paper.pdf)
```
 2020/file/49ca03822497d26a3943d5084ed59130-Paper.pdf.

```

-----

Stephen A. Cook. The complexity of theorem-proving procedures. In Proceedings of the Third Annual
_ACM Symposium on Theory of Computing, STOC ’71, pp. 151–158, New York, NY, USA, 1971._
Association for Computing Machinery. ISBN 9781450374644. doi: 10.1145/800157.805047. URL
```
 https://doi.org/10.1145/800157.805047.

```
Markus Dreyer and Jason Eisner. Graphical models over multiple strings. In Proceedings of the 2009
_Conference on Empirical Methods in Natural Language Processing, pp. 101–110, Singapore, August_
[2009. Association for Computational Linguistics. URL https://aclanthology.org/D09-1011.](https://aclanthology.org/D09-1011)

Jason Eisner. Expectation semirings: Flexible EM for finite-state transducers. In Gertjan van Noord (ed.),
_Proceedings of the ESSLLI Workshop on Finite-State Methods in Natural Language Processing (FSMNLP),_
[Helsinki, August 2001. URL http://cs.jhu.edu/~jason/papers/#eisner-2001-fsmnlp.](http://cs.jhu.edu/~jason/papers/#eisner-2001-fsmnlp)
Extended abstract (5 pages).

Jenny Rose Finkel, Alex Kleeman, and Christopher D. Manning. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL-08: HLT, pp. 959–967, Columbus, Ohio, June 2008.
[Association for Computational Linguistics. URL https://aclanthology.org/P08-1109.](https://aclanthology.org/P08-1109)

Andreas Galanis, Daniel Stefankovic, and Eric Vigoda. Inapproximability of the partition function for the
antiferromagnetic Ising and hard-core models. Combinatorics, Probability and Computing, 25:500 –
559, 2016.

Zoubin Ghahramani. Probabilistic machine learning and artificial intelligence. Nature, 521(7553):452–459,
[2015. doi: 10.1038/nature14541. URL https://doi.org/10.1038/nature14541.](https://doi.org/10.1038/nature14541)

Radu Grigore. Java generics are Turing complete. _CoRR, abs/1605.05274, 2016._ [URL http:](http://arxiv.org/abs/1605.05274)
```
 //arxiv.org/abs/1605.05274.

```
[Josh Haberman. Parsing C is literally undecidable, Aug 2013. URL https://blog.reverberate.](https://blog.reverberate.org/2013/08/parsing-c-is-literally-undecidable.html)
```
 org/2013/08/parsing-c-is-literally-undecidable.html.

```
Geoffrey E. Hinton. Training products of experts by minimizing contrastive divergence. Neural
_Comput., 14(8):1771–1800, August 2002. ISSN 0899-7667. doi: 10.1162/089976602760128018. URL_
```
 https://doi.org/10.1162/089976602760128018.

```
Y. Huang, A. Sethy, K. Audhkhasi, and B. Ramabhadran. Whole sentence neural language models. In
_[ICASSP, April 2018. doi: 10.1109/ICASSP.2018.8461734. URL https://ieeexplore.ieee.org/](https://ieeexplore.ieee.org/document/8461734)_
```
 document/8461734.

```
Thomas Jech. On Gödel’s second incompleteness theorem. Proceedings of the American Mathematical
_Society, 121(1):311–313, 1994. doi: 10.2307/2160398._

Frederick Jelinek. Interpolated estimation of Markov source parameters from sparse data. In Proc.
_Workshop on Pattern Recognition in Practice, 1980, 1980._

Mark Jerrum and Alistair Sinclair. Polynomial-time approximation algorithms for the Ising model. SIAM
_Journal on computing, 22(5):1087–1116, 1993._

S. Katz. Estimation of probabilities from sparse data for the language model component of a speech
recognizer. IEEE Transactions on Acoustics, Speech, and Signal Processing, 35(3):400–401, 1987. doi:
10.1109/TASSP.1987.1165125.

T. Kuhn, H. Niemann, and E.G. Schukat-Talamazzini. Ergodic hidden Markov models and polygrams for
language modeling. In Proceedings of ICASSP ’94. IEEE International Conference on Acoustics, Speech
_and Signal Processing, volume i, pp. I/357–I/360 vol.1, 1994. doi: 10.1109/ICASSP.1994.389282._

John D. Lafferty, Andrew McCallum, and Fernando Pereira. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In ICML, 2001.

Richard H. Lathrop. On the learnability of the uncomputable. In ICML, 1996.

Yann LeCun, Sumit Chopra, Raia Hadsell, Marc’Aurelio Ranzato, and Fu-Jie Huang. A tutorial on energybased learning. In G. Bakir,T. Hofman,B. Schölkopf,A. Smola,and B. Taskar (eds.), _Predicting Structured_
_[Data. MIT Press, 2006. URL http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf.](http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf)_


-----

[Erich L Lehmann and George Casella. Theory of point estimation. Springer, 2006. URL https:](https://link.springer.com/book/10.1007%2Fb98854)
```
 //link.springer.com/book/10.1007%2Fb98854.

```
Chu-Cheng Lin, Aaron Jaech, Xin Li, Matthew R. Gormley, and Jason Eisner. Limitations of autoregressive
models and their alternatives. In Proceedings of the 2021 Conference of the North American Chapter of
_the Association for Computational Linguistics: Human Language Technologies, pp. 5147–5173, Online,_
June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.405. URL
```
 https://aclanthology.org/2021.naacl-main.405.

```
Michael Luby and Eric Vigoda. Fast convergence of the Glauber dynamics for sampling independent sets.
_Random Structures & Algorithms, 15(3-4):229–241, 1999._

Zhuang Ma and Michael Collins. Noise contrastive estimation and negative sampling for conditional
[models: Consistency and statistical efficiency. In EMNLP, 2018. URL https://www.aclweb.org/](https://www.aclweb.org/anthology/D18-1405.pdf)
```
 anthology/D18-1405.pdf.

```
Tomas Mikolov, M. Karafiát, L. Burget, J. Cernocký, and S. Khudanpur. Recurrent neural network based
language model. In INTERSPEECH, 2010.

Art B. Owen. _Monte Carlo theory, methods and examples._ Unpublished, 2013. [URL https:](https://artowen.su.domains/mc/)
```
 //artowen.su.domains/mc/.

```
Art B. Owen and Yi Zhou. Safe and effective importance sampling. Journal of the American Statistical
_Association, 95:135–143, 2000._

Jorge Pérez, Pablo Barceló, and Javier Marinkovic. Attention is Turing-complete. Journal of Machine
_[Learning Research, 22(75):1–35, 2021. URL http://jmlr.org/papers/v22/20-302.html.](http://jmlr.org/papers/v22/20-302.html)_

Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are
[unsupervised multitask learners. Unpublished, 2019. URL https://d4mucfpksywv.cloudfront.](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)
```
 net/better-language-models/language-models.pdf.

```
Ronald Rosenfeld, Stanley Chen, and Xiaojin Zhu. Whole-sentence exponential language models: A
vehicle for linguistic-statistical integration. Computer Speech & Language, 15(1):55–73, January 2001.
[URL https://doi.org/10.1006/csla.2000.0159.](https://doi.org/10.1006/csla.2000.0159)

Ben Sandbank. Refining generative language models using discriminative learning. In EMNLP, 2008. URL
```
 https://www.aclweb.org/anthology/D08-1006.pdf.

```
H.T. Siegelmann and E.D. Sontag. On the computational power of neural nets. Journal of Computer and
_System Sciences, 50(1):132–150, 1995. ISSN 0022-0000. doi: https://doi.org/10.1006/jcss.1995.1013._
[URL https://www.sciencedirect.com/science/article/pii/S0022000085710136.](https://www.sciencedirect.com/science/article/pii/S0022000085710136)

Michael Sipser. Introduction to the Theory of Computation. Course Technology, Boston, MA, third edition,
2013. ISBN 113318779X.

Noah A. Smith and Jason Eisner. Contrastive estimation: Training log-linear models on unlabeled data. In
_Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),_
pp. 354–362, Ann Arbor, Michigan, June 2005. Association for Computational Linguistics. doi:
[10.3115/1219840.1219884. URL https://aclanthology.org/P05-1044.](https://aclanthology.org/P05-1044)

Daniel W. Stroock. An Introduction to Markov Processes, volume 230 of Graduate Texts in Mathematics.
Springer, Heidelberg, 2 edition, 2014. ISBN 978-3-642-40522-8. doi: 10.1007/978-3-642-40523-5.

A. M. Turing. On Computable Numbers, with an Application to the Entscheidungsproblem. Proceedings of
_the London Mathematical Society, s2–42(1):230–265, 01 1937. ISSN 0024-6115. doi: 10.1112/plms/_
[s2-42.1.230. URL https://doi.org/10.1112/plms/s2-42.1.230.](https://doi.org/10.1112/plms/s2-42.1.230)

L. G. Valiant. A theory of the learnable. Commun. ACM, 27(11):1134–1142, November 1984. ISSN
[0001-0782. doi: 10.1145/1968.1972. URL https://doi.org/10.1145/1968.1972.](https://doi.org/10.1145/1968.1972)

Todd L. Veldhuizen. C++ templates are Turing complete. Technical report, Indiana University Computer
[Science, 2003. URL https://rtraba.files.wordpress.com/2015/05/cppturing.pdf.](https://rtraba.files.wordpress.com/2015/05/cppturing.pdf)


-----

[Keith Wansbrough. Instance declarations are universal, 1998. URL https://www.lochan.org/](https://www.lochan.org/keith/publications/undec.html)
```
 keith/publications/undec.html.

```
Adam B. Yedidia and S. Aaronson. A relatively small Turing machine whose behavior is independent of set
theory. ArXiv, abs/1605.04343, 2016.


-----

exact asymptotic

deterministic  (Theorem 2)  (§5; but no useful guarantee in finite time)

randomized  (Theorem 4) ?; but  for rejection sampling (Theorem 5) and
 for importance sampling (Theorem 6) when
paired with autoregressive proposal distributions


Table A.1: Summary of negative results: neither deterministic or randomized algorithms can estimate EBM partition
functions accurately. On the other hand, popular sampling schemes such as rejection and importance sampling require
their autoregressive proposal distributions to be uncomputable.

A Summary of negative results

In Table A.1, we give an overview of the negative results in this paper: deficiencies of several estimators of
the partition function in an energy-based sequence model.

B Weighted Turing machines

We extend the definition of Turing machines into weighted Turing machines as follows. A weighted
Turing machine 푀 is described as a 7-tuple (푄, Σ, 훿, 푞init, 퐹, update_num, update_denom), where the
first 5 components (푄, Σ, 훿, 푞init, 퐹) have definitions following that of standard Turing machines. The
last two additions are weight-update functions: both update_num and update_denom have signature
_푄_ →A, where A ≜ {same, carry}.

At the end of time step 푖, assuming the current state is 푞, we define

num푖 = num2[푖] + num푖−1 _푖−1_ ifif update_num update_num((푞푞)) = = same carry


and similarly,


denom푖 = denom2[푖] + denom푖−1 _푖−1_ ifif update_denom update_denom((푞푞)) = = same carry.


To keep our proof of model family capacity brief, we (arbitrarily) define that ∀푞 ∈ _퐹_ ∪{푞init},
steps, we sayupdate_num[num](푞)[푟] =/denom update_denom푟 is the weight of an input(푞) = same. Finally, upon arriving at a halting state 풙 ∈ B[∗] under 푀, if [num][푟]/denom푟 is a rational number. ∈ _퐹_ in 푟 time11

C Definition of sequence model families and EC-complete parametric
families

Following Pérez et al. (2021), we say a sequence model family is a set of seq-to-seq models
**N = {푁휽** : 휽 ∈ 횯 ⊆ Q[∗]}, where every 푁휽 ∈ **N recognizes language 퐿휽** if and only if the seq-to-seq
model 푁휽, paired with embedding function 푓 : B → Q[푑], initial states 풔 ∈ Q[푑], and polytime
termination decision function 푔 : Q[푑] B, accepts every string 풙 _퐿휽. And we define 푁휽_ to
→ ∈
**accept 풙** = [푥1 . . . 푥푇 ] ∈ B[∗] if and only if there exists 푟 ∈ N, such that 푁휽 upon input embeddings
_푓_ _푥1_ _. . . 푓_ _푥푇_, produces output embeddings 풚푟 at the 푟-th time step, where 푔 _풚푟_ = 1.

[ ( ) ( )] ( )

A sequence family 횯 is EC-complete if given any ˜푝 ∈ EC (as a description of a weighted Turing machine),
we can construct a parameter vector 휽 횯 such that there exist polytime functions 푤 _푝_ : Q[푑] N 0
∈ → ∪{ }
and 푤푞 : Q[푑] N 0, where whenever on input 풙 = _푥1 . . . 푥푇_, if 푔 _풚푟_ = 1 for some output
embeddings 풚→푟 (that is, ∪{ 푁}휽 accepts 풙 in 푟 time steps), _[푤][푝]_ [(] [[풚]푟 [)]/푤푞 (풚푟 ) =] ˜푝(풙)(. )

11When [num]푟 denom푟 is not a rational number (because denom푟 = 0), we say the weight of input string 풙 is
/
undefined. However, we do not encounter that in this work: all strings in an EC language have rational weights. In any
case, we only require that our family contain weighted Turing machines that return rational numbers, not that they be
the only members.


-----

D Proofs of theorems from main text

**Theorem 1. The class of one-encoder-layer, four-decoder-layer Transformer networks with positional**
_encodings (푛,_ [1]/푛, [1]/푛[2], 2[푛]) is EC-complete.

_Proof. To model all weighted Turing machines defined in §2.2, we extend the Turing-complete one-_
encoder-layer three-decoder-layer Transformer network construction introduced by Pérez et al. (2021) with
one additional layer. We also modify the original positional embedding function pos : N → Q[푑] to include
a new component. In this proof, we let

pos(푖) = [0, . . ., 0, 1, 푖, [1]/푖, [1]/푖[2], 2[푖]]

instead of pos(푖) = [0, . . ., 0, 1, 푖, [1]/푖, [1]/푖[2]] as in Pérez et al. (2021).

For the sake of clarity, our construction is a ‘stack-on’ construction: the encoder layer and the first 3 decoder
layers are largely identical to the design of Pérez et al. (2021), with the only difference being necessary
changes to accommodate our one additional positional embeddings component.12 It may be possible to
strengthen our results by showing that one-encoder-layer three-decoder-layer Transformer networks
with the original positional embeddings — the parametrization family Pérez et al. (2021) showed to
be Turing-complete — are EC-complete as well, with a more involved construction. We leave such an
improvement as future work.

We claim our fourth layer of the decoder has output 풚푟 = 풚푟[′] [+][ 풘][푟] [, where]

-  풚푟[′] [is a zero-padded version of the original Transformer output embeddings from][ Pérez et al.]

(2021). 풚푟[′] [is of the form]

[ _푞[푟]_ _,_ _푠[푟]_ _, 푚_ [(][푟] [−][1][)] _, 0, . . ., 0]_

where _푞[푟]_ denotes an one-hot vector of size _푄_ where the 푞[푟] -th component is 1 (again following

J K J K | |

the notation of Pérez et al. (2021)).

-  And 풘푟 is of the form

J K



[0푞, 0푠, 0, num푟 _, denom푟_ _, 0, . . ., 0]_ (3)

where num푟 N 0 and denom푟 N 0 are defined in §2.2.
∈ ∪{ } ∈ ∪{ }

Now we describe how 풘푟 can be computed from [풚0[′] _[. . .][ 풚]푟[′]_ []][ using the attention mechanism, with the help]
of our new positional embeddings. Specifically, we want to show that we can construct feedforward
networks 푄4, 퐾4, and 푉4 such that

_풚푖_ = Att[ ]푄4 _풚푖[′′][)][, 퐾][4][(][Y]푖[′′][)][,푉][4]_ [(][Y]푖[′′][)][]
(

where 풚푖[′′] = _풚푖[′]_ [+][ pos][(][푖][)][,][ Y][′] = [풚1[′] _[. . .][ 풚]푖[′][]][, and][ Y]푖[′′]_ = Y[′] + [pos(1), . . ., pos(푖)]. We let
_푄4_ ( _풚[′′]) = [0, . . ., 0, 1, 0, 0, 0, 0] be a constant vector, and 퐾4(Y푖[′′][)][ =][ Y]푖[′′][. Finally, we let][ 푉][4][(][Y]푖[′′][)][ =]_

[0푞, 0푠, 0, I(update_num(푞[푖]) = carry)2[푖], I(update_denom(푞[푖]) = carry)2[푖], 0, . . ., 0].

_푄4 is a constant function (such that it always attends to the unity component of Y푖[′′][), so it can be]_
implemented as a single-layer feedforward network. 퐾4 is the identity function, which can also be
implemented as a single-layer feedforward network. 푉4 on the hand can be implemented as a fixed-size
feedforward network, with the piecewise-linear sigmoidal function 휎: in the case of denom푖, the network
would first project 푞[푖] to 풂 = [I(update_denom(푞[푖]) = carry), I(update_denom(푞[푖]) = same)] (using
the one-hot _푞[푖]_ segment from 풚푖[′][), multiply it by][ 풃] [=][ [][2][푖][,][ 0][]][ (with the help of nonlinearity from][ 휎][), and]
put 풂 _풃_ at the position of denom푖 in equation (3). The component at num푖 in equation (3) can be
⊗
computed likewise.
J K

12Since we increase the output embeddings’ dimension by 1, we also need to pad all matrices in the original
construction by additional zero columns/rows, such that our new positional embeddings’ new component has no effect
on any computation in the encoder layer, and the first 3 decoder layers.


-----

Given any position 푟 ∈ N, we have

_풚푟_ = Att(푄4( _풚푟[′′][)][, 퐾][4]_ [(][Y]푟[′′][)][,푉][4][(][Y]푟[′′][))][ =][ [][0][푞][,]
**0푠,**
0,

1 _푟_

I `update_num` _푞[푖]_ = carry 2[푖],

_푟_ ( ( ) )

_푖=1_

Õ

1 _푟_

I `update_denom` _푞[푖]_ = carry 2[푖],

_푟_ ( ( ) )

_푖=1_

Õ

0,
_. . .,_
0].

Let extract_avg_num _풚푟_ be an affine transformation that extracts the _푄_ Σ 2[] [nd] component
( ) | | + | | +
from 풚푟, and extract_avg_denom _풚푟_ be an affine transformation that extracts the _푄_ Σ 3[] [rd]
( ) | | + | | +
component from 풚푟, we have [ ]

`extract_avg_num` _풚푟_ _푟푖=1_ [I][(][update_num][(][푞][푖][)][ =][ carry][)][2][푖] [ ]

`extract_avg_denom(` _풚)푟_ [=] _푟푖=1_ [I][(][update_denom][(][푞][푖][)][ =][ carry][)][2][푖]
( ) Í

= Ínum푟

denom푟

which is the weight of input 풙 = [푥1 . . . 푥푇 ] as we defined in §2.2. □

**Theorem 2. Let 횯** _be a parametric EC-complete sequence model family. And let 횯푘_ ⊂ 횯 _be bĳective_
_with G푘_ _. There is no 푘_ ∈ Q>0 for which there exists an exact estimator _푍[ˆ]_ _푘_ _that takes as input 휽_ ∈ 횯푘 _as_
_input, and computes_ _푍[ˆ]_ _푘_ (휽) = 푍 _푝˜휽_ _[in finite time.]_

_Proof. We can reduce Halt to computing our partition function. For the sake of contradiction, let us_
assume that for some 푘 ∈ Q>0, the exact estimator _푍[ˆ]_ _푘_ exists. Our reduction from Halt of input-free Turing
machines is as follows: Given any deterministic input-free Turing machine 푀, we build a weighted
deterministic Turing machine: 푀 [′] (Appendix B). 푀 [′] takes as input 풙 ∈ B[∗], and outputs weight [1]/3 [|][풙] [|+][1] + 푘
when 풙 encodes an accepting trace of 푀 [′]. Otherwise, 푀 [′] outputs weight [1]/3 [|][풙] [|+][1].

_푀_ [′] always returns a weight for 풙 in polytime. By the assumptions of EC-complete families (§2.4), we
can build a parameter vector 휽 횯 such that ˜푝 _푀_ [′] = ˜푝휽. Since from the definitions of _푘_ we know
∈ G
_푝˜푀_ [′] = ˜푝 _푀,푘_, we have 휽 횯푘 .
∈

We have thus completed our reduction: if 푍[ˆ] _푘_ existed, we could decide whether any given input-less
deterministic Turing machine 푀 halts, by first constructing the weighted Turing machine 푀 [′], then the
corresponding 휽. By our assumption, _푍[ˆ]_ _푘_ ˜푝휽 = 푘 1 if and only if _풙_ B[∗] that is an accepting path of
( ) + ∃ ∈
_푀_ [′], which is true if and only if 푀 halts for some finite steps. Since whether 푀 halts is undecidable, we
have arrived at a contradiction (Turing, 1937; Sipser, 2013). Therefore for all 푘 Q, the algorithm _푍[ˆ]_ _푘_ does
∈
not exist. □

**Theorem 3. Assuming ZFC axioms and that assuming they is consistent, there exists an EC weighted**
_language_ _푏[˜]_ ∈G1 such that (a) 푍푏˜ [=][ 푐] [∈{][1][,][ 2][}][ exists, but (b) there is no proof of][ 푍]푏[˜] [=][ 푐][.]

_Proof. Our proof hinges on Gödel’s second incompleteness theorem: no consistent axiomatic system_
which includes Peano arithmetic (e.g., ZFC) can prove its own consistency. We construct a Turing machine
_푀푏_ that enumerates all provable propositions under ZF, and halts if and only if 푀푏 proves the proposition
1 = 0. One such 푀푏 with 7, 918 states has been built by Yedidia & Aaronson (2016).13

_푀푏_ is an input-free deterministic Turing machine. We construct a weighted Turing machine 푀 [′] from 푀푏,

13Subsequent efforts have led to a construction of 푀푏 [with 748 states: https://turingmachinesimulator.](https://turingmachinesimulator.com/shared/vgimygpuwi)
```
com/shared/vgimygpuwi.

```

-----

in the manner of the proof of Theorem 2. We let 푀 [′] return weight 1 [1] 3[|][풙] [|+][1], in the case 푀푏 halts with
+ /
traceknow from the definitions of 풙. 푀 [′] returns a weight in polytime, and therefore defines a weighted language G1 that 푍푏˜ [is either 1 or 2.] _푏[˜]_ ∈G1 ⊂ EC. We

Assume to the contrary that there exists a proof that 푍푏˜ [=][ 1. Then we know][ 푀][푏] [did not halt. And therefore]
the proof would also imply that ZFC is consistent, which violates Gödel’s second incompleteness theorem.
On the other hand, if there were a proof that 푍푏˜ [=][ 2, it would imply][ 푀][푏] [halted and ZFC is not consistent.]
We therefore arrive at a contradiction. □

**Lemma 1. Let 횯** _be an EC-complete parametric family. There is no multiplicative factor 휖_ ∈ Q>1 for
_which every 휽_ ∈ 횯 _can have its partition function approximated with_ **Z[ˆ]** _휖_ (휽) within a factor of 휖 _— with_
_probability greater than_ [2]/3. That is, we cannot have

_푃_ _휖_ _푍_ _푝˜휽_ [≤] **Z[ˆ]** _휖_ _휽_ _휖푍_ _푝˜휽_ _>_ [2] 3. (2)
([1]/ ) ( ) ≤ /
 

_Proof. In this proof, we make use of the distribution family_ _휖_ 2 (§3.1). We assume to the contrary
G
that a multiplicative bound 휖 satisfying equation (2) exists. Recall that our assumptions state that
_푃_ [ ]1/휖 ≤ **Z[ˆ]** _휖_ (휽)/푍푝˜휽 [≤] _[휖]_ [][ >][ 2][/][3][.][14][ Let][ 푀] [be an input-free Turing machine. Andlet][ ˜]푝휽 = ˜푝 _푀,휖_ 2 ∈G휖 2 where

_휽_ 횯. Ifthe Turing machine 푀 halts, _푍_ _푝˜_ _푀,휖_ 2 [=][ 1][+][휖] [2][; therefore,] _[푃]_ 1 _휖_ [2] _휖_ **Z휖** _휽_ 1 _휖_ [2] _휖_ _>_
∈ ( + )[1]/ ≤ **[ˆ]** ( ) ≤( + )

2 3. Similarly, if 푀 does not halt, we have 푍 _푝˜_ _푀,휖_ 2 [=][ 1; therefore,] _[ 푃]_ 1 _휖_ **Zˆ** _휖_ _휽_ _휖_ _>_ [2] 3. By
/ / ≤ ( ) ≤ /

combining the two conditions, we know that 푃 (I(푀 halts)) = 푃 I(Z[ˆ] _휖_ (휽) ≥ _휖_ + [1]/휖 ∧ **Z[ˆ]** _휖_ (휽) > 휖) =

_푃_ I(Z[ˆ] _휖_ (휽) ≥ _휖_ + [1]/휖 ) .  
 

Therefore, given _휖_ _푍_ _푝˜휽_ [≤] **Z[ˆ]** _휖_ _휽_ _휖푍_ _푝˜휽_ [, we can decide if][ 푀] [halts by checking if][ ˆ]Z휖 _휽_ _휖_ [1] _휖_ .
([1]/ ) ( ) ≤ ( ) ≥ + /
Since the condition ([1]/휖 ) 푍 _푝˜휽_ [≤] **Z[ˆ]** _휖_ (휽) ≤ _휖푍_ _푝˜휽_ [only holds][ 2][/][3][ of all time, we then derandomize the]
randomized **Z[ˆ]** _휖_ to get a deterministic estimator: recall that a randomized exact estimator finishes
computation in finite time, regardless of the content of the random tape 휏. There, there are only finitely many
finitely long possible ‘random’ sequences that _푍[ˆ]_ _휖_ will read, which we can enumerate. More concretely:

_푃_ [ ][ ˆ]Z휖 _휽_ _휖_ [1] _휖_ [] = E I _푍[ˆ]_ _휖_ _휽, 휏_ _휖_ [1] _휖_
( ) ≥ + / _휏_ ( ( ) ≥ + / )

=  1 2[푚]휽,휖 I ˆ푍 _휖_ _휽, 휏_  _휖_ [1] _휖_ (4)

/ ( ( ) ≥ + / )

_휏_ B[푚][휽][,휖]
∈Õ

where 푚휽,휖 N is the maximum prefix length of the random tape that _푍[ˆ]_ _휖_ _휽, 휏_ will use on any 휏 B[N].
∈ ( ) ∈
Again 푚휽,휖 is guaranteed to exist because of our assumption _푍[ˆ]_ _휖_ _휽, 휏_ ends in finite time. Since equation (4)
( )

is a finite sum of computable functions, it is also computable.

From the computability of equation (4), it follows that we can derive a deterministic algorithm from **Z[ˆ]** _휖_
that decides whether an arbitrary input-free Turing machine halts, which is not possible. Therefore,
there is no푃([1]/휖 ≤ **Z[ˆ]** _휖 휖(퐺∈)/푍Q≤>1휖, such that the randomized exact estimator) > 2/3 for all 휽_ ∈ 횯. **Z[ˆ]** _휖_ satisfies the multiplicative bound□

**Theorem 4.that there exists a randomized exact estimator Let 횯** _be an EC-complete parametric family. There is no multiplicative boundZ[ˆ]_ _휖_ _that guarantees_ [1]/휖 ≤ [E][ [][ ˆ]Z휖 (휽)]/푍푝˜휽 [≤] 휖 _[휖]∈[, for every]Q>1 such_
_휽_ 횯 _where ˜푝휽_ _is normalizable._
∈

_Proof. We define 휏_ to be distributed according 푝 _휏. Therefore the mean E_ [Z[ˆ] _휖_ (휽)] can be expanded as
E휏∼ _푝휏_ [Z[ˆ] _휖_ (휽)] = _휏_ ∈B[N][ 푝] _휏_ [(][휏][)][ ˆ]푍 _휖_ (휽, 휏). Following the same derandomization technique in the proof of

Lemma 1, we can find some 푚 ∈ N such that _휏_ ∈B[N][ 푝] _휏_ [(][휏][)][ ˆ]푍 _휖_ (휽, 휏) = _휏_ ∈B[푚] [1][/][2][푚]푍[ˆ] _휖_ (휽, 휏) in finite
time. And subsequently, we can compute[Í] E **Z[ˆ]** _휖_ _휽_ exactly in finite time. Let the exact estimator be 푍 _휖_ .

[ ( )]
We then have 푍 _휖_ _휽_ = E **Z[ˆ]** _휖_ _휽_ . [Í] [Í]
( ) [ ( )]

14As is common, e.g., in defining the complexity class BPP (see Arora & Barak 2006), the fraction [2]/3 is arbitrary.
Any proportion bounded away from [1]/2 will work.


-----

Assuming to the contrary that we could guarantee [1] _휖_ _푍_ _휖_ _휽_ _휖. Since 푍_ _휖_ is a deterministic estimator,
/ ≤ ( ) ≤
we can write 푃 _휖_ _푍_ _휖_ _휽_ _휖_ = 1, which contradicts Lemma 1. Therefore such an estimator 푍 _휖_ _휽_
([1]/ ≤ ( ) ≤ ) ( )
does not exist. □

**Theorem 5. Using ZFC as our axiom set and assuming they are consistent, then there exists a normalizable**
EC weighted language ˜푝, where there does not exist a consistent locally normalized proposal distribution
_푞_ ∈ LN, and 푐푞 ∈ Q>0, such that it can be proven ∀풙 ∈ B[∗], _푝[˜]_ (풙)/푞 (풙) < 푐푞.

where it is proven thatProof. By contradiction; let ∀풙 ∈ ˜푝B=[∗], _푏[˜]푝[˜], where(풙)/푞_ (풙) < 푐<푏[˜] is first introduced in ∞, where 푐 ∈ §3.2Q>0.. Assume that there exists 푞 ∈ LN

We can either prove ZFC is consistent, or is inconsistent, as follows:

1. By our assumptions, 푞 ∈ LN scores every string 풙 as 푞휽 (풙) 푓 (휽) ∈ Q>0. Also, because 푞 is
consistent and locally normalized, there exists 푛 N such that we can prove _풙_ _>푛_ _풙_ : 풙
∈ ∀ ∈X { ∈
B[∗], |풙| > 푛}, 푞(풙) < [1]/푐 (Proposition 1). We will just prove the existence of 푛 constructively by
enumerating the strings in B[∗] in increasing length. Let the complement set X≤푛 = B[∗] −X>푛.

2. The proof then examines the finitely many strings in X≤푛.

(a) If any of these strings 풙 [′] has _푏[˜]_ _풙_ [′] _> 1, then we know 풙_ [′] encodes an accepting trace of 푀푏
( )
(§3.2). Therefore, 푀푏 halts, which implies there is a proof of the inconsistency of ZFC.

(b) If none of these stringssuch that _푏[˜]_ (풙 [′′]) > 1. This is because of our assumption ∈X≤푛 has _푏[˜]_ (풙) > 1, then we know there is also no string 푐 ≥ _푏[˜]_ (풙)/푞 (풙), which in turn means 풙 [′′] ∈X>푛,
that 푏[˜] _풙_ is less than 1 on these long strings _>푛. Therefore, 푀푏_ does not halt, which
( ) X
implies there is a proof of the consistency of ZFC.


Assuming ZFC is consistent, neither a proof of ZFC, or a disproof of ZFC, is possible. We have therefore
therefore arrived at a contradiction. Therefore, 푞 does not exist. □

**Theorem 6. Let 횯** _be an EC-complete parametric family. Assuming ZFC axioms and assuming they are_
_consistent, there exists 휽_ ∈ 횯 _where there does not exist a consistent locally normalized “proposal”_
_distribution 푞_ LN such that it can be proven Var푞 **Zˆ** **_휽[푞],푁_** _< 푐_ ≠ _, where 푐_ Q>0.
∈ ∞ ∈
 

_Proof. let ˜푝_ = _푏[˜]_, where _푏[˜]_ is first introduced in §3.2. Assume that there exists 푞 ∈ LN where it is proven
that Var푞 **Z[ˆ]** **_휽[푞],푁_** [)][ =][ 푘< 푐] [∈] [Q][<][0][ ≠] [∞][.]
(

We have


Var푞 **Z[ˆ]** **_휽[푞],푁_** [)]
(

≜ [1]

_푁_

( Õ풙 ∈B[∗]

_푍_ [2]푝˜휽

Õ



[)]

2

˜푝휽 _풙_
( ) _푞_ _풙_

_푞_ _풙_ ( )

∈B[∗] " ( ) 

_푝휽[2]_ [(][풙][)]

_푞_ _풙_

**_풙_** B[∗] !

Õ∈ ( ) [−] [1]


_푍_ _푝˜휽_
−


= 푘,


where 푝휽 (풙) ≜ _푝˜푍휽_ _푝(˜휽풙)_ [. After some manipulation, we have]


_푝휽[2]_ [(][풙][)] _푝휽_ 2

_푞_ _풙_ [=][ 푁푘]푍[+]푝˜[ 푍]휽 2 [˜]
( )

=푠 ≤푁푘+1
| {z }


**_풙_** ∈B[∗]


_푝휽[2]_ [(][풙][)]
Since _풙_ B[∗], _푞_ **_풙_**
∀ ∈ ( ) [≥] [0, we have]


_푝휽[2]_ [(][풙][)]
_풙_ B[∗], (5)
∀ ∈ _푞_ _풙_

( ) [≤] _[푠][;]_


-----

in particular, if 풙 [′] encodes a halting trace of 푀푏, from §3.1 we know

_푝휽_ _풙_ [′]
_푝휽_ _풙_ [′] = [˜] ( )
( ) _푍_ _푝˜휽_

1 31 |풙′ |+1
+
=
 2


_>_ [1]/2. (6)

Combining equations (5) and (6), we have for any halting trace 풙 [′] of 푀푏,

1 _푝휽[2]_ [(][풙] [′][)]

4푞(풙 [′]) _[<]_ _푞(풙_ [′]) [≤] _[푠.]_


After some more arrangement,

_푞_ _풙_ [′]
( ) ≥ 4[1]푠 [≥]


1

4(푁푐 + 1) _[>][ 0.]_


4(푁푘 + 1) [≥]


As in the proof of Theorem 5, the existence of such a 푞 allows us to either prove or disprove the consistency
of ZFC. For the sake of completeness, we include a proof sketch below:

1. By our assumptions, 푞 ∈ LN scores every string 풙 as 푞휽 (풙) 푓 (휽) ∈ Q>0. Also, because 푞 is
consistent and locally normalized, there exists 푛 N such that we can prove _풙_ _>푛_ _풙_ :
1 ∈ ∀ ∈X {
_풙_ ∈ B[∗], |풙| > 푛}, 푞(풙) < 4( _푁푐+1)_ [(][Proposition 1][). We will just prove the existence of][ 푛]

constructively by enumerating the strings in B[∗] in increasing length. Let the complement set
X≤푛 = B[∗] −X>푛.

2. The proof then examines the finitely many strings in X≤푛.

(a) If any of these strings 풙 [′] has ˜푝휽 _풙_ [′] _> 1, then we know 풙_ [′] encodes an accepting trace of
( )
_푀푏_ (§3.2). Therefore, 푀푏 halts, which implies there is a proof of the inconsistency of ZFC.

(b) If none of these strings ∈X≤푛 has ˜푝휽 (풙) > 1, then we know there is also no string
_풙_ [′′] _>푛, such that ˜푝휽_ _풙_ [′′] _> 1, since ˜푝휽_ _풙_ [′] _> 1_ _푏_ _풙_ [′] _> 1_ _푀푏_ halts;
∈X ( ) ( ) ⇐⇒ [˜] ( ) ⇐⇒
and we have already shown that for any accepting trace 풙 [′] of 푀푏, 푞(풙 [′]) ≥ [1]/4( _푁푐+1)._
Therefore, 푀푏 does not halt, which implies there is a proof of the consistency of ZFC.


Assuming ZFC is consistent, neither a proof of ZFC, or a disproof of ZFC, is possible. We have therefore
therefore arrived at a contradiction. Therefore, such a 푞 does not exist. □

**Theorem 7.and decides whether Let 횯** _be an ˜푝휽1 and EC-complete family. There is no algorithm that takes ˜푝휽2 are the same weighted language._ _휽_ 1 ∈ 횯, 휽 2 ∈ 횯 _as input,_

_Proof. Assuming to the contrary that such an algorithm Distinct exists. We would be able to reduce Halt_
of input-free Turing machines to Distinct: we first construct 휽 1 횯 such that ˜푝휽1 _풙_ = [1] 3 [|][풙] [|+][1]. We then
construct 휽 2 ∈ 횯 such that ˜푝휽2 = ˜푝 _푀,1 ∈G1 (§3.1). Distinct ∈(휽_ 1, 휽 2) is YES if and only if( ) / _푀_ halts. □

**Theorem 8. For any ˜푝** ∈ EC and for any EC-complete family 횯, there is no algorithm Better ˜푝 _[that]_
_takes two parameter vectors 휽_ 1 횯, 휽 2 횯, and returns YES if KL _푝_ _푝휽1_ KL _푝_ _푝휽2_ _, and NO_
_otherwise, where 푝, 푝휽1, 푝휽2 are string distributions defined by ∈_ ∈ ˜푝, ˜푝휽(1, ˜||푝휽2 respectively.) ≥ ( || )

_Proof. By contradiction; assume that for some ˜푝_ ∈ EC, Better ˜푝 [exists. We know there exists a weighted]
Turing machine (also denoted as ˜푝) that on any string 풙, terminates in 푂 (poly(|풙|)) and outputs ˜푝(풙)
(Appendix B).

We show how we can reduce from Halt to Better ˜푝[. Given any arbitrary input-less Turing machine][ 푀][,]
we can define a new weighted Turing machine ˜푝[′]푀 [that on input string][ 풙][,][ ˜]푝[′]푀 [first simulates][ ˜]푝 on it and
keeps a record of ˜푝(풙) somewhere on the tape. ˜푝[′]푀 [then verifies whether][ 풙] [is an encoded accepting trace]
of 푀. If 풙 is indeed an encoded accepted trace of 푀, ˜푝[′]푀 [outputs][ ˜]푝(풙) + 1. Otherwise ˜푝[′]푀 [outputs][ ˜]푝(풙).

Let푝˜(풙 휽),1 ∀ ∈풙 ∈횯Bparametrize ˜[∗]. _푝[′]푀_ [, and let][ 휽] [2][ ∈] [횯] [parametrize ˜]푝, such that ˜푝휽1 (풙) = ˜푝[′]푀 [(][풙][)][,][ ˜]푝휽2 (풙) =


-----

We know that KL( _푝||_ _푝) = 0 for any distribution 푝. better ˜푝_ [(][휽] 1[,][ 휽] 2[)][ returns YES if and only if]
KL( _푝||_ _푝휽1_ ) = 0 ≥ KL( _푝||_ _푝휽2), which implies that ˜푝휽2 and ˜푝휽1 define the same distribution, which in_
turn implies that ∀풙 ∈ B[∗], ˜푝(풙) = ˜푝[′]푀 [(][풙][)][, and that][ 푀] [never halts. Similarly,][ Better][ ˜]푝 [(][휽] 1[,][ 휽] 2[)][ returns]
NO if and only if 푀 halts. We have thus completed our reduction from Halt; and therefore algorithm
Better ˜푝 [does not exist for all ˜]푝 ∈ EC. □

**Theorem 9. Let 푝** _be any LN weighted language. Any ˜푝_ ∈ EC where ∀풙 ∈ _푉_ [∗], ˜푝(풙) ≤ _푝(풙) has a_
_computable 푍_ _푝˜_ _[.]_

_Proof. We prove Theorem 9 by constructing an algorithm_ _푍[ˆ]_ _푝˜_ [:][ Q]>0 0 [that approximates][ 푍] _푝˜_ [within]

[→] [Q]≥
any desired positive rational error 휖: namely |푍[ˆ] _푝˜_ [(][휖][) −] _[푍]_ _푝˜_ [| ≤] _[휖][.]_

Let _푍[ˆ]푛_ = _ℓ=0_ **_풙:|풙_** |=ℓ _[푝][(][풙][)][. We first observe that][ lim]푛→∞_ _[푍]푛_ [=][ 1 by][ Proposition 1][. In other words,]
lim푛→∞ 1 − _푍[ˆ]푛Í= 0, or equivalently, given any 휖> 0, there exists 푛_ ∈ N such that for all 푛[′] ≥ _푛, 푛[′]_ ∈ N,
1 _푍푛[′]_ _< 휖[Í][푛]_ .
( − [ˆ] )

Therefore, given any 휖> 0, ∃푛 ∈ N that divides B[∗] in two sets: X≥푛 = {풙 : 풙 ∈ B[∗], |풙| ≥ _푛}, where_
**_풙_** _푛_ _[푝][(][풙][)][ < 휖][, and][ X]<푛_ [=][ B][∗] [−X] _푛[. We are guaranteed to find][ 푛]_ [by enumerating all candidates from]
∈X≥ ≥
N.
Í

We can thus implement _푍[ˆ]_ _푝˜_ [as the following program: given][ 휖] [∈] [Q]>0[, we first find the smallest][ 푛] [∈] [N][,]
and partition푍ˆ _푝˜_ [(][휖][)][ =][ Í]풙 B<푛[∗] into two sets푝[˜] _풙_ . X<푛, X≥푛 as we described in the previous paragraph. We then return
∈X ( )

We argue that _푍[ˆ]_ _푝˜_ [is a computable function. We first repeat that since][ 푛] [∈] [N][ exists, we will find][ 푛] [in finite]
time, simply by enumerating. And since the set _<푛_ B[∗] is finite, _푍[ˆ]_ _푝˜_ [(][휖][)][ =][ Í]풙 _<푛_ _푝[˜]_ _풙_ can computed
X ⊂ ∈X ( )
in finite time (under our assumption ˜푝 ∈ EC, ∀풙 ∈ B[∗], ˜푝(풙) can be computed in time 푂 (poly(|풙|))).
Since the program we described above terminates in finite time, 푍[ˆ] _푝˜_ [is a computable function.]

What remains is to show that the approximation error |푍 _푝˜_ [−] _푍[ˆ]_ _푝˜_ [(][휖][)|][ is no greater than][ 휖][;][ i.e.][, that]
|푍 _푝˜_ [−] _푍[ˆ]_ _푝˜_ [(][휖][)| ≤] _[휖][.]_

It is easy to show that 0 ≤ _푍_ _푝˜_ [−] _푍[ˆ]_ _푝˜_ [(][휖][)][, after which we only need to show that][ 푍] _푝˜_ [−] _푍[ˆ]_ _푝˜_ [(][휖][) ≤] _[휖][.]_
Expressing both terms as sums, we have that **_풙_** B[∗] _푝[˜]_ _풙_ **_풙_** _<푛_ _푝[˜]_ _풙_ = **_풙_** _푛_ _푝[˜]_ _풙_, which is a
∈ ( ) − [Í] ∈X ( ) ∈X≥ ( )
sum of nonnegative terms and thus nonnegative.

To show that 푍 _푝˜_ [−] _푍[ˆ]_ _푝˜_ [(][휖][) ≤] _[휖][, we express both terms as sums again:][Í]_ [Í]

_푝˜(풙) −_ _푝˜(풙) =_ _푝˜(풙)._
**_풙Õ∈B[∗]_** **_풙Õ∈X<푛_** **_풙Õ∈X≥푛_**

By the definition of ˜푝, we have **_풙_** _푛_ _푝[˜]_ _풙_ **_풙_** _푛_ _[푝][(][풙][)][. The right-hand side is equal to 1][ −]_ _푍[ˆ]푛,_
∈X≥ ( ) ≤ [Í] ∈X≥
which by construction is less than 휖. Therefore, by substituting to get 푍 _푝˜_ [−] _푍[ˆ]_ _푝˜_ [(][휖][) ≤] [1][ −] _푍[ˆ]푛_ and using the
transitive property of inequality, we have that[Í] _푍_ _푝˜_ [−] _푍[ˆ]_ _푝˜_ [(][휖][)][ < 휖][. Combined with the previous paragraph’s]
result, we have shown that |푍 _푝˜_ [−] _푍[ˆ]_ _푝˜_ [(][휖][)| ≤] _[휖][.]_

□

E Connection between expressive sequence distributions and Lin et al.
(2021)’s proof that EC ≠ ELN

Our construction of expressive sequence distributions is similar to Lin et al. (2021)’s construction of an EC
weighted language that is not in ELN. Whereas each weighted machine ˜푝 _푀,푘_ _푘_ corresponds to
∈G
execution traces of a deterministic Turing machine 푀, Lin et al. (2021) defined a single weighted language
_푝˜ELN for all Turing machines. Fore the sake of completeness, we describe the definition of ˜푝ELN from Lin_
et al. (2021) below. Let enc be a prefix-free encoding function that maps a deterministic Turing machine 푀
to a Boolean string where the function inverse enc[−][1] exists. And let ˜푝ELN be a weighted language over
Boolean strings, where ˜푝ELN (풙) = [1]/3[|][풙][|+][1] if 풙 is of the form 풙 [(][1][)] _풙_ [(][2][)], where 풙 [(][1][)] encodes some Turing
machine 푀, and 풙 [(][2][)] encodes an accepting execution path of 푀 on an empty input. (Such a path may be


-----

represented as a sequence of transitions of 푀 that begins with an initial state and ends at an accepting
state.) Lin et al. (2021) then showed that prefix probabilities of ˜푝ELN are uncomputable under an ELN,
since Halt reduces to computing 푍 (풙 [(][1][)] ).

Theorem 9 implies that ˜푝ELN we have just described above has a computable partition function:

**Theorem 10. Let 푍** _be the partition function of ˜푝ELN. 푍_ _is computable._

_Proof. Let 푝(풙) =_ [1]/3[|][풙][|+][1]. 푝 ∈ ELN ⊂ LN because 푝(· | ˆ풙) = [1]/3 for all valid prefixes ˆ풙. Since

Theorem 9푝˜ELN ∈ EC. by Lin et al. (2021, Theorem 5) and ∀풙 ∈ B[∗], ˜푝ELN (풙) ≤ _푝(풙), we have 푍_ is computable by□

Similarly, the ‘sparse version’ of ˜푝ELN introduced in (Lin et al., 2021, Theorem 6) can be shown to have a
computable partition function as well.

Since one of our goals is to clearly demonstrate that we can construct a single weighted language ∈ EC that
has an uncomputable partition function assuming ZFC (e.g., _푏[˜]_ in §3.2), we define weighted languages in
_푘_ to have at most one ‘high’ weight, as opposed to ˜푝 in Lin et al. (2021, Theorem 5), where each 풙 [(][1][)] that
G
encodes a halting machine has a ‘high weight’ suffix 풙 [(][2][)] .

In fact, under our construction we can directly show EC ≠ ELN, using the uncomputability of _푏[˜]_ from

Theorem 3:

**Corollary 1 (EC ≠** ELN under ZFC; slightly weaker version of Theorem 5 in Lin et al. (2021)). Assuming
_the axiomatic system of ZFC, EC ≠_ ELN.

_Proof.function. Since We know there exists a weighted language푏[˜]_ EC, 푏[˜] _풙_ Q 0, _풙_ B[∗]. Therefore for all strings푏[˜] ∈ EC (§3.2) that has an uncomputable partition 풙 B[∗], 푏 _풙_ = _푏[˜]_ (풙) _푍푏˜_ [is an]
∈ ( ) ∈ ≥ ∀ ∈ ∈ ( ) /
uncomputable number.

Assuming to the contrary that _푏[˜]_ ELN. By definition 푏 _풙_ = _푏_ _푥푡_ _풙<푡_, where each 푏 _푥푡_ _풙<푡_
∈ ( ) ( | ) ( | ) ∈
Q≥⊬ and is computable. Since the set of computable numbers is closed under multiplication, 푏(풙) would
also be computable, which contradicts our assumption. Therefore _푏[˜]_ ≠ ELN, which implies EC ≠ ELN. □

[Î]

F Negative results possible on finite parameter subspaces

In §3.2, we mention that the pathological EBM _푏[˜]_ can show negative results regarding finite subsets of 횯.
Here, we provide a concrete example.

**Corollary 2. Assuming ZFC axioms and assuming they are consistent, there exists 휽** ∈ 횯 _such that_
_푍_ _푝˜휽_ [∈] [Q]>0 _[exists, but there is no algorithm][ ˆ]푍proof that takes 휽_ ∈ 횯 _as input, and outputs a set of strings_
_풙푛_ : 푛< 푁 _, 푁_ N where
{ } ∈

-  there is a proof that _푛=1_ _푝[˜]휽_ (풙) ≥ [1]/2푍 _푝˜휽_ _[; or]_

-  there is a proof that _푛=1_ _푝[˜]휽_ (풙) < [1]/2푍 _푝˜휽_ _[.]_

[Í][푁]

_Proof. Assuming to the contrary that[Í][푁]_ _푍[ˆ]proof existed. We construct 휽_ ∈ 횯 such that ∀풙 ∈ B[∗], ˜푝휽 (풙) = _푏[˜]_ (풙).
Either proof resulting from _푍[ˆ]proof_ (휽): {풙푛 : 푛< 푁 } can be used to prove or disprove the consistency of
ZFC. □

Corollary 2 states that there exists a ‘problematic EBM’ — namely _푏[˜]_ — where we cannot guarantee to well
approximate its partition function, by accumulating finitely many string weights, regardless of the manner
of accumulation (i.e., how we choose strings) or the number of strings we enumerate over. We discuss this
in further details in §2.5.

G Impossibility of model selection in fixed-size Transformer EBM families

Theorem 11 is a sibling theorem to Theorem 8. Just as we show 푍푏˜ [is uncomputable (][§3.2][), we can prove]
that model selection is not only impossible for EC-complete parametric families (where the length of a
parameter vector is unbounded), but also impossible for fixed-size Transformer EBMs with a large enough


-----

embedding size.15

**Theorem 11. Assuming ZFC as our axiomatic system, for any ˜푝** ∈ EC, there exists 푑0 ∈ N such that for
_all 푑_ ≥ _푑0, ˜푝_ _can be captured by one-encoder-layer four-decoder-layer Transformer networks 횯[(][푑][)]_

_(Theorem 1) with embedding size 푑, where there is no provably correct algorithm Better ˜푝,푑_ _[that takes]_
_two parameter vectors 휽_ 1 횯[(][푑][)] _, 휽_ 2 횯[(][푑][)] _, and returns YES if KL_ _푝_ _푝휽1_ KL _푝_ _푝휽2_ _, and NO_
_otherwise, where 푝, 푝휽1, 푝 ∈휽2 are string distributions defined by ∈_ ˜푝, ˜푝휽(1, ˜||푝휽2 respectively.) ≥ ( || )

_Proof. Let 푀푏_ be the input-free unweighted Turing machine we built in our proof of Theorem 3, whose
behavior is independent of ZFC. Let 푀0 be any weighted Turing machine that defines the EC weighted
language ˜푝. And let 푀1 be a weighted Turing machine that weights 풙 as ˜푝(풙) + 1 if and only if 풙 encodes
an accepting trace of 푀푏; and 푀1 weights 풙 as ˜푝(풙) otherwise. Since checking whether 풙 is a valid trace
of 푀푏 is in 푂 (poly(|풙|)), 푀1 defines an EC weighted language.

Let 푝 _푀1 be the string distribution defined by 푀1. By an argument similar to our proof of Theorem 8, no_
algorithm that is provably correct can decide if KL _푝_ _푝_ _푀1_ KL _푝_ _푝_ .
( || ) ≥ ( || )

We note that for any weighted Turing machine 푀 with fewer than 푛 states, we can build another weighted
Turing machine 푀 [′] which has 푛 states, such that 푀 and 푀 [′] define the same weighted language, simply by
having (finitely many) additional unreachable states in 푀 [′]. Since any weighted Turing machine with 푛
states can be implemented as a 1-encoder-layer-4-decoder-layer Transformer networks with an embedding
vectors within a fixed-size model family withsize ∈ _푂_ (푛), it follows that there exists 푑0 ∈ N 푑 such that both≥ _푑0._ _푀0 and 푀1 can be encoded as parameter□_


15We use 횯[(][푑][)] to denote a Transformer family with embedding size 푑.


-----

