# STABILITY BASED GENERALIZATION BOUNDS
## FOR EXPONENTIAL FAMILY LANGEVIN DYNAMICS

**Anonymous authors**
Paper under double-blind review

ABSTRACT

We study the generalization of noisy stochastic mini-batch based iterative algorithms based on the notion of stability. Recent years have seen key advances in
data-dependent generalization bounds for noisy iterative learning algorithms such
as stochastic gradient Langevin dynamics (SGLD) based on (Mou et al., 2018; Li
et al., 2020) and related approaches (Negrea et al., 2019; Haghifam et al., 2020).
In this paper, we unify and substantially generalize stability based generalization
bounds and make three technical advances. First, we bound the generalization
error of general noisy stochastic iterative algorithms (not necessarily gradient descent) in terms of expected stability, which in turn can be bounded by the expected
Le Cam Style Divergence (LSD). Such bounds have a O(1/n) sample dependence
unlike many existing bounds with O(1/[√]n) dependence. Second, we introduce
Exponential Family Langevin Dynamics (EFLD) which is a substantial generalization of SGLD and which allows exponential family noise to be used with gradient descent. We establish data-dependent expected stability based generalization
bounds for general EFLD. Third, we consider an important new special case of
EFLD: noisy sign-SGD, which extends sign-SGD by using Bernoulli noise over
_{−1, +1}, and we establish optimization guarantees for the algorithm. Further,_
we present empirical results on benchmark datasets to illustrate the our bounds
are non-vacuous and quantitatively much sharper than existing bounds.

1 INTRODUCTION

Recent years have seen renewed interest and advances in characterizing generalization performance
of learning algorithms in terms of stability, which considers change in performance of a learning
algorithm based on change of a single training point (Hardt et al., 2016; Bousquet & Elisseeff, 2002;
Li et al., 2020; Mou et al., 2018). For stochastic gradient descent (SGD), Hardt et al. (2016) established generalization bounds based on uniform stability (Hardt et al., 2016; Bousquet & Elisseeff,
2002), although the analysis needed rather small step sizes ηt = 1/t which is not useful in practice.
While improving the analysis for SGD has remained a challenge, advances have been made on noisy
SGD algorithms, especially stochastic gradient Langevin dynamics (SGLD) (Welling & Teh, 2011;
Mou et al., 2018; Li et al., 2020), which adds Gaussian noise to the stochastic gradients of marginal
variance σt[2][. In parallel, there has been key developments on related information-theoretic general-]
ization bounds applicable to SGLD type algorithms (Negrea et al., 2019; Haghifam et al., 2020; Xu
& Raginsky, 2017; Russo & Zou, 2016; Pensia et al., 2018).

While these developments have led to major advances in analyzing generalization of noisy SGD
algorithms, they each have certain limitations which leave room for further improvements. Using
uniform stability, Mou et al. (2018) established a bound for SGLD of the form _[K]n_ _t_ _[η]t[2][/σ]t[2]_ [which]

depends onbound has a desirable dependency of K, the global Lipschitz constant for the loss, and with step size O(1/n) on the samples, but has an undesirable dependence on ηt ≤pPσt ln 2/K. The
_K, and the step sizes, bounded by σt/K, are too small. Mou et al. (2018) also presented another_
bound which addresses some of these issues, but gets an undesirable O(1/[√]n) sample dependence.
By building on the developments of Russo & Zou (2016); Xu & Raginsky (2017); Pensia et al.
(2018), Negrea et al. (2019) made advances from the information theoretic perspective and established bounds for SGLD which have the desirable dependence on the norm of gradient incoherence,
i.e., difference in gradients over different mini-batches, avoids dependence on Lipschitz constant
_K, and is applicable to unbounded sub-Gaussian losses, but have an undesirable O(1/[√]n) sample_


-----

dependence. Haghifam et al. (2020) made further advances on the problem from an information theoretic perspective based on the conditional mutual information framework of Steinke & Zakynthinou
(2020) and obtained generalization bounds based on gradient incoherence with O(1/n) sample dependence, but their analysis holds for full batch Langevin dynamics, not mini-batch SGLD. Li et al.
(2020) made advances on such bounds based on the notion of Bayes-stability, by combining ideas
from PAC-Bayes bounds into stability, and established a bound of the form _n[c]_ _t_ _[η]t[2][g][e][(][t][)][/σ]t[2]_ [for]

bounded losses, where ge(t) is the expected gradient norm square at step t. While the bound avoids
dependency on the Lipschitz constant K, the dependence on the gradient norm makes such boundspP
much weaker than the information theoretic bounds of Negrea et al. (2019); Haghifam et al. (2020)
which depend on the norm of gradient incoherence, which are typically orders of magnitude smaller.
Further, the analysis of Li et al. (2020) still needs small step sizes, bounded by σt/K.

In this paper, we build on the core strengths of such existing approaches, most notably the O(1/n)
sample dependence of stability based bounds (Mou et al., 2018; Li et al., 2020) and the dependence on gradient incoherence for information theoretic bounds (Negrea et al., 2019; Haghifam
et al., 2020), and develop a framework (Section 2) for developing generalization bounds for noisy
stochastic iterative (NSI) algorithms. Our framework considers generalization based on the concept
of expected stability, rather than uniform stability (Hardt et al., 2016; Bousquet & Elisseeff, 2002;
Bousquet et al., 2020; Mou et al., 2018), which yields distribution dependent generalization bounds
and avoids the worst-case setting of uniform stability. Building on Li et al. (2020), we show that
expected stability of general NSI algorithms can be bounded by the expected Le Cam Style Divergence with dependence on parameter distributions from mini-batches differing by one sample. In
Section 3, we introduce Exponential Family Langevin Dynamics (EFLD), a family of noisy gradient
descent algorithms based on exponential family noise. Special cases of EFLD include SGLD and
noisy versions of Sign-SGD or quantized SGD algorithms widely used in practice (Bernstein et al.,
2018a;b; Jin et al., 2020; Alistarh et al., 2017)Our main result provides an expected stability based
generalization bound applicable to any EFLD algorithm with a O(1/n) sample dependence and a
dependence on gradient incoherence, rather than gradient norms. Existing generalization bounds for
SGLD (Li et al., 2020; Negrea et al., 2019) usually use properties of the Gaussian distribution, and
our analysis on EFLD illustrates that this was unnecessary. We also consider optimization guarantees for EFLD and establish such results for noisy Sign-SGD and SGLD. Through experiments on
benchmark datasets (Section 4), we illustrate that our bounds are non-vacuous and quantitatively
much sharper than existing bounds (Li et al., 2020; Negrea et al., 2019).

**Related work. Uniform stability has been a classical approach for bounding generalization error**
(Bousquet & Elisseeff, 2002; Bousquet et al., 2020; Feldman & Vondrak, 2018; 2019), pioneered by
Rogers & Wagner (1978); Devroye & Wagner (1979). Beyond the aforementioned work, there has
been recent work on differential privacy that analyzes the uniform stability of differentially private
SGD (DP-SGD) (Hardt et al., 2016; Bassily et al., 2020). Beyond uniform stability, informationtheoretic approaches (Russo & Zou, 2016; Xu & Raginsky, 2017) that bounds the generalization
error by the mutual information between the algorithm input S and the algorithm output w, have
been used for deriving generalization bounds for noisy iterative algorithms (Pensia et al., 2018; Bu
et al., 2019). Along this line of literature, Negrea et al. (2019); Haghifam et al. (2020); Rodr´ıguezG´alvez et al. (2021) prove data-dependent generalization bounds dropping dependence on the Lipschitz constant. Further, tighter bounds (Haghifam et al., 2020; Zhou et al., 2021; Rodr´ıguez-G´alvez
et al., 2021; Neu, 2021; Hellstr¨om & Durisi, 2021) are proposed based on conditional mutual information (Steinke & Zakynthinou, 2020; Gr¨unwald et al., 2021; Hellstr¨om & Durisi, 2020). Due to
space limitations, an extended discussion of the related work is deferred to Appendix A.

2 GENERALIZATION BOUNDS WITH EXPECTED STABILITY

In the setting of statistical learning, there is an instance space Z, a hypothesis space W, and a loss
function ℓ : W ×Z 7→ R+. Let D be an unknown distribution of Z and let S ∼ _D[n]_ be n i.i.d. draws
from D. For any specific hypothesis w, the population and empirical loss are respectively
_∈W_ _n_
given by LD(w) ≜ Ez∼D[ℓ(w, z)], and LS(w) ≜ _n[1]_ _i=1_ _[ℓ][(][w][, z][i][)][. For any distribution][ P][ over]_

the hypothesis space, we respectively denote the expected population and empirical loss asn
P
_LD(P_ ) ≜ Ez _DEw_ _P [ℓ(w, z)],_ and _LS(P_ ) ≜ [1] Ew _P [ℓ(w, zi)] ._ (1)
_∼_ _∼_ _n_ _∼_

_i=1_

Consider a randomized algorithm A which works with S = _zX1, . . ., zn_ _D[n]_ and cre_{_ _} ∼_


-----

ates a distribution over the hypothesis space W. For convenience, we will denote the distribution as A(S). The focus of the analysis is to bound the generalization error of A defined as:
gen(A(S)) ≜ _LD(A(S)) −_ _LS(A(S)) . We will assume A is permutation invariant, i.e., the or-_
dering of samples in S do not modify A(S), an assumption satisfied by most learning algorithms.
We will focus on developing bounds for the expectation ES[LD(A(S)) − _LS(A(S))], and discuss_
high-probability bounds in the Appendix B.

2.1 BOUNDS BASED ON EXPECTED STABILITY

We start our analysis by noting that the expected generalization error can be upper bounded by
_expected stability based on the Hellinger divergence (Sason & Verdu, 2016; Li et al., 2020):_
_H_ [2](P _∥P_ _[′]) =_ [1]2 **w[(]** _p(w) −_ _p[′](w))[2]dw._

**Proposition 1. LetR** _Spn_ _D[n]pand let Sn[′]_ _[be a dataset obtained by replacing][ z][n]_ _[with]_
_obtained by running randomized algorithmzn[′]_ _[∼]_ _[D][. Let][ A][(][S][n][)][, A] ∼[(][S]n[′]_ [)][ respectively denote the distributions over the hypothesis space] A on Sn, Sn[′] _[. Assume that for][ S][n]_ _[∈]_ _[S][n]_ _[ W]_
EW _A(Sn)[ℓ[2](W, z)]_ _c0/2, c0 > 0. With H(_ _,_ ) denoting the Hellinger divergence, we have[∼] _[D][n][,][ ∀][z][ ∈Z][,]_
_∼_ _≤_ _·_ _·_

_|ESn∼Dn_ [LD(A(Sn)) − _LS(A(Sn))]| ≤_ _c0ESn∼Dn_ Ezn′ _[∼][D]_ 2H [2][ ]A(Sn), A(Sn[′] [)] _._ (2)
q

**Remark 2.1. Proposition 1 does not need bounded losses. Just the second moment of ℓ(W, z** ), W ∼
_A(Sn), Sn_ _D[n],_ _z_ need to be bounded. The assumption is satisfied by bounded losses. It is
instructive to compare the assumption to that in recent information theoretic bounds (Haghifam et al., ∼ _∀_ _∈Z_
2020; Xu & Raginsky, 2017), where one assumes ℓ(w, Z), Z ∼ _D, ∀w ∈W to be sub-Gaussian._
**Remark 2.2. The bound in Proposition 1 is in terms of expected stability where we consider**
ES _Dn_ Ezn′ _[∼][D][[][· · ·][ ]][, an important departure from bounds based on][ uniform stability][ (Elisseeff et al.,]_
_∼_
2005; Bousquet & Elisseeff, 2002; Mou et al., 2018; Bousquet et al., 2020; Feldman & Vondrak,
2018; 2019) where one considers supS,S′∈Z _n,|S\S′|=1[· · · ]. Replacing sup by E makes the bounds_
distribution dependent, and arguably leads to quantitatively tighter bounds.


Note that the Hellinger divergence can be bounded by the KL divergence.

**Proposition 2. For any distributions P and P** _[′], 2H_ [2](P, P _[′]) ≤_ min _KL(P, P_ _[′]),_

2.2 EXPECTED STABILITY OF NOISY STOCHASTIC ITERATIVE A LGORITHMS


1
2 _[KL][(][P, P][ ′][)]_


We consider a general family of noisy stochastic iterative (NSI) algorithms. Given S ∼ _D[n], such_
iterative algorithms have two additional sources of randomness in each iteration t: (a) a stochastic
mini-batch of samples SBt, with _SBt_ = b, drawn uniformly at random with replacement from
_|_ _|_
_S; and (b) noise ξt suitably included in the iterative update. Given a trajectory of past iterates_
_W0:(t−1) = w0:(t−1), the new iterate Wt is drawn from a distribution PBt,ξt|w0:(t−1) over W:_

_Wt ∼_ _PBt,ξt|w0:(t−1)_ (W ) . (3)

We will drop the conditioning w0:(t−1) to avoid clutter in the sequel. Let PT, PT[′] [denote the marginal]
distributions over hypotheses w ∈W after T steps of the algorithm based on Sn, Sn[′] [respec-]
tively. Further, let P0:(t 1) denote the joint distribution over W0:(t 1) = (W0, . . ., Wt 1), and
_−_ _−_ _−_
let Pt| ≡ _PBt,ξt|w0:(t−1) compactly denote the conditional distribution on Wt conditioned on the_
trajectory W0:(t 1) = w0:t 1. Following (Negrea et al., 2019; Haghifam et al., 2020; Pensia et al.,
_−_ _−_
2018), we use the following chain rule for KL-divergence: KL(PT _PT[′]_ [)][ ≤] _[KL][(][P][0:][T][ ∥][P]0:[ ′]_ _T_ [) =]
_T_ _∥_
_t=1_ [E][P]0:(t−1) [[][KL][(][P][t][|][∥][P][ ′]t|[)]][. Let][ ¯]S ∼ _D[n][+1], and let Sn, Sn[′]_ [be size][ n][ subsets of][ ¯]S such that
_Sn =_ _Z1, . . ., Zn_ 1, Zn and Sn[′] [=][ {][Z][1][, . . ., Z][n][−][1][, Z]n[′] _[}][, where][ Z]n[′]_ [=][ Z][n][+1][.] Let S0 =
P _{_ _−_ _}_
_{Z1, . . ., Zn−1}. The algorithms we consider use a mini-batch of size b in each iteration uniformly_
sampled from n samples. Let the set of all mini-batch index sets be denoted by G. Let the set of all
mini-batch index sets A drawn from S0 be denoted by G0. Note that _G0_ = _n−b_ 1 . Let G1 denote
_|_ _|_
the set of all mini-batch index sets B which includes the last sample, viz. zn for S with mini-batches
_n_ 1 _n_ 1   _n_ 1 _n_
and zn[′] [for][ S]n[′] [. Note that][ |][G][1][|][ =] _b−−1_ . Also note that |G0| + |G1| = _−b_ + _b−−1_ = _b_ = |G|.

Following Li et al. (2020), we can bound their conditional KL-divergences      KL  (Pt|∥Pt[′] |[)][ in terms]
of a Le Cam Style Divergence (LSD). While the classical Le Cam divergence (Sason & Verdu,


-----

2016) is LCD(P _∥P_ _[′]) ≜_ [1]2 (dPdP − +dPdP ′[′])2 (where dP denotes the density), our bounds in terms of

_LSD(Pt||∥Pt[′]|[)][ ≜]_ (dPBt,dPξtR−At,dPξtBt,[′] **_ξt_** [)][2] _, Bt ∈_ _G1, At ∈_ _G0. Note that PBt,ξt and PB[′]_ _t,ξt_ [represent]

the distribution of WR _t for Sn and Sn[′]_ [respectively since the mini-batch][ S][B]t [of][ S][n] [and][ S]n[′] [differs in]
the n-th sample. Putting everything together, we have the following LSD based bound.
**Lemma 1. Consider a noisy stochastic iterative algorithms of the form (3) with mini-batch size**
_b ≤_ _n/2. Then, with c1 =_ _√2c0 (with c0 as in Proposition 1), we have_


2

_dPBt,ξt −_ _dPB[′]_ _t,ξt_ _dξt_

_dPAt,ξt_ 


_|ESn_ [LD(A(Sn))−LSn (A(Sn))]| ≤ _c1_


_b_

_n_ [E][S][n] [E][z][′]


_._




_t=1_ _W0:(t−1)Bt∈G1At∈G0_

X


**_ξt_**


(4)
**Remark 2.3. Li et al. (2020) essentially has this result for SGLD and inspired our work. Our proofs**
are significantly simpler and, more importantly, illustrates applicability to general noisy iterative
algorithms of the form (3), not just SGLD with Gaussian noise as in Li et al. (2020).
**Remark 2.4. Note that the bound does not assume the loss to be bounded, depends on expectations**
over samples Sn, zn[′] [, trajectories][ w]0:(t 1)[, and mini-batches][ B][t][, A][t][. Further, the bound depends on]
_−_
the distribution discrepancy as captured by the expected LSD.
**Remark 2.5. The bound seems to worsen with b, the size of the mini-batch. As we shown in Sec-**
tion 3, the expected LSD term has a _b[1][2][ dependence for the Exponential Family Langevin dynamics]_

(EFLD) models we introduce, so the leading b is neutralized.

3 EXPONENTIAL FAMILY LANGEVIN DYNAMICS


In recent years, considerable advances have been made in establishing generalization bounds for
stochastic gradient Langevin dynamics (SGLD) (Li et al., 2020; Pensia et al., 2018; Negrea et al.,
2019; Haghifam et al., 2020). As an example of NSI algorithms of the form (3), SGLD adds an
isotropic Gaussian noise at every step of SGD: wt+1 = wt − _ηt∇ℓ(wt, SBt_ ) + N 0, σt[2][I][d], where
_ℓ(wt, SBt_ ) is the stochastic gradient on mini-batch Bt, ηt is the step size, and σt[2] [is noise vairance.]
_∇_   

In this paper, we introduce a substantial generalization of SGLD called Exponential Family
Langevin Dynamics (EFLD) which uses general exponential family noise in noisy iterative updates
of the form (3). In addition to being a mathematical generalization of the popular SGLD, the proposed EFLD provides flexibility to use noise gradient algorithms with different representation of the
gradient, e.g., Bernoulli noise for Sign-SGD, discrete distribution for quantized or finite precision
SGD, etc. (Canonne et al., 2020; Alistarh et al., 2017; Jiang & Agrawal, 2018; Yang et al., 2019).

3.1 EXPONENTIAL FAMILY LANGEVIN DYNAMICS (EFLD)
Exponential families (Barndorff-Nielsen, 2014; Brown, 1986; Wainwright & Jordan, 2008) constitute a large family of parametric distributions which include Gaussian, Bernoulli, gamma, Poisson, Dirichlet, etc., as special cases. Exponential families are typically represented in terms of
natural parameters θ, and we consider component-wise independent distributions with scaled natural parameter θα = θ/α with scaling α > 0, i.e., pψ(ξ, θα) = exp( **_ξ, θα_** _ψ(θα))π0(ξ) =_
_p_ _⟨_ _⟩−_
_j=1_ [exp(][ξ][j][θ][jα][ −] _[ψ][j][(][θ][jα][))][π][0][(][ξ][j][)][, where][ ξ][ is the sufficient statistic,][ ψ][(][θ][α][) =][ P]j[p]=1_ _[ψ][j][(][θ][jα][)][ is]_
the log-partition function, and π0(ξ) = _j=1_ _[π][0][(][ξ][j][)][ is the base measure. Note that][ α][ = 1][ gives]_
Q
the canonical form of the exponential family distributions. For general scaling α > 0, for some
cases the base measure π0 may depend on the scaling, i.e.,[Q][p] _π0,α. A scaling α > 0 is valid as long_
as exp(⟨ξ, θα⟩ is integrable, i.e., **_ξ_** [exp(][⟨][ξ][,][ θ][α][⟩][π][0][(][ξ][)][d][ξ][ <][ ∞][. Further,][ ψ][ is a smooth function by]

construction (Barndorff-Nielsen, 2014; Banerjee et al., 2005; Wainwright & Jordan, 2008) and the

R

smoothness of ψ implies ∇[2]ψ(θα) ≤ _c2I._

Exponential family Langevin dynamics (EFLD) uses noisy stochastic gradient updates similar to
SGLD, but using exponential family noise rather than Gaussian noise as in SGLD. In particular, for
mini-batch SBt, EFLD updates are as follows: with step size ρt > 0

**wt = wt** 1 _ρtξt,_ **_ξt_** _pψ(ξ; θBt,αt_ ), (5)
where _−_ _−_ _∼_
_pψ(ξ; θBt,αt_ ) = exp( **_ξ, θBt,αt_** _ψ(θBt,αt_ ))π0(ξ), **_θBt,αt_** = _. (6)_
_⟨_ _⟩−_ ≜ **_[θ]α[B]t[t]_** _[∇][ℓ][(][w][t]α[−]t[1][, S][B][t]_ [)]


-----

For EFLD, the natural parameter θBt,αt at step t is simply a scaled version of the mini-batch gradient
_∇ℓ(wt−1, SBt_ ). We first show that EFLD becomes SGLD when the exponential family is Gaussian,
and becomes a noisy version of sign-SGD (Bernstein et al., 2018a;b) when the exponential family is
Bernoulli over {−1, +1}. More details and examples are in Appendix C.1.
**Example 3.1 (SGLD). SGLD uses scaled Gaussian noise with ψ(θ) =** **_θ_** 2[/][2][, α][t] [=] _σt/ηt,_
_∥_ _∥[2]_

so that pψ(ξ; θBt,αt ) = N (θBt _, αt[2][I][d][)][. By taking][ ρ][t]_ [=][ √][η][t][σ][t][, the update (5) based on]p[ ρ][t][ξ][t] [is]
distributed as N (ρtθBt _, ρ[2]t_ _[α]t[2][I][d][) =][ N]_ [(][η][t][∇][ℓ][(][w][t][−][1][, S][B]t [)][, σ]t[2][I][d][)][. Thus the EFLD update reduces to]
the SGLD update: wt = wt−1 − _ηt∇ℓ(wt−1, SBt_ ) + N 0, σt[2][I][d] _._

**Example 3.2 (Noisy Sign-SGD). By taking ρt = ηt and component-wise ξj** 1, 1 _, π0(ξj) =_
1exponential family distribution, ψ(θ) = log(exp(−θ)+exp( pθψ))( in exponential family update equation (5), theξ; θBt,αt ) becomes pθ Bt,αt,j (ξj) = exp(− ∈{−θBt,αt,jexp( jξ-th component ofj **_θ)+exp(Bt,αt,j}_** **_θ)Bt,αt,j_** ) [.]

Thus, the EFLD update reduces to a noisy version of Sign-SGD: wt = wt 1 _ηtξt, ξt,j_
_pθBt,αt,j_ (ξj), j ∈ [d], where θBt,αt = ∇ℓ(wt−1, SBt )/αt is the scaled mini-batch gradient.− _−_ _∼_

3.2 EXPECTED STABILITY OF EXPONENTIAL FAMILY LANGEVIN DYNAMICS

From Lemma 1, conditioned on a trajectory w0:(t−1), mini-batches SBt _, SAt_, we can get generalization bound by suitably bounding the Le Cam Style Divergence (LSD) given by: IAt,Bt =

2
([dP]Bt,ξt _[−][dP][ ′]Bt,ξt_ [)]
**_ξt_** _dPAt,ξt_ _dξt. For EFLD, the density functions dPBt,ξt are exponential family densi-_

ties pψ(ξ; θBt,αt ) as in (5)-(6), and we have the following bound on the per step LSD:

## RTheorem 1. For a given set S[¯] D[n][+1] and wt 1 at iteration (t 1), let ∆t wt 1 ( S[¯]) =

_∼_ _−_ _−_ _|_ _−_
maxz,z′ _S¯_ 2 _[. Further, for a][ c][2][-smooth log-partition function][ ψ][,]_
_∈_ _[∥∇][ℓ]_ [(][w][t][−][1][, z][)][ −∇][ℓ] [(][w][t][−][1][, z][′][)][∥]
_let the scaling αt|wt−1 be data-dependent such that αt[2]|wt−1_ _t|wt−1_ [(][S][n][+1][)][. Then, we have]

_[≥]_ [8][c][2][∆][2]

5c2

Note thatIAt,Bt ≤ SB5t andc2∥θ SBtB[′],αtt −[only differ at samples]θBt[′] _[,α][t]_ _[∥]2[2]_ [=] 2αt[2]|wt−[ z]1 _[n]h[ and]∇ℓ[ z](wn[′]_ [. The above bound can now be directly]t−1, SBt ) −∇ℓ  wt−1, SB[′] _t_ 2[2]i _,_ (7)
applied to Lemma 1 to get expected stability based generalization bounds for any EFLD algorithm.
**Theorem 2. Consider an exponential family Langevin dynamics (EFLD) algorithm of the form (5)-**
(6) with a c2-smooth log-partition function ψ. Then, for mini-batch size b _n/2, with c = c0√5c2_
_≤_
_hidden to avoid clutter), we have(with c0 as in Lemma 1) and αt[2]|_ _[≥]_ [8][c][2][∆]t[2]|[(][S][n][+1][)][ (as in Theorem 1, with the conditioning on][ w][t][−][1]


ES[LD(A(S)) _LS(A(S))]_ _c_ [1]
_|_ _−_ _| ≤_ _n_


_∥∇ℓ_ (wt−1, zn) −∇ℓ (wt−1, zn[′] [)][∥][2]2


_αt[2]|_


_Sn+1_


_t=1_ _W0:(t−1)_

X


(8)

**Remark 3.1. Theorem 2 captures the generalization error of SGLD, which is a special case of**
EFLD. Our bound has the same dependence on n, T, step size ηt as the bound in Li et al.
(2020). However, our bound is numerically sharper because we replace the gradient norms, i.e.,
1

which is quantitatively smaller than gradient norms as we show in the experiment section. Then _z∈S_ _[∥][ℓ][(][w][t][, z][)][∥]_ [in Li et al. (2020) and with gradient discrepancy][ ∥∇][ℓ][(][w][t][, z][)][ −∇][ℓ][(][w][t][, z][′][)][∥][,]
bound in Negrea et al. (2019) depends onP _gradient incoherence which is empirically smaller than_
gradient discrepancy as observed in the experiment section, their bound depends on 1/[√]n, which is
worse than the 1/n dependence in our bound.
**Remark 3.2. EFLD can be extended to work with anisotropic noise by using θBt,αt** =
_ℓ(wt_ 1, SBt ) **_αt in (6) where αt_** R[p] determines different scaling for each dimension and
_⊘∇_ denotes Hadamard division. Theorems 1 and 2 can be extended to such anisotropic noise by using− _⊘_ _∈_
**_α-scaled norms for the gradient discrepancy, i.e.,_** **g** **g[′]** 2,α [=][ P]j[(][g][j][ −] _[g]j[′]_ [)][2][/α]j[2][.]
_∥_ _−_ _∥[2]_

**Remark 3.3. The condition on αt is a data-dependent quantity, which can be computed along the**
training process. It gives much more benign condition of the step size compared to those in the
related work (Mou et al., 2018; Li et al., 2020, Hardt et al. 2016), which require step size being
bounded by σt/L. However, the step sizes in Theorem 2 need to be bounded by σt/∆t( S[¯]), which
is considerably more relaxed since ∆t( S[¯]) is much smaller than Lipschitz constant L, which is a
uniform bound over the whole parameter space. Also, usually one would expect ∆t( S[¯]) to decrease
as training proceeds since the gradients shrink as the loss function being minimized. Thus, the
constraint on step size does not require the step sizes to be as small as σt/L.


-----

3.3 PROOF SKETCHES OF MAIN RESULTS: THEOREMS 1 AND 2

We focus on Theorem 1. To avoid clutter, we drop the subscript t for the analysis and note that
the analysis holds for any step t. When the density dPB,ξ = pψ(ξ; θB,α), by mean-value theorem,
for each ξ, we have pψ(ξ; θB,α) − _pψ(ξ; θB′,α) = ⟨θB,α −_ **_θB,α, ∇θ˜B,α_** _[p][ψ][(][ξ][; ˜]θB,α)⟩, for some_
**_θ˜B,α = γξθB,α + (1 −_** _γξ)θB,α[′]_ [where][ γ][ξ][ ∈] [[0][,][ 1]][. Then,]

_IA,B =_ _pψ(ξ; θB,α) −_ _pψ(ξ; θB′,α)_ 2 _dξ =_ _⟨θB,α −_ **_θB,α[′]_** _[,][ ξ][ −∇]θ[˜]B,α_ _[ψ][(][ξ][; ˜]θB,α)⟩[2]_ _p[2]ψ[(][ξ][; ˜]θB,α)_ _dξ,_

_pψ(ξ; θA,α)_ _pψ(ξ; θA,α)_

**_ξ_**    **_ξ_**

## Z Z


since pψ(ξ; θ[˜]B,α) = exp( **_ξ,_** **_θ[˜]B,α_** _ψ(θ[˜]B,α))π0(ξ)._
_⟨_ _⟩−_

**Handling Distributional Dependence of** **_θ[˜]B. Note that we cannot proceed with the analysis with_**
the density term depending on **_θ[˜]B,α since_** **_θ[˜]B,α depends on ξ. So, we first bound the density term_**
depending on **_θ[˜]B,α in terms of exponential family densities with parameters θB,α and θB,α using_**
_c2-smoothness of ψ._

**Lemma 2. For some γξ ∈** [0, 1], **_θ[˜]B,α = γξθB,α + (1 −_** _γξ)θB,α[′]_ _[, we have]_

exp **_ξ,_** **_θ[˜]B,α_** _ψ(θ[˜]B,α)_
_⟨_ _⟩−_

exp _c2_ **_θB,α_** **_θB′,α_** 2 _._
max exp **_ξ, θB,α_** h _ψ(θB,α)_ _, exp [_ **_ξ, θBi_** _′,α_ _ψ(θB′,α)]_ _≤_ _∥_ _−_ _∥[2]_
_⟨_ _⟩−_ _⟨_ _⟩−_
 
    

**Bounding the Density Ratio. Next we focus on the density ratio p[2]ψ[(][ξ][,][ ˜]θB,α)/pψ(ξ; θA,α). By**
Lemma 2, it suffices to focus on p[2]ψ[(][ξ][,][ θ][B,α][)][/p][ψ][(][ξ][;][ θ][A,α][)][ or the equivalent term for][ θ][B][′][,α][. We show]
that the density ratio can be bounded by another exponential family with parameters (2θB,α **_θA,α)._**
_−_

**Lemma 3. For any ξ, we have**

exp [ **_ξ, 2θB,α_** 2ψ(θB,α)]

exp [⟨ **_ξ, θA,α⟩−_** _ψ(θA,α)]_ _≤_ exp 2c2∥θB,α − **_θA,α∥2[2]_** exp [⟨ξ, (2θB,α − **_θA,α⟩−_** _ψ(2θB,α −_ **_θA,α)] ._**
_⟨_ _⟩−_
 

The analysis for the term p[2]ψ[(][ξ][,][ θ][B][′][,α][)][/p][ψ][(][ξ][;][ θ][A,α][)][ is exactly the same.]


**Bounding the Integral.** Ignoring multiplicative terms which do not depend on ξ for the
moment, the analysis needs to bound an integral term of the form **_ξ[⟨][θ][B,α][ −]_** **_[θ]B,α[′]_** _[,][ ξ][ −]_

_∇ψ(ξ; θ[˜]B,α)⟩[2]_ _pψ(ξ; 2θB,α −_ **_θA,α)dξ, and a similar term with p[2]ψ[(][ξ][; 2][θ][B][′][,α][ −]_** **_[θ][A,α][)][.]_** First,
note that _ψ(ξ; θ[˜]B,α) = ˜µB,α, the expectation parameter for pψ(ξ; θ[˜]B,α)R Wainwright & Jordan_
_∇_
(2008); Banerjee et al. (2005). The integral, however, is with respect to pψ(ξ; 2θB,α **_θA,α)._**
_−_
We handle this discrepancy by using ξ −∇ψ(ξ; θ[˜]B,α) = (ξ − E[ξ]) + (E[ξ] −∇ψ(ξ; θ[˜]B,α)),
and decomposing as sum-of-squares. Quadratic form for the first term yields the covariance
E[(ξ E[ξ])(ξ E[ξ])[T] ] = _ψ(θ2θB,α_ **_θA,α_** ) _c2I, by smoothness. The second term de-_
_−_ _−_ _∇[2]_ _−_ _≤_
pends on the difference of gradients _ψ(2θB,α_ **_θA,α)_** _ψ(θ[˜]B,α) which, using smoothness and_
additional analysis, can be bounded by the norm of ∇ _−_ (θB,α −∇θA,α). All the pieces can be put together
_−_
to get the bound in Theorem 1, which when used in Lemma 1 yields Theorem 2.

3.4 OPTIMIZATION GUARANTEES FOR EFLD
We now establish optimization guarantees for two examples of EFLD, i.e., Noisy Sign-SGD with
Bernoulli noise over {−1, +1} and SGLD with Gaussian noise.

**Noisy Sign-SGD. For mini-batch Bt and scaling αt, mini-batch Noisy Sign-SGD updates the pa-**
rameters as wt = wt 1 _ηtξt, where each component j_ [d]
_−_ _−_ _∈_

**_ξt,j ∼_** _pθBt,αt,j_ (x) = exp( **_θBexp(t,αt,jx) + exp(θBt,αt,j)θBt,αt,j)_** _[, x][ ∈{−][1][,][ +1][}]_ (9)

_−_

where θBt,αt = _ℓ(wt_ 1, SBt )/αt is the scaled mini-batch gradient. The full-batch version uses
_∇_ _−_
parameters EBt [θBt,αt ] = _LS(wt_ 1) For the optimization analysis, we assume that the loss is
_∇_ _−_
smooth and mini-batch gradients are unbiased, symmetric, and sub-Gaussian.


-----

**Assumption 1. The loss function LS satisfies: for all w and w[′], for some non-negative constant**
_K⃗_ := [K1, . . ., Kd], we have LS(w) ≤ _LS(w[′]) + ∇LS(w[′])[T]_ (w − **w[′]) +** 2[1] _i_ _[K][i][(][w][i][ −]_ **[w]i[′][)][2][.]**

**Assumption 2. Given wt−1, the mini-batch gradient ∇ℓ(wt−1, SBt** ) is (a) unbiased, i.e.,P
EBt **wt** 1 _ℓ(wt_ 1, SBt ) = _LS(wt_ 1); _(b) symmetric, i.e., the density p(x) of x_
_|_ _−_ _∇_ _−_ _∇_ _−_ _≡_
_∇ℓ(wt−1, SBt_ ) is symmetric around its expectation LS(wt−1): p(x) = p(2∇LS(wt−1) − _x) and_
_(c) sub-Gaussian, i.e., for any λ > 0, any v s.t. ∥v∥2 = 1, EBt|wt−1 exp λ⟨v, ∇ℓ(wt−1, SBt_ ) −
_∇LS(wt−1)⟩≤_ exp(λ[2]κ[2]t _[/][2)][ for some constant][ κ][t]_ _[>][ 0][.]_
Based on the assumptions, we have the following optimization guarantee for mini-batch noisy SignSGD. We defer the optimization guarantee for full-batch noisy Sign-SGD to Appendix D.

**Theorem 3. Under Assumption 1 and 2, for mini-batch noisy Sign-SGD with step size ηt = 1/√T** _,_

_αt satisfying c_ _αt_ max[√2κt, 4 _LS(wt_ 1) ], we have for any S and any initialization

**w0** _≥_ _≥T_ _∥∇_ _−_ _∥∞_

1

E _LS(wt)_ 2 _LS(w0)_ _LS(w[∗]) + [1]_ _K_ 1 _,_ (10)

" _T_ _t=1_ _∥∇_ _∥[2]#_ _≤_ _√[4][c]T_  _−_ 2 _[∥]_ _[⃗]_ _∥_ 

X

_where the expectation is taken over the randomness of algorithm._
**SGLD. We acknowledge that the following optimization result of SGLD exists in various forms, as**
noisy gradient descent algorithms have been studied in literature such as differential privacy, where
SGLD can be viewed as DP-SGD (Bassily et al., 2014; Wang & Xu, 2019) and the proof technique
boils down to bounding the stochastic variance of the noisy gradient (Shamir & Zhang, 2013).
_sizeTheorem 4. ρt =_ _[√]η Under Assumption 1 and 2, withtσt, αt =_ _σt/ηt), |Bt| = b, and K ηit = = K,√1T ∀[, can achieve]i ∈_ [d], for any S, SGLD (EFLD with step

_T_ p

1 1 _t=1_ _[α]t[4]_ [+ log][ T]

_T_ _t=1_ E∥∇LS(wt)∥[2] _≤_ _O_  _√T_  + O _K [p][ P][T]_ _√T_ ! _,_ (11)

X

_where the expectation is over the randomness of the algorithm._
The error rate of SGLD depends on the noise variance αt. One can choose a decaying noise variance
4
such as αt = 1/√t to guarantee the convergence. Then the rate will become O(log T/√T ). We

note that similar to the optimization guarantees of DP-SGD, the convergence rate depends on the
dimension of the gradient p due to the isotropic Gaussian noise. Special noise structures such as
anisotropic noise that aligned with the gradient structure can reduce the dependence on dimension
(Kairouz et al., 2020; Zhang et al., 2021; Asi et al., 2021; Zhou et al., 2020).

4 EXPERIMENTS

In this section, we conduct a series of experiments to evaluate our generalization error bounds. For
SGLD, we aim to compare the proposed bound in Theorem 2 with existing bounds in Li et al. (2020),
Negrea et al. (2019), and Rodr´ıguez-G´alvez et al. (2021) for various datasets. Note that the bound
presented in Rodr´ıguez-G´alvez et al. (2021) is an extension of that in Haghifam et al. (2020) from
full-batch setting to mini-batch setting . We also evaluate the optimization performance of proposed
Noisy Sign-SGD by comparing it with the original sign-SGD (Bernstein et al., 2018a) and present
the corresponding generalization bound in Theorem 2.

The details of our model architectures, learning rate scheduling, hyper-parameter selections and
additional experimental results can be found in Appendix E. We acknowledge that we did not achieve
the state-of-the art predictive performance, mainly due to the simplicity of our model architectures.
With more complex model and further tuning, the prediction results could be improved.

4.1 STOCHASTIC GRADIENT LANGEVIN DYNAMICS
**Comparison with existing work. We have derived theoretical generalization error bounds that**
depend on the data-dependent quantity gradient discrepancy, i.e., ∥∇ℓ (wt, zn) −∇ℓ (wt, zn[′] [)][∥][2]2[.]
Existing bounds in Li et al. (2020) and Negrea et al. (2019) have also improved the Lipschitz constant
in Mou et al. (2018) to a data-dependent quantity. As shown in Figure 1 (a)-(d), by combining with
the empirical training error, all four generalization error bounds can be used to bound the empirical
test error, but our bound is able to generate a much tighter upper bound. Such difference is mainly
due to the fact that we replace the squared gradient norm in Li et al. (2020), the squared norm of


-----

(a) MNIST, αt[2] (b) CIFAR-10, αt[2] (c) Fashion, αt[2] (d) Fashion, αt[2]

_[≈]_ [0][.][1] _[≈]_ [0][.][1] _[≈]_ [0][.][1] _[≈]_ [0][.][01]

(e) MNIST, αt[2] (f) CIFAR-10, αt[2] (g) Fashion, αt[2] (h) Fashion, αt[2]

(i) MNIST, αt[2] (j) CIFAR-10, αt[2] (k) Fashion, αt[2] (l) Fashion, αt[2]

_[≈]_ [0][.][1] _[≈]_ [0][.][1] _[≈]_ [0][.][1] _[≈]_ [0][.][01]

Figure 1: Numerical results for training CNN using SGLD (σt = 2ηt/βt) on MNIST, Fashion
MNIST and CIFAR-10. X-axis shows the number of training epochs. (a)-(d) shows our bound is

p

non-vacuous and can be used to bound the empirical test error. (e)-(h) compare our bound with
the existing bounds and show the effect on αt[2][. (i)-(l) show the key factors in each bound, i.e.,]
the squared gradient norm in Li et al. (2020), the gradient incoherence in Negrea et al. (2019),
the two-sample incoherence in Rodr´ıguez-G´alvez et al. (2021), and the gradient discrepancy in our
bound. Our bounds are numerically sharper than existing bounds, and larger αt[2] [leads to tighter]
generalization bounds which is consistent with the theoretical analysis.

_gradient incoherence in Negrea et al. (2019), and that of two-sample incoherence in Rodr´ıguez-_
G´alvez et al. (2021) with the gradient discrepancy. Results in Figure 1 (e)-(h) show that our bounds
are much sharper than those of Li et al. (2020) because our gradient discrepancy (Figure 1 (i)-(l))
is usually 2-4 order of magnitude smaller than the squared gradient norms appeared in Li et al.
(2020). Our bounds are also sharper than those of Negrea et al. (2019) and Rodr´ıguez-G´alvez et al.
(2021) due to an improved dependence on n from an order of 1/[√]n to 1/n. Note that, even though
the gradient incoherence in Negrea et al. (2019) is about 1 to 2 order of magnitude smaller than
the gradient discrepancy for simple problems such as MNIST and Fashion-MNIST, the difference
between the gradient incoherence and our gradient discrepancy reduces as the problem becomes
harder (see results for CIFAR-10 in Figure 1(j)).

**Effect of Randomness. Motivated by Zhang et al. (2017), we train CNN with SGLD on a smaller**
subset of MNIST dataset (n = 10000) with randomly corrupted labels. The corruption fraction
varies from 0% (without label corruption) to 60%. As shown in Figure 2 (d), for long enough training
time, all experiments with different level of label randomness can achieve almost zero training error.
However, the one with higher level of randomness has higher generalization/test error (Figure 2 (a)
dashed lines). Our generalization bound also becomes larger as the randomness increases since the
corresponding gradient discrepancy increases.

4.2 NOISY SIGN-SGD

**Optimization. Figure 3 (a)-(d) show the training dynamics of Noisy Sign-SGD under various se-**
lections of αt. As αt 0, Noisy Sign-SGD matches both the optimization trajectory as well as the
final test accuracy of the original Sign-SGD (Bernstein et al., 2018a). However, as → _αt increases, the_
probability of getting 1 approaches 0.5, and ξt approximates a uniform distribution. As a result, the
corresponding Noisy Sign-SGD still converges, but the generalization performance is much worse.


-----

(a) Bounded Test Error (b) Our Bound (c) Gradient Discrepancy (d) Training Error

Figure 2: Numerical results for training CNN using SGLD (σt = 0.2ηt) on a subset of MNIST (n =
10000) with different randomness on labels. (a) demonstrates that, as the randomness increases, the
empirical test error (dashed lines) increases but still can be bounded by our generalization bound by
combining the empirical training error (solid lines). (b) presents our bound in Theorem 2. (c) shows
the gradient discrepancy ∥∇ℓ (wt, zn) −∇ℓ (wt, zn[′] [)][∥][2]2[. (d) plots the training error. The gradient]
discrepancy increases as randomness increases, so does our generalization bound.

(a) CNN, MNIST (b) CNN, Fashion (c) ResNet-18, CIFAR10 (d) ResNet-18, CIFAR100

(e) MNIST, αt = 1 (f) MNIST, αt = 0.01 (g) Fashion, αt = 1 (h) Fashion, αt = 0.01
Figure 3: (a)-(d) show the training dynamics of CNN on MNIST and Fashion-MNIST, and ResNet18 on CIFAR-10 and CIFAR-100 using noisy sign-SGD with different scaling αt. Legends indicate
sign-SGD matches both the optimization trajectory as well as the final test accuracy of the originalthe choice of αt and the numbers in brackets are test errors at convergence. As αt → 0, Nosiy
sign-SGD (Bernstein et al., 2018a). (e)-(f) show that empirical test error can be bounded by our
bound and the corresponding training error. The larger αt is the sharper our bound is.

**Generalization Bound. Figure 3(e)-(f) show that our bound successfully bounds the empirical test**
error. The larger αt is the sharper the upper bound is. However, larger αt would slow down and
adversely affect the optimization, e.g., Figure 3 (a)-(d) blue and orange lines. In practice, one needs
to balance the optimization error and generalization by choosing a suitable scaling αt.

5 CONCLUSIONS

Inspired by recent advances in stability based and information theoretic approaches to generalization
bounds (Mou et al., 2018; Pensia et al., 2018; Negrea et al., 2019; Li et al., 2020; Haghifam et al.,
2020), we have presented a framework for developing such bounds based on expected stability for
noisy stochastic iterative (NSI) learning algorithms. We have also introduced Exponential Family
Langevin Dynamics (EFLD), a family of noisy gradient descent algorithms based on exponential
family noise, including SGLD and Noisy Sign-SGD as two special cases. We have developed an
expected stability based generalization bound applicable to any EFLD algorithm with a O(1/n)
sample dependence and a dependence on gradient incoherence, rather than gradient norms. Further,
we have provided optimization guarantees for EFLD and establish such results for Noisy Sign-SGD
and SGLD. Our experiments on various benchmarks illustrate that our bounds are non-vacuous and
quantitatively much sharper than existing bounds (Li et al., 2020; Negrea et al., 2019).


-----

REFERENCES

Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. Qsgd:
Communication-efficient sgd via gradient quantization and encoding. In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural
_Information Processing Systems 30, pp. 1709–1720. Curran Associates, Inc., 2017._

Hilal Asi, John Duchi, Alireza Fallah, Omid Javidbakht, and Kunal Talwar. Private adaptive gradient
methods for convex optimization. In International Conference on Machine Learning, pp. 383–
392. PMLR, 2021.

Arindam Banerjee, Srujana Merugu, Inderjit S Dhillon, and Joydeep Ghosh. Clustering with bregman divergences. Journal of machine learning research, 6(10), 2005.

Ole Barndorff-Nielsen. Information and exponential families: in statistical theory. John Wiley &
Sons, 2014.

Raef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efficient
algorithms and tight error bounds. In 2014 IEEE 55th Annual Symposium on Foundations of
_Computer Science, pp. 464–473. IEEE, 2014._

Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Guha Thakurta. Private stochastic
convex optimization with optimal rates. _Advances in neural information processing systems,_
2019.

Raef Bassily, Vitaly Feldman, Crist´obal Guzm´an, and Kunal Talwar. Stability of stochastic gradient
descent on nonsmooth convex losses. Advances in Neural Information Processing Systems, 33,
2020.

Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree Anandkumar.
signsgd: Compressed optimisation for non-convex problems. In International Conference on
_Machine Learning, pp. 560–569. PMLR, 2018a._

Jeremy Bernstein, Jiawei Zhao, Kamyar Azizzadenesheli, and Anima Anandkumar. signsgd with
majority vote is communication efficient and fault tolerant. In International Conference on Learn_ing Representations, 2018b._

St´ephane Boucheron, G´abor Lugosi, and Pascal Massart. Concentration inequalities: A nonasymp_totic theory of independence. Oxford university press, 2013._

Olivier Bousquet and Andr´e Elisseeff. Stability and generalization. Journal of Machine Learning
_Research, 2:499–526, 2002._

Olivier Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy. Sharper bounds for uniformly stable
algorithms. In Conference on Learning Theory, pp. 610–626. PMLR, 2020.

Lawrence D Brown. Fundamentals of statistical exponential families: with applications in statistical
decision theory. Ims, 1986.

Yuheng Bu, Shaofeng Zou, and Venugopal V Veeravalli. Tightening mutual information based
bounds on generalization error. In 2019 IEEE International Symposium on Information Theory
_(ISIT), pp. 587–591. IEEE, 2019._

Mark Bun, Cynthia Dwork, Guy N Rothblum, and Thomas Steinke. Composable and versatile
privacy via truncated cdp. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory
_of Computing, pp. 74–86, 2018._

Cl´ement L Canonne, Gautam Kamath, and Thomas Steinke. The discrete gaussian for differential
privacy. In NeurIPS, 2020.

Xiangyi Chen, Tiancong Chen, Haoran Sun, Zhiwei Steven Wu, and Mingyi Hong. Distributed
training with heterogeneous data: Bridging median-and mean-based algorithms. arXiv preprint
_arXiv:1906.01736, 2019._


-----

Luc Devroye and Terry Wagner. Distribution-free inequalities for the deleted and holdout error
estimates. IEEE Transactions on Information Theory, 25(2):202–207, 1979.

Andre Elisseeff, Theodoros Evgeniou, Massimiliano Pontil, and Leslie Pack Kaelbing. Stability of
randomized learning algorithms. Journal of Machine Learning Research, 6(1), 2005.

Vitaly Feldman and Jan Vondrak. Generalization bounds for uniformly stable algorithms. In Pro_ceedings of the 32nd International Conference on Neural Information Processing Systems, pp._
9770–9780, 2018.

Vitaly Feldman and Jan Vondrak. High probability generalization bounds for uniformly stable algorithms with nearly optimal rate. In Conference on Learning Theory, pp. 1270–1279. PMLR,
2019.

Peter Gr¨unwald, Thomas Steinke, and Lydia Zakynthinou. Pac-bayes, mac-bayes and conditional mutual information: Fast rate bounds that handle general vc classes. _arXiv preprint_
_arXiv:2106.09683, 2021._

Mahdi Haghifam, Jeffrey Negrea, Ashish Khisti, Daniel M Roy, and Gintare Karolina Dziugaite.
Sharpened generalization bounds based on conditional mutual information and an application to
noisy, iterative algorithms. Advances in Neural Information Processing Systems, 2020.

Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In International Conference on Machine Learning, pp. 1225–1234, 2016.

Fredrik Hellstr¨om and Giuseppe Durisi. Generalization bounds via information density and conditional information density. IEEE Journal on Selected Areas in Information Theory, 1(3):824–839,
2020.

Fredrik Hellstr¨om and Giuseppe Durisi. Fast-rate loss bounds via conditional information measures
with applications to neural networks. In 2021 IEEE International Symposium on Information
_Theory (ISIT), pp. 952–957. IEEE, 2021._

Peng Jiang and Gagan Agrawal. A linear speedup analysis of distributed deep learning with sparse
and quantized communication. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. CesaBianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 2525–
2536. Curran Associates, Inc., 2018.

Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle
points efficiently. In International Conference on Machine Learning, pp. 1724–1732, 2017.

Chi Jin, Praneeth Netrapalli, Rong Ge, Sham M Kakade, and Michael I Jordan. On nonconvex
optimization for machine learning: Gradients, stochasticity, and saddle points. arXiv preprint
_arXiv:1902.04811, 2019._

Richeng Jin, Yufan Huang, Xiaofan He, Tianfu Wu, and Huaiyu Dai. Stochastic-sign sgd for federated learning with theoretical guarantees. arXiv preprint arXiv:2002.10940, 2020.

Peter Kairouz, M´onica Ribero, Keith Rush, and Abhradeep Thakurta. Dimension independence in
unconstrained private erm via adaptive preconditioning. arXiv preprint arXiv:2008.06570, 2020.

Alex Krizhevsky. Learning Multiple Layers of Features from Tiny Images. Technical Report Vol.
1. No. 4., University of Toronto, 2009.

Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

Jian Li, Xuanyuan Luo, and Mingda Qiao. On generalization error bounds of noisy gradient methods
for non-convex learning. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=SkxxtgHKPS.

Wenlong Mou, Liwei Wang, Xiyu Zhai, and Kai Zheng. Generalization bounds of sgld for nonconvex learning: Two theoretical viewpoints. In Conference on Learning Theory, pp. 605–638.
PMLR, 2018.


-----

Jeffrey Negrea, Mahdi Haghifam, Gintare Karolina Dziugaite, Ashish Khisti, and Daniel M Roy.
Information-theoretic generalization bounds for sgld via data-dependent estimates. In Advances
_in Neural Information Processing Systems, 2019._

Gergely Neu. Information-theoretic generalization bounds for stochastic gradient descent. arXiv
_preprint arXiv:2102.00931, 2021._

Ankit Pensia, Varun Jog, and Po-Ling Loh. Generalization error bounds for noisy, iterative algorithms. In 2018 IEEE International Symposium on Information Theory (ISIT), pp. 546–550. IEEE,
2018.

David Pollard. A user’s guide to measure theoretic probability. Number 8. Cambridge University
Press, 2002.

Borja Rodr´ıguez-G´alvez, Germ´an Bassi, Ragnar Thobaben, and Mikael Skoglund. On random
subset generalization error bounds and the stochastic gradient langevin dynamics algorithm. In
_2020 IEEE Information Theory Workshop (ITW), pp. 1–5. IEEE, 2021._

William H Rogers and Terry J Wagner. A finite sample distribution-free performance bound for
local discrimination rules. The Annals of Statistics, pp. 506–514, 1978.

Daniel Russo and James Zou. Controlling bias in adaptive data analysis using information theory.
In Artificial Intelligence and Statistics, pp. 1232–1240. PMLR, 2016.

Igal Sason and Sergio Verdu. f -divergence inequalities. IEEE Transactions on Information Theory,
62, 2016.

Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Convergence results and optimal averaging schemes. In International conference on machine learning,
pp. 71–79. PMLR, 2013.

Thomas Steinke and Lydia Zakynthinou. Reasoning about generalization via conditional mutual
information. In Conference on Learning Theory, pp. 3437–3452. PMLR, 2020.

Alexandre B Tsybakov. Introduction to nonparametric estimation. Springer Science & Business
Media, 2008.

Martin J Wainwright and Michael Irwin Jordan. Graphical models, exponential families, and vari_ational inference. Now Publishers Inc, 2008._

Di Wang and Jinhui Xu. Differentially private empirical risk minimization with smooth non-convex
loss functions: A non-stationary view. In Proceedings of the AAAI Conference on Artificial Intel_ligence, volume 33, pp. 1182–1189, 2019._

Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In
_International Conference on Machine Learning, ICML ’11, pp. 681–688, 2011._

Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.

Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learning algorithms. Advances in Neural Information Processing Systems, 2017:2525–2534, 2017.

Guandao Yang, Tianyi Zhang, Polina Kirichenko, Junwen Bai, Andrew Gordon Wilson, and Christopher De Sa. Swalp: Stochastic weight averaging in low-precision training. 36th International
_Conference on Machine Learning (ICML), 2019._

Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In 5th International Conference on Learning
_Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings._
OpenReview.net, 2017. URL https://openreview.net/forum?id=Sy8gdB9xx.

Huanyu Zhang, Ilya Mironov, and Meisam Hejazinia. Wide network learning with differential privacy. arXiv preprint arXiv:2103.01294, 2021.


-----

Ruida Zhou, Chao Tian, and Tie Liu. Individually conditional individual mutual information bound
on generalization error. In 2021 IEEE International Symposium on Information Theory (ISIT),
pp. 670–675. IEEE, 2021.

Yingxue Zhou, Steven Wu, and Arindam Banerjee. Bypassing the ambient dimension: Private sgd
with gradient subspace identification. In International Conference on Learning Representations,
2020.


-----

