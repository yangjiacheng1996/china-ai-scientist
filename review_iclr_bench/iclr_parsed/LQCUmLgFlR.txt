# ON OPTIMAL EARLY STOPPING: OVERPARAMETRIZA## TION VERSUS UNDERPARAMETRIZATION

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Early stopping is a simple and widely used method to prevent over-training neural networks. We develop theoretical results to reveal the relationship between
optimal early stopping time and model dimension as well as sample size of the
dataset for certain linear regression models. Our results demonstrate two very different behaviors when the model dimension exceeds the number of features versus
the opposite scenario. While most previous works on linear models focus on the
latter setting, we observe that in common deep learning tasks, the dimension of
the model often exceeds the number of features arising from data. We demonstrate experimentally that our theoretical results on optimal early stopping time
corresponds to the training process of deep neural network. Moreover, we study
the effect of early stopping on generalization and demonstrate that optimal early
stopping can help mitigate “double descent” in various settings.

1 INTRODUCTION

Generalization, accuracy, and computation are three of the major aspects in deploying large scale
machine learning models. They are often brought together into an optimization framework, where
the first two aspects concern stationary solution and the last aspect concerns convergence properties of the algorithm. As a result, most previous works have analyzed them separately, seeking to
strike a balance between generalization and accuracy first then understand computational complexity (Zhang, 2002). These approaches often design a regularizer in the form of a penalty term added
to the objective function (c.f., Nakkiran et al., 2020b, and references therein). While this method
is effective in many settings, it can prove challenging to determine the regularization that guarantees appealing behaviors. Adding to the complication is the fact that they are often designed to
avoid overfitting when the optimization algorithm converges. It is not obvious how the behavior will
change if the algorithm stops before the convergence of the optimization procedure.

Recent success of deep learning models combined with gradient based optimization has prompted
increasingly many works to focus on blending the computation aspect into the picture of
generalization–accuracy tradeoff (Du et al., 2018). Such approaches often leverage implicit properties of the optimization algorithms in conjunction of the model and the data structures and are thus
referred to as implicit regularization methods. In training of deep neural networks, techniques such
as stochastic gradient descent, batch normalization (Ioffe & Szegedy, 2015), and dropouts (Srivastava et al., 2014) are widely used to grant generalization properties implicitly.

In this paper, we study regularization via early stopping (Morgan & Bourlard, 1989; Zhang & Yu,
2005; Yao et al., 2007). While in practice, early stopping is often forced by computational budget
constraints, we focus on its efficacy in avoiding overfitting towards the training dataset. In the
presence of label noise, in particular, we study the common strategy of concluding the training
process at the optimal early stopping time, which achieves the lowest test risk before it increases
again. One crucial question is how this optimal early stopping time relates to the model size and
sample size. Answering this question not only provides guidance for the model training process in
practice, but also contributes to the understanding of the generalization property of different models.

**A Conundrum on Optimal Early Stopping** (Ali et al., 2019) studied the regularization effect
of early stopping on least square regression, but did not provide explicit characterization of the
optimal early stopping time with respect to the model and data complexity. As a first step, we


-----

Figure 2: Optimal early stopping time vs. hidden layer
width trained using two-layer
ReLU networks on randomly
generated points of p = 10 clusters with 20% label noise.


Figure 1: Left: Optimal early stopping time vs. model width d for
CIFAR-100 with 20% label noise trained using ResNet networks
with convolutional layers of widths [d, 2d, 4d, 8d]. Right: Testing
error vs. time for CIFAR-100 with 20% label noise trained using
ResNet networks using different model widths.


compute the optimal early stopping time of the model studied in (Ali et al., 2019) and discover
that under the setting adopted therein, the optimal early stopping time increases along with the
model size. However, this trend is contrary to what we observed in traning neural networks. We
observed that when training popular datasets such as MNIST, CIFAR-10/100 using deep neural
networks, the optimal early stopping time decreases as the model size increases (see Figure 1).
These two contradicting trends indicate that previous theoretical models are not sufficient to explain
what happens in practice.

**Overparametrization vs. Underparametrization** We noticed that despite the huge models used
to train those datasets, their labels are actually generated by low dimensional representations or
features (See Section 4.2). Previous theoretical works have primarily focused on the case where
the model dimension is approximately the same or less than the number of features instead of the
opposite scenario (Ali et al., 2019; Belkin et al., 2020; Nakkiran et al., 2020b). To complete the
missing pieces in previous analyses, we propose a new model where the label is generated by a
low-dimensional transformation of the input data. We define the case where the model size is
larger than the number of features as being overparametrized. We verify experimentally that this
overparametrized case better reflects what happens in training neural networks. This new overparametrization setting demonstrates very different behavior from the previous theoretical model,
which resolves the contradiction we previously observed.

We illustrate both theoretically and experimentally that the optimal stopping time behaves very differently when the number of features is smaller (overparametrization) or larger (underparametrization) than the model size. In the overparametrization setting, the optimal early stopping time decreases as the model dimension increases or as the sample size decreases, similar to what we observe
when training deep neural networks for image classification tasks. On the other hand, in the underparametrization setting, the optimal early stopping time increases as the model dimension increases
or as the number of samples increases.We corroborate experimentally that these trends persist when
training various linear models and neural networks (e.g., Figure 2).

**Generalization of the early stopped-models** Even for the same model, stopping at the optimal
early stopping time versus after the training converges can lead to solutions with very different
generalization properties. Another problem we study is how the test risk of models changes as a
function of other training parameters when stopped at the optimal early stopping time. In particular,
it has been shown that without regularization, the test risk can exhibit the so-called “double descent”
phenomenon (Belkin et al., 2018; Nakkiran et al., 2020a), where the risk as a function of model
size or sample size experience two distinct phases of descent. This phenomenon can make deciding
the best model parameters challenging. We demonstrate that early stopping helps mitigate double
descent in multiple settings.


1.1 RELATED WORKS

The test risk of simplified machine learning models has been studied in a long line of works. A partial list of papers that studied linear models similar to ours includes (Bartlett et al., 2020; Chen et al.,


-----

2020a; Dobriban et al., 2018; Hastie et al., 2019; Montanari et al., 2019; Mitra, 2019; Muthukumar
et al., 2020)

Regularization is widely used in training machine learning models to prevent overfitting. (Dobriban
et al., 2018; Dobriban & Sheng, 2020; Hastie et al., 2019; Kobak et al., 2020; Mei & Montanari,
2019; Nakkiran et al., 2020b) studied the test risk of ridge regression. In addition, a lot of recent
works studied the theoretical guarantee of implicit regularization in model training (Derezi´nski et al.,
2019; Vaskevicius et al., 2019; HaoChen et al., 2020; Blanc et al., 2020; Razin & Cohen, 2020). In
our work, we focus on regularization using early stopping. (Li et al., 2020) proved that early stopping
is robust to label noise in certain settings. (Ali et al., 2019; 2020) related the risk of ridge regression
to that of early stopping. (Vaˇskeviˇcius et al., 2020) studied early stopped mirror descent. Our work
characterizes the time that achieves the optimal stopping in various settings and demonstrates how
optimal early stopping affects generalization.

The double descent phenomenon has a long history of being studied in the literature. (Loog et al.,
2020) gave a brief history of the early works studying this topic. A partial list of works that tackled
related problem in the setting of least square regression includes (Belkin et al., 2018; 2020; Bartlett
et al., 2020; Deng et al., 2019; Hastie et al., 2019; Mei & Montanari, 2019; Muthukumar et al., 2020;
d’Ascoli et al., 2020; Chen et al., 2020a; Yang et al., 2020). (Nakkiran et al., 2020a) demonstrated
double descent can also occur as a function of number of samples. (Nakkiran et al., 2020b) shows
that optimal regularization can mitigate double descent. Our work differs from previous works by
studying the effect of early stopping on double descent.

Finally, our work studies the low rank structure of the feature representations. (Li et al., 2018;
Dusenberry et al., 2020) have also empirically explored and leveraged this property.

2 OVERPARAMETRIZATION

In this section, we introduce and analyze the overparametrization setting, where the model size
exceeds the number of features. This setting completes the missing piece of previous theoretical
analysis on large models with excessive parameters. For data (x, y), the input x ∈ R[d] is from a high
dimensional space. The label y ∈ R is generated from a low-dimensional feature representation
_z ∈_ R[p], p ≤ _d, which is mapped from x by a low-dimensional transformation. A lot of common_
deep learning applications falls under this setting. Despite the huge model size used, the underlying
feature space dimension can be low. For example, we observe that the labels y in CIFAR10, a widely
used image classification problem with 10 classes, can be generated by a feature space of dimension
a little more than 10 (See Section 4.2).

Formally, we consider the following linear regression problem with d > p. Let the covariate/input
matrix X ∈ R[n][×][d] be a random matrix such that each entry of it is generated from N (0, 1) independently. Let x1, ..., xn be the row vectors of the matrix X. For a semi-orthogonal matrix P ∈ R[p][×][d]
with p < d and some unknown parameter θ[∗], the response is generated by

_yi = ⟨Pxi, θ[∗]⟩_ + ϵi, (1)

(where the label noisexi, yi). Let ϵ ∈ R[n] be the vector with entries given by ϵi ∼N (0, σ[2]) is independent of ϵ xii. and is generated independently for each

We let Sover denote the overparametrization setting studied in this section. This setting is inspired by
the overparametrization property of many real datasets and models. For example, to classify images
using deep neural network, the labels can usually be determined by features extracted from certain
important parts of the images. The other parts of the image, though do not contribute to the label
much, are still input to the model. When the neural network is wide, the network has the ability to
extract far more features than what contribute to the label. This character of the deep neural network
is what the setting Sover attempts to model.

Our data assumption has been studied in many related literature (Belkin et al., 2020; Nakkiran et al.,
2020b). Our analysis only relies on the concentration property of singular values of the data matrix
_X, so it is possible relax the data assumptions on X to bounds on the minimum and maximum_
singular values of the data matrix (Vaswani & Nayer, 2016). The linear model has been widely used
as the first step to study more complicated models as well due to its simple form (Bartlett et al.,
2020; Chen et al., 2020a; Dobriban et al., 2018; Hastie et al., 2019; Montanari et al., 2019; Mitra,


-----

2019; Muthukumar et al., 2020). It is possible to extend our results to non-linear settings such as the
random feature setting. In this paper, we stick to the simplest model so that we can get a simple and
explicit characterization of the optimal early stopping time.

For fixed semi-orthogonal matrix P and parameter θ[∗], let _P,θ∗_ be the joint distribution of a single
_D_
data point (x, y) given P and θ[∗]. For an estimator β, define the population risk

_R(β) = E(˜x,y˜)∼DP,θ∗_ (⟨x, β˜ _⟩−_ _y˜)[2][i]_ _._
h

We consider the standard linear least squares problem


1
min 2 _[.]_ (2)
_β∈R[d]_ 2n _[∥][y][ −]_ _[Xβ][∥][2]_

Applying gradient flow starting from 0 on equation 2 gives a continuous differential equation over
time t,
_d_

(3)

_dt_ _[β][X,y][(][t][) =][ X]n[⊤]_ [(][y][ −] _[Xβ][X,y][(][t][))][.]_

with an initial condition βX,y(0) = 0. We can solve the differential equation exactly and obtain


+
_β(t) =_ _X_ _[⊤]X_ _I −_ exp _−tX_ _[⊤]X/n_ _X_ _[⊤]y,_ (4)

for all t ≥ 0 (Ali et al., 2019). Here, for any matrix     _A , exp(A) is defined as_ exp(A) := _n=0_ _An[n]!_ [.]

In practice, one would discretize the gradient flow and perform gradient descent. We discuss the
error due to such discretization in Appendix E.

[P][∞]

For any fixed dataset y ∈ R[n] and X ∈ R[n][×][d] given by fixed P and ϵ, let βX,y(t) be the gradient
flow solution of standard linear regression problem at time t. We further assume that P ∈ R[p][×][d] is a
uniformaly random semi-orthogonal matrix.

Our goal is to study the expected risk of the estimator βX,y(t) over P and ϵ. We denote the expected
risk at time t for dataset size n as

_RX,θ∗_ (t) = EP,ϵ [R(βX,y(t))] .

We study the optimal early stopping time, the time t that achieves the first local minimum of the
expected risk. We characterize the optimal early stopping time for different parameter choices. We
let topt denote the optimal stopping time, omitting (X, θ[∗]) when clear from context,


_d_

(5)

_dt_ _[R][X,θ][∗]_ [(][t][′][)][ <][ 0][ for][ 0][ < t][′][ < t,][ and][ d]dt _[R][X,θ][∗]_ [(][t][) = 0][.]


_topt(X, θ[∗]) = min t s.t._


2.1 OPTIMAL EARLY STOPPING TIME

With the gradient flow solution βX,y, we can derive a high probability upper and lower bound on
_topt._

**Theorem 1 (Optimal early stopping time in overparametrization setting). In setting** _over, for a_

2 _S_

_fixed parameter θ[∗]_ _and noise variance σ[2], when n_ _d, let γ =_ _√n+√√2 logd_ _n_ _. For γ_ 1, with
_≤_ _≤_

_probability at least 1 −_ _n[2]_ _[over the randomness of][ X][, the optimal early stopping time]_  _[ t][opt][ satisfies]_

_n_ 1 _γ_ 2 _θ∗_ 22 _n_ 1 + _γ_ 2 _θ∗_ 22

1 + _γ_ 2 d log 1 +   _−_ _[√]σ[2]_ _∥_ _∥_ ! _≤_ _topt ≤_ 1 _γ_ 2 d log 1 +   _[√]σ[2]_ _∥_ _∥_ ! _._

_[√]_ _−_ _[√]_

2

_When _ _n > d_ _, let γ =_ _√d+√[√]n2 log d_ _. For γ_ 1, with probability at least   1 _d_ _[over the randomness]_

_≥_ _−_ [2]

_of X, the optimal early stopping time_  _topt satisfies_


2 n _θ∗_ 22
_∥_ _∥_

_dσ[2]_




2 n _θ∗_ 22
_∥_ _∥_

_dσ[2]_




1 + 1
_−_ _√[1]γ_



1 + 1 + [1]
_√γ_



2 [log]
1 + _√1γ_



_≤_ _topt ≤_


2 [log]
1 − _√1γ_



-----

Note that our definitions of n _γ differ slightly in the case n ≤_ _d and n > d, but they are both of_
order Θ[˜] _d_ . Theorem 1 gives an approximation of the optimal stopping time up to a constant

as long as  _γ is constantly bounded away from 1. When γ ≤_ 14 [(roughly][ n][ ≤] [4][d][),omitting the]
logarithmic terms,3(dn+n) [log] 1 + _[n]4[∥]dσ[9]4[θ][n]d[∗][2][∥][≤]_ _≤[t][opt]t[ ≤]opt ≤[2]d[n]_ [. When]d5+nn [log][ γ][ ≥]1 + 3[4][ (roughly][n]dσ[∥][θ][∗][2][∥] .[ n][ ≥]Combining these two cases shows that[4][d][),][ omitting the logarithmic terms,]

when γ is bounded away from  1, topt is about Θ _d+nn_ [log][ n]d . When γ is very close to 1, the upper

bound on topt in Theorem 1 can be loose. We show in Appendix C, that in the asymptotic case where 

_n and d are large and_ _[∥]σ[θ][∗][2][∥]_ is lower bounded, the approximation topt = Θ[˜] _d+nn_ still holds when

1
4 _[< γ <][ 4][. In the asymptotic analysis we also achieve tighter constants in the bounds of the optimal]_ 
early stopping time. Furthermore, in Appendix F, using numerical experiments, we show that our
theoretical result can be a good approximation of the optimal stopping time for all γ.

Now, we discuss some implications of Theorem 1. The following lemma computes RX,θ[∗] (topt), the
expected risk at time topt, and holds for all input matrix X regardless of how it is generated.

**Lemma 1. For all n, p, d ∈** N such that p ≤ _d, let X ∈_ R[n][×][d] _and θ[∗]_ _∈_ R[p] _be fixed. Let_
_λ1 ≥_ _... ≥_ _λd be the eigenvalues of the matrix_ _n[1]_ _[X]_ _[⊤][X][. Then,]_

_d_ _d_

_RX,θ∗_ (t) = σ[2] + exp ( 2tλi) 2 + _[σ][2]_ 1 _λi_ = 0 [1] (1 exp( _tλi))[2]_ _._ (6)

_i=1_ _−_ _[∥][θ]d[∗][∥][2]_ _n_ _i=1_ _{_ _̸_ _}_ _λi_ _−_ _−_

X X

In equation 6, the second term is the approximation error due to early stopping. The third term is
due to overfitting to the label noise, which increases as the training time increases. In presence of
label noise, the optimal early stopping finds the balance point of these two terms.


Theorem 1 shows that the optimal early stopping time is roughly Θ[˜]


_n+n_ _d_, when n and d are around



the same order. When n ≫ _d, the optimal early stopping time is roughly Θ(log_ _[n]d_ [)][.][ It implies that]

**when the number of features is smaller than the model dimension, optimal early stopping time**
**increases as n increases or d decreases. Section F.1 corroborates this trend using experiments. In**
addition, we show in Section 4.2 and Section 4.3 that training deep neural networks on large real
datasets can also follow these two trends. An intuitive explanation of such phenomenon is when the
sample size is large, the label noise has less effect on model training, so the model can be trained
longer before overfitting starts. On the contrary, when d increases, the number of parameters is large
so that even small updates of each parameter can result in overfitting.

2.2 RISK MONOTONICITY


It has been observed empirically that without early stopping or other types of regularization, when
the number of samples n increases, the test error can experience double descent in presence of
label noise. However, optimal early stopping can possibly mitigate the double descent phenomenon
(Nakkiran et al., 2020a). In Proposition 1, we bound the expected risk at the optimal stopping time
for varying data size n and attempt to explain this phenomenon.
**Proposition 1. In setting Sover, for a fixed parameter θ[∗]** _and noise variance σ[2]. Let topt = αn/(n +_
_d) for some constant α. Let λ1 ≥_ _... ≥_ _λd be the eigenvalues of the matrix_ _n[1]_ _[X]_ _[⊤][X][. Then,]_

_R1_ EX [RX,θ∗ (topt)] _R2._ (7)
_≤_ _≤_

_where_

_d_

_θ[∗]_

_R1 = σ[2]_ + EX exp ( 2αnλi/d) _∥_ _∥[2]_ _, and R2 = R1 + [1]_

" _i=1_ _−_ # _d_ 2 _[ασ][2][.]_
X

_R1 and R2 decrease monotonically as n increases._


Theorem 1 shows that the optimal early stopping time is roughly topt ≈ _nαn+d_ [for a small constant]

_α up the logarithmic factors. When_ _[∥]σ[θ][∥][2][∗]_ is not too small, equation 7 gives a small region that

bounds the expected risk at topt. We are able to show that both the upper and the lower bounds of the


-----

region decrease as n increases. This implies that optimal early stopping can mitigate sample-wise
**double descent. We use experiments to corroborate our computation and give more details on this**
observation in Section 4 and Appendix F.

A related question is whether stopping at optimal early stopping time can mitigate the double descent
due to increasing model size d. (Nakkiran et al., 2020a) observed that double descent can still exist at
optimal early stopping. An theoretical analysis of this question can be an interesting future direction.

3 UNDERPARAMETRIZATION

In this section, we study the underparametrization setting, where the number of features exceeds the
model size. This setting shows very different behavior from the overparametrization setting. In this
setting, the label y is generated by some covariate z, which is in a high-dimensional space, but the
model only has access to a low-dimension projection of z.

Formally, we consider the following linear regression problem with d ≤ _p. Let the covariate matrix_
_Z ∈_ R[n][×][p] be a random matrix such that each entry of it is generated from N (0, 1) independently.
Let z1, ..., zn be the row vectors of the matrix Z. For some unknown paratmeter θ[∗], the reponse is
generated by
_yi = ⟨zi, θ[∗]⟩_ + ϵi. (8)
_Pwhere the label noise ∈_ R[p][×][d] with d ≤ _p ϵ, leti ∼N X =(0 ZP, σ[2]. we apply gradient flow on) is independent of zi and y (X, yi. For a semi-orhotgonal matrix). For fixed semi-orthogonal_
matrix P and parameter θ[∗], let _P,θ∗_ be the joint distribution of a single data point (x, y). For an
_D_
estimator β, define the population risk

_R(β) = E(˜x,y˜)∼DP,θ∗_ (⟨x, β˜ _⟩−_ _y˜)[2][i]_ _._
h

Similar as in the overparametrizations setting, we consider the gradient flow solution (equation 4) of
the standard linear least square problem (equation 2). For any fixed dataset y ∈ R[n] and X ∈ R[n][×][d]
given by fixed Z, P and ϵ, let βX,y(t) be the solution of the gradient flow given in equation 4. We
further assume that P ∈ R[p][×][d] is a uniformly random semi-orthogonal matrix.

We let Sunder denote the above setting. Sunder studies the case where we only have partial access to
the features that determine the label y and train the model on what is available.

Our goal is to study the expected risk of the estimator βX,y(t) over Z, P and ϵ. We denote the
expected risk over Z, P and ϵ at time t as

_Rθ∗_ (t) = EZ,P,ϵ [R(βX,y(t))] .

Note that here in Sunder, we take expectation on Z when computing the expected risk. In Sover, we
do not take expectation on X and instead get a high probability bound on X. This slight difference
is due to the different methods we use in analyzing these two settings. We study the optimal early
stopping time, the time t that achieves the first local minimum of the expected risk. We charaterize
the optimal early stopping time for different choices of p, n and d. We let topt denote the optimal
stopping time, omitting θ[∗] when clear from context,


_d_

(9)

_dt_ _[R][θ][∗]_ [(][t][′][)][ <][ 0][ for][ 0][ < t][′][ < t,][ and][ d]dt _[R][θ][∗]_ [(][t][) = 0][.]


_topt(θ[∗]) = min t s.t._


We are able to give a linear approximation of the optimal stopping time, topt.
**Theorem 2 (Optimal early stopping time in underparametrization setting). In setting Sunder, for a**
_early stopping timefixed parameter θ[∗]_ _tand noise varianceopt satisfies_ _σ[2], if (8n + 9d + 16) ∥θ[∗]∥2[2]_ _[≤]_ _[pσ][2][ +][ p][ ∥][θ][∗][∥]2[2][, the optimal]_

_n ∥θ[∗]∥2[2]_ _topt_ 2n∥θ[∗]∥2[2] _._

2 _pσ[2]_ + (p − _d) ∥θ[∗]∥2[2]_ _≤_ _≤_ _pσ[2]_ + (p − _d) ∥θ[∗]∥2[2]_
 

Theorem 2 approximates the early optimal stopping topt up to a constant when p is larger that n and
_d by a constant factor. For d that is close to p, numerical experiments show that the approximation_


-----

Θ˜ _pσ[2]+(np∥θ[∗]d∥)2[2]_ _θ[∗]_ 2 still holds. Theorem 2 implies that when the number of features is larger

_−_ _∥_ _∥[2]_

**than the model dimension, optimal early stopping time increases as**  _n or d increases. An intu-_
itive explanation is when the model is when the model size and the sample size are small, the model
does not have enough information to train so that the model tends to fit the noise easier.
**Proposition 2.n** _θ[∗]_ 2 _In setting Sunder, for a fixed parameter θ[∗]_ _and noise variance σ[2], let topt =_
_∥_ _∥[2]_

_pσ[2]+(p−d)∥θ[∗]∥2[2]_ _[. Then, when][ (8][n][ + 9][d][ + 16)][ ∥][θ][∗][∥]2[2]_ _[≤]_ _[pσ][2][ +][ p][ ∥][θ][∗][∥]2[2][, we have]_

_R1 ≤_ _Rθ[∗]_ (topt) ≤ _R2,_

2nd _θ[∗]_ 2 3nd _θ[∗]_ 2
_where R1 = σ[2]_ + ∥θ[∗]∥2[2] _[−]_ _p[2]σ[2]+p(p∥−d∥)[4]∥θ[∗]∥2[2]_ _[and][ R][2][ =][ σ][2][ +][ ∥][θ][∗][∥]2[2]_ _[−]_ 4(p[2]σ[2]+p(∥p−d∥)[4]∥θ[∗]∥2[2][)] _[.]_


_Here, R1 ≥_ 0.8R2 and both R1 and R2 decrease when d and n increase.

Proposition 2 gives an upper bound and a lower bound on the risk at the approximated optimal early
stopping time. Under the assumptionProposition 2 supports the observation that d and n risk at optimal early stopping time monotonically are small, R1 and R2 give a tight region (R1 ≥ 0.8R2).
**decreases as n and d increases. Numerical experiments are presented in Section F.1.**

4 EXPERIMENT

In this section, we run simulations for settings Sover and Sunder and demonstrate that training deep
neural networks also follows the trends our theoretical analysis identifies. We provide additional
experimental results in Appendix F.

4.1 LINEAR REGRESSION

(We moved the experiments results on linear regression to the appendix so we can have more space
to address the comments from the reviewers.)

4.2 DEEP LEARNING WITH NEURAL NETWORKS

We study the optimal early stopping time in neural network training. We demonstrate that datasets
MNIST, CIFAR-10 and CIFAR-100 have low-rank representations. Such representations show that
training common deep neural networks on these datasets falls under the overparametrization setting.
Then, we demonstrate experimentally that the optimal early stopping time follows the pattern we
identify for the overparametrization setting.

Figure 3: Scaled singular values of representations obtained from MNIST, CIFAR-10 and CIFAR100.

**Existence of low-rank structure in image classification.** Many previous works observe that
low-rank structures exist and can be leveraged in deep image classification tasks (Van der Maaten
& Hinton, 2008; Dusenberry et al., 2020). This structure facilitates the self-supervised learning and
transfer learning paradigm where image features are learned and fixed close to the input layer, reducing the complexity of image-to-label mapping. Then only the last layer is trained to adapt to and
classify out-of-domain images (Chen et al., 2020b). We follow this approach and demonstrate the
existence of the low-rank structures in Figure 3, where we plot the singular values of the representations of MNIST, CIFAR-10, and CIFAR-100. First, we train CNN/ResNet-60 (He et al., 2016a;b) to
convergence with zero training loss (no label noise). Then, we obtain the representations produced
by the trained neural networks on MNIST, CIFAR-10, and CIFAR-100. Even when trained with a
very wide neural network, only the first 10/100 singular values of the representations are significant


-----

compared to the others. This suggests that low-rank feature representations of the datasets exist. We
believe this phenomenon may be of independent interest to understanding neural networks.

(A1) (B1) (C1)

Optimal early stopping time

(A2) (B2) (C2)

Figure 4: In (A1) (B1) (C1), optimal early stopping time decreases with increasing model sizes for
MNIST, CIFAR-10, and CIFAR-100. In (A2) (B2) (C2), optimal early stopping time increases with
increasing sample sizes for MNIST, CIFAR-10, and CIFAR-100.

**Overparameterization behavior in image classification using neural networks** In this section,
we study the optimal early stopping time of MNIST, CIFAR-10, and CIFAR-100. We train MNIST
using a fully connected network, and CIFAR-10/100 using ResNet. Part of our code in this part
builds on open source code from (Nakkiran et al., 2020a). We add 20% of label noise to all datasets
For MNIST, a simple ReLU network with one hidden layer is used. We vary the hidden layer
width over d = [10, 20, ..., 60]. For CIFAR-10/100, we use a family of ResNet networks with
convolutional layers of widths [d, 2d, 4d, 8d] for different layer depths. The sample size ranges
from n = [1e[5], 2e[5], ..., 6e[5]] for MNIST, and n = [1e[5], 2e[5], ..., 5e[5]] for CIFAR-10/100. Inputs are
normalized to [−1, 1] with standard data-augmentation. We use stochastic gradient descent with
cross-entropy loss, learning rate η = 0.1 for MNIST, CIFAR-10, and η = 0.05 for CIFAR-100, and
minibatch size B = 512. The optimal stopping time is computed as topt = ηT _B[n]_ [, where][ T][ is the]

epoch with the optimal early stopping performance. To select the optimal stopping time, we first
apply a moving averaging window with length 5 to mitigate the effect of mini-batch noise. After
smoothing, we select the point with the optimal stopping time with the first minimal testing error
(the lowest point of the first ‘valley’, ignoring small local minima).

We show in Figure 4 the optimal early stopping time for all three datasets with varying n and d. In
(A1) (A2) (A3), we observe that the optimal early stopping time decreases with increasing model
width. In (B1) (B2) (B3), we observe that the optimal early stopping time increases with larger sample sizes. Due to the existence of low-rank representations, Figure 4 falls under the overparametrization setting even for small d. The observations are consistent with our theoretical analysis and the
experimental results on linear models in the overparametrization setting. When trained with ultra
small d < 10, the models show great instability. We provide additional experimental results supporting the underparametrization setting in Section 4.3.

4.3 CLASSIFICATION USING TWO-LAYER NEURAL NETWORKS


Training datasets like CIFAR-10/100 using ultra small models is unstable, so in this section, we use
simpler datasets to verify our theoretical results for the underparametrization setting. In this experiment, we use make classification function in sklearn (Pedregosa et al., 2011) to generate clusters of
points normally distributed around vertices of a high-dimensional hypercube and assigns an equal
number of clusters to each class. We generate 2000 samples (50-50 train-test split) of 20 dimensions/features, but only p = 10 dimensions/features of each sample are informative features. We
add 20% label noise to the dataset. The optimal stopping time is selected similarly as in Section 4.2.


-----

We train the data using two-layer ReLU networks with varying hidden layer width d. The blue area
in Figure 5 gives 0.5 standard deviation of 6 runs and the blue line gives the mean. In Figure 5a,
when d ≤ _p = 10, we observe the underparamatrization behavior where the optimal early stopping_
time increases as the model size d increases. When d ≥ 10 = p, the optimal stopping time decreases
with d. We can further observe that the test loss decreases with increasing model sizes at the optimal
early stopping time.

(a) (b)

Figure 5: (a) Optimal stopping time as a function of model size. (b) Test loss as a function of model
size.

5 CONCLUSION AND FUTURE WORK

In this paper, we propose a new model of overparametrization to study the optimal early stopping
time. This model captures the phenomenon that the model size usually exceeds the number of
features in practice. We believe this model of overparametrization can be of independent interest.
One future direction is the performance of other optimizers or techniques, e.g., stochastic gradient
descent or data augmentation, on this model. Moreover, we give an explicit characterization of the
early stopping time and the resulting model. Future works can explore other regularization properties
of this early stopped model, e.g., whether this early stopping helps when there is a domain shift in
the testing dataset.

Another direction worth exploring is to extend our results to nonlinear settings. Extending our result
to the lazy training regime of neural networks (e.g., random feature regime and neural tangent kernel
regime) can be relatively straightforward. One approach is to first locate the features extracted at
initizalition and then show how gradient flow progresses on the features extracted. Theoretical
analysis of the non-lazy training is still a largely open question, so extending this result to non-lazy
setting can be more challenging. To extend this result to the non-lazy setting for a fully nonlinear
neural network model, one needs to study how the features extracted change and how the weights
on the features change at the same time.


-----

REFERENCES

Alnur Ali, J Zico Kolter, and Ryan J Tibshirani. A continuous-time view of early stopping for least
squares regression. In The 22nd International Conference on Artificial Intelligence and Statistics
, pp. 1370–1378. PMLR, 2019.

Alnur Ali, Edgar Dobriban, and Ryan Tibshirani. The implicit regularization of stochastic gradient
flow for least squares. In Proceedings of the 37th International Conference on Machine Learning
, pp. 233–244. PMLR, 2020.

Peter L Bartlett, Philip M Long, G´abor Lugosi, and Alexander Tsigler. Benign overfitting in linear
regression. Proceedings of the National Academy of Sciences, 117(48):30063–30070, 2020.

Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine learning
practice and the bias-variance trade-off. arXiv preprint arXiv:1812.11118, 2018.

Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. SIAM
Journal on Mathematics of Data Science, 2(4):1167–1180, 2020.

Guy Blanc, Neha Gupta, Gregory Valiant, and Paul Valiant. Implicit regularization for deep neural
networks driven by an ornstein-uhlenbeck like process. In Conference on learning theory, pp.
483–513. PMLR, 2020.

Lin Chen, Yifei Min, Mikhail Belkin, and Amin Karbasi. Multiple descent: Design your own
generalization curve. arXiv preprint arXiv:2008.01036, 2020a.

Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning,
pp. 1597–1607. PMLR, 2020b.

St´ephane d’Ascoli, Levent Sagun, and Giulio Biroli. Triple descent and the two kinds of overfitting:
Where & why do they appear? arXiv preprint arXiv:2006.03509, 2020.

Kenneth R Davidson and Stanislaw J Szarek. Local operator theory, random matrices and banach
spaces. Handbook of the geometry of Banach spaces, 1(317-366):131, 2001.

Zeyu Deng, Abla Kammoun, and Christos Thrampoulidis. A model of double descent for highdimensional binary linear classification. arXiv preprint arXiv:1911.05822, 2019.

Michał Derezi´nski, Feynman Liang, and Michael W Mahoney. Exact expressions for double descent
and implicit regularization via surrogate random design. arXiv preprint arXiv:1912.04533, 2019.

Edgar Dobriban and Yue Sheng. Wonder: Weighted one-shot distributed ridge regression in high
dimensions. Journal of Machine Learning Research, 21(66):1–52, 2020.

Edgar Dobriban, Stefan Wager, et al. High-dimensional asymptotics of prediction: Ridge regression
and classification. The Annals of Statistics, 46(1):247–279, 2018.

Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous
models: Layers are automatically balanced. arXiv preprint arXiv:1806.00900, 2018.

Michael Dusenberry, Ghassen Jerfel, Yeming Wen, Yian Ma, Jasper Snoek, Katherine Heller, Balaji
Lakshminarayanan, and Dustin Tran. Efficient and scalable bayesian neural nets with rank-1
factors. In International conference on machine learning, pp. 2782–2792. PMLR, 2020.

Jeff Z HaoChen, Colin Wei, Jason D Lee, and Tengyu Ma. Shape matters: Understanding the
implicit bias of the noise covariance. arXiv preprint arXiv:2006.08680, 2020.

Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in highdimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016a.


-----

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630–645. Springer, 2016b.

Roger A Horn and Charles R Johnson. Matrix analysis . Cambridge university press, 2012.

Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning, pp. 448–456.
PMLR, 2015.

Dmitry Kobak, Jonathan Lomond, and Benoit Sanchez. The optimal ridge penalty for real-world
high-dimensional data can be zero or negative due to the implicit ridge regularization. Journal of
Machine Learning Research, 21(169):1–16, 2020.

Chunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the intrinsic dimension
of objective landscapes. arXiv preprint arXiv:1804.08838, 2018.

Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gradient descent with early stopping is
provably robust to label noise for overparameterized neural networks. In International Conference
on Artificial Intelligence and Statistics, pp. 4313–4324. PMLR, 2020.

Marco Loog, Tom Viering, Alexander Mey, Jesse H Krijthe, and David MJ Tax. A brief prehistory
of double descent. Proceedings of the National Academy of Sciences, 117(20):10625–10626,
2020.

Song Mei and Andrea Montanari. The generalization error of random features regression: Precise
asymptotics and double descent curve. arXiv preprint arXiv:1908.05355, 2019.

Partha P Mitra. Understanding overfitting peaks in generalization error: Analytical risk curves for
_l 2 and l 1 penalized interpolation. arXiv preprint arXiv:1906.03667, 2019._

Andrea Montanari, Feng Ruan, Youngtak Sohn, and Jun Yan. The generalization error of maxmargin linear classifiers: High-dimensional asymptotics in the overparametrized regime. arXiv
preprint arXiv:1911.01544, 2019.

Nelson Morgan and Herv´e Bourlard. Generalization and parameter estimation in feedforward nets:
Some experiments. Advances in neural information processing systems, 2:630–637, 1989.

Vidya Muthukumar, Kailas Vodrahalli, Vignesh Subramanian, and Anant Sahai. Harmless interpolation of noisy data in regression. IEEE Journal on Selected Areas in Information Theory, 1(1):
67–83, 2020.

Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. In International Conference on Learning Representations, 2020a.

Preetum Nakkiran, Prayaag Venkat, Sham Kakade, and Tengyu Ma. Optimal regularization can
mitigate double descent. arXiv preprint arXiv:2003.01897, 2020b.

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research
, 12:2825–2830, 2011.

Noam Razin and Nadav Cohen. Implicit regularization in deep learning may not be explainable by
norms. arXiv preprint arXiv:2005.06398, 2020.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929–1958, 2014.

Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
learning research, 9(11), 2008.

Tomas Vaskevicius, Varun Kanade, and Patrick Rebeschini. Implicit regularization for optimal
sparse recovery. Advances in Neural Information Processing Systems, 32:2972–2983, 2019.


-----

Tomas Vaˇskeviˇcius, Varun Kanade, and Patrick Rebeschini. The statistical complexity of earlystopped mirror descent. arXiv preprint arXiv:2002.00189, 2020.

Namrata Vaswani and Seyedehsara Nayer. A simple generalization of a result for random matrices
with independent sub-gaussian rows. arXiv preprint arXiv:1612.00127, 2016.

Zitong Yang, Yaodong Yu, Chong You, Jacob Steinhardt, and Yi Ma. Rethinking bias-variance
trade-off for generalization of neural networks. In International Conference on Machine Learning
, pp. 10767–10777. PMLR, 2020.

Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto. On early stopping in gradient descent learning. Constructive Approximation, 26(2):289–315, 2007.

Tong Zhang. Covering number bounds of certain regularized linear function classes. Journal of
Machine Learning Research, 2(Mar):527–550, 2002.

Tong Zhang and Bin Yu. Boosting with early stopping: Convergence and consistency. The Annals
of Statistics, 33(4):1538–1579, 2005.


-----

