# Safe Linear-Quadratic Dual Control with Almost Sure Performance Guarantee

**Anonymous authors**
Paper under double-blind review

ABSTRACT

This paper considers the linear-quadratic dual control problem where the system
parameters need to be identified and the control objective needs to be optimized
in the meantime. Contrary to existing works on data-driven linear-quadratic regulation, which typically provide error or regret bounds within a certain probability,
we propose an online algorithm that guarantees the asymptotic optimality of the
controller in the almost sure sense. Our dual control strategy consists of two parts:
a switched controller with time-decaying exploration noise and Markov parameter
inference based on the cross-correlation between the exploration noise and system
output. Central to the almost sure performance guarantee is a safe switched control strategy that falls back to a known conservative but stable controller when the
actual state deviates significantly from the target state. We prove that this switching strategy rules out any potential destabilizing controllers from being applied,
while the performance gap between our switching strategy and the optimal linear
state feedback is exponentially small. Under our dual control scheme, the parameter inference error scales as O(T _[−][1][/][4+][ϵ]), while the suboptimality gap of control_
performance scales as O(T _[−][1][/][2+][ϵ]), where T is the number of time steps, and ϵ_
is an arbitrarily small positive number. Simulation results on an industrial process
example are provided to illustrate the effectiveness of our proposed strategy.

1 INTRODUCTION

One of the most fundamental and well-studied problems in optimal control, Linear-Quadratic Regulation (LQR) has recently aroused renewed interest in the context of data-driven control and reinforcement learning. Considering it is usually challenging to obtain an exact system model from first
principles, and that the system may slowly change over time due to various reasons, e.g., component
wear-out, data-driven regulation of unknown linear systems has become an active research problem
in the intersection of machine learning and control, with recent works including e.g., Dean et al.
(2019); Mania et al. (2019); Cohen et al. (2019); Wagenmaker & Jamieson (2020). In particular,
from the perspective of reinforcement learning theory, the LQR problem has become a standard
benchmark for continuous control.

In this paper, we focus on the dual control (Feldbaum, 1960) setting, also known as online adaptive
control in the literature, where the same policy must be adopted to identify the system parameters
and optimize the control objective, leading to the well-known exploration-exploitation dilemma.
Recently, it was shown by Simchowitz & Foster (2020) that the optimal regret for this problem setting scales as Θ([˜] _√T_ ), which can be achieved with probability 1 − _δ using a certainty equivalent_

control strategy, where the learner selects control inputs according to the optimal controller for the
current estimate of the system while injecting time-decaying exploration noise. However, the strategy proposed in this work, like those in its predecessors (Abbasi-Yadkori & Szepesv´ari, 2011; Dean
et al., 2018; Mania et al., 2019), may have a nonzero probability δ of failing. Furthermore, it shall
be noticed that δ has been chosen as a fixed design parameter in the aforementioned works, which
implies the probability of failing does not converge to zero even if the policy is run indefinitely. The
above observation gives rise to the question that we address in this paper:

Can we design a learning scheme for LQR dual control, such that the policy
adopted in almost every trajectory converges to the optimal policy?


-----

We identify that the above goal can hardly be achieved by a naive certainty equivalent learning
scheme. Qualitatively, the system parameters learned from data are always corrupted by random
noise, and as a result, the controller proposed in previous works may destabilize the system, albeit
with a small probability, causing catastrophic system failure. Based on the above reasoning, we
propose a notion of bounded-cost safety for the LQR dual control problem: we recognize a learning
scheme to be safe if no destabilizing control policy is applied during the entire learning process.

In this paper, we propose a learning scheme that satisfies the above definition of bounded-cost safety,
and guarantees both the parameter inference error and the suboptimality gap of controller performance converge to zero almost surely. Our strategy consists of two parts: a safe switched controller
and a parameter inference algorithm. The switched controller can be viewed as a safety-augmented
version of the certainty equivalent controller: it normally selects control inputs according to the
optimal linear feedback for the currently estimated system parameters, but falls back to a conservative controller for several steps when the actual state deviates significantly from the target state.
We prove that this switching strategy ensures the bounded-cost safety of the learning process, while
only inducing a suboptimality gap that decays exponentially as the switching threshold increases.
For the parameter inference part, in contrast to the direct least-squares approach for estimating the
matrices A, B widely adopted in the literature, we estimate the Markov parameters, also known as
the impulse response of the system, based on the cross-correlation between the exploration noise and
the system output, and establish the almost-sure convergence using a law of large numbers for martingales. We prefer this approach for its clarity in physical meaning and simplicity of convergence
analysis, but we do not foresee substantial difficulty in replacing our parameter inference module
with standard least-squares. We prove that under the above described learning scheme, the parameter inference error scales as O(T _[−][1][/][4+][ϵ]), while the suboptimality gap of control performance scales_
as O(T _[−][1][/][2+][ϵ]), where T is the number of time steps, and ϵ is an arbitrarily small positive number._
Both the above results match the corresponding asymptotic rates in Simchowitz & Foster (2020),
which provides an rate-optimal algorithm for online LQR in the high-probability regime.

The main contributions of this paper are as follows:

1. We propose a practical notion of safety for the LQR dual control problem that has not been
considered in the literature, and provide an instance of safe learning scheme based on a
switching strategy.

2. We prove almost sure convergence rates of parameter inference error and suboptimality
gap of control performance for our scheme, which match the corresponding optimal rates
in the high-probability regime. To the best of our knowledge, this is the first analysis of the
almost sure convergence rate for online LQR.

The rest of this paper is organized as follows: Section 2 gives a brief introduction of LQR, formulates
the LQR dual control problem, and defines the performance metrics as well as the notion of boundedcost safe learning scheme. Section 3 presents and interprets our algorithm. Section 4 states the main
theoretical results and characterizes the convergence rates. Section 5 provides simulation results
on an industrial process example to illustrate the effectiveness of our proposed strategy. Section 6
summarizes the related literature. Finally, Section 7 gives concluding remarks and discusses future
directions.

2 PROBLEM FORMULATION

We consider the control of the following discrete-time linear system:

_xk+1 = Axk + Buk + wk,_ (1)

whereWe assume xk ∈ xR0[n] is the state vector, (0, X0), wk _uk ∈ (0R, W[p]_ is the input vector, and), and that x0, w0, w1 w, . . .k ∈ are pairwise independent.R[n] is the process noise.
We also assume w.l.o.g. that ∼N (A, B ∼N) is controllable.

We consider control policies of the form

_π : R[n]_ _× R[q]_ _→_ R[p] _× R[q], (uk, ξk+1) = π(xk, ξk),_ (2)

which can be either deterministic or stochastic, whereNotice that we allow the flexibility of the policy being non-Markovian with the introduction of ξk ∈ R[q] is the internal state of the policy. ξk,


-----

and we also use the simplified notation uk = π(xk) if π is Markovian. The performance of a policy
_π can be characterized by the infinite-horizon quadratic cost_


_T −1_

_x[⊤]k_ _[Qx][k]_ [+][ u][⊤]k _[Ru][k]_

" _k=0_
X


_J_ _[π]_ = lim sup
_T →∞_


1

_T_ [E]


(3)


where Q ≻ 0, R ≻ 0 are known weight matrices specified by the system operator.

We denote the optimal cost and the optimal control law by

_J_ _[∗]_ = inf _J_ _[π]._ (4)
_π_ _[J]_ _[π][, π][∗]_ _[∈]_ [arg min]π


It is well-known that the optimal policy is a linear function of the state π[∗](x) = K _[∗]x, with associ-_
ated cost J _[∗]_ = tr (WP _[∗]), where P_ _[∗]_ is the solution to the discrete-time algebraic Riccati equation

_P_ _[∗]_ = Q + A[⊤]P _[∗]A −_ _A[⊤]P_ _[∗]B_ _R + B[⊤]P_ _[∗]B_ _−1 B⊤P ∗A,_ (5)

and the linear feedback control gain K _[∗]_ can be determined by  

_K_ _[∗]_ = − _R + B[⊤]PB_ _−1 B⊤P ∗A._ (6)

Based on the definitions above, we can use  _J_ _[π]_ _J_ _[∗]_ to measure of the suboptimality gap of a specific
_−_
policy π.

In the online LQR setting, the system and input matrices A, B are assumed unknown, and the learning process can be viewed as deploying a sequence of time-varying policies _πk_ with the dual
_{_ _}_
objectives of exploring the system parameters and stabilizing the system. To characterize the safety
of a learning process, we make the definitions below:

**Definition 1. A policy π is destabilizing if J** _[π]_ = +∞.

**Definition 2. A learning process applying policies {πk}k[∞]=0** _[is][ bounded-cost safe][, if][ π][k][ is not desta-]_
_bilizing for any time k and for any realization of the noise process._

Notice that Definition 1 is a generalization of the common notion of destabilizing linear feedback
gain, i.e., π(x) = Kx is destabilizing when ρ(A + BK) ≥ 1, which is equivalent to J _[π]_ = +∞.
Based on the notion of destabilizing policies, we propose the concept of bounded-cost safety in
Definition 2, which requires that destabilizing policies, or policies with unbounded cost, are never
applied. It should be pointed out that bounded-cost safety does not guarantee the stability of trajectories, but is an indicator of the reliability of a learning scheme.

We assume the system is open-loop strictly stable, i.e., ρ(A) < 1. Indeed, if there is a known
stabilizing linear feedback gain K0, then we can apply the dual control scheme on the pre-stabilized
system (A + BK0, B) instead of (A, B). Existence of such a known stabilizing linear feedback
gain a standard assumption in previous works on online LQR (Mania et al., 2019; Simchowitz &
Foster, 2020), and relatively easy to establish through coarse system identification (Dean et al., 2019;
Faradonbeh et al., 2018b) or adaptive stabilization methods (Faradonbeh et al., 2018a; 2019).

3 ALGORITHM

The complete algorithm we propose for LQR dual control is presented in Algorithm 1. The modules
in the algorithm will be described later in this section.

3.1 SAFE CONTROL POLICY

This subsection describes the policy for determining the control input uk. The first n + p steps are
a warm-up period where purely random inputs are injected. Afterwards, in each step, we inject a
exploitation term ˜u plus a polynomial-decaying exploratory noise (k + 1)[−][β]ζ, where the decay rate
_β ∈_ (0, 1/2) is a constant. The exploitation term ˜u is a modified version of the certainty equivalent
control input _K[ˆ]_ _kxk, and this modification, described in Algorithm 2, is crucial to the safety of the_
learning process, which we will detail below.


-----

**Algorithm 1 Safe LQR dual control**

**Input: State dimension n, input dimension p, exploratory noise decay rate β**

1: for k = 0, 1, . . ., n + p − 1 do
2: _ξk+1_ 0

3: Apply control input ← _uk_ (k + 1)[−][β]ζk, where ζk (0, Ip)
_←_ _∼N_

4: for k = n + p, n + p + 1, . . . do
5: Observe the current state xk

6: **for τ = 0, 1, . . ., n1** +k p − 1 do _τ_ _−1_

7: _Hˆk,τ ←_ _k −_ _τ_ _i=τ_ +1(i − _τ_ )[β] "xi − _t=0_ _Hˆk,tu˜i−t−1#_ _ζi[⊤]−τ_ _−1_

X X

8: Reconstruct _A[ˆ]k,_ _B[ˆ]k from_ _H[ˆ]k,0, . . .,_ _H[ˆ]k,n+p_ 1 using Algorithm 3
_−_

9: Compute certainty equivalent feedback gain _K[ˆ]_ _k by replacing A, B with_ _A[ˆ]k,_ _B[ˆ]k in (5),(6)_

10: Determine policy πk( _,_ ) _π(_ _,_ ; k, _K[ˆ]_ _k, β), where π is described by Algorithm 2_

_·_ _·_ _←_ _·_ _·_

11: (uk, ξk+1) _πk(xk, ξk); record ˜uk_ _u, ζ˜_ _k_ _ζ, where ˜u, ζ are the corresponding vari-_
ables generated when executing the policy ← _←_ _←_

12: Apply control input uk


**Algorithm 2 Safe policy π(x, ξ; k, K, β)**

**Input: Arguments: system state x, policy internal state ξ; Parameters: step k, linear feedback gain**
_K, exploratory noise decay rate β_

**Output: Control input u and next policy internal state ξ[′], i.e., (u, ξ[′]) = π(x, ξ; k, K, β)**

1: if ξ > 0 then
2: _u˜ ←_ 0, ξ[′] _←_ _ξ −_ 1

3: else
4: **if max{∥K∥** _, ∥x∥} ≥_ log k then

5: _u˜ ←_ 0, ξ[′] _←⌊log k⌋_

6: **else**

7: _u˜ ←_ _Kx, ξ[′]_ _←_ 0

8: u ← _u˜ + (k + 1)[−][β]ζ, where ζ ∼N (0, I)_


In short, we stop injecting the exploitation input for ⌊log k⌋ + 1 consecutive steps, if either the state
norm _xk_ or the norm of the feedback gain _Kk_ exceeds the threshold log k. Recall from (2) that
_∥_ _∥_ _∥_ [ˆ] _∥_
we use ξk to denote the internal state of the policy, and in Algorithm 1, ξk is a counter that records
how many steps are left in the “non-action” period. Essentially, we utilize the innate stability of
the system to prevent the state from exploding catastrophically. This “non-action” mechanism is a
critical feature of our control design, without which the controller learned from data may destabilize
the system, albeit with a small probability, causing system failure in practice and forbidding the
establishment of almost sure performance guarantees in theory. We provide an ablation study of this
“non-action” mechanism in Section 5.

We choose both the switching threshold and the length of the “non-action” period to be timegrowing. The enlarging threshold corresponds to diminishing degree of conservativeness, which
is essential for the policy performance to converge to the optimal performance. Meanwhile, the prolonging “non-action” period rules out the potential oscillation of state caused by the frequent switching of the controller (see Appendix B for an illustrative example of this oscillation phenomenon).
In particular, it can be shown that the suboptimality gap incurred by the switching strategy scales
as O(tM exp(−cM [2])), where M is the switching threshold, t is the length of the “non-action” period, and c is a system-dependent constant (see Lemma 10 in Appendix A.1.2). With both M and t
growing as O(log k), the contribution of our switching strategy to the overall suboptimality gap is
merely _O[˜](1)._


-----

3.2 PARAMETER INFERENCE

The Markov parameters of the system described in (1) are defined as

_Hτ ≜_ _A[τ]_ _B,_ _τ = 0, 1, . . . ._ (7)

The Markov parameter sequence {Hτ _}τ[∞]=0_ [can be interpreted as the impulse response of the system.]
It shall be noted that finite terms of the Markov parameter sequence would suffice to characterize
the system, since higher-order Markov parameters can be represented as the linear combination
of lower-order Markov parameters using the characteristic polynomial of A in combination with
Cayley-Hamilton theorem. In particular, our inference algorithm estimates the first n + p terms
of the impulse response. We denote our estimate of Hτ at step k by _H[ˆ]k,τ_, whose expression is
specified in line 7 of Algorithm 1. Based on the cross-correlation between the current state xk and a
past exploratory noise input ζk _τ_ 1, we can produce an unbiased estimate of Hτ at each step, and
_−_ _−_
_Hˆk,τ is a cumulative average of such unbiased estimates, whose almost-sure convergence to Hτ can_
be established through a law of large numbers for martingales (see Appendix A.1.3).

**Remark 1. From a computational perspective, line 7 of Algorithm 1 can be decomposed to enable**
_efficient recursive updates. To see this, we can rearrange this expression as_


_τ_ _−1_

_Hˆk,t_
_t=0_

X


(i − _τ_ )[β]u˜i−t−1ζi[⊤]−τ _−1_
_i=τ_ +1

X


_i=τ_ +1(i − _τ_ )[β]xiζi[⊤]−τ _−1_ _[−]_

X


_Hˆk,τ =_


_k −_ _τ_


_k −_ _τ_


_the weighted sum of τ + 1 cumulative averages, each of which can be updated with a constant_
_amount of computation in each step. Therefore, the time complexity of each iteration and the total_
_memory consumption are constant._

From the inferred Markov parameters _H[ˆ]k,τ_ _τ_ =0, we can reconstruct _A[ˆ]k,_ _B[ˆ]k by fitting the data_
_{_ _}[n][+][p][−][1]_
obtained from several rollouts on the virtual system described by _H[ˆ]k,τ_ _τ_ =0 . The procedure is
_{_ _}[n][+][p][−][1]_
detailed in Algorithm 3.

**Algorithm 3 Restoring system and input matrices from Markov parameters**

**Input: Markov parameter estimate sequence** _H[ˆ]0,_ _H[ˆ]1, . . .,_ _H[ˆ]n+p_ 1, number of simulated trajecto_−_
ries N

**Output: System matrices estimate** _A,[ˆ]_ _B[ˆ]_

0 0 0 _· · ·_ 0
_Hˆ0_ 0 0 0

1: Construct block Toeplitz matrix  _Hˆ1_ _Hˆ0_ 0 _· · ·· · ·_ 0
_T ←_  ... ... ... ... ... 

 
Hˆn+p 1 _Hˆn+p_ 2 _Hˆn+p_ 3 0
 _−_ _−_ _−_ _· · ·_ 

2: Choose N independent random input trajectories, each (n + p)-long, denoted by **ui**
_←_
_u[(1)]i_ _u[(2)]i_ _u[(]i[N]_ [)] _,_ _i = 0, 1, . . ., n + p_ 1.

_· · ·_ _−_

3: Stackh the inputs i vertically and compute states:
_U_ _[v]_ _←_

4: Stack[u0; u1; · · ·the ; uinputsn+p−1], [andx1; x2states; · · · ; xnhorizontally+p] =: X1[v] _[←T U]to_ _[v]_ form the following matrices:
_U_ _[h]_ _←_ [u0 **u1** _· · ·_ **un+p−1], X1[h]** _←_ [x1 **x2** _· · ·_ **xn+p], X0[h]** _←_

[0n×N **x1** _· · ·_ **xn+p−1]**

5: Compute estimate ˆB _Aˆ_ 1[h] ; 0[h] _† ._
_←X_ _U_ _[h]_ _X_
   

It can be shown the procedure described in Algorithm 3 restores the actual system and input matrices
as long as the Markov parameter estimates are accurate:


**Lemma 1. When** _H[ˆ]τ = Hτ = A[τ]_ _B for all τ = 0, 1, . . ., n + p_ 1, and the matrix _U_ _[h]_
_−_ 0[h]
X

_in Algorithm 3 has full row rank, the result of Algorithm 3 satisfies_ _A[ˆ] = A,_ _B[ˆ] = B._


_defined_


-----

_Proof. When_ _H[ˆ]τ = A[τ]_ _B, we have X1[h]_ [=][ A][X][ h]0 [+][ B][U] _[h][ = [][B]_ _A]_ _U0[h][h]_ . Since _U0[h][h]_ has full row
X  X 

rank, we have _U0[h][h]_ _U0[h][h]_ _†_ = I, and therefore [B _A] = X1[h]_ _U0[h][h]_ _†_ = ˆB _Aˆ_ .
X  X  X 

 

**Remark 2. Lemma 1 guarantees the consistency of our controller, i.e., accurate Markov parameter**
_estimates would generate an optimal exploitation input. The full-row-rank condition in Lemma 1,_
_naturally satisfied by randomly generated inputs, guarantees that the inputs provide sufficient ex-_
_citation to reconstruct system matrices from impulse responses. For the ease of analysis, it is also_
_legitimate to use fixed, rather than random ui’s for each time step k of the outer loop of Algorithm 1,_
_as long as the rank condition is satisfied._

4 MAIN RESULTS

The main theoretical properties of our proposed LQR dual control scheme are stated as follows:

**Theorem 1. Assuming A is stable, the learning process described in Algorithm 1 is bounded-cost**
_safe according to Definition 2._

**Theorem 2. Assuming A is stable, and 0 < β < 1/2, for** _H[ˆ]k,τ computed in Algorithm 1, the_
_following limit holds almost surely, i.e., with probability 1:_


_Hˆk,τ_ _Hτ_
_−_ = 0, (8)

_k[−][γ][+][ϵ]_


lim
_k→∞_


_where γ = 1/2 −_ _β > 0, for any ϵ > 0 and any τ = 0, 1, . . ., n + p −_ 1.

**Remark 3. Theorem 2 states that our estimate of the Markov parameters converge at the order**
_O (k[−][γ][+][ϵ]). As a corollary, our estimate of the system and input matrices A, B also converge at_
_O (k[−][γ][+][ϵ]) (see Appendix A.4). This convergence rate coincides with the one guaranteed by the_
_more commonly used least-squares method, for which one can establish the convergence of esti-_
_mates of A, B at O (k[−][γ][+][ϵ]) using concentration bounds for martingale least-squares (e.g., Lemma_
_E.1 in Simchowitz & Foster (2020)). Therefore, our parameter inference algorithm based on cross-_
_correlation can be viewed as a competitive alternative to the least-squares estimator. One feature_
_of our cross-correlation approach compared to the least-squares approach, however, is that the es-_
_timation of Markov parameters can be easily extended to the partially observable LQR setting like_
_the one considered in Zheng et al. (2020). Also, the convergence of our cross-correlation approach_
_is not based on the sub-Gaussianity of the process noise (see Appendix A.3), which allows straight-_
_forward generalization to long-tailed noise models._

**Theorem 3. Assuming A is stable, and 0 < β < 1/2, let πk be the control policy used at step k in**
_Algorithm 1, then with J_ _[π][k]_ _and J_ _[∗]_ _defined in (3), (4) respectively, for any ϵ > 0, the following limit_
_holds almost surely, i.e., with probability 1:_

_J_ _[π][k]_ _J_ _[∗]_
lim _−_ (9)
_k_ _k[−]_ [min(2][β,][2][γ][)+][ϵ][ = 0][,]
_→∞_


_where γ = 1/2 −_ _β > 0._

Due to space limits, all the proofs are deferred to the appendix.

**Remark 4. According to Theorem 2, the convergence rate γ is maximized when β →** 0[+]. How_ever, the exploration term k[−][β]_ _does not decay in this case, and the control performance J_ _[π][k]_
_will not converge to the optimal performance J_ _[∗]._ _To achieve the fastest convergence of J_ _[π][k]_ _,_
_we need to choose the decay rate of the exploration term to be β = 1/4, which maximizes_
min(2β, 2γ) = min(2β, 1 − 2β). When β = 1/4, we have both parameter estimation errors and
_policy suboptimality gaps scaling at_ _O[˜](1/√T_ ), which matches the asymptotic rates of correspond
_ing components in the (with-high-probability) regret-optimal algorithm proposed in Simchowitz &_
_Foster (2020). However, due to the challenging nature of the analysis of the nonlinear closed-loop_
_system under our safe control scheme, the exact regret of our scheme is still under investigation._


-----

5 SIMULATION

In this section, the performance of our proposed algorithm is evaluated using Tennessee Eastman
Process (TEP), an industrial process example for benchmarking linear controllers (Ricker, 1993).
We used a simplified version of TEP from Liu et al. (2020), where the state and input dimensions
are n = 8, p = 4, and the spectral radius of the system matrix is ρ(A) ≈ 0.96. We assume
_Q, R, are identity matrices, and W, X0 are also identity matrices, i.e., the process noise is i.i.d._
standard Gaussian. We choose N = 50 (in Algorithm 3) for all the experiments. In the practical
implementation, the estimated system matrices _A[ˆ]k,_ _B[ˆ]k and the feedback gain_ _K[ˆ]_ _k are updated only_
at steps ⌊10[n/][2]⌋(n ∈ N) to speed up the computation.

To illustrate the impact of the parameter β on the convergence of the algorithm, we perform 100
independent experiments for each of β ∈{0, 1/4, 1/2}, with 10[8] steps in each experiment. Fig. 1

shows the error of the estimated system and input matrices, i.e., _A[ˆ]k −_ _A_ and _B[ˆ]k −_ _B_ against


|es, i.e., � � �ˆ Ak −A � � �and � � �ˆ Bk −B � � �|Col2|Col3|Col4|Col5|Col6|Col7|Col8|� � �agains|
|---|---|---|---|---|---|---|---|---|
|101 100 10−1 10−2 10−3|||||||||
|||||||||β = 0 β = 1/4 β = 1/2|
||||||||||
|100 10−1 10−2 10−3|||||||||
||||||||||
||||||||||
||||||||||
||||||||||


10[1] 10[2] 10[3] 10[4] 10[5] 10[6] 10[7] 10[8]


10[−][2]

|ustrate the impact of the paramete endent experiments for each of β|Col2|Col3|Col4|Col5|Col6|Col7|Col8|er β on ∈{0, 1/|
|---|---|---|---|---|---|---|---|---|
|the error of the estimated system e k for different values of β. 101 100 10−1 2||||||||and inp|
|||||||||β = 0 β = 1/4 β = 1/2|
||||||||||
||||||||||
|100 10−1 2|||||||||
||||||||||
||||||||||

10[1] 10[2] 10[3] 10[4] 10[5] 10[6] 10[7] 10[8]


Figure 1: Error of estimated system matrices for different β against time k. The solid lines are the
median among the experiments, and the shades represent the range among the experiments.

From Fig. 1, one can see when β is 0 or 1/4, the estimation error of the system matrices converges to
zero as time k goes to infinity, and the convergence approximately follows a power law. Furthermore,
the convergence speed of the estimation error is significantly faster when β = 0. Meanwhile, when
_β = 1/2, the estimate error diverges in some of the experiments. The above observations are_
consistent with the theoretical result in Theorem 2, where it is stated that the parameter estimates
converge with rate O (k[−][γ][+][ϵ]), with γ = 1/2 _−_ _β, i.e., convergence is only guaranteed with β < 1/2_
and is faster when β is smaller.

Now we consider the performance of policies with different values of β. To quantify the policy
performance, one shall in theory compute J _[π][k]_ as defined in (3). However, the policies πk are
nonlinear, rendering it very difficult, if not impossible, to compute J _[π][k]_ analytically. As a surrogate
approach for evaluating a policy πk, we define the empirical cost _J[ˆ][π][k]_ as


_⊤_ _⊤_
_x[(]t[i][)]_ _Qx[(]t[i][)]_ + _u[(]t[i][)]_ _Ru[(]t[i][)][,]_
  


_T −1_

_t=0_

X


_Jˆ[π][k]_ = [1]


_i=1_


where x[(]t[i][)][, u]t[(][i][)] are states and inputs collected from the closed-loop system under πk in N independent T -long sample paths indexed by i. In our simulations, we take T = 10000, N = 10 to evaluate
each stabilizing policy, which yields consistent results in practice. Fig. 2 shows the empirical performance of controllers with different values of β against time k.

From Fig. 2, one can observe that among our choices of β, only β = 1/4 drives the controller toward
the optimal one. For β = 0, although the parameter estimates converge the fastest as discussed
above, the performance of the resulting closed-loop system is even worse than the free system. This
is because the exploration term (k + 1)[−][β]ζk in the control input does not decay, and the resulting
controller is a noisy one. Meanwhile, for β = 1/2, diverging parameter estimates would lead to illperforming controllers. The observations are consistent with the theoretical conclusion indicated by
Theorem 3 that β = 1/4 corresponds to the optimal trade-off between exploration and exploitation.


-----

10[−]10[1] [1] 10[2] 10[3] 10[4] 10[5] 10[6] 10[7] 10

|β = 0|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|102.2 β = 1/4 β = 1/2 Optimal 102.0 Free 101.8 Jˆπk 101.6 101.4 100 101 102 103 104 105 106 107 108 k re 2: The empirical performance of controllers for different β against time k. For each polic he empirical cost Jˆπk is obtained by approximating the actual cost with random sampling fro losed-loop system. The solid lines are the median among the experiments, and the shade sent the interval between the first and third quantiles among the experiments. The purple an n dashed lines represent the analytical optimal cost, and the analytical cost of the control-fre m, respectively. ly, we compare our algorithm with the naive centainty equivalence algorithm, which applie Kˆ kx k+k−βζ at each step. We invoke both algorithms with the “optimal” parameter β = 1/4 k perform 20 independent experiments for each algorithm. We consider the system paramete ation error and the policy suboptimality gap for each algorithm, as we did in Fig. 1 and Fig. 2 results of this comparison experiment are presented in Fig. 3. In a proportion of the experiment ally 7 out of 20), certainty equivalence causes the estimation error and the policy suboptimalit to diverge, while our algorithm ensures convergence in each experiment. Although existin s (e.g. Simchowitz & Foster (2020)) contain methods for decreasing the failure probability wit rm-up period for estimation, it should be noted that the failure probability is always nonzero a as a linear feedback policy with learned gain is deployed. Even compared to the case wher ertainty equivalence algorithm converges, our algorithm still delivers slightly lower estimatio and significantly better policy performance, which indicates that the conservativeness cause ur switching mechanism does not harm the long-term performance. CE CE CE 101 Safe 101 Safe OpS ta if me al Free −B 100 Jˆπk 100 ˆ Bk 10−1||||||||||||||||
||||||||||||||||Safe Optimal Free|
|||||||||||||||||
|100||||||||||||||||
|||||||||||||||||
|||||||||||||||||


10[1] 10[2] 10[3] 10[4] 10[5] 10[6] 10[7] 10[8]


10[1] 10[2] 10[3] 10[4] 10[5] 10[6] 10[7] 10[8]


Figure 3: Comparison between our proposed algorithm (Safe) and the certainty equivalence algorithm (CE), in terms of estimation error and policy performance. In a proportion of the experiments,
CE causes the estimation error and the policy suboptimality gap to diverge, while our algorithm
ensures convergence in each experiment.

6 RELATED WORKS


The concept of dual control was initiated by Feldbaum (1960), and since then sustained research efforts have been devoted to learning to make decisions in unknown environments. Classical literature
on adaptive control, e.g., Astr¨[˚] om & Wittenmark (1973); Lai & Wei (1986); Guo & Chen (1988);
Guo (1996), have addressed the dual control of linear systems described by the ARMAX model
with the objective of tracking a reference signal. This objective do not consider the cost of exerting
control input and is less involved than the LQR setting. For the rest of this section, we restrict our
focus to the LQR of unknown linear systems described by the state-space model.


-----

In the offline learning regime, system identification and controller synthesis based on the estimated
system are performed in separate stages. The study of linear system identification has been developed for decades, with classical asymptotic guarantees summarized in Ljung (1999), and there has
been a recent interest in non-asymptotic system identification based on a finite-size dataset. One can
generate the dataset either by injecting random inputs to the system (Oymak & Ozay, 2019; Zheng
& Li, 2020), or using active learning techniques to achieve optimal system-dependent finite-time
rate (Wagenmaker & Jamieson, 2020). For a summary of recent results on non-asymptotic linear
system identification, please refer to Zheng & Li (2020).

A natural step following system identification is to synthesize a controller based on the identification
result and analyze the resulting closed-loop system’s performance. The Coarse-ID control framework (Dean et al., 2019) adopts robust control techniques to derive controllers taking identification
error into account, resulting in high-probability bounds on the control performance suboptimality
gap. This framework has been applied to LQR (Dean et al., 2019), SISO system with output feedback (Boczar et al., 2018), and LQG (Zheng et al., 2020). As an alternative approach, the certainty
equivalence framework synthesizes the controller by treating the identified system as the truth (Mania et al., 2019; Tsiamis et al., 2020), which is shown to achieve a faster convergence rate of control
performance, at the cost of higher sensitivity to poorly estimated system parameters. At the intersection of the above two methods, an optimistic robust framework was proposed to achieve a combination of fast convergence rate and robustness (Umenberger & Sch¨on, 2020). It should be pointed out
the offline algorithms in the above works and the online setting we consider in the present work are
not mutually exclusive but complementary to each other: online algorithms can continually refine
the controller obtained by offline methods without interrupting normal system operation.

The problem of designing controllers that improve over time has been studied in the context of online LQR and regret analysis. The online LQR setting was initially proposed by Abbasi-Yadkori &
Szepesv´ari (2011), and since then, Optimism in the Face of Uncertainty (OFU), Thompson Sampling (TS) and ε-greedy exploration have been applied to solve the online LQR problem. AbbasiYadkori & Szepesv´ari (2011) applied OFU principle to prove a theoretical _O[˜](√T_ ) regret upper

bound, but their method is computationally intractable. Cohen et al. (2019) proposed a tractable algorithm for OFU based on semidefinite programming to realize _O[˜](√T_ ) regret. TS has been shown

to achieve _O[˜](√T_ ) (frequentist) regret for scalar system (Abeille & Lazaric, 2018), and _O[˜](√T_ ) ex
pected regret in a Beyesian setting (Ouyang et al., 2017). Recently, Faradonbeh et al. (2020a) and
Mania et al. (2019) prove that randomized Certainty Equivalent (CE) control with ε-greedy exploration can also achieve _O[˜](√T_ ) regret, and Simchowitz & Foster (2020) prove that _O[˜](√T_ ) regret is

indeed the fundamental limit for the general online LQR setting. Under further assumptions, e.g.,
partially known system parameters, logarithm regret bound may be achieved (Cassel et al., 2020).
The aforementioned results and the fundamental limit hold either with high probability or in expectation, and lack almost sure guarantee. Faradonbeh et al. (2020b) prove that _O[˜](√T_ ) regret for

both TS and CE and hold almost surely, but under the restrictive assumption that the closed-loop
system remains stable when the policy is being employed. Our work attempts to fill in the gap by
performing an almost sure analysis for online LQR without assumptions on the system other than
the existence a known initial stabilizer.

7 CONCLUSION

In this paper, we propose a safe LQR dual control scheme based on a switching, which guarantees
almost sure convergence to zero of both parameter inference error and suboptimality gap of control
performance. The convergence rate of inference error is O(T _[−][1][/][4+][ϵ]), while that of the suboptimal-_
ity gap is O(T _[−][1][/][2+][ϵ]), both of which match the known optimal rates proved in the non-asymptotic_
setting. For future works, we plan to extend the notion of regret to our almost-sure convergence
setting, and formally establish the fundamental limits for this setting. It would also be interesting to investigate the exact convergence rates, especially dimension dependence, of our parameter
inference scheme and the standard least-squares procedure.


-----

REFERENCES

Yasin Abbasi-Yadkori and Csaba Szepesv´ari. Regret bounds for the adaptive control of linear
quadratic systems. In Proceedings of the 24th Annual Conference on Learning Theory, pp. 1–
26. JMLR Workshop and Conference Proceedings, 2011.

Marc Abeille and Alessandro Lazaric. Improved regret bounds for thompson sampling in linear
quadratic control problems. In International Conference on Machine Learning, pp. 1–9. PMLR,
2018.

Karl Johan Astr¨[˚] om and Bj¨orn Wittenmark. On self tuning regulators. Automatica, 9(2):185–199,
1973.

Ross Boczar, Nikolai Matni, and Benjamin Recht. Finite-data performance guarantees for the
output-feedback control of an unknown system. In 2018 IEEE Conference on Decision and Con_trol (CDC), pp. 2994–2999. IEEE, 2018._

Asaf Cassel, Alon Cohen, and Tomer Koren. Logarithmic regret for learning linear quadratic regulators efficiently. In International Conference on Machine Learning, pp. 1328–1337. PMLR,
2020.

Alon Cohen, Tomer Koren, and Yishay Mansour. Learning linear-quadratic regulators efficiently
with only _√T regret. In International Conference on Machine Learning, pp. 1300–1309. PMLR,_

2019.

Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. Regret bounds for
robust adaptive control of the linear quadratic regulator. arXiv preprint arXiv:1805.09388, 2018.

Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu. On the sample complexity of the linear quadratic regulator. Foundations of Computational Mathematics, pp. 1–47,
2019.

Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Finite-time adaptive stabilization of linear systems. IEEE Transactions on Automatic Control, 64(8):3498–3505,
2018a.

Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Finite time identification in unstable linear systems. Automatica, 96:342–353, 2018b.

Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Randomized algorithms for data-driven stabilization of stochastic linear systems. In 2019 IEEE Data Science
_Workshop (DSW), pp. 170–174. IEEE, 2019._

Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. Input perturbations
for adaptive control and learning. Automatica, 117:108950, 2020a.

Mohamad Kazem Shirani Faradonbeh, Ambuj Tewari, and George Michailidis. On adaptive linear–
quadratic regulators. Automatica, 117:108982, 2020b.

AA Feldbaum. Dual control theory. i. Avtomatika i Telemekhanika, 21(9):1240–1249, 1960.

L Guo and HF Chen. Convergence rate of els based adaptive tracker. Syst. Sci & Math. Sci, 1:
131–138, 1988.

Lei Guo. Self-convergence of weighted least-squares with applications to stochastic adaptive control. IEEE transactions on automatic control, 41(1):79–89, 1996.

Tze Lai and Ching-Zong Wei. Extended least squares and their applications to adaptive control and
prediction in linear systems. IEEE Transactions on Automatic Control, 31(10):898–906, 1986.

Hanxiao Liu, Yilin Mo, Jiaqi Yan, Lihua Xie, and Karl H. Johansson. An online approach to physical
watermark design. IEEE Transactions on Automatic Control, 65(9):3895–3902, Sep 2020. ISSN
0018-9286, 1558-2523, 2334-3303. doi: 10.1109/TAC.2020.2971994.


-----

Lennart Ljung. System identification. Wiley encyclopedia of electrical and electronics engineering,
pp. 1–19, 1999.

Horia Mania, Stephen Tu, and Benjamin Recht. Certainty equivalence is efficient for linear quadratic
control. arXiv preprint arXiv:1902.07826, 2019.

Yi Ouyang, Mukul Gagrani, and Rahul Jain. Learning-based control of unknown linear systems
with thompson sampling. arXiv preprint arXiv:1709.04047, 2017.

Samet Oymak and Necmiye Ozay. Non-asymptotic identification of lti systems from a single trajectory. In 2019 American control conference (ACC), pp. 5655–5661. IEEE, 2019.

N Lawrence Ricker. Model predictive control of a continuous, nonlinear, two-phase reactor. Journal
_of Process Control, 3(2):109–123, 1993._

Max Simchowitz and Dylan Foster. Naive exploration is optimal for online lqr. In International
_Conference on Machine Learning, pp. 8937–8948. PMLR, 2020._

Anastasios Tsiamis, Nikolai Matni, and George Pappas. Sample complexity of kalman filtering for
unknown systems. In Learning for Dynamics and Control, pp. 435–444. PMLR, 2020.

Jack Umenberger and Thomas B Sch¨on. Optimistic robust linear quadratic dual control. In Learning
_for Dynamics and Control, pp. 550–560. PMLR, 2020._

Andrew Wagenmaker and Kevin Jamieson. Active learning for identification of linear dynamical
systems. In Conference on Learning Theory, pp. 3487–3582. PMLR, 2020.

Yang Zheng and Na Li. Non-asymptotic identification of linear dynamical systems using multiple
trajectories. IEEE Control Systems Letters, 5(5):1693–1698, 2020.

Yang Zheng, Luca Furieri, Maryam Kamgarpour, and Na Li. Sample complexity of lqg control for
output feedback systems. arXiv preprint arXiv:2011.09929, 2020.


-----

A PROOF OF MAIN RESULTS

A.1 PRELIMINARIES FOR PROOFS

A.1.1 NOTATIONS

We will use the following notations throughout the proofs:

**Notation** **Definition**

_X_ _[⊤]_ transpose of matrix X

_∥X∥_ _, ∥x∥_ norm of matrix X or vector x, 2-norm by default

_X_ _F_ Frobenius norm of matrix X
_∥_ _∥_

tr(X) trace of matrix X


_ρ(X)_ spectral radius of matrix X

_X ≻_ 0 matrix X is positive definite

_X ≺_ _Y_ for matrix X, Y, Y − _X ≻_ 0

_λmin(X)_ minimum eigenvalue of matrix X


_κ(X)_ ratio between maximum and minimum eigenvalues for matrix X 0, i.e., κ(X) = _X_ _/λmin(X)_
_≻_ _∥_ _∥_

vec(X) vectorization of matrix X

_X ⊗_ _Y_ Kronecker product of matrices X and Y

**1S** indicator function of set S


_f_ (x) ∼O(g(x)) there exists M > 0, such that |f (k)| ≤ _M × g(k) for all_
_k ∈_ N

_f_ (x) ∼ _O[˜](g(x))_ there exists M > 0 and n ∈ N, such that |f (k)| ≤ _M ×_
_g(k) × log(k)[n]_ for all k ∈ N

In addition, we define the following short-hand notation for characterizing the almost-sure asymptotic convergence or growth rate of random processes: for a random variable (vector, or matrix)
sequence _xk_, we denote that xk (α) if for all ϵ > 0 and almost all realizations of randomness,
_{_ _}_ _∼C_
basic properties ofwe have xk ∼O ( Ck[α](α[+])[ϵ] functions:), i.e., limk→∞ _∥xk∥_ _/k[α][+][ϵ a.s.]= 0. The following lemma establishes some_
**Lemma 2. Assume xk, yk are random variable (vector, or matrix) sequences with proper dimen-**
_sions, then the following properties hold:_

_1. If xk ∼C(α), yk ∼C(β), then xk + yk ∼C (max(α, β))._

_2. If xk_ (α), yk (β), then xkyk (α + β).
_∼C_ _∼C_ _∼C_

_3. If f is a function differentiable at 0 and xk_ _C(α), α < 0, then f (xk)_ _f_ (0) _C(α)._
_∼_ _−_ _∼_


_Proof. The first two claims follow trivially from the definition of C(α). For the third claim, notice_
that for all realizations of randomness, limk→∞ _∥xk∥_ = 0. Therefore, Taylor expansion of f (x)
at x = 0 gives f (xk) − _f_ (0) = Df (0)xk + O(∥xk∥[2]), where Df is the Fr´echet derivative of f .
Dividing both sides by k[α][+][ϵ] leads to the conclusion.

A.1.2 PROPERTIES OF A CLASS OF SWITCHED LINEAR SYSTEMS

We state and prove some properties of a class of switched linear systems that will be useful for the
proof of all the main theorems of this paper.

Consider the dynamical system
_xk+1 = Akxk + wk,_ (10)


-----

where xk, wk R[n], x0 (0, W 1), wk (0, Wk), x0, _wk_ _x0:k_ are pairwise independent, and Ak, W ∈k are functions of ∈N _x0:−k and k ∼N. We assume_ _{_ _|_ _}_

_Wk_ 0, _Ak_ _A,_ _Wk_ _W,_ _k._ (11)
_≻_ _∥_ _∥≤_ _∥_ _∥≤_ _∀_

We first prove a few miscellaneous lemmas that will be useful shortly:

**Lemma 3. Let X ∼** _χ[2](p), then P(X ≥_ _M_ ) ≤ 2[p/][2] exp(−M/4) for any M > 0.

_Proof. Applying the Chernoff bound, we have_

P(X ≥ _M_ ) ≤ E e[tX] _/e[tM]_ [] = (1 − 2t)[−][p/][2] exp(−tM )

for any 0 < t < 1/2. Choosing t = 1/4 leads to the conclusion.


**Lemma 4. Let w** R[p], w (0, W ), W 0, then P( _w_ _M_ ) 2[p/][2] exp 4−MW[2] _._
_∈_ _∼N_ _≻_ _∥_ _∥≥_ _≤_ _∥_ _∥_
 

_Proof. Let v = W_ _[−][1][/][2]w, then v ∼N (0, I) and hence ∥v∥[2]_ _∼_ _χ[2](p). From ∥w∥[2]_ = w[⊤]w =
_v[⊤]Wv ≤∥v∥[2]_ _∥W_ _∥, we have P(∥w∥≥_ _M_ ) = P(∥w∥[2] _≥_ _M_ [2]) ≤ P(∥v∥[2] _≥_ _M_ [2]/ ∥W _∥)._
Applying Lemma 3 leads to the conclusion.

**Lemma 5. Let 0 < x ≤** 1/2 and 1 < α < 2, then _n=0_ _[x][α][n][ <][ 2][x/][(][α][ −]_ [1)][.]

_Proof. By α > 1, we have α[n]_ 1 = (1+ _α_ 1)[n] [P]1 > n[∞] (α 1). Therefore, in view of 0 < x < 1,
_−_ _−_ _−_ _−_
we have


_∞_

_x[α][n]_ = x

_n=0_

X


_x[α][n][−][1]_ _< x_

_n=0_

X


_x[(][α][−][1)][n]_ =

_n=0_

X


1 _x[α][−][1][ .]_
_−_


In view of the inequality (1+t)[r] _≤_ 1+rt for t ≥−1, 0 ≤ _r ≤_ 1, we have x[α][−][1] = (1+x−1)[α][−][1] _≤_
1 + (α − 1)(x − 1), and therefore,

_x_ _x_ 2x

1 − _x[α][−][1][ ≤]_ (α − 1)(1 − _x)_ _[≤]_ _α −_ 1

when 0 < x ≤ 1/2, and the conclusion follows.

Now we state several properties of the system described in (10)-(11):


**Lemma 6. For the system described in (10)-(11), assuming there exists 0 < ρ < 1 and P ≻** 0 such
_that A[⊤]k_ _[PA][k][ ≺]_ _[ρP][ for every][ A][k][, then when][ M][ ≥]_ 3W κ(P )/(1 − _ρ[1][/][4]), there is_
p


2[n/][2+1]
P ( _xk_ _M_ )
_∥_ _∥≥_ _≤_ _ρ[−][1][/][2]_ 1 [exp] _−_ [(1][ −]4W[ρ] κ[1][/]([4]P[)][2])[M][ 2]

_−_ 

_Proof. From (10), we have_


_∀k._


_xk = Ak_ 1xk 1 + wk 1
_−_ _−_ _−_
= Ak 1(Ak 2xk 2 + wk 2) + wk 1
_−_ _−_ _−_ _−_ _−_
= · · ·
= wk 1 + Ak 1wk 2 + + Ak 1Ak 2 _A1w0 + Ak_ 1Ak 2 _A0x0._
_−_ _−_ _−_ _· · ·_ _−_ _−_ _· · ·_ _−_ _−_ _· · ·_

Pre-multiplying by P [1][/][2] on both sides and applying the triangle inequality, we have
_P_ [1][/][2]xk _≤_ _P_ [1][/][2]wk−1 + _P_ [1][/][2]Ak−1wk−2 + · · · + _P_ [1][/][2]Ak−1 · · · A1w0 +
_P_ [1][/][2]Ak−1 · · · A0x0 _._ (12)


-----

From A[⊤]k _[PA][k][ ≺]_ _[ρP]_ [, we have for any][ w][ ∈] [R][n][, k][ ∈] [N][,]

_w[⊤]A[⊤]k_ _[PA][k][w < ρw][⊤][Pw][ ⇒]_ _P_ [1][/][2]Akw _< ρ[1][/][2]_ _P_ [1][/][2]w _._ (13)

Applying (13) to (12) recursively, we have
_P_ [1][/][2]xk _≤_ _P_ [1][/][2]wk−1 + ρ[1][/][2] _P_ [1][/][2]wk−2 + · · · + ρ[(][k][−][1)][/][2] _P_ [1][/][2]w0 + ρ[k/][2] _P_ [1][/][2]x0 _._

(14)

Meanwhile, we have


_xk_ _M_ = _P_ _P_ [1][/][2]xk _M_
_{∥_ _∥≥_ _}_ _[−][1][/][2]_ _≥_
n o

1

_P_ _P_ [1][/][2]xk _M_ =
_⊆_ _[−][1][/][2]_ _≥_ ( _λmin(P_ )
n o

= _P_ [1][/][2]xk _M_ _λmin(P_ ) _._ p
_≥_
n p o

Therefore, for any σ ∈ (ρ[1][/][2], 1), in view of (14), we have


_P_ [1][/][2]xk _M_
_≥_


_xk_ _M_ _P_ [1][/][2]xk _M_ _λmin(P_ )
_{∥_ _∥≥_ _} ⊆_ _≥_
n p o

_P_ [1][/][2]xk _M_ _λmin(P_ )(1 _σ[k][+1])_
_⊆_ _≥_ _−_
n p o

_⊆_ _P_ [1][/][2]wk−1 + ρ[1][/][2] _P_ [1][/][2]wk−2 + · · · + ρ[(][k][−][1)][/][2] _P_ [1][/][2]w0 + ρ[k/][2] _P_ [1][/][2]x0 _≥_
n

_M_ _λmin(P_ )(1 _σ[k][+1])_

_−_

p o

_⊆_ _P_ [1][/][2]wk−1 _≥_ (1 − _σ)M_ _λmin(P_ ) _∪_ _ρ[1][/][2]_ _P_ [1][/][2]wk−2 _≥_ _σ(1 −_ _σ)M_ _λmin(P_ )
n p o n p

_ρ[(][k][−][1)][/][2]_ _P_ [1][/][2]w0 _σ[k][−][1](1_ _σ)M_ _λmin(P_ )

_· · · ∪_ _≥_ _−_ _∪_
n p o

_ρ[k/][2]_ _P_ [1][/][2]x0 _σ[k](1_ _σ)M_ _λmin(P_ ) _._
_≥_ _−_
n p o

Taking the union bound, we have

P( _xk_ _M_ ) P _P_ [1][/][2]wk 1 (1 _σ)M_ _λmin(P_ ) +
_∥_ _∥≥_ _≤_ _−_ _≥_ _−_
 _σ_ p 

P _P_ [1][/][2]wk 2 _λmin(P_ ) +
_−_ _≥_ _ρ[1][/][2][ (1][ −]_ _[σ][)][M]_
 p 

_· · · +_

_σ_ _k−1_

P _P_ [1][/][2]w0 (1 _σ)M_ _λmin(P_ ) +

_≥_  _ρ[1][/][2]_  _−_ !

p

_k_

_σ_

P _P_ [1][/][2]x0 (1 _σ)M_ _λmin(P_ ) _._

_≥_  _ρ[1][/][2]_  _−_ !

p

Since P [1][/][2]wk _x0:k_ 0, P [1][/][2]WkP [1][/][2][], and _Wk_ _W, applying Lemma 4, we have_
_|_ _∼N_ _∥_ _∥≤_
 

P _P_ [1][/][2]wk 1 (1 _σ)M_ _λmin(P_ ) _x0:k_ 2[n/][2] exp
_−_ _≥_ _−_ _≤_ _−_ [(1][ −]4[σ][)]W[2][M]k [ 2][λ]P[min][(][P] [)]
 p   _∥_ _∥∥_ _∥_ 

2[n/][2] exp _._
_≤_ _−_ [(1]4[ −]W κ[σ][)]([2]P[M]) [ 2]
 


(15)


Noticing that the RHS of (15) does not depend on x0:k, we have


_λmin(P_ ) 2[n/][2] exp
_≤_ _−_ [(1]4[ −]W κ[σ][)]([2]P[M]) [ 2]
 


_P_ [1][/][2]wk−1 _≥_ (1 − _σ)M_



-----

Similarly, we have

P _P_ [1][/][2]wk 2
_−_


.
.
.


_λmin(P_ ) exp _[σ][2]_
_≤_ _−_ [(1]4[ −]W κ[σ][)]([2]P[M]) [ 2] _·_ _ρ_
 


_ρ[1][/][2][ (1][ −]_ _[σ][)][M]_


_σ_ _k−1_ _σ2_ _k−1[!]_

P _P_ [1][/][2]w0 (1 _σ)M_ _λmin(P_ ) exp _,_

_≥_  _ρ[1][/][2]_  _−_ ! _≤_ _−_ [(1]4[ −]W κ[σ][)]([2]P[M]) [ 2] _·_  _ρ_ 

_k_ p 2 _k[!]_

_σ_ _σ_

P _P_ [1][/][2]x0 (1 _σ)M_ _λmin(P_ ) exp _._

_≥_  _ρ[1][/][2]_  _−_ ! _≤_ _−_ [(1]4[ −]W κ[σ][)]([2]P[M]) [ 2] _·_  _ρ_ 

p

Choose σ = ρ[1][/][4], then σ[2]/ρ = ρ[−][1][/][2]. Assuming w.l.o.g. that ρ ∈ (1/4, 1), we have σ[2]/ρ =
_ρ[−][1][/][2]_ _∈_ (1, 2). When M ≥ 3W κ(P )/(1 − _ρ[1][/][4]), there is_
p

2[n/][2] exp _< exp_ _<_ [1]
_−_ [(1]4[ −]W κ[σ][)]([2]P[M]) [ 2] _−_ 4[3] 2 _[.]_
   


Applying Lemma 5 leads to the conclusion.

**Lemma 7. For the system described in (10)-(11), assume there exists 0 < ρ < 1, P ≻** 0 and M > 0
_such that A[⊤]k_ _[PA][k][ ≺]_ _[ρP][ as long as][ ∥][x][k][∥≥]_ _[M]_ _[. Let][ V][k][ =][ x]k[⊤][Px][k][, then]_

EVk < [(][M][ 2][A][ 2][ +][ W][ )][ ∥][P] _[∥]_ _,_ _k._

1 _ρ_ _∀_
_−_


_Proof. We first derive a recursive bound on EVk: from (10), we have_

_Vk+1 = x[⊤]k+1[Px][k][+1]_ [=][ x][⊤]k _[A]k[⊤][PA][k][x][k]_ [+ 2][w]k[⊤][PA][k][x][k] [+][ w]k[⊤][Pw][k][.] (16)

When ∥xk∥ _< M_, we have x[⊤]k _[A]k[⊤][PA][k][x][k][ ≤]_ _[M][ 2][A][ 2][ ∥][P]_ _[∥][. Otherwise, by][ A]k[⊤][PA][k][ ≺]_ _[ρP]_ [, we]
have x[⊤]k _[A]k[⊤][PA][k][x][k][ < ρx]k[⊤][Px][k][ =][ ρV][k][. Therefore, in view of the fact that][ M][ 2][A][ 2][ ∥][P]_ _[∥≥]_ [0][ and]
_ρVk_ 0, we have
_≥_

_x[⊤]k_ _[A]k[⊤][PA][k][x][k]_ _[≤]_ [max][{][M][ 2][A][ 2][ ∥][P] _[∥]_ _[, ρV][k]_ _[≥]_ [0][} ≤] _[M][ 2][A][ 2][ ∥][P]_ _[∥]_ [+][ ρV][k][,] (17)

which holds for any k. Substituting (17) into (16), we have

_Vk+1 ≤_ _ρVk + ηk + C,_ (18)

where ηk = 2wk[⊤][PA][k][x][k][ +][ w]k[⊤][Pw][k][, C][ =][ M][ 2][A][ 2][ ∥][P] _[∥][. With such defined][ η][k][, we have]_

Eηk ≤ _W ∥P_ _∥_ _,_ (19)

where we noticed that E[wk[⊤][PA][k][x][k][] =][ E][[][E][[][w]k[⊤][PA][k][x][k][ |][ x][0:][k][]] =][ E][[][E][[][w][k][ |][ x][0:][k][]][⊤][A][k][x][k][] = 0][, and]
that E[wk[⊤][Pw][k][] =][ E][[][E][[][w]k[⊤][Pw][k][ |][ x][0:][k][]] =][ E][[tr(][W][k][P] [)]][ ≤] _[W][ ∥][P]_ _[∥][. Taking the expectation on both]_
sides of (18), we have

EVk+1 ≤ _ρEVk + (M_ [2]A [2] + W ) ∥P _∥_ _._ (20)

We can proceed using induction: we have

EV0 = E[x[⊤]0 _[Px][0][] =][ E][[tr(][W][−][1][P]_ [)]][ ≤] _[W][ ∥][P]_ _[∥]_ _[<][ (][M][ 2][A][ 2][ +][ W][ )][ ∥][P]_ _[∥]_ _._

1 − _ρ_

Assuming EVk < [(][M][ 2][A][ 2]1−[+]ρ[W][ )][∥][P][ ∥], it follows from (20) that EVk+1 < [(][M][ 2][A][ 2]1−[+]ρ[W][ )][∥][P][ ∥], which con
cludes our proof.

**Lemma 8. For the system described in (10)-(11), assume that there exists 0 < ρ < 1, P ≻** 0 and
_M > 0 such that A[⊤]k_ _[PA][k][ ≺]_ _[ρP][ as long as][ ∥][x][k][∥≥]_ _[M]_ _[, and that][ A][k][ only depends on][ ∥][x][k][∥][. Let]_
_Vk = x[⊤]k_ _[Px][k][, then]_

EVk[2] _[<][ (][M][ 2][A][ 2][ +][ W][ )][ ∥][P]_ _[∥][2]_ (1 + ρ)(M [2]A [2] + W ) + 4A [2]W κ(P ) _,_ _k._

(1 _ρ)(1_ _ρ[2])_ _∀_
_−_ _−_
 


-----

_Proof. From the inequality (18) in the proof of Lemma 7, we have_

EVk[2]+1 _k_ [+][ E][η]k[2] [+][ C] [2][ + 2][ρ][E][[][V][k][η][k][] + 2][ρC][E][V][k] [+ 2][C][E][η][k][,] (21)

_[≤][ρ][2][E][V][ 2]_

where ηk = 2wk[⊤][PA][k][x][k][ +][ w]k[⊤][Pw][k][, C][ =][ M][ 2][A][ 2][ ∥][P] _[∥][. Let us bound][ E][η][k][,][ E][V][k][,][ E][η]k[2][,][ E][[][V][k][η][k][]]_
which appear in the RHS of (21) respectively:

_• Eηk ≤_ _W ∥P_ _∥_ from (19) in the proof of Lemma 7.

_• EVk < (C + W ∥P_ _∥)/(1 −_ _ρ) according to the conclusion of Lemma 7._

_• Eηk[2]_ [= 4][E][[][x]k[⊤][A]k[⊤][Pw][k][w]k[⊤][PA][k][x][k][] + 4][E][[][w]k[⊤][PA][k][x][k][w]k[⊤][Pw][k][] +][ E][[][w]k[⊤][Pw][k][w]k[⊤][Pw][k][]][ ≤]
4E ∥xk∥[2] _A_ [2]W ∥P _∥[2]_ + W [2] _∥P_ _∥[2],_ where we noticed E[wk[⊤][PA][k][x][k][w]k[⊤][Pw][k][]] =
E[E[wk[⊤][PA][k][x][k][w]k[⊤][Pw][k] _| xk]] = tr_ E[AkxkE[wk[⊤][Pw][k][w]k[⊤][P][ |][ x][k][]]] = 0 due to
symmetry. From Vk = x[⊤]k _[Px][k][ ≥]_ _[λ][min][(][P]_ [)][ ∥][x][k][∥][2][, we can further deduce][ E][η]k[2]
4A [2]W ∥P _∥_ _κ(P_ )EVk + W [2] _∥P_ _∥[2]._ _[≤]_

E[Vkηk] = 2E[wk[⊤][PA][k][V][k][] +][ E][[][V][k][w]k[⊤][Pw][k][]] = 2E[E[wk[⊤][PA][k][x][k][V][k] _xk]] +_

_•_ _|_
E[E[vkwk[⊤][Pw][k][ |][ x][k][]] = 2][E][[][E][[][w][k][ |][ x][k][]][⊤][PA][k][x][k][V][k][] +][ E][[][V][k][E][[][w]k[⊤][Pw][k][ |][ x][k][]]][ ≤]
_W ∥P_ _∥_ EVk.

Substituting the above terms into the RHS of (21), we have

_C + W_ _P_
EVk[2]+1 _[< ρ][2][E][V][ 2]k_ [+] 4A [2]W _P_ _κ(P_ ) + 2ρ(C + W _P_ ) _∥_ _∥_ +
_∥_ _∥_ _∥_ _∥_ 1 _ρ_

_−_

_C_ [2] + 2 CW ∥P _∥_ + W [2] _∥P_ _∥[2]_ 

= ρ[2]EVk[2] [+][ C][ +][ W][ ∥][P] _[∥]_ (1 + ρ)(C + W _P_ ) + 4A [2]W _P_ _κ(P_ ) _._ (22)

1 _ρ_ _∥_ _∥_ _∥_ _∥_
_−_
 

We can then proceed using induction: we have

EV0[2]

_[≤]_ _[W][ 2][ ∥][P]_ _[∥][2][ = (][W][ ∥][P]_ _[∥][)(][W][ ∥][P]_ _[∥][)]_

_< [C][ +][ W][ ∥][P]_ _[∥]_ (C + W _P_ )

1 _ρ_ _∥_ _∥_
_−_

_C + W_ _P_
_<_ _∥_ _∥_ (1 + ρ)(C + W _P_ ) + 4A [2]W _P_ _κ(P_ ) _._

(1 _ρ)(1_ _ρ[2])_ _∥_ _∥_ _∥_ _∥_
_−_ _−_

Assuming EVk _<_ (1C+ρW)(1 ∥Pρ ∥[2]) (1 + _ρ)(C + W_ _P_ ) + 4A [2]W _P_ _κ(P_ ), we also have

_−_ _−_ _∥_ _∥_ _∥_ _∥_

EVk+1 < (1C+ρW)(1 ∥Pρ ∥[2]) (1 + ρ)(C + W _P_ ) + 4A [2]W _P_ _κ(P_ ) by (22). Therefore, we have

_−_ _−_ _∥_ _∥_ _∥_ _∥_
 


_C + W_ _P_
EVk < _∥_ _∥_ (1 + ρ)(C + W _P_ ) + 4A [2]W _P_ _κ(P_ )

(1 _ρ)(1_ _ρ[2])_ _∥_ _∥_ _∥_ _∥_
_−_ _−_


= [(][M][ 2][A][ 2][ +][ W][ )][ ∥][P] _[∥][2]_ (1 + ρ)(M [2]A [2] + W ) + 4A [2]W κ(P )

(1 − _ρ)(1 −_ _ρ[2])_



for any k.


A linear dynamical system (1) driven by our safe switching policy can be viewed as an instance of
the system described in (10)-(11). Formally, we consider the following system:

_xk+1 = Axk + Buk + wk,_ (23)

We consider the following two classes of policies:

_• Linear feedback policy: uk = π[K](xk) = Kxk._
Safe switching policy: (uk, ξk+1) = ¯π[K,M,t](xk, ξk), where M > 0 is the switching thresh
_•_
old and t ∈ N[∗] is the “non-action” duration, and ¯π[K,M,t] determines uk, ξk+1 as follows:

**– If ξk > 0, then uk = 0, ξk+1 = ξk −** 1;


-----

**– If ξk = 0, max{∥K∥, ∥xk∥} ≥** _M_, then uk = 0, ξk+1 = t − 1;
**– If ξk = 0, max{∥K∥, ∥xk∥} < M**, then uk = Kxk, ξk+1 = 0.

Notably, the linear feedback policy can be viewed as a special case of the safe switching
policy where the switching threshold is infinity, i.e., π[K] = ¯π[K,][+][∞][,][1].

For the simplicity of expressions, when a safe switching policy ¯π[K][k][,M][k][,t][k] is applied at each step k,
let us define the following notations:

_i(0) = 0, i(k + 1) =_ _i(k) + 1_ _ui(k) ̸= 0_ _xk = xi(k)._ (24)
_i(k) + ti(k)_ _ui(k) = 0_ _[,][ ˜]_


In plain words, _x˜k_ = _xi(k)_ is the subsequence of _xk_ where “non-action” steps are skipped.
_{_ _}_ _{_ _}_ _{_ _}_
In addition, let

_W =_ lim _,_ (25)
_t→∞_ _[W][ +][ AWA][⊤]_ [+][ · · ·][ +][ A][t][W] [(][A][t][)][⊤]

which is a finite value since A is stable.

**Lemma 9. For the system described in (23)-(25), assume a safe switching policy ¯π[K][k][,M][k][,t][k]** _is_
_applied at each step k, Mk_ _M for any k, and there exists 0 < ρ < 1 and P_ 0 such that
_A[⊤]PA ≺_ _ρP_ _, then_ _≤_ _≻_

E ∥xk∥[4] _< 8_ _Q(M, ρ, P_ ) + W [2]κ(P )[2][] _,_

_where_ 

(M, ρ, P ) = [(][M][ 2][A][ 2][ +][ W][ )][ ∥][P] _[∥][2]_ (1 + ρ)(M [2]A [2] + W ) + 4A [2]W κ(P ) _,_
_Q_ (1 _ρ)(1_ _ρ[2])_

_−_ _−_

_A = ∥A∥_ + ∥B∥ _M._  


_Proof. According to (23) and (24), the state trajectory_ _x˜k_ evolves subject to the dynamics
_{_ _}_

_x˜k+1 = Akx˜k + ˜wk,_ (26)

where

_Ak =_ _A_ max{∥Kk∥ _, ∥x˜k∥} ≥_ _M_ _,_
_A + BKk_ otherwise


_w˜k_ ˜xk (0, Wk), Wk = _W_ max{∥Kk∥ _, ∥x˜k∥} ≥_ _M_ _._
_|_ _∼N_ _W + AWA[⊤]_ + + A[t][k] _W_ (A[t][k] )[⊤] otherwise
 _· · ·_

Let Vk = x[⊤]k _[Px][k][ and][ ˜]Vk = ˜x[⊤]k_ _[P][ ˜]xk. Since A[⊤]PA ≺_ _ρP_, the system (26) satisfies the assumption
of Lemma 8, and _Wk_ _W,_ _Ak_ _A_ + _B_ _M = A . Therefore, and therefore,_
_∥_ _∥≤_ _∥_ _∥≤∥_ _∥_ _∥_ _∥_

EV[˜]k[2] _[<][ (][M][ 2][A][ 2][ +][ W][ )][ ∥][P]_ _[∥][2]_ (1 + ρ)(M [2]A [2] + W ) + 4A [2]W κ(P ) = (M, ρ, P )λmin(P )[2].

(1 _ρ)(1_ _ρ[2])_ _Q_
_−_ _−_
  (27)

Next, to establish the relationship between EV[˜]k[2] [and][ E][V][ 2]k [, notice that]

_xk = A[k][−][i][(][j][)]x˜j + ˜wk,_ (28)

where j = sup{s ∈ N | i(s) ≤ _k}, i.e., ˜xj is the last state in {x˜k} that physically occurs no later_
than xk, and ˜wk = _l=0_ _A[k][−][i][(][j][)][−][1][−][l]wi(j)+l, Since A is stable, we have ∥E[ ˜wk ˜wk[⊤][]][∥≤]_ _[W][ .]_
Pre-multiplying both sides of (28) with P [1][/][2] and applying the triangle inequality, we have

[P][k][−][i][(][j][)][−][2]
_P_ [1][/][2]xk _P_ [1][/][2]A[k][−][i][(][j][)]x˜j + _P_ [1][/][2]w˜k _._
_≤_


-----

Therefore, applying the power means inequality ( _[a][+]2_ _[b]_ [)][4][ ≤] _[a][4][+]2_ _[b][4]_, we have

4 4 4[]
_Vk[2]_ [=] _P_ [1][/][2]xk _≤_ 8 _P_ [1][/][2]A[k][−][i][(][j][)]x˜j + _P_ [1][/][2]w˜k
 2

2

= 8 _x˜[⊤]j_ _A[k][−][i][(][j][)][][⊤]_ _PA[k][−][i][(][j][)]x˜j_ + _w˜k[⊤][P][ ˜]wk_

  2[]     !

_≤_ 8 _ρ[2(][k][−][i][(][j][))][ ˜]Vj[2]_ [+] _w˜k[⊤][P][ ˜]wk_ _._

Taking the expectation on both sides of the above inequality and applying (27), we get   

EVk[2] (M, ρ, P )λmin(P )[2] + W [2] _P_ _._

_[≤]_ [8] _Q_ _∥_ _∥[2][i]_
h

Finally, using the fact that Vk = x[⊤]k _[Px][k][ ≥]_ _[λ][min][(][P]_ [)][ ∥][x][k][∥][2][, we have]

E _xk_ EVk[2] (M, ρ, P ) + W [2]κ(P )[2][] _,_
_∥_ _∥[4]_ _≤_ _λmin(P_ )[2][ ≤] [8] _Q_



which concludes our proof.

**Lemma 10. For the system described in (23)-(25), assume K is a stabilizing linear feedback (i.e.,**
_ρ(A + BK) < 1) gain with ∥K∥≤_ _M_ _, and there exists 0 < ρ < 1 and P ≻_ 0 such that
_A[⊤]PA ≺_ _ρP, (A_ + _BK)[⊤]P_ (A + _BK) ≺_ _ρP_ _. Let J be the cost (defined in (3)) associated with the_
_linear feedback policy π[K], and J_ _[M,t]_ _be the cost associated with the safe switching policy ¯π[K,M,t]._
_Then_


_J_ _[M,t]_ _J_ 2 1(ρ, P, K) (M, t, ρ, P, K) + (M, t, ρ, P, K)[2],
_−_ _≤_ _C_ _G_ _G_

1/2

_W κ(P_ ) _Q + K_ _[⊤]RK_

1(ρ, P, K) = _,_
_C_ 1 _ρ_ !

_−_

(M, t, ρ, P, K) = 2(ρ, K)t (M, ρ, P ) + W [2]κ(P )[2][][1][/][4] (M, ρ, P ),
_G_ _C_ _Q_ _E_

_∞_



2(ρ, K) = _Q + K_ _[⊤]RK_ _BK_ (A + BK)[s] [2][n/][2+7][/][4]
_C_ _∥_ _∥·_ _∥_ _∥·_ _ρ[−][1][/][2]_ 1 _[,]_

_s=0_

_−_

X

_(_ 2(ρ, K) < _because ρ(A + BK) < 1),_
_C_ _∞_

(M, ρ, P ) = [(][M][ 2][A][ 2][ +][ W][ )][ ∥][P] _[∥][2]_ (1 + ρ)(M [2]A [2] + W ) + 4A [2]W κ(P )
_Q_ (1 _ρ)(1_ _ρ[2])_

_−_ _−_


(M, ρ, P ) = exp _._
_E_ _−_ [(1][ −]4W[ρ] κ[1][/]([4]P[)][2])[M][ 2]
 


_where_


_In particular, if the system specification A, B, Q, R, W, the stabilizing linear feedback gain K, and_
_the parameters ρ, P are all fixed, we have_

_J_ _[M,t]_ _−_ _J ∼O_ _tM exp(−cM_ [2])

_as M →∞, t →∞, tM exp(−cM_ [2]) → 0, where  _c =_ [(1]4W[−][ρ] κ[1]([/]P[4] )[)][2] _[.]_

_Proof. Let_ _xk_ _,_ _uk_ denote the state and input trajectories driven by π[K], and _x¯k_ _,_ _u¯k_ denote
_{_ _}_ _{_ _}_ _{_ _}_ _{_ _}_
the state and input trajectories driven by ¯π[K,M,t]. Then from π[K](x) = Kx and ¯π[K,M,t](x) ∈
_{Kx, 0}, we have_

1 _T −1_ 1 _T −1_

_J = lim_ _x[⊤]k_ [(][Q][ +][ K] _[⊤][RK][)][x][k]_ = lim E _x[⊤]k_ [(][Q][ +][ K] _[⊤][RK][)][x][k]_ _,_
_T_ _T_ _T_ _T_
_→∞_ [E] " _k=0_ # _→∞_ _k=0_

X X  

1 _T −1_ 1 _T −1_

_J_ _[M,t]_ lim sup E _x¯[⊤]k_ [(][Q][ +][ K] _[⊤][RK][)¯]xk_ = lim sup E _x¯[⊤]k_ [(][Q][ +][ K] _[⊤][RK][)¯]xk_ _._
_≤_ _T →∞_ " _T_ _k=0_ # _T →∞_ _T_ _k=0_

X X  


-----

Therefore, we only need to prove

E _x¯[⊤]k_ [(][Q][ +][ K] _[⊤][RK][)¯]xk_ E _x[⊤]k_ [(][Q][ +][ K] _[⊤][RK][)][x][k]_
_−_ _≤_
 2 1(ρ, P, K ) (M, t, ρ, P, K ) + (M, t, ρ, P, K )[2]

_C_ _G_ _G_

for every k. Let _Q[˜] = Q + K_ _[⊤]RK. Since Q ≻_ 0 and R ≻ 0, we have _Q[˜] ≻_ 0, and therefore

_∥x∥Q˜_ [≜] _x[⊤]Qx[˜]_ defines a norm. In the follows, we bound E ∥x¯k∥Q[2]˜ _[−]_ [E][ ∥][x][k][∥]Q[2]˜ [.]
q

We notice that


_x¯k = (A + BK)¯xk−1 + wk−1 −_ _BKx¯k−11{u¯k−1=0}_
= (A + BK) (A + BK)¯xk−2 + wk−2 − _BKx¯k−21{u¯k−2=0}_ + wk−1−

_BK_ _x¯k−11{u¯k−1=0}_ 
= · · ·

_k−1_ _k−1_

= (A + BK)[k]x0 + _s=0(A + BK)[s]wk−s−1 −_ _s=0(A + BK)[s]BKx¯k−s−11{u¯k−s−1=0}_

X X


_k−1_

(A + BK)[s]BKx¯k−s−11{u¯k−s−1}[.]
_s=0_

X


= xk
_−_


Hence,


_k−1_

_∥x¯k∥Q˜_ _[≤∥][x][k][∥]Q[˜]_ [+] _s=0_ (A + BK)[s]BKx¯k−s−11{u¯k−s−1} ˜Q

X

_k−1_

_≤∥xk∥Q˜_ [+] _Q[˜]_ _∥BK∥_ _∥(A + BK)[s]∥_ _x¯k−s−11{u¯k−s−1}_ _._

_s=0_

X

2

From the fact that E(X1 + · · · + Xn)[2] _≤_ EX1[2] [+][ · · ·][ +] EXn[2], where X1, . . ., Xn are

arbitrary random variables with bounded second-order moments, we havep p 

_k−1_

E ∥x¯k∥Q[2]˜ _[≤]_ qE ∥xk∥Q[2]˜ [+] _Q[˜]_ _∥BK∥_ Xs=0 _∥(A + BK)[s]∥_ rE h∥x¯k−s−1∥[2] **1{u¯k−s−1=0}i[!][2]** _._

By Cauchy-Schwarz inequality, we have

2

E ∥x¯k∥Q[2]˜ _[≤]_ E ∥xk∥Q[2]˜ [+] _Q[˜]_ _∥BK∥_ _ks=0−1_ _∥(A + BK)[s]∥_ E ∥x¯k−s−1∥[4][][ 1]4 P (¯uk−s−1 = 0)! _._

q X 

By Lemma 9, we have E _x¯k_ _s_ 1 8[ (M, ρ, P ) + W [2]κ(P )[2]] for any s, and hence,
_∥_ _−_ _−_ _∥[4]_ _≤_ _Q_

E ∥x¯k∥Q[2]˜ _[≤]_ E ∥xk∥Q[2]˜ [+] _Q[˜]_ _∥BK∥_ 2[3][/][4][Q(M, ρ, P ) + W [2]κ(P )[2]][1][/][4]
q 2

_k−1_

(A + BK)[s] P (¯uk _s_ 1 = 0) _._ (29)
_s=0_ _∥_ _∥_ _−_ _−_ !

X


According to the update rule of ξk in ¯π[K,M,t], we have


_t−1_

_{∥x¯k−t∥≥_ _M_ _} ._
_τ_ =0

[

_t−1_

P ( _x¯k_ _t_ _M_ ) .
_∥_ _−_ _∥≥_
_τ_ =0

X


_{u¯k = 0} ⊆_

Taking the union bound, we have

P (¯uk = 0) ≤


-----

By Lemma 6, we have

for every k and t, and hence


2[n/][2+1]
P ( _x¯k_ _t_ _M_ )
_∥_ _−_ _∥≥_ _≤_ _ρ[−][1][/][2]_ 1 _[E][(][M, ρ, P]_ [)]

_−_


P (¯uk = 0) _t_ [2][n/][2+1] (30)
_≤_ _ρ[−][1][/][2]_ 1 _[E][(][M, ρ, P]_ [)]

_−_

for every k. Substituting (30) into (29), we get

E _x¯k_ _Q˜_ E _xk_ _Q˜_ [+] _Q[˜]_ _BK_
_∥_ _∥[2]_ _[≤]_ _∥_ _∥[2]_ _∥_ _∥_ _ρ[2][−][n/][1][/][2+7][2]_ _[/][4]1_ [[][Q][(][M, ρ, P] [) +][ W][ 2][κ][(][P] [)][2][]][1][/][4]
q _−_ 2

_k−1_

_s=0_ _∥(A + BK)[s]∥E(M, ρ, P_ )!

X 2

_≤_ E ∥xk∥Q[2]˜ [+][ C][2][(][ρ, K][)[][Q][(][M, ρ, P] [) +][ W][ 2][κ][(][P] [)][2][]][1][/][4][E][(][M, ρ, P] [)]
q 2 

= E ∥xk∥Q[2]˜ [+][ G][(][M, t, ρ, P, K][)]
q 

= E ∥xk∥Q[2]˜ [+ 2] E ∥xk∥Q[2]˜ _[G][(][M, t, ρ, P, K][) +][ G][(][M, t, ρ, P, K][)][2][.]_
q


Applying Lemma 7 with M = 0, we have

E[x[⊤]k _[Px][k][]][ ≤]_ _[W][ ∥][P]_ _[∥]_

1 _ρ [,]_
_−_


and hence

Therefore,

and hence


E ∥xk∥Q[2]˜ _[≤]_ _Q[˜]_ _∥xk∥[2]_ _≤_ _[W][ κ]1[(][P]_ [)]ρ[ ∥][Q][∥] = C1(ρ, P, K)[2].

_−_

E _x¯k_ _Q˜_ _Q˜_
_∥_ _∥[2]_ _[−]_ [E][ ∥][x][k][∥][2] _[≤]_ [2][C][1][(][ρ, P, K][)][G][(][M, t, ρ, P, K][) +][ G][(][M, t, ρ, P, K][)][2][,]

_J_ _[M,t]_ _J_ 2 1(ρ, P, K) (M, t, ρ, P, K) + (M, t, ρ, P, K)[2],
_−_ _≤_ _C_ _G_ _G_


which concludes our proof.

A.1.3 A LAW OF LARGE NUMBERS FOR MARTINGALES

Let _k_ be a filtration of σ-algebras and _Sk_ be a matrix-valued stochastic process adapted to
_{F_ _}_ _{_ _}_
_F{Fk] =k}, we call Sk holds for all {Sk} a matrix-valued martingale (with respect to the filtration k. Now we can state a law of large numbers for matrix-valued martingales: {Fk}) if E[Sk+1 |_

**Lemma 11. (Liu et al., 2020, Lemma 4) If Sk = Φ0 + Φ1 + · · · + Φk is a matrix-valued martingale**
_such that_
E ∥Φk∥[2] _∼C(β),_

_where 0_ _β < 1, then Sk/k converges to 0 almost surely. Furthermore,_
_≤_

_Sk_ _β_ 1

_−_ _._
_k_ 2

_[∼C]_  


-----

A.1.4 A PERTURBATION ANALYSIS OF ALGEBRAIC RICCATI EQUATION

An interesting property of LQR is that near the optimal controller (which corresponds to the solution of the discrete algebraic Riccati equation), the perturbation of the discrete Lyapunov equation
solution is quadratic in the perturbation of controller gain. Basically, this results in the convergence
speed of the certainty equivalent control performance being twice that of the estimation error. Similar results have been reported in (Mania et al., 2019; Simchowitz & Foster, 2020), and here we
present a simplified version that would suffice for our purpose.
**Lemma 12. Consider the discrete Lyapunov equation**
_A[⊤]XA −_ _X + Q = 0,_
_where Q ≻_ 0 and ρ(A) < 1, then the unique positive definite solution X satisfies

_X_ _F_ _I_ _A[⊤]_ _A[⊤][][−][1]_
_∥_ _∥_ _≤_ _−_ _⊗_ 2 _[∥][Q][∥][F][ .]_
 


_Proof. Vectorizing the Lyapunov equation, we get_

vec(X) = _I_ _A[⊤]_ _A[⊤][][−][1]_ vec(Q),
_−_ _⊗_
and hence
 

_X_ _F =_ vec(X) 2 _I_ _A[⊤]_ _A[⊤][][−][1]_ _I_ _A[⊤]_ _A[⊤][][−][1]_
_∥_ _∥_ _∥_ _∥_ _≤_ _−_ _⊗_ 2 _[∥][vec(][Q][)][∥][2][ =]_ _−_ _⊗_ 2 _[∥][Q][∥][F][ .]_
   

**Lemma 13. Let P be the solution to the discrete algebraic Riccati equation**

_P = Q + A[⊤]PA −_ _A[⊤]PB_ _R + B[⊤]PB_ _−1 B⊤PA,_
_and let K be defined as_
  

_K = −_ _R + B[⊤]PB_ _−1 B⊤PA._

_Assume_ _K[ˆ] is such that_ _A[˜] = A + BK[ˆ] is stable, and _ _P[ˆ] satisfies the discrete Lyapunov equation_
_Pˆ = Q + K[ˆ]_ _[⊤]RK[ˆ] + A[˜][⊤]P[ˆ]A.[˜]_ (31)


_Let ∆P = P[ˆ] −_ _P, ∆K = K[ˆ] −_ _K, then_

_∥∆P_ _∥F ≤_ _I −_ _A[˜][⊤]_ _⊗_ _A[˜][⊤][][−][1]_ 2 _∥R∥F ∥∆K∥F[2]_ _[.]_


_Proof. It can be shown P satisfies the discrete Lyapunov equation_

_P = Q + K_ _[⊤]RK + (A + BK)[⊤]P_ (A + BK). (32)

Meanwhile, substituting ∆P = P[ˆ] − _P, ∆K = K[ˆ] −_ _K into (31), we get_
_P + ∆P_

= Q + (K + ∆K)[⊤] _R (K + ∆K) + (A + B (K + ∆K))[⊤]_ (P + ∆P ) (A + B (K + ∆K))

= Q + K _[⊤]RK + ∆K_ _[⊤]R∆K + (A + BK)[⊤]P_ (A + BK) + A[˜][⊤]∆P _A,[˜]_ (33)
where for the second equality we used the fact
_R + B[⊤]PB_ _K + B[⊤]PA = 0._
By taking the difference between (33) and (32), we can see ∆P also satisfies a discrete Lyapunov
  
equation:


∆P = ∆K _[⊤]R∆K + A[˜][⊤]∆P_ _A.[˜]_
Applying Lemma 12, we obtain

∆P _F_ _I_ _A[⊤]_ _A[⊤][][−][1]_ ∆K _[⊤]R∆K_ _F_
_∥_ _∥_ _≤_ _−_ [˜] _⊗_ [˜] 2


_≤_ _I −_ _A[˜][⊤]_ _⊗_ _A[˜][⊤][][−][1]_ 2 _∥R∥F ∥∆K∥F[2]_ _[,]_


which concludes our proof.


-----

A.2 PROOF OF THEOREM 1

_Proof. According to our definition of bounded-cost safety, we only need to prove J_ _[π][k]_ _< +∞_ for
every k, where J _[π][k]_ is defined in (3).

Let _xi_ _,_ _ui_ denote the state and input trajectories driven by πk. According to Algorithm 2, we
_{_ _}_ _{_ _}_
have ui _K[ˆ]_ _kxi, 0_, and therefore,
_∈{_ _}_

1 _T −1_ 1 _T −1_

_J_ _[π][k]_ lim sup E _x[⊤]i_ _Q + K[ˆ]_ _k[⊤][R][ ˆ]Kk_ _xi_ = lim sup E _x[⊤]i_ _Q + K[ˆ]_ _k[⊤][R][ ˆ]Kk_ _xi_ _._
_≤_ _T →∞_ " _T_ _i=0_ # _T →∞_ _T_ _i=0_

X   X h   i

Notice that under πk, there is xi+1 = Axi + wi as long as ∥xi∥≥ log k, and since A is stable, there
exists 0 < ρ < 1 and P ≻ 0 such that A[⊤]PA ≺ _ρP_ . By Lemma 7, we have

E _x[⊤]i_ _[Px][i]_ _<_ [((log][ k][)][2][A][ 2][ +][ ∥][W] _[∥][)][ ∥][P]_ _[∥]_ _,_

1 − _ρ_

 

where A = max _A_ _,_ _A + BK[ˆ]_ _k_ . Therefore, by x[⊤]i _[Px][i]_ _λmin(P_ ) _xi_ and
_{∥_ _∥_ _∥_ _∥}_ _≥_ _∥_ _∥[2]_

_x[⊤]i_ _Q + K[ˆ]_ _k[⊤][R][ ˆ]Kk_ _xi ≤∥Q + K[ˆ]_ _k[⊤][R][ ˆ]Kk∥∥xi∥[2], we have_
 

_Kk[⊤][R][ ˆ]Kk_
E _x[⊤]i_ _Q + K[ˆ]_ _k[⊤][R][ ˆ]Kk_ _xi_ _<_ [((log][ k][)][2][A][ 2][ +][ ∥][W] _[∥][)][κ][(][P]_ [)][∥][Q][ + ˆ] _∥_ _._
h   i 1 − _ρ_

This implies

_Kk[⊤][R][ ˆ]Kk_
_J_ _[π][k]_ _∥_ _< +_ _,_
_≤_ [((log][ k][)][2][A][ 2][ +][ ∥][W]1[∥][)][κ][(]ρ[P] [)][∥][Q][ + ˆ] _∞_

_−_

which concludes our proof.

A.3 PROOF OF THEOREM 2


_Proof. We shall assume throughout the proof that_ _k_ is the σ-algebra generated by the random
_{F_ _}_
variables _x0, w0, . . ., wk, ζ0, . . ., ζk_ .
_{_ _}_

We first cast the system (1) into a static form by writing xk as


_k−1_

_A[t]Buk_ _t_ 1 +
_−_ _−_
_t=0_

X


_k−1_

_A[t]wk−t−1_
_t=0_

X


_xk = A[k]x0 +_

= A[k]x0 +


_k−1_

_Ht_ _u˜k_ _t_ 1 + (k _t)[−][β]ζk_ _t_ 1
_−_ _−_ _−_ _−_ _−_
_t=0_

X 


_k−1_

_A[t]wk−t−1._ (34)
_t=0_

X


In order to estimate Hτ, post-multiply both sides of (34) with ζk[⊤] _τ_ 1 [and rearrange the terms, and]
_−_ _−_
we get

_k−1_ _k−1_

_xkζk[⊤]_ _τ_ 1 [=] _A[t]wk_ _t_ 1 + A[k]x0 _ζk[⊤]_ _τ_ 1 [+] (k _t)[−][β]Htζk_ _t_ 1ζk[⊤] _τ_ 1[+]
_−_ _−_ _t=0_ _−_ _−_ ! _−_ _−_ _t=0_ _−_ _−_ _−_ _−_ _−_

X X

_k−1_

_Htu˜k_ _t_ 1ζk[⊤] _τ_ 1[.] (35)
_−_ _−_ _−_ _−_
_t=0_

X


To take the expectation of (35), notice that for any time step k, the variables x0, ζ0, ζ1, . . ., ζk,
_w0, w1, . . ., wk, ˜u0, ˜u1, . . ., ˜uk+1 are measurable w.r.t. Fk, which, together with the independence_
among x0, ζ0, ζ1, . . ., ζk, w0, w1, . . ., wk, leads to the following relations:

_I_ if k1 = k2
E _ζk1_ _ζk[⊤]2_ 2[−][1] = 0 otherwise, (36)

_[| F][k]_ 
 

E _u˜k1_ _ζk[⊤]2_ 2[−][1] = 0other values otherwiseif k1 ≤ _k2,_ (37)

_[| F][k]_ 
 

E _wk1_ _ζk[⊤]2_ 2[−][1] = 0, (38)

_[| F][k]_

E x0ζk[⊤]2 2[−][1] = 0, (39)

_[| F][k]_
 


-----

for any two time steps k1 0 and k2 1.
_≥_ _≥_

Now let us prove the conclusion using induction on τ .

First consider the case τ = 0: we have


_k−1_

_i=0_

X


_Hˆk,0_ _H0 = [1]_
_−_ _k_


(i + 1)[β]xiζi[⊤]−1 _[−]_ _[H][0]_ = k[1]



(i + 2)[β]xi+1ζi[⊤] _._ (40)

_[−]_ _[H][0]_



_i=1_


Let Φk = (k + 1)[β]xk+1ζk[⊤]

_[−]_ _[H][0][. By substituting (35) and (36) to (39) into (40) it can be seen]_
E[Φk | Fk−1] = 0, and hence Sk ≜ [P]i[k]=0 [Φ][i][ is a martingale w.r.t.][ {F][k][}][. Furthermore, we can]
verify E ∥Φk∥[2] _∼C(2β): by Cauchy-Schwarz inequality,_

E (k + 1)[β]xk+1ζk[⊤] (k + 1)[2][β] E _xk+1_ E _ζk_ _._
_≤_ _∥_ _∥[4]_ _∥_ _∥[4]_
q q

By the procedure described in Algorithm 2, the switching threshold is no larger than log k for every

[2]

step before k, and therefore, according to Lemma 9,

E ∥xk∥[4] _< 8_ _Q(log k, ρ, P_ ) + W [2]κ(P )[2][] _∼O_ (log k)[8][] _∼C(0),_

where 0 < ρ < 1 and P ≻ 0 are constants such that A[⊤]PA  _≺_ _ρP_, which exist due to the
stability of A, Q is defined as in Lemma 9, and W is defined in (25). Also taking note of the fact
E ∥ζk∥[4] = p(p + 2) ∼C(0), we have

E (k + 1)[β]xk+1ζk[⊤] (2β),
_∼C_

and hence,

[2]

E Φk = E (k + 1)[β]xk+1ζk[⊤] E (k + 1)[β]xk+1ζk[⊤] + _H0_ 2
_∥_ _∥[2]_ _[−]_ _[H][0]_ _≤_ _∥_ _∥_

2 E (k + 1)[β]xk+1ζk[⊤] + _H0_   (2β). 
_≤_ [2] ∥ _∥[2][]_ _∼C_


By applying Lemma 11 to the martingale _Sk_ defined above, we get[2]
_{_ _}_

_Hˆk,0_ _H0_ _β_ _._
_−_ _∼C_ _−_ [1]2
 

Now assume that τ ≥ 1 and that we already have

_Hˆk,t_ _Ht_ _β_ (41)
_−_ _∼C_ _−_ 2[1]
 

for t = 0, 1, . . ., τ − 1. Then


_Hˆk,τ_ _Hk,τ_
_−_


_τ_ _−1_

_xi −_ _t=0_ _Hˆk,tu˜i−t−1_

X


(i − _τ_ )[β]

(i + 1)[β]

(

(i + 1)[β]

(


_ζi[⊤]−τ_ _−1_ _[−]_ _[H][k,τ]_


_k −_ _τ_

1

_k −_ _τ_

1

_k −_ _τ_


_i=τ_ +1

_k−τ_ _−1_

_i=0_

X

_k−τ_ _−1_

_i=0_

X


_τ_ _−1_

_Hˆk,tu˜i+τ_ _−t_
_t=0_

X


_ζi[⊤]_

# _[−]_ _[H][k,τ]_

_ζi[⊤]_

_[−]_ _[H][k,τ]_ )


_xi+τ_ +1
_−_

_xi+τ_ +1
_−_


_τ_ _−1_

_Htu˜i+τ_ _−t_
_t=0_

X


_τ_ _−1_

_t=0_

X


1 _k−τ_ _−1_

(i + 1)[β]u˜i+τ _tζi[⊤][.]_ (42)

_k_ _τ_ _−_
_−_ _i=0_

X


_Hˆk,t_ _Ht_
_−_


Completely similarly to the case τ = 0, we can show


_k−τ_ _−1_

_i=0_

X


_τ_ _−1_

_Htu˜i+τ_ _−t_
_t=0_

X


_β_
_∼C_ _−_ 2[1]



(i + 1)[β]


_ζi[⊤]_

_[−]_ _[H][k,τ]_


_xi+τ_ +1
_−_


_k −_ _τ_


-----

Meanwhile, for each of t = 0, 1, . . ., τ − 1, define Φ[t]k [= (][k][ + 1)][β][ ˜]uk−tζk[⊤][. In view of (37) it can be]
shown E[Φ[t]k _[| F][k][−][1][] = 0][, and hence][ S]k[t]_ [≜] [P]i[k]=0 [Φ]k[t] [is a martingale w.r.t.][ {F][k][}][. Furthermore, by]
the procedure described in Algorithm 2, ∥u˜k−t∥[2] _≤_ (log(k − _t))[4]_ _≤_ (log k)[4], we have

E Φ[t]k (k + 1)[2][β] (log k)[4] E _ζk_ (2β).
_≤_ _∥_ _∥[2]_ _∼C_

By applying Lemma 11 to the martingale[2] _Sk[t]_ [defined above, we get]


_k−τ_ _−1_

_i=0_ (i + 1)[β]u˜i+τ _−tζi[⊤]_ _[∼C]_ β − [1]2

X


_k −_ _τ_


which together with (41) implies


_β_
_∼C_ _−_ 2[1]
 


_k−τ_ _−1_

_i=0_ (i + 1)[β]u˜i+τ _−tζi[⊤]_ _[∼C]_ 2 β − [1]2

X


_Hˆk,t_ _Ht_
_−_


_k −_ _τ_


Now that we have shown the RHS of (42) is the sum of τ + 1 matrices, each of order _β_ 2, we
_C_ _−_ [1]

get

  

_Hˆk,τ_ _Hk,τ_ _β_ _._
_−_ _∼C_ _−_ 2[1]
 

According to our definition of C(α), this is to say it holds almost surely that


_Hˆk,τ_ _Hτ_
_−_ = 0,

_k[−][γ][+][ϵ]_


lim
_k→∞_


where γ = 1/2 − _β > 0, for any ϵ > 0, which concludes our proof._

A.4 PROOF OF THEOREM 3

Let us denote by JW[π] [the cost of a policy][ π][ when acting on a system in the form (1) with process]
noise covariance W . Let us consider the following variants of policies:

_• π[∗]: optimal policy, i.e., π[∗](x) = K_ _[∗]x, with K_ _[∗]_ defined in (6).

ˆπk: certainty equivalent policy with no exploratory noise, i.e., ˆπk(x) = K[ˆ] _kx._

_•_

˜πk: safe switching policy with no exploratory noise, i.e., Algorithm 2 invoked as

_•_
_π(x, ξ; k,_ _K[ˆ]_ _k, +_ ).
_∞_

_πk: safe switching policy with exploratory noise, i.e., the actually applied policy at step k._

_•_


Notice that J _[π][k]_ = JW[π][k] [and][ J] _[∗]_ [=][ J]W[π][∗] [. Our plan is decomposing][ J] _[π][k][ −]_ _[J]_ _[∗]_ [as]

_J_ _[π][k]_ _J_ _[∗]_ = _JW[π][k]_ _Wπk +(k+1)[−][2][β]_ _I_ + _JWπ[˜]k +(k+1)[−][2][β]_ _I_ _Wπk +(k+1)[−][2][β]_ _I_
_−_ _[−]_ _[J][ ˜]_ _[−]_ _[J][ ˆ]_
_JWπ[ˆ]k +(k+1)[−][2][β]_ _I_ _Wπk_ + _JWπ[ˆ]k_ _W_ _,_ 

_[−]_ _[J][ ˆ]_ _[−]_ _[J]_ _[π][∗]_
   

and bounding the RHS terms respectively. We next tackle these terms in reverse order:


1. Jthe cost isWπ[ˆ]k _[−]_ _[J]W[π][∗] J[: for an arbitrary stabilizing linear feedback policy]W[π]_ [= tr(][WP] [)][, where][ P][ solves the discrete Lyapunov equation (32). By][ π][(][x][) =][ Kx][, we know]
Theorem 2, the Markov parameter estimates converge as C(−γ). Since with random input

in Algorithm 3, the matrix _U_ _[h]_ is full row rank, it follows that the pseudo-inverse operator

0[h]

X 

is differentiable, which together with Lemma 2 guarantees that _A[ˆ]k_ _A,_ _B[ˆ]k_ _B_ ( _γ)._
Due to the differentiability of the stabilizing solution of the discrete algebraic equation, − _−_ _∼C_ _−_
there is also _K[ˆ]_ _k_ _K_ _[∗]_ ( _γ). In particular,_ _K[ˆ]_ _k_ converges to K _[∗]_ almost surely,
_−_ _∼C_ _−_ _{_ _}_
and since K _[∗]_ is stabilizing, we have for almost every realization of randomness, _K[ˆ]_ _k is_
also stabilizing for sufficiently large k. Assuming w.l.o.g. that _K[ˆ]_ _k is stabilizng for any_


-----

corresponding tok, we have JWπ[ˆ]k _[−] K[J][∗]W[π], also the solution to the Riccati equation (5), and[∗]_ [= tr(][W] [(][P][k][ −] _[P][ ∗][))][, where][ P][ ∗]_ [is the Lyapunov equation solution] Pk is the Lyapunov
equation solution corresponding to _K[ˆ]_ _k. According to Lemma 13, we have_

_−1_ 2
_∥Pk −_ _P_ _[∗]∥F ≤_ _I −_ _A[˜][⊤]k_ _[⊗]_ _A[˜][⊤]k_ 2 _∥R∥F_ _K[ˆ]_ _k −_ _K_ _[∗]_ _F_ _[,]_
 

_−1_
where _A[˜]k = A + BK[ˆ]_ _k. Since every_ _A[˜]k is stable, we have_ _I_ _A[⊤]k_ _A[⊤]k_ is
_−_ [˜] _[⊗]_ [˜] 2
 

a continuous function of _A[˜]k. Meanwhile,_ _A[˜]k_ converges to A + BK _[∗], which implies_
_−1_ _{_ _}_
_I −_ _A[˜][⊤]k_ _[⊗]_ _A[˜][⊤]k_ 2 is bounded. Hence, _K[ˆ]_ _k −_ _K_ _[∗]_ _∼C(−γ) implies Pk −_ _P_ _[∗]_ _∼_
  _πk_

2. JCW(π[ˆ]−k +(2γk+1)). Finally, we have[−][2][β] _I_ _Wπk_ [: from] JW[ J][ˆ] [ ˆ]Wπ[−]k [= tr(][J]W[π][∗] [= tr(][WP][k][W][)][, with][(][P][k][ −][ P][k][P][ defined the same as above, we have][ ∗][))][ ∼C][(][−][2][γ][)][.]

_JWπ[ˆ]k +(k+1)[−][2][β]_ _I_ _[−]_ _[J][ ˆ]Wπk_ [= tr((][k][ + 1)][−][2][β][P][k][)][. Since][ {][P][k][}][ converges to][ P][ ∗] _[≻]_ [0][, we have]

_JWπ[ˆ]k +(k+1)[−][2][β]_ _I_ _[−]_ _[J]W[ ˆ]πk_

_[−]_ _[J][ ˆ]_ _[∼C][(][−][2][β][)][.]_

3. Jditions of Lemma 10, we first fixWπ[˜]k +(k+1)[−][2][β] _I_ _[−]_ _[J][ ˆ]Wπk +(k+1)[−][2][β]_ _I_ [: we can basically apply Lemma 10. To verify the con-] ρ, P : since A + BK _[∗]_ is stable, for any fixed Q ≻ 0,
the discrete Lyapunov equation (A + BK _[∗])[⊤]P_ (A + BK _[∗]) + Q = P has a solution_
_P ≻_ 0. Therefore, there exists P ≻ 0, 0 < ρ < 1, such that (A + BK _[∗])[⊤]P_ (A +
_BK_ _[∗]) < ρP_ . Since _K[ˆ]_ _k_ converges to K _[∗]_ almost surely, we may assume w.l.o.g.
_{_ _}_
that (A + BK[ˆ] _k)[⊤]P_ (A + BK[ˆ] _k) < ρP for any k._ Furthermore, since A is stable,
there is A[t] _→_ 0 as t →∞, and therefore, with t = ⌊log k⌋ sufficiently large, we
also have (A[t])[⊤] _PA[t]_ _< ρP_ . For an upper bound of the noise magnitude, we insert
_W + I in place of W in (25) to compute W ._ Now that A, B, Q, R, W, ρ, P are all
fixed, we can apply the conclusion of Lemma 10 by taking the limit _K[ˆ]_ _k_ _K_ _[∗]_ to ob_πk_ _πk_ _→_
tain JW[˜] +(k+1)[−][2][β] _I_ _W +(k+1)[−][2][β]_ _I_

_[−]_ _[J][ ˆ]_ _[∼O][((log][ k][)][2][ exp(][−][c][(log][ k][)][2][))][ ∼C][(][−∞][)][.]_

4. Jdiffer only in theW[π][k] _[−]_ _[J][ ˜]Wπk +(k+1) u[−][2][⊤]k[β]_ _I[Ru][: these two costs are associated with the same closed-loop system and][k][ terms. In particular,][ J]W[π][k]_ _[−]_ _[J][ ˜]Wπk +(k+1)[−][2][β]_ _I_ [= tr((][k][ + 1)][−][2][β][R][)][ ∼]

_C(−2β)._

Adding up the above four terms using Lemma 2, we obtain J _[π][k]_ _−J_ _[∗]_ _∼C(max{−2β, −2γ, −∞}) ∼_
_C(−_ min{2β, 2γ}). According to our definition of C(α), this is to say it holds almost surely that

_J_ _[π][k]_ _J_ _[∗]_
lim _−_
_k_ _k[−]_ [min][{][2][β,][2][γ][}][+][ϵ][ = 0][,]
_→∞_

for any ϵ > 0, which concludes our proof.

B AN ILLUSTRATION OF OSCILLATION UNDER SWITCHING

In this section, we provide a simple illustrative example to explain why we need prolonging “nonaction” period in Algorithm 2.

Consider a simple two-dimensional noise-free system

_xk+1 = Akxk,_

where the candidates for Ak are


0.5 2
_A0 =_
0 0.5



0.5 0
_, A1 =_
2 0.5



It can be seen ρ(A0) = ρ(A1) = 0.5, i.e., both the system matrices are stable.

Now consider the following switching strategy:


-----

_• If ∥xk∥≥_ _M_, then apply A0 for t consecutive steps.

Otherwise, apply A1.

_•_

Figure 4 shows simulation results with x0 = (0.1, 1), M = 1 and t = 1, 2. We can observe that
even for this simple system, the frequent switching caused by t = 1 may cause the state to oscillate,
while t = 2 suffices to suppress the oscillation. Indeed, we can verify ρ(A1A0) 4.5 > 1 even
_≈_
though both A0 and A1 are stable. However, as long as A0 is stable, A1A[t]0 [will eventually become]
stable as t is chosen to be sufficiently large. This explains why we use prolonging t in our policy.

Threshold

2 Init. state

_t = 1_
_t = 2_

1

0

_−1_

_−1_ 0 1 2 3 4


Figure 4: Illustrative example of oscillation under switching. In this example, t = 1 causes the state
to oscillate, while t = 2 can suppress the oscillation.


-----

