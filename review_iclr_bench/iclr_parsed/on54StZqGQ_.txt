# DEGRADATION ATTACKS ON CERTIFIABLY ROBUST
## NEURAL NETWORKS

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Certifiably robust neural networks employ provable run-time defenses against adversarial examples by checking if the model is locally robust at the input under evaluation. We
show through examples and experiments that these defenses are inherently over-cautious.
Specifically, they flag inputs for which local robustness checks fail, but yet that are not
adversarial; i.e., they are classified consistently with all valid inputs within a distance of
_ϵ. As a result, while a norm-bounded adversary cannot change the classification of an_
input, it can use norm-bounded changes to degrade the utility of certifiably robust networks by forcing them to reject otherwise correctly classifiable inputs. We empirically
demonstrate the efficacy of such attacks against state-of-the-art certifiable defenses.

1 INTRODUCTION

An adversarial example for a neural classifier is the result of applying small modifications to a correctly
classified valid input such that the modified input is classified incorrectly. For neural classifiers trained in
a standard manner, it has been shown that adversarial examples are rampant (Szegedy et al., 2014; Carlini
& Wagner, 2017). While training models in an adversarially-aware manner helps mitigate the issue to
an extent, it does not solve the problem, as evidenced by the large gaps between the clean accuracy and
verified robust accuracy[1] of models.

Certifiably robust classifiers (Weng et al., 2018; Wong et al., 2018; Cohen et al., 2019; Leino et al., 2021)
offer the most rigorous solution to the problem and provably protect models against adversarial attacks.
These classifiers are constructed by composing a standard classifier with a certified run-time defense. The
defenses aim to detect adversarial examples during model evaluation, by checking if the model is ϵ-locally
robust at the evaluated input. If the check fails, the input is flagged as (potentially) adversarial and the
model rejects the input. However, a rejection is not free of cost. Every time the model rejects an input,
a user of the model has to resort to some other process other than their presumed first choice (the defended
model) to make a prediction, reducing model utility.

We show that existing certified run-time defenses are overly cautious and susceptible to erroneously
flagging non-adversarial inputs as adversarial. This over-cautiousness is inherent in the design of such
defenses, and it is manifested even when the ϵ-local robustness check is exact (i.e., the defense is complete).
Not only does this lead to a degradation in model utility because of unnecessary rejections, but it also
exposes models to a new line of attack that we refer to as degradation attacks. We develop new attacks
that are aimed at causing certifiably robust classifiers to reject inputs frequently, and we show that this
is a significant problem in practice. For state-of-the-art certifiably robust models, such as GloRo Nets
(Leino et al., 2021) and randomized smoothed models (Cohen et al., 2019), our attacks succeed on as
many as 56% of the inputs where the model is already known to be robust. This is particularly distressing
given the already considerable computational costs of training and defending certifiably robust models.

As a concrete scenario, consider an autonomous driving system that uses a neural classifier for labeling
road signs. It is unsafe for the classifier to misclassify adversarially perturbed road signs, and we want
to prevent this at all costs. One simple strategy is for the classifier to always reject inputs and hand over
decision-making to the human driver. This is perfectly safe behavior, but this model has zero utility. Ideally,
we want the model to hand over control (i.e., the run-time defense should raise a flag) only when the
perturbed example is actually going to cause misclassification. Our result implies that an adversary can
cause the model to hand over control to the human driver even when a perturbed input would not have

1% of test inputs where the model is accurate and also locally robust


-----

been misclassified. Although this does not happen for all the inputs (as in our simple example), it happens
often enough that the adversary can cause a drastic and unnecessary reduction in model utility.

Degradation attacks are successful because certified run-time defenses do not account for the data manifold.
An adversarial example is obtained by applying modifications to valid inputs. We formalize valid inputs
by means of a set M which is the support set of the underlying input distribution that characterizes the
learning problem. Although the exact description of M is unknown, we know that both the training and
test data used are in M. In order for a model to be free of adversarial examples, it only needs to be locally
robust at every point in M. Certified defenses, however, try to enforce local robustness at all evaluated
inputs, irrespective of whether the input belongs to M. An incorrectly flagged non-adversarial input is
then an input at which the local robustness check fails but if the model were not to reject the input, it would
be classified consistently with all valid inputs within an ϵ distance. While a norm-bounded adversary
cannot change the classification of an input in the presence of a certified run-time defense, it can apply
norm-bounded modifications to existing valid inputs (i.e., inputs in M) and force the model to reject
otherwise correctly classifiable inputs, thereby degrading the model’s utility.

As we discuss in Section 4, defenders against degradation attacks have two options: they can use existing
methods for constructing certifiably robust models but trained and validated with double the radius that
the adversary is allowed for perturbations, or they can develop new defense techniques that account for
the fact that models only need to be locally robust at points in M. We evaluate the ramifications of the
former option and leave the latter for future work.

To summarize, the main contributions of our work are as follows: (1) we describe new attacks, referred
to as degradation attacks, that can force certifiably robust models (with either deterministic or stochastic
defense mechanisms) to frequently and unnecessarily reject inputs; (2) we empirically demonstrate the
severity of the problem for models using state-of-the-art certified defense mechanisms like GloRo Nets
and Randomized Smoothing; (3) we make explicit the set, M, from which valid inputs are drawn, and
this helps us explain that certified run-time defenses, based on checking ϵ-local robustness, are susceptible
to degradation attacks, as they enforce local robustness at all inputs and not just inputs in M; and (4) we
discuss two possible defenses against degradation attacks, and evaluate the one based on doubling the
radius enforced by certifiably robust models.

The rest of this paper is organized as follows. In Section 2, we revisit definitions of adversarial robustness
and sketch a general approach for certified defenses. In Section 3, we demonstrate how a certified defense
can unnecessarily reject inputs; we present degradation attack algorithms to this end, and evaluate their
efficacy. In Section 4, we sketch two defenses against degradation attacks, and concretely evaluate one
of these proposals. In Section 5, we describe the related work. Finally, we conclude in Section 6.

2 REVISITING DEFINITIONS OF ADVERSARIAL ROBUSTNESS

A neural classifier f ∈R[d] _→L is a function from a d-dimensional real vector space, R[d], to a finite set of_
labels, L. The inputs to the classifier are drawn from some distribution with support set M ⊆R[d]. In other
words, the training and test sets are comprised of elements from M and their corresponding labels. Though
the description of M is not available to us, it plays a key role in formalizing the notion of adversarial
examples.

An adversarial example for a neural classifier is the result of applying small modifications to a correctly
classified valid input such that the modified input is classified incorrectly. Formally, an input is valid if
it belongs to M. Definition 1 below, proposed by Szegedy et al. (2014), formalizes the notion of a targeted
adversarial example.
**Definition 1 (Targeted Adversarial Example). Given a neural classifier f ∈R[d]** _→L and an input x∈R[d],_
_an input x[′]_ :=x+r is an adversarial example with respect to a target label l _∈L, ℓp metric, and a fixed_
_constant ϵ∈R if the solution r to the following optimization problem is such that ||r||p_ _≤ϵ:_

_Minimize ||r||p subject to f(x+r)=l and x+r_ _∈[0,1][d]_

_Untargeted adversarial examples, or simply adversarial examples, are defined similarly except that the_
constraint f(x+r)=l is replaced with f(x+r)≠ _f(x)._

As stated informally in (Carlini & Wagner, 2017; Szegedy et al., 2014) x needs to be a valid input, i.e.,
_x_ _∈_ _M, for x[′]_ to be an adversarial example. Making this requirement explicit and formal (via set M) is
a key step in the design of our degradation attacks.


-----

**Algorithm 2.1: Prediction with a certified run-time defense**


**Inputs: A model f ∈R[d]** _→L, an input x[′]_ _∈R[d], an attack bound ϵ∈R, and a distance metric ℓp_
**Output: A prediction l** _∈L ∪{⊥}_

**1 PredictWithDefense(f, x[′]** _, ϵ, ℓp):_

**2** **if f is (ϵ,ℓp)−locally robust at x[′]** **then**

**3** **return f(x[′])**

**4** **else**

**5** **return ⊥**

A classifier is protected from adversarial examples with respect to a valid input x if it is locally robust
at x. As stated in Definition 2, a classifier is locally robust at x, if its prediction does not change in an
_ϵ-ball centered at x._

**Definition 2 (Local Robustness). A neural classifier f ∈R[d]** _→L is (ϵ,ℓp)-locally robust at x∈R[d]_ _if,_

_∀x[′]_ _∈R[d]. ||x−x[′]||p_ _≤ϵ =⇒_ _f(x)=f(x[′])_

The formal notions of validity and local robustness enable us to make the following key observation.

**Observation 3. A classifier is protected from all adversarial examples if it is locally robust at all points**
_in M._

**Certified run-time defenses.** Classifiers, when composed with certified run-time defenses, are
guaranteed protection from adversarial examples. These defenses, briefly surveyed in Section 5, detect
adversarial examples during model evaluation (or run-time) by checking if the model is locally robust
at the input being evaluated. Algorithm 2.1 describes the manner in which certified run-time defenses
are deployed for protection.

The common assumption is that an adversary has white-box access to f as well as to valid inputs (denoted
_x), and that the adversary may or may not succeed in building adversarial examples (denoted x[′]) that_
are ϵ-close to x under the ℓp metric. Given input x[′], a certified run-time defense helps decide whether
_x[′]_ is an adversarial example or not. As described by PredictWithDefense (lines 1-5), the defense
mechanism is first used to check if the model f is locally robust at x[′] (line 2). If the check passes, then
_x[′]_ is guaranteed to not be an adversarial example and the model prediction at x[′] is returned as the label
(line 3). However, if the check fails, then x[′] maybe an adversarial example, though it not guaranteed to
be one. The model conservatively rejects the input, indicated here as returning the ⊥ label (line 5).

3 DEGRADATION ATTACKS

In this section, we describe our degradation attack algorithms and demonstrate their efficacy empirically.
First, in Section 3.1, by means of an example, we demonstrate how certified defenses can lead to
unnecessary rejections. Next, in Section 3.2, we describe our degradation attack algorithms against
deterministic and stochastic certified defenses. Finally, in Section 3.3, we evaluate our attacks empirically.

3.1 AN EXAMPLE OF A CERTIFIED DEFENSE FAILURE


Certified run-time defenses can erroneously flag a non-adversarial
input as adversarial because they enforce local robustness at all
inputs and not just inputs in M. Consider the example in Figure 1.
_x is a valid input from M. Let us assume that none of the points in_
the gray region belong to M. A binary classifier f ∈R[2] _→{W,G}_
assigns label W to all points in the white region (which includes
_x and x[′]) and label G to all points in the gray region. x and x[′]_
are ϵ apart and the radius for both the circles is ϵ. The (ϵ,l2)local robustness check at x[′] will fail since the ϵ-ball centered at x[′]
includes white and gray points. However, x[′] is not an adversarial


Figure 1: A flagged non-adversarial
example


-----

**Algorithm 3.1: Degradation attack algorithm**

**Inputs: A model f ∈R[d]** _→L, an input x∈M, an attack bound ϵ∈R, and a distance metric ℓp_
**Output: An attack input x[′]** _∈R[d]_

**1 DegradationAttack(f, x, ϵ, ℓp):**

**2** _x[′′]_ := Attack(f,x,2ϵ,ℓp)

**3** _x[′]_ := Project(x,x[′′],ϵ,ℓp)

**4** **return x[′]**

**Algorithm 3.2: Smoothed projected gradient descent attack (SPGD)**


**Inputs: A model f ∈R[d]** _→R[|][L][|]_ mapping inputs to logit outputs, a loss function L, an input x∈M, an attack
bound ϵ∈R, a distance metric ℓp, a step size η, a number of steps N, a number of samples n, and a noise
parameter σ

**Output: An attack input x[′]** _∈R[d]_

**1 SmoothedPgdAttack(f, L, x, ϵ, ℓp, η, N, n, σ):**

**2** _x[′]_ :=x

**3** **for 0 ≤** _step < N do_

**4** _ζ ∼N_ (0,σ)[n][×][d] 1

**5** _x[′]_ :=x[′]+ _x′_ _n_ _i[f][(][x][′][+][ζ][i][)]_
_∇_ _L_

**6** _x[′]_ := Project(x,x[′],ϵ,ℓp)

  P 

**7** **return x[′]**

input since it shares its label with all the ϵ-close inputs in M. Consequently, the model rejection of x[′] is
unnecessary and causes a degradation in model utility. This happens even if the local check is precise and
the bound ϵ is tight

3.2 ALGORITHMS


Algorithm 3.1 describes a simple degradation attack against a deterministic defense. Intuitively, given a
valid input x (on which the classifier f is (ϵ,ℓp)-locally robust, for some fixed ϵ and distance metric ℓp),
we want to find an input x[′] such that _x_ _x[′]_ _p_ _ϵ and the classifier f is not (ϵ,ℓp)-locally robust at x[′]._
The attack re-purposes existing white-box attack algorithms (like Carlini-Wagner (Carlini & Wagner, 2017) || _−_ _||_ _≤_
and PGD (Madry et al., 2018)) that search for adversarial examples. In particular, the attack invokes an
off-the-shelf white-box attack algorithm, indicated by Attack, to find an adversarial example x[′′] in a ball
of radius 2ϵ centered at x (line 2). Next, x[′′] is simply projected onto the ϵ-sphere centered at x (line 3). For
_ℓ2, Project is defined as x+min{ϵ, ||x[′′]−x||p}·_ _||xx[′′][′′]−−xx||p_ [, whereas for][ ℓ][∞] [we simply need to clip][ x][′′][.]

**Attacking stochastic defenses. Stochastic defenses, such as randomized smoothing, transform a base**
classifier into a smoothed classifier by convolving the base classifier with a Gaussian distribution. The
smoothed classifier does not have an explicit representation, and to evaluate it on a single input, the base
classifier needs to evaluated on as many as 100,000 samples (Cohen et al., 2019). Moreover, the lack of
explicit function representation precludes the use of off-the-shelf white-box attack algorithms that perform
gradient descent in the input space. While one could simply perform a standard white-box attack on the
base classifier, we found it was more effective to implicitly attack the smoothed model by taking the
average gradient over a set of randomly generated samples at each gradient descent step. This gives us a
“smoothed PGD” (SPGD) attack tailored for use against stochastic defenses. Our SPGD procedure is given
in Algorithm 3.2.

Note that Algorithms 3.1 and 3.2 are only meant to serve as proofs-of-concept for degradation attacks. As
our empirical results in Section 3.3.1 demonstrate, even these simple algorithms can be highly effective in
degrading model utility.


-----

3.3 EVALUATING ATTACK EFFICACY

Our empirical evaluation is designed to measure the susceptibility of state-of-the-art robust models and
certified run-time defenses to utility degradation attacks. For our experiments, we consider GloRo
Nets (Leino et al., 2021) and randomized smoothed models (Cohen et al., 2019). These approaches lead to
models with the best known verified robust accuracies (VRA), with respect to the ℓ2 metric, on a variety of
popular image classification datasets like MNIST (LeCun et al., 2010), CIFAR-10 (Krizhevsky, 2009), and
ImageNet (Deng et al., 2009).

GloRo Nets incorporate global Lipschitz bound computation into the model architecture. The global
Lipschitz bound is used to implement the local robustness check at an input under evaluation. Moreover,
these local robustness checks are backpropagated over during training to encourage learning of models with
a high degree of robustness. In contrast, randomized smoothing transforms a base classifier into a smoothed
classifier by convolving the base classifier with a Gaussian distribution. The smoothed classifier typically
demonstrates a much higher degree of robustness than the base classifier as long as the base classifier has
been trained to classify well under Gaussian noise. The method uses statistical techniques to compute the
local (certified) robustness radius R of the smoothed classifier at a given input, so an (ϵ,ℓ2)-local robustness
check simply amounts to checking if the radius R at the evaluated input is greater than ϵ or not. Radius R is
only probabilistically valid, so all the results for randomized smoothing in this section are also probabilistic
statements. However, this probabilistic nature of R is typically ignored when metrics like VRA are reported
on randomized smoothing in the literature, and we do the same here.

We conducted two sets of experiments. In our first set of experiments, described in Section 3.3.1, we
attack certifiably robust models at inputs in the test set using the approaches described in Section 3.2.
This demonstrates the efficacy of our attack algorithms, and gives a lower bound on the efficacy of
degradation-style attacks in general. Our second set of experiments, described in Section 3.3.2, measure the
susceptibility of models to degradation attacks assuming an all-powerful adversary, i.e., an adversary that
succeeds whenever the model is susceptible to a degradation attack at inputs in the test set. This provides
an upper bound on the efficacy of degradation attacks.

To measure the efficacy of degradation attacks on a particular model with a certified run-time defense
and a dataset, we construct two subsets of the test set for the given dataset, that we refer to as testR and
_testA. The former, testR, is the set of test inputs on which the model is certified to be (ϵ,ℓp)-locally robust._
To construct testR, we simply apply the (ϵ,ℓp)-local robustness check at every input in the test set. If the
check passes at an input x, it is added to testR. The latter, testA, is a subset of testR, constructed using a
degradation attack, such that for each input x∈testA the model is certified (ϵ,ℓp)-locally robust at x but
there exists an input x[′] in the ϵ-ball centered at x such that the model cannot be certified (ϵ,ℓp)-locally
robust at x[′]. Since the model is certified locally robust at x, we know that x and x[′] share the same label
and x[′] is not an adversarial input (assuming that there exists no other point of a different class label in M
that is ϵ-close to x[′]). Yet, the model would reject x[′] since the local robustness check fails at x[′]. Thus, testA
represents the set of test inputs where the model is susceptible to degradation attacks. The ratio _/_ _testR_

_[|][test][A][|]_ _|_ _|_
represents the false positive rate of the certified run-time defense, i.e., the rate at which the certified defense
erroneously causes the model to reject inputs. We measure model susceptibility and attack efficacy in terms
of the false positive rate. A value of one indicates that the model is susceptible to a degradation attack on
all test inputs where it is certified locally robust, while a value of zero indicates that model is safe from
degradation attacks on all certifiably robust test inputs.

We train GloRo Nets using the publicly available code[2]. For randomized smoothed models, we use the
pre-trained models made available by the authors [3]. The GloRo Nets used in the evaluation are composed
of convolution and fully-connected layers. For MNIST (ϵ =0.3), the model has two convolution layers
and two fully-connected layers (2C2F), for MNIST (ϵ = 1.58), the model is 4C3F, and for CIFAR-10
(ϵ=0.141), the model is 6C2F. For CIFAR-10, the randomized smoothed model uses a 110-layer residual
network as the base classifier, and for ImageNet, a ResNet-50 model is used as the base classifier We
implemented our attacks in Python, using TensorFlow and PyTorch. All our experiments were run on
an NVIDIA TITAN RTX GPU with 24 GB of RAM, and a 4.2GHz Intel Core i7-7700K with 32 GB of
RAM.

[2https://github.com/klasleino/gloro](https://github.com/klasleino/gloro)
[3https://github.com/locuslab/smoothing](https://github.com/locuslab/smoothing)


-----

|Col1|baseline VRA false positive rate utility reduction (%) (%) (%)|
|---|---|
|MNIST (ϵ=0.3)|95.0 6.0 5.7|
|MNIST (ϵ=1.58)|61.8 56.1 34.7|
|CIFAR-10 (ϵ=0.141)|60.0 11.4 6.6|


Table 1: Lower bounds for false positive rates induced by degradation attacks against GloRo Nets. Models
are trained and evaluated using the same ϵ value.

|Col1|σ|baseline VRA false positive rate utility reduction (%) (%) (%)|
|---|---|---|
|CIFAR-10 (ϵ=0.5)|0.25|48.0 50.0 24.0|
|CIFAR-10 (ϵ=0.5)|0.50|41.0 29.3 12.0|
|ImageNet (ϵ=1.0)|0.50|46.0 17.4 8.0|



Table 2: Lower bounds for false positive rates induced by degradation attacks against Randomized
Smoothing. Similarly to the evaluation of Cohen et al. (2019) results are obtained on a sample of 100
arbitrary test points, as Randomized Smoothing is costly to evaluate.

3.3.1 LOWER BOUNDS ON ATTACK EFFICACY

To evaluate the efficacy of the attack algorithms presented in Section 3.2 and compute lower bounds on
the efficacy of degradation attacks, we construct testA as follows. An input x∈testR is added to testA if
the attack algorithm succeeds in finding an attack input x[′] in the ϵ-ball at x such that the model cannot
be certified robust at x[′]. Table 1 presents lower bounds on degradation attack efficacy against GloRo
Nets, measured by performing our degradation attack described in Algorithm 3.1. Table 2 presents lower
bounds on degradation attack efficacy against Randomized Smoothing, measured by performing our SPGD
degradation attack described in Algorithm 3.2. We employ the same ϵ values for GloRo Nets as used by
Leino et al. (2021), and employ commonly used ϵ values for Randomized Smoothing that result in visually
imperceptible perturbations.

We see that against both GloRo Nets and Randomized Smoothing our attacks decrease model utility to
varying degrees. In some cases the reduction in utility was substantial, with false positive rates at 50% or
more on MNIST (GloRo Net) and CIFAR-10 (Randomized Smoothing). Overall, false positive rates were
higher when the robustness radius was relatively higher; e.g., the false positive rate on MNIST GloRo Nets
increases from 6% to 56% when the radius ϵ increased from 0.3 to 1.58. This may be in part because at
smaller radii, many points may be likely to be far more robust than required.

Despite the fact that an explicit representation of smoothed models is not available even to a white-box
attacker, we find that our SPGD attack was able to successfully degrade the utility of models defended
by Randomized Smoothing. On smoothed models we observe that models trained with a larger noise
parameter σ are less affected by false positives; however, this comes at the cost of a notable decrease in
baseline utility.

In Appendix B, we report the result of using off-the-shelf PGD with an ϵ-bound as a degradation attack
algorithm for attacking GloRo Net models. We also present a visualization of successful degradation attack
inputs in the same section.

3.3.2 UPPER BOUNDS ON ATTACK EFFICACY

To compute upper bounds on the efficacy of degradation attacks, i.e., upper bounds on the false positive
rates, we construct testA as follows. We apply a (2ϵ,ℓp)-local robustness check at every input in testR.
If this stronger check passes at an input x, it means that no matter how the adversary perturbs x within
the ϵ-ball centered at x, the model is always (ϵ,ℓp)-locally robust at the perturbed input, and the certified
defense (whether complete or incomplete) cannot be forced to unnecessarily reject the input as long as it


-----

(a) (b) (c) (d)

Figure 2: Upper bounds for false positive rates on Gloro Nets

(a) (b) (c) (d)

Figure 3: Upper bounds for false positive rates on Randomized Smoothing

satisfies a monotonicity property [4]. GloroNets and randomized smoothing are both monotonic in this sense
(see Appendix A). However, if the (2ϵ,ℓp)-local robustness check fails at x, then even though the model is
certified (ϵ,ℓp)-locally robust at x, there may exist an input x[′] in the ϵ-ball at x where the model cannot
be certified (ϵ,ℓp)-locally robust and an adversary can force an unnecessary rejection. Therefore, if the
stronger check fails at x, we add it to testA.

Figures 2a and 2b show the false positive rates at different radii (i.e., ϵ values) for GloRo Net models trained
to be robust against the ϵ values indicated in the legend. The ϵ values of 0.3 and 1.58 for MNIST and 0.141
for CIFAR-10 are the same as the ones used for evaluating the lower bounds. Figure 2 also reports results
for models trained to be robust against twice these ϵ values. This is for the purpose of evaluating one of our
proposed defenses against degradation attacks (see Section 4.1). We see that the false positive rates are
quite high and consistently approach one as the radius increases. The models trained with higher ϵ values
are more resilient to degradation attacks, but they pay the price of a lower VRA, specially at lower radius
values (Figures 2c and 2d). By comparing the upper bounds to the lower bounds obtained in Section 3.3.1,
we see that our degradation attacks recover between [1]/4 and [1]/2 of the best possible degradation.

Interestingly the susceptibility to degradation attacks does not appear to result from the GloRo Net’s
overapproximation of local robustness. Leino et al. (2021) report that the Lipschitz bounds on the MNIST
models are far tighter than on the CIFAR-10 model, yet the CIFAR-10 model is not more susceptible to
degradation. As explained in Section 1, this is because the threat of degradation attacks arises intrinsically
from any defense that filters based on local robustness at run-time, not because of the overapproximate
nature of practical defenses.

Figures 3a and 3b show the false positive rates at different radii for each of the randomized smoothed
models. Again, the false positive rates are high and approach one as ϵ increases. Models that are smoothed
using larger σ values unsurprisingly demonstrate more resilience to degradation attacks at the cost of a
reduced VRA (Figures 3c and 3d). The sudden jumps in the false positive rate to one indicate the ϵ value
where all points in testR also end up in testA; i.e., the model is susceptible to degradation attacks at all the
points where it is certified locally robust.

4 DEFENDING AGAINST DEGRADATION ATTACKS

In this section, we sketch two possible ways of defending against degradation attacks, and empirically
evaluate the effectiveness of our first proposal.

4We say that a certification method cert:Rd×R+ _→bool is monotonic if, ∀x,x′_ _∈Rd, ϵ>0 . cert(x,ϵ) ∧||x−_
_x[′]_ _p_ _<ϵ =_ `cert(x[′],ϵ` _x_ _x[′]_ _p)._
_||_ _⇒_ _−||_ _−_ _||_


-----

4.1 DEFENSE VIA RADIUS DOUBLING

A simple strategy to defend against degradation attacks is to enforce local robustness, when training and
validating certifiably robust models, using twice the radius that the norm-bounded adversary is allowed for
adversarial and degradation attacks. This strategy is motivated by the intuition that if a model is 2ϵ robust
at a point x in M, then no matter how the adversary perturbs x within the ϵ-ball centered at x, the model is
always (ϵ,ℓp)-locally robust at the perturbed input, and an (ϵ,ℓp)-local robustness defense is likely (or even
guaranteed, in the case of a complete defense) to not reject the input unnecessarily.

Training a model for 2ϵ-robustness is not guaranteed to help because of the frequently observed inverse
relationship between accuracy and robustness. The model may be less accurate than the one trained for
_ϵ-robustness, and may also reject more frequently, even on points in M. But validating the model against_
2ϵ is certainly helpful. The 2ϵ-VRA on validation data is a lower bound on the accuracy one can expect on
test data, in the face an ϵ-adversary capable of adversarial as well as degradation attacks. This also suggests
that, when evaluating certified defenses, one ought to report not just ϵ-VRA, but also 2ϵ-VRA. Validating
against 2ϵ can also help in configuring the certified run-time defense. In case the rejection rate is too high at
a particular 2ϵ, the certified defense can be configured to enforce local robustness at a smaller ϵ value. This
reduces vulnerability to degradation attacks at the cost of an increased vulnerability to adversarial attacks.

We evaluate this defense strategy on GloRo Net models using the same architectures and datasets as in
Section 3.3. However, we re-train all the models using twice the ϵ values that we want to defend against.
Figure 2 summarizes the results. We see that training against 2ϵ-robustness helps reduce the false positive
rates compared to the models trained against ϵ-robustness. Moreover, this improvement is achieved without
significantly affecting the VRA of the models. These results suggest that training against 2ϵ-robustness can
reduce the vulnerability to degradation attacks, but the false positive rates still remain concerning.

While smoothed models are not explicitly trained for a specific degree of robustness, and therefore this
approach does not directly apply, the noise parameter, σ, does allow one to control the trade-off between
robustness and accuracy. Cohen et al. (2019) found that the best VRA is typically achieved when σ _≈_ _[ϵ]/2._
Table 2 shows that by increasing σ beyond this point, one can obtain better resistance to degradation.
However, as Figures 3c and 3d show, increasing σ also negatively impacts utility.

4.2 DEFENSE VIA AN M -MEMBERSHIP ORACLE

Certified run-time defenses which rely on local robustness checks are susceptible to degradation attacks
because these defenses are overly strong, i.e., they try to enforce local robustness at all evaluated inputs,
even though they only need to enforce local robustness at inputs in M. To ‘fix’ these certified defenses,
instead of checking local robustness at the given input x[′], one could instead check if x[′] shares its label with
all the inputs in M that are ϵ-close to x[′]. If the check passes, then x[′] is guaranteed to be a non-adversarial
input, but, more importantly, if the check fails then x[′] is guaranteed to be an adversarial input.

This fix works because an input x[′] is an adversarial example if and only if there exists some input x∈M
that is ϵ-close to x[′] and is labeled differently from x[′]. To implement our proposed fix, we would need an
explicit representation for M, or at least an oracle for membership in M. A membership oracle may be
used to search for members of M that are ϵ-close to a given x[′]. While an exhaustive search for all such
members is likely infeasible, we may allocate a fixed budget of computational resources to the oracle-based
check. If, within the allocated budget, we find an input x ∈ _M such that x is ϵ-close to x[′]_ but it has a
different label, then we have detected an adversarial example and can reject x[′]. However, if the check fails
to find such an x within the budget, we can fall back to checking local robustness at x[′]. Practically, one
could use an Out-of-Distribution (OOD) detector as an M-membership oracle. Although OOD detectors
can be imprecise and susceptible to adversarial attacks themselves (Sehwag et al., 2019; Bitterwolf et al.,
2020), some recent developments (Chen et al., 2020) look promising. We leave this for future work.

5 RELATED WORK

The discovery of adversarial examples has led to an intense effort in recent years towards formalizing the
problem, as well as designing new attack and defense mechanisms.

**Formalizations.** The presence of adversarial examples has been popularly formalized as the absence of
local robustness in a neural classifier. However, only constant functions are locally robust everywhere, so


-----

this definition fails to capture the notion of a classifier free from adversarial examples. Leino et al. (2021)
propose the notion of global robustness for classifiers that are allowed to reject inputs. They define global
robustness as having a margin (predicted as ⊥ or rejection) of sufficient width between classes. Their
observation is that checking points for ϵ-local robustness corresponds to a margin of width 2ϵ; i.e., there
is a factor of two difference when referring to global vs. local robustness. Moreover, Yang et al. (2020b)
show that in natural image datasets, differently labeled images are indeed separated by a distance larger
than twice the perturbation radii used in adversarial example experiments. These observations demonstrate
that for real-world datasets, the support set of the input distribution is much smaller than the entire input
space and inputs belonging to different classes are separated. A different notion of global robustness is
proposed by Ruan et al. (2019). They define a model to be globally robust if it is locally robust at every
input in a finite test set. None of these definitions capture the idea that, to be protected from adversarial
examples, a model only needs to be locally robust at every input in the support set, M.

**Adversarial attacks.** Algorithms for constructing adversarial attacks can be divided in to two major
categories. White-box attacks assume that they have access to the model internals, i.e., the model
architecture and weights. Their primary strategy is to perform gradient descent in the input space so as to
find an input that maximizes the loss while satisfying the constraint of staying within the ϵ-ball centered at
the original input. Some popular and successful attack algorithms of this nature include the ones proposed
by Goodfellow et al. (2015), Carlini & Wagner (2017), and Madry et al. (2018). Black-box attacks only
assume query access to the model (Papernot et al., 2017). In other words, their threat model is weaker than
the one assumed by white-box attacks. The degradation attacks proposed in this paper use adversarial attack
algorithms as a sub-procedure and are agnostic to whether these algorithms are white-box or black-box.

**Heuristic and certified defenses.** Heuristic defenses against adversarial examples do not provide any
guarantees about their defensive capabilities. These include approaches that modify the training objectives,
modify the neural classifier post-training, or embed run-time checks to flag adversarial examples during
evaluation. The lack of guarantees suggests that these defenses can be broken and it has indeed been
demonstrated (Athalye et al., 2018; Tramer et al., 2020) that a number of published defenses are breakable.
In this paper, we focus on certified run-time defenses that are deployed during model evaluation and, if an
example is adversarial, are guaranteed to flag it (though incomplete defenses can also report false positives).
Such defenses check if the classifier is locally robust at the evaluated input. Guaranteed local robustness
checks are implemented using a variety of approaches that include constraint solving and formal methods
(Huang et al., 2017; Katz et al., 2017; Gehr et al., 2018; Singh et al., 2019), optimization (Bastani et al.,
2016; Dvijotham et al., 2018; Raghunathan et al., 2018; Wong & Kolter, 2018; Tjeng et al., 2019), Lipschitz
bounds computation (Weng et al., 2018; Leino et al., 2021), and stochastic smoothing (Lecuyer et al., 2019;
Cohen et al., 2019; Yang et al., 2020a). We show that such defenses are overly cautious and often flag even
non-adversarial inputs as adversarial.

The two algorithms (PREDICT and CERTIFY) from (Cohen et al., 2019) can both return ABSTAIN when
the prediction (and the certified radius) can not be computed for the given input, due to the imprecision of
the statistical analysis. This can be potentially exploited by an adversary who can force a certifiably robust
classifier to unnecessarily abstain as has been noted in (Cohen et al., 2019). Our attack is different (but
similar in spirit) in that it works in cases that CERTIFY does not abstain and is able to reliably compute the
prediction and the certified radius R, but this radius may be smaller than the desired ϵ for inputs that are in
fact non-adversarial.

6 CONCLUSION

In this paper, we have showed that certified run-time defenses against adversarial examples, based on
local robustness checks, are inherently overcautious and can reduce model utility due to unnecessary input
rejections. This is a consequence of these defenses enforcing local robustness at all evaluated inputs even
though local robustness only needs to be enforced at valid inputs, i.e., inputs in the support set, M, of
the input distribution. An adversary can exploit this over-cautiousness; though they cannot change the
classification of an input, they can apply norm-bounded modifications to valid inputs and force the model
to reject otherwise correctly classifiable inputs. We have presented concrete degradation attacks of this
nature that can be implemented using off-the-shelf white-box attack algorithms. Moreover, our empirical
evaluation demonstrates that even state-of-the-art certifiably robust models, like randomized smoothed
models and GloRo Nets, are highly susceptible to utility degradation attacks.


-----

REPRODUCIBILITY STATEMENT

We will make our attack code and experiment scripts available via GitHub. For all our experiments with
randomized smoothed models, we directly use the pre-trained models made available by the authors of the
[paper at https://github.com/locuslab/smoothing. For the experiments with GloRo Nets,](https://github.com/locuslab/smoothing)
[we trained the models using the publicly available code for training GloRo Nets at https://github.](https://github.com/klasleino/gloro)
[com/klasleino/gloro.](https://github.com/klasleino/gloro)

ETHICS STATEMENT

Our work sheds light on existing vulnerabilities in state-of-the-art certifiably robust neural classifiers.
These degradation attacks could be deployed by malicious entities to degrade the utility of deployed
models. However, by putting this knowledge out in the public domain and making practitioners aware of
the existence of degradation attacks, we hope that precautions can be taken to protect existing systems.
Moreover, it highlights the need to harden future systems against such attacks.

REFERENCES

Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security:
Circumventing defenses to adversarial examples. In International conference on machine learning, pp.
274–283. PMLR, 2018.

Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios Vytiniotis, Aditya V. Nori, and Antonio
Criminisi. Measuring neural net robustness with constraints. In Proceedings of the 30th International
_Conference on Neural Information Processing Systems, NIPS’16, pp. 26212629, Red Hook, NY, USA,_
2016. Curran Associates Inc. ISBN 9781510838819.

Julian Bitterwolf, Alexander Meinke, and Matthias Hein. Certifiably adversarially robust detection of
out-of-distribution data. Advances in Neural Information Processing Systems, 33, 2020.

N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium
_on Security and Privacy (SP), pp. 39–57, Los Alamitos, CA, USA, may 2017. IEEE Computer Society._
[doi: 10.1109/SP.2017.49. URL https://doi.ieeecomputersociety.org/10.1109/SP.](https://doi.ieeecomputersociety.org/10.1109/SP.2017.49)
[2017.49.](https://doi.ieeecomputersociety.org/10.1109/SP.2017.49)

Jiefeng Chen, Yixuan Li, Xi Wu, Yingyu Liang, and Somesh Jha. Robust out-of-distribution detection for
neural networks. arXiv preprint arXiv:2003.09711, 2020.

Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing.
In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Confer_ence on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 1310–1320._
[PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/v97/cohen19c.html.](https://proceedings.mlr.press/v97/cohen19c.html)

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255.
Ieee, 2009.

Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy Mann, and Pushmeet Kohli. A dual
approach to scalable verification of deep networks. In Proceedings of the Thirty-Fourth Conference
_Annual Conference on Uncertainty in Artificial Intelligence (UAI-18), pp. 162–171, Corvallis, Oregon,_
2018. AUAI Press.

Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, and Martin
Vechev. Ai2: Safety and robustness certification of neural networks with abstract interpretation. In 2018
_IEEE Symposium on Security and Privacy (SP), pp. 3–18, 2018._

Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning
_Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015._
[URL http://arxiv.org/abs/1412.6572.](http://arxiv.org/abs/1412.6572)


-----

Xiaowei Huang, Marta Kwiatkowska, Sen Wang, and Min Wu. Safety verification of deep neural networks.
In International Conference on Computer Aided Verification, pp. 3–29. Springer, 2017.

Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An efficient smt
solver for verifying deep neural networks. In International Conference on Computer Aided Verification,
pp. 97–117. Springer, 2017.

Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.

Yann LeCun, Corinna Cortes, and Chris Burges. Mnist handwritten digit database, 2010.

Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified robustness
to adversarial examples with differential privacy. In 2019 IEEE Symposium on Security and Privacy
_(SP), pp. 656–672. IEEE, 2019._

Klas Leino, Zifan Wang, and Matt Fredrikson. Globally-robust neural networks. In International Conference
_on Machine Learning (ICML), 2021._

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning
_Representations, 2018._

Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram
Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia
_conference on computer and communications security, pp. 506–519, 2017._

Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial examples. In
_[International Conference on Learning Representations, 2018. URL https://openreview.net/](https://openreview.net/forum?id=Bys4ob-Rb)_
[forum?id=Bys4ob-Rb.](https://openreview.net/forum?id=Bys4ob-Rb)

Wenjie Ruan, Min Wu, Youcheng Sun, Xiaowei Huang, Daniel Kroening, and Marta Kwiatkowska. Global
robustness evaluation of deep neural networks with provable guarantees for the hamming distance. In
_Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19,_
pp. 5944–5952. International Joint Conferences on Artificial Intelligence Organization, 7 2019. doi:
[10.24963/ijcai.2019/824. URL https://doi.org/10.24963/ijcai.2019/824.](https://doi.org/10.24963/ijcai.2019/824)

Vikash Sehwag, Arjun Nitin Bhagoji, Liwei Song, Chawin Sitawarin, Daniel Cullina, Mung Chiang, and
Prateek Mittal. Analyzing the robustness of open-world machine learning. In Proceedings of the 12th
_ACM Workshop on Artificial Intelligence and Security, pp. 105–116, 2019._

Gagandeep Singh, Timon Gehr, Markus Puschel, and Martin Vechev. An abstract domain for certifying¨
neural networks. Proc. ACM Program. Lang., 3(POPL), January 2019.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and
Rob Fergus. Intriguing properties of neural networks. In Yoshua Bengio and Yann LeCun (eds.), 2nd
_International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16,_
_[2014, Conference Track Proceedings, 2014. URL http://arxiv.org/abs/1312.6199.](http://arxiv.org/abs/1312.6199)_

Vincent Tjeng, Kai Y. Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed
[integer programming. In International Conference on Learning Representations, 2019. URL https:](https://openreview.net/forum?id=HyGIdiRqtm)
[//openreview.net/forum?id=HyGIdiRqtm.](https://openreview.net/forum?id=HyGIdiRqtm)

Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to
adversarial example defenses. Advances in Neural Information Processing Systems, 33, 2020.

Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning, and
Inderjit Dhillon. Towards fast computation of certified robustness for relu networks. In International
_Conference on Machine Learning, pp. 5276–5285. PMLR, 2018._

Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial
polytope. In International Conference on Machine Learning, pp. 5286–5295. PMLR, 2018.

Eric Wong, Frank R. Schmidt, Jan Hendrik Metzen, and J. Zico Kolter. Scaling provable adversarial
defenses. In Proceedings of the 32nd International Conference on Neural Information Processing
_Systems, NIPS’18, pp. 84108419, Red Hook, NY, USA, 2018. Curran Associates Inc._


-----

Greg Yang, Tony Duan, J Edward Hu, Hadi Salman, Ilya Razenshteyn, and Jerry Li. Randomized
smoothing of all shapes and sizes. In International Conference on Machine Learning, pp. 10693–10705.
PMLR, 2020a.

Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Russ R Salakhutdinov, and Kamalika Chaudhuri. A
closer look at accuracy vs. robustness. In NeurIPS, 2020b.


-----

A MONOTONICITY OF LOCAL ROBUSTNESS CERTIFIERS

**Definition 4 (Monotonicity of local robustness certifiers). We say that a certification method cert :**
R[d]×R[+] _→bool is monotonic if_

_∀x,x[′]_ _∈R[d], ϵ>0 . cert(x,ϵ) ∧( ||x−x[′]||p_ =δ)<ϵ =⇒ `cert(x[′],ϵ−δ)`

**Definition 5 (Exact local robustness certifier). An exact certification method is one for which**
`certlent to local robustness).(x,ϵ) ⇐⇒∀x[′]` _∈_ R[d].||x − _x[′]||p ≤_ _ϵ =⇒_ _F_ (x) = F (x[′]) (in other words, certification is equiva
**Lemma 6. All exact certification methods are monotonic.**

_Proof. This follows essentially from the triangle inequality. Let x ∈_ R[d] and ϵ > 0, and suppose that
`cert(x,ϵ). Thus, since cert is exact, we have that ∀x[′]` _∈R[d]_ _. ||x−x[′]||p_ _≤ϵ =⇒_ _F_ (x)=F (x[′]).

Consider x[′] _∈R[d]_ for which ||x[′]−x||≤ϵ, and let δ =||x−x[′]||. Now for x[′′] _∈R[d], assume ||x[′]−x[′′]||≤ϵ−δ._
Thus,

_ϵ ≥_ _δ+||x[′]−x[′′]||+δ = ||x−x[′]||+||x[′]−x[′′]||_

_≥||x−x[′′]||_ by the triangle inequality.

Thus, because we have cert(x,ϵ), we may conclude that F (x[′])= _F_ (x)= _F_ (x[′′]). This tells us that the
classifier F is (ϵ−δ)-locally robust at x[′], which, by completion gives us cert(x[′],ϵ−δ).

**Definition 7 (Lipschitz-based local robustness certifier). A Lipschitz-based certification method is one**
_which, given an upper bound, Kij(x,ϵ) on the local Lipschitz constant of the margin fj_ _−fi at x,[5]_ _certifies_
_a point x, where F_ (x)=j, exactly when

max _fi(x)+ϵKij(x,ϵ)_ _fj(x)._
_i=j_ _≤_
_̸_


**Lemma 8. All Lipschitz-based certification methods are monotonic.**

_Proof. Let Kij(x,ϵ) be an upper bound on the Lipschitz constant of the function fj(x)_ _fi(x). That is,_
_−_
_Kij(x,ϵ) is the maximum rate of change of fj(x)−fi(x) within an ℓp ball of radius ϵ centered at x._

Let x ∈ R[d] and ϵ > 0, and suppose that cert(x,ϵ). Consider x[′] _∈_ R[d] for which ||x[′] _−_ _x|| ≤_ _ϵ, and let_
_δ_ =||x−x[′]||.

We begin with the observation that the ℓp ball of radius ϵ centered at x—we will denote this as B(x, ϵ)—
contains B(x[′], ϵ−δ). This, too, follows essentially from the triangle inequality:

_z_ _∈B(x[′], ϵ−δ) =⇒||x[′]−z||≤ϵ−δ_ =ϵ−||x−x[′]||

=⇒||x−x[′]||+||x[′]−z||≤ϵ
=⇒||x−z||≤ϵ by the triangle inequality
=⇒ _z_ _∈B(x, ϵ)_

From this, we conclude that Equation 1 holds.

_Kij(x,ϵ)_ _Kij(x[′],ϵ_ _δ)_ (1)
_≥_ _−_

With this in mind, we proceed as follows. Let j = _F_ (x); i.e. j is the class predicted by the classifier at
_x. Since cert(x, ϵ) implies that the classifier is ϵ-locally-robust at x, and x[′]_ _∈B(x,ϵ), we conclude that_
_F_ (x[′])=j.

Because we assume that cert is Lipschitz-based (Definition 7), we have Equation 2.

_i=j . fj(x)_ _fi(x)_ _ϵKij(x,ϵ)_ (2)
_∀_ _̸_ _−_ _≥_

5note that the global Lipschitz constant is such a bound


-----

|Col1|baseline VRA false positive rate utility reduction (%) (%) (%)|
|---|---|
|MNIST (ϵ=0.3)|95.0 6.1 5.9|
|MNIST (ϵ=1.58)|61.8 57.0 34.3|
|CIFAR-10 (ϵ=0.141)|60.0 11.2 6.6|


Table 3: False positive rates induced by ϵ-PGD degradation attack against GloRo Nets.

Figure 4: Visualizations of successful degradation attacks on CIFAR-10 (ϵ=0.141).

Since ||x−x[′]||=δ<ϵ, by the definition of Kij we have that

_i=j . fj(x[′])_ _fi(x[′])_ _fj(x)_ _fi(x)_ _δKij(x,ϵ)_
_∀_ _̸_ _−_ _≥_ _−_ _−_
_ϵKij(x,ϵ)_ _δKij(x,ϵ)=(ϵ_ _δ)Kij(x,ϵ)_ by Equation 2
_≥_ _−_ _−_

(ϵ _δ)Kij(x[′],ϵ_ _δ)_ by Equation 1
_≥_ _−_ _−_

This is sufficient to conclude that we have cert(x[′], ϵ−δ).


GloRo Nets are Lipschitz-based, therefore they are monotonic by Lemma 8. Aside from the fact that
nondetermism/uncertainty arises because of the need to sample to evaluate the smoothed function, Randomized Smoothing provides the exact robustness radius on the smoothed function. That is, in the infinite
sample limit, Randomized Smoothing is an exact method, and therefore monotonic by Lemma 6; though
in practice this property only holds with high probability (i.e., the probability can be bounded from below).
We note, however, that the robustness guarantees provided by Randomized Smoothing are analogously
probabilistic.

B LOWER BOUNDS ON ATTACK EFFICACY (MORE RESULTS)

B.1 EFFICACY OF ϵ-PGD ATTACK

We evaluated the degradation attack efficacy of a simpler version of Algorithm 3.1. In this simpler
algorithm, line 2 of Algorithm 3.1 is replaced by a call to the PGD algorithm Madry et al. (2018) with an
_ϵ bound instead of a 2ϵ bound. The attack input x[′′]_ returned by an ϵ-bounded PGD is guaranteed to be
within the ϵ-ball centered at x, so we no longer need to project x[′′] as in line 3, and directly return x[′′] as the
candidate degradation attack input.

Table 3 presents the results of using this simpler algorithm for degradation attacks against GloRo Net
models. The models, datasets, and ϵ values are the same as used for evaluating the efficacy of Algorithm 3.1.
The efficacy of an ϵ-PGD attack is similar to Algorithm 3.1 (presented in Table 1).

B.2 VISUALIZATION OF SUCCESSFUL DEGRADATION ATTACK INPUTS

Figure 4 provides samples of inputs that constitute successful degradation attacks. In each pair of images,
the original image is shown on the left and the perturbed image (the attack) is shown on the right. As
expected, the pairs of images are visually indistinguishable; thus the adversary can be considered to have
successfully delivered an imperceptible attack.

C DEGRADATION ATTACKS AGAINST ℓ DEFENSES
_∞_

In this section, we evaluate the efficacy of degradation attacks against models defended with a ℓ certified
_∞_
defense. In particular, we consider models trained and defended using the convex outer adversarial polytope


-----

|Col1|baseline VRA false positive rate utility reduction (%) (%) (%)|
|---|---|
|MNIST (ϵ=0.1)|98.9 91.9 90.9|


Table 4: Lower bounds for false positive rates induced by degradation attack against KW.

|Col1|baseline VRA false positive rate utility reduction (%) (%) (%)|
|---|---|
|MNIST (ϵ=0.1)|98.9 100.0 98.9|



Table 5: Upper bounds for false positive rates induced by degradation attack against KW.

approach of Wong & Kolter (2018), that we refer to as KW. We compute lower and upper bounds in a
similar manner as described in Section 3.3, using KW for the (ϵ,ℓ )-local robustness check. For the lower
_∞_
bounds, the set testA is constructed using Algorithm 3.1 where Project is implemented by clipping x[′′].

Table 4 reports the lower bounds, i.e., the efficacy of Algorithm 3.1 against KW. Table 5 reports the upper
bounds. We see that ℓ defenses like KW are extremely susceptible to degradation attacks—the simple
_∞_
attack given by Algorithm 3.1 succeeds approximately 92% of the time, and the upper bounds indicate that
it may be possible for a more sophisticated adversary to succeed 100% of the time.

While striking, these results are not entirely unexpected, given the geometry of ℓ space. Defenses
_∞_
based on robustness certification are susceptible to degradation attacks unless the underlying model is (2ϵ,
_ℓp)-robust. In ℓ∞_ space, the volume contained by a ball overlaps much more closely with [0,1][d] (the typical
the domain for, e.g., image data), which intuitively means that the domain is “used up” much more quickly
as the required robustness radius increases. By contrast, in high-dimensional Euclidean space, the radius
can be as large as _√d before it necessarily envelops the entire domain._


-----

