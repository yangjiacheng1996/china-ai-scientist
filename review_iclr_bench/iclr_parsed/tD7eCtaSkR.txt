# IMPROVED DETERMINISTIC l2 ROBUSTNESS ON CIFAR-10 AND CIFAR-100

**Sahil Singla[1], Surbhi Singla[2], Soheil Feizi[1]**
University of Maryland, College Park
{ssingla,sfeizi}@umd.edu[1], surbhisingla1995@gmail.com[2]

ABSTRACT

Training convolutional neural networks (CNNs) with a strict Lipschitz constraint
under the l2 norm is useful for provable adversarial robustness, interpretable gradients and stable training. While 1-Lipschitz CNNs can be designed by enforcing
a 1-Lipschitz constraint on each layer, training such networks requires each layer
to have an orthogonal Jacobian matrix (for all inputs) to prevent the gradients
from vanishing during backpropagation. A layer with this property is said to
be Gradient Norm Preserving (GNP). In this work, we introduce a procedure to
certify the robustness of 1-Lipschitz CNNs by relaxing the orthogonalization of
the last linear layer of the network that significantly advances the state of the art
for both standard and provable robust accuracies on CIFAR-100 (gains of 4.80%
and 4.71%, respectively). We further boost their robustness by introducing (i) a
novel Gradient Norm preserving activation function called the Householder activation function (that includes every GroupSort activation) and (ii) a certificate
regularization. On CIFAR-10, we achieve significant improvements over prior
works in provable robust accuracy (5.81%) with only a minor drop in standard
[accuracy (−0.29%). Code for reproducing all experiments in the paper is available](https://github.com/singlasahil14/SOC)
[at https://github.com/singlasahil14/SOC.](https://github.com/singlasahil14/SOC)

1 INTRODUCTION

Given a neural network f : R[d] _→_ R[k], the Lipschitz constant[1] Lip(f ) enforces an upper bound on
how much the output is allowed to change in proportion to a change in the input. Previous work has
demonstrated that a small Lipschitz constant is useful for improved adversarial robustness (Szegedy
et al., 2014; Cissé et al., 2017), generalization bounds (Bartlett et al., 2017; Long & Sedghi, 2020),
interpretable gradients (Tsipras et al., 2018) and Wasserstein distance estimation (Villani, 2008).
Lip(f ) also upper bounds the increase in the norm of gradient during backpropagation and can thus
prevent gradient explosion during training, enabling us to train very deep networks (Xiao et al., 2018).
While heuristic methods to enforce Lipschitz constraints (Miyato et al., 2018; Gulrajani et al., 2017)
have achieved much practical success, they do not provably enforce a bound on Lip(f ) globally and
it remains challenging to achieve similar results when Lip(f ) is provably bounded.

Using the property: Lip(g ◦ _h) ≤_ Lip(g) Lip(h), the Lipschitz constant of the neural network can
be bounded by the product of the Lipschitz constant of all layers. While this allows us to construct
1-Lipschitz neural networks by constraining each layer to be 1-Lipschitz, Anil et al. (2018) identified
a key difficulty with this approach. Because a 1-Lipschitz layer can only reduce the norm of gradient
during backpropagation, backprop through each layer reduces the gradient norm, resulting in small
gradient values for layers closer to the input, making training slow and difficult. To address this
problem, they introduce Gradient Norm Preserving (GNP) architectures where each layer preserves
the gradient norm during backpropagation. This involves constraining the Jacobian of each linear
layer to be an orthogonal matrix and using a GNP activation function called GroupSort. GroupSort
activation function (Anil et al., 2018) first separates the vector of preactivations z ∈ R[m] into groups
of pre-specified sizes, sorts each group in the descending order and then concatenates these sorted
groups. When the group size is 2, the resulting activation function is called MaxMin.

1Unless specified, we assume the Lipschitz constant under the l2 norm in this work.


-----

For 1-Lipschitz CNNs, the robustness certificate for a sample x from class l is computed as
_f_ (x)/√2 where _f_ (x) = fl(x) maxi=l fi(x). Naturally, larger values of fl(x) and smaller
_M_ _M_ _−_ _̸_

values of maxj=l fj(x) will lead to larger certificates. This requires the last weight matrix in the
_̸_
network, denoted by W ∈ R[k][×][m] (k is the number of classes, m is the dimension of the penultimate
layer, m > k), to enforce the following constraints throughout training:

_∀j, ∥Wj,:∥2 = 1,_ _i ̸= j, Wj,: ⊥_ **Wi,:**

where Wi,: denotes the i[th] row of W. Now suppose that for some input image with label l, we want
to update W to increase the logit for the l[th] class. Since ∥Wl,:∥2 is constrained to be 1, the logit can
only be increased by changing the direction of the vector Wl,:. Because the other rows **Wi,:, i** = l
_{_ _̸_ _}_
are constrained to be orthogonal to Wl,:, this further requires an update for all the rows of W. Thus
during training, any update made to learn some class must necessitate the forgetting of information
relevant for the other classes. This can be particularly problematic when the number of classes k and
thus the number of orthogonality constraints per row (i.e., k − 1) is large (such as in CIFAR-100).

To address this limitation, we propose to keep the last weight layer of the network unchanged. But
then the resulting function is no longer 1-Lipschitz and the certificate _f_ (x)/√2 is not valid. Thus,
_M_

we introduce a new certification procedure that does not require the last weight layer of the network
**W to be orthogonal. Our certificate is then given by the following equation:**

_fl(x)_ _fi(x)_
min _−_
_i≠_ _l_ _∥Wl,: −_ **Wi,:∥2**

However, a limitation of using the above certificate is that because the weight layers are completely
unconstrained, larger norms of rows (i.e.,thus smaller certificate values. To address this limitation, we normalize all rows to be of unit norm ∥Wi,:∥) can result in larger values of ∥Wl,: − **Wi,:∥2 and**
before computing the logits. While this still requires all the rows to be of unit norm, their directions
are now allowed to change freely thus preventing the need to update other rows and forgetting of
learned information. We show that this provides significant improvements when the number of classes
is large. We call this procedure Last Layer Normalization (abbreviated as LLN). On CIFAR-100, this
significantly improves both the standard (> 3%) and provable robust accuracy (> 4% at ρ = 36/255)
across multiple 1-Lipschitz CNN architectures (Table 1). Here, ρ is the l2 attack radius.

Another limitation of existing 1-Lipschitz CNNs (Li et al., 2019b; Trockman & Kolter, 2021; Singla
& Feizi, 2021) is that their robustness guarantees do not scale properly with the l2 radius ρ. For
example, the provable robust accuracy of (Singla & Feizi, 2021) drops ∼ 30% at ρ = 108/255
compared to 36/255 on CIFAR-10 (Table 2). To address this limitation, we introduce a certificate
regularization denoted by CR (Section 5) that when used along with the Householder activation
results in significantly improved provable robust accuracy at larger radius values with minimal loss in
standard accuracy. On the CIFAR-10 dataset, we achieve significant improvements in the provable
robust accuracy for large ρ = 108/255 (min gain of +4.96%) across different architectures with
minimal loss in the standard accuracy (max drop of −0.56%). Results are in Table 2.

Additionally, we characterize the MaxMin activation function as a special case of the more general
_Householder (HH) activations. Recall that given z ∈_ R[m], the HH transformation is a linear function
reflecting z about the hyperplane v[T] **x = 0 (∥v∥2 = 1), given by (I −** 2vv[T] )z where I − 2vv[T] is
orthogonal because ∥v∥2 = 1. The Householder activation function σv is defined below:

**z,** **v[T]** **z > 0,**
_σv(z) =_ (I 2vv[T] )z, **v[T]** **z** 0. (1)
 _−_ _≤_

First, note that since z = (I − 2vv[T] )z along v[T] **z = 0, σv is continuous. Moreover, the Jacobian**
_∇z σv is either I or I −_ 2vv[T] (both orthogonal) implying σv is GNP. Since these properties hold
_∀_ **v : ∥v∥2 = 1, v can be learned during the training. In fact, we prove that any GNP piecewise linear**
function that changes from Q1z to Q2z (Q1, Q2 are square orthogonal matrices) along v[T] **z = 0**
must satisfy Q2 = Q1 **I −** 2vv[T][ ] to be continuous (Theorem 1). Thus, this characterization proves
that every GroupSort activation is a special case of the more general Householder activation function
 
(example in Figure 1, discussion in Section 6).

In summary, in this paper, we make the following contributions:


-----

-  We introduce a certification procedure without orthogonalizing the last linear layer called Last
_Layer Normalization. This procedure significantly enhances the standard and provable robust_
accuracy when the number of classes is large. Using the LipConvnet-15 network on CIFAR-100,
our modification achieves a gain of +4.71% in provable robust accuracy (at ρ = 36/255) with a
gain of +4.80% in standard accuracy (Table 1).

-  We introduce a Certificate Regularizer that significantly advances the provable robust accuracy
with a small reduction in standard accuracy. Using LipConvnet-15 network on CIFAR-10, we
achieve +5.81% improvement in provable robust accuracy (at ρ = 108/255) with only a −0.29%
drop in standard accuracy over the existing methods (Table 2).

-  We introduce a class of piecewise linear GNP activation functions called Householder or HH
activations. We show that the MaxMin activation is a special case of the HH activation for certain
settings. We prove that Householder transformations are necessary for any GNP piecewise linear
function to be continuous (Theorem 1).

2 RELATED WORK

**Provably Lipschitz convolutional neural networks: The class of fully connected neural networks**
(FCNs) which are Gradient Norm Preserving (GNP) and 1-Lipschitz were first introduced by Anil
et al. (2018). They orthogonalize weight matrices and use GroupSort as the activation function
to design each layer to be GNP. While there have been numerous works on enforcing Lipschitz
constraints on convolution layers (Cissé et al., 2017; Tsuzuku et al., 2018; Qian & Wegman, 2019;
Gouk et al., 2020; Sedghi et al., 2019), they either enforce loose Lipschitz bounds or are not scalable
to large networks. To ensure that the Lipschitz constraint on convolutional layers is tight, multiple
recent works try to construct convolution layers with an orthogonal Jacobian matrix (Li et al., 2019b;
Trockman & Kolter, 2021; Singla & Feizi, 2021). These approaches avoid the aforementioned issues
and allow the training of large, provably 1-Lipschitz CNNs while achieving impressive results.

**Provable defenses against adversarial examples: A provably robust classifier is one for which**
we can guarantee that the classifier’s prediction remains constant within some region around the
input. Most of the existing methods for provable robustness either bound the Lipschitz constant
of the neural network or the individual layers (Weng et al., 2018; Zhang et al., 2019; 2018; Wong
et al., 2018; Wong & Kolter, 2018; Raghunathan et al., 2018; Croce et al., 2019; Singh et al., 2018;
Singla & Feizi, 2020; Zhang et al., 2021; 2022; Wang et al., 2021; Huang et al., 2021). However,
these methods do not scale to large and practical networks on the ImageNet dataset (Deng et al.,
2009). To scale to such large networks, randomized smoothing (Liu et al., 2018; Cao & Gong,
2017; Lécuyer et al., 2018; Li et al., 2019a; Cohen et al., 2019; Salman et al., 2019; Levine et al.,
2019; Kumar et al., 2020a;b) has been proposed as a probabilistically certified defense. However,
certifying robustness with high probability requires generating a large number of noisy samples
leading to high inference-time computation. In contrast, the defense we propose is deterministic and
hence not comparable to randomized smoothing. While Levine & Feizi (2021) provide deterministic
robustness certificates using randomized smoothing, their certificates are in the l1 norm and not
directly applicable for the l2 threat model studied in this work. We discuss the differences between l1
and l2 certificates in Appendix Section C.

3 PROBLEM SETUP AND NOTATION

For a vector v, vj denotes its j[th] element. For a matrix A, Aj,: and A:,k denote the j[th] row and k[th]
column respectively. Both Aj,: and A:,k are assumed to be column vectors (thus Aj,: is the transpose
of j[th] row of A). Aj,k denotes the element in j[th] row and k[th] column of A. A:j,:k denotes the
matrix containing the first j rows and k columns of A. The same rules are directly extended to higher
order tensors. I denotes the identity matrix, R to denote the field of real numbers. For θ ∈ R, J[+](θ)
and J[−](θ) denote the orthogonal matrices with determinants +1 and −1 defined as follows:

cos θ sin θ cos θ sin θ
**J[+](θ) =** **J[−](θ) =** (2)
− sin θ cos θ sin θ _−_ cos θ

We construct a 1-Lipschitz neural network, f : R[d] _→_ R[k] (d is the input dimension, k is the
number of classes) by composing 1-Lipschitz convolution layers and GNP activation functions.


-----

To certify robustness for some input x with prediction l, we first define the margin of prediction:
_f_ (x) = max (0, fl(x) maxi=l fi(x)) where fi(x) is the logit for class i and l is the correct
_M_ _−_ _̸_
label. Using Theorem 7 in Li et al. (2019b), we can derive the robustness certificate (in the l2 norm)
as Mf (x)/√2. Thus, the l2 distance of x to the decision boundary is lower bounded by Mf (x)/√2:

min min (3)
_i≠_ _l_ _fi(x[∗])=fl(x[∗])_ _√2_

_[∥][x][∗]_ _[−]_ **[x][∥][2][ ≥M][f]** [(][x][)]

We often use the abbreviation fi − _fj : R[D]_ _→_ R to denote the function so that:

(fi − _fj) (x) = fi(x) −_ _fj(x),_ _∀_ **x ∈** R[D]

Our goal is to train the neural network f to achieve the maximum possible provably robust accuracy
while also simultaneously improving (or maintaining) standard accuracy.

4 LAST LAYER NORMALIZATION


To ensure that the network is 1-Lipschitz so that the certificate in equation (3) is valid, existing
1-Lipschitz neural networks require the weight matrices of all the linear layers of the network to be
orthogonal. For the weight matrix in the last layer of the network (that maps the penultimate layer
neurons to the logits), W ∈ R[k][×][m] (k is the number of classes, m is the dimension of the penultimate
layer, m > k), this enforces the following constraints on each row Wi,: ∈ R[m]:
_∀j, ∥Wj,:∥2 = 1,_ _∀i ̸= j, Wj,: ⊥_ **Wi,:** (4)
Now suppose that for some input x with label l, we want to update W to increase the logit for the
_l[th]_ class. Since ∥Wl,:∥2 is constrained to be 1, the gradient update can only change the direction of
the vector Wl,:. But now, because the other rows **Wi,:, i** = l are constrained to be orthogonal to
_{_ _̸_ _}_
**Wl,:, this further requires an update for all the rows of W. This has the negative effect that during**
training, any update made to learn some class must necessitate the forgetting of information relevant
for the other classes. This can be particularly problematic when the number of classes k is large (such
as in CIFAR-100) and thus the number of orthogonality constraints per row i.e. k − 1 is large.

To address this limitation, first observe that the neural network from the input layer to the penultimate
layer (i.e excluding the last linear layer) is 1-Lipschitz. Let g : R[d] _→_ R[m] be this function so that
_f_ (x) = Wg(x) + b. This equation suggests that even if W is not orthogonal, the Lipschitz constant
of the function fl _fi, can be computed by multiplying the Lipschitz constant of g (which is 1) and_
that of (Wl,: **W −i,:) (which is** **Wl,:** **Wi,:** 2). The robustness certificate can then be computed
as _f_ (x)/ _−Wl,:_ **Wi,:** 2. This procedure leads to the following proposition: ∥ _−_ _∥_
_M_ _∥_ _−_ _∥_
**Proposition 1. Given 1-Lipschitz continuous function g : R[d]** _→_ R[m] _and W ∈_ R[k][×][m], b ∈ R[k] _(k_
_is the number of classes), construct a new function f : R[d]_ _→_ R[k] _defined as: f_ (x) = Wg(x) + b.
_Let fl(x) > maxi=l fi(x). The robustness certificate (under the l2 norm) is given by:_
_̸_

_fl(x)_ _fi(x)_
min min _−_
_i≠_ _l_ _fl(x[∗])=fi(x[∗])_ _[∥][x][∗]_ _[−]_ **[x][∥][2][ ≥]** [min]i≠ _l_ _∥Wl,: −_ **Wi,:∥2**

Proof is in Appendix Section A.1.

However, in our experiments, we found that using this procedure directly i.e without any constraint
on the weight matrix W often results in large norms of row vectors **Wi,:** 2, and thus large values
_∥_ _∥_
ofrows of the matrix to be of unit norm before computing the logits so that for the input ∥Wl,: − **Wi,:∥2 and smaller certificates (Theorem 1). To address this problem, we normalize all x, the logit**
_gi(x) can be computed as follows:_

_gi(x) = [(][W][i,][:][)][T][ f]_ [(][x][)] + bi

**Wi,:** 2
_∥_ _∥_

The robustness certificate can then be computed as follows:

_gl(x)_ _gi(x)_ **Wj,:** _T_
mini≠ _l_ _∥Wl,[(][n]:_ [)] _−[−]_ **[W]i,[(][n]:** [)][∥][2] _,_ where ∀j, Wj,[(][n]:[)] [=] _∥Wj,:∥2_ _, gj(x) =_ Wj,[(][n]:[)] _f_ (x) + bj

While each row Wi,: is still constrained to be of unit norm, unlike with orthogonality constraints
(equation (4)), their directions are allowed to change freely. This provides significant improvements
when the number of classes is large. We call this procedure Last Layer Normalization (LLN).


-----

(a) Case 1: z1 sin(θ/2) − _z2 cos(θ/2) > 0_ (b) Case 2: z1 sin(θ/2) − _z2 cos(θ/2) ≤_ 0

Figure 1: Illustration of the Householder activation, σθ. In each colored region, σθ is linear. The
Jacobian is I when (z1, z2) lies in the pink region (Case 1) and I 2vv[T] in the other region (Case 2)
_−_
where v = [sin(θ/2) _−_ cos(θ/2)][T] . Both of these matrices are orthogonal implying σθ is GNP.

5 CERTIFICATE REGULARIZATION

A limitation of using cross entropy loss for training 1-Lipschitz CNNs is that it is not explicitly
designed to maximize the margin _f_ (x) and thus, the robustness certificate. That is, once the cross
_M_
entropy loss becomes small, the gradients will no longer try to further increase the margin _f_ (x)
_M_
even though the network may have the capacity to learn bigger margins.

To address this limitation, we can simply subtract the certificate i.e _f_ (x)/√2 from the usual
_−M_

cross entropy loss function during training. Observe that we subtract the certificate because we want to
_maximize the certificate values while minimizing the cross entropy loss. However, in our experiments_
we found that this regularization term excessively penalizes the network for the misclassified examples
and as a result, the certificate values for the correctly classified inputs are not large. Thus, we propose
to use the following regularized loss function during training:

_f_ (x)

min E(x,l) _D_ _ℓ_ (fΩ(x), l) _γ relu_ _M_ (5)
Ω _∼_ _−_ _√2_

  

In the above equation, fΩ denotes the 1-Lipschitz neural network parametrized by Ω, fΩ(x) denotes
the logits for the input x, ℓ (fΩ(x), l) is the cross entropy loss for input x with label l and γ > 0 is
the regularization coefficient for maximizing the certificate. We have the minus sign in front of the
regularization term γ relu( _f_ (x)/√2) because we want to maximize the certificate while minimizing
_M_

the cross entropy loss. For wrongly classified inputs, _f_ (x)/√2 < 0 = relu( _f_ (x)/√2) = 0.
_M_ _⇒_ _M_

This ensures that the optimization tries to increase the certificates only for the correctly classified
inputs. We call the above mentioned procedure Certificate Regularization (abbreviated as CR).

6 HOUSEHOLDER ACTIVATION FUNCTIONS


Recall that given z ∈ R[m], the Householder (HH) transformation reflects z about the hyperplane
**v[T]** **x = 0 where ∥v∥2 = 1. The linear transformation is given by the equation (I −** 2vv[T] )z where
**I −** 2vv[T] is orthogonal because ∥v∥2 = 1. Now, consider the nonlinear function σv defined below:

**Definition 1. (Householder Activation of Order 1) The activation function σv : R[m]** _→_ R[m], applied
_on z ∈_ R[m], is called the m-dimensional Householder Activation of Order 1:

**z,** **v[T]** **z > 0,**
_σv(z) =_ (I 2vv[T] )z, **v[T]** **z** 0. (6)
 _−_ _≤_

Since σv is linear when v[T] **z > 0 or v[T]** **z < 0, it is also continuous in both cases. At the hyperplane**
separating the two cases (i.e., v[T] **z = 0) we have: (I −** 2vv[T] )z = z − 2(v[T] **z)v = z (both linear**
functions are equal). Thus, σv is continuous ∀ **z ∈** R[m]. Moreover, the Jacobian is either I or


-----

**I −** 2vv[T] which are both square orthogonal matrices. Thus, σv is also GNP and 1-Lipschitz. Since
these properties hold for all v satisfying ∥v∥2 = 1, v can be made a learnable parameter.

While the above arguments suggest that HH transformations are sufficient to ensure such functions
are continuous, we also prove that they are necessary. That is, we prove that if a GNP piecewise
linear function g : R[m] _→_ R[m] transitions between different linear functions Q1z and Q2z (in an
open set S ⊂ R[m]) along a hyperplane v[T] **z = 0 (where ∥v∥2 = 1), then g is continuous in S if**
_and only if Q2 = Q1(I −_ 2vv[T] ). This theoretical result provides a general principle for designing
piecewise linear GNP activation functions. The formal result is stated in the following Theorem:
(Theorem 1.∥v∥2 = 1) such that Given an open set S ∩{z : v S[T] ⊂z = 0R[m]} ̸, orthogonal square matrices= ∅, the function g defined as follows: Q1 ̸= Q2, and vector v ∈ R[m]

_g(z) =_ **Q1z,** **z ∈** _S, v[T]_ **z > 0,** (7)
**Q2z,** **z** _S, v[T]_ **z** 0
 _∈_ _≤_

_is continuous in S if and only if Q2 = Q1(I −_ 2vv[T] ).

Proof of Theorem 1 is in Appendix A.2. Note that since the matrix I − 2vv[T] has determinant −1,
the above theorem necessitates that det(Q1) = det(Q2) i.e the determinant of the Jacobian must
_−_
change sign whenever the Jacobian of a piecewise linear GNP activation function changes.

Recall that for the MaxMin activation function, MaxMin(z1, z2) = (z1, z2) if z1 > z2 and (z2, z1)
**Jotherwise. Thus, the Jacobian of[−](π/2). Using Theorem 1, we can easily prove that MaxMin for z1 > z MaxMin2 case is is a special case of the more general I = J[+](0) while for z1 ≤** _z2 is_
Householder activation functions where the Jacobian J[−](π/2) is replaced with J[−](θ) and the
conditions z1 > z2 are replaced with z1 sin(θ/2) > z2 cos(θ/2) (similarly for ≤). The construction
of Householder activations in 2 dimensions, denoted by σθ, is given in the following corollary:
**Corollary 1. The function σθ : R[2]** _→_ R[2] _defined as_

1 0 _z1_ if z1 sin (θ/2) _z2 cos (θ/2) > 0_
0 1 _z2_ _−_

_σθ (z1, z2) =_     (8)

 cos θ sin θ _z1_ if z1 sin (θ/2) _z2 cos (θ/2)_ 0

sin θ cos θ _z2_ _−_ _≤_
 _−_   

_is continuous and is called_ 2D Householder Activation of Order 1.

The two cases are demonstrated in Figure 1a and Figure 1b, respectively. Since σθ is continuous, GNP
and 1-Lipschitz ∀ _θ ∈_ R, θ is a learnable parameter. For θ = π/2 in equation (8), σθ is equivalent to
MaxMin. Thus, σθ is at least as expressive as MaxMin.

A major limitation of using σv (equation (6)) directly is that it has only 2 linear regions and is thus
limited in its expressive power. In contrast, MaxMin first divides the preactivation z ∈ R[m] (assuming
_m is divisible by 2) into m/2 groups of size 2 each. Since each group has 2 linear regions, we get_
2[m/][2] linear regions from the m/2 groups. Thus, to increase the expressive power, we similarly divide
**z into m/2 groups of size 2 each and apply the 2-dimensional Householder activation function of**
Order 1 (σθ) to each group resulting in 2[m/][2] linear regions (same as MaxMin).

To apply σθ to the output of a convolution layer z ∈ R[m][×][n][×][n] (m is the number of channels and
_n × n is the spatial size), we first split z into 2 tensors along the channel dimension giving the tensors:_
**z:m/2,:,: and zm/2:,:,:. Each of these tensors is of size m/2 × n × n giving n[2]m/2 groups. We use**
the same θ for each pair of channels (irrespective of spatial location) resulting in m/2 learnable
parameters. We initialize each θ = π/2 so that σθ is equivalent to MaxMin at initialization.

7 EXPERIMENTS

Our goal is to evaluate the effectiveness of the three changes proposed in this work: (a) Last Layer
Normalization, (b) Certificate regularization and (c) Householder activation functions. We perform
experiments under the setting of provably robust image classification on CIFAR-10 and CIFAR-100
datasets using the same 1-Lipschitz CNN architectures used by Singla & Feizi (2021) (LipConvnet-5,
10, 15, . . ., 40) due to their superior performance over the prior works. We compare with the three


-----

orthogonal convolution layers in the literature: SOC (Singla & Feizi, 2021), BCOP (Li et al., 2019b)
and Cayley (Trockman & Kolter, 2021) using MaxMin as the activation function.

We use SOC with MaxMin as the primary baseline for comparison in the maintext due to their superior
performance over the prior works (BCOP, Cayley). Results using BCOP and Cayley convolutions are
given in Appendix Sections H and I for completeness. We use the same implementations for these
convolution layers as given in their respective github repositories. We compare the provable robust
accuracy using 3 different values of the l2 perturbation radius ρ = 36/255, 72/255, 108/255. In
both Tables 1 and 2, for all networks, we use SOC as the convolution layer. Using SOC, we achieve
the same bound on the approximation error of an orthogonal Jacobian as achieved in Singla & Feizi
(2021) i.e. 2.42 × 10[−][6] across all convolution layers in all networks. Thus, even for the network with
40 layers, this results in maximum Lipschitz constant of (1 + 2.42 × 10[−][6])[40] = 1.000097 ≤ 1.0001.
Thus, the Lipschitz constant across all our networks is bounded by 1.0001. The symbol HH (in
Tables 1, 2) is for the 2D Householder Activation of order 1 or σθ (defined in equation (8)).

All experiments were performed using 1 NVIDIA GeForce RTX 2080 Ti GPU. All networks were
trained for 200 epochs with initial learning rate of 0.1, dropped by a factor of 0.1 after 100 and 150
epochs. For Certificate Regularization (or CR), we set the parameter γ = 0.5.

7.1 RESULTS ON CIFAR-100

In Table 1, for each architecture, the row "SOC + MaxMin" uses the MaxMin activation, "+ LLN"
adds Last Layer Normalization (uses MaxMin), "+ HH" replaces MaxMin with σθ (also uses LLN)
, "+ CR" also adds Certificate Regularization with γ = 0.1 (uses both σθ and LLN). The column,
"Increase (Standard)" denotes the increase in standard accuracy relative to "SOC + MaxMin".

By adding LLN (the row "+ LLN"), we observe gains in standard (min gain of 1.10%) and provable
robust accuracy (min gain of 1.71% at ρ = 36/255) across all the LipConvnet architectures (gains
relative to "SOC + MaxMin"). These gains are smallest for the LipConvnet-40 network with the
maximum depth. However, replacing MaxMin with the σθ activation further improves the standard
(min gain of 3.65%) and provable robust accuracy (min gain of 4.46% at ρ = 36/255) across
all networks (again relative to "SOC + MaxMin"). We observe that replacing MaxMin with σθ
significantly improves the performance of the deeper LipConvnet-35, 40 networks.

Adding CR further improves the provable robust accuracy while only slightly reducing the standard
accuracy. Because LLN significantly improves the standard accuracy, we compare the standard
accuracy numbers between rows "+ CR" and "+ LLN" to evaluate the drop due to CR. We observe
a small drop in standard accuracy (−0.04%, −0.11%) only for LipConvnet-5 and LipConvnet-15
networks. For the other networks, the standard accuracy actually increases.

7.2 RESULTS ON CIFAR-10

In Table 2, for each architecture, the row "SOC + MaxMin" uses the MaxMin activation, the row "+
HH" uses σθ activation (replacing MaxMin) and the row "+ CR" also adds Certificate Regularization
with γ = 0.1 (again using σθ as the activation). Due to the small number of classes in CIFAR-10, we
do not observe significant gains using Last Layer Normalization or LLN (Appendix Table 10). Thus,
we do not include LLN for any of the results in Table 2. The column, "Increase (108/255)" denotes
the increase in provable robust accuracy with ρ = 108/255 relative to "SOC + MaxMin".

For LipConvnet-25, 30, 35, 40 architectures, we observe gains in both the standard and provable
robust accuracy by replacing MaxMin with the HH activation (i.e σθ). Similar to what we observe
for CIFAR-100 in Table 1, the gains in provable robust accuracy (ρ = 108/255) are significantly
higher for deeper networks: LipConvnet-35 (3.65%) and LipConvnet-40 (4.35%) with decent gains
in standard accuracy (1.71 and 1.61% respectively).

Again similar to CIFAR-100, adding CR further boosts the provable robust accuracy while slightly
reducing the standard accuracy. Comparing "+ CR" with "SOC + MaxMin", we observe small
drops in standard accuracy for LipConvnet-5, 10, . . ., 30 networks (max. drop of −0.56%), and
gains for LipConvnet-35 (+0.52%) and LipConvnet-40 (+0.96%). For provable robust accuracy
(ρ = 108/255), we observe very significant gains of > 4.96% for all networks and > 8% for the
deeper LipConvnet-35, 40 networks.


-----

|Architecture Methods|Standard Provable Robust Acc. (ρ =) Increase Accuracy 36/255 72/255 108/255 (Standard)|
|---|---|
|SOC + MaxMin + LLN LipConvnet-5 + HH + CR|42.71% 27.86% 17.45% 9.99% _ 45.86% 31.93% 21.17% 13.21% +3.15% 46.36% 32.64% 21.19% 13.12% +3.65% 45.82% 32.99% 22.48% 14.79% +3.11%|
|SOC + MaxMin + LLN LipConvnet-10 + HH + CR|43.72% 29.39% 18.56% 11.16% _ 46.88% 33.32% 22.08% 13.87% +3.16% 47.96% 34.30% 22.35% 14.48% +4.24% 47.07% 34.53% 23.50% 15.66% +3.35%|
|SOC + MaxMin + LLN LipConvnet-15 + HH + CR|42.92% 28.81% 17.93% 10.73% _ 47.72% 33.52% 21.89% 13.76% +4.80% 47.72% 33.97% 22.45% 13.81% +4.80% 47.61% 34.54% 23.16% 15.09% +4.69%|
|SOC + MaxMin + LLN LipConvnet-20 + HH + CR|43.06% 29.34% 18.66% 11.20% _ 46.86% 33.48% 22.14% 14.10% +3.80% 47.71% 34.22% 22.93% 14.57% +4.65% 47.84% 34.77% 23.70% 15.84% +4.78%|
|SOC + MaxMin + LLN LipConvnet-25 + HH + CR|43.37% 28.59% 18.18% 10.85% _ 46.32% 32.87% 21.53% 13.86% +2.95% 47.70% 34.00% 22.67% 14.57% +4.33% 46.87% 34.09% 23.41% 15.61% +3.50%|
|SOC + MaxMin + LLN LipConvnet-30 + HH + CR|42.87% 28.74% 18.47% 11.21% _ 46.18% 32.82% 21.52% 13.52% +3.31% 46.80% 33.72% 22.70% 14.31% +3.93% 46.92% 34.17% 23.21% 15.84% +4.05%|
|SOC + MaxMin + LLN LipConvnet-35 + HH + CR|42.42% 28.34% 18.10% 10.96% _ 45.22% 32.10% 21.28% 13.25% +2.80% 46.21% 32.80% 21.55% 14.13% +3.79% 46.88% 33.64% 23.34% 15.73% +4.46%|
|SOC + MaxMin + LLN LipConvnet-40 + HH + CR|41.84% 28.00% 17.40% 10.28% _ 42.94% 29.71% 19.30% 11.99% +1.10% 45.84% 32.79% 21.98% 14.07% +4.00% 45.03% 32.57% 22.37% 14.76% +3.19%|


Table 1: Results for provable robustness against adversarial examples on the CIFAR-100 dataset.
Results with BCOP and Cayley convolutions are in Appendix Tables 13 and 14.

8 CONCLUSION

In this work, we introduce a procedure to certify robustness of 1-Lipschitz convolutional neural
networks without orthogonalizing of the last linear layer of the network. Additionally, we introduce a
certificate regularization that significantly improves the provable robust accuracy for these models at
higher l2 radii. Finally, we introduce a class of GNP activation functions called Householder (or HH)
activations and prove that the Jacobian of any Gradient Norm Preserving (GNP) piecewise linear
function is only allowed to change via Householder transformations for the function to be continuous


-----

|Architecture Methods|Standard Provable Robust Acc. (ρ =) Increase Accuracy 36/255 72/255 108/255 (108/255)|
|---|---|
|SOC + MaxMin LipConvnet-5 + HH + CR|75.78% 59.18% 42.01% 27.09% _ 76.30% 60.12% 43.20% 27.39% +0.30% 75.31% 60.37% 45.62% 32.38% +5.29%|
|SOC + MaxMin LipConvnet-10 + HH + CR|76.45% 60.86% 44.15% 29.15% _ 76.86% 61.52% 44.91% 29.90% +0.75% 76.23% 62.57% 47.70% 34.15% +5.00%|
|SOC + MaxMin LipConvnet-15 + HH + CR|76.68% 61.36% 44.28% 29.66% _ 77.41% 61.92% 45.60% 31.10% +1.44% 76.39% 62.96% 48.47% 35.47% +5.81%|
|SOC + MaxMin LipConvnet-20 + HH + CR|76.90% 61.87% 45.79% 31.08% _ 76.99% 61.76% 45.59% 30.99% -0.09% 76.34% 62.63% 48.69% 36.04% +4.96%|
|SOC + MaxMin LipConvnet-25 + HH + CR|75.24% 60.17% 43.55% 28.60% _ 76.37% 61.50% 44.72% 29.83% +1.23% 75.21% 61.98% 47.93% 34.92% +6.32%|
|SOC + MaxMin LipConvnet-30 + HH + CR|74.51% 59.06% 42.46% 28.05% _ 75.25% 59.90% 43.85% 29.35% +1.30% 74.23% 60.64% 46.51% 34.08% +6.03%|
|SOC + MaxMin LipConvnet-35 + HH + CR|73.73% 58.50% 41.75% 27.20% _ 75.44% 61.05% 45.38% 30.85% +3.65% 74.25% 61.30% 47.60% 35.21% +8.01%|
|SOC + MaxMin LipConvnet-40 + HH + CR|71.63% 54.39% 37.92% 24.13% _ 73.24% 58.12% 42.24% 28.48% +4.35% 72.59% 59.04% 44.92% 32.87% +8.74%|


Table 2: Results for provable robustness against adversarial examples on the CIFAR-10 dataset.
Results with BCOP and Cayley convolutions are in Appendix Tables 7 and 8.

which provides a general principle for designing piecewise linear GNP functions. These ideas lead to
improved deterministic ℓ2 robustness certificates on CIFAR-10 and CIFAR-100.


-----

ACKNOWLEDGEMENTS

This project was supported in part by NSF CAREER AWARD 1942230, a grant from NIST
60NANB20D134, HR001119S0026-GARD-FP-052, HR00112090132, ONR YIP award N00014-221-2271, Army Grant W911NF2120076.

REPRODUCIBILITY

[The code for reproducing all experiments in the paper is available at https://github.com/](https://github.com/singlasahil14/SOC)
[singlasahil14/SOC.](https://github.com/singlasahil14/SOC)

ETHICS STATEMENT

This paper introduces novel set of techniques for improving the provable robustness of 1-Lipschitz
CNNs. We do not see any foreseeable negative consequences associated with our work.

REFERENCES

Cem Anil, James Lucas, and Roger B. Grosse. Sorting out lipschitz function approximation. In
_ICML, 2018._

Peter L. Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Proceedings of the 31st International Conference on Neural Information
_Processing Systems, NIPS’17, pp. 6241–6250, USA, 2017. Curran Associates Inc. ISBN 978-1-_
[5108-6096-4. URL http://dl.acm.org/citation.cfm?id=3295222.3295372.](http://dl.acm.org/citation.cfm?id=3295222.3295372)

Xiaoyu Cao and Neil Zhenqiang Gong. Mitigating evasion attacks to deep neural networks via
region-based classification. In Proceedings of the 33rd Annual Computer Security Applications
_Conference, ACSAC 2017, pp. 278–287, New York, NY, USA, 2017. Association for Computing_
[Machinery. ISBN 9781450353458. doi: 10.1145/3134600.3134606. URL https://doi.org/](https://doi.org/10.1145/3134600.3134606)
[10.1145/3134600.3134606.](https://doi.org/10.1145/3134600.3134606)

Moustapha Cissé, Piotr Bojanowski, Edouard Grave, Yann N. Dauphin, and Nicolas Usunier. Parseval
networks: Improving robustness to adversarial examples. In Doina Precup and Yee Whye Teh (eds.),
_Proceedings of the 34th International Conference on Machine Learning, ICML 2017, Sydney, NSW,_
_Australia, 6-11 August 2017, volume 70 of Proceedings of Machine Learning Research, pp. 854–_
[863. PMLR, 2017. URL http://proceedings.mlr.press/v70/cisse17a.html.](http://proceedings.mlr.press/v70/cisse17a.html)

Jeremy M. Cohen, Elan Rosenfeld, and J. Zico Kolter. Certified adversarial robustness via randomized
smoothing. In ICML, 2019.

Francesco Croce, Maksym Andriushchenko, and Matthias Hein. Provable robustness of relu networks
via maximization of linear regions. AISTATS 2019, 2019.

J. Deng, Wei Dong, R. Socher, L. Li, K. Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248–255,
2009.

Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael J. Cree. Regularisation of neural
networks by enforcing lipschitz continuity, 2020.

Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C
Courville. Improved training of wasserstein gans. In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances
_in Neural Information Processing Systems, volume 30, pp. 5767–5777. Curran Asso-_
[ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/](https://proceedings.neurips.cc/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf)
[892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf.](https://proceedings.neurips.cc/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf)


-----

Yujia Huang, Huan Zhang, Yuanyuan Shi, J Zico Kolter, and Anima Anandkumar. Training certifiably
robust neural networks with efficient local lipschitz bounds. In A. Beygelzimer, Y. Dauphin,
P. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems,
[2021. URL https://openreview.net/forum?id=FTt28RYj5Pc.](https://openreview.net/forum?id=FTt28RYj5Pc)

Aounon Kumar, Alexander Levine, Soheil Feizi, and Tom Goldstein. Certifying confidence via
randomized smoothing. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.),
_Advances in Neural Information Processing Systems, volume 33, pp. 5165–5177. Curran Asso-_
[ciates, Inc., 2020a. URL https://proceedings.neurips.cc/paper/2020/file/](https://proceedings.neurips.cc/paper/2020/file/37aa5dfc44dddd0d19d4311e2c7a0240-Paper.pdf)
[37aa5dfc44dddd0d19d4311e2c7a0240-Paper.pdf.](https://proceedings.neurips.cc/paper/2020/file/37aa5dfc44dddd0d19d4311e2c7a0240-Paper.pdf)

Aounon Kumar, Alexander Levine, Tom Goldstein, and Soheil Feizi. Curse of dimensionality on randomized smoothing for certifiable robustness. In Hal Daumé III and Aarti Singh
(eds.), Proceedings of the 37th International Conference on Machine Learning, volume 119 of
_Proceedings of Machine Learning Research, pp. 5458–5467. PMLR, 13–18 Jul 2020b. URL_
[http://proceedings.mlr.press/v119/kumar20b.html.](http://proceedings.mlr.press/v119/kumar20b.html)

Mathias Lécuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and S. K. K. Jana. Certified
robustness to adversarial examples with differential privacy. In IEEE S&P 2019, 2018.

Alexander Levine and Soheil Feizi. Improved, deterministic smoothing for l1 certified robustness. In
_ICML, 2021._

Alexander Levine, Sahil Singla, and Soheil Feizi. Certifiably robust interpretation in deep learning,
2019.

Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Certified adversarial robustness with additive noise. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alche-Buc, E. Fox, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 32, pp. 9464–9474. Curran As[sociates, Inc., 2019a. URL https://proceedings.neurips.cc/paper/2019/file/](https://proceedings.neurips.cc/paper/2019/file/335cd1b90bfa4ee70b39d08a4ae0cf2d-Paper.pdf)
[335cd1b90bfa4ee70b39d08a4ae0cf2d-Paper.pdf.](https://proceedings.neurips.cc/paper/2019/file/335cd1b90bfa4ee70b39d08a4ae0cf2d-Paper.pdf)

Qiyang Li, Saminul Haque, Cem Anil, James Lucas, Roger Grosse, and Jörn-Henrik Jacobsen.
Preventing gradient attenuation in lipschitz constrained convolutional networks. Conference on
_Neural Information Processing Systems, 2019b._

Xuanqing Liu, Minhao Cheng, Huan Zhang, and C. Hsieh. Towards robust neural networks via
random self-ensemble. In ECCV, 2018.

Philip M. Long and Hanie Sedghi. Generalization bounds for deep convolutional neural networks. In
_[International Conference on Learning Representations, 2020. URL https://openreview.](https://openreview.net/forum?id=r1e_FpNFDr)_
[net/forum?id=r1e_FpNFDr.](https://openreview.net/forum?id=r1e_FpNFDr)

Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. In International Conference on Learning Representations, 2018.
[URL https://openreview.net/forum?id=B1QRgziT-.](https://openreview.net/forum?id=B1QRgziT-)

Haifeng Qian and Mark N. Wegman. L2-nonexpansive neural networks. In International Confer_[ence on Learning Representations, 2019. URL https://openreview.net/forum?id=](https://openreview.net/forum?id=ByxGSsR9FQ)_
[ByxGSsR9FQ.](https://openreview.net/forum?id=ByxGSsR9FQ)

Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Semidefinite relaxations for certifying
robustness to adversarial examples. In NeurIPS, 2018.

Hadi Salman, Jerry Li, Ilya Razenshteyn, Pengchuan Zhang, Huan Zhang, Sebastien Bubeck, and
Greg Yang. Provably robust deep learning via adversarially trained smoothed classifiers. In
H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Ad_vances in Neural Information Processing Systems, volume 32, pp. 11292–11303. Curran As-_
[sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/](https://proceedings.neurips.cc/paper/2019/file/3a24b25a7b092a252166a1641ae953e7-Paper.pdf)
[3a24b25a7b092a252166a1641ae953e7-Paper.pdf.](https://proceedings.neurips.cc/paper/2019/file/3a24b25a7b092a252166a1641ae953e7-Paper.pdf)

Hanie Sedghi, Vineet Gupta, and Philip M. Long. The singular values of convolutional layers. In
_[International Conference on Learning Representations, 2019. URL https://openreview.](https://openreview.net/forum?id=rJevYoA9Fm)_
[net/forum?id=rJevYoA9Fm.](https://openreview.net/forum?id=rJevYoA9Fm)


-----

Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus Puschel, and Martin T. Vechev. Fast and
effective robustness certification. In NeurIPS, 2018.

Sahil Singla and Soheil Feizi. Second-order provable defenses against adversarial attacks. In
Hal Daumé III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine
_Learning, volume 119 of Proceedings of Machine Learning Research, pp. 8981–8991. PMLR,_
[13–18 Jul 2020. URL http://proceedings.mlr.press/v119/singla20a.html.](http://proceedings.mlr.press/v119/singla20a.html)

[Sahil Singla and Soheil Feizi. Skew orthogonal convolutions. In ICML, 2021. URL https:](https://arxiv.org/abs/2105.11417)
[//arxiv.org/abs/2105.11417.](https://arxiv.org/abs/2105.11417)

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning
_[Representations, 2014. URL http://arxiv.org/abs/1312.6199.](http://arxiv.org/abs/1312.6199)_

Asher Trockman and J Zico Kolter. Orthogonalizing convolutional layers with the cayley transform.
[In International Conference on Learning Representations, 2021. URL https://openreview.](https://openreview.net/forum?id=Pbj8H_jEHYv)
[net/forum?id=Pbj8H_jEHYv.](https://openreview.net/forum?id=Pbj8H_jEHYv)

Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. In ICLR, 2018.

Yusuke Tsuzuku, I. Sato, and Masashi Sugiyama. Lipschitz-margin training: Scalable certification of
perturbation invariance for deep neural networks. In NeurIPS, 2018.

Cedric Villani. Optimal transport, old and new, 2008.

Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and J Zico Kolter.
Beta-CROWN: Efficient bound propagation with per-neuron split constraints for neural network
robustness verification. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan (eds.),
_[Advances in Neural Information Processing Systems, 2021. URL https://openreview.](https://openreview.net/forum?id=ahYIlRBeCFw)_
[net/forum?id=ahYIlRBeCFw.](https://openreview.net/forum?id=ahYIlRBeCFw)

Lily Weng, Huan Zhang, Hongge Chen, Zhao Song, Cho-Jui Hsieh, Luca Daniel, Duane Boning, and
Inderjit Dhillon. Towards fast computation of certified robustness for ReLU networks. In Jennifer
Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine
_Learning, volume 80 of Proceedings of Machine Learning Research, pp. 5276–5285. PMLR,_
[10–15 Jul 2018. URL https://proceedings.mlr.press/v80/weng18a.html.](https://proceedings.mlr.press/v80/weng18a.html)

Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th In_ternational Conference on Machine Learning, volume 80 of Proceedings of Machine Learning_
_Research, pp. 5286–5295, Stockholmsmässan, Stockholm Sweden, 10–15 Jul 2018. PMLR. URL_
[http://proceedings.mlr.press/v80/wong18a.html.](http://proceedings.mlr.press/v80/wong18a.html)

Eric Wong, Frank R. Schmidt, Jan Hendrik Metzen, and J. Zico Kolter. Scaling provable adversarial
defenses. In NeurIPS, 2018.

Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Pennington.
Dynamical isometry and a mean field theory of CNNs: How to train 10,000-layer vanilla convolutional neural networks. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th
_International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning_
_Research, pp. 5393–5402, Stockholmsmässan, Stockholm Sweden, 10–15 Jul 2018. PMLR. URL_
[http://proceedings.mlr.press/v80/xiao18a.html.](http://proceedings.mlr.press/v80/xiao18a.html)

Bohang Zhang, Tianle Cai, Zhou Lu, Di He, and Liwei Wang. Towards certifying linfinity robustness
using neural networks with linfinity-dist neurons. In ICML, 2021.

Bohang Zhang, Du Jiang, Di He, and Liwei Wang. Boosting the certified robustness of l-infinity
[distance nets. In International Conference on Learning Representations, 2022. URL https:](https://openreview.net/forum?id=Q76Y7wkiji)
[//openreview.net/forum?id=Q76Y7wkiji.](https://openreview.net/forum?id=Q76Y7wkiji)


-----

Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural network
robustness certification with general activation functions. In Advances in Neural Information
_Processing Systems (NIPS), arXiv preprint arXiv:1811.00866, dec 2018._

Huan Zhang, Pengchuan Zhang, and Cho-Jui Hsieh. Recurjac: An efficient recursive algorithm for
bounding jacobian matrix of neural networks and its applications. In AAAI Conference on Artificial
_Intelligence (AAAI), arXiv preprint arXiv:1810.11783, dec 2019._


-----

A PROOFS

A.1 PROOF OF PROPOSITION 1

_Proof. We proceed by computing the Lipschitz constant of the function fl_ _fi._
The gradient of the function: fl − _fi at x can be computed using the chain rule: −_

_∇x (fl −_ _fi) = (Wl,: −_ **Wi,:)[T]** _∇xg_

Sinceequation as follows: g is given to be 1-Lipschitz, the Lipschitz constant of fl − _fi can be computed using the above_


_∥∇x (fl −_ _fi) ∥2 ≤∥_ (Wl,: − **Wi,:)[T]** (∇xg) ∥2
_∥∇x (fl −_ _fi) ∥2 ≤∥Wl,: −_ **Wi,:∥2∥∇xg∥2 ≤∥Wl,: −** **Wi,:∥2**

Thus, the distance of x to the decision boundary fl − _fi = 0, is lower bounded by:_

_fl(x)_ _fi(x)_
min _−_
_fl(x[∗])=fi(x[∗])_ **Wl,:** **Wi,:** 2

_[∥][x][∗]_ _[−]_ **[x][∥][2][ ≥]** _∥_ _−_ _∥_

Thus, the distance to decision boundary across all classes i ̸= l is lower bounded by:

_fl(x)_ _fi(x)_
min min _−_
_i≠_ _l_ _fl(x[∗])=fi(x[∗])_ _[∥][x][∗]_ _[−]_ **[x][∥][2][ ≥]** [min]i≠ _l_ _∥Wl,: −_ **Wi,:∥2**

A.2 PROOF OF THEOREM 1


_Proof. We first prove that if Q2 = (I −_ 2vv[T] )Q1, then the function g is continuous.
First, observe that for v[T] **z > 0, g(z) = Q1z which is continuous.**
Similarly, for v[T] **z < 0, g(z) = Q2z which is again continuous.**
This proves that the function g is continuous when v[T] **z > 0 or v[T]** **z < 0.**
Thus, to prove continuity ∀ **z ∈** _S, we must prove that:_

**Q1z = Q2z** **z : v[T]** **z = 0** (9)
_∀_

Since Q2 = Q1(I − 2vv[T] ), we have:

**Q2 −** **Q1 = −2Q1vv[T]**

(Q2 **Q1) z =** 2 **Q1vv[T][ ]** **z =** 2Q1v **v[T]** **z**
_−_ _−_ _−_

The above equation directly proves (9).     

Now, we prove the other direction i.e if g is continuous in S then, Q2 = Q1(I − 2vv[T] ).
Since g is continuous for all z : v[T] **z = 0, we have:**


**Q2z = Q1z** **z : v[T]** **z = 0**
_∀_

(Q2 **Q1) z = 0** **z : v[T]** **z = 0**
_−_ _∀_

Since z ∈ R[m], we know that the set of vectors: {z : v[T] **z = 0} spans a m −** 1 dimensional subspace.
This in turn implies thatThus, the null space of Q Q2 −2 −QQ1 is of size1 is a rank one matrix given by the following equation: m − 1.

**Q2 −** **Q1 = uv[T]** (10)

where the vector u remains to be determined.
Since Q1 and Q2 are orthogonal matrices, we have the following set of equations:

**Q[T]2** **[Q][2]** [=] **Q1 + uv[T][ ][T][  ]Q1 + uv[T][ ]** (11)

**Q2Q[T]2** [=]  Q1 + uv[T][   ]Q1 + uv[T][ ][T] (12)
 


-----

We first simplify equation (11):

**Q[T]2** **[Q][2]** [=] **Q[T]1** [+][ vu][T][   ]Q1 + uv[T][ ]

_T_
**I = I + v**  Q[T]1 **[u]** + **Q[T]1** **[u]** **v[T]** + **u[T]** **u** **vv[T]**

_T_
0 = v **Q[T]1 [u]** + **Q[T]1 [u]** **v[T]+** **u[T] u** **vv[T]**

**u[T] u** **vv[T]** = v  **u[T]** **Q** 1 +  Q[T]1 **[u]v[T]**
_−_
        


Right multiplying both sides by v and using ∥v∥2 = 1, we get:

_−_ **u[T]** **u** **v =** **u[T]** **Q1v** **v + Q[T]1** **[u]**

**Q[T]1 [u][ =][ −]** **u[T]** **u ** + u[T] **Q1v** **v = λv**

**u = λQ1v ,** where λ =  **u[T]** **u + u[T]** **Q1v** (13)
_−_

Substituting u using equation (13) in equation (10), we get:  

**Q2 −** **Q1 = λQ1vv[T]**

**Q2 = Q1** **I + λvv[T][ ]**

Since Q[T]2 **[Q][2]** [=][ I][, we have:]  

**Q[T]2** **[Q][2]** [=] **Q1** **I + λvv[T][ ][T]** **Q1** **I + λvv[T][ ]**

**Q[T]2** **[Q][2]** [=]  I + λ  **vv[T][ ]** **Q[T]1** **[Q][1]** **I +  λvv[T][ ]**

**I =** **I + λ vv[T][   ]I + λvv[T][ ]**  

**I = I  + 2λvv[T]** + λ[2]vv[T]

=⇒ _λ = 0 or λ = −2_

Since λ = 0 would imply Q1 = Q2 which is not allowed by the assumption of the Theorem that
**Q1** = Q2.
_λ = ̸ −2 is the only possibility allowed._
This proves the other direction i.e:

**Q2 = Q1** **I −** 2vv[T][ ]
 

A.3 PROOF OF COROLLARY 1

_Proof. Subsitute Q1, v as follows in Theorem 1._

1 0
**Q1 =**
0 1
 


+ sin (θ/2)
**v =**
− cos (θ/2)

**Q2 = I −** 2vv[T]

1 0 sin (θ/2)
**Q2 =** 2 [sin (θ/2) cos (θ/2)]
0 1 _−_ cos (θ/2) _−_
  − 

1 0 sin[2] (θ/2) sin (θ/2) cos (θ/2)
**Q2 =** 2 _−_
0 1 _−_ sin (θ/2) cos (θ/2) cos[2] (θ/2)
  −

1 2 sin[2] (θ/2) 2 sin (θ/2) cos (θ/2)
**Q2 =** _−_
2 sin (θ/2) cos (θ/2) 1 + 2 cos[2] (θ/2)
 

cos(θ) sin(θ)
**Q2 =**
sin(θ) _−_ cos(θ)

Theorem 1 then directly implies the corollary.


-----

A.4 PROOF OF THEOREM 2

**Theorem 2. Given: 0 ≤** _θ0 < θ1 · · · < θ2n = 2π + θ0 such that_ _i=0_ [(][θ][2][i][+1][ −] _[θ][2][i][) =][ π][ and]_
_αi = 2_ _j=0_ _[θ][i][−][j][(][−][1)][j][, The function][ σ][Θ][ :][ R][2][ →]_ [R][2][ is continuous, GNP and][ 1][-Lipschitz where]
Θ = [θ0, θ1, . . ., θ2n−1] (also called 2D Householder Activation of order[P][n][−] n[1]):

[P][i]σΘ(z1, z2) = ( 1)cos[i] sin αi _αi_ ( 1)sin[i][+1] αcosi _αi_ _zz12_ _θi ≤_ _ϕ < θi+1_ (14)

 _−_ _−_   

_where ϕ ∈_ [θ0, θ2n = 2π + θ0) and cos(ϕ) = z1/ _z1[2]_ [+][ z]2[2][,][ sin(][ϕ][) =][ z][2][/] _z1[2]_ [+][ z]2[2][.]
p p

_Proof. We are given the following:_


_n−1_

_i=0_ (θ2i+1 − _θ2i) = π,_ _αi = 2_

X


_θi−j(−1)[j]_ (15)
_j=0_

X


Note that by definition (equation (14)), the function is linear for θi < ϕ < θi+1 and hence
continuous.
Furthermore, since ϕ [θ0, θ2n), we proceed to prove continuity for the following two cases:
_∈_
**Case 2Case 1::** _θθ0i − < ϕ < θϵ < ϕ < θ0 + ϵ,i + ϵ,_ _θ2n −ϵ >ϵ < ϕ < θ 0, i ≥_ 1 2n, _ϵ > 0_

**Proof for Case 1:**
Using equationθi < ϕ < θi + ϵ (14)., we know that σΘ realizes different linear functions for θi − _ϵ < ϕ < θi and_
Thus, for σΘ to be continuous, we require that the two linear functions be the same at the boundary
i.e ϕ = θi.
We first write the input (z1, z2) in terms of shifted polar coordinates i.e: (r cos(ϕ), r sin(ϕ)) where
_r =_ _z1[2]_ [+][ z]2[2] [and][ cos][ ϕ][ =][ z][1][/] _z1[2]_ [+][ z]2[2][,][ sin][ ϕ][ =][ z][2][/] _z1[2]_ [+][ z]2[2][, ϕ][ ∈] [[][θ][0][, θ][0][ + 2][π][)]

Thus, the function for θi _ϵ < ϕ < θi is given by:_

p _−_ p p

_σΘ (r cos ϕ, r sin ϕ) =_ cos αi−1 sin αi−1 _r cos ϕ_ (16)
(−1)[i][−][1] sin αi−1 (−1)[i] cos αi−1 r sin ϕ

Similarly, the function for θi < ϕ < θi + ϵ is given by:

_σΘ (r cos ϕ, r sin ϕ) =_ cos αi sin αi _r cos ϕ_ (17)
( 1)[i] sin αi ( 1)[i][+1] cos αi _r sin ϕ_
 _−_ _−_   

The difference in the function values at the boundary i.e ϕ = θi, obtained by subtracting equations
(17) and (16) is given as follows:
cos αi sin αi _r cos θi_ cos αi−1 sin αi−1 _r cos θi_
( 1)[i] sin αi ( 1)[i][+1] cos αi _r sin θi_ _−_ ( 1)[i][−][1] sin αi 1 ( 1)[i] cos αi 1 _r sin θi_
 _−_ _−_     _−_ _−_ _−_ _−_   

= r (−1)[i] sincos α αi −i −(−cos1) α[i][−]i−[1] 1sin αi−1 (−1)[i][+1]sincos α αi −i −sin(− α1)i−[i] 1cos αi−1 cossin θ θii

= r ( 1)cos[i](sin αi − αi + sincos αi α−1i 1) ( 1)[i]sin[+1](cos αi − αsini + cos αi−1 αi 1) cossin θ θii
 _−_ _−_ _−_ _−_   

Using sum-to-product trigonometric identities, the above equals:


2 sin _αi−12−αi_



sin _αi−12+αi_



2 sin _αi−2αi−1_



cos _αi+2αi−1_



cos θi
sin θi


cos θi
sin θi




= r

= 2r

= 2r


2( 1)[i] sin _αi+2αi−1_
_−_



2( 1)[i][+1] cos _αi−12−αi_
_−_



cos _αi−2αi−1_



cos _αi−12+αi_



sin _αi−12−αi_



sin _αi−12+αi_



sin _αi−12−αi_ cos _αi+2αi−1_
_−_

_αi_ 1 _αi_ _αi_ 1+αi
( 1)[i][+1] cos _−2−_ cos _−2_ 
_−_
  

_−_ cos _αi−12+αi_ cossin θ θii
 i [] 


( 1)[i] sin _αi+2αi−1_
_−_

sin _αi−12−αi_

_αi_ 1 _αi_
( 1)[i] cos _−2−_
_−_



cos _αi−2αi−1_
 

sin _αi−1+αi_



h 


-----

Using equation (15), we directly have: αi = 2θi _αi_ 1. Thus, the above equation reduces to:
_−_ _−_

= 2r ( 1)sin ([i] cos (θi −θiαi)αi) [sin (θi) _−_ cos (θi)] cossin θ θii = 00..
 _−_ _−_     

Hence, the linear functions given by equations (16) and (17) are equal at ϕ = θi. This proves that the
function is continuous for Case 1.

**Proof for Case 2:**
Using equation (14), we know that σΘ realizes different linear functions for θ0 < ϕ < θ0 + ϵ and
_θ2n_ _ϵ < ϕ < θ2n._
Thus, for − _σΘ to be continuous, we require that the two linear functions be the same at the boundary_
i.e ϕ = θ0.
As before, we first write the input (z1, _z2) in terms of shifted polar coordinates i.e:_
(r cos(ϕ), r sin(ϕ)).
Thus, the function for θ0 < ϕ < θ0 + ϵ is given by:

_σΘ (r cos ϕ, r sin ϕ) =_ cos α0 sin α0 _r cos ϕ_ (18)
sin α0 cos α0 _r sin ϕ_
 _−_   

Similarly, the function for θ2n − _ϵ < φ < θ2n is given by:_

_σΘ (r cos ϕ, r sin ϕ) =_ cos α2n−1 sin α2n−1 _r cos ϕ_ (19)
(−1)[i] sin α2n−1 (−1)[i][+1] cos α2n−1 r sin ϕ

Using equation (15), α2n 1 is given as follows:
_−_

2n−1 _n−1_

_α2n_ 1 = 2 _θ2n_ 1 _i(_ 1)[i] = 2 (θ2i+1 _θ2i) = 2π_
_−_ _i=0_ _−_ _−_ _−_ _i=0_ _−_

X X

Thus, equation (19) reduces to:

1 0 _r cos ϕ_
_σΘ (r cos ϕ, r sin ϕ) =_ (20)
0 1 _r sin ϕ_
   

The difference in the function values at the boundary i.e ϕ = θ0, obtained by subtracting equations
(20) and (18) is given as follows:

1 0 _r cos θ0_ cos α0 sin α0 _r cos θ0_
0 1 _r sin θ0_ _−_ sin α0 cos α0 _r sin θ0_
     _−_   

= r 1 − cos α0 _−_ sin α0 cos θ0
sin α0 1 + cos α0 sin θ0
 _−_   

Using the trigonometric identities: 1 − cos(θ) = 2 sin[2](θ/2), 1 + cos(θ) = 2 cos[2](θ/2) and
sin(θ) = 2 sin(θ/2) cos(θ/2), we have:

= r 2 sin[2]( _[α]2[0]_ [)] _−2 sin(_ _[α]2[0]_ [) cos(][ α]2[0] [)] cos θ0

2 sin( _[α]2[0]_ [) cos(][ α]2[0] [)] 2 cos[2]( _[α]2[0]_ [)] sin θ0

−   

= 2r −sin(cos([α]2[0][α]2[)][0] [)] sin( α20 [)] _−_ cos( _[α]2[0]_ [)][ ]cossin θ θ00

Using equation (15), we have: α0 = 2θ0. Thus, the above equation reduces to:


= 2r sin(θ0)
cos(θ0)
−



[sin(θ0) _−_ cos(θ0)] cossin θ θ00



0.
0.


Hence, the linear functions given by equations (18) and (20) are equal at ϕ = θ0.
This proves that the function is continuous for Case 2.


-----

B SELECTION OF γ USING CROSS VALIDATION

Using 5000 held out samples from CIFAR-10, we tested 6 different values of γ shown in Table 3 and
selected γ = 0.5 because it resulted in less than 0.5% decrease in standard accuracy while 4.96%
increase in provably robust accuracy. We used the LipConvnet-5 network with the 2D Householder
activation function of order 2 i.e σθ.

|Architecture γ|Standard Provable Robust Acc. (ρ =) Increase Accuracy 36/255 72/255 108/255 (108/255)|
|---|---|
|0. 0.10 0.25 LipConvnet-5 0.50 0.75 1.00|75.82% 59.66% 42.78% 26.92% _ 75.58% 59.74% 42.94% 28.04% +0.94% 75.54% 60.22% 43.98% 29.50% +2.58% 75.30% 60.08% 45.36% 31.88% +4.96% 74.14% 60.36% 46.08% 33.44% +6.52% 73.86% 60.30% 46.80% 34.60% +7.68%|



Table 3: Results for provable robustness against adversarial examples on the CIFAR-10 dataset for
cross validation using 5000 held out samples from the training set.

C DIFFERENCES BETWEEN l1 AND l2 CERTIFICATE

By the equivalence of norms, we have the following result:


**x ∈** R[d],


_d_ **x** 1 (21)
_∥_ _∥_


**x** 1 **x** 2
_∥_ _∥_ _≤∥_ _∥_ _≤_


Let us assume that we have an l1 certificate for the input x so that the prediction of a neural network
remains constant in a region of l1 radius ρ1 around the input x i.e:

**x[∗]** **x** 1 _ρ1_ (22)
_∥_ _−_ _∥_ _≤_

We want to compute the l2 certificate implied by the certificate given in equation (22). Let ρ2 be the
_l2 certificate so that:_

_∥x[∗]_ _−_ **x∥2 ≤** _ρ2 =⇒∥x[∗]_ _−_ **x∥1 ≤** _ρ1_

Using equation (21), we have the following set of equations:


_∥x[∗]_ _−_ **x∥2 ≤** _ρ2 =_


_∥x[∗]_ _−_ **x∥1 ≤** _ρ2 =⇒∥x[∗]_ _−_ **x∥1 ≤**


_dρ2_


Using equation (22), we have the following:
_√dρ2 ≤_ _ρ1 =⇒_ _ρ2 ≤_ _√[ρ][1]d_

Hence, the l2 norm certificate induced by the l1 norm certificate can be significantly smaller for high
dimensional inputs. Using the l1 certificate used in Levine & Feizi (2021) i.e ρ1 = 4 and d = 32√3

for CIFAR-10 and CIFAR-100, we get an implied certificate of ρ2 = 4/(32√3) = 0.07225. In

contrast, in this work we study l2 robustness for much higher certificate values i.e 36/255 = 0.14118.


D RESULTS USING REVISED LIPSCHITZ CONSTANTS

In Tables 4 and 5, we show results where the Lipschitz constant was computed using product of
Lipschitz constant of all layers. The Lipschitz constant of each layer was computed using direct power
iteration on the linear layer (using 50 iterations) and not using the approximation bound provided in
Singla & Feizi (2021).


-----

|Architecture Methods|Standard Provable Robust Acc. (ρ =) Increase Accuracy 36/255 72/255 108/255 (Standard)|
|---|---|
|SOC + MaxMin + LLN LipConvnet-5 + HH + CR|42.71% 27.86% 17.45% 9.99% _ 45.86% 31.93% 21.17% 13.21% +3.15% 46.36% 32.64% 21.19% 13.12% +3.65% 45.82% 32.99% 22.48% 14.79% +3.11%|
|SOC + MaxMin + LLN LipConvnet-10 + HH + CR|43.72% 29.39% 18.56% 11.16% _ 46.88% 33.32% 22.08% 13.87% +3.16% 47.96% 34.30% 22.35% 14.48% +4.24% 47.07% 34.53% 23.50% 15.66% +3.35%|
|SOC + MaxMin + LLN LipConvnet-15 + HH + CR|42.92% 28.81% 17.93% 10.73% _ 47.72% 33.52% 21.89% 13.76% +4.80% 47.72% 33.97% 22.45% 13.81% +4.80% 47.61% 34.54% 23.16% 15.09% +4.69%|
|SOC + MaxMin + LLN LipConvnet-20 + HH + CR|43.06% 29.34% 18.66% 11.20% _ 46.86% 33.48% 22.14% 14.10% +3.80% 47.71% 34.22% 22.93% 14.57% +4.65% 47.84% 34.77% 23.70% 15.83% +4.78%|
|SOC + MaxMin + LLN LipConvnet-25 + HH + CR|43.37% 28.59% 18.17% 10.85% _ 46.32% 32.87% 21.53% 13.86% +2.95% 47.70% 34.00% 22.67% 14.57% +4.33% 46.87% 34.09% 23.41% 15.61% +3.50%|
|SOC + MaxMin + LLN LipConvnet-30 + HH + CR|42.87% 28.74% 18.47% 11.20% _ 46.18% 32.82% 21.52% 13.52% +3.31% 46.80% 33.72% 22.70% 14.31% +3.93% 46.92% 34.17% 23.21% 15.84% +4.05%|
|SOC + MaxMin + LLN LipConvnet-35 + HH + CR|42.42% 28.34% 18.10% 10.96% _ 45.22% 32.10% 21.28% 13.25% +2.80% 46.21% 32.80% 21.55% 14.13% +3.79% 46.88% 33.64% 23.34% 15.73% +4.46%|
|SOC + MaxMin + LLN LipConvnet-40 + HH + CR|41.84% 28.00% 17.40% 10.28% _ 42.94% 29.71% 19.30% 11.99% +1.10% 45.84% 32.79% 21.98% 14.07% +4.00% 45.03% 32.56% 22.37% 14.76% +3.19%|


Table 4: Results for provable robustness against adversarial examples on the CIFAR-100 dataset. For
all networks in this table, the maximum deviation of the Lipschitz constant from 1 was measured to
be 2.4609 × 10[−][5]. The values shown in red were reduced by 0.01% from the corresponding values
in Table 1 due to the correction in Lipschitz constant. All other values remained unchanged.


-----

|Architecture Methods|Standard Provable Robust Acc. (ρ =) Increase Accuracy 36/255 72/255 108/255 (108/255)|
|---|---|
|SOC + MaxMin LipConvnet-5 + HH + CR|75.78% 59.18% 42.01% 27.09% _ 76.30% 60.12% 43.20% 27.39% +0.30% 75.31% 60.37% 45.62% 32.38% +5.29%|
|SOC + MaxMin LipConvnet-10 + HH + CR|76.45% 60.86% 44.15% 29.15% _ 76.86% 61.52% 44.91% 29.90% +0.75% 76.23% 62.57% 47.70% 34.15% +5.00%|
|SOC + MaxMin LipConvnet-15 + HH + CR|76.68% 61.36% 44.27% 29.66% _ 77.41% 61.92% 45.60% 31.10% +1.44% 76.39% 62.96% 48.47% 35.47% +5.81%|
|SOC + MaxMin LipConvnet-20 + HH + CR|76.90% 61.87% 45.79% 31.08% _ 76.99% 61.76% 45.59% 30.99% -0.09% 76.34% 62.63% 48.68% 36.04% +4.96%|
|SOC + MaxMin LipConvnet-25 + HH + CR|75.24% 60.17% 43.54% 28.60% _ 76.37% 61.50% 44.72% 29.83% +1.23% 75.21% 61.98% 47.93% 34.92% +6.32%|
|SOC + MaxMin LipConvnet-30 + HH + CR|74.51% 59.06% 42.46% 28.04% _ 75.25% 59.90% 43.85% 29.35% +1.30% 74.23% 60.64% 46.51% 34.08% +6.04%|
|SOC + MaxMin LipConvnet-35 + HH + CR|73.73% 58.50% 41.75% 27.20% _ 75.44% 61.05% 45.38% 30.85% +3.65% 74.25% 61.30% 47.60% 35.21% +8.01%|
|SOC + MaxMin LipConvnet-40 + HH + CR|71.63% 54.39% 37.92% 24.13% _ 73.24% 58.12% 42.23% 28.48% +4.35% 72.59% 59.04% 44.92% 32.87% +8.74%|


Table 5: Results for provable robustness against adversarial examples on the CIFAR-10 dataset. For
all networks in this table, the maximum deviation of the Lipschitz constant from 1 was measured to
be 2.2072 × 10[−][5]. The values shown in red were reduced by 0.01% from the corresponding values
in Table 2 due to the correction in Lipschitz constant. All other values remained unchanged.


-----

(a) The output of σθ always lies in the pink (b) In each colored region, the function is linear with the
region. Applying σφ on this where φ ̸= θ + Jacobian mentioned. For the function to be continuous,
2two linear regions (light and original pink).nπ, n ∈ Z further divides this region into we must have{θ0, θ1, θ2, θ3 θ} can be chosen as learnable parameters.3 − _θ2 + θ1 −_ _θ0 = π. Thus, any 3 of_

Figure 2: Constructing Higher Order Householder activations (J[+] and J[−] defined in equation (2))

E VERIFICATION THAT σθ(z1, z2) ALWAYS LIES ON ONE SIDE OF THE
HYPERPLANE


Consider the case: z1 sin (θ/2) − _z2 cos (θ/2) > 0_
In this case σθ(z1, z2) = (z1, z2) and the result follows directly.
Consider the other case: z1 sin (θ/2) − _z2 cos (θ/2) ≤_ 0
_a1_ cos θ sin θ _z1_ _z1 cos θ + z2 sin θ_

= =

a2 sin θ _−_ cos θ z2 z1 sin θ − _z2 cos θ_

_a1 sin (θ/2) −_ _a2 cos (θ/2) = (z1 cos θ + z2 sin θ) sin (θ/2) −_ (z1 sin θ − _z2 cos θ) cos (θ/2)_
= z1 (cos θ sin (θ/2) − sin θ cos (θ/2)) + z2 (sin θ sin (θ/2) + cos θ cos (θ/2))
= −z1 sin (θ/2) + z2 cos (θ/2)

Since z1 sin (θ/2) − _z2 cos (θ/2) ≤_ 0, we have −z1 sin (θ/2) + z2 cos (θ/2) ≥ 0.

F HIGHER ORDER HOUSEHOLDER ACTIVATION FUNCTIONS


We know thatmax(z1, z2) > min( MaxMin(z1, z2z), applying1, z2) = (max( MaxMinz1 again gives the same result i.e, z2), min(z1, z2)) where z MaxMin1, z2 ∈ RMaxMin =. Because
_◦_
MaxMin. Now consider the function σθ (discussed in the maintext, given below for convenience):

1 0 _z1_ if z1 sin (θ/2) _z2 cos (θ/2) > 0_
0 1 _z2_ _−_

_σθ (z1, z2) =_    

 cos θ sin θ _z1_ if z1 sin (θ/2) _z2 cos (θ/2)_ 0

sin θ cos θ _z2_ _−_ _≤_
 _−_   



From Figure 1, we observe that if (u1, u2) = σθ(z1, z2), then (u1, u2) always lies on the right side
of the hyperplane (pink colored region). In other words, u1 sin (θ/2) − _u2 cos (θ/2) > 0 ∀_ _z1, z2_
(proof in Appendix E). This further implies that σθ ◦ _σθ = σθ._

However from Figure 2a in the maintext, we observe that if we use a different angle φ where
_φ ̸= θ + 2nπ for some n ∈_ Z, then σφ(u1, u2) ̸= (u1, u2) for all (u1, u2) in the pink colored region
(u1 sin (θ/2) − _u2 cos (θ/2) > 0). This motivates us to construct the function σ[(][n][)]_ : R[2] _→_ R[2]
defined as follows:

_σ[(][n][)]_ = σθ ◦ _σθ ◦_ _σθ . . . ◦_ _σθ_ (23)
_n times, θ’s can be different_
| {z }

Clearly, σ[(][n][)] has a larger number of linear regions than σθ and thus expected to have more expressive
power. However, a drawback of using σ[(][n][)] is that it requires a sequential application of σθ which can


-----

be expensive if the number of iterations n is large. To address this limitation, first observe that σθ realizes the same linear function for (z1, z2) and (cz1, cz2) when c > 0 i.e ∇(z1,z2) σθ = ∇(cz1,cz2) σθ.
Since σθ is piecewise linear, σθ(cz1, cz2) = cσθ(z1, z2). Thus, the input of the next function in
the iteration is scaled by c as well and its linear function (or the Jacobian) remains unchanged. By
induction, same holds for all the subsequent iterations. By chain rule, the Jacobian of composition
of functions is equal to the product of Jacobian of each individual function. Since the Jacobian
of each function is unchanged on scaling by c > 0, the Jacobian ∇(z1,z2) σ[(][n][)] also remains unchanged: ∇(z1,z2) σ[(][n][)] = ∇(cz1,cz2) σ[(][n][)]. This suggests that it is possible to determine the Jacobian
_∇(z1,z2) σ[(][n][)]_ for the input (z1, z2) by first converting to the polar coordinates ( _z1[2]_ [+][ z]2[2][, ϕ][)][ and]

then using the phase angle ϕ alone (where cos(ϕ) = z1/ _z1[2]_ [+][ z]2[2][,][ sin(][ϕ][) =][ z][2]p[/] _z1[2]_ [+][ z]2[2][).]

This motivates us to construct another GNP piecewise linear activation that only depends on thep p
phase of the input but unlike σθ, it is allowed to have more than 2 linear regions without requiring a
sequential application. This construction is given in the following theorem (example in Figure 2b):

**Theorem 2. Given: 0 ≤** _θ0 < θ1 · · · < θ2n = 2π + θ0 such that_ _i=0_ [(][θ][2][i][+1][ −] _[θ][2][i][) =][ π][ and]_
_αi = 2_ _j=0_ _[θ][i][−][j][(][−][1)][j][, The function][ σ][Θ][ :][ R][2][ →]_ [R][2][ is continuous, GNP and][ 1][-Lipschitz where]
Θ = [θ0, θ1, . . ., θ2n−1] (also called Householder Activation of order[P] n in[n][−] 2[1] _dimensions):_

[P][i]

_σΘ(z1, z2) =_ ( 1)cos[i] sin αi _αi_ ( 1)sin[i][+1] αcosi _αi_ _zz12_ _θi ≤_ _ϕ < θi+1_
 _−_ _−_   


_where ϕ ∈_ [θ0, θ2n = 2π + θ0) and cos(ϕ) = z1/


_z1[2]_ [+][ z]2[2][,][ sin(][ϕ][) =][ z][2][/]


_z1[2]_ [+][ z]2[2][.]


Using the definition of αi, α2n 1 can be computed as follows:
_−_


2n−1 _n−1_ _n−1_

_α2n_ 1 = 2 _θ2n_ 1 _j(_ 1)[j] = 2 (θ2n 1 2j _θ2n_ 2 2j) = 2 (θ2j+1 _θ2j) = 2π_
_−_ _j=0_ _−_ _−_ _−_ _j=0_ _−_ _−_ _−_ _−_ _−_ _j=0_ _−_

X X X

_σΘ(z1, z2) =_ cos α2n−1 sin α2n−1 _z1_ = (z1, z2) _θ2n_ 1 _ϕ < θ2n(= θ0 + 2π)_
− sin α2n−1 cos α2n−1 z2 _−_ _≤_

By continuity, σΘ(z1, z2) = (z1, z2) for ϕ = θ0. Thus if we set θ0 = 0, σΘ is fixed to be identity
when ϕ = 0 (or z2 = 0). However, a learnable θ0 offers the flexibility of choosing arbitrary intervals
around the ϕ = θ0 to be the identity function (while of course allowing θ0 = 0). Since we can choose
any interval of 2π for the phase angle, we choose ϕ ∈ [θ0, θ2n = θ0 + 2π) instead of the usual

[0, 2π) to allow this possibility. We call ( _z1[2]_ [+][ z]2[2][, ϕ][)][, the][ shifted polar coordinates][.]

Additionally, we make the following observations about Theorem 2. First, by constructionp _σΘ has 2n_
linear regions. Second, since _i=0_ [(][θ][2][i][+1][ −] _[θ][2][i][) =][ π][, the sum of angles subtended by linear regions]_
with determinant −1 equals π. This in turn implies that sum of angles subtended by linear regions
with determinant +1 must also equal[P][n][−][1] _π. Third, again using_ _j=0_ [(][θ][2][j][+1][ −] _[θ][2][j][) =][ π][, we know that]_
only 2n − 1 of the 2n parameters in [θ0, θ1, θ2, . . ., θ2n−1] can be chosen independently implying
that σΘ has 2n 1 learnable parameters. In contrast, σ[(][n][)] has only[P][n][−][1] _n learnable parameters. Fourth,_
_−_
when n = 1, σΘ reduces to the original σθ activation.

Because every function of the form σ[(][n][)] (equation (23)) can have potentially 2[n] linear regions, while
_σΘ has only 2n linear regions, σΘ cannot express every function of the form σ[(][n][)]. The primary_
benefit of using σΘ is that it can be easily applied by first determining the angle ϕ (using shifted
polar coordinates), the region [θi, θi+1) to which ϕ belongs and the Jacobian for this region. This
requires 1 multiplication with the Jacobian instead of n required for σ[(][n][)].

G EXTENSION TO HIGHER DIMENSIONS

We introduced Householder activation function of Order 1 in m dimensions in maintext Definition 1.
However, it suffers from the limitation that it has only 2 linear regions thus limiting its expressive
power. The construction given in Appendix Section F allows more than 2 linear regions but is valid
only for 2 dimensional inputs. This motivates us to construct Householder activations that depend on
all the m components of input z ∈ R[m], (m ≥ 3) while allowing for more than 2 linear regions.


-----

A straightforward way of constructing such an activation function is to apply an orthogonal matrix
**Q ∈** R[m][×][m], followed by dividing the output Qz into groups of size 2 each and then applying σθ to
each group. However, since 1-Lipschitz neural networks involve multiplication with an orthogonal
weight matrix followed by GNP activation anyway, this construction is trivial because it does not
lead to additional gains in expressive power over using 2 dimensional σΘ activation functions.

Recall that the function σv is given by the following equation:

**z,** **v[T]** **z > 0,**
_σv(z) =_ (I 2vv[T] )z, **v[T]** **z** 0. (24)
 _−_ _≤_

By a similar analysis as for the 2-dimensional case (Figure 2 in maintext), a repeated application of
_σv leads to increased number of linear regions and thus higher expressive power. This motivates us_
to construct the function σ[(][m,n][)] : R[m] _→_ R[m] by applying the function σv (equation (24)) n times
iteratively with different learnable parameter v at each iteration:

_σ[(][m,n][)]_ = σv ◦ _σv ◦_ _σv . . . ◦_ _σv_
_n times, v’s can be different_
| {z }

Since σv realizes the same linear function for both the inputs z and cz i.e ∇z σv = ∇cz σv when
_c > 0, σ[(][m,n][)]_ satisfies this property as well. This suggests that it is possible to determine the Jacobian
of σ[(][m,n][)] for the given input z by projecting z onto a unit sphere: z/ **z** 2. Moreover, we want our
_∥_ _∥_
constructed function to have at least 2n linear regions while requiring k iterations of σv where k is
independent of n. This motivates the following open question:

**Open Problem. Can non-trivial order-n (n > 1) householder activation functions with 2n linear**
_regions be constructed for m dimensional input (m > 2) using k iterations of σv where k is_
_independent of n (but may depend on m)?_

H ADDITIONAL RESULTS ON CIFAR-10

The rows "BCOP", "Cayley" and "SOC (baseline)" all use the MaxMin activation function. "SOC
+ HH" replaces MaxMin with 2D Householder activation of order 1 (σθ), "+ CR" adds Certificate
Regularization (CR) with γ = 0.1 (while using σθ as the activation function).

In Table 9, the row "SOC + HH[(2)]" uses Householder activation of order 2 (σΘ defined in Theorem
2), "+ CR" adds Certificate Regularization (CR) with γ = 0.1 (while using the HH activation of order
2 i.e σΘ as the activation function).

None of the results in Tables 7, 8 and 9 include Last Layer Normalization (LLN).


-----

|Architecture|Running times (seconds) MaxMin HH|
|---|---|
|LipConvnet-5 LipConvnet-10 LipConvnet-15 LipConvnet-20 LipConvnet-25 LipConvnet-30 LipConvnet-35 LipConvnet-40|3.7864 3.86 5.3677 5.6014 7.234 7.3503 9.536 9.3753 11.0512 11.2692 12.5135 13.6866 14.5539 15.0921 17.1907 17.1928|


Table 6: Inference times for various networks on the complete test dataset of CIFAR-10 with 10000
samples. None of these networks include Last Layer Normalization (LLN).

|Architecture Methods|Standard Provable Robust Acc. (ρ =) Increase Accuracy 36/255 72/255 108/255 (108/255)|
|---|---|
|BCOP Cayley LipConvnet-5 SOC (baseline) SOC + HH + CR|74.25% 58.01% 40.34% 25.21% -1.88% 72.37% 55.92% 38.65% 24.27% -2.82% 75.78% 59.18% 42.01% 27.09% (+0%)|
||76.30% 60.12% 43.20% 27.39% +0.30% 75.31% 60.37% 45.62% 32.38% +5.29%|
|BCOP Cayley LipConvnet-10 SOC (baseline) SOC + HH + CR|74.47% 58.48% 40.77% 26.16% -2.99% 74.30% 57.99% 40.75% 25.93% -3.22% 76.45% 60.86% 44.15% 29.15% (+0%)|
||76.86% 61.52% 44.91% 29.90% +0.75% 76.23% 62.57% 47.70% 34.15% +5.00%|
|BCOP Cayley LipConvnet-15 SOC (baseline) SOC + HH + CR|73.86% 57.39% 39.33% 24.86% -4.80% 71.92% 54.55% 37.67% 23.50% -6.16% 76.68% 61.36% 44.28% 29.66% (+0%)|
||77.41% 61.92% 45.60% 31.10% +1.44% 76.39% 62.96% 48.47% 35.47% +5.81%|
|BCOP Cayley LipConvnet-20 SOC (baseline) SOC + HH + CR|69.84% 52.10% 34.75% 21.09% -9.99% 68.87% 51.88% 35.56% 21.72% -9.36% 76.90% 61.87% 45.79% 31.08% (+0%)|
||76.99% 61.76% 45.59% 30.99% -0.09% 76.34% 62.63% 48.69% 36.04% +4.96%|



Table 7: Results for provable robustness on the CIFAR-10 dataset using shallow networks.
None of these results include Last Layer Normalization (LLN).


-----

|Architecture Methods|Standard Provable Robust Acc. (ρ =) Increase Accuracy 36/255 72/255 108/255 (108/255)|
|---|---|
|BCOP Cayley LipConvnet-25 SOC (baseline) SOC + HH + CR|68.47% 49.92% 31.99% 18.62% -9.98% 64.00% 45.55% 29.24% 16.99% -11.61% 75.24% 60.17% 43.55% 28.60% (+0%)|
||76.37% 61.50% 44.72% 29.83% +1.23% 75.21% 61.98% 47.93% 34.92% +6.32%|
|BCOP Cayley LipConvnet-30 SOC (baseline) SOC + HH + CR|64.11% 43.39% 25.02% 12.15% -15.90% 58.83% 38.68% 22.07% 10.68% -17.37% 74.51% 59.06% 42.46% 28.05% (+0%)|
||75.25% 59.90% 43.85% 29.35% +1.30% 74.23% 60.64% 46.51% 34.08% +6.03%|
|BCOP Cayley LipConvnet-35 SOC (baseline) SOC + HH + CR|63.05% 41.71% 23.30% 11.36% -15.84% 53.55% 32.37% 16.18% 6.33% -20.87% 73.73% 58.50% 41.75% 27.20% (+0%)|
||75.44% 61.05% 45.38% 30.85% +3.65% 74.25% 61.30% 47.60% 35.21% +8.01%|
|BCOP Cayley LipConvnet-40 SOC (baseline) SOC + HH + CR|60.17% 38.86% 21.20% 9.08% -15.05% 51.26% 27.90% 12.06% 3.81% -20.32% 71.63% 54.39% 37.92% 24.13% (+0%)|
||73.24% 58.12% 42.24% 28.48% +4.35% 72.59% 59.04% 44.92% 32.87% +8.74%|


Table 8: Results for provable robustness against adversarial examples on the CIFAR-10 dataset.
None of these results include Last Layer Normalization (LLN).


-----

|Architecture Methods|Standard Provable Robust Acc. (ρ =) Increase Accuracy 36/255 72/255 108/255 (108/255)|
|---|---|
|SOC + HH(2) LipConvnet-5 + CR|75.85% 59.66% 42.68% 27.44% +0.35% 74.85% 60.56% 44.96% 31.98% +4.59%|
|SOC + HH(2) LipConvnet-10 + CR|76.80% 61.44% 44.91% 29.70% +0.55% 76.30% 62.11% 47.85% 34.49% +5.34%|
|SOC + HH(2) LipConvnet-15 + CR|77.41% 62.21% 45.11% 30.49% +0.83% 75.73% 62.21% 47.92% 35.26% +5.60%|
|SOC + HH(2) LipConvnet-20 + CR|76.69% 61.58% 45.39% 30.89% -0.19% 75.72% 62.61% 48.30% 35.29% +4.21%|
|SOC + HH(2) LipConvnet-25 + CR|76.12% 61.24% 44.81% 29.63% +1.03% 75.38% 61.94% 47.67% 34.22% +5.62%|
|SOC + HH(2) LipConvnet-30 + CR|75.09% 60.01% 44.22% 29.39% +1.34% 74.88% 61.23% 46.63% 34.02% +5.97%|
|SOC + HH(2) LipConvnet-35 + CR|73.93% 58.61% 42.29% 28.47% +1.27% 74.14% 60.72% 46.67% 34.64% +7.44%|
|SOC + HH(2) LipConvnet-40 + CR|70.90% 54.96% 38.94% 24.90% +0.77% 72.28% 57.67% 43.00% 30.66% +6.53%|


Table 9: Results for provable robustness on CIFAR-10 using HH activation of Order 2 (σΘ).
Increase (108/255) is calculated with respect to SOC baseline (from Tables 7, 8).
None of these results include Last Layer Normalization (LLN).

|Architecture Methods|Standard Provable Robust Acc. (ρ =) Increase Accuracy (36/255) (72/255) (108/255) (Standard)|
|---|---|
|SOC (no LLN) LipConvnet-5 SOC + LLN|75.78% 59.18% 42.01% 27.09% (+0%) 75.78% 59.58% 42.45% 27.20% +0.00%|
|SOC (no LLN) LipConvnet-10 SOC + LLN|76.45% 60.86% 44.15% 29.15% (+0%) 76.69% 61.08% 44.04% 29.19% +0.24%|
|SOC (no LLN) LipConvnet-15 SOC + LLN|76.68% 61.36% 44.28% 29.66% (+0%) 76.84% 61.94% 45.51% 30.28% +0.16%|
|SOC (no LLN) LipConvnet-20 SOC + LLN|77.05% 61.87% 45.79% 31.08% (+0%) 76.71% 61.44% 44.92% 30.19% -0.34%|
|SOC (no LLN) LipConvnet-25 SOC + LLN|75.24% 60.17% 43.55% 28.60% (+0%) 76.54% 61.21% 44.18% 29.47% +1.30%|
|SOC (no LLN) LipConvnet-30 SOC + LLN|74.51% 59.06% 42.46% 28.05% (+0%) 74.26% 58.97% 41.82% 26.93% -0.25%|
|SOC (no LLN) LipConvnet-35 SOC + LLN|73.73% 58.50% 41.75% 27.20% (+0%) 74.32% 59.05% 42.34% 28.14% +0.59%|
|SOC (no LLN) LipConvnet-40 SOC + LLN|71.63% 54.39% 37.92% 24.13% (+0%) 74.03% 58.27% 41.75% 27.12% +2.40%|



Table 10: Results for provable robustness on the CIFAR-10 dataset with and without LLN


-----

I ADDITIONAL RESULTS ON CIFAR-100

All results in Tables 13, 14 and 15 include Last Layer Normalization (LLN).

The rows "BCOP", "Cayley" and "SOC (baseline)" all use the MaxMin activation function. "SOC
+ HH" replaces MaxMin with 2D Householder activation of order 1 (σθ), "+ CR" adds Certificate
Regularization (CR) with γ = 0.1 (while using σθ as the activation function).

In Table 15, the row "SOC + HH[(2)]" uses Householder activation of order 2 (σΘ defined in Theorem
2), "+ CR" adds Certificate Regularization (CR) with γ = 0.1 (while using the HH activation of order
2 i.e σΘ as the activation function).

|Architecture Methods|Standard Provable Robust Acc. (ρ =) Increase Accuracy 36/255 72/255 108/255 (Standard)|
|---|---|
|SOC + MaxMin + LLN LipConvnet-5 + HH + CR|42.71% 27.86% 17.45% 9.99% _ 45.86% 31.93% 21.17% 13.21% +3.15% 46.36% 32.64% 21.19% 13.12% +3.65% 45.82% 32.99% 22.48% 14.79% +3.11%|
|SOC + MaxMin + LLN LipConvnet-10 + HH + CR|43.72% 29.39% 18.56% 11.16% _ 46.88% 33.32% 22.08% 13.87% +3.16% 47.96% 34.30% 22.35% 14.48% +4.24% 47.07% 34.53% 23.50% 15.66% +3.35%|



Table 11: Results for provable robustness against adversarial examples on the CIFAR-100 dataset.

|Architecture|Running times (in seconds) MaxMin (no LLN) MaxMin (LLN) HH (LLN)|
|---|---|
|LipConvnet-5 LipConvnet-10 LipConvnet-15 LipConvnet-20 LipConvnet-25 LipConvnet-30 LipConvnet-35 LipConvnet-40|3.7568 3.5002 3.6673 5.3714 5.5482 5.5533 7.3092 7.2595 7.3127 9.005 9.2043 9.308 10.9321 10.7868 11.726 12.3198 13.2168 13.6275 14.427 14.575 15.7069 16.0911 16.2535 17.1015|



Table 12: Inference times for various networks on the CIFAR-100 test dataset. Similar to CIFAR-10
(in Table 6), these numbers are for the whole test dataset with 10000 samples.


-----

|Architecture Methods|Standard Provable Robust Acc. (ρ =) Increase Accuracy 36/255 72/255 108/255 (108/255)|
|---|---|
|BCOP Cayley LipConvnet-5 SOC (baseline) SOC + HH + CR|46.31% 31.55% 20.34% 12.52% -0.69% 44.61% 31.01% 19.84% 12.43% -0.78% 45.86% 31.93% 21.17% 13.21% (+0%)|
||46.36% 32.64% 21.19% 13.12% -0.09% 45.82% 32.99% 22.48% 14.79% +1.58%|
|BCOP Cayley LipConvnet-10 SOC (baseline) SOC + HH + CR|45.36% 31.71% 20.48% 12.40% -1.47% 45.79% 31.91% 20.69% 12.78% -1.09% 46.88% 33.32% 22.08% 13.87% (+0%)|
||47.96% 34.30% 22.35% 14.48% +0.61% 47.07% 34.53% 23.50% 15.66% +1.79%|
|BCOP Cayley LipConvnet-15 SOC (baseline) SOC + HH + CR|43.70% 30.11% 19.85% 12.29% -1.47% 45.05% 31.60% 20.32% 12.93% -0.83% 47.72% 33.52% 21.89% 13.76% (+0%)|
||47.72% 33.97% 22.45% 13.81% +0.05% 47.61% 34.54% 23.16% 15.09% +1.33%|
|BCOP Cayley LipConvnet-20 SOC (baseline) SOC + HH + CR|39.77% 27.20% 17.44% 10.49% -3.61% 39.68% 26.93% 17.06% 10.48% -3.62% 46.86% 33.48% 22.14% 14.10% (+0%)|
||47.71% 34.22% 22.93% 14.57% +0.47% 47.84% 34.77% 23.70% 15.84% +1.74%|


Table 13: Results for provable robustness on the CIFAR-100 dataset using shallow networks.
All of these results include Last Layer Normalization (LLN).


-----

|Architecture Methods|Standard Provable Robust Acc. (ρ =) Increase Accuracy 36/255 72/255 108/255 (108/255)|
|---|---|
|BCOP Cayley LipConvnet-25 SOC (baseline) SOC + HH + CR|34.15% 21.57% 13.52% 7.97% -5.89% 33.93% 21.93% 13.68% 8.19% -5.67% 46.32% 32.87% 21.53% 13.86% (+0%)|
||47.70% 34.00% 22.67% 14.57% +0.71% 46.87% 34.09% 23.41% 15.61% +1.75%|
|BCOP Cayley LipConvnet-30 SOC (baseline) SOC + HH + CR|29.73% 18.69% 10.80% 6% -7.52% 28.67% 18.05% 10.43% 6.09% -7.43% 46.18% 32.82% 21.52% 13.52% (+0%)|
||46.80% 33.72% 22.70% 14.31% +0.79% 46.92% 34.17% 23.21% 15.84% +2.32%|
|BCOP Cayley LipConvnet-35 SOC (baseline) SOC + HH + CR|25.65% 14.88% 8.47% 4.30% -8.95% 27.75% 16.37% 9.52% 5.40% -7.85% 45.22% 32.10% 21.28% 13.25% (+0%)|
||46.21% 32.80% 21.55% 14.13% +0.88% 46.88% 33.64% 23.34% 15.73% +2.48%|
|BCOP Cayley LipConvnet-40 SOC (baseline) SOC + HH + CR|30.66% 18.68% 10.46% 5.92% -6.07% 25.54% 14.91% 8.37% 4.40% -7.59% 42.94% 29.71% 19.30% 11.99% (+0%)|
||45.84% 32.79% 21.98% 14.07% +2.08% 45.03% 32.57% 22.37% 14.76% +2.77%|


Table 14: Results for provable robustness on the CIFAR-100 dataset using deeper networks.
All of these results include Last Layer Normalization (LLN).


-----

|Architecture Methods|Standard Provable Robust Acc. (ρ =) Increase Accuracy 36/255 72/255 108/255 (108/255)|
|---|---|
|SOC + HH(2) LipConvnet-5 + CR|46.61% 32.50% 21.34% 13.22% +0.01% 46.69% 33.22% 22.34% 14.30% +1.09%|
|SOC + HH(2) LipConvnet-10 + CR|47.47% 33.32% 21.84% 13.75% -0.01% 47.53% 34.52% 23.06% 15.07% +1.31%|
|SOC + HH(2) LipConvnet-15 + CR|47.19% 33.67% 22.36% 13.78% -0.09% 47.22% 34.04% 22.98% 15.28% +1.41%|
|SOC + HH(2) LipConvnet-20 + CR|47.86% 33.93% 22.44% 14.41% +0.31% 47.54% 34.32% 23.53% 15.54% +1.44%|
|SOC + HH(2) LipConvnet-25 + CR|47.86% 33.97% 22.78% 14.59% +0.73% 47.50% 34.38% 23.92% 15.92% +2.06%|
|SOC + HH(2) LipConvnet-30 + CR|46.23% 32.64% 21.95% 14.00% +0.48% 46.36% 33.20% 22.70% 14.85% +1.33%|
|SOC + HH(2) LipConvnet-35 + CR|46.06% 32.35% 21.33% 13.65% +0.40% 45.78% 33.24% 22.39% 14.78% +1.53%|
|SOC + HH(2) LipConvnet-40 + CR|43.81% 30.59% 20.08% 12.56% +0.57% 45.61% 32.50% 22.36% 14.84% +2.85%|


Table 15: Results for provable robustness on CIFAR-100 using HH activation of Order 2 (σΘ).
Increase (108/255) is calculated with respect to SOC baseline (from Tables 13, 14).
All of these results include Last Layer Normalization (LLN).

|Architecture Methods|Standard Provable Robust Acc. (ρ =) Increase Accuracy (36/255) (72/255) (108/255) (Standard)|
|---|---|
|SOC (no LLN) LipConvnet-5 SOC + LLN|42.71% 27.86% 17.45% 9.99% (+0%) 45.86% 31.93% 21.17% 13.21% +3.15%|
|SOC (no LLN) LipConvnet-10 SOC + LLN|43.72% 29.39% 18.56% 11.16% (+0%) 46.88% 33.32% 22.08% 13.87% +3.16%|
|SOC (no LLN) LipConvnet-15 SOC + LLN|42.92% 28.81% 17.93% 10.73% (+0%) 47.72% 33.52% 21.89% 13.76% +4.80%|
|SOC (no LLN) LipConvnet-20 SOC + LLN|43.06% 29.34% 18.66% 11.20% (+0%) 46.86% 33.48% 22.14% 14.10% +3.80%|
|SOC (no LLN) LipConvnet-25 SOC + LLN|43.37% 28.59% 18.18% 10.85% (+0%) 46.32% 32.87% 21.53% 13.86% +2.95%|
|SOC (no LLN) LipConvnet-30 SOC + LLN|42.87% 28.74% 18.47% 11.21% (+0%) 46.18% 32.82% 21.52% 13.52% +3.31%|
|SOC (no LLN) LipConvnet-35 SOC + LLN|42.42% 28.34% 18.10% 10.96% (+0%) 45.22% 32.10% 21.28% 13.25% +2.80%|
|SOC (no LLN) LipConvnet-40 SOC + LLN|41.84% 28.00% 17.40% 10.28% (+0%) 42.94% 29.71% 19.30% 11.99% +1.10%|



Table 16: Results for provable robustness on the CIFAR-100 dataset with and without LLN


-----

