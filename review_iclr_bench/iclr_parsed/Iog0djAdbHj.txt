# BETTER SUPERVISORY SIGNALS
## BY OBSERVING LEARNING PATHS


**Yi Ren**
UBC
renyi.joshua@gmail.com


**Shangmin Guo**
University of Edinburgh
s.guo@ed.ac.uk

ABSTRACT


**Danica J. Sutherland**
UBC and Amii
dsuth@cs.ubc.ca


Better-supervised models might have better performance. In this paper, we first
clarify what makes for good supervision for a classification problem, and then
explain two existing label refining methods, label smoothing and knowledge distillation, in terms of our proposed criterion. To further answer why and how better supervision emerges, we observe the learning path, i.e., the trajectory of the
model’s predictions during training, for each training sample. We find that the
model can spontaneously refine “bad” labels through a “zig-zag” learning path,
which occurs on both toy and real datasets. Observing the learning path not only
provides a new perspective for understanding knowledge distillation, overfitting,
and learning dynamics, but also reveals that the supervisory signal of a teacher
network can be very unstable near the best points in training on real tasks. Inspired by this, we propose a new knowledge distillation scheme, Filter-KD, which
improves downstream classification performance in various settings.

1 INTRODUCTION

In multi-class classification problems, we usually supervise our model with “one-hot” labels: label
vectors y which have yi = 1 for one i, and 0 for all other dimensions. Over time, however, it
has gradually become clear that this “default” setup is not always the best choice in practice, in
that other schemes can yield better performance on held-out test sets. One such alternative is to
summarize a distribution of human annotations, as Peterson et al. (2019) did for CIFAR10. An
alternative approach is label smoothing (e.g. Szegedy et al., 2016), mixing between a one-hot label
and the uniform distribution. Knowledge distillation (KD), first training a teacher network on the
training set and then a student network on the teacher’s output probabilities, was originally proposed
for model compression (Hinton et al., 2015) but can also be thought of as refining the supervision
signal: it provides “soft” teacher outputs rather than hard labels to the student.

Knowledge distillation is promising because it requires no additional annotation effort, but – unlike
label smoothing – can still provide sample-specific refinement. Perhaps surprisingly, knowledge
distillation can improve student performance even when the teacher is of exactly the same form as
_the student and trained on the same data; this is known as self-distillation (Furlanello et al., 2018;_
Zhang et al., 2019). There have been many recent attempts to explain knowledge distillation and
specifically self-distillation (e.g. Menon et al., 2021; Allen-Zhu & Li, 2020; Tang et al., 2020), from
both optimization and supervision perspectives. We focus on the latter area, where it is usually
claimed that the teacher provides useful “dark knowledge” to the student through its labels.

Inspired by this line of work, we further explore why and how this improved supervisory signal
emerges during the teacher’s one-hot training. Specifically, we first clarify that given any input
sample x, good supervision signals should be close (in L2 distance) to the ground truth categorical
distribution, i.e., p[∗](y | x). We then show that a neural network (NN) can automatically refine “bad
labels”, those where p[∗](y | x) is far from the training set’s one-hot vector.[1] During one-hot training,
the model prediction on such a sample first moves towards p[∗](y | x), and then slowly converges to
its supervisory label, following a “zig-zag” pattern. A well-trained teacher, one that does not overfit

1This might be because x is ambiguous (perhaps p∗(y | x) is flat, or we simply got a sample from a lesslikely class), or because the one-hot label has been corrupted through label noise or otherwise is “wrong.”


-----

to particular training labels, can thus provide supervisory signals closer to p[∗](y | x). We justify
analytically (Section 3.3) that this pattern is common in gradient descent training. Our explanations
cause us to recognize that this signal can be better identified by taking a moving average of the
teacher’s prediction, an algorithm we term Filter-KD. This approach yields better supervision and
hence better downstream performance, especially when there are many bad labels.

_After completing this work, we became aware of an earlier paper (Liu et al., 2020) studying almost_
_the same problem in a similar way. We discuss differences between the papers throughout._

2 SUPERVISION INFLUENCES GENERALIZATION

We begin by clarifying how the choice of supervisory signal affects the learned model.

2.1 CHOICES OF SUPERVISION SIGNAL

In K-way classification, our goal is to learn a mapping f : X → ∆[K] that can minimize the risk


_R(f_ ) ≜ E
(x,y) P[[][L][(][y, f] [(][x][))] =]
_∼_


_p(x) p(y|x) L(y, f_ (x)) dx dy, (1)


Here the label y is an integer ranging from 1 to K, and f gives predictions in the probability simplex
∆[K] (a nonnegative vector of length K summing to one); L(y, f (x)) is the loss function, e.g. crossentropy or square loss. The input signal x is usually high dimensional, e.g., an image or a sequence of
word embeddings. The joint distribution of (x, y) is P, whose density[2] can be written as p(x)p(y|x).
In practice, as P is unknown, we instead (approximately) minimize the empirical risk


_Remp(f, D) ≜_


1

_N_ [1][(][y][n][ =] _[k][)][ L][(][k, f]_ [(][x][n][)) =]

_n=1_

X


1

_yn_ **[L][(][f]** [(][x][n][))][,] (2)
_N_ **[e][T]**


_n=1_


_k=1_


where{0, 1} 1[K] (is its one-hot vector form.yn = k) is an indicator function which equals L(f (xn)) = (L(1, f (x 1n if)), . . ., L yn = k(K, f or 0(x otherwise, andn))) ∈ R[K] is the loss eyn ∈
for each possible label. In Remp, the N training pairs D ≜ _{(xn, yn)}n[N]=1_ [are sampled i.i.d. from][ P][.]

Comparing (2) to (1), we can see p(x) is approximated by an uniform distribution over the samples,
which is reasonable. However, using an indicator function (i.e., one-hot distribution) to approximate
_p(y | x) bears more consideration. For example, if a data point x is quite vague and its true p(y|x)_
is flat or multimodal, we might hope to see x multiple times with different label y during training.
But actually, most datasets have only one copy of each x, so we only ever see one corresponding ey.
Although Remp is an unbiased estimator for R, if we used a better (e.g. lower-variance) estimate of
_p(y | x), we could get a better estimate for R and thus, hopefully, better generalization._

Specifically, suppose we were provided a “target” distribution ptar(y | x) (written in vector form as
**ptar(x)) for each training point x, as** = (xn, ptar(xn)) _n=1[. Then we could use]_
_D[′]_ _{_ _}[N]_


_Rtar(f, D[′]) ≜_


1

(3)
_N_ **[p][tar][(][x][n][)][T][L][(][f]** [(][x][n][))][.]


_N_ _[p][tar][(][y][n][ =][ k][ |][ x][n][)][ L][(][k, f]_ [(][x][n][)) =]

_n=1_

X


_n=1_


_k=1_


Standard training with Remp is a special case of Rtar, using ptar(xn) = eyn . The CIFAR10H dataset
(Peterson et al., 2019) is one attempt at a different ptar, using multiple human annotators to estimate
**ptar. Label smoothing (e.g. Szegedy et al., 2016) sets ptar to a convex combination of ey and the**
constant vector _K[1]_ **[1][. In knowledge distillation (KD;][ Hinton et al.][,][ 2015][), a teacher is first trained on]**

_D, then a student learns from D[′]_ with ptar based on the teacher’s outputs. All three approaches yield
improvements over standard training with Remp.

2Because we are working with classification problems, we use densities with respect to a product of some
arbitrary measure on x (probably Lebesgue) with counting measure on y, and assume that these densities exist
for notational convenience. None of our arguments will depend on the choice of base measure.


-----

60 Class0 0.840 0.05 Noisy p

40200 Class1Class2 0.8380.836 Noisy p 0.040.03 OHTLSGTKDESKD

20 0.834 OHTLS 0.02

40 Accuracy on test set0.832 GTKD ECE on test set0.01

60 0.830 ESKD 0.00

60 40 20 0 20 40 60 0 5L2-distance of p_tar and p*10 15 20 25 30 0 L2-distance of p_tar and p*10 20 30 40 50

(b) ACC vs **ptar** **p[∗]** 2
_∥_ _−_ _∥_


0 L2-distance of p_tar and p*10 20 30 40

(c) ECE v.s. **ptar** **p[∗]** 2
_∥_ _−_ _∥_


(a) tSNE of toy dataset.


Figure 1: Experiments on a toy dataset when learning from different ptar. In (b-c), the horizontal
axis represents **ptar** **p[∗]** 2, and the vertical axis is the generalization performance. OHT means
one-hot training (on ∥ _R −emp), LS means label smoothing, GT means ground truth training with∥_ **p[∗], KD**
is knowledge distillation, and ESKD is early-stopped KD. The Spearman correlation coefficient for
results in (b) is -0.930 with p-value 1.9 × 10[−][53]; for (c) is 0.895 with p-value 2.7 × 10[−][43].

2.2 MEASURING THE QUALITY OF SUPERVISION

Choosing a different ptar, then, can lead to a better final model. Can we characterize which ptar will
do well? We propose the following, as a general trend.

**Hypothesis 1. Suppose we train a model supervised by ptar, that is, we minimize Rtar(f,** ). Then,
_D[′]_
_smaller average L2 distance between ptar and the ground truth p[∗]_ _on these samples, i.e. small_
Ex [ **ptar(x)** **p[∗](x)** 2], will in general lead to better generalization performance.
_∥_ _−_ _∥_

This hypothesis is suggested by Proposition 3 of Menon et al. (2021), which shows (tracking constants omitted in their proof) that for any predictor f and loss bounded as L(y, ˆy) ≤ _ℓ,_

2

E (Rtar(f, ) _R(f_ ))[2][] **p[T]tar[L][(][f]** [(][x][))] + ℓ[2]K E _._ (4)
_D[′]_ _−_ _≤_ _N[1]_ [Var]x **x**
_D[′]_     _[∥][p][tar][(][x][)][ −]_ **[p][∗][(][x][)][∥][2]**

When N is large, the second term will dominate the right-hand side, implying smaller average
**ptar** **p[∗]** will lead to Rtar being a better approximation of the true risk R; minimizing it should
_∥then lead to a better learned model. This suggests that the quality of the supervision signal can be −_ _∥_
roughly measured by its L2 distance to the ground truth p[∗]. Appendix B slightly generalizes the
result of Menon et al. with bounds based on total variation (L1) and KL divergences; we focus on
the L2 version here for simplicity.

To further support this hypothesis, we conduct experiments on a synthetic Gaussian problem (Figure 1 (a); details in Appendix C), where we can easily calculate p[∗](y | x) for each sample. We
first generate several different ptar by adding noise[3] to the ground truth p[∗], then train simple 3-layer
NNs under that supervision. We also show five baselines: one-hot training (OHT), label smoothing
(LS), KD, early-stopped KD (ESKD), and ground truth (GT) supervision (using p[∗]). KD refers to a
teacher trained to convergence, while ESKD uses a teacher stopped early based on validation accuracy. We early-stop the student’s training in all settings. From Figure 1 (b-c), it is clear that smaller
**ptar** **p[∗]** 2 leads to better generalization performance, as measured either by accuracy (ACC) or
_∥expected calibration error (ECE) −_ _∥_ [4] on a held-out test set. Appendix C has more detailed results.

3 INSIGHTS FROM THE LEARNING PATH

In the toy example of Section 2, we see that ESKD outperforms other baselines in accuracy by
a substantial margin (and all baselines are roughly tied in ECE). We expect that supervision with
smaller **ptar** **p[∗]** 2 leads to better generalization performance, but it is not clear how better ptar
emerges from when the teacher in ESKD is trained using one-hot labels. This section will answer ∥ _−_ _∥_
this, by observing the learning paths of training samples.

3Whenever we mention adding noise to p∗, we mean we add independent noise to each dimension, and then
re-normalize it to be a distribution. Large noise can thus flip the “correct” label.
4ECE measures the calibration of a model (Guo et al., 2017). Briefly, lower ECE means the model’s confidence in its predictions is more accurate. See Appendix A for details.


-----

Figure 2: Normalized (divided by _√2) distance between output distribution q and p[∗]_ during the

one-hot training in different stages (left to right: initial, early stop, convergence). In these figures,
results about the NNs trained under different supervisions using this fashion in AppendixEx ∥q(x) − **p[∗](x)∥2 of Hypothesis 1 is the mean height of all points in the figure. We provide more F.**

3.1 PAY MORE ATTENTION TO HARDER SAMPLES

For a finer-grained understanding of early stopping the teacher, we would like to better understand how the teacher’s predictions evolve in training. Assuming Hypothesis 1, the main factor
is Ex∥q(x) − **p[∗](x)∥2, where q is the teacher’s output probability distribution. We expect, though,**
that this term will vary for different x, in part because some samples are simply more difficult to
learn. As a proxy for this, we define base difficulty as ∥ey − **p[∗](x)∥2, which is large if:**

-  x is ambiguous: p[∗] has several large components, so there is no one-hot label near p[∗].

-  x is not very ambiguous (there is a one-hot label near p[∗]), but the sample was “unlucky” and
drew y from a low-probability class.

Figure 2 shows these two quantities at three points in training: initialization, the point where ESKD’s
teacher stops, and convergence. At initialization, most points[5] have large **q(x)** **p[∗](x)** 2. By the
_∥_ _−_ _∥_
point of early stopping, most q(x) values are roughly near p[∗]. At convergence, however, q(x) **ey,**
_≈_
as the classifier has nearly memorized the training inputs, leading to a diagonal line in the plot.

It is clear the biggest contributors to Ex∥q(x) − **p[∗](x)∥2 when training a model to convergence are**
the points with high base difficulty. Per Hypothesis 1, these are the points we should most focus on.

3.2 LEARNING PATH OF DIFFERENT SAMPLES

To better understand how q changes for each sample, we track the model’s outputs on all training
samples at each training step. Figure 3 shows four samples with different base difficulty, with the
vectors of three probabilities plotted as points on the simplex (details in Appendix A).

The two easy samples very quickly move to the correct location near ey (as indicated by the light
color until reaching the corner). The medium sample takes a less direct route, drifting off slightly towards p[∗], but still directly approaches ey. The hard sample, however, does something very different:
it first approaches p[∗], but then veers off towards ey, giving a “zig-zag” path. In both the medium
and hard cases, there seems to be some “unknown force” dragging the learning path towards p[∗]; in
both cases, the early stopping point is not quite perfect, but is noticeably closer to p[∗] than the final
converged point near ey. In other words, during one-hot training, the NNs can spontaneously refine
_the “bad labels.” Under Hypothesis 1, this partly explains the superior performance of ESKD to_
KD with a converged teacher.[6]

These four points are quite representative of all samples under different toy-dataset settings. In
Appendix E, we also define a “zig-zagness score” to numerically summarize the learning path shape,
and show it is closely correlated to base difficulty.

5The curve structure is expected: points with p∗ _≈_ ( 13 _[,][ 1]3_ _[,][ 1]3_ [)][ are near the middle of the base difficulty]

range, and all points are initialized with fairly ambiguous predictions q.
6KD’s practical success is also related to the temperature and optimization effects, among others; better
supervision is not the whole story. We discuss the effect of various hyperparameters in Appendix D.


-----

Figure 3: Learning path of samples with different base difficulty. Corners correspond to one-hot
vectors. Colors represent training time: transparent at initialization, dark blue at the end of training.

3.3 EXPLANATION OF PATTERNS IN THE LEARNING PATH

The following decomposition will help explain the “unknown force” pushing q towards p[∗].

**Proposition 1. Let z[t](x) ≜** _f_ (w[t], x) denote the network output logits with parameters w[t], and
**q[t](x) = Softmax(z[t](x)) the probabilities. Let w[t][+1]** ≜ **w[t]** _−_ _η ∇w_ **ptar(xu)[T]L(q[t](xu))** _be the_
_result of applying one step of SGD to w[t]_ _using the data point (xu, ptar(xu)) with learning rate η._
  
_Then the change in network predictions for a particular sample xo is_

**q[t][+1](xo)** **q[t](xo) = η** (xo) (xo, xu) **ptar(xu)** **q[t](xu)** + (η[2] **wz(xu)** op[)][,]
_−_ _A[t]_ _K[t]_ _−_ _O_ _∥∇_ _∥[2]_

_where_ (xo) = **zq[t](xo) and** (xo, xu) = ( **w z(xo)** **wt** ) ( **wz(xu)** **wt** )[T] _are K_ _K matrices._
_A[t]_ _∇_ _K[t]_ _∇_ _|_ _∇_ _|_ _×_

The matrix K[t] is the empirical neural tangent kernel, which we can think of roughly as a notion of
“similarity” between xo and xu based on the network’s representation at time t, which can change
during training in potentially complex ways. In very wide networks, though, K[t] is nearly invariant
throughout training and nearly independent of the initialization (Jacot et al., 2018; Arora et al.,
2019). The gradient norm can be controlled by gradient clipping, or bounded with standard SGD
analyses when optimization doesn’t diverge. Appendix G has more details and the proof.

Figure 4 shows the learning path of a hard sample during one-hot training, say xo, where the label
(the small blue path), and one update based oneyo is far from p[∗](xo). In each epoch, w will receive xo (the big red path). At any time N − 1 updates based on the loss of t, “dissimilar” xu ̸= xo
samples will have small (xo, xu) (as measured, e.g., by its trace), and hence only slightly affect
_K[t]_
the predicted label for xo. Similar samples will have large (xo, xu), and hence affect its updates
_K[t]_
much more; because p[∗] is hopefully similar for similar x values, it is reasonable to expect that the
mean of ey for data points with similar x will be close to p[∗](xo). Thus the net effect of updates for
**xu ̸= xo should be to drag q(xo) towards p[∗](xo). This is the “unknown force” we observed earlier.**

The other force affecting q(xo) during training
is, of course, the update based on xo, driving
balance over the course training?q(xo) towards eyo . How do these two forcesEarly on,
any∥eyu x −u, because near initializationq[t](xu)∥2 is relatively large for nearly q[t](xu) will
be relatively flat. Hence the size of the updates
for “similar” xu and the update for xo should
be comparable, meaning that, if there are at
least a few “similar” training points, q[t](xo)
will move towards p[∗](xo). Throughout training, some of these similar samples will become
well-classified, so thatcomes small, and their updates will no longer ∥eyu − **q[t](xu)∥2 be-**
exert much force on q(xo). Thus, the xo updates begin to dominate, causing the zig-zag Figure 4: Updates of q(xo) over training.
pattern as the learning path turns towards eyo .
For easy samples, where p[∗] and eyo are in the same direction, these forces agree and lead to fast
convergence. On samples like the “medium” point in Figure 3, the two forces broadly agree early
on, but the learning path deviates slightly towards p[∗] en route to eyo .


-----

Figure 5: Learning path on CIFAR10. In the first two panels we record q[t] for each batch, while in
the last panel we record it for each epoch. See Appendix E for the learning paths of more samples.

Liu et al. (2020) prove that a similar pattern occurs while training a particular linear model on data
with some mislabeled points, and specifically that stopping training at an appropriate early point
will give better predictions than continuing training.

4 LEARNING PATHS ON REAL TASKS

Our analysis in Sections 2 and 3 demonstrate that the model can spontaneously refine the bad labels
during one-hot training: the zig-zag learning path first moves towards the unknown p[∗]. But is this
also true on real data, for more complex network architectures? We will now show that they do,
although seeing the patterns requires a little more subtlety.

We visualize the learning path of data points while training a ResNet18 (He et al., 2016) on CIFAR10
for 200 epochs as an example. The first panel of Figure 5 shows the learning path of an easy sample.[7]
We can see that q[t] converges quickly towards the left corner, the one-hot distribution for the correct
class, because the color of the scattered points in that figure are quite light. At the early stopping
epoch, q[t] has already converged to ey. However, for a hard sample, it is very difficult to observe
any patterns from the raw path (blue points): there are points almost everywhere in this plot. This
is likely caused by the more complex network and dataset, as well as a high learning rate in early
training. To find the hidden pattern in this high-variance path, we treat q[t] as a time series signal with
many high-frequency components. Thus we can collect its low-frequency components via a lowpass filter and then draw that, i.e., taking a exponential moving average (EMA) on q[t] (red points).
This makes it clear that, overall, the pattern zig-zags, first moving towards the unknown true label
before eventually turning to memorize the wrong label.

This filtering method not only helps us observe the zig-zag pattern, but can also refine the labels.
We use the following two experiments to verify this. First, we train the model on CIFAR10H using
one-hot labels. As CIFAR10H provides phum, which can be considered as an approximation of p[∗],
we track the mean distance between q and phum during training. In the first panel of Figure 6, which
averages the distance of all 10k training samples, the difference between the blue qkd curve and red
**qfilt curve is quite small. However, we can still observe that the red curve is lower than the blue**
one before overfitting: filtering can indeed refine the labels. This trend is more significant when
considering the most difficulty samples, as in the second panel: the gap between curves is larger.

To further verify the label refinement ability of filtering, we randomly choose 1,000 of the 50,000
CIFAR10 training samples, and flip their labels to a different y[′] ≠ _y. The last panel in Figure 6 tracks_
how often the most likely prediction of the network, argmax q[t], recovers the true original label,
rather than the provided random label, for the corrupted data points. At the beginning of the model’s
training, the initialized model randomly guesses labels, getting about 10% of the 1,000 flipped labels
correct. During training, the model spontaneously corrects some predictions, as the learning path
first moves towards p[∗]. Eventually, though, the model memorizes all its training labels. Training
with the predictions from the 400th epoch corresponds to standard KD (no corrected points). Early
stopping as in ESKD would choose a point with around 70% of labels corrected. The filtered path,
though, performs best, with over 80% corrected.

7Without knowing p∗, we instead use zig-zag score – see Appendix E – to measure the difficulty.


-----

Figure 6: Filtering can refine the labels in both clean and noisy label case.

Figure 7: Test accuracy under different noise ratio σ. Solid lines are the means while shade region
are the standard errors for 3 runs with different random seeds (shaded range is the standard error).
Last panel compare the influence of different temperatures. Each thin rectangle plot represents a
different σ = {0, 0.1, ..., 0.8}, in which we plot the results with different τ = {0.5, 1, 2, 4, 10}.

5 FILTERING AS A METHOD FOR KNOWLEDGE DISTILLATION

Figure 6 gives a clear motivation for a new knowledge distillation method, which we call Filter-KD
(Algorithm 1): train the student from the smoothed predictions of the teacher network. Specifically,
we maintain a look-up tablesample. Note that in one epoch, each qsmooth ∈ qRsmooth[N] _[×][K](xnto store a moving average of) will be updated only once. We check the early qt for each training_
stopping criterion with the help of a validation set. Afterwards, the teaching supervision qsmooth is
ready, and we can train a student network under its supervision. This corresponds to using a moving
average of the teacher model “in function space,” i.e. averaging the outputs of the function over time.

Compared to ESKD, Filter-KD can avoid the extremely high variance of q[t] during training. Unlike
Figure 5, which plots q[t] after every iteration of training, most practical implementations only consider early-stopping at the end of each epoch. This is equivalent to down-sampling the noisy learning
path (as in the last panel of Figure 5), further exacerbating the variance of q[t]. Thus ESKD will likely
select a bad ptar for many data points. Filter-KD, by contrast, has much more stable predictions.

We will show the effectiveness of this algorithm shortly, but first we discuss its limitations. Compared to ESKD, the running time of Filter-KD might be slightly increased. Furthermore, compared
to the teaching model in ESKD, the Filter-KD requires a teaching table qsmooth, which will require
substantial memory when the dataset is large. One alternative avoiding the need for this table would
be to instead take an average “in parameter space,” like e.g. momentum parameter updating as in
Szegedy et al. (2015). We empirically find that, although this helps the model converge faster, it does
not lead to a better teacher network; see Appendix I. Thus, although Filter-KD has clear drawbacks,
we hope that our explanations here may lead to better practical algorithms in the future.

Similarly inspired by this spontaneous label refining mechanism (or early stopping regularization),
Liu et al. (2020) and Huang et al. (2020) each propose algorithms aiming at the noisy label problem.
We discuss the relationship between these three methods in Appendix H.

5.1 QUANTITATIVE RESULTS OF FILTER-KD

We now compare the performance of Filter-KD and other baselines on a real dataset. We focus on
self-distillation and a fixed temperature τ = 1 (except Table 2 and last panel in Figure 7), as we


-----

|Col1|Eff Res Res Mob Res VGG → → →|
|---|---|
|Teacher Student ESKD FilterKD|86.83 77.23 77.98 78.09 72.58 71.04 81.16 73.13 72.64 83.03 75.79 74.49|


Table 2: Teacher→student, on CIFAR100.


|Noise σ|0% 5% 10% 20%|
|---|---|
|OHT ESKD FilterKD|56.95 53.02 52.02 30.52 58.61 53.53 52.99 36.55 59.32 56.43 55.51 40.81|


Table 1: Results on TinyImageNet dataset.


want the only difference among these methods to be ptar. Thus we can conclude the improvement
we achieved comes purely from the refined supervision. See Appendix D for other temperatures.

|Col1|Accuracy OHT KD ESKD FilterKD|ECE OHT KD ESKD FilterKD|
|---|---|---|


|CIFAR10 CIFAR100|95.34 95.39 95.42 95.63 78.07 78.40 78.83 80.09|0.026 0.027 0.027 0.007 0.053 0.061 0.067 0.029|
|---|---|---|



Table 3: Quantitative comparison of generalization performance for ResNet18 self-distillation
(mean value of 5 runs). Refer to Table 6 for detailed results.

**Input: Dataset** (xn, yn) _n=1[, where][ I][n]_ [=][ {][1][,][ 2][, ..., N] _[}][ is the index for each pair]_
_{_ _}[N]_

_# Train the teacher_
Initialize a network model, initialize an N _K matrix called qsmooth_
_×_
Go through the entire dataset, calculate qsmooth[n] = Softmax(model(xn))
**for epoch = 1, 2, ..., T do**

**for n ∈{1, 2, . . ., N** _} in random order do_

_pˆ = Softmax(model(xn))_
**qsmooth[n] = (1** _α)_ **qsmooth[n] + α** ˆp
_−_ _·_ _·_

Update parameters based on loss = CrossEntropy(p, yˆ _n),_

**end for**
Check the early stopping criterion; stop training if satisfied

**end for**
_# Train the student_
For each input xn, set ptar = qsmooth[n]; train the network under the supervision of ptar


**Algorithm 1: Filter-KD. α controls the cut-off frequency of low-pass filter (0.05 here).**

The first task we consider is noisy-label classification, where we train and validate the model on a
dataset with some labels randomly flipped. After training, all the models are evaluated on a clean
held-out test set. The experiments are conducted on CIFAR (Figure 7) and TinyImageNet (Table 1),
under different noise ratios σ: σ = 0.1 means 10% of the labels are flipped. In Figure 7, an
interesting trend can be observed: the enhancement brought by Filter-KD is not significant when
_σ is too small or too large. That is reasonable because for small σ, few samples have bad labels,_
thus the possible enhancement might not be large. When σ is too high, the labels of similar points
become less reliable, and the learning path will no longer head as reliably towards p[∗]. Thus, for
very high noise ratios, the performance of Filter-KD decays back towards that of OHT.

Filter-KD also outperforms other methods in both accuracy and calibration on the clean dataset, as
illustrated in Table 3. This remains true in the more common KD case when the teacher network is
bigger than the student; see results in Table 2. Note that in Table 2, we no longer keep τ = 1, as the
baselines are more sensitive to τ when the teacher is large (see Appendix D for more discussion);
instead we optimize τ for each setting. Here “Eff” is short for EfficientNet-B1 (Tan & Le, 2019),
“Res” is short for ResNet18, “Mobi” is short for MobileNetV2 (Sandler et al., 2018), “VGG” is
short for VGG-11 (Simonyan & Zisserman, 2015).

In summary, the explanation in Section 4 and experimental results in this section demonstrate that
better supervisory signal, which can be obtained by Filter-KD, can enhance the prediction performance in both clean and noisy-label case. (It is also possible that the algorithm improves perfor

-----

mance for other reasons as well, especially in the clean label case; the influence of temperature may
be particularly relevant.)

6 RELATED WORK

**Human label refinement** Peterson et al. (2019) use a distribution of labels obtained from multiple
annotators to replace the one-hot label in training, and find that doing so enhances both the generalization performance of trained models and their robustness under adversarial attacks. However, this
approach is relatively expensive, as it requires more annotation effort than typical dataset creation
techniques; several experienced annotators are required to get a good estimate of p[∗].

**Label smoothing** Another popular method is label smoothing (Szegedy et al., 2016), discussed
in the previous sections. We believe that suitable smoothing can decrease **ptar** **p[∗]** 2 to some
extent, but the lower bound of this gap should be large, because label smoothing treats each class ∥ _−_ _∥_
and each sample in the same way. Much prior research (e.g. M¨uller et al., 2019) shows that KD
usually outperforms label smoothing as well.

**KD and ESKD** Knowledge distillation, by contrast, can provide a different ptar for each training
sample. KD was first proposed by Hinton et al. (2015) for model compression. Later, Furlanello
et al. (2018); Cho & Hariharan (2019); Zhang et al. (2019); Yuan et al. (2020) found that distillation
can help even in “self-distillation,” when the teacher and student have identical structure. Our work
applies to both self-distillation and larger-teacher cases.

Another active direction in research about KD is finding explanations for its success, which is often
still considered somewhat mysterious. Tang et al. (2020) decompose the “dark knowledge” provided
by the teacher into three parts: uniform, intra-class and inter-class. This explanation also overlaps
with ours: if we observe the samples with different difficulty in the same class, we are discussing
intra-class knowledge; if we observe samples with two semantically similar classes, we then have
inter-class knowledge. Among previous work, the most relevant study is that of Menon et al. (2021).
Our Hypothesis 1 is suggested by their main claim, which focuses on the question of what is a good
**ptar. Our work builds off of theirs by helping explain why this good ptar emerges, and how to find**
_a better one. Besides, by looking deeper into the learning path of samples with different difficulty,_
our work might shed more light on KD, anti-noisy learning, supervised learning, overfitting, etc.

**Noisy label task** Learning useful information from noisy labels is a long-standing task (Angluin
& Laird, 1988; Natarajan et al., 2013). There are various ways to cope with it; for instance, Menon
et al. (2019) use gradient clipping, Patrini et al. (2017) use loss correction, Huang et al. (2020)
change the supervision during training, and Zhang et al. (2020) employ extra information. It is
possible to combine KD-based methods with traditional ones to further enhance performance; our
perspective of learning paths also provides further insight into why KD can help in this setting.

7 DISCUSSION

In this paper, we first claim that better supervision signal, i.e., ptar that is closer to p[∗], leads to
better generalization performance; this is supported by results of Menon et al. (2021) and further
empirical suggestions given here. Based on this hypothesis, we explain why LS and KD outperform
one-hot training using experiments on a toy Gaussian dataset. To further understand how such
better supervision emerges, we directly observe the behavior of samples with different difficulty by
projecting them on a 2-D plane. The observed zig-zag pattern is well-explained by considering the
tradeoff between two forces, one pushing the prediction to be near that of similar inputs, the other
pushing the prediction towards its training label; we give an intuitive account based on Proposition 1
of why this leads to the observed zig-zag pattern.

To apply these findings on real tasks, in which the data and network are more complex, we conduct
low-pass filter on the learning path, and propose Filter-KD to further enhance ptar. Experimental
results on various settings (datasets, network structures, and label noise) not only show the advantage
of the proposed method as a method for a teacher in knowledge distillation methods, but also help
verify our analysis of how generalization and supervision work in training neural network classifiers.


-----

ACKNOWLEDGEMENTS

This research was enabled in part by support provided by the Canada CIFAR AI Chairs program,
WestGrid, and Compute Canada.

REFERENCES

Zeyuan Allen-Zhu and Yuanzhi Li. Towards understanding ensemble, knowledge distillation and
self-distillation in deep learning. arXiv preprint arXiv:2012.09816, 2020.

Dana Angluin and Philip Laird. Learning from noisy examples. Machine Learning, 2(4):343–370,
1988.

Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang.
On exact computation with an infinitely wide neural net. In Advances in Neural Information
_Processing Systems, 2019._

Jang Hyun Cho and Bharath Hariharan. On the efficacy of knowledge distillation. In Proceedings
_of the IEEE/CVF International Conference on Computer Vision, pp. 4794–4802, 2019._

Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar.
Born again neural networks. In Proceedings of International Conference on Machine Learning,
pp. 1607–1616. PMLR, 2018.

Mengya Gao, Yujun Shen, Quanquan Li, Junjie Yan, Liang Wan, Dahua Lin, Chen Change Loy,
and Xiaoou Tang. An embarrassingly simple approach for knowledge distillation. arXiv preprint
_arXiv:1812.01819, 2018._

Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. In International Conference on Machine Learning, pp. 1321–1330. PMLR, 2017.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
770–778, 2016.

Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
_Computer Vision and Pattern Recognition, pp. 9729–9738, 2020._

Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
_preprint arXiv:1503.02531, 2015._

Lang Huang, Chao Zhang, and Hongyang Zhang. Self-adaptive training: beyond empirical risk
minimization. Advances in Neural Information Processing Systems, 33, 2020.

Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems, 2018.

Ziheng Jiang, Chiyuan Zhang, Kunal Talwar, and Michael C Mozer. Characterizing structural regularities of labeled data in overparameterized models. Proceedings of International Conference on
_Machine Learning, 2021._

Taehyeon Kim, Jaehoon Oh, Nakyil Kim, Sangwook Cho, and Se-Young Yun. Understanding
knowledge distillation. https://openreview.net/forum?id=tcjMxpMJc95, 2019.

Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-learning
regularization prevents memorization of noisy labels. Advances in Neural Information Processing
_Systems, 2020._

Aditya Krishna Menon, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Can gradient
clipping mitigate label noise? In International Conference on Learning Representations, 2019.


-----

Aditya Krishna Menon, Ankit Singh Rawat, Sashank Reddi, Seungyeon Kim, and Sanjiv Kumar.
A statistical perspective on distillation. In International Conference on Machine Learning, pp.
7632–7642. PMLR, 2021.

Rafael M¨uller, Simon Kornblith, and Geoffrey Hinton. When does label smoothing help? In
_Advances in Neural Information Processing Systems, 2019._

Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with
noisy labels. Advances in Neural Information Processing Systems, 26:1196–1204, 2013.

Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making
deep neural networks robust to label noise: A loss correction approach. In Proceedings of the
_IEEE Conference on Computer Vision and Pattern Recognition, pp. 1944–1952, 2017._

Joshua C Peterson, Ruairidh M Battleday, Thomas L Griffiths, and Olga Russakovsky. Human
uncertainty makes classification more robust. In Proceedings of the IEEE/CVF International
_Conference on Computer Vision, pp. 9617–9626, 2019._

Karsten Roth, Timo Milbich, Bj¨orn Ommer, Joseph Paul Cohen, and Marzyeh Ghassemi. S2sd:
Simultaneous similarity-based self-distillation for deep metric learning. Proceedings of Interna_tional Conference on Machine Learning, 2021._

David Ruppert. Efficient estimations from a slowly convergent Robbins-Monro process. Technical
report, Cornell University Operations Research and Industrial Engineering, 1988.

Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. MobileNetV2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE Conference on
_Computer Vision and Pattern Recognition, pp. 4510–4520, 2018._

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. Proceedings of International Conference on Learning Representations, 2015.

Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
_Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1–9, 2015._

Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the Inception architecture for computer vision. In Proceedings of the IEEE Conference on
_Computer Vision and Pattern Recognition, pp. 2818–2826, 2016._

Mingxing Tan and Quoc Le. EfficientNet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine Learning, pp. 6105–6114. PMLR, 2019.

Jiaxi Tang, Rakesh Shivanna, Zhe Zhao, Dong Lin, Anima Singh, Ed H Chi, and Sagar Jain. Understanding and improving knowledge distillation. arXiv preprint arXiv:2002.03532, 2020.

Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Jiashi Feng. Revisiting knowledge distillation
via label smoothing regularization. In Proceedings of the IEEE/CVF Conference on Computer
_Vision and Pattern Recognition, pp. 3903–3911, 2020._

Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. Proceedings of International
_Conference on Learning Representations, 2017._

Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Be your
own teacher: Improve the performance of convolutional neural networks via self distillation. In
_ICCV, 2019._

Zhilu Zhang and Mert R Sabuncu. Self-distillation as instance-specific label smoothing. Advances
_in Neural Information Processing Systems, 2020._

Zizhao Zhang, Han Zhang, Sercan O Arik, Honglak Lee, and Tomas Pfister. Distilling effective
supervision from severe label noise. In Proceedings of the IEEE/CVF Conference on Computer
_Vision and Pattern Recognition, pp. 9294–9303, 2020._


-----

Code, including the experiments producing the figures and a Filter-KD implementation, is available
[at https://github.com/Joshua-Ren/better_supervisory_signal.](https://github.com/Joshua-Ren/better_supervisory_signal)

A MISCELLANEOUS BACKGROUND

**Calculation of ECE** Expected calibration error is a measurement about how well the predicted
confidence represent the true correctness likelihood (Guo et al., 2017). For example, if our model
gives 100 predictions, each with confidence, say, q(y = k|x) ∈ [0.7, 0.8]). Then we might expect
there are 70 80 correct predictions among those 100 ones. To calculate this, we first uniformly
_∼_ _m_ 1
dividewhose confidence falls into [0, 1] into M bins, with each bin represented by Im. Then, ECE is calculated as: Im ∈ _M−_ _[,][ m]M_ and Bm is all the x samples
  


_Bm_
_|_ _|_


ECE =


_acc(Bm)_ _conf_ (Bm) _,_ (5)
_|_ _−_ _|_


_m=1_


where acc(Bm) = _B1m_ _i_ _Bm_ [1][(ˆ]yi = ki), conf (Bm) = _B1m_ _i_ _Bm_ _[q][(][y][ =][ k][i][|][x][i][)][,][ k][i][ is the]_

_|_ _|_ _∈_ _|_ _|_ _∈_

true label and ˆyi = argmax[q(y **xi)] is the model’s prediction. All the ECE mentioned in this paper**

P _|_ P

is calculated by setting M = 10.

**Barycentric coordinate system** When visualizing the learning path, it is problematic to directly
choose two dimensions and draw them in the Cartesian coordinate system, as illustrated in the first
panel in Figure 8. In geometry, a suitable way to project a 3-simplex vector onto a 2-D plane is
converting it to a point in a barycentric coordinate system. Specifically, we have three basis vectors:

**v0 = [0, 0][T], v1 = [1, 0][T], and v2 =** 12 _[,]_ _√23_ T, which are the corner and two edges of an equilateral

triangle respectively. Then the 2D-coordinate is calculated ash i (x, y) = [v0; v1; v2] **q. So every**
_·_
points in the left corner of a Cartesian system plane can be converted to the triangle in a barycentric
system, as illustrated in the last panel in Figure 8.


Figure 8: How to project a probability vector on to the plane of Barycentric coordinate system.

B RISK ESTIMATE VARIANCE BOUND

The following result is a generalization of Proposition 3 of Menon et al. (2021), whose proof we
replicate and extend here:

**Proposition 2. Let L be any bounded loss, L(y, ˆy) ≤** _ℓ< ∞, and consider Rtar of (3). For any_
_predictor f : X →_ ∆[K], we have that

E (Rtar(f, ) _R(f_ ))[2][] **p[T]tar[L][(][f]** [(][x][))] + ξ,
_D[′]_ _−_ _≤_ _N[1]_ [Var]x
_D[′]_
  


-----

_where ξ can be any of the following seven quantities:_

2
_ℓ[2]K_ E
**x**
 _[∥][p][tar][(][x][)][ −]_ **[p][∗][(][x][)][∥][2]2**

_ℓ[2][ ]E_
**x**

_[∥][p][tar][(][x][)]2[ −]_ **[p][∗][(][x][)][∥][1]**

2ℓ[2][ ]E KL(ptar(x) **p[∗](x))** 2ℓ[2] E
**x** _∥_ **x** [KL(][p][tar][(][x][)][ ∥] **[p][∗][(][x][))]**

p 2

2ℓ[2][ ]E KL(p[∗](x) **ptar(x))** 2ℓ[2] E
**x** _∥_ **x** [KL(][p][tar][(][x][)][ ∥] **[p][∗][(][x][))]**

_ℓp[2]_ E KL(ptar(x) **p[∗](x)) + KL(p[∗](x)** **ptar(x))** _._
**x** _∥_ _∥_
 

_Proof. To begin,_

2
E (Rtar(f, D[′]) − _R(f_ ))[2][] = Var E _._
_D[′]_ _D[′][ [][R][tar][(][f,][ D][′][)][ −]_ _[R][(][f]_ [)] +] _D[′][ [][R][tar][(][f,][ D][′][)][ −]_ _[R][(][f]_ [)]]

For the variance term, since _R(f_ ) is a constant and Rtar is an average of _N i.i.d. terms, we get_


Var

_N_ [Var]x [[][p][tar][(][x][)][T][L][(][f] [(][x][))]][.]

_D[′][ [][R][tar][(][f,][ D][′][)][ −]_ _[R][(][f]_ [)] = 1]

The other term, as Rtar is an average of i.i.d. terms and R(f ) = Ex p[∗](x)[⊤]L(f (x)), is
E 2 = E 2 _._
**x** [(][p][tar][(][x][)][ −] **[p][∗][(][x][))][⊤][L][(][f]** [(][x][))]
_D[′][ [][R][tar][(][f,][ D][′][)][ −]_ _[R][(][f]_ [)]]
    

For the first bound, we apply the Cauchy-Schwarz inequality,


(ptar(x) **p[∗](x))[⊤]L(f** (x)) **ptar(x)** **p[∗](x)** 2 **L(f** (x)) 2;
_−_ _≤∥_ _−_ _∥_ _∥_ _∥_

since the elements of L(f (x)) are each at most ℓ, the term ∥L(f (x))∥2 is at most

For the other bounds, we instead apply H¨older’s inequality, yielding


_Kℓ._


(ptar(x) − **p[∗](x))[⊤]L(f** (x)) ≤∥ptar(x) − **p[∗](x)∥1∥L(f** (x))∥∞ _≤_ _ℓ∥ptar(x) −_ **p[∗](x)∥1.**

The KL bounds follow by Pinsker’s inequality and then Jensen’s inequality. The last bound, for the
Jeffreys divergence, combines the two KL bounds.

C TOY GAUSSIAN DATASET

Figure 9: Correlation between test accuracy and **ptar** **p[∗]** 2 for different settings.
_∥_ _−_ _∥_

Figure 10: Correlation between test ECE and **ptar** **p[∗]** 2 for different settings.
_∥_ _−_ _∥_

In Sections 2 and 3, we apply a toy Gaussian dataset to verify two facts derived from Hypothesis 1,
so as the learning path of specific samples. Here we provide more details about this dataset.


-----

|Col1|Run1|Run2|Run3|
|---|---|---|---|
||τ=0.5 τ=1 τ=2 τ=10|τ=0.5 τ=1 τ=2 τ=10|τ=0.5 τ=1 τ=2 τ=10|
|OHT ESKD FilterKD|94.87 - - - 95.02 95.27 95.34 95.05 95.18 95.87 95.66 94.81|95.15 - - - 95.41 95.41 95.39 94.76 95.11 95.56 95.71 94.81|94.48 - - - 94.11 94.65 94.88 94.75 95.09 95.31 95.22 95.16|
|OHT ESKD FilterKD|77.85 - - - 78.28 78.56 79.28 79.09 78.43 79.57 79.72 79.65|78.23 - - - 78.50 78.20 79.3 78.81 79.23 79.61 79.45 79.01|77.99 - - - 78.12 78.31 78.84 78.36 79.75 80.32 80.41 79.99|


Table 4: The influence of temperature in self-distillation on CIFAR10/100 dataset.

**Generate the dataset** Here, we have N samples, each sample a 3-tuple (x, y, p[∗]). To get one
sample, we first select the label y = k following an uniform distribution over all K classes. After
that, we sample the input signal x _y=k_ (µk, σ[2]I), where σ is the noisy level for all the samples.
**_µk is the mean vector for all the samples in class|_** _∼N_ _k. Each µk is a 30-dim vector, in which each_
dimension is randomly selected from _δµ, 0, δµ_ . Such a process is similar to selecting 30 different
_{−_ _}_
features for each class. Finally, we calculate the true Bayesian probability of this sample, i.e.,
_p[∗](y|x)._

an uniform distribution, we haveCalculate the ground truth probability p[∗](y **x) =We use the fact thatp(x|y=k)** _p[∗](y|x) ∝_ _p(x|y)p(y). As y follows_
_|_ _j≠_ _k_ _[p][(][x][|][y][=][j][)]_ [. Following][ p][(][x][|][y][ =][ k][)][ ∼N] [(][µ][k][, σ][2][I][)][,]

e[sk]
we find p[∗](y **x) should have a Softmax form, i.e.,P** _p =_
_|_ _j≠_ _k_ [e][sj][ . Specifically, we have:]
P

e[s][k]
_p[∗](y = k_ **x) =** _si =_ 2[.] (6)
_|_ _j=k_ [e][s][j][ ;] _−_ 2σ[1] [2][ ∥][x][ −] **_[µ][i][∥][2]_**

_̸_

P


**Setup of experiments in Figure 1** in this experiment, we generate a toy Gaussian dataset with
_K = 3, σ = 2 and N = 10[5]. To reduce the variance of test error, we make a train/valid/test_
split with ratio [0.05 0.05, 0.9]. We train an MLP with 3 hidden layers, each with 128 hidden
units and ReLU activations. We first conduct experiments on some baseline settings, i.e., learning
from one-hot supervision (OHT for short), from smoothed label supervision (LS for short), from
a converged teacher’s prediction (KD for short), and from an early-stopped teacher’s prediction
(ESKD for short). In OHT case, an NN is trained under the supervision of ey. If we train this
NN until the convergence of training accuracy, we obtain the ptar for the KD case. If we select the
snapshot of that NN based on the best validation accuracy, we obtain the ptar for the ESKD case. If
we directly set ptar = 0.9ey +0.1u, where u is an uniform distribution over K classes, we obtain the
supervision for LS case. With different supervisions, we train a new network with identical structure
until the validation accuracy no longer increase. Furthermore, to see a trend between generalization
ability and **ptar** **p[∗]** 2, we run the experiment 200 times under different noisy supervisions.
_∥_ _−_ _∥_

D TEMPERATURE IN DIFFERENT KD METHODS

Temperature is an important hyper-parameter in different kinds of KD methods, which might influence the performance a lot. In this appendix, we will discuss why we prefer τ = 1.

**The role played by τ** In general, the loss function in KD has the form:


1
_L = β_
_·_ _τ_ [2]




_· H(q[τ]_ _, p[τ]tar[) + (1][ −]_ _[β][)][ · H][(][q][,][ e][y][)][,]_ (7)


where β ∈ [0, 1] is another hyper-parameter to trade-off the importance between one-hot label and
teacher’s predictions. Furthermore, for this loss, the gradient of L to logits z is:


_∂L_ 1

= β
_∂zi_ _·_ _τ_




_· (q[τ]i_ _[−]_ **[p]tar[τ]** _i_ [) + (1][ −] _[β][)][ ·][ (][q]i_ _[−]_ **[e][y]i** [)][.] (8)


-----

Figure 11: The influence of label smoothing factor on LS (first row) and different τ on KD (other
rows) under two different settings of the toy dataset. It is clear that these hyper-parameters won’t
influence the performance too much as long as they are in a good region.

**Why we use τ = 1** The most important reason for us to choose τ = 1 (as well as β = 1), even with
the risk of providing sub-optimal performance, is that we want to observe how much enhancement
is brought by refining the label. In our mind, KD’s success comes from the following two aspects:

(a) better label (i.e., ptar is better than ey);

(b) better learning dynamics (soften q to q[τ] make the training easier).

The first one provides the student with more useful knowledge, and the second one helps the student
to extract it. The focus of our paper is the improvement in labels. When the temperature is not 1,
both (a) and (b) are influenced, as we are using q[τ] (i.e., the smoothed student output) to match p[τ]tar
(i.e., the smoothed target) during training and using q to inference during testing. If we fix τ = 1,
the only difference between Filter-KD, ESKD, KD, LS, and OHT training is ptar. Under such a
condition, we believe the fact that Filter-KD/ESKD outperforms OHT (strictly better in each run) is
enough to conclude KD methods can provide better labels. Furthermore, as Filter-KD is proposed
to mitigate the high-variance issue in ESKD, and we can directly observe the zig-zag learning path
of samples with bad labels, we believe it is reasonable to make the conclusion in the paper.

Furthermore, using τ ̸= 1 doesn’t substantially
change our analysis. As illustrated in Equa- 0.90 0.902
tion (8), the gradient shows that q[τ] moves to- 0.85 0.901
wards p[τ]tar [in each update. Hence our analysis] 0.80 0.900
in Proposition 1 still holds, which means the Test accuracy0.75 Test accuracy0.899
trade-off between the two forces still exists. 0.70

_τ = 1 is reasonably good in our settings_ 0.80 0.902
In fact, the optimal τ depends on many set- 0.750.70 0.9010.900
tings, e.g., teacher’s size, optimizer, learning Test accuracy0.650.60 Test accuracy0.899
rate (scheduler), and etc: there is still no con- 0.55 0 5 10 15 20 0.898 1 2 3 4 5
sensus on what is the best choice of τ for all Temperature Temperature

0.90 0.902

0.85 0.901

0.80 0.900

Test accuracy0.75 Test accuracy0.899

0.898

0.70

0 5 10 15 20 1 2 3 4 5

0.90 0.903
0.85
0.80 0.902
0.75 0.901
0.70 0.900

Test accuracy0.650.60 Test accuracy0.899

0.55 0.898

0 5 10 15 20 1 2 3 4 5

Temperature Temperature

settings. So for the experiments in the main
context, we fix τ and fine-tune other hyper- Figure 12: First row: large teacher to small stuparameters to achieve good results. However, dent; second row: self-distill. Left column: τ in a
to verify this choice of τ is not terrible, we per- larger range; right column: τ in a small range.
formed a coarse grid search on both CIFAR and
toy examples. As illustrated in Table 4 and Figure 11, choosing τ ∈ [1, 2] seems to be good choice: the performance doesn’t significantly decay
until τ is too large.

Some readers might also curious about why our results don’t match the common notion that the
optimal τ should be 4, which is first provided in Hinton et al. (2015) and followed by much later


-----

1.0

0.8

0.6

0.4

0.2

0.0


Wrong labels Correct labels


Figure 13: The learning path of samples with correct and wrong labels.

work (e.g. Gao et al., 2018; Zagoruyko & Komodakis, 2017; Cho & Hariharan, 2019). Some works
also use a grid search to conclude that a temperature as high as 20 should be the best choice (Yuan
et al., 2020; Kim et al., 2019). We find this mismatch comes from the relative difference between
the network size of teacher and student. In short, when distilling from a large teacher to a small
student (as most of the cases in aforementioned works), high τ is preferred. When conducting selfdistillation, τ need not be that high. For example, Zhang et al. (2019) and Roth et al. (2021) claim
_τ = 1 is the best choice, while Zhang & Sabuncu (2020) claim τ ≈_ 2 is the best. To further
verify this, we compare the temperature trend between two cases on the toy dataset. In Figure 12,
the first row is distilling a 10-layer, 256-width MLP to a 3-layer, 32-width MLP; the second row is
self-distillation between 3-layer, 32-width MLPs.


At the same time, we also run a grid search the smoothing factor α of our Filter-KD in Table 5.

|Smoothing α|0.01 0.05 0.1 0.2 0.5 1 (ESKD)|
|---|---|
|FilterKD|79.50 80.00 79.83 79.59 78.48 78.39|


Table 5: The influence of the smoothing factor in FilterKD on CIFAR100 (mean of 3 runs). For
comparison, OHT obtained 77.64.

E HOW REPRESENTATIVE IS THE ZIG-ZAG PATTERN?


In the main paper, we only visualize the zig-zag learning path of a few samples in Figures 3 and 5.
The readers might then wonder whether this pattern is representative across the whole dataset. To
verify this, we define a quantitative metric called zig-zag score. Specifically, we first calculate the
integral of each dimension of the prediction:


_Qi∈{1,...,K} ≜_


**q[t]i[(][x][n][)][.]** (9)
_t=1_

X


We then use the highest Qi among i ∈{1, ..., K} \ {y}, where y is the label’s class, as the zig-zag
score. In other words, we focus on the behavior of q on those dimensions that are not the training
label. If this score is large, we might expect the neighbouring samples exert high influence on the
path, and vice versa. Note that as we have _i_ _[Q][i][ =][ constant for any sample (as][ q][t][ is a K-simplex),]_

this zig-zag score might correlated with C-score of Jiang et al. (2021). However, their focuses are
different: C-score is similar to _Qi=y and focuses more on how fast the prediction converge to the_
_−_ [P]
label; zig-zag score, on the other hand, is more about how much the path deviates towards a different
class, possibly the label close to p[∗].

With this score, we test the correlation between base difficulty and zig-zag score on toy datasets
under different settings. From Figure 14, it is safe to conclude that samples with higher base difficulty will have a more zig-zagging path during training. We also compare the expected zig-zag
score of the 1000 samples with flipped label in CIFAR10 (recall the experiment in Figure 6). In
Figure 13, it is clear that the zig-zagness of these wrong label samples (which we are sure have high
base difficulty) is significantly higher than the average.

Finally, we also randomly select some data samples in each class of CIFAR10 and visualize their
learning paths. Figure 15 shows random samples for the noisy label experiment. It is clear that the


-----

Figure 14: Correlation between base difficulty and zig-zag score in four different toy datasets.

learning path of easy samples with correct labels converge fast while that of samples with wrong
labels is zig-zag. In Figure 16, which shows the learning path with high zig-zag score when the
training set is clean, we can still observe some samples with zig-zag path. These samples might be
ambiguous, with a quite flat p[∗]. However, as we do not know the true p[∗] of these samples, it is
impossible to provide a result like Figure 13.

F DISTANCE GAP UNDER DIFFERENT SUPERVISIONS

Figure 2 provides the distance gap between q and corresponding p[∗] for each training sample in
different training stages, under the supervision of one-hot label ey. From the results in this figure, we
notice that the behavior of hard samples contributes more to the success of ESKD. Here we further
visualize how these gaps changes when the model is trained under different types of supervision,
i.e., ground truth, LS and ESKD.

In the first row, which is the result of label smoothing, we see the **ptar** **p[∗]** 2 (i.e., red dots) has
a “V”-shape. The vertex location depends on the smoothing parameter we choose. However, as ∥ _−_ _∥_
discussed previously, label smoothing has only one parameter to control ptar, which is like using a
linear model to fit a high-order function. So, although a proper smoothing value can bring better supervision than one-hot label, its upper bound might be limited. Regarding the training dynamics, we
can see a similar trend as the results shown in the one-hot case, i.e., all the samples first move down
and then converge to ptar. From the middle panel, we might expect label smoothing to outperform
the one-hot case, because the scatters are closer to the x-axis, which represents the ground truth p[∗].

The second and the third rows demonstrate the KD and ESKD case. Results in the KD case are
quite similar to the one-hot case in Figure 2, because the converged ptar is close to ey. However, we
can still expect KD to outperform one-hot training, because the ptar is closer to p[∗] than ey is. The
last panel in this row also demonstrates that **q** **p[∗]** 2 might be smaller than **ptar** **p[∗]** 2, which
can be considered as an explanation for why iterated self-distillation, e.g., Born Again Networks ∥ _−_ _∥_ _∥_ _−_ _∥_
(Furlanello et al., 2018), can improve the performance. For the ESKD case, we see the overfitting
almost disappear: the distribution of the blue points do not change too much after the early stopping
criterion is satisfied.

In the last row, which illustrates the training under the supervision of p[∗], we find all the blue points
move toward the x-axis, i.e., their ptar = p[∗], and finally converge to it. There is also no overfitting
in this case. From the last two panels, we see the disperse of blue points is significantly smaller than
all other settings, which means the network’s prediction is quite close to p[∗]. Hence the performance
of this case is the best.


-----

(a) Samples with clean labels.

(b) Samples with wrong labels.

Figure 15: Random selection of samples in CIFAR10 with 1000 flipped labels.


-----

Figure 16: Random selection of samples with high zig-zag score in clean CIFAR10.


-----

Figure 17: Distance gap of each sample under different supervision: label smoothing, KD, ESKD,
and ground-truth training.


-----

G MORE ABOUT THE DECOMPOSITION AND NTK MODEL

_Proof of Proposition 1. Recall z(x) = f_ (w, x) is the vector of output logits, and q = Softmax(z)
is the output probability. We are taking a step of SGD observing xu, and observing the change in
predictions on xo. We begin with a Taylor expansion,

**q[t][+1](xo)** **q[t](xo)** = **wq[t](xo)** **wt** **w[t][+1]** **w[t][]** + **w[t][+1]** **w[t]** _._

_−_ _∇_ _|_ _·_ _−_ _O_ _∥_ _−_ _∥[2][]_

_K_ 1 _K_ 1 _K_ _d_ _d_ 1
_×_ _×_ _×_   _×_  

To evaluate the leading term, we plug in the definition of SGD and repeatedly use the chain rule:| {z } | {z } | {z } | {z }

T

**wq[t](xo)** **wt** **w[t][+1]** **w[t][]** = **zq[t](xo)** **zt** **wz[t](xo)** **wt** _η_ **wL(xu)** **wt**
_∇_ _|_ _·_ _−_ _∇_ _|_ _· ∇_ _|_ _·_ _−_ _∇_ _|_
_K_ _d_ _d_ 1 _K_ _K_ _K_ _d_ 1 _d_
_×_   _×_   _×_ _×_    _×_ 

T

| {z } | {z } = |zq[t](x{zo) **zt** } |wz[t](x{zo) **wt** } _η_ **zL(|xu)** **z{zt** **w}z[t](xu)** **wt**

_∇_ _|_ _· ∇_ _|_ _·_ _−_ _∇_ _|_ _· ∇_ _|_
_K_ _K_ _K_ _d_ 1 _K_ _K_ _d_
_×_ _×_   _×_ _×_ 

= | _η_ {zzq[t](x}o) |zt {zwz[t](x}o) **w|t** {zwz[t](xu)} **w|t** [][T] {z **zL}(xu)** **zt** [][T]
_−_ _∇_ _|_ _·_ _∇_ _|_ _·_ _∇_ _|_ _·_ _∇_ _|_
_K_ _K_ _K_ _d_ _d_ _K_ _K_ 1
_×_  _×_   _×_    _×_

= η | (xo{z) }(xo,| xu) {zptar(}xu)| **q[t](x{zu)** _._ } | {z }
_· A[t]_ _· K[t]_ _·_ _−_
  


For the higher-order term, using as above that

**w[t][+1]** **w[t]** = _η_ **wz[t](xu)** **w[t][ ·]** **ptar(xu)** **q[t](xu)**
_−_ _−_ _∇_ _|[T]_ _−_

and noting that, since the vectors are probabilities, **ptar( xu)** **q[t](xu)** is bounded, we have that
_∥_ _−_ _∥_

**w[t][+1]** **w[t]** = _η[2]_ ( **wz(xu)** **wt** )[T] op tar[(][x][u][)][−][q][t][(][x][u][)][∥][2][] = _η[2]_ **wz(xu)** op _._
_O_ _∥_ _−_ _∥[2][]_ _O_ _∥_ _∇_ _|_ _∥[2]_ _[∥][p]_ _O_ _∥∇_ _∥[2]_
      

In the decomposition,


_q1(1_ _q1)_ _q1q2_ _q1qK_
_−_ _−_ _· · ·_ _−_
_q2q1_ _q2(1_ _q2)_ _q2qK_

(xo) = _zq[t](xo)_ **zt =**  _−_ . _−._ _· · ·_ _−_ .  (10)
_A[t]_ _∇_ _|_ .. .. ... ..

 _qKq1_ _qKq2_ _qK(1_ _qK)_
 _−_ _−_ _· · ·_ _−_ 
  _[,]_

which is a symmetric positive semi-definite (PSD) matrix[8] with trace tr(A[t](xo)) = 1 − [P]i[K]=1 _[q]i[2][.]_
As we have _i_ _[q][i][ = 1][, it is easy to check the trace of this matrix is larger at the beginning of training]_

(when q tends to be flat) than that at the end of training (q tends to be peaky), as illustrated by most
panels in Figure 18. Given that the trace of a matrix is the sum of its eigenvalues and (xo) is PSD,

[P] _A[t]_

smaller tr(A[t](xo)) = 1 − [P]i[K]=1 _[q]i[2]_ [means this matrix will tend to shrink its inputs more. Hence the]
change in predictions tends to decrease when q[t] becomes more peaky.

The second term in that expression, K[t](xo, xu), is the outer product of gradients at xo and xu.
Intuitively, if their gradients have similar directions, this matrix is large, and vice versa. This matrix
is known as the empirical neural tangent kernel, and it can change through the course of training
as the network’s notion of “similarity” evolves. For appropriately initialized very wide networks
trained with very small learning rates, K[t] remains almost constant during the course of training, and
is almost independent of the initialization of the network parameters; the kernel it converges to is
known as the neural tangent kernel (Jacot et al., 2018; Arora et al., 2019).

**Learning Dynamics of q(xo)** In Proposition 1, we decompose q[t][+1](xo) _−_ **q[t](xo) into three parts;**
we use this to analyze what the learning path of a training sample might be like in Section 3.3. Here
we will provide more detailed illustration of the two groups of force imposed on q(xo).

Specifically, q[t][+1](xo) − **q[t](xo) will be influenced by two variables, i.e., updating sample xu and**
time t. We discuss their effects separately. For xu, only the last two terms, i.e., (xo, xu) and
_K[t]_
(ptar(xu) **q[t](xu)) depends on it.**
_−_

8The matrix A can be observed to be the covariance matrix of a categorical distribution with item probabilities q, and hence PSD.


-----

Figure 18: Upper panels: how tr( (xn)) changes during training. Each panel represents a specific
_A[t]_
**xn. The panels are ordered by their integral difficulty, from left-to-right and up-to-down. The x-**
axis is the number of epochs, and the y-axis is tr( (xn)). Lower panels: the correlation between
_A[t]_
cos(xo, xu) and tr( ). Panels are ordered by the integral difficulty of xo. The subtitle of the panel
_K[0]_
gives the sample ID, the class it belongs to (i.e., A, B, or C) and the color of their corresponding
class (i.e., A is blue, B is orange and C is green).

In Section 3.3, we claim that if xo and xu are similar, the norm of K[0](xo, xu) might be large, and vice
versa. Here we empirically demonstrate this using a toy Gaussian dataset, as illustrated in Figure 18.
The figure shows how the similarity between xo and xu correlates with tr(K[0](xo, xu)). Each panel
represents one xo, which is claimed in the title of the subfigures. The x-axis represents the rank of
the cosine similarity between observed xo and all N training samples (including itself). The y-axis
is the trace of (xo, xu). The color of each scatter point is the true label of xu. From the figure, we
_K[0]_
can observe a clear decreasing trend, which means smaller similarity leads to larger tr( (xo, xu)).
_K[0]_
Additionally, the term (ptar(xu) **q[t](xu)) provides a direction that q[t](xo) should move towards.**
_−_

We claim in Section 3.3 that at any point in the input space, the labels of these input samples might
follow the ground truth distribution, i.e., p[∗](y|x). Hence most of the neighbouring x might pull
**q(xo) towards its ground truth p[∗](y** **xo). We will discuss the norm of this term when discussing the**
_|_
influence of t. In short, at any time, the neighboring xu will impose stronger force on xo and the
direction of the force roughly points to the ground truth p[∗](y **xo).**
_|_


-----

As discussed, K[t] is roughly constant over t in the very-wide limit. For finite width networks, however, it adapts to reflect the network’s new “understanding” of similarities. For instance, it might
learn that certain types of images are more semantically similar than the randomly-initialized network thought. This does not fundamentally change our intuitions as long as it doesn’t happen too
often, but could potentially lead to more complicated zig-zag patterns as the network’s estimate of
**p[∗]** from neighboring points perhaps improves over time.

Over the course of training, (xo) and (ptar(xu) **q[t](xu)) will also change. In practical regimes,**
_A[t]_ _−_
none of these terms have an easy analytical expression w.r.t. t: q[t] is quite complicated. Thus, we provide some intuition, with experimental verification. In Figure 18, we show how tr( (xo)) depends
_A[t]_
on q[t]: flat q[t] leads to larger trace. A similar trend also exists in the norm of (ptar(xu) **q[t](xu)).**
_−_
As the initialized q tend to be flat, updates of any samples will influence network’s parameters a
lot. When the training progresses, those easy samples converge fast, so their **ptar(xu)** **q[t](xu)** 2
_∥_ _−_ _∥_
and tr( ) become small. However, as the q[t] for the hard sample is still far away from its ey, the
_A[t]_
large **ptar(xu)** **q[t](xu)** 2 and tr( ) will finally drag q[t] back towards the one-hot distribution, as
_∥_ _−_ _∥_ _A[t]_
illustrated in Figure 3.

H COMPARISON TO LIU ET AL. (2020) AND HUANG ET AL. (2020)

The main claim of this paper is that better supervisory signals can enhance the generalization ability
of the trained model. Inspired by the success of KD, we find that the neural network can spontaneously refine those “bad” labels during training by observing their learning path. The learning path
of those hard samples will first move towards their true p(y|x) and then converge to their label ptar
or ey. We explain why this phenomenon occurs by expanding the gradients of each training sample.
This phenomenon is also explained in Proposition 1, and formally proved for a particular softmax
regression model by Liu et al. (2020). As a complement, we propose an explanation using an NTK
model, and experimentally verify it by observing the learning path and distance gap during training.

Another difference between these two works is that we consider the problem of refining supervisory
signals, while Liu et al. (2020) consider correcting wrong labels (a special case of “bad” supervision). Our work provides additional emphasis and empirical study for the clean-label case.

Regarding the algorithm, Liu et al. (2020) design an effective regularization term inspired by early
stopping regularization. They apply exponential moving average (EMA) on the model’s output when
calculating this regularization term to further enhance the performance. This method is similar to
that proposed by Huang et al. (2020), who switch between optimizing the training loss and an
objective based on the EMA of the model over the course of training.

Although it bears significant similarity to these methods, Filter-KD does not change the course of
training the teacher model. Rather, we propose (based on the high variance of the zig-zag learning
paths) to simply use the the EMA of that model’s outputs as a teacher for later distillation.

We suspect that these three algorithms work because of essentially the same underlying principle,
whether we think of this as being based on the zig-zag learning path or as early-stopping regularization. We expect that this principle will be helpful moving forward in the field’s understanding of the
learning dynamics of SGD methods for neural networks.

I LOW PASS FILTER ON NETWORK PARAMETERS

In Section 4, we point out the high variance issue of the traditional KD methods after observing
the learning path of those hard samples. We then propose a Filter-KD algorithm to smooth the
output of each training sample during training. Such a low pass filtering method is quite similar
to the momentum mechanism used in self-supervised learning, e.g., from the classic method of
Ruppert (1988) to MOCO (He et al., 2020), which conduct low pass filtering on each parameter of
the network. As mentioned before, the proposed Filter-KD algorithm might require more memory
when the dataset becomes larger, because qsmooth would record every training sample’s prediction.

Conducting low pass filtering on network parameters might be a good way to solve this. To verify
whether this method works, we train a ResNet18 on CIFAR100, using one-hot supervision. At the
beginning of training, we copy the training model and call it tracking model. At the end of each


-----

Figure 19: Behavior of the tracking model (with low pass filter on each parameters). ResNet18
trained for 150 epochs on CIFAR100.

update (i.e., each batch), the parameters of training model is updated as usual while the tracking
model’s parameters are updated with momentum. Specifically, for the training model, w[t]train[+1] [=]
**w[t]train** [+][ η][∇][L][, for the tracking model,][ w]track[t][+1] [= (1][ −] _[α][)][w]track[t]_ [+][ α][w]train[t][+1][. We train the model for]
150 epochs, and show the learning curve of test accuracy in the left panel of Figure 19. We see the
performance of the tracking model converges faster than the training model, which is reasonable
because filtering parameters is regularizing the training process. However, the converging accuracy
of this two models are the same. At the same time, we find this tracking model is not as good as
**qsmooth when teaching a new model, as illustrated in the right panel of Figure 19.**

As this paper mainly focuses on the behavior of the model’s prediction, the discussion and experiments of filtering in parameter space is limited. However, as many papers demonstrates the
effectiveness of this momentum mechanism, we think it is important to explore the relationship

|further.|Col2|Col3|
|---|---|---|
||Test ACC|Test ECE|
||Run1 Run2 Run3 Run4 Run5|Run1 Run2 Run3 Run4 Run5|
|OHT KD ESKD FilterKD|95.35 95.30 95.42 95.23 95.42 95.30 95.38 95.42 95.44 95.42 95.29 95.41 95.39 95.58 95.42 95.66 95.68 95.49 95.76 95.58|0.027 0.027 0.026 0.025 0.025 0.027 0.027 0.027 0.026 0.026 0.026 0.029 0.025 0.028 0.027 0.005 0.006 0.008 0.011 0.006|
|OHT KD ESKD FilterKD|78.27 78.31 77.97 77.78 78.02 78.64 78.55 78.03 78.18 78.49 78.84 78.73 78.85 78.97 78.74 79.87 79.93 80.19 80.22 80.23|0.053 0.053 0.052 0.053 0.054 0.060 0.057 0.062 0.066 0.059 0.065 0.066 0.067 0.070 0.066 0.028 0.026 0.034 0.028 0.031|



Table 6: Each run of results in Table 3. In each run, the initialization of student networks for different
methods are controlled to be the same.


-----

