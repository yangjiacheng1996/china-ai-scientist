# JOINT SHAPLEY VALUES: A MEASURE OF JOINT FEA## TURE IMPORTANCE


**Chris Harris**
Tokyo, Japan
Raptor Financial Technologies
chrisharriscjh@gmail.com

**Colin Rowat**
Economics
University of Birmingham, UK
c.rowat@bham.ac.uk


**Richard Pymar**
Economics, Mathematics & Statistics
Birkbeck College University of London, UK
r.pymar@bbk.ac.uk

ABSTRACT


The Shapley value is one of the most widely used measures of feature importance
partly as it measures a feature’s average effect on a model’s prediction. We introduce joint Shapley values, which directly extend Shapley’s axioms and intuitions:
joint Shapley values measure a set of features’ average contribution to a model’s
prediction. We prove the uniqueness of joint Shapley values, for any order of
explanation. Results for games show that joint Shapley values present different
insights from existing interaction indices, which assess the effect of a feature within
a set of features. The joint Shapley values provide intuitive results in ML attribution
problems. With binary features, we present a presence-adjusted global value that is
more consistent with local intuitions than the usual approach.

1 INTRODUCTION

_Game theory’s Shapley value partitions the value arising from joint efforts among individual agents_
(Shapley, 1953). Specifically, denote by N = {1, . . ., n} a set of agents, and by G[N] the set of games
on N, where a game is a set function v from 2[N] to R with v(∅) = 0. Then v(S) is the worth created
by coalition S ⊆ _N_ . When there is no risk of confusion, we omit braces to indicate singletons (e.g. i
rather than {i}) and denote a set’s cardinality by the corresponding lower case letter (e.g. s = |S|).

For any agent i, Shapley’s value is then


_s! (n_ _s_ 1)!
_−_ _−_ [v (S _i)_ _v (S)] ._ (1)

_n!_ _∪_ _−_


_ψi (v) ≡_


_S⊆N_ _\{i}_


This is the average worth that i adds to possible coalitions S, weighted as follows: if the set of agents,
_S, has already ‘arrived’, draw the next agent, i, to arrive uniformly over the remaining N_ _\S agents._

Shapley’s value is widely used in explainable AI’s attribution problem, partitioning model predictions
among individual features (q.v. Štrumbelj & Kononenko (2014); Lundberg & Lee (2017)) after the
model has been trained. Evaluating the prediction function at a specific feature value corresponds to
an agent’s presence; evaluating it at a reference (or baseline) feature value corresponds to the agent’s
absence. A feature’s Shapley value is its average marginal contribution to the model’s predictions.

When features are correlated, individual measures of importance may mislead (Bhatt et al., 2020;
Patel et al., 2021), as measures of individual significance such as the t-test do. Thus, Shapley’s value
has been extended to sets of features (Grabisch & Roubens, 1999a; Marichal et al., 2007; Alshebli
et al., 2019; Dhamdhere et al., 2020). As these extensions introduce axioms not present in Shapley,
they do not preserve the Shapley value’s intuition.


-----

We extend Shapley’s axioms to sets of features, randomizing over sets rather than individual features.
The resulting joint Shapley value thus directly extends Shapley’s value to a measure of sets of feature
importance: the average marginal contribution of a set of features to a model’s predictions.

Our approach’s novelty is seen in our extension of the null axiom: in Shapley (1953), a null agent
contributes nothing to any set of agents to which it may belong; here, a null set contributes nothing
to any set to which it may belong. By contrast, interaction indices (Grabisch & Roubens, 1999a;
Alshebli et al., 2019; Dhamdhere et al., 2020) recursively decompose sets into individual elements,
retaining the original Shapley null axiom. Thus, these indices measure sets’ contributions relative to
their constituent elements — so are complementary to the joint Shapley value.

The generalised Shapley value (Marichal et al., 2007) is closer to our work, but differs in a number of
respects: our probabilities are independent of the size of the set of features under consideration[1]; our
efficiency axiom is fully joint, while theirs are based on singletons and pairs; our symmetry axiom
provides uniqueness without reliance on recursion or partnership axioms.

To illustrate, consider a null coalition, T, whose individual members contribute positively to coalitions
that they join. Interaction indices assign a negative value to T, as the individual members act
discordantly together. However, from a joint feature importance viewpoint, T should be assigned
value 0. The joint Shapley value matches this intuition.

In the movie review application presented below, the joint Shapley values reveal contributions of
collections of words, including grammatical features such as negation ({disappointed} versus {won’t,
disappointed}), and adjectives ({effort} versus {terrific, effort}).

Like the Shapley-Taylor interaction index (Dhamdhere et al., 2020), our efficiency axiom depends
on a positive integer k, the order of explanation, which limits the number of joint Shapley values to
those for subsets up to cardinality k. As there are 2[N] _−_ 1 non-empty subsets of N, the full set of
joint Shapley values rapidly becomes unmanageable otherwise. In practice, k should be set to trade
off insight (favouring higher k) against computational cost (favouring lower k).

For each k, the extended axioms yield a unique joint Shapley value. Unlike in Shapley (1953), the
joint anonymity and symmetry axioms are not interchangeable: each imposes distinct restrictions.

Section 2 presents and extends the original Shapley axioms. Section 3 introduces joint Shapley values,
deriving them as the unique solution to the extended axioms. Section 4 illustrates joint Shapley
values in the game theoretical environment and applies them to the Boston housing (Harrison &
Rubinfeld, 1978) and Rotten Tomatoes movie review (Pang & Lee, 2005) datasets, comparing them to
interaction indices and presenting a sampling technique to facilitate calculation. Section 5 concludes.
The Appendix collects proofs and other supplemental material.

2 EXTENDING SHAPLEY’S AXIOMS

For a game v ∈G[N], and a permutation σ on N, denote a permuted game by σv ∈G[N] such that
_σv(σ(S)) = v(S), ∀_ _S ⊆_ _N_, where σ(S) = {σ(i) : i ∈ _S}. An index φ(v) of the game v ∈G[N]_ is
any real-valued function on 2[N] .

The original axioms uniquely satisfied by the (standard) Shapley value ψ, are:

**LI linearity : ψ is a linear function on G[N]**, i.e. ψ(v + w) = ψ(v) + ψ(w) and ψ(av) = aψ(v) for
any v, w ∈G[N] and a ∈ R.

**NU null : An agent that adds no worth to any coalition has no value, i.e. if v(S ∪{i}) = v(S) for**
all S _N_ _i_, then ψi(v) = 0. This axiom is sometimes called dummy.
_⊆_ _\ {_ _}_

**EF efficiency : The sum of the values of all agents is equal to the worth of the entire set, i.e. for all**
_v ∈G[N]_, _i=1_ _[ψ][i][(][v][) =][ v][(][N]_ [)][.]

**AN anonymity : For any σ on N and any v**, ψi(v) = ψσ(i)(σv), for all i _N_ .

[P][n] _∈G[N]_ _∈_

1Measures of joint significance (e.g. F -tests) and model selection (e.g. BIC or AIC) penalize larger feature
sets to avoid overfitting. As joint Shapley values are calculated after model training, this problem does not arise.


-----

**SY symmetry : If two agents add equal worth to all coalitions that they can both join then they**
receive equal value: if v(S _i_ ) = v(S _j_ ) _S_ _N_ _i, j_ then ψi(v) = ψj(v). This
_∪{_ _}_ _∪{_ _}_ _∀_ _⊆_ _\ {_ _}_
is strictly weaker than anonymity.

Now extend each of these axioms in natural ways to conditions on sets rather than singletons. Below,
_φS(v) denotes an index for coalition S on game v._

**JLI joint linearity : φ is a linear function on G[N]**, i.e. φ(v + w) = φ(v) + φ(w) and φ(av) = aφ(v)
for any v, w ∈G[N] and a ∈ R. (This axiom has not been modified.)

**JNU joint null : A coalition that adds no worth to any coalition has no value, i.e. if v(S ∪** _T_ ) = v(S)
for all S ⊆ _N \ T_, then φT (v) = 0.

**JEF joint efficiency : The sum of the values of all coalitions up to cardinality k is equal to the worth**
of the entire set, i.e. for all v ∈G[N],

_φT (v) = v(N_ ).

X


∅≠|TT |≤ ⊆kN :

**JAN joint anonymity : For any σ on N and any v ∈G[N]**, φT (v) = φσ(T )(σv), for all T ⊆ _N_ .
**JSY joint symmetry : If two coalitions perform equally when joining coalitions that they can both**
join and for other coalitions they add no worth then they receive an equal value, i.e. if

1. v(S ∪ _T_ ) = v(S ∪ _T_ _[′]) for all S ⊆_ _N \ (T ∪_ _T_ _[′]),_
2. v(S ∪ _T_ ) = v(S) for all S ⊆ _N \ T such that S ∩_ _T_ _[′]_ ≠ ∅,
3. v(S ∪ _T_ _[′]) = v(S) for all S ⊆_ _N \ T_ _[′]_ such that S ∩ _T ̸= ∅,_
then φT (v) = φT ′ (v).

Axiom JSY only equates the joint Shapley values for coalitions T and T _[′]_ if they contribute identically
to coalitions that they may both join, and contribute nothing to the other coalitions. Axioms JLI, JEF
and JAN are all also used in Dhamdhere et al. (2020). Our joint null and joint symmetry notions
appear to be new: they reflect our interest in a set of features’ contribution to a model’s predictions,
so that the set’s cardinality should not play a role in determining its value.

3 JOINT SHAPLEY VALUES

Our main result is that there is a unique solution to axioms JLI, JNU, JEF, JAN and JSY, the joint
Shapley value. The uniqueness is up to the k[th] order of explanation; we say nothing about |T _| > k._
**Theorem 1. For each order of explanation k ∈{1, . . ., n}, there is a unique (up to the k[th]** _order of_
_explanation) index φ[J]_ _which satisfies axioms JLI, JNU, JEF, JAN and JSY. It has the form_


_φ[J]T_ [(][v][) =]


_q|S|[v(S ∪_ _T_ ) − _v(S)]_
_S⊆XN_ _\T_


_for each ∅_ = T _N with_ _T_ _k, where (q0, . . ., qn_ 1) uniquely solves the recursive system
_̸_ _⊆_ _|_ _| ≤_ _−_

_q0 =_ _ki=11_ _ni_ _,_ _qr =_ Psrsk=(−=1∧1r(n−−kr)∨) 0 _n −srsrqs_ [;] (2)

_for all r_ 1, . . ., n 1. P    P   
_∈_ _−_

When k = 1, the joint Shapley values coincide with Shapley’s values.[2] When k = n, we have

_r_ _r_ ( 2)r−j

_qr =_ _−_ _r_ 0, . . ., n 1 _._

_j_ 2[n][−][j] 1 _[,]_ _∀_ _∈{_ _−_ _}_

_j=0_   _−_

X

For each k, the constants qr are non-negative and satisfy _s=n−k_ _ns_ _qs = 1. Further, as the joint_
Shapley value is similar in form to equation (1)’s standard Shapley value, the value of coalition T
  

2The Shapley-Taylor interaction index, φST, also has this property (Dhamdhere et al., 2020).[P][n][−][1]


-----

depends on its marginal contribution to other coalitions; unlike interaction indices, it does not depend
on the worth of its constituent agents.

As with the Shapley value, the joint Shapley value can be seen as the worth brought by ‘arriving’
agents but, rather than arriving one at a time, they can now also arrive in coalitions. We develop this
interpretation in Appendix A.

We show here the implication of each of the joint axioms introduced above and prove Theorem 1.

It is already known that joint linearity restricts measures to be linear combinations of worths:
**Lemma 1 (Grabisch & Roubens (1999a), Proposition 1). If φ satisfies JLI, then for every ∅** ≠ _T ⊆_ _N_
_there exists a family of real constants_ _a[S]T_
_{_ _[}][S][⊆][N][ such that for every][ v][ ∈G][N]_ _[,]_

_φT (v) =_ _a[T]S_ _[v][(][S][)][.]_

_SX⊆N_

Axiom JNU then constrains the values of the constants {a[T]S _[}][:]_

**Lemma 2. Suppose φ satisfies JLI and JNU and let** _a[S]T_
_Lemma 1. Then for every ∅_ = T _N and ∅_ = S _{N_ _[}] T[S][⊆], a[N,][T]S[ ∅][=][̸][=][T][ −][ ⊆][N][a]S[T][ be the constants from]T_ _[. Further, for every]_
_̸_ _⊆_ _̸_ _⊆_ _\_ _∪_
∅ = T _N_ _, S_ _N_ _T_ _, and ∅_ = H ⊊ _T_ _, a[T]S_ _H_ [= 0][.]
_̸_ _⊆_ _⊆_ _\_ _̸_ _∪_

SCombining these two lemmas yields:
**Proposition 1. Suppose φ satisfies JLI and JNU. Then there exist constants {p[T]** (S)} that depend on
_T and S such that for every ∅_ ≠ _T ⊆_ _N and v ∈G[N]_

_φT (v) =_ _p[T]_ (S)[v(S ∪ _T_ ) − _v(S)]._ (3)

_S⊆XN_ _\T_

Now establish a condition on the {p[T] (S)} values which must be satisfied under JEF:
**Proposition 2. For each order of explanation k, φ satisfies axioms JLI, JNU and JEF if and only if**
_for every ∅_ ≠ _T ⊆_ _N with |T_ _| ≤_ _k and v ∈G[N]_ _, φT (v) =_ _S⊆N_ _\T_ _[p][T][ (][S][)[][v][(][S][ ∪]_ _[T]_ [)][ −] _[v][(][S][)]][ with]_

_p[T]_ (S) _satisfying_

[P]

 _δN_ (S) = _p[T]_ (S _T_ ) _p[T]_ (S), (4)

_\_ _−_

∅|≠ _TXT |≤ ⊆kS:_ ∅≠ _|TTX ⊆ |≤Nk\S:_

_for all ∅_ ≠ _S ⊆_ _N_ _, where δN_ (S) equals 1 if S = N and 0 otherwise.


Recall that symmetry axiom SY is strictly weaker than anonymity axiom AN (Malawski, 2020).
This is not the case for their joint counterparts: each imposes a different constraint on the constants
_{p[T]_ (S)}, as we shall see here. First, consider the effect of imposing JAN.
**Proposition 3. For each order of explanation k, φ satisfies axioms JLI, JNU, JEF and JAN if and only**
_if for every ∅_ ≠ _T ⊆_ _N and v ∈G[N]_ _, φT (v) =_ _S⊆N_ _\T_ _[p][T][ (][S][)[][v][(][S][ ∪]_ _[T]_ [)][ −] _[v][(][S][)]][ with]_ _p[T]_ (S)

_satisfying (4) and_

[P] 

_p[T]_ (S) = p[T][ ′] (S[′]) ∀ ∅ ≠ _T, T_ _[′]_ _⊆_ _N, S ⊆_ _N \ T, S[′]_ _⊆_ _N \ T_ _[′]_ _s.t. s = s[′], t = t[′]._ (5)

The analogous result for JSY is:
**Proposition 4. For each order of explanation k, φ satisfies axioms JLI, JNU, JEF and JSY if and only**
_if for every ∅_ ≠ _T ⊆_ _N and v ∈G[N]_ _, φT (v) =_ _S⊆N_ _\T_ _[p][T][ (][S][)[][v][(][S][ ∪]_ _[T]_ [)][ −] _[v][(][S][)]][ with]_ _p[T]_ (S)

_satisfying (4) and_

[P] 

_p[T]_ (S) = p[T][ ′] (S) ∀ ∅ ≠ _T, T_ _[′]_ _⊆_ _N, S ⊆_ _N \ (T ∪_ _T_ _[′])._ (6)

Combining Propositions 2–4 completes the proof of Theorem 1.

The computational complexity of deriving the (q0, . . ., qn−1) is O(nk[2]). Once these have been
determined, the joint Shapley values can be calculated; the complexity of doing so is O(3[n] _∧_ (2[n]n[k])).


-----

4 EXPERIMENTS

4.1 GAME THEORETICAL

We present two game theoretical models from Dhamdhere et al. (2020) with known ‘ground truths’.
For each, we compare the joint Shapley value, the Shapley interaction index (Grabisch & Roubens,
1999b), the generalised Shapley value (Marichal et al., 2007), the added-value index (Alshebli et al.,
2019), and the Shapley-Taylor interaction index (Dhamdhere et al., 2020), respectively:


1 _n_ _t_ _−1_

_−_ ( 1)[t][−][l] _v (S_ _L),_ _T_ _N_ ;

_n_ _t + 1_ _s_ _−_ _∪_ _∀_ _⊆_
_−_   XL⊆T

(n − _s −_ _t)!s!_

(n _t + 1)! [[][v][ (][S][ ∪]_ _[T]_ [)][ −] _[v][ (][S][)] ;]_
_−_


_φ[SI]T_ [(][v][)][ ≡]

_φ[GS]T_ (v)
_≡_


_S⊆N_ _\T_

_S⊆XN_ _\T_


1 _c! (s_ _c_ 1)!

_v (T_ ) _−_ _−_ [v (C _i)_ _v (C)] ;_
_−_ 2[n][−][1] _s!_ _∪_ _−_
Xi∈T _S⊆XN_ :i∈S _CX⊆S\i_

_L_ _T_ [(][−][1)][t][−][l][ v][ (][L][)] if t < k

(Pnk _⊆S⊆N_ _\T_ _n−s_ 1 _−1_ _L⊆T_ [(][−][1)][t][−][l][ v][ (][S][ ∪] _[L][)]_ if t = k.

P    P


_φ[AV]T_ (v) _v (T_ )
_≡_ _−_


_φ[ST]T_ (v; k)
_≡_


Table 1: Joint and interaction measures in n = 3 game theory examples

Shapley _φ[SI]_ _φ[GS]_ _φ[AV]_ _φ[ST]_ (vm; k) _φ[J]_ (vm; k)
(k = 1) _k = 2_ _k = 3_ _k = 2_ _k = 3_


_ii, j_ 01 The majority game:1/3 1/03 vm (T ) = 111// when32 _t− ≥[1]1//332, and is equal to1/03_ 001 otherwise 12//99 24//2121
_N_ 1 _−2_ 1 0 _−2_ 3/21

A linear model with crosses: vc (T ) = _i_ _T_ [1 +][ c][ max][ {][0][, t][ −] [2][}][ for][ c][ ∈] [R]
_ii, j_ 12 1/3 (3 + c) 1/3 (3 +1/ c3c) 11//32 (3 + (4 + c c)) _−[1]/∈/126cc_ 1/31c 10 51//1818 (2 + (8 + c c)) 51//2121 (2 + (8 + c c))
_N_ 3 + c _c_ 3 + c [P]−[1]3/4c _c_ 3/21 (3 + c)

In the n = 3 majority game, Table 1 shows that, while Shapley-Taylor assigns 0 to singletons, the
joint Shapley value recognises that singletons contribute positively when joining existing coalitions.
The Shapley-Taylor and Shapley Interaction indices assign negative values to N (for discordant
interaction between members); the joint Shapley rewards N for adding worth in its own right.

In the linear model with crosses, joint Shapley values’ signs can again differ from those of interaction
indices: let c = −2, which sets φ[J]1 [= 0][: if][ i][ = 1][ joins the empty coalition or either singleton, it adds]
unit worth; however, when i = 1 joins {2, 3}, it subtracts unit worth. Averaging (according to the
arrival order), gives a net contribution of 0: coalition 1 does not contribute any worth in expectation.
By contrast, the Shapley-Taylor value always (for k > 1) assigns value to singletons equal to their
worth, as it is not (by design) capturing information about expected contributions of features.

4.2 THE AI/ML ATTRIBUTION PROBLEM

Following Štrumbelj & Kononenko (2010), let f be a prediction function, and x = (x1, . . ., xn)
_∈A_
an instance from the feature space. For a set of features S ⊆ _N_, define the prediction difference
_vx(S) when only features in S are known, as_

1
_vx(S)_ [f (τ (x, z, S)) _f_ (z)], (7)
_≡_ _−_

_|A|_ **_zX∈A_**

where τ (x, z, S) ∈A is defined as τ (x, z, S)i equal to xi for i ∈ _S and zi otherwise. Thus vx(S)_
is the difference between the expected prediction when only the feature values of x in S are known,


_vx(S)_
_≡_


_|A|_


-----

and the expected prediction when no feature values are known. Then for each T ⊆ _N_, φ[J]T [(][v][x][)][ is the]
contribution of features T to the prediction f (x). To estimate φ[J]T [(][v][x][)][, let][ X][ be a random variable]
whose law coincides with the law of the number of agents already present when T arrives in the
arrival interpretation (so that the law of X depends on t, the size of T ):


_−1_


_n_ _t_
P(X = i) = _−_
_i_



_n −_ _t_
_j_



_n−t_

_j=0_

X


_qi_


_qj_


for each i ∈{0, . . ., n} and S be a random variable uniform on the set of subsets of N \ T of size
_X. Once X is sampled, we can generate the set of agents already arrived, called S, by choosing it_
uniformly from all subsets of N _\T of size precisely X. Then_

_n−t_ _n_ _t_

_φ[J]T_ [(][v][x][) =] _−_ _qjE[vx(T_ ) _vx(_ )].

_j_ _∪S_ _−_ _S_

_j=0_  

X

Thus, by the law of large numbers, we can estimate this expectation by taking an average of
_vi(T ∪_ _Si) −_ _vi(Si) where Si has the same distribution as S and vi(S) = f_ (τ (x, zi, S)) − _f_ (zi),
where zi is chosen uniformly from the feature space. Refer to φ[J]T [(][v][x][)][ as the][ local][ joint Shapley]
value of features T at instance x. We can combine local values to obtain, for each feature, a
_global joint Shapley value. We do so in two ways. The first is the standard methodology (q.v._
Lundberg & Lee (2017)), which averages the absolute values of locals. We introduce the second for
models with exclusively binary variables by considering presence/absence of features T in x. This
_presence-adjusted global joint Shapley value is defined as_

1
_φ˜[J]T_ [(][f] [) =] _|A|_ **_xX∈A(2 · 1x(T )=1 −_** 1)φ[J]T [(][v][x][)][,] (8)

where 1x(T )=1 is one if all features T are present in x, and 0 otherwise. Covert et al. (2020)
introduced SAGE (Shapley Additive Global Importance), a more sophisticated treatment of global
influence measures that maintains efficiency at the global level.

All experiments are run on a single Intel(R) Core(TM) i7-6820HQ CPU. Training details and tuning
parameters are provided in the accompanying code.

4.2.1 SIMULATED DATA

Consider three features, x1, x2 and x3, each uniformly drawn from (0, 1) and a dataset of 50
observations. We first consider independently drawn features; we then investigate correlation by
fixing x2 = 1 − _x1. We use simplified ‘ML models’, f (x), to obtain exact global joint Shapley_
values, averaging the absolute values of local joint Shapley values derived from equation (7). Table 2
displays the results.

Table 2: Uniform random variables; k = 3

_x1_ _x2_ _x3_ _x1, x2_ _x1, x3_ _x2, x3_ _x1, x2, x3_

independent features
_f1 (x) = x1_ 0.122 0 0 0.049 0.049 0 0.037
_f2 (x) = x1 + x2_ 0.122 0.114 0 0.061 0.049 0.045 0.046
_f3 (x) = x1 −_ _x2_ 0.122 0.114 0 0.062 0.049 0.045 0.047

correlated features: x2 = 1 − _x1_
_f2 (x)_ 0.122 0.122 0 0 0.049 0.049 0
_f3 (x)_ 0.122 0.122 0 0.098 0.049 0.049 0.073

**Independent variables** As only x1 influences f1, only coalitions including it receive value; the
more diluted its role, the lower the value; x2 and x3 are symmetrically irrelevant. For f2, x1 and
_x2 play equal roles, and receive equal values (up to variance due to the sample size). For f2 and f3,_
_{x1, x2} receives similar value, and more than in f1, where only x1 influenced model predictions._
Comparing f2 and f3 shows that values only differ for coalitions containing both x1 and x2.


_φ˜[J]T_ [(][f] [) =]


_|A|_


-----

**Correlated features** _f2 now exhibits what we term a cancellation effect between x1 and x2,_
assigning a value of zero to coalitions containing both. Similarly, f3 reveals an enhancement effect:
values assigned to coalitions with both x1 and x2 are larger than in the independent case.

Enhancement and cancellation effects do not uniquely identify underlying phenomena. To illustrate,
let x1, x2 and x3 be independent Bernoulli(0.5) random variables. Setting k = 3 and letting n →∞,
we obtain the exact presence-adjusted global joint Shapleys in Table 3.

Table 3: Bernoulli(0.5) random variables; k = 3

_f (x)_ _x1_ _x2_ _x3_ _x1, x2_ _x1, x3_ _x2, x3_ _x1, x2, x3_

_f1 (x) = x1_ 5/21 0 0 1/21 1/21 0 1/56
_f2 (x) = x1 + x2_ 5/21 5/21 0 2/21 1/21 1/21 1/28
_f4 (x) = x1x2_ 5/42 5/42 0 1/14 1/42 1/42 3/112

The joint Shapley values containing x1 and x2 are larger for models f2 and f4 than for model f1,
a different sort of enhancement effect from that above. However, the joint Shapley value for x1 is
smaller in f4 than it is in f1 or f2, again a different type of cancellation effect.

Finally, joint Shapley values can provide insight into the black-box model’s structure. If,
for example, f can be decomposed as f (x) = g (x1) + h (x2, . . ., xn) with independent
Bernoulli(0.5) random variables, then the presence-adjusted global joint Shapley value of x1 is

1/2 (g (1) _g (0))_ _s=0_ _n−s_ 1 _qs =_ [5]/21, as shown in the table. If the joint Shapley value deviates
_−_
from this, we reject the decomposition, as in the case of f4.

[P][n][−][1]   

4.2.2 BOSTON HOUSING DATA

For comparability, we follow Dhamdhere et al. (2020) by training a random forest on the Boston housing dataset (Harrison & Rubinfeld, 1978), computing global values using the first notion discussed
above. Table 4 presents the largest and smallest global joint Shapley and Shapley-Taylor values.

Table 4: Joint Shapley values for the Boston dataset

Shapley _φ[ST]_ (f ; k) _φ[J]_ (f ; k)
(k = 1) _k = 2_ _k = 3_ _k = 2_ _k = 3_

RM: 2.57 RM: 3.12 RM: 3.12 LSTAT: 0.63 LSTAT: 0.42
LSTAT: 2.47 LSTAT: 2.04 LSTAT: 2.04 RM: 0.56 RM: 0.34
AGE: 0.81 DIS: 1.55 DIS: 1.55 LSTAT, RM: 0.30 AGE: 0.11
DIS: 0.63 CRIM: 1.37 CRIM: 1.37 AGE, LSTAT: 0.21 LSTAT, RM: 0.11
CRIM: 0.46 B: 1.31 DIS, LSTAT: 1.33 AGE, RM: 0.20 NOX: 0.10
NOX: 0.44 NOX: 1.15 B: 1.31 DIS, RM: 0.19 DIS: 0.08

. . . .
PTRATIO: 0.27 .. .. .. ..
B: 0.24 CHAS, RM: 0.15 AGE, CRIM, RM: 0.21 CHAS, TAX: 0.02 RAD, TAX, ZN: 0.00
TAX: 0.22 LSTAT, TAX: 0.15 AGE, DIS, PTRATIO: 0.21 RAD, ZN: 0.01 CHAS: 0.00
INDUS: 0.19 INDUS, RAD: 0.15 DIS, LSTAT, PTRATIO: 0.21 CHAS, RAD: 0.01 CHAS, RAD, TAX: 0.00
RAD: 0.13 NOX, TAX: 0.14 AGE, LSTAT, PTRATIO: 0.20 ZN: 0.01 CHAS, RAD, ZN: 0.00
ZN: 0.07 DIS, INDUS: 0.13 AGE, NOX, RM: 0.20 CHAS: 0.01 CHAS, TAX, ZN: 0.00
CHAS: 0.07 DIS, PTRATIO: 0.12 B, CRIM, LSTAT: 0.19 CHAS, ZN: 0.01 CHAS, ZN: 0.00

For k = 1, the joint Shapley values are the classical Shapley values. The unimportance of CHAS
seems to reflect its low variance (only 35 of 506 units back onto the Charles).

For k = 2, LSTAT and RM still have the largest joint Shapley values, with {LSTAT, RM} having
third largest. The top pairs all involve LSTAT or RM, indicating that these variables also contribute
to explaining house prices jointly with other variables. By contrast, {LSTAT, RM} is the 16th
largest Shapley-Taylor interaction value. The k = 2 singleton joint Shapley values are fairly evenly
distributed throughout the pairs; by contrast, the singleton k = 2 Shapley-Taylor values outrank all
the pairs. This is consistent with our extension of axiom NU to not favour singletons. The largest joint
Shapley value involving NOX is {NOX, LSTAT}, which is about five times as large as {NOX, DIS}


-----

and {NOX, RAD}. This is consistent with Harrison and Rubinfeld’s observation that NOX offset
RAD and DIS, but reinforced LSTAT: knowing the values of NOX and LSTAT adds a lot of predictive
power; knowing the values of NOX and RAD or DIS tends to wash out additional predictive power.

For k = 3, singletons again dominate: the largest triple is {AGE, LSTAT, RM}, the three individually
most important features. Continuing the above analysis, the joint Shapley value of {DIS, RAD} is
about 0.015, while that of {DIS, NOX, RAD} is about 0.008, and that of {DIS, LSTAT, RAD} is
about 0.022. We understand this to mean {DIS, NOX, RAD} contains the same sort of information as
{DIS, RAD}, but with NOX offsetting the other variables, while LSTAT brings novel socio-economic
information to the distance variables DIS and RAD, which it continues to partially offset. Similarly,
the least important joint features include the individually unimportant CHAS and its variants —echoing the role played by RM and LSTAT at the top of the list. While {DIS, LSTAT} has a ShapleyTaylor value of 1.33 (indicating a large interaction between the pair but not its overall importance),
its joint Shapley is 0.06 (ranked 18th) — directly assigning a value of importance.

Table 4’s values are exact. Figure 1 demonstrates the estimation procedure above, showing convergence of the sampled values as the iterations increase. As with other sampling procedures (e.g.
Štrumbelj & Kononenko (2014)), this cuts the complexity to order n to a linear power of k; as k → _n,_
the complexity approaches 2[n] times a polynomial in n (compared to 3[n] for the exact φ[J] ).


0.00020

0.00015

0.00010

0.00005

0.00000

0 50 100 150 200 250 300

Number of iterations


1.9

1.8

L2 Norm 1.7

1.6

1.5

1.4

50 100 150 200 250 300

Number of iterations


Figure 2: Difference between consecutive φ[J]
samples averages converges to zero; k = 2
movie review #2


Figure 1: Sampled φ[J] converges to exact φ[J]
with L[2] norm ≈ 1.45 ; k = 2 Boston

4.2.3 MOVIE REVIEWS


We train a fully connected neural network (two hidden layers, 16 units per layer, ReLU activations)
on the binary movie review classifications in Pang & Lee (2005). From the full set of reviews, we
remove a test block of 100 (picked to include those in Table 1 of Dhamdhere et al. (2020)) for analysis.
We encode reviews as the 1000 most common words in the corpus, augmented by key words in
Table 1 of Dhamdhere et al. (2020) to aid comparison of measures, for a total of 1004. (The DSA
features are drawn from positive reviews: {won’t}, {disappointed}, {both}, {inspiring}, {a}, {crisp},
{excellent}, {youthful}, {John}, {terrific}.) Binary accuracy is typically c. 76% after four epochs.

Table 5’s local joint Shapley values are tiny as each of the 2[1004] _−_ 1 joint features has a tiny effect on
the probability of a positive review. In the vein of Dhamdhere et al., we identify intuitive effects:

1. negation: the negative local joint Shapley values for {disappointed} and {be, disappointed}
become positive when negated by adding {won’t} to the coalition.

2. enhancement: the positive local joint Shapley values for {both} and {and} are enhanced by
the positive {and, both}.

3. context: the sign of local joint Shapley values involving {well} depend on the context; while
{you, well} is positive, {left, well} is negative.

4. lost potential: the positive local joint Shapley values involving {fascinating} become
negative when {been} is added.

5. adjectival: while {director} and {effort} are individually negative, they become positive
when qualified with the adjectives {winning} and {terrific}, respectively.


-----

Table 5: Examples of local joint Shapley values in the Pang & Lee (2005) movie reviews

Review joint Shapleys


{disappointed}: −2 × 10[−][5]

{won’t}: 6 × 10[−][5]

1: negation: aficionados of the whodunit won’t be disappointed {be, disappointed}: −9 × 10[−][8]

{won’t, disappointed}: 6 × 10[−][8]

{won’t, be, disappointed}: 5 × 10[−][9]

{both}: 2 × 10[−][4]
2: enhancement: both inspiring and pure joy {and}: 6 × 10[−][5]

{and, both}: 1 × 10[−][6]

{you, well}: 9 10[−][7]
3: context: you wish Jacquot had left well enough alone _×_

{left, well}: −3 × 10[−][7]

{would}: −1 × 10[−][4]

{fascinating}: 2 10[−][4]

4: lost potential: fascinating little thriller that would have been perfect _×_

{would, fascinating}: 3 × 10[−][7]

{would, been, fascinating}: −1 × 10[−][8]

{effort}: −1 × 10[−][5]

{director}: 9 10[−][6]

5: adjectival: director ...award-winning .. . make a terrific effort _−_ _×_

{terrific, effort}: 8 × 10[−][7]

{winning, director}: 5 × 10[−][7]

For global values, as the features are binary, we compute the presence-adjusted global joint Shapleys,
_φ˜[J]_ . Comparing the DSA features’ global _φ[˜][J]_ across the test reviews for k = 1 and k = 2 shows
{terrific} to be the largest, followed by {both} then {excellent}. This reflects prevalence in the training
set, where {terrific} appears four times as often in positive reviews. Similarly, on the negative _φ[˜][J]_
side, {John} is followed by {disappointed}: {John} appears thrice as often in negative reviews.

The largest pairs are {both, inspiring} followed by {excellent, youthful}. All of the pairs, except
{John, terrific}, have a positive sign: these pairs typically do not appear outside the positive reviews
from which they were drawn. As our encoding discards sequential information, the pairs are smaller
than the singletons: a coalition merely indicates co-occurrence in a review, rather than a bigram.

As the 10 DSA features are a very small subset of all 1004, we cannot asses whether JEF holds.
However, as a health check note: the average presence-adjusted local joint Shapleys over the positive
reviews is about 20% larger than that over the negative reviews. They are both negative: as these
features are drawn from positive reviews, but largely absent in any given review, their effect is
negative. Further, Table 6 indicates that the ranking of single features is largely preserved as k
increases from 1 to 2: the exception is {a, crisp}: in the corpus, {crisp} is typically preceded by {a}.

Figure 2 shows the estimated values’ convergence.

Table 6: Largest presence-adjusted global joint Shapley values on DSA features

_k = 1_ terrific both excellent won’t crisp youthful a inspiring disappointed John
_k = 2_ terrific both excellent won’t a youthful inspiring crisp disappointed John

5 CONCLUSIONS

The joint Shapley value directly extends Shapley’s value to measure the effect of a set of features on a
model’s predictions. Further work is needed to maintain properties like global efficiency (Covert et al.,
2020) and to make sampling more efficient (Williamson & Feng, 2020; Mitchell et al., 2021). We
believe that understanding complex models is labor intensive. Nevertheless, we envisage a common
workflow that computes k = 1 Shapley values, followed by k = 2 to identify strong pairwise effects;
analyses for k ≥ 3 then respond to the analyst’s evolving questions about the model’s functioning.


-----

6 ETHICS STATEMENT

This paper contributes to the literature on explainable AI, an important component of the Fairness,
Accountability and Transparency research agenda for ethical AI. It does not use human subjects; it
only draws on publicly available datasets; the work has not been sponsored, and does not seek to
promote any third organisations; none of the authors face any conflict of interest.

7 REPRODUCIBILITY STATEMENT

Our proofs and source code are available in the accompanying supplemental material; all data are
taken from the public domain.

REFERENCES

Bedoor K Alshebli, Tomasz P Michalak, Oskar Skibski, Michael Wooldridge, and Talal Rahwan.
A measure of added value in groups. ACM Transactions on Autonomous and Adaptive Systems
_(TAAS), 13(4):1–46, 2019._

Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep
Ghosh, Ruchir Puri, José M. F. Moura, and Peter Eckersley. Explainable machine learning in
deployment. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency,
pp. 648–657, 2020.

Ian Covert, Scott Lundberg, and Su-In Lee. Understanding global feature contributions with additive
importance measures. arXiv preprint arXiv:2004.00668, 2020.

Kedar Dhamdhere, Ashish Agarwal, and Mukund Sundararajan. The Shapley Taylor interaction
index. In International Conference on Machine Learning, pp. 9259–9268. PMLR, 2020.

Michel Grabisch and Marc Roubens. An axiomatic approach to the concept of interaction among
players in cooperative games. International Journal of Game Theory, 28(4):547–565, 1999a.

Michel Grabisch and Marc Roubens. Probabilistic interactions among players of a cooperative game.
In Beliefs, Interactions and Preferences in Decision Making, pp. 205–216. Springer, 1999b.

David Harrison and Daniel L Rubinfeld. Hedonic housing prices and the demand for clean air.
_Journal of Environmental Economics and Management, 5(1):81–102, 1978._

Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Advances
_in Neural Information Processing Systems, pp. 4765–4774, 2017._

Marcin Malawski. A note on equal treatment and symmetry of values. In Transactions on Computa_tional Collective Intelligence XXXV, pp. 76–84. Springer, 2020._

Jean-Luc Marichal, Ivan Kojadinovic, and Katsushige Fujimoto. Axiomatic characterizations of
generalized values. Discrete Applied Mathematics, 155(1):26–43, 2007.

Rory Mitchell, Joshua Cooper, Eibe Frank, and Geoffrey Holmes. Sampling permutations for Shapley
value estimation. arXiv preprint arXiv:2104.12199, 2021.

Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization
with respect to rating scales. In Proceedings of the ACL, 2005.

Neel Patel, Martin Strobel, and Yair Zick. High dimensional model explanations: an axiomatic
approach. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Trans_parency, pp. 401–411, 2021._

Lloyd S. Shapley. A value for n-person games. In Harold William Kuhn and Albert William
Tucker (eds.), Contributions to the theory of games, volume II of Annals of Mathematical Studies,
chapter 17, pp. 307–317. Princeton University Press, Princeton, 1953.


-----

Brian Williamson and Jean Feng. Efficient nonparametric statistical inference on population feature
importance using Shapley values. In International Conference on Machine Learning, pp. 10282–
10291. PMLR, 2020.

Erik Štrumbelj and Igor Kononenko. An efficient explanation of individual classifications using game
theory. The Journal of Machine Learning Research, 11:1–18, 2010.

Erik Štrumbelj and Igor Kononenko. Explaining prediction models and individual predictions with
feature contributions. Knowledge and Information Systems, 41(3):647–665, 2014.

In this supplementary material we discuss the arrival order interpretation, present the proofs of all
results, and discuss a notion of joint symmetry obtained by removing conditions 2 and 3 from JSY.

A ARRIVAL ORDER INTERPRETATION

As discussed in the paper, the joint Shapley value can be viewed in terms of the worth brought by
‘arriving’ agents but, rather than arriving one at time, they can now also arrive in coalitions. To
be precise, consider this procedure: at time 0, no agents have arrived; at each t ∈{1, 2, . . .}, the
next set of agents to arrive is chosen uniformly from the set of non-empty subsets of size at most
_k of the remaining (yet to arrive) agents. Then φ[J]T_ [is the expected worth brought by coalition][ T]
when it arrives (a coalition is assigned zero worth if it does not arrive at any time). To see this,
denote by Ai the coalition to arrive at time i, by Bi the union of all coalitions that have arrived
up to time i: Bi = _j_ _i_ _[A][j][, and by][ p][T][ the probability that at some time coalition][ T][ arrives,]_

_≤_
_pT = P(∃_ _i : Bi = T_ ). We have the recursive relationship:

_n_ [S]


P(Bi 1 = S)P(Ai = T _S_ _Bi_ 1 = S)
_−_ _\_ _|_ _−_


_pT =_


_S⊊T :_
_|S|≥|T |−k_

_S⊊T :_
_|S|≥|XT |−k_


_i=1_

_n_

_i=1_

X


_−1_

_n −|S|_
_r_
 []




_−1_

_n −_ _s_
_r_
 []




_t_
_s_



(n−|S|)∧k

_r=1_

X


_|T |−1_

_s=(|TX |−k)∨0_


(n−s)∧k

_r=1_

X


P(Bi 1 = S)
_−_


_pS_


where in the last line S is any set of size s. Thus we see that pT only depends on T through its
cardinality, and defining

1

(n _t)_ _k_ _−_
_−_ _∧_ _n_ _t_

_pˆt := pT_ _−_

 _r_

_r=1_  []

X

for any T with |T _| = t, the expected worth brought by coalition_  T under this procedure is


P(Bi 1 = S, Ai = T )[v(S _T_ ) _v(S)] =_
_−_ _∪_ _−_
_S⊆XN_ _\T_


_pˆ|S|[v(S ∪_ _T_ ) − _v(S)]._
_S⊆XN_ _\T_


_i=1_


Further, we have the relationship


_pˆt =_ Pts−=(r(n=11−t−t)k∧)k∨0 _n −srttps_

and since we know that p0 = 1 (we start with no agents having arrived) andP    _pn = 1 (we finish with_
all agents having arrived) we also have the identities:


_n_
_s_



_n_
_s_



_n−1_

_s=Xn−k_


1 =


_pˆs,_ 1 = ˆp0


_r=1_


By comparing with the identities defining (q0, . . ., qn 1), we deduce that ˆpt = qt for all t
_−_ _∈_
_{0, . . ., n −_ 1}, which verifies this arrival interpretation.


-----

B PROOFS

_Proof of Lemma 2. For the first statement, for each ∅_ ≠ _S ⊆_ _N \ T consider the game_

1 if R = S or R = S _T,_
_vS(R) =_ 0 otherwise. _∪_


Then for such S, by JNU, φT (vS) = 0. By this equality, Lemma 1, and the definition of vS,


0 = φT (vS) = _a[T]R[v][S][(][R][) =][ a][T]S_ [+][ a]S[T] _T_ _[.]_

_∪_
_RX⊆N_


For the second statement, for each ∅ ≠ _H ⊊_ _T let αH be a constant and for S ⊆_ _N \ T_, consider
the game

_αH_ if R = S _H for some ∅_ = H ⊊ _T,_
_x[α]S_ [(][R][) =] _∪_ _̸_
0 otherwise.


By JNU, φT (x[α]S [) = 0][ for every][ S][ ⊆] _[N][ \][ T]_ [. Thus by Lemma 1]


0 = φT (x[α]S [) =]


_a[T]R[x]S[α]_ [=]
_RX⊆N_


_a[T]S_ _H_ _[x]S[α][(][S][ ∪]_ _[H][) =]_
_∪_
∅≠ XH⊊T


_a[T]S_ _H_ _[α][H]_ _[.]_
_∪_
∅≠ XH⊊T


Since this holds for every choice of constants αH, it follows that a[T]S _H_ [= 0][ for all][ S][ ⊆] _[N][ \][ T][ and]_
_∪_
∅ ≠ _H ⊊_ _T_, as required.

_Proof of Proposition 1. By Lemma 1 there exist constants {a[T]S_ _[}][S][⊆][N,][ ∅][̸][=][T][ ⊆][N][ such that for every][ v]_
and ∅ ≠ _T ⊆_ _N_,


_a[T]S_ _[v][(][S][) =]_ _a[T]S_ _[v][(][S][) +]_ _a[T]S_ _H_ _[v][(][S][ ∪]_ _[H][) +][ a]S[T]_ _T_ _[v][(][S][ ∪]_ _[T]_ [)]

 _∪_ _∪_

_SX⊆N_ _S⊆XN_ _\T_ ∅≠ XH⊊T



(a[T]S _[v][(][S][) +][ a]S[T]_ _T_ _[v][(][S][ ∪]_ _[T]_ [)) =] _a[T]S_ _T_ [[][v][(][S][ ∪] _[T]_ [)][ −] _[v][(][S][)];]_
_∪_ _∪_
_S⊆XN_ _\T_ _S⊆XN_ _\T_


_φT (v) =_


where the last two equalities owe to Lemma 2. The proof is complete by setting p[T] (S) = a[T]S _T_ [.]
_∪_

_Proof of Proposition 2. Suppose φ satisfies axioms JLI, JNU and JEF. Then by Proposition 1 the_
constants {p[T] (S)} exist, such that for every v and ∅ ≠ _T ⊆_ _N_,

_φT (v) =_ _p[T]_ (S)[v(S ∪ _T_ ) − _v(S)]._

_S⊆XN_ _\T_

Now for each ∅ ≠ _R ⊆_ _N consider the identity game_

1 if S = R,
_wR(S) =_ 0 otherwise.



Then for every ∅ ≠ _T ⊆_ _N with |T_ _| ≤_ _k,_


_p[T]_ (S)[wR(S _T_ ) _wR(S)]._
_∪_ _−_
_S⊆XN_ _\T_


_φT (wR) =_


Note that the term wR(S ∪ _T_ ) − _wR(S) in the above sum is equal to 1 only when S ⊊_ _R and_
_T = R \ S, i.e. only when S = R \ T and ∅_ ≠ _T ⊆_ _R. Further note that this term is equal to −1_
only when S = R and T ̸= ∅, i.e. when S = R and ∅ ≠ _T ⊆_ _N \ R (as must have S ∩_ _T = ∅). In_
all other cases, this term is 0. Hence we deduce from JEF that


_p[T]_ (R \ T ) −


_p[T]_ (R).


_δN_ (R) = wR(N ) =


∅|≠ _TT |≤ ⊆kR:_


∅≠ _T ⊆N_ _\R:_
_|T |≤k_


-----

Now we show the implication in the other direction. If φT (v) = _S_ _N_ _T_ _[p][T][ (][S][)[][v][(][S][ ∪]_ _[T]_ [)][ −] _[v][(][S][)]]_

_⊆_ _\_
then it is immediate that JLI and JNU are satisfied. For JEF, we wish to show that for every v,

[P]

_p[T]_ (S)[v(S ∪ _T_ ) − _v(S)] = v(N_ ).

∅≠|TXT |≤ ⊆kN : _S⊆XN_ _\T_

Note that for each ∅ ≠ _R ⊆_ _N_, the coefficient of v(R) on the left-hand side in the above equation is

_p[T]_ (R \ T ) − _p[T]_ (R).

∅|≠ _TXT |≤ ⊆kR:_ ∅≠ _|TTX ⊆ |≤Nk\R:_

But by equation (4), this is equal to δN (R).

_Proof of Proposition 3. In light of Proposition 2 we just have to consider JAN._

**Only if: Suppose φ satisfies JLI, JNU, JEF and JAN. First, we shall establish that**

_p[T]_ (S) = p[T] (S[′]) ∀ ∅ ≠ _T ⊆_ _N, S, S[′]_ _⊆_ _N \ T s.t. s = s[′]._ (9)

Fix such a T, S and S[′]. Consider again the identity game, wS and let σ be a self-inverse permutation
such that S 7→ _S[′], S[′]_ _7→_ _S, and σ ({i}) = {i} for all i ̸∈_ _S ∪_ _S[′]. As T ⊆_ _N \ (S ∪_ _S[′]) and_
_σ (T_ ) = T, we have by JAN

_φT (wS) = φσ−1(T ) (wS) = φT (σwS)_

where


1 if R = σ (S) = S[′]
_σwS (R) = wS_ _σ[−][1]_ (R) =
0 otherwise

  


= wS′ (R) .


Hence we obtain φT (wS) = φT (σwS) = φT (wS′ ). Next, from Proposition 1 we have


_p[T]_ (Q) [wS (Q ∪ _T_ ) − _wS (Q)] = −p[T]_ (S),
_Q⊆XN_ _\T_


_φT (wS) =_


and similarly φT (wS′ ) = _p[T]_ (S[′]). Hence we obtain p[T] (S) = p[T] (S[′]), showing (9).
_−_

Using induction on s, we now establish that (5) holds. Fix T and T _[′]_ of the same size. For the base
case, suppose s = s[′] = n − _t. For S ⊆_ _N \ T and S[′]_ _⊆_ _N \ T_ _[′], this forces S = N \ T and_
_S[′]_ = N \ T _[′]. Now consider the game_

1 if r = n
_xn (R) =_
0 otherwise,


so that xn (R) = 1 if and only if R = N . Define a self-inverse permutation σ so that σ (T ) = T _[′],_
_σ (T_ _[′]) = T and σ ({i}) = {i} for all i ̸∈_ (T ∪ _T_ _[′]). Then by JAN and as σxn = xn,_

_φT (xn) = φσ−1(T ′)(xn) = φT ′_ (σxn) = φT ′ (xn).

Next, from Proposition 1,

_φT (xn) =_ _p[T]_ (Q) [xn (T ∪ _Q) −_ _xn (Q)] = p[T]_ (N \ T ),

_Q⊆XN_ _\T_

and similarly φT ′ (wn) = p[T][ ′] (N \ T _[′]). Hence we obtain p[T]_ (N \ T ) = p[T][ ′] (N \ T _[′]) which_
establishes the base case.

We now suppose that p[T] (S) = p[T][ ′] (S[′]) for all s = s[′] _≥_ _n −_ _c where S ⊆_ _N \ T_, S[′] _⊆_ _N \ T_ _[′]_ and
_c is a positive integer. We shall show that p[T]_ (S) = p[T][ ′] (S[′]) for all s = s[′] _≥_ _n −_ _c −_ 1 where where
_S ⊆_ _N \ T and S[′]_ _⊆_ _N \ T_ _[′]. To this end, consider the game_

1 if r _n_ _c_ 1 + t,
_x (R) =_ _≥_ _−_ _−_ _._
0 otherwise



-----

Thus, as before, we may write


_p[T]_ (Q) [x (Q ∪ _T_ ) − _x (Q)] =_
_Q⊆XN_ _\T_


_p[T]_ (Q) .

_n−c−1Q≤⊆Xq<nN_ _\−T_ _c−1+t_

_p[T]_ (Q) .


_φT (x) =_


Similarly,
_φT_ _[′] (x) =_


_Q[′]⊆N_ _\T_ _[′]_

_n−c−1≤q[′]<n−c−1+t_

Again, by JAN, we prove that φT (x) = φT _[′] (x). Define a self-inverse permutation σ so that σ (T_ ) =
_T_ _[′], σ (T_ _[′]) = T and σ ({i}) = {i} for all i ̸∈_ (T ∪ _T_ _[′]). As worth in game x depends only on a_
coalition’s cardinality, we have σx = x. Thus, by JAN, φT (x) = φσ(T )(σx) = φσ(T )(x) = φT ′ (x).

However,


_p[T][ ′]_ (Q[′]) ;


_p[T]_ (Q)+

_n−c−1Q<q<n⊆XN_ _\−T_ _c−1+t_


_p[T]_ (Q) =


_p[T]_ (Q)+

_Q[′]⊆XN_ _\T_ _[′]_

_n−c−1<q[′]<n−c−1+t_


_φT (x) =_


_qQ=⊆n−Nc\−T1_


_qQ=⊆n−Nc\−T1_


and
_φT ′ (x) =_


_p[T][ ′]_ (Q[′]) +


_p[T][ ′]_ (Q[′])


_Q[′]⊆N_ _\T_ _[′]_ _Q[′]⊆N_ _\T_ _[′]_

_q[′]=n−c−1_ _n−c−1<q[′]<n−c−1+t_

which gives, by the inductive hypothesis and φT (x) = φT ′ (x),

_p[T]_ (Q) = _p[T][ ′]_ (Q[′]) .

X X


_Q[′]⊆N_ _\T_ _[′]_

_q[′]=n−c−1_


_qQ=⊆n−Nc\−T1_


But by (9), p[T] (Q) = p[T] (Q[′]) if q = q[′]. Thus the above equation becomes

_n_ _t_ _n_ _t_
_−_ _p[T]_ (Q) = _−_ _p[T][ ′]_ (Q[′])
_n_ _c_ 1 _n_ _c_ 1
 _−_ _−_   _−_ _−_ 

for any Q ⊆ _N \_ _T and Q[′]_ _⊆_ _N \_ _T_ _[′]_ with q = q[′] = n _−_ _c_ _−_ 1. Thus, p[T] (Q) = p[T][ ′] (Q[′]), completing
the inductive step, and the ‘only if’ statement.

**If: Suppose (5) is satisfied. Fix a permutation σ on N and game v ∈G[N]** . Then for any ∅ ≠ _T ⊆_ _N_,

_φT (σv) =_ _p[T]_ (S) [σv (S ∪ _T_ ) − _σv (S)] =_ _p[T]_ (S) _v_ _σ[−][1]_ (S ∪ _T_ ) _−_ _v_ _σ[−][1]_ (S)

_S⊆XN_ _\T_ _S⊆XN_ _\T_       

= _p[T]_ (S) _v_ _σ[−][1]_ (S) ∪ _σ[−][1]_ (T ) _−_ _v_ _σ[−][1]_ (S) _._

_S⊆XN_ _\T_       

Defining the set S[′] = σ[−][1] (S) allows us to rewrite the above as


_φT (σv) = · · · =_ _p[T]_ (σ (S[′])) _v_ _S[′]_ _∪_ _σ[−][1]_ (T ) _−_ _v (S[′])_

_S[′]⊆NX\σ[−][1](T )_     

= _p[σ][−][1][(][T][ )]_ (S[′]) _v_ _S[′]_ _σ[−][1]_ (T ) _v (S[′])_ = φσ−1(T )(v),

_∪_ _−_
_S[′]⊆NX\σ[−][1](T )_     


with the penultimate step due to condition (5).

_Proof of Proposition 4. In light of Proposition 2 we just have to consider JSY._

**Only if: Suppose φ satisfies JLI, JNU, JEF and JSY, fix ∅** ≠ _T, T_ _[′]_ _⊆_ _N_, and consider again the
identity game wR. Then for any ∅ ≠ _R ⊆_ _N \ (T ∪_ _T_ _[′]),_

-  wR(S _T_ ) = 0 = wR(S _T_ ) for all S _N_ (T _T_ ),
_∪_ _∪_ _[′]_ _⊆_ _\_ _∪_ _[′]_


-----

-  wR(S ∪ _T_ ) = 0 = wR(S) for all S ⊆ _N \ T such that S ∩_ _T_ _[′]_ ≠ ∅,

-  wR(S ∪ _T_ _[′]) = 0 = wR(S) for all S ⊆_ _N \ T_ _[′]_ such that S ∩ _T ̸= ∅._


Hence by JSY φT (wR) = φT ′ (wR). But φT (wR) = p[T] (R) and φT ′ (wR) = p[T] (R). This shows
that p[T] (R) = p[T][ ′] (R) for all ∅ ≠ _R ⊆_ _N \ (T ∪_ _T_ _[′]). To show that p[T]_ (∅) = p[T][ ′] (∅) we consider
the game

1 if S = ∅,
_w[∗](S) =_ _̸_
0 otherwise.


Then

-  w[∗](S ∪ _T_ ) = 1 = w[∗](S ∪ _T_ _[′]) for all S ⊆_ _N \ (T ∪_ _T_ _[′]),_

-  w[∗](S ∪ _T_ ) = 1 = w[∗](S) for all S ⊆ _N \ T such that S ∩_ _T_ _[′]_ ≠ ∅ (since then S ̸= ∅),

-  w[∗](S ∪ _T_ _[′]) = 1 = w[∗](S) for all S ⊆_ _N \ T_ _[′]_ such that S ∩ _T ̸= ∅_ (since then S ̸= ∅).


It thus follows by JSY that φT (w[∗]) = φT ′ (w[∗]). However, φT (w[∗]) = p[T] (∅) and φT ′ (w[∗]) =
_p[T][ ′]_ (∅), which gives the required identity and shows that (6) holds.

**If: Now we show the implication in the other direction. Suppose (6) holds and v ∈G[N]** satisfies the
three conditions in JSY. Then


_p[T]_ (S)[v(S ∪ _T_ ) − _v(S)] =_ _p[T]_ (S)[v(S ∪ _T_ ) − _v(S)]_
_S⊆XN_ _\T_ _S⊆NX\(T ∪T_ _[′])_

_p[T][ ′]_ (S)[v(S ∪ _T_ _[′]) −_ _v(S)] =_ _p[T][ ′]_ (S)[v(S ∪ _T_ _[′]) −_ _v(S)] = φT ′_ (v).
_S⊆NX\(T ∪T_ _[′])_ _S⊆XN_ _\T_ _[′]_


_φT (v) =_


Hence JSY is satisfied.

_Proof of Theorem 1. We have to show that there exists exactly one choice of constants {p[T]_ (S)}
which satisfy equations (4)–(6). Notice that satisfying (5) and (6) is equivalent to satisfying

_p[T]_ (S) = p[T][ ′] (S) ∀ _S ⊆_ _N \ T, S[′]_ _⊆_ _N \ T_ _[′]_ s.t. s = s[′].

Thus p[T] (S) does not depend on T at all, and only depends on the cardinality of S. Let qs denote
_p[T]_ (S) for any S ⊆ _N \ T_ . Then we can re-write equation (4) in terms of qs as

_n−1_ _n_

1 = _qi,_ (10)

_i_

_i=n_ _k_  

X−

_qs =_ _sik=(−1s(n−ks)∨)_ 0 _n_ _si_ _sqi_ _s_ 1, . . ., n 1 _._ (11)

P _i=1∧_ _−_  −i _∀_ _∈{_ _−_ _}_

Note that for any q0, equation (11)P fully determines all other   _qi, for i ∈{1, . . ., n −_ 1} and q0 is
then determined by (10). Thus there is at most one solution. However, we have already identified
(see the arrival-order discussion in Appendix A) that a solution to this recurrence is given by

(q0, . . ., qn−1) = (ˆp0, . . ., ˆpn−1), for which ˆp0 = _ki=1_ _ni_ .
P   [][−][1]

C STRONG JOINT SYMMETRY

We examine the effect of removing conditions 2 and 3 from JSY. As it turns out, this leads to the
non-existence of an index. To be precise, we consider replacing axioms JAN and JSY with:

**SJS strong joint symmetry : fix ∅** ≠ _T, T_ _[′]_ _⊆_ _N_ . Then

_v (S ∪_ _T_ ) = v (S ∪ _T_ _[′]) ∀S ⊆_ _N_ _\ (T ∪_ _T_ _[′])_
_φT (v) = φT ′ (v) ._
_⇒_


-----

**Proposition 5. There is no index φ satisfying axioms JLI, JNU, JEF, and SJS that is guaranteed to**
_exist for all games._

_Proof. Since φ satisfies JLI, JNU, and JEF, by Proposition 2,_


_p[T]_ (S)[v(S ∪ _T_ ) − _v(S)]_
_S⊆XN_ _\T_


_φT (v) =_


with _p[T]_ (S) satisfying (4), for any game v . We consider two games v1, v2 . As
_{_ _}_ _∈G[N]_ _∈G[{][1][,][2][}]_
_N = {1, 2}, (4) gives p[{][1][}](∅) = p[{][2][}]({1}), p[{][2][}](∅) = p[{][1][}]({2}), and p[{][1][,][2][}](∅) + p[{][1][}](∅) +_
_p[{][2][}](∅) = 1._

Suppose v1({1}) = v1({1, 2}) = 1, v1({2}) = 0. SJS thus gives that φ{1}(v1) = φ{1,2}(v1), i.e.
_p[{][1][,][2][}](∅) = p[{][1][}](∅) + p[{][2][}](∅) which implies p[{][1][,][2][}](∅) = 1/2._

Suppose also that v2({1}) = v2({1, 2}) = v2({2}) = 1. SJS gives that φ{1}(v2) = φ{2}(v2) =
_φ_ 1,2 (v2), i.e. p[{][1][,][2][}](∅) = p[{][1][}](∅) = p[{][2][}](∅) which implies p[{][1][,][2][}](∅) = 1/3, giving a contra_{_ _}_
diction.

Thus, SJS is too strong a notion of symmetry, imposing linear restrictions on sets of unequal sizes.


-----

