# SHORT OPTIMIZATION PATHS LEAD TO GOOD GENER## ALIZATION

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Optimization and generalization are two essential aspects of machine learning. In
this paper, we propose a framework to connect optimization with generalization by
analyzing the generalization error based on the length of optimization trajectory
under the gradient flow algorithm after convergence. Through our approach, we
show that, with a proper initialization, gradient flow converges following a short
path with an explicit length estimate. Such an estimate induces a length-based
generalization bound, showing that short optimization paths after convergence
indicate good generalization. Our framework can be applied to broad settings. For
example, we use it to obtain generalization estimates on three distinct machine
learning models: underdetermined ℓp linear regression, kernel regression, and
overparameterized two-layer ReLU neural networks.

1 INTRODUCTION

From the perspective of statistical learning theory, the goal of machine learning is to find a predictive
function that can give accurate predictions on new data. For supervised learning problems, empirical
risk minimization (ERM) is a common practice to achieve this goal. The idea of ERM is to minimize
a cost function on observed data by an optimization algorithm. Therefore, a fundamental question is
_whether an optimization algorithm produces a solution with good generalization._

Recent works have shed light on providing theoretical explanations to this question from different
angles. One line of works considered the case when gradient methods converge to minimal norm
solutions (Bartlett et al., 2020; Tsigler & Bartlett, 2020; Liang & Rakhlin, 2020; Liang et al., 2020)
on kernel regression, and then analyzed the generalization of those minimal norm solutions. However,
the phenomenon of norm minimization has been known to happen only for the quadratic loss with an
appropriate initialization. Another line of works focused on overparameterized models, e.g., neural
networks under the Neural Tangent Kernel (NTK) regime (Allen-Zhu et al., 2019; Arora et al., 2019;
Cao & Gu, 2020; Ji & Telgarsky, 2020; Chen et al., 2021), proving that overparameterized neural
networks trained by (stochastic) gradient descent ((S)GD) have good generalization performance on
certain target functions (for example, polynomial functions).

Although existing works have made significant progress on the interplay of optimization and generalization, they focused on studying specific models, such as the NTK and models possessing
minimal norm solutions. In this paper, instead, we study general loss function conditions that induce
interesting connections between optimization and generalization.

We start with a simple observation, as shown in Figure 1, that under a generic random initialization,
the generalization performance for both linear regression model and random feature regression model
is closely related to the length of the optimization path[1] after convergence. In particular, short
optimization path are associated with good generalization. Here, by length we mean the trajectory
or path length of the parameter evolution during training. This is not the “length of time” used for
training, as is usually analyzed in early-stopping type of algorithms. Thus, the generalization error
concerns the weights of model trained to completion by empirical risk minimization. This empirical
investigation motivates us to use the trajectory length to connect optimization and generalization.
Intuitively, the length of the optimization path can be viewed as a kind of capacity control, and a

1For discrete iterations, we use the sum of the distance between every two consecutive iterations to represent
the length of the optimization path (See details in Appendix A.1).


-----

Figure 1: Illustrations of the relationship between optimization path lengths and generalization. (Left)
Linear regression model. (Right) Random feature regression model with the feature extracted by a
neural network. We train both models by gradient descent under Gaussian initialization N (µ, σ[2]) with
varied µ and σ, and record the trajectory lengths len(w[(0)], ∞) from initialization until convergence.
We observe that short optimization paths lead to good generalization. See experiment settings and
more numerical results in Appendix A.1 & A.2.

short path signifies low “complexity”. In other words, a general condition that guarantees a short
optimization path can be used to induce good generalization performance. Thus, inspired by the
theory in (Bolte et al., 2007) that Łojasiewicz gradient inequality (LGI) induces an explicit bound
for the gradient flow path, we consider Uniform-LGI (Definition 1). This is a modified version of
LGI and plays a critical role in obtaining a length estimate for the gradient flow trajectory. Once the
length is estimated, we can use our length-based generalization bounds to show the generalization
performance.

**Contributions. We summarize our contributions as follows:**

-  We focus on the gradient flow algorithm and propose a framework for combining optimization and
generalization. This framework is based on the length of the gradient flow trajectory, and its key
component is the Uniform-LGI property.

-  We then prove that under appropriate conditions, gradient flow returns a global minimum and gives
an explicit length-estimate for the gradient flow trajectory (Theorem 1). If this length-estimate has
an uniform bound for a certain initialization method, then we can give a length-based generalization
bound (Theorem 2).

-  We further show applications of Theorem 1 and Theorem 2 to obtain generalization bounds on
underdetermined ℓp linear regression (Theorem 3), kernel regression (Theorem 4), and overparameterized two-layer ReLU neural networks (Theorem 5). These bounds match or expand the type of
scenarios where we can rigorously establish the phenomenon of benign overfitting.

2 RELATED WORKS

**Optimization. Theoretically analyzing the training process of most machine learning models is**
a challenging problem as the loss landscapes are highly non-convex. One approach to studying
non-convex optimization problems is to use the Polyak-Łojasiewicz (PL) condition (Polyak, 1963),
which characterizes the local geometry of loss landscapes and ensures the existence of global minima.
It is shown in (Karimi et al., 2016) that GD admits linear convergence for a class of optimization
objective functions under the PL condition. In this paper, we consider the Łojasiewicz gradient
inequality (Lojasiewicz, 1965), an extended version of the PL condition that can be applied to more
cases. In particular, we modify the original LGI to get length estimates of the optimization paths,
which will be used to derive length-based generalization bounds.

**Generalization. Traditional VC dimension-based generalization bounds depend on the number**
of parameters and will be vacuous for huge models such as overparameterized neural networks. To
overcome this limitation, several non-vacuous generalization bounds are proposed. For example, the
norm/margin-based generalization bounds (Neyshabur et al., 2015; Bartlett et al., 2017; Golowich


-----

et al., 2018; Neyshabur et al., 2019), and the PAC-Bayes-based bounds (Dziugaite & Roy, 2017;
Neyshabur et al., 2018; Zhou et al., 2019; Rivasplata et al., 2020). However, these bounds tend to
focus less on optimization, e.g., norm-based generalization bounds may not discuss how small-norm
solutions are obtained through practical training. In this paper, we connect the optimization and
generalization by deriving generalization bounds based on the optimization trajectory length.

**Interface between optimization and generalization.** Implicit bias builds the bridge between
optimization and generalization, which has been widely studied to explain the generalization ability
of machine learning models. Recent works (Soudry et al., 2018a;b; Nacson et al., 2019a;b; Lyu
& Li, 2020) showed that linear classifiers or deep neural networks trained by GD/SGD maximizes
the margin of the separating hyperplanes and therefore generalizes well. Other works (Arora et al.,
2019; Zou & Gu, 2019; Cao & Gu, 2020; Ji & Telgarsky, 2020; Chen et al., 2021) concentrated on
overparameterized neural networks, showing that GD/SGD trajectories fall within the NTK regime,
where the minimizer has good generalization due to the small “complexity” of the parameter space. In
this work, we focus on specific conditions on loss functions under which we can connect optimization
and generalization based on the lengths of optimization paths.

3 MAIN RESULTS

In this section, we aim to address optimization and generalization together by proving results based
on the Uniform-LGI property. We begin with describing notations and the problem setting in Section
3.1. Then we give an explicit length estimate for the gradient flow trajectory in Section 3.2. Lastly,
we show a length-based generalization bound in Section 3.3.

3.1 SETUP AND NOTATIONS

Consider a supervised learning problem on a hypothesis space F = _f_ (w, ·) : R[d] _−→_ R | w ∈W,
whereset S = W is a parameter set in Euclidean space. Given a loss function(xi, yi) _i=1_ _ℓ_ : R × R −→ R, and a training
a joint distribution { _}[n]_, the goal of ERM is to optimize the empirical loss function[⊆] [R][d][ ×][ R][ with][ n][ independent and identically distributed (i.i.d.) samples from]n(w) on S:
_D_ _L_


arg min _n(w) := [1]_
_w_ _L_ _n_


_ℓ_ (f (w, xi), yi) .
_i=1_

X


We assume that each input vector and label have bounded norms. Specifically, we assume that, for
any (x, y) following D, ∥x∥ = 1, |y| ≤ 1. This can be achieved by data normalization. To simplify
the analysis, we optimize the empirical loss by gradient flow[2]:

_dw[(][t][)]_

= _n(w[(][t][)]), t_ [0, + ), (1)
_dt_ _−∇L_ _∈_ _∞_

where w[(][t][)] is the parameter value at time t, w[(0)] is the initialization parameter set, and _n(w) is_
_∇L_
the gradient of _n(w) with respect to w. Given the gradient flow, we may define the length of the_
_L_ _t_

gradient flow curve from w[(0)] to w[(][t][)] as len(w[(0)], t) := 0 _[dw]ds[(][s][)]_ _ds._

**Notations. We use ∥·∥** to denote the ℓ2 norm of a vector or the spectral norm of a matrix, and useR
_∥·∥F to denote the Fronenius norm of a matrix. For a set S ∈_ R[n], we use ∂S to denote its boundary.
We use d( 1, 2) to represent the Euclidean distance between two sets 1, 2, which is defined as
_S_ _S_ _S_ _S_
_d(_ 1, 2) = inf _s1_ _s2_ : s1 1, s2 2 . For two vectors, we use _,_ to denote their inner
product. LetS _S_ _λ {min ∥(A) − and λ∥max( ∈SA) be the smallest and largest eigenvalues of a symmetric matrix ∈S_ _}_ _⟨_ _⟩_ _A._
For any positive integer n, we denote [n] = {1, 2, . . ., n}. We use O(·) to stand for Big-O notation.

The classical LGI gives a lower bound of the gradient of a differentiable function based on its value
above its minimum. Many functions, e.g., real analytic functions and subanalytic functions, satisfy
this property, at least locally (Bolte et al., 2007). Here, we require a global version of this inequality
as a condition to control the gradient flow trajectory. Let us define this notion below.
**Definition 1 (Uniform-LGI). A function L(w) satisfies Uniform-LGI on a set S with two constants**
_c > 0 and θ ∈_ [ [1]2 _[,][ 1)][, if][ ∥∇L][(][w][)][∥≥]_ _[c][ (][L][(][w][)][ −]_ [min][v][∈W][ L][(][v][))][θ][,][ ∀][w][ ∈S][.]

2Here we assume the existence of the gradient flow and the limit limt
_−→∞_ _[w][(][t][)][.]_


-----

In the special case when θ = 1/2 and c = 2µ, the Uniform-LGI corresponds to the µ-PL condition

_[√]_
(Karimi et al., 2016). Here we give examples of functions satisfying the Uniform-LGI but not the PL
condition: L(w) = w[2][k], k ∈ Z[+]. When k ≥ 2, L(w) satisfies the Uniform-LGI on R with c = 2k
and θ = 1 − 1/2k.

3.2 GRADIENT FLOW TRAJECTORY LENGTH ESTIMATE

Our first result shows that under the uniform-LGI condition, gradient flow returns a global minimum
with an explicit estimate of the gradient flow trajectory length as in the following theorem.

**Theorem 1. For any initialization w[(0)], if Ln(w) satisfies Uniform-LGI on a closed set Sn with**
_constants cn, θn such that Sn ⊇_ _B_ _w[(0)], rn(w[(0)])_ _, where_
  _n(w[(0)])_  minw _n(w)_ 1−θn

_rn(w[(0)]) =_ _L_ _−_ _L_ _,_ (2)

_cn(1_ _θn)_

  _−_ 

_then w[(][t][)]_ _converges to a global minimum w[(][∞][)]_ _with the trajectory length upper bounded by_
_len(w[(0)],_ ) _rn(w[(0)]). The convergence rate of_ _n(w[(][t][)]) is given by:_
_∞_ _≤_ _L_

_n[t][ ]_

_θn = [1]_ _n(w[(][t][)])_ min _n(w[(0)])_ min _,_

1 2 [:] _L_ _−_ _w_ _[L][n][(][w][)][ ≤]_ _[e][−][c][2]_ _L_ _−_ _w_ _[L][n][(][w][)]_
_n(w[(][t][)])_ min _n(w[(0)])_ min _,_

_where2_ _[< θ] Mn[n] =[ <] c[ 1 :][2]n[(2][θ]L[n]_ _−n(ww[(0)][L])_ _[n][(][w]min[)][ ≤]w[(1 +]n(w[ M])_ _[n]2[t]θ[)]n[−]−[1]1[/].[(2][θ][n][−][1)][ ]L_ _−_ _w_ _[L][n][(][w][)]_

_[−]_ [1)] _L_ _−_ _L_

The proof of Theorem 1 is given in Appendix B.  

This theorem yields that once the loss function satisfies the Uniform-LGI around the initialization,
gradient flow returns a global minimum with an explicit trajectory length estimate. Moreover, for
any fixed n, len(w[(0)], ∞) decreases as the initial loss value decreases. The calculation of cn, θn for
different n should be analyzed case by case based on the loss landscape, as shown in Section 4.

3.3 HOW DOES THE OPTIMIZATION PATH LENGTH AFFECT GENERALIZATION?

The Uniform-LGI property allows us to have an upper bound for the gradient flow trajectory length.
In this subsection, we use the length estimate we get in Theorem 1 to study generalization.

Throughout this subsection, we assume that there exists an almost everywhere differentiable function
Ψ : R[p][+][q] _−→_ R such that f (w, ·) can be represented in the following form,

_∀x ∈_ R[d], _f_ (w, x) = Ψ _α1[⊤][x, . . ., α]p[⊤][x, β][1][, . . ., β][q]_ (3)

withvec is the vectorization operator that concatenates all elements into a column vector. α1, . . ., αp ∈ R[d], β1, . . ., βq ∈ R, and w = vec (  _{α1, . . ., αp, β1, . . ., β_ _q}) ∈_ R[pd][+][q]. Here

A wide class of functions can be represented in the form (3). Examples include linear functions
_f_ (w, x) = w[⊤]x and two-layer neural networks f (w, x) = a[⊤]ϕ (Wx) with a ∈ R[m], W ∈ R[m][×][d].

**Additional notations. For the loss function ℓ, we use Lℓ(S) to denote its Lipschitz constant (the**
maximal gradient norm) on with respect to its first argument. For Ψ in (3), we define LΨ( ) :=
_S_ _S_
_⊤_
_L[(1)]Ψ_ [(][S][)][,][ · · ·][, L]Ψ[(][p][)][(][S][)][, L]Ψ[(][p][+1)](S), · · ·, L[(]Ψ[p][+][q][)](S), where L[(]Ψ[i][)][(][S][)][ is the Lipschitz constant of]

Ψ on S with respect to the i-th variable. Let w[(0)] := vec _α1[(0)][, . . ., α]p[(0)][, β]1[(0)][, . . ., β]q[(0)]_ and

use (w) to denote the expected loss E(x,y) [ℓ(f (w, xn), y)]. For a = (a1, . . ., apo)[⊤] and
_LD_ _∼D_
_b = (b1, . . ., bq)[⊤], we define Sa,b = {w : ∀i ∈_ [p], j ∈ [q], ∥αi∥≤ _ai, |βj| ≤_ _bj}, and Ma,b =_
supw _a,b,_ _x_ 1, _y_ 1 ℓ (f (w, x), y).
_∈S_ _∥_ _∥≤_ _|_ _|≤_

In the following theorem, we give a path-based generalization bound if one has a length estimate
for optimization trajectory length len(w[(0)], ∞). Based on the Uniform-LGI property, Theorem 1
serves as a sufficient condition to ensure that the trajectory length can be estimated. In that sense, we
connect optimization with generalization. The rigorous statement is as follows:


-----

**Theorem 2. Consider an initialization method so that w[(0)]** _is independent of the training samples and_
_suppose that for any δ ∈_ (0, 1), there exist Mδ, Rn,δ > 0 such that _w[(0)]_ _≤_ _Mδ, len(w[(0)], ∞) ≤_
_Rn,δ with probability at least 1 −_ _δ over the initialization and the training samples. Then, we have_
_with probability at least 1 −_ _δ over initialization and the training samples, the generalization error of_
_the global minimum w[(][∞][)]_ _is bounded as:_


2RLℓ( _a,b)_ _LΨ(_ _a,b)_
(w[(][∞][)]) min sup _S_ _∥_ _S_ _∥_ +3Ma,b
_LD_ _≤_ _w_ _a_ + _b_ _R[2]_ _√n_

_[L][n][(][w][)+]∥_ _∥[2]_ _∥_ _∥[2]≤_


3(p + q) + log(2/δ)

_,_
2n

(4)


_where R =_


2(Mδ + Rn,δ).


The proof of Theorem 2 is given in Appendix C. There are five key terms in (4). To make the bound
easier to understand, we discuss them in sequence:

-  Mδ: This is a high probability upper bound for the ℓ2 norm of the initialized vector _w[(0)]_ . In
practice, for commonly used initialization methods in deep learning, such as Xavier initialization
(Glorot & Bengio, 2010) and Kaiming initialization (He et al., 2015), the norm _w[(0)]_ is uniformly
bounded with high probability.

-  Rn,δ: This is an high probability upper bound for the optimization path length over the initialization
and training samples. Theorem 2 assumes that such a bound exists, and show that it immediately
implies a generalization estimate based on this bound. Theorem 1 gives a sufficient condition to
obtain such a path estimate, in which case it depends on the Uniform-LGI constants cn, θn and
data distribution D. The asymptotic analysis of Rn,δ is problem-dependent, and we provide several
examples in Section 4.

-  Lℓ( _a,b) and Ma,b: They depend on the loss function ℓ. For example, for any loss function_
_S_
_ℓ_ : R × R −→ [0, 1] that is 1-Lipschitz in the first argument, Lℓ(Sa,b) = Ma,b = 1.

-  _LΨ(_ _a,b)_ : This term relies on the structure of f . For example, when f is linear, Ψ(x, y) = x + _y,_
_∥_ _S_ _∥_
then _LΨ(_ _a,b)_ = _√2._
_∥_ _S_ _∥_

**Remark 1. Theorem 2 shows that if the optimization trajectory length is small, then this implies a**
_good generalization bound. First, note that our generalization bound concerns the weights of the_
_final model. Nevertheless, by the convergence analysis in Theorem 1, our framework can be extended_
_to derive generalization estimates that evolves according the length of time of training. See Appendix_
_C.1 for additional results. Second, we emphasize again that the generalization bound in Theorem_
_2 only relies on a path length estimate, and theorem 1 gives a sufficient condition to ensure that_
_such an estimate exists. Consequently, as long as one can obtain path length estimates (say from_
_stochastic analysis of SGD), one can still apply Theorem 2 to obtain generalization estimates. Path_
_length estimates for other types of training algorithms is an interesting future direction._

4 APPLICATIONS

In this section, we apply the framework to three models. To obtain clean expressions of the generalization bound in terms of the sample size n, it is helpful to consider a range of n relating to the
dimension d. In particular, we consider underdetermined systems where the ratio n/d remains finite
unless stated otherwise:

_γ0, γ1_ (0, ), s.t. _d, γ0d_ _n = n(d)_ _γ1d._
_∃_ _∈_ _∞_ _∀_ _≤_ _≤_

Note that this setting is non-asymptotic since we do not require d to be infinite. For each application,
to simplify the analysis, we evaluate the test error on a loss function _ℓ[˜]_ : R × R −→ [0, 1] is 1-Lipschitz
(on the first argument) with _ℓ[˜](y, y) = 0. The following steps are taken in our framework:_

**Step 1. Establish the Uniform-LGI property and find the Uniform-LGI constants cn and θn.**

**Step 2. Apply Theorem 1 to get optimization results to estimate path length.**

**Step 3. Apply Theorem 2 to get generalization results from the path length estimates.**


-----

4.1 UNDERDETERMINED ℓp LINEAR REGRESSION

We begin with an underdetermined linear regression model f (w, x) = w[⊤]x with an ℓp loss function
(p ≥ 2 is an even positive integer):

_n_

_p_

arg min _n(w) := [1]_ _w[⊤]xi_ _yi_ _,_ (5)
_w∈R[d]_ _L_ _pn_ _i=1_ _−_

X   

where the input data matrix X = (x1, . . ., xn)[⊤] _∈_ R[n][×][d] has full row rank. Then the above regression
model has at least one global minimum with zero loss.

**Target function. Suppose the training data is generated from an underlying function g : R[d]** _−→_ R
with yi = g(xi), ∀i ∈ [n]. Let Y = (y1, · · ·, yn)[⊤], and assume that there exits c[∗] _> 0 such that_


arg min _n(w) := [1]_
_w∈R[d]_ _L_ _pn_


_i=1_


_∥Y∥≤_ _c[∗]_


_λmax(_ ). (6)
_XX_ _[⊤]_


The inequality (6) actually indicates that g is Lipschitz with a dimension independent Lipschitz
constant. Functions satisfying (6) include linear/non-linear functions. For example, g(x) = ϕ(x[⊤]w[∗]),
where w[∗] R[d] with _w[∗]_ 2 _c[∗]_ for some constant c[∗], and ϕ( ) is Lipschitz with ϕ(0) = 0.
_∈_ _∥_ _∥_ _≤_ _·_
**Assumption 1. The entries of X are i.i.d. subgaussian random variables with zero mean, unit**
_variance, and subgaussian moments[3]_ _bounded by 1._

This assumption allows us to study the spectral properties of the sample matrix by random matrix
theory. Especially, when n(d)/d converges to some constant γ ∈ (0, 1), the Marchenko–Pastur law
(Marˇcenko & Pastur, 1967) shows that λmin( )/n converges to (1 _γ)[2]_ almost surely. The
_XX_ _[⊤]_ _−_ _[√]_
non-asymptotic results are provided in (Rudelson & Vershynin, 2010).

Performing our three-step analysis, we get the following results:
**Theorem 3. Consider the undertermined ℓp linear regression model (5). Suppose that there exists a**
_universal constant c0_ 1 such that _d,_ _w[(0)]_ 2
_≥_ _∀_ _[≤]_ _[c][0][.]_

**_Step 1. Ln(w) satisfies the Uniform-LGI globally on R[d]_** _with_


_λmin(_ )
_XX_ _[⊤]_


_cn = p[1][−][1][/p]_


_θn = 1 −_ 1/p.


**_Step 2._** _n(w[(][t][)]) converges to zero linearly for p = 2 and sublinearly for p_ 4, i.e.,
_L_ _≥_

_p = 2,_ _n(w[(][t][)])_ exp 2λmin( )t/n _n(w[(0)]);_
_L_ _≤_ _−_ _p_ _XX_ _[⊤]_ _L_

_p_ 4, _n(w[(][t][)])_ (1 +  Mt)[−] _p−2_ _n(w[(0)]),_
_≥_ _L_ _≤_ _L_

_where M = p[1][−]_ _p[2] (p_ 2) _[λ][min][(]n[XX][ ⊤][)]_ _n(w[(0)])[1][−]_ _p[2] ._

_−_ _L_

**_Step 3. Under Assumption 1, for any target function that satisfies (6), we have with probability at_**
_least 1 −_ _δ −_ _τ_ _[d][−][n][+1]_ _−_ _τ_ _[d]_ _over the samples,_

log(1/δ)

E(x,y) _ℓ˜_ _f_ (w[(][∞][)], x), y _n[−][1][/p][]_ + _,_
_∼D_ _≤O_ _O_ r _n_ !
h  i 

_where τ ∈_ (0, 1) depends only on the subgaussian moment of the entries.

The proof of Theorem 3 is given in Appendix D.1. This theorem shows that compared to the PL
condition that corresponds to p = 2, the uniform-LGI is more general and can be applied to more
cases.

**Comparison. This result is related to (Bartlett et al., 2020) that studied the phenomenon of benign**
overfitting in high-dimensional ℓ2 linear regression. Our result coincides with theirs as both results
uncover some scenarios for benign overfitting in linear regression. In particular, (Bartlett et al.,

3The subgaussian moment of X is defined as inf _M ≥_ 0 | Ee[tX] _≤_ _e[M][2][t][2][/][2], ∀t ∈_ R .
n o


-----

2020) focused on the minimum ℓ2 norm estimator. They showed that, if the eigenvalue sequence of
the covariance operator Σ := E[xx[⊤]] have suitable decay rates, then the generalization error will
decrease to zero as n increases. In contrast, our result differs from theirs in the problem settings.
Specifically, we do not assume the minimum norm property and consider the optimization process that
is neglected in (Bartlett et al., 2020). Also, we evaluate the generalization error of the convergence
point on a globally Lipschitz loss function. In our settings, entries of each input x are i.i.d. random
variables, meaning that Σ = Id _d without decaying eigenvalues. The requirement of i.i.d. entries_
_×_
for each input is a limitation due to the lack of non-asymptotic results of the spectral properties for
random matrices with non-i.i.d. entries. However, this limitation can be overcome if one considers
the asymptotic results. Moreover, our result works for ℓp loss functions with any even positive integer
_p. This expands the understanding of benign overfitting in the high-dimensional setting._

4.2 KERNEL REGRESSION

Consider a positive definite kernel k : X × X −→ R with its corresponding feature map φ : R[d] _−→F_
satisfying ⟨φ(x), φ(y)⟩F = k(x, y). We assume that |k(x, x)| ≤ 1, ∀x ∈X . Let H be the
reproducing kernel Hilbert space (RKHS) with respect to k. If F = R[s], then the kernel regression
model with ℓp loss is to solve the following problem

_n_

_p_

arg min _n(w) := [1]_ _w[⊤]φ(xi)_ _yi_ _,_ (7)
_w∈R[s]_ _L_ 2n _i=1_ _−_

X   

where p ≥ 2 is an even integer. Similar to the ℓp linear regression case, we consider the following
target function class:

**Target function. Suppose the training data is generated from an underlying function g : R[d]** _−→_ R
with yi = g(xi), ∀i ∈ [n]. We further assume that there exits a constant c[∗] _> 0 such that_

_c[∗]_ _λmax(k(_ _,_ )), (8)
_∥Y∥≤_ _·_ _X_ _X_

where k( _,_ ) is the n _n kernel matrix on_ p with k( _,_ )ij = k(xi, xj).
_X_ _X_ _×_ _X_ _X_ _X_

Here, we list an example of class of functions that satisfies (8): g(x) = ϕ(φ(x)[⊤]w[∗]) where w[∗] _∈_ R[s]
with ( _s)_ _w[∗]_ 2 _c[∗]_ for some constant c[∗], and ϕ( ) is Lipschitz with ϕ(0) = 0.
_∀_ _∥_ _∥_ _≤_ _·_

To get the generalization results of kernel regression, we will discuss two types of kernels separately:
radial basis function (RBF) kernel and inner product kernel.

**RBF kernel. We study the RBF kernel of the form k(x, y) = ϱ (∥y −** _x∥) for a certain RBF ϱ. For_
the input data, we define the separation distance of X as SD := [1]2 [min][i][̸][=][j][ ∥][x][i][ −] _[x][j][∥]_ _[,][ ∀][i, j][ ∈]_ [[][n][]][.]

**Inner product kernel. For the inner product kernel, we consider k(x, y) = ϱ** _x[⊤]d_ _y_ .
 


arg min _n(w) := [1]_
_w∈R[s]_ _L_ 2n


_i=1_


Following (El Karoui et al., 2010), we make the following assumption on the function ϱ:
**Assumption 2. ϱ is C** [3] _in a neighborhood of 0 with ϱ(0) = 0, ϱ(1) > ϱ[′](0) ≥_ 0, ϱ[′′](0) ≥ 0.

Now we are ready to apply our three-step analysis to get optimization and generalization results. For
the RBF kernel, the generalization result depends on the separation distance of the samples. For the
inner product kernel, we study the high-dimensional random kernel matrix.
**Theorem 4. Consider the kernel regression model (7). Suppose that there exists a universal constant**
_c0_ 1 such that _s,_ _w[(0)]_ 2
_≥_ _∀_ _[≤]_ _[c][0][.]_

**_Step 1. Ln(w) satisfies the Uniform-LGI globally on R[s]_** _with_


_λmin(k(_ _,_ ))
_X_ _X_


_cn = p[1][−][1][/p]_


_θn = 1 −_ 1/p,


_where cn is controlled by the kernel and input samples._

**_Step 2._** _n(w[(][t][)]) converges to zero linearly for p = 2 and sublinearly for p_ 4, i.e.,
_L_ _≥_

_p = 2,_ _n(w[(][t][)])_ exp ( 2λmin(k( _,_ ))t/n) _n(w[(0)]);_
_L_ _≤_ _−_ _p_ _X_ _X_ _L_

_p_ 4, _n(w[(][t][)])_ (1 + Mt)[−] _p−2_ _n(w[(0)]),_
_≥_ _L_ _≤_ _L_


-----

_where M = p[1][−]_ _p[2] (p_ 2) _[λ][min][(][k]n[(][X]_ _[,][X]_ [))] _n(w[(0)])[1][−]_ _p[2] ._

_−_ _L_

**_Step 3. For any target function that satisfies (8) we have:_**

-  For the RBF kernel[4], suppose that ϱ : R 0 R 0 is a decreasing function and ϱ ( _x_ ) _L[1](R[d])._
_If there exists two positive constants qmin≥_ _− and→_ _q≥max such that SD ∈_ [qmin, qmax∥] for all∥ _∈_ _n, then_
_with probability at least 1 −_ _δ over the samples,_

log(1/δ)

E(x,y) _ℓ˜_ _f_ (w[(][∞][)], x), y _n[−][1][/p][]_ + _._
_∼D_ _≤O_ _O_ r _n_ !
h  i 

-  For the inner product kernel, under Assumption 1 and Assumption 2, if d is large enough and δ > 0
_is small enough such that d[−][1][/][2][  ][√]3δ[−][1][/][2]_ + log[0][.][51] _d_ _≤_ 0.5(ϱ(1) _−_ _ϱ[′](0)), then with probability_

_at least 1_ _δ_ _d[−][2]_ _over the samples, we have_
_−_ _−_ 

log(1/δ)

E(x,y) _ℓ˜_ _f_ (w[(][∞][)], x), y _n[−][1][/p][]_ + _._
_∼D_ _≤O_ _O_ r _n_ !
h  i 

The proof of Theorem 4 is given in Appendix D.2.
**Example 1. RBF kernels satisfying the conditions and assumptions in Theorem 4 include (1)**
_Gaussian: ϱ(r) = e[−][ρr][2]_ _, ρ > 0; (2) (inverse) Multiquadrics: ϱ(r) = (ρ + r[2])[β/][2], ρ > 0, β ∈_
R\2N, β < −d.

_Inner product kernels satisfying the conditions and assumptions in Theorem 4 include (1) Polynomial_
_kernel: ϱ(r) = r[β], β ∈_ Z[+], β ≥ 2; (2) NTK corresponding to Two-layer ReLU neural networks on
S[d][−][1](√d): ϱ(r) = _[r][(][π][−][arccos(]2π_ _[r][))]_ _._

**Comparison. The p = 2 result of the inner product kernel is related to (Liang & Rakhlin, 2020)**
who derived generalization bounds for the minimum RKHS norm estimator. They showed that
when the data covariance matrix and the kernel matrix enjoy certain decay of the eigenvalues, the
generalization bound vanishes as n goes to infinity. For example, for exponential kernel and the
covariance matrixα Σ := E[xx[⊤]] with the j-th eigenvalue λj(Σ) = j[−][α], the ℓ2 generalization bound
becomes O(n[−] 2α+1 ) when α ∈ (0, 1) and n > d. In comparison, we get an optimal generalization

bound O _n[−][1][/][2][]_ for identity covariance matrix on a globally Lipschitz loss function. Again, we
emphasize that the i.i.d. assumption can be relaxed if more new results of random matrix theory are
 
available. Further, we extend the works in (Liang & Rakhlin, 2020) by proving a new result of the
RBF kernel. Note that the result of the RBF kernel is not under the high-dimensional setting; thus it
is not a direct adaptation of (Liang & Rakhlin, 2020), and the proof itself is of independent interest.


4.3 OVERPARAMETERIZED TWO-LAYER NEURAL NETWORKS

The first two applications are on traditional machine learning models. Indeed, our framework can be
applied not only to linear/kernel regression but also to neural network models. In this subsection, we
use our framework to study shallow neural networks in an overparameterization regime.

First, define a two-layer ReLU neural network under the NTK parameterization (Lee et al., 2019):

1
_f_ (w, x) = _√mw2[⊤][ϕ][(][W][1][x][)][,]_

where ϕ(x) = max{0, x}, x ∈ R[d] is the input, W1 ∈ R[m][×][d], w2 ∈ R[m] are parameters, w =
vec ({W1, w2}) ∈ R[m][(][d][+1)], and m is the width (number of hidden units).

We consider minimizing the quadratic loss by gradient flow:


arg min _n(w) := [1]_
_w_ _L_ 2n

4Here d is fixed and n is varied.


(f (w, xi) _yi)[2]._ (9)
_−_
_i=1_

X


-----

**Random initialization. W1[(0)]** is drawn from Gaussian N (0, _d[1]_ [I][m][×][d][)][ and][ w]2[(0)],i [are drawn i.i.d. from]

uniform distribution U _{−1, 1}, ∀i ∈_ [m].

Following the setting in (Du et al., 2019), we only train the hidden layer W1 and leave the output
layer w2 as random initialization to simplify analyses.

**NTK matrix. The NTK matrix Θ(t) is defined as: Θij(t) =** _W1_ _f_ (w[(][t][)], xi), _W1_ _f_ (w[(][t][)], xj),
_∇_ _∇_
and denote Θ by the limiting matrix[5]: Θij = x[⊤]i _[x][j][E]w∼N (0,_ _d[1]_ [I][d][)] _ϕ[′](w[⊤]xi)ϕ[′](w[⊤]xj)_ _, ∀i, j ∈_ [n].

Similarly, we apply our three-step analysis to derive the following results. 

[b] [b]

**Theorem 5. Consider the two-layer ReLU neural network model (9). For any δ ∈** (0, 1), if

_m ≥_ poly _n, λ[−]min[1]_ [(]Θ)[b] _, δ[−][1][], then_


**_Step 1. With probability at least 1_** _δ over training samples and random initialization,_ _n(w[(][t][)])_
_−_ _L_
_satisfies the Uniform-LGI on_ _w[(][t][)]_ : t ≥ 0 _with_


_cn =_ _λmin(Θ)[b]_ _/n,_ _θn = 1/2._
q

**_Step 2._** _n(w[(][t][)]) converges to zero with a linear convergence rate:_
_L_


_n(w[(][t][)])_ exp _λmin(Θ)[b]_ _t/n_ _n(w[(0)])._
_L_ _≤_ _−_ _L_
 

**_Step 3._** _Under Assumption 1, for any target function that satisfies (6), if γ1_ (0, 1), then with
_probability at least 1 −_ _δ −_ _τ_ _[d][−][n][+1]_ _−_ _τ_ _[d]_ _over the samples and random initialization, ∈_

log(n/δ)

E(x,y) _ℓ˜_ _f_ (w[(][∞][)], x), y _,_
_∼D_ _≤O_ r _n_ !
h  i

_where τ ∈_ (0, 1) depends only on the subgaussian moment of the entries.

The proof of Theorem 5 is deferred to Appendix D.3.


**Comparison. Our result is related to (Arora et al., 2019), which gave an NTK-based generalization**
bound for overparameterized two-layer ReLU neural networks. This result matches with theirs in the
sense that we discover some underlying functions that are provably learnable. Examples of learnable
target functions in (Arora et al., 2019) include polynomials y = (β[⊤]x)[p], non-linear activations
_y = cos(β[⊤]x) −_ 1, y = ϕ[˜](β[⊤]x) with _ϕ[˜](z) = z · arctan(z/2), ∥β∥≤_ 1. Our result, furthermore,
expands the target function class that is provably learnable since we only require _ϕ[˜] to be Lipshcitz. In_
addition, they set the standard deviation of the initialization to be at most O(1/n), whereas we use a
different initialization with order O(1/√d) that is more often applied in practice. Our result is also

related to (Liu et al., 2020), which proved that overparameterized deep neural networks satisfy the
PL condition. Further, we extend this work by analyzing the generalization based on the length of
optimization trajectories.

5 CONCLUSION

In this work, we address when and why does an optimization algorithm finds a minimum with good
generalization performance. For this problem, we propose a framework to bridge optimization and
generalization based on the trajectory length. The pivotal component is the Uniform-LGI property:
a condition on the loss function, by which we show that gradient flow returns a global minimum
with an explicit length estimate. Further, we derive a length-based generalization bound given such a
length estimate. Finally, we apply the framework to three machine learning models with certain target
functions. By estimating the trajectory lengths, we get non-vacuous generalization bounds that do not
suffer from the curse of dimensionality. This framework is not a direct variant of the NTK method,
and the results show that our framework is favorable for inducing connections between optimization
and generalization.

5Here λmin(Θ)b changes with n.


-----

6 REPRODUCIBILITY STATEMENT

For the experiments, details about the models and the synthetic data are described in Appendix A.1
and Appendix A.2. For the theoretical results, we have included clear explanations of all assumptions.
The complete proofs are provided in Appendix B, C & D.

REFERENCES

Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. In Advances in neural information processing systems,
pp. 6158–6169, 2019.

Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In International
_Conference on Machine Learning, pp. 322–332. PMLR, 2019._

Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6240–6249, 2017.

Peter L Bartlett, Philip M Long, Gábor Lugosi, and Alexander Tsigler. Benign overfitting in linear
regression. Proceedings of the National Academy of Sciences, 117(48):30063–30070, 2020.

Jérôme Bolte, Aris Daniilidis, and Adrian Lewis. The łojasiewicz inequality for nonsmooth subanalytic functions with applications to subgradient dynamical systems. SIAM Journal on Optimization,
17(4):1205–1223, 2007.

Yuan Cao and Quanquan Gu. Generalization error bounds of gradient descent for learning overparameterized deep relu networks. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 34, pp. 3349–3356, 2020.

Zixiang Chen, Yuan Cao, Difan Zou, and Quanquan Gu. How much over-parameterization is
sufficient to learn deep relu networks? In International Conference on Learning Representations,
2021.

Benedikt Diederichs and Armin Iske. Improved estimates for condition numbers of radial basis
function interpolation matrices. Journal of Approximation Theory, 238:38–51, 2019.

Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2019.

Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. In Proceedings
_of the 33rd Annual Conference on Uncertainty in Artificial Intelligence (UAI), 2017._

Noureddine El Karoui et al. The spectrum of kernel random matrices. Annals of statistics, 38(1):
1–50, 2010.

Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and
_Statistics, 2010._

Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. In Conference On Learning Theory, pp. 297–299. PMLR, 2018.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
_conference on computer vision, pp. 1026–1034, 2015._

Ziwei Ji and Matus Telgarsky. Polylogarithmic width suffices for gradient descent to achieve
arbitrarily small test error with shallow relu networks. In International Conference on Learning
_Representations, 2020._


-----

Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximalgradient methods under the polyak-łojasiewicz condition. In Joint European Conference on
_Machine Learning and Knowledge Discovery in Databases, pp. 795–811. Springer, 2016._

Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selection.
_Annals of Statistics, pp. 1302–1338, 2000._

Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes.
Springer Science & Business Media, 2013.

Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha SohlDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in Neural Information Processing Systems, volume 32, 2019.

Tengyuan Liang and Alexander Rakhlin. Just interpolate: Kernel “ridgeless” regression can generalize.
_The Annals of Statistics, 48(3):1329–1347, 2020._

Tengyuan Liang, Alexander Rakhlin, and Xiyu Zhai. On the multiple descent of minimum-norm
interpolants and restricted lower isometry of kernels. In Conference on Learning Theory, pp.
2683–2711. PMLR, 2020.

Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Loss landscapes and optimization in over-parameterized
non-linear systems and neural networks. arXiv preprint arXiv:2003.00307, 2020.

S. Lojasiewicz. Ensembles semi-analytiques. IHES notes, 1965.

Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks.
In International Conference on Learning Representations, 2020.

V A Marˇcenko and L A Pastur. Distribution of eigenvalues for some sets of random matrices.
_Mathematics of the USSR-Sbornik, 1(4):457–483, apr 1967._

Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT
press, 2018.

Stephen J Montgomery-Smith. The distribution of rademacher sums. Proceedings of the American
_Mathematical Society, 109(2):517–522, 1990._

Mor Shpigel Nacson, Suriya Gunasekar, Jason D Lee, Nathan Srebro, and Daniel Soudry. Lexicographic and depth-sensitive margins in homogeneous and non-homogeneous deep models. arXiv
_preprint arXiv:1905.07325, 2019a._

Mor Shpigel Nacson, Nathan Srebro, and Daniel Soudry. Stochastic gradient descent on separable
data: Exact convergence with a fixed learning rate. In The 22nd International Conference on
_Artificial Intelligence and Statistics, pp. 3051–3059. PMLR, 2019b._

Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376–1401, 2015.

Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A PAC-bayesian approach to
spectrally-normalized margin bounds for neural networks. In International Conference on Learning
_Representations, 2018._

Behnam Neyshabur, Zhiyuan Li, Srinadh Bhojanapalli, Yann LeCun, and Nathan Srebro. The role
of over-parametrization in generalization of neural networks. In International Conference on
_Learning Representations, 2019._

B. Polyak. Gradient methods for the minimisation of functionals. Ussr Computational Mathematics
_and Mathematical Physics, 3:864–878, 1963._

Omar Rivasplata, Ilja Kuzborskij, Csaba Szepesvári, and John Shawe-Taylor. Pac-bayes analysis
beyond the usual bounds. arXiv preprint arXiv:2006.13057, 2020.


-----

Mark Rudelson and Roman Vershynin. Smallest singular value of a random rectangular matrix.
_Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of_
_Mathematical Sciences, 62(12):1707–1739, 2009._

Mark Rudelson and Roman Vershynin. Non-asymptotic theory of random matrices: extreme singular
values. In Proceedings of the International Congress of Mathematicians 2010 (ICM 2010) (In 4
_Volumes) Vol. I: Plenary Lectures and Ceremonies Vols. II–IV: Invited Lectures, pp. 1576–1602._
World Scientific, 2010.

Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. The Journal of Machine Learning Research, 19(1):
2822–2878, 2018a.

Daniel Soudry, Elad Hoffer, and Nathan Srebro. The implicit bias of gradient descent on separable
data. In International Conference on Learning Representations, 2018b.

Alexander Tsigler and Peter L Bartlett. Benign overfitting in ridge regression. arXiv preprint
_arXiv:2009.14286, 2020._

Holger Wendland. Scattered Data Approximation. Cambridge Monographs on Applied and Computational Mathematics. Cambridge University Press, 2004.

Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P. Adams, and Peter Orbanz. Non-vacuous generalization bounds at the imagenet scale: a PAC-bayesian compression approach. In International
_Conference on Learning Representations, 2019._

Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural
networks. In Advances in Neural Information Processing Systems, 2019.


-----

A EXPERIMENTS

A.1 EXPERIMENTS SETTINGS

In this subsection, we describe the details of our experiments in the introduction part (Figure 1).

**Linear Regression model. The model that we use is underdetermined ℓ2 linear regression without**
regularization. The data set consists of 130 data points (xi, yi) _i=1_
_xi(_ _i_ [130]) are uniformly drawn from the 200-dimensional unit sphere. All labels { _}[130]_ _[⊆]_ [R][200][ ×][ R] y[. The inputs]i( _i_ [130])
_∀_ _∈_ _∀_ _∈_
are generated by a linear target function yi = β[⊤]xi with some β ∈ R[200] that satisfies ∥β∥ = 1.
We train this model by gradient descent w[(][k][+1)] = w[(][k][)] _η_ _n(w[(][k][)]) with step size η = 0.05_
_−_ _∇L_
on the mean square loss (MSE). Entries of w[(0)] are initialized i.i.d. from Gaussian distribution
_N_ (µ, σ[2]) with µ = 2[−][5], 2[−][2], 2[1], 2[4] and σ = 2[−][5], 2[−][2], 2[1], 2[4] respectively. We stop the training
once the difference of loss between two consecutive steps _n(w[(][K][+1)])_ _n(w[(][K][)])_ _< 10[−][8]. Then_
_|L_ _−L_ _|_
len(w[(0)], ) is approximated by _k=1_
length len( ∞w[(0)], ∞), the training error after convergence and the test error on the convergence point.[∥][w][(][k][)][ −] _[w][(][k][−][1)][∥][. For each][ pair of][ µ][ and][ σ][, we record the]_

[P][K]

**Random feature model. In this experiment, the model that we consider is a two-layer ReLU neural**
network w2[⊤][ϕ][(][W][1][x][)][. Here][ ϕ][ is the ReLU function,][ W][1]
the last layer w2, and thus this can be viewed as a random feature model with features extracted[∈] [R][200][×][200][, w][2] _[∈]_ [R][200][, We only train]
by ϕ(W1x). The data set consists of 130 data points (xi, yi) _i=1_
_xi(_ _i_ [130]) are uniformly drawn from the 200-dimensional unit sphere. All labels { _}[130]_ _[⊆]_ [R][200][ ×][ R] y[. The inputs]i( _i_ [130])
_∀_ _∈_ _∀_ _∈_
are generated by a teacher network _w2[⊤][ϕ][(]W[c]1xi) with the same architecture for some_ _w2,_ _W1. We_
train only the top layer by gradient descent with momentum 0.9 and step seize 0.05 under the mean
square loss (MSE). Entries of w2[(0)] bare initialized i.i.d. from (µ, σ[2]) with µ = 5, b2, 1[c], 4 and
_N_ _−_ _−_
_σ = 2[−][5], 2[−][2], 2[1], 2[4]_ respectively. We stop the training once the difference of loss between two
consecutive steps is less than 10[−][8]. For each pair of µ and σ, we use the same method as on linear
regression to approximate the trajectory length. Then we record the training error after convergence
and the test error on the convergence point.

**Varied learning rate. In this experiment, we provide additional results by only changing the learning**
rate. Figure 2, 3 and 4 correspond to step size η = 0.01, 0.1, 0.5 respectively. We observe that the
optimization path lengths are nearly the same for both small learning rate and moderate learning rate,
and the numerical results are associate with our theory.

Figure 2: learning rate = 0.01


-----

Figure 3: learning rate = 0.1

Figure 4: learning rate = 0.5

A.2 MNIST CLASSIFICATION

In this subsection, we calculate the trajectory length on the MNIST classification problem to show
the relation between optimization and generalization under different initializations.

**Experiment settings. In this experiment, we train a two-layer ReLU neural network with 100 hidden**
units. We use stochastic gradient descent with mini-batch 64, momentum 0.9 and initial learning rate
0.1 to train the model. Parameters of the hidden layer are initialized by standard random initialization
method in Pytorch, and the parameters of the top layer are initialized i.i.d. from Gaussian distribution
_N_ (µ, σ[2]) with µ = 2[−][5], 2[0], 2[5] and σ = 2[−][5], 2[−][2], 2[1], 2[4] respectively. During training the model,
we reduce the learning rate by a factor 0.1 once learning stagnates. We stop the training once the
cross-entropy loss decreases to 0.01 or the number of epochs reaches 1000. For each pair of µ
and σ, we calculate the trajectory length by _k=1_ _w[(][k][)]_ _w[(][k][−][1)]_ . We plot the relation between
_−_
optimization and generalization in terms of the trajectory length in Figure 5.

[P][K]

Figure 5: MNIST classification


-----

From this figure, we can see that for different random initializations, short optimization paths is
associated with good generalization gap. This numerical result exhibits the generality of our theory
and suggests that the path length plays an important role to connect optimization and generalization.

B PROOF OF THEOREM 1

In this section we will prove Theorem 1, our proof is based on the next lemma, showing that the
gradient flow trajectory is always inside _n._
_S_

**Lemma B.1. For any initialization w[(0)], if** _n(w) satisfies Uniform-LGI on a closed set_ _n_
_L_ 1−θn _S_ _⊇_

_B_ _w[(0)], rn(w[(0)])_ _with constants cn, θn, where rn(w[(0)]) = [(][L][n][(][w][(0)][)][−]cn[min](1−[w]θ[ L]n)[n][(][w][)][)]_ _, then_
  


_w[(][t][)]_ _n,_ _t_ [0, ).
_∈S_ _∀_ _∈_ _∞_

_T = inf_ _t_ 0, w[(][t][)] _/_ _n_ _,_
_{_ _≥_ _∈S_ _}_


_Proof. Let_


then it is sufficient to prove that T = ∞. Otherwise, if T < ∞, then by the continuity of the curve
_{w[(][t][)]}t≥0, we know that w[(][T][ )]_ is in the boundary of Sn, therefore

len(w[(0)], T ) _d(w[(0)], ∂_ _n)_ _rn(w[(0)])._ (10)
_≥_ _S_ _≥_

Now we consider the following two cases.

_Case (i)._ _n(w[(][T][ )]) = minw_ _n(w). Since_ _n(w[(][t][)]) is non-increasing, we have_ _n(w[(][t][)]) =_
_L_ _L_ _L_ _L_
mint ∈ [0w L, Tn](, meaning thatw) and w[(][t][)] _T= = w ∞[(][T][ )], which contradicts withfor t ≥_ _T_ . Notice that T < Sn ∞ is a closed set, thus. _w[(][t][)]_ _∈Sn for_

_Case (ii)._ _n(w[(][T][ )]) > minw_ _n(w). By chain rule, we have for all t_ [0, T ],
_L_ _L_ _∈_

_d_ _n(w[(][t][)])_
_L_ = _n(w[(][t][)]), [dw][(][t][)]_

_dt_ _∇L_ _dt_
 


_dw[(][t][)]_

= _n(w[(][t][)])_
_−_ _∇L_ _dt_

_θn_ _dw[(][t][)]_
_cn_ _n(w[(][t][)])_ min
_≤−_ L _−_ _w_ _[L][(][w][)]_ _dt_


Then we can bound the trajectory length len(w[(0)], T ) as


_T_
len(w[(0)], T ) =

0

Z

_T_
_≤_ 0
Z


_dw[(][t][)]_

_dt_


) = _[dt]_

0 _dt_

Z

_T_ _−θn_

_n(w[(][t][)])_ min _d_ _n(w[(][t][)])_

_≤_ 0 _−_ _c[1]n_ _L_ _−_ _w_ _L_
Z 1  _[L][(][w][)]_ 1−θn 1−θn []

= _n(w[(0)])_ min _n(w[(][T][ )])_ min

_cn(1_ _θn)_ _L_ _−_ _w_ _−_ _L_ _−_ _w_
_−_  _[L][(][w][)]_  _[L][(][w][)]_

_< rn(w[(0)]),_
(11)


which contradicts with (10).

Now we begin to prove Theorem 1.


-----

_Proof. By Lemma B.1, we know that_ _n(w[(][t][)]) satisfies Uniform-LGI for all t_ [0, ). Then by
_L_ _∈_ _∞_
the proof of Lemma B.1, we have ∀t ∈ [0, ∞),

1 1−θn 1−θn []
len(w[(0)], t) _n(w[(0)])_ min _n(w[(][t][)])_ min _._
_≤_ _cn(1_ _θn)_ _L_ _−_ _w_ _−_ _L_ _−_ _w_

_−_  _[L][(][w][)]_  _[L][(][w][)]_ (12)

For the convergence rate, note that


_n(w[(][t][)])_ minw (w)
_L_ _−_ _L_ = _n(w[(][t][)]), [dw][(][t][)]_

_dt_ _∇L_ _dt_

 


2 (13)
= _n(w[(][t][)])_
_−_ _∇L_

2θn
_c[2]n_ _n(w[(][t][)])_ min _._
_≤−_ L _−_ _w_ _[L][(][w][)]_

Therefore
_−2θn_
_n(w[(][t][)])_ min _d_ _n(w[(][t][)])_ min _c[2]n[dt.]_
L _−_ _w_ _[L][(][w][)]_ L _−_ _w_ _[L][(][w][)]_ _≤−_

Integrating on both sides of the equation, we can get ∀t ∈ [0, ∞),

when θn = 2[1] [,]

_n[t][ ]_
_n(w[(][t][)])_ min _n(w[(0)])_ min ; (14)
_L_ _−_ _w_ _[L][n][(][w][)][ ≤]_ _[e][−][c][2]_ _L_ _−_ _w_ _[L][n][(][w][)]_

when [1]2 _[< θ][n][ <][ 1][,]_

_n(w[(][t][)])_ min _n(w[(0)])_ min _,_ (15)
_L_ _−_ _w_ _[L][n][(][w][)][ ≤]_ [(1 +][ Mt][)][−][1][/][(2][θ][n][−][1)][ ]L _−_ _w_ _[L][n][(][w][)]_

where M = c[2]n[(2][θ][n] _n(w[(0)])_ minw _n(w)_ 2θn−1.

_[−]_ [1)] _L_ _−_ _L_

Taking the limit on both sides of (14) and (15), since   w[(][∞][)] is the limit of w[(][t][)], we have

_n(w[(][∞][)]) = min_
_L_ _w_ _[L][n][(][w][)][.]_

Hence w[(][∞][)] is a global minimum. Taking the limit on both sides of (12), we get

len(w[(0)], ) _rn(w[(0)])._
_∞_ _≤_

This completes the proof.

C PROOF OF THEOREM 2

In this section, we will prove Theorem 2. This proof is based on the Rademacher complexity theory
and the covering number of ℓ2 balls. Now we introduce some known technical lemmas that are used
to build our proof.

The first lemma gives a generalization bound of a function class based on the Rademacher complexity,
which is proved in (Mohri et al., 2018).

**Lemma C.1. Consider a family of functions F mapping from Z to [a, b]. Let D denote the distribution**
_according to which samples are drawn. Then for any δ > 0, with probability at least 1 −_ _δ over the_
_draw of an i.i.d. sample S =_ _z1, . . ., zn_ _of size n, the following holds for all f_ _:_
_{_ _}_ _∈F_


Ez [f (z)]
_∼D_ _−_ _n[1]_


log(2/δ)

2n


_f_ (zi) 2 _S(_ ) + 3(b _a)_
_≤_ _R_ _F_ _−_
_i=1_

X


-----

_where_ _S(_ ) is the empirical Rademacher complexity with respect to the sample S, defined as:
_R_ _F_


_RS(F) = Eσ_


sup
_f_ _∈F_


_σif_ (zi)
_i=1_

X


_Here {σi}i[n]=1_ _[are i.i.d. random variables drawn from][ U]_ _[{−][1][,][ 1][}][.]_

In the next lemma, we prove a shifted version of the Ledoux-Talagrand contraction inequality (Ledoux
& Talagrand, 2013), which is useful to bound the length-based Rademacher complexity.

**Lemma C.2. Let g : R −→** R be a convex and increasing function. Let ϕi : R −→ R be L-Lipschitz
_functions, then for any bounded set T ⊂_ R and any t[(0)] _∈_ R, we have


_n_

_σi_ _ϕi(ti)_ _ϕi_ _t[(0)]i_ Eσ _g_
_−_  _≤_ 
_i=1_

X   

 

_n_

_σi_ _ϕi(ti)_ _ϕi_ _t[(0)]i_ 2LEσ
_−_  _≤_
_i=1_

X   




Eσ _g_





_and_

Eσ


_L_ sup



([t][−][t][(0)])[∈][T]



_σi_ _ti_ _t[(0)]i_
_i=1_ _−_

X 


_,_






sup
([t][−][t][(0)])[∈][T]

sup



([t][−][t][(0)])[∈][T]



_σi_ _ti_ _t[(0)]i_
_i=1_ _−_

X 


_._






sup
([t][−][t][(0)])[∈][T]



The special case for t[(0)] = 0 corresponds to the original Ledoux-Talagrand contraction inequality.
Here we prove a shifted version.

_Proof. First notice that_

_n_ _n_

Eσ g  sup _σi_ _ϕi(ti) −_ _ϕi_ _t[(0)]i_  = Eσ1,...,σn−1 Eσn g  sup _σi_ _ϕi(ti) −_ _ϕi_ _t[(0)]i_

([t][−][t][(0)])[∈][T] _i=1_ ([t][−][t][(0)])[∈][T] _i=1_

X    X   

     

Let un−1(t) = _i=1_ _[σ][i]_ _ϕi(ti) −_ _ϕi_ _t[(0)]i_, then
  

_n_

[P][n][−][1]

Eσn _g_ sup _σi_ _ϕi(ti)_ _ϕi_ _t[(0)]i_

  _−_ 

([t][−][t][(0)])[∈][T] _i=1_

X   

  

= [1]2 _[g]_  sup _un−1(t) +_ _ϕn(tn) −_ _ϕn_ _t[(0)]n_  + 2[1] _[g]_  sup _un−1(t) −_ _ϕn(tn) −_ _ϕn_ _t[(0)]n_  _._

([t][−][t][(0)])[∈][T] ([t][−][t][(0)])[∈][T]
     
   

Suppose that the above two suprema can be reached at t[′] and _t[˜] respectively, i.e.,_

sup _un−1(t) +_ _ϕn(tn) −_ _ϕn_ _t[(0)]n_ = un−1 (t[′]) + _ϕn (t[′]n[)][ −]_ _[ϕ][n]_ _t[(0)]n_ ;
([t][−][t][(0)])[∈][T]
     

sup _un−1(t) −_ _ϕn(tn) −_ _ϕn_ _t[(0)]n_ = un−1 _t˜_ _−_ _ϕn_ _t˜n_ _−_ _ϕn_ _t[(0)]n_ _._
([t][−][t][(0)])[∈][T]
           


_._






Otherwise we add an arbitrary positive number ε in the above equations. Therefore,


_n_

_g_ sup _σi_ _ϕi(ti)_ _ϕi_ _t[(0)]i_

  _−_

([t][−][t][(0)])[∈][T] _i=1_

X   

 

_un−1 (t[′]) +_ _ϕn (t[′]n[)][ −]_ _[ϕ][n]_ _t[(0)]n_ + 2[1]
   i


Eσn

= [1]

h


_un−1_ _t˜_ _−_ _ϕn_ _t˜n_ _−_ _ϕn_ _t[(0)]n_
        i


-----

Without loss of generality, we assume

_un−1 (t[′]) +_ _ϕn (t[′]n[)][ −]_ _[ϕ][n]_ _t[(0)]n_ _≥_ _un−1_ _t˜_ + _ϕn_ _t˜n_ _−_ _ϕn_ _t[(0)]n_ ;

(16)
_un−1_ _t˜_ _−_ ϕn _t˜n_ _−_ _ϕn_ t[(0)]n  _≥_ _un−1 ( t[′]) −_ _ϕn ( t[′]n[)][ −]_ _[ϕ][n]_ t[(0)]n  _._
           

For the other cases, the method remains the same. We set

_a = un−1_ _t˜_ _−_ _ϕn_ _t˜n_ _−_ _ϕn_ _t[(0)]n_ _,_

_b = un−1_  t˜ _−_ L _t˜n  −t[(0)]n_ _,_  

_a[′]_ = un−1 ( t[′]) + L _t[′]n_ _n_  _,_

_[−]_ _[t][(0)]_
 

_b[′]_ = un−1 (t[′]) + _ϕn (t[′]n[)][ −]_ _[ϕ][n]_ _t[(0)]n_ _._
  

Now our goal is to prove:
_g(a) −_ _g(b) ≤_ _g (a[′]) −_ _g (b[′]) ._ (17)

Considering the following four cases:


1. tand[′]n _[≥]_ _[t]n[(0)]_ and _t[˜]n ≥_ _t[(0)]n_ [. By the Lipschitzness of][ ϕ]n [and equation (16) we know][ a][ ≥] _[b, b][′][ ≥]_ _[b][,]_
(a _b)_ (a[′] _b[′]) = ϕn (t[′]n[)][ −]_ _[ϕ][n]_ _t˜n_ _L_ _t[′]n_ _tn_ _._
_−_ _−_ _−_ _−_ _[−]_ [˜]
     

If t[′]n _tn, we can get a_ _b_ _a[′]_ _b[′]. By the fact that g is convex and increasing, we have_
_g(y +[≥] x)[˜] −_ _g(x) is increasing in −_ _≤ y for every−_ _x ≥_ 0. Hence for x = a − _b,_

_g(a) −_ _g(b) = g(b + x) −_ _g(b) ≤_ _g (b[′]_ + x) − _g(b[′]) ≤_ _g (a[′]) −_ _g (b[′]) ._

If t[′]n _[<][ ˜]tn, we change ϕn into −ϕn and switch t[′]_ and _t[˜], and the proof is similar._


2. t[′]n _n_ and _t[˜]n_ _t[(0)]n_ [. Similarly, by changing the signs we can get the same result.]

_[≤]_ _[t][(0)]_ _≤_

3. t[′]n _n_ and _t[˜]n_ _t[(0)]n_ [. For this case we have][ a][ ≤] _[b][ and][ b][′][ ≤]_ _[a][′][, so][ g][(][a][) +][ g][ (][b][′][)][ ≤]_ _[g][(][b][) +][ g][ (][a][′][)][.]_

_[≥]_ _[t][(0)]_ _≤_

4. tand finally we get[′]n _[≤]_ _[t]n[(0)]_ and _t[˜]n ≥ gt([(0)]na) +[. For this case we can change] g (b[′]) ≤_ _g(b) + g (a[′])._ _[ ϕ]n_ [to][ −][ϕ]n[, then we have][ a][ ≥] _[b][ and][ a][′][ ≤]_ _[b][′][,]_

Thus equation (17) yields that


_n_

Eσn _g_ sup _σi_ _ϕi(ti)_ _ϕi_ _t[(0)]i_

  _−_ 

([t][−][t][(0)])[∈][T] _i=1_

X   

  

= 2[1] _g_ _un−1 (t[′]) +_ _ϕn (t[′]n[)][ −]_ _[ϕ][n]_ _t[(0)]n_ + [1]2 _g_ _un−1_ _t˜_ _−_ _ϕn_ _t˜n_ _−_ _ϕn_ _t[(0)]n_

h    i h          i

_≤_ 2[1] _g_ _un−1 (t[′]) + L_ _t[′]n_ _[−]_ _[t]n[(0)]_ + [1]2 _g_ _un−1_ _t˜_ _−_ _L_ _t˜n −_ _t[(0)]n_

h   i h      i

_≤_ 2[1] g ([t][−]sup[t][(0)])[∈][T] _un−1 (t) + L_ _tn −_ _t[(0)]n_  + 2[1] g ([t][−]sup[t][(0)])[∈][T] _un−1 (t) −_ _L_ _tn −_ _t[(0)]n_

  

    


_g_






=Eσn


([t][−]sup[t][(0)])[∈][T] _un−1 (t) + σnL_ _tn −_ _t[(0)]n_



-----

Applying the same method to σn 1, . . ., σ1 successively, we obtain the first inequality
_−_


_n_

_σi_ _ϕi(ti)_ _ϕi_ _t[(0)]i_
_−_
_i=1_

X   


_g_






Eσ _g_

 _≤_ 


_L_ sup



([t][−][t][(0)])[∈][T]



_σi_ _ti_ _t[(0)]i_
_i=1_ _−_

X 


_._






Eσ


sup
([t][−][t][(0)])[∈][T]


For the second inequality, since |x| = [x]+ + [x]− with [x]+ = max(0, x) and [x]− = max(0, −x),


_n_

sup _σi_ _ϕi(ti)_ _ϕi_ _t[(0)]i_

_−_ 

([t][−][t][(0)])[∈][T] _i=1_

X   

_n_ 

sup _σi_ _ϕi(ti)_ _ϕi_ _t[(0)]i_ + Eσ
([t][−][t][(0)])[∈][T] " _i=1_ _−_ +

X   [#]

_n_ 

sup _σi_ _ϕi(ti)_ _ϕi_ _t[(0)]i_ _,_

([t][−][t][(0)])[∈][T] " _i=1_ _−_ +

X   [#]

 


Eσ





Eσ
_≤_ 

=2Eσ


_n_

_σi_ _ϕi(ti)_ _ϕi_ _t[(0)]i_

" _i=1_ _−_
X   [#]


sup
([t][−][t][(0)])[∈][T]


where the last equality is by [−x]− = [x]+ and σ has the same distribution with −σ.

A simple fact is that


_n_

_σi_ _ϕi(ti)_ _ϕi_ _t[(0)]i_

" _i=1_ _−_
X   [#]


_n_

_σi_ _ϕi(ti)_ _ϕi_ _t[(0)]i_
_−_
_i=1_

X   


sup
([t][−][t][(0)])[∈][T]


sup
([t][−][t][(0)])[∈][T]


Since max(0, x) is convex and increasing, then by the first inequality we have


_n_

_σi_ _ϕi(ti)_ _ϕi_ _t[(0)]i_

" _i=1_ _−_
X   [#]


_n_

_σi_ _ϕi(ti)_ _ϕi_ _t[(0)]i_
_−_
_i=1_

X   


= Eσ

 

 

Eσ
_≤_ 

_≤_ _LEσ_


Eσ


sup
([t][−][t][(0)])[∈][T]


sup
([t][−][t][(0)])[∈][T]


_σi_ _ti_ _t[(0)]i_
_i=1_ _−_

X 


_L_ sup



([t][−][t][(0)])[∈][T]



_σi_ _ti_ _t[(0)]i_
_i=1_ _−_

X 


sup
([t][−][t][(0)])[∈][T]


This completes the proof.

Now we apply Lemma C.2 to bound the empirical Rademacher complexity of an element-wise
distance constrained function class. In the following lemma, all the notations are consistent with
Theorem 2 unless stated otherwise.

**Lemma C.3. Given a function class Fa,b := {x 7→** _f_ (w, x) : w ∈Sa,b} and sample S =
_x1, . . ., xn_ _with_ _xi_ = 1 for all i [n], then we have
_{_ _}_ _∥_ _∥_ _∈_


_a_ + _b_
_∥_ _∥[2]_ _∥_ _∥[2]_ _LΨ(_ _a,b)_ _._

_n_ _∥_ _S_ _∥_


_S(_ _a,b)_
_R_ _F_ _≤_


-----

_Proof. By definition,_

_nRS(Fa,b) = Eσ_

= Eσ

= Eσ


sup
_w∈Sa,b_

sup
_w∈Sa,b_

sup
_w∈Sa,b_


_σif_ (w, xi)
_i=1_

X

_n_

_σif_ (w, xi)
_i=1_

X


_n_

_σif_ (0, xi)

" _i=1_
X


_−_ Eσ


_σi (f_ (w, xi) − _f_ (0, xi))
_i=1_

X


Now we decompose the term f (w, xi) _f_ (0, xi) as:
_−_
_f_ (w, xi) _f_ (0, xi)
_−_

=Ψ _x[⊤]i_ _[α][1][, . . ., x]i[⊤][α][p][, β][1][, . . ., β][q]_ Ψ (0, . . ., 0, 0, . . ., 0)
_−_

=(Ψ (x[⊤]i _[α][1][,...,x]i[⊤][α][p][,β][1][,...,β][q][)][−][Ψ][(][0][,...,x]_ _i[⊤][α][p][,β][1][,...,β][q][))][+][(][Ψ][(][0][,...,x]i[⊤][α][p][,β][1][,...,β][q][)][−][Ψ][(][0][,][0][,...,x]i[⊤][α][p][,β][1][,...,β][q][))]_
+ + (Ψ (0, . . ., 0, 0, . . ., 0, βq) Ψ (0, . . ., 0, 0, . . ., 0)) .
_· · ·_ _−_

Then by the above decomposition and Lemma C.2, we have


Eσ


_σi (f_ (w, xi) − _f_ (0, xi))
_i=1_

X


sup
_w∈Sa,b_


_L[(1)]Ψ_ [(][S][a,b][)][E][σ]
_≤_

Notice that

Eσ

And


+ + L[(]Ψ[p][+][q][)]( _a,b)Eσ_
_· · ·_ _S_


_σix[⊤]i_ _[α][1]_
_i=1_

X


sup
_w∈Sa,b_


sup

"w∈Sa,b


_σiβq_
_i=1_

X


sup _σix[⊤]i_ _[α][1]_
_w∈Sa,b_ _i=1_

X


= Eσ


sup _σix[⊤]i_ _[α][1]_
_∥α1∥≤a1_ _i=1_

X


_≤_ _a1Eσ_ "

_a1vEσ_
_≤_ u

u
u

= a1t√n.


_σixi_
_i=1_ #

X

_n_

_σixi_



_i=1_

X




2[]




Eσ


= Eσ


sup
_w∈Sa,b_


_σiβq_
_i=1_

X


sup _σiβq_
_|βq|≤bq_ _i=1_

X


_≤_ _bqEσ_


_σi_
_i=1_ #

X

_n_

_σi_



_i=1_

X




2[]




_≤_ _bquEσ_

u
u

= bqt√n.

Therefore, by the Cauchy-Schwarz inequality, we can get


(p+q)
_L[(1)]Ψ_ [(][S][a,b][)][a][1]√n + + LΨ ( _a,b)bq√n_
_≤_ _· · ·_ _S_

_n_ _a_ + _b_ _LΨ(_ _a,b)_ _._

_≤_ _∥_ _∥[2]_ _∥_ _∥[2][]_ _∥_ _S_ _∥_

r




Eσ


sup
_w∈Sa,b_


_σi (f_ (w, xi) − _f_ (0, xi))
_i=1_

X


-----

Finally, we have


_∥a∥[2]_ + ∥b∥[2]


_S(_ _a,b)_
_R_ _F_ _≤_


_LΨ(_ _a,b)_ _._
_∥_ _S_ _∥_


Lemma C.3 gives an upper bound of the Rademacher complexity based on the element-wise distance.
Notice that _w[(][∞][)]_ _≤_ _w[(0)]_ + len(w[(0)], ∞). To obtain a length-based generalization bound, we
consider to use Sa,b to cover the length-constrained space {w : ∥w∥≤ _R}, and then taking a union_
bound. For the ℓ2 ball covering number, we use the following result from (Neyshabur et al., 2019,
Lemma 11).
**Lemma C.4. Given any ϵ, D, β > 0, consider the set Sβ[D]** [=] _x ∈_ R[D] : ∥x∥≤ _β_ _. Then there exist_

_N sets_ _Ti_ _i=1_ _[of the form][ T][i][ =]_ _x_ R[D] : _xj_ _αj[i]_ _[,][ ∀][j][ ∈]_ [[][D][]] _such that Sβ[D]_ _i=1_ _[T][i][ and]_
_{_ _}[N]_ _∈_ _K+D |_ 1 _| ≤_ _[⊆]_ [S][N]
_α[i]_ _≤_ _β(1 + ϵ), ∀i ∈_ [N ] where N = _D−1−_ _and_
   _D_

_K =_ _._

(1 + ϵ)[2] 1

 _−_ 

**Lemma C.5. For any two positive integers n, k with n ≥** _k, we have_
_n_ _en_ _k_

_._

_k_ _≤_ _k_
   


_Proof. Note that_
_n_
_k_


The last step is by


_n!_

_k!(n −_ _k)!_ _[≤]_ _[n]k[k]!_ _[≤]_ _[e][k][ ]_ _[n]k_


_k_



_k[i]_

_i!_ _[≥]_ _[k]k[k]!_ _[.]_


_e[k]_ =


_i=0_


Now combining Lemma C.1, C.3, C.4 and C.5, we are ready to prove Theorem 2.

_Proof of Theorem 2. First we apply Lemma C.4 with ϵ =_ _√2 −_ 1, D = p + q, and β = Mδ + Rn,δ,

then there exist N sets _ak,bk such that Sβ[D]_ _k=1_ _a[k]_ + _b[k]_ _√2β, with_

2D 1 _S_ _[⊆]_ [S][N] _[S][a][k][,b][k][ and]_ _∥_ _∥[2]_ _∥_ _∥[2]_ _≤_
_N =_ _D−−1_ . q

Therefore, for each parameter space   _ak,bk_, by Lemma C.3 we have
_S_

2

_RS(Fak,bk_ ) ≤ _n_ _[β]_ _LΨ(Sak,bk_ ) _._

r

Notice that the local Lipschitz constant of ℓ in _ak,bk is Lℓ(_ _ak,bk_ ). Hence, by Lemma C.1 and the
_S_ _S_
Ledoux-Talagrand contraction inequality, for anytraining sample, the following holds for all w _δ >ak,bk 0:_, with probability at least 1 − _δ/N over the_
_∈S_

_√2βLℓ(_ _ak,bk_ ) _LΨ(_ _ak,bk_ ) log(2N/δ)
_LD(w) ≤Ln(w) + [2]_ _S_ _√n_ _S_ + 3Mβ 2n _,_

r

where Mβ = sup∥ak∥2+∥bk∥2≤2β2 supw∈Sak,bk,∥x∥≤1,|y|≤1 ℓ (f (w, x), y).

Since w[(][∞][)] _Sβ[D]_ _k=1_
probability at least∈ 1[⊆] −[S]δ[N] over the initialization[S][a][k][,b][k] [, by taking the union bound over all sets] I the training sample, _[ S][a][k][,b][k]_ [, we can get with]

2√2βLℓ( _a,b)_ _LΨ(_ _a,b)_ log(2N/δ)
_LD(w[(][∞][)]) ≤Ln(w[(][∞][)]) +_ _∥a∥[2]+sup∥b∥[2]≤2β[2]_ _S_ _√n ∥_ _S_ _∥_ + 3Ma,br 2n _._


-----

Theorem 1 already showed that _n(w[(][∞][)]) = minw_ _n(w). Thus it remains to bound the term log N_ .
For D = 1, N = 1. For D ≥ 2 L, by Lemma C.5, _L_

_e(2D_ 1)
log N (D 1) log _−_ _< 2.1(D_ 1) < 3D = 3(p + q).
_≤_ _−_ _D_ 1 _−_
 _−_ 


Finally, let R =


2β, we complete the proof of Theorem 2.


C.1 ADDITIONAL OF RESULTS FOR THE GENERALIZATION DURING TRAINING THE MODEL

Theorem 2 gives a length-based generalization bound for the final model. In this subsection, we apply
our framework to derive generalization estimates that evolves according to the length of time (number
of epochs) of training by combining the length estimate obtained in Theorem 1.

The approach is to give a generalization bound for early stopping when the loss value first reaches
_ε_ 0. The idea is that, when there exists T > 0 such that _n(w[T]_ ) = ε, then by the inequality
_≥_ _L_
(11) in the proof of Lemma B.1, we can get an upper bound for the length len(w[(0)], T ) in terms
of _n(w[(0)]), minw_ (w), ε, cn, θn. Finally, we can get a generalization bound by our new length
estimate. L _L_

To get a clean expression for the generalization bound, we assume the optimal value of the empirical
loss function _n(w) is zero. Then the rigorous statement is stated as follows:_
_L_

**Corollary 1. Consider a training criterion of early stopping that the training is stopped once the**
_empirical loss value first reaches ε ≥_ 0. Then for any given ε ≥ 0, under the notations and conditions
_in Theorem 1, suppose that for any δ ∈_ (0, 1), there exists rn,δ such that rn(w[(0)]) ≤ _rn,δ with_
_probability at least 1 −_ _δ over the initialization and the training samples. Then, we have with_
_probability at least 1 −_ _δ over initialization and the training samples, the generalization error for the_
_stopping parameter w is given by:_


2rε,n,δLℓ( _a,b)_ _LΨ(_ _a,b)_
_S_ _∥_ _S_ _∥_ +3Ma,b
_√n_


3(p + q) + log(2/δ)

_, (18)_
2n


(w) _ε_ + sup
_LD_ _≤_ _∥a∥[2]+∥b∥[2]≤rε,n,δ[2]_


_ε[1][−][θn]_
_Mδ + rn,δ −_ _cn(1−θn)_


_where rε,n,δ =_


**Remark 2. Corollary 1 shows a trade-off between ε and the term rε,n,δ. The case for ε = 0**
_corresponds to the combining results of Theorem 2 and Theorem 1._

_Proof. The proof is straightforward. Notice that by the inequality (11), we can bound the length_
len(w[(0)], w) as

_ε[1][−][θ][n]_
len(w[(0)], w) _rn(w[(0)])_
_≤_ _−_ _cn(1_ _θn)_ _[,]_

_−_

where rn(w[(0)]) is specified in equation (2). Then by the same argument in the proof of Theorem 2,
_ε[1][−][θn]_
we may directly replace rn(w[(0)]) with rn(w[(0)]) − _cn(1−θn)_ [to get the desired bound.]

D PROOFS FOR SECTION 4


In this section, our goal is to prove all the theorems in Section 4. A crucial part of the proofs is the
spectral analysis of the random matrix X . Therefore, we start with introducing the non-asymptotic
results of λmax( ) and λmin( ) from (Rudelson & Vershynin, 2010).
_XX_ _[⊤]_ _XX_ _[⊤]_

The first result is from (Rudelson & Vershynin, 2010, Proposition 2.4), characterizing the nonasymptotic behavior of the largest singular value of subgaussian matrices.


-----

**Lemma D.1. Let A be an N** _×n random matrix whose entries are independent mean zero subgaussian_
_random variables whose subgaussian moments are bounded by 1. Then for every t ≥_ 0, with
_probability at least 1 −_ 2e[−][ct][2] _over the randomness of the entries,_


_N +_ _n) + t,_

_[√]_


_λmax(AA[⊤])_ _C(_
_≤_


_where c and C are two positive constants that depend only on the subgaussian moment of the entries._

The second result is from (Rudelson & Vershynin, 2009, Theorem 1.1), characterizing the nonasymptotic behavior of the smallest singular value of subgaussian matrices.

**Lemma D.2. Let A be an N × n random matrix whose entries are independent and identically**
_distributed subgaussian random variables with zero mean and unit variance. If N > n, then for_
_every ε > 0, with probability at least 1 −_ (C1ε)[N] _[−][n][+1]_ _−_ _c[N]1_ _[over the randomness of the entries,]_


_λmin(A[⊤]A)_ _ε(_
_≥_


_N −_


_n −_ 1),


_where C1 > 0 and c1 ∈_ (0, 1) depend only on the subgaussian moment of the entries.

D.1 PROOF OF THEOREM 3

In this section, we will prove Theorem 3 based on the three steps in our framework. All the notations
are consistent with Theorem 3 unless stated otherwise.

_Proof of Theorem 3. First, we prove the result for Step 1._

For a vector a = (a1, . . ., an)[⊤] _∈_ R[n], we use a[◦][m] to denote the element-wise power, i.e., a[◦][m] =
(a[m]1 _[, . . ., a]n[m][)][⊤][. For the][ ℓ][p]_ [linear regression loss function][ L][n][(][w][)][, notice that]


_w[⊤]xi_ _yi_ _p−1 xi = 1_
_−_ _n_ _[X][ ⊤]_ [(][X] _[w][ −Y][)][◦][(][p][−][1)][ .]_



_n(w) = [1]_
_∇L_ _n_


_i=1_


Then since X has full row rank, we have ∀w ∈ R[d],

_n(w)_ = [1] ( _w_ )[◦][(][p][−][1)]
_∥∇L_ _∥_ _n_ _X_ _[⊤]_ _X_ _−Y_

_λmin(_ )
_XX_ _[⊤]_ ( _w_ )[◦][(][p][−][1)]

_≥_ _n_ _X_ _−Y_

p

_λmin(_ )

= _nXX_ _[⊤]_ _∥X_ _w −Y∥2[p]p[−]−[1]2_

p

_λmin(_ )

_≥_ _nXX_ _[⊤]_ _∥X_ _w −Y∥p[p][−][1]_ _· n[1][/p][−][1][/][2]_

p


_λmin(_ )
_XX_ _[⊤]_


= p[1][−][1][/p]


_n(w)[1][−][1][/p]._
_L_

_θn = 1 −_ 1/p.


Therefore,


_λmin(_ )
_XX_ _[⊤]_


_cn = p[1][−][1][/p]_


For Step 2, note thatinto Theorem 1. minw Ln(w) = 0, then the result can be proved by directly plugging cn and θn


-----

Next, we prove the result for Step 3. By Theorem 1 and the property of the target function, we have
for any w[(0)] that satisfies _w[(0)]_ 2

_[≤]_ _[c][0][,]_ 1/p

_√n_ _p_ _n(w[(0)])_
_rn(w[(0)]) =_ _L_
 λmin(XX _[⊤])_

p _w[(0)]_ _p_

= n[1][/][2][−][1][/p] _X_ _−Y_

_λmin(_ )
_XX_ _[⊤]_

p _w[(0)]_ 2

_≤_ _n[1][/][2][−][1][/p]_ _Xλmin(−Y_ )

_XX_ _[⊤]_

pλmax( )

_n[1][/][2][−][1][/p]_ _XX_ _[⊤]_
_≤_ s _λmin(XX_ _[⊤]) [(][c][0][ +][ c][∗][)][ .]_

Now we apply Lemma D.1 with A = and t = log(4c _/δ)_, then we have with probability at least
_X_

1 _δ/2 over the samples,_ q
_−_

_λmax(_ ) _C([√]n +_ _√d) +_ log(4/δ) _,_ (19)
_XX_ _[⊤]_ _≤_ _c_

r

q

where c and C are two positive constants that depend only on the subgaussian moment of the entries.

1Similarly, let − _τ_ _[d][−][n][+1]_ _− τ =τ_ _[d] cover the samples,1 ∈_ (0, 1), ε = τ/C1 > 0, then Lemma D.2 implies that with probability at least

_λmin(_ ) (√d _√n_ 1), (20)
_XX_ _[⊤]_ _≥_ _C[τ]1_ _−_ _−_

q

where C1 > 0 and τ ∈ (0, 1) depend only on the subgaussian moment of the entries.

Taking the union bound, we have with probability at least 1−δ/2−τ _[d][−][n][+1]_ _−τ_ _[d]_ over the initialization
and the training samples,


log(4/δ)

_rn(w[(0)])_ _C([√]n +_ _√d) +_ _c_
_√n_ _≤_ _n[−][1][/p]_ (c0 + c[∗]) _Cτ1_ [(]√d − _[√]nq −_ 1)

_n_ log(4/δ)
_C_ _d_ [+ 1] + _cd_
_n[−][1][/p]_ (c0 + c[∗])
_≤_  pCτ1 1  qn−d 1

_−_

_CC1([√]γ1 + 1) +q_ _C1_ _γ1 log(4cn_ _/δ)_
_n[−][1][/p]_ (c0 + c[∗])
_≤_ _τ_ (1 _γ1q)_

_−_ _[√]_


(21)


log(1/δ)


_n[−][1][/p]_ + n[−][1][/p]


_≤O_


Recall for the linear regression model (5), f (w, x) = w[⊤]x. Thus Ψ(x) = x is an identity function
with p = 1, q = 0, and _LΨ(_ _a,b)_ = 1 for any a, b. Since the loss function _ℓ[˜]_ is bounded by 1
_∥_ _S_ _∥_
and 1-Lipschitz, we know that Lℓ(Sa,b) = MR = 1 for any a and b. Finally by Theorem 2 and
_ℓ˜(y, y) = 0, we have with probability at least 1 −_ _δ/2 over the samples,_

E(x,y) _ℓ˜_ _f_ (w[(][∞][)], x), y _√2(rn(w[(0)]) + c0)_ + 3 3 + log(4/δ) _._
_∼D_ _≤_ [2] _√n_ 2n

r

h  i

Combining the inequality (21), we have with probability at least 1 − _δ −_ _τ_ _[d][−][n][+1]_ _−_ _τ_ _[d]_ over training
samples,

log(1/δ)

E(x,y) _ℓ˜_ _f_ (w[(][∞][)], x), y _n[−][1][/p][]_ + _._
_∼D_ _≤O_ _O_ r _n_ !
h  i 


-----

This completes the proof of Theorem 3.

D.2 PROOF OF THEOREM 4

In this section, we will prove Theorem 4. First, we present some useful lemmas for proving our
results, and then we give the proofs of Theorem 4 for the RBF kernel and the inner product kernel
separately.

For the RBF kernel k(x, y) = ϱ (∥y − _x∥), the following two lemmas give non-asymptotic bounds_
for λmax(k( _,_ )) and λmin(k( _,_ )) based on the separation distance SD of .
_X_ _X_ _X_ _X_ _X_

The first lemma is from (Diederichs & Iske, 2019, Lemma 3.1), providing an upper bound for
_λmax(k(_ _,_ )).
_X_ _X_

**Lemma D.3. For the RBF kernel, if ϱ : R** 0 R 0 is a decreasing function, then
_≥_ _−→_ _≥_


(t + 2)[d][−][1]ϱ(t · SD), (22)
_t=1_

X


_λmax(k(_ _,_ )) _ϱ(0) + 3d_
_X_ _X_ _≤_


_and the sum of the infinite series in equation (22) is finite if and only if ϱ (∥x∥) ∈_ _L[1](R[d])._

The next lemma is from (Wendland, 2004, Theorem 12.3), giving a lower bound for λmin(k( _,_ )).
_X_ _X_

**Lemma D.4. Suppose that k is a positive-definite RBF kernel. If ϱ (∥x∥) ∈** _L[1](R[d]), one can define_
_the Fourier transform of ϱ as ˆϱ(ω) := (2π)[−][d/][2][ R]_

R[d][ ϱ][(][ω][)][e][−][ix][⊤][ω][dω][. With a decreasing function]
_ϱ0(M_ ) and two constants Md, Cd defined as

_d_

1 _Md_
_ϱ0(M_ ) := inf _ϱ(x),_ _Md = 6.38d,_ _Cd =_ _,_
_∥x∥≤2M_ [ˆ] 2Γ(d/2 + 1)  2[3][/][2] 


_where Γ is the gamma function. Then a lower bound on λmin(k(_ _,_ )) is given by
_X_ _X_

_λmin(k(X_ _, X_ )) ≥ _Cd · ϱ0 (Md/SD) · SD[−][d]._


For the inner product kernel k(x, y) = ϱ _x[⊤]d_ _y_



, it is shown in (El Karoui et al., 2010) that the


kernel matrix k(X _, X_ ) can be approximated by the linear combination of all-ones matrix 11[⊤], sample
covariance matrix XX _[⊤]_ and identity matrix. To obtain non-asymptotic results on the spectra of the
kernel matrix k(X _, X_ ), we borrow the technique from (Liang & Rakhlin, 2020, Proposition A.2),
and show the result for subgaussian entries in the next lemma.

**Lemma D.5. For the inner product kernel, suppose that the entries of X are i.i.d. subgaussian**
_random variables with zero mean and unit variance, then with probability at least 1 −_ _δ −_ _d[−][2]_ _over_
_the entries,_
_k(X_ _, X_ ) − _k[lin](X_ _, X_ ) _≤_ _d[−][1][/][2][ ]δ[−][1][/][2]_ + log[0][.][51] _d_ _,_


_where k[lin](X_ _, X_ ) is defined as


_k[lin](_ _,_ ) := _ϱ(0) +_ _[ϱ][′′][(0)]_
_X_ _X_ _d_



11[⊤] + ϱ[′](0) _[XX][ ⊤]_


+ (ϱ(1) _ϱ(0)_ _ϱ[′](0)) In_ _n._
_−_ _−_ _×_


_Proof. Note that the sample covariance matrix Σd = Id_ _d, then by applying (Liang & Rakhlin, 2020,_
_×_
Proposition A.2) with subgaussian random entries we can prove this lemma.

**Lemma D.6. Suppose that A, B ∈** R[n][×][n] _are two symmetric matrices, then we have_

_λmin(A + B)_ _λmin(A) + λmin(B)._
_≥_


-----

_Proof. Note that for any x ∈_ R[n] with ∥x∥ = 1,

_x[⊤](A + B)x = x[⊤]Ax + x[⊤]Bx_ _λmin(A) + λmin(B)._
_≥_

By definition, we have

_λmin(A + B) =_ inf
_x_ =1 _[x][⊤][(][A][ +][ B][)][x][ ≥]_ _[λ][min][(][A][) +][ λ][min][(][B][)][,]_
_∥_ _∥_

which completes the proof.

Now we are ready to prove Theorem 4.

_Proof of Theorem 4. For Step 1. Notice that ∀w ∈_ R[s],

_n(w)_ = [1] _φ(_ )[⊤] (φ( )w )[◦][(][p][−][1)]
_∥∇L_ _∥_ _n_ _X_ _X_ _−Y_

_λmin(φ(_ )φ( )[⊤])
_X_ _X_ (φ( )w )[◦][(][p][−][1)]

_≥_ _n_ _X_ _−Y_

p

_λmin(k(_ _,_ ))

= _n_ _X_ _X_ _∥φ(X_ )w −Y∥2[p]p[−]−[1]2

p

_λmin(k(_ _,_ ))

_≥_ _n_ _X_ _X_ _∥φ(X_ )w −Y∥p[p][−][1] _· n[1][/p][−][1][/][2]_

p


_λmin(k(_ _,_ ))
_X_ _X_


= p[1][−][1][/p]


_n(w)[1][−][1][/p]._
_L_


Therefore, Ln(w) satisfies the Uniform-LGI globally on R[s] with


_λmin(k(_ _,_ ))
_X_ _X_


_cn = p[1][−][1][/p]_


_θn = 1 −_ 1/p.


Theorem 1 we can directly get the result.For Step 2. Since k is a positive-definite kernel, and θn = 1 − 1/p, then minw Ln(w) = 0, thus by

The proof of Step 3 is two-sided. First, since ∀x ∈X _, ∥φ(x)∥_ = _k(x, x) ≤_ 1, then the kernel

regression model (7) can be viewed as ℓp linear regression on inputsp φ(X ). Hence, Ψ is an identity
function with p = 1, q = 0, and ∥LΨ(Sa,b)∥ = Lℓ(Sa,b) = MR = 1 for any a, b. This means that
we only need to bound the term rn(w[(0)])/[√]n.

By Theorem 1 and the property of the target function, for any w[(0)] that satisfies _w[(0)]_ 2
have _[≤]_ _[c][0][, we]_

1/p
_√n_ _p_ _n(w[(0)])_
_rn(w[(0)]) =_ _L_

_λ min(k(_ _,_ ))
_X_ _X_

p _φ(_ )w[(0)] _p_

= n[1][/][2][−][1][/p] _X_ _−Y_

_λmin(k(_ _,_ ))
_X_ _X_

_≤_ _n[1][/][2][−][1][/p]_ pφλ(Xmin)w(k[(0)]( _−Y,_ ))2 (23)

_X_ _X_

pφ( )w[(0)] +

_≤_ _n[1][/][2][−][1][/p]_ _λXmin(k(_ _,_ _∥Y∥))_

_X_ _X_

pλmax(k( _,_ ))

_n[1][/][2][−][1][/p]_ _X_ _X_
_≤_ s _λmin(k(_ _,_ )) [(][c][0][ +][ c][∗][)][ .]

_X_ _X_


-----

Then for the RBF kernel, Lemma D.3 and Lemma D.4 indicate that there exists a positive constant
_C(ϱ, d, qmin, qmax) that only depends on ϱ, d, qmin, qmax such that_

_λmax(k(_ _,_ ))
_X_ _X_ _n_ 1,

_λmin(k(_ _,_ )) _∀_ _≥_
_X_ _X_ _[≤]_ _[C][(][ϱ, d, q][min][, q][max][)][,]_

which implies that for all initialization w[(0)],

_rn(w[(0)])_
_√n_ _≤O_ _n[−][1][/p][]_ _._ (24)


Therefore, by Theorem 2 and Lemma C.1, we have with probability at least 1 − _δ over training_
samples,

log(1/δ)

E(x,y) _ℓ˜_ _f_ (w[(][∞][)], x), y _n[−][1][/p][]_ + _,_
_∼D_ _≤O_ _O_ r _n_ !
h  i 

which completes the proof for the RBF kernel.


For the inner product kernel, to obtain an upper bound for _[λ]λ[max]min([(]k[k]([(][X],[,][X]))[))]_ [, first notice that]

_X_ _X_

_λmax(k(_ _,_ )) = _k(_ _,_ )
_X_ _X_ _∥_ _X_ _X_ _∥_ (25)

_≤_ _k[lin](X_ _, X_ ) + _k(X_ _, X_ ) − _k[lin](X_ _, X_ ) _._

By Lemma D.6, we can get

_λmin(k(_ _,_ )) _λmin(k[lin](_ _,_ )) + λmin(k( _,_ ) _k[lin](_ _,_ ))
_X_ _X_ _≥_ _X_ _X_ _X_ _X_ _−_ _X_ _X_ (26)

_λmin(k[lin](_ _,_ )) _k(_ _,_ ) _k[lin](_ _,_ ) _._
_≥_ _X_ _X_ _−_ _X_ _X_ _−_ _X_ _X_

Under Assumption 2, Lemma D.5 implies that
_k[lin](X_ _, X_ ) _≤_ _[ϱ][′′]d[(0)]_ 11[⊤] + _[ϱ][′]d[(0)]_ _XX_ _[⊤]_ + (ϱ(1) − _ϱ[′](0))_

+ ϱ[′](0) _[λ][max][(][XX][ ⊤][)]_ + (ϱ(1) _ϱ[′](0))_

_≤_ _[nϱ][′′]d[(0)]_ _d_ _−_

_γ1ϱ[′′](0) + ϱ[′](0)_ _[λ][max][(][XX][ ⊤][)]_ + (ϱ(1) _ϱ[′](0)),_
_≤_ _d_ _−_


and
_λmin(k[lin](_ _,_ )) _ϱ(1)_ _ϱ[′](0) > 0._
_X_ _X_ _≥_ _−_

Thus, by equation (26) we have

_λmin(k(_ _,_ )) (ϱ(1) _ϱ[′](0))_ _k(_ _,_ ) _k[lin](_ _,_ ) _._ (27)
_X_ _X_ _≥_ _−_ _−_ _X_ _X_ _−_ _X_ _X_

Under Assumption 1, by equation (19), we have with probability at least 1 − _δ/3 over the samples,_

2

_λmax(_ ) _n_ log(6/δ)
_XX_ _[⊤]_ _C_ +

_d_ _≤_ r _d_ [+ 1] r _cd_ !

2

_γ1 log(6/δ)_

_C ([√]γ1 + 1) +_ _._

_≤_ r _cn_ !


Therefore, by equation (25), with probability at least 1 − _δ/3 over the samples,_

2

_γ1 log(6/δ)_

_λmax(k(_ _,_ )) _γ1ϱ[′′](0)+ϱ[′](0)_ _C ([√]γ1 + 1) +_ +ϱ(1) _ϱ[′](0)+_ _k(_ _,_ ) _k[lin](_ _,_ )
_X_ _X_ _≤_ r _cn_ ! _−_ _X_ _X_ _−_ _X_ _X_

(28)


-----

By Lemma D.5, for large d and small δ such that d[−][1][/][2][  ][√]3δ[−][1][/][2] + log[0][.][51] _d_ _≤_ 0.5(ϱ(1) − _ϱ[′](0)),_

we have with probability at least 1 _δ/3_ _d[−][2]_ over the entries,
_−_ _−_ 
_k(X_ _, X_ ) − _k[lin](X_ _, X_ ) _≤_ 0.5(ϱ(1) − _ϱ[′](0))._

Then equation (27) and (28) yields that with probability at least 1 − 2δ/3 − _d[−][2]_ over the samples,

_λmin(k(_ _,_ )) 0.5(ϱ(1) _ϱ[′](0)),_
_X_ _X_ _≥_ _−_

2

_γ1 log(4/δ)_

_λmax(k(_ _,_ )) _γ1ϱ[′′](0) + ϱ[′](0)_ _C ([√]γ1 + 1) +_ + 1.5(ϱ(1) _ϱ[′](0))._
_X_ _X_ _≤_ r _cn_ ! _−_

Hence, by equation (23), with probability at least 1 − 2δ/3 − _d[−][2]_ over the samples, we have


_rn(√wn[(0)])_ _≤O_


log(1/δ)

_n[1][/][2+1][/p]_

p


_n[−][1][/p]_ +


(29)


Combining Theorem 2, we get with probability at least 1 − _δ −_ _d[−][2]_ over the samples,

log(1/δ)

E(x,y) _ℓ˜_ _f_ (w[(][∞][)], x), y _n[−][1][/p][]_ + _,_
_∼D_ _≤O_ _O_ r _n_ !
h  i 

which completes the proof.

D.3 PROOF OF THEOREM 5


In this section, we will prove Theorem 5. We first introduce some important lemmas for proving our
final results. Lemma D.7 shows that the smallest eigenvalue of the NTK matrix Θ(t) has a lower
bounded given the overparameterization, by which we can prove the optimization result. In Lemma
D.8, we show that the eigenvalues of the NTK matrix are related to the data covariance matrix. Then
by combining Lemma D.11 and Lemma D.9 we can prove the generalization result.

**Lemma D.7. For any δ ∈** (0, 1), if m ≥ poly _n, λ[−]min[1]_ [(]Θ)[b] _, δ[−][1][], then with probability at least_
1 _δ over the random initialization,_ 
_−_

_λmin(Θ(t))_ Θ), _t_ 0.
_≥_ 2[1] _[λ][min][(]_ [b] _∀_ _≥_

_Proof. The proof is the same as the proof of (Du et al., 2019, Lemma 3.4)._


**Lemma D.8.**

_λmin(Θ)[b]_ _λmin_ _/4._
_≥_ _XX_ _[⊤][]_
 

_Proof. Notice that for ReLU activation ϕ, a simple fact is that ϕ[′](ax) = ϕ[′](x) holds for any x ∈_ R
given that a > 0. Therefore,


Θij = x[⊤]i _[x][j][E]w∼N (0,_ _d[1]_ [I][d][)] _ϕ[′](w[⊤]xi)ϕ[′](w[⊤]xj)_
b = x[⊤]i _[x][j][E]w∼N (0,Id)_ _ϕ[′](w[⊤]xi)ϕ[′](w[⊤]xj)_

_i_ _[x][j][(][π][ −]_ [arccos(] _[x]i[⊤][x][j][))]_ 
= _[x][⊤]_

2π


_i_ _[x][j]_
= _[x][⊤]_

4

_i_ _[x][j]_
= _[x][⊤]_


_i_ _[x][j]_
+ _[x][⊤]_ arcsin(x[⊤]i _[x][j][)]_
2π


+ [1]

2π


_∞_ (2k)!

_i_ _[x][j][)][2][k][+2][.]_
4[k](k!)[2](2k + 1) [(][x][⊤]

_k=0_

X


-----

Then


Θ =

_[XX]4_ _[ ⊤]_
b

=

_[XX]4_ _[ ⊤]_


(2k)!

4[k](k!)[2](2k + 1) _XX_ _[⊤][][◦][(2][k][+2)]_
 

(2k)!

4[k](k!)[2](2k + 1) (X _[⊤])[⊙][(2][k][+2)][][⊤]_ (X _[⊤])[⊙][(2][k][+2)],_




_∞_

+ [1]

2π

_k=0_

X

_∞_

+ [1]

2π

_k=0_

X


where ◦ is the element-wise product, and ⊙ is the Khatri-Rao product[6].

Since (X _[⊤])[⊙][(2][k][+2)][][⊤]_ (X _[⊤])[⊙][(2][k][+2)]_ is positive semidefinite, we have
 

_λmin(Θ)[b]_ _λmin_ _/4,_
_≥_ _XX_ _[⊤][]_
 

which completes the proof.

In the next lemma, we adopt an inequality from (Montgomery-Smith, 1990).

**Lemma D.9. If {σi}i[n]=1** _[are i.i.d. drawn from][ U]_ _[{−][1][,][ 1][}][, then for any][ x][ = (][x][1][, . . ., x][n][)][⊤]_ _[∈]_ [R][n][,]
_with probability at least 1 −_ _δ over σ,_


2 log(2/δ) ∥x∥ _._


_σixi_
_i=1_

X


The following lemma gives a sharp bound for a Chi-square variable, which is from (Laurent &
Massart, 2000, Lemma 1).

**Lemma D.10. Let (Y1, . . ., YD) be i.i.d. Gaussian variables, with mean 0 and variance 1. Then with**
_probability at least 1 −_ _δ over Y,_


1
_D log_

_δ_




1
+ 2 log

_δ_




_Yi[2]_
_i=1_ _[≤]_ _[D][ + 2]_

X


The next lemma is quoted from (Arora et al., 2019, Lemma 5.4), giving an upper bound for the
empirical Rademacher complexity if one has an accurate estimate for the distance with respect to
each hidden unit.

**Lemma D.11. Given R > 0, consider the following function class**

= _x_ _f_ (w, x) : _W1,r_ _W1[(0)],r_ _R(_ _r_ [m]), _W1_ _W1[(0)]_
_F_ n _7→_ _−_ _≤_ _∀_ _∈_ _−_ _F_ _[≤]_ _[B]o_

_withwith probability at least W1,r ∈_ R[d] _the r-th row of 1 −_ _δ over the random initialization, the empirical Rademacher complexity W1. Then for an i.i.d. sample S = {x1, . . ., xn} and every B > 0,_
_is bounded as:_


1/4[!]


2 log(2/δ)
1 +

_m_




+ 2R[2][√]md + R


_S(_ )
_R_ _F_ _≤_


2 log(2/δ).


2n


Now we are ready to prove Theorem 5.

_⊗_ is the Kronecker product.6For A = (a1, . . ., an) ∈ Rm×n, B = (b1, . . ., bn) ∈ Rp×n, then A _⊙_ _B = [a1 ⊗_ _b1, . . ., an ⊗_ _bn], where_


-----

_Proof of Theorem 5. For Step 1. By Lemma D.7, if m ≥_ poly _n, λ[−]min[1]_ [(]Θ)[b] _, δ[−][1][], then with proba-_
bility at least 1 _δ over the random initialization,_ 
_−_


_n(w[(][t][)])_ = [1]
_∇L_ _n_

= [1]

_n_

= [1]


(f (w, xi) _yi)_ _f_ (w[(][t][)], xi)
_−_ _∇_
_i=1_

X

_∇f_ (w[(][t][)], X )[⊤] []f (w[(][t][)], X ) −Y


_f_ (w[(][t][)], X ) −Y _⊤_ _∇f_ (w(t), X )∇f (w(t), X )⊤ _f_ (w[(][t][)], X ) −Y
q 2λmin(Θ(t))   

_n(w[(][t][)])_

_n_ _L_

q


_λmin(Θ)[b]_

_n_


_n(w[(][t][)])_
_L_


holds for any t 0, which means that _n(w[(][t][)]) satisfies the Uniform-LGI for any t_ 0 with
_≥_ _L_ _≥_

_cn =_ _λmin(Θ)[b]_ _/n,_ _θn = 1/2._
q

For Step 2. By equation (13), we can directly get _n(w[(][t][)]) converges to zero with a linear convergence_
_L_
rate:


_n(w[(][t][)])_ exp _λmin(Θ)[b]_ _t/n_ _n(w[(0)])._
_L_ _≤_ _−_ _L_
 

For Step 3. By equation (12) and Lemma D.8, the length len(w[(0)], ∞) can be bounded as


_n_ _n(w[(0)])_
_L_ 4

_λmin(Θ)[b]_ _≤_


_n_ _n(w[(0)])_
_L_

_λmin(_ ) _[.]_
_XX_ _[⊤]_


len(w[(0)], ) _rn(w[(0)]) = 2_
_∞_ _≤_

By the property of the target function, we have


2

1
_√mw2[⊤][ϕ][(][W][ (0)]1_ _xi) −_ _yi_
 


v 2
u
u
t [1]


_n_ _n(w(0)) =_
_L_


_i=1_


_n_ 2

1
2 _[ϕ][(][W][ (0)]1_ _xi)_ + yi[2]

v _√mw[⊤]_
u _i=1_  
uX
t _n_ 2 _n_

1
2 _[ϕ][(][W][ (0)]1_ _xi)_ + _yi[2]_

v _√mw[⊤]_ v
u _i=1_   u _i=1_
uX uX
t _n_ t

v _m[1]_ _w2[⊤][ϕ][(][W][ (0)]1_ _xi)_ 2 + c[∗] _λmax(XX_ _[⊤])._
uu Xi=1   q
t


Since the entries of w2 are drawn i.i.d. from U _{−1, 1}, then by Lemma D.9, for each i ∈_ [n], with
probability at least 1 _δ/6n over w2,_
_−_

2 12n 2
_w2[⊤][ϕ][(][W][ (0)]1_ _xi)_ 2 log _ϕ(W1[(0)]xi)_ _._
_≤_ _δ_
   


-----

Taking the union bound over all i = 1, 2, . . ., n, we have with probability at least 1 − _δ over the_
random initialization,


2
_w2[⊤][ϕ][(][W][ (0)]1_ _xi)_




[1]
v _m_
u
u
t


v _m_
u
u
t [2 log(12][n/δ][)]

2 log(12n/δ)

_m_

r

2 log(12n/δ)

_m_

r

2 log(12n/δ)

_m_

r

2 log(12n/δ)

r


_ϕ(W1[(0)]xi)_

_i=1_

X

_ϕ_ _W1[(0)]X_ _[⊤][]F_


_W1[(0)]X_ _[⊤]_ _F_

_W1[(0)]_
_F_

_[∥X∥]_

_W1[(0)]_ _F_ _λmax(XX_ _[⊤])._

q


_i=1_


(30)


For the Gaussian random matrix W1[(0)] _∼N_ (0, _d[1]_ [I][m][×][d][)][, by Lemma D.10, we have with probability]

at least 1 − _δ/6 over the random initialization,_


_W1[(0)]_

_m_


log(6/δ) + [2 log(6][/δ][)] _._ (31)

_md_ _md_


_≤_ 1 + 2


Taking the union bound of equation (19), (20), (30) and (31), if m ≥ poly _n, λ[−]min[1]_ [(]Θ)[b] _, δ[−][1][], then_

with probability at least 1 − _τ_ _[d][−][n][+1]_ _−_ _τ_ _[d]_ _−_ 5δ/6 over the samples and random initialization _I,_


_n_ _n(w[(0)])_
_L_

_λmin(_ )
_XX_ _[⊤]_


sup _rn(w[(0)])_ sup
_w[(0)]∈I,(x,y)∈D_ _≤_ _w[(0)]∈I,(x,y)∈D_


_√n +_
log(n/δ)


log(1/δ) + [log(1][/δ][)]

_md_ _md_


_√d +_ log(1/δ)

_d_ _n_ 1

p

_−_ _[√]_ _−_


1 + +

_≤O_ u r _md_ _md_ !

u

t

_√γ1 + 1 +_ log(1/δ)/n
log(n/δ)

_≤O_ 1 _γ1_

_−p[√]_

p

_≤O_ log(n/δ) _._
p 


Therefore, with probability at least 1−τ _[d][−][n][+1]_ _−τ_ _[d]_ _−5δ/6 over the samples and random initialization,_


_rn(√wn[(0)])_ _≤O_


log(n/δ)


(32)


For the r-th row of W1[(][t][)][, we begin to bound the distance] _W1[(],r[t][)]_ _[−]_ _[W][ (0)]1,r_ for each r ∈ [m].


-----

Notice that
_dW1[(],r[t][)]_

_dt_

Hence,


_∇W (1,rt)_ _[L][n][(][w][(][t][)][)]_

_n_

1 1

_f_ (w[(][t][)], xi) _yi_ 2,rϕ[′](W1[(],r[t][)][x][i][)][x][i]

_n_ _−_ _√mw_

_i=1_

X  


1

_f_ (w[(][t][)], xi) _yi_

_n[√]m_ _−_

_i=1_

X

1
_√nm_ _f_ (w[(][t][)], X ) −Y

2 _n(w[(0)])_
_L_ exp _λmin(Θ)[b]_ _t/2n_

_m_ _−_

r




_W1[(],r[t][)]_ 1,r _t_ _dW1[(],r[s][)]_ 2n 2Ln(w[(0)]) =

_[−]_ _[W][ (0)]_ _≤_ Z0 _ds_ _[ds][ ≤]_ _λmin(Θ)[b]_ r _m_ s

Now we apply Lemma D.11 with B = rn(w[(0)]), R = supw(0)∈I,(x,y)∈D

get with probability at least 1 − _δ/12 over the random initalization,_

1/4[!]

_B_ 2 log(24/δ)
_S(_ ) 1 + + 2R[2][√]md + R
_R_ _F_ _≤_ _√2n_ _m_

 


2n

_rn(w[(0)])._
_mλmin(Θ)[b]_

2n

_mλmin(Θ)[b]_ _[r][n][(][w][(0)][)][, we]_

q

2 log(24/δ).


Then by equation (32), if m ≥ poly _n, λ[−]min[1]_ [(]Θ)[b] _, δ[−][1][], then with probability at least 1 −_ _τ_ _[d][−][n][+1]_ _−_

_τ_ _[d]_ _−_ 11δ/12 over the samples and random initialization,


log(n/δ)


_S(_ )
_R_ _F_ _≤O_


Finally, by Lemma C.1, we have with probability at least 1 − _τ_ _[d][−][n][+1]_ _−_ _τ_ _[d]_ _−_ _δ over the samples and_
random initialization,


E(x,y) _ℓ˜_ _f_ (w[(][∞][)], x), y
_∼D_ _≤O_
h  i


log(n/δ)


for some constant τ ∈ (0, 1) that depend only on the subgaussian moment of the entries.

This completes the proof.


-----

