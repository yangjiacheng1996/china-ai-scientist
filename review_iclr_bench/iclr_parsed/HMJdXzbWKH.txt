# ONLINE TARGET Q-LEARNING WITH REVERSE EXPE## RIENCE REPLAY: EFFICIENTLY FINDING THE OPTIMAL
# POLICY FOR LINEAR MDPS

**Naman Agarwal, Prateek Jain, Dheeraj Nagaraj, Praneeth Netrapalli**
Google Research
_{namanagarwal,prajain,dheeraj,pnetrapalli}@google.com_

**Syomantak Chaudhuri**
University of California, Berkeley
syomantak@berkeley.edu

ABSTRACT

Q-learning is a popular Reinforcement Learning (RL) algorithm which is widely
deployed with function approximation (Mnih et al., 2015). In contrast, existing
theoretical results are pessimistic about Q-learning. For example, Q-learning does
not converge even with linear function approximation for linear MDPs ((Baird,
1995)) and even for tabular MDPs with synchronous updates, Q-learning has suboptimal sample complexity (Li et al., 2021; Azar et al., 2013). The goal of this
work is to bridge the gap between practical success of Q-learning and the relatively
pessimistic theoretical results. The starting point of our work is the observation
that in practice, Q-learning is used with two important modifications: (i) training with two networks, called online network and target network simultaneously
(online target learning, or OTL), and (ii) experience replay (ER) (Mnih et al.,
2015). While they play a significant role in the practical success of Q-learning,
a thorough theoretical understanding of how these two modifications improve the
convergence behavior of Q-learning has been missing in literature. By carefully
combining Q-learning with OTL and reverse experience replay (RER) (a form of
experience replay), we present novel methods Q-Rex and Q-RexDaRe (Q-Rex+
data reuse). We show that Q-Rex efficiently finds the optimal policy for linear
MDPs (or more generally for MDPs with zero inherent Bellman error with linear approximation (ZIBEL)) and provide non-asymptotic bounds on sample complexity – the first such result for a Q-learning method for this class of MDPs under standard assumptions. Furthermore, we demonstrate that Q-RexDaRe in fact
achieves near optimal sample complexity in the tabular setting, improving upon
the existing results for vanilla Q-learning.

1 INTRODUCTION

Reinforcement Learning (RL) has been shown to be highly successful in practice for a variety of
long term decision making problems (Mnih et al., 2015). Several classical works have studied RL
methods like TD-learning, Q-learning and their variants (Sutton & Barto, 2018; Bertsekas, 2011;
Borkar & Meyn, 2000; Sutton, 1988; Tsitsiklis & Van Roy, 1997; Watkins & Dayan, 1992; Watkins,
1989) but the guarantees are mostly asymptotic and therefore do not sufficiently answer important
questions that are relevant to practitioners who struggle with constraints on the number of data points
and the computation power. Recent works provide non-asymptotic results for a variety of important
settings (Kearns & Singh, 1999; Beck & Srikant, 2012; Qu & Wierman, 2020; Ghavamzadeh et al.,
2011; Bhandari et al., 2018; Chen et al., 2020; 2019; Dalal et al., 2018a;b; Doan et al., 2020; Gupta
et al., 2019; Srikant & Ying, 2019; Weng et al., 2020; Yang & Wang, 2019; Zou et al., 2019a).

Despite a large body of work, several aspects of fundamental methods like Q-learning (Watkins &
Dayan, 1992) are still ill-understood. Q-learning’s simplicity and the ability to learn from off-policy


-----

data makes it widely applicable. However, theoretical analyses show that even with linear function
_approximation and when the approximation is exact, Q-learning can fail to converge even in simple_
examples (Baird, 1995; Boyan & Moore, 1995; Tsitsiklis & Van Roy, 1996). Furthermore, even in
the simple case of tabular RL with synchronous updates, Q-learning is known to have sub-optimal
sample complexity (Wainwright, 2019a; Li et al., 2021).

However, Q-learning has seen tremendous practical success when deployed with “heuristic” modifications like experience replay (ER) and online target learning (OTL). ER is used to alleviate the
issues arising due highly dependent samples in an episode whereas OTL helps stabilize the Q iteration. Mnih et al. (2015) conducted extensive experiments to show that both these techniques, are
essential for the success of Q-learning. But, existing analyses for ER with Q-learning either require
stringent assumptions (Carvalho et al., 2020) to ensure convergence to a good Q value, or assume
that ER provides i.i.d. samples which might not hold in practice (Fan et al., 2020; Carvalho et al.,
2020). In this paper, we attempt to bridge the gap between theory and practice, by rigorously investigating how Q-learning performs with these practical heuristics. We thus introduce two model
free algorithms: Q-Rex and Q-RexDaRe that combine the standard Q-learning with OTL and re_verse experience replay (RER). RER is a form of ER which was recently studied to unravel spurious_
correlations present while learning form Markovian data in the context of system identification (Jain
et al., 2021b). We show that OTL stabilizes the Q value by essentially serving as a variance reduction technique and RER unravels the spurious correlations present in the Markovian data to remove
inherent biases introduced in vanilla Q learning.

These simple modifications have surprisingly far-reaching consequences. Firstly, this allows us to
show that unlike vanilla Q-learning, Q-Rex finds the optimal policy for MDPs with an exact linear
function representation of the Bellman operator. and allows us to derive non-asymptotic sample
complexity bounds. In the tabular setting, Q-Rex even with asynchronous data is able to match the
best known bounds for Q-learning with synchronous data. Its variant Q-RexDaRe, which reuses
old samples, admits nearly optimal sample complexity for recovering the optimal Q-function in the
tabular setting. Previously, only Q-learning methods with explicit variance-reduction techniques (not
popular in practice) (Wainwright, 2019b; Li et al., 2020b) or model based methods (Agarwal et al.,
2020; Li et al., 2020a) were known to achieve such a sample complexity bound. Our experiments
show that when the algorithmic parameters are chosen carefully, Q-Rex and its variants outperform
both vanilla Q-learning and OTL+ER+Q-learning with the same parameters (see Appendix A).

To summarize, in this work, we study Q-learning with practical heuristics like ER and OTL, and
propose two concrete methods Q-Rex and Q-RexDaRe based on OTL and reverse experience
replay – a modification of the standard ER used in practice. We show that Q-Rex is able to find
the optimal policy for ZIBEL MDPs, with a strong sample complexity bound which is the first such
result for Q-learning. We also show that Q-RexDaRe obtains nearly optimal sample complexity for
the simpler tabular setting despite not using any explicit variance reduction technique. See Table 1
for a comparison of our guarantees against the state-of-the-results for the tabular setting.

**Organization** We review related works in next subsection. In Section 2 we develop the MDP
problem which we seek to solve and present our algorithm, Q-Rex in Section 3. The main theoretical
results are presented in Section 4. We present a brief overview of the analysis in Section 5 and
present our experiments in Section A. We provide minimax lower bounds for the asynchronous
tabular setting in Section K. Most of the formal proofs are relegated to the appendix.

1.1 RELATED WORKS

**Tabular Q-learning** Tabular MDPs are the most basic examples of MDPs where the state space
(S) and the action space (A) are both finite and the Q-values are represented by assigning a unique
co-ordinate to each state-action pair. This setting has been well studied over the last few decades and
convergence guarantees have been derived in both asymptotic and non-asymptotic regimes for popular model-free and model-based algorithms. Azar et al. (2013) shows that the minimax lower bounds
on the sample complexity of obtaining the optimal Q-function up-to ϵ error is (1|S||A|γ)[3]ϵ[2][, where][ γ][ is]

_−_
the discount factor. Near sample-optimal estimation is achieved by several model-based algorithms
(Agarwal et al., 2020; Li et al., 2020a) and model-free algorithms like variance reduced Q-learning
(Wainwright, 2019b; Li et al., 2020b). (Li et al., 2021) also shows that vanilla Q-learning with standard step sizes, even in the synchronous data setting – where transitions corresponding to each state


-----

|Paper|Algorithm|Data Type|Sample Complexity|
|---|---|---|---|
|(GHAVAMZADEH ET AL., 2011)|SPEEDY Q-LEARNING|SYNCHRONOUS||S||A| ϵ2(1−γ)4|
|(WAINWRIGHT, 2019B)|VARIANCE REDUCED Q-LEARNING|SYNCHRONOUS||S||A| ϵ2(1−γ)3|
|(LI ET AL., 2020B)|VARIANCE REDUCED Q-LEARNING|ASYNCHRONOUS|1 µminϵ2(1−γ)3|
|(LI ET AL., 2020B)|Q-LEARNING|ASYNCHRONOUS|1 µminϵ2(1−γ)5|
|(LI ET AL., 2021)|Q-LEARNING|SYNCHRONOUS||S||A| ϵ2(1−γ)4|
|THIS WORK, THEOREM 2|Q-LEARNING+ OTL + RER (Q-REX)|ASYNCHRONOUS||S||A| ϵ2(1−γ)4|
|THIS WORK, THEOREM 3|Q-REX+ DATA-REUSE (Q-REXDARE)|ASYNCHRONOUS|max( d¯, ϵ1 ) 2 µmin(1−γ)3|


Table 1: Comparison of tabular Q-learning based algorithms. _d[¯]_ is maximum size of support
1 _≤|S|_
of P (·|s, a). In the case of asynchronous setting, _µmin_ [is roughly equivalent to][ |S||A|][ in the syn-]

chronous setting. We use the color green to represent results with optimal dependence on (1 _−_ _γ)[−][1]._

action pair are sampled independently at each step – suffers from a sample complexity of (1|S||A|−γ)[4]ϵ[2]

and the best known bounds in the asynchronous setting – where data is derived from a Markovian
trajectory and only one Q value is updated in each step – is (1|S||A|γ)[5]ϵ[2][ . These results seem unsatis-]

_−_
factory since γ ∈ (0.99, 1) in most practical applications. In contrast, our algorithm Q-Rex with
_asynchronous data has a sample complexity that matches Q-learning bound with synchronous data_
and its data-efficient variant Q-RexDaRe has near minimax optimal sample complexity (see Table 1). For details on model based algorithms, and previous works with sub-optimal guarantees we
refer to (Agarwal et al., 2020; Li et al., 2020b). We note that the lower bounds apply only to the
synchronous case (i.e, when every state-action pair is sampled at every step). We provide minimax
lower-bounds which show that the bound is tight in the asynchronous case too (see Theorem 5 in
Section K), where |S||A| in the synchronous case of (Azar et al., 2013) is replaced by _µmin1_ [.]

**Q-learning with Linear Function Approximation** Since tabular Q-learning is intractable in most
practical RL problems due to a large state space S, function approximation is deployed. Linear
function approximation is the simplest such case where the Q-function is approximated with a linear
function of the ‘feature embedding’ associated with each state-action pair. However, Q-learning can
be shown to diverge even in the simplest cases as was first noticed in (Baird, 1995), which also
introduced residual gradient methods which converged rather slowly but provably. We will only
discuss recent works closest to our work and refer the reader to (Carvalho et al., 2020; Yang &
Wang, 2019) for a full survey of various works in this direction. SARSA is the on-policy control
variant of Q-learning where the challenge is to explore the state-space while learning the optimal
policy. Unlike Q-learning, SARSA is inherently stable due to its on-policy nature (Gordon, 2000).
Therefore, we do not compare our results to the results of on-policy control algorithms like SARSA.
We refer to (Zou et al., 2019b; Perkins & Precup, 2002; Melo et al., 2008) for further details.

Yang & Wang (2019) consider MDPs with approximate linear function representation. They require
additional assumptions like finite state-action space and existence of known anchor subsets which
might not hold in practice. Our results on the other hand hold with standard assumptions, with
_asynchronous updates and can handle infinite state-action spaces (see Theorem 1). Similarly, Chen_
et al. (2019) consider Q-learning with linear function approximation which need not be exact. But
the result requires the restrictive assumption that the offline policy is close to the optimal policy. In
contrast, we consider the less general but well-studied case of MDPs with zero inherent Bellman
error and provide global convergence without restrictive assumptions on the behaviour policy.

Under the most general conditions Maei et al. (2010) present the Greedy-GQ algorithm which converges to a point asymptotically instead of diverging. Similar results are obtained by Carvalho et al.
(2020) for Coupled Q-learning, a 2-timescale variant of Q-learning which uses a version of OTL and
ER[1]. This algorithm experimentally resolves the popular counter-examples provided by (Tsitsiklis
& Van Roy, 1996; Baird, 1995). However, the value function guarantees in Carvalho et al. (2020,

1The version of ER used in Carvalho et al. (2020) makes the setting completely synchronous as opposed to
the asynchronous setting considered by us.


-----

Theorem 2) (albeit without sample complexity guarantees) requires very stringent assumptions and
even in the case of tabular Q-learning might not converge to the optimal policy.

**Experience Replay and Reverse Experience Replay** Reinforcement learning involves learning
on-the-go with highly correlated correlated data. Iterative learning algorithms like Q-learning can
sometimes get coupled to the Markov chain resulting in sub-optimal convergence. Experience replay
(ER) was introduced in order to mitigate this drawback (Lin, 1992) – here a large FIFO buffer of
a fixed size stores the streaming data and the learning algorithm samples a data point uniformly at
random from this buffer at each step. This makes the samples look roughly i.i.d., thus breaking the
harmful correlations. Reverse experience replay (RER) is a form of ER which stores data in a buffer
but processes the data points in the reverse order as stored in the buffer. This was introduced in
entirely different contexts by (Rotinov, 2019; Jain et al., 2021b;a). In this work, we note that reverse
order traversal endows a super-martingale structure which yields the strong concentration result in
Theorem 4, which is not possible with forward order traversal (see (Jain et al., 2021b, Section 3.1)
for a brief demonstration of this fact). We can also look at RER is through the lens of Dynamic
programming (Bertsekas, 2011) – where the value function is evaluated backwards starting from
time T to time 1 - similar to how RER bootstraps present to the future.

In the context of reinforcement learning, works like Bhandari et al. (2018); Zou et al. (2019b) obtain
finite time convergence guarantees for RL algorithms under the mixing assumptions just like this
work. The strategy followed in these works is that if two samples are _O[˜](τmix) time apart, then_
they are approximately independent and thus analysis for i.i.d. data can be applied. The sample
complexity to obtain ϵ error here is O( _[τ]ϵ[mix][2][ )][. Note that this is no better than keeping one every][ τ][mix]_

samples and throwing away the rest and under general mis-specified linear function representation,
we might not be able to do any better (Bresler et al., 2020). In this work, we show that when the
linear approximation is well specifed (ZIBEL), we can use RER to obtain a sample complexity of
_O(τmix +_ _ϵ[1][2][ )][, where the mixing time serves as a cut-off.]_

**Online Target Learning** OTL (Mnih et al., 2015) maintains two different Q-values (called online
Q-value and target Q-value) where the target Q-value is held constant for some time and only the
online Q-value is updated by ‘bootstrapping’ to the target. After a number of such iterations, the
target Q-value is set to the current online Q value. OTL thus attempts to mitigate the destabilizing
effects of bootstrapping by removing the ‘moving target’. This technique has been noted to allow
for an unbiased estimation of the bellman operator (Fan et al., 2020) and when trained with large
batch sizes is similar to the well known neural fitted Q-iteration (Riedmiller, 2005).

2 PROBLEM SETTING

**Markov Decision Process** We consider infinite horizon, time homogenous Markov Decision Processes (MDPs) and we denote an MDP by MDP(S, A, γ, P, R) where S is the state space, A is the
action space, γ ∈ [0, 1) is the discount factor, P (s[′]|s, a) is the probability of transition to state s[′]
from the state s on action a. We assume that S and A are compact subsets of R[n] (for some n ∈ N).
_R : S × A →_ [0, 1] is the deterministic reward associated with every state-action pair.

We will think of an MDP as an agent that is aware of its current state and it can choose the action to
be taken. Suppose the agent takes action π(s), where π : S →A, at state s ∈S, then P along with
the ‘policy’ π induces a Markov chain over S, whose transition kernel is denoted by P _[π]. We write_
the γ-discounted value function of the MDP starting at state s to be:


_V (s, π) = E[_


_γ[t]R(St, At)|S0 = s, At = π(St)∀t]._ (1)
_t=0_

X


It is well-known that under mild assumptions, there exists at least one optimal policy π[∗] such that
the value function V (s, π) is maximized for every sand that there is an optimal Q-function, Q[∗] :
R, such that one can find the optimal policy as π[∗](s) = arg maxa _Q[∗](s, a), optimal_
_S × A →_ _∈A_
value function as V _[∗](s) = maxa∈A Q[∗](s, a) and it satisfies the following fixed point equation._
_Q[∗](s, a) = R(s, a) + γEs′_ _P (_ _s,a)[max_ (s, a) _._ (2)
_∼_ _·|_ _a[′]_ _[Q][∗][(][s][′][, a][′][)]]_ _∀_ _∈S × A_
_∈A_

(2) can be alternately viewed as Q[∗] being the fixed point of the Bellman operator T, where
(Q)(s, a) = R(s, a) + γEs′ _P (_ _s,a)[max_
_T_ _∼_ _·|_ _a[′][ Q][(][s][′][, a][′][)]][.]_


-----

The basic task at hand is to estimate Q[∗](s, a) from a single trajectory (st, at)[T]t=1 [such that]
_sEt [+1rt ∼|st =P s, a(·|stt, a =t a) along with rewards] = R(s, a). We refer to Section (rt)[T]t=1[, where] B for rigorous definitions.[ r][1][, . . ., r][T]_ [are random variables such that]

**Q-learning** Since the transition kernel P (and hence the Bellman operator T ) is often unknown
in practice, Equation (2) cannot be directly used to to estimate the optimal Q-function. To this end,
we resort to estimating Q[∗] using observations from the MDP. An agent traverses the MDP and we
obtain the state, action, and the reward obtained at each time step. We assume the off-policy setting
which means that the agent is not in our control, i.e., it is not possible to choose the agent’s actions;
rather, we just observe the state, the action, and the corresponding reward. Further, we assume that
the agent follows a time homogeneous policy π(s) for choosing its action at state s.

Given a trajectory {st, at, rt}t[T]=1 [generated using some unknown behaviour policy][ π][, we aim to]
estimate Q[∗] in a model-free manner - i.e, estimate Q[∗] without directly estimating P . We further
assume that the trajectory is given to us as a data stream so we can not arbitrarily fetch the data for
any time instant. A popular method to estimate Q[∗] is using the Q-learning algorithm. In this online
algorithm, we maintain an estimate of Q[∗](s, a) at time t, Qt(s, a) and the estimate is updated at
time t for (s, a) = (st, at) in the trajectory. Formally, with step-sizes given as _ηt_, Q-learning
_{_ _}_
performs the following update at time t,

_Qt+1(st, at) = (1_ _ηt)Qt(st, at) + ηt_ _rt + γ max_
_−_  _a[′]∈A_ _[Q][t][(][s][t][+1][, a][′][)]_ (3)

_Qt+1(s, a) = Qt(s, a)_ (s, a) = (st, at).
_∀_ _̸_


In this work, we focus on two special classes of MDPs which are popular in literature.

**Linear Markov Decision Process** Linear MDPs (Jin et al., 2020) is a popular example of exact
linear approximation for which statistically and computationally tractable algorithms are available.
We use the definition from Jin et al. (2020), stated as Definition 1.
**Definition 1. An MDP(S, A, γ, P, R) is a linear MDP with feature map φ : S × A →** R[d], if

_1. there exists a vector θ ∈_ R[d] _such that R(s, a) = ⟨φ(s, a), θ⟩, and_

_2. there exists d unknown (signed) measures over_ _β(_ ) = _β1(_ ), . . ., βd( ) _such that the_
_S_ _·_ _{_ _·_ _·_ _}_
_transition probability P_ (·|s, a) = ⟨φ(s, a), β(·)⟩.

In the rest of this paper, in the tabular setting we assume that the dimension d = |S × A| and we use
a one hot embedding where we map (s, a) _φ(s, a) = es,a, a unique standard basis vector. It is_
_→_
easy to show that this system is a linear MDP (Jin et al., 2020) and Q-learning in this setting reduces
to the standard tabular Q-learning (3). However, when the assumption of a tabular MDP allows us
to obtain stronger results, we will present the analysis separately.

**Inherent Bellman Error** There is another widely studied class of MDPs which admit a good
linear representation (Zanette et al., 2020; Munos & Szepesv´ari, 2008; Szepesv´ari & Smart, 2004).

**Definition 2. (ZIBEL MDP) For an M = MDP(S, A, γ, P, R) with a feature map φ : S ×A →** R[d],
_we define the inherent Bellman error (IBE(M)) as:_

_θsup∈R[d][ inf]θ[′]∈R[d]_ (s,asup)∈S×A _⟨φ(s, a), θ[′]⟩−_ _R(s, a) −_ _γEs′∼P (·|s,a) supa[′]∈A⟨θ, φ(s[′], a[′])⟩_

_If IBE(M) = 0, then call this MDP a ZIBEL (zero inherent Bellman error with linear function_
_approximation) MDP._

The class of ZIBEL MDPs is strictly more general than the class of linear MDPs (Zanette et al.,
2020). Both these classes of MDPs have the property that there exists a vector w[∗] _∈_ R[d] such that
the optimal Q-function, Q[∗](s, a) = ⟨φ(s, a), w[∗]⟩ for every (s, a) ∈S × A, which can be explicity
expressed as a function of θ, β and Q[∗]. More generally, they allow us to lift the Bellman iteration to
R[d] _exactly and update our estimates for w[∗]_ values directly (Lemmas 3, 4).

Hence, we can focus on estimating Q[∗] by estimating w[∗]. To this end, the standard Q-Learning
approach to learning the Q function can be extended to the linear case as follows:


-----

_rt + γ max_ _φ(st, at)._
_a[′]∈A[⟨][φ][(][s][t][+1][, a][′][)][, w][t][⟩−⟨][φ][(][s][t][, a][t][)][, w][t][⟩]_


_wt+1 = wt + ηt_


The above update can be seen as a gradient descent step on the loss function f (wt) =
( _φ(st, at), wt_ target)[2] where target = rt+γ maxa′ _φ(st+1, a[′]), wt_ . This update while heavily
_⟨_ _⟩−_ _⟨_ _⟩_
used in practice, has been known to be unstable and does not converge to w[∗] in general. The reason often cited for this phenomenon is the presence of the ‘deadly triad’ of bootstrapping, function
approximation, and off-policy learning.

2.1 ASSUMPTIONS

We make the following assumptions on the MDPs considered through the paper in order to present
our theoretical results.
_RAssumption 1.(s, a) ∈_ [0, 1]. The MDP M has IBE(M) = 0 (Definition 2), ∥φ(s, a)∥2 ≤ 1. Furthermore,
**Assumption 2. Let Φ := {φ(s, a) : (s, a) ∈S × A}. Φ is compact, span(Φ) = R[d]** _and (s, a) →_
_φ(s, a) is measurable._

Even when span(Φ) ̸= R[d], our results hold after we discard the space orthogonal to the span of embedding vectors in Assumption 4 and note that Q-Rex does not update the iterates along span(Φ)[⊥].
**Definition 3. For r > 0, let N** (Φ, ∥· ∥2, r) be the r-covering number under the standard Euclidean
_norm over R[d]. Define:_

_∞_
_CΦ :=_ 0 log N (Φ, ∥∥2, r)dr
Z

p

of tabular MDPs it is easy to show thatObserve that since Φ is a subset of the unit Euclidean ball in CΦ _C[√]log d._ R[d], CΦ ≤ _C√d. However, in the case_
_≤_
**Definition 4. We define the norm ∥· ∥φ over R[d]** _by ∥x∥φ = sup(s,a) |⟨φ(s, a), x⟩|._

This is the natural norm of interest for the problem (Lemmas 2 and 4). We assume the existence
of a fixed (random) behaviour policy π : S → ∆(A) which selects a random action corresponding
to each state. At each step, given (st, at) = (s, a), st+1 _P_ ( _s, a) and at+1_ _π(st+1). This_
gives us a Markov kernel over which specifies the law of ∼ (s·|t+1, at+1) conditioned on ∼ (st, at).
_S ×A_
We will denote this kernel by P _[π]. This setting is commonly known as the off-policy asynchronous_
setting. We make the following assumption which is standard in this line of work.
**Assumption 3. There exists a unique stationary distribution µ for the kernel P** _[π]. Moreover, this_
_Markov chain is exponentially ergodic in the total variation distance with mixing time τmix. That is,_
_there exist a constant Cmix for every t ∈_ N

_x∈S×Asup_ TV((P _[π])[t](x, ·), µ) ≤_ _Cmix exp(−t/τmix)_

_In the tabular setting, we will use the standard definition of τmix instead:_

_τmix = inf{t :_ _x∈S×Asup_ TV((P _[π])[t](x, ·), µ) ≤_ 1/4} .


_Here TV refers to the total variation distance._
**Assumption 4. There exists κ > 0 such that: E(s,a)∼µφ(s, a)φ[⊤](s, a) ⪰** _κ[I]_ _[.]_

**Remark 1. (Bresler et al., 2020, Theorem 1) shows that even linear regression with Markovian data,**
_zero noise and ℓ[2]_ _recovery is hard when the condition number κ or the mixing time τmix are too large._
_Hence, our setup of noisy reinforcement learning with ℓ[∞]_ _error also requires these quantities to be_
_small. Therefore, Assumptions 3 and 4 are necessary in order to obtain non-trivial bounds._

In the tabular setting, Assumption 4 manifests itself as _κ[1]_ [=][ µ][min][ := min][(][s,a][)][ µ][(][s, a][)][ which is also]

standard (Li et al., 2020b). Whenever we discuss high probability bounds (i.e, probability at least
1 − _δ), we assume that δ ∈_ (0, 1/2). Similarly, we will assume that the discount factor γ ∈ (1/2, 1)
so that we can absorb poly(1/γ) factors into constants.


-----

**Algorithm 1 Q-Rex**

1: Input: learning rates η, horizon T, discount factor γ, trajectory Xt = {st, at, rt}, Buffer size
_B, Buffer gap u, Number of inner loop buffers N_

2: Total buffer size: S ← _B + u, Outer-loop length: K ←_ _NST_ [, Initialization][ w]1[1][,][1] = 0

3: for k = 1, . . ., K do
4: **for j = 1, . . ., N do**

5: Form buffer Buf = _X1[k,j][, . . ., X]S[k,j]_ _i_ _XNS(k_ 1)+S(j 1)+i
_{_ _[}][, where,][ X]_ _[k,j]_ _←_ _−_ _−_

6: Define for all i ∈ [1, S], φ[k,j]i ≜ _φ(s[k,j]i_ _, a[k,j]i_ ).

7: **for i = 1, . . ., B do**


8: _wi[k,j]+1_ [=][ w]i[k,j]+η rB[k,j]+1−i [+][ γ][ max]a[′]∈A[⟨][φ][(][s]B[k,j]+2−i[, a][′][)][, w]1[k,][1][⟩−⟨][φ]B[k,j]+1−i[, w]i[k,j]⟩ _φ[k,j]B+1−i_

9: **Option I: w1[k,j][+1]** = wB[k,j]+1

10: **Option II: w1[k,j][+1]** = _B[1]_ _Bi=1_ _[w]i[k,j]+1_

11: **Option I: w1[k][+1][,][1]** = w1[k,N] [+1]

12: **Option II: w1[k][+1][,][1]** = _N[1]_ _Nl=2P+1_ _[w]1[k,l]_

13: Return w1[K][+1][,][1]

P

Figure 1: Illustration of Online Target Q-learning with Reverse Experience Replay

3 OUR ALGORITHM

As discussed in the introduction, we incorporate RER and OTL into Q-learning and introduce the
algorithms Q-Rex (Online Target Q-learning with reverse experience replay, Algorithm 1), its sample efficient variant Q-RexDaRe (Q-Rex + data reuse, Algorithm 2) and its episodic variant EpiQRex (Episodic Q-Rex, Algorithm 3). Since Q-RexDaRe and EpiQ-Rex are only minor modifications
of Q-Rex, we refer the reader to the appendix for their pseudocode.

Q-Rex is parametrized by K the number of iterations in the outer-loop, N the number of buffers
within an outer-loop iteration, B the size of a buffer and u the gap between the buffers. The algorithm
has a three-loop structure where at the start of every outer-loop iteration (indexed by k ∈ [K]), we
checkpoint our current guess of the Q function given by w1[k,][1][. Each outer-loop iteration corresponds]
to an inner-loop over the buffer collection with N buffers, i.e. at iteration j ∈ [N ], we collect a
buffer of size B + u consecutive state-action-reward tuples. For every collected buffer we consider
the first B collected experiences and perform the target based Q-learning update in the reverse order
for these experiences. We refer Figure 1 for an illustration of the processing order. Of note, is the
usage of checkpointed target network in the RHS of the Q-learning update through the entirety of
the outer-loop iteration, i.e. for a fixed k and for all j, i, our algorithm sets

_wi[k,j]+1_ [=][ w]i[k,j] + η rB[k,j]+1−i [+][ γ][ max]a[′]∈A[⟨][φ][(][s]B[k,j]+2−i[, a][′][)][,][ w]1[k][,][1][⟩−⟨][φ]B[k,j]+1−i[, w]i[k,j]⟩ _φ[k,j]B+1−i_

Figure 1 provides an illustration of the processing order for our updates. It can be seen that the
number of experiences collected through the run of the algorithm is T = KN (B + u). For the sake
of simplicity, we will assume that the initial point, w1[1][,][1] = 0. Essentially the same results hold for
arbitrary initial conditions. Q-RexDaRe is a modification of Q-Rex where we re-use the data from
the first outer-loop iteration (i.e, data from k = 1) in every outer-loop iteration (i.e, k > 1).


-----

|Setting|K|N|u|B|η|
|---|---|---|---|---|---|
|ZIBEL MDP (THEOREM 1)|≥1|  > C B3κ log δ(K 1−κ η γ)|≥C1τmix log( Cmix δKN)|= 10u|< C2 min( CΦ2+(1 lo− gγ (K)2 /δ)), B1)|
|TABULAR MDP (THEOREM 2)|≥C2  log  −−1 γγ2 11|> C B4 µτ mmi ix log( |S|| δA|K) n|≥C1τmix log( K δN )|= 10u|< |SC |3 log  | δA|K|
|TABULAR MDP (THEOREM 3)|≥C2log  γ1 γ 1−1−|> C B4 µτ mmi ix log( |S| δ|A|) n|≥C1τmix log( K δN )|= 10u|< C3 log(1  − |Sγ |) δ|2 d¯ A||
|ZIBEL MDP (THEOREM 1)|β1 (1−γ)|κβ2 max  ϵ2(C 1−Φ2+ γ)β 42, 1 τmix|τmix log  K δN|10u|min  (1 C− Φ2γ +) β4ϵ 32, B1|
|TABULAR MDP (THEOREM 2)|β12 1−γ|µm1 max β5, ηβ τm1 ix in|τmix log  K δN|10u|(1−γ)3 min(ϵ, ϵ2) β5|
|TABULAR MDP (THEOREM 3)|β1 (1−γ)|µm1 max β5, ηβ τm1 ix in|τmix log  K δN|10u|minϵ2(1 β− 4γ)3, (1 d− ¯βγ 5) 2, ϵ( √1− d¯βγ 4)3|


Table 2: Parameter constraints (first 3 rows) and choice for < ϵ error (last 3 rows) for our algorithms.
Here the poly-log factors βi are given by β1 = log (1−γ) min(1 _ϵ,1)_, β3 = log (1−γ)δ min(1 _ϵ,1)_,

_K_
_β2 = log(κ) + β3, β4 = log_ _|S||A|δ_, β5 = log _|S||A|δ_   .    
     

**Remark 2. For the sake of clarity, we only analyze the algorithms Q-Rex and Q-RexDaRe for data**
_from a single trajectory with Option I. Option II involves averaging of the iterates which boosts the_
_convergence – indeed we can obtain much better bounds in this setting by using standard analysis._

4 MAIN RESULTS


We will now provide finite time convergence analysis and sample complexity for the algorithms
Q-Rex and Q-RexDaRe. Recall that K is the number of outer-loops, N is the number of buffers
inside an outer-loop iteration, B is the buffer size and u is the size of the gap. In what follows, we
will take u = O[˜](τmix), B = 10u, K = O[˜]( 1 1 _γ_ [)][. We also note that the total number of samples used]

_−_
is NK(B + u) for Q-Rex and N (B + u) for Q-RexDaRe since we reuse data in each outer-loop
iteration. In what follows, by Q[K]1 [+1][,][1](s, a), we denote _φ(s, a), w1[K][+1][,][1]_ which is our estimate for
_⟨_ _⟩_
the optimal Q function. Here w1[K][+1][,][1] is the output of either Q-Rex or Q-RexDaRe at the end of
_K outer-loop iterations. Define ∥Q[K]1_ [+1][,][1] _−_ _Q∥∞_ := sup(s,a)∈S×A _Q[K]1_ [+1][,][1](s, a) − _Q[∗](s, a)_ . We
first consider the performance of Q-Rex with data derived from a linear MDP (Defintion 1) or a
ZIBEL MDP (Definition 2) and satisfying the Assumptions in Section 2.1.
**Theorem 1 (ZIBEL /Linear MDP). Suppose we run Q-Rex using Option I with data from an MDP**
_with IBE = 0. There exists constants C1, C2, C3, C4, C5 > 0 such that whenever the parameter_
_bounds given in Table 2 (row 1) are satisfied, then with probability at-least 1 −_ _δ, we must have:_

_K_

_∥Q[K]1_ [+1][,][1] _−_ _Q[∗]∥∞_ _≤_ 1γ−[K]γ [+][ C][4] _δ(1Kκ−γ)[4][ exp]_ _−_ _[ηNB]κ_ + C5r _ηhCΦ[2](1[+log]−γ)[4]_ _δ_ i

_Given ϵ ∈_ (0, (1−1 _γ)_ []][, and the parameters as given in Table]q  _[ 2][ (row 4)(up to constant factors), then]_

_with probability at-least 1 −_ _δ: ∥Q1[K][+1][,][1](s, a) −_ _Q[∗](s, a)∥∞_ _< ϵ. This has a sample complexity_

Θ(NKB) = O[˜] _κ max_ (1C−Φ[2]γ[+1])[5]ϵ[2][,][ τ]1−[mix]γ
  

We now consider the performance of Q-Rex and Q-RexDaRe in the case of tabular MDPs. We
refer to Table 1 for a comparison of our results to the state-of-art results provided in literature for
Q-learning based algorithms.
**Remark 3. To the best of our knowledge, Theorem 1 presents the first non-asymptotic convergence**
_results for Q-learning based methods for ZIBEL MDPs under standard assumptions. Notice that_
_the sample complexity scales as_ _ϵ[1][2][ +][ τ][mix][ instead of][ τ]ϵ[mix][2][ like in][ Zou et al.][ (][2019b][);][ Bhandari et al.]_

_(2018). This is because in the case of ZIBEL MDPs RER brings out the super-martingale structure_
_present in the problem which forward pass does not._
**Theorem 2 (Tabular MDP). Suppose we run Q-Rex using Option I with data derived from tabular**
_MDPs. Whenever the algorithmic parameters are picked as given in Table 2 (row 2) for some_
_universal constants C1, . . ., C5, we obtain with probability at-least 1_ _δ:_
_−_

_K_ _K_

_∥Q[K][+1]_ _−_ _Q[∗]∥∞_ _< C5_ " 1γ−[L]γ [+] exp −(1[ηµ]−γ[min])2[2] _[NB]_  + _η log(1 _ _−γ|S||A|)δ[3]_  + r _η log(1 _ _−γ|S||A|)δ[3]_  #


-----

_Where L =_ logb11K−1 _γ_ _. Given ϵ ∈_ (0, 1−1 _γ_ []][, and the parameters are picked as given in Table][ 2][ (row]

_5), then with probability at-least 1_ _δ, we have:_ _Q[K][+1]_ _Q[∗]_ _< ϵ . This gives us a sample_
_−_ _∥_ _−_ _∥∞_
_complexity of_
Θ(NKB) = O[˜] _µmin1_ [max] (1 _γ)[4]_ min(1 _ϵ,ϵ[2])_ _[,][ τ]1_ [mix]γ _._

_−_ _−_

  

Even though the sample complexity provided in Theorem 2 matches the sample complexity of syn_chronous Q-learning even with asynchronous data, it is still sub-optimal with respect to the min-max_
lower bounds (i.e, it has a dependence of has a dependence of (1 1γ)[4][ instead of the optimal] (1 1γ)[3][ ).]

_−_ _−_

We resolve this gap for Q-RexDaRe in Theorem 3. For tabular MDPs, the number states can be
large but the support of P (·|s, a) is bounded in most problems of practical interest. Consider the
following assumption (note that this holds for every tabular MDP with _d[¯] = |S|.)_

**Assumption 5. Tabular MDP is such that |supp(P** (·|s, a))| ≤ _d[¯] ∈_ N.

**Theorem 3 (Tabular MDP with Data Reuse). For tabular MDPs, suppose additionally Assumption 5**
_holds and we run Q-RexDaRe using Option I. There exist universal constants C1, C2, C3, C4 such_
_that when the parameter values satisfy the bounds in Table 2 (row 3), with probability at-least 1_ _−_ _δ:_

_K_

_∥Q[K]1_ [+1][,][1] _−_ _Q[∗]∥∞_ _≤_ _C_ " exp − _[ηµ](1[min]−2γ[NB])[2]_ +γ[K] + _η log(1  |S||A|−γ)δ[3]_  pd¯ + q (1−ηγ)[3][ log]  K|S||A|δ #

_Suppose ϵ ∈_ (0, 1−1 _γ_ []][. If we choose the parameters as per Table][ 2][ (row 6), then with probability]

_at-least 1_ _δ we have:_ _Q[K][+1](s, a)_ _Q[∗]_ _< ϵ. The sample complexity in this case is_
_−_ _∥_ _−_ _∥∞_

Θ(NB) = O[˜] _µmin1_ [max] _τmix,_ _ϵ[2](11_ _γ)[3][,]_ (1 _d¯γ)[2][,]_ _ϵ(1√dγ¯)[3]_ _._

_−_ _−_ _−_

  

5 OVERVIEW OF THE ANALYSIS

We divide the analysis of Q-Rex and Q-RexDaRe into two parts: Analysis of w1[k,][1] obtained at the
end of outer-loop iteration k and the analysis of the algorithm within the outer-loop. The algorithm
reduces to SGD for linear regression with Markovian data within an outer-loop due to OTL. That is,
we try to find w1[k][+1][,][1] such that _w1[k][+1][,][1], φ(s, a)_ _R(s, a) + Es′_ _P (_ _s,a) supa′_ _w1[k,][1][, φ][(][s][′][, a][′][)][⟩][.]_
_⟨_ _⟩≈_ _∼_ _·|_ _⟨_
Therefore, we write w1[k][+1][,][1] = T (w1[k,][1][) +][ ϵ][k][(][w]1[k,][1][)][, where][ T][ is the][ γ][ contractive Bellman operator]
whose unique fixed point is w[∗] and ϵk is the noise to be controlled. Following a similar setting in
in (Jain et al., 2021b), we control ϵk with the following steps:
(1) We introduce a fictitious coupled process (see Section C) (˜st, ˜at, ˜rt) where the data in different
buffers are exactly independent (since the gaps of size u make the buffers approximately independent) and show that the algorithm run with the fictitious data has the same output as the algorithm
run with the actual data with high probability when u is large enough.
(2) We give a bias-variance decomposition (Lemma 5) for the error ϵk where the exponentially decaying bias term helps forget the initial condition and the variance term arises due the inherent noise
in the samples.
(3) We control the bias and variance terms separately in order to ensure that the noise ϵk is small
enough. RER plays a key role in controlling the variance term by endowing it with a supermartingale structure, which is not possible with forward order traversal (see Theorem 4).

The procedure described above allows us to show that w1[k][+1][,][1] _≈T (w1[k,][1][)][ uniformly for][ k][ ≤]_ _[K][,]_
which directly gives us a convergence bound to the fixed point of T i.e, w[∗] (Theorem 1). In the
tabular case, the approximate Bellman iteration connects to the analysis of synchronous Q-learning
in (Li et al., 2021), which allows us to obtain a better convergence guarantee (Theorem 2). To obtain
convergence guarantees for Q-RexDaRe, we first observe that if we re-use the data used in outerloop iteration 1 in all future outer-loop iterations k > 1, ϵk(w1[k,][1][)][ might not be small since][ w]1[k,][1]
depends on ϵk(·). However, (w1[k,][1][)][k][ approximates the deterministic path of the noiseless Bellman]
iterates: ¯w1[1][,][1] := w1[1][,][1] and ¯w1[k][+1][,][1] := T ( ¯w1[k,][1][)][. Since][ ∥][ϵ][k][(][w]1[k,][1][)][∥][∞] _[≤∥][ϵ][k][(][w]1[k,][1][)][ −]_ _[ϵ][k][( ¯]w1[k,][1][)][∥][∞]_ [+]
_∥ϵ[k]( ¯w1[k,][1][)][∥][∞][, we argue inductively that][ ∥][ϵ][k][(][w]1[k,][1][)][ −]_ _[ϵ][k][( ¯]w1[k,][1][)][∥][∞]_ _[≈]_ [0][ since][ w]1[k,][1] _≈_ _w¯1[k,][1]_ and
_∥ϵ[k]( ¯w1[k,][1][)][∥][∞]_ _[≈]_ [0][ since][ ¯]w1[k,][1] is a deterministic sequence and hence w1[k][+1][,][1] _≈_ _w¯1[k][+1][,][1]._


-----

ACKNOWLEDGMENTS

Most of this work was done when D.N. was a graduate student at MIT and was supported in part
by NSF grant DMS-2022448. Part of this work was done when D.N. was a visitor at the Simons
Institute for Theory of Computing, Berkeley. We would also like to thank Gaurav Mahajan for
introducing us to low-inherent Bellman error setting, and providing intuition that our technique
might be applicable in this more general setting (than linear MDP) as well.


-----

REFERENCES

Alekh Agarwal, Sham Kakade, and Lin F Yang. Model-based reinforcement learning with a generative model is minimax optimal. In Conference on Learning Theory, pp. 67–83. PMLR, 2020.

Mohammad Gheshlaghi Azar, R´emi Munos, and Hilbert J Kappen. Minimax pac bounds on the
sample complexity of reinforcement learning with a generative model. Machine learning, 91(3):
325–349, 2013.

Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In
_Machine Learning Proceedings 1995, pp. 30–37. Elsevier, 1995._

Carolyn L Beck and Rayadurgam Srikant. Error bounds for constant step-size q-learning. Systems
_& control letters, 61(12):1203–1208, 2012._

Dimitri P Bertsekas. Dynamic programming and optimal control 3rd edition, volume ii. Belmont,
_MA: Athena Scientific, 2011._

Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference
learning with linear function approximation. In Conference on learning theory, pp. 1691–1692.
PMLR, 2018.

Vivek S Borkar and Sean P Meyn. The ode method for convergence of stochastic approximation
and reinforcement learning. SIAM Journal on Control and Optimization, 38(2):447–469, 2000.

Justin Boyan and Andrew W Moore. Generalization in reinforcement learning: Safely approximating the value function. Advances in neural information processing systems, pp. 369–376, 1995.

Guy Bresler, Prateek Jain, Dheeraj Nagaraj, Praneeth Netrapalli, and Xian Wu. Least squares regression with markovian data: Fundamental limits and algorithms. arXiv preprint arXiv:2006.08916,
2020.

Diogo Carvalho, Francisco S. Melo, and Pedro Santos. A new convergent variant of q-learning with
linear function approximation. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 19412–19421. Cur[ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/](https://proceedings.neurips.cc/paper/2020/file/e1696007be4eefb81b1a1d39ce48681b-Paper.pdf)
[file/e1696007be4eefb81b1a1d39ce48681b-Paper.pdf.](https://proceedings.neurips.cc/paper/2020/file/e1696007be4eefb81b1a1d39ce48681b-Paper.pdf)

Zaiwei Chen, Sheng Zhang, Thinh T Doan, John-Paul Clarke, and Siva Theja Maguluri. Finitesample analysis of nonlinear stochastic approximation with applications in reinforcement learning. arXiv preprint arXiv:1905.11425, 2019.

Zaiwei Chen, Siva Theja Maguluri, Sanjay Shakkottai, and Karthikeyan Shanmugam. Finite-sample
analysis of contractive stochastic approximation using smooth convex envelopes. arXiv preprint
_arXiv:2002.00874, 2020._

Gal Dalal, Bal´azs Sz¨or´enyi, Gugan Thoppe, and Shie Mannor. Finite sample analyses for td (0) with
function approximation. In Thirty-second AAAI conference on artificial intelligence, 2018a.

Gal Dalal, Gugan Thoppe, Bal´azs Sz¨or´enyi, and Shie Mannor. Finite sample analysis of twotimescale stochastic approximation with applications to reinforcement learning. In S´ebastien
Bubeck, Vianney Perchet, and Philippe Rigollet (eds.), Proceedings of the 31st Conference On
_Learning Theory, volume 75 of Proceedings of Machine Learning Research, pp. 1199–1233._
[PMLR, 06–09 Jul 2018b. URL https://proceedings.mlr.press/v75/dalal18a.](https://proceedings.mlr.press/v75/dalal18a.html)
[html.](https://proceedings.mlr.press/v75/dalal18a.html)

Thinh T Doan, Lam M Nguyen, Nhan H Pham, and Justin Romberg. Convergence rates of accelerated markov gradient descent with applications in reinforcement learning. arXiv preprint
_arXiv:2002.02873, 2020._

Jianqing Fan, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang. A theoretical analysis of deep qlearning. In Learning for Dynamics and Control, pp. 486–489. PMLR, 2020.


-----

David A Freedman. On tail probabilities for martingales. the Annals of Probability, pp. 100–118,
1975.

Mohammad Ghavamzadeh, Hilbert Kappen, Mohammad Azar, and R´emi Munos. Speedy qlearning. Advances in neural information processing systems, 24:2411–2419, 2011.

Sheldon Goldstein. Maximal coupling. Zeitschrift f¨ur Wahrscheinlichkeitstheorie und verwandte
_Gebiete, 46(2):193–204, 1979._

Geoffrey J Gordon. Reinforcement learning with function approximation converges to a region.
_Advances in neural information processing systems, 13, 2000._

Harsh Gupta, R. Srikant, and Lei Ying. Finite-time performance bounds and adaptive learning rate selection for two time-scale reinforcement learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett (eds.),
_Advances in Neural Information Processing Systems,_ volume 32. Curran Associates,
Inc., 2019. URL [https://proceedings.neurips.cc/paper/2019/file/](https://proceedings.neurips.cc/paper/2019/file/e354fd90b2d5c777bfec87a352a18976-Paper.pdf)
[e354fd90b2d5c777bfec87a352a18976-Paper.pdf.](https://proceedings.neurips.cc/paper/2019/file/e354fd90b2d5c777bfec87a352a18976-Paper.pdf)

Prateek Jain, Suhas S Kowshik, Dheeraj Nagaraj, and Praneeth Netrapalli. Near-optimal offline and streaming algorithms for learning non-linear dynamical systems. _arXiv preprint_
_arXiv:2105.11558, 2021a._

Prateek Jain, Suhas S Kowshik, Dheeraj Nagaraj, and Praneeth Netrapalli. Streaming linear system
identification with reverse experience replay. arXiv preprint arXiv:2103.05896, 2021b.

Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pp. 2137–2143.
PMLR, 2020.

Michael Kearns and Satinder Singh. Finite-sample convergence rates for q-learning and indirect
algorithms. Advances in neural information processing systems, pp. 996–1002, 1999.

Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Breaking the sample size barrier
in model-based reinforcement learning with a generative model. Advances in neural information
_processing systems, 33, 2020a._

Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, and Yuxin Chen. Sample complexity of asynchronous
q-learning: Sharper analysis and variance reduction. In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33,
[pp. 7031–7043. Curran Associates, Inc., 2020b. URL https://proceedings.neurips.](https://proceedings.neurips.cc/paper/2020/file/4eab60e55fe4c7dd567a0be28016bff3-Paper.pdf)
[cc/paper/2020/file/4eab60e55fe4c7dd567a0be28016bff3-Paper.pdf.](https://proceedings.neurips.cc/paper/2020/file/4eab60e55fe4c7dd567a0be28016bff3-Paper.pdf)

Gen Li, Changxiao Cai, Yuxin Chen, Yuantao Gu, Yuting Wei, and Yuejie Chi. Is q-learning minimax optimal? a tight sample complexity analysis. arXiv preprint arXiv:2102.06548, 2021.

Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching.
_Machine learning, 8(3-4):293–321, 1992._

Hamid Reza Maei, Csaba Szepesv´ari, Shalabh Bhatnagar, and Richard S. Sutton. Toward off-policy
learning control with function approximation. In Proceedings of the 27th International Con_ference on International Conference on Machine Learning, ICML’10, pp. 719–726. Omnipress,_
2010. ISBN 9781605589077.

Francisco S Melo, Sean P Meyn, and M Isabel Ribeiro. An analysis of reinforcement learning
with function approximation. In Proceedings of the 25th international conference on Machine
_learning, pp. 664–671, 2008._

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529–533, 2015.

R´emi Munos and Csaba Szepesv´ari. Finite-time bounds for fitted value iteration. Journal of Machine
_Learning Research, 9(5), 2008._


-----

Daniel Paulin. Concentration inequalities for markov chains by marton couplings and spectral methods. Electronic Journal of Probability, 20:1–32, 2015.

Theodore Perkins and Doina Precup. A convergent form of approximate policy iteration. Advances
_in neural information processing systems, 15, 2002._

Guannan Qu and Adam Wierman. Finite-time analysis of asynchronous stochastic approximation
and q-learning. In Conference on Learning Theory, pp. 3185–3205. PMLR, 2020.

Martin Riedmiller. Neural fitted q iteration–first experiences with a data efficient neural reinforcement learning method. In European conference on machine learning, pp. 317–328. Springer,
2005.

Egor Rotinov. Reverse experience replay. arXiv preprint arXiv:1910.08780, 2019.

Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation
andtd learning. In Conference on Learning Theory, pp. 2803–2830. PMLR, 2019.

Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3
(1):9–44, 1988.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.

Csaba Szepesv´ari and William D Smart. Interpolation-based q-learning. In Proceedings of the
_twenty-first international conference on Machine learning, pp. 100, 2004._

John N Tsitsiklis and Benjamin Van Roy. Feature-based methods for large scale dynamic programming. Machine Learning, 22(1):59–94, 1996.

John N Tsitsiklis and Benjamin Van Roy. An analysis of temporal-difference learning with function
approximation. IEEE transactions on automatic control, 42(5):674–690, 1997.

Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018.

Martin J Wainwright. Stochastic approximation with cone-contractive operators: Sharp ℓ -bounds
_∞_
for q-learning. arXiv preprint arXiv:1905.06265, 2019a.

Martin J Wainwright. Variance-reduced q-learning is minimax optimal. _arXiv preprint_
_arXiv:1906.04697, 2019b._

Martin J. Wainwright. High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge Series
in Statistical and Probabilistic Mathematics. Cambridge University Press, 2019c. doi: 10.1017/
9781108627771.

Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992.

Christopher John Cornish Hellaby Watkins. Learning from delayed rewards. 1989.

Bowen Weng, Huaqing Xiong, Lin Zhao, Yingbin Liang, and Wei Zhang. Momentum q-learning
with finite-sample convergence guarantee. arXiv preprint arXiv:2007.15418, 2020.

Lin Yang and Mengdi Wang. Sample-optimal parametric q-learning using linearly additive features.
In International Conference on Machine Learning, pp. 6995–7004. PMLR, 2019.

Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near optimal policies with low inherent bellman error. In International Conference on Machine Learning,
pp. 10978–10989. PMLR, 2020.

Shaofeng Zou, Tengyu Xu, and Yingbin Liang. Finite-sample analysis for sarsa with linear function approximation. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso[ciates, Inc., 2019a. URL https://proceedings.neurips.cc/paper/2019/file/](https://proceedings.neurips.cc/paper/2019/file/9f9e8cba3700df6a947a8cf91035ab84-Paper.pdf)
[9f9e8cba3700df6a947a8cf91035ab84-Paper.pdf.](https://proceedings.neurips.cc/paper/2019/file/9f9e8cba3700df6a947a8cf91035ab84-Paper.pdf)

Shaofeng Zou, Tengyu Xu, and Yingbin Liang. Finite-sample analysis for sarsa with linear function
approximation. Advances in neural information processing systems, 32, 2019b.


-----

A EXPERIMENTS

Even though OTL has a stabilizing effect on the Q-iteration, it reduces the rate of bias decay since
the values are not updated for a long time. Therefore, the success of our procedure depends on
picking the right values for the parameters N, B and Option I vs. Option II. However, under the
right conditions the algorithms which include OTL+RER converge to a much smaller final error
as illustrated by the examples we provide in this section. If a better sample complexity is desired,
then Q-RexDaRe can be used as shown by Theorem 3. Further research is needed to identify the
practical conditions such as (function approximation coarseness, MDP reward structure etc.) under
which techniques like OTL+RER help.

**100-State Random Walk** We first consider the episodic MDP (Sutton & Barto, 2018, example
6.2) – but with 100 states instead of 1000. Here the agent can either move left or right on a straight
line, receiving a reward of 0 at each step. Reaching the right terminal point ends the episode with
reward 1, while the left terminal point ends the episode with reward 0. In each episode, the initial
point is uniformly random and the offline policy chooses right and left directions uniformly at random. We use state aggregation to obtain a linear function representation (Sutton & Barto, 2018)
with total 10 aggregate states. Along with 2 actions, this leads to a 20 dimensional embedding.
We compare vanilla Q-learning, OTL+ER+Q-learning and EpiQ-Rex (Option II, N = 1). The
same step size (0.01) was chosen for all the algorithms. OTL+ER+Q has the same structure as
EpiQ-Rex and was run with the same parameters as EpiQ-Rex . The main difference between the
two algorithms is that OTL+ER+Q processes the experiences collected in each episode in a random
order processing instead of reverse order. Refering to Figure 4, we note that the bias decay for EpiQRex and OTL+ER+Q are slower than vanilla Q-learning due to the online target structure. However
EpiQ-Rex converges to a better solution than the other algorithms.

**Mountain Car** We run an online control type experiment with the Mountain car problem (Sutton
& Barto, 2018, Example 10.1). The task here is to control the car and help it reach the correct peak
(which ends the episode) as soon as possible (i.e, terminate the episode with fewest steps). The agent
receives a reward of −1 unless the correct peak is reached. Here we run EpiQ-Rex with Option I
and N = 1, OTL+ER+Q-learning and vanilla Q-learning. The k-th episode is generated with the
policy at the end of k − 1-th episode for each of the three algorithms. We use a n = 4 tile coding
to represent the Q values for a given action, each of which has 4 × 4 squares. The step-size was
picked as 0.1/n and the result was averaged over 500 runs of the experiment. We refer to Figure 2
for the outcomes. For the last 300 episodes, the mean episode length for Vanilla Q-learning was
145, EpiQ-Rex was 136 and OTL+ER+Q-learning was 143 (rounded to the nearest integer).

**Grid-World** We consider the grid-world problem which is a tabular MDP (Sutton & Barto, 2018,
Example 3.5), which is a continuing task. Here, an agent can walk north, south, east or west in a
5 × 5 grid. Trying to fall off the grid accrues a reward of −1, while reaching certain special states
accrues a reward of 10 or 5. In our example, we also add a unif[−0.5, 0.5] noise to all rewards to
make the problem harder. We run the grid world experiment with the discount factor γ = 0.9 and
step size 0.05 for Vanilla Q-learning, Q-Rex (Option II, B = 3000, N = 1) and OTL+ER+Qlearning which is run with the same parameters as Q-Rex but with random samples from the buffer.
We run the experiment 30 times and plot the error in the Q-values vs. time in Figure 3.

**Baird’s Counter-Example** Consider the famous Baird’s counter-example shown in Figure 6. The
features corresponding to each state is shown and the reward for any transition is 0. Thus, w[∗] = 0
is the optimal solution. Since Assumption 4 made in this work is violated for this example, we
consider the analog of the problem where at each step, a state is sampled uniformly at random and
the corresponding transition, along with the 0 reward is used to learn the vector w. In the experiment,
we set the problem and algorithmic parameters to be γ = 0.99 and η = 0.01/√5 (the factor of _√5_

normalizes the features to satisfy Assumption 1). We set B = 50, u = 0, and N = 5 since there is
no need for keeping a gap between the buffers in this experiment. Figure 7 shows the non-convergent
behavior of vanilla Q-learning while OTL-based Q-learning converges. Note that since the sampling
of state, action and reward is done in a uniformly random fashion, it is not relevant to use reverse
experience replay. It is easy to see that with data reuse, we only need few samples to ensure all states
are covered; the rate of convergence would be same as that of OTL+Q-learning.


-----

Figure 2: Mountain Car problem Figure 3: Grid-world problem

Figure 4: 100 State Random Walk Figure 5: Linear System Problem


Figure 6: Feature embedding in the modified
Baird’s counter-example; ⃗ei represents the ith canonical basis vector in R[7]. Each transition shown occurs with probability 1.


Figure 7: Performance of vanilla Q-learning
as compared to OTL+Q-learning, averaged
over 10 independent runs


**Linear Dynamical System with Linear Rewards** We also compare the algorithms on a linear
dynamical systems problem. While the problem described next is strictly not a linear MDP, the
value functions for certain policies can be written as a linearly in terms of the initial condition; this
state evolves asis made precise next. Consider a linear dynamical system with initial state being Xt+1 = AXt + ηt, where A ∈ R[d][×][d]. The reward obtained for such transition X0 ∈ R[d]. The
is given by ⟨Xt, θ⟩ for a fixed θ ∈ R[d]. The maximum singular value of A is chosen less than 1
to ensure that the system is stable. The infinite horizon γ-discounted expected reward is given by
_⟨X0,_ _i=0[(][γA][T][ )][i][θ][⟩][. Thus, the expected reward can be written as][ ⟨][X][0][, w][∗][⟩][, where]_

[P][∞] _w[∗]_ = (I − _γA[T]_ )[−][1]θ.

Since there are no actions in this case, the Q learning algorithms reduce to value function approximation (i.e, TD(0) type algorithms). We take the embedding φ(Xt) = Xt, the identity mapping.
We considered d = 5, γ = 0.99, η = 0.01, and a randomly generated normal matrix A and θ. We
Option II for Q-Rex along with B = 75, u = 25, and N = 5 for the experiments. For OTL+ER+Qlearning, we keep the same parameters, but with random order sampling, while ER+Q-learning does
not include OTL. The results shown in Figure 5 are averaged over 100 independent runs of the
experiment. Note that the errors considered are using iterate averaging.


-----

As seen from Figure 5, Q-Rex outperforms vanilla Q-learning and OTL+ER-based Q-learning as
one would expect based on the theory presented in this work. However, it is interesting to note that
ER+Q-learning performs slightly better than Q-Rex. One possible reason could be due to the fact
that in Q-Rex, the target gets updated at a slower rate at the cost of reducing bias. However, setting
a smaller value of NB might resolve the issue.

B DEFINITIONS AND NOTATIONS

B.1 Q-REXDARE

The psuedocode for Q-RexDaRe is given in Algorithm 2. Note that we reuse the sample data in
every outer-loop iteration instead of drawing fresh samples.

**Algorithm 2 Q-RexDaRe**

1: Input: learning rates η, horizon T, discount factor γ, trajectory Xt = {st, at, rt}, number of
outer-loops K buffer size B, buffer gap u

2: Total buffer size: S ← _B + u_
3: Number of buffers loops: N ← _T/S_
4: w1[1][,][1] = 0 initialization

5: for k = 1, . . ., K do
6: **for j = 1, . . ., N do**

7: Form buffer Buf = {X1[k,j][, . . ., X]S[k,j][}][, where,]

_Xi[k,j]_ _←_ _XS(j−1)+i_


8: Define for all i ∈ [1, S], φ[k,j]i ≜ _φ(s[k,j]i_ _, a[k,j]i_ ).

9: **for i = 1, . . ., B do**

10: _wi[k,j]+1_ [=][ w]i[k,j]+η _rB[k,j]+1_ _i_ [+][ γ][ max][a][′][∈A][⟨][φ][(][s]B[k,j]+2 _i[, a][′][)][, w]1[k,][1]_ _B+1_ _i[, w]i[k,j]_ _φ[k,j]B+1_ _i_
_−_ _−_ _[⟩−⟨][φ][k,j]_ _−_ _⟩_ _−_

11: _w1[k,j][+1]_ = wB[k,j]+1 h i

12: _w1[k][+1][,][1]_ = w1[k,N] [+1]

13: Return w0[T][ +1]

B.2 EPIQ-REX

The psuedocode for EpiQ-Rex is given in Algorithm 3. Note that the buffers here are individual
episodes and can vary in size due to inherent randomness. We do not require a gap here since
separate episodes are assumed to be independent.

**MDP definition** Here we construct the MDP with a (possibly) random reward r. We consider
non-episodic, i.e. infinite horizon, time homogenous Markov Decision Processes (MDPs) and we
denote an MDP by MDP(S, A, γ, P, r) where S denotes the state space, A denotes the action space,
_γ represents the discount factor, P_ (s[′]|s, a) represents the probability of transition to state s[′] from
the state s on action a. We assume for purely technical reasons that S and A are compact subsets
of R[n] for some n ∈ N). r is a reward process (not to be confused with Markov Reward Processes
i.e, MRP) indexed by S × A × S, such that r(s, a, s[′]) ∈ [0, 1] almost surely. We will skip the
measure theoretic details of the definitions and assume that the sequence of i.i.d. reward processes
(rt)t∈N can be jointly defined over a Polish probability space. The function R : S × A → [0, 1]
represents the deterministic reward R(s, a) obtained on taking action a at state s and is defined by
_R(s, a) = E_ Es′∼P (·|s,a)r(s, a, s[′]) . Here the expectation is with respect to both the randomness
in the reward process and in state s[′].
 

Now, given a trajectory, (st, at)[T]t=1[, which is independent of i.i.d sequence of rewards processes]
(rt)[T]t=1[. We observe][ (][s][t][, a][t][, r][t][(][s][t][, a][t][, s][t][+1][))][T]t=1[, and we will henceforth denote][ r][t][(][s][t][, a][t][, s][t][+1][)][ by]
just rt.


-----

**Algorithm 3 EpiQ-Rex**

1: Input: learning rates η, number of episodes T, discount factor γ, Epsiodes Et
_{(s[t]1[, a][t]1[, r]1[t]_ [)][, . . .,][ (][s][y]Bt _[, a]B[t]_ _t_ _[, r]B[t]_ _t_ [)][}][, Number of buffer per outer-loop iteration N.]

2: Number of outer loop iterations: K ← _T/N_
3: w1[1][,][1] = 0 initialization

4: for k = 1, . . ., K do
5: **for j = 1, . . ., N do**

6: _t ←_ (k − 1) ∗ _N + j_

7: Collect experience and form buffer Buf = {X1[t][, . . ., X]B[t] _t_ _[}][, where,]_


_Xi[t]_ [= (][s]i[t][, a][t]i[, r]i[t][)]


9:8: **forDefine for all i = 1, . . ., B i ∈ do[1, Bt −** 1], φ[k,j]i ≜ _φ(s[k,j]i_ _, a[k,j]i_ ).

10: _wi[k,j]+1_ [=][ w]i[k,j]+η _rB[k,j]t_ _i_ [+][ γ][ max][a][′][∈A][⟨][φ][(][s]B[k,j]t+1 _i[, a][′][)][, w]1[k,][1]_ _Bt_ _i[, w]i[k,j]_ _φ[k,j]Bt_ _i_
_−_ _−_ _[⟩−⟨][φ][k,j]−_ _⟩_ _−_

11: **Option I: w1[k,j][+1]** = wh _B[k,j]t_ i

12: **Option II:w1[k,j][+1]** = _Bt1−1_ _Bi=2t_ _[w]i[k,j]_

13: _w1[k][+1][,][1]_ = w1[k,N] [+1] P

14: Return w1[K,][1]


**Notation** Due to three loop nature of our algorithm it will be convenient to define some simplifying
notation. To this end, consider the outer loop with index k ∈ [K] and buffer number j ∈ [N ] inside
this outer loop. Further given an i [S] define a time index as t[k,j]i = NS(k 1) + S(j 1) + i.
_∈_ _−_ _−_
We now denote the i-th state-action-reward tuple inside this buffer by

(s[k,j]i _, a[k,j]i_ _, ri[k,j]) = (stk,ji_ _[, a][t]i[k,j]_ _[, r][t]i[k,j]_ [)][.]

Similarly, Ri[k,j] = R(s[k,j]i _, a[k,j]i_ ). For conciseness, we define for all i, j, k, φ[k,j]i := φ(s[k,j]i _, a[k,j]i_ ).
Since we are processing the data in the reverse order within the buffer, the following notation will
be useful for analysis:

_s[k,j]i_ [:=][ s]B[k,j]+1 _i[, a][k,j]i_ [:=][ a]B[k,j]+1 _i[, r][k,j]i_ [:=][ r]B[k,j]+1 _i[, R][k,j]i_ [:=][ R]B[k,j]+1 _i_ [and][ φ][k,j]i [:=][ φ]B[k,j]+1 _i[.]_
_−_ _−_ _−_ _−_ _−_ _−_ _−_ _−_ _−_ _−_

C COUPLED PROCESS

It can be seen that the buffers are approximately i.i.d. whenever we take u = O(τmix log _[T]δ_ [)][ when-]

ever Assumption 3 is satisfied. For the sake of clarity of analysis, we will consider exactly independent buffers. That is, we assume the algorithms are run with a fictitious trajectory (˜st, ˜at, ˜rt),
where we assume that the fictitious trajectory is generated such that the first state of every buffer is
sampled from the stationary distribution µ. We show that we can couple this fictitious process (i.e,
define it on a common probability space as the original process (st, at, rt)) such that

P _k=1_ _j=1_ _i=1_ _i_ _, ri[k,j], a[k,j]i_ ) = (˜s[k,j]i _, ˜ri[k,j], ˜a[k,j]i_ ) 1 _δ ._ (4)
_∩[K]_ _[∩][N]_ _[∩][B][+1][{][(][s][k,j]_ _}_ _≥_ _−_
 

Notice that the equality does not hold within the gaps between the buffer which are of size u but
inside the buffers of size B only. That is, the sequence of iterates obtained by running the algorithm
with the original data (st, at, rt) is the same as the sequence of iterates obtained by running the algorithm with the fictitious coupled data (˜st, ˜at, ˜rt) with high probability. We state this result formally
in Lemma 16 and prove it in Section M.1. Henceforth, we will assume that we run the algorithm
with data (˜st, ˜at, ˜rt) and refer to Lemma 16 to carry over the results to the original data set with
high probability. We analogously define _φ[˜][k,j]i_ . We will denote the iterates of the algorithm run with
the coupled trajectory as ˜wi[k,j] instead of wi[k,j] and will focus on it entirely. We now provide some
definitions based on the above process. These definitions will be used repeatedly in our analysis.


-----

D BASIC STRUCTURAL LEMMAS

We first note some basic structural lemmas regarding ZIBEL MDPs under the assumptions in Section 2.1. We refer to Section M for the proofs.

**Lemma 1. For tabular MDPs satisfying the assumptions in Section 2.1, for both Q-Rex and Q-**
_RexDaRe, we have that for every k, j, i we have that_ _w˜i[k,j]_ _φ_ 1 1 _γ_
_∥_ _∥_ _≤_ _−_

The lemma above says, in particular, that the Q-value estimate given by our algorithm never exceeds
1

1 _γ_ [due to][ 0][ initialization. The proof is a straightforward induction argument, which we omit. We]
_−_
will henceforth use Lemma 1 without explicitly mentioning it.

**Lemma 2. Suppose Qw(s, a) := ⟨w, φ(s, a)⟩** _and let Q[∗](s, a) = ⟨w[∗], φ(s, a)⟩_ _be the optimal Q_
_function. Then:_
sup _Qw(s, a)_ _Q[∗](s, a)_ = _w_ _w[∗]_ _φ_
(s,a)∈S×A _|_ _−_ _|_ _∥_ _−_ _∥_

_Moreover, we must have: ∥x∥≥∥x∥φ ≥_ _[∥]√[x]κ[∥] for any x ∈_ R[d].

**Lemma 3. For any w0 ∈** R[d], there exists a unique w1 ∈ R[d] _such that_

_⟨w1, φ(s, a)⟩_ = R(s, a) + γEs′∼P (·|s,a) supa[′]∈A⟨φ(s[′], a[′]), w0⟩.


_We will denote this mapping w0 →_ _w1 by w1 = T (w0)._

**Lemma 4.Moreover, we have: T : R[d] ∥→w[∗]R∥[d]φ ≤is γ1 contractive in the norm−1** _γ_ _[and][ ∥][w][∗][∥≤]_ 1√−κγ _[.]_ _∥· ∥φ. The unique fixed point of T is w[∗]._

In view of Lemma 4, we can begin to look at the following noiseless Q-iteration. Let ¯w[1] = w1[1][,][1] = 0
and ¯w[k][+1] = T ( ¯w[k]). This converges geometrically to w[∗] with contraction coefficient γ under the
norm _φ. In our case, however, we only have sample access to the operator_ . Therefore, our
_∥· ∥_ _T_
Q-iteration at the end of k-th outer loop can be written as ˜w1[k][+1][,][1] = T ( ˜w1[k,][1][) + ˜]ϵk where ˜ϵk is the
error introduced via sampling which needs to be controlled.

E BIAS VARIANCE DECOMPOSITION

We begin our analysis by providing a bias-variance decomposition of the error with respect to the
noiseless Q-iteration at every loop. We will need the following definitions which we use repeatedly
through our analysis.

Given step size η, outer loop index k and buffer index j, we define the following contraction matrices
for a, b ∈ [B]. _b_

_H˜a,b[k,j]_ [:=] _I −_ _ηφ[˜][k,j]i_ [φ[˜][k,j]i ][⊤][] _._ (5)

_i=a_

Y 

Whenever a > b, we will define _H[˜]a,b[k,j]_ [:=][ I][. In the tabular setting, we define for any][ (][s, a][)][ ∈S × A]
by _N[˜]_ _[k](s, a) to be the number of samples of (s, a) seen in the outer loop k (excluding the gaps),i.e._


_N˜_ _[k](s, a) =_


1((˜s[k,j]i _, ˜a[k,j]i_ ) = (s, a))
_i=1_

X


_j=1_


Further we denote by _N[˜]i[k,j](s, a), the number of samples of (s, a) seen in the outer loop k in the_
buffers post the buffer j as well as the number of samples of (s, a) in buffer j before iteration i.
Formally,


_i−1_

1 (˜s[k,j]r _[,][ ˜]a[k,j]r_ [) = (][s, a][)]
_r=1_

X  


_N˜i[k,j](s, a) :=_


1 (˜s[k,l]r _[,][ ˜]a[k,l]r_ [) = (][s, a][)]
_r=1_

X  


_l=j+1_


-----

We define the error term for any w:

_ϵ˜[k,j]i_ (w) := r˜i[k,j] _−_ _R[˜]i[k,j]_ + γ supa[′]∈A⟨w, φ(˜s[k,j]i+1[, a][′][)][⟩−] _[γ][E]s[′]∼P (·|s˜[k,j]i_ _,a˜[k,j]i_ ) _a[sup][′]∈A⟨φ(s[′], a[′]), w⟩_ _. (6)_

Finally we define the following shorthands for any i, j, k:


_w˜[k][+1][,][∗]_ := T ( ˜w1[k,][1][)] _ϵ˜[k,j]i_ := ˜ϵ[k,j]i ( ˜w1[k,][1][)] _L˜[k,j]_ := η


_ϵ˜[k,j]i_ _H˜1[k,j],i_ 1φ[˜][k,j]i
_−_
_i=1_

X


Given the above definition, the following the lemma provides the Bias-Variance decomposition
which is the core of our analysis.
**Lemma 5 (Bias-Variance Decomposition). For every k, we have that,**

1 _N_ _j+1_


_ϵ˜k := ˜w1[k][+1][,][1]_ _−_ _w˜[k][+1][,][∗]_ = _H˜1[k,j],B[( ˜]w1[k,][1]_ _−_ _w˜[k][+1][,][∗]) +_

_j=N_

Y


_H˜1[k,l],BL[˜][k,j]_ (7)
_l=N_

Y


_j=1_


Here and later on in the paper, we use the reverse order in the product to highlight the convention that
higher indices l in _H[˜]1[k,l],B_ [appear towards the left side of the product and further define][ Q]l[N]=[+1]N _H[˜]1[k,l],B_ [=]
_I. We call the first term in Equation (7) as the bias term and it decays geometrically with N and_
the second term is called the variance, which has zero mean. We will bound these terms separately.
Since tabular setting allows for improved analysis of error, we will provide special cases for the
tabular setting with refined bounds. We refer to Section M.5 for the proof of Lemma 5.

F BOUNDING THE BIAS TERM

F.1 TABULAR CASE

In the tabular case, we have the following expression for bias. We omit the proof since it follows
from a simple calculation.
**Lemma 6. In the tabular setting, we have:**


_H˜1[k,j],B[( ˜]w1[k,][1]_ _−_ _w˜[k][+1][,][∗])⟩_ = (1 − _η)N˜_ _[k](s,a)⟨w˜1[k,][1]_ _−_ _w˜[k][+1][,][∗], φ(s, a)⟩_
_j=N_

Y


_⟨φ(s, a),_


For a particular outer loop k, we show that _N[˜]_ _[k](s, a) is Ω(µminNB) with high probability whenever_
_NB is large enough. For this we use (Paulin, 2015, Theorem 3.4) similar to the proof of (Li et al.,_
2020b, Lemma 8). We give the proof in Section M.6.

**Lemma 7. There exists a constant C such that whenever B ≥** _τmix and NB ≥_ _C_ _µ[τ]min[mix]_ [log(][ |S||A|]δ ),

_with probability at least 1 −_ _δ, we have that for every (s, a) ∈S × A:_
_N˜_ _[k](s, a) ≥_ [1]2 _[µ][(][s, a][)][NB][ ≥]_ [1]2 _[µ][min][NB]_

F.2 ZIBEL MDP CASE


**Lemma 8. Suppose g ∈** R[d] _is fixed and ηB <_ [1]4 _[. Then, the following hold:]_

_1._


E∥ _H˜1[k,j],B[g][∥][2][ ≤]_ [exp(][−] _[ηNB]κ_ [)][∥][g][∥][2] (8)

_j=N_

Y


_2. With probability at-least 1 −_ _δ, we have:_


1

_κ_

_H˜1[k,j],B[g][∥][φ][ ≤]_ [exp(][−] _[ηNB]κ_ [)]

_δ_

_j=N_ r _[∥][g][∥][φ]_

Y


E∥


We refer to Section L.1 for the proof.


-----

G BOUNDING THE VARIANCE TERM

G.1 TABULAR CASE

In the tabular case, it is clear that:

_N_ _j+1_ _N_ _B_

_⟨φ(s, a),_ _H˜1[k,l],BL[˜][k,j]⟩_ = (1 − _η)N˜i[k,j]_ (s,a)ηϵ˜[k,j]i 1((˜s[k,j]i _, ˜a[k,j]i_ ) = (s, a)) (9)

_j=1_ _l=N_ _j=1_ _i=1_

X Y X X

Now, we note that _N[˜]i[k,j](s, a) depends on data in buffers l > j and when i1 < i inside buffer j._
Following the discussion in (Li et al., 2021), we define the vector VarP (Vk), VarP (V _[∗]) ∈_ R[S×A]
such that

2

VarP (Vk)(s, a) = Es′∼P (·|s,a) asup[′]∈A⟨φ(s[′], a[′]), ˜w1[k,][1][⟩−] [E][s][′][∼][P][ (][·|][s,a][)] _a[sup][′]∈A⟨φ(s[′], a[′]), ˜w1[k,][1][⟩]_ (10)

2

VarP (V _[∗])(s, a) = Es′∼P (·|s,a)_ asup[′]∈A⟨φ(s[′], a[′]), w[∗]⟩− Es′∼P (·|s,a) supa[′]∈A⟨φ(s[′], a[′]), w[∗]⟩ (11)

More generally, we define


2
_asup[′]∈A⟨φ(s[′], a[′]), w⟩−_ Es′∼P (·|s,a) supa[′]∈A⟨φ(s[′], a[′]), w⟩


VarP (V )(s, a; w) = Es′∼P (·|s,a)


Similarly, we define _L[˜][k,j](w) by replacing w1[k,][1]_ with any fixed, arbitrary w. It is easy to see that:

E _ϵ˜[k,j]i_ (w) _s[k,j]i_ _, ˜a[k,j]i_ ) = (s, a) 2(1 + γ[2]VarP (V )(s, a; w)) (12)
_|_ _|[2]_ _≤_
 

**Lemma 9. In the tabular setting, suppose[(˜]** _w is a fixed vector such that ⟨w, φ(s, a)⟩∈_ [0, 1−1 _γ_ []][ for]

_every (s, a) ∈S × A. Fix (s, a) ∈S × A. Then there exists a universal constant C such that with_
_probability atleast 1 −_ _δ, we have that:_

_Then,_

_N_ _j+1_

_H˜1[k,l],BL[˜][k,j](w)_ _η log(2/δ)(1 + γ[2]VarP (V )(s, a; w)) + C [η][ log(2][/δ][)]_

_j=1_ _l=N_ _⟩_ _[≤]_ _[C]_ 1 − _γ_

X Y p

_[⟨][φ][(][s, a][)][,]_

G.2 ZIBEL MDP CASE

We will use an appropriate exponential super-martingale to bound the error term in the ZIBEL MDP
case just like in the proof of (Jain et al., 2021b, Lemma 27). The following thereom summarizes the
result and we refer to Section L.3 for its proof.
**Theorem 4. Suppose x, w ∈** R[d] _are fixed. Then, there exists a universal constant C such that with_
_probability at least 1 −_ _δ, we have:_

_N_ _j+1_

_j=1_ _l=N_ _H˜1[k,l],BL[˜][k,j](w)⟩_ _[≤]_ _[C][∥][x][∥]_ [(1 +][ ∥][w][∥][φ][)] _η log(2/δ) ._ (13)

X Y p

_[⟨][x,]_

By a direct application of (Vershynin, 2018, Theorem 8.1.6), we derive the following corollary. We
remind the reader that CΦ is the covering number defined in Definition 3.
**Corollary 1. Suppose x, w ∈** R[d] _are fixed. Then, there exists a universal constant C such that with_
_probability at least 1 −_ _δ, we have:_

_N_ _j+1_
Xj=1 _lY=N_ _H˜1[k,l],BL[˜][k,j](w)_ _φ_ _≤_ _C(1 + ∥w∥φ)[√]η_ CΦ + qlog( [2]δ [)] _._


-----

In order to apply the theorem above, we will need to control _w1[k,][1]_
following lemma presents such a bound and we refer to Section ∥ L.4 for the proof.[∥][φ][ uniformly for all][ k][. The]

**Lemma 10. Suppose ˜w1[k,][1]** _are the iterates of Q-Rex with coupled data from a ZIBEL MDP._

_There exist universal constants C, C1, C2 such that whenever NB > C1_ _[κ]η_ [log] _δ(1Kκγ)_ _, η <_

_−_

_C2_ _Cφ[2]_ [+log(](1−γ)[K/δ][2] [)][ and][ ηB <][ 1]4 _[, with probability at-least][ 1][ −]_ _[δ][, the following hold:]_  


_1. For every 1_ _k_ _K,_ _w˜1[k,][1]_
_≤_ _≤_ _∥_ _[∥][φ][ ≤]_

_2. For every 1_ _k_ _K,_ _ϵ˜k_ _φ_
_≤_ _≤_ _∥_ _∥_ _≤_
q

H PROOF OF THEOREM 1

_Proof. Consider the Q learning iteration:_


1−γ

_δ(125Kκγ)[2][ exp(][−]_ _[ηNB]κ_ [) +][ C](1 _[√]γ[η])_ _CΦ +_
_−_ _−_

h


log( [2]δ[K] [)]


_w˜1[k][+1][,][1]_ = T ( ˜w1[k,][1][) + ˜]ϵk .

Using Lemma 4, we conclude: ˜w1[k][+1][,][1] _−_ _w[∗]_ = T ( ˜w1[k,][1][)][ −T][ (][w][∗][) +][ ϵ][k][ and thence:]

_w˜1[k][+1][,][1]_ _w[∗]_ _φ_ _γ_ _w˜1[k,][1]_ _w[∗]_ _φ + sup_ _ϵl_ _φ ._
_∥_ _−_ _∥_ _≤_ _∥_ _−_ _∥_ _l_ _K_ _∥_ _∥_
_≤_

Unrolling the recursion above, we conclude:


_∥w˜1[K][+1][,][1]_ _−_ _w[∗]∥φ ≤_ _γ[K]∥w[∗]∥φ + [sup][l]1[≤][K][ ∥]γ[ϵ][l][∥][φ]_

_−_

Now, we invoke item 2 of Lemma 10 along with the constraints on N, B, K, η and Lemma 2 to
conclude the result.

I PROOF OF THEOREM 2

In this section we analyze the output of Q-Rex in the tabular setting and obtain convergence guarantees. To connect with the standard theory for tabular MDP Q-learning in (Li et al., 2021), let
us use the standard Q-function notation where we assume for all k, _Q[˜][k,]1_ [1] R[S×A] and we have
_∈_
that _Q[˜][k,]1_ [1][(][s, a][) =][ ⟨]w[˜]1[k,][1][, φ][(][s, a][)][⟩][.] Since φ(s, a) are the standard basis vectors, we must have
_Q˜[k,]1_ [1] = ˜w1[k,][1][. In the tabular setting, we see by using Lemmas][ 5][,][ 6][,][ 7][, and][ 9][ that for any][ δ >][ 0][,]
there exists a universal constant C, whenever NB ≥ _C_ _µ[τ]min[mix]_ [log(][ |S||A|]δ _[K]_ ), with probability at-least

1 − _δ, for every k ∈_ [K] and every (s, a) ∈S × A:

_Q˜[k]1[+1][,][1]_ = _Q˜[k,]1_ [1] + ˜ϵk, (14)
_T_
h i

where ˜ϵk ∈ R[S×A] is such that for all (s, a),

_|ϵ˜k(s, a)| ≤_ _C_ _η log_ _K|S||A|δ_ (1 + γ[2]VarP (V )(s, a; Q[˜][k,]1 [1][)) +][ C η][ log(]1 _[ K][|S||A|]γδ_ )

+ (1r − _η)µmin2NB_ Q[˜][k,]1 [1] _−T_ _Q˜[k,]1_ [1] _._ _−_ (15)

h i _∞_

Now that we have set-up the notation, we will roughly follow the analysis methods used in (Li et al.,
2021). Since we start our algorithm with _Q[˜]1[1][,][1]_ = 0, and rt [0, 1] almost surely, we can easily show
_∈_

that _Q[˜][k,]1_ [1][(][s, a][)][ ∈] [[0][,] 1−1 _γ_ []][ for every][ k, s, a][. Therefore, we upper bound] _Q[˜][k,]1_ [1][−T] _Q˜[k,]1_ [1] _≤_ 1−1 _γ_

h i _∞_


-----

in Equation (15) to conclude that with probability at-least 1 − _δ, for every k ∈_ [K] and every
(s, a) ∈S × A:


_η log(_ _[K][|S||A|]δ_ )γ[2] [VarP (Vk)(s, a)] + αη (16)


_ϵ˜k(s, a)_ _C_
_|_ _| ≤_


_K_
Where αη := C _η log_ 1−|S||A|γδ + C _η log_ _K|S||A|δ_ + exp _−_ _[ηµ]1−[min]γ2_ _[NB]_
   r   

 

Now we define ∆k = _Q˜[k,]1_ [1] _Q[∗]_ and πk : to be the deterministic policy given
_−_ _S →A_
by Q[k,]1 [1] i.e, πk(s) := arg supa _Q[˜][k,]1_ [1][(][s, a][)][ and][ π][∗] [to be optimal policy given by][ π][∗][(][s][) :=]
_∈A_
arg supa _Q[∗](s, a). We use the convention that we pick a single maximizing action using some_
_∈A_
rule whenever there are multiple. Similarly, we let P _[π][k]_ _, to be the Markov transition kernel over_
given by P _[π][k]_ ((s, a), (s[′], a[′])) = P (s[′] _s, a)1(a[′]_ = πk(s[′])). Similarly, we define P _[π][∗]_ with
_S × A_ _|_
respect to the policy π[∗]. It is easy to show that:

_T ( Q[˜][k,]1_ [1][) =][ R][ +][ γP][ π][k][ ˜]Q[k,]1 [1] _._

Similarly,

_Q[∗]_ = T (Q[∗]) = R + γP _[π][∗]_ _Q[∗]_ _._

Furthermore given any Q ∈ R[S×A], letting πQ being the greedy policy with respect to the function Q
we have that for any policy π, it can be easily seen from the definitions that following element-wise
inequality follows:

_P_ _[π]Q ≤_ _P_ _[π][Q]_ _Q._

We now use Equation (14) along with the equations above to conclude:

_γP_ _[π][∗]_ ∆k + ˜ϵk ≤ ∆k+1 ≤ _γP_ _[π][k]_ ∆k + ˜ϵk . (17)

Here, the inequality is assumed to be point-wise. By properties of Markov transition kernels, we can
write:


_K_

_k=1_ _γ[K][−][k][ ]P_ _[π][∗]_ [][K][−][k] _ϵ˜k + γ[K][ ]P_ _[π][∗]_ [][K] ∆1 ≤ ∆K+1

X

_K_ _k+1_

_≤_ _γ[K][−][k]_ _P_ _[π][l]_

_k=1_ _l=K_

X Y


_ϵ˜k + γ[K]_


_P_ _[π][l]_

_l=K_

Y


∆1 . (18)


Here, we use the convention that _l=K_ _[P][ π][l][ =][ I][. We bound the lower bound and the upper bound]_
given in Equation (18) separately in order to bound ∥∆K+1∥∞.

We first consider the lower bound. Using ([Q][K][+1] Azar et al., 2013, Lemma 7), we have:


(I − _γP_ _[π][∗]_ )[−][1][p]VarP (V _[∗])_ _∞_ _[≤]_


2

(19)
(1 _γ)[3][ .]_
_−_


We also note from (Li et al., 2021, Equation 64) and basic calculations that:


VarP (V _[∗])∥∞_ _≤_


_∥VarP (Vk) −_ VarP (V _[∗])∥∞_

4

(20)
1 − _γ_ _[∥][∆][k][∥][∞]_ _[.]_


VarP (Vk) −


-----

Using Equations (15) (16), we conclude that there exist universal constants C, C1 such that with
probability at least 1 − _δ (interpreting the inequalities as element-wise):_

_[K]_

_K_

∆K+1 ≥− _[α][η]1[ +] −[ γ]γ[K]_ _−_ _Cqηγ[2]_ log _|S||A|δ_ _k=1_ _γ[K][−][k][ ]P_ _[π][∗]_ [][K][−][k][ p]VarP (Vk)

   X

_[K]_

_K_

_≥−_ _[α][η]1[ +][ γ]γ[K]_ _−_ _C_ _ηγ[2]_ log _|S||A|δ_ _γ[K][−][k][ ]P_ _[π][∗]_ [][K][−][k][ p]VarP (V _[∗])_

_−_ q _k=1_

   X

1 _K_ _[K]_

_−_ _C1_ 1 _γ_ _ηγ[2]_ log _|S||A|δ_ _γ[K][−][k][p]∥∆k∥∞_

r _−_ q _k=1_

   X

_K_

_≥−_ _[α][η]1[ +][ γ]γ[K]_ _−_ _C_ _ηγ[2]_ log _|S||A|δ_ (I − _γP_ _[π][∗])[−][1][p]VarP (V_ _[∗])_

_−_ q

  

1 _K_ _[K]_

_−_ _C1_ 1 _γ_ _ηγ[2]_ log _|S||A|δ_ _γ[K][−][k][p]∥∆k∥∞_

r _−_ q _k=1_

   X

_ηγ[2]_ _K_

_≥−_ _[α][η]1[ +][ γ]γ[K]_ _−_ _Cs_ (1 _γ)[3][ log]_ _|S||A|δ_

_−_ _−_

  

1 _K_ _[K]_

_−_ _C1_ 1 _γ_ _ηγ[2]_ log _|S||A|δ_ _γ[K][−][k][p]∥∆k∥∞_ _._ (21)

r _−_ q _k=1_

   X

In the above chain, the first inequality follows from Equations (16) (18), the second inequality from
Equation (20) and the fourth inequality from Equation (19). For the upper bound consider the
following set of equations interpreting them element-wise which hold for a universal constant C and
with probability at least 1 − _δ._

_K_ _k+1_ 1

∆K+1 ≤ _k=1_ _γ[K][−][k]_ _l=K_ _P_ _[π][l]_ ! _ϵ˜k + γ[K]_ _l=K_ _P_ _[π][l]_ ! ∆1

X Y Y

_[K]_ _k+1_

_K_

_≤_ _[α][η]1[ +] −[ γ]γ[K]_ + Cqηγ[2] log _|S||A|δ_ _k=1_ _γ[K][−][k]_ _l=K_ _P_ _[π][l]_ ! VarP (Vk)

   X Y p

_[K]_ _k+1_

_K_

+ C _ηγ[2]_ log _|S||A|δ_ _γ[K][−][k]_ _P_ _[π][l]_ VarP (Vk)

_≤_ _[α][η]1[ +] −[ γ]γ[K]_ q _k=1_ vu _l=K_ !

   X u Y

t

_k+1_

_[K]_ _K_ _k_ _K_ _k_

= _[α][η][ +][ γ][K]_ + C _ηγ[2]_ log _K|S||A|δ_ _γ_ 2− _γ_ 2− _P_ _[π][l]_ VarP (Vk)

1 _γ_ v
_−_ q _k=1_ u _l=K_ !

   X u Y

t

_K_ _K_ _k+1_

_K_

+ C _ηγ[2]_ log _|S||A|δ_ _γ[K][−][k]_ _γ[K][−][k]_ _P_ _[π][l]_ VarP (Vk)

_≤_ _[α][η]1[ +] −[ γ]γ[K]_ q vuk=1 vuk=1 _l=K_ !

  uX uX Y

t t

_K_ _k+1_

_ηγ[2]_ _K_

+ C _|S||A|δ_ _γ[K][−][k]_ _P_ _[π][l]_ VarP (Vk)

_≤_ _[α][η]1[ +] −[ γ]γ[K]_ s 1 − _γ_ [log] vuk=1 _l=K_ !

  uX Y

t _K_ _k+1_

_ηγ[2]_ _K_ _γ[K/][2]_

_≤_ _[α][η]1[ +] −[ γ]γ[K]_ + Cs 1 − _γ_ [log] _|S||A|δ_ vuk=K/2+1 _γ[K][−][k]_ _l=K_ _P_ _[π][l]_ ! VarP (Vk) + (1 − _γ)[3][ .]_

  u X Y

t (22)

In the above chain, the first inequality follows from Equation (18), the second inequality follows
from Equation (16), the third inequality follows from Jensen’s inequality and noting that P _[π]_ is a
Markov operator, the fourth inequality via Cauchy-Schwartz and the last inequality by noting that
that VarP (Vk) 1/(1 _γ)[2]._
_∥_ _∥∞_ _≤_ _−_


-----

It can now be verified that (Li et al., 2021, Lemma 5) applies in our setting to conclude that:


_K_ _k+1_

_γ[K][−][k]_ _P_ _[π][l]_

_k=K/2+1_ _l=K_

X Y


VarP (Vk) ≤


1 + 2 max
_K/2+1≤k≤K_ _[∥][∆][k][∥][∞]_


_γ[2](1 −_ _γ)[2]_


Using the equation above, along with Equations (22) and (21), we conclude there exists a universal
constant C such that with probability at least 1 − _δ, we have:_


_η_ _K_

_|S||A|_
(1 _γ)[3][ log]_ _δ_
_−_
 


_∥∆K+1∥∞_ _≤_ _[α][η]1[ +][ γ]γ[K]_ +C

_−_


_γ[K/][2]_
1 + max
_K/2+1_ _k_ _K_ (1 _γ)_ [(23)]
_≤_ _≤_ _[∥][∆][k][∥][∞]_ [+] _−_


We note that this works with every K replaced with l for any l ≤ _K, importantly under the_
same event (with probability at least 1 _δ) described above for which Equation (23) holds. De-_
_K_ _−_ 1
fine L = 2[s][ for some][ s][ =][ ⌈][log][2][(1 +][ C][ log(] 1 _γ_ [))][⌉][. Under the conditions of the Theorem, i.e,]

2 _−_

_K > C2_ (1 1 _γ)_ log( 1 1 _γ_ [)], we have _[γ]1[L/]γ[2]_ _[<][ 1][. Therefore we conclude from the discussion above]_

_−_ _−_ _−_

that for every K _l_ _L, we must have:_
_≥_ _≥_

_η_ _K_

_∥∆l+1∥∞_ _≤_ _[α]1[η][ +][ γ]γ_ _[L]_ + C (1 _γ)[3][ log]_ _|S||A|δ_ 1 + _l/2+1maxk_ _l_

_−_ r _−_ _≤_ _≤_ _[∥][∆][k][∥][∞]_

  [r]

_K_ _K_

+ C _η log_ _|S||A|δ_ + C _η log_ _|S||A|δ_ max (24)

_≤_ _[α]1[η][ +][ γ]γ_ _[L]_ s (1 _γ)[3]_ s (1 _γ)[3]_ _l/2+1_ _k_ _l_

_−_   −    −  r _≤_ _≤_ _[∥][∆][k][∥][∞]_

To analyze this recursion, we have the following lemma which establishes hyper-contractivity,
whose proof we defer to Section M.8.
**Lemma 11. Suppose α, β ≥** 0. Consider the function f :2 R[+] _→_ R[+] _given by f_ (u) = α + β[√]u.

_Then, f has the unique fixed point: u[∗]_ := _β+[√]β2_ [2]+4α _. For t_ N, denoting f [(][t][)] _to be the t fold_

_∈_

 

_composition of f with itself, we have for any u ∈_ R[+]:


1

2[t] _._


_|f_ [(][t][)](u) − _u[∗]| ≤_ _β[(2][−]_


2[t][−][1][ )]|u − _u[∗]|_


Now consider for 0 ≤ _a < s,_
_ua :=_ sup ∆l+1
_K_ _∥_ _∥∞_

2[s][−][a][ ≤][l][≤][K]

_K_ _K_
_η log_ _|S||A|_ _η log_ _|S||A|_

In lemma 11, define f with α = _[α][η]1−[+]γ[γ][L]_ + Cr (1−γ)δ[3] and β = Cr (1−γ)δ[3] . The

fixed point u[∗] is such that:      

_K_ _K_

_u[∗]_ _C_ _|S||A|δ_ + _[α][η][ +][ γ][L]_ + _η log_ _|S||A|δ_ (25)
_≤_  (1 _γ)[3]_ 1 _γ_ s (1 _γ)[3]_ 

  −  _−_   − 
 _[η][ log]_ 

Clearly, by Equation (f (·) and the fact that u240 ≤), we have:1−1 _γ_ [(Lemma] ua ≤[ 1][):]f (ua−1) and ∥∆K+1∥≤ _f_ (us−1). By monotonicity of

_us−1 ≤_ _f_ [(][s][−][1)](u0) ≤ _f_ [(][s][−][1)][ ] 1−1 _γ_ _._



Therefore,
_∥∆K+1∥∞_ _≤_ _f_ (us−1) ≤ _f_ [(][s][)][ ] 1−1 _γ_




-----

Applying Lemma 11, we conclude:

_∥∆K+1∥∞_ _≤_ _u[∗]_ + β[(][2][−] 2[s]1[−][1][ )] 1−1 _γ_ _[−]_ _[u][∗]_ 21[s] (26)

Note that under the constraints on the parameter η and K as stated in the Theorem, we must have

1

_u[∗]_ (1 1γ)[3][ . By our choice of][ s][, we must have:][ |] (1 1 _γ)_ 2[s] _C_ _[′]. Using this in Equation 26_
_≤_ _−_ 1 _−_ _[−]_ _[u][∗][|]_ _≤_

and the fact that β[(][2][−] 2[s][−][1][ )] _≤_ _β + β[2], we conclude that with probability at-least 1 −_ _δ, we must_

have:


_K_ _K_

∆K+1 _C_ _|S||A|δ_ + _[α][η][ +][ γ][L]_ + _η log_ _|S||A|δ_
_∥_ _∥≤_  (1 _γ)[3]_ 1 _γ_ s (1 _γ)[3]_ 

  −  _−_   − 
 _[η][ log]_ 

This proves the first part of the theorem. For the second part, we directly substitute the values
provided to verify that we indeed obtain ϵ error.

J PROOF OF THEOREM 3

We will now show uniform convergence type result under Assumption 5. For (s, a) ∈S × A and
_s[′]_ _∈_ supp(P (·|s, a)). We define the random variables for all k:


_Pˆk(s[′]|s, a) = η_ (1 − _η)N˜i[k,j]_ (s,a)1(˜s[k,j]i+1 [=][ s][′][,][ ˜]s[k,j]i = s, ˜a[k,j]i = a)

_j=1_ _i=1_

X X

_N_ _B_

_P¯k(s[′]_ _s, a) := η_ (1 _η)N˜i[k,j]_ (s,a)P (s′ _s, a)1(˜s[k,j]i_ = s, ˜a[k,j]i = a).
_|_ _−_ _|_

_j=1_ _i=1_

X X


**Lemma 12. Suppose Assumption 5 holds. Then, with probability at-least 1 −** _δ, we must have for_
_any fixed (s, a):_


_P[ˆ]k(s[′]_ _s, a)_ _Pk(s[′]_ _s, a)_ _C_
_|_ _|_ _−_ [¯] _|_ _| ≤_
_s[′]∈suppX(P (·|s,a))_


_ηd[¯] log(4/δ) + Cηd[¯] log(4/δ)_


We refer to Section L.5 for the proof. We now proceed with the proof of Theorem 3. Recall the
noiseless iteration ¯w[k] defined in the discussion following the statement of Lemma 4. We define
_D¯_ _k := w0[k,][0]_ _w¯[k]. Observe that we cannot apply Lemma 9 as in the proof of Theorem 2 where_
_−_
we used w = w0[k,][0] in order to bound ∥ϵk∥∞. This is because of data re-use which causes w0[k,][0] to
depend on the ‘variance’ term.

However, note that ¯w[k] is a deterministic sequence and we can apply Lemma 9 and then use the fact
that ¯w[k] _w0[k,][0]_ to show a similar concentration inequality. To this end, we prove the following
_≈_
lemma:
**Lemma 13. In the tabular setting, we have almost surely:**


_j+1_

_H˜1[k,l],BL[˜][k,j](w) −_
_l=N_

Y


_j+1_

_H˜1[k,l],BL[˜][k,j](v)_
_l=N_

Y


_φ(s, a),_


_j=1_

_γ_ _w_ _v_ _φ_
_≤_ _∥_ _−_ _∥_


_j=1_


_P[ˆ]k(s[′]_ _s, a)_ _Pk(s[′]_ _s, a)_ (27)
_|_ _|_ _−_ [¯] _|_ _|_
_s[′]∈suppX(P (·|s,a))_


The proof follows from elementary arguments via. the mean value theorem and triangle inequality.
We refer to Section M.7 for the proof. We are now ready to give the proof of Theorem 3.

_Proof of Theorem 3. We proceed with a similar setup as the proof of Theorem 2. Notice that due to_
data reuse, the noise ϵk in outer loop k is given by ϵ1( ˜w1[k,][1][)]

_Q[k]1[+1][,][1]_ = T _Q[k,]1_ [1] + ϵ1( Q[˜][k,]1 [1][)][ .]
h i


-----

We also define the noiseless Q iteration such that _Q[¯][1]1[,][1]_ = Q[˜]1[1][,][1] and _Q[¯][k]1[+1][,][1]_ = T ( Q[¯][k,]1 [1][)][. Let][ ¯]∆k :=
_Q˜[k,]1_ [1] _−_ _Q[¯][k,]1_ [1][.]

Now, by Lemma 5, we can write down

_N_ _j+1_

_ϵ1( Q[˜][k,]1_ [1][)(][s, a][) = (1][ −] _[η][)]N˜_ [1](s,a)( ˜Q[k,]1 [1] _−_ _Q[˜][k,][∗]) + ⟨φ(s, a),_ _H˜1[1],B[,l]_ _L[˜][1][,j]( Q[˜][k,]1_ [1][)][⟩]

_j=1_ _l=N_

X Y

_N_ _j+1_


= (1 _η)N˜_ [1](s,a)( ˜Q[k,]1 [1] _Q[k,][∗])+_ _φ(s, a),_
_−_ _−_ [˜]

_N_ _j+1_


_H˜1[1],B[,l]_ _L˜[1][,j]( Q[˜][k,]1_ [1][)][ −] _L[˜][1][,j]( Q[¯][k,]1_ [1][)]
_l=N_

Y 


_j=1_


_H˜1[1],B[,l]_ _L[˜][1][,j]( Q[¯][k,]1_ [1][)] (28)
_l=N_

Y


_φ(s, a),_


_j=1_


We bound each of the terms above separately. By union bound, the following statements all hold

we must haveQsimulataneously with probability at-least˜[k,]1 [1][(][s, a][)][,][ ¯]Q[k,]1 [1][(]N[s, a]˜ [(1)][)]([ ∈]s, a[[0])[,] 1≥1 _γ_ []][. Using Lemmas]µmin2 _[NB] 1[.]−_ _δA simple observation using recursion shows that. By Theorem[ 12][ and][ 13][ we conclude that we must have:] 7, whenever NB > C_ _µ[τ]min[mix]_ [log(][ |S||A]δ ),

_−_

_N_ _j+1_

sup _φ(s, a),_ _H˜1[1],B[,l]_ _L˜[1][,j]( Q[˜][k,]1_ [1][)][ −] _L[˜][1][,j]( Q[¯][k,]1_ [1][)]
(s,a)∈S×A Xj=1 _lY=N_  

_≤_ _C∥∆[¯]_ _k∥∞_ _ηd[¯] log_ _|S||A|δ_ + ηd[¯] log _|S||A|δ_ (29)

q

     []

Now observe that _Q[¯][k,]1_ [1] is a deterministic sequence. Therefore, using Lemma 9 uniformly for every
_k ≤_ _K and (s, a) ∈S × A:_


_j+1_


sup
(s,a)∈S×A


_K_
_|S||A|_
(1 + VarP ( V[¯]k)(s, a)) + C [η][ log] _δ_

1 _γ_

  −

(30)


_N_ _j+1_

_φ(s, a),_ _j=1_ _l=N_ _H˜1[1],B[,l]_ _L[˜][1][,j]( Q[¯][k,]1_ [1][)] _[≤]_ _[C]_

X Y

Where, VarP ( V[¯]k) := VarP (V )(s, a; Q[¯][k,]1 [1][)][.]


_K_
_η log_ _|S||A|δ_
 


We now combine all the above with Equation (28) to show that with probability at least (1 − _δ), we_
must have uniformly for every k ≤ _K and (s, a) ∈S × A:_

_|ϵ˜1( Q[˜][k,]1_ [1][)(][s, a][)][| ≤] 1 1 _γ_ [exp] _−_ _[ηµ][min]2_ _[NB]_ + C∥∆[¯] _k∥∞_ _ηd[¯] log_ _|S||A|δ_ + ηd[¯] log _|S||A|δ_

_−_   q    _K_   []

_K_ _|S||A|δ_

+ C _η log_ _|S||A|δ_ (1 + VarP ( V[¯]k)(s, a)) + C [η][ log]1 _γ_ (31)
q   − 

  

We will now present a crude bound on ∥∆[¯] _k∥∞_ to reduce the analysis of Q-RexDaRe to the analysis
of Q-Rex.

**Claim 1. Whenever η** _C1_ (1−γ)[2] _, with probability at-least 1_ _δ, we must have for every_
_≤_ _d¯ log_ _|S||A|_ _−_

_δ_

_k ≤_ _K uniformly:_   

exp
_−_ _[ηµ][min]2_ _[NB]_ _η_ _K_
_∥∆[¯]_ _k∥∞_ _≤_ _C_  (1−γ)[2]  + C (1−γ)[4][ log] _|S||A|δ_

q

  

_Applying this directly to Equation (31), we conclude that with probability at least (1 −_ _δ), we must_
_have uniformly for every k ≤_ _K and (s, a) ∈S × A:_


-----

_K_

_|ϵ˜1( Q[˜][k,]1_ [1][)(][s, a][)][| ≤] (1 _C_ _γ) [exp]_ _−_ _[ηµ][min]2_ _[NB]_ + C [η][ log](1 _|S||A|γδ)[2]_ _d¯_

_−_     −  p _K_

_K_ _|S||A|δ_

+ C _η log_ _|S||A|δ_ (1 + VarP ( V[¯]k)(s, a)) + C [η][ log]1 _γ_ (32)
q   − 

  

_Proof of Claim 1. First note that T is γ contractive under the sup norm. Therefore,_

_∥∆[¯]_ _k+1∥∞_ = _w1[k,][1][)][ −T][ ( ¯]w1[k,][1][) + ˜]ϵk_

_∞_

_≤_ _[T][ ( ˜]w1[k,][1][)][ −T][ ( ¯]w1[k,][1][)]_ + ∥ϵ˜k∥∞
_∞_

_≤_ _γ[T]∥[ ( ˜]∆[¯]_ _k∥∞_ + ∥ϵ˜k∥∞ (33)


In Equation (31), note that VarP ( V[¯]k)(s, a) ≤ (1−1γ)[2][ . Therefore, under the conditions of this Claim,]

we can take C1 small enough so that uniformly for every (s, a) and k ≤ _K with probability at-least_
(1 − _δ), Equation (31) becomes:_


1 − _γ_

2




exp 2
_−_ _[ηµ][min][NB]_
_∥ϵ˜k( Q[˜][k,]1_ [1][)][∥][∞] _[≤]_  1 _γ_  + ∥∆[¯] _k∥∞_

_−_


_η_ _K_

_|S||A|_
(1−γ)[2][ log] _δ_
 


+ C


Combining the display above and the fact that ˜ϵk = ˜ϵ1( Q[˜][k,]1 [1][)][ in Equation (][33][), we conclude that]
with probability at-least 1 − _δ, for every k ≤_ _K uniformly:_

exp
_−_ _[ηµ][min]2_ _[NB]_ _η_ _K_

_∥∆[¯]_ _k+1∥∞_ _≤_ [1 +]2 _[ γ]_ _∥∆[¯]_ _k∥∞_ +  1−γ  + C (1−γ)[2][ log] _|S||A|δ_ (34)

q

  

Unrolling the recursion above and using the fact that ∥∆[¯] 1∥∞ = 0, we conclude the statement of the
claim.

We also note that the deterministic iterations _Q[¯][k,]1_ [1] converges exponentially in sup norm to Q[∗] due
to γ contractivity of T . That is:

_γ[k]_
_∥Q[¯][k,]1_ [1] _−_ _Q[∗]∥∞_ _≤_ (1 _γ)_ (35)

_−_

We are now ready to connect up with the proof of Theorem 2 with minor modifications. We follow
the same analysis as the proof of Theorem 2 but with VarP ( V[˜]k) replaced with VarP ( V[¯]k). Similar
to the proof of Theorem 2, we can control VarP ( V[¯]k) with respect to VarP (V _[∗]) and ∥Q[¯][k,]1_ [1] _−_ _Q[∗]∥∞_
along with Equation (35). We also replace αη with

_K_ _K_

_αη[dr]_ [:=] (1 _C_ _γ) [exp]_ _−_ _[ηµ][min]2_ _[NB]_ + C [η][ log](1 _|S||A|γδ)[2]_ _d¯ + C [η][ log]1_ _|S||A|γδ_

_−_     −  p   − 

Therefore, we conclude a version of Equation (23):


_η_ [+][ γ][K] _η_ _K_ _γ[K/][2]_
_∥∆K+1∥∞_ _≤_ _[α][dr]1_ _γ_ + C (1 _γ)[3][ log]_ _|S||A|δ_ s1 + (1 _γ)_ (36)

_−_ r _−_ _−_

  

Where ∆k := Q[k,]1 [1]
we conclude the result.[−][Q][∗][. Using the bounds on the parameters given in the statement of the Theorem,]


-----

K MINIMAX LOWERBOUNDS FOR TABULAR MDPS

Suppose Θ denotes the class of all tuples (M, π) where M is a tabular MDP and π is an exploratory
policy such that under the policy π, the MDP achieves a stationary distribution µ with minimum
probability at-least µmin (see section 2.1). The rewards are almost surely in the set [0,1]. We
receive the stationary sequence (st, at, rt)[T]t=1 [from MDP][ M][ under policy][ π][, which we denote as]
(st, at, rt)[T]t=1
with f ((st, at, r[∼]t)[(][T]t[M]=1[, π][)][. We write the minimax risk as:][)][. Let][ F][ be the class of all estimators][ f][ which estimate][ Q][∗] [for the MDP][ M]

_L(Θ, T_ ) := inff _∈F_ (Msup,π)∈Θ E(st,at,rt)Tt=1[∼][(][M][,π][)][∥][f] [((][s][t][, a][t][, r][t][)]t[T]=1[)][ −] _[Q][∗][∥][∞]_

**Theorem 5. There exists a constant C such that, for every µmin** (0, 1/4), γ (0, 1/2) and
_T_ _C_ _µmin(11_ _γ)_ _[, we must have:]_ _∈_ _∈_
_≥_ _−_


(θ, T ) _C_ _._
_L_ _≥_ s _T_ (1 _γ)[3]µmin_

_−_

1

_ϵ[2]µmin(1−γ)[3][ in order to achieve][ ϵ][ error for the Q values.]_


_Therefore, we need T ≥_


_Proof. We will use Le-Cam’s two point method to prove the result. Consider a class of MDPs_
denoted by MDP(q, p) (p, q ∈ [0, 1]) with state space S = {0, 1}, only one possible action, reward
function R : S → R, R(s) = s, and discount factor γ ∈ [0, 1). The transition probability for the
MDP is given by P (0|0) = 1 − _q, P_ (1|0) = q, P (0|1) = p and P (1|1) = 1 − _p._

Under these conditions, the stationary distribution is given by µ(0) = _p+p_ _q_ [and][ µ][(1) =] _p+q_ _q_ [and the]

value function can be written as


_V (1) =_


1 _γ(1_ _p)_ _γph(q)_ _[,]_
_−_ _−_ _−_


_h(q)_
_V (0) =_

where h(q) := 1 _γγq(1_ _q)_ [. Since there is only one action in each state, the value function coincides]1 − _γ(1 −_ _p) −_ _γph(q)_

_−_ _−_
with the Q function.

For showing the lower bound, consider two MDPs with parameters (p1, q) and (p2, q) with q <
min{p1, p2} and p1, p2 < 0.5. The stationary trajectories of length T under the two MDPs, denoted
by the random variables X1:[(][i]T[)] [for][ i][ = 1][,][ 2][, satisfies]

KL(X1:[(2)]T 1:T [)][ ≤] 2q(p1 − _p2)[2]_

_[||][X]_ [(1)] (p1 + q)(p2 + q)[2] ln 2 [+][ T][ 2](p[q]2 +[(][p][1] q[ −])p[p]1 ln 2[2][)][2] _[,]_


using KL(Ber(a) Ber(b)) _b ln 2_ whenever b 1/2.

Observe that µ[(2)]min|| [=][ µ][(2)] ≤[(1) =][2(][a][−]p[b]2[)]q[2]+q [and][ µ]min[(1)] _≤[=][ µ][(1)][(1) =]_ _p1q+q_ [. Combining this with the KL]

divergence bound above, we conclude that KL(X1:[(2)]T 1:T [)][ <] 18 [if we set Now let][ p][1][ =] 1−2 _γ_ [,]

_p1_ _[||][X]_ [(1)]
_p2 −_ _p1 = c_ _T µ[(1)]min_ [for small enough constant][ c][ and][ T][ ≳] _[p][(][p]q[+][q][)]_ . (notice that the equation gives

_p2 < 0.5 whenq_ _T satisfies the condition above and p1 < 0.25). By Pinsker’s inequality, we must_
have TV(X1:[(2)]T _[, X]1:[(1)]T_ [)][ ≤] 2[1] [.]


Elementary calculations show that |V [(1)](1)−V [(2)](1)| ≳ _[|](1[p][1]−[−]γ[p])[2][2][|][ =]_

1−2 _γ_ [. Therefore:]


_C_ _p1_

(1−γ)[2] _T µ[(1)]min_ [since][ q < p][1][ =]
q


_|V_ [(1)](1) − _V_ [(2)](1)| ≳


_T_ (1 _γ)[3]µ[(1)]min_
_−_


-----

Consider the semi-metric ρ(x, y) = 1 _γ(1_ _x1)_ _γxh(q)_ 1 _γ(1_ _y1)_ _γyh(q)_ for x, y (0, 1). Let

_−_ _−_ _−_ _[−]_ _−_ _−_ _−_ _∈_

Pi be the distribution of X1:[(][i]T[)] [, and let the distribution be parameterised by][ θ][(][P][i][) =][ V][ (][i][)][(1)][ for]
_i = 1, 2. Let_ _θ[ˆ] be any estimator based on the observations. By (Wainwright, 2019c, Chapter 15),_


min max _θ_ _θ(P_ ) _C_
_θˆ_ _P ∈{P1,P2}_ [E][P] _|[ˆ] −_ _|_ _≥_
h i

_C1_
_≥_


1

_T_ (1 _γ)[3]µ[(1)]min_ (1 − TV(X1:[(2)]T _[, X]1:[(1)]T_ [))]
_−_

1

(37)
_T_ (1 _γ)[3]µ[(1)]min_
_−_


_⇒_ for less than ϵ error, T has to be at-least


1

_µ[(1)]min[ϵ][2][(1][−][γ][)][3][ .]_


_q =_ _[µ]1[min]−2[(1]µmin[−][γ][)]_ [,][ T][ ≳] (1−γ1)µmin [, we can ensure that][ µ]min[(1)] [= 2][µ][min][ and][ µ]min[(2)] _[≥]_ _[µ][min][, which ensures]_

that MDP(p1, q) and MDP(p2, q) are both elements of Θ. Therefore, the two point risk in the LHS
of Equation (37) lower bounds L(Θ, T ), which allows us to conclude the result by substituting
_µ[(1)]min_ [=][ µ][min][.]

L PROOFS OF CONCENTRATION INEQUALITIES

L.1 PROOF OF LEMMA 8

First, we leverage the techniques established in (Jain et al., 2021b, Lemma 28) to show Lemma 14.
The proof follows by a simple re-writing of the proof of the aforementioned lemma which uses a
linear approximation (in η) to _H[˜]1[k,j],B[. We omit the proof for the sake of clarity.]_

**Lemma 14. Suppose ηB <** [1]3 _[. Then, the following PSD inequalities hold almost surely:]_


_B_

_φ˜[k,j]i_ _φ˜[k,j]i_ _⊤_ _H˜1[k,j],B_ _⊤_ _H˜1[k,j],B_ 1 1 _ηB2ηB_
_i=1_ _⪯_ _[⪯]_ _[I][−][2][η]_ _−_ _−_

X     


_B_

_φ˜[k,j]i_ _φ˜[k,j]i_ _⊤_
_i=1_

X  

(38)


_I_ 2η 1 + 1 _ηB2ηB_
_−_ _−_



_Proof of Lemma 8. We begin by proving the first part. Using Assumption 1 and the fact that the_
decoupled trajectory (˜s, ˜a)t is assumed to be mixed at the start of every buffer, we begin by first
noting from Lemma 14 that:

0 ⪯ E( H[˜]1[k,j],B[)][⊤]H[˜]1[k,j],B _[⪯]_ _[I][ −]_ _[ηB][E][(][s,a][)][∼][µ][φ][(][s, a][)(][φ][(][s, a][))][⊺]_ _[⪯]_ [(1][ −] _[ηB]κ_ [)][I .] (39)

Now, observe that:

1 _N_ _−1_ 1

_∥_ _H˜1[k,j],B[g][∥][2][ =][ g][⊤]_ ( H[˜]1[k,j],B[)][⊤] [h]( H[˜]1[k,N],B [)][⊤]H[˜]1[k,N],B _H˜1[k,j],B[g]_ (40)

_j=N_ _j=1_ _j=N_ 1

Y Y i Y−

Now note that _H[˜]1[k,N],B_ [is independent of][ ˜]H1[k,j],B [for][ j][ ≤] _[N][ −]_ [1][. Therefore, taking conditional expecta-]
tion conditioned on H1[k,j],B [for][ j][ ≤] _[N][ −]_ [1][ in Equation (][40][) and using Equation (][39][), we conclude:]


_H˜1[k,j],B[g][∥][2][ ≤]_ [(1][ −] _[ηB]κ_ [)][E][∥]
_j=N_

Y


E∥ _H˜1[k,j],B[g][∥][2][ ≤]_ [(1][ −] _[ηB]κ_ [)][E][∥] _H˜1[k,j],B[g][∥][2]_

_jY=N_ _j=YN_ _−1_

Applying the equation above inductively, we conclude the result.


E∥


We now prove Part 2. We apply Markov’s inequality to Part 1 along with Lemma 2 to show that
with probability at least 1 − _δ:_

1 1 exp

_H˜1[k,j],B[g][∥][φ][ ≤∥]_ _H˜1[k,j],B[g][∥≤]_ −√[ηNB]δ _κ_  _g_ exp( _κ_ [)] _κδ_

_∥_ _j=N_ _j=N_ _∥_ _∥≤_ _−_ _[ηNB]_ _[∥][g][∥][φ][ .]_

Y Y q


-----

L.2 PROOF OF LEMMA 9

_Proof. We intend to apply Freedman’s inequality (Freedman, 1975) like in (Li et al., 2021, Theorem_
4), but in an asynchronous fashion and with Markovian data. Here, reverse experience replay endows
our problem with the right filtration structure. Using Equation (9), we can write

_N_ _j+1_ _N_ _B_


_H˜1[k,l],BL[˜][k,j](w)⟩_ = η
_l=N_

Y


_Xi[k,j]_ (41)
_i=1_

X


_⟨φ(s, a),_


_j=1_


_j=1_


Where Xi[k,j] := (1 _η)N[˜]i[k,j]_ (s,a)ϵ˜[k,j]i 1 (˜s[k,j]i _, ˜a[k,j]i_ ) = (s, a) . This allows us to define the sequence
_−_
of sigma algebras Fi[k,j] = σ((˜s[k,l]m _[,][ ˜]a[k,l]m _ [) : (][l > j][ and][ m][ ∈] [[][B][])][ or][ (][l][ =][ j][ and][ m][ ≤] _[i][))][ - that is,]_
it is the sigma algebra of all states and rewards which appeared before and including (˜s[k,j]i _, ˜a[k,j]i_ )
inside the buffer j and all the states in buffers l > j. Notice that _N[˜]i[k,j](s, a) is measurable with_
respect to the sigma algebra _i_ . Using the fact that the buffers are independent, we conclude that:
_F_ _[k,j]_

E _Xi[k,j]_ _i_ = 0 and
_|F_ _[k,j]_
h i

E _Xi[k,j]_ _i_ 2 1 + γ[2]VarP (V )(s, a; w) 1 (˜s[k,j]i _, ˜a[k,j]i_ ) = (s, a) (1 _η)[2 ˜]Ni[k,j]_ (s,a) .
_|_ _|[2]|F_ _[k,j]_ _≤_ _−_
h i     


It is also clear from our assumptions that |Xi[j,k]| ≤ 1−2 _γ_ [almost surely. Consider the almost sure]

inequality for the sum of conditional variances:


_W_ _[k]_ =


E _Xi[k,j]_ _i_
_|_ _|[2]|F_ _[k,j]_
_i=1_

X h


_j=1_



_[K]_ _B_
2 1 + γ[2]VarP (V )(s, a; w) 1 (˜s[k,j]i _, ˜a[k,j]i_ ) = (s, a) (1 _η)[2 ˜]Ni[k,j]_ (s,a)
_≤_ _−_

_j=1_ _i=1_

  X X   

_N˜_ _[k](s,a)−1_ 1 + γ2VarP (V )(s, a; w)
= 2 1 + γ[2]VarP (V )(s, a; w) (1 _η)[2][t]_ 2

_−_ _≤_ _η_
_t=0_ 

  X


(42)


We now apply (Li et al., 2021, Equation (144),Theorem 4) with R


1

1 _γ_ [and][ σ][2]
_−_


1+γ[2]VarP (V )(s,a;w)


to conclude the result.


L.3 PROOF OF THEOREM 4

_Proof. For the sake of convenience, in this proof we will take σ[2]_ := 4(1 + ∥w∥φ[2] [)][. Suppose][ λ][ ∈] [R][.]
For 1 ≤ _m ≤_ _N consider:_

_N_ _m+1_ _N_ _m+1_ _⊤_ _N_ _j+1_
_−_ _−_

_Xm := ηλ[2]σ[2]_ _x,_ _H˜1[k,l],B_ _H˜1[k,l],B_ _x_ + λ _x,_ _H˜1[k,l],B_ _L˜[k,j](w)_
 _l=N_ _l=N_ !  _j=N_ _m+1_ _l=N_ ! 

Y Y X− Y

_X0 := ∥x∥[2]ηλ[2]σ[2]_

Here we use the convention that _l=N_ _H[˜]1[k,l],B_ [=][ I][ whenever][ j][ + 1][ > N] [. We claim that the sequence]
exp(Xm) forms a super martingale under an appropriate filtration. In this proof only, consider the
sigma algebra _m to be the sigma algebra of all the state action reward tuples in buffers[Q][j][+1]_ _N, . . ., N_
_F_ _−_
_m + 1 and let F0 be the trivial sigma algebra. Notice that exp(Xm) is Fm measurable._

**Lemma 15. Fix N** _m_ 1. Suppose Y R[d] _is a_ _m_ 1 measurable random vector. Then,
_≥_ _≥_ _∈_ _F_ _−_


exp _ηλ[2]σ[2]_ _Y,_ _H[˜]1[k,N],B_ _[−][m][+1]_ _H˜1[k,N],B_ _[−][m][+1]_ _⊤_ _Y_ + λ _Y,_ _L[˜][k,N]_ _[−][m][+1](w)_
_⟨_ _⟩_ _⟨_ _⟩_
   

_[F][m][−][1]_


_≤_ exp  ηλ[2]σ[2]∥Y ∥[2][]


-----

_In particular, taking Y =_ _Nl=−Nm+2_ _H˜1[k,l],B_ _⊤_ _x, we conclude that exp(Xm) is a super martingale_
_with respect to the filtrationQ Fm_ 

_Proof of Lemma 15. In the proof of this lemma, we will drop the superscripts k, m for the sake of_
convenience and due to conditioning on Fm−1, we will treat Y as a constant. Now we define the
natural filtration on the buffer under consideration (Gi)[B]i=1 [where][ G][i][ is the sigma algebra of the all]
state-action tuples from (s1, a1), . . ., (si, ai) and rewards (rh)1≤h<i.

Note that by the definition of _L[˜](w), we write:_ _L[˜](w) =_ _i=1_ _[η]ϵ[˜]i(w) H[˜]1,i−1φ[˜]i. With this in mind,_
for h ∈ [B] define _L[˜]h(w) =_ _i=1_ _[η]ϵ[˜]i(w) H[˜]1,i−1φ[˜]i. Now,_ _L[˜](w) = ηϵ˜B(w) H[˜]1,B−1φ[˜]B + L[˜]B−1(w)_

[P][B]

Now, notice that the random variables[P][h] _⟨Y,_ _L[˜]B−1⟩,⟨Y,_ _H[˜]1,B−1φ[˜]B⟩_ and ⟨Y, _H[˜]1[k,m],B_ _H˜1[k,m],B_ _⊤_ _Y ⟩_ are

_GB measureable. Furthermore, we must have: E_ _ϵ˜B(w)_ _GB_ = 0 and |ϵ˜B(w)| ≤2(1 + ∥w∥φ) ≤
_√2σ. Therefore, applying conditional Hoeffding’s lemma, we have:_
 

E exp _ηλ[2]σ[2]⟨Y,_ _H[˜]1,B_ _H˜1,B_ _⊤_ _Y ⟩_ + λ⟨Y, _L[˜](w)⟩_
     

= exp _ηλ[2]σ[2]_ _Y,_ _H[˜]1,B_ _H˜1,B_ _⊤_ _Y_ + λ _Y,_ _L[˜]B_ 1(w)[G][B] E exp _ληϵ˜B(w)_ _Y,_ _H[˜]1,B_ 1φ[˜]B
_⟨_ _⟩_ _⟨_ _−_ _⟩_ _⟨_ _−_ _⟩_
      

exp _ηλ[2]σ[2]_ _Y,_ _H[˜]1,B_ _H˜1,B_ _⊤_ _Y_ + λ _Y,_ _L[˜]B_ 1(w) exp _λ[2]η[2]σ[2]_ _Y,_ _H[˜]1,B_ 1φ[˜]B _[G][B]_
_≤_ _⟨_ _⟩_ _⟨_ _−_ _⟩_ _|⟨_ _−_ _⟩|[2][]_
     (43)

In the third step we have used the conditional version of Hoeffding’s lemma. Now, consider Z :=
( H[˜]1,B−1)[⊤]Y . Clearly,

_Y,_ _H[˜]1,B_ _H˜1,B_ _⊤_ _Y_ + η _Y,_ _H[˜]1,B_ 1φ[˜]B = Z _[⊤]H[˜]B,B[⊤]_ _H[˜]B,BZ + ηZ_ _[⊤]φ[˜]B(φ[˜]B)[⊤]Z_
_⟨_ _⟩_ _|⟨_ _−_ _⟩|[2]_
 

= Z _[⊤]_ [h]I _ηφ[˜]B(φ[˜]B)[⊤]_ + _φB_ _η[2][ ˜]φB(φ[˜]B)[⊤][i]_ _Z_
_−_ _∥_ [˜] _∥[2]_

_≤∥Z∥[2]_

Here we have used the fact that η < 1 and _φB_ _< 1. Using the bounds above in Equation (43), we_
_∥_ [˜] _∥_
conclude:

E exp _ηλ[2]σ[2]⟨Y,_ _H[˜]1,B_ _H˜1,B_ _⊤_ _Y ⟩_ + λ⟨Y, _L[˜]B(w)⟩_
     

exp _ηλ[2]σ[2]_ _Y,_ _H[˜]1,B_ 1 _H˜1,B_ 1 _⊤_ _Y_ + λ _Y,_ _L[˜]B_ 1[G](w[B]) (44)
_≤_ _⟨_ _−_ _−_ _⟩_ _⟨_ _−_ _⟩_
   

Using Equation (44) recursively, we conclude the first statement of the lemma. The last part of the
lemma follows easily from the definition of Xm.


By Lemma 15, we conclude that exp(Xm) is a super martingale with respect to the filtration _m._
_F_
Therefore, we must have:

E exp(XN ) ≤ exp(X0) = exp(η∥x∥[2]λ[2]σ[2]) .

It is also clear that XN _λ_ _x,_ _j=N_ _m+1_ _jl=+1N_ _H[˜]1[k,l],B_ _L˜[k,j](w)_ . Applying Chernoff bound
_≥_ _−_

with we conclude that the concentration inequality in Equation ( Q  13). We can then directly apply
(Vershynin, 2018, Theorem 8.1.6) to Equation ([P][N] 13) in order to obtain uniform concentration bounds.


-----

L.4 PROOF OF LEMMA 10

_Proof of Lemma 10. By definition of ˜ϵk, we have: ˜w1[k][+1][,][1]_ = T ( ˜w1[k,][1][) + ˜]ϵk. Therefore, for any
(s, a) ∈S × A, we must have:

_⟨φ(s, a), ˜w1[k][+1][,][1]⟩_ = R(s, a) + γEs′∼P (·|s,a) supa[′]∈A⟨φ(s[′], a[′]), ˜w1[k,][1][⟩] [+][ ⟨][φ][(][s, a][)][,][ ˜]ϵk⟩ _._

Using the fact that R(s, a) ∈ [0, 1], we conclude:

_w˜1[k][+1][,][1]_ _φ_ 1 + γ _w˜1[k,][1]_ _ϵk_ _φ_ (45)
_∥_ _∥_ _≤_ _∥_ _[∥][φ][ +][ ∥][˜]_ _∥_

By independence of outer-loops for the coupled data, we note that ˜w1[k,][1] is independent of the data
in buffer k. Therefore we can apply Theorem 4 (and resp. Lemma 8) conditionally with w = ˜w1[k,][1]
(and resp. g = ˜w1[k,][1] _w[∗]), the bias variance decomposition given in Lemma 5 and the bound on_
_−_
_∥w[∗]∥φ in Lemma 4 to conclude that with probability at-least 1 −_ _δ, for every k ≤_ _K:_


_Kκ_


exp(− _[ηNB]κ_ [)(][∥]w[˜]1[k,][1][∥][φ][ +]


_ϵ˜k_ _φ_
_∥_ _∥_ _≤_


1 _γ_ [)]
_−_

log( [2]δ[K] [)] (46)




+ C(1 + _w˜1[k,][1]_ _CΦ +_
_∥_ _[∥][φ][)][√][η]_



We now choose constants C1 and C2 in the statement of the Lemma such that Equation 46 implies:


_∥ϵ˜k∥φ ≤_ [1][ −]2 _[γ]_ _∥w˜1[k,][1][∥][φ][ + 1]_

Using the equation above in Equation (45), we conclude:

_∥w˜1[k][+1][,][1]∥φ ≤_ 2 + [1 +]2 _[ γ]_ _∥w˜1[k,][1][∥][φ][ .]_


Unrolling the recursion above and noting w1[1][,][1] = 0, we conclude that with probability at-least 1 _δ,_
we must have ∥w˜1[k,][1][∥][φ][ ≤] 1−4 _γ_ [for every][ k][ ≤] _[K][. The bound in item 2 follows by using item 1,] −_

Equation (46) and the fact that ˜w1[k,][1] is independent of the data in buffer k due to our coupling.

L.5 PROOF OF LEMMA 12


_{−byProof. [ 1d[¯], 1]. Consider For the sake of convenience, we will take}d[¯]:_ _Y ∈{−1, 1}d[¯]. We consider the class of random variables indexed by elements ofd[¯] = |supp(P_ (·|s, a))| and index supp(P (·|s, a))

∆(Y ; s, a) = _Ys′_ _Pˆk(s[′]_ _s, a)_ _P_ (s[′] _s, a)_

_|_ _−_ [¯] _|_
_s[′]_

X h i

The proof proceeds in a similar way to the proof of Lemma 9 via the Freedman inequality. To bring
out the similarities we define similar notation. Consider the sequence of sigma algebras _i_ for
_F_ _[k,j]_
_i ∈_ [B] and j ∈ [K] as defined in the proof of Lemma 9. We now define

_Xi[k,j](Y ) := (1−η)N˜i[k,j]_ (s,a) _Ys′_ 1(˜s[k,j]i+1 [=][ s][′][,][ ˜]s[k,j]i = s, ˜a[k,j]i = a) − _P_ (s[′]|s, a)1(˜s[k,j]i = s, ˜a[k,j]i = a)

_s[′]_

[X] 

We note that E _Xi[k,j](Y )_ _i_ = 0 and _Xi[k,j](Y )_ 2 almost surely and a simple calculation
_|F_ _[k,j]_ _|_ _| ≤_

reveals that: E hXi[k,j](Y ) _i_ i (1 _η)[2 ˜]Ni[k,j]_ 1(˜s[k,j]i = s, ˜a[k,j]i = a)
_|_ _|[2]|F_ _[k,j]_ _≤_ _−_

It is also clear that:h ∆(Y ; s, a) =i _η_ _j=1_ _Bi=1_ _[X]i[k,j](Y ). Apply Freedman’s concentration in-_
equality, we conclude for any fixed Y 1, 1 _d[¯] and (s, a)_,
_∈{−_ P} _∈S × A_

[P][K]

P(|∆(Y ; s, a)| > C _η log(2/δ) + η log(2/δ)) ≤_ _δ_
p


-----

Applying a union bound over all Y ∈−1, 1[d], we conclude:


_ηd[¯] log(4/δ) + ηd[¯] log(4/δ)) ≤_ _δ ._


P( sup
_Y ∈{−1,1}d[¯]_ _[|][∆(][Y][ ;][ s, a][)][|][ > C]_

We complete the proof by noting that


_P[ˆ]k(s[′]_ _s, a)_ _Pk(s[′]_ _s, a)_
_|_ _|_ _−_ [¯] _|_ _|_
_s[′]∈suppX(P (·|s,a))_


sup
_Y ∈{−1,1}d[¯]_ _[|][∆(][Y][ ;][ s, a][)][|][ =]_

M TECHNICAL LEMMAS

M.1 COUPLING LEMMA


We first introduce some useful notation: Let D[k,j] (resp. _D˜_ _[k,j]) be the tuple of random variables_
(s[k,j]i _, a[k,j]i_ _, ri[k,j])[B]i=1[+1]_ [(resp.][ (˜]s[k,j]i _, ˜a[k,j]i_ _, ˜ri[k,j])[B]i=1[+1][).]_

**Lemma 16. Suppose**


_Cτmix log(_ _[T]δ_ [)][ in the tabular setting]

_Cτmix log(_ _CmixT_ _δ_ [)][ in the general setting] (47)


_u ≥_


_Then, we can define the sequences (st, at, rt) and (˜st, ˜at, ˜rt) on a common probability space such_
_that:_

_1. The tuples D[k,j]_ _and_ _D[˜]_ _[k,j]_ _have the same distribution for every (k, j),_

_2. The sequence_ _D[˜]_ _[k,j]_ _for k ≤_ _N, j ≤_ _K is i.i.d._

_3. Equation (4) holds_

_Proof. The proof of this lemma for the tabular case is a rewriting of the the proofs of Lemmas 1,2,3_
in (Bresler et al., 2020). For the general state space case, we apply appropriate modifications as
pioneered in (Goldstein, 1979).

M.2 PROOF OF LEMMA 2

_Proof. The first part follows from the definitions. For the second part, note that:_ _x_ _x_ _φ_
_∥_ _∥≥∥_ _∥_
follows from Cauchy-Schwarz inequality and Assumption 1. For the reverse inequality we use
Assumption 4 to show that:

_∥x∥φ[2]_ _[≥]_ [E](s,a)∼µ[⟨][φ][(][s, a][)][, x][⟩][2][ ≥∥][x]κ[∥][2]

M.3 PROOF OF LEMMA 3

_Proof. Existence is guaranteed by Definition (2) and uniqueness follows from the assumption that_
span(Φ) = R[d].

M.4 PROOF OF LEMMA 4

have:Proof. Suppose w0, ˜w0 ∈ R[d] are arbitrary. Let w1 = T (w0) and ˜w1 = T ( ˜w0). For any φ(s, a), we

_|⟨φ(s, a), w1 −_ _w˜1⟩| = γ_ Es′∼P (·|s,a) supa[′] _⟨φ(s[′], a[′]), w0⟩−_ Es′∼P (·|s,a) supa[′] _⟨φ(s[′], a[′]), ˜w0⟩_ (48)
_∈A_ _∈A_


-----

By Assumption 2, for every fixed s[′] there exist a[′]max[(][s][′][)][,][ ˜]a[′]max[(][s][′][)][ such that]

_asup[′]∈A⟨φ(s[′], a[′](s[′])), w0⟩_ = ⟨φ(s[′], ˜a[′]max[(][s][′][))][, w][0][⟩] and _asup[′]∈A⟨φ(s[′], a[′]), ˜w0⟩_ = ⟨φ(s[′], ˜a[′]max[(][s][′][))][,][ ˜]w0⟩.

Therefore for any s[′] it holds that,

_φ(s[′], ˜a[′]max[)][, w][0]_ _w0_ sup _φ(s[′], a[′]), w0_ sup _φ(s[′], a[′]), ˜w0_ _φ(s[′], a[′]max[)][, w][0]_ _w0_
_⟨_ _[−]_ [˜] _⟩≤_ _a[′]_ _⟨_ _⟩−_ _a[′]_ _⟨_ _⟩≤⟨_ _[−]_ [˜] _⟩_
_∈A_ _∈A_

=⇒ supa[′] _⟨φ(s[′], a[′]), w0⟩−_ _asup[′]_ _⟨φ(s[′], a[′]), ˜w0⟩_ _≤∥w0 −_ _w˜0∥φ ._
_∈A_ _∈A_

Combining this with Equation (48), we conclude the γ-contractivity of T . By contraction mapping theorem, T has a unique fixed point we conclude that it is w[∗] by considering Q(s, a) :=
_⟨w[∗], φ(s, a)⟩_ and showing that it satisfies the bellman optimality condition in Equation (2). For the
norm inequality, we note that since R(s, a) [0, 1] by assumption and w[∗] = (w[∗]), we have:
_∈_ 1 _T_
_|⟨φ(s, a), w[∗]⟩| ≤_ 1 + γ∥w[∗]∥φ. Therefore, ∥w[∗]∥≤ 1−γ [. The second norm equality follows from]

Lemma 2.


M.5 PROOF OF LEMMA 5

_Proof. We write the iteration in the outer-loop k as:_

_w˜i[k,j]+1_ [=] _I_ _ηφ[˜][k,j]i_ [[˜]φ[k,j]i []][⊤][i] _w˜i[k,j]_ + ηφ[˜][k,j]i
_−_ _−_ _−_ _−_
h


_r˜[k,j]i_ [+][ γ][ sup] _w˜1[k,][1][, φ][(˜]s[k,j](i_ 1)[, a][′][)][⟩]
_−_ _a[′]∈A⟨_ _−_ _−_


Using the fact that ⟨w˜[k][+1][,][∗], _φ[˜][k,j]−i_ _[⟩]_ [= ˜]R−[k,j]i [+][ γ][E]s[′]∼P (·|s˜[k,j]−i _[,]a[˜][k,j]−i_ [)][ sup][a][′][∈][A][⟨][φ][(][s][′][, a][′][)][,][ ˜]w1[k,][1][⟩][, and recall-]
ing the notation in (5),(6), we write:

_w˜i[k,j]+1_ _w[k][+1][,][∗]_ = _I_ _ηφ[˜][k,j]i_ [[˜]φ[k,j]i []][⊤][i ]w˜i[k,j] _w[k][+1][,][∗][]_ + ηφ[˜][k,j]i _[ϵ][k,j]i_ _[.]_

_[−]_ [˜] _−_ _−_ _−_ _−_ _−_ _−_
h

Therefore,
_w˜1[k,j][+1]_ _−_ _w˜[k][+1][,][∗]_ = H[˜]1[k,j],B[( ˜]w1[k,j] _−_ _w˜[k][+1][,][∗]) + L[˜][k,j]_

Unfurling further, and using the fact that ˜w1[k,N] [+1] = ˜w1[k][+1][,][1], we conclude the statement of the
lemma.

M.6 PROOF OF LEMMA 7

We let _N˜_ _[k,j](s, a) :=_ _i=1_ [1][((˜]s[k,j]i _, ˜a[k,j]i_ ) = (s, a)). We want to show that the quantity
_Nj=1_ _N[˜]_ _[k,j](s, a) concentrates around its expectation. Note that EN[˜]_ _[k,j](s, a) = Bµ(s, a) and that_
_N˜_ _[k,j](s, a) are i.i.d. for j[P][B][N_ ]. From the proof of (Paulin, 2015, Theorem 3.4) and the fact that
P 1 _∈_
_γps ≥_ 2τmix [, we conclude:]


E exp(λ( _N˜_ _[k,j](s, a) −_ _Bµ(s, a))) =_ E exp(λ( N[˜] _[k,j](s, a) −_ _Bµ(s, a)))_

_j=1_ _j=1_

X Y

8N (B + 2τmix)µ(s, a)τmixλ2
exp
_≤_ 1 20λτmix
 _−_ 

Following the Chernoff bound in the proof of (Paulin, 2015, Theorem 3.4), we conclude:


(49)

(50)


_t[2]_
P( _N[˜]_ _[k](s, a)_ _NBµ(s, a)_ _> t)_ 2 exp
_|_ _−_ _|_ _≤_ _−_ 16τmix(B + 2τmix)Kµ(s, a) + 40tτmix



-----

Taking t = [1]2 _[NBµ][(][s, a][)][, whenever][ B][ ≥]_ _[τ][mix][ (by assumption), we must have:]_

P _N˜_ _[k](s, a) ≤_ [1]2 _[NBµ][(][s, a][)]_ _≤_ 2 exp _−C_ _[KBµ]τmix[(][s,a][)]_
   

for some constant C. Further, via a union bound over all state action pairs (s, a), we conclude the
statement of the lemma.

M.7 PROOF OF LEMMA 13

We first prove a simple consequence of multivariate calculus.
**Lemma 17. Let f : R[n]** _→_ R be defined by f (x) = supi xi. Then, for every x, y ∈ R[n], there exists
_ζ ∈_ R[d] _such that ∥ζ∥1 = 1, ζi ≥_ 0 and:
_f_ (x) − _f_ (y) = ⟨ζ, x − _y⟩_

_Proof. We consider the log-sum-exp function._ Given L R[+], define fL(x) :=
1 _∈_

_L_ [log (][P]i[n]=1 [exp(][Lx][i][))][. An elementary calculation shows that:]

log(n)

+ sup _xi_ _fL(x)_ sup _xi_
_L_ _i_ _≥_ _≥_ _i_


Therefore, for any fixed x,


lim
_L→∞_ _[f][L][(][x][) =][ f]_ [(][x][)][ .]

exp(Lxi)
Now, ∇fL(x) = (p1, . . ., pn) where pi = _nj=1_ [exp(][Lx][j] [)] [.] Clearly, ⟨∇fL(x), ei⟩≥ 0 and

_∥∇fL(x)∥1 = 1. By the mean value theorem there existsP_ _βL such that:_

_fL(x)_ _fL(y) =_ _fL(βL), x_ _y_ _._ (51)
_−_ _⟨∇_ _−_ _⟩_

Now, the simplex in R[n] (denoted by ∆n) is compact. Therefore, there exists a sub-sequence
_Lin Equation (k →∞_ such that51), we conclude the result. limk→∞ _∇fLk_ (βLk ) = ζ ∈ ∆n. Taking limit along the sub-sequence Lk

_Proof of Lemma 13. In the tabular setting, we have the following expression using Equation (41):_


_B_

(1 _η)N˜i[k,j]_ (s,a)1 (˜s[k,j]i _, ˜a[k,j]i_ ) = (s, a) _ϵ˜[k,j]i_ (w) _ϵ˜[k,j]i_ (v) (52)
_−_ _−_
_i=1_

X h i h i


L.H.S = _[ηγ]_


_j=1_


Where


_ϵ˜[k,j]i_ (w) := asup[′]∈A⟨w, φ(˜s[k,j]i+1[, a][′][)][⟩−] [E]s[′]∼P (·|s˜[k,j]i _,a˜[k,j]i_ ) _a[sup][′]∈A⟨φ(s[′], a[′]), w⟩_

Now, by an application of Lemma 17, we show that for some η(·|s[′], w, v) ∈ ∆(A), depending only
on (s[′], w, v), we have:


_asup[′]∈A⟨w, φ(s[′], a[′]), w⟩−_ _asup[′]∈A⟨w, φ(s[′], a[′]), v⟩_ =


_ζ(a[′]|s[′], w, v)⟨φ(s[′], a[′]), w −_ _v⟩_
_aX[′]∈A_


Now, using the definition of _P[ˆ]k(_ _s, a) and_ _P[¯](_ _s, a) in the discussion preceding Lemma 12, we can_

_·|_ _·|_
simplify Equation (52) to:


_Pˆk(s[′]_ _s, a)_ _Pk(s[′]_ _s, a)_ _ζ(a[′]_ _s[′], w, v)_ _φ(s[′], a[′]), w_ _v_ (53)
_|_ _−_ [¯] _|_ _|_ _⟨_ _−_ _⟩_



L.H.S. = γ


_s[′]∈supp(P (·|s,a))_
_a[′]∈A_


We conclude the statement of the lemma by an application of the Holder inequality.


-----

M.8 PROOF OF LEMMA 11

_Proof. We solve for f_ (u[∗]) = u[∗] to obtain the relation: u[∗] = α + β√u[∗], which after discarding the

2

_β+[√]β[2]+4α_

negative solution for _√u[∗]_ yields the unique solution: u[∗] = 2 . To prove the second

 

part, consider for arbitrary x, y ∈ R[+], the following hyper-contractivity:

_|f_ (x) − _f_ (y)| = β|[√]x − _[√]y|_

_≤_ _β_ _|x −_ _y|_ (54)

The second step follows from the fact that |[√]a − _√b| ≤p_ _|a −_ _b|. Since u[∗]_ = f (u[∗]), we apply the

inequality above:

p


_|f_ [(][t][)](u) − _u[∗]| = |f_ [(][t][)](u) − _f_ [(][t][)](u[∗])|


_|f_ [(][t][−][1)](u) − _f_ [(][t][−][1)](u[∗])| (55)


_≤_ _β_

We then conclude the result by induction.


-----

