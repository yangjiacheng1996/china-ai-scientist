# DEGREE: DECOMPOSITION BASED EXPLANATION
## FOR GRAPH NEURAL NETWORKS

**Qizhang Feng[1], Ninghao Liu[2], Fan Yang[3], Ruixiang Tang[3], Mengnan Du[1], Xia Hu[3]**

1Department of Computer Science and Engineering, Texas A&M University
2Department of Computer Science, University of Georgia
3Department of Computer Science, Rice University
_{qf31,dumengnan}@tamu.edu, ninghao.liu@uga.edu, {fy19,rt39,xia.hu}@rice.edu_

ABSTRACT

Graph Neural Networks (GNNs) are gaining extensive attention for their application in graph data. However, the black-box nature of GNNs prevents users
from understanding and trusting the models, thus hampering their applicability. Whereas explaining GNNs remains a challenge, most existing methods fall
into approximation based and perturbation based approaches with suffer from
faithfulness problems and unnatural artifacts, respectively. To tackle these problems, we propose DEGREE (Decomposition based Explanation for GRaph nEural
nEtworks) to provide a faithful explanation for GNN predictions. By decomposing the information generation and aggregation mechanism of GNNs, DEGREE
allows tracking the contributions of specific components of the input graph to the
final prediction. Based on this, we further design a subgraph level interpretation
algorithm to reveal complex interactions between graph nodes that are overlooked
by previous methods. The efficiency of our algorithm can be further improved
by utilizing GNN characteristics. Finally, we conduct quantitative and qualitative
experiments on synthetic and real-world datasets to demonstrate the effectiveness
of DEGREE on node classification and graph classification tasks.

1 INTRODUCTION

Graph Neural Networks (GNNs) play an important role in modeling data with complex relational
information (Zhou et al., 2018), which is crucial in applications such as social networking (Fan et al.,
2019), advertising recommendation (Liu et al., 2019), drug generation (Liu et al., 2020), and agent
interaction (Casas et al., 2019). However, GNN suffers from its black-box nature and lacks a faithful
explanation of its predictions.

Recently, several approaches have been proposed to explain GNNs. Some of them leverage gradient or surrogate models to approximate the local model around the target instance (Huang et al.,
2020; Baldassarre & Azizpour, 2019; Pope et al., 2019). Some other methods borrow the idea from
perturbation based explanation (Ying et al., 2019; Luo et al., 2020; Lucic et al., 2021), under the
assumption that removing the vital information from input would significantly reduce output confidence. However, approximation based methods do not guarantee the fidelity of the explanation
obtained, as Rudin (2019) states that a surrogate that mimics the original model possibly employs
distinct features. On the other hand, perturbation based approaches may trigger the adversarial nature of deep models. Chang et al. (2018) reported this phenomenon where masking some parts of
the input image introduces unnatural artifacts. Additionally, additive feature attribution methods (Vu
& Thai, 2020; Lundberg & Lee, 2017) such as gradient based methods and GNNExplainer only provide a single heatmap or subgraph as explanation. The nodes in graph are usually semantically
individual and we need a fine-grained explanation to the relationships between them. For example,
in organic chemistry, the same functional group combined with different structures can exhibit very
different properties.

To solve the above problems, we propose DEGREE (Decomposition based Explanation for GRaph
nEural nEtworks), which measures the contribution of components in the input graph to the GNN
prediction. Specifically, we first summarize the intuition behind the Context Decomposition (CD)


-----

algorithm (Murdoch et al., 2018) and propose that the information flow in GNN’s message propagation mechanism is decomposable. Then we design the decomposition schemes for the most
commonly used layers and operations in GNNs, so as to isolate the information flow from distinct
node groups. Furthermore, we explore the subgraph-level explanation via an aggregation algorithm
that utilizes DEGREE and the structural information of the input graph to construct a series of subgraph sets as the explanation. DEGREE guarantees explanation fidelity by directly analyzing GNN
feed-forward propagation, instead of relying on input perturbation or the use of alternative models.
DEGREE is non-additive and can therefore uncover non-linear relationships between nodes. We
quantitatively and qualitatively evaluate the DEGREE on both synthetic and real-world datasets to
validate the effectiveness of our method. The contributions of this work are summarized as follows:

-  We propose a new explanation method (DEGREE) for GNNs, from the perspective of decomposition. By elucidating the feed-forward propagation mechanism within GNN, DEGREE allows
capturing the contribution of individual components of the input graph to the final prediction.

-  We propose an aggregation algorithm that provides important subgraphs as explanation in order
to mine the complex interactions between graph components. We combine the property of the
message propagation mechanism to further reduce the computation.

-  We evaluate DEGREE on both synthetic and real-world datasets. The quantitative experiments
show that our method could provide faithful explanations. The qualitative experiments indicate
that our method may capture the interaction between graph components.

2 RELATED WORK

Despite the great success in various applications, the black-box nature of deep models has long been
criticized. Explainable Artificial Intelligence (XAI) tries to bridge the gap by understanding the
internal mechanism of deep models (Du et al., 2019). Meanwhile, the need to tackle non-Euclidean
data, such as geometric information, social networks, has given rise to the development of GNNs.
Similar to the tasks on image or text data, GNNs focus on node classification (Henaff et al., 2015),
graph classification (Xu et al., 2018; Zhang et al., 2018), and link prediction (Zhang & Chen, 2018;
Cai & Ji, 2020). Message passing mechanism allows the information to flow from one node to
another along edges, and empowers GNNs with convolutional capabilities for graph data.

While the explainability in image and text domains is widely studied (Shrikumar et al., 2017; Sundararajan et al., 2017; Simonyan et al., 2013), the explainability of GNN is on the rise. First, some
recent work adapts the interpretation methods used for traditional CNNs to GNNs (Baldassarre &
Azizpour, 2019; Pope et al., 2019). They employ gradient values to investigate the contribution
of node features to the final prediction. However, these methods ignore the topological information, which is a crucial property of graph data. Second, some methods trace the model prediction
back to the input space in a backpropagation manner layer by layer (Schwarzenberg et al., 2019;
Schnake et al., 2020). Third, some methods define a perturbation-based interpretation whereby they
perturb node, edge, or node features and identify the components that affect the prediction most.
Specifically, GNNExplainer and PGExplainer (Ying et al., 2019) maximize the mutual information
between perturbed input and original input graph to identify the important features. Causal Screening (Wang et al., 2021) searches for the important subgraph by monitoring the mutual information
from a cause-effect standpoint. CF-GNNExplainer (Lucic et al., 2021) proposes to generate counterfactual explanations by finding the minimal number of edges to be removed such that the prediction
changes. In addition, XGNN (Yuan et al., 2020) builds a model-level explanation for GNNs by
generating a prototype graph that can maximize the prediction. Moreover, due to the discrete and
topological nature of graph data, XGNN defines graph generation as a reinforcement learning task
instead of gradient ascent optimization.

Many previous explanation methods for GNNs suffer from adversarial triggering issues, faithfulness issues and additive assumptions. To this end, we propose a decomposition based explanation
for GNNs (DEGREE) to remedy these problems. DEGREE enables to track the contribution of
the components from the input graph to the final prediction by decomposing a trained GNN. Thus,
DEGREE guarantees the integrity of the input and eliminates the adversarial triggering issue of the
perturbation-based approach. Since no surrogate models are used, DEGREE guarantees its faithfulness. Meanwhile, by integrating the decomposition to the normal layer, DEGREE does not have any
additional training process.


-----

3 DEGREE: DECOMPOSITION BASED EXPLANATION FOR GNNS

B A B A B

A
C C C

D GCN D GCNs D

Input E Intermediate E Final E
Graph F Graph F Graph F

(a) (b) (c)

Node Target

B Feature Group

A

C B NN Target Background

D Portion Portion

Target Background
Message Message

(d) (e)

Figure 1: Illustration of the DEGREE for decomposing GCN. Node features or latent embeddings
contain target portion (orange hemisphere) and an background portion (blue hemisphere). (a)-(c)
show the workflow of the GCN, exhibiting only the messages aggregation for node A. (d) demonstrates message aggregation after decomposition. (e) demonstrates the decomposed message flow.


In this section, we introduce the details of the proposed explanation method. First, we introduce
the notations and problem definition. Then, we discuss the general idea of decomposition based
explanation. Finally, we develop concrete decomposition schemes for different GNNs layers.

3.1 PROBLEM DEFINITION

We first introduce the notations used in this work. Given a graph G = (V, E), where V is the set of
nodes and E is the set of edges between nodes. The adjacency matrix of G is denoted as A ∈ R[N] _[×][N]_,
where N = is the number of nodes, so = _v1, v2, ..., vN_ . The nodes are associated with
_|V|_ _V_ _{_ _}_
features, and the feature matrix is denoted as X ∈ R[N] _[×][F]_, where F is the feature dimension.

In this work, we focus on explaining GNN-based classification models. Let f denote the target
GNN model. f computes the likelihood of a node or graph belonging to the target class, where
_f : G 7→_ R[|V|] or f : G 7→ R, for node or graph classification respectively.

The goal of explanation is to find the most important subgraph in G given f (G), which requires
measuring the contribution of all possible subgraphs and find the ones with high contribution scores.
However, there are two challenges to be addressed. (1) Given any subgraph of interest, how to
estimate its contribution without breaking up the input graph? (2) The number of all subgraphs in G
is usually very large, so how to choose candidate subgraphs for improving explanation efficiency?
We tackle the first challenge in the following part of Sec 3 and solve the second one in Sec 4.

3.2 DECOMPOSITION BASED EXPLANATION


In general, a prediction model f contains multiple layers Lt, t 1, . . ., T :
_∈{_ _}_

_f_ (X) = LT ◦ _LT −1 ◦· · · ◦_ _L2 ◦_ _L1(X)._ (1)

Let X[t] denotes the input to Lt, so X[t + 1] = Lt(X[t]) and X[1] = X. The symbol denotes
_◦_
function composition. Here X[t] ∈ R[N] _[×][F][t]_ is the embedding matrix at t-th layer, where Ft is the
latent dimension. The embedding vector of the i-th node at t-th layer is denoted as Xi[t].

The core idea of decomposition based explanation is that, given a target node group (or subgraph) of
interest, we estimate its contribution score to model prediction merely through feed-forward propagation. We call the information propagated from the target group as target portion, and the rest
of information is called background portion. It is worth noting that, a node is in the target group
does not necessarily mean it is important, while it only means we are interested in its importance
score. In the following, we use γ and β to denote the target and background portion, respectively.
Let m ∈{0, 1}[N], where mi = 1 means vi belongs to the target group and otherwise mi = 0.


-----

Then, the decomposition is initialized from the layer of node features, where the target portion and
background portion of the input feature matrix are: X[γ] = diag(m)X and X[β] = (I − _diag(m))X,_
respectively. In a neural network, information from different parts of input are merged in the feedforward process into latent representations, which poses challenges for explanation. Suppose the
target and background portion in X[t] are known from prior layer, we could explain the model if
we can still distinguish the information flows of the two portions inside Lt. That is, at layer Lt,
suppose its input can be decomposed as X[t] = X[γ][t] + X[β][t], the following relations need to hold
for explanation:


_L[D]t_ [(][X][γ][[][t][]][,][ X][β][[][t][]) =] _, B(X[γ][t], X[β][t])_ (2)

 **X[γ]** [t+1] **X[β]** [t+1] 
 

_Lt(X[t]) = X[t + 1] =|[Γ(][X][γ] X[[][t]{z[]][,][γ][ X][t + 1] +[β][[][t][])]}_ | **X[β][t{z + 1],** } (3)


where L[D]t [(][·][,][ ·][)][ denotes the decomposed version of layer][ L][t][.][ Γ(][·][,][ ·][)][ and][ B][(][·][,][ ·][)][ corresponds to the]
contribution of the target and the background portion to layer Lt. X[γ][t + 1] and X[β][t + 1] denotes
the target and background portion of X[t + 1] as the input to the next layer. The decomposition
above goes from the input, through all intermediate layers, to the final prediction. If a target node
group or subgraph is important, then it should contributes to most of the prediction, meaning that
Γ(X[γ][T ], X[β][T ]) ≈ _f_ (X).

3.3 INTUITIONS BEHIND DECOMPOSITION BASED EXPLANATION FOR GNN

The intuition behind decomposition based explanation could be summarized as two rules: (1) the
target and background portion at a higher layer mainly comes from the target and background portion
at the lower layer respectively; (2) ideally there should be little interaction between the target portion
and the background portion. Please note that the partition is not dimension-wise, meaning that each
latent dimension may contain information from both target and background portions.

Figure 1 briefly illustrates the working principle of GNNs: the model computes neural message for
each node pair and aggregates message for them from their neighbors. A major step of decomposing
GNNs is that: the target and background portion of a node are aggregated from the target and
background portion of its neighbours, respectively. This can be easily illustrated by the distributive
nature of the GNN information aggregation mechanism:

**X[t + 1] = AX[t] = A** **X[γ][t] + X[β][t]** = AX[γ][t] + AX[β][t] _._ (4)
  **X[γ]** [t+1] **X[β]** [t+1]

Nevertheless, the above equation is only a conceptual illustration. A real GNN model could con
| {z } | {z }

sist of various layers, such as graph convolution layers, fully connected layers, activation layers
and pooling layers. Several challenges still need to be tackled to develop an effective explanation
method. First, how to design the decomposition scheme for different types of layers? Second,
how to efficiently find out the important nodes and subgraphs, by choosing the appropriate target/background group given all possible node combinations?

3.4 DECOMPOSING GNN LAYERS

In this work, we consider the decomposition scheme for two commonly used GNN architectures:
GCN (Kipf & Welling, 2016) and GAT (Veliˇckovi´c et al., 2017).

3.4.1 DECOMPOSING GCNS

The GCN architecture consists of graph convolution, fully connected layers, ReLU and maxpooling.

**Graph Convolution Layer: The graph convolution operation pass messages between nodes:**


**X[t + 1] = D[˜]** _[−]_ 2[1] ˜AD[˜] _[−]_ [1]2 X[t]W + b, (5)

where W and b denote the trainable weights and bias. Here b is optional. **A[˜]** = A + I denotes the
adjacency matrix with self loop. The matrix **D[˜]** _i,i =_ _j_ **A[˜]** _i,j is the diagonal degree matrix of_ **A[˜]** .

The corresponding decomposition can be designed as follows:

[P]


-----

**_γ[t] = D[˜]_** _[−]_ 2[1] ˜AD[˜] _[−]_ 2[1] X[γ][t]W, β[t] = ˜D[−] [1]2 ˜AD[˜] _[−]_ 2[1] X[β][t]W, (6)

**_γ[t]_** **_β[t]_**

**X[γ][t + 1] = γ[t] + b ·** **_γ[t]_** + _β[t]_ _[,][ X][β][[][t][ + 1] =][ β][[][t][] +][ b][ ·]_ **_γ[t]_** + **_β[t]_** (7)

where X[γ][t] and X[β][t] is the target and background portion of X[t], respectively. The derivation of

_[,]_

**_γ[t] and β[t] is intuitive since graph convolution is a linear operation. Motivated by (Singh et al.,_**
2018), γ[t] and β[t] have to compete for their share of b as in Eq 7. **_γ[t]_** _∈_ R[F]t+1 measures the
dimension-wise magnitude of X[γ][t] after the linear mapping ( **_β[t]_** is defined similarly).

**Fully Connected Layer: A fully connected layer prevalent in the model is shown below:**

**X[t + 1] = X[t]Θ + b,** (8)

where Θ and b denote trainable weights and bias. Structure-wise, it is very similar to the GCN. The
decomposition can be designed as:

**X[γ][t]Θ** **X[β][t]Θ**

**X[γ][t+1] = X[γ][t]Θ+b**

_γ_ _β_ _γ_ _β_

_·_ **X** [t]Θ + **X** [t]Θ **X** [t]Θ + **X** [t]Θ

**ReLU Activation: For the activation operator ReLU, we use the telescoping sum decomposition[,][ X][β][[][t][+1] =][ X][β][[][t][]Θ+][b][·]** _[.][ (9)]_
from Murdoch & Szlam (2017). We update the target term first and then update the background
term by subtracting this from total activation:

**X[γ][t + 1] = ReLU** **X[γ][t]** _, X[β][t + 1] = ReLU_ **X[γ][t] + X[β][t]** _−_ ReLU **X[γ][t]** _._ (10)

**Maxpooling: We track the node indices selected by pooling in both target and background portion. **      

3.4.2 DECOMPOSING GATS

The graph attention layer in GAT is similar to Eq. 5, but uses the attention coefficients αi,j to
aggregate the information (an alternative way to understand Eq. 5 is that αi,j = ( D[˜] _[−]_ [1]2 **A[˜]** **D[˜]** _[−]_ [1]2 )i,j):

exp LeakyReLU **Xi[t]W** **Xj[t]W** **a**
_∥_
_αi,j =_    [] (11)

_k∈Ni∪{i}_ [exp] LeakyReLU **Xi[t]W∥Xk[t]W** **a**
P    [] _[,]_

where represents the concatenation operation. W and a are parameters. Xi[t] denotes the embed_∥_
ding of node i at layer Lt. Ni denotes the neighbors of node i.

Therefore, a graph attention layer can be seen as consisting of four smaller layers: linear mapping,
concatenation, LeakyReLU activation, and softmax operation. Decomposing a linear mapping is as
trivial as decomposing an FC layer. To decompose the concatenation operator:

**Xi[t]∥Xj[t] = X[γ]i** [[][t][]][∥][X]j[γ][[][t][] +][ X]i[β][[][t][]][∥][X]j[β][[][t][]][.] (12)

For LeakyReLU, the idea of decomposition is the same as ReLU. For softmax operation, we split the
coefficients proportionally to the exponential value of the target and the background term of input:

exp **X[γ][t]**
**X[γ][t + 1] = softmax** **X[t]** _,_

_·_ _γ_   _β_

exp **X** [t] + exp **X** [t]

   (13)

   

exp **X[β][t]**
**X[β][t + 1] = softmax** **X[t]** _._

_·_ _β_   _γ_

exp **X** [t] + exp **X** [t]

  

   

Here we employ the similar motivation that used to split bias term in Eq. 7, and let **X[γ][t]** and
**X[β][t]** to compete for the original value. The detail of decomposing the attention coefficients can
be found in Appendix B.


-----

4 SUBGRAPH-LEVEL EXPLANATION VIA AGGLOMERATION

Through decomposition, we could compute the contribution score of any given node groups. However, this is not enough for explaining GNNs. Our goal of explanation is to find the most important
subgraph structure, but it is usually impossible to exhaustively compute and compare the scores of
all possible subgraphs. In this section, we design an agglomeration algorithm to tackle the challenge.

4.1 CONTEXTUAL CONTRIBUTION SCORE

We first introduce a new scoring function to be used in our algorithm. Different from the absolute
_contribution scores provided by decomposition, in many scenarios, we are more interested in the_
_relative contribution of the target compared to its contexts. Let V_ _[γ]_ _⊂V be the target node group,_
and f _[D](·) be the contribution score calculated from decomposition. The relative contribution of V_ _[γ]_
averaged over different contexts is calculated as:


_f_ _[D](V_ _[γ]_ _∪C) −_ _f_ _[D](C)_ _,_ (14)
i


_ϕ(_ ) ≜ E _RW_ ( _L(_ _γ_ ))
_V_ _[γ]_ _C∼_ _N_ _V_


where C is the context around V _[γ], and NL (V_ _[γ]) contains the neighboring nodes of V_ _[γ]_ within Lhops. Here we use a random walk process RW () to sample C within the neighborhood around V _[γ]._
The reason for sampling within the neighborhood is based on the information aggregation, where a
node collects the information from its neighbors within certain hops constrained by the GNN depth.

4.2 SUBGRAPHS CONSTRUCTION VIA AGGLOMERATION

Our agglomeration algorithm initializes from individual nodes and terminates when the whole graph
structure is included. Specifically, the interpretation process constructs a series of intermediate
subgraph sets E = {S1, ..., SI _}, where Si = {B1, ..., BMi_ _} contains Mi subgraphs. At each step,_
the algorithm searches for the candidate node or node group v that most significantly affects the
contribution of subgraph _m, m_ 1, ..., Mi according to the ranking score r(v):
_B_ _∈{_ _}_

_s(v) ≜_ _ϕ_ _{v} ∪Bm_ _−_ _ϕ (Bm), r(v) ≜_ _s(v) −_ Ev[′][ ]s(v[′]) _, s.t. v, v[′]_ _∈N_ (Bm), (15)
   

where ( _m) is the set of neighbor nodes or node groups to_ _m. Here s(v) measures the influence_
_N_ _B_ _B_
of v to _m, while r(v) further revises the value by considering the relative influence of v compared_
_B_
to other candidates v[′]. At the beginning of our algorithm, Bm = ∅. A node v is selected if r(v) ≥
_q_ maxv′ r(v[′]), and we set q = 0.6 in experiments. The selected nodes are merged into subgraphs
_·_
to form _i+1. Small subgraphs will be merged into larger ones, so we have Mi_ _Mj, i_ _j._
The algorithm executes the above steps repeatedly and terminates until all nodes are included (i.e., S _≤_ _≥_
_Mi = 1), or a certain pre-defined step budget is used up. Further details of the algorithm can be_
found in Section C of the Appendix.

5 EXPERIMENTS

5.1 EXPERIMENTAL DATASETS

Following the setting in previous work (Ying et al., 2019), we adopt both synthetic datasets and
real-world datasets. The statistic of all datasets are given in Sec A in the Appendix.

5.1.1 SYNTHETIC DATASETS

-  BA-Shapes. BA-Shapes is a unitary graph based on a 300-node Barab´asi-Albert (BA)
graph (Barab´asi & Albert, 1999). 80 five-node motifs are randomly attached to the base graph.
The motif is a ”house” structured network in which the points are divided into top-nodes, middlenodes, or bottom-nodes. 10% of random edges are attached to perturb the graph.

-  BA-Community. BA-Community dataset is constructed by combining two BA-Shapes graphs.
To distinguish the nodes, the distribution of features of nodes in different communities differs.
There are eight node classes based on the structural roles and community membership.


-----

-  Tree-Cycles. The Tree-Cycles dataset germinates from an eight-level balanced binary tree base
graph. The motif is a six-node cycle. 80 motifs are randomly added to the nodes of the base graph.
The nodes are classified into two classes, i.e., base-nodes and motif-nodes.

-  Tree-Grids. It is constructed in the same way as the Tree-Cycles dataset. The Tree-Grid dataset
has the same base graph while replacing the cycle motif with a 3-by-3 grid motif.

5.1.2 REAL-WORLD DATASETS

-  MUTAG. It is a dataset with 4,337 molecule graphs. Every graph is labeled according to their
mutagenic effect on the bacterium. As discussed in (Debnath et al., 1991), the molecule with
chemical group NH2 or NO2 and carbon rings are known to be mutagenic. Since non-mutagenic
molecules have no explicit motifs, only mutagenic ones are presented during the analysis.

-  Graph-SST2. It is a dataset of 70,042 sentiment graphs, which are converted through Biaffine
parser (Liu et al., 2021). Every graph is labeled according to its sentiment, either positive or
negative. The nodes denote words, and edges denote their relationships. The node features are
initialized as the pre-trained BERT word embeddings (Devlin et al., 2019).

5.2 EXPERIMENTAL SETUP

5.2.1 EVALUATION METRICS

The interpretation problem is formalized as a binary classification problem distinguishing between
important and unimportant structures (nodes or edges, depending on the nature of ground truth). A
good explanation should assign high scores to the important structures and low scores to unimportant
ones. We consider the nodes within the motif to be important for the synthetic dataset and the rest
to be unimportant. In the MUTAG dataset, the ”N-H” and ”N-O” edges are important, and the rest
are unimportant. We conduct quantitative experiments on the synthetic datasets and the MUTAG
dataset, and qualitative experiments on the MUTAG dataset and the Graph-SST2 dataset. We adopt
the Area Under Curve (AUC) to evaluate the performance quantitatively.

5.2.2 BASELINES METHODS AND IMPLEMENTATION DETAILS

**Baselines Methods.** We compare with four baselines methods: GRAD (Ying et al., 2019),
GAT (Veliˇckovi´c et al., 2017), GNNExplainer (Ying et al., 2019) and PGExplainer (Luo et al.,
2020). (1) GRAD computes the gradients of GNN output with respect to the adjacency matrix or
node features. (2) GAT averages the attention coefficients across all graph attention layers as edge
importance. (3) GNNExplainer optimizes a soft mask of edges or node features by maximizing the
mutual information. (4) PGExplainer learns an MLP (Multi-layer Perceptron) model to generate the
mask using the reparameterization trick (Jang et al., 2017).

**Construction of Target Models. We use all synthetic datasets together with the MUTAG dataset for**
quantitative evaluation experiments. We train a GCN and GAT model as the model to be explained
for all datasets following the setup of previous work. Meanwhile, we construct DEGREE(GCN)
and DEGREE(GAT) as the decomposed version for our method. We set the number of GNN layers
to 3 for all datasets, except for the Tree-Grid dataset where it is 4. Since the 3-hop neighbors of
some target nodes has only in-motif nodes (no negative samples). For the qualitative evaluation
experiment, we use the MUTAG dataset and Graph-SST2 dataset. For all the model training, we use
Adam optimizer. All the datasets are divided into train/validation/test sets.

**Explainer Setting. For all baseline methods, we keep the default hyper-parameters. For baselines**
(e.g., PGExplainer) who need training additional modules, we also split the data. We also split data
for baselines requiring additional training (e.g. PGExplainer). We use all nodes in the motif for
evaluation. For explainers that only provide node explanation, we average the score of its vertexes
as edge explanation. The details of explainer settings can be found in Sec A in Appendix.

5.3 QUANTITATIVE EVALUATION

In this section, we introduce experimental results on both synthetic datasets and the MUTAG dataset.
For node classification, the computation graph only contains nodes within l-hop from the target
node, where l is the number of model layer. The reason is that the nodes outside the computation


-----

|Col1|Table 1: Quantitative Experiment Result. Explanation AUC|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|Task|Node Classification||||Graph Classification|
|Dataset|BA-Shapes|BA-Community|Tree-Cycles|Tree-Grid|MUTAG|
|GRAD|0.882|0.750|0.905|0.612|0.787|
|GAT|0.815|0.739|0.824|0.667|0.763|
|GNNExplainer|0.832|0.750|0.862|0.842|0.742|
|PGExplainer|0.963|0.894|0.960|0.907|0.836|
|DEGREE(GCN)|0.991±0.005|0.984±0.005|0.958±0.004|0.925±0.040|0.875±0.028|
|DEGREE(GAT)|0.990±0.008|0.982±0.010|0.919±0.027|0.935±0.031|0.863±0.042|


Time Efficiency(s)

|GNNExplainer|0.65|0.78|0.69|0.72|0.43|
|---|---|---|---|---|---|
|PGExplainer|116.72(0.014)|35.71(0.024)|117.96(0.09)|251.37(0.011)|503.52(0.012)|
|DEGREE(GCN)|0.44|1.02|0.25|0.37|0.83|
|DEGREE(GAT)|1.98|2.44|0.96|1.03|0.79|



Figure 2: The subgraph agglomeration results on MUTAG dataset. The first row shows a correct

DEGREE(GAT) 1.98 2.44 0.96 1.03 0.79

prediction. The second and the third row report two typical examples of errors. Red is mutagenic,
blue is non-mutagenic, gray is not selected. The colored edges link the selected nodes. The process
goes from left to right. The graph on the far left in each row displays the score for individual nodes.

graph will not influence the final prediction. Table 1 shows the explanation AUC and time efficiency
(the training time is shown outside the parentheses for PGExplainer). We have the following key
findings. First, DEGREE achieves SOTA performances in most scenarios, showing its advantages
in faithfulness over baseline methods. Second, the performance of DEGREE on GCN and GAT
models can achieve similar high performance. This observation demonstrates the adaptability of
our approach. Third, the improvement of AUC on BA-Community (˜9%) and MUTAG (˜5%) is
more noticeable, where the two datasets distinguish themselves from others is that their features
are not constants. It thus shows that our explanation method could well handle node features as
they propagate through the graph structure. In terms of efficiency, DEGREE is implemented by
decomposing the built-in forward propagation function, so there is no training process. The time
cost is highly correlated to the complexity of the target model and the input size. We report further
quantitative experiments in Appendix D.

5.4 QUALITATIVE EVALUATION

In this section, we use Graph-SST2 and MUTAG datasets to visualize the explanation and demonstrate the effectiveness of our subgraph agglomeration algorithm in Sec 4.


-----

“Maybe it’s asking too much, but if a movie is truly going to inspire me, I want a little more than this.”

“Though Ford and Neeson capably hold our interest, but it's just not a thrilling movie.”

Figure 3: The subgraph agglomeration results on the Graph-SST2 dataset. The first row shows an
incorrect prediction, the second row shows the correct one. Red is negative, blue is positive.

In the first example, we show three visualizations from the MUTAG dataset in Figure 2. The first
row represents a correctly predicted instance. Our model successfully identifies the ”NO2” motif as
a moderately positive symbol for mutagenicity. The ”H” or the carbon ring is considered a negative
sign for mutagenicity. Once the ”NO2” and the ring join, they become a strong positive symbol for
mutagenicity. This phenomenon is consistent with the knowledge that the carbon rings and ”NO2”
groups tend to be mutagenic (Debnath et al., 1991). We check instances with wrong predictions
and show two representative examples. From the second row in Fig. 2, the GCN model precisely
finds out the ”NH2” motif with the ring motif as a strong mutagenic symbol. But another wandering
part without connection shows a strong non-mutagenic effect, ultimately leading to an incorrect
prediction. The second row shows another typical failure pattern. The model catches the ”NH2” and
part of the carbon ring as a mutagenic symbol, but the ”CH3” on the bottom right shows a strong
non-mutagenic effect. The model erroneously learns a negative interaction between them.

In the second example, we show two visualizations for the Graph-SST2 dataset in Figure 3. The
sentence in the first row is labeled negative, yet its prediction is wrong. Our algorithm can explain
the decision that the GNN model regards first half of the sentence (”Maybe it’s asking too much”)
as negative, the second half (”going to inspire me”, ”want a little more than this”) as positive. But
the model can not tell the subjunctive tone behind the word ”if”, and consequently yields a positive
but incorrect prediction. The sentence in the second row is negative, and the prediction is correct.
Our algorithm precisely identifies the positive part (”Though Ford and Neeson capably hold our
interest”) and the negative part (”but its just not a thrilling movie”). Moreover, it reveals that the
GCN model can correctly learn the transition relationship between these two components.

We observe that our method can detect non-linear interactions between subgraphs throughout the
agglomeration process from above examples. It can help to diagnose the incorrect predictions and
enhance the credibility of the model. More visualizations and efficiency study are in Appendix E.

6 CONCLUSIONS

In this work, we present DEGREE which explains a GNN by decomposing its feedforward propagation process. After summarizing the fundamental rules for designing decomposition based explanation, we propose concrete decomposition schemes for those commonly used layers in GNNs.
We also design an algorithm to provide subgraph-level explanation via agglomeration, which efficiently employs the topological information in graphs. Experimental results show that DEGREE
outperforms baselines in terms of faithfulness and can capture meaningful structures in graph data.


-----

REFERENCES

Federico Baldassarre and Hossein Azizpour. Explainability techniques for graph convolutional networks, 2019.

Albert-L´aszl´o Barab´asi and R´eka Albert. Emergence of scaling in random networks. science, 286
(5439):509–512, 1999.

Lei Cai and Shuiwang Ji. A multi-scale approach for graph link prediction. In Proceedings of the
_AAAI Conference on Artificial Intelligence, volume 34, pp. 3308–3315, 2020._

Sergio Casas, Cole Gulino, Renjie Liao, and Raquel Urtasun. Spatially-aware graph neural networks
for relational behavior forecasting from sensor data. arXiv preprint arXiv:1910.08233, 2019.

Chun-Hao Chang, Elliot Creager, Anna Goldenberg, and David Duvenaud. Explaining image classifiers by counterfactual generation. arXiv preprint arXiv:1807.08024, 2018.

Asim Kumar Debnath, Rosa L Lopez de Compadre, Gargi Debnath, Alan J Shusterman, and Corwin Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. Journal of medicinal
_chemistry, 34(2):786–797, 1991._

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding, 2019.

Mengnan Du, Ninghao Liu, and Xia Hu. Techniques for interpretable machine learning. Communi_cations of the ACM, 63(1):68–77, 2019._

Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural
networks for social recommendation. In The World Wide Web Conference, pp. 417–426, 2019.

Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured
data. arXiv preprint arXiv:1506.05163, 2015.

Qiang Huang, Makoto Yamada, Yuan Tian, Dinesh Singh, Dawei Yin, and Yi Chang.
Graphlime: Local interpretable model explanations for graph neural networks. arXiv preprint
_arXiv:2001.06216, 2020._

Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax, 2017.

Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.

Jian Liang, Bing Bai, Yuren Cao, Kun Bai, and Fei Wang. Adversarial infidelity learning for model
interpretation. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge
_Discovery & Data Mining, pp. 286–296, 2020._

Cheng-Hao Liu, Maksym Korablyov, Stanisław Jastrzebski, Paweł Włodarczyk-Pruszy´nski, Yoshua
Bengio, and Marwin HS Segler. Retrognn: Approximating retrosynthesis by graph neural networks for de novo drug design. arXiv preprint arXiv:2011.13042, 2020.

Meng Liu, Youzhi Luo, Limei Wang, Yaochen Xie, Hao Yuan, Shurui Gui, Haiyang Yu, Zhao Xu,
Jingtun Zhang, Yi Liu, Keqiang Yan, Haoran Liu, Cong Fu, Bora Oztekin, Xuan Zhang, and
Shuiwang Ji. DIG: A turnkey library for diving into graph deep learning research. arXiv preprint
_arXiv:2103.12608, 2021._

Ninghao Liu, Qiaoyu Tan, Yuening Li, Hongxia Yang, Jingren Zhou, and Xia Hu. Is a single vector
enough? exploring node polysemy for network embedding. In Proceedings of the 25th ACM
_SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 932–940, 2019._

Ana Lucic, Maartje ter Hoeve, Gabriele Tolomei, Maarten de Rijke, and Fabrizio Silvestri. Cfgnnexplainer: Counterfactual explanations for graph neural networks, 2021.

Scott Lundberg and Su-In Lee. A unified approach to interpreting model predictions, 2017.


-----

Dongsheng Luo, Wei Cheng, Dongkuan Xu, Wenchao Yu, Bo Zong, Haifeng Chen, and Xiang
Zhang. Parameterized explainer for graph neural network. arXiv preprint arXiv:2011.04573,
2020.

W James Murdoch and Arthur Szlam. Automatic rule extraction from long short term memory
networks. arXiv preprint arXiv:1702.02540, 2017.

W James Murdoch, Peter J Liu, and Bin Yu. Beyond word importance: Contextual decomposition
to extract interactions from lstms. arXiv preprint arXiv:1801.05453, 2018.

Phillip E Pope, Soheil Kolouri, Mohammad Rostami, Charles E Martin, and Heiko Hoffmann. Explainability methods for graph convolutional neural networks. In Proceedings of the IEEE/CVF
_Conference on Computer Vision and Pattern Recognition, pp. 10772–10781, 2019._

Cynthia Rudin. Stop explaining black box machine learning models for high stakes decisions and
use interpretable models instead. Nature Machine Intelligence, 1(5):206–215, 2019.

Thomas Schnake, Oliver Eberle, Jonas Lederer, Shinichi Nakajima, Kristof T. Sch¨utt, Klaus-Robert
M¨uller, and Gr´egoire Montavon. Higher-order explanations of graph neural networks via relevant
walks, 2020.

Robert Schwarzenberg, Marc H¨ubner, David Harbecke, Christoph Alt, and Leonhard Hennig. Layerwise relevance visualization in convolutional text graph classifiers, 2019.

Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through
propagating activation differences. In International Conference on Machine Learning, pp. 3145–
3153. PMLR, 2017.

Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.

Chandan Singh, W James Murdoch, and Bin Yu. Hierarchical interpretations for neural network
predictions. arXiv preprint arXiv:1806.05337, 2018.

Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In
_International Conference on Machine Learning, pp. 3319–3328. PMLR, 2017._

Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.

Minh N Vu and My T Thai. Pgm-explainer: Probabilistic graphical model explanations for graph
neural networks. arXiv preprint arXiv:2010.05788, 2020.

Xiang Wang, Yingxin Wu, An Zhang, Xiangnan He, and Tat seng Chua. Causal screening to
interpret graph neural networks, 2021. [URL https://openreview.net/forum?id=](https://openreview.net/forum?id=nzKv5vxZfge)
[nzKv5vxZfge.](https://openreview.net/forum?id=nzKv5vxZfge)

Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? arXiv preprint arXiv:1810.00826, 2018.

Rex Ying, Dylan Bourgeois, Jiaxuan You, Marinka Zitnik, and Jure Leskovec. Gnnexplainer: Generating explanations for graph neural networks. Advances in neural information processing systems,
32:9240, 2019.

Hao Yuan, Jiliang Tang, Xia Hu, and Shuiwang Ji. Xgnn: Towards model-level explanations of
graph neural networks. In Proceedings of the 26th ACM SIGKDD International Conference on
_Knowledge Discovery & Data Mining, pp. 430–438, 2020._

Hao Yuan, Haiyang Yu, Jie Wang, Kang Li, and Shuiwang Ji. On explainability of graph neural
networks via subgraph explorations. arXiv preprint arXiv:2102.05152, 2021.

Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. arXiv preprint
_arXiv:1802.09691, 2018._


-----

Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning
architecture for graph classification. In Proceedings of the AAAI Conference on Artificial Intelli_gence, volume 32, 2018._

Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li,
and Maosong Sun. Graph neural networks: A review of methods and applications. arXiv preprint
_arXiv:1812.08434, 2018._


-----

A DATASETS AND EXPERIMENTAL SETTING

In this section, we introduce the detail of the datasets as wall as the experimental setting. The code
[can be found at https://anonymous.4open.science/r/DEGREE-3128.](https://anonymous.4open.science/r/DEGREE-3128)

**Model setup: We adopt GCN model and GAT model architecture for corresponding experiments.**
We use GCN(#in channel, #out channel, activation) to denote a GCN layer, and similar notation for GAT layer and fully connected layer.

The structure of GCN model for node classification task is following:
GCN(#feature, 20, ReLU ) _−_ GCN(#feature, 20, ReLU ) _−_ GCN(#feature, 20, ReLU ) _−_
FC(20, 20, ReLU ) − FC(20, #label, Softmax).

For graph classification task, we adopt a Max-pooling layer before the FC layers:
GCN(#feature, 20, ReLU ) _−_ GCN(#feature, 20, ReLU ) _−_ GCN(#feature, 20, ReLU ) _−_
_Maxpooling −_ FC(20, 20, ReLU ) − FC(20, #label, Softmax).

For GAT model architecture, we replace all GCN layers with GAT layer and keep the remaining
setting unchanged. For experiments on Tree-Grid dataset, we adopt 4-layer GCN/GAT model by
adding one more GCN/GAT layer with same setting before FC layers.

**Dataset statistic: The statistics of synthetic datasets and real-world datasets are reported in Table 2.**

Table 2: Statistics of the dataset

Dataset # of nodes # of edges # of graphs # of labels features

BA-Shapes 700 4,110 1 4 Constant
BA-Community 1,400 8,920 1 8 Generated from Labels
Tree-Cycles 871 1,950 1 2 Constant
Tree-Grid 1,231 3,410 1 2 Constant

MUTAG 131,488 266,894 4,337 2 Node Class
Graph-SST2 714,325 1,288,566 70,042 2 BERT Word Embedding

**Experimental setting: For all datasets, we use a train/validation/test split of 80%/10%/10%. For**
all synthetic datasets, the GCN model is trained for 1,000 epochs and the GAT model is trained for
200 epochs. For MUTAG dataset, the GCN and GAT model is trained 30 epochs. For Graph-SST2
dataset, the GCN model is trained 10 epochs. We use Adam optimizer and set the learning rate
to 0.005, the other parameters remain at their default values. We also report the accuracy metric
reached on each dataset in Table 3.

Table 3: Accuracy performance of GNN models

|Dataset|BA-Shapes|BA-Community|Tree-Cycles|Tree-Grid|MUTAG|Graph-SST2|
|---|---|---|---|---|---|---|


|Task|Node Classification|Graph Classification|
|---|---|---|


|Model|GCN GAT|GCN GAT|GCN GAT|GCN GAT|GCN GAT|GCN|
|---|---|---|---|---|---|---|


|Training Validation Testing|0.96 0.98 0.97 0.96 0.93 0.94|0.99 0.83 0.88 0.85 0.87 0.83|0.91 0.93 0.90 0.92 0.89 0.92|0.85 0.83 0.84 0.84 0.81 0.80|0.80 0.81 0.78 0.80 0.77 0.79|0.91 0.90 0.88|
|---|---|---|---|---|---|---|



**Hardware setting: We introduce the hardware that we use for the experiments.**

CPU: AMD EPYC 7282 16-Core Processor.

GPU: GeForce RTX 3090 NVIDIA-SMI: 460.32.03 Driver Version: 460.32.03 CUDA Version:
11.2.


-----

B ATTENTION DECOMPOSITION

To calculate the attention coefficient, we need to first calculate the pre-normalized attention coefficient between node i and node j as:

_α˜i,j = LeakyReLU_ ([Xi[t]W _||Xj[t]W_ ]a)

And we use ˜αi,j to denote a vector which consist of the pre-normalized attention coefficients between node i and its neighbors. Then we calculate the normalized attention coefficient of node i via
_Softmax over its neighbors:_

_αi = Softmax(˜αi)_

We use Softmax(| · |) to measure the dimension-wise magnitude, and let them compete for the
original value. The division between two vectors is element-wise.

C ALGORITHM

We conclude the computation steps of subgraph-level explanation (Sec 4) in Algorithm 1, 2 and 3.

**Algorithm 1: The algorithm of subgraph agglomeration**
**Data: Graph G = (V, E), Label y, Target model f**, Hyperparameter q.
**Result: Explanation tree T** .
**Score Metric Function: ϕ from Algorithm 3.**
**Initialization: score queue ScoresQ ←** ∅, explanation tree T ← ∅.
**for v ∈V do**

_ScoresQ.add_ _{v}, priority = ϕ(f, y, {v})_

**end**

  

**while ScoresQ is not empty do**

Select Base Subgraph Set B = ScoresQ.top(q)
T.add(B)
_ScoresQ ←_ ∅
**for BCandidate Subgroup Setm ∈B do** = Get Candidate(, _m) with Algorithm 2_

_C_ _G_ _B_
**for c ∈C do**

_ScoresQ.add(c, priority = ϕ(f, y, c)_ _ϕ(f, y,_ _m))_
_−_ _B_
**end**
**end**
_ScoresQ = |ScoresQ −_ E(ScoresQ)|
**end**
**return T**


**Algorithm 2: The algorithm of candidate node set selection**
**Data: Graph G = (V, E), Subgraph Bm.**
**Result: Candidate Subgraph Set C.**
_C ←_ ∅
Neighbour nodes _n¯_ _e =< n, ¯n >, e_ _, n_ _m, ¯n_ _m_
_N ←{_ _|_ _∈E_ _∈B_ _∈V \ B_ _}_
**for v ∈N do**

.add( _m_ _v_ )
_C_ _B_ _∪{_ _}_
**end**
**return C**


-----

**Algorithm 3: The algorithm of score computation ϕ**
**Data: Graph G = (V, E), model f**, NodeSet N
**Result: score ϕ(f, y, N** )
Sample a context set by Random Walk within the L-hop neighbor region of .
_S_ 1 _N_
**Return: ϕ(f** _[D], y, N_ ) = _|S|_ _s∈S_ [(][f] [(][N ∪] _[s][)][ −]_ _[f]_ [(][s][))]

P

D EFFICIENCY STUDY

DEGREE is achieved by decomposing the feedforward process of the target model. Thus the efficiency is highly dependent on the model structure. We report the statistic of time consuming on
each dataset for GCN and GAT model in Table 4.

We quantified the relationship between the size of the calculation graph and the time taken. The
result is reported in Figure 4

Figure 4: The quantitative studies of efficiency for different datasets and models. For the synthetic
datasets, the horizontal coordinate represents the number of nodes in computation graph. For the
MUTAG dataset, the horizontal coordinate represents the number of edges in computation graph.

Table 4: Efficiency performance.

|Dataset|BA-Shapes|BA-Community|Tree-Cycles|Tree-Grid|MUTAG|
|---|---|---|---|---|---|


|Model|GCN GAT|GCN GAT|GCN GAT|GCN GAT|GCN GAT|
|---|---|---|---|---|---|


|Avg. Time (s)|0.44 1.98|1.02 2.44|0.25 0.96|0.37 1.03|0.83 0.79|
|---|---|---|---|---|---|



E QUALITATIVE EXPERIMENTS EXAMPLES

In this section, we report more qualitative evaluation results on the Graph-SST2 and the MUTAG
datasets. The results are reported in Figure 5 and 6.We also investigate the time efficiency of our
agglomeration algorithm in terms of the relationship between q, the node number and the time spent.

We also investigate the time efficiency of our agglomeration algorithm in terms of the relationship
between q, the node number and the time spent.

F ADDITIONAL EXPERIMENTS FOR REBUTTAL

F.1 QUANTITATIVE EVALUATION

In this section, we perform additional experiments comparing DEGREE with GNN-LRP Schnake
et al. (2020) and SubgraphX Yuan et al. (2021). We use the MUTAG dataset and the Graph-SST2
dataset, as presented in Sec 5.1. The target model is the same as that introduced in Sec 5.2.2. Note
that we modify our method to search only for nodes that boost the score of the class of interest.
We employ the ACC Liang et al. (2020) as an evaluation metric. The ACC reflects the consistency
of predictions based on the whole graph and between interpreted subgraphs. Thus, ACC does not


-----

(a) Well, it probably won't have you swinging from the trees hooting it's praises, but it's definitely worth taking a look.

(b) Trouble every day is a success in some sense, but it's hard to like a film so cold and dead.

(c) Makes an unusual but pleasantly haunting debut behind the camera.

(d) Not everything in this ambitious comic escapade works, but Coppola, along with his sister, Sofia, is a real

filmmaker

Figure 5: The subgraph agglomeration results on the Graph-SST2 dataset with a GCN graph classifier. All instances all correctly predicted. Red is negative and blue is positive.

require ground truth label. We further define the sparsity as the ratio of the size of the explanation
subgraph to the original graph. At the same sparsity, the higher the ACC, the better the interpretation.
Figure 8 shows the ACC of DEGREE, GNN-LRP and SubgraphX under various sparsity. We can
find that DEGREE has competitive performance compared to GNN-LRP and SubgraphX. Besides,
DEGREE has better time efficiency.

F.2 QUALITATIVE COMPARISON

In this section we make a qualitative comparison between DEGREE and SubgraphX. We randomly
select a number of similar molecules and visualize the explanations generated by DEGREE and
SubgraphX. We report them in the Figure 9. We can find that none of the subgraphs generated by
SubgraphX include the ’N-H’ or ’N-O’. They only select the carbon ring as the important part. In
contrast, DEGREE can precisely indicate that the mutagenicity is caused by the ’N-H’ or ’N-O’.

F.3 FORWARD-LOOKING EXPERIMENT

In this section, we present a simple prospective experiment from the early stages of this work. The
dataset was generated by modifying the MUTAG dataset by selecting half of the graphs in the dataset
and picking a node at random in each graph, giving it a special feature value of 1 while giving the
other nodes a background white noise feature. Our task is to predict whether a graph contains special
nodes or not. We train a 3-layer GCN which achieves 100% accuracy. We then use DEGREE to
calculate the contribution score for each node. DEGREE is able to locate special nodes with 100%
accuracy. Figure 10 shows the visualisation.


-----

Figure 6: The subgraph agglomeration results on MUTAG dataset with GCN graph classifier. All
instances are incorrectly predicted. Red is mutagenic, blue is non-mutagenic, gray is not selected.
The colored edges link the selected nodes.


-----

Figure 7: Relationship between the time efficiency(s), graph size and q on the MUTAG dataset.

Figure 8: ACC of DEGREE, GNN-LRP and SubgraphX on MUTAG and Graph-SST2.


-----

Figure 9: Qualitative comparison of DEGREE and SubgraphX. The first row shows the interpretation generated by SubgraphX. The second row is generated by DEGREE. The red color indicates
mutagenicity.

Figure 10: Visualization of the forward-looking experiment. DEGREE can locate the special node
with 100% accuracy.


-----

