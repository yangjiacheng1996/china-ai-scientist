# THE EVOLUTION OF UNCERTAINTY
## OF LEARNING IN GAMES


**Yun Kuen Cheung[∗]**
Royal Holloway
University of London


**Georgios Piliouras[∗]**
Singapore University of
Technology and Design

ABSTRACT


**Yixin Tao[∗]**
London School of
Economics


Learning-in-games has become an object of intense interest for ML due to its
connections to numerous AI architectures. We study standard online learning in
games but from a non-standard perspective. Instead of studying the behavior of a
single initial condition and whether it converges to equilibrium or not, we study the
behavior of a probability distribution/measure over a set of initial conditions. This
initial uncertainty is well motivated both from a standard game-theoretic perspective (e.g. a modeler’s uncertainty about the agents’ initial beliefs, random external
signals) as well as from a ML one (e.g. noisy measurements, system initialization
from a dataset distribution). Despite this, little is formally known about whether
and under what conditions uncertainty is amplified or reduced in these systems.
We use the popular measure of differential entropy to quantify the evolution of
uncertainty. We find that such analysis shares an intimate relationship with volume analysis, a technique which was recently used to demonstrate the occurrence
of Lyapunov chaos when using Multiplicative Weights Update (MWU) or Followthe-Regularized-Leader (FTRL) algorithms in zero-sum games. This allows us to
show that the differential entropy of these learning-in-game systems increases linearly with time, formalizing their increased unpredictability over time. We showcase the power of the framework by applying it in the study of multiple related
systems, including different standard online optimization algorithms in numerous
games and dynamics of evolutionary game theory.

1 INTRODUCTION

A primary goal of ML research is to understand the behavior of learning algorithms in various
settings. One standard approach is to determine from each initial condition whether a learning algorithm converges to a local optimum or stable state. Yet, in the context of online learning in games,
and more generally in distributed multi-agent learning, it is natural to consider the evolution of a
probability distribution over initial conditions instead. In these settings, each agent forms an initial
belief on her own, and she typically does not reveal her belief to other agents or external modelers/analysts. For a modeler to understand the possible behaviors of the system while being uncertain
of the agents’ beliefs, one natural approach is to infer how the initialization distributes by using data
of observations from the past. The modeler then uses this inference to predict the likelihoods of
different outcomes in the future, either by simulation or by analysis. Also, random initialization can
happen due to random external signals, e.g. weather, as well as noisy measurements. For readers
who wish to learn more mathematical aspects and intuition behind of such models, see Appendix A.
In such cases, it is critical to understand whether the initial probability distribution evolves toward stability, or if its uncertainty gets amplified. Such analysis provides insight into the learning
system’s robustness against random initialization and environmental perturbations. If a system coordinator desires stability but the analysis shows the uncertainty gets amplified, she ought to coordinate
with the agents to make changes, e.g. encourage them to use other online learning algorithms.
To analyze how uncertainty evolves, we need a measure of it. A natural choice is entropy.
In his seminal work in 1948, Claude Shannon formulated an axiomatic foundation of information theory and introduced the seminal definition of Shannon entropy (SE): given a discrete random variable with possible outcomes x1, . . ., xn which occur with probability p1, . . ., pn, its SE is

_∗E-mails: yunkuen.cheung@rhul.ac.uk, georgios@sutd.edu.sg, Y.Tao16@lse.ac.uk_


-----

_−_ [P]i[n]=1 _[p][i][ log][ p][i][ =][ E][ [log(1][/p][i][)]][. Entropy is the canonical measure of uncertainty: when][ p][i][ = 1]_
for some i, the distribution is considered certain and its entropy is zero; uniform distribution is
viewed as the most uncertain, and it attains the maximum possible entropy of log n. For continuous
random variables with probability density function g, Shannon proposed an extension of SE called
the differential entropy (DE), defined as E [log(1/g(x))] = − _X_ _[g][(][x][) log][ g][(][x][)][ d][x][, where][ X][ is the]_

support set of the random variable. We will analyze how DE evolves in multi-agent learning.

R

**Our Contributions.** In our model, a learning-in-game system starts with a distribution over a
set of initial conditions in the cumulative payoff space, a coordinate system which is inherent in
the implementations of many online learning algorithms. The initial distribution evolves over time
depending on the combination of agents’ learning algorithms and the underlying game. We focus on
two popular learning algorithms, Multiplicative Weights Update (MWU) and its optimistic variant
(OMWU).[1] The game settings include the standard two-player matrix games, and one-population
games which are fundamental in biological evolution models. We show that the DE of a broad range
of learning-in-game systems increases linearly with time (up to a certain limit), formalizing their
increased unpredictability. Such systems include MWU in zero-sum games, OMWU in coordination
games, and MWU in certain one-population games which reward more to intra-species play than to
inter-species play. Our results apply to any initial distribution of absolutely continuous random
vectors with finite differential entropy.
At this point one may naturally wonder: What level of uncertainty does a linear increase of DE
indicate? What are its implications? To answer these questions, it is best to compare ourselves
against other simple benchmarks. Consider the following simple learning system: in each round,
the payoffs for each action are generated from a uniform distribution over [−1, 1]. In the cumulative payoff space, the distribution at time T converges to a multivariate Gaussian distribution with
variance Θ(T ), and hence the entropy grows at a rate of O(log T ), much slower than linear growth.
In Section 4.1, we present an implication of linear DE growth in learning-in-game systems. Briefly
speaking, the DE cannot increase substantially for an indefinite amount of time in such systems.
Thus, the distribution after a sufficiently long time must concentrate in a region that yields slower
DE growth or decline; such region does not cover any point in the cumulative payoff space that corresponds to an interior Nash equilibrium. We will refer to this phenomenon by “the grand escape”.
See Theorem 4.5 for the formal description of the grand escape phenomenon; an additional assumption, namely the initial distribution has bounded support, is needed to establish this theorem. For the
implications in information theory, we refer readers to Chapter 8 of Cover & Thomas (2006).
The central tool in analyzing the changes of DE is the Jacobian matrix of our multi-agent dynamical system. This is also the key notion in volume analysis, which was used in a recent series
of papers to demonstrate Lyapunov chaos in learning-in-game systems. (Cheung & Piliouras (2019;
2020); Cheung & Tao (2020)) By showing that the determinant of the Jacobian matrix is strictly
above 1 in a large domain of the cumulative payoff space, they showed that the volume (Lebesgue
measure) of a small set of initial conditions will blow up exponentially to become a set of large
diameter, thus demonstrating a general Lyapunov chaos phenomenon in learning-in-game. Indeed,
the same property of Jacobian matrix guarantees linear increase of DE. Additionally, we present the
first Jacobian analysis of online learning in one-population games. Finally, our DE analysis is robust
against small perturbations. Consequently, our results extend to the setting where the game in each
round can be different, as long as the payoffs are perturbations of each other. Such settings capture
many games in the real world, where we know the “reference values” of the payoffs, but the accurate
payoffs in each round differ slightly due to unknown external (random or adversarial) factors.
Our model and analysis can be viewed as an a strengthening of volume analysis. Volume analysis
focuses on whether it is possible to reach a state or not, whereas in our model we are also concerned
with how probable/likely it is to reach a state. More explicitly, a state can be reached but only
with a tiny chance. When studying chaos and volume, such states matter, but less so when studying
uncertainty as their contributions to entropy will be low. To illuminate this, we simulate MWU (with
step-size 0.01) in Matching Pennies game, and present a few plots in Figure 2. The top-left plot
shows the initial set, which is a small square around the unique Nash equilibrium. The top-right plot

1As we shall point out in Section 3, all our results about MWU also extend easily to the broad family
of Follow-the-Regularized-Leader (FTRL) algorithms, which is a generalization of MWU. For clarity of our
presentation, we choose to focus on MWU in this paper.


-----

1.00

0.75

0.50

0.25

0.00

0.25

0.50

0.75

1.00

1.00

0.75

0.50

0.25

0.00

0.25

0.50

0.75

1.00


1.00

0.75

0.50

0.25

0.00

0.25

0.50

0.75

1.00


1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00

1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00


1.00 0.75 0.50 0.25 0.00 0.25 0.50 0.75 1.00

0.58

0.55

0.52

0.49

0.46




0.75

0.50

0.25

0.00

0.750.500.25 0.500.25

0.75

0.000.250.500.75


Figure 1: “Possible” vs. “Probable”: In the heat-map, red (warm) and blue (cool) colors represent
high and low densities respectively.

shows the range of possibility after 40000 steps[2]. In our model, we assume the initial distribution
is uniform over the small square, and plot the heat-map of probability densities after 40000 steps
(bottom-left). We observe that the states in the boundary of the vortex are more probable to occur,
while the densities around the Nash equilibrium decline. The bottom-right plot shows the densities
that generate the heat-map.

**Further Related Work.** Learning models that explicitly study the effects random initialization
have received relatively little attention in game theory where static equilibrium concepts are typically
the preferred solution. Lahkar & Seymour (2013) studies online learning under random initialization in population dynamics, where one population game is a common payoff model. In population
dynamics, there exists a large population of agents who have different initial choices of mixed strategies, modeled as a distribution. The dynamics proceed by randomly pairing up agents in each round
to play a game. Lahkar & Seymour (2014) extends this framework to other learning dynamics to
describe the evolution of a distribution of mixed strategies in a population playing variants of RockPaper-Scissors games establishing local convergence to the interior equilibrium. Recently, further
learning models inspired by Q-learning dynamics have been defined (Hu et al., 2019). Very little is
formally known about the behavior of such models particularly under instability.
For zero-sum games with an interior Nash equilibrium, “the grand escape” phenomenon follows
from Bailey & Piliouras (2018); Cheung (2018), who showed that MWU diverges away from Nash.
Our results are more general, since their analysis only works with those zero-sum game dynamics,
while our technique applies to many other game dynamics as well. Such instability results in zerosum games are undesirable for ML applications such as GANs thus a lot of effort has been invested
in developing algorithms that achieve convergence Daskalakis et al. (2017); Mertikopoulos et al.
(2019); Daskalakis & Panageas (2018); Gidel et al. (2019); Mokhtari et al. (2020); Perolat et al.
(2021). Our research direction here is in some sense orthogonal. We aim to characterize in a more
detailed fashion the behavior of learning dynamics despite their instability.
The instability of MWU, FTRL and other optimization-driven dynamics in games has attracted
a lot of attention in recent years. One formal technique to establish the unpredictability of MWU

2This phenomenon was first discussed in Cheung & Piliouras (2019) and called the “von Neumann vortex”.


-----

and other dynamics is based on proving Li-Yorke chaos, which has been established robustly in different families of routing/potential/Cournot games (Palaiopanos et al. (2017); Chotibut et al. (2020);
Bielawski et al. (2021); Cheung et al. (2021)). The techniques in those papers are independent from
ours as they require the identification of one dimensional invariant manifolds, and Li-Yorke chaos
implies the existence of periodic orbits, which is not possible in our systems. In a very recent series
of works, Flokas et al. (2020); Giannou et al. (2021) established that all (even partially) mixed equilibria in all games are not asymptotically stable for any choice for FTRL dynamics both in discrete
as well as in the idealized continuous-time limit. In fact, it is possible to construct simple matrix
games such that the orbits of the resulting dynamics can be arbitrarily complex Andrade et al. (2021).
If, inspired by ML applications, we allow for more complex differentiable games it becomes even
simpler to establish strong instability results for effectively any type of training algorithms (Letcher
(2020); Balduzzi et al. (2020)). The sweeping nature of these strong negative results showcases the
practical interest in uncovering more information about the nature of unstable dynamics of games.
Our proposed framework introduces a novel, quantitative methodology for their study.
The notion of differential entropy (DE) has been used as uncertainty measure in many works
across multiple disciplines. We note that while the extended definition of DE seems natural, it is
not considered as the perfect generalization of SE since it misses some favorable properties, and
therefore, as quoted from the popular information theory text of Cover & Thomas (2006), “there
is need for some care in using the concept”.[3] Nevertheless, DE remains an effective measure of
uncertainty[4], which is commonly used in physics, economics, control theory, engineering, biology,
medical research and beyond. For a general discussion, we refer readers to the text of Cover &
Thomas (2006). For background information of evolutionary game theory, we recommend Hofbauer
& Sigmund (1998); Sandholm (2010); Vega-Redondo (1996).

2 MODEL

We use bold small letters to denote vectors, and bold capital letters to denote matrices. Let ∆[d] denote
the probability simplex of dimension d: ∆[d] := {x ∈ R[d] : for each j, xj ≥ 0, and _j=1_ _[x][j][ = 1][}][.]_

**MWU and OMWU.** When an agent uses MWU, she wants to choose among d 2 pure actions
_≥[P][d]_
based on the past cumulative payoffs to these actions. The process starts at discrete time t = 0,
where the agent chooses an initial cumulative payoff vector p[0] _∈_ R[d]. At any time t ≥ 1, upon
receiving the payoffs to the each of the d actions at time t − 1, which are denoted by a vector
**r[t][−][1]** _∈_ R[d], the agent updates the cumulative payoff vector by the rule
_j = 1, 2, . . ., d,_ _p[t]j_ _j_ + ϵ _rj[t][−][1],_ (1)
_∀_ _[←]_ _[p][t][−][1]_ _·_
where ϵ > 0 is the step-size of the update. At any time t ≥ 0, the agent chooses randomly among the
_d pure actions depending on the value of p[t]. Precisely, the agent chooses a mixed strategy x[t]_ _∈_ ∆[d]
which is a function of p[t].For MWU, x[t] is determined by the rule


_∀j = 1, 2, . . ., d,_ _x[t]j_ _[←]_ [exp(][p]j[t] [)][/]


exp(p[t]k[)]
_k=1_

X


(2)


For OMWU, we start with p[0] = p[1], and for t ≥ 2,
_j = 1, 2, . . ., d,_ _p[t]j_ _j_ + ϵ (2rj[t][−][1] _rj[t][−][2]),_ (3)
_∀_ _[←]_ _[p][t][−][1]_ _·_ _−_

and x[t] is determined by the same rule (2).
In general, the payoff vectors r[t][−][1] can be arbitrary, and they may or may not depend on the
mixed strategy x[t][−][1]. However, in the context of learning-in-game, the underlying game generates
the payoff vectors that depend on the mixed strategies of the agents involved. We discuss two popular
game models relevant to our work, namely two-player matrix games and one-population games.

3Two issues with DE are: (1) DE can be negative and indeed it has no finite lower bound, while SE is always
positive; (2) DE depends on the choice of coordinate system. The positivity of SE is favorable in information
theory where they want to measure information contents of different communications. But for the purpose of
measuring and comparing uncertainty, positivity does not seem relevant. (2) is not an issue for us, since we will
always stick to a fixed coordinate system which is natural in learning-in-games.
4DE captures uncertainty well, e.g. for all popular distributions (Gaussian, uniform, exponential, Rayleigh
etc), their DE increases with their variances. Also, among all distributions over a bounded support set, the
uniform distribution over the support set attains maximum DE.


-----

**Two-Player Matrix Games.** In a two-player matrix game, Player 1 has n actions and Player 2
has m actions. The game is specified by two matrices A, B ∈ [−1, 1][n][×][m]. When Player 1 chooses
action j and Player 2 chooses action k, Ajk, Bjk are the payoffs to Players 1 and 2 respectively.
When the players choose mixed strategies, the payoffs are extended via expectation. Denote the
mixed strategies of Players 1 and 2 by x ∈ ∆[n] and y ∈ ∆[m]. The payoff to action j of Player 1 is

[Ay]j, while the payoff to action k of Player 2 is [B[T]x]k. When both players use MWU or OMWU
to play this game repeatedly, we have the following discrete-time dynamical system, where p[t], q[t]
denote the cumulative payoff vectors at time t of Players 1 and 2 respectively. The two players
choose initial cumulative payoff vectors p[0], q[0].

_∀t ≥_ 1, **p[t]** _←_ **p[t][−][1]** + ϵ · Ay[t][−][1];

**q[t]** _←_ **q[t][−][1]** + ϵ · B[T]x[t][−][1]. (4)

Note that x[t][−][1] is a function of p[t][−][1] as specified in (2); similarly, y[t][−][1] is a function of q[t][−][1]. Thus,
we can also view (4) as an iterated function that maps from (p[t][−][1], q[t][−][1]) to (p[t], q[t]).
In the above setting, the game (A, B) is the same at all times t. It is also interesting to consider
scenarios where the game varies in each round. Our results extend to settings where there is a
“reference game” (A, B), but at each round the actual game being played is a perturbation of the
reference game. Precisely, the game at time t is (A[t], B[t]) = (A, B) + (∆[t]A[,][ ∆][t]B[)][, where][ ∆][t]A[,][ ∆][t]B
are matrices with maximum absolute entry at most β, for some small β > 0.

**One-Population Games.** One-population games is a fundamental model in mathematical biology,
which is widely used to explain the evolution of a population of different of species over time. An
one-population game is similar to a two-player game, but now Player 1 is playing against herself;
there is only one player (or in biology term, one population) in this game, while a mixed strategy
**x[t]** represents the fraction of different species among the population. An one-population game is
specified by a matrix A ∈ R[n][×][n]. When the mixed strategy of the player is x ∈ ∆[n], the payoff to
action j is [Ax]j. The resultant discrete-time dynamical system with MWU is

_∀t ≥_ 1, **p[t]** _←_ **p[t][−][1]** + ϵ · Ax[t][−][1], (5)

where x[t][−][1] is a function of p[t][−][1] as in (2).

**Random Initial Cumulative Payoff Vectors.** In Section 1, we explained why we consider uncertainty of the initial cumulative payoff vectors. This can be captured by a model where these vectors
follow a distribution. We use two-player game for discussion below; for one-population game it is
similar. Assume that the initial vectors (p[0], q[0]) follow a joint probability distribution in the domain
R[n] _× R[m]. As the initial vectors are random and they are updated according to the discrete-time_
dynamical system (4), for any t ≥ 0, (p[t], q[t]) also follows a distribution, which is the main object we analyze in this work. We are interested in knowing certain statistics of uncertainty of this
distribution, say differential entropy, which is the subject of the next section.

3 DIFFERENTIAL ENTROPY

In this section, we discuss how changes of differential entropy (DE) can be analyzed in general, and
provide some intuition of the relationship between DE and volume. To proceed, we need a crucial
notion called the Jacobian matrix. Let f : R[d] _→_ R[d] be a smooth function. Its Jacobian matrix is


_∂f1_ _∂f1_ _∂f1_
_∂x1_ _∂x2_ _∂xd_

_· · ·_
. . .
.. .. ... ..
_∂fd_ _∂fd_ _∂fd_
_∂x1_ _∂x2_ _∂xd_

_· · ·_


**Jf =** .. .. ... .. _._

_∂fd_ _∂fd_ _∂fd_

 _∂x1_ _∂x2_ _· · ·_ _∂xd_ 
 

We also need multivariate integration by substitution; see Appendix B for a discussion of it.
Let X be an absolutely continuous random variable in R[d]. Let g be its probability density function, whose support set is X . The DE of X is h(X) := − _X_ _[g][(][x][) log][ g][(][x][)][ d][x][, which we assume]_

to be finite. For any function f : R[d] R[d], f (X) is another random variable; we denote its prob_→_ R
ability density function by ˆg, and its support set by Y. If f is a diffeomorphism, using integration


**Jf =**


-----

by substitution, we have ˆg(y) = g(f _[−][1](y)) ·_ det Jf −1 (y) . Using integration by substitution again,
the DE of f (X) can be computed as follows:

_h(f_ (X)) = − _gˆ(y) log ˆg(y)dy_
ZY

= − _g(f_ _[−][1](y)) ·_ det Jf −1 (y) _· log_ _g(f_ _[−][1](y)) ·_ det Jf −1 (y) dy
ZY

= − _g(x) ·_ det Jf −1 (f (x)) _· log_ _g (x) ·_ det Jf −1 (f (x)) _· |det[]_ **Jf** (x)| dx. (6)
ZX

By the Inverse Function Theorem, det Jf −1 (f (x )) _· |det Jf_ (x)| = 1. Thus,[]

_h(f_ (X)) _h(X) =_ _g(x) log_ det Jf (x) dx (7)
_−_ _|_ _|_
ZX

when the integral on the RHS is finite. This is true if det Jf (x) is a differentiable and bounded
_|_ _|_
function of x, which holds in all learning-in-game systems we study. Thus, the Jacobian’s determinant directly affects the change of DE. If there exists α > 0 such that det Jf (x) 1 + α for
_|_ _| ≥_
all x ∈X, then h(f (X)) − _h(X) ≥_ log(1 + α) > 0. In order to use (7) to analyze the changes
of DE of learning in two-player matrix games, we view the dynamical system (4) as a function that
maps from (p[t][−][1], q[t][−][1]) to (p[t], q[t]) in each round t ≥ 1. If the change of DE per round is at least
log(1 + α) > 0, then the DE increases linearly with time.

**The Relationship Between DE and Volume.** Before discussing the relationship, we first recap
what is volume. Let X ⊂ R[d] be a measurable set with positive Lebesgue measure (which is sometimes called the volume of the set), and let f : R[d] _→_ R[d] be a diffeomorphism. The volume of f (X )
is

at leastX _[|][det] (1+[ J][f]α[(])[x]. If this occurs in a dynamical system every round, the volume of the initial set grows[)][|][ d][x][. When][ |][det][ J][f]_ [(][x][)][| ≥] [1 +][ α][ for all][ x][ ∈X] [, the volume increases by a factor of]

R

exponentially with time, and hence also its diameter, establishing that Lyapunov chaos occurs.
The occurrence of the Jacobian in both the formulae for computing volume and the change of
DE in (7) is not a coincidence, but with a simple intuition behind. The Jacobian yields a linear
approximation of f locally: for any x ∈ R[d] and any small perturbation vector ∆x ∈ R[d], f (x +
∆x) _f_ (x) + Jf (x) ∆x. We consider a tiny hypercube with volume v around x, and its image
_≈_ _·_
under the function f . By the linear approximation, the image is approximately a parallelotope
(the high-dimensional analogue of parallelogram), whose volume is known to be det Jf (x) _v. By_
_|_ _| ·_
partitioning X into infinitesimal tiny hypercubes and summing up, this is intuitively how the integral
formula for computing volume is obtained. Next, suppose the probability density at x is g(x). The
probability of the tiny hypercube is approximately g(x) · v. After the transformation f, this amount
of probability is spread approximately uniformly in the parallelotope, thus the new density at f (x)
is g(x)/ det Jf (x) . If the volume increases locally at x, i.e. det Jf (x) _> 1, it intuitively means_
_|_ _|_ _|_ _|_
the uncertainty level increases locally. This intuition is captured by DE: the contribution to DE by
the tiny hypercube isg(x) _v log_ detg( Jxf)· (vx −) _g(x) · v log(g(x) · v), while the contribution to DE by the parallelotope is_
_−_ _·_ _|_ _|_ [. Thus, the local change of DE is][ g][(][x][)] _[·]_ _[v][ log][ |][det][ J][f]_ [(][x][)][|][, which has the same]

sign as ( det Jf (x) 1). To summarize, local volume increase is equivalent local DE increase.
_|_ _| −_
This equivalence allows us to spot that for those learning systems in two-player matrix games
studied in the series of volume analysis papers of Cheung & Piliouras (2019; 2020); Cheung &
Tao (2020), their DE with our model increases linearly in a large domain of the cumulative payoff
space. We present this result formally Section 4.1, and present an interesting consequence of the DE
growth. Partly motivated by Lahkar & Seymour (2013), we also transfer the techniques to analyzing
one-population games in Section 4.2, and we present two sufficient conditions on these games that
leads to linear DE growth with MWU.
As first spotted by Cheung & Piliouras (2019), these results not only cover MWU and OMWU,
but also the broad family of FTRL algorithms, since the properties of the Jacobian for FTRL is very
similar to that of MWU. (This is not surprising since FTRL is a generalization of MWU.)

4 APPLICATIONS

In this section, we consider two applications: MWU in two-player zero-sum games and onepopulation games. In Appendix C, we also consider OMWU dynamics in two-player coordination


-----

games and one-population games, and the settings where the game in each round is perturbed. A
two-player game (A, B) is zero-sum if A = −B. We focus on non-trivial games, defined below.

**Definition 4.1 (Cheung & Piliouras (2019)) A zero-sum game (A, −A) is trivial if there exists**
_real numbers a1, a2, · · ·, an and b1, b2, · · ·, bm such that Ajk = aj + bk. The same definition_
_applies for one-population game A._

Cheung & Piliouras pointed out that a trivial game is not interesting as the players have dominant
strategies. They provided a measure of distance of a zero-sum game (A, −A) from triviality by

_c(A) =_ min (8)
_a1,_ _,an,b1,_ _,bm_ [[max(][A][jk][ −] _[a][j][ −]_ _[b][k][)][ −]_ [min(][A][jk][ −] _[a][j][ −]_ _[b][k][)]]_

_···_ _···_

4.1 MWU IN TWO-PLAYER ZERO-SUM GAME

Recall from (4) that in zero-sum game (A, −A), MWU dynamic in the cumulative payoff space is

**p[t][+1]** = p[t] + ϵ · Ay(q[t]) and **q[t][+1]** = q[t] + ϵ · (−A)[⊤]x(p[t])

where x(p[t]) and y(q[t]) are the mixed strategies of two players at time t: _xj(p[t])_ =
exp(p[t]j[)][/][ P]j[′][ exp(][p]j[t] _[′]_ [)][ and][ y][k][(][q][t][) = exp(][q]k[t] [)][/][ P]k[′][ exp(][q]k[t] _[′]_ [)][. Two different regions are consid-]

ered: the interior region S1 := {(p, q) | x(p) ≥ _δ and y(q) ≥_ _δ}, and the boundary region S2,_
which is the complement to S1. Note that if the game possesses an interior Nash equilibrium, then
the equilibrium lies in S1 for any sufficiently small δ.
Let g[t](p, q) denote the probability density of (p, q) at round t. Recall from 7 that the change
of entropy is _g[t](p, q) log_ det Jf (p, q) d(p, q). If g[t](p, q) is fully supported by S1, then by
_|_ _|_
using the Jacobian analysis of Cheung & Piliouras (2019), who showed that the determinant of the
Jacobian matrix is strictly larger thanR 1 in S1, we show the differential entropy increases linearly.

**Theorem 4.2 For any non-trivial two-player zero-sum game (A, −A), with MWU dynamics of**
_step-size ϵ_ min 1/(32n[2]m[2]), δ[2]c(A)[2]/8 _, if g[t](p, q) > 0 only for (p, q)_ _S1, then the differ-_
_ential entropy will increase by at least ≤_ _{_ 1+δ[2]δc[2](cA}(A)[2])·[2]ϵ[2]ϵ/[2]8/8 _[in round][ t][.]_ _∈_

_·_

**Proof:** For MWU dynamics, f (p, q) = (p + _ϵAy(q), q_ _−_ _ϵA[⊤]x(p)) by (2). Cheung & Piliouras_
(2019) showed that if ϵ min 1/(32n[2]m[2]), δ[2]c(A)[2]/8, then det Jf (p, q) 1 + _[δ][2][c][(]8[A][)][2]_ _ϵ[2]_
_≤_ _{_ _}_ _≥_ _·_

for any (p, q) _S1, where c(A) is the measure of the distance of a zero-sum game (A,_ **A)**
_∈_ _r_ _−_
from triviality (8). As log(1 + r) ≥ 1+r [for any][ r >][ −][1][,] _g[t](p, q) log |det Jf_ (p, q)| d(p, q) ≥

1+δ[2]δc[2](cA(A)[2])·[2]ϵ[2]ϵ/[2]8/8 _[.]_ R □

_·_

Actually, the assumption that g[t](p, q) is fully supported in S1 can be weakened. We show that
if a sufficiently large probability lies in S1, then the differential entropy still increases linearly. With
this result, we can show Theorem 4.5 below, which states that there exists infinitely many rounds,
such that in each of these rounds t, the probability of (p[t], q[t]) ∈ _S2 will be at least some constant._

**Theorem 4.3 If** _S1_ _[g][(][p][,][ q][)][ d][(][p][,][ q][)][ ≥]_ _[λ][, then the entropy will increase by at least]_ 1+δ[2]δc[2](cA(A)[2])·[2]ϵ[2]ϵ/[2]8/8

_λ −_ 1−ϵ[3]ϵ[3][ ·][ (1][ −] _[λ]R[)][ in round][ t][, which is strictly positive whenever][ λ >]_ _δ[2]c16(Aϵ_ )[2][ .] _·_ _[·]_

**Proof:** Cheung & Piliouras (2019) showed that if ϵ ≤ min{1/(32n[2]m[2]), δ[2]c(A)[2]/8}, then
det Jf (p, q) 1+ _[δ][2][c][(]8[A][)][2]_ _ϵ[2]_ for any (p, q) _S1, and det Jf_ (p, q) 1 _ϵ[3]_ for any (p, q) _S2._

Recall that the change of entropy is ≥ _·_ _g[t](p, q) log ∈_ _|det Jf_ (p, q)| d(p, ≥ q). As− log(1 + r) ≥ 1+ ∈r _r_ [for]

any r > 1, the change of entropy is at least
_−_ R

_g[t](p, q) log_ 1 + _[δ][2][c][(][A][)][2]_ _ϵ[2]_ d(p, q) + _g[t](p, q) log(1_ _ϵ[3]) d(p, q)_
_S1_ 8 _·_ _S2_ _−_

Z   Z

_δ[2]c(A)[2]_ _ϵ[2]/8_ _ϵ[3]_

_·_ _g[t](p, q) d(p, q)_ _g[t](p, q) d(p, q)._
_≥_ 1 + δ[2]c(A)[2] _ϵ[2]/8_ _S1_ _−_ 1 _ϵ[3]_ _S2_ □

_·_ Z _−_ Z

In Theorem 4.3, for any fixed δ and c(A), the lower bound of λ decreases with ϵ. In particular,
if ϵ = δ[2]c(A)[2]/64, the lower bound is 1/4, i.e. it only requires a small probability in S1 for the DE


-----

to strictly increase. Theorem 4.3 indicates that the DE growth is at least linear (Ω(T )) as long as the
distribution has a sufficient amount of probability lying in S1. Next, in Lemma 4.4 we show that the
DE growth is at most logarithmic (O(log T )) whenever the initial distribution has bounded support.
These two seemingly contradicting bounds can only be compatible for one reason: the condition
needed by Theorem 4.3 will eventually be violated, i.e. there is insufficient amount of probability
lying in S1 after a sufficiently long time. We formalize this in Theorem 4.5 below.

**Lemma 4.4 Suppose the initial distribution has bounded support in [−b, b][n][+][m], for some b > 0.**
_Then the differential entropy at time T is at most (n + m) log(ϵT + b)._

**Proof:** Since every entry in A is bounded in the interval [ 1, 1], we have _p[t]j[+1]_ _p[t]j[| ≤]_ _[ϵ][ and]_
_−_ _|_ _−_
_qk[t][+1]_ _qk[t]_ _[| ≤]_ _[ϵ][. Thus, the distribution at time][ T][ has bounded support in][ [][−][ϵT][ −]_ _[b, ϵT][ +][ b][]][n][+][m][.]_
_|_ _−_
Within this box, the maximal differential entropy is attained by the uniform distribution over it,
which is (n + m) log(ϵT + b). □

**Theorem 4.5 For any non-trivial two-player zero-sum games (A, −A), with MWU dynamics of**
_step-size ϵ_ min 1/(32n[2]m[2]), δ[2]c(A)[2]/8 _, for any initial distribution with bounded support, and_
_≤_ 16ϵ{ _}_
_for any λ >_ _δ[2]c(A)[2][, there exists an infinite sequence of times][ (][T][1][, T][2][,][ · · ·][ )][ such that the probability]_

_in S2 at each round Ti is at least 1 −_ _λ._

The above theorem gives a lower bound of 1 − _λ on the probability in the boundary area for_
infinitely many steps, which is the ”grand escape” phenomenon we mentioned in the introduction.

4.2 MWU IN ONE POPULATION GAME


Recall from (5) that in one-population game A, MWU dynamics in the cumulative payoff space is
**p[t][+1]** = p[t] + ϵ **Ax(p[t]), where xk(p[t]) =** exp(p[t]k[)]

Similar to the two-player zero-sum games, we also consider two different regions: the interior · Pk[′][ exp(][p][t]k[′] [)] [.]
region S1 = {p | x(p) ≥ _δ}, and the boundary region S2, which is the complement to S1. Let_
_g[t](p) denote the probability density of p at time t. We will show that for some sub-classes of one-_
population games A, MWU dynamics increase differential entropy linearly under some conditions.
Also we show the analogue of Theorem 4.5: there exists infinitely many rounds, such that at each
of these rounds t, the probability of p[t] _∈_ _S2 will be at least a constant. First of all, we present the_
Jacobian analysis of such systems.

**Jacobian Analysis.** The Jacobian of the dynamics is J = I + ϵ · R, where I is the n × n identity
matrix, and R is an n × n matrix, in which

exp(pk)
_Rjk = Ajk_ _Ajk[′]_ [exp(][p][k][′] [) exp(][p][k][)] _Ajk_ _Ajk[′]_ _xk[′]_ (p) _._

_k[′′][ exp(][p][k][′′]_ [)][ −] _k[′]_ ([P]k[′′][ exp(][p][k][′′] [))][2][ =][ x][k][(][p][)] _−_ _k[′]_ !
X X

For simplicity, with a little abuse of notation, we useP _xk as a shorthand of xk(p)._
Next, we calculate the determinant of the Jacobian. When we compute it using the Leibniz
formula, it can be expressed in the following form, which is a polynomial of ϵ:
det(J(p)) = 1 + ϵ · (first order term) + ϵ[2] _· (second order term) + · · ·,_
where the first order term is

_Rkk =_ _xkAkk_ _xkxk′_ _Akk′ = [1]_ _xkxk′_ (Akk + Ak′k′ _Akk′_ _Ak′k)._

Xk Xk _−_ Xk,k[′] 2 _kX≠_ _k[′]_ _−_ _−_

Thus, the following lemma gives the condition with which the first order term is non-negative. Note
that in order to have a increasing differential entropy, non-negative first order term is a necessary
condition for a small enough step size.

**Lemma 4.6 The first order term is non-negative for all possible x if and only if Akk + Ak′k′**
_≥_
_Akk′ + Ak′k for all k, k[′]. Moreover, if Akk + Ak′k′_ _Akk′ + Ak′k for all k, k[′]_ _and the inequality_
_≥_
_is strict for some k, k[′], then the first order term is strictly positive for all x ≥_ _δ._

In the terminology of mathematical biology, Akk is the payoff to species k for the intra-species
play within the species, while Akk′ is the payoff to species k for the inter-species play between
species k and k[′]. Informally speaking, the condition in the above lemma is: the one-population
game rewards intra-species play more than to inter-species play.


-----

**Two Sufficient Conditions for Linear DE Growth.** Recall that to demonstrate linear DE growth,
we want the determinant of the Jacobian to be strictly larger than 1. By Lemma 4.6, a clear sufficient
condition is: Akk + Ak′k′ ≥ _Akk′ + Ak′k for all k and k[′], and there exist k, k[′]_ such that the
inequality is strict. But when there is no k, k[′] with strict inequality, it is still possible to have linear
DE growth if the second order term is strictly positive. We present precise statements regarding
these two cases below. To proceed, we define s(A), which is positive for the first case:s(A) =
1
2

_[·][ max][k,k][′]_ _[{][A][kk][ +][ A][k][′][k][′][ −]_ _[A][kk][′][ −]_ _[A][k][′][k][}][.]_
For the first case, we now bound the higher order terms. Note that since _Akk′_ 1 and hence
_|_ _| ≤_
_Rkk′_ 2. When expanding the determinant using the Leibniz formula, we get a polynomial of ϵ.
_|_ _| ≤_
For any i ≥ 2, each ϵ[i] term in the expansion must be of the format ϵ[i] times a product of i factors
of the form Rk1,k2, so each such term has absolute value bounded by 2[i]ϵ[i]. To count the number
of such terms, note that within each term, the collection of k1’s is same as the collection of k2’s,
but the order of k2 can be any permutation of the k1’s. A simple counting shows there are at most
_ni_ _i!_ _n[i]_ such terms, and hence the i-th order coefficient is bounded by n[i] 2[i]. Therefore, when

_·_ _≤_ _·_
 ϵ  16n[2][,][ det][ J][(][p][)][ ≥] [1 +][ δ][2][s]2[(][A][)] _ϵ for all p_ _S1, and det J(p)_ 1 8n[2]ϵ[2] for all p _S2._

_≤_ _[δ][2][s][(][A][)]_ _·_ _∈_ _≥_ _−_ _∈_

With a similar argument as in two-player zero-sum game, we achieve the following theorems.

**Theorem 4.7 If** _S1_ _[g][t][(][p][)][ d][p][ ≥]_ _[λ][, then the entropy will increase by at least]_ 1+δ[2]δs[2](sA(A)·)ϵ/ϵ/2 2

8n[2]ϵ[2] 32n[2]ϵ _·_ _[·][ λ][ −]_

1 8n[2]ϵ[2][ ·][ (1][ −] _[λ][)]R[ in round][ t][, which is strictly positive whenever][ λ >]_ _δ[2]s(A)_ _[.]_
_−_

**Theorem 4.8 In a population game A such that Akk + Ak′k′** _Akk′ + Ak′k for all k and k[′]_ _and_
_≥_
_there existfor any initial distribution with bounded support, and for any k and k[′]_ _such that the inequality is strict, with MWU dynamics of step-size λ >_ _δ32[2]sn(A[2]ϵ)_ _[, there exists an infinite] ϵ ≤_ _[δ]16[2][s][(]n[A][2][,][)]_

_series of time (T1, T2, · · · ) such that, for any i, the probability in S2 at round Ti is no less than 1−λ._

In Appendix D, in the same spirit we did in Figure 2, we present a few plots of MWU in an
one-population game that satisfies the condition in the above theorem.
Next, we consider the case that Akk + _Ak′k′ = Akk′ +_ _Ak′k for all k and k[′]. In this case, the first_
order term is 0. Therefore, we will focus on the second order term, which is _k1<k2_ [(][R][k]1[k]1 _[R][k]2[k]2_

_Rk1k2_ _Rk2k1_ ), which upon expansion becomes _[−]_

[P]

_xk1_ _xk2_ _Ak1k2_ _Ak1k′_ _xk′_ _Ak2k1_ _Ak2,k′_ _xk′_ _._

_−_ 2[1] k1,k2 _−_ _k[′]_ ! _−_ _k[′]_ ![]

X X

Let C(A,B)(x, y) =[X] _k1,k2_ _[x][k]1_ _[y][k]2_ [(][A][k]1[k]2 _k[′][ A][k]1[k][′]_ _[y][k][′]_ [) (][B][k]1[k]2 _k[′][ B][k][′][,k]2_ _[x][k][′]_ [)][.] Then
_−_ [P] _[−]_ [P] _[−]_ [P]

the second order term is [1]2 _[C][(][A][,][A][⊤][)][(][x][,][ x][)][. We will show this term is non-negative.]_

**Lemma 4.9 If Akk + Ak′k′** = _Akk′ + Ak′k for all k and k[′], then_ 21 _[C][(][A][,][A][⊤][)][(][x, x][)]_ =

12 _[C][(][A][,][−][A][)][(][x, x][)][ ≥]_ _[δ][2][c]8[(][A][)]_ _._


**Proof:** By Cheung & Tao (2020), C(A,B)(x, y) = C(A,B+T)(x, y) if Tkk′ = uk + vk′ for all k
and k[′]. As (A[⊤] _−_ (−A))ij = Aji + Aij = Aii + Ajj, the first equality holds. The inequality
follows from Cheung & Piliouras (2019). □
Thus, for the determinant of the Jacobian, the first order term is 0 and the second order term is at
least _[δ][2][c]8[(][A][)]_ in S1. Moreover, the i-th order term is bounded by n[i]2[i]ϵ[i]. Therefore, when ϵ ≤ _[δ]256[2][c][(]n[A][3][)][,]_

det J(p) 1 + _[δ][2][c]16[(][A][)]_ _ϵ[2]_ for any p _S1, and det J(p)_ 1 16n[3]ϵ[3] for x _S2._
_≥_ _·_ _∈_ _≥_ _−_ _∈_

**Theorem 4.10 If** _S1_ _[g][t][(][p][)][ d][p][ ≥]_ _[λ][ then the entropy will increase by at least]_ 1+δ[2]δc[2](cA(A)·)ϵ[2]ϵ/[2]16/16

16n[3]ϵ[3] _·_ _[·][ λ][ −]_

1 16n[3]ϵ[3][ ·][ (1][ −] _[λ][)]R[ in round][ t][, which is strictly positive whenever][ λ >][ 512]δ[2]c([n]A[3][ϵ])_ _[.]_
_−_

**Theorem 4.11 In a population game A such that Akk + Ak′k′ = Akk′ + Ak′k for all k and k[′]**

_and A is non-trivial, with MWU dynamics of step-size ϵ ≤_ _[δ]256[2][c][(]n[A][3][)][, for any initial distribution with]_

_bounded support, and for any λ >_ _δ[512][2]c([n]A[3][ϵ])_ _[, there exists an infinite series of time][ (][T][1][, T][2][,][ · · ·][ )][ such]_

_that, for any i, the probability in S2 at round Ti is at least 1 −_ _λ._


-----

ACKNOWLEDGMENTS

We thank several anonymous reviewers for their suggestions. This research/project is supported in
part by the National Research Foundation, Singapore under its AI Singapore Program (AISG Award
No: AISG2-RP-2020-016), NRF 2018 Fellowship NRF-NRFF2018-07, NRF2019-NRF-ANR095
ALIAS grant, grant PIE-SGP-AI-2020-01, AME Programmatic Fund (Grant No. A20H6b0151)
from the Agency for Science, Technology and Research (A*STAR) and Provost’s Chair Professorship grant RGEPPV2101. Yixin Tao acknowledges ERC Starting Grant ScaleOpt-757481.

REPRODUCIBILITY STATEMENT

We present complete proofs for all theoretical results in this work.

ETHICS STATEMENT

We do not see any ethical or future societal consequences of this work.

REFERENCES

Gabriel P Andrade, Rafael Frongillo, and Georgios Piliouras. Learning in matrix games can be
arbitrarily complex. COLT, 2021.

James P. Bailey and Georgios Piliouras. Multiplicative weights update in zero-sum games. In EC,
pp. 321–338, 2018.

David Balduzzi, Wojciech M Czarnecki, Thomas W Anthony, Ian M Gemp, Edward Hughes, Joel Z
Leibo, Georgios Piliouras, and Thore Graepel. Smooth markets: A basic mechanism for organizing gradient-based learners. arXiv preprint arXiv:2001.04678, 2020.

Jakub Bielawski, Thiparat Chotibut, Fryderyk Falniowski, Grzegorz Kosiorowski, Michał Misiurewicz, and Georgios Piliouras. Follow-the-regularized-leader routes to chaos in routing games.
_ICML, 2021._

Yun Kuen Cheung. Multiplicative weights updates with constant step-size in graphical constant-sum
games. In NeurIPS 2018, pp. 3532–3542, 2018.

Yun Kuen Cheung and Georgios Piliouras. Vortices instead of equilibria in minmax optimization:
Chaos and butterfly effects of online learning in zero-sum games. In Conference on Learning
_Theory, pp. 807–834. PMLR, 2019._

Yun Kuen Cheung and Georgios Piliouras. Chaos, extremism and optimism: Volume analysis of
learning in games. In NeurIPS 2020, 2020.

Yun Kuen Cheung and Yixin Tao. Chaos of learning beyond zero-sum and coordination via game
decompositions. In International Conference on Learning Representations, 2020.

Yun Kuen Cheung, Stefanos Leonardos, and Georgios Piliouras. Learning in markets: Greed leads
to chaos but following the price is right. In Zhi-Hua Zhou (ed.), IJCAI, pp. 111–117. ijcai.org,
2021. doi: 10.24963/ijcai.2021/16.

Thiparat Chotibut, Fryderyk Falniowski, Michał Misiurewicz, and Georgios Piliouras. The route to
chaos in routing games: When is price of anarchy too optimistic? NeurIPS, 2020.

Thomas M. Cover and Joy A. Thomas. Elements of Information Theory 2nd Edition (Wiley Series
_in Telecommunications and Signal Processing). Wiley-Interscience, 2006. ISBN 0471241954._

Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient descent in
min-max optimization. arXiv preprint arXiv:1807.03907, 2018.

Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training gans with
optimism. arXiv preprint arXiv:1711.00141, 2017.

Lampros Flokas, Emmanouil-Vasileios Vlatakis-Gkaragkounis, Thanasis Lianeas, Panayotis Mertikopoulos, and Georgios Piliouras. No-regret learning and mixed nash equilibria: They do not
mix. NeurIPS, 2020.


-----

David Heaver Fremlin. Measure theory, volume 4. Torres Fremlin, 2000.

Angeliki Giannou, Emmanouil-Vasileios Vlatakis-Gkaragkounis, and Panayotis Mertikopoulos.
Survival of the strictest: Stable and unstable equilibria under regularized learning with partial
information. COLT, 2021.

Gauthier Gidel, Reyhane Askari Hemmat, Mohammad Pezeshki, R´emi Le Priol, Gabriel Huang, Simon Lacoste-Julien, and Ioannis Mitliagkas. Negative momentum for improved game dynamics.
In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 1802–1811.
PMLR, 2019.

Josef Hofbauer and Karl Sigmund. Evolutionary Games and Population Dynamics. Cambridge
University Press, 1998. doi: 10.1017/CBO9781139173179.

Shuyue Hu, Chin-wing Leung, and Ho-fung Leung. Modelling the dynamics of multiagent qlearning in repeated symmetric games: a mean field theoretic approach. Advances in Neural
_Information Processing Systems, 32:12125–12135, 2019._

Ratul Lahkar and Robert M. Seymour. Reinforcement learning in population games. Games and
_Economic Behavior, 80(C):10–38, 2013. doi: 10.1016/j.geb.2013.02.006._

Ratul Lahkar and Robert M Seymour. The dynamics of generalized reinforcement learning. Journal
_of Economic Theory, 151:584–595, 2014._

Alistair Letcher. On the impossibility of global convergence in multi-loss optimization. _arXiv_
_preprint arXiv:2005.12649, 2020._

Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chandrasekhar, and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going
the extra (gradient) mile. International Conference on Learning Representations (ICLR), 2019.

Aryan Mokhtari, Asuman Ozdaglar, and Sarath Pattathil. A unified analysis of extra-gradient and
optimistic gradient methods for saddle point problems: Proximal point approach. In International
_Conference on Artificial Intelligence and Statistics, pp. 1497–1507. PMLR, 2020._

Gerasimos Palaiopanos, Ioannis Panageas, and Georgios Piliouras. Multiplicative weights update
with constant step-size in congestion games: Convergence, limit cycles and chaos. NeurIPS,
2017.

Julien Perolat, Remi Munos, Jean-Baptiste Lespiau, Shayegan Omidshafiei, Mark Rowland, Pedro
Ortega, Neil Burch, Thomas Anthony, David Balduzzi, Bart De Vylder, et al. From poincar´e recurrence to convergence in imperfect information games: Finding equilibrium via regularization.
In International Conference on Machine Learning, pp. 8525–8535. PMLR, 2021.

William H. Sandholm. Population Games and Evolutionary Dynamics. MIT Press, 2010.

Fernando Vega-Redondo. Evolution, games, and economic behaviour. Oxford University Press,
1996.


-----

A MOTIVATING EXAMPLE

To explain our mathematical model intuitively, we present the following simple one-dimensional
example. Let f (x) = 2x, and suppose that X [0] is a random variable uniform on the interval [0, 1].
Then X [1] = f (X [0]) becomes another random variable, which is uniform on [0, 2]. Analogously,
_X_ [2] = f (X [1]) is a new random variable being uniform on [0, 4]. Generally speaking, the problem is
to understand how the initial random variable X [0] evolves under a given deterministic iterated update
rule f, and to study how the uncertainty evolves using the measure of differential entropy. In our
paper, we focus on such systems which arise in the context of learning-in-games.
In a learning-in-game system, X [0] = (p[0], q[0]) represents a distribution over initial beliefs (initial
cumulative payoffs) of the two players, and X _[t]_ = (p[t], q[t]) represents the distribution of cumulative
payoffs at time t. Such a probabilistic formulation is natural from the perspective of an external
analyst (who is not a player of the game), who cannot be certain about what the initial beliefs the
players have. For her to proceed with an analysis of the system, a natural approach is to infer the
initial distributions from either the past data, or her knowledge about how random external signals
influence the initializations. The iterated update rule f in this case is determined by the learning
algorithms used by the players, and by the underlying game. For instance, if both players are using
MWU in a bimatrix game (A, B), the update rule f is given by (4).
Unlike the motivating example, for learning-in-game systems the dimensions can be arbitrarily
high, and the update rule is non-linear. Thus, deriving the exact evolution of the distribution of X _[t]_
is very hard in general. But we can still use the popular statistical measure, differential entropy
(DE), to understand the evolution of uncertainty. We show that the DE increases linearly with time
for various combinations of learning algorithms and games, including MWU in zero-sum games,
OMWU in coordination games, and MWU in certain one-population games. Section 2 has precise
descriptions of the learning algorithms and the games. These results can be extended to a broad
family of non-zero-sum and non-coordination games, via the Jacobian analysis by Cheung & Tao
(2020). The results also extend to gradient ascent, as it is known to be a special case of FTRL.

B INTEGRATION BY SUBSTITUTION

A general theorem of integration by substitution is stated below.

**Theorem B.1 (Fremlin (2000), Theorem 263D) Let D ⊂** R[d] _be any measurable set, and φ : D →_
R[d] _be an injective function differentiable relative to its domain at each point of D. For each x ∈_ _D,_
_let Jφ(x) be the Jacobian of φ relative to D at x. Then for any real-valued function h defined on_
_φ(D),_


_h(x) dx =_
_φ(D)_


_h(φ(x))_ det Jφ(x) dx
_· |_ _|_


_if either integral is well-defined, provided that h(φ(x))_ det Jφ(x) _is interpreted as zero when_
_· |_ _|_
det Jφ(x) = 0 and h(φ(x)) is undefined.

In Section 3, we use integration by substitution twice. First, in the above theorem, by setting
_h = g, φ = f_ _[−][1]_ and D = f (X _[′]) for any measurable set X_ _[′]_ _⊂_ _X, we have_
ZX _[′][ g][(][x][)][ d][x][ =]_ Zf (X _[′])_ _g(f_ _[−][1](x)) · | det Jf −1_ (x)| dx .

Recall that g is a probability density function of the absolutely continuous random variable X, so
the LHS of the above equality is same asis the probability density function of f (X P) [, i.e.X ∈X ˆg(x[′]) =]. Thus, the integrand of the integral in the RHS g(f _[−][1](x)) · | det Jf −1_ (x)|.
To derive (6), in the above theorem, we set h(x) = −gˆ(x) log ˆg(x), φ = f, D = X and hence
_φ(D) = Y._

C MORE APPLICATIONS

C.1 OMWU IN TWO-PLAYER COORDINATION GAMES

A two-player game (A, B) is a coordination game if A = B. Similar to the MWU dynamics in twoplayer zero-sum games, here we also look at two different areas: S1 = {(p, q)|x(p) ≥ _δ and y(q) ≥_


-----

_δ} and S2, which is the complement to S1, S2 = S1. For a coordination game (A, A), Cheung &_
Piliouras (2020) showed that when (p, q) _S1, then det Jf_ (x) 1 + _[δ][2][c][(]8[A][)][2]_ _ϵ[2]_ when ϵ is small
_∈_ _≥_ _·_

enough and, when (p, q) _S2, then det Jf_ (x) 1 _O(ϵ[3]). Therefore, by a calculation which is_
_∈_ _≥_ _−_
similar to MWU in two-player zero-sum game, we obtain the following theorems.

**Theorem C.1 If** _S1_ _[g][t][(][p][,][ q][)][ d][(][p][,][ q][)][ ≥]_ _[λ][ then the entropy increases by at least]_ 1+δ[2]δc[2](cA(A)[2])·[2]ϵ[2]ϵ/[2]8/8

_·_ _[·]_ _[λ]_ _[−]_
_O(ϵ[3])_ (1 _λ)._
_·_ _−_ R

**Theorem C.2 For any non-trivial two-player coordination games (A, A), with OMWU dynamics**
_of a small enough step-size ϵ, for any initial distribution with bounded support, and for any λ such_
_that ϵ = o(λ), there exists an infinite series of time (T1, T2, · · · ), such that the probability in S2 at_
_round Ti is at least 1 −_ _λ._

C.2 OMWU IN ONE POPULATION GAMES

As pointed out in Cheung & Piliouras (2020), the continuous analogue of OMWU, in the context
of online learning, is as follows. There are n actions. Let p(t) ∈ R[n] denote the cumulative payoff
vector up to time t, and u(t) ∈ R[n] denote the instantaneous payoff vector to the n actions at time t.
The continuous analogue is
**p˙** = u + ϵ · ˙u. (9)
In one-population game, u(t) = A _·_ **x(t), where for the logit conversion used in MWU and OMWU,**
_xk = exp(pk)/_ _ℓ_ [exp(][p][ℓ][)][. It is easy to compute that]

_∂xk_ _∂xk_

[P] = xk (xk)[2] and = _xkxℓ_ when ℓ = k.

_∂pk_ _−_ _∂xℓ_ _−_ _̸_

Then for any ℓ = 1, 2, . . ., n, we have


_∂[Ax]j_

_∂pℓ_


_Ajk_ _[∂x][k]_ = Ajℓxℓ
_·_ _∂pℓ_ _−_


_Ajkxkxℓ_ = xℓ (Ajℓ [Ax]j) .
_−_


Following the notation in MWU case, _[∂][[]∂p[Ax]ℓ[]][j]_ = Rjℓ. By the chain rule, we have


_u˙_ _j =_ [d][[][Ax][]][j] =

dt

Putting the above equality into (9) yields

_p˙j = [Ax]j + ϵ ·_


_Rjℓ_ ˙pℓ.

_·_
_ℓ=1_

X

_n_

_Rjℓ_ ˙pℓ,

_·_
_ℓ=1_

X


which is a recurrence of the variables ˙p1, ˙p2, . . ., ˙pn. Unwinding this recurrence yields


_Rjℓ_ [Ax]ℓ + (ϵ[2]).

_·_ _O_
_ℓ=1_

X


_p˙j = [Ax]j + ϵ ·_


The above system, upon Euler discretization to a discrete-time dynamical system with time interval
_ϵ, becomes_


_j,_ _p[t]j[+1]_ = p[t]j [+][ ϵ][ ·][ [][Ax][]][j] [+][ ϵ][2][ ·]
_∀_


_Rjℓ_ [Ax]ℓ + (ϵ[3]). (10)

_·_ _O_
_ℓ=1_

X


Observe that on the RHS of (10), the first two terms coincide with that in the MWU case. The
remaining terms can be viewed as a second-order perturbation. Consequently, the Jacobian of the
system equation 10 is a second-order perturbation of the Jacobian for the MWU case, and hence
the constant and first-order terms of the determinants of these two Jacobians are the same, which is
_n_
_j=1_ _[R][jj][.]_
P Therefore, for the case that Akk + Ak[′]k[′] _Akk[′] + Ak[′]k for all k and k[′]_ and there exist k and k[′]

_≥_
such that the inequality is strict, we obtain the following theorems. Recall that S1 = {p|x(p) ≥ _δ}_
and S2, which is the complement to S1, S2 = S1; and s(A) = maxk,k′ _Akk_ +Ak′k′ _Akk′_ _Ak′k_ .
_{_ _−_ _−_ _}_


-----

**Theorem C.3 If** _S1_ _[g][t][(][p][)][ d][p][ ≥]_ _[λ][, then the entropy increases by at least]_ 1+δ[2]δs[2](sA(A)ϵ/)ϵ/2 2 _[·][ λ][ −]_ _[O][(][ϵ][2][)][ ·]_

(1 _λ) for a small enough step size ϵ._
_−_ R

**Theorem C.4 In an one-population game A such that Akk + Ak′k′** _Akk′ + Ak′k for all k and k[′]_
_≥_
_and there exist k and k[′]_ _such that the inequality is strict, with OMWU dynamics with a small enough_
_step-size ϵ, for any initial distribution with bounded support, and for any λ such that ϵ = o(λ), there_
_exists an infinite series of time (T1, T2, · · · ) such that, for any i, the probability in S2 at round Ti is_
_no less than 1 −_ _λ._

For the case that Akk + _Ak′k′ = Akk′ +_ _Ak′k for all k and k[′], we consider the second-order term._
The second-order terms of the two determinants (MWU and OMWU) are not the same though. The
second-order term of the determinant of Jacobian for the OMWU case is


_∂Rjℓ_

[Ax]ℓ.
_∂pj_ _·_


_RjjRkk_
1≤j<kX≤n _−_


_RjkRkj +_
1≤j<kX≤n


_RjℓRℓj +_

_j=1_ _ℓ=1_

X X


_j=1_


_ℓ=1_


Observe that the first two summations are just the same as those in the MWU case. Next, a straightforward calculation gives

_∂Rjℓ_

= Rjj **1j=ℓ** _xjRjℓ_ _xℓRjj,_
_∂pj_ _·_ _−_ _−_

where 1j=ℓ = 1 if j = ℓ, and 0 otherwise. Hence


_∂Rjℓ_

[Ax]ℓ =
_∂pj_ _·_


_Rjj_ [Ax]j
_j=1_ _·_ _−_

X

_n_

_Rjj_ [Ax]j
_j=1_ _·_ _−_

X


(xjRjℓ + xℓRjj) [Ax]j
_·_

_j=1_ _ℓ=1_

X X


_j=1_


_ℓ=1_


_xj[Ax]j_
_j=1_

X


_Rjℓ_
_−_
_ℓ=1_

X


_Rjj[Ax]j_
_j=1_

X


_xℓ._

_ℓ=1_

X


Since _ℓ=1_ _[x][ℓ]_ [= 1][ and][ P]ℓ[n]=1 _[R][jℓ]_ [= 0][, we have][ P]j[n]=1 _nℓ=1_ _∂R∂pjℓj_

second-order term is _[·][ [][Ax][]][ℓ]_ [= 0][. To conclude, the]

P

[P][n] _n_ _n_


_RjjRkk_
1≤j<kX≤n _−_


_RjkRkj +_
1≤j<kX≤n


_RjℓRℓj_

_ℓ=1_

X


_j=1_


(Rjj)[2] +
_j=1_

X

2

_n_

_Rjj_

 

_j=1_

X
 


_RjjRkk +_
1≤j<kX≤n

_RjjRkk +_
1≤j<kX≤n


_RjkRkj_
1≤j<kX≤n

_RjkRkj_
1≤j<kX≤n


Note that when Akk + _Ak′k′ = Akk′ +_ _Ak′k for all k and k[′],_ _j_ _[R][jj][ = 0][. Therefore, the second]_

order term of OMWU is the negative of the second order term of MWU, which will be smaller than

8 . [P]

_−_ _[δ][2][c][(][A][)]_

**Theorem C.5 In an one-population game A such that Akk +** _Ak′k′_ _Akk′ +_ _Ak′k for all k and k[′],_
_with OMWU dynamics, if_ _p_ _S1_ _[g][(][p][)][ d][p][ ≥]_ _[λ][, then the entropy decreases by at least] ≥_ 1+δ[2]δc[2](cA(A)[2])ϵ[2][2]ϵ[2][ ·]

_∈_

_λ_ _O(ϵ[3])_ (1 _λ) for a small enough step size ϵ._
_−_ _·_ _−_ R

C.3 PERTURBATIONS TO PAYOFFS

In the main body, we focus on two-player zero-sum games. In this appendix, we discuss the Jacobian
analysis for two-player general games based on Cheung & Tao (2020), and explain why it is robust
against perturbations.


-----

First, we define the following function on the cumulative payoff space of two-player games,
which depends on a matrix M ∈ R[n][×][m].[5]

_LM(p, q) := [1]_ _xj(p)xj′_ (p)yk(q)yk′ (q) (Mjk + Mj′k′ _Mjk′_ _Mj′k)[2],_

4 _·_ _−_ _−_

11≤≤k,kXj,j[′][′]≤≤mn

where xj, xj′ _, yk, yk′ is as defined in (2)._
For any general game (A, B), we can decompose it into the sum of a zero-sum game and a
coordination game as (A, B) = (Z, −Z) + (C, C), where Z = (A − **B)/2 and C = (A + B)/2.**
Cheung & Tao (2020) showed that the determinant of the Jacobian matrix of MWU with step-size ϵ
in the game (A, B) is
1 + (LZ(p, q) _LC(p, q)) ϵ[2]_ + (ϵ[4]).
_−_ _O_

We rewrite LZ(p, q) _LC(p, q) as_
_−_


_xj(p)xj[′]_ (p)yk(q)yk[′] (q) · (Ajk + Aj[′]k[′] − _Ajk[′] −_ _Aj[′]k)(Bjk[′] + Bj[′]k −_ _Bjk −_ _Bj[′]k[′]_ ).


1≤j,j[′]≤n
1≤k,k[′]≤m

Thus, if


max (11)
_j,j[′],k,k[′][ [(][A][jk][ +][ A][j][′][k][′][ −]_ _[A][jk][′][ −]_ _[A][j][′][k][)(][B][jk][′][ +][ B][j][′][k][ −]_ _[B][jk][ −]_ _[B][j][′][k][′]_ [)]][ ≥] _[L >][ 0]_

and
min (12)
_j,j[′],k,k[′][ [(][A][jk][ +][ A][j][′][k][′][ −]_ _[A][jk][′][ −]_ _[A][j][′][k][)(][B][jk][′][ +][ B][j][′][k][ −]_ _[B][jk][ −]_ _[B][j][′][k][′]_ [)]][ ≥] [0][,]

then in the domain S1 = {(p, q) | ∀j, k, xj(p) ≥ _δ and yk(q) ≥_ _δ}, the determinant is at least_
1 + δ[4]Lϵ[2]/8 for all sufficiently small ϵ.
Since the product (Ajk + Aj′k′ _Ajk′_ _Aj′k)(Bjk + Bj′k′_ _Bjk′_ _Bj′k) depends on A, B_
_−_ _−_ _−_ _−_
continuously, it is natural to expect that when A, B are slightly perturbed, the value of the product
changes also slightly. To make this precise, we suppose that in each round t, the actual payoff
matrices are (A[t], B[t]), and their entries are perturbations from (A, B) by at most an absolute value
of β ≤ 2.[6] Recall that the entries in A, B are bounded in the interval [−1, 1], so the absolute
values ofperturbations to their values are bounded by Ajk + Aj[′]k[′] − _Ajk[′] −_ _Aj[′]k and Bjk 4[′]β +. By the lemma below, for the game Bj[′]k −_ _Bjk −_ _Bj[′]k[′] are bounded by (A 4, while the[t], B[t]), the_
bound in (11) still hold by replacing its lower bound with L − 34β, and the bound in (12) still hold
by replacing its lower bound with −34β. Thus the determinant of the Jacobian matrix in S1 is at
least 1 + (δ[4]L/4 − 34β)ϵ[2]/2 ≥ 1 + δ[4]Lϵ[2]/16 if β ≤ _δ[4]L/272._

**Lemma C.6 If β ≤** 2, |a|, |b| ≤ 4 and |a[′] _−_ _a|, |b[′]_ _−_ _b| ≤_ 4β, then a[′]b[′] _≥_ _ab −_ 34β.

**Proof:−34β.** Let a[′] _−_ _a = βa, b[′]_ _−_ _b = βb. Then a[′]b[′]_ _−_ _ab = bβa + aβb + βaβb ≥−16β −_ 16β − _β[2]□≥_

D PLOTS OF UNCERTAINTY EVOLUTION OF MWU IN ONE-POPULATION
GAME

5This function was denoted by C(M,−M)(p, q) in Cheung & Tao (2020), but to avoid confusion and cluster
of notation, we change it to LM(p, q).
6We assume that the entries in both (A, B) and (At, Bt) are in the interval [−1, 1], so β is at most 2.


-----

2.0

1.5

1.0

0.5

0.0

0.5

1.0

1.5

2.0

2.0

1.5

1.0

0.5

0.0

0.5

1.0

1.5

2.0


2.0

1.5

1.0

0.5

0.0

0.5

1.0

1.5

2.0


2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0

2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0


2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0

0.46

0.44

0.42

0.40

0.38

1.00




1.0


0.75

0.50

0.25

1.5 0.00

1.0 0.25

0.5

0.0 0.50

0.5 0.75


1.5


Figure 2: We plot the evolution of uncertainty for MWU in an one-population game specified by

0.1 1.0 _−1.0_

the matrix A = _−1.0_ 0.1 1.0, with step-size ϵ = 0.01. As we did in Figure 2, in the top

" 1.0 _−1.0_ 0.1 #

left plot is the initial distribution, which is uniform in the small square. The top right plot shows the
“possibility” plot after 6000 steps, and the bottom left plot shows the corresponding heat-map plot
of probability densities.


-----

