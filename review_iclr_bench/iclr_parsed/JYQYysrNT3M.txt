# REINFORCEMENT LEARNING WITH EX-POST MAX- MIN FAIRNESS

**Anonymous authors**
Paper under double-blind review

ABSTRACT

We consider reinforcement learning with vectorial rewards, where the agent receives a vector of K ≥ 2 different types of rewards at each time step. The agent
aims to maximize the minimum total reward among the K reward types. Different from existing works that focus on maximizing the minimum expected total
reward, i.e. ex-ante max-min fairness, we maximize the expected minimum total
reward, i.e. ex-post max-min fairness. Through an example and numerical experiments, we show that the optimal policy for the former objective generally does
not converge to optimality under the latter, even as the number of time steps T
grows. Our main contribution is a novel algorithm, Online-ReOpt, that achieves
near-optimality under our objective, assuming an optimization oracle that returns
a near-optimal policy given any scalar reward. The expected objective value under Online-ReOpt is shown to converge to the asymptotic optimum as T increases.
Finally, we propose offline variants to ease the burden of online computation in
Online-ReOpt, and we propose generalizations from the max-min objective to
concave utility maximization.

1 INTRODUCTION

The prevailing paradigm in reinforcement learning (RL) concerns the maximization of a single
_scalar reward. On one hand, optimizing a single scalar reward is sufficient for modeling simple_
tasks. On the other hand, in many complex tasks there are often multiple, potentially competing,
rewards to be maximized. Expressing the objective function as a single linear combination of the
rewards can be constraining and insufficiently expressive for the nature of these complex tasks. In
addition, a suitable choice of the linear combination is often not clear a priori.

In this work, we consider the reinforcement learning with max-min fairness (RL-MMF) problem.
The agent accumulates a vector of K 1 time-average rewards _V[¯]1:T = ( V[¯]1:T,k)[K]k=1_
time steps, and aims to maximize E[min ≥k 1,...,K _V[¯]1:T,k]. The maximization objective represents[∈]_ [R][K][ in][ T]
_∈{_ _}_
_ex-post max-min fairness, in contrast to the objective of ex-ante max-min fairness by maximizing_
mink∈{1,...,K} E[ V[¯]1:T,k].

Our main contributions are the design and analysis of the Online-ReOpt algorithm, which achieves
near-optimality for the ex-post max-min fairness objective. More specifically, the objective under
Online-ReOpt converges to the optimum as T increases. Our algorithm design involves a novel
adaptation of the multiplicative weight update method (Arora et al., 2012), in conjunction with
a judiciously designed re-optimization schedule. The schedule ensures that the agent adapts his
decision to the total vectorial reward collected at a current time point, while allowing enough time
for the currently adopted policy to converge before switching to another policy.

En route, we highlight crucial differences between the ex-ante and ex-post max-min fairness objectives, by showing that an optimal algorithm for the former needs not converge to the optimality even
when T increases. Finally, our results are extended to the case of maximizing E[g( V[¯]1:T )], where g
is a Lipschitz continuous and concave reward function.


-----

2 RELATED WORKS

The Reinforcement Learning with Max-Min Fairness (RL-MMF) problem described is related to an
emerging body of research on RL with ex-ante concave reward maximization. The class of ex-ante
concave reward maximization problems include the maximization of g(E[ V[¯]1:T ]), as well as its ex_ante variants, including the long term average variant g(E[limT_ _V1:T ]) and its infinite horizon_
_→∞_ [¯]
discounted reward variant. The function g : R[K] _→_ R is assumed to be concave.

The class of ex-ante concave reward maximization problems is studied by the following research
works. Chow et al. (2017) study the case where g is specialized to the Conditional Value-at-Risk
objective. Hazan et al. (2019) study the case when g models the entropy function over the probability
distribution over the state space, in order to construct a policy which induces a distribution over the
state space that is as close to the uniform distribution as possible. Miryoosefi et al. (2019) study
the case of minimizing the distance between E[ V[¯]1:T ] and a target set in R[K]. Lee et al. (2019) study
the objective of state marginal matching, which aims to make the state marginal distribution match
a given target state distribution. Pareto optimality of E[ V[¯]1:T ] and its ex-ante variants are studied in
(Mannor & Shimkin, 2004; G´abor et al., 1998; Barrett & Narayanan, 2008; Van Moffaert & Now´e,
2014). Lastly, a recent work Zahavy et al. (2021) provides a unifying framework that encompasses
many of the previously mentioned works, by studying the problem of maximizing g(E[ V[¯]1:T ]) and
its ex-ante variants, where g is concave and Lipschitz continuous. Our contributions, which concern
the ex-post max-min fairness E[mink 1,...,K _V[¯]1:T,k] and its generalization to the ex-post concave_
_∈{_ _}_
case, are crucially different from the body of works on the ex-ante case. The difference is further
highlighted in the forthcoming Section 3.2.

Additionally, a body of works Altman (1999); Tessler et al. (2019); Le et al. (2019); Liu et al.
(2020) study the setting where g is a linear function, subject to the constraint that E[ V[¯]1:T ] (or its
ex-ante variants) is contained in a convex feasible region, such as a polytope. There is another line
of research works Tarbouriech & Lazaric (2019); Cheung (2019); Brantley et al. (2020) focusing
on various online settings. The works Tarbouriech & Lazaric (2019); Cheung (2019) focus on the
ex-post setting like ours, but they crucially assume that the underlying g is smooth, which is not the
case for our max-min objective nor the case of Lipschitz continuous concave functions. In addition,
the optimality gap (quantified by the notion of regret) degrades linearly with the number of states,
which makes their applications to large scale problems challenging. Brantley et al. (2020) focus on
the ex-ante setting, different from our ex-post setting, and their optimality gap also degrades linearly
with the number of states.

3 MODEL


**Set up. An instance of the Reinforcement Learning with Max-Min Fairness (RL-MMF) problem is**
specified by the tuple ( _, s1,_ _, T,_ ). The set is a finite state space, and s1 is the initial
state. In the collection _S =_ _As_ _s O_, the set _Ss contains the actions that the agent can take when ∈S_
_A_ _{A_ _}_ _∈S_ _A_
he is at state s. Each set As is finite. The quantity T ∈ N is the number of time steps.

When the agent takes action a ∈As at state s, he receives the array of stochastic outcomes
(s[′], U (s, a)), governed by the outcome distribution O(s, a). For brevity, we abbreviate the relationship as (s[′], U (s, a)) ∼O(s, a). The outcome s[′] _∈S is the subsequent state he transits to. The_
outcome U (s, a) = (Uk(s, a))[K]k=1 [is a random vector lying in][ [][−][1][,][ 1]][K][ almost surely. The random]
variable Uk(s, a) is the amount of type-k stochastic reward the agent receives. We allow the random
variables s[′], U1(s, a), . . . UK(s, a) to be arbitrarily correlated.

**Dynamics. At time t ∈{1, . . . T** _}, the agent observes his current state st. Then, he selects an action_
_at_ _st_ . After that, he receives the stochastic feedback (st+1, Vt(st, at)) (st, at). We denote
_Vt ∈A(st, at) = (Vt,k(st, at))[K]k=1[, where][ V][t,k][(][s][t][, a][t][)][ is the type-][k][ stochastic reward received at time] ∼O_ _[ t][.]_
The agent select the actions {at}t[T]=1 [with a policy][ π][ =][ {][π][t][}]t[T]=1[, which is a collection of functions.]
For each t, the function πt inputs the history Ht 1 = _q=1[{][s][q][, a][q][, V][q][(][s][q][, a][q][)][}][ and the current state]_
_−_ _∪[t][−][1]_
_{policyst}, and outputs π. A policy a πt ∈A is stationary if for allst_ . We use the notation t, Ht 1 a, s[π]tt it holds that[to highlight that the action is chosen under] πt(Ht 1, st) = ¯π(st) for some
_−_ _−_
function ¯π, where ¯π(s) ∈As for all s. With a slight abuse of notation, we identify a stationary
policy with the function ¯π.


-----

**Objective. We denote** _V[¯]1:[π]t_ [=] 1t _tq=1_ _[V][q][(][s][q][, a]q[π][)][ as the time average vectorial reward during]_
time 1 to t under policy π. The agent’s over-arching goal is to design a policy π that maximizes
E[gmin( V[¯]1:[π]T [)]][, where][ g][min][ :][ R][K][ →]P[R][ is defined as][ g][min][(][v][) = min][k][∈{][1][,...,K][}] _[v][k][. Denoting][ ¯]V1:[π]T,k_
as the k-th component of the vector _V[¯]1:[π]T_ [, the value][ g][min][( ¯]V1:[π]T [) = min][k][ ¯]V1:[π]T,k [is the minimum time]
average reward, among the reward types 1, . . ., K. The function gmin is concave, and is 1-Lipschitz
w.r.t. over the domain R[K].
_∥· ∥∞_

When K = 1, the RL-MMF problem reduces to the conventional RL problem with scalar reward
maximization. The case of K > 1 is more subtle. Generally, the optimizing agent needs to focus on
different reward types in different time steps, contingent upon the amounts of the different reward
types at the current time step. Since the max-min fairness objective could lead to an intractable
optimization problem, we aim to design a near-optimal policy for the RL-MMF problem.

3.1 REGRET

We quantify the near-optimality of a policy π by the notion of regret, which is the difference between
a benchmark opt(P(gmin)) and the expected reward E[gmin( V[¯]1:[π]T [)]][. Formally, the regret of a policy][ π]
in a T time step horizon is

Reg(π, T ) = opt(P(gmin)) − E[gmin( V[¯]1:[π]T [)]][.] (1)

The benchmark opt(P(gmin)) is a fluid approximation to the expected optimum. To define
opt(P(gmin)), we introduce the notation p = {p(s[′]|s, a)}s∈S,a∈As, where p(s[′]|s, a) is the probability of transiting to s[′] from s, a. In addition, we introduce v = {v(s, a)}s∈S,a∈As, where
_v(s, a) = E[U_ (s, a)] is the vector of the K expected rewards. The benchmark opt(P(gmin)) is
the optimal value of the maximization problem P(gmin). For any g : R[K] _→_ R, we define


P(g): max


_v(s, a)x(s, a)_
_s∈SX,a∈As_


_p(s|s[′], a[′])x(s[′], a[′])_ _∀s ∈S_ (2a)
_s[′]∈SX,a[′]∈As′_


s.t.


_x(s, a) =_
_aX∈As_


_x(s, a) = 1_ (2b)
_s∈SX,a∈As_

_x(s, a)_ 0 _s_ _, a_ _s._ (2c)
_≥_ _∀_ _∈S_ _∈A_

The concave maximization problem P(gmin) serves as a fluid relaxation to RL-MMF. For each s ∈
_, a_ _s, the variable x(s, a) can be interpreted as the frequency of the agent visiting state s and_
_S_ _∈A_
taking action a. The set of constraints (2a) stipulates that the rate of transiting out of a state s is equal
to the rate of transiting into the state s for each s ∈S, while the sets of constraints (2b, 2c) require
that {x(s, a)}s∈S,a∈As forms a probability distribution over the state-action pairs. Consequently,
opt(P(gmin)) is an asymptotic (in T ) upper bound to the expected optimum.

Our goal is to design a policy π such that its regret[1] Reg(T ) satisfies


Reg(T ) = opt(P(gmin)) E[gmin( V[¯]1:[π]T [)]][ ≤] _[D]_ (3)
_−_ _T_ _[γ]_

assume the access to anholds for all initial state optimization oracle s1 ∈S and all T ∈ Λ, which returns a near-optimal policy given any scalarN, with parameters D, γ > 0 independent of T . We
reward. For ϑ ∈ R[K], define the linear function gϑ : R[K] _→_ R as gϑ(w) = ϑ[⊤]w = _k=1_ _[ϑ][k][w][k][.]_
The oracle Λ inputs ϑ ∈ R[K], and outputs a policy π satisfying

[P][K]

opt(P(gϑ)) E[gϑ( V[¯]1:[π]T [)] =][ opt][(][P][(][g][ϑ][))][ −] [E][[][ϑ][⊤]V[¯]1:[π]T []][ ≤] _[D][lin]_ (4)
_−_ _T_ _[β]_

for all initial stateassuming β > 0, we are assuming that the output policy s1 ∈S and all T ∈ N, with parameters π is near-optimal, in the sense that the Dlin, β > 0 independent of T . By
difference opt(P(gϑ)) − E[ϑ[⊤]V[¯]1:[π]T []][ converges to 0 as][ T][ tends to the infinity. A higher][ β][ signifies a]

1We omit the notation with π for brevity sake


-----

faster convergence, representing a higher degree of near-optimality. We refer to ϑ as a scalarization
of v, with the resulting scalarized reward being ϑ[⊤]v(s, a) for each s, a.

Our algorithmic frameworks involve invoking Λ as a sub-routine on different ϑ’s. In other words,
we assume an algorithmic sub-routine that solves the underlying RL problem with scalar reward
(the case of K = 1), and delivers an algorithm that ensures max-min fairness (the case of K ≥ 1).
Finally, while the main text focuses on gmin, our algorithm design and analysis can be generalized to
the case of concave g, as detailed in Appendix C.

3.2 COMPARISON BETWEEN MAXIMIZING E[gMIN( V[¯]1:[π]T [)]][ AND][ g][MIN][(][E][[ ¯]V1:[π]T [])]

Before introducing our algorithms, we illustrate the difference between the objectives of maximizing
E[gmin( V[¯]1:[π]T [)]][ and][ g][min][(][E][[ ¯]V1:[π]T [])][ by the deterministic instance in Figure 1, with initial state][ s][1][ =][ s][o][.]

The figure depicts an instance with K = 2. An arc represents 0 0
an action that leads to a transition from its tail to its head. For 0 0 0 1
example, the arc from s[o] to s[ℓ] represents the action a[oℓ], with 1 _s[ℓ]_  0 _s[o]_  0 _s[r]_ 0
_p(s[ℓ]_ _s[o], a[oℓ]) = 1. Likewise, the loop at s[ℓ]_ represents the    0 0   
_|_

0 0
0 0

_s[ℓ]_ _s[o]_ _s[r]_

 0  0

  

action a[ℓℓ] with p(s[ℓ] _s[ℓ], a[ℓℓ]) = 1. Each arc is labeled with_
_|_ Figure 1: States and actions are     
its vectorial reward, which is deterministic. For example, with
certainty we have V (s[o], a[oℓ]) = 00 and V (s[ℓ], a[ℓℓ]) = 01 . represented by circles and arcs.

Consider two stationary policies  π[ℓ], π[r], defined as π[ℓ] (s[r]) = a[ro], π[ℓ](s[o]) = a[oℓ], π[ℓ](s[ℓ]) = a[ℓℓ] and
_π[r](s[r]) = a[rr], π[r](s[o]) = a[or], π[r](s[ℓ]) = a[ℓo]. The policy π[ℓ]_ always seeks to transit to s[ℓ], and then
loop at s[ℓ] indefinitely, likewise for π[r]. With certainty, _V[¯]1:[π]T[ℓ]_ [=] 1 01/T, _V[¯]1:[π]T[r]_ [=] 1−01/T .
_−_

The objective gmin(E[ V[¯]1:[π]T [])][ is maximized by choosing][ π][ran][ uniformly at random from the collection]     
_{π[ℓ], π[r]}. We have E[ V[¯]1:[π]T[ran][] =]_ 11//22−−11//(2(2TT ) ), leading to the optimal value of 1/2 − 1/(2T ). More
generally, existing research focuses on maximizing   _g(E[ V[¯]1:[π]T_ [])][ for certain concave][ g][, and the related]
objectives of maximizing g(limT →∞ E[ V[¯]1:[π]T [])][ or][ g][(][E][[][P]t[∞]=1 _[α][t][V][t][(][s][t][, a]t[π][)])][, where][ α][ ∈]_ [(0][,][ 1)][ is the]
discount factor. In these research works, a near-optimal policy π is constructed by first generating a
collection Π of stationary policies, then sampling π uniformly at random from Π.

Interestingly, πran is sub-optimal for maximizing E[gmin( V[¯]1:[π]T [)]][. Indeed,][ Pr( ¯]V1:[π]T[ran] [=] 1 01/T ) =
_−_

Pr( V[¯]1:[π]T[ran] [=] 1−01/T ) = 1/2, so we have E[gmin( V[¯]1:[π]T[ran] [)] = 0][ for all][ T] [. Now, consider the deter-]  
ministic policy πsw, which first follows π[ℓ] for the first _T/2_ time steps, then follows π[r] in the
   _⌊_ _⌋_
remaining _T/2_ time steps. We have _V[¯]1:[π]T,k[sw]_
_gmin( V[¯]1:[π]T[sw][)] ⌈[ ≥]_ [1][/]⌉[2][ −] [2][/T] [. Note that][ g][min][(][E][[ ¯]V1:[π]T[sw][≥][])][ ≥][1][/][g][2][min][ −][(][E][2][/T][[ ¯]V1:[ for each][π]T[ran] [])][ −] [2][/T][ k][ ∈{][, so the policy][1][,][ 2][}][, meaning that][ π][sw][ is also]
near-optimal for maximizing gmin(E[ V[¯]1:[π]T [])][.]

Altogether, an optimal policy for maximizing gmin(E[ V[¯]1:[π]T [])][ can be far from optimal for maximizing]
E[gmin( V[¯]1:[π]T [)]][. In addition, for the latter objective, it is intuitive to imitate][ π][sw][, which is to partition]
the horizon into episodes and run a suitable stationary policy during each episode. A weakness to
_πsw is that its partitioning requires the knowledge on T_ . While our algorithm follows the intuition
to imitate πsw, we propose an alternate partitioning that allows does not require T as an input.

4 ONLINE-REOPT ALGORITHM FOR RL-MMF

We propose the Online-ReOpt algorithm, displayed in Algorithm 1. The algorithm runs in episodes.
An episode m ∈{1, 2, . . .} starts at time τ (m) (defined in Line 2), and ends at time τ (m + 1) − 1.
Before the start of episode m, the algorithm computes the scalarization ϑτ (m) based on the Multiplicative Weight Update (MWU), which we detail later. Then, the algorithm invokes the optimization oracle Λ, which returns a policy πm that is near-optimal for the MDP with scalar rewards
_rm = {rm(s, a)}s,a, where rm(s, a) = ϑ[⊤]τ_ (m)[v][(][s, a][)][. Note that we only assume a black-box access]
to Λ, and the parameters Dlin, β do not need to be input to the Algorithm. Finally, the algorithm runs
policy πm during episode m. The Online Re-Opt algorithm is an anytime algorithm, since it does
not require T as an input. Rather, it requires knowing T only during the terminal time step T . To
complete the description of the algorithm, we provide the details about the scalarization.


-----

**Algorithm 1 Online-ReOpt for gmin**

1: Inputs: Optimization oracle Λ.
2: Set τ (m) = ⌊m[3][/][2]⌋ for m ∈ N.
3: for Episode m = 1, 2, . . . do
4: Define ϑτ (m) according to (5).

5: Compute policy πm Λ(ϑτ (m)).
_←_

6: **for Time t = τ** (m), . . ., τ (m + 1) − 1 do

7: Choose action at = πm(st).

8: Observe the outcomes Vt(st, at) and the next state st+1.

9: **if t = T then**

10: Break the for loops and terminate the algorithm.

11: **end if**

12: **end for**

13: end for


**Scalarization by MWU. At a time step t, we define the scalarization ϑt = {ϑt,k}k[K]=1** [as]

exp _−ηt−1_ _tj−=11_ _[V][j,k][(][s][j][, a][j][)]_
_ϑt,k =_ _,_ (5)

_K_ h _t_ 1 i
_κ=1_ [exp] _−ηtP−1_ _j−=1_ _[V][j,κ][(][s][j][, a][j][)]_
h i

where P P

_√log K_
_ηt_ 1 = (6)
_−_ max{(t − 1)[2][/][3], 1} _[.]_

In particular, at the start of each episode m, we apply (5) with t = τ (m) in Line 4. For the case
_m = 1, we have ϑτ_ (1),k = 1/K for all k ∈{1, . . ., K}, meaning that all reward types are assigned
with the same weight at the beginning. The exponent 2/3 in the learning rate ητ (m) 1 in (5) is
_−_
different from the conventional choice of 1/2 (Arora et al., 2012). Our exponent is chosen for
optimizing the resulting regret bound from our forthcoming analysis. We follow the approach in
Chapter 7 in (Orabona, 2019) to define a time-varying learning rate.

The scalarization ϑt by (5) promotes max-min fairness. Consider two reward types k, k[′] with
_t−1_
_q=1_ _[V][q,k][(][s][q][, a][q][)][ >][ P]q[t][−]=1[1]_ _[V][q,k][′]_ [(][s][q][, a][q][)][. We have][ ϑ][t,k][′][ > ϑ][t,k][, meaning that a higher weight]
is assigned to reward type k[′] than type k. This implies that there is a higher emphasis on increasing
P
the type-k[′] reward, which is in shortage as compared to type k, than the type-k reward. Hence,
max-min fairness is promoted.

4.1 THEORETICAL GUARANTEES

We provide the following theoretical guarantee for Online-ReOpt.

**Theorem 1 Consider the RL-MMF problem. Online-ReOpt, displayed in Algorithm 1, satisfies**

_Reg(T_ ) + [144][D][lin] (7)
_≤_ [114]T[√][1][log][/][3] _[ K]_ _T_ _[β/][3][,]_

_where Dlin, β are parameters related to the optimization oracle Λ._

Theorem 1 is a generalization result, in the sense that it generalizes the ability of achieving nearoptimality for the case of K = 1 to the case of K ≥ 1. Indeed, as long as β > 0, meaning that
the regret of the optimization oracle Λ diminishes with a growing T on any RL with scalar reward
problem, the regret bound (7) in Theorem 1 also tends to zero as T increases.

Theorem 1 is proved in Appendix section C.3. The first regret term in (7) arises from two sources:
(a) the regret of the MWU algorithm, (b) the update delay on the scalarization due to the episodic
structure. To elaborate on (b), consider a time step t in episode m. Recall that the scalarization ϑt by
(5) promotes max-min fairness, and ideally we should have employed the policy returned by Λ(ϑt)
at time t. In contrast, in Online-ReOpt the action at is determined by πm, the output of Λ(ϑτ (m)).


-----

Item (b) accounts for the regret due to using ϑτ (m) rather then ϑt. We crucially use the fact that ϑt
are slowly changing in t so that the resulting regret is still diminishing with t.

The second term in (7) is due to the regret of the optimization oracle Λ. The exponent β/3 in the term
is less than the exponent β in (4), as each policy πm is run for only τ (m +1) _−_ _τ_ (m) = O( _τ_ (m))

many time steps. Our design of _τ_ (m) _m=1_ [allows a shorter time frame for][ π][m] [to converge to its]
_{_ _}[M]_ p
expected reward, as compared to running a policy for T steps in (4). When we increase the episode
length τ (m+1)−τ (m), the regret due to (b) increases, while the regret due to the second term in (7)
decreases, and vice versa. Our design of {τ (m)}m[M]=1 [strikes an optimal (in terms of our analysis)]
balance between these two sources of regret.

The regret bound (7) does not feature a direct dependence on the sizes of the state and action spaces.
The dependence on the hardness of the underlying MDP is only reflected through the parameters
_Dlin, β. Therefore, apart from the deterioration of the exponent β to β/3 and the first term in (7),_
our algorithm does not introduce any overhead in the generalization from the case of K = 1 to
the case of K ≥ 1. Improving the exponent β/3 in (7) is an interesting open question. Finally,
Theorem 1 is generalized to the case of maximizing a concave utility objective E[g( V[¯]1:[π]T [)]][, where]
_g is Lipschitz continuous and concave. We detail the generalization in the model, algorithm and_
theoretical results in Appendix C.1.

4.2 OFFLINE VARIANTS TO ONLINE-REOPT

While the Online-ReOpt Algorithm achieves near-optimality, the efficiency of its implementation
could be hindered by the need of online computation in Line 5 in Algorithm 1. Indeed, in order to
compute πm, the agent has to input the optimization oracle Λ and the scalarization ϑτ (m), which is
only known at the end of time step τ (m) − 1. In the case when the optimization oracle involves
heavy computation, for example training deep neural networks, such online computation may not be
realistic.

In this section, we propose Offline-ReOpt, which is a variant of Online-ReOpt that does not require
invoking Λ during the horizon. The Offline-ReOpt is obtained from the Online-ReOpt by modifying
two lines in Algorithm 1, as enumerated below. The full algorithm of Offline-ReOpt is provided in
Appendix section A.1.

1. Replace the input of Λ in Line 1 with the input of the policy family Π = {(ϑ, π[(][ϑ][)])}ϑ∈Ω.
The index set Ω is a finite subset of {ϑ ∈ R[K] : ∥ϑ∥1 = 1, ϑ ≥ 0}, the collection of all
possible scalarizations. For each ϑ ∈ Ω, the policy π[(][ϑ][)] is the output of Λ(ϑ).

2. Replace the online computation in Line 5 with these two lines:

(a) Identify _ϑ[˜]τ_ (m) ∈ Ω that achieves minϑ∈Ω _ϑ −_ _ϑτ_ (m) 1 _[.]_

(b) Select policy πm = π[( ˜]ϑτ (m)).

In item (1), all the policies in Π are computed before the execution of the algorithm, unlike the case
in Online-ReOpt. Consequently, in item (2), the selection of policy πm does not require invoking
the optimization oracle Λ.

The main idea behind item (2) is that, in the case when the desired scalarization ϑτ (m) does not lie in
Ω, we chooses the surrogate scalarization _ϑ[˜]τ_ (m) that is closest to ϑτ (m), so that the resulting policy
_π[( ˜]ϑτ_ (m)) will be a reasonable approximation to the desired policy π(ϑτ (m)).

In order for the surrogate _ϑ[˜]τ_ (m) to be close to ϑτ (m), it is desirable for the finite index set Ω to be so
diverse that every scalarization ϑτ (m) would be in a close neighborhood of a scalarization in Ω. We
propose two families of Ω for the desired diversification. The first is the random point family, detailed in Appendix section A.2. The family is constructed by sampling random points in the domain
_{ϑ ∈_ R[K] : ∥ϑ∥1 = 1, θ ≥ 0} of all possible scalarizations. The second is the imitation based fam**ily, also detailed in Appendix section A.2. The family is constructed by first running Online-ReOpt**
multiple times, then collecting the scalarizations and the corresponding policies generated.


-----

5 EXPERIMENTS

We evaluate our proposed algorithms and benchmark algorithms in a controlled queueing system
involving vectorial rewards. For each of the algorithms, we first run the algorithm for Zpo = Zan × Ξ
independent trials, resulting in the Zpo average vectorial rewards[2] _{V[¯]1:[(][z]T[an][,ξ][)]}1≤zan≤Zan,1≤ξ≤Ξ. We_
plot the following three quantities against T :

**Ex-post Fairness:** Ψ =[¯] _Z1an_ Ξ1 _Zzanan=1_ Ξξ=1 _[g][min]_ _V¯1:[(][z]T[an][,ξ][)]_, an estimate to E[gmin( V[¯]1:T )].

_•_

1 PZan P 1 Ξ  
**Ex-ante Fairness:** Γ =[¯] _Zan_ _zan=1_ _[g][min]_ Ξ _ξ=1_ _V[¯]1:[(][z]T[an][,ξ][)]_, an estimate to gmin(E[ V[¯]1:T ]).

_•_

**Type k rewards for each 1P** _k_ _K:_ Φ[¯] _k =P_ _Z1an_ Ξ1 _Zzanan=1_ Ξξ=1 _V[¯]1:[(][z]T,k[an][,ξ][)][, an estimate to]_

_•_ _≤_ _≤_

E[ V[¯]1:T,k].
P P


We define the upper and lower error bars respectively as the 75 and 25-percentiles of the data, see
Appendix section B.1 for details. For the forthcoming discussions, we denote ek as the k-th standard
basis vector for k ∈{1, . . . K} in R[K]. In addition, we denote 1K, 0K as the all one vector and the
all zero vector in R[K].

5.1 QUEUING NETWORK

Queuing problems are studied extensively due to their relevance in fields such as manufacturing and
in communication systems. In our evaluation, we focus on a discrete-time queuing system. The
queuing network that we have tested our algorithms on, consisting of two servers and four queues
arranged in a bidirectional fashion, has been previously studied in works by Rybko & Stolyar (1992),
Kumar & Seidman (1990), Chen & Meyn (1998), de Farias & Van Roy (2003) and Banijamali et al.
(2019). This network is shown in Figure 2.

Figure 2: A bi-directional four-queue network

There are two servers in the system. Server 1 only serves Queue 1 or Queue 4 with service rates
_µ1 = 0.3 and µ4 = 0.3 respectively, and where Server 2 similarly only serves Queue 2 or Queue 3_
at the rates of µ2 = 0.3 and µ3 = 0.3. Arrivals occur at a rate of λ1 = 0.2 and λ2 = 0.2 at Queues
1 and 3 respectively. An arrival that gets served at Queue 1 by Server 1 then progresses to Queue 2,
and only leaves the system after it has been served by Server 2. Likewise, arrivals at Queue 3 have
to be served by Server 2, before moving on to Queue 4 to be served by Server 1 in order to leave
the system. Each queue i has a maximum length of Li = 9, and a customer is rejected at a queue if
the queue is at its full capacity. Conversely, an empty queue remains at length 0 even if an action is
taken to serve that queue.

The state of the system is thus defined by the vector xt = (xt,1, xt,2, xt,3, xt,4) whereby xt,i represents the length of the queue i at time t. At each time step t, a decision has to be made by each
server to serve only one or neither of its queues, which we can represent by a 4-component vector
**_at = (at,1, at,2, at,3, at,4) ∈{0, 1}[4], where at,i = 1 indicates the decision to serve Queue i at time_**
_t, and at,i = 0 otherwise. Note that the condition of being able to only serve one queue at each_
server naturally imposes the constraints at,1 + at,4 ≤ 1 and at,2 + at,3 ≤ 1 at each t, meaning that
_As = {a ∈{0, 1}[4]_ : a1 + a4 ≤ 1, a2 + a3 ≤ 1} for each s.

2To avoid clutter, we omit the upper-script for the algorithm.


-----

The transition dynamics for the system can then defined by the following equation when 0 < xt,i <
_Li, where ei refers to the basis vector in R[4]:_


**_xt + e1_** with probability λ1
**_xt + e3_** with probability λ2
**_xt + e2 −_** **_e1_** with probability µ1a1
**_xt_** **_e2_** with probability µ2a2
_−_
**_xt + e4 −_** **_e3_** with probability µ3a3
**_xt_** **_e4_** with probability µ4a4
_−_
**_xt_** otherwise


(8)


**_xt+1 =_**


We define the type-i reward at time t as Vt,i(x, a) = 1 − _[x]L[t,i]i_ [, for][ i][ ∈{][1][, . . .,][ 4][}][. Recall that][ x][t,i][ is]

the queue length of Queue i at time t. The reward Vt,i(x, a) is equal to 1 if Queue i is empty, and the
reward Vt,i(x, a) decreases linearly with the length of Queue i at time t. In particular, Vt,i(x, a) = 0
if Queue i is full. Altogether, the agent’s reward for Queue i at time t positively correlates with the
degree of idleness of the Queue. The maximization of gmin( V[¯]1:T ) = min1 _i_ 4 _V[¯]1:T,i is equivalent_
_≤_ _≤_
to the minimization of time-average queue lengths among all queues, hence enforcing all queues to
be stable simultaneously.

5.1.1 SIMULATION RESULTS

In our simulation, we evaluate 5 algorithms. Three of them are our proposed algorithms, namely
Online-ReOpt, Offline-ReOpt with Random Point Family and Offline-ReOpt with Imitation based
family. The other two are existing baselines. The Meta Algorihtm by Zahavy et al. (2021) is
the state-of-the-art for maximizing gmin(E[ V[¯]1:T ]), while the longer queue first heuristic is a wellestablished algorithm in the queuing theory literature. As the name suggests, each server serves
the longer of the two queues at each time round. We ran each of the algorithms with the following
parameters:gorithms employ the same optimization oracle T = 100000, Zan = 10 and Ξ = 100 Λ, with the same hyper-parameters and architecture,, meaning Zpo = Zan × Ξ = 1000.[3] All of the ala double deep Q-learning network algorithm (Double DQN) by Hasselt et al. (2016).

Figure 3: Ex-ante Fairness of various algorithms in a queuing network

Figure 3 plots the quantity Ψ[¯] against T under the 5 algorithms. Notice in Figure 3 how the Offline and Online-ReOpt algorithms, as well as the Meta Algorithm by Zahavy et al. (2021) perform
similarly well in terms of ex-ante fairness. Among them, the Meta Algorithm has the best performance, since the Re-Optimization schedule in our proposed algorithms compromises the ex-ante
fairness objective. All algorithms demonstrate converging behavior, in the sense that the error bars
diminishes as T grows.

3Except for Online-ReOpt, where we set Ξ = 5 since running an online algorithm for 1000 trials is not as
practical as running an offline algorithm, which only needs to be trained once.


-----

Figure 4: Ex-post Fairness of various algorithms in a queuing network

Figure 4 plots the quantity Γ[¯] against T under the 5 algorithms. In terms of ex-post fairness, the Offline and Online-ReOpt algorithms perform significantly better than Meta Algorithm and the Longer
Queue First Heuristic. The sub-optimality of the Meta Algorithm corroborates with Section 3.2 that
policies designed for the ex-ante fairness objective could be far from optimal for the ex-post fairness
objective. While the Meta Algorithm has a similar performance to the Longer Queue First heuristic,
the former has a significantly wider error bar than the latter, meaning that the latter is more stable.

Figure 5: Type k-rewards for 1 ≤ _k ≤_ _K of various algorithms in a queuing network. Figure is read_
from left to right, top to bottom.

Figure 5 plots Φi against T for i ∈{1, . . ., 4}. In a nutshell, the plotted lines explain the trends in
Figure 3, while the error bars shed light on the trends in Figure 4. Firstly, the plotted lines indicate
that the Meta Algorithm has the highest (or close to the highest) individual average reward Φi for
each queue, signifying that the Meta Algorithm has the highest E[ V[¯]1:T,i] for each i ∈{1, . . ., 4}.
This explains the superiority of the Meta Algorithm shown in Figure 3.

When we focus on the error bars, the plots in Figure 5 tell a different story. Notably, the error bars
for the Meta Algorithm is significantly wider than others, meaning that the Zpo trials of the Meta
Algorithm have significantly different results from one another.[4] When we unpack the summands
in Φ1, . . ., Φ4 and compute the minimum reward in each trial, it results in Figure 4, which is vastly
different from Figure 3, signifying the ex-ante and ex-post objectives are fundamentally different.

As a final remark, our numerical experiments do not imply that the Longer Queue Heuristic is a
worse algorithm than the other 4 algorithms. Indeed, the Longer Queue Heuristic does not require
the knowledge of λ1, λ2, µ1, . . ., µ4, whereas the other 4 algorithms crucially uses these parameters
for generating their policies. In addition, the Longer Queue Heuristic is computationally much less
onerous than the others. Finally, the Longer Queue Heuristic demonstrates converging behaviors in
all the plots, in the sense that the error bars diminish when T increases.

4It is helpful to revisit Section 3.2, where Zpo trials would result in _Zpo/2 outcomes of_ 1 01/T and

_Zpo/2 outcomes of_ 1−01/T . _≈_   _−_ 
_≈_
  


-----

REFERENCES

E. Altman. Constrained Markov Decision Processes. Chapman and Hall, 1999.

Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a metaalgorithm and applications. Theory of Computing, 8(6):121–164, 2012.

Ershad Banijamali, Yasin Abbasi-Yadkori, Mohammad Ghavamzadeh, and Nikos Vlassis. Optimizing over a restricted policy class in mdps. In Proceedings of the Twenty-Second International
_Conference on Artificial Intelligence and Statistics, volume 89, pp. 3042–3050. PMLR, 16–18_
Apr 2019.

Leon Barrett and Srini Narayanan. Learning all optimal policies with multiple criteria. In Proceed_ings of the 25th International Conference on Machine Learning, ICML ’08, pp. 41–47, New_
York, NY, USA, 2008. Association for Computing Machinery. ISBN 9781605582054. doi:
[10.1145/1390156.1390162. URL https://doi.org/10.1145/1390156.1390162.](https://doi.org/10.1145/1390156.1390162)

Kiant´e Brantley, Miroslav Dud´ık, Thodoris Lykouris, Sobhan Miryoosefi, Max Simchowitz, Aleksandrs Slivkins, and Wen Sun. Constrained episodic reinforcement learning in concave-convex
and knapsack settings. In Advances in Neural Information Processing Systems 33, 2020.

R.-R. Chen and S. Meyn. Value iteration and optimization of multiclass queueing networks. In Pro_ceedings of the 37th IEEE Conference on Decision and Control (Cat. No.98CH36171), volume 1,_
pp. 50–55 vol.1, 1998. doi: 10.1109/CDC.1998.760588.

Wang Chi Cheung. Regret minimization for reinforcement learning with vectorial feedback and
complex objectives. In Advances in Neural Information Processing Systems 32, pp. 724–734,
2019.

Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained reinforcement learning with percentile risk criteria. J. Mach. Learn. Res., 18(1):6070–6120, January
2017. ISSN 1532-4435.

D. P. de Farias and B. Van Roy. The linear programming approach to approximate dynamic programming. Operations Research, 51(6):850–865, 2003.

Zolt´an G´abor, Zsolt Kalm´ar, and Csaba Szepesv´ari. Multi-criteria reinforcement learning. In Pro_ceedings of the Fifteenth International Conference on Machine Learning, ICML ’98, pp. 197–205,_
San Francisco, CA, USA, 1998. Morgan Kaufmann Publishers Inc. ISBN 1558605568.

Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double qlearning. In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI’16,
pp. 2094–2100. AAAI Press, 2016.

Elad Hazan, Sham Kakade, Karan Singh, and Abby Van Soest. Provably efficient maximum entropy exploration. In Proceedings of the 36th International Conference on Machine Learning,
volume 97, pp. 2681–2691. PMLR, 09–15 Jun 2019.

P.R. Kumar and T.I. Seidman. Dynamic instabilities and stabilization methods in distributed realtime scheduling of manufacturing systems. _IEEE Transactions on Automatic Control, 35(3):_
289–298, 1990. doi: 10.1109/9.50339.

Hoang Le, Cameron Voloshin, and Yisong Yue. Batch policy learning under constraints. In Pro_ceedings of the 36th International Conference on Machine Learning, volume 97, pp. 3703–3712._
PMLR, 09–15 Jun 2019.

Lisa Lee, Benjamin Eysenbach, Emilio Parisotto, Eric P. Xing, Sergey Levine, and Ruslan Salakhutdinov. Efficient exploration via state marginal matching. CoRR, abs/1906.05274, 2019. URL
[http://arxiv.org/abs/1906.05274.](http://arxiv.org/abs/1906.05274)

Yongshuai Liu, Jiaxin Ding, and Xin Liu. Ipo: Interior-point policy optimization under constraints. _Proceedings of the AAAI Conference on Artificial Intelligence, 34(04):4940–4947,_
[Apr. 2020. doi: 10.1609/aaai.v34i04.5932. URL https://ojs.aaai.org/index.php/](https://ojs.aaai.org/index.php/AAAI/article/view/5932)
[AAAI/article/view/5932.](https://ojs.aaai.org/index.php/AAAI/article/view/5932)


-----

Shie Mannor and Nahum Shimkin. A geometric approach to multi-criterion reinforcement learning.
_J. Mach. Learn. Res., 5:325–360, 2004._

Sobhan Miryoosefi, Kiant´e Brantley, Hal Daume III, Miro Dudik, and Robert E Schapire. Reinforcement learning with convex constraints. In Advances in Neural Information Processing Systems
_32, pp. 14093–14102. 2019._

Francesco Orabona. A modern introduction to online learning. CoRR, abs/1912.13213, 2019.

A. N. Rybko and Aleksandr Stolyar. On the ergodicity of stochastic processes describing functioning
of open queueing networks. Problemy Peredachi Informatsii, (3):3–26, July 1992. ISSN 05552923.

Shai Shalev-Shwartz. Online learning: Theory, algorithms, and applications, Jul 2007.

Shai Shalev-Shwartz. Online learning and online convex optimization. Found. Trends Mach. Learn.,
4(2):107–194, 2012.

Jean Tarbouriech and Alessandro Lazaric. Active exploration in markov decision processes. In The
_22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, volume 89,_
pp. 974–982. PMLR, 2019.

Chen Tessler, Daniel J. Mankowitz, and Shie Mannor. Reward constrained policy optimization. In
_International Conference on Learning Representations, 2019._

Kristof Van Moffaert and Ann Now´e. Multi-objective reinforcement learning using sets of pareto
dominating policies. J. Mach. Learn. Res., 15(1):3483–3512, January 2014. ISSN 1532-4435.

Tom Zahavy, Brendan O’Donoghue, Guillaume Desjardins, and Satinder Singh. Reward is enough
for convex mdps. ArXiv, abs/2106.00661, 2021.


-----

