# ON THE BENEFITS OF MAXIMUM LIKELIHOOD
### ESTIMATION FOR REGRESSION AND FORECASTING

**Pranjal Awasthi, Abhimanyu Das, Rajat Sen & Ananda Theertha Suresh**
Google Research
{pranjalawasthi, abhidas, senrajat, theertha}@google.com

ABSTRACT

We advocate for a practical Maximum Likelihood Estimation (MLE) approach
towards designing loss functions for regression and forecasting, as an alternative
to the typical approach of direct empirical risk minimization on a specific target
metric. The MLE approach is better suited to capture inductive biases such as prior
domain knowledge in datasets, and can output post-hoc estimators at inference time
that can optimize different types of target metrics. We present theoretical results
to demonstrate that our approach is competitive with any estimator for the target
metric under some general conditions. In two example practical settings, Poisson
and Pareto regression, we show that our competitive results can be used to prove
that the MLE approach has better excess risk bounds than directly minimizing
the target metric. We also demonstrate empirically that our method instantiated
with a well-designed general purpose mixture likelihood family can obtain superior
performance for a variety of tasks across time-series forecasting and regression
datasets with different data distributions.

1 INTRODUCTION

The task of fitting a regression model for a response variable y against a covariate vector x ∈ R[d]
is ubiquitous in supervised learning in both linear and non-linear settings (Lathuilière et al., 2020;
Mohri et al., 2018) as well as non-i.i.d settings like multi-variate forecasting (Salinas et al., 2020;
Wang et al., 2019). The end goal in regression and forecasting problems is often to use the resulting
model to obtain good performance in terms of some target metric of interest on the population level
(usually measured on a previously unseen test set). The mean-squared error or the mean absolute
deviation are examples of common target metrics.

In this paper, our focus is on the choice of loss function that is used to train such models, which is an
important question that is often overlooked, especially in the deep neural networks context where the
emphasis is more on the choice of network architecture (Lathuilière et al., 2020).

Perhaps the most common method used by practitioners for choosing the loss function for a particular
regression model is to directly use the target metric of interest as the loss function for empirical risk
_minimization (ERM) over a function class on the training set. We denote this approach for choosing a_
loss function as Target Metric Optimization (TMO). This is especially more common with the advent
of powerful general purpose function optimizers like deep networks and has also been rigorously
analyzed for simpler function classes (Mohri et al., 2018).

Target Metric Optimization seems like a reasonable approach - if the practitioner knows about the
target metric of interest for prediction using the model, it seems intuitive to optimize for the same
objective on the training data. Prior work (both theoretical and applied) has both advocated for and
argued against TMO for regression problems. Many prominent works on regression (Goldberger
et al., 1964; Lecue & Mendelson, 2016) use the TMO approach, though most of them assume that
the data is well behaved (e.g. sub-Gaussian noise). In terms of applications, many recent works on
time-series forecasting (Wu et al., 2020; Oreshkin et al., 2019; Sen et al., 2019) also use the TMO
approach directly on the target metric. On the other hand, the robust regression literature has long
advocated for not using the target metric directly for ERM in the case of contamination or heavy
tailed response/covariate behaviour (Huber, 1992; Hsu & Sabato, 2016; Zhu & Zhou, 2021; Lugosi
& Mendelson, 2019a; Audibert et al., 2011; Brownlees et al., 2015) on account of its suboptimal


-----

high-probability risk bounds. However, as noted in (Prasad et al., 2020), many of these methods
are either not practical (Lugosi & Mendelson, 2019a; Brownlees et al., 2015) or have sub-optimal
empirical performance (Hsu & Sabato, 2016). Even more practical methods such as (Prasad et al.,
2020) would lead to sufficiently more computational overhead over standard TMO.

Another well known approach for designing a loss function is Maximum Likelihood Estimation (MLE).
Here one assumes that the conditional distribution of y given x belongs to a family of distributions
_p(y|x; θ) parameterized by θ ∈_ Θ (McCullagh & Nelder, 2019). Then one can choose the negative
log likelihood as the loss function to optimize using the training set, to obtain an estimate _θθ[ˆ]θmle. This_
approach is sometimes used in the forecasting literature (Salinas et al., 2020; Davis & Wu, 2009)
where the choice of a likelihood can encode prior knowledge about the data. For instance a negative
binomial distribution can be used to model count data. During inference, given a new instance x[′],
one can output the statistic from p(y **_x[′]; θθ[ˆ]θmle) that optimizes the target metric, as the prediction_**
_|_
value (Gneiting, 2011). MLE also seems like a reasonable approach for loss function design - it is
folklore that the MLE is asymptotically optimal for parameter estimation, in terms of having the
smallest asymptotic variance among all estimators (Heyde, 1978; Rao, 1963), when the likelihood is
well-specified. However, much less is known about finite-sample, fixed-dimension analysis of MLE,
which is the typical regime of interest for the regression problems we consider in this paper. An
important practical advantage for MLE is that model training is agnostic to the choice of the target
metric - the same trained model can output estimators for different target metrics at inference time.
Perhaps the biggest argument against the MLE approach is the requirement of knowing the likelihood
distribution family. We address both these topics in Section 5.

Both TMO and MLE can be viewed as offering different approaches to selecting the loss function
for a given regression model. In this paper, we argue that in several settings, both from a practical
and theoretical perspective, MLE might be a better approach than TMO. This result might not be
immediately obvious apriori - while MLE does benefit from prior knowledge of the distribution,
TMO also benefits from prior knowledge of the target metric at training time.

Our main contributions are as follows:

**Competitiveness of MLE: In Section 3, we prove that under some general conditions on the family**
of distributions and a property of interest, the MLE approach is competitive with any estimator for
the property. We show that this result can be applied to fixed design regression problems in order to
prove that MLE can be competitive (up to logarithmic terms) with any estimator in terms of excess
square loss risk, under some assumptions.

**Example Applications: In Section 4.1, we apply our general theorem to prove an excess square loss**
bound for the the MLE estimator for Poisson regression with the identity link (Nelder & Wedderburn,

1972; Lawless, 1987). We show that these bounds can be better than those of the TMO estimator,
which in this case is least-squares regression. Then in Section 4.2, we show a similar application
in the context of Pareto regression i.e y|x follows a Pareto distribution. We show that MLE can be
competitive with robust estimators like the one in (Hsu & Sabato, 2016) and therefore can be better
than TMO (least-squares).

**Empirical Results:** We propose the use of a general purpose mixture likelihood family (see
Section 5) that can capture a wide variety of prior knowledge across datasets, including zero-inflated
or bursty data, count data, sub-Gaussian continuous data as well as heavy tailed data, through different
choices of (learnt) mixture weights. Then we empirically show that the MLE approach with this
likelihood can outperform ERM for many different commonly used target metrics like WAPE, MAPE
and RMSE (Hyndman & Koehler, 2006) for two popular forecasting and two regression datasets.
Moreover the MLE approach is also shown to have better probabilistic forecasts (measured by
quantile losses (Wen et al., 2017)) than quantile regression (Koenker & Bassett Jr, 1978; Gasthaus
et al., 2019; Wen et al., 2017) which is the TMO approach in this case.

2 PRIOR WORK ON MLE

Maximum likelihood estimators (MLE) have been studied extensively in statistics starting with
the work of Wald (1949); Redner (1981), who showed the maximum likelihood estimates are
asymptotically consistent for parametric families. Fahrmeir & Kaufmann (1985) showed asymptotic


-----

normality for MLE for generalized linear models. It is also known that under some regularity
assumptions, MLE achieves the Cramer-Rao lower bound asymptotically (Lehmann & Casella, 2006).
However, we note that none of these asymptotic results directly yield finite sample guarantees.

Finite sample guarantees have been shown for certain problem scenarios. Geer & van de Geer (2000);
Zhang (2006) provided uniform convergence bounds in Hellinger distance for maximum likelihood
estimation. These ideas were recently used by Foster & Krishnamurthy (2021) to provide algorithms
for contextual bandits. There are other works which study MLE for non-parametric distributions e.g.,
Dümbgen & Rufibach (2009) showed convergence rates for log-concave distributions. There has
been some works (Sur & Candès, 2019; Bean et al., 2013; Donoho & Montanari, 2016; El Karoui,
2018) that show that MLE can be sub-optimal for high dimensional regression i.e when the dimension
grows with the number of samples. In this work we focus on the setting where the dimension does
not scale with the number of samples.

Our MLE results differ from the above work as we provide finite sample competitiveness guarantees.
Instead of showing that the maximum likelihood estimator converges in some distance metric, we
show that under some mild assumptions it can work as well as other estimators. Hence, our methods
are orthogonal to known well established results in statistics.

Perhaps the closest to our work is the competitiveness result of Acharya et al. (2017), who showed
that MLE is competitive when the size of the output alphabet is bounded and applied to profile
maximum likelihood estimation. In contrast, our work applies for unbounded output alphabets and
can provide stronger guarantees in many scenarios.

3 COMPETITIVENESS OF MLE

In this section, we will show that under some reasonable assumptions on the likelihood family, the
MLE is competitive with any estimator in terms of estimating any property of a distribution from the
family. We will then show that this result can be applied to derive bounds on the MLE in some fixed
design regression settings that can be better than that of TMO. We will first setup some notation.

**Notation: Given a positive semi-definite symmetric matrix M**, ∥x∥M := x[T] **_Mx is the matrix_**
norm of the vector x. λ(M ) denotes an eigenvalue of a symmetric square matrix M ; specifically
_λmax(M_ ) and λmin(M ) denote the maximum and minimum eigenvalues respectively. The letter f
is used to denote general probability densities. We use p to denote the conditional probability density
of the response given the covariate. ∥·∥1 will be overloaded to denote the ℓ1 norm between two
probability distributions i.e ∥p − _p[′]∥1 :=_ _|p(z) −_ _p[′](z)|dz. DKL(p1; p2) will be used to denote the_
will denote anKL-divergence between the two distributions. If ϵ-net i.e any point z ∈Z has a corresponding pointR _Z is a set equipped with a norm z[′]_ _∈N_ (ϵ, Z ∥·∥) s.t, then ∥z − Nz[′]∥≤(ϵ, Zϵ).
B[d]r [denotes the ball centered at the origin with radius][ r][ and][ S]r[d][−][1] denotes its surface. We define

[n] := {1, 2, · · ·, n}. | · | denotes the size of the enclosed set.

**General Competitiveness:** We first consider a general family of distributions F over the space
_Z. For a sample z ∼_ _f (for z ∈Z and f ∈F), the MLE distribution is defined as fz =_
argmaxf _f_ (z). We are interested in estimating a property π : of these distributions from
_∈F_ _F →W_
an observed sample. The following definition will be required to impose some joint conditions on the
distribution family and the property being estimated, that are needed for our result.
**Definition 1. The tuple (F, π), where F is a set of distributions and π : F →W a property**
_of those distributions, is said to be (T, ϵ, δ1, δ2)-approximable, if there exists a set of distribu-_
_tions_ _F ⊆F[˜]_ _s.t |F| ≤[˜]_ _T and for every f ∈F, there exists a_ _f[˜] such that ∥f −_ _f[˜]∥1 ≤_ _δ1 and_

Prz∼f _π(fz) −_ _π( f[˜]z)_ _≥_ _ϵ_ _≤_ _δ2, where_ _f[˜]z = argmax ˜f_ _∈F[˜]_ _f[˜](z) and W has a norm ∥·∥._
 

The above definition states that the set of distributions has a finite δ-cover, [˜] in terms of the ℓ1
_F_ _F_
distance. Moreover the cover is such that solving MLE on the cover and applying the property π on
the result of the MLE is not too far from π applied on the MLE over the whole set F. This property
is satisfied trivially if F is finite. We note that it is also satisfied by some commonly used set of
distributions and corresponding properties. Now we state our main result.
**Theorem 1. Let ˆπ be an estimator such that for any f ∈F and z ∼** _f_ _, Pr(∥π(f_ ) − _πˆ(z)∥≥_ _ϵ) ≤_ _δ._
_Let Ff be a subset of F that contains f such that with probability at least 1_ _−_ _δ, fz ∈Ff and (Ff_ _, π)_


-----

_is (T, ϵ, δ1, δ2)-approximable. Then the MLE satisfies the following bound,_

Pr(∥π(f ) − _π(fz)∥≥_ 3ϵ) ≤ (T + 3)δ + δ1 + δ2.

We provide the proof of Theorem 1 in Appendix A. We also provide a simpler version of this result
for finite distribution families as Theorem 3 in Appendix A for the benefit of the reader.

**Competitiveness in Fixed Design Regression:** Theorem 1 can be used to show that MLE is
competitive with respect to any estimator for square loss minimization in fixed design regression. We
will first formally introduce the setting. Consider a fixed design matrix X ∈ R[n][×][d] where n is the
number of samples and d the feature dimension. We will work in a setting where n ≫ _d. The target_
vector is a random vector given by y[n] _∈_ R[n]. Let yi be the i-th coordinate of y[n] and xi denote the
_i-th row of the design matrix. We assume that the target is generated from the conditional distribution_
given xi such that,

_yi_ _p(_ **_xi;_** _θθθ[∗]), θθθ[∗]_ Θ.
_∼_ _·|_ _∈_

We are interested in optimizing a target metric ℓ(·, ·) given an instance of the random vector y[n]. The
final objective is to optimize,


min _θθ[∗])_
_h∈H_ [E][y][i][∼][p][(][·|][x][i][;][θ]


_ℓ(yi, h(xi))_
_i=1_

X


where H is a class of functions. In this context, we are interested in comparing two approaches.

**TMO (see (Mohri et al., 2018)).** This is standard empirical risk minimization on the target metric where given an instance of the random vector y[n] one outputs the estimator _h[ˆ] =_
minh∈H _n[1]_ _ni=1_ _[ℓ][(][y][i][, h][(][x][i][))][.]_

**MLE and post-hoc inferenceP** (see (Gneiting, 2011)). In this method one first solves for the parameter
in the distribution family that best explains the empirical data by MLE i.e.,


_θθˆθmle := argminθ∈Θ_ _L(y[n]; θ), where L(y[n]; θ) :=_


log p(yi **_xi; θ)_**
_−_ _|_
_i=1_

X


Then during inference given a sample **_xi_** the predictor is defined as, _h˜(xi)_ :=
argminyˆ [E]y _p(_ **_xi;θθθ[ˆ]mle)[[][ℓ][(][y,][ ˆ]y)] or in other words we output the statistic from the MLE distribu-_**
_∈_ _·|_

tion that optimizes the loss function of interest. For instance if ℓ is the square loss, then _h[˜](xi) will be_
the mean of the conditional distribution p(y **_xi; θθ[ˆ]θmle)._**
_|_

We will prove a general result using Theorem 1 when the target metric ℓ is the square loss and H is a
linear function class. Moreover, the true distribution p(·|xi; _θθθ[∗]) is such that E[yi] = ⟨θθθ[∗], xi⟩_ for all
_i ∈_ [n] i.e we are in the linear realizable setting.

In this case the quantity of interest is the excess square loss risk given by,


(θ) := [1]
_E_ _n_


_n_

_i=1_ Eyn _∥y[n]_ _−_ **_Xθ∥2[2]_** _[−]_ _n[1]_

X


Eyn _∥y[n]_ _−_ **_Xθθθ[∗]∥2[2]_** [=][ ∥][θ][ −] _[θ]θθ[∗]∥Σ[2]_ _[,]_ (1)
_i=1_

X


where Σ := ([P]i **_[x][i][x]i[T]_** [)][/n][ is the normalized covariance matrix, and][ θ]θθ[∗] is the population minimizer

of the target metric over the linear function class. Now we are ready to state the main result.

**Theorem 2. Consider a fixed design regression setting where the likelihood family is parameterized**
_by θ ∈_ Θ ⊆ B[d]w _[and][ |N]_ [(][ϵ,][ Θ][ ∩] [B]w[d] [)][| ≤|N] [(][ϵ,][ B][d]w[)][|][ for a small enough][ ϵ][. Further the following]
_conditions hold,_

_1. DKL(p(yi; θ); p(yi; θ[′]))_ _L_ **_θ_** **_θ[′]_** 2.
_≤_ _∥_ _−_ _∥_
_2. The negative log-likelihood L(y[n]; θ) as a function of θ is α-strongly convex and β-smooth, w.p._
_at least 1 −_ _δ._

_Further suppose there exists an estimator θest such that E(θest) ≤_ (c1 + c2 log(1/δ))[η]/n, where
_c1, c2 are problem dependent quantities and η > 0. Then the MLE estimator also satisfies,_


-----

_η_



_c1 + c2d_ log n + log(wLλmax(Σ)) + log(β/α) + log [1]δ
 


(θθ[ˆ]θmle) = O
_E_

_w.p at least 1 −_ _δ._


We provide the proof in Appendix C. The proof involves proving the conditions in Theorem 1 and
bounding the size of the cover T .

In order to better understand Theorem 2, let us consider a typical case where there exists a possibly
complicated estimator such that (θest) = O((d + log(1/δ))/n). In this case the above theorem
_E_
implies that MLE will be competitive with this estimator up to a log n factor. In many cases the MLE
might be much simpler to implement than the original estimator but would essentially match the
same error bound. We now provide specific examples in subsequent sections.

4 APPLICATIONS OF COMPETITIVENESS RESULT

In this section we will specialize to two examples, Poisson regression and Pareto regression, where
we show that MLE can be better than TMO through the use of our competitive result in Theorem 2.

4.1 POISSON REGRESSION

We work in the fixed design setting in Section 3 and assume that the conditional distribution of y|x is
Poisson i.e,

_i_ _[e][−][µ][i]_
_p(yi = k_ **_xi;_** _θθθ[∗]) =_ _[µ][k]_ where, µi = _θθθ[∗], xi_ _> 0,_ (2)
_|_ _k!_ _⟨_ _⟩_

for all i ∈ [n]. Poisson regression is a popular model for studying count data regression which
naturally appears in many applications like demand forecasting (Lawless, 1987). Note that here we
study the version of Poisson regression with the identity link function (Nelder & Wedderburn, 1972),
while another popular variant is the one with exponential link function (McCullagh & Nelder, 2019).
We choose the identity link function for a fair comparison of the two approaches as it is realizable
for both the approaches under the linear function class i.e the globally optimal estimator in terms
of population can be obtained by both approaches. The exponential link function would make the
problem non-realizable under a linear function class for the TMO approach.

We make the following natural assumptions. Let Σ = ([P][n]i=1 **_[x][i][x]i[T]_** [)][/n][ be the design covariance]
matrix as before and M = ([P][n]i=1 _[µ][i][u][i][u]i[T]_ [)][/n][, where][ u][i][ =][ x][i][/][∥][x][i][∥]2[. Let][ χ][ and][ ζ][ be the condition]
numbers of the matrices M and Σ respectively.
**Assumption 1. The parameter space Θ and the design matrix X satisfy the following,**

_• (A2) (A1) The design matrix is such that The parameter space Θ = {θ ∈ λminR[d](Σ): ∥θ >∥2 0 ≤ andw, min(xi_ 2∥θ∥R2[2][,][ ⟨] for all[θ][,][ x][i][⟩] i[)][ ≥] [[γ >]n]. [ 0][,][ ∀][i][ ∈] [[][n][]][}][.]

_•_ _R[2]_ _∥_ _∥_ _≤_ _∈_

_• (A3) Let λmin(M_ ) ≥ 4nγ[2][ (][d][ log(24][χ][) + log(1][/δ][))][ and] _λmax(M_ )(d log(24χ) + log(1/δ)) ≤

_√nλmin(M_ )/16, for a small δ (0, 1) 1.
_∈_ p

The above assumptions are fairly mild. For instance λmin is Ω(1[˜] _/d) for random Gaussian covariance_
matrices (Bai & Yin, 2008). The other part merely requires that λmin(M ) = Ω([˜] _dλmax(M_ )/n).

We are interested in comparing MLE with TMO for the square loss which is just the least-squaresp
estimator i.e _θθ[ˆ]θls := argminθ_ Θ _n1_ _[∥][y][n][ −]_ **_[Xθ][∥]2[2][. Note that it is apriori unclear as to which approach]_**
_∈_
would be better in terms of the target metric because on one hand the MLE method knows the
distribution family but on the other hand TMO is explicitly geared towards minimizing the square
loss during training.

Least squares analysis is typically provided for regression under sub-Gaussian noise. By adapting
existing techniques (Hsu et al., 2012), we show the following guarantee for Poisson regression with
least square loss. We provide a proof in Appendix D for completeness.

1Note that the constants can be further tightened in our analysis.


-----

**Lemma 1. Let µmax = maxi µi. The least squares estimator** _θθ[ˆ]θls satisfies the following loss bounds_
_w.p. at least 1 −_ _δ,_

_O_ _µmaxn_ log [1]δ [+][ d] _if µmax_ (log(1/δ) + d log 6)/2

_E(θθ[ˆ]θls) =_ (O  _n1_ log  [1]δ [+][ d] 2[] _otherwise ≥_

   

Now we present our main result in this section which uses the competitiveness bound in Theorem 2
coupled with the existence of a superior estimator compared to TMO, to show that the MLE estimator
can have a better bound than TMO.

In Theorem 4 (in Appendix F), under some mild assumptions on the covariates, we construct an
estimator θest with the following bound for the Poisson regression setting,

_d + log( 1δ_ [)]
(θest) _c_ _θθθ[∗]_ _λmax(Σ)_ _._ (3)
_E_ _≤_ _· ∥_ _∥[2]_ _·_ _n_
 

The construction of the estimator is median-of-means tournament based along the lines of (Lugosi &
Mendelson, 2019a) and therefore the estimator might not be practical. However, this immediately
gives the following bound on the MLE as a corollary of Theorem 2.
**Corollary 1. Under assumption 1 and the conditions of Theorem 4, the MLE estimator for the**
_Poisson regression setting satisfies w.p. at least 1 −_ _δ,_

_δ_ [)]

(θθ[ˆ]θmle) = O _θθθ[∗]_ _λmax(Σ)_ _[d][(log][ n][ + log(][wRλ][max][(Σ)) + log][ χ][ + log][ 1]_ _._
_E_ _∥_ _∥[2]_ _·_ _n_
 

The bound in Corollary 1 can be better than the bound for _θθ[ˆ]θls in Lemma 1. In the sub-Gaussian_
region, the bound in Lemma 1 scales linearly with µmax which can be prohibitively large even when
a few covariates have large norms. The bound for the MLE estimator in Corollary 1 has no such
dependence. Further, in the sub-Exponential region the bound in Lemma 1 scales as _O[˜](d[2]/n) while_
the bound in Corollary 1 has a _O[˜](d/n) dependency, up to log-factors. In Appendix G, we also show_
that when the covariates are one-dimensional, an even sharper analysis is possible, that shows that
the MLE estimator is always better than least squares in terms of excess risk. In Appendix I.8, we
perform a simulated experiment that shows a linear growth of the test error w.r.t λmax(Σ), further
showing the efficacy of our theoretical bounds.

4.2 PARETO REGRESSION

Now we will provide an example of a heavy tailed regression setting where it is well-known that
TMO for the square loss does not perform well (Lugosi & Mendelson, 2019a). We will consider the
Pareto regression setting given by,


_p(yi_ **_xi) =_** _[bm]i[b]_ _, mi =_ _[b][ −]_ [1]
_|_ _yi[b][+1]_ _b_


_θθθ[∗], xi_ for yi _mi_ (4)
_⟨_ _⟩_ _≥_


provided ⟨θθθ[∗], xi⟩ _> γ for all i ∈_ [n]. Thus yi is Pareto given xi and E[yi|xi] = µi := ⟨θθθ[∗], xi⟩. We
will assume that b > 4 such that 4 + ϵ-moment exists for ϵ > 0. As in the Poisson setting, we choose
this parametrization for a fair comparison between TMO and MLE i.e in the limit of infinite samples
_θθθ[∗]_ lies in the linear solution space for both TMO (least squares) and MLE.

As before, to apply Theorem 2 we need an estimator with a good risk bound. We use the estimator in
Theorem 4 of (Hsu & Sabato, 2016), which in the fixed design pareto regression setting yields,

(θest) = 1 + O _d log 1δ_ _∥θ∗∥2Σ_
_E_ _n_ _b(b_ 2) _[.]_
   _−_

Note that the above estimator might not be easily implementable, however this yields the following
corollary of Theorem 2, which is a bound on the performance of the MLE estimator.
**Corollary 2. Under assumptions of our Pareto regression setting, the MLE estimator satisfies w.p.**
_at least 1 −_ _δ,_

_d[2][]log n + log ζ + log_ _[bwRλ]γ[max][(Σ)]_ + log [1]δ

(θθ[ˆ]θmle) = 1 + O Σ
_E_  _n_   _b(b_ 2) _[.]_

_−_

  _[∥][θ][∗][∥][2]_


-----

The proof is provided in Appendix H. It involves verifying the two conditions in Theorem 2 in the
Pareto regression setting.

The above MLE guarantee is expected to be much better than what can be achieved by TMO which
is least-squares. It is well established in the literature (Hsu & Sabato, 2016; Lugosi & Mendelson,
2019a) that ERM on square loss cannot achieve a O(log(1/δ)) dependency in a heavy tailed regime;
instead it can achieve only a O( 1/δ) rate.
p

5 CHOICE OF LIKELIHOOD AND INFERENCE METHODS

In this section we discuss some practical considerations for MLE, such as adapting to a target metric
of interest at inference time, and the choice of the likelihood family.

**Inference for different target metrics:** In most practical settings, the trained regression model
is used at inference time to predict the response variable on some test set to minimize some target
metric. For the MLE based approach, once the distribution parameters are learnt, this involves using
an appropriate statistic of the learnt distribution at inference time (see Section 3). For mean squared
error and mean absolute error, the estimator corresponds to the mean and median of the distribution,
but for several other commonly used loss metrics in the forecasting domain such as Mean Absolute
Percentage Error (MAPE) and Relative Error (RE) (Gneiting, 2011; Hyndman & Koehler, 2006),
this estimator corresponds to a median of a transformed distribution (Gneiting, 2011). Please see
Appendix I for more details. This ability to optimize the estimator at inference time for different
target metrics using a single trained model is another advantage that MLE based approaches have
over TMO models that are trained individually for specific target metrics.

**Mixture Likelihood:** An important practical question when performing MLE-based regression
is to decide which distribution family to use for the response variable. The goal is to pick a
distribution family that can capture the inductive biases present in the data. It is well known that a
misspecified distribution family for MLE might adversely affect generalization error of regression
models (Heagerty & Kurland, 2001). At the same time, it is also desirable for the distribution family
to be generic enough to cater to diverse datasets with potentially different types of inductive biases,
or even datasets for which no distributional assumptions can be made in advance.

A simple approach that we observe to work particularly well in practice with regression models using
deep neural networks is to assume the response variable comes from a mixture distribution, where
each mixture component belongs to a different distribution family and the mixture weights are learnt
along with the parameters of the distribution. More specifically, we consider a mixture distribution of
_k components p(y|x; θ1, . . ., θk, w1, . . ., wk) =_ _j=1_ _[w][j][p][j][(][y][|][x][;][ θ][j][)][, where each][ p][j][ characterizes]_
a different distribution family, and the mixture weights wj and distribution parameters θj are learnt
together. We would like to have a mixture distribution that can handle different situations like sparse

[P][k]
data, sub-Exponential and sub-Gaussian tails, count data as well as heavy tailed data. Moreover it
should be applicable to continuous valued datasets in general.

We use a three component mixture of (i) the constant 0 (zero-inflation for dealing with bi-modal
sparse data), (ii) a continuous version of negative binomial where n and p are learnt and (iii) a
Pareto distribution where the scale parameter is learnt. We provide more details about the continuous
version of negative binomial distribution in Appendix I.2. Our experiments in Section 6 show that
this mixture shows promising performance on a variety of datasets.

This will increase the number of parameters and the resulting likelihood might require non-convex
optimization. However, we empirically observed that with sufficiently over-parameterized networks
and gradient-based optimizers, this is not a problem in practice (Fig. 4 shows a convergence curve).

6 EMPIRICAL RESULTS

We present empirical results on two time-series forecasting problems and two regression problems
using neural networks. We will first describe our models and baselines. Our goal is to compare the
MLE approach with the TMO approach for three target metrics per dataset.


-----

Model Favorita M5

MAPE WAPE RMSE MAPE WAPE RMSE

MLE(ZNBP)TMOTMOTMOTMO(MSE)(MAE)(MAPE)(Huber) 0.61210.39830.31390.31990.432±±±±±0.00330.00110.00110.00750.0012 0.28910.22580.25280.23660.2238±±±±±0.00090.00160.00180.00230.0006 **161.4919164.6521192.3823164.7006175.3782±±±±±1.51851.38710.71780.82350.4748** **0.38640.44520.38920.47220.5045±±±±±0.00010.00050.00010.00070.004** 0.26770.31430.28390.2690.266±±±±±0.00020.00010.00020.00080.0007 11.37997.21337.05037.0937.507±±±±±0.01330.01520.0230.00940.1965

Table 1: We provide the MAPE, WAPE and RMSE metrics for all the models on the test set of two time-series datasets. The confidence
intervals provided are one standard error over 50 experiments, for each entry. TMO(<loss>) refers to TMO using the <loss>. For the MLE row,
we only train one model per dataset. The same model is used to output a different statistic for each column during inference. For MAPE, we
output the optimizer of MAPE given in Section I.5. For WAPE we output the median and for RMSE we output the mean.

Model Bicycle Share Gas Turbine

MAPE WAPE RMSE MAPE WAPE RMSE

TMOMLE(ZNBP)TMOTMOTMO(MSE)(MAE)(Huber)(MAPE) 0.25030.25940.19690.25360.2382±±±±±0.00110.00180.00080.00120.0011 0.14140.14210.14690.14360.1235±±±±±0.00040.00030.00030.00120.001 **767.4368889.1173878.5815901.1357899.9163±±±±±1.96547.12741.30591.49434.8219** 0.98770.88840.81080.9020.774±±±±±0.01280.00540.00190.01180.0009 0.34960.35980.33790.33890.8189±±±±±0.00490.00040.00410.00190.001 3.05731.45471.59921.56281.5789±±±±±0.00540.00190.00820.00710.0067

Table 2: We provide the MAPE, WAPE and RMSE metrics for all the models on the test set of two regression datasets. The confidence
intervals provided are one standard error over 50 experiments, for each entry. TMO(<loss>) refers to TMO using the <loss>. For the MLE row,
we only train one model per dataset. The same model is used to output a different statistic for each column during inference. For MAPE, we
output the optimizer of MAPE given in Section I.5. For WAPE we output the median and for RMSE we output the mean.

**Common Experimental Protocol: Now we describe the common experimental protocol on all the**
datasets (we get into dataset related specifics and architectures subsequently). For a fair comparison
the architecture is kept the same for TMO and MLE approaches. For each dataset, we tune the
hyper-parameters for the TMO(MSE) objective. Then these hyper-parameters are held fixed for all
models for that dataset i.e only the output layer and the loss function is modified. We provide all the
[details in Appendix I. Our code will be available here.](https://github.com/rajatsen91/mle_reg_forecast)

For the MLE approach, the output layer of the models map to the MLE parameters of the mixture
distribution introduced in Section 5, through link functions. The MLE output has 6 parameters, three
for mixture weights, two for negative binomial component and one for the scale parameter in Pareto.
The choice of the link functions and more details are specified in Appendix I.3. The loss function
used is the negative log-likelihood implemented in Tensorflow Probability (Dillon et al., 2017). Note
that for the MLE approach only one model is trained per dataset and during inference we output
the statistic that optimizes the target metric in question. We refer to our MLE based models that
employ the mixture likelihood from Section 5 as MLE(ZNBP)loss, where ZNBP refers to the mixture
components Zero, Negative-Binomial and Pareto.

For TMO, the output layer of the models map to ˆy and we directly minimize the target metric in
question. Note that this means we need to train a separate model for every target metric. Thus we
have one model each for target metrics in {’MSE’, ’MAE’, ’MAPE’}. Further we also train a model
using the Huber loss [2]. In order to keep the number of parameters the same as that of MLE, we add
an additional 6 neurons to the TMO models.

6.1 EXPERIMENTS ON FORECASTING DATASETS

We perform our experiments on two well-known forecasting datasets used in Kaggle competitions.

1. The M5 dataset (M5, 2020) consists of time series data of product sales from 10 Walmart stores in
three US states. The data consists of two different hierarchies: the product hierarchy and store
location hierarchy. For simplicity, in our experiments we use only the product hierarchy consisting
of 3K individual time-series and 1.8K time steps.

2. The Favorita dataset (Favorita, 2017) is a similar dataset, consisting of time series data from
Corporación Favorita, a South-American grocery store chain. As above, we use the product
hierarchy, consisting of 4.5k individual time-series and 1.7k time steps.

The task is to predict the values for the last 14 days all at once. The preceding 14 days are used for
validation. We provide more details about the dataset generation for reproducibility in Appendix I.

2The Huber loss is commonly used in robust regression (Huber, 1992; Lugosi & Mendelson, 2019a)


-----

Model MAPE WAPE RMSE

MLE(NB) 0.3314+/-0.0016 0.2521+/-0.002 175.501+/-1.1928

MLE(ZNB) 0.3186+/-0.0011 0.2453+/-0.002 170.0075+/-1.282

MLE(ZNBP) **0.3139±0.0011** **0.2238±0.0009** **164.6521+/-1.5185**

Table 4: We perform an ablation study on the Favorita dataset,
where we progressively add the components of our mixture distribution. There are three MLE models in the progression: Negative Binomial (NB), Zero-Inflated Negative Binomial (ZNB) and finally ZNBP.


Model p10QL p90QL

TMO (Quantile) 0.0973±0.0002 0.0628±0.0019

MLE(ZNBP) **0.0788±0.0008** **0.0536±0.0007**

Table 3: The MLE model predicts the empirical quantile of interest during inference. It is compared with Quantile regression (TMO
based). The results, averaged over 50 runs along with the corresponding confidence intervals are presented.


The base architecture for the baselines as well as our model is a seq-2-seq model (Sutskever et al.,
2014). The encoders and decoders both are LSTM cells (Hochreiter & Schmidhuber, 1997). The
architecture is illustrated in Figure 1 and described in more detail in Appendix I.

We present our experimental results in Table 1. On both the datasets the MLE model with the
appropriate inference-time estimator for a metric is always better than TMO trained on the same
target metric, except for WAPE in M5 where MLE’s performance is only marginally worse. Note
that the MLE model is always the best or second best performing model on all metrics, among all
TMO models. For TMO the best performance is not always achieved for the same target metric. For
instance, TMO(MAE) performs better than TMO(MSE) for the RMSE metric on the Favorita dataset.
In Table 4 we perform an ablation study on the Favorita dataset, where we progressively add mixture
components resulting in three MLE models: Negative Binomial, Zero-Inflated Negative Binomial
and finally ZNBP. This shows that each of the components add value in this dataset.

6.2 EXPERIMENTS ON REGRESSION DATASETS

We perform our experiments on two standard regression datasets,

1. The Bicyle Share dataset (Bicycle, 2017) has daily counts of the total number of rental bikes.
The features include time features as well as weather conditions such as temperature, humidity,
windspeed etc. A random 10% of the dataset is used as test and the rest for training and validation.
The dataset has a total of 730 samples.

2. The Gas Turbine dataset (Kaya et al., 2019) has 11 sensor measurements per example (hourly)
from a gas turbine in Turkey. We consider the level of NOx as our target variable and the rest
as predictors. There are 36733 samples in total. We use the official train/test split. A randomly
chosen 20% of the training set is used for validation. The response variable is continuous.

For all our models, the model architecture is a fully connected DNN with one hidden layer that has
32 neurons. Note that for categorical variables, the input is first passed through an embedding layer
(one embedding layer per feature), that is jointly trained. We provide further details like the shape of
the embedding layers in Appendix I. The architecture is illustrated in Figure 2.

We present our results in Table 2. On the Bicycle Share dataset, the MLE(ZNBP)based model
performs optimally in all metrics and often outperforms the TMO models by a large margin even
though TMO is a separate model per target metric. On the Gas Turbine dataset, the MLE based model
is optimal for WAPE and RMSE, however it does not perform that well for the MAPE metric.

In Table 3, we compare the MLE based approach versus quantile regression (TMO based) on the
Bicycle Share dataset, where the metric presented is the normalized quantile loss (Wang et al., 2019).
We train the TMO model for the corresponding quantile loss directly and the predictions are evaluated
on normalized quantile losses as shown in the table. The MLE based model is trained by minimizing
the negative log-likelihood and then during inference we output the corresponding empirical quantile
from the predicted distribution. MLE(ZNBP)outperforms TMO(Quantile) significantly.

**Discussion: We compare the approaches of direct ERM on the target metric (TMO) and MLE**
followed by post-hoc inference time optimization for regression and forecasting problems. We prove
a general competitiveness result for the MLE approach and also show theoretically that it can be
better than TMO in the Poisson and Pareto regression settings. Our empirical results show that our
proposed general purpose likelihood function employed in the MLE approach can uniformly perform
well on several tasks across four datasets. Even though this addresses some of the concerns about
choosing the correct likelihood for a dataset, some limitations still remain for example concerns about
the non-convexity of the log-likelihood. We provide a more in-depth discussion in Appendix J.


-----

REFERENCES

Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin,
Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-scale
machine learning. In 12th USENIX symposium on operating systems design and implementation
_(OSDI 16), pp. 265–283, 2016._

Jayadev Acharya, Hirakendu Das, Alon Orlitsky, and Ananda Theertha Suresh. A unified maximum
likelihood approach for estimating symmetric properties of discrete distributions. In International
_Conference on Machine Learning, pp. 11–21. PMLR, 2017._

Jean-Yves Audibert, Olivier Catoni, et al. Robust linear least squares regression. The Annals of
_Statistics, 39(5):2766–2794, 2011._

Zhi-Dong Bai and Yong-Qua Yin. Limit of the smallest eigenvalue of a large dimensional sample
covariance matrix. In Advances In Statistics, pp. 108–127. World Scientific, 2008.

Derek Bean, Peter J Bickel, Noureddine El Karoui, and Bin Yu. Optimal m-estimation in highdimensional regression. Proceedings of the National Academy of Sciences, 110(36):14563–14568,
2013.

Bicycle. Bicycle share dataset. [https://www.kaggle.com/contactprad/](https://www.kaggle.com/contactprad/bike-share-daily-data/)
[bike-share-daily-data/, 2017.](https://www.kaggle.com/contactprad/bike-share-daily-data/)

Christian Brownlees, Emilien Joly, and Gábor Lugosi. Empirical risk minimization for heavy-tailed
losses. The Annals of Statistics, 43(6), Dec 2015. ISSN 0090-5364. doi: 10.1214/15-aos1350.
[URL http://dx.doi.org/10.1214/15-AOS1350.](http://dx.doi.org/10.1214/15-AOS1350)

Yunshun Chen, Aaron TL Lun, and Gordon K Smyth. From reads to genes to pathways: differential
expression analysis of rna-seq experiments using rsubread and the edger quasi-likelihood pipeline.
_F1000Research, 5, 2016._

Richard A Davis and Rongning Wu. A negative binomial model for time series of counts. Biometrika,
96(3):735–749, 2009.

Joshua V Dillon, Ian Langmore, Dustin Tran, Eugene Brevdo, Srinivas Vasudevan, Dave Moore,
Brian Patton, Alex Alemi, Matt Hoffman, and Rif A Saurous. Tensorflow distributions. arXiv
_preprint arXiv:1711.10604, 2017._

David Donoho and Andrea Montanari. High dimensional robust m-estimation: Asymptotic variance
via approximate message passing. Probability Theory and Related Fields, 166(3):935–969, 2016.

Lutz Dümbgen and Kaspar Rufibach. Maximum likelihood estimation of a log-concave density and
its distribution function: Basic properties and uniform consistency. Bernoulli, 15(1):40–68, 2009.

Noureddine El Karoui. On the impact of predictor geometry on the performance on high-dimensional
ridge-regularized generalized robust regression estimators. Probability Theory and Related Fields,
170(1):95–175, 2018.

Ludwig Fahrmeir and Heinz Kaufmann. Consistency and asymptotic normality of the maximum
likelihood estimator in generalized linear models. The Annals of Statistics, 13(1):342–368, 1985.

Favorita. Favorita forecasting dataset. [https://www.kaggle.com/c/](https://www.kaggle.com/c/favorita-grocery-sales-forecasting/)
[favorita-grocery-sales-forecasting/, 2017.](https://www.kaggle.com/c/favorita-grocery-sales-forecasting/)

Dylan J Foster and Akshay Krishnamurthy. Efficient first-order contextual bandits: Prediction,
allocation, and triangular discrimination. arXiv preprint arXiv:2107.02237, 2021.

Jan Gasthaus, Konstantinos Benidis, Yuyang Wang, Syama Sundar Rangapuram, David Salinas,
Valentin Flunkert, and Tim Januschowski. Probabilistic forecasting with spline quantile function
rnns. In The 22nd international conference on artificial intelligence and statistics, pp. 1901–1910.
PMLR, 2019.

Sara A Geer and Sara van de Geer. Empirical Processes in M-estimation, volume 6. Cambridge
university press, 2000.


-----

Tilmann Gneiting. Making and evaluating point forecasts. Journal of the American Statistical
_Association, 106(494):746–762, 2011._

Arthur Stanley Goldberger et al. Econometric theory. Econometric theory., 1964.

Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT Press, 2016.

Patrick J Heagerty and Brenda F Kurland. Misspecified maximum likelihood estimates and generalised linear mixed models. Biometrika, 88(4):973–985, 2001.

C.C. Heyde. On an optimal asymptotic property of the maximum likelihood estimator of
a parameter from a stochastic process. _Stochastic Processes and their Applications, 8(1):_
1–9, 1978. ISSN 0304-4149. [URL https://www.sciencedirect.com/science/](https://www.sciencedirect.com/science/article/pii/0304414978900649)
[article/pii/0304414978900649.](https://www.sciencedirect.com/science/article/pii/0304414978900649)

Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735–1780, 1997.

Daniel Hsu and Sivan Sabato. Loss minimization and parameter estimation with heavy tails. Journal
_[of Machine Learning Research, 17(18):1–40, 2016. URL http://jmlr.org/papers/v17/](http://jmlr.org/papers/v17/14-273.html)_
[14-273.html.](http://jmlr.org/papers/v17/14-273.html)

Daniel Hsu, Sham M Kakade, and Tong Zhang. Random design analysis of ridge regression. In
_Conference on learning theory, pp. 9–1. JMLR Workshop and Conference Proceedings, 2012._

Peter J Huber. Robust estimation of a location parameter. In Breakthroughs in statistics, pp. 492–518.
Springer, 1992.

Rob John Hyndman and Ann B Koehler. Another look at measures of forecast accuracy. International
_Journal of Forecasting, 22(4):679–688, 2006._

Heysem Kaya, PINAR TÜFEKC[˙]I, and Erdinc Uzun. Predicting co and no x emissions from gas
turbines: novel data and a benchmark pems. Turkish Journal of Electrical Engineering & Computer
_Sciences, 27(6):4783–4796, 2019._

Bernhard Klar. Bounds on tail probabilities of discrete distributions. Probability in the Engineering
_and Informational Sciences, 14(2):161–171, 2000._

Roger Koenker and Gilbert Bassett Jr. Regression quantiles. Econometrica: journal of the Economet_ric Society, pp. 33–50, 1978._

Stéphane Lathuilière, Pablo Mesejo, Xavier Alameda-Pineda, and Radu Horaud. A comprehensive
analysis of deep regression. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42
(9):2065–2081, 2020. doi: 10.1109/TPAMI.2019.2910523.

Jerald F Lawless. Negative binomial and mixed poisson regression. The Canadian Journal of
_Statistics/La Revue Canadienne de Statistique, pp. 209–225, 1987._

G. Lecue and S. Mendelson. Learning subgaussian classes: Upper and minimax bounds. In S.
_Boucheron and N. Vayatis, editors, Topics in Learning Theory. Societe Mathematique de France,_
2016.

Erich L Lehmann and George Casella. Theory of point estimation. Springer Science & Business
Media, 2006.

Gábor Lugosi and Shahar Mendelson. Mean estimation and regression under heavy-tailed distributions: A survey. Foundations of Computational Mathematics, 19(5):1145–1190, 2019a.

Gábor Lugosi and Shahar Mendelson. Sub-gaussian estimators of the mean of a random vector. The
_annals of statistics, 47(2):783–794, 2019b._

M5. M5 forecasting dataset. [https://www.kaggle.com/c/](https://www.kaggle.com/c/m5-forecasting-accuracy/)
[m5-forecasting-accuracy/, 2020.](https://www.kaggle.com/c/m5-forecasting-accuracy/)


-----

Davis J McCarthy, Yunshun Chen, and Gordon K Smyth. Differential expression analysis of
multifactor rna-seq experiments with respect to biological variation. Nucleic acids research, 40
(10):4288–4297, 2012.

Peter McCullagh and John A Nelder. Generalized linear models. Routledge, 2019.

Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. MIT
press, 2018.

John Ashworth Nelder and Robert WM Wedderburn. Generalized linear models. Journal of the
_Royal Statistical Society: Series A (General), 135(3):370–384, 1972._

Ryan O’Donnell. Analysis of boolean functions. Cambridge University Press, 2014.

Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-beats: Neural basis
expansion analysis for interpretable time series forecasting. arXiv preprint arXiv:1905.10437,
2019.

Adarsh Prasad, Arun Sai Suggala, Sivaraman Balakrishnan, and Pradeep Ravikumar. Robust estimation via robust gradient estimation. Journal of the Royal Statistical Society: Series B (Statistical
_Methodology), 82(3):601–627, 2020._

C.R. Rao. Criteria of estimation in large samples. Sankhy¯a, 25, Ser A, 1963.

Richard Redner. Note on the consistency of the maximum likelihood estimate for nonidentifiable
distributions. The Annals of Statistics, pp. 225–228, 1981.

Philippe Rigollet. High-dimensional statistics. [https://ocw.mit.edu/courses/](https://ocw.mit.edu/courses/mathematics/18-s997-high-dimensional-statistics-spring-2015/lecture-notes/MIT18_S997S15_Chapter2.pdf)
[mathematics/18-s997-high-dimensional-statistics-spring-2015/](https://ocw.mit.edu/courses/mathematics/18-s997-high-dimensional-statistics-spring-2015/lecture-notes/MIT18_S997S15_Chapter2.pdf)
[lecture-notes/MIT18_S997S15_Chapter2.pdf, 2015.](https://ocw.mit.edu/courses/mathematics/18-s997-high-dimensional-statistics-spring-2015/lecture-notes/MIT18_S997S15_Chapter2.pdf)

Alesandro Rinaldo. Sub-exponential concentration. [http://www.stat.cmu.edu/](http://www.stat.cmu.edu/~arinaldo/Teaching/36709/S19/Scribed_Lectures/Feb5_Aleksandr.pdf)
[~arinaldo/Teaching/36709/S19/Scribed_Lectures/Feb5_Aleksandr.pdf,](http://www.stat.cmu.edu/~arinaldo/Teaching/36709/S19/Scribed_Lectures/Feb5_Aleksandr.pdf)
2019.

Mark D Robinson and Gordon K Smyth. Small-sample estimation of negative binomial dispersion,
with applications to sage data. Biostatistics, 9(2):321–332, 2008.

David Salinas, Valentin Flunkert, Jan Gasthaus, and Tim Januschowski. Deepar: Probabilistic
forecasting with autoregressive recurrent networks. International Journal of Forecasting, 36(3):
1181–1191, 2020.

Rajat Sen, Hsiang-Fu Yu, and Inderjit Dhillon. Think globally, act locally: A deep neural network
approach to high-dimensional time series forecasting. arXiv preprint arXiv:1905.03806, 2019.

Niranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger. Gaussian process optimization in the bandit setting: No regret and experimental design. arXiv preprint arXiv:0912.3995,
2009.

Pragya Sur and Emmanuel J Candès. A modern maximum-likelihood theory for high-dimensional
logistic regression. Proceedings of the National Academy of Sciences, 116(29):14516–14525,
2019.

Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.
_arXiv preprint arXiv:1409.3215, 2014._

Abraham Wald. Note on the consistency of the maximum likelihood estimate. The Annals of
_Mathematical Statistics, 20(4):595–601, 1949._

Yuyang Wang, Alex Smola, Danielle Maddix, Jan Gasthaus, Dean Foster, and Tim Januschowski.
Deep factors for forecasting. In International Conference on Machine Learning, pp. 6607–6617.
PMLR, 2019.

Ruofeng Wen Wen, Kari Torkkola, and Balakrishnan Narayanaswamy. A multi-horizon quantile
recurrent forecaster. In NIPS Time Series Workshop, 2017.


-----

Zonghan Wu, Shirui Pan, Guodong Long, Jing Jiang, Xiaojun Chang, and Chengqi Zhang. Connecting
the dots: Multivariate time series forecasting with graph neural networks. In Proceedings of the
_26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp._
753–763, 2020.

Tong Zhang. From epsilon-entropy to kl-entropy: Analysis of minimum information complexity
density estimation. The Annals of Statistics, 34(5):2180–2210, 2006.

Ziwei Zhu and Wenjing Zhou. Taming heavy-tailed features by shrinkage. In International Conference
_on Artificial Intelligence and Statistics, pp. 3268–3276. PMLR, 2021._


-----

A PROOF FOR COMPETITIVENESS OF MLE

**Theorem 3. Let ˆπ be an estimator for a property π, such that for any f ∈F and z ∼** _f_ _, Pr(∥π(f_ ) _−_
_πˆ(z)∥≥_ _ϵ) ≤_ _δ. Then the MLE based estimator satisfies the following bound,_

Pr( _π(f_ ) _π(fz)_ 2ϵ) ( + 1)δ.
_∥_ _−_ _∥≥_ _≤_ _|F|_

_Proof of Theorem 3. By triangle inequality and by the properties,_

_π(f_ ) _π(fz)_ _π(f_ ) _πˆ(z)_ + _πˆ(z)_ _π(fz)_ _._
_∥_ _−_ _∥≤∥_ _−_ _∥_ _∥_ _−_ _∥_

Hence,
1 _π(f_ ) _π(fz)_ 2ϵ 1 _π(f_ ) _πˆ(z)_ _ϵ_ [+ 1] _πˆ(z)_ _π(fz)_ _ϵ[.]_
_∥_ _−_ _∥≥_ _≤_ _∥_ _−_ _∥≥_ _∥_ _−_ _∥≥_

We now take expectation of both LHS and RHS of the above equation with respect to the distribution
_f_ . For the LHS, observe that

E[1 _π(f_ ) _π(fz)_ 2ϵ] = Pr( _π(f_ ) _π(fz)_ 2ϵ).
_∥_ _−_ _∥≥_ _∥_ _−_ _∥≥_

For the first term in the RHS

E[1 _π(f_ ) _πˆ(z)_ _ϵ[]][ ≤]_ [Pr(][∥][π][(][f] [)][ −] _π[ˆ](z)_ _ϵ)_ _δ,_
_∥_ _−_ _∥≥_ _∥≥_ _≤_

by the assumption in the theorem. Combining the above three equations yield that

Pr( _π(f_ ) _π(fz)_ 2ϵ) E[1 _πˆ(z)_ _π(fz)_ _ϵ[] +][ δ.]_
_∥_ _−_ _∥≥_ _≤_ _∥_ _−_ _∥≥_

We now prove that E[1∥πˆ(z)−π( f[˜]z)∥≥ϵ[]][ ≤|F|][δ][.]


E[1 _πˆ(z)_ _π(fz)_ _ϵ[] =]_
_∥_ _−_ _∥≥_

(a)
_≤_

_≤_
Z

(b)


_f_ (z)1∥πˆ(z)−π(fz)∥≥ϵ

_fz(z)1∥πˆ(z)−π(fz)∥≥ϵ_

_f_ (z)1∥πˆ(z)−π(f )∥≥ϵ
_f_

X

_z_ _f_ (z)1∥πˆ(z)−π(f )∥≥ϵ

Z

Pr _π(z)_ _π(f_ ) _ϵ)_
_z_ _f[(][∥][ˆ]_ _−_ _∥≥_
_∼_

_δ = |F|δ,_


where (a) uses the fact that fz is the MLE estimate over set F and hence fz(z) ≥ _f_ (z). (b) follows
from Fubini’s theorem.

Now we prove an extension of our last theorem that can deal with infinite likelihood families.

_Proof of Theorem 1. Let_ _F[˜] be the cover of Ff that satisfies assumptions in Definition 1. By triangle_
inequality and by the properties of [˜], with probability at least 1 _δ2_ _δ,_
_F_ _−_ _−_

_π(f_ ) _π(fz)_ _π(f_ ) _πˆ(z)_ + _πˆ(z)_ _π( f[˜]z)_ + _π( f[˜]z)_ _π(fz)_
_∥_ _−_ _∥≤∥_ _−_ _∥_ _∥_ _−_ _∥_ _∥_ _−_ _∥_

_π(f_ ) _πˆ(z)_ + _πˆ(z)_ _π( f[˜]z)_ + ϵ
_≤∥_ _−_ _∥_ _∥_ _−_ _∥_

We have used the fact that fz ∈Ff w.p at least 1 − _δ. Hence with probability at least 1 −_ _δ2 −_ _δ,_

1 _π(f_ ) _π(fz)_ 3ϵ 1 _π(f_ ) _πˆ(z)_ _ϵ_ [+ 1] _πˆ(z)_ _π( f[˜]z)_ _ϵ[.]_
_∥_ _−_ _∥≥_ _≤_ _∥_ _−_ _∥≥_ _∥_ _−_ _∥≥_


-----

We now take expectation of both LHS and RHS of the above equation with respect to the distribution
_f_ . For the LHS, observe that

E[1 _π(f_ ) _π(fz)_ 3ϵ] = Pr( _π(f_ ) _π(fz)_ 3ϵ).
_∥_ _−_ _∥≥_ _∥_ _−_ _∥≥_

For the first term in the RHS


E[1 _π(f_ ) _πˆ(z)_ _ϵ[]][ ≤]_ [Pr(][∥][π][(][f] [)][ −] _π[ˆ](z)_ _ϵ)_ _δ,_
_∥_ _−_ _∥≥_ _∥≥_ _≤_

by the assumption in the theorem. Combining the above three equations yield that

Pr( _π(f_ ) _π(fz)_ 3ϵ) E[1 _πˆ(z)_ _π( f[˜]z)_ _ϵ[] + 2][δ][ +][ δ][2][.]_
_∥_ _−_ _∥≥_ _≤_ _∥_ _−_ _∥≥_

We now prove that E[1 _πˆ(z)_ _π( f[˜]z)_ _ϵ[]][ ≤]_ _[Tδ][ +][ δ][1][. Let][ ˜]f be the distribution in_ [˜]f that is at most δ1
_∥_ _−_ _∥≥_ _F_
away from f . Then,


E[1∥πˆ(z)−π( f[˜]z)∥≥ϵ[] =]

(a)
_≤_

(b)
_≤_

(c)
_≤_

_≤_

(d)


_z_ _f_ (z)1∥πˆ(z)−π( f[˜]z)∥≥ϵ

Z

_z_ _f˜(z)1∥πˆ(z)−π( f[˜]z)∥≥ϵ_ [+][ ∥][f][ −] _f[˜]∥1_

Z

_z_ _f˜(z)1∥πˆ(z)−π( f[˜]z)∥≥ϵ_ [+][ δ][1]

Z

_z_ _f˜z(z)1∥πˆ(z)−π( f[˜]z)∥≥ϵ_ [+][ δ][1]

Z

Zz _f˜_ _f˜(z)1∥πˆ(z)−π( f[˜]z)∥≥ϵ_ [+][ δ][1]

X∈F[˜]

_f˜_ Zz _f˜(z)1∥πˆ(z)−π( f[˜])∥≥ϵ_ [+][ δ][1]

X∈F[˜]

Pr ( _π( f[˜])_ _πˆ(z)_ _ϵ) + δ1_
_z_ _f_ _∥_ _−_ _∥≥_
_f˜_ _∼_ [˜]

X∈F[˜]


_≤_ ( _δ) + δ1 = Tδ + δ1,_

_f˜_

X∈F[˜]

where the (a) follows from the definition of ℓ1 distance between distributions, (b) follows from
the properties of the cover, (c) uses the fact that _f[˜]z is the MLE estimate over set_ _F[˜] and hence_
_f˜z(z)_ _f_ (z). (d) follows from Fubini’s Theorem.
_≥_ [˜]

B OTHER GENERAL RESULTS FOR THE MLE

In this section, we prove that the log likelihood of the MLE distribution at the observed data point is
is close to the log-likelihood of the ground truth at the data point, upto an additive factor that depends
on the Shtarkov sum of the family.
**Lemma 2. Let z ∼** _f_ _. The maximum likelihood estimator satisfies the following inequality with_
_probability at least 1 −_ _δ,_


log f (z) log fz(z) log f (z) + log _fz(z)_ + log [1]
_≤_ _≤_ Xz∈Z ! _δ [,]_

_where_ _z_ _[f][z][(][z][)][ is the Shtarkov sum of the family][ F][. Furthermore, if][ F][ is finite, then]_

_∈Z_

log f (z) log fz(z) log f (z) + log + log [1]

[P] _≤_ _≤_ _|F|_ _δ [,]_

_Further is the distribution family is finite, then_


log f (z) log fz(z) log f (z) + log + log [1]
_≤_ _≤_ _|F|_ _δ [.]_


-----

_Proof. Let K =_ [1]δ


_z_ _[f][z][(][z][)][.]_
_∈Z_

_fz(z)_
Pr(f (z) _fz(z)/K) = Pr_
_≤_ _Kf_ (z)
 _[≥]_ [1]

(a)
E _fz(z)_
_≤_ _Kf_ (z)
 

= [1] _fz(z) = δ,_

_K_

_zX∈Z_


where (a) follows from Markov’s inequality. The first part of the lemma follows by taking the
logarithm and substituting the value of K. For the second part, observe that if F is finite then,


_fz(z)_
_≤_
_zX∈Z_


1 = |F|.
_fX∈F_


_f_ (z) =
_fX∈F_


_f_ (z) =
_zX∈Z_


_z∈Z_


_f_ _∈F_


We now prove a result of Shtarkov’s sum, which will be useful in other scenarios.

**Lemma 3. Let w = π(z) for some property π.**


_fw(w)_
_≤_
_w_

X

_pg(z)(z) =_
X


_fz(z)._
_z_

X

_fw(z) =_

_z:g(z)=w_

X


_Proof._


_fz(z)_
_≥_


_fw(w)._


C GENERAL FIXED DESIGN RESULT

In this section, we will prove Theorem 2 which is an application of Theorem 1 to the fixed design
regression setting with the square loss as the target metric. The theorem holds under some reasonable
assumptions. We will fist prove some intermediate lemmas.

**Lemma 4. Let f** (·; θ) = _i=1_ _[p][(][·|][x][i][;][ θ][)][ in the fixed design setting. If][ ∥][θ][ −]_ **_[θ][′][∥]2_** _[≤]_ _[δ][ then,]_

_f_ ( ; θ) _f_ ( ; θ[′]) 1 _√2nLδ._

[Q][n] _∥_ _·_ _−_ _·_ _∥_ _≤_

_under the assumptions of Theorem 2._


_Proof. By Pinskers’ inequality we have the following chain,_


_f_ ( ; θ) _f_ ( ; θ[′]) 1
_∥_ _·_ _−_ _·_ _∥_ _≤_


2DKL(f ( ; θ); f ( ; θ[′]))

_·_ _·_


2 _DKL(p(_ **_xi; θ); p(_** **_xi; θ[′]))_**

_≤_ v _·|_ _·|_

u _i=1_
u X

_≤_ t√2nLδ

**Lemma 5.the Poisson regression setting. Then with probability at least Assume the conditions of Theorem 2 holds. Let** 1θθ[˜]θmle − := argmaxδ we have, **_θ∈N (˜ϵ,Θ)_** _[L][(][y][n][,][ θ][)][, in]_


_ϵ_
(θθ[ˆ]θmle) (θθ[˜]θmle)
_|E_ _−E_ _| ≤_ [4][wλ][max]n [(Σ)˜]


_α_ _[.]_


-----

_Proof. We assume that the event in (2) of Theorem 2 holds._

(y[n]; θθ[ˆ]θmle) (y[n]; θc) _ϵ[2],_
_|L_ _−L_ _| ≤_ _[β]2 [˜]_

where θc is the closest point in N (˜ϵ, Θ) to _θθ[ˆ]θmle. Therefore, by definition_

(y[n]; θθ[˜]θmle) (y[n]; θθ[ˆ]θmle) + (β/2)˜ϵ[2].
_L_ _≤L_

By virtue of the strong-convexity we have,


2

_ϵ[2]._

2 _[≤]_ _α[β]_ [˜]


_θθθmle_ _θθθmle_

[ˆ] _−_ [˜]


Now by triangle inequality we have,


(θθ[˜]θmle) = _θθθmle_ _θθθ[∗]_ _θθθmle_ _θθθ[∗]_
_E_ _|_ _|_ _−_ Σ _[−]_ _−_ Σ[|]

_λmax(Σ)β_

_θθθ[ˆ]mle_ _θθθmle_ [˜]
_≤_ _−_ [˜] Σ r _α_

[ˆ] _[≤]_


(θθ[ˆ]θmle)
_E_ _−_


Now we can use the fact |a − _b| ≤_ 2 max([√]a,


_b)|[√]a −_


_b| and that Θ ⊆_ B[d]w [to conclude,]


(θθ[ˆ]θmle) (θθ[˜]θmle) 4wλmax(Σ)˜ϵ
_|E_ _−E_ _| ≤_


_α_ _[.]_


_Proof of Theorem 2. Consider the net N_ (φ, θ). If φ = δ[2]/(2nL), then by Lemma 4 the net forms a
_δ cover on F. Note that under the conditions of Theorem 2 |N_ (φ, θ)| ≤ (3w/φ)[d].

Next note that in the application of Theorem 1, we should set

_ϵ = [(][c][1][ +][ c][2][ log(1][/δ][))][η]_ _._

_n_

Thus if we set φ in place of ˜ϵ in Lemma 5 we would need the following to apply Theorem 1,


_β_
_α_ [= (][c][1][ +][ c][2][ log(1]n _[/δ][))][η]_


4wλmax(Σ)φ


Thus, φ can be such that,

log [1] log(2nL), 0.5 log _[β]_

_φ_ _α_ [+ log(4][wλ][max][(Σ)) + log(][n][)]

_[≤]_ [max]


From above we can apply Theorem 1 with the above ϵ, δ1 = δ2 = δ and T = (3w/φ)[d]. Therefore,
we get


(θθ[ˆ]θmle) 3 [(][c][1][ +][ c][2][ log(1][/δ][))][η]
_E_ _≥_ _n_


_≤_ (T + 5)δ. (5)


We can set δ = δ/(T + 5) to then conclude that w.p at least 1 − _δ,_

(c1 + c2d(log(w) + log(1/φ)))η
(θθ[ˆ]θmle) = O
_E_ _n_


Substituting the bound on log _φ[1]_ [yields the result.]


-----

D LEAST SQUARES FOR POISSON

We begin by restating the following general statement about OLS.
**Lemma 6 (see Theorem 2.2 in (Rigollet, 2015)). Suppose r = rank(X[T]** **X) and φφφ ∈** R[n][×][r] _be an_
_orthonormal basis for the column-span of X. Then we have the following result,_


(θθ[ˆ]θls)
_E_ _≤_ _n[4]_


sup (u[T] ˜ϵϵϵ)[2], (6)
**_u∈S1[r][−][1]_**


where ˜ϵϵϵ = φφφ[T] _ϵϵϵ. Here, ϵ = y[n]_ _−_ **_Xθθθ[∗]_** for the given response sample y[n].

Now are at a position to prove Lemma 1.

_Proof of Lemma 1. We are interested in tail bounds on the RHS of (6). We will first analyze tail_
bounds for a fixed u S[r]1[−][1]. We have the following chain,
_∈_

E[exp{s⟨u, ˜ϵϵϵ⟩}] = E[exp{s⟨φu, ϵ⟩}] := E[exp{s⟨v, ϵ⟩}]


E[exp{sviϵi}] =
_i=1_

Y


_i=1_ E[exp(svi(yi − _µi))]_

Y


where µi = ⟨θθθ[∗], xi⟩. Now bounding each term separately we have,

E[exp(svi(yi − _µi))] = exp(−sviµi) exp(µi(exp(svi) −_ 1))

We have,


exp(µi(exp(svi) 1)) = exp
_−_
_i=1_

Y


_µi(exp(svi)_ 1)
_−_
_i=1_

X


_µi_

_j! [(][sv][i][)][j]_


1

_j!_ [(][sv][i][)][j]


exp

 _≤_


_µisvi +_






= exp

_≤_ exp


_µmax_
_j=2_

X


_i=1_


_j=1_


_i=1_


(7)


_µisvi + µmax(exp(s) −_ _s −_ 1)


where we have used the fact that ||v||p ≤||v||2 = 1 for all p ≥ 2. Thus we have,

E[exp{s⟨u, ˜ϵϵϵ⟩}] ≤ exp(µmax(exp(s) − _s −_ 1)).

This means that the RV ⟨u, ˜ϵϵϵ⟩ is sub-exponential with parameter ν[2] = 2µmax and α = 0.56. Thus if
_t ≤_ 4µmax then we have that for a fixed u,

_t[2]_
P( **_u,_** ˜ϵϵϵ _t)_ exp _._ (8)
_⟨_ _⟩} ≥_ _≤_ _−_ 4µmax
 

Thus by a union bound over the ϵ-net with ϵ = 2, we have that wp 1 − _δ,_

sup _ϵϵϵ_ 8µmax(log(1/δ) + r log 6).
**_u_** _⟩≤_

_[⟨][u][,]_ [˜]
p

Thus we get the risk bound wp 1 − _δ,_

(θθ[ˆ]θls) (log(1/δ) + r log 6). (9)
_E_ _≤_ [32][µ]n[max]


For the sub-exponential region when t ≥ _µmax we get the bound,_

_E(θθ[ˆ]θls) ≤_ [16]n [max] (log(1/δ) + r log 6)[2], µ[2]max



We get the bound by setting r = d.


-----

E COMPETITIVENESS FOR POISSON REGRESSION

**Lemma 7. Let f** (·; θ) = _i=1_ _[p][(][·|][x][i][;][ θ][)][ where][ p][ is defined in Eq.][ (][2][)][. If][ ∥][θ][ −]_ **_[θ][′][∥]2_** _[≤]_ _[δ][ then,]_

2nw

[Q][n] _∥f_ (·; θ) − _f_ (·; θ[′])∥1 ≤ _Rr_ _γ_ _δ._


_Proof. By Pinskers’ inequality we have the following chain,_


_f_ ( ; θ) _f_ ( ; θ[′]) 1
_∥_ _·_ _−_ _·_ _∥_ _≤_


2DKL(f ( ; θ); f ( ; θ[′]))

_·_ _·_


2
v
u
u
t

2
v
u
u
t


_DKL(p(_ **_xi; θ); p(_** **_xi; θ[′]))_**

_·|_ _·|_
_i=1_

X

_n_

( **_θ, xi_** (log( **_θ, xi_** ) log( **_θ[′], xi_** )) ( **_θ, xi_** **_θ[′], xi_** ))
_⟨_ _⟩_ _⟨_ _⟩_ _−_ _⟨_ _⟩_ _−_ _⟨_ _⟩−⟨_ _⟩_
_i=1_

X


Notice that the absolute value of the derivative of a log x − _x wrt x, is upper bounded by a/γ −_ 1 if
_x ≥_ _γ > 0 and also a ≥_ _γ. Therefore, following from above we have,_

_n_ **_θT xi_**

_f_ ( ; θ) _f_ ( ; θ[′]) 1 2 1 **_θ, xi_** **_θ[′], xi_**
_∥_ _·_ _−_ _·_ _∥_ _≤_ vu _i=1_ _γ_ _−_ |⟨ _⟩−⟨_ _⟩|_

u X

_n_ **_θT xi_** t

2 1 **_θ_** **_θ[′]_** 2R

_≤_ v _γ_ _−_ _∥_ _−_ _∥_

u _i=1_ 
u X
t

2nwR[2]

_≤_ s _γ_ _∥θ −_ **_θ[′]∥2,_**

thus concluding the proof.

Now we prove high probability bounds on the strong convexity and smoothness of the likelihood in
the context of Poisson regression.

**Lemma 8. Let χ be the condition number of the matrix M** _, where ui = xi/_ **_xi_** 2. Then with
_∥_ _∥_
_probability at least 1 −_ 2δ, we have


_λmin_ _∇θ[2]_ _[L][(][y][n][;][ θ][)]_
 


_n_

(10)
2∥θ∥[2][ λ][min][(][M] [)][,]


_provided nλmin(M_ ) ≥ 4γ1[2][ (][d][ log(24][χ][) + log(1][/δ][))][ and] _λmax(M_ )(d log(24χ) + log(1/δ)) ≤

_√nλmin(M_ )/16.

p

_Proof. We start with the expression of the Hessian of L(y[n]; θ),_


_yixix[T]i_

**_θ, xi_**
_⟨_ _⟩[2][ ≽]_


_yiuiu[T]i_ := L(θ),

_∥θ∥[2]_


_∇θ[2]_ _[L][ =]_


_i=1_


_i=1_


where ui = xi/ **_xi_** 2.
_∥_ _∥_

For a fixed unit vector u S[d]1[−][1] let us define,
_∈_

_n_

_yiu[T]_ **_uiu[T]i_** **_[u]_**

_L(u, θ) :=_ :=

_i=1_ **_θ_**

X _∥_ _∥[2]_

where vi = ⟨ui, u⟩[2]/∥θ∥[2].


_yivi,_
_i=1_

X


-----

Following the definition of sub-exponential RV in (Rinaldo, 2019),


_ν[2]_ =


_νi[2][, α][ = 0][.][56 max]_ _vi_


(11)


_yivi_ _SE_
_∈_


where νi = 2µivi[2][. This implies that wp atleast][ 1][ −] [2][δ/C][ we have,]

_C_

_yivi_ _µivi_ 2 _µivi[2]_ [log]

_|_ _i_ _−_ _i_ _| ≤_ s  _δ_
X X X

if,


_C_

s _µivi[2]_ [log] _δ_ _≤_ 2( _µivi[2][)][/][(max]i_ _vi)._

 
X X

Note that the above condition is mild and is satisfied when λmin _i_ _[µ][i][u][i][u]i[T]_ _≥_ log(C/δ)/4γ[2], as

minu _i_ _[µ][i][v][i][ ≥]_ _[λ][min]_ _i_ _[µ][i][u][i][u]i[T]_ . Here, C is a problem dependent constant that will be chosen P 

later.

P  P 

Now consider an ϵ-net in ℓ2 norm over the surface unit sphere denoted by N (ϵ, d). It is well known
that (ϵ, d) (3/ϵ)[d]. Now any z S[d]1[−][1] can be written as z = x + u where x (ϵ, d) and
_|N_ _| ≤_ _∈_ _∈N_
**_u_** B[d]ϵ _[−][1]. Therefore, we have that_
_∈_

_L(z, θ)_ _L(x, θ)_ _λmax(L(θ))ϵ_
_≥_ _−_

A similar argument as above gives us,


_λmax(L(θ)) = max_ _L(u, θ)_ max _L(u, θ)_
**_u∈B[d]1[−][1]_** _≤_ **_x∈N (ϵ,d)_** _[L][(][x][,][ θ][) + 1]2 u[max]∈B1[d][−][1]_

= _λmax(L(θ))_ 2 max
_⇒_ _≤_ **_x_** (ϵ,d) _[L][(][x][,][ θ][)]_
_∈N_

Thus by an union bound over the net we have wp 1 − 2δ and other conditions,


2ϵ

_λmax_

! _−_ _θ_ 2

_∥_ _∥[2]_

_µi_ **_u, ui_** _C_
_⟨_ _⟩[2]_ log

**_θ_** _δ_
_∥_ _∥[4]_ 


_µiuiu[T]i_


_µiuiu[T]i_


**_z∈minS1[d][−][1]_** _L(z, θ) ≥_


_λmin_
_θ_ 2
_∥_ _∥[2]_


_µi_ **_u, ui_** _C_

max 2(1 + 2ϵ) _⟨_ _⟩[2]_ log _._
_−_ **_u∈S1[d][−][1]_** vuu _i_ _∥θ∥[4]_  _δ_ 

t[X]

if the conditions above hold with C = (3/ϵ)[d]. We can now set ϵ = 1/(8 _∗_ _χ) where χ is the condition_
number of the matrix _i_ _[µ][i][u][i][u]i[T]_ [. Now by virtue of that fact that] _λmax([P]_ _µiuiu[T]i_ [) log(][C/δ][)][ ≤]

_λmin_ _i_ _[µ][i][u][i][u]i[T]_ _/16[P] we have the result._ p
 P 

Now we will prove a result on the smoothness of the negative log likelihood for the Poisson regression
setting.
**Lemma 9. With probability at least 1 −** 2δ, we have

_λmax_ **_θ[L][(][y][n][;][ θ][)]_** _λmax(M_ ), (12)
 ∇[2]  _≤_ [2][nR]γ[2] [2]

_if λmin(M_ ) _R[2](d log 6 + log(1/δ))/4nγ[2]._
_≥_

_Proof. We start by writing out the Hessian again but this time upper bounding it in semi-definite_
ordering,


_yixix[T]i_

**_θ, xi_**
_⟨_ _⟩[2][ ≼]_


_yixix[T]i_

_γ[2]_


_yiR[2]uiu[T]i_ := L(θ)

_γ[2]_


_∇θ[2]_ _[L][ =]_


_i=1_


_i=1_


_i=1_


-----

For a fixed unit vector u S[d]1[−][1] let us define,
_∈_

_n_

_yiR[2]u[T]_ **_uiu[T]i_** **_[u]_**

_L(u, θ) :=_ :=

_γ[2]_

_i=1_

X


_yivi,_
_i=1_

X


where vi = R[2]⟨ui, u⟩[2]/γ[2]. Proceeding as in Lemma 8 we conclude that,


_ν[2]_ =


_νi[2][, α][ = 0][.][56 max]_ _vi_


(13)


_yivi_ _SE_
_∈_


where νi = 2µivi[2][. This implies that wp atleast][ 1][ −] [2][δ/C][ we have,]

_C_

_yivi_ _µivi_ 2 _µivi[2]_ [log]

_|_ _i_ _−_ _i_ _| ≤_ s  _δ_
X X X

if,


_C_
_µivi[2]_ [log]

_δ_




2( _µivi[2][)][/][(max]_ _vi)._
_≤_ _i_
X


Note that the above condition is mild and is satisfied when λmin _i_ _[µ][i][u][i][u]i[T]_ _≥_ _R[2]_ log(C/δ)/4γ[2],

as minu _i_ _[µ][i][v][i][ ≥]_ _[λ][min]_ _i_ _[µ][i][u][i][u]i[T]_ . Here, C is a problem dependent constant that will be chosen P 

later.

P  P 

Now consider an 1/2-net in ℓ2 norm over the surface unit sphere denoted by N (0.5, d). It is well
known that (ϵ, d) (3/ϵ)[d]. Now any z S[d]1[−][1] can be written as z = x+u where x (0.5, d)
_|N_ _| ≤_ _∈_ _∈N_
and u ∈ B[d]0.[−]5[1][. Therefore, we have that,]

_λmax(L(θ)) = max_ _L(u, θ)_ max _L(u, θ)_
**_u∈B[d]1[−][1]_** _≤_ **_x∈N (ϵ,d)_** _[L][(][x][,][ θ][) + 1]2 u[max]∈B1[d][−][1]_


= _λmax(L(θ))_ 2 max
_⇒_ _≤_ **_x_** (ϵ,d) _[L][(][x][,][ θ][)]_
_∈N_

Thus we set C = 6[d] and obtain wp at least 1 − 2δ,

_λmax(L(θ))_ 2 _[nR][2]_
_≤_ _γ[2][ λ][max][(][M]_ [)][,]

provided 4 _λmax(M_ ) log(C/δ) _λmax(M_ )/[√]n.

_≤_

p

_Proof of Corollary 1. We show that the conditions of Theorem 2 hold._

_Creating Nets: First we need to create an ϵ-net over the parameter space Θ. We start by creating an_
_ϵ-net over the sphere with radius w. Now suppose, ϵ < γ/2R. Then we remove all centers θc if ∃i_
s.t **_θc, xi_** _< γ/2. This is a valid ϵ-net over Θ as all net partitions that are removed do not have any_
_⟨_ _⟩_
points lying in Θ. In subsequent section, we will always follow this strategy to create ϵ-nets over
subsets of Θ.

_Strong Convexity and Smoothness: From Lemma 8 and 9 we have that,_


_β_

_χ._

_α_ [=][ w]γ[2][R][2] [2]

with probability 1 − _O(δ)._

_KL Divergence: The L in Theorem 2 is bounded by 2wR[2]/γ according to Lemma 7._


Combining the above into Theorem 2 and using the estimator in Section F we get our result.


-----

F MEDIAN OF MEANS ESTIMATOR FOR POISSON

In this section we will design a median of means estimator for the Poisson regression model based
on the estimator proposed in the work of Lugosi & Mendelson (2019b). Recall that we have a fixed
design matrix X ∈ R[n][×][d] with rows x1, . . ., xn, and for each i ∈ [n], yi is drawn from a Poisson
distribution with mean µi = θθθ[∗] _· xi. We will further assume that the design matrix is chosen from an_
(L, 4) hyper-contractive distribution. Mathematically this implies that for any unit vector u

E[ Σ[−] 2[1] x · u 4] ≤ _L · E[_ Σ[−] 2[1] x · u 2]2.

For simplicity we will assume that  _L = O(1). In the above definition note that _  E is the empirical
expectation over teh fixed design. This is a benign assumption and for instance would be satisfied if
the design matrix is drawn from a sub-Gaussian distribution. In the general case, the obtained bounds
will scale with L. We have the following guarantee associated with Algorithm 1.

**Algorithm 1: Median of Means Estimator**
**Input: Samples S = {(x1, y1), . . ., (xn, yn)}, confidence parameter δ.**
**Step 1: Compute Σ = E[xx[⊤]]. Form S[′]** = {x[′]1 [=][ y][1][Σ][−] [1]2 x1, . . ., x[′]n [=][ y][n][Σ][−] 2[1] xn}.

**Step 2: Randomly partition S[′]** into k blocks of size n/k each where k = 20⌈log( [1]δ [)][⌉][.]

**Step 3: Feed in the k blocks to the median-of-means estimator of Lugosi & Mendelson (2019b)**
to get ˆv.
**Step 4: Return** **_θ[ˆ] = Σ[−]_** 2[1] ˆv.


**Theorem 4. There is an absolute constant c > 0 such that with probability at least 1** _−_ _δ, Algorithm 1_
_outputs_ **_θ[ˆ] such that_**

_d + log(_ [1]δ [)]
_∥θ[ˆ] −_ _θθθ[∗]∥Σ[2]_ _[≤]_ _[c][ · ∥][θ]θθ[∗]∥[2]_ _· λmax(Σ)_ _n_ _._ (14)
  

_Proof. The proof is exactly along the lines of the proof of Theorem 1 in the work of (Lugosi &_
Mendelson, 2019b) that we repeat here for the sake of completeness since the original theorem is not
explicitly stated for a fixed design setting. To begin with notice that

E[yΣ[−] 2[1] x] = E[(θθθ[∗] _· x)Σ[−]_ 2[1] x)] (15)

= E[(1 _θθθ[∗]_ _· x)Σ[−]_ 2[1] x] (16)

= Σ 2 _θθθ[∗]._ (17)


Hence if ˆv is the output of the median of the means estimator in Step 3 of Algorithm 1, then
1 1
the least squares error of **_θ[ˆ] is exactly_** **vˆ** Σ 2 _θθθ[∗]_ . For convenience define µ[′] = Σ 2 _θθθ[∗]_ and
1 1 _∥_ _−_ _∥[2]_
Σ[′] = E[x[′]x[′⊤]] − Σ 2 _θθθ[∗]θθθ[∗⊤]Σ_ 2 . Exactly as in Lugosi & Mendelson (2019b) our goal is to show that
**_µ[′]_** beats any other vector v in the median of means tournament if v is far away from µ[′]. To quantify
this define

_Tr(Σ[′])_ _λmax(Σ[′])_

_r = max_ 400 _, 4√10_ _._

_n_ _n_

r r

For a fixed vector v of length r, and block  _Bj, µ[′]_ beats v if 


_−_ [2]n[k]


(x[′]i
_iX∈Bj_ _[−]_ **_[µ][′][)][ ·][ v][ +][ r >][ 0][.]_**


Let us denote by σi,j 0, 1 a random variable representing whether data point i is in block j or
not. By Chebychev’s inequality we get that with probability at least ∈{ _}_ 9/10,

_n_ _iX∈Bj(x[′]i_ _[−]_ **_[µ][′][)][ ·][ v]_** _≤_ _n[k]_ _√10sXi_ E[σi,j[2] [((][x][′]i _[−]_ **_[µ][′][)][ ·][ v][)][2][]]_** (18)

_[k]_

= _n[k]_ _√10_ _np_ _n[1]_ E[((x[′]i (19)

s _i_ _[−]_ **_[µ][′][)][ ·][ v][)][2][]][.]_**

X


-----

Here p is the probability of a point belonging to block k. Noting that np = Θ(n/k) we get that with
probability at least 9/10,


_kλmax(Σ[′])_


(x[′]i
_iX∈Bj_ _[−]_ **_[µ][′][)][ ·][ v]_**


(20)


10r


Applying binomial tail estimates we get that with probability at least 1 − _e[−][k/][180], µ[′]_ beats v on at
least 8/10 of the blocks. By applying the covering argument verbatim as in the proof of Theorem 1
in Lugosi & Mendelson (2019b) we get that with probability at least 1 − _δ, ˆv will satisfy_

**_vˆ_** Σ 12 _θθθ[∗]_ _c_ _λmax(Σ[′])_ _d + log(_ [1]δ [)] _._
_∥_ _−_ _∥[2]_ _≤_ _·_ _n_
  

Finally, it remains to bound the spectrum of Σ[′]. We have


1 1

Σ[′] = E[y[2]Σ[−] 2[1] xx[⊤]Σ[−] 2[1] ] − Σ 2 _θθθ[∗]θθθ[∗⊤]Σ_ 2 (21)

_⪯_ E[((θ · x)[2] + θ · x)Σ[−] [1]2 xx[⊤]Σ[−] 2[1] ] (22)

_⪯_ _T1 + T2,_ (23)


where


2 1
_T1 = E[_ _θθθ[∗]_ _· x_ Σ− 2 xx[⊤]Σ[−] 2[1] ], (24)

_T2 = E[ θθθ[∗]_ _· xΣ[−]_ 2[1] xx[⊤]Σ[−] 2[1] ]. (25)

To bound T1, T2 we note that for any function  _m(x) we have_

E[m(x)Σ[−] 2[1] xx[⊤]Σ[−] 2[1] ] ⪯ E[m[2](x)]I. (26)

Using the above inequality and the fact that the design matrix isp (O(1), 4) hyper-contractive we get
that


_E[(θθθ[∗]_ _· x)[2]] = O(∥θθθ[∗]∥_ _λmax(Σ))_ (27)

_E[(θθθ[∗]_ _· x)[4]] = O(∥θθθ[∗]∥p[2]λmax(Σ))._ (28)


Combining the above we get that


Σ[′] _⪯_ _T1 + T2_ (29)
_⪯_ _O(λmax(Σ))I._ (30)

G 1-D POISSON REGRESSION

When the covariates are one dimensional, a sharper analysis can actually be performed to show that
the MLE dominates TMO in all regimes in the Poisson regression setting considered above.
**Lemma 10. There exists an absolute constant c ≥** 2 such that for any δ ∈ [ _n[1][c][,][ 1)][, it holds with]_

_probability at least 1 −_ _δ that,_

(θ[ˆ]ls) = [1] _n_ (yi _θls)[2]_ _ni=1n_ _[|][x][i][|][3]_ log 1 _._ (31)
_E_ _n_ _i=1_ _−_ [ˆ] _≤_ [4][ · |]n[θ][∗][|] _·_ P _i=1_ _[x]i[2]_  _δ_ 

X

_The bound above is also tight i.e with constant probability it holds thatP_

(θ[ˆ]ls) = [1] _n_ (yi _θls)[2]_ = Ω _|θ∗|_ _ni=1n_ _[|][x][i][|][3]_ _._ (32)
_E_ _n_ Xi=1 _−_ [ˆ]  _n_ _[·]_ P _i=1_ _[x]i[2]_ 

P:=B(θ[ˆ]ls)
| {z }

We provide the proofs in later in the section. Having established the bound for least squares estimator,
we next prove the following upper bound on the mean squared error achieved by the MLE.


-----

**Theorem 5. There exists an absolute constant c ≥** 2 such that for any δ ∈ [ _n[1][c][,][ 1)][, it holds with]_

_probability at least 1 −_ _δ that,_

_n_

_i=1_ _[x]i[2]_ 2

_E(θ[ˆ]mle) = n[1]_ Xi=1(yi − _θ[ˆ]mle)[2]_ _≤_ [4][ · |]n[θ][∗][|] _·_  [P]ni[n]=1 _[|][x][i][|]_  log _δ_ . (33)

P:=B(θ[ˆ]mle)
| {z }

It is easy to see that the covariate dependent term in the bound on the mean squared error achieved by
_θˆmle (defined in Eq. 32) is always better the corresponding term in the bound achieved by_ _θ[ˆ]ls (defined_
in Eq. 33). To see this notice that,

(θ[ˆ]ls) _i=1_ _i=1_
_B_ = [(][P][n] _[|][x][i][|][3][)(][P][n]_ _[|][x][i][|][)]_ 1. (from Cauchy-Schwarz inequality.)

_B(θ[ˆ]mle)_ ([P][n]i=1 _[x]i[2][)][2]_ _≥_

Furthermore, in many cases the bound achieved by _θ[ˆ]mle can be significantly better than the one_
achieved by _θ[ˆ]ls. As an example consider a skewed data distribution where_ _n of the xi’s take a large_

_[√]_
value of _n, while the remaining data points take a value of n[ϵ], where ϵ is a small constant. In this_

_[√]_
case we have that,

_B(θ[ˆ]ls)_ = [(][P]i[n]=1 _[x]i[3][)(][P]i[n]=1_ _[x][i][)]_ = Ω(n[ϵ]).

_B(θ[ˆ]mle)_ ([P][n]i=1 _[x]i[2][)][2]_

_Proof of Lemma 10. Notice that_


(θ[ˆ]ls) = [1] _θls_ _θ[∗])[2](_
_E_ _n_ [(ˆ] _−_


_x[2]i_ [)][.] (34)
_i=1_

X


Hence, it is enough to bound the parameter distance, i.e., (θ[ˆ]ls _θ[∗])[2]. In order to do that we first_
notice that yixi is a sub-exponential random variable with parameters − _νi[2][, α][ where where][ ν]i[2]_ [= 2][µ][i][x]i[2][,]
_µi = θ[∗]xi, and αi = 0.56xi (Rinaldo, 2019). In other words,_

_yixi ∈_ _SE(νi[2][, α][)][,]_ (35)

Thus from the bound on a sum of independent sub-exponential variables we have that,


_yixi_ _SE(ν[2]_ =
_∈_


_νi[2][, α][ = 0][.][56 max]_ _xi)._ (36)
_i_

2 exp 2ν[2] if t _α_
_−_ _[t][2]_ _≤_ _[ν][2]_
(2 exp 2α  otherwise

_−_ _[t]_
  


Thus we have that,

P _|_ _i_ _yixi −_ _i_

X X

This means, that w.p at least 1 − 2δ,


_µixi_ _t_
_| ≥_


1
_µix[2]i_ [) log]

_δ_




_yixi_
_−_


_µixi_ 2
_| ≤_
_i_

X

1
_µix[2]i_ [) log]

_δ_




if


if

( _µix[2]i_ [) log] 1 2 _µix[2]i_

s _i_  _δ_  _≤_ 0.56 maxi xi

X [P]

The condition above is satisfied under our assumptions on xi and δ, thereby leading to the bound

3

(θ[ˆ]ls _θ[∗])_ 2 _w log(1/δ)_ _xi_ _._
_−_ _≤_ pPi _[x]i[2]_
p

The bound on the MSE claimed in the lemma then follows. P


-----

Now we prove the lower bound. Again it is enough to show a lower bound on _θls_ _θ[∗]_ . Notice that
_|[ˆ]_ _−_ _|_

_n_
_θˆls_ _θ[∗]_ = _i=1[(][y]n[i][ −]_ _[µ][i][)][x][i]_ _._ (37)
_−_ P _i=1_ _[x]i[2]_

Define the random variable Z = _i=1[(][y][i][ −]_ _[µ][i][)]P[x][i][. We will show anti-concentration for][ Z][ by]_
computing the hyper-contractivity of the random variable. Recall that a random varibale Z is η-HC
(hyper-contractive) if E[Z [4]] _η[4]E[Z_ [2]][2]. Next we have
_≤_ [P][n]


(yi _µi)xi][2]_ =
_i=1_ _−_

Xn

(yi _µi)xi][2]_
_i=1_ _−_

X


E[Z [2]] = E[

E[Z [4]] = E[


_µix[2]i_ _[.]_ (38)
_i=1_

X


= _µi(1 + 3µi)x[4]i_ [+ 2] _µiµjx[2]i_ _[x]j[2]_

Xi=1 Xi≠ _j_

:= ∆. (39)


2
_n_
_i=1_ _[µ][i][x]i[2]_

Hence Z is η-HC with η[4] = P ∆ .

 

From anti-concentration of hyper-contractive random variables (O’Donnell, 2014) we have


P( _Z_
_|_ _| ≥_ 2[1]

_n_

(yi _µi)xi_
_i=1_ _−_ _| ≥_ 2[1]

X


E[Z [2]]) ≥ Ω(η[4]). (40)


Hence we get that


|


_µix[2]i_ Ω(η[4])

 _≥_

_i=1_

X

 _n_ 2

_i=1_ _[µ][i][x]i[2]_

_≥_  P ∆ 


(41)

(42)


Next notice that since µi ≥ _γ for all i (Assumption2_ 1), we have that 1 + 3µi ≤ (3 + _γ[1]_ [)][µ][i][. This]

implies that ∆ _≤_ (3 + _γ[1]_ [)] _ni=1_ _[µ][i][x]i[2]_ . Hence if γ is a constant then with probability at least

1

3+ _γ[1]_ [= Ω(1)][ we have]  P 

1 _i=1_ _[x]i[3][)]_
_E(θ[ˆ]ls) = Ω_ _n_ _ni=1_ _[x]i[2]_ _._ (43)
 _[·][ (][P][n]_ 

P

_Proof of Theorem 5. Recall that_ _θ[ˆ]mle is defined as_


_i=1_ _θxi −_ _yi log(θxi)._ (44)

X


_θˆmle = argmin_
_θ∈Θ_


Setting the gradient of the objective to zero, we get the following closed form expression for _θ[ˆ]mle._

_n_
_θˆmle =_ _ni=1_ _[y][i]_ _._ (45)
Pi=1 _[x][i]_
P


-----

Next, we note that Z = _i=1_ _[y][i][ is a poission random random variable with parameter][ µ][ =]_
_n_
_i=1_ _[θ][∗][x][i][. From tail bounds for Poisson random variables (][Klar][,][ 2000][) we have that for any][ ϵ >][ 0][,]_

[P][n]
P P[|Z − _µ| > ϵ] ≤_ 2e[−] _µ[ϵ]+[2]ϵ ._ (46)

Taking ϵ = c[√]µ log( [1]δ [)][, we get that with probability at least][ 1][ −] _[δ][,]_

_n_

1
_yi_ _µ_ _µ log(_ [1] (47)

_|_ _i=1_ _−_ _| ∈_ 2 _[,][ 2]_ _δ_ [)][,]

X  [r]

provided that log( [1]δ [)][ < µ][ (that holds for our choice][ δ][, once][ n][ is large enough). Hence we conclude]

that with probability at least 1 − _δ, the mean squared error of_ _θ[ˆ]mle is bounded by_

_n_ _n_
_i=1_ _[θ][∗][x][i]_ _i=1_ _[y][i]_
_θmle_ _θ[∗]_ = _n_ _n_
_|[ˆ]_ _−_ _|_ P _i=1_ _[x][i]_ _−_ Pi=1 _[x][i]_
P _µ log(_ [1]δ [)] P

= O _n_
q

_i=1_ _[x][i]_

 

= O P|θ[∗]| log(n [1]δ [)] _._ (48)
q

_i=1_ _[x][i]_

 

The bound on the mean squared error follows from the above.pP

H COMPETITIVENESS FOR PARETO REGRESSION


We verify the conditions of Theorem 2 for the Pareto regression setting.
**Lemma 11. Let f** (·; θ) = _i=1_ _[p][(][·|][x][i][;][ θ][)][ where][ p][ is defined in Eq.][ (][4][)][. If][ ∥][θ][ −]_ **_[θ][′][∥]2_** _[≤]_ _[δ][ then,]_

2bnδR

[Q][n] _∥f_ (·; θ) − _f_ (·; θ[′])∥1 ≤ s _γ_

_Proof. By Pinskers’ inequality we have the following chain,_


_f_ ( ; θ) _f_ ( ; θ[′]) 1
_∥_ _·_ _−_ _·_ _∥_ _≤_


2DKL(f ( ; θ); f ( ; θ[′]))

_·_ _·_


2
v
u
u
t


_DKL(p(_ **_xi; θ); p(_** **_xi; θ[′]))_**

_·|_ _·|_
_i=1_

X


2b log mi log m[′]i[|]

_≤_ vu _i=1_ _|_ _−_

uX
t _n_

2b **_θ, xi_** **_θ[′], xi_**
_|⟨_ _⟩−⟨_ _⟩|_

_≤_ v _γ_

u _i=1_
uX
t

2bn∥θ − **_θ[′]∥2R_** _._

_≤_ s _γ_

The second inequality follows from the fact that log x is Lipschitz with parameter L if x > 1/L. The
last inequality follows from Cauchy-Schwarz and the norm bound on xi’s.

Now we prove the rest of the conditions.
**Lemma 12. We have the following smoothness bound,**

_λmax(_ **_θ[L][)][ ≤]_** [(][b][ −] [1)] _λmax(Σ)._
_∇[2]_ _γ[2]_


-----

_Proof. We start by writing out the Hessian,_


(b − 1) _i_ [:=][ L][(][θ][)]

**_θ, xi_**
_⟨_ _⟩[2][ x][i][x][T]_


_∇θ[2]_ _[L][ =]_


_i=1_


Using the fact that **_θ, xi_** _γ gives us the result._
_⟨_ _⟩≥_

**Lemma 13. We have the following strong convexity bound,**

_λmin(_ **_θ[L][)][ ≥]_** [(][b][ −] [1)]
_∇[2]_ _w[2]R[2][ λ][min][(Σ)][.]_

_Proof. It follows from the expression of the Hessian in Lemma 12 and using the fact_ **_θ, xi_**
_⟨_ _⟩≤_
**_θ_** 2 **_xi_** 2
_∥_ _∥_ _∥_ _∥_

_Proof of Corollary 2. We show that the conditions of Theorem 2 hold._

_Creating Nets: First we need to create an ϵ-net over the parameter space Θ. We start by creating an_
_ϵ-net over the sphere with radius w. Now suppose, ϵ < γ/2R. Then we remove all centers θc if ∃i_
s.t **_θc, xi_** _< γ/2. This is a valid ϵ-net over Θ as all net partitions that are removed do not have any_
_⟨_ _⟩_
points lying in Θ. In subsequent section, we will always follow this strategy to create ϵ-nets over
subsets of Θ.

_Strong Convexity and Smoothness: From Lemma 13 and 12 we have that,_

_β_

_ζ._

_α_ [=][ w]γ[2][R][2] [2]

_KL Divergence: The L in Theorem 2 is bounded by 2bR/γ according to Lemma 11._

Combining the above into Theorem 2 and using the estimator in (Hsu & Sabato, 2016) we get our
result.

I MORE ON EXPERIMENTS

We provide more experimental details in this section.

I.1 METRICS

The metrics and loss functions used are as follows:

**MSE: The metric is**

_n_

1

(ˆyi _yi)[2]._

_n_ _i=1_ _−_

X

RMSE is just the square-root of this metric.

**MAE: The metric is**

_n_

1

_yˆi_ _yi_ _._

_n_ _i=1|_ _−_ _|_

X


**WAPE: The metric is** _ni=1n[|]y[ˆ]i −_ _yi|_
_i=1_
P

_[|][y][i][|]_

**MAPE: The metric is** P
1 _yi_
_n_ _yi_

_i:Xyi=0̸_

[1][ −] [ˆ]


-----

**Quantile Loss: The reported metrics in Table 3 are the normalized quantile losses defined as,**

_n_

2ρ(yi _yˆi)Iyi_ _yˆi_ [+ 2][ρ][(ˆ]yi _yi)Iyi<yˆi_
_−_ _≥_ _n_ _−_ _._ (49)

_i=1_ _i=1_

X _[|][y][i][|]_

During training the unnormalized version is used for quantile regression.P


**Huber Loss: The loss is given by,**

_Lδ(y, ˆy) =_


12 [(][y][ −] _y[ˆ])[2],_ if |y − _yˆ| ≤_ _δ_
_δ|y −_ _yˆ| −_ _[δ]2[2]_ _[,]_ otherwise.


I.2 MORE DETAILS ABOUT THE MIXTURE DISTRIBUTION

We use a mixture distribution between zero, a continuous extension of negative binomial (NB)
distribution and Pareto. The continuous extension of negative binomial is such that the p.d.f at y is
proportional to,

Γ(n + k)
_p(y)_
_∝_ Γ(k + 1)Γ(n) [(1][ −] _[p][)][n][p][k]_

given parameters n and p. That is, the p.m.f of a regular discrete NB distribution is written in terms
of Gamma functions and then we extend that to non-integral points up to proportionality, such that
the measure sums to 1. This definition of NB is the standard implementation in Tensorflow (Abadi
et al., 2016; Dillon et al., 2017), in order to support both discrete and continuous data . It has been
used in many regression datasets before, especially in the field of genomics where the collected data
is a discrete continuous mixture (Robinson & Smyth, 2008; Chen et al., 2016; McCarthy et al., 2012).

_Why not use a discrete-continuous mixture of zero, discrete NB and Pareto?_

A mixture of these three distributions is a discrete continuous mixture, whose CDF is well-defined. It
is possible to define a likelihood in a standard manner such that it has a component proportional to
the pdf of Pareto at all points and to this we add dirac delta functions proportional to magnitude of
the negative-binomial pmf at non-negative integral points. We have an extra mass at zero to account
for the zero component. The sum of the dirac masses along with the integral of the Pareto density
sums to one. However, such a likelihood is not very useful in practice. For instance, if the data is
mostly continuous but not heavy tailed, the log-likelihood would mostly have the Pareto component
(because non-integral points have no contribution from the other components) which is not a desirable
outcome, as a NB distribution can better model sub-Gaussian and sub-Exponential data in terms of
moments.

I.3 MAPPING OF OUTPUTS FOR ZNBP

For the ZNBP model we require an output dimension size of 6. The first three dimensions are mapped
through a softmax layer (Goodfellow et al., 2016) to mixture weights. The fourth dimension is
mapped to the ’n’ in negative-binomial likelihood through the link function,

_x + 1,_ if x > 0
_φ(x) =_
1/(1 _x)_ otherwise _[.]_
 _−_

The fifth dimension is mapped to ’p’ of the negative-binomial though the sigmoid function. The last
dimension is mapped to the scale parameter for the Pareto component using the φ(x) link function
above.

I.4 HARDWARE

We use the Tesla V100 architecture GPU for our experiments. We use Intel Xeon Silver 4216 16-Core,
32-Thread, 2.1 GHz (3.2 GHz Turbo) CPU and our post-hoc inference for MLE is parallelized over
all the cores.


-----

I.5 MORE DETAILS ABOUT INFERENCE FOR MLE

We follow the approach of monte-carlo sampling. For each inference sample x[′], we generate 10k
samples from the learnt distribution p( **_x[′]; θθ[ˆ]θmle). Then we compute the correct statistics. For MSE,_**

_·|_
RMSE the statistic is just the mean and for WAPE, MAE it is the median. For a quantile, it is the
corresponding quantile from the empirical distribution. For any loss of the form,

_β_

_y_
_ℓ(y, ˆy) =_

_yˆ_

 

the optimal statistic is the median from the distribution proportional to y[β]p(y **_x[′]; θθ[ˆ]θmle) (Gneiting,_**

[1][ −] _|_

2011). Note that the MAPE falls under the above with β = −1 and relative error corresponds to
_β = 1. The statistic can be computed by importance weighing the empirical samples._


I.6 MODELS, HYPERPARAMETERS AND TUNING

MLE TMO

Relu

Link Function Adj.

Relu

FC (1 Hidden) FC (1 Hidden)

LSTM LSTM LSTM LSTM LSTM LSTM


Encoder


Decoder


Figure 1: Time-Series Seq-2-Seq models. The MLE config is shown on the left and the TMO config
is shown at the right. The main difference is the output dimension and the loss function. In order to
keep the number of parameters the same, in the TMO model we add an extra layer of size 6 with
Relu activation (shown as Adj. (adjustment))

For the time-series datasets, the model is a seq-2-seq model with one hidden layer LSTMs for both
the encoder and the decoder. The hidden layer size is h = 256. The output layer of the LSTM is
connected to the final output layer through one hidden layer also having h neurons. Note that h was
tuned in the set [8, 16, 32, 64, 128, 256] and for all datasets and models 256 was chosen. In order to
be keep the number of parameters exactly the same, in the TMO models we add an extra layer with
ReLU with 6 neurons before the output.

We tuned the learning rate for Adam optimizer in the range [1e-5, 1e-1] in log-scale. The batch-size
was also tuned in [64, 128, 256, 512] and the Huber-δ in [2[i] for i in range(-8, 8)]. The learning rate
was eventually chosen as 2.77e-3 for both datasets, as it was close to the optimal values selected for
all baseline models. The batch-size was chosen to be 512 and the Huber-δ was 32 and 64 for M5 and
Favorita respectively.

For the regression datasets the model is a DNN with one hidden layer of size 32. For the Bicycle
dataset the categorical features had there own embedding layer. The features [season, month, weekday,
weathersit] had embedding layer sizes [2,4,4,2].

We tuned the learning rate for Adam optimizer in the range [1e-5, 1e-1] in log-scale. The batch-size
was also tuned in [64, 128, 256, 512] and the Huber-δ in [2[i] for i in range(-8, 8)]. The learning rate
was eventually chosen as 3e-3 for the gas turbine dataset based on the best perforamce of the baseline
models and 1.98e-3 for the gas turbine dataset. The batch-size was chosen to be 512 and the Huber-δ
was 128 and 32 for Bicyle Share and Gas turbine respectively.

For the ZNBP model we also tune the α parameter in the pareto component between [3, 4, 5]. The
value of 3 was selected for all datasets, except for gas turbine where we used α = 5.


-----

## MLE TMO

Link Function

Linear

Relu

Linear layer

Relu

Linear Layer


Figure 2: Fully connected network for regression models. The MLE config is shown on the left and
the TMO config is shown at the right. The main difference is output dimension and the loss function.

Dataset _n_ _d_

M5 1879 256
Favorita 1653 256
Bicycle 584 32
Gas Turbine 22039 32

Table 5: Note that here d refers to the dimension of the last layer of the architecture used in the
respective datasets.

We used a batched version of GP-UCB (Srinivas et al., 2009) to tune the hyper-parameters. We used
Tensorflow (Abadi et al., 2016) to train our models. In Table 5, we provide the number of samples
and the dimension of the last layer in each of our datasets.

I.7 DATASETS

For the Favorita and M5 dataset we used the product hierarchy over the item-level time series. Along
with the item-level (leaf) time-series, we also add all the higher-level (parent) time-series from the
product hierarchy (i.e. we add family and class level time-series for Favorita, and department and
category level time series for M5). The time-series for a parent time-series is obtained as the mean
of the time-series of its children. This is closer to a real forecasting setting in practice where one is
interested in all levels of the hierarchy. The metrics reported are over all the time-series (both parents
and leaves) treated equally. The history length for our predictions is set to 28.

For the M5 dataset the validation scores are computed using the predictions from time steps 1886
to 1899, and test scores on steps 1900 to 1913. For the Favorita dataset the validation scores are
computed using the predictions from time steps 1660 to 1673, and test scores on steps 1674 to 1687.

The train test splits are as mentioned in the main paper. For the Gas turbine dataset we use the official
train test split. For the Bicycle share data there is no official split, but we use a randomly chosen fixed
10% as the test set for all our experiments.


-----

I.8 ADDITIONAL EXPERIMENTS AND FIGURES

In order to show the dependency of λmax(Σ) in the bound in Corollary 1 we perform a simulated
experiment. We generate a dataset with n = 5000 and d = 10 such that each coordinate of x
is distributed i.i.d from a uniform distribution between [0, U ]. In out experiment, we vary U in
_{1, 2, · · ·, 9}. Then y is genearted from a Poisson distribution with rate ⟨θθθ[∗], x⟩_ for a fixed θθθ[∗]. This
varies λmax(Σ) which is equal to U [2]/12. We train and validate on 2500 samples with early stopping
and plot the squared loss achieved on the test by the _θθ[ˆ]θmle based estimator in Figure 3 versus λmax(Σ)._
We can clearly see a linear relationship which further validates our theoretical results.

In Figure 4, we plot the average training loss as a function of training iterations for the
MLE(ZNBP)model. We can see that the loss converges to a minima.

Figure 3: Test squared error versus λmax(Σ). We can clearly see a linear relationship. Each point in
the plot is averaged over 10 runs and we plot the standard error bars.

Figure 4: We plot the average training loss for the MLE(ZNBP)model as a function of training
iterations. We can observe that the training curve converges.

J EXTENDED DISCUSSION AND LIMITATIONS

We advocate for MLE with a suitably chosen likelihood family followed by post-hoc inference tuned
to the target metric of interest, in favor of ERM directly with the target metric. On the theory side,
we prove a general result that shows competitiveness of MLE with any estimator for a target metric
under fairly general conditions. Application of the bound in the case of MSE for Poisson regression
and Pareto regression is shown. We believe that our general result is of independent interest and can
be used as a tool to prove competitiveness of MLE for a wide variety of problems. Such applications
can be an interesting direction of future work.

On the empirical side we show that a well designed mixture likelihood like the one from Section 5 can
adapt quite well to different datasets as the mixture weights are trained. As we have mentioned before,


-----

the MLE log-likelihood loss in such cases can be non-convex which might lead to some limitations in
terms of optimization. However, we observed that this is usually not a problem in practice and the
solutions that can be reached by mini-batch SGD can be quite good in terms of performance.

In conclusion we would recommend the following protocol for a practitioner based on our theoretical
and empirical observations:

If the overall problem is convex for TMO but introducing a MLE loss makes the problem nonconvex, then the gains from the MLE approach might be neutralized by the added hardness of the
non-convexity introduced. An example of such a situation is TMO for minimizing square loss on
a linear function class, which is just least-squares linear regression, but introduction of a mixture
likelihood like the one in Section 4 makes the problem non-convex. In this case it might be better
to stick with TMO or at least proceed with caution with the MLE approach. Note that if the chosen
MLE retains the convexity of the problem, for example Poisson MLE in Section 4.1, then we would
still recommend going with the MLE approach.

However, in many practical scenarios when training using a deep network, the TMO approach is
non-convex to begin with, even when the target metric itself is something simple and convex like the
square loss. In such a case we would recommend the MLE approach with a likelihood class that can
capture inductive biases about the dataset. This is because both TMO and MLE are non-convex and it
is better to capitalize on the potential gains from the MLE approach.

Finally, note that the user can always choose between TMO and even between different likelihood
classes through cross-validation in a practical setting. If the practitioner would like to forgo the
decision making in choosing the likelihood class, we recommend using a versatile likelihood like the
mixture likelihood in Sections 5.

We do not anticipate this work to have any negative social or ethical impact.


-----

