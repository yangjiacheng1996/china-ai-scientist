# LEARNING AND CONTROLLING THE SOURCE-FILTER
## REPRESENTATION OF SPEECH WITH A VARIATIONAL AUTOENCODER

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Understanding and controlling latent representations in deep generative models
is a challenging yet important problem for analyzing, transforming and generating various types of data. In speech processing, inspiring from the anatomical
mechanisms of phonation, the source-filter model considers that speech signals
are produced from a few independent and physically meaningful continuous latent
factors, among which the fundamental frequency and the formants are of primary
importance. In this work, we show that the source-filter model of speech production naturally arises in the latent space of a variational autoencoder (VAE) trained
in an unsupervised fashion on a dataset of natural speech signals. Using only a few
seconds of labeled speech signals generated with an artificial speech synthesizer,
we experimentally demonstrate that the fundamental frequency and formant frequencies are encoded in orthogonal subspaces of the VAE latent space and we develop a weakly-supervised method to accurately and independently control these
speech factors of variation within the learned latent subspaces. Without requiring additional information such as text or human-labeled data, we propose a deep
generative model of speech spectrograms that is conditioned on the fundamental
frequency and formant frequencies, and which is applied to the transformation of
speech signals.

1 INTRODUCTION

High-dimensional data such as natural images or speech signals exhibit some form of regularity
which prevents their dimensions from varying independently from each other. This suggests that
there exists a latent representation of smaller dimension from which the high-dimensional observed
data were generated. Discovering the hidden properties of complex data is the goal of representation learning, and deep latent-variable generative models have emerged as promising unsupervised
approaches (Goodfellow et al., 2014; Kingma & Welling, 2014; Rezende et al., 2014; Chen et al.,
2016; Higgins et al., 2017; Kim & Mnih, 2018; Chen et al., 2018). The variational autoencoder
(VAE) (Kingma & Welling, 2014; Rezende et al., 2014), which is equipped with both a generative
and inference model, can be used not only for data generation but also for analysis and transformation. As an explicit model of a probability density function (pdf), the VAE can also be used as a
learned prior for solving inverse problems such as compressed sensing (Bora et al., 2017), speech
enhancement (Bando et al., 2018; Leglaive et al., 2018), or source separation (Kameoka et al., 2019;
Jayaram & Thickstun, 2020). Making sense of the latent representation learned by a VAE and controlling the underlying continuous factors of variation in the data are important challenges to build
more expressive and interpretable generative models and probabilistic priors.

Previous works on representation learning with deep generative models, in particular VAEs, have
mostly focused on images (Higgins et al., 2017; Kim & Mnih, 2018; Chen et al., 2018; Locatello
et al., 2019; 2020). Yet, it is not always easy to define the ground-truth latent factors of variation
involved in the generation of natural images. For speech data, the latent factors of variation can
be directly related to the anatomical mechanisms of speech production. This makes speech data
interesting for investigating the disentangled representation learning capabilities of VAEs, complementary to studies dealing with images. A key concept for characterizing the structure of speech
signals is deduced from the source-filter model proposed by Fant (1970). This model, described in


-----

more detail in Section 2.2, implies that a speech signal is mainly characterized by a few continuous
latent factors of variation corresponding to the vibration of the vocal folds (i.e., the source), which
defines the fundamental frequency, and the resonances of the vocal tract (i.e., the filter), which define the formants. The source-filter model is at the core of various fundamental speech processing
techniques such as cepstral representations and linear predictive coding (LPC) (Rabiner & Schafer,
2010). Valin & Skoglund (2019); Wang et al. (2019) and Juvela et al. (2019) have recently shown
that the efficiency of neural speech vocoders can be largely improved by leveraging the sourcefilter model. Other works investigating the interaction between the source-filter model and neural
networks include Lee et al. (2019) and Choi et al. (2021). All these studies illustrate the interest
of combining deep learning techniques with more traditional signal processing models and algorithms. In this work, we interpret and control the latent space of a VAE from the perspective of the
source-filter model of speech production, which can be beneficial for various applications in speech
analysis, transformation, and synthesis.

We first train a VAE on a dataset of about 25 hours of unlabeled speech signals. Then, using only a
few seconds of labeled speech signals generated with an artificial speech synthesizer, we propose a
method to analyze and control the fundamental frequency and the formant frequencies in the latent
representation of the previously trained VAE. Our contributions are the following: (i) We experimentally demonstrate that the fundamental frequency and the frequency of the first three formants
are encoded in orthogonal subspaces of the VAE latent space. This shows that a vanilla VAE trained
in an unsupervised fashion is able to learn a representation that is compliant with the source-filter
model of speech production. (ii) We develop a weakly-supervised method to precisely and independently control the source-filter continuous latent factors of speech variation within the learned
subspaces. We put in evidence the orthogonality of these subspaces, which allows us to perform
speech transformations in a disentangled manner (i.e., modifying one of the factors does not affect
the others). (iii) Without requiring additional information such as text or human-labeled data, we
propose a deep generative model of speech spectrograms conditioned on the fundamental frequency
and formant frequencies. To the best of our knowledge, this is the first study showing the link between the classical source-filter model of speech production and the representation learned in the
latent space of a VAE. Thanks to this link, we propose a principled method to generate speech data
controlled with interpretable trajectories (of e.g., fundamental frequency and formant frequencies).

2 BACKGROUND

2.1 VARIATIONAL AUTOENCODER

Generative modeling consists in learning a probabilistic model of an observable random variable
identically distributed (i.i.d.) observations ofx ∈X ⊂ R[D]. Let D = {x1, ..., xN ∈X} x. The empirical distribution of be a dataset of N = #D independent and x is defined by
_pˆ(x) =_ _N1_ **xn** _[δ][(][x][ −]_ **[x][n][)][, where][ δ][ is the Dirac delta function, which is null everywhere ex-]**

_∈D_
cept in 0 where it takes the value 1.

P

The variational autoencoder (VAE) (Kingma & Welling, 2014; Rezende et al., 2014) attempts to
approximate ˆp(x) with a pdf pθ(x) parametrized by θ. High-dimensional data such as natural images
or speech signals exhibit some form of regularity which prevents the D dimensions of x from varying
independently from each other. We can thus assume that there exists a latent variable z ∈ R[L], with
_L ≪_ _D, from which the observed data were generated. Accordingly, the model distribution in_
the VAE is defined by marginalizing the joint distribution of the latent and observed data, pθ(x) =
_pθ(x_ **z)p(z)dz.**
_|_

In this work, the observed data vectorR **x ∈** R[D]+ [denotes the power spectrum of a short frame of]
speech signal (i.e., a column of the short-time Fourier transform (STFT) power spectrogram). Its
entries are non negative and its dimension D equals the number of frequency bins. We use the
Itakura-Saito VAE (IS-VAE) (Bando et al., 2018; Leglaive et al., 2018; Girin et al., 2019) defined by

_D_
_p(z) = N_ (z; 0, I), _pθ(x|z) =_ _d=1_ [Exp] [x]d; [vθ(z)][−]d [1] _,_ (1)

where N and Exp denote the densities of the multivariate Gaussian and univariate exponential distri-Y  
butions, respectively, and [v]d denotes the d-th entry of v. The inverse scale parameters of pθ(x|z)
are provided by a neural network called the decoder, parametrized by θ and taking z as input.


-----

The marginal likelihood pθ(x) and the posterior distribution pθ(z **x) are intractable due to the non**
_|_
linearities of the decoder, so it is necessary to introduce an inference model qφ(z **x)** _pθ(z_ **x),**
_|_ _≈_ _|_
which in the VAE is usually defined by
_qφ(z_ **x) =** (z; µφ(x), diag **vφ(x)** ), (2)
_|_ _N_ _{_ _}_
where the mean and variance parameters are provided by a neural network called the encoder
network, parametrized by φ and taking x as input. Then, the VAE training consists in maximizing a lower-bound of ln pθ(x), called the evidence lower-bound (ELBO) and defined by
(θ, φ) = Epˆ(x) Eqφ(z **x) [pθ(x** **z)]** _DKL (qφ(z_ **x)** _p(z))_ . During training, the generative and
_L_ _|_ _|_ _−_ _|_ _∥_
inference model parameters θ and φ are jointly estimated by maximizing the ELBO, using (vari 
ants of) stochastic gradient descent with the so-called reparameterization trick (Kingma & Welling,
2014; Rezende et al., 2014).

2.2 SOURCE-FILTER MODEL OF SPEECH PRODUCTION

The source-filter model of speech production (Fant, 1970) is at the basis of many speech processing
systems. It considers that the production of speech results from the interaction of a source signal
with a linear filter. In voiced speech, the source originates from the vibration of the vocal folds,
which produces a quasi-periodic glottal sound wave whose fundamental frequency defines the pitch.
In unvoiced speech, a noise source is produced by a turbulent airflow or an acoustic impulse. The
source signal is modified by the vocal tract, which is assumed to act as a linear filter. The cavities of
the vocal tract give rise to resonances, which are called the formants and are characterized by their
frequency, amplitude and bandwidth. By moving the speech articulators such as the tongue, lips,
and jaw, humans modify the shape of their vocal tract, which results in a change of the acoustic filter
and the associated resonances. This is how the different elementary speech sounds called phonemes
are produced to form syllables, words and sentences.

The power spectra and the spectral envelopes of two French vowels are displayed in Figure 1. The
spectral envelopes show that the formant frequencies are different for the two vowels. In this example however, the harmonic structure of the spectra shows that the fundamental frequency is the
same for the two vowels. Formant frequencies are important distinctive features of vowels. In a
first approximation, they can be related to the opening of the mouth, the front/rear position of the
tongue, and the rounding of the lips for the first, second, and third formant respectively. For voiced
phonemes, humans are able to control the formants independently of the pitch (i.e., to change the
filter independently of the source (Fant, 1970)) and of each other (MacDonald et al., 2011). The independence of the source and filter characteristics makes the speech signals an interesting material
for representation learning methods, especially with deep generative latent-variable models.

In the present study, in addition to the pre-trained IS-VAE speech spectrogram model, we also
assume the availability of an artificial speech synthesizer allowing for an accurate and independent
control of the fundamental frequency and formants. In this work, we use Soundgen (Anikin, 2019),
a parametric synthesizer based on the source-filter model of speech production. For a given speech
sound, the voiced component of the source signal is generated by a sum of sine waves, the noise
component by a filtered white noise, and both components are then summed and passed through a
linear filter simulating the effect of the human vocal tract. Importantly, this synthesizer allows us to
easily generate artificial speech data labeled with the fundamental frequency and formant frequency
values.

3 ANALYZING AND CONTROLLING SOURCE-FILTER FACTORS OF SPEECH
VARIATION IN THE VAE

In this section, from a VAE trained on natural speech signals and a few artificially-generated labeled
speech signals, we propose (i) a method to learn latent subspaces encoding source-filter factors of
speech variation, (ii) a simple strategy to measure the disentanglement of the learned representation,
and (iii) a weakly-supervised approach to control the continuous factors of variation in the learned
subspaces and generate corresponding speech signals.

Let fi denote the speech factor of variation (in Hz) corresponding to the fundamental frequency, for
_i = 0, and to the formant frequencies, for i ∈{1, 2, ...}. Let Di denote a dataset of artificially-_
generated speech vectors (more precisely short-term power spectra) synthesized by varying only fi,


-----

/a/ /i/


frequency (Hz)


frequency (Hz)


Figure 1: Power spectrum (solid black line) and spectral envelop (orange dashed line) for two vowels
uttered by a male speaker.

all other factors {fj, j ̸= i} being arbitrarily fixed. All examples in Di are labeled with the factors of
variation. It would be very difficult to build such a dataset from existing corpora of unlabeled natural
speech. In contrast, it is an easy task using an artificial speech synthesizer such as Soundgen, which
precisely takes the fundamental frequency and formant parameters as input, and outputs waveforms
from which we extract power spectra.


3.1 LEARNING LATENT SUBSPACES ENCODING SOURCE-FILTER FACTORS OF VARIATION


Let ˆp[(][i][)](x) denote the empirical distribution associated with _i, defined similarly as ˆp(x). We also_
_D_
introduce the following marginal distribution over the latent vectors:

1
_qˆφ[(][i][)][(][z][) =]_ _qφ(z_ **x)ˆp[(][i][)](x)dx =** _qφ(z_ **xn).** (3)
_|_ # _i_ _|_
Z _D_ **xnX∈Di**

In the literature, this quantity is referred to as the aggregated posterior (Makhzani et al., 2016).
However, qφ(z **x) is usually aggregated over the empirical distribution ˆp(x) such that the aggregated**
_|_
posterior is expected to match with the prior p(z) (Chen et al., 2018; Dai & Wipf, 2018). In contrast,
in Equation (3) we aggregate over the “biased” data distribution ˆp[(][i][)](x), where we know only one
latent factor varies. This defines the explicit inductive bias (Locatello et al., 2019) that we exploit to
learn the latent source-filter representation of speech in the VAE.

In the following of the paper, without loss of generality, we assume that, for each data vector in _i,_
_D_
the associated latent vector z has been centered by subtracting the mean vector

1
**_µφ(Di) = Eqˆφ[(][i][)][(][z][)][[][z][] =]_** # _i_ **_µφ(xn)._** (4)

_D_ **xnX∈Di**

Because only one factor varies in _i, we expect latent vectors drawn from the “biased” aggregated_
_D_
posterior in Equation (3) to live on a lower-dimensional manifold embedded in the original latent
space R[L]. We assume this manifold to be a subspace characterized by its semi-orthogonal basis
matrixproblem: Ui ∈ R[L][×][M][i], 1 ≤ _Mi < L. This matrix is computed by solving the following optimization_
**U∈minR[L][×][Mi]** [E]q[ˆ]φ[(][i][)][(][z][)] **z −** **UU[⊤]z** 2 _,_ _s.t. U[⊤]U = I._ (5)

The space spanned by the columns of Uhi is a subspace of the original latent space[2]i R[L] in which the
latent vectors associated with the variation of the factor fi in Di are expected to live. In Appendix A,
we show that, similarly to principal component analysis (PCA) (Pearson, 1901), the solution to the
optimization problem (5) is given by the Mi eigenvectors corresponding to the Mi largest eigenvalues of

1
**Sφ(** _i) =_ **_µφ(xn)µφ(xn)[⊤]_** + diag **vφ(xn)** **_µφ(_** _i)µφ(_ _i)[⊤]._ (6)
_D_ # _i_ _{_ _}_ _−_ _D_ _D_

_D_ **xnX∈Di**  

The dimension Mi of the subspace can be chosen such as to retain a certain percentage of the data
variance in the latent space. Note that the only source of supervision used here is the knowledge that
only the factor fi varies in the dataset Di.


_qˆφ[(][i][)][(][z][) =]_ _qφ(z|x)ˆp[(][i][)](x)dx =_
Z


**_µφ(Di) = Eqˆφ[(][i][)][(][z][)][[][z][] =]_**


**Sφ(** _i) =_
_D_


3.2 DISENTANGLEMENT ANALYSIS OF THE LATENT REPRESENTATION

As defined by Higgins et al. (2018), a representation is disentangled if it is possible to learn orthogonal latent subspaces associated with each factor of variation, whether they are single- or multidimensional. The approach presented in the previous subsection exactly follows this definition and


-----

offers a natural and straightforward way to objectively measure if the unsupervised VAE managed
to learn a disentangled representation of the factors of variation under consideration. First, by simply looking at the eigenvalues associated with the columns ofamount of variance that is retained by the projection UiU[⊤]i [. If a small number of components] Ui ∈ R[L][×][M][i], we can measure the[ M][i]
represents most of the variance, it indicates that only a few intrinsic dimensions of the latent space
are dedicated to the factor of variation fi and varying this factor can be done by affine transformations. Second, if for two different factors of variation fi and fj, with i ̸= j, the columns of Ui are
orthogonal to those of Uj, this indicates that the two factors are encoded in orthogonal subspaces
and therefore disentangled. It should however be verified experimentally that applying transformations by moving onto the subspace associated with fi generalizes to values of {fj, j ̸= i} different
than the ones used in _i._
_D_

3.3 CONTROLLING THE FACTORS OF VARIATION FOR SPEECH TRANSFORMATION AND
GENERATION

So far, for each factorthat encodes its variations in the dataset fi, we have defined a methodology to learn a latent subspacei, containing a few examples of speech data generated Ui ∈ R[L][×][M][i]
_D_
by an artificial synthesizer. Making now use of the labels in Di, we learn a regression model gηi :
Rthe latent subspace defined by+ 7→ R[M][i] from the factor f Ui, whose value is denoted byi. The parameters ηi are defined as the solution of the following y ∈ R+, to the data coordinates in
optimization problem:

_c_ 1

minη Eqˆφ[(][i][)][(][z][,y][)] **gη(y) −** **U[⊤]i** **[z]** 2 = # _i_ **gη(yn) −** **U[⊤]i** **_µφ(xn) −_** **_µφ(Di)_** 2 _,_

n h i _D_ (xn,yXn)∈Di    o

[2] [2] (7)

where ˆqφ[(][i][)][(][z][, y][) =] _qφ(z|x)ˆp[(][i][)](x, y)dx, ˆp[(][i][)](x, y) is the empirical distribution associated withc_

_i, considering now both the speech data vector x and the label y, and_ = denotes equality up to

R

_D_
an additive constant w.r.t. η. The dataset Di is very small with only a few hundreds examples,
and as it is synthetic and labels are not provided by human annotators, the problem can be considered very weakly supervised. For simplicity and because it revealed efficient for this task, gηi is
chosen as a piece-wise linear regression model learned independently for each output coordinate
_m ∈{1, ..., Mi}. This choice is supported by the fact that the semi-orthogonal matrix Ui decorre-_
lates the data (Bengio et al., 2013). Solving the optimization problem (7) then basically consists in
solving a linear system of equations (Jekel & Venter, 2019).

We can now transform a speech spectrogram by analyzing it with the VAE encoder, then linearly
moving in the learned subspaces using the above regression model, and finally resynthesizing it with
the VAE decoder. Given a source latent vector z and a target value y for the factor fi, we apply the
following affine transformation:

**z˜ = z** **UiU[⊤]i** **[z][ +][ U][i][g][η]i** [(][y][)][.] (8)
_−_

This transformation consists in (i) subtracting the projection of z onto the subspace associated with
the factor of variation fi; and (ii) adding the target component provided by the regression model
**gηi mapped from the learned subspace to the original latent space by the matrix Ui. This operation**
allows us to move only in the latent subspace associated with the factor fi. If this subspace is
orthogonal to the latent subspaces associated with the other factors _fj, j_ = i, the latter should
_{_ _̸_ _}_
remain the same between z and ˜z, only fi should be modified. This process can be straightforwardly
generalized to multiple factors, by subtracting and adding terms corresponding to each one of them.

Finally, as the prior p(z) and inference model qφ(z **x) are Gaussian (see Equations (1) and (2)), the**
_|_
transformation in Equation (8) has the following probabilistic formulation (using U[⊤]i **[U][i][ =][ I][):]**

_p(˜z; fi = y) =_ **z˜; Uigηi** (y), I **UiU[⊤]i** (9)
_N_ _−_
 

_qφ(˜z|x; fi = y) = N_ **z˜; Uigηi** (y) + (I − **UiU[⊤]i** [)][µ][φ][(][x][)][,][ (][I][ −] **[U][i][U]i[⊤][) diag][{][v][φ][(][x][)][}]** _._ (10)

The prior in Equation (9) is now conditioned on the factor _fi and can be used to generate speech data_
given input trajectories of fundamental frequency and formant frequencies. As we assumed centered
latent data, the mean vector µφ( _i) defined in Equation (4) must be added to ˜z before mapping this_
_D_
vector through the generative model pθ(x **z).**
_|_


-----

(a) Fundamental frequency and formant transformations of the vowel /a/ uttered by a female speaker.


(b) Spectrogram generated from input trajectories of
the fundamental frequency and formant frequencies.


(c) Left: original spectrogram of a speech signal uttered by a female speaker; Middle: transformed spectrogram
where the fundamental frequency is set constant over time; Right: transformed spectrogram where the original
voiced speech signal (left) is converted into a whispered speech signal (i.e., the pitch is removed).

Figure 2: Qualitative example of modified and generated spectrograms with the proposed method.
The color bar indicates the power in dB.

4 EXPERIMENTS

This section presents qualitative and quantitative experimental results of the proposed method for
controlling the fundamental frequency and formant frequencies of speech signals with a VAE. The
VAE is trained on about 25 hours of multi-speaker speech data from the Wall Street Journal (WSJ0)
dataset (Garofolo et al., 1993a). The data space dimension is 513 and the latent space dimension is
16. For a given factor of variation, the corresponding latent subspace is learned (see Section 3.1)
using short trajectories of speech power spectra (corresponding to a few seconds of speech) generated with Soundgen (Anikin, 2019), all other factors being arbitrarily fixed. When solving the
optimization problem (5), the latent subspace dimension Mi of each factor of variation is chosen
such that 80% of the data variance is retained. This leads M0 = 4, M1 = 1 and M2 = M3 = 3. The
regression models used to control the speech factors of variation in the latent space (see Section 3.3)
are learned on the same trajectories, but using the labels that correspond to the input control parameters of Soundgen (i.e., fundamental frequency and formant frequencies values). More details on the
experimental set-up can be found in Appendix B. Given a generated or transformed spectrogram,
we use Waveglow (Prenger et al., 2019) to reconstruct the time-domain signal.

4.1 QUALITATIVE RESULTS

In Figure 2a, we illustrate the ability of the proposed method to modify the fundamental frequency
and formant frequencies in an accurate and independent manner. The spectrogram contains five
segments of equal length. The first segment corresponds to the original spectrogram of the steady
vowel /a/ uttered by a female speaker. In the following segments, we vary successively each individual factor fi, for i = 0 to 3, as indicated by the black lines in the figure. Variations of f0 modify
the harmonic structure of the signal while keeping the formant structure unaltered. Variations of fi,
_i ∈{1, 2, 3}, modify the formant frequencies (i.e., the vocal tract resonances, as indicated by the_
color map) while keeping the fundamental frequency unaltered. Figure 2b represents a spectrogram
generated by using the conditional prior in equation (9) (generalized to conditioning on multiple factors). We can see that the characteristics of the generated speech spectrogram match well with the
input trajectories represented by the lines in the figure. In Figure 2c, from left to right we show the
original spectrogram of a speech signal uttered by a female speaker (left), and the transformed spectrograms where the fundamental frequency is set constant over time (middle) and where the pitch
has been removed (i.e., the original voiced speech signal is converted into a whispered speech signal)
(right). This last spectrogram is simply obtained by subtracting to z its projection onto the latent


-----

Table 1: Performance for pitch (f0) and formant (f1, f2)
transformations of English vowels.

**Factor** **Method** **NISQA (↑)** **MOSnet** (↑) _δf0 (%, ↓)_ _δf1 (%, ↓)_ _δf2 (%, ↓)_

_f0_ TD-PSOLA 2.32 ± 0.55 2.60 ± 0.36 3.8 ± 2.5 6.3 ± 2.8 3.7 ± 0.9
WORLD 2.49 ± 0.60 2.46 ± 0.46 4.5 ± 0.6 3.7 ± 1.8 2.3 ± 0.7
VAE baseline 1.94 ± 0.43 2.44 ± 0.28 6.2 ±2.8 10.4 ± 2.4 6.2 ± 0.9
Proposed 2.08 ± 0.48 2.55 ± 0.40 0.8 ± 0.2 7.2 ± 1.3 3.6 ± 1.2

_f1_ VAE baseline 1.84 ± 0.50 2.40 ± 0.10 11.3 ± 4.2 15.1 ± 3.5 6.0 ± 1.2
Proposed 1.85 ± 0.40 2.45 ± 0.12 6.0 ± 1.6 8.4 ± 3.2 5.7 ± 0.4

_f2_ VAE baseline 2.01 ± 0.40 2.40 ± 0.08 19.5 ± 3.2 10.7 ± 0.5 10.9 ± 1.9
Proposed 2.03 ± 0.43 2.42 ± 0.41 8.5 ± 1.1 8.7 ± 1.1 6.2 ± 1.5


Figure 3: Correlation matrix of the
learned latent subspaces basis vectors.


subspace corresponding to f0 (i.e., by considering only the two first terms in the right-hand side of
Equation (8)). It results in a spectrogram where the harmonic component is neutralized, while preserving the original formant structure. This is remarkable considering that the VAE was not trained
on whispered speech signals, and it further confirms that the proposed method dissociates the source
and the filter contributions in the VAE latent space. Other qualitative results can be found in Appendix C (visualization of trajectories in the learned latent subspaces and additional examples of
[generated and transformed speech spectrograms) and at https://tinyurl.com/iclr2022](https://tinyurl.com/iclr2022)
(including audio examples).

4.2 QUANTITATIVE RESULTS

Due to space limitations, we only provide in this section quantitative results related to the fundamental frequency (f0) and the first two formant frequencies (f1 and f2). Additional results can be
found in Appendix D, including the performance for modifications of the third formant frequency
(f3), the performance with a VAE trained on other datasets (e.g., a dataset of French speech signals),
and the performance on the TIMIT dataset (Garofolo et al., 1993b), which is richer than the English
vowel one in terms of phonemes but is not labeled with the fundamental and formant frequencies,
making the evaluation less reliable.

4.2.1 ORTHOGONALITY OF THE LATENT SUBSPACES

Following the discussion in Section 3.2, we compute the dot product between all pairs of unit vectors
in the matricesis mainly diagonal. Except for a correlation value of Ui ∈ R[L][×][M][i], i ∈{0, 1, 2}. Figure 3 shows that the resulting correlation matrix −0.23 across f1 and the first component of
_f2, all other values are below 0.13 (in absolute value), confirming the orthogonality of the learned_
subspaces and thus the disentanglement of the learned source-filter representation of speech.

4.2.2 PITCH AND FORMANT TRANSFORMATIONS OF ENGLISH VOWELS

**Experimental set-up** In this experiment, we quantitatively evaluate the performance of the proposed method regarding the modification of the fundamental frequency and formant frequencies in
speech signals (see Section 3.3). We use a corpus of 12 English vowels uttered by 50 male and 50
female speakers (Hillenbrand et al., 1995). This dataset is labeled with the fundamental and formant
frequencies. We transform each signal in this dataset by varying one single factor fi at a time, for
_i ∈{0, 1, 2}. The ranges of transformation for the fundamental frequency, first formant frequency,_
and second formant frequency are respectively [100, 300] Hz, [300, 900] Hz, and [1100, 2700] Hz,
with a step of 1 Hz, 10 Hz and 20 Hz respectively. For the modification of each factor fi, we measure the performance regarding three aspects: First, in terms of accuracy by comparing the target
value for the factor (see Equation (8)) and its estimation computed from the modified output speech
signal. Second, in terms of disentanglement, by comparing the values of fj for j ̸= i, before and
after modification of the factor fi. Third, in terms of speech naturalness of the transformed signal.

**Metrics** Accuracy and disentanglement are measured in terms of relative absolute error (in percent, the lower the better). For a given factor fi, it is defined by δfi = 100% × |yˆ − _y|/y where y is_


-----

the target value of fi and ˆy its estimation from the output transformed signal. Let us take the example
of a modification of f0: δf0 measures the accuracy of the transformation on f0 while δf1 and δf2
are used to assess if the other factors of variation f1 and f2 remained unchanged after modifying f0.
We use CREPE (Kim et al., 2018) to estimate the fundamental frequency and Parselmouth (Jadoul
et al., 2018), which is based on PRAAT, to estimate the formant frequencies. Regarding speech naturalness, we use the objective measure provided by NISQA (Mittag & M¨oller, 2020). This metric
(the higher the better) was developed in the context of speech transformation algorithms and it was
shown to highly correlate with subjective mean opinion scores (MOS) (i.e., human ratings). As a
reference, the score provided by NISQA on the original dataset of English vowels (i.e., without any
processing) is equal to 2.60 ± 0.53.

**Methods** We compare the proposed approach with several methods from the literature: (i) Timedomain pitch-synchronous overlap-and-add (TD-PSOLA) (Moulines & Charpentier, 1990) performs fundamental frequency modifications through the decomposition of the signal into pitchsynchronized overlapping frames. (ii) WORLD (Morise et al., 2016) is a vocoder also used for
fundamental-frequency modifications. It decomposes the speech signal into three components characterizing the fundamental frequency, the aperiodicity, and the spectral envelope. (iii) The method
proposed by Hsu et al. (2017b) (here referred to as “VAE baseline”) consists in applying translations
directly in the latent space of the VAE. Unlike the proposed approach, this method requires predefined latent attribute representations µsrc and µtrgt associated with the source and target values of the
factor to be modified, respectively. In particular, computing µsrc requires analyzing the input speech
signal, for instance to estimate the fundamental frequency, which is not the case for the proposed
method. The source and target latent attribute representations are then used to perform the translation ˜z = z − **_µsrc + µtrgt, where z and ˜z are respectively the original and modified latent vectors. To_**
ensure fair comparison, we build dictionaries of predefined latent attribute representations using the
same artificially-generated speech data that were used in the weakly-supervised training stage of the
proposed method. All the methods we compare with require a pre-processing of the input speech
signal to compute the input trajectory of the factor to be modified (e.g. the fundamental frequency),
which is not the case of the proposed method.

**Discussion** Experimental results (mean and standard deviation) are shown in Table 1. Compared
to the VAE baseline, the proposed method obtains better performance in terms of accuracy, disentanglement, and naturalness. These results confirm the effectiveness of performing the transformations
in the learned latent subspaces and not directly in the latent space, as well as the advantage of
using regression models instead of predefined latent attribute representations. Regarding f0 transformation, WORLD obtains the best performance in terms of disentanglement, which is because
the source and filter contributions are decoupled in the architecture of the vocoder. In terms of naturalness, WORLD and then TD-PSOLA obtain the best performance. This may be explained by
the fact that these method operate directly in the time domain, therefore they do not suffer from
phase reconstruction artifacts, unlike the proposed and VAE baseline methods. Naturalness is indeed greatly affected by phase reconstruction artificats, even from an unaltered speech spectrogram
(i.e., without transformation). Phase reconstruction in a multi-speaker setting is still an open problem in speech processing. We want to emphasize that the objective of this study is not to compete
with traditional signal processing methods such as TD-PSOLA and WORLD. It is rather to advance
on the understanding of deep generative modeling of speech signals and to compare honestly with
highly-specialized traditional systems. TD-PSOLA and WORLD exploit signal models specifically
designed for the task at hand, which for instance prevents them to be used for modifying formant
frequencies. In contrast, the proposed method is fully based on learning and the same methodology
applies for modifying the fundamental or formant frequencies.

5 RELATED WORK

Variants of the VAE have recently led to considerable progress in disentangled representation learning (Kim & Mnih, 2018; Higgins et al., 2017; Chen et al., 2018). From experimental analyses on
image data, these methods suggest that a vanilla VAE cannot learn a disentangled representation. In
the present study, we experimentally show that learning a disentangled source-filter representation
of speech using a simple VAE is possible, complying with the definition of disentanglement proposed in (Higgins et al., 2018). Locatello et al. (2019; 2020) recently showed both theoretically and
from a large-scale experimental study that the unsupervised learning of disentangled representations


-----

is impossible without inductive biases on both the models and the data. In the present study, we precisely employ a few examples of artificially-generated labeled speech data in order to disentangle
the latent representation of a simple VAE, in terms of source-filter factors of speech variation.

Modifying speech signals characteristics such as the pitch or timbre is a highly covered research
topic where deep generative modeling has recently emerged as a promising approach. Voice conversion consists in modifying speaker characteristics while keeping the linguistic information unchanged, e.g. (Kaneko & Kameoka, 2018; Kaneko et al., 2019; Kameoka et al., 2020). Many voice
conversion methods use text information as input (Wang et al., 2018; Zhang et al., 2019; Kulkarni
et al., 2021; Li et al., 2021), which greatly helps for transferring non-verbal vocal attributes from
one speech signal to another, but it requires datasets labeled with text. Qian et al. (2020) and Webber
et al. (2020) recently proposed speech transformation methods based on latent representation learning without resorting to textual information. The interpretability of the learned representations is
enforced by the design of the model. Qian et al. (2020) proposed to use three independent encoder
networks to decompose a speech signal into fundamental frequency, timbre and rhythm latent representations. Webber et al. (2020) focused on controlling source-filter parameters in speech signals,
where the ability to control a given parameter (e.g. fundamental frequency) is enforced explicitly
using labeled data and adversarial learning. In this approach, each parameter to be controlled requires a dedicated training of the model. Moreover, these methods are speaker-dependent, as speech
generation in Qian et al. (2020) is conditioned on the speaker identity and Webber et al. (2020)
used a single-speaker training dataset. This contrasts with the proposed method which is speakerindependent, and in which the source-filter representation of speech naturally emerges in the latent
space of a single unsupervised VAE.

Hsu et al. (2017a;b) proposed to use VAEs for modifying the speaker identity and the phonemic
content of speech signals. In (Hsu et al., 2017b), this is achieved by translations in the latent space.
This is the baseline VAE approach we presented and compared with in Section 4. It requires to know
predefined values of the latent representations associated with the source/target speech attributes to
be modified. Conversely, Equation (8) in the proposed method does not require the knowledge of
the factor fi associated with source vector z, it only requires the value associated with the target
vector ˜z. Moreover, Hsu et al. (2017b) did not address the control of continuous factors of speech
variation in the VAE latent space, contrary to the present work.

Several methods have been recently proposed to control continuous factors of variation in deep generative models (Jahanian et al., 2019; Plumerault et al., 2020; Goetschalckx et al., 2019; H¨ark¨onen
et al., 2020), focusing essentially on generative adversarial networks. They consist in identifying
and then moving onto semantically meaningful directions in the latent space of the model. The
present work is inspired by (Plumerault et al., 2020), which assumes that a factor of variation can
be predicted from the projection of the latent vector along a specific axis, learned from artificially
generated trajectories. The proposed method is however more generic, thanks to the learning of
latent subspaces associated to the latent factors and to the introduction of a general formalism based
on the use of “biased” aggregated posteriors. Moreover, these previous works on controlling deep
generative models only allow for moving “blindly” onto semantically meaningful directions in the
latent space. In the present study, we are able to generate data conditioned on a specific target value
for a given factor of variation. Finally, previous work focused on image data, and to the best of our
knowledge, this is the first approach to control source-filter factors of speech variation in a VAE.

6 CONCLUSION

In this work, using only a few seconds of artificially generated labeled speech data, we showed
that the fundamental frequency and formant frequencies are encoded in orthogonal latent subspaces
of an unsupervised VAE and we proposed a weakly-supervised method to control these attributes
within the learned subspaces. The method generalizes well when applied to natural speech signals.
To the best of our knowledge, this is the first approach that, with a single methodology, is able to
extract, identify and control the source and filter low-level speech attributes within a VAE latent
space. The proposed method could also be applied to other types of data such as natural or face
images, provided that one can create a few synthetic images that capture variations in a single latent
factor of interest, independently of others. For example, this would be possible for factors encoding
the position, rotation and scale of objects in natural images, or some facial expressions.


-----

7 REPRODUCIBILITY STATEMENT

To ensure reproducibility, the implementation of the method and the data generated with soundgen
that are used to learn the latent subspaces and regression models will be made freely available online.

REFERENCES

Andrey Anikin. Soundgen: An open-source tool for synthesizing nonverbal vocalizations. Behavior
_Research Methods, 51(2):778–792, 2019._

Yoshiaki Bando, Masato Mimura, Katsutoshi Itoyama, Kazuyoshi Yoshii, and Tatsuya Kawahara.
Statistical speech enhancement based on probabilistic integration of variational autoencoder and
non-negative matrix factorization. In IEEE International Conference on Acoustics, Speech and
_Signal Processing (ICASSP), pp. 716–720, 2018._

Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798–1828,
2013.

Christopher M. Bishop. Pattern recognition and machine learning. Information science and statistics. Springer, 2006. ISBN 9780387310732.

Ashish Bora, Ajil Jalal, Eric Price, and Alexandros G Dimakis. Compressed sensing using generative models. In International Conference on Machine Learning (ICML), pp. 537–546, 2017.

Ricky T. Q. Chen, Xuechen Li, Roger Grosse, and David Duvenaud. Isolating sources of disentanglement in variational autoencoders. In Advances in Neural Information Processing Systems
_(NeurIPS), 2018._

Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
_Advances in Neural Information Processing Systems (NeurIPS), 2016._

Hyeong-Seok Choi, Juheon Lee, Wansoo Kim, Jie Hwan Lee, Hoon Heo, and Kyogu Lee. Neural analysis and synthesis: Reconstructing speech from self-supervised representations. arXiv
_preprint arXiv:2110.14513, 2021._

Bin Dai and David Wipf. Diagnosing and enhancing VAE models. In International Conference on
_Learning Representations (ICLR), 2018._

Kate Dupuis and M Kathleen Pichora-Fuller. Toronto emotional speech set (tess)-younger
talker angry. 2010.

Gunnar Fant. Acoustic theory of speech production. Number 2. Walter de Gruyter, 1970.

John S. Garofolo, David Graff, Doug Paul, and David Pallett. CSR-I (WSJ0) Sennheiser
[LDC93S6B. https://catalog.ldc.upenn.edu/LDC93S6B, 1993a. Philadelphia: Lin-](https://catalog.ldc.upenn.edu/LDC93S6B)
guistic Data Consortium.

John S. Garofolo, Lori F. Lamel, William M. Fisher, Jonathan G. Fiscus, David S. Pallett, Nancy L.
Dahlgren, and Victor Zue. TIMIT acoustic phonetic continuous speech corpus. In Linguistic data
_consortium, 1993b._

Laurent Girin, Fanny Roche, Thomas Hueber, and Simon Leglaive. Notes on the use of variational
autoencoders for speech and audio spectrogram modeling. In International Conference on Digital
_Audio Effects (DAFx), pp. 1–8, 2019._

Lore Goetschalckx, Alex Andonian, Aude Oliva, and Phillip Isola. GANalyze: Toward visual definitions of cognitive image properties. In Proceedings of the IEEE/CVF International Conference
_on Computer Vision, pp. 5744–5753, 2019._

Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Infor_mation Processing Systems (NeurIPS), 2014._


-----

Erik H¨ark¨onen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. GANSpace: Discovering
interpretable gan controls. In Advances in Neural Information Processing Systems (NeurIPS), pp.
9841–9850, 2020.

Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. Beta-vae: Learning basic visual concepts with a
constrained variational framework. In International Conference on Learning Representations
_(ICLR), 2017._

Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo J. Rezende,
and Alexander Lerchner. Towards a definition of disentangled representations. arXiv preprint
_arXiv:1812.02230, 2018._

James Hillenbrand, Laura A. Getty, Michael J. Clark, and Kimberlee Wheeler. Acoustic characteristics of American English vowels. The Journal of the Acoustical society of America, 97(5):
3099–3111, 1995.

Pierre-Edouard Honnet, Alexandros Lazaridis, Philip N Garner, and Junichi Yamagishi. The siwis
french speech synthesis database? design and recording of a high quality french database for
speech synthesis. Technical report, Idiap, 2017.

Wei-Ning Hsu, Yu Zhang, and James Glass. Unsupervised learning of disentangled and interpretable
representations from sequential data. In Advances in Neural Information Processing Systems
_(NeurIPS), 2017a._

Wei-Ning Hsu, Yu Zhang, and James Glass. Learning latent representations for speech generation
and transformation. In Interspeech, pp. 1273–1277, 2017b.

Keith Ito and Linda Johnson. The lj speech dataset. [https://keithito.com/](https://keithito.com/LJ-Speech-Dataset/)
[LJ-Speech-Dataset/, 2017.](https://keithito.com/LJ-Speech-Dataset/)

Yannick Jadoul, Bill Thompson, and Bart de Boer. Introducing Parselmouth: A Python interface to
Praat. Journal of Phonetics, 71:1–15, 2018.

Ali Jahanian, Lucy Chai, and Phillip Isola. On the “steerability” of generative adversarial networks.
In International Conference on Learning Representations (ICLR), 2019.

Vivek Jayaram and John Thickstun. Source separation with deep generative priors. In International
_Conference on Machine Learning (ICML), pp. 4724–4735, 2020._

Charles F. Jekel and Gerhard Venter. Pwlf: a python library for fitting 1D continuous piecewise lin[ear functions. URL: https://github.com/cjekel/piecewise_linear_fit_py/](https://github.com/cjekel/piecewise_linear_fit_py/raw/master/paper/pwlf_Jekel_Venter_v2.pdf)
[raw/master/paper/pwlf_Jekel_Venter_v2.pdf, 2019.](https://github.com/cjekel/piecewise_linear_fit_py/raw/master/paper/pwlf_Jekel_Venter_v2.pdf)

Lauri Juvela, Bajibabu Bollepalli, Vassilis Tsiaras, and Paavo Alku. GlotNet—a raw waveform
model for the glottal excitation in statistical parametric speech synthesis. IEEE/ACM Transactions
_on Audio, Speech, and Language Processing, 27(6):1019–1030, 2019._

Hirokazu Kameoka, Li Li, Shota Inoue, and Shoji Makino. Supervised determined source separation
with multichannel variational autoencoder. Neural computation, 31(9):1891–1914, 2019.

Hirokazu Kameoka, Wen-Chin Huang, Kou Tanaka, Takuhiro Kaneko, Nobukatsu Hojo, and
Tomoki Toda. Many-to-many voice transformer network. IEEE/ACM Transactions on Audio,
_Speech, and Language Processing, 29:656–670, 2020._

Takuhiro Kaneko and Hirokazu Kameoka. Cyclegan-vc: Non-parallel voice conversion using cycleconsistent adversarial networks. In European Signal Processing Conference (EUSIPCO), pp.
2100–2104, 2018.

Takuhiro Kaneko, Hirokazu Kameoka, Kou Tanaka, and Nobukatsu Hojo. Stargan-vc2: Rethinking
conditional methods for stargan-based voice conversion. arXiv preprint arXiv:1907.12279, 2019.

Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In International Conference on
_Machine Learning (ICML), pp. 2649–2658, 2018._


-----

Jong Wook Kim, Justin Salamon, Peter Li, and Juan Pablo Bello. Crepe: A convolutional representation for pitch estimation. In IEEE International Conference on Acoustics, Speech and Signal
_Processing (ICASSP), pp. 161–165, 2018._

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
_Conference on Learning Representations (ICLR), 2015._

Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. In International Confer_ence on Learning Representations (ICLR), 2014._

Ajinkya Kulkarni, Vincent Colotte, and Denis Jouvet. Improving transfer of expressivity for endto-end multispeaker text-to-speech synthesis. In European Signal Processing Conference (EU_SIPCO), 2021._

Juheon Lee, Hyeong-Seok Choi, Chang-Bin Jeon, Junghyun Koo, and Kyogu Lee. Adversarially
trained end-to-end korean singing voice synthesis system. In Interspeech, 2019.

Simon Leglaive, Laurent Girin, and Radu Horaud. A variance modeling framework based on variational autoencoders for speech enhancement. In IEEE International Workshop on Machine Learn_ing for Signal Processing (MLSP), pp. 1–6, 2018._

Tao Li, Shan Yang, Liumeng Xue, and Lei Xie. Controllable emotion transfer for end-to-end speech
synthesis. In International Symposium on Chinese Spoken Language Processing (ISCSLP), pp.
1–5, 2021.

Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard
Sch¨olkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. In International Conference on Machine Learning (ICML),
pp. 4114–4124, 2019.

Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar R¨atsch, Sylvain Gelly, Bernhard
Sch¨olkopf, and Olivier Bachem. A sober look at the unsupervised learning of disentangled representations and their evaluation. Journal of Machine Learning Research, 21:1–62, 2020.

Ewen N. MacDonald, David W. Purcell, and Kevin G. Munhall. Probing the independence of formant control using altered auditory feedback. The Journal of the Acoustical Society of America,
129(2):955–965, 2011.

Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial
autoencoders. In ICLR 2016 Workshop, International Conference on Learning Representations
_(ICLR), 2016._

Gabriel Mittag and Sebastian M¨oller. Deep learning based assessment of synthetic speech naturalness. In Interspeech, 2020.

Masanori Morise, Fumiya Yokomori, and Kenji Ozawa. World: a vocoder-based high-quality speech
synthesis system for real-time applications. IEICE Transactions on Information and Systems, 99
(7):1877–1884, 2016.

Eric Moulines and Francis Charpentier. Pitch-synchronous waveform processing techniques for
text-to-speech synthesis using diphones. Speech communication, 9(5-6):453–467, 1990.

Karl Pearson. On lines and planes of closest fit to systems of points in space. The London, Edin_burgh, and Dublin philosophical magazine and journal of science, 2(11):559–572, 1901._

Antoine Plumerault, Herv´e Le Borgne, and C´eline Hudelot. Controlling generative models with continuous factors of variations. In International Conference on Learning Representations (ICLR),
2020.

Ryan Prenger, Rafael Valle, and Bryan Catanzaro. Waveglow: A flow-based generative network for
speech synthesis. In IEEE International Conference on Acoustics, Speech and Signal Processing
_(ICASSP), pp. 3617–3621, 2019._


-----

Kaizhi Qian, Yang Zhang, Shiyu Chang, Mark Hasegawa-Johnson, and David Cox. Unsupervised
speech decomposition via triple information bottleneck. In International Conference on Machine
_Learning (ICML), pp. 7836–7846, 2020._

Lawrence Rabiner and Ronald Schafer. Theory and applications of digital speech processing. Prentice Hall Press, 2010.

Danilo J. Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. International Conference on Machine Learning
_(ICML), pp. 1278–1286, 2014._

Jean-Marc Valin and Jan Skoglund. Lpcnet: Improving neural speech synthesis through linear prediction. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),
pp. 5891–5895, 2019.

Xin Wang, Shinji Takaki, and Junichi Yamagishi. Neural source-filter waveform models for statistical parametric speech synthesis. IEEE/ACM Transactions on Audio, Speech, and Language
_Processing, 28:402–415, 2019._

Yuxuan Wang, Daisy Stanton, Yu Zhang, RJ-Skerry Ryan, Eric Battenberg, Joel Shor, Ying Xiao,
Ye Jia, Fei Ren, and Rif A Saurous. Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis. In International Conference on Machine Learning (ICML),
pp. 5180–5189, 2018.

Jacob J Webber, Olivier Perrotin, and Simon King. Hider-finder-combiner: An adversarial architecture for general speech signal modification. In Interspeech, pp. 3206–3210, 2020.

Ya-Jie Zhang, Shifeng Pan, Lei He, and Zhen-Hua Ling. Learning latent representations for style
control and transfer in end-to-end speech synthesis. In IEEE International Conference on Acous_tics, Speech and Signal Processing (ICASSP), pp. 6945–6949, 2019._


-----

A SOLUTION TO THE LATENT SUBSPACE LEARNING PROBLEM

In this Appendix, we show that the solution to the optimization problem (5) is given by the principal
eigenvectors of Sφ( _i) in Equation (6)._
_D_

Without loss of generality, we formulate the problem for a centered version of the latent data

**z** **z** **_µφ(_** _i),_ (11)
_←_ _−_ _D_

where µφ( _i) is defined in Equation (4). This centering also affects the inference model originally_
_D_
defined in Equation (2), as follows:

_qφ(z_ **x) =** (z; µφ(x) **_µφ(_** _i), diag_ **vφ(x)) .** (12)
_|_ _N_ _−_ _D_ _{_

Using Equation (3), the fact that U[⊤]U = I, and Equation (12), the cost function in the optimization
problem (5) can be rewritten as follows:

1

Eqˆφ[(][i][)][(][z][)] **z −** **UU[⊤]z** 2 = # _i_ Eqφ(z|xn) **z −** **UU[⊤]z** 2

h i _D_ **xnX∈Di** h i

[2] [2]

1

= tr (I **UU[⊤])** Eqφ(z **xn)[zz[⊤]]**

( _−_ #Di **xnX∈Di** _|_ )

= tr (I **UU[⊤])Sφ(** _i)_ _,_ (13)
_−_ _D_

where Sφ( _i) is defined in Equation (6). From this last equality, we see that the optimization_
_D_
problem (5) is equivalent to

max **U[⊤]Sφ(** _i)U_ _,_ _s.t. U[⊤]U = I._ (14)
**U∈R[L][×][Mi]** [tr] _D_


Very similarly to PCA (Pearson, 1901), the solution is given by the Mi dominant eigenvectors of
**Sφ(Di) (i.e., associated to the Mi largest eigenvalues) (Bishop, 2006, Section 12.1).**

B EXPERIMENTAL SETUP

B.1 VAE TRAINING

To train the IS-VAE model (Bando et al., 2018; Leglaive et al., 2018; Girin et al., 2019), we use
the Wall Street Journal (WSJ0) dataset (Garofolo et al., 1993a) which contains 25 hours of speech
signals sampled at 16 kHz, including 52 female and 49 male speakers. The time-domain spech
signals are converted to power spectrograms using the short-time Fourier transform (STFT) with an
analysis window of length 64 ms and an overlap of 75%. The architecture of the VAE is shown
in Figure 4, the encoder and decoder networks each have three dense layers. A hyperbolic tangent
(tanh) activation function is used at each layer, except for the output layers of the encoder and
decoder where we use the identity function. The VAE input/output dimension is D = 513 (we
only keep the non redundant part of the power spectrum at a given time frame) and the latent vector
dimension is set to L = 16. We train the model using the Adam optimizer (Kingma & Ba, 2015)
with a learning rate equal to 0.001.

B.2 ANALYZING AND CONTROLLING SOURCE-FILTER FACTORS OF SPEECH VARIATION IN
THE VAE

For a given factor of variation, the corresponding latent subspace is learned (see Section 3.1) using
trajectories of speech power spectra generated with Soundgen (Anikin, 2019), all other factors being
arbitrarily fixed. For f0, the trajectory contains 226 points (which corresponds to 3.6 seconds of
speech) evenly spaced in the range [85, 310] Hz, f1, f2 and f3 are fixed to 600 Hz, 1500 Hz, and
3200 Hz, respectively. For f1, the trajectory contains 401 points (which corresponds to 6.4 seconds
of speech) evenly spaced in the range [200, 1000] Hz, f0, f2 and f3 are fixed to 140 Hz, 1600 Hz,
and 3200 Hz, respectively. For f2, the trajectory contains 401 points evenly spaced in the range

[800, 2800] Hz, f0, f1 and f3 are fixed to 140 Hz, 500 Hz, and 3200 Hz, respectively. For f3, the
trajectory contains 241 points (which corresponds to 3.9 seconds of speech) evenly spaced in the


-----

Activation function
**tanh**

A fully connected

FC

linear layer

FC

16

FC FC

256 64

FC

16

Reparametrization
trick

FC FC FC

64 256 513

Figure 4: Architecture of the VAE. The number of neurons for each fully-connected layer is indicated below “FC”, within each gray box. The input/output vector dimension of the VAE is 513 and
the latent vector dimension is 16.

range [2000, 3200] Hz, f0, f1 and f2 are fixed to 140 Hz, 500 Hz, and 1200 Hz, respectively. The
amplitude of the formants is set to 30 dB, and the bandwidth is automatically calculated from the
fromant frequencies by Soundgen, using a formula derived from human phonetic research (Anikin,
2019). The regression models used to control the speech factors of variation in the latent space (see
Sections 3.3) are learned on the same trajectories, but using the labels that correspond to the input
control parameters of Soundgen.

C ADDITIONAL QUALITATIVE RESULTS

C.1 VISUALIZATION OF THE LEARNED LATENT SUBSPACES

For i = 0, 1, 2 and 3, Figures 5a, 5b, 5c, 5d are respectively obtained by projecting the latent mean
vectors µφ(x) ∈ R[L], for all data vectors x ∈Di, within the latent subspace characterized by
**Uthe latent subspace dimensioni ∈** R[L][×][M][i] (i.e., we perform dimensionality reduction). In the previously reported experiments, Mi of each factor of variation was chosen such that 80% of the data
variance was retained in the latent space. It resulted in M0 = 4, M1 = 1 and M2 = M3 = 3. In this
section, for visualization purposes, we set Mi = 3 for all i ∈{0, 1, 2, 3}. However, we can see that
the f1 trajectory (Figure 5b) is mainly concentrated along a single axis. Regarding f0 (Figure 5a),
setting M0 = 3 retained 78% of the variance of D0 in the latent space. An important observation
that we make from these figures is that two data vectors x and x[′], corresponding to values of a given
factor that are close, have projections of µφ(x) and µφ(x[′]) that are also close in the learned latent
subspaces. This can be seen from the color bars which indicate the values of the factors of variation.
It indicates that the learned representation preserves the notion of proximity in terms of fundamental
frequency and formant frequencies.

In Figure 5e, we project three different datasets 1, defined for three different values of f2. Sim_D_
ilarly, in Figure 5f we show the trajectories associated with the projection of three datasets 2,
_D_
defined for three different values of f1. We notice that as expected, the trajectories are almost identical and only differ by a translation.


-----

(a) f0 trajectory corresponding to D0. (b) f1 trajectory corresponding to D1.

(c) f2 trajectory corresponding to D2. (d) f3 trajectory corresponding to D3.


(e) Three trajectories for f1, associated with different values of f2.


(f) Three trajectories for f2, associated with different values of f1.


Figure 5: Visualization of trajectories in the learned latent subspaces.


-----

C.2 EXAMPLES OF GENERATED SPEECH SPECTRA

(a) Generated spectra with 3 different values of f0. (b) Generated spectra with 3 different values of f1.

(c) Generated spectra with 3 different values of f2. (d) Generated spectra with 3 different values of f3.

Figure 6: Power spectra (solid black line) and spectral envelopes (dashed orange line) obtained
using the conditional prior in equation (9) (generalized to conditioning on multiple factors). Each
subfigure contains three plots where we vary the value of one single factor at a time: f0 in (a), f1 in
(b), f2 in (c) and f3 in (d).


-----

C.3 EXAMPLES OF TRANSFORMED SPEECH SPECTROGRAMS

(a) Original spectrogram of the vowel /ae/ uttered by a male speaker.

(b) Transformation of the fundamental frequency f0. (c) Transformation of the 1st formant frequency f1.

(d) Transformation of the 2nd formant frequency f2. (e) Transformation of the 3rd formant frequency f3.

Figure 7: Figure (a) shows the spectrogram of a vowel uttered by a male speaker. Figures (b), (c),
(d) and (e) show transformations of this spectrogram with the proposed method, where we vary f0,
_f1, f2, and f3, respectively. The target value for these factors is indicated by the dashed blue line._


-----

Figure 8: Each line in this figure corresponds to a speech signal uttered by a different speaker. Left:
spectrogram of the original speech signal; Middle: transformed spectrogram where the fundamental frequency is set constant over time; Right: transformed spectrogram where the original voiced
speech signal (left) is converted into a whispered speech signal (i.e., the fundamental frequency is
removed).


-----

D ADDITIONAL QUANTITATIVE RESULTS

D.1 TRANSFORMATIONS OF THE THIRD FORMANT FREQUENCY ON THE ENGLISH VOWELS
DATASET

Table 2 completes Table 1 by including transformations of the third formant frequency in the range

[2200, 3200] Hz, with a step of 20 Hz. The new column δf3 also measures the performance in terms
of disentanglement when modifying other factors fi = f3. These additional results do not modify
the conclusions drawn in Section 4. _̸_

Table 2: Performance (mean and std) for the fundamental frequency (f0) and formant frequencies
(f1, f2 and f3) transformations of English vowels.

**Factor** **Method** NISQA (↑) _δf0 (%, ↓)_ _δf1 (%, ↓)_ _δf2 (%, ↓)_ _δf3 (%, ↓)_

_f0_ TD-PSOLA 2.32 ± 0.55 3.8 ± 2.5 6.3 ± 2.8 3.7 ± 0.9 2.1 ± 0.5
WORLD 2.49 ± 0.60 4.5 ± 0.6 3.7 ± 1.8 2.3 ± 0.7 1.2 ± 0.2
VAE baseline 1.94 ± 0.43 6.21 ±2.8 10.4 ± 2.4 6.2 ± 0.9 4.5 ± 0.2
Proposed 2.08 ± 0.48 0.8 ± 0.2 7.2 ± 1.3 3.6 ± 1.2 3.8 ± 0.3

_f1_ VAE baseline 1.84 ± 0.5 11.3 ± 4.2 15.1 ± 3.5 6.0 ± 1.2 4.2 ± 0.4
Proposed 1.85 ± 0.4 6.0 ± 1.6 8.4 ± 3.2 5.7 ± 0.4 4.4 ± 0.3

_f2_ VAE baseline 2.01 ± 0.4 19.5 ± 3.2 10.7 ± 0.5 10.9 ± 1.9 5.8 ± 0.6
Proposed 2.03 ± 0.43 8.5 ± 1.1 8.7 ± 1.1 6.2 ± 1.5 5.8 ± 0.2

_f3_ VAE baseline 1.82 ± 0.14 27.0 ± 1.5 13.0 ± 1.3 12.0 ± 1.8 7.3 ± 1.5
Proposed 1.94 ± 0.48 8.3 ± 1.0 8.6 ± 0.7 4.9 ± 0.9 2.0 ± 0.4

D.2 ROBUSTNESS WITH RESPECT TO DIFFERENT TRAINING DATASETS FOR THE VAE

This Section investigates the robustness of the proposed method with respect to different datasets
used to train the VAE model. This table presents results for modifications of the fundamental frequency only, applied to the English vowels dataset. We considered the following training datasets:

-  WSJ: the Wall street Journal (Garofolo et al., 1993a) dataset that was used in the previous experiments.

-  SIWIS: the SIWIS French speech synthesis dataset (Honnet et al., 2017), which contains more
than ten hours of French speech recordings.

-  TESS: the Toronto emotional speech dataset (Dupuis & Pichora-Fuller, 2010), which contains
2800 utterances spoken by two actresses using different emotions (anger, disgust, fear, happiness,
pleasant surprise, sadness, and neutral).

-  LJspeech: the LJspeech dataset (Ito & Johnson, 2017), which contains 13100 short audio clips of
a single speaker reading passages from 7 non-fiction books.

The artificially-generated speech dataset used for learning the latent subspaces and the regression
models along with the test dataset of English vowels remain the same. It can be seen in Table 3
that the performance remains quite stable with different VAE training datasets. WSJ is the largest
dataset and therefore obtains the best performance. Interestingly, the results obtained with the SIWIS
dataset of French speech signals remain satisfactory, even if there is a mismatch between the training
(French) and testing (English) datasets.


-----

Table 3: Performance (mean and std) for fundamental frequency (f0) transformations of English
vowels using different datasets for training the unsupervised VAE model.

**Dataset** NISQA (↑) _δf0 (%, ↓)_ _δf1 (%, ↓)_ _δf2 (%, ↓)_ _δf3 (%, ↓)_

WSJ 2.08 ± 0.48 0.8 ± 0.2 7.2 ± 1.3 3.6 ± 1.2 3.8 ± 0.3
SIWIS 1.93 ± 0.43 1.2 ± 0.5 10.0 ± 4.2 8.3 ± 1.1 14.0 ± 0.2
TESS 1.98 ± 0.50 2.7 ± 2.3 9.3 ± 3.5 9.0 ± 0.8 7.0 ± 0.2
LJspeech 1.96 ± 0.40 1.2 ± 0.6 9.3 ± 1.2 5.6 ± 0.6 4.6 ± 0.1

D.3 EVALUATION ON THE TIMIT DATASET

In this Section, we evaluate the proposed method on the TIMIT dataset (Garofolo et al., 1993b),
using the VAE trained on the WSJ dataset. TIMIT is a corpus of phonemically and lexically transcribed speech of American English speakers of different sexes and dialects. We used the test corpus
containing 1680 utterances. Because we are interested in studying the interaction between modifications of the fundamental frequency and formant frequencies, we only evaluate the method on the
phonemes that are voiced (40 phonemes over a total of 52), which can be identified using the annotations. The ranges of transformation for the fundamental frequency, first, second and third formant
frequencies are respectively [100, 300] Hz, [300, 900] Hz, [1100, 2700] Hz, and [2200, 3200] Hz with
a step of 10 Hz, 50 Hz, 100 Hz and 50 Hz, respectively. The metrics and evaluated methods remain
the same as those described in Section 4.2.2.

TIMIT is phonemically richer than the English vowels dataset previously used, however it is not
labeled with the fundamental and formant frequencies. Therefore, we do not have the ground truth
values which makes the evaluation more difficult than with the English vowels dataset. Instead
of the ground truth, we use the formant frequencies and the fundamental frequency computed on
the original speech utterances (i.e., before transformation) using the same tools as described in
Section 4.2.2. This makes the evaluation on TIMIT less reliable than on the English vowels dataset,
but it allows us to test the methods on a larger variety of phonemes.

Results are presented in Table 4. They are very consistent with the ones obtained on the English
vowels dataset (see Tables 1 and 2), which further strengthens the conclusions drawn in the discussion paragraph of Section 4.2.2.

Table 4: Performance (mean and std) for the fundamental frequency (f0) and formant frequencies
(f1, f2 and f3) transformations on the TIMIT dataset.

**Factor** **Method** NISQA (↑) _δf0 (%, ↓)_ _δf1 (%, ↓)_ _δf2 (%, ↓)_ _δf3 (%, ↓)_

_f0_ TD-PSOLA 2.36 ± 0.50 2.4 ± 1.9 7.9 ± 0.6 4.5 ± 0.3 3.9 ± 0.2
WORLD 2.45 ± 0.47 0.3 ± 0.1 7.1 ± 1.2 6.2 ± 0.4 4.2 ± 0.2
VAE baseline 1.59 ± 0.43 16.1 ±6.3 17.0 ± 3.0 12.1± 0.2 10.9 ± 1.3
Proposed 2.28 ± 0.57 0.8 ± 0.6 9.1 ± 1.1 8.3 ± 0.9 6.0 ± 1.8

_f1_ VAE baseline 1.42 ± 0.34 10.1 ± 2.8 16.4 ± 1.4 12.4 ± 0.9 11.2 ± 2.6
Proposed 1.66 ± 0.31 7.1 ± 3.6 9.2 ± 0.8 9.0 ± 1.3 7.8 ± 1.1

_f2_ VAE baseline 1.46 ± 0.30 19.3 ± 5.0 16.4 ± 0.8 20.3 ± 6.3 11.5 ± 0.5
Proposed 1.49 ± 0.30 9.1 ± 2.2 8.3 ± 1.3 4.3 ± 1.3 8.1 ± 0.2

_f3_ VAE baseline 1.40 ± 0.48 20.4 ± 1.0 17.4 ± 0.2 14.4 ± 0.2 11.7 ± 2.3
Proposed 1.48 ± 0.42 8.5 ± 1.9 8.7 ± 0.9 5.7 ± 2.1 2.5 ± 1.8


-----

