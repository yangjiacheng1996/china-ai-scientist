# InstaHide’s Sample Complexity When Mixing Two Private Images

**Anonymous authors**
Paper under double-blind review

Abstract

Inspired by InstaHide challenge [Huang, Song, Li and Arora’20], [Chen, Song
and Zhuo’20] recently provides one mathematical formulation of InstaHide
attack problem under Gaussian images distribution. They show that it
suffices to use O(n[k]priv[priv][−][2][/][(][k][priv][+1)]) samples to recover one private image in

_n[O]priv[(][k][priv][)]_ + poly(npub) time for any integer kpriv, where npriv and npub denote
the number of images used in the private and the public dataset to generate a
mixed image sample. Under the current setup for the InstaHide challenge of
mixing two private images (kpriv = 2), this means n[4]priv[/][3] [samples are sufficient]
to recover a private image. In this work, we show that npriv log(npriv) samples
are sufficient (information-theoretically) for recovering all the private images.

1 Introduction

Collaboratively training neural networks based on sensitive data is appealing in many AI
applications, such as healthcare, fraud detection, and virtual assistants. How to train neural
networks without compromising data confidentiality and prediction accuracy has become an
important and common research goal Shokri & Shmatikov (2015); Ryffel et al. (2018); Phong
et al. (2018); McMahan et al. (2017); Konečn`y et al. (2016) in both academia and industry.

Huang et al. (2020b) recently proposed an approach called InstaHide for image classification.
The key idea is to train the model on a dataset where (1) each image is a mix of kpriv private
images and kpub public images, and (2) each pixel is independently sign-flipped after the
mixing. Instahide shows promising prediction accuracy on MNIST Deng (2012), CIFAR10 Krizhevsky (2012), CIFAR-100, and ImageNet datasets Deng et al. (2009). TextHide
Huang et al. (2020a) applies InstaHide’s idea to text datasets and achieves promising results
on natural language processing tasks.

To understand the security aspect of InstaHide in realistic deployment scenarios, InstaHide
authors present an InstaHide challenge Challenge (2020) that involves npriv = 100 private
images, ImageNet dataset as the public images, m = 5000 sample images (each image is a
combination of kpriv = 2 private images and kpub = 4 public images and the sign of each pixel
is randomly flipped). The challenge is to recover a private image given the set of sample
images.

Chen et al. (2021) is a theoretical work that formulates the InstaHide attack problem as a
recovery problem. It also provides an algorithm to recover a private image assuming each
private and public image is a random Gaussian image (i.e., each pixel is an i.i.d. draw
from (0, 1)). The algorithm shows that O(n[k]priv[priv][−][2][/][(][k][priv][+1)]) sample images are sufficient to
_N_
recover one private image. Carlini et al. (2020) provides the first heuristic-based practical
attack for the InstaHide challenge (kpriv = 2), and it can generate images that are visually
similar to the private images in the InstaHide challenge dataset. Luo et al. (2021) provides
the first heuristic-based practical attack for the InstaHide challenge (kpriv = 2) when data
augmentation is enabled.

Although the InstaHide challenge is considered broken by several researchers, the current
InstaHide challenge is itself too simple, and it is unclear whether the existing attacks Carlini


-----

et al. (2020); Luo et al. (2021) can still work when we use InstaHide to protect large numbers
of private images (large n) Arora (2020).

This raises an important question:

_What’s the minimal number of InstaHide images needed to recover a private image?_

This question is worthwhile to consider because it is a quantitative measure for how secure
InstaHide is. With the same formulation in Chen et al. (2021), we achieve a better upper
bound on the number of samples needed to recover private images when kpriv = 2. Our new
algorithm can recover all the private images using only Ω(npriv log(npriv)) samples.[1] This
significantly improves the state-of-the-art theoretical results Chen et al. (2021) that requires
_n[4]priv[/][3]_ [samples to recover a single private image. However, our running time is exponential]
in the number of private images (npriv) and polynomial in the number of public images
(npub), where the running time of the algorithm in Chen et al. (2021) is polynomial in npriv
and npub. In addition, we provide a four-step framework to compare our attacks with the
attacks presented in Carlini et al. (2020) and Chen et al. (2021). We hope our framework
can inspire more efficient attacks on InstaHide-like approaches and can guide the design of
future-generation deep learning algorithms on sensitive data.

1.1 Our result

Chen et al. (2021) formulates the InstaHide attack problem as a recovery problem that given
sample access to an oracle that can generate as much as InstaHide images you want, there
are two goals : 1) sample complexity, minimize the number InstaHide images being used, 2)
running time, use those InstaHide images to recover the original images as fast as possible.

Similar to Chen et al. (2021), we consider the case where private and public data vectors are
Gaussians. Let Spub denote the set of public images with |Spub| = npub, let Spriv denote the
set of private images with _Spriv_ = npriv. The model that produces InstaHide image can be
_|_ _|_
described as follows:

-  Pick kpub vectors from public data set and kpriv vectors from private data set.



-  Normalize kpub vectors by 1/


_kpub and normalize kpriv vectors by 1/_


_kpriv._



-  Add kpub + kpriv vectors together to obtain a new vector, then flip each coordinate of
that new vector independently with probability 1/2.

We state our result as follows:
**Theorem 1.1 (Informal version of Theorem 3.1). Let kpriv = 2. If there are npriv private**
_vectors and npub public vectors, each of which is an i.i.d. draw from N_ (0, Idd), then as long
_as_

_d = Ω(poly(kpub) log(npub + npriv)),_

_there is some m = O(npriv log npriv) such that, given a sample of m random synthetic vectors_
_independently generated as above, one can exactly recover all the private vectors in time_

_O(dm[2]_ + dn[2]pub [+][ n]pub[2][ω][+1] + mn[2]pub[) +][ d][2][O][(][m][)]

_with high probability._

**Notations.** For any positive integer n, we use [n] to denote the set {1, 2, · · ·, n}. For a set
_S, we use supp(S) to denote the support of S, i.e., the indices of its elements. We also use_
supp(w) to denote the support of vector w ∈ R[n], i.e. the indices of its non-zero coordinates.
For a vector x, we use ∥x∥2 to denotes entry-wise ℓ2 norm. For two vectors a and b, we use
_a_ _b to denote a vector where i-th entry is aibi. For a vector a, we use_ _a_ to denote a vector
_◦_ _|_ _|_
where theto denote the restriction of i-th entry is |ai|. Given a vector v to the coordinates indexed by v ∈ R[n] and a subset S. _S ⊂_ [n] we use [v]S ∈ R[|][S][|]

1For the worst case distribution, Ω(npriv) is a trivial sample complexity lower bound.


-----

**Contributions.** Our contributions can be summarized into the following folds.

-  We propose an algorithm that recover all the private images using only
Ω(npriv log(npriv)) samples in the recent theoretical framework of attacking InstaHide
Huang et al. (2020b) when mixing two private image, improving the state-of-art
result of Chen et al. (2021).

-  We summarize existing methods of attacking InstaHide into a unifying framework.
By examining the functionality of each steps we identify the connection of a key step
with problems in graph isomorphism. We also reveal the vulnerability of existing
method to recover all private images by showing hardness to recover all images.

**Organizations.** In Section 2 we formulate our attack problem. In Section 3 we present
our algorithm and main results. In Section 4 we conclude our paper and discuss future
directions.

2 Preliminaries

We use the same setup as Chen et al. (2021).

**Definition 2.1 (Image matrix notation, Definition 2.2 in Chen et al. (2021)). Let image**
_tomatrix n images each with X ∈_ R[d][×][n] _be a matrix whose columns consist of vectors d pixels taking values in R. It will also be convenient to refer to the x1, . . ., xn ∈_ R[d] _corresponding_
_rows of X as p1, . . ., pd ∈_ R[n].

We define public set, private set and synthetic images following the setup in Huang et al.
(2020b).

**Definition 2.2 (Public/private notation, Definition 2.3 in Chen et al. (2021)). We will refer**
_togiven a vector Spub ⊂_ [n] and w ∈ SRpriv[n], we will refer to = [n]\Spub as the set of public and private images respectively, and supp(w) ∩ _Spub and supp(w) ∩_ _Spriv as the public and_
_private coordinates of w respectively._
**Definition 2.3 (Synthetic images, Definition 2.4 in Chen et al. (2021)). Given sparsity**
_whichlevels k [wpub]S ≤|pub andSpub [|, kw]privSpriv ≤| areS kprivpub|, image matrix- and kpriv-sparse respectively, the corresponding synthetic X ∈_ R[d][×][n] _and a selection vector w ∈_ R[n] _for_
_image is the vector y[X][,w]_ = |Xw| ∈ R[d] _where | · | denotes entrywise absolute value. We say_
_thatdataset X ∈ YR ∈[d][×]R[n][m]and a sequence of selection vectors[×][d]_ _consisting of the images (y[X][,w][1] w, . . ., y1, . . ., w[X][,w][m]m) ∈[⊤].R[n]_ _give rise to a synthetic_

We consider Gaussian image which is a common setting in phase retrieval Candes et al.
(2013); Netrapalli et al. (2017); Candes et al. (2015).

**Definition 2.4 (Gaussian images, Definition 2.5 in Chen et al. (2021)). We say that X is a**
_random Gaussian image matrix if its entries are sampled i.i.d. from N_ (0, 1).

Distribution over selection vectors follows from variants of Mixup Zhang et al. (2017). Here
we ℓ2 normalize all vectors for convenience of analysis. Since kpriv is a small constant, our
analysis can be easily generalized to other normalizations.
**Definition 2.5 (Distribution over selection vectors, Definition 2.6 in Chen et al. (2021)).**

_Let D be the distribution over selection vectors defined as follows. To sample once from D,_
_draw random subset T1 ⊂_ _Spub, T2 ⊆_ _Spriv of size kpub and kpriv and output the unit vector_
_whose i-th entry is 1/_ _kpub if i ∈_ _T1, 1/_ _kpriv if i ∈_ _T2, and zero otherwise.[2]_

For convenience we will definep pub and privp operators as follows,
**Definition 2.6 (Public/private operators). We define function pub(·) and priv(·) such that**
_for vector w ∈_ R[n], pub(w) ∈ R[n][pub] _will be the vector which only contains the coordinates of_
_w corresponding to the public subset Spub, and priv(w) ∈_ R[n][priv] _will be the vector which only_
_contains the coordinates of w corresponding to the private subset Spriv._

2Note that any such vector does not specify a convex combination, but this choice of normalization
is just to make some of the analysis later on somewhat cleaner, and our results would still hold if we
chose the vectors in the support of D to have entries summing to 1.


-----

**Algorithm 1 Recovering All Private Images when kpriv = 2**

1: procedure RecoverAll(Y) _▷_ Theorem 3.1, Theorem 1.1

2: _▷_ InstaHide dataset Y = (y[X][,w][1] _, . . ., y[X][,w][m]_ )[⊤] _∈_ R[m][×][d]

3: _▷_ Step 1. Retrieve Gram matrix

4:5: **M ←** _kpriv+1_ _kpub_ _[·][ GramExtract][(][Y][,]_ 2(k▷pubStep 2. Subtract Public images from Gram matrix1+kpriv) [)] _▷_ Algorithm 1 in Chen et al. (2021)

6: **for i ∈** [m] do

7: _Si ←_ LearnPublic({(pj)Spub _, yj[X][,w][i]_ )}j∈[d]) _▷_ Algorithm 2 in Chen et al. (2021)

8: **end for**

9: **Wpub ←** (pub(vec(S1))1, . . ., pub(vec(Sm)))[⊤] _▷_ **Wpub ∈{0, 1}[m][×][n][pub]**

10: **Mpriv ←** _kpriv · (M −_ _kpub_ **[W][pub][W]pub[⊤]** [)]

11: _▷_ Step 3. Assign original images

13:12: **Wpriv ←** AssigningOriginalImages(Mpriv, npriv) _▷_ Step 4. Solving system of equations.▷ Algorithm 2

14: **Ypub =** _√k1pub_ **[W][pub][X]pub[⊤]** _▷_ **Xpub ∈** R[d][×][n][pub], Ypub ∈ R[m][×][d], Wpub ∈{0, 1}[m][×][n][pub]

15: _X ←_ SolvingSystemofEquations(Wpriv, _kprivYpub,_ _kprivY)_ _▷_ Algorithm 3

16: **return** _X_

p p

17: end proceduree

[e]

For subset _S ⊂_ _S we will refer to vec(S[e]) ∈_ R[n] as the vector that vec(S[e])i = 1 if i ∈ _S[e]_
and vec(S[e])i = 0 otherwise. We define the public and private components of W and Y for
convenience.[e]

**Definition 2.7 (Public and private components of image matrix and selection vectors). For**
_a sequence of selection vectors w1, . . ., wm ∈_ R[n] _we will refer to_

**W = (w1, . . ., wm)[⊤]** _∈_ R[m][×][n]

_as the mixup matrix._

_Specifically, we will refer to Wpub_ 0, 1 _as the public component of mixup matrix_
_and Wpriv_ 0, 1 _as the private component of mixup matrix, i.e., ∈{_ _}[m][×][n][pub]_
_∈{_ _}[m][×][n][priv]_

pub(W1,∗) priv(W1,∗)
_._ _._

**Wpub =** _kpub ·_  _.._  _∈{0, 1}[m][×][n][pub], Wpriv =_ _kpriv ·_  _.._  _∈{0, 1}[m][×][n][priv]_ _._

pub(Wm, ) priv(Wm, )

p  _∗_  p  _∗_ 

   

_We will refer to Xpub ∈_ R[d][×][n][pub] _as public component of image matrix which only contains the_
_columns of X ∈_ R[d][×][n] _corresponding to the public subset Spub, and Xpriv ∈_ R[d][×][n][priv] _as private_
_component of image matrix which only contains the columns of X ∈_ R[d][×][n] _corresponding to_
_the private subset Spriv._

_Furthermore we define Ypub ∈_ R[m][×][d] _as public contribution to InstaHide images and Ypriv ∈_
R[m][×][d] _as private contribution to InstaHide images:_


1

**WpubX[⊤]pub[,]** **Ypriv =**
_kpub_


1

**WprivX[⊤]priv[.]**
_kpriv_


**Ypub =**


Instead of considering only one private image recovery as Chen et al. (2021), here we consider
a harder question which requires to recover all the private images.

e


**Problem 1 (Exact Private image recovery). Let X ∈** R[d][×][n] _be a Gaussian image matrix._
_Given access to the public images {xs}s∈Spub and the synthetic dataset (y[X][,w][1]_ _, . . . , y[X][,w][m]_ ),
_wherefor which there exists a one-to-one mapping w1, . . . , wm ∼D are unknown selection vectors, output a set of vectors φ from {xs}s∈Spriv to {xs}s∈Spriv { satisfyingxs}s∈Spriv_
_φ(xs)j = (xs)j,_ _j_ [d].
_∀_ _∈_ e
e


-----

3 Recovering All Private Images when kpriv = 2

In this section, we prove our main algorithmic result. Our algorithm follows the high-level
procedure introduced in section A. The details are elaborated in following subsections.

**Theorem 3.1 (Main result). Let Spub ⊂** [n], and let npub = |Spub| and npriv = |Spriv|.
_Let kpriv = 2. Let k = kpriv + kpub. If d ≥_ Ω poly(kpub, kpriv) log(npub + npriv) _and m ≥_
Ω _k[poly(][k][priv][)]npriv log npriv_ _, then with high probability over X and the sequence of randomly_
  
_chosen selection vectors w1, . . ., wm_ _, there is an algorithm which takes as input the_
_synthetic dataset _ **Y[⊤]** = ( _y[X][,w][1]_ _, . . ., y ∼D[X][,w][m]) ∈_ R[d][×][m] _and the columns of X indexed by_
_Spub, and outputs npriv images {xs}s∈Spriv for which there exists one-to-one mapping φ from_
_{xs}s∈Spriv to {xs}s∈Spriv satisfying φ(xs)j = (xs)j for all j ∈_ [d]. Furthermore, the algorithm
_runs in time_
e
e e

_O(m[2]d + dn[2]pub_ [+][ n]pub[2][ω][+1] + mn[2]priv [+ 2][m][ ·][ mn]priv[2] _[d][)][.]_

**Remark 3.2. Our result improves on Chen et al. (2021) on two aspects. First, we reduce the**
_sample complexity from n[k]priv[priv][−][2][/][(][k][priv][+1)]_ _to npriv log npriv when kpriv = 2. Note that our sample_
_complexity is optimal up to logarithmic factors since finding unique solutions of linear system_
_requires at least npriv sample complexity. Second, we can recover all private images exactly_
_rather than recovering a single image, which is highly desirable for real-world practitioners._
_Furthermore notice that fixing all public images, multiplying any private image by −1 might_
_not keep InstaHide images unchanged. Thus information theoretically, we are able to recover_
_all private images precisely (not only absolute values) as long as we have access to sufficient_
_synthetic images. In fact, from the proof of Lemma 3.7 our sample complexity suffices to_
_achieve exact recovery._

3.1 Retrieving Gram matrix

In this section, we present the algorithm for retrieving the Gram matrix.

**Lemma 3.3 (Retrieve Gram matrix, Chen et al. (2021)). Let n = npub + npriv. Suppose**
_d = Ω(log(m/δ)/η[4])._ _For a random Gaussian image matrix X ∈_ R[d][×][n] _and arbitrary_
_w1, . . ., wm_ S[d][−]0[1][, let][ Σ][∗] _[be the output of][ GramExtract][ when we set][ η][ = 1][/][2][k][. Then]_
_∈_ _≥_
_with probability 1 −_ _δ over the randomness of X, we have that Σ[∗]_ = k · WW[⊤] _∈_ R[m][×][m].
_Furthermore, GramExtract runs in time O(m[2]d)._

We briefly describe how this is achieved. Without loss of generality, we may assume
_Spriv = [n], since once we determine the support of public images Spub, we can easily subtract_
the contribution of them. Consider a matrix Y ∈ R[m][×][d] whose rows are y[X][,w][1] _, . . ., y[X][,w][m]._
Then, it can be written as


_p1, w1_ _pd, w1_
_|⟨_ _⟩|_ _· · ·_ _|⟨_ _⟩|_
. .
.. ... ..
_p1, wm_ _pd, wm_ _._
_|⟨_ _⟩|_ _· · ·_ _|⟨_ _⟩|_


**Y =**


Since X is a Gaussian matrix, we can see that each column of Y is the absolute value
of an independent draw of N (0, WW[⊤]). We define this distribution as N [fold](0, WW[⊤]),
and it can be proved that the covariance matrix of N [fold](0, WW[⊤]) can be directly related
**WW[⊤]. Then, the task becomes estimating the covariance matrix of N** [fold](0, WW[⊤]) from
_d independent samples (columns of Y), which can be done by computing the empirical_
estimates.

3.2 Remove public images

In this section, we present the algorithm of subtracting public images from Gram matrix.
Formally, given any synthetic image y[X][,w] we recover the entire support of [w]Spub (essentially
supp([w]Spub)).


-----

**Lemma 3.4 (Subtract public images from Gram matrix, Chen et al. (2021)). Let n =**
_npriv + npub. For any δ ≥_ 0, if d = Ω(poly(kpub)/ log(n/δ)), then with probability at least
1 − _δ over the randomness of X, we have that the coordinates output by LearnPublic are_
_exactly equal to supp([w]Spub_ ). Furthermore, LearnPublic runs in time O(dn[2]pub [+][ n][2]pub[ω][+1]),
_where ω ≈_ 2.373 is the exponent of matrix multiplication Williams (2012).

Note that this problem is closely related to the Gaussian phase retrieval problem. However,
we can only access the public subset of coordinates for any image vector pi. We denote these
partial vectors as {[pi]Spub}i∈[d]. The first step is to construct a matrix **M ∈** R[n][pub][×][n][pub]:

_d_

[f]

**M = d[1]** _i=1((yi[X][,w])[2]_ _−_ 1) · ([pi]Spub[pi][⊤]Spub _[−]_ **[I][)][.]**

X

f

It can be proved that when pi’s are i.i.d standard Gaussian vectors, the expectation of
**M is M =** [1]2 [[][w][]][S][pub][[][w][]]S[⊤]pub[. However, when][ d][ ≪] _[n][,][ f]M is not a sufficiently good spectral_

approximation of M, which means we cannot directly use the top eigenvector of **M. Instead,**
with high probabilityf [w]Spub can be approximated by the top eigenvector of the solution of
the following semi-definite programming (SDP):

[f]

_npub_

max **M, Z** _s.t. tr[Z] = 1,_ _Zi,j_ _kpub._
_Z_ 0 _⟩_ _|_ _| ≤_
_⪰_ _[⟨][f]_ _i,j=1_

X

Hence, the time complexity of this step is O(dn[2]pub [+][ n][2]pub[ω][+1]), where the first term is the time

cost for constructing **M and the second term is the time cost for SDP Jiang et al. (2020).**

3.3 Assigning encoded images to original images[f]

We are now at the position of recovering Wpriv ∈ R[m][×][n][priv] from private Gram matrix
**Mpriv** R[m][×][m]. Recall that Mpriv = WprivWpriv[⊤]
mixup matrix with column sparsity ∈ _kpriv. By recovering mixup matrix[∈]_ [R][m][×][m][ where][ W][priv] W[ ∈{] from private Gram[0][,][ 1][}][m][×][n][priv][ is the]
matrix M the attacker maps each synthetic image y[X][,w][i] _, i ∈_ [m] to two original images
_xi1_ _, . . ., xikpriv (to be recovered in the next step) in the private data set, where kpriv = 2._

On the other hand, in order to recover original image xi from private data set, the attacker
needs to know precisely the set of synthetic images y[X][,w][i] _, i_ [m] generated by xi. Therefore
_∈_
this step is crucial to recover the original private images from InstaHide images. We provide
an algorithm and certify that it outputs the private component of mixup matrix with sample
complexity m = Ω(npriv log npriv).

As noted by Chen et al. (2021), the intricacy of this step lies in the fact that a family of
sets may not be uniquely identifiable from the cardinality of all pairwise intersections. This
problem is formally stated in the following.

**Problem 2 (Recover sets from cardinality of pairwise intersections). Let Si** [n], i

[m] be n sets with cardinality k. Given access to the cardinality of pairwise intersections ⊂ _∈_
_Si_ _Sj_ _for all i, j_ [m], output a family of sets _Si_ [n], i [m] for which there exists
_|_ _∩_ _|_ _∈_ _⊂_ _∈_
_a one-to-one mapping φ from_ _Si, i ∈_ [m] to Si, i ∈ [m] satisfying φ(S[e]j) = Sj for all
_j_ [m]. [e]
_∈_

[e]


In real world applications, attackers may not even have access to precise cardinality of
pairwise intersections _Si_ _Sj_ for all i, j [m] due to errors in retrieving Gram matrix
_|_ _∩_ _|_ _∈_
and public coordinates. Instead, attackers often face a harder version of the above problem,
where they only know whether _Si_ _Sj_ is an empty set for i, j [m]. However for mixing
_|_ _∩_ _|_ _∈_
two private images, the two problems are the same.

We now provide a solution to this problem. First we define a concept closely related to the
above problem.


-----

**Algorithm 2 Assigning Original Images**

1: procedure AssigningOriginalImages(Mpriv, npriv)
2:3:4: **Mif nG ←priv <M 5priv then −▷** **MI** priv ∈ R[m][×][m] is Private Gram matrix, npriv is the number of private images

5: **for H ∈{0, 1}[n][priv][×][n][priv]** **do**

6:7: **Mif MH ←H =adjacency matrix of the line graph of MG then** _H_

8: **W ←** **W[f] ∪{WH** _}_ _▷_ **WH is the incidence matrix of H**

9: **end if**

10: **end forf**

11: **return** **W**

12: **end if**

13: Reconstruct G from MG _▷_ By Theorem C.2

[f]

14: **return W** _▷_ The incidence matrix of G

15: end procedure


**Definition 3.5 (Distinguishable). For matrix M ∈** R[m][×][m], we say M is distinguishable if
_there exists unique solution W = (w1, . . ., wm)[⊤]_ _(up to permutation of rows) to the equation_
**WW[⊤]** = M such that wi supp( priv) for all i [m].
_∈_ _D_ _∈_

**Lemma 3.6 (Assign InstaHide images to the original images). When m = Ω(npriv log npriv),**
_let Wpriv = (w1, . . ., wm)[⊤]_ _where wi, i ∈_ [m] are sampled from distribution Dpriv and Mpriv =
**WprivWpriv[⊤]**

_[∈]_ [R][m][×][m][. Then with high probability][ M][priv][ is distinguishable and algorithm]
AssigningOriginalImages inputs private Gram matrix Mpriv 0, 1, 2 _correctly_
_∈{_ _}[m][×][m]_
_outputs Wpriv ∈{0, 1}[m][×][n][priv]_ _with row sparsity kpriv = 2 such that WprivWpriv[⊤]_ [=][ M][priv][.]
_Furthermore AssigningOriginalImages runs in time O(mnpriv)._

The proof of Lemma 3.6 is deferred to Appendix C. We consider graph G = (V, E), |V | =
_nand eachpriv and | eE = (| = mvi, v where eachj)_ _E correspond to an encrypted image generated from two original vi ∈_ _V corresponds to an original image in private data set_
_∈_
images corresponding to vi and vj. We define the Gram matrix of graph G = (V, E), denoted
bythe incidence matrix of G. That is MG ∈{0, 1, 2}[m][×][m] where m = |[3]E|, to be MG = WW[⊤] _−_ **I where W ∈{0, 1}[m][×][n][priv]** is

_e1_ _e1_ _e1_ _em_
_|_ _∩._ _|_ _· · ·_ _|_ _∩._ _|_

**MG =**  .. ... ..  _∈{0, 1, 2}[m][×][m]._

_em_ _e1_ _em_ _em_

| _∩_ _|_ _· · ·_ _|_ _∩_ _|_
 

We can see that MG actually correspond the line graph L(G) of the graph G. We similarly call
a graph G distinguishable if there exists no other graph G[′] such that G and G[′] have the same
Gram matrix (up to permutations of edges), namely MG = MG′ (for some ordering of edges).
To put it into another word, if we know MG, we can recover G uniquely. Therefore, recovering
**Wgraph is distinguishable if and only if its Gram matrix from M can be viewed as recovering graph G from its Gram matrix MG is distinguishable. MG ∈** R[m][×][m], and a

This problem was studied since the 1970s and fully resolved by Whitney Whitney (1992). In
fact, from a line graph L(G) one can first identify a tree of the original graph G and then
proceed to recover the whole graph. The proof is then completed from well-known facts in
random graph theory Erdős & Rényi (1960) that G is connected with high probability when
_m = Ω(npriv log npriv). This paradigm can potentially be extended to handle k ≥_ 3 case with
more information of G. Intuitively, this is achievable for a dense subgraph of G, such as the
local structure identified by Chen et al. (2021). More discussion can be found in Appendix C.

3.4 Solving a large system of equations

In this section, we solve the step 4, recovering all private images by solving an ℓ2-regression
problem. Formally, given mixup coefficients Wpriv (for private images) and contributions to

3With high probability, W will not have multi-edge. So, most entries of M will be in {0, 1}.


-----

InstaHide images from public images Ypub we recover all private images Xpriv (up to absolute
value).
**Lemma 3.7 (Solve ℓ2-regression with hidden signs). Given Wpriv ∈** R[m][×][n][priv] _and Ypub, Y ∈_
R[m][×][d]. For each i ∈ [d], let Y∗,i ∈ R[m] _denote the i-th column of Y and similarily for Ypub∗,i,_
_the following ℓ2 regression_

min
_zi_ R[n][priv][ ∥|][W][priv][z][i][ +][ Y][pub][∗][,i][| −] **[Y][∗][,i][∥][2][.]**
_∈_

_for all i_ [d] can be solve by SolvingSystemOfEquations in time O(2[m] _mn[2]priv_
_∈_ _·_ _[·][ d][)][.]_

_Proof. Suppose Wpriv = [w1_ _w2_ _· · ·_ _wm][⊤]. Then, the ℓ2-regression we considered actu-_
ally minimizes


( _wj[⊤][z][i]_ [+][ Y][pub]j,i[| −] **[Y][j,i][)][2][ =]**
_|_
_j=1_

X


(wj[⊤][z][i] [+][ Y][pub]j,i
_j=1_ _[−]_ _[σ][j]_ _[·][ Y][j,i][)][2][,]_

X


where σj ∈{−1, 1} is the sign of wjzi[∗] [for the minimizer][ z]i[∗][.]

Therefore, in Algorithm 3, we enumerate all possible σ ∈{±1}[m]. Once σ is fixed, the
optimization problem becomes the usual ℓ2-regression, which can be solved in O(n[ω]priv [+][mn][2]priv[)]
time. Since we assume m = Ω(npriv log(npriv)) in the previous step, the total time complexity
is O(2[m] _· mn[2]priv[)][.]_

If signm (Wprivz + Ypub∗,i) = σ holds for z = minz∈R[n]priv ∥Wprivz + Ypub∗,i − _σ ◦_ **Y∗,i∥2, then**
_j=1[(][|][w]j[⊤][z][i][ +][ Y][pub]j,i[| −]_ **[Y][j,i][)][2][ = 0][ and][ z][ is the unique minimizer of the signed][ ℓ][2][-regression]**
problem almost surely.
P

Indeed, if we have for σ ̸= _σ, Wpriv(Xpriv[⊤]_ [)]∗,i [+][ Y][pub]∗,i _[−]_ _[σ][ ◦]_ **[Y][∗][,i][ = 0][ and][ W][priv]z[e] + Ypub∗,i −**

_σ ◦_ **Y∗,i = 0 hold, then from direct calculations we come to e** **Wprivz = σ ◦** _σ ◦_ (Wpriv(Xpriv[⊤] [)]∗,i [+]

**Ye** pub∗,i) − **Ypub∗,i. This indicates that σ ◦** _σ ◦_ (Wpriv(Xpriv[⊤] [)]∗,i [+]e[ Y][pub]∗,ie[)][ −] **[Y][pub]∗,i** [lies in a]

_npriv-dimensional subspace of R[m]. Noting that (Xpriv[⊤]_ [)]j,i [and][ Y][pub]j,i [are i.i.d sampled from]
Gaussian, the event above happens with probability zero sincee _m_ _npriv. Thus, we can_
_≫_
repeat this process for all i [d] and solve all zi’s.
_∈_

We also show that ℓ2-regression with hidden signs is in fact a very hard problem. Although
empirical methods may bypass this issue by directly applying gradient descent, real world
practitioners taking shortcuts would certainly suffer from a lack of apriori theoretical
guarantees when facing a large private dataset.
**Theorem 3.8 (Lower bound of ℓ2-regression with hidden signs, informal version of The-**
orem D.4.). There exists a constant ϵ > 0 such that it is NP-hard to (1 + ϵ)-approximate
minz∈Rn ∥|Wz| − _y∥2, where W ∈{0, 1}[m][×][n]_ _is row 2-sparse and y ∈{0, 1}[m]._

We will reduce the MAX-CUT problem to the ℓ2-regression. MAX-CUT is a well-known
NP-hard problem Berman & Karpinski (1999). A MAX-CUT instance is a graph G = (V, E)
with n vertices and m edges. The goal is to find a subset of vertices S ⊆ _V such that_
the number of edges betweenfurther assume G is 3-regular, that is, each vertex has degree 3. The full proof is deferred to S and V \S is maximized, i.e., maxS⊆V |E(S, V \S)|. We can
Appendix D.

3.5 Putting everything together

Now we are in the position to prove Theorem 3.1.

_Proof. By Lemma 3.3 the matrix computed in Line 4 satisfies M = WW[⊤]. By Lemma 3.4,_
Line 7 correctly computes the indices of all public coordinaes of wi, i [m]. Therefore from
_∈_

**M = WW[⊤]** = WprivWpriv[⊤] _[/k][priv]_ [+][ W][pub][W]pub[⊤] _[/k][pub][,]_


-----

**Algorithm 3 Solving a large system of equations**

1: procedure SolvingSystemOfEquations(Wpriv, Ypub, Y)
2:3: **for i = 1 →** _d do_ _▷_ **Wpriv ∈** R[m][×][n][priv] _, Ypub ∈_ R[m][×][d], Y ∈ R[m][×][d]

4: _xi_

5: **for ←∅ σ ∈{−1, +1}[m]** **do**

6:7: e _zif ← sign(minWz∈privR[n]zpriv + ∥ YWpubpriv∗z,i +) = Y σpub then∗,i −_ _σ ◦_ **Y∗,i∥2**

8: _xi_ _xi_ _z_

9: **end if ←** _∪_

10: **end for**

e e

11: **end for**

12: _X_ _x1,_ _,_ _xd_
_←{_ _· · ·_ _}_

13: **return** _X_

14: end proceduree e e

[e]

the Gram matrix computed in Line 10 satisfies Mpriv = WprivWpriv[⊤] [.]

We can now apply Lemma 3.6 to find the private components of mixup weights. Indeed, the
output of Line 12 is exactly

priv(W1,∗)
.

**Wpriv = kpriv ·**  ..  _∈{0, 1}[m][×][n][priv]_ _._

priv(Wm,∗)
 

Based on the correctness of private weights, the output in Line 15 is exactly all private
images by Lemma 3.7. This completes the proof of correctness of Algorithm 1.

By Lemma 3.3, Line 4 takes O(m[2]d) time. By Lemma 3.4 Line 7 runs in time O(dn[2]pub[+][n][2]pub[ω][+1]).
By Lemma 3.6 private weights can be computed in time mnpriv. Line 10 and Line 14 can be
computed efficiently in time O(m[ω]). Finally Line 15 is computes in time O(2[m] _mn[2]priv_
by Lemma 3.7. Combining all these steps we have that the total running time of Algorithm 1· _[·][ d][)]_
is bounded by O(m[2]d + dn[2]pub [+][ n][2]pub[ω][+1] + mn[2]priv [+ 2][m][ ·][ mn]priv[2] _[d][)][.]_

4 Conclusion and Future Work

We show that Ω(npriv log npriv) samples suffice to recover all private images under the current
setup for InstaHide challenge of mixing two private images. We show that a key step in
attacking procedure can be formulated as a problem of graph isomorphism and prove the
uniqueness and hardness of recovery. Our approach has significantly advanced the state-ofthe-art approach Chen et al. (2021) that requires n[4]priv[/][3] [samples to recover a single private]
image. In addition, we present a theoretical framework to reason about the similarities and
differences of existing attacks Carlini et al. (2020); Chen et al. (2021) and our attack on
Instahide.

Based on our framework, there are several interesting directions for future study:

-  How to generalize our result to recover all private images when mixing more than
two private images?

-  How to extend this framework to analyze multi-task phase retrieval problem with
real-world data?

Real-world security is not a binary issue. We hope that our theoretical contributions shed
light on the discussion of safety for distributed training algorithms and provide inspirations
for the development of better practical privacy-preserving machine learning methods.


-----

5 Ethics Statement

Our work can potentially create negative societal impacts because our theoretical results
can inspire new efficient practical attack methods to machine learning models protected by
InstaHide.

6 Reproducibility Statement

This is a theory paper. We explicitly stated our assumptions and we provided the complete
proofs in supplementary materials.

References

Sanjeev Arora. Instahide. http://www.offconvex.org/2020/11/11/instahide/, 2020.

Piotr Berman and Marek Karpinski. On some tighter inapproximability results. In Inter_national Colloquium on Automata, Languages, and Programming, pp. 200–209. Springer,_
1999.

J Candes, Emmanuel, Thomas Strohmer, and Vladislav Voroninski. Phaselift: Exact and
stable signal recovery from magnitude measurements via convex programming. Communi_cations on Pure and Applied Mathematics, 66(8):1241–1274, 2013._

J Candes, Emmanuel, Xiaodong Li, and Mahdi. Soltanolkotabi. Phase retrieval via wirtinger
flow: Theory and algorithms. IEEE Transactions on Information Theory, 61(4):1985–2007,
2015.

Nicholas Carlini, Samuel Deng, Sanjam Garg, Somesh Jha, Saeed Mahloujifar, Mohammad Mahmoody, Shuang Song, Abhradeep Thakurta, and Florian Tramer. An attack
on instahide: Is private learning possible with instance encoding? _arXiv preprint_
_arXiv:2011.05315, 2020._

InstaHide Challenge. Instahide challenge. https://github.com/Hazelsuko07/InstaHideChallenge,
2020.

Sitan Chen, Xiaoxiao Li, Zhao Song, and Danyang Zhuo. On instahide, phase retrieval, and
sparse matrix factorization. In ICLR, 2021.

Daniele Giorgio Degiorgi and Klaus Simon. A dynamic algorithm for line graph recognition.
In International Workshop on Graph-Theoretic Concepts in Computer Science, pp. 37–48.
Springer, 1995.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern
_recognition (CVPR), pp. 248–255, 2009._

L. Deng. The mnist database of handwritten digit images for machine learning research [best
of the web]. IEEE Signal Processing Magazine, 29(6):141–142, 2012. doi: 10.1109/MSP.
2012.2211477.

Paul Erdős and Alfréd Rényi. On the evolution of random graphs. Publ. Math. Inst. Hung.
_Acad. Sci, 5(1):17–60, 1960._

Dimitris Fotakis, Michael Lampis, and Vangelis Th Paschos. Sub-exponential approximation
schemes for csps: From dense to almost sparse. In 33rd Symposium on Theoretical Aspects
_of Computer Science (STACS 2016). Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik,_
2016.

Baihe Huang, Shunhua Jiang, Zhao Song, and Runzhou Tao. Solving tall dense sdps in the
current matrix multiplication time. arXiv preprint arXiv:2101.08208, 2021.


-----

Yangsibo Huang, Zhao Song, Danqi Chen, Kai Li, and Sanjeev Arora. Texthide: Tackling
data privacy in language understanding tasks. In The Conference on Empirical Methods
_in Natural Language Processing (Findings of EMNLP), 2020a._

Yangsibo Huang, Zhao Song, Kai Li, and Sanjeev Arora. Instahide: Instance-hiding schemes
for private distributed learning. In International Conference on Machine Learning (ICML),
2020b.

Russell Impagliazzo and Ramamohan Paturi. On the complexity of k-sat. _Journal of_
_Computer and System Sciences, 62(2):367–375, 2001._

Michael S Jacobson, André E Kézdy, and Jenő Lehel. Recognizing intersection graphs of
linear uniform hypergraphs. Graphs and Combinatorics, 13(4):359–367, 1997.

Haotian Jiang, Tarun Kathuria, Yin Tat Lee, Swati Padmanabhan, and Zhao Song. A faster
interior point method for semidefinite programming. In 61st Annual IEEE Symposium on
_Foundations of Computer Science (FOCS), 2020._

Jakub Konečn`y, H Brendan McMahan, Felix X Yu, Peter Richtárik, Ananda Theertha
Suresh, and Dave Bacon. Federated learning: Strategies for improving communication
efficiency. arXiv preprint arXiv:1610.05492, 2016.

Alex Krizhevsky. Learning multiple layers of features from tiny images. University of Toronto,
05 2012.

Philippe GH Lehot. An optimal algorithm to detect a line graph and output its root graph.
_Journal of the ACM (JACM), 21(4):569–575, 1974._

AG Levin and Regina Iosifovna Tyshkevich. Edge hypergraphs. Diskretnaya Matematika, 5
(1):112–129, 1993.

Dajie Liu, Stojan Trajanovski, and Piet Van Mieghem. Iligra: an efficient inverse line graph
algorithm. Journal of Mathematical Modelling and Algorithms in Operations Research, 14
(1):13–33, 2015.

L Lovász. Problem, beitrag zur graphentheorie und deren auwendungen, vorgstragen auf
dem intern. koll, 1977.

Xinjian Luo, Xiaokui Xiao, Yuncheng Wu, Juncheng Liu, and Beng Chin Ooi. A fusiondenoising attack on instahide with data augmentation. arXiv preprint arXiv:2105.07754,
2021.

Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial
_Intelligence and Statistics, pp. 1273–1282. PMLR, 2017._

Praneeth Netrapalli, Prateek Jain, and Sujay. Sanghavi. Phase retrieval using alternating
minimization. In Advances in Neural Information Processing Systems (NeurIPS), pp.
1273–1282, 2017.

L. T. Phong, Y. Aono, T. Hayashi, L. Wang, and S. Moriai. Privacy-preserving deep learning
via additively homomorphic encryption. IEEE Transactions on Information Forensics and
_Security, 13(5):1333–1345, 2018. doi: 10.1109/TIFS.2017.2787987._

Svatopluk Poljak, Vojtěch Rödl, and Daniel TURZiK. Complexity of representation of graphs
by set systems. Discrete Applied Mathematics, 3(4):301–312, 1981.

Nicholas D Roussopoulos. A max {m, n} algorithm for determining the graph h from its line
graph g. Information Processing Letters, 2(4):108–112, 1973.

Theo Ryffel, Andrew Trask, Morten Dahl, Bobby Wagner, Jason Mancuso, Daniel Rueckert,
and Jonathan Passerat-Palmbach. A generic framework for privacy preserving deep
[learning. CoRR, abs/1811.04017, 2018. URL http://arxiv.org/abs/1811.04017.](http://arxiv.org/abs/1811.04017)


-----

Reza Shokri and Vitaly Shmatikov. Privacy-preserving deep learning. In Proceedings of
_the 22nd ACM SIGSAC Conference on Computer and Communications Security, CCS_
’15, pp. 1310–1321, New York, NY, USA, 2015. Association for Computing Machinery.
[ISBN 9781450338325. doi: 10.1145/2810103.2813687. URL https://doi.org/10.1145/](https://doi.org/10.1145/2810103.2813687)
```
 2810103.2813687.

```
Maciej M Syslo. A labeling algorithm to recognize a line digraph and output its root graph.
_Information Processing Letters, 15(1):28–30, 1982._

Hassler Whitney. Congruent graphs and the connectivity of graphs. In Hassler Whitney
_Collected Papers, pp. 61–79. Springer, 1992._

Virginia Vassilevska Williams. Multiplying matrices faster than coppersmith-winograd. In
_Proceedings of the forty-fourth annual ACM symposium on Theory of computing (STOC),_
pp. 887–898. ACM, 2012.

Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond
[empirical risk minimization. In ICLR. https://arxiv.org/abs/1710.09412, 2017.](https://arxiv.org/abs/1710.09412)

IE Zverovich. An analogue of the whithey theorem for edge graphs of multigraphs, and edge
multigraphs. Discrete Mathematics and Applications, 7(3):287–294, 1997.


-----

A A Unified Framework to Compare with Existing Attacks

|Refs|Rec.|k priv|Samples|Step 1|Step 2|Step 3|Step 4|
|---|---|---|---|---|---|---|---|
|Chen|one|2 ≥|m ≥nkpriv−2/(kpriv+1)|dm2|dn2 + n2 puω b+1 pub|m2|2k p2 riv|
|Ours|all|= 2|m ≥n log n priv priv|dm2|dn2 + n2 puω b+1 pub|mn priv|2m n2 d · priv|


**Refs** **Rec.** _kpriv_ **Samples** **Step 1** **Step 2** **Step 3** **Step 4**

Chen one _≥_ 2 _m ≥_ _n[k][priv][−][2][/][(][k][priv][+1)]_ _dm[2]_ _dn[2]pub_ [+][ n][2]pub[ω][+1] _m[2]_ 2[k]priv[2]

Ours all = 2 _m ≥_ _npriv log npriv_ _dm[2]_ _dn[2]pub_ [+][ n][2]pub[ω][+1] _mnpriv_ 2[m] _· n[2]priv[d]_


Table 1: A summary of running times in different steps between ours and Chen et al. (2021).
This table only compares the theoretical result. Let kpriv denote the number of private images we
select in InstaHide image. Let d denote the dimension of image. Let npub denote the number of
images in public dataset. Let npriv denote the number of images in private dataset. We provide a
computational lower bound for Step 4 in Appendix D. There is no algorithm that solves Step 4
in 2[o][(][n][priv][)] time under Exponential Time Hypothesis (ETH) (Theorem D.4). Let Rec. denote the
Recover.

Our attack algorithm (Algorithm 1) contains four steps for kpriv = 2. We can prove
_m = O(npriv log(npriv)) suffices for exact recovery. Our algorithm shares similarities as two_
recent attack results : one is a practical attack Carlini et al. (2020), the other is a theoretical
attack Chen et al. (2021). In the next few paragraphs, we describe our attack algorithm in
four major steps. For each step, we also give a comparison with the corresponding step in
Carlini et al. (2020) and Chen et al. (2021).

-  Step 1. Section 3.1. Recover the Gram matrix M = WW[⊤] _∈_ R[m][×][m] of mixup
weights W from synthetic images Y. This Gram matrix contains all inner products
of mixup weights _wi, wj_ . Intuitively this measures the similarity of each pair of
_⟨_ _⟩_
two synthetic images and is a natural start of all existing attacking algorithms.

**– For this step, Carlini et al. (2020)’s attack uses a pre-trained neural network on**
public dataset to construct the Gram matrix.

**– For this step, note that Y follows folded Gaussian distribution whose covariance**
matrix is directly related to M. We can thus solve this step by estimating
the covariance of a folded Gaussian distribution. This is achieved by using
Algorithm 2 in Chen et al. (2021). It takes O(m[2]d) time.

-  Step 2. Section 3.2. Recover all public image coefficients and substract the
contribution of public coefficients from Gram matrix M to obtain Mpriv. This step
is considered as one of the main computational obstacle for private image recovery.

**– For this step, Carlini et al. (2020)’s attack: 1) they treat public images as**
noise, 2) they don’t need to take care of the public images’ labels, since current
InstaHide Challenge doesn’t provide label for public images.

**– For this step, we invoke a paradigm in sparse phase retrieval via using general**
SDP solver to approximate the principle components of the Gram matrix of
public coefficients. Chen et al. (2021) proved that this method exactly outputs
all public coefficients. The time of this step has two parts : 1) formulating
the matrix, which takes dn[2]pub[, 2) solving a SDP with][ n][2]pub _[×][ n]pub[2]_ [size matrix]
variable and O(n[2]pub[)][ constraints, which takes][ n][2]pub[ω][+1] time Jiang et al. (2020);
Huang et al. (2021), where ω is the exponent of matrix multiplication.

-  Step 3. Section 3.3. Recover private coefficients Wpriv ∈ R[m][×][n][priv] from private
Gram matrix Mpriv (Mpriv = WprivWpriv[⊤] [), this step takes][ O][(][m][ ·][ n][2]priv[)][ time.]

**– For this step, Carlini et al. (2020)’s attack uses K-means to figure out cliques**
and then solves a min-cost max flow problem to find the correspondence between
InstaHide image and original image (see Appendix B for detailed discussions).

**– For this step Chen et al. (2021) starts by finding a local structure called**
“floral matrix” in the Gram matrix. They prove the existence of this local
structure when m _npriv[k][priv][−][2][/][(][k][priv][+1)]. Then Chen et al. (2021) can recover private_
_≥_
coefficients within that local structure using nice combinatorial properties of
the “floral matrix”.


-----

**– For this step, we note the fact that in kpriv = 2 case the mixup matrix corresponds**
to the incident matrix of a graph G and the Gram matrix corresponds to its
line graph L(G) (while kpriv 3 cases correspond to hypergraphs). We can
_≥_
then leverage results in graph isomorphism theory to recover the all private
coefficients. In particular, when m ≥ Ω(npriv log npriv) the private coefficients
are uniquely identifiable from the Gram matrix.

-  Step 4. Section 3.4. Solve d independent ℓ2-regression problems to find private
images Xpriv. Given Wpriv R[m][×][n][priv] and Y R[m][×][d]. For each i [d], let Y _,i_ R[m]
denote the i-th column of ∈ Y, we need to solve the following ∈ _ℓ2 regression ∈_ _∗_ _∈_

min
_z_ R[n][priv][ ∥|][W][priv][z][ +][ Y][pub][∗][,i][| −|][Y][∗][,i][|∥][2][.]
_∈_

The classical ℓ2 regression can be solved in an efficient way in both theory and
practice. However, here we don’t know the random signs and we have to consider
all 2[m] possibilities. In fact we show that solving ℓ2 regression with hidden signs is
NP-hard.

**– For this step, Carlini et al. (2020)’s attack is a heuristic algorithm that uses**
gradient descent.

**– For this step, we enumerate all possibilities of random signs to reduce it to**
standard ℓ2 regressions. Chen et al. (2021)’s attack is doing the exact same
thing as us. However, since their goal is just recovering one private image (which
means m = O(k[2])) they only need to guess 2[k][2] possibilities.

B Summary of the Attack by Carlini et al. Carlini et al. (2020)

This section summarizes the result of Carlini et al, which is an attack of InstaHide when
_kpriv = 2. Carlini et al. (2020). We first briefly describe the current version of InstaHide_
Challenge. Suppose there are npriv private images, the InstaHide authors Huang et al.
(2020b) first choose a parameter T, this can be viewed as the number of iterations in the deep
learning training process. For each t ∈ [T ], Huang et al. (2020b) draws a random permutation
_πt : [npriv] →_ [npriv]. Each InstaHide image is constructed from a private image i, another
private image πt(i) and also some public images. Therefore, there are T · npriv InstaHide
images in total. Here is a trivial observation: each private image shown up in exactly 2T
InstaHide images (because kpriv = 2). The model in Chen et al. (2021) is a different one: each
InstaHide image is constructed from two random private images and some random public
images. Thus, the observation that each private image appears exactly 2T does not hold. In
the current version of InstaHide Challenge, the InstaHide authors create the InstaHide labels
(a vector that lies in R[L] where the L is the number of classes in image classification task)
in a way that the label of an InstaHide image is a linear combination of labels (i.e., one-hot
vectors) of the private images and not the public images. This is also a major difference
compared with Chen et al. (2021). Note that Carlini et al. (2020) won’t be confused about,
for the label of an InstaHide image, which coordinates of the label vector are from the private
images and which are from the public images.

-  Step 1. Recover a similarity[4] matrix M ∈{0, 1, 2}[m][×][m].

**– Train a deep neural network based on all the public images, and use that neural**
network to construct the similarity matrix M.

-  Step 2. Treat public image as noise.

-  Step 3. Clustering. This step is divided into 3 substeps.
The first substep uses the similarity matrix M to construct Tnpriv clusters of InstaHide
images based on each InstaHide image such that the images inside one cluster shares
a common original image.
The second substep runs K-means on these clusters, to group clusters into npriv
groups such that each group corresponds to one original image.

4In Carlini et al. (2020), they call it similarity matrix, in Chen et al. (2021) they call it Gram
matrix. Here, we follow Carlini et al. (2020) for convenience.


-----

The third substep constructs a min-cost flow graph to compute the two original
images that each InstaHide image is mixed from.

**– Grow clusters. Figure 1 illustrates an example of this step. For a subset S of**
InstaHide images (S ⊂ [m]), we define insert(S) as


Insert(S) = S arg max
_∪_ _i_ [m]
_∈_


**Mi,j**

Xj∈S


For each i ∈ [m], we compute set Si ⊂ [m] where Si = insert[(][T/][2)]({i}).

**– Select cluster representatives. Figure 1 illustrates an example of this step.**
Define distance between clusters as

dist(i, j) =

_[|]S[S][i]i[ ∩]_ _S[S][j]j[|]_

_|_ _∪_ _|_ _[.]_

Run k-means using metric dist : [m] × [m] → R and k = npriv. Result is npriv
groups C1, . . ., Cnpriv [m]. Randomly select a representative ri _Ci, for each_
_i_ [npriv]. _⊆_ _∈_
_∈_

**– Computing assignments. Construct a min-cost flow graph as Figure 2, with**
weight matrix **W ∈** R[m][×][n][priv] defined as follows:

1

[f] **Wi,j =** **Mi,k.**

_|Srj_ _|_ _kX∈Srj_

f

for i [m], j [npriv]. After solving the min-cost flow (Figure 3), construct the
_∈_ _∈_
assignment matrix W ∈ R[m][×][n][priv] such that Wi,j = 1 if the edge from i to j has
flow, and 0 otherwise.

-  Step 4. Recover original image. From Step 3, we have the unweighted assignment
matrix W ∈{0, 1}[m][×][n][priv] . Before we recover the original image, we need to first
recover the weight of mixing, which is represented by the weighted assignment matrix
**U ∈** R[m][×][n][priv] . To recover weight, we first recover the label for each cluster group,
and use the recovered label and the mixed label to recover the weight.

**– First, we recover the label for each cluster, for all i** [npriv]. Let L denote the
_∈_
number of classes in the classification task of InstaHide application. For j ∈ [m],
let yj ∈ R[L] be the label of j.


label(i) =


supp(yj) [L].
_∈_
_j∈[m]\,Wj,i=1_


Then, for i ∈ [m] and j ∈ [npriv] such that Wi,j = 1, define Ui,j = yi,label(j) for
_| supp(yi)| = 2 and Ui,j = yi,label(j)/2 for | supp(yi)| = 1._
Here, W ∈{0, 1}[m][×][n][priv] is the unweighted assignment matrix and U ∈ R[m][×][n][priv]
is the weighted assignment matrix. For Wi,j = 0, let Ui,j = 0.

**– Second, for each pixel i ∈** [d], we run gradient descent to find the original
images. Let Y R[m][×][d] be the matrix of all InstaHide images, Y _,i denote the_
_∈_ _∗_
_i-th column of Y.[5]_

min
_z_ R[n][priv][ ∥|][U][z][| −|][Y][∗][,i][|∥][2][.]
_∈_

C Missing proofs for Theorem 3.6

For simplicity, let W denote Wpriv and M denote Mpriv in this section.

5The description of the attack in Carlini et al. (2020) recovers original images by using gradient
descent for minz∈R[n]priv ∥U|z| −|Y∗,i|∥2, which we believe is a typo.


-----

C.1 A graph problem (kpriv = 2)

**Theorem C.1 (Whitney (1992)). Suppose G and H are connected simple graphs and**
_L(G)_ =[∼] L(H). If G and H are not K3 and K1,3, then G =[∼] H. Furthermore, if |V (G)| ≥ 5,
_then an isomorphism of L(G) uniquely determines an isomorphism of G._

In other words, this theorem claims that given M = WW[⊤], if the underlying W is not the
incident matrix of K3 or K1,3, W can be uniquely identified up to permutation. Theorem C.1
can also be generalized to the case when G has multi-edges Zverovich (1997).

On the other hand, a series of work Roussopoulos (1973); Lehot (1974); Syslo (1982); Degiorgi
& Simon (1995); Liu et al. (2015) showed how to efficiently reconstruct the original graph
from its line graph:
**Theorem C.2 (Liu et al. (2015)). Given a graph L with m vertices and t edges, there exists**
_an algorithm that runs in time O(m + t) to decide whether L is a line graph and output the_
_original graph G. Furthermore, if L is promised to be the line graph of G, then there exists_
_an algorithm that outputs G in time O(m)._

With Theorem C.1 and Theorem C.2, Theorem 3.6 follows immediately:

_Proof of Theorem 3.6. First, since m = Ω(npriv log(npriv)), a well-known fact in random_
graph theory by Erdős and Rényi Erdős & Rényi (1960) showed that the graph G with
incidence matrix W will almost surely be connected. Then, we compute MG = M − **I, the**
adjacency matrix of the line graph L(G). Theorem C.1 implies that G can be uniquely
recovered from MG as long as npriv is large enough. Finally, We can reconstruct G from MG
by Theorem C.2.

For the time complexity of Algorithm 2, the reconstruction step can be done in O(m)
time. Since we need to output the matrix W, we will take O(mnpriv)-time to construct the
adjacency matrix of G. Here, we do not count the time for reading the whole matrix M into
memory.

C.2 General case (kpriv > 2)

The characterization of M and W as the line graph and incidence graph can be generalized
to kpriv > 2 case, which corresponds to hypergraphs.

Suppose M = WW[⊤] with kpriv = k > 2. Then, W can be recognized as the incidence
matrix of a k-uniform hypergraph G, i.e., each hyperedge contains k vertices. MG = M − **I**
corresponds to adjacency matrix of the line graph of hypergraphei, ej being two hyperedges. Now, we can see that each entry of M GG: ( is inMG {)i,j0 =, . . ., k |ei ∩}.ej| for

Unfortunately, the identification problem becomes very complicated for hypergraphs. Lovász
Lovász (1977) stated the problem of characterizing the line graphs of 3-uniform hypergraphs
and noted that Whitney’s isomorphism theorem (Theorem C.1) cannot be generalized to
hypergraphs. Hence, we may not be able to uniquely determine the underlying hypergraph
and we should just consider a more basic problem:
**Problem C.3 (Line graph recognition for hypergraph). Given a simple graph L = (V, E)**
_and k ∈_ N, decide if L is the line graph of a k-uniform hypergraph G.

Even for the recognition problem, it was proved to be NP-complete for fixed k ≥ 3 Levin &
Tyshkevich (1993); Poljak et al. (1981). However, Problem C.3 becomes tractable if we add
more constraints to the underlying hypergraph G. First, suppose G is a linear hypergraph,
i.e., the intersection of two hyperedges is at most one. If we further assume the minimum
degree of G is at least 10, i.e., each vertex are in at least 10 hyperedges, there exists a
polynomial-time algorithm for the decision problem. Similar result also holds for k > 3
Jacobson et al. (1997). Let the edge-degree of a hyperedge be the number of triangles in the
hypergraph containing that hyperedge. Jacobson et al. (1997) showed that assuming the
minimum edge-degree of G is at least 2k[2] _−_ 3k + 1, there exists a polynomial-time algorithm


-----

|npriv = 4 1 2 3 4 npriv = 4 T = 2 (1, 2) (1, 3) (2, 3) (2, 4) (3, 4) (3, 1) (4, 1) (4, 2)|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|cluster 1 init||cluster 2 init||cluster 3 init||cluster 4 init||cluster 5 init||cluster 6 init||cluster 7 init||cluster 8 init||
|(1, 2) (1, 3) (2, 3) (2, 4) (3, 4) (3, 1) (4, 1) (4, 2)||(1, 2) (1, 3) (2, 3) (2, 4) (3, 4) (3, 1) (4, 1) (4, 2)||(1, 2) (1, 3) (2, 3) (2, 4) (3, 4) (3, 1) (4, 1) (4, 2)||(1, 2) (1, 3) (2, 3) (2, 4) (3, 4) (3, 1) (4, 1) (4, 2)||(1, 2) (1, 3) (2, 3) (2, 4) (3, 4) (3, 1) (4, 1) (4, 2)||(1, 2) (1, 3) (2, 3) (2, 4) (3, 4) (3, 1) (4, 1) (4, 2)||(1, 2) (1, 3) (2, 3) (2, 4) (3, 4) (3, 1) (4, 1) (4, 2)||(1, 2) (1, 3) (2, 3) (2, 4) (3, 4) (3, 1) (4, 1) (4, 2)||
|||||||||||||||||
|cluster 1 grow on 1||cluster 2 grow on 2||cluster 3 grow on 4||cluster 4 grow on 1||cluster 5 grow on 3||cluster 6 grow on 4||cluster 7 grow on 3||cluster 8 grow on 2||
|(1, 2) (1, 3) (2, 3) (2, 4) (3, 4) (3, 1) (4, 1) (4, 2)||(1, 2) (1, 3) (2, 3) (2, 4) (3, 4) (3, 1) (4, 1) (4, 2)||(1, 2) (1, 3) (2, 3) (2, 4) (3, 4) (3, 1) (4, 1) (4, 2)||(1, 2) (1, 3) (2, 3) (2, 4) (3, 4) (3, 1) (4, 1) (4, 2)||(1, 2) (1, 3) (2, 3) (2, 4) (3, 4) (3, 1) (4, 1) (4, 2)||(1, 2) (1, 3) (2, 3) (2, 4) (3, 4) (3, 1) (4, 1) (4, 2)||(1, 2) (1, 3) (2, 3) (2, 4) (3, 4) (3, 1) (4, 1) (4, 2)||(1, 2) (1, 3) (2, 3) (2, 4) (3, 4) (3, 1) (4, 1) (4, 2)||
|C1 C2 C3 C4 share 1 share 2 share 4 share 3 (1, 2) (1, 3) (1, 2) (1, 3) (1, 2) (1, 3) (1, 2) (1, 3) (2, 3) (2, 4) (2, 3) (2, 4) (2, 3) (2, 4) (2, 3) (2, 4) (3, 4) (3, 1) (3, 4) (3, 1) (3, 4) (3, 1) (3, 4) (3, 1) (4, 1) (4, 2) (4, 1) (4, 2) (4, 1) (4, 2) (4, 1) (4, 2)||||||||||||||||


Figure 1: An example about cluster step in Carlini et al. (2020) for T = 2 and npriv = 4.
First, starting from each InstaHide image (top), the algorithm grows cluster Si with size 3
(middle). Then, we use K-means for K = 4 to compute 4 groups C1, . . ., C4 (bottom), these
groups each correspond to one original image.

to decide whether L is the line graph of a linear k-uniform hypergraph. Furthermore, in the
yes case, the algorithm can also reconstruct the underlying hypergraph. We also note that
without any constraint on minimum degree or edge-degree, the complexity of recognizing
line graphs of k-uniform linear hypergraphs is still unknown.


-----

InstaHide images

_T · npriv_


original images

_npriv_


_c = 1, w =_ **Wi,j**


source


terminal


(1, 2)

[f] (1, 2) (1, 3)

_C1_

(4, 1) (3, 1)

_c = 2, w = 0_ (2, 3) _c = 2T_, w = 0

(3, 4)

(1, 2) (2, 4)

_C2_

(2, 3) (4, 2)

(4, 1)

(1, 3)

(3, 4) (2, 4)

_C3_

(4, 1) (4, 2)

(2, 4)

(3, 1)

(2, 3) (1, 3)

_C4_

(3, 4) (3, 1)

(4, 2)


Figure 2: The construction of the graph for min-cost max flow. c denotes the flow capacity of the
edge, and w denote the weight of the edge. The graph contains T · npriv nodes for each InstaHide
images, npriv nodes for each original images, a source and a terminal. There are three types of edges:
i) (left) from the source to each InstaHide image node, with flow capacity 2 and weight 0; ii) (middle)
from each InstaHide image node i to each original image node j, with flow capacity 1 and weight
**Wi,j; iii) (right) from each original image node to the terminal, with flow capacity 2T and weight 0.**


InstaHide images

_T · npriv_


original images

_npriv_


_c = 1, w =_ **Wi,j**


source


terminal


(1, 2)

[f] (1, 2) (1, 3)

_C1_

(4, 1) (3, 1)

_c = 2, w = 0_ (2, 3) _c = 2T_, w = 0

(3, 4)

(1, 2) (2, 4)

_C2_

(2, 3) (4, 2)

(4, 1)

(1, 3)

(3, 4) (2, 4)

_C3_

(4, 1) (4, 2)

(2, 4)

(3, 1)

(2, 3) (1, 3)

_C4_

(3, 4) (3, 1)

(4, 2)


Figure 3: The result of solving the min-cost flow in Figure 2. Each InstaHide image is
assigned to two clusters, which ideally correspond to two original images. In reality, a cluster
may not contain all InstaHide images that share the same original image.

D Computational Lower Bound

The goal of this section is to prove that the ℓ2-regression with hidden signs is actually a
very hard problem, even for approximation (Theorem D.4), which implies that Algorithm 3
cannot be significantly improved. For simplicity we consider Spub = ∅.

We first state an NP-hardness of approximation result for 3-regular MAX-CUT.

**Theorem D.1 (Imapproximability of 3-regular MAX-CUT, Berman & Karpinski (1999)).**
_For every ϵ > 0, it is NP-hard to approximate 3-regular MAX-CUT within a factor of r + ϵ,_
_where r ≈_ 0.997.


-----

If we assume the Exponential Time Hypothesis (ETH), which a plausible assumption in
theoretical computer science, we can get stronger lower bound for MAX-CUT.
**Definition D.2 (Exponential Time Hypothesis (ETH), Impagliazzo & Paturi (2001)). There**
_exists a constant ϵ > 0 such that the time complexity of n-variable 3SAT is at least 2[ϵn]._
**Theorem D.3 (Fotakis et al. (2016)). Assuming ETH, there exists a constant 0 < r[′]** _< 1_
_such that no 2[o][(][n][)]-time algorithm can r[′]-approximate the MaxCut of an n-vertex, 5-regular_
_graph._

With Theorem D.1 and Theorem D.3, we can prove the following inapproximability result
for the ℓ2-regression problem with hidden signs.
**Theorem D.4 (Lower bound of ℓ2-regression with hidden signs). There exists a constant**
_ϵ > 0 such that it is NP-hard to (1 + ϵ)-approximate_

min (1)
_z_ R[n][ ∥|][Wz][| −] _[y][∥][2][,]_
_∈_

_where W ∈{0, 1}[m][×][n]_ _is row 2-sparse and y ∈{0, 1}[m]._

_Furthermore, assuming ETH, there exists a constant ϵ[′]_ _such that no 2[o][(][n][)]-time algorithm can_
_ϵ[′]-approximate Eq. (1)._

_Proof. Given a 3-regular MAX-CUT instance G, we construct an ℓ2-regression instance (W, y)_
with W ∈{0, 1}[m][′][×][n] and y ∈{0, 1}[m][′] where m[′] = m + cn = (1 + 3c/2)m and c = 10[6] as
follows.

-  For each i [m], let the i-th edge of G be ei = _u, v_ . We set Wi, to be all zeros
_∈_ _{_ _}_ _∗_
except the u-th and v-th coordinates being one. That is, we add a constraint _zu_ +zv .
_|_ _|_
And we set yi = 0.

-  For each j ∈ [n], we set Wm+c(j−1)+1,∗, . . ., Wm+cj,∗ to be all zero vectors except
the j-th entry being one. That is, we add c constraints of the form _zj_ . And
_|_ _|_
_ym+c(j_ 1)+1 = = ym+cj = 1.
_−_ _· · ·_

**Completeness.** Let opt be the optimal value of max-cut of G and let Sopt be the optimal
subset. Then, for each u ∈ _Sopt, we set zu = 1; and for u /∈_ _Sopt, we set zu = −1. For the_
first type constraints |zu + zv|, if u and v are cut by Sopt, then |zu + zv| = 0; otherwise
_|zu +_ _zv| = 2. For the second type constraints |zj|, all of them are satisfied by our assignment._
Thus, ∥Wz − _y∥2[2]_ [= 4(][m][ −] [opt][)][.]

**Soundness.** Let η be a constant such that r < η < 1, where r is the approximation lower
bound in Theorem D.1. Let δ = 110−cη [. We will show that, if there exits a][ z][ such that]

_Wz_ _y_ 2
_∥_ _−_ _∥[2]_ _[≤]_ _[δm][′][, then we can recover a subset][ S][ with cut-size][ ηm][.]_

It is easy to see that the optimal solution lies in [−1, 1][n]. Since for z /∈ [−1, 1][n], we can
always transform it to a new vector z[′] [ 1, 1][n] such that _Wz[′]_ _y_ 2 _Wz_ _y_ 2.
_∈_ _−_ _∥_ _−_ _∥_ _≤∥_ _−_ _∥_

Suppose z ∈{−1, 1}[n] is a Boolean vector. Then, we can pick S = {i ∈ [n] : zi = 1}. We
have the cut-size of S is

_|E(S, V \S)| ≥_ _m −_ _δm[′]/4_
= m − _δ(1 + 3c/2)m/4_
= (1 − _δ/4 −_ 3cδ/8)m
_≥_ _ηm,_

where the last step follows from δ ≤ [8(1]2+6[−][η]c[)] [.]

For general z ∈ [−1, 1][n], we first round z by its sign: let zi = sign(zi) for i ∈ [n]. We will
show that

_Wz_ _y_ 2 2
_∥_ _−_ _∥[2]_ _[−∥][Wz][ −]_ _[y][∥][2]_ _[≤]_ [48]c [m]


-----

which implies


_∥Wz −_ _y∥2[2]_ [=][ ∥][Wz][ −] _[y][∥]2[2]_ [+ (][∥][Wz][ −] _[y][∥]2[2]_ _[−∥][Wz][ −]_ _[y][∥]2[2][)]_

_δm[′]_ + [48]
_≤_ _c [m.]_

Then, we have the cut-size of S is

_|E(S, V \S)| ≥_ _m −_ (δm[′] _−_ 48m/c)/4
= (1 − _δ/4 −_ 3cδ/8 − 12/c)m
_≥_ _ηm,_


where the last step follows from δ 2+6c
_≤_ [8(1][−][η][−][12][/c][)]

Let ∆i := |zi − _zi| = 1 −|zi| ∈_ [0, 1]. We have


(zui + zvi )[2] _−_ (zui + zvi )[2] + c ·
_i=1_

X

_m_

(zui + zvi )[2] _−_ (zui + zvi )[2] _−_ _c ·_
_i=1_

X

_m_

(zui + zvi )[2] _−_ (zui + zvi )[2] _−_ _c ·_
_i=1_

X


( _zj_ 1)[2] ( _zj_ 1)[2]
_|_ _| −_ _−_ _|_ _| −_
_j=1_

X

_n_

( _zj_ 1)[2]
_|_ _| −_
_j=1_

X

_n_

∆[2]j
_j=1_

X


_∥Wz −_ _y∥2[2]_ _[−∥][Wz][ −]_ _[y][∥]2[2]_ [=]


∆[2]j
_j=1_

X


4|∆ui + ∆uj _| −_ _c ·_
_i=1_

X

_n_

12∆i _c∆[2]i_
_i=1_ _−_

X


_≤_ [72]c [n]

= [48]

_c [m,]_

where the first step follows by the construction of W and y. The second step follows from
_zj_ = 1 for all j [n]. The third step follows from the definition of ∆j. The forth step
_|_ _|_ _∈_
follows from |zui + zuj _| ∈_ [0, 2]. The fifth step follows from the degree of the graph is 3. The
fifth step follows from the minimum of the quadratic function 12x − _cx[2]_ in [0, 1] is [72]c [. The]

last step follows from m = 3n/2.

Therefore, by the completeness and soundness of reduction, if we take ϵ ∈ (0, δ), Theorem D.1
implies that it is NP-hard to (1 + ϵ)-approximate the ℓ2-regression, which completes the
proof of the first part of the theorem.

For the furthermore part, we can use the same reduction for a 5-regular graph. By choosing
proper parameters (c and δ), we can use Theorem D.3 to rule out 2[o][(][n][)]-time algorithm for
_O(1)-factor approximation. We omit the details since they are almost the same as the first_
part.


-----

