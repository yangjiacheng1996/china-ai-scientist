# UNRAVELING MODEL-AGNOSTIC META-LEARNING
## VIA THE ADAPTATION LEARNING RATE

**Yingtian Zou, Fusheng Liu, Qianxiao Li**
National University of Singapore, Singapore
_{yingtian, fusheng}@u.nus.edu, qianxiao@nus.edu.sg_

ABSTRACT

Model-Agnostic Meta-Learning (MAML) aims to find initial weights that allow
fast adaptation to new tasks. The adaptation (inner loop) learning rate in MAML
plays a central role in enabling such fast adaptation. However, how to choose
this value in practice and how this choice affects the adaptation error remains
less explored. In this paper, we study the effect of the adaptation learning rate in
meta-learning with mixed linear regression. First, we present a principled way
to estimate optimal adaptation learning rates that minimize the population risk
of MAML. Second, we interpret the underlying dependence between the optimal
adaptation learning rate and the input data. Finally, we prove that compared with
empirical risk minimization (ERM), MAML produces an initialization with a
smaller average distance to the task optima, consistent with previous practical
findings. These results are corroborated with numerical experiments.

1 INTRODUCTION

_Meta-learning or learning to learn provides a paradigm where a machine learning model aims to_
find a general solution that can be quickly adapted to new tasks. Due to its fast adaptability, metalearning has been widely applied to challenging tasks such as few-shot learning (Vinyals et al., 2016;
Snell et al., 2017; Rusu et al., 2018), continual learning (Finn et al., 2019; Javed & White, 2019),
and neural architecture search (Zhang et al., 2019; Lian et al., 2019). One promising approach in
meta-learning is Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017), which consists of two
loops of optimization. In the outer loop, MAML aims to learn a good meta-initialization that can
be quickly adapted to new task in the inner loop with limited adaptation (parameter optimization)
steps. The double loops optimization serve as “learning-to-adapt” process, thus enabling the trained
model to adapt to new tasks faster than direct Empirical Risk Minimization (ERM) algorithms (Finn
et al., 2017; Raghu et al., 2020). Recent works (Nichol et al., 2018; Fallah et al., 2020; Collins
et al., 2020; Raghu et al., 2020) attribute the fast adaptability to the phenomenon that the learned
meta-initialization lies in the vicinity of all task solutions. However, the theoretical justification of this
empirical statement, and more generally how fast adaptability of MAML depends on the inner loop
optimization remains unclear. As a key component of MAML, the adaptation (inner loop) learning
rate (hereafter called α) is shown empirically to plays a crucial role in determining the performance
of the learned meta-initialization (Rajeswaran et al., 2019). In particular, the value of α bridges
ERM and MAML, in the sense that the latter reduces to the former when α = 0. However, from a
theoretical viewpoint, the dependence of MAML performance on the choice of α remains unclear,
and furthermore, there lacks a precise practical guideline on how to pick a near-optimal value.

In this paper, we address these issues by answering the following two questions: (1) How to choose
_the optimal α that minimizes population risk of MAML? (2)What is the effect of α on fast adaptability_
_of MAML? To this end, we consider the mixed linear regression problem with random feature models._
For the first question, we derive the optimal α which minimizes the population risk of MAML in the
limit of an infinite number of tasks. This can then be used to estimate an effective α prior to training.
Moreover, we analyze the underlying statistical dependency between the optimal α and the input
data, e.g. relation to the moments of data distribution. This in turn allows the heuristic application of
our results beyond linear models, and we demonstrate this with experiments. To answer the second
question, we compare MAML with an ERM algorithm (without inner loop optimization) in order
to reflect the effect of α in optimization. As stated in many works, like Nichol et al. (2018), that


-----

meta-initialization learned by MAML in parameter space is close to all training tasks thus contributes
to fast adaptability. We conduct an experiment and observe that MAML with a not too large α yields
a shorter mean distance to task optima than ERM. To justify this empirical finding, we define a
metric measuring the expected geometric distance between the learned meta-initialization and task
optima. We prove that in our setting, the MAML solution indeed possess a smaller value of this
metric compared with that of ERM for small α, providing theoretical evidence for the observed
phenomena. Our contributions can be summarized as follows:

-  We provide a principled way to select the optimal adaptation learning rate α[∗] for MAML
which minimizes population risk (Theorem 1 & Proposition 1). We also interpret the
underlying statistical dependence of α[∗] to input data (Corollary 1) with two examples.

-  We validate the observation that MAML learns a good meta-initialization in the vicinity of
the task optima, which reveals the connection between the adaptation learning rate α and
the fast adaptability in optimization. (Theorem 2)

-  We also extend our result about the choice of α[∗] to more practical regime, including deep
learning. All of our theoretical results are well corroborated with experimental results.

2 PROBLEM FORMULATION

We study the MAML algorithm under the mixed linear regression setting. Suppose we have a task T
that is sampled from the distribution D(T ). Each task T corresponds to a linear relationship

**_xT,1_**

**_yT = Φ(XT )aT, XT =_** **_x· · ·T,K_** ! _, XT ∈_ R[K][×][d][x] _, Φ(XT ) ∈_ R[K][×][d], aT ∈ R[d].

(2.1)
where XT ∈ R[K][×][d][x] is the input data of task T which has K vector samples {xT,1, ..., xT,K}, xT,j ∈
R[d][x] i.i.d sampled from D(x) [1]. For each input data, we have a mapping ϕ : R[d][x] _→_ R[d] transform
each point of XT from input data space R[d][x] to a d-dimensional feature space R[d] where we denote
the transformation of all data in task T by Φ(XT ) = [ϕ(XT,1), ..., ϕ(XT,K)][⊤] as the feature of
that task. Then, we assume optimal solutioncorresponding label yT ∈ R[K] can be obtained from ( aT ∈ R2.1[d] for task). _T is i.i.d sampled from D(a). The_

Our target is to learn a model to minimize the risk of different tasks across D(T ). Note that each task
_T is determined by a feature-solution pair (Φ(XT ), aT ). Therefore, we can formulate this multi-task_
problem with parameter space R[d] and loss function ℓ as

min (2.2)
**_w_** R[d][ E][T][ ∼D][(][T][ )][ [][ℓ] [(][w][;][ T] [)] = min]w R[d][ E][a][∼D][(][a][)][E][X][∼D][(][x][)][ [][ℓ] [(][w][; Φ(][X][)][,][ a][)]]
_∈_ _∈_

To solve this problem, ERM and MAML algorithms yield different iterations. Specifically, ERM uses
all data from all tasks to directly minimize the square error loss ℓ, such that population risk of ERM is


(2.3)


_r(w, K) :=_ E E (2.3)
_L_ **_a∼D(a)_** _X∼D(x)_ _K_ 2

As a counterpart, MAML first adapts with an adaptation learning rate α on each task using its training

[Φ(][X][)][w][ −] [Φ(][X][)][a]

set – a subset of task data in the inner loop. Then, in the outer loop, MAML minimizes the evaluation
loss for each adapted task-specific solution using a validation set. For simplicity, since data is i.i.d
sampled from the same distribution, we first consider the setting where all data in each task is used as
training set and validation set in our main results. We present later the extension of these results to
the case with a different train-validation split. (Please refer to Appendix H.1)


_Lr(w, K) :=_ **_a∼DE(a)_** _X∼DE_ (x)


Thus, the general population risk of one-step MAML is defined by


1
_m(w, α, K) :=_ E E _ℓ_ **_w_** _α_ **_wℓ_** (w; Φ(X), a); Φ(X), a (2.4)
_L_ **_a∼D(a)_** _X∼D(x)_ _K_   _−_ _∇_ Inner Loop 

1For simplicity, we denote this sampling and stacking multiple examples to a matrix process as| {z } _X ∼D(x)_


-----

In practice, we use the empirical objective function as a surrogate objective function. We first sample
_N tasks with task optima_ **_a1, ..., aN_** from (a) and then sample K data for each task. Then, the
_{_ _}_ _D_
empirical risk of MAML can be specified as [ˆ]m
_L_

_N_ 2

1
_Lˆm(w, α, N, K) :=_ _NK_ _i=1_ _i_ _[−]_ [Φ(][X][i][)][a][i] 2 (2.5)

X

where wi[′] [=] **_w −_** 2αΦ(Xi)[⊤] (Φ(Xi)w − Φ(Xi)ai) /K[Φ(][X][i]is adapted parameters of task[)][w][′] _i after inner_
loop. Correspondingly, we apply ERM algorithm to the same problem by removing inner loop
 
(setting α = 0), thus the empirical risk of ERM is denoted as [ˆ]r(w, N, K). In addition, we follow
_L_
the original MAML (Finn et al., 2017) to use the same α for training and testing.

**Notation** We denote an optimal adaptation learning rate as α[∗]. Global minima of empirical risk of
MAML and ERM (when they are unique) are denoted by wm, wr. We write 1, ..., N as [N ] and
_{_ _}_
use ∥· ∥ to denote the Euclidean norm. We use subscripts to index the matrices/vectors corresponding
to task instances, and bracketed subscripts to index the entries of matrices. Other notations are
summarized in Appendix Table 1.
**Assumption 1 (Normalization). For simplicity, we consider a centered parameter space such that**
Ea (a)[a] = 0 and Var[a] = σa[2][.]
_∼D_

**Assumption 2 (Bounded features). With probability 1, the covariance matrix of input features**
Φ(X)[⊤]Φ(X) is positive definite and has uniformly bounded eigenvalues from above by λS > 0 and
_below by λI > 0._

3 MAIN RESULTS

In this section, we analyze MAML through the adaptation learning rate α. Our derived insights are
summarized into three theoretical results: (1) The estimation of an optimal adaptation learning rate
_α[∗]_ which minimizes MAML population risk; (2) The statistical meaning of α[∗] in terms of the data
distribution, and (3) The geometric interpretation of the effect of α on fast adaptability of MAML
compared to ERM.

3.1 ON THE OPTIMAL ADAPTATION LEARNING RATE α[∗]

We focus on the underparameterized case (K _d). Given the empirical objective functions_ [ˆ]r, [ˆ]m
_≥_ _L_ _L_
defined in (2.5), we can derive the global minima by the first-order optimality condition. We obtain
the global minimum of ERM wr and minimum of MAML wm in the following closed-forms,

_−1_

**_wr = wr_** Φ(Xi), ai _i_ [N ] = Φ(Xi)[⊤]Φ(Xi) Φ(Xj)[⊤]Φ(Xj)aj
_{_ _}_ _∈_

_i_ [N ]  _j_ [N ] 

    X∈  X−1∈ [(3.1)]

**_wm(α) = wm_** _{Ci(α), ai}i∈[N_ ] = _Ci(α)[⊤]Ci(α)_ _Cj(α)[⊤]Cj(α)ai_

_i_ [N ]  _j_ [N ] 

    X∈  X∈

where Ci(α) = Φ(Xi) _I −_ (2α/K)Φ(Xi)[⊤]Φ(Xi) _, Ci(α) ∈_ R[K][×][d] can be viewed as the adapted
feature of task i. Observe that wm(α) (and thus the MAML algorithm) depends on α. If α = 0,
 
MAML reduces to ERM. For large α, instabilities may occur, thus there may exist an optimum, α[∗]
that minimizes the MAML population risk. The later intuition is worthwhile to be proved, from
which we do not have a principled way to guide the choice of optimal hyperparameter α[∗] for MAML
so far. To this end, we focus on the generalization error by taking the population risk on the global
minimum of empirical risk. In particular, we consider the population risk of the MAML optimizer in
the average sense, where the average population risk is

_L¯m(α, N, K) = EwmLm(wm, α, K)_ (3.2)

whose minimizer we denote as α[∗](N, K). In this way, we eliminate randomness of the global
minimum wm learned from sampled tasks. The following theorem gives a precise value of α[∗](N, K)
in the limit N →∞.


-----

**Theorem 1. Under assumptions 1 & 2, we have as N →∞, α[∗](N, K) →** _αlim[∗]_ [(][K][)][, where]

_αlim[∗]_ [(][K][) =][ K][ tr[][E][X] [[(Φ(][X][)][⊤][Φ(][X][))][2][]]] (3.3)

2 tr[EX [(Φ(X)[⊤]Φ(X))[3]]] _[,]_

Φ(X) ∈ R[K][×][d], K is the sample size per task and N is the number of tasks.

The proof is found in Appendix B. In this theorem, we give the nearly optimum αlim[∗] [which is an]
alternative form for true optimal α, namely α[∗], to minimize the MAML generalization error. As
dictated in (3.3), the desired α[∗] is determined by the feature covariance matrix in expectation.
**Remark. The precise derivation of the case where N is finite is complicated, thus we derive the**
_limiting case here as an estimator of true α[∗]. Our estimation αlim[∗]_ _[is the unique minimum. We will]_
_show later that this allows us to compute near optimal values efficiently in practice, each of which is_
_close to the optimal α[∗](N, K) in corresponding problem._
**Remark. The estimator (3.3) can be generalized to different scenarios. For overparameterized**
_models, we obtain a similar result for the minimum norm solution if the number of tasks N is limited_
_(NK ≪_ _d). Further, we show a computationally efficient estimator (H.15) in Appendix H.2. For deep_
_learning, we can compute a range of effective α values based on αlim[∗]_ _[. We also give the numerical]_
_form when the training data is different from the test data in each task. These are presented in_
_Appendix H.4 and H.1 respectively._

In the above we considered the average population risk (3.2). This simplifies the calculations of
finding the α[∗]. Below, we justify this simplification by showing that in the limit of large number of
tasks, the average population risk is a good estimate of the true population risk.
**Proposition 1 (Informal). Assume u = C(α)[⊤]C(α)a is sub-gaussian random variable with sub-**
_gaussian norm_ **_u(i)_** Ψ2 _L, assumption 1 & 2 hold, then with probability at least 1_ _δ that_
_∥_ _∥_ _≤_ _−_

_dε(α, K)_

_Lm(α, N, K)_ _[≤]_ _[L]K[2]_ [max] (r _N_ [2] log [2]δ [, ε][(][α, K]N [)] log [2]δ ) (3.4)

_where[L] ε[m](α, K[(][w][m]) =[, α, K](1[)][ −]/(c[¯]0 + α)[2]). Here c0 > 0 is a constant and d is the feature size._
_O_

The proof is found in Appendix C. Proposition 1 complements Theorem 1 by guaranteeing that the
gap between the average population risk and population risk with same argument α will disappear
along with N goes to infinity. Large α makes the bound tighter while small α makes ε(α, K)
converge to a positive constant; thus (3.4) provides a non-vacuous bound with regard to α. Hence, it
is justified to make an estimation of α[∗] using the average case. By Theorem 1 and Proposition 1, we
give an explicit form to estimate α[∗] for MAML where this estimation αlim[∗] [is not too far from the]
true α[∗] of a specific case. Later experiments show our estimation αlim[∗] [is close to true][ α][∗] [in both]
underparameterized and overparameterized models (see Section 5.1). This is meaningful for selecting
an α[∗] minimizing MAML risk, instead of randomly choosing it. Previous work (Bernacchia, 2021)
explores on this by giving a range of α[∗] may exist for the linear model. Instead, we show a fine result
that we provide a certain value estimator of α[∗]. (Details refer to Appendix H.5)

**Relation to data distribution.** After estimating the value of α[∗] through Theorem 1, we are now
interested in the statistical interpretation of α[∗]. In particular, we aim to summarize the dependence
of an estimation of a[∗] on the distribution of the inputs and tasks. This in turn allows us to devise
strategies for choosing near optimal α for MAML beyond the simple settings considered here.
**Corollary 1. With a feature mapping ϕ : R[d][x]** _→_ R[d] _for each data x ∈_ R[d][x] _, the αlim[∗]_ _[in Theorem][ 1]_

_will satisfy the following inequality_


1

2dσ[2](ϕ(x1), . . ., ϕ(xK)) _[≤]_ _[α]lim[∗]_ _[≤]_

_where σ[2](ϕ(x1), . . ., ϕ(xK)) is variance of the feature._


_d_

(3.5)
2σ[2](ϕ(x1), . . ., ϕ(xK))


See proof in Appendix D. According to Corollary 1, we can see that αlim[∗] [is bounded by the statistics]
of the input data. These bounds are governed by the standard derivation terms. More specifically, our
estimator (3.3) holds an inverse relationship to higher order moment of data distribution while its


-----

(a) Visualization of solutions and trajectory (b) Mean solution distances

Figure 1: (a) Visualization of trajectory of MAML solution wm(α). Orange dots are task optima
_{ai}[N_ ] of sampled tasks, where location of ai is decided by its entries. Red dots highlighted in red
circle are newly coming tasks. Green cross is wr, (α = 0) while the purple trajectory is generated as
_α increasing. Red star is wm(αlim[∗]_ [)][. (b) Average euclidean distances of][ w][m][(][α][)][ and][ {][a][i][}][[][N] []][ display]
corresponding points in left figure. Black arrow is the tangent line. Best viewed in colors.

bounds (3.5) have an inverse relationship to data variance. As a consequence, the αlim[∗] [for different]
problems mainly depend on the standard derivations. For example, αlim[∗] [and thus][ α][∗] [will shrink to]
zero as the variance of data increases and vice versa. In other words, small α is tailored to those
tasks with large data variance when the model size is fixed. To illustrate the insight more clearly,
we present two examples – regression with polynomial basis functions (Example 1) and the case
where Φ(X) is a random matrix with a prescribed distribution (see Appendix E). In this following
example, we narrow the range and get the exact relationship where the expression of αlim[∗] [rather]
than its bounds depend on data variance and model size d. In later experiments, we also validate this
relationship on various models with different basis functions.
**Example 1for each task. Consider polynomial basis function (Polynomial basis function). Assume we have ϕ : R → KR i.i.d samples[d], where ϕ(x) = (1 x1, ..., x, ..., xK ∼N[d][−][1])(0. Then, σ[2])**
_value of αlim[∗]_ _[has an inverse relationship to][ σ][2][ and dimension][ d][ (Proof is in Appendix][ E][).]_

3.2 GEOMETRIC INTERPRETATION OF MAML ADAPTATION

In another direction, we aim to investigate geometric properties of the meta-initialization learned by
MAML as α varies. In previous experimental investigations, it is suggested that MAML learns the
near meta-initialization to all tasks Nichol et al. (2018) or trade-offs on easy and hard tasks Fallah
et al. (2020). We can also observe the new phenomena in toy experiments. As shown in the Figure

1 (a), we sampled 500 tasks in R[2] parameter space. Specifically, we i.i.d sample and stack data as
_XGreen cross shows the location of MAML solutioni ∈_ R[K][×][2], ∼D(x) and task optima ai ∼D(a), a wi ∈m(αR)[2] with(scattered orange dots) for each task α = 0, namely ERM solution w ir..
Since D(x), D(a) are some symmetric zero-mean distributions, the optimal solution is expected at
the origin. When several new training tasks (with higher penalties) have been added as shown in the
red circle area, then new wr will be closer to new tasks. Along α increasing, wm(α) generates a
trajectory shown as the purple curve. The dynamics of global minimum wm(α) will start from green
cross and move away from the red circle until reach an optimum location of point (red star wm(αlim[∗] [)][)]
which minimizes the total distances to red and orange dots (Other cases shown in Appendix H.4).

It indicates that the effect of α (inner loop optimization) is to help MAML minimize total distances
to all training task optima. Unlike ERM learning a biased solution to dense tasks area, MAML
converges to a distance-aware solution that tries to minimize the distances within one-step adaptation
at stepsize α. The αlim[∗] [is the optimum adaptation stepsize to learn the optimum location of point, or]
nearest point, to all tasks. Figure 1 (b) displays the mean distance for each point in purple trajectory to
all tasks. As we can see, the distance decreases at beginning as α increases until reach the minimum.

To theoretically prove the insight in Figure 1, we characterize it by measuring the average postadaptation distance between the meta-initialization (global minimum) learned by a specific algorithm
and task optima in a task distribution.


-----

**Definition 1 (Average Distance under Fast Adaptation). Given task distribution D(T** ), meta_initialization w[0]_ _[learned by algorithm][ A][, optimum][ a][T][ of task][ T]_ _[, the average distance under]_
_A_
_t-step (t ≥_ 0) fast adaptation is defined by

_t_

_t(w[0]_ E _,T_ **_w[t]_** _,T_ [=][ w][0] **_wℓ(w[j]_** _,T_ _[, T]_ [)] (3.6)
_F_ _A[) :=]_ _T ∼D(T )_ _[∥][w]A[t]_ _[−]_ **_[a][T]_** _[∥][2][,]_ _A_ _A_ _[−]_ _[η]_ Xj=1 _∇_ _A_

_where w[t]_ _,T_ _[is the adapted parameter of task][ T][ with][ t][ steps,][ η][ is the step size,][ ℓ]_ _[is the loss function.]_
_A_

_Ft evaluates the distance between adapted parameters and true task optimum for a given meta-_
initialization at any adaptation step t. If t is small, Ft describes the fast adaptation error in solution
distance of the meta-initialization learned by an algorithm. Hence, we can measure the fast adaptability
of MAML with Ft(wm). Observe that for small α, wm can be linearized as
**_wm(α) = wr + α∇αwm(0) + O(α[2])_** (3.7)
In this regime, the effect of MAML is dictated by the α gradient _αwm(0), which can be visualized_
_∇_
as the tangent of the purple curve at the green cross in Figure 1(b). By comparing Ft of the metainitializations wr, wm learned by ERM and MAML, we are able to find the connection between
_α and the fast adaptability in meta-learning, at least in the small alpha regime. For simplicity, we_
assume that the input data features are uncorrelated, thus the covariance matrix is diagonal.
**Theorem 2. Let wm(α), wr be the meta-initializations learned from T1, ..., TN by MAML and ERM.**

2λ[4]S _[K][+][K][√]4λ[8]S_ [+1][.][5][e]cλ[4]I [(4][λ][6]S _[−][λ][6]I_ [)][/λ][3]S
_With Ft(·), under Assumption 1 & 2, for any α ∈_ 0, _−_ _λ[2]I_ [(4][λ][6]S _[−][λ][6]I_ [)] _at number_
 

_of step t, we have_

2t 2
4αd _c_

ET1,...,TN (T ) [ _t(wr)_ _t(wm(α))]_ 1 (3.8)
_∼D_ _F_ _−F_ _≥_ _−_ [2]K [η] _[λ][S]_ _NKλ[3]S_
 

_where η is the step size in Definition 1,_ _c > 0 is a constant._ e

See proof in Appendix G. This theorem prove our insight at small α that MAML has smaller average

e

solution distance than ERM. As it illustrated in Theorem 2, at any step t 0, _t(wm(α))_ _t(wr)_
_≥_ _F_ _≤F_
holds if α is smaller than some constant. This means adapting to different tasks with MAML metainitialization leads to shorter average solution distance than ERM’s at any number of adaptation steps.
But the gap will disappear along number of steps t increasing to infinity, which is sensible. Note
that even t = 0, this inequality still holds true. Therefore the meta-initialization of MAML wm
has shorter expected distance to new task than ERM wr before adaptation. Theorem 2 has revealed
the connection between α and fast adaptability. Even with small α, MAML learns a more adaptive
solution than ERM which is closer to the new tasks in expectation enabling quick approximation.
It benefits from learning a closer meta-initialization for all tasks on average. Thus α plays a role in
learning a distance-aware solution. This result is consistent with our observation in the Figure 1.

Compared to ERM algorithms, the fast adaptability of MAML stems from the learned metainitialization determined by the adaptation learning rate α. When facing a multi-task problem,
traditional ERM algorithms bias its learned initialization to minimize the averaged risk. However, this
strategy fails to take the further adaptation into account, and thus learns a solution far from unknown
task optima. On the contrary, MAML learns a distant-aware meta-initialization and converges to the
vicinity of all task optima with a limited adaptation budget (Nichol et al., 2018; Rajeswaran et al.,
2019), or tends to favor “hard tasks” (Fallah et al., 2020; Collins et al., 2020). Hence, before adaptation, ERM may have lower population risk than MAML. However, after adaptation, the situation
will reverse since MAML can adapt to most unknown task optima closer (see Figure 5(a)). This
benefit is also illustrated by (Zhou et al., 2020) that the shorter solution distance leads to a better
meta-initialization for fast adaptation. We note that “task hardness” may not always be easy to define,
especially for non-linear cases (Collins et al., 2020). Here, we instead focus on directly analyzing the
geometric distance (Theorem 2), which has substantiated the aforementioned findings in optimization
behavior from different angles.

4 RELATED WORK

Meta learning learns a general solution based on previous experience which can be quickly adapted
to unknown tasks (Finn et al., 2017; Li et al., 2017; Snell et al., 2017; Vinyals et al., 2016; Nichol


-----

60

50

40

30

20

10


-3 -2 -1 0 1 2 3 4 5 6 7 (×10 4)

|* lim K=15,N=50 K=19,N=85|* lim K=15,N=50 K=19,N=85|
|---|---|
|||
|||


lim

K=15,N=50
K=19,N=85

(b) NTK (Normal initialization)


35 - 
lim

30 K=23,N=68

25 K=7,N=112

20

15

10

5

0

-3 -2 -1 0 1 2 3 4 5 6 7 (×10 4)


(a) NTK (Uniform initialization)


Figure 2: Loss of overparameterized quadratic regression with regard to α. Triangles in the dash-dot
line is the mean loss across whole tasks. The error bar denotes 95% confidence interval on different
tasks. Red stars are estimations αlim[∗] [.]

et al., 2018; Grant et al., 2018; Harrison et al., 2018; Rusu et al., 2018; Rajeswaran et al., 2019;
Finn & Levine, 2018; Rajeswaran et al., 2019; Finn et al., 2018; Yin et al., 2020). One promising
approach to meta-learning is Model-Agnostic Meta-Learning (MAML) (Finn et al., 2017) which
learns a meta-initialization such that the model can adapt to a new task via only few steps gradient
descent. Understanding the fast adaptability of meta-learning algorithm, especially on MAML, is
now an important question. As a variant of MAML, (Nichol et al., 2018) attribute fast adaptation
to the shorter solution distance, and devises a first-order approximation algorithm based on this
intuition. Other first-order methods (Denevi et al., 2019; Zhou et al., 2019) try to achieve adaptation
by adding a regularized term to get a distance-aware solution. (Raghu et al., 2020) shows that
even performing inner loop optimization on part of the parameters still leads to fast adaptation. A
shared empirical finding of these results is that MAML produces initial weights that are closer to the
population optimum of individual tasks on average, and it is argued that this partly contributes to its
fast adaptibility. Here, we present a rigorous result that confirms the distance reduction property of
MAML, at least in the considered setting, lending theoretical backing to these empirical observations.

On the theoretical front, analyses of meta-learning mainly focus on generalization error bounds and
convergence rates (Amit & Meir, 2018; Denevi et al., 2019; Finn et al., 2019; Balcan et al., 2019;
Khodak et al., 2019; Zhou et al., 2019; Fallah et al., 2020; Ji et al., 2020b; Zhou et al., 2020; Ji et al.,
2020a). For example, Fallah et al. (2020) studies MAML by recasting it as SGD on a modified loss
function and bound the convergence rate using the batch size and smoothness of the loss function.
Ji et al. (2020b) extend this result to the multi-step version of MAML. Other works (Charles &
Konecnˇ y`, 2020; Wang et al., 2021; Gao & Sener, 2020; Collins et al., 2020) investigate the MAML
optimization landscape and the trade-off phenomena in terms of task difficulty: e.g. MAML tend to
find meta-initializations that are closer to difficult tasks. However, the effect of inner loop learning
rate α on the MAML dynamics and learned solution are not explored in these works.

Of particular relevance is the work of Bernacchia (2021), which derives, under an ideal setting of
Gaussian inputs and regression coefficients, a range of α values that can help guide its choice. In
this paper, we adopt a more general setting, where we do not assume specific input distributions. We
derive a precise optimal value of α (instead of a range), which can be estimated from input data.
Furthermore, we show using experiments that the optimal values may not be negative (c.f. Bernacchia
(2021)) in the standard meta-learning setting, where the same α is used for training and testing.

5 EXPERIMENTS

5.1 ESTIMATION OF α[∗]

We verify our theorem through Neural Tangent Kernel (NTK) (Jacot et al., 2018) and deep learning
on the Omniglot dataset (Lake et al., 2011). In the former setting, we followed the problem setup in
(Bernacchia, 2021) to perform quadratic regression. Different from their model size of 60, we used
a two-layer Neural Tangent Kernel (NTK) (Jacot et al., 2018) with sufficiently wide hidden layers
(size 10, 000). Then, we can estimate α[∗] by the neural tangent feature to obtain αest[∗] [= 1][/][(2][NK]σ[˜][2])


-----

(a) Omniglot 20w1s Test Loss (b) Omniglot 20w1s Test Accuracy

Figure 3: Test loss and accuracy on Omniglot 20-way 1-shot classification. The blue and orange
line represent the test loss (left) and test accuracy (right) of original configuration in ANIL (Raghu
et al., 2020) paper and our online estimation. The shadows are the standard deviation of multiple
experiments with different seeds.



0.375

0.350 N(0, )

0.325 U( 12 /2, 12 /2)

Exp(1/ )

0.300

*lim

0.275

0.250

0.225

0.200

0 10 20 30 40 50

2 of D(x)

(a) Gaussian Basis function


0.00020 N(0, )

U( 12 /2, 12 /2)

0.00015

*lim

0.00010

0.00005

0.00000

0 5 10 15 20 25 30

2 of D(x)

(b) NTK with Uniform Initialization


Figure 4: Value of αlim[∗] [along the data variance,][ σ][2][. Different curves are different data distributions.]
(a)The feature of Gaussian basis function. These curves can be perfectly fitted by an inverse
proportional function. (b) The feature of uniformly initialized NTK model.

(σ˜ is the variance of NTK feature, whose derivation is found in Appendix H.2). Shown as a vertical
dotted line ending with the red star in Figure 2, we can see our estimation is nearly optimal. To
reduce fortuity, we choose arbitrary values of N, K to compute the estimation αlim[∗] [. Furthermore, we]
also test our estimation on uniform initialization with other groups of hyper-parameters and obtained
similar results. Then, for deep learning classification, we use online estimation to compute α[∗] for
ANIL Raghu et al. (2020) on the Omniglot dataset Lake et al. (2011). To keep training stable, we
normalize the features before the last layer and compute the corresponding αest[∗] [. Then, we compare]
our estimation scheme with the default selection method where the model and training learning rates
are the same. Test loss and accuracy are reported with mean and variance in Figure 3. Both training
schemes achieve similar results after 4 × 10[4] iterations. We only plot the first 1.5 × 10[4] iters (20
iters per scale) to see the differences clearly. As shown, our estimation of α[∗] converges faster than
that in the default configuration. Other experimental parameters and additional results, including
non-central distributions and deep regression experiments, are found in Appendix H.4. Overall, these
experiments suggest that our estimation derived in the idealized linear setting can guide practical
hyper-parameter selection.


5.2 RELATION OF DATA VARIANCE AND OPTIMAL α

In this section, we verified our theoretical results of αlim[∗] [and its relation to data variance. As]
drawn the Figure 4, value of αlim[∗] [and data variance have an inverse relationship. We first verified]


-----

(a) Adaptation Loss Gap (b) Fast Adaptation Distance Gap

Figure 5: With different data distributions, xi ( 5, 5), (0, 2), Exp(1) (curves in different
colors) (a) the loss difference between MAML and ERM with ∼U _−_ _N_ _t steps adaptation on each task_

_i[[][ℓ]t[i][(][w][m][)][ −]_ _[ℓ][i]t[(][w][r][)]][ (][ ℓ][i]t[(][w][m][)][ is the][ t][-step adaptation loss on task][ i][ from the MAML learned]_
initialization wm) and (b) Average solution distance gap of MAML and ERM after t-step adaptation,

Pt(wm) _t(wr)._

_F_ _−F_

this with a Gaussian basis function Φ(X)(ij) = exp(− _X(ij) −_ _µj_ 2 /2σi2[)][. Then, we conducted]
experiments on three different data distributions: normal distribution N (0, σ), uniform distribution
  
_U_ (−√12σ/2, _√12σ/2) and exponential distribution Exp(1/σ). From in (a), we can see the smooth_

curves perfectly fitted with some inverse proportional function e.g. y = 0.35/σ[2]. Next, we used
NTK as the basis function to verify our result in overparameterized regime. We used two layers MLP
with width= 10, 240 and uniform initialization to compute the neural tangent feature. As we can see
from Figure 4(b), the diagram also shows the tendency that αlim[∗] [decreases as][ σ][ increasing. As a]
consequence, variance, as a part of the statistical property of data, will influence α[∗].


5.3 FAST ADAPTATION

To understand the effect of α on _t, we set α = 10[−][4]_ to train MAML such that its global minimum
_F_
**_wm is inched from ERM wr. Then, we tracked their adaptation losses and adaptation errors_**
with growing adaptation steps, shown in the Figure 5. Adaptation loss for task i is defined by
_ℓ[i]t[(][w][) =][ ∥][Φ(][X][i][)][Adapt][(][w][, i, t, η][)]_ _[−]_ **_[y][i][∥][2][ where][ Adapt][(][w][, i, t, η][)][ is][ t][-step adaptation parameter with]_**
learning rate η = 1e − 5. The adaptation loss difference between MAML and ERM is described
as _i=1_ _[ℓ]t[i][(][w][m][)][ −]_ _[ℓ][i]t[(][w][r][)][. From Figure][ 5][ (a) we can see, the loss of MAML is marginally higher]_
than ERM before adapting. But the difference dramatically decreases to negative values, which
illustrates that MAML has better performance than ERM with only few steps adaptation. Similar

[P][5000]
results appear on various data distributions: uniform distribution U (−5, 5), normal distribution
_N_ (0, 2) and exponential distribution Exp(1). It makes sense because wr, wm are the minimizers
of non-adaptation loss and one-step adaptation loss, respectively. Then we plot the difference of
adaptation errors in distance _t(wm)_ _t(wr) along adaptation step t. In Figure 5(b) we can see,_
_F_ _−F_
_Ft of MAML is always smaller than ERM’s, including t = 0. Since Ft measures distances of adapted_
solution and task optimum solution, this result has substantiated our Theorem 2. Furthermore, it
also demonstrate that the effect of α, even it is small, is acting as the guide to find a distance-aware
meta-initialization for target tasks which possesses faster adaptability compared to ERM.

6 CONCLUSION

In this paper, we investigated MAML through the lens of adaptation learning rate α. We gave a
principled way to estimate an optimal adaptation learning rate α[∗] minimizing MAML population
risk. We also try to interpret the role of α statistically and geometrically. Further investigation has
revealed the underlying data statistics that α[∗] depends on. This statistical dependency also motivates
us to explore other effect of α, such as the optimization behavior in a geometric context. By studying
the role of α on optimization, we confirmed theoretically that MAML obtains solutions with shorter
average distance to individual task optima than ERM - an empirical observation that was suggested
to contributes to MAML’s fast adaptability. We believe these results are instructive in contributing to
the theoretical understanding of meta-learning and its algorithm design.


-----

7 ACKNOWLEDGEMENT

This research/project is supported by the National Research Foundation, Singapore under its AI
Singapore Programme (AISG Award No: AISG-GC-2019-001-2A). Any opinions, findings and
conclusions or recommendations expressed in this material are those of the author(s) and do not
reflect the views of National Research Foundation, Singapore. Q. Li is supported by the National
Research Foundation, Singapore, under the NRF fellowship (NRF-NRFF13-2021-0005).

REFERENCES

Ron Amit and Ron Meir. Meta-learning by adjusting priors based on extended pac-bayes theory. In
_ICML, 2018._

Maria-Florina Balcan, Mikhail Khodak, and Ameet Talwalkar. Provable guarantees for gradient-based
meta-learning. In International Conference on Machine Learning, pp. 424–433. PMLR, 2019.

Alberto Bernacchia. Meta-learning with negative learning rates. In International Conference on
_Learning Representations, 2021._

Andrea Braides. A handbook of gamma-convergence. In Handbook of Differential Equations:
_stationary partial differential equations, volume 3, pp. 101–213. Elsevier, 2006._

Zachary Charles and Jakub Konecnˇ y. On the outsized importance of learning rates in local update`
methods. arXiv preprint arXiv:2007.00878, 2020.

Liam Collins, Aryan Mokhtari, and Sanjay Shakkottai. Why does maml outperform erm? an
optimization perspective. arXiv preprint arXiv:2010.14672, 2020.

Giulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil. Learning-to-learn stochastic gradient descent with biased regularization. In International Conference on Machine Learning,
pp. 1566–1575. PMLR, 2019.

Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. On the convergence theory of gradient-based
model-agnostic meta-learning algorithms. In International Conference on Artificial Intelligence
_and Statistics, pp. 1082–1092. PMLR, 2020._

Chelsea Finn and Sergey Levine. Meta-learning and universality: Deep representations and gradient descent can approximate any learning algorithm. In International Conference on Learning
_Representations, 2018._

Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume
_70, pp. 1126–1135. JMLR. org, 2017._

Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In
_Advances in Neural Information Processing Systems, pp. 9516–9527, 2018._

Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online meta-learning. In
_International Conference on Machine Learning, pp. 1920–1930. PMLR, 2019._

Katelyn Gao and Ozan Sener. Modeling and optimization trade-off in meta-learning. Advances in
_Neural Information Processing Systems, 33, 2020._

Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths. Recasting gradientbased meta-learning as hierarchical bayes. arXiv preprint arXiv:1801.08930, 2018.

James Harrison, Apoorva Sharma, and Marco Pavone. Meta-learning priors for efficient online
bayesian regression. arXiv preprint arXiv:1807.08912, 2018.

Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and´
generalization in neural networks. In Advances in Neural Information Processing Systems, 2018.

Khurram Javed and Martha White. Meta-learning representations for continual learning. arXiv
_preprint arXiv:1905.12588, 2019._


-----

Kaiyi Ji, Jason D Lee, Yingbin Liang, and H Vincent Poor. Convergence of meta-learning with
task-specific adaptation over partial parameters. In Advances in Neural Information Processing
_Systems, 2020a._

Kaiyi Ji, Junjie Yang, and Yingbin Liang. Multi-step model-agnostic meta-learning: Convergence
and improved algorithms. arXiv preprint arXiv:2002.07836, 2020b.

Mikhail Khodak, Maria-Florina F Balcan, and Ameet S Talwalkar. Adaptive gradient-based metalearning methods. In Advances in Neural Information Processing Systems, pp. 5915–5926, 2019.

Brenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum. One shot learning of
simple visual concepts. In Proceedings of the annual meeting of the cognitive science society,
volume 33, 2011.

Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-sgd: Learning to learn quickly for few-shot
learning. arXiv preprint arXiv:1707.09835, 2017.

Dongze Lian, Yin Zheng, Yintao Xu, Yanxiong Lu, Leyu Lin, Peilin Zhao, Junzhou Huang, and
Shenghua Gao. Towards fast adaptation of neural architectures with meta learning. In International
_Conference on Learning Representations, 2019._

Albert W Marshall, Ingram Olkin, and Barry C Arnold. Inequalities: theory of majorization and its
_applications, volume 143. Springer, 1979._

Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv
_preprint arXiv:1803.02999, 2018._

Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature
reuse? towards understanding the effectiveness of maml. In International Conference on Learning
_Representations, 2020._

Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with implicit
gradients. In Advances in Neural Information Processing Systems, pp. 113–124, 2019.

Mark Rudelson and Roman Vershynin. Hanson-wright inequality and sub-gaussian concentration.
_Electronic Communications in Probability, 18:1–9, 2013._

Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero, and Raia Hadsell. Meta-learning with latent embedding optimization. arXiv preprint
_arXiv:1807.05960, 2018._

Rajesh Sharma, Madhu Gupta, and Girish Kapoor. Some better bounds on the variance with
applications. J. Math. Inequal, 4(3):355–363, 2010.

Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
_Advances in Neural Information Processing Systems, 2017._

Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching
networks for one shot learning. In Advances in Neural Information Processing Systems, 2016.

Xiang Wang, Shuai Yuan, Chenwei Wu, and Rong Ge. Guarantees for tuning the step size using a
learning-to-learn approach. In International Conference on Machine Learning, pp. 10981–10990.
PMLR, 2021.

Mingzhang Yin, George Tucker, Mingyuan Zhou, Sergey Levine, and Chelsea Finn. Meta-learning
without memorization. In International Conference on Learning Representations, 2020.

Chris Zhang, Mengye Ren, and Raquel Urtasun. Graph hypernetworks for neural architecture search.
In International Conference on Learning Representations, 2019.

Pan Zhou, Xiaotong Yuan, Huan Xu, Shuicheng Yan, and Jiashi Feng. Efficient meta learning via
minibatch proximal update. In Advances in Neural Information Processing Systems, 2019.

Pan Zhou, Yingtian Zou, Xiao-Tong Yuan, Jiashi Feng, Caiming Xiong, and Steven C. H. Hoi. Task
similarity aware meta learning: Theory-inspired improvement on maml. In 4th Workshop on
_Meta-Learning at NeurIPS, 2020._


-----

A DEFINITIONS, NOTATIONS AND LEMMAS

**Notation** We denote an optimal adaptation learning rate as α[∗]. Global minima of empirical risk of
MAML and ERM (when they are unique) are denoted by wm, wr. We write 1, ..., N as [N ] and
_{_ _}_
use ∥· ∥ to denote the Euclidean norm. We use subscripts to index the matrices/vectors corresponding
to task instances, and bracketed subscripts to index the entries of matrices. For function f depends on
_a, b, x, we omit other variables by f_ (..., x) when we discuss with x.

Table 1: High-frequency notation table.

|Symbols Definition|Symbols Definition|
|---|---|


|α Adaptation learning rate α∗ Optimal adaptation learning rate α∗, α∗ Estimation (limit) of α∗ lim est λ, λ Min/max of eigenvalues I S a Task optimum of task i i d Feature dimension|K, k, k All/train/val/ sample size per task 1 2, Population risk of MAML/ERM Lm Lr ˆ, ˆ Empirical risk of MAML/ERM Lm Lr ¯ Average population risk Lm N Number of training tasks w, w Global minimum of MAML/ERM m r|
|---|---|



**Definition 2 (Gamma Convergence). Let Fn :** R for each n N. We say that (Fn)n N Γ
_X →_ Γ _∈_ _∈_

_-converges to F :_ R, and write Γ limn _Fn = F or Fn_ _F, if_
_X →_ _−_ _→∞_ _→_

-  For every x _and every (xn)n_ N such that xn _x in_
_∈X_ _∈_ _→_ _X_

_F_ (x) lim inf
_≤_ _n_
_→∞_ _[F][n][ (][x][n][)]_

-  for every x _, there exists some (xn)n_ N such that xn _x in_ _and_
_∈X_ _∈_ _→_ _X_

_F_ (x) ≥ lim supn→∞ _[F][n][ (][x][n][)]_

**Definition 3 (Lower Semicontinuous Envelope). Given F : X →** R, the lower semicontinuous
_envelope (or relaxation) of F is the ”greatest lsc function bounded above by F_ _”:_

_F_ [lsc](x) := sup{G(x) | G : X → R is lsc and G ≤ _F on X}_

= inf lim inf
_n→∞_ _[F][ (][x][n][)][ |][ (][x][n][)][n][∈][N][ ⊆X][ and][ x][n][ →]_ _[x]_

**Lemma 1 (Remark 2.2, (Braides, 2006n** )). If Fn uniform converge to F _, then Fno→Γ_ _F_ _[lsc]_ _where F_ _[lsc]_
_is Lower Semicontinuous Envelope of F_ _._

**Lemma 2 (Γ-Convergence, (Braides, 2006)). Let X be a topological space. Let {Fn} be a equi-**
_coercive family of functions and let Fn Γ -converges to F in X, then_

-  limn _dn = d where dn = inf_ _x_ _X and d = inf_ _x_ _X F_ (x). That is, the minima converges
_→∞_ _∈_ _∈_
_Fn(x)_

-  The minimizers of Fn converge to a minimizer of F _._

**Proposition 2. If both A and B are positive semidefinite, the inequality is true:**

tr(AB) ≤ tr(A) tr(B). (A.1)

_and if A is n-by-n symmetric PSD, we have_


tr(A[2])
_≥_ [tr(]n[A][)][2]


(A.2)


-----

_Proof. Let a = tr(A). For PSD matrix A, we have A ⪯_ _aI. Then_

tr(AB) = tr(B[1][/][2]AB[1][/][2]) ≤ tr(B[1][/][2](aI)B[1][/][2]) = tr(A) tr(B). (A.3)
For second inequality, we can apply spectral decomposition on A as A = QDQ[−][1]. So we have


tr(A) = tr(QDQ[−][1]) = tr(DQQ[−][1]) = tr(D) =


_λi._


where _λi_ _, i_ [1, n] is the eigenvalues of matrix A. Then by Cauchy-Schwarz inequality we can get
_{_ _}_ _∈_ _n_ 2 _n_

tr(A)[2] = _λi_ _n_ _λ[2]i_ = n tr(A[2])

_i=1_ ! _≤_ _i=1_ !

X X

**Lemma 3 (Hanson-Wright inequality, (Rudelson & Vershynin, 2013)). Let X = (X1, . . ., Xn) ∈** R[n]
_be a random vector with independent components Xi which satisfy EXi = 0 and_ _Xi_ _ψ2_ _L. Let_
_A be an n × n matrix. Then, for every t ≥_ 0, _∥_ _∥_ _≤_

_t[2]_ _t_
P _X_ _[⊤]AX_ EX _[⊤]AX_ _> t_ 2 exp _c min_ _,_
_−_ _≤_ _−_ _L[4]_ _A_ HS _L[2]_ _A_
  _∥_ _∥[2]_ _∥_ _∥_ 


_where_ _ξ_ _ψ2 = supp_ 1 p[−][1][/][2] (E _X_ )[1][/p] _is sub-gaussian norm,_ _A_ = maxx=0 _Ax_ 2/ _x_ 2 is
_∥_ _∥_ _≥_ _|_ _|[p]_ _∥_ _∥_ _̸_ _∥_ _∥_ _∥_ _∥_
_operator norm and_ _A_ _HS = ([P]i,j_
_∥_ _∥_ _[|][a][i,j][|][2][)][1][/][2][ is Hilbert-Schmidt (or Frobenius) norm.]_

**Definition 4 (Fast Adaptation Error). Given the task distribution D(T** ), meta-initialization w[0], the
_optimal solution of task T is wT[∗]_ _[, then][ t][-step fast adaptation error is defined by]_
_Ft(w[0], D(T_ )) := _T ∼DE(T )_ _[∥][w]T[t]_ _[−]_ **_[w]T[∗]_** _[∥]2[2]_ (A.4)

_where wT[t]_ [=][ w][0][ −] _[η][ P]j[t]_ _[∇]wi[j]_ _[ℓ][i][(][w]T[j]_ [)][ is the adapted parameter of task][ T][ with][ t][ steps.]

**Lemma 4 (Ruhe’s trace inequality, (Marshall et al., 1979)). If A, B are n × n positive semidefinite**
_Hermitian matrices with eigenvalues,_
_a1_ _an_ 0, _b1_ _bn_ 0
_≥· · · ≥_ _≥_ _≥· · · ≥_ _≥_
_respectively, then_


_i=1_ _aibn−i+1 ≤_ tr(AB) ≤

X


_aibi_
_i=1_

X


**Proposition 3. For any positive random variableEx** (x)(x[2]) 2 Ex _x ∼D(x)(x)(Ex)x, we have following inequality holds true(x)(x3)_ 0 (A.5)
_∼D_ _−_ _∼D_ _∼D_ _≤_
  


_Proof. With the fact that_
Ex (x)(x[2]) 2 Ex (x)(x)Ex (x)(x3)
_∼D_ _−_ _∼D_ _∼D_

2

  

= _R_ _x[2]p(x) dx_ _−_
Z  Z

where x > 0 and p(x) > 0.


_xp(x) dx_
 Z


(A.6)
_x[3]p(x) dx_



1 [1]

Let f = (xp(x)) 2 > 0 and g = _x[3]p(x)_ 2 > 0, with Cauchy-Schwarz Inequality,

2

  

_R_ _fg dx_ _≤_ _R_ _f_ [2] dx _R_ _g[2]_ dx (A.7)

Z  Z  Z 

we have that

2 2

2
Ex (x)(x[2]) = _xp(x)_ _x[3]p(x) dx_ = _fg dx_
_∼D_ _R_ _R_
Z  Z 
   p p2 2 (A.8)

_≤_ _R_ _xp(x)_ dx _R_ _x[3]p(x)_ dx
Z p  Z p 

=Ex (x)(x)Ex (x)(x[3])
_∼D_ _∼D_


-----

B PROOF OF THEOREM 1

**Proof Sketch** We list our proof steps as follows

1. Get global minima of ERM and MAML by first order optimality condition.
2. Let average MAML population risk _L[¯]m as the target in order to eliminate the randomness_
(Proposition 1 guarantees the upper bound between _L[¯]m and population risk Lm)._

3. Approximate this target function _L[¯]m by another function L[apx]m_ which is the limit of _L[¯]m as_
number of tasks N →∞.

4. According to positive definiteness, we can get the range of α.
5. With notion of gamma convergence, the minimizer of _L[¯]m will also converge to the minimizer_
of L[apx]m [.]

6. Our estimation of α[∗] is in the range of α in Step 4.

**Notation for this proof: For simplicity, we omit the arguments of the function if its symbol has a**
index e.g. Φi = Φ(Xi), Ci = Ci(α). Then we give the full proof on estimation of α[∗].


_Proof. Recall that the global minimum closed form for ERM and MAML are_

1

_N_ _−_ _N_

**_wr(N, K) = wr_** _{Φ(Xi), ai}i∈[N_ ] = _i=1_ Φ[⊤]i [Φ][i]! _i=1_ Φ[⊤]j [Φ][j][a][j]
   X 1 X

_N_ _−_ _N_

**_wm(α, N, K) = wm_** _{Ci(α), ai}i∈[N_ ] = _i=1_ _Ci[⊤][C][i]!_ []j=1 _Cj[⊤][C][j][a][j]_
   X X




(B.1)


where Ci = Φi − [2]K[α] [Φ][i][Φ]i[⊤][Φ][i][, C][i][ ∈] [R][K][×][d][.]

Since wm depends on random variables a1, ..., aN . The average population risk of MAML, defined
in (3.2), where
¯m(α, N, K) = Ewm _m(wm, α, K) = Ea1,...,aN_ (a) _m(wm, α, K)_ (B.2)
_L_ _L_ _∼D_ _L_
of global minimum wm in (B.1) can be written as
¯m(α, N, K) = Ea1,...,aN (a) _m(wm, α, K)_
_L_ _∼D_ _L_

= [1] E E E

_K_ _{ai}i[N]=1[∼D][(][a][)]_ **_a∼D(a)_** _X∼D(x)_ _[∥][C][(][α][)(][w][m][ −]_ **_[a][)][∥][2]_**

1 2

_N_ _−_ _N_

= [1] E E _C(α)_ _Ci[⊤][C][i]_ _Cj[⊤][C][j][a][j]_ **_a_**

_K_ **_a,{ai}i[N]=1[∼D][(][a][)]_** _X∼D(x)_  _i=1_ ! []j=1  _−_ 

X X

   (B.3)

_N_ _−1_
Let Λj = _Ci[⊤][C][i]_ _Cj[⊤][C][j][, j][ ∈]_ [[1][, N] []][,][ Λ][j][ ∈] [R][d][×][d][. The (][B.3][) can be rewritten as,]

_i=1_

 
P

2

_N_

¯m(α, N, K) = [1] E E _C(α)_ Λjaj **_a_**
_L_ _K_ **_a,{ai}i[N]=1[∼D][(][a][)]_** _X∼D(x)_ j=1 _−_ 

X

_N_  _⊤_  _N_

= [1] E E Λiai _C(α)[⊤]C(α)_ Λjaj

_K_ **_a,{ai}i[N]=1[∼D][(][a][)]_** _X∼D(x)_ " _i=1_ ! j=1 

X X (B.4)

_N_ _N_ _⊤_  

**_a[⊤]C(α)[⊤]C(α)_** Λjaj Λiai _C(α)[⊤]C(α)a_
_−_ j=1  _−_ _i=1_ !

X X
 

+ a[⊤]C(α)[⊤]C(α)a

#


-----

Under Assumption 1 and a is independent to X, then we have


_⊤_

_C(α)[⊤]C(α)_

!


¯m(α, N, K) = [1]
_L_ _K_


Λiai
_i=1_

X


Λjaj
_j=1_

X


_X∼D(x)_


_{ai}i[N]=1[∼D][(][a][)]_


+ σa[2] [tr] _C(α)[⊤]C(α)_
 


(B.5)


= _[σ]a[2]_


tr Λ[⊤]j _[C][(][α][)][⊤][C][(][α][)Λ][j]_
_j=1_

X  


+ tr _C(α)[⊤]C(α)_



 



_X∼D(x)_


Let L[apx]m [(][α][)][ be an approximation function of][ ¯]Lm(α, N, K).


_L[apx]m_ [(][α][)][ ≜] _[σ]a[2]_

_K_

= _[σ]a[2]_


E
_X_ (x) [tr[][C][(][α][)][⊤][C][(][α][)]]
_∼D_

_⊤_

E _I_ Φ(X)[⊤]Φ(X) _I_
_X_ (x) [tr] _−_ [2]K[α] [Φ(][X][)][⊤][Φ(][X][)] _−_ [2]K[α] [Φ(][X][)][⊤][Φ(][X][)]
_∼D_ "   [#]

(B.6)


Then the approximation error will be

_K_

[¯]m(α, N, K) _L[apx]m_ [(][α][)][|]
_σa[2]_ _|L_ _−_


tr Λ[⊤]j _[C][(][α][)][⊤][C][(][α][)Λ][j]_
_j=1_

X  


(B.7)


_X∼D(x)_

E
_X∼D(x)_


_−1_
!


1

_N_ _−_

_Ci[⊤][C][i]_
_i=1_ !

X


_Cj[⊤][C][j]_






_Ci[⊤][C][i]_
_i=1_

X


_C(α)[⊤]C(α)_


_Cj[⊤][C][j]_


tr

)

_j=1_

X


where


_Ci[⊤][C][i]_ [= Φ][⊤]i [Φ][i] _i_ [Φ][i][)][2][ + 4][α][2] _i_ [Φ][i][)][3] (B.8)

_[−]_ [4]K[α] [(Φ][⊤] _K_ [2][ + (Φ][⊤]

With Assumption 2, there exists constants 0 < c1 < c2 where

_c1_ Φ(Xi) _F_ (B.9)
_≤∥_ _∥[2]_ _[≤]_ _[c][2][,][ ∀][i][ ∈]_ [[][N] []]

With Proposition 2 hold, ∀i ∈ [N ] we have


tr(Ci[⊤][C][i][) = tr] Φ[⊤]i [Φ][i] _i_ [Φ][i][)][2][ + 4][α][2] _i_ [Φ][i][)][3]

_[−]_ [4]K[α] [(Φ][⊤] _K_ [2][ (Φ][⊤]
 

4α 4α2
= tr Φ[⊤]i [Φ][i] tr _i_ [Φ][i][)][2] + tr _i_ [Φ][i][)][3]
_−_ _K_ [(Φ][⊤] _K_ [2][ (Φ][⊤]
   
  

sup Φi _F_ inf _i_ [Φ][i][)][2][] + 4][α][2] Φi _F_
_≤_ _i∈[N_ ] _∥_ _∥[2]_ _[−]_ [4]K[α] _i∈[N_ ] [tr[(Φ][⊤] _K_ [2][ sup]i∈[N ] _∥_ _∥[6]_

=c2 inf _i_ [Φ][i][)][2][] + 4][α][2] 2
_−_ [4]K[α] _i∈[N_ ] [tr[(Φ][⊤] _K_ [2][ c][3]

_c2_ 1 [+ 4][α][2] 2
_≤_ _−_ _Kd[4][α]_ _[c][2]_ _K_ [2][ c][3]


(B.10)


-----

Then by applying multiple times of Proposition 2 we have


_−1_
!


1

_N_ _−_

_Ci[⊤][′][ C][i][′]_
_i[′]=1_ !

X


_Cj[⊤][C][j]_






_Ci[⊤][C][i]_
_i=1_

X


_C(α)[⊤]C(α)_


_Cj[⊤][C][j]_


tr

_j=1_

X

_N_

tr

_j=1_

X


_X∼D(x)_

E
_X∼D(x)_

E
_X∼D(x)_

E
_X∼D(x)_


1 1

_N_ _N_ _−_ _N_ _−_

= E tr _Cj[⊤][C][j][C]j[⊤][C][j]_ _Ci[⊤][C][i]_ _Ci[⊤][′][ C][i][′]_ _C(α)[⊤]C(α)_
_X∼D(x)_ _j=1_  _i=1_ ! _i[′]=1_ !

X X X

_N_  _N_ _−2[]_

E tr (Cj[⊤][C][j][)][2][] tr _Ci[⊤][C][i]_ tr _C(α)[⊤]C(α)_
_≤_ _X∼D(x)_ _j=1_  _i=1_ !

X  X   

 1[]2

_N_ _N_ _−_

2

E tr _Cj[⊤][C][j]_ tr _Ci[⊤][C][i]_ tr _C(α)[⊤]C(α)_
_≤_ _X∼D(x)_ _j=1_  _i=1_ !

X    X   

  1[]2

3 _N_ _−_

_N_ _c2_ 1 [+ 4][α][2] 2 E _Ci[⊤][C][i]_
_≤_  _−_ _Kd[4][α]_ _[c][2]_ _K_ [2][ c][3] _X∼D(x)_ [tr]  _i=1_ !

X

 


(B.11)


Next, we need upper bound the last inverse term. With Assumption 2 we know that all eigenvalues
of Φ(X)[⊤]Φ(X), X ∼D(x) are bounded by [λI _, λS]. Let Call(α) =_ _i=1_ _[C]i[⊤][C][i][, then with]_
probability 1 the max/min eigenvalues of Call(α) will have following constraints,

[P][N]
_λmin(Call(α))_ _N_ (λI 4αλ[2]S[/K][ + 4][α][2][λ][3]I _[/K]_ [2][)]
 _λmax(Call(α))_ _≤≥_ _N_ (λS − − 4αλ[2]I _[/K][ + 4][α][2][λ][3]S[/K]_ [2][)] (B.12)

Since the Call(α) is a positive matrix, we need have the constrain on λmin(Call(α)) > 0, which
means


0, [K][(][λ]S[2] _[−]_ _λ[4]S_ _[−]_ _[λ]I[4][)]_

2pλ[3]I


_K(λ[2]S_ [+] _λ[4]S_ _I_ [)]

_,_

_[−]_ _[λ][4]_

2pλ[3]I _∞_


(B.13)


_α ∈_


There exists a positive definite matrix λs(α, N )I where

_Call(α)_ _λmin(Call)I_ (B.14)
_⪰_

and the following inequality is easy to get

tr(Call(α)) tr(λmin(Call)I)
_≥_ (B.15)

tr(Call[−][1][(][α][))][ ≤] [tr(][λ]min[−][1] [(][C][all][)][I][)]


So the last inverse term will be

1[]

_N_ _−_

E _Ci[⊤][C][i]_
_X∼D(x)_ [tr]  _i=1_ !

X

 


2

1 1

= O

_N_ (λI 4αλ[2]S[/K][ + 4][α][2][λ][3]I _[/K]_ [2][)][d] _N_ [2]
_−_  


(B.16)

(B.17)


Apply these inequalities to (B.7), we can get the upper bound

3

¯m(α, N, K) _L[apx]m_ [(][α][)] _a_ _N_ _c1 −_ _Kd[4][α]_ _[c]2[2]_ [+][ 4]K[α][2][2][ c]1[3] 1
_L_ _−_ _≤_ _[σ]K[2]_ _N_ [2](λI 4αλ[2]S[/K][ + 4][α][2][λ][3]I _[/K]_ [2][)][2][d][2][ =][ O] _N_

_[·]_ _−_ 

_K(λ[2]S_ _[−][√]λ[4]S_ _[−][λ][4]I_ [)] _K(λ[2]S_ [+][√]λ[4]S _[−][λ][4]I_ [)]
When α ∈ 0, 2λ[3]I _∪_ 2λ[3]I _, ∞_, the limit will go to zero,
   


¯Lm(α, N, K) − _L[apx]m_ [(][α][)] = 0 (B.18)


lim sup
_N_
_→∞_ _α_ 0, _K(λ[2]S_ _[−][√]λ[4]S_ _[−][λ]I[4]_ [)]

_∈"_ 2λ[3]I !∪


_K(λ[2]S_ [+][√]λ[4]S _[−][λ]I[4]_ [)] _,_

2λ[3]I _∞_


-----

which means _L[¯]m(α, N, K) will uniformly converge to L[apx]m_ [(][α][)][ for][ α][ belongs to the interval above.]
Note that [¯]m(α, N, K) is a continuous function of α. So with Lemma 1, we have
_L_

Γ lim ¯m(α, N, K) = L[apx]m [(][α][)] (B.19)
_−_ _N_ _L_
_→∞_

So we have estimation of true α[∗], denoted as αlim[∗]
_αlim[∗]_ [= arg min]α _[L]m[apx][(][α][)]_

= arg minα _X_ E(X) [tr] Φ(X)[⊤]Φ(X) − [4]K[α] [(Φ(][X][)][⊤][Φ(][X][))][2][ + 4]K[α][2][2][ (Φ(][X][)][⊤][Φ(][X][))][3]
_∼D_  

= _[K][ tr[][E][X]_ [[(Φ(][X][)][⊤][Φ(][X][))][2][]]]

2 tr[EX [(Φ(X)[⊤]Φ(X))[3]]]
(B.20)


According to Lemma 2 where α[∗](N, K) is the minimizer of [¯]m(α, N, K)
_L_

_α[∗](N, K) = arg min_ ¯m(α, N, K), lim _lim[.]_ (B.21)
_α_ _L_ _N_
_→∞_ _[α][∗][(][N, K][) =][ α][∗]_

C PROOF OF PROPOSITION 1


**Proposition 4 (Formal state of Proposition 1). Assume u = C(α)[⊤]C(α)a is sub-gaussian ran-**
_dom variable with sub-gaussian norm_ **_u(i)_** Ψ2 = supp 1 p[−][1][/][2][  ]E **_u(i)_** _L. Then with_
_∥_ _∥_ _≥_ _|_ _|[p][][1][/p]_ _≤_
_probability at least 1 −_ _δ that_
_Lm(α, N, K)_ _[≤]_ _[L]K[2]_ [max] (r _dε(Nα, K[2]_ ) log [2]δ [, ε][(][α, K]N [)] log [2]δ ) (C.1)

_if α ∈[L][m]0[(],[w]K[m](λ[, α, K][2]S_ _[−]2[√]λ[3]Iλ[)][4]S[ −][−][λ][4]I[¯][)]_ _∪_ _K(λ[2]S_ [+]2[√]λ[3]Iλ[4]S _[−][λ][4]I_ [)] _, ∞_ _and_
   

_ε(α, K) = (λS −_ 4αλ[2]I _[/K][ + 4][α][2][λ]S[3]_ _[/K]_ [2][)][/][(][λ][I] _[−]_ [4][αλ]S[2] _[/K][ + 4][α][2][λ]I[3][/K]_ [2][)][2]

_when assumption 1 & 2 holds. d is the feature size, N is the number of tasks and K is the sample_
_size per task._

Here, we follow the same proof notation as Theorem 1.


_Proof. By definition, we have_
¯m(α, N, K) = Ea1,...,aN (a) _m(wm(α, N, K), α, K)_ (C.2)
_L_ _∼D_ _L_

1
_N_ _−_
Similar to (B.3), let Λj = _i=1_ _[C]i[⊤][C][i]_ _Cj[⊤][C][j][, we have,]_
P 

_m(wm, α, K) = [1]_ E E
_L_ _K_ **_a∼D(a)_** _X∼D(x)_ _[∥][C][(][α][)(][w][m][(][α, N, K][)][ −]_ **_[a][)][∥][2]_**


= [1]

_K_

= [1]


_C(α)_


Λjaj **_a_**
_j=1_ _−_

X


**_a∼D(a)_** _X∼D(x)_

E E
**_a∼D(a)_** _X∼D(x)_


_N_ _⊤_

Λiai
_i=1_ !

X


(C.3)


_C(α)[⊤]C(α)_


Λjaj
_j=1_

X


_⊤_
!


_−a[⊤]C(α)[⊤]C(α)_

+a[⊤]C(α)[⊤]C(α)a


 _−_


_C(α)[⊤]C(α)a_


Λjaj
_j=1_

X


Λiai
_i=1_

X


-----

With Assumption 1, second term and third term of (C.3) will be cancelled. So the Lm is

_N_ _⊤_ _N_

_m = [1]_ E Λiai _C(α)[⊤]C(α)_ Λjaj + _[σ]a[2]_ E
_L_ _K_ _X∼D(x)_  _i=1_ ! j=1  _K_ _X∼D(x)_ [tr[][C][(][α][)][⊤][C][(][α][)]]

X X

   (C.4)

As the comparison, [¯]m(α, N, K) is the averaged function of _m, which is given by_
_L_ _L_

_N_ _⊤_ _N_

¯m = [1] E E Λiai _C(α)[⊤]C(α)_ Λjaj
_L_ _K_ _X∼D(x)_ " _{ai}i[N]=1[∼D][(][a][)]_ _i=1_ ! j=1 

X X (C.5)

 

+ σa[2] [tr] _C(α)[⊤]C(α)_
#
  

Let A be the common matrix of cross-term of ai and aj, then for each term in (C.5),


_−1_
!


_−1_
!


Λ[⊤]i _[C][(][α][)][⊤][C][(][α][)Λ][j]_ [=][ C]i[⊤][C][i] _Ck[⊤][C][k]_ _C(α)[⊤]C(α)_ _Ck[⊤][′]_ _[C][k][′]_ _Cj[⊤][C][j]_

_k=1_ ! _k[′]=1_ !

_Ci_ X X

_A_

| {z }e

=CiACj| {z }

So cancel their second terms, the difference of _m(wm, α) and_ [¯]m(α, N, K) will be

[e] [e] _L_ _L_

_m(wm, α, K)_ _m(α, N, K)_
_L_ _−_ _L[¯]_


(C.6)

(C.7)


_N_ _N_

_a_

= [1] E **_a[⊤]i_** _CiACjaj_ + _[σ][2]_ E _C(α)[⊤]C(α)_

_K_ _X∼D(x)_  _i=1_ _j=1_  _K_ hhhhhhhhhhX∼D(x) [tr] h

X X   
 [e] [e]N N

E E **_a[⊤]i_** _CiACjaj + σa[2]hhhhhhhtr_ _C(α)[⊤]C(αh)_

_−_ _K[1]_ _{ai}i[N]=1[∼D][(][a][)]_ _X∼D(x)_  _i=1_ _j=1_

X X   

_N_ _N_  [e] [e] _N_ _N_

= [1] E **_a[⊤]i_** _CiACjaj_ E **_a[⊤]i_** _CiACjaj_

_K_ _X∼D(x)_  _i=1_ _j=1_ _−_ _{ai}i[N]=1[∼D][(][a][)]_ _i=1_ _j=1_ 

X X X X
 _N_ _N_ [e] [e] _N_ _N_ [e] [e] 

= [1] E **_u[⊤]i_** _[A][u][j]_ E **_u[⊤]i_** _[A][u][j]_

_K_ _X∼D(x)_  _i=1_ _j=1_ _[−]_ _{ai}i[N]=1[∼D][(][a][)]_ _i=1_ _j=1_ 

X X X X
 


where ui = _Ciai = Ci[⊤][C][i][a][i][ is sub-gaussian random variable and with Assumption][ 1][,]_

Eui = Eai = 0 (C.8)

[e]

Let U[N ] = (u1; ...; uN ), ∈ R[Nd], we write the quadratic form into a bilinear form for each product
term u[⊤]i _[A][u][j]_


_. . ._
...
_. . ._


**_u1_**
.
.
.
**_uN_**


. . .

_U[[⊤]N_ ]AU[N ] = (u[⊤]1 _[, ...,][ u]N[⊤]_ [)] .. ... .. .. = **_u[⊤]i_** _[A][u][j]_ (C.9)

_A_ _. . ._ _A_ **_uN_** Xi Xj

   

[e]    

where
_A = 1N_ 1[⊤]N _A_ R[Nd][×][Nd] (C.10)

_[⊗]_ [A][,][ ∥] [e]∥∈

is a N × N block matrix and ⊗ is Kronecker product. And the relations ofe _A and_ _A[˜] are_

_A_ = N _A_ _,_ _A_ _HS = N_ [2][ X] _A[2](i,j)_ (C.11)
_∥_ [e]∥ _∥_ _∥_ _∥_ [e]∥

_i,j_


-----

By applying Hanson-Wright inequality we have

1
Pr _Lm(wm, α, K) −_ _L¯m(α, N, K)_ _> t_ = Pr _K_ [N ]AU[N ] − EU[[⊤]N ]AU[N ] _[> t]_
 
   (C.12)

_t[2]_ _t_

_≤2 exp_ "−[U]c min[ ⊤] [e] _L[4]_ _A_ HS _,_ _L[e][2]_ _A_ !#

_∥_ [e]∥[2] _∥_ [e]∥

Further with Cauchy Inequality, we can get the following equation

1 1

_N_ _−_ _N_ _−_

_A_ = _Ck[⊤][C][k]_ _C(α)[⊤]C(α)_ _Ck[⊤][C][k]_
_∥_ _∥_ _k=1_ ! _k=1_ !

X X

1 2 (C.13)

_N_ _−_

_Ck[⊤][C][k]_ _C(α)[⊤]C(α)_

_≤_ _k=1_ !

X

According to Theorem 1, all eigenvalues of second term falls in4αλ[2]I _[/K][ + 4][α][2][λ][3]S[/K]_ [2][]][ and of order][ 1][/N][ for first term.] [λI − 4αλ[2]S[/K][ + 4][α][2][λ][3]I _[/K]_ [2][, λ][S][ −]

(λS 4αλ[2]I _[/K][ + 4][α][2][λ][3]S[/K]_ [2][)]
_A_ _−_ (C.14)
_∥_ _∥≤_ _N_ [2](λI − 4αλ[2]S[/K][ + 4][α][2][λ]I[3][/K] [2][)][2]

Let ε(α, K) = (λS − 4αλ[2]I _[/K][ + 4][α][2][λ][3]S[/K]_ [2][)][/][(][λ][I][ −] [4][αλ][2]S[/K][ + 4][α][2][λ][3]I _[/K]_ [2][)][2]

_A_ = N _A_ (C.15)
_∥_ [e]∥ _∥_ _∥≤_ _[ε][(][α, K]N_ [)]

Next, we can boundupper bounded by _∥A∥HS by ∥A∥. It’s obvious that ∥A∥HS ≤_ rank(A)∥A∥. So ∥A∥HS[2] [can be]
p


1

_N_ _−_

_Ck(α)[⊤]Ck(α)_
_k=1_ !

X


_A_ _HS_
_∥_ _∥[2]_ _[≤]_ [rank][(][A][)][∥][A][∥][2][ ≤] [rank][(][A][)]


_C(α)[⊤]C(α)_


(C.16)


_≤_ [rank][(][A]N[)][ε][4][2][(][α, K][)] _≤_ _[dε][2]N[(][α, K][4]_ [)]


Thus, the ∥A[e]∥HS[2] [will no more than]

_∥A[e]∥HS[2]_ _[≤]_ _[ε][2][(][α, K][)][ d]N_ [2] (C.17)

In summary, we can get the bound

_t[2]N_ [2] _tN_
Pr _m(wm, α, K)_ ¯m(α, N, K) _> t_ 2 exp _c min_
_L_ _−_ _L_ _≤_ _−_ _L[4]dε[2](α, K)_ _[,]_ _L[2]ε(α, K)_
  
   (C.18)

Finally, we rewrite the inequality by eliminating t, we have at least 1 − _δ,_

_dε(α, K)_

_Lm(wm, α, K) −_ _L¯m(α, N, K)_ _≤_ _[L]K[2]_ [max] (r _N_ [2] log [2]δ [, ε][(][α, K]N [)] log [2]δ ) (C.19)


D PROOF OF COROLLARY 1

_Proof. Recall our estimation of α[∗]_ is given by,

_αlim[∗]_ [= arg min]α _[L]m[apx][(][α][) =][ K]2 tr[[ tr[]E[E]X[X][(Φ([[(Φ(]X[X])[)][⊤][⊤]Φ([Φ(]X[X]))[))][3][2]]][]]]_ (D.1)


-----

For each task, we have K samples with d dimensional features Φ(X) ∈ R[K][×][d]. Since Φ(X)[⊤]Φ(X)
is positive definite matrix, by applying spectral decomposition, we have

tr EX [(Φ(X)[⊤]Φ(X))[2]] =EX tr[(Φ(X)[⊤]Φ(X))[2]]


=EX tr[(U ΣX _U_ _[⊤])(U_ ΣX _U_ _[⊤])]_

= tr EX [Σ[2]X []]


(D.2)


where U is an orthogonal matrix and ΣX is the a diagonal matrix filled by eigenvalues λ1, ..., λd of
the covariance matrix of the feature. It’s easy to prove in Principle Component Analysis (PCA) that
tr E(ΣX ) is the variance of features where

_σ[2](ϕ(x1), . . ., ϕ(xK)) = [1]_

_K_ [tr][ E][X] [[(Φ(][X][)][ −] _[µ][)][⊤][(Φ(][X][)][ −]_ _[µ][)]]_


= [1]


_K_

(ϕ(xi) _µ)[2]_ = [1]
_−_ _K_
_i=1_

X


(D.3)


_λi_
_i=1_

X


= [1]

_K_ [tr][ E][(Σ][X] [)]

where ϕ(xi) ∈ R[d] is each row of Φ(X) and µ is zero.

With Jensen’s inequality, we have
1
_X_ [)][ ≤] [[tr][ E][(Σ][X] [)]][p][,][ (][p][ ≥] [1)] (D.4)
_d_ [[tr][ E][(Σ][X] [)]][p][ ≤] [tr][ E][(Σ][p]

Thus, we can write the inequalities
_K[tr EX_ (ΣX )][2]

2d[tr EX (ΣX )][3][ ≤] _[K]2 tr[[[tr]E[ E]X[X][(Φ([[(Φ(]X[X])[)][⊤][⊤]Φ([Φ(]X[X]))[))][3][2]]][]]]_ 2[tr EX (ΣX )][3]]

_[≤]_ _[Kd][[tr][ E][X]_ [(Σ][X] [)]][2] (D.5)

_K_ _Kd_

2d tr EX (ΣX ) 2 tr[EX [(Φ(X)[⊤]Φ(X))[3]]] 2 tr EX (ΣX )

_[≤]_ _[K][[tr][ E][X]_ [[(Φ(][X][)][⊤][Φ(][X][))][2][]]] _[≤]_

thereby


1

2dσ[2](ϕ(x1), . . ., ϕ(xK)) _[≤]_ _[α]lim[∗]_ _[≤]_

EXAMPLES


_d_

(D.6)
_σ[2](ϕ(x1), . . ., ϕ(xK))_


**Example 1 (Normal, Polynomial feature)and a is a random vector from zero-mean distribution. Consider polynomial basis functionAssume we have K i.i.d samples x1, ..., xK ∼N ϕ :(0 R, σ →[2])**
R[d], where ϕ(y) = (1, ..., y[d][−][1]).


_x1_ _. . ._ _x[d]1[−][1]_
. . .
. . .
. . .
_xK_ _. . ._ _x[d]K[−][1]_


_ϕ(x1)_

.
.
.

_ϕ(xk)_


(E.1)


Φ(X) =


. . . . . . .
. . . = . . . . (E.1)
. . . . . . .

_ϕ(xk)_ 1 _xK_ _. . ._ _x[d]K[−][1]_

  
  

_αlim[∗]_ [=][ Ktr][[][E][X] [[(Φ(][X][)][⊤][Φ(][X][))][2][]]] (E.2)

2tr[EX [(Φ(X)[⊤]Φ(X))[3]]]


Since we have

So that


2

+ . . . +

!


2
!


_x[(]i[j][−][1)+0]_
_i=1_

X


_x[(]i[j][−][1)+][d]_
_i=1_

X


_tr[EX_ [(Φ(X)[⊤]Φ(X))[2]]] =Ex

=Ex

=K


_j=1_

_d_

_j=1_

X


2

_K_

_x[j]i_ [+][m][−][2]
_i=1_ !

X


(E.3)


_m=1_


E[x[2(][j][+][m][−][2)]] + (K − 1)K

_j=1_ _m=1_

X X


E[2][x[(][j][+][m][−][2)]]
_m=1_

X


_j=1_


-----

Similarly, the denominator is

_tr[EX_ [(Φ(X)[⊤]Φ(X))[3]]]

_d_ _d_ _d_

=Ex

"

X X X


_K_ _K_ _K_

_x[j]i_ [+][m][−][2] _x[j]i[′][+][l][−][2]_ _x[j]t[+][l][−][2]_
_i=1_ ! i[′]=1 !# " t=1

X X X


!#


_j=1_

_d_

_j=1_

X

_d_

_j=1_

X


_l=1_

_d_

_l=1_

X

_d_

_l=1_

X


_m=1_


_K_

_x[j]t[+][l][−][2]_

" t=1
X


!#


_x0[2][j][+][l][−][3]_ + . . . + x[j]i [+][d][−][2]x[j]i[′][+][l][−][2] _x[j]t[+][l][−][2]_

=1 " t=1 !#

X _dK[2]_ X

 
 

_d_ | {z } _K_

x[2]i _[j][+][m][+][l][−][4]_ + . . . + _x[j]i_ [+][d][−][2]x[j]i[′][+][l][−][2] + . . . _x[j]t[+][l][−][2]_

=1 " t=1 !#

X _dK_ _dK(K_ 1) X

 _−_ 

_d_  

| {z } | {z }

_KE[x[3][j][+][m][+2][l][−][6]] + 3K(K −_ 1)E[2][x[2][j][+2][l][−][4]]E[x[j][+][m][−][2]]

_m=1_

X  


=Ex

=Ex


(E.4)

(E.5)


_j=1_


_l=1_


+(K [3] _−_ 3K [2] + 2K)E[3][x[(][j][+][m][+][l][−][3)]]


If K = 1, σ → 0 the optimal αlim[∗] [will be]

_d_ _d_
_j=1_ _m=1_ [E][[][x][2(][j][+][m][−][2)][]]

_αlim[∗]_ [=] 2 _jP=1_ _dlP=1_ _dm=1_ [E][[][x][3][j][+][m][+2][l][−][6][]]

2i=0d−2[[][C]P[(][i][ + 1]P[,][ 1)][ −] [2][C][(][i][ −] _[d][ + 1][,][ 1)]][E][[][x][2][i][]]_
= [P][d] (E.5)
P 2 _j=1_ _[g][2][(][d, j][)][E][[][x][j][]]_

_i2=0d−2_ _[g][1][(][d, i][)][σ][2][i][(2][i][ −]_ [1)!!] 1
= [P][3][d] =

2P _j=0_ _[g][2][(][d, j][)][σ][2][j][(2][j][ −]_ [1)!!] _O_  _σ[2]_ 

_n_ [P][3][d][−][3]
where C(n, k) = _k_ is the binomial coefficient and g1(d, i) = C(i + 1, 1) − 2C(i − _d + 1, 1)._
 

If K →∞, σ → 0

_αlim[∗]_ [=] 2 _j=1_ (Kdl=1 − 1)Kdm=1[2][ P][(][K][d]j=1[3][ −]P[3]dm[K]=1[2][ + 2][E][2][[][K][x][(][)][j][E][+][3][m][[][x][−][(][2)][j][+][]] _[m][+][l][−][3)][]]_

_d_ _d_
_j=1P_ _mP=1_ [E][2][[][x][(][j][+][m][−][2)][]]

= [P]d _[d]_ _d_ _d_ (1)

_O_
_j=1P_ _l=1P_ _m=1_ [E][3][[][x][(][j][+][m][+][l][−][3)][]]

(E.6)

P 2i=0d−2P[g][1][(][d, i]P[)][E][2][[][x][i][]]

= 3d 3 (1)

_−_ _O_

Pj=0 _[g][3][(][d, j][)][E][3][[][x][j][]]_

P _di=1−1_ _[g][1][(][d, i][)[][σ][2][i][(2][i][ −]_ [1)!!]][2] 1

=

_⌈jP3=1d/2⌉−1_ _g3(d, j)[σ[2][j](2j_ 1)!!][3][ O][(1) =][ O]  _σ[2]_ 
_−_
P

We show the coefficients of each moment in the Figure 6. As we can see, denominator becomes
dominant since the coefficient of every moment, number of terms and order of moment are all larger
(higher) than numerator.

So in this case, the αlim[∗] [has an inverse relationship with][ σ][2][.]

**Example 2 (Random Matrices)** Assume all elements Yij in feature matrix are independent. Then
let Y be a random matrix we have


-----

800 g1 1750 g1
g2 1500 g3

600

1250

1000

400

750

200 500

250

0 0

0 50 100 150 200 250 300 0 20 40 60 80 100 120 140

index i, (d=50) index i, (d=50)


Figure 6: Example of d = 50 with coefficients of each moment given by g1(d, i), g2(d, i), g3(d, i).


_Ki=1_ _[Y][ 2]i1_ _. . ._ _Ki=1_ _[Y][i][1][Y][id]_
. . .

Φ(X)[⊤]Φ(X) =  P .. .. P .. 

 _Ki=1_ _[Y][id][Y][i][1]_ _. . ._ _Ki=1_ _[Y][ 2]id_ 

For the numerator in (D.1),  

P P

_d_ _d_ _K_ _K_

_tr[Ex[(Φ(X)[⊤]Φ(X))[2]]] =EYij_ _,i∈[K],j∈[d]_ _YitYis_ _Yi′tYi′s_

_t=1_ _s=1_ _i=1_ ! i[′]=1

X X X X


(E.7)

(E.8)

(E.9)

(E.10)

(E.11)


=dKE[Y [4]] + (dK(K − 1) + d(d − 1)K)E[2][Y [2]]

+ d(d − 1)K(K − 1)E[4][Y ]

Here, the row m column n entry of (Φ(X)[⊤]Φ(X))[2] is

_d_ _K_ _K_

(Φ(X)[⊤]Φ(X))[2](mn) [=] _YimYis_ _Yi′nYi′s_

_s=1_ _i=1_ ! i[′]=1 !

X X X

The diagonal entry of (Φ(X)[⊤]Φ(X))[3] at row m will given by

_d_ _d_ _K_ _K_ _K_

(Φ(X)[⊤]Φ(X))[3](mm) [=] _YimYis_ _Yi′nYi′s_ _YjmYjs_

_n=1_ s=1 _i=1_ ! i[′]=1 ! []j=1 

X X X X X

Similarly, the denominator is   
_tr[Ex[(Φ(X)[⊤]Φ(X))[3]]]_

_d_ _d_ _d_ _K_ _K_ _K_

=EYij _,i∈[K],j∈[d]_ _m=1_ _n=1_ s=1 _i=1_ _YimYis! i[′]=1_ _Yi′nYi′s! []j=1_ _YjmYjs_

X X X X X X

= KdE[Y [6]] + Kd(d − 1)(E[Y [5]]E[Y ] + E[Y [4]]E[Y [2]] + E[2][Y [3]]) 
_i=i[′]=j_
|+ b1 + K(K 1)d(d 1)(E[{zY [3]]E[3][Y ] + E[Y [3]]E[Y [2]]E[Y ] +} E[2][Y [2]]E[2][Y ])

_−_ _−_
_i=i[′]≠_ _j_

+ b|1 + K(K 1)d(d 1)(E[Y [3]]E[3][Y{z ] + E[Y [3]]E[Y [2]]E[Y ] + E[2][Y [2]]E[2][Y ])}
_−_ _−_
_i≠_ _i[′]=j_

+ b|1 + K(K 1)d(d 1)(E[Y [4]]E[2][Y{z ] + E[3][Y [2]] + E[2][Y [2]]E[2][Y ]) }
_−_ _−_
_i=j≠_ _i[′]_

+ b|2 + (K [3] 3K [2] + 2K)d(d 1)({z E[6][Y ] + E[2][Y [2]]E[2][Y ] + E[Y [2]}]E[4][Y ])
_−_ _−_
_i≠_ _i[′]≠_ _j_
| {z }


-----

where b1 = K(K − 1)dE[Y [4]]E[Y [2]], b2 = (K [3] _−_ 3K [2] + 2K)dE[3][Y [2]].

If K = 1, the optimal αlim[∗] [will be]

_αlim[∗]_ [=][ d][E][[][Y][ 4][]] (E.12)

2dE[Y [6]] [=][ E]2E[[][[Y]Y[ 4][6][]]]


If K →∞ and d →∞,

_K(dK(K_ 1) + d(d 1)K)E[2][Y [2]] + d(d 1)K [2](K 1)E[4][Y ]
_αlim[∗]_ [=] _−_ _−_ _−_ _−_

(K [3] _−_ 3K [2] + 2K)[dE[3][Y [2]] + d(d − 1)(E[6][Y ] + E[2][Y [2]]E[2][Y ] + E[Y [2]]E[4][Y ])]

_dE[2][Y_ [2]] + d(d 1)E[4][Y ]
= _−_

_dE[3][Y_ [2]] + d(d − 1)(E[6][Y ] + E[2][Y [2]]E[2][Y ] + E[Y [2]]E[4][Y ]) _[O][(1)]_

E[4][Y ]
=

E[6][Y ] + E[2][Y [2]]E[2][Y ] + E[Y [2]]E[4][Y ] _[O][(1)][ ≈]_ [E]E[[][[Y]Y[ 4][6][]]] _[O][(1)]_


(E.13)


Both two examples are related to the high order moments of data distributions. In polynomial feature
example, we focus on the gaussian distributed data and its inverse relationship to data variance. As
for any random matrix, it depends on the fourth moment over sixth moment.

F PROPOSITION FOR THEOREM 2

**Proposition 5. ∃ε, α ∈** [−ε, ε] global minimum of MAML wm(α) is given by following equation

1

_N_ _−_ _N_

4

**_wm(α) =wr + α_** Φ(Xi)[⊤]Φ(Xi)

_i_ ! [] _j_ _K_ [(Φ(][X][j][)][⊤][Φ(][X][j][))][2][ (][w][s][ −] **_[a][j][)]_**

(F.1)

X X

_α_  

_α[w]m[0]_ [(][ξ][)]

+ _∇[2]_ (α _ξ)[2]dξ_

0 2! _−_

Z

_where wr is ERM global minimum._

_Proof. As for the MAML, the global minimum is,_

1

_N_ _−_ _N_

**_wm(α) =_** _Ci[⊤][C][i]_ _Cj[⊤][C][j][a][j]_ (F.2)

_i=1_ ! []j=1 

X X

 

with Ci(α) = Φi − [2]K[α] [Φ][i][Φ]i[⊤][Φ][i][, C][i][(][α][)][ ∈] [R][K][×][d][.]

Let Wα = _i=1_ _[C]i[⊤][(][α][)][C][i][ and][ ν][α][ denote][ P]j[N]=1_ _[C]j[⊤][C][j][a][j][, then]_

[P][N] _N_

_⊤_

_αWα =_ _α_ Φi _i_ [Φ][i] Φi _i_ [Φ][i]
_∇_ _∇_ " _i=1_  _−_ [2]K[α] [Φ][i][Φ][⊤]   _−_ [2]K[α] [Φ][i][Φ][⊤] [#]

X (F.3)

_N_

= _i_ [Φ][i][)][2][ + 8][α] _i_ [Φ][i][)][3]

" _i=1_ _−_ _K[4]_ [(Φ][⊤] _K_ [2][ (Φ][⊤] #
X


Similarly, we have


_N_

_i_ [Φ][i][)][2][a][i] [+ 8][α] _i_ [Φ][i][)][3][a][i]

" _i=1_ _−_ _K[4]_ [(Φ][⊤] _K_ [2][ (Φ][⊤]
X


(F.4)


_∇ανα =_


-----

For first-order derivative, we have the following form,

_∇αwm(0) =Wα[−][1][|][α][=0]_ [[][∇][α][ν]1[α][|][α][=0] _[−∇][α][W][α][|][α][=0][w][m][(0)]]_

_N_ _−_ _N_ _N_

4

= Φ[⊤]i [Φ][i] _j_ [Φ][j][)][2][a][j] [+] _l_ [Φ][l][)][2][w][m][(0)]

_i=1_ ! []j=1 _−_ _K[4]_ [(Φ][⊤] _l=1_ _K_ [(Φ][⊤]

X X X

_N_ _−1_  _N_

4

= Φ[⊤]i [Φ][i] _j_ [Φ][j][)][2][(][w][m][(0)][ −] **_[a][j][)]_**

_i=1_ ! []j=1 _K_ [(Φ][⊤] 

X X

 


(F.5)

(F.6)

(F.7)


Recall that


1
_−_ _N_

Φ[⊤]j [Φ][j][a][j]

! []j=1

X



Φ[⊤]i [Φ][i]
_i=1_

X


**_wm(0) = ws =_**


So for small α, we have the Taylor expansion at zero that


_α_
**_wm(α) =wm(0) +_** _αwm(0) +_
_∇_ 0
Z


_α_

_α[w]m[0]_ [(][ξ][)]

**_wm(0) +_** _∇[2]_ (α _ξ)[2]dξ_

0 2! _−_

Z

1
_−_ _N_

4

Φ(Xi)[⊤]Φ(Xi)

_K_ [(Φ(][X][j][)][⊤][Φ(][X][j][))][2][ (][w][s][ −] **_[a][j][)]_**

! [] _j_

X



=wr + α


_α_

0

Z


_α[w]m[0]_ [(][ξ][)]
_∇[2]_ (α _ξ)[2]dξ_

2! _−_


G PROOF OF THEOREM 2

**Proof Sketch** We list our proof steps as follows

1. Based on Definition 1, our target is to illustrate that fast adaptation distance gap between
**_wm and wr is always negative which means MAML has smaller distance to all the tasks at_**
any adaptation steps in expectation.

2. We first get the linearized expression of wm by Proposition 5.

3. Compute fast adaptation distance gap ∆ = ET1,...,TN (T )Ft(wm) _t(wr) across same_
_∼D_ _−F_
task distribution D(T ) and take expectations with respect to all random variables.

4. With lemma of trace inequalities and assumptions, we can get the upper bound for the
dominant term of ∆, refer to (G.30).

5. Bound the reminder terms, we can get the range of α.


**Notation for this proof Follow the Theorem 1, we omit the arguments of the function if its symbol**
has a index e.g. ΦT = Φ(XT ). Covariance matrix Φ[⊤]T [Φ][T][ =][ G][T][ and inverse of sum covariance]
matrix V = ([P]i [N ] [Φ]i[⊤][Φ][i][)][−][1][ for short.]

_∈_

_Proof. For each task T sampled from distribution D(T_ ), gradient descent iteration yields


**_wT[t][+1]_** = wT[t] _[−]_ _[η][∇][ℓ][T]_ [(][w]T[t] [)]

= wT[t] _T_ [(Φ][T] **_[w][T]_** **_[a][T]_** [)]

_[−]_ [2]K[η] [Φ][⊤] _[−]_ [Φ][T]

= _I_ _T_ [Φ][T] **_wT[t]_** [+ 2][η] _T_ [Φ][T] **_[a][T]_**
_−_ [2]K[η] [Φ][⊤] _K_ [Φ][⊤]
 

_t+1_ _t+1_

2η

= _I_ _T_ [Φ][T] **_w[0]_** + _I_ _T_ [Φ][T]
_−_ [2]K[η] [Φ][⊤] _K_ _−_ [2]K[η] [Φ][⊤]
  _j=1_ 

X


(G.1)


_j−1_
Φ[⊤]T [Φ][T] **_[a][T]_**



-----

where w[0] is the initialization. Let GT = Φ[⊤]T [Φ][T][, the adapted error will be]


_t_ _t_

2η

**_wm[0]_** [+]

_K_

 _j=1_

X

_t_

2η

_, ST =_

_K_

_j=1_

X


_j−1_

_I −_ [2]K [η] _[G][T]_ _GT aT −_ **_aT_**
 


_I −_ [2]K [η] _[G][T]_


**_wT[t]_**
_∥_ _[−]_ **_[a][T]_** _[∥]_ [=]

For simplicity, let


(G.2)


_j−1_

_I −_ [2]K [η] _[G][T]_ _GT aT −_ **_aT_**




_QT =_ _I −_ [2]K [η] _[G][T]_



then with Definition 1, we can get t-step fast adaptation error for MAML,


_Ft(wm[0]_ [) =] _T ∼DE(T )_

and the ERM fast adaptation error is

_Ft(wr[0][) =]_ _T ∼DE(T )_

Note that the sum of geometric series in ST is

_t_ 2η _j−1_

_ST =_ _I_ _GT aT_ **_aT =_**

_j=1_ _K_  _−_ [2]K [η] _[G][T]_  _−_

X


_Q[t]T_ **_[w]m[0]_** [+][ S][T] (G.3)

[2]

_Q[t]T_ **_[w]r[0]_** [+][ S][T] (G.4)

[2]


_t_
_−_ _I_



"I − I − [2]K [η] _[G][T]_


**_aT =_** _Q[t]T_ **_[a][T]_** [(G.5)]
_−_


Now, let’s focus on the error gap of MAML and ERM, denoted as ∆, then we have

**∆** = E _m[)][ −F][t][(][w]r[0][)]_
_T1,...,TN_ _∼D(T )_ _[F][t][(][w][0]_

(G.6)

= E _Q[t]T_ **_wm[0]_** [+][ w]r[0] + 2ST, Q[t]T **_wm[0]_** _r_
_T1,...,TN_ _,T ∼D(T )_      _[−]_ **_[w][0] []_**


For small α, we get its linear expansion in Proposition 5


_α_
**_wm[0]_** [(][α][) =][ w]r[0] [+][ α][∇][α][w]m[0] [(0) +]

0

Z

_N_ _−1_

= wr[0] [+][ α] _Gj_

 _j_  []

X
  


_α[w]m[0]_ [(][ξ][)]
_∇[2]_ (α _ξ)[2]dξ_

2! _−_


(G.7)


+ R1






4

_j_ **_wr[0]_**
_K [G][2]_ _[−]_ **_[a][j]_**
 


where R1 = _α0_ _∇α[2]_ **_[w]2!m[0]_** [(][ξ][)] (α _ξ)[2]dξ is the reminder term. So it would be_

_−_

R

**∆** = E E _Q[t]T_ 2wr[0] [+][ α][∇][α][w]m[0] [(0) +][ R][1] + 2ST, αQ[t]T _m[(0)]_ (G.8)
**_wm[0]_** _[,][w]r[0]_ _T ∼D(T )_ _[∇][α][w][0]_ 

1   
_N_ _−_
Let V = _j_ _[G][j]_ and the first derivative _αwm[0]_ [(0)][ is split into]
_∇_
P  _N_ _N_

4 4

_α∇αwm[0]_ [(0) =][ V]  _K [G]j[2][w]r[0]_ _−_ _V_  _K [G]j[2][a][j]_ (G.9)

_j_ _j_

X X

thus inner product will be four product terms and a reminder term which is   

_N_ _N_

**∆** = [8][α] E E _Q[t]T_ **_[w]r[0][, Q][t]T_** _[V]_ _G[2]j_ **_[w]r[0]_** _Q[t]T_ **_[w]r[0][, Q][t]T_** _[V]_ _G[2]j_ **_[a][j]_**

_K_ _{ai}[N_ ] _T ∼D(T )_ *  _j_ + _−_ -   _j_ +

 X X

_N_  N  



+ _ST, Q[t]T_ _[V]_ _G[2]j_ **_[w]r[0]_** _ST, Q[t]T_ _[V]_ _G[2]j_ **_[a][j]_** 1

-   _j_ + _−_ -   _j_ +[]

X X 
    (G.10)

 [+][ R][2][ +][ R][′]


-----

where the remainder terms are

_R2 = α[2]_ _∇αwm[0]_ [(0)][,][ ∇][α][w]m[0] [(0)] _,_ _R1[′]_ [=][ α][2] _R1, ∇αwm[0]_ [(0)] (G.11)
  

Now, let’s look at the expectation. Recall that task T is defined by random variables (Φ(XT ), aT ).
With simultaneously diagonalizable assumption, all feature covariance matrix in tasks can be factorized to

Φ(XT )[⊤]Φ(XT ) = GT = U ΣT U _[∗]_ (G.12)

where U is the basis of features, ΣT is the only random variable of GT . So the data of each task
depends on eigenvalue diagonal matrix ΣT . So taking expectation over T means the two independent
expectation EΣT (Σ), EaT (a). Similarly, wr[0] [in previous section is expressed by]
_∼D_ _∼D_

1

_N_ _−_ _N_

**_wr[0]_** [=] _Gi_ _Gjaj_

_i=1_ ! []j=1  (G.13)

X X

 

ET1,...,TN (wr[0][) =][E] **_ai_** [N ] [E] _Gi_ [N ] [(][w]r[0][) =][ E] **_ai_** [N ] [E] Σi [N ] [(][w]r[0][)]
_⇒_ _{_ _}_ _{_ _}_ _{_ _}_ _{_ _}_

For each product term in (G.10), we list four main terms as following. First term is

_N_

E _Q[t]T_ **_[w]r[0][, Q]T[t]_** _[V]_ _G[2]j_ **_[w]r[0]_**
_T1,...,TN_ _,T ∼D(T )_ -  _j_ +

X

_N_ _N_ _N_

= _{ai}[NE]∼D(a)_ _{GiE}[N_ ] *Q[t]T _[V]_ _i_ _Giai!_ _,_ [4]K [α] _[Q]T[t]_ _[V]_ _j_ _G[2]j_ _[V]_ _k_ _Gkak!+_ (G.14)

X X X

_N_ _N_

= E tr _GiV Q[t]T_ _[Q]T[t]_ _[V]_ _G[2]j_ _V Gi_
_{Gi}[N_ ]  _i=1_  _j_  

X X
   

Similarly, we can get the second term

_N_ _N_

E _Q[t]T_ **_[w]r[0][, Q][t]T_** _[V]_ _G[2]j_ **_[a][j]_** = E E _Q[t]T_ **_[w]r[0][, Q][t]T_** _[V]_ _G[2]j_ **_[a][j]_**
_{Ti}[N_ ],T ∼D(T ) -  _j_ + _{ai}[N_ ] _{Gi}[N_ ] -  _j_ +

X X (G.15)

_N_

= E tr _GiV Q[t]T_ _[Q]T[t]_ _[V G]i[2]_
_{Gi}[N_ ] " _i=1_ #

X

For third and fourth terms, EaT (a) is a marginal expectation of ET (T ), thus
_∼D_ _∼D_


_ST, Q[t]T_ _[V]_


_G[2]j_ **_[w]r[0]_**


_−Q[t]T_ **_[a][T]_** _[, Q][t]T_ _[V]_


_G[2]j_ **_[w]r[0]_**


_Ti_ [N ],T (T )
_{_ _}_ _∼D_


**_aT_** (a) **_wr[0][,][{][G][i][}][[][N]_** []]
_∼D_


=0
(G.16)
Similarly, the fourth term is


_ST, Q[t]T_ _[V]_


_G[2]j_ **_[a][j]_**


= 0 (G.17)


**_a[N_** ] (a) _T_ (T )
_∼D_ _∼D_


-----

So overall, the we care about above four terms as a function of α, N, t, ... denoted as δt(α, N )

_δt(α, N_ ) =∆ _R2_ _R1[′]_
_−_ _−_


= [8][α]


_G[2]j_ _V Gi_

 

 

#

_G[2]j_ _V Gi_

_N_  _−_


E tr _GiV Q[t]T_ _[Q]T[t]_ _[V]_
_i}[N_ ]  _i=1_  _j_

X X
 _N_ 

E tr _GiV Q[t]T_ _[Q]T[t]_ _[V G]i[2]_
_{Gi}[N_ ] " _i=1_

X


E tr
_Gi_ [N ]
_{_ _}_


_−_ [8]K[α]


= [8][α]

_K_

= [8][α]

_K_

= [8][α]


(G.18)


_GiV Q[t]T_ _[Q]T[t]_ _[V]_
_i=1_

X

_N_

_GiV Q[t]T_ _[Q]T[t]_ _[V]_
_i=1_

X

_N_

_V Q[t]T_ _[Q]T[t]_ _[V]_



_i=1_

X




_GiV Q[t]T_ _[Q]T[t]_ _[V G]i[2]_
_i=1_

X


E tr
_Gi_ [N ]
_{_ _}_

E tr
_Gi_ [N ]
_{_ _}_

E tr
_Gi_ [N ]
_{_ _}_


_G[2]j_


_V Gi_ _G[2]i_

 _−_


_G[2]j_


_V G[2]i_ _i_

 _[−]_ _[G][3]_


By simultaneously diagonalizable assumption, we have


_δt(α, N_ ) = [8][α]

_K_

= [8][α]


(U Σ[2]j _[U][ ⊤][)(][U][ b]ΣN_ _U_ _[⊤])(U_ Σ[2]i _[U][ ⊤][)][ −]_ [(][U] [Σ]i[3][U][ ⊤][)]

_N_


_V Q[t]T_ _[Q]T[t]_ _[V]_
_i=1_

X


E tr
Σi [N ]
_{_ _}_

E tr
Σi [N ]
_{_ _}_


= [8][α] E tr _V Q[t]T_ _[Q]T[t]_ _[V]_ _U_ Σ[2]i ΣN Σ[2]j _[U][ ⊤]_ _[−]_ _U_ Σ[3]i _[U][ ⊤]_

_K_ _{Σi}[N_ ]   _i_ _j_ _i_ 

X X X

 _−1_  [b]  (G.19)

where ΣN = _k∈[N_ ] [Σ][k] is a PD matrix and Q[t]T [= (][I][ −] [(2][η/K][)][G][T][ )][t][ is a exponential decay]
term w.r.t η. With probability 1,P  _λsI ⪯_ Σi ⪯ _λxI_

[b]

(Nλx)[−][1]I ΣN (Nλs)[−][1]I (G.20)
_⪯_ [b] _⪯_

Note that V Q[t]T _[Q]T[t]_ _[V][ = (][Q]T[t]_ _[V][ )][⊤][Q]T[t]_ _[V][ is a symmetric positive definite matrix where]_

_V Q[t]T_ _[Q]T[t]_ _[V][ =][U][ b]ΣN_ _U_ _[⊤]Q[2]T[t][U][ b]ΣN_ _U_ _[⊤]_

2t

=U ΣN _U_ _UU_ _U_ ΣN _U_

_[⊤]_ _[⊤]_ _−_ [2]K [η] _[U]_ [Σ][T][ U][ ⊤] _[⊤]_
 

2t (G.21)

=U Σ[b] _N_ _U_ _U_ _I_ _U_ _U_ ΣN _U[b]_

_[⊤]_ _−_ [2]K[η] [Σ][T] _[⊤]_ _[⊤]_
 

2t

=U Σ[b] _N_ _I_ ΣN _U_ [b]

_−_ [2]K[η] [Σ][T] _[⊤]_
 

Note that m-th diagonal entry of E{[b]Σi}[N ] _Ni_ _Nj_ _[U]_ [Σ]bi[2]ΣN Σ[2]j _[U][ ⊤]_ _[−]_ [P][N]i _[U]_ [Σ]i[3][U][ ⊤][] is

(Eλ2)2 P P

_⃗e[⊤](m)_ _NEλ_ _NEλ_ _⃗e(m) = [(][Eλ][2][)]NEλ[2][b][ −]_ _[EλEλ][3]_ _∥⃗e(m)∥[2][ (C-S)]≤_ 0 (G.22)

 _[−]_ _[EλEλ][3]_ 

where (C-S) is according to Cauchy-Schwarz inequality for integrals in Proposition 3.

So the above matrix is a negative definite matrix. Now, let us can derive following trace inequality
for NSD and PSD.

**Proposition 6. If A is a n-by-n negative definite matrix and B is a n-by-n PSD matrix, we have**

_tr(AB)_ _λmin(B)tr(A)_ (G.23)
_≤_


-----

_Proof. By Ruhe’s trace inequality (Lemma 4), we have tr(AB) ≥_ [P]i _[λ][i][(][A][)][λ][n][−][i][+1][(][B][)][. Eigen-]_

values of A are negative where λi(A) < 0, ∀i ∈ [n]. So we have tr(AB) ≤ _λmin(B)_ _i_ _[λ][i][(][A][) =]_

_λmin(B)tr(A)_

[P]

So with Proposition 6, we have


_N_ _N_ _N_

_δt(α, N_ ) E _λmin_ _V Q[t]T_ _[Q]T[t]_ _[V]_ tr _U_ Σ[2]i ΣN Σ[2]j _[U][ ⊤]_ _[−]_ _U_ Σ[3]i _[U][ ⊤]_ (G.24)
_≤_ [8]K[α] _{Σi}[N_ ]  _i_ _j_ _i_ 

   X X X

 

with (G.20) and (G.21) [b]

2t 2t

1

_λmin_ _V Q[t]T_ _[Q]T[t]_ _[V]_ = λmin ΣN _I_ ΣN = _I_ (G.25)

 _−_ [2]K[η] [Σ][T]  ! _N_ [2]λ[2]x  _−_ [2][ηλ]K _[x]_ 

  

b b

For the second part, we have

1

_N_ _N_ _N_ _N_ _N_ _N_ _−_ _N_

tr _U_ Σ[2]i ΣN Σ[2]j _[U][ ⊤][−]_ _U_ Σ[3]i _[U][ ⊤]_ = tr Σ[2]i Σk Σ[2]j Σ[3]i

 _i_ _j_ _i_ !  _i_ _j_ _k_ ! _[−]_ _i_ 

X X X X X X X
 [b] _N_ _−1_ _N_ _N_ _N_ 

= tr Σk Σ[2]j [Σ]i[2] Σ[3]i

 _k_ ! _i_ _j_ _[−]_ _i_ 

X X X X

 _N_ _−1[]_ _N_ _N_  _N_ _N_

_λmin_ Σk tr Σ[2]j [Σ]i[2] Σ[3]i [Σ][k]
_≤_  _k_ !  _i_ _j_ _[−]_ _i_ _k_ 

X X X X X

 _N_ _−1[]_  _N_ _N_ _N_ _N_ 

1
= Σk tr Σ[2]j [Σ]i[2] Σ[3]i [Σ][k]

_λxN_  _k_ !  _i_ _j_ _[−]_ _i_ _k_ 

X X X X X

   (G.26)

Overall, with probability 1, we have

2t _N_ _N_ _N_ _N_

8αd
_δt(α, N_ ) 1 E tr Σ[2]j [Σ]i[2] Σ[3]i [Σ][k] (G.27)
_≤_ _N_ [3]Kλ[3]x  _−_ [2][ηλ]K _[x]_  _{Σi}[N_ ]  _i_ _j_ _[−]_ _i_ _k_ 

X X X X
 

Specifically,

_N_ _N_ _N_ _N_

E tr Σ[2]j [Σ]i[2] Σ[3]i [Σ][k]
_{Σi}[N_ ]  _i_ _j_ _[−]_ _i_ _k_ 

X X X X
 

= tr N E(Σ[4]) _N_ E(Σ[4]) + (N [2] _N_ )E[2](Σ[2]) (N [2] _N_ )E(Σ[3])E(Σ) (G.28)

_−_ _−_ _−_ _−_

 _i=j_ _i=k_ _i≠_ _j_ _i≠_ _k_ 
 _d_ 
| {z } | {z } | {z } | {z }

= (N [2] _N_ ) [E(Σ[2])][2](i)
_−_ _i=1_ _[−]_ [[][E][(Σ][3][)]][(][i][)][[][E][(Σ)]][(][i][)]

X

With Proposition 3 we know that any eigenvalue λ = Σ(i) > 0 obeys the statistical condition
E[2][λ[2]] − E[λ[3]]E[λ]. Thus there exists a constant _c > 0 such that_

_N_ _N_ _N_ _N_

E tr Σ[2]j [Σ][2]i e Σ[3]i [Σ][k] _<_ _cd(N_ [2] _N_ ) (G.29)
_{Σi}[N_ ]  _i_ _j_ _[−]_ _i_ _k_  _−_ _−_

X X X X

Finally, we have   e

2t 2
8αd _c_ 1

_δt(α, N_ ) ≤− 1 − [2]K [η] _[λ][x]_ _Kλ[3]x_ _N_ _N_ [2] (G.30)
  e  _[−]_ [1] 


-----

Now, let’s bound the remainder terms R1[′] _[, R][2]_ [where]


_R1[′]_ [=] E E
**_a[N_** ] (a) _T_ (T )
_∼D_ _∼D_


_Q[t]T_ _[R][1][, αQ]T[t]_ _[V]_


4

_j_ **_wr[0]_**
_K [G][2]_ _[−]_ **_[a][j]_**
 


(G.31)


_R1 is the remainder term in Taylor expansion (G.7). With Integral Mean Value Theorem_


_α_
_R1 =_

0

Z


_α[w]m[0]_ [(][ξ][)]
_∇[2]_ (α _ξ)[2]dξ =_ _[α][2]_ _α[w]m[0]_ [(][ξ][′][)] (G.32)

2! _−_ 2 _[∇][2]_


We have the locally Lipschitz property for C _[∞]_ function wm(α), in a small region with small α such
that

_R1_ _α[w]m[0]_ [(0)] (G.33)
_≈_ _[α]2[2]_

_[∇][2]_

Then we have


1

_N_ _−_ _N_

_α[w]m[0]_ [(0) = 8] _Gk_ _G[3]i_ **_[a][i]_** _i_ **_[w]s[0]_** [+][ KG]i[2] _m[(][α][)]_
_∇[2]_ _K_ [2] _k=1_ ! _i=1_ _[−]_ _[G][3]_ _[∇][α][w][0]_ !

X X

1

_N_ _−_ _N_ _N_

= [8] _Gk_ _G[3]i_ _[G][j][a][i]_ _i_ _[G][j][w]s[0]_ [+ 4][G]i[2][G]j[2] **_wr[0]_**

_K_ [2] _k=1_ ! [] _i=1_ _j=1_ _[−]_ _[G][3]_ _[−]_ **_[a][j]_** 

X X X   

_N_ _−2_  _N_ _N_ 

= [8] _Gk_ (4G[2]i _[G]j[2][w]r[0]_ _i_ _[G][j][w]s[0][) +]_ _G[3]i_ _[G][j][a][i]_ _i_ _[G]j[2][a][j]_

_K_ [2] _k=1_ ! [] _i=1_ _j=1_ _[−]_ _[G][3]_ _[−]_ [4][G][2] 

X X X   

_N_ _−2_  _N_ _N_ 

= [8] _Gk_ (4G[2]i _[G]j[2]_ _i_ _[G][j][)(][w]r[0]_

_K_ [2] _k=1_ ! [] _i=1_ _j=1_ _[−]_ _[G][3]_ _[−]_ **_[a][j][)]_**

X X X

  (G.34)

Recall (above equation isF.6) the wr[0] [= (][P]i[N] [Φ]i[⊤][Φ][i][)][−][1][(][P]i[N][′][ Φ]i[⊤][′][ Φ][i][′] **_[a][i][′]_** [)][ while][ E][a][N ][∼D][(][a][)] **_[w]s[0]_** _[−]_ **_[a][i]_** [= 0][. The][ R]1[′] [in]


_R1[′]_ [=4][α][3] E

_K_ [3] _T_ (T ) [tr]
_∼D_


_j=1(4G[2]i_ _[G]j[2]_ _[−]_ _[G]i[3][G][j][)][V][ 2][Q]T[2][t][V G]j[2]_

X


_i=1_

_N_

_i=1_

X

_N_

_i=1_

X


= [4][α][3] E

_K_ [3] _T_ (T ) [tr]
_∼D_


= [4]K[α][3] _T ∼DE(T )_ [tr]  _i=1_ _j=1_ _U_ (4Σ[2]j [Σ][2]i [Σ]j[2] _[−]_ [Σ]j[2][Σ][3]i [Σ][j][)][U][ ⊤][U][ b]Σ[2]N _[Q][2]T[t]ΣN_ _U_ _[⊤]_

X X
 _N_ _N_ [b]

E (4Σ[2]i [Σ]j[4] _i_ [Σ]j[3][)] tr Σ[3]N _[Q][2]T[t]_

_≤_ [4]K[α][3][3] _T ∼D(T )_ [tr]  _i=1_ _j=1_ _[−]_ [Σ][3] 

X X h i

3  2t  _N_ bN
4α _d_

1 sup E tr (4Σ[2]i [Σ]j[4] _i_ [Σ]j[3][)]

_≤_  _N_ [3]K [3]   _−_ [2]K [η] _[λ][s]_ _i,j∈[N_ ] Σi,Σj  _i=1_ _j=1_ _[−]_ [Σ][3] 

X X
 


(G.35)

(G.36)


And the eigenvalues of Σi are bounded in [λs, λx] where


(4Σ[2]i [Σ]j[4] _i_ [Σ]j[3][)] = tr 3N EΣ[6] + (N [2] _N_ )E(4Σ[2]Σ[4] Σ[3]Σ[3])
_j=1_ _[−]_ [Σ][3]  _−_ _−_

X 



_≤_ _d_ 3Nλ[6]x [+ (][N][ 2][ −] _[N]_ [)(4][λ]x[6] _[−]_ _[λ]s[6][)]_

_dN_ [2](4λ[6]x _s[)]_ 
_≤_ _[−]_ _[λ][6]_


E tr
Σi,Σj


_i=1_


-----

In summary,

2t

_x_ _s[)]_
_R1[′]_ _[−]_ _[λ][6]_ 1 (G.37)

_[≤]_ [4][α][3][d][2]NK[(4][λ][6][3] _−_ [2]K [η] _[λ][s]_

 

Similar to R1[′] [let’s bound the][ R][2] [in (][G.10][),]

_N_ _N_

4 4

_R2 =α[2]_ **_a[N_** ]∼DE (a) _T ∼DE(T )_ *Q[t]T _[V]_ _j_ _K [G]j[2]_ **_wr[0]_** _[−]_ **_[a][j]_** _, Q[t]T_ _[V]_ _j_ _K [G]j[2]_ **_wr[0]_** _[−]_ **_[a][j]_** +

X    X   

_N_ _N_

(G.38)

= [16][α][2] E _G[2]i_ _[V Q]T[2][t][V G]i[2]_ E Σ[4]i tr Σ[2]N _[Q][2]T[t]_

_K_ [2] _T ∼D(T )_ [tr] " _i=1_ # _≤_ _T ∼D(T )_ [tr] " _i=1_ #

X 2t X h i

b

= [16][α][2][d][2][λ]x[4] 1

_λ[2]s[NK]_ [2] _−_ [2]K [η] _[λ][s]_

 

Thus the final constraint of α will be

2t 4α3d2(4λ6x _s[)]_ _x_ _c_ 1

1 _[−]_ _[λ][6]_ + [16][α][2][d][2][λ][4] _rˆ[8][αd][2][e]_ 0
_−_ [2]K [η] _[λ][s]_ _NK_ [3] _λ[2]s[NK]_ [2] _−_ _Kλ[3]x_ _N_ _N_ [2] _≤_
1 2t  _α2(4λ6x_ _[−]_ _[λ]s[6][)]_ + [4][αλ]x[4] _r_ [2][e]c 1  0[−] [1]  (G.39)

_−_ [2]K [η] _[λ][s]_ _K_ [2] _λ[2]s[K][ −]_ [ˆ]λ[3]x _−_ _N[1][2]_ _≤_
    

_α[2](4λ[6]x_ _[−]_ _[λ]s[6][)]_ + [4][αλ]x[4] _r_ [2][e]c 1 0

_K_ [2] _λ[2]s[K][ −]_ [ˆ]λ[3]x _−_ _N[1]_ _≤_

 

where ratio factor ˆr = [(1 2η/Kλx) / (1 2η/Kλs)][2][t]. We have the extreme points of α
_−_ _−_

2

_x_ 4λ[4]x _x[−][λ]s[6][)]_ _c_
_−_ _λ[4][2]s[λ][K][4]_ _[ ±]_ _λ[2]s[K]_ + 4 [(4][λ]K[6] [2] _rˆλ[2][3]x[e]_ 1 − _N[1]_
_α(N_ ) = (G.40)

r 2 [(4][λ]Kx[6] _[−][2][λ]s[6][)]_   

For K ∈ Z[+] _≥_ 1, small α, t = 0 and large t we have

_rˆ = [(1_ 2η/Kλx) / (1 2η/Kλs)][2][t] = (1) (G.41)
_−_ _−_ _O_

So finally, the α needs to satisfy


_x[K][ +][ K]_
_α(N_ ) ≤ _α(2) =_ _[−][2][λ][4]_

H EXPERIMENTS

H.1 PRACTICAL FORM OF THEOREM 1


4λ[8]x [+ 1][.][5][e]cλ[4]s[(4][λ][6]x _s[)][/λ][3]x_

_[−]_ _[λ][6]_ (G.42)

_λ[2]s[(4][λ][6]x_ _[−]_ _[λ]s[6][)]_


For practical use, we show a numerical form to estimate α[∗] for the case where number of tasks is
finite and training data u ∈ R[k][1] is different from validation data t ∈ R[k][2] . The corresponding feature
matrices are Φ(u) and Φ(t). Let’s derive corollary of Theorem 1 for realistic meta-learning setting.
**Corollary 2. If training feature Φ(u) ∈** R[k][1][×][d] _is different from validation feature Φ(t) ∈_ R[k][2][×][d]
_for every task, then_

_k1Extr[Φ(u)[⊤]Φ(u)(Φ(t)[⊤]Φ(t)]_
_αlim[∗]_ [(][k][1][) =] (H.1)

2Extr[Φ(u)[⊤]Φ(u)(Φ(t)[⊤]Φ(t)Φ(u)[⊤]Φ(u)] _[.]_

_Proof. Similar to proof of Theorem 1, we have_


1

_N_ _−_ _N_

_Ci(α)[⊤]C[b]i(α)_ _Ci(α)[⊤]C[b]i(α)ai_
_i=1_ ! _i=1_

X X

b b


(H.2)


**_wm_** _{xi, ai}i∈[N_ ], N, k1, k2, α
 


-----

where _Ci(α) = Φ(ti)(I −_ [2]k[α]1 [Φ(][u][i][)][⊤][Φ(][u][i][))][ ∈] [R][k][2][×][d][.]

The corresponding average population risk will be

[b]

¯m(N,k1, k2, α) = Ea1,...,aN (a) _m(wm, α, k1, k2)_
_L_ _∼D_ _L_

2 (H.3)

= E E _C(α)_ **_wm(_** **_xi, ai_** _i_ [N ], N, k1, k2, α) **_a_**
**_a,{ai}i[N]=1[∼D][(][a][)]_** **_x∼D(x)_** _{_ _}_ _∈_ _−_

  

[b]

Then we can define similar approximation function L[apx]m [(][α][)][ as (][B.6][) where]


_L[apx]m_ [(][α][)][ ≜] [E]x (x) [tr] _C(α)[⊤]C[b](α)_ (H.4)
_∼D_
h i

And with same bound for Φ(ui) _,_ Φ(ti) [c1, c2], we haveb
_∥_ _∥_ _∥_ _∥∈_

Γ lim ¯m(k1, k2, N, α) = L[ˆ][apx]m [(][α][)] (H.5)
_−_ _N_ _L_
_→∞_

With Gamma convergence lemma, we can get the final estimation αlim[∗] [in (][H.1][).]

In experiments, we use the following numerical form

_k1_ _Ni=1_ _[tr][[Φ(][u][i][)][⊤][Φ(][u][i][)(Φ(][t][i][)][⊤][Φ(][t][i][)]]_
_αlim[∗]_ [=] _._ (H.6)

2 _j=1_ _[tr]P[[Φ(][u][j][)][⊤][Φ(][u][j][)(Φ(][t][j][)][⊤][Φ(][t][j][)Φ(][u][j][)][⊤][Φ(][u][j][)]]_

to evaluate our estimation.

[P][N]

H.2 OVERPARAMETERIZED SETTING

Let’s consider overparameterized setting. Thus, we have feature for each task (K < d), Ψ =

[ψ(x1), ..., ψ(xK)][⊤] _∈_ R[K][×][d]. Correspondingly, empirical objective of ERM is given by


ˆr(w) =
_L_


Ψiw **_yi_** _._
_∥_ _−_ _∥_
_i=1_

X


_NK_


Assume meta-initialization is the mean of all task optima that ¯a = mean(a1, ..., aN ). Then concatenate all task features we have


Ψ1(1)

_. . ._

ΨN (K)


_, Ψ[all]_ R[NK][×][d] (H.7)

 _∈_


Ψall =

So MAML objective is given by


1
ˆm(w, α, N, K) =
_L_ _NK_

_[∥][Ψ][all][w][′][ −]_ **_[y][all][∥][2]_** (H.8)

1
= **_a_**

_NK_ _[∥][C][all][(][α][)][w][ −]_ [¯]∥[2]

where w[′] = **_w −_** 2α/(NK)Ψall Ψ[⊤]all[w][ −] [Ψ]all[⊤] **_a[¯]_** is adapted parameters and Call(α) = Ψall(I _−_
(2α/NK)Ψ[⊤]all[Ψ][all][)][. The minimum norm solution is]
   

**_wm (..., N, K, α) = Call(α)[⊤]_** [ ]Call(α)Call(α)[⊤][][−][1] _Call(α)¯a_ (H.9)

Note that, in overparameterized setting, Theorem 1 will not perfectly give the precise form for α[∗].
But the technique can be easily extend to large d setting where


_L¯m(α, N, K) =Ea,a¯,x[∥][C][(][α][)(][w]m_ _[−]_ **_[a][)][∥][2]_**

=Ea,a¯,x **_wm[⊤]_** _[C][(][α][)][⊤][C][(][α][)][w][m]_ [+][ a][⊤][C][(][α][)][⊤][C][(][α][)][a]

=Ex tr[Call(α)[⊤]Call[gram]Call(α)C(α)[⊤]C(α)Call(α)[⊤]Call[gram]Call(α)]

+ Ex tr[C(α)[⊤]C(α)]


(H.10)


-----

where Call[gram] = (Call(α)Call(α)[⊤])[−][1].

By Proposition 2, the [¯]m(α, N, K) will be upper bounded where
_L_

¯m(α, N, K) =Ex tr[Call[gram]Call(α)C(α)[⊤]C(α)Call(α)[⊤]] + Ex tr[C(α)[⊤]C(α)]
_L_

=Ex tr[Call(α)[⊤]Call[gram]Call(α)C(α)[⊤]C(α)] + Ex tr[C(α)[⊤]C(α)]

Ex tr[Call(α)[⊤]Call[gram]Call(α)]Ex tr[C(α)[⊤]C(α)] + Ex tr[C(α)[⊤]C(α)]
_≤_

=2Ex tr(C(α)[⊤]C(α))


(H.11)


Hence, minimizing the upper bound also tells us how to select α[∗]. In another word, we are seeking
an estimation αest[∗] [nearly minimize the upper bound.]

_αest[∗]_ [= arg min]α [E][x][ tr(][C][(][α][)][⊤][C][(][α][))] (H.12)

and the C(α)[⊤]C(α) is a covariance matrix where

4α[2]

_C(α)[⊤]C(α) = Ψ[⊤]Ψ_ (H.13)
_−_ _NK[4][α]_ [(Ψ][⊤][Ψ)][2][ +] _N_ [2]K [2][ (Ψ][⊤][Ψ)][3]


We can derive that


_αest[∗]_ [=][ NK][E][x][ tr(Ψ][⊤][Ψ)][2] (H.14)

2Ex tr(Ψ[⊤]Ψ)[3]


Since d is large, its computational cost is high. Here we assume second moment of all elements of
Ψ[⊤]Ψ are ˜σ[2], which means ˜σ is the variance of all elements of features. Finally we have


_αest[∗]_ [=]


1

(H.15)
2NKσ˜[2]


Let’s take the Neural Tangent Kernel (NTK) (Jacot et al., 2018) for example,


_f_ (w, x) = f (w[init], x) + ∇f (w[init], x)[⊤](w − **_w[init])._** (H.16)

Then we have neural tangent feature

Ψ[⊤]i **_[w][ =][ ∇][f]_** [(][w][init][, X][i][)][⊤][(][w][ −] **_[w][init][)]_**

(H.17)
Ψi _f_ (w[init], Xi)
_⇒_ _≈_

Next, we stack all the features to Ψall to compute the variance e.g. recall σ = Ψall.std(). After that,
we can compute the estimation αest[∗] [using (][H.15][).]

H.3 EXPERIMENTAL SETUP

**Estimation of α[∗], underparameterized model** We set hyperparameters dimension d = 20, number
of training/validation samples per task K = 50, number of tasks N = 5000. Each x is i.i.d sampled
from a distribution U (−5, 5) while each a is i.i.d sampled from high dimension Gaussian distribution
_N_ (0, 3I). Then computing the Ordinary Least Square (OLS) solution with different α, we can show
the training loss landscape in terms ofthe minimizer of the training loss. Our estimation α. The true α α[∗]lim[∗](N ) = arg min[described in Theorem]α minw L([ 1]α,[ is evaluated by] 5000, 50, w) is
comparing the error to true α[∗].

**Estimation of α[∗], overparameterized model** We perform the nonlinear regression on two different
models. The first is quadratic regression using neural tangent feature (see H.2). All hyperparameters
are set to be same as (Bernacchia, 2021) where

**w** **w0, [ν][2]** _b_ (0, σ[2]) **x** (0, Ip) _y_ **x, w, b** (x[T] **w + b)[2], σ[2][]**
_∼N_ _p [I][p]_ _∼N_ _∼N_ _|_ _∼N_
 

 

But to guarantee the overparameterization, we set hidden size with 10, 000, which means the total
dimension will be 30, 001. Then we perform quadratic function regression with ranging α value
to see the test loss. After that, we can evaluate the how accurate our estimation using (H.15) is by


-----

comparing to optimum of test loss. Second experiment is sine function regression using 3-layer MLPs
activated with ReLU. The data and labels are generated from a stochastic function

_y = a sin(x + b), a, b ∼U(0, π), x ∼U(−5, 5)._

To get a good representation, we pre-train the model with ERM loss and then freeze the first two
layers as the feature extractor. Then we use the output from second layer as the random feature to
train last layer on 1, 000 training tasks. At the same time, αlim[∗] [can be computed from the features of]
1, 000 training tasks. Then last layer trained with differnet α will be evaluated on 10, 000 test tasks.

**Fast adaptation distance** We run experiments with random matrices. For each task, the data
are i.i.d drawn for the prescribed distributions which represent three common types, Uniform
_U_ (−5, 5), Gaussian N (0, 2) and Exponential Exp(1). Specifically, each entry in random matrix
_X ∈_ R[K][×][d], (K = 50, d = 20) is sampled variable from a distribution X(ij) ∼D(x). The feature
map is an identity map Φ(X) = X. First we sample 5000 training tasks to compute the closed-form
meta-initializations for ERM and MAML with a small α (10[−][4]). Then we perform t-step adaptation
on 5000 test tasks and compare the fitting losses and the Average Distance under Fast Adaptation
_t(wm),_ _t(wr). The learning rate η in fast adaptation evaluation is 10[−][5]._
_F_ _F_

**Estimation of α[∗], deep learning** In our experiments, we valid our estimation of α[∗] on sine
regression and few-shot classification.

-  For deep regression, we follow the (Finn et al., 2017) to perform sine regression with
3-layer MLP whose hidden size is 40. Then each task is an instance in stochastic function
_y = a sin(x + b), a, b ∼U(0, π) while the training set is 10 i.i.d sampled data pair from the_
corresponding sine function and test set is consists of another sampled 256 points. During
test, we sample 10, 000 tasks to evaluate the learned model.

-  For deep classification, we follow the Omniglot experiments in (Raghu et al., 2020). Here,
we adopt the online estimation scheme to compute the α[∗] for the adaptation learning rate of
last layer. To this end, we apply α[∗] = 1/(2 _Nway_ _Nshot_ ˆσ[2]) where ˆσ[2] is mean of
_×_ _×_ _×_
covariance of the normalized feature F _[⊤]F_, F = (F1/∥F1∥, ..., FNway/∥FNway∥). Then
_αbuffer[∗]_ [in the buffer is online updated using][ α]buffer[∗] _[←]_ [0][.][9][α]buffer[∗] [+ 0][.][1][α][∗][.]

H.4 ADDITIONAL RESULT

H.4.1 OPTIMIZATION BEHAVIOR

We add more illustrative experiments on visualizing the trajectory of global minima of MAML. We
consider normally distributed task optima with centralized data and uncentralized data. As shown in
7, the MAML minimum still try to balance the distances to different task optima. But the situation
is more complex in uncentralized data (second row). However, α always minimize the geometric
distance at beginning where the shape of mean distance function appears to be convex. This has
confirmed our Theorem 2 where small α always lead to a shorter mean distance to different task
optima than ERM algorithm.

H.4.2 ESTIMATION OF α[∗] ON BASIS FUNCTION FEATURE

Firstly, we used the random matrix for taskare i.i.d sampled fromfunction. Gaussian basis functiondepends only on the distance between the input and some fixed point. Thirdly, we used polynomial U (−5, 5). Secondly, we created the random features using Gaussian basis Φ(X)(ij) = exp( i as the feature matrix,− _X(ij) −_ _µj_ 2 Φ /i2 ∈σi2R[)][ is a function whose value][K][×][d]. All elements of Φi
  
feature Φ(X)(i,:) = (c0, · · ·, cnx[d](i[−]) [1][)][ which is based on Taylor series. With][ N][ tasks, we compute]
the one-step adaptation loss. Optimal learning rate minimizing the loss is denoted by α[∗](N ). The
error gap between true optimum and estimation |α[∗](N )−αlim[∗] _[|][ with three random features are shown]_
in Figure 8 (a), (b) & (c) respectively. To reduce random errors, there was an average of 10 sampling
trials, shown as the solid lines. The shadow area represents standard deviation. As the number of
tasks N increasing, the estimation error will shrink down to zero and its uncertainty reduces as well.
So our estimation is reliable and accurate when number of tasks becomes large.


-----

Figure 7: Additional results for optimization behavior with normally distributed task optima. Left
column: Visualization of trajectory of MAML solution. Orange dots are task optima {a}[N ] of
sampled tasks, where location of ai is decided by its entries. Red dot highlighted in circle is new
coming task. Green cross is wr, (α = 0) while the purple trajectory is generated as α increasing.
Red star is wm(αlim[∗] _[, ...][)][. Right column: Average euclidean distance of][ w][m][(][α, ...][)][ and][ {][a][}][[][N]_ []][ and]
corresponding points in left figure. First row: centralized data x ∼N (0, 2), a ∼N (0, 3I). Second
row: uncentralized data x ∼U(0, 5), a ∼N (0, 3I). Best viewed in colors.


0.025 0.25 0.7

*|lim0.020 0.20 0.60.5
(*)N 0.0150.010 0.150.10 0.40.30.2
|0.005 0.05 0.1

0.000 0.00 0.0

2[1] 2[3] 2[5] 2[7] 2[9] 2[11] 2[13] 2[1] 2[3] 2[5] 2[7] 2[9] 2[11] 2[13] 2[1] 2[3] 2[5] 2[7] 2[9] 2[11] 2[13]

Number of tasks Number of tasks Number of tasks

(a)


Number of tasks

(b)


Number of tasks

(c)


Figure 8: Estimation error |α[∗](N ) − _αlim[∗]_ _[|][ along task number][ N][ increasing (][K][ is fixed). The blue]_
line in the shadow is mean of the error. The shadow area is the standard deviation of the errors. (a)
Random matrices (b) Gaussian basis function (c) Polynomial basis function.

We use Gaussian basis function as the random feature and conduct the experiments on different
types of distribution to evaluate our estimation quality. Then, we use uniform/normal distribution
with zero mean U, N as the stereotype of central symmetric distribution. In experiments, we set
_d = 10, K = 15, N = 3000 and the parameters in Gaussian function depends on the range of data._
As shown in the Figure 9, the estimation αlim[∗] [is close to true optimum,][ α][∗] [in four different cases:]
(a) data is sampled from a central uniform distribution U (−5, 5), task optima are sampled from a
central normal distribution N (0, 3[2]I); (b) data is sampled from a non-central normal distribution
_N_ (0, 2[2]), task optima are sampled from a central normal distribution N (0, 3[2]I); (c) data is sampled
from a central uniform distribution U (−5, 5), task optima are sampled from a non-central normal
distribution (5, I); (d) data is sampled from a non-central Chi-Sqaure distribution χ[2](7), task
_N_ 1
optima are sampled from a imbalanced Zipf distribution P (x = k) = _ζ(s)_ _[k][−][s][ where][ ζ][(][s][)][ is the]_


-----

|α|0.001|0.005|0.01|0.05|0.08|0.1|0.15|0.2|0.3|0.4|0.5|
|---|---|---|---|---|---|---|---|---|---|---|---|
|Pre MSE|829|822|822|827|829|827|825|821|826|825|826|
|Post MSE|829|820|817|807|800|797|805|820|861|955|989|


_α_ 0.001 0.005 0.01 0.05 0.08 **0.1** 0.15 0.2 0.3 0.4 0.5

Pre MSE 829 822 822 827 829 827 825 821 826 825 826

Post MSE 829 820 817 807 800 **797** 805 820 861 955 989

Table 2: Test loss of one-step sinusoid regression with neural network feature. First row is the discrete
test values of α, second row is the Mean Square Error(MSE) loss before adaptation and third row is
the loss after adaptation. All loss values are digits after the decimal point

Riemann Zeta function. Note that results in (c) and (d) are beyond our Assumption 1. So our theorem
can be extended to more general scenarios.

Figure 9: Evaluate estimation αlim[∗] [on different types of distributions. (a) Central data distribution and]
central task optima distribution. (b) Non-central data distribution and central task optima distribution.
(c) Central data distribution and Non-central task optima distribution. (d) Non-symmetric non-central
distributions for data and task optima.

H.4.3 ESTIMATION OF α[∗] ON NEURAL NETWORK FEATURE

We used neural network based feature to verify our theorem in underparameterized (original model
size in Finn et al. (2017)) and overparameterized setting (NK < d). In former setting, we used
3-layer Multilayer Perceptron (MLP) activated with ReLU for sine functions family regression where
each task is to regress an instance in stochastic function y = a sin(x + b), a, b ∼U(0, π). We used
ERM to train and freeze the first 2 layers (as feature extractor) and then only fine-tune last layer
with MAML. Compute αlim[∗] [through features of sampled training tasks we got][ α]lim[∗] [= 0][.][10319][. As]
shown in Table. 2, the optimal α of lowest MSE after adapting is 0.1 which is nearest discrete value
in table to αlim[∗] [.]

H.4.4 HEURISTIC ESTIMATION RANGE FOR DEEP LEARNING

To make it practical for deep learning, we give the heuristic estimation range where α[∗] it might
be based on our theorem. Previously, we show (3.3) for underparameterized model (K > d) and
(H.15) for overparameterized model (NK ≪ _d). Besides, the trace term for covariance matrix_
in underparameterized setting can be simplied by Kdσ˜[2] where ˜σ[2] is the covariance of the feature
(second moment). So here, the heuristic estimation by merging these two settings, where it derived as


_αlim[∗]_ [=]


1

(H.18)
2 min(NK, d)˜σ[2]


-----

where N, K are number of training task and its training sample size, d is model size. Next, we show
a simple way to estimate the range of ˜σ[2]. In general, each element of feature with probability 1
will fall into the [0, 1]. Then, given ˜n observations, we have (Popoviciu’s inequality on covariance
(Sharma et al., 2010))
1

_σ[2]_ (H.19)
2˜n _≤_ [1]4

_[≤]_ [˜]

_Proof. Assume with probability 1, each element x of Φ(X) has x ∈_ [m, M ].

Define a function f in terms of random variable x by

_f_ (t) = E (x − _t)[2][]_ (H.20)

Computing the derivative f, and solving the minimum

_[′]_

_f_ _[′](t) = −2E[x] + 2t = 0 ⇒_ _f_ (E[x]) = mint∈R _[f]_ [(][t][)] (H.21)

So we have the covariance has following upper bound

_M + m_
_σ˜[2]_ = f (E[x]) _f_ (H.22)
_≤_ 2
 

where

2[#]

_M + m_
_f_ = E _x_ = [1] ((x _m) + (x_ _M_ ))[2][] = [(][M][ −] _[m][)][2]_ (H.23)

2 _−_ _[M][ +]2_ _[ m]_ 4 [E] _−_ _−_ 4

  " 



Thus for m = 0, M = 1, we have

_σ˜[2]_ = [1] (H.24)
_≤_ [(1][ −]4 [0)][2] 4

for an independent sample of ˜n observations from a bounded probability distribution, the von
Szokefalvi Nagy inequality shows that

_σ˜[2]_ = [1] (H.25)
_≥_ [(][M][ −]2˜n[m][)][2] 2˜n


Here, we conduct following deep regression experiments to evaluate our heuristic estimation. We plot
the estimated range of α[∗] given by our bounds – the red area between the two star-lines in Figure

10. Then we perform quadratic regression with 2-layer neural network follow the hyperparameter
in (Bernacchia, 2021). From Figure 10(a), we can see, the optimal α for this task is positive and
our estimated range includes suboptimal points. Follow the setting of (Finn et al., 2017) (All
hyperparameters are same), we use 3-layer NN with hidden size 40 to test sine regression tasks. As
shown in the Figure 10(b), our estimated range includes the optimal α and other good α.


H.5 RELATION TO NEGATIVE LEARNING RATE

As we mentioned before, (Bernacchia, 2021) show negative learning rate minimizing the test loss of
MAML. In this section, we compare their results with ours. Specifically, we follow the setting of
underparameterized experiment (Bernacchia, 2021) where the defined hyperparameters are set to be
same, nt = 5, nv = 25, nr = 10, m = 40, p = 30, σ = 0.2, ν = 0.2. Parameters are sampled from
following distributions

**w** **w0, [ν][2]** **x** (0, Ip) _y_ **x, w** **x[T]** **w, σ[2][]**
_∼N_ _p [I][p]_ _∼N_ _|_ _∼N_
 

 

We conduct experiments on numerical fitting loss on meta-learning instead of the closed-forms _L[¯][test]_
in theorems (Bernacchia, 2021)[2]. As we can see from the Figure 11, (Bernacchia, 2021) only give the
result on special case where αr = 0.2 is fixed. However, this strategy highly depends on the selection
of αr that may not achieve the minimum of meta-learning loss.

2Test losses are computed on standard meta-learning regression


-----

(a) Quadratic Regression 2-layer NN (b) Sine Regression 3-layer NN

Figure 10: Heuristic estimation of α[∗] range of deep learning. (a) Quadratic regression on 2-layer
neural network. (b) Sine regression on 3-layer neural network.

Figure 11: Comparison of our estimation and (Bernacchia, 2021) on underparameterized mixed linear
regression. X-axis is the discrete values of αt and Y -axis is the test loss of MAML. First row, test
loss with respect to αt while the left one shows same α for meta-training and meta-testing and right
one is fixed αt = 0.2 strategy. Second row, comparison of test loss of different strategies and the
suggested range of minimizers given by their paper (pink and green diamonds) and our estimation
(red diamond). Best viewed in color.

In addition, we run deep learning experiments to demonstrate that optimal learning rate α is positive.
All hyperparameters and generating process are set to be same as the non-linear regression experiments
in (Bernacchia, 2021). Furthermore, we train the 2-layer neural network to regress quadratic functions
with 5 adaptation steps and evaluate models on same 10 folds with each fold consists of 1000 test
tasks. The results (with error bar) are shown in the Figure 12. As we can see, the optimal learning
rate for both strategies are positive.


-----

Figure 12: Test losses with reference to adaptation learning rate in meta-training of deep quadratic
regression in (Bernacchia, 2021). X-axis is the discrete values of αt and Y -axis is the test loss of
MAML. Left: test loss with fixed αt = 0.01 and varying αr. Right: test loss with same αt and αr


-----

