# ON THE PITFALLS OF HETEROSCEDASTIC UNCERTAINTY ESTIMATION WITH PROBABILISTIC NEURAL NETWORKS

**Maximilian Seitzer[1], Arash Tavakoli[1], Dimitrije Anti´c[2], Georg Martius[1]**

1
Max Planck Institute for Intelligent Systems, T¨ubingen, Germany
2
University of T¨ubingen, T¨ubingen, Germany
maximilian.seitzer@tuebingen.mpg.de

ABSTRACT

Capturing aleatoric uncertainty is a critical part of many machine learning systems.
In deep learning, a common approach to this end is to train a neural network to
estimate the parameters of a heteroscedastic Gaussian distribution by maximizing
the logarithm of the likelihood function under the observed data. In this work, we
examine this approach and identify potential hazards associated with the use of
log-likelihood in conjunction with gradient-based optimizers. First, we present a
synthetic example illustrating how this approach can lead to very poor but stable
parameter estimates. Second, we identify the culprit to be the log-likelihood loss,
along with certain conditions that exacerbate the issue. Third, we present an
alternative formulation, termed β−NLL, in which each data point’s contribution
to the loss is weighted by the β-exponentiated variance estimate. We show that
using an appropriate β largely mitigates the issue in our illustrative example.
Fourth, we evaluate this approach on a range of domains and tasks and show that
it achieves considerable improvements and performs more robustly concerning
hyperparameters, both in predictive RMSE and log-likelihood criteria.

1 INTRODUCTION

Endowing models with the ability to capture uncertainty is of crucial importance in machine learning.
Uncertainty can be categorized into two main types: epistemic uncertainty and aleatoric uncertainty
(Kiureghian & Ditlevsen, 2009). Epistemic uncertainty accounts for subjective uncertainty in the
model, one that is reducible given sufficient data. By contrast, aleatoric uncertainty captures the
stochasticity inherent in the observations and can itself be subdivided into homoscedastic and
_heteroscedastic uncertainty. Homoscedastic uncertainty corresponds to noise that is constant across_
the input space, whereas heteroscedastic uncertainty corresponds to noise that varies with the input.

There are well-established benefits for modeling each type of uncertainty. For instance, capturing
epistemic uncertainty enables effective budgeted data collection in active learning (Gal et al., 2017),
allows for efficient exploration in reinforcement learning (Osband et al., 2016), and is indispensable
in cost-sensitive decision making (Amodei et al., 2016). On the other hand, quantifying aleatoric
uncertainty enables learning of dynamics models of stochastic processes (e.g. for model-based or
offline reinforcement learning) (Chua et al., 2018; Yu et al., 2020), improves performance in semantic
segmentation, depth regression and object detection (Kendall & Gal, 2017; Harakeh & Waslander,
2021), and allows for risk-sensitive decision making (Dabney et al., 2018; Vlastelica et al., 2021).

We examine a common approach for quantifying aleatoric uncertainty in neural network regression.
By assuming that the regression targets follow a particular distribution, we can use a neural network
to predict the parameters of that distribution, typically the input-dependent mean and variance when
assuming a heteroscedastic Gaussian distribution. Then, the parameters of the network can be learned
using maximum likelihood estimation (MLE), i.e. by minimizing the negative log-likelihood (NLL)
criterion using stochastic gradient descent. This simple procedure, which is the de-facto standard (Nix
& Weigend, 1994; Lakshminarayanan et al., 2017; Kendall & Gal, 2017; Chua et al., 2018), is known
to be subject to overconfident variance estimates. Whereas strategies have been proposed to alleviate
this specific issue (Detlefsen et al., 2019; Stirn & Knowles, 2020), we argue that an equally important


-----

0.3

0.2


0.3

0.0

-0.3


0.1


0.0

|Col1|Col2|NLL Loss MSE Loss Optimal|Col4|Col5|NLL Loss MSE Loss Optimal|Col7|Col8|
|---|---|---|---|---|---|---|---|
|||||||||
|||||||||
|||||||||


NLL Loss
MSE Loss
Optimal


0 5 10 0.00 0.25 0.50 0.75 1.00

Input X Update Steps _×10[7]_

Figure 1: Training a probabilistic neural network to fit a simple sinusoidal fails. Left: Learned
predictions (orange line) after 10[7] updates, with the shaded region showing the predicted standard
deviation. The target function is given by y(x) = 0.4 sin(2πx) + ξ, where ξ is Gaussian noise with a
standard deviation of 0.01. Right: Root mean squared error (RMSE) over training, mean and standard
deviation over 10 random seeds. For comparison, we plot the training curve when using the mean
squared error as the training objective – achieving an optimal mean fit (dashed line) in 10[5] updates.
This behavior is stable across different optimizers, hyperparameters, and architectures (see Sec. B.2).


10


issue is that this procedure can additionally lead to subpar mean fits. In this work, we analyze and
propose a simple modification to mitigate this issue.

**Summary of contributions** We demonstrate a pitfall of optimizing the NLL loss for neural network
regression, one that hinders the training of accurate mean predictors (see Fig. 1 for an illustrative
example). The primary culprit is the high dependence of the gradients on the predictive variance.
While such dependence is generally known to be responsible for instabilities in joint optimization of
mean and variance estimators (Takahashi et al., 2018; Stirn & Knowles, 2020), we identify a fresh
perspective on how this dependence can further be problematic. Namely, we hypothesize that the
issue arises due to the NLL loss scaling down the gradient of poorly-predicted data points relative to
the well-predicted ones, leading to effectively undersampling the poorly-predicted data points.

We then introduce an alternative loss formulation, termed β−NLL, that counteracts this by weighting
the contribution of each data point to the overall loss by its β-exponentiated variance estimate, where
_β controls the extent of dependency of gradients on predictive variance. This formulation subsumes_
the standard NLL loss for β = 0 and allows to lessen the dependency of gradients on the variance
estimates for 0 < β ≤ 1. Interestingly, using β = 1 completely removes such dependency for
training the mean estimator, yielding the standard mean squared error (MSE) loss – but with the
additional capacity of uncertainty estimation. Finally, we empirically show that our modified loss
formulation largely mitigates the issue of poor fits, achieving considerable improvements on a range
of domains and tasks while exhibiting more robustness to hyperparameter configurations.

2 PRELIMINARIES


Let X, Y be two random variables describing the input and target, following the joint distribution
_P_ (X, Y ). We assume that Y is conditionally independent given X and that it follows some probability
distribution P (Y | X). In the following, we use the common assumption that Y is normally
distributed given X; i.e. P (Y | X) = N (µ(X), σ[2](X)), where µ : R[M] _7→_ R and σ[2] : R[M] _7→_ R[+]
are respectively the true input-dependent mean and variance functions.[1] Equivalently, we can write
_Y = µ(X)+ϵ(X), with ϵ(X) ∼N_ (0, σ[2](X)); i.e. Y is generated from X by µ(X) plus a zero-mean
Gaussian noise with variance σ[2](X). This input-dependent variance quantifies the heteroscedastic
uncertainty or input-dependent aleatoric uncertainty.

To learn estimates ˆµ(X), ˆσ[2](X) of the true mean and variance functions, it is common to use a
neural network fθ parameterized by θ. Here, ˆµ(X) and ˆσ[2](X) can be outputs of the final layer (Nix
& Weigend, 1994) or use two completely separate networks (Detlefsen et al., 2019). The variance
output is hereby constrained to the positive region using a suitable activation function, e.g. softplus.

1For notational convenience, we focus on univariate regression but point out that the work extends to the
multivariate case as well.


-----

**(MSE for mean)**

Figure 2: Illustration of the pitfall when training with NLL (negative log-likelihood) versus our
solution. An initial inhomogeneous feature space granularity (see Sec. 3.1) results early on in different
fitting quality. The implicit weighting of the squared error in NLL can be seen as biased data-sampling
with p(x) ∝ _σ[2]1(x)_ [(see Eq.][ 6][). Badly fit parts are increasingly ignored during training. On the right,]

the effect of our solution (Eq. 7) on the relative importance of data points is shown.


The optimal parameters θNLL[∗] [can then be found using maximum likelihood estimation (MLE) by]
minimizing the negative log-likelihood (NLL) criterion LNLL under the distribution P (X, Y ):

1 _µ(X))[2]_

_θNLL[∗]_ [= arg min] NLL(θ) = arg min E _σ[2](X) + [(][Y][ −]_ [ˆ] + const _._ (1)
_θ_ _L_ _θ_ _X,Y_ 2 [log ˆ] 2ˆσ[2](X)

 

In contrast, standard regression minimizes the mean squared error (MSE) MSE:
_L_

(Y _µˆ(X))[2]_

_θMSE[∗]_ [= arg min] MSE(θ) = arg min E _−_ _._ (2)
_θ_ _L_ _θ_ _X,Y_ 2

 

In practice, Eq. 1 and Eq. 2 are optimized using stochastic gradient descent (SGD) with mini-batches
of samples drawn from P (X, Y ). The gradients of LNLL w.r.t. (with respect to) ˆµ(X), ˆσ[2](X) are
given by


ˆµ(X) − _Y_

_σˆ[2](X)_




_σˆ[2](X) −_ (Y − _µˆ2(X))[2]_

2 _σˆ[2](X)_
  


_µˆ[L]NLL[(][θ][) =][ E]_
_∇_ _X,Y_

ANALYSIS


_σˆ[2]_ _[L]NLL[(][θ][) =][ E]_
_∇_ _X,Y_


(3, 4)


We now return to the example of trying to fit a sinusoidal function from Sec. 1. Recall from Fig. 1
that using the Gaussian NLL as the objective resulted in a suboptimal fit. In contrast, using MSE as
the objective the model converged to the optimal mean fit in a reasonable time. We now analyze the
reasons behind this surprising result.

From Eq. 3, we see that the true mean µ(X) is the minimizer of the NLL loss. It thus becomes
clear that a) the solution found in Fig. 1 is not the optimal one, and b) the NLL objective should,
in principle, drive ˆµ(X) to the optimal solution µ(X). So, why does the model not converge to the
optimal solution? We identify two main culprits for this behavior of the Gaussian NLL objective:

1. Initial flatness of the feature space can create an undercomplex but locally stable mean fit.
This fit results from local symmetries and requires a form of symmetry breaking to escape.

2. The NLL loss scales the gradient of badly-predicted points down relative to well-predicted
points, effectively undersampling those points. This effect worsens as training progresses.

These culprits and their effect on training are illustrated in Fig. 2 (left). If the network cannot fit a
certain region yet because its feature space (spanned by the last hidden layer) is too coarse, it would
then perceive a high effective data variance. This leads to down-weighting the data from such regions,
fueling a vicious cycle of self-amplifying the increasingly imbalanced weighting. In the following,
we analyze these effects and their reasons in more detail.

3.1 SYMMETRY AND FEATURE NON-LINEARITY

It is instructive to see how the model evolves during training as shown in Fig. 3. The network first
learns essentially the best linear fit while adapting the variance to match the residuals. The situation


-----

(a) After 1 000 updates.

0.4


(b) After 10 000 updates.

0.4


(c) After 500 000 updates.

0.4


0.0

-0.4


0.0

-0.4


0.0

-0.4


12


12


12


Input X


Input X


Input X


Figure 3: Model fit using the NLL loss at different stages of training shown in orange with ±σ
uncertainty band. Black dots mark training data. Fitting the function begins from the left and is
visibly slow.


is locally stable. That is, due to the symmetries of errors below and above the mean fit, there is no
incentive to change the situation. Symmetry breaking is required for further progress. One form of
symmetry breaking comes with the inherent stochasticity of mini-batch sampling in SGD, or the
natural asymmetries contained in the dataset due to, e.g., outliers. Moreover, we hypothesize that the
local non-linearity of the feature space plays an important role in creating the necessary non-linear fit.

Let us consider the non-linearity of the feature space. This quantity is not easy to capture. To
approximate it for a dataset D, we compute how much the Jacobian Jf [of the features][ f] [(][x][)][ w.r.t. the]
input varies in an L2-ball with radius r around a point x, denoted as the Jacobian variance:[2]

2

1 1
_V (x) =_ _|Bx|_ _xX[′]∈Bx_ Jf [(][x][′][)][ −] _|Bx|_ _x[′′]X∈Bx_ _Jf_ [(][x][′′][)] _,_ _Bx = {x[′]_ _∈D_ : ∥x − _x[′]∥2 ≤_ _r}._ (5)

Figure 4 visualizes the Jacobian variance over the input space as a function of the training progress.
Although initially relatively flat, it becomes more granular in parts of the input space, the parts which
are later well fit. The region with low Jacobian variance remains stuck in this configuration (see
Fig. 1). This provides evidence that the non-linearity of the feature space is important for success or
failure of learning on this dataset. However, why does gradient descent not break out of this situation?


10[0]


10[−][2]


10[−][4]

1

Update Steps

0

0 4 8 12

Input X

Figure 4: Jacobian variance over training time,
using the mean of matrix V (x) (see Eq. 5).


1

10[−][4]

Update Steps

0

0 4 8 12

Input X

Figure 5: Probability of sampling a data point at
input x over training time.


3.2 INVERSE-VARIANCE WEIGHTING EFFECTIVELY UNDERSAMPLES

The answer lies in an imbalanced weighting of data points across the input space. Recall that the
gradient ∇µˆ[L]NLL [of the NLL w.r.t. the mean scales the error][ ˆ]µ(X) − _Y by_ _σˆ[2](1X)_ [(Eq.][ 3][). As]

symmetry is broken and the true function starts to be fit locally, the variance quickly shrinks in
these areas to match the reduced MSE. If the variance is well-calibrated, the gradient becomes
_µˆ(X)−Y_ _µˆ(X)−Y_ 1

_σˆ[2](X)_ _≈_ (ˆµ(X)−Y )[2][ =] _µˆ(X)−Y_ [. Data points with already low error will get their contribution in]

the batch gradient scaled up relatively to high error data points – “rich get richer” self-amplification.
Thus, NLL acts contrary to MSE which focuses on high-error samples. If the true variance σ[2] on
the well-fit regions is much smaller than the errors on the badly-fit regions, or there are much more
well-fit than badly-fit points, then learning progress is completely hindered on the badly-fit regions.

2This is a form of approximative second-order derivative computed numerically, which also gives non-zero
results for networks with relu activation (in contrast to, for example, the Hessian).


-----

Another way to view this is to interpret the different weighting of points as changing the training
_distribution P_ (X, Y ) to a modified distribution _P[˜](X, Y ) in which points with high error have a_
lower probability of getting sampled. This can be shown by defining _P[˜](X, Y ) = Z_ _[−][1][ P]σ[ (][2][X,Y](X)[ )]_ [,]

where Z = _Pσ ([2]x,y(x))_ [d][x][d][y][ is a normalizing constant, and recognizing that the gradient of the NLL is]

proportional to the gradient of the MSE loss in Eq. 2 under the modified data distribution _P[˜](X, Y ):_

R

(Y _µˆ(X))[2]_

_∇µˆ[L]NLL[(][θ][) =][ Z][ ·][ E]X,Y ∼P˜ (X,Y )[[ˆ]µ(X) −_ _Y ] ∝∇µˆ[E]X,Y ∼P˜ (X,Y )_ _−_ 2 _._ (6)

 

In Fig. 5, we plot _P[˜](X, Y ) over training time for our sinusoidal example. It can be seen that the_
virtual probability of sampling a point from the high-error region drops over time until it is highly
unlikely to sample points from this region (10[−][5] as opposed to 10[−][3] for uniform sampling). We
show that this behavior also carries over to a real-world dataset in Sec. B.3.

Sometimes, “inverse-variance weighting” is seen as a feature of the Gaussian NLL (Kendall & Gal,
2017) which introduces a self-regularizing property by allowing the network to “ignore” outlier
points with high error. This can be desirable if the predicted variance corresponds to data-inherent
unpredictability (noise), but it is undesirable if it causes premature convergence and ignorance of hardto-fit regions, as shown above. In our method, we enable control over the extent of self-regularization.

4 METHOD

In this section, we develop a solution method to mitigate these issues with NLL training. Our
approach, which we term β−NLL, allows choosing an arbitrary loss-interpolation between NLL and
MSE while keeping calibrated uncertainty estimates.

4.1 VARIANCE-WEIGHTING THE GRADIENTS OF THE NLL

The problem we want to address is the premature convergence of NLL training to highly suboptimal
mean fits. In Sec. 3, we identified the relative down-weighting of badly-fit data points in the NLL
loss together with its self-amplifying characteristic as the main culprit. Effectively, NLL weights
the mean-squared-error per data point with _σ1[2][, which can be interpreted as sampling data points]_

with P (x) ∝ _σ1[2][ . Consequently, we propose modifying this distribution by introducing a parameter]_

_β allowing to interpolate between NLL’s and a completely uniform data point importance. The_
resulting sampling distribution is given by P (x) ∝ _[σ]σ[2][2][β][ and illustrated in Fig.][ 2][ (right).]_

How could this weighting be achieved? We simply introduce the variance-weighting term σ[2][β] to the
_LNLL loss such that it acts as a factor on the gradient. We denote the resulting loss as β−NLL:_

1 _µ(X))[2]_

_β_ NLL := E _σˆ[2][β](X)_ _σ[2](X) + [(][Y][ −]_ [ˆ] + const _,_ (7)
_L_ _−_ _X,Y_ _⌊_ _⌋_ 2 [log ˆ] 2ˆσ[2](X)

  

where ⌊·⌋ denotes the stop gradient operation. By stopping the gradient, the variance-weighting term
acts as an adaptive, input-dependent learning rate. In this way, the gradients of _β_ NLL are:
_L_ _−_

ˆµ(X) _Y_ ˆσ[2](X) (Y _µˆ(X))[2]_

_µˆ[L]β_ NLL[(][θ][) =][ E] _−_ _,_ _σˆ[2]_ _[L]β_ NLL[(][θ][) =][ E] _−_ _−_ _._ (8, 9)
_∇_ _−_ _X,Y_ _σˆ[2][−][2][β](X)_ _∇_ _−_ _X,Y_ 2ˆσ[4][−][2][β](X)

   

Naturally, for β = 0, we recover the original NLL loss. For β = 1 the gradient w.r.t. µ in Eq. 8 is
equivalent to the one of MSE. However, for the variance, the gradient in Eq. 9 is a new quantity with
2σ[2] in the denominator. For values 0 < β < 1, we get different loss interpolations. Particularly
interesting is the case of β = 0.5, where the data points are weighted with _σ[1]_ [(inverse standard]

deviation instead of inverse variance). In our experiments (Sec. 5), we find that β = 0.5 generally
achieves the best trade-off between accuracy and log-likelihood. A Pytorch implementation of the
loss function is provided in Sec. D.5.

Note that the new loss _β_ NLL is not meant for performance evaluation, rather it is designed to result
_L_ _−_
in meaningful gradients. Due to the weighting term, the loss value does not reflect the model’s quality.
The model performance during training should be monitored with the original negative log-likelihood
objective and, optionally, with RMSE for testing the quality of the mean fit.


-----

10[−][6] 10[−][3] 10[0] 10[−][6] 10[−][3] 10[0] 10[−][6] 10[−][3] 10[0]

(a) NLL (b) MSE (c) _β_ NLL with β = 0.5
_L_ _L_ _L_ _−_

1500 1500 1500

1000 1000 1000

Count Count Count

500 500 500

0 0 0

Residual Error Residual Error Residual Error

Figure 6: Distribution of residual prediction errors depending on the loss function for the ObjectSlide
dataset (see Sec. 5.2). Dashed lines show predictive RMSE. (a) The NLL loss ( NLL) yields
_L_
multimodal residuals. There is a long tail of difficult data points that are ignored, while easy ones are
fit to high accuracy. (b) The MSE loss ( MSE) results in a log-normal residual distribution. (c) Our
_L_
_β−NLL loss (Lβ−NLL) yields highly accurate fits on easy data points without ignoring difficult ones._

4.2 ALLOCATION OF FUNCTION APPROXIMATOR CAPACITY

Even though MSE, NLL, and _β_ NLL all have the same optima w.r.t. the mean (and also the
_L_ _L_ _L_ _−_
variance in the case of NLL and _β_ NLL), optimizing them leads to very different solutions. In
_L_ _L_ _−_
particular, because these losses weight data points differently, they assign the capacity of the function
_approximator differently. Whereas the MSE loss gives the same weighting to all data points, the_
NLL loss gives high weight to data points with low predicted variance and low weight to those with
high variance. β−NLL interpolates between the two. The behavior of the NLL loss is appropriate
if these variances are caused by true aleatoric uncertainty in the data. However, due to the use of
function approximation, there is also the case where data points cannot be well predicted (maybe
only transiently). This would result in high predicted variance, although the ground truth is corrupted
by little noise. The different loss functions thus vary in how they handle these difficult data points.

An example of how the differences between the losses manifest in practice is illustrated in Fig. 6.
Here we show the distribution of the residuals for a dynamics prediction dataset containing easy
and hard to model areas. The NLL loss essentially ignores a fraction of the data by predicting high
uncertainty. By analyzing those data points, we found that they were actually the most important
_data points to model correctly (because they captured non-trivial interactions in the physical world)._

How important are the data points with high uncertainty? Are they outliers (i.e. do they stem from
truly noisy regions), to which we would be willing to allocate less of the function approximator’s
capacity? Or are they just difficult samples that are important to fit correctly? The answer is taskdependent and, as such, there is no one-loss-fits-all solution. Rather, the modeler should choose
which behavior is desired. Our β−NLL loss makes this choice available through the β parameter.

5 EXPERIMENTS

In our experiments, we ask the following questions and draw the following conclusions:

Sec. 5.1: Does β−NLL fix the pitfall with NLL’s convergence?
**Yes, β−NLL converges to good mean and uncertainty estimates across a range of β values.**

Sec. 5.2: Does β−NLL improve over NLL in practical settings? How sensitive to hyperparameters
_is β−NLL? We investigate a diverse set of real-world domains: regression on the UCI_
datasets, dynamics model learning, generative modeling on MNIST and Fashion-MNIST,
and depth-map prediction from natural images.
**Yes, β−NLL generally performs better than NLL and is considerably easier to tune.**

Sec. 5.3: How does β−NLL compare to other loss functions for distributional regression? We
compare with a range of approaches: learning to match the moments of a Gaussian
(termed “moment matching” (MM); see Sec. A), using a Student’s t-distribution instead of
a Gaussian (Detlefsen et al., 2019), or putting different priors on the variance and using
variational inference (xVAMP, xVAMP*, VBEM, VBEM*) (Stirn & Knowles, 2020).
**It depends. Different losses make different trade-offs, which we discuss in Sec. 5.3.**


-----

(a) β = 0 (NLL) (b) β = 0.5 (c) β = 1


1e-03

3e-04

1e-04

3e-05


1e-03

3e-04

1e-04

3e-05


1e-03

3e-04

1e-04

3e-05


0.2

0.1


0.2

0.1


0.2

0.1


0.0


0.0


0.0


Architecture Architecture Architecture

Figure 7: Convergence properties analyzed on the sinusoidal regression problem. RMSE after 200 000
epochs, averaged over 3 independent trials, is displayed by color codes (lighter is better) as a function
of learning rate and model architecture (see Sec. D.1). The original NLL (β = 0) does not obtain
good RMSE fits for most hyperparameter settings. Figure S3 shows results for the NLL metric.

(a) NLL (b) β = 0.25 (c) β = 0.5 (d) β = 1 (e) MM (f) std. dev.


10


10

0

gt std
data std

0 10

0 10

0 10


Figure 8: Fits for the heteroscedastic sine example from Detlefsen et al. (2019) (a-e). Dotted lines

0 10

show the ground truth mean and ±2σ, respectively. (f) The predicted standard deviations (with

_β = 1_

0 10

shaded std. over 10 independent trials) with the same color code. Note that β = 0.5 and β = 1 graphs
lie on top of one another. Inside the training regime, all β−NLL variants (a-d) yield well-calibrated
uncertainty estimates. Moment matching (e) significantly underestimates the variance everywhere.


We refer the reader to Sec. C and Sec. D for a description of datasets and training settings. We
evaluate the quality of predictions quantitatively in terms of the root mean squared error (RMSE) and
the negative log-likelihood (NLL).

5.1 SYNTHETIC DATASETS


**Sinusoidal without heteroscedastic noise** We first perform an extended investigation of our illustrative example from Fig. 1 – a sine curve with a small additive noise: y = 0.4 sin(2πx) + ξ,
with ξ being Gaussian noise with standard deviation σ = 0.01. One would expect that a network
with sufficient capacity can easily learn to fit this function. Figure 7 inspects this over a range of
architectures and learning rates.

We find that for the standard NLL loss (β = 0), the networks do not converge to a reasonable mean
fit. There is a trend that larger networks and learning rates show better results, but when comparing
this to β−NLL with β = 0.5 we see that the networks are indeed able to fit the function without any
issues. As expected, the same holds for the mean squared error loss (MSE) and β−NLL with β = 1.
The quality of the fit w.r.t. NLL is shown in Fig. S3.

**Sinusoidal with heteroscedastic noise** We sanity-check that β−NLL is still delivering good
uncertainty estimates on the illustrative example from Detlefsen et al. (2019) – a sine curve with
increasing amplitude and noise: y = x sin(x) + xξ1 + ξ2, with ξ1 and ξ2 being Gaussian noise with
standard deviation σ = 0.3. Figure 8 (a-e) displays the predictions of the best models (w.r.t. NLL
validation loss) and (f) compares the predicted uncertainties over 10 independent trials. Fitting the
mean is achieved with all losses. On the training range, β−NLL with β > 0 learns virtually the same
uncertainties as the NLL loss.

5.2 REAL-WORLD DATASETS


**UCI Regression Datasets** As a standard real-world benchmark in predictive uncertainty estimation,
we consider the UCI datasets (Hernandez-Lobato & Adams´, 2015). Table 1 gives an overview
comparing different loss variants. We refer to Sec. B.4 for the full results on all 12 datasets. The
results are encouraging: β−NLL achieves predictive log-likelihoods on par with or better than the
NLL loss while clearly improving the predictive accuracy on most datasets.


-----

Table 1: Results for UCI Regression Datasets. We report predictive log-likelihood and RMSE (±
standard deviation). Ties denotes the number of datasets (out of 12) for which the method cannot be
statistically distinguished from the best method (see Sec. B.4). We compare with Student-t (Detlefsen
et al., 2019) and xVAMP/VBEM (Stirn & Knowles, 2020). Section B.4 lists the full results.

LL ↑ RMSE ↓

Loss _β_ Ties concrete energy naval yacht Ties concrete energy naval yacht


_LLLLLLLStudent-txVAMPxVAMP*VBEMVBEM*βββββMMMSE−−−−−NLLNLLNLLNLLNLL_ 00.250.50.751.0 10 -3.07—3 -3.254 -3.315 -3.295 -3.270 -3.492 -3.236 -3.067 -3.032 -3.148 -2.99 ± ± ± ± ± ± ± ± ± ± ±— 0.31 -3.22 0.51 -2.82 0.36 -2.41 0.34 -2.80 0.33 -3.37 0.38 -4.26 0.14 -2.46 0.15 -2.47 0.13 -2.41 0.07 -4.29 0.13 -1.91 ± ± ± ± ± ± ± ± ± ± ±— 1.41 12.46 0.82 13.78 0.72 13.99 0.59 13.63 0.58 13.59 0.50 12.73 0.34 12.47 0.32 12.44 0.32 12.80 0.16 8.05 0.21 13.10 ± ± ± ± ± ± ± ± ± ± ±— 0.13 1.18 -2.86 0.33 -1.97 0.40 -2.47 0.62 -1.87 0.30 -2.27 0.64 -11.2 0.48 -1.23 0.60 -0.99 0.55 -1.04 0.47 -0.98-2.65 ± ± ± ± ± ± ± ± ± ± ±— 5.18 1.14 1.68 0.55 1.07 31.0 0.55 0.33 0.47 0.10 0.24 12 4.965 6.086 5.797 5.618 5.679 5.555 6.285 5.828 5.448 5.359 5.219 5.17 ± ± ± ± ± ± ± ± ± ± ± ± 0.65 2.25 0.74 1.81 0.65 1.12 0.73 1.31 0.77 1.54 0.82 2.19 0.64 0.92 0.59 2.26 0.64 1.87 0.73 2.00 0.58 1.29 0.59 1.08 ± ± ± ± ± ± ± ± ± ± ± ± 0.34 0.0021 0.30 0.0012 0.25 0.0006 0.45 0.0004 0.54 0.0004 0.28 0.0005 0.11 0.0004 0.34 0.0026 0.32 0.0023 0.26 0.0020 0.33 0.0009 0.17 0.0015 ± ± ± ± ± ± ± ± ± ± ± ± 0.0006 1.22 0.0004 1.73 0.0002 2.35 0.0001 1.97 0.0000 2.08 0.0001 3.02 0.0001 0.78 0.0009 1.34 0.0004 0.99 0.0006 1.13 0.0004 1.66 0.0005 0.65 ± ± ± ± ± ± ± ± ± ± ± ± 0.47 1.00 1.44 1.03 1.13 1.38 0.25 0.63 0.43 0.66 0.84 0.20

(a) ObjectSlide (b) Fetch-PickAndPlace

|LNLL Lβ-NLL(β =0.5) Lβ-NLL(β =1.0)|Col2|Col3|
|---|---|---|
||||


_−10_ _−5_

NLL


RMSE


_×10_ _[−][2]_



Density

0 1 2 3 4 _−20_ _−10_

RMSE _×10_ _[−][3]_ NLL


Figure 9: Sensitivity analysis of loss functions to hyperparameters on the dynamics model learning
tasks: ObjectSlide (a) and Fetch-PickAndPlace (b). The distributions over validation RMSE and NLL
are shown as a function of hyperparameters, based on a grid search over different model configurations
(see Sec. D.2). While the NLL loss is highly sensitive when evaluating RMSE, the β−NLL loss
shows much less sensitivity and yields good results regardless of the exact configuration.

**Dynamics models** As a major application of uncertainty estimation lies in model-based reinforcement learning (RL), we test the different loss functions on two dynamics predictions tasks of varying
difficulty, ObjectSlide, and Fetch-PickAndPlace. In both tasks, the goal is to predict how an object will move from the current state and the agent’s action. Whereas ObjectSlide (Seitzer et al.,
2021) is a simple 1D-environment, Fetch-PickAndPlace (Plappert et al., 2018) is a complex 3D
robotic-manipulation environment. The models are trained on trajectories collected by RL agents.

For both datasets, we perform a grid search over different hyperparameter configurations (see Sec. D.2)
for a sensitivity analysis to hyperparameters settings, presented in Fig. 9. It reveals that NLL is
vulnerable to the choice of hyperparameters, whereas β−NLL achieves good results over a wide
range of configurations. The best performing configurations for each loss are then evaluated on a
hold-out test set (Table 2). One can see that the NLL loss results in poor predictive performance
and also exhibits quite a high variance across random seeds. Our method yields high accuracy and
log-likelihood fits for a range of β values, with β = 0.5 generally achieving the best trade-off.

**Generative modeling and depth-map prediction** For generative modeling, we train variational
autoencoders (Kingma & Welling, 2014) with probabilistic decoders on MNIST and Fashion-MNIST.
For the task of depth regression, we modify a state-of-the-art method (AdaBins; Bhat et al. (2021))
and test it on the NYUv2 dataset (Silberman et al., 2012) with our loss (Fig. S6). Table 3 presents
selected results for both tasks, yielding similar trends as before. We refer to Sec. B.5 and Sec. B.6 for
more details, including qualitative results.


5.3 COMPARISON TO OTHER LOSS FUNCTIONS

The previous sections have demonstrated that our β−NLL loss has clear advantages over the NLL
loss. However, the comparison to other loss functions requires a more nuanced discussion. First,


-----

Table 2: Test results for dynamics models, using best configurations found in a grid search. The
reported standard deviations are over 5 random seeds. We compare with Student-t (Detlefsen et al.,
2019) and xVAMP/VBEM (Stirn & Knowles, 2020).

**1D-Slide** **Fetch-PickAndPlace**

Loss _β_ RMSE ↓ LL ↑ RMSE ↓ LL ↑

_Lβ−NLL_ 0 0.0192 ± 0.006 7.97 ± 3.62 0.00163 ± 0.00008 18.72 ± 7.32
_Lβ−NLL_ 0.25 0.0107 ± 0.004 9.03 ± 0.47 0.00102 ± 0.00004 24.43 ± 1.64
_Lβ−NLL_ 0.5 0.0064 ± 0.002 9.28 ± 0.75 0.00096 ± 0.00002 24.68 ± 0.08
_Lβ−NLL_ 0.75 0.0087 ± 0.003 6.61 ± 1.83 0.00098 ± 0.00001 22.77 ± 0.17
_Lβ−NLL_ 1.0 0.0074 ± 0.001 6.58 ± 0.29 0.00102 ± 0.00001 21.32 ± 0.07
_LMM_ 0.0078 ± 0.001 diverges 0.00104 ± 0.00003 19.33 ± 1.31
_LMSE_ 0.0068 ± 0.001 — 0.00103 ± 0.00000 —
Student-t 0.0155 ± 0.006 11.30 ± 0.03 0.00117 ± 0.00001 30.44 ± 0.08
xVAMP 0.0118 ± 0.002 10.58 ± 0.19 0.00128 ± 0.00005 29.02 ± 0.12
xVAMP* 0.0199 ± 0.006 10.89 ± 0.10 0.00128 ± 0.00001 29.19 ± 0.08
VBEM 0.0039 ± 0.000 3.79 ± 0.00 0.00104 ± 0.00003 17.39 ± 0.29
VBEM* 0.0280 ± 0.011 10.13 ± 0.49 0.00118 ± 0.00003 28.62 ± 0.15

Table 3: Selected results for generative modeling and depth-map prediction. Left: Training variational
autoencoders on MNIST and Fashion-MNIST. Right: Depth-map prediction on NYUv2. Full results
can be found in Table S3 and Table S4.


**MNIST** **Fashion-MNIST**

Loss _β_ RMSE ↓ LL ↑ RMSE ↓ LL ↑

_Lβ−NLL_ 0 0.237 ± 0.002 2116 ± 55 0.170 ± 0.001 1940 ± 104
_Lβ−NLL_ 0.5 0.151 ± 0.003 2220 ± 25 0.125 ± 0.003 1639 ± 52
_Lβ−NLL_ 1.0 0.152 ± 0.001 1706 ± 30 0.138 ± 0.002 1142 ± 26
Student-t 0.273 ± 0.002 4291 ± 103 0.182 ± 0.002 2857 ± 9
xVAMP* 0.225 ± 0.001 3062 ± 215 0.160 ± 0.002 2150 ± 131
VBEM* 0.176 ± 0.008 3213 ± 238 0.150 ± 0.003 2244 ± 78


**NYUv2**

Loss _β_ RMSE ↓ LL ↑

_Lβ−NLL_ 0 0.3854 -4.52
_Lβ−NLL_ 0.5 0.3789 -7.50
_Lβ−NLL_ 1.0 0.3845 -5.10
_LMSE_ 0.3776 —
_L1_ 0.3850 —
SI Loss 0.419 —


we also test an alternative loss function based on matching the moments of the Gaussian ( MM; see
_L_
Sec. A). While this loss results in high accuracy, it is unstable to train and exhibits poor likelihoods.

Second, we test several loss functions based on a Student’s t-distribution, including xVAMP and
VBEM (Stirn & Knowles, 2020). These approaches generally achieve better likelihoods than the
_β−NLL; we conjecture this is because their ability to maintain uncertainty about the variance results_
in a better fit (in terms of KL divergence) when the variance is wrongly estimated. In terms of
predictive accuracy, β−NLL outperforms the Student’s t-based approaches. Exceptions are some of
the UCI datasets with limited data (e.g. “concrete” and “yacht”) where xVAMP and VBEM are on par
or better than β−NLL. This is likely because these methods can mitigate overfitting by placing a prior
on the variance. However, xVAMP and VBEM are also non-trivial to implement and computationally
heavy: both need MC samples to evaluate the prior; for xVAMP, training time roughly doubles as
evaluating the prior also requires a second forward pass through the network. In contrast, β−NLL is
simple to implement (see Sec. D.5) and introduces no additional computational costs.

6 CONCLUSION

We highlight a problem frequently occurring when optimizing probabilistic neural networks using
the common NLL loss: training gets stuck in suboptimal function fits. With our analysis, we reveal
the underlying reason: initially badly-fit regions receive increasingly less weight in the loss which
results in premature convergence. We propose a simple solution by introducing a family of loss
functions called β−NLL. Effectively, the gradient of the original NLL loss is scaled by the βexponentiated per-sample variance. This allows for a meaningful interpolation between the NLL
and MSE loss functions while providing well-behaved uncertainty estimates. The hyperparameter
_β gives practitioners the choice to control the self-regularization strength of NLL: how important_
should high-noise regions or difficult-to-predict data points be in the fitting process. In most cases,
_β = 0.5 will be a good starting point. We think the problem discussed in this paper is primarily why_
practitioners using the Gaussian distribution in regression or generative modeling tasks often opt
for a constant or homoscedastic (global) variance, as opposed to the more general heteroscedastic
(data-dependent) variance. We hope that our simple solution contributes to changing this situation by
improving the usability and performance of modeling data uncertainty with deep neural networks.


-----

ACKNOWLEDGMENTS

The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS)
for supporting Maximilian Seitzer. Georg Martius is a member of the Machine Learning Cluster
of Excellence, EXC number 2064/1 – Project number 390727645. We acknowledge the financial
support from the German Federal Ministry of Education and Research (BMBF) through the Tubingen¨
AI Center (FKZ: 01IS18039B).

REPRODUCIBILITY STATEMENT

All settings are described in detail in Sec. C and Sec. D. We make full code and data available under
[https://github.com/martius-lab/beta-nll.](https://github.com/martius-lab/beta-nll)

REFERENCES

Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mane.´
Concrete problems in AI safety. ArXiv, abs/1606.06565, 2016.

Shariq Farooq Bhat, Ibraheem Alhashim, and Peter Wonka. AdaBins: Depth estimation using
adaptive bins. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition
_(CVPR), pp. 4008–4017, Los Alamitos, CA, USA, jun 2021. IEEE Computer Society. doi:_
[10.1109/CVPR46437.2021.00400. URL https://doi.ieeecomputersociety.org/](https://doi.ieeecomputersociety.org/10.1109/CVPR46437.2021.00400)
[10.1109/CVPR46437.2021.00400.](https://doi.ieeecomputersociety.org/10.1109/CVPR46437.2021.00400)

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. OpenAI Gym. ArXiv, abs/1606.01540, 2016.

Xiaotian Chen, Xuejin Chen, and Zheng-Jun Zha. Structure-aware residual pyramid network for
monocular depth estimation. In Proceedings of the Twenty-Eighth International Joint Conference
_on Artificial Intelligence, IJCAI-19, pp. 694–700. International Joint Conferences on Artificial_
[Intelligence Organization, 7 2019. doi: 10.24963/ijcai.2019/98. URL https://doi.org/10.](https://doi.org/10.24963/ijcai.2019/98)
[24963/ijcai.2019/98.](https://doi.org/10.24963/ijcai.2019/98)

Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In
S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 31. Curran Asso[ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/](https://proceedings.neurips.cc/paper/2018/file/3de568f8597b94bda53149c7d7f5958c-Paper.pdf)
[3de568f8597b94bda53149c7d7f5958c-Paper.pdf.](https://proceedings.neurips.cc/paper/2018/file/3de568f8597b94bda53149c7d7f5958c-Paper.pdf)

Will Dabney, Georg Ostrovski, David Silver, and Remi Munos. Implicit quantile networks for´
distributional reinforcement learning. In Jennifer Dy and Andreas Krause (eds.), Proceedings of
_the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine_
_[Learning Research, pp. 1096–1105. PMLR, 10–15 Jul 2018. URL https://proceedings.](https://proceedings.mlr.press/v80/dabney18a.html)_
[mlr.press/v80/dabney18a.html.](https://proceedings.mlr.press/v80/dabney18a.html)

Nicki S. Detlefsen, Martin Jørgensen, and Søren Hauberg. Reliable training and estimation of
variance networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and´
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As[sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/](https://proceedings.neurips.cc/paper/2019/file/07211688a0869d995947a8fb11b215d6-Paper.pdf)
[07211688a0869d995947a8fb11b215d6-Paper.pdf.](https://proceedings.neurips.cc/paper/2019/file/07211688a0869d995947a8fb11b215d6-Paper.pdf)

Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model
uncertainty in deep learning. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings
_of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine_
_Learning Research, pp. 1050–1059, New York, New York, USA, 20–22 Jun 2016. PMLR. URL_
[https://proceedings.mlr.press/v48/gal16.html.](https://proceedings.mlr.press/v48/gal16.html)

Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep Bayesian active learning with image data.
In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference


-----

_on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 1183–_
[1192. PMLR, 06–11 Aug 2017. URL https://proceedings.mlr.press/v70/gal17a.](https://proceedings.mlr.press/v70/gal17a.html)
[html.](https://proceedings.mlr.press/v70/gal17a.html)

Ali Harakeh and Steven L. Waslander. Estimating and evaluating regression predictive uncertainty
in deep object detectors. In International Conference on Learning Representations (ICLR), 2021.
[URL https://openreview.net/forum?id=YLewtnvKgR7.](https://openreview.net/forum?id=YLewtnvKgR7)

Jose Miguel Hern´ andez-Lobato and Ryan P. Adams. Probabilistic backpropagation for scalable´
learning of Bayesian neural networks. In Francis Bach and David Blei (eds.), Proceedings of
_the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine_
_[Learning Research, pp. 1861–1869, Lille, France, 07–09 Jul 2015. PMLR. URL https://](https://proceedings.mlr.press/v37/hernandez-lobatoc15.html)_
[proceedings.mlr.press/v37/hernandez-lobatoc15.html.](https://proceedings.mlr.press/v37/hernandez-lobatoc15.html)

Alex Kendall and Yarin Gal. What uncertainties do we need in Bayesian deep learning for computer
vision? In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran As[sociates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/](https://proceedings.neurips.cc/paper/2017/file/2650d6089a6d640c5e85b2b88265dc2b-Paper.pdf)
[2650d6089a6d640c5e85b2b88265dc2b-Paper.pdf.](https://proceedings.neurips.cc/paper/2017/file/2650d6089a6d640c5e85b2b88265dc2b-Paper.pdf)

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
_Conference on Learning Representations (ICLR), 2015._

Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. In International Conference
_on Learning Representations (ICLR), 2014._

Armen Der Kiureghian and Ove Ditlevsen. Aleatory or epistemic? does it matter? _Struc-_
_tural Safety, 31(2):105–112, 2009. ISSN 0167–4730. doi: https://doi.org/10.1016/j.strusafe._
2008.06.020. [URL https://www.sciencedirect.com/science/article/pii/](https://www.sciencedirect.com/science/article/pii/S0167473008000556)
[S0167473008000556. Risk Acceptance and Risk Communication.](https://www.sciencedirect.com/science/article/pii/S0167473008000556)

Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,
R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing
_[Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.](https://proceedings.neurips.cc/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf)_
[cc/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf.](https://proceedings.neurips.cc/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf)

Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh. From big to small: Multi-scale
local planar guidance for monocular depth estimation. ArXiv, abs/1907.10326, 2019.

David A. Nix and Andreas S. Weigend. Estimating the mean and variance of the target probability distribution. In Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN’94),
volume 1, pp. 55–60, 1994. doi: 10.1109/ICNN.1994.374138.

Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration
via bootstrapped DQN. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Curran Asso[ciates, Inc., 2016. URL https://proceedings.neurips.cc/paper/2016/file/](https://proceedings.neurips.cc/paper/2016/file/8d8818c8e140c64c743113f563cf750f-Paper.pdf)
[8d8818c8e140c64c743113f563cf750f-Paper.pdf.](https://proceedings.neurips.cc/paper/2016/file/8d8818c8e140c64c743113f563cf750f-Paper.pdf)

Cristina Pinneri, Shambhuraj Sawant, Sebastian Blaes, and Georg Martius. Extracting strong
policies for robotics tasks from zero-order trajectory optimizers. In International Conference on
_[Learning Representations (ICLR), 2021. URL https://openreview.net/forum?id=](https://openreview.net/forum?id=Nc3TJqbcl3)_
[Nc3TJqbcl3.](https://openreview.net/forum?id=Nc3TJqbcl3)

Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker, Glenn Powell,
Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder, Vikash Kumar, and Wojciech
Zaremba. Multi-goal reinforcement learning: Challenging robotics environments and request for
research. ArXiv, abs/1802.09464, 2018.

Maximilian Seitzer, Bernhard Scholkopf, and Georg Martius. Causal influence detection for im-¨
proving efficiency in reinforcement learning. In M. Ranzato, A. Beygelzimer, K. Nguyen, P. S.
Liang, J. W. Vaughan, and Y. Dauphin (eds.), Advances in Neural Information Processing Systems,


-----

[volume 34. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/](https://proceedings.neurips.cc/paper/2021/file/c1722a7941d61aad6e651a35b65a9c3e-Paper.pdf)
[paper/2021/file/c1722a7941d61aad6e651a35b65a9c3e-Paper.pdf.](https://proceedings.neurips.cc/paper/2021/file/c1722a7941d61aad6e651a35b65a9c3e-Paper.pdf)

Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support
inference from RGBD images. In Andrew Fitzgibbon, Svetlana Lazebnik, Pietro Perona, Yoichi
Sato, and Cordelia Schmid (eds.), Computer Vision – ECCV 2012, pp. 746–760, Berlin, Heidelberg,
2012. Springer Berlin Heidelberg. ISBN 978-3-642-33715-4.

Andrew Stirn and David A. Knowles. Variational variance: Simple and reliable predictive variance
parameterization. ArXiv, abs/2006.04910, 2020.

Hiroshi Takahashi, Tomoharu Iwata, Yuki Yamanaka, Masanori Yamada, and Satoshi Yagi. Student-t
variational autoencoder for robust density estimation. In Proceedings of the Twenty-Seventh
_International Joint Conference on Artificial Intelligence, IJCAI-18, pp. 2696–2702. International_
Joint Conferences on Artificial Intelligence Organization, 7 2018. doi: 10.24963/ijcai.2018/374.
[URL https://doi.org/10.24963/ijcai.2018/374.](https://doi.org/10.24963/ijcai.2018/374)

Marin Vlastelica, Sebastian Blaes, Cristina Pinneri, and Georg Martius. Risk-averse zero-order trajectory optimization. In Aleksandra Faust, David Hsu, and Gerhard Neumann (eds.), Proceedings of
_the 5th Conference on Robot Learning, volume 164 of Proceedings of Machine Learning Research,_
[pp. 444–454. PMLR, 08–11 Nov 2021. URL https://proceedings.mlr.press/v164/](https://proceedings.mlr.press/v164/vlastelica22a.html)
[vlastelica22a.html.](https://proceedings.mlr.press/v164/vlastelica22a.html)

Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y. Zou, Sergey Levine,
Chelsea Finn, and Tengyu Ma. MOPO: Model-based offline policy optimization. In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances
_in Neural Information Processing Systems, volume 33, pp. 14129–14142. Curran Asso-_
[ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/](https://proceedings.neurips.cc/paper/2020/file/a322852ce0df73e204b7e67cbbef0d0a-Paper.pdf)
[a322852ce0df73e204b7e67cbbef0d0a-Paper.pdf.](https://proceedings.neurips.cc/paper/2020/file/a322852ce0df73e204b7e67cbbef0d0a-Paper.pdf)


-----

## APPENDIX

A THE MOMENT MATCHING LOSS

We also investigated an alternative loss function designed to counter the imbalanced weighting of
data points by the NLL loss, which we call “moment matching” (MM). It uses standard squared
error losses to estimate the moments of the target distribution, i.e. directly estimating the sufficient
statistics of the target distribution. In our experiments, we found that this loss function generally fixes
the problems with premature convergence when using NLL, but it also leads to underestimation of
_L_
variances and exhibits considerable training instabilities.

To fully describe a Gaussian distribution, only the first two moments µ, σ[2] need to be estimated. So,
why not directly define losses based on the moment estimators? Starting from the conditional mean
of the targets, E[Y | X], we can define the squared deviation as a loss and see that the MSE is an
upper bound:

(E[Y | X] − _µˆ(X))[2]_ = E[(Y − _µˆ(X)) | X][2]_ _≤_ E (Y − _µˆ(X))[2]_ _| X_ := LµMMˆ _[.]_ (S1)

_µ_  
Notice that LMM[ˆ] [is the standard MSE loss. Thus, learning a mean fit][ ˆ]µ(x) can follow the standard
(non-distributional) regression procedures. Interestingly, due to the moment matching viewpoint, we
can analogously define a loss for fitting the variance (or the second central moment). Variance is
defined asE (Y _µˆ E(X(Y)) −[2]_ _µσˆ(X[2]())X[2])_ _| X2_ _X. As such, by analogy, we can define the following loss:. To use the same physical unit as_ _µMM[, we reformulate it in] LσMM[ˆ][2]_ [:=]

_−_ _−_ _|_ _L[ˆ]_
terms of the standard deviation as:h   i

2

_LσMM[ˆ]_ [:=][ E]"q(Y − _µˆ(X))[2]_ _−_ _σˆ(X)_ _[X]#_ _._ (S2)

Thus, the moment matching loss can be expressed simply by the sum of the two losses:µ _σ_ _σ_ _σ[2]_ _LMM =_
_LMM[ˆ]_ [+][ L]MM[ˆ] [. We found that using][ L]MM[ˆ] [instead of][ L]MM[ˆ] [makes it easier to balance the losses.]

Interestingly, our β−NLL subsumes the moment matching loss if we allow for different values of β
to be used for mean and variance estimation. In particular, usingand β = 2 for the variance results in the same gradients as using _β[1]2_ _[L]−µMM[ˆ]NLL[+] with[ 1]4_ _[L]σMM[ˆ] β[2]_ = 1[as the loss. Note] for the mean

that we did not investigate this connection further, and also did not perform any experiments with
different values of β for mean and variance.

B ADDITIONAL RESULTS

B.1 FURTHER CONSEQUENCES OF INVERSE-VARIANCE WEIGHTING

In Sec. 3.2, we interpreted the inverse-variance weighting of the NLL as training under a different
data distribution _P[˜](X, Y ) in which points with high error have a low probability of being sampled. A_
consequence of this is that the distribution _P[˜](X, Y ) continuously shifts while the model is improving_
its fit. For datasets where the noise level is low on parts of the input space and the underlying
function can be modeled to high accuracy, an interesting phenomenon can be observed: even though
the training curve for RMSE looks like it indicates convergence (i.e. it flattens out), the model can
actually still be adapting with the changing training distribution. It is just that the changes happen
on data points that already have such low prediction error relative to the average error that further
improvements are virtually invisible in the average error. The actual training progress can be revealed
using a histogram of the prediction errors on a log-scale, in the manner of Fig. 6. Eventually, the
training distribution stabilizes when for all data points, either the prediction error reaches the noise
level at that point, or the variance can not decrease further at that point because it reaches a manually
set lower bound. If no lower bound on the variance is set, the distribution may never stabilize, leading
to training instabilities when the variance reaches close to zero.


-----

B.2 SYNTHETIC DATASET

In Fig. S1, we replicate the experiment from Fig. 1 (training the NLL loss on a sinusoidal for 10[7]
updates) on several more random seeds. In order to test the dependence of our reported issue on
the optimizer, we repeat the experiment from Fig. 1 but use different optimizers than Adam with
_β1 = 0.9, β2 = 0.999. The results are shown in Fig. S2. We find that none of the configurations_
reaches below an RMSE of 0.1 (the optimal value corresponds to an RMSE of 0.01), indicating that
the issue is occurring stably across optimization settings. In Fig. S3, we provide the results for the
NLL metric on the sinusoidal dataset, complementing the results of Fig. 7 (see discussion in Sec. 5.1).


0.4


0.4

0.0


0.4

0.0


0.0

-0.4


-0.4


-0.4


12


12


12


Input X


Input X


Input X


Figure S1: Repeating the experiment from Fig. 1, i.e. training with NLL loss for 10[7] update steps.
The observed behavior is stable across different independent trials.


0.30

0.25

0.20

0.15

0.10


0.30

0.25

0.20

0.15

0.10


0.05


0.001

0.0005


0.01

0.005


0.0001


Figure S2: Using different optimizers to train on the sinusoidal from Fig. 1 with the NLL loss. The
color code indicates the mean RMSE over 3 independent trials per optimizer setting. Black indicates
that all trials diverged. Training was done for 2 · 10[6] update steps and used architecture 2 from
Table S5. The observed behavior is stable across optimization settings.

(a) β = 0 (NLL) (b) β = 0.5 (c) β = 1


1e-03

3e-04

1e-04

3e-05


1e-03

3e-04

1e-04

3e-05


1e-03

3e-04

1e-04

3e-05


_−1_

_−2_


_−1_

_−2_


_−1_

_−2_


0 1 2 3 4 _−3_ 0 1 2 3 4 _−3_ 0 1 2 3 4 _−3_

Architecture Architecture Architecture

Figure S3: Convergence properties analyzed on the sinusoidal toy regression problem. Same as
Fig. 7 but for the negative log-likelihood (NLL) criterion. Due to the bad mean fit, the original NLL
loss (β = 0) is also bad for most hyperparameter settings. With β > 0.5 good fits are obtained for
many settings. Also for β = 1, which corresponds to MSE for fitting the mean, good uncertainty
predictions are obtained with our β−NLL as testified by the low NLL scores.


B.3 ANALYSIS OF SAMPLING PROBABILITIES ON FETCH-PICKANDPLACE

In Fig. S4, we show how the analysis from Sec. 3.2 transfers to a real world dataset, namely FetchPickAndPlace. The figure shows how the distribution of effective sampling probability evolves
during training (over a fixed set of training points) and compares that against a proxy “oracle”: the
distribution of effective sampling probabilities when using the squared residuals from a model trained


-----

with the MSE loss. The mismatch between the two distributions demonstrates that optimizing the
NLL loss drastically undersamples in comparison to the reference, effectively never sampling some
data points. This further corroborates our analysis in Sec. 3.2.


|Initial|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|LNLL LMSE (trained)||||||||
|||||||||
|||||||||
|||||||||
|||||||||


10[−][8] 10[−][5] 10[−][2]

_LNLL_
_LMSE (trained)_

Sampling Probability


Epoch 1

10[−][8] 10[−][5] 10[−][2]

Sampling Probability


Epoch 10


600

400

200

0

600

400

200


600

400

200

0

600

400

200


600

400

200

0

600

400

200


10[−][8] 10[−][5] 10[−][2]

Sampling Probability

Epoch 200


Epoch 50

10[−][8] 10[−][5] 10[−][2]

Sampling Probability


Epoch 100

10[−][8] 10[−][5] 10[−][2]

Sampling Probability


10[−][8] 10[−][5] 10[−][2]

Sampling Probability


Figure S4: Undersampling behavior of LNLL on Fetch-PickAndPlace. The plot shows how the
distribution of effective sampling probability evolves over training time, taken over 2 000 fixed
training points sampled at the initial epoch. The dashed blue histogram shows the distribution of
effective sampling probabilities when using the squared residuals from a model trained with MSE loss
_LMSE. This gives a reference distribution that LNLL should roughly match, taking into account the_
relative hardness of prediction on different samples. The LNLL drastically undersamples compared to
the reference (note the log-scale), effectively never sampling some points.

B.4 UCI DATASETS


In this section, we include results for predictive log-likelihood (Table S1) and RMSE (Table S2) for
all UCI datasets we evaluated on.

We find that baselines based on the Student’s t-distribution (xVAMP and VBEM (Stirn & Knowles,
2020)) tend to have better predictive log-likelihood than β−NLL, although there is also a dataset
where β−NLL performs better (“naval”) or where there is no statistically significant improvement
(“housing”, “kin8m”, “wine-red”, “wine-white”). For RMSE, LMSE unsurprisingly performs best.
Our β−NLL is often on par with LMSE and on par or better than the other baselines, except for
“yacht”. At the same time, our method is very simple to implement (see Sec. D.5) and computationally
lightweight compared to xVAMP and VBEM, which require the costly evaluation of a prior and
Monte-Carlo sampling at each training step.

B.5 GENERATIVE MODELING WITH VARIATIONAL AUTOENCODERS


We test different loss functions on the task of generative modeling using variational autoencoders
(VAEs) (Kingma & Welling, 2014). To this end, we parameterize the decoder distribution p(x | z)
with N _µ(z), σ[2](z)_, where the mean µ(z) and variance σ[2](z) are outputs of a neural network. We
train the VAE by maximizing the ELBO Eq(z _x)[log p(x_ _z)]_ _DKL(q(z_ _x)_ _p(z)), plugging in_
   _|_ _|_ _−_ _|_ _||_
different loss functions for log p(x | z). Following Stirn & Knowles (2020), we evaluate the logposterior predictive likelihood log Eq(z _x)[p(x_ _z)]. We approximate the expectation using a finite_
_|_ _|_
mixture of 20 Monte-Carlo samples from q(z | x). To compute the RMSE, we take the mean of that
mixture. We compare β−NLL against LMM, LNLL with a fixed variance of 1, Student-t (Takahashi
et al., 2018; Detlefsen et al., 2019), xVAMP and VBEM (Stirn & Knowles, 2020).

We evaluate on MNIST and FashionMNIST. Table S3 presents quantitative results. VBEM achieves
the best reconstruction error at the expense of poor log-likelihood. Vice versa, Student-t, xVAMP,
xVAMP*, and VBEM* achieve strong log-likelihoods but worse reconstruction errors. β−NLL with
_β > 0 provides a good compromise between log-likelihood and reconstruction error. Figure S5_


-----

Table S1: Results for UCI Regression Datasets. Predictive log-likelihood (higher is better) and
standard deviation, together with dataset size, input and output dimensions. Best mean value in bold.
Results that are not statistically distinguishable from the best result are marked with †.

carbon concrete energy housing
(10721, 5, 3) (1030, 8, 1) (768, 8, 2) (506, 13, 1)

_Lβ−NLL(β = 0)_ 11.36 ± 2.11 -3.25 ± 0.31 -3.22 ± 1.41 -2.86 ± 0.50
_Lβ−NLL(β = 0.25)_ 10.91 ± 2.42 -3.31 ± 0.51 -2.82 ± 0.82 -2.75 ± 0.42
_Lβ−NLL(β = 0.5)_ 10.22 ± 4.00 -3.29 ± 0.36 -2.41 ± 0.72 -2.64 ± 0.36[†]

_Lβ−NLL(β = 0.75)_ 10.82 ± 1.37 -3.27 ± 0.34 -2.80 ± 0.59 -2.72 ± 0.42[†]
_Lβ−NLL(β = 1.0)_ 3.31 ± 20.88 -3.23 ± 0.33 -3.37 ± 0.58 -2.85 ± 0.87
MM 4.72 5.80 -3.49 0.38 -4.26 0.50 -3.42 1.01
_L_ _±_ _±_ _±_ _±_
MSE — — — —
_L_
Student-t **15.59 ± 0.43** -3.07 ± 0.14[†] -2.46 ± 0.34 -2.47 ± 0.24[†]

xVAMP 13.28 ± 0.19 -3.06 ± 0.15[†] -2.47 ± 0.32 -2.43 ± 0.21[†]

xVAMP* 13.17 ± 0.26 -3.03 ± 0.13[†] -2.41 ± 0.32 -2.45 ± 0.22[†]
VBEM 5.68 ± 0.70 -3.14 ± 0.07 -4.29 ± 0.16 -2.56 ± 0.15
VBEM* 13.23 ± 0.36 **-2.99 ± 0.13** **-1.91 ± 0.21** **-2.42 ± 0.22**

kin8m naval power protein
(8192, 8, 1) (11934, 16, 2) (9568, 4, 1) (45730, 9, 1)

_Lβ−NLL(β = 0)_ 1.140 ± 0.039[†] 12.46 ± 1.18 -2.807 ± 0.057 -2.80 ± 0.05
_Lβ−NLL(β = 0.25)_ 1.142 ± 0.026[†] 13.78 ± 0.33[†] -2.801 ± 0.053 -2.79 ± 0.05
_β_ NLL(β = 0.5) 1.141 0.046[†] **13.99** **0.40** -2.805 0.052 -2.78 0.02
_L_ _−_ _±_ _±_ _±_ _±_
_Lβ−NLL(β = 0.75)_ 1.137 ± 0.041[†] 13.63 ± 0.62[†] -2.806 ± 0.053 -2.79 ± 0.02
_Lβ−NLL(β = 1.0)_ 1.126 ± 0.041 13.59 ± 0.30 -2.810 ± 0.051 -2.80 ± 0.03
MM 0.999 0.063 12.73 0.64 -2.918 0.092 -2.98 0.06
_L_ _±_ _±_ _±_ _±_
MSE — — — —
_L_
Student-t **1.155 ± 0.037** 12.47 ± 0.48 **-2.738 ± 0.026** **-2.56 ± 0.02**
xVAMP 1.147 ± 0.037[†] 12.44 ± 0.60 -2.788 ± 0.032 -2.74 ± 0.03
xVAMP* 1.147 ± 0.036[†] 12.80 ± 0.55 -2.785 ± 0.036 -2.71 ± 0.02
VBEM 1.019 ± 0.054 8.05 ± 0.13 -2.819 ± 0.018 -2.85 ± 0.01
VBEM* 1.138 ± 0.036[†] 13.10 ± 0.47 -2.783 ± 0.031 -2.73 ± 0.01

superconductivity wine-red wine-white yacht
(21263, 81, 1) (1599, 11, 1) (4898, 11, 1) (308, 6, 1)


_Lβ−NLL(β = 0)_ -3.60 ± 0.23 -1.03 ± 0.24[†] -1.059 ± 0.074[†] -2.86 ± 5.18
_Lβ−NLL(β = 0.25)_ -3.56 ± 0.14 -0.98 ± 0.12[†] -1.041 ± 0.064[†] -1.97 ± 1.14
_Lβ−NLL(β = 0.5)_ -3.60 ± 0.10 -0.99 ± 0.16[†] -1.036 ± 0.065[†] -2.47 ± 1.68
_Lβ−NLL(β = 0.75)_ -3.72 ± 0.11 -1.02 ± 0.22[†] -1.039 ± 0.060[†] -1.87 ± 0.55
_Lβ−NLL(β = 1.0)_ -3.83 ± 0.09 -0.97 ± 0.10[†] -1.040 ± 0.067[†] -2.27 ± 1.07
MM -4.45 0.39 -1.22 0.43 -1.135 0.093 -11.24 31.03
_L_ _±_ _±_ _±_ _±_
MSE — — — —
_L_
Student-t **-3.38 ± 0.04** -0.94 ± 0.10[†] -1.034 ± 0.062[†] -1.23 ± 0.55[†]

xVAMP -3.40 ± 0.03 -0.95 ± 0.06[†] -1.038 ± 0.050[†] -0.99 ± 0.33[†]

xVAMP* -3.40 ± 0.04[†] **-0.94 ± 0.06** -1.029 ± 0.048[†] -1.04 ± 0.47[†]

VBEM -3.63 ± 0.09 -0.95 ± 0.07[†] **-1.028 ± 0.048** -2.65 ± 0.10
VBEM* -3.40 ± 0.04[†] -0.94 ± 0.07[†] -1.031 ± 0.057[†] **-0.98 ± 0.24**


-----

Table S2: Results for UCI Regression Datasets. RMSE (lower is better) and standard deviation,
together with dataset size, input and output dimensions. Best mean value in bold. Results that are not
statistically distinguishable from the best result are marked with †.

carbon concrete energy housing
(10721, 5, 3) (1030, 8, 1) (768, 8, 2) (506, 13, 1)

_Lβ−NLL(β = 0)_ 0.0068 ± 0.0029[†] 6.08 ± 0.65 2.25 ± 0.34 3.56 ± 1.07[†]

_Lβ−NLL(β = 0.25)_ 0.0069 ± 0.0028[†] 5.79 ± 0.74 1.81 ± 0.30 3.48 ± 1.15[†]

_Lβ−NLL(β = 0.5)_ 0.0068 ± 0.0029[†] 5.61 ± 0.65 1.12 ± 0.25 3.42 ± 1.04[†]

_Lβ−NLL(β = 0.75)_ 0.0069 ± 0.0028[†] 5.67 ± 0.73 1.31 ± 0.45 3.43 ± 1.07[†]

_Lβ−NLL(β = 1.0)_ 0.0073 ± 0.0026[†] 5.55 ± 0.77[†] 1.54 ± 0.54 3.50 ± 0.95[†]
MM 0.0097 0.0034 6.28 0.82 2.19 0.28 4.02 1.18
_L_ _±_ _±_ _±_ _±_
_LMSE_ 0.0068 ± 0.0028[†] **4.96 ± 0.64** **0.92 ± 0.11** 3.24 ± 1.08[†]

Student-t 0.0067 ± 0.0029[†] 5.82 ± 0.59 2.26 ± 0.34 3.48 ± 1.17[†]

xVAMP 0.0067 ± 0.0029[†] 5.44 ± 0.64[†] 1.87 ± 0.32 3.23 ± 1.00[†]

xVAMP* 0.0067 ± 0.0029[†] 5.35 ± 0.73[†] 2.00 ± 0.26 3.38 ± 1.15[†]

VBEM 0.0074 ± 0.0026[†] 5.21 ± 0.58[†] 1.29 ± 0.33 3.32 ± 1.06[†]

VBEM* **0.0067 ± 0.0029** 5.17 ± 0.59[†] 1.08 ± 0.17 **3.19 ± 1.02**

kin8m naval power protein
(8192, 8, 1) (11934, 16, 2) (9568, 4, 1) (45730, 9, 1)

_Lβ−NLL(β = 0)_ 0.087 ± 0.004 0.0021 ± 0.0006 4.06 ± 0.18[†] 4.49 ± 0.11
_Lβ−NLL(β = 0.25)_ 0.083 ± 0.003 0.0012 ± 0.0004 4.04 ± 0.18[†] 4.35 ± 0.05[†]

_Lβ−NLL(β = 0.5)_ 0.082 ± 0.003[†] 0.0006 ± 0.0002 4.04 ± 0.17[†] 4.31 ± 0.02[†]

_Lβ−NLL(β = 0.75)_ 0.081 ± 0.004[†] 0.0004 ± 0.0001[†] 4.04 ± 0.15[†] 4.28 ± 0.02[†]

_β_ NLL(β = 1.0) 0.081 0.003[†] **0.0004** **0.0000** 4.06 0.18[†] 4.31 0.05[†]
_L_ _−_ _±_ _±_ _±_ _±_

MM 0.082 0.003[†] 0.0005 0.0001 4.07 0.16[†] 4.32 0.07[†]
_L_ _±_ _±_ _±_ _±_

_LMSE_ **0.081 ± 0.003** 0.0004 ± 0.0001[†] **4.01 ± 0.19** **4.28 ± 0.07**
Student-t 0.085 ± 0.005 0.0026 ± 0.0009 4.02 ± 0.16[†] 4.76 ± 0.24
xVAMP 0.081 ± 0.003[†] 0.0023 ± 0.0004 4.03 ± 0.17[†] 4.38 ± 0.05[†]

xVAMP* 0.082 ± 0.003[†] 0.0020 ± 0.0006 4.03 ± 0.18[†] 4.31 ± 0.02[†]

VBEM 0.082 ± 0.003[†] 0.0009 ± 0.0004 4.09 ± 0.15[†] 4.31 ± 0.01[†]

VBEM* 0.082 ± 0.004[†] 0.0015 ± 0.0005 4.02 ± 0.18[†] 4.35 ± 0.09[†]

superconductivity wine-red wine-white yacht
(21263, 81, 1) (1599, 11, 1) (4898, 11, 1) (308, 6, 1)

_Lβ−NLL(β = 0)_ 13.87 ± 0.50 0.636 ± 0.038[†] 0.691 ± 0.032[†] 1.22 ± 0.47
_Lβ−NLL(β = 0.25)_ 13.50 ± 0.49 0.638 ± 0.036[†] 0.687 ± 0.039[†] 1.73 ± 1.00
_Lβ−NLL(β = 0.5)_ 13.02 ± 0.47 0.635 ± 0.037[†] 0.685 ± 0.035[†] 2.35 ± 1.44
_Lβ−NLL(β = 0.75)_ 13.20 ± 0.46 0.638 ± 0.035[†] 0.689 ± 0.034[†] 1.97 ± 1.03
_Lβ−NLL(β = 1.0)_ 13.42 ± 0.41 0.639 ± 0.035[†] 0.684 ± 0.031[†] 2.08 ± 1.13
MM 13.68 0.79 0.652 0.044[†] 0.692 0.032[†] 3.02 1.38
_L_ _±_ _±_ _±_ _±_
_LMSE_ **12.48 ± 0.40** 0.633 ± 0.036[†] **0.684 ± 0.038** 0.78 ± 0.25[†]

Student-t 13.52 ± 0.60 0.636 ± 0.038[†] 0.688 ± 0.036[†] 1.34 ± 0.63
xVAMP 13.33 ± 0.52 0.635 ± 0.035[†] 0.691 ± 0.032[†] 0.99 ± 0.43
xVAMP* 13.42 ± 0.59 0.633 ± 0.035[†] 0.685 ± 0.032[†] 1.13 ± 0.66
VBEM 12.72 ± 0.57[†] 0.639 ± 0.041[†] 0.685 ± 0.035[†] 1.66 ± 0.84
VBEM* 13.15 ± 0.43 **0.633 ± 0.040** 0.686 ± 0.036[†] **0.65 ± 0.20**


-----

shows qualitative examples. Our β−NLL loss with β > 0 allows to learn good reconstructions
and meaningful uncertainties. Moreover, images produced by sampling latents from the prior are
semantically meaningful, indicating that adding our loss function does not break the disentangling
properties of VAEs. Compare that to Student-t, xVAMP(*), and VBEM(*), which do not produce
similarly clear images when sampling from the prior.

Table S3: Results for generative modeling with variational autoencoders on MNIST and FashionMNIST. We report RMSE and posterior predictive log-likelihood (LL) with standard deviation over 5
independent trials.

**MNIST** **Fashion-MNIST**

Loss RMSE ↓ LL ↑ RMSE ↓ LL ↑

_LNLL (σ[2]_ = 1) 0.153 ± 0.002 -730 ± 0 0.143 ± 0.002 -729 ± 0
_Lβ−NLL(β = 0)_ 0.237 ± 0.002 2116 ± 55 0.170 ± 0.001 1940 ± 104
_Lβ−NLL(β = 0.25)_ 0.181 ± 0.004 2511 ± 88 0.140 ± 0.002 2010 ± 39
_Lβ−NLL(β = 0.5)_ 0.151 ± 0.003 2220 ± 25 0.125 ± 0.003 1639 ± 52
_Lβ−NLL(β = 0.75)_ 0.142 ± 0.001 1954 ± 50 0.131 ± 0.001 1331 ± 32
_Lβ−NLL(β = 1.0)_ 0.152 ± 0.001 1706 ± 30 0.138 ± 0.002 1142 ± 26
_LMM_ 0.260 ± 0.000 385 ± 11 0.295 ± 0.000 -151 ± 4
Student-t 0.273 ± 0.002 4291 ± 103 0.182 ± 0.002 2857 ± 9
xVAMP 0.225 ± 0.002 2989 ± 268 0.161 ± 0.001 2158 ± 100
xVAMP* 0.225 ± 0.001 3062 ± 215 0.160 ± 0.002 2150 ± 131
VBEM 0.114 ± 0.001 719 ± 9 0.108 ± 0.000 660 ± 2
VBEM* 0.176 ± 0.008 3213 ± 238 0.150 ± 0.003 2244 ± 78

B.6 DEPTH REGRESSION

We evaluate β−NLL on the task of depth regression on the NYUv2 dataset (Silberman et al., 2012).
For this purpose, we use a state-of-the-art method for depth regression, AdaBins (Bhat et al., 2021),
and train it with different loss functions. Note that we remove the Mini-ViT Transformer module
from the model, thus our results are not directly comparable with those reported by Bhat et al. (2021).

Table S4 presents quantitative results. β−NLL with β > 0 achieves better RMSE than the NLL
loss. β−NLL with β = 0.5 again provides a good trade-off, achieving similar RMSE as LMSE and
performing better on some of the other metrics. Figure S6 shows qualitative examples. Compared to
NLL, the depth maps predicted by β NLL with β > 0 are noticeably sharper.
_L_ _−_

Table S4: Results for depth regression on the NYUv2 dataset (Silberman et al., 2012). We adapt a stateof-the-art network for depth regression from AdaBins (Bhat et al., 2021) and train it with different
loss functions. In contrast to AdaBins, our network does not include the Mini-ViT Transformer
module and thus our results are not directly comparable with those originally reported. For reference,
we also report numbers from other recent literature on this task. Notably, the Gaussian NLL in all
variants clearly outperforms the Scale Invariant (SI) loss, despite the latter being a loss function
specifically designed for the task of depth regression. We refer the reader to Bhat et al. (2021) for a
description of the metrics.

Method _δ1_ _δ2_ _δ3_ REL RMSE log10 LL
_↑_ _↑_ _↑_ _↓_ _↓_ _↓_ _↑_

_Lβ−NLL(β = 0)_ 0.8855 0.9796 0.9959 0.1094 0.3854 0.0462 -4.52
_Lβ−NLL(β = 0.25)_ 0.8887 0.9812 0.9956 0.1081 0.3818 0.0458 -8.15
_Lβ−NLL(β = 0.5)_ 0.8885 0.9813 0.9956 0.1093 0.3789 0.0458 -7.50
_Lβ−NLL(β = 0.75)_ 0.8902 0.9804 0.9952 0.1095 0.3800 0.0462 -7.35
_Lβ−NLL(β = 1.0)_ 0.8872 0.9813 0.9958 0.1088 0.3845 0.0467 -5.10
_LMSE_ 0.8890 0.9806 0.9960 0.1086 0.3776 0.0461 —
_L1_ 0.8877 0.9798 0.9955 0.1073 0.3850 0.0459 —

SI Loss (Bhat et al., 2021) 0.881 0.980 0.996 0.111 0.419 — —
AdaBins (Bhat et al., 2021) 0.903 0.984 0.997 0.103 0.364 0.044 —
BTS (Lee et al., 2019) 0.885 0.978 0.994 0.110 0.392 0.047 —
DAV (Chen et al., 2019) 0.882 0.980 0.996 0.108 0.412 — —


-----

**MNIST** **Fashion-MNIST**


dataset

mean
std
_∼posterior_
_∼N_ (0, 1)

mean
std
_∼posterior_
_∼N_ (0, 1)

mean
std
_∼posterior_
_∼N_ (0, 1)

mean
std
_∼posterior_
_∼N_ (0, 1)

mean
std
_∼posterior_
_∼N_ (0, 1)

mean
std
_∼posterior_
_∼N_ (0, 1)

mean
std
_∼posterior_
_∼N_ (0, 1)

mean
std
_∼posterior_
_∼N_ (0, 1)

mean
std
_∼posterior_
_∼N_ (0, 1)

mean
std
_∼posterior_
_∼N_ (0, 1)

mean
std
_∼posterior_
_∼N_ (0, 1)

mean
std
_∼posterior_
_∼N_ (0, 1)


_Lβ−NLL(β = 0)_

_Lβ−NLL(β = 0.25)_

_Lβ−NLL(β = 0.5)_

_Lβ−NLL(β = 0.75)_

_Lβ−NLL(β = 1.0)_

_LNLL (σ[2]_ = 1)

MM
_L_

Student-t

xVAMP

xVAMP*

VBEM

VBEM*


Figure S5: Generative modeling with variational autoencoders on MNIST and Fashion-MNIST. The
overall first row shows inputs from the test set. For each method, we present posterior predictive
means (i.e. reconstructions), the posterior predictive standard deviations, samples from the posterior
predictive distribution, and finally samples using the prior (0, 1). NLL(σ[2] = 1) refers to Gaussian
_N_ _L_
log-likelihood with a fixed variance of 1. Values are clipped to the interval [0, 1]. Examples are not
cherry-picked.


-----

Input

Ground
Truth

pred.

_Lβ−NLL_
(β = 0.0)

std.

pred.

_Lβ−NLL_
(β = 0.5)

std.

pred.

_Lβ−NLL_
(β = 0.75)

std.

pred.

_Lβ−NLL_
(β = 1.0)

std.

MSE pred.
_L_

Figure S6: Example results for depth regression on the NYUv2 dataset (Silberman et al., 2012).
First two rows show input image and ground truth depth map, where black values in the depth map
represent missing values. For each method, we present the predicted depth map (pred.), and the
aleatoric uncertainty in form of the predicted standard deviation (std.). Results for _β_ NLL with
_L_ _−_
_β > 0 are noticeably sharper. The last column shows a magnified view on the previous picture._


-----

C DATASETS AND TRAINING SETTINGS

**Sinusoidal without heteroscedastic noise** This dataset is created by taking 1 000 uniformly spaced
points on the interval [0, 12] as inputs x and applying the function y(x) = 0.4 sin(2πx) + ξ to them
to create the targets y, where ξ is Gaussian noise with a standard deviation of 0.01.

**Sinusoidal with heteroscedastic noise** We use the synthetic data as introduced in Detlefsen et al.

(2019). From the functional form y = x sin(x) + xξ1 + ξ2, with Gaussian noise with standard
deviation σ = 0.3 for ξ1 and ξ2, we sample 500 points uniformly spaced in the interval [0, 10]. The
model is an MLP with one hidden layer of 50 units and tanh activations (as used in Detlefsen et al.
(2019) and Stirn & Knowles (2020)).

**UCI Datasets** We use the UCI datasets suite commonly used to benchmark uncertainty estimation,
stemming from the UCI Machine Learning Repository.[3] In particular, we use the training-test protocol
from (Hernandez-Lobato & Adams´, 2015; Gal & Ghahramani, 2016), and their data splits[4], except
for “carbon”, “energy”, “naval”, “superconductivity”, and “wine-white”, where we generate our own
random splits.

Inputs and targets are whitened on the training set. Metrics are reported in the original scale of
the data. Each dataset is divided into 20 randomly sampled train-test splits (80%-20%). For each
split, we further randomly divide the training set into 80% training data and 20% validation data and
search for an optimal learning rate from the set {10[−][4], 3 · 10[−][4], 7 · 10[−][4], 10[−][3], 3 · 10[−][3], 7 · 10[−][3]}
by monitoring log-likelihood on the validation set. We train for a maximum of 20 000 updates, except
for the larger “kin8m”, “power plant”, “protein”, and “naval” datasets where we train for a maximum
of 100 000 updates. We perform early-stopping with a patience of 50 epochs, retrain the model
with the best found learning rate on the full training set, and then evaluate on the test split. The
reported performance and standard deviations are taken as averages over all test splits. Note that
the performance we report is not comparable with other publications, as performance is known to
differ strongly over different data splits. Some other works also perform early-stopping on the test
set, which distorts the results.

We use a single-layer relu hidden network with 50 neurons, except for “protein” where we use 100
neurons. The batch size is 256.

Following Stirn & Knowles (2020), for each method, we report the number of datasets for which the
method is statistically indistinguishable from the respective best method in Table 1 (Ties). For this
purpose, we performed a two-sided Kolmogorov-Smirnov test with a p ≤ 0.05 significance level.

**ObjectSlide** This environment consists of an agent whose task is to slide an object to a target
location (Seitzer et al., 2021). The continuous state space consists of 4 dimensions: agent and object
positions and velocities, and the continuous action space is a one-dimensional movement command.
The forward prediction task consists of predicting the change in object position in the next state from
the current state and action. The dataset we use consists of 180 000 transitions collected using a
random policy, which we split into training, validation, and testing sets with 60 000 transitions each.
Inputs and targets are whitened on the training set. Metrics are reported in the original scale of the
data. We train for a maximum of 5 000 epochs with a batch size of 256, and evaluate the model with
the best validation log-likelihood on the test set afterwards.

**Fetch-PickAndPlace** We use the Fetch-PickAndPlace environment (Plappert et al., 2018) from
OpenAI Gym (Brockman et al., 2016) as a challenging real-world scenario. The task of the agent is
to use a position-controlled 7 DoF robotic arm to lift an object to a target location in space. The state
space is 25-dimensional and the action space is 4-dimensional. As in ObjectSlide, the prediction task
is to predict the 3-dimensional change in object position from the current state and action. We use
840 000 transitions collected using the APEX method (Pinneri et al., 2021) as our dataset, which we
split into 70% training, 15% validation, and 15% testing data. Inputs and targets are whitened on the
training set. Metrics are reported in the original scale of the data. We train for a maximum of 500

[3https://archive.ics.uci.edu](https://archive.ics.uci.edu)
[4available under https://github.com/yaringal/DropoutUncertaintyExps](https://github.com/yaringal/DropoutUncertaintyExps)


-----

epochs with a batch size of 256, and evaluate the model with the best validation log-likelihood on the
test set afterwards.

**NYUv2 Depth Regression** We use the dataset in the variant provided by Lee et al. (2019).[5]

Training settings and evaluation protocol were taken from Bhat et al. (2021). We train for 25 epochs
using a batch size of 16 and validate the model every 100 updates. We use the model with the best
“REL” metric on the validation set for testing.

D HYPERPARAMETER SETTINGS AND IMPLEMENTATION DETAILS

For all experiments, we used the Adam optimizer (Kingma & Ba, 2015) with standard settings
_β1 = 0.9, β2 = 0.999. We parameterize the Gaussian distribution using two linear layers on top_
of shared features produced by an MLP. The variance ˆσ[2](x) is constrained to the positive region
using the softplus(x) = log(1 + exp(x)) activation function. We additionally add a small constant
of 10[−][8] to prevent the variance from collapsing to zero and clamp the maximum variance to 1 000.

Some baselines use a Student’s t-distribution (Student’s t, xVAMP(*), VBEM(*)) as their predictive
distribution. This distribution results from integrating out the unknown variance of a Gaussian with a
learned Gamma prior on the inverse variance (Detlefsen et al., 2019; Stirn & Knowles, 2020). We
parametrize the Gamma distribution in terms of data-dependent alpha and beta parameters, i.e. α(x)
and β(x), which are computed using linear layers on top of the shared features. In this case, the
MLP has three outputs: mean, alpha, and beta. Both alpha and beta are constrained to the positive
region using the softplus activation. We add a positive constant of 1.001 for alpha and 10[−][8] _· 0.001_
for beta. Alpha is clamped to a maximum value of 1 000 and beta to 10[−][8] _· 999. These values_
were chosen such that the resulting variance matches the range (10[−][8], 1000] while ensuring that the
degrees-of-freedom parameter ν of the Student’s t is always greater than 2.

For xVAMP and VBEM, the MLP outputs a fourth term, π(x), representing the logits of a categorical
distribution that specifies the mixture weights of the prior. We initialize the prior parameters exactly
the same as Stirn & Knowles (2020). For these methods, the objective function additionally contains
a KL divergence between a Gamma distribution and a mixture-of-Gamma distributions. Following
Stirn & Knowles (2020), we approximate this KL divergence using 20 Monte-Carlo samples.

D.1 SINUSOIDAL REGRESSION PROBLEM

The sinusoidal fit in Fig. 1 results from a network of two hidden layers with 128 neurons per layer
and tanh activations, optimized with a learning rate of 5 · 10[−][4] and a batch size of 100. For the
experiment in Sec. 5.1, we scan over learning rates and architectures with different hidden layers and
units per layer, as detailed in Table S5.

Table S5: Architectures used for the sinusoidal regression task. Fully-connected feed-forward neural
networks with tanh activations.

Architecture # 0 1 2 3 4

# Hidden Layers 2 2 2 3 3
# Units per Layer 32 64 128 128 256

D.2 OBJECTSLIDE AND FETCH-PICKANDPLACE

For each tested loss function, we performed a grid search on the ObjectSlide and Fetch-PickAndPlace
datasets. We report the parameters we scanned over in Table S6. Table S7 reports the model
configurations with the best validation log-likelihood on the grid search. For the results in Table 2,
we retrained the best model configuration with five different random seeds and evaluated them on the
hold-out test set.

[5available under https://github.com/cogaplex-bts/bts](https://github.com/cogaplex-bts/bts)


-----

Table S6: Hyperparameter settings for our grid search on the ObjectSlide and Fetch-PickAndPlace
datasets. We run 96 configurations per loss function.

Hyperparameter Set of Values

Learning Rate _{3 · 10[−][5], 10[−][4], 3 · 10[−][4], ·10[−][3]}_
# Hidden Layers _{2, 3, 4}_
# Units per Layer _{128, 256, 386, 512}_
Activation _{tanh, relu}_

Table S7: Best hyperparameters found by grid search on ObjectSlide and Fetch-PickAndPlace
datasets, measured by best log-likelihood on the validation set.


(a) ObjectSlide

Method _β_ LR Layers Act.

MSE 10[−][3] 3 128 relu
_L_ _×_
NLL 10[−][3] 3 128 relu
_L_ _×_
_Lβ−NLL_ 0.25 10[−][3] 3 × 128 relu
_Lβ−NLL_ 0.5 10[−][3] 3 × 128 relu
_Lβ−NLL_ 0.75 10[−][3] 3 × 128 relu
_Lβ−NLL_ 1.0 10[−][3] 3 × 128 relu
MM 10[−][3] 3 128 relu
_L_ _×_
Student-t 10[−][3] 2 × 386 relu
xVAMP 10[−][4] 4 × 128 relu
xVAMP* 10[−][4] 3 × 256 relu
VBEM 3 · 10[−][4] 2 × 256 tanh
VBEM* 10[−][3] 2 × 386 relu

D.3 VARIATIONAL AUTOENCODERS


(b) Fetch-PickAndPlace

Method _β_ LR Layers Act.

MSE 10[−][3] 4 128 relu
_L_ _×_
NLL 3 10[−][4] 4 128 relu
_L_ _·_ _×_
_Lβ−NLL_ 0.25 3 · 10[−][4] 4 × 128 relu
_Lβ−NLL_ 0.5 3 · 10[−][4] 4 × 128 relu
_Lβ−NLL_ 0.75 10[−][3] 4 × 128 relu
_Lβ−NLL_ 1.0 10[−][3] 4 × 128 relu
MM 10[−][3] 4 128 relu
_L_ _×_
Student-t 3 · 10[−][4] 3 × 256 relu
xVAMP 10[−][4] 3 × 386 relu
xVAMP* 10[−][4] 3 × 386 relu
VBEM 10[−][3] 3 × 386 relu
VBEM* 10[−][4] 3 × 386 relu


We largely follow Stirn & Knowles (2020) for their training settings for the VAE experiment. In
particular, we use an encoder with three layers of 512, 256, 128 neurons and a decoder with three
layers of 128, 256, 512 neurons, all with relu activations. The latent space is 10-dimensional for
MNIST and 25-dimensional for FashionMNIST. We train the VAEs for a maximum of 1 000 epochs,
using Adam with a learning rate of 0.0003 and a batch size of 256. Early-stopping with a patience of
50 epochs is performed on the log-likelihood of the validation set. The validation set consists of 20%
of the MNIST/FashionMNIST training set.

D.4 DEPTH REGRESSION

We use the official implementation of AdaBins (Bhat et al., 2021),[6] thereby reproducing their exact
training settings and evaluation protocol. We remove the AdaBins/mini-ViT Transformer from the
model. Instead, the feature map output by the U-Net is reduced to two channels using a 1 × 1
convolution, where we use the first channel as the mean predictor and the second channel as the
variance predictor. In this setting, both mean and variance are constrained to positive numbers by
a softplus activation. On top of that, we add a positive offset to ensure a minimum output value of
10[−][3] for the mean (the minimum possible depth value) and 10[−][6] for the variance and clamp both
mean and variance to a maximum value of 10.

[6https://github.com/shariqfarooq123/AdaBins](https://github.com/shariqfarooq123/AdaBins)


-----

D.5 IMPLEMENTATION OF BETA-NLL IN PYTORCH

1 def beta_nll_loss(mean, variance, target, beta):

2 """Compute beta-NLL loss

3

4 :param mean: Predicted mean of shape B x D

5 :param variance: Predicted variance of shape B x D

6 :param target: Target of shape B x D

7 :param beta: Parameter from range [0, 1] controlling relative

8 weighting between data points, where ‘0‘ corresponds to

9 high weight on low error points and ‘1‘ to an equal weighting.

10 :returns: Loss per batch element of shape B

11 """

12 loss = 0.5 * ((target - mean) ** 2 / variance + variance.log())

13

14 if beta > 0:

15 loss = loss * variance.detach() ** beta

16

17 return loss.sum(axis=-1)


-----

