# ADVERSARIALLY ROBUST MODELS MAY NOT TRANS## FER BETTER: SUFFICIENT CONDITIONS FOR DOMAIN
# TRANSFERABILITY FROM THE VIEW OF REGULARIZA## TION

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Machine learning (ML) robustness and generalization are fundamentally correlated: they essentially concern about data distribution shift under adversarial and
natural settings, respectively. Thus, it is critical to uncover their underlying connections to tackle one based on the other. On one hand, recent studies show that
more robust (adversarially trained) models are more generalizable to other domains. On the other hand, there lacks of theoretical understanding of such phenomenon, and it is not clear whether there are counterexamples. In this paper, we
aim to provide sufficient conditions for this phenomenon considering different factors that could affect both, such as norm of the last layer, Jacobian norm, and data
augmentations (DA). In particular, we propose a general theoretical framework
indicating factors that can be reformed as a function class regularization process,
which could lead to improvements of domain generalization. Our analysis, for the
first time, shows that “robustness” is actually not the causation for domain generalization; rather, robustness induced by adversarial training is a by-product of such
function class regularization. We then discuss in details about different properties
of DA and we prove that under certain conditions, DA can be viewed as regularization and therefore improve generalization. We conduct extensive experiments
to verify our theoretical findings and show several counterexamples where robustness and generalization are negatively correlated when the sufficient conditions
are not satisfied.

1 INTRODUCTION

Domain generalization (or transferability) is the task of training machine learning models with
data from one or more source domains that can be adapted to a target domain, often via lowcost fine-tuning. Thus, domain generalization refers to approaches designed to address the
natural data distribution shift problem (Muandet et al., 2013; Rosenfeld et al., 2021). A wide array of
approaches have been proposed to address domain transferability, including fine-tuning the last layer
of DNNs (Huang et al., 2018), invariant feature optimization (Muandet et al., 2013), efficient model
selection for fine-tuning (You et al., 2019), and optimal transport based domain adaptation (Courty
et al., 2016). Improving domain generalization has emerged as an important task in the machine
learning community: for instance, it is among the key technologies to enable an autonomous driving
vehicle trained in city scenarios to make correct decisions in the countryside as well.

On the other hand, robust machine learning aims to tackle the problem of
adversarial data distribution shift. Both empirical and certified robust learning approaches
have been proposed, such as empirical adversarial training (Madry et al., 2018) and certified
defenses based on deterministic and probabilistic approaches (Cohen et al., 2019).

As domain transferability and robust machine learning tackle different kinds of data distribution
shifts, this work seeks to uncover their underlying connections and tradeoffs. For instance, recent
studies suggest that adversarially robust models are more domain transferable (Salman et al., 2020),
which, in turn, provides new insights on improving domain generalization. However, a theoretical
analysis of their relationship is still lacking, and it is unclear whether such positive correlations al

-----

Figure 1: Illustration of robustness and domain transferability in different conditions.

**_Sufficient conditions for domain transferability_**

**_Adversarially robust models may_**

**_(a) Data augmentations_** **_(b) Jacobian norm_** **_(c) Last-Layer norm_** _…_ **_not transfer better_**

**_(a)_** **_(b)_** **_(c)_**

**Source**

**Domain** **Feature Extractor** **Last Layer**

ways hold. In this paper, we take the first steps towards formally analyzing the relationship between
model robustness and domain transferability to answer the following questions: What are sufficient
_conditions for domain transferability? Is model robustness the cause of domain transferability? Can_
_robustness and domain transferability be negatively correlated?_

To answer the above questions and uncover the underlying relationship between robustness and
domain transferability, we propose a general theoretical framework that characterizes sufficient conditions for domain transferability from the view of function class regularization. Our analysis shows
that if the function class of feature extractors is more regularized, the model based on a feature
extractor trained from the function class, composed with a fine-tuned last layer, can be more transferable. Formally, we prove that there is a monotone relation between the regularization strength
and a tight upper bound on the relative domain transfer loss.

Under the proposed framework, we analyze several common factors for model training, including
the Jacobian norm, the last layer norm, data augmentation, and adversarial training as shown in
Fig. 1. In particular, controlling the Jacobian norm and last layer norm can be viewed as functionclass regularization, thus can be analyzed in our framework. We also analyze how other common
regularization operations can be mapped to function class regularization. For instance, we consider
noise-dependent and independent data augmentation procedures based on feature average and loss
average aggregation algorithms.

We conduct extensive experiments on ImageNet (CIFAR-10 as target domain) and CIFAR-10
(SVHN as target domain) based on different models to verify our analysis. We show that regularization can control domain transferability, and robustness and domain transferability can be negatively
correlated, which are counter-examples against Salman et al. (2020). Taken together, this indicates
that robustness is not a cause of transferability.

**Technical Contributions. We aim to uncover the underlying relationship between robustness and**
domain transferability and lay out the sufficient conditions for transferability from the view of regularization. We make both theoretical and empirical contributions.

-  We propose a theoretical framework to analyze the sufficient conditions for domain transferability
from the view of function class regularization. We provably show that stronger regularization
on the feature extractor implies a decreased tight upper bound on the relative transferability loss;
while model robustness could be arbitrary.

-  We prove the tightness of our transferability upper bound, and provide the generalization bound
of the relative transferability loss from the view of regularization.

-  We analyze several factors such as different data augmentations (e.g., rotation and Gaussian) under
the framework, and show how they can be mapped to function class regularization and therefore
affect transferability.

-  We conduct extensive experiments on different datasets and model architectures to verify our theoretical claims. We also show several counterexamples that indicate significant negative correlation
between robustness and the relative domain transferability.

2 RELATED WORK

**Domain Transferability has been analyzed in different settings. Muandet et al. present a gener-**
alization bound for classification task based on the properties of the assumed prior over training
environments. Rosenfeld et al. model domain transferability/generalization as an online game and
show that generalizing beyond the convex hull of training environments is NP-hard, and Zhang et al.
provides a generalization bound for distributions with sufficiently small H-divergence. Given the


-----

complexity of domain transferability analysis, recent empirical studies show that adversarially robust models transfer better (Salman et al., 2020). In this paper, we aim to relax the assumptions and
focus on understanding the domain transferability from the view of regularization and theoretically
show whether “robustness” is indeed a causation for transferability or not.

**Model Robustness is an important topic given recent diverse adversarial attacks (Goodfellow et al.,**
2014; Carlini & Wagner, 2017). These attacks may be launched without access to model parameters (Tu et al., 2019) or even with the model predicted label alone (Chen et al., 2020). Different
approaches have been proposed to improve model robustness against adversarial attack. Adversarial training has been shown to be effective empirically (Madry et al., 2018; Zhang et al., 2019a;
Miyato et al., 2018). Some studies have shown that robustness is property related to other model
characteristics, such as transferability and invertibility (Engstrom et al., 2019).

3 SUFFICIENT CONDITIONS FOR DOMAIN TRANSFERABILITY

In this section, we theoretically analyze the problem of domain transferability from the view of
regularization and discuss some sufficient conditions for good transferability. All of the proofs are
provided in Section A in the appendix.

**Notations. We denote the input space as X** ; the feature space as Z and the output space as Y. Let
the fine-tuning function class be g ∈G. Given a feature extractor f : X →Z and a fine-tuning
function g :, the full model is g _f :_ . We denote as the set of distributions
_Z →Y_ _◦_ _X →Y_ _PX×Y_
on X × Y. The loss function is denoted by ℓ : Y × Y → R+, and the population loss based on data
distribution and a model g _f is defined as_
_D ∈PX×Y_ _◦_

_ℓ_ (g _f_ ) := E(x,y) [ℓ(g _f_ (x), y)].
_D_ _◦_ _∼D_ _◦_

Before diving into the details, we first provide the following example to illustrate why one might
investigate domain transferability from the view of regularization.

3.1 EXAMPLE: ROBUSTNESS AND TRANSFERABILITY ARE INDEPENDENT

In this subsection, we construct a simple example where domain transferability depends on regularization, yet domain transferability and robustness are independent. Moreover, this example serves
as motivation to consider domain transferability from the view of regularization.

Given the source and target distributions _S,_ _T_ P, we denote their marginal distributions
on the input space as _S_ [and][ D]T[X] [, respectively. We consider the case that] D _D_ _∈_ _X×Y_ _[ X ⊂]_ [R][m][ being a]
_X_ _D[X]_
low-dimensional manifold in R[m], and Y = R[d]. Given an input x ∈X, the ground truth target for
the source domain is yS(x) generated by a function yS : R[m] _→_ R[d]. Similarly, we define yT for
the target domain. In this example, for simplicity, we neglect the fine-tuning process but directly
consider learning a function f : R[m] _→_ R[d] with a norm ∥· ∥ on R[d]. For the source domain we have
the population loss:

_ℓDS_ (f ) = Ex∼DSX [[][∥][f] [(][x][)][ −] _[y][S][(][x][)][∥][]][.]_

A distribution P on the input space, defines a norm of a function f : R[m] R[d] as
_D ∈_ _X_ _X_ _→_

_f_ := Ex [ _f_ (x) ],
_∥_ _∥D_ _∼D_ _∥_ _∥_

where we view two functions f1, f2 as the same if _f1_ _f2_ = 0. Therefore, we can define the
source domain loss ℓDS (f ) = ∥f − _yS∥DSX_ [and the target domain loss] ∥ _−_ _∥D_ _[ ℓ][D][T][ (][f]_ [) =][ ∥][f][ −] _[y][T][ ∥][D]T[X]_ [.]

For the sake of illustration, we consider the simple case where the input distributions DS[X] _[,][ D]T[X]_ [are]
the same, and hence we denote D = DS[X] [=][ D]T[X] [. Note that][ y][S][ and][ y][T][ are different.]

Denoting a function space = _f : R[m]_ R[d] _f_ _<_, we assume that yS, yT and we
can compare f, yS, yT in the same space. Therefore, given F _{_ _→_ _| ∥_ _∥D_ _∞} c > 0 as a regularization parameter, the ∈F_
domain transferability problem can be defined as:


Learning a source model: _fc[D][S]_ _∈_ arg minf _ℓDS_ (f ), s.t. ∥f _∥D ≤_ _c;_ (1)
_∈F_

Testing on a target domain: _ℓDT (fc[D][S]_ ),


-----

where the minimizer fc[D][S] := yS min{1, _∥ySc∥D_

_[}][, the source domain loss is][ ℓ][D][S]_ [(][f] [) =][ ∥][f][ −] _[y][S][∥][D][,]_
a minimizer of (1) and provide an intuitive illustration in Figure 2.and the target domain loss is ℓDT (f ) = ∥f − _yT ∥D. We prove in Proposition 3.1 that fc[D][S]_ is indeed

We show that the robustness can be independent to domain transferability as follows. Consider the
adversarial robustness of fc[D][S] on an input x ∈X (e.g., maxδ:∥δ∥2≤ϵ ℓ(fc[D][S] (x + δ), yS(x))). Since
the transferred loss ℓDT (fc[D][S] ) only evaluates fc[D][S] on X which is a low-dimensional manifold in
R[m], an adversarial perturbation δ ∈ R[m] could make x + δ ∈ R[m]\X when the loss function is
sufficiently big outside the manifold X . Therefore, the robustness could be arbitrarily bad without
changing the value of ℓDT (fc[D][S] ), i.e., the performance of the source model on the target domain.

As we can see, the robustness is independent to domain transferability in this example. On the

|Col1|Source Loss = ℓ𝒟𝑆(𝑓 𝑐𝒟𝑆) Transferred Loss = ℓ𝒟𝑇(𝑓 𝑐𝒟𝑆) Relative Domain Transferability Loss = ℓ𝒟𝑇(𝑓 𝑐𝒟𝑆) −ℓ𝒟𝑆(𝑓 𝑐𝒟𝑆)|
|---|---|
|||


contrary, if we change the perspective to consider the the regularization parameter c, we have the
following interesting finding. An illustration of the finding is shown in Figure 2, and a more formal
statement is provided in Proposition 3.1.

ℓ𝑦𝒟𝑇𝑇(𝑓𝑐𝒟𝑆) 𝑐 𝑓𝑐𝒟𝑦𝑆𝑆ℓ𝒟𝑆(𝑓𝑐𝒟𝑆) Transferred Loss = Source Loss = Relative Domain ℓ𝒟𝑆(𝑓ℓ𝑐𝒟𝒟𝑇𝑆(𝑓) 𝑐𝒟𝑆)

0 Transferability Loss = ℓ𝒟𝑇(𝑓𝑐𝒟𝑆) − ℓ𝒟𝑆(𝑓𝑐𝒟𝑆)

Different 𝑐

||𝑓||𝒟 ≤𝑐

ℱ

Source Training Function Class Size 𝑐

The left figure illustrates the example in the function space F given a regularization parameter
The right figure shows the relations between domain transferability and the c. In this example, the weaker the
regularization effect (greater c) is, the greater the relative domain transferability loss (violet arrow becomes).


**Proposition 3.1. Given the problem defined above, fc[D][S]** _is a minimizer of equation 1. If c ≥_ _c[′]_ _≥_ 0,
_then the relative domain transferability loss ℓDT (fc[D][S]_ ) − _ℓDS_ (fc[D][S] ) ≥ _ℓDT (fc[D][′][ )][S]_ _[ −]_ _[ℓ][D]S_ [(][f][ D]c[′][ )][S] _[.]_

We can see that robustness is not sufficient to characterize domain transferability. However, there is
a monotone relation between the regularization strength and the relative transferability loss, where
adversarial robustness could be arbitrary. Similar behavior is also observed in our experiments, as
we will discuss in Section 4. Naturally, these findings motivate the study of the connections between
the regularization of the training process and domain transferability in general, as we consider next.

3.2 UPPER BOUND OF THE RELATIVE DOMAIN TRANSFERABILITY

In this subsection, we consider the general transferability problem with fine-tuning. We prove that
there is a monotone relationship between the regularization strength and relative domain transferability loss. We also present a tight upper bound on the relative domain transferability loss. Denote
the training algorithm aschosen from a function class A that takes a data distributionA and a fine-tuning function D and outputs a feature extractor gA[D] _fA[D]_ _[∈F][A]_
relative domain transferability. F _[∈G][. Next we formally define]_
**Definition 1 (Relative Domain Transferability Loss). Given the training algorithm A and a pair of**
_distributions_ _S,_ _T_ _, the relative domain transferability loss between_ _S,_ _T is defined_
_to be the difference of fine-tuned losses, i.e., D_ _D_ _∈PX×Y_ _D_ _D_

_τ_ (A; DS, DT ) := infg _[ℓ][D][T][ (][g][ ◦]_ _[f][ D]A_ _[S]_ [)][ −] _[ℓ][D]S_ [(][g]A[D][S] _◦_ _fA[D][S]_ [)][.]
_∈G_

Note that the training algorithm A is not required to be optimal, i.e., it could be the case that
_ℓDS_ (gA[D][S] _◦_ _fA[D][S]_ [)][ >][ inf] _[g][∈G][,f]_ _[∈F]A_ _[ℓ][D]S_ [(][g][ ◦] _[f]_ [)][. As we can see, the smaller][ τ] [(][A][;][ D][S][,][ D][T][ )][ is, the]
better the model’s relative performance becomes on the target domain.

Another perspective of Definition 1 is thatFrom this perspective, the transferred loss is the source loss plus an additional term to be upper inf _g∈G ℓDT (g_ _◦fA[D][S]_ [) =][ ℓ][D]S [(][g]A[D][S] _[◦][f][ D]A_ _[S]_ [)+][τ] [(][A][;][ D][S][,][ D][T][ )][.]
bounded by a certain distance metric between the source and target distributions – as is common
in the literature of domain adaptation (e.g., Ben-David et al. (2007); Zhao et al. (2019)). The key
question of the “distance metric” remains unanswered. To this end, we propose the following.


-----

**Definition 2 ((G, F)-pseudometric). Given a fine-tuning function class G, a feature extractor func-**
_tion class_ _and distributions_ _S,_ _T_ _, the (_ _,_ )-pseudometric between _S,_ _T is_
_F_ _D_ _D_ _∈PX×Y_ _G_ _F_ _D_ _D_
_d_ _,_ ( _S,_ _T ) := sup_ inf
_G_ _F_ _D_ _D_ _f_ _∈F_ _|_ _g∈G_ _[ℓ][D][S]_ [(][g][ ◦] _[f]_ [)][ −] _g[inf]∈G_ _[ℓ][D][T][ (][g][ ◦]_ _[f]_ [)][|][.]

_Since the fine-tuning function class is usually simple and fixed, we will use d_ _as an abbreviation in_
_F_
_the context where G is clear._

It can be easily verified that d _,_ is a pseudometric that measures the distance between two distri_G_ _F_
butions, as shown in the following proposition.
**Proposition 3.2. d** _,_ ( _,_ ) : R+ satisfies the following properties; (Symme_try) d_ _,_ ( _S,_ _T ) =G_ _F d·_ _·,_ ( PTX×Y, _S × P); (Triangle Inequality)X×Y →_ : d _,_ ( _S,_ _T )_
_G_ _F_ _D_ _D_ _G_ _F_ _D_ _D_ _∀D[′]_ _∈PX×Y_ _G_ _F_ _D_ _D_ _≤_
_d_ _,_ ( _S,_ ) + d _,_ ( _,_ _T ); (Weak Zero Property)_ : d _,_ ( _,_ ) = 0.
_G_ _F_ _D_ _D[′]_ _G_ _F_ _D[′]_ _D_ _∀D ∈PX×Y_ _G_ _F_ _D_ _D_

In this section, we consider a fixed fine-tuning function class G and feature extractor function class
_FA given by the training algorithm A. Thus, we denote dG,F as dFA for the remainder of the paper._
With the definition of dFA, we can derive the following result.
**Theorem 3.1. Given a training algorithm A, for** _S,_ _T_ _we have_
_∀D_ _D_ _∈PX×Y_
_τ_ (A; _S,_ _T )_ _d_ _A_ ( _S,_ _T ),_
_D_ _D_ _≤_ _F_ _D_ _D_

_or equivalently,_ _ginf_ _[ℓ][D][T][ (][g][ ◦]_ _[f][ D]A_ _[S]_ [)][ ≤] _[ℓ][D]S_ [(][g]A[D][S] _◦_ _fA[D][S]_ [) +][ d][F]A [(][D][S][,][ D][T][ )][.]
_∈G_

**Interpretation: As we can see, the above theorem provides sufficient conditions for good domain**
transferability. There is a monotone relation between the regularization strength and d _A_ ( _S,_ _T ),_
_F_ _D_ _D_
i.e., the upper bound on the relative domain transferability loss τ (A; DS, DT ). More explicitly, if
a training algorithm A[′] has FA′ ⊆FA, then dFA′ (DS, DT ) ≤ _dFA_ (DS, DT ). Moreover, small
_d_ _A_ ( _S,_ _T ) implies good relative domain transferability. From this perspective, we can see that_
_F_ _D_ _D_
we need both small dFA (DS, DT ) and small source loss ℓDS (gA[D][S] _◦_ _fA[D][S]_ [)][ to guarantee good ab-]
solute domain transferability. Note that there is a possible trade-off, i.e., with FA being smaller,
_dFA_ (DS, DT ) decreases but possibly ℓDS (gA[D][S] _◦_ _fA[D][S]_ [)][ increases due to the limited power of][ F][A][.]
On the other hand, there may not be such trade-off if DS and DT are close enough such that
_d_ _A_ ( _S,_ _T ) is small._
_F_ _D_ _D_

To make the upper bound more meaningful, we need to study the tightness of it.
**Theorem 3.2.G includes the zero function, and any training algorithm Given any source distribution DS ∈PX× ARd, denote, any fine-tuning function class G where**

_ϵ := ℓDS_ (gA[D][S] _◦_ _fA[D][S]_ [)][ −] _g_ inf,f _A_ _[ℓ][D][S]_ [(][g][ ◦] _[f]_ [)][.]
_∈G_ _∈F_

_We assume some properties of the loss function ℓ_ : R[d] _× R[d]_ _→_ R+: it is differentiable and strictly
_convex w.r.t. its first argument; ℓ(y, y) = 0 for any y_ R[d]; and limr inf _y:_ _y_ 2=r ℓ([⃗]0, y) = _,_
_∈_ _→∞_ _∥_ _∥_ _∞_
_whereDT ∈P[⃗]0 is the zero vector. Then, for any distributionX×Rd with its marginal on X being D[X]_ _such that D[X]_ _on X_ _, there exist some distributions_
_τ_ (A; _S,_ _T )_ _d_ _A_ ( _S,_ _T )_ _τ_ (A; _S,_ _T ) + ϵ,_
_D_ _D_ _≤_ _F_ _D_ _D_ _≤_ _D_ _D_

_or equivalently d_ _A_ ( _S,_ _T )_ _ϵ_ _τ_ (A; _S,_ _T )_ _d_ _A_ ( _S,_ _T )._
_F_ _D_ _D_ _−_ _≤_ _D_ _D_ _≤_ _F_ _D_ _D_

**Interpretation: In the above theorem, we show that given any A, DS, and the marginal D[X]**, there
exists some conditional distributions of y|x such that by composing it with the given D[X] we have
a distribution DT to make the bound in Theorem 3.1 ϵ-tight. Note that ϵ is the difference between
the source loss and the its infimum, i.e., with a good enough algorithm A, the ϵ could be arbitrarily
small.

3.3 GENERALIZATION UPPER BOUND OF THE RELATIVE DOMAIN TRANSFERABILITY


Here we investigate the proposed theory on relative transferability with finite samples. For a distribution, we denote its empirical distribution with n samples as . That being said,
_D ∈PX×Y_ _D[n]_

_n_

_ℓ_ (x,y) _ℓ(g_ _f_ (x[b]i), yi),
_D[n]_ [(][g][ ◦] _[f]_ [) =][ E] _∼D[b][n]_ [[][ℓ][(][g][ ◦] _[f]_ [(][x][)][, y][)] = 1]n _◦_

_i=1_

b X


-----

where (xi, yi) are i.i.d. samples from D. Therefore, given two distributions DS, DT ∈PX×Y, the
empirical (G, F)-pseudometric between them is dG,F (DS[n][,][ b]DT[n] [)][.]

Note that d _,_ is not only a pseudometric of distributions, but also a complexity measure, and we
_G_ _F_
will first connect it with the Rademacher complexity. [b]
**Definition 3 (Empirical Rademacher Complexity (Bartlett & Mendelson, 2002; Koltchinskii,**
2001)). Denote the loss function class induced by G, F as

_,_ := _hg,f :_ R+ _g_ _, f_ _,_ _where hg,f_ (x, y) := ℓ(g _f_ (x), y).
_LG_ _F_ _{_ _X × Y →_ _|_ _∈G_ _∈F}_ _◦_

_Given an empirical distribution_ _D[n]_ _(i.e., n data samples), the Rademacher complexity of it is_

_n_

Rad [b] sup _ξih(xi, yi)_ _,_
_D[n]_ [(][L][G][,][F] [) := 1]n [E][ξ] "h∈LG,F _i=1_ #
b X

_where ξ ∈_ R[n] _are Rademacher variables, i.e., each ξi is i.i.d. uniformly distributed on {−1, 1}._

We can see that if there is a F _[′]_ _⊆F, then RadD[n]_ [(][L][G][,][F] _[′]_ [)][ ≤] [Rad][ b]D[n] [(][L][G][,][F] [)][. With the above defini-]
tions, we have the following lemma connecting the b (G, F)-pseudometric to Rademacher complexity.
**Lemma 3.1. Assuming the individual loss function ℓ** : Y × Y → [0, c], given any distribution
_and_ _δ > 0, with probability_ 1 _δ we have_
_D ∈PX×Y_ _∀_ _≥_ _−_

ln(4/δ)

_dG,F_ (D, _D[n]) ≤_ 2RadD[n] [(][L][G][,][F] [) + 3][c] 2n _._

r

b

Therefore, denoting again dFA as d[b]G,FA, the empirical version of Theorem 3.1 is as follows.
**Theorem 3.3. Assuming the individual loss function ℓ** : [0, c], given _S,_ _T_ _,_
_for ∀δ > 0 with probability ≥_ 1 − _δ we have_ _Y × Y →_ _∀D_ _D_ _∈PX×Y_

ln(8/δ)

_τ_ (A; _DS[n][,][ D][T]_ [)][ ≤] _[d][F]A_ [(][ b]DS[n][,][ b]DT[n] [) + 2Rad][ b]DT[n] [(][L][G][,][F][A] [) + 4Rad][ b]DS[n] [(][L][G][,][F][A] [) + 9][c] 2n _._

r

We can see that a smaller feature extractor function class[b] _FA implies both a smaller dFA and the_
Rademacher complexity. Therefore, the monotone relation between the regularization strength and
the upper bound on the relative domain transferability loss also holds for the empirical settings.

Other than direct regularization, empirically we find that the transferability is also related to the use
of data augmentation. Can we explain such phenomena from the view of regularization again? We
discuss this question next.

3.4 WHEN CAN DATA AUGMENTATION BE VIEWED AS REGULARIZATION?

In this subsection, we discuss the connections between data augmentation (DA) and regularization.
We present the results and their interpretation in this subsection, while deferring the detailed discussion to the Section B in the appendix.

Empirical research has shown evidence of the regularization effect of DA (Hern´andez-Garc´ıa &
K¨onig, 2018a;b). However, there is a lack of theoretical understanding on when can data augmentation be viewed as regularization in general. In an attempt to address this question, we consider a
general DA setting of affine transformation (Perez & Wang, 2017) with parameters (W⋆, b⋆) whose
distribution represents specific DA.

**General Settings. We consider the fine-tuning function g : R[d]** _→_ R as a linear layer, which will
be concatenated to the feature extractor f : R[m] _→_ R[d]. Given a model g ◦ _f_, we use the squared
loss ℓ(g ◦ _f_ (x), y) = (g ◦ _f_ (x) − _y)[2], and accordingly apply second-order Taylor expansion to the_
objective function to study the effect of data augmentation.

**DA categories. We discuss two categories of DA, feature-level DA and data-level DA. Feature-level**
DA (Wong et al., 2016; DeVries & Taylor, 2017) requires the transformation to be performed in the
learned feature space: given a data sample x ∈ R[m] and a feature extractor f, the augmented feature
is W⋆f (x) + b⋆ where W⋆ _∈_ R[d][×][d], b⋆ _∈_ R[d] are sampled from a distribution. On the other hand,


ln(4/δ)

2n


ln(8/δ)

2n


-----

data-level DA requires the transformation to be performed in the input space: given a data sample x,
the augmented sample is W⋆x + b⋆ where W⋆ _∈_ R[m][×][m], b⋆ _∈_ R[m] are sampled from a distribution.

**Intuition on sufficient conditions. For either the feature-level or the data-level DA, the intuitions**
given by our analysis are similar. Our results (Theorem B.1&B.2) suggest that the following conditions indicate the regularization effects of a data augmentation: 1) EW⋆ [W⋆] = I; 2) Eb⋆ [b⋆] = _[⃗]0;_
3) W⋆ and b⋆ are independent, where I is the identity matrix and _[⃗]0 is the zero vector; 4) W⋆_ is not a
constant if it is the feature-level DA; 5) DA is of a small magnitude if it is the data-level DA.

**Empirical verification. Combining with Theorem 3.3, it suggests that DA satisfying the conditions**
above may improve the relative domain transferability. In fact, it matches the empirical observations in Section 4. Concretely, 1) Gaussian noise satisfies the four conditions, and empirically the
Gaussian noise improves domain transferability while robustness decreases a bit (Figure 5); 2) Ro**_tation, which rotates input image with a predefined fixed angle with predefined fixed probability,_**
_violates EW⋆_ [W⋆] = I, and empirically the rotation barely affects domain transferability (Figure 14
in Appendix D.3); 3) Translation, which moves the input image for a predefined distance along a
pre-selected axis with fixed probability, violates Eb⋆ [b⋆] = _[⃗]0, and empirically the translation dis-_
tance barely co-relates to the domain transferability (Figure 14 in Appendix D.3).

4 EVALUATION

4.1 EXPERIMENTAL SETTING

**Source model training** We train our model on two source domains: CIFAR-10 and ImageNet.
Unless specified, we will use the training setting as follows[1]. For CIFAR-10, we train the model
with 200 epochs using the momentum SGD optimizer with momentum 0.9, weight decay 0.0005,
an initial learning rate 0.1 which decays by a factor of 10 at the 100-th and 150-th epoch. For
ImageNet, we train the model with 90 epochs using the momentum SGD optimizer with momentum
0.9, weight decay 0.0001, an initial learning rate 0.1 which decays by a factor of 10 at the 30-th and
60-th epoch. We use the standard cross entropy loss denote asis the trained model and x, y are the input and label respectively. To evaluate the robustness on the LCE(hs, x, y), where hs = gs ◦ _f_
source domain, we follow the evaluation setting in Ilyas et al. (2019) and perform the PGD attack
with 20 steps using ϵ = 0.25. We also evaluate the robustness using AutoAttack in Appendix D.4.
For both tasks we use ResNet-18 as the model structure. We provide results of other model structures
in appendix D.2.

**Domain Transferability** We evaluate the transferability from CIFAR-10 to SVHN and from ImageNet to CIFAR-10. For the ImageNet transferability, we focus on CIFAR as the target domain,
since it is the domain that is the most positively correlated with robustness as shown in Salman et al.
(2020). We evaluate the fixed-feature transfer where only the last fully-connected layer is fine-tuned
following our theoretical framework. We fine-tune the last layer with 40 epochs using momentum
SGD optimizer with momentum 0.9, weight decay 0.0005, an initial learning rate 0.01 which decays by a factor of 10 at the 20-th and 30-th epoch. To mitigate the impact of benign accuracy,
we evaluate the relative domain transfer accuracy (DT Acc) as follows. Let accsrc and acctgt be
the accuracy of the a fine-tuned model on source and target domain, and acc[v]src [and][ acc]tgt[v] [be the]
accuracy of vanilla model (i.e., models trained with standard setting) on source and target domain,
then the relative DT accuracy is defined as:

DT Acc = (acctgt − _accsrc) −_ (acc[v]tgt _[−]_ _[acc]src[v]_ [)]

We also provide the results of absolute DT accuracy in Appendix D.1.

4.2 RELATIONSHIP BETWEEN ROBUSTNESS AND TRANSFERABILITY UNDER
CONTROLLABLE CONDITIONS

We train the model under different controllable conditions to validate our analysis. In particular,
we train the methods by controlling different regularization or data augmentations to evaluate the
change of model robustness and transferability.

[1These settings are inherited from the standard training algorithms for CIFAR-10 (https://github.](https://github.com/kuangliu/pytorch-cifar)
[com/kuangliu/pytorch-cifar) and ImageNet (https://github.com/pytorch/examples/](https://github.com/kuangliu/pytorch-cifar)
[tree/master/imagenet).](https://github.com/pytorch/examples/tree/master/imagenet)


-----

CIFAR10 -> SVHN


ImageNet -> CIFAR10

3

3

18 20 22 24

|Col1|Col2|Col3|LLR( l) -0.01 0 0.01 0.1|
|---|---|---|---|
|||||
|R: -0.91|8|||


LLR( l[)]

-0.01
0
0.01
0.1

R: -0.918

Robust Acc (%)


10 20 30

|Col1|CIFAR10|-> SVHN|
|---|---|---|
|||LLOT(||gs||2)|
|||0.01 0.1 1.0 10.0|
||||
|R:|-0.916||


Robust Acc (%)


20 30 40

|Col1|ImageNet|-> CIFAR10|Col4|
|---|---|---|---|
|||LLOT(||gs|| 0.5 1 2 5|2)|
|||||
|R:|-0.954|||


Robust Acc (%)


10


10

10 20 30

|Col1|Col2|LLR( l)|Col4|
|---|---|---|---|
|||-0. 0 0.1 1.0|1|
|||||
|||||
|: -0.782||||


Robust Acc (%)


Figure 3: Relationship between robustness and transferability under different norms of last layer,
via training with last-layer regularization (LLR) and last-layer orthogonalization (LLOT).

**Controlling the norm of last layer** As shown in our framework, domain transferability is related
with the regularization on the feature extractor. Here we regularize the transferability by controlling
the norm of last linear layer gs. Intuitively, when we force the norm of gs to be large during training,
the corresponding norm of f will be regularized to be small. We use two approaches to control the
last layer norm:

-  Last-layer regularization (LLR): we impose a strong l2-regularizer with parameter λl specifically
on the weight of gs and therefore our training loss becomes: LLLR(hs, x, y) = LCE(hs, x, y) +
_λl · ||gs||F, where ||gs||F is the frobenius norm of the weight matrix of gs._

-  Last-layer orthogonal training (LLOT): we directly control the l2-norm of gs with orthogonal
training (Huang et al. (2020)). The orthogonal training will enforce the weight to become a 1norm matrix and we multiply a constant to obtain the desired norm _gs_ 2.
_||_ _||_

The result of LLR and LLOT are shown in Figure 3. We observe that when we regularize the norm
of last layer to be large (i.e. smaller λ in LLR and larger ||gs||2 in LLOT), the domain transferability
will increase while the model robustness will decrease (their negative correlation is significant with
Pearson’s coefficient around −0.9). This is because larger last layer norm will produce a feature
extractor f with smaller norm, which, according to our analysis, leads to a better domain transferability. On the other hand, the model gs _f will have a larger norm and therefore becomes less_
robust under adversarial attacks. _◦_

**Controlling the norm of feature extractor** We directly regularize the feature extractor f and
check the impact on domain transferability. We implement two regularization as follows:

-  Jacobian regularization (JR): we follow the approach in Hoffman et al. (2019) to apply JR on
the feature extractor. Given modelLCE(hs, x, y) + λj _J(f, x)_ _F_ [, where] hs =[ J] g[(][f, x]s ◦[)][ denotes the Jacobian matrix of]f, the training loss becomes: L[ f][ on]JR[ x](h[ and]s, x, y[ || · ||]) =[F]
is the frobenius norm. · || _||[2]_

-  Weight Decay (WD): we impose a strong weight decay with factor λw on the feature extractor
_f during training. This is equivalent to imposing a strong l2-regularizer with factor λw on the_
feature extractor (excluding the last layer).

The results under JR and WD are shown in Figure 4. We observe that with larger regularization on
the feature extractor, the model shows higher domain transferability, which matches our analysis.
Meanwhile, the robustness decreases significantly with large regularizer. This is because a large
regularization will harm the model performance on source domain and lead to low model robustness.


CIFAR10 -> SVHN


ImageNet -> CIFAR10


CIFAR10 -> SVHN


ImageNet -> CIFAR10


20 30 40 50 60 70

|JR(|j)|Col3|Col4|Col5|
|---|---|---|---|---|
|0 1 1|00 000||||
||||||
||||||
|: 0.3|13||||


Robust Acc (%)


10 20

|Col1|Col2|WD(|w)|Col5|
|---|---|---|---|---|
||0.000 0.001 0.005 0.01|0.000 0.001 0.005 0.01||5|
||||||
|R: -0.734|||||


Robust Acc (%)


40

10

15 25 35 45 55

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
|JR( 0 0|j) .01||||
|0 1|.1 .0||||
||||||
||||R:|0.105|


Robust Acc (%)


40

20


15

10


15

10


0

10 15 20

|Col1|Col2|WD(|w)|
|---|---|---|---|
|||0.0 0.0 0.0|001 005 01|
|||||
|||||
|R: -0.984||||


Robust Acc (%)


Figure 4: Relationship between robustness and transferability when we regularize the feature extractor with Jacobian Regularization (JR) and weight decay (WD).


-----

CIFAR10 -> SVHN


ImageNet -> CIFAR10


CIFAR10 -> SVHN


ImageNet -> CIFAR10


60

40

20


10


40

10

20 30 40 50 60 70

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
|Gaus|s( ) 0 0.05||||
||0.25 1.0||||
||||||
||||R:|0.16|


Robust Acc (%)


20 25 30

|Po|s(b)|Col3|Col4|Col5|
|---|---|---|---|---|
||1 4 8||||
||||||
||||||
|R:|0.880||||


Robust Acc (%)


20 30 40 50

|Pos(b)|Col2|Col3|Col4|
|---|---|---|---|
|1 4 8||||
|||||
|R: 0.566||||


Robust Acc (%)


20 30

|Col1|Col2|Gauss( ) 0 0.05 0.25|
|---|---|---|
|R: -0|.384||
||||


Robust Acc (%)


Figure 5: Relationship between robustness and transferability when we use Gaussian noise (Gauss)
and posterize (Pos) as data augmentations.


ResNet18


ResNet18


WideResNet-50


WideResNet-50


80

60

40

20


80

60

40

20


|Col1|Col2|Bl|ur(k)|
|---|---|---|---|
||||1 5 11|
|||||
|R: -0|.842|||


15 20

Robust Acc (%)


10


|Col1|Col2|Col3|Col4|
|---|---|---|---|
||B|B|lur(k) 1 5 11|
|||||
|R: -0.|852|||


22 24

Robust Acc (%)


15

10


|Col1|Col2|Rescale(m 1 2|)|
|---|---|---|---|
||4 8|4 8||
|||||
|||||


0 10 20

Robust Acc (%)


|Col1|Col2|Rescal|e(m) 1 2|
|---|---|---|---|
||||4 8|
|||||
|||||


0 10 20

Robust Acc (%)


Figure 6: Relationship between robustness and transferability on ImageNet when we use rescale and
blur as data augmentations.

**Data augmentation** As shown in Section 3.4, data augmentation can be viewed as a type of regularization during training and thus affects the domain transferability. Here we consider both noise
dependent and independent data augmentations.


**Noise-dependent data augmentation We include two noise-dependent augmentations:**

-  Gaussian Noise data augmentation (Gauss): we add zero-mean Gaussian noise with variance σ[2]
to the input image.

-  Posterize (Pos): we truncate each channel of one pixel value into b bits (originally they are 8 bits).

The results of Gauss and Pos are shown in Figure 5. We observe that the domain transferability
of trained model keeps improving with larger data augmentation, which matches our theory. The
robustness also benefits from a small data augmentation, but decreases when it becomes large.

**Resolution-related (noise-independent) data augmentation.** Specifically, for ImageNet to
CIFAR-10 transferability, we consider two resolution-related data augmentations. The intuition is
that when the target domain has a lower resolution than the source domain (ImageNet is 224 × 224
while CIFAR-10 is 32 _×_ 32), the data augmentations that down-sample the inputs during the training
on source domain will help transferability. We consider the below resolution-related augmentations:

-  Rescale: we rescale the input to be m times smaller (i.e., shape ImageNet as (224/m)×(224/m))
and then rescale them back to the original size.

-  Blur: we apply Gaussian blurring with kernel size k on the input. The Gaussian kernel is created
with a standard deviation randomly sampled from [0.1, 2.0].

We show the results of rescaling and blurring in Figure 6. The experiments are evaluated only
for ImageNet to CIFAR-10, and we include the results of both ResNet18 (the default model) and
WideResNet50. We can see that these data augmentations help with transferability to target domain,
although the robustness on the source domain decreases since these augmentations do not include
any robustness-related operations.


5 CONCLUSIONS

In this work, we theoretically analyze the sufficient conditions for domain transferability based on
the view of function class regularization. We also conduct experiments to verify our claims and
observe some counterexamples that shows a negative correlation between robustness and domain
transferability. These results are helpful in the domain generalization of machine learning models.


-----

ETHICS STATEMENT

Our work focuses on theoretically and empirically studying the domain transferability of a machine
learning model. All the datasets and packages we use are open-sourced. We do not have ethical
concerns in our paper.

REPRODUCIBILITY STATEMENT

We have tried our best to provide training details to facilitate reproducing our results. In Section 4.1
we provide detailed results on how to train the model and how to transfer the trained model to target
domain, as well as how we evaluate our model. We also upload the zip file of our code with the
submission. We will open-source our code once accepted.

REFERENCES

Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463–482, 2002.

Shai Ben-David, John Blitzer, Koby Crammer, Fernando Pereira, et al. Analysis of representations
for domain adaptation. Advances in neural information processing systems, 19:137, 2007.

Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
_ieee symposium on security and privacy (sp), pp. 39–57. IEEE, 2017._

Jianbo Chen, Michael I Jordan, and Martin J Wainwright. Hopskipjumpattack: A query-efficient
decision-based attack. In 2020 ieee symposium on security and privacy (sp), pp. 1277–1294.
IEEE, 2020.

Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, pp. 1310–1320. PMLR, 2019.

Nicolas Courty, R´emi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal transport for domain adaptation. IEEE transactions on pattern analysis and machine intelligence, 39(9):1853–
1865, 2016.

Terrance DeVries and Graham W Taylor. Dataset augmentation in feature space. arXiv preprint
_arXiv:1702.05538, 2017._

Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon Tran, and Aleksander Madry. Adversarial robustness as a prior for learned representations. _arXiv preprint_
_arXiv:1906.00945, 2019._

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.

Alex Hern´andez-Garc´ıa and Peter K¨onig. Data augmentation instead of explicit regularization. arXiv
_preprint arXiv:1806.03852, 2018a._

Alex Hern´andez-Garc´ıa and Peter K¨onig. Further advantages of data augmentation on convolutional
neural networks. In International Conference on Artificial Neural Networks, pp. 95–103. Springer,
2018b.

Judy Hoffman, Daniel A Roberts, and Sho Yaida. Robust learning with jacobian regularization.
_arXiv preprint arXiv:1908.02729, 2019._

Haoshuo Huang, Qixing Huang, and Philipp Krahenbuhl. Domain transfer through deep activation
matching. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 590–605,
2018.

Lei Huang, Li Liu, Fan Zhu, Diwen Wan, Zehuan Yuan, Bo Li, and Ling Shao. Controllable orthogonalization in training dnns. In Proceedings of the IEEE/CVF Conference on Computer Vision
_and Pattern Recognition, pp. 6429–6438, 2020._


-----

Andrew Ilyas, Shibani Santurkar, Logan Engstrom, Brandon Tran, and Aleksander Madry. Adversarial examples are not bugs, they are features. Advances in neural information processing
_systems, 32, 2019._

Vladimir Koltchinskii. Rademacher penalties and structural risk minimization. IEEE Transactions
_on Information Theory, 47(5):1902–1914, 2001._

Clare Lyle, Marta Kwiatkowksa, and Yarin Gal. An analysis of the effect of invariance on generalization in neural networks. In International conference on machine learning Workshop on
_Understanding and Improving Generalization in Deep Learning, volume 1, 2019._

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
_Learning Representations, 2018._

Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a
regularization method for supervised and semi-supervised learning. IEEE transactions on pattern
_analysis and machine intelligence, 41(8):1979–1993, 2018._

Krikamol Muandet, David Balduzzi, and Bernhard Sch¨olkopf. Domain generalization via invariant
feature representation. In International Conference on Machine Learning, pp. 10–18. PMLR,
2013.

Luis Perez and Jason Wang. The effectiveness of data augmentation in image classification using
deep learning. arXiv preprint arXiv:1712.04621, 2017.

Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. An online learning approach to interpolation and extrapolation in domain generalization. arXiv preprint arXiv:2102.13128, 2021.

Kevin Roth, Yannic Kilcher, and Thomas Hofmann. Adversarial training is a form of data-dependent
operator norm regularization. arXiv preprint arXiv:1906.01527, 2020.

Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry. Do adversarially robust imagenet models transfer better? arXiv preprint arXiv:2007.08489, 2020.

Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, and
Shin-Ming Cheng. Autozoom: Autoencoder-based zeroth order optimization method for attacking black-box neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 33, pp. 742–749, 2019.

Sebastien C Wong, Adam Gatt, Victor Stamatescu, and Mark D McDonnell. Understanding data
augmentation for classification: when to warp? In 2016 international conference on digital image
_computing: techniques and applications (DICTA), pp. 1–6. IEEE, 2016._

Kaichao You, Ximei Wang, Mingsheng Long, and Michael Jordan. Towards accurate model selection in deep unsupervised domain adaptation. In International Conference on Machine Learning,
pp. 7124–7133. PMLR, 2019.

Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
Theoretically principled trade-off between robustness and accuracy. In International Conference
_on Machine Learning, pp. 7472–7482. PMLR, 2019a._

Linjun Zhang, Zhun Deng, Kenji Kawaguchi, Amirata Ghorbani, and James Zou. How does mixup
help with robustness and generalization? arXiv preprint arXiv:2010.04819, 2020.

Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm for
domain adaptation. In International Conference on Machine Learning, pp. 7404–7413. PMLR,
2019b.

Han Zhao, Remi Tachet Des Combes, Kun Zhang, and Geoffrey Gordon. On learning invariant
representations for domain adaptation. In International Conference on Machine Learning, pp.
7523–7532. PMLR, 2019.


-----

# Appendix

A PROOFS

**Proposition A.1 (Proposition 3.1 Restated). Given the problem defined in subsection 3.1, fc[D][S]** _is a_
_minimizer of equation 1. Moreover, if c ≥_ _c[′]_ _≥_ 0, then the relative domain transfer loss ℓDT (fc[D][S] )−
_ℓDS_ (fc[D][S] ) ≥ _ℓDT (fc[D][′][ )][S]_ _[ −]_ _[ℓ][D]S_ [(][f][ D]c[′][ )][S] _[.]_

First, let’s verify thatProof. Recall that ℓDS (f ) = ∥f − _yS∥D, ℓDT (f_ ) = ∥f − _yT ∥D and fc[D][S]_ := yS min{1, _∥ySc∥D_ _[}][.]_

_fc[D][S]_ _∈_ arg minf _ℓDS_ (f ), s.t. ∥f _∥D ≤_ _c._
_∈F_

_cIf, we have c ≥∥yS∥D, then fc[D][S]_ := yS achieves the minimum. If c < ∥yS∥D, then for any f ∈F : ∥f _∥D ≤_


_ℓDS_ (f ) == ∥ ∥f(1 − −y∥Sy∥ScD∥D ≥∥[)][y][S]y[∥]S[D]∥[ =]D −∥[ ∥][y][S]f[ −]∥D∥ ≥∥ySc∥DyS[y][S]∥D[∥][D] −[ =]c[ ℓ][D][S] [(][f][ D]c _[S]_ ).

Therefore, fc[D][S] indeed achieves the minimum.

Now, let’s prove the proposition. For any c ≥∥yS∥D, we have ℓDS (fc[D][S] ) = 0 and ℓDT (fc[D][S] )
is a constant. Therefore, there is no difference for all c ≥∥yS∥D, and the proposition holds for
_c ≥_ _c[′]_ _≥∥yS∥D. Then, We only need to verify the case for ∥yS∥≥_ _c ≥_ _c[′]:_

_ℓDS_ (fc[D][′][ )][S] _[ −]_ _[ℓ][D]S_ [(][f][ D]c _[S]_ ) = c − _c[′]_ = ∥ _∥ySc∥D_ _[y][S][ −]_ _∥ySc[′]∥D_ _[y][S][∥][D]_

= ∥fc[D][′] _[S]_ _−_ _fc[D][S]_ _∥D = ∥fc[D][′]_ _[S]_ _−_ _yT + yT −_ _fc[D][S]_ _∥D_

_≥|∥fc[D][′]_ _[S]_ _−_ _yT ∥D −∥yT −_ _fc[D][S]_ _∥D|_

_≥∥fc[D][′]_ _[S]_ _−_ _yT ∥D −∥yT −_ _fc[D][S]_ _∥D_

= ℓDT (fc[D][′][ )][S] _[ −]_ _[ℓ][D]T_ [(][f][ D]c _[S]_ ).

Rearranging the above inequality gives the proposition.

**Proposition A.2 (Proposition 3.2 Restated). d** _,_ ( _,_ ) : R+ satisfies the follow_ing three properties._ _G_ _F_ _·_ _·_ _PX×Y ×PX×Y →_

_1. (Symmetry) d_ _,_ ( _S,_ _T ) = d_ _,_ ( _T,_ _S)._
_G_ _F_ _D_ _D_ _G_ _F_ _D_ _D_

_2. (Triangle Inequality) For_ _: d_ _,_ ( _S,_ _T )_ _d_ _,_ ( _S,_ )+ _d_ _,_ ( _,_ _T )._
_∀D[′]_ _∈PX×Y_ _G_ _F_ _D_ _D_ _≤_ _G_ _F_ _D_ _D[′]_ _G_ _F_ _D[′]_ _D_

_3. (Weak Zero Property) For ∀D ∈PX×Y_ _: dG,F_ (D, D) = 0.

_Proof. Recall that_

_d_ _,_ ( _S,_ _T ) := sup_ inf
_G_ _F_ _D_ _D_ _f_ _∈F_ _|_ _g∈G_ _[ℓ][D][S]_ [(][g][ ◦] _[f]_ [)][ −] _g[inf]∈G_ _[ℓ][D][T][ (][g][ ◦]_ _[f]_ [)][|][.]

We can see that the symmetry and weak zero property are obvious. For triangle inequality, given
:
_∀D[′]_ _∈PX×Y_

_d_ _,_ ( _S,_ _T ) = sup_ inf
_G_ _F_ _D_ _D_ _f_ _∈F_ _|_ _g∈G_ _[ℓ][D][S]_ [(][g][ ◦] _[f]_ [)][ −] _g[inf]∈G_ _[ℓ][D][T][ (][g][ ◦]_ _[f]_ [)][|]

= sup inf
_f_ _∈F_ _|_ _g∈G_ _[ℓ][D][S]_ [(][g][ ◦] _[f]_ [)][ −] _g[inf]∈G_ _[ℓ][D][′]_ [(][g][ ◦] _[f]_ [) + inf]g∈G _[ℓ][D][′]_ [(][g][ ◦] _[f]_ [)][ −] _g[inf]∈G_ _[ℓ][D][T][ (][g][ ◦]_ _[f]_ [)][|]

sup( inf
_≤_ _f_ _∈F_ _|_ _g∈G_ _[ℓ][D][S]_ [(][g][ ◦] _[f]_ [)][ −] _g[inf]∈G_ _[ℓ][D][′]_ [(][g][ ◦] _[f]_ [)][|][ +][ |][ inf]g∈G _[ℓ][D][′]_ [(][g][ ◦] _[f]_ [)][ −] _g[inf]∈G_ _[ℓ][D][T][ (][g][ ◦]_ _[f]_ [)][|][)]

sup inf inf
_≤_ _f_ _∈F_ _|_ _g∈G_ _[ℓ][D][S]_ [(][g][ ◦] _[f]_ [)][ −] _g[inf]∈G_ _[ℓ][D][′]_ [(][g][ ◦] _[f]_ [)][|][ + sup]f _∈F_ _|_ _g∈G_ _[ℓ][D][′]_ [(][g][ ◦] _[f]_ [)][ −] _g[inf]∈G_ _[ℓ][D][T][ (][g][ ◦]_ _[f]_ [)][|]

= d _,_ ( _S,_ ) + d _,_ ( _,_ _T )._
_G_ _F_ _D_ _D[′]_ _G_ _F_ _D[′]_ _D_


-----

**Proposition A.3. Denote the function class**

_,_ := _hg,f :_ R+ _g_ _, f_ _,_ _where hg,f_ (x, y) := ℓ(g _f_ (x), y).
_LG_ _F_ _{_ _X × Y →_ _|_ _∈G_ _∈F}_ _◦_

_Let d :_ R+ be a metric on _, and assume_ _h_ _,_ _is L-Lipschitz continuous with_
_X × Y →_ _X × Y_ _∀_ _∈LG_ _F_
_respect to the metric d. Then, we have_


_d_ _A_ ( _S,_ _T )_ _L_ _W_ ( _S,_ _T ),_
_F_ _D_ _D_ _≤_ _·_ _D_ _D_

_where W_ (DS, DT ) is the Wasserstein distance:

_W_ ( _S,_ _T ) =_ sup E(x,y) _S_ [φ(x, y)] E(x,y) _T [φ(x, y)]_ _s.t. φ is 1-Lipschitz._
_D_ _D_ _φ:X×Y→R_ _∼D_ _−_ _∼D_

_Proof. Recall that_


_d_ _,_ ( _S,_ _T ) := sup_ inf
_G_ _F_ _D_ _D_ _f_ _∈F_ _|_ _g∈G_ _[ℓ][D][S]_ [(][g][ ◦] _[f]_ [)][ −] _g[inf]∈G_ _[ℓ][D][T][ (][g][ ◦]_ _[f]_ [)][|][.]

By the definition of inf, for _ϵ > 0 there exist gS,ϵ, gT,ϵ_ such that
_∀_ _∈G_

inf
_g_ _[ℓ][D][S]_ [(][g][ ◦] _[f][ϵ][)][ ≥]_ _[ℓ][D][S]_ [(][g][S,ϵ][ ◦] _[f][ϵ][)][ −]_ _[ϵ]_
_∈G_

inf
_g_ _[ℓ][D][T][ (][g][ ◦]_ _[f][ϵ][)][ ≥]_ _[ℓ][D][T][ (][g][T,ϵ][ ◦]_ _[f][ϵ][)][ −]_ _[ϵ.]_
_∈G_


By the definition of sup, there exists fϵ such that
_∈F_

_d_ _,_ ( _S,_ _T )_ inf
_G_ _F_ _D_ _D_ _≤|_ _g_ _[ℓ][D][S]_ [(][g][ ◦] _[f][ϵ][)][ −]_ _g[inf]_ _[ℓ][D][T][ (][g][ ◦]_ _[f][ϵ][)][|][ +][ ϵ]_
_∈G_ _∈G_

= max inf
_{g_ _[ℓ][D][S]_ [(][g][ ◦] _[f][ϵ][)][ −]_ _g[inf]_ _[ℓ][D][T][ (][g][ ◦]_ _[f][ϵ][)][,][ inf]g_ _[ℓ][D][T][ (][g][ ◦]_ _[f][ϵ][)][ −]_ _g[inf]_ _[ℓ][D][S]_ [(][g][ ◦] _[f][ϵ][)][}][ +][ ϵ]_
_∈G_ _∈G_ _∈G_ _∈G_

_≤_ max{ℓDS (gT,ϵ ◦ _fϵ) −_ _ginf∈G_ _[ℓ][D][T][ (][g][ ◦]_ _[f][ϵ][)][, ℓ][D][T][ (][g][S,ϵ][ ◦]_ _[f][ϵ][)][ −]_ _g[inf]∈G_ _[ℓ][D][S]_ [(][g][ ◦] _[f][ϵ][)][}][ +][ ϵ]_

max _ℓ_ _S_ (gT,ϵ _fϵ)_ _ℓ_ _T (gT,ϵ_ _fϵ), ℓ_ _T (gS,ϵ_ _fϵ)_ _ℓ_ _S_ (gS,ϵ _fϵ)_ + 2ϵ.
_≤_ _{_ _D_ _◦_ _−_ _D_ _◦_ _D_ _◦_ _−_ _D_ _◦_ _}_ (2)

Let’s first consider the first term in the max{·, ·} above.

_ℓ_ _S_ (gT,ϵ _fϵ)_ _ℓ_ _T (gT,ϵ_ _fϵ)_
_D_ _◦_ _−_ _D_ _◦_

= L ( _L[1]_ _[ℓ][D][S]_ [(][g][T,ϵ][ ◦] _[f][ϵ][)][ −]_ _L[1]_ _[ℓ][D][T]_ [(][g][T,ϵ][ ◦] _[f][ϵ][))]_
_·_

= L (E(x,y) _S_ [ _L[1]_ _[ℓ][(][g][T,ϵ][ ◦]_ _[f][ϵ][(][x][)][, y][)]][ −]_ [E][(][x,y][)][∼D][T] [[][ 1]L _[ℓ][(][g][T,ϵ][ ◦]_ _[f][ϵ][(][x][)][, y][)])]_
_·_ _∼D_

_≤_ _L · W_ (DS, DT ),

where the inequality is due to that both _L[1]_ _[ℓ][(][g][S,ϵ][ ◦]_ _[f][ϵ][(][x][)][, y][)][ and][ 1]L_ _[ℓ][(][g][T,ϵ][ ◦]_ _[f][ϵ][(][x][)][, y][)][ are 1-Lipschitz]_

w.r.t. (x, y) and the metric d.

Similarly, we also have


_ℓ_ _T (gS,ϵ_ _fϵ)_ _ℓ_ _S_ (gS,ϵ _fϵ)_ _L_ _W_ ( _S,_ _T )._
_D_ _◦_ _−_ _D_ _◦_ _≤_ _·_ _D_ _D_

Therefore, equation 2 implies

_d_ _,_ ( _S,_ _T )_ _L_ _W_ ( _S,_ _T ) + 2ϵ._
_G_ _F_ _D_ _D_ _≤_ _·_ _D_ _D_

Letting ϵ → 0 completes the proof.


**Proposition A.4. Consider multi-class classification where Y = [k] for some k ≥** 2. Define the
_loss function ℓ_ _as_

_ℓ(g_ _f_ (x), y) = 1 arg max
_◦_ _{_ _j_ [k][(][g][ ◦] _[f]_ [(][x][))][j][ ̸][=][ y][}]
_∈_

_Let δT V (DS, DT ) denote the total variation distance. Then we have_

_d_ _A_ ( _S,_ _T )_ _δT V (_ _S,_ _T )_
_F_ _D_ _D_ _≤_ _D_ _D_


-----

_Proof. Fix f ∈F. By the definition of inf, there exists gT,ϵ such that_

inf
_|_ _g_ _[ℓ][D][S]_ [(][g][ ◦] _[f]_ [)][ −] _g[inf]_ _[ℓ][D][T][ (][g][ ◦]_ _[f]_ [)][|]
_∈G_ _∈G_

inf
_≤|_ _g_ _[ℓ][D][S]_ [(][g][ ◦] _[f]_ [)][ −] _[ℓ][D][T][ (][g][T,ϵ][ ◦]_ _[f]_ [)][|][ +][ ϵ]
_∈G_

_ℓ_ _S_ (gT,ϵ _f_ ) _ℓ_ _T (gT,ϵ_ _f_ ) + ϵ
_≤|_ _D_ _◦_ _−_ _D_ _◦_ _|_
= E(x,y) _S_ [ℓ(gT,ϵ _f_ (x), y)] E(x,y) _T [ℓ(gT,ϵ_ _f_ (x), y)] + ϵ
_|_ _∼D_ _◦_ _−_ _∼D_ _◦_ _|_

= P(x,y) _S_ [1 arg max
_|_ _∼D_ _{_ _j_ [k][(][g][T,ϵ][ ◦] _[f]_ [(][x][))][j][ ̸][=][ y][}][]][ −] [P][(][x,y][)][∼D][T][ [][1][{][arg max]j [k][(][g][T,ϵ][ ◦] _[f]_ [(][x][))][j][ ̸][=][ y][}][]][|][ +][ ϵ]
_∈_ _∈_

(3)

equation 3 asLet A be the event such that A = {(x, y) : arg maxj∈[k](gT,ϵ ◦ _f_ (x))j ̸= y}. Then we can write

(3) = P(x,y) _S_ [A] P(x,y) _T [A]_ + ϵ
_|_ _∼D_ _−_ _∼D_ _|_

sup P(x,y) _S_ [B] P(x,y) _T [B]_ + ϵ
_≤_ _B_ _|_ _∼D_ _−_ _∼D_ _|_


= δT V (DS, DT ) + ϵ

Send ϵ → 0. Noting that f ∈F was arbitrary, apply sup to both sides gives us the desired inequality.

Theorem 3.1 can be proved easily by definition.
**Theorem A.1 (Theorem 3.1 Restated). Given a training algorithm A, for** _S,_ _T_ _we_
_have_ _∀D_ _D_ _∈PX×Y_

_τ_ (A; _S,_ _T )_ _d_ _A_ ( _S,_ _T ),_
_D_ _D_ _≤_ _F_ _D_ _D_

_or equivalently,_ _ginf_ _[ℓ][D][T][ (][g][ ◦]_ _[f][ D]A_ _[S]_ [)][ ≤] _[ℓ][D]S_ [(][g]A[D][S] _◦_ _fA[D][S]_ [) +][ d][F]A [(][D][S][,][ D][T][ )][.]
_∈G_

_Proof. By definition,_

_τ_ (A; DS, DT ) = infg _[ℓ][D][T][ (][g][ ◦]_ _[f][ D]A_ _[S]_ [)][ −] _[ℓ][D]S_ [(][g]A[D][S] _◦_ _fA[D][S]_ [)]
_∈G_

_≤_ _ginf_ _[ℓ][D][T][ (][g][ ◦]_ _[f][ D]A_ _[S]_ [)][ −] _g[inf]_ _[ℓ][D][S]_ [(][g][ ◦] _[f][ D]A_ _[S]_ [)]
_∈G_ _∈G_

_≤| infg_ _[ℓ][D][T][ (][g][ ◦]_ _[f][ D]A_ _[S]_ [)][ −] _g[inf]_ _[ℓ][D][S]_ [(][g][ ◦] _[f][ D]A_ _[S]_ [)][|]
_∈G_ _∈G_

sup inf
_≤_ _f_ _∈FA_ _|_ _g∈G_ _[ℓ][D][T][ (][g][ ◦]_ _[f]_ [)][ −] _g[inf]∈G_ _[ℓ][D][S]_ [(][g][ ◦] _[f]_ [)][|]

= d _A_ ( _S,_ _T )._
_F_ _D_ _D_


To prove Theorem 3.2, we first prove the following interesting lemma.
**Lemma A.1. Let** _r_ := _y_ R[d] _y_ 2 = r _denotes the (d_ 1)-dimensional sphere in R[d] _with_
_S_ _[d][−][1]_ _{_ _∈_ _| ∥_ _∥_ _}_ _−_
_radius r > 0. If a function h :_ _r_ R[d] _satisfies_
_S_ _[d][−][1]_ _→_

_y_ _r_ : _h(y), y_ _< 0,_ (4)
_∀_ _∈S_ _[d][−][1]_ _⟨_ _⟩_

_then we have_
_⃗0_ conv(h( _r_ )),
_∈_ _S_ _[d][−][1]_

_i.e.,_ _[⃗]0 is in the convex hull of_ _h(y)_ _y_ _r_ _._
_{_ _|_ _∈S_ _[d][−][1]}_

_Proof. We assume that_ _[⃗]0 /_ conv(h( _r_ )) and prove by contradiction. Since _[⃗]0 /_ conv(h( _r_ )),
_∈_ _S_ _[d][−][1]_ _∈_ _S_ _[d][−][1]_
we can find a hyperplane that separates _[⃗]0 and the convex set conv(h(_ _r_ )). By the separating
_S_ _[d][−][1]_
hyperplane theorem there exists a nonzero vector v ∈ R[d] and c ≥ 0 such that

**_z_** conv(h( _r_ )) : **_z, v_** _c_ 0. (5)
_∀_ _∈_ _S_ _[d][−][1]_ _⟨_ _⟩≥_ _≥_


-----

We choose y = rv/∥v∥2 and observe that h(y) ∈ conv(h(Sr[d][−][1])). Hence, by equation 5 we have

_⟨h(y), y⟩≥_ 0,

which contradicts to the condition of equation 4. Therefore, it must be that _[⃗]0_ conv(h( _r_ )).
_∈_ _S_ _[d][−][1]_

**Theorem A.2function class G (Theorem 3.2 Restated) where G includes the zero function, and any training algorithm. Given any source distribution DS ∈PX× ARd, denote, any fine-tuning**

_ϵ := ℓDS_ (gA[D][S] _◦_ _fA[D][S]_ [)][ −] _g_ inf,f _A_ _[ℓ][D][S]_ [(][g][ ◦] _[f]_ [)][.]
_∈G_ _∈F_

_We assume some properties of the sample individual loss function ℓ_ : R[d] _× R[d]_ _→_ R+: it is
_differentiable and strictly convex w.r.t._ _its first argument; ℓ(y, y) = 0 for any y ∈_ R[d]; and
limr inf _y:_ _y_ 2=r ℓ([⃗]0, y) = _. Then, for any distribution_ _on_ _, there exist some distri-_
_→∞_ _∥_ _∥_ _∞_ _D[X]_ _X_
_butions_ _T_ _with its marginal on_ _being_ _such that_
_D_ _∈PX×Y_ _X_ _D[X]_

_τ_ (A; _S,_ _T )_ _d_ _A_ ( _S,_ _T )_ _τ_ (A; _S,_ _T ) + ϵ._
_D_ _D_ _≤_ _F_ _D_ _D_ _≤_ _D_ _D_

_Proof. The τ_ (A; _S,_ _T )_ _d_ _A_ ( _S,_ _T ) is proved by Theorem 3.1, we only need to prove that_
_D_ _D_ _≤_ _F_ _D_ _D_
there exists some _T_ with its marginal on being such that
_D_ _∈PX×Y_ _X_ _D[X]_

_dFA_ (DS, DT ) ≤ _τ_ (A; DS, DT ) + ϵ = infg _[ℓ][D][T][ (][g][ ◦]_ _[f][ D]A_ _[S]_ [)][ −] _g_ inf,f _A_ _[ℓ][D][S]_ [(][g][ ◦] _[f]_ [)][.]
_∈G_ _∈G_ _∈F_

We begin by observing that limr inf _y:_ _y_ 2=r ℓ([⃗]0, y) =, and thus there exists r > 0 such that
_→∞_ _∥_ _∥_ _∞_

_∀y ∈Sr[d][−][1]_ : _ℓ([⃗]0, y) ≥_ _ℓDS_ ([⃗]0) = E(x,y)∼DS [ℓ([⃗]0, y)], (6)

where _r_ := _y_ R[d] _y_ 2 = r denotes the (d 1)-dimensional sphere with radius r. Note
_S_ _[d][−][1]_ _{_ _∈_ _| ∥_ _∥_ _}_ _−_
the we abuse the notion a bit to let _[⃗]0 also denotes the zero function (i.e., maps all input to zero)._
Now, let us define at the following set

:= 1ℓ([⃗]0, y) _y_ _r_ _,_
_V_ _{∇_ _|_ _∈S_ _[d][−][1]}_

where ∇1 is taking the gradient w.r.t. the first argument of ℓ(·, ·). By the strict convexity of ℓ(·, y),
we have

_ℓ(y, y)_ _ℓ([⃗]0, y) >_ 1ℓ([⃗]0, y), y _._
_−_ _⟨∇_ _⟩_

Noting that ℓ(y, y) = 0 is the unique minimum of ℓ(·, y), we have ℓ([⃗]0, y) > 0. Accordingly,

_y_ _r_ : 0 > _ℓ([⃗]0, y) >_ 1ℓ([⃗]0, y), y _._
_∀_ _∈S_ _[d][−][1]_ _−_ _⟨∇_ _⟩_

Having the above property, and noting that 1ℓ([⃗]0, ) : _r_ R[d], we can invoke Lemma A.1 to
_∇_ _·_ _S_ _[d][−][1]_ _→_
see that

_⃗0 ∈_ conv(V).

Therefore, there exists n points _yi_ _i=1_ _r_ such that
_{_ _}[n]_ _[⊂S]_ _[d][−][1]_

_n_

_⃗0 =_ _ci_ 1ℓ([⃗]0, yi), (7)

_∇_
_i=1_

X

where ci > 0 and _i=1_ _[c][i][ = 1][.]_

Therefore, we can define the target distributiondistribution of y conditioned on[P][n] **_x is: y = yi with probability DT as the following. Given any ci. Now we verify the distribution x ∼D[X]_**, the DT
indeed makes the bound ϵ-tight. Denote a strictly convex function h : R[d] _→_ R+ as the following


_h(·) :=_


_ciℓ(_ _, yi)._

_·_
_i=1_

X


-----

Since h is strictly convex and ∇h([⃗]0) = _[⃗]0 (equation 7), we can see that h([⃗]0) achieves the unique_
global minimum of h on R[d].

Therefore, given the DT, for any ∀f ∈FA we have
inf
_g∈G_ _[ℓ][D][T][ (][g][ ◦]_ _[f]_ [) = inf]g∈G [E][(][x][,y][)][∼D][T][ [][ℓ][(][g][ ◦] _[f]_ [(][x][)][, y][)]]

_n_

= inf _ciℓ(g_ _f_ (x), yi)
_g∈G_ [E][x][∼D][X] " _i=1_ _◦_ #

X


= inf
_g∈G_ [E][x][∼D][X][ [][h][(][g][ ◦] _[f]_ [(][x][))]]

= h([⃗]0) (G contains the zero function)

_n_

= _ciℓ([⃗]0, yi)._ (8)

_i=1_

X

Recall that d _A_ ( _S,_ _T ) = supf_ _A_ inf _g_ _ℓ_ _T (g_ _f_ ) inf _g_ _ℓ_ _S_ (g _f_ ), we can see that
_F_ _D_ _D_ _∈F_ _|_ _∈G_ _nD_ _◦_ _−_ _∈G_ _D_ _◦_ _|_


_ciℓ([⃗]0, yi)_ inf (9)
_−_ _g_ _[ℓ][D][S]_ [(][g][ ◦] _[f]_ [)][|]
_i=1_ _∈G_

X


_d_ _A_ ( _S,_ _T ) = sup_
_F_ _D_ _D_ _f_ _∈FA_

By equation 6, for _f_ _A, we have_
_∀_ _∈Fn_


_ciℓ([⃗]0, yi) ≥_ _ℓDS_ ([⃗]0) = ℓDS ([⃗]0 ◦ _f_ ) ≥ _ginf_ _[ℓ][D][S]_ [(][g][ ◦] _[f]_ [)][.]
_i=1_ _∈G_

X

Hence, we can continue as


_ciℓ([⃗]0, yi)_ inf
_−_ _g_ _[ℓ][D][S]_ [(][g][ ◦] _[f]_ [)]
_i=1_ _∈G_

X


_ciℓ([⃗]0, yi)_ inf
_−_ _g_ _,f_ _A_ _[ℓ][D][S]_ [(][g][ ◦] _[f]_ [)]
_i=1_ _∈G_ _∈F_

X


(9) = sup
_f_ _∈FA_


= infg _[ℓ][D][T][ (][g][ ◦]_ _[f][ D]A_ _[S]_ [)][ −] _g_ inf,f _A_ _[ℓ][D][S]_ [(][g][ ◦] _[f]_ [)] (by equation 8)
_∈G_ _∈G_ _∈F_

= infg _[ℓ][D][T][ (][g][ ◦]_ _[f][ D]A_ _[S]_ [)][ −] _[ℓ][D]S_ [(][g]A[D][S] _◦_ _fA[D][S]_ [) +][ ℓ][D]S [(][g]A[D][S] _◦_ _fA[D][S]_ [)][ −] _g_ inf,f _A_ _[ℓ][D][S]_ [(][g][ ◦] _[f]_ [)]
_∈G_ _∈G_ _∈F_

= τ (A; DS, DT ) + ϵ.
Therefore, it holds that d _A_ ( _S,_ _T )_ _τ_ (A; _S,_ _T ) + ϵ, and thus the theorem._
_F_ _D_ _D_ _≤_ _D_ _D_

**Lemma A.2 (Lemma 3.1 Restated). Assuming the individual loss function ℓ** : Y ×Y → [0, c], given
_any distribution_ _and_ _δ > 0, with probability_ 1 _δ we have_
_D ∈PX×Y_ _∀_ _≥_ _−_

ln(4/δ)

_dG,F_ (D, _D[n]) ≤_ 2RadD[n] [(][L][G][,][F] [) + 3][c] 2n _._

r

b

_Proof. Given any δ > 0, f_ _, g[b]_,, and taking any hg,f _,_ (Definition 3),
with probability ≥ 1 − _δ we have ∈F_ _∈G_ _D ∈PX×Y_ _n_ _∈LG_ _F_

_ℓD(g ◦_ _f_ ) − _ℓD[n]_ [(][g][ ◦] _[f]_ [) =][ E][(][x,y][)][∼D][[][h][g,f] [(][x, y][)]][ −] _n[1]_ _hg,f_ (xi, yi)

_i=1_

b X

ln(2/δ)

2Rad _,_ (10)
_≤_ _D[n]_ [(][L][G][,][F] [) + 3][c] 2n

r

where the inequality is by the well-known Rademacher complexity uniform bound. Similarly, b


_n_

_ℓ_ _hg,f_ (xi, yi)
_D[n]_ [(][g][ ◦] _[f]_ [)][ −] _[ℓ][D][(][g][ ◦]_ _[f]_ [) =][ E][(][x,y][)][∼D][[][−][h][g,f] [(][x, y][)]][ −] _n[1]_ _−_

_i=1_

b X

ln(2/δ)

2Rad
_≤_ _D[n]_ [(][−L][G][,][F] [) + 3][c] 2n

r

b ln(2/δ)

= 2Rad _._ (11)
_D[n]_ [(][L][G][,][F] [) + 3][c] 2n

r

b


-----

The probability that both events equation 10 and equation 11 happen can be upper bounded by union
bound, i.e.,

Pr((10) ∧ (11)) = 1 − Pr((10)[c] _∨_ (11)[c]) ≥ 1 − (Pr((10)[c]) + Pr((11)[c])) ≥ 1 − 2δ.

Therefore, combining the above with probability ≥ 1 − _δ we have_


ln(4/δ)

_ℓ_ (g _f_ ) _ℓ_ _._ (12)
_|_ _D_ _◦_ _−_ _D[n]_ [(][g][ ◦] _[f]_ [)][| ≤] [2Rad][ b]D[n] [(][L][G][,][F] [) + 3][c] 2n

r

With equation 12, we can prove the lemma as the following. Givenb _∀ϵ > 0, by the definition of_
infimum there exists a gϵ such that
_∈G_

_ℓD(gϵ ◦_ _f_ ) < infg∈G _[ℓ][D][(][g][ ◦]_ _[f]_ [) +][ ϵ.]

By equation 12, with probability ≥ 1 − _δ we have_


ln(4/δ)

2n


_ℓ_
_D[n]_ [(][g][ϵ][ ◦] _[f]_ [)][ ≤] _[ℓ][D][(][g][ϵ][ ◦]_ _[f]_ [) + 2Rad][ b]D[n] [(][L][G][,][F] [) + 3][c]

Moreover, by definitionb

inf
_g_ _[ℓ]D[b][n]_ [(][g][ ◦] _[f]_ [)][ ≤] _[ℓ]D[b][n]_ [(][g][ϵ][ ◦] _[f]_ [)][.]
_∈G_

Combining the above three inequalities we have


ln(4/δ)

2n

r

ln(4/δ)

_._
2n


inf
_g_ _[ℓ]D[b][n]_ [(][g][ ◦] _[f]_ [)][ <][ inf]g _[ℓ][D][(][g][ ◦]_ _[f]_ [) +][ ϵ][ + 2Rad][ b]D[n] [(][L][G][,][F] [) + 3][c]
_∈G_ _∈G_

Letting ϵ → 0, we can see that


inf _._
_g_ _[ℓ]D[b][n]_ [(][g][ ◦] _[f]_ [)][ ≤] _g[inf]_ _[ℓ][D][(][g][ ◦]_ _[f]_ [) + 2Rad][ b]D[n] [(][L][G][,][F] [) + 3][c] 2n
_∈G_ _∈G_ r

Similarly, we can derive the above inequality again but with D and _D[n]_ switched. Therefore,

ln(4/δ)

inf [b] _._
_|_ _g_ _[ℓ]D[b][n]_ [(][g][ ◦] _[f]_ [)][ −] _g[inf]_ _[ℓ][D][(][g][ ◦]_ _[f]_ [)][| ≤] [2Rad][ b]D[n] [(][L][G][,][F] [) + 3][c] 2n
_∈G_ _∈G_ r

Since the above inequality holds for ∀f ∈F, taking the supremum over f ∈F gives the lemma.

**Lemma A.3. Assuming the individual loss function ℓ** : Y × Y → [0, c], given any distributions
_S,_ _T_ _and_ _δ > 0, with probability_ 1 _δ we have_
_D_ _D_ _∈PX×Y_ _∀_ _≥_ _−_


ln(8/δ)

2n


_dFA_ (DS, DT ) ≤ _dFA_ (DS[n][,][ b]DT[n] [) + 2(Rad][ b]DS[n] [(][L][G][,][F][A] [) + Rad][ b]DT[n] [(][L][G][,][F][A] [)) + 6][c]

_Proof. By Proposition 3.2, we apply the triangle inequality to derive[b]_


_dFA_ (DS, DT ) ≤ _dFA_ (DS, _DT[n]_ [) +][ d][F]A [(][ b]DT[n] _[,][ D][T]_ [)]

_≤_ _dFA_ (DS[n][,][ b]D[b]T[n] [) +][ d][F]A [(][ b]DT[n] _[,][ D][T]_ [) +][ d][F]A [(][ b]DS[n][,][ D][S][)][.]

By Lemma 3.1, we can apply the union bound argument (e.g., see the proof of Lemma 3.1) to bound
_dFA_ (DT[n] _[,][ D][T][ )][ and][ d][F]A_ [(][ b]DS[n][,][ D][S][)][. That being said,][b] _[ ∀][δ][′][ >][ 0][ with probability][ ≥]_ [1][ −] [2][δ][′][ we have]

ln(4/δ[′])

[b] _dFA_ (DS[n][,][ D][S][)][ ≤] [2Rad][ b]DS[n] [(][L][G][,][F][A] [) + 3][c] 2n

r

ln(4/δ[′])

_dFA_ (D[b]T[n] _[,][ D][T]_ [)][ ≤] [2Rad][ b]DT[n] [(][L][G][,][F][A] [) + 3][c] 2n _._

r

Therefore,

[b]


ln(4/δ[′])

2n


_dFA_ (DT[n] _[,][ D][T]_ [) +][ d][F]A [(][ b]DS[n][,][ D][S][)][ ≤] [2(Rad][ b]DS[n] [(][L][G][,][F][A] [) + Rad][ b]DT[n] [(][L][G][,][F][A] [)) + 6][c]

Denoting δ = 2δ[′] gives the lemma.

[b]


-----

**Theorem A.3 (Theorem 3.3 Restated). Given** _S,_ _T_ _, for_ _δ > 0 with probability_
_≥_ 1 − _δ we have_ _∀D_ _D_ _∈PX×Y_ _∀_


ln(8/δ)

_τ_ (A; _DS[n][,][ D][T]_ [)][ ≤] _[d][F]A_ [(][ b]DS[n][,][ b]DT[n] [) + 2Rad][ b]DT[n] [(][L][G][,][F][A] [) + 4Rad][ b]DS[n] [(][L][G][,][F][A] [) + 9][c] 2n

r

[b]

_Proof. For ∀δ > 0, from the proof of Lemma A.3 we can see that with probability ≥_ 1 − _δ:_


ln(8/δ)

(13)
2n

ln(8/δ)

_,_
2n


_dFA_ (DS[n][,][ D][S][)][ ≤] [2Rad][ b]DS[n] [(][L][G][,][F][A] [) + 3][c]

_dFA_ (D[b]T[n] _[,][ D][T]_ [)][ ≤] [2Rad][ b]DT[n] [(][L][G][,][F][A] [) + 3][c]

and Lemma A.3 holds. Therefore

[b]


_S_ _S_ _S_
_τ_ (A; _DS[n][,][ D][T]_ [) = inf]g _[ℓ][D][T][ (][g][ ◦]_ _[f]AD[n]_ [)][ −] _[ℓ]D[b]S[n]_ [(][g]AD[n] _◦_ _fAD[n]_ [)]
_∈G_ b b b

_S_ _S_

[b] _≤_ _ginf_ _[ℓ][D][T][ (][g][ ◦]_ _[f]AD[n]_ [)][ −] _g[inf]_ _[ℓ]D[b]S[n]_ [(][g][ ◦] _[f]AD[n]_ [)]

_∈G_ b _∈G_ b

_S_ _S_ _S_ _S_
= infg _[ℓ][D][T][ (][g][ ◦]_ _[f]AD[n]_ [)][ −] _g[inf]_ _[ℓ][D][S]_ [(][g][ ◦] _[f]AD[n]_ [) + inf]g _[ℓ][D][S]_ [(][g][ ◦] _[f]AD[n]_ [)][ −] _g[inf]_ _[ℓ]D[b]S[n]_ [(][g][ ◦] _[f]AD[n]_ [)]
_∈G_ b _∈G_ b _∈G_ b _∈G_ b

_d_ _A_ ( _S,_ _T ) + d_ _A_ ( _S[,][ D][S][)]_
_≤_ _F_ _D_ _D_ _F_ _D[n]_

ln(8/δ)

_≤_ _dFA_ (DS, DT ) + 2RadD[b] _S[n]_ [(][L][G][,][F][A] [) + 3][c] 2n

r

b ln(8/δ)

_≤_ _dFA_ (DS[n][,][ b]DT[n] [) + 2Rad][ b]DT[n] [(][L][G][,][F][A] [) + 4Rad][ b]DS[n] [(][L][G][,][F][A] [) + 9][c] 2n _,_

r

where the first inequality is by definition of infimum, the second inequality is by the Definition 2,

[b]

the third inequality is by equation 13 and the last inequality is by Lemma A.3.

B DATA AUGMENTATION (DA) AS REGULARIZATION


In this section, we discuss data augmentation (DA) as a concrete example of regularization for training feature extractor fA[D][S] [, and explore its impact on the function class][ F][A][ discussed in Section 3.]

Empirical research has shown evidence of the regularization effect of DA (Hern´andez-Garc´ıa &
K¨onig, 2018a;b). However, there is a lack of theoretical analysis, and thus we aim to construct a
theoretical framework to understand under what sufficient conditions DA can be viewed as regularization on the feature extractor function class _A. We categorize DA into feature-level DA and_
_F_
_data-level DA, and for each category, we analyze different DA algorithms to characterize the suf-_
ficient conditions under which DA regularizes the function class _A. Combined with analysis in_
_F_
Theorem 3.3, we also provide concrete sufficient conditions to tighten the upper bound of relative
transferability τ (A; _D[n], DT )._

**General Settings. For the following discussion we apply a general DA setting of affine transfor-**
mation (Perez & Wang, 2017), taking the form of[b] _x⋆_ = W⋆[⊤][x][ +][ b][⋆][, where][ (][x, x][⋆][)][ is a pair of the]
original and augmented samples, (W⋆, b⋆) are parameters representing specific DA policies. We set
_g : R[d]_ _→_ R as the linear layer corresponding to the weight matrix Wg, which will be composed with
the feature extractor f : R[m] _→_ R[d]. We use squared loss for ℓ : R × R → R, and let ℓD[n],A[(][g][ ◦] _[f]_ [)]
be the objective function given by training algorithm A from Theorem 3.3.
b

B.1 FEATURE-LEVEL DA (A[F L])

Feature-level DA (Wong et al., 2016; DeVries & Taylor, 2017) requires the transformation to be
performed in the learned feature space, which gives us an augmented feature W⋆f (x) + b⋆. We
use Loss-Averaging algorithm where we take an average of the loss over augmented features for


-----

training. Denote the training algorithm based on feature-level DA as A[F L], the objective function is
as below.

_n_

_ℓ_ _,A[F L]_ [(][g][ ◦] _[f]_ [) = 1] EW⋆,b⋆ _ℓ_ _g_ _W⋆f_ (xi) + b⋆ _, yi_ _._
_D[n]_ _n_ _◦_

_i=1_

b X     

**Theorem B.1. Apply feature-level DA with affine transformation parameters (W⋆, b⋆) s.t.** _1)_
EW⋆ [W⋆] = Im; 2) W⋆ _̸≡_ Im (i.e., W⋆ _is not an identity matrix); 3) Eb⋆_ [b⋆] = _[⃗]0m; 4) W⋆_ _and_
_b⋆_ _are independent. Set ℓ_ : R × R → R as squared loss; Define ∆W⋆ := W⋆ _−_ Im, then we have

_ℓ_ _,A[F L]_ [(][g][ ◦] _[f]_ [) =][ ℓ] [b] _,A[(][g][ ◦]_ _[f]_ [) + Ω][A][F L] _[,]_
_D[n]_ _D[n]_

_n_

_where ΩAF L =_ _n[1]_ _i=1_ [E][W]⋆ b _f_ (xi)[⊤]∆W⋆ _Wg_ 2 + Eb⋆ _b[⊤]⋆_ _[W][g]_ 2 _._

P h i h i

[2] [2]

_Proof. ℓ[′′](Wg[⊤]_ _[◦]_ _[f]_ [(][x][i][)) = 2][ for][ ℓ] [as squared loss. Apply Taylor expansion to][ ℓ] _g ◦_ _W⋆f_ (xi) +

_b⋆_ _, yi_ around f (xi), all higher-than-two order terms will vanish:   
 


_g_ _W⋆f_ (xi) + b⋆ _, yi_
_◦_
   []

_Wg[⊤]_ _[◦]_ _[f]_ [(][x][i][)][, y][i] + Wg[⊤][(∆][W]⋆ _[f]_ [(][x][i][) +][ b][⋆][)][ℓ][′][(][W][ ⊤]g _[◦]_ _[f]_ [(][x][i][)][, y][i][)+]



EW⋆,b⋆

=EW⋆,b⋆


1
_g_ [(∆][W]⋆ _[f]_ [(][x][i][) +][ b][⋆][)(∆][W]⋆ _[f]_ [(][x][i][) +][ b][⋆][)][⊤][ℓ][′′][(][W][ ⊤]g
2 _[W][ ⊤]_ _[◦]_ _[f]_ [(][x][i][)][, y][i][)][W][g]



=ℓ _Wg[⊤]_ _[◦]_ _[f]_ [(][x][i][)][, y][i] + EW⋆,b⋆ _Wg[⊤][(∆][W]⋆_ _[f]_ [(][x][i][) +][ b][⋆][)(∆][W]⋆ _[f]_ [(][x][i][) +][ b][⋆][)][⊤][W][g]
  h i

=ℓ _Wg[⊤]_ _[◦]_ _[f]_ [(][x][i][)][, y][i] + EW⋆ _f_ (xi)[⊤]∆W⋆ _Wg_ 2 + Eb⋆ _b[⊤]⋆_ _[W][g]_ 2 ;

The second equality holds since  E∆W⋆h= EW⋆ [W⋆ Im[2]] = 0i (m,mh) and E[2]bi⋆ = _[⃗]0m; The third_
_−_

equality holds since Wi and bi are independent. Therefore, ℓ _,A[F L]_ [(][g][◦][f] [) :=][ 1]n _ni=1_ EW⋆,b⋆ _ℓ_ _g_
_D[n]_ _◦_

 

b P

_W⋆f_ (xi) + b⋆ _, yi_ = ℓ _,A[(][g][ ◦]_ _[f]_ [) + Ω][A][F L] [.]
_D[n]_
   [] b

**_Interpretation. ΩAF L is composed of two segments: 1) l2 regularization to an f_** -dependent scalar
averaged over W⋆ and xi; 2) l2 regularization to an f -independent scalar averaged over b⋆. Due to
the regularization effect on f from the first segment of ΩAF L, we can reasonably expect the function
class _A′ enabled by A[F L]_ to be a subset of that enabled by a general training algorithm A.
_F_

**_Sufficient conditions. Combined with Theorem 3.3, the sufficient conditions to tighten the upper_**
bound dFA (DS[n][,][ b]DT[n] [)][ for the relative transferability][ τ] [(][A][;][ b]DS, DT ) are: feature-level DA (A[F L]) with
parameters satisfying: 1) EW⋆ [W⋆] = Im; 2) W⋆ _̸≡_ Im; 3) Eb⋆ [b⋆] = _[⃗]0m; 4) W⋆_ and b⋆ are
independent.[b]

B.2 DATA-LEVEL DA (A[DL])

Data-level DA requires that the transformation to be performed in the input space to generate augmented samples W⋆x + b⋆. We cover analysis on two ubiquitous algorithms for data-level DA
training: Prediction-Averaging (A[DL]P _[)][ (Lyle et al., 2019) and][ Loss-Averaging (][A]L[DL][)][ (Wong et al.,]_
2016). The difference between A[DL]P and A[DL]L lies in whether we take the average of the prediction
or the losses:


_ℓDb[n],A[DL]P_ [(][g][ ◦] _[f]_ [) := 1]n

_ℓDb[n],A[DL]L_ [(][g][ ◦] _[f]_ [) := 1]n


_ℓ_ EW⋆,b⋆ _g ◦_ _f_ (W⋆xi + b⋆) _, yi_ ; (14)
_i=1_

Xn     

EW⋆,b⋆ _ℓ_ _g ◦_ _f_ (W⋆xi + b⋆), yi _._
_i=1_

X    


-----

_parametersTheorem B.2. (W Define the data-level deviation caused by data-level DA⋆, b⋆) from the original data sample as ∆xi := (W⋆_ _−_ Im A)x[DL]i + b∈{⋆, and defineA[DL]P _[, A]L[DL] ∆[}][ with][3]x_ [:=]

Exi,W⋆,b⋆ ∆xi 2 _. Suppose we apply data-level DA s.t. 1) EW⋆_ [W⋆] = Im; 2) Eb⋆ [b⋆] = _[⃗]0m; 3)_

(∆[j]x[)][ ≈]h[0][,][ ∀][j][ ∈] [N]i [+][, j][ ≥] [3][; 4)][ W][⋆] _[and][ b][⋆]_ _[are independent. Define][ ∆][W]⋆_ [:=][ W][⋆] _[−]_ [I][m]
_O_ [3] _[∈]_ [R][m][×][m][,]
∆yi [:=][ W][ ⊤]g _[f]_ [(][x][i][)][ −] _[y][i]_ _[∈]_ [R][. Let][ W][ (]g[k][)] _∈_ R be the k[th] _dimension component of Wg and then define_

_wi,b(k) := Wg[(][k][)]∆yi_ _i[)][ as]_

_[∈]_ [R][; Denote the Hessian matrix of the][ k][th][ dimension component in][ f] [(][x]

_Hf[(][k][)][,i]; Let ∇f be the Jacobian matrix ofb_ _f_ _, then we have_

_ℓ_ _,A[DL]_ [(][g][ ◦] _[f]_ [) =][ ℓ] [b] _,A[(][g][ ◦]_ _[f]_ [) + Ω][A][DL][ +][ O][(∆]x[3] [)][,]
_D[n]_ _D[n]_

_n_ _d_

_where ΩADLP_ = _n[1]_ _i=1_ b _k=1_ _[w][i,][(][k][)]_ _tr_ EW⋆ [∆xi ∆[⊤]xi []][H]f[(][k][)][,i] _, where ∆xi = (W⋆_ I)[⊤]xi + _b⋆;_

_−_

ΩADLL = ΩADLP +Pn[1] _ni=1P_ EW⋆ _x[⊤]i_ h[∆][W]⋆ _[∇][f]_ [(][x][i][)][W][g] 2 [+][ E][b][⋆]ib[⊤]⋆ _[∇][f]_ [(][x][i][)][W][g] 2 _._

h i
P

_Proof. Let ∆fi,ADLP_ := EW⋆,b⋆ _f_ (W⋆[⊤][x][i] [+][ b][⋆][)][ −] _[f]_ [(][x][i][)][, then][2] [2]

∆fi,ADLP :=EW⋆,b⋆ _f_ (W⋆[⊤][x][i] [+][ b][⋆][)][ −] _[f]_ [(][x][i][)]

=EW⋆,b⋆ _f_ (xi)[⊤](∆xi ) + [1] ∆[⊤]xi _[H]f[(][k][)][,i](xi)∆xi_ 2[)]
_∇_ 2 [E][W][⋆][,b][⋆] _d_ [+][ O][(][E][W][⋆][,b][⋆] _[∥][∆][x][i]_ _[∥][3]_
h i h i

= [1] ∆[⊤]xi _[H]f[(][k][)][,i](xi)∆xi_ 2[)][,] (15)

2 [E][W][⋆][,b][⋆] _d_ [+][ O][(][E][W][⋆][,b][⋆] _[∥][∆][x][i]_ _[∥][3]_
h i

where [·[(][k][)]]d denotes a d-dimensional vector and k denotes the k[th] dimension element. Since ℓ
is squared loss, the third-and-higher derivative are 0, therefore, the third-and-higher order terms in
Taylor expansion to ℓ EW⋆,b⋆ _g ◦_ _f_ (W⋆xi + b⋆) _, yi_ around f (xi) will vanish:

_ℓ_ EW⋆,b ⋆ _g_ _f(W⋆xi + b⋆)_ _, yi_  
_◦_

=ℓ g ◦ _f_ (xi), yi + Wg[⊤][(∆]fi,A _[DL]P_ [)][ℓ][′][ ]g ◦ _f_ (xi), yi +

1
2 [W][ ⊤]g [(∆]fi,A[DL]P [)(∆][f][i][,A]P[DL] [)][⊤][W][g][ℓ][′′][ ]g ◦ _f_ (xi), yi 

=ℓ _g ◦_ _f_ (xi), yi + Wg[⊤][(∆]fi,A[DL]P [)][ℓ][′][ ]g ◦ _f_ (xi), yi + O(EW⋆,b⋆ _∥∆xi_ _∥2[4][)]_ (16)

Substitute Eq. (15) into the first-order term in Eq. (16), we have   

_Wg[⊤][(∆]fi,A[DL]P_ [)][ℓ][′][ ]g ◦ _f_ (xi), yi =Wg[⊤][E][W]⋆[,b]⋆ ∆[⊤]xi _[H]f[(][k][)][,i]∆xi_ _d[∆]y[b]i_ [+][ O][(][E]W⋆,b⋆ _[∥][∆]xi_ _[∥][3]2[)]_
 _d_ h i

=∆yi _Wg[(][k][)]EW⋆,b⋆_ ∆[⊤]xi _[H]f[(][k][)][,i]∆xi_ + (EW⋆,b⋆ ∆xi 2[)]

_O_ _∥_ _∥[3]_
_k=1_

_db_ X h i

= _wi,(k)tr_ EW⋆,b⋆ [∆xi ∆[⊤]xi []][H]f[(][k][)][,i] + (EW⋆,b⋆ ∆xi 2[)][.]

_O_ _∥_ _∥[3]_
_k=1_

X   

(17)


Substitute Eq. (17) into Eq. (16), we have

_ℓ_ EW⋆,b⋆ _g ◦_ _f_ (W⋆xi + b⋆) _, yi_ =ℓ _g ◦_ _f_ (xi), yi
      


_wi,(k)tr_ EW⋆,b⋆ [∆xi ∆[⊤]xi []][H]f[(][k][)][,i]
_k=1_

X  


_O(EW⋆,b⋆_ _∥∆xi_ _∥2[3][)][.]_ (18)

Substitute Eq. (18) into Eq. (14) which is the definition of ℓD[n],A[DL]P [(][g][ ◦] _[f]_ [)][, and recall that][ ∆]x[3] [:=]

Exi,W⋆,b⋆ ∆xi 2, we have b
h i _n_

_ℓD[n],A[DL]P_ [(][g][ ◦] _[f]_ [) := 1][3] _n_ _ℓ_ EW⋆,b⋆ _g ◦_ _f_ (W⋆xi + b⋆) _, yi_ = ℓD[n],A[(][g][ ◦] _[f]_ [) + Ω][A][DL]P + O(∆[3]x[)][.]

_i=1_

b X      b (19)


-----

Let ∆fi,ADLL := f (W⋆[⊤][x][i] [+][ b][⋆][)][ −] _[f]_ [(][x][i][) =][ ∇][f] [(][x][i][)][⊤][(∆][W]⋆ _[x][i]_ [+][ b][⋆][) +][ O][(][∥][∆][x]i _[∥][2]2[)][.]_

Applying Taylor expansion to EW⋆,b⋆ _ℓ_ _g ◦_ _f_ (W⋆xi + b⋆), yi around f (xi) will give us

EW⋆,b⋆ _ℓ_ _g ◦_ _f_ (W⋆xi + b⋆), yi =ℓ  g ◦ _f_ (xi), yi + Wg[⊤][E][W]⋆[,b]⋆ ∆fi,ADLL _ℓ[′][ ]g ◦_ _f_ (xi), yi +

1

    2 [W][ ⊤]g [E][W]⋆[,b]⋆ (∆ _fi,ADLL_ [)(∆][f][i][,A] _L[DL]_ [)][⊤][]Wgℓ[′′][ ]g ◦ _f_ (xi), yi

(20)

 

Since EW⋆,b⋆ ∆fi,ADLL = ∆fi,ADLP [, the first-order term in Eq. (20) is exactly Eq. (17):]

_Wg[⊤][E][W]⋆[,b]⋆_ ∆fi,ADLL _ℓ[′][ ]g ◦_ _f_ (xi), yi

=Wg[⊤][∆]fi,A[DL]P _[ℓ][′][ ]g ◦_ _f_ (xi), yi 

_d_



= _wi,(k)tr_ EW⋆,b⋆ [∆xi ∆[⊤]xi []][H]f[(][k][)][,i] + (EW⋆,b⋆ ∆xi 2[)] (21)

_O_ _∥_ _∥[3]_
_k=1_

X   

The second-order term in Eq. (20) is
1
2 _[W][ ⊤]g_ [E][W]⋆[,b]⋆ (∆fi,ADLL [)(∆][f][i][,A]L[DL] [)][⊤][]Wgℓ[′′][ ]g ◦ _f_ (xi), yi

=Wg[⊤][E][W]⋆[,b]⋆ (∇f (xi)[⊤](∆W⋆ _xi + b⋆)(∆W⋆_ _xi + b⋆)[⊤]∇f_ (xi) _Wg + O(EW⋆,b⋆_ _∥∆xi_ _∥2[4][)]_

=EW⋆ _x[⊤]i_ [∆]W[⊤] _⋆_ _[∇][f]_ [(][x][i][)][W][g] 2 [+][ E][b][⋆] _b[⊤]⋆_ _[∇][f]_ [(][x][i][)][W][g] 2 [+][ O][(][E][W][⋆][,b][⋆] _[∥][∆][x][i]_ _[∥]2[4][)]_ (22)

Substituting Eq. (21) and Eq. (22) into Eq. (20), we have

[2] [2]

EW⋆,b⋆ _ℓ_ _g ◦_ _f_ (W⋆xi + b⋆), yi

_d_

   

=ℓ _g_ _f_ (xi), yi + _wi,(k)tr_ EW⋆,b⋆ [∆xi ∆[⊤]xi []][H]f[(][k][)][,i] +
_◦_

_k=1_

   X   

EW⋆ _x[⊤]i_ [∆][W]⋆ _[∇][f]_ [(][x][i][)][W][g] 2 [+][ E][b][⋆] _b[⊤]⋆_ _[∇][f]_ [(][x][i][)][W][g] 2 [+][ O][(][E][W][⋆][,b][⋆] _[∥][∆][x][i]_ _[∥]2[4][)]_ (23)

Substitute Eq. (23) into the definition of ℓ[2] _D[n],A[DL]L_ [(][g][ ◦] _[f]_ [)][, then] [2]

_n_

b

_ℓD[n],A[DL]L_ [(][g][ ◦] _[f]_ [) := 1]n EW⋆,b⋆ _ℓ_ _g ◦_ _f_ (W⋆xi + b⋆), yi = ℓD[n],A[(][g][ ◦] _[f]_ [) + Ω][A][DL]L + O(∆[3]x[)]

_i=1_

b X     b (24)

The proof is complete by Eq. (19) and Eq. (24).


**_Interpretation. ΩADLP_** and ΩADLL turn out to be: 1) ΩADLP is a weighted trace expectation dependent
on the Hessian matrix of f ; 2) ΩADLL is equivalent to ΩADLP together with the summation of two
norm expectations dependent on _f_ . Therefore, the data-level DA algorithms A[DL]P and A[DL]L are
_∇_
expected to regularizebe reasonably expected as a subset of f so that the f function class FA enabled by general training algorithm FA[DL] enabled by A[DL] _∈{ AA[DL]P._ _[, A]L[DL][}][ would]_

**_Sufficient conditions._** Combined with Theorem 3.3, the sufficient conditions indicated here to
tighten the upper bound dFA (DS[n][,][ b]DT[n] [)][ of the relative transferability][ τ] [(][A][;][ b]DS, DT ) are: datalevel DA (A[DL]) with DA parameters satisfying that 1) EW⋆ [W⋆] = Im; 2) Eb⋆ [b⋆] = _[⃗]0m; 3)_
(∆[j]x[)][ ≈] [0][,][ ∀][j][ ∈] [N][+][, j][ ≥] [3][; 4)][ W][b] _[⋆]_ [and][ b][⋆] [are independent.]
_O_

**_Empirical verification. We further provide empirical verification in Section 4 for the sufficient_**
conditions above, investigating the concrete cases of DA methods: 1) Gaussian noise satisfies the
sufficient conditions, then we empirically show that Gaussian noise improves domain transferability
while robustness decreases a bit (Figure 5); 2) Rotation, which rotates input image with a predefined
fixed angle with predefined fixed probability, violates EW⋆ [W⋆] = Im, and we empirically show that
rotation barely affect domain transferability (Figure 14 in Appendix D.3); Translation, which moves
the input image for a predefined distance along a pre-selected axis with fixed probability, violates
Eb⋆ [b⋆] = _[⃗]0m (Figure 14 in Appendix D.3)._


-----

**Corollary B.2.1. If the neural network in Theorem B.2 is activated by Relu or Max-pooling, then**
_Theorem B.2 becomes_

_ℓ_ _,A[DL]_ [(][g][ ◦] _[f]_ [) =][ ℓ] [b] _,A[(][g][ ◦]_ _[f]_ [) + Ω][A][DL][ +][ O][(∆]x[3] [)][,]
_D[n]_ _D[n]_

_where ΩADLP_ = 0; ΩADLL b= _n[1]_ _ni=1_ EW⋆ _x[⊤]i_ [∆][W]⋆ _[∇][f]_ [(][x][i][)][W][g] 2 [+][ E][b][⋆] _b[⊤]⋆_ _[∇][f]_ [(][x][i][)][W][g] 2 _._

h i
P

[2] [2]

_Proof. Denote an L−layer NN g ◦_ _f_ (x) := Wg[⊤] _[·][ z][[][L][−][1]][, where][ z][[][l][]][ :=][ σ][[][l][−][1]][(][W][ ⊤][l−1]_ _[·][ z][[][l][−][1]][)][,]_
_l = 1, 2, 3, ..., L −_ 1; Define that σ[[0]](W[0][⊤] _[·][ z][[0]][) :=][ x][, then][ ∇][2][ ]g ◦_ _f_ (x) = 0 (B.2 of Zhang et al.
(2020)). Since ∇[2][ ]g ◦ _f_ (x) = Wg[⊤] _[· ∇][2][f]_ [(][x][)][, we have][ ∇][2][f] [(][x][) = 0][.] 

Combine this with Theorem B.2, we have

_n_ _d_

ΩADLP = n[1] _wi,(k)_ _tr_ EW⋆ [∆xi ∆[⊤]xi []][H]f[(][k][)][,i] = 0;

_i=1_ _k=1_

X X _n_ h  i

ΩADLL = ΩADLP + n[1] EW⋆ _x[⊤]i_ [∆][W]⋆ _[∇][f]_ [(][x][i][)][W][g] 2 [+][ E][b][⋆] _b[⊤]⋆_ _[∇][f]_ [(][x][i][)][W][g] 2

_i=1_

_n_ X h i

[2] [2]

= n[1] EW⋆ _x[⊤]i_ [∆][W]⋆ _[∇][f]_ [(][x][i][)][W][g] 2 [+][ E][b][⋆] _b[⊤]⋆_ _[∇][f]_ [(][x][i][)][W][g] 2 _._

_i=1_

X h i

[2] [2]


**_Remark. Corollary B.2.1 analyzes special cases (Relu/ Max-pooling activation) of Theorem B.2,_**
giving notably different regularization effect: in these cases the A[DL]P (average the prediction) fails as
a regularizer, therefore, doesn’t fulfill our sufficient conditions for improving domain transferability
(Theorem 3.3); A[DL]L (average the loss) only reserves the regularization on _f_ -dependent norms,
_∇_
but no longer regularizes _f_ (x). Since A[DL]L still induces regularization, the induced sufficient
_H_
conditions analyzed after Theorem B.2 for promoting domain transferability won’t be affected.

C ADVERSARIAL TRAINING AS A REGULARIZER

In this section, we show, under certain conditions, why adversarial training may improve domain
generalization by viewing adversarial training as a function class regularizer.

We first provide some notation. Let

= _fθ(_ ) = W _[L]φ[L][−][1](W_ _[L][−][1]φ[L][−][2](. . . ) + b[L][−][1]) + b[L]_
_F_ _{_ _·_ _}_

where φ[j] are activations, W _[j], b[j]_ are weight matrix and bias vector, θ is the collection of parameters
(i.e. θ = (W [1], b[1], . . ., W _[L], b[L]). For the rest of the article, assume that φ[j]_ are just ReLUs.

Now fix x ∈X . Define the preactivation as

_x[1]_ := W [1]x + b[1]


_x[j]_ := W _[j]φ[j][−][1](x[j][−][1]) + b[j]_ _, j ≥_ 2

e

Define the activation pattern φx := (e _φ[1]x[, . . ., φ][L]x_ _[−]e[1])_ 0, 1 such that for each j [L 1]
_∈{_ _}[m]_ _∈_ _−_

_φ[j]x_ [=][ 1][(]x[e][j] _> 0)_

where 1 is applied elementwise.

Now, given an activation pattern φ ∈{0, 1}[m], we define the preimage X(φ) := {x ∈ R[d] : φx = φ}
**Theorem C.1. (In the proof of theorem 1 in (Roth et al., 2020))**

_Let ϵ > 0 s.t. Bϵ[p][(][x][)][ ⊂]_ _[X][(][φ][x][)][ where][ B]ϵ[p][(][x][)][ denotes the][ l][p]_ _[ball centered at][ x][ with radius][ ϵ][. Let]_
_p = {1, 2, ∞} and q be the Holder conjugate of p (i.e._ _p[1]_ [+][ 1]q [= 1][). Then]

E(x,y)∼P [l(y, f (x)) + λ _x[∗]maxBϵ[p][(][x][)][ ∥][f]_ [(][x][)][ −] _[f]_ [(][x][∗][)][∥][q][] =][ E][(][x,y][)][∼][P][ [][l][(][y, f] [(][x][)) +][ λ][ ·][ ϵ] _v[∗]_ :maxv[∗] _p_ 1 _Jf_ (x)v _q[]]_
_∈_ _∥_ _∥_ _≤_


-----

**Interpretation: This theorem provides an equivalence between the objective functions for adversar-**
ial training (left term) and jacobian regularization (right term). We give some intuition on the size of
_ϵ. Let us first consider a shallow 2 layer network f_ (x) = W [2]φ(W [1]x+b[1]). Suppose W [2] _∈_ R[m][2][×][m][1]
and W [1] _∈_ R[m][1][×][d]. Given a matrix M, let Mj denote the jth row of M . We study the activation
pattern φx which equals


1{W1[1][x][ +][ b]1[1][}]
.
.
.
1{Wm[1] 1 _[x][ +][ b]m[1]_ 1 _[}]_


_φx = (φ[1]x[) =]_


We wish to compute the largest radius ϵ such that the activation pattern φx is constant within Bϵ[2][(][x][)][.]
This is simply the distance from x to the closest hyperplane of the form HW 1j _[,b]j[1]_ [=][ {][x][ ∈] [R][d][ :]
_Wj[1][x][ +][ b]j[1]_ [= 0][}][ where][ j][ = 0][, . . ., m][1][ (i.e.][ ϵ][ = min][j][ dist][(][x, H]Wj[1][,b]j[1] [)][). In particular, if][ W][ 1][ =][ I][d][×][d]
and b[1] = 0, ϵ = minj∈d|xj|.

Furthermore, we note that ϵ is nondecreasing as a function of the number of layers. However, it has
been observed empirically in (Roth et al., 2020) that approximate correspondence holds in a much
larger ball.
**Definition 4. (source and target function class) Let G[S], G[T]** _be fine tuning function classes for source_
_and target domains, respectively. We define the class of source models as_

_H[S]_ = G[S] _◦F = {g[S]_ _◦_ _fθ : g[S]_ _∈G[S], fθ ∈F}_

_and the class of target models as_


_H[T]_ = G[T] _◦F = {g[T]_ _◦_ _fθ : g[T]_ _∈G[T]_ _, fθ ∈F}_

**Definition 5. (empirical training objective with jacobian regularization) Let λ, ϵ > 0. Take any**
_hypothesis hθ = g[S]_ _◦_ _fθ ∈H[S]. Let_ _R[ˆ](hθ) =_ _n[1]_ _ni=1_ _[ℓ][(][h][θ][(][x][i][)][, y][i][)][ denote the empirical risk where]_

_l(ˆy, y) =_ _yˆ_ _y_ _. We define the empirical training objective with jacobian regularization as_
_∥_ _−_ _∥[2]_ P


Obj[A]λ [(][h][θ][) = ˆ]R(hθ) + _[λ][ ·][ ϵ]_


_Jhθ_ (xi) 2
_∥_ _∥_
_i=1_

X


**Theorem C.2. Fix regularization strength λ > 0. Define**

_Fλ[A]_ [=][ {][f][ A]θ _[∈F][ :][ ∃][g][S][ ∈G][S][ s][.][t][.][ Obj]λ[A][(][g][S][ ◦]_ _[f][ A]θ_ [)][ ≤] [Obj]λ[A][(][0][)][}]

_where 0 denotes the zero function (i.e. the class of feature extractors that outperform the zero_
_function). Suppose (x, y)_ _is bounded such that max (_ _x_ _,_ _y_ 2) _R. Fix δ > 0._
_∈X × Y_ _∥_ _∥∞_ _∥_ _∥_ _≤_
_Suppose we additionally restrict our fine tuning class models to linear models where_

_G[S]_ = {W : W ∈ R[d][×][n], n ≥ 1, minj _∥Wj∥2 ≥_ _δ}_

_(where Wj is the jth column of W_ _) and_

_G[T]_ = {W : W ∈ R[d][×][n], n ≥ 1}

_(Here we are abusing notation to let g[S]_ _∈_ _G[S]_ _to denote the last linear layer as well as the fine_
_tuning function)._

_Then for 0 ≤_ _λ1 < λ2_
_Fλ[A]2_ [⊊] _[F]λ[A]1_ [⊊] _[F]_

_(where ⊊_ _denotes proper subset). In particular, if Hλ[A,T]_ = G[T] _◦Fλ[A][, we have]_

_λ2_ ⊊ _λ1_ ⊊
_H[A,T]_ _H[A,T]_ _H[T]_

**Interpretation:**

At the high level, this theorem captures the idea that minimizing the empirical risk with jacobian
regularization puts a constraint on the set of feature extractors. In particular, Fλ[A]1 [represents the]
potential class of feature extractors we select after training with jacobian regularization. Therefore,


-----

the class of fine tuned modelsthe target domain is smaller than the class of fine tuned models Hλ[A,T]1 with feature extractors trained with jacobian regularization for H with feature extractors trained
without any regularization. Furthermore, we show that the space of feature extractors shrinks as we
increase the regularization stength λ. Since we showed in section 3.3 that smaller function classes
have smaller dFA, this theorem shows that jacobian regularization reduces dFA . To connect back to
adversarial training, if ϵ satisfies the hypothesis in theorem C.1, we have that
E(x,y)∼P [l(y, f (x)) + λ _x[∗]maxBϵ[p][(][x][)][ ∥][f]_ [(][x][)][ −] _[f]_ [(][x][∗][)][∥][q][] =][ E][(][x,y][)][∼][P][ [][l][(][y, f] [(][x][)) +][ λ][ ·][ ϵ] _v[∗]_ :maxv[∗] _p_ 1 _Jf_ (x)v _q[]]_
_∈_ _∥_ _∥_ _≤_

Therefore, minimizing the training objective with jacobian regularization is equivalent to minimizing
the adversarial training objective. Using this connection, this theorem essentially shows that, given
sufficient number of samples, adversarial training reduces the class of feature extractors which in
turn reduces dFA .

Finally, we comment on the assumption that _g[S]_ _> δ. Since δ > 0 is arbitrary, we can make it as_
small as we like and thus we are essentially excluding the 0 last layer which is hardly a constraint on
the function class. This assumption is necessary as we are considering regularization on the whole
model g _◦_ _f as opposed to regularization on just the feature extractor. Thus, this assumption prevents_
the scenario where only the last linear layer is regularized.

_Proof. We first show that if 0 ≤_ _λ1 < λ2, we have that_
_Fλ[A]2_ [⊊] _[F]λ[A]1_ [⊊] _[F]_

We first prove the following lemma

**Lemma C.1. Suppose the conditions of theorem C.2 are satisfied. Suppose additionally we have**
_that y :=_ _n1_ _di=1_ _[y][i][ ̸][= 0][ (note this occurs with probability 1 if marginal distribution over][ Y][ is]_
_continuous). Then for every λ_ 0, there exists a function fθ _λ_ _[and a fine tuning layer][ g][∗]_ _[∈G][S]_
_such that_ P _≥_ _∈F_ _[A]_
Obj[A]λ [(][g][∗] _[◦]_ _[f][θ][) = inf]_ _λ_ [(][g][ ◦] _[f][θ][) = Obj]λ[A][(][0][)]_
_g_ _G[S][ Obj][A]_
_∈_

_Choose another λ[′]_ _≥_ 0 (can equal λ). Then there exists a g[∗′] _∈G[S]_ _be the fine tuning layer such_
_that inf_ _g_ _S Obj[A]λ[′]_ [(][g][ ◦] _[f][θ][) = Obj][A]λ[′]_ [(][g][∗′][ ◦] _[f][θ][)][ and]_
_∈G_

_n_

1
_n_ _∥Jg∗′◦fθ_ (xi)∥2 > 0

_i=1_

X

_Proof. Fix α ≥_ 0 and c > α · R. Set biases


_b[1]_ =

0 _. . ._
0 _. . ._
. .
. .
. .
_. . ._ _. . ._




 _[b][j][ =][ 0][, j][ ≥]_ [2]


and weights


0 _. . ._
0 _. . ._
. .
. .
. .
_. . ._ _. . ._


_W_ [1] = . . . .

. . . .
. . . .

0 _. . ._ _. . ._ 0 
  
  _[W][ j][ =]_ 

Define xi,j be the jth entry of the data point xi. Define
_αi := α · xi,1_


_W_ [1] =




 _[, j][ ≥]_ [2]


_α := [1]_

_n_

_y := [1]_


_αi_
_i=1_

X

_d_

_yi_
_i=1_

X


-----

Now we observe that for a fixed λ ≥ 0 and any g ∈G[S], we have that

_n_

Obj[A]λ [(][g][ ◦] _[f][θ][) = ˆ]R(g ◦_ _fθ) + λ · ϵ_ _n[1]_ _∥Jg◦fθ_ (xi)∥2

_i=1_

X


0 _. . ._
0 _. . ._
. .
. .
. .
_. . ._ _. . ._


2

_n_ _g11_

.

(αxi,1 + c)  ..  _yi_

_−_

Xi=1 _gd1_ 2

 

_n_  

(αi + c)g1 _yi_ 2 [+][ λ][ ·][ ϵ][ 1]
_i=1_ _∥_ _−_ _∥[2]_ _n_

X


= [1]

_n_

= [1]

_n_

= [1]


+ λ _ϵ_ [1]
_·_ _n_


_i=1_


_α_ _g1_ 2
_∥_ _∥_
_i=1_

X


_i=1_ _∥(αi + c)g1 −_ _yi∥2[2]_ [+][ λ][ ·][ ϵα][ ∥][g][1][∥]2

X


Therefore,

is equivalent to solving

inf
_w∈R[d]_ : ∥w∥2≥δ


inf _λ_ [(][g][ ◦] _[f][θ][)]_
_g∈G[S][ Obj][A]_

_n_

_∥(αi + c)w −_ _yi∥2[2]_ [+][ λ][ ·][ ϵα][ ∥][w][∥]2 (25)
_i=1_

X


Utilizing lagrange multipliers, we find the minimizer is


_y_

(26)
_∥y∥_


_w = δ ·_


when c ≥ _[∥][y]δ[∥][2]_ [.]

Now consider the function

_S(c, α) = [1]_


_i=1_ _∥(α · xi,1 + c)g1 −_ _yi∥2[2]_ [+][ λ][ ·][ ϵα][ ∥][g][1][∥]2

X


Note that this function is continuous with respect to the input (c, α). Now fix α = 0, c = _[∥][y]δ[∥][2]_ [. Set]

_w = δ_ _yy_
_·_ _∥_ _∥_ [. Then we have that]


_S(_ _[∥][y][∥][2]_


_, 0) = [1]_


_n_

_y_ _yi_ 2 _[<][ 1]_
_∥_ _−_ _∥[2]_ _n_
_i=1_

X


_∥yi∥2[2]_ [= Obj]λ[A][(][0][)]
_i=1_

X


The inequality comes from the fact that we assumed y ̸= 0 and noting that y is the minimizer of the
function p(z) = _n[1]_ 2[. Continuity of][ S][ ensures that there exists][ α][0][ >][ 0][ such that]

_[∥][z][ −]_ _[y][i][∥][2]_ _n_

_S(_ _[∥][y][∥][2]_ _, α0) <_ [1] _yi_ 2 [= Obj]λ[A][(][0][)]

_δ_ _n_ _∥_ _∥[2]_

_i=1_

X

Now consider U (t) = S((1 + t) _[∥][y]δ[∥][2]_ _[,][ (1 +][ t][)][α][0][)][ for][ t][ ≥]_ [0][. Note that][ U][ is continuous with respect]

to t. Furthermore, we note that t →∞ implies U (t) →∞ which implies there exists some time
_t = Tf such that U_ (Tf ) > Obj[A]λ [(][0][)][. Therefore, by the intermediate value theorem, there exists a]
time t = T such that U (T ) = Obj[A]λ [(][0][)][. Finally, set][ c][ = (][T][ + 1)][ ∥][y]δ[∥][2] [,][ α][ = (][T][ + 1)][α][0][, and][ g][∗] [as]

the matrix where g1[∗] [=][ δ][ ·] _∥yy∥_ [and][ 0][ for the other columns. By equation][ 25][ and equation][ 26][ we have]

Obj[A]λ [(][g][∗] _[◦]_ _[f][θ][) = inf]_ _λ_ [(][g][ ◦] _[f][θ][) =][ U]_ [(][T] [) = Obj]λ[A][(][0][)]
_g∈G[S][ Obj][A]_

Furthermore, if we choose another λ[′] _≥_ 0, since c = (T + 1) _[∥][y]δ[∥][2]_ _>_ _[∥][y]δ[∥][2]_ by equation 26, we have

that

Obj[A]λ[′] [(][g][∗′][ ◦] _[f][θ][) = inf]_ _λ[′]_ [(][g][ ◦] _[f][θ][)]_
_g∈G[S][ Obj][A]_


-----

and

_n_

1
_n_ _∥Jg∗′◦fθ_ (xi)∥2 = α _g[∗′]_ 2 [=][ αδ]

_i=1_

X

which is nonzero as δ > 0 and α = (T + 1)α0 > 0.


Clearly, we have Fλ[A]2 _[⊂F]λ[A]1_ [. If we can show that][ f][θ]1 _[̸∈F]λ[A]2_ [then we have][ F]λ[A]2 [⊊] _[F]λ[A]1_ [.]

Using lemma C.1 we can find fθ1 ∈Fλ[A]1 [such that]

inf _λ1_ [(][g][ ◦] _[f][θ]1_ [) = Obj][A]λ1 [(][0][)]
_g_ _G[S][ Obj][A]_
_∈_

In addition lemma C.1 guarantees minimizers g1[∗] [and][ g]2[∗] [such that]


Obj[A]λ1 [(][g]1[∗] 1 [) = inf] _λ1_ [(][g][ ◦] _[f][θ]1_ [)][ and][ 1]

_[◦]_ _[f][θ]_ _g∈G[S][ Obj][A]_ _n_

Obj[A]λ2 [(][g]2[∗] 1 [) = inf] _λ2_ [(][g][ ◦] _[f][θ]1_ [)][ and][ 1]

_[◦]_ _[f][θ]_ _g∈G[S][ Obj][A]_ _n_

Thus, we have that


_Jg1∗[◦][f][θ]_ [(][x][i][)] 2 _[>][ 0]_

_Jg2∗[◦][f][θ]1_ [(][x][i][)] 2 _[>][ 0]_


_i=1_

_n_

_i=1_

X


_n_

Obj[A]λ1 [(][g]2[∗] _[◦]_ _[f][θ]1_ [) = ˆ]R(g2[∗] _[◦]_ _[f][θ]1_ [) +][ λ][2] _[·][ ϵ][ 1]n_ _i=1_ _Jg2∗[◦][f][θ]1_ [(][x][i][)] 2

Xn

_>_ _R[ˆ](g2[∗]_ _[◦]_ _[f][θ]1_ [) +][ λ][1] _[·][ ϵ][ 1]n_ _i=1_ _Jg2∗[◦][f][θ]1_ [(][x][i][)] 2 since λ2 > λ1

Xn

_≥_ _R[ˆ](g1[∗]_ _[◦]_ _[f][θ]1_ [) +][ λ][1] _[·][ ϵ][ 1]n_ _i=1_ _Jg1∗[◦][f][θ]1_ [(][x][i][)] 2 def of g1[∗]

X

= Obj[A]λ1 [(][0][)] lemma C.1


Thus fθ1 ̸∈Fλ[A]2 [which implies][ F]λ[A]2 [⊊] _[F]λ[A]1_ [. It remains to show for][ λ][1][ ≥] [0][, we have that][ F]λ[A]1 [⊊] _[F][.]_

Consider any g . For j [L], define W _[j]_ as the weight matrix where W _[j]_ = Id _d (identity_
_∈G[S]_ _∈_ _×_
matrix) for j [L 1] and let the final weight W _[L]_ = B _Id_ _d for some constant B > 0. Set the_
bias vectors b ∈[j] = 0 − for j ≥ 2. Let the first bias equal b[1] = · R ·× 1 where 1 is the vector of all 1’s and
_R is the upper bound such that ∥x∥∞_ _≤_ _R. Set θ = (W_ [1], b[1], . . ., W _[L], b[L]) and let hθ = g ◦_ _fθ_

We compute


Obj[A]λ1 [(][h][θ][) = ˆ]R(hθ) + λ1 _ϵ_ [1]
_·_ _n_


_Jhθ_ (xi) 2
_∥_ _∥_
_i=1_

X


= [1]

_n_

= [1]

_n_

_≥_ _n[1]_


_n_

_B(xi + R1)_ _yi_ + [1]
_∥_ _−_ _∥[2]_ _n_
_i=1_

X


_Jhθ_ (xi) 2
_∥_ _∥_
_i=1_

X


_B(xi + R1)_ _yi_ + B _g_ 2
_∥_ _−_ _∥[2]_ _∥_ _∥_
_i=1_

Xn

_∥B(xi + R1) −_ _yi∥[2]_ + Bδ
_i=1_

X


We note that sending B →∞ we get Obj[A]λ1 [(][h][θ][)][ →∞] [which implies that there exists a][ B][ =][ B][′]

such that Obj[A]λ1 [(][h][θ][)][ >][ Obj]λ[A]1 [(][0][)][. Setting][ B][ =][ B][′][ implies][ f][θ] _[̸∈F]λ[A]1_ [.]


-----

D EXTRA EXPERIMENT RESULTS

D.1 ABSOLUTE TRANSFERABILITY


We show the results with absolute transferability in Figure 7,8,9 and 10 respectively.


10 20 30

|CIFAR10|->|SVHN|
|---|---|---|
|||LLR( l) -0.1|
|||0 0.1 1.0|
||||
|: -0.798|||


Robust Acc (%)


ImageNet -> CIFAR10


CIFAR10 -> SVHN


20 30 40

|Col1|ImageNet|-> CIFAR10|Col4|
|---|---|---|---|
|||LLOT(||gs||2|)|
|||0.5 1 2 5||
|||||
|R:|-0.929|||


Robust Acc (%)


40

35

30

25

20


10 20 30

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||LLOT(||gs||2 0.01 0.1|)|
|||1.0 10.0||
|||||
|||||
|R:|-0.904|||


Robust Acc (%)


76

74

72

70


40

35

30


76

74


72


70

18 20 22 24

|Col1|Col2|Col3|LLR(|l)|
|---|---|---|---|---|
||||-0.01 0 0.01||
||||0.1||
||||||
|R: -0.92|2||||


Robust Acc (%)


Figure 7: Robustness and absolute transferability when we control the norm of last layer with lastlayer regularization (LLR) and last-layer orthogonal training (LLOT) with different parameters.


CIFAR10 -> SVHN


ImageNet -> CIFAR10


CIFAR10 -> SVHN


ImageNet -> CIFAR10


35

30

25


90

85

80

75


75

70


35

30


|JR( 0 0|j) .001|Col3|Col4|Col5|
|---|---|---|---|---|
|0 1|.01 .0||||
||||||
||||||
|R: 0.9|02||||


15 25 35 45 55

Robust Acc (%)


65

|WD( w) 0.000 0.000 0.001|1 5|Col3|
|---|---|---|
||||


10 15 20

Robust Acc (%)


|Col1|Col2|Col3|R: -0|.619|
|---|---|---|---|---|
||||||
|JR(|j)||||
|0 1 1 1|0 00 000||||


20 30 40 50 60 70

Robust Acc (%)


|Col1|Col2|Col3|Col4|
|---|---|---|---|
||WD( w) 0.000 0.001|||
||||5|
|||0.005 0.01||


10 20

Robust Acc (%)


Figure 8: Robustness and absolute transferability when we regularize the feature extractor with
Jacobian Regularization (JR) and weight decay (WD) with different parameters.


CIFAR10 -> SVHN


ImageNet -> CIFAR10


CIFAR10 -> SVHN


ImageNet -> CIFAR10


20 30 40 50 60 70

|Gau|ss( ) 0 0.05 0.25|Col3|Col4|Col5|
|---|---|---|---|---|
||1.0||||
||||||
||||||
|R: 0.|768||||


Robust Acc (%)


20 30

|Col1|Col2|Gauss( ) 0 0.05 0.25|
|---|---|---|
||||
|R: -0|.306||


Robust Acc (%)


45

40


45

40

35

30


77

75


80

75


73


35


20 25 30

|Po|s(b) 1|Col3|Col4|Col5|
|---|---|---|---|---|
||4 8||||
||||||
||||||
|R:|0.963||||


Robust Acc (%)


20 30 40 50

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|R: 0.629||||


Robust Acc (%)


Figure 9: Robustness and absolute transferability when we use Gaussian noise (Gauss) and posterize
(Pos) as data augmentations with different parameters.

D.2 RESULTS OF OTHER MODEL STRUCTURES


To further validate our evaluation results, we evaluate the experiments on another model structure.
We use a simpler CNN model for CIFAR-10 to SVHN and a more complicated WideResNet-50 for
ImageNet to CIFAR-10. The CNN model consists of four convolutional layer with 3 _×_ 3 kernels and
32,32,64,64 channels respectively, followed by two hidden layer with size 256. A 2 _×_ 2 max pooling
is calculated after the second and fourth layer. Other settings are the same as in the main text. Note
that in some settings the new model cannot converge, and therefore we will omit the result. In
addition, Jacobian regularization cannot be applied on WideResNet-50 because of the large memory
cost, so we do not include it in the figures. The results are shown in Figure 11, 12 and 13.


-----

ImageNet -> CIFAR10


ImageNet -> CIFAR10


80

75


90

80


15 20

|Col1|Col2|Bl|ur(k)|
|---|---|---|---|
||||1 5 11|
|||||
|R: -0|.557|||


Robust Acc (%)


0 10 20

|Col1|Col2|Rescale(m 1 2|
|---|---|---|
||4 8|4 8|
|R: -0.981|||


Robust Acc (%)


Figure 10: Robustness and absolute transferability when we use rescale and blur as data augmentations with different parameters.


CIFAR10 -> SVHN

5

16 18 20 22

|Col1|Col2|Col3|
|---|---|---|
|LLR( l) -0.1 0 0.1|||
||||
|R: -0.46|8||


Robust Acc (%)


ImageNet -> CIFAR10


CIFAR10 -> SVHN


ImageNet -> CIFAR10


24 28 32

|Col1|Col2|Col3|LLR( l) -0.0 0|
|---|---|---|---|
|||0.01 0.1|0.01 0.1|
|R: -|0.890|||


Robust Acc (%)


15 25 35

|Col1|L|LOT(||gs|| 0.01 0.1 1.0|2)|
|---|---|---|---|
|||||
|||||
|R:|-0.994|||


Robust Acc (%)


25 27

|Col1|Col2|LLOT(|||gs||2)|
|---|---|---|---|
||||1 2 5|
|||||
|R: -0.|977|||


Robust Acc (%)


10


Figure 11: Robustness and transferability for the other model structure when we control the norm
of last layer with last-layer regularization (LLR) and last-layer orthogonal training (LLOT) with
different parameters.

D.3 DATA AUGMENTATIONS THAT VIOLATE SUFFICIENT CONDITION


We study rotation and translation, the two data augmentations that violate the sufficient condition
for regularization. The result is shown in Figure 14. We observe that these augmentations do not
have an obvious impact on domain transferability.

D.4 ROBUSTNESS EVALUATION WITH AUTOATTACK


Besides PGD attack, we also evaluate the model robustness using the stronger AutoAttack. We
use APGD-CE, APGD-T and FAB-T as the sub-attacks in AutoAttack with 100 steps. Since the
accuracy will decrease after the stronger attack, we use a slightly smaller ϵ = 0.2 to better visualize
the trend. The results are shown in Fig. 15. We can observe that the trend is similar with what we
observed before when we used the PGD attack - domain generalization is an effect of regularization
and data augmentation, and it is sometimes negatively correlated with model robustness. Also,


CIFAR10 -> SVHN


CIFAR10 -> SVHN


ImageNet -> CIFAR10


20 40 60

|Col1|Col2|JR(|j)|
|---|---|---|---|
|||0 1 1 1|0 00 000|
|||||
|R: -0.832||||


Robust Acc (%)


10 20

|Col1|Col2|WD( w) 0.000|5|
|---|---|---|---|
|||0.001 0.005 0.01||
|||||
|||||
|R: -0.6|07|||


Robust Acc (%)


30

20

10


10


10

20


10 15 20 25

|Col1|Col2|WD( 0.0|w) 001|
|---|---|---|---|
|||0.0 0.0|005 01|
|||||
|||||
|R: -0.94|1|||


Robust Acc (%)


Figure 12: Robustness and transferability for the other model structure when we regularize the feature extractor with Jacobian Regularization (JR) and weight decay (WD) with different parameters.


-----

CIFAR10 -> SVHN


ImageNet -> CIFAR10


20 30 40

|Col1|CIFAR10|-> SVHN|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
|||Pos(|||
||||Pos(|b) 1|
|R|: 0.776|||4 8|


Robust Acc (%)


ImageNet -> CIFAR10


60

40

20


15

10


20 30 40 50

|Gauss( 0 0.0 0.2|) 5 5|Col3|Col4|Col5|
|---|---|---|---|---|
|1.0|||||
||||||
||||||
|R: 0.35|8||||


Robust Acc (%)


20 30 40

|Pos(b) 1|Col2|Col3|
|---|---|---|
|4 8|||
||||
|R: 0.472|||


Robust Acc (%)


20 30 40

|Col1|Col2|Col3|Gauss( ) 0 0.05 0.25|
|---|---|---|---|
|R:|-0.505|||
|||||


Gauss( )

0
0.05
0.25

R: -0.505

Robust Acc (%)


Figure 13: Robustness and transferability for the other model structure when we use Gaussian noise
(Gauss) and posterize (Pos) as data augmentations with different parameters.


CIFAR10 -> SVHN


ImageNet -> CIFAR10


10 15 20

|Col1|Col2|CIFAR10|-> SVHN|Col5|
|---|---|---|---|---|
||||||
||||||
||||Translate 0 0.0 0.1|(d) 5|
|R|: 0|.716|0.2|5|


Robust Acc (%)


ImageNet -> CIFAR10

17 20 23

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
|||Translate( 0 0.05 0.1|d)|
|R:|0.238|0.25||


Robust Acc (%)


20 25

|Col1|Col2|R|otate( ) 0|
|---|---|---|---|
||||15 45|
|: 0|.012|||


Robust Acc (%)


20 25

|Col1|Col2|Rotate( 0|)|
|---|---|---|---|
||15 45|15 45||
|R: 0.9|78|||


Robust Acc (%)


Figure 14: Relationship between robustness and transferability when we use rotation and translation
as data augmentations.

augmentations like rotation and translation, which violates the sufficient condition, do not improve
the domain generalization.


-----

CIFAR10 -> SVHN


CIFAR10 -> SVHN


CIFAR10 -> SVHN


CIFAR10 -> SVHN


10 15 20

|Col1|Col2|LLR( l|)|
|---|---|---|---|
|||-0. 0 0.1|1|
||1.0|1.0||
|||||
|: -0.846||||


Robust Acc (%)


10 15 20

|Col1|Col2|Col3|LLOT(||gs|||2)|
|---|---|---|---|---|
||||0.0 0.1 1.0 10.|1 0|
||||||
|R: -|0.|947|||


Robust Acc (%)


40

20


15

10


10


10

40

30

20

10


20 30 40 50 60 70

|J|R(|j)|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
||0 1 1|00 000||||||
|||||||||
|||||||||
|R:|0.3|16||||||


Robust Acc (%)

CIFAR10 -> SVHN


5 10

|Col1|Col2|W|D(|w)|
|---|---|---|---|---|
||||0.0005 0.001 0.005 0.01||
||||||
||||||
|R: -0.7|24||||


Robust Acc (%)


CIFAR10 -> SVHN

20 30 40 50 60 70

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|||||||||
|Ga|uss( 0 0.|) 05||||||
||0. 1.|25 0||||||
|||||||||
||||||R:|0.1|46|


Robust Acc (%)


CIFAR10 -> SVHN



5

0

20 30 40 50

|Pos(b) 1|Col2|Col3|Col4|
|---|---|---|---|
|4 8||||
|||||
|||||
|: 0.57|9|||


Robust Acc (%)


15 20

|Rotate( 0|)|Col3|
|---|---|---|
|15 45|||
||||
|R|: 0.439||


Robust Acc (%)


8 10 12

|Col1|CIFAR10|-> SVHN|Col4|
|---|---|---|---|
|||||
|||||
|||||
|||Translate( 0 0.05|d)|
|R: 0|.680|0.1 0.25||


Robust Acc (%)


Figure 15: Relationship between robustness and transferability on CIFAR-10 when we use AutoAttack to evaluate model robustness.


-----

