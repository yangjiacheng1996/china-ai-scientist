# DEPTH WITHOUT THE MAGIC: INDUCTIVE BIAS OF NATURAL GRADIENT DESCENT

**Anonymous authors**
Paper under double-blind review

ABSTRACT

In gradient descent, changing how we parametrize the model can lead to drastically different optimization trajectories, giving rise a surprising range of meaningful inductive biases: identifying sparse classifiers or reconstructing low-rank
matrices without explicit regularization. This implicit regularization has been hypothesised to be a contributing factor to good generalization in deep learning.
However, natural gradient descent is approximately invariant to reparameterization, it always follows the same trajectory and finds the same optimum. The
question naturally arises: What happens if we eliminate the role of parameterization, which solution will be found, what new properties occur? We characterize
the behaviour of natural gradient flow in deep linear networks for separable classification under logistic loss and deep matrix factorization. Some of our findings
extend to nonlinear neural networks with sufficient but finite over-parametrization.
We demonstrate that there exist learning problems where natural gradient descent
fails to generalize, while gradient descent with the right architecture peforms well.

1 INTRODUCTION


There is plenty of empirical evidence that the choice
of network architecture is an important determinant
of the success of deep learning (He et al., 2015;
Vaswani et al., 2017). The empirical observations
are now supported by theoretical work into the role
that parameter-to-hypothesis mapping plays in determining inductive biases of gradient-based learning.
Unregularized gradient descent can efficiently find
low-rank solutions in matrix completion problems
(Arora et al., 2019), sparse solutions in separable
classification (Gunasekar et al., 2018) or compressed
sensing (Vaˇskeviˇcius et al., 2019). Valle-P´erez et al.
(2018) studied deep neural networks and found evidence that the parameter-hypothesis mapping[1] is biased towards simpler functions as measured by Kolmogorov complexity. Taken together, these observations and findings have lead the community to hypothesize that


Figure 1: Illustration of parametrizationdependence of EGD and independence of
NGD. Consider two parameter spaces ( 1,
_W_
2) and two optimization trajectories in
_W_
each: one EGD, one NGD. If we map these
into the hypothesis space (H) then EGD finds
different optima, but NGD finds the same.


_The parameter-to-hypothesis mapping influences the inductive biases of gradient-based learning_
_and may play an important role in generalization._

In parallel to improving architectures, considerable research was done to improve optimization algorithms for deep learning, with a focus on faster convergence and robustness to hyperparameters.
Among the most advanced optimization methods are natural gradient descent (NGD) techniques.
An intuitive motivation for NGD is that it improves convergence by implicitly lifting the problem
from parameter-space, where the loss is non-convex and poorly behaved to the Riemannian manifold

1The mapping between the parameter space and the set of hypotheses as seen on Figure 1


-----

of hypotheses, where the loss is better behaved. From the perspective of inductive biases, the most
interesting aspect of NGD is its approximate invariance to reparametrization.

_Natural gradient descent eliminates the effect of parameter-to-hypothesis mapping._

These two observations invite questions about the nature of inductive biases in NGD as well as the
role of parametrization-dependence in generalization. The first, practical, implication is as follows:
if the parameter-to-hypothesis mapping really does play an important role in generalization, then
eliminating its influence on the optimization path may be undesirable, and consequently the pursuit
of implementing exact NGD in deep architectures may be counterproductive. Secondly, studying
the behaviour of NGD in various models and tasks may give us new insights about the importance
of parametrization, and could perhaps offer a way to experimentally or theoretically test hypotheses.

In this paper we study the inductive bias of natural gradient descent in deep linear models. These
models are particularly suited for our analysis because (a) efficient algorithms exist to calculate
exact natural gradients which is otherwise computationally intractable and (b) the inductive biases
of Euclidean gradient descent (EGD) in these models have been thoroughly studied and understood.

We make the following contributions:

-  In linear classification, we show that NGF is invariant under invertible transformations of
data (Theorems 1&2) and as a consequence it cannot recover the ℓp large margin solutions
that EGD tends to converge to.

-  We further show that (in case of separable classification) when the number of parameters
exceeds the number of datapoints, NGF interpolates training labels in a way similar to
ordinary least squares or ridgeless regression (Theorems 3&4).

-  We demonstrate experimentally that there exist learning problems where NGD can not
reach good generalization performance, while EGD with the right architecture can succeed.

-  To perform experiments, we extended the work of Bernacchia et al. (2018) to derive efficient and numerically stable algorithms for calculating exact natural gradients in diagonal
networks (Gunasekar et al., 2018) and deep matrix factorization (Arora et al., 2019).
Before stating our main theoretical and experimental results we review some relevant background
on parametrization-dependent implicit regularization and natural gradients.

2 BACKGROUND

2.1 SEPARABLE CLASSIFICATION WITH DEEP LINEAR MODELS

In this article we consider binary classification datasets (xn, yn), n = 1, . . ., N separable by a
_{_ _}_
homogeneous linear classifier with a positive margin ( i. e. ∃β[∗] s.t. ynx[⊤]n **_[β][∗]_** _[≥]_ [1][ ∀][n][). (We use the]
notation X = (x1 **xN** )[⊤]). In such situation β[∗] is not unique and there may be many separating
hyperplanes which all achieve · · · 0 training loss - it is up to the inductive biases of the learning algorithm to select one. Soudry et al. (2017) studied the dynamics of unregularized Euclidean gradient
descent on logistic loss and found that the iterate β(t) converges to the well-known ℓ2 large margin
classifier in direction, that is


limt→∞ _|ββ((tt))|_ [=] _|ββ[∗]ℓ[∗]ℓ22_ _[|][ where][ β]ℓ[∗]2_ [= arg min]β∈R[D][ ||][β][||][2][ s.t.][ y][n][x]n[⊤][β][ ≥] [1] _∀n._

Importantly, Gunasekar et al. (2018) later showed that this behaviour changes if the gradient descent
is performed on a different parametrization. In this paper we will focus on L-layer linear diagonal
networks (Gunasekar et al., 2018), where β = w1 **w2** _. . ._ **wL, using** to denote elementwise product. When we adjust parameters w1, . . ., ⊙ wL through Euclidean gradient descent, ⊙ _⊙_ _⊙_ **_β(t)_**
converges to the ℓ 2

_L_ [large margin separator defined as]


limt→∞ _|ββ((tt))|_ [=] _|ββ[∗]diag[∗]diag[|][ where][ β]diag[∗]_ [= arg min]β∈R[D][ ||][β][||][ 2]L [s.t.][ y][n][x]n[⊤][β][ ≥] [1] _∀n._

A remarkable consequence of this is that unregularized gradient descent can find sparse classifiers,
without any form of explicit regularization. In fact, this inductive bias is even more sparsity-seeking
than the typically used ℓ1 regularization (see e. g. Koh et al., 2007; Tibshirani, 1996). Figure 2
illustrates this behaviour in a 2D example.


-----

A: direct parametrization, EGD B: L = 4 diagonal network, EGD C: direct parametrization, NGD D:L = 4 diagonal network, NGD

Figure 2: Implicit regularization of EGD and NGD on logistic loss in separable classification.
EGD reaches different optima depending on parametrization: fully connected networks reach ℓ2
large margin (A), while L-layer linear diagonal networks reach the ℓ _L2_ [-large margin solution which]

favours sparsity (B), while L-layer linear diagonal networks reach the ℓ _L2_ [-large m. NGD converges]

to the same optimum irrespective of the parametrization (C, D).


2.2 MATRIX COMPLETION VIA DEEP MATRIX FACTORIZATION


EGF, EGD, NGF, NGD,


The task of matrix completion involves recovering an
unknown matrix β[∗] _∈_ R[D][×][D] from a randomly chosen subset of observed entries[2]. The problem is clearly
underdefined: there are infinitely many matrices that
match the observed entries. It is common to make additional assumptions about β[∗], most commonly that that
it has low rank, under which it becomes identifiable.

One approach to matrix completion under the low-rank
assumption is based on explicit regularization (e.g. nuclear norm) which leads to a convex optimization problem. Another common approach is matrix factorization
using an underparametrized representation β = UV
where the sizes of U ∈ R[D][×][R] and V ∈ R[R][×][D] are restricted to ensure β’s rank is at most R. Learning then
proceeds by minimizing the non-convex mean-squared
reconstruction error in U, V via gradient descent.


EGF,


EGD,


NGF,


NGD,


reconstruction error in U, V via gradient descent.

Figure 3: Illustration of the neural tangent

Remarkably, Gunasekar et al. (2017) showed that kernel in EGF, EGD, NGF and NGD (left
the gradient-based matrix factorization method tends _to right) in matrix factorization models of_
to converge to low-rank solutions even in the over- different depth (top to bottom). The alparametrized setting, i.e. when β = W1W2 where gorithms take gradient steps to minimise
_W1 and W2 are full square matrices, without any ex-_ the squared error on a single observation
plicit regularization. This was later extended by Arora at the middle of the matrix. Each panel
et al. (2019), who studied the deep matrix product shows how entries of the full 11 × 11
parametrization of the form β = W1W2 _WL. Arora_ matrix move from a random initial state.
et al. (2019) ran experiments for different matrix com- · · · When L ≥ 2, Euclidean gradient methpletion tasks varying initialization, depth and number of ods also move entries where there is no
observations and compared them to minimum nuclear observation - enabling implicit regularizanorm solution. When the number of observed entries tion towards low-rank solutions. By conis large gradient descent in deep matrix factorization trast, and due to invariance, natural gradimodels tended to the minimum nuclear norm solution. ent methods move only the single entry to
However, in the interesting case of fewer observed en- match the observation.
tries, the behaviour was different. Gradient descent preferred solutions with lower effective rank at the expense of higher nuclear norm. From the evolution
of the singular values of β they also concluded that the implicit regularization is towards low rank
that becomes stronger as depth grows.


2to simplify presentation we assume the matrices are square, but our arguments hold more generally.


-----

2.3 NATURAL GRADIENT DESCENT

In the next section we briefly introduce some notation and key properties of natural gradient descent
(NGD, Amari, 1997; Pascanu & Bengio, 2013). Intuitively, one can think of NGD as a gradient
descent method, but not in the Euclidean space (with the Euclidean metric) of parameters, but instead
on the Riemannian manifold of probabilistic models the parameters define (equipped with a different
metric). More specifically, let’s say that the parameter of interest is θ, where θ defines a probabilistic
model p(y|x, θ). We assume that we wish to minimize the log loss under this model, i. e. l(θ, x, y) =
_−_ log p(y|x, θ) and L(θ) = _n=1_ _[l][(][θ,][ x][n][, y][n][)][. Then, NGD is usually defined as]_

_θ(t + 1) = θ(t)_ _ηF_ (θ) _θ_ (θ), where (1)

[P][N] _−_ _[−][1]_ _∇_ _L_

_F_ (θ) = EX [EY |X;θ[∇θL(θ)∇θ[⊤][L][(][θ][)]]] (2)

is the average Fisher information matrix and η is the step size. In the above definition, EY _X;θ is_
taken over the distribution specified by θ, but distribution with respect to which the expectation | EX
is calculated can be arbitrarily chosen. In this article we use the empirical distribution of training
data, though other choices are possible (Pascanu & Bengio, 2013). We will also consider natural
gradient flow (NGF) the continuous limit of NGD, analogously defined as

_θ˙ =_ _F_ (θ) _θ_ (θ). (3)
_−_ _[−][1]_ _∇_ _L_

We also note, that F (θ) is not generally invertible, and indeed it will not be in some of the cases we
will consider. Therefore, it is more correct to define NGF as any trajectory θt which satisfies

_F_ (θ) θ[˙] = _θ_ (θ). (4)
_−∇_ _L_

The natural gradient direction is thus only unique within the eigenspace of F (θ). Of all natural
gradient directions, one common choice is to use the Moore-Penrose pseudoinverse of F :

_θ˙ =_ _F_ [+](θ) _θ_ (θ). (5)
_−_ _∇_ _L_

We have seen how in EGD, different parametrization of the same problem leads to drastically different trajectories and optima. However, NGD with infinitesimally small learning rate (i. e. NGF)
always follows the same trajectory in model-space and this finds the same optimum, irrespective of
how it is parametrized, provided that the parametrization is smooth and locally invertible. Below we
formally state this property Amari (1997), alongside a short proof for illustration.
**Statement (Invariance of NGF under reparametrization). Let w and θ be two parameter vectors**
_Jacobianrelated by the mapping J =_ _∂∂θwtt_ _[and (2)] θ =[ F] P[(][θ](w[t][)][ are both full rank for all]) and consider natural gradient flow in[ t][. If][ w][t][ follows natural gradient flow] w. Assume that (1) the_

_starting from w0 then θt = P(wt) follows NGF, i. e. it solves_ _θ[˙]t = −F_ (θt)[+]∇θt _L(X, θt)._

3 NATURAL GRADIENTS UNDER LOGISTIC LOSS ON SEPARABLE DATA

We have seen in Section 2.1 that when trained on separable data with the logistic loss EGD tends
to converge to large margin classifiers. To illustrate how NGD differs, we first prove an invariance
property which, as we will see, rules out large margin behaviour. We state this property separately
when N < D and when N ≥ _D in the theorems that follow. We denote the number of data points_
with N and the number of input features with D.
**Theorem 1. Let’s assume, that N < D, X is full rank and A is an invertible D × D matrix. Let**
**_βt = βt(X, y) be the trajectory of NGF and β[′]t_** [=][ β]t[(][XA][⊤][,][ y][)][ (the trajectory of NGF on data]
_XA[⊤]). Then Xβ = XA[⊤]β[′]_ _(with the assumption that β and β[′]_ _have equivalent initial conditions)._
_Proof sketch. We use the notation s = Xβ and s[′]_ = XA[⊤]β and prove that st = s[′]t[. The full proof]
can be found in Appendix C.1.
**Theorem 2. Let βt(X, y) be the trajectory of NGF and let A be a D** _D invertible transfor-_
_×_
_mation. If N ≥_ _D, X has full rank and we consider NGF on the transformed data XA[⊤], then_
_A[⊤]βt(XA[⊤], y) = βt(X, y) (with the assumption that β and β[′]_ _have equivalent initial condi-_
_tions)._
_Remark. When N ≥_ _D and X is full rank, the size of F_ (β) is D × D and its rank is D, therefore
the Fisher information matrix of β is invertible.


-----

_Proof sketch. First let’s say β[′]t_ [=][ β]t[(][XA][⊤][,][ y][)][ and][ v][⊤] [=][ β][′⊤][A][. Then we prove the following:]


**_β[′]_** (ynβ[′⊤]Axn) = A **_v_** (ynv[⊤]xn) and _F_ (β[′]) = AF (v)A[⊤]. (6)
_∇_ _L_ _∇_ _L_


Hence we get:


**_β˙_** _′ = F_ (β′)−1 **_β′_** (β′) and **_v˙ = F_** (v)[−][1] **_v_** (v). (7)
_∇_ _L_ _∇_ _L_

So if v and β[′] have the same initialization, then vt = β[′]t[. Full proof can be found in Appendix C.2.]

**Conclusion. Let ut(X, y) denote the trajectory of Xβt, which is the linear function β[⊤]t** **[x][ evaluated]**
_at each of the datapoints xn. Then ut(XA[⊤], y) = ut(X, y)._

_Proof._ _ut(XA[⊤], y) = XA[⊤]βt(XA[⊤], y) = Xβt(X, y) = ut(X, y)_

One special case of this invariance property is invariance to scaling the dimensions of input data
(when A is diagonal). Imagine we scale any dimension by a constant a, NGF counteracts it by
scaling the corresponding coordinate of β by a[−][1]. We see now why this rules out characterising
implicit regularization of NGD as minimizing non-data-dependent norms of β. In particular, it rules
out the ℓp large-margin behaviour we have seen in EGD.
_Remark. Let A be a D × D invertible transformation and let β[∗](X, y) be the ℓ2 large margin solu-_
tion,does not have the invariance property, namely there exists a dataset i.e. β[∗](X, y) = argmin ∥β∥2 subject to ynβ[⊤]xn ≥ 1 ∀n. Then the (X, y ℓ) and a transformation A2 large margin classifier
such that A[⊤]β[∗]t [(][XA][⊤][,][ y][)][ ̸][=][ β]t[∗][(][X,][ y][)][. We include a proof by counterexample in Appendix D.]

Having ruled out norm-based implicit regularization, it’s natural to consider other statistical methods
that exhibit invariance under invertible data transformations. One candidate is ridge-less regression
or ordinary least squares (OLS), whose parameter is given by the formula βOLS = (X _[⊤]X)[−][1]X_ _[⊤]y._
As it turns out, the connection between NGD in linear regression and the OLS estimate run deeper
than sharing this invariance property.
**Theorem 3. If N < D and X is full rank, if parameters βt of a linear model follow natural**
_gradient flow under logistic loss, the logits st = Xβt follow an asymptotically linear trajectory_
_with direction vector y._
_Remark. The Fisher information matrix w.r.t. β is F_ (β) = X _[⊤]D(β)X, where D(β) is diagonal_
with positive elements on the diagonal. We see, that rank(F ) = rank(X) ≤ _N_, so F is singular,
thus several NGF paths are possible. When rank(X) = N, β has D − _N degrees of freedom and_
we did describe β on N dimensions. That’s why we consider s instead of β.

This Theorem follows from the more general Theorem 4 which we will state later.

Informally, when we have more parameters than datapoints, NGD discovers a solution that interpolates the training labels y (encoded as −1s and +1s) perfectly just like ordinary least squares does
in this case. Furthermore, if one uses the Moore-Penrose pseudoinverse to calculate the descent
direction, i. e. Eqn. (5), then βt converges in direction to the OLS parameter.

In general cases, OLS interpolation and large-margin (LM) methods find qualitatively different solutions in classification tasks. While the LM solution is typically a linear combination of a small
subset of training data (the support vectors), in OLS all datapoints are support vectors. As shown in
(Hsu et al., 2020), under some conditions this difference disappears in the highly overparametrised
regime - when D > N log N . An implication of Theorem 3 is that this phenomenon, known as support vector proliferation, occurs in NGF when D > N . Thus there is a regime where NGF and EGF
find qualitatively different classifiers, with different generalisation properties (Hsu et al., 2020).

Theorem 3 provided useful in the context of linear models but it turns out it is relatively straightforward to extend this to a result which holds for non-linear overparametrized models as well.
_IfTheorem 4. wt follows natural gradient flow on the logistic loss with labels Let w ∈_ R[P] _, P ≥_ _N be the parameters of a classifier with logits y and the Jacobian s = s(X J; wt =) ∈∂∂wsRtt_ _[N][is]._

_of full rank, then st grows asymptotically linearly with direction vector y._
_Remark. If our network is linear J = X, so Theorem 3 is a special case of Theorem 4 indeed._
_Proof sketch. The main idea is that, since J is full rank, by parametrization invariance of NGD the_
trajectory of s is determined by the trajectory of the corresponding β.
**s˙ =** _F_ (s) **s** (s) (8)
_−_ _[−][1]_ _∇_ _L_


-----

D = 1000 classification, sparsity s = 20

|Col1|EGD, L = 1 NGD NGD (pop.) EGD, L = 2 EGD, L = 3 EGD, L = 4|Col3|
|---|---|---|



500 1000 1500 2000 2500

number of observations


200 400 600 800 1000

learned coefficients

EGD, L = 2

EGD, L = 1

NGD

index of input feature



100%

90%

80%

70%

60%

50%


10


dynamics of logits sn(t)/t = (t)[T]xn/t

positive datapoints
negative datapoints

0 5

time


Figure 4: NGD and EGD in a 1000 dimensional sparse classification task, where the ground truth
classifier has 20 non-zero components. Left: Test accuracy of EGD depends on parametrizaion.
When there are there are fewer datapoints than dimensions, EGD with 2 or 3-layer diagonal
parametrization can reach up to 90% accuracy. By contrast, when averaging the Fisher infromation on training samples (dotted line) NGD performs at chance level when N < D. It performs
worse than EGD even when N ≥ _D, or when using the population Fisher calculated on a much_
larger set of samples (dashed line). Middle: Under NGD, when N < D, logits of the model grow
linearly, proportional to the binary label. Right: Coefficient vector β learnt by EGD in different architectures and NGD when N = 2500: In the 2-layer diagonal network, corresponding to ℓ1 implicit
regularisation, β becomes sparse. In the 1-layer model, the solution is substantially less sparse, but
the overall structure is learnt. NGD fails to learn the sparse structure.

Then we can calculate F (s) which turns out to be diagonal, so we have N independent differential
equations. We solve them to get the result. The details of the proof can be found in the Appendix C.


3.1 EXPERIMENTS

In order to validate and illustrate our findings we have run two main simulations, with results presented in Figures 2 and 4. In both experiments we considered the direct parametrization β = w and
the diagonal parametrizationorder to run these experiments we needed to implement an efficient algorithm for computing natural β = w1 ⊙· · · ⊙ **wL (Gunasekar et al., 2018) for different depth L. In**
gradients in these models: naively calculating and then inverting the Fisher information matrix is
computationally inefficient and numerically unstable. We therefore developed an algorithm that exploits the structure in the Fisher information matrix, extending the work of Bernacchia et al. (2018)
for diagonal networks. The details of our algorithms can be found in Appendix B.2.

In Experiment 1 we illustrated EGD and NGD in a 2D toy classification dataset. Positive and negative classes were generated such that they are separable by the the axis-aligned separator, but there
exists a non-axis-aligned separator with a higher margin. Based on the findings of Gunasekar et al.
(2018) we expected EGD to find the large margin solution when L is low, and the axis-aligned solution when L is sufficiently large. The results in panels a and b of Figure 2 confirm these predictions.
Figure 2c-d illustrate the parametrization-independence of NGD: it converges to the same solution
irrespective of parametrization. The solution is different from both the EGD solutions.

In Experiment 2 we focused on generalization performance. We generated a 1000-dimensional
dataset with standard Gaussian X, and a sparse ground-truth separator whose first 20 components
were set to 1, the rest were 0. Methods with explicit or implicit regularization towards sparse solutions should enjoy good generalization even when N < D. Confirming our expectations, we
observed that EGD in diagonal parametrizations (L = 2, L = 3) performed best on this task. The
deeper diagonal model (L = 4) was on par with the shallow solution, we expect that our 2 million EGD steps were simply not long enough for the implicit regularization to kick in (Moroshko
et al., 2020). The NGD solution on the other hand completely fails to generalize when N < D and
does relatively poorly even as N > D. This catastrophic performance is remedied by averaging
the Fisher information on a larger dataset - i. .e. using the population Fisher (Amari et al., 2020),
but even this variant of NGD fails to match the performance of EGD. The middle panel of Figure 4
validates the predictions of Theorem 4: logits from the model converge to ty. Finally, the right-hand
panels of Figure 4 show that NGD was unable to identify the sparse structure, which the diagonal
model infers best, and even the shallow model approximately finds.


-----

70

60

50

40

30

20

10


1000 1250 1500 1750 2000 2250 2500

EGD L=1
NGD
EGD L=2
EGD L=3

number of observations


1000 1200 1400 1600 1800 2000 2200 2400

L = 2, init from L = 2

20 L = 2, init from L = 4

L = 2, init from L = 6

EGD L=1 15 LL = 3, init from = 3 init from LL = 6 = 3
NGD
EGD L=2 10

test loss

EGD L=3

5

0

number of observations


1000 1250 1500

number of observations


Figure 5: Performance of unregularized EGD and NGD in rank-5 matrix completion tasks using
different architectures. Left and Middle: Using deep matrix product parametrizations with L ≥ 2
layers, EGD can reach low training error and identify low-rank solutions even when the number of
observations is small. By contrast, NGD in the same problem works similarly to EGD in the naive
parametrization and fails to generalize completely. Right: 2 (orange) and 3 (green) layer models
were initialized by collapsing randomly initialized deeper models to test the effect of initialization
separately from the effect of EGD dynamics. Initialization plays a negligible role in the inductive
bias of EGD in deep matrix factorization.

4 MATRIX COMPLETION WITH NATURAL GRADIENT DESCENT


As we have seen in Section 2.2, EGD in the deep matrix product parametrization β = W1 _WL_
converges to low-rank solutions. However, when L = 1, i.e. when we run EGD directly on · · · β, the
solution we find is trivial: entries of β where we have observation will converge to the observed
value, while other entries won’t move. Due to parameter-invariance, NGD cannot differentiate between parametrizations of different depth, it is natural to expect that it will fail the same way as EGD
does when L = 1. Let’s look at NGD in matrix completion.

In matrix completion we minimize the squared reconstruction error, which corresponds to the log
loss in an isotropic Gaussian observation model with β as mean. In a Gaussian model, the Fisher
Information Matrix of β becomes F (β) = _σ1n[2]_ _[I][, where][ σ]n[2]_ [is the observation noise. The observation]

noise σn[2] [is assumed a constant, and is inconsequential here as it cancels with the] _σ1n[2]_ [term in the log]

loss. Consequently, without loss of generality, we can consider F (β) the identity.

**Statement. Let’s apply NGF for the problem of matrix completion. EGF in the direct parametriza-**
_tion (β = w) is equivalent to NGF under any parametrization θ for which J =_ _[∂]∂θ[w]t[t]_ _[is full rank.]_

The proof of the statement can be found in Appendix E. This implies that NGF will completely fail
to generalize, i. e. make an accurate prediction of any unobserved entry of the matrix.

Figure 3 illustrates the key property of the dynamics which allows EGD to generalize in deeper
parametrizations. Each panel shows values of the neural tangent kernel (NTK) (Jacot et al., 2018),
its equivalent object for NGF called the natural NTK (Rudner et al., 2019), or their discretized
versions. For matrix factorization the NTK K (θ) is a (D × D) × (D × D) tensor which depends
on the parameters θ where ki, j, k, l(θ) measures how much the entry βi,j moves in reaction to a
negative loss gradient w.r.t. βk,l. In these visualizations, we set D = 11, and we plot the heatmap
of ki,j,5,5. We can see that when we parametrize β directly, the NTK is simply the identity, only
the entry β5,5 moves. However, when L = 2, EGD can now respond to the gradient signal at β5,5
by moving entries in the fifth row of W1 or in the fifth column of W2. This, in turn, might result in
moving βi,5 or β5,i as well. This explains the cross pattern seen in Figure 3 first panel in the second
row. This non-identity NTK is what allows generalization to happen as ’information flows’ from
observations to unobserved entries of β. However, in NGF, the natural NTK remains the identity
irrespective of parametrization. This is true even in the approximately invariant NGD.

For our Matrix Factorization experiments we had to develope a scalable and numerically stable
algorithm for computing the natural gradient. We did this by extending the algorithm of Bernacchia et al. (2018) to matrix factorization. Exploiting the structure of the Jacobian in the deep
matrix product parametrization (β = W1 _WL) we calculate the natural gradient w.r.t. Wl as_
_∇˜_ _Wl_ _L =_ _L1_ _[B]l[⊤]+ ˜∇βLA[+]l_ [, where][ A][l][ =][ Q] · · ·i[l][−]=1[1] _[W][i][ and][ B][l][ =][ Q]i[L]=l+1_ _[W][i][. We note that][ A][i][ and]_


-----

_Bi are matrices that are readily computed during the forward and backward pass of reverse-mode_
automatic differentiation of the loss. The details of the derivation can be found in Appendix B.4.

Using this algorithm, in Figure 5 we experimentally verify that NGD finds a trivial optimum in
deep matrix product parametrizations of varying depth. We follow the experimental setup of Arora
et al. (2019) and reproduce their results for EGD. We performed an extensive grid search of hyperparameters and found no setting where NGD would achieve non-trivial performance.

5 SUMMARY AND DISCUSSION
Inductive biases of gradient-based learning are driven to a large extent by the way we parametrize
our hypothesis. Natural gradient descent (NGD), on the other hand, ignores the parametrization
and implicitly optimizes over the manifold of hypothesis. This invited the question whether NGD
exhibits any of the useful implicit regularization that EGD has been shown to have. We characterized
the behaviour of NGD over logistic loss, and found that in the overparametrized regime, NGD
converges to the ordinary least squares interpolant of training labels. This is in contrast with the
large-margin-type behaviour EGD exhibits. In experiments we found that in the models we studied,
NGD fails to generalize as well as EGD with the right parametrization.
5.1 OTHER RELATED WORK
**Approximate NGD algorithms: Since exact NGD is computationally prohibitive, a great deal of**
research has been devoted to developing approximate NGD algorithms for deep leaning: K-FAC
Grosse & Martens (2016); Martens & Grosse (2015) exploits the approximately Kronecker structure of the Fisher information matrix, while, while Bernacchia et al. (2018) start from exact gradient
descent in linear neural networks and then apply the formula verbatim to the non-linear case. Another line of work aims at improving the invariance properties of NGD algorithms bringing them
closer to ideal of NGF (Song et al., 2018; Luk & Grosse, 2018). Our motivation differs in that are
not focused on designing better NGD algorithms, instead we raise the question whether closer approximation of NGF is desirable in the first place. In order to perform experiments that validate our
findings we develop efficient exact natural gradient descent algorithms in overparametrized linear
models extending the work of Bernacchia et al. (2018).

**Convergence Rates for NGD: The main reason for using NGD in deep learning is the intuitive**
notion it might speed up convergence by virtue of being invariant to parametrization (Amari, 1997;
Pascanu & Bengio, 2013; Martens, 2014). This intuition is backed up by theory: Amari (1998)
proved fast convergence on a quadratic loss; Bernacchia et al. (2018) proved fast convergence for
deep linear models under quadratic loss; more recently, Zhang et al. (2021) gave a proof of fast
convergence which holds for a broad class of overparametrized networks and also extends to KFAC; Rudner et al. (2019) analysed NGD in the neural tangent kernel (NTK) regime. Our work
differs in that our primary interest is not whether NGD converges fast, but to better understand and
illustrate possible trade-offs between fast convergence and generalization.

**Generalization of NGD: Wilson et al. (2017) were the first to propose that faster convergence**
may come at the cost of diminished generalization performance in deep learning. Much like our
work, Wilson et al. (2017) provided illustrative examples where different methods reach qualitatively
different solutions. They focused on adaptive learning rate algorithms like Adam, but due to the
connections between Adam and the empirical Fisher information, one might speculate that their
findings would extend to NGD as well Zhang et al. (2019) argued against the notion that NGD
may not generalize well, and supported their argument with a generalization bound which holds for
both NGD and EGD. However, generalization bounds often fail to predict the empirically observed
performance of deep learning (Jiang et al., 2019, see e. g.). In a setting most closely resembling
our work Amari et al. (2020) studied generalisation of preconditioned GD for minimising squared
loss and found that the optimal preconditioner depends on several factors: EGD generalises better
for clean labels, but in scenarios like misspecification or when the labels are noisy, NGD may have
an advantage. Finally, Wadia et al. (2021) argued that second order information of the input data which some second-order optimisation methods can’t utilize well, is key to good generalisation in
some neural network architectures. This general connection is related to our Theorems 1 and 2.

5.2 Q&A
_Q: How about stochastic gradients? Following Gunasekar et al. (2017; 2018); Arora et al. (2019) we_
analysed only full-batch gradient descent. This allowed us to prove properties of gradient flow, i. e. in


-----

the limit of infinitesimally small learning rates, which is not a meaningful limit in SGD. This line of
work demonstrates that useful inductive biases exist in gradient-based learning even in the absence
of gradient noise. Indeed, recent empirical evidence suggests that stochasticity may not be necessary
for good generalization in deep networks (see e. g. Geiping et al., 2021). In practice, we expect the
question of generalization to be complex, with multiple factors like stochasticity or parametrizationdependence playing a role. We propose that analysing NGD is a useful tool in understanding this
complex interplay, as it acts as a form of ablation by eliminating parametrization-dependence.

_Q: Does this mean NGD does not generalize well? Not necessarily. We show that there are cases_
where it does not, but it is possible that in other situations the inductive biases of NGD are more
helpful than those of EGD + parametrization, especially when trained on large data. Intuitively,
our theorems suggest that NGD may be too efficient at minimising the training loss at the cost of
poorer generalisation. However, in our experiments we saw that averaging the Fisher information
matrix over test data may remedy this, which would be in line with the practical recommendation of
Pascanu & Bengio (2013). Empirical evidence for generalization in exact NGD is sparse due to the
computational cost. Some works report good test performance using approximate methods (Grosse
& Martens, 2016; Bernacchia et al., 2018) or small models (Pascanu & Bengio, 2013), but since the
focus in these works was on demonstrating the usefulness of new methods, it is questionable how
thorough these comparisons were. Zhang et al. (2019); Amari et al. (2020) studied generalisation
of natural gradient methods theoretically in limited settings and provided some empirical evidence
to support their claims. A systematic empirical investigation similar to (Wilson et al., 2017) may be
more informative on this question.

_Q: Does initialization play a role? Changing the parametrization may influence generalisation in_
at least three ways: (1) initialization, (2) training dynamics, and (3) constraining the hypothesis
space. As weights are often initialized from a parameter-wise independent distribution, these may
give rise to a non-trivial and parameter-dependent initial distribution in hypothesis-space. ValleP´erez et al. (2018) argued that in deep networks, this manifests as a form of simplicity bias. In
our models, initialisation has a simlicity bias, too: if matrices W1, . . ., WL are drawn from an
isotropic Gaussian, their product β will be effectively low-rank with an increasing probability as
_L increases. By replacing EGD by NGD, we only eliminate the influence of parametrization on_
training dynamics, but the effects of initialization remain. It is therefore important to disentangle
relative importance of initialisation (1), and parameter-dependent dynamics (2). To this end, we
designed a set of additional experiments, where we controlled the effect of initialization separately
from the effects through dynamics. We initialised deep matrix factorisation models by drawing each
component matrix W1 as a product of independent Gaussian matrices, then ran EGD. Thus, we
were able to create models behaving like a L = 6 layer model at initialization but L = 2 layer
model during training. We found that the effect of initialization on generalization performance was
negligible compared to the effects of training dynamics (Figure 5.c), at least in deep linear models.
We further note that initialization plays a very important role in the limit of infinitely wide networks,
too, where initialization scale determines whether the network behaves like a linear kernel machine,
or more like the behaviour we describe in finite networks here (Woodworth et al., 2020).

_Q: What if you calculate Fisher information on test data? Pascanu & Bengio (2013) noted that in_
deep learning, averaging the Fisher information over test data, rather than training data seemingly
improves performance. In our theorems and experiments we assume averaging over the training
data, sometimes referred to as the sample Fisher information (see e. g. Amari et al., 2020) as this
makes our proofs tractable. In our high-dimensional sparse classivication experiment in Figure 4
we tested the performance of NGD when the Fisher information is averaged over a large number
of samples, called the population Fisher, and we found that generalisation performance improved,
but still did not match that of EGD, especially when sparsity-inducing diagonal parametrisations are
used.

_Q: What about other forms of natural gradients? In addition to the Fisher-Rao natural gradients that_
we consider here, there are other forms of natural gradients, such as those based on the Wasserstein
metric (Li & Montufar, 2018; Arbel et al., 2019). When considering this broader family of natural
gradient descent, it is natural to ask if the choice of metric may give rise to different inductive biases
in NGD similarly to how different parametrizations effect EGD differently. We think this is a fertile
area for future research.


-----

REPRODUCIBILITY STATEMENT

Python code to reproduce our results (including all Figures except Figure 1) can be found in the
following (anonymized) git repository which contains unit tests and documentation:
[https://anonymous.4open.science/r/deeplinear-2F10](https://anonymous.4open.science/r/deeplinear-2F10)

REFERENCES

Shun-ichi Amari. Neural learning in structured parameter spaces - natural riemannian gradient. In
M. C. Mozer, M. Jordan, and T. Petsche (eds.), Advances in Neural Information Processing Sys_[tems, volume 9. MIT Press, 1997. URL https://proceedings.neurips.cc/paper/](https://proceedings.neurips.cc/paper/1996/file/39e4973ba3321b80f37d9b55f63ed8b8-Paper.pdf)_
[1996/file/39e4973ba3321b80f37d9b55f63ed8b8-Paper.pdf.](https://proceedings.neurips.cc/paper/1996/file/39e4973ba3321b80f37d9b55f63ed8b8-Paper.pdf)

Shun-ichi Amari. Natural gradient works efficiently in learning. Neural Computation, 10:251–276,
[2 1998. ISSN 0899-7667. doi: 10.1162/089976698300017746. URL http://direct.mit.](http://direct.mit.edu/neco/article-pdf/10/2/251/813415/089976698300017746.pdf)
[edu/neco/article-pdf/10/2/251/813415/089976698300017746.pdf.](http://direct.mit.edu/neco/article-pdf/10/2/251/813415/089976698300017746.pdf)

Shun-ichi Amari, Jimmy Ba, Roger Grosse, Xuechen Li, Atsushi Nitanda, Taiji Suzuki, Denny Wu,
and Ji Xu. When does preconditioning help or hurt generalization? [6 2020. URL https:](https://arxiv.org/abs/2006.10732v4)
[//arxiv.org/abs/2006.10732v4.](https://arxiv.org/abs/2006.10732v4)

Michael Arbel, Arthur Gretton, Wuchen Li, and Guido Montufar. Kernelized wasserstein natural
[gradient. 10 2019. URL https://arxiv.org/abs/1910.09652v4.](https://arxiv.org/abs/1910.09652v4)

Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
[factorization. Advances in Neural Information Processing Systems, 32, 5 2019. URL http:](http://arxiv.org/abs/1905.13655)
[//arxiv.org/abs/1905.13655.](http://arxiv.org/abs/1905.13655)

Alberto Bernacchia, M´at´e Lengyel, and Guillaume Hennequin. Exact natural gradient in deep linear
networks and its application to the nonlinear case. Advances in Neural Information Processing
_[Systems, 31, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/](https://proceedings.neurips.cc/paper/2018/hash/7f018eb7b301a66658931cb8a93fd6e8-Abstract.html)_
[7f018eb7b301a66658931cb8a93fd6e8-Abstract.html.](https://proceedings.neurips.cc/paper/2018/hash/7f018eb7b301a66658931cb8a93fd6e8-Abstract.html)

Donald W. Fausett and Charles T. Fulton. Large Least Squares Problems Involving Kronecker
Products. SIAM Journal on Matrix Analysis and Applications, 15(1), 1994. ISSN 0895-4798.
doi: 10.1137/s0895479891222106.

Jonas Geiping, Micah Goldblum, Phillip E. Pope, Michael Moeller, and Tom Goldstein. Stochas[tic training is not necessary for generalization. 9 2021. URL https://arxiv.org/abs/](https://arxiv.org/abs/2109.14119v1)
[2109.14119v1.](https://arxiv.org/abs/2109.14119v1)

Roger Grosse and James Martens. A kronecker-factored approximate fisher matrix for convolution
layers. 33rd International Conference on Machine Learning, ICML 2016, 2:851–874, 2 2016.
[URL https://arxiv.org/abs/1602.01407v2.](https://arxiv.org/abs/1602.01407v2)

Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Srebro. Implicit regularization in matrix factorization. Advances in Neural Information Process_[ing Systems, 2017-December:6152–6160, 5 2017. URL https://arxiv.org/abs/1705.](https://arxiv.org/abs/1705.09280v1)_
[09280v1.](https://arxiv.org/abs/1705.09280v1)

Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Implicit bias of gradient descent
on linear convolutional networks. Advances in Neural Information Processing Systems, 2018[December:9461–9471, 6 2018. URL http://arxiv.org/abs/1806.00468.](http://arxiv.org/abs/1806.00468)

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern
_[Recognition, 2016-December:770–778, 12 2015. URL https://arxiv.org/abs/1512.](https://arxiv.org/abs/1512.03385v1)_
[03385v1.](https://arxiv.org/abs/1512.03385v1)

Daniel Hsu, Vidya Muthukumar, and Ji Xu. On the proliferation of support vectors in high dimen[sions. 9 2020. URL https://arxiv.org/abs/2009.10670v1.](https://arxiv.org/abs/2009.10670v1)


-----

Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. Advances in Neural Information Processing Systems, 2018[December:8571–8580, 6 2018. URL https://arxiv.org/abs/1806.07572v4.](https://arxiv.org/abs/1806.07572v4)

Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic generalization measures and where to find them. In International Conference on Learning
_Representations, 2019._

Kwangmoo Koh, Seung-Jean Kim, Stephen Boyd, and Yi Lin. An interior-point method for largescale 1-regularized logistic regression. Journal of Machine Learning Research, 8:1519–1555,
2007.

Wuchen Li and Guido Montufar. Natural gradient via optimal transport. Information Geometry, 1:
[181–214, 3 2018. URL https://arxiv.org/abs/1803.07033v5.](https://arxiv.org/abs/1803.07033v5)

Kevin Luk and Roger Grosse. A coordinate-free construction of scalable natural gradient. 2018.
[URL https://arxiv.org/abs/1808.10340v1.](https://arxiv.org/abs/1808.10340v1)

James Martens. New insights and perspectives on the natural gradient method. Journal of Machine
_[Learning Research, 21:1–76, 12 2014. URL https://arxiv.org/abs/1412.1193v11.](https://arxiv.org/abs/1412.1193v11)_

James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. 32nd International Conference on Machine Learning, ICML 2015, 3:2398–2407, 3
[2015. URL https://arxiv.org/abs/1503.05671v7.](https://arxiv.org/abs/1503.05671v7)

Edward Moroshko, Suriya Gunasekar, Blake Woodworth, Jason D. Lee, Nathan Srebro, and
Daniel Soudry. Implicit bias in deep linear classification: Initialization scale vs training accuracy. Advances in Neural Information Processing Systems, 2020-December, 7 2020. URL
[https://arxiv.org/abs/2007.06738v1.](https://arxiv.org/abs/2007.06738v1)

Razvan Pascanu and Yoshua Bengio. Revisiting natural gradient for deep networks. 1st International
_Conference on Learning Representations, ICLR 2013 - Workshop Track Proceedings, 1 2013._
[URL https://arxiv.org/abs/1301.3584v7.](https://arxiv.org/abs/1301.3584v7)

Tim GJ Rudner, Florian Wenzel, Yee Whye Teh, and Yarin Gal. The natural neural tangent kernel:
Neural network training dynamics under natural gradient descent. In 4th workshop on Bayesian
_Deep Learning (NeurIPS 2019), 2019._

Yang Song, Jiaming Song, and Stefano Ermon. Accelerating natural gradient with higher-order
invariance. 35th International Conference on Machine Learning, ICML 2018, 11:7491–7514, 3
[2018. URL https://arxiv.org/abs/1803.01273v2.](https://arxiv.org/abs/1803.01273v2)

Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The
implicit bias of gradient descent on separable data. _6th International Conference on Learn-_
_ing Representations, ICLR 2018 - Conference Track Proceedings, 19:1–57, 10 2017._ URL
[https://arxiv.org/abs/1710.10345v4.](https://arxiv.org/abs/1710.10345v4)

Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Sta_tistical Society: Series B (Methodological), 58:267–288, 1 1996._ ISSN 2517-6161. doi:
10.1111/J.2517-6161.1996.TB02080.X. [URL https://onlinelibrary.wiley.com/](https://onlinelibrary.wiley.com/doi/full/10.1111/j.2517-6161.1996.tb02080.x)
[doi/full/10.1111/j.2517-6161.1996.tb02080.x.](https://onlinelibrary.wiley.com/doi/full/10.1111/j.2517-6161.1996.tb02080.x)

Guillermo Valle-P´erez, Chico Q. Camargo, and Ard A. Louis. Deep learning generalizes because
the parameter-function map is biased towards simple functions. 7th International Conference
_[on Learning Representations, ICLR 2019, 5 2018. URL https://arxiv.org/abs/1805.](https://arxiv.org/abs/1805.08522v5)_
[08522v5.](https://arxiv.org/abs/1805.08522v5)

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in Neural Informa_[tion Processing Systems, 2017-December:5999–6009, 6 2017. URL https://arxiv.org/](https://arxiv.org/abs/1706.03762v5)_
[abs/1706.03762v5.](https://arxiv.org/abs/1706.03762v5)


-----

Tomas Vaˇskeviˇcius, Varun Kanade, and Patrick Rebeschini. Implicit regularization for optimal
sparse recovery. _Advances in Neural Information Processing Systems, 32, 9 2019._ URL
[https://arxiv.org/abs/1909.05122v1.](https://arxiv.org/abs/1909.05122v1)

Neha S Wadia, Daniel Duckworth, Samuel S Schoenholz, Ethan Dyer, and Jascha Sohl-Dickstein.
Whitening and second order optimization both make information in the dataset unusable during
training, and can reduce or prevent generalization. 2021.

Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht. The
marginal value of adaptive gradient methods in machine learning. Advances in Neural Informa_[tion Processing Systems, 2017-December:4149–4159, 5 2017. URL https://arxiv.org/](https://arxiv.org/abs/1705.08292v2)_
[abs/1705.08292v2.](https://arxiv.org/abs/1705.08292v2)

Blake Woodworth, Suriya Gunasekar, Jason D Lee, Edward Moroshko, Pedro Savarese, Daniel
Soudry, Nathan Srebro, Jacob Abernethy, and Shivani Agarwal. Kernel and rich regimes
in overparametrized models. volume 125, pp. 3635–3673. PMLR, 7 2020. [URL https:](https://proceedings.mlr.press/v125/woodworth20a.html)
[//proceedings.mlr.press/v125/woodworth20a.html.](https://proceedings.mlr.press/v125/woodworth20a.html)

Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning (still) requires rethinking generalization. Communications of the ACM, 64:107–
[115, 3 2021. ISSN 15577317. doi: 10.1145/3446776. URL https://dl.acm.org/doi/](https://dl.acm.org/doi/abs/10.1145/3446776)
[abs/10.1145/3446776.](https://dl.acm.org/doi/abs/10.1145/3446776)

Guodong Zhang, James Martens, and Roger Grosse. Fast convergence of natural gradient descent
for overparameterized neural networks. Advances in Neural Information Processing Systems, 32,
[5 2019. ISSN 10495258. URL https://arxiv.org/abs/1905.10961v2.](https://arxiv.org/abs/1905.10961v2)

A USEFUL LEMMAS

We will need the following lemma in the proof of Theorem 1,4.
**Lemma 1. If we solve a separable classification problem with natrual gradient flow with separator**
**_β and output s, then the gradient and the Fisher information matrix are the following (in case of_**
_linear network this means s = Xβ):_

[∇sL(s)]i = −yi(1 − _φ(yisi))_ (9)

[F (s)]i,j = δi,jφ(si)(1 − _φ(si)_ (10)


1 _e[−][u]_

1+e[−][u][ and][ φ][(][−][u][) = 1][ −] _[φ][(][u][) =]_ 1+e[−][u][ .]


_Proof. First note that φ(u) =_


_N_

log(1 + e[−][y][n][s][n] ) = (11)

1 +[−][y][i] e[e][−][−][y][y][i][i][s][s][i][i][ =][ −][y][i][(1][ −] _[φ][(][y][i][s][i][))]_

_n=1_

X



[ **s** (s)]i = _[∂][L]_ = _[∂]_
_∇_ _L_ _∂si_ _∂si_


Using Equation (11) we get the following:

[F (s)]i,j = [Ey[ **s** (s) **s** [= [][E][y][[][y][i][(1][ −] _[φ][(][y][i][s][i][))][y][j][(1][ −]_ _[φ][(][y][j][s][j][))]]][i,j]_ [=]
_∇_ _L_ _∇[⊤][L][(][s][)]]][i,j]_

= Eyi [yi(1 Eφ(yy[(1isi −))]φE(yyji[syij))(1[2]] _φ(yjsj))]_ _ifif_ _ii == j j_ [=]

( _−_ _−_ _̸_

= Eyi [yi(1 _φφ(s(yi)(1i))] −Eyφj_ [(ysji(1)) _φ(yj))]_ _ifif_ _ii == j j_

( _−_ _−_ _̸_

Now we get the following:


(12)


Eyi [yi(1 _φ(yisi))] = φ(si)(1_ _φ(si))_ (1 _φ(si))(1_ _φ(_ _si)) = 0_ (13)
_−_ _−_ _−_ _−_ _−_ _−_

Hence we get:

[F (s)]i,j = δi,jφ(si)(1 − _φ(si))._ (14)


-----

The next lemma is essential in all computation connected to matrix completion with matrix factorization.
**Lemma 2. If we assume that the product matrix β comes from a Gaussian distribution with fixed**
_σnI standard deviation and µ mean, then the Fisher information matrix of the product matrix in_
_matrix factorization is F_ (β) = _σ1n[2]_ _[I][.]_

_Proof. Because of the assumption:_

_p(X_ _θ) =_ (X _µ, σnI),_ (15)
_|_ _N_ _|_

where θ is the parameters of the model (µ, σn).

_∇θ log p(X|θ) = ∇θ log (_ _σn√1_ 2π [e]− [(][X]2[−]σn[2][µ][)2] ) = ∇µ(log( _σn√1_ 2π [)][ −] [(][X]2[ −]σn[2][µ][)][2] ) = [(][X]σ[ −]n[2] _[µ][)]_ _,_

(16)
therefore we can compute the Fisher as

_F_ (β) = EX _p(X_ _θ)_ _θ log p(X_ _θ)_ _θ_ [log][ p][(][X][|][θ][)] = EX _p(X_ _θ)_ (X − _µ)(X −_ _µ)⊤_ = _[σ]n[2]_ _[I]_ = [1]
_∼_ _|_ _∇_ _|_ _∇[⊤]_ _∼_ _|_ _σn[4]_ _σn[4]_ _σn[2]_

 

  (17)


B EXACT NATURAL GRADIENTS IN LINEAR MODELS

B.1 SIMPLE LINEAR MODEL LOGISTIC LOSS

To obtain the natural gradient [˜] **_β_** with respect to β, we have to solve the following linear system:
_∇_ _L_

_F_ (β) [˜] **_β_** = **_β_** _,_ (18)
_∇_ _L_ _∇_ _L_

where F (β) is the Fisher information matrix and **_β_** is the (Euclidean) gradient. Under the logistic
_∇_ _L_
loss the Fisher information matrix becomes

_F_ (β) = X _[⊤]_ diag[φ(Xβ) ⊙ _φ(−Xβ)]X,_ (19)

where φ is the logistic sigmoid which is applied elementwise to vector arguments and ⊙ denotes
elementwise product. The gradient of the logistic loss is as follows:

**_β_** = (y _X)[⊤]φ(_ (y _X)β)_ (20)
_∇_ _L_ _−_ _⊙_ _−_ _⊙_

Mathematically, we could use these expressions and solve the linear system Equation (18), however,
this would be potentially numerically unstable for reasons outlined below. Let’s introduce the notation _X[˜] = y ⊙_ _X and u = X[˜]_ **_β to simplify the formulæ. Due to symmetry, in the Fisher information_**
all occurrences of X can be replaced by _X[˜]_ . This gives rise to the following expressions for the
Fisher information matrix:
_F_ (β) = X[˜] _[T]_ diag[φ(u) ⊙ _φ(−u)] X[˜]_ (21)
and the gradient:
**_β_** = _Xφ(_ _u)._ (22)
_∇_ _L_ _−_ [˜] _−_
As the classifier gets better, components of u increase and diverges to +∞. As a consequence both
_F_ (β) and **_β_** are expected to become small, from the term φ( _u). This could lead to issues with_
_∇_ _L_ _−_
numerical stability. To solve this, we rewrite both using following identity:


1 _e[−][u]_

(23)

1 + e[u][ =] 1 + e[−][u][ =][ e][−][u][φ][(][u][)]


_φ(−u) =_


obtaining:


_F_ (β) = e[−][u][max][ ˜]X _[T]_ diag[e[−][u][+][u][max] _φ[2](u)] X[˜]_ (24)

**_β_** = _e[−][u][max][ ˜]Xe[−][u][+][u][max]_ _φ(u),_ (25)
_∇_ _L_ _−_

where umax is the largest entry of u. We have thus isolated the term responsible for poor numerical
performance into a multiplicative term e[−][u][max] which we can simply leave out when solving the linear system. The remaining terms are well-behaved even as u increases, provided that the difference
between elements of u is not too large.


-----

B.2 DIAGONAL LINEAR NETWORK UNDER LOGISTIC LOSS

In a diagonal linear network we express β = w1 **wL. Here we will discuss how we compute**
the natural gradient with respect to wl. _⊙· · ·⊙_

We now solve the following (underdetermined) system of linear equations, which we write using
using Einstein summation notation:

_F_ (β)i,jJj,l,k [˜] **wl,k** = **_βi_** _,_ (26)
_∇_ _L_ _∇_ _L_

where Jj,l,k = _∂∂wβl,kj_ [is the Jacobian of the mapping from][ w][ to][ β][. In this specific parametrization,]

most entries of J is non-zero. Let’s denote the product of the first l − 1 weight vectors as al and the
product of the last L − _l −_ 1 weight vectors as bl so we can have:

**_βi = w1,i_** **wl** 1,i **wl,i wl+1,i** **wL,i** = al,iwl,ibl,i. (27)
_· · ·_ _−_ _· · ·_
**al,i** **bl,i**

Thus, the Jacobian becomes:| {z } | {z }

_Ji,l,k =_ **al,ibl,i** if i = j (28)
 0 if i ̸= j

Substituting this back, we have to solve the following system of equations:

_F_ (β)i,jJj,l,k [˜] _wl,k_ = **_βi_** (29)
_∇_ _L_ _∇_ _L_

_F_ (β)i,jal,jbl,j [˜] _wl,j_ = **_βi_** _._ (30)
_∇_ _L_ _∇_ _L_

(31)


To ensure numerical stability, we use the same trick as in B.2.

Since the above system of equations is underdetermined, we could choose different solutions. In our
experiments we used the pytorch.linalg.lstsq least squares solver which finds the solution
with the lowest ℓ2 norm.

B.3 SEPARABLE CLASSIFICATION


First note that in our model (∀n ∈{1, 2, · · · N _}, φ(s) =_


1

1+e[−][s][ )]


1
_p(yn = 1|xn, β) =_ 1 + e[−][y][n][x]n[⊤][β][ =][ φ][(][−][y][n][x]n[⊤][β][)]

(32)

1
_p(yn = −1|xn, β) = 1 −_ 1 + e[−][y][n][x]n[⊤][β][ = 1][ −] _[φ][(][−][y][n][x]n[⊤][β][)]_

So the loss function is (∀n ∈{1, 2, · · · N _})_

_ℓ(ynx[⊤]n_ **_[β][) = log(1 +][ e][−][y][n][x]n[⊤][β])_** (33)


and


log(1 + e[−][y][n][x]n[⊤][β]) (34)

_n=1_

X


_L(β) =_


Until now this did not depend on the parametrization. Now look at the parametrizations we used in
our article.
If we use a fully connected network the gradient is the following:


_ynxne[−][y][n][x]n[⊤][β]_
_−_

1 + e[−][y][n][x]n[⊤][β]


_∇β log(1 + e[−][y][n][x]n[⊤][β]) =_
_n=1_

X


**_β_** (β) =
_∇_ _L_


_n=1_


(35)


_ynxn_
_−_

1 + e[y][n][x]n[⊤][β][ =]


_−ynxn(1 −_ _φ(ynx[⊤]n_ **_[β][))][.]_**
_n=1_

X


_n=1_


-----

The Fisher information matrix is the following:

_F_ (β) = EX [EY _X_ [ **_βℓ(_** _ynx[⊤]n_ **_[β][)][∇]β[⊤][ℓ][(][−][y][n][x]n[⊤][β][)]] =]_**
_|_ _∇_ _−_


= [1]

_N_

= [1]

_N_

= [1]

_N_

= [1]


EY _X_ [xnx[⊤]n [(1][ −] _[φ][(][y][n][x]n[⊤][β][))][2][] =]_
_|_
_n=1_

X

_N_

**xnx[⊤]n** [(][φ][(][x]n[⊤][β][)(1][ −] _[φ][(][x]n[⊤][β][))][2][ + (1][ −]_ _[φ][(][x]n[⊤][β][))(1][ −]_ _[φ][(][−][x]n[⊤][β][))][2][) =]_
_n=1_

X

_N_

**xnx[⊤]n** [(][φ][(][x]n[⊤][β][)(1][ −] _[φ][(][x]n[⊤][β][))][2][ + (1][ −]_ _[φ][(][x]n[⊤][β][))][φ][2][(][x]n[⊤][β][)) =]_
_n=1_

X

_N_

**xnx[⊤]n** _[φ][(][x]n[⊤][β][)(1][ −]_ _[φ][(][x]n[⊤][β][))]_
_n=1_

X


(36)


If we use a diagonal network β = w1 **w2** **wL** 1 **wL, where**
_⊙_ _⊙· · · ⊙_ _−_ _⊙_
**w =** **w1[⊤]** **w2[⊤]** **wL[⊤]** _⊤. The gradient is the following:_

_· · ·_
   **_β_** = J _[⊤]_ **w** (37)

_∇_ _L_ _∇_ _L_

where J (the Jacobian) is the following


_J =_ _∂∂wβ1_ _∂∂wβL_

_· · ·_




(38)


where
_∂β_
" _∂wn_


= _∂βi_ = δi,j

_∂[wn]j_

_i,j_



[wk]i (39)
_k=1Y,k≠_ _i_


The Fisher information matrix is the following:


_F_ (w) = J _[⊤]F_ (β)J (40)

B.4 MATRIX FACTORIZATION

Before we compute the natural gradient of matrix factorization let us introduce some notations:
**_β = W1W2 . . . WL, as before and_**
_θ = vec(β),_ (41)

**w = vec(W1, W2, . . ., WL),** (42)

where vec vectorizes the matrices to obtain a column vector. θ is a reparametrization of w, so
_θ = P(w) and let J =_ _∂[∂θ]w_ [. With this notation, let’s compute the natural gradient with respect to the]

parametrization w.

˜ **w** = F (w)[−][1] **w** = (J _[⊤]F_ (θ))J)[−][1](J _[⊤]_ _θ_ ) = J _[−][1]F_ (θ)[−][1] _θ_ (43)
_∇_ _L_ _∇_ _L_ _∇_ _L_ _∇_ _L_

We use the assumption that J is full rank and because of F (θ) = I is invertible (J _[⊤]F_ (θ))J)[−][1] =
_J_ _[−][1]F_ (θ)[−][1]J _[−⊤]. Thus, the natural gradient simplifies to_

˜ **w** = J _[−][1]_ _θ_ (44)
_∇_ _L_ _∇_ _L_

and multiplying by J we obtain
_J_ [˜] **w** = _θ_ _._ (45)
_∇_ _L_ _∇_ _L_

We can consider the Jacobian like L consecutive matrices

_J = [J1J2 . . . JL]_ (46)

where Ji = _∂vec∂θ(Wi)_ [, and note that][ ∇][θ][L][ =][ vec][(][∇][β][L][)][ and][ ˜] **w** = vec( [˜] _W1,W2,...WL_ ). Rewrite

_∇_ _L_ _∇_ _L_
equation 45:
_Jvec( [˜]_ _W1,W2,...WL_ ) = vec( **_β_** ). (47)
_∇_ _L_ _∇_ _L_


-----

If we solve the following equation for i = 1, . . . L, then the concatenation of vectors vec( [˜] _Wi_ )
_∇_ _L_
will solve equation 47 as well.

_Jivec( ∇[˜]_ _Wi_ _L) = L[1]_ _[vec][(][∇][β][L][)]_ (48)


Let Ai = W1W2 . . . Wi 1 and Bi = Wi+1Wi+2 . . . WL and using notation for the Kronecker
_−_ _⊗_
product and utilize the property vec(ABC) = (C _[⊤]_ _⊗_ _A)vec(X) we get_

_Ji =_ _[∂vec][(][β][)]_ = _[∂][(][B]i[⊤]_ _[⊗]_ _[A][i][)][vec][(][W][i][)]_ = Bi[⊤] (49)

_∂vec(Wi) [=][ ∂vec]∂vec[(][A]([i][W]W[i]i[B])_ _[i][)]_ _∂vec(Wi)_ _[⊗]_ _[A][i][,]_

thus we need to solve
(Bi[⊤] _[⊗]_ _[A][i][)][vec][( ˜]∇Wi_ _L) = L[1]_ _[vec][(][∇][β][L][)]_ (50)

for vec( [˜] _Wi_ ). One can do this by exploiting properties of the Kronecker product and using Moore_∇_ _L_
Penrose pseudo-inverses as follows:

_vec( [˜]_ _Wi_ ) = [1] _i_ + _A+i_ [)][vec][(][∇][β][L][) = 1] _i_ + **_β_** _A+i_ [)] (51)
_∇_ _L_ _L_ [(][B][⊤] _⊗_ _L_ [(][B][⊤] _∇_ _L_


We note that when Ai and Bi are near full-rank, using the pseudoinverses may not be numerically
stable. Fausett & Fulton (1994) instead proposed a solution based on QR decomposition, and even
discussed an approach which extends to the rank deficient case. In practice we found that this
was not necessary for ours experiments. As a result, in our implementation we use the formula

_L1_ _[B]i[⊤]+∇βLA+i_ [to update the factor matrices with the natural gradient.]

C PROOF OF THEOREMS

C.1 PROOF OF THEOREM 1

**Statement. Let’s assume, that N < D, X is full rank and A is an invertible D × D matrix. Let**
**_βt = βt(X, y) be the trajectory of NGF and β[′]t_** [=][ β]t[(][XA][⊤][,][ y][)][ (the trajectory of NGF on data]
_XA[⊤]). Then Xβ = XA[T]_ **_β[′]._**

_Proof. Let s = Xβ and s[′]_ = XA[T] **_β[′]. The gradient and the Fisher information matrix are the_**
following (the calculation can be found in Lemma 1).

[∇sL(s)]i = −yi(1 − _φ(yisi))_ (52)

[F (s)]i,j = δi,jφ(si)(1 − _φ(si))_

The exact same can be said about s[′], so s and s[′] are the solutions of the same differential equations,
so if we use the same initialization st = s[′]t[.]

C.2 PROOF OF THEOREM 2

**Statement. Let βt(X, y) be the trajectory of NGF and let A be a D** _D invertible transforma-_
_×_
_tion. If N ≥_ _D, X has full rank and we consider NGF on the transformed data XA[⊤], then_
_A[⊤]βt(XA[⊤], y) = βt(X, y)._

_Proof. Let β[′]_ be the trajectory of NGF on the transformed data:

**_β[′]t_** [=][ β]t[(][XA][⊤][,][ y][)][.] (53)

To run NGF on β[′] we need its Fisher information matrix. Note that the Fisher information matrix
of linear models with logistic-loss is

_F_ (β) = X _[⊤]diag[φ(Xβ) ⊙_ _φ(−Xβ)]X._ (54)

by Appendix B.1. Note that in this case the rank of the Fisher information matrix is D, so it is
invertible. Same is true for β[′]. Let’s compute the Fisher information matrix of β[′].


_F_ (β[′]) = [1]


Eyn _Axn_ [ **_β[′]_** _ℓ(ynβ[′⊤]Axn)_ **_β[′]_** _[ℓ][(][y][n][β][′⊤][A][x][n][)]]_ (55)
_|_ _∇_ _∇[⊤]_
_n=1_

X


-----

First, specify **_β[′]_** _ℓ(ynβ[′⊤]Axn) and use the notation v[⊤]_ = β[′⊤]A.
_∇_

**_β[′]_** _ℓ(ynβ[′⊤]Axn) = J_ _[⊤]_ **_v⊤_** _ℓ(ynv[⊤]xn)_ (56)
_∇_ _∇_


where J = _[∂v]∂β[⊤][′][ .]_

_k=1_ **_[β]k[′]_** _[A][k,i]_

_Ji,j =_ _[∂][v][i]_ = _[∂]_ [P][d] = Aj,i (57)

_∂β[′]j_ _∂β[′]j_

Therefore J = A[⊤] _⇔_ _J_ _[⊤]_ = A and

**_β[′]_** _ℓ(ynβ[′⊤]Axn) = A_ **_vℓ(ynv[⊤]xn)._** (58)
_∇_ _∇_

We now can continue the computation of the Fisher:


_F_ (β[′]) = [1]


Eyn|Axn [A∇v⊤ _ℓ(ynv[⊤]xn)∇v[⊤][⊤]_ _[ℓ][(][y][n][v][⊤][x][n][)][A][⊤][] =]_
_n=1_

X


(59)


= A( [1]


Eyn|xn [∇v⊤ _ℓ(ynv[⊤]xn)∇v[⊤][⊤]_ _[ℓ][(][y][n][v][⊤][x][n][)])][A][⊤]_ [=][ AF] [(][v][)][A][⊤][.]
_n=1_

X


Note, that the Fisher of v must be invertible as well from the previous Equation. Let’s see the NGF
on β[′]:


_′ =_ _F_ (β′)−1 **_β′_** (β′⊤XAT, y) = (AF (v)A⊤)−1
_−_ _∇_ _L_ _−_


**_β[′]_** _ℓ(ynβ[′⊤]Axn) =_
_∇_
_n=1_

X


(60)


= (A[⊤])[−][1]F (v)[−][1]A[−][1]A **_vℓ(ynv[⊤]xn) =_** (A[⊤])[−][1]F (v)[−][1] **_v_** (v)
_−_ _∇_ _−_ _∇_ _L_

_n=1_

X


We also have the following (by the Chain Rule):

**_v˙ = Jβ[˙]_** _′ = A⊤β˙_ _′_ (61)

Now from Equation (60) and (61) we get:

**_v˙ = F_** (v)[−][1] **_v_** (v) (62)
_∇_ _L_

This is the same differential equation as the one β is a solution of. So if they are initialized the same
way vt = βt(X, y), so βt = vt = A[⊤]β[′].

C.3 PROOF OF THEOREM 4

**Statement. If N < D and β is the separator. Let s be the output. Let the Jacobi matrix from β to**
**s be J =** _∂[∂s]β_ _[. If][ J][ is full rank][ s][ is asymptotically linear with direction vector][ y][.]_

_Proof. First let’s note that by the invariance property of NGF the trajectory of s is defined by the_
trajectory of β.
**s˙ =** _F_ (s) **s** (s) (63)
_−_ _[−][1]_ _∇_ _L_
Let’s assume s is 1-dimensional. In this case s = s, x1 = x and y = y can be used since we have
only one data point. To solve equation (63) we need the gradient and the Fisher information matrix
which are the following (the calculation can be found in the Appendix B.2)


_s_ (s) = _y(1_ _φ(ys))_ (64)
_∇_ _L_ _−_ _−_


The Fisher information matrix:


_F_ (s) = φ(s)(1 − _φ(s))_ (65)

Then Equation (63) can be written as:

_s˙ =_ _[y][(1][ −]_ _[φ][(][ys][))]_ (66)

_φ(s)(1 −_ _φ(s))_


-----

Now we rescale our data points s.t. ˜x = −x, so ˜s = −s and ˜y = −y = 1. Hence we get:

_∂s˜_ 1

(67)

_∂t_ [=] _φ(˜s)_

Which can be solved and the solution is


log(1 + es[˜]) = t + c _⇐⇒_ _s˜ = log(e[t][+][c]_ _−_ 1) (68)

By equation (68) we get the asymptotic behaviour

_s˜_ log(e[t][+][c] 1)
lim _−_ = (69)
_t→∞_ _t + c_ [= lim]t→∞ _t + c_

_e[t][+][c]_ 1
= lim (70)
_t_ _e[t][+][c]_ 1 [= lim]t 1 _e[−][(][t][+][c][)][ = 1]_
_→∞_ _−_ _→∞_ _−_

(From (69) to (70) we use L’Hopital Rule).
Hence we proved Theorem 4. for N = 1. Now let’s assume, that N > 1. Now we write down the
gradient again:

[∇sL(s)]i = −yi(1 − _φ(yisi))_ (71)
And the Fisher information matrix:

[F (s)]i,j = δi,jφ(si)(1 − _φ(si))_ (72)

So now if we substitute in Equation (71) and Equation (72) to Equation (63). We can rescale, so
_y˜i = 1_ _∀i as we did in the previous case. Hence we get the following:_

_φ(˜s1)(11_ _φ(˜s1))_ 0 0 (1 _φ(˜s1))_ _φ(˜1s1)_
_−_ 1 _· · ·_ _−_ _−_ 1

_∂˜s_  0 _φ(˜s2)(1−φ(˜s2))_ _· · ·_ 0  _−(1 −_ _φ(˜s2))_  _φ(˜s2)_

_∂t_ [=][ −]  ... ... ... ...   ...   ...

 0 0 _· · ·_ _φ(˜sN_ )(11−φ(˜sN ))  −(1 − _φ(˜sN_ )) [=]  _φ(˜s1N_ )
   (73)

Hence we got N independent differential equations which are exactly the same as in the N = 1
is a constant. Socase. So in each dimension s _ty + c ˜ss is asymptotically, where cs_ R[D] _tis a constant. + c for some c. Hence ˜s ≈_ _t1 + c, where c ∈_ R[D]
_≈_ _∈_

D COUNTEREXAMPLE FOR THE INVARIANCE OF ℓ2 LARGE MARGIN
SOLUTION

The counterexample is the following:

1 2
_A =_, y = 1 and X = (2 3)
1 0 _−_
− 

0
Then β[∗](X, y) = argmin∥β∥2 subject to 2β1 − 3β2 ≥ 1, therefore β[∗](X, y) = 3 . Further− [1] 

more β[∗](XA[⊤], y) = argmin **_β_** 2 subject to 4β1 2β2 1, therefore β[∗](XA[⊤], y) = _−_ [1]4,
_∥_ _∥_ _−_ _−_ _≥_ 0
 

but A[⊤]β[∗](XA[⊤], y) = _−_ 4[1] = 0 = β[∗](X, y).

2 _̸_ 3

− [1]  − [1] 


E PROOF OF THE STATEMENT ABOUT THE PARAMETRIZATION INVARIANCE
OF NGF

**Statement. Let w and θ be two parameter vectors related by the mapping θ = P(w) and consider**
_natural gradient flow in w. Assume that (1) the Jacobian J =_ _∂[∂θ]w[t]t_ _[and (2)][ F]_ [(][θ][t][)][ are both full rank]

_for all t. If wt follows natural gradient flow starting from w0 then θt = P(wt) follows NGF, i. e. it_
_solves_ _θ[˙]t = −F_ (θt)[+]∇θt _L(X, θt)._


-----

_Proof. We use that F_ (w) = J _[⊤]F_ (θ)J which follows from the definition of F :

_F_ (w) = EX [ **w** (X, w) **w[L][(][X,][ w][)] =][ E][X]** [[][J] _[⊤][∇][θ][L][(][X, θ][)][∇][⊤]θ_
_∇_ _L_ _∇[⊤]_ _[L][(][X, θ][)][J][] =][ J]_ _[⊤][F]_ [(][θ][)][J]

The invariance statement follows:


_θ˙ =_ (P(w˙ **t)) = J ˙wt = −JF** (wt)[+]∇wt _L(X, wt) =_

= _JJ_ [+]F (θt)[+](J _[⊤])[+]J_ _[⊤]_ _θt_ (X, θt) = _F_ (θt)[+] _θt_ (X, θt).
_−_ _∇_ _L_ _−_ _∇_ _L_

F PROOF OF THE STATEMENT ABOUT NGD IN MATRIX COMPLETION

**Statement. Let’s apply NGF for the problem of matrix completion. EGF in the direct parametriza-**
_tion (β = w) is equivalent to NGF under any parametrization θ for which J =_ _[∂]∂θ[w]t[t]_ _[is full rank.]_

_Proof.J =_ _∂ First let’s consider a parametrization∂θw_ [is full rank. Then by the invariance property if] θ s.t. the direct parametrization[ θ][t][ is the solution of the NGF with the] β = w (= P(θ)) and

arbitrary parametrization, then wt = P(θt) is the solution of:

**w˙** = **w** (w)
_−∇_ _L_

Which agrees with the EGF with direct parametrization.

G INVARIANCE PROPERTY OF OLS

We show the same transformation invariance property for OLS that we showed in Theorem 1,2 for
NGF. Again, we split the problem into two cases: N < D and N ≥ _D. Note that for the problem_
_Xβ = y the Ordinary least squares solution is β = (X_ _[⊤]X)[−][1]X_ _[⊤]y if the columns of X is linearly_
independent.

**Statement. Let N < D, A is an invertible D × D matrix. If β is the solution of the Ordenary least**
_squares problem for the matrix X and β[′]_ _for XA[⊤], then Xβ = XA[⊤]β[′]._

_Proof. Immediately follows from the definition of the problems: Xβ = y and XA[⊤]β[′]_ = y.

**Statement. Let N ≥** _D, X has full rank and A is an invertible D × D matrix. If β is the solution of_
_the OLS problem for the matrix X and β[′]_ _for XA[⊤], then A[⊤]β[′]_ = β.

_Proof._

_A[⊤]β[′]_ = A[⊤]((XA[⊤])[⊤]XA[⊤])[−][1](XA[⊤])[⊤]y = A[⊤]A[⊤−][1](X _[⊤]X)[−][1]A[−][1]AX_ _[⊤]y = β_

H SUPPLEMENTARY FIGURES


-----

Figure 6: During peer review, reviewers requested a lower dimensional variant of the experiment
reported in Figure 4. Instead of 1000 dimensions, in this experiment we used D = 50, and instead
of S = 20 non-zero components, the real β had S = 5 non-zero entries. The experimental setup and
hyperparameters were otherwise not changed from Figure 4. The 5-layer diagonal network performs
poorly, which is likely a result of sensitivity to hyperparameters, we expect that with additional finetuning of the hyperparameters for this experiment, L = 4 would do at least as well as the shallow
_L = 1 model._


-----

