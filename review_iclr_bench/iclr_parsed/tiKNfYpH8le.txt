p p

# PARETO NAVIGATION GRADIENT DESCENT: A FIRST- ORDER ALGORITHM FOR OPTIMIZATION IN PARETO SET

**Anonymous authors**
Paper under double-blind review

ABSTRACT

1 Many modern machine learning applications, such as multi-task learning, require finding

2 optimal model parameters to trade-off multiple objective functions that may conflict with

3 each other. The notion of the Pareto set allows us to focus on the set of (often infinite number

4 of) models that cannot be strictly improved. But it does not provide an actionable procedure

5 for picking one or a few special models to return to practical users. In this paper, we

6 consider optimization in Pareto set (OPT-in-Pareto), the problem of finding Pareto models

7 that optimize an extra reference criterion function within the Pareto set. This function can

8 either encode a specific preference from the users, or represent a generic diversity measure

9 for obtaining a set of diversified Pareto models that are representative of the whole Pareto

10 set. Unfortunately, despite being a highly useful framework, efficient algorithms for OPT
11 in-Pareto have been largely missing, especially for large-scale, non-convex, and non-linear

12 objectives in deep learning. A naive approach is to apply Riemannian manifold gradient

13 descent on the Pareto set, which yields a high computational cost due to the need for eigen
14 calculation of Hessian matrices. We propose a first-order algorithm that approximately

15 solves OPT-in-Pareto using only gradient information, with both high practical efficiency

16 and theoretically guaranteed convergence property. Empirically, we demonstrate that our

17 method works efficiently for a variety of challenging multi-task-related problems.


18 1 INTRODUCTION

19 Although machine learning tasks are traditionally framed as optimizing a single objective. Many modern

20 applications, especially in areas like multitask learning, require finding optimal model parameters to minimize

21 multiple objectives (or tasks) simultaneously. As the different objective functions may inevitably conflict

22 with each other, the notion of optimality in multi-objective optimization (MOO) needs to be characterized by

23 the Pareto set: the set of model parameters whose performance of all tasks cannot be jointly improved.

24 Focusing on the Pareto set allows us to filter out models that can be strictly improved. However, the Pareto

25 set typically contains an infinite number of parameters that represent different trade-offs of the objectives.

26 For m objectives ℓ1, . . ., ℓm, the Pareto set is often an (m 1) dimensional manifold. It is both intractable
_−_

27 and unnecessary to give practical users the whole exact Pareto set. A more practical demand is to find some

28 user-specified special parameters in the Pareto set, which can be framed into the following optimization in

29 _Pareto set (OPT-in-Pareto) problem:_

30 _Finding one or a set of parameters inside the Pareto set of ℓ1, . . ., ℓm that minimize a reference criterion F_ _._

31 Here the criterion function F can be used to encode an informative user-specific preference on the objectives

32 _ℓ1, . . ., ℓm, which allows us to provide the best models customized for different users. F can also be an_

33 _non-informative measure that encourages, for example, the diversity of a set of model parameters. In this_


-----

p p

34 case, optimizing F in Pareto set gives a set of diversified Pareto models that are representative of the whole

35 Pareto set, from which different users can pick their favorite models during the testing time.

36 OPT-in-Pareto provides a highly generic and actionable framework for multi-objective learning and opti
37 mization. However, efficient algorithms for solving OPT-in-Pareto have been largely lagging behind in deep

38 learning where the objective functions are non-convex and non-linear. Although has not been formally studied,

39 a straightforward approach is to apply manifold gradient descent on F in the Riemannian manifold formed by

40 the Pareto set (Hillermeier, 2001; Bonnabel, 2013). However, this casts prohibitive computational cost due

41 to the need for eigen-computation of Hessian matrices of _ℓi_ . In the optimization and operation research
_{_ _}_

42 literature, there has been a body of work on OPT-in-Pareto viewing it as a special bi-level optimization

43 problem (Dempe, 2018). However, these works often heavily rely on the linearity and convexity assumptions

44 and are not applicable to the non-linear and non-convex problems in deep learning; see for examples in Ecker

45 & Song (1994); Jorge (2005); Thach & Thang (2014); Liu & Ehrgott (2018); Sadeghi & Mohebi (2021) (just

46 to name a few). In comparison, the OPT-in-Pareto problem seems to be much less known and under-explored

47 in the deep learning literature.

48 In this work, we provide a practically efficient first-order algorithm for OPT-in-Pareto, using only gradient

49 information of the criterion F and objectives _ℓi_ . Our method, named Pareto navigation gradient descent
_{_ _}_

50 (PNG), iteratively updates the parameters following a direction that carefully balances the descent on F and

51 _ℓi_, such that it guarantees to move towards the Pareto set of _ℓi_ when it is far away, and optimize F in a
_{_ _}_ _{_ _}_

52 neighborhood of the Pareto set. Our method is simple, practically efficient and has theoretical guarantees.

53 In empirical studies, we demonstrate that our method works efficiently for both optimizing user-specific

54 criteria and diversity measures. In particular, for finding representative Pareto solutions, we propose an

55 energy distance criterion whose minimizers distribute uniformly on the Pareto set asymptotically (Hardin

56 & Saff, 2004), yielding a principled and efficient Pareto set approximation method that compares favorably

57 with recent works such as Lin et al. (2019); Mahapatra & Rajan (2020). We also apply PNG to improve the

58 performance of JiGen (Carlucci et al., 2019b), a multi-task learning approach for domain generalization, by

59 using the adversarial feature discrepancy as the criterion objective.

60 **Related Work There has been a rising interest in MOO in deep learning, mostly in the context of multi-task**

61 learning. But most existing methods can not be applied to the general OPT-in-Pareto problem. A large body

62 of recent works focus on improving non-convex optimization for finding some model in the Pareto set, but

63 cannot search for a special model satisfying a specific criterion (Chen et al., 2018; Kendall et al., 2018; Sener

64 & Koltun, 2018; Yu et al., 2020; Chen et al., 2020; Wu et al., 2020; Fifty et al., 2020; Javaloy & Valera, 2021).

65 One exception is Mahapatra & Rajan (2020); Kamani et al. (2021), which searches for models in the Pareto

66 set that satisfy a constraint on the ratio between the different objectives. The problem they study can be

67 viewed as a special instance of OPT-in-Pareto. However, their approaches are tied with special properties of

68 the ratio constraint and do not apply to the general OPT-in-Pareto problem.

69 There has also been increasing interest in finding a compact approximation of the Pareto set. Navon et al.

70 (2020); Lin et al. (2020) use hypernetworks to approximate the map from linear scalarization weights to

71 the corresponding Pareto solutions; these methods could not fully profile non-convex Pareto fronts due

72 to the limitation of linear scalarization (Boyd et al., 2004), and the use of hypernetwork introduces extra

73 optimization difficulty. Another line of works (Lin et al., 2019; Mahapatra & Rajan, 2020) approximate

74 the Pareto set by training models with different user preference vectors that rank the relative importance

75 of different tasks; these methods need a good heuristic design of preference vectors, which requires prior

76 knowledge of the Pareto front. Ma et al. (2020) leverages manifold gradient to conduct a local random walk

77 on the Pareto set but suffers from the high computational cost. Deist et al. (2021) approximates the Pareto set

78 by maximizing hypervolume, which requires prior knowledge for choosing a good reference vector.

79 Multi-task learning can also be applied to improve the learning in many other domains including domain

80 generalization (Dou et al., 2019; Carlucci et al., 2019a; Albuquerque et al., 2020), domain adaption (Sun


-----

p p

81 et al., 2019; Luo et al., 2021), model uncertainty (Hendrycks et al., 2019; Zhang et al., 2020; Xie et al., 2021),

82 adversarial robustness (Yang & Vondrick, 2020) and semi-supervised learning (Sohn et al., 2020). All of

83 those applications utilize a linear scalarization to combine the multiple objectives and it is thus interesting to

84 apply the proposed OPT-in-Pareto framework, which we leave for future work.


85 2 BACKGROUND ON MULTI-OBJECTIVE OPTIMIZATION

86 We introduce the background on multi-objective optimization (MOO) and Pareto optimality. For notation,

87 we denote by [m] the integer set 1, 2, ...., m, and R+ the set of non-negative real numbers. Let =
_m_ _{_ _}_ _C[m]_
88 _ω ∈_ R[m]+ _[,]_ _i=1_ _[ω][i][ = 1]_ be the probability simplex. We denote by ∥·∥ the Euclidean norm.

89 Let _θ ∈_ R[n]Pbe a parameter of interest (e.g., the weights in a deep neural network). Let ℓ(θ) =

90 [ℓ1(θ), . . ., ℓm(θ)] be a set of objective functions that we want to minimize. For two parameters θ, θ[′] _∈_ R[n],

91 we write ℓ(θ) **_ℓ(θ[′]) if ℓi(θ)_** _ℓi(θ[′]) for all i_ [m]; and write ℓ(θ) **_ℓ(θ[′]) if ℓ(θ)_** **_ℓ(θ[′]) and_**
_⪰_ _≥_ _∈_ _≻_ _⪰_

92 **_ℓ(θ) ̸= ℓ(θ[′]). We say that θ is Pareto dominated (or Pareto improved) by θ[′]_** if ℓ(θ) ≻ **_ℓ(θ[′]). We say that θ is_**

93 Pareto optimal on a set Θ ⊆ R[n], denoted as θ ∈ Pareto(Θ), if there exists no θ[′] _∈_ Θ such that ℓ(θ) ≻ **_ℓ(θ[′])._**

94 The Pareto global optimal set P _[∗∗]_ := Pareto(R[n]) is the set of points (i.e., θ) which are Pareto optimal on

95 the whole domain R[n]. The Pareto local optimal set of ℓ, denoted by P _[∗], is the set of points which are Pareto_

96 optimal on a neighborhood of itself:


_P_ _[∗]_ := {θ ∈ R[n] : there exists a neighborhood Nθ of θ, such that θ ∈ Pareto(Nθ)} .

97 The (local or global) Pareto front is the set of objective vectors achieved by the Pareto optimal points, e.g.,

98 the local Pareto front is F _[∗]_ = {ℓ(θ) : θ ∈P _[∗]}. Because finding global Pareto optimum is intractable for_

99 non-convex objectives in deep learning, we focus on Pareto local optimal sets in this work; in the rest of the

100 paper, terms like “Pareto set” and “Pareto optimum” refer to Pareto local optimum by default.

**Pareto Stationary Points Similar to the case of single-objective optimization, Pareto local optimum implies**
a notion of Pareto stationarity defined as follows. Assume ℓ is differentiable on R[n]. A point θ is called Pareto
stationary if there must exists a set of non-negative weights ω1, . . ., ωm with _i=1_ _[ω][i][ = 1][, such that][ θ][ is a]_
stationary point of the ω-weighted linear combination of the objectives: ℓω(θ) := _i=1_ _[ω][i][ℓ][i][(][θ][)][.][ Therefore,]_
the set of Pareto stationary points, denoted by, can be characterized by

[P][m]
_P_

_m_ [P][m]

:= _θ_ Θ : g(θ) = 0 _,_ _g(θ) := min_ _ωi_ _ℓi(θ)_ _,_ (1)
_P_ _{_ _∈_ _}_ _ω_ _∇_ _||[2]_
_∈C[m][ ||]_ _i=1_

X


101 where g(θ) is the minimum squared gradient norm of ℓω among all ω in the probability simplex C[m] on [m].

102 Because g(θ) can be calculated in practice, it provides an essential way to access Pareto local optimality.

103 **Finding Pareto Optimal Points A main focus of the MOO literature is to find a (set of) Pareto optimal**

104 points. The simplest approach is linear scalarization, which minimizes ℓω for some weight ω (decided, e.g.,

105 by the users) in C[m]. However, linear scalarization can only find Pareto points that lie on the convex envelop

106 of the Pareto front (see e.g., Boyd et al., 2004), and hence does not give a complete profiling of the Pareto

107 front when the objective functions (and hence their Pareto front) are non-convex.

_Multiple gradient descent (MGD) (Désidéri, 2012) is an gradient-based algorithm that can converge to a_
Pareto local optimum that lies on either the convex or non-convex parts of the Pareto front, depending on the
initialization. MGD starts from some initialization θ0 and updates θ at the t-th iteration by


min _,_ (2)
_i∈[m]_ _[∇][ℓ][i][(][θ][t][)][⊤][v][ −]_ [1]2 _[∥][v][∥][2]_


_θt+1_ _θt_ _ξvt,_ _vt := arg max_
_←_ _−_ _v∈R[n]_


-----

p p

108 where ξ is the step size and vt is an update direction that maximizes the worst descent rate among all

109110 objectives, sincefollowing direction ∇ vℓi. When using a sufficiently small step size(θt)[⊤]v ≈ (ℓi(θt) − _ℓi(θt −_ _ξv))/ξ approximates the descent rate of objective ξ, MGD ensures to yield a Pareto improvement ℓi when_

111 (i.e, decreasing all the objectives) on θt unless θt is Pareto (local) optimal; this is because the optimization in

112 Equation (2) always yields mini∈[m] ∇ℓi(θt)[⊤]vt ≤ 0 (otherwise we can simply flip the sign of vt).

Using Lagrange strong duality, the solution of Equation (2) can be framed into


_ωi,t_ _ℓi(θt),_ where _ωi,t_ _i=1_ [= arg min] (3)
_∇_ _{_ _}[m]_ _ω_
_i=1_ _∈C[m][ ∥∇][θ][ℓ][ω][(][θ][t][)][∥]_ _[.]_

X


_vt =_


113 It is easy to see from Equation (3) that the set of fixed points of MDG (which satisfy vt = 0) coincides with

114 the Pareto stationary set P _[∗]._

115 A key disadvantage of MGD, however, is that the Pareto point that it converges to depends on the initialization

116 and other algorithm configurations in a rather implicated and complicated way. It is difficult to explicitly

117 control MGD to make it converge to points with specific properties.

118 3 OPTIMIZATION IN PARETO SET


119 The Pareto set typically contains an infinite number of points. In the optimization in Pareto set (OPT-in
120 Pareto) problem, we are given an extra criterion function F (θ) in addition to the objectives ℓ, and we want to

121 minimize F in the Pareto set of ℓ, that is,

min (4)
_θ∈P_ _[∗]_ _[F]_ [(][θ][)][.]

122 For example, one can find the Pareto point whose loss vector ℓ(θ) is the closest to a given reference point

123 _r ∈_ R[m] by choosing F (θ) = ∥ℓ(θ) − _r∥[2]. We can also design F to encourages ℓ(θ) to be proportional to r,_

124 i.e., ℓ(θ) ∝ _r; a constraint variant of this problem was considered in Mahapatra & Rajan (2020)._

125 We can further generalize OPT-in-Pareto to allow the criterion F to depend on an ensemble of Pareto points

126 _θ1, ..., θN_ jointly, that is,
_{_ _}_

min (5)
_θ1,...,θN_ _∈P_ _[∗]_ _[F]_ [(][θ][1][, ..., θ][N] [)][.]

For example, if F (θ1, . . ., θN ) measures the diversity among {θi}i[N]=1[, then optimizing it provides a set of]
diversified points inside the Pareto set P _[∗]. An example of diversity measure is_

_F_ (θ1, . . ., θN ) = E(ℓ(θ1), . . ., ℓ(θN )), with _E(ℓ1, . . ., ℓN_ ) = **_ℓi_** **_ℓj_** _,_ (6)
Xi≠ _j_ _∥_ _−_ _∥[−][2]_

127 where E is known as an energy distance in computational geometry, whose minimizer can be shown to give

128 an uniform distribution asymptotically when N →∞ (Hardin & Saff, 2004). This formulation is particularly

129 useful when the users’ preference is unknown during the training time, and we want to return an ensemble of

130 models that well cover the different areas of the Pareto set to allow the users to pick up a model that fits their

131 needs regardless of their preference. The problem of profiling Pareto set has attracted a line of recent works

132 (e.g., Lin et al., 2019; Mahapatra & Rajan, 2020; Ma et al., 2020; Deist et al., 2021), but they rely on specific

133 criterion or heuristics and do not address the general optimization of form Equation (5).

134 **Manifold Gradient Descent One straightforward approach to OPT-in-Pareto is to deploy manifold gradient**

135 descent (Hillermeier, 2001; Bonnabel, 2013), which conducts steepest descent of F (θ) in the Riemannian

137136 iteration along the direction of the projection ofmanifold formed by the Pareto set P _[∗]. Initialized at ∇F θ(0θ ∈Pt) on the tangent space[∗], manifold gradient descent updates T (θt) at θt in P_ _[∗] θ,_ _t at the t-th_

_θt+1 = θt −_ _ξProjT (θt)(∇F_ (θt)).


-----

p p

138 By using the stationarity characterization in Equation (1), under proper regularity conditions, one can

139 show that the tangent space (θt) equals the null space of the Hessian matrix _θ[ℓ][ω]t_ [(][θ][t][)][, where][ ω][t][ =]
_T_ _∇[2]_

140141 this null space of Hessian matrix. Although numerical techniques such as Krylov subspace iteration (arg minω∈Cm ∥∇θℓω(θt)∥. However, the key issue of manifold gradient descent is the high cost for calculatingMa

142 et al., 2020) or conjugate gradient descent (Koh & Liang, 2017) can be applied, the high computational cost

143 (and the complicated implementation) still impedes its application in large scale deep learning problems. See

144 Section 1 for discussions on other related works.


145 4 PARETO NAVIGATION GRADIENT DESCENT FOR OPT-IN-PARETO

146 We now introduce our main algorithm, Pareto Navigating Gradient Descent (PNG), which provides a practical

147 approach to OPT-in-Pareto. For convenience, we focus on the single point problem in Equation (4) in the

148 presentation. The generalization to the multi-point problem in Equation (5) is straightforward. We first

149 introduce the main idea and then present theoretical analysis in Section 4.1.


**Main Idea We consider the general incremental updating rule of form**

_θt+1_ _θt_ _ξvt,_
_←_ _−_

150 where ξ is the step size and vt is an update direction that we shall choose to achieve the following desiderata

151 in balancing the decent of _ℓi_ and F :
_{_ _}_

152 i) When θt is far away from the Pareto set, we want to choose vt to give Pareto improvement to θt, moving it

153 towards the Pareto set. The amount of Pareto improvement might depend on how far θt is to the Pareto set.

154 ii) If the directions that yield Pareto improvement are not unique, we want to choose the Pareto improvement

155 direction that decreases F (θ) most.


156 iii) When θt is very close to the Pareto set, e.g., having a small g(θ), we want to fully optimize F (θ).

We achieve the desiderata above by using the vt that solves the following optimization:

1

_vt = arg min_ s.t. _θℓi(θt)[⊤]v_ _φt,_ _i_ [m] _,_ (7)
_v∈R[n]_  2 _[∥∇][F]_ [(][θ][t][)][ −] _[v][∥][2]_ _∇_ _≥_ _∀_ _∈_ 

157 where we want vt to be as close to ∇F (θt) as possible (hence decrease F most), conditional on that the

158 decreasing rate ∇θℓi(θt)[⊤]vt of all losses ℓi are lower bounded by a control parameter φt. A positive φt

159 enforces that ∇θt _ℓi(θ)[⊤]vt is positive for all ℓi, hence ensuring a Pareto improvement when the step size is_

160 sufficiently small. The magnitude of φt controls how much Pareto improvement we want to enforce, so we

161 may want to gradually decrease φt when we move closer to the Pareto set. In fact, varying φt provides an

162 intermediate updating direction between the vanilla gradient descent on F and MGD on _ℓi_ :
_{_ _}_

163 i) If φt = −∞, we have vt = ∇F (θt) and it conducts a pure gradient descent on F without considering {ℓi}.

164 ii) If φt → +∞, then vt approaches to the MGD direction of {ℓi} in Equation (2) without considering F .

In this work, we propose to choose φt based on the minimum gradient norm g(θt) in Equation (1) as a
surrogate indication of Pareto local optimality. In particular, we consider the following simple design:

_φt =_ _−∞_ if g(θt) ≤ _e,_ (8)
_αtg(θt)_ if g(θt) > e,


165 where e is a small tolerance parameter and αt is a positive hyper-parameter. When g(θt) > e, we set φt to be

166 proportional to g(θt), to ensure Pareto improvement based on how far θt is to Pareto set. When g(θt) ≤ _e,_

167 we set φt = −∞ which “turns off” the control and hence fully optimizes F (θ).

168 In practice, the optimization in Equation (7) can be solved efficiently by its dual form as follows.


-----

p p

**Theorem 1. The solution vt of Equation (7), if it exists, has a form of**


_λi,t_ _ℓi(θt),_ (9)
_∇_
_t=1_

X


_vt = ∇F_ (θt) +


_with {λi,t}t[m]=1_ _[the solution of the following dual problem]_


max
_λ_ R[m]+ _−_ [1]2 _[||∇][F]_ [(][θ][t][) +]
_∈_


_λt_ _ℓi(θt)_ +
_∇_ _||[2]_
_i=1_

X


_λiφt._ (10)
_i=1_

X


169 The optimization in Equation (10) can be solved efficiently for a small m (e..g, m ≤ 10), which is the case

170 for typical applications. We include the details of the practical implementation in Appendix B.

171 4.1 THEORETICAL PROPERTIES


172 We provide a theoretical quantification on how PNG guarantees to i) move the solution towards the Pareto

173 set (Theorem 2); and ii) optimize F in a neighborhood of Pareto set (Theorem 3). To simplify the result and

174 highlight the intuition, we focus on the continuous time limit of PNG, which yields a differentiation equation

175 dθt = −vtdt with vt defined in Equation (7), where t ∈ R+ is a continuous integration time.

176 **Assumption 1. Let {θt : t ∈** R+} be a solution of dθt = −vtdt with vt in Equation (7); φk in Equation (8);

178177 _e >with 0 F; and[∗]_ := inf αt ≥θ∈R0n, F∀t ∈(θ)R >+ −∞. Assumeand ℓ F[∗]i _and[:= inf] ℓ_ _[θ]are continuously differentiable on[∈][R][n][ ℓ][i][(][θ][)][ >][ −∞][. Assume][ sup]θ∈R R[n][ ∥∇][n], and lower bounded[F]_ [(][θ][)][∥≤] _[c][.]_

179 Technically, dθt = −vtdt is a piecewise smooth dynamical system whose solution should be taken in the

180 Filippov sense using the notion of differential inclusion (Bernardo et al., 2008). The solution always exists

181 under mild regularity conditions although it may not be unique. Our results below apply to all solutions.

**Pareto Optimization on ℓ** We now show that the algorithm converges to the vicinity of Pareto set quantified
by a notion of Pareto closure. For ϵ ≥ 0, let Pϵ be the set of Pareto ϵ-stationary points: Pϵ = {θ ∈
R[n] : g(θ) ≤ _ϵ}. The Pareto closure of a set Pϵ, denoted by P_ _ϵ is the set of points that perform no worse than_
at least one point in _ϵ, that is,_
_P_

_ϵ :=_ _θ_ _ϵ_ _θ_ _,_ _θ_ = _θ[′]_ R[n] : **_ℓ(θ[′])_** **_ℓ(θ)_** _._
_P_ _∪_ _∈P_ _{_ _}_ _{_ _}_ _{_ _∈_ _⪯_ _}_

182 Therefore, P _ϵ is better than or at least as good as Pϵ in terms of Pareto efficiency._

**Theorem 2θte** _e, then for any time (Pareto Improvement on t < te,_ **_ℓ). Under Assumption 1, assume θ0 ̸∈Pe, and te is the first time when_**
_∈P_

d _i_ [)]

min _t_ _._
dt _[ℓ][i][(][θ][t][)][ ≤−][α][t][g][(][θ][t][)][,]_ _s_ [0,t] _[g][(][θ][s][)][ ≤]_ [min][i][∈][[][m][]][(][ℓ][i][(][θ][0][)][ −] _[ℓ][∗]_
_∈_ 0 _[α][s][d][s]_

183 _Therefore, the update yields Pareto improvement on ℓ_ _when θt ̸∈Pe and αtgR(θt) > 0._

_t_
184 _Further, if_ 0 _[α][s][d][s][ = +][∞][, then for any][ ϵ > e][, there exists a finite time][ t][ϵ][ ∈]_ [R][+][ on which the solution enters]

185 _ϵ and stays within_ _ϵ afterwards, that is, we have θtϵ_ _ϵ and θt_ _ϵ for any t_ _tϵ._
_P_ R _P_ _∈P_ _∈P_ _≥_

186 Here we guarantee that θt must enter Pϵ for some time (in fact infinitely often), but it is not confined in Pϵ.

187 On the other hand, θt does not leave P _ϵ after it first enters Pϵ thanks to the Pareto improvement property._

188 **Optimization on F We now show that PNG finds a local optimum of F inside the Pareto closure P** _ϵ in an_

189 approximate sense. We first show that a fixed point θ of the algorithm that is locally convex on F and ℓ must

190 be a local optimum of F in the Pareto closure of {θ}, and then quantify the convergence of the algorithm.


-----

p p

191 **Lemma 1. Under Assumption 1, assume θt ̸∈Pe is a fixed point of the algorithm, that is,** [d]d[θ]t[t] [=][ −][v][t][ = 0][,]

192 _and F_ _, ℓ_ _are convex in a neighborhood θt, then θt is a local minimum of F in the Pareto closure {θt}, that is,_

193 _there exists a neighborhood of θt in which there exists no point θ[′]_ _such that F_ (θ[′]) < F (θt) and ℓ(θ[′]) ⪯ **_ℓ(θt)._**

194195 unconstrained local minimum ofOn the other hand, if θt ∈Pe, we have F when v Ft = is locally convex on ∇F (θt), and hence a fixed point with θt. [d]d[θ]t[t] [=][ −][v][t][ = 0][ is an]

**Theorem 3. Let ϵ > e and assume gϵ := supθ** _g(θ): θ_ _ϵ_ _< +_ _and supt_ 0 αt < _. Under_
_{_ _∈P_ _}_ _∞_ _≥_ _∞_
_Assumption 1, when we initialize from θ0_ _ϵ, we have_
_∈P_

2 _t_

dθs

min + [1] _αs (αsgϵ + c[√]gϵ) ds._
_s∈[0,t]_ ds _≤_ _[F]_ [(][θ][0][)]t[ −] _[F][ ∗]_ _t_ Z0

196 _In particular, if we have αt = α = const, then mins_ [0,t] dθs/ds = 1/t + α[√]gϵ _._
_∈_ _∥_ _∥[2]_ _O_

197 _If_ _∞0_ _αt[γ][d][t <][ +][∞]_ _[for some][ γ][ ≥]_ [1][, we have][ min]s∈[0,t] _[∥][d][θ][s][/][d][s][∥][2][ =][ O][(1][/t] _ [ +][ √][g][ϵ][/t][1][/γ][)][.]

198 Combining the results in TheoremR 2 and 3, we can see that the choice of sequence {αt : t ∈ R+} controls how

199 fast we want to decrease ℓ vs. F . Large αt yields faster descent on ℓ, but slower descent on F . Theoretically,

200 using a sequence that satisfies _αtdt = +∞_ and _αt[γ][d][t <][ +][∞]_ [for some][ γ >][ 1][ allows us to ensure that]

201 both mins [0,t] g(θs) and mins [0,t] dθ/ds converge to zero. If we use a constant sequence αt = α, it

202 introduces an∈ _O(α[√]gϵ) term that does not vanish asR∈_ _∥_ _∥[2]_ R t → +∞. However, we can expect that gϵ is small when

203 _ϵ is small for well-behaved functions. In practice, we find that constant αt works sufficiently well._


204 5 EMPIRICAL RESULTS

205 We introduce three applications of OPT-in-Pareto with PNG: Singleton Preference, Pareto approximation and

206 improving multi-task based domain generalization method. We also conduct additional study on how the

207 learning dynamics of PNG changes with different initialization and hyper-parameters (αt and e), which are

208 included in Appendix C.3. Other additional results that are related to the experiments in Section 5.1 and 5.2

209 and are included in the Appendix will be introduced later in their corresponding sections.


210 5.1 FINDING PREFERRED PARETO MODELS

211 We consider the synthetic example used in Lin et al. (2019); Mahapatra & Rajan (2020), which consists of

212 two losses: ℓ1(θ) = 1 exp( _θ_ _η_ ) and ℓ2(θ) = 1 exp( _θ + η_ ), where η = n[−][1][/][2] and n = 10
_−_ _−∥_ _−_ _∥[2]_ _−_ _−∥_ _∥[2]_

213 is dimension of the parameter θ.

214 **Ratio-based Criterion We first show that PNG can solve the search problem under the ratio constraint of**

215 objectives in Mahapatra & Rajan (2020), i.e., finding a point θ Ω with Ω= _θ : r1ℓ1(θ) = r2ℓ2(θ) =_
_∈P_ _[∗]_ _∩_ _{_

216 _... = rmℓm(θ)_, given some preference vector r = [r1, ..., rm]. We apply PNG with the non-uniformity
_}_

217 score defined in Mahapatra & Rajan (2020) as the criterion, and compare with their algorithm called exact

218 Pareto optimization (EPO). We show in Figure 1(a)-(b) the trajectory of PNG and EPO for searching models

219 with different preference vector r, starting from the same randomly initialized point. Both PNG and EPO

220 converge to the correct solutions but with different trajectories. This suggests that PNG is able to achieve

221 the same functionality of finding ratio-constraint Pareto models as Mahapatra & Rajan (2020); Kamani et al.

222 (2021) do but being versatile to handle general criteria. We refer readers to Appendix C.1.1 for more results

223 with different choices of hyper-parameters and the experiment details.

224 **Other Criteria We demonstrate that PNG is able to find solutions for general choices of F** . We consider

225 the following designs of F : 1) weighted ℓ2 distance w.r.t. a reference vector r ∈ R[m]+ [, that is,][ F][wd][(][θ][) =]


-----

p p

l2

1.0

0.8

0.6


task preference


task preference


weighted distance


complex cosine


1.0

0.8


1.0

0.8


1.0

0.8

0.6

0.4

0.2


0.6

0.4


0.6

0.4


0.4

0.2

0.0


0.2

0.0


0.2

0.0


0.0

|Col1|Col2|Pareto Front|
|---|---|---|
|||EPO|
||||
||||
||||
||||
||||

|Col1|Col2|Pareto Front|
|---|---|---|
|||PNG|
||||
||||
||||
||||
||||

|Col1|Col2|
|---|---|
|||
|||
|||
|Pareto Front||
|PNG Target||

|Col1|Col2|Col3|
|---|---|---|
||||
||||
||Pareto Front||
||||
||PNG Target||


Pareto Front
PNG
Target


Pareto Front
PNG
Target


0.0 0.2 (a)0.4 0.6 0.8 1.0 l1 0.0 0.2 0.4(b) 0.6 0.8 1.0 l1 0.0 0.2 (c)0.4 0.6 0.8 1.0 l1 0.0 0.2 (d)0.4 0.6 0.8 1.0 l1

Figure 1: (a)-(b): the trajectory of finding Pareto models that satisfy different ratio constraints (shown in
different colors) on the two objectives ℓ1, ℓ2 using EPO and PNG; we can see that PNG can achieve the
same goal as EPO (with different trajectories) while being a more general approach. (c)-(d): the trajectory of
finding Pareto models that minimize the weighted distance and complex cosine criterion using PNG. The
green dots indicate the converged models. We can see that PNG can successfully locate the correct Pareto
models that minimize different criteria.

_m_
226
_i=1[(][ℓ][i][(][θ][)][ −]_ _[r][i][)][2][/r][i][; and 2) complex cosine: in which][ F][ is a complicated function related to the cosine]_

227228 of task objectives, i.e.,Pdistance can be viewed as finding a Pareto model that has the losses close to some target value Fcs = − cos (π(ℓ1(θ) − _r1)/2) + (cos(π(ℓ(θ2) −_ _r2)) + 1)[2]. Here the weighted r, which can be ℓ2_

229 viewed as an alternative approach to partition the Pareto set. The design of complex cosine aims to test whether

230 PNG is able to handle a very non-linear criterion function. In both cases, we take r1 = [0.2, 0.4, 0.6, 0.8] and

231 _r2 = 1 −_ _r1. We show in Fig 1(c)-(d) the trajectory of PNG. As we can see, PNG is able to correctly find the_

232 optimal solutions of OPT-in-Pareto. We also test PNG on a more challenging ZDT2-variant used in Ma et al.

233 (2020) and a larger scale MTL problem (Liu et al., 2019). We refer readers to Appendix C.1.2 and C.1.3 for

234 the setting and results.


235 5.2 FINDING DIVERSE PARETO MODELS

236 **Setup We consider the problem of finding diversified points from the Pareto set by minimizing the energy**

237 distance criterion in Equation (6). We use the same setting as Lin et al. (2019); Mahapatra & Rajan (2020).

238 We consider three benchmark datasets: (1) MultiMNIST, (2) MultiFashion, and (3) MultiFashion+MNIST.

239 For each dataset, there are two tasks (classifying the top-left and bottom-right images). We consider LeNet

240 with multihead and train N = 5 models to approximate the Pareto set. For baselines, we compare with linear

241 scalarization, MGD (Sener & Koltun, 2018), and EPO (Mahapatra & Rajan, 2020). For the MGD baseline,

242 we find that naively running it leads to poor performance as the learned models are not diversified and thus we

243 initialize the MGD with 60-epoch runs of linear scalarization with equally distributed preference weights and

244 runs MGD for the later 40 epoch. We refer the reader to Appendix C.2.1 for more details of the experiments.

245 **Metric and Result We measure the quality of how well the found models {θ1, . . ., θN** _} approximate the_

246 Pareto set using two standard metrics: Inverted Generational Distance Plus (IGD+) (Ishibuchi et al., 2015)

247 and hypervolume (HV) (Zitzler & Thiele, 1999); see Appendix C.2.2 for their definitions. We run all the

248 methods with 5 independent trials and report the averaged value and its standard deviation in Table 1. We

249 report the scores calculated based on loss (cross-entropy) and accuracy on the test set. The bolded values

250 indicate the best result with p-value less than 0.05 (using matched pair t-test). In most cases, PNG improves

251 the baselines by a large margin. We include ablation studies in Appendix C.2.3 and additional comparisons

252 with the second-order approach proposed by Ma et al. (2020) in Appendix C.2.4.


253 5.3 APPLICATION TO MULTI-TASK BASED DOMAIN GENERALIZATION ALGORITHM

254 JiGen (Carlucci et al., 2019b) learns a domain generalizable model by learning two tasks based on linear

255 scalarization, which essentially searches for a model in the Pareto set and requires choosing the weight of


-----

p p

|Data|Method|Loss HV↑(10−2) IGD+↓(10−2)|Acc HV↑(10−2) IGD+↓(10−2)|
|---|---|---|---|
|Multi-MNIST|Linear MGD EPO PNG|7.48 ± 0.11 0.14 ± 0.034 7.69 ± 0.10 0.051 ± 0.011 7.87±0.16 0.069 ± 0.028 7.86±0.11 0.042±0.012|9.27 ± 0.024 0.036 ± 0.0084 9.27 ± 0.023 0.0078 ± 0.0010 9.17 ± 0.032 0.065 ± 0.018 9.39±0.036 0.0056±0.0022|
|Multi-Fashion|Linear MGD EPO PNG|0.38 ± 0.059 0.13 ± 0.013 0.42 ± 0.064 0.046 ± 0.016 0.36 ± 0.058 0.31 ± 0.11 0.47±0.066 0.016±0.0022|4.76 ± 0.019 0.064 ± 0.012 4.77 ± 0.019 0.023±0.0030 4.78 ± 0.030 0.21 ± 0.020 4.81±0.021 0.023±0.0031|
|Fashion-MNIST|Linear MGD EPO PNG|5.01 ± 0.057 0.167 ± 0.054 5.09 ± 0.069 0.060 ± 0.029 4.60 ± 0.166 0.233 ± 0.054 5.27±0.054 0.048±0.027|8.46 ± 0.046 0.110 ± 0.035 8.40 ± 0.045 0.049±0.011 8.12 ± 0.041 0.385 ± 0.077 8.53±0.047 0.046±0.022|



Table 1: Results of approximating the Pareto set by different methods on three MNIST benchmark datasets.
The numbers in the table are the averaged value and the standard deviation. Bolded values indicate the
statistically significant best result with p-value less than 0.5 based on matched pair t-test.

|PACS|art paint cartoon sketches photo|Avg|
|---|---|---|
|D-SAM DeepAll|0.7733 0.7243 0.7783 0.9530 0.7785 0.7486 0.6774 0.9573|0.8072 0.7905|
|JiGen JiGen+adv JiGen+PNG|0.8009 ± 0.004 0.7363 ± 0.007 0.7046 ± 0.013 0.9629±0.002 0.7923 ± 0.006 0.7402 ± 0.004 0.7188 ± 0.005 0.9617 ± 0.001 0.8014±0.005 0.7538±0.001 0.7222±0.006 0.9627±0.002|0.8012 ± 0.002 0.8033 ± 0.001 0.8100±0.005|



Table 2: Comparing different methods for domain generalization on PACS using ResNet-18. The values in
table are the testing accuracy with its standard deviation. The bolded values are the best models with p-value
less than 0.1 based on match-pair t-test.

256 linear scalarization carefully. It is thus natural to study whether there is a better mechanism that dynamically

257 adjusts the weights of the two losses so that we eventually learn a better model. Motivated by the adversarial

258 feature learning (Ganin et al., 2016), we propose to improve JiGen such that the latent feature representations

259 of the two tasks are well aligned. This can be framed into an OPT-in-Pareto problem where the criterion is

260 the discrepancy of the latent representations (implemented using an adversarial discrepancy module in the

261 network) of the two tasks. PNG is applied to solve the optimization. We evaluate the methods on PACS (Li

262 et al., 2017), which covers 7 object categories and 4 domains (Photo, Art Paintings, Cartoon, and Sketches).

263 The model is trained on three domains and tested on the rest of them. Our approach is denoted as JiGen+PNG

264 and we also include JiGen + adv, which simply adds the adversarial loss as regularization and two other

265 baseline methods (D-SAM (D’Innocente & Caputo, 2018) and DeepAll (Carlucci et al., 2019b)). For the three

266 JiGen based approaches, we run 3 independent trials and for the other two baselines, we report the results in

267 their original papers. Table 2 shows the result using ResNet-18, which demonstrates the improvement by the

268 application of the OPT-in-Pareto framework. We also include the results using AlexNet in the Appendix. We

269 refer readers to Appendix C.4 for the additional results and more experiment details.


270 6 CONCLUSION

271 This paper studies the OPT-in-Pareto, a problem that has been studied in operation research with restrictive

272 linear or convexity assumption but largely under-explored in deep learning literature, in which the objectives

273 are non-linear and non-convex. Applying algorithms such as manifold gradient descent requires eigen
274 computation of the Hessian matrix at each iteration and thus can be expensive. We propose a first-order

275 approximation algorithm called Pareto Navigation Gradient Descent (PNG) with theoretically guaranteed

276 descent and convergence property to solve OPT-in-Pareto.


-----

p p


277 REFERENCES

278 Isabela Albuquerque, Nikhil Naik, Junnan Li, Nitish Keskar, and Richard Socher. Improving out-of
279 distribution generalization via multi-task self-supervised pretraining. arXiv preprint arXiv:2003.13525,

280 2020.

281 Mario Bernardo, Chris Budd, Alan Richard Champneys, and Piotr Kowalczyk. Piecewise-smooth dynamical

282 _systems: theory and applications, volume 163. Springer Science & Business Media, 2008._

283 Silvere Bonnabel. Stochastic gradient descent on riemannian manifolds. IEEE Transactions on Automatic

284 _Control, 58(9):2217–2229, 2013._

285 Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge university press,

286 2004.

287 Fabio M. Carlucci, Antonio D’Innocente, Silvia Bucci, Barbara Caputo, and Tatiana Tommasi. Domain

288 generalization by solving jigsaw puzzles. In Proceedings of the IEEE/CVF Conference on Computer Vision

289 _and Pattern Recognition (CVPR), June 2019a._

290 Fabio M Carlucci, Antonio D’Innocente, Silvia Bucci, Barbara Caputo, and Tatiana Tommasi. Domain

291 generalization by solving jigsaw puzzles. In Proceedings of the IEEE/CVF Conference on Computer Vision

292 _and Pattern Recognition, pp. 2229–2238, 2019b._

293 Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient normalization

294 for adaptive loss balancing in deep multitask networks. In International Conference on Machine Learning,

295 pp. 794–803. PMLR, 2018.

296 Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning Chai, and Dragomir

297 Anguelov. Just pick a sign: Optimizing deep multitask models with gradient sign dropout. In H. Larochelle,

298 M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing

299 _[Systems, volume 33, pp. 2039–2050. Curran Associates, Inc., 2020. URL https://proceedings.](https://proceedings.neurips.cc/paper/2020/file/16002f7a455a94aa4e91cc34ebdb9f2d-Paper.pdf)_

300 [neurips.cc/paper/2020/file/16002f7a455a94aa4e91cc34ebdb9f2d-Paper.pdf.](https://proceedings.neurips.cc/paper/2020/file/16002f7a455a94aa4e91cc34ebdb9f2d-Paper.pdf)

301 Timo M Deist, Monika Grewal, Frank JWM Dankers, Tanja Alderliesten, and Peter AN Bosman.

302 Multi-objective learning to predict pareto fronts using hypervolume maximization. _arXiv preprint_

303 _arXiv:2102.04523, 2021._

304 Stephan Dempe. Bilevel optimization: theory, algorithms and applications. TU Bergakademie Freiberg,

305 Fakultät für Mathematik und Informatik, 2018.

306 Jean-Antoine Désidéri. Multiple-gradient descent algorithm (mgda) for multiobjective optimization. Comptes

307 _Rendus Mathematique, 350(5-6):313–318, 2012._

308 Qi Dou, Daniel C Castro, Konstantinos Kamnitsas, and Ben Glocker. Domain generalization via model
309 agnostic learning of semantic features. arXiv preprint arXiv:1910.13580, 2019.

310 Antonio D’Innocente and Barbara Caputo. Domain generalization with domain-specific aggregation modules.

311 In German Conference on Pattern Recognition, pp. 187–198. Springer, 2018.

312 Joseph G Ecker and Jung Hwan Song. Optimizing a linear function over an efficient set. Journal of

313 _Optimization Theory and Applications, 83(3):541–563, 1994._

314 Christopher Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil, and Chelsea Finn. Measuring and

315 harnessing transference in multi-task learning. arXiv preprint arXiv:2010.15413, 2020.


10


-----

p p


316 Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International

317 _conference on machine learning, pp. 1180–1189. PMLR, 2015._

318 Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette,

319 Mario March, and Victor Lempitsky. Domain-adversarial training of neural networks. Journal of Machine

320 _[Learning Research, 17(59):1–35, 2016. URL http://jmlr.org/papers/v17/15-239.html.](http://jmlr.org/papers/v17/15-239.html)_

321 DP Hardin and EB Saff. Discretizing manifolds via minimum energy points. Notices of the AMS, 51(10):

322 1186–1194, 2004.

323 Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. Using self-supervised learning can

324 improve model robustness and uncertainty. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc,

325 E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Cur
326 [ran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/](https://proceedings.neurips.cc/paper/2019/file/a2b15837edac15df90721968986f7f8e-Paper.pdf)

327 [a2b15837edac15df90721968986f7f8e-Paper.pdf.](https://proceedings.neurips.cc/paper/2019/file/a2b15837edac15df90721968986f7f8e-Paper.pdf)

328 Claus Hillermeier. Generalized homotopy approach to multiobjective optimization. Journal of Optimization

329 _Theory and Applications, 110(3):557–583, 2001._

330 Hisao Ishibuchi, Hiroyuki Masuda, Yuki Tanigaki, and Yusuke Nojima. Modified distance calculation in

331 generational distance and inverted generational distance. In International conference on evolutionary

332 _multi-criterion optimization, pp. 110–125. Springer, 2015._

333 Adrián Javaloy and Isabel Valera. Rotograd: Dynamic gradient homogenization for multi-task learning. arXiv

334 _preprint arXiv:2103.02631, 2021._

335 Jesús M Jorge. A bilinear algorithm for optimizing a linear function over the efficient set of a multiple

336 objective linear programming problem. Journal of Global Optimization, 31(1):1–16, 2005.

337 Mohammad Mahdi Kamani, Rana Forsati, James Z Wang, and Mehrdad Mahdavi. Pareto efficient fairness in

338 supervised learning: From extraction to tracing. arXiv preprint arXiv:2104.01634, 2021.

339 Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses for

340 scene geometry and semantics. In Proceedings of the IEEE conference on computer vision and pattern

341 _recognition, pp. 7482–7491, 2018._

342 Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In International

343 _Conference on Machine Learning, pp. 1885–1894. PMLR, 2017._

344 Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M. Hospedales. Deeper, broader and artier domain

345 generalization. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), Oct

346 2017.

347 Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Hospedales. Learning to generalize: Meta-learning for

348 domain generalization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32,

349 2018a.

350 Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao. Deep

351 domain generalization via conditional invariant adversarial networks. In Proceedings of the European

352 _Conference on Computer Vision (ECCV), pp. 624–639, 2018b._

353 Xi Lin, Hui-Ling Zhen, Zhenhua Li, Qingfu Zhang, and Sam Kwong. Pareto multi-task learning. arXiv

354 _preprint arXiv:1912.12854, 2019._

11


-----

p p


355 Xi Lin, Zhiyuan Yang, Qingfu Zhang, and Sam Kwong. Controllable pareto multi-task learning. arXiv

356 _preprint arXiv:2010.06313, 2020._

357 Shikun Liu, Edward Johns, and Andrew J Davison. End-to-end multi-task learning with attention. In

358 _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1871–1880,_

359 2019.

360 Zhengliang Liu and Matthias Ehrgott. Primal and dual algorithms for optimization over the efficient set.

361 _Optimization, 67(10):1661–1686, 2018._

362 Xiaoyuan Luo, Shaolei Liu, Kexue Fu, Manning Wang, and Zhijian Song. A learnable self-supervised task

363 for unsupervised domain adaptation on point clouds. arXiv preprint arXiv:2104.05164, 2021.

364 Pingchuan Ma, Tao Du, and Wojciech Matusik. Efficient continuous pareto exploration in multi-task learning.

365 In International Conference on Machine Learning, pp. 6522–6531. PMLR, 2020.

366 Debabrata Mahapatra and Vaibhav Rajan. Multi-task learning with user preferences: Gradient descent with

367 controlled ascent in pareto optimization. In International Conference on Machine Learning, pp. 6597–6607.

368 PMLR, 2020.

369 Aviv Navon, Aviv Shamsian, Gal Chechik, and Ethan Fetaya. Learning the pareto front with hypernetworks.

370 _arXiv preprint arXiv:2010.04104, 2020._

371 Javad Sadeghi and Hossein Mohebi. Solving optimization problems over the weakly efficient set. Numerical

372 _Functional Analysis and Optimization, pp. 1–33, 2021._

373 Ozan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. In S. Bengio, H. Wallach,

374 H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Pro
375 _[cessing Systems, volume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.](https://proceedings.neurips.cc/paper/2018/file/432aca3a1e345e339f35a30c8f65edce-Paper.pdf)_

376 [cc/paper/2018/file/432aca3a1e345e339f35a30c8f65edce-Paper.pdf.](https://proceedings.neurips.cc/paper/2018/file/432aca3a1e345e339f35a30c8f65edce-Paper.pdf)

377 Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support

378 inference from rgbd images. In European conference on computer vision, pp. 746–760. Springer, 2012.

379 Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Do
380 gus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning

381 with consistency and confidence. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and

382 H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 596–608. Cur
383 [ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/](https://proceedings.neurips.cc/paper/2020/file/06964dce9addb1c5cb5d6e3d9838f733-Paper.pdf)

384 [06964dce9addb1c5cb5d6e3d9838f733-Paper.pdf.](https://proceedings.neurips.cc/paper/2020/file/06964dce9addb1c5cb5d6e3d9838f733-Paper.pdf)

385 Yu Sun, Eric Tzeng, Trevor Darrell, and Alexei A Efros. Unsupervised domain adaptation through self
386 supervision. arXiv preprint arXiv:1909.11825, 2019.

387 Phan Thien Thach and TV Thang. Problems with resource allocation constraints and optimization over the

388 efficient set. Journal of Global Optimization, 58(3):481–495, 2014.

389 Sen Wu, Hongyang R. Zhang, and Christopher Ré. Understanding and improving information transfer

390 [in multi-task learning. In International Conference on Learning Representations, 2020. URL https:](https://openreview.net/forum?id=SylzhkBtDB)

391 [//openreview.net/forum?id=SylzhkBtDB.](https://openreview.net/forum?id=SylzhkBtDB)

392 Sang Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu Ma, and Percy Liang. In-n-out: Pre
393 training and self-training using auxiliary information for out-of-distribution robustness. In International

394 _[Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=](https://openreview.net/forum?id=jznizqvr15J)_

395 [jznizqvr15J.](https://openreview.net/forum?id=jznizqvr15J)

12


-----

p p

396 Junfeng Yang and Carl Vondrick. Multitask learning strengthens adversarial robustness. 2020.

397 Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gra
398 dient surgery for multi-task learning. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and

399 H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 5824–5836. Cur
400 [ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/](https://proceedings.neurips.cc/paper/2020/file/3fe78a8acf5fda99de95303940a2420c-Paper.pdf)

401 [3fe78a8acf5fda99de95303940a2420c-Paper.pdf.](https://proceedings.neurips.cc/paper/2020/file/3fe78a8acf5fda99de95303940a2420c-Paper.pdf)

402 Linfeng Zhang, Muzhou Yu, Tong Chen, Zuoqiang Shi, Chenglong Bao, and Kaisheng Ma. Auxiliary training:

403 Towards accurate and robust models. In Proceedings of the IEEE/CVF Conference on Computer Vision

404 _and Pattern Recognition, pp. 372–381, 2020._

405 Eckart Zitzler and Lothar Thiele. Multiobjective evolutionary algorithms: a comparative case study and the

406 strength pareto approach. IEEE transactions on Evolutionary Computation, 3(4):257–271, 1999.


13


-----

p p

407 A THEORETICAL ANALYSIS

**Theorem 1 [Dual of Equation (7)]** _The solution vt of Equation (7), if it exists, has a form of_


_vt = ∇F_ (θt) +


_λi,t_ _ℓi(θt),_
_∇_
_i=1_

X


_with {λi,t}i[m]=1_ _[the solution of the following dual problem]_


_m_ _m_

max _λt_ _ℓi(θt)_ + _λiφt,_
_λ_ R[m]+ _−_ 2[1] _∇_
_∈_ _i=1_ _i=1_

X X

408 _where R[m]+_ _[is the set of nonnegative][ m][-dimensional vectors, that is,][∇][F]_ [(][θ][t][) +] [ R]+[m] [=][ {][λ][ ∈] [R][m][ :][ λ][i] _[≥]_ [0][,][ ∀][i][ ∈] [[][m][]][}][.]


409 _Proof. By introducing Lagrange multipliers, the optimization in Equation (7) is equivalent to the following_

410 minimax problem:


1

min _λi_ _φt_ _ℓi(θt)[⊤]v_ _._
_v∈R[n][ max]λ∈R[m]+_ 2 _[∥∇][F]_ [(][θ][t][)][ −] _[v][∥][2][ +]_ _i=1_ _−∇_

X   

With strong duality of convex quadratic programming (assuming the primal problem is feasible), we can
exchange the order of min and max, yielding


1

max Φ(λ) := min _λi_ _φt_ _ℓi(θt)[⊤]v_ _._
_λ∈R[m]+_ ( _v∈R[n]_ 2 _[∥∇][F]_ [(][θ][t][)][ −] _[v][∥][2][ +]_ _i=1_ _−∇_ )

X   

411 It is easy to see that the minimization w.r.t. v is achieved when v = ∇F (θt) + _i=1_ _[λ][i][∇][ℓ][i][(][θ][t][)][. Correspond-]_

412 ingly, the Φ(λ) has the following dual form:

_m_ 2 _m_ [P][m]

max _λi_ _ℓi(θt)_ + _λiφt._
_λ_ R[m]+ _−_ 2[1] _∇_
_∈_ _i=1_ _i=1_

X X

413 This concludes the proof.

_[∇][F]_ [(][θ][t][) +]

**Theoremwhen θte** **2 [Pareto Improvement one, then for any time t < te ℓ,]** _Under Assumption 1, assume θ0 ̸∈Pe, and te is the first time_
_∈P_

d _i_ [)]

min _t_ _._
dt _[ℓ][i][(][θ][t][)][ ≤−][α][t][g][(][θ][t][)][,]_ _s_ [0,t] _[g][(][θ][t][)][ ≤]_ [min][i][∈][[][m][]][(][ℓ][i][(][θ][0][)][ −] _[ℓ][∗]_
_∈_ 0 _[α][s][d][s]_

414 _Therefore, the update yields Pareto improvement on ℓ_ _when θt_ _e and αtgR(θt) > 0._

_t_ _̸∈P_
415 _Further, if_ 0 _[α][s][d][s][ = +][∞][, then for any][ ϵ > e][, there exists a finite time][ t][ϵ][ ∈]_ [R][+][ on which the solution enters]

416 _ϵ and stays within_ _ϵ afterwards, that is, we have θtϵ_ _ϵ and θt_ _ϵ for any t_ _tϵ._
_P_ R _P_ _∈P_ _∈P_ _≥_

_Proof. i) When t < te, we have g(θt) > e and hence_

d

(11)
dt _[ℓ][i][(][θ][t][) =][ −∇][ℓ][i][(][θ][t][)][⊤][v][t][ ≤−][φ][t][ =][ −][α][t][g][(][θ][t][)][,]_


417418 where we used the constraint oflosses _ℓi_ when αtg(θt) > 0. _∇ℓi(θt)[⊤]vt ≥_ _φt in Equation (7). Therefore, we yield strict decent on all the_

_{_ _}_

14


-----

p p

ii) Integrating both sides of Equation (11):

_t_
0 _[α][s][g][(][θ][s][)d][s]_

min _t_ _t_ _t_ _._
_s_ [0,t] _[g][(][θ][s][)][ ≤]_ _≤_ _[ℓ][i][(][θ][0][)][ −]_ _[ℓ][i][(][θ][t][)]_ _≤_ _[ℓ][i][(][θ][0][)][ −]_ _[ℓ][∗]_
_∈_ R 0 _[α][s][d][s]_ 0 _[α][s][d][s]_ 0 _[α][s][d][s]_

419 This yields the result since it holds for everyR _i_ [m]. R R
_∈_

420 If _∞0_ _αtdt = +∞, then we have mins∈[0,t] g(θs) →_ 0 when t → +∞. Assume there exists an ϵ > e,

421 such that θt never enters _ϵ at finite t. Then we have g(θt)_ _ϵ for t_ R+, which contradicts with

R _P_ _≥_ _∈_

422 mins [0,t] g(θs) 0.
_∈_ _→_

423 iii) Assume there exists a finite time t[′] (tϵ, + ) such that θt′ _ϵ. Because ϵ > e and g is continuous,_ _e_
_∈_ _∞_ _̸∈P_ _P_

424425 is in the interior ofpoint, that is, there exists a point Pϵ ⊆P _ϵ. Therefore, the trajectory leading to t[′′]_ _∈_ [tϵ, t[′]), such that {θt : t ∈ [ θt[′′]t′, t ̸∈P[′]]} ̸∈Pϵ must pass throughe. But because the algorithm can P _ϵ \ Pe at some_

426 not increase any objective ℓi outside of Pe, we must have ℓ(θt′ ) ⪯ **_ℓ(θt′′_** ), yielding that θt′ ∈{θt′′ _} ⊆P_ _ϵ,_

427 where _θt′′_ is the Pareto closure of _θt′′_ ; this contradicts with the assumption.
_{_ _}_ _{_ _}_

428 **Lemma 1** _Under Assumption 1, assume θt ̸∈Pe is a fixed point of the algorithm, that is,_ [d]d[θ]t[t] [=][ −][v][t][ = 0][,]

429 _and F_ _, ℓ_ _are convex in a neighborhood θt, then θt is a local minimum of F in the Pareto closure {θt},_

430 _that is, there exists a neighborhood of θt in which there exists no point θ[′]_ _such that F_ (θ[′]) < F (θt) and

431 **_ℓ(θ[′])_** **_ℓ(θt)._**
_⪯_

_Proof. Note that minimizing F in_ _θt_ can be framed into a constrained optimization problem:
_{_ _}_

min _F_ (θ) _s.t._ _ℓi(θ)_ _ℓi(θt),_ _i_ [m].
_θ_ _≤_ _∀_ _∈_

432 In addition, by assumption, θ = θt satisfies vt = ∇F (θt) + _i=1_ _[λ][i,t][∇][ℓ][i][(][θ][t][) = 0][, which is the KKT]_

433 stationarity condition of the constrained optimization. It is also obvious to check that θ = θt satisfies the

434 feasibility and slack condition trivially. Combining this with the local convexity assumption yields the

[P][m]

435 result.

**Theorem 3 [Optimization of F** **]** _Let ϵ > e and assume gϵ := supθ_ _g(θ): θ_ _ϵ_ _< +_ _and_
_{_ _∈P_ _}_ _∞_
supt 0 αt < _. Under Assumption 1, when we initialize from θ0_ _ϵ, we have_
_≥_ _∞_ _∈P_

2 _t_

dθs

min + [1] _αs (αsgϵ + c[√]gϵ) ds._
_s∈[0,t]_ ds _≤_ _[F]_ [(][θ][0][)]t[ −] _[F][ ∗]_ _t_ Z0

436 _In particular, if we have αt = α = const, then mins_ [0,t] dθs/ds = 1/t + α[√]gϵ _._
_∈_ _∥_ _∥[2]_ _O_

437 _If_ _∞0_ _αt[γ][d][t <][ +][∞]_ _[for some][ γ][ ≥]_ [1][, we have][ min]s∈[0,t] _[∥][d][θ][s][/][d][s][∥][2][ =][ O][(1][/t] _ [ +][ √][g][ϵ][/t][1][/γ][)][.]
R

_Proof. i) The slack condition of the constrained optimization in Equation (7) says that_


_λi,t_ _ℓi(θt)[⊤]vt_ _φt_ = 0, _i_ [m]. (12)
_∇_ _−_ _∀_ _∈_
  


This gives that


_m_ _⊤_

_λi,t_ _ℓi(θt)_
_i=1_ _∇_ !

X


_vt_ =
_∥_ _∥[2]_


_F_ (θt) +
_∇_


_vt_


= ∇F (θt)[⊤]vt +


_λi,tφt_ //plugging Equation (12). (13)
_i=1_

X

15


-----

p p

If θt ̸∈Pe, we have φt = αtg(θt) and this gives

d

dt _[F]_ [(][θ][t][) =][ −∇][F] [(][θ][t][)][⊤][v][t][ =][ −∥][v][t][∥][2][ +]


_m_

dθt
_λi,tφt =_
_−_ dt
_i=1_

X


_λi,tαtg(θt)_
_i=1_

X


If θt is in the interior of Pe, then we run typical gradient descent of F and hence has

2

d dθt

_._

dt _[F]_ [(][θ][t][) =][ −∥][v][t][∥][2][ =][ −] dt

If θt is on the boundary of Pe, then by the definition of differential inclusion, dθ/dt belongs to the convex
hull of the velocities that it receives from either side of the boundary, yielding that


d dθt

dt _[F]_ [(][θ][t][) =][ −] dt


_m_

dθt
_λi,tαtg(θt)_
_≤−_ dt
_i=1_

X


+ β


_λi,tαtg(θt),_
_i=1_

X


where β ∈ [0, 1]. Combining all the cases gives

d dθt

dt _[F]_ [(][θ][t][)][ ≤−] dt


_λi,tαtg(θt)._
_i=1_

X


Integrating this yields


_t_

0

Z


_t_

0

Z

_t_

0

Z


ds
_≤_ _[F]_ [(][θ][0][)]t[ −] _[F][ ∗]_

_≤_ _[F]_ [(][θ][0][)]t[ −] _[F][ ∗]_


dθs

ds


_≤_ [1]t


dθs

ds


+ [1]

_t_

+ [1]


min
_s∈[0,t]_


_λi,sαsg(θs)ds_
_i=1_

X

_αs (αsgϵ + c[√]gϵ) ds,_


where the last step used Lemma 2 with φt = αtg(θt):

_m_

_λi,tαtg(θt)_ _αt[2][g][(][θ][t][) +][ cα][t]_
_≤_
_i=1_

X


_g(θt)_ _αt[2][g][ϵ]_ [+][ cα][t]√gϵ,
_≤_


438 and here we used g(θt) ≤ _gϵ because the trajectory is contained in P_ _ϵ following Theorem 2._

439 The remaining results follow Lemma 4.

440 A.0.1 TECHNICAL LEMMAS

**Lemma 2. Assume Assumption 1 holds. Define g(θ) = minω∈Cm ∥[P]i[m]=1** _[ω][i][∇][ℓ][i][(][θ][)][∥][2][, where][ C][m][ is the]_
_probability simplex on [m]. Then for the vt and λi,t defined in Equation (7) and Equation (10), we have_


_λi,tg(θt) ≤_ max _φt + c_
_i=1_

X 


_g(θt), 0_


441 _Proof. The slack condition of the constrained optimization in Equation (7) says that_

_λi,t_ _ℓi(θ)[⊤]vt_ _φt_ = 0, _i_ [m].
_∇_ _−_ _∀_ _∈_

Sum the equation over i ∈ [m] and note that  _vt = ∇F_ (θt) + _i=1_ _[λ][i,t][∇][ℓ][i][(][θ][t][)][. We get]_

_m_ 2 _m_ _⊤_ _m_

_i=1_ _λi,t∇ℓi(θt)_ + _i=1_ _λi,t∇ℓi(θt)!_ [P]∇[m]F (θ) − _i=1_ _λi,tφt = 0._ (14)

X X X


16


-----

p p


Define


_xt =_


_λi,t_ _ℓi(θt)_
_∇_
_i=1_

X


_λ¯t =_


_λi,t,_ _gt = g(θt) = min_
_i=1_ _ω∈C[m]_

X


_ωi_ _ℓi(θt)_
_∇_
_i=1_

X


Then it is easy to see that xt ≥ _λ[¯][2]t_ _[g][t][. Using Cauchy-Schwarz inequality,]_

_m_ _⊤_ _m_

_λi,t_ _ℓi(θ)_ _F_ (θt) _F_ (θt) _λi,t_ _ℓi(θ)_
_i=1_ _∇_ ! _∇_ _≤∥∇_ _∥_ _i=1_ _∇_ _[≤]_ _[c][√][x][t][,]_

X X

where we used _F_ (θt) _c by Assumption 1. Combining this with Equation (14), we have_
_∥∇_ _∥≤_
_xt_ _λ¯tφt_ _c[√]xt._
_−_ _≤_

442 Applying Lemma 3 yields the result.

**Lemma 3. Assume φ ∈** R, and x, λ, c, g ∈ R+ are non-negative real numbers and they satisfy

_x_ _λφ_ _c[√]x,_ _x_ _λ[2]g._
_|_ _−_ _| ≤_ _≥_

443 _Then we have λg ≤_ max(0, φ + c[√]g).

_Proof. Square the first equation, we get_


_f_ (x) := (x − _λφ)[2]_ _−_ _c[2]x ≤_ 0,

where f is a quadratic function. To ensure that f (x) ≤ 0 has a solution that satisfies x ≥ _λ[2]g, we need to_
have f (λ[2]g) ≤ 0, that is,
_f_ (λ[2]g) = (λ[2]g − _λφ)[2]_ _−_ _c[2]λ[2]g ≤_ 0.
444 This can hold under two cases:

445 Case 1: λ = 0;

446 Case 2: |λg − _φ| ≤_ _c[√]g, and hence φ −_ _c[√]g ≤_ _λg ≤_ _φ + c[√]g._

Under both case, we have
_λg_ max(0, φ + c[√]g).
_≤_

447

**Lemma 4. Let** _αt : t_ R+ R+ be a non-negative sequence with A := _∞0_ _αt[γ][d][t]_ 1/γ < _, where_
_{_ _∈_ _} ⊆_ _∞_
_γ ≥_ 1, and B = supt αt < ∞. Then we have  R 

_t_

1

_αs[2]_ [+][ α][s] ds (B + 1)At[−][1][/γ].

_t_ 0 _≤_

Z

  

_Proof. Let η =_ _γ_ _γ_ 1 [, so that][ 1][/η][ + 1][/γ][ = 1][. We have by Holder’s inequality,]

_−_

_t_ _t_ 1/γ _t_ 1/η

0 _αsds ≤_ 0 _αs[γ][d][s]_ 0 1[η]ds _≤_ _At[1][/η]_ = At[1][−][1][/γ].

Z Z  Z 

and hence

_t_ _t_

1

_αs[2]_ [+][ α][s] ds _αsds_ (B + 1)At[−][1][/γ].

_t_ 0 _≤_ _[B][ + 1]t_ 0 _≤_

Z Z

448   

17


-----

p p

**Algorithm 1 Pareto Navigating Gradient Descent**

1: Initialize θ0; decide the step size ξ, and the control function φ in Equation (8) (including the threshold
_e > 0 and the descending rate_ _αt_ ).
_{_ _}_

2: for iteration t do


_λi,t_ _ℓi(θt),_ (15)
_∇_
_i=1_

X


_θt+1 ←_ _θt −_ _ξvt,_ _vt = ∇F_ (θt) +


where λi,t = 0, ∀i ∈ [m] if g(θt) ≤ _e, and {λi,t}t[m]=1_ [is the solution of Equation (][10][) with][ φ][(][θ][t][) =]
_αtg(θt) when g(θt) > e._

3: end for

449 B PRACTICAL IMPLEMENTATION


450 **Hyper-parameters** Our algorithm introduces two hyperparameters {αt} and e over vanilla gradient descent.

451 We use constant sequence αt = α and we take α = 0.5 unless otherwise specified. We choose e by

_m_
452 _e = γe0, where e0 is an exponentially discounted average of_ _m[1]_ _i=1_

453 it automatically scales with the magnitude of the gradients of the problem at hand. In the experiments of this[∥∇][ℓ][i][(][θ][t][)][∥][2][ over the trajectory so that]

454 paper, we simply fix γ = 0.1 unless specified. P

455 **Solving the Dual Problem** Our method requires to calculate {λi,t}t[m]=1 [with the dual optimization problem]

456 in Equation (10), which can be solved with any off-the-shelf convex quadratic programming tool. In this

457 work, we use a very simple projected gradient descent to approximately solve Equation (10). We initialize

458 _{λi,t}t[m]=1_ [with a zero vector and terminate when the difference between the last two iterations is smaller than]

459 a threshold or the algorithm reaches the maximum number of iterations (we use 100 in all experiments).

460 The whole algorithm procedure is summarized in Algorithm 1.

461 C EXPERIMENTS

462 C.1 FINDING PREFERRED PARETO MODELS

463 C.1.1 RATIO-BASED CRITERION

The non-uniformity score from (Mahapatra & Rajan, 2020) that we use in Figure 1 is defined as


_ℓˆt(θ)_

1/m


_ℓˆt(θ) =_ _rtℓt(θ)_ (16)

_s_ [m] _[r][s][ℓ][s][(][θ][)]_ _[.]_
_∈_

P


_FNU(θ) =_


_ℓˆt(θ) log_
_t=1_

X


464 We fix the other experiment settings the same as Mahapatra & Rajan (2020) and use γ = 0.01 and α = 0.25

465 for this experiment reported in the main text. We defer the ablation studies on the hyper-parameter α and γ to

466 Section C.3.

467 C.1.2 ZDT2-VARIANT


468 We consider the ZDT2-Variant example used in Ma et al. (2020) with the same experiment setting, in

469 which the Pareto set is a cylindrical surface, making the problem more challenging. We consider the

470 same criteria, e.g. weighted distance and complex cosine used in the main context with different choices

471 of r1 = [0.2, 0.4, 0.6, 0.8]. We use the default hyper-parameter set up, choosing α = 0.5 and r = 0.1.

18


-----

p p


weighted distance

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||Pareto PNG|Front|||
|||Target||||
|||||||
|||||||
|||||||
|||||||
|||||||



0.0 0.2 0.4 0.6 0.8 1.0 l1


complex cosine

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|||||||||
||||||Pare PNG|to Front||
||||||Targ|et||
|||||||||
|||||||||
|||||||||



0.0 0.2 0.4 0.6 0.8 1.0 l1


Figure 2: Trajectories of solving OPT-in-Pareto with weighted distance and complex cosine as criterion using
PNG. The green dots are the final converged models. PNG is able to successfully locate the correct models in
the Pareto set.

472 For complex cosine, we use MGD updating for the first 150 iterations. Figure 2 shows the trajectories,

473 demonstrating that PNG works pretty well for the more challenging ZDT2-Variant tasks.


474 C.1.3 GENERAL CRITERIA: THREE-TASK LEARNING ON THE NYUV2 DATASET

475 We show that PNG is able to handle large-scale multitask learning problems by deploying it on a three
476 task learning problem (segmentation, depth estimation, and surface normal prediction) on NYUv2 dataset

477 (Silberman et al., 2012). The main goal of this experiment is to show that: 1. PNG is able to handle

478 OPT-in-Pareto in a large-scale neural network; 2. With a proper design of criteria, PNG enables to do

479 targeted fine-tuning that pushes the model to move towards a certain direction. We consider the same

480 training protocol as Liu et al. (2019) and use the MTAN network architecture. Start with a model trained

481 with equally weighted linear scalarization and our goal is to further improve the model’s performance

482 on segmentation and surface normal estimation while allowing some sacrifice on depth estimation. This

483 can be achieved by many different choices of criterion and in this experiment, we consider the following

484 design: F (θ) = (ℓseg(θ) × ℓsurface(θ))/(0.001 + ℓdepth(θ)). Here ℓseg, ℓsurface and ℓdepth are the loss functions

485 for segmentation, surface normal prediction and depth estimation, respectively. The constant 0.001 in the

486 denominator is for numeric stability. We point out that our design of criterion is a simple heuristic and might

487 not be an optimal choice and the key question we study here is to verify the functionality of the proposed

488 PNG. As suggested by the open-source repository of Liu et al. (2019), we reproduce the result based on the

489 provided configuration. To show that PNG is able to move the model along the Pareto front, we show the

490 evolution of the criterion function and the norm of the MGD gradient during the training in Figure 3. As we

491 can see, PNG effectively decreases the value of criterion function while the norm of MGD gradient remains

492 the same. This demonstrates that PNG is able to minimize the criterion by searching the model in the Pareto

493 set. Table 3 compares the performances on the three tasks using standard training and PNG, showing that

494 PNG is able to improve the model’s performance on segmentation and surface normal prediction tasks while

495 satisfying a bit of the performance in depth estimation based on the criterion.


496 C.2 FINDING DIVERSE PARETO MODELS

497 C.2.1 EXPERIMENT DETAILS


19


-----

p p

|Algorithm|Segmentation Depth Surface Normal|
|---|---|
||Angle Distance (Higher Better) (Lower Better) Within t◦ (Lower Better) mIoU Pix Acc Abs Err Rel Err Mean Median 11.25 22.5 30|
|Standard PNG|27.09 56.36 0.6143 0.2618 31.46 27.37 19.51 41.71 54.61 28.23 56.66 0.6161 0.2632 31.06 26.50 21.06 43.41 55.93|



Table 3: Comparing the multitask performance of standard training using linear scalarization with equally
weighted losses and the targeted fine-tuning based on PNG.


498 We train the model for 100 epochs using Adam op
499 timizer with batch size 256 and 0.001 learning rate.

500 To encourage diversity of the models, following the

501 setting in Mahapatra & Rajan (2020), we use equally

502 distributed preference vectors for linear scalarization

503 and EPO. Note that the stochasticity of using mini
504 batches is able to improve the performance of Pareto

505 approximation for free by also using the intermedi
506 ate checkpoints to approximate P. To fully exploit

507 this advantage, for all the methods, we collect check
508 points every epoch to approximate P, starting from

509 epoch 60.

510 C.2.2 EVALUATION METRIC DETAILS


Criterion
Norm of MGD Grad

Values

Itertions


Figure 3: The evolution of Criterion F and the norm
of MGD gradient when trained using PNG on NYUv2
dataset with MTAN network. PNG effectively decreases the criterion while ensuring the model is within
the Pareto set, since the norm of MGD gradient remains
unchanged.


511 We introduce the definition of the used metric for

512 evaluation. Given a set [ˆ] = _θ1, . . ., θN_ that we
_P_ _{_ _}_

513 use to approximate P, its IGD+ score is defined as:


IGD+( [ˆ]) =
_P_


)dµ(θ), _q(θ,_ [ˆ]) = min
_P_ _[∗]_ _[q][(][θ,][ ˆ]P_ _P_ _θˆ∈P[ˆ]_


**_ℓ(θ[ˆ]) −_** **_ℓ(θ)_**


514 where µ is some base measure that measures the importance of θ ∈P and (t)+ := max(t, 0), applied on

515 each element of a vector. Intuitively, for each θ, we find a nearest _θ[ˆ] ∈_ _P[ˆ] that approximates θ best. Here_

516 the (·)+ is applied as we only care the tasks that _θ[ˆ] is worse than θ. In practice, a common choice of µ can_

517 be a uniform counting measure with uniformly sampled (or selected) models from P. In our experiments,

518 since we can not sample models from P, we approximate P by combining _P[ˆ] from all the methods, i.e.,_

519 _m_ {Linear,MGD,EPO,PNG} [ˆ]m, where _P[ˆ]m is the approximation set produced by algorithm m._
_P ≈∪_ _∈_ _P_

520 This approximation might not be accurate but is sufficient to compare the different methods,

521 The Hypervolume score of _P[ˆ], w.r.t. a reference point ℓ[r]_ _∈_ R[m]+ [, is defined as]

HV( [ˆ]) = µ **_ℓ_** = [ℓ1, ..., ℓm] R[m] _θ_ _, s.t. ℓt(θ)_ _ℓt_ _ℓ[r]t_ _,_
_P_ _∈_ _| ∃_ _∈_ _P[ˆ]_ _≤_ _≤_ _[∀][t][ ∈]_ [[][m][]]
n o

522 where µ is again some measure. We use ℓ[r] = [0.6, 0.6] for calculating the Hypervolume based on loss and

523 set µ to be the common Lebesgue measure. Here we choose 0.6 as we observe that the losses of the two tasks

524 are higher than 0.6 and 0.6 is roughly the worst case. When calculating Hypervolume based on accuracy, we

525 simply flip the sign.

20


-----

p p

|Col1|Col2|Loss Acc Hv ↑(10−2) IGD ↓(10−2) Hv ↑(10−2) IGD ↓(10−2)|
|---|---|---|
|γ = 0.1|α = 0.25 α = 0.5 α = 0.75|7.89 0.11 0.041 0.012 9.39 0.038 0.0056 0.002 ± ± ± ± 7.86 0.12 0.043 0.012 9.39 0.038 0.0056 0.002 ± ± ± ± 7.84 0.11 0.045 0.013 9.38 0.037 0.0057 0.002 ± ± ± ±|
|α = 0.5|γ = 0.01 γ = 0.1 γ = 0.25|7.86 0.12 0.042 0.012 9.39 0.038 0.0056 0.002 ± ± ± ± 7.86 0.12 0.043 0.012 9.39 0.038 0.0056 0.002 ± ± ± ± 7.85 0.11 0.042 0.012 9.39 0.036 0.0056 0.002 ± ± ± ±|



Table 4: Ablation study based on Multi-Mnist dataset with different choice of α and γ.

526 C.2.3 ABLATION STUDY


527 We conduct ablation study to understand the effect of α and γ using the Pareto approximation task on

528 Multi-Mnist. We compare PNG with α = 0.25, 0.5, 0.75 and γ = 0.01, 0.1, 0.25. Figure 4 summarizes the

529 result. Overall, we observe that PNG is not sensitive to the choice of hyper-parameter.

530 C.2.4 COMPARING WITH THE SECOND ORDER APPROACH


531 We give a discussion on comparing our approach with the second order approaches proposed by Ma et al.

532 (2020). In terms of algorithm, Ma et al. (2020) is a local expansion approach. To apply Ma et al. (2020),

533 in the first stage, we need to start with several well distributed models (i.e., the ones obtained by linear

534 scalarization with different preference weights) and Ma et al. (2020) is only applied in the second stage to

535 find the neighborhood of each model. The performance gain comes from the local neighbor search of each

536 model (i.e. the second stage).

537 In comparison, PNG with energy distance is a global search approach. It improves the well-distributedness

538 of models in the first stage (i.e. it’s a better approach than simply using linear scalarization with different

539 weights). And thus the performance gain comes from the first stage. Notice that we can also apply Ma et al.

540 (2020) to PNG with energy distance to add extra local search to further improve the approximation.

541 In terms of run time comparison. We compare the wall clock run time of each step of updating the 5 models

542 using PNG and the second order approach in Ma et al. (2020). We calculate the run time based on the

543 multi-MNIST dataset using the average of 100 steps. PNG uses 0.3s for each step while Ma et al. 2020 uses

544 16.8s. PNG is 56x faster than the second order approach. And we further argue that, based on time complexity

545 theory, the gap will be even larger when the size of the network increases.

546 C.3 UNDERSTANDING PNG DYNAMICS

547 We draw more analysis to understand the training dynamics of PNG.


548 **Different Staring Points** We give analysis on PNG with different initializations showing that PNG is

549 more robust to the initialization than other approaches such as Lin et al. (2019). We consider the Pareto set

550 approximation tasks and reuse synthetic example introduced in Section 5.1. We consider learning 5 models to

551 approximate the Pareto front staring from two different bad starting points. Specifically, in the upper row of

552 Figure 4, we consider initializing the models using linear scalarization. Due to the concavity of the Pareto

553 front, linear scalarization can only learns models at the two extreme end of the Pareto front. The second row

554 uses MGD for initialization and the models is scattered at an small region of the Pareto front. Different from

555 the algorithm proposed by Lin et al. (2019) which relies on a good initialization, using the proposed energy

21


-----

p p

l2 l2 l2 l2

1.0 1.0 1.0 1.0

0.8 0.8 0.8 0.8

0.6 0.6 0.6 0.6

0.4 0.4 0.4 0.4

0.2 Pareto Front 0.2 0.2 0.2

0.0 Models 0.0 0.0 0.0

0.0 0.2 0.4 0.6 0.8 1.0 l1 0.0 0.2 0.4 0.6 0.8 1.0 l1 0.0 0.2 0.4 0.6 0.8 1.0 l1 0.0 0.2 0.4 0.6 0.8 1.0 l1

1.l02 1.l02 1.l02 1.l02

0.8 0.8 0.8 0.8

0.6 0.6 0.6 0.6

0.4 0.4 0.4 0.4

0.2 0.2 0.2 0.2

0.0 0.0 0.0 0.0

0.0 0.2 0.4 0.6 0.8 1.0 l1 0.0 0.2 0.4 0.6 0.8 1.0 l1 0.0 0.2 0.4 0.6 0.8 1.0 l1 0.0 0.2 0.4 0.6 0.8 1.0 l1


Figure 4: Evolution of models from different initialization. Upper row uses initialization with linear
scalarization and lower row uses initialization from MDG. From left to right: the evolution of models during
training. PNG is robust to initializations. In both two cases of very poor initialization, PNG is still able to
move the models so that they are eventually well distributed on the Pareto set.

556 distance function, PNG pushes the models to be equally distributed on the Pareto Front without the need of

557 any prior information of the Pareto front even with extremely bad starting point.


558 **Trajectory Visualization with Different Hyper-parameters** We also give more visualization on the PNG

559 trajectory when using different hyper-parameters. We reuse synthetic example introduced in Section 5.1

560 for studying the hyper-parameters α and γ. We fix α = 0.25 and vary γ = 0.1, 0.05, 0.01, 0.1; and fix

561 _γ = 0.01 and vary α = 0.1, 0.25, 0.5, 0.75. Figure 5 plots the trajectories. As we can see, when γ is properly_

562 chosen, with different α, PNG finds the correct models with different trajectories. Different α determines the

563 algorithm’s behavior of balancing the descent of task losses or criterion objectives. On the other hand, with

564 too large γ, the algorithm fails to find a model that is close to P _[∗], which is expected._

565 C.4 IMPROVING MULTITASK BASED DOMAIN GENERALIZATION


566 We argue that many other deep learning problems also have the structure of multitask learning when multiple

567 losses presents and thus optimization techniques in multitask learning can also be applied to those domains.

568 In this paper we consider the JiGen (Carlucci et al., 2019b). JiGen learns a model that can be generalized to

569 unseen domain by minimizing a standard cross-entropy loss ℓclass for classification and an unsupervised loss

570 _ℓjig based on Jigsaw Puzzles:_

_ℓ(θ) = (1 −_ _ω)ℓclass(θ) + ωℓjig(θ)._

571 The ratio between two losses, i.e. ω, is important to the final performance of the model and requires a

572 careful grid search. Notice that JiGen is essentially searching for a model on the Pareto front using the linear

573 scalarization. Instead of using a fixed linear scalarization to learn a model, one natural questions is that

22


-----

p p

|Col1|Col2|P|areto Front|
|---|---|---|---|
|||P|NG|
|||||
|||||
|||||
|||||
|||||


l2 task preference l2 task preference l2 task preference l2 task preference

1.0 Pareto Front 1.0 1.0 1.0

PNG

0.8 0.8 0.8 0.8

0.6 0.6 0.6 0.6

0.4 0.4 0.4 0.4

0.2 0.2 0.2 0.2

0.0 0.0 0.0 0.0

0.0 0.2 0.4 0.6 0.8 1.0 l1 0.0 0.2 0.4 0.6 0.8 1.0 l1 0.0 0.2 0.4 0.6 0.8 1.0 l1 0.0 0.2 0.4 0.6 0.8 1.0 l1

l2 task preference l2 task preference l2 task preference l2 task preference

1.0 1.0 1.0 1.0

0.8 0.8 0.8 0.8

0.6 0.6 0.6 0.6

0.4 0.4 0.4 0.4

0.2 0.2 0.2 0.2

0.0 0.0 0.0 0.0

0.0 0.2 0.4 0.6 0.8 1.0 l1 0.0 0.2 0.4 0.6 0.8 1.0 l1 0.0 0.2 0.4 0.6 0.8 1.0 l1 0.0 0.2 0.4 0.6 0.8 1.0 l1


Figure 5: Ablation study on OPT-in-Pareto with different ratio constraint of objectives. Upper row, from
left to right: fixing α = 0.25, γ = 0.1, 0.05, 0.01, 0.001; Lower row, from left to right: fixing γ = 0.01,
_α = 0.1, 0.25, 0.5, 0.75. By comparing the figures in the first row, we find that choosing a too large γ make_
the final converged model be far away from the Pareto set, which is as expected. By comparing the figures in
the second row, we find that changing α make PNG give different priority in making Pareto improvement or
descent on F . When α is larger (the right figures), PNG will first move the model to Pareto set and start to
decrease F after that.

23


-----

p p


574 whether it is possible to design a mechanism that dynamically adjusts the ratio of the losses so that we can

575 achieve to learn a better model.

576 We give a case study here. Motivated by the adversarial feature learning (Ganin et al., 2016), we propose

577 to improve JiGen such that the latent feature representations of the two tasks are well aligned. Specifically,

578 suppose that Φclass(θ) = {φclass(xi, θ)}i[n]=1 [and][ Φ][jig][(][θ][) =][ {][φ][jig][(][x][i][, θ][)][}]i[n]=1 [is the distribution of latent feature]

579 representation of the two tasks, where xi is the i-th training data. We consider FPD as some probability metric

580 that measures the distance between two distributions, we consider the following problem:

min
_θ∈P_ _[∗]_ _[F][PD][[Φ][class][(][θ][)][,][ Φ][jig][(][θ][)]][.]_


581 With PD as the criterion function, our algorithm automatically reweights the ratio of the two tasks such that

582 their latent space is well aligned.

583 **Setup We fix all the experiment setting the same as Carlucci et al. (2019b). We use the Alexnet and Resnet-18**

584 with multihead pretrained on ImageNet as the multitask network. We evaluate the methods on PACS (Li et al.,

585 2017), which covers 7 object categories and 4 domains (Photo, Art Paintings, Cartoon and Sketches). Same to

586 Carlucci et al. (2019b), we trained our model considering three domains as source datasets and the remaining

587 one as target. We implement FPD that measures the discrepancy of the feature space of the two tasks using

588 the idea of Domain Adversarial Neural Networks (Ganin & Lempitsky, 2015) by adding an extra prediction

589 head on the shared feature space to predict the whether the input is for the classification task or Jigsaw task.

590 Specifically, we add an extra linear layer on the shared latent feature representations that is trained to predict

591 the task that the latent space belongs to, i.e.,


log(σ(w[⊤]φclass(xi, θ))) + log(1 − _σ(w[⊤]φclass(xi, θ)))._
_i=1_

X


_FPD(Φclass(θ), Φjig(θ)) = min_
_w,b_


592 Notice that the optimal weight and bias for the linear layer depends on the model parameter θ, during the

593 training, both w, b and θ are jointly updated using stochastic gradient descent. We follow the default training

594 protocol provided by the source code of Carlucci et al. (2019b).

595 **Baselines Our main baselines are JiGen (Carlucci et al., 2019b); JiGen + adv, which adds an extra domain**

596 adversarial loss on JiGen; and our PNG with domain adversarial loss as criterion function. In order to run

597 statistical test for comparing the methods, we run all the main baselines using 3 random trials. We use the

598 released source code by Carlucci et al. (2019b) to obtained the performance of JiGen. For JiGen+adv, we use

599 an extra run to tune the weight for the domain adversarial loss. Besides the main baselines, we also includes

600 TF (Li et al., 2017), CIDDG (Li et al., 2018b), MLDG (Li et al., 2018a), D-SAM (D’Innocente & Caputo,

601 2018) and DeepAll (Carlucci et al., 2019b) as baselines with the author reported performance for reference.

602 **Result The result is summarized in Table 5 with bolded value indicating the statistical significant best methods**

603 with p-value based on matched-pair t-test less than 0.1. Combining Jigen and PNG to dynamically reweight

604 the task weights is able to implicitly regularizes the latent space without adding an actual regularizer which

605 might hurt the performance on the tasks and thus improves the overall result.

24


-----

p p

|Method|Art paint Cartoon Sketches Photo|Avg|
|---|---|---|


|Method|Art paint Cartoon Sketches Photo AlexNet|Avg|
|---|---|---|
|TF CIDDG MLDG D-SAM DeepAll|0.6268 0.6697 0.5751 0.8950 0.6270 0.6973 0.6445 0.7865 0.6623 0.6688 0.5896 0.8800 0.6387 0.7070 0.6466 0.8555 0.6668 0.6941 0.6002 0.8998|0.6921 0.6888 0.7001 0.7120 0.7152|
|JiGen JiGen + adv Jigen + PNG|0.6855 ± 0.004 0.6889±0.002 0.6831±0.011 0.8946 ± 0.008 0.6857 ± 0.004 0.6837 ± 0.003 0.6753 ± 0.008 0.8980 ± 0.001 0.6914±0.005 0.6903±0.002 0.6855±0.007 0.9044±0.003|0.7380 ± 0.002 0.7357 ± 0.003 0.7429±0.002|


|Jigen + PNG|± ± ± ± 0.6914±0.005 0.6903±0.002 0.6855±0.007 0.9044±0.003 ResNet-18|± 0.7429±0.002|
|---|---|---|
|D-SAM DeepAll|0.7733 0.7243 0.7783 0.9530 0.7785 0.7486 0.6774 0.9573|0.8072 0.7905|
|JiGen JiGen + adv JiGen + PNG|0.8009 ± 0.004 0.7363 ± 0.007 0.7046 ± 0.013 0.9629±0.002 0.7923 ± 0.006 0.7402 ± 0.004 0.7188 ± 0.005 0.9617 ± 0.001 0.8014±0.005 0.7538±0.001 0.7222±0.006 0.9627±0.002|0.8012 ± 0.002 0.8033 ± 0.001 0.8100±0.005|



Table 5: Comparing different algorithms for domain generalization using dataset PACS and two network
architectures.

25


-----

