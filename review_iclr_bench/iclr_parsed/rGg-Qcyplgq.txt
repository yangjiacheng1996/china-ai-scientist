# DISTRIBUTIONAL PERTURBATION FOR EFFICIENT EXPLORATION IN DISTRIBUTIONAL REINFORCEMENT LEARNING

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Distributional reinforcement learning aims to learn distribution of return under
stochastic environments. Since the learned distribution of return contains rich information about the stochasticity of the environment, previous studies have relied
on descriptive statistics, such as standard deviation, for optimism in the face of
uncertainty. However, using the uncertainty from an empirical distribution can
hinder convergence and performance when exploring with the certain criterion
that has an one-sided tendency on risk in these methods. In this paper, we propose a novel distributional reinforcement learning that explores by randomizing
risk criterion to reach a risk-neutral optimal policy. First, we provide a perturbed
distributional Bellman optimality operator by distorting the risk measure in action selection. Second, we prove the convergence and optimality of the proposed
method by using the weaker contraction property. Our theoretical results support
that the proposed method does not fall into biased exploration and is guaranteed
to converge to an optimal return distribution. Finally, we empirically show that
our method outperforms other existing distribution-based algorithms in various
environments including Atari games.

1 INTRODUCTION

Distributional reinforcement learning (DRL) learns the stochasticity of returns in the reinforcement
learning environments and has shown remarkable performance in several benchmark tasks. Its model
generates the approximated distribution of returns, where the mean value implies the traditional Qvalue (Bellemare et al., 2017; Dabney et al., 2018b; Choi et al., 2019). Learning procedure with
stochasticity through return distribution is represented by parametric (epistemic) uncertainty, which
is due to insufficient or inaccurate data, and intrinsic (aleatoric) uncertainty, which is inherently
possessed randomness in the environment. (Chow et al., 2015; Dabney et al., 2018a) The learned
stochasticity gives rise to the notion of risk-sensitivity, and some distributional reinforcement learning algorithms distort the learned distribution to create a risk-averse or risk-seeking policy.

Another way to employ the uncertainty is to design an efficient exploration method which is essential
to find an optimal behavior with a few number of trials. Optimism in the face of uncertainty (OFU)
is one of the fundamental exploration principles that employs parametric uncertainty to promote
exploring less understood behaviors and to construct confidence set. Most OFU algorithms select an
action with the highest upper-confidence bound (UCB) of uncertainty which can be considered as the
optimistic decision at the moment (Chen et al., 2017; Ciosek et al., 2019). In deep RL, several OFU
studies often model the uncertainty explicitly through the Bayesian posterior, which is estimated by
using neural networks. However, learning the representation of high-dimensional state-action space
and Bellman update simultaneously leads to unstable propagation(Yang et al., 2021).

On the other hand, DRL can provide more statistical information during control such as mode, median, or variance by addressing full characteristics of the return distribution. Despite the richness
of information for return distribution, only a few DRL methods have tried to employ the benefits
of distributional perspective for exploration (Tang & Agrawal, 2018; Mavrin et al., 2019; Clements
et al., 2019). To estimate the uncertainty from distributional outputs, descriptive statistics that is
composed of a mixture of intrinsic and parametric uncertainty can be utilized. Unfortunately, sepa

-----

rating these two types of uncertainty during learning is not a trivial task. Mavrin et al. (2019) propose
a distribution-based OFU exploration that schedules a decaying bonus rate to suppress the effect of
intrinsic uncertainty, which unintentionally induces a risk-seeking policy. Although OFU based approaches try to reduce parametric uncertainty by revisiting the state with high uncertainty, there
exists the side effect that the criteria unfortunately forces the agent to chase the intrinsic uncertainty
(risk) simultaneously during updates. Relying on a specific criteria causes an one-sided tendency on
risk and makes an agent consistently select certain actions during exploration as degrading performance. We call this phenomenon as stuckness.

In this paper, we introduce perturbed quantile regression (PQR) which perturbs the criterion on uncertainty by randomizing the risk criterion in action selection. First, the distributional perturbation on
return distribution is to re-evaluate the estimate of return by distorting the learned distribution with
perturbation weight. Unlike the typical worst-case approach in risk-sensitive settings or OFU based
approaches, we instead randomly sample a risk measure from an ambiguity set, which represents
that the risk setting is ambiguous when the characteristics of a given environment are unknown. We
empirically demonstrate on the stochastic variant of N-Chain environment (Osband et al., 2016) that
a randomized scheme is more effective than OFU to alleviate the sub-optimality problem of tendency
to obtain risk-seeking policies. Second, any risk-measure with some time-varying perturbation constraint allows us to average over all possible risk-sensitive behaviors to achieve the optimal policy.
We provide a sufficient condition for the convergence of return distribution in the weaker contraction property. Our method covers the full elements of the reinforcement learning with distributional
perspective from the novel exploration method with distributional perturbation to the theoretically
guaranteed convergence of return distribution, which has the same fixed-point with the standard
Bellman optimality operator. The proposed algorithm is based on QR-DQN, which is a baseline of
DRL architecture to learn a return distribution, and we show that PQR outperforms QR-DQN and
other DRL baselines in several benchmark environments, LunarLander-v2 and Atari games.

2 BACKGROUNDS & RELATED WORKS

2.1 DISTRIBUTIONAL REINFORCEMENT LEARNING

We consider a Markov decision process (MDP) which is defined as a tuple (S, A, P, R, γ) where S
is a finite state space, A is a finite action space, P : S×A×S → [0, 1] is the transition probability, R
is the random variable of rewards in [−Rmax, Rmax], and γ ∈ [0, 1) is the discount factor. We define a
stochastic policy π(·|s) which is a conditional distribution over A given state s. For a fixed policy π,
we denote Z _[π](s, a) as a random variable of return distribution of state-action pair (s, a) following_
the policy π. We attain Z _[π](s, a) =_ _t=0_ _[γ][t][R][(][S][t][, A][t][)][, where][ S][t][+1][ ∼]_ _[P]_ [(][·|][S][t][, A][t][)][, A][t][ ∼] _[π][(][·|][S][t][)]_
and S0 = s, A0 = a. Then, we define an action-value function as Q[π](s, a) = E[Z _[π](s, a)] in_

[ _Vmax, Vmax] where Vmax = Rmax/(1_ _γ). For regularity, we further notice that the space of action-_
_−_ [P]−[∞]
value distributions Z has the first moment bounded by Vmax:

= _Z :_ _P(R)_ E[ _Z(s, a)_ ] _Vmax,_ (s, a) _._
_Z_ _S × A →_ _|_ _|_ _≤_ _∀_


In distributional RL, the return distribution for the fixed π can be computed via dynamic programming with the distributional Bellman operator defined as (Bellemare et al., 2017),

## τ [π]Z(s, a) =D R(s, a) + γZ(S[′], A[′]), S[′] P ( s, a), A[′] π( S[′])
_∼_ _·|_ _∼_ _·|_

_D_
where = denotes that both random variables share the same probability distribution. We can compute
the optimal return distribution by using the distributional Bellman optimality operator defined as,

_D_
## τ Z(s, a) = R(s, a) + γZ(S[′], a[∗]), S[′] ∼ P (·|s, a), a[∗] = arg maxa[′] EZ[Z(S[′], a[′])].

Bellemare et al. (2017) have shown that τ _[π]_ is a contraction in a maximal form of the Wasserstein
metric but τ is not a contraction in any metric. Combining with the expectation operator, Eτ is a
contraction so that we can guarantee that the expectation of Z converges to the optimal state-action
value, while the convergence of a return distribution itself is not guaranteed.


-----

2.2 EXPLORATION ON DISTRIBUTIONAL REINFORCEMENT LEARNING

In this section, we will briefly describe the main practical algorithm for DRL with a deep neural
network, and explain the derived exploration method. To combine with deep RL, a parametric distribution Zθ is used to learn a return distribution by using _τ . Dabney et al. (2018b) have employed a_
quantile regression to approximate the full distribution by letting Zθ(s, a) = _N[1]_ _Ni=1_ _[δ][θ]i[(][s,a][)][ where]_

the parameter θ represents the locations of a mixture of N Dirac delta functions. Each θi represents
the value where the cumulative probability is τi = _Ni_ [. Then, by using the quantile representation]P

with the distributional Bellman optimality operator, the problem can be formulated as a minimization problem as,


_θ = arg min_
_θ[′][ D][ (][Z][θ][′]_ [(][s][t][, a][t][)][,] _[τ][ Z][θ][−]_ [(][s][t][, a][t][)) :=]


1

_τˆi_ [(][r][t] [+][ γθ]j[−][(][s][t][+1][, a][′][)][ −] _[θ]i[′][(][s][t][, a][t][))]]_
_N_ [[][ρ][κ]


_i=1_


_j=1_


where (st, at, rt, st+1) is a given transition pair, ˆτi = _[τ][i][−][1]2[+][τ][i]_, a[′] := arg maxa′ EZ[Zθ(st+1, a[′])],

_ρ[κ]τˆi_ [(][x][) :=][ |]τ[ˆ]i _δ_ _x<0_ _κ(x), and_ _κ(x) := x[2]/2 for_ _x_ _κ and_ _κ(x) := κ(_ _x_ 2 _[κ][)][, otherwise.]_
_−_ _{_ _}|L_ _L_ _|_ _| ≤_ _L_ _|_ _|−_ [1]

Based on the quantile regression, Dabney et al. (2018b) have proposed a quantile regression deep Q
network (QR-DQN) that shows better empirical performance than the categorical approach (Bellemare et al., 2017) since the quantile regression does not restrict the bounds for return. As deep RL
typically did, QR-DQN adjusts ϵ-greedy schedule, which selects the greedy action with probability
1 − _ϵ and otherwise selects random available actions uniformly. The majority of QR-DQN variants_
(Dabney et al., 2018a; Yang et al., 2019) rely on the same exploration method. However, such approaches do not put aside inferior actions from the selection list and thus suffers from a loss (Osband
et al., 2019). Hence, selecting a statistically plausible action is crucial for efficient exploration.

In recent studies, Mavrin et al. (2019) modifies the criterion of selecting an action for efficient
exploration in the face of uncertainty. Using left truncated variance as a bonus term to estimate
optimistic way and decaying ratio ct to suppress the intrinsic uncertainty, DLTV was proposed as an
uncertainty-based exploration in DRL without using ϵ-greedy exploration. At timestep t, the action
selection of DLTV can be described as:


log t


_a[∗]_ = arg max
_a[′]_


EP [Z(s[′], a[′])] + ct


_σ+[2]_ [(][s][′][, a][′][)] _, ct = c_



_, σ+[2]_ [=]


(θ N

2

_[−]_ _[θ][i][)][2][,]_


2N


_i=_ _[N]2_


where θi’s are the values of quantile level τi. DLTV shows that a constant schedule degrades the
performance significantly compared to a decaying schedule.

2.3 RISK IN DISTRIBUTIONAL REINFORCEMENT LEARNING

Instead of an expected value, risk-sensitive RL tries to maximize a risk measure such as MeanVariance (Zhang et al., 2020), Value-at-Risk (VaR) (Chow et al., 2017), or Conditional Value-at-Risk
(CVaR) (Rockafellar et al., 2000; Rigter et al., 2021), which result in different classes of optimal
policy. Especially, Dabney et al. (2018a) interprets risk measures as the expected utility function of
the return, i.e., EZ[U (Z(s, a))]. Under this interpretation, risk-sensitive RL can be formulated as
the maximization problem with various types of utility functions. If the utility function U is linear,
the policy obtained under such risk measure is called risk-neutral. If U is concave or convex, the
resulting policy is termed as risk-averse or risk-seeking, respectively. In general, a distortion risk
_measure is a generalized expression of risk measure generated from the distortion function._
**Definition 1. Let h : [0, 1] →** [0, 1] be a distortion function such that h(0) = 0, h(1) = 1 and non_decreasing. Given a probability space (Ω, F, P) and a random variable Z : Ω_ _→_ R, a distortion
**_risk measure ρh corresponding to a distortion function h is defined by:_**

_∞_
_ρh(Z) := E[h][(][P][)][Z] =_ _z [∂]_

_∂z_ [(][h][ ◦] _[F][Z][)(][z][)][dz,]_

Z−∞

_where FZ is the cumulative distribution function of Z._

In fact, non-decreasing property of h makes it possible to distort the distribution of Z while satisfying
the fundamental property of CDF. Note that the concavity and the convexity of distortion function


-----

Figure 1: Illustration of the N-Chain environment starting from state s2. To emphasize the stochasticity, the reward of state s4 was set as a mixture model composed of two Gaussian distributions.
Blue arrows indicate the risk-neutral optimal policy in this MDPs.

also imply risk-averse or risk-seeking behavior, respectively. Dhaene et al. (2012) showed that any
distorted expectation can be expressed as weighted averages of quantiles. In other words, generating
a distortion risk measure is equivalent to choosing a reweighting distribution.

Fortunately, distributional RL has a suitable configuration to apply those uncertainty-based approaches that could naturally expand the class of policies. Chow et al. (2015) and Stanko & Macek
(2019) considered risk-sensitive RL with a CVaR objective, where risk is related to robust decision
making. Dabney et al. (2018a) expanded the class of policies on arbitrary distortion risk measures
and investigated the effects of a distinct distortion risk measures by changing the sampling distribution for quantile targets τ . Unlike the usual risk-sensitive RL, DLTV applied the risk measure
only on action selection, while it keeps the standard objective to obtain a risk-neutral optimal policy.
Our analysis shows that risk-based exploration can utilize risk measures in two different ways: (1)
selecting action and (2) evaluating the value function by using distorted (perturbed) expectation.

3 PERTURBATION IN DISTRIBUTIONAL REINFORCEMENT LEARNING

3.1 MOTIVATION

Distribution-based OFU exploration (Moerland et al., 2018; Keramati et al., 2020) was proposed
to give a bonus for the uncertainty that can be extracted from the distribution. However, we found
that keeping optimism on uncertainty tends to select sub-optimal behaviors over a long exploration.
For example, suppose we choose a criterion based on mean-standard deviation with coefficient ct.
Consider two actions a1, a2 with mean µ1, µ2 and variance σ1, σ2 respectively, under the following
based onconditions: OFU. µ1 ≥ To change the decision towards the true optimal actionµ2, σ1 ≤ _σ2, and µ1 + ctσ1 ≤_ _µ2 + ctσ2. Then, the agent prefers to select a1, the following steps need a2_
to be spent:

_η = min_ _t[′]_ _> t : ct′_ _t._
_≤_ _[µ]σ[1]2[ −]_ _[µ]σ1[2]_ _−_
 _−_ 

Hence, if there is a bias in the criterion itself, such stuckness often occurs and degrades the performance since the agent does not have experience with the optimal policy during that period.

To demonstrate such shortcomings of OFU exploration in distributional RL framework, we build
a representative environment that is easy to interpret intuitively among the cases in which intrinsic
uncertainty exists. We experiment on the stochastic variant of N-Chain environment used in Osband
et al. (2016) as a toy experiment. A schematic diagram of the N-Chain environment is shown in
Figure 1. The reward is only given in the leftmost and rightmost states and the game terminates when
one of the reward states is reached. We set the leftmost reward as N (10, 0.1[2]) and the rightmost
reward as [1]2 _[N]_ [(5][,][ 0][.][1][2][) +][ 1]2 _[N]_ [(13][,][ 0][.][1][2][)][ which has a lower mean as][ 9][ but higher variance. The agent]

always starts from the middle state s2 and should move toward the leftmost state s0 to achieve the
greatest expected return. For each state, the agent can take one of six available actions: left, right,
and 4 no-op actions. The optimal policy with respect to mean is to move left twice from the start.
Despite the simple configuration, the possibility to obtain a higher reward in the suboptimal state
than the optimal state makes the agent difficult which policy is optimal until it experiences enough
to detect the characteristics of each distribution. Thus, the goal of our toy experiment is to evaluate
how quickly each algorithm could find a risk-neutral optimal policy.


-----

20000

17500

15000

12500

10000

7500

5000

2500


5000 10000 15000 20000

|QR|DQN|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
|DL p-|TV DLTV||||||
|PQ|R||||||
||||||||
||||||||
||||||||
||||||||
||||||||


QRDQN
DLTV
p-DLTV
PQR

Steps

(b)


(a)


Figure 2: (a) Empirical return distribution of DLTV during training in N-Chain environment. The
dashed lines denote the exact mean, and the dots on the x-axis denote the perturbed mean of each
action. No-op actions are not shown for visibility. (b) Total count of performing true optimal action.
The oracle (dashed line) is to perform the true optimal action from start to end.

Rather than applying a decaying schedule to suppress intrinsic uncertainty, we naively modify the
optimism into a randomized risk criterion and named the perturbed variant as p-DLTV. We compare
QR-DQN and DLTV with our randomized variants of two algorithms PQR and p-DLTV to examine
the effect of randomized risk criteria. In short, p-DLTV is a straightforward modification of DLTV
where randomness is given to the coefficient ct through normal distribution. We will describe our
main algorithm, PQR, in detail in Section 3.4.

In Figure 2(a), DLTV fails to estimate the true optimal return distribution of action a1. Due to the
erroneous estimation, the agent takes longer to recognize its error. Hence, the deterministic selection
based on a fixed criteria could mislead toward exploitation rather than exploration. This indicates that
DLTV may not gather experiences well in a complex environment that requires deep exploration. In
Figure 2(b), we plot the point when the optimal policy was actually performed for each algorithm
to show stuckness. Since the optimal policy consists of only the same index a1, we plot the total
count of performing the optimal action with 10 different seeds. The slope indicates the ratio of
performing the optimal policy on average. Hence, the interval with a slope of 1 implies that the
optimal policy was performed every time. From the slope of each line, it is observed that DLTV
selects the suboptimal action even if the optimal policy was initially performed. Although the mean
return of a1 (move left) is estimated to be superior, the agent only selects a2 (move right) during
training due to its consistent optimism on uncertainty. Even if DLTV has spent enough number of
time steps to choose the true optimal policy, the remaining procedure is already close to greedy
selection as it starts from a decreased coefficient.

Surprisingly, p-DLTV alleviates the stuckness early by randomly choosing a plausible action. We
also propose a theoretically guaranteed algorithm to be converged, PQR which shows a much steeper
line by quickly obtaining the optimal policy. In Section 3.2, we derive the first theoretical sufficient
condition for the convergence of exploration method for DRL, which implies that PQR has the same
unique fixed point as the standard distributional Bellman equation.


3.2 PERTURBED DISTRIBUTIONAL BELLMAN OPTIMALITY OPERATOR

To choose statistically plausible actions which may be maximal for certain criteria, we are interested
in generating a distortion risk measure involved in a pre-defined constraint set called an ambiguity
_set. The ambiguity set, originated from distributionally robust optimization (DRO) literature, is a_
family of distribution characterized by a certain statistical distance such as ϕ-divergence or Wasser_stein distance (Esfahani & Kuhn, 2018; Shapiro et al., 2021). In this paper, we will examine the_
ambiguity set defined by the discrepancy between distortion risk measure and expectation. We say
the sampled reweighting distribution ξ as (distributional) perturbation and define it as follows:

**Definition 2. (Perturbation, Perturbation Gap, and Ambiguity Set) Given a probability space**
(Ω, F, P), let X : Ω _→_ R be a random variable and Ξ = _ξ : ξ(w) ≥_ 0, _w∈Ω_ _[ξ][(][w][)][P][(][w][)][dw][ = 1]_
 R


-----

_be a set of probability density functions. For a given constraint set U ⊂_ Ξ, we say ξ ∈U as a
**_(distributional) perturbation from U and denote the ξ−weighted expectation of X as follows:_**

Eξ[X] := _X(w)ξ(w)P(w)dw,_
Zw∈Ω

_which can be interpreted as the expectation of X under perturbed probability distribution ξP. We_
_further define d(X; ξ) = |E[X] −_ Eξ[X]| as perturbation gap of X with respect to ξ. Then, for a
_given constant ∆_ _≥_ 0, we define the ambiguity set with the bound ∆ _as_

∆(X) = _ξ_ Ξ : d(X; ξ) ∆ _._
_U_ _∈_ _≤_
n o

For brevity, we omit the input w from a random variable unless confusing. Since ξ is a probability
density function, Eξ[X] is an induced risk measure with respect to a reference measure P. Intuitively,
_ξ(w) can be viewed as a distortion to generate a different probability measure and allow to vary the_
risk tendency. The aspect of using distortion risk measures looks similar to IQN (Dabney et al.,
2018a). However, instead of changing the sampling distribution of quantile level τ implicitly, we
reweight each quantile from the ambiguity set. This allows us to control the maximum allowable
distortion with bound ∆, whereas in IQN the risk measure does not change throughout learning. In
Section 3.4, we will suggest a practical method to construct the ambiguity set.

Now, we characterize perturbed distributional Bellman optimality operator _τ ξ for a fixed perturba-_
tion ξ ∆(Z) written as below:
_∈U_

_D_
## τ ξZ(s, a) = R(s, a) + γZ(S[′], a[∗](ξ)), S[′] ∼ P (·|s, a), a[∗](ξ) = arg maxa[′] Eξ,P [Z(s[′], a[′])].

Notice that ξ ≡ 1 corresponds to a base expectation, i.e., Eξ,P = EP, which recovers the standard
distributional Bellman optimality operator _τ . To understand the progress of PDBOO intuitively, we_
add the pipeline figure in Appendix A.4.

Specifically, updating via τ ξ can be described in two steps. 1) Select the superior action in terms
of an arbitrary distortion risk measure instead of mean. We specify the distortion risk measure by
sampling the perturbation ξ from the ambiguity set. 2) Compute the return distribution through
dynamic programming which follows the standard Bellman optimality equation. In risk-sensitive
DRL or distributionally robust RL, the Bellman optimality equation is reformulated for a pre-defined
risk measure (Chow et al., 2015; Smirnova et al., 2019; Yang, 2020). In this sense, PDBOO has
a significant distinction in that it performs dynamic programming that adheres to the risk-neutral
optimal policy while randomizing the risk criterion at every step.

If we consider the time-varying bound of ambiguity set, scheduling ∆t is a key ingredient to determine whether PDBOO will efficiently explore or converge. The following lemma plays an important
role to guarantee the convergence of PDBOO.
**Lemma 3. If ξt converges to 1 uniformly on Ω, then Eτ ξt also converges to Eτ uniformly on Z**
_for all s ∈S and a ∈A._

Intuitively, if an agent continues to sample the distortion risk measure from a fixed ambiguity set
with a constant ∆, there is a possibility of selecting sub-optimal actions after sufficient exploration,
which may not guarantee eventual convergence. Hence, it will be crucial to schedule a constraint of
ambiguity set properly at each action selection to guarantee convergence.

Based on the quantile model Zθ, our algorithm can be summarized into two parts. First, we aim
to minimize the expected discrepancy between Zθ and τ ξZθ− where ξ is sampled from ambiguity
set ∆. To clarify notation, we write Eξ[ ] as a ξ weighted expectation and Eξ _P(_ ∆)[ ] as an
_U_ _·_ _−_ _∼_ _U_ _·_
expectation with respect to ξ which is sampled from ∆. Then, our goal is to minimize the perturbed
_U_
distributional Bellman objective with sampling procedure P:

min (1)
_θ[′][ E][ξ][t][∼][P][(][U][∆][t]_ [)][[][D][(][Z][θ][′] [(][s, a][)][,] _[τ][ ξ][t]_ _[Z][θ][−]_ [(][s, a][))]]

where we use the Huber quantile loss as a discrepancy on Zθ′ and _τ ξZθ−_ at timestep t. It is different
from DRO which performs the worst-case optimization by using a minimax objective. By using
expectation instead of max operator, we investigate risk-neutral exploration that can avoid overly


-----

pessimistic policies. Second, considering a sequence ξt which converges uniformly to 1 so that τ ξt
converges uniformly to original τ, we derive a sufficient condition of ∆t that the expectation of
iterated operator Eτ ξn:1 = Eτ ξnτ ξn−1 _τ ξ1 has a unique fixed point with the same solution as_
the standard solution. _· · ·_

3.3 CONVERGENCE OF THE PERTURBED DISTRIBUTIONAL BELLMAN OPTIMALITY
OPERATOR

In this section, we will provide the theoretical result of PDBOO about its convergence through
E[Z [(][n][)]] where the iteration procedure is denoted as Z [(][n][+1)] := τ ξn+1 _Z_ [(][n][)] and Z [(0)] = Z for each
timestep n > 0.
**Theorem 4. For a sequence of bound ∆n, let** [¯]∆n (Z [(][n][−][1)]) := _s,a_ _n_ _Z_ [(][n][−][1)](s, a) _. If we_
_U_ _[U][∆]_
_sample ξn from_ _U[¯]∆n_ (Z [(][n][−][1)]) for every iteration and _n=1_ [∆][n][ <][ ∞] _[holds, then, the expectation] _ 
_of iterated operator τ ξn:1 has a fixed point E[Z_ _[∗]]. Moreover, the following bound holds,[T]_

_k_

_∞_ [P][∞]

sup E[Z [(][n][)](s, a)] E[Z _[∗](s, a)]_ 2γ[k][−][1]Vmax + 2 _γ[i](∆k+2_ _i + ∆k+1_ _i)_ _._
_s,a_ _−_ _≤_ _k=n_ _i=1_ _−_ _−_ !

X X

Theorem 4 states that _τ ξn:1 converges while it does not satisfy the γ-contraction property. Note that_
the fixed point E[Z _[∗]] is not yet guaranteed to be unique for any Z ∈Z. Fortunately, we could show_
that E[Z _[∗]] is, in fact, the solution of the standard Bellman optimality equation which is already well_
known to have a unique solution.
**Theorem 5. If {∆n} follows the assumption in Theorem 4, then E[Z** _[∗]] is the unique solution of_
_Bellman optimality equation._

From two theoretical results, PDBOO is guaranteed to have the same unique fixed point as the
standard Bellman operator under weaker contraction property. Unlike other distribution-based or
risk-sensitive approaches, PDBOO is a novel operator having compatibility for obtaining a riskneutral optimal policy. Also, it is also observed that this randomized method is more efficient than
the OFU-based exploration by quickly alleviating from sub-optimal policies.

3.4 PERTURBED QUANTILE REGRESSION FOR DEEP Q-NETWORK

We propose a perturbed quantile regression (PQR) that is a practical algorithm for distributional
reinforcement learning. We model a return distribution using quantile regression and update the
quantile model by minimizing the objective function (1) induced by PDBOO. To compute the target
distribution of (1), we propose a sampling method of ξ from ambiguity set ∆. Since we employ
_U_
a quantile model, sampling a reweight function ξ can be reduced into sampling an N -dimensional
weight vectorthe QR-DQN setup, note that the condition ξ := [ξ1, · · ·, ξN ] where _i=1w[ξ][i]Ω[ =][ξ][(][ N][w][)][ and][P][(][w][ ξ][)][dw][i][ ≥][ = 1][0][ for all][ turns into][ i][ ∈{][ P][1][,][ · · ·]i[N]=1[, N]N1_ _[ξ][}][i][. Based on][ = 1][ since]_

_i_ _∈_
the quantile level is set as τi = _N_ [.] [P][N]R

A key issue is how to construct an ambiguity set with bound ∆t and then sample ξ. A natural class
of distribution for practical use is the symmetric Dirichlet distribution with concentration β, which

**Algorithm 1: Perturbed QR-DQN (PQR)**
**Input: transition (s, a, r, s[′]), discount γ ∈** [0, 1), timestep t > 0, epsilon ϵ > 0, concentration β
Initialize ∆0 > 0
∆t ∆0t[−][(1+][ϵ][)]
_←_

**_ξ_** max **1[N]** + ∆t(N **_x_** **1[N]** ), 0 where x Dir(β) # Sample ξ ∆t (Z [(][t][)])
_←_ _−_ _∼_ _∼_ _U[¯]_

**_ξ_** _N_ **_ξ/[P]_** _ξi_ 
_←_
_a[∗]_ arg maxa′ Eξ[Z(s[′], a[′])] # Select greedy action with distorted expectation
_←_
## τ θj r + γθj(s[′], a[∗]), j
_t ← ←t + 1_ _∀_

**Output:** _i=1_ [E][j][[][ρ]τ[κ]ˆi [(][τ][ θ][j][ −] _[θ][i][(][s, a][))]]_

[P][N]


-----

∆µ ∆σ
(ˆµ − _µ)_ (ˆσ − _σ)_

QR-DQN 1.23 **0.01**

DLTV -1.02 1.01

p-DLTV **0.02** 0.38

PQR(ours) -0.03 **-0.01**

Figure 3: (Left) Empirical return distribution plot in N-Chain environment. Since QR-DQN does
not depend on other criterion, the dots are omitted. (Right) Mean and standard-deviation difference
between each algorithm and ground truth distribution.

represents distribution over distributions. (i.e. x ∼ Dir(β).) If β is small, then, most of the mass is
concentrated on a few elements. Otherwise, all elements are similar to each other and produce evenly
distributed weight. By using the Dirichlet distribution, we sample a random vector, x ∼ Dir(β), and
define the reweight distribution as ξ := 1[N] + α(N **_x −_** **1[N]** ). From the construction of ξ, we have
1 _α_ _ξi_ 1 + α(N 1) for all i and it follows that 1 _ξi_ _α(N_ 1) for all i. By controlling
_α −, we can bound the deviation of ≤_ _≤_ _−_ _ξi from 1 and bound the perturbation gap as |_ _−_ _| ≤_ _−_

sups,a _[|][E][[][Z][(][s, a][)]][ −]_ [E][ξ][[][Z][(][s, a][)]][|][ = sup]s,a Zw∈Ω _Z(w; s, a)(1 −_ _ξ(w))P(w)dw_

_≤_ _wsup∈Ω|1 −_ _ξ(w)| sups,a_ [E][[][|][Z][(][s, a][)][|][]][ ≤] _w[sup]∈Ω|1 −_ _ξ(w)|Vmax ≤_ _α(N −_ 1)Vmax.


Hence, letting α ≤ (N _−∆1)Vmax_ [is sufficient to obtain][ d][(][Z][;][ ξ][)][ ≤] [∆] [in the quantile setting. We set]

_β = 0.05 · 1[N]_ to generate a constructive perturbation ξn which gap is close to the bound ∆n. To
satisfy the condition stated in Theorem 4, we set ∆t = ∆0t[−][(1+][ϵ][)] where ∆0 is a hyperparameter.
The detailed procedure of the proposed method is summarized in Algorithm 1.

4 EXPERIMENTAL RESULTS AND DETAILS

In this section, we evaluate the performance on N-Chain, LunarLander and Atari games to demonstrate the efficiency of randomized criteria, comparing with QR-DQN (ϵ-greedy) and DLTV (OFU).

**N-Chain.** As the mean of each return is designed to be similar, it is useful to examine the learning
behavior of the empirical return distribution for each algorithm. Figure 3 shows the empirical PDF
of return distribution by using Gaussian kernel density estimation. PQR estimates the ground truth
much better than other baselines with much closer mean and standard-deviation. Although the optimal policy was performed, QR-DQN overestimates the optimal Q-value of (s2, a1) as ˆµ = 9.33
while the ground truth is computed as µ = 10γ[2] = 8.1. PQR estimated the target relatively better
compared to QR-DQN as the agent often chooses an alternative action from a risk-neutral perspective. Only by chainging from optimism to a randomized scheme, p-DLTV made a much better
estimate than DLTV.

**LunarLander-v2.** Next, we evaluate the performance on LunarLander-v2 environment where its
input is given by 8-dimensional coordinates with 4 different discrete actions. The goal of the agent
is to reach the landing pad with a given threshold of 200 points without crashing. Without obtaining
the extra point of +100, the agent cannot reach the threshold.

Figure 4(a) shows that p-DLTV and PQR have reached the threshold 200 points faster than the
other two algorithms. Significantly, p-DLTV successfully reached the goal just by randomizing the
criterion from OFU while DLTV did not learn to obtain the extra point. The result shows that onesided tendency on risk leads to learning failure of an agent. We note that the randomized risk criterion
could alleviate the stuckness within an affordable time budget.


-----

LunarLander-v2


300

200

100

0

100

200


Hitting Time (M)

QR-DQN 0.23


DLTV N.A

p-DLTV 0.18


PQR(ours) **0.16**

(b)

|Col1|Col2|Col3|
|---|---|---|
||||
||QRD|QN|
||DLTV p-DL PQR|TV|


0.05 0.10 0.15 0.20 0.25 0.30

QRDQN
DLTV
p-DLTV
PQR

Steps (Million)

(a)


Figure 4: (a) Evaluation curves on LunarLander-v2. All curves are the average of three random seeds
and the shaded area represents the standard deviation. We smoothed the curve over 5 consecutive
steps. (b) Hitting time to reach a given threshold.

**Atari Games.** Lastly, we evaluate the performance in Atari games, including multiple environments, each of which contained intrinsic uncertainty in different ways.






Gravitar-v4

1000

800

600

QRDQN

400 DLTV

Average Score 200 p-DLTVPQR

0 Training Frames (Million)50 100 150 200


Kangaroo-v4

15000

12500

10000

7500

5000

Average Score 2500

0

0 Training Frames (Million)50 100 150 200


Seaquest-v4

40000

30000

20000

Average Score10000

0

0 Training Frames (Million)50 100 150 200


Figure 5: Evaluation curves on Atari games. We smoothed all curves over 10 consecutive steps with

Pong-v4

20

15

10

Average Score 5

0 0 Training Frames (Million)10 20 30 40

three random seeds. In case of Pong-v4, we resize the x-axis, since it can easily obtain the optimal
policy with few interactions due to its environmental simplicity.

We plot the average performance of each algorithm in the evaluation step on 200M frames with 3
random seeds. In Figure 5, PQR achieves the shortest time to hit the asymptotic highest performance
in various Atari games. Moreover, our proposed method is stable with relatively low variance in all
four environments. As the bound of perturbation gap converges to 0, the plausible action is chosen
from the shrinking ambiguity set, and thus the agent gradually performs only the optimal policy.
Further experimental details are in Appendix D

In all experiments, just adding randomness to the coefficient ct shows the significant improvement
supporting that the randomized risk criterion was superior to OFU in distributional RL. In the majority of environments where intrinsic uncertainties exist, OFU has difficulty in making a decision
that matches risk-neutral purpose, because two uncertainties are intertwined during learning. We
observe that PQR outperforms the baseline by showing faster convergence without degrading the
performance. All experimental results support that the randomizing scheme is more effective than
OFU in distributional RL.

5 CONCLUSIONS


In this paper, we proposed a general framework of risk-based exploration which captures the characteristics of a return distribution. Without resorting to a pre-defined risk criterion, we revealed and
resolved the stuckness where one-sided tendency on risk can lead to biased action selection. To
our best knowledge, this paper is the first attempt in DRL to integrate risk-sensitivity and exploration by using time-varying Bellman objective with theoretical analysis. We prove that PDBOO is
theoretically guaranteed for convergence, and has a unique fixed point in a weaker condition than
contraction. By obtaining the same fixed point as the standard Bellman optimality operator, we can
obtain a risk-neutral optimal policy even if the agent explored with distortion risk measure. Our
method shows better empirical results compared to recent exploration methods for distributional RL
with various environments.


-----

REFERENCES

Marc G Bellemare, Will Dabney, and R´emi Munos. A distributional perspective on reinforcement
learning. In International Conference on Machine Learning, pp. 449–458. PMLR, 2017.

Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G. Bellemare.
[Dopamine: A Research Framework for Deep Reinforcement Learning. 2018. URL http://](http://arxiv.org/abs/1812.06110)
[arxiv.org/abs/1812.06110.](http://arxiv.org/abs/1812.06110)

Richard Y Chen, Szymon Sidor, Pieter Abbeel, and John Schulman. Ucb exploration via qensembles. arXiv preprint arXiv:1706.01502, 2017.

Yunho Choi, Kyungjae Lee, and Songhwai Oh. Distributional deep reinforcement learning with a
mixture of gaussians. In 2019 International Conference on Robotics and Automation (ICRA), pp.
9791–9797. IEEE, 2019.

Yinlam Chow, Aviv Tamar, Shie Mannor, and Marco Pavone. Risk-sensitive and robust decisionmaking: a cvar optimization approach. arXiv preprint arXiv:1506.02188, 2015.

Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone. Risk-constrained reinforcement learning with percentile risk criteria. The Journal of Machine Learning Research, 18
(1):6070–6120, 2017.

Kamil Ciosek, Quan Vuong, Robert Loftin, and Katja Hofmann. Better exploration with optimistic
actor-critic. arXiv preprint arXiv:1910.12807, 2019.

William R Clements, Bastien Van Delft, Benoˆıt-Marie Robaglia, Reda Bahi Slaoui, and S´ebastien
Toth. Estimating risk and uncertainty in deep reinforcement learning. arXiv preprint

arXiv:1905.09638, 2019.

Will Dabney, Georg Ostrovski, David Silver, and R´emi Munos. Implicit quantile networks for distributional reinforcement learning. In International conference on machine learning, pp. 1096–1105.
PMLR, 2018a.

Will Dabney, Mark Rowland, Marc Bellemare, and R´emi Munos. Distributional reinforcement learning with quantile regression. In Proceedings of the AAAI Conference on Artificial Intelligence,
2018b.

Jan Dhaene, Alexander Kukush, Dani¨el Linders, and Qihe Tang. Remarks on quantiles and distortion
risk measures. European Actuarial Journal, 2(2):319–328, 2012.

Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization
using the wasserstein metric: Performance guarantees and tractable reformulations. Mathematical
Programming, 171(1):115–166, 2018.

Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. In Thirty-second AAAI conference on artificial intelligence, 2018.

Ramtin Keramati, Christoph Dann, Alex Tamkin, and Emma Brunskill. Being optimistic to be
conservative: Quickly learning a cvar policy. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 34, pp. 4436–4443, 2020.

Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, and
Michael Bowling. Revisiting the arcade learning environment: Evaluation protocols and open
problems for general agents. Journal of Artificial Intelligence Research, 61:523–562, 2018.

Borislav Mavrin, Hengshuai Yao, Linglong Kong, Kaiwen Wu, and Yaoliang Yu. Distributional
reinforcement learning for efficient exploration. In International conference on machine learning,
pp. 4424–4434. PMLR, 2019.

Thomas M Moerland, Joost Broekens, and Catholijn M Jonker. The potential of the return distribution for exploration in rl. arXiv preprint arXiv:1806.04242, 2018.


-----

Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped dqn. Advances in neural information processing systems, 29:4026–4034, 2016.

Ian Osband, Benjamin Van Roy, Daniel J Russo, Zheng Wen, et al. Deep exploration via randomized
value functions. J. Mach. Learn. Res., 20(124):1–62, 2019.

John Quan and Georg Ostrovski. DQN Zoo: Reference implementations of DQN-based agents,
[2020. URL http://github.com/deepmind/dqn_zoo.](http://github.com/deepmind/dqn_zoo)

Antonin Raffin, Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, and Noah Dor[mann. Stable baselines3. https://github.com/DLR-RM/stable-baselines3, 2019.](https://github.com/DLR-RM/stable-baselines3)

Marc Rigter, Bruno Lacerda, and Nick Hawes. Risk-averse bayes-adaptive reinforcement learning.

arXiv preprint arXiv:2102.05762, 2021.

R Tyrrell Rockafellar, Stanislav Uryasev, et al. Optimization of conditional value-at-risk. Journal of

risk, 2:21–42, 2000.

Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczynski. Lectures on stochastic

programming: modeling and theory. SIAM, 2021.

Elena Smirnova, Elvis Dohmatob, and J´er´emie Mary. Distributionally robust reinforcement learning.

arXiv preprint arXiv:1902.08708, 2019.

Silvestr Stanko and Karel Macek. Risk-averse distributional reinforcement learning: A cvar optimization approach. In IJCCI, pp. 412–423, 2019.

Yunhao Tang and Shipra Agrawal. Exploration by distributional reinforcement learning. arXiv

preprint arXiv:1805.01907, 2018.

Derek Yang, Li Zhao, Zichuan Lin, Tao Qin, Jiang Bian, and Tie-Yan Liu. Fully parameterized quantile function for distributional reinforcement learning. Advances in neural information processing
systems, 32:6193–6202, 2019.

Insoon Yang. Wasserstein distributionally robust stochastic control: A data-driven approach. IEEE

Transactions on Automatic Control, 2020.

Tianpei Yang, Hongyao Tang, Chenjia Bai, Jinyi Liu, Jianye Hao, Zhaopeng Meng, and Peng
Liu. Exploration in deep reinforcement learning: A comprehensive survey. arXiv preprint

arXiv:2109.06668, 2021.

Shangtong Zhang, Bo Liu, and Shimon Whiteson. Mean-variance policy iteration for risk-averse
reinforcement learning. arXiv preprint arXiv:2004.10888, 2020.


-----

A PROOF

A.1 TECHNICAL LEMMA

Before proving our theoretical results, we present two inequalities for supremum to clear the description.

1. supx∈X|f (x) + g(x)| ≤ _xsup∈X|f_ (x)| + supx∈X|g(x)|


2. _xsup∈Xf_ (x) − _xsup[′]∈Xg(x[′])_ _≤_ _x,xsup[′]∈X|f_ (x) − _g(x[′])|_

_Proof of 1. Since |f_ (x) + g(x)| ≤|f (x)| + |g(x)| holds for all x ∈ _X,_

_xsup∈X_ _|f_ (x) + g(x)| ≤ _xsup∈X(|f_ (x)| + |g(x)|) ≤ _xsup∈X_ _|f_ (x)| + supx∈X _|g(x)|_


_Proof of 2. Since_ _∥a∥−∥b∥_ _≤∥a −_ _b∥_ for any norm ∥· ∥ and for a large enough M,

_x,xsup[′]∈X|f_ (x) − _g(x[′])| ≥_ _xsup∈X|f_ (x) − _g(x)|= supx∈X|(f_ (x) + M ) − (g(x) + M )|

_≥_ supx∈X(f (x) + M ) − _xsup∈X(g(x) + M_ )

= supx∈X _f_ (x) − _xsup[′]∈X_ _g(x[′])_


A.2 PROOF OF THEOREM 3

**Theorem 3. If ξt converges to 1 uniformly on Ω, then Eτ ξt also converges to Eτ uniformly on Z**
for all s ∈S and a ∈A.

_Proof. Recall that_ = _Z :_ _P(R)_ E[ _Z(s, a)_ ] _Vmax,_ (s, a) . Then for any Z
_Z_ _S × A →_ _|_ _|_ _≤_ _∀_ _∈Z_
and ξ Ξ, n o
_∈_

E[ _τ ξZ_ ] _Rmax + γ [R][max]_
_|_ _|_ _≤_ 1 _γ_ [=][ R]1 [max]γ [=][ V][max][.]

_−_ _−_

which implies PDBOO is closed in, i.e. τ ξZ for all ξ Ξ. Hence, for any sequence ξt,
_Z_ _∈Z_ _∈_
_Z_ [(][n][)] = τ ξn:1 _Z_ for any n 0.
_∈Z_ _≥_

Since ξt converges to 1 uniformly on Ω, there exists T such that for any t > T,

sup _ξt(w)_ 1 _ϵ._
_w∈Ω_ _|_ _−_ _| ≤_

For any Z ∈Z, s ∈S, a ∈A, and t > T, by using H¨older’s inequality,


sup sup sup
_Z∈Z_ _s,a_ _[|][E][ξ][t]_ [[][Z][(][s, a][)]][ −] [E][[][Z][(][s, a][)]][|][ = sup]Z∈Z _s,a_


_w∈Ω(1 −_ _ξt(w))Z(s, a, w)P(w)dw_


_w∈Ω_ _|Z(s, a, w)|P(w)dw_


sup _ξt(w)_ 1 sup sup
_≤_ _w∈Ω_ _|_ _−_ _|_ _Z∈Z_ _s,a_

_≤_ _ϵVmax_

which implies that Eξt converges to E uniformly on Z for all s, a.


-----

By using A.1, we can get the desired result.

sup sup
_Z∈Z_ _s,a_ _[|][E][[][τ][ ξ][t]_ _[Z][(][s, a][)]][ −]_ [E][[][τ][ Z][(][s, a][)]][|]

_≤_ _Zsup∈Z_ sups,a _[|][E][[][τ][ ξ][t]_ _[Z][(][s, a][)]][ −]_ [E][ξ][t] [[][τ][ ξ][t] _[Z][(][s, a][)]][|][ + sup]Z∈Z_ sups,a _[|][E][ξ][t]_ [[][τ][ ξ][t] _[Z][(][s, a][)]][ −]_ [E][[][τ][ Z][(][s, a][)]][|]


_≤_ _ϵVmax + γ supZ∈Z_ sups,a [E][s][′]  _a[′][ E][ξ][t]_ [[][Z][(][s][′][, a][′][)]][ −] [sup]a[′′][ E][[][Z][(][s][′][, a][′′][)]]

_≤_ _ϵVmax + γ supZ_ _ssup[′],a[′][ |][E][ξ][t]_ [[][Z][sup][(][s][′][, a][′][)]][ −] [E][[][Z][(][s][′][, a][′][)]][|]
_∈Z_

_≤_ _ϵVmax + γϵVmax_
= (1 + γ)ϵVmax.

A.3 PROOF OF THEOREM 4


**Theorem 4. For a sequence of bound ∆n, let** [¯]∆n (Z [(][n][−][1)]) := _s,a_ _n_ _Z_ [(][n][−][1)](s, a) . If we
_U_ _[U][∆]_

sample ξn from _U[¯]∆n_ (Z [(][n][−][1)]) for every iteration and _n=1_ [∆][n][ <][ ∞] [holds, then, the expectation]  
of iterated operator τ ξn:1 has a fixed point E[Z _[∗]]. Moreover, the following bound holds,[T]_

[P][∞] _k_

_∞_

sup E[Z [(][n][)](s, a)] E[Z _[∗](s, a)]_ 2γ[k][−][1]Vmax + 2 _γ[i](∆k+2_ _i + ∆k+1_ _i)_ _._
_s,a_ _−_ _≤_ _k=n_ _i=1_ _−_ _−_ !

X X

_Proof. We denote a[∗]i_ [(][ξ][n][) = arg max] Eξn [Zi[(][n][−][1)](s[′], a[′])] as the greedy action of Zi[(][n][−][1)] under
_a[′]_

perturbation ξn. Also, we denote sup
_s,a_

_[| · |][ which is the supremum norm over][ s][ and][ a][ as][ ∥· ∥][sa][.]_

Before we start from the term E[Z [(][k][+1)]] − E[Z [(][k][)]] _sa[, for a given][ (][s, a][)][,]_
E[Z [(][k][+1)](s, a)] − E[Z [(][k][)](s, a)]

_≤_ _γ sups[′]_ E[Z [(][k][)](s[′], a[∗](ξk+1))] − E[Z [(][k][−][1)](s[′], a[∗](ξk))]

_γ sup_ E[Z [(][k][)](s[′], a[∗](ξk+1)] max + max
_≤_ _s[′]_ _−_ _a[′][ E][[][Z]_ [(][k][)][(][s][′][, a][′][)]] _a[′][ E][[][Z]_ [(][k][)][(][s][′][, a][′][)]]



max + max
_−_ _a[′][ E][[][Z]_ [(][k][−][1)][(][s][′][, a][′][)]] _a[′][ E][[][Z]_ [(][k][−][1)][(][s][′][, a][′][)]][ −] [E][[][Z] [(][k][−][1)][(][s][′][, a][∗][(][ξ][k][))]]

_k_ 

_≤_ _γssup[′],a[′]_ E[Z [(][k][)](s[′], a[′])] − E[Z [(][k][−][1)](s[′], a[′])] + γ sups[′] E[Z [(][i][)](s[′], a[∗](ξi+1))]

_i=k_ 1

X− 

max
_−_ _a[′][ E][[][Z]_ [(][i][)][(][s][′][, a][′][)]]
 _k_

_γ_ E[Z [(][k][)]] E[Z [(][k][−][1)]] sup E[Z [(][i][)](s[′], a[∗](ξi+1))]
_≤_ _−_ _sa_ [+][ γ] _s[′]_

_i=k_ 1

X− 

Eξi+1 [Z [(][i][)](s[′], a[∗](ξi+1))] + max
_−_ _a[′][ E][ξ][i][+1]_ [[][Z] [(][i][)][(][s][′][, a][′][)]][ −] [max]a[′′][ E][[][Z] [(][i][)][(][s][′][, a][′′][))]]

_k_ 

_γ_ E[Z [(][k][)]] E[Z [(][k][−][1)]] sup E[Z [(][i][)](s[′], a[′])] Eξi+1 [Z [(][i][)](s[′], a[′])]
_≤_ _−_ _sa_ [+ 2][γ] _s[′],a[′]_ _−_

_i=k_ 1

X−  

_k_

_γ_ E[Z [(][k][)]] E[Z [(][k][−][1)]] ∆i+1
_≤_ _−_ _sa_ [+ 2][γ]

_i=Xk−1_

where we use A.1.1 in third and fifth line and A.1.2 in sixth line.


-----

Taking a supremum over s and a, then for all k > 0,

_k_

E[Z [(][k][+1)]] E[Z [(][k][)]] E[Z [(][k][)]] E[Z [(][k][−][1)]] _γ∆i+1_
_−_ _sa_ _−_ _sa_ [+ 2]

_[≤]_ _[γ]_ _i=Xk−1_

_k−1_

_γ[2]_ E[Z [(][k][−][1)]] E[Z [(][k][−][2)]] _γ[2]∆i+1 + 2_
_≤_ _−_ _sa_ [+ 2]

_i=Xk−2_

.
.
.


_γ∆i+1_
_i=Xk−1_


_k_

_γ[k]_ E[Z [(1)]] E[Z] _γ[i](∆k+2_ _i + ∆k+1_ _i)_
_≤_ _−_ _sa_ [+ 2] _−_ _−_

_i=1_

X

_k_

2γ[k]Vmax + 2 _γ[i](∆k+2_ _i + ∆k+1_ _i)_
_≤_ _−_ _−_

_i=1_

X

_γ_

1 _γ_ _[<][ ∞]_ [and][ P]i[∞]=1 [∆][i][ <][ ∞] [by assumption, we have]
_−_

_k_

_i=1_ _γ[i]∆k+1−i →_ 0

X


Since _i=1_ _[γ][i][ =]_

[P][∞]


which is resulted from the convergence of Cauchy product of two sequences _γ[i]_ and ∆i . Hence,
_{_ _}_ _{_ _}_
_{E[Z_ [(][k][)]]} is a Cauchy sequence and therefore converges for every Z ∈Z.

Let E[Z _[∗]] be the limit point of the sequence {E[Z_ [(][n][)]]}. Then,
E[Z _[∗]]_ E[Z [(][n][)]] E[Z [(][n][+][l][)]] E[Z [(][n][)]]
_−_ _sa_ [= lim]l _−_ _sa_
_→∞_

_∞_

E[Z [(][k][+1)]] E[Z [(][k][)]]

_≤_ _−_ _sa_

_k=n_

X

_∞_ _k_

= 2γ[k]Vmax + 2 _γ[i](∆k+2_ _i + ∆k+1_ _i)_ _._

_−_ _−_

_k=n_ _i=1_

X  X 

■

A.4 PROOF OF THEOREM 5

**Theorem 5. If {∆n} follows the assumption in Theorem 4, then E[Z** _[∗]] is the unique solution of_
Bellman optimality equation.

_Proof. The proof follows by linearity of expectation. Denote the Q-value based operator as_ _τ[¯] . Note_
that ∆n converges to 0 with regularity of Z implies that ξn converges to 1 uniformly on Ω. By
Theorem 3, for a given ϵ > 0, there exists a constant K = max(K1, K2) such that for every
_k_ _K1,_
_≥_

sup _τ ξk_ E[Z] _τ E[Z]_ _sa_
_Z_ _∥_ [¯] _−_ [¯] _∥_ _≤_ 2[ϵ] _[.]_
_∈Z_

Since _τ[¯] is continuous, for every k_ _K2,_
_≥_

## τ E[Z [(][k][)]] τ E[Z [∗]] sa
_∥_ [¯] _−_ [¯] _∥_ _≤_ 2[ϵ] _[.]_

Thus, it holds that
_∥τ[¯] ξk+1_ E[Z [(][k][)]] − _τ[¯] E[Z_ _[∗]]∥sa ≤∥τ[¯] ξk+1_ E[Z [(][k][)]] − _τ[¯] E[Z_ [(][k][)]]∥sa + ∥τ[¯] E[Z [(][k][)]] − _τ[¯] E[Z_ _[∗]]∥sa_

_≤_ _Zsup∈Z_ _∥τ[¯] ξk+1_ E[Z] − _τ[¯] E[Z]∥sa + ∥τ[¯] E[Z_ [(][k][)]] − _τ[¯] E[Z_ _[∗]]∥sa_

_≤_ 2 [ϵ] [+][ ϵ]2


= ϵ.


-----

Therefore, we have
E[Z _[∗]] = lim_ _τ¯ ξk+1_ E[Z [(][k][)]] = τ[¯] E[Z _[∗]]_
_k→∞_ [E][[][Z] [(][k][)][] = lim]k→∞ [E][[][Z] [(][k][+1)][] = lim]k→∞ [E][[][τ][ ξ][k][+1] _[Z]_ [(][k][)][] = lim]k→∞

Since the standard Bellman optimality operator has a unique solution, we derived the desired result.
■

B ALGORITHM PIPELINE

Figure 6: Pipeline of PDBOO.

Figure 6 shows the pipeline of our algorithm. With the schedule of perturbation bound ∆n, the
_{_ _}_
ambiguity set U∆n (Zn−1) can be defined by previous Zn−1. For each step, (distributional) perturbation ξn is sampled from ∆n (Zn 1) by the symmetric Dirichlet distribution and then PDBOO τ ξn
_U_ _−_
can be performed.

C IMPLEMENTATION DETAILS

Except for each own hyperparameter, our algorithms and DLTV shares the same hyperparameter and
network architecture with QR-DQN. Also, we set up p-DLTV by only multiplying a gaussian noise
_N_ (0, 1) to the coefficient of DLTV. We do not combine any additional improvements such as double
Q-learning, dueling network, prioritized replay, and n-step update. Experiments on LunarLander-v2
and Atari games were performed with 3 random seeds.

C.1 N-CHAIN

For hyperparameter settings, we initialize all agents with a random policy for 500 steps and then
train by 20K steps with 10 random seeds. ϵ−greedy policy which is only executed on QR-DQN
annealed linearly from 1 to 0.01 over the first 2500 steps. The batch size was 64 and the discount
factor was γ = 0.9. We update the network every 1 step and the number of steps to update targets
was 25 steps.

C.2 LUNARLANDER-V2

The hyperparameters of QR-DQN were followed by the settings reported in Raffin et al. (2019)
for a fair comparison. Our experiments used 2 layers of MLP with 256 hidden units. We used the
experience replay with batch size 128 and the buffer size 1 × 10[5]. The number of quantiles N
was 170 and γ = 0.995. As a stochastic gradient optimizer, we adopt Adam with a learning rate
1.5 × 10[−][3] with a linear decaying schedule. ϵ-greedy schedule was only performed on QR-DQN.
For the rest, we used ct = 0.05, and ∆= 5 × 10[4]. We evaluated each algorithm for every 10K
training steps by averaging 5 episodes.

C.3 ATARI GAMES

For a fair comparison, our hyperparameter setting is aligned with Dabney et al. (2018b). The number
of quantile fraction N is 200. We set γ = 0.99 and ct = 50 which refers to Mavrin et al. (2019) and
∆= 1 × 10[6]. We use ϵ-greedy with threshold ϵ = 0.01 at training stage and ϵ = 0.001 at test stage.


-----

C.4 PSEUODOCODE OF P-DLTV

**Algorithm 2: Perturbed DLTV (p-DLTV)**
**Input: (s, a, r, s[′]), γ ∈** [0, 1)
_Q(s[′], a[′]) =_ _N[1]_ _j_ _[θ][j][(][s][′][, a][′][)]_

_ct ∼_ _c N_ (0, [ln]t[ t]P[)] # Randomize the coefficient

_a[∗]_ arg maxa′ (Q(s[′], a[′]) + ct _σ+[2]_ [(][s][′][, a][′][))]
_←_

## τ θj r + γθj(s[′], a[∗]), j q
_←_ _∀_

**Output:** _i=1_ [E][j][[][ρ]τ[κ]ˆi [(][τ][ θ][j][ −] _[θ][i][(][s, a][))]]_

[P][N]

D FURTHER EXPERIMENTAL RESULTS OF ATARI GAMES


We test our algorithm under 30 no-op settings to align with previous works. We compare our baseline
results with results from the DQN Zoo framework (Quan & Ostrovski, 2020), which provides the
full benchmark results on 57 Atari games at 50M and 200M frames. We report the average of the best
scores over 5 seeds for each baseline algorithms up to 50M frames. However, recent studies tried to
follow the setting proposed by Machado et al. (2018) for reproducibility, where they recommended
using sticky actions. Hence, we provide all human normalized scores results across 55 Atari games
including previous report of Yang et al. (2019), Dopamine and DQN Zoo framework to help the
follow-up researchers as a reference. We exclude Defender and Surround which is not reported on
Yang et al. (2019) because of relialbility issues in the Dopamine framework.

|Col1|Mean|Median|
|---|---|---|
|DQN-dopamine(50M) DQN-zoo(50M) DQN-zoo(200M) DQN(200M)|401% 507% 804% 221%|51% 69% 84% 79%|
|QR-DQN-dopamine(50M) QR-DQN-zoo(50M) QR-DQN-zoo(200M) QR-DQN(200M)|562% 707% 1714% 902%|93% 119% 174% 193%|
|IQN-dopamine(50M) IQN-zoo(50M) IQN-zoo(200M) IQN(200M)|940% 1057% 2070% 1112%|124% 143% 229% 218%|
|RAINBOW-dopamine(50M) RAINBOW-zoo(50M) RAINBOW-zoo(200M) RAINBOW(200M)|965% 1366% 2115% 1213%|123% 154% 246% 230%|
|PQR(50M)|1121%|124%|



Table 1: Mean and median of best scores across 55 Atari games on 50M frames, measured as percentages of human baseline (Castro et al., 2018; Yang et al., 2019; Quan & Ostrovski, 2020)

Table 1 provides the mean and median human normalized scores across 55 Atari games. Due to
the high computational cost, our algorithm was evaluated on 50M frames to provide results over as
many environments as possible. It is observed that PQR shows better performance in terms of both
mean and median metrics than QR-DQN. Since our method is based on QR-DQN, we would expect
that PDBOO can be combined with IQN (Dabney et al., 2018a) or techniques in Rainbow (Hessel
et al., 2018) as an efficient exploration method, and the performance can be further improved. Since
ourmethod is based on QR-DQN, PDBOO can be combined with IQN (Dabney et al., 2018a) or
the techniques in Rainbow (Hessel et al., 2018), such as double q-learning, n-step updates, dueling
networks and prioritized experience replay. We would expect that PQR for efficient exploration
would benefit from the additional improvement of performance as IQN and Rainbow outperform
QR-DQN.


-----

|GAMES|RANDOM|HUMAN|DQN(50M)|QR-DQN(50M)|IQN(50M)|RAINBOW(50M)|PQR(50M)|
|---|---|---|---|---|---|---|---|
|Alien Amidar Assault Asterix Asteroids Atlantis BankHeist BattleZone BeamRider Berzerk Bowling Boxing Breakout Centipede ChopperCommand CrazyClimber DemonAttack DoubleDunk Enduro FishingDerby Freeway Frostbite Gopher Gravitar Hero IceHockey Jamesbond Kangaroo Krull KungFuMaster MontezumaRevenge MsPacman NameThisGame Phoenix Pitfall Pong PrivateEye Qbert Riverraid RoadRunner Robotank Seaquest Skiing Solaris SpaceInvaders StarGunner Tennis TimePilot Tutankham UpNDown Venture VideoPinball WizardOfWor YarsRevenge Zaxxon|227.8 5.8 222.4 210.0 719.1 12850.0 14.2 2360.0 363.9 123.7 23.1 0.1 1.7 2090.9 811.0 10780.5 152.1 -18.6 0.0 -91.7 0.0 65.2 257.6 173.0 1027.0 -11.2 29.0 52.0 1598.0 258.5 0.0 307.3 2292.3 761.4 -229.4 -20.7 24.9 163.9 1338.5 11.5 2.2 68.4 -17098.1 1236.3 148.0 664.0 -23.8 3568.0 11.4 533.4 0.0 16256.9 563.5 3092.9 32.5|7127.7 1719.5 742.0 8503.3 47388.7 29028.1 753.1 37187.5 16926.5 2630.4 160.7 12.1 30.5 12017.0 7387.8 35829.4 1971.0 -16.4 860.5 -38.7 29.6 4334.7 2412.5 3351.4 30826.4 0.9 302.8 3035.0 2665.5 22736.3 4753.3 6951.6 8049.0 7242.6 6463.7 14.6 69571.3 13455.0 17118.0 7845.0 11.9 42054.7 -4336.9 12326.7 1668.7 10250.0 -8.3 5229.2 167.6 11693.2 1187.5 17667.9 4756.5 54576.9 9173.3|1633.9 344.6 3744.7 5994.6 1590.9 383243.6 477.1 22167.1 8276.5 644.5 49.2 90.2 350.0 6912.0 1081.7 109112.5 9695.5 -14.9 850.5 15.2 24.6 614.8 4289.8 271.2 10821.5 -4.1 524.3 8146.9 10626.8 25251.6 0.4 2427.6 7260.8 12646.5 0.0 19.2 542.7 6541.3 7467.2 32714.0 35.7 3239.3 -13445.9 3388.0 1314.2 30631.9 0.0 3439.9 192.8 11192.4 154.2 258626.0 4914.8 18540.6 4890.3|1891.7 761.2 13951.4 21225.7 2113.3 962130.0 1267.9 31242.0 14710.0 936.0 58.5 99.7 478.6 9334.4 1453.6 111783.3 116973.3 -7.0 2238.6 33.4 33.5 4346.2 7425.6 617.6 11823.4 -1.0 1411.5 14885.9 11004.2 37140.8 0.0 3385.0 12595.6 37830.1 0.0 21.0 100.0 14885.8 10019.4 55506.2 56.6 8990.9 -9198.1 2523.6 1960.3 63950.3 0.0 7835.8 229.2 45653.5 17.5 182452.1 19040.8 23385.8 9632.6|1991.0 853.6 18910.2 33625.8 2305.9 907430.0 1298.1 33382.4 23600.8 960.9 63.3 99.6 556.8 8124.0 2054.5 134466.5 115749.8 -7.6 2345.7 27.0 33.8 6187.6 34520.2 535.9 13628.9 -2.6 1336.8 14504.8 10028.9 44248.7 0.2 3075.4 12538.4 37153.7 0.0 21.0 100.0 15681.3 14311.3 55873.9 55.5 18943.3 -9310.5 3612.9 3058.2 81730.5 0.0 6971.4 209.0 65773.2 54.1 416941.4 15892.2 23115.6 8686.6|4507.8 2649.4 11921.0 35954.1 1917.8 954790.0 1089.8 36797.6 17280.9 2191.4 64.9 99.9 349.4 6055.8 6277.6 168808.9 98203.8 0.2 2350.7 38.5 34.0 9902.2 37955.5 2253.4 38189.0 2.5 15191.8 14670.3 9055.3 32520.8 80.3 3894.8 11609.4 61410.6 0.0 21.0 160.0 26067.9 17785.7 54265.5 67.6 3057.5 -14901.8 3714.6 2746.3 111760.9 0.0 12731.9 199.9 77524.5 1.1 589122.4 13362.6 69092.9 23620.4|2455.8 938.4 10759.2 10490.5 1662.0 897640.0 1038.8 28470.5 10224.9 137873.1 86.9 97.1 380.3 7291.2 1300.0 84390.9 73794.0 -7.5 2341.2 31.7 34.0 4148.2 47054.5 635.8 12579.2 -1.4 2121.8 14617.1 9746.1 43258.6 0.0 2928.9 10298.2 20453.8 0.0 21.0 372.4 15267.4 11175.3 50854.7 60.3 19652.5 -9299.3 2640.0 1749.4 62920.6 -1.0 6506.4 231.3 36008.1 993.3 465578.3 6132.8 27674.4 10806.6|


Table 2: Raw scores for a single seed across all 55 games, starting with 30 no-op actions. We report
the best scores for DQN, QR-DQN, IQN and Rainbow on 50M frames, averaged by 5 seeds. Reference values were provided by DQN Zoo framework (Quan & Ostrovski, 2020)


-----

|GAMES|RANDOM|HUMAN|DQN(50M)|QR-DQN(50M)|IQN(50M)|RAINBOW(50M)|PQR(50M)|
|---|---|---|---|---|---|---|---|
|Alien Amidar Assault Asterix Asteroids Atlantis BankHeist BattleZone BeamRider Berzerk Bowling Boxing Breakout Centipede ChopperCommand CrazyClimber DemonAttack DoubleDunk Enduro FishingDerby Freeway Frostbite Gopher Gravitar Hero IceHockey Jamesbond Kangaroo Krull KungFuMaster MontezumaRevenge MsPacman NameThisGame Phoenix Pitfall Pong PrivateEye Qbert Riverraid RoadRunner Robotank Seaquest Skiing Solaris SpaceInvaders StarGunner Tennis TimePilot Tutankham UpNDown Venture VideoPinball WizardOfWor YarsRevenge Zaxxon|227.8 5.8 222.4 210 719.1 12850.0 14.2 2360.0 343.9 123.7 23.1 0.1 1.7 2090.9 811 10780.5 152.1 -18.6 0 -91.7 0 65.2 257.6 173.0 1027 -11.2 29.0 52.0 1598 258.5 0.0 307.3 2292.3 761.4 -229.4 -20.7 24.9 163.9 1338.5 11.5 2.2 68.4 -17098.1 1236.3 148.0 664.0 -23.8 3568.0 11.4 533.4 0 16256.9 563.5 3092.9 32.5|7127.7 1719.5 742 8503.3 47388.7 29028.1 753.1 37187.5 16926.5 2630.4 160.7 12.1 30.5 12017.0 7387.8 35829.4 1971.0 -16.4 860.5 -38.7 29.6 4334.7 2412.5 3351.4 30826.4 0.9 302.8 3035.0 2665.5 22736.3 4753.3 6951.6 8049.0 7245.6 6463.7 14.6 69571.3 13455.0 17118.0 7845.0 11.9 42054.7 -4336.9 12326.7 1668.7 10250.0 -9.3 5229.2 167.6 11693.2 1187.5 17667.9 4756.5 54576.9 9173.3|1688.1 888.2 1615.9 3326.1 828.2 388466.7 720.2 15110.3 4771.3 529.2 38.5 80.0 113.5 3403.7 1615.3 111493.8 4396.7 -16.7 799.5 12.3 25.8 760.2 3495.8 250.7 12316.4 -6.7 500.0 6768.2 6181.1 20418.8 2.6 2727.2 5697.3 5833.7 -16.8 13.2 1884.6 8216.2 9077.8 39703.1 25.8 1585.9 -17038.2 2029.5 1361.1 1676.5 -0.1 3200.9 138.8 10405.6 50.8 216042.7 2664.9 20375.7 1928.6|2754.2 841.6 2233.1 3540.1 1333.4 879022.0 964.1 25845.6 7143.0 603.2 55.3 96.6 40.7 3562.5 1600.3 108493.9 3182.6 7.4 2062.5 48.4 33.5 8022.8 3917.1 821.3 14980.0 -4.5 802.3 4727.3 8073.9 20988.3 300.5 3313.9 7307.9 4641.1 -3.4 19.2 680.7 17228.0 13389.4 44619.2 53.6 4667.9 -14401.6 2361.7 940.2 23593.3 19.2 6622.8 209.9 29890.1 1099.6 250650.0 2841.8 66055.9 8177.2|4016.3 1642.8 4305.6 7038.4 1336.3 897558.0 1082.8 29959.7 7113.7 627.3 33.6 97.8 164.4 3746.1 6654.1 131645.8 7715.5 20.2 2268.1 41.9 33.5 7824.9 11192.6 1083.5 18754.0 0.0 1118.8 11385.4 8661.7 33099.9 0.7 4714.4 9432.8 5147.2 -0.4 19.9 1287.3 15045.5 14868.6 50534.1 65.9 20081.3 -13755.6 2234.5 3115.0 60090.0 3.5 9820.6 250.4 44327.6 1134.5 486111.5 6791.4 57960.3 12048.6|2076.2 1669.6 2535.9 5862.3 1345.1 870896.0 1104.9 32862.1 6331.9 697.8 55.0 96.3 69.8 5087.6 5982.0 135786.1 6346.4 17.4 2255.6 37.6 33.2 5697.2 7102.1 926.2 31254.8 2.3 656.7 13133.1 6292.5 26707.0 501.2 3406.4 9389.5 8272.9 0 19.4 4298.8 17121.4 15748.9 51442.4 63.6 3916.2 -17960.1 2922.2 1908.0 39456.3 0.0 9324.4 252.2 18790.7 1488.9 536364.4 7562.7 31864.4 14117.5|2455.8 938.4 10759.2 10490.5 1662.0 897640.0 1038.8 28470.5 10224.9 137873.1 86.9 97.1 380.3 7291.2 1044.0 84390.9 73794.0 -7.5 2341.2 31.7 34.0 4148.2 47054.5 635.8 12579.2 -1.4 2121.8 14617.1 9746.1 43258.6 0.0 2928.9 10298.2 20453.8 0.0 21.0 372.4 15267.4 11175.3 50854.7 60.3 19652.5 -9299.3 2640.0 1749.4 62920.6 -1.0 6506.4 231.3 36008.1 993.3 465578.3 6132.8 27674.4 10806.6|


Table 3: Raw scores for a single seed across all 55 games. We report the best scores for DQN, QRDQN, IQN, and Rainbow on 50M frames, averaged by 5 seeds. Reference values were provided by
Dopamine framework (Castro et al., 2018).


-----

|GAMES|RANDOM|HUMAN|DQN(200M)|QR-DQN(200M)|IQN(200M)|PQR(50M)|
|---|---|---|---|---|---|---|
|Alien Amidar Assault Asterix Asteroids Atlantis BankHeist BattleZone BeamRider Berzerk Bowling Boxing Breakout Centipede ChopperCommand CrazyClimber DemonAttack DoubleDunk Enduro FishingDerby Freeway Frostbite Gopher Gravitar Hero IceHockey Jamesbond Kangaroo Krull KungFuMaster MontezumaRevenge MsPacman NameThisGame Phoenix Pitfall Pong PrivateEye Qbert Riverraid RoadRunner Robotank Seaquest Skiing Solaris SpaceInvaders StarGunner Tennis TimePilot Tutankham UpNDown Venture VideoPinball WizardOfWor YarsRevenge Zaxxon|227.8 5.8 222.4 210.0 719.1 12850.0 14.2 2360.0 363.9 123.7 23.1 0.1 1.7 2090.9 811.0 10780.5 152.1 -18.6 0.0 -91.7 0.0 65.2 257.6 173.0 1027.0 -11.2 29.0 52.0 1598.0 258.5 0.0 307.3 2292.3 761.4 -229.4 -20.7 24.9 163.9 1338.5 11.5 2.2 68.4 -17098.1 1236.3 148.0 664.0 -23.8 3568.0 11.4 533.4 0.0 16256.9 563.5 3092.9 32.5|7127.7 1719.5 742.0 8503.3 47388.7 29028.1 753.1 37187.5 16926.5 2630.4 160.7 12.1 30.5 12017.0 7387.8 35829.4 1971.0 -16.4 860.5 -38.7 29.6 4334.7 2412.5 3351.4 30826.4 0.9 302.8 3035.0 2665.5 22736.3 4753.3 6951.6 8049.0 7242.6 6463.7 14.6 69571.3 13455.0 17118.0 7845.0 11.9 42054.7 -4336.9 12326.7 1668.7 10250.0 -9.3 5229.2 167.6 11693.2 1187.5 17667.9 4756.5 54576.9 9173.3|1620.0 978.0 4280.4 4359.0 1364.5 279987.0 455.0 29900.0 8627.5 585.6 50.4 88.0 385.5 4657.7 6126.0 110763.0 12149.4 -6.6 729.0 -4.9 30.8 797.4 8777.4 473.0 20437.8 -1.9 768.5 7259.0 8422.3 26059.0 0.0 3085.6 8207.8 8485.2 -286.1 19.5 146.7 13117.3 7377.6 39544.0 63.9 5860.6 -13062.3 3482.8 1692.3 54282.0 12.2 4870.0 68.1 9989.9 163.0 196760.4 2704.0 18098.9 5363.0|4871.0 1641.0 22012.0 261025.0 4226.0 971850.0 1249.0 39268.0 34821.0 3117.0 77.2 99.9 742.0 12447.0 14667.0 161196.0 121551.0 21.9 2355.0 39.0 34.0 4384.0 113585.0 995.0 21395.0 -1.7 4703.0 15356.0 11447.0 76642.0 0.0 5821.0 21890.0 16585.0 0.0 21.0 350.0 572510.0 17571.0 64262.0 59.4 8268.0 -9324.0 6740.0 20972.0 77495.0 23.6 10345.0 297.0 71260.0 43.9 705662.0 25061.0 26447.0 13113.0|7022.0 2946.0 29091.0 342016.0 2898.0 978200.0 1416.0 42244.0 42776.0 1053.0 86.5 99.8 734.0 11561.0 16836.0 179082.0 128580.0 5.6 2359.0 33.8 34.0 4324.0 118365.0 911.0 28386.0 0.2 35108.0 15487.0 10707.0 73512.0 0.0 6349.0 22682.0 56599.0 0.0 21.0 200.0 25750.0 17765.0 57900.0 62.5 30140.0 -9289.0 8007.0 28888.0 74677.0 23.6 12236.0 293.0 88148.0 1318.0 698045.0 31190.0 28379.0 21772.0|2455.8 938.4 10759.2 10490.5 1662.0 897640.0 1038.8 28470.5 10224.9 137873.1 86.9 97.1 380.3 7291.2 1044.0 84390.9 73794.0 -7.5 2341.2 31.7 34.0 4148.2 47054.5 635.8 12579.2 -1.4 2121.8 14617.1 9746.1 43258.6 0.0 2928.9 10298.2 20453.8 0.0 21.0 372.4 15267.4 11175.3 50854.7 60.3 19652.5 -9299.3 2640.0 1749.4 60920.6 -1.0 6506.4 231.3 36008.1 993.3 465578.3 6132.8 2764.4 10806.6|


Table 4: Raw scores for a single seed across all 55 games, starting with 30 no-op actions. Note that
PQR was evaluated on 50M frames. We report the published scores for DQN, QR-DQN, and IQN
on 200M frames. Reference values from Yang et al. (2019).


-----

