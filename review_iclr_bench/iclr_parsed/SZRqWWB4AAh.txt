# SABAL: SPARSE APPROXIMATION-BASED BATCH ACTIVE LEARNING

**Anonymous authors**
Paper under double-blind review

ABSTRACT

We propose a novel and general framework (i.e., SABAL) that formulates batch
active learning as a sparse approximation problem. SABAL aims to find a weighted
subset from the unlabeled data pool such that the corresponding training loss function approximates its full data pool counterpart. We realize the general framework
as a sparsity-constrained discontinuous optimization problem that explicitly balances uncertainty and representation for large-scale applications, for which we
propose both greedy and iterative hard thresholding schemes. The proposed method
can adapt to various settings, including both Bayesian and non-Bayesian neural
networks. Numerical experiments show that that SABAL achieves state-of-the-art
performance across different settings with lower computational complexity.

1 INTRODUCTION

Over the last decade, deep neural networks have achieved promising results in various learning tasks.
However, obtaining labels for a complex training dataset can be challenging in practice, as the data
annotation is usually a time-consuming process that may require professional knowledge in certain
applications such as in medicine (Hoi et al., 2006; Shen et al., 2021). Active Learning (AL) (Settles,
2009) is commonly employed to mitigate the problem of scarce labeled data – enabling efficient
model training with limited annotation costs. Given a partially labeled dataset, active learning ideally
selects data samples that are the best for learning. Specifically, it aims to iteratively query the most
helpful data to ask an oracle (human annotator) to annotate. The queried data samples can be added
back to the labeled data pool, and the model is updated. This process is repeated until the model has
achieved the desired performance. Intelligently identifying the most valuable data for annotation,
also known as the query strategy, is the key problem in active learning.

A common strategy is to take the prediction uncertainty or data representation as the metric for
data query. This uncertainty-based approach (Settles, 2009; Tong & Koller, 2001; Gal et al., 2017;
Beluch et al., 2018) works by querying samples with high uncertainty, but often results in selecting
correlated and redundant data samples in each batch (Kirsch et al., 2019; Ducoffe & Precioso, 2018).
_Representation-based approaches (Sener & Savarese, 2017; Yang & Loog, 2019) aim to select a subset_
of data that represents the whole unlabeled dataset, but tend to be computationally expensive and
sensitive to batch sizes (Ash et al., 2019; Shui et al., 2020). More recently, several hybrid approaches
that try to take both uncertainty and representation into consideration have shown advantages (Ash
et al., 2019; Shui et al., 2020; Sinha et al., 2019). This paper takes this hybrid view towards an active
learning framework that balances the trade-off between uncertainty and representation.

Besides hybrid approaches, deep Bayesian active learning has also gained attention due to recent
advances in Bayesian deep learning. Several Bayesian approaches (Gal et al., 2017; Kirsch et al.,
2019) leverage model uncertainty measurements (Gal & Ghahramani, 2015; 2016) determined by
Bayesian neural networks, while other works (Pinsler et al., 2019) leverage progress in Bayesian
Coreset problems (Zhang et al., 2021; Huggins et al., 2016; Campbell & Broderick, 2019). However,
as most existing Bayesian approaches are explicitly designed for Bayesian neural networks, another
goal of this paper is to propose a general method for both Bayesian and non-Bayesian models.

For deep models, it is reasonable to query a large batch of data simultaneously to reduce model
update frequency. The batch selection approach is known as batch active learning. Taking an
optimization perspective, finding the best batch is NP-hard in general. Two common approaches
for such combinatorial problems are the greedy and clustering approaches. Greedy algorithms


-----

select one data sample in sequence until the batch budget is exhausted (Kirsch et al., 2019; Bıyık
et al., 2019; Chen & Krause, 2013). Here, specific conditions of the acquisition function such
as submodularity (Nemhauser et al., 1978) are required to guarantee a good optimization result.
Clustering algorithms regard cluster centers as their queried batch (Sener & Savarese, 2017; Ash
et al., 2019), but can be computationally expensive. To our knowledge, except for Pinsler et al.
(2019) that focus on the Bayesian models, so far active learning has rarely been studied from a sparse
_approximation perspective. This is despite the ubiquity of sparse approximation in signal processing_
for tasks such as dictionary learning (Aharon et al., 2006) and compressed sensing (Donoho, 2006)
due to its performance for discovering a sparse representation while avoiding redundant information.
Here we employ sparse approximation methods for batch active learning tasks.

Our main contributions are summarized in the following. We propose a novel and flexible Sparse
Approximation-based Batch Active Learning framework, i.e., SABAL. We show how SABAL
generalizes batch active learning as a sparse approximation problem and can adapt to different
settings and models. The central intuition of SABAL is finding a weighted subset from the unlabeled
data pool so that its corresponding training loss approximates the full-set loss function in a function
space. We realize the SABAL framework as an efficient finite-dimensional optimization problem:
First, we derive an upper bound to balance the trade-off between uncertainty and representation in a
principled way. Second, we approximate the loss functions using finite-dimensional approximation.
This results in a sparsity-constrained discontinuous optimization problem, for which we propose
several efficient optimization algorithms. We demonstrate the advantages of SABAL in experiments
for both Bayesian and non-Bayesian batch active learning settings.

The structure of this manuscript is as follows. In Section 2, we formulate the general framework of
SABAL, and in Section 3, we realize the framework into a finite-dimensional discontinuous sparse
optimization problem. To solve the resulting optimization problem, we propose two optimization
algorithms in Section 4. Related work are discussed in Section 5 and Appendix Section B. Results of
our experiments are presented in section 6, and all proofs are provided in Appendix Section C.

2 BATCH ACTIVE LEARNING AS SPARSE APPROXIMATION

This section introduces the preliminaries and the general formulation of batch active learning as a
sparse approximation problem.

**Preliminaries** Vectors are denoted as bold lower case letters, e.g., w ∈ R[n]. The l0 pseudonorm of a vector w is denoted as **_w_** 0, i.e., the number of non-zero elements of w. We denote
_∥_ _∥_
R+ := [0, +∞). Distributions are denoted in script, e.g., P, and a random variable is denoted by
tilde, e.g., ˜y ∼ _P. We denote sets in calligraphy or in uppercase Greek alphabet (e.g., D, Θ), and_
additionally we denote [n] := {1, 2, . . ., n}. In supervised learning, given a labeled training dataset
_Dl := {(xi, yi)}i[n]=1[l]_ [, where we denote their domain to be][ x][ ∈X][ and][ y][ ∈Y][, the empirical goal is to]
minimize a loss function Ll(θ) := (xi,yi) _l_ _[ℓ][(][x][i][,][ y][i][;][ θ][)][ formed by the training dataset, where]_

_∈D_
**_θ ∈_** Θ ⊂ R[m] is the parameter of the model and ℓ is a loss function evaluated on individual pairs
of data. Without loss of generality, we assume[P] Θ R[m] is compact and ℓ(x, y; ) : Θ R is in a
_⊂_ _·_ _→_
normed space ( (Θ, R), ) for all x, y. We further assume the constant function f : Θ 1 is
_L_ _∥· ∥†_ _→_
included in (Θ, R). The “ ” in the norm : (Θ, R) R+, representing its definition is a
_L_ _†_ _∥· ∥†_ _L_ _→_
placeholder that will be discussed later.

**Batch Active Learning** Besides the labeled dataset Dl, there is an unlabeled dataset Du :=
_{xj}j[n]=1[u]_ [where the labels are unknown but could be acquired at a high cost through human labeling.]
Combining two datasets, the ideal loss function to minimize w.r.t. θ is

(xi,yi) _l_ _[ℓ][(][x][i][,][ y][i][;][ θ][) +][ P]xj_ _u_ _[ℓ][(][x][j][,][ y]j[⋆][;][ θ][)][,]_ (1)
_∈D_ _∈D_

where yj[⋆] [is the unknown true label corresponding to the data]P **_[ x][j][. Since acquiring true labels could]_**
be costly, we have to impose a budget b (b < nu) on the number of label acquisitions. Therefore, the
_batch active learning problem is to find a subset S ⊂Du such that we can obtain a good model by_
optimizing the following loss function w.r.t. θ,


(xi,yi) _l_ _[ℓ][(][x][i][,][ y][i][;][ θ][) +][ P]xj_ _[ℓ][(][x][j][,][ y]j[⋆][;][ θ][)][,]_ where = b. (2)
_∈D_ _∈S_ _|S|_


-----

**Generalized Batch Active Learning** We start our method by generalizing the classical formulation
(equation 2) by considering an importance weight for each unlabeled data. That is, we aim to find
a sparse non-negative vector w ∈ R[n]+[u] [such that we can obtain a good model by optimizing the]
following loss function w.r.t. θ:

(xi,yi) _l_ _[ℓ][(][x][i][,][ y][i][;][ θ][) +][ P]xj_ _u_ _[w][j][ℓ][(][x][j][,][ y]j[⋆][;][ θ][)][,]_ where **_w_** 0 = b. (3)
_∈D_ _∈D_ _∥_ _∥_

A key questionP now is—what is the criterion for a good w? Comparing the ideal loss function
(equation 1) and the sparse importance weighted loss (equation 3), the only difference is their
unlabeled data loss functions. Therefore, a straight-forward informal criterion for a good importance
weight w is that the two unlabeled data loss functions are close to each other, i.e.,


_L[⋆]w[(][θ][) := 1]_ _wjℓ(xj, yj[⋆][;][ θ][)]_ _L[⋆](θ) := [1]_

_b_ _≈_ _nu_

**_xjX∈Du_**


_ℓ(xj, yj[⋆][;][ θ][)][.]_
**_xjX∈Du_**


However, as the true labels are unknown, we cannot compute L[⋆]w [and][ L][⋆][. Luckily, we can have]
an estimator for the true labels, i.e., estimation based on the labeled data p(˜yj **_xj,_** _l) or an_
approximation of it. Denote P(xj) as an estimated distribution, so ˜yj _P(xj), then the informal |_ _D_
criterion for a good importance weight w then becomes _∼_


_L˜w(θ) := [1]_ _wjℓ(xj, ˜yj; θ)_ _L˜(θ) := [1]_

_b_ _≈_ _nu_

**_xjX∈Du_**


_ℓ(xj, ˜yj; θ)._ (4)
**_xjX∈Du_**


Thus, we are one step closer to evaluating the quality of a weighted selection. The next question is
how to measure the difference between _L[˜] and_ _L[˜]w._

**Difference Between Two Loss Functions(Θ, R) is equipped with the norm**, a straight-forward measurement of the difference betweenGiven the two loss functions _L,[˜]_ _L[˜]w ∈L(Θ, R), where_
_L_ _∥· ∥†_
them is ∥L[˜] − _L[˜]w∥†. However, observing that the optimization of a loss function is shift-invariant, the_
difference between two loss functions should also be shift-invariant. For example, for ∀L ∈L(Θ, R)
we have arg minθ Θ(L(θ) + c) = arg minθ Θ L(θ) for _c_ R, implying that L + c should be
_∈_ _∈_ _∀_ _∈_
treated the same as L. Therefore, to account for the shift-invariance, we define q : L(Θ, R) → R+ as

_q(L) := inf_ _L_ (Θ, R). (5)
_c∈R_ _[∥][L][ +][ c][∥][†][,]_ _∀_ _∈L_

Note that we abuse the notation a bit, i.e., the c in L + c should be the constant function that maps
every θ ∈ Θ to c. The above definition has some nice properties that make it a good difference
measurement of two loss functions, as proved in proposition C.1 in the appendix. In particular, q(·)
satisfies the triangle inequality, and q(L + c) = q(L) for any constant c. Therefore, we can formulate
the generalized batch active learning problem as the following sparse approximation problem.

**Problem 1 (Sparse Approximation-based Batch Active Learning). Given the shift-invariant seminorm**
_q induced by the norm_ _(equation 5), and a label estimation distribution P, the generalized_
_∥· ∥†_
_batch active learning problem (equation 4) is formally defined as_

arg minw R[nu]+ EP[q(L[˜] − _L[˜]w)]_ _s.t._ _∥w∥0 = b,_ (6)
_∈_

_where EP stands for the expectation over ˜yj ∼_ _P(xj) for ∀j ∈_ [nu].

Problem 1 (SABAL) offers a general framework for batch active learning and can be applied with
various settings, i.e., both the norm and the individual loss function ℓ can be chosen based on
_∥· ∥†_
specific problems and applications. In the next section, we introduce two practical realizations of
equation 6 for Bayesian and non-Bayesian active learning respectively.

3 SPARSE APPROXIMATION AS FINITE-DIMENSIONAL OPTIMIZATION

In this section, we transform the sparse approximation problem (equation 6) into a finite-dimensional
sparse optimization problem. First, we address an issue regarding the sampling of EP. Then, we
discuss some concrete choices of P and that lead to a finite-dimensional sparse optimization.
_∥· ∥†_


-----

**Addressing the Sampling Issue** In equation 6, the expectation EP is taken over the product
space ofbe intractable for large datasets. However, it has an upper bound where the complexity of the (˜y1, . . ., ˜ynu ) and each sample has to be remembered for future optimization, which can
optimization is independent of the number of samples from P. First, by the triangle inequality
EP[q(L[˜] − _L[˜]w)] = EP[q(L[˜] −_ EP[L[˜]] + EP[L[˜]] − EP[L[˜]w] + EP[L[˜]w] − _L[˜]w)]_

_≤_ EP[q(L[˜] − EP[L[˜]])] + EP[q(L[˜]w − EP[L[˜]w])] + q(EP[L[˜]] − EP[L[˜]w]) _. (7)_
(i): variance (ii): approximation bias

We can see that it offers a trade-off between bias and variance, where the bias term is immediately

| {z } | {z }

tractable by expanding _L,[˜]_ _L[˜]w:_
(ii) = q(EP[ _n[1]u_ **_xj_** _∈Du_ _[ℓ][(][x][j][,][ ˜]yj; ·)] −_ EP[ [1]b **_xj_** _∈Du_ _[w][j][ℓ][(][x][j][,][ ˜]yj; ·)])_

= q(( _n[1]u_ **_xPj_** _∈Du_ [E][P][(][x]j [)][[][ℓ][(][x][j][,][ ˜]yj; ·)]) − (P[1]b **_xj_** _∈Du_ _[w][j][E][P][(][x]j_ [)][[][ℓ][(][x][j][,][ ˜]yj; ·)])). (8)

It remains to address the variance term (i). Recall that the more accurate P is, the more accurate our

P P

approximation is. Given the decision wj > 0, if the label of xj is acquired, i.e., the oracle (human
annotator) will offer us its true label yj[⋆][, and the labeling distribution would be improved. That being]
said, the distribution of ˜yj given xj and wj > 0 will be concentrated on its true label yj[⋆][,][ i.e.][,]


_P(xj)_ if wj = 0
_δyj[⋆]_ if wj > 0 _[,]_ where w ∈ R[n]+[u] (9)


**_y˜j_** _Pw(xj) :=_
_∼_


where δyj[⋆] [denotes the distribution that][ ˜]yj can only be yj[⋆][. However, the improved distribution][ P][w]
is not known before the acquisition of the true labels yj[⋆] [for][ w][j][ >][ 0][. Fortunately, although][ P][w][(][x][j][)]
is not known, it is known that the corresponding variance for ˜yj would be zero no matter what its
label is. Applying this trick, we show in the following proposition that the term (i) with the improved
label distribution Pw has an upper bound that does not require to know the true labels.
**Proposition 3.1. Let w** R[n]+[u] _[and][ ∥][w][∥][0]_ [=][ b][, by replacing the][ P][ by the improved estimation]
_∈_
_distribution Pw (equation 9) into (i) in equation 7, we have_

EPw [q(L[˜] − EPw [L[˜]])] + EPw [q(L[˜]w − EPw [L[˜]w])] ≤ **_xjX∈Du_** **1(wj = 0) · σj,** (10)

_where σj :=_ _n1u_ [E][P][(][x][j] [)][[][q][(][ℓ][(][x][j][,][ ˜]yj; ·) − EP(xj )[ℓ(xj, ˜yj; ·)])] is the individual variance, and 1(·)

_is the indicator function._

Therefore, combining equation 8 and equation 10, we have a more tractable form of the sparse
approximation, i.e.,

arg minw∈R[nu]+ _q(EP[L[˜]] −_ EP[L[˜]w]) + **_xjX∈Du_** **1(wj = 0) · σj** s.t. _∥w∥0 = b,_ (11)

Intuitively, such decomposition of bias and variance naturally provides metrics of uncertainty and
_representation for active learning, where the variance itself is a metric of uncertainty, meanwhile the_
bias term measures how well a subset of selected data can represent the whole unlabeled data. Now,
it remains to specify the choice of, i.e., the norm that induces q (equation 5).
_∥· ∥†_

**Formulation of the Finite-Dimensional Optimization** We consider two concrete choices of the
for Bayesian and non-Bayesian settings respectively.
_∥· ∥†_

1. In the Bayesian setting, we can easily sample θi _π := p(θ_ _l) from the posterior. Utilizing_
the posterior, we make the norm more concrete by considering the ∼ _| D_ _L[2](π)-norm, i.e.,_
_∥· ∥†_
_∥L∥π[2]_ [=][ E][θ][∼][π][[][L][(][θ][)][2][]][. Accordingly,]
_q(L)[2]_ = inf _π_ [= inf] (12)
_c∈R_ _[∥][L][ +][ c][∥][2]_ _c∈R_ [E][θ][∼][π][[(][L][(][θ][) +][ c][)][2][] =][ E][θ][∼][π][[(][L][(][θ][)][ −] [E][θ][∼][π][[][L][(][θ][)])][2][]][.]

The posterior π tells us where and how to evaluate the “magnitude” of L. Noting that equation 12
is in the form of an expectation, we can draw m samples θi _π to approximate it. Denote_
**_g :=_** _√1m_ [. . ., (L(θi) _L), . . . ][⊤]i=1...m_ _L :=_ _m[1]_ _∼mi=1_ _[L][(][θ][i][)][. equation][ 12][ becomes]_
_−_ [¯] _[∈]_ [R]m[m][ where][ ¯]

1 P
_q(L)[2]_ (L(θi) _L)[2]_ = **_g_** 2[,] (13)
_≈_ _m_ _−_ [¯] _∥_ _∥[2]_

_i=1_

X

where ∥g∥2 is simply the Euclidean norm of the m-dimensional vector g.


-----

2. In the non-Bayesian setting, we evaluate the loss function in a local “window” based on the current
model. We consider the ∥· ∥∞-norm over a Euclidean ball Br(θ0) of radius r centered at the
current model parameter θ0, i.e., _L_ = maxθ _r(θ0)_ _L(θ)_ . Moreover, in the Euclidean ball
_∥_ _∥∞_ _∈B_ _|_ _|_
we approximate L(θ) _L(θ0) +_ _L(θ0)[⊤](θ_ **_θ0). Therefore, we have_**
_≈_ _∇_ _−_

_q(L) = inf_ max
_c∈R_ _[∥][L][ +][ c][∥][∞]_ [= inf]c∈R **_θ∈Br(θ0)_** _[|][L][(][θ][) +][ c][|]_

inf max (14)
_≈_ _c∈R_ **_θ∈Br(θ0)_** _[|][L][(][θ][0][) +][ ∇][L][(][θ][0][)][⊤][(][θ][ −]_ **_[θ][0][) +][ c][|][ =][ r][∥∇][L][(][θ][0][)][∥][2][.]_**

Note that ∥∇L(θ0)∥2 is the Euclidean norm of the gradient vector ∇L(θ0) ∈ R[m].

To estimate the label distribution, P(xj) = p(˜yj **_xj,_** _l) can be directly applied on Bayesian_
models by estimating the predictive model posterior. For non-Bayesian models, one could utilize | _D_
the calibrated model prediction (Guo et al., 2017) as the label distribution. Finally, plugging either
of the two approximations of q(L) into equation 11, and squaring all of the terms for the ease of
optimization, we can formulate the sparse approximation problem as the following finite-dimensional
optimization problem, where α > 0 offers a trade-off between bias and variance.

arg minw∈R[nu]+ _∥v −_ Φw∥2[2] [+][ α] **_xjX∈Du_** **1(wj = 0) · σj[2]** s.t. _∥w∥0 = b,_ (15)

where we denote v ∈ R[m], Φ ∈ R[m][×][n][u] and σj as

_nu_

**_v := n[1]u_** EP(xj )[gj(˜yj)], Φ := [1]b [(][E][P][(][x][1][)][[][g][1][(˜]y1)], . . ., EP(xnu )[gnu (˜ynu )]),

_j=1_

X

_σj = [1]_ EP(xj )[ **_gj(˜yj)_** EP(xj )[gj(˜yj)] 2],

_nu_ _∥_ _−_ _∥_

[. . ., (ℓ(xj, ˜yj; θi) _ℓ), . . . ][⊤]i=1...m[,]_ _ℓ¯_ := _m[1]_ _mi=1_ _[ℓ][(][x][j][,][ ˜]yj; θi)_ if use (13)
**_gj(˜yj) :=_** _ℓ(xj, ˜yj; θ0)_ _−_ [¯] if use (14).
∇ P

(16)


In practice, it is often the case that the number of parameters is less than the number of samples,
_i.e., m < nu, even for over-parameterized neural networks where the gradient of the last layer is_
commonly used to represent the full-model gradient (Katharopoulos & Fleuret, 2018; Ash et al.,
2019). Therefore, if the batch size is big, i.e., b > m, the approximation bias ∥v − Φw∥2[2] [may be]
under-determined with infinitely many w to make v = Φw, and the optimization (equation 15)
may be ”overfitted”. To make our method more stable, we include a ℓ2 regularizer β∥w − **1∥2[2]** [with]
_β > 0. Finally, since wj_ 0, minimizing α **_xj_** _u_ **[1][(][w][j][ = 0)][ ·][ σ]j[2]** [is equivalent to minimizing]
_≥_ _∈D_

_−α_ **_xj_** _∈Du_ **[1][(][w][j][ >][ 0)][ ·][ σ]j[2][. Consequently, we have the following optimization problem.]**

[P]

**Problem 2 (Sparse Approximation as Finite-dimensional Optimization). The finite-dimensional**
_optimization for generalized batch active learning is[P]_


arg min **_v_** Φw 2
**_w_** R[nu]+ _∥_ _−_ _∥[2]_ _[−]_ _[α]_
_∈_


**1(wj > 0) · σj[2]** [+][ β][∥][w][ −] **[1][∥]2[2]** _s.t._ _∥w∥0 = b._ (17)
**_xjX∈Du_**


While simplified, the result is a sparse discontinuous optimization problem generally difficult to solve.
In the next section, we propose two optimization algorithms for equation 17 by exploiting its unique
properties. The overall procedure of SABAL in practice is presented in Algorithm 3 Appendix A.

4 OPTIMIZATION ALGORITHMS

This section focuses on optimizing Problem 2. Rewrite equation 17 f (w) := f1(w) + f2(w), where

_f1(w) := ∥v −_ Φw∥2[2] [+][ β][∥][w][ −] **[1][∥]2[2][,]** _f2(w) := −α_ **_xj_** _∈Du_ **[1][(][w][j][ >][ 0)][ ·][ σ]j[2][.]**

The optimization has two major difficulties, i.e., the nonconvex sparsity constraint **_w_** 0 = b and the

[P] _∥_ _∥_

discontinuous objective function f2. When it comes to sparsity-constrained optimization, there are


-----

two schemes that are widely considered — greedy (Nemhauser et al., 1978; Campbell & Broderick,
2019) and iterative hard thresholding (IHT) (Zhang et al., 2021; Khanna & Kyrillidis, 2018). However,
Problem 2 introduces the new difficulty other than the sparsity constraint, i.e., the discontinuous
component f2, which violate the assumptions of many of these methods which require the use of
gradient. Instead, we propose two algorithms (Algorithm 1&2) specifically for Problem 2 under the
two schemes respectively, while incorporating the discontinuity.

We introduce some notations used in this section. Given a vector g, we denote [g]+ as g with its
negative elements set to 0. For an index j, we denote gj or (g)j to be its j[th] element. For an index
set, we denote [g] to be the vector where ([g] )j = (g)j if j and ([g] )j = 0 if j / .
_S_ _S_ _S_ _∈S_ _S_ _∈S_
Moreover, we denote e[j] to be the unit vector where (e[j])j = 1 and (e[j])i = 0 for ∀i ̸= j.

Although two algorithms use different schemes, they share the same two sub-procedures: a line search
and de-bias step (Algorithm 4 and 5 in Appendix D), which significantly improve the optimization
performance (Zhang et al., 2021). The line search sub-procedure optimally solves the problem
arg minµ∈R f1(w − _µu), i.e., given a direction u, what is the best step size µ to move the w along_
**_u. The de-bias sub-procedure adjusts a sparse w in its own sparse support for a better solution._**


**Opt. Algorithm: Greedy** The core idea of
the greedy approach is noted in line 3 Algorithm 1, where it chooses an index j to move
a step of size τ that minimizes the objective,
_i.e., j ←_ arg minj∈[nu]\S (f1(w + τ **_e[j]) −_**
_f1(w)) + (f2(w + τ_ **_e[j])_** _f2(w)). By ap-_
_−_
proximating f1(w + _τ_ **_e[j])_** _f1(w) by its first-_
_−_
order approximation _f1(w), τ_ **_e[j]_**, and not_⟨∇_ _⟩_
ing that f2(w + τ **_e[j]) −_** _f2(w) = −ασj[2][, we]_
have the greedy step (line 3) in Algorithm 1.
After choosing the index j to include, line 5
chooses an optimal step to move, followed by
a de-bias step that further improves the solution in the current sparse support supp(w).

**Opt. Algorithm: Proximal iterative hard**
**thresholding** The core idea of the proximal
IHT (Algorithm 2) is noted in line 6, where it
combines both the hard thresholding and the
proximal operator. It minimizes the discontinuous f2 in a neighbourhood of the solution s
obtained by minimizing f1, while satisfying
the constraints. As discussed in the section D,
the inner optimization (line 6) can be done optimally by simply picking the top-b elements
from nu elements. After this core step, a debias step improves the solution w within its
sparse support, followed by a momentum step.


**Algorithm 1: SABAL-Greedy**
**Parameter: sparsity b; step size τ** .

**1 w ←** **0; S ←∅**

**2 repeat**

**3** _j_ arg min _τ_ ( _f1(w))j_ _ασj[2]_
_←_ _j∈[nu]\S_ _∇_ _−_

**4** _S ←S ∪{j}_ _(update selection)_

**5** _µ ←_ LineSearch(e[j], w)

**6** **_w ←_** De-bias(w − _µe[j])_

**87 untilw |S|j ← =0 b for;** _∀wj < 0_ _(w ∈_ R+[n][u] _[)]_

**Return: w**


tion in the current sparse support supp(w).

**Algorithm 2: SABAL-IHT**

**Opt. Algorithm: Proximal iterative hard** **Parameter: sparsity b; number of iterations T** .
**thresholding** The core idea of the proximal **1 w ←** **0; z ←** **0**
IHT (Algorithm 2) is noted in line 6, where it **2 repeat**
combines both the hard thresholding and the **3** **_w[′]_** _←_ **_w_** _(save previous w)_
proximal operator. It minimizes the discontin- **4** _µ_ LineSearch( _f1(z), z)_

_←_ _∇_

uousobtained by minimizingthe constraints. As discussed in the section f2 in a neighbourhood of the solution f1, while satisfying D s, **56** **_sw ← ←zw −∈Rarg minµ[nu]+∇[,][∥]f[w]1([∥]z[0][≤])_** _[b]_ 12 _[∥][w](gradient descent)[−]_ **_[s][∥]2[2]_** [+] _[f][2][(][w][)]_
the inner optimization (line 6) can be done op-timally by simply picking the top-frombias step improves the solutionsparse support, followed by a momentum step. nu elements. After this core step, a de- wb within its elements **1110789 untilwwτz ← T ←j ← ← iterationsLineSearch(wDe-bias(0 − forτ ∀(ww;w −j <)ww 0 −[′])** **_w[′], w)(momentum)(w ∈_** R+[n][u] _[)]_

**Return: w**

**Complexity Analysis** We analyze the time
complexity of the proposed algorithms with
respect to the number of data samples n, and the batch size b of batch active learning. Except for line 6
Algorithm 2, all steps are of time complexity O(n). The line 6 Algorithm 2 is finding the b smallest
elements, which can be done in O(n log(b)). Therefore, the time complexity for SABAL-Greedy is
_O(nb), and the time complexity for SABAL-IHT is O(n log(b)). Comparing to the time complexity_
_O(nb[2]) of the state-of-the-art method BADGE (Ash et al., 2019), the two proposed algorithms can_
be much faster, especially with a large batch size b used in practice.


5 RELATED WORK

Our method has several characteristics: (1) it’s a hybrid active learning approach. (2) it’s a general
framework with building blocks easily adapted to both Bayesian and non-Bayesian settings. (3) it


-----

formulates data acquisition as a sparse approximation problem. This section focuses on discussing
some most relevant works, and explain how they motivate and compare to our work. Other related
works are discussed in Appendix B.

As data acquisitions with the trade-off between uncertainty and representation have attracted attention,
several recent works have proposed hybrid active learning methods. One of the state-of-art methods,
BADGE (Ash et al., 2019), captures uncertainty through the lens of gradients, and samples diverse
batches on the gradient embedding by the k-MEANS++ seeding algorithm. However, one of the
downsides of BADGE is the high run-time complexity, as data acquisition speed is crucial in practice.
Sinha et al. (2019) train a Variational Autoencoder and a discriminator in an adversarial fashion.
The discriminator predicts a sample as unlabeled based on its likelihood of representativeness, and a
batch of samples with the lowest confidence will be queried, but their adversarial method is difficult
to apply to general and Bayesian neural networks. Our proposed method explicitly balances the
trade-offs between uncertainty and representation by bias and variance decomposition.

Coreset selection is a common high-level idea used in active learning, and methods vary in how one
characterizes the closeness of a chosen coreset to the full-set. Sener & Savarese (2017) characterizes
the closeness as how much a coreset covers the full-set in the Euclidean distance in a feature space.
It derives an upper bound for the coreset loss based on the Lipschitz continuity and transforms the
original problem to a KCenter problem. However, their method relies on good feature representation,
which is not always guaranteed in practice. Pinsler et al. (2019) is mainly based on existing Bayesian
inference literature, especially the Bayesian Coreset problem (Campbell & Broderick, 2019). They
characterizes the closeness as how much the core-set log-posterior approximates the full-set logposterior, with the log-posterior directly derived from the Bayes’ rule. However, their problem
formulation relies on the Bayesian setting and Bayesian models, and conducting posterior inference
is non-trivial for non-Bayesian models. In constrast, we propose SABAL, which characterizes the
closeness in a more general sense, i.e., through a semi-norm function directly on the difference
between the coreset loss function and the full-set loss function.

6 EXPERIMENT RESULTS

We demonstrate that SABAL is a flexible batch active learning framework with relatively small time
complexity by evaluating its performance on image classification tasks with various models under
different settings. First, using Bayesian neural networks, we show the effectiveness of SABAL on
Bayesian batch active learning. Next, we demonstrate that SABAL can also adapt well to general
batch active learning with general neural networks. Finally, we show that SABAL also has runtime
advantages compared to other state-of-art methods. We also conduct an ablation study to show how
SABAL balances the trade-offs between uncertainty and representation in Appendix E.1.

We have a fixed training, validation, and testing set in each experiment. The model is initially trained
on small amounts of labeled data randomly selected from the training set and then iteratively performs
the data acquisition and annotation. The model is reinitialized and retrained at the beginning of
each active learning iteration. After the model is well trained, its testing accuracy is evaluated on
the testing set as a measure of the performance. All experiments are repeated multiple times using
5 random seeds (3 for the small model LeNet-5 (LeCun et al., 2015)), and the results are reported
as mean and standard deviations. The performance of each iteration are shown in learning curve
plots. To better visualize the overall performance of AL methods, We also measure the area under
curve (AUC) scores of the learning curve of different AL methods across different datasets. The
top two AUC scores are highlighted in bald. We implement SABAL using both proximal IHT and
greedy as two different optimization methods for the sparse approximation, denoted as SABAL-IHT
and SABAL-Greedy, compared with following baselines in literature: (1) Random: A naive baseline
that selects a batch of data uniformly at random. (2) BALD (Houlsby et al., 2011): An uncertaintybased Bayesian method that selects a batch of data with maximum mutual information between
model parameters and predictions. (3) Entropy (Wang & Shang, 2014): An uncertainty-based
non-Bayesian method that selects a batch of data with maximum entropy of the model predictions
Hthat reformulates the coreset selection as a KCenter problem in the feature embedding space. (yi | xi; θ). (4) KCenter (Sener & Savarese, 2017): A representation-based non-Bayesian method (5)
**BADGE (Batch Active Learning by Diverse Gradient Embeddings) (Ash et al., 2019): A hybrid**
non-Bayesian method that samples a diverse batch of data using the k-MEANS++ seeding algorithm.


-----

**(6) Bayesian Coreset (Pinsler et al., 2019): A Bayesian batch active learning approach based on the**
Bayesian Coreset problem (Huggins et al., 2016; Campbell & Broderick, 2019).

**SABAL for Bayesian Active Learning** We first implement our experiments on Bayesian neural
networks and perform Bayesian active learning on Fashion MNIST (LeCun et al., 1998), CIFAR10 (Krizhevsky et al., 2009), and CIFAR-100 (Krizhevsky et al., 2009). For fair comparison, we keep
the same experiment settings of Pinsler et al. (2019), using a Bayesian neural network consisting of
a ResNet-18 (He et al., 2016) feature extractor. The posterior inference is obtained by variational
inference (Wainwright & Jordan, 2008; Blundell et al., 2015) at the last layer, and model predictive
posteriors p(˜yj **_xj,_** _l) are estimated using 100 samples. Equation 13 is used to solve the finite-_
dimensional optimization problem, because sampling from the posterior distribution in a Bayesian | _D_
neural network will be efficient by leveraging the local reparameterization trick (Kingma et al.,
2015). Besides Random, we mainly focus on comparing with state-of-the-art approaches specifically
designed for Bayesian active learning: BALD and Bayesian Coreset.

We then evaluate SABAL’s performace. On Fashion MNIST dataset, we use 100 samples for random
projections, 1000 seed data, and query 1000 samples for 9 iterations. On CIFAR-10 and CIFAR-100,
two more complicated datasets, we use 2000 samples for random projections, 3000 (10000 for CIFAR100) seed data, and query 5000 samples for 4 iterations. Because Bayesian Coreset usually finds a
much smaller batch than requested, for a fair comparison, we let Bayesian Coreset acquire more data
than the batch size, and stop the acquisition as long as it has selected a full batch of data. It can be
seen in Table 1 and Figure 1 that both SABAL-IHT and SABAL-Greedy show some advantages on
Fashion MNIST dataset. On CIFAR-10 and CIFAR-100, we find SABAL-Greedy performs better
than SABAL-IHT while outperforming other baselines, including the Bayesian Coreset, one of the
current state-of-the-art approach in the literature under Bayesian settings.

Table 1: AUC Score (± std.) for different AL methods on Bayesian active learning. AUC measures
the overall performance improvement across number of queries. Results show the proposed SABAL-*
matches or outperforms all baselines.

**Dataset** **Bayesian Coreset** **SABAL-Greedy** **SABAL-IHT** **BALD** **Random**

Fashion MNIST 89.53 ± 0.25 **89.83 ± 0.26** **89.97 ± 0.23** 89.72 ± 0.23 88.39 ± 0.32
CIFAR10 **77.73 ± 0.70** **78.28 ± 0.53** 77.55 ± 0.83 77.61 ± 0.59 76.36 ± 0.59
CIFAR100 **42.40 ± 0.29** **42.53 ± 0.34** 42.30 ± 0.29 41.99 ± 0.60 42.10 ± 0.24

Figure 1: Active learning results on Bayesian models. Solid lines and shaded areas represent means
and standard deviations of test accuracy over different seeds. Our method especially SABAL-Greedy
outperforms most baselines and the SOTA method Bayesian Coreset.

**SABAL for General Active Learning** We then implement our experiments on general convolutional neural networks, including LeNet-5 (LeCun et al., 2015) and VGG-16 (Simonyan & Zisserman,
2014) architectures without any Bayesian layers, using MNIST (LeCun et al., 1998), SVHN (Netzer
et al., 2011), and CIFAR-10 (Krizhevsky et al., 2009) datasets. We utilize calibrated prediction
of current model with temperature scaling (Guo et al., 2017) to approximate the label distribution
_p(˜yj_ **_xj,_** _l). Because the gradient of the last layer represents the full-model gradient (Ash et al.,_

2019 |), we can easily solve the optimization problem with gradient embedding as equation D 14. We
compare with popular non-Bayesian baselines: Random, Entropy, KCenter, and BADGE.

To evaluate the performance of SABAL, on MNIST dataset with LeNet-5 model, we use 40 seed
data, and query 40 samples for 15 iterations. On SVHN and CIFAR-10 with VGG-16 model, which
contains more complicated real-world color images, we use 1000 (3000 for CIFAR-10) seed data,


-----

and query 1000 (3000 for CIFAR-10) samples for 5 iterations. The results are shown in Table 2 and
Figure 2. In general, SABAL-Greedy also performs better than SABAL-IHT and outperform most
baselines, while achieving comparable performance to the strong baseline BADGE, the current SOTA
non-Bayesian method in literature, but SABAL requires much less acquisition time than BADGE
especially on large models. In addition, most methods perform similarly on CIFAR-10, and we
conjecture that for CIFAR-10 each sample is informative enough and thus random selection can
achieve good enough performance.

Table 2: AUC Score (± std.) for different AL methods on general active learning. AUC measures the
overall performance improvement across number of queries. Results show the proposed SABAL-*
matches or outperforms all baselines.

**Dataset** **BADGE** **SABAL-Greedy** **SABAL-IHT** **KCenter** **Entropy** **Random**

MNIST **91.24 ± 0.48** 90.89 ± 0.38 **91.07 ± 0.45** 89.57 ± 1.02 90.68 ± 0.81 86.48 ± 1.11
SVHN 86.92 ± 0.71 **87.23 ± 0.47** 86.84 ± 0.57 **87.04 ± 0.80** 86.28 ± 1.05 85.52 ± 0.51
CIFAR10 **68.20 ± 0.56** **68.01 ± 0.66** 67.80 ± 0.69 67.98 ± 0.63 67.94 ± 0.64 67.05 ± 0.59

Figure 2: Active learning results on general (non-Bayesian) models. Solid lines and shaded areas
represent means and standard deviations of test accuracy over different seeds. Our method performs
comparable or better than baselines and the SOTA method BADGE.

**Run Time Comparison** Our experiments show that SABAL can achieve comparable performance
of SOTA methods while requiring much less acquisition time. We compare the empirical results of
runtime complexity of SABAL with other baselines in non-Bayesian active learning experiment as an
example. Here, we consider the acquisition time of the first query, where the unlabeled data pool has
the largest size compared with later queries. Large models and datasets (SVHN and CIFAR-10 on
VGG-16) are used to better illustrate the runtime complexity. Results are shown in Table 3. It can be
seen that SABAL requires much less runtime than BADGE especially when queried batch is large,
and even less than KCenter in most cases.

Table 3: first query’s acquisition time (± std.) of different AL methods on two large datasets.
**SABAL-* shows big runtime advantage.**

**Dataset** **Method** **Time (unit:s)** **Dataset** **Method** **Time (unit:s)**

SVHN BADGE 732.18 ± 26.29 CIFAR10 BADGE 1207.19 ± 121.09
SABAL-Greedy 201.65 ± 3.54 SABAL-Greedy 333.67 ± 3.53
SABAL-IHT 211.04 ± 10.65 SABAL-IHT 174.28 ± 3.27
KCenter 309.99 ± 0.81 KCenter 258.29 ± 1.75
Entropy 16.46 ± 0.29 Entropy 11.76 ± 0.03
Random 0.81 ± 0.02 Random 1.42 ± 0.02

7 CONCLUSION

We introduce the SABAL as a novel framework that formulates batch active learning as a sparse
approximation problem. It balances representation and uncertainty in a principled way, and has the
flexibility to adapt to both Bayesian and non-Bayesian models. We realize the SABAL framework as
a finite-dimensional optimization problem, efficiently solvable by the proposed greedy or proximal
IHT algorithms. Numerical experiments demonstrate the strong performance of SABAL, comparable
to the state-of-the-art with lower time complexity. For the future works, although the hyperparameter
_α offers a controllable trade-off between the variance and bias, it is still not well-understood how to_
strike the best balance. An in-depth theoretical analysis of the SABAL optimizations, as well as other
instantiations of our general framework, would also have the potential to inspire discoveries of even
better batch active learning algorithms.


-----

8 ETHICS STATEMENT

This work is proposing an active learning approach for more efficient data acquisition and model
training, we do not expect any obvious ethical issue from this work.

9 REPRODUCIBILITY STATEMENT

We have included the code and instructions to reproduce our work in the supplementary material. We
also provide the experiment settings, training details and hyperparameters in the Appendix.

REFERENCES

Michal Aharon, Michael Elad, and Alfred Bruckstein. K-svd: An algorithm for designing overcomplete dictionaries for sparse representation. IEEE Transactions on signal processing, 54(11):
4311–4322, 2006.

Jordan T Ash, Chicheng Zhang, Akshay Krishnamurthy, John Langford, and Alekh Agarwal. Deep
batch active learning by diverse, uncertain gradient lower bounds. arXiv preprint arXiv:1906.03671,
2019.

William H Beluch, Tim Genewein, Andreas Nurnberger, and Jan M K¨ ohler. The power of ensembles¨
for active learning in image classification. In Proceedings of the IEEE Conference on Computer
_Vision and Pattern Recognition, pp. 9368–9377, 2018._

Erdem Bıyık, Kenneth Wang, Nima Anari, and Dorsa Sadigh. Batch active learning using determinantal point processes. arXiv preprint arXiv:1906.07975, 2019.

Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural network. In International Conference on Machine Learning, pp. 1613–1622. PMLR, 2015.

Trevor Campbell and Tamara Broderick. Automated scalable bayesian inference via hilbert coresets.
_The Journal of Machine Learning Research, 20(1):551–588, 2019._

Yuxin Chen and Andreas Krause. Near-optimal batch mode active learning and adaptive submodular
optimization. In International Conference on Machine Learning, pp. 160–168. PMLR, 2013.

Sanjoy Dasgupta. Two faces of active learning. Theoretical computer science, 412(19):1767–1781,
2011.

David L Donoho. Compressed sensing. IEEE Transactions on information theory, 52(4):1289–1306,
2006.

Melanie Ducoffe and Frederic Precioso. Adversarial active learning for deep networks: a margin
based approach. arXiv preprint arXiv:1802.09841, 2018.

Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Insights and applications.
In Deep Learning Workshop, ICML, volume 1, pp. 2, 2015.

Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In international conference on machine learning, pp. 1050–1059.
PMLR, 2016.

Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep bayesian active learning with image data.
_arXiv preprint arXiv:1703.02910, 2017._

Yonatan Geifman and Ran El-Yaniv. Deep active learning over the long tail. _arXiv preprint_
_arXiv:1711.00941, 2017._

Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. In International Conference on Machine Learning, pp. 1321–1330. PMLR, 2017.

Steve Hanneke et al. Theory of disagreement-based active learning. Foundations and Trends® in
_Machine Learning, 7(2-3):131–309, 2014._


-----

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770–778, 2016.

Patrick Hemmer, Niklas Kuhl, and Jakob Sch¨ offer. Deal: Deep evidential active learning for image¨
classification. arXiv preprint arXiv:2007.11344, 2020.

Steven CH Hoi, Rong Jin, Jianke Zhu, and Michael R Lyu. Batch mode active learning and its
application to medical image classification. In Proceedings of the 23rd international conference
_on Machine learning, pp. 417–424, 2006._

Neil Houlsby, Ferenc Huszar, Zoubin Ghahramani, and M´ at´ e Lengyel. Bayesian active learning for´
classification and preference learning. arXiv preprint arXiv:1112.5745, 2011.

Jonathan H Huggins, Trevor Campbell, and Tamara Broderick. Coresets for scalable bayesian logistic
regression. arXiv preprint arXiv:1605.06423, 2016.

Angelos Katharopoulos and Franc¸ois Fleuret. Not all samples are created equal: Deep learning with
importance sampling. In International conference on machine learning, pp. 2525–2534. PMLR,
2018.

Rajiv Khanna and Anastasios Kyrillidis. Iht dies hard: Provable accelerated iterative hard thresholding.
In International Conference on Artificial Intelligence and Statistics, pp. 188–198. PMLR, 2018.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
_arXiv:1412.6980, 2014._

Diederik P Kingma, Tim Salimans, and Max Welling. Variational dropout and the local reparameterization trick. arXiv preprint arXiv:1506.02557, 2015.

Andreas Kirsch, Joost van Amersfoort, and Yarin Gal. Batchbald: Efficient and diverse batch
acquisition for deep bayesian active learning. In Advances in Neural Information Processing
_Systems, pp. 7026–7037, 2019._

Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.

Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to´
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

Yann LeCun et al. Lenet-5, convolutional neural networks. URL: http://yann. lecun. com/exdb/lenet,
20(5):14, 2015.

Xin Li and Yuhong Guo. Adaptive active learning for image classification. In Proceedings of the
_IEEE Conference on Computer Vision and Pattern Recognition, pp. 859–866, 2013._

George L Nemhauser, Laurence A Wolsey, and Marshall L Fisher. An analysis of approximations for
maximizing submodular set functions—i. Mathematical programming, 14(1):265–294, 1978.

Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. 2011.

Robert Pinsler, Jonathan Gordon, Eric Nalisnick, and Jose Miguel Hern´ andez-Lobato. Bayesian batch´
active learning as sparse subset approximation. In Advances in Neural Information Processing
_Systems, pp. 6359–6370, 2019._

Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set
approach. arXiv preprint arXiv:1708.00489, 2017.

Burr Settles. Active learning literature survey. Technical report, University of Wisconsin-Madison
Department of Computer Sciences, 2009.

Maohao Shen, Jacky Y. Zhang, Leihao Chen, Weiman Yan, Neel Jani, Brad Sutton, and Oluwasanmi
Koyejo. Labeling cost sensitive batch active learning for brain tumor segmentation. In 2021
_IEEE 18th International Symposium on Biomedical Imaging (ISBI), pp. 1269–1273, 2021. doi:_
10.1109/ISBI48211.2021.9434098.


-----

Changjian Shui, Fan Zhou, Christian Gagne, and Boyu Wang. Deep active learning: Unified and´
principled method for query and training. In International Conference on Artificial Intelligence
_and Statistics, pp. 1308–1318. PMLR, 2020._

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.

Samarth Sinha, Sayna Ebrahimi, and Trevor Darrell. Variational adversarial active learning. In
_Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5972–5981, 2019._

Simon Tong and Daphne Koller. Support vector machine active learning with applications to text
classification. Journal of machine learning research, 2(Nov):45–66, 2001.

Martin J Wainwright and Michael Irwin Jordan. Graphical models, exponential families, and
_variational inference. Now Publishers Inc, 2008._

Dan Wang and Yi Shang. A new active labeling method for deep learning. In 2014 International
_joint conference on neural networks (IJCNN), pp. 112–119. IEEE, 2014._

Keze Wang, Dongyu Zhang, Ya Li, Ruimao Zhang, and Liang Lin. Cost-effective active learning for
deep image classification. IEEE Transactions on Circuits and Systems for Video Technology, 27
(12):2591–2600, 2016.

Yazhou Yang and Marco Loog. Single shot active learning using pseudo annotators. Pattern
_Recognition, 89:22–31, 2019._

Jacky Y. Zhang, Rajiv Khanna, Anastasios Kyrillidis, and Oluwasanmi Koyejo. Bayesian coresets:
Revisiting the nonconvex optimization perspective. In International Conference on Artificial
_Intelligence and Statistics, pp. 2782–2790. PMLR, 2021._


-----

# SABAL: Sparse Approximation-based Batch Active Learning
 Appendix

A THE OVERALL PROCEDURE

**Algorithm 3: SABAL: Sparse Approximation-based Batch Active Learning**
**Input: Intial parameters θ, initial unlabeled pool Du, initial labeled pool Dl = ∅, initial number**
of samples b0, query batch size b, number of iterations T .

**12 Query a random batch Train the model using** _S00. of b0 data from Dl, update Du ←Du\S0 and Dl ←Dl ∪S0._

_S_

**3 for t = 1, 2, . . ., T do**

**4** For each data xj _u, estimate its label distribution ˜yj_ _P(xj)._

**5** Compute vector g ∈Dj by sampling or gradient embedding using ∼ 16.

**67** ComputeFind sparse weight v, Φ, and w σj s.t. for each ∥w∥0 x =j ∈D b as specified in sectionu and form equation 4 17. .

**1089** Select a batch of dataUpdateReinitialize and retrain the model using updated Du ←Du\St S andt = D {lx ←Dj ∈Dl ∪Su | wt. _j > 0} and query their labels.l, update model parameters θ_
_D_

**11 end**

**Return: Final model parameters θ.**


B MORE RELATED WORK

Active learning has been widely studied by the machine learning community. As most classic
approaches have already been discussed in a detail in Settles (2009); Dasgupta (2011); Hanneke et al.
(2014), we will briefly review some recent works in deep active learning.

Existing query strategies can mainly be categorized as uncertainty-based and representation-based.
Uncertainty-based approaches look for data samples the model is mostly uncertain about. Meanwhile,
under the Bayesian setting, several recent works leverage the Bayesian neural network to well measure
the model uncertainty. Gal & Ghahramani (2015; 2016) proves the Monte-Carlo dropout (MC
Dropout) as an approximation of performing Bayesian inference, and enables efficient uncertainty
estimations in neural networks. Gal et al. (2017) utilizes MC Dropout for approximating posterior
distributions and adapts Houlsby et al. (2011) as their uncertainty based acquisition function, and
similarly, Kirsch et al. (2019) proposes a batch-mode approach based on Gal et al. (2017) and shows
some improvements through a more accurate measurement of mutual information between the data
batch and model parameters. While MC Dropout becomes prevalent for uncertainty estimation,
Beluch et al. (2018) shows ensemble-based methods lead to better performance because of more
calibrated uncertainty estimation, and another recent work Hemmer et al. (2020) also proposes a new
uncertainty estimation method by replacing the softmax output of a neural network with the parameter
of Dirichlet density. Other non-Bayesian approaches sometimes combine uncertainty estimation with
other metrics: Li & Guo (2013) combines an information density measure to maximize the mutual
information between selected samples and remaining unlabeled samples under the Gaussian Process
setting. Wang et al. (2016) selects data based on several classic uncertainty metrics and incorporate a
cost-efficient strategy by pseudo labeling the confident samples.

Representation-based approaches attempt to query diverse data samples that could best represent
the overall unlabeled dataset. A recent work proposed by Sener & Savarese (2017) defines the
active learning as a core-set selection problem. They derive an upper bound for the core-set loss
and construct representative batches by solving a k-Center problem in the feature space. In Geifman
& El-Yaniv (2017), the authors also explore the deep active learning with core-sets, but build the
core-sets in the farthest-first compression scheme.


-----

C PROOFS

**Proposition C.1. q : L(Θ, R) →** R+ defined in (5) is a shift-invariant seminorm satisfying the
_following properties:_

_1. q(L1 + L2) ≤_ _q(L1) + q(L2) for ∀L1, L2 ∈L(Θ, R);_ _(triangle inequality)_

_2. q(cL) = |c|q(L) for ∀L ∈L(Θ, R), ∀c ∈_ R; _(absolute homogeneity)_

_3. q(L + c) = q(L) for ∀L ∈L(Θ, R), ∀c ∈_ R; _(shift-invariance)_

_4. q(L) = 0 if and only if L maps every θ ∈_ Θ to a constant.

_In other words, q defines a norm in the space of shift-equivalence classes of loss functions._

_Proof. Recall that_

_q(L) := inf_ _L_ (Θ, R).
_c∈R_ _[∥][L][ +][ c][∥][†][,]_ _∀_ _∈L_

We prove the four properties respectively in the following.

1. The triangle inequality is inherited from the sub-additivity of the norm . For _L_
_∥· ∥†_ _∀_ _∈_
_L(Θ, R), we have_

_q(L1 + L2) = inf_ inf
_c∈R_ _[∥][L][1][ +][ L][2][ +][ c][∥][†][ =]_ _c1,c2∈R_ _[∥][L][1][ +][ L][2][ +][ c][1][ +][ c][2][∥][†]_

inf
_≤_ _c1,c2∈R_ _[∥][L][1][ +][ c][1][∥][†][ +][ ∥][L][2][ +][ c][2][∥][†]_

= (inf
_c∈R_ _[∥][L][1][ +][ c][∥][†][) + (inf]c∈R_ _[∥][L][2][ +][ c][∥][†][)]_

= q(L1) + q(L2).

2. The absolute homogeneity is also inherited from the absolute homogeneity of the norm
. The case for c = 0 is obvious, and for c = 0 we have
_∥· ∥†_ _̸_

_q(cL) =_ _c_ _q(L) = inf_
_|_ _|_ _c1∈R_ _[∥][cL][ +][ c][1][∥][†][ = inf]c1∈R_ _[|][c][| · ∥][L][ +][ c][1][/c][∥][†]_

= inf
_c2∈R_ _[|][c][| · ∥][L][ +][ c][2][∥][†][ =][ |][c][|][q][(][L][)][.]_

3. By the definition of q(·), we have the shift-invariance of q(·).

4. The “if” part can be proved by definition, i.e., q(c) = inf _c1∈R ∥c1 + c∥† = ∥0∥† = 0._

For the “only if” part, we need to be more rigorous by defining fc to be the function that
(maps, Θ to) is a one-dimensional normed space. Letting c ∈ R. We further define F := {fc | c L ∈ R} ⊂L(Θ, R)(Θ and, R q), and we can see(L) = 0, we have
_F_ _∥· ∥†_ _∈L_

inf
_fcϵ_ _∈F_ _[∥][L][ +][ f][c][∥][†][ = 0][.]_

Therefore, for ∀ϵ > 0, ∃cϵ ∈ R such that

_∥L + fcϵ_ _∥† ≤_ _ϵ_
= _fcϵ_ = _L + fcϵ_ _L_ _ϵ +_ _L_ _._
_⇒∥_ _∥†_ _∥_ _−_ _∥† ≤_ _∥_ _∥†_

That being said, for∥Riesz’s lemma we havefc∥† ≤ 1 + ∥L∥†}, and we can see 0 < ϵ < B compact. 1, we have B is a closed ball in ∥fcϵ _∥† ≤_ 1 + F ∥L. As∥†. Denote F is one-dimensional, by B = {fc ∈F |

As limϵ 0 _L + fcϵ_ = 0, i.e., fcϵ _L, by the compactness of_ we have L .
Therefore,→ _L ∥ is also a constant function. Note that this conclusion does not require∥†_ _→_ _B_ _L(Θ ∈B, R)_
to be complete.


-----

**Proposition C.2 (Proposition 3.1 Restated). As w** R[n]+[u] _[and][ ∥][w][∥][0]_ [=][ b][, by replacing the][ P][ by the]
_∈_
_improved estimation distribution Pw (equation 9) into (i) in equation 7, we have_


EPw [q(L[˜] − EPw [L[˜]])] + EPw [q(L[˜]w − EPw [L[˜]w])] ≤


**1(wj = 0) · σj,**
**_xjX∈Du_**


_where σj :=_ _n1u_ [E][P][(][x][j] [)][[][q][(][ℓ][(][x][j][,][ ˜]yj; ·) − EP(xj )[ℓ(xj, ˜yj; ·)])] is the individual variance, and 1(·)

_is the indicator function._

_Proof. Recall that_


_L˜w(θ) := [1]_


_wjℓ(xj, ˜yj; θ),_ _L˜(θ) := [1]_

_nu_

**_xjX∈Du_**


_ℓ(xj, ˜yj; θ),_
**_xjX∈Du_**


_P(xj)_ if wj = 0
**_w_** R[n]+[u][,]
_δyj[⋆]_ if wj > 0 _[,]_ _∈_


**_y˜j_** _Pw(xj) :=_
_∼_


where δyj[⋆] [denotes the distribution that][ ˜]yj can only be yj[⋆][. Therefore, by the definition of][ P][w][, we]
have

EPw(xj ) _q_ _ℓ(xj, ˜yj; ·) −_ EPw(xj )[ℓ(xj, ˜yj; ·)] = 0, if wj > 0.
   

Plugging the above definitions into EPw [q(L[˜]w − EPw [L[˜]w])] we have


 _nu_

 [1]


EPw [q(L[˜]w − EPw [L[˜]w])] = EPw


_q_






_wj_ _ℓ(xj, ˜yj; ·) −_ EPw(xj )[ℓ(xj, ˜yj; ·)]
**_xjX∈Du_**  


 _nu_

 [1]


= EPw


_q_






**1(wj > 0)wj** _ℓ(xj, ˜yj; ·) −_ EPw(xj )[ℓ(xj, ˜yj; ·)]
**_xjX∈Du_**  


= 0.



 (18)


Therefore, we only need to care about the EPw [q(L[˜] − EPw [L[˜]])].

EPw [q(L[˜] EPw [L[˜]])] = EPw _q_ _ℓ(xj, ˜yj;_ ) EPw(xj )[ℓ(xj, ˜yj; )]
_−_   _nu_ _·_ _−_ _·_ 

**_xjX∈Du_**

 1 [1] 

EPw(xj ) _q_ _ℓ(xj, ˜yj;_ ) EPw(xj )[ℓ(xj, ˜yj; )]

_≤_ _nu_ _·_ _−_ _·_

**_xjX∈Du_**    

= **1(wj = 0) [1]** EP _q_ _ℓ(xj, ˜yj;_ ) EP(xj )[ℓ(xj, ˜yj; )]

_nu_ _·_ _−_ _·_

**_xjX∈Du_**    

= **1(wj = 0) · σj,** (19)

**_xjX∈Du_**


where the inequality is by the triangle inequality and the absolute homogeneity of q(·) (Proposition C.1). Combining equation 18 and equation 19, we have the proposition proved.

D OMITTED ALGORITHMS

In this section we present the two sub-procedures, i.e., line search and de-bias, shared by two main
optimization algorithms (Algorithm 1&2), as well as how the optimization (line 6) in Algorithm 2 is
solved optimally.

The line search sub-procedure (Algorithm 4) optimally solve the problem of arg minµ∈R f1(w−µu),
_i.e., given a direction u what is the best step size to move the w along u. The de-bias sub-procedure_
(Algorithm 5) adjusts a sparse w in its own sparse support for a better solution.


-----

**Algorithm 5: De-bias(w)**
**Input: starting point w.**
**Output: improved w.**

**1 u** [ _f1(w)]supp(w)_ _(in-support grad.)_
_←_ _∇_

**2 µ ←** LineSearch(u, w)

**3 w ←** **_w −_** _µu_ _(in-support adjustment)_

**Return: w**


**Algorithm 4: LineSearch(u, w)**
**Input: direction u; starting point w.**
**Output: step size µ.**

**1 µ ←** _[⟨][Φ][w][−]∥Φ[v][,]u[Φ]∥[u]2[2][⟩][+][+][β][β][∥][⟨][u][w][∥][−][2]2_ **[1][,][u][⟩]** _(optimal µ)_

**Return: µ**


Recall the inner optimization (line 6) of Algorithm 2 is

**_w ←_** **_w∈Rarg min[nu]+_** _[,][∥][w][∥][0][≤][b]_ 12 _[∥][w][ −]_ **_[s][∥]2[2]_** [+][ f][2][(][w][)][.]

Noting that [1]2 _[∥][w][ −]_ **_[s][∥]2[2]_** [+] _[f][2][(][w][) =][ P]j∈[nu][(][ 1]2_ [(][w][j][ −] _[s][j][)][2][ −]_ _[ασ]j[2][)][, this step can be done optimally by]_

simply picking the top-b elements, as shown in the following. Given a b-sparse support set [nu],
_S ⊂_
we can see that

**_w∈R[nu]+_** min[,][supp][(][w][)][⊆S] _j∈[nu][(][ 1]2_ [(][w][j][ −] _[s][j][)][2][ −]_ _[ασ]j[2][) =][ P]j∈S_ [(][ 1]2 [[][−][s][j][]]+[2] _[−]_ _[ασ]j[2][)][.]_
P

Therefore, line 6 in Algorithmthe resulting b-sparse index set as 2 can be done by: (1) find the S _[⋆]; (2) let w ←_ [[s]S _⋆_ ]+. _b smallest (_ [1]2 [[][−][s][j][]]+[2] _[−]_ _[ασ]j[2][)][, denoting]_


E MORE EXPERIMENT RESULTS

E.1 ABLATION STUDY: TRADE-OFF OF UNCERTAINTY AND REPRESENTATION

We perform an ablation study to understand better the trade-off
between the variance and the bias terms in our final formulation equation 15. To remove the bias term, we query the data with
top variances. To remove the variance term, we query the data
by only minimizing the approximation bias, i.e., setting α = 0,
under both IHT and Greedy optimizations respectively. We take
two datasets MNIST and CIFAR-10 in the Bayesian experiment as
examples. Results in Figure 3 demonstrate the necessity of taking
both uncertainty and representation into consideration during the
data acquisitions for ideal performance, while for some datasets
like CIFAR-10, the variance contributes much more significantly.

F IMPLEMENTATION DETAILS


All experiments are written in PyTorch 1.8.1. All hyper-parameters
are chosen to ensure models achieve good and stable performance
on each dataset, and they are kept identical for all active learning
approaches.

F.1 BAYESIAN ACTIVE LEARNING EXPERIMENT


Figure 3: Ablation Study Results on Bayesian models.


**Model Architecture** We use the exact same model as (Pinsler
et al., 2019), it is a Bayesian neural network consisting of a ResNet-18 (He et al., 2016) feature
extractor followed by a fully connected layer with a ReLU activation, and a final layer allows sampling
by local reparametrization (Kingma et al., 2015) with a softmax activation.

**Optimization and Hyperparameter Selection** Due to larger models and more complicated classification tasks, e.g., CIFAR-100, data augmentation(including random cropping and random horizontal
flipping) and learning rate scheduler are used in this experiment to achieve good model performance.
The model is optimized with the Adam (Kingma & Ba, 2014) optimizer using default exponential
decay rates (0.9, 0.999) for the moment estimates. Table 4 shows the hyper-parameters in experiment


-----

on Bayesian batch active learning, where bs denotes the batch size in dataloader during the model
training, lr denotes the learning rate, and wd denotes the weight decay. The hyper-parameters are
chosen through grid search.

Table 4: Hyperparameters used in Bayesian active learning experiment

**Dataset** **Method** **Epoch** **_bs_** **_α_** **_β_** **_lr_** **_wd_**

Fashion MNIST SABAL-IHT 200 256 1 10[−][3] 0.001 5 × 10[−][4]

Fashion MNIST SABAL-Greedy 200 256 2 0.5 0.001 5 × 10[−][4]

CIFAR-10 SABAL-IHT 200 256 1 10[−][6] 0.001 5 × 10[−][4]

CIFAR-10 SABAL-Greedy 200 256 2 1 0.001 5 × 10[−][4]

CIFAR-100 SABAL-IHT 200 256 1 10[−][6] 0.001 5 × 10[−][4]

CIFAR-100 SABAL-Greedy 200 256 1 0.5 0.001 5 × 10[−][4]


F.2 GENERAL ACTIVE LEARNING EXPERIMENT

**Model Architecture** On MNIST dataset, we use LeNet-5 model (LeCun et al., 2015). On SVHN
and CIFAR10 datasets, we use VGG-16 model (Simonyan & Zisserman, 2014).

**Optimization and Hyperparameter Selection** All models are trained using the cross entropy loss
with SGD optimizer, and no data augmentation or learning rate scheduler is used. Tabel 5 shows the
hyper-parameters in experiment on general batch active learning, where bs denotes the batch size in
dataloader during the model training, lr denotes the learning rate, m denotes the momentum, and wd
denotes the weight decay. The hyper-parameters are chosen through grid search.

Table 5: Hyperparameters used in general active learning experiment

**Dataset** **Method** **Epoch** **_bs_** **_α_** **_β_** **_lr_** **_wd_**

MNIST SABAL-IHT 150 32 10[−][8] 10[−][4] 0.01 0.9 5 × 10[−][4]

MNIST SABAL-Greedy 150 32 10[−][8] 10[−][1] 0.01 0.9 5 × 10[−][4]

SVHN SABAL-IHT 150 128 10[−][8] 10[−][4] 0.01 0.9 5 × 10[−][4]

SVHN SABAL-Greedy 150 128 10[−][1] 10[−][1] 0.01 0.9 5 × 10[−][4]

CIFAR-10 SABAL-IHT 100 128 10[−][8] 10[−][6] 0.001 0.9 5 × 10[−][4]

CIFAR-10 SABAL-Greedy 100 128 10[−][2] 10[−][1] 0.001 0.9 5 × 10[−][4]


-----

