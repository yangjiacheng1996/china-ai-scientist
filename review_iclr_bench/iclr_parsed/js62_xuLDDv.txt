# IS FAIRNESS ONLY METRIC DEEP? EVALUATING AND ADDRESSING SUBGROUP GAPS IN DML

**Natalie Dullerud[1], Karsten Roth[2], Kimia Hamidieh[1], Nicolas Papernot[1], Marzyeh Ghassemi[3]**

1University of Toronto & Vector Institute, 2University of Tübingen, 3MIT

ABSTRACT

Deep metric learning (DML) enables learning with less supervision through its
emphasis on the similarity structure of representations. There has been much
work on improving generalization of DML in settings like zero-shot retrieval,
but little is known about its implications for fairness. In this paper, we are the
first to evaluate state-of-the-art DML methods trained on imbalanced data, and
to show the negative impact these representations have on minority subgroup
performance when used for downstream tasks. In this work, we first define fairness
in DML through an analysis of three properties of the representation space – interclass alignment, intra-class alignment, and uniformity – and propose finDML, the
**_f_** airness in non-balanced DML benchmark to characterize representation fairness.
Utilizing finDML, we find bias in DML representations to propagate to common
downstream classification tasks. Surprisingly, this bias is propagated even when
training data in the downstream task is re-balanced. To address this problem,
we present Partial Attribute De-correlation (PARADE) to de-correlate feature
representations from sensitive attributes and reduce performance gaps between
subgroups in both embedding space and downstream metrics.

1 INTRODUCTION

Deep metric learning (DML) extends standard metric learning to deep neural networks, where the
goal is to learn metric spaces such that embedded data sample distance is connected to actual semantic
similarities (Globerson & Roweis, 2006; Weinberger et al., 2006; Hoffer & Ailon, 2018; Wang et al.,
2014). The explicit optimization of similarity makes deep metric spaces well suited for usage in
unseen classes, such as zero-shot image or video retrieval or facial re-identification (Milbich et al.,
2021; Roth et al., 2020c; Musgrave et al., 2020; Hoffer & Ailon, 2018; Wang et al., 2014; Schroff
et al., 2015; Wu et al., 2018; Roth et al., 2020c; Brattoli et al., 2020; Hu et al., 2014; Deng et al.,
2019; Liu et al., 2017). However, while DML is effective in establishing notions of similarity, work
describing potential fairness issues is limited to individual fairness in standard metric learning (Ilvento,
2020), disregarding embedding models.

Indeed, the impacts and metrics of fairness are well studied in machine learning (ML) generally,
and representation learning specifically (Dwork et al., 2012; Mehrabi et al., 2019; Locatello
et al., 2019b). This is especially true on high-risk tasks such as facial recognition and judicial
decision-making (Chouldechova, 2017; Berk, 2017), where there are known risks to minoritized subgroups (Samadi et al., 2018). Yet, relatively little work has been done in the domain of DML (Rosenberg et al., 2021). It is crucial to address this knowledge gap – if DML embeddings are used to create
_upstream embeddings that facilitate downstream transfer tasks, biases may propagate unknowingly._

To tackle this issue, this work first proposes a benchmark to characterize f airness in non-balanced
**_DML - finDML. finDML introduces three subgroup fairness definitions based on feature space_**
performance metrics – recall@k, alignment and group uniformity. These metrics measure clustering
ability and generalization performance via feature space uniformity. Thus, we select the metrics
for our definitions to enforce independence between inclusion in a particular cluster or class, and
a protected attribute (given the ground-truth label). We leverage existing datasets with fairness
limitations (CelebA (Liu et al., 2015) and LFW (Huang et al., 2007)) and induce imbalance in training
data of standard DML benchmarks, CARS196 (Krause et al., 2013) and CUB200 (Wah et al., 2011),
in order to create an effective benchmark for fairness analysis in DML.


-----

Making use of finDML, we then perform an evaluation of 11 state-of-the-art (SOTA) DML methods
representing frequently used losses and sampling strategies, including: ranking-based losses (Wang
et al., 2014; Hoffer & Ailon, 2018), proxy-based (Kim et al., 2020) losses, semi-hard sampling (Schroff et al., 2015) and distance-weighted sampling (Wu et al., 2018). Our experiments
suggest that imbalanced data during upstream embedding impacts the fairness of all benchmarks
methods in both upstream embeddings (subgroup gaps up to 21%) as well as downstream classifications (subgroup gaps up to 45.9%). This imbalance is significant even when downstream
**classifiers are given access to balanced training data, indicating that data cannot naively be**
**used to de-bias downstream classifiers from imbalanced embeddings.**

Finally, inspired by prior work in DML on multi-feature learning (Milbich et al., 2020), we introduce
PARtial Attribute DE-correlation (PARADE). PARADE addresses imbalance by de-correlating two
learned embeddings: one learnt to represent similarity in class labels, and one learnt to represent
similarity in the values of a sensitive attribute, which is discarded at test-time. This creates a model in
which the ultimate target class embeddings have been de-correlated from the sensitive attributes of the
input. We note that as opposed to previous work on variational latent spaces, PARADE de-correlates
a learned similarity metric. We find that PARADE reduces gaps of SOTA DML methods by up to 2%
downstream in finDML.

In total, our contributions can be summarized as follows:

1. We define finDML; introducing three definitions of fairness in DML to capture multifaceted minoritized subgroup performance in upstream embeddings through focus on feature
representation characteristics across subgroups, and five datasets for benchmarking.

2. We analyze SOTA DML methods using finDML, and find that common DML approaches
are significantly impacted by imbalanced data. We show empirically that learned embedding
bias cannot be overcome by naive inclusion of balanced data in downstream classifiers.

3. We present PARADE, a novel adaptation of previous zero-shot generalization techniques
to enhance fairness guarantees through de-correlation of class discriminative features with
sensitive attributes.


2 BACKGROUND

**Deep Metric Learning** DML extends standard metric learning by fusing feature extraction and
learning a parametrized metric space into one end-to-end learnable setup. In this setting, a large
convolutional network ψ provides the mapping to a feature space Ψ, while a small network f, usually
a single linear layer, generates the final mapping to the metric or embedding space Φ. The overall
mapping from the image space X is thus given by ϕ = f ◦ _ψ. Generally, the embedding space is_
projected on the unit hypersphere S _[D][−][1]_ through normalization (Weisstein, 2002; Wu et al., 2018;
Roth et al., 2020c; Wang & Isola, 2020) to limit the volume of the representation space with increasing
embedding dimensionality. The embedding network ϕ is then trained to provide a metric space Φ
that operates well under some predefined, usually non-parametric metric such as the Euclidean or
cosine distance defined over Φ.

Typical objectives used to learn such metric spaces range from contrastive ranking-based training
using tuples of data, such as pairwise (Hadsell et al., 2006), triplet- (Schroff et al., 2015; Wu et al.,
2018) or higher-order tuple-based training (Sohn, 2016; Wang et al., 2020a), procedures to bring
down the effective complexity of the tuple space (Schroff et al., 2015; Harwood et al., 2017; Wu
et al., 2018) or the introduction of learnable tuple constituents (Movshovitz-Attias et al., 2017; Qian
et al., 2019; Kim et al., 2020).

More recent work (Milbich et al., 2020; Roth et al., 2020c; Jacob et al., 2019) extends standard DML
training through incorporation of objectives going beyond just sole class label discrimination: e.g.,
through the introduction of artificial samples (Lin et al., 2018; Duan et al., 2018), regularization of
higher-order moments (Jacob et al., 2019), curriculum learning (Zheng et al., 2019; Harwood et al.,
2017; Roth et al., 2020a), knowledge distillation (Roth et al., 2020b) or the inclusion of additional
features (DiVA) to produce diverse and de-correlated representations (Milbich et al., 2020).

**DML Evaluation** Standard performance measures reflect the goal of DML: namely, optimizing
an embedding space Φ for best transfer to new test classes via learning semantic similarities. As


-----

Figure 1: a) Visualization of the standard DML pipelines and the aspects of intra-class alignment
and uniformity in the embedding space. b) Infographic of the fairness issue in DML, where learned
representational bias can even transfer to downstream models building on previously learned representations. c) Layout of our proposed PARADE approach to better incorporate sensitive attribute
context and improve representational fairness.

immediate applications are commonly found in zero-shot clustering or image retrieval, respective
retrieval and clustering metrics are predominantly utilized for evaluation. Recall@k (Jegou et al.,
2011) or mean average precision measured on recall (Roth et al., 2020c; Musgrave et al., 2020)
typically estimate retrieval performance. Normalized mutual information (NMI) on clustered embeddings (Manning et al., 2010) is used as a proxy for clustering quality (see Supplemental for detailed
definitions). We leverage these performance metrics to inform finDML and our experiments.

**Fairness in Classification** Formalizing fairness in ML continues to be an open problem (Mehrabi
et al., 2019; Chen et al., 2018a; Chouldechova, 2017; Berk, 2017; Locatello et al., 2019b; Chouldechova & Roth, 2018; Dwork et al., 2012; Hardt et al., 2016; Zafar et al., 2017). In classification,
definitions for fairness such as demographic parity, equalized odds, and equality of opportunity, rely
on model outputs across the random variables of protected attribute and ground-truth label (Dwork
et al., 2012; Hardt et al., 2016).

**Fairness in Representations** A more relevant family of fairness definitions for DML would be
those explored in fairness for general representation learning (Edwards & Storkey, 2015; Beutel
et al., 2017; Louizos et al., 2015; Madras et al., 2018). Here, the goal is to learn a fair mapping
from an original domain to a latent domain so that classifiers trained on these representations are
more likely to be agnostic to the sensitive attribute in unknown downstream tasks. This assumption
distinguishes our setting from previous fairness work in which the downstream tasks are known at
train time (Madras et al., 2018; Edwards & Storkey, 2015; Moyer et al., 2018; Song et al., 2019;
Jaiswal et al., 2019). DML differs from this form of representation learning as it aims to learn a
mapping capturing semantic similarity, as opposed to latent space representation.

Earlier works in fair representation learning intended to obfuscate any information about sensitive
attributes to approximately satisfy demographic parity (Zemel et al., 2013) while a wealth of more
recent works focus on using adversarial methods or feature disentanglement in latent spaces of
VAEs (Locatello et al., 2019a; Kingma & Welling, 2013; Gretton et al., 2006; Louizos et al., 2015;
Amini et al., 2019; Alemi et al., 2018; Burgess et al., 2018; Chen et al., 2018b; Kim & Mnih, 2018;
Esmaeili et al., 2019; Song et al., 2019; Gitiaux & Rangwala, 2021; Rodríguez-Gálvez et al., 2020;
Sarhan et al., 2020; Paul & Burlina, 2021; Chakraborty et al., 2020). In this setting, the literature
has focused on optimizing on approximations of the mutual information between representations
and sensitive attributes: maximum mean discrepancy (Gretton et al., 2006) for deterministic or
variational (Li et al., 2014; Louizos et al., 2015) autoencoders (VAEs); cross-entropy of an adversarial
network that predicts sensitive attributes from the representations (Edwards & Storkey, 2015; Xie
et al., 2017; Beutel et al., 2017; Zhang et al., 2018; Madras et al., 2018; Adel et al., 2019; Zhao &


-----

Gordon, 2019; Xu et al., 2018); balanced error rate on both target loss and adversary loss (Zhao et al.,
2019); Weak-Conditional InfoNCE for conditional contrastive learning (Tsai et al., 2021).

PARADE shares aspects of these previous methods in its choice of de-correlation or disentanglement.
However, PARADE de-correlates the learned similarity metric as opposed to the latent space. In
addition, with DML-specific criteria, PARADE learns similarities over the sensitive attribute while
not directly removing all information about the sensitive attribute, as the sensitive attribute and target
class embeddings share a base network.

3 EXTENDING FAIRNESS TO DML - finDML BENCHMARK

To characterize fairness with finDML, this section introduces the key constituents – definitions to
characterize fairness in embedding spaces and respective benchmark datasets.

3.1 PRELIMINARIES

Our embedding space fairness definitions rely on embedding space metrics adapted from (Wang
& Isola, 2020) and (Roth et al., 2020c), namely alignment and uniformity. Both metrics we use to
characterize embeddings for our definitions in the next section (intra- as well as inter-class alignment
and uniformity) have been successfully linked to generalization performance in contrastive selfsupervised and metric learning models (Wang & Isola, 2020; Roth et al., 2020c; Sinha et al., 2020).
Alignment succinctly captures the similarity structure learned by the representation space with respect
to the target labels through measuring distances between pairs of samples. On the other hand, notions
of uniformity can differ. Uniformity of the sample distribution over the hypersphere has been studied
through the radial basis function (RBF) over pairs of samples. Alternatively, uniformity of the feature
_space has been studied through the KL-divergence DKL between the discrete uniform distribution UD_
and the sorted singular value distribution Sϕ(X) of the representation space ϕ on dataset X.

_UKL(X) =_ KL D, _ϕ(X)_ (1)
_D_ _U_ _S_

Here, lower scores indicate more significant directions of variance in learned representations. Both

  

introduced notions of uniformity represent important aspects of the embedding space, but the
computational overhead in computing RBF over all pairs of samples in large datasets makes it
impractical for our uses and is less interpretable than UKL. Therefore, we leave the uniformity metric
utilized in finDML general, but utilize UKL for our experiments.

3.2 DEFINING FAIRNESS

Building on the aforementioned performance metrics, we introduce three definitions for fairness in
the embedding spaces of DML models. As the recall@k and alignment metrics inform inclusion
in an embedded cluster (or class), we follow fair classification literature in the motivation for our
first fairness definition: inclusion in a class should be independent of a protected attribute given the
ground-truth label. Thus, we examine the probability of encountering a data instance of the same
class in a data point’s k-nearest neighbors to form the first definition. The second definition relies
on equal expectation of alignment across sensitive attribute values. Departing from classification
literature, our third definition encapsulates fairness in a task-agnostic sense (as DML is often applied
in such settings): fairness across the “goodness" of the learned features via a uniformity metric.

Let X denote the input data, and A a protected attribute variable. Denote Xa the partition of X
with attribute a ∈ _A. To recap common DML terminology, a positive pair of samples is defined as_
(x1, x2) ∈ _X × X s.t. the class label of x1 and x2 are identical. A negative pair of samples is defined_
as (x1, x2) ∈ _X × X such that the class label of x1 and x2 differ. Let Pa denote the set of all positive_
_pairs s.t. at least one of x1 or x2 has attribute a ∈_ _A, and analogously for Na and negative pairs._
**Definition 1 (K-Close Fairness). Define NNk : Φ ⊂S** _[D][−][1]_ _→P(X) as a function that receives a_
_point ϕ(x) ∈_ Φ and returns a set in the powerset of X, P(X), containing points in X that map to
_the k nearest neighbors of ϕ(x) in Φ. Thus, ϕ is k-close fair with respect to attribute A if:_

Pr _x_ _NNk(ϕ(x))s.t.Y (˜x) = Y (x)) = Pr_ _x_ _NNk(ϕ(x))s.t.Y (˜x) = Y (x))_ _a, b_ _A_
_x_ _Xa[(][∃][˜] ∈_ _x_ _Xb[(][∃][˜] ∈_ _∀_ _∈_
_∈_ _∈_

(2)
_Note: the criteria weakens as k increases, similar to recall@k._


-----

**Definition 2 (Alignment). ϕ is fair, according to alignment with respect to attribute A, if:**
E(x1,x2)∈Pa [||ϕ(x1) − _ϕ(x2)||[2]] = E(x1,x2)∈Pb_ [||ϕ(x1) − _ϕ(x2)||[2]]_ (3)

E(x1,x2)∈Na [||ϕ(x1) − _ϕ(x2)||[2]] = E(x1,x2)∈Nb_ [||ϕ(x1) − _ϕ(x2)||[2]]_ _∀a, b ∈_ _A_ (4)
_i.e. the expectation of the alignment is equal across domain of A._

**Definition 3 (Uniformity Across Groups). ϕ is fair, according to uniformity, and with respect to**
_attribute A, if the expectation of the uniformity is equal across domain of A:_
_U_ (ϕ(Xa)) = U (ϕ(Xb)) _a, b_ _A_ (5)
_∀_ _∈_
_where U_ (·) denotes some measure of uniformity over a set V ∈S _[D][−][1]._

3.3 CONSTRUCTED finDML BENCHMARK DATASETS

_finDML encompasses existing DML benchmark datasets, CUB200 and CARS196, and facial recogni-_
tion datasets, CelebA and LFW (Wah et al., 2011; Krause et al., 2013; Liu et al., 2015; Huang et al.,
2007). For fairness analysis, we investigate bird color in CUB200[1], Race in LFW and Skintone in
CelebA (Kumar et al., 2009). A detailed description of dataset and attribute labeling is included in
the Supplemental. To create additional fairness benchmarks, we induce class imbalance in CUB200
and CARS196, as both datasets are naturally balanced w.r.t. class.

**Manually Introduced Class Imbalance We introduce imbalance by reducing the number of training**
data samples of 50 randomly selected classes by 90% (Imbalanced). We run an experiment with the
original datasets as a balanced control (Balanced) for comparison. In the imbalanced setting, we
adjust (increase) the number of training samples of the majoritized groups to match the number of
datapoints in the balanced control experiments. We average metrics over 10 sets of 50 randomly
selected classes for imbalanced experiments. We use the standard ratio of 50 − 50 for train-test
split of these datasets, but split over number of data points per class, as opposed to splitting over
the classes themselves. The manually imbalanced datasets are used to benchmark standard DML
methods, validate our framework, and analyze downstream effects.

Although dataset imbalance does not constitute the sole source of bias in machine learning applications, unfairness as a result of imbalance is the most well-understood in the literature (Chen et al.,
2018a). Additionally, we do not assume for our naturally imbalanced datasets, particularly the facial
datasets, that attribute imbalance is the only source of bias we observe.

4 PARTIAL ATTRIBUTE DE-CORRELATION (PARADE)

In this section, we present Partial Attribute De-correlation, or PARADE, in which we incorporate
adversarial separation (Milbich et al., 2020) during training to de-correlate separate embeddings.
We enumerate several significant changes: 1) only target embedding released at test-time; 2) triplet
formation and loss term w.r.t. sensitive attribute; 3) de-correlation with sensitive attribute as opposed
to de-correlation to reduce redundancy in concatenated feature space. These two representations
branch off from the deep metric embedding model at the last layer. The two representations encode
the similarity metrics learned over the sensitive attribute and target class, respectively. The sensitive
attribute embedding layer is discarded at test time. The resulting network expresses a similarity
metric with respect to the target class, de-correlated from the sensitive attribute (Figure 1). Therefore,
PARADE figuratively optimizes the first two fairness definitions proposed in Section 3.2 via an
objective that maximizes independence between the sensitive attribute and target class.

**Objective Term Per Embedding** To achieve efficient training and de-correlation of the target class
and the sensitive attribute embedding layers, we simultaneously train both layers that branch from
the penultimate layer of the model and de-correlate at each iteration. Because PARADE must learn
one embedding w.r.t. target class (ϕtarg) and one embedding w.r.t. the sensitive attribute (ϕSA), we
introduce separate objectives for each embedding:


_targ = [1]_
_L_ _N_


(t) _SA = [1]_
_L_ _L_ _N_
_t∼TXtarg_


_L(t)_
_t∼TXSA_


1While bird color in CUB200 does not represent a real-world fairness setting, CUB200 is widely used as a
DML benchmark. Thus, a fairness angle allows fairness analysis of previous methods benchmarked on CUB200.


-----

Sensitive Target class

|Col1|Col2|Col3|
|---|---|---|


brown
white
grey
black
blue
red
green
olive
yellow
orange
iridescent
buff


Figure 2: A t-SNE (Maaten & Hinton, 2008) visualization of the two distinct PARADE embeddings
for bird color CUB200 experiments: the sensitive attribute embedding (left) and the class label
embedding (right). In the sensitive attribute embedding, both example images are mapped to clusters
with birds of the same plumage (yellow and blue, respectively). Due to de-correlation, in the class
label embedding, the images are separated from the region of space with other birds of the same
plumage, but are still well-clustered, indicating that PARADE can find other attributes to distinguish
these species clusters.

where N is the number of training triplet samples, and L represents a generic loss function, such as
triplet loss (Hoffer & Ailon, 2018). We use t ∼Ttarg to illustrate sampling over triplets of the form
(xa, xp, xn) where xa and xp are of the same target class and xa and xn are of differing target classes.
Similarly, t ∼TSA indicates sampling over triplets of the form (xa, xp, xn) where xa and xp are of
the same sensitive attribute subgroup and xa and xn are of differing sensitive attribute subgroups.
See Figure 2 for a t-SNE visualization of the distinct embeddings of PARADE.

**Partial De-correlation** In order to minimize the correlation between ϕ[targ] and ϕ[SA], we use the
adversarial separation (de-correlation) method from (Milbich et al., 2020), which minimizes the
mutual information between a pair of embeddings. The task of mutual information minimization
is accomplished through learning an MLP to maximize the pair’s correlation, c, and consequently
performing a gradient reversal R, which inverts the gradients during backpropagation. The MLP, ξ,
is trained to maximize c(ϕ[targ]i _, ϕ[SA]i_ ) = _R(ϕ[targ]i_ ) _ξ(R(ϕ[SA]i_ )) 2[, s.t.][ J][ denotes element-wise]
_∥_ _∥[2]_
multiplication. Combining the loss terms results in total loss:

[J]

_LP ARADE = Ltarg + αSALSA −_ _ρ · c(ϕ[targ], ϕ[SA])_

where αSA weights the sensitive attribute loss and ρ weights the degree of de-correlation. ρ modulates
the de-correlation term to allow ψ to retain some attribute information (i.e. partial de-correlation).
Thus, the deployed modelfeature representations, as ϕ αtargSA =LSA ftarg appears in the loss function back-propagated through the full ◦ _ψ can retain information about the sensitive attribute in its_
model ψ. The extent to which the sensitive attribute affects the output features is controlled by αSA;
we suggest optimizing αSA (0, 1) and ρ through maximization of worst-group performance (Lahoti
et al., 2020) (See Supplemental C.5 for further analysis of PARADE hyperparameters). ∈

5 EXPERIMENTS

**Baseline DML Methods For all datasets, we use a ResNet-50 (He et al., 2016) architecture with best**
performing parameters on a validation set (for further implementation details, see Supplemental).
To investigate a sweeping set of frequently used DML methods, we benchmark across a diverse,
representative set of 11 techniques, including: three standard ranking-based losses (margin, triplet,
n-pair, and contrastive) three batch mining strategies (random, semi-hard and distance-weighted
sampling) and three common loss functions (multisimilarity loss, ArcFace loss for handling facial
datasets, and proxy-based loss, ProxyNCA) (Hoffer & Ailon, 2018; Hadsell et al., 2006; Wu et al.,
2018; Sohn, 2016; Hadsell et al., 2006; Kim et al., 2020; Wang et al., 2020a; Deng et al., 2019; Wu
et al., 2018; Schroff et al., 2015). See Supplementary for more details.


-----

**Fairness Evaluation In the embedding space, we analyze fairness via performance gaps between**
minoritized groups and majoritized groups, or worst-group performance gaps (Lahoti et al., 2020).
For fairness of the feature representations, we compute gaps in three metrics: recall@k and NMI
for intra- and inter-class distance (Section 3.2), and the uniformity measure UKL corresponding to
Definition 3 (defined in Section 3.1).

**Training and Evaluation on Downstream Classifiers To link fairness performance in the embedding**
space to downstream classification (in which more extensive prior work has been completed), we
train downstream classifiers and evaluate classification bias. After training the DML model with
the aforementioned criteria, the network is fixed. The output embeddings from the image training
datasets, in addition to the class labels, are used to train four downstream classification models: logistic
regression (LR), support vector machine (SVM), K-Means (KM), and random forest (RF) (Pedregosa
et al., 2011). In the manually imbalanced upstream setting, we train downstream classifiers on
the original balanced image datasets to ascertain if bias incurred in the embedding can propagate
downstream even if the downstream classifier is trained with real balanced data.

We execute class imbalanced experiments for CARS196 and CUB200 and vary the level of imbalance
between minoritized and majoritized classes in the upstream training set.

**PARADE Configuration We test Partial Attribute De-correlation, PARADE, by training models in**
the listed settings: manually color imbalanced dataset for CUB200, CelebA and LFW. The attribute
used to train the sensitive attribute embedding for each dataset, and the attribute used for fairness
evaluation. We compare PARADE with margin loss and distance-weighted sampling (Wu et al., 2018)
to standard margin loss and distance-weighted sampling.

6 RESULTS

6.1 SOTA DML METHODS HAVE LARGE FAIRNESS GAPS IN finDML BENCHMARK

Our experiments indicate that current DML methods encounter crucial fairness limitations in the
presence of imbalanced training data. Table 1 (along with a corresponding table for CARS196 in the
Supplemental) demonstrate that gaps in the manually class imbalanced setting are greater than the
balanced control setting. In four combinations of loss functions and sampling strategies, we do not
observe a scenario in which the class imbalanced setting achieves a smaller gap than the control in the
embedding space, nor the downstream classification. This is particularly significant due to the nature
of sampling strategies studied (Wu et al., 2018; Schroff et al., 2015), which batch samples to force
the model to correct “hard" examples. The results validate finDML as a benchmark and framework
for fairness through the lens of well-studied fairness characterization in classification.

Interestingly, Table 1 displays non-negligible gaps in downstream performance metrics recall and
precision even in the balanced control case. This could represent stenography of underlying structures
in the data, such as car color or bird size. More likely, however, these gaps are due to use of macroaveraging in recall and precision calculations. Nonetheless, the manually class imbalanced settings
consistently produce larger gaps.

6.2 PROPAGATION OF BIAS TO DOWNSTREAM TASKS

The tabular results emphasize a significant result: naive re-balancing with real data downstream cannot
overcome bias incurred in the upstream embedding in any setting studied. Indeed, Table 1 exhibits
propagation of bias from upstream embeddings (trained on imbalanced data) to downstream tasks
(trained on fixed upstream embeddings with a re-balanced dataset). To provide additional context for
the result, we direct to increasing use of DML models as components of larger classification models.
This trend is arising in literature such as supervised contrastive learning, and recent developments
in pre-training and lifting DML models for classification (Khosla et al., 2020). This necessitates
tackling bias in the representation space of DML as opposed to patches downstream, and emphasizes
the importance of defining fairness in this setting as done in our work.

**Impact of imbalance degree on lack of fairness** Figure 3 shows that gaps in downstream classification mimic those upstream, even as we vary the level of imbalance introduced when training
the upstream embedding. Here, the random forest classifier sees greater gaps in downstream metrics


-----

Table 1: Gap study on CUB200-2011. Average gaps in representation space and downstream classification (logistic regressor) over 10 seeds between minoritized and majoritized classes in manually
class imbalanced experiments (Imbalanced) and control experiments (Balanced) for CUB200-2011.
Results for CARS196 are available in the supplementary with similar conclusions. Bold represents
larger gap for each method shown (Loss · Batch Mining).

|Experiments → Balanced Imbalanced Balanced Imbalanced|Col2|Col3|Col4|
|---|---|---|---|
|Objective →||Margin · Distance|Margin · Semi-hard|
|UPSTREAM EMBEDDING|Recall@1 NMI U KL|0.017 ± 0.007 0.212 ± 0.029 −0.001 ± 0.004 0.112 ± 0.012 −0.042 ± 0.003 0.0 ± 0.002|0.02 ± 0.007 0.187 ± 0.031 −0.004 ± 0.004 0.092 ± 0.017 −0.048 ± 0.004 0.002 ± 0.004|
|DOWNSTREAM CLASSIFICATION|Precision Recall Accuracy|0.339 ± 0.007 0.39 ± 0.014 0.36 ± 0.007 0.424 ± 0.018 0.014 ± 0.002 0.131 ± 0.027|0.33 ± 0.004 0.393 ± 0.015 0.351 ± 0.005 0.43 ± 0.016 0.016 ± 0.005 0.131 ± 0.031|
|Objective →||Triplet · Distance|Triplet · Semi-hard|
|UPSTREAM EMBEDDING|Recall@1 NMI U KL|0.019 ± 0.006 0.159 ± 0.031 −0.001 ± 0.004 0.103 ± 0.016 −0.054 ± 0.006 −0.004 ± 0.009|0.019 ± 0.006 0.168 ± 0.036 −0.004 ± 0.006 0.082 ± 0.016 −0.051 ± 0.006 0.014 ± 0.011|
|DOWNSTREAM CLASSIFICATION|Precision Recall Accuracy|0.336 ± 0.005 0.41 ± 0.014 0.357 ± 0.004 0.459 ± 0.016 0.016 ± 0.003 0.179 ± 0.031|0.338 ± 0.007 0.384 ± 0.014 0.359 ± 0.007 0.426 ± 0.016 0.02 ± 0.005 0.134 ± 0.031|


|Recal|l@1|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|NMI UKL||||||
|||||||
|||||||
|||||||
|||||||


|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
||sion|||||
|Preci|sion|||||
|Recal Accu|l racy|||||
|||||||
|||||||


Upstream Embedding Downstream (RF)

Recall@1 0.5

0.15 NMI

_UKL_ 0.4

0.10 Precision

Gap Gap 0.3 Recall

Accuracy

0.05 0.2

0.00 0.1

50-50 40-60 30-70 20-80 10-90 50-50 40-60 30-70 20-80 10-90

Imbalance Percentage Imbalance Percentage


Figure 3: Impact of varying imbalance between the minoritized and majoritized classes on upstream
embedding and downstream classifier (RF) in the manually class imbalanced CARS196 experiments.
(Note: the imbalance percentage 50 − 50 is equivalent to the balanced setting). Gaps increase both
upstream and downstream with more imbalance introduced to the upstream training data.

than the control, even when manual imbalance is set at 40 − 60 upstream, and the downstream
training dataset is balanced. For results with additional downstream classifiers, see Supplemental.
This experiment demonstrates that the propagation of bias to downstream will occur even with lower
levels of imbalance, and does not appear to depend on the downstream classifier chosen.

6.3 REDUCED SUBGROUP GAPS THROUGH PARTIAL DE-CORRELATION WITH SENSITIVE
ATTRIBUTE

Table 2a shows results for performance gaps between relevant subgroups in both facial recognition
datasets. PARADE shows strong results for CUB200 bird color dataset, primarily reducing gaps
downstream and accordingly to recall@1 (Definition 1). PARADE can reliably reduce gaps for
both the representation space and downstream classifiers on LFW. Interestingly, we observe that the
majoritized subgroup (“White") had worst performance of all “Race" subgroups (see Supplemental),
contrary to previous results (Samadi et al., 2018).[3] As such, we measure gaps between the worstperforming subgroup and others.

2Due to the great number of singleton classes in LFW, recall@1 is discarded as a metric.
3Note: minoritized subgroups can still encounter notable bias across other axes more difficult to measure (Radford & Espenshade, 2014).


-----

Table 2: Comparison between PARADE and standard losses with distance-weighted sampling of
average gaps in representation space and downstream classification (logistic regressor) over 3 seeds
between minoritized and majoritized groups in (a) facial dataset studies, namely on CelebA (w.r.t.
“Fitzpatrick Skintone") and between worst-performing subgroup and other subgroups in LFW[2](w.r.t.
“Race") with Margin loss and (b) bird color experiments for CUB200 image dataset (w.r.t. color) with
_Margin and Triplet loss. Bold represents smaller gap (better fairness performance)._

(a)

(b)

|Facial CelebA (skintone) LFW (race) Datasets PARADE Margin · Distance PARADE Margin · Distance|Col2|CelebA (skintone)|LFW (race)|
|---|---|---|---|
|||PARADE Margin · Distance|PARADE Margin · Distance|
|UPSTREAM EMBEDDING|Recall@1 NMI UKL|0.085 ± 0.009 0.122 ± 0.005 −0.012 ± 0.003 −0.002 ± 0.003 −0.04 ± 0.011 −0.03 ± 0.007|0.075 ± 0.014 0.068 ± 0.013 0.041 ± 0.003 0.048 ± 0.003 0.163 ± 0.003 0.165 ± 0.005|
|DOWNSTREAM CLASSIFICATION|Precision Recall Accuracy|0.146 ± 0.006 0.1 ± 0.007 0.141 ± 0.007 0.098 ± 0.007 0.131 ± 0.006 0.082 ± 0.005|0.004 ± 0.002 0.005 ± 0.005 0.003 ± 0.001 0.007 ± 0.006 0.009 ± 0.003 0.012 ± 0.009|


|CUB200-2011 color|Col2|PARADE (M · D) Margin · Distance PARADE (T · D) Triplet · Distance|Col4|
|---|---|---|---|
|UPSTREAM EMBEDDING|Recall@1 NMI UKL|0.172 ± 0.021 0.176 ± 0.041 0.349 ± 0.031 0.326 ± 0.184 0.167 ± 0.013 0.153 ± 0.013|0.172 ± 0.027 0.195 ± 0.051 0.372 ± 0.291 0.359 ± 0.024 0.174 ± 0.035 0.159 ± 0.018|
|DOWNSTREAM CLASSIFICATION|Precision Recall Accuracy|0.317 ± 0.046 0.333 ± 0.049 0.352 ± 0.039 0.363 ± 0.046 0.163 ± 0.018 0.153 ± 0.028|0.248 ± 0.038 0.308 ± 0.119 0.276 ± 0.042 0.337 ± 0.123 0.148 ± 0.049 0.154 ± 0.029|



For CelebA, we find for standard methods the minoritized subgroups to generally perform worst.
PARADE excels at gap reduction upstream but encounters larger subgroup gaps downstream compared to standard methods. While PARADE does reduce downstream gaps between light skintones (I,
II, and III), and the two lighter dark skintones (IV, V), gaps increase between lighter skintones and
the darkest skintone (VI) (see Supplemental). Because skintone VI constitutes < 1% of the CelebA
dataset, PARADE is likely not able to learn similarity between faces over attributes besides skintone.
And PARADE is prevented from learning similarities based on skintone due to de-correlation. In
such settings, PARADE could be combined with oversampling minoritized subgroups to ensure better
performance.

In general, the results show promising benefits of PARADE to adequately address and improve on
the challenge of subgroup gaps for DML models used in facial recognition; and in the standard DML
dataset CUB200, for recall@1 upstream (Definition 1) and across metrics downstream (Table 2b).

7 DISCUSSION

In this work, we introduce the finDML benchmark, a framework for fairness in deep metric learning
(§3.2). We demonstrate the fairness limitations of established DML techniques, and the surprising
propagation of embedding space bias to downstream classifiers. Importantly, we find that this bias
cannot be addressed at the level of downstream classifiers but instead needs to be addressed at the
DML stage. We investigate the limit of this propagation in manually introduced imbalance, and
finally show that PARADE can reduce subgroup gaps in several settings.

**Limitations** PARADE suffers from pitfalls similar to other “fairness with awareness" methods:
PARADE uses information only on pre-defined sensitive attributes and therefore can be unfair w.r.t.
other sensitive attributes. PARADE does have an advantage in addressing the combinatorial number
of attributes considered in multi-attribute fairness through DML, which will scale sub-combinatorially
in time/space complexity. We also note that subgroup gaps are not sufficient to capturing societal
understandings of fairness, and there is no consensus as to how to remedy such gaps (Chouldechova &
Roth, 2018; Dwork et al., 2012; Hardt et al., 2016; Zemel et al., 2013; Zafar et al., 2017). Additionally,
while PARADE intentionally optimizes Definitions 1 and 2, we provide no explicit guarantee and
optimization of uniformity, Definition 3, remains an open problem. Finally, PARADE does incur
slight decrease in overall performance, similar to other methods (Wick et al., 2019) (see Supplemental
for per-subgroup performance and additional fairness-utility trade-off analysis for PARADE).


-----

**Code of Ethics Statement** The work presented here deals with fairness in deep metric learning.
A portion of our studies in the paper focus on CARS196 and CUB200-2011 datasets, which have
consistently been used in benchmarking novel DML frameworks (Krause et al., 2013; Wah et al.,
2011). The fairness analysis considered for CUB200-2011 deals with bird color, which does not, to
our knowledge, correspond with any societal problems relating to fairness. Nonetheless, as CUB2002011 is used in a litany of papers for SOTA performance comparison, finDML includes CUB200 so
that DML methods can be analyzed w.r.t. fairness on a dataset used in their original paper.

We do include facial recognition datasets and tasks and analyze fairness with respect to facial
attributes. Facial recognition does raise ethical concerns in practice. We note that our paper attempts
to address primary social concerns in facial identity recognition. We do not encourage the task of
facial attribute recognition, and solely use labeled attributes that correspond to known axes of bias
for fairness analysis (e.g. Race and Skintone). As PARADE has solely been tested in two widely
used public facial recognition datasets, we cannot guarantee fairness nor privacy in practical settings
with private facial datasets.

**Reproducibility Statement** Additional experimental results discussed in the main paper and others
are contained in Supplemental C. Implementation details including attribute information, generation
of attributes, training parameters, metric calculation and gap computation are listed in Supplemental D.
[Code available here: https://github.com/ndullerud/dml-fairness.](https://github.com/ndullerud/dml-fairness)

ACKNOWLEDGEMENTS

We would like to acknowledge and thank our sponsors, who support our research with financial and inkind contributions: CIFAR through the Canada CIFAR AI Chair, Intel, NFRF through an Exploration
grant, NSERC through the COHESA Strategic Alliance, International Max Planck Research School
for Intelligent Systems (IMPRS-IS), European Laboratory for Learning and Intelligent Systems
(ELLIS) PhD program, Mitacs Accelerate through internship program, and Microsoft Research.
Resources used in preparing this research were provided, in part, by the Province of Ontario, the
Government of Canada through CIFAR, and companies sponsoring the Vector Institute. We would
like to thank members of the CleverHans Lab and HealthyML Lab for their invaluable feedback.

REFERENCES

Tameem Adel, Isabel Valera, Zoubin Ghahramani, and Adrian Weller. One-network adversarial
fairness. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 2412–
2420, 2019.

Alexander Alemi, Ben Poole, Ian Fischer, Joshua Dillon, Rif A Saurous, and Kevin Murphy. Fixing a
broken elbo. In International Conference on Machine Learning, pp. 159–168. PMLR, 2018.

Alexander Amini, Ava P Soleimany, Wilko Schwarting, Sangeeta N Bhatia, and Daniela Rus.
Uncovering and mitigating algorithmic bias through learned latent structure. In Proceedings of the
_2019 AAAI/ACM Conference on AI, Ethics, and Society, pp. 289–295, 2019._

Richard Berk. An impact assessment of machine learning risk forecasts on parole board decisions
and recidivism. Journal of Experimental Criminology, 13(2):193–216, 2017.

Alex Beutel, Jilin Chen, Zhe Zhao, and Ed H Chi. Data decisions and theoretical implications when
adversarially learning fair representations. arXiv preprint arXiv:1707.00075, 2017.

Biagio Brattoli, Joseph Tighe, Fedor Zhdanov, Pietro Perona, and Krzysztof Chalupka. Rethinking
zero-shot video classification: End-to-end training for realistic applications. In Proceedings of the
_IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020._

Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins,
and Alexander Lerchner. Understanding disentangling in β-vae. arXiv preprint arXiv:1804.03599,
2018.


-----

Souradip Chakraborty, Ekansh Verma, Saswata Sahoo, and Jyotishka Datta. Fairmixrep: Selfsupervised robust representation learning for heterogeneous data with fairness constraints. In 2020
_International Conference on Data Mining Workshops (ICDMW), pp. 458–463. IEEE, 2020._

Irene Chen, Fredrik D Johansson, and David Sontag. Why is my classifier discriminatory? arXiv
_preprint arXiv:1805.12002, 2018a._

Ricky TQ Chen, Xuechen Li, Roger Grosse, and David Duvenaud. Isolating sources of disentanglement in variational autoencoders. arXiv preprint arXiv:1802.04942, 2018b.

Victoria Cheng, Vinith M. Suriyakumar, Natalie Dullerud, Shalmali Joshi, and Marzyeh Ghassemi. Can you fake it until you make it? impacts of differentially private synthetic data on
downstream classification fairness. In Proceedings of the 2021 ACM Conference on Fairness,
_Accountability, and Transparency, FAccT ’21, pp. 149–160, New York, NY, USA, 2021. Associa-_
tion for Computing Machinery. ISBN 9781450383097. doi: 10.1145/3442188.3445879. URL
[https://doi.org/10.1145/3442188.3445879.](https://doi.org/10.1145/3442188.3445879)

Alexandra Chouldechova. Fair prediction with disparate impact: A study of bias in recidivism
prediction instruments. Big data, 5(2):153–163, 2017.

Alexandra Chouldechova and Aaron Roth. The frontiers of fairness in machine learning. arXiv
_preprint arXiv:1810.08810, 2018._

J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009.

J. Deng, J. Guo, N. Xue, and S. Zafeiriou. Arcface: Additive angular margin loss for deep face
recognition. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 4685–4694, 2019. doi: 10.1109/CVPR.2019.00482.

Y. Duan, W. Zheng, X. Lin, J. Lu, and J. Zhou. Deep adversarial metric learning. In 2018 IEEE/CVF
_Conference on Computer Vision and Pattern Recognition, pp. 2780–2789, 2018. doi: 10.1109/_
CVPR.2018.00294.

Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness through
awareness. In Proceedings of the 3rd innovations in theoretical computer science conference, pp.
214–226, 2012.

Harrison Edwards and Amos Storkey. Censoring representations with an adversary. arXiv preprint
_arXiv:1511.05897, 2015._

Babak Esmaeili, Hao Wu, Sarthak Jain, Alican Bozkurt, Narayanaswamy Siddharth, Brooks Paige,
Dana H Brooks, Jennifer Dy, and Jan-Willem Meent. Structured disentangled representations. In
_The 22nd International Conference on Artificial Intelligence and Statistics, pp. 2525–2534. PMLR,_
2019.

Xavier Gitiaux and Huzefa Rangwala. Fair representations by compression. In Proceedings of the
_AAAI Conference on Artificial Intelligence, volume 35, pp. 11506–11515, 2021._

Amir Globerson and Sam T. Roweis. Metric learning by collapsing classes. In Y. Weiss,
B. Schölkopf, and J. C. Platt (eds.), Advances in Neural Information Processing Sys_tems 18, pp. 451–458. MIT Press, 2006._ [URL http://papers.nips.cc/paper/](http://papers.nips.cc/paper/2947-metric-learning-by-collapsing-classes.pdf)
[2947-metric-learning-by-collapsing-classes.pdf.](http://papers.nips.cc/paper/2947-metric-learning-by-collapsing-classes.pdf)

Arthur Gretton, Karsten Borgwardt, Malte Rasch, Bernhard Schölkopf, and Alex Smola. A kernel
method for the two-sample-problem. Advances in neural information processing systems, 19:
513–520, 2006.

Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant
mapping. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
2006.

Moritz Hardt, Eric Price, and Nathan Srebro. Equality of opportunity in supervised learning, 2016.


-----

Ben Harwood, BG Kumar, Gustavo Carneiro, Ian Reid, Tom Drummond, et al. Smart mining for
deep metric learning. In Proceedings of the IEEE International Conference on Computer Vision,
pp. 2821–2829, 2017.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770–778, 2016.

Elad Hoffer and Nir Ailon. Deep metric learning using triplet network, 2018.

J. Hu, J. Lu, and Y. Tan. Discriminative deep metric learning for face verification in the wild. In 2014
_IEEE Conference on Computer Vision and Pattern Recognition, 2014._

Gary B. Huang, Vidit Jain, and Erik Learned-Miller. Unsupervised joint alignment of complex
images. In ICCV, 2007.

Christina Ilvento. Metric learning for individual fairness, 2020.

Pierre Jacob, David Picard, Aymeric Histace, and Edouard Klein. Metric learning with horde: Highorder regularizer for deep embeddings. In The IEEE Conference on Computer Vision and Pattern
_Recognition (CVPR), 2019._

Ayush Jaiswal, Rob Brekelmans, Daniel Moyer, Greg Ver Steeg, Wael AbdAlmageed, and Premkumar
Natarajan. Discovery and separation of features for invariant representation learning. arXiv preprint
_arXiv:1912.00646, 2019._

Herve Jegou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search.
_IEEE transactions on pattern analysis and machine intelligence, 33(1):117–128, 2011._

Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning, 2020.

Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In International Conference on
_Machine Learning, pp. 2649–2658. PMLR, 2018._

Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak. Proxy anchor loss for deep metric
learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
_(CVPR), June 2020._

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR
_[2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:](http://arxiv.org/abs/1412.6980)_
[//arxiv.org/abs/1412.6980.](http://arxiv.org/abs/1412.6980)

Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint_
_arXiv:1312.6114, 2013._

Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for finegrained categorization. In Proceedings of the IEEE International Conference on Computer Vision
_Workshops, pp. 554–561, 2013._

Neeraj Kumar, Alexander C. Berg, Peter N. Belhumeur, and Shree K. Nayar. Attribute and simile
classifiers for face verification. In 2009 IEEE 12th International Conference on Computer Vision,
pp. 365–372, 2009. doi: 10.1109/ICCV.2009.5459250.

Preethi Lahoti, Alex Beutel, Jilin Chen, Kang Lee, Flavien Prost, Nithum Thain, Xuezhi Wang, and
Ed H. Chi. Fairness without demographics through adversarially reweighted learning, 2020.

Hui Li, Maryellen L. Giger, Benjamin Q. Huynh, and Natalia O. Antropova. Deep learning in breast
cancer risk assessment: evaluation of convolutional neural networks on a clinical dataset of fullfield digital mammograms. Journal of medical imaging (Bellingham, Wash.), 4(4):041304–041304,
[Oct 2017. ISSN 2329-4302. doi: 10.1117/1.JMI.4.4.041304. URL https://pubmed.ncbi.](https://pubmed.ncbi.nlm.nih.gov/28924576)
[nlm.nih.gov/28924576.](https://pubmed.ncbi.nlm.nih.gov/28924576)


-----

Yujia Li, Kevin Swersky, and Richard Zemel. Learning unbiased features. _arXiv preprint_
_arXiv:1412.5244, 2014._

Xudong Lin, Yueqi Duan, Qiyuan Dong, Jiwen Lu, and Jie Zhou. Deep variational metric learning.
In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss (eds.), Computer Vision
_– ECCV 2018, pp. 714–729, Cham, 2018. Springer International Publishing. ISBN 978-3-030-_
01267-0.

Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep
hypersphere embedding for face recognition. IEEE Conference on Computer Vision and Pattern
_Recognition (CVPR), 2017._

Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
_Proceedings of International Conference on Computer Vision (ICCV), December 2015._

Stuart P. Lloyd. Least squares quantization in pcm. IEEE Trans. Information Theory, 28:129–136,
1982.

Francesco Locatello, Gabriele Abbati, Tom Rainforth, Stefan Bauer, Bernhard Schölkopf, and Olivier
Bachem. On the fairness of disentangled representations. arXiv preprint arXiv:1905.13662, 2019a.

Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Schölkopf,
and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled representations. In international conference on machine learning, pp. 4114–4124. PMLR,
2019b.

Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard Zemel. The variational fair
autoencoder. arXiv preprint arXiv:1511.00830, 2015.

Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine
_learning research, 9(Nov):2579–2605, 2008._

David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning adversarially fair and
transferable representations. In International Conference on Machine Learning, pp. 3384–3393.
PMLR, 2018.

Christopher Manning, Prabhakar Raghavan, and Hinrich Schütze. Introduction to information
retrieval. Natural Language Engineering, 16(1):100–103, 2010.

Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey
on bias and fairness in machine learning. arXiv preprint arXiv:1908.09635, 2019.

Timo Milbich, Karsten Roth, Homanga Bharadhwaj, Samarth Sinha, Yoshua Bengio, Björn Ommer,
and Joseph Paul Cohen. Diva: Diverse visual feature aggregation for deep metric learning. CoRR,
[abs/2004.13458, 2020. URL https://arxiv.org/abs/2004.13458.](https://arxiv.org/abs/2004.13458)

Timo Milbich, Karsten Roth, Samarth Sinha, Ludwig Schmidt, Marzyeh Ghassemi, and Björn
Ommer. Characterizing generalization under out-of-distribution shifts in deep metric learning.
_[CoRR, abs/2107.09562, 2021. URL https://arxiv.org/abs/2107.09562.](https://arxiv.org/abs/2107.09562)_

Yair Movshovitz-Attias, Alexander Toshev, Thomas K. Leung, Sergey Ioffe, and Saurabh Singh. No
fuss distance metric learning using proxies. In Proceedings of the IEEE International Conference
_on Computer Vision (ICCV), Oct 2017._

Daniel Moyer, Shuyang Gao, Rob Brekelmans, Greg Ver Steeg, and Aram Galstyan. Invariant
representations without adversarial training. arXiv preprint arXiv:1805.09458, 2018.

Kevin Musgrave, Serge J. Belongie, and Ser-Nam Lim. A metric learning reality check. CoRR,
[abs/2003.08505, 2020. URL https://arxiv.org/abs/2003.08505.](https://arxiv.org/abs/2003.08505)

William Paul and Philippe Burlina. Generalizing fairness: Discovery and mitigation of unknown
sensitive attributes. arXiv preprint arXiv:2107.13625, 2021.


-----

F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,
12:2825–2830, 2011.

Qi Qian, Lei Shang, Baigui Sun, Juhua Hu, Hao Li, and Rong Jin. Softtriple loss: Deep metric
learning without triplet sampling. In Proceedings of the IEEE/CVF International Conference on
_Computer Vision (ICCV), October 2019._

Alexandria Walton Radford and Thomas J Espenshade. No longer separate, not yet equal: race and
_class in elite college admission and campus life. Princeton University Press, 2014._

Borja Rodríguez-Gálvez, Ragnar Thobaben, and Mikael Skoglund. A variational approach to privacy
and fairness. arXiv preprint arXiv:2006.06332, 2020.

Harrison Rosenberg, Brian Tang, Kassem Fawaz, and Somesh Jha. Fairness properties of face
recognition and obfuscation systems, 2021.

[Karsten Roth and Biagio Brattoli. Deep-metric-learning-baselines. https://github.com/](https://github.com/Confusezius/Deep-Metric-Learning-Baselines)
[Confusezius/Deep-Metric-Learning-Baselines, 2019.](https://github.com/Confusezius/Deep-Metric-Learning-Baselines)

Karsten Roth, Timo Milbich, and Bjorn Ommer. Pads: Policy-adapted sampling for visual similarity
learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
_(CVPR), June 2020a._

Karsten Roth, Timo Milbich, Björn Ommer, Joseph Paul Cohen, and Marzyeh Ghassemi. S2sd:
Simultaneous similarity-based self-distillation for deep metric learning, 2020b.

Karsten Roth, Timo Milbich, Samarth Sinha, Prateek Gupta, Bjorn Ommer, and Joseph Paul Cohen.
Revisiting training strategies and generalization performance in deep metric learning. In Hal Daumé
III and Aarti Singh (eds.), Proceedings of the 37th International Conference on Machine Learning,
volume 119 of Proceedings of Machine Learning Research, pp. 8242–8252. PMLR, 13–18 Jul
[2020c. URL http://proceedings.mlr.press/v119/roth20a.html.](http://proceedings.mlr.press/v119/roth20a.html)

Samira Samadi, Uthaipon Tantipongpipat, Jamie Morgenstern, Mohit Singh, and Santosh Vempala.
The price of fair pca: One extra dimension. arXiv preprint arXiv:1811.00103, 2018.

Mhd Hasan Sarhan, Nassir Navab, Abouzar Eslami, and Shadi Albarqouni. Fairness by learning
orthogonal disentangled representations. In European Conference on Computer Vision, pp. 746–
761. Springer, 2020.

Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face
recognition and clustering. In Proceedings of the IEEE Conference on Computer Vision and Pattern
_Recognition (CVPR), June 2015._

Samarth Sinha, Karsten Roth, Anirudh Goyal, Marzyeh Ghassemi, Hugo Larochelle, and Animesh
Garg. Uniform priors for data-efficient transfer, 2020.

Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In Advances in
_Neural Information Processing Systems, pp. 1857–1865, 2016._

Jiaming Song, Pratyusha Kalluri, Aditya Grover, Shengjia Zhao, and Stefano Ermon. Learning
controllable fair representations. In The 22nd International Conference on Artificial Intelligence
_and Statistics, pp. 2164–2173. PMLR, 2019._

Yao-Hung Hubert Tsai, Martin Q Ma, Han Zhao, Kun Zhang, Louis-Philippe Morency, and Ruslan
Salakhutdinov. Conditional contrastive learning: Removing undesirable information in selfsupervised representations. arXiv preprint arXiv:2106.02866, 2021.

C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The caltech-ucsd birds-200-2011 dataset.
Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.

Jian Wang, Feng Zhou, Shilei Wen, Xiao Liu, and Yuanqing Lin. Deep metric learning with angular
loss. In Proceedings of the IEEE International Conference on Computer Vision, pp. 2593–2601,
2017.


-----

Jiang Wang, Yang song, Thomas Leung, Chuck Rosenberg, Jinbin Wang, James Philbin, Bo Chen,
and Ying Wu. Learning fine-grained image similarity with deep ranking, 2014.

Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere, 2020.

Xun Wang, Xintong Han, Weilin Huang, Dengke Dong, and Matthew R. Scott. Multi-similarity loss
with general pair weighting for deep metric learning, 2020a.

Zeyu Wang, Klint Qinami, Ioannis Christos Karakozis, Kyle Genova, Prem Nair, Kenji Hata, and
Olga Russakovsky. Towards fairness in visual recognition: Effective strategies for bias mitigation,
2020b.

Kilian Q Weinberger, John Blitzer, and Lawrence K. Saul. Distance metric learning for large margin nearest neighbor classification. In Y. Weiss, B. Schölkopf,
and J. C. Platt (eds.), _Advances_ _in_ _Neural_ _Information_ _Processing_ _Systems_ _18,_
pp. 1473–1480. MIT Press, 2006. URL [http://papers.nips.cc/paper/](http://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf)
[2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.](http://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf)
[pdf.](http://papers.nips.cc/paper/2795-distance-metric-learning-for-large-margin-nearest-neighbor-classification.pdf)

Eric W. Weisstein. Hypersphere, 2002.

Michael Wick, swetasudha panda, and Jean-Baptiste Tristan. Unlocking fairness: a trade-off revisited. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Asso[ciates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/](https://proceedings.neurips.cc/paper/2019/file/373e4c5d8edfa8b74fd4b6791d0cf6dc-Paper.pdf)
[373e4c5d8edfa8b74fd4b6791d0cf6dc-Paper.pdf.](https://proceedings.neurips.cc/paper/2019/file/373e4c5d8edfa8b74fd4b6791d0cf6dc-Paper.pdf)

Chao-Yuan Wu, R. Manmatha, Alexander J. Smola, and Philipp Krähenbühl. Sampling matters in
deep embedding learning, 2018.

Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, and Graham Neubig. Controllable invariance
through adversarial feature learning. arXiv preprint arXiv:1705.11122, 2017.

Depeng Xu, Shuhan Yuan, Lu Zhang, and Xintao Wu. Fairgan: Fairness-aware generative adversarial
networks. In 2018 IEEE International Conference on Big Data (Big Data), pp. 570–575. IEEE,
2018.

Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P Gummadi. Fairness
constraints: Mechanisms for fair classification. In Artificial Intelligence and Statistics, pp. 962–970.
PMLR, 2017.

Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations.
In International conference on machine learning, pp. 325–333. PMLR, 2013.

Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adversarial
learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pp. 335–
340, 2018.

Han Zhao and Geoffrey J Gordon. Inherent tradeoffs in learning fair representations. arXiv preprint
_arXiv:1906.08386, 2019._

Han Zhao, Amanda Coston, Tameem Adel, and Geoffrey J Gordon. Conditional learning of fair
representations. arXiv preprint arXiv:1910.07162, 2019.

Wenzhao Zheng, Zhaodong Chen, Jiwen Lu, and Jie Zhou. Hardness-aware deep metric learning.
_The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019._


-----

# SUPPLEMENTAL MATERIAL

A ADDITIONAL BACKGROUND

A.1 DEEP METRIC LEARNING DEFINITIONS

Here, we iterate through some common DML criteria and batch mining strategies more formally
than in the main paper. Throughout this section, let X denote the input data, ϕ(X) the embedded
data, Y the class label, and let Y (x) denote the value of the ground truth class label for data instance
_x. Denote the set of all positive pairs with respect to class label Y as P = {(x1, x2) ∈_ _X × X :_
_Y (x1) = Y (x2), x1_ = x2 . Denote the set of all negative pairs with respect to class label Y as
N = {(x1, x2) ∈ _X × ̸_ _X :} Y (x1) ̸= Y (x2)}. We use the notation (xa, xp, xn) ∈_ _X × X × X to_
denote a triplet with an anchor sample xa, positive sample xp where Y (xa) = Y (xp), and negative
sample xn where Y (xa) ̸= Y (xn).

**Batch Sampling and Mining** The batch sampling procedure in deep metric learning methods
differ from that of generic deep classifiers in that canonical loss functions require tuples or pairs of
samples in order to utilise ranking objectives as training surrogates to learn an appropriate similarity
metric. To ensure that tuples with positive and negative examples can be extracted from the batch,
the Samples-Per-Class-n (SPC-n) heuristic (see e.g. Roth et al. (2020c)) is generally used, where
commonly n = 2, 4, 8. Given a batch size b, the SPC-n technique randomly selects b/n classes from
which n training samples are then drawn randomly to be included in each batch B.

After feeding the batch through the network, tuples are mined from the batch to use in the loss
function. We refer to mining in this paper either as batch mining or overload the both as batch
_sampling terminology. The naive solution to tuple mining is random mining, in which all possible_
tuples of the form (xa, xp, xn) are considered and b are randomly chosen from the batch. However,
this method lacks the capacity to utilize valuable information about the current embedding space, and
is prone to significant redundancy in the training signal Schroff et al. (2015); Wu et al. (2018).
**Definition 4example from (Random Mining)xp** : Y (xp. Hu et al. (2014) For each) = Y (xa), xp = xa _and a negative example from xa ∈B, we randomly draw a positivexn_ :
_Y (xn)_ = Y (x {a) _to form the triplet ∈B_ (xa, xp, xn) ̸. _}_ _{_ _∈B_
_̸_ _}_

Intuitively, this could be mitigated by hard mining heuristics searching for negative samples that are
closer to the anchor sample in the embedding space than positive samples, thereby always ensuring a
significant training signal. Unfortunately, such approaches are prone to heavy overfitting, training
instability and large gradient variance, thereby commonly resulting in less-than-optimal solutions
(see e.g. Schroff et al. (2015); Harwood et al. (2017); Wu et al. (2018)). Recent approaches thus
establish more lenient heuristics, such as through the introduction of slack parameters to the hard
mining objective (e.g. semi-hard mining Schroff et al. (2015) or softhard mining Roth & Brattoli
(2019)).
**Definition 5xp** : Y ( (Semi-hard Mining)xp) = Y (xa), xp = x. For eacha _, and a negative example from the set xa ∈B, we randomly draw a positive example from_
_{_ _∈B_ _̸_ _}_

_{xn ∈B : Y (xn) ̸= Y (xa), ∥ϕ(xa) −_ _ϕ(xn)∥2[2][∥][ϕ][(][x][a][)][ −]_ _[ϕ][(][x][p][) +][ γ][∥][2]2[}]_

_where γ ∈_ R is a slack parameter, to form the triplet (xa, xp, xn).

While other adaptive means (e.g. Harwood et al. (2017); Roth et al. (2020a)) have shown strong
performance improvements, modern predefined heuristics such as distance-weighted tuple mining
Wu et al. (2018) offer a better cost-to-performance tradeoff Roth et al. (2020a). Here, the heuristic
leverages the fact that embeddings are commonly normalized to have unit L2 norm for regularization
purposes Wu et al. (2018). This ensures a distribution over a unit hypersphere, in which explicit
pairwise distributions can be established Weisstein (2002); Wu et al. (2018). By inverting this
distribution, distance-weighted mining can thus encourage a much more diverse coverage of tuple
difficulties, improving generalization performance and reducing gradient variance Wu et al. (2018).


-----

**Definition 6 (Distance-weighted). For embedding spaces normalized to the (D −** 1)-dimensional
_hypersphere S_ _[D][−][1], we haveWeisstein (2002); Wu et al. (2018) the following pairwise sampling_
_distribution q(•, •):_

_[D][−][3]_

2

_q (d (ϕ(xi), ϕ(xj)))_ _d (ϕ(xi), ϕ(xj))[D][−][2]_ 1
_∝_ _−_ [1]4 _[d][ (][ϕ][(][x][i][)][, ϕ][(][x][j][))]_
 

_for embedding pairs (ϕ(xi), ϕ(xj))_ _and Euclidean distance d(_ _,_ ). For each xa _,_
_we randomly draw a positive example from ∈S_ _[D][−][1]xp_ : Y (xp) = Y (xa), x•p •= xa _, and sample a ∈B_
_negative example based on an inverse distance distribution w.r.t. {_ _∈B_ _q:_ _̸_ _}_

_P_ (xn _xa)_ _min(λ, q[−][1](d(ϕ(xi), ϕ(xj))))_
_|_ _∝_

_where λ ∈_ R defines a clipping parameter to avoid potentially erroneous training samples.

**Examined Objectives** The primary goal of DML loss functions is to provide a training surrogate
that implicitly optimizes for desired metric space quantities by narrowing down the expected distance
between positive pairs of samples and expanding on the expected distance between negative pairs of
samples in the embedding space. Most commonly employed pair Hadsell et al. (2006) and tripledbased Schroff et al. (2015); Hoffer & Ailon (2018) ranking losses penalize close negative pairs and
disparate positive pairs up to a predefined margin to avoid overclustering. Using P(x) to denote all
positive pairs containing x
P(x) = {(x1, x2) ∈ P : x1 = x}

and N(x) to denote all negative pairs containing x

N(x) = {(x1, x2) ∈ N : x1 = x}

we define

**Definition 7 (Contrastive).** _Hadsell et al. (2006) Given a batch B, and pairs of samples S over_
_B × B, the contrastive objective is defined as:_


_contr = [1]_
_L_ _b_

_with margin γ._


IY (xi)=Y (xj )d (ϕ(xi), ϕ(xj)) + IY (xi)≠ _Y (xj_ ) [γ − _d (ϕ(xi), ϕ(xj))]+_
(xiX,xj )∈S


**Definition 8 (Triplet). Hoffer & Ailon (2018) The triplet loss extends the contrastive objective with**
_sample triplets and can be defined as:_


_tripl = [1]_
_L_ _b_

_with margin γ._



[d (ϕ(xa), ϕ(xp)) _d (ϕ(xa), ϕ(xn)) + γ]+_
_−_


(xa,xp,xn )∈T
_Y (xa_ )=Y (xp )≠ _Y (xn_ )


Margin loss extends the triplet objective through the inclusion of a learnable boundary β between
positive and negative pairs Wu et al. (2018). In our experiments, we utilise β = 1.2. These criteria
are widely used (see e.g. Roth et al. (2020c); Musgrave et al. (2020)) and require mining to make use
of the batch information.

**Definition 9 (Margin).** _Wu et al. (2018) The margin objective integrates the learnable distance_
_boundary β between positive and negative pairs of samples for a relative ordering of pairs with_
_respect to β as_


_γ + IY (xi)=Y (xj_ ) (d (ϕ(xi), ϕ(xj)) _β)_ IY (xi)=Y (xj ) (d (ϕ(xi), ϕ(xj)) _β)_
_−_ _−_ _̸_ _−_
(xiX,xj )∈S


_Lmargin =_


Going beyond pairs and triplets, one can also consider the case of more general n-tuples, which was
investigated e.g. in the N-Pair objective Sohn (2016) and the Multisimilarity loss Wang et al. (2020a).


-----

**Definition 10 (N-Pair). Sohn (2016) N-Pair loss is a simple augmentation of the triplet framework**
_in which all negatives in the batch B are incorporated in the objective function as:_

_Lnpair = [1]b_ _Y (xa(xa,xp)=XY (xp)∈B),a≠_ _p_ log 1 + _Y (xaxnX)≠_ _∈BY (xn_ ) exp  ϕ(xa)[∗][,T] _ϕ(xn) −_ _ϕ(xa)[∗][,T]_ _ϕ(xp)[∗][]_ +

 _ν_ 

_b_ _∥ϕ(xi)[∗]∥2[2]_ (6)

_[·]_ Xi∈B

_where ν denotes an embedding regularization parameter due to slow convergence for normalized_
_embeddings stated in Sohn (2016)_
**Definition 11 (Multisimilarity). Wang et al. (2020a) Multisimilarity loss fits into the ranking loss**
_category, but in addition to evaluation of cosine similarity between positive-anchor pairs and negative-_
_anchor pairs, the objective evaluates positive-positive and negative-negative pairs with respect to the_
_anchor:_


_sc (ϕ(xi), ϕ(xj))_ _sc (ϕ(xi), ϕ(xj)) > minxj_ _∈P(xi) sc (ϕ(xi), ϕ(xj)) −_ _ϵ_
_sc (ϕ(xi), ϕ(xj))_ _sc (ϕ(xi), ϕ(xj)) < maxxk∈N(xi) sc (ϕ(xi), ϕ(xk)) + ϵ_
0 _otherwise_


_s[∗]c_ [(][x][i][, x][j][) =]


_multisim = [1]_
_L_ _b_


1 +






exp ( _α (s[∗]c_ [(][ϕ][(][x][i][)][, ϕ][(][x][j][))][ −] _[λ][))]_ +
_−_ 
_xj_ _∈XP(xi)_




_α_ [log]


_xi∈B_


1

1 + exp (β (s[∗]c [(][ϕ][(][x][i][)][, ϕ][(][x][k][))][ −] _[λ][))]_

_β_ [log] 

_k∈XN(xi)_



_where cosine similarity sc(x, y) = x[T]_ _y for two normalized vectors x, y_ _X._
_∈_


(7)


Notably, the Multisimilarity loss employs a masking process as a stand-in for the lack of batchmining heuristic. While this proves to be similarly successfull in addressing the tuple sampling
complexity issue, this can also be addressed through the usage of proxy-samples. These are dummy
variables that represent various contextual properties (such as mean class representations) to serve as
standing for actual samples, which is found e.g. in the ArcFace Deng et al. (2019) or ProxyNCA loss
Movshovitz-Attias et al. (2017).
**Definition 12 (Proxy-NCA). Kim et al. (2020) ProxyNCA learns class proxies, or class centers,**
_which each represent a class in the set of unique classes Y. Then, each anchor from the batch is_
_sampled and a positive or negative proxy ψ[c]_ _∈_ R[d] _per class c ∈Y is introduced in lieu of a positive_
_or negative sample, respectively, giving:_


exp _d_ _ϕ(xi), ψY (xi)_

_proxy =_ log _−_
_L_ _−_ [1]b _xXi∈B_ _c∈Y\{Y  (xi)} [exp (][−][d][ (][ϕ][(][x][i][)][, ψ][c][)]_ !

**Definition 13 (Arcface). Deng et al. (2019) Arcface combines proxy and angular loss methods (e.g.P**
_in Wang et al. (2017)) to enforce an angular margin between the embeddings ϕ and a proxy (or_
_approximate center) W ∈_ R[c][×][d] _for each class, giving the following:_

exp _s_ cos _WY[T] (xi)[ϕ][(][x][i][) +][ γ][ = 0][.][5]_
_·_

_arc =_ log
_L_ _−_ [1]b _xXi∈B_ exp _s · cos_ _WY[T] (xi)[ϕ][(][x][i][) +][ γ][ = 0][.][5]_ + _Y (xixj)≠_ _∈BY (xj )_ [exp]s · cos _WY[T] (xj_ )[ϕ][(][x][i][)]

     

_where the angular component is encoded in additive angular margin penalty[P]_ _γ, and s is a scaling_
_parameter, which denotes the radius of the effective utilized hypersphere S._

**Standard Performance Metrics** Performance metrics in deep metric learning aim to capture
the quality of the similarity metric learned by the deep embedding model. Therefore, standard
performance metrics in DML reflect the closeness between samples of the same class, the separability


-----

of samples of different classes, the clustering quality of embedding, and the uniformity over the
hypersphere embedding space, which has been linked to zero-shot generalization capability Wang
& Isola (2020), as discussed in Section 2. In our experiments, we utilize recall@1 Jegou et al.
(2011), normalized mutual information Manning et al. (2010) between cluster labels assigned by the
well-known K-Means Lloyd (1982) algorithm and ground-truth class labels, and UKL to measure
the closeness between samples of same class, cluster quality of the embedding (and hence, the
separability of distinct classes) and uniformity, respectively. Here, we define these metrics formally,
but we note that there exist multitudinous performance metrics for DML that we do not define here or
use explicitly for our results, including f1 score, mean average precision (mAP), and recall@k for
_k > 1 Jegou et al. (2011)._
**Definition 14 (Recall@k). Jegou et al. (2011) Given k ∈{1, . . ., |X|}, denote NNk as defined in**
_Definition 1. Then, Recall@k is measured as:_

_Recall@k =_ 1 1 _∃x˜ ∈_ _NNk(x) : Y (˜x) = Y (x)_

_X_ 0 _else_
_|_ _|_ _xX∈X_ 

**Definition 15 (Normalized Mutual Information Score on Clusters). Manning et al. (2010) Let C a**
_clustering algorithm, such as K-Means Lloyd (1982) with the number of clusters set to |Y |, such_
_that C(x) indicates the cluster label for data point x ∈_ _X. The normalized mutual information score_
_between the target labels Y and the cluster labels C is measured as:_

2 _I(Y (X); C(X))_
_NMI =_ _·_

_H(Y (X)) + H(C(X))_


_where for random variables X, Y, I(·, ·) denotes the mutual information function:_

_I(X; Y ) = H(Y ) −_ _H(Y |X)_

_and H(·) denotes the entropy function:_

_H(X) = −_ Pr(x) log(Pr(x))

_xX∈X_

The performance metric UKL, used to measure feature uniformity for our empirical evaluations, is
defined in Section 3.1.

A.2 CLASSIFICATION FAIRNESS DEFINITIONS

Fairness definitions and criteria in classification are briefly mentioned in Section 2 of the main paper.
Here, we provide explicit formulas for the most common fairness definitions, including demographic
parity, equalized odds, and equality of opportunity Hardt et al. (2016), and provide some additional
context on fairness definition evolution.
**Definition 16 (Demographic Parity). The predictor** _Y[ˆ] satisfies demographic parity with respect to_
_attribute A and class Y if the predictor is independent of A:_

Pr[ Y[ˆ] = 1|A = a] = Pr[ Y[ˆ] = 1|A = b] _∀a, b ∈_ _A_

Specifically, demographic parity has largely been used over the years as a simple and intuitive
definition of fairness, in which a classifier is said to satisfy demographic parity if the sensitive
attribute is independent of the output of the classifier. While demographic parity provides a simple
fairness definition, the measure cannot capture fairness in classification tasks where the ground-truth
label is inherently related to a certain attribute value Li et al. (2017).

**Definition 17 (Equalized Odds). The predictor** _Y[ˆ] satisfies demographic parity with respect to_
_attribute A and class Y if the predictor is independent of A conditional on Y :_

Pr[ Y[ˆ] = 1|A = a, Y = y] = Pr[ Y[ˆ] = 1|A = b, Y = y] _∀a, b ∈_ _A, ∀y ∈{0, 1}_

_from Hardt et al. (2016)._

**Definition 18 (Equality of Opportunity). The predictor** _Y[ˆ] satisfies demographic parity with respect_
_to attribute A and class Y if the predictor is independent of A conditional on positively labelled Y :_


Pr[ Y[ˆ] = 1|A = a, Y = 1] = Pr[ Y[ˆ] = 1|A = b, Y = 1] _∀a, b ∈_ _A_

_from Hardt et al. (2016)._


-----

This lead to the introduction of other fairness definitions that capture such nuances, the most wellknown of which are probably equalized odds and equality of opportunity Hardt et al. (2016). However,
fairness metrics overall have been criticized due to the choice of protected attribute over which to
measure, and the inability of these metrics to capture bias with respect to certain attributes which are
not known at test-time. We discuss this to a limited extent in Section 7.

B DATASET SUMMARY STATISTICS

Figure 4: Class distribution in CARS196. Histograms visualizing the distribution over number of
samples per class in the train (left) and test (right) datasets in CARS196.

Figure 5: Class distribution in CUB200. Histograms visualizing the distribution over number of
samples per class in the train (left) and test (right) datasets in CUB200.

Black Blue Brown Buff Green Grey Iridescent Olive Orange Red White Yellow

Train 21.20 5.58 18.08 3.01 0.37 19.20 0.51 0.49 1.02 3.52 13.35 13.65
Test 21.17 5.56 18.11 3.04 0.39 19.21 0.51 0.51 1.01 3.52 13.29 13.68

Table 3: Summary statistics for CUB200 bird color The percentage of the dataset constituted by each
bird color in CUB200, in the train dataset and test dataset, respectively.

C ADDITIONAL RESULTS

C.1 CARS196

Additional results for all loss and batch mining strategies for the manually class imbalanced experiments and balanced controls for CARS196 are located in Tables 6 and 7. K-Means was also


-----

Figure 6: Class distribution in CelebA. Histograms visualizing the distribution over number of
samples per class in the train (left) and test (right) datasets in CelebA.

I II III IV V VI

Train 1.10 32.04 47.92 15.12 3.20 0.61
Test 1.24 32.09 48.09 14.81 3.22 0.55

Table 4: Summary statistics for CelebA Fitzpatrick Skintone. The percentage of the dataset constituted
by each Fitzpatrick Skintone in CelebA, in the train dataset and test dataset, respectively.

Figure 7: Class distribution in LFW. Histograms visualizing the distribution over logarithm of number
of samples per class in the train (left) and test (right) datasets in LFW.

Asian Black Indian White

Train 8.43 4.17 1.71 85.70
Test 6.27 4.61 1.79 87.33

Table 5: Summary statistics for LFW Race The percentage of the dataset constituted by each Race in
LFW, in the train dataset and test dataset, respectively.

tested as a downstream classifier but showed poor performance. The impact of varying imbalance
in the manually class imbalanced CARS196 experiments with all tested downstream classifiers is
displayed in Table 8. Additional results for benchmarking of further fairness improvement methods
in downstream classification of "imbalanced" embeddings (aside from naive use of balanced datasets)
are shown in Table 8.


-----

Overall
BalancedContrastive · DistanceImbalanced BalancedMargin · DistanceImbalanced BalancedMargin · Semi-hardImbalanced

LR Recall@1AccuracyPrecisionRecallUNMIKL 000000......861878877909433876 ± ± ± ± ± ± 0 0 0 0 0 0......003002002003004002 000000......84884887945784683 ± ± ± ± ± ± 0 0 0 0 0 0......005006007003008006 000000......85488389409687988 ± ± ± ± ± ± 0 0 0 0 0 0......002002002003002002 000000......81986186486709186 ± ± ± ± ± ± 0 0 0 0 0 0......004008004004005002 000000......8588761338568386 ± ± ± ± ± ± 0 0 0 0 0 0......002005005004004005 000000......853811861856133852 ± ± ± ± ± ± 0 0 0 0 0 0......005006007005003005

RF AccuracyPrecisionRecall 000...855859855 ± ± ± 0 0 0...002003003 000...832835831 ± ± ± 0 0 0...006006006 000...81681582 ± ± ± 0 0 0...003003003 000...758757763 ± ± ± 0 0 0...01101101 000...819822817 ± ± ± 0 0 0...005005005 000...79478879 ± ± ± 0 0 0...006006005

SVM AccuracyPrecisionRecall 000...876875874 ± ± ± 0 0 0...002002002 000...85285585 ± ± ± 0 0 0...006006007 000...882888881 ± ± ± 0 0 0...002002002 000...863875863 ± ± ± 0 0 0...004003004 000...863867863 ± ± ± 0 0 0...005005005 000...8678686 ± ± ± 0 0 0...004004004

Overall
BalancedMultisimilarityImbalanced BalancedProxy-NCAImbalanced BalancedTriplet · DistanceImbalanced BalancedTriplet · Semi-hardImbalanced


UPSTREAM
EMBEDDING

DOWNSTREAM
CLASSIFICATION

UPSTREAM
EMBEDDING


LR Recall@1AccuracyPrecisionRecallUNMIKL 000000......858886889898151885 ± ± ± ± ± ± 0 0 0 0 0 0......002001001003002001 000000......839875878881155873 ± ± ± ± ± ± 0 0 0 0 0 0......005005004004003005 000000......887898915901083898 ± ± ± ± ± ± 0 0 0 0 0 0......001003002003001003 000000......85887787909487689 ± ± ± ± ± ± 0 0 0 0 0 0......005005005005003005 000000......866885903888304883 ± ± ± ± ± ± 0 0 0 0 0 0......003002001002003002 000000......84887288487429387 ± ± ± ± ± ± 0 0 0 0 0 0......004006004005003003 000000......794828849829397825 ± ± ± ± ± ± 0 0 0 0 0 0......003004001003009003 000000......778818834821382816 ± ± ± ± ± ± 0 0 0 0 0 0......006005007005007005

RF AccuracyPrecisionRecall 000...825831824 ± ± ± 0 0 0...004004004 000...794797793 ± ± ± 0 0 0...005006005 000...852857852 ± ± ± 0 0 0...003003003 000...823825823 ± ± ± 0 0 0...007008008 000...858861857 ± ± ± 0 0 0...003003003 000...836838835 ± ± ± 0 0 0...004004004 000...808811807 ± ± ± 0 0 0...004004004 000...792795791 ± ± ± 0 0 0...008008008

SVM AccuracyPrecisionRecall 000...888893887 ± ± ± 0 0 0...001001001 000...876886876 ± ± ± 0 0 0...004004004 000...894902894 ± ± ± 0 0 0...002001002 000...871887871 ± ± ± 0 0 0...003003003 000...887892886 ± ± ± 0 0 0...002002003 000...878884877 ± ± ± 0 0 0...004004003 000...835839834 ± ± ± 0 0 0...003003003 000...83482983 ± ± ± 0 0 0...005005005


DOWNSTREAM
CLASSIFICATION


Table 6: Overall results on CARS196. Metrics over entire test dataset in representation space and
downstream classification (LR, RF, and SVM) over 10 seed in manually class imbalanced experiments
(Imbalanced) and control experiments (Balanced) for CARS196.

Subgroup Gap
BalancedContrastive · DistanceImbalanced BalancedMargin · DistanceImbalanced BalancedMargin · Semi-hardImbalanced


LR Recall@1AccuracyPrecisionRecallUNMIKL _−−000000....861002336351..013093 ± ± ± ± ± ± 0 0 0 0 0 0....003003004004..004004_ 000000......14710640701143983 ± ± ± ± ± ± 0 0 0 0 0 0......005023013013011016 _−−000000....854003355368..016033 ± ± ± ± ± ± 0 0 0 0 0 0....002003008008..005002_ 000000......8191244024261201 ± ± ± ± ± ± 0 0 0 0 0 0......013003008014013015 _−−000000....004353367..83018038 ± ± ± ± ± ± 0 0 0 0 0 0....002007008008..006006_ 00000.....11540301243111 ± ± ± ± ± 0 0 0 0 0.....016018013005014

RF AccuracyPrecisionRecall 000...001358373 ± ± ± 0 0 0...006005005 000...115396409 ± ± ± 0 0 0...021013015 000...002374387 ± ± ± 0 0 0...004006006 000...315441502 ± ± ± 0 0 0...022013013 000...002362376 ± ± ± 0 0 0...008007008 000...231429481 ± ± ± 0 0 0...024014016

SVM AccuracyPrecisionRecall 000...00334733 ± ± ± 0 0 0...006004004 000...086332338 ± ± ± 0 0 0...022023027 000...00236335 ± ± ± 0 0 0...008003008 000...03928327 ± ± ± 0 0 0...027013024 000...002347361 ± ± ± 0 0 0...006011011 000...055328327 ± ± ± 0 0 0...018018018

Subgroup Gap
BalancedMultisimilarityImbalanced BalancedProxy-NCAImbalanced BalancedTriplet · DistanceImbalanced BalancedTriplet · Semi-hardImbalanced


UPSTREAM
EMBEDDING

DOWNSTREAM
CLASSIFICATION

UPSTREAM
EMBEDDING


LR Recall@1AccuracyPrecisionRecallUNMIKL _−−00000....0002858352365..01403 ± ± ± ± ± ± 0 0 0 0 0 0.....002002006006.003004_ 000000......117839396116418005 ± ± ± ± ± ± 0 0 0 0 0 0......018005014014004018 _−−000000....003887334347..011129 ± ± ± ± ± ± 0 0 0 0 0 0....001005006006..004002_ 000000......85815543814800546 ± ± ± ± ± ± 0 0 0 0 0 0......016005019012014005 _−−000000....866001343356..013047 ± ± ± ± ± ± 0 0 0 0 0 0....003004005005..001005_ 000000......848131404109431011 ± ± ± ± ± ± 0 0 0 0 0 0......006017016006011013 _−−000000....794004357371..018034 ± ± ± ± ± ± 0 0 0 0 0 0....003005004004..002013_ 000000......77839208302312421 ± ± ± ± ± ± 0 0 0 0 0 0.....016.00601501100901

RF AccuracyPrecisionRecall 000..378389.0 ± ± ± 0 0 0.006..005005 000...44148526 ± ± ± 0 0 0...023011009 00.0.379.390 ± ± ± 0 0 0..005.005005 000...221476458 ± ± ± 0 0 0...02201101 00..0361375.0 ± ± ± 0 0 0.004..006006 000...16842545 ± ± ± 0 0 0...009015008 000...003361374 ± ± ± 0 0 0...005004004 000...393426141 ± ± ± 0 0 0...01201202

SVM AccuracyPrecisionRecall 000...002349362 ± ± ± 0 0 0...002006005 000...047258267 ± ± ± 0 0 0...01503203 00..0335349.0 ± ± ± 0 0 0.003..006006 000...07627329 ± ± ± 0 0 0...026013029 00..0339353.0 ± ± ± 0 0 0.004..007007 000...063297295 ± ± ± 0 0 0...0180303 000...002354368 ± ± ± 0 0 0...005007006 000...364372073 ± ± ± 0 0 0...01501602


DOWNSTREAM
CLASSIFICATION


Table 7: Gap study on CARS196. Average gaps in representation space and downstream classification
(LR, RF, and SVM) over 10 seeds between minoritized and majoritized classes in manually class
imbalanced experiments (Imbalanced) and control experiments (Balanced) for CARS196.

C.2 CUB200

Additional results for all loss and batch mining strategies for the manually class imbalanced experiments and balanced controls for CUB200 are located in Tables 9 and 10. K-Means was also tested as
a downstream classifier but showed poor performance. The impact of varying imbalance in the manually class imbalanced experiments in the upstream embedding, and all tested downstream classifiers
is displayed in Table 9. Additional results for benchmarking of further fairness improvement methods


-----

Table 8: Benchmarking additional fairness improvement methods in downstream classification on
_CARS196 (Classes). Overall performance and subgroup gaps for Domain-Independent Training and_
Oversampling (Wang et al., 2020b) on CARS196 in class imbalanced experiments with upstream
embedding trained on imbalanced dataset.

(a) Domain-Independent Training


|METRIC ↓|Contr. (D)|Margin (D)|Margin (Sem.)|Msim.|ProxyNCA|Triplet (D)|Triplet (S)|
|---|---|---|---|---|---|---|---|


**Overall**

|ACCURACY PRECISION RECALL|0.812 ± 0.011 0.834 ± 0.009 0.811 ± 0.011|0.834 ± 0.009 0.861 ± 0.004 0.833 ± 0.009|0.820 ± 0.008 0.847 ± 0.004 0.818 ± 0.008|0.842 ± 0.008 0.872 ± 0.004 0.840 ± 0.008|0.869 ± 0.005 0.878 ± 0.005 0.869 ± 0.005|0.836 ± 0.007 0.865 ± 0.009 0.834 ± 0.008|0.742 ± 0.007 0.804 ± 0.008 0.740 ± 0.008|
|---|---|---|---|---|---|---|---|



**Gap**


|ACCURACY PRECISION RECALL|0.001 ± 0.027 0.304 ± 0.021 0.260 ± 0.024|0.010 ± 0.018 0.275 ± 0.021 0.218 ± 0.022|0.017 ± 0.021 0.313 ± 0.019 0.258 ± 0.018|0.018 ± 0.018 0.247 ± 0.027 0.182 ± 0.027|0.120 ± 0.018 0.398 ± 0.015 0.398 ± 0.018|0.022 ± 0.017 0.236 ± 0.022 0.175 ± 0.022|0.094 ± 0.021 0.289 ± 0.016 0.177 ± 0.016|
|---|---|---|---|---|---|---|---|


(b) Oversampling


|METRIC ↓|Contr. (D)|Margin (D)|Margin (Sem.)|Msim.|ProxyNCA|Triplet (D)|Triplet (S)|
|---|---|---|---|---|---|---|---|


**Overall**

|ACCURACY PRECISION RECALL|0.851 ± 0.007 0.854 ± 0.006 0.853 ± 0.006|0.862 ± 0.004 0.864 ± 0.004 0.862 ± 0.004|0.853 ± 0.006 0.855 ± 0.006 0.853 ± 0.006|0.875 ± 0.004 0.877 ± 0.004 0.875 ± 0.004|0.878 ± 0.004 0.880 ± 0.005 0.878 ± 0.004|0.875 ± 0.005 0.876 ± 0.004 0.875 ± 0.004|0.820 ± 0.005 0.822 ± 0.005 0.821 ± 0.005|
|---|---|---|---|---|---|---|---|



**Gap**


|ACCURACY PRECISION RECALL|0.128 ± 0.023 0.398 ± 0.012 0.423 ± 0.015|0.099 ± 0.014 0.386 ± 0.017 0.403 ± 0.017|0.102 ± 0.020 0.391 ± 0.014 0.414 ± 0.014|0.097 ± 0.019 0.383 ± 0.015 0.396 ± 0.019|0.136 ± 0.017 0.422 ± 0.014 0.436 ± 0.017|0.108 ± 0.018 0.387 ± 0.013 0.406 ± 0.015|0.109 ± 0.019 0.387 ± 0.011 0.413 ± 0.012|
|---|---|---|---|---|---|---|---|


LR


SVM


RF


0.5

0.4

0.3

0.2

0.1

50-50 40-60 30-70 20-80 10-90

Imbalance Percentage


0.5

0.4

0.3

0.2

0.1

|5|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
||||||
||||||
||||||



50-50 40-60 30-70 20-80 10-90

Imbalance Percentage


0.5

0.4

0.3

0.2

0.1

50-50 40-60 30-70 20-80 10-90

Imbalance Percentage


metric

Precision
Recall
Accuracy


Figure 8: Impact of varying imbalance between the minoritized and majoritized classes on various
downstream classifiers (RF, LR and SVM) in the manually class imbalanced CARS196 experiments.
(Note: the imbalance percentage 50 − 50 is equivalent to the balanced setting). Gaps increase for all
classifiers downstream with more imbalance introduced to the upstream training data.

in downstream classification of "imbalanced" embeddings (aside from naive use of balanced datasets)
are shown in Table 11. Benchmarking of fairness improvement methods in downstream classification
for bird color are shown in Table 12. Per-subgroup and overall results for CUB200 color experiments
with standard margin-distance and PARADE are displayed in Table 13.


C.3 CELEBA

Additional results for all loss and batch mining strategies for the CelebA dataset are located in
Tables 14 and 15. Additional PARADE results for subgroup gaps excluding Fitzpatrick Skintone VI
(as mentioned in Section 6.3) are located in Table 16.


-----

Overall
BalancedContrastive · DistanceImbalanced BalancedMargin · DistanceImbalanced BalancedMargin · Semi-hardImbalanced

LR Recall@1AccuracyPrecisionRecallUNMIKL 000000......87281581739781579 ± ± ± ± ± ± 0 0 0 0 0 0......002002002003002002 000000......7828598114498181 ± ± ± ± ± ± 0 0 0 0 0 0......005005005003005007 000000......786861815822076816 ± ± ± ± ± ± 0 0 0 0 0 0......003003002001001002 000000......85682783107682778 ± ± ± ± ± ± 0 0 0 0 0 0......005004006006001006 000000......77585680981511381 ± ± ± ± ± ± 0 0 0 0 0 0......004006003004004003 000000......76681882211381885 ± ± ± ± ± ± 0 0 0 0 0 0......005006005005002005

RF AccuracyPrecisionRecall 000...776785776 ± ± ± 0 0 0...002003002 000...787793787 ± ± ± 0 0 0...007006007 000...757762757 ± ± ± 0 0 0...003003003 000...735735737 ± ± ± 0 0 0...00900901 000...768774769 ± ± ± 0 0 0...002002002 000...758762758 ± ± ± 0 0 0...004004004

SVM AccuracyPrecisionRecall 000...813823813 ± ± ± 0 0 0...002003002 000...811821811 ± ± ± 0 0 0...007007007 000...82781181 ± ± ± 0 0 0...002002002 000...8438282 ± ± ± 0 0 0...007006005 000...808818808 ± ± ± 0 0 0...004003003 000...815829815 ± ± ± 0 0 0...004004004

Overall
BalancedMultisimilarityImbalanced BalancedProxy-NCAImbalanced BalancedTriplet · DistanceImbalanced BalancedTriplet · Semi-hardImbalanced


UPSTREAM
EMBEDDING

DOWNSTREAM
CLASSIFICATION

UPSTREAM
EMBEDDING


LR Recall@1AccuracyPrecisionRecallUNMIKL 000000......77981385713981482 ± ± ± ± ± ± 0 0 0 0 0 0......004006004003001004 000000......788833857146836833 ± ± ± ± ± ± 0 0 0 0 0 0......005004004002004004 000000......807824873056828824 ± ± ± ± ± ± 0 0 0 0 0 0......004003003001003003 00000...0..828073833828.868 ± ± ± ± ± ± 0 0 0 0 0 0..007....005005001005005 000000......7928662748268282 ± ± ± ± ± ± 0 0 0 0 0 0......001001003002005001 000000......795861828277833828 ± ± ± ± ± ± 0 0 0 0 0 0......007005005005006005 000000......761848802336808803 ± ± ± ± ± ± 0 0 0 0 0 0......004005003004003003 00000.....843806321811806 ± ± ± ± ± 0 0 0 0 0.....004004007004005

RF AccuracyPrecisionRecall 000...75475476 ± ± ± 0 0 0...004003003 000...754755755 ± ± ± 0 0 0...005005005 000...761768762 ± ± ± 0 0 0...006007006 000...768774768 ± ± ± 0 0 0...007006007 000...786794786 ± ± ± 0 0 0...004004004 000...789792789 ± ± ± 0 0 0...008009008 000...776782777 ± ± ± 0 0 0...003003003 000...774778774 ± ± ± 0 0 0...007006007

SVM AccuracyPrecisionRecall 000...812825812 ± ± ± 0 0 0...002003002 000...828848828 ± ± ± 0 0 0...006005006 000...818834819 ± ± ± 0 0 0...002002002 000...816852817 ± ± ± 0 0 0...005004005 000...819829819 ± ± ± 0 0 0...002003002 000...84583183 ± ± ± 0 0 0...005004005 000...798806798 ± ± ± 0 0 0...001002001 000...808816808 ± ± ± 0 0 0...005005005


DOWNSTREAM
CLASSIFICATION


Table 9: Overall results on CUB200. Metrics over entire test dataset in representation space and
downstream classification (LR, RF, and SVM) over 10 seed in manually class imbalanced experiments
(Imbalanced) and control experiments (Balanced) for CUB200.

Subgroup Gap
BalancedContrastive · DistanceImbalanced BalancedMargin · DistanceImbalanced BalancedMargin · Semi-hardImbalanced


LR Recall@1AccuracyPrecisionRecallUNMIKL _−−000000....011014333354..009112 ± ± ± ± ± ± 0 0 0 0 0 0....004004003004..002004_ 000000......168181109417004462 ± ± ± ± ± ± 0 0 0 0 0 0......028029015012011016 _−−000000....008008337356..008043 ± ± ± ± ± ± 0 0 0 0 0 0....005003005005..005002_ 000000.....212131112424.390 ± ± ± ± ± ± 0 0 0 0 0 0..002....014029027012018 _−−00000...0.009331351.01.00905 ± ± ± ± ± ± 0 0 0 0 0 0....008.006006007.004003_ 000000......18713109239300243 ± ± ± ± ± ± 0 0 0 0 0 0......016031031017004015

RF AccuracyPrecisionRecall 000...013339359 ± ± ± 0 0 0...004007006 000...121386391 ± ± ± 0 0 0...026014015 000...009347365 ± ± ± 0 0 0...005006006 000...325428495 ± ± ± 0 0 0...03501101 000...34236201 ± ± ± 0 0 0...006007007 000...255418478 ± ± ± 0 0 0...031014014

SVM AccuracyPrecisionRecall 000...014326345 ± ± ± 0 0 0...003008007 000...10636236 ± ± ± 0 0 0...021032024 000...009332348 ± ± ± 0 0 0...004008008 000...043301278 ± ± ± 0 0 0...028023027 000...009329348 ± ± ± 0 0 0...006006006 000...058329323 ± ± ± 0 0 0...02901702

Subgroup Gap
BalancedMultisimilarityImbalanced BalancedProxy-NCAImbalanced BalancedTriplet · DistanceImbalanced BalancedTriplet · Semi-hardImbalanced


UPSTREAM
EMBEDDING

DOWNSTREAM
CLASSIFICATION

UPSTREAM
EMBEDDING


LR Recall@1AccuracyPrecisionRecallUNMIKL _−−000000....008009337356..008036 ± ± ± ± ± ± 0 0 0 0 0 0....009006008008..004002_ _−000000.....187141113391427.003 ± ± ± ± ± ± 0 0 0 0 0 0.....031032016016019.003_ _−−000000....007337356..01009131 ± ± ± ± ± ± 0 0 0 0 0 0....005005005006..004003_ _−000000.....169142428455.256012 ± ± ± ± ± ± 0 0 0 0 0 0.....027015018019.03005_ _−−000000....009011335355..007057 ± ± ± ± ± ± 0 0 0 0 0 0....004002005004..003006_ _−000000.....159179103459.41004 ± ± ± ± ± ± 0 0 0 0 0 0.....014031031016016.009_ _−−00000....0009011336357..05101 ± ± ± ± ± ± 0 0 0 0 0 0.....006005006006.006005_ 000000......168134082014384426 ± ± ± ± ± ± 0 0 0 0 0 0......036031016011014016

RF AccuracyPrecisionRecall 000...009348365 ± ± ± 0 0 0...006006005 000...28242848 ± ± ± 0 0 0...012034011 000...009355372 ± ± ± 0 0 0...0090101 000...214436443 ± ± ± 0 0 0...02600901 000...34736401 ± ± ± 0 0 0...005006007 000...19240944 ± ± ± 0 0 0...014035014 000...01134136 ± ± ± 0 0 0...006004007 000...393437175 ± ± ± 0 0 0...01301503

SVM AccuracyPrecisionRecall 000...009335352 ± ± ± 0 0 0...003005004 000...048284307 ± ± ± 0 0 0...02702302 000...00934733 ± ± ± 0 0 0...006003004 000...063347299 ± ± ± 0 0 0...026022022 000...011334353 ± ± ± 0 0 0...003006006 000...071324315 ± ± ± 0 0 0...027014017 000...012334355 ± ± ± 0 0 0...002005005 000...35608234 ± ± ± 0 0 0..012.01403


DOWNSTREAM
CLASSIFICATION


Table 10: Gap study on CUB200. Average gaps in representation space and downstream classification
(LR, RF, and SVM) over 10 seeds between minoritized and majoritized classes in manually class
imbalanced experiments (Imbalanced) and control experiments (Balanced) for CUB200.

C.4 LFW

Additional results for all loss and batch mining strategies for the LFW dataset are located in Tables 18
and 19. Per-subgroup results for LFW to demonstrate worst-group performance for the “White" subgroup (as mentioned in Section 6.3) are located in Table 20. Benchmarking of fairness improvement
methods in downstream classification for bird color are shown in Table 21.


-----

Table 11: Benchmarking additional fairness improvement methods in downstream classification on
_CUB200 (Classes). Overall performance and subgroup gaps for Domain-Independent Training and_
Oversampling (Wang et al., 2020b) on CUB200-2011 in class imbalanced experiments with upstream
embedding trained on imbalanced dataset.

(a) Domain-Independent Training

|METRIC ↓|Contr. (D)|Margin (D)|Margin (Sem.)|Msim.|ProxyNCA|Triplet (D)|Triplet (S)|
|---|---|---|---|---|---|---|---|



**Overall**


|ACCURACY PRECISION RECALL|0.782 ± 0.008 0.805 ± 0.011 0.782 ± 0.008|0.809 ± 0.004 0.840 ± 0.006 0.809 ± 0.004|0.794 ± 0.005 0.827 ± 0.005 0.795 ± 0.005|0.805 ± 0.004 0.847 ± 0.005 0.805 ± 0.004|0.823 ± 0.006 0.836 ± 0.006 0.823 ± 0.006|0.798 ± 0.005 0.842 ± 0.005 0.798 ± 0.004|0.749 ± 0.006 0.806 ± 0.006 0.749 ± 0.005|
|---|---|---|---|---|---|---|---|


**Gap**

|ACCURACY PRECISION RECALL|0.034 ± 0.033 0.340 ± 0.024 0.308 ± 0.027|0.003 ± 0.030 0.304 ± 0.031 0.253 ± 0.035|0.014 ± 0.031 0.301 ± 0.022 0.249 ± 0.025|0.024 ± 0.031 0.264 ± 0.028 0.189 ± 0.031|0.140 ± 0.030 0.410 ± 0.022 0.415 ± 0.024|0.024 ± 0.030 0.262 ± 0.032 0.188 ± 0.036|0.077 ± 0.031 0.270 ± 0.018 0.177 ± 0.021|
|---|---|---|---|---|---|---|---|



(b) Oversampling


|METRIC ↓|Contr. (D)|Margin (D)|Margin (Sem.)|Msim.|ProxyNCA|Triplet (D)|Triplet (S)|
|---|---|---|---|---|---|---|---|


**Overall**

|ACCURACY PRECISION RECALL|0.811 ± 0.005 0.814 ± 0.005 0.812 ± 0.005|0.828 ± 0.005 0.831 ± 0.005 0.828 ± 0.005|0.818 ± 0.004 0.822 ± 0.004 0.819 ± 0.004|0.832 ± 0.005 0.835 ± 0.005 0.833 ± 0.005|0.828 ± 0.005 0.833 ± 0.005 0.828 ± 0.005|0.829 ± 0.006 0.832 ± 0.006 0.829 ± 0.006|0.806 ± 0.005 0.811 ± 0.004 0.807 ± 0.005|
|---|---|---|---|---|---|---|---|



**Gap**

|ACCURACY PRECISION RECALL|0.182 ± 0.027 0.421 ± 0.011 0.464 ± 0.015|0.131 ± 0.028 0.386 ± 0.015 0.420 ± 0.018|0.129 ± 0.030 0.391 ± 0.016 0.428 ± 0.018|0.142 ± 0.035 0.391 ± 0.016 0.426 ± 0.020|0.170 ± 0.026 0.428 ± 0.016 0.455 ± 0.017|0.177 ± 0.032 0.409 ± 0.015 0.457 ± 0.017|0.135 ± 0.032 0.385 ± 0.013 0.427 ± 0.015|
|---|---|---|---|---|---|---|---|



Table 12: Benchmarking additional fairness improvement methods in downstream classification on
_CUB200 (Color). Overall performance and subgroup gaps for Domain-Independent Training and_
Oversampling (Wang et al., 2020b) on CUB200-2011 in bird color experiments.


(a) Domain-Independent Training

|METRIC ↓|Margin (D)|
|---|---|



**Overall**


(b) Oversampling

|METRIC ↓|Margin (D)|
|---|---|



**Overall**


|ACCURACY PRECISION RECALL|0.490 ± 0.005 0.896 ± 0.003 0.489 ± 0.006|
|---|---|


**Gap**


|ACCURACY PRECISION RECALL|0.802 ± 0.002 0.816 ± 0.002 0.802 ± 0.002|
|---|---|


**Gap**

|ACCURACY PRECISION RECALL|0.426 ± 0.017 0.185 ± 0.108 0.353 ± 0.108|
|---|---|

|ACCURACY PRECISION RECALL|0.143 ± 0.019 0.323 ± 0.063 0.348 ± 0.064|
|---|---|


C.5 EXPLORATION OF FAIRNESS - UTILITY TRADEOFF AND VARYING HYPERPARAMETERS IN
PARADE

We vary αSA and ρ in the PARADE objective to explore the relationship between the overall
performance, subgroup gap, and worst-group performance in PARADE. As stated in the main
paper, we optimize αSA and ρ via worst-group performance. Results of this analysis are displayed in
Figure 12. We use our exploration to expound on how to optimize for αSA and ρ. As seen in Figure 12,
a clear trend that inversely relates overall performance, and fairness as measured by subgroup gap and
worst-group performance is seen for the uniformity metric, UKL over the grid of αSA and ρ values
(Note that higher values of UKL correspond to worse performance). Recall@1 and NMI demonstrate
noisier relationships between overall performance and fairness; and several αSA, ρ choices appear to
select an optimal tradeoff. In Figure 12, for Recall@1, we observe that at the location αSA = 0.1,
_ρ = 500. in the optimization grid, PARADE reaches peak overall performance and fairness (measured_
by low subgroup gap and high performance for the worst-performing subgroup) simultaneously. Thus,
we could conclude that this choice of αSA and ρ represents an optimal tradeoff for utility and fairness


-----

**Color** overall overall black black blue blue brown brown

**Method** Parade Margin (D) Parade Margin (D) Parade Margin (D) Parade Margin (D)

Recall@1AccuracyPrecisionRecallNMIUKL 000000......785819812860071812 ± ± ± ± ± ± 0 0 0 0 0 0......003003003001002003 000000......786822816861076815 ± ± ± ± ± ± 0 0 0 0 0 0......003001003002001002 000000......780403375832164790 ± ± ± ± ± ± 0 0 0 0 0 0......006007007005002009 000000......777412837384153798 ± ± ± ± ± ± 0 0 0 0 0 0......005016004014003004 000000......837399840367296867 ± ± ± ± ± ± 0 0 0 0 0 0......053016025052004009 000000......401841863366275877 ± ± ± ± ± ± 0 0 0 0 0 0......022010026022007011 000000......396773831357156804 ± ± ± ± ± ± 0 0 0 0 0 0......005005013008004001 000000......390779838351148812 ± ± ± ± ± ± 0 0 0 0 0 0......016008007016003006

**Color** buff buff green green grey grey iridescent iridescent olive

**Method** Parade Margin (D) Parade Margin (D) Parade Margin (D) Parade Margin (D) Parade


Recall@1AccuracyPrecisionRecallNMIUKL 000000......787250209789824349 ± ± ± ± ± ± 0 0 0 0 0 0......008006005005003003 000000......792253212808824343 ± ± ± ± ± ± 0 0 0 0 0 0......006013012015004011 _−111010.....000000000000262.000 ± ± ± ± ± ± 0 0 0 0 0 0.....000000000000024.000_ _−111100.....000000000000252.000 ± ± ± ± ± ± 0 0 0 0 0 0.....000000000000021.000_ 000000......337751853292790142 ± ± ± ± ± ± 0 0 0 0 0 0......015014013009007005 000000......338754297849796137 ± ± ± ± ± ± 0 0 0 0 0 0......010004022021006002 111110......000000000000369000 ± ± ± ± ± ± 0 0 0 0 0 0......000000000000000013 111010......000000000800000318 ± ± ± ± ± ± 0 0 0 0 0 0......000000000447000017 _−000000.....289589173600164.000 ± ± ± ± ± ± 0 0 0 0 0 0.....077038048033008.000_

**Color** olive orange orange red red white white yellow yellow

**Method** Margin (D) Parade Margin (D) Parade Margin (D) Parade Margin (D) Parade Margin (D)

Recall@1AccuracyPrecisionRecallNMIUKL 000000......620240000160151660 ± ± ± ± ± ± 0 0 0 0 0 0......063087000049005060 000000......302839701261295867 ± ± ± ± ± ± 0 0 0 0 0 0......010027060024004017 000000......863303657263277860 ± ± ± ± ± ± 0 0 0 0 0 0......040079036076009028 000000......917555839542445923 ± ± ± ± ± ± 0 0 0 0 0 0......018052032053003017 000000......919559830544411927 ± ± ± ± ± ± 0 0 0 0 0 0......017048033049011009 000000......729387792357176752 ± ± ± ± ± ± 0 0 0 0 0 0......012018008018003004 000000......706380787342166737 ± ± ± ± ± ± 0 0 0 0 0 0......011012010011003002 000000......846503842481215879 ± ± ± ± ± ± 0 0 0 0 0 0......003012011011005001 000000......862530864509202884 ± ± ± ± ± ± 0 0 0 0 0 0......012022005022004008

Table 13: Absolute performance for all CUB200 subgroups. Metrics over each bird color subgroup in
the CUB200 test dataset respectively, in representation space and downstream classification (logistic
regressor) over 3 seeds for standard methods and PARADE in CUB200.

LR SVM RF

0.5 0.5 0.5

0.4 0.4 0.4

Gap 0.3 Gap 0.3 Gap 0.3 metric

Precision

0.2 0.2 0.2 Recall

Accuracy

0.1 0.1 0.1

50-50 40-60 30-70 20-80 10-90 50-50 40-60 30-70 20-80 10-90 50-50 40-60 30-70 20-80 10-90

Imbalance Percentage Imbalance Percentage Imbalance Percentage


Figure 9: Impact of varying imbalance between the minoritized and majoritized classes on various
downstream classifiers (RF, LR and SVM) in the manually class imbalanced CUB200 experiments.
(Note: the imbalance percentage 50 − 50 is equivalent to the balanced setting). Gaps increase for all
classifiers downstream with more imbalance introduced to the upstream training data.

Overall
Arcface Margin · Distance N-Pair · N-Pair
PARADE Standard PARADE Standard Standard


Recall@1 0.897 0.002 0.888 0.002 0.885 0.002 0.922 0.001 0.11 0.002

UPSTREAM _±_ _±_ _±_ _±_ _±_
EMBEDDING NMI 0.91 ± 0.0 0.902 ± 0.001 0.901 ± 0.003 0.929 ± 0.0 0.61 ± 0.0

_UKL_ 0.019 0.001 0.017 0.0 0.336 0.013 0.237 0.006 2.595 0.055
_±_ _±_ _±_ _±_ _±_

Accuracy 0.891 0.0 0.89 0.001 0.692 0.004 0.831 0.002 0.017 0.0
DOWNSTREAM _±_ _±_ _±_ _±_ _±_
CLASSIFICATION LR PrecisionRecall 00..72174 ± ± 0 0..00 00..721741 ± ± 0 0..001001 00..54655 ± ± 0 0..003003 00..652674 ± ± 0 0..003003 00..004011 ± ± 0 0..00


Table 14: Overall results on CelebA. Metrics over entire test dataset in representation space and
downstream classification (logistic regressor) over 3 seeds for standard methods and PARADE in
CelebA.

in PARADE as measured by Recall@1. By the other displayed metrics, we see that αSA = 0.1,
_ρ = 500. demonstrates a reasonable utility-fairness tradeoff. Therefore, the choice of αSA = 0.1,_
_ρ = 500. would be optimal for PARADE in CUB200 bird color setting. Note that the choice of_
where to operate within this trade-off should depend on the application that is being targeted. For


-----

Subgroup Gap
Arcface Margin · Distance N-Pair · N-Pair
PARADE Standard PARADE Standard Standard

Recall@1 0.135 0.008 0.128 0.003 0.085 0.009 0.122 0.005 0.023 0.013

EUMBEDDINGPSTREAM _UNMIKL_ _−−00..003054 ± ± ± 0 0..004003_ _−−00..05201 ± ± ± 0 0..002003_ _−−00..01204 ± ± ± 0 0..011003_ _−−00..00203 ± ± ± 0 0..007003_ _−−−00..102015 ± ± ± 0 0..002038_

Accuracy 0.068 0.002 0.069 0.002 0.131 0.006 0.082 0.005 0.006 0.002
CDLASSIFICATIONOWNSTREAM LR PrecisionRecall 00..087084 ± ± ± 0 0..003002 00..087083 ± ± ± 0 0..004003 00..146141 ± ± ± 0 0..006007 0.0098.1 ± ± ± 0 0.007.007 00..001002 ± ± ± 0 0..001001


Table 15: Gap study on CelebA. Average gaps in representation space and downstream classification
(logistic regressor) over 3 seeds between minoritized and majoritized classes (Fitzpatrick Skintone)
for standard methods and PARADE in CelebA.

Margin · Distance
PARADE Standard


Recall@1 _−0.035 ± 0.006_ 0.005 ± 0.004
NMI _−0.004 ± 0.003_ 0.004 ± 0.002
_UKL_ 0.04 0.011 0.084 0.006
_±_ _±_

Precision 0.021 ± 0.006 0.039 ± 0.002
Recall 0.018 ± 0.006 0.029 ± 0.002
Accuracy 0.011 ± 0.005 0.018 ± 0.002


UPSTREAM
EMBEDDING

DOWNSTREAM
CLASSIFICATION (LR)


Table 16: Gap study on CelebA excluding Fitzpatrick Skintone VI. Average gaps in representation
space and downstream classification (logistic regressor) over 3 seeds between minoritized and
majoritized classes (Fitzpatrick Skintone) where the darkest skintone (VI) is excluded for standard
methods and PARADE in CelebA.

**Skintones** Overall Overall Skintone 1 Skintone 1 Skintone 2 Skintone 2 Skintone 3

**Method** Parade Margin (D) Parade Margin (D) Parade Margin (D) Parade

Recall@1AccuracyPrecisionNMI@1RecallUKL 000000......885901336550546692 ± ± ± ± ± ± 0 0 0 0 0 0......002003013003003004 000000......922929237652674831 ± ± ± ± ± ± 0 0 0 0 0 0......001000006003003002 000000......738933436398421578 ± ± ± ± ± ± 0 0 0 0 0 0......005007020011014009 000000......858961419639656783 ± ± ± ± ± ± 0 0 0 0 0 0......011000002005005004 000000......887923350566604707 ± ± ± ± ± ± 0 0 0 0 0 0......004004014003003004 000000......930947260696739843 ± ± ± ± ± ± 0 0 0 0 0 0......001001004002003002 000000......907927350558578716 ± ± ± ± ± ± 0 0 0 0 0 0......001002012003003004

**Skintones** Skintone 3 Skintone 4 Skintone 4 Skintone 5 Skintone 5 Skintone 6 Skintone 6

**Method** Margin (D) Parade Margin (D) Parade Margin (D) Parade Margin (D)

Recall@1AccuracyPrecisionNMI@1RecallUKL 000000......937947241672708842 ± ± ± ± ± ± 0 0 0 0 0 0......003001006003003003 000000......850927339471518632 ± ± ± ± ± ± 0 0 0 0 0 0......002002012005005005 000000......893946240644695798 ± ± ± ± ± ± 0 0 0 0 0 0......003000008003002002 000000......785943371355386545 ± ± ± ± ± ± 0 0 0 0 0 0......017002012011011009 000000......838957288570602747 ± ± ± ± ± ± 0 0 0 0 0 0......004004013001001002 000000......642948547258275430 ± ± ± ± ± ± 0 0 0 0 0 0......018004010005005010 000000......628960483492511678 ± ± ± ± ± ± 0 0 0 0 0 0......006009014021019015

Table 17: Absolute performance for all CelebA subgroups. Metrics over each Fitzpatrick Skintone
subgroup in the CelebA test dataset respectively, in representation space and downstream classification
(logistic regressor) over 3 seeds for standard methods and PARADE in CelebA.

example, here we use Recall@1 to determine the optimal choice of hyperparameters and validate
with the other two considered metrics. However, for LFW, which has a high population of singleton
classes (see Figure 7), NMI would be a better metric to use for selecting optimal point.

D IMPLEMENTATION DETAILS

D.1 DATASET ATTRIBUTE INFORMATION

Dataset manipulation for the CARS196 and CUB200 manually class imbalanced experimentsis
explained in Section 3.3.


-----

Sensitive Target class

II
III
IV
V
VI
I


Figure 10: A t-SNE (Maaten & Hinton, 2008) visualization of the two distinct PARADE embeddings
for Fitzpatrick Skintone CelebA experiments: the sensitive attribute embedding (left) and the class
label embedding (right).

Overall
Arcface Margin · Distance N-Pair · N-Pair
PARADE Standard PARADE Standard Standard


Recall@1 0.268 0.01 0.306 0.008 0.329 0.002 0.381 0.004 0.187 0.006

UPSTREAM _±_ _±_ _±_ _±_ _±_
EMBEDDING NMI 0.849 ± 0.005 0.859 ± 0.002 0.865 ± 0.001 0.869 ± 0.001 0.854 ± 0.001

_UKL_ 0.118 0.021 0.089 0.014 0.129 0.001 0.103 0.001 1.815 0.011
_±_ _±_ _±_ _±_ _±_

Accuracy 0.8 0.0 0.804 0.017 0.762 0.002 0.8 0.003 0.887 0.002
DOWNSTREAM _±_ _±_ _±_ _±_ _±_
CLASSIFICATION RF Precision 0.789 ± 0.003 0.793 ± 0.016 0.767 ± 0.001 0.788 ± 0.004 0.878 ± 0.003
Recall 0.827 ± 0.0 0.831 ± 0.015 0.801 ± 0.003 0.823 ± 0.003 0.913 ± 0.002


Table 18: Overall results on LFW. Metrics over entire test dataset in representation space and
downstream classification (random forest) over 3 seeds for standard methods and PARADE in LFW.
**Note: Due to the number of singleton classes in LFW, Recall@1 is not considered a good metric of**
performance for this dataset.

Subgroup Gap
Arcface Margin · Distance N-Pair · N-Pair
PARADE Standard PARADE Standard Standard

Recall@1 0.039 0.017 0.061 0.017 0.075 0.014 0.068 0.013 0.054 0.01

UPSTREAM _±_ _±_ _±_ _±_ _±_
EMBEDDING NMI 0.048 ± 0.011 0.057 ± 0.003 0.041 ± 0.003 0.048 ± 0.003 0.048 ± 0.003

_UKL_ 0.176 0.019 0.157 0.011 0.163 0.003 0.165 0.005 0.357 0.012
_±_ _±_ _±_ _±_ _±_

Accuracy 0.04 0.01 0.038 0.012 0.049 0.005 0.038 0.005 0.025 0.004
DOWNSTREAM _±_ _±_ _±_ _±_ _±_
CLASSIFICATION RF Precision 0.036 ± 0.018 0.041 ± 0.014 0.04 ± 0.005 0.037 ± 0.007 0.025 ± 0.007
Recall 0.066 ± 0.017 0.076 ± 0.015 0.066 ± 0.006 0.071 ± 0.007 0.041 ± 0.006


Table 19: Gap study on LFW. Average gaps in representation space and downstream classification
(random forest) over 3 seeds between minoritized and majoritized classes (Race) for standard methods
and PARADE in LFW.

Asian Black Indian White
PARADE Standard PARADE StandardMargin · DistancePARADE Standard PARADE Standard


CDLASSIFICATIONEUOWNSTREAMMBEDDINGPSTREAM RF Recall@1AccuracyPrecisionRecallUNMIKL 000000......26271589430471381 ± ± ± ± ± ± 0 0 0 0 0 0......007028003012003013 000000......8289147262653172 ± ± ± ± ± ± 0 0 0 0 0 0......014014008005013003 000000......289827858743456751 ± ± ± ± ± ± 0 0 0 0 0 0......011005001008009008 000000......331853882759417758 ± ± ± ± ± ± 0 0 0 0 0 0......01600700501401301 000000......238772948664141657 ± ± ± ± ± ± 0 0 0 0 0 0......027011007006002008 000000......325814951133702711 ± ± ± ± ± ± 0 0 0 0 0 0......03201200700501101 000000......338754862747137773 ± ± ± ± ± ± 0 0 0 0 0 0......004002001002002002 00000..0...794769107798.39868 ± ± ± ± ± ± 0 0 0 0 0. 0....004003004001003.0


Table 20: Absolute performance for all LFW subgroups. Metrics over each Race subgroup in the
LFW test dataset respectively, in representation space and downstream classification (random forest)
over 3 seeds for standard methods and PARADE in LFW.

For CUB200 bird color experiments, we utilized the labeled bird color attributes from Wah et al.
(2011). Each image can have multiple “primary color" labels. Therefore, we take the mode over


-----

Table 21: Benchmarking additional fairness improvement methods in downstream classification
_on LFW. Overall performance and subgroup gaps for Domain-Independent Training and Oversam-_
pling (Wang et al., 2020b) on LFW with Race attribute.


(a) Domain-Independent Training

|METRIC ↓|ArcFace|Margin (D)|N-Pair|
|---|---|---|---|



**Overall**


(b) Oversampling

|METRIC ↓|ArcFace|Margin (D)|N-Pair|
|---|---|---|---|



**Overall**


|ACCURACY PRECISION RECALL|0.759 ± 0.017 0.793 ± 0.010 0.799 ± 0.012|0.753 ± 0.002 0.786 ± 0.003 0.792 ± 0.003|0.861 ± 0.005 0.872 ± 0.003 0.889 ± 0.003|
|---|---|---|---|


**Gap**


|ACCURACY PRECISION RECALL|0.775 ± 0.017 0.771 ± 0.014 0.815 ± 0.014|0.767 ± 0.004 0.762 ± 0.002 0.807 ± 0.002|0.881 ± 0.004 0.873 ± 0.005 0.909 ± 0.004|
|---|---|---|---|


**Gap**

|ACCURACY PRECISION RECALL|0.093 ± 0.011 0.030 ± 0.011 0.040 ± 0.012|0.095 ± 0.006 0.020 ± 0.009 0.044 ± 0.008|0.029 ± 0.009 0.029 ± 0.010 0.026 ± 0.010|
|---|---|---|---|

|ACCURACY PRECISION RECALL|0.070 ± 0.012 0.020 ± 0.013 0.023 ± 0.013|0.072 ± 0.006 0.024 ± 0.009 0.031 ± 0.011|0.032 ± 0.006 0.027 ± 0.009 0.036 ± 0.008|
|---|---|---|---|


Sensitive Target class

White
Indian
Black
Asian


Figure 11: A t-SNE (Maaten & Hinton, 2008) visualization of the two distinct PARADE embeddings
for Race LFW experiments: the sensitive attribute embedding (left) and the class label embedding
(right).

|Dataset|Protected Attribute|Protected Attribute Values|
|---|---|---|


|Col1|Col2|Black, Blue, Brown, Buff, Green, Grey, Iridescent|
|---|---|---|
|CUB200-2011|Color|Black, Blue, Brown, Buff, Green, Grey, Iridescent Olive, Orange, Red, White, Yellow|
|CelebA|Fitzpatrick Skintone Category|I, II, III, IV, V, VI|
|LFW|Race|Asian, Black, Indian, White|



Table 22: Summarizing attribute information. Protected attribute examined and associated values
taken by the protected attribute in each dataset analyzed w.r.t. a sensitive attribute in the main paper
(CUB200, CelebA, LFW).

all bird colors labeled for each image in order to determine a single bird color associated with the
image. For CelebA, we calculate the Fitzpatrick skintone based on the image pixel information for
each image. The calculation is described in Section D.2. For LFW, we construct the “Race" attribute
from labels of “White", “Black", “Asian," and “Indian" as labelled by Kumar et al. (2009). For each
of these attributes, the labelling provided by Kumar et al. (2009) has a float value, which we map
to binary values: the image is considered to have the attribute if the value is greater than 0, and the
image is considered to not have the attribute if the value is less than 0. Naturally, the labelling is not
necessarily correct for each image, as the confidence about the “Race" labelling can be quite low for
some images. We remove all images without at least one of these attributes, though we note that
these attributes do not encompass all races. Therefore, our analysis may not be relevant for other
races not labelled by Kumar et al. (2009).


-----

Figure 12: Exploring fairness-utility tradeoffs in PARADE on CUB200 over grid of αSA and ρ values.
Overall performance (left column), subgroup gap (middle column) and worst-group performance
(right column) over metrics Recall@1 (top row), NMI (middle row), and UKL (bottom row) in
PARADE on CUB200. αSA and ρ in PARADE objective (Section 4) varied from 0.1 to 0.9, and 1 to
3000, respectively.

D.2 FITZPATRICK SKINTONE CALCULATION

We follow the methods from Cheng et al. (2021) for calculation of Fitzpatrick Skintone based on
image pixel information. However, we calculate these values for CelebA, as opposed to CelebAHQ. As CelebA-HQ incorporates higher resolution images, but has fewer images, our process of
Fitzpatrick Skintone calculation on CelebA is slightly modified to account for lower resolution, and
differing image size.

In Cheng et al. (2021), two sample skin patches are selected from each image of CelebA-HQ to
determine the skintone. We select three sample skin patches, as we are forced to reduce the dimensions
of the patches to account for the smaller image size of CelebA. Additionally, we leverage facial
landmark attributes provided by CelebA Liu et al. (2015) in order to choose our sample patches.
Specifically, given the (x, y) landmarks for the left eye, right eye, and nose for each image, we choose
to sample square patches of size 20 × 20 (all 3 color channels are selected) with the following center
points:
(xleft eye, ynose)
(xright eye, ynose)
(xnose, ynose)
The first two center points are intended to capture the likely location of the left and right cheeks,
respectively, as these are likely located below each eye and adjacent to the nose. The last center
point is the nose. We note that this protected attribute generation is not perfect. In some cases, such
label generation can accidentally use aspects of the background, if, the individual’s face position
in the image is not facing forward. Also, extreme lighting can lead to misclassification of skintone.
Nonetheless, we believe the procedure provides a good approximation of Fitzpatrick skintone category,
but do not recommend these attribute labels for use outside of fairness analysis.


-----

The selected sample patches are converted to CIELab-space to retrieve the L (luminance) and b
(yellow) values. We then calculate the Mean Individual Typology Angle (ITA) value:


_L_ 50
_ITA = arctan_ _−_

_b_





[180][◦]
_×_ _π_


Table 23: Fitzpatrick Skin Tone Categories corresponding to Mean ITA values, information taken
from Cheng et al. (2021)

ITA Range Fitzpatrick Category Description

50 ≤ ITA I Extremely Light
40 ≤ ITA < 50 II Very Light
30 ≤ ITA < 40 III Light / Somewhat Light
20 ≤ ITA < 30 IV Dark / Somewhat Dark
10 ≤ ITA < 20 V Very Dark
ITA < 10 VI Extremely Dark

Based on the Mean ITA calculation, we classify each image into one of the 6 Fitzpatrick skintone
categories, as listed in Table 23. To calculate subgroup gaps, we calculate gaps between the mean
value over the 3 lightest Fitzpatrick skintones and the mean value over the 3 darkest Fitzpatrick
skintones.

D.3 TRAINING PARAMETERS

For CUB200 and CARS196, we did not perform hyperparameter search but followed reported
hyperparameters from Roth et al. (2020c) for best performance with an ImageNet Deng et al. (2009)
pretrained ResNet50 He et al. (2016) and frozen batch normalization layers. As detailed in Roth
et al. (2020c), we train for 150 epochs with embedding dimension 128, learning rate 0.00001
with no scheduler, and weight decay 0.0004. We train with a batch size of 128, with the Adam
optimizer Kingma & Ba (2015) over five seeds inclusive for the balance control datasets, and for
CUB200 color experiments; and seeds 0 − 9 for the manually class imbalanced experiments. For
training transforms, we normalize each image using color channel means (0.485, 0.456, 0.406) and
standard deviations (0.229, 0.224, 0.225), randomly crop the image and re-size to 224 × 224 and
horizontally flip with probability 0.5. For testing transforms, we normalize each image with the
aforementioned color channel means and standard deviations, resize to 256 × 256, and center crop to
224 × 224.

For CelebA and LFW, we performed hyperparameter search over the following hyperparameters:
**architectures: ResNet50 He et al. (2016), and SE-Net50 (both with and without frozen batch**
normalization layers); number of training epochs; learning rates; last linear layer learning rate
(differ from other layer learning rates); learning rate schedulers; embedding dimensions: 64, 128,
256; pre-training; image augmentations. We evaluated hyperparameter sets on a validation set we
cut from the typical training set (20% of training set), and chose the set of hyperparameters with
best recall@k score for CelebA and best NMI score for LFW. NMI is used for LFW due to the high
number of singleton classes present in the dataset (recall@1 is meaningless for singleton classes).

For CelebA, we train on the ResNet50 He et al. (2016) architecture with frozen batch normalization layers, for 125 epochs with learning rate 0.00001, and no scheduler, weight decay 0.0004,
Adam Kingma & Ba (2015) optimizer, and batch size of 128. For training transforms, we normalize
each image using color channel means (0.5, 0.5, 0.5) and standard deviations (0.5, 0.5, 0.5), resize to
256 × 256, center crop to 224 × 224 and horizontally flip with probability 0.5. For testing transforms,
we normalize each image with the aforementioned color channel means and standard deviations,
resize to 256 × 256, and center crop to 224 × 224. We average over runs with seeds 0 − 2, inclusive.

For LFW, we train on the ResNet50 He et al. (2016) architecture with frozen batch normalization
layers, for 125 epochs with initial learning rate 0.00001 for all model parameters except the last linear
layer, which has initial learning rate 0.0001, and a multi-step learning rate scheduler which reduces
the learning rate by a factor of 0.3 at epochs 50 and 100, weight decay 0.0004, Adam Kingma & Ba


-----

(2015) optimizer, and batch size of 64. For training transforms, we normalize each image using color
channel means (0.5, 0.5, 0.5) and standard deviations (0.5, 0.5, 0.5), resize to 256 × 256, center crop
to 224 × 224 and horizontally flip with probability 0.5. For testing transforms, we normalize each
image with the aforementioned color channel means and standard deviations, resize to 256 × 256,
and center crop to 224 × 224. We average over runs with seeds 0 − 2, inclusive.

For each dataset we chose a set of loss and batch mining strategies that have historically been used
for the relevant task, encompassing a broad range of methods, and / or achieved good performance.
However, for n-pair loss and sampling, good performance was not achieved for the facial datasets
despite use in the past for facial recognition Sohn (2016). For manually class imbalanced experiments
with CARS196 and CUB200 and the associated balanced controls, we used: margin loss / distanceweighted sampling, margin loss / semi-hard sampling, triplet loss / distance-weighted sampling, triplet
loss / semi-hard sampling, contrastive loss / distance-weighted sampling, multisimilarity loss, and
proxy-NCA loss. For the color experiments with CUB200, we used: margin loss / distance-weighted
sampling. For CelebA and LFW, we used: margin loss / distance-weighted sampling, arcface loss,
and n-pair loss and sampling. For all testing and evaluation experiments with PARADE, we used
margin loss and distance-weighted sampling, but PARADE can be used with any loss and mining
strategy.

**DML-specific parameters** Here we list the hyperparameters that we use for each evaluated loss
function and batch mining strategy, if applicable. Refer to A for explicit formulas associated with the
parameters here. We set γ = 0.2 in semi-hard mining. For distance-weighted mining, we set λ = 0.5
and clip the maximum distance to 1.4. In the triplet objective, we use γ = 0.2 for triplet loss. For
margin loss, the learning rate of the boundary β is set to 0.0005, with initial value 1.2 and triplet
margin γ = 0.2. For N-Pair uses embedding regularization parameter ν = 0.005. In Multisimilarity
loss, we use α = 2, β = 40, λ = 0.5 and ϵ = 0.1. Finally, for ArcFace, additive angular margin
penalty is set to γ = 0.5, while scaling parameter s = 16 and class centers are optimized with
learning rate 0.0005.

The two PARADE parameters, αSA and ρ, as described in Section 4, were optimized via worst-group
performance over a grid search. For CUB200, we set αSA = 0.3, ρ = 1500. For CelebA, we set
_αSA = 0.1, ρ = 1000. For LFW, we set αSA = 0.3, ρ = 100._

D.4 FAIRNESS EVALUATION

For each dataset, we calculate subgroup gaps between the majoritized and minoritized subgroup
(CARS196, CUB200 class, CelebA) or between the worst-performing subgroup and other subgroups
(LFW). In CUB200 color experiments, due to the large number of subgroups, we calculate the gap
between the top 6 performing subgroups and the bottom 6 performing subgroups (there are 12 total
subgroups).

**Upstream** In the upstream embedding tasks, in which we denote ϕ as the embedding function for
the learned model, and use A(x) to denote the value of the attribute A for data point x, we calculate
recall@1 for data samples in X with associated class label Y and attribute a ∈ _A as:_

Recall@k = 1 1 _∃x˜ ∈_ _NNk(x) : Y (˜x) = Y (x)_

_x_ _X : A(x) = a_ 0 else
_|{_ _∈_ _}|_ _{x∈XX:A(x)=a}_ 

Note here that the nearest neighbors function is computed with respect to all x ∈ _X, not exclusively_
_x ∈_ _X with attribute a ∈_ _A, but the input to the nearest neighbors function is exclusively {x ∈_ _X :_
_A(x) = a}. To calculate NMI, let C be the output of a clustering algorithm C on the entire dataset_
_X, i.e._ = C(X) and let denote the output of clustering algorithm C restricted to some subset
_C_ _C|S_
_S ⊂_ _X. The important note here is that the clustering algorithm is run over the entire dataset, but_
expresses the cluster labels only for _X. Then, we measure NMI for data samples in X with_
_C|S_ _S ⊂_
associated class label Y and attribute a ∈ _A as:_

_NMI =_ _I(Y ({x ∈_ _X : A(x) = a}); C|{x∈X:A(x)=a})_

_H(Y ({x ∈_ _X : A(x) = a})) + H(C|{x∈X:A(x)=a})_

We calculate UKL for data samples in X with attribute a ∈ _A as:_
_UKL(X) = DKL_ _UD, Sϕ({x∈X:A(x)=a})_
  


-----

where _ϕ(_ _x_ _X:A(x)=a_ ) denotes the singular values over ϕ( _x_ _X : A(x) = a_ ).
_S_ _{_ _∈_ _}_ _{_ _∈_ _}_

**Downstream** In the downstream tasks, for data samples in X with class label Y, and predictor _Y[ˆ],_
let Y (x) express the value of the ground-truth label for data sample x and let _Y[ˆ] (x) express the value_
of the predicted label. Then, we denote TPa[(][y][)] the number of true positives with attribute a _A:_
_∈_

_TPa[(][y][)]_ = _x_ _X : A(x) = a, Y (x) = y,_ _Y[ˆ] (x) = y_
_{_ _∈_ _}_

_FPa[(][y][)]_ the number of false positives with attribute a _A_
_∈_

_FPa[(][y][)]_ = _x_ _X : A(x) = a, Y (x) = y,_ _Y[ˆ] (x)_ = y
_{_ _∈_ _̸_ _}_

and FNa[(][y][)] the number of false negatives with attribute a _A:_
_∈_

_FNa[(][y][)]_ = _x_ _X : A(x) = a, Y (x)_ = y, _Y[ˆ] (x) = y_
_{_ _∈_ _̸_ _}_

for y ∈ _Y ._

We calculate macro-averaged recall for data samples in X with associated class label Y and attribute
_a ∈_ _A as:_

1 _TPa[(][y][)]_
Recall =

_|Y |_ _yX∈Y_ _TPa[(][y][)]_ + FNa[(][y][)]

where |Y | is the number of possible class labels, i.e. the size of the set of all possible values of Y . We
calculate macro-averaged precision for data samples in X with associated class label Y and attribute
_a ∈_ _A as:_

1 _TPa[(][y][)]_
Precision =

_|Y |_ _yX∈Y_ _TPa[(][y][)]_ + FPa[(][y][)]

We calculate accuracy for data samples in X with associated class label Y and attribute a ∈ _A as:_

_Y (x)_
Accuracy = _}|_

_[|{][x][ ∈]_ _[X][ :]x[ A][(][x]X[) =] : A[ a, Y](x) =[ (][x][) = ˆ] a_

_|{_ _∈_ _}|_

The subgroup gaps are then considered to be the difference between the metric value for the majori_tized subgroup and the metric value for the minoritized subgroup (CARS196, CUB200, CelebA);_
or between the metric value for each subgroup with better performance than the worst-performing
_subgroup and the metric value for the worst-performing subgroup (LFW). As stated in Section D.4,_
for CUB200 bird color experiments, the subgroup gaps were calculated between the top performing
50% of subgroups and bottom performing 50 of subgroups.


-----

