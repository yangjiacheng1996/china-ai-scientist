# SHALLOW AND DEEP NETWORKS ARE NEAR- OPTIMAL APPROXIMATORS OF KOROBOV FUNCTIONS


**Mo¨ıse Blanchard**
Operations Research Center
Massachusetts Institute of Technology
Cambridge, MA, 02139
moiseb@mit.edu


**Amine Bennouna**
Operations Research Center
Massachusetts Institute of Technology
Cambridge, MA, 02139
amineben@mit.edu

ABSTRACT


In this paper, we analyze the number of neurons and training parameters that a neural network needs to approximate multivariate functions of bounded second mixed
derivatives — Korobov functions. We prove upper bounds on these quantities for
shallow and deep neural networks, drastically lessening the curse of dimensionality. Our bounds hold for general activation functions, including ReLU. We further
prove that these bounds nearly match the minimal number of parameters any continuous function approximator needs to approximate Korobov functions, showing
that neural networks are near-optimal function approximators.

1 INTRODUCTION

Neural networks have known tremendous success in many applications such as computer vision and
pattern detection (Krizhevsky et al., 2017; Silver et al., 2016). A natural question is how to explain their practical success theoretically. Neural networks are shown to be universal (Hornik et al.,
1989): any Borel-measurable function can be approximated arbitrarily well by a neural network with
sufficient number of neurons. Furthermore, universality holds for as low as 1-hidden-layer neural
network with reasonable activation functions. However, these results do not specify the needed
number of neurons and parameters to train. If these numbers are unreasonably high, the universality
of neural networks would not explain their practical success.

We are interested in evaluating the number of neurons and training parameters needed to approximate a given function within ϵ with a neural network. An interesting question is how do these
numbers scale with ϵ and the dimensionality of the problem, i.e., the number of variables. Mhaskar
(1996) showed that any function of the Sobolev space of order r and dimension d can be approximated within ϵ with a 1-layer neural network with O(ϵ[−] _[d]r ) neurons and an infinitely differentiable_

activation function. This bound exhibits the curse of dimensionality: the number of neurons needed
for an ϵ−approximation scales exponentially in the dimension of the problem d. Thus, Mhaskar’s
bound raises the question of whether this curse is inherent to neural networks.

Towards answering this question, DeVore et al. (1989) proved that any continuous function approximator (see Section 5) that approximates all Sobolev functions of order r and dimension d within
_ϵ, needs at least Θ(ϵ[−]_ _[d]r ) parameters. This result meets Mhaskar’s bound and confirms that neural_

networks cannot escape the curse of dimensionality for the Sobolev space. A main question is then
for which set of functions can neural networks break this curse of dimensionality.

One way to circumvent the curse of dimensionality is to restrict considerably the considered space of
functions and focus on specific structures adapted to neural networks. For example, Mhaskar et al.
(2016) showed that compositional functions with regularity r can be approximated within ϵ with
deep neural networks with O(d · ϵ[−] _r[2] ) neurons. Other structural constraints have been considered_

for compositions of functions (Kohler & Krzy˙zak, 2016), piecewise smooth functions (Petersen &
Voigtlaender, 2018; Imaizumi & Fukumizu, 2019), or structures on the data space, e.g., lying on
a manifold (Mhaskar, 2010; Nakada & Imaizumi, 2019; Schmidt-Hieber, 2019). Approximation
bounds have also been obtained for the function approximation from data under smoothness constraints (Kohler & Krzy˙zak, 2005; Kohler & Mehnert, 2011) and specifically on mixed smooth Besov


-----

spaces which are known to circumvent the curse of dimensionality (Suzuki, 2018). Another example
is the class of Sobolev functions of order d/α and dimension d for which Mhaskar’s bound becomes
_O(ϵ[−][α]). Recently, Montanelli et al. (2019) considered bandlimited functions and showed that they_
can be approximated within ϵ by deep networks with depth O((log [1]ϵ [)][2][)][ and][ O][(][ϵ][−][2][(log][ 1]ϵ [)][2][)][ neu-]

rons. Weinan et al. (2019) showed that the closure of the space of 2-layer neural networks with
specific regularity (namely a restriction on the size of the network’s weights) is the Barron space.
They further show that Barron functions can be approximated within ϵ with 2-layer networks with
_O(ϵ[−][2]) neurons. Similar line of work restrict the function space with spectral conditions, to write_
functions as limits of shallow networks (Barron, 1994; Klusowski & Barron, 2016; 2018).

In this work, we are interested in more general and generic spaces of functions. Our space of interest is the space of multivariate functions of bounded second mixed derivatives, the Korobov space.
This space is included in the Sobolev space but is reasonably large and general. The Korobov space
presents two motivations. First, it is a natural candidate for a large and general space included in the
Sobolev space where numerical approximation methods can overcome the curse of dimensionality
to some extent (see Section 2.1). Second, Korobov spaces are practically useful for solving partial
differential equations (Korobov, 1959) and have been used for high-dimensional function approximation (Zenger & Hackbusch, 1991; Zenger, 1991). Recently, Montanelli & Du (2019) showed that
deep neural networks with depth O(log [1]ϵ [)][ and][ O][(][ϵ][−] 2[1] (log [1]ϵ [)] 3(d2−1) +1) neurons can approximate

Korobov functions within ϵ, lessening the curse of dimensionality for deep neural networks asymptotically in ϵ. While they used deep structures to prove their result, the question of whether shallow
neural networks also break the curse of dimensionality for the Korobov space remains open.

In this paper, we study deep and shallow neural network’s approximation power for the Korobov
space and make the following contributions:

**1. Representation power of shallow neural networks. We prove that any Korobov function**
can be approximated within ϵ with a 2-layer neural network with ReLU activation function with
_O(ϵ[−][1](log_ [1]ϵ [)] 3(d2−1) +1) neurons and O(ϵ− 2[1] (log [1]ϵ [)] 3(d2−1) ) training parameters (Theorem 3.1). We

further extend this result to a large class of commonly used activation functions (Theorem 3.4).
Asymptotically in ϵ, our bound can be written as O(ϵ[−][1][−][δ]) for all δ > 0, and in that sense breaks
the curse of dimensionality for shallow neural networks.

**2. Representation power of deep neural networks. We show that any function of the Korobov**
space can be approximated within ϵ with a deep neural network of depth log2(d) + 1 independent
of ϵ, with non-linear C[2] activation function, O(ϵ[−] 2[1] (log [1]ϵ [)] 3(d2−1) ) neurons and ⌈ _O(ϵ⌉[−]_ 2[1] (log [1]ϵ [)] 3(d2−1) )

training parameters (Theorem 4.1). This result improves that of Montanelli & Du (2019) who constructed an approximating neural network with larger depth (log [1]ϵ [log][ d][)][ –increasing with][ ϵ][– and]

3(d 1) _O_

larger number of neurons O(ϵ[−] 2[1] (log [1]ϵ [)] 2− +1). However, they used ReLU activation function.

**3. Near-optimality of neural networks as function approximators. Under the continuous func-**
tion approximators model introduced by DeVore et al. (1989), we prove that any continuous function approximator needs Θ(ϵ[−] 2[1] (log [1]ϵ [)] _d−2 )1_ parameters to approximate Korobov functions within ϵ

(Theorem 5.2). This lower bound nearly matches our established upper bounds on the number of
training parameters needed by deep and shallow neural networks to approximate functions of the
Korobov space, proving that they are near-optimal function approximators of the Korobov space.

Table 1 summarizes our new bounds and existing bounds on shallow and deep neural network approximation power for the Korobov space, Sobolev space and bandlimited functions. Our proofs
are constructive and give explicit structures to construct such neural networks with ReLU and general activation functions. Our constructions rely on sparse grid approximation introduced by Zenger
(1991), and studied in detail in Bungartz (1992); Bungartz & Griebel (2004). Specifically, we use the
sparse grid approach to approximate smooth functions with sums of products then construct neural
networks which approximate this structure. A key difficulty is to approximate the product function.
In particular in the case of shallow neural networks, we propose, to the best of our knowledge, the
first architecture approximating the product function with polynomial number of neurons. To derive our lower bound on the number of parameters needed to approximate the Korobov space, we
construct a linear subspace of the Korobov space, with large Bernstein width. This subspace is then
used to apply a general lower bound on nonlinear approximation derived by DeVore et al. (1989).


-----

The rest of the paper is structured as follows. In Section 2, we formalize our objective and introduce
the sparse grids approach. In Section 3 (resp. 4), we prove our bounds on the number of neurons
and training parameters for Korobov functions approximation with shallow (resp. deep) networks.
Finally, we formalize in Section 5 the notion of optimal continuous function approximators and
prove our novel near-optimality result.




|Space|Nb. of neurons|Nb. of training parameters|Depth|Activation σ|Reference|
|---|---|---|---|---|---|
|W r,p|ϵ−d r|ϵ−d r|1|C∞, non-poly|Mhaskar (1996)|
|X2,∞|ϵ−1(log 1 )3(d 2−1)+1 ϵ|ϵ−1 2 (log 1 ϵ )3(d 2−1)|2|ReLU-like|This paper|
|X2,∞|ϵ−3 2 (log 1 ϵ )3(d 2−1)|ϵ−1 2 (log 1 ϵ )3(d 2−1)|2|Sigmoid-like|This paper|
|W r,p|ϵ−d r log 1 ϵ|ϵ−d r log 1 ϵ|O(log 1 ) ϵ|ReLU|Yarotsky (2017) Liang & Srikant (2016)|
|Bandlimited functions|ϵ−2(log 1 )2 ϵ|ϵ−2(log 1 )2 ϵ|O((log 1 )2) ϵ|ReLU|Montanelli et al. (2019)|
|X2,∞|ϵ−1 2 (log 1 ϵ )3(d 2−1)+1|ϵ−1 2 (log 1 ϵ )3(d 2−1)|O(log d · log 1 ϵ )|ReLU|Montanelli & Du (2019)|
|X2,∞|ϵ−1 2 (log 1 ϵ )3(d 2−1)|ϵ−1 2 (log 1 ϵ )3(d 2−1)|⌈log d⌉+ 1 2|C2, non-linear|This paper|


Table 1: Approximation results for Sobolev W _[r,p]_ and Korobov X [2][,][∞] functions by shallow and deep
networks. Number of neurons and training parameters are given in O notation.

2 PRELIMINARIES

In this work, we consider feed-forward neural networks, using a linear output neuron and a nonlinear activation function σ : R → R for the other neurons, such as the popular rectified unit (ReLU)
_σ(x) = max(x, 0), the sigmoid σ(x) = (1 + e[−][x])[−][1]_ or the Heaviside function σ(x) = 1{x≥0}.
Let d ≥ 1 be the dimension of the input. We define a 1-hidden-layer network with N neurons as
**_x 7→_** [P]k[N]=1 _[u][k][σ][(][w]k[⊤][x][ +][ b][k][)][,][ where][ w][k][ ∈]_ [R][d][,][ b][k][ ∈] [R][ for][ i][ = 1][, . . ., N] [, are parameters. A neural]
network with several hidden layers is obtained by feeding the outputs of a given layer as inputs to the
next layer. We study the expressive power of neural networks, i.e., the ability to approximate a target
function f : R[d] _→_ R with as few neurons as possible, on the unit hyper-cube Ω:= [0, 1][d]. Another
relevant metric is the number of parameters that need to be trained to approximate the function, i.e.,
the number of parameters of the approximating network (uk, wk and bk) depending on the function
to approximate. We will adopt L[∞] norm as a measure of approximation error.

We now define some notations necessary to introduce our function spaces of interest. For an integer
_r, we denote C[r]_ the space of one dimensional functions differentiable r times and with continuous derivatives. In our analysis, we consider functions f with bounded mixed derivatives. For a
multi-index α ∈ N[d], the derivative of order α is D[α]f := _∂x[α]1∂[1][|][α][...∂x][|][1]_ _f_ _αdd_ _[,][ where][ |][α][|][1][ =][ P]i[d]=1_ _[|][α][i][|][.]_

Two common function spaces in a compact Ω _⊂_ R[d] are the Sobolev spaces W _[r,p](Ω) of functions_
having weak partial derivatives up to order r in L[p](Ω) and the Korobov spaces X _[r,p](Ω) of functions_
vanishing at the boundary and having weak mixed second derivatives up to order r in L[p](Ω).

_W_ _[r,p](Ω) =_ _f_ _L[p](Ω) : D[α]f_ _L[p](Ω),_ **_α_** 1 _r_ _,_
_{_ _∈_ _∈_ _|_ _|_ _≤_ _}_
_X_ _[r,p](Ω) = {f ∈_ _L[p](Ω) : f_ _|∂Ω_ = 0, D[α]f ∈ _L[p](Ω), |α|∞_ _≤_ _r}._

where ∂Ω denotes the boundary of Ω, **_α_** 1 = _i=1_ _i=1,...,d_
tively the L[1] and infinity norm. Note that Korobov spaces | _|_ _[|][α][i][|][ and] X[ |][r,p][α](Ω)[|][∞]_ are subsets of Sobolev spaces[= sup] _[|][α][i][|][ are respec-]_
_W_ _[r,p](Ω). For p =_, the usual norms on these spaces are given by[P][d]
_∞_

_f_ _W r,p(Ω) := max_ _f_ _X_ _r,p(Ω) := max_
_|_ _|_ **_α_** 1 _r_ _|_ _|_ **_α_** _r_
_|_ _|_ _≤_ _[∥][D][α][f]_ _[∥][∞]_ _[,]_ _|_ _|∞≤_ _[∥][D][α][f]_ _[∥][∞]_ _[,]_

For simplicity, we will write | · |2,∞ for | · |X 2,∞ . We focus our analysis on approximating functions on the Korobov space X [2][,][∞](Ω) for which the curse of dimensionality is drastically lessened
and we show that neural networks are near-optimal. Intuitively, a key difference compared to the
Sobolev space is that Korobov functions do not have high frequency oscillations in all directions
at a time. Such functions may require an exponential number of neurons Telgarsky (2016) and are
one of the main difficulties for Sobolev space approximation, which therefore exhibits the curse of


-----

dimensionality DeVore et al. (1989). On the contrary, the Korobov space prohibits such behaviour
by ensuring that functions can be differentiable on all dimensions together. Further discussions and
concrete examples are given in Appendix A.

2.1 THE CURSE OF DIMENSIONALITY

We adopt the point of view of asymptotic results in ϵ (or equivalently, in the number of neurons),
which is a well-established setting in the neural networks representation power literature (Mhaskar,
1996; Bungartz & Griebel, 2004; Yarotsky, 2017; Montanelli & Du, 2019) and numerical analysis
literature (Novak, 2006). In the rest of the paper, we use O notation which hide constants in d. For
each result, full dependencies in d are provided in appendix. Previous efforts to quantify the number
of neurons needed to approximate large general class of functions, showed that neural networks and
most classical functional approximation schemes exhibit the curse of dimensionality. For example,
for Sobolev functions, Mhaskar proved the following approximation bound.

**Theorem 2.1 (Mhaskar (1996)). Let p, r ≥** 1, and σ : R → R be an infinitely differentiable
_activation function, non-polynomial on any interval of R. Let ϵ > 0 sufficiently small. For any_
_f ∈_ _W_ _[r,p], there exists a shallow neural network with one hidden layer, activation function σ, and_
_O(ϵ[−]_ _[d]r ) neurons approximating f within ϵ for the infinity norm._

Therefore, the approximation of Sobolev functions by neural networks suffers from the curse of
dimensionality since the number of neurons needed grows exponentially with the input space dimension d. This curse is not due to poor performance of neural networks but rather to the choice
of the Sobolev space. DeVore et al. (1989) proved that any learning algorithm with continuous parameters needs at least Θ(ϵ[−] _[d]r ) parameters to approximate the Sobolev space W_ _[r,p]. This shows that_

the class of Sobolev functions suffers inherently from the curse of dimensionality and no continuous
function approximator can overcome it. We detail this notion later in Section 5.

The natural question is whether there exists a reasonable and sufficiently large class of functions
for which there is no inherent curse of dimensionality. Instead of the Sobolev space, we aim to add
more regularity to overcome the curse of dimensionality while preserving a reasonably large space.
The Korobov space X [2][,][∞](Ω)—functions with bounded mixed derivatives—is a natural candidate:
it is known in the numerical analysis community as a reasonably large space where numerical approximation methods can lessen the curse of dimensionality (Bungartz & Griebel, 2004). Korobov
functions have been introduced for solving partial differential equations (Korobov, 1959; Smolyak,
1963), and have then been used extensively for high-dimensional function approximation (Zenger
& Hackbusch, 1991; Bungartz & Griebel, 2004). This space of functions is included in the Sobolev
space, but still reasonably large as the regularity condition concerns only second order derivatives.
Two questions are of interest. First, how many neurons and training parameters a neural network
needs to approximate any Korobov function within ϵ in the L[∞] norm? Second, how do neural
networks perform compared to the optimal theoretical rates for Korobov spaces?

2.2 SPARSE GRIDS AND HIERARCHICAL BASIS

In this subsection, we introduce sparse grids which will be key in our neural networks constructions. These were introduced by Zenger (1991) and extensively used for high-dimensional function
approximation. We refer to Bungartz & Griebel (2004) for a thorough review of the topic.

The goal is to define discrete approximation spaces with basis functions. Instead of a classical
uniform grid partition of the hyper-cube [0, 1][d] involving n[d] components, where n is the number
of partitions in each coordinate, the sparse grid approach uses a smarter partitioning of the cube
preserving the approximation accuracy while drastically reducing the number of components of the
grid. The construction involves a 1-dimensional mother function φ which is used to generate all the
functions of the basis. For example, a simple choice for the building block φ is the standard hat
function φ(x) := (1 _x_ )+. The hat function is not the only possible choice. In the latter proofs
_−|_ _|_
we will specify which mother function is used, in our case either the interpolates of Deslauriers &
Dubuc (1989) (which we define rigorously later in our proofs) or the hat function φ which can be
seen as the Deslaurier-Dubuc interpolet of order 1. These more elaborate mother functions enjoy
more smoothness while essentially preserving the approximation power.


-----

_φ3,1_ _φ2,1_ _φ3,3_ _φ1,1_ _φ3,5_ _φ2,3_ _φ3,7_

_W1_
_W2_
_W3_

_W_

0 1

Figure 1: Hierarchical basis from the sparse grid construction using the hat function (1 )+.
_−| · |_


Assume the mother function has support inof local functions φlj _,ij : [0, 1] −→_ R for all [− ljk, k ≥ ]1. For and j 1 = 1 ≤ _ij, . . ., d ≤_ 2[l][j], it can be used to generate a set− 1 with support _ij2−[lj][,]k_ _[ i][j]2[+][lj][k]_

as follows, φlj _,ij_ (x) := φ(2[l][j] _x_ _ij), x_ [0, 1]. We then define a basis of d-dimensional functionsh i
_−_ _∈_
by taking the tensor product of these 1-dimensional functions. For all l, i ∈ N[d] with l ≥ **1 and**
**1** **_i_** 2[l] **1 where 2[l]** denotes (2[l][1] _, . . ., 2[l][d]_ ), define φl,i(x) := _j=1_ _[φ][l]j_ _[,i]j_ [(][x][j][)][,][ x][ ∈] [R][d][. For a]
_≤_ _≤_ _−_
fixed l ∈ N[d], we will consider the hierarchical increment space Wl which is the subspace spanned
by the functions _φl,i : 1_ **_i_** 2[l] **1**, as illustrated in Figure 1,[Q][d]
_{_ _≤_ _≤_ _−_ _}_

_Wl := span{φl,i, 1 ≤_ **_i ≤_** 2[l] _−_ **1, ij odd for all 1 ≤** _j ≤_ _d}._

Note that in the hierarchical increment Wl, all basis functions have disjoint support. Also, Korobov
functions X [2][,p](Ω) can be expressed uniquely in this hierarchical basis. Precisely, there is a unique
representation of u ∈ _X_ [2][,p](Ω) as u(x) = **_l,i_** _[v][l][,][i][φ][l][,][i][(][x][)][,][ where the sum is taken over all multi-]_

indices l ≥ **1 and 1 ≤** **_i ≤_** 2[l] _−_ **1 where all components of i are odd. In particular, all basis**
functions are linearly independent. Notice that this sum is infinite, the objective is now to define a[P]
finite-dimensional subspace of X [2][,p](Ω) that will serve as an approximation space. Sparse grids use
a carefully chosen subset of the hierarchical basis functions to construct the approximation space
_Vn[(1)]_ := **_l_** 1 _n+d_ 1 _[W][l][. When][ φ][ is the hat function, Bungartz and Griebel Bungartz & Griebel]_

_|_ _|_ _≤_ _−_
(2004) showed that this choice of approximating space leads to a good approximating error.

**Theorem 2.2[L]** (Bungartz & Griebel (2004)). Let f ∈ _X_ [2][,][∞](Ω) and fn[(1)] _be the projection of f on the_
_subspace Vn[(1)]. We have,_ _f_ _fn[(1)]_ 2[−][2][n]n[d][−][1][] _. Furthermore, if vl,i denotes the coefficient_
_∥_ _−_ _[∥]∞_ [=][ O]
_of φl,i in the decomposition of fn[(1)]_ _in Vn[(1)], then we have the upper bound _ _|vl,i| ≤_ 2[−][d]2[−][2][|][l][|][1] _|f_ _|2,∞,_
_for all l, i ∈_ N[d] _with |l|1 ≤_ _n + d −_ 1, 1 ≤ **_i ≤_** 2[l] _−_ **1 where i has odd components.**

3 THE REPRESENTATION POWER OF SHALLOW NEURAL NETWORKS


It has recently been shown that deep neural networks, with depth scaling with ϵ, lessen the curse of
dimensionality on the numbers of neuron needed to approximate the Korobov space Montanelli &
Du (2019). However, to the best of our knowledge, the question of whether shallow neural networks
with fixed universal depth — independent of ϵ and d — escape the curse of dimensionality as well
for the Korobov space remains open. We settle this question by proving that shallow neural networks
also lessen the curse of dimensionality for Korobov space.
**Theorem 3.1. Let ϵ > 0. For all f** _X_ [2][,][∞](Ω), there exists a neural network with 2 layers,
_ReLU activation, O(ϵ[−][1](log_ [1]ϵ [)] 3(d2−1) +1 ∈) neurons, and O(ϵ− 2[1] (log [1]ϵ [)] 3(d2−1) ) training parameters

_that approximates f within ϵ for the infinity norm._

In order to prove Theorem 3.1, we construct the approximating neural network explicitly. The first
3
step is to construct a neural network architecture with two layers and O(d 2 ϵ[−] 2[1] log [1]ϵ [)][ neurons that]

approximates the product function p : x ∈ [0, 1][d] _7−→_ [Q]i[n]=1 _[x][i][ within][ ϵ][ for all][ ϵ >][ 0][.]_

**Proposition 3.2. For all ϵ > 0, there exists a neural network with depth 2, ReLU activation and**
3
(d 2 ϵ[−] 2[1] log [1]ϵ [)][ neurons, that approximates the product function][ p][ :][ x][ ∈] [[0][,][ 1]][d][ −] _i=1_ _[x][i]_
_O_ _→_ [Q][d]

_within ϵ for the infinity norm._

**Sketch of proof** The proof builds upon the observation that p(x) = exp([P][d]i=1 [log][ x][i][)][.][ We]
construct an approximating 2-layer neural network where the first layer approximates log xi for
1 ≤ _i ≤_ _d, and the second layer approximates the exponential. We illustrate the construction in_


-----

_x1 x2_


_xd_


1



_O(d_ 2 ϵ[−] 2 log [1]ϵ [)] _· · ·_

_· · ·_ _· · ·_ _· · ·_ _· · ·_

log x1 log x2 log xd
f f _· · ·_ f

exp

(ϵ[−] 2[1] log [1] [)]

^d
_i=1[x][i]_ g
Q

Figure 2: Shallow neural network with ReLU activation implementing the product function3 _i=1_ _[x][i]_
within ϵ in infinity norm. The network has O(d 2 ϵ[−] 2[1] log [1]ϵ [)][ neurons on the first layer and]

_O(ϵ[−]_ 2[1] log [1]ϵ [)][ neurons on the second layer.] [Q][d]

Figure 2. More precisely, fix ϵ > 0. Consider the function hϵ : x [0, 1] max(log x, log ϵ).
_∈_ 1 _7→_
We approximate hϵ within _d[ϵ]_ [with a piece-wise affine function with][ O][(][d] 2 ϵ[−] 2[1] log [1]ϵ [)][ pieces, then]

represent this piece-wise affine function with a single layer neural network _h[ˆ]ϵ with the same number_
of neurons as the number of pieces (Lemma B.1, Appendix B.1). This 1-layer network then has
_∥dimensionhϵ −_ _h[ˆ]ϵ∥ i∞, approximating≤_ _d[ϵ]_ [. The first layer of our final network is the union of] log xi. Similarly, consider the exponential[ d][ copies of] g : x [ ˆ]hRϵ: one for eache[x]. We

_∈_ _−_ _7→_
construct a 1-layer neural network ˆgϵ with O(ϵ[−] 2[1] log [1]ϵ [)][ neurons with][ ∥][g][ −] _g[ˆ]ϵ∥∞_ _≤_ _ϵ. This will_

serve as second layer. Formally, the constructed network ˆpϵ is ˆpϵ = ˆgϵ _di=1_ _h[ˆ]ϵ(xi)_ . This 2-layer

3
neural network has O(d 2 ϵ[−] 2[1] log [1]ϵ [)][ neurons and verifies][ ∥]p[ˆ]ϵ − _p∥∞_ _≤Pϵ._ 

We use this result to prove Theorem 3.1 and show that we can approximate any Korobov function
_f ∈_ _X_ [2][,][∞](Ω) within ϵ with a 2-layer neural network of O(ϵ[−] [1]2 (log [1]ϵ [)] 3(d2−1) ) neurons. Consider

the sparse grid construction of the approximating space Vn[(1)] using the standard hat function as
mother function to create the hierarchical basis Wl (introduced in Section 2.2). The key idea is to
construct a shallow neural network approximating the sparse grid approximation and then use the
result of Theorem 2.2 to derive the approximation error. Let fn[(1)] be the projection of f on the
subspace Vn[(1)] defined in Section 2.2. fn[(1)] can be written as fn[(1)][(][x][) =][ P](l,i) _Un[(1)]_ _[v][l][,][i][φ][l][,][i][(][x][)][,]_

_∈_

where Un[(1)] contains the indices (l, i) of basis functions present in Vn[(1)]. We can use Theorem 2.2
and choose n carefully such that fn[(1)] approximates f within ϵ for L[∞] norm. The goal is now to
approximate fn[(1)] with a shallow neural network. Note that the basis functions can be written as
a product of univariate functions φl,i = _j=1_ _[φ][l]j_ _[,i]j_ [. We can therefore use a similar structure to]
the product approximation of Proposition 3.2 to approximate the basis functions. Specifically, the
first layer approximate the d(2[n] _−_ 1) = O[Q](ϵ[d][−] 2[1] (log [1]ϵ [)] _d−2 )1_ terms log φlj _,ij necessary to construct_

the basis functions of Vn[(1)] and a second layer to approximate the exponential in order to obtain
approximations of the O(2[n]n[d][−][1]) = O(ϵ[−] [1]2 (log [1]ϵ [)] 3(d2−1) ) basis functions of Vn[(1)]. We provide a

detailed figure illustrating the construction, Figure 5 in Appendix B.3.


The shallow network that we constructed in Theorem 3.1 uses the ReLU activation function. We
extend this result to a larger class of activation functions which include commonly used ones.
**Definition 3.3. A sigmoid-like activation function σ : R →** R is a non-decreasing function having
_finite limits in ±∞. A ReLU-like activation function σ : R →_ R is a function having a horizontal
_asymptote in_ _i.e. σ is bounded in R_ _, and an affine (non-horizontal) asymptote in +_ _, i.e._
_−∞_ _−_ _∞_
_there exists b > 0 such that σ(x) −_ _bx is bounded in R+._

Most common activation functions fall into these classes. Examples of sigmoid-like activations include Heaviside, logistic, tanh, arctan and softsign activations, while ReLU-like activations include
ReLU, ISRLU, ELU and soft-plus activations. We extend Theorem 3.1 to all these activations.


-----

**Theorem 3.4. For any approximation tolerance ϵ > 0, and for any f** _X_ [2][,][∞](Ω) there exists
_a neural network with depthwithin ϵ for the infinity norm, with 2 and O O(ϵϵ[−][−]2[1][1] (loglog([1]ϵ[1]ϵ[)]_ 3([3(]d[d]2−2[−]1)[1)])+1 training parameters that approximates) (resp. O(ϵ− ∈32 log( [1]ϵ [)] 3(d2−1) )) neurons f

_for a ReLU-like (resp. sigmoid-like) activation._

  

We note that these results can further be extended to more general Korobov spaces X _[r,p]. Indeed, the_
main dependence of our neural network architectures in the parameters r and p arise from sparse grid
approximation. Bungartz & Griebel (2004) show that results similar to Theorem 2.2 can be extended
to various values of r, p and different error norms with a similar sparse grid construction. For
instance, we can use these results combined with our proposed architecture to show that the Korobov
space X _[r,][∞]_ can be approximated in infinite norm by neural networks with O(ϵ[−] _r[1] (log_ [1]ϵ [)] _r+1r_ (d−1))

training parameters and same number of neurons up to a polynomial factor in ϵ.

4 THE REPRESENTATION POWER OF DEEP NEURAL NETWORKS


Montanelli & Du (2019) used the sparse grid approach to construct deep neural networks with
ReLU activation, approximating Korobov functions with O(ϵ[−] 2[1] (log [1]ϵ [)] 3(d2−1) +1) neurons, and

depth (log [1]ϵ [)][ for the][ L][∞] [norm. We improve this bound for deep neural networks with][ C][2][ non-]
_O_ 3(d 1)

linear activation functions. We prove that we only need O(ϵ[−] 2[1] (log [1]ϵ [)] 2− ) neurons and fixed

depth, independent of ϵ, to approximate the unit ball of the Korobov space within ϵ in the L[∞] norm.
**Theorem 4.1. Let σ ∈C[2]** _be a non-linear activation function. Let ϵ > 0. For any function_
_f_ _X_ [2][,][∞](Ω), there exists a neural network of depth log2 d + 1, with ReLU activation on
_the first layer and activation function ∈_ _σ for the next layers, ⌈_ (⌉ϵ[−] 2[1] (log [1]ϵ [)] 3(d2−1) ) neurons, and

3(d 1) _O_

_O(ϵ[−]_ 2[1] (log [1]ϵ [)] 2− ) training parameters approximating f within ϵ for the infinity norm.

Compared to the bound of shallow networks in Theorem 3.1, the number of neurons for deep networks is lower by a factor O([√]ϵ), while the number of training parameters is the same. Hence, deep
neural networks are more efficient than shallow neural network in the sense that shallow networks
need more “inactive” neurons to reach the same approximation power, but have the same number of
parameters. This gap in the number of “inactive” neurons can be consequent in practice, as we may
not know exactly which neurons to train and which neurons to fix.

This new bound on the number of parameters and neurons matches the approximation power of
sparse grids. In fact, sparse grids use Θ(ϵ[−] 2[1] (log [1]ϵ [)] 3(d2−1) ) parameters (weights of basis functions)

to approximate Korobov functions within ϵ. Our construction in Theorem 4.1 shows that deep neural
networks with fixed depth in ϵ can fully encode sparse grids approximators. Neural networks are
therefore more powerful function approximators. In particular, any sparse grid approximation using
_O(N_ (ϵ)) parameters, can be represented exactly by a neural network using O(N (ϵ)) neurons.

The deep approximating network (see Figure 3) has a very similar structure to our construction of
an approximating shallow network of Theorem 3.1. The main difference lies in the approximation
of the product function. Instead of using a 2-layer neural network, we now use a deep network. The
following result shows that deep neural networks can represent exactly the product function.
**Proposition 4.2 (Lin et al. (2017), Appendix A). Let σ be C[2]** _non linear activation function. For_
_any approximation error ϵ > 0, there exists a neural network with ⌈log2 d⌉_ _hidden layers and_
_activation σ, using at most 8d neurons arranged in a binary tree network that approximates the_
_product function_ _i=1_ _[x][i][ on][ [0][,][ 1]][d][ within][ ϵ][ for the infinity norm.]_

An important remark is that the structure of the constructed neural network is independent of ϵ. In

[Q][d]

particular, the depth and number of neurons is independent of the approximation precision ϵ, which
we refer to as exact approximation. It is known that an exponential number of neurons is needed in
order to exactly approximate the product function with a 1-layer neural network Lin et al. (2017),
however, the question of whether one could approximate the product with a shallow network and
a polynomial number of neurons, remained open. In Proposition 3.2, we answer positively to this
question by constructing an ϵ-approximating neural network of depth 2 with ReLU activation and
3
_O(d_ 2 ϵ[−] 2[1] log [1]ϵ [)][ neurons. Using the same ideas as in Theorem 3.4, we can generalize this result to]


-----

_O(ϵ[−]_ [1]2 (log [1]ϵ [)] _d−2 )1_ functions φlj _,ij_ (xj)


_x1_ _x2_


_xd_

_· · ·_

_· · ·_ _· · ·_ _· · ·_

_φ1,1(x1)_ _φn,2[n]−1(x1) φ1,1(x2) φn,2[n]−1(x2)_ _φn,2[n]−1(xd)_

_P1,1_ _⌈log2 d⌉_ _· · ·_ _Pl,i_ _· · ·_

_d_

_φl,i(x) =_ _φlj_ _,ij_ (xj)

_j=1_

Y


) basis functions φl,i



_· · ·_

1,1(x2

_⌉_ _· · ·_

_φ_

_f_ (x)


_Un[(1)]_ = O(ϵ[−] 2[1] (log [1]ϵ [)] 3(d2−1) ) basis functions φl,i

Figure 3: Deep neural network approximating a Korobov function f _X_ [2][,][∞](Ω) within ϵ. The
complete network has O(ϵ[−] 2[1] (log [1]ϵ [)] 3(d2−1) ) neurons and depth ⌈log2 d ∈⌉ + 1.

3
obtain an ϵ-approximating neural network of depth 2 with O(d 2 ϵ[−] [1]2 log [1]ϵ [)][ neurons for a ReLU-like]

activation, or O(d[2]ϵ[−][1] log [1]ϵ [)][ neurons for a sigmoid-like activation.]

5 NEURAL NETWORKS ARE NEAR-OPTIMAL FUNCTION APPROXIMATORS

In the previous sections, we proved upper bounds on the number of neurons and training parameters
needed by deep and shallow neural networks to approximate the Korobov space X [2][,][∞](Ω). We
now investigate how good is the performance of neural networks as function approximators. We
prove a lower bound on the number of parameters needed by any continuous function approximator
to approximate the Korobov space. In particular, neural networks, deep and shallow, will nearly
match this lower bound, making them near-optimal function approximators. Let us first formalize
the notion of continuous function approximators, following the framework of DeVore et al. (1989).

For any Banach space X —e.g., a function space—and a subset K ⊂X of elements to approximate,
we define a continuous function approximator with N parameters as a continuous parametrization
_a : K →_ R[N] together with a reconstruction scheme which is a N -dimensional manifold MN :
R[N] _→X_ . For any element f ∈ _K, the approximation given is MN_ (a(f )): the parametrization a
is derived continuously from the function f and then given as input to the reconstruction manifold
that outputs an approximation function in X . The error of this function approximator is defined
as EN,a, _N (K)_ := supf _K_ _f_ _N_ (a(f )) _. The best function approximator for space K_
minimizes this error. The minimal error for spaceM _X_ _∈_ _|_ _−M_ _K|X_ is given by
_EN_ (K) = min
_X_ _a,_ _N_ _[E][N,a,][M][N][ (][K][)][X][ .]_
_M_

In other terms, a continuous function approximator with N parameters cannot hope to approximate
_K better than within EN_ (K) . A class of function approximators is a set of function approximators
_X_
with a given structure. For example, neural networks with continuous parametrizations are a class
of function approximators where the number of parameters is the number of training parameters.
We say that a class of function approximators is optimal for the space of functions K if it matches
this minimal error asymptotically in N, within a constant multiplicative factor. In other words,
the number of parameters needed by the class to approximate functions in K within ϵ matches
asymptotically, within a constant, the least number of parametersThe norm considered in the approximation of the functions of N K needed to satisfy is the norm associated to the EN (K)X ≤ _ϵ._
space . DeVore et al. (1989) showed that this minimum error EN (K) is lower bounded by the
_X_ _X_
Bernstein width of the subset K ⊂X, defined as
_bN_ (K) := sup sup _ρ : ρU_ (XN +1) _K_ _,_
_X_ _XN_ +1 _{_ _⊂_ _}_


-----

where the outer sup is taken over all N + 1 dimensional linear sub-spaces of X, and U (Y ) denotes
the unit ball of Y for any linear subspace Y of X .

**Theorem 5.1 (DeVore et al. (1989)). Let** _a Banach space and K_ _. EN_ (K) _bN_ (K) _._
_X_ _⊂X_ _X ≥_ _X_

We prove a lower bound on the least number of parameters any class of continuous function approximators needs to approximate functions of the Korobov space.

**Theorem 5.2. Take** = L[∞](Ω) and K = _f_ _X_ [2][,][∞](Ω) : _f_ _X_ 2,∞(Ω) 1 _the unit ball of_
_X_ _{_ _∈_ _|_ _c_ _|_ _≤_ _}_
_the Korobov space. Then, there existsϵ > 0, a continuous function approximator approximating c > 0 with EN_ (K)X K ≥ withinN [2][ (log] ϵ in[ N] L[)][d][∞][−][1]norm uses at least[.][ Equivalently, for]

Θ(ϵ[−] 2[1] (log [1]ϵ [)] _d−2 )1_ _parameters._

**Sketch of proof** We seek an appropriate subspace XN +1 in order to get lower bound the Bernstein
width bN (K), which in turn provides a lower bound on the approximation error (Theorem 5.1). To
_X_
do so, we use the Deslaurier-Dubuc interpolet of degree 2, φ[(2)] (see Figure 6) which is C[2]. Using the
sparse grids approach, we construct a hierarchical basis in X [2][,][∞](Ω) using φ[(2)] as mother function
and define XN +1 as the approximation space Vn[(1)]. Here n is chosen such that the dimension of
_Vn[(1)]_ is roughly N + 1. The goal is to estimate sup _ρ : ρU_ (XN +1) _K_, which will lead to a
_{_ _⊂_ _}_
bound on bN (K) . To do so, we upper bound the Korobov norm by the L[∞] norm for elements of
_X_
_XN_ +1. For any function u ∈ _XN_ +1 we can write u = **_l,i_** _[v][l][,][i][ ·][ φ][l][,][i][. Using a stencil representation]_

of the coefficients vl,i, we are able to obtain an upper bound |u|X 2,∞ _≤_ Γd∥u∥∞ where Γd =
(2[2][n]n[d][−][1]). Then, bN (K) 1/Γd which yields the desired bound.[P]
_O_ _X ≥_

This lower bound matches within a logarithmic factor the upper bound on the number of training
parameters needed by deep and shallow neural networks to approximate the Korobov space within ϵ:
_O(ϵ[−]_ [1]2 (log [1]ϵ [)] 3(d2−1) ) (Theorem 3.1 and Theorem 4.1). It exhibits the same exponential dependence

in d with base log [1]ϵ [and the same main dependence on][ ϵ][ of][ ϵ][−] 2[1] . Note that the upper and lower bound

can be rewritten as O(ϵ[−][1][/][2][−][δ]) for all δ > 0. Moreover, our constructions in Theorem 3.1 and
Theorem 4.1 are continuous, which comes directly from the continuity of the sparse grid parameters
(see bound on vl,i in Theorem 2.2). Our bounds prove therefore that deep and shallow neural
networks are near optimal classes of function approximators for the Korobov space. Interestingly,
the subspace XN +1 our proof uses to show the lower bound is essentially the same as the subspace
we use to approximate Koborov functions in our proof of the upper bounds (Theorem 3.1 and 4.1).
The difference is in the choice of the interpolate φ to construct the basis functions, degree 2 for the
former (which provides needed regularity for the proof), and 1 for the later.

6 CONCLUSION AND DISCUSSION

We proved new upper and lower bounds on the number of neurons and training parameters needed by
shallow and deep neural networks to approximate Korobov functions. Our work shows that shallow
and deep networks not only lessen the curse of dimensionality but are also near-optimal.

Our work suggests several extensions. First, it would be very interesting to see if our proposed
theoretical near-optimal architectures have powerful empirical performance. While commonly used
structures (e.g. Convolution Neural Networks, or Recurrent Neural Networks) are motivated by
properties of the data such as symmetries, our structures are motivated by theoretical insights on
how to optimally approximate a large class of functions with a given number of neurons and parameters. Second, our upper bounds (Theorem 3.1 and 4.1) nearly match our lower bounds (Theorem
5.2) on the least number of training parameters needed to approximate the Korobov space. We
wonder if it is possible to close the gap between these bounds and hence prove neural network’s
optimality, e.g., one could prove that sparse grids are optimal function approximators by improving our lower bound to match sparse grid number of parameters O(ϵ[−] 2[1] (log [1]ϵ [)] 3(d2−1) ). Finally, we

showed the near-optimality of neural networks among the set of continuous function approximators.
It would be interesting to explore lower bounds (analog to Theorem 5.2) when considering larger sets
of function approximators, e.g., discontinuous function approximators. Could some discontinuous
neural network construction break the curse of dimensionality for the Sobolev space? The question
is then whether neural networks are still near-optimal in these larger sets of function approximators.


-----

ACKNOWLEDGMENTS

The authors are grateful to Tomaso Poggio and the MIT 6.520 course teaching staff for several
discussions, remarks and comments that were useful to this work.

REFERENCES

Andrew R Barron. Approximation and estimation bounds for artificial neural networks. Machine
_learning, 14(1):115–133, 1994._

Hans-Joachim Bungartz. D¨unne Gitter und deren Anwendung bei der adaptiven L¨osung der dreidi_mensionalen Poisson-Gleichung. Technische Universit¨at M¨unchen, 1992._

Hans-Joachim Bungartz and Michael Griebel. Sparse grids. Acta numerica, 13(1):147–269, 2004.

Gilles Deslauriers and Serge Dubuc. Symmetric iterative interpolation processes. In Constructive
_approximation, pp. 49–68. Springer, 1989._

Ronald A DeVore, Ralph Howard, and Charles Micchelli. Optimal nonlinear approximation.
_Manuscripta mathematica, 63(4):469–478, 1989._

Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are universal approximators. Neural networks, 2(5):359–366, 1989.

Masaaki Imaizumi and Kenji Fukumizu. Deep neural networks learn non-smooth functions effectively. In The 22nd international conference on artificial intelligence and statistics, pp. 869–878.
PMLR, 2019.

Jason M Klusowski and Andrew R Barron. Risk bounds for high-dimensional ridge function combinations including neural networks. arXiv preprint arXiv:1607.01434, 2016.

Jason M Klusowski and Andrew R Barron. Approximation by combinations of relu and squared
relu ridge functions with l1 and l0 controls. IEEE Transactions on Information Theory, 64(12):
7649–7656, 2018.

Michael Kohler and Adam Krzy˙zak. Adaptive regression estimation with multilayer feedforward
neural networks. Nonparametric Statistics, 17(8):891–913, 2005.

Michael Kohler and Adam Krzy˙zak. Nonparametric regression based on hierarchical interaction
models. IEEE Transactions on Information Theory, 63(3):1620–1630, 2016.

Michael Kohler and Jens Mehnert. Analysis of the rate of convergence of least squares neural
network regression estimates in case of measurement errors. Neural Networks, 24(3):273–279,
2011.

NM Korobov. Approximate solution of integral equations. Doklady Akademii Nauk SSSR, 128(2):
235–238, 1959.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6):84–90, 2017.

Shiyu Liang and Rayadurgam Srikant. Why deep neural networks for function approximation?
_arXiv preprint arXiv:1610.04161, 2016._

Henry W Lin, Max Tegmark, and David Rolnick. Why does deep and cheap learning work so well?
_Journal of Statistical Physics, 168(6):1223–1247, 2017._

Hrushikesh N Mhaskar. Neural networks for optimal approximation of smooth and analytic functions. Neural computation, 8(1):164–177, 1996.

Hrushikesh N Mhaskar. Eignets for function approximation on manifolds. Applied and Computa_tional Harmonic Analysis, 29(1):63–87, 2010._


-----

Hrushikesh N Mhaskar, Qianli Liao, and Tomaso Poggio. Learning functions: when is deep better
than shallow. arXiv preprint arXiv:1603.00988, 2016.

Hadrien Montanelli and Qiang Du. New error bounds for deep ReLU networks using sparse grids.
_SIAM Journal on Mathematics of Data Science, 1(1):78–92, 2019._

Hadrien Montanelli, Haizhao Yang, and Qiang Du. Deep ReLU networks overcome the curse of
dimensionality for bandlimited functions. arXiv preprint arXiv:1903.00735, 2019.

Ryumei Nakada and Masaaki Imaizumi. Adaptive approximation and estimation of deep neural
network with intrinsic dimensionality. 2019.

Erich Novak. Deterministic and stochastic error bounds in numerical analysis. 2006.

Philipp Petersen and Felix Voigtlaender. Optimal approximation of piecewise smooth functions
using deep ReLU neural networks. Neural Networks, 108:296–330, 2018.

Johannes Schmidt-Hieber. Deep ReLU network approximation of functions on a manifold. arXiv
_preprint arXiv:1908.00695, 2019._

David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of Go with deep neural networks and tree search. Nature, 529(7587):484–489, 2016.

Sergei Abramovich Smolyak. Quadrature and interpolation formulas for tensor products of certain
classes of functions. In Doklady Akademii Nauk, volume 148, pp. 1042–1045. Russian Academy
of Sciences, 1963.

Taiji Suzuki. Adaptivity of deep ReLU network for learning in Besov and mixed smooth Besov
spaces: optimal rate and curse of dimensionality. arXiv preprint arXiv:1810.08033, 2018.

Matus Telgarsky. Benefits of depth in neural networks. In Conference on learning theory, pp.
1517–1539. PMLR, 2016.

E Weinan, Chao Ma, and Lei Wu. Barron spaces and the compositional function spaces for neural
network models. arXiv preprint arXiv:1906.08039, 2019.

Dmitry Yarotsky. Error bounds for approximations with deep ReLU networks. Neural Networks,
94:103–114, 2017.

Ch Zenger and W Hackbusch. Parallel algorithms for partial differential equations. In Notes on
_Numerical Fluid Mechanics, volume 31, pp. 241–251. Vieweg, 1991._

Christoph Zenger. Sparse grids. Parallel Algorithms for Partial Differential Equations, 31, 1991.

APPENDIX

A ON KOROBOV FUNCTIONS

In this section, we further discuss Korobov functions X [2][,p](Ω). Korobov functions enjoy more
smoothness than Sobolev functions: smoothness for X [2][,p](Ω) is measured in terms of mixed deriva_tives of order two. Korobov functions X_ [2][,p](Ω) can be differentiated twice in each coordinates
simultaneously, while Sobolev W [2][,p](Ω) functions can only be differentiated twice in total. For
example, in two dimensions, for a function f to be Korobov it is required to have

_∂f_ _, [∂f]_ _, [∂][2][f]_ _, [∂][2][f]_ _,_ _∂[2]f_ _,_ _∂[3]f_ _,_ _∂[3]f_ _,_ _∂[4]f_ _L[p](Ω),_

_∂x1_ _∂x2_ _∂[2]x1_ _∂[2]x2_ _∂x1∂x2_ _∂[2]x1∂x2_ _∂x1∂[2]x2_ _∂[2]x1∂[2]x2_ _∈_


while for f to be Sobolev it requires only

_∂f_ _, [∂f]_ _, [∂][2][f]_ _, [∂][2][f]_ _,_ _∂[2]f_ _L[p](Ω)._

_∂x1_ _∂x2_ _∂[2]x1_ _∂[2]x2_ _∂x1∂x2_ _∈_


-----

The former can be seen fromand W _[r,p](Ω)._ _|α|∞_ _≤_ 2 and the latter from |α|1 ≤ 2 in the definition of X _[r,p](Ω)_

We now provide intuition on why Korobov functions are easier to approximate. One of the key
difficulties in approximating Sobolev functions are possible high frequency oscillations which may
require an exponential number of neurons (Telgarsky, 2016). For instance, consider functions which
have similar structure to W(n...n) (defined in Subsection 2.2): for any smooth basis function φ with
support on the unit cube (see Figure 6 for example), consider the linear function space formed by
linear combinations of dilated function φ with support on each cube d-dimensional grid of step 2[−][n].
This corresponds exactly to the construction of W(n...n) which uses the product of hat function on
each dimension as basis function φ. This function space can have strong oscillations in all directions
at a time. The Korobov space prohibits such behavior by ensuring that functions can be differentiated twice on each dimension, simultaneously. As a result, functions cannot oscillate in all directions
at a time without having large Korobov norm. We end this paragraph by comparing the Korobov
space to the space of bandlimited functions which was shown to avoid the curse of dimensionality
(Montanelli et al., 2019). These are functions for which the support frequency components is restricted to a fixed compact. Intuitively, approximating these functions can be achieved because the
set of frequencies is truncated to a compact, which then allows to sample frequencies and obtain
approximation guarantees. Instead of imposing a hard constraint of cutting high frequencies, the
Korobov space asks for smoothness conditions which do not prohibit high frequencies but rather
impose a budget for high frequency oscillations. We precise this idea in the next example.

A concrete example of Korobov functions is given by an analogous of the function space Vn[(1)]
which we used as approximation space in the proof of our results (see Section 2.2). Similarly to the
previous paragraph, one should use a smooth basis function to ensure differentiability. Recall that
_Vn[(1)]_ is defined as
_Vn[(1)]_ := _Wl._

_|l|1≤Mn+d−1_

Intuitively, this approximation space introduces a ”budget” of oscillations for all dimensions through
the constraint _i=1_ _[l][i][ ≤]_ _[n][ +][ d][ −]_ [1][. As a result, dilations of the basis function can only occur in a]
restricted set of directions at a time, which ensures that the Korobov norm stays bounded.

[P][d]

B PROOFS OF SECTION 3

B.1 APPROXIMATING THE PRODUCT FUNCTION


3
In this subsection, we construct a neural network architecture with two layers and O(d 2 ϵ[−] 2[1] log [1]ϵ [)]

neurons that approximates the product function p : x ∈ [0, 1][d] _7−→_ [Q]i[n]=1 _[x][i][ within][ ϵ][ for all][ ϵ >][ 0][,]_
which proves Proposition 3.2. We first prove a simple lemma to represent univariate piece-wise
affine functions by shallow neural networks.
**Lemma B.1. Any one dimensional continuous piece-wise affine function with m pieces is repre-**
_sentable exactly by a shallow neural network with ReLU activation, with m neurons on a single_
_layer._

_Proof. This is a simple consequence from Proposition 1 in Yarotsky (2017). We recall the proof for_
completeness. Let x1 _xm_ 1 be the subdivision of the piece-wise affine function f . We use
a neural network of the form ≤· · · ≤ _−_


_m−1_

_wk(x_ _xk)+_ _w0(x1_ _x)+,_
_k=1_ _−_ _−_ _−_

X


_g(x) := f_ (x1) +


where w0 is the slope of f on the piece ≤ _x1, w1 is the slope of f on the piece [x1, x2],_

_i=1_ _[w][i][(][x][k][+1][ −]_ _[x][i][)]_
_wk =_ _[f]_ [(][x][k][+1][)][ −] _[f]_ [(][x][1][)][ −] [P][k][−][1] _,_

_xk+1_ _xk_
_−_

for k = 1, · · ·, m _−_ 2, and wm−1 = ˜w − [P][m]k=1[−][2] _[w][k][ where][ ˜]w is the slope of f on the piece ≥_ _xm−1._
Notice that f and g coincide on all xk for 1 ≤ _k ≤_ _m −_ 1. Furthermore, g has same slope as f on
each pieces, therefore, g = f .


-----

We can approximate univariate right continuous functions by piece-wise affine functions, and then
use Lemma B.1 to represent them by shallow neural networks. The following lemma shows that
_O(ϵ[−][1]) neurons are sufficient to represent an increasing right-continuous function with a shallow_
neural network.

**Lemma B.2.and let ϵ > 0 Let. There exists a shallow neural network with ReLU activation, with f : I −→** [c, d] be a right-continuous increasing function where d I−ϵ is an interval,c _neurons on_

_a single layer, that approximates f within ϵ for the infinity norm._

 

_Proof. Let m =_ _d−ϵ_ _c_ Define a subdivision of the image interval c _y1_ _. . ._ _ym_ _d where_

_yk = c_ + _kϵ for k = 1_ _, . . ., m_ . Note that this subdivision contains exactly ≤ _≤ d−ϵ_ _c_ pieces. Now define ≤ _≤_

a subdivision of I, x1 ≤ _x2 ≤_ _. . . ≤_ _xm by_  

_xk := sup{x ∈_ _I, f_ (x) ≤ _yk},_

for k = 1, . . ., m. This subdivision stills has _d−ϵ_ _c_ pieces. We now construct our approximation

function _f[ˆ] on I as the continuous piece-wise affine function on the subdivision x1_ _. . ._ _xm such_

  _≤_ _≤_

that _f[ˆ](xk) = yk for all 1 ≤_ _k ≤_ _m and_ _f[ˆ] is constant before x1 and after xm (see Figure 4). Let_
_x ∈_ _I._

-  If x ≤ _x1, because f is increasing and right-continuous, c ≤_ _f_ (x) ≤ _f_ (x1) ≤ _y1 = c + ϵ._
Therefore |f (x) − _f[ˆ](x)| = |f_ (x) − (c + ϵ)| ≤ _ϵ._

-  Iffˆ( xx)k ≤ < xyk+1 ≤. Thereforexk+1, we have |f (x) y −k < ff[ˆ](x)| ≤(x)y ≤k+1f −(xykk+1 =) ≤ ϵ. _yk+1. Further note that yk ≤_

-  If xm < x, then ym < f (x) ≤ _d. Again, |f_ (x) − _f[ˆ](x)| = |f_ (x) − _ym| ≤_ _d −_ _ym ≤_ _ϵ._

Therefore _f_ _f_ _ϵ. We can now use Lemma B.1 to end the proof._
_∥_ _−_ [ˆ]∥∞ _≤_

.
.

_c = y0_

_x_

_· · ·_

|y|Col2|
|---|---|
|||
|||
|• • • • • • •|•|
|• • • • • • •||
|• • • • • •||
|• • • • •||
|• • • •||
|• • •||
|• •||
|||
|||
|x x x 0 2 3 · · ·||


Figure 4: Approximation of a right-continuous increasing function (blue) in an interval [c, d] within
_ϵ by a piece-wise linear function (red) with_ _ϵ_

regular subdivision of the y axis of step ϵ and constructing a linear approximation in the pre-image ⌊ _[d][−][c]_ _[⌋]_ [pieces. The approximation is constructed using a]
of each part of the subdivision.

If the function to approximate has some regularity, the number of neurons needed for approximation
can be significantly reduced. In the following lemma, we show that O(ϵ[−] 2[1] ) neurons are sufficient

to approximate a C[2] univariate function with a shallow neural network.


-----

_with ReLU activation, withLemma B.3. Let f : [a, b] −√1→2ϵ_ [min(][c, d] ∈C|f[2], and let[′′]|(1 + µ ϵ >(f, ϵ 0)). There exists a shallow neural network, (b − _a)_ _∥f_ _[′′]∥∞) neurons on a single_

_layer, where µ(f, ϵ)_ 1 as ϵ 0, that approximates f within ϵ for the infinity norm.
_→_ _→_ R p p

_Proof. See Appendix B.2.1._

We will now use the ideas of Lemma B.2 and Lemma B.3 to approximate a truncated log function,
which we will use in the construction of our neural network approximating the product.

**Corollary B.4. Let ϵ > 0 sufficiently small and δ > 0. Consider the truncated logarithm function**
log : [δ, 1] R. There exists a shallow neural network with ReLU activation, with ϵ[−] 2[1] log [1]δ
_−→_

_neurons on a single layer, that approximates f within ϵ for the infinity norm._

_Proof. See Appendix B.2.2._

We are now ready to construct a neural network approximating the product function and prove
Proposition 3.2. The proof builds upon on the observation that _i=1_ _[x][i][ = exp(][P]i[d]=1_ [log][ x][i][)][.]
We construct an approximating 2-layer neural network where the first layer computes log xi for
1 _i_ _d, and the second layer computes the exponential. We illustrate the construction of the_
_≤_ _≤_ [Q][d]
proof in Figure 2.

**Proof of Proposition 3.2** Fix ϵ > 0. Consider the function hϵ : x ∈ [0, 1] 7→ max(log x, log ϵ) ∈

[log ϵ, 0]. Using Corollary B.4, there exists a neural network _h[ˆ]ϵ : [0, 1]_ [log ϵ, 0] with
1 +the ϵ ⌈-approximation ofd 12 ϵ[−] 2[1] log [1]ϵ _[⌉]_ [neurons on a single layer such that] hϵ : x ∈ [ϵ, 1] 7→ log x ∈ [log ϵ, 0][ ∥], then extend this function to[h][ϵ][ −] _h[ˆ]ϵ∥∞_ _≤_ _dϵ_ [. Indeed, one can take] −→ [0, ϵ] with a

constant equal to log ϵ. The resulting piece-wise affine function has one additional segment corresponding to one additional neuron in the approximating function. Similarly, consider the exponential
_g : x_ R _e[x]_ [0, 1]. Because g is right-continuous increasing, we can use Lemma B.3 to con_∈_ _−_ _7→_ _∈_

struct a neural network ˆgϵ : R− _−→_ [0, 1] with 1 + _√12ϵ_ [log][ 1]ϵ neurons on a single layer such that

_g_ _gˆϵ_ _ϵ. Indeed, again one can take the ϵ-approximation ofl_ m _gϵ : x_ [log ϵ, 0] _e[x]_ [0, 1],
_∥_ _−_ _∥∞_ _≤_ _∈_ _7→_ _∈_
then extend this function to (−∞, log ϵ] with a constant equal to ϵ. The corresponding neural network has an additional neuron. We construct our final neural network _φ[ˆ]ϵ (see Figure 2) as_


_φˆϵ = ˆgϵ_


_hˆϵ(xi)_
_i=1_

X


Note that _φ[ˆ]ϵ can be represented as a 2-layer neural network: the first layer is composed of the union_
1
of the 1+ _d_ 2 ϵ[−] 2[1] log [1]ϵ _h[i]ϵ_ [:][ x][ ∈] [[0][,][ 1]][d][ 7→]

_hˆϵ(xi) ∈_ R⌈ for each dimension[⌉] [neurons composing each of the] i ∈{1, . . ., d}. The second layer is composed of the[ 1][-layer neural networks]3 [ ˆ] 1+ _√12ϵ_ [log][ 1]ϵ

neurons of ˆgϵ. Hence, the constructed neural network _φ[ˆ]ϵ has O(d_ 2 ϵ[−] [1]2 log [1]ϵ [)][ neurons. Let us now]l m

analyze the approximation error. Let x ∈ [0, 1][d]. For the sake of brevity, denote ˆy = _i=1_ _h[ˆ]ϵ(xi)_
and y = _i=1_ [log(][x][i][)][. We have,]

[P][d]

_d_

_φϵ(x[P])_ _[d]_ _p(x)_ _φϵ(x)_ exp(ˆy) + exp(ˆy) exp(y) _ϵ +_ _xi_ exp(ˆy _y)_ 1 _,_
_|_ [ˆ] _−_ _| ≤|_ [ˆ] _−_ _|_ _|_ _−_ _| ≤_ _i=1_ _· |_ _−_ _−_ _|_

Y

where we used the fact that |φ[ˆ]ϵ(x) − exp(ˆy)| = |gˆϵ(ˆy) − _g(ˆy)| ≤∥gˆϵ −_ _g∥∞_ _≤_ _ϵ._

First suppose that x _ϵ. In this case, for all i_ 1, . . ., d we have _hϵ(xi)_ log(xi) = _hϵ(xi)_
_≥_ _∈{_ _}_ _|[ˆ]_ _−_ _|_ _|[ˆ]_ _−_
_hϵ(xi)| ≤_ _d[ϵ]_ [. Then,][ |]y[ˆ]−y| ≤ _ϵ. Consequently, |φ[ˆ]ϵ(x)−p(x)| ≤_ _ϵ+max(|e[ϵ]−1|, |e[−][ϵ]−1|) ≤_ 3ϵ, for

_ϵ > 0 sufficiently small. Without loss of generality now suppose x1_ _ϵ. Then ˆy_ _hϵ(x1)_ log ϵ,
_≤_ _≤_ _≤_
so by definition of ˆgϵ, we have 0 _φϵ(x) = ˆgϵ(ˆy)_ exp(log ϵ) = ϵ. Also, 0 _p(x)_ _ϵ so finally_
_≤_ [ˆ] _≤_ _≤_ _≤_
_φϵ(x)_ _p(x)_ _ϵ._
_|_ [ˆ] _−_ _| ≤_


-----

**Remark B.5. Note that using Lemma B.2 instead of Lemma B.3 to construct approximating shallow**
_d_
_networks for 1_ log and exp would yield approximation functions _h[ˆ]ϵ with O(_ _ϵ_ [log][ 1]ϵ ) neurons and ˆgϵ

_with_ ( _ϵ_ ) neurons. Therefore, the corresponding neural network would approximate the product
_O_  

_p with O(d[2]ϵ[−][1]_ log [1]ϵ [)][ neurons.]

B.2 MISSING PROOFS OF SECTION B.1

B.2.1 PROOF OF LEMMA B.3


_Proof. Similarly as the proof of Lemma B.2, the goal is to approximate f by a piece-wise affine_
functionfˆ coincide onf[ˆ] defined on a subdivision x0, . . ., xm+1. We first analyse the error induced by a linear approximation of the x0 = a ≤ _x1 ≤_ _. . . ≤_ _xm ≤_ _xm+1 = b such that f and_
function on each piece. Let x ∈ [u, v] for u, v ∈ _I. Using the mean value theorem, there exists_
_αx_ [u, x] such that f (x) _f_ (u) = f (αx)(x _u) and βx_ [x, v] such that f (v) _f_ (x) =
_f_ ( ∈βx)(v _x). Combining these two equalities, we get, −_ _[′]_ _−_ _∈_ _−_

_[′]_ _−_

_f_ (x) _f_ (u) (x _u)_ _[f]_ [(][v][)][ −] _[f]_ [(][u][)] = [(][v][ −] _[x][)(][f]_ [(][x][)][ −] _[f]_ [(][u][))][ −] [(][x][ −] _[u][)(][f]_ [(][v][)][ −] _[f]_ [(][x][))]
_−_ _−_ _−_ _v_ _u_ _v_ _u_

_−_ _−_

= (x _u)(x_ _v)_ _[f][ ′][(][β][x][)][ −]_ _[f][ ′][(][α][x][)]_
_−_ _−_ _v_ _u_

_βx_ _−_
_αx_ _[f][ ′′][(][t][)][dt]_

= (x _u)(v_ _x)_
_−_ _−_ _v_ _u_

R

_−_

Hence,

_βx_
_αx_ _[f][ ′′][(][t][)][dt]_

_f_ (x) = f (u) + (x _u)_ _[f]_ [(][v][)][ −] _[f]_ [(][u][)] + (x _u)(v_ _x)_ _._ (1)
_−_ _v_ _u_ _−_ _−_ _v_ _u_

R

_−_ _−_

We now apply this result to bound the approximation error on each pieces of the subdivision. Let
_k_ [m]. Recall _f[ˆ] is linear on the subdivision [xk, xk+1] and_ _f[ˆ](xk) = f_ (xk) and _f[ˆ](xk+1) =_
_∈_
_f_ (xk+1). Hence, for all x [xk, xk+1], _f[ˆ](x) = f_ (xk) + (x _xk)_ _[f]_ [(][x]x[k]k[+1]+1[)][−]x[f] _k[(][x][k][)]_ . Using Equation
_∈_ _−_ _−_

equation 1 with u = xk and v = xk+1, we get,

_βx_
_αx_ _[f][ ′′][(][t][)][dt]_

_∥f −_ _f[ˆ]∥∞,[xk,xk+1] ≤_ _x∈[xsupk,xk+1]_ Rxk+1 − _xk_

_xk+1_

_≤_ 2[1] [(][x][k][+1][ −] _[x][(][k][x][)][ −]x[x]k_ _[k][)(][x]|f[k][+1][′′](t[ −])|dt[x][)]_

Z

_≤_ 2[1] [(][x][k][+1][ −] _[x][k][)][2][∥][f][ ′′][∥][∞][,][[][x][k][,x][k][+1][]][.]_

Therefore, using a regular subdivision with step _∥f_ _[′′]2ϵ∥∞_ [yields an][ ϵ][-approximation of][ f][ with]

(b−a)√[√]2∥ϵf _[′′]∥∞_ pieces. q
 

We now show that for any µ > 0, there exists an ϵ-approximation of f with at most R _[√]√2|fϵ[′′]|_ (1 + µ)

pieces. To do so, we use the fact that the upper Riemann sum for _[√]f_ _[′′]_ converges to the integral
since _[√]f_ _[′′]_ is continuous on [a, b]. First define a partition a = X0 ≤ _XK = b of [a, b b_ ] such that
the upper Riemann sum R([√]f _[′′]) on this subdivision satisfies R([√]f_ _[′′]) ≤_ (1 + µ/2) _a_ _√f ′′. Now_

define on each interval Ik of the partition a regular subdivision with step _∥f_ _[′′]2ϵ∥Ik_ [as before. Finally,]R

consider the subdivision union of all these subdivisions, and construct the approximationq _f[ˆ] on this_
final subdivision. By construction, _f_ _f_ _ϵ because the inequality holds on each piece of the_
_∥_ _−_ [ˆ]∥∞ _≤_
subdivision. Further, the number of pieces is

_K_ 1
_−_ 1 + (Xi+1 − _Xi) sup[Xi,Xi+1]_ _√f ′′_ = + K _|f_ _[′′]|_ (1 + µ),

_√2ϵ_ _[R][(]√[√]2[f]ϵ[ ′′][)]_ _≤_ _√2ϵ_

_i=0_ R p

X

for ϵ > 0 small enough. Using Lemma B.1 we can complete the proof.


-----

_d(2[n]_ _−_ 1) = O(ϵ[−] 2[1] (log [1]ϵ [)] _d−2 )1_ functions φlj _,ij_ (xj)


_x1_ _x2_


_xd_






1 _· · ·_
(d 2 ϵ[−] 2[1] log [1]ϵ [)]

_· · ·_ _· · ·_ _· · ·_ _· · ·_ _· · ·_ _· · ·_ _· · ·_ _· · ·_

log^ φ1,1(x1) log^ φn,2[n]−1(x1) log^ φ1,1(x2) log^ φn,2[n]−1(x2) log^ φn,2[n]−1(xd)

_· · ·_ _· · ·_ _· · ·_ _· · ·_ _d_

_O(ϵ[−]_ 2[1] log [1]ϵ [)] _φl,i(x) :=_ _φ]lj_ _,ij_ (xj)

Y

_f_ (x)

g


_φ]lj_ _,ij_ (xj)
_j=1_

Y


_Un[(1)]_ = O(ϵ[−] 2[1] (log [1]ϵ [)] 3(d2−1) ) basis functions φl,i

Figure 5: Shallow neural network with ReLU activation approximating a Korobov function f

_d+1_ _∈_

_Xand[2][,][∞](Ω)(ϵ[−] within[1][  ]log_ [1]ϵ ϵ in infinity norm. The network has[3(][d]2[−][1)] +1) neurons on the second layer. O(ϵ[−][1](log [1]ϵ [)] 2 ) neurons on the first layer

_O_



B.2.2 PROOF OF COROLLARY B.4

_Proof. In view of Lemma B.3, the goal is to show that we can remove the dependence of µ(f, ϵ) in_
_δ. This essentially comes from the fact that the upper Riemann sum behaves well for approximating_
1
log. Consider the subdivision x0 := δ ≤ _x1 ≤ϵ_ _. . . ≤_ _xm ≤_ _xm+1 := 1 with m =_ _ϵ˜_ [log][ 1]δ where

_ϵ˜ := log(1 +_ _√2ϵ), such that xk = e[log][ δ][+][k][˜], for k = 0, . . ., m_ 1. Denote ˆf the corresponding

_−_  
piece-wise affine approximation. Similarly to the proof of Lemma B.3, for k = 0, . . ., m − 1,

_ϵ_ 1)2

_∥_ log −f[ˆ]∥∞,[xk,xk+1] ≤ [1]2 [(][x][k][+1][ −] _[x][k][)][2][∥][f][ ′′][∥][∞][,][[][x][k][,x][k][+1][]][ ≤]_ [(][e][˜] −2 _≤_ _ϵ._

The proof follows.

B.3 PROOF OF THEOREM 3.1: APPROXIMATING THE KOROBOV SPACE X [2][,][∞](Ω)


In this subsection, we prove Theorem 3.1 and show that we can approximate any Korobov function
_f ∈_ _X_ [2][,][∞](Ω) within ϵ with a 2-layer neural network of O(ϵ[−] [1]2 (log [1]ϵ [)] 3(d2−1) ) neurons. We illustrate

the construction in Figure 5. Our proof combines the constructed network approximating the product
function and a decomposition of f as a sum of separable functions, i.e. a decomposition of the form


_φ[(]j[k][)][(][x][j][)][,]_ _∀x ∈_ [0, 1][d].
_j=1_

Y


_f_ (x) ≈


_k=1_


Consider the sparse grid construction of the approximating space Vn[(1)] using the standard hat function as mother function to create the hierarchical basis Wl (introduced in Section 2.2). We recall that
the approximation space is defined as Vn[(1)] := **_l_** 1 _n+d_ 1 _[W][l][. We will construct a neural network]_

_|_ _|_ _≤_ _−_
approximating the sparse grid approximation and then use the result of Theorem 2.2 to derive the
approximation error. Figure 5 gives an illustration of the construction.

[L]

Let fn[(1)] be the projection of f on the subspace Vn[(1)]. fn[(1)] can be written as


_fn[(1)][(][x][) =]_


_vl,iφl,i(x),_

(l,i) _Un[(1)]_

X∈


-----

where Un[(1)] contains the indices (l, i) of basis functions present in Vn[(1)] i.e.

_Un[(1)]_ := {(l, i), _|l|1 ≤_ _n + d −_ 1, 1 ≤ **i ≤** 2[l] _−_ **1, ij odd for all 1 ≤** _j ≤_ _d}._ (2)

Throughout the proof, we explicitly construct a neural network that uses this decomposition to approximate fn[(1)][. We then use Theorem 2.2 and choose][ n][ carefully such that][ f][ (1)]n approximates
_f within ϵ for L[∞]_ norm. Note that the basis functions can be written as a product of univariate
functions φl,i = _j=1_ _[φ][l]j_ _[,i]j_ [. We can therefore use the product approximation of Proposition 3.2]
to approximate the basis functions. Specifically, we will use one layer to approximate the terms
log φlj _,ij and a second layer to approximate the exponential.[Q][d]_

We now present in detail the construction of the first layer. First, recall that φlj _,ij is a piece-wise_
affine function with subdivision 0 ≤ _[i][j]2[−][lj][1]_ _≤_ 2i[lj]j _[ ≤]_ _[i][j]2[+1][lj]_ _≤_ 1. Define the error term ˜ϵ := 2|f _|ϵ2,∞_ [.]

We consider a symmetric subdivision of the interval _ij_ _−1+˜ϵ_ _,_ _[i][j]_ [+1][−]ϵ[˜] . We define it as follows:

2[lj] 2[lj]

_ij_ 1+˜ϵ _ij_ _ij_ +1 _ϵ˜_
_x0 =_ _−2[lj]_ _≤_ _x1 ≤· · · ≤_ _xm+1 =_ 2[lj][ ≤] _[x][m][+2]h[ ≤· · · ≤]_ _[x][2][m]i[+2][ =]_ 2[lj]− where m =

_ϵ10_ [log][ 1]ϵ˜ and ϵ0 := log(1 + 2˜ϵ/d), such that

j k p


_ϵ+kϵ0_
_xk =_ _[i][j][ −]_ [1 +][ e][log ˜] 0 _k_ _m,_

2[l][j] _≤_ _≤_


_ϵ+(2m+2−k)kϵ0_
_xk =_ _[i][j][ + 1][ −]_ _[e][log ˜]_ _m + 2_ _k_ 2m + 2.

2[l][j] _≤_ _≤_

Note that with this definition, the terms log(2[l][j] _xk_ _ij) form a regular sequence with step ϵ0. We_
now construct the piece-wise affine functionincides with log φlj _,ij on x0, · · ·, x2m+2 and is constant on ˆglj_ _,ij on the subdivision −_ [0, x0] and x [0 ≤· · · ≤x2m+2, 1]x. By Lemma B.1,2m+2 which cothis function can be represented by a 1-layer neural network with as much neurons as the number of

pieces of ˆg, i.e. at most 2 3ϵ˜d [log][ 1]ϵ˜ [neurons for][ ϵ][ sufficiently small. A similar proof to that of Corol-]

lary B.4 shows that ˆg approximatesq max(log φlj _,ij_ _, log(˜ϵ/3)) within ˜ϵ/(3d) for the infinity norm._
We use this construction to compute in parallel, ˜ϵ/(3d)-approximations of max(log φlj _,ij_ (xj), log ˜ϵ)
functions that we will need, in order to compute thefor all 1 ≤ _j ≤_ _d, and 1 ≤_ _lj ≤_ _n, 1 ≤_ _ij ≤_ 2[l][j] where d-dimensional function basis of the approxima- ij is odd. These are exactly the 1-dimensional
tion space Vn[(1)]. There are d(2[n] 1) such univariate functions, therefore our first layer contains at
_−_

most 2[n][+1]d 3ϵ˜d [log][ 1]ϵ˜ [neurons.]
q


We now turn to the second layer. The result of the first two layers will be ˜ϵ/3-approximations of
_φl,i for all (l, i) ∈_ _Un[(1)][. Recall that][ U][ (1)]n_ contains the indices for the functions forming a basis of
the approximation space Vn[(1)]. To do so, for each indexes (l, i) _Un[(1)]_ we construct a 1-layer neural
_∈_
network approximating the function exp, which will compute an approximation of exp(ˆgl1,i1 +· · ·+
_gˆld,id_ ). The approximation of exp is constructed in the same way as for Lemma B.3. Consider a
regular subdivision of the interval [log(˜ϵ/3), 0] with step 2(˜ϵ/3), i.e. x0 := log(˜ϵ/3) ≤ _x1 ≤_

3

_· · · ≤_ _xm ≤_ _xm+1 = 0 where m =_ 2˜ϵ [log][ 3]ϵ˜, such thatp _xk = log ˜ϵ + k√2˜ϵ,_ 0 ≤ _k ≤_ _m._

Construct the piece-wise affine functionjqh[ˆ] on the subdivisionk _x0 ≤· · · ≤_ _xm+1 which coincides_
with exp on x0, · · ·, xm+1 and is constant on (−∞, x0]. Lemma B.3 shows that _h[ˆ] approximates_
exp on R within ˜ϵ for the infinity norm. Again, Lemma B.1 gives a representation of _h[ˆ] as a 1-layer_
_−_

neural network with as many neurons as pieces in _h[ˆ] i.e. 1 +_ 2˜3ϵ [log][ 3]ϵ˜ . The second layer is

the union of 1-layer neural networks approximating exp within ˜lqϵ/3, for each indexesm (l, i) ∈ _Un[(1)][.]_

Therefore, the second layer contains _Un[(1)]_ 1 + 2˜3ϵ [log][ 3]ϵ˜ neurons. As shown in Bungartz &

Griebel (2004),  lq m

_n−1_ _d_ 1 + i _d−1_ _n + d_ 1 _nd−1_

_Un[(1)]_ = 2[i]· _−d_ 1 = (−1)[d]+2[n] _i_ _−_ (−2)[d][−][1][−][i] = 2[n]· (d 1)! [+][ O][(][n][d][−][2][)]

_i=0_  _−_  _i=0_    _−_

X X


-----

Therefore, the second layer has O 2[n n](d−[d][−]1)![1] _ϵ[˜][−]_ 2[1] log [1]ϵ˜ neurons. Finally, the output layer computes

the weighted sum of the basis functions to approximate  _fn[(1)][. Denote by][ ˆ]fn[(1)]_ the corresponding
function of the constructed neural network (see Figure 5), i.e.


_vl,i_ _h[ˆ]_
_·_


_fˆn[(1)]_


_gˆ(xj)_ _._



_j=1_

X




(l,i)∈Un[(1)]


Let us analyze the approximation error of our neural network. The proof of Proposition 3.2 shows
that the output of the two first layers h _dj=1_ _g[ˆ](·j)_ approximates φl,i within ˜ϵ. Therefore, we

obtain ∥fn[(1)] _−_ _f[ˆ]n[(1)][∥]∞_ _[≤]_ _ϵ[˜]_ (l,i)∈Un[(1)] _[|][v]P[l][,][i][|][.][ We now use approximation bounds from Theorem 2.2]_

on fn[(1)][.]

[P]

_ϵ_

_∥f −f[ˆ]n[(1)][∥][∞]_ _[≤∥][f][ −][f][ (1)]n_ _[∥][∞][+][∥][f][ (1)]n_ _[−]f[ˆ]n[(1)][∥][∞]_ _[≤]_ [2][ · |]8[f][d][|][2][,][∞] _·2[−][2][n]·A(d, n)+_ 2|f _|2,∞_ (l,i) _Un[(1)]_ _|vl,i|,_

X∈


where


where

_d_ 1 + i

_|vl,i| ≤|f_ _|2,∞2[−][d][ X]_ 2[−][i] _·_ _−d_ 1 _≤|f_ _|2,∞._

(l,iX)∈Un[(1)] _i≥0_  _−_ 

Let us now take nϵ = min _n :_ 2|f8|[d]2,∞ 2[−][2][n]A(d, n) 2 . Then, using the above inequality shows

_≤_ _[ϵ]_

that the neural network _f[ˆ]n[(1)]nϵ_ [approximates][ f][ within][ ϵ][ for the infinity norm. We will now estimate]o
the number of neurons in each layer of this network. Note that

_[d][−][1]_

_nϵ ∼_ 2 log 2 1 [log 1]ϵ and 2[n][ϵ] _≤_ 8 _d2 (2 log 2)_ _d4−2 (1_ _d_ 1)! 12 r _|f_ _|ϵ2,∞_ log [1]ϵ  2 _· (1 + O(1))._

_−_

(3)
We can use the above estimates to show that the constructed neural network has at most N1 (resp.
_N2) neurons on the first (resp. second) layer where_


(l,i)∈Un[(1)]


1

2 log 2 [log 1]ϵ



_[d][+1]_

8 _d2 (2 log 2)8√6dd[2]−2 d1_ ! 12 _ϵ_ log [1]ϵ 2 _,_

_[· |][f]_ _[|][2][,][∞]_  

3 [3(][d][−][1)]

8[d/][2](2 log 2)4√3d3(2 _d2−1)_ _d!_ 32 _· [|][f]_ _[|]ϵ[2][,][∞]_ log [1]ϵ 2

 


_N1_
_ϵ_ 0
_∼→_

_N2_
_ϵ_ 0
_∼→_


+1


This proves the bound the number of neurons. Finally, to prove the bound on the number of training
parameters of the network, notice that the only parameters of the network that depend on the function
_f are the parameters corresponding to the weighs vl,i of the sparse grid decomposition. This number_
is |Un[(1)]ϵ _[|][ =][ O][(2][n][ϵ]_ _[n]ϵ[d][−][1]) = O(ϵ[−]_ [1]2 (log [1]ϵ [)] 3(d2−1) ).

B.4 PROOF OF THEOREM 3.4: GENERALIZATION TO GENERAL ACTIVATION FUNCTIONS

We start by formalizing the intuition that a sigmoid-like (resp. ReLU-like) function is a function that
resembles the Heaviside (resp. ReLU) function by zooming out along the x (resp. x and y) axis.

**Lemma B.6. Let σ be a sigmoid-like activation with limit a (resp. b) in −∞** _(resp.+∞). For_
_any δ > 0 and error tolerance ϵ > 0, there exists a scaling M > 0 such that x_ _b_ _a_ _a_
_7→_ _[σ][(][Mx]−_ [)] _−_

_approximates the Heaviside function within ϵ outside of (−δ, δ) for the infinity norm. Furthermore,_
_this function has values in [0, 1]._

_Let σ be a ReLU-like activation with asymptote b · x + c in +∞. For any δ > 0 and error tolerance_
_ϵ > 0, there exists a scaling M > 0 such that x_ _Mb_ _approximates the ReLU function within ϵ_
_7→_ _[σ][(][Mx][)]_

_for the infinity norm._


-----

_Proof. Let δ, ϵ > 0 and σ a sigmoid-like activation with limit a (resp. b) in −∞_ (resp. +∞). There
exists x0 > 0 sufficiently large such that (b−a)|σ(x)−a| ≤ _ϵ for x ≤−x0 and (b−a)|σ(x)−b| ≤_ _ϵ_
for x _x0. It now suffices to take M := x0/δ to obtain the desired result._
_≥_

Now let σ be a ReLU-like activation with oblique asymptote bx in +∞ where b > 0. Let M such
that _σ_ _Mbϵ for x_ 0 and _σ(x)_ _bx_ _Mbϵ for x_ 0. One can check that _[σ][(]Mb[Mx][)]_
_|_ _| ≤_ _≤_ _|_ _−_ _| ≤_ _≥_ _|_ _[| ≤]_ _[ϵ][ for]_

_x_ 0, and _[σ][(]Mb[Mx][)]_ _x_ _ϵ for x_ 0.
_≤_ _|_ _−_ _| ≤_ _≥_

Using this approximation, we reduce the analysis of sigmoid-like (resp. ReLU-like) activations to
the case of a Heaviside (resp ReLU) activation to prove the desired theorem.

**Proof of Theorem 3.4** We start by the class of ReLU-like activations. Let σ be a ReLU-like
activation function. Lemma B.6 shows that one can approximate arbitrarily well the ReLU activation
with a linear map σ. Take the neural network approximator _f[ˆ] of a target function f given by Theorem_
3.1. At each node, we can add the linear map corresponding to x _Mb_ with no additional neuron
_7→_ _[σ][(][Mx][)]_

nor parameter. Because the approximation is continuous, we can take M > 0 arbitrarily large in
order to approximate _f[ˆ] with arbitrary precision on the compact [0, 1][d]._

The same argument holds for sigmoid-like activation functions in order to reduce the problem to
Heaviside activation functions. Although quadratic approximations for univariate functions similar
to Lemma B.3 are not valid for general sigmoid-like activations – in particular the Heaviside — we
can obtain an analog to Lemma B.2 as Lemma B.7 given in the Appendix B.4.1. This results is an
increased number of neurons. In order to approximate a target function f ∈ _X_ [2][,][∞](Ω), we use the
same structure as the neural network constructed for ReLU activations and use the same notations
as in the proof of Theorem 3.1. The first difference lies in the approximation of log φlj _,ij in the_
first layer. Instead of using Corollary B.4, we use Lemma B.7. Therefore, [12]ϵ˜[d] [log][ 3]ϵ˜ [neurons are]

needed to compute ain the approximation of the exponential in the second layer. Again, we use Lemma B.7 to construct ˜ϵ/(3d)-approximation of max(log φlj _,ij_ _, log(˜ϵ/3)). The second difference is_
a ˜ϵ/3-approximation of the exponential on R− with [6]ϵ˜ [neurons for the second layer. As a result,]

the first layer contains at most 2[n][+2 3][d]ϵ˜[2] [log][ 1]ϵ˜ [neurons for][ ϵ][ sufficiently small, and the second layer]

contains _Un[(1)]_ [6]ϵ˜ [neurons. Using the same estimates as in the proof of Theorem 3.1 shows that the]

constructed neural network has at most N1 (resp. N2) neurons on the first (resp. second) layer where

_N1 ∼ϵ→0_ 8 _d2 (2 log 2)3 · 2[5]_ _· dd[5]−2[/] d1[2]_ ! 12 _|fϵ|22332,∞_ log [1]ϵ  _[d][+1]2_ _,_

_N2 ∼ϵ→0_ 8 _d2 (2 log 2)24 · d3(32d2−1)_ _d!_ 23 _· [|][f]ϵ[|]22323,∞_ log [1]ϵ  [3(][d]2[−][1)] _._


This ends the proof.

B.4.1 PROOF OF LEMMA B.7

**Lemma B.7. Let σ be a sigmoid-like activation. Let f : I −→** [c, d] be a right-continuous in_creasing function where I is an interval, and let ϵ > 0. There exists a shallow neural network_
_with activation σ, with at most 2_ _[d][−]ϵ_ _[c]_ _neurons on a single layer, that approximates f within ϵ for the_

_infinity norm._


_Proof. The proof is analog to that of Lemma B.2. Let m =_ _ϵ_

using the monotony ofof the image interval c f ≤, we can define a subdivision ofy1 ≤ _. . . ≤_ _ym ≤_ _d where y ⌊ Ik[d], = x[−][c]1[⌋] c ≤[. We define a regular subdivision] + kϵ. . . ≤ for kxm = 1 such that, . . ., m x, thenk :=_
sup _x_ _I, f_ (x) _yk_ . Let us first construct an approximation neural network _f[ˆ] with the Heaviside_
_{_ _∈_ _≤_ _}_
activation. Consider

_m−1_

_fˆ(x) := y1 + ϵ_ **1** _x_ 0 _._

_−_ _[x][i][ +]2[ x][i][+1]_ _≥_
_i=1_  

X


-----

Let x ∈ [c, d] and k such that x ∈ [xk, xk+1]. We have by monotony yk ≤ _f_ (x) ≤ _yk+1 and_
_yk = y1 + (k −_ 1)ϵ ≤ _f[ˆ](x) ≤_ _y1 + kϵ = yk+1. Hence,_ _f[ˆ] approximates f within ϵ in infinity norm._

Let δ < mini=1,...,m(xi+1 _xi)/4 and σ a general sigmoid-like activation with limits a in_ and
_−_ _−∞_
_b in +_ . Take M given by Lemma B.7 such that _[σ][(]b[Mx]a_ [)] _a approximates the Heaviside function_
_∞_ _−_ _−_

within 1/m outside of (−δ, δ) and has values in [0, 1]. Using the same arguments as above, the
function


_Mx_ _M_ _[x][i][+]2[x][i][+1]_
_−_

_a_
_b_ _a_  _−_
_−_


_m−1_

_i=1_

X


_fˆ(x) := y1 + ϵ_


approximates f within 2ϵ for the infinity norm. The proof follows.

C PROOFS OF SECTION 4

C.1 PROOF OF THEOREM 4.1: APPROXIMATING KOROBOV FUNCTIONS WITH DEEP NEURAL
NETWORKS

Let ϵ > 0. We construct a similar structure to the network defined in Theorem 3.1 by using the
sparse grid approximation of Subsection 2.2. For a given n, let fn[(1)] be the projection of f in the
approximation space Vn[(1)] (defined in Subsection 2.2) and Un[(1)] (defined in equation 2) the set of
indices (l, i) of basis functions present in Vn[(1)]. Recall fn[(1)] can be uniquely decomposed as


_fn[(1)][(][x][) =]_


_vl,iφl,i(x)._


(l,i)∈Un[(1)]

where φl,i = _j=1_ _[φ][l]j_ _[,i]j_ [are the basis functions defined in Subsection 2.2. In the first layer,]
we use the product-approximating neural network given by Proposition 4.2 to compute the ba-we compute exactly the piece-wise linear hat functions φlj _,ij_, then in the next set of layers,

[Q][d]
sis functions φl,i = _j=1_ _[φ][l]j_ _[,i]j_ [(see Figure 3). The output layer computes the weighted sum]

(l,i) _Un[(1)]_ _[v][l][,][i][φ][l][,][i][(][x][)][ and outputs][ f][ (1)]n_ [. Because the approximation has arbitrary precision, we can]
_∈_

[Q][d]

chose the network of Proposition 4.2 such that the resulting networkP _f[ˆ] verifies_ _f_ _fn[(1)]_
_∥_ [ˆ] − _[∥]∞_ _[≤]_ _[ϵ/][2][.]_

More precisely, as φlj _,ij is piece-wise linear with four pieces, we can compute it exactly with four_
neurons with ReLU activation on a single layer (Lemma B.1). Our first layer is composed of the
union of all these ReLU neurons, for the d(2[n] _−_ 1) indices lj, ij such that 1 ≤ _j ≤_ _d, 1 ≤_ _lj ≤_ _n,_
second set of layers is composed of the union of product-approximating neural networks to compute1 ≤ _ij ≤_ 2[l][j] and ij is odd. Therefore, it contains at most d2[n][+2] neurons with ReLU activation. The
_φl,i for all (l, i) ∈_ _Un[(1)][.][ This set of layers contains][ ⌈][log]2_ _[d][⌉]_ [layers with activation][ σ][ and at most]
_Un[(1)]_
_|φl,i with arbitrary precision. Consequently, the final output of the complete neural network is an[| ·][ 8][d][ neurons. The output of these two sets of layers is an approximation of the basis functions]_
approximation of fn[(1)] with arbitrary precision. Similarly to the proof of Theorem 3.1, we can chose
depth at mostthe smallest n log such that2 d + 2 ∥ andf − Nf neurons wheren[(1)][∥]∞ _[≤]_ _[ϵ/][2][ (see equation 3 for details). Finally, the network has]_



[3(][d][−][1)]

2




2[5] _d[5][/][2]_
_N = 8d|Un[(1)]ϵ_ _[| ∼]ϵ→0_ 8 _d2 (2 log 2)·_ 3(d2−1)


_|f_ _|2,∞_


log [1]


_d!_


The parameters of the network depending on the function are exactly the coefficients vl,i of the
sparse grid approximation. Hence, the network has O(ϵ[−] 2[1] (log [1]ϵ [)] 3(d2−1) ) training parameters.


-----

_φ[(1)]_


_−3_ _−2_ _−1_ 0 1 2 3

|−3 −2 −1 0 1 2 3|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|φ(3)||||||||
|||||||||


_φ[(2)]_


_−3_ _−2_ _−1_ 0 1 2 3

Figure 6: Deslaurier-Dubuc interpolets of degree 1, 2 and 3.

D PROOFS OF SECTION 5

D.1 PROOF OF THEOREM 5.2: NEAR-OPTIMALITY OF NEURAL NETWORKS FOR KOROBOV
FUNCTIONS

Our goal is to define an appropriate subspace XN +1 in order to get a good lower bound on the
Bernstein width bN (K), defined in equation 5, which in turn provides a lower bound on the ap_X_
proximation error (Theorem 5.1).

To do so, we introduce the Deslaurier-Dubuc interpolet φ[(][L][)] : R → R. The construction of this
function uses an interpolating scheme on binary rationals. First, φ[(][L][)] is defined on all integers
_φ[(][L][)](k) = 1k=0 for k_ Z. Then, we define the function on half integers [2][k]2[+1] by fitting a polyno_∈_

mial of degree 2L − 1 interpolating the standard hat function φ at k − _L + 1, · · ·, k + L. Iteratively,_
we define the interpolet on binary rationals of the form [2]2[k][j][+1][+1][ from the value of the interpolet on ratio-]

nals with denominator 2[j]. Specifically, let Pj,k[(][L][)] [be the unique polynomial of degree][ 2][L][ interpolating]

_φ[(][L][)]_ at points 2[k][j][′][ for][ k][′][ ∈{][k][ −] _[L][ + 1][,][ · · ·][, k][ +][ L][}][. We set]_

2k + 1 2k + 1
_φ[(][L][)]_ 2[j][+1] := Pj,k[(][L][)] 2[j] _._
   

For example, for L = 2 we get φ[(2)]( [2]2[k][j][+1][+1][ ) :=] 169 _[φ][(2)][(][ k]2[j][ ) +]_ 169 _[φ][(2)][(][ k]2[+1][j][ )][ −]_ 161 _[φ][(2)][(][ k]2[−][j][ )][2]_ _[ −]_

1

16 _[φ][(2)][(][ k]2[+3][j][ )][. This process defines the interpolet on binary rationals. We then extend the func-]_

tion to the real line by continuity. See Figure 6 for an illustration. Deslauriers & Dubuc (1989)
proved that the regularity of the interpolet is an increasing function of L the degree of interpolation.
We now prove results in the case L = 2.

**Lemma D.1. The interpolet of degree 2, φ[(2)]** _is C[2]_ _and has support Supp_ _φ[(2)][]_ = [−3, 3].
 

_Proof. See Appendix D.2.1._

We will now use the interpolet φ[(2)] to construct the subspace XN +1. Using the sparse grids approach, we can construct a hierarchical basis in X [2][,][∞](Ω) using the interpolet φ[(2)] as mother function. In the remaining of the proof, we will write φ instead of φ[(2)] for simplicity, and use the
notations and definitions related to sparse grids, introduced in Subsection 2.2. Because EN (K) is
decreasing in N, it suffices to show the result for Nn when we define our space XN +1 to be exactly
the approximating space Vn[(1)] of sparse grids XNn+1 := Vn[(1)]. The following equation establishes
the relation between n and Nn. In the following, for simplicity, we will write N instead of Nn.


_n−1_ _d_ 1 + i

2[i][−][d] _−_
_d_ 1
_i=0_  _−_

X


_d_ 1
_n_ _−_
1 = 2[n]
_−_ _·_ (d 1)! [+][ O][(][n][d][−][2][)]
 _−_


_N = dim(Vn[(1)])_ 1 =
_−_


2[|][l][|][1][−][1]−1 =
_|l|1≤Xn+d−1_


-----

First, let us give some properties about the subspace XN +1.

**Proposition D.2. Let u ∈** _XN_ +1 and write it in decomposed form u = **_l,i_** _[v][l][,][i][ ·][ φ][l][,][i][, where the]_

_sum is taken over all multi-indices corresponding to basis functions of XN_ +1. The coefficients vl,i
_can be computed in the following way._

[P]

_d_

_vl,i =_ _Ilj_ _,ij_ _u =: Il,iu_

 

_j=1_

Y
 

_where Ilj_ _,ij_ _u = u(_ 2[i][j][j][ )][ −] 16[9] _[u][(][ i][j]2[−][j][ )][1]_ _[ −]_ 16[9] _[u][(][ i][j]2[+1][j][ ) +][ 1]16_ _[u][(][ i][j]2[−][j][ ) +][3]_ [ 1]16 _[u][(][ i][j]2[+3][j][ )][. Here,][ I][l][,][i][ denotes the]_

_d-dimensional stencil which gives a linear combination of values of u at 5[d]_ _nodal points._


_Proof. See Appendix D.2.2._

Note that the stencil representation of the coefficients gives directly |vl,i| ≤ 5[d]∥u∥∞. We are now
ready to make our estimates. The goal is to compute sup _ρ : ρU_ (XN +1) _K_, which will lead
_{_ _⊂_ _}_
to a bound on bN (K) . To do so, it suffices to upper bound the Korobov norm by the L[∞] norm
_X_
for elements of XN +1. In fact, if Γd > 0 satisfies for all u ∈ _U_ (XN +1), |u|X 2,∞ _≤_ Γd∥u∥∞, then
_bN_ (K)X ≥ 1/Γd.

Now take u ∈ _XN_ +1 and let us write u = **_l,i_** _[v][l][,][i][ ·][ φ][l][,][i][. Note that basis functions in the same]_

hierarchical class Wl are almost disjoint. More precisely, at each point x ∈ Ω, at most 3[d] basis
functions φl,i have non-zero contribution to u[P](x). Therefore, for any 0 **_α_** 2 **1,**
_≤_ _≤_ _·_


_D[α]u_ 3[d] max
_∥_ _∥∞_ _≤_ **1** **_i_** 2[l] **1, i odd**

_|l|1≤Xn+d−1_ _≤_ _≤_ _−_ _[|][v][l][,][i][| ·][ 2][⟨][α][,][l][⟩][∥][D][α][φ][∥][∞]_

_n−1_ _d_ 1 + i

= 60[d]|φ|X 2,∞(Ω)∥u∥∞ _·_ 2[2][i] _−d_ 1

_i=0_  _−_ 

X

60[d]
= _n[d][−][1]_ + (n[d][−][2]) _u_ _._

(d 1)! _[|][φ][|][X]_ [2][,][∞][(Ω)][ ·][ 2][2][n][  ] _O_ _∥_ _∥∞_
_−_


60[d]
Finally, denoting by Cd the constant (d 1)! _[|][φ][|][X]_ [2][,][∞][(Ω)][, we get for][ n][ sufficiently large]

_−_

1 1
_bN_ (K)X ≥ 2Cd _·_ 2[2][n] _n[d][−][1][ .]_

_·_

Furthermore, recall1)!(log 2)[d][−][1] (log NN N)[d] =[−][1][ .][ Finally we obtain for some constant](d−11)! [2][n][ ·][ n][d][−][1][ ·]  1 + O   1n  _. Therefore,[ c][d][ >][ 0][,] n ∼_ [log]log 2[ N] [, and][ 2][n][ ∼] [(][d][ −]

_·_


1

_N_ [2][ (log][ N] [)][d][−][1][.]


_bN_ (K)X ≥ _cd_


We conclude by analyzing the minimum number of parameters in order to get an ϵ-approximation of
the Korobov unit ball K. Define nϵ := min{n : _C2d_ 2[2][n]·1n[d][−][1][ ≤] _[ϵ][}][. This yields][ n][ϵ][ ∼]_ 2 log 21 [log][ 1]ϵ [,]

and 2[n] _∼_ _C2d_ [(2 log 2)][d][−][1] _√ϵ·(log1 1ϵ_ [)] _d−2_ 1 [. The number of needed parameters to obtain an][ ϵ][ approx-]

imation Kq is therefore

_[d][−][1]_

2

_Nϵ_ _Cd_ [1] log [1] _,_
_∼_ [˜] _·_ _√ϵ_ _ϵ_

 

for some constant _C[˜]d > 0._

Interestingly, the subspace XN +1 our proof uses to show the lower bound is essentially the same as
the subspace we use to approximate Koborov functions in our proof of the upper bounds (Theorem
3.1, 4.1). The difference is in the choice of the interpolate φ to construct the basis functions, degree
1 for the former, and 2 for the later.


-----

D.2 MISSING PROOFS OF SUBSECTION D.1

D.2.1 PROOF OF LEMMA D.1

**Lemma D.3. The interpolet of degree 2, φ[(2)]** _is C[2]_ _and has support Supp_ _φ[(2)][]_ = [−3, 3].
 

_Proof. To analyze the regularity of the interpolet, we introduce the trigonometric polynomial_

_k_

_P_ [(2)](θ) := _φ[(2)]_ _e[ikθ]_ = 1 + [9]

2 16 [(][e][iθ][ +][ e][−][iθ][)][ −] 16[1] [(][e][3][iθ][ +][ e][−][3][iθ][)][.]

_kX∈Z_  


We can write P [(2)](θ) as


4
#


sin θ

_P_ [(2)](θ) = " sin _θ2_ # _· S(θ),_

where S(θ) = [1]4 _[−]_ 16[1] [(][e][iθ][ +][ e][−][iθ][)][ is a trigonometric polynomial of degree]   [ 1][. Deslaurier and Dubuc]

(Deslauriers & Dubuc, 1989, Theorem 7.11) showed that the interpolet φ[(2)] has regularity log 2
_−_ [log][ r]

where r is the spectral radius of the matrix B := [sj 2k] 1 _j,k_ 1 where sj are the coefficients ofj k
_−_ _−_ _≤_ _≤_
the trigonometric polynomial S(θ). In our case, matrix B writes

_−1/16_ _−1/16_ 0

_B =_ 0 1/4 0 _,_

0 _−1/16_ _−1/16!_

and has spectral radius r = 1/4. Therefore, the regularity of φ[(2)] is at least log 2 = 2. For the
_−_ [log][ r]

support, one can check that more generally Supp(φ[(][L][)]) = [−2L + 1, 2L − 1]j. k


_P_ [(2)](θ) =


D.2.2 PROOF OF PROPOSITION D.2

**Proposition D.4. Let u ∈** _XN_ +1 and write it in decomposed form u = **_l,i_** _[v][l][,][i][ ·][ φ][l][,][i][, where the]_

_sum is taken over all multi-indices corresponding to basis functions of XN_ +1. The coefficients vl,i
_can be computed in the following way._

[P]


_vl,i =_ _Ilj_ _,ij_ _u =: Il,iu_

 

_j=1_

Y
 

_where Ilj_ _,ij_ _u = u(_ 2[i][j][j][ )][ −] 16[9] _[u][(][ i][j]2[−][j][ )][1]_ _[ −]_ 16[9] _[u][(][ i][j]2[+1][j][ ) +][ 1]16_ _[u][(][ i][j]2[−][j][ ) +][3]_ [ 1]16 _[u][(][ i][j]2[+3][j][ )][. Here,][ I][l][,][i][ denotes the]_

_d-dimensional stencil which gives a linear combination of values of u at 5[d]_ _nodal points._

_Proof. We start by looking at the 1-dimensional case. One can check that Il,iφ˜l,[˜]i_ [=][ 1][˜]l=l,[˜]i=i[. Indeed,]
if [˜]l > l of if [˜]l = l and [˜]i ̸= i, then φ˜l,[˜]i [will be zero at the nodal values of][ I][l,i][. Further, if][ ˜]l < l the
iterative construction of φ gives directly.

_i_ _i + 1_ _i_ 1 _i + 3_ _i_ 3

_φ˜l,[˜]i_ 2[j] = 16[9] _[φ][˜]l,[˜]i_ 2[j] + 16[9] _[φ][˜]l,[˜]i_ _−2[j]_ _−_ 16[1] _[φ][˜]l,[˜]i_ 2[j] _−_ 16[1] _[φ][˜]l,[˜]i_ _−2[j]_ _._

         

Therefore, we obtainproves the stencil representation of Il,iφ˜l,[˜]i [= 0][. Finally,] vl,i for dimension[ I][l,i][u][ =][ P] 1˜l,. Finally, using the tensor product approach[˜]i _[v][˜]l.[˜]i_ _[·][ I][l,i][φ][˜]l,[˜]i_ [=][ v][l,i][ ·][ I][l,i][φ][l,i][ =][ v][l,i][.][ This]
of the stencil operator Il,i we can generalize the formula to general dimensions.


-----

