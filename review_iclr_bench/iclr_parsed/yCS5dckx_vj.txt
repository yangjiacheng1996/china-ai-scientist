# TOWARDS DEMYSTIFYING REPRESENTATION LEARN## ING WITH NON-CONTRASTIVE SELF-SUPERVISION

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Non-contrastive methods of self-supervised learning (such as BYOL and SimSiam) learn representations by minimizing the distance between two views of the
same image. These approaches have achieved remarkable performance in practice, but it is not well understood how the representation is learned based on the
augmentation process. Tian et al. (2021) explained why the representation does
not collapse to zero and proposed DirectPred that sets the predictor directly. In
our work, we analyze a generalized version of DirectPred, called DirectSet(α).
We show that in a simple linear network, DirectSet(α) provably learns a desirable
projection matrix and also reduces the sample complexity on downstream tasks.
Our analysis suggests that weight decay acts as an implicit threshold that discard
the features with high variance under augmentation, and keep the features with
low variance. Inspired by our theory, we simplify DirectPred by removing the expensive eigen-decomposition step. On CIFAR-10, CIFAR-100, STL-10 and ImageNet, DirectCopy, our simpler and more computationally efficient algorithm,
rivals or even outperforms DirectPred.

1 INTRODUCTION

Self-supervised learning recently emerges as a promising direction to learn representations without
manual labels. While contrastive learning (Oord et al., 2018; Tian et al., 2019; Bachman et al.,
2019; He et al., 2020; Chen et al., 2020a) minimizes the distance of representation between positive pairs, and maximizes such distances between negative pairs, recently, non-contrastive selfsupervised learning (abbreviated as nc-SSL) is able to learn nontrivial representation with only
positive pairs, using an extra predictor and a stop-gradient operation. Furthermore, the learned
representation shows comparable (or even better) performance for downstream tasks (e.g., image
classification) (Grill et al., 2020; Chen & He, 2020). This brings about two fundamental questions: (1) why the learned representation does not collapse to trivial (i.e., constant) solutions, and
(2) without negative pairs, what representation nc-SSL learns from the training and how the learned
representation reduces the sample complexity in downstream tasks.

While many theoretical results on contrastive SSL (Arora et al., 2019; Lee et al., 2020; Tosh et al.,
2020; Wen & Li, 2021) do exist, similar study on nc-SSL has been very rare. As one of the first
work towards this direction, Tian et al. (2021) show that while the global optimum of the noncontrastive loss is indeed a trivial one, following gradient direction in nc-SSL, one can find a local
optimum that admits a nontrivial representation. Based on their theoretical findings on gradientbased methods, they proposed a new approach, DirectPred, that directly sets the predictor using the
eigen-decomposition of the correlation matrix of input before the predictor, rather than updating it
with gradient methods. As a method for nc-SSL, DirectPred shows comparable or better performance in multiple datasets, including CIFAR-10 (Krizhevsky et al., 2009), STL-10 (Coates et al.,
2011) and ImageNet (Deng et al., 2009), compared to BYOL (Grill et al., 2020) and SimSiam (Chen
& He, 2020) that optimize the predictor using gradient descent.

While Tian et al. (2021) address the first question, i.e., why the learned representation does not
collapse, they do not address the second question, i.e., what representation is learned in nc-SSL and
how the learned representation is related to the data distribution and augmentation process and in
turn whether it reduces the sample complexity in downstream tasks.


-----

**Main Contributions.** In this paper, we make a first attempt towards the second question, by studying a family of algorithms named DirectSet(α), in which the DirectPred algorithm proposed by Tian
et al. (2021) is a special case with α = 1/2. Our contribution is two-folds:

First, we perform a theoretical analysis on DirectSet(α) with linear networks. Our analysis shows
that there exists an implicit threshold, determined by weight decay parameter η, that governs which
features are learned and which are discarded. More specifically, the threshold is applied to the variance of the feature across different data augmentations (or “views”) of the same instance: nuisance
_features (features with high variances under augmentation) are discarded, while invariant features_
(i.e., with low variances) are kept. We further make a formal statement on the sample complexity
of the learning process and performance guarantees of the downstream tasks, in the linear setting
similar to Tian et al. (2021). To our knowledge, this is the first sample complexity result in nc-SSL.

Second, we show that DirectCopy, a special case of DirectSet(α) when α = 1, performs comparably
with (or even outperforms) DirectPred in downstream tasks in CIFAR-10, CIFAR-100, STL-10
and ImageNet. In DirectCopy, the predictor can be set without the expensive eigen-decomposition
operation, which makes DirectCopy much simpler and more efficient than DirectPred.

**Related works. In nc-SSL, different techniques are proposed to avoid collapsing. BYOL and Sim-**
Siam use an extra predictor and stop gradient operation. Beyond these, BatchNorm (including its
variants (Richemond et al., 2020)), de-correlation (Zbontar et al., 2021; Bardes et al., 2021; Hua
et al., 2021), whitening (Ermolov et al., 2021), centering (Caron et al., 2021), and online clustering (Caron et al., 2020) are all effective ways to enforce implicit contrastive constraints among
samples for collapsing prevention. We study BYOL and SimSiam as representative nc-SSL methods.

**Organization. The paper is organized as follows. Section 2-3 introduce DirectSet(α), prove it**
learns a projection matrix onto the invariant features, and the learned representation reduces sample
complexity in downstream tasks. Section 4 demonstrates that DirectCopy achieves comparable or
even better performance than the original DirectPred algorithm in various datasets, and Section 5
shows ablation experiments. Finally, limitation and future works are discussed in Section 6-7.

2 PRELIMINARIES

2.1 NOTATIONS
We use Id to denote the d×d identity matrix and simply write I when the dimension is clear. For any
linear subspacethe projection matrix S in R P[d]S, we use equals UU PS[⊤] ∈, where the columns ofR[d][×][d] to denote the projection matrix on U constitute a set of orthonormal bases S. More precisely,
for subspace S. We use N (µ, Σ) to denote the Gaussian distribution with mean µ and covariance Σ.

We use ∥·∥ to denote spectral norm for a matrix, or ℓ2 norm for a vector and use ∥·∥F to denote
Frobenius norm for a matrix. For a real symmetric matrixd _A ∈_ R[d][×][d] whose eigen-decomposition is
_i=1_ _[λ][i][u][i][u]i[⊤][,][ we use][ |][A][|][ to denote][ P]i[d]=1_ _[|][λ][i][|][ u][i][u]i[⊤][. If][ A][ is also positive semi-definite, we use][ A][α]_

to denoteP _i=1_ _[λ]i[α][u][i][u]i[⊤]_ [for any positive][ α][ ∈] [R][.]

2.2 DIRECT[P][d] SET(α) AND DIRECTCOPY

In nc-SSL, recent methods as BYOL (Grill et al., 2020) and SimSiam (Chen & He, 2020) employ a
dual pair of Siamese networks (Bromley et al., 1994): one side is a composition of an online network
(including a projector) and a predictor network, the other side is a target network (see Figure 1 for
a simple example). The target network has the same architecture as the online network, but has
potentially different weights. Given an input x, two augmented views x1, x2 are generated, and the
network is trained to match the representation of x1 (through the online network and the predictor
network) and the representation of x2 (through the target network). More precisely, suppose the
online network and the target network are two mappings fθ, fθa : R[d] _7→_ R[h] and the predictor
network is a mapping gθp : R[h] _7→_ R[h], the network is trained to minimize the following loss:

2

_gθp (fθ(x1))_ _fθa_ (x2)

_L(θ, θp, θa) := [1]_ _._

2 [E][x][1][,x][2] _gθp (fθ(x1))_ _[−]_ [StopGrad]  _∥fθa_ (x2)∥ 

In BYOL and SimSiam, the online network and the target network are trained by running gradient
methods on L. The target network is not trained by gradient methods; instead, it is directly set with


-----

BYOL: ℓ% loss
Gradient methods 𝑔!"(𝑓! [𝑥]" [)]

Stop-Gradient

Predictor

DirectSet(𝛼):

𝑊" [(𝜃]"[)]

𝐹[%]
𝑊" [=]

∥𝐹[%] ∥ [+ 𝜖𝐼,] 𝑓![(𝑥]"[)] 𝑓!![(𝑥]#[)]

with 𝐹= 𝔼&!𝑓' [𝑥]# [𝑓]' [𝑥]# [(]

Online Target

Gradient EMA of 𝑊
methods 𝑊(𝜃) 𝑊! [(𝜃]![)]


𝑥# Augmentation 𝑥$

𝑥

Figure 1: Problem Setup. Comparison between BYOL and DirectSet(α) on a linear network.

the weights in the online network (Chen & He, 2020) or an exponential moving average (EMA) of
the online network (Grill et al., 2020; He et al., 2020; Chen et al., 2020b).

The DirectSet(α) algorithm, as shown in Figure 1, directly sets the predictor based on the correlation
matrix F of the predictor inputs:

_F_ _[α]_
_Wp =_

_∥F_ _[α]∥_ [+][ ϵI,]

where F = Ex1 _fθ(x1)fθ(x1)[⊤]. In practice, F is estimated by a moving average over batches. That_
is, _F[ˆ] = µF[ˆ] + (1 −_ _µ)EB[fθ(x1)fθ(x1)[⊤]], where EB is the expectation over one batch._

In the original DirectPred proposed by Tian et al. (2021), α is fixed at 1/2. To compute _F[ˆ][1][/][2], one_
needs to first compute the eigen-decomposition of _F[ˆ], and then taking the root of each eigenvalue._
This step of eigen-decomposition can be expensive especially when the representation dimension
_h is high. To avoid the eigen-decomposition step, we propose DirectCopy (α = 1), in which the_
predictor Wp is a direct copy of the _F[ˆ] (with normalization and regularization)[1]. As we shall see,_
DirectCopy enjoys both theoretical guarantees and strong empirical performance.

3 THEORETICAL ANALYSIS OF DIRECTSET(α)

Deep linear networks have been widely used as a tractable theoretical model for studying nonconvex
loss landscapes (Kawaguchi, 2016; Du & Hu, 2019; Laurent & Brecht, 2018) and nonlinear learning
dynamics (Saxe et al., 2013; 2019; Lampinen & Ganguli, 2018; Arora et al., 2018). However, most
of them are for supervised learning setting. Tian et al. (2021) analyzed nc-SSL on a linear network,
but did not analyze their proposed approach DirectPred. Here, we analyze the representation learning process of DirectSet(α) on a minimal setting where the online network fθ is a single linear layer.
We also verify DirectSet(α) works for practical nonlinear deep models and realistic datasets.

3.1 SETUP

In this subsection, we define the network model, data distribution and simplify DirectSet(α) algorithm for our theoretical analysis. We consider the following network model (see Figure 1),
**Assumption 1 (Linear network model). The online, predictor and target network are all single-layer**
_linear network without bias, with weight matrices denoted as W, Wp, Wa ∈_ R[d][×][d] _respectively._

For the data distribution, we assume the input space is a direct sum of a invariant feature subspace
and a nuisance feature subspace. Specifically, we assume
**Assumption 2 (Data distribution). The input x is sampled from N** (0, Id), and its augmented view
_x1, x2 are independently sampled from N_ (x, σ[2]PB), where B is a (d − _r)-dimensional subspace._
_We denote S as the orthogonal subspace of B in R[d]._

1Computing the spectral norm of ˆF is much faster than computing the eigen-decomposition of _F[ˆ], because_
the former only needs the top eigen-vector of _F[ˆ]. Table 4 shows that the spectral norm can also be replaced by_
Frobenius norm or no normalization, and similar performance can be achieved.


-----

In this simple data distribution, subspace S corresponds to the features that are invariant to augmentations and its orthogonal subspace B is the nuisance subspace which the augmentation changes. We
will prove that DirectSet(α) can learn the projection matrix onto S subspace. Note in the previous
work (Tian et al., 2021), they assumed the covariance of the augmentation distribution to be σ[2]I and
did not study what representation is learned.

For the convenience of analysis, we consider a simplified version of DirectSet(α). We compute the
loss function without normalizing the two representations, so the population loss is

_L(W, Wa, Wp) := [1]_ (1)

2 [E][x][1][,x][2][ ∥][W][p][Wx][1][ −] [StopGrad][ (][W][a][x][2][)][∥][2][,]

and the empirical loss is

_n_

2

_Lˆ(W, Wp, Wa) := [1]_ _WpWx[(]1[i][)]_ 2 [)] _,_ (2)

2n _i=1_ _[−]_ [StopGrad][(][W][a][x][(][i][)]

X

where x[(][i][)]’s are independently sampled from (0, I), and augmented views x[(]1[i][)] and x[(]2[i][)] are inde_N_
pendently sampled from (x[(][i][)], σ[2]PB). To train our model, first, we initialize W as δI with δ a
_N_
positive real number. We run gradient flow or gradient descent on online network W with weight
decay η, and set the the target network Wa = W. For clarity of presentation, when training on the
practice; when training on the empirical loss, we setpopulation loss, we set Wp as (W Exxx[⊤]W _[⊤])[α]_ = ( WWWp as[⊤] (W)[α] _n[1]instead ofni=1_ _[x][(] ([i][)]W[[][x][(]E[i][)]x[]]1[⊤]x[W]1x[ ⊤][⊤]1[)][W][α][.][ ⊤][ Here, we][)][α][ as in]_

set the predictor regularization ϵ = 0 and its influence will be studied in Section 5.
P

In the following, DirectSet(α) is shown to recover the projection matrix PS with polynomial number
of samples. Furthermore, given that the learned matrix is close to PS, the sample complexity on
downstream tasks is reduced.

3.2 GRADIENT FLOW ON POPULATION LOSS


In this section, we show that DirectSet(α) running on the population loss with infinitesimal learning
rate and η weight decay can learn the projection matrix onto the invariant feature subspace S.
**Theorem 1. Suppose network architecture and data distribution follow Assumption 1 and Assump-**
_tion 2, respectively. Suppose we initialize online network W as δI, and run DirectSet(α) on popu-_
_lation loss (see Eqn. 1) with infinitesimal step size and η weight decay. If we set the weight decay_

1/(2α)

_coefficient η_ 4(1+1σ[2]) _[,][ 1]4_ _and initialization scale δ >_ 1−[√]21−4η _, then W converges to_
_∈_

1/(2α)

1+[√]21−4η  _PS when time goes to infinity._  
 

Theorem 1 shows that when the weight decay is in certain range, and when the initialization is
large enough, the online network can converge to the desired projection matrix PS [2]. In sequel,
we explain how the dynamics of W leads to a projection matrix and how the weight decay and
initialization scale come into play. We leave the full proof in Appendix B.1. We also consider the
setting when Wp is set as (W Ex1 _x1x[⊤]1_ _[W][ ⊤][)][α][ in Appendix B.4 and extend the result to deep linear]_
networks in Appendix C.

Due to the identity initialization, we can ensure that W is always a real symmetric matrix and is
simultaneously diagonalizable with PB. We can then analyze the evolution of each eigenvalue in W
separately. Under our assumptions, it turns out that all the eigenvalues whose eigenvectors lie in the
_B subspace share the same value λB, and all the eigenvalues in the S subspace share the value λS_
as shown in the following time dynamics:

_λ˙_ _B = λB_ _−(1 + σ[2]) |λB|[4][α]_ + |λB|[2][α] _−_ _η_ _,_ _λ˙_ _S = λS_ _−|λS|[4][α]_ + |λS|[2][α] _−_ _η_ _._ (3)
h i h i

Next, we show λB converges to zero and λS converges to a positive number, which immediately
implies that W converges to some scaling of PS.

1/(2α)

2Note that Theorem 1 also holds with negative initialization δ < 1−[√]21−4η _, in which case W_
_−_

1/(2α)

1+[√]1 4η  
converges to 2 _−_ _PS. Our other results can be extended to negative δ in a similar way._
_−_
 


-----

|O|Col2|𝜆 "# 𝜆 "$ 𝜆"|
|---|---|---|


evolvement of the eigenvalues of F when it’s trained by DirectCopy with ϵ = 0.2 on STL-10. With weight

𝜆!

O 𝜆!

𝜆"

Bad Good
Basin Basin

O 𝜆"# 𝜆"$ 𝜆"

Figure 2: Left: With appropriate weight decay, λB always converge to zero; λS converges to zero when it’s
initialized in the bad basin and converges to positive λ[+]S [when it’s initialized in the good basin.][ Middle:][ The]

decay η = 0.0004 (bottom), the eigen-spectrum at epoch 95 has sharp drop; while the drop is much milder
when η = 0 (top). Right: Similar phenomenon on CIFAR-10 with ϵ = 0.3.

Similar as the analysis in Tian et al. (2021), when η > 4(1+1σ[2]) _[,][ we know][ ˙]λB < 0 for any λB > 0_

and λB = 0 is a stable stationary point, as illustrated in Figure 2 (top, left). Therefore, as long as
_η >_ 4(1+1σ[2]) [,][ λ][B][ must converge to zero. When][ 0][ < η <][ 1]4 _[,][ there are three non-negative solutions]_

1/(2α) 1/(2α)

to _λ[˙]_ _S = 0, which are 0, λ[−]S_ [=] 1−[√]21−4η and λ[+]S [=] 1+[√]21−4η _. As illustrated in_

Figure 2 (bottom, left), if initialization _δ > λ_ _[−]S_ [(good basin),][ λ][S][ converges to a positive value] _[ λ]S[+][;]_
if 0 < δ < λ[−]S [(bad basin),][ λ][S][ converges to zero.]

**Thresholding role of weight decay in feature learning:** While Tian et al. (2021) shows why ncSSL does not collapse, one key question is how nc-SSL learns useful features and how the method
determines which feature is learned. Now it is clear: the weight decay factor η makes a call on what
features should be learned. Nuisance features subject to significant change under data augmentation
has larger variance σ[2] and 4(1+1σ[2]) _[< η][, the eigenspace corresponds to this feature goes to zero;]_

on the other hand, invariant features that are robust to data augmentation has much smaller σ[2] and
1

4(1+σ[2]) _[> η][ and these features are kept. In our above analysis,][ B][ subspace corresponds to the]_
nuisance features and collapses to zero; S subspace corresponds to the invariant features (whose
variance was assumed as zero for simplicity) and is kept after training.

Figure 2 (middle and right) shows the spectrum of F (which is the correlation matrix of the predictor
inputs) when the network is trained by DirectSet(1) with and without weight decay η on STL10
and CIFAR10: when η = 0, the eigen-spectrum of F in later epochs does not have a sharp drop
compared with the case of η = 0.0004. This means that the nuisance features are not significantly
suppressed when η = 0.

Therefore, it is crucially important to choose weight decay appropriately: a too small η may not be
sufficient to suppress the nuisance features; a too large η can also collapse the invariant features. As
shown in Section 5, both cases lead to worse downstream performance.

3.3 GRADIENT DESCENT ON EMPIRICAL LOSS

In this section, we then proceed to prove that DirectCopy (one special case of DirectSet(α) with
_α = 1) successfully learns the projection matrix given polynomial number of samples._
**Theorem 2. Suppose network architecture and data distribution are as defined in Assumption 1**
_and Assumption 2, respectively. Suppose we initialize online network as δI, and run DirectCopy_
_on empirical loss (see Eqn. 2) with γ step size and η weight decay. Suppose the noise scale σ[2]_

_is a positive constant, the weight decay coefficient η_ 4(1+1+σ[2]σ/[2]4) _[,][ 1+3]4(1+[σ]σ[2][/][2])[4]_ _and the initialization_
_∈_
 


-----

_scale δ is a constant at least 1/√2. Choose the step size γ as a small enough constant. For any_

_accuracy ˆϵ > 0, given n ≥_ _poly(d, 1/ϵˆ) number of samples, with probability at least 0.99 there_
_exists t = O(log(1/ϵˆ)) such that (here_ _Wt is the online network weights at the t-th step):_

1 + 1 4η

_Wt_ [f] _[√]_ _−_ _PS_ _ϵ._
_−_ r 2
f _[≤]_ [ˆ]

The proof proceeds by first proving that gradient descent on the population loss converges in linear
rate and then couples the gradient descent dynamics on empirical loss and that on population loss.
See the detailed proof in Appendix B.2.

3.4 SAMPLE COMPLEXITY ON DOWNSTREAM TASKS

In this section, we show that the learned representations can indeed reduce the sample complexity
on the downstream tasks. We consider the following data distribution for the down-stream task:

**Assumption 3 (Downstream data distribution). Each input x[(][i][)]** _is sampled from N_ (0, Id) and its
_label y[(][i][)]_ = _x[(][i][)], w[∗]_ + ξ[(][i][)], where w[∗] _is the ground truth vector with unit ℓ2 norm and ξ[(][i][)]_ _is_
_independently sampled from N_ (0, β[2]). We assume the ground truth w[∗] _lies on an r-dimensional_
_subspace S and we denote the projection matrix on subspace S simply as P_ _._

In practice, usually the semantically relevant features (S subspace here) are invariant to augmentations and the nuisance features (orthogonal subspace of S) have high variance under augmentations.
Therefore, by previous analysis, we expect DirectSet(α) to learn the projection matrix P.

Suppose {(x[(][i][)], y[(][i][)])}i[n]=1 [are][ n][ training samples. Each input][ x][(][i][)][ is transformed by a matrix][ ˆ]P ∈
R[d][×][d] (for example the learned online network W ) to get its representation _Px[ˆ]_ [(][i][)]. The regularized

2

loss is then defined as _L[ˆ](w) :=_ 21n _ni=1_ _Pxˆ_ [(][i][)], w _y[(][i][)]_ + _[ρ]2_

_−_ _[∥][w][∥][2][ .][ In the below theorem,]_

we show that when _P_ _P_ P D E
_−_ [ˆ] _F_ [is small, the above ridge regression can recover the ground truth][ w][∗]

given only O(r) number of samples.

**Theorem 3. Suppose the downstream data distribution is as defined in Assumption 3. Suppose**
_P[ˆ]_ _P_ _ϵ with ˆϵ < 1. Choose the regularizer coefficient ρ = ˆϵ[1][/][3]. For any ζ < 1/2, given_

_n ≥ −O(r +log(1F_ _[≤]_ [ˆ] _/ζ)) number of samples, with probability at least 1_ _−_ _ζ, the training loss minimizer_
_wˆ satisfies_

_√r +_ log(1/ζ)

_P[ˆ] ˆw −_ _w[∗]_ _≤_ _O_ _ϵˆ[1][/][3]_ + β p√n ! _._

In the above theorem, when n is at least O _β[2](r+log(1ϵˆ[2][/][3]_ _/ζ))_ _, we have_ _P[ˆ] ˆw_ _w[∗]_ _O(ˆϵ[1][/][3])._

_−_ _≤_

Note that if we directly estimate ˆw without transforming the inputs by  _P[ˆ], we need Ω(d) number_
of samples to ensure that ∥wˆ − _w[∗]∥≤_ _o(1) (Wainwright, 2019). The proof of Theorem 3 follows_
from bounding the difference between _P[ˆ] ˆw and w[∗]_ by matrix concentration inequalities and matrix
perturbation bounds. The full proof is in Appendix B.3.

4 EMPIRICAL PERFORMANCE OF DIRECTCOPY

In the previous analysis, we show DirectSet(α), and in particular DirectCopy (DirectSet(α) with
_α = 1), could recover the input feature structure with polynomial samples and make the down-_
stream task more sample efficient in a simple linear setting. Compared with the original DirectPred
(DirectSet(α) with α = 1/2), DirectCopy is a simpler and computationally more efficient algorithm
since it directly set the predictor as the correlation matrix F, without the eigen-decomposition step.
By our analysis in Theorem 1, DirectCopy also learns the projection matrix PS with larger scale [3]

1/(2α)

3Recall that in Theorem 1 under DirectSet(α), online matrix W converges to 1+[√]21−4η _PS. So_

with a larger α, the scalar in front of PS becomes larger.  


-----

compared with DirectPred, which suggests that the invariant features learned by DirectCopy are
stronger and more distinguishable. Next, we show that DirectCopy is on par with (or even outperforms) the original DirectPred in various datasets, when coupling with deep nonlinear models on
real datasets.

4.1 RESULTS ON STL-10, CIFAR-10 AND CIFAR-100

We use ResNet-18 (He et al., 2016) as the backbone network, a two-layer nonlinear MLP as the
projector, and a linear predictor. Unless specified otherwise, SGD is used as the optimizer with
weight decay η = 0.0004. To evaluate the quality of the pre-trained representations, we follow
the linear evaluation protocol. Each setting is repeated 5 times to compute the mean and standard
deviation. The accuracy is reported as “mean±std”. Unless explicitly specified, we use learning rate
_γ = 0.01, regularization ϵ = 0.2 on STL-10; γ = 0.02, ϵ = 0.3 on CIFAR-10 and γ = 0.03, ϵ = 0.3_
on CIFAR-100. See more detailed experiment settings in Appendix A.

_STL-10_

|Col1|Num of epochs 100 300 500|
|---|---|


|DirectCopy DirectPred DirectPred (freq=5) SGD baseline|77.83±0.56 77.86±0.16 77.54±0.11 75.06±0.52|82.01±0.28 78.77±0.97 79.90±0.66 75.25±0.74|82.95±0.29 78.86±1.15 80.28±0.62 75.25±0.74|
|---|---|---|---|


|Col1|epochs 100|
|---|---|



_CIFAR-100_ Table 2: ImageNet Top-1 accu
|DirectCopy DirectPred DirectPred (freq=5) SGD baseline|84.02±0.37 85.21±0.23 84.93±0.29 84.49±0.20|89.17±0.12 88.88±0.15 88.83±0.10 88.57±0.15|89.62±0.10 89.52±0.04 89.56±0.13 89.33±0.27|
|---|---|---|---|


|DirectCopy DirectPred SGD Baseline|68.8 68.5 68.6|
|---|---|


|DirectCopy DirectPred DirectPred (freq=5) SGD baseline|55.40±0.19 56.60±0.27 56.43±0.21 54.94±0.50|61.06±0.14 61.65±0.18 62.01±0.22 60.88±0.59|62.23±0.06 62.68±0.35 63.15±0.27 61.42±0.89|
|---|---|---|---|



Table 1: STL-10/CIFAR-10/CIFAR-100 Top-1 accuracy of DirectCopy.
The numbers for DirectPred, DirectPred (freq=5) and SGD baseline on
STL-10/CIFAR-10 are obtained from Tian et al. (2021).

**STL-10:** We evaluate the quality of the learned representation after each epoch, and report the
best accuracy in the first 100/300/500 epochs in Table 1. DirectCopy achieves substantially better
performance than the original DirectPred and SGD baseline, especially when trained with longer
epochs. DirectPred (freq=5) means the predictor is set by DirectPred every 5 batchs, and is trained
with gradient updates in other batchs, which outperforms DirectPred in later epochs, but is still much
worse than DirectCopy. The SGD baseline is obtained by training the linear predictor using SGD.

**CIFAR-10/100:** For CIFAR-10, DirectCopy is slighly worse than DirectPred at epoch 100, but
catches up and gets even better performance in epoch 300 and 500 (Table 1). For CIFAR-100, at
earlier epochs, the performance of DirectCopy is not as good as DirectPred, but the gap gradually
diminishes in later epochs. Both DirectCopy and DirectPred outperfoms the SGD baseline. DirectPred (freq=5) achieves even better performance, but at the cost of a more complicated algorithm.

4.2 RESULTS ON IMAGENET

Following BYOL (Grill et al., 2020), we use ResNet-50 as the backbone and a two-layer MLP as
the projector. We use LARS (You et al., 2017) optimizer and trains the model for 100 epochs. See
more detailed experiment settings in Appendix A.

For fairness, we compare DirectCopy to the gradient-based baseline which uses the same-sized linear
predictor as ours. As shown in Table 2, at 100-epoch, this baseline achieves 68.6 top-1 accuracy,
which is already significantly higher than BYOL with two-layer predictors reported in the literature
(e.g., Chen & He (2020) reports 66.5 top-1 under 100-epoch training). DirectCopy using normalized
_F accumulated with EMA µ = 0.99 on the correlation matrix, regularization parameter ϵ = 0.01_
achieves 68.8 under the same setting, better than this strong baseline. In contrast, DirectPred (Tian
et al., 2021) achieves 68.5, slightly lower than the linear baseline.


-----

|O|Bad Basin|Good Basin 𝜆"# 𝜆"$ 𝜆|
|---|---|---|

|O|Good Basin 𝜆"# 𝜆"$ 𝜆|
|---|---|


STL-10, DirectCopy with ϵ = 1 completely fails. On CIFAR-10, although DirectCopy with ϵ = 1

𝜆" 𝜆" 𝜆"

Bad Good Good Bad
Basin Basin Basin Basin

Increase 𝜖 Increase 𝜖

O 𝜆"# 𝜆"$ 𝜆" O 𝜆"# 𝜆"$ 𝜆" O 𝜆"

Figure 3: Left: Change of _λ[˙]_ _S when predictor regularization ϵ increases. Right: Eigenvalues of F when_
trained by DirectCopy under different ϵ on CIFAR-10 for 100 epochs.

5 ABLATION STUDY

In this section, we study the influence of predictor regularization ϵ, normalization method, weight
decay and degree α on the performance of DirectCopy.

**Predictor regularization:** Table 3 shows that when the predictor regularization ϵ increases, the
performance of DirectCopy on STL-10 and CIFAR-10 improves at first and then deteriorates. On

achieved reasonable performance at epoch 300, it’s still much worse than ϵ = 0.3.

To better understand the role of ϵ, we analyze the simple linear setting as in Section 3.1 while
setting Wp = WW _[⊤]_ + ϵI. Recall that λB is the eigenvalue of W in B subspace and λS is that in S
subspace. When the weight decay is appropriate, λB still converges to zero. On the other hand, the
dynamics for λS is as follows:

_λ˙_ _S =_ _λS_ _λ[2]S_ [+][ ϵ][ −] [1][ −√][1][ −] [4][η] _λ[2]S_ [+][ ϵ][ −] [1 +][ √][1][ −] [4][η] _._
_−_ 2 2

   

Increasing ϵ shifts the two positive stationary points λ[−]S _[, λ]S[+]_ [towards zero. As illustrated in Figure 3]

(left), as ϵ increases, when λ[+]S [is still positive, the good attraction basin expands, which means][ λ][S]
can converge to a positive value from a smaller initialization; when λ[+]S [shifts to zero,][ λ][S][ converges]
to zero regardless the initialization size. See the full analysis in Appendix D.

Intuitively, a reasonable ϵ can alleviate representation collapse, but a too large ϵ also encourages
representation collapse. As shown in Figure 3 (right), when ϵ increases from zero, more eigenvalues
of F becomes large; but when ϵ exceeds 0.3, eigenvalues of F begin to collapse.

**Normalization on F** **:** In our experiments, we have been normalizing F by its spectral norm
before adding the regularization: Wp = F/ ∥F _∥_ + ϵI. It turns out that we can also normalize
_F by its Frobenius norm or simply skip the normalization step. In Table 4, we see comparable_
performance from DirectCopy with Frobenius normalization or no normalization, especially when
trained longer.


_STL-10_

|Col1|Number of epochs 100 300|
|---|---|



_CIFAR-10_

|ϵ = 0 ϵ = 0.1 ϵ = 0.2 ϵ = 1|76.57±0.66 78.05±0.14 77.83±0.56 31.10±0.80|81.19±0.39 81.60±0.15 82.01±0.28 31.10±0.80|
|---|---|---|


|ϵ = 0 ϵ = 0.1 ϵ = 0.3 ϵ = 1|80.53±1.14 83.97±0.25 84.02±0.37 57.38±11.62|86.07±0.71 88.58±0.11 89.17±0.12 83.15±4.24|
|---|---|---|



Table 3: STL-10/CIFAR-10 Top-1 accuracy of
DirectCopy with varying regularization ϵ.


_STL-10_

|Col1|Number of epochs 100 300|
|---|---|



_CIFAR-10_

|Spectral Frobenius None|77.83±0.56 77.71±0.18 77.81±0.20|82.01±0.28 82.06±0.28 82.00±1.24|
|---|---|---|


|Spectral Frobenius None|84.02±0.37 84.33±0.25 81.76±0.34|89.17±0.12 89.62±0.14 89.21±0.17|
|---|---|---|



Table 4: STL-10/CIFAR-10 Top-1 accuracy of
DirectCopy with F matrix normalized by spectral
norm/Frobenius norm or no normalization.


**Weight decay:** Table 5 shows that when weight decay η increases, the performance of DirectCopy
improves at first and then deteriorates. This fits our analysis on simple linear networks. Basically,
when the weight decay η increases, it can suppress the nuisance features more effectively, but a too
large weight decay also collapses the useful features.


-----

_STL-10_


_STL-10_

|Col1|Number of epochs 100 300|
|---|---|



_CIFAR-10_

|α = 2 α = 1 α = 1/2 α = 1/4|76.80±0.22 80. 77.83±0.56 82. 77.82±0.37 77. 76.82±0.36 76.|90±0.18 01±0.28 83±0.37 82±0.36|
|---|---|---|


|α = 2 α = 1 α = 1/2 α = 1/4|82.96±0.56 88. 84.02±0.37 89. 84.88±0.21 88. 84.78±0.21 87.|60±0.11 17±0.12 32±0.57 82±0.32|
|---|---|---|



Table 6: STL-10/CIFAR-10 Top-1 accuracy of
DirectSet(α) with varying degree α.


Number of epochs

100 300


_CIFAR-10_

|η = 0 η = 0.0004 η = 0.001 η = 0.01|71.94±0.93 77.83±0.56 77.65±0.16 58.12±0.94|
|---|---|


|η = 0 η = 0.0004 η = 0.001 η = 0.01|79.15±0.08 84.02±0.37 83.91±0.33 65.31±1.19|
|---|---|



Table 5: STL-10/CIFAR-10 Top-1 accuracy of
DirectCopy with varying weight decay.


**Predictor degree:** We compare DirectCopy against DirectSet(α) with α = 2, 1/2, 1/4. Table 6

shows that DirectCopy outperforms other algorithms on STL-10. On CIFAR-10, DirectCopy is
slightly worse at epoch 100, but catches up in later epochs.

6 BEYOND LINEAR MODELS: LIMITATIONS AND DISCUSSION

Figure 4: Eigenvalues of F when trained by DirectCopy, BYOL with linear predictor and BYOL with twolayer nonlinear predictor on CIFAR-10 for different epochs. Top-1 accuracy at 500 epoch is 89.62 for DirectCopy, 88.83 for BYOL with linear predictor and 90.25 for BYOL with two-layer nonlinear predictor.

As a linear model used to study the behavior of nc-SSL, our model does not capture all of its
intriguing empirical phenomena. For example, we observed that the discarded nuisance features
gradually come back after training over longer epochs. Moreover, whether it comes back or not is
related to the downstream task performance. In Figure 4 on CIFAR-10 dataset, both DirectCopy
and BYOL with two-layer nonlinear predictor show this resurgence of nuisance features, as well as
strong performance, while BYOL with linear predictor does not seem to learn new features even
when trained longer, which might explain its worse performance.

One conjecture is that at the beginning of training, weight decay prioritize the invariant features (i.e.,
low variance under augmentation) over nuisance ones. The invariant features then grow, building
their own supporting low-level features. After that, the nuisance feature, which is also useful, are
gradually picked up in later stage. Since the low-level features are already trained through previous
steps of back-propagation, the nuisance features are encouraged to use them as the supporting features, rather than creating their own. In contrast, if we train both the invariant and nuisance features
simultaneously, they will compete over the limited pool of low-level supporting features defined by
the capacity of the network, leading to worse learned representations. We believe understanding
these phenomena require analysis on the non-linear networks, and we leave it as future work.

7 CONCLUSION

In this paper, we have proved DirectSet(α) can learn the desirable projection matrix in a linear
network setting and reduce the sample complexity on down-stream tasks. Our analysis sheds light on
the crucial role of weight decay in nc-SSL, which discards the features that have high variance under
augmentations and keep the invariant features. Inspired by the analysis, we also designed a simpler
and more efficient algorithm DirectCopy, which achieves comparable or even better performance
than the original DirectPred (Tian et al., 2021) on various datasets.

We view our paper as an initial step towards demystifying the representation learning in nc-SSL.
Many mysteries lie beyond the explanation of the current theory and we leave them for future work.


-----

REFERENCES

Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In ICML. PMLR, 2018.

Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient
descent for deep linear neural networks. In ICLR, 2019.

Philip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing
mutual information across views. arXiv preprint arXiv:1906.00910, 2019.

Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization
for self-supervised learning. arXiv preprint arXiv:2105.04906, 2021.

Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard S¨ackinger, and Roopak Shah. Signature verification using a“ siamese” time delay neural network. NeurIPS, 1994.

Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervised learning of visual features by contrasting cluster assignments. NeurIPS, 2020.

Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou, Julien Mairal, Piotr Bojanowski, and
Armand Joulin. Emerging properties in self-supervised vision transformers. _arXiv preprint_
_arXiv:2104.14294, 2021._

Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020a.

Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. arXiv preprint
_arXiv:2011.10566, 2020._

Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum
contrastive learning. arXiv preprint arXiv:2003.04297, 2020b.

Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In International conference on artificial intelligence and statistics, 2011.

J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR, 2009.

Simon Du and Wei Hu. Width provably matters in optimization for deep linear neural networks. In
_ICML, 2019._

Aleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, and Nicu Sebe. Whitening for selfsupervised representation learning. In International Conference on Machine Learning, pp. 3015–
3024. PMLR, 2021.

Rong Ge, Qingqing Huang, and Sham M Kakade. Learning mixtures of gaussians in high dimensions. In Proceedings of the forty-seventh annual ACM symposium on Theory of computing, pp.
761–770. ACM, 2015.

Jean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin Tallec, Pierre H Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi
Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint
_arXiv:2006.07733, 2020._

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.

Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In CVPR, 2020.

Tianyu Hua, Wenxiao Wang, Zihui Xue, Yue Wang, Sucheng Ren, and Hang Zhao. On feature
decorrelation in self-supervised learning. ICCV, 2021.

Kenji Kawaguchi. Deep learning without poor local minima. NeurIPS, 2016.


-----

Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.

Andrew K Lampinen and Surya Ganguli. An analytic theory of generalization dynamics and transfer
learning in deep linear networks. In ICLR, 2018.

Thomas Laurent and James Brecht. Deep linear networks with arbitrary loss: All local minima are
global. In ICML, pp. 2902–2907. PMLR, 2018.

Jason D Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo. Predicting what you already know helps:
Provable self-supervised learning. arXiv preprint arXiv:2008.01064, 2020.

Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.

Pierre H. Richemond, Jean-Bastien Grill, Florent Altch´e, Corentin Tallec, Florian Strub, Andrew
Brock, Samuel Smith, Soham De, Razvan Pascanu, Bilal Piot, and Michal Valko. Byol works
even without batch statistics. arXiv, 2020.

Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.

Andrew M Saxe, James L McClelland, and Surya Ganguli. A mathematical theory of semantic
development in deep neural networks. Proc. Natl. Acad. Sci. U. S. A., 2019.

Gilbert W Stewart. On the perturbation of pseudo-inverses, projections and linear least squares
problems. SIAM review, 19(4):634–662, 1977.

Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. arXiv preprint
_arXiv:1906.05849, 2019._

Yuandong Tian, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning dynamics
without contrastive pairs. arXiv preprint arXiv:2102.06810, 2021.

Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive learning, multi-view redundancy, and linear models. arXiv preprint arXiv:2008.10150, 2020.

Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
_arXiv:1011.3027, 2010._

Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018.

Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cambridge University Press, 2019.

Zixin Wen and Yuanzhi Li. Toward understanding the feature learning process of self-supervised
contrastive learning. arXiv preprint arXiv:2105.15134, 2021.

Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks.
_arXiv:1708.03888, 2017._

Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and St´ephane Deny. Barlow twins: Self-supervised
learning via redundancy reduction. ICML, 2021.


-----

A DETAILED EXPERIMENT SETTING

**STL-10, CIFAR-10, CIFAR-100** : We use ResNet-18 (He et al., 2016) as the backbone network,
a two-layer nonlinear MLP (with batch normalization, ReLU activation, hidden layer width 512,
output width 128) as the projector, and a linear predictor. Unless specified otherwise, SGD is used
as the optimizer with momentum 0.9, weight decay η = 0.0004 and batch size 128. The EMA
parameter for the target network is set as 0.996 and the EMA parameter µ of the correlation matrix _F[ˆ]_
is set as 0.5. Our code is adapted from Tian et al. (2021) [4], and we follow the same data augmentation
process.

To evaluate the quality of the pre-trained representations, we follow the linear evaluation protocol.
Each setting is repeated 5 times to compute the mean and standard deviation. The accuracy is
reported as “mean±std”. Unless explicitly specified, we use learning rate γ = 0.01, regularization
_ϵ = 0.2 on STL-10; γ = 0.02, ϵ = 0.3 on CIFAR-10 and γ = 0.03, ϵ = 0.3 on CIFAR-100._

**ImageNet** : Following BYOL (Grill et al., 2020), we use ResNet-50 as the backbone and a twolayer MLP (with batch normalization, ReLU, hidden layer width 4096, output width 256) as the
projector. We use LARS (You et al., 2017) optimizer and trains the model for 100 epochs, with a
batch size 4096. The learning rate is 7.2, which is linearly scaled from the base learning rate 0.45
at batch size 256. Other setups such as weight decay (η = 1e[−][6]), target EMA (scheduled from 0.99
to 1), augmentation recipe (color jitters, blur, etc.), and linear evaluation protocol are the same as
BYOL.

B PROOFS OF SINGLE-LAYER LINEAR NETWORKS

B.1 GRADIENT FLOW ON POPULATION LOSS

In this section, we give the proof of Theorem 1, which shows that DirectSet(α) running on the
population loss with infinitesimal learning rate and η weight decay can learn the projection matrix
onto subspace S.
**Theorem 1. Suppose network architecture and data distribution follow Assumption 1 and Assump-**
_tion 2, respectively. Suppose we initialize online network W as δI, and run DirectSet(α) on popu-_
_lation loss (see Eqn. 1) with infinitesimal step size and η weight decay. If we set the weight decay_

1/(2α)

_coefficient η_ 4(1+1σ[2]) _[,][ 1]4_ _and initialization scale δ >_ 1−[√]21−4η _, then W converges to_
_∈_

1/(2α)

1+[√]1 4η    

2 _−_ _PS when time goes to infinity._

 

As we already mentioned in the main text, Theorem 1 is proved by analyzing each eigenvalue of W
separately. We show that the eigenvalues in the B subspace converge to zero, and the eigenvalues in
the S subspace converge to the same positive number, which immediately implies that W converges
to a scaling of the projection matrix PS.

**Proof of Theorem 1. We can compute the gradient in terms of W as follows,**


_∇L(W_ ) =Ex1,x2 _Wp[⊤]_ [(][W][p][Wx][1] _[−]_ _[W][a][x][2][)][ x]1[⊤]_
=Wp[⊤] _WpW_ Ex1 _x1x[⊤]1_ _[−]_ _[W][a][E][x]1[,x]2_ _[x][2][x][⊤]1_ _._

Note that the two augmented views x1, x  2 are sampled by first sampling input _x from N_ (0, Id), and
then independently sampling x1, x2 from (x, σ[2]PB). Therefore, we know Ex1 _x1x[⊤]1_ [=][ I][ +][ σ][2][P][B]
_N_
and Ex1,x2 _x2x[⊤]1_ [=][ I.][ Recall that we run gradient flow on][ W][ with weight decay][ η,][ so the dynamics]
on W is as follows:
_W˙_ =Wp[⊤][(][−][W][p][W] [(][I][ +][ σ][2][P][B][) +][ W][a][)][ −] _[ηW,]_

where the first term comes from the gradient and the second term is due to weight decay.

Since W is initialized as δI, and Wa = W, Wp = (WW _[⊤])[α], so we know initially W, Wp, Wa, I and_
_PB are all simultaneously diagonalizable, which then implies_ _W[˙]_ is simultaneously diagonalizable

4Their open source code is at https://github.com/facebookresearch/luckmatters/tree/main/ssl


-----

with W . This argument can continue to show that at any time point, W, Wp, Wa, I and PB are
all simultaneously diagonalizable. Since W is always a real symmetric matrix, we have Wp =
(WW _[⊤])[α]_ = |W _|[2][α]_ _. The dynamics on W can then be written as_

_W˙_ = _W_ ( _W_ _W_ (I + σ[2]PB) + W ) _ηW_
_|_ _|[2][α]_ _−|_ _|[2][α]_ _−_

=W (I + σ[2]PB) _W_ + _W_ _η_ _._
_−_ _|_ _|[4][α]_ _|_ _|[2][α]_ _−_
 

Let the eigenvalue decomposition of W be _i=1_ _[λ][i][u][i][u]i[⊤][,][ with span][(][{][u][d][−][r][+1][,][ · · ·][, u][d][}][)][ equals to]_
subspace B. We can separately analyze the dynamics of each λi. Furthermore, we know λ1, _, λr_
_· · ·_
have the same value λS and λd _r+1,_ _, λd have the same value λB. Next, we separately show that_
_−_ _· · ·_ [P][d]
_λB converge to zero and λS converges to a positive value._

**Dynamics for λB:** We can write down the dynamics for λB as follows:

_λ˙_ _B = λB_ _−(1 + σ[2]) |λB|[4][α]_ + |λB|[2][α] _−_ _η_

Similar as the analysis in Tian et al. (2021), whenh _η >_ 4(1+1σ[2]) _[,][ we know]i_ [ ˙]λB < 0 for any λB > 0

and λB = 0 is a critical point. This means, as long as η > 4(1+1σ[2]) [,][ λ][B][ must converge to zero.]

**Dynamics for λS:** We can write down the dynamics for λS as follows:

_λ˙_ _S = λS_ _−|λS|[4][α]_ + |λS|[2][α] _−_ _η_ _._
h i

When 0 < η < 14 _[,][ we know][ ˙]λS > 0 for λ[2]S[α]_ 1−[√]21−4η _,_ [1+][√]2[1][−][4][η] and _λ[˙]_ _S < 0 for λ[2]S[α]_
_∈_ _∈_

1+[√]21−4η _,_ _. Furthermore, we know_ _λ[˙]_ _S = 0 when_ _λ[2]S[α]_ = 1+[√]21−4η _. Therefore, as long as_

_∞_

0 < η < [1]4 [and initialization] _[ δ][2][α][ >][ 1][−√]2[1][−][4][η]_, we know λ[2]S[α] [converges to][ 1+][√]2[1][−][4][η] _._

1/(2α)

Overall, we know when 4(1+1σ[2]) _[< η <][ 1]4_ [and][ δ >] 1−[√]21−4η _, we have λB converge to_

1/(2α) 1/(2α)

zero and λS converge to 1+[√]21−4η _. That is, matrix_ _W converges to_ 1+[√]21−4η _PS._
    □

B.2 GRADIENT DESCENT ON EMPIRICAL LOSS

In this section, we prove that DirectCopy successfully learns the projection matrix given polynomial
number of samples.
**Theorem 2. Suppose network architecture and data distribution are as defined in Assumption 1**
_and Assumption 2, respectively. Suppose we initialize online network as δI, and run DirectCopy_
_on empirical loss (see Eqn. 2) with γ step size and η weight decay. Suppose the noise scale σ[2]_

_is a positive constant, the weight decay coefficient η_ 4(1+1+σ[2]σ/[2]4) _[,][ 1+3]4(1+[σ]σ[2][/][2])[4]_ _and the initialization_
_∈_

_scale δ is a constant at least 1/√2. Choose the step size γ as a small enough constant. For any_

_accuracy ˆϵ > 0, given n ≥_ _poly(d, 1/ϵˆ) number of samples, with probability at least 0.99 there_
_exists t = O(log(1/ϵˆ)) such that (here_ _Wt is the online network weights at the t-th step):_

1 + 1 4η

_Wt_ [f] _[√]_ _−_ _PS_ _ϵ._
_−_ r 2
f _[≤]_ [ˆ]

When running gradient descent on the empirical loss, the eigenspace of _Wt can shift and become no_
longer simultaneously diagonalizable with PB. So we cannot independently analyze each eigenvalue
of _Wt as before, which brings significant challenge into the analysis. Instead of directly analyzing[f]_
the dynamics of _Wt, we first show that the gradient descent iterates Wt on the population loss_
converges to[f] _PS in linear rate, and then show that_ _Wt stays close to Wt within certain iterations._

[f]

[f]


-----

**Lemma 1. In the setting of Theorem 2, let Wt be the gradient descent iterations on the population**
_loss L. Given any accuracy ˆϵ > 0, for any t ≥_ _C log(1/ϵˆ), we have_

1 + 1 4η

_[√]_ _−_ _PS_ _ϵ,_

2

r

_[≤]_ [ˆ]

_where C is a positive constant._

_[W][t][ −]_

The proof of Lemma 1 is similar as the gradient flow analysis in Section 3.2. Next, we show that the
gradient descent trajectory on the empirical loss stays close to the gradient descent trajectory on the
population loss within O(log(1/ϵˆ)) iterations.

**Lemma 2. In the setting of Theorem 2, let Wt be the gradient descent iterations on the population**
_loss and let_ _Wt be the gradient descent iterations on the empirical loss. For any accuracy ˆϵ > 0,_
_given n ≥_ _poly(d, 1/ϵˆ) number of samples, with probability at least 0.99, for any t ≤_ _C log(1/ϵˆ),_
_we have_

[f]
_Wt_ _Wt_ _ϵ,ˆ_
_−_ _≤_

_where the constant C comes from Lemma 1._

[f]

Then the proof of Theorem 2 directly follows from Lemma 1 and Lemma 2.

**Proof of Theorem 2. According to Lemma 1, we know given any accuracy ˆϵ[′], for t = C log(1/ϵˆ),**
we have

1 + 1 4η

_[√]_ _−_ _PS_ _ϵ[′],_

2

r

_[≤]_ [ˆ]

where C is a positive constant.

_[W][t][ −]_

According to Lemma 2, we know given n ≥ poly(d, 1/ϵˆ[′]) number of samples, with probability at
least 0.99,
_Wt_ _Wt_ _ϵˆ[′]._
_−_ _≤_

Therefore, we have [f]

1 + 1 4η 1 + 1 4η

_Wt_ _[√]_ _−_ _PS_ _[√]_ _−_ _PS_ _Wt_ _Wt_ 2ˆϵ[′].
_−_ r 2 r 2 [+] _−_ _≤_

Replacingf ˆϵ[′] by ˆϵ/2 finishes the proof.[≤] [f] □

_[W][t][ −]_

In section B.2.1, we give the proof of Lemma 1 and Lemma 2. Proofs of some technical lemmas are
left in Appendix B.5.

B.2.1 PROOFS FOR LEMMA 1 AND LEMMA 2

**Proof of Lemma 1. Similar as in Theorem 1, we can show that at any step t, Wt is simultaneously**
diagonalizable with Wa,t, Wp,t, I and PB. The update on Wt is as follows,

_Wt+1 = Wt + γWt_ _−(I + σ[2]PB)Wt[4]_ [+][ W][ 2]t _[−]_ _[η]_ _._
  

Let the eigenvalue decomposition of Wt be _i=1_ _[λ][i,t][u][i][u]i[⊤][,][ with span][(][{][u][d][−][r][+1][,][ · · ·][, u][d][}][)][ equals]_
to subspace B. We can separately analyze the dynamics of each λi,t. Furthermore, we know
_λ1,t,_ _, λr,t have the same value λS,t and λd_ _r+1,t,_ _, λd,t have the same value λB,t. Next,_
_· · ·_ [P][d]− _· · ·_
we separately show that λB,t converge to zero and λS,t converges to a positive value in linear rate.

**Dynamics of λB,t:** We show that

0 _λB,t_ (1 _γC1)[t]δ_
_≤_ _≤_ _−_

for any step size γ ≤ _C2, where C1, C2 are two positive constants._


-----

According to the gradient update, we have

_λB,t+1 = λB,t + γλB,t_ _−(1 + σ[2])λ[4]B,t_ [+][ λ]B,t[2] _[−]_ _[η]_ _._

We only need to prove that for any λB,t [0, δ], we have 
_∈_

_−(1 + σ[2])λ[4]B,t_ [+][ λ]B,t[2] _[−]_ _[η][ =][ −][Θ(1)][.]_

This is true since η 4(1+1+σ[2]σ/[2]4) _[,][ 1+3]4(1+[σ]σ[2][/][2])[4]_ and σ[2], δ are two positive constants.
_∈_
 


**Dynamics of λS:** We show that

0 ≤ _S,t_ _[−]_ [1 +][ √]2[1][ −] [4][η] _[≤]_ [(1][ −] _[γC][3][)][t]_ 2

for any step size γ ≤ _C[λ]4,[2] where C3, C4 are two positive constants.[δ][2][ −]_ [1 +][ √][1][ −] [4][η]

There are two cases to consider: when the initialization scale δ[2] [1/2, [1+][√]2[1][−][4][η]
_∈_

1 + 1 4η

0 ≤ [1 +][ √]2[1][ −] [4][η] _−_ _λ[2]B,t_ _[≤]_ [(1][ −] _[γC][3][)][t]_ _√2_ _−_ _−_ _δ[2]_

 

when the initialization scale δ[2] _>_ [1+][√]2[1][−][4][η] _, we prove_

0 ≤ _λ[2]B,t_ _[−]_ [1 +][ √]2[1][ −] [4][η] _≤_ (1 − _γC3)[t]_ _δ[2]_ _−_ [1 +][ √]2[1][ −] [4][η]

 

We focus on the second case; the proof for the first case is similar.

According to the gradient update, we have


], we prove


_λS,t+1 =λS,t + γλS,t_ _−λ[4]S,t_ [+][ λ]S,t[2] _[−]_ _[η]_
 

=λS,t − _γλS,t_ _λ[2]S,t_ _[−]_ [1][ −√]2[1][ −] [4][η] _λ[2]S,t_ _[−]_ [1 +][ √]2[1][ −] [4][η]

   

We only need to show that λS,t _λ[2]S,t_ 2 = Θ(1) for any λ[2]S,t 2 _, δ]. This is_

_[−]_ [1][−√][1][−][4][η] _[∈]_ [[][ 1+][√][1][−][4][η]

true because η 4(1+1+σ[2]σ/[2]4) _[,][ 1+3]4(1+[σ]σ[2][/][2])[4]_ and σ[2], δ are two positive constants.
_∈_
 


Overall, we know that there exists constant step size such that after t = O(log(1/ϵˆ)) steps, we have


1 + 1 4η

_[√]_ _−_


0 _λB,t_ _ϵˆ and_
_≤_ _≤_


r

_[λ][S,t][ −]_

1 + 1 4η

_[√]_ _−_



_[≤]_ _ϵ.[ˆ]_

_n_

1

_x[(]1[i][)][[][x]2[(][i][)][]][⊤]_

_n_

_i=1_ !!

X


This then implies,

1 + 1

_[√]_ _−_

2

r

_[W][t][ −]_

**Proof of Lemma 2. We know the update on** _Wt is_

_n_

1

_Wt+1 −_ _W[f]t = γW[f]p,t[⊤]_ _−W[f]p,tW[f]t_ _n_ _i=1_ _x[f][(]1[i][)][[][x]1[(][i][)][]][⊤]_

X

and the update onf _Wt is_


_PS_



_[≤]_ _ϵ.[ˆ]_


+ _Wa,t_

[f]


_γηW[f]t,_
_−_


_Wt+1 −_ _Wt = γWp,t[⊤]_ _−Wp,tWt_ _I + σ[2]PB_ + Wa,t _−_ _γηWt._

Next, we bound _Wt+1_ _Wt_ (Wt+1  _Wt)_ _. According to Lemma 3, we know with probability _  
_−_ [f] _−_ _−_

at least 1 − _O(d[2]) exp_ _−Ω(ˆϵ[′][2]n/d[2])_ _,_

_n_ [f] _n_ _n_

  


_x[(]1[i][)][[][x]1[(][i][)][]][⊤]_ _[−]_ _[I][ −]_ _[σ][2][P][B]_
_i=1_

X


_x[(]1[i][)][[][x]2[(][i][)][]][⊤]_ _[−]_ _[I]_
_i=1_

X


_x[(][i][)][x[(][i][)]][⊤]_ _−_ _I_ _ϵ[′]._
_i=1_ _[≤]_ [ˆ]

X


-----

Recall that we set _Wa,t =_ _Wt and set Wa,t as Wt, so we have_ _Wa,t −_ _Wa,t_ = _Wt −_ _Wt_ _._

1 _n_
Also since we set _Wp,t =_ _Wt_ _n_ _i=1_ _[x][(][i][)][[][x][(][i][)][]][⊤][ f]Wt[⊤]_ and set Wp,t = WtWt[⊤][,][ we have]

[f] [f] [f] [f]

_Wp,t_ _Wp,t_ = O _Wt_ _Wt_   + ˆPϵ[′][] since _Wt_ = O(1).
_−_ [f] _−_ [f] _∥_ _∥_


Combing the above bounds and recall γ is a constant, we have

[f] [f]

_Wt+1_ _Wt_ (Wt+1 _Wt)_ = O _Wt_ _Wt_ + ˆϵ[′][] _._
_−_ [f] _−_ _−_ _−_

Therefore, 

[f] [f]

_Wt −_ _Wt_ _≤_ _C1[t]ϵ[ˆ][′],_

where C1 is a constant larger than 1. So for any t ≤ _C log(1/ϵˆ), we have_
_Wt_ _Wt_ [f] _C1[C][ log(1][/]ϵ[ˆ])ϵˆ[′]_ (1/ϵˆ)[C][2] _ϵˆ[′],_
_−_ _≤_ _≤_

for some positive constant C2. Choosing ˆϵ[′] = ˆϵ[C][2][+1], we know as long as n poly(d, 1/ϵˆ), with
probability at least 0.99, for any[f] t ≤ _C log(1/ϵˆ), we have_ _≥_
_Wt_ _Wt_ _ϵ.ˆ_
_−_ _≤_

□

[f]

B.3 SAMPLE COMPLEXITY ON DOWN-STREAM TASKS


In this section, we give a proof for Theorem 3, which shows that the learned representations can
indeed reduce sample complexity in downstream tasks.
**Theorem 3. Suppose the downstream data distribution is as defined in Assumption 3. Suppose**
_P[ˆ]_ _P_ _ϵ with ˆϵ < 1. Choose the regularizer coefficient ρ = ˆϵ[1][/][3]. For any ζ < 1/2, given_

_n ≥ −O(r +log(1F_ _[≤]_ [ˆ] _/ζ)) number of samples, with probability at least 1_ _−_ _ζ, the training loss minimizer_
_wˆ satisfies_

_√r +_ log(1/ζ)

_P[ˆ] ˆw −_ _w[∗]_ _≤_ _O_ _ϵˆ[1][/][3]_ + β p√n ! _._

Suppose (x[(][i][)], y[(][i][)]) _i=1_ [are][ n][ training samples in the downstream task, let][ X][ ∈] [R][n][×][d][ be the data]
_{_ _}[n]_
matrix with its i-th row equal to x[(][i][)]. Denote y ∈ R[n] as the label vector with its i-th entry as y[(][i][)].
Each input x[(][i][)] is transformed by a matrix _P[ˆ] ∈_ R[d][×][d] to get its representation _Px[ˆ]_ [(][i][)]. The regularized
loss can be written as

2

_L(w) := [1]_ _XPw[ˆ]_ _y_ + _[ρ]_

2n _−_ 2 _[∥][w][∥][2][ .]_

This is the ridge regression problem on inputs {( Px[ˆ] [(][i][)], y[(][i][)])}i[n]=1[, and the unique global minimizer]
_wˆ has the following close form:_

1 _−1 1_
_wˆ =_ _n_ _Pˆ[⊤]X_ _[⊤]XP[ˆ] + ρI_ _n_ _Pˆ[⊤]X_ _[⊤]y_ (4)
 

With the above closed form of ˆw, the proof of Theorem 3 follows by bounding the difference between _P[ˆ] ˆw and w[∗]_ by matrix concentration inequalities and matrix perturbation bounds. Some proofs
of technical lemmas are left in Appendix B.5.

**Proof of Theorem 3. Denoting** _P[ˆ] as P + ∆, we know_ ∆ _F_ _ϵˆ by assumption. We can also write_
_y as Xw[∗]_ + ξ where ξ ∈ R[n] is the noise vector with its ∥ i-th entry equal to∥ _≤_ _ξ[(][i][)]. Then, we can divide_
_wˆ into two terms,_

1 _−1 1_
_wˆ =_ _n_ _Pˆ[⊤]X_ _[⊤]XP[ˆ] + ρI_ _n_ _Pˆ[⊤]X_ _[⊤]y_
 

1 _−1 1_ 1 _−1 1_
= _n_ _Pˆ[⊤]X_ _[⊤]XP[ˆ] + ρI_ _n_ _[P][ ⊤][X]_ _[⊤]_ [(][Xw][∗] [+][ ξ][) +] _n_ _Pˆ[⊤]X_ _[⊤]XP[ˆ] + ρI_ _n_ [∆][⊤][X] _[⊤]_ [(][Xw][∗] [+][ ξ][)]
   


Let’s first give an upper bound for the second term that comes from the error term ∆[⊤].


-----

**Upper bounding** _n1_ _P[ˆ][⊤]X_ _[⊤]XP[ˆ] + ρI_ _−1 1n_ [∆][⊤][X] _[⊤]_ [(][Xw][∗] [+][ ξ][)] We first bound the norm of

1  
_n_ [∆][⊤][X] _[⊤][Xw][∗][.][ According to Lemma 5, we know with probability at least][ 1][ −]_ [exp(][−][Ω(][n][))][,]
_√[1]n_ ∆[⊤]X _[⊤]_ _ϵ). Since Xw[∗]_ is a standard Gaussian vector with dimension n, according
_F_

_[≤]_ _[O][(ˆ]_

to Lemma 8, with probability at least 1 exp( Ω(n)), _√[1]n_ _Xw[∗]_ _O(1). Therefore, we have_
_−_ _−_ _≤_

_n[1]_ [∆][⊤][X] _[⊤][Xw][∗]_ _≤_ _O(ˆϵ)._

Then we bound the norm of _n1_ [∆][⊤][X] _[⊤][ξ][.]_ According to Lemma 8, we know

with probability at least 1 exp( Ω(n)), _√[1]n_ _ξ_ _O(β). According to Lemma 6, we_
_−_ _−_ _≤_

know with probability at least 1 − _ζ/3,_ ∆[⊤]X _[⊤]ξ¯_ _≤_ _O_ _ϵˆ_ log(1/ζ) _. Therefore, we have_

_n[1]_ [∆][⊤][X] _[⊤][ξ]_ _O_ _βϵˆ[√]log(1√n_ _/ζ)_ _._  p 
_≤_
 

Since λmin _n1_ _P[ˆ][⊤]X_ _[⊤]XP[ˆ] + ρI_ _≥_ _ρ, we have_ _n1_ _P[ˆ][⊤]X_ _[⊤]XP[ˆ] + ρI_ _−1_ _[≤]_ _ρ[1]_ _[.][ Combining with]_

above bound on _n[1]_ [∆][⊤][X] _[⊤]_ [(][Xw][∗] [+][ ξ][)], we know with probability at least  1 _−_ exp(−Ω(n)) _−_ _ζ/3,_

1 _−1 1_ _ϵˆ_ _ϵ_ log(1/ζ)

_n_ _Pˆ[⊤]X_ _[⊤]XP[ˆ] + ρI_ _n_ [∆][⊤][X] _[⊤]_ [(][Xw][∗] [+][ ξ][)] _ρ_ [+][ β][ˆ] _ρ[√]n_ _._

  _[≤]_ _[O]_ p !

**Analyzing** _n1_ _P[ˆ][⊤]X_ _[⊤]XP[ˆ] + ρI_ _−1 1n_ _[P][ ⊤][X]_ _[⊤]_ [(][Xw][∗] [+][ ξ][)] We can write _n1_ _P[ˆ][⊤]X_ _[⊤]XP[ˆ]_ as

1
_n_ _[P][ ⊤][X]_ _[⊤][XP][ +][ E,][ where]_ 

_E = [1]_

_n_ [∆][⊤][X] _[⊤][XP][ + 1]n_ _[P][ ⊤][X]_ _[⊤][X][∆+ 1]n_ [∆][⊤][X] _[⊤][X][∆][.]_


Let’s first bound the spectral norm of XP. Since P is a projection matrix on an r-dimensional
subspace S, we can write P as UU _[⊤], where U ∈_ R[d][×][r] has columns as an orthonormal basis of
subspace S. According to Lemma 4, we know with probability at least 1 − exp(−Ω(n)),

1 1

Ω(1) _σmin_ _σmax_ _O(1)._
_≤_ _√nXU_ _≤_ _√nXU_ _≤_

   

Since ∥U _∥≤_ 1, we have _√[1]n_ _XP_ = _√[1]n_ _XUU_ _[⊤]_ _≤_ _O(1)._

According to Lemma 5, we know with probability at least 1 − exp(−Ω(n)),
1
_√nX∆_ _F_ _≤_ _O(ˆϵ)._

So overall, we know _E_ _E_ _F_ _O(ˆϵ)._
_∥_ _∥≤∥_ _∥_ _≤_

Then, we can write
1 _−1_ 1 _−1_

_n_ _Pˆ[⊤]X_ _[⊤]XP[ˆ] + ρI_ = _n_ _[P][ ⊤][X]_ _[⊤][XP][ +][ ρI]_ + F.

   


According to the perturbation bound for matrix inverse (Lemma 11), we have ∥F _∥≤_ _O(_ _ρϵ[ˆ][2][ )][.][ Then,]_

we have
1 _−1 1_ 1 _−1 1_

_n_ _Pˆ[⊤]X_ _[⊤]XP[ˆ] + ρI_ _n_ _[P][ ⊤][X]_ _[⊤]_ [(][Xw][∗] [+][ ξ][) =] _n_ _[P][ ⊤][X]_ _[⊤][XP][ +][ ρI]_ _n_ _[P][ ⊤][X]_ _[⊤][Xw][∗]_

   

+ F [1]

_n_ _[P][ ⊤][X]_ _[⊤][Xw][∗]_

1 _−1_ 1

+ + F

_n_ _[P][ ⊤][X]_ _[⊤][XP][ +][ ρI]_ _n_ _[P][ ⊤][X]_ _[⊤][ξ]_

  !


-----

We first show that the first term is close to w[∗]. Let the eigenvalue decomposition of _n[1]_ _[P][ ⊤][X]_ _[⊤][XP]_

be V ΣV _[⊤], where V ’s columns are an orthonormal basis for subspace S. Here Σ ∈_ R[r][×][r] is the
diagonal matrix that contains all the eigenvalues of _n[1]_ _[P][ ⊤][X]_ _[⊤][XP]_ [. According to Lemma 4, we]

know that with probability at least 1 − exp(−Ω(n)), all the non-zero eigenvalues of _n[1]_ _[P][ ⊤][X]_ _[⊤][XP]_

are Θ(1).

Then, it’s not hard to show that


1 _−1 1_

_n_ _[P][ ⊤][X]_ _[⊤][XP][ +][ ρI]_ _n_ _[P][ ⊤][X]_ _[⊤][XP][ −]_ _[P]_

 

This immediately implies that



_[≤]_ _[O][(][ρ][)][.]_


1 _−1 1_

_n_ _[P][ ⊤][X]_ _[⊤][XP][ +][ ρI]_ _n_ _[P][ ⊤][X]_ _[⊤][Xw][∗]_ _[−]_ _[w][∗]_

  _[≤]_ _[O][(][ρ][)]_

Next, we bound the norm of the second term F _n[1]_ _[P][ ⊤][X]_ _[⊤][Xw][∗][.][ Similar as before, we know]_

with probability at least 1 − exp(−Ω(n)), _√[1]n_ _Xw[∗]_ _≤_ _O(1) and_ _√[1]n_ _P_ _[⊤]X_ _[⊤]_ _≤_ _O(1). There-_
fore, we have

1 1 ˆϵ
_n_ _[P][ ⊤][X]_ _[⊤][Xw][∗]_ _[≤∥][F]_ _[∥]_ _√nP_ _[⊤]X_ _[⊤]_ _√nXw[∗]_ _[≤]_ _[O]_  _ρ[2]_  _._

Finally, let’s bound the third term[F][ 1] 1n _[P][ ⊤][X]_ _[⊤][XP][ +][ ρI]_ _−1 + F_ _n1_ _[P][ ⊤][X]_ _[⊤][ξ.][ We first bound the]_

norm of _n[1]_ _[P][ ⊤][X]_ _[⊤][ξ.][ with probability at least] _ [ 1][ −] [exp(][−][Ω(] _[n][))][,][ we know]_ _[ ∥][ξ][∥≤]_ [2][β][√][n.][ Therefore,]

we know _n[1]_ _[P][ ⊤][X]_ _[⊤][ξ]_ _≤_ _O(β/[√]n)_ _P_ _[⊤]X_ _[⊤]ξ¯_ _, where ¯ξ = ξ/ ∥ξ∥_ _. According to Lemma 7,_

with probability at least 1 − _ζ/3, we have_ _P_ _[⊤]X_ _[⊤]ξ¯_ _≤_ _√r + O(_ log(1/ζ)). Overall,

with probability at least 1 exp( Ω(n)) _ζ/3,_
_−_ _−_ _−_ p
1 _√rβ +_ log(1/ζ)β

_n_ _[P][ ⊤][X]_ _[⊤][ξ]_ _√n_ _._

_[≤]_ _[O]_ p !

It’s not hard to verify that for any vector 1n _[P][ ⊤][X]_ _[⊤][XP][ +][ ρI]_ _−1 + F_ _v_ _≤_ _O(∥v∥). Since v_ _n[1]∈[P][ ⊤][X]R[d][⊤][ξ][ lies on subspace]in the subspace[ S,] S,[ we have] we have_
  1   _−1_ 1 _√rβ +_ log(1/ζ)β

_n_ _[P][ ⊤][X]_ _[⊤][XP][ +][ ρI]_ + F _n_ _[P][ ⊤][X]_ _[⊤][ξ]_ _√n_ _._

  ! _[≤]_ _[O]_ p !

Combining the above analysis and taking a union bound over all the events, we know
with probability at least 1 − exp(−Ω(n)) − 2ζ/3,

_ϵ_ _ϵ_ _ϵ_ log(1/ζ) _√rβ +_ log(1/ζ)β

_∥wˆ −_ _w[∗]∥_ = O _ρ + ρ[ˆ]_ [+ ˆ]ρ[2][ +][ β][ˆ]pρ[√]n + p√n !

Suppose n ≥ _O(log(1/ζ)) and setting ρ = ˆϵ[1][/][3], we further have with probability at least 1 −_ _ζ,_

_ϵ[2][/][3][p]log(1/ζ)_ _√rβ +_ log(1/ζ)β

_∥wˆ −_ _w[∗]∥_ =O _ϵˆ[1][/][3]_ + _[β][ˆ]_ _√n_ + p√n !

_√r +_ log(1/ζ)

_≤O_ _ϵˆ[1][/][3]_ + β p√n ! _,_


where the last inequality assumes ˆϵ < 1.


-----

We can also bound _P[ˆ] ˆw −_ _w[∗]_ as follows,
_P[ˆ] ˆw −_ _w[∗]_ = _P[ˆ] ˆw −_ _P ˆw + P ˆw −_ _Pw[∗]_

_≤_ _P[ˆ] ˆw −_ _P ˆw_ + ∥P ˆw − _Pw[∗]∥_

_≤_ _P[ˆ] −_ _P_ _∥wˆ∥_ + ∥P _∥∥wˆ −_ _w[∗]∥_

_√r +_ log(1/ζ)

_≤ϵOˆ_ 1 + ˆϵ[1][/][3] + β _√n_

p


_√r +_
_ϵˆ[1][/][3]_ + β


log(1/ζ)
_√n_

p


+ O


_√r +_
_ϵˆ[1][/][3]_ + β


log(1/ζ)
_√n_

p


_≤O_


B.4 ANALYSIS WITH Wp := (W Ex1 _x1x[⊤]1_ _[W][ ⊤][)][α]_

In this section, we prove that DirectSet(α) can also learn the projection matrix when we set Wp :=
(W Ex1 _x1x[⊤]1_ _[W][ ⊤][)][α][. For the network architecture and data distribution, we follow exactly the same]_
setting as in Section 3.2. Therefore, we know Wp := (W Ex1 _x1x[⊤]1_ _[W][ ⊤][)][α][ = (][W]_ [(][I][ +] _[σ][2][P][B][)][W][ ⊤][)][α][.]_

**Theorem 4. Suppose network architecture and data distribution are as defined in Assumption 1 and**
_Assumption 2, respectively. Suppose we initialize online network W as δI, and run DirectPred(α)_
_on population loss (see Eqn. 1) with infinitesimal step size and η weight decay. Suppose we set Wa =_
_W and Wp = (W_ Ex1 _x1x[⊤]1_ _[W][ ⊤][)][α][.][ Assuming the weight decay coefficient][ η][ ∈]_ 4(1+σ1[2])[1+2][α][,][ 1]4

1/(2α) 1/(2α)

_and initialization scale δ >_ 1−[√]21−4η _, we know W converges to_ 1+[√]21−4η _PS_

_when time goes to infinity._    

The only difference from Theorem 4 is that now the initialization δ is only required to be larger than
1

4(1+σ[2])[1+2][α][ . The proof is almost the same as in Theorem 1.]

**Proof of Theorem 4. Similar as in the proof of Theorem 1, we can write the dynamics on W is as**
follows:
_W˙_ =Wp[⊤][(][−][W][p][W] [(][I][ +][ σ][2][P][B][) +][ W][a][)][ −] _[ηW]_

= _W_ [2](I + σ[2]PB) ( _W_ [2](I + σ[2]PB) _W_ (I + σ[2]PB) + W ) _ηW_
_−_ _−_

=W (I + σ[2]PB)[1+2][α] _W_ + _W_ _η_ _._
_−_ _[α]_ _|_ _|[4][α]_ _|_ _|[2][α]_ _−[α]_
 


**Dynamics for λB:** We can write down the dynamics for λB as follows:

_λ˙_ _B = λB_ _−(1 + σ[2])[1+2][α]_ _|λB|[4][α]_ + |λB|[2][α] _−_ _η_

When η > 4(1+σ1[2])[1+2][α][,][ we know]h[ ˙]λB < 0 for any λB > 0 and λB = 0i is a critical point. This

means, as long as η > 4(1+σ1[2])[1+2][α][,][ λ][B][ must converge to zero.]


**Dynamics for λS:** The dynamics is same as when setting Wp = (WW _[⊤])[α],_

_λ˙_ _S = λS_ _−|λS|[4][α]_ + |λS|[2][α] _−_ _η_ _._
h i

so when 0 < η < 4[1] [and initialization][ δ][2][α][ >][ 1][−√]2[1][−][4][η], we know λ[2]S[α] [converges to][ 1+][√]2[1][−][4][η] _._

1/(2α)

Overall, we know when 4(1+σ1[2])[1+2][α][ < η <][ 1]4 [and][ δ >] 1−[√]21−4η _, we have λB converge to_

1/(2α) 1/(2α)

zero and λS converge to 1+[√]21−4η _. That is, matrix_ _W converges to_ 1+[√]21−4η _PS._
    □


-----

B.5 TECHNICAL LEMMAS

_OLemma 3.(d/ϵˆ[2]), with probability at least Suppose {x[(][i][)], x[(]1[i][)][, x] 12[(][i] −[)][}]i[n]O=1(d[are sampled as decribed in Section 3.][2]) exp_ _−Ω(ˆϵ[2]n/d[2])_ _, we have_ _Suppose n ≥_
  


_x[(]1[i][)][[][x]1[(][i][)][]][⊤]_ _[−]_ _[I][ −]_ _[σ][2][P][B]_
_i=1_

X


_x[(]1[i][)][[][x]2[(][i][)][]][⊤]_ _[−]_ _[I]_
_i=1_

X


_x[(][i][)][x[(][i][)]][⊤]_ _−_ _I_
_i=1_

X


_n_ _x[(]1[i][)][[][x]1[(][i][)][]][⊤]_ _[−]_ _[I][ −]_ _[σ][2][P][B]_ _[,]_ _n_ _x[(]1[i][)][[][x]2[(][i][)][]][⊤]_ _[−]_ _[I]_ _[,]_ _n_ _x[(][i][)][x[(][i][)]][⊤]_ _−_ _I_ _ϵ._

_i=1_ _i=1_ _i=1_ _[≤]_ [ˆ]

X X X

**Proof of Lemma 3. For each x[(]1[i][)][, we can write it as][ x][(][i][)][ +][ z]1[(][i][)]** where x[(][i][)] _∼N_ (0, I) and z1[(][i][)]
(0, σ[2]PB). So we have
_N_

_n_ _n_

1

_x[(]1[i][)][[][x]1[(][i][)][]][⊤]_ [= 1] _x[(][i][)][x[(][i][)]][⊤]_ + z1[(][i][)][[][z]1[(][i][)][]][⊤] [+][ x][(][i][)][[][z]1[(][i][)][]][⊤] [+][ z]1[(][i][)][[][x][(][i][)][]][⊤][] _._

_n_ _n_

_i=1_ _i=1_

X X 


According to Lemma 9, we know as long as n ≥ _O(d/ϵˆ[2]), with probability at least 1 −_
exp(−Ω(ˆϵ[2]n)), _n_


1

_x[(][i][)][x[(][i][)]][⊤]_ _I_

_n_ _−_

_i=1_

X

Similarly, with probability at least 1 − exp(−Ω(ˆϵ[2]n)),



_[≤]_ _ϵ.[ˆ]_


1

_z1[(][i][)][[][z]1[(][i][)][]][⊤]_ _[−]_ _[σ][2][P][B]_ _ϵ._

_n_

_i=1_ _[≤]_ [ˆ]

X

Next we bound _n[1]_ _ni=1_ _[x][(][i][)][[][z]1[(][i][)][]][⊤]_ _. We know each entry in matrix_ _n[1]_ _ni=1_ _[x][(][i][)][[][z]1[(][i][)][]][⊤]_ [is the]

average of n zero-mean O(1)-subexponential independent random variables. Therefore, according

P P

to the Bernstein’s inequality, for any fixed entry (k, l), with probability at least 1 − exp _−ϵˆ[2]n/d[2][]_ _,_

_n_  

1

_x[(][i][)][z1[(][i][)][]][⊤]_ _ϵ/d.ˆ_

" _n_ _i=1_ #k,l _≤_

X

Taking a union bound over all the entries, we know with probability at least 1 − _d[2]_ exp _−ϵˆ[2]n/d[2][]_ _,_

_n_ _n_

1 1  

_x[(][i][)][z1[(][i][)][]][⊤]_ _x[(][i][)][z1[(][i][)][]][⊤]_ _ϵ.ˆ_

_n_ _n_ _≤_

_i=1_ _[≤]_ _i=1_ _F_

X X

The same analysis also applies to _n[1]_ _ni=1_ _[z]1[(][i][)][[][x][(][i][)][]][⊤]_ _. Combing all the bounds, we know with_

probability at least 1 _O(d[2]) exp_ Ω(ˆϵ[2]n/d[2]) _,_

P

_−_ _−_
  


1

_x[(]1[i][)][[][x]1[(][i][)][]][⊤]_ _[−]_ _[I][ −]_ _[σ][2][P][B]_ _ϵ._

_n_

_i=1_ _[≤]_ [4ˆ]

X

Similarly, we can prove that with probability at least 1 − _O(d[2]) exp_ _−Ω(ˆϵ[2]n/d[2])_
 


1

_x[(]1[i][)][[][x]2[(][i][)][]][⊤]_ _[−]_ _[I]_ _ϵ._

_n_

_i=1_ _[≤]_ [4ˆ]

X

Changing ˆϵ to ˆϵ[′]/4 finishes the proof. □

**Lemma 4. Let X ∈** R[n][×][d] _be a standard Gaussian matrix, and let U ∈_ R[d][×][r] _be a matrix with_
_orthonormal columns. Suppose n ≥_ 2r, with probability at least 1 − exp(−Ω(n)), we know

1 1

Ω(1) _λmin_ _λmax_ _O(1)._
_≤_ _n_ _[U][ ⊤][X]_ _[⊤][XU]_ _≤_ _n_ _[U][ ⊤][X]_ _[⊤][XU]_ _≤_

   


-----

**Proof of Lemma 4. Since U has orthonormal columns, we know XU is a n × r matrix with each**
entry independently sampled from N (0, 1). According to Lemma 9, we know when n ≥ 2r, with
probability at least 1 − exp(−Ω(n)),

1 1

Ω(1) _σmin_ _σmax_ _O(1)._
_≤_ _√nXU_ _≤_ _√nXU_ _≤_

   

This immediately implies that

1 1

Ω(1) _λmin_ _λmax_ _O(1)._
_≤_ _n_ _[U][ ⊤][X]_ _[⊤][XU]_ _≤_ _n_ _[U][ ⊤][X]_ _[⊤][XU]_ _≤_

   

□

**Lemma 5. Let ∆** _be a d_ _×_ _d matrix with Frobenius norm ˆϵ, and let X be a n_ _×_ _d standard Gaussian_
_matrix. We know with probability at least 1 −_ exp(−Ω(n)),
1
_√nX∆_ _F_ _≤_ _O(ˆϵ)._


**Proof of Lemma 5. Let the singular value decomposition of ∆** be U ΣV _[⊤], where U, V have or-_
thonormal columns and Σ is a diagonal matrix with diagonals equal to singular values σi’s. Since
_∥∆∥F = ˆϵ, we know_ _i=1_ _[σ]i[2]_ [= ˆ]ϵ[2].

Since U is an orthonormal matrix, we know[P][d] _X[ˆ] := XU is still an n × d standard Gaussian matrix._
Next, we bound the Frobenius norm of _X := X[ˆ]_ Σ. It’s not hard to verify that all the entries in _X are_
sum of independent and sub-exponential random variables, we have for everyindependent Gaussian variables and _Xij[e] ∼N_ (0, σj[2][)][.][ According to the Bernstein’s inequality for] t > 0, [e]

[e]

_t[2]_ _t_

Pr  _i∈[nX],j∈[d]_ _Xij[2]_ _[−]_ _[n]ϵ[ˆ][2]_ _≥_ _t_ _≤_ 2 exp "−c min _i∈[n],j∈[d]_ _[σ]j[4]_ _,_ maxj∈[d] σj[2] !# _._

 e  P

Since _j=1_ 2[σ]j[2] = ∥∆∥F[2] = ˆϵ[2], we know maxj∈[d] σj[2] _≤_ _ϵˆ[2]. We also have_ _j∈[d]_ _[σ]j[4]_ _≤_

_j_ [d] _[σ]j[2]_ = ˆϵ[4]. Therefore, we have
_∈_ [P][d] [P]

P 

2
_t_

Pr  _i_ [n],j [d] _Xij[2]_ _[−]_ _[n]ϵ[ˆ][2]_ _≥_ _t_ _≤_ 2 exp −c min  _nϵˆ[4][, t]ϵˆ[2]_  _._

_∈_ X∈

 e  2

Replacing t by nϵˆ[2], we concluded that with probability at least 1 exp( Ω(n)), _X_ _ϵ[2]._

Furthermore, since _V_ _[⊤]_ = 1, we have _−_ _−_ _F_ _[≤]_ [2][n][ˆ]

[e]

1 1 1
_√nX∆_ _F_ = _√n_ _XV_ _[⊤]_ _F_ _≤_ _√n_ _X_ _F_ _∥V ∥≤_ _O(ˆϵ)._
e e □

**Lemma 6. Let ∆[⊤]** _be a d×d matrix with Frebenius norm ˆϵ and let X_ _[⊤]_ _be a d×n standard Gaussian_
_matrix. Let_ _ξ[¯] be a unit vector with dimension n. We know with probability at least 1 −_ _ζ/3,_
∆[⊤]X _[⊤]ξ¯_ _≤_ _O(ˆϵ_ log(1/ζ)).
p

**Proof of Lemma 6. Let the sigular value decomposition of ∆[⊤]** be U ΣV _[⊤]. We know X_ _[⊤]ξ[¯] is a d-_
dimensional standard Gaussian vector. Further, we know V _[⊤]X_ _[⊤]ξ[¯] is also a d-dimensional standard_
Gaussian vector. So ΣV _[⊤]X_ _[⊤]ξ[¯] has independent Gaussian entries with its i-th entry distributed_
as N (0, σi[2][)][.][ According to the Bernstein’s inequality for sum of independent and sub-exponential]
random variables, we have for every t > 0,

_t2_
Pr ΣV _[⊤]X_ _[⊤]ξ¯_ _−_ _ϵˆ[2]_ _≥_ _t_ _≤_ 2 exp _−c min_ _ϵˆ[4][, t]ϵˆ[2]_ _._
h i   

[2]


-----

Choosing t as O(ˆϵ[2] log(1/ζ)), we know with probability at least 1 − _ζ/3, we have_
ΣV _[⊤]X_ _[⊤]ξ¯_ _≤_ _O_ _ϵˆ[2]_ log(1/ζ) _._

Since _U_ = 1, we further have   
_∥_ _∥_ [2]
∆[⊤]X _[⊤]ξ¯_ = _U_ ΣV _[⊤]X_ _[⊤]ξ¯_ _≤∥U_ _∥_ ΣV _[⊤]X_ _[⊤]ξ¯_ _≤_ _O_ _ϵˆ_ log(1/ζ)
 p  □

**Lemma 7. Let P ∈** R[d][×][d] _be a projection matrix on a r-dimensional subspace, and let_ _ξ[¯] be a unit_
_vector in R[d]. Let X_ _[⊤]_ _be a d × n standard Gaussian matrix that is independent with P and ξ. With_
_probability at least 1 −_ _ζ/3, we have_
_P_ _[⊤]X_ _[⊤]ξ¯_ _≤_ _[√]r + O(plog(1/ζ))._

**Proof of Lemma 7. Since P is a projection matrix on an r-dimensional subspace, we can write P**
as UU _[⊤], where U ∈_ R[d][×][r] has orthonormal columns. We know U _[⊤]X_ _[⊤]_ is still a standard Gaussian
matrix with dimension r × n. Furthermore, U _[⊤]X_ _[⊤]ξ[¯] is an r-dimensional standard Gaussian vector._
According to Lemma 8, with probability at least 1 − _ζ/3, we have_
_U_ _[⊤]X_ _[⊤]ξ¯_ _≤_ _[√]r + O(_ log(1/ζ)).

Since _U_ = 1, we further have p
_∥_ _∥_
_P_ _[⊤]X_ _[⊤]ξ¯_ = _UU_ _[⊤]X_ _[⊤]ξ¯_ _≤∥U_ _∥_ _U_ _[⊤]X_ _[⊤]ξ¯_ _≤_ _[√]r + O(_ log(1/ζ)).
p □

C ANALYSIS OF DEEP LINEAR NETWORKS

In this section, we extend the analysis in Section 3.2 to deep linear networks. We consider the same
data distribution as defined in Assumption 2. We consider the following network,
**Assumption 4 (Deep linear network). The online network is an l-layer linear networks**
_WlWl_ 1 _W1 with each Wi_ R[d][×][d]. The target network has the same architecture with weight
_matrices−_ _W · · ·a,lWa,l_ 1 _Wa,1 ∈. For convenience, we denote W as WlWl_ 1 _W1 and denote Wa_
_as Wa,lWa,l−1 · · · W−_ _a, · · ·1._ _−_ _· · ·_

**Training procedure:** At the initialization, we initialize each Wi as δ[1][/l]Id. Through the training,
we fix Wp as _WW_ _[⊤][][α]_ and fix each Wa,i as Wi. We run gradient flow on every Wi with weight
decay η. The population loss is
 

_L(_ _Wi_ _, Wp,_ _Wa,i_ ) := [1]
_{_ _}_ _{_ _}_ 2 [E][x][1][,x][2][ ∥][W][p][W][l][W][l][−][1][ · · ·][ W][1][x][1][ −] [StopGrad][(][W][a,l][W][a,l][−][1][ · · ·][ W][a,][1][x][2][)][∥][2][ .]

**Theorem 5. Suppose the data distribution and network architecture satisfies Assumption 2 and As-**
_sumption 4, respectively. Suppose we train the network as described above. Assuming the weight_
_decay coefficient_


_η_ 2αl(2αl+2l−2)[1+ 1]α _[−]_ _αl[1]_ _α_ _[−]_ _αl[1]_ _, and initialization scale δ_
_∈_  (4αl+2[1] _l−2)[2+ 1]α_ _[−]_ _αl[1]_ (1+σ[2])[1+ 1]α _[−]_ _αl[1]_ _[,][ 2][αl](4[(2]αl[αl]+2[+2]l−[l][−]2)[2)][2+ 1][1+ 1]α_ _[−]_ _αl[1]_  _≥_

2αl+2l 2 2α
4αl+2l−−2 _, we know[1]_ _W converges to cPS as time goes to infinity, where c is a positive number_
 2αl+2l 2 2α

_within_ 4αl+2l−−2 _, 1_ _._
  

Similar as in the setting of single-layer linear networks, we prove Theorem 5 by analyzing the
dynamics of the eigenvalues of W. Note that with constant α, the upper/lower bounds for η and
scalar c in the Theorem are always constants no matter how large l is.

**Proof of Theorem 5. For j** _i, we use W[j:i] to denote WjWj_ 1 _Wi and for j < i have_
_W[j:i] = I. We use similar notations for ≥_ _Wa,[j:i]. For each Wi, we can compute its dynamics as−_ _· · ·_
follows:
_W˙_ _i =_ _WpW[l:i+1]_ _⊤_ _WpW_ (I + σ[2]PB) _W[i_ 1:1] _⊤_ + _WpWa,[l:i+1]_ _⊤_ _Wa_ _Wa,[i_ 1:1] _⊤_ _ηWi._
_−_ _−_ _−_ _−_
              


-----

It’s clear that through the training all Wi’s remains the same and they are simultaneously diagonalizable with Wp, I and PB. We also have Wa = W and Wp = |W _|[2][α]_ _. Since we will ensure that W_
is always positive semi-definite so Wp = _W_ = W [2][α] = Wi[2][αl]. So the dynamics for each Wi
_|_ _|[2][α]_
can be simplified as follows:

_W˙_ _i =_ _Wi[4][αl][+2][l][−][1](I + σ[2]PB) + Wi[2][αl][+2][l][−][1]_ _ηWi._
_−_ _−_

Let the eigenvalue decomposition of Wi be _i=1_ _[ν][i][u][i][u]i[⊤][,][ with span][(][{][u][d][−][r][+1][,][ · · ·][, u][d][}][)][ equals to]_
subspace B. We can separately analyze the dynamics of each νi. Furthermore, we know ν1, _, νr_
_· · ·_
have the same value νS and νd _r+1,_ _, νd have the same value νB. We can write down the dy-_
_−_ _· · ·_ [P][d]
namics for νS and νB as follows,

_ν˙S =_ _νS[4][αl][+2][l][−][1]_ + νS[2][αl][+2][l][−][1] _ηνS,_
_−_ _−_

_ν˙B =_ _νB[4][αl][+2][l][−][1](1 + σ[2]) + νB[2][αl][+2][l][−][1]_ _ηνB._
_−_ _−_

Let λS be the eigenvalue of W corresponding to eigen-directions u1, · · ·, ur, and let λB be the
eigenvalue of W corresponding to eigen-directions ud−r+1, · · ·, ud. We know λS = νS[l] [and][ λ][B][ =]
_νB[l]_ _[.][ So we can write down the dynamics for][ λ][B][ as follows,]_

_λ˙_ _B = lνB[l][−][1]ν˙B =_ _lνB[4][αl][+3][l][−][2](1 + σ[2]) + lνB[2][αl][+3][l][−][2]_ _lηνB[l]_
_−_ _−_

= _lλ[4]B[α][+3][−][2][/l](1 + σ[2]) + lλB[2][α][+3][−][2][/l]_ _lηλB,_
_−_ _−_

and similarly for λS we have


_λ˙_ _S =_ _lλ[4]S[α][+3][−][2][/l]_ + lλ[2]S[α][+3][−][2][/l] _lηλS._
_−_ _−_

**Dynamics for λB:** We can write the dynamics on λB as follows,

_λ˙_ _B = lλBg(λB),_

where g(λB) := _λB[4][α][+2][−][2][/l](1 + σ[2]) + λB[2][α][+2][−][2][/l]_ _η. We show that when η is large enough,_
_−_ _−_
_g(λB) is negative for any positive λB. We compute the maximum value of g(λB) for λB > 0. We_
first compute the derivative of g as follows:

_g[′](λB) =_ (4α + 2 2/l)(1 + σ[2])λB[4][α][+1][−][2][/l] + (2α + 2 2/l)λ[2]B[α][+1][−][2][/l]
_−_ _−_ _−_
=λB[2][α][+1][−][2][/l] _−(4α + 2 −_ 2/l)(1 + σ[2])λ[2]B[α] [+ (2][α][ + 2][ −] [2][/l][)] _._

2αl+2l 2
It’s clear that( (4αl+22αll+22)(1+l−2 _gσ[2][′])([,]λ[ +]B[∞]) >[)][.][ Therefore, the maximum value of] 0 for _ _λ[2]B[α]_ _∈_ (0, (4αl+2l−2)(1+− _[ g]σ[(][2][λ])[B][)][ and][)][ for positive][ g][′][(][λ][B][)][ <][ λ][B][ 0][ takes at][ for][ λ][ λ]B[2][α]B[∗]_ [=]∈

_−_ [1]

(4αl+22αll+2−2)(1+l−2 _σ[2])_ 2α and

 2αl + 2l 2 2+ _α[1]_ _[−]_ _αl[1]_ 2αl + 2l 2 1+ _α[1]_ _[−]_ _αl[1]_

_g(λ[∗]B[) =][ −]_ _−_ (1 + σ[2]) + _−_ _η_

(4αl + 2l 2)(1 + σ[2]) (4αl + 2l 2)(1 + σ[2]) _−_

 _−_   _−_ 

2αl(2αl + 2l 2)[1+][ 1]α _[−]_ _αl[1]_
= _−_

(4αl + 2l − 2)[2+][ 1]α _[−]_ _αl[1]_ (1 + σ[2])[1+][ 1]α _[−]_ _αl[1]_ _[−]_ _[η.]_

As long as η > 2αl(2αl+2l−2)[1+ 1]α _[−]_ _αl[1]_

(4αl+2l−2)[2+ 1]α _[−]_ _αl[1]_ (1+σ[2])[1+ 1]α _[−]_ _αl[1]_ _[,][ we know][ g][(][λ][B][)][ <][ 0][ for any][ λ][B][ >][ 0][, which]_

further implies that _λ[˙]_ _B < 0 for any λB > 0. So λB converges to zero._


**Dynamics for λS :** We can write down the dynamics on λS as follows,

_λ˙_ _S = lλSh(λS),_

where h(λS) = _λS[4][α][+2][−][2][/l]_ + λ[2]S[α][+2][−][2][/l] _η. We compute the derivative of h as follows:_
_−_ _−_

_h[′](λS) = λ[2]S[α][+1][−][2][/l]_ _−(4α + 2 −_ 2/l)λ[2]S[α] [+ (2][α][ + 2][ −] [2][/l][)] _._
  


-----

[1] [1]
So h(λS) is increasing in (0, 24αlαl+2+2ll−22 2α ) and is decreasing in ( 24αlαl+2+2ll−22 2α, ). The maxi_−_ [1] _−_ _∞_

mum value of h for positive λS takes at λ[∗]S [=] 42αlαl+2+2ll−22 2α and we have 
_−_
  _α_ _αl_

_[−]_ [1]

_h(λ[∗]S[) = 2][αl][(2][αl][ + 2][l][ −]_ [2)][1+][ 1] _η._

(4αl + 2l − 2)[2+][ 1]α _[−]_ _αl[1]_ _−_

As long as η < [2][αl](4[(2]αl[αl]+2[+2]l−[l][−]2)[2)][2+ 1][1+ 1]α _[−]α_ _[−]αl[1]_ _αl[1]_ _, we have h(λ[∗]S[)][ >][ 0][.][ Furthermore, since][ h][ is increasing in]_

(0, λ[∗]S[)][ and is decreasing in][ (][λ]S[∗] _[,][ ∞][)][ and][ h][(0)][, h][(][∞][)][ <][ 0][,][ we know there exists][ λ][−]S_ _S[)][, λ][+]S_
(λ[∗]S[,][ ∞][)][ such that][ h][(][λ][S][)][ <][ 0][ in][ (0][, λ][−]S [)][,][ h][(][λ][S][)][ >][ 0][ in][ (][λ]S[−][, λ]S[+][)][ and][ h][(][λ][S][)][ <][∈][ 0][(0][ in][, λ][ (][∗][λ]S[+][,][ ∞][∈][)][.]
Therefore, as long as δ _λ[∗]S_ _[> λ]S[−][,][ we have][ λ][S][ converges to][ λ]S[+][.][ Since][ h][(1)][ <][ 0][,][ we know]_

[1] _≥_
_λ[+]S_ _[∈]_ [(] 24αlαl+2+2ll−−22 2α, 1).
 

Overall as long as η 2αl(2αl+2l−2)[1+ 1]α _[−]_ _αl[1]_ _α_ _[−]_ _αl[1]_ _, we know W_
_∈_  (4αl+2l−2)[2+ 1]α _[−]_ _αl[1]_ (1+σ[2])[1+ 1]α _[−]_ _αl[1]_ _[,][ 2][αl](4[(2]αl[αl]+2[+2][1]l−[l][−]2)[2)][2+ 1][1+ 1]α_ _[−]_ _αl[1]_ 

converges to cPS, where c is a positive number within ( 24αlαl+2+2ll−−22 2α, 1). □
 

D ANALYSIS OF PREDICTOR REGULARIZATION.


In this section, we study the influence of predictor regularization in a simple linear setting. In
particular, we consider the same setting as in Section 3.2 except that we set Wp := (WW _[⊤])[α]_ + ϵI.

**Theorem 6. In the setting of Theorem 1 except that we set Wp = (WW** _[⊤])[α]_ + ϵI. We have



[1]
_−_ _ϵ, 0_ 2α, we have W con



-  when ϵ [0, [1+][√]2[1][−][4][η] ), as long as δ > max 1−[√]21−4η
_∈_ [1]

_verges to_ 1+[√]21−4η _ϵ_ 2α PS;  

_−_

 

-  when ϵ 2 _, W always converges to zero._
_≥_ [1+][√][1][−][4][η]


**Proof of Theorem 6. We can write the dynamics of W as follows,**
_W˙_ =Wp[⊤][(][−][W][p][W] [(][I][ +][ σ][2][P][B][) +][ W][a][)][ −] _[ηW]_

2
=W (I + σ[2]PB) _W_ + ϵI + _W_ + ϵI _η_ _._
_−_ _|_ _|[2][α]_ _|_ _|[2][α]_ _−_
     

Let the eigenvalue decomposition of W be _i=1_ _[λ][i][u][i][u]i[⊤][,][ with span][(][{][u][d][−][r][+1][,][ · · ·][, u][d][}][)][ equals to]_
subspace B. We can separately analyze the dynamics of each λi. Furthermore, we know λ1, _, λr_
_· · ·_
have the same value λS and λd _r+1,_ _, λd have the same value λB._
_−_ _· · ·_ [P][d]

**Dynamics for λB:** We can write down the dynamics for λB as follows:

2

_λ˙_ _B = λB_ _−(1 + σ[2])_ _|λB|[2][α]_ + ϵ + _|λB|[2][α]_ + ϵ _−_ _η_

When η > 4(1+1σ[2]) _[,][ we still know]_ [ ˙]λB < 0 for any λB > 0 and _λB = 0 is a critical point. So_ _λB_

converges to zero.

**Dynamics for λS:** We can write down the dynamics for λS as follows:

2

_λ˙_ _S =λS_ _−_ _|λS|[2][α]_ + ϵ + _|λS|[2][α]_ + ϵ _−_ _η_

     

= _λS_ _λS_ + ϵ _λS_ + ϵ _,_
_−_ _|_ _|[2][α]_ _−_ [1][ −√]2[1][ −] [4][η] _|_ _|[2][α]_ _−_ [1 +][ √]2[1][ −] [4][η]

   

where the second inequality assumes 0 < η < [1]4 _[.][ We have]_


-----

[1]
_−_ _ϵ, 0_ 2α, we have λS con



-  when ϵ [0, [1+][√]2[1][−][4][η] ), as long as δ > max 1−[√]21−4η
_∈_ [1]

1+[√]1 4η 2α  
verges to 2 _−_ _ϵ_ _> 0;_

_−_

 

-  when ϵ 2 _, λS always converges to zero._
_≥_ [1+][√][1][−][4][η]

E TECHNICAL TOOLS

E.1 NORM OF RANDOM VECTORS


The following lemma shows that a standard Gaussian vector with dimension n has ℓ2 norm concentrated at _n._

_[√]_

**Lemma 8 (Theorem 3.1.1 in Vershynin (2018)). Let X = (X1, X2, · · ·, Xn) ∈** R[n] _be a random_
_vector with each entry independently sampled from N_ (0, 1). Then

Pr[ _∥x∥−_ _[√]n_ _≥_ _t] ≤_ 2 exp(−t[2]/C [2]),

_where C is an absolute constant._

E.2 SINGULAR VALUES OF GAUSSIAN MATRICES

The following lemma shows a tall random Gaussian matrix is well-conditioned with high probability.

**Lemma 9 (Corollary 5.35 in Vershynin (2010)). Let A be an N × n matrix whose entries are**
_independent standard normal random variables. Then for every t ≥_ 0 with probability at least
1 − 2 exp(−t[2]/2) one has
_√N_ _n_ _t_ _smin(A)_ _smax(A)_ _√N +_ _n + t_

_−_ _[√]_ _−_ _≤_ _≤_ _≤_ _[√]_

E.3 PERTURBATION BOUND FOR MATRIX PSEUDO-INVERSE

With a lowerbound on σmin(A), we can get bounds for the perturbation of pseudo-inverse.

**Lemma 10 (Theorem 3.4 in Stewart (1977)). Consider the perturbation of a matrix A ∈** R[m][×][n] :
_B = A + E. Assume that rank(A) = rank(B) = n, then_
_B[†]_ _−_ _A[†]_ _≤_ _√2_ _A[†]_ _B[†]_ _∥E∥_ _._

The following corollary is particularly useful for us.

**Lemma 11 (Lemma G.8 in Ge et al. (2015)). Consider the perturbation of a matrix A ∈** R[m][×][n] :
_B = A + E where_ _E_ _σmin(A)/2. Assume that rank(A) = rank(B) = n, then_
_∥_ _∥≤_
_B[†]_ _A[†]_ 2√2 _E_ _/σmin(A)[2]._
_−_ _≤_ _∥_ _∥_


-----

