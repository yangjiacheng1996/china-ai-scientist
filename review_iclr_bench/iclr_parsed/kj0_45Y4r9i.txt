# DISCRIMINATIVE SIMILARITY FOR DATA CLUSTERING


**Yingzhen Yang**
School of Computing and Augmented Intelligence
Arizona State University
Tempe, AZ 85281, USA
yingzhen.yang@asu.edu

ABSTRACT


**Ping Li**
Cognitive Computing Lab
Baidu Research
Bellevue, WA 98004, USA
liping11@baidu.com


Similarity-based clustering methods separate data into clusters according to the
pairwise similarity between the data, and the pairwise similarity is crucial for their
performance. In this paper, we propose Clustering by Discriminative Similarity
_(CDS), a novel method which learns discriminative similarity for data clustering._
CDS learns an unsupervised similarity-based classifier from each data partition,
and searches for the optimal partition of the data by minimizing the generalization
error of the learnt classifiers associated with the data partitions. By generalization analysis via Rademacher complexity, the generalization error bound for the
unsupervised similarity-based classifier is expressed as the sum of discriminative
similarity between the data from different classes. It is proved that the derived discriminative similarity can also be induced by the integrated squared error bound
for kernel density classification. In order to evaluate the performance of the proposed discriminative similarity, we propose a new clustering method using a kernel as the similarity function, CDS via unsupervised kernel classification (CDSK),
with its effectiveness demonstrated by experimental results.

1 INTRODUCTION

Similarity-based clustering methods segment the data based on the similarity measure between the
data points, such as spectral clustering (Ng et al., 2001), pairwise clustering method (Shental et al.,
2003), K-means (Hartigan & Wong, 1979), and kernel K-means (Sch¨olkopf et al., 1998). The success of similarity-based clustering highly depends on the underlying pairwise similarity over the
data, which in most cases are constructed empirically, e.g., by Gaussian kernel or the K-NearestNeighbor (KNN) graph. In this paper, we model data clustering as a multiclass classification problem and seek for the data partition where the associated classifier, trained on cluster labels, can have
low generalization error. Therefore, it is natural to formulate data clustering problem as a problem
of training unsupervised classifiers: a classifier can be trained upon each candidate partition of the
data, and the quality of the data partition can be evaluated by the performance of the trained classifier. Such classifier trained on a hypothetical labeling associated with a data partition is termed an
unsupervised classifier.

We present Clustering by Discriminative Similarity (CDS), wherein discriminative similarity is derived by the generalization error bound for an unsupervised similarity-based classifier. CDS is based
on a novel framework of discriminative clustering by unsupervised classification wherein an unsupervised classifier is learnt from unlabeled data and the preferred hypothetical labeling should
minimize the generalization error bound for the learnt classifier. When the popular Support Vector
Machines (SVMs) is used in this framework, unsupervised SVM (Xu et al., 2004) can be deduced. In
this paper, a similarity-based classifier motivated by similarity learning (Balcan et al., 2008; Cortes
et al., 2013), is used as the unsupervised classifier. By generalization analysis via Rademacher complexity, the generalization error bound for the unsupervised similarity-based classifier is expressed as
sum of pairwise similarity between the data from different classes. Such pairwise similarity, parameterized by the weights of the unsupervised similarity-based classifier, serves as the discriminative
similarity. The term “discriminative similarity” emphasizes the fact that the similarity is learnt so as

Yingzhen Yang’s work was conducted as a consulting researcher at Baidu Research - Bellevue, WA, USA.


-----

to improve the discriminative capability of a certain classifier such as the aforementioned unsupervised similarity-based classifier.

1.1 CONTRIBUTIONS AND MAIN RESULTS

Firstly, we present Clustering by Discriminative Similarity (CDS) where discriminative similarity
is induced by the generalization error bound for unsupervised similarity-based classifier on unlabeled data. The generalization bound for such similarity-based classifier is of independent interest,
which is among the few results of generalization bounds for classification using general similarity functions (Section B.1 of Appendix). When the general similarity function is set to a Positive
Semi-Definite (PSD) kernel, the derived discriminative similarity between two data points xi,xj is
_Sij[K]_ [= 2(][α][i] [+][α][j][ −][λα][i][α][j][)][K][(][x][i][ −] **[x][j][)][, where][ K][ can be an arbitrary PSD kernel and][ α][i][ is the kernel]**
weight associated with xi. With theoretical and empirical study, we argue that Sij[K] [should be used]
for data clustering instead of the conventional kernel similarity corresponding to uniform kernel
weights. In the case of binary classification, we prove that the derived discriminative similarity Sij[K]
has the same form as the similarity induced by the integrated squared error bound for kernel density
classification (Section A of the appendix). Such connection suggests that there exists informationtheoretic measure which is implicitly equivalent to our CDS framework for unsupervised learning,
and our CDS framework is well grounded for learning similarity from unlabeled data.

Secondly, based on our CDS model, we develop a clustering algorithm termed Clustering by Discriminative Similarity via unsupervised Kernel classification (CDSK) in Section 5. CDSK uses a
PSD kernel as the similarity function, and outperforms competing clustering algorithms, including
nonparametric discriminative similarity based clustering methods and similarity graph based clustering methods, demonstrating the effectiveness of CDSK. When the kernel weights _αi_ are uniform,
_{_ _}_
CDSK is equivalent to kernel K-Means (Sch¨olkopf et al., 1998). CDSK is more flexible by learning
adaptive kernel weights associated with different data points.

1.2 CONNECTION TO RELATED WORKS

Our CDS model is related to a class of discriminative clustering methods which classify unlabeled
data by various measures on discriminative unsupervised classifiers, and the measures include generalization error (Xu et al., 2004) or the entropy of the posterior distribution of the label (Gomes
et al., 2010). Discriminative clustering methods (Xu et al., 2004) predict the labels of unlabeled
data by minimizing the generalization error bound for the unsupervised classifier with respect to
the hypothetical labeling. Unsupervised SVM is proposed in Xu et al. (2004) which learns a binary
classifier to partition unlabeled data with the maximum margin between different clusters. The theoretical properties of unsupervised SVM are further analyzed in Karnin et al. (2012). Kernel logistic
regression classifier is employed in Gomes et al. (2010), and it uses the entropy of the posterior distribution of the class label by the classifier to measure the quality of the hypothetical labeling. CDS
model performs discriminative clustering based on a novel unsupervised classification framework by
considering similarity-based or kernel classifiers which are important classification methods in the
supervised learning literature. In contrasts with kernel similarity with uniform weights, the induced
discriminative similarity with learnable weights enhances its capability to represent complex interconnection between data. The generalization analysis for CDS is primarily based on distribution free
Rademacher complexity. While Yang et al. (2014a) propose nonparametric discriminative similarity
for clustering, the nonparametric similarity requires probability density estimation which is difficult
for high-dimensional data, and the fixed nonparametric similarity is not adaptive to complicated data
distribution.

The paper is organized as follows. We introduce the problem setup of Clustering by Discriminative Similarity in Section 3. We then derive the generalization error bound for the unsupervised
similarity-based classifier for CDS in Section 4 where the proposed discriminative similarity is induced by the error bound. The application of CDS to data clustering is shown in Section 5. Throughout this paper, the term kernel stands for the PSD kernel if no special notes are made.


-----

2 SIGNIFICANCE OF CDSK OVER EXISTING DISCRIMINATIVE AND
SIMILARITY-BASED CLUSTERING METHODS

Effective data similarity highly depends on the underlying probabilistic distribution and geometric
structure of the data, and these two characteristics leads to “data-driven” similarity, such as Zhu
et al. (2014); Bicego et al. (2021); Ng et al. (2001); Shental et al. (2003); Hartigan & Wong (1979);
Sch¨olkopf et al. (1998) and similarity based on geometric structure of the data, such as the subspace
structure (Sparse Subspace Clustering, or SSC in Elhamifar & Vidal (2013)). Note that the sparse
graph method, ℓ[1]-Graph (Yan & Wang, 2009), has the same formulation as SSC. Most existing clustering methods based on data-driven or geometric structure-driven similarity suffer from a common
deficiency, that is, the similarity is not explicitly optimized for the purpose of separating underlying
clusters. In particular, the Random Forest-based similarity (Zhu et al., 2014; Bicego et al., 2021) is
extracted from features in decision trees. Previous works about subspace-based similarity (Yan &
Wang, 2009; Elhamifar & Vidal, 2013) try to make sure that only data points lying on or close to
the same subspace have nonzero similarity, so that data points from the same subspace can form a
cluster. However, it is not guaranteed that features in the decision trees are discriminative enough to
separate clusters, because the candidate data partition (or candidate cluster labels) do not participate
in the feature or similarity extraction process. Note that synthetically generated negative class are
suggested in Zhu et al. (2014); Bicego et al. (2021) to train unsupervised random forest, however, the
synthetic labels are not for the original data. Moreover, it is well known that the existing subspace
learning methods only obtain reliable subspace-based similarity with restrictive geometric assumptions on the data and the underlying subspaces, such as large principal angle between intersecting
subspaces (Soltanolkotabi & Candes, 2012; Elhamifar & Vidal, 2013).

Therefore, it is particularly important to derive similarity for clustering which meets two requirements: (1) discriminative measure with information such as cluster partition is used to derive such
similarity so as to achieve compelling clustering performance; (2) it requires less restrictive assumptions on the geometric structure of the data than current geometric structure-based similarity learning
methods, such as subspace clustering (Yan & Wang, 2009; Elhamifar & Vidal, 2013).

**Significance.** The proposed discriminative similarity of this paper meets these two requirements.
First, the discriminative similarity is derived by the generalization error bound associated with candidate cluster labeling, and minimizing the objective function of our optimization problem for clustering renders a joint optimization of discriminative similarity and candidate cluster labeling in a way
such that the similarity-based classifier has small generalization error bound. Second, our framework
only assumes a mild classification model in Definition 3.1, which only requires an unknown joint
distribution over data and its labels. In this way, the restrictive geometric assumptions are avoided in
our method. Compared to the existing discriminative clustering methods, such as MMC (Xu et al.,
2004), BMMC (Chen et al., 2014), RIM (Gomes et al., 2010), and the other discriminative clustering
methods such as (Huang et al., 2015; Nguyen et al., 2017), the optimization problem of CDSK with
discriminative similarity-based formulation is much easier to solve and it enjoys convexity and efficiency in each iteration of coordinate descent described in Algorithm 1. In particular, as mentioned
in Section D of the appendix, the first step (11) of each iteration can be solved by efficient SVD or
other randomized large-scale SVD methods, and the second step (12) of each iteration can be solved
by efficient SMO (Platt, 1998). Moreover, the optimization problems in these two steps are either
convex or having closed-form solution. In contrast, MMC requires expensive semidefinite programming. RIM has to solve a nonconvex optimization problem and its formulation does not guarantee
that the trained multi-class kernelized logistic regression has low classification error on candidate
labeling, which explains why it has inferior performance compared to our method. The discriminative Extreme Learning Machine (Huang et al., 2015) trains ELM using labels produced by a simple
clustering method such as K-means, and the potentially poor cluster labels by the simple clustering
method can easily result in unsatisfactory performance of this method. The discriminative Bayesian
nonparametric clustering (Nguyen et al., 2017) and BMMC (Chen et al., 2014) require extra efforts
of sampling hidden variables and tuning hyperparameters to generate the desirable number of clusters (or model selection), which could reduce the effect of discriminative measures used in these
Bayesian nonparametric methods.


-----

3 PROBLEM SETUP

We introduce the problem setup of the formulation of clustering by unsupervised classification.
Given unlabeled data **xl** _l=1_
beling which is optimal in some sense. Each hypothetical labeling corresponds to a candidate data { _}[n]_ _[⊂]_ [R][d][, clustering is equivalent to searching for the hypothetical la-]
partition. Figure 1 illustrates four binary hypothetical labelings which correspond to four partitions
of the data, and the data is divided into two clusters by each hypothetical labeling.

|+− +− − − + − − + +|+− ++ − − + + − − −|− ++ + − − + + − − −|
|---|---|---|


+ −

+ + − − −

− −

+ +


Figure 1: Illustration of binary hypothetical labelings

The discriminative clustering literature (Xu et al., 2004; Gomes et al., 2010) has demonstrated the
potential of multi-class classification for clustering problem. Inspired by the natural connection
between clustering and classification, we proposes the framework of Clustering by Unsupervised
Classification which models clustering problem as a multi-class classification problem. A classifier
is learnt from unlabeled data with a hypothetical labeling, which is associated with a candidate
partition of the unlabeled data. The optimal hypothetical labeling is supposed to be the one such that
its associated classifier has the minimum generalization error bound. To study the generalization
bound for the classifier learnt from hypothetical labeling, the concept of classification model is
needed. Given unlabeled data {xl}l[n]=1[, a classification model][ M][Y][ is constructed for any hypothetical]
labeling Y = {yl}l[n]=1 [as follows.]

**Definition 3.1. The classification model corresponding to the hypothetical labeling Y = {yl}l[n]=1** [is]
defined as MY = (S, F ). S = {xl, yl}l[n]=1 [are the labeled data by the hypothetical labeling][ Y][, and]
_S are assumed to be i.i.d. samples drawn from the some unknown joint distribution PXY, where_
(X, Y ) is a random couple, X ∈X ⊆ R[d] represents the data in some compact domain X, and
_Y ∈{1, 2, ..., c} is the class label of X, c is the number of classes. F is a classifier trained on S._
The generalization error of the classification model M is defined as the generalization error of the
_Y_
classifier F in M .
_Y_

**The basic assumption of CDS is that the optimal hypothetical labeling minimizes the gener-**
**alization error bound for the classification model. With f being different classifiers, different**
discriminative clustering models can be derived. When SVMs is used as the classifier F in the
above discriminative model, unsupervised SVM (Xu et al., 2004) is obtained.

In Balcan et al. (2008), the authors proposes a classification method using general similarity functions. The classification rule measures the similarity of the test data to each class, and then assigns
the test data to the class such that the weighed average of the similarity between the test data and the
training data belonging to that class is maximized over all the classes. Inspired by this classification
method, we now consider using a general symmetric and continuous function S : X × X → [0, 1]
as the similarity function in our CDS model. We propose the following hypothesis,


_αiS(x, xi)._ (1)
_i_ : yi=y

X


_hS(x, y) =_


In the next section, we derive generalization bound for the unsupervised similarity-based classifier
based on the above hypothesis, and such generalization bound leads to discriminative similarities
for data clustering. When S is a PSD kernel, minimizing the generalization error bound amounts to
minimization of a new form of kernel similarity between data from different clusters, which lays the
foundation of a new clustering algorithm presented in Section 5.

4 GENERALIZATION BOUND FOR SIMILARITY-BASED CLASSIFIER

In this section, the generalization error bound for the classification model in Definition 3.1 with the
unsupervised similarity-based classifier is derived as a sum of discriminative similarity between the
data from different classes.


-----

4.1 GENERALIZATION BOUND

The following notations are introduced before our analysis. Let α = [α1, . . ., αn][⊤] be the nonzero
weights that sum up to 1, α[(][y][)] be a n × 1 column vector representing the weights belonging to
class y such that αi[(][y][)] is αi if y = yi, and 0 otherwise. The margin of the labeled sample (x, y) is
defined as mhS (x, y) = hS(x, y) − argmaxy′≠ _yhS(x, y[′]), the sample (x, y) is classified correctly_
if mhS (x, y) 0.
_≥_

The general similarity-based classifier fS predicts the label of the input x by fS(x) =
argmaxy∈{1,...,c}hS(x, y). We then begin to derive the generalization error bound for fS using the
Rademacher complexity of the function class comprised of all the possible margin functionsThe Rademacher complexity (Bartlett & Mendelson, 2003; Koltchinskii, 2001) of a function class mhS .
is defined below:

**Definition 4.1. Let {σi}i[n]=1** [be][ n][ i.i.d. random variables such that Pr][[][σ][i][ = 1] =][ Pr][[][σ][i][ =][ −][1] =][ 1]2 [.]

The Rademacher complexity of a function class A is defined as


R( ) = E _σi_ _,_ **xi**
_A_ _{_ _}_ _{_ _}_


(2)


sup
_h∈A_


_σih(xi)_
_i=1_

X


In order to analyze the generalization property of the classification rule using the general similarity
function, we first investigate the properties of general similarity function and its relationship to PSD
kernels in terms of eigenvalues and eigenfunctions of the associated integral operator. The integral
operator (LSf )(x) = _S(x, t)f_ (t)dt is well defined. It can be verified that LS is a compact
operator since S is continuous. According to the spectral theorem in operator theory, there exists
an orthonormal basis _ϕR1, ϕ2, . . ._ of which is comprised of the eigenfunctions of LS, where
_{_ _}_ _L[2]_
_L[2]_ is the space of measurable functions which are defined over X and square Lebesgue integrable.
_ϕk is the eigenfunction of LS with eigenvalue λk if LSϕk = λkϕk. The following lemma shows_
that under certain assumption on the eigenvalues and eigenfunctions of LS, a general symmetric and
continuous similarity can be decomposed into two PSD kernels.

**Lemma 4.1. Suppose S : X × X →** [0, 1] is a symmetric continuous function, and {λk} and {ϕk}
are the eigenvalues and eigenfunctions of LS respectively. Suppose _k≥1_ _λk|ϕk(x)|[2]_ _< C for some_

constant C > 0. Then S(x, t) = _λkϕk(x)ϕk(t) for any x, t_, and it can be decomposed as

_k≥1_ _∈X[P]_

the difference between two positive semi-definite kernels: S(x, t) = S[+](x, t) _S[−](x, t), with_

[P] _−_


_S[+](x, t) =_


_λkϕk(x)ϕk(t),_ _S[−](x, t) =_
_k : λXk≥0_


_λk_ _ϕk(x)ϕk(t)._ (3)
_|_ _|_
_k:λk<0_

X


We now use a regularization term to bound the Rademacher complexity for the classification


_c_

**_α[(][y][)][⊤]S[+]α[(][y][)]_** and Ω[−](α) =
_y=1_

P


rule using the general similarity function. Let Ω[+](α) =


_c_

_y=1_ **_α[(][y][)][⊤]S[−]α[(][y][)]_** with [S[+]]ij = S[+](xi, xj) and [S[−]]ij = S[−](xi, xj). The space Hy of all the

hypothesisP _hS(_ _, y) associated with label y is defined as_

_·_


_αiS(x, xi): α_ **0, 1[⊤]α = 1, Ω[+](α)** _B[+2], Ω[−](α)_ _B[−][2]_
_≥_ _≤_ _≤_ _}_
_i_ : yi=y

X


_HS,y = {(x, y) →_


for 1 _≤_ _y_ _≤_ _c, with positive number B[+]_ and B[−] which bound Ω[+] and Ω[−] respectively. Let the hypothesis space comprising all possible margin functions be HS = {(x, y) →
_mhS_ (x, y): hS(x, y) _S,y_ . We then present the main result in this section about the generaliza_∈H_ _}_
tion error of unsupervised similarity-based classifier fS.

**Theorem 4.2. Given the discriminative model M** = ( _, fS), suppose Ω[+](α)_ _B[+2], Ω[−](α)_
_Y_ _S_ _≤_ _≤_
_BThen with probability[−][2], supx∈X |S[+](x, 1 x −)| ≤δ over the labeled dataR[2], supx∈X |S[−](x, x S) with respect to any distribution in| ≤_ _R[2]_ for positive constants B[+] P, BXY[−], underand R.


-----

the assumptions of Lemma 4.1, the generalization error of the general classifier fS satisfies

_R(fS) =Pr [Y_ = fS(X)]
_̸_

_Rn(fS) + [8][R][(2][c][ −]_ [1)][c][(][B][+][ +][ B][−][)] + 16c(2c − 1)(B+ + B−)R2 + 1 log [4]δ
_≤_ [b] _γ[√]n_ _γ_ 2n [,]

  [s]

(4)

_n_ _hS_ (xi,yi) argmaxy =yhS (xi,y′)

where _Rn(fS) =_ _n[1]_ Φ _−_ _γ_ _′̸_ is the empirical loss of fS on the labeled

_i=1_

data, γ > 0 is a constant andP  Φ is defined as Φ(x) = min 1, max 0, 1 _x_ . Moreover, if γ 1,

[b] _{_ _{_ _−_ _}}_ _≥_

the empirical loss _Rn(fS) satisfies_

_n_ _n_

_αi + αj_

_Rn(fS)_ 1 [b] _S(xi, xj) + [1]_ 2(αi + αj)S(xi, xj)1Iyi=yj _._ (5)
_≤_ _−_ _nγ[1]_ 2 _nγ_ _̸_

_i,jX=1_ 1≤Xi<j≤n

b

The indicator function 1IE in (5) is 1 if event E is true, and 0 otherwise.

**Remark 4.3. Lemma E.3 in the Appendix shows that the Rademacher complexity of HS is bounded**
in terms of B[+] and B[−], and that is why these two quantities appear on the RHS of (4). In addition,
when S is a Positive Semi-Definite (PSD) kernel K, it can be verified that S[−] _≡_ 0, S = S[+].
**Remark 4.4. When the decomposition S = S[+]** _−_ _S[−]_ exists and S[+], S[−] are PSD kernels, S is the
kernel of some Reproducing Kernel Kre˘ın Space (RKKS) (Mary, 2003). Ong et al. (2004) and Loosli
et al. (2016) analyzed the problem of learning SVM-style classifiers with indefinite kernels from the
Kre˘ın space. However, their work does not show when and how an indefinite and general similarity
function can have PSD decomposition, as well as the generalization analysis for the similarity-based
classifier using such general indefinite function as similarity measure. Our analysis deals with these
problems in Lemma 4.1 and Theorem 4.2. It should be emphasized that our generalization bound is
of independent interest in supervised learning, because it is among the few results of generalization
bounds using general similarity-based classifier. Section B.1 shows that the our bound is a principled
result with strong connection to established generalization error bound for Support Vector Machines
(SVMs) or Kernel Machines.

4.2 CLUSTERING BY DISCRIMINATIVE SIMILARITY

We let

_Sij[sim]_ = 2(αi + αj)S(xi, xj) 2λαiαjS[+](xi, xj) 2λαiαjS[−](xi, xj) (6)
_−_ _−_

be the discriminative similarity between data from different classes, which is induced by the generalization error bound (4) for the unsupervised general similarity-based classifier fS. Minimizing the
bound (4) motivates us to consider the optimization problem that minimizes _Rn(fS) + λ_ Ω[+](α) +
Ω[−](α) . Replacing _Rn(fS) by its upper bound in (5), we consider the following problem,_
 

_n_ _n_ [b]

 _αi + αj_

min [b] _Sij[sim][1]Iyi=yj_ _S(xi, xj) + λ(α[⊤]S[+]α + α[⊤]S[−]α)_
**_α,Y_** 1≤Xi<j≤n _̸_ _−_ _i,jX=1_ 2

_s.t. α ≥_ **0, 1[⊤]α = 1, Y = {yi}i[n]=1[,]** (7)

where λ > 0 is the weighting parameter for the regularization term Ω[+](α) + Ω[−](α). Note that
we do not set λ to [16][c][(2][c][−][1)][R][2]√[+8]2γ√2R(2c−1)c exactly matching the RHS of (4), because λ controls

the weight of the regularization term which bounds the unknown complexity of the function class
_S. Note that (7) encourages the discriminative similarity Sij[sim]_ between the data from different
_H_
classes small. The optimization problem (7) forms the formulation of Clustering by Discriminative
Similarity (CDS).

By Remark 4.3, when S is a PSD kernel K, S[−] 0, S = S[+], Sij[sim] reduces to the following
_≡_
discriminative similarity for PSD kernels:

_Sij[K]_ [= 2(][α][i] [+][ α][j] (8)

_[−]_ _[λα][i][α][j][)][K][(][x][i]_ _[−]_ **[x][j][)][,][ 1][ ≤]** _[i, j][ ≤]_ _[n,]_


-----

and Sij[K] [is the similarity induced by the unsupervised kernel classifier by the kernel][ K][.]


Without loss of generality, we set K = Kτ (x) = exp( 2τ [2]2[ )][ which is the isotropic Gaussian]
_−_ _[∥][x][∥][2]_

kernel with kernel bandwidth τ > 0, and we omit the constant that makes integral of K unit.

When setting the general similarity function to kernel Kτ, CDS aims to minimize the error bound
for the corresponding unsupervised kernel classifier, which amounts to minimizing the following
objective function

_n_ _n_

_αi + αj_

min _Sij[K][1]Iyi=yj_ _Kτ_ (xi **xj) + λα[⊤]Kα,** (9)
**_α∈Λ,Y={yi}i[n]=1_** 1≤Xi<j≤n _̸_ _−_ _i,jX=1_ 2 _−_

where Sij[K] [is defined in (][8][) with][ K][ =][ K][τ] [.][ K][ ∈] [R][n][×][n][ and][ K][ij][ =][ K][τ] [(][x][i][ −] **[x][j][)][.][ λ][ is tuned]**
such that Sij[K]
also be induced from the perspective of kernel density classification by kernel density estimators[≥] [0][, e.g.,][ λ][ ≤] [2][. In Section][ A][, it is shown that the discriminative similarity (][8][) can]
with nonuniform weights. It supports the theoretical justification for the induced discriminative
similarity in this section.


_αi + αj_


min
**_α_** Λ, = _yi_ _i=1_
_∈_ _Y_ _{_ _}[n]_


5 APPLICATION TO DATA CLUSTERING

In this section, we propose a novel data clustering method termed Clustering by Discriminative
Similarity via unsupervised Kernel classification (CDSK) which is an empirical method inspired by
our CDS model when the similarity function is a PSD kernel K = Kτ . In accordance with the CDS
model in Section 4.2, CDSK aims to minimize (9). However, problem (9) involves minimization
with respect to discrete cluster labels = _yi_ which is NP-hard. In addition, it potentially results
_Y_ _{_ _}_
in a trivial solution which puts all the data in a single cluster due to the lack of constraints on
the cluster balance. When Y is a binary matrix where each column is a membership vector for a


particular cluster, 1≤i<j≤n _Sij[K][1]Iyi≠_ _yj =_ [1]2 [Tr(][Y][⊤][L][K][Y][)][. Therefore, (][9][) is relaxed in the proposed]

optimization problem for CDSK below:P


_αi + αj_


1
min
**_α∈Λ,Y∈R[n][×][c]_** 2 [Tr(][Y][⊤][L][K][Y][)][ −]


_Kτ_ (xi **xj) + λα[⊤]Kα** _s.t. Y[⊤]D[K]Y = Ic,_
_−_

(10)


_i,j=1_


where Λ = **_α_** : α **0, 1[⊤]α = 1**, S[K]ij [=][ S]ij[K][,][ L][K][ =][ D][K][ −] **[S][K][ is the graph Laplacian computed]**
_{_ _≥_ _}_
with S[K], D[K] is a diagonal matrix with each diagonal element being the sum of the corresponding


row of S[K]: [D[K]]ii = **S[K]ij** [,][ I][c][ is a][ c][ ×][ c][ identity matrix,][ c][ is the number of clusters. The]

_j=1_

constraint in (10) is used to balance the cluster size. This is because minimizing (P 9) without any
constraint on the cluster size results in a trivial solution where all data points form a single cluster.
Inspired by spectral clustering (Ng et al., 2001), the constraint Y[⊤]D[K]Y = Ic used in CDSK
prevents imbalanced data clusters.

Problem (10) is optimized by coordinate descent. In each iteration of coordinate descent, optimization with respect to Y is performed with fixed α, which is exactly the same problem as that of
spectral clustering with a solution formed by the smallest c eigenvectors of the normalized graph
Laplacian (D[K])[−][1][/][2]L[K](D[K])[−][1][/][2]; then the optimization with respect to α is performed with fixed
**Y, which is a standard constrained quadratic programming problem. The iteration of coordinate**
descent proceeds until convergence or the maximum iteration number M is achieved. Each iteration solves two subproblems, (11) and (12). In order to promote sparsity of α, α can be initialized

2

by solving _i=1_ **xi** _j=i_ **[x][j][α][j]**
_−_ [P] _̸_ 2 [+][ τ] _[∥][α][∥][0][ for a positive weighting parameter][ τ][ = 0][.][1][. The]_

algorithm of CDSK is described in Algorithm 1.

[P][n]

Furthermore, Section C in the appendix explains the theoretical properties of the coordinate descent
algorithm for problem (10).

The baseline named SC-NS performs spectral clustering on the nonparametric similarity proposed
in Yang et al. (2014a). The baseline named SC-MS first constructs a similarity matrix between data


-----

**Algorithm 1 Clustering by Discriminative Similarity via unsupervised Kernel classification (CDSK)**

Input: Unlabeled dataset {xl}l[n]=1[, parameter][ λ][, maximum iteration number][ M] [.]

**for t ←** 1 to M do

With fixed α, solve


min _s.t. Y[⊤]D[K]_ **Y = Ic,** (11)
**Y∈R[n][×][c][ Tr(][Y][⊤][L][K]** **[Y][)]**


With fixed Y, solve

min
**_α_** Λ, [Tr(][Y][⊤][L][K] **[Y][)][ −]**
_∈_


_αi + αj_


_Kτ_ (xi **xj) + λα[⊤]Kα**
_−_


_i,j=1_


_s.t. Y[⊤]D[K]_ **Y = Ic,** (12)

**end for**
Perform K-Means Clustering on rows of Y to obtain the clustering result.

denoted by W, where Wij = Kτ (xi − **xj), then optimize the kernel bandwidth h by minimizing**

**xi** _di_ **Wijxj** 2 where di = **Wij. SC-MS then performs spectral clustering on W with**
_i_ _∥_ _−_ [1] _j_ _∥_ _j_

the kernel bandwidthP P _h obtained from the optimization.[P]_

To demonstrate the advantage of the proposed parametric discriminative similarity, we compare
CDSK to various baseline clustering methods. SC stands for Spectral Clustering, which is the best
performer among spectral clustering with similarity matrix set by Gaussian kernel (SCK), spectral
clustering with similarity matrix set by a manifold-based similarity learning method (SC-MS) (Karasuyama & Mamitsuka, 2013), and spectral clustering with similarity matrix set by the nonparametric
discriminative similarity (SC-NS) in Yang et al. (2014a). In SC-MS, Gaussian kernel is used as data
similarity, and the parameters of the diagonal covariance matrix is optimized so as to minimize
the data reconstruction error term. SC-NS minimizes nonparametric kernel similarity between data
across different clusters, which is the same objective as that of kernel K-Means (Sch¨olkopf et al.,

1998), so its performance is the same as kernel K-Means. Please refer to Section 2 for discussion
about other baselines.

**Datasets. We conduct experiments on the Yale face dataset, UCI Ionosphere dataset, the MNIST**
handwritten digits dataset and the Georgia Face dataset. The Yale face dataset has face images of 15
people with 11 images for each person. The Ionosphere data contains 351 points of dimensionality
34. The Georgia Face dataset contains images of 50 people, and each person is represented by 15
color images with cluttered background. COIL-20 dataset has 1440 images of size 32 × 32 for
20 objects with background removed in all images. The COIL-100 dataset contains 100 objects
with 72 images of size 32 × 32 for each object. CMU PIE face data contains 11554 cropped face
images of size 32 × 32 for 68 persons, and there are around 170 facial images for each person under
different illumination and expressions. The UMIST face dataset is comprised of 575 images of size
112 × 92 for 20 people. CMU Multi-PIE (MPIE) data (Gross et al., 2010) contains 8916 facial
images captured in four sessions. The MNIST handwritten digits database has a total number of
70000 samples of dimensionality 1024 for digits from 0 to 9. The digits are normalized and centered
in a fixed-size image. The Extended Yale Face Database B (Yale-B) dataset contains face images
for 38 subjects with 64 frontal face images taken under different illuminations for each subject.
CIFAR-10 dataset consists of 50000 training images and 10000 testing images in 10 classes, and
each image is a color one of size 32 × 32, and we perform data clustering using all the training and
testing images. We also use the miniImageNet dataset used in Vinyals et al. (2016) to evaluate the
potential of clustering methods. MiniImageNet consists of 60, 000 color images of size 84 _×_ 84 with
100 classes, and each class has 600 images. MiniImageNet is known to be more complex than the
CIFAR-10 dataset, and we perform clustering on the 64 classes in miniImageNet which are used for
few-shot learning, so 38, 400 images are used for clustering. For every clustering method involving
randomness such as K-Means, we report the average performance of running it for 10 times.

**Performance Measures and Tuning λ by Cross Validation. We use Accuracy (AC) and Normal-**
ized Mutual Information (NMI) (Zheng et al., 2004) as the performance measures. The results of
different clustering methods are shown in Table 1 and Table 2 in the format of AC(NMI). Except


-----

for SC-MS, the kernel bandwidth in all methods is set as the variance of the pairwise Euclidean
distance between the data. λ is the weight for the regularization term in our derived generalization
bound. As explained in Section B.1 of the appendix, λ plays the same role as the weight in the regularization term of SVMs or Kernel Machines. Following the common practice in the literature of
SVM or Kernel Machines, λ can be tuned by Cross-Validation (CV). While this is an unsupervised
learning task and these is no labeled data for CV, we still developed a well-motivated CV procedure. Following the practice in Mairal et al. (2012), we randomly sampled 10% of the given data
as the validation data, then perform CDSK on the validation data. The best λ is chosen among the
discrete values between [0.05, 05] with a step of 0.05 which minimizes the average entropy of the
obtained embedding matrix Y R[n][×][c] by Algorithm 1, where the average entropy is compute as

_n_ _∈_

_n1_ entropy(softmax(Y[i])). This is because we would like to choose λ which renders the most

_i=1_

confident clustering embedding. We perform CDSK on all the datasets using this tuning strategy andP
observe improved performance as shown in the above two tables. For clustering methods involving
random operations, the average performance over 10 runs is reported.

**Computational Complexity.** Suppose the optimization of CDSK comprises M iterations of coordinate descent. The first subproblem (11) in Algorithm 1 takes O(n[2]c) steps using truncated
Singular Value Decomposition (SVD) by Krylov subspace iterative method. We adopt Sequential Minimal Optimization (SMO) (Platt, 1998) to solve the second subproblem (12), which takes
roughly O(n[2][.][1]) steps as reported in Platt (1998). Therefore, the overall time complexity of CDSK
is O(Mcn[2] + Mn[2][.][1]). M is set to 20 throughout all the experiments.

Table 1: Clustering results on Yale-B, Ionosphere, Georgia Face, COIL-20, COIL-100, CMU PIE
and UMIST Face.

|M` et` ho` ds````D `ata `se `t|Yale-B|Ionosphere|Georgia Face|COIL-20|COIL-100|CMU PIE|UMIST Face|
|---|---|---|---|---|---|---|---|
|K-Means|0.09(0.13)|0.71(0.13)|0.50(0.69)|0.65(0.76)|0.49(0.75)|0.08(0.19)|0.42(0.64)|
|SC|0.11(0.15)|0.74(0.22)|0.52(0.70)|0.43(0.62)|0.28(0.59)|0.07(0.18)|0.42(0.61)|
|ℓ1-Graph (Elhamifar & Vidal, 2013)|0.79(0.78)|0.51(0.12)|0.54(0.70)|0.79(0.91)|0.53(0.80)|0.23(0.34)|0.44(0.65)|
|SMCE (Elhamifar & Vidal, 2011)|0.34(0.39)|0.68(0.09)|0.60(0.74)|0.88(0.88)|0.56(0.81)|0.16(0.34)|0.45(0.66)|
|Lap-ℓ1-Graph (Yang et al., 2014b)|0.79(0.78)|0.50(0.09)|0.58(0.73)|0.79(0.91)|0.56(0.81)|0.30(0.51)|0.50(0.69)|
|RAG (Zhu et al., 2014)|0.13(0.19)|0.70(0.11)|0.17(0.38)|0.50(0.64)|0.58(0.81)|0.14(0.34)|0.26(0.28)|
|MMC (Xu et al., 2004)|0.71(0.69)|0.75(0.21)|0.42(0.58)|0.80(0.89)|0.61(0.63)|0.22(0.30)|0.51(0.56)|
|BMMC (Chen et al., 2014)|0.65(0.63)|0.70(0.15)|0.34(0.41)|0.82(0.93)|0.64(0.69)|0.18(0.23)|0.55(0.61)|
|RIM (Gomes et al., 2010)|0.62(0.74)|0.59(0.08)|0.39(0.56)|0.77(0.82)|0.71(0.79)|0.26(0.34)|0.40(0.53)|
|RatioRF (Bicego et al., 2021)|0.39(0.53)|0.62(0.05)|0.18(0.40)|0.65(0.75)|0.36(0.64)|0.15(0.36)|0.29(0.34)|
|CDSK (Ours)|0.83(0.86)|0.76(0.25)|0.60(0.74)|0.93(0.97)|0.78(0.92)|0.32(0.50)|0.67(0.80)|



Table 2: Clustering results on CMU Multi-PIE which contains the facial images captured in four
sessions (S1 to S4), MNIST, CIFAR-10, and Mini-ImageNet

|````|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|M` et` ho` ds````D `ata `se `t|MPIE S1|MPIE S2|MPIE S3|MPIE S4|MNIST|CIFAR-10|Mini-ImageNet|
|KM|0.12(0.50)|0.13(0.48)|0.13(0.48)|0.13(0.49)|0.52(0.47)|0.19(0.06)|0.27(0.33)|
|SC|0.13(0.53)|0.14(0.51)|0.14(0.52)|0.15(0.53)|0.38(0.36)|0.21(0.04)|0.29(0.35)|
|ℓ1-Graph (Elhamifar & Vidal, 2013)|0.59(0.77)|0.70(0.81)|0.63(0.79)|0.68(0.81)|0.57(0.61)|0.28(0.24)|0.28(0.37)|
|SMCE (Elhamifar & Vidal, 2011)|0.17(0.55)|0.19(0.53)|0.19(0.52)|0.18(0.53)|0.65(0.67)|0.31(0.30)|0.29(0.37)|
|Lap-ℓ1-Graph (Yang et al., 2014b)|0.59(0.77)|0.70(0.81)|0.63(0.79)|0.68(0.81)|0.56(0.60)|0.29(0.30)|0.29(0.37)|
|RAG (Zhu et al., 2014)|0.34(0.75)|0.30(0.69)|0.31(0.68)|0.29(0.70)|0.59(0.51)|0.22(0.10)|0.18(0.33)|
|MMC (Xu et al., 2004)|0.49(0.58)|0.51(0.60)|0.53(0.65)|0.50(0.61)|0.64(0.60)|0.31(0.28)|0.19(0.34)|
|BMMC (Chen et al., 2014)|0.40(0.51)|0.44(0.59)|0.45(0.61)|0.49(0.66)|0.66(0.69)|0.29(0.26)|0.16(0.32)|
|RIM (Gomes et al., 2010)|0.50(0.63)|0.52(0.68)|0.55(0.71)|0.51(0.67)|0.54(0.62)|0.20(0.25)|0.17(0.38)|
|RatioRF (Bicego et al., 2021)|0.54(0.85)|0.55(0.86)|0.64(0.86)|0.62(0.86)|0.48(0.39)|0.20(0.09)|0.26(0.38)|
|CDSK (Ours)|0.66(0.85)|0.72(0.88)|0.68(0.87)|0.73(0.89)|0.76(0.75)|0.46(0.39)|0.31(0.41)|



6 CONCLUSION
We propose a new clustering framework termed Clustering by Discriminative Similarity (CDS),
which searches for the optimal partition of data where the associated unsupervised classifier has
minimum generalization error bound. Under this framework, discriminative similarity is induced
by the generalization error bound for unsupervised similarity-based classifier, and CDS minimizes
discriminative similarity between different clusters. It is also proved that the discriminative similarity can be induced from kernel density classification. Based on CDS, we propose a new clustering
method named CDSK (CDS via unsupervised kernel classification), and demonstrate its effectiveness in data clustering.


-----

REFERENCES

Maria-Florina Balcan, Avrim Blum, and Nathan Srebro. A theory of learning with similarity functions. Mach. Learn., 72(1-2):89–112, 2008.

Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. J. Mach. Learn. Res., 3:463–482, March 2003.

Manuele Bicego, Ferdinando Cicalese, and Antonella Mensi. Ratiorf: a novel measure for random
forest clustering based on the tversky’s ratio model. IEEE Transactions on Knowledge and Data
_Engineering, pp. 1–1, 2021._

Changyou Chen, Jun Zhu, and Xinhua Zhang. Robust bayesian max-margin clustering. In Zoubin
Ghahramani, Max Welling, Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger (eds.),
_Advances in Neural Information Processing Systems 27: Annual Conference on Neural Informa-_
_tion Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pp. 532–540,_
2014.

Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. Multi-class classification with maximum margin multiple kernel. In Proceedings of the 30th International Conference on Machine
_Learning (ICML), pp. 46–54, Atlanta, GA, 2013._

Ehsan Elhamifar and Ren´e Vidal. Sparse manifold clustering and embedding. In Advances in Neural
_Information Processing Systems (NIPS), pp. 55–63, Granada, Spain, 2011._

Ehsan Elhamifar and Ren´e Vidal. Sparse subspace clustering: Algorithm, theory, and applications.
_IEEE Trans. Pattern Anal. Mach. Intell., 35(11):2765–2781, 2013._

Evarist Gine and Joel Zinn. Some limit theorems for empirical processes. Ann. Probab., 12(4):
929–989, 11 1984.

Mark Girolami and Chao He. Probability density estimation from optimally condensed data samples.
_IEEE Trans. Pattern Anal. Mach. Intell., 25(10):1253–1264, 2003._

Ryan Gomes, Andreas Krause, and Pietro Perona. Discriminative clustering by regularized information maximization. In Advances in Neural Information Processing Systems (NIPS), pp. 775–783,
Vancouver, Canada, 2010.

Ralph Gross, Iain A. Matthews, Jeffrey F. Cohn, Takeo Kanade, and Simon Baker. Multi-pie. Image
_Vis. Comput., 28(5):807–813, 2010._

J. A. Hartigan and M. A. Wong. A K-means clustering algorithm. Applied Statistics, 28:100–108,
1979.

Gao Huang, Tianchi Liu, Yan Yang, Zhiping Lin, Shiji Song, and Cheng Wu. Discriminative clustering via extreme learning machine. Neural Networks, 70:1–8, 2015.

Masayuki Karasuyama and Hiroshi Mamitsuka. Manifold-based similarity adaptation for label propagation. In Advances in Neural Information Processing Systems (NIPS), pp. 1547–1555, Lake
Tahoe, NV, 2013.

Zohar Shay Karnin, Edo Liberty, Shachar Lovett, Roy Schwartz, and Omri Weinstein. Unsupervised
SVMs: On the complexity of the furthest hyperplane problem. In Proceedings of the 25th Annual
_Conference on Learning Theory (COLT), pp. 2.1–2.17, Edinburgh, Scotland, 2012._

JooSeuk Kim and Clayton D. Scott. Performance analysis for l 2 kernel classification. In Advances
_in Neural Information Processing Systems (NIPS), pp. 833–840, Vancouver, Canada, 2008._

JooSeuk Kim and Clayton D. Scott. Robust kernel density estimation. J. Mach. Learn. Res., 13:
2529–2565, 2012.

V. Koltchinskii and D. Panchenko. Empirical margin distributions and bounding the generalization
error of combined classifiers. Ann. Statist., 30(1):1–50, 02 2002. doi: 10.1214/aos/1015362183.


-----

Vladimir Koltchinskii. Rademacher penalties and structural risk minimization. IEEE Trans. Inf.
_Theory, 47(5):1902–1914, 2001._

James R. Lee, Shayan Oveis Gharan, and Luca Trevisan. Multi-way spectral partitioning and higherorder cheeger inequalities. In Howard J. Karloff and Toniann Pitassi (eds.), Proceedings of the
_44th Symposium on Theory of Computing Conference, STOC 2012, New York, NY, USA, May 19_

_- 22, 2012, pp. 1117–1130. ACM, 2012._

Ga¨elle Loosli, St´ephane Canu, and Cheng Soon Ong. Learning SVM in kre˘ın spaces. IEEE Trans.
_Pattern Anal. Mach. Intell., 38(6):1204–1216, 2016._

Ravi Sastry Ganti Mahapatruni and Alexander G. Gray. CAKE: convex adaptive kernel density
estimation. In Proceedings of the Fourteenth International Conference on Artificial Intelligence
_and Statistics (AISTATS), pp. 498–506, Fort Lauderdale, FL, 2011._

Julien Mairal, Francis Bach, and Jean Ponce. Task-driven dictionary learning. IEEE Transactions
_on Pattern Analysis and Machine Intelligence, 34(4):791–804, 2012._

Xavier Mary. Hilbertian subspaces, subdualities and applications. Ph.D. Dissertation, Institut Na_tional des Sciences Appliquees Rouen, 2003._

Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. On spectral clustering: Analysis and an algorithm.
In Advances in Neural Information Processing Systems (NIPS), pp. 849–856, Vancouver, Canada],
2001.

Vu Nguyen, Dinh Q. Phung, Trung Le, and Hung Bui. Discriminative bayesian nonparametric
clustering. In Carles Sierra (ed.), Proceedings of the Twenty-Sixth International Joint Conference
_on Artificial Intelligence, IJCAI 2017, Melbourne, Australia, August 19-25, 2017, pp. 2550–2556._
ijcai.org, 2017.

Cheng Soon Ong, Xavier Mary, St´ephane Canu, and Alexander J. Smola. Learning with nonpositive kernels. In Proceedings of the Twenty-first International Conference on Machine Learn_ing (ICML), Banff, Alberta, Canada, 2004._

John Platt. Sequential minimal optimization: A fast algorithm for training support vector machines.
Technical report, 1998.

Bernhard Sch¨olkopf, Alexander Smola, and Klaus-Robert M¨uller. Nonlinear component analysis as
a kernel eigenvalue problem. Neural Comput., 10(5):1299–1319, July 1998.

Noam Shental, Assaf Zomet, Tomer Hertz, and Yair Weiss. Pairwise clustering and graphical models. In Advances in Neural Information Processing Systems (NIPS), pp. 185–192, Vancouver and
Whistler, Canada, 2003.

Mahdi Soltanolkotabi and Emmanuel J. Candes. A geometric analysis of subspace clustering with
outliers. Ann. Statist., 40(4):2195–2238, 08 2012.

Oriol Vinyals, Charles Blundell, Tim Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg,
Isabelle Guyon, and Roman Garnett (eds.), Advances in Neural Information Processing Systems
_29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016,_
_Barcelona, Spain, pp. 3630–3638, 2016._

Linli Xu, James Neufeld, Bryce Larson, and Dale Schuurmans. Maximum margin clustering. In
_Advances in Neural Information Processing Systems (NIPS), pp. 1537–1544, Vancouver, Canada],_
2004.

Shuicheng Yan and Huan Wang. Semi-supervised learning by sparse representation. In Proceedings
_of the SIAM International Conference on Data Mining (SDM), pp. 792–801, Sparks, NV, 2009._

Yingzhen Yang, Feng Liang, Shuicheng Yan, Zhangyang Wang, and Thomas S. Huang. On a theory
of nonparametric pairwise similarity for clustering: Connecting clustering to classification. In
_Advances in Neural Information Processing Systems (NIPS), pp. 145–153, Montreal, Canada,_
2014a.


-----

Yingzhen Yang, Zhangyang Wang, Jianchao Yang, Jiangping Wang, Shiyu Chang, and Thomas S.
Huang. Data clustering by laplacian regularized l1-graph. In Proceedings of the Twenty-Eighth
_AAAI Conference on Artificial Intelligence (AAAI), pp. 3148–3149, Qu´ebec City, Canada, 2014b._

Xin Zheng, Deng Cai, Xiaofei He, Wei-Ying Ma, and Xueyin Lin. Locality preserving clustering for
image database. In Proceedings of the 12th ACM International Conference on Multimedia (MM),
pp. 885–891, New York, NY, 2004.

Xiatian Zhu, Chen Change Loy, and Shaogang Gong. Constructing robust affinity graphs for spectral
clustering. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, pp. 1450–
1457, 2014.

A CONNECTION TO KERNEL DENSITY CLASSIFICATION

In this section, we show that the discriminative similarity (8) can also be induced from kernel density classification with varying weights on the data, and binary classification is considered in this
section. For any classification model M = ( _, f_ ) with hypothetical labeling and the labeled
_Y_ _S_ _Y_
data S = {xi, yi}i[n]=1[, suppose the joint distribution][ P][XY][ over][ X × {][1][,][ 2][}][ has probabilistic density]
function p(x, y). Let PX be the induced marginal distribution over the data with probabilistic density function p(x). Robust kernel density estimation methods (Girolami & He, 2003; Kim & Scott,
2008; Mahapatruni & Gray, 2011; Kim & Scott, 2012) suggest the following kernel density estimator where the kernel contributions of different data points are reflected by different nonnegative
weights that sum up to 1:


_αiKτ_ (x **xi), 1[⊤]α = 1, α** 0, (13)
_−_ _≥_
_i=1_

X


_p(x) = τ0_


where τ0 = (2π)[d/]1 [2]h[d][ . Based on (][13][), it is straightforward to obtain the following kernel density]

estimator of the density function p(x, y):


_αiKτ_ (x **xi).** (14)
_−_
_i:yi=y_

X


_p(x, y) = τ0_


Kernel density classifier is learnt from the labeled data S and constructed by kernel density estimators (14). Kernel density classifier resembles the Bayes classifier, and it classifies the test
data x based on the conditional label distribution P (Y |X = x), or equivalently, x is assigned
to class 1 if _p(x, 1) −_ _p(x, 2) ≥_ 0, otherwise it is assigned to class 2. Intuitively, it is preferred
that the decision function _r(x, α) =_ _p(x, 1) −_ _p(x, 2) is close to the true Bayes decision func-_
tion r = p(x, 1) _p(x, 2). Girolami & He (2003); Kim & Scott (2008) propose to use Integrated_
b _−_ b
Squared Error (ISE) as the metric to measure the distance between the kernel density estimators and
b b b
their true counterparts, and the oracle inequality is obtained that relates the performance of the L2
classifier in Kim & Scott (2008) to the best possible performance of kernel density classifier in the
same category. ISE is adopted in our analysis of kernel density classification, and the ISE between
the decision function _r and the true Bayes decision function r is defined as_
b ISE(r, r) = ∥r − _r∥L[2]_ 2 [=] _r −_ _r)[2]dx._ (15)

R[d][ (][b]

Z

The upper bound for the ISE ISE(r, _r) also induces discriminative similarity between the data from_

b b

different classes, which is presented in the following theorem.
**Theorem A.1. Let n1 =** 1Iyi=1 and b _n2 =_ 1Iyi=2. With probability at least 1 2n2 exp

_i=1_ _i=1_ _−_ _−_

2(n − 1)ε[2][] _−_ 2n exp _−[P]2nε[2][]_ over the labeled data[P] _S, the ISE between the decision function _
_r(x, α) and the true Bayes decision function r(x) satisfies_
 

1

ISE(r, r) ISE(r, r) + τ1K(α) + 2τ0 _,_ (16)

b _≤_ _[τ]n[0]_ _n_ 1 [+][ ε]

 _−_ 

where

b d b _n_

ISE(r, r) = 4 (αi + αj)Kτ (xi **xj)1Iyi=yj** (αi + αj)Kτ (xi **xj),** (17)

1≤Xi<j≤n _−_ _̸_ _−_ _i,jX=1_ _−_

d b


-----

_K(α) = α[⊤](K[√]2h[)][α][ −]_ [4]


_αiαjK[√]2h[(][x][i][ −]_ **[x][j][)1]Iyi=yj** _,_ (18)

_̸_

1≤Xi<j≤n


and K[√]2h [is the gram matrix evaluated on the data][ {][x][i][}]i[n]=1 [with the kernel][ K][√]2h[.]

Let λ1 > 0 be a weighting parameter, then the cost function ISE + λ1K(α), designed according to
the empirical term ISE(r, r) and the regularization term K(α) in the ISE error bound (16), can be
expressed as

[d]
b _n_

ISE + λ1K(α) _Sij[ise][1]Iyi=yj_ (αi + αj)Kτ (xi **xj) + λ1α[⊤]K[√]2h[α][,]**
_≤_ 1≤Xi<j≤n _̸_ _−_ _i,jX=1_ _−_
d

where the first term is comprised of sum of similarity between data from different classes with similarity Sij[ise] [= 4(][α][i][ +][ α][j][ −] _[λ][1][α][i][α][j][)][K][τ]_ [(][x][i][ −] **[x][j][)][, and][ S]ij[ise]** [is the discriminative similarity induced]
by the ISE bound for kernel density classification. Note that Sij[ise] [has the same form as the discrim-]
inative similarity Sij[K] [(][8][) induced by our CDS model, up to a scaling constant and the choice of the]
balancing parameter λ. The proof of Theorem A.1 is deferred to Section E.3.

B MORE EXPLANATION ABOUT THE THEORETICAL RESULTS IN
SECTION 4.1

B.1 THEORETICAL SIGNIFICANCE OF THE BOUND (4)

To the best of our knowledge, our generalization error bound (4) is the first principled result about
generalization error bound for general similarity-based classifier with strong connection to the established generalization error bound for Support Vector Machines (SVMs) or Kernel Machines.

We now explain this claim. When the similarity function S is a PSD kernel function, we have
_S[−]_ _≡_ 0, S = S[+] as explained in Remark 4.3. As a reminder, S is the general similarity function
used in the similarity-based classification, and S[+], S[−] are PSD kernel functions, and it is proved
that S can be decomposed by S = S[+] _−_ _S[−]_ under the mild conditions of Lemma 3.1. It follows that
we can set Ω[−](α) = 0 and B[−] = 0. Plugging B[−] = 0 in the derived generalization error bound
for the general similarity-based classification (4), we have

Prob [Y = fS(X)] _Rn(fS) + [8][R][(2][c][ −]_ [1)][cB][+] + 16c(2c − 1)B+R2 + 1 log [4]δ (19)
_̸_ _≤_ [b] _γ[√]n_ _γ_ 2n [.]

  [s]

_c_
According to its definition, Ω[+](α) = **_α[(][y][)][⊤]Sα[(][y][)]_** because S = S[+]. We define B := B[+].

_y=1_

_c_

P

Because Ω[+](α) ≤ _B[+2]_ as mentioned in Theorem 3.2, B satisfies _y=1_ **_α[(][y][)][⊤]Sα[(][y][)]_** _≤_ _B[2]. As a_

result, when S is a PSD kernel function, inequality (19) becomes P

Prob [Y = fS(X)] _Rn(fS) + [8][R][(2][c][ −]_ [1)][cB] + 16c(2c − 1)R2B + 1 log [4]δ (20)
_̸_ _≤_ [b] _γ[√]n_ _γ_ 2n [,]

  [s]

_c_
with _y=1_ **_α[(][y][)][⊤]Sα[(][y][)]_** _≤_ _B[2]._
P

Note that the bound (20) is in fact the generalization error bound for supervised learning when using
_S as the similarity function in the similarity-based classification. At the end of this subsection, we_


_c_

_y=1_ **_α[(][y][)][⊤]Sα[(][y][)]_** _≤_ _B[2]_ _⇒_ **_α[⊤]Sα ≤_** _cB[2], where c is the number of_

P


provide a lemma proving that

classes.


Now we compare the generalization error bound (20) to the established generalization error bound
for Kernel Machines in Bartlett & Mendelson (2003, Theorem 21) for the case that c = 2 with


-----

notations adapted to our analysis. The bound in Bartlett & Mendelson (2003, Theorem 21) is for
binary classification, which is presented as follows:

_√RB_ 8 log 4/δ
Prob [Y = fS(X)] _Rn(fS) + [4]_ _, α[⊤]Sα_ _B[2]._ (21)
_̸_ _≤_ [b] _γ[√]n_ [+] _γ_ [+ 1] 2n _≤_

  [r]

Comparing our generalization error bound (2) with c = 2 to the max-margin generalization error
bound (21), it can be easily seen that the two bounds are equivalent up to a constant scaling factor.
In fact, our bound (2) is more general which handles multi-class classification.


_c_
**Lemma B.1. When S is a PSD kernel, then** _y=1_ **_α[(][y][)][⊤]Sα[(][y][)]_** _≤_ _B[2]_ _⇒_ **_α[⊤]Sα ≤_** _cB[2]._
P


_Proof. Let H be the Reproducing Kernel Hilbert Space associated with the PSD kernel function S,_
and is also called the feature space associated with S. We use _,_ to denote the inner product
_H_ _⟨·_ _·⟩H_
in the feature space . Then we have S(xi, xj) = Sij = Φ(xi), Φ(xj) where Φ is the feature
_H_ _c_ _⟨_ _⟩H_

mapping associated with H. Because α = _y=1_ **_α[y], it can be verified that_**
P


**_α[⊤]Sα =_**


_y=1⟨ey, ey⟩H, where ey_

P


_αiαjSij_ = _αiΦ(xi),_
_j=1_ _⟨i=1_

P P


_αjΦ(xj)_ _c_
_j=1_ _⟩≤_

P


_i=1_

P


_c_
_i_ : yi=y **_α[y]i_** [Φ(][x][i][)][. It follows that][ α][⊤][S][α][ ≤] _[c]y=1_ **_α[(][y][)][⊤]Sα[(][y][)]_** _≤_ _cB[2]._

P P

B.2 TIGHTNESS OF THE BOUND

Note that the generalization error Pr [Y = f (X)] is bounded by _Rn(f_ ) =

_n_ _[h]S_ [(][x]i[,y]i[)][−] [P] _hS_ (xi,y) _̸_

_n1_ Φ _yγ≠_ _yi_ in Theorem 4.2. The underlying principle behind this bound andb

_i=1_

all such bounds in the statistical machine learning literature such asP   Bartlett & Mendelson (2003) is
the following property about empirical process (adjusted using our notations):

R(H) ≤ Ex,y suph∈H _|Ex,yRn(f_ ) − _Rn(f_ )| ≤ 2R(H), (22)

where Ex,y indicates expectation with respect to random couple[b] (x, y) _PXY, and PXY is a joint_
_∼_
distribution in a discriminative model M = ( _, f_ ). (22) is introduced in the classical properties
_Y_ _S_
of empirical process in Gine & Zinn (1984). By Lemma E.2 of this paper, with probability at least
i.i.d.
1 _δ over the data_ **xi** _i=1_ _PXY,_
_−_ _{_ _}[n]_ _∼_


ln2n[2]δ [=][ O][(][ B]√n ) (23)


R(H) ≤ [(2][c]√[ −]n[1)][c] _B +_


2Bc(2c − 1)


for some constant B. It follows from (22), (23), and concentration inequality (such as McDiarmid’s
inequality) that for each sufficiently large n, with large probability, suph Ex,yRn(f ) _Rn(f_ )
_∈H |_ _−_ _|_
is less than ( _√[B]n_ ). Therefore, we can bound the expectation of the empirical loss, i.e., Ex,yRn(f ),
_O_
tightly using the empirical loss Rn(f ) uniformly over the function space . [b]
_H_

[b]

C THEORETICAL PROPERTIES OF THE COORDINATE DESCENT ALGORITHM
IN SECTION 5

In this subsection, we give a detailed explanation about the theoretical properties of the coordinate
descent algorithm presented in Section 5. We first explain how the objective function of CDSK (10)
is connected to the objective function (9) developed in our theoretical analysis. It should be emphasized that (9) cannot be directly used for data clustering since it cannot avoids the trivial solution
where all the data are in a single cluster. We adopt the broadly used formulation of normalized


-----

_c_
cut and use _k=1_ cut(vol(AAk,kA[¯])k) to replace _i<j_ _Sij[K][1]Iyi≠_ _yj in (9), leading to the following optimization_

problem: P [P]

_c_ _n_

cut(Ak, **A[¯]** _k)_ _αi + αj_

min _Kτ_ (xi **xj) + λα[⊤]Kα ≜** _Q[¯](α,_ ), (9’)
**_α∈Λ,Y={yi}i[n]=1_** _k=1_ vol(Ak) _−_ _i,j=1_ 2 _−_ _Y_

X X

where {A}k[c] =1 [are][ c][ data clusters according to the cluster labels][ {][y][i][}][,][ ¯]Ak is the complement of Ak,
cut(A, B) = _Sij[K][,][ vol(][A][) =]_ _Sij[K][. We have the following theorem, which]_

**xi∈A,xj** _∈B_ **xi∈A,1≤j≤n**

can be derived based on TheoremP 4.1 in the work of multi-way Cheeger inequalities (P Lee et al.,
2012).

_c_ _c_

**Theorem C.1. minα∈Λ,Y** _k=1_ cut(vol(AAk,kA[¯])k) ≲ _t=1_ _σt(L[nor]), where L[nor]_ is the normalized graph

Laplacian L[nor] = (D[K])[−][1]P[/][2]L[K](D[K])[−][1][/][2], aP ≲ _b indicates a < Cb for some constant C and_
_σt(_ ) indicates the t-th smallest singular value of a matrix.

_·_

Based on Theorem C.1, we resort to solve the following more tractable problem, that is,


_n_

_αi + αj_

2

_i,j=1_

X


_Kτ_ (xi − **xj) + λα[⊤]Kα ≜** _Q(α),_ (9”)


min _σt(L[nor])_
**_α_** Λ _−_
_∈_ _t=1_

X


because Q (9”) is an upper bound for _Q[¯] (9’) up to a constant scaling factor. It can be verified that_
problem (10) is equivalent to (9”), and (9”) is the underlying optimization problem for data clustering
in Section 5. The following proposition shows that the iterative coordinate descent algorithm in
Section 5 reduces the value of Q at each iteration.

**Proposition C.2. The coordinate descent algorithm for problem (10) reduces the value of the ob-**
jective function Q(α) at each iteration.

_n_
_Proof. Let Q[′](α, Y) ≜_ Tr(Y[⊤]L[K]Y) _αi+2_ _αj_ _Kτ_ (xi **xj) + λα[⊤]Kα, and we use su-**
_−_ _i,j=1_ _−_

perscript to denote the iteration number of coordinate descent. At iterationP _m, after solving the_
subproblems (11) and (12), we have Q[′](α[(][m][)], Y[(][m][)]) = Q(α[(][m][)]). At iteration m + 1, by solving
the subproblems (4) and (3) in order again, we have Q[′](α[(][m][+1)], Y[(][m][+1)]) = Q(α[(][m][+1)]). Because of the nature of coordinate descent, Q[′](α[(][m][+1)], Y[(][m][+1)]) ≤ _Q[′](α[(][m][)], Y[(][m][)]), it follows that_
_Q(α[(][m][+1)]) ≤_ _Q(α[(][m][)])._

Based on Proposition C.2, the iterations of coordinate descent are similar to that of EM algorithms
and they reduce the value of Q, where Y plays the role of latent variable for EM algorithms.

D MORE DETAILS ABOUT OPTIMIZATION OF CDSK

The optimization of CDSK comprises M iterations of coordinate descent, wherein each iteration
solves the following two subproblems.

1) With constant α,

min _s.t. Y[⊤]D[K]Y = Ic,_ (24)
**Y** R[n][×][c][ Tr(][Y][⊤][L][K][Y][)]
_∈_

2) With constant Y,


_αi + αj_


min
**_α_** Λ, [Tr(][Y][⊤][L][K][Y][)][ −]
_∈_ _i,j=1_

X


_Kτ_ (xi **xj) + λα[⊤]Kα**
_−_


_s.t. Y[⊤]D[K]Y = Ic,_ (25)


-----

The first subproblem (24) takes O(n[2]c) steps using truncated Singular Value Decomposition (SVD)
by Krylov subspace iterative method. We adopt Sequential Minimal Optimization (SMO) (Platt,
1998) to solve the second subproblem (25), which takes roughly O(n[2][.][1]) steps as reported in Platt
(1998). SMO is an iterative algorithm where each iteration of SMO solves the quadratic programming (25) with respect to only two elements of the weights α, so that each iteration of SMO can be
performed efficiently. Therefore, the overall time complexity of CDSK is O(Mcn[2] + Mn[2][.][1]).

E PROOFS

E.1 PROOF OF LEMMA 4.1

Before stating the proof of Lemma 4.1, we introduce the famous spectral theorem in operator theory
below.

**Theorem E.1. (Spectral Theorem) Let L be a compact linear operator on a Hilbert space H. Then**
there exists in H an orthonormal basis {ϕ1, ϕ2, . . .} consisting of eigenvectors of L. If λk is the
eigenvalue corresponding to ϕk, then the set _λk_ is either finite or λk 0 when k . In
addition, the eigenvalues are real if L is self-adjoint. { _}_ _→_ _→∞_

Recall that the integral operator by S is defined as

(LSf )(x) = _S(x, t)f_ (t)dt,
Z

and we are ready to prove Lemma 4.1.

**Proof of Lemma 4.1. It can be verified that LS is a compact operator. Therefore, according to**
Theorem E.1, {ϕk} is an orthogonal basis of L[2]. Note that ϕk is the eigenfunction of LS with
eigenvalue λk if LSϕk = λkϕk.

With fixed x ∈X, we then have

_m+ℓ_

_λkϕk(x)ϕk(t)_

_k=m_

X

_[m][+][ℓ]_ _[m][+][ℓ]_

_λk_ _ϕk(x)_ 2 _λk_ _ϕk(t)_ 2

_≤_ _|_ _||_ _|[2][][ 1] ·_ _|_ _||_ _|[2][][ 1]_

_k=m_ _k=m_

  X   X

_[m][+][ℓ]_

2

_√C_ _λk_ _ϕk(x)_ _._
_≤_ _|_ _||_ _|[2][][ 1]_

_k=m_

  X

It follows that the series _λkϕk(x)ϕk(t) converges to a continuous function ex uniformly on t._

_k≥1_

This is because ϕk = _[L]λ[S]k[ϕ][P][k]_ is continuous for nonzero λk.

On the other hand, for fixed x ∈X, as a function in L[2],


_S(x, ·) =_


_⟨S(x, ·), ϕk⟩ϕk =_
_kX≥1_


_λkϕk(x)ϕk(_ ).

_·_
_kX≥1_


Therefore, for fixed x, S(x, ) = _λkϕk(x)ϕk(_ ) = ex( ) almost surely w.r.t the Lebesgue
_∈X_ _·_ _k≥1_ _·_ _·_

measure. Since both are continuous functions, we must have S(x, t) = _λkϕk(x)ϕk(t) for any_

[P] _k≥1_

**t** . It follows that S(x, t) = _λkϕk(x)ϕk(t) for any x, t_ .
_∈X_ _k≥1_ _∈X_ [P]

[P]

We now consider two series which correspond to the positive eigenvalues and negative eigenvalues
of LS, namely _λkϕk(x)ϕk(_ ) and _λk_ _ϕk(x)ϕk(_ ). Using similar argument, for fixed

_k : λk≥0_ _·_ _k:λk<0_ _|_ _|_ _·_

P P


-----

**x, both series converge to a continuous function, and we let**


_S[+](x, t) =_

_S[−](x, t) =_


_λkϕk(x)ϕk(t),_
_k : λXk≥0_

_λk_ _ϕk(x)ϕk(t)._
_|_ _|_
_k:λk<0_

X


_S[+](x, t) and S[−](x, t) are continuous function in x and t. All the eigenvalues of S[+]_ and S[−] are
nonnegative, and it can be verified that both are PSD kernels since


_cicjS[+](xi, xj) =_
_i,j=1_

X


_cicj_
_i,j=1_

X


_λkϕk(xi)ϕk(xj)_
_k : λXk≥0_


_λk_
_k : λXk≥0_


_cicjϕk(xi)ϕk(xj)_
_i,j=1_

X


_λk(_ _ciϕ(xi))[2]_ 0.

_≥_

_k : λXk≥0_ Xi=1


Similarly argument applies to S[−]. Therefore, S is decomposed as S(x, t) = S[+](x, t) − _S[−](x, t)._

E.2 PROOF OF THEOREM 4.2

Lemma E.3 will be used in the Proof of Theorem 4.2. The following lemma is introduced for the
proof of Lemma E.3, whose proof appears in the end of this subsection.
**Lemma E.2. The Rademacher complexity of the class HS satisfies**


R( _S,y)._ (26)
_H_
_y=1_

X


R( _S)_ (2c 1)
_H_ _≤_ _−_


_c_ _c_
**Lemma E.3. Define Ω[+](α) =** **_α[(][y][)][⊤]S[+]α[(][y][)]_** and Ω[−](α) = **_α[(][y][)][⊤]S[−]α[(][y][)]._** When

_y=1_ _y=1_

_{Ωsupx[+]i(}xαi[n]∈X=1) ≤ |[, the Rademacher complexity of the class]S[−]B(x[+2], x,Ω)| ≤[−](α)R ≤[2]_ for someB[−][2] for positive constantP R > 0, then with probability at least[ H][S][ satisfies] B[+] and B[−], supP **x∈X 1 | −S[+]δ( over the datax, x)| ≤** _R[2],_


R( _S)_ + 2c(2c 1)(B[+] + B[−])R[2]
_H_ _≤_ _[R][(2][c][ −]_ [1)]√[c][(]n[B][+][ +][ B][−][)] _−_


ln [2]δ (27)

2n [.]


**Proof of Lemma E.3 . According to Lemma 4.1, S is decomposed into two PSD kernels as S =**
_S[+]_ _−S[−]. Therefore, the are two Reproducing Kernel Hilbert Spaces HS[+]_ [and][ H]S[−] [that are associated]
withwith S S[+][+](andx, t S) =[−] ⟨respectively, and the canonical feature mappings inϕ[+](x), ϕ[+](t)⟩HK+ [and][ S][−][(][x][,][ t][) =][ ⟨][ϕ][−][(][x][)][, ϕ][−][(][t][)][⟩] H[H]K[−]S[+][. In the following text, we][and][ H]S[−] [are][ ϕ][+][ and][ ϕ][−][,]

will omit the subscripts HK[+] [and][ H]K[−] [without confusion.]


For any 1 ≤ _y ≤_ _c,_

_hS(x, y) =_


_αiS(x, xi) =_ **w[+], ϕ[+](x)** **w[−], ϕ[−](x)**
_⟨_ _⟩−⟨_ _⟩_
_i_ : yi=y

X


with ∥w[+]∥[2] = α[(][y][)][⊤]S[+]α[(][y][)] _≤_ _B[+2]_ and ∥w[−]∥[2] = α[(][y][)][⊤]S[−]α[(][y][)] _≤_ _B[−][2]. Therefore,_

_HS,y ⊆_ _H[˜]S,y = {(x, y) →⟨w[+], ϕ[+](x)⟩−⟨w[−], ϕ[−](x)⟩,_

_∥w[+]∥[2]_ _≤_ _B[+2], ∥w[−]∥[2]_ _≤_ _B[−][2]}, 1 ≤_ _y ≤_ _c,_


-----

and R( _S,y)_ R( [˜]S,y). Since we are deriving upper bound for R( _S,y), we slightly abuse the_
_H_ _⊆_ _H_ _H_
notation and let HS,y represent _H[˜]S,y in the remaining part of this proof._

For x, t ∈ R[d] and any hS ∈HS,y, we have

_hS(x)_ _hS(t)_ = **w[+], ϕ[+](x)** **w[−], ϕ[−](x)** **w[+], ϕ[+](t)** + **w[−], ϕ[−](t)**
_|_ _−_ _|_ _|⟨_ _⟩−⟨_ _⟩−⟨_ _⟩_ _⟨_ _⟩|_

= |⟨w[+], ϕ[+](x) − _ϕ[+](t)⟩_ + ⟨w[−], ϕ[−](t) − _ϕ[−](x)⟩|_

_≤_ _B[+]∥ϕ[+](x) −_ _ϕ[+](t)∥_ + B[−]∥ϕ[−](x) − _ϕ[−](t)∥)_

_≤_ (B[+] + B[−]) _S[+](x, x) + S[+](t, t) + 2_ _S[+](x, x)S[+](t, t)_
q

2R[2](B[+] + B[−]). p
_≤_


We now approximate the Rademacher complexity of the function class HS,y with its empirical
version R(HS,y) using the samplen _{xi}. For eachc 1 ≤_ _y ≤_ _c, Define Ec_ _{[(][y]x[)]i}_ [=][ b]R(HS,y) =

E{σi} sup[b] _hS_ (·,y)∈HS,y _n[1]_ _i=1_ _σihS(xi, y)_, then _y=1_ R(HS,y) = E{xi} _y=1_ _E{[(][y]x[)]i}_, and

  h i

P P P

**x1,...,supxn,x[′]t** _Ex[(][y]1[)],...,xt−1,xt,xt+1,...,xn_ _[−]_ _[E]x[(][y]1[)],...,xt−1,x[′]t[,][x][t][+1][,...,][x][n]_

_n_ _′_

_t[, y][)]_

= sup sup [1] _σihS(xi, y)_ sup [1] _σihS(xi, y) +_ _[h][S][(][x]_
**x1,...,xn,x[′]t**  _hS_ (·,y)∈HS,y _n_ _i=1_ _−_ _hS_ (·,y)∈HS,y _n_ _i=t_ _n_

X X̸

_n_ _′_

sup E[E][{]σ[σ]i _[i][}]_ sup [1] _σihS(xi, y)_ sup [1] _σihS(xi, y) +_ _[h][S][(][x]t[, y][)]_
_≤_ **x1,...,xn,x[′]t** _{_ _}"_ _hS_ (·,y)∈HS,y _n_ _i=1_ _−_ _hS_ (·,y)∈HS,y _n_ _i=t_ _n_

X X̸

_n_ _′_

_t[, y][)]_

sup E _σi_ sup [1] _σihS(xi, y)_ [1] _σihS(xi, y) +_ _[h][S][(][x]_
_≤_ **x1,...,xn,x[′]t** _{_ _}"_ _hS_ (·,y)∈HS,y _n_ _i=1_ _−_ _n_ _i=t_ _n_ #

X X̸

_n_ _′_

1 1 _t[, y][)]_

sup E _σi_ sup _σihS(xi, y)_ _σihS(xi, y) +_ _[h][S][(][x]_
_≤_ **x1,...,xn,x[′]t** _{_ _}"_ _hS_ (·,y)∈HS,y _n_ Xi=1 _−_  _n_ Xi≠ _t_ _n_  #


_hS(xt, y)_


_′_
_t[, y][)]_
_−_ _[h][S][(][x]n_


= sup E _σi_ sup
**xt,x[′]t** _{_ _}"_ _hS_ (·,y)∈HS,y

_._

_≤_ [2][R][2][(][B][+]n[ +][ B][−][)]


_n_

_≤_ [2][R][2][(][B][+][+][B][−][)][c]


It follows that _Ex[(][y]1[)],...,xt−1,xt,xt+1,...,xn_

_y=1_ _[−]_

cording to the McDiarmid’s Inequality,P


_E[(][y][)]_
_y=1_ **x1,...,xt−1,x[′]t[,][x][t][+1][,...,][x][n]**

P


. Ac

_c_

_nε[2]_
R( _S,y)_ _ε_ 2 exp _._ (28)
_H_ _≥_ _≤_ _−_ 2(B[+] + B[−])[2]R[4]c[2]
_y=1_

X i   


Pr


R( _S,y)_
_H_ _−_
_y=1_

X

b


-----

Now we derive the upper bound for the empirical Rademacher complexity:


(29)

ln [2]δ

2n [.]

(30)


E _σi_
_{_ _}_
_y=1_

X


R( _S,y) =_
_H_
_y=1_

X

b


sup
_hS_ _∈HS,y_


_σihS(xi)_
_j=1_

X


_i=1_


_≤_ _n[1]_

_≤_ _n[1]_


E _σi_
_{_ _}_
_y=1_

X

_c_

E _σi_
_{_ _}_
_y=1_

X


_σi_ **w[+], ϕ[+](xi)** **w[−], ϕ[−](xi)**
_⟨_ _⟩−⟨_ _⟩_
_i=1_

X  


sup
_∥w[+]∥≤B[+],∥w[−]∥≤B[−]_


_σiϕ[+](xi)_ + B[−]
_∥_ _∥_
_i=1_

X


_σiϕ[−](xi)_
_∥_
_i=1_

X


_B[+]∥_


= _[B][+][c]_

_n_ [E][{][σ][i][}]


+ _[B][−][c]_

_n_ [E][{][σ][i][}]


_σiϕ[+](xi)_
_∥_
_i=1_

X


_σiϕ[−](xi)_
_∥_
_i=1_

X


_≤_ _[B]n[+][c]_

_≤_ _[B]n[+][c]_


_n_

+ _[B][−][c]_ E _σi_ _σiϕ[−](xi)_

_n_ vu _{_ _}_ "∥ _i=1_ _∥[2]_

u X

_n_ t

_s[−](xi, xi)_ (B[+] + B[−]).

X _≤_ _√[Rc]n_


E _σi_ _σiϕ[+](xi)_
vu _{_ _}_ "∥ _i=1_ _∥[2]_
u X
t _n_

_s[+](xi, xi) +_ _[B][−][c]_

v v
u u
uX u


By Lemma E.2, (28) and (29), with probability at least 1 − _δ, we have_


_c_

R( _S,y)_ + 2c(2c 1)(B[+] + B[−])R[2]
_H_ _≤_ _[R][(2][c][ −]_ [1)]√[c][(]n[B][+][ +][ B][−][)] _−_
_y=1_

X


R( _S)_ (2c 1)
_H_ _≤_ _−_


**Proof of Theorem 4.2 . According to Theorem 2 in Koltchinskii & Panchenko (2002), with proba-**
bility 1 − _δ over the labeled data S with respect to any distribution in P, the generalization error of_
the kernel classifier fS satisfies

ln 2/δ

_R(fS)_ _Rn(fS) + [8]_ _,_ (31)
_≤_ [b] _γ_ [R][(][H][S][) +] 2n

r

_n_

where _Rn(fS) =_ _n[1]_ Φ( _mhS (γxi,yi)_ ) is empirical error of the classifier for γ > 0. Due to the facts

_i=1_

that mh[b]S (x, y) = hSP(xi, yi) − argmaxy′≠ _yi_ _hS(xi, y[′]), α is a positive vector and S is nonnegative,_
we have mhS (x, y) _hS(xi, yi)_ _hS(xi, y). Note that Φ(_ ) is a non-increasing function, it
_≥_ _−_ _y[P]≠_ _yi_ _·_

follows that


_hS(xi, y)_

Φ( _[m][h][S]_ [(][x][i][, y][i][)] ) Φ _y≠_ _yi_

_γ_ _≤_ _γ_
 _[h][S][(][x][i][, y][i][)][ −]_ [P]


(32)


Applying Lemma E.3, (4) holds with probability 1 _δ. When γ_ 1, it can be verified that

_c_ _n_ _−_ _≥_

_hS(xi, yi) −_ _y[P]≠_ _yi_ _hS(xi, y)_ _≤_ _y=1_ _hS(xi, y) ≤_ _i=1_ _αi = 1 ≤_ _γ for all (xi, yi), so that_

P P

_hS(xi, y)_ _h(xi, yi)_ _h(xi, y)_

Φ _y≠_ _yi_ 1 _−_ _y[P]≠_ _yi_ _._ (33)

_γ_ _≤_ _−_ _γ_

 _[h][S][(][x][i][, y][i][)][ −]_ [P] 


-----

Note that when hS(xi, yi) _hS(xi, y)_ 0, then _γ[1]_ _hS(xi, yi)_ _hS(xi, y)_ [0, 1] so
_−_ _y[P]=yi_ _≥_ " _−_ _y[P]=yi_ # _∈_

_̸_ _̸_

that Φ _[h]S_ [(][x]i[,y]i[)][−]yγ[P]≠ _yi_ _hS_ (xi,y) = 1 _h(xi,yi)−yγ[P]≠_ _yi_ _h(xi,y)_ . If hS(xi, yi) _hS(xi, y) < 0, we_

have Φ _[h]S_ [(][x]i[,y]i[)][−]yγ[P]≠ _yi_ _hS_ (xi,y) 1 − 1 _h(xi,yi)−yγ[P]≠_ _yi_ _h(xi,y)_ . Therefore, ( − _y[P]≠_ 33yi) always holds.

_≤_ _≤_ _−_

 

By the definition of _Rn(fS), (32), and (33), (5) is obtained._

[b]

**Remark E.4. It can be verified that the image of the similarity function S in Lemma 4.1 and The-**
orem 4.2 can be generalized from [0, 1] to [0, a] for any a ∈ R, a > 0 with the condition γ ≥ 1
replaced by γ _a. This is because LS is a compact operator for continuous similarity function_
_≥_ _c_

_S :_ [0, a], and _hS(xi, yi)_ _hS(xi, y)_ _hS(xi, y)_ _a. Furthermore, given a_
_X × X →_ _−_ _y[P]≠_ _yi_ _≤_ _y=1_ _≤_

symmetric and continuous function S : X × X → [c, d], c, dP ∈ R, c < d, we can obtain a symmetric
and continuous function S[′] : X ×X → [0, 1] by setting S[′] = _[S]b−[−]a[a]_ [, and then apply all the theoretical]

results of this paper to CDS with S[′] being the similarity function for the similarity-based classifier.

**Proof of Lemma E.2. Inspired by Koltchinskii & Panchenko (2002), we first prove that the**
Rademacher complexity of the function class formed by the maximum of several hypotheses is
bounded by two times the sum of the Rademacher complexity of the function classes that these
hypothesis belong to. That is,


R( _S,y),_ (34)
_H_
_y=1_

X


R( max) 2
_H_ _≤_


where Hmax = {max{h1, . . ., hk} : hy ∈HS,y, 1 ≤ _y ≤_ _k} for 1 ≤_ _k ≤_ _c −_ 1.

If no confusion arises, the notations ( _σi_ _,_ **xi, yi** ) are omitted in the subscript of the expectation
_{_ _}_ _{_ _}_
operator in the following text, i.e., E _σi_ _,_ **xi,yi** is abbreviated to E. According to Theorem 11
_{_ _}_ _{_ _}_
of Koltchinskii & Panchenko (2002), it can be verified that

_n_ +[]

1

E _σi_ _,_ **xi,yi** sup _σihS(xi)_
_{_ _}_ _{_ _}_  _h∈Hmax_ _n_ _i=1_ !

X

_k_  _n_  +[]

1

E _σi_ _,_ **xi,yi** sup _σihS(xi)_ _._

_≤_ _y=1_ _{_ _}_ _{_ _}_  _h∈HS,y_ _n_ _i=1_ !

X X

 

Therefore,

_n_

1

R( max) = E _σi_ _,_ **xi,yi** sup _σihS(xi)_
_H_ _{_ _}_ _{_ _}_ "h∈Hmax _n_ _i=1_ #

X

_n_

1 +

E _σi_ _,_ **xi,yi** sup _σihS(xi)_
_≤_ _{_ _}_ _{_ _}_ " _h∈Hmax_ _n_ _i=1_ #

  Xn 

+

+E _σi_ _,_ **xi,yi** sup _σihS(xi)_
_{_ _}_ _{_ _}_ " _h∈Hmax_ _−_ _n[1]_ _i=1_ #

  _nX_ 

1 +

= 2E _σi_ _,_ **xi,yi** sup _σihS(xi)_
_{_ _}_ _{_ _}_ " _h∈Hmax_ _n_ _i=1_ #

_k_   X _n_ 

1 +

2 E _σi_ _,_ **xi,yi** sup _σihS(xi)_
_≤_ _y=1_ _{_ _}_ _{_ _}_ " _h∈HS,y_ _n_ _i=1_ #

X   X 


-----

2 E _σi_ _,_ **xi,yi**
_≤_ _{_ _}_ _{_ _}_

_y=1_

X


R( _S,y)._ (35)
_H_
_y=1_

X


sup
_h∈HS,y_


_σihS(xi)_
_i=1_

X


= 2


The equality in the third line of (35) is due to the fact that −σi has the same distribution as σi. Using
this fact again, (34), we have


R( _S) = E_ _σi_ _,_ **xi,yi**
_H_ _{_ _}_ _{_ _}_

= E _σi_ _,_ **xi,yi**
_{_ _}_ _{_ _}_


sup
_mhS ∈HS_

sup
_mhS ∈HS_


_σimhS_ (xi, yi)
_i=1_

X


_σi_ _mhS_ (xi, y)1Iy=yi
_i=1_ _y=1_

X X


E _σi_ _,_ **xi,yi**
_{_ _}_ _{_ _}_
_y=1_

X


sup
_mhS ∈HS_


_σimhS_ (xi, y)1Iy=yi
_i=1_ #

X

_n_

_σimhS_ (xi, y)(21Iy=yi 1)

_S_ _i=1_ _−_

X

_n_

_σimhS_ (xi, y)
_i=1_ #

X


_≤_ 2[1]n


E _σi_ _,_ **xi,yi** sup

2[1]n _y=1_ _{_ _}_ _{_ _}_ "mhS _S_

X _∈H_

_c_

+ [1] E _σi_ _,_ **xi** sup

2n _y=1_ _{_ _}_ _{_ _}_ "mhS _S_

X _∈H_


= [1]


E _σi_ _,_ **xi**
_{_ _}_ _{_ _}_
_y=1_

X


(36)

_n_

_σihS(xi, y[′])_
_i=1_

X

(37)


sup
_mhS ∈HS_


_σimhS_ (xi, y)
_i=1_

X


Also, for any given 1 ≤ _y ≤_ _c,_


1
_n_ [E][{][σ][i][}][,][{][x][i][}]


sup
_mhS ∈HS_


_σimhS_ (xi, y)
_i=1_

X


= [1]

_n_ [E][{][σ][i][}][,][{][x][i][}]

_≤_ _n[1]_ [E][{][σ][i][}][,][{][x][i][}]

+ [1]

_n_ [E][{][σ][i][}][,][{][x][i][}]


_σihS(xi, y) −_ _σiargmaxy′≠_ _yhS(xi, y[′])_
_i=1_

X


sup
_hS_ (·,y)∈HS,y,y=1...c


sup
_hS_ (·,y)∈HS,y


_σihS(xi, y)_
_i=1_

X


_σiargmaxy′≠_ _yhS(xi, y[′])_
_i=1_

X


sup
_hS_ (·,y[′])∈HS,y[′] _[,y][′][̸][=][y]_


_≤_ _n[1]_ [E][{][σ][i][}][,][{][x][i][}]


+ [2] E _σi_ _,_ **xi**

_n_ _{_ _}_ _{_ _}_

_yX[′]≠_ _y_


sup
_hS_ (·,y)∈HS,y


_σihS(xi, y)_
_i=1_

X


sup
_hS_ ( _,y[′])_ _S,y_

_·_ _∈H[′]_


Combining (36) and (37),


1
_n_ [E][{][σ][i][}][,][{][x][i][}]


R( _S)_
_H_ _≤_


sup
_hS_ (·,y)∈HS,y


_σihS(xi, y)_
_i=1_

X


_y=1_

_c_ _c_

_y=1_ _y=1_

X X


E _σi_ _,_ **xi**
_{_ _}_ _{_ _}_
_yX[′]≠_ _y_


_σihS(xi, y[′])_
_i=1_

X

_n_

_σihS(xi, y)_
_i=1_

X


sup
_hS_ ( _,y[′])_ _S,y_

_·_ _∈H[′]_


E _σi_ _,_ **xi**
_{_ _}_ _{_ _}_
_y=1_

X


= (2c − 1)

= (2c − 1)


sup
_hS_ (·,y)∈HS,y


R( _S,y)._ (38)
_H_
_y=1_

X


-----

E.3 PROOF OF THEOREM A.1

_Proof. According to definition of ISE,_

ISE(r, r) = _r −_ _r)[2]dx =_ _r(x, α)[2]dx −_ 2 _r(x, α)r(x)dx +_ (39)

R[d][ (][b] R[d][ b] R[d][ b] R[d][ r][(][x][)][2][dx.]

Z Z Z Z

For a given distribution,b

R[d][ r][(][x][)][2][dx][ is a constant. By Gaussian convolution theorem,]

R


_r(x, α)[2]dx = τ1_ **_α[(][y][)][⊤](K[√]2h[)][α][(][y][)][ −]_** _[τ][1]_

ZR[d][ b] _y=1_

X

where τ1 = (2π)[d/][2]1(√2h)[d][ . Moreover,]


2αiαjK[√]2h[(][x][i][ −] **[x][j][)1]Iyi=yj** _,_ (40)

_̸_

1≤Xi<j≤n


_r(x, α)r(x)dx_
R[d][ b]

Z

= _p(x, 1)p(x, 1)dx +_

R[d][ b]

Z

Note that


_p(x, 2)p(x, 2)dx −_
R[d][ b]


_p(x, 1)p(x, 2)dx −_
R[d][ b]


_p(x, 2)p(x, 1)dx._
R[d][ b]

(41)


_p(x, 1)p(x, 1)dx =_

_τ0_ ZR[d][ b] _j : yj_ =1 ZR[d][ α][j][K][τ] [(][x][ −] **[x][j][)][p][(][x][,][ 1)][dx,]**

X

we then use the empirical term _i :P i≠_ _j_ _αj_ _Kτn (xi1−xj_ )1Iyi =1 to approximate the integral R[d][ α][j][K][τ] [(][x][ −]

**xj)p(x, 1)dx. Since E{xi,yi}i≠** _j_ " _i :P i≠_ _j_ _αj_ _K−τn (−xi1−xj_ )1Iyi =1 # = R[d][ α][j][K][τ] [(][x][ −] **[x][j][)]R[p][(][x][,][ 1)][dx][, and]**

R


_i :P i≠_ _j_ _αj_ _Kτn (−xi1−xj_ )1Iyi =1, therefore,


bounded difference holds for


_αjKτ_ (xi **xj)1Iyi=1**
_i_ : i=j _−_

Pr _̸_ 2 exp 2(n 1)ε[2][]

 P _n_ 1 _−_  _≤_ _−_ _−_

_−_ ZR[d][ α][j][K][τ] [(][x][ −] **[x][j][)][p][(][x][,][ 1)][dx]** _[≥]_ _[α][j][ε]_

   
 

It follows that with probability at least 1 − 2n1 exp _−_ 2(n − 1)ε[2][], where ni is the number of data
points with label i,
 

_αjKτ_ (xi **xj)**
_i,j : i≠_ _j,yPi=yj_ =1 _−_ _p(x, 1)p(x, 1)dx_ _αjε._ (42)

_n −_ 1 _−_ _τ[1]0_ ZR[d][ b] _[≤]_ _j : yj_ =1

X

Similarly, with probability at least 1 − 2n2 exp _−_ 2(n − 1)ε[2][],

_αjKτ_ (xi **xj)**  
_i,j : i≠_ _j,yPi=yj_ =2 _−_ _p(x, 2)p(x, 2)dx_ _αjε._ (43)

_n −_ 1 _−_ _τ[1]0_ ZR[d][ b] _[≤]_ _j : yj_ =2

X

It follows from (42) and (43) that with probability at least 1 − 2n exp _−_ 2(n − 1)ε[2][],

_αjKτ_ (xi **xj)**  
_i,j : i≠_ _j,yi=yj_ _−_ _p(x, 1)p(x, 1) +_ _p(x, 2)p(x, 2)_ _dx_ (44)

P

_n −_ 1 _−_ _τ[1]0_ ZR[d] _[≤]_ _[ε.]_

  

In the same way, with probability at least 1 2nb exp 2nε[2][], b
_−_ _−_

_αjKτ_ (xi **xj)**  
_i,j : yi≠_ _yj_ _−_ _p(x, 1)p(x, 2) +_ _p(x, 2)p(x, 1)_ _dx_ (45)

P

_n_ _−_ _τ[1]0_ ZR[d] _[≤]_ _[ε.]_

  
b b


-----

Based on (44) and (45), with probability at least 1 − 2n2 exp _−_ 2(n − 1)ε[2][] _−_ 2n exp _−_ 2nε[2][]

_αjKτ_ (xi **xj)**  αjKτ (xi **xj)**  

ISE(r, r) 2τ0 _i,j : yPi≠_ _yj_ _−_ 2τ0 _i,j : i≠_ Pj,yi=yj _−_
_≤_ _n_ _−_ _n_ 1

2 _−_

b + τ1 **_α[(][y][)][⊤](K[√]2h[)][α][(][y][)][ −]_** _[τ][1]_ 2αiαjK[√]2h[(][x][i][ −] **[x][j][)1]Iyi=yj + 2τ0ε**

_̸_

Xy=1 1≤Xi<j≤n


(αi + αj)Kτ (xi **xj)1Iyi=yj**
1 _i<j_ _n_ _−_ _̸_
_≤_ _≤_

P


(αi + αj)Kτ (xi **xj)**
_i,j=1_ _−_

P


4τ0
_≤_


_τ0_
_−_


**_α[(][y][)][⊤](K[√]2h[)][α][(][y][)][ −]_** _[τ][1]_
_y=1_

X


2αiαjK[√]2h[(][x][i][ −] **[x][j][)1]Iyi=yj + 2τ0(**

_̸_

1≤Xi<j≤n


+ τ1


_n_ 1 [+][ ε][)][.]
_−_

(46)


The conclusion of this theorem can be obtained from (46).


-----

