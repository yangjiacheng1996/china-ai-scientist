# A SCALING LAW FOR SYN2REAL TRANSFER: HOW MUCH IS YOUR PRE-TRAINING EFFECTIVE?

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Synthetic-to-real transfer learning is a framework in which a synthetically generated
dataset is used to pre-train a model to improve its performance on real vision tasks.
The most significant advantage of using synthetic images is that the ground-truth
labels are automatically available, enabling unlimited expansion of the data size
without human cost. However, synthetic data may have a huge domain gap, in
which case increasing the data size does not improve the performance. How can
we know that? In this study, we derive a simple scaling law that predicts the
performance from the amount of pre-training data. By estimating the parameters
of the law, we can judge whether we should increase the data or change the
setting of image synthesis. Further, we analyze the theory of transfer learning by
considering learning dynamics and confirm that the derived generalization bound is
consistent with our empirical findings. We empirically validated our scaling law on
various experimental settings of benchmark tasks, model sizes, and complexities of
synthetic images.

1 INTRODUCTION

The success of deep learning relies on the availability of large data. If the target task provides limited
data, the framework of transfer learning is preferably employed. A typical scenario of transfer
learning is to pre-train a model for a similar or even different task and fine-tune the model for the
target task. However, the limitation of labeled data has been the main bottleneck of supervised
pre-training. While there have been significant advances in the representation capability of the models
and computational capabilities of the hardware, the size and the diversity of the baseline dataset have
not been growing as fast (Sun et al., 2017). This is partially because of the sheer physical difficulty of
collecting large datasets from real environments (e.g., the cost of human annotation).

In computer vision, synthetic-to-real (syn2real) transfer is a promising strategy that has been attracting
attention (Su et al., 2015; Movshovitz-Attias et al., 2016; Georgakis et al., 2017; Tremblay et al.,
2018; Hinterstoisser et al., 2019; Borrego et al., 2018; Chen et al., 2021). In syn2real, images used for
pre-training are synthesized to improve the performance on real vision tasks. By combining various
conditions, such as 3D models, textures, light conditions, and camera poses, we can synthesize an
infinite number of images with ground-truth annotations. Syn2real transfer has already been applied
in some real-world applications. Teed & Deng (2021) proposed a simultaneous localization and
mapping (SLAM) system that was trained only with synthetic data and demonstrated state-of-the-art
performance. The object detection networks for autonomous driving developed by Tesla was trained
with 370 million images generated by simulation (Karpathy, 2021).

The performance of syn2real transfer depends on the similarity between synthetic and real data. In
general, the more similar they are, the stronger the effect of pre-training will be. On the contrary,
if there is a significant gap, increasing the number of synthetic data may be completely useless, in
which case we waste time and computational resources. A distinctive feature of syn2real is that we
can control the process of generating data by ourselves. If a considerable gap exists, we can try
to regenerate the data with a different setting. But how do we know that? More specifically, in a
standard learning setting without transfer, a “power law”-like relationship called a scaling law often
holds between data size and generalization errors (Rosenfeld et al., 2019; Kaplan et al., 2020). Is
there such a rule for pre-training?


-----

pretrain task

objdet semseg mulclass normal


0.98
0.97
0.96
0.95
0.94

0.96

0.95

0.94

0.93

0.90
0.88
0.86
0.84
0.82

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
||||||||||
||||||||||
||||||||||
||||||||||
|G|||||||||
|||G|||||||
||||G|G|||||
||||||G||G||
|||||||||G|

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
||||||||||
||||||||||
|G|||||||||
|||G|||||||
||||G|G|G||||
||||||||G|G|
||||||||||
||||||||||
||||||||||

|G|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
||||||||||
||||||||||
||||||||||
|||G|||||||
||||||||||
||||||||||
||||G|G|||||
||||||G||G||
|||||||||G|

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
||||||||||
||||||||||
||||||||||
||||||||||
|G||G|G||||||
|||||G|G||G||
|||||||||G|
||||||||||
||||||||||

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
||||||||||
||||||||||
||||||||||
|G|||||||||
|||G|G||||||
|||||G|G||G|G|

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
||||||||||
||||||||||
|G|||||||||
|||G|G|G|||||
||||||G||G|G|
||||||||||

|G|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
||||G|G|G||G|G|
|||G|||||||
||||||||||
||||||||||
||||||||||
||||||||||

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
||||||||||
|||G|G|G|G||G||
|G|||||||||
|||||||||G|
||||||||||
||||||||||

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
||||||||||
||||||||||
||||||||||
||||||||||
||||||||||
|G|||||||||
|||G|G||||||
|||||G|G||G|G|

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
||||||||||
||||||||||
||||||||||
|G|||||||||
|||G|G|G|||||
||||||G||G|G|
||||||||||
||||||||||

|G|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
|||G|||||||
||||G||||||
|||||G|||||
||||||||||
||||||G||G|G|
||||||||||
||||||||||
||||||||||

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
||||||||||
||||||||||
||||||||||
||||||||||
||||||||||
|G|||||||||
|||G|G|G|G||||
||||||||G|G|


10[3] 10[3.5] 10[4] 10[4.5] 10[3] 10[3.5] 10[4] 10[4.5] 10[3] 10[3.5] 10[4] 10[4.5] 10[3] 10[3.5] 10[4] 10[4.5]

# of pretrain images

Figure 1: Empirical results of syn2real transfer for different tasks. We conducted four pre-training
tasks: object detection (objdet), semantic segmentation (semseg), multi-label classification
(mulclass), surface normal estimation (normal), and three fine-tuning tasks for benchmark
datasets: object detection for MS-COCO, semantic segmentation for ADE20K, and single-label
classification (sinclass) for ImageNet. The y-axis indicates the test error for each fine-tuning task.
Dots indicate empirical results and dashed lines indicate the fitted curves of scaling law (1). For more
details, see Section 4.2.


In this study, we find that the generalization error on fine-tuning is explained by a simple scaling law,

test error ≃ _Dn[−][α]_ + C, (1)


where coefficient D > 0 and exponent α > 0 describe the convergence speed of pre-training, and
constant C ≥ 0 determines the lower limit of the error. We refer to α as pre-training rate and C
as transfer gap. We can predict how large the pre-training data should be to achieve the desired
accuracy by estimating the parameters α, C from the empirical results. Additionally, we analyze the
dynamics of transfer learning using the recent theoretical results based on the neural tangent kernel
(Nitanda & Suzuki, 2021) and confirm that the above law agrees with the theoretical analysis. We
empirically validated our scaling law on various experimental settings of benchmark tasks, model
sizes, and complexities of synthetic images.

Our contributions are summarized as follows.



-  From empirical results and theoretical analysis, we elicit a law that describes how generalization
scales in terms of data sizes on pre-training and fine-tuning.

-  We confirm that the derived law explains the empirical results for various settings in terms of
pre-training/fine-tuning tasks, model size, and data complexity (e.g., Figure 1). Furthermore, we
demonstrate that we can use the estimated parameters in our scaling law to assess how much
improvement we can expect from the pre-training procedure based on synthetic data.

-  We theoretically derive a generalization bound for a general transfer learning setting and confirm
its agreement with our empirical findings.

2 RELATED WORK


**Supervised pre-training for visual tasks** Many empirical studies show that the performance at
a fine-tuning task scales with pre-training data (and model) size. For example, Huh et al. (2016)
studied the scaling behavior on ImageNet pre-trained models. Beyond ImageNet, Sun et al. (2017)
studied the effect of pre-training with pseudo-labeled large-scale data and found a logarithmic scaling
behavior. Similar results were observed by Kolesnikov et al. (2019).


-----

**Syn2real transfer** The utility of synthetic images as supervised data for computer vision tasks
has been continuously studied by many researchers (Su et al., 2015; Movshovitz-Attias et al., 2016;
Georgakis et al., 2017; Tremblay et al., 2018; Hinterstoisser et al., 2019; Borrego et al., 2018; Chen
et al., 2021; Newell & Deng, 2020; Devaranjan et al., 2020; Mousavi et al., 2020; Hodaˇn et al., 2019).
These studies found positive evidence that using synthetic images is helpful to the fine-tuning task. In
addition, they demonstrated how data complexity, induced by e.g., light randomization, affects the
final performance. For example, Newell & Deng (2020) investigated how the recent self-supervised
methods perform well as a pre-training task to improve the performance of downstream tasks. In this
paper, following this line of research, we quantify the effects under the lens of the scaling law (1).

**Neural scaling laws** The scaling behavior of generalization error, including some theoretical works
(e.g., Amari et al., 1992), has been studied extensively. For modern neural networks, Hestness et al.
(2017) empirically observed the power-law behavior of generalization for language, image, and
speech domains with respect to the training size. Rosenfeld et al. (2019) constructed a predictive form
for the power-law in terms of data and model sizes. Kaplan et al. (2020) pushed forward this direction
in the language domain, describing that the generalization of transformers obeys the power law in
terms of a compute budget in addition to data and model sizes. Since then, similar scaling laws have
been discovered in other data domains (Henighan et al., 2020). Several authors have also attempted
theoretical analysis. Hutter (2021) analyzed a simple class of models that exhibits a power-law n[−][β]
in terms of data size n with arbitrary β > 0. Bahri et al. (2021) addressed power laws under four
regimes for model and data size. Note that these theoretical studies, unlike ours, are concerned with
scaling laws in a non-transfer setting.

Hernandez et al. (2021) studied the scaling laws for general transfer learning, which is the most
relevant to this study. A key difference is that they focused on fine-tuning data size as a scaling
factor, while we focus on pre-training data size. Further, they found scaling laws in terms of the
transferred effective data, which is converted data amount necessary to achieve the same performance
gain by pre-training. In contrast, Eq. (1) explains the test error with respect to the pre-training data
size directly at a fine-tuning task. Other differences include task domains (language vs. vision) and
architectures (transformer vs. CNN).

**Theory of transfer learning** Theoretical analysis of transfer learning has been dated back to
decades ago (Baxter, 2000) and has been pursued extensively. Among others, some recent studies
(Maurer et al., 2016; Du et al., 2020; Tripuraneni et al., 2020) derived an error bound of a fine-tuning
task in the multi-task scenario based on complexity analysis; the bound takes an additive form
_O(An[−][1][/][2]_ + Bs[−][1][/][2]), where n and s are the data size of pre-training and fine-tuning, respectively,
with coefficients A and B. Neural network regression has been also discussed with this bound
(Tripuraneni et al., 2020). In the field of domain adaptation, error bounds have been derived in
relation to the mismatch between source and target input distributions (Ganin et al., 2016; Acuna
et al., 2021). They also proposed algorithms to adopt a new data domain. However, unlike in this
study, no specific learning dynamics has been taken into account. In the area of hypothesis transfer
learning (Fei-Fei et al., 2006; Yang et al., 2007), among many theoretical works, Du et al. (2017) has
derived a risk bound for kernel ridge regression with transfer realized as the weights on the training
samples. The obtained bound takes a similar form to our scaling law. However, the learning dynamics
of neural networks initialized with a pre-trained model has never been explored in this context.

3 SCALING LAWS FOR PRE-TRAINING AND FINE-TUNING

The main obstacle in analyzing the test error is that we have to consider interplay between the effects
of pre-training and fine-tuning. Let L(n, s) ≥ 0 be the test error of a fine-tuning task with pre-training
data size n and fine-tuning data size s. As the simplest case, consider a fine-tuning task without
pre-training (n = 0), which boils the transfer learning down to a standard learning setting. In this
case, the prior studies of both classical learning theory and neural scaling laws tell us that the test
error decreases polynomially[1] with the fine-tuning data size s, that is, L(0, s) = Bs[−][β] + E with
decay rate β > 0 and irreducible loss E ≥ 0. The irreducible loss E is the inevitable error given by

1For classification with strong low-noise condition, it is known that the decay rate can be exponential (Nitanda
& Suzuki, 2019). However, we focus only on the polynomial decay without such strong condition in this paper.


-----

the best possible mapping; it is caused by noise in continuous outputs or labels. Hereafter we assume
_E = 0 for brevity._

3.1 INDUCTION OF SCALING LAW WITH SMALL EMPIRICAL RESULTS


To speculate a scaling law, we conducted (a) (b)
preliminary experiments.[2] We pre-trained 0.90
ResNet-50 by a synthetic classification task 0.85

0.9

and fine-tuned by ImageNet. Figure 2 (a)
presents the log-log plot of error curves test error 0.80 0.8
with respect to pre-training data size n,
where each shape and color indicates a dif- 0.75 0.7
ferent fine-tuning size s. It shows that the 10[3] 10[3.5] 10[4] 10[4.5] 10 4.1 10 4.4 10 4.7 10[5] 10[5.5] 10[6]

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|
|---|---|---|---|---|---|---|---|---|---|---|---|---|
||||||||||||||
||||||||||||||
||||||||||||||
|||||||||105 105.5 1|||||
||||||||||||||

pre-training effect diminishes for large n. # of pretrain images # of finetune images
In contrast, Figure 2 (b) presents the rela
Figure 2: Scaling curves with different (a) pre-training

tions between the error and the fine-tuning

size and (b) fine-tuning size.

size s with different n. It indicates the error
drops straight down regardless of n, confirming the power-law scaling with respect to s. The above
observations and the fact that L(0, s) decays polynomially are summarized as follows.
**Requirement 1. lims** _L(n, s) = 0._
_→∞_
**Requirement 2. limn** _L(n, s) = const._
_→∞_
**Requirement 3. L(0, s) = Bs[−][β].**

Requirements 1 and 3 suggest the dependency of n is embedded in the coefficient B = g(n), i.e., the
pre-training and fine-tuning effects interact multiplicatively. To satisfy Requirement 2, a reasonable
choice for the pre-training effect is g(n) = n[−][α] + γ; the error decays polynomially with respect to n
but has a plateau at γ. By combining these, we obtain

_L(n, s) = δ(γ + n[−][α])s[−][β],_ (2)


where α, β > 0 are decay rates for pre-training and fine-tuning, respectively, γ ≥ 0 is a constant, and
_δ > 0 is a coefficient. The exponent β determines the convergence rate with respect to fine-tuning_
data size. From this viewpoint, δ(γ + n[−][α]) is the coefficient factor to the power law. The influence
of the pre-training appears in this coefficient, where the constant term δγ comes from the irreducible
loss of the pre-training task and n[−][α] expresses the effect of pre-training data size. The theoretical
consideration in Section E.5 suggests that the rates α and β can depend on both the target functions
of pre-training and fine-tuning as well as the learning rate.

3.2 THEORETICAL DEDUCTION OF SCALING LAW


Next, we analyze the fine-tuning error from a purely theoretical point of view. To incorporate the
effect of pre-training that is given as an initialization, we need to analyze the test error during the
training with a given learning algorithm such as SGD. We apply the recent development by Nitanda
& Suzuki (2021) to transfer learning. The study successfully analyzes the generalization of neural
networks in the dynamics of learning, showing it achieves minmax optimum rate. The analysis uses
the framework of the reproducing kernel Hilbert space given by the neural tangent kernel (Jacot et al.,
2018).

For theoretical analysis of transfer, it is important to formulate a task similarity between pre-training
and fine-tuning. If the tasks were totally irrelevant (e.g., learning MNIST to forecast tomorrow’s
weather), pre-training would have no benefit. Following Nitanda & Suzuki (2021), for simplicity of
analysis, we discuss only a regression problem with square loss. We assume that a vector input x and
scalar output y follow y = φ0(x) for pre-training and y = φ0(x) + φ1(x) for fine-tuning, where we
omit the output noise for brevity; the task types are identical sharing the same input-output form, and
task similarity is controlled by φ1.

We analyze the situation where the effect of pre-training remains in the fine-tuning even for large
data size (s →∞). More specifically, the theoretical analysis assumes a regularization term as the

2The results are replicated from Appendix C.2; see the subsection for more details.


-----

_ℓ2-distance between the weights and the initial values, and a smaller learning rate than constant in_
the fine-tuning. Hence we control how the pre-training effect is preserved through the regularization
and learning rate. Other assumptions made for theoretical analysis concern the model and learning
algorithm; a two-layer neural network having M hidden units with continuous nonlinear activation[3]
is adopted; for optimization, the averaged SGD (Polyak & Juditsky, 1992), an online algorithm, is
used for a technical reason.

The following is an informal statement of the theoretical result. See Appendix E for details. We
emphasize that our result holds not only for syn2real transfer but also for transfer learning in general.

**Theorem 1 (Informal). Let** _fˆn,s(x) be a model of width M pre-trained by n samples_
(x1, y1), . . ., (xn, yn) and fine-tuned by s samples (x[′]1[, y]1[′] [)][, . . .,][ (][x]s[′] _[, y]s[′]_ [)][ where inputs][ x, x][′][ ∼] _[p][(][x][)]_
_are i.i.d. with the input distribution p(x) and y = φ0(x) and y[′]_ = ϕ(x[′]) = φ0(x[′]) + φ1(x[′]). Then
_the generalization error of the squared loss L(n, s) =_ _f[ˆ]n,s(x)_ _ϕ(x)_ _is bounded from above with_
_|_ _−_ _|[2]_
_high probability as_
_ExL(n, s) ≤_ _A1(cM + A0n[−][α])s[−][β]_ + εM _._ (3)
_εM and cM can be arbitrary small for large M_ _; A0 and A1 are constants; the exponents α and β_
_depend on φ0, φ1, p(x), and the learning rate of fine-tuning._

The above bound (3) shows the correspondence with the empirical derivation of the full scaling
law (2). Note that the approximation error εM is omitted in (2).

We note that the derived bound takes a multiplicative form in terms of the pre-training and fine-tuning
effects, which contrasts with the additive bounds such as An[−][1][/][2] + _Bs[−][1][/][2]_ (Tripuraneni et al., 2020).
The existing studies consider the situation where a part of a network (e.g., backbone) is frozen during
fine-tuning. Therefore, the error of pre-training is completely preserved after fine-tuning, and both
errors appear in an additive way. This means that the effect of pre-training is irreducible by the effect
of fine-tuning, and vice versa. In contrast, our analysis deals with the case of re-optimizing the entire
network in fine-tuning. In that case, the pre-trained model is used as initial values. As a result, even
if the error in pre-training is large, the final error can be reduced to zero by increasing the amount of
fine-tuning data.

3.3 INSIGHTS AND PRACTICAL VALUES

The form of the full scaling law (2) suggests that there are two scenarios depending on whether
fine-tuning data is big or small. In “big fine-tune” regime, pre-training contributes relatively little.
By taking logarithm, we can separate the full scaling law (2) into the pre-training part u(n) =
log(n[−][α] + γ) and the fine-tuning part v(s) = −β log s. Consider to increase n by squaring it.
Since the pre-training part cannot be reduced below log(γ) as u(n) > u(n[2]) > log(γ), the relative
improvement (u(n[2]) − _u(n))/v(s) becomes infinitesimal for large s. Figure 2 (b) confirms this_
situation. Indeed, prior studies provide the same conclusion that the gain from pre-training can easily
vanish (He et al., 2018; Newell & Deng, 2020) or a target task accuracy even degrade (Zoph et al.,
2020) if we have large enough fine-tuning data.

The above observation, however, does not mean pretraining is futile. Dense prediction tasks such as depth
estimation require pixel-level annotations, which critically
limits the number of labeled data. Pre-training is indispens- Target Error
able in such “small fine-tune” regime. Based on this, we Log Pre-training Size
hereafter analyze the case where the fine-tuning size s is (a) (b)
fixed. By eliminating s-dependent terms in (2), we obtain
a simplified law (1) by setting D = δs[−][β] and C = δγs[−][β].
After several evaluations, these parameters including α
can be estimated by the nonlinear least squares method
(see also Section 4.1). (c) (d)

Log Error

Target Error

Log Pre-training Size

(a) (b)

(c) (d)

As a practical benefit, the estimated parameters of the Figure 3: Pre-training scenarios.
simplified law (1) bring a way to assess syn2real transfer.
Suppose we want to solve a classification task that requires at least 90% accuracy with limited labels.

3ReLU is not included in this class, but we can generalize this condition; see (Nitanda & Suzuki, 2021).


-----

We generate some number of synthetic images and pre-train with them, and we obtain 70% accuracy
as Figure 3 (a). How can we achieve the required accuracy? It depends on the parameters of the
scaling law. The best scenario is (b) — transfer gap C is low and pre-training rate α is high. In
this case, increasing synthetic images eventually leads the required accuracy. In contrast, when
transfer gap C is larger than the required accuracy (c), increasing synthetic images does not help to
solve the problem. Similarly, for low pre-training rate α (d), we may have to generate tremendous
amount of synthetic images that are computationally infeasible. In the last two cases, we have to
change the rendering settings such as 3D models and light conditions to improve C and/or α, rather
than increasing the data size. The estimation of α and C requires to compute multiple fine-tuning
processes. However, the estimated parameters tell us whether we should increase data or change the
data generation process, which can reduce the total number of trials and errors.

4 EXPERIMENTS

4.1 SETTINGS

For experiments, we employed the following transfer learning protocol. First, we pre-train a model
that consists of backbone and head networks from random initialization until convergence, and we
select the best model in terms of the validation error of the pre-training task. Then, we extract the
backbone and add a new head to fine-tune all the model parameters. For notations, the task names
of object detection, semantic segmentation, multi-label classification, single-label classification,
and surface normal estimation are abbreviated as objdet, semseg, mulclass, sinclass,
and normal, respectively. The settings for transfer learning are denoted by arrows. For example,
objdet→semseg indicates that a model is pre-trained by object detection, and fine-tuned by
semantic segmentation. The experiments were conducted on an in-house cluster containing NVIDIA
V100 GPUs. The total amount of computation was approximately 1700 GPU days (200 for image
rendering, 1300 for pre-training, and 200 for fine-tuning). All the results including Figure 1 are
shown as log-log plots.

**Pre-training:** We prepared four tasks: mulclass, objdet, semseg, and normal. We used
ResNet-based models, where backbones were ResNet-50, unless otherwise specified, and the head
networks were customized for each task. Synthetic images for pre-training were generated by
BlenderProc (Denninger et al., 2019), an image renderer that can handle several domain randomization
methods. For rendering, we used the setting of the BOP challenge 2020 (Hodaˇn et al., 2020) as our
default setting. We used 172 3D models, where ten objects appeared on average for each image.
We applied texture randomization for walls and a floor, randomization for area and point lights,
and randomization for the camera. In most cases, the models were pre-trained with 64,000 images.
We trained all models for the same fixed number of iterations depending on pre-training tasks and
selected the best models for fine-tuning, which were validated by another 1000 synthetic images
generated in the same way.

**Fine-tuning:** We evaluated sinclass by ImageNet (Russakovsky et al., 2015), objdet by
MS-COCO (Lin et al., 2014), and semseg by ADE20K (Zhou et al., 2016). The number of images
used was 1% of each data set (roughly, 12,000 for ImageNet, 1000 for COCO, and 200 for ADE20K).
We fine-tuned the pre-trained models with these subsets of data for a fixed number of iterations and
reported the error metrics for validation sets at the last iteration. The metrics were top-1 accuracy
for classification, mean mAP for MS-COCO, and mean IoU for ADE20K. These metrics take their
values from 0 to 1, and we converted them into errors such as 1 - accuracy.[4]

**Curve fitting:** After obtaining the empirical errors _L[ˆ], we estimated the parameters of (1) by non-_
linear least squares in the log-log space. We solved the minimization problem of _i_ _L(ni, s)_
log(Dn[−]i _[α]_ + C) with a fixed fine-tuning data size s and pre-training data sizes n[|][ log ˆ]i = 2[i] 1000 −
_|[2]_ _×_
for data point index i = 0, . . ., 6. In the experiments, we empirically encountered some instability

[P]
between D and α. We fixed D = 0.48 by the median values of D’s for all the settings and estimated
_α and C independently for each case. We explain this procedure with more details in Appendix D._

4Although the cross-entropy loss is commonly used, several studies (Sharma & Kaplan, 2020; Bahri et al.,
2021) show that the scaling laws also hold for 1 - accuracy.


-----

4.2 SCALING LAW UNIVERSALLY EXPLAINS DOWNSTREAM PERFORMANCE FOR VARIOUS
TASK COMBINATIONS

Figure 1 shows the test errors of each fine-tuning task and fitted learning curves with Eq. (1), which
describes the effect of pre-training data size n for all combinations of pre-training and fine-tuning
tasks. The scaling law fits with the empirical fine-tuning test errors with high accuracy in most cases.


4.3 BIGGER MODELS REDUCE THE TRANSFER GAP

model G r18 r34 r50 r101 r152


mulclass → sinclass objdet → objdet

0.935


mulclass → sinclass objdet → objdet

G

G 0.96


0.84

0.82


0.90

0.88

0.86

0.84

0.82


0.95

0.94


0.930


0.80


0.925

|G|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|||||||||
|||||||||
|||||||||


G


20 30 50 20 30 50

# of parameters (Million)


0.93

|Col1|Col2|G|Col4|G|G|G G|
|---|---|---|---|---|---|---|
||||||||

|Col1|G G|Col3|G G|G|
|---|---|---|---|---|
||||||


10[3] 10[3.5] 10[4] 10[4.5] 10[3] 10[3.5] 10[4] 10[4.5]

# of pretrain images


Figure 4: Effect of model size. Best viewed in color. **Left:** The scaling curves for
mulclass→sinclass and objdet→objdet cases. The meanings of dots and lines are the
same as those in Figure 1. Right: The estimated transfer gap C (y-axis) versus the model size (x-axis)
in log-log scale. The dots are estimated values, and the lines are linear fittings of them.

We compared several ResNet models as backbones in mulclass→sinclass and
objdet→objdet to observe the effects of model size. Figure 4 (left) shows the curves of
scaling laws for the pre-training data size n for different sizes of backbone ResNet-x, where
_x ∈{18, 34, 50, 101, 152}. The bigger models attain smaller test errors. Figure 4 (right) shows_
the values of the estimated transfer gap C. The results suggest that there is a roughly power-law
relationship between the transfer gap and model size. This agrees with the scaling law with respect to
the model size shown by Hernandez et al. (2021).


4.4 SCALING LAW CAN EXTRAPOLATE FOR MORE PRE-TRAINING IMAGES


objdet → objdet

|GG|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|
|---|---|---|---|---|---|---|---|---|---|---|
||GG||||||||||
||||GG||||||||
|||||GG|GG||GG||||
|||||||GG|||GG|GG|
||||||||||||
||||||||||||
||||||||||||
||||||||||||
||||||||||||
||||||||||||


GG


10[3.5] 10[4] 10[4.5] 10[5] 10[5.5] 10[6]

# of pretrain images


fitting

powerlaw

proposed


0.96

0.95

0.94

0.93

0.92

0.91


model powerlaw proposed

r50 0.0099 0.0024
r101 0.0074 0.0016
r152 0.0091 0.0017


model

G r50


r101

r152


Figure 5: Ability to extrapolate. Left: The solid lines represent the fitted power law and the dashed
curves represent the fitted scaling law (1), in which the laws were fitted using the empirical errors
where the pre-training size n was less than 64,000 (the first five dots). The vertical dashed line
indicates where n = 64,000. Right: The root-mean-square errors between the laws and the actual
test errors in the area of extrapolation (the last five dots).


-----

We also evaluated the extrapolation ability of the scaling law. We increased the number of synthetic
images from the original size (n = 64,000) to 1.28 million, and see how the fitted scaling law predicts
the unseen test errors where n > 64,000. As a baseline, we compared the power-law model, which
is equivalent to the derived scaling law (1) with C = 0. Figure 5 (left) shows the extrapolation
results in objdet→objdet setting, which indicates the scaling law follows the saturating trend
in regions with large pre-training sizes for all models, while the power-law model fails to capture it.
The prediction errors is numerically shown in Figure 5 (right), which again shows our scaling law
achieves better prediction performance.

4.5 DATA COMPLEXITY AFFECTS BOTH PRE-TRAINING RATE AND TRANSFER GAP


5000

4500

4000

3500

0.8

0.7

0.6

0.5


appearance

G multiple


objdet → objdet

|G|Col2|G|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
|G G||G|G|G|G||G|G|
|||G|G G|G G|G G||G|G|
|||||||G|GG||
|||||||||G|
||||||||||



1000 3000 10000 30000

# of pretrain images


single

fix

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|
|---|---|---|---|---|---|---|---|---|---|
|||||||||||
|||||||||||
||||G||G|||||


G


light


0.96

0.95


random

|Col1|Col2|Col3|G|Col5|Col6|Col7|Col8|Col9|Col10|
|---|---|---|---|---|---|---|---|---|---|


G

G


background

G fix


0.96

0.95

0.94


random


0.94


G

G

G


obj texture

appear

light

background


w/

single

fix
fix


w/o

single

fix
fix


w/

multiple

fix
fix


w/o

multiple

fix
fix


w/

multiple
random

fix


w/o

multiple
random

fix


w/

multiple
random
random


w/o

multiple
random
random


object

G w/ texture

w/o texture


Figure 6: Effect of synthetic image complexity. Best viewed in color. Left: Scaling curves of
different data complexities. Right: Estimated parameters. The error bars represent the standard error
of the estimate in least squares.

We examined how the complexity of synthetic images affects fine-tuning performance. We controlled
the following four rendering parameters: Appearance: Number of objects in each image; single
or multiple (max 10 objects). Light: Either an area and point light is randomized or fixed in
terms of height, color, and intensity. Background: Either the textures of floor/wall are randomized
or fixed. Object texture: Either the 3D objects used for rendering contain texture (w/) or not (w/o).
Indeed, the data complexity satisfies the following ordered relationships: single < multiple in
_appearance, fix < random in light and background, and w/o < w/ in object texture[5]. To quantify_
the complexity, we computed the negative entropy of the Gaussian distribution fitted to the last
activation values of the backbone network. For this purpose, we pre-trained ResNet-50 as a backbone
with MS-COCO for 48 epochs and computed the empirical covariance of the last activations for all
the synthetic data sets.

The estimated parameters are shown in Figure 6, which indicates the following (we discuss the
implications of these results further in Section 5.1).

-  Data complexity controlled by the rendering settings correlates with the negative entropy, implying
the negative entropy expresses the actual complexity of pre-training data.

-  Pre-training rate α correlates with data complexity. The larger complexity causes slower rates of
convergence with respect to the pre-training data size.



-  Transfer gap C mostly correlates negatively with data complexity, but not for object texture.

As discussed in Section 4.1, we have fixed the value of D to avoid numerical instability, which might
cause some bias to the estimates of α. We postulate, however, the value of D depends mainly on
the fine-tuning task and thus has a fixed value for different pre-training data complexities. This can
be inferred from the theoretical analysis in Appendix E.5: the exponent β in the main factor s[−][β]
of D does not depend on the pre-training data distribution but only on the fine-tuning task or the
pre-training true mapping. Thus, the values of D should be similar over the different complexities,
and the correlation of α preserves.


5The object category of w/o is a subset of w/, and w/ has a strictly higher complexity than w/o.


-----

5 CONCLUSION AND DISCUSSION

In this paper, we studied how the performance on syn2real transfer depends on pre-training and
fine-tuning data sizes. Based on the experimental results, we found a scaling law (1) and its
generalization (2) that explain the scaling behavior in various settings in terms of pre-training/finetuning tasks, model sizes, and data complexities. Further, we present the theoretical error bound for
transfer learning and found our theoretical bound has a good agreement with the scaling law.

5.1 IMPLICATION OF COMPLEXITY RESULTS IN SECTION 4.5

The results of Section 4.5 has two implications. First, data complexity (i.e., the diversity of images)
largely affects the pre-training rate α. This is reasonable because if we want a network to recognize
more diverse images, we need to train it with more examples. Indeed, prior studies (Sharma &
Kaplan, 2020; Bahri et al., 2021) observed that α is inversely proportional to the intrinsic dimension
of the data (e.g., dimension of the data manifold), which is an equivalent concept of data complexity.

Second, the estimated values of the transfer gap C suggest that increasing the complexity of data is
generally beneficial to decrease C, but not always. Figure 6 (right) shows that increasing complexities
in terms of appearance, light, and background reduces the transfer gap, which implies that these
rendering operations are most effective to cover the fine-tuning task that uses real images. However,
the additional complexity in object texture works negatively. We suspect that this occurred because of
_shortcut learning (Geirhos et al., 2020). Namely, adding textures to objects makes the recognition_
problem falsely easier because we can identify objects by textures rather than shapes. Because CNNs
prefer to recognize objects by textures (Geirhos et al., 2018; Hermann et al., 2019), the pre-trained
models may overfit to learn the texture features. Without object textures, pre-trained models have
to learn the shape features because there is no other clue to distinguish the objects, and the learned
features will be useful for real tasks.

5.2 LESSONS TO TRANSFER LEARNING AND SYNTHETIC-TO-REAL GENERALIZATION

Our results suggest the transfer gap C is the most crucial factor for successful transfer learning
because C determines the maximum utility of pre-training. Large-scale pre-training data can be
useless when C is large. In contrast, if C is negligibly small, the law is reduced essentially to n[−][α],
which tells that the volume of pre-training data is directly exchanged to the performance of fine-tuning
tasks. Our empirical results suggest two strategies for reducing C: 1) Use bigger models and 2) fill
the domain gap in terms of the decision rule and image distribution. For the latter, existing techniques
such as domain randomization (Tobin et al., 2017) would be helpful.

5.3 LIMITATIONS OF THIS STUDY

-  In the experiments, the scale of data is relatively limited (million-scale, not billion).

-  We only examined ResNet as a network architecture (no Transformers).

-  Although there are various visual tasks, our study only covers a few of them. Extending our
observations to other visual tasks such as depth estimation, instance segmentation, and keypoint
detection, as well as to other data domains such as language is future work.

-  The theoretical results assume several conditions that may contradict the actual setting in the
experiments. For example, our theory relies on ASGD instead of vanilla SGD. Also, the task types
are assumed to be identical for the pre-training and fine-tuning tasks.

-  In this study, we focus on finding a general rule of transfer learning, rather than improving absolute
performance on specific tasks. We used popular vision tasks such as classification and ready-made
rendering settings that is not designed to pre-train for the tasks. We expect to observe more
performance gain with other syn2real-friendly tasks such as optical flow and elaborate rendering
settings in future work.

REFERENCES

David Acuna, Guojun Zhang, Marc T Law, and Sanja Fidler. f-domain-adversarial learning: Theory
and algorithms. arXiv preprint arXiv:2106.11344, 2021.


-----

Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
[neural networks, going beyond two layers. CoRR, abs/1811.04918, 2018. URL http://arxiv.](http://arxiv.org/abs/1811.04918)
[org/abs/1811.04918.](http://arxiv.org/abs/1811.04918)

Shun-ichi Amari, Naotake Fujita, and Shigeru Shinomoto. Four types of learning curves. Neural
_Computation, 4(4):605–618, 1992._

Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In Proceedings
_of the 36th International Conference on Machine Learning, volume 97, pp. 322–332, 2019._

Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural
scaling laws. arXiv preprint arXiv:2102.06701, 2021.

Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, volume 30. Curran
Associates, Inc., 2017.

Jonathan Baxter. A model of inductive bias learning. Journal of artificial intelligence research, 12:
149–198, 2000.

Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee. Yolact: Real-time instance segmentation.
In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 9157–9166,
2019.

João Borrego, Atabak Dehban, Rui Figueiredo, Plinio Moreno, Alexandre Bernardino, and José
Santos-Victor. Applying domain randomization to synthetic data for object category detection.
_arXiv preprint arXiv:1807.09834, 2018._

A. Caponnetto and E. De Vito. Optimal rates for regularized least-squares algorithm. Foundations of
_Computational Mathematics, 7(3):331–368, 2007._

Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous
convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017.

Wuyang Chen, Zhiding Yu, Shalini De Mello, Sifei Liu, Jose M. Alvarez, Zhangyang Wang, and
Anima Anandkumar. Contrastive syn-to-real generalization. arXiv preprint arXiv:2104.02290,
2021.

Maximilian Denninger, Martin Sundermeyer, Dominik Winkelbauer, Youssef Zidan, Dmitry Olefir, Mohamad Elbadrawy, Ahsan Lodhi, and Harinandan Katam. Blenderproc. arXiv preprint
_arXiv:1911.01911, 2019._

Jeevan Devaranjan, Amlan Kar, and Sanja Fidler. Meta-sim2: Unsupervised learning of scene
structure for synthetic data generation. In European Conference on Computer Vision, pp. 715–733.
Springer, 2020.

Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient Descent Finds Global
Minima of Deep Neural Networks. In Proceedings of the 36th International Conference on
_Machine Learning, volume 97, pp. 1675–1685, 2019._

Simon S Du, Jayanth Koushik, Aarti Singh, and Barnabas Poczos. Hypothesis Transfer Learning
via Transformation Functions. In I Guyon, U V Luxburg, S Bengio, H Wallach, R Fergus,
S Vishwanathan, and R Garnett (eds.), Advances in Neural Information Processing Systems,
[volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/](https://proceedings.neurips.cc/paper/2017/file/352fe25daf686bdb4edca223c921acea-Paper.pdf)
[paper/2017/file/352fe25daf686bdb4edca223c921acea-Paper.pdf.](https://proceedings.neurips.cc/paper/2017/file/352fe25daf686bdb4edca223c921acea-Paper.pdf)

Simon S. Du, Wei Hu, Sham M. Kakade, Jason D. Lee, and Qi Lei. Few-shot learning via learning
the representation, provably. arXiv preprint arXiv:2002.09434, 2020.

Li Fei-Fei, R Fergus, and P Perona. One-shot learning of object categories. IEEE Transactions on
_Pattern Analysis and Machine Intelligence, 28(4):594–611, 2006. doi: 10.1109/TPAMI.2006.79._


-----

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks.
_The journal of machine learning research, 17(1):2096–2030, 2016._

Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and
Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves
accuracy and robustness. arXiv preprint arXiv:1811.12231, 2018.

Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias
Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine
_Intelligence, 2(11):665–673, 2020._

Georgios Georgakis, Arsalan Mousavian, Alexander C. Berg, and Jana Kosecka. Synthesizing
training data for object detection in indoor scenes. arXiv preprint arXiv:1702.07836, 2017.

Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training
ImageNet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.

Kaiming He, Ross Girshick, and Piotr Dollár. Rethinking imagenet pre-training. arXiv preprint
_arXiv:1811.08883, 2018._

Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo
Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford,
Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, and Sam McCandlish. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701,
2020.

Katherine L. Hermann, Ting Chen, and Simon Kornblith. The origins and prevalence of texture bias
in convolutional neural networks. arXiv preprint arXiv:1911.09071, 2019.

Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for transfer.
_arXiv preprint arXiv:2102.01293, 2021._

Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad,
Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable,
empirically. arXiv preprint arXiv:1712.00409, 2017.

Stefan Hinterstoisser, Olivier Pauly, Hauke Heibel, Martina Marek, and Martin Bokeloh. An
annotation saved is an annotation earned: Using fully synthetic training for object instance
detection. arXiv preprint arXiv:1902.09967, 2019.

Tomáš Hodaˇn, Vibhav Vineet, Ran Gal, Emanuel Shalev, Jon Hanzelka, Treb Connell, Pedro Urbina,
Sudipta N Sinha, and Brian Guenter. Photorealistic image synthesis for object instance detection.
In 2019 IEEE International Conference on Image Processing (ICIP), pp. 66–70. IEEE, 2019.

Tomáš Hodaˇn, Martin Sundermeyer, Bertram Drost, Yann Labbé, Eric Brachmann, Frank Michel,
Carsten Rother, and Jiˇrí Matas. BOP challenge 2020 on 6D object localization. European
_Conference on Computer Vision Workshops (ECCVW), 2020._

Minyoung Huh, Pulkit Agrawal, and Alexei A. Efros. What makes imagenet good for transfer
learning? arXiv preprint arXiv:1608.08614, 2016.

Marcus Hutter. Learning curve theory. arXiv preprint arXiv:2102.04074, 2021.

Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in Neural Information Processing Systems 31, pp.
8571–8580. Curran Associates, Inc., 2018.

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.
_arXiv preprint arXiv:2001.08361, 2020._


-----

[Andrej Karpathy. Tesla ai day. https://www.youtube.com/watch?v=j0z4FweCy4M,](https://www.youtube.com/watch?v=j0z4FweCy4M)
2021.

James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis
Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic
forgetting in neural networks. _Proceedings of the National Academy of Sciences of the_
_United States of America, 114(13):3521–3526, mar 2017. ISSN 1091-6490. doi: 10.1073/_
[pnas.1611835114. URL http://www.ncbi.nlm.nih.gov/pubmed/28292907http:](http://www.ncbi.nlm.nih.gov/pubmed/28292907 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5380101)
[//www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5380101.](http://www.ncbi.nlm.nih.gov/pubmed/28292907 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5380101)

Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,
and Neil Houlsby. Big transfer (bit): General visual representation learning. arXiv preprint
_arXiv:1912.11370, 2019._

Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European
_conference on computer vision, pp. 740–755. Springer, 2014._

Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie.
Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer
_vision and pattern recognition, pp. 2117–2125, 2017._

Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic
segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 3431–3440, 2015.

Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The benefit of multitask
representation learning. _Journal of Machine Learning Research, 17(81):1–32, 2016._ URL
[http://jmlr.org/papers/v17/15-242.html.](http://jmlr.org/papers/v17/15-242.html)

Mehdi Mousavi, Aashis Khanal, and Rolando Estrada. Ai playground: Unreal engine-based data
ablation tool for deep learning. In International Symposium on Visual Computing, pp. 518–532.
Springer, 2020.

Yair Movshovitz-Attias, Takeo Kanade, and Yaser Sheikh. How useful is photo-realistic rendering
for visual learning? arXiv preprint arXiv:1603.08152, 2016.

Alejandro Newell and Jia Deng. How useful is self-supervised pretraining for visual tasks? arXiv
_preprint arXiv:2003.14323, 2020._

Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Proceedings of The 28th Conference on Learning Theory, pp. 1376–1401, 2015.

Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nathan Srebro. Exploring Generalization in Deep Learning. In Advances in Neural Information Processing Systems 30, pp.
5947–5956, 2017.

Atsushi Nitanda and Taiji Suzuki. Stochastic gradient descent with exponential convergence rates of
expected classification errors. In Proceedings of the Twenty-Second International Conference on
_Artificial Intelligence and Statistics, volume 89 of Proceedings of Machine Learning Research, pp._
1417–1426, 2019.

Atsushi Nitanda and Taiji Suzuki. Optimal rates for averaged stochastic gradient descent under neural
tangent kernel regime. In International Conference on Learning Representations, 2021.

Atsushi Nitanda, Geoffrey Chinot, and Taiji Suzuki. Gradient descent can learn less overparameterized two-layer neural networks on classification problems, 2020.

Boris T Polyak and Anatoli B Juditsky. Acceleration of stochastic approximation by averaging. SIAM
_journal on control and optimization, 30(4):838–855, 1992._


-----

Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object
detection with region proposal networks. IEEE transactions on pattern analysis and machine
_intelligence, 39(6):1137–1149, 2016._

Jonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction
of the generalization error across scales. arXiv preprint arXiv:1909.12673, 2019.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet
Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115
(3):211–252, 2015. doi: 10.1007/s11263-015-0816-y.

Utkarsh Sharma and Jared Kaplan. A neural scaling law from the dimension of the data manifold.
_arXiv preprint arXiv:2004.10802, 2020._

Hao Su, Charles R Qi, Yangyan Li, and Leonidas J Guibas. Render for cnn: Viewpoint estimation in
images using cnns trained with rendered 3d model views. In Proceedings of the IEEE International
_Conference on Computer Vision, pp. 2686–2694, 2015._

Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable
effectiveness of data in deep learning era. In Proceedings of the IEEE international conference on
_computer vision, pp. 843–852, 2017._

Taiji Suzuki. Fast generalization error bound of deep learning from a kernel perspective. In
_Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics,_
volume 84, pp. 1397–1406, 2018.

Zachary Teed and Jia Deng. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras.
_arXiv preprint arXiv:2108.10869, 2021._

Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Domain
randomization for transferring deep neural networks from simulation to the real world. arXiv
_preprint arXiv:1703.06907, 2017._

Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani, Cem Anil, Thang
To, Eric Cameracci, Shaad Boochoon, and Stan Birchfield. Training deep networks with synthetic
data: Bridging the reality gap by domain randomization. arXiv preprint arXiv:1804.06516, 2018.

Nilesh Tripuraneni, Michael I. Jordan, and Chi Jin. On the theory of transfer learning: The importance
of task diversity. arXiv preprint arXiv:2006.11650, 2020.

Colin Wei and Tengyu Ma. Improved Sample Complexities for Deep Neural Networks and Robust
Classification via an All-Layer Margin. In International Conference on Learning Representations,
[2020. URL https://openreview.net/forum?id=HJe_yR4Fwr.](https://openreview.net/forum?id=HJe_yR4Fwr)

Jun Yang, Rong Yan, and Alexander G Hauptmann. Cross-Domain Video Concept Detection
Using Adaptive Svms. In Proceedings of the 15th ACM International Conference on Multimedia,
MM ’07, pp. 188–197, New York, NY, USA, 2007. Association for Computing Machinery.
[ISBN 9781595937025. doi: 10.1145/1291233.1291276. URL https://doi.org/10.1145/](https://doi.org/10.1145/1291233.1291276)
[1291233.1291276.](https://doi.org/10.1145/1291233.1291276)

Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba.
Semantic understanding of scenes through the ade20k dataset. arXiv preprint arXiv:1608.05442,
2016.

Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene
parsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and
_pattern recognition, pp. 633–641, 2017._

Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, Ekin D. Cubuk, and Quoc V. Le.
Rethinking pre-training and self-training. arXiv preprint arXiv:2006.06882, 2020.


-----

# Appendix

A TRAINING DETAILS

A.1 OBJECT DETECTION

We used Faster-RCNN (Ren et al., 2016) with FPN (Lin et al., 2017) as object detection models and
ResNet (Goyal et al., 2017) as a backbone network of Faster-RCNN.

We used the following training procedure: We trained the model using momentum SGD of momentum
0.9 with weight decay of 10[−][4]. The global batch size was set to 64 when training ResNet18, ResNet34,
ResNet50, and ResNet101. The batch size was set to 32 when training ResNet152 to avoid out-ofmemory errors. The batch statistics in batch normalization layers were computed across all GPUs.
We used a base image size of 640 × 640 in the same way as YOLACT training (Bolya et al., 2019).
We used mixed16 training to reduce the memory footprint. We also adopted random horizontal
flipping as data augmentation to images. The learning rate was set to 0.02, and we used the cosine
decay with a warmup scheme. The warmup length is 120,000 images (3,750 iterations for ResNet152
and 1,875 iterations for other models). As for evaluation, we followed the standard settings in COCO
dataset (Ren et al., 2016).

We pre-trained the model with 14,400,000 images (450,000 iterations for ResNet152 and 225,000
iterations for other models). We used the models that achieved the best mmAP as the initial value of
fine-tuning.

We used COCO (Lin et al., 2014) as the fine-tuning dataset. We trained the model with 1,440,000
images (45,000 iterations for ResNet152 and 22,500 iterations for other models) during fine-tuning.

A.2 SEMANTIC SEGMENTATION

We used DeepLabV3 (Chen et al., 2017) with the softmax cross-entropy loss as the semantic
segmentation model and ResNet50 (Goyal et al., 2017) as its backbone. The model configuration
follows the implementation in torchvision[6]. It should be noted that DeepLabV3 requires dilated
ResNet as the backbone, which is not the case in object detection and classification tasks. Even though,
the shapes of weight tensors of dilated ResNet50 exactly match those of non-dilated ResNet50; thus
we can use the pre-trained weights of dilated and non-dilated ResNet50 interchangeably.

The learning procedure is based on the reference implementation[7] of torchvision. We added an
auxiliary branch based on FCN (Long et al., 2015) which takes conv4 of the backbone as the input.
In the computation of loss function, the loss for the auxiliary branch is computed in the same way as
for the main branch and is added to the overall loss after multiplying by the factor 0.5. The model
was trained using momentum SGD of momentum 0.9 with weight decay of 10[−][4]. The global batch
size was set to 32. The batch statistics in batch normalization layers were computed across all GPUs.
During training, images were first resized so that the length of the shorter edge becomes an integer
uniformly chosen from [520 × 0.5, 520 × 2], then horizontally flipped with probability 0.5, finally
randomly cropped to 480 × 480. The learning rate (LR) was decayed according to the polynomial
LR schedule of rate 0.9 and initial LR of 0.02. For the parameters of the auxiliary classifier, the LR
was multiplied by 10. The evaluation was performed once every 3,125 iterations (almost equivalent
to 5 epochs in full ADE20K). In the evaluation, images were resized so that the length of the shorter
edge becomes 520.

In pre-training, we trained 125,000 iterations which roughly equals 200 epochs in full ADE20K
(Zhou et al., 2017). We used the model that achieved the best mIoU as the initial value of fine-tuning.
We pre-trained models using our synthetic datasets. When training with them, backgrounds (points at
which no foreground objects were present) were also considered to be a separate class in semantic
segmentation.

[6https://github.com/pytorch/vision](https://github.com/pytorch/vision)
[7https://github.com/pytorch/vision/tree/master/references/segmentation](https://github.com/pytorch/vision/tree/master/references/segmentation)


-----

We used the ADE20K (Zhou et al., 2017) datasets as the fine-tuning target. In fine-tuning, we trained
the model for 18,750 iterations, which correspond to 30 epochs of full ADE20K. The metric was
mIoU score.

A.3 MULTI-LABEL CLASSIFICATION

We used ResNet (Goyal et al., 2017) with binary cross-entropy used as the loss function in multi-label
classification. We used the following training procedure: We trained the model using momentum
SGD of momentum 0.9 with weight decay of 10[−][4]. The batch size was set as 32 per GPU, thus
256 in total. We trained the models for 112,500 iterations. The input size of the images was simply
resized to 640 × 640. We adopted random horizontal flipping as data augmentation to images. The
learning rate was set to 0.1. The cosine decay with a warmup scheme was used. The warmup length
was 120,000 images. The evaluation was performed once every 120,000 images. In the evaluation,
the image size was the same as that used during training, and data augmentation was not used. We
used the mAP score as the metric.

A.4 SINGLE-LABEL CLASSIFICATION

As in multi-label classification, we used ResNet. The softmax cross-entropy was used as the loss
function of the single-label classification. The learning procedure is based on (Goyal et al., 2017).
However, we used cosine decay for the learning rate scheduling.

A.5 SURFACE NORMAL ESTIMATION

As in semantic segmentation, we used DeepLabV3 (Chen et al., 2017) as the model for surface
normal estimation. The model configuration and the training procedure were exactly the same as in
semantic segmentation, except for the following changes:

-  Dimension of output channels was changed to 3, each of which corresponds to the 3 axes of the
normal vector,

-  Initial LR was changed to 0.04,

-  Length of pre-training was 200,000 iterations, which corresponds to 100 epochs in our synthetic
dataset,

-  Random flipping was not performed during the data augmentation, and

-  Loss function was the average of the value, 1.0 − n · ˆn, which was computed for each valid pixel
where n is the ground-truth normal vector and ˆn is the model output (after L2 normalization).

B SYNTHETIC DATA DETAILS

The data generation strategy is based on the “on surface sampling” setting in the BoP challenge
dataset[8]. In this setting, the sampled objects will be spawned in a cube-shaped room with one point
light and one surface light. As the objects to be spawn, we used all the BoP object sets, i.e., LM,
T-LESS, ITODD, HB, YCB-V, RU-APC, IC-BIN, IC-MI, TUD-L, and TYO-L.[9]. There are 173
objects in total. After generated a random scene (position of objects, lights, etc), we took 10 pictures
by 10 different camera poses. This means that, if we have 10K images in total, there are 1K unique
scenes, and 9K images are inflated by just changing the camera angle and position.

To control the data complexity, we selected four attributes in the generation strategy and prepared
two options for each.

_Appearance controls how many objects are generated in a room in a single scene. For each scene,_
we randomly select ten objects for the multiple setting and one object for the single setting.

_Light controls the light sources. In the random setting, the color, height, and strength of lights are_
randomized. In contrast, in the fix setting, they are all fixed.

[8https://github.com/DLR-RM/BlenderProc/tree/main/examples/bop_challenge](https://github.com/DLR-RM/BlenderProc/tree/main/examples/bop_challenge)
[9https://bop.felk.cvut.cz/datasets/](https://bop.felk.cvut.cz/datasets/)


-----

_Background controls the texture of the room, i.e., floor and walls. In the random setting, we assign_
a random PBR material from the CC0 Textures[10] library, and we selected one carpet texture
for the fix setting.

_Object texture controls the object set to be used. The BoP object set is consists of several types_
of object sets as described above. Among them, T-LESS and ITODD consist of industryrelevant objects and they do not have textures and colors.[11] For the w/o setting, we only
used such texture-less objects to be sampled. In contrast, we sample all the 173 objects
include T-LESS and ITODD in the w/ setting.

We generated eight variations of datasets by changing these attributes, which were used in the
experiments of Section 4.5. Figure 7 shows the example of generated images with the value of each
attribute.

[10https://ambientcg.com](https://ambientcg.com)
11T-Less and ITODD contain 30 and 28 objects, respectively.


-----

object texture: w/
appearance: multiple
light: random
background: random

object texture: w/o
appearance: multiple
light: random
background: random

object texture: w/
appearance: multiple
light: random
background: fix

object texture: w/
appearance: multiple
light: fix
background: fix

object texture: w/o
appearance: multiple
light: random
background: fix

object texture: w/o
appearance: multiple
light: fix
background: fix

object texture: w/
appearance: single
light: fix
background: fix

object texture: w/o
appearance: single
light: fix
background: fix


Figure 7: Example of generated datasets


-----

pretrain task

objdet semseg mulclass normal


0.8

0.6

0.4

0.2

0.95

0.90

0.85

0.80

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
||G|||
|G||G||

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|G|G|||
||||G|

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||G||
|||||
|||G||
|G||||

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|G||G||
|||||

|G|Col2|
|---|---|
|||
|||
|||
|||

|G|Col2|
|---|---|
|G||
|||
|||
|||
|||

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|G||||
|||||
|||||
|||||
|||||

|G|Col2|
|---|---|
|||
|||
|||
||G|


G G

G


G

G

G


finetune task

Figure 8: The estimated values of the pre-training rate α and the transfer gap C in the cross-task
setting (as the same as Figure 1). The error bars present the standard error of the estimates in the
least squares.


32

64


16

32


64


finetune ratio (%)


pretrain size (K) G 2 G 8 G

|G G|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||||||||
||GGGGGGG||||||
|||GGGGGGG|||||
|||||GGGGGGG|||
||||||GGGGGGG||
|||||||GGGGGGG|



10[4.5] 10[5] 10[5.5]

# of finetune images


16


0.7

0.5

0.3


0.7

0.5

0.3

|Col1|G|G|G|G|Col6|G|G|
|---|---|---|---|---|---|---|---|
||G|G|G|G||G|G|
||G|G|G|G||G|G|
||G|G|G|G||G|G|
||G|G|G|G||G|G|
||G|G|G|G||G|G|


10[3] 10[3.5] 10[4] 10[4.5]

# of pretrain images


Figure 9: Empirical and fitting results for various pre-train and fine-tune data sizes in
mulclass→sinclass. All curves are fitted using the full law (2). Best viewed in color. Left:
Effect of pre-training data size (x-axis) for fixed fine-tuning data sizes. Right: Effect of fine-tuning
data size (x-axis) for fixed pre-training data sizes.

C ADDITIONAL EXPERIMENTS


C.1 ESTIMATED PARAMETERS IN THE CROSS-TASK SETTING

Figure 8 shows the estimated parameters (α, C) at the experiments described in Section 4.2. Note
that the result of α at normal→semseg is omitted because its estimated value is highly unstable
(the standard deviation is larger than 1).


C.2 FULL SCALING LAW COLLECTIVELY RELATES PRE-TRAINING AND FINE-TUNING DATA
SIZE

Next, we verify the validity of the full scaling law (2). In the mulclass→sinclass setting
with ResNet-50, we changed the fine-tuning data size from 2% to 64% of the ImageNet.[12] We then
fitted all results by a single equation (2) to estimate the parameters except the irreducible loss E

12ImageNet contains a class imbalance problem. If we use 100% of the ImageNet, we cannot provide the
same sample size per class. To eliminate the effect of class imbalance, we made the sampling ratio to keep the
balance up to 64% and excluded the case of 100%.


-----

(we assumed E = 0 from the preliminary results in Figure 2). The results in Figure 9 show that all
empirical test errors are explained remarkably well by Eq. (2), which has only four parameters to fit
in this case. The estimated parameters are α = 0.544, β = 0.322, γ = 0.478, and δ = 41.8.

C.3 LINEARIZED RESULTS


The transfer gap C in (1) causes a plateau of the scaling law. Conversely, if we subtract the estimated
_C from the results, we must be able to recover the power-law scaling. To confirm this, we subtracted_
the estimated C from the empirical errors _L[ˆ] of the previous results. Figures 10–12 show the modified_
version of scaling law fittings. Overall, the empirical errors behave linearly along with the estimated
power-law term Dn[−][α]. Note that, in mulclass→semseg and normal→semseg, a few points
of _L[ˆ] become negative after subtracting C, and these points are not depicted._

pretrain task


objdet semseg mulclass normal


0.030
0.010

0.003
0.001

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
|G|G||||||||
||||G|G|||||
||||||G||||
|||||||G|||
||||||||||
||||||||||

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|||||||||
|G||||||||
||G|G||||||
||||G|G||||
||||||||G|
|||||||G||

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|||G|||||||||||||||
||||G|G|G||G|G|||||||||
||||||||||||||||||
||||||||||G||||||||
||||||||||||G|G|G|G|||
||||||||||||||||||
||||||||||||||||||
||||||||||||||||G||


1e−02

1e−05

1e−08

0.100

0.010

0.001

|G|G|Col3|G|G|G|G|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
||||||||||
||||||||||
||||||||||
||||||||||
||||||||||

|G|G|G|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|G|G|G|G|G||G|G|
|||||||||
|||||||||
|||||||||
|||||||||
|||||||||

|G|Col2|Col3|Col4|G|G|Col7|Col8|G|Col10|Col11|G|G|G|G|G|Col17|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
||||||||||||||||||
||||||||||||||||||
||||||||||||||||||
||||||||||||||||||
||||||||||||||||||

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
|G|G||||||||
||||G|G||G|||
||||||G|||G|
||||||||||

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|G|G|||||||
|||G|G|G||||
|||||||G|G|
|||||||||

|G|Col2|G|G|G|G|Col7|G|G|Col10|Col11|Col12|Col13|Col14|Col15|Col16|Col17|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
||||||||||G||G||||||
|||||||||||||G|G|G|||
||||||||||||||||G||
|||||||||||||||||G|


10[3] 10[3.5] 10[4] 10[4.5] 10[3] 10[3.5] 10[4] 10[4.5] 10[3] 10[3.5] 10[4] 10[4.5] 10[3] 10[3.5] 10[4] 10[4.5]

# of pretrain images

Figure 10: The linearized version of Figure 1.


mulclass → sinclass objdet → objdet

0.030


0.10

0.05


model

G r18

r34

r50

r101

r152


0.010


G G

G


0.03


0.003

|Col1|G|G|G|G|Col6|G|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
||||||||||
||||||||G||


G

|Col1|G|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
|||G|G|G||G|
||||||||
||||||||


G


10[3] 10[3.5] 10[4] 10[4.5] 10[3] 10[3.5] 10[4] 10[4.5]

# of pretrain images


Figure 11: The linearized version of Figure 4.


D EMPIRICAL DEPENDENCY BETWEEN α AND D

When C is non-zero, the joint estimation of D and α in (1) have an issue of numerical stability due
to the small number of observations and noise, which can cause high dependence on each other.
Figure 13 (left) shows the curves of (1) with C = 0.5, where the solid red curve is D = 0.5, α = 0.4
and the dashed blue line is D = 1, α = 0.5. We see that both curves are almost indistinguishable for


-----

appearance

G multiple


objdet → objdet

|G|Col2|G|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|
|---|---|---|---|---|---|---|---|---|---|---|
|G||G|G|G|||||||
||||G|G||G||G|G G||
|G||G|G||||||G||
|||||||G||||GG|
|||||G|||||||
||||||||||||
||||||||||G||
|||||||G|||||
||||||||||||



1000 3000 10000 30000

# of pretrain images


3e−02

1e−02


single

fix


light


3e−03

1e−03


random


background

G fix


random


3e−04


object

G w/ texture

w/o texture


Figure 12: The linearized version of Figure 6.


2.0

1.5

1.0

0.5

0.0


1.0

0.7

0.5


10[2] 10[5] 10[8]


0.00 0.25 0.50 0.75 1.00


Figure 13: Examples of parameter dependency.


a large n. Figure 13 (right) shows the actual landscape in terms of α and D of the nonlinear leastsquares at objdet→objdet, the bright areas indicate the fitting loss is small. We see that there
is a quadratic-like trajectory in the landscape, which implies the solutions are somehow redundant.
Similar landscapes were observed for other tasks (Figure 14).

pretrain task


objdet semseg mulclass normal


2.0
1.5
1.0
0.5
0.0

2.0
1.5
1.0
0.5
0.0

2.0
1.5
1.0
0.5
0.0


0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0

α

Figure 14: Loss landscapes of curve fittings.


-----

To avoid this issue, we fixed a common D for all the cases and estimated α for each. To determine
_D, we used the following procedure. First, we prepared two global parameters ˆα,_ _D[ˆ] and set 0.5 as_
their initial values. Then, we fitted the curves by two equations, Dn[−]α[ˆ] + C and ˆDn[−][α] + C, and
estimated α and D. Next, we computed the median of α and substituted them into ˆα. We did the
same for D and _D[ˆ]_ . After a few iterations, we got a converged value of _D[ˆ] = 0.48. In the experiments,_
we used the value for D and fixed it.

E DETAILS OF THEORETICAL ANALYSIS

This section gives details of the theoretical discussions given in Section 3.2. For the analysis of
learning and generalization bound, we use the techniques developed recently by Nitanda & Suzuki
(2021). There are many works on the generalization of neural networks. To list a few, Neyshabur
et al. (2015), Neyshabur et al. (2017), Bartlett et al. (2017), Wei & Ma (2020), and Suzuki (2018)
analyze the generalization of neural networks based on complexity bounds. These generalization
bounds, however, do not consider an algorithm of learning, such as stochastic gradient descent (SGD).
Recently, learning dynamics of neural networks has been analyzed based on Neural Tangent Kernel
(NTK) Jacot et al. (2018) and global convergence of wide neural networks has been revealed AllenZhu et al. (2018); Du et al. (2019). Based on the NTK framework, Arora et al. (2019) and Nitanda
et al. (2020) showed a generalization bound of the gradient descent learning of neural networks. More
recently, Nitanda & Suzuki (2021) focused the functional space given by NTK and showed that the
two-layer neural network with averaged SGD achieves the minimax optimal rate with respect to the
function class used in the standard theory of function estimation with kernels. We employ the method
of Nitanda & Suzuki (2021), which is the most suitable for our analysis of transfer learning: it enables
to examine the dependence on the initial parameter in the learning, and avoids the assumption of a
positive margin of eigenvalues used in Arora et al. (2019) and Nitanda et al. (2020).

E.1 PROBLEM SETTING

In the pre-training, the task is to learn the target function φ0 with T0 training data (˜xi, ˜yi)[T]i=1[0] [, where]
_yi = φ0(xi), while in the fine-tuning phase, the network is initialized by the final parameter learned_
by the pre-training, and the whole parameter is updated in the training. We assume that the target
function in the fine-tuning is given by

_ϕ(x) = φ0(x) + φ1(x),_ (4)

and T1 training data is given by (xj, yj)[T]j=1[1] [with][ y][j][ =][ ϕ][(][x][j][)][. In this setting, the goal of the fine-]
tuning phase will be to learn the additional function φ1 mainly. Note that, for simplicity of analysis,
we assume noiseless training data, i.e., we assume the supervised signal yj is given by a deterministic
function of xj, but extension to more general cases is not difficult as discussed in Nitanda & Suzuki
(2021). In the analysis, the data are assumed to satisfy x ∈ R[d], ∥x∥2 = 1 and y ∈ [0, 1]. The
distribution of the input data x is denoted by ρX, and the same for the pre-training and fine-tuning.

For tractable theoretical analysis, we consider a simple scalar-valued two-layer neural network model
with M hidden units:


_arσ(b[T]r_ _[x][)][.]_ (5)
_r=1_

X


_gΘ(x) =_


We omit the bias term, but with obvious modification, it is not difficult to include it (see Nitanda &
Suzuki (2021)).

As in Nitanda & Suzuki (2021), we consider the averaged stochastic gradient descent (ASGD), where
one training sample is given at every time step for the stochastic gradient descent as in online learning,
and all the parameters in the time course are averaged after the final time step for the inference, that
is, after proceeding up to prescribed T (T = T0 or T1) time steps, the parameter to be used in the
inference is given by


(T ) :=


Θ[(][t][)]. (6)

_t=0_

X


_T + 1_


The final network uses this averaged parameter, i.e., the final network is given by g (T ) (x).
Θ


-----

The parameter is initialized as Θ[(0)] = (a[(0)]1 _[, b]1[(0)][, . . ., a]M[(0)][, b]M[(0)][)][. For the pre-trainig, each][ b][(0)][ is]_
independently given by the uniform distribution on the unit sphere. As in Nitanda & Suzuki (2021),
_a[(0)]_ are initilized as 1 or 1 so that gΘ(0) = 0. As explained before, the initial parameter of the

fine-tuning is the same as the averaged parameter of the pre-training − Θ(preT0)[. The objective function to]
minimize for the pre-training and fine-tuning is given by the following regularized empirical risk:


_L(Θ) := [1]_


(yi _gΘ(xi))[2]_ + _[λ]_
_−_ 2


_∥a −_ _a[(0)]∥2[2]_ [+]


_br_ _b[(0)]r_ 2 _,_ (7)
_∥_ _−_ _[∥][2]_
o


where λ is the regularization coefficient, which is a hyperparameter. The values of λ in the pre-training
and fine-tuning can be different, and denoted by λ0 and λ1, respectively. Note that the regularization
in Eq. (7) is not the most common ℓ2-regularization, where ∥Θ∥2[2] [is used for the regularization.]
When applied in fine-tuning, however, the above regularization can be interpreted as elastic weight
_consolidation (Kirkpatrick et al., 2017), which prevents forgetting the pre-trained parameters._

We consider online learning, in which at every step t (t ≤ _T −_ 1), one datum xi is sampled from ρX
independently, and (xi, yi) (or (˜xi, ˜yi) in pre-training) is used to update the parameter according to
the gradient descent:

Θ[(][t][+1)] = Θ[(][t][)] _η [∂L][(Θ][(][t][)][)]_ _,_ (8)
_−_ _∂Θ_

where η is a learning rate. More explicitly,


(gΘ(t) _yt)σ(b[(]t[t][)][T]_ _xt),_
_−_

(gΘ(t) _yt)arσ[′](b[(]t[t][)][T]_ _xt)xt._ (9)
_−_


_a[(]r[t][+1)]_ _−_ _a[(0)]r_ = (1 − _ηλ)(a[(]r[t][)]_ _−_ _a[(0)]r_ [)][ −]

_b[(]r[t][+1)]_ _−_ _b[(0)]r_ = (1 − _ηλ)(b[(]r[t][)]_ _−_ _b[(0)]r_ [)][ −]

E.2 NEURAL TANGENT KERNEL


In the theoretical analysis, the neural tangent kernel (NTK, Jacot et al., 2018) is used for approximating the dynamics of ASGD by a linear functional recursion on the corresponding function space. The
NTK of this model is given by

_k∞(x, x[′]) = Eb(0)[σ(b[(0)][T]_ _x)σ(b[(0)][T]_ _x[′])] + x[T]_ _x[′]Eb(0)[σ[′](b[(0)][T]_ _x)σ[′](b[(0)][T]_ _x[′])]._ (10)

The positive definite kernel k naturally defines a reproducing kernel Hilbert space (RKHS), which
_∞_
is denoted by .
_H∞_

The integral operator Σ∞ on L[2](ρX ) is defined by

Σ∞f := _k∞(·, x)f_ (x)dρX (x). (11)
Z

It is known that Σ admits eigendecomposition
_∞_

Σ _ψs = γsψs,_ (12)
_∞_

wheredescending order. Mercer’s theorem tells that ψs is an eigenvector with ∥ψs∥L2(ρX k) = 1has an expansion: and γ1 ≥ _γ2 ≥_ _. . . > 0 are eigenvalues in_
_∞_


_k_ (x, x[′]) =
_∞_


_γsψs(x)ψs(x[′]),_
_s=1_

X


where the convergence is understood as in L[2](ρX ) for general, and absolutely and uniformly if ρX is
a uniform distribution on a compact set.

E.3 ASSUMPTIONS

For theoretical analysis, we make the following assumptions. For an operator Σ, the range of Σ is
denoted by R(Σ).


-----

(A1) The activation function σ is differentiable up to the second order, and there exists C > 0
such that _σ[′′]_ _C,_ _σ[′]_ 2, and _σ(u)_ 1 + _u_ for _u_ R.
_∥_ _∥∞_ _≤_ _∥_ _∥∞_ _≤_ _|_ _| ≤_ _|_ _|_ _∀_ _∈_

(A2) supp(ρX ) ⊂{x ∈ R[d] _| ∥x∥≤_ 1} and y ∈ [−1, 1].
(A3) There exist 1/2 ≤ _r0, r1 ≤_ 1 such that φ0 ∈R(Σ[r]∞[0] [)][ and][ φ][1] _[∈R][(Σ]∞[r][1]_ [)][.]

(A4) There exists ξ > 1 such that γℓ = Θ(ℓ[−][ξ]).

As in Nitanda & Suzuki (2021), Assumption (A1) assumes that the activation σ is differentiable in
this paper. In Nitanda & Suzuki (2021), however, they have developed a theory on how to extend
the results to the case of ReLU by approximating it with a smooth function. It is well known that
Assumption (A4) specifies the complexity of the hypothesis class (Caponnetto & De Vito,
_H∞_
2007); a faster eigen-decay (large ξ) implies the small complexity of the class. The assumption
(A3) controls the smoothness of the target functions φ0, φ1. In fact, the functions are included
in, since φi (Σ[1][/][2] _ℓ_ _[a][ℓ][ψ][ℓ][, the]_
_H∞_ _∈R_ _∞_ [)][ ⊂H]∞[. When a function][ f][ has the expansion][ f][ =][ P]

assumption f (Σ[r]
_∈R_ _∞[)][ means][ a][ℓ]_ [=][ o][(][ℓ][−][ξr][−][1][/][2][)][. A function with a larger][ r][ is smoother, which]
is easier to learn. It is known (Caponnetto & De Vito, 2007; Nitanda & Suzuki, 2021) that ξ and
_r are the two basic parameters to control the convergence rate of generalization attained by kernel_
regression for a large sample size. Under the assumptions (A3) and (A4), given N i.i.d. training
data (Xi, Yi) with Xi ∼ _ρX and Yi = ϕ0(Xi) + εi with additive noise εi ∼_ _N_ (0, σ[2]), the kernel
ridge regression ˆϕλ with the regularization parameter λ = N _[−][ξ/][(2][rξ][+1)]_ achieves the generalization
_E[_ _ϕˆλ_ _ϕ0_ _L[2](ρX_ )[] =][ O][(][N][ −][2][rξ/][(2][rξ][+1)][)][ for any function][ ϕ][0][ with][ ϕ][0][ ∈R][(Σ][r][)][, and it is known]
_∥_ _−_ _∥[2]_
this rate is optimal.


In the sequel, when φ ∈R(Σ[r]) and φ = Σ[r]ψ, we write ∥Σ[−][r]φ∥ := ∥ψ∥.

E.4 GENERALIZATION BOUND

The dynamical behavior of pre-training can be discussed exactly in the setting of Nitanda & Suzuki
(T0)
(2021). Let _φ0 be the result of pre-training, i.e.,_ _φ0 := gΘ(preT0)_ [, where][ Θ]pre [is the averaged parameter]

by ASGD. By optimizing the regularization parameter λ0, Corollary 1 in Nitanda & Suzuki (2021)
shows that for sufficiently large[b] _T0, with a choice of[b]_ _λ0 = T0[−][ξ/][(2][r][0][ξ][+1)],_

2r0 _ξ_
_−_ 2r0 _ξ+1_
_E∥φ[b]0 −_ _φ0∥L[2]_ [2](ρX ) _[≤]_ _[ε][M][ +][ cT]0_ 1 + ∥Σ[−]∞[r][0] _[φ][0][∥]L[2]_ [2](ρX ) (13)

with high probability, where c is a universal constant and  εM can be arbitrarily small for a large 2r0 _ξ_

_−_ 2r0 _ξ+1_
_M_ . As discussed in Section E.3, it is known (Caponnetto & De Vito, 2007) that the rate T0

achieves the minimax optimal rate with respect to T0 over the class specified by r0 and ξ.

In fine-tuning, the initial parameter is given by Θ[(0)] = Θ(preT0)[, and ASGD is applied with][ (][x][t][, y][t][)][ for]
_t = 1, . . ., T1._


By extending Theorem 1 in Nitanda & Suzuki (2021), we can derive a generalization bound in the
following theorem. Recall that the regularization coefficient and learning rate of fine-tuning are
denoted by λ1 and η1, respectively.
**Theorem 2. Suppose Assumptions (A1)-(A3) hold. After pre-training that gives Eq. (13), fine-**
_tune the network by Eq. (9) with a learning rate η1 and regularization coefficient λ1 that satisfy_
Σ _op_ _λ1 > 0 and 4(6 + λ1)η1_ 1. Then, for any ε > 0, δ (0, 1), and T1 N, there exists
_∥Mthe random initiailzation of pre-training:0∞ ∈∥N such that for any ≥_ _M ≥_ _M0, the following bound holds with probability at least ≤_ _∈_ _∈_ 1 − _δ over_
_E_ _gΘ(T1)_ _ϕ_ _L[2](ρX_ )
_∥_ _−_ _∥[2]_
_ε + c0λ[2]1[r][0]_ Σ[−][r][0] _L[2](ρX_ ) [+][ c][1][λ]1[2][r][1] Σ[−][r][1] _L[2](ρX_ )
_≤_ _∥_ _∞_ _[φ][0][∥][2]_ _∥_ _∞_ _[φ][1][∥][2]_

_c2_
+ _T1 + 1_ _λ[−]1_ [1][E][∥]φ[b]0 − _φ0∥L[2]_ [2](ρX ) 1 + ∥Σ[−]∞[r][0] _[φ][0][∥]L[2]_ [2](ρX ) + λ[2]1[r][1][−][1]∥φ0∥L[2] [2](ρX ) [+][ ∥][φ][1][∥][2]H∞

_c3_   
+ (T1 + 1)[2]η1[2] _λ[−]1_ [2][E][∥]φ[b]0 − _φ0∥L[2]_ [2](ρX ) 1 + ∥Σ[−]∞[r][0] _[φ][0][∥]L[2]_ [2](ρX ) + λ[2]1[r][1][−][2]∥φ0∥L[2] [2](ρX ) [+][ λ]1[−][1][∥][φ][1][∥]H[2] _∞_

_c4_    
+ 1 + _ϕ_ _L[2](ρX_ ) Tr Σ (Σ + λ1I)[−][1][], (14)

_T1 + 1_ _∥_ _∥H[2]_ _∞_ [+ 24][∥][Σ][−]∞[r][0] _[ϕ][∥][2]_ _∞_ _∞_
   


-----

_where_ _φ0 is the result of pre-training and ci (i = 0, 1, 2, 3, 4) are universal constants._

The term ε is arbitrarily small for a large value of M, i.e., wide network. The proof of Theorem 2

[b]

will be given in Section E.6.

E.5 ANALYSIS OF CONVERGENCE RATES

We consider the rates of the generalization bound for E _gΘ(T1)_ _ϕ_ _L[2](ρX_ ) [with respect to][ T][1][. As]
_∥_ _−_ _∥[2]_
vary according to the configurations oftypical cases, we assume λ1 → 0 and η λ11 = and O η(1)1 with respect to as T1 →∞. The dominant terms in Eq. (14) may T1. We will show the rates in some
settings that are relevant to transfer learning.

First, note that under Assumption (A4), the factor tr[Σ∞(Σ∞ + λ1I)[−][1]] is given by (Caponnetto &
De Vito, 2007)
Tr Σ∞(Σ∞ + λ1I)[−][1][] = O(λ[−]1 [1][/ξ]). (15)

By neglecting ε, the terms in Eq. (14) thus have the following rates:

(a0) λ[2]1[r][0] _,_ (a1) λ[2]1[r][1] _,_ (b) T1[−][1]λ[−]1 [1][R][0][,] (c) T1[−][1]λ[2]1[r][1][−][1], (d) T1[−][1],

(e) T1[−][2]η1[−][2][λ]1[−][2][R][0][,] (f ) T1[−][2]η1[−][2][λ]1[2][r][1][−][2] (g) T1[−][2]η1[−][2][λ]1[−][1][,] (h) T1[−][1]λ[−]1 [1][/ξ]. (16)

Here R0 := E∥φ[b]0 − _φ0∥L[2]_ [2](ρX ) [is of constant rate with respect to][ T][1][, but explicitly shown for the]
later use.

Since λ1 0 and r1 1/2, the terms (c) and (d) are of smaller rate than (b). Likewise, (f) and (g)
are smaller than (e). The candidates of dominant terms are thus (a0), (a1), (b), (e), (g), and (h). → _≥_

E.5.1 LARGE REGULARIZATION COEFFICIENT

In transfer learning, it is reasonable to use strong regularization in fine-tuning, which encourages the
parameters to stay close to the initial value that is obtained in the pre-training. In this subsection, we


consider the case where λ1 is larger than T


2r1 _ξ+1_, which would be the optimal rate if the network


_−_ 2r1 _ξ+1_
was trained with random initialization (see Nitanda & Suzuki (2021)). If λ1 = T1 was taken,

it is easy to see that the influence of pre-training would not appear explicitly in the convergence rate.
In the sequel, we write F _G(T1) if there are a > 0 and T_ such that F _aG(T1) for all T1_ _T_ .
In this notation, we assume ≪ _[∗]_ _≤_ _≥_ _[∗]_

_ξ_
_−_ 2r1 _ξ+1_
_λ1_ _T1_ _._ (17)

_ξ_ _≫_
_−_ 2r1 _ξ+1_
The rate λ1 = T1 is given by equating the rates of (a1) and (h). Therefore, under the

assumption of Eq. (17), the rate (a1) is larger than (h), and thus it suffices to consider (a0), (a1), (b)
and (e) as the candidates of dominant terms. Note that, if λ1 0, the terms (a0) and (a1) decrease,
while (b) and (e) increase to infinity. _→_

We will discuss below the possible cases of dominant terms under the assumption Eq. (17). In
the analysis, although the error of the pre-training R0 is regarded as a constant, we yet wish to
consider the dependence of the fine-tuning result on R0. We thus set the regularization coefficient λ1
dependent on R0, and show that in all the cases, the generalization bound takes the form

_E_ _gΘ(T1)_ _ϕ_ _L[2](ρX_ ) 0 _[T][ −]1_ _[β],_ (18)
_∥_ _−_ _∥[2]_ _[≤]_ _[ε][ +][ CR][ν]_

where ν > 0 is a constant. As R0 _c[′]_ + A[′]T0[−][α][′] from Eq. (13), the factor R0[α] [can be bounded from]
above as _≤_
_R0[ν]_ 0

_[≤]_ _[c][ +][ AT][ −][α][′][ν]_
for large T0. As a result, we obtain


_E∥gΘ(T1) −_ _ϕ∥L[2]_ [2](ρX ) _[≤]_ _[ε][ +][ C][(][c][ +][ AT][ −]0_ _[α])T1[−][β],_ (19)


-----

where C, c, A are constants. As we will see, the exponents α and β depend on r0, r1, ξ and η1.
Eq. (19) accords with the bound in Theorem 1.

In the sequel, we use ζ ≥ 0 for the learning rate such that η1 = T1[−][ζ].

**Case I: Small learning rate T1λ1η1[2]** In this case, (e) (b). We equate (a0) or (a1) with (e)
to obtain λ1 for achieving the best possible upper bound of the two terms.[≪] [1][.] _≫_

**(I-A)that the best choice of r0 ≥** _r1. Since λ λ11 → is_ 0 for T1 →∞, (a1) is larger than (a0). By equating (a1) and (e), we find

_−_ _r[1]1+1[−][ζ]_
_λ1 = O_ _T1_ _._

We further consider

  

_−_ _r[1]1+1[−][ζ]_
_λ1 = T1_ _R0[ν]_

for dependence on R0. To determine ν, we assume that R0 is a small value, and consider the
rate of (a1) and (e) with respect to R0 after plugging the above λ1 to them. By equating the

_−_ [2][r]r[1(1]1+1[−][ζ][)] _−_ [2][r]r[1(1]1+1[−][ζ][)]
rates of (a1) T1 _R0[2][r][1][ν]_ and (e) T1 _R0[1][−][2][ν], the best possible rate of R0 is attained by_

_ν = 1/(2r1 + 2). The dominant rate of Eq. (16) is thus_


_−_ [2][r]r[1(1]1+1[−][ζ][)]
_T1_

_−_ _r[1]1+1[−][ζ]_
_λ1 = T1_


_r1_

0r1+1 (20)

1

02(r1+1) (21)


attained by


We need to identify the conditions on ζ1 to meet the requirements. The condition λ1 0 is equivalent

_ξ_
_−_ 2r1 _ξ+1_ _→_
to ζ < 1. There are two other conditions: λ1 _T1_ and T1λ1η1[2] [is of constant]

_ξ_ _≫_ _[≪]_ [1][. Given][ R][0]
rate, the former is equivalent to − 2r1+1 _[≤−]_ 1+[1][−]r[ζ]1 [, which results in]

_ζ_
_≥_ _[r][1]2[ξ]r[ + 1]1ξ + 1[ −]_ _[ξ]_ _[.]_

The latter condition is equivalent to 1 2ζ _r1+1_
_−_ _−_ [1][−][ζ] _[≤]_ [0][, which is]

_r1_
_ζ_
_≥_ 2r1 + 1 _[.]_


It is not difficult to see
_r1_

2r1 + 1 _[> r][1]2[ξ]r[ + 1]1ξ + 1[ −]_ _[ξ]_

for ξ > 1. As a result, the condition on ζ is
_r1_ (22)

2r1 + 1

_[≤]_ _[ζ <][ 1][.]_

If η1 = T1[−][ζ] is taken to satisfy this condition, the optimal rate of λ1 is given by Eq. (21). Finally, the
resulting generalization bound is given by


_E_ _gΘ(T1)_ _ϕ_ _L[2](ρX_ ) 1− [2][r]r[1(1]1+1[−][ζ][)]
_∥_ _−_ _∥[2]_ _[≤]_ _[ε][ +][ cT]_


_r1_

0r1+1 _._ (23)


**(I-B) r1 > r0: In this case, (a0) is of larger rate than (a1). By a similar argument to (I-A), with the**
rate

1

_−_ _r[1]0+1[−][ζ]_ 2r0+2
_λ1 = T1_ _R0_ _,_ (24)

The generalization bound is given by


_E_ _gΘ(T1)_ _ϕ_ _L[2](ρX_ ) 1− [2][r]r[0(1]0+1[−][ζ][)]
_∥_ _−_ _∥[2]_ _[≤]_ _[ε][ +][ cT]_


_r0_

0r0+1 _._ (25)


The condition on ζ is


_r0_
max

2r0 + 1 _[,][ (2][r][1][ −]2r[r]1[0]ξ[)] + 1[ξ][ + 1][ −]_ _[ξ]_

n


_≤_ _ζ < 1._ (26)


-----

|r, r 0 1|η = T −ζ 1|Bound|
|---|---|---|
|r r 0 ≥ 1 r < r 0 1 r r 0 ≥ 1 r < r ≤r + ξ 2− ξ1 0 1 0|2rr 11 ≤ζ < 1 +1 n o max 2rr 00 +1, (2r1− 2rr 10 ξ) +ξ+ 11−ξ ≤ζ < 1 ζ ≤ 2rr 11 +1 ζ ≤ 2rr 00 +1|c′ T−2r2 0r ξ0 +ξ ·r1r +1 1 T−2r r1 1(1 +− 1ζ) ε + c + 1 0 1 c′ T−2r2 0r ξ0 +ξ ·r0r +0 1 T−2r r0 0(1 +− 1ζ) ε + c + 1 0 1 c′ T−2r2 0r ξ0 +ξ ·2r2 1r +1 1 T−2r2 1r +1 ε + c + 1 1 0 1 c′ T−2r2 0r ξ0 +ξ ·2r2 0r +0 1 T−2r2 0r +0 ε + c + 1 1 0 1|


Table 1: Generalization bounds in various conditions.


2r0 2r0

2r0+1 _−_ 2r0+1


**(Case II): large learning rate T1λ1η1[2]** Next, we consider the case where the learning rate η1
is large so that T1λ1η1[2] _[≫]_ [1][.]
rate than (e). _[≫]_ [1][, which includes the constant][ η][1][. Under this condition, (b) is of larger]

**(II-A) r0 ≥** _r1. In this case, (a1) is of larger rate than (a0). A similar argument to (I-A) provides_


2r1+1


02r1+1 _,_ (27)


_λ1 = T_

and the generalization bound is given by


2r1
_E_ _gΘ(T1)_ _φ0_ _L[2](ρX_ ) 1− 2r1+1
_∥_ _−_ _∥[2]_ _[≤]_ _[ε][ +][ cT]_


2r1

02r1+1 _._ (28)


_−_ 2r1 _ξ+1_
The conditions are λ1 _T1_ and T1λ1η1[2]

and the latter is equivalent to ≫ _ζ_ 2rr11+1 [. The resulting condition on][≫] [1][. The former condition always holds for][ ζ][ is] _[ ξ >][ 1][,]_
_≤_

_r1_
0 < ζ (29)
_≤_ 2r1 + 1 _[.]_


**(II-B) r1 > r0: In this case, (a0) is of larger rate. With**


2r0+1


02r0+1 _,_ (30)


_λ1 = T_


the generalization bound is given by

2r0
_E_ _gΘ(T1)_ _φ0_ _L[2](ρX_ ) 1− 2r0+1
_∥_ _−_ _∥[2]_ _[≤]_ _[ε][ +][ cT]_


2r0

02r0+1 _._ (31)


_−_ 2r1 _ξ+1_ _r0_
The conditions λ1 ≫ _T1_ and T1λ1η1[2] _[≫]_ [1][ are respectively][ r][1] _[≤]_ _[r][0]_ [+][ ξ]2[−]ξ[1] [and][ ζ][ ≤] 2r0+1 [.]

Thus, we require

_r0_
0 < ζ _r0 < r1_ _r0 +_ _[ξ][ −]_ [1] (32)
_≤_ 2r0 + 1 _[,]_ _≤_ 2ξ [,]


In summary, the generalization bounds in various conditions are summarized in Table 1.

E.6 PROOF OF THEOREM 2

The proof of Theorem 2 is based on the application of the theory in Nitanda & Suzuki (2021) to the
fine-tuning phase, adapting the initialization given by the result of pre-training _φ0._

In the sequel, we focus on the fine-tuning with T1 samples with yt = ϕ(xt). Recall that

[b]

_ϕ(x) = φ0(x) + φ1(x)._


-----

E.6.1 REFERENCE ASGD ON RKHS

We use a surrogate sequence of functions in an RKHS for the proof. Let kM be the random feature
approximation of the TNK k, i.e.,
_∞_


_M_

_σ(b[T]r_ _[x][)][σ][(][b]r[T]_ _[x][′][) +][ x][T][ x][′]_

_M_

_r=1_

X


_kM_ (x, x[′]) = [1]


_σ[′](b[T]r_ _[x][)][σ][′][(][b]r[T]_ _[x][′][)][,]_ (33)
_r=1_

X


where (br)[M]r=1 [is i.i.d. random sample from the uniform distribution on the unit sphere][ S][d][−][1][. The]
associated RKHS is denoted by _M_ .
_H_

A reference ASGD is defined by the following update rule of functions in the RKHS _M_ :
_H_

_g[(][t][+1)]_ = (1 _ηλ)g[(][t][)]_ _η(g[(][t][)](xt)_ _yt)kM_ ( _, xt),_ (t = 0, . . ., T 1) (34)
_−_ _−_ _−_ _·_ _−_

with the initialization given by g[(0)] := _φ0. The average is taken at the final step:_

1 _T1_
_g¯[(][T][b][1][)]_ := _g[(][t][)]._ (35)

_T1 + 1_ _t=0_

X

By considering continual learning of pre-training and fine-tune, a slight modification of (Nitanda &
Suzuki, 2021, Propososion A) derives the following proposition.
**Proposition 3.there is M** = M Assume (A1) and (A2). Suppose that(T1, ε) N such that during the fine-tuning learning η1λ1 < 1. Then for any T1 ∈ N and ε > 0,
_∗_ _∗_ _∈_

_g¯[(][t][)]_ _gΘ(t)_ _L∞(ρX_ ) _ε,_ (36)
_∥_ _−_ _∥_ _≤_

_holds for any M ≥_ _M∗_ _and 0 ≤_ _t ≤_ _T1._

This proposition shows that, if we use a very wide network, the learning of ASGD in the parameter
space can be approximated by the reference ASGD on the RKHS with negligible error.

The generalization bound will be given by the following decomposition:

_gΘ(T1)_ _ϕ_ _L[2](ρX_ ) Θ(T1) _g¯[(][T][1][)]_ _L[2](ρX_ ) [+ 2][∥]g[¯][(][T][1][)] _ϕ_ _L[2](ρX_ )[,] (37)
_∥_ _−_ _∥[2]_ _[≤]_ [2][∥][g] _−_ _∥[2]_ _−_ _∥[2]_

in which the first term of the right hand side is bounded by Proposition 3 with an arbitrary small value
_ε for large M_ . The second term will be discussed in the next subsection.

E.6.2 CONVERGENCE RATES OF REFERENCE ASGD

In this section, we write λ and η for λ1 and η1 for simplicity. The covariance operators Σ and ΣM
_∞_
for H∞ and HM, respectively, are defined by

Σ∞ := EρX [k∞(·, X) ⊗ _k∞(·, X)[∗]],_
ΣM := EρX [kM (·, X) ⊗ _kM_ (·, X)[∗]], (38)

where ∗ denotes the adjoint; equivalently,

Σ∞f = _k∞(·, x)f_ (x)dρX (x), ΣM _h =_ _kM_ (·, x)h(x)dρX (x),
Z Z

for f ∈H∞, h ∈HM . The regularized target functions φ[(]M,λ[i][)] [(][i][ = 0][,][ 1][) are defined by]

_φ[(]M,λ[i][)]_ [:= (Σ][M][ +][ λI][)][−][1][Σ][M] _[φ][i]_ (i = 0, 1). (39)

_φ[(][i][)],λ_ [is defined similarly with][ Σ][∞][. Note that][ ϕ][M,λ][ := (Σ][M][ +][ λI][)][−][1][Σ][M] _[ϕ][ =][ φ]M,λ[(0)]_ [+][ φ]M,λ[(1)] [.]
_∞_

First, we decompose ∥g¯[(][T][1][)] _−_ _ϕ∥L[2]_ [2](ρX ) [by]

_∥g¯[(][T][1][)]_ _−_ _ϕ∥L[2]_ [2](ρX ) [=][ ∥]g[¯][(][T][1][)] _−_ _ϕM,λ + φ[(0)]M,λ_ [+][ φ]M,λ[(1)] _[−]_ _[φ][0][ −]_ _[φ][1][∥]L[2]_ [2](ρX )

_≤_ 3∥g¯[(][T][1][)] _−_ _ϕM,λ∥L[2]_ [2](ρX ) [+ 3][∥][φ]M,λ[(0)] _[−]_ _[φ][0][∥]L[2]_ [2](ρX ) [+ 3][∥][φ]M,λ[(1)] _[−]_ _[φ][1][∥]L[2]_ [2](ρX )[.]


-----

The second and third terms are known to have a bound, with high probability, (Nitanda & Suzuki,
2021, Propositions C and D)

_∥φM,λ[(][i][)]_ _[−]_ _[φ][i][∥]L[2]_ [2](ρX ) _[≤]_ _[ε][ +][ λ][2][r][i]_ _[∥][Σ]∞[−][r][i]_ _[φ][i][∥]L[2]_ [2](ρX )[,] (i = 0, 1), (40)

where ε is arbitrarily small for large M . We have thus, with high probability,

_∥g¯[(][T][1][)]_ _−ϕ∥L[2]_ [2](ρX ) _[≤]_ _[ε][+3][∥]g[¯][(][T][1][)]_ _−ϕM,λ∥L[2]_ [2](ρX ) [+3][λ][2][r][0] _[∥][Σ][−]∞[r][0]_ _[φ][0][∥]L[2]_ [2](ρX ) [+3][λ][2][r][1] _[∥][Σ][−]∞[r][1]_ _[φ][1][∥]L[2]_ [2](ρ(41)X )[.]

As shown in Nitanda & Suzuki (2021), the term ∥g¯[(][T][1][)] _−_ _ϕM,λ∥L[2]_ [2](ρX ) [can be analyzed by the bias]
and noise terms of the stochastic recursion on RKHS Eq. (34), which is rewritten as

_g[(][t][+1)]_ = (I _ηHt_ _ηλI)g[(][t][)]_ + ηytkM ( _, xt),_ (42)
_−_ _−_ _·_

where
_Ht := kM_ (·, xt) ⊗ _kM_ (·, xt)[∗],
is a one-sample estimate of ΣM . By subtracting ϕM,λ from both hand sides of Eq. (42), we have

_g[(][t][+1)]_ _−_ _ϕM,λ = (I −_ _ηHt −_ _ηλI)(g[(][t][)]_ _−_ _ϕM,λ) + βt,_ (43)

where
_βt = ηytkM_ (·, xt) − _η(Ht + λI)(ΣM + λI)[−][1]ΣM_ _ϕM,λ_
is the zero mean noise term. Using this recursive formula, Nitanda & Suzuki (2021) derives a bound:

_c1_
_∥g¯[(][T][ )]_ _−_ _ϕM,λ∥L[2]_ [2](ρX ) _[≤]_ _T1 + 1_ _[∥][(Σ][M][ +][ λI][)][−][1][/][2][(][g][(0)][ −]_ _[ϕ][M,λ][)][∥]L[2]_ [2](ρX )

_c2_
+ (T1 + 1)[2]η[2][ ∥][(Σ][M][ +][ λI][)][−][1][(][g][(0)][ −] _[ϕ][M,λ][)][∥]L[2]_ [2](ρX )

_c3_
+ 1 + _ϕ_ _L[2](ρX_ ) [+ 24][∥][Σ][−][r][1] _L[2](ρX_ ) Tr[ΣM (ΣM + λI)[−][1]].

_T1 + 1_ _∥_ _∥[2]_ _∞_ _[ϕ][∥][2]_
   (44)


To bound this expression further, we use Proposition B in Nitanda & Suzuki (2021)

_∥(ΣM + λI)[−][1][/][2]φ[(]M,λ[i][)]_ _[∥]L[2]_ [2](ρX ) _[≤]_ [2][∥][φ][i][∥]H[2] _∞_ _[.]_ (45)

Then, using the decomposition g[(0)] _−_ _ϕM,λ = (φ[ˆ]0 −_ _φ0) + (φ0 −_ _φ[(0)]M,λ[)][ −]_ _[φ][(1)]M,λ[, we obtain that,]_
with high probability,

(ΣM + λI)[−][1][/][2](g[(0)] _ϕM,λ)_ _L[2](ρX_ )
_∥_ _−_ _∥[2]_

_≤_ 3∥(ΣM + λI)[−][1][/][2](φ[ˆ]0 − _φ0)∥L[2]_ [2](ρX ) [+ 3][∥][(Σ][M][ +][ λI][)][−][1][/][2][(][φ][0][ −] _[φ]M,λ[(0)]_ [)][∥]L[2] [2](ρX )

+ 3 (ΣM + λI)[−][1][/][2]φ[(1)]M,λ[∥]L[2] [2](ρX )
_∥_

_≤_ _λ[3]_ _[∥]φ[ˆ]0 −_ _φ0∥L[2]_ [2](ρX ) [+ 3]λ _[λ][2][r][0]_ _[∥][φ][0][∥]L[2]_ [2](ρX ) [+][ ε][M][ + 6][∥][φ][1][∥][2]H∞ _[,]_ (46)

where we use Eq. (40) for the second and third terms and Eq. (45) for the fourth term in the last
inequality. Similarly, with high probability, we have

(ΣM + λI)[−][1](g[(0)] _ϕM,λ)_ _L[2](ρX_ )
_∥_ _−_ _∥[2]_

_≤_ 3∥(ΣM + λI)[−][1](φ[ˆ]0 − _φ0)∥L[2]_ [2](ρX ) [+ 4][∥][(Σ][M][ +][ λI][)][−][1][(][φ][0][ −] _[φ]M,λ[(0)]_ [)][∥]L[2] [2](ρX )

+ 3 (ΣM + λI)[−][1]φ[(1)]M,λ[∥]L[2] [2](ρX )
_∥_

_≤_ _λ[3][2][ ∥]φ[ˆ]0 −_ _φ0∥L[2]_ [2](ρX ) [+ 3]λ[2][ λ][2][r][0] _[∥][φ][0][∥]L[2]_ [2](ρX ) [+][ ε][M][ + 6]λ _[∥][φ][1][∥]H[2]_ _∞_ _[.]_ (47)

It is also known (Nitanda & Suzuki, 2021, Proposition B) that, for λ Σ,
_≤∥_ _∞∥_

Tr[ΣM (ΣM + λI)[−][1]] 3Tr[Σ (Σ + λI)[−][1]]. (48)
_≤_ _∞_ _∞_

Combining Eqs.(41), (44), (46), (47), and (48), we obtain the assertion of the theorem.


-----

