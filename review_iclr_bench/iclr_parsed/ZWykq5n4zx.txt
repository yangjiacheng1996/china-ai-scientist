# BOOSTING THE CONFIDENCE OF NEAR-TIGHT GEN## ERALIZATION BOUNDS FOR UNIFORMLY STABLE
# RANDOMIZED ALGORITHMS

**Anonymous authors**
Paper under double-blind review

ABSTRACT

High probability generalization bounds of uniformly stable learning algorithms
have recently been actively studied with a series of near-tight results established
by Feldman & Vondrak (2019); Bousquet et al. (2020). However, for randomized
algorithms with on-average uniform stability, such as stochastic gradient descent
(SGD) with time decaying learning rates, it still remains less well understood if
these deviation bounds still hold with high confidence over the internal randomness of algorithm. This paper addresses this open question and makes progress towards answering it inside a classic framework of confidence-boosting. To this end,
we first establish an in-expectation first moment generalization error bound for
randomized learning algorithm with on-average uniform stability, based on which
we then show that a properly designed subbagging process leads to near-tight high
probability generalization bounds over the randomness of data and algorithm. We
further substantialize these generic results to SGD to derive improved high probability generalization bounds for convex or non-convex optimization with natural
time decaying learning rates, which have not been possible to prove with the existing uniform stability results. Specially for deterministic uniformly stable algorithms, our confidence-boosting results improve upon the best known generalization bounds in terms of a logarithmic factor on sample size, which moves a step
forward towards resolving an open question raised by Bousquet et al. (2020).

1 INTRODUCTION

In many statistical machine learning problems, the ultimate goal is to design a suitable algorithm
_WA : of an Euclidean space such that the following population risk function evaluated at the model is Z_ _[N]_ _7→W that maps a training data set S = {zi}i∈[N_ ] ∈Z _[N]_ to a model A(S) in a closed subset
as small as possible:
_R(A(S)) := EZ_ [ℓ(A(S); Z)],
_∼D_
where ℓ : W × Z 7→ R[+] is a non-negative bounded loss function whose value ℓ(w; z) measures
the loss evaluated at z with parameter w, and D represents a distribution over Z. It is generally the
case that the underlying data distribution is unknown, and in this case the training data is usually
i.i.d.
assumed to be an i.i.d. set, i.e., S _∼D[N]_ . Then, a natural alternative measurement that mimics the
computationally intractable population risk is the empirical risk defined by


_N_

_RS(A(S)) := EZ_ Unif(S)[ℓ(A(S); Z)] = [1] _ℓ(A(S); zi)._
_∼_ _N_

_i=1_

X

The bound on the difference between population and empirical risks is of central interest in understanding the generalization performance of learning algorithm A. Particularly, we hope to derive
a suitable law of large numbers, i.e., a sample size vanishing rate bN such that the generalization
bound |RS(A(S)) − _R(A(S))| ≲_ _bN holds with high probability over the randomness of S and po-_
tentially the randomness of A as well. Provided that A(S) is an almost minimizer of the empirical
risk function RS, say RS(A(S)) ≲ minw _RS(w) + bN_, the generalization bound immediately
implies an excess risk bound R(A(S)) − min∈W _w∈W R(w) ≲_ _bN +_ _√1N_ [due to the standard risk de-]

composition and Hoeffding’s inequality. Therefore, generalization bounds also play a crucial role in
understanding the stochastic optimization performance of a learning algorithm.


-----

The generalization bounds can be naturally implied by uniform bounds onRS(w) (Bartlett et al., 2006; Bottou & Bousquet, 2008). While broadly applicable (e.g., to non- supw∈W |R(w) −
_|_
convex problems) and leading to tight generalization in some specific regimes (e.g., margin-based
learning (Kakade et al., 2009)), uniform convergence bounds in general cases might suffer from the
polynomial dependence on dimensionality and thus are not suitable for high-dimensional models
which are ubiquitous in modern machine learning. Alternatively, a powerful proxy for analyzing the
generalization bounds is the stability of learning algorithms to changes in the training dataset. Since
the seminal work of Bousquet & Elisseeff (2002), stability has been extensively demonstrated to
beget dimension-independent generalization bounds for deterministic learning algorithms (Mukherjee et al., 2006; Shalev-Shwartz et al., 2010), as well as for randomized learning algorithms (such
as bagging and SGD) (Elisseeff et al., 2005; Hardt et al., 2016). So far, the best known results about
generalization bounds are offered by approaches based on the notion of uniform stability (Feldman
& Vondrak, 2018; 2019; Bousquet et al., 2020). Inspired by these recent breakthrough results, we
seek to derive sharper high-probability generalization bounds for randomized learning algorithms
with on-average uniform stability. A concrete working example of our study is the widely used
stochastic gradient descent (SGD) algorithm that carries out the following recursion for all t ≥ 1
with learning rate ηt > 0:
_wt := Π_ (wt 1 _ηt_ _wℓ(wt_ 1; zξt )), (1)
_W_ _−_ _−_ _∇_ _−_
where ξt [N ] is a uniform random index of data under with or without replacement sampling,
and Π is the Euclidean projection operator associated with ∈ . Despite its popularity in the study
_W_ _W_
of stability theory (Hardt et al., 2016; Zhou et al., 2019; Lei & Ying, 2020), the high probability
generalization bounds of SGD are still relatively under explored through the lens of uniform stability.

1.1 PRIOR RESULTS

Let us now briefly review some state-of-the-art generalization bounds for uniformly stable algorithms and their randomized variants. We denote by S =[.] _S[′]_ if a pair of data sets S and S[′] differ
in a single data point. A randomized learning algorithm A is said to have on-average γN -uniform
stability if it satisfies the following uniform bound
_S=[.]_ supS[′],z _|EA[ℓ(A(S); z)] −_ EA[ℓ(A(S[′]); z)]| ≤ _γN_ _._
_∈Z_

This definition is equivalent to the concept of uniform stability defined over the on-average loss
EA[ℓ(A(S); z)]. Suppose that the loss function is Lipschitz and bounded in the interval (0, 1]. Then
essentially it has been shown in Feldman & Vondrak (2019) that for any δ ∈ (0, 1), with probability
at least 1 − _δ over S, the on-average generalization error is upper bounded by_

_N_ log (1/δ)
EA [R(A(S)) _RS(A(S))]_ ≲ _γN log(N_ ) log + _._ (2)
_|_ _−_ _|_ _δ_ _N_
  r

Recently, Bousquet et al. (2020) derived a slightly improved uniform stability bound that implies

1 log (1/δ)
EA [R(A(S)) _RS(A(S))]_ ≲ _γN log(N_ ) log + _._ (3)
_|_ _−_ _|_ _δ_ _N_
  r

These generalization bounds are near-tight (up to a logarithmic factor log(N )) in the sense of a lower
bound on sum of random functions provided in that paper. While sharp in the dependence on sample
size, one common limitation of the above uniform stability implied generalization bounds lies in
that these high-probability results only hold in expectation with respect to the internal randomness
of algorithm.

Further suppose that A has γN -uniform stability with probability at least 1 _δ[′]_ for some δ[′] (0, 1)
_−_ _∈_
over the randomness of A, i.e.,

PA (S=[.] supS[′],z _|ℓ(A(S); z) −_ _ℓ(A(S[′]); z)| ≤_ _γN_ ) _≥_ 1 − _δ[′]._ (4)

_∈Z_

Suppose that the randomness of A is independent of the training set S. Then with probability at
least 1 − _δ −_ _δ[′]_ over S and A, the bound of Bousquet et al. (2020) naturally implies

1 log (1/δ)
_R(A(S))_ _RS(A(S))_ ≲ _γN log(N_ ) log + _._ (5)
_|_ _−_ _|_ _δ_ _N_
  r


-----

**Algorithm 1: Confidence-Boosting for Randomized Learning**

i.i.d.
**Input : A randomized learning algorithm A and a training data set S = {zi}i∈[N** ] _∼D[N]_ .
**Output: Ak∗** (Sk∗ ).
Uniformly divide S into K disjoint subsets such that S = _k_ [K] _[S][k][ and][ |][S][k][|][ =][ N]K_ [,][ ∀][k][ ∈] [[][K][]][.]

_∈_

**for k = 1, 2, ..., K do**

Estimate Ak(Sk) as an output of the randomized algorithm[S] _A over subset Sk._
**end**
Compute k[∗] = arg mink∈[K] _RS\Sk_ (Ak(Sk)) − _RSk_ (Ak(Sk)) .


This is by far the best known generalization bound of randomized stable algorithms that hold with
high probability jointly over data and algorithm. The result, however, relies heavily on the highprobability uniform stability condition expressed in equation 4. For the SGD (see equation 1) with
fixed learning ratein equation 4 (Bassily et al., 2020). For SGD with time decaying learning rate that has been widely ηt ≡ _η, it is possible to show that γN ≲_ _η√T +_ _[ηT]N_ [and][ δ][′][ =][ N][ exp(][−] _[N]2_ [)]

studied in theory Harvey et al. (2019); Rakhlin et al. (2012) and applied in practice for training
popular deep nets such as ResNet and DenseNet Bengio et al. (2017), it is not clear if the condition
in equation 4 is still valid for γN and δ[′] of interest. On the other hand, it is possible to show (see
the proofs of Corollary 1 and 2) that SGD with time decaying learning rate has desirable on-average
uniform stability.

More specially for randomized learning methods such as bagging (Breiman, 1996) and SGD equation 1, the randomness of algorithm can be precisely characterized by a vector of i.i.d. parameters
_ξ =_ _ξ1, ..., ξT_ which are independent on data S. In such cases, suppose that A(S; ξ) has uniform
stability with respect to { _}_ _ξ at any given S, i.e., supξ=[.]_ _ξ[′][ |][ℓ][(][A][(][S][;][ ξ][))][ −]_ _[ℓ][(][A][(][S][;][ ξ][′][))][| ≤]_ _[ρ][T][ . Then the]_
high probability bound established in Elisseeff et al. (2005) shows that with probability at least 1−δ,

1 + NγN 1
_|R(A(S)) −_ _RS(A(S))| ≲_ _γN +_ _√N_ + _√TρT_ log _δ_ _._ (6)
  [s]  

Provided that γN ≲ _N1_ [and][ ρ][T][ ≲] _T1_ [, the above bound shows that the generalization bound scales]

as O( _√1N_ [+] _√1T_ [)][ with high probability. However, the above bound will show no guarantee on]

convergence if γN ≳ _√1N_ [and/or][ ρ][T][ ≳] _√1T_ [. For example, this is actually the case for SGD with]

time decaying learning rate ηt = O( [1]t [)][ on non-convex loss functions in which][ γ][N][ ≲] _√NT_ [and][ ρ][T]

can scale as large as O(1).

**Open problem and motication. Keeping the merits and deficiencies of above recalled prior results**
in mind, it still remains an open issue if the existing deviation bounds can hold with high confidence
for randomized algorithms with on-average uniform stability (such as SGD with decaying learning
rates). The goal of the present study is to derive sharper high-probability generalization bounds for
randomized algorithms that hold jointly over the randomness of data and algorithm, based on the
relatively weaker notion of on-average uniform stability rather than its high probability counterpart.

1.2 OVERVIEW OF OUR RESULTS

The confidence-boosting technique of Schapire (1990) is a classical meta approach that allows us
to boost the dependence of a learning algorithm on the failure probability δ from 1/δ to log(1/δ),
at a certain cost of computational complexity. The fundamental contribution of our work is to
reveal that the confidence-boosting trick yields near-tight high probability generalization bounds
for uniformly stable randomized learning algorithms. The novelty lies in a refined analysis of the
in-expectation first moment generalization error bound for a randomized learning algorithm with onaverage uniform stability, which leads to improved high-probability generalization bounds over the
randomness of data and algorithm via confidence-boosting. More specifically, given a randomized
learning algorithm A, we consider the following subbagging process over training set:

**Boosting the Confidence via Subbagging.** We independently run A over K disjoint and uniformly divided training subsets _Sk_ _k_ [K] to obtain solutions _Ak(Sk)_ _k_ [K]. Then we e_{_ _}_ _∈_ _{_ _}_ _∈_


-----

valuate the validation error of each candidate solution over its complementary training subset,
and output Ak∗ (Sk∗ ) that has the smallest gap between training error and validation error, i.e.,
_k[∗]_ = arg mink∈[K] _RS\Sk_ (Ak(Sk)) − _RSk_ (Ak(Sk)) . Specially when A is deterministic, this reduces to a standard subbagging process, namely a variation of bagging using without-replacement
sampling for subsets generation (see, e.g., Andonova et al., 2002). In general, this is essentially a
subbagging procedure with greedy model selection for randomized algorithms over multiple disjoint subsets. Here we have assumed without loss of generality that N is a multiplier of K. The
considered procedure of confidence-boosting for randomized learning is outlined in Algorithm 1.

**Main Results. In what follows, we highlight our main results on the generalization bounds of the**
output of Algorithm 1 along with the implications for SGD and deterministic algorithms:

_• General results. Suppose that the loss is Lipschitz and bounded in the range of (0, 1]. Our main_
result in Theorem 1 show that for any δ ∈ (0, 1), setting K ≍ log( [1]δ [)][ yields the following]

generalization bound of the output of Algorithm 1 that holds with probability at least 1 − _δ over_
_S and {Ak}k∈[K]:_


_K_ log(K/δ)

_|R(Ak∗_ (Sk∗ )) − _RS(Ak∗_ (Sk∗ ))| ≲ _K[1]_ _γm2, NK_ [+][ γ][m][,][ N]K [+] r _N_ ! + r _N_ _,_

where γm,N and γm2,N are respectively mean-uniform stability and mean-square-uniform stabilityp
bounds introduced in Definition 1. In contrast to the bound in equation 6, our bound is not relying
on the uniform stability with respect to the random bits of algorithm.



_• Stronger generalization bounds for SGD via confidence-boosting. We then use our general result-_
s to study the benefit of confidence-boosting on the generalization bounds of SGD-w (SGD via
with-replacement sampling as outlined in Algorithm 2). The main results are a series of corollaries of Theorem 1 when substantialized to SGD with smooth (Corollary 1) or non-smooth (Corollary 2) convex loss, and smooth non-convex loss functions (Corollary 3). For an instance, our
result in Corollary 1 showcases that when invoked to SGD-w on smooth convex loss with learning rates ηt = O( _√[1]t_ [)][, the generalization bound of the output of Algorithm 1 with][ K][ ≍] [log(][ 1]δ [)][ is]

upper bounded by


log(T )


_T +_


_N log(1/δ)_


_|R(ASGD-w,k∗_ (Sk∗ )) − _RS(ASGD-w,k∗_ (Sk∗ ))| ≲


Compared with the O( _√NT_ [)][ in-expectation bound of smooth convex SGD (Hardt et al., 2016), our]

above bound is competitive in order while it holds with high confidence.

_• Sharper bounds for uniformly stable algorithms via confidence-boosting. Specially for a deter-_
ministic learning algorithm A that has γN -uniform stability for each data set size N 1, we
_≥_
further show through Corollary 4 that the following bound holds for the output of Algorithm 1
with K = log( [1]δ [)][:]


log(K/δ)

_|R(A(Sk∗_ )) − _RS(A(Sk∗_ ))| ≲ _γ NK_ [+] _N_ _._

r

In the case of γN ≲ _√1N_ [which holds in some popular learning paradigms such as regularized]

ERM, the above bound implies (recall K ≍ log( [1]δ [)][)]

log(1/δ)

_R(A(Sk∗_ )) _RS(A(Sk∗_ )) ≲ _,_
_|_ _−_ _|_ _N_

r

which is sharper than the best known result in equation 5 (under δ[′] = 0) in the sense of the
removal of a log(N ) factor. This is a side contribution of our work that might be of independent
interest towards answering an open question raised by Bousquet et al. (2020).


In addition to the generalization bounds, we have also derived a high probability excess risk bound
for uniformly stable randomized learning with confidence-boosting. More specifically, with a proper
modification of the output of Algorithm 1, we can show that with probability at least 1 − _δ over S_
and {Ak}k∈[K]:

log(K/δ)

_R(Ak∗_ (Sk∗ )) − _wmin_ _[R][(][w][)][ ≲]_ _[γ][m][,][ N]K_ [+ ∆][opt][ +] _N_ _,_
_∈W_ r

where ∆opt is the in-expectation empirical risk optimization error as given by equation 7.


-----

2 GENERALIZATION ANALYSIS WITH CONFIDENCE-BOOSTING

In this section, we present a set of generic results on the generalization bounds of randomized learning algorithms with confidence-boosting as described in Algorithm 1.

2.1 PRELIMINARIES AND A KEY LEMMA

We first introduce the following concept of mean-uniform stability that serves as a powerful tool for
analyzing the generalization bounds of randomized learning algorithms (Hardt et al., 2016; Elisseeff
et al., 2005; Bassily et al., 2020).
**Definition 1 (Mean and Mean-Square Uniform Stability of Randomized Algorithms). Let A :**
_Z_ _[N]_ _7→W be a randomized learning algorithm that maps a data set S ∈Z_ _[N]_ _to a model A(S) ∈W._
_Then A is said to have γm,N_ _-mean-uniform stability if for every N_ 1,
_≥_
sup
_S=[.]_ _S[′][ E][A][ [][∥][A][(][S][)][ −]_ _[A][(][S][′][)][∥][]][ ≤]_ _[γ][m][,N]_ _[.]_

_Moreover, A is said to have γm2,N_ _-mean-square-uniform stability if for every N_ 1,
_≥_

sup _A(S)_ _A(S[′])_ _γm2,N_ _._
_S=[.]_ _S[′][ E][A]_ _∥_ _−_ _∥[2][]_ _≤_


Here the algorithm outputs A(S) and A(S[′]) share the same random bits associated with the algorithm. The above defined notion of mean-uniform stability is also known as Uniform Argument
Stability (UAS) (Bassily et al., 2020), which was originally introduced by Liu et al. (2017) for nonparametric hypotheses. The notion of mean-square uniform stability is stronger than mean-uniform
stability in the sense that the former naturally implies the latter such that γm,N _γm2,N_ . More_≤_ _[√]_
over, we say a function f is G-Lipschitz continuous over W if |f (w) − _f_ (w[′])| ≤ _G∥w −_ _w[′]∥_ for all
_w, w[′]_ _∈W, and it is L-smooth if ∥∇f_ (w) −∇f (w[′])∥≤ _L∥w −_ _w[′]∥_ for all w, w[′] _∈W._

Inspired by a second moment bound for generalization error of uniformly stable algorithms
from Bousquet et al. (2020, Section 5), we first establish the following lemma which states that if
a randomized learning algorithm has γm,N -mean-uniform stability, then its on-average first moment
1
generalization error bound will be as small as O(γm,N + _[√]γm2,N +_ _√N_ [)][ when the loss function is]

Lipschitz continuous. This result is an adaptation of the second moment bound derived by Bousquet
et al. (2020) to on-average uniform stable randomized algorithms, and that bound is also the source
of γm2,N entering into play. For completeness, we provide a proof for this result in Appendix B.1.

**Lemma 1.stability and Suppose that a randomized learning algorithm γm2,N** _-mean-square-uniform stability. Assume that the loss function A : Z_ _[N]_ _7→W has γm,N ℓ-mean-uniformis G-Lipschitz_
_with respect to its first argument and is bounded in the range of [0, M_ ]. Then we have

EA,S [|R(A(S)) − _RS(A(S))|] ≤_ _G[√]γm2,N + Gγm,N +_ _√[M]N_ _._


**Remark 1. In comparison to the on-average bound |EA,S [R(A(S)) −** _RS(A(S))]| ≤_ _Gγm,N (see,_
_e.g., Hardt et al., 2016, Theorem 2.2), our on-average first moment bound in Lemma 1 is substantial-_
_ly stronger and it turns out to play a crucial role in deriving high probability bounds for randomized_
_algorithms. When A is deterministic, the above bound reduces to the explicitly or implicitly known_
_first moment bound for uniformly stable algorithms (see, e.g., Feldman & Vondrak, 2018; Bousquet_
_et al., 2020) which is tighter in logarithmic factors than the one implied by the p-th moment in-_
_equality of (Bousquet et al., 2020, Theorem 4). In this case, our bound is stronger than the bound_
_from Bousquet & Elisseeff (2002, Lemma 9) which essentially scales as_ _√1N_ [+][√][γ][m][,N][ in our notation.]

2.2 MAIN RESULTS ON GENERALIZATION BOUND

Let us recall the subbagging process as described in Algorithm 1: we independently run A over
_K even and disjoint training subsets_ _Sk_ _k_ [K] to obtain solutions _Ak(Sk)_ _k_ [K], and then pick
_{_ _}_ _∈_ _{_ _}_ _∈_
_Ak∗_ (Sk∗ ) that has the smallest difference between training error and validation error (over the complementary training subset S _Sk∗_ ). The following theorem is our main result about the high prob_\_
ability generalization bound of the output Ak∗ (Sk∗ ) evaluated over the entire training set S. See
Appendix B.2 for its proof which builds largely on the first moment bound in Lemma 1 and the fact
that at least one of the solutions generated by subbagging generalizes well with high probability.


-----

**Theorem 1.stability and Suppose that a randomized learning algorithm γm2,N** _-mean-square-uniform stability as well. Assume that the loss function A : Z_ _[N]_ _7→W has γm,N_ _-mean-uniform ℓ_ _is G-_
_Lipschitz with respect to its first argument and is bounded in the range of [0, M_ ]. Then for any
_α, δ ∈_ (0, 1) and K ≥ 1−1α [log(][ 4]δ [)][, with probability at least][ 1][ −] _[δ][ over the randomness of][ S][ and]_

_{Ak}k∈[K], the output of Algorithm 1 satisfies_

1 _K_ log(K/δ)
_|R(Ak∗_ (Sk∗ )) − _RS(Ak∗_ (Sk∗ ))| ≲ _αK_ _G[p]γm2, NK_ [+][ Gγ][m][,][ N]K [+][ M] r _N_ ! + M r _N_ _._

**Remark 2. To gain some intuition on the superiority of our bound in Theorem 1, let us consider**
_K ≍_ log 1δ _as allowed in the conditions. If γm,N ≲_ _√1N_ _[and][ γ][m][2][,N][ ≲]_ _N[1]_ _[, then our high-probability]_

log(1/δ)

_bound in Theorem 1 roughly scales as _  _N_ _which is sharper than the on-average bounds_

_in equation 2 and equation 3 with γN ≲_ _√q1N_ _[. More precise consequences of these general results on]_

_SGD and deterministic uniformly stable estimators such as ERMs and full gradient descent method_
_will be discussed shortly in the sections to follow._

**Remark 3. In sharp contrast to the bound in equation 5 that requires high probability uniform**
_stability and the bound in equation 6 that assumes uniform stability over the random bits of algo-_
_rithm, our bound in Theorem 1 holds under a substantially milder notion of mean(-square)-uniform_

_stability over data. In terms of the tightness of bound, note that the confidence term_ log(1N/δ) _is_

_necessary even for an algorithm with fixed output. The uniform stability terms γm, NK_ _[and]q[ γ][m][2][,][ N]K_ _[are]_

_also near-tight as the the algorithm output can change arbitrarily with respect to these quantities._

2.3 ON EXCESS RISK BOUNDS

To understand the optimization performance of a randomized learning algorithm A with confidenceboosting, we further study here the excess risk bounds of Algorithm 1 which are of special interest
for stochastic convex optimization problems. In the following analysis, the global minimizer of the
population risk and in-expectation empirical risk sub-optimality of the randomized algorithm are
respectively denoted by


_w[∗]_ := arg min _R(w) and ∆opt := EA,S_
_w∈W_


_RS(A(S))_ min _._ (7)
_−_ _w_ _[R][S][(][w][)]_
_∈W_ 


In order to derive the excess risk guarantees, we first need to slightly modify the output of Algorithm 1 as Ak∗ (Sk∗ ) where k[∗] = arg mink [K] RS _Sk_ (Ak(Sk)). The following theorem is our
_∈_ _\_
main result about the high probability excess risk bounds of such a modified output of confidenceboosting. See Appendix B.3 for its proof.

**Theorem 2. Suppose that a randomized learning algorithm A : Z** _[N]_ _7→W has γm,N_ _-mean-uniform_
_stability. Assume that the loss function ℓ_ _is G-Lipschitz with respect to its first argument and is_
_bounded in the range of [0, M_ ]. Then for any α, δ ∈ (0, 1) and K ≥ 1−1α [log(][ 4]δ [)][, with probability]

_at least 1 −_ _δ over the randomness of S and {Ak}k∈[K], the modified output of Algorithm 1 satisfies_


_R(Ak[∗]_ (Sk[∗] )) _R(w[∗]) ≲_ [1]
_−_ _α_


log(K/δ)


_Gγm, NK_ [+ ∆][opt] + M




**Remark 4. Unlike the generalization error bounds, the excess risk bounds established in Theorem 2**
_are not relying on the mean-square uniform stability of the algorithm, but with an additional term of_
_in-expectation optimization error for minimizing the empirical risk. For deterministic optimization_
_algorithms such as ERMs with ∆opt = 0, similar excess risk bounds can be implied by the generic_
_results of Shalev-Shwartz et al. (2010, Theorem 26) developed for the confidence-boosting approach._

1 1
**Remark 5. Consider K ≍** log _δ_ _and γm,N ≲_ _√N_ _[. Then the bound in Theorem 2 roughly scales]_

log(1/δ)

_as_ ( _N_ + ∆opt).   
_O_
q

Finally, we comment on the difference between the generalization error and excess risk analysis
inside the considered confidence-boosting framework. Since the excess risk is non-negative and


-----

**Algorithm 2: SGD via With-Replacement Sampling (ASGD-w)**

i.i.d.
**Input : Data set S = {zi}i∈[N** ] _∼D[N]_, step-sizes {ηt}t≥1, #iterations T, initialization w0.
**Output: ¯wT =** _T[1]_ _t_ [T ] _[w][t][.]_

_∈_
**for t = 1, 2, ..., T do**

P

Uniformly randomly sample an index ξt [N ] with replacement;
Compute wt = Π (wt 1 _ηt_ _wℓ(wt ∈1; zξt_ )).
_W_ _−_ _−_ _∇_ _−_
**end**


its in-expectation bound is standardly known for uniformly stable learning algorithms, the highconfidence bound in Theorem 2 can be easily derived via invoking Markov inequality to the independent runs of algorithm over K disjoint subsets. The generalization error analysis, however, is
way more challenging in the sense that establishing tight in-expectation first moment generalization
bound (see Lemma 1) for uniformly stable randomized algorithms is by itself highly non-trivial.

3 IMPLICATIONS FOR STOCHASTIC GRADIENT DESCENT

In this section we demonstrate the applications of the generic bound in Theorem 1 to the widely
used SGD algorithm. We focus on a variant of SGD under with-replacement sampling as outlined
in Algorithm 2, which we call ASGD-w. In what follows, we denote by _ASGD-w,k_ _k_ _K the outputs_
_{_ _}_ _∈_
of ASGD-w over subsets _Sk_ _k_ _K when applied with Algorithm 1. Our results readily extend to the_
_{_ _}_ _∈_
without-replacement variant of SGD and the corresponding results can be found in Appendix D.

3.1 CONVEX OPTIMIZATION WITH SMOOTH LOSS

For smooth and convex losses such as logistic loss, we can derive the following result as a direct
consequence of Theorem 1 with α = 1/2 to ASGD-w. See Appendix C.1 for its proof.
**Corollary 1. Suppose that the loss function is ℓ(·; ·) is convex, G-Lipschitz and L-smooth with**
_respect to its first argument, and is bounded in the range of [0, M_ ]. Consider Algorithm 1 specified
_toprobability at least ASGD-w with learning rate 1 −_ _δ over the randomness of ηt ≤_ 2/L for all t ≥ S1. Then for any and {ASGD-w,k δ} ∈k∈(0[K,] 1), the generalization error and K ≍ log( [4]δ [)][, with]

_is upper bounded as |R(ASGD-w,k∗_ (Sk∗ )) − _RS(ASGD-w,k∗_ (Sk∗ ))| ≲

2[]

_T_ _T_ _T_

_G[2]v_ [1] _ηt[2]_ [+ 1] _ηt_ + _[G][2]_ _ηt + M_ log(1/δ) _._

u _N_ t=1 _N_ _t=1_ ! _N_ _t=1_ r _N_
u X X X
u
t   2

**Remark 6. For the conventional step-size choice of ηt =** _L√t_ _[, the high probability generalization]_

_bound in Corollary 1 is dominated by_ ( log(NT ) + _√T +[√]NN log(1/δ)_ ), which matches the corre_O_

_sponding O(_ _√NT_ [)][ in-expectation bound for SGD with smooth convex losses (Hardt et al., 2016).]q

3.2 CONVEX OPTIMIZATION WITH NON-SMOOTH LOSS


Now we turn to study the case where the loss is convex but not necessarily smooth, such as the
hinge loss and absolute loss. The following result as a direct consequence of Theorem 1 for the
specification of Algorithm 1 to ASGD-w with non-smooth convex loss and time varying learning rate
_{ηt}t≥1. Its proof is provided in Appendix C.2._
**Corollary 2. Suppose that the loss function is ℓ(·; ·) is convex and G-Lipschitz with respect to its first**
_argument, and is bounded in the range of [0, M_ ]. Consider Algorithm 1 specified to ASGD-w. Then
_for any δ ∈_ (0, 1) and K ≍ log( [4]δ [)][, with probability at least][ 1][ −] _[δ][ over the randomness of][ S][ and]_

_ASGD-w,k_ _k_ [K], the generalization error satisfies _R(ASGD-w,k[∗]_ (Sk[∗] )) _RS(ASGD-w,k[∗]_ (Sk[∗] )) ≲
_{_ _}_ _∈_ _|_ _−_ _|_

2

_T_ _T_ _T_ _T_

log(1/δ)

_G[2]v_ _ηt[2]_ [+ 1]N [2] _ηt_ + G[2]v _ηt[2]_ [+][ G]N[2] _ηt + M_ _N_ _._

ut=1 _t=1_ ! ut=1 _t=1_ r
uX X uX X
t t


-----

**Remark 7. For constant rateslog(1 ηt ≡/δ)** _η, Corollary 2 admits a high probability generalization bound_

_of scale O(η√T + η_ _N[T]_ [+] _N_ ) which matches the near-optimal rate by Bassily et al. (2020,

_Theorem 3.3). More importantly, our deviation bound in Corollary 2 still holds for time varyingq_
_learning rates._

3.3 NON-CONVEX OPTIMIZATION WITH SMOOTH LOSS


We further study the performance of Algorithm 1 for SGD on smooth but not necessarily convex
loss functions, such as normalized sigmoid loss (Mason et al., 1999). The following result is a direct
application of Theorem 1 to ASGD-w with smooth non-convex loss. See Appendix C.3 for its proof.
**Corollary 3. Suppose that the loss function is ℓ(·; ·) is G-Lipschitz and L-smooth with respect to**
_its first argument, and is bounded in the range of [0, M_ ]. Consider Algorithm 1 specified to ASGD-w
_with ηt ≤_ _L[1]_ _[. Let][ u][t][ :=][ η]t[2]_ [+ 2][η][t] _tτ−=11_ [exp(][L][ P]i[t]=[−]τ[1]+1 _[η][i][)][η][τ][ for all][ t][ ≥]_ [1][. Then for any][ δ][ ∈] [(0][,][ 1)]

_and K ≍_ log( [4]δ [)][, with probability at least]P [ 1] _[−]_ _[δ][ over the randomness of][ S][ and][ {][A][SGD-w][,k][}][k][∈][[][K][]][, the]_

_generalization error is upper bounded as |R(ASGD-w,k∗_ (Sk∗ )) − _RS(ASGD-w,k∗_ (Sk∗ ))| ≲

_T_ _T_ _T_ _T_

[1] log(1/δ)

_G[2]_ exp _L_ _ητ_ _ut +_ _[G][2]_ exp _L_ _ητ_ _ηt + M_ _._

v _N_ _N_ _N_
u _t=1_ _τ_ =t+1 ! _t=1_ _τ_ =t+1 ! r
u X X X X

**Remark 8.t For the constant learning rates ηt ≡** _LT1_ _[, Corollary 3 admits high probability gener-]_

_alization bound of scale O(_ log(1N/δ) ). For time decaying learning rates ηt = _Lνt1_ _[with arbitrary]_
q _T_ [1][/ν] log(T ) log(1/δ)

_ν_ 1, it can be verified that the corresponding bound is of scale ( _νN_ + _N_ ).
_≥_ _O_
q q

4 IMPLICATIONS FOR DETERMINISTIC UNIFORMLY STABLE ALGORITHMS


This section is devoted to showing that confidence-boosting is also beneficial for deriving stronger
generalization bounds for uniformly stable deterministic learning algorithms. First we note that
when there is no internal randomness in A, the definition of mean(-square)-uniform stability reduces
to the conventional concept of γN -uniform stability for deterministic algorithms given by
sup
_S=[.]_ _S[′][ ∥][A][(][S][)][ −]_ _[A][(][S][′][)][∥≤]_ _[γ][N][ =][ γ][m][,N][ =][ √][γ][m][2][,N]_ _[.]_

Let us now consider a specification of Algorithm 1 to a uniformly stable deterministic algorithm A.
Since there is no randomness contained in A, we have that Ak(Sk) = A(Sk) for all k [K] in such
_∈_
a deterministic case. Then the following result is a direct consequence of Theorem 1 when applied
to the considered deterministic learning regime.
**Corollary 4. Suppose that a deterministic learning algorithm A : Z** _[N]_ _7→W has γN_ _-uniform_
_stability. Assume that the loss function ℓ_ _is G-Lipschitz with respect to its first argument and is_
_bounded in [0, M_ ]. Then for any α, δ ∈ (0, 1) and K ≥ [log(4]1−α[/δ][)] _[, with probability at least][ 1][ −]_ _[δ][ over]_

_the randomness of S, the output of Algorithm 1 satisfies_


log(K/δ)


_|R(A(Sk∗_ )) − _RS(A(Sk∗_ ))| ≲


_Gγ N_

_K_ [+][ M]


+ M


_αK_


in the above corollary. In the regimeTo demonstrate the superiority of our bound over prior ones, let us consider γN ≲ _√1N_ [which is of interest in many popular deterministic] α = 0.5 and K ≍ log( [1]δ [)]

learning paradigms/algorithms such as regularized ERM (Shalev-Shwartz et al., 2009) and full gradient descent (Feldman & Vondrak, 2019), Corollary 4 implies a generalization bound for A(Sk∗ )

over the data set S that scales as _R(A(Sk[∗]_ )) _RS(A(Sk[∗]_ )) ≲ log(1N/δ) . In comparison, the best
_|_ _−_ _|_

known bound in equation 5 essentially from Bousquet et al. (2020) gives (keep in mind thatq _δ[′]_ = 0
in the deterministic case) _R(A(S))_ _RS(A(S))_ ≲ [log(]√N[N] [)] log( [1]δ [)][. As we can see that inside the]
_|_ _−_ _|_

carefully designed framework of confidence-bossting via subbagging, our generalization bound gets
rid of the logarithmic factor log(N ) from the above best known result, though the generalization is
with respect to the estimation over a specific part of the sample. We expect this result will fuel future
research towards fully resolving the corresponding open question raised in Bousquet et al. (2020).


-----

5 OTHER RELATED WORK

The idea of using stability of a learning algorithm, namely the sensitivity of estimated model to the
changes in training data, for generalization performance analysis dates back to the seventies (Vapnik & Chervonenkis, 1974; Rogers & Wagner, 1978; Devroye & Wagner, 1979). For deterministic
learning algorithms, algorithmic stability has been extensively studied with a bunch of applications
to establishing strong generalization and excess risk bounds for stable learning models like k-NN
and regularized ERMs (Bousquet & Elisseeff, 2002; Zhang, 2003; Klochkov & Zhivotovskiy, 2021).
The stability theory for randomized learning algorithms was formally introduced and investigated
by Elisseeff et al. (2005). In a recent breakthrough work (Hardt et al., 2016), it was shown in that
the solution obtained via stochastic gradient descent is expected to be stable and generalize well for
smooth convex and non-convex loss functions. For non-smooth convex losses, the stability induced
generalization bounds of SGD have been established in expectation (Lei & Ying, 2020) or deviation (Bassily et al., 2020). In Kuzborskij & Lampert (2018), a set of data-dependent generalization
bounds for SGD were derived based on the stability of algorithm. More broadly, generalization
bounds for stable learning algorithms that converge to global minima were established in Charles
& Papailiopoulos (2018); Lei & Ying (2021). For non-convex sparse learning, algorithmic stability theory has been applied to derive the generalization bounds of the popularly used iterative hard
thresholding (IHT) algorithm (Yuan & Li, 2021). The uniform stability bounds on SGD have also
been extensively used for designing differential privacy stochastic optimization algorithms (Bassily
et al., 2019; Feldman et al., 2020).

Bagging (or bootstrap aggregating) is one of the earliest and most popular ensemble methods that
has been widely applied to reduce the variance for unstable learning algorithms such as decision
tree and neural networks (Breiman, 1996; Opitz & Maclin, 1999), and sometimes stable algorithms
such as SVMs (Valentini & Dietterich, 2003). As an important variant of bagging, subbagging has
been proposed to reduce the computational cost of bagging via training base models under withoutreplacement sampling (B¨uhlmann, 2012). The stability and generalization bounds of bagging have
been analyzed for both uniform (Elisseeff et al., 2005) and non-uniform (Foster et al., 2019) averaging schemes. Unlike these prior results for bagging with averaging aggregation, our bounds
are obtained based on a confidence-boosting greedy aggregation scheme which turns out to yield
sharper dependence on the uniform stability parameter.

The confidence-boosting technique has long been applied for obtaining sharp high-probability excess risk bounds from the corresponding strong in-expectation bounds (Shalev-Shwartz et al., 2010;
Mehta, 2017). For generic statistical learning problems, confidence-boosting has been used to convert any low-confidence learning algorithm with linear dependence on 1/δ to a high-confidence
algorithm with logarithmic factor log(1/δ). For learning with exp-concave losses, a relevant ERM
estimator with in-expectation fast rate of convergence was converted to a high-confidence learning
algorithm with an almost identical fast rate of convergence up to a logarithmic factor on 1/δ (Mehta,
2017). While sharing a similar spirit of boosting the confidence, our generalization analysis is substantially more challenging than those prior excess risk analysis in terms of tightly deriving inexpectation first moment generalization bound for uniformly stable randomized algorithms.

6 CONCLUSIONS

In this paper we presented a generic confidence-boosting method for deriving near-optimal high
probability generalization bounds for uniformly stable randomized learning algorithms. At a nutshell, our main results in Theorem 1 and Theorem 2 reveal that a carefully designed subbagging
process in Algorithm 1 can yield high-confidence generalization and risk bounds under the notion
of mean(-square)-uniform stability. Our theory has been substantialized to SGD on both convex and
non-convex losses to obtain stronger generalization bounds especially in the case of time decaying
learning rates. When reduced to deterministic algorithms, the proposed method removes a logarithmic factor on sample size from the best known bounds. While sharper in the dependence on sample
size, our confidence-boosting results are only applicable to one of the independent runs of algorithm
1
_A over K disjoint training subsets of equal size with K_ log _δ_ . It is so far not clear if these
_≍_

near-optimal bounds can be further extended to the full-batch setting where the generalization is

  

with respect to the evaluation of algorithm over the entire sample. We leave the full understanding
of such an open issue raised by Bousquet et al. (2020) for future investigation.


-----

REFERENCES

Savina Andonova, Andre Elisseeff, Theodoros Evgeniou, and Massimiliano Pontil. A simple algorithm for learning stable machines. In ECAI, pp. 513–517, 2002.

Peter L Bartlett, Michael I Jordan, and Jon D McAuliffe. Convexity, classification, and risk bounds.
_Journal of the American Statistical Association, 101(473):138–156, 2006._

Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Thakurta. Private stochastic convex
optimization with optimal rates. arXiv preprint arXiv:1908.09970, 2019.

Raef Bassily, Vitaly Feldman, Cristobal Guzman, and Kunal Talwar. Stability of stochastic gradient
descent on nonsmooth convex losses. In Advances in Neural Information Processing Systems, pp.
1–10, 2020.

Yoshua Bengio, Ian Goodfellow, and Aaron Courville. Deep learning, volume 1. MIT press Massachusetts, USA:, 2017.

L´eon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In Advances in Neural
_Information Processing Systems, pp. 161–168, 2008._

Olivier Bousquet and Andr´e Elisseeff. Stability and generalization. Journal of Machine Learning
_Research, 2(Mar):499–526, 2002._

Olivier Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy. Sharper bounds for uniformly stable
algorithms. In Conference on Learning Theory, pp. 610–626, 2020.

Leo Breiman. Bagging predictors. Machine learning, 24(2):123–140, 1996.

Peter B¨uhlmann. Bagging, boosting and ensemble methods. In Handbook of computational statis_tics, pp. 985–1022. Springer, 2012._

Zachary Charles and Dimitris Papailiopoulos. Stability and generalization of learning algorithms
that converge to global optima. In International Conference on Machine Learning, pp. 744–753,
2018.

Luc Devroye and Terry Wagner. Distribution-free inequalities for the deleted and holdout error
estimates. IEEE Transactions on Information Theory, 25(2):202–207, 1979.

Andre Elisseeff, Theodoros Evgeniou, and Massimiliano Pontil. Stability of randomized learning
algorithms. Journal of Machine Learning Research, 6(Jan):55–79, 2005.

Vitaly Feldman and Jan Vondrak. Generalization bounds for uniformly stable algorithms. In Ad_vances in Neural Information Processing Systems, pp. 9747–9757, 2018._

Vitaly Feldman and Jan Vondrak. High probability generalization bounds for uniformly stable algorithms with nearly optimal rate. In Conference on Learning Theory, pp. 1270–1279, 2019.

Vitaly Feldman, Tomer Koren, and Kunal Talwar. Private stochastic convex optimization: optimal
rates in linear time. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of
_Computing, pp. 439–449, 2020._

Dylan J Foster, Spencer Greenberg, Satyen Kale, Haipeng Luo, Mehryar Mohri, and Karthik Sridharan. Hypothesis set stability and generalization. In Advances in Neural Information Processing
_Systems, 2019._

Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In International Conference on Machine Learning, pp. 1225–1234, 2016.

Nicholas JA Harvey, Christopher Liaw, Yaniv Plan, and Sikander Randhawa. Tight analyses for nonsmooth stochastic gradient descent. In Conference on Learning Theory, pp. 1579–1613. PMLR,
2019.

Sham M Kakade, Karthik Sridharan, and Ambuj Tewari. On the complexity of linear prediction:
Risk bounds, margin bounds, and regularization. In Advances in Neural Information Processing
_Systems, pp. 793–800, 2009._


-----

Yegor Klochkov and Nikita Zhivotovskiy. Stability and deviation optimal risk bounds with convergence rate o(1/n). In Advances in Neural Information Processing Systems, 2021.

Ilja Kuzborskij and Christoph Lampert. Data-dependent stability of stochastic gradient descent. In
_International Conference on Machine Learning, pp. 2820–2829, 2018._

Yunwen Lei and Yiming Ying. Fine-grained analysis of stability and generalization for sgd. In
_International Conference on Machine Learning, 2020._

Yunwen Lei and Yiming Ying. Sharper generalization bounds for learning with gradient-dominated
objective functions. In International Conference on Learning Representations, 2021.

Tongliang Liu, G´abor Lugosi, Gergely Neu, and Dacheng Tao. Algorithmic stability and hypothesis
complexity. In International Conference on Machine Learning, pp. 2159–2167. PMLR, 2017.

Llew Mason, Jonathan Baxter, Peter Bartlett, and Marcus Frean. Boosting algorithms as gradient
descent in function space. In Advances in Neural Information Processing Systems, pp. 512–518,
1999.

Nishant Mehta. Fast rates with high probability in exp-concave statistical learning. In Artificial
_Intelligence and Statistics, pp. 1085–1093. PMLR, 2017._

Sayan Mukherjee, Partha Niyogi, Tomaso Poggio, and Ryan Rifkin. Learning theory: stability is
sufficient for generalization and necessary and sufficient for consistency of empirical risk minimization. Advances in Computational Mathematics, 25(1-3):161–193, 2006.

David Opitz and Richard Maclin. Popular ensemble methods: An empirical study. Journal of
_Artificial Intelligence Research, 11:169–198, 1999._

Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for
strongly convex stochastic optimization. In International Conference on Machine Learning, pp.
1571–1578, 2012.

William H Rogers and Terry J Wagner. A finite sample distribution-free performance bound for
local discrimination rules. The Annals of Statistics, pp. 506–514, 1978.

Robert E Schapire. The strength of weak learnability. Machine learning, 5(2):197–227, 1990.

Mark Schmidt, Nicolas L Roux, and Francis R Bach. Convergence rates of inexact proximal-gradient
methods for convex optimization. In Advances in Neural Information Processing Systems, pp.
1458–1466, 2011.

Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Stochastic convex optimization. In Conference on Learning Theory, 2009.

Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability
and uniform convergence. Journal of Machine Learning Research, 11(Oct):2635–2670, 2010.

Giorgio Valentini and Thomas G Dietterich. Low bias bagged support vector machines. In Interna_tional Conference on Machine Learning, pp. 752–759, 2003._

V. N. Vapnik and A. Ya. Chervonenkis. Theory of Pattern Recognition [in Russian]. Nauka, 1974.

Xiao-Tong Yuan and Ping Li. Stability and risk bounds of iterative hard thresholding. In Interna_tional Conference on Artificial Intelligence and Statistics, pp. 1702–1710. PMLR, 2021._

Tong Zhang. Leave-one-out bounds for kernel methods. Neural Computation, 15(6):1397–1437,
2003.

Yi Zhou, Huishuai Zhang, and Yingbin Liang. Understanding generalization error of sgd in nonconvex optimization. In International Conference on Acoustics, Speech, and Signal Processing
_(ICASSP), 2019._


-----

A AUXILIARY LEMMAS

We need the following lemma from Hardt et al. (2016) which shows that SGD iteration is nonexpansive on convex and smooth loss.

**Lemma 2 (Hardt et al. (2016)). Assume that f is convex and L-smooth. Then for any w, w[′]** _∈W_
_and α ≤_ 2/L, we have the following bound holds

_∥w −_ _α∇f_ (w) − (w[′] _−_ _α∇f_ (w[′]))∥≤∥w − _w[′]∥._

The following lemma, which can be proved by induction (see, e.g., Schmidt et al., 2011), will be
used to prove the main results in Section 3.

**Lemma 3. Assume that the nonnegative sequence** _uτ_ _τ_ 1 satisfies the following recursion for all
_{_ _}_ _≥_
_t_ 1:
_≥_ _t_


_u[2]t_ [+]

_[≤]_ _[S][t]_


_ατ_ _uτ_ _,_
_τ_ =1

X


_holds for allwith {Sτ_ _}τ_ _≥ t1 ≥ an increasing sequence,1:_ _S0 ≥_ _u[2]0_ _[and][ α][τ]t_ _[≥]_ [0][ for all][ τ] _[. Then, the following inequality]_


_ut_
_≤_


_St +_


_ατ_ _._
_τ_ =1

X


B PROOFS FOR THE RESULTS IN SECTION 2

In this section, we present the technical proofs for the main results stated in Section 2.

B.1 PROOF OF LEMMA 1

We need the following lemma essentially from Bousquet et al. (2020) that provides a first moment
bound for the sum of random functions.

**Lemma 4. Let S = {Z1, Z2, ..., ZN** _} be a set of i.i.d. random variables valued in Z. Let g1, ..., gN_
_be a set of measurable functions gi :_ R that satisfy ES[gi[2][(][S][)]][ ≤] _[M][ 2][ and][ E][Z]i_ [[][g][i][(][S][)] = 0]
_Z_ _[N]_ _7→_
_for all i ∈_ [N ]. Then we have

_N_

ES "Xi=1 _gi(S)_ # _≤_ sXi≠ _j_ ES,S(j) h gi(S) − _gi(S[(][j][)])2[i]_ + M _√N,_

_where S[(][j][)]_ = _Z1, ..., Zj_ 1, Zj[′] _[, Z][j][+1][, ..., Z][N]_ _[}][ and][ S][′][ =][ {][Z]1[′]_ _[, Z]2[′]_ _[, ..., Z]N[′]_ _[}][ is another i.i.d. sample]_
_{_ _−_
_from the same distribution as that of S._

_Proof. We reproduce the proof in view of the argument in Bousquet et al. (2020, Section 5) showing_
that {gi} are weakly correlated. For any i ̸= j, since EZi [gi(S)] = 0 and EZj [gj(S)] = 0, we can
verify that

ES _gi(S[(][j][)])gj(S)_ = ES _Zj_ EZj _gi(S[(][j][)])gj(S)_ _S_ _Zj_ = ES _Zj_ _gi(S[(][j][)])EZj [gj(S)_ _S_ _Zj]_ = 0,
_\_ _|_ _\_ _\_ _|_ _\_
h i h h ii h i

where we have used the independence of the elements in S ∪{Zj[′] _[}][. Similarly, we can show that]_

ES _gi(S)gj(S[(][i][)])_ = ES _gi(S[(][j][)])gj(S[(][i][)])_ = 0.
h i h i


Then it follows that for any i ̸= j,

_|ES [gi(S)gj(S)]| =_ ES,S(i),S(j) [gi(S)gj(S)]

= ES,S(i),S(j) (gi(S) − _gi(S[(][j][)]))(gj(S) −_ _gj(S[(][i][)]))_
h

_≤ES,S(i),S(j)_ (gi(S) − _gi(S[(][j][)]))(gj(S) −_ _gj(S[(][i][)]))_
h


-----

Based on the above bound and Jensen’s inequality we get


ES _gi(S)_

" _i=1_ #

X

2[]

_N_ _N_

vES _gi(S)_ = ES [gi(S)gj(S)] + ES[gi[2][(][S][)]]

_≤uuu_  Xi=1 ! vuu _i≠_ _j_ Xi=1

t   t[X]

_≤_ ES,S(i),S(j) (gi(S) − _gi(S(j)))(gj(S) −_ _gj(S(i)))_ + M _√N_
sXi≠ _j_  []

1 2 2[i]

ES,S(i),S(j) _gi(S)_ _gi(S[(][j][)])_ + _gj(S)_ _gj(S[(][i][)])_ + M

_≤_ 2 _−_ _−_
s _i=j_

X̸ h     

= ES,S(j) _gi(S) −_ _gi(S[(][j][)])_ 2[i] + M _√N._
sXi≠ _j_ h  

This proves the desired bound.


Now we are ready to prove the result in Lemma 1.

_Proof of Lemma 1. Let us consider hi(S) := R(A(S))_ _ℓ(A(S); Zi) and gi(S) = hi(S)_
_−_ _−_
EZi [hi(S)] for i ∈ [N ]. Then by assumption we have
EZi [gi(S)] = 0, ES[gi[2][(][S][)]][ ≤] _[M][ 2][.]_
For each i ∈ [N ], let S[(][i][)] denote a random data set that is identical to S except that one of the
_Zi is replaced by another random sample Zi[′][. For any][ i][ ̸][=][ j][, since the loss is non-negative and]_
_G-Lipschitz, it can be verified that_


_|gi(S) −_ _gi(S[(][j][)])| ≤_ max _hi(S) −_ _hi(S[(][j][)])_ _,_ EZi [hi(S) − _hi(S[(][j][)])]_

_,_

n o

_≤_ max _G∥A(S) −_ _A(S[(][j][)])∥, EZi_ _G∥A(S) −_ _A(S[(][j][)])∥_

which readily implies n h io

2[]
ES,S(j) _gi(S) −_ _gi(S[(][j][)])_ _≤_ ES,S(j) _G[2]∥A(S) −_ _A(S[(][j][)])∥[2][i]_ _._
  h

Then invoking Lemma 4 to _gi_ yields
_{_ _}_

_N_

ES "Xi=1 _gi(S)_ # _≤sXi≠_ _j_ ES,S(j) h gi(S) − _gi(S[(][j][)])2[i]_ + M _√N_

_≤G_ ES,S(j) _∥A(S) −_ _A(S[(][j][)])∥[2][]_ + M _√N._
sXi≠ _j_ 

Further, it can be verified that


(A.1)

(A.2)


ES

=ES

=ES

=ES


EZi [hi(S)]
_i=1_ #

X

_N_

_i=1_ EZi [R(A(S)) − _ℓ(A(S); Zi)]_ #

X

_N_

EZi [EZi[′] [[][ℓ][(][A][(][S][);][ Z]i[′][)]][ −] _[ℓ][(][A][(][S][);][ Z][i][)]]_
_i=1_ #

X

_N_

EZi EZi[′] [[][ℓ][(][A][(][S][);][ Z]i[′][)]][ −] [E][Z]i[′] [[][ℓ][(][A][(][S][(][i][)][);][ Z]i[′][)]]
_i=1_

X h


_≤ES,S(i)_ _G∥A(S) −_ _A(S[(][i][)])∥_
h


-----

By combining equation A.1 and equation A.2 we obtain

EA,S [|R(A(S)) − _RS(A(S))|]_


= [1]

_N_ [E][A,S]


(gi(S) + EZi [hi(S)])
_i=1_

X


_≤_ _N[G]_ [E][A]  _i=j_ ES,S(j) _∥A(S) −_ _A(S[(][j][)])∥[2][]_ + _N[G]_ [E][A,S,S][(][i][)] _∥A(S) −_ _A(S[(][i][)])∥_ + _√[M]N_

sX̸  h i
 

_≤_ _N[G]_ _i=j_ EA,S,S(j) _∥A(S) −_ _A(S[(][j][)])∥[2][]_ + _N[G]_ [E][A,S,S][(][i][)] _∥A(S) −_ _A(S[(][i][)])∥_ + _√[M]N_

sX̸  h i

_≤G[√]γm2,N + Gγm,N +_ _√[M]N_ _,_


where in the last inequality we have used the stability conditions on A. The proof is completed.

B.2 PROOF OF THEOREM 1

We first establish the following intermediate result that captures the effects of subbagging on randomized algorithms: it basically tells that with K ≍ log( [1]δ [)][, at least one of the solutions generated]

by subbagging generalizes well with high probability.

**Lemma 5.stability and Suppose that a randomized learning algorithm γm2,N** _-mean-square-uniform stability as well. Assume that the loss function A : Z_ _[N]_ _7→W has γm,N_ _-mean-uniform ℓ_ _is G-_
_Lipschitz with respect to its first argument and is bounded in the range of [0, M_ ]. Then for any α, δ ∈
(0, 1) and K ≥ [log(2]1−α[/δ][)] _[, with probability at least][ 1][ −]_ _[δ][ over the randomness of][ {][(][A][k][, S][k][)][}][k][∈][[][K][]][,]_

_the sequence_ _Ak(Sk)_ _k_ [K] generated by Algorithm 1 satisfies
_{_ _}_ _∈_


min
_k∈[K]_ _[|][R][(][A][k][(][S][k][))][ −]_ _[R][S][k]_ [(][A][k][(][S][k][))][|][ ≲] _α[1]_


_G[p]γm2, NK_ [+][ Gγ][m][,][ N]K [+][ M]


_Proof. From Lemma 1 we have that over randomized algorithm A and data_ _S[˜] with |S[˜]| =_ _K[N]_ [,]


_K_

EA, ˜S _|R(A( S[˜])) −_ _R ˜S[(][A][( ˜]S))|_ _≤_ _G[p]γm2, NK_ [+][ Gγ][m][,][ N]K [+][ M] _N [.]_

r

h i

Since _Ak, Sk_ _k_ [K] are independent to each other, by Markov inequality we know that
_{_ _}_ _∈_

_K_

P{Ak,Sk} _kmin[K]_ _α_ _G[p]γm2, NK_ [+][ Gγ][m][,][ N]K [+][ M] _N_

_∈_ _[|][R][(][A][k][(][S][k][))][ −]_ _[R][S][k]_ [(][A][k][(][S][k][))][| ≥] [1] r !!

which implies the desired bound.

Next we proceed to prove the main result in Theorem 1.

_Proof of Theorem 1. Let us consider the following three events:_


_≤_ _α[K]_ _≤_ _δ,_

_K log(K/δ)_

s (K 1)N

_−_


1
_|R(Ak∗_ (Sk∗ )) − _RS(Ak∗_ (Sk∗ ))| ≲ _αK_ _G[p]γm2, NK_ [+][ Gγ][m][,][ N]K [+][ M]

_K log(K/δ)_

max _,_
_k∈[K]_ _[|][R][(][A][k][(][S][k][))][ −]_ _[R][S][\][S][k]_ [(][A][k][(][S][k][))][|][ ≲] _[M]_ s (K − 1)N )


_E :=_

_E1 :=_

_E2 :=_


+ M

!

_._

!)


min
_k∈[K]_ _[|][R][(][A][k][(][S][k][))][ −]_ _[R][S][k]_ [(][A][k][(][S][k][))][|][ ≲] _α[1]_


_G[p]γm2, NK_ [+][ Gγ][m][,][ N]K [+][ M]


-----

the following inequality is valid:We can show that E ⊇E1 ∩E2. Indeed, suppose that E1 and E2 simultaneously occur. Consequently

_R(Ak[∗]_ (Sk[∗] )) _RS(Ak[∗]_ (Sk[∗] ))
_|_ _−_ _|_

= _K [R][S][k][∗]_ [(][A][k][∗] [(][S][k][∗] [))][ −] _[K][ −]K_ [1] _RS\Sk∗_ (Ak∗ (Sk∗ ))

_≤_ _K[1][R][(][|][R][A][(][k][A][∗]_ [(][k][S][∗][k][(][∗][S][))][k][∗][ −][))][ −][1] _[R][S][k][∗]_ [(][A][k][∗] [(][S][k][∗] [))][|][ +][ K][ −]K [1] _R(Ak∗_ (Sk∗ )) − _RS\Sk∗_ (Ak∗ (Sk∗ ))

_≤_ _K[1]_ _RS\Sk∗_ (Ak∗ (Sk∗ )) − _RSk∗_ (Ak∗ (Sk∗ )) + _R(Ak∗_ (Sk∗ )) − _RS\Sk∗_ (Ak∗ (Sk∗ ))

= ζ1 _K[1]_ _k[min][K]_ _RS\Sk_ (Ak(Sk)) − _RSk_ (Ak(Sk)) + _R(Ak∗_ (Sk∗ )) − _RS\Sk∗_ (Ak∗ (Sk∗ ))

_∈_

= K[1] _k[min][K]_ _RS\Sk_ (Ak(Sk)) − _R(Ak(Sk)) + R(Ak(Sk)) −_ _RSk_ (Ak(Sk))

_∈_

+ _R(Ak∗_ (Sk∗ )) − _RS\Sk∗_ (Ak∗ (Sk∗ ))

_≤_ _K[1]_ _k[min][K]_ _K_ _k[max][K]_ _RS\Sk_ (Ak(Sk)) − _R(Ak(Sk))_

_∈_ _[|][R][(][A][k][(][S][k][))][ −]_ _[R][S][k]_ [(][A][k][(][S][k][))][|][ + 1] _∈_

+ _R(Ak∗_ (Sk∗ )) − _RS\Sk∗_ (Ak∗ (Sk∗ ))

_≤_ _K[1]_ _k[min][K]_ _K_ _kmax[K]_ _R(Ak(Sk)) −_ _RS\Sk_ (Ak(Sk))

_∈_ _[|][R][(][A][k][(][S][k][))][ −]_ _[R][S][k]_ [(][A][k][(][S][k][))][|][ +][ K][ + 1] _∈_

_ζ2_ _K_ _K log(K/δ)_
≲ _αK[1]_ _G[p]γm2, NK_ [+][ Gγ][m][,][ N]K [+][ M] r _N_ ! + M s (K − 1)N [,]

where in “ζ1” we have used the definition of k[∗], and “ζ2” follows from 1, 2. With leading terms
_E_ _E_
preserved in the above we can see that E occurs.

Next we can show that PS, _Ak_ ( 1) 2 [. Toward this end, let us consider the following events for]
_{_ _}_ _E_ _≤_ _[δ]_

all k ∈ [K]:


_K log(K/δ)_


_R(Ak(Sk))_ _RS_ _Sk_ (Ak(Sk)) ≲ _M_
_|_ _−_ _\_ _|_


_E1[k]_ [:=]


_E1_ _|_ _k_ _k_ _−_ _S\Sk_ _k_ _k_ _|_ (K 1)N

_−_

Clearly, it is true that E1 = _k=1_ _[E]1[k][. It is sufficient to prove that][ P][S,A]k_ _E1[k]_ _≤_ 2δK [holds for each]

_k_ [K]. Indeed, consider the random indication function β(S, Ak) := **1k**
_∈_ _E1_ [associated with the]

[T][K]

event E1[k][. Then we have the following holds for each][ k][ ∈] [[][K][]][:]


PS,Ak 1

_E_ _[k]_

=ES,Ak [β(S, A _k)]_

=EAk,Sk ES _Sk_ _Ak,Sk [β(S, Ak)_ _Ak, Sk]_
_\_ _|_ _|_

_ζ1_
=EAk,Sk ES _Sk [β(S, Ak)_ _Ak, Sk]_
_\_ _|_
 


_K log(K/δ)_

(K − 1)N


=EAk,Sk

_ζ2_
_≤EAk,Sk_


PS _Sk_
_\_


_R(Ak(Sk))_ _RS_ _Sk_ (Ak(Sk)) ≳ _M_
_|_ _−_ _\_ _|_


_Ak, Sk_
_|_


_ζ2_ _δ_ _δ_
EAk,Sk =
_≤_ 2K 2K [,]

 _[|][ A][k][, S][k]_

where in “ζ1” we have used the independence between _Ak, Sk_ and S _Sk, and “ζ2” is due to_
_{_ _}_ _\_
Hoeffding’s inequality conditioned on _Ak, Sk_, keeping in mind that Ak(Sk) is independent on
_{_ _}_
the data set S \ Sk of size (1 − 1/K)N . It follows by union probability that


_E1[k]_ _≤_ 2[δ] _[.]_



PS, _Ak_
_{_ _}_


1 = PS, _Ak_
_E_ _{_ _}_



PS,Ak
_k=1_

X


1
_E_ _[k]_

_k=1_

[


-----

Further, from the part(b) of Lemma 5 we have PS, _Ak_ ( [¯]2) 2 [. Combining this and the preceding]
_{_ _}_ _E_ _≤_ _[δ]_

bound yields

PS, _Ak_ ( ) PS, _Ak_ ( 1 2) 1 PS, _Ak_ ¯1 PS, _Ak_ ( [¯]2) 1

This implies the desired result in part(b) as{ _}_ _E_ _≥_ _{_ _}_ _E_ _∩E_ _≥_ _− K/({K −}_  1)E _≤_ _−2 for K{_ _≥}_ _E2._ _≥_ _−_ 2[δ] _[−]_ 2 [δ] [= 1][ −] _[δ.]_


B.3 PROOF OF THEOREM 2

We first present the following simple lemma about the in-expectation risk bounds of a randomized
algorithm which will be used in our analysis.
**Lemma 6. Suppose that a randomized learning algorithm A : Z** _[N]_ _7→W has γm,N_ _-mean-uniform_
_stability. Assume that the loss function ℓ_ _is G-Lipschitz with respect to its first argument. Then we_
_have_
EA,S [R(A(S)) − _R(w[∗])] ≤_ _Gγm,N + ∆opt._

_Proof. By risk decomposition we can show that_

EA,S [R(A(S)) − _R(w[∗])]_
=EA,S [R(A(S)) − _RS(A(S)) + RS(A(S)) −_ _RS(w[∗]) + RS(w[∗]) −_ _R(w[∗])]_
_≤|EA,S [R(A(S)) −_ _RS(A(S))]| + ∆opt_
_≤Gγm,N + ∆opt,_

where in the last inequality we have used the on-average generalization bound by Hardt et al. (2016,
Theorem 2.2).

**Lemma 7. Suppose that a randomized learning algorithm A : Z** _[N]_ _7→W has γm,N_ _-mean-uniform_
_stability. Assume that the loss function ℓ_ _is G-Lipschitz with respect to its first argument. Then_
_for any α, δ ∈_ (0, 1) and K ≥ log(21−α/δ) _[, with probability at least][ 1][ −]_ _[δ][ over the randomness of]_

(Ak, Sk) _k_ [K], the sequence _Ak(Sk)_ _k_ [K] generated by Algorithm 1 with the modified output
_{_ _}_ _∈_ _{_ _}_ _∈_
_satisfies_

_kmin[K]_ _[R][(][A][k][(][S][k][))][ −]_ _[R][(][w][∗][)][ ≲]_ _α[1]_ _Gγm, NK_ [+ ∆][opt] _._
_∈_

 

_Proof. Recall the modified output Ak[∗]_ (Sk[∗] ) where k[∗] = arg mink [K] RS _Sk_ (Ak(Sk)). From
_∈_ _\_
Lemma 6 we have that over randomized algorithm A and data _S[˜] with |S[˜]| =_ _K[N]_ [,]

EA, ˜S _R(A( S[˜])) −_ _R(w[∗])_ _≤_ _Gγm, NK_ [+ ∆][opt][.]
h i


Since _Ak, Sk_ _k_ [K] are independent to each other, by Markov inequality we know that
_{_ _}_ _∈_

P{Ak,Sk} _kmin[K]_ _[R][(][A][k][(][S][k][))][ −]_ _[R][(][w][∗][)][ ≥]_ _α[1]_ _Gγm, NK_ [+ ∆][opt] _≤_ _α[K]_ _≤_ _δ,_

 _∈_  []

which implies the desired bound in part(b).

Next we proceed to prove the main result in Theorem 2.

_Proof of Theorem 2. Let us consider the following three events:_


_R(Ak∗_ (Sk∗ )) _RS(Ak∗_ (Sk∗ )) ≲ [1]
_|_ _−_ _|_ _α_


_K log(K/δ)_

(K − 1)N


_E :=_

_E1 :=_


_Gγm, NK_ [+ ∆][opt] + M




_K log(K/δ)_


_K log(K/δ_

1 := max
_E_ (k∈[K] _[|][R][(][A][k][(][S][k][))][ −]_ _[R][S][\][S][k]_ [(][A][k][(][S][k][))][|][ ≲] _[M]_ s (K − 1)N

_E2 :=_ _kmin[K]_ _[R][(][A][k][(][S][k][))][ −]_ _[R][(][w][∗][)][ ≲]_ _α[1]_ _Gγm, NK_ [+ ∆][opt] _._
 _∈_  []


-----

quently the following inequality is valid:Similarly, we show that E ⊇E1 ∩E2. Indeed, suppose that E1 and E2 simultaneously occur. Conse
_R(Ak∗_ (Sk∗ )) _R(w[∗])_
_−_
=R(Ak∗ (Sk∗ )) − _RS\Sk∗_ (Ak∗ (Sk∗ )) + RS\Sk∗ (Ak∗ (Sk∗ )) − _R(w[∗])_

=ζ1R(Ak∗ (Sk∗ )) − _RS\Sk∗_ (Ak∗ (Sk∗ )) + mink [K] _[R][S][\][S][k]_ [(][A][k][(][S][k][))][ −] _[R][(][w][∗][)]_
_∈_

=R(Ak∗ (Sk∗ )) − _RS\Sk∗_ (Ak∗ (Sk∗ )) + mink [K] _RS\Sk_ (Ak(Sk)) − _R(Ak(Sk)) + R(Ak(Sk)) −_ _R(w[∗])_
_∈_



_≤_ _kmin[K][(][R][(][A][k][(][S][k][))][ −]_ _[R][(][w][∗][)) + 2 max]k_ [K] _RS\Sk_ (Ak(Sk)) − _R(Ak(Sk))_
_∈_ _∈_

_ζ2_ _K log(K/δ)_
≲ _α[1]_ _Gγm, NK_ [+ ∆][opt] + M s (K 1)N [,]

  _−_


where in “ζ1” we have used the definition of k[∗], and “ζ2” follows from 1, 2. With leading terms
_E_ _E_
preserved in the above we can see that E occurs.

Based on the same proof argument as that of the part(b) of Theorem 1 we can show that
PS, _Ak_ ( 1) 2δ [. Further, from the part(b) of Lemma 7 we have][ P][S,][{][A][k][}][( ¯]2) 2δ [. Combin-]
_{_ _}_ _E_ _≤_ _E_ _≤_
ing this and the preceding bound yields

PS, _Ak_ ( ) PS, _Ak_ ( 1 2) 1 PS, _Ak_ ¯1 PS, _Ak_ ( [¯]2) 1
_{_ _}_ _E_ _≥_ _{_ _}_ _E_ _∩E_ _≥_ _−_ _{_ _}_  E  _−_ _{_ _}_ _E_ _≥_ _−_ 2[δ] _[−]_ 2 [δ] [= 1][ −] _[δ.]_

This implies the desired result in part(b) as K/(K − 1) ≤ 2 for K ≥ 2.


C PROOFS FOR THE RESULTS IN SECTION 3

In this section, we present the technical proofs for the main results stated in Section 3.

C.1 PROOF OF COROLLARY 1

We begin with presenting and proving the following lemma that gives the mean(-square)-uniform
stability bounds for ASGD-w on convex and smooth loss functions such as logistic loss.

**Lemma 8. Suppose that the loss function is ℓ(·; ·) is convex, G-Lipschitz and L-smooth with respect**
_to its first argument. Assume thatsuch that_ _ηt ≤_ 2/L for all t ≥ 1. Then ASGD-w has mean-uniform stability

sup _ηt,_
_S=[.]_ _S[′][ E][A][SGD-w][ [][∥][A][SGD-w][(][S][)][ −]_ _[A][SGD-w][(][S][′][)][∥][]][ ≤]_ [2]N[G] _tX∈[T ]_

_and has mean-square-uniform stability such that_

2[]

_T_ _T_

sup _ASGD-w(S)_ _ASGD-w(S[′])_ _ηt[2]_ [+ 1] _ηt_ _._
_S=[.]_ _S[′][ E][A][SGD-w]_ _∥_ _−_ _∥[2][]_ _≤_ [40]N[G][2] t=1 _N_ _t=1_ !
 X X

 

_Proof. The first mean-uniform stability bound can be straightforwardly derived based on the argu-_
ment of Hardt et al. (2016, Theorem 3.7). We focus on proving the second mean-square-uniform
stability bound. For any pair of S, S[′], let us define the sequences _wt_ _t_ [T ] and _wt[′][}]t_ [T ] [that are]
_{_ _}_ _∈_ _{_ _∈_
respectively generated over S and S[′] via ASGD-w via sample path ξ = _ξt_ _t_ [T ]. Note by assumption
_{_ _}_ _∈_
that w0 = w0[′] [. We distinguish the following two complementary cases.]

**Case I: zξt = zξ[′]** _t_ **[.][ In this case, by invoking Lemma 2 we immediately get]**


_wt_ _wt[′][∥][2][ =][∥][Π][W]_ [(][w][t][−][1] _t_ [))][ −] [Π][W] [(][w]t[′] 1 _t_ 1[;][ z]ξ[′] _t_ [))][∥][2]
_∥_ _−_ _[−]_ _[η][t][∇][w][ℓ][(][w][t][−][1][;][ z][ξ]_ _−_ _[−]_ _[η][t][∇][w][ℓ][(][w][′]−_

_wt_ 1 _ηt_ _wℓ(wt_ 1; zξt ) (wt[′] 1 _t_ 1[;][ z]ξ[′] _t_ [))][∥][2]
_≤∥_ _−_ _−_ _∇_ _−_ _−_ _−_ _[−]_ _[η][t][∇][w][ℓ][(][w][′]−_

_wt_ 1 _wt[′]_ 1[∥][2][.]
_≤∥_ _−_ _−_ _−_


(A.3)


-----

**Case II: zξt ̸= zξ[′]** _t_ **[.][ In this case, we have]**

_wt_ _wt[′][∥][2][ =][∥][Π][W]_ [(][w][t][−][1]
_∥_ _−_ _[−]_ _[η][t][∇][f]_ [(][w][))][ −] [Π][W] [(][w][′][ −] _[α][∇][f]_ [(][w][′][))][∥][2]

_wt_ 1 _ηt_ _wℓ(wt_ 1; zξt ) (wt[′] 1 _t_ 1[;][ z]ξ[′] _t_ [))][∥][2]
_≤∥_ _−_ _−_ _∇_ _−_ _−_ _−_ _[−]_ _[η][t][∇][w][ℓ][(][w][′]−_ 2

_wt_ 1 _wt[′]_ 1[∥] [+][ η][t][(][∥∇][w][ℓ][(][w][t][−][1][;][ z][ξ]t [)][∥] [+][ ∥∇][w][ℓ][(][w]t[′] 1[;][ z]ξ[′] _t_ [)][∥][)] (A.4)
_≤_ _∥_ _−_ _−_ _−_ 2 _−_

_≤_  ∥wt−1 − _wt[′]−1[∥]_ [+ 2][Gη][t] 

=∥ wt−1 − _wt[′]−1[∥][2][ + 4][Gη][t][∥][w][t][−][1]_ _[−]_ _[w]t[′]−1[∥]_ [+ 4][G][2][η]t[2][,]

where in the last but inequality we have used ℓ(·; ·) is G-Lipschitz with respect to its first argument.

Let βt = βt(S, S[′], ξ) := 1{zξt ≠ _zξt′_ _[}][ be the random indication function associated with event][ z][ξ][t][ ̸][=]_

_zξ[′]_ _t_ [. Based on the recursion forms equation A.3 and equation A.4 and the condition][ w][0][ =][ w]0[′] [we]
can show that for all t ≥ 1,


_wt_ _wt[′][∥][2][ ≤]_
_∥_ _−_


4Gβτ _ητ_ _wτ_ 1 _wτ[′]_ 1[∥] [+]
_τ_ =1 _∥_ _−_ _−_ _−_

X


4G[2]βτ _ητ[2][.]_
_τ_ =1

X


Then applying Lemma 3 with simple algebraic manipulation yields


_t_ _t_ 2[]

_wt_ _wt[′][∥][2][ ≤]_ [8][G][2] _βτ_ _ητ[2]_ [+ 4] _βτ_ _ητ_ _._
_∥_ _−_ τ =1 _τ_ =1 !

X X
 

Since by assumption S and S[′] differ only in a single element, under the scheme of uniform sampling
without replacement, we can see that βt(S, S[′], ξ) Bernoulli(1/N ) and _βt(S, S[′], ξ)_ is an
_∼_ _{_ _}_
i.i.d. sequence of Bernoulli random variables. It follows that

Eξ[t] _wt_ _wt[′][∥][2][]_
_∥_ _−_

_t_ _t_ 2[]



8G[2] Eξ[t] [βτ ]ητ[2] [+ 4][E][ξ][t] _βτ_ _ητ_
_≤_ τ =1  _τ_ =1 ! 

X X
  


_∥wt −_ _wt[′][∥][2][ ≤]_ [8][G][2]


_βτ_ _ητ[2]_ [+ 4]
_τ_ =1

X


=8G[2] Eξ[t] [βτ + 4βτ[2][]][η]τ[2] [+ 4] 1Eξ[t] [βτ _βτ ′_ ] ητ _ητ ′_

 

_τX=1_ _τX≠_ _τ_ _[′]_

 _t_ _t_ 2[] _T_  _T_ 2[]

=8G[2] _ητ[2]_ [+ 4] _ητ_ 40G[2] _ητ[2]_ [+ 1] _ητ_ _,_

 _N_ _τ_ =1 _N_ [2] _τ_ =1 ! _≤_  _N_ _τ_ =1 _N_ [2] _τ_ =1 !

X X X X

 [5]   [1] 

where we have used Eξt [βt] = Eξt [βt[2][] =][ 1]N [. The convexity of squared Euclidean norm leads to]

Eξ _w¯T_ _w¯T[′]_ _Tt=1_ [E][ξ][t] _∥wt −_ _wt[′][∥][2][]_ 40G[2] _T_ _ηt[2]_ [+ 1] _T_ _ηt_ 2[] _._
_∥_ _−_ _[∥][2][]_ _≤_ P T _≤_  _N_ _t=1_ _N_ [2] _t=1_ !
 X X

 [1] 

Note that the above holds for any S =[.] _S[′], i.e.,_

2[]

_T_ _T_

sup _w¯T_ _w¯T[′]_ _ηt[2]_ [+ 1] _ηt_ _._
_S=[.]_ _S[′][ E][ξ]_ _∥_ _−_ _[∥][2][]_ _≤_ [40]N[G][2] t=1 _N_ _t=1_ !
 X X

 

The proof is concluded.

With Lemma 8 in place, we are ready to prove Corollary 1.


_Proof of Corollary 1. From Lemma 8 we know that ASGD-w has mean-uniform stability with param-_
eter
_γm,N = [2][G]_ _ηt,_

_N_

_tX∈[T ]_


-----

and mean-square-uniform stability with parameter


2[]

_T_ _T_

_γm2,N = [40][G][2]_ _ηt[2]_ [+ 1] _ηt_ _._

_N_ t=1 _N_ _t=1_ !

X X

The desired results then follow immediately via invoking Theorem 1 with  _α = 1/2._


_γm2,N = [40][G][2]_


_T_

_ηt[2]_ [+ 1]

_N_

_t=1_

X


C.2 PROOF OF COROLLARY 2

We first establish the following lemma on the mean(-square)-uniform stability of ASGD-w in the case
of non-smooth convex loss.
**Lemma 9. Suppose that the loss function is ℓ(·; ·) is convex and G-Lipschitz with respect to its first**
_argument. Then ASGD-w has mean-uniform stability such that_


_T_

sup _ηt[2]_ [+ 4][G]
_S=[.]_ _S[′][ E][A][SGD-w][ [][∥][A][SGD-w][(][S][)][ −]_ _[A][SGD-w][(][S][′][)][∥][]][ ≤]_ [2][G]vut=1 _N_

uX

_and has mean-square-uniform stability such that_ t

_T_
sup _ASGD-w(S)_ _ASGD-w(S[′])_ 40G[2] _ηt[2]_ [+ 32][G][2]
_S=[.]_ _S[′][ E][A][SGD-w]_ _∥_ _−_ _∥[2][]_ _≤_ _t=1_ _N_ [2]
 X


_ηt,_
_t=1_

X

_T_

_ηt_
_t=1_

X


2
!


_Proof. Let us define the sequences_ _wt_ _t_ [T ] and _wt[′][}]t_ [T ] [that are respectively generated over][ S]
_{_ _}_ _∈_ _{_ _∈_
and S[′] via ASGD-w via sample path ξ = _ξt_ _t_ [T ]. Suppose that S =[.] _S[′]_ and consider a hitting time
_{_ _}_ _∈_
variable t0 = inf{t : zξt ̸= zξ[′] _t_ _[}][. Let][ β][t][ =][ β][t][(][S, S][′][, ξ][) :=][ 1]{zξt_ ≠ _zξt[′]_ _[}][ be the random indication]_

(2020, Lemma 3.1) thatfunction associated with event zξt ̸= zξ[′] _t_ [. Conditioned on][ t][0][, it has been shown by Bassily et al.]


_∥wt −_ _wt[′][∥≤]_ [2][G]


_βτ_ _ητ_ _._ (A.5)
_τ_ =1

X


_τ_ =t0 _ητ[2]_ [+ 4][G] _τ_ =t0+1 _βτ_ _ητ ≤_ 2G

X X


_ητ[2]_ [+ 4][G]
_τ_ =1

X


Then we can show the following for all t ≤ _T_

Eξ[t] [∥wt − _wt[′][∥][]][ ≤]_ [2][G]


_t_ _t_

_ητ[2]_ [+ 4][G] _ητ_ _,_

_N_

_τ_ =1 _τ_ =1

X X


where we have used the fact that _βt_ is an i.i.d. sequence of Bernoulli(1/N ) random variables.
_{_ _}_
The convexity of Euclidean norm leads to


_T_
_t=1_ [E][ξ][t] [[][∥][w][t][ −] _[w]t[′][∥][]]_

_T_

P


_T_

_ηt[2]_ [+ 4][G]

_N_

_t=1_

X


Eξ[T ] [∥w¯T − _w¯T[′]_ _[∥][]][ ≤]_


_≤_ 2G


_ηt,_
_t=1_

X


which is the first desired bound. Similarly, based on the square of the bound equation A.5 we can
show that

_t_ _t_ 2[]

Eξ[t] _wt_ _wt[′][∥][2][]_ Eξ[t] 8G[2] _ητ[2]_ [+ 32][G][2] _βτ_ _ητ_
_∥_ _−_ _≤_  _τ_ =1 _τ_ =1 !
 X X

t _t_ 

=8G[2] _ητ[2]_ [+ 32][G][2][E][ξ][t] _βτ[2][η]τ[2]_ [+] _βτ_ _βτ ′_ _ητ_ _ητ ′_

 

_τX=1_ _τX=1_ _τX≠_ _τ_ _[′]_

_t_  _t_ 
=8G[2] _ητ[2]_ [+ 32][G][2]  _N_ _ητ[2]_ [+ 1]N [2] _ητ_ _ητ ′_ 

_τX=1_ _τX=1_ _τX≠_ _τ_ _[′]_

_t_  [1] _t_ 2 
40G[2] _ητ[2]_ [+ 32][G][2] _ητ_ _,_
_≤_ _τ_ =1 _N_ [2] _τ_ =1 !
X X


-----

where we have used Eξt [βt] = Eξt [βt[2][] =][ 1]N [. It follows directly from the convexity of loss that]


2

_T_

_ηt_
_t=1_ !

X


Eξ[T ] _w¯T_ _w¯T[′]_ 40G[2]
∥ _−_ _[∥][2][]_ _≤_

The proof is concluded.


_T_

_ηt[2]_ [+ 32][G][2]

_N_ [2]

_t=1_

X


Equipped with Lemma 9, we are now in the position to prove Corollary 2.

_Proof of Corollary 2. From Lemma 9 we know that ASGD-w with non-smooth convex loss has_
mean(-square)-uniform stability with parameters


2

_T_

_ηt_
_t=1_ !

X


_T_

_ηt[2]_ [+ 32][G][2]

_N_ [2]

_t=1_

X


_T_

_ηt[2]_ [+ 4][G]

_N_

_t=1_

X


_ηt,_ _γm2,N = 40G[2]_
_t=1_

X


_γm,N = 2G_


The desired results then follow immediately via invoking Theorem 1 with α = 1/2.

C.3 PROOF OF COROLLARY 3

We first establish the following lemma on the mean(-square)-uniform stability of ASGD-w in the
considered non-convex regime.

**Lemma 10. Suppose that the loss function is ℓ(·; ·) is G-Lipschitz and L-smooth with respect to its**
_first argument. Consider ηt_ 1/L. Let
_≤_

_t−1_ _t−1_

_ut := ηt[2]_ [+ 2][η][t] exp _L_ _ηi_ _ητ_

_τ_ =1 _i=τ_ +1 !

X X

_for all t ≥_ 1. Then ASGD-w has mean-uniform stability such that


sup
_S=[.]_ _S[′][ E][A][SGD-w][ [][∥][A][SGD-w][(][S][)][ −]_ _[A][SGD-w][(][S][′][)][∥][]][ ≤]_ [2]N[G]

_and has mean-square-uniform stability such that_

sup _ASGD-w(S)_ _ASGD-w(S[′])_
_S=[.]_ _S[′][ E][A][SGD-w]_ _∥_ _−_ _∥[2][]_ _≤_ [4]N[G][2]



exp

_t=1_

X

_T_

exp

_t=1_

X


_ητ_
_τ_ =t+1

X


_ηt,_

_ut._

!


3L _ητ_

_τ_ =t+1

X


_Proof. Let us define the sequences_ _wt_ _t_ [T ] and _wt[′][}]t_ [T ] [that are respectively generated over]
_{_ _}_ _∈_ _{_ _∈_ _._
_S and S[′]_ via ASGD-w via sample path ξ = _ξt_ _t_ [T ]. Suppose that S = S[′]. Let us consider
_{_ _}_ _∈_
∆t := Eξ[t] [∥wt − _wt[′][∥][]][. Then based on the arguments of Hardt et al. (2016, Theorem 3.8) we]_
know that with probability 1 _−_ _N[1]_ [over][ ξ][t][,][ ∥][w][t][ −] _[w]t[′][∥≤]_ [(1+] _[η][t][L][)][∥][w][t][−][1]_ _[−]_ _[w]t[′]−1[∥][, and][ ∥][w][t]_ _[−]_ _[w]t[′][∥≤]_

_∥wt−1 −_ _wt[′]−1[∥]_ [+ 2][Gη][t] [with probability][ 1]N [. Therefore we have]

∆t 1 (1 + ηtL)∆t 1 + [1]
_≤_ _−_ _N[1]_ _−_ _N_ [(∆][t][−][1][ + 2][Gη][t][)]
 

= 1 (1 + ηtL) + [1] ∆t 1 + [2][Gη][t]
_−_ _N[1]_ _N_ _−_ _N_
  

= 1 + 1 _ηtL_ ∆t 1 + [2][Gη][t]
_−_ _N[1]_ _−_ _N_
   

exp 1 _ηtL_ ∆t 1 + [2][Gη][t]
_≤_ _−_ _N[1]_ _−_ _N_
  

exp (ηtL) ∆t 1 + [2][Gη][t] _,_
_≤_ _−_ _N_


-----

where we have used 1 + x ≤ exp(x). Then we can unwind the above recursion form to obtain that
for all t ≥ 1,


2Gητ


= [2][G]


_ητ_ _,_ (A.6)


∆t
_≤_


exp (ηiL)
_i=τ_ +1

Y


exp

_τ_ =1

X


_ηi_
_i=τ_ +1

X


_τ_ =1


where we have used ∆0 = 0. The convexity of Euclidean norm leads to

_T_ _T_
_t=1_ [E][ξ][t] [[][∥][w][t][ −] _[w]t[′][∥][]]_

Eξ[T ] [ _w¯T_ _w¯T[′]_ exp
_∥_ _−_ _[∥][]][ ≤]_ P _T_ _≤_ [2]N[G] _t=1_

X


_ητ_
_τ_ =t+1

X


_ηt,_


which immediately implies the first desired bound as it holds for all S =[.] _S[′]._

To show the mean-square-uniform stability bound, let us consider ∆[˜] _t := Eξ[t]_ _wt_ _wt[′][∥][2][]. Then_
_∥_ _−_
we can verify that with probability 1 − _N[1]_ [over][ ξ][t][,][ ∥][w][t][ −] _[w]t[′][∥][2][ ≤]_ [(1 +][ η][t][L][)][2][∥][w] _[t][−][1]_ _[−]_ _[w]t[′]−1[∥][2][, and]_

with probability _N[1]_ [,]

_wt_ _wt[′][∥][2][ ≤]_ [(][∥][w][t][−][1] _t_ 1[∥] [+ 2][Gη][t][)][2][ =][ ∥][w][t][−][1] _t_ 1[∥][2][ + 4][Gη][t][∥][w][t][−][1] _t_ 1[∥] [+ 4][G][2][η]t[2][.]
_∥_ _−_ _[−]_ _[w][′]−_ _[−]_ _[w][′]−_ _[−]_ _[w][′]−_

Therefore we have


∆˜ _t_ 1
_≤_ _−_ _N[1]_


1
_≤_ _−_ _N[1]_



(1 + ηtL)[2][ ˜]∆t 1 + [1]
_−_ _N_


∆˜ _t−1 + 4Gηt∆t−1 + 4G[2]ηt[2]_


_t−1_

exp

_τ_ =1

X

_ut_
{z


_t−1_

_ηi_
_i=τ_ +1

X


∆˜ _t_ 1 + [4][G][2]
_−_ _N_


(1 + ηtL)[2] + [1]


_ηt[2]_ [+ 2][η][t]


_ητ_


(2ηtL + ηt[2][L][2][)] ∆˜ _t_ 1 + [4][G][2][u][t]
_−_ _N_


(2ηtL + ηt[2][L][2][)] ∆˜ _t_ 1 + [4][G][2][u][t]
_−_ _N_



= 1 + 1 (2ηtL + ηt[2][L][2][)] ∆˜ _t_ 1 + [4][G][2][u][t]
_−_ _N[1]_ _−_ _N_
   

exp 1 (2ηtL + ηt[2][L][2][)] ∆˜ _t_ 1 + [4][G][2][u][t]
_≤_ _−_ _N[1]_ _−_ _N_
  

exp 2ηtL + ηt[2][L][2][][ ˜]∆t 1 + [4][G][2][u][t] _,_
_≤_ _−_ _N_
 

where in the second inequality we have used the bound equation A.6 on ∆t. Recall that ∆[˜] 0 = 0.
Then we can unwind the above recursion form to obtain


_t_ _t_ _t_ _t_

∆˜ _t_ exp 2ηiL + ηi[2][L][2][] _uτ_ exp 3L _ηi_
_≤_ [4]N[G][2] _τ_ =1 (i=τ +1 ) _≤_ [4]N[G][2] _τ_ =1 _i=τ_ +1

X Y   X X

where we have used ηt 1/L. It follows immediately from the convexity that
_≤_

Eξ[T ] _∥w¯T −_ _w¯T[′]_ _[∥][2][]_ _≤_ PTt=1 [E][ξ][t] T∥wt − _wt[′][∥][2][]_ _≤_ [4]N[G][2] _t=1T_ exp 3L _τ_ =Tt+1 _ητ_
 X X

which is the second desired bound. The proof is completed.


_uτ_ _,_

_ut,_

!


With Lemma 10 in place, we proceed to prove the main result in Corollary 3.

_Proof of Corollary 3. From Lemma 10 we know that ASGD-w with smooth non-convex loss has_
mean(-square)-uniform stability with parameters


_ηt,_ _γm2,N = [4][G][2]_


_γm,N = [2][G]_


exp

_t=1_

X


_ητ_
_τ_ =t+1

X


exp

_t=1_

X


3L _ητ_

_τ_ =t+1

X


_ut._


The desired results then follow immediately via invoking Theorem 1 with α = 1/2.


-----

**Algorithm 3: SGD via Without-Replacement Sampling (ASGD-wo)**

i.i.d.
**Input : Data set S = {zi}i∈[N** ] _∼D[N]_, step-sizes {ηt}t≥1, #iterations T, initialization w0.
**Output: ¯wT =** _T[1]_ _t_ [T ] _[w][t][.]_

_∈_
**for t = 1, 2, ..., T do**

P

Uniformly randomly sample an index ξt [N ] with or without replacement;
Compute wt = Π (wt 1 _ηt_ _wℓ(wt ∈1; zξt_ )).
_W_ _−_ _−_ _∇_ _−_
**end**


D AUGMENTED RESULTS FOR SGD UNDER WITHOUT-REPLACEMENT
SAMPLING

In this section, we further consider applying our main results in Theorem 1 to the variant of SGD
under without-replacement sampling (ASGD-wo), as is outlined in Algorithm 3. For the sake of simplicity and readability, we only consider single-epoch processing with T ≤ _N_, and we focus on the
case where the loss is convex but non-smooth. The extensions of our analysis to multi-epoch processing, i.e., T ≤ _rN for some integer r ≥_ 1, and to convex or non-convex smooth loss functions
are more or less straightforward and thus are omit.

We start by establishing the following lemma on the mean(-square)-uniform stability of ASGD-wo
which can be easily proved based on the result from Bassily et al. (2020, Lemma 3.1).
**Lemma 11. Suppose that the loss function is ℓ(·; ·) is convex and G-Lipschitz with respect to its first**
_argument. Consider T ≤_ _N_ _. Then ASGD-wo has mean-uniform stability such that_


sup
_S=[.]_ _S[′][ E][A][SGD-wo][ [][∥][A][SGD-wo][(][S][)][ −]_ _[A][SGD-wo][(][S][′][)][∥][]][ ≤]_ [2]N[G]

_and has mean-square-uniform stability such that_


_ηt[2][,]_
_t=t0_

X

_T_

_ηt[2][.]_
_t=t0_

X


_t0=1_


sup _ASGD-wo(S)_ _ASGD-wo(S[′])_
_S=[.]_ _S[′][ E][A][SGD-wo]_ _∥_ _−_ _∥[2][]_ _≤_ [4]N[G][2]



_t0=1_


_Proof. Let ¯wT (S, ξ) and ¯wT (S[′], ξ) respectively be the output generated over S =_ _zi_ _i_ [N ] and
_{_ _}_ _∈_
_S[′]_ = {zi[′][}][i][∈][[][N] []] [by][ A][SGD-wo][ via sample path][ ξ][ =][ {][ξ][t][}][t][∈][[][T][ ]][. Recall that][ T][ ≤] _[N]_ [. Let us define a]
hitting time variable t0 = inf _t : zξt_ = zξ[′] _t_ _[}][. Since][ S .]= S[′], the uniform randomness of ξt implies_
that _{_ _̸_
P (t0 = j) = N [1] _[,]_ _j ∈_ [N ].

Given t ∈ [T ], it follows from Bassily et al. (2020, Lemma 3.1) that


_∥wt −_ _wt[′][∥][2][ ≤]_ [4][G][2]


_ητ[2][.]_
_τ_ =t0

X


Then we have

_t_ _t_ _T_ _T_

Eξ[t] _wt_ _wt[′][∥][2][]_ _ητ[2]_ _ητ[2][.]_
_∥_ _−_ _≤_ [4]N[G][2] _t0=1_ _τ_ =t0 _[≤]_ [4]N[G][2] _t0=1_ _τ_ =t0
 X X X X

The convexity of squared Euclidean norm leads to

Eξ[T ] _w¯T_ _w¯T[′]_ _Tt=1_ [E][ξ][t] _∥wt −_ _wt[′][∥][2][]_ _T_ _T_ _ηt[2][.]_
_∥_ _−_ _[∥][2][]_ _≤_ P T _≤_ [4]N[G][2] _t0=1_ _t=t0_
 X X

Similarly we can show


Eξ[T ] [ _w¯T_ _w¯T[′]_
_∥_ _−_ _[∥][]][ ≤]_ [2]N[G]


_ηt[2][.]_
_t=t0_

X


_t0=1_


The proof is completed.


-----

The following result is a direct consequence of Theorem 1 when invoking Algorithm 1 to ASGD-wo
with non-smooth convex loss.

**Corollary 5. Suppose that the loss function is ℓ(·; ·) is convex and G-Lipschitz with respect to its**
_first argument, and is bounded in the range of [0, M_ ]. Consider Algorithm 1 specified to ASGD-wo
_with T ≤_ _N_ _. Then for any δ ∈_ (0, 1) and K ≥ 2 log( [4]δ [)][, with probability at least][ 1][ −] _[δ][ over the]_

_randomness of S and_ _ASGD-wo,k_ _k_ [K], the output of ASGD-wo satisfies
_{_ _}_ _∈_

_R(ASGD-wo,k[∗]_ (Sk[∗] )) _RS(ASGD-wo,k[∗]_ (Sk[∗] ))
_|_ _−_ _|_

_T_ _T_ _T_ _T_

[1] log(K/δ)

≲G[2] _ηt[2]_ [+][ G][2] _ηt[2]_ [+][ M] _._

v _N_ _N_ v _N_
u _t0=1_ _t=t0_ _t0=1_ ut=t0 r
u X X X uX
t t

_Proof. From Lemma 11 we know that ASGD-wo has mean(-square)-uniform stability with parameters_


_T_

_ηt[2][,]_ _γm2,N = [4][G][2]_

_N_

_t=t0_

X


_γm,N = [2][G]_


_ηt[2][.]_
_t=t0_

X


_t0=1_


_t0=1_


The results then follow immediately via invoking Theorem 1 with α = 1/2.

**Remark 9. Specially for constant learning rates ηt ≡** _η, Corollary 5 admits a high probability_

_generalization bound of scale O_ _√ηTN_ [+][ η][ T] _N√T_ + log(1N/δ) _. For general time varying learning_
 q 

_rates, our bound in Corollary 5 still holds with high probability. For example, when ηt ∝_ [1]t _[, the]_

_generalization bound scales as O_ log(NT ) + _√NT_ [+] log(1N/δ) _with high probability._
q q 


-----

