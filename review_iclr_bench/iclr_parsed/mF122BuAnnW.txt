# LOCALIZED RANDOMIZED SMOOTHING
## FOR COLLECTIVE ROBUSTNESS CERTIFICATION

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Models for image segmentation, node classification and many other tasks map a
single input to multiple labels. By perturbing this single shared input (e.g. the
image) an adversary can manipulate several predictions (e.g. misclassify several
pixels). A recent collective robustness certificate provides strong guarantees on
the number of predictions that are simultaneously robust. This method is however
limited to strictly local models, where each prediction is associated with a small
receptive field. We propose a more general collective certificate for the larger class
of softly local models, where each output is dependent on the entire input but assigns different levels of importance to different input regions (e.g. based on their
proximity in the image). The certificate is based on our novel localized randomized smoothing approach, where the random perturbation strength for different
input regions is proportional to their importance for the outputs. The resulting locally smoothed model yields strong collective guarantees while maintaining high
prediction quality on both image segmentation and node classification tasks.

1 INTRODUCTION

There is a wide range of tasks that require models making multiple predictions based on a single input. For example, semantic segmentation requires assigning a label to each pixel in an image. When
deploying such multi-output classifiers in practice, their robustness should be a key concern. After
all – just like simple classifiers (Szegedy et al., 2014) – they can fall victim to adversarial attacks
(Xie et al., 2017; Z¨ugner & G¨unnemann, 2019; Belinkov & Bisk, 2018). Even without an adversary,
random noise or measuring errors could cause one or multiple predictions to unexpectedly change.

In the following, we derive a method that provides provable guarantees on how many predictions
can be changed by an adversary. Since all outputs operate on the same input, they also have to be
attacked simultaneously by choosing a single perturbed input. While attacks on a single prediction
may be easy, attacks on different predictions may be mutually exclusive. We have to explicitly
account for this fact to obtain a proper collective robustness certificate that provides tight bounds.

There already exists a dedicated collective robustness certificate for multi-output classifiers
(Schuchardt et al., 2021), but it is only benefical for models we call strictly local, where each output
depends only on a small, well-defined subset of the input. One example are graph neural networks
that classify each node in a graph based only on its neighborhood. Multi-output classifiers used in
practice, however, are often only softly local. While – unlike strictly local models – all of their predictions are in principle dependent on the entire input, each output may assign different importance
to different components. For example, deep convolutional networks used for image segmentation
can have very small effective receptive fields (Luo et al., 2016; Liu et al., 2018b), i.e. primarily
use a small region of the input in labeling each pixel. Many models used in node classification are
based on the homophily assumption that connected nodes are mostly of the same class. Thus, they
primarily use features from neighboring nodes to classify each node. Even if an architecture is not
inherently softly local, a model may learn a softly local mapping through training. For example, a
transformer (Vaswani et al., 2017) can in principle attend to any part of an input sequence. However,
in practice the learned attention maps may be ”sparse”, with the prediction for each token being
determined primarily by a few (not necessarily nearby) tokens (Shi et al., 2021).

While an adversarial attack on a single prediction of a softly local model is conceptually no different
from that on a single-output classifier, attacking multiple predictions simultaneously can be much


-----

Figure 1: Localized randomized smoothing applied to semantic segmentation. We assume that the
most relevant information for labeling a pixel is contained in other nearby pixels. We partition the
input image into multiple grid cells. For each grid cell, we sample noisy images from a localized
smoothing distribution that applies more noise to far-away, less relevant grid cells. Segmenting all
noisy images using base model g, cropping the result and computing the majority vote (i.e. the most
common label for each pixel) yields a local segmentation mask. These per-cell segmentation masks
can then be combined into a complete segmentation mask.

more challenging. By definition, adversarial attacks have to be unnoticeable, meaning the adversary
only has a limited budget for perturbing the input. When each output is focused on a different part of
the input, the adversary has to decide on where to allocate their adversarial budget and may be unable
to attack all outputs at once. Our collective robustness certificate explicitly accounts for this budget
allocation problem faced by the adversary and can thus provide stronger robustness guarantees.

Our certificate is based on randomized smoothing (Liu et al., 2018a; L´ecuyer et al., 2019; Cohen
et al., 2019). Randomized smoothing is a versatile black-box certification method that has originally
been proposed for single-output classifiers. Instead of directly analysing a model, it constructs a
smoothed classifier that returns the most likely prediction of the model under random perturbations
of its input. One can then use statistical methods to certify the robustness of this smoothed classifier.
We discuss more details in Section 2. Randomized smoothing is typically used with i.i.d. noise:
Each part of the input (e.g. each pixel) independently undergoes random perturbations sampled
from the same noise distribution. One can however also use non-i.i.d. noise (Eiras et al., 2021). This
results in a smoothed classifier that is certifiably more robust to parts of the input that are smoothed
with higher noise levels (e.g. larger standard deviation).

We apply randomized smoothing to softly-local multi-output classifiers in a scheme we call local_ized randomized smoothing: Instead of using the same smoothing distribution for all outputs, we_
randomly smooth each output (or set of outputs) using a different non-i.i.d. distribution that matches
its inherent soft locality. Using a low noise level for the most relevant parts of the input allows us
to retain a high prediction quality (e.g. accuracy). Less relevant parts of the input can be smoothed
with a higher noise level. The resulting certificates (one per output) explicitly quantify how robust
each prediction is to perturbations of which section of the input – they are certificates of soft locality.

After certifying each prediction independently using localized randomized smoothing, we construct
a (mixed-integer) linear program that combines these per-prediction base certificates into a collective
certificate that provably bounds the number of simultaneously attackable predictions. This linear
program explicitly accounts for soft locality and the budget allocation problem it causes for the
adversary. This allows us to prove much stronger guarantees of collective robustness than simply
certifying each prediction independently. Our core contributions are:

-  Localized randomized smoothing, a novel smoothing scheme for multi-output classifiers.

-  A variance smoothing method for efficiently certifying smoothed models on discrete data.

-  A collective certificate that leverages our identified common interface for base certificates.

2 BACKGROUND AND RELATED WORK

**Randomized smoothing.** Randomized smoothing is a flexible certification technique that can
be used for various data types, perturbation models and tasks. For simplicity, we focus on a


-----

classification certificate for l2 perturbations (Cohen et al., 2019). Assume we have a continuous D-dimensional input space R[D], a label set Y and a classifier g : R[D] _→_ Y. We can use
isotropic Gaussian noise with standard deviation σ ∈ R+ to construct the smoothed classifier
_f = argmaxy∈Y Prz∼N (x,σ) [g(z) = y] that returns the most likely prediction of base classifier_
_g under the input distribution_ [1]. Given an input x ∈ R[D] and the smoothed prediction y = f (x), we
want to determine whether the prediction is robust to all l2 perturbations of magnitude ϵ, i.e. whether
**_x[′]_** : **_x[′]_** **_x_** 2 _ϵ : f_ (x[′]) = y. Let q = Prz (x,σ) [g(x) = y] be the probability of g predicting
_∀_ _||_ _−_ _||_ _≤_ _∼N_
label y. The prediction of our smoothed classifier is robust if ϵ < σΦ[−][1](q) (Cohen et al., 2019).
This result showcases a trade-off we alluded to in the previous section: The certificate can become
stronger if the noise-level (here σ) is increased. But doing so could also lower the accuracy of the
smoothed classifier or reduce q and thus weaken the certificate.

**White-box certificates for multi-output classifiers. There are multiple recent methods for certify-**
ing the robustness of specific multi-output models (see, for example, (Tran et al., 2021; Z¨ugner &
G¨unnemann, 2019; Bojchevski & G¨unnemann, 2019; Z¨ugner & G¨unnemann, 2020; Ko et al., 2019;
Ryou et al., 2021; Shi et al., 2020; Bonaert et al., 2021)) by analyzing their specific architecture and
weights. They are however not designed to certify collective robustness. They can only determine
independently for each prediction whether or not it can be adversarially attacked.

**Collective robustness certificates. Most directly related to our work is the certificate of Schuchardt**
et al. (2021). Like ours, it combines many per-prediction certificates into a collective certificate.
But, unlike our novel localized smoothing approach, their certification procedure is only beneficial
for strictly local models, i.e. models whose outputs operate on small subsets of the input. Furthermore, their certificate assumes binary data, while our certificate defines a common interface for
various data types and perturbation models. A more detailed comparison can be found in Section D.
Recently, Fischer et al. (2021) proposed a certificate for semantic segmentation. They consider a different notion of collective robustness: They are interested in determining whether all predictions are
robust. In Section C.4 we discuss their method in detail and show that, when used for certifying our
notion of collective robustness (i.e. the number of robust predictions), their method is no better than
certifying each output independently using the certificate of Cohen et al. (2019). Furthermore, our
certificate can be used to provide equally strong guarantees for their notion of collective robustness
by checking whether the number of certified predictions equals the overall number of predictions.
Another method that can be used for certifying collective robustness is center smoothing (Kumar &
Goldstein, 2021). Center smoothing bounds how much a vector-valued prediction changes w.r.t to a
distance function under adversarial perturbations. With the l0 pseudo-norm as the distance function,
center smoothing bounds how many predictions of a classifier can be simultaneously changed.

**Randomized smoothing with non-i.i.d. noise. While not designed for certifying collective ro-**
bustness, two recent certificates for non-i.i.d. Gaussian (Fischer et al., 2020) and uniform smoothing (Eiras et al., 2021) can be used as a component of our collective certification approach: They
can serve as per-prediction base certificates, which can then be combined into our stronger collective certificate (more details in Section 4) . Note that we do not use the procedure for optimizing the
smoothing distribution proposed by Eiras et al. (2021), as this would enable adversarial attacks on
the smoothing distribution itself and invalidate the certificate (see discussion by Wang et al. (2021)).

3 COLLECTIVE THREAT MODEL

Before certifying robustness, we have to define a threat model, which specifies the type of model
that is attacked, the objective of the adversary and which perturbations they are allowed to use. We
assume that we have a multi-output classifier f : X[D][in] _→_ Y[D][out], that maps from a Din-dimensional
vector space to Dout labels from label set Y. We further assume that this classifier f is the result of
randomly smoothing a base classifier g, as discussed in Section 2. To simplify our notation, we write
_fn to refer to the function x 7→_ _f_ (x)n that outputs the n-th label. Given this multi-output classifier
_f_, an input x ∈ X[D][in] and the resulting vector of predictions y = f (x), the objective of the adversary
is to cause as many predictions from a set of targeted indices T ⊆{1, . . ., Dout} to change. That
is, their objective is minx′∈Bx _n∈T_ [I [][f][n][(][x][′][) =][ y][n][]][, where][ B][x][ ⊆] [X][D][in][ is the perturbation model.]

Importantly, note that the minimization operator is outside the sum, meaning the predictions have to

P

1In practice, all probabilities have to be estimated using Monte Carlo sampling (see discussion in Section C).


-----

be attacked using a single input. As is common in robustness certification, we assume a norm-bound
perturbation model. That is, given an input x ∈ X[D][in], the adversary is only allowed to use perturbed
inputs from the set Bx = **_x[′]_** _∈_ X[D][in] _| ||x[′]_ _−_ **_x||p ≤_** _ϵ_ with p, ϵ ≥ 0.


4 A RECIPE FOR COLLECTIVE CERTIFICATES

Before discussing technical details, we provide a high-level overview of our method. In localized
randomized smoothing, we assign each output gn of a base classifier g its own smoothing distribution
Ψ[(][n][)] that matches our assumptions or knowledge about the base classifier’s soft locality, i.e. for each
_n_ 1, . . ., Dout choose a Ψ[(][n][)] that induces more noise in input components that are less relevant
_∈{_ _}_
for gn. For example, in Fig. 1, we assume that far-away regions of the image are less relevant and
thus perturb pixels in the bottom left with more noise when classifying pixels in the top-right corner.
The chosen smoothing distributions can then be used to construct the smoothed classifier f .

Given an input x ∈ X[D][in] and the corresponding smoothed prediction y = f (x), randomized
smoothing makes it possible to compute per-prediction base certificates. That is, for each yn, one
can compute a set H[(][n][)] _⊆_ X[D][in] of perturbed inputs that the prediction is robust to, i.e. ∀x[′] _∈_ Hn :
_fn(x[′]) = yn. Our motivation for using non-i.i.d. distributions is that the H[(][n][)]_ will guarantee more
robustness for input dimensions smoothed with more noise, i.e. quantify model locality.

The objective of our adversary is minx′∈Bx _n∈T_ [I [][f][n][(][x][′][) =][ y][n][]][ with collective perturbation]

model Bx X[D][in]. That is, they want to change as many predictions from the targeted set T as
possible. A trivial lower bound can be obtained by counting how many predictions are – according ⊆ P
to the base certificates – provably robust to the collective threat model. This can be expressed as

_n_ T [min][x][′][∈][B][x] [I] **_x[′]_** H[(][n][)][]. In the following, we refer to this as the na¨ıve collective certificate.
_∈_ _∈_

Thanks to our proposed localized smoothing scheme, we can use the following, tighter bound:P 

min I [fn(x[′]) = yn] min I **_x[′]_** H[(][n][)][i] _,_ (1)
**_x[′]∈Bx_** _nX∈T_ _≥_ **_x[′]∈Bx_** _nX∈T_ h _∈_

which preserves the fact that the adversary has to choose a single perturbed input. Because we use
different non-i.i.d. smoothing distributions for different outputs, we provably know that each fn has
varying levels of robustness for different parts of the input and that these robustness levels differ
among outputs. Thus, in the r.h.s. problem the adversary has to allocate their limited budget across
various input dimensions and may be unable to attack all predictions at once, just like when attacking
the classifier in the l.h.s. objective (recall Section 1). This makes our collective certificate stronger
than the na¨ıve collective certificate, which allows each prediction to be attacked independently.

As stated in Section 1, the idea of combining base certificates into stronger collective certificates
has already been explored by Schuchardt et al. (2021). But instead of using localized smoothing
to capture the (soft) locality of a model, their approach leverages the fact that perturbations outside
an output’s receptive field can be ignored. For softly local models, which have receptive fields
covering the entire input, their certificate is no better than the na¨ıve certificate. Another novel
insight underlying our approach is that various non-i.i.d. randomized smoothing certificates share
a common interface, which makes our method applicable to diverse data types and perturbation
models. In the next section, we formalize this common interface. We then discuss how it allows us
to compute the collective certificate from Eq. 1 using (mixed-integer) linear programming.

5 COMMON INTERFACE FOR BASE CERTIFICATES

A base certificate for a predictionprovably robust to, i.e ∀x[′] _∈_ Hn : f ynn( =x[′] f) =n( yxn) is a set. Note that base certificates do not have to be exact, Hn ⊆ X[D][in] of perturbed inputs that yn is
but have to be sound, i.e. they do not have to specify all inputs to which the fn are robust but they
must not contain any adversarial examples. As a common interface for base certificates, we propose
that the sets Hn are parameterized by a weight vector w[(][n][)] _∈_ R[D][in] and a scalar η[(][n][)] that define a
linear constraint on the element-wise distance between perturbed inputs and the clean input:


_Din_

_wd[(][n][)]_ _x[′]d_
_d=1_ _· |_ _[−]_ _[x][d][|][κ][ < η][(][n][)]_

X


H[(][n][)] =


**_x[′]_** _∈_ X[D][in]


(2)


-----

The weight vector encodes how robust yn is to perturbations of different components of the input.
The scalar κ is important for collective robustness certification, because it encodes which collective
perturbation model the base certificate is compatible with. For example, κ = 2 means that the base
certificate can be used for certifying collective robustness to l2 perturbations.

In the following, we present two base certificates implementing our interface: One for l2 perturbations of continuous data and one for perturbations of binary data. In Section B, we further present a
certificate for binary data that can distinguish between adding and deleting bits and a certificate for
_l1 perturbations of continuous data. All base certificates guarantee more robustness for parts of the_
input smoothed with a higher noise level. The certificates for continuous data are based on known
results (Fischer et al., 2020; Eiras et al., 2021) and merely reformulated to match our proposed interface, so that they can be used as part of our collective certification procedure. The certificates for
discrete data however are original and based on the novel concept of variance smoothing.

**Gaussian smoothing for l2 perturbations of continuous data The first base certificate is a gen-**
eralization of Gaussian smoothing to anisotropic noise, a corollary of Theorem A.1 from (Fischer
et al., 2020). In the following, diag(z) refers to a diagonal matrix with diagonal entries z and
Φ[−][1] : [0, 1] → R refers to the the standard normal inverse cumulative distribution function.
**Proposition 1. Given an output gn : R[D][in]** _→_ Y, let fn(x) = argmaxy∈Y Prz∼N (x,Σ) [gn(z) = y]
_be the corresponding smoothed output with Σ = diag (σ)[2]_ _and σ ∈_ R[D]+[in][. Given an input][ x][ ∈] [R][D][in]

_and smoothed prediction yn = fn(x), let q = Prz_ (x,Σ) [gn(z) = yn]. Then, **_x[′]_** H[(][n][)] :

_fn(x[′]) = yn with H[(][n][)]_ _defined as in Eq. 2, wd =_ _σ1d_ [2][,]∼N[ η][ =] Φ[(][−][1)](q) 2 and κ = 2. ∀ _∈_

**Bernoulli variance smoothing for perturbations of binary data ** For binary data, we use a smooth-
ing distribution (x, θ) with θ [0, 1][D][in] that independently flips the d’th bit with probability θd,
_F_ _∈_
i.e. for x, z 0, 1 and z (x, θ) we have Pr[zd = xd] = θd. A corresponding certificate
could be derived by generalizing (Lee et al., 2019), which considers a single shared ∈{ _}[D][in]_ _∼F_ _̸_ _θ ∈_ [0, 1] with
_∀d : θd = θ. However, the cost for computing this certificate would be exponential in the number_
of unique values in θ. We therefore propose a more efficient alternative. Instead of constructing
a smoothed classifier that returns the most likely labels of the base classifier (as discussed in Section 2), we construct a smoothed classifier that returns the labels with the highest expected softmax
scores (similar to CDF-smoothing (Kumar et al., 2020)). For this smoothed model, we can compute a robustness certificate in constant time. The certificate requires determining both the expected
value and variance of softmax scores. We therefore call this method variance smoothing. While we
use it for binary data, it is a general-purpose technique that can be applied to arbitrary domains and
smoothing distributions (see discussion in Section B.2). In the following, we assume the label set Y
to consist of numerical labels {1, . . ., |Y|}, which simplifies our notation.
**Theorem 1. Given an output gn : {0, 1}[D][in]** _→_ ∆|Y| mapping to scores from the |Y|-dimensional
_probability simplex, let fn(x) = argmaxy∈YEz∼F_ (x,θ) [gn(z)y] be the corresponding smoothed
_classifier with θ ∈_ [0, 1][D][in]. Given an input x ∈{0, 1}[D][in] _and smoothed prediction yn = fn(x),_
_let µ = Ez_ (x,θ) [gn(z)y] and σ[2] = Varz (x,θ) [gn(z)y]. Then, **_x[′]_** H[(][n][)] : fn(x[′]) = yn
_with H[(][n][)]_ _defined as in Eq. 2,∼F_ _wd = ln_ (1−θθdd∼F)[2] + 1[(][θ][d]θ[)]d[2] _, η = ln_ 1 + ∀σ1[2] _∈µ_ 2 2[] _and κ = 0._

_−_ _−_ [1]

     

6 COMPUTING THE COLLECTIVE ROBUSTNESS CERTIFICATE

With our common interface for base certificates in place, we can discuss how to compute the collective robustness certificate minx′∈Bx _n∈T_ [I] **_x[′]_** _∈_ H[(][n][)][] from Eq. 1. The result bounds the number

of predictions yn with n 1, . . ., Dout that can be simultaneously attacked by the adversary. In
_∈{_ P _}_ 
the following, we assume that the base certificates were obtained by using a smoothing distribution
that is compatible with our lp collective perturbation model (i.e. κ = p), for example by using Gaussian noise for p = 2 or Bernoulli noise for p = 0. Inserting the definition of our base certificate
interface from Eq. 2 and rewriting our perturbation model Bx = **_x[′]_** _∈_ X[D][in] _| ||x[′]_ _−_ **_x||p ≤_** _ϵ_ as
**_x[′]_** X[D][in] _d=1_ _d_, our objective from Eq. 1 can be expressed as
_∈_ _|_ _[|][x][′]_ _[−]_ _[x][d][|][p][ ≤]_ _[ϵ][p][o]_
n _Din_ _Din_

[P]min[D][in] I _wd[(][n][)]_ _x[′]d_ s.t. _x[′]d_ (3)

**_x[′]∈X[D][in]_** _nX∈T_ "Xd=1 _· |_ _[−]_ _[x][d][|][p][ < η][(][n][)]#_ Xd=1 _|_ _[−]_ _[x][d][|][p][ ≤]_ _[ϵ][p][.]_


-----

We can see that the perturbed input x[′] only affects the element-wise distances |x[′]d[−][x][d][|][p][. Rather than]
optimizing x[′], we can instead directly optimize these distances, i.e. determine how much adversarial
budget is allocated to each input dimension. For this, we define a vector of variables b R[D]+[in] (or
_∈_
**_b ∈{0, 1}[D][in]_** for binary data). Replacing sums with inner products, we can restate Eq. 3 as

min I **_b[T]_** **_w[(][n][)]_** _< η[(][n][)][i]_ s.t. sum **_b_** _ϵ[p]._ (4)
**_b∈RD+in_** _nX∈T_ h _{_ _} ≤_

In a final step, we replace the indicator functions in Eq. 4 with a vector of boolean variables t ∈
0, 1 . Define the constants η[(][n][)] = ϵ[p] min 0, mind wd[(][n][)] . Then,
_{_ _}[D][out]_ _·_
 

min _tn_ s.t. _n : b[T]_ **_w[(][n][)]_** _tnη[(][n][)]_ + (1 _tn)η[(][n][)],_ sum **_b_** _ϵ[p]._ (5)
**_b∈RD+in_** _,t∈{0,1}[D][out]_ _nX∈T_ _∀_ _≥_ _−_ _{_ _} ≤_

is equivalent to Eq. 4. The first constraint guarantees that tn can only be set to 0 if the l.h.s. is
greater or equal η[(][n][)], i.e. only when the base certificate can no longer guarantee robustness. The
term involving η[(][n][)] ensures that for tn = 1 the problem is always feasible[2]. Eq. 5 can be solved
using any mixed-integer linear programming solver.

While the resulting MILP bears some semblance to that of Schuchardt et al. (2021), it is conceptually
different. When evaluating their base certificates, they mask out parts of the budget vector b based
on a model’s strict locality, while we weigh the budget vector based on the soft locality guaranteed
by the base certificates. In addition, thanks to the interface specified in Section 5, our problem
only involves a single linear constraint per prediction, making it much smaller and more efficient
to solve. Interestingly, when using randomized smoothing base certificates for binary data, our
certificate subsumes theirs, i.e. can provide the same robustness guarantees (see Section D.2).

**Improving efficiency. Still, the efficiency of our certificate in Eq. 5. certificate can be further**
improved. In Section A, we show that partitioning the outputs into Nout subsets sharing the same
smoothing distribution and the the inputs into Nin subsets sharing the same noise level (for example
like in Fig. 1), as well as quantizing the base certificate parameters η[(][n][)] into Nbin bins reduces
anddimensionality. We further derive a linear relaxation of the mixed-integer problem, which can bethe number of variables and constraints from Nout · Nbins + 1, respectively.We can thus control the problem size independent of the data’s Din + Dout and Dout + 1 to Nin + Nout · Nbins
more efficiently solved while preserving the soundness of the certificate.

7 LIMITATIONS

The main limitation of our approach is that it assumes softly local models. While it can be applied to
arbitrary multi-output classifiers, it may not necessarily result in better certificates than randomized
smoothing with i.i.d. distributions. Furthermore, choosing the smoothing distributions requires some
a-priori knowledge or assumptions about which parts of the input are how relevant to making a
prediction. Our experiments show that natural assumptions like homophily can be sufficient for
choosing effective smoothing distributions. But doing so in other tasks may be more challenging.

A limitation of (most) randomized smoothing certificates is that they use sampling to approximate
the smoothed classifier. Because we use different smoothing distributions for different outputs, we
can only use a fraction of the samples for each output. As discussed in Section A.1, we can alleviate
this problem by sharing smoothing distributions among multiple outputs. Our experiments show that
despite this issue, our method outperforms certificates that use a single smoothing distribution. Still,
future work should try to improve the sample efficiency of randomized smoothing (for example by
developing more methods for de-randomized smoothing (Levine & Feizi, 2020)).Any such advance
could then be incorporated into our localized smoothing framework.

8 EXPERIMENTAL EVALUATION

Our experimental evaluation has three objectives 1.) Verifying our main claim that localized randomized smoothing offers a better trade-off between accuracy and certifiable robustness than smoothing

2 (n) _T_ (n)
Because η is the smallest value b **_w_** can take on, i.e. minb∈RD+in **_b[T]_** **_wd[(][n][)]_** s.t. sum{b} ≤ _ϵ[p]._


-----

with i.i.d. distributions. 2.) Determining to what extend the linear program underlying the proposed
collective certificate strengthens our robustness guarantees. 3.) Assessing the efficacy of our novel
variance smoothing certificate for binary data. Any of the used datasets and classifiers only serve
as a means of comparing certificates. We thus use well-known and well-established architectures
instead of overly focusing on maximizing prediction accuracy by using the latest SOTA models.

We use two metrics to quantify certificate strength: Certified accuracy (i.e. the percentage of correct
and certifiably robust predictions) and certified ratio (i.e. the percentage of certifiably robust predictions, regardless of correctness)[3]. As single-number metrics, we report the AUC of the certified
accuracy/ratio functions w.r.t. adversarial budget ϵ (not to be confused with certifying some AUC
metric). For localized smoothing, we evaluate both the na¨ıve collective certificate, i.e. certifying
predictions independently (see Section 4), and the proposed LP-based certificate (using the linearly
relaxed version from Appendix A.4). We compare our method to two baselines using i.i.d. randomized smoothing: The na¨ıve collective certificate and center smoothing (Kumar & Goldstein, 2021).

For softly local models, the certificate of Schuchardt et al. (2021) is equivalent to the na¨ıve baseline.
When used to certify the number of robust predictions, the segmentation certificate of Fischer et al.
(2021) is at most as strong as the na¨ıve baseline (see Section C.4). Thus, our method is compared
to all existing collective certificates listed in Section 2. In all experiments, we use Monte Carlo
randomized smoothing. More details on the experimental setup can be found in Section E.

8.1 SEMANTIC SEGMENTATION


**Dataset and model.** We evaluate our certificate for continuous data andon the Pascal-VOC 2012 segmentation valida- l2 perturbations 1.0
tion set. Training is performed on 10582 pairs 0.8 localized
of training samples extracted from SBD[4] (Har- 0.6 Na¨ıve
iharan et al., 2011), To increase batch sizes localized
and thus allow a more thorough investigation 0.4 Na¨ıve i.i.d.
of different smoothing parameters, all images

0.2

are downscaled to 50% of their original size. Certified accuracy
Our base model is a U-Net segmentation model 0.0

|Col1|Col2|Col3|P|Col5|
|---|---|---|---|---|
|||L|P||
|||l|ocalized Na¨ıve||
|||l|ocalized||
||||Na¨ıve i.i.d.||
||||||
||||||

with a ResNet-18 backbone. To obtain accu- 0.0 0.5 1.0 1.5 2.0
rate and robust smoothed classifiers, base mod- Adversarial budget ϵ
els should be trained on the smoothing distribution. We thus train 51 different instances of our Figure 2: Certified ratios of U-Net models unbase model, augmenting the training data with der varying ϵ. We compare the na¨ıve i.i.d. basetime, when evaluating a baseline i.i.d. certificatea different σtrain ∈{0, 0.01, . . ., 0.5}. At test 0line (.25, σσ = 0max = 1.4) to localized smoothing (.5). Combining the base certifi-σmin =
with smoothing distribution N (0, σ), we load cates via linear programming (solid orange line)
the model trained with σtrain = σ. To perform instead of evaluating them independently (dotted
localized randomized smoothing, we choose pa- orange line) outperforms the baseline.
rametersages into regular grids of size σmin, σmax ∈ R+ and partition all im- 4 × 6 (similar to
example Fig. 1). To classify pixels in grid cell (i, j), we sample noise for grid cell (k, l) using
(0, σ[′]), with σ[′] [σmin, σmax] chosen proportional to the distance of (i, j) and (k, l) (more de_N_ _∈_
tails in Section E.2.1). As the base model, we load the one trained with σtrain = σmin. Using the
same distribution at train and test time for the i.i.d. baselines but not for localized smoothing is meant
to skew the results in the baseline’s favor. But, in Section E.2.3, we also repeat our experiments using
the same base model for i.i.d. and localized smoothing.

**Evaluation. The main goal of our experiments on segmentation is to verify that localized smoothing**
can offer a better trade-off between accuracy and certifiable robustness. That is, for all or most
_σ, there are σmin, σmax such that the locally smoothed model has higher accuracy and certifiable_
we can not evaluate all possible combinations. We therefore use the following scheme: We focuscollective robustness than i.i.d. smoothing baselines using N (0, σ). Because σ, σmin, σmax ∈ R+,
on the case σ ∈ [0, 0.5], which covers all distributions used in (Kumar & Goldstein, 2021) and

3In the case of image segmentation, we compute these metrics per image and then average over the dataset.
4Also known as ”Pascal trainaug”


-----

(Fischer et al., 2021). First, we evaluate our two baselines for five σ ∈{0.1, 0.2, 0.3, 0.4, 0.5}.
This results in baseline models with diverse levels of accuracy and robustness (e.g. the accuracy
of the na¨ıve baseline shrinks from 87.7% to 64.9% and the AUC of its certified accuracy grows
from 0.17 to 0.644). We then test whether, for each of the σ, we can find σmin, σmax such that the
locally smoothed models attains higher accuracy and is certifiably more robust. Finally, to verify
that {0.1, 0.2, 0.3, 0.4, 0.5} were not just a particularly poor choice of baseline parameters, we fix
the chosen σmin, σmax. We then perform a fine-grained search over σ [0, 0.5] with resolution 0.01
_∈_
to find a baseline model that has at least the same accuracy and certifiable robustness (as measured
by certificate AUC) as any of the fixed locally smoothed models. If this is not possible, this provides
strong evidence that the proposed smoothing scheme and certificate indeed offer a better trade-off.

Fig. 2 shows one example. For σ = 0.4, the na¨ıve i.i.d. baseline has an accuracy of 72.5%. With
_σmin = 0.25, σmax = 1.5, the proposed localized smoothing certificate yields both a higher accu-_
racy of 76.4% and a higher certified accuracy for all ϵ. It can certify robustness for ϵ up to 1.825,
compared to 1.45 of the baseline and the AUC of its certified accuracy curve is 43.1% larger. Fig. 2
also highlights the usefulness of the linear program we derived in Section 5: Evaluating the localized smoothing base certificates independently, i.e. computing the na¨ıve collective certificate (dotted
orange line), is not sufficient for outperforming the baseline. But combining them via the proposed
linear program drastically increases the certified accuracy

The results for all other combinations of smoothing distribution parameters, both baselines and both
metrics of certificate strength can be found in Section E.2.3. Tables 1 and 2 summarize the first part
of our evaluation procedure, in which we optimize the localized smoothing parameters. Safe for one
exception (with σ = 0.2, center smoothing has a lower accuracy, but slightly larger certified ratio),
the locally smoothed models have the same or higher accuracy, but provide stronger robustness
guarantees. The difference is particularly large for σ ∈{0.3, 0.4, 0.5}, where the accuracy of models
smoothed with i.i.d. noise drops off, while our localized smoothing distribution preserves the most
relevant parts of the image to allow for high accuracy. Table 5 summarizes the second part of our
evaluation scheme, in which we perform a fine-grained search over [0, 0.5]. We find that there is no
_σ such that either of the i.i.d. baselines can outperform any of the chosen locally smoothed models_
w.r.t. AUC of their certified accuracy or certified ratio curves. This is ample evidence for our claim
that localized smoothing offers a better trade-off than i.i.d. smoothing. Also, the collective LPs
caused little computational overhead (avg. 0.68 s per LP, more details in Section E.2.3).

8.2 NODE CLASSIFICATION

**Dataset and model. We evaluate our certificate for binary data on the Cora-ML node classification**
dataset. We use two different base-models: Approximate Personalized Propagation of Neural Predictions (APPNP) (Klicpera et al., 2019) and a 6-layer Graph Convolutional network (GCN) (Kipf
& Welling, 2017). Both models have a receptive field that covers most or all of the graph, meaning
they are softly local. For details on model and training parameters, see Section E.3.1.

As center smoothing has only been derived for Gaussian smoothing, we only compare to the na¨ıve
baseline. For both, the baseline and our localized smoothing certificate, we use sparsity-aware randomized smoothing (Bojchevski et al., 2020), i.e. flip 1-bits and 0-bits with different probabilities
(θ[−] and θ[+], respectively), which allows us to certify different levels of robustness to deletions and
additions of bits. With localized randomized smoothing, we use the variance smoothing base certificate derived in Section B.2.2. We choose the distribution parameters for localized smoothing based
on an assumption of homophily, i.e. nearby nodes are most relevant for classifying a node. We partition the graph into 5 clusters and define parameters θmin[±] [and][ θ]max[±] [. When classifying a node in]
cluster i, we randomly smooth attributes in cluster j with θij[+][, θ]ij[−] [that are based on linearly interpo-]
lating in [θmin[−] _[, θ]max[−]_ []][ and][ [][θ]min[−] _[, θ]max[−]_ []][ based on the affinity of the clusters (details in Section E.3.1).]

**Evaluation. We first evaluate the new variance-based certificate and compare it to the certificate**
derived by Bojchevski et al. (2020). For this, we use only one cluster, meaning we use the same
smoothing distribution for both. Fig. 11 in Section E.3 shows that the variance certificate is weaker
than the baseline for additions, but better for deletions. It appears sufficiently effective to be used as
a base certificate and integrated into a stronger, collective certificate.

The parameter space of our smoothing distributions is large. For the localized approach we have
four continuous parameters, as we have to specify both the minimal and maximal noise values.


-----

1.0

0.8

0.6

0.4

0.2

0.0


1.0

0.8

0.6

0.4

0.2

0.0

|Col1|Col2|Col3|ed lized .|
|---|---|---|---|
||L N N|P localiz a¨ıve loca a¨ıve i.i.d||

|LP localiz Na¨ıve loca Na¨ıve i.i.d|ed lized .|
|---|---|


LP localized
Na¨ıve localized

Na¨ıve i.i.d.


LP localized
Na¨ıve localized

Na¨ıve i.i.d.


10 20 30 40 50

Number of adversarial additions


10 20 30 40 50

Number of adversarial deletions


Figure 3: Certified accuracy for an APPNP model under varying number of attribute additions (left)
and deletions (right). We compare localized smoothing (θmin[+] [= 0][.][05][,][ θ]min[−] [= 0][.][65][,][ θ]max[+] [= 0][.][08][,]
_θmax[−]_ [= 0][.][95][) to two separately optimized na¨]ıve i.i.d. smoothing baselines: addition (θ[+] = 0.04,
_θ[−]_ = 0.61) and deletion (θ[+] = 0.04, θ[−] = 0.68). Combining the localized smoothing base
certificates via linear programming (solid orange line) instead of evaluating them independently
(dotted line) allows us to outperform the baselines for most adversarial budgets.

Therefore, it is difficult to show that our approach achieves a better accuracy-robustness trade-off
over the whole noise space. However, we can investigate the accuracy-robustness trade-off within
some areas of this space. For the localized approach we choose a few fixed combinations of the noise
parameters θ±min and θmax[±] [. To show our claim, we then optimise the baselines with parameters]
in an interval around our θmin[+] [and][ θ]min[−] [. This is a smaller space, as the baselines only have two]
parameters. We select the baseline whose certified accuracy curve has the largest AUC. We perform
the search for the best baseline for the addition and deletion scenario independently, i.e., the best
baseline model for addition and deletion does not have to be the same.

In Fig. 3, we see the certified accuracy of an APPNP model for a varying number of attribute additions and deletions (left and right respectively). To find the best distribution parameters for the
baselines, we evaluated combinations of θ[+] _∈{0.04, 0.055, 0.07} and θ[−]_ _∈_ [0.1, . . ., 0.827], using
11 equally spaced values for the interval. For adversarial additions, the best baseline yields a certified accuracy curve with an AUC of 4.51 compared to our 5.65. The best baseline for deletions has
an AUC of 7.76 compared to our 16.26. Our method outperforms these optimized baselines for most
adversarial budgets, while maintaining the same clean accuracy (i.e. certified accuracy at ϵ = 0).
Experiments with different noise parameters and classifiers can be found in Section E.3. In general,
we find that we significantly outperform the baseline when certifying robustness to deletions, but
often have weaker certificates for additions (which may be inherent to the variance smoothing base
certificates). Due to the large continuous parameter space, we cannot claim that localized smoothing
outperforms the na¨ıve baseline everywhere. However, our results show that, for the tested parameter
regions, localized smoothing can provide a significantly better accuracy-robustness trade-off.

We found that using the collective LP instead of na¨ıvely combining the base certificates can result in
much stronger certificates: The AUC of the certified accuracy curve (averaged over all experiments)
increased by 38.8% and 33.6% for addition and deletion, respectively. The collective LPs caused
little computational overhead (avg. 10.9 s per LP, more details in Section E.3.3).


9 CONCLUSION

In this work, we have proposed the first collective robustness certificate for softly local multi-output
classifiers. It is based on localized randomized smoothing, i.e. randomly smoothing different
outputs using different non-i.i.d. smoothing distributions matching the model’s locality. We have
shown how per-output certificates based on localized smoothing can be computed and that they
share a common interface. This interface allows them to be combined into a strong collective robustness certificate. Experiments on image segmentation and node classification tasks demonstrate
that localized smoothing can offer a better robustness-accuracy trade-off than existing randomized
smoothing techniques. Our results show that locality is linked to robustness, which suggests the
research direction of building more effective local models to robustly solve multi-output tasks.


-----

10 REPRODUCIBILITY STATEMENT

We prove all theoretic results that were not already derived in the main text in Appendices A to C. To
ensure reproducibility of the experimental results we provide detailed descriptions of the evaluation
process with the respective parameters in Section E.2 and Section E.3. Code will be made available
to reviewers via an anonymous link posted on OpenReview, as suggested by the guidelines.

11 ETHICS STATEMENT

In this paper, we propose a method to increase the robustness of machine learning models against
adversarial perturbations and to certify their robustness. We see this as an important step towards
general usage of models in practice, as many existing methods are brittle to crafted attacks. Through
the proposed method, we hope to contribute to the safe usage of machine learning. However, robust
models also have to be seen with caution. As they are harder to fool, harmful purposes like mass
surveillance are harder to avoid. We believe that it is still necessary to further research robustness
of machine learning models as the positive effects can outweigh the negatives, but it is necessary to
discuss the ethical implications of the usage in any specific application area.

REFERENCES

T.W. Anderson. Confidence limits for the value of an arbitrary bounded random variable with a continuous distribution function. In Bulletin of The International and Statistical Institute, volume 43,
pp. 249–251, 1969.

Yonatan Belinkov and Yonatan Bisk. Synthetic and natural noise both break neural machine
translation. In International Conference on Learning Representations, 2018. [URL https:](https://openreview.net/forum?id=BJ8vJebC-)
[//openreview.net/forum?id=BJ8vJebC-.](https://openreview.net/forum?id=BJ8vJebC-)

Aleksandar Bojchevski and Stephan G¨unnemann. Certifiable robustness to graph perturbations. In
_Advances in Neural Information Processing Systems, volume 32, pp. 8319–8330, 2019._

Aleksandar Bojchevski, Johannes Klicpera, and Stephan G¨unnemann. Efficient robustness certificates for discrete data: Sparsity-aware randomized smoothing for graphs, images and more. In
_International Conference on Machine Learning, pp. 1003–1013. PMLR, 2020._

Gregory Bonaert, Dimitar I. Dimitrov, Maximilian Baader, and Martin Vechev. Fast and precise
certification of transformers. In Proceedings of the 42nd ACM SIGPLAN International Confer_ence on Programming Language Design and Implementation, PLDI 2021, pp. 466–481, New_
York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383912. doi:
[10.1145/3453483.3454056. URL https://doi.org/10.1145/3453483.3454056.](https://doi.org/10.1145/3453483.3454056)

Carlo Bonferroni. Teoria statistica delle classi e calcolo delle probabilita. Pubblicazioni del R
_Istituto Superiore di Scienze Economiche e Commericiali di Firenze, 8:3–62, 1936._

G. Bradski. The OpenCV Library. Dr. Dobb’s Journal of Software Tools, 2000.

Alexander Buslaev, Vladimir I. Iglovikov, Eugene Khvedchenya, Alex Parinov, Mikhail Druzhinin,
and Alexandr A. Kalinin. Albumentations: Fast and flexible image augmentations. Information,
[11(2), 2020. ISSN 2078-2489. doi: 10.3390/info11020125. URL https://www.mdpi.com/](https://www.mdpi.com/2078-2489/11/2/125)
[2078-2489/11/2/125.](https://www.mdpi.com/2078-2489/11/2/125)

C. J. Clopper and E. S. Pearson. The use of confidence or fiducial limits illustrated in the case of the
[binomial. Biometrika, 26(4):404–413, 1934. ISSN 00063444. URL http://www.jstor.](http://www.jstor.org/stable/2331986)
[org/stable/2331986.](http://www.jstor.org/stable/2331986)

Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
_International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning_
_[Research, pp. 1310–1320. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.](https://proceedings.mlr.press/v97/cohen19c.html)_
[press/v97/cohen19c.html.](https://proceedings.mlr.press/v97/cohen19c.html)


-----

Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling language for convex
optimization. Journal of Machine Learning Research, 17(83):1–5, 2016.

A. Dvoretzky, J. Kiefer, and J. Wolfowitz. Asymptotic Minimax Character of the Sample Distribution Function and of the Classical Multinomial Estimator. The Annals of Mathematical Statis_[tics, 27(3):642 – 669, 1956. doi: 10.1214/aoms/1177728174. URL https://doi.org/10.](https://doi.org/10.1214/aoms/1177728174)_
[1214/aoms/1177728174.](https://doi.org/10.1214/aoms/1177728174)

Francisco Eiras, Motasem Alfarra, M. Pawan Kumar, Philip H. S. Torr, Puneet K. Dokania, Bernard
Ghanem, and Adel Bibi. Ancer: Anisotropic certification via sample-wise volume maximization.
2021.

Marc Fischer, Maximilian Baader, and Martin Vechev. Certified defense to image transformations
via randomized smoothing. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin
(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 8404–8417. Cur[ran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/](https://proceedings.neurips.cc/paper/2020/file/5fb37d5bbdbbae16dea2f3104d7f9439-Paper.pdf)
[file/5fb37d5bbdbbae16dea2f3104d7f9439-Paper.pdf.](https://proceedings.neurips.cc/paper/2020/file/5fb37d5bbdbbae16dea2f3104d7f9439-Paper.pdf)

Marc Fischer, Maximilian Baader, and Martin Vechev. Scalable certified segmentation via randomized smoothing. In International Conference on Machine Learning, pp. 3340–3351. PMLR,
2021.

Bharath Hariharan, Pablo Arbel´aez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik. Semantic contours from inverse detectors. In 2011 International Conference on Computer Vision,
pp. 991–998, 2011. doi: 10.1109/ICCV.2011.6126343.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
770–778, 2016. doi: 10.1109/CVPR.2016.90.

Sture Holm. A simple sequentially rejective multiple test procedure. _Scandinavian Journal of_
_[Statistics, 6(2):65–70, 1979. ISSN 03036898, 14679469. URL http://www.jstor.org/](http://www.jstor.org/stable/4615733)_
[stable/4615733.](http://www.jstor.org/stable/4615733)

George Karypis and Vipin Kumar. A fast and high quality multilevel scheme for partitioning irregular graphs. SIAM Journal on Scientific Computing, 20(1):359–392, 1998. doi: 10.1137/
[S1064827595287997. URL https://doi.org/10.1137/S1064827595287997.](https://doi.org/10.1137/S1064827595287997)

Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon,
_[France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. URL https:](https://openreview.net/forum?id=SJU4ayYgl)_
[//openreview.net/forum?id=SJU4ayYgl.](https://openreview.net/forum?id=SJU4ayYgl)

Johannes Klicpera, Aleksandar Bojchevski, and Stephan G¨unnemann. Predict then propagate:
Graph neural networks meet personalized pagerank, 2019.

Ching-Yun Ko, Zhaoyang Lyu, Lily Weng, Luca Daniel, Ngai Wong, and Dahua Lin. POPQORN:
Quantifying robustness of recurrent neural networks. In Kamalika Chaudhuri and Ruslan
Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning,
volume 97 of Proceedings of Machine Learning Research, pp. 3468–3477. PMLR, 09–15 Jun
[2019. URL https://proceedings.mlr.press/v97/ko19a.html.](https://proceedings.mlr.press/v97/ko19a.html)

Aounon Kumar and Tom Goldstein. Center smoothing: Provable robustness for functions with
metric-space outputs. 2021.

Aounon Kumar, Alexander Levine, Soheil Feizi, and Tom Goldstein. Certifying confidence via randomized smoothing. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.),
_Advances in Neural Information Processing Systems, volume 33, pp. 5165–5177. Curran Asso-_
[ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/](https://proceedings.neurips.cc/paper/2020/file/37aa5dfc44dddd0d19d4311e2c7a0240-Paper.pdf)
[37aa5dfc44dddd0d19d4311e2c7a0240-Paper.pdf.](https://proceedings.neurips.cc/paper/2020/file/37aa5dfc44dddd0d19d4311e2c7a0240-Paper.pdf)


-----

Mathias L´ecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified
robustness to adversarial examples with differential privacy. In 2019 IEEE Symposium on Security
_and Privacy, SP 2019, San Francisco, CA, USA, May 19-23, 2019, pp. 656–672. IEEE, 2019. doi:_
[10.1109/SP.2019.00044. URL https://doi.org/10.1109/SP.2019.00044.](https://doi.org/10.1109/SP.2019.00044)

Guang-He Lee, Yang Yuan, Shiyu Chang, and Tommi S. Jaakkola. Tight certificates of adversarial
robustness for randomly smoothed classifiers. 2019.

Alexander Levine and Soheil Feizi. (de)randomized smoothing for certifiable defense against
patch attacks. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Ad_vances in Neural Information Processing Systems, volume 33, pp. 6465–6475. Curran Asso-_
[ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/](https://proceedings.neurips.cc/paper/2020/file/47ce0875420b2dbacfc5535f94e68433-Paper.pdf)
[47ce0875420b2dbacfc5535f94e68433-Paper.pdf.](https://proceedings.neurips.cc/paper/2020/file/47ce0875420b2dbacfc5535f94e68433-Paper.pdf)

Xuanqing Liu, Minhao Cheng, Huan Zhang, and Cho-Jui Hsieh. Towards robust neural networks via
random self-ensemble. In Computer Vision – ECCV 2018, pp. 381–397. Springer International
Publishing, 2018a. doi: 10.1007/978-3-030-01234-2 23.

Yongge Liu, Jianzhuang Yu, and Yahong Han. Understanding the effective receptive field in semantic image segmentation. Multimedia Tools and Applications, 77(17):22159–22171, jan 2018b.
doi: 10.1007/s11042-018-5704-3.

Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective receptive
field in deep convolutional neural networks. In Proceedings of the 30th International Conference
_on Neural Information Processing Systems, NIPS’16, pp. 4905–4913, Red Hook, NY, USA, 2016._
Curran Associates Inc. ISBN 9781510838819.

[MOSEK ApS. MOSEK Optimizer API for Python 9.2.46, 2019. URL https://docs.mosek.](https://docs.mosek.com/9.2/pythonapi/index.html)
[com/9.2/pythonapi/index.html.](https://docs.mosek.com/9.2/pythonapi/index.html)

Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention –
_MICCAI 2015, pp. 234–241. Springer International Publishing, 2015. ISBN 978-3-319-24574-4._

Wonryong Ryou, Jiayu Chen, Mislav Balunovic, Gagandeep Singh, Andrei Dan, and Martin Vechev.
Scalable polyhedral verification of recurrent neural networks. In Alexandra Silva and K. Rustan M. Leino (eds.), Computer Aided Verification, pp. 225–248, Cham, 2021. Springer International Publishing. ISBN 978-3-030-81685-8.

Jan Schuchardt, Aleksandar Bojchevski, Johannes Klicpera, and Stephan G¨unnemann. Collective
robustness certificates: Exploiting interdependence in graph neural networks. In International
_[Conference on Learning Representations, 2021. URL https://openreview.net/forum?](https://openreview.net/forum?id=ULQdiUTHe3y)_
[id=ULQdiUTHe3y.](https://openreview.net/forum?id=ULQdiUTHe3y)

Han Shi, Jiahui Gao, Xiaozhe Ren, Hang Xu, Xiaodan Liang, Zhenguo Li, and James Tin-Yau
Kwok. Sparsebert: Rethinking the importance analysis in self-attention. In Marina Meila and
Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning,
_ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learn-_
_[ing Research, pp. 9547–9557. PMLR, 2021. URL http://proceedings.mlr.press/](http://proceedings.mlr.press/v139/shi21a.html)_
[v139/shi21a.html.](http://proceedings.mlr.press/v139/shi21a.html)

Zhouxing Shi, Huan Zhang, Kai-Wei Chang, Minlie Huang, and Cho-Jui Hsieh. Robustness verification for transformers. In International Conference on Learning Representations, 2020. URL
[https://openreview.net/forum?id=BJxwPJHFwS.](https://openreview.net/forum?id=BJxwPJHFwS)

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on
_[Learning Representations, 2014. URL http://arxiv.org/abs/1312.6199.](http://arxiv.org/abs/1312.6199)_

Hoang-Dung Tran, Neelanjana Pal, Patrick Musau, Diego Manzanas Lopez, Nathaniel Hamilton,
Xiaodong Yang, Stanley Bak, and Taylor T. Johnson. Robustness verification of semantic segmentation neural networks using relaxed reachability. In Computer Aided Verification, pp. 263–286,
Cham, 2021. ISBN 978-3-030-81685-8.


-----

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st Inter_national Conference on Neural Information Processing Systems, NIPS’17, pp. 6000–6010, Red_
Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.

Lei Wang, Runtian Zhai, Di He, Liwei Wang, and Li Jian. Pretrain-to-finetune adversarial training
[via sample-wise randomized smoothing, 2021. URL https://openreview.net/forum?](https://openreview.net/forum?id=Te1aZ2myPIu)
[id=Te1aZ2myPIu.](https://openreview.net/forum?id=Te1aZ2myPIu)

Cihang Xie, Jianyu Wang, Zhishuai Zhang, Yuyin Zhou, Lingxi Xie, and Alan Yuille. Adversarial examples for semantic segmentation and object detection. In International Conference on
_Computer Vision. IEEE, 2017._

Pavel Yakubovskiy. Segmentation models pytorch. [https://github.com/qubvel/](https://github.com/qubvel/segmentation_models.pytorch)
[segmentation_models.pytorch, 2020.](https://github.com/qubvel/segmentation_models.pytorch)

Dinghuai Zhang, Mao Ye, Chengyue Gong, Zhanxing Zhu, and Qiang Liu. Black-box certification with randomized smoothing: A functional optimization based framework. In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances
_in Neural Information Processing Systems,_ volume 33, pp. 2316–2326. Curran Asso[ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/](https://proceedings.neurips.cc/paper/2020/file/1896a3bf730516dd643ba67b4c447d36-Paper.pdf)
[1896a3bf730516dd643ba67b4c447d36-Paper.pdf.](https://proceedings.neurips.cc/paper/2020/file/1896a3bf730516dd643ba67b4c447d36-Paper.pdf)

Daniel Z¨ugner and Stephan G¨unnemann. Adversarial attacks on graph neural networks via meta
[learning. In International Conference on Learning Representations, 2019. URL https://](https://openreview.net/forum?id=Bylnx209YX)
[openreview.net/forum?id=Bylnx209YX.](https://openreview.net/forum?id=Bylnx209YX)

Daniel Z¨ugner and Stephan G¨unnemann. Certifiable robustness and robust training for graph convolutional networks. In Proceedings of the 25th ACM SIGKDD International Conference on
_Knowledge Discovery & Data Mining, 2019._

Daniel Z¨ugner and Stephan G¨unnemann. Certifiable robustness of graph convolutional networks
under structure perturbations. In Proceedings of the 26th ACM SIGKDD International Conference
_on Knowledge Discovery & Data Mining, pp. 1656–1665, 2020._

A IMPROVING EFFICIENCY

In this section, we discuss different modifications to our collective certificate that improve its sample efficiency and allow us fine-grained control over the size of the collective linear program. We
further discuss a linear relaxation of our collective linear program. All of the modifications preserve
the soundness of our collective certificate, i.e. we still obtain a provable bound on the number of
predictions that can be simultaneously attacked by an adversary. To avoid constant case distinctions, we first present all results for real-valued data, i.e. X = R, before mentioning any additional
precautions that may be needed when working with binary data.

A.1 SHARING SMOOTHING DISTRIBUTIONS AMONG OUTPUTS

In principle, our proposed certificate allows a different smoothing distribution Ψ[(][n][)] to be used per
output gn of our base model. In practice, where we have to estimate properties of the smoothed
classifier using Monte Carlo methods, this is problematic: Samples cannot be re-used, each of the
many outputs requires its own round of sampling. We can increase the efficiency of our localized
smoothing approach by partitioning our Dout outputs into Nout subsets that share the same smoothing distribution. When making smoothed predictions or computing base certificates, we can then
reuse the same samples for all outputs within each subsets.

More formally, we partition our Dout output dimensions into sets K[(1)], . . ., K[(][N][out)] with

˙ out

(6)
_i=1_ [K][(][i][)][ =][ {][1][, . . ., D][out][}][.]

[[N]


-----

We then associate each set K[(][i][)] with a smoothing distribution Ψ[(][i][)]. For each base model output gn
withfn(x n) = argmax ∈ K[(][i][)], we then use smoothing distributiony∈Y Prz∼Ψ(i) [f (x + z) = y] (note that we use a different smoothing paradigm Ψ[(][i][)] to construct the smoothed output fn, e.g.
for binary data, see Section 5).

A.2 QUANTIZING CERTIFICATE PARAMETERS

Recall that our base certificates from Section 5 are defined by a linear inequality: A prediction
_ysomen = p f ≥n(x0). The weight vectors is robust to a perturbed input w[(][n][)]_ _∈_ R x[D][′][in]∈only depend on the smoothing distributions. AX[D][in] if _d=1_ _[w]d[(][n][)]_ _· |x[′]d_ _[−]_ _[x][d][|][p][ < η][(][n][)][, for]_
side of effect of sharing the same smoothing Ψ[(][i][)] among all outputs from a set[P][D] K[(][i][)], as discussed
in the previous section, is that the outputs also share the same weight vector w[(][i][)] _∈_ R[D][in] with
_∀n ∈_ K[(][i][)] : w[(][i][)] = w[(][n][)]. Thus, for all smoothed outputs fn with n ∈ K[(][i][)], the smoothed
prediction yn is robust if _d=1_ _[w]d[(][i][)]_ _[· |][x]d[′]_ _[−]_ _[x][d][|][p][ < η][(][n][)][.]_

Evidently, the base certificates for outputs from a set K[(][i][)] only differ in their parameter η[(][n][)]. Recall
that in our collective linear program we use a vector of variables[P][D] **_t ∈{0, 1}[D][out]_** to indicate which
predictions are robust according to their base certificates (see Section 6). If there are two outputs fn
and fm with η[(][n][)] = η[(][m][)], then fn and fm have the same base certificate and their robustness can
be modelled by the same indicator variable. Conversely, for each set of outputs K[(][i][)], we only need
one indicator variable per unique η[(][n][)]. By quantizing the η[(][n][)] within each subset K[(][i][)] (for example
by defining equally sized bins between minn∈K(i) η[(][n][)] and maxn∈K(i) η[(][n][)] ), we can ensure that
there is always a fixed number Nbins of indicator variables per subset. This way, we can reduce the
number of indicator variables from Dout to Nout · Nbins.

To implement this idea, we define matrix of thresholds E ∈ R[N][out][×][N][bins] with ∀i : min {Ei,:} ≤
minn∈K(i) _η[(][n][)]_ _| n ∈_ K[(][i][)][ ]. We then define a function ξ : {1, . . ., Nout} × R → R with
  _ξ(i, η) = max (_ _Ei,j_ _j_ 1, . . ., Nbins _Ei,j < η_ ) (7)

_{_ _|_ _∈{_ _∧_ _}_

that quantizes base certificate parameter η from output subset K[(][i][)] by mapping it to the next
smallest threshold in Ei,:. For feasibility, like in Section 6 we need to compute the constant
_η[(][i][)]_ = minb∈RD+in **_b[T]_** **_wd[(][i][)]_** s.t. sum{b} ≤ _ϵ[p]_ to ensure feasibility of the problem. Note that, be
cause all outputs from a subset K[(][i][)] share the same weight vector w[(][i][)], we only have to compute
this constant once per subset. We can bound the collective robustness of the targeted dimensions T
of our vector of predictions y = f (x) as follows:

min _Ti,j_ _n ∈_ T ∩ K[(][i][)] _ξ_ _i, η[(][n][)][]_ = Ei,j (8)

_i_ 1,...,Nout _j_ 1,...,Nbins
_∈{_ X _}_ _∈{_ X _}_ n  o

s.t. _i, j : b[T]_ **_w[(][i][)]_** _Ti,jη[(][i][)]_ + (1 _Ti,j)Ei,j,_ sum **_b_** _ϵ[p]_ (9)
_∀_ _≥_ _−_ _{_ _} ≤_

**_b ∈_** R[D]+[in][,] **_T ∈{0, 1}[N][out][×][N][bins]_** _._ (10)

Constraint Eq. 9 ensures that Ti,j is only set to 0 if b[T] **_w[(][i][)]_** _≥_ _Ei,j, i.e. all predictions from subset_
K[(][i][)] whose base certificate parameter η[(][n][)] is quantized to Ei,j are no longer robust. When this is
the case, the objective function decreases by the number of these predictions. For Nout = Dout,
_Nbins = 1 and En,1 = η[(][n][)], we recover our general certificate from Section 6. Note that, if the_
quantization maps any parameter η[(][n][)] to a smaller number, the set H[(][n][)] becomes more restrictive,
i.e. yn is considered robust to a smaller set of perturbed inputs. Thus, Eq. 8 is a lower bound on our
general certificate from Section 6.

A.3 SHARING NOISE LEVELS AMONG INPUTS

Similar to how partitioning the output dimensions allows us to control the number of output variables
**_t, partitioning the input dimensions and using the same noise level within each partition allows us_**
to control the number of variables b that model the allocation of adversarial budget.

Assume that we have partitioned our output dimensions into Nout subsets K[(1)], . . ., K[(][N][out], with
outputs in each subset sharing the same smoothing distribution Ψ[(][i][)], as explained in Section A.1.


-----

Let us now define Nin input subsets J[(1)], . . ., J[(][N][in][)] with

˙ out

(11)
_i=1_ [J][(][i][)][ =][ {][1][, . . ., D][out][}][.]

[[N]

Recall that a prediction yn = fn(x) with n ∈ K[(][i][)] is robust to a perturbed input x[′] _∈_ X[D][in]

ifing distributions.d=1 _[w]d[(][i][)]_ _· |x[′]d_ _[−]Assume that we choose each smoothing distribution[x][d][|][p][ < η][(][n][)][ and that the weight vectors][ w][(][i][)][ only depend on the smooth-] Ψ[(][i][)]_ such that ∀l ∈
1[P], . . ., N[D] in _,_ _d, d[′]_ J[(][l][)] : wd[(][i][)] = wd[(][i][′][)][, i.e. all input dimensions within each set][ J][(][l][)][ have the]
_{_ _}_ _∀_ _∈_
same weight. This can be achieved by choosing Ψ[(][i][)] so that all dimensions in each input subset J[l]

are smoothed with the noise level (note that we can still use different Ψ[(][i][)], i.e. different noise levels
for smoothing different sets of outputs). For example, one could use a Gaussian distribution with
covariance matrix Σ = diag (σ)[2] with _l_ 1, . . ., Nin _,_ _d, d[′]_ J[(][l][)] : σd = σd′ .
_∀_ _∈{_ _}_ _∀_ _∈_

In this case, the evaluation of our base certificates can be simplified. Prediction yn = fn(x) is robust
to a perturbed input x[′] _∈_ X[D][in] if


_wd[(][i][)]_ _d_ (12)
_d=1_ _[· |][x][′]_ _[−]_ _[x][d][|][p][ < η][(][n][)]_

X


_Nin_

= _u[(][i][)]_ _x[′]d_ _< η[(][n][)],_ (13)

Xl=1  _·_ _dX∈J[(][l][)]_ _|_ _[−]_ _[x][d][|][p]_

 

with u ∈ R[N][in] and ∀i ∈{1, . . ., Nout}, ∀l ∈{1, . . ., Nin}, ∀d ∈ J : u[i]l [=][ w]d[i] [. That is, we]
can replace each weight vector w[(][i][)] that has one weight wd[(][i][)] per input dimension d with a smaller
weight vector u[(][i][)] with one weight u[(]l[i][)] per input subset J[(][l][)].

For our linear program, this means that we no longer need a budget vector b R[D]+[in] to model the
_∈_
element-wise distance _x[′]d_
_|_ _[−]_ _[x][d][|][p][ in each dimension][ d][. Instead, we can use a smaller budget vector]_
**_bCombined with the quantization of certificate parameters from the previous section, our optimization ∈_** R[N]+[in] to model the overall distance within each input subset J[(][l][)], i.e. _d∈J[(][l][)][ |][x]d[′]_ _[−]_ _[x][d][|][p][.]_

problem becomes P

min _Ti,j_ _n ∈_ T ∩ K[(][i][)] _ξ_ _i, η[(][n][)][]_ = Ei,j (14)

_i_ 1,...,Nout _j_ 1,...,Nbins
_∈{_ X _}_ _∈{_ X _}_ n  o

s.t. _i, j : b[T]_ **_u[(][i][)]_** _Ti,jη[(][i][)]_ + (1 _Ti,j)Ei,j,_ sum **_b_** _ϵ[p],_ (15)
_∀_ _≥_ _−_ _{_ _} ≤_

**_b ∈_** R[N]+[in][,] **_T ∈{0, 1}[N][out][×][N][bins]_** _._ (16)

with u ∈ R[N][in] and ∀i ∈{1, . . ., Nout}, ∀l ∈{1, . . ., Nin}, ∀d ∈ J : ωl[i] [=][ w]d[i] [. For][ N][out][ =][ D][out][,]
_Nin = Din, Nbins = 1 and En,1 = η[(][n][)], we recover our general certificate from Section 6._

When certifying robustness for binary data, we impose different constraints on b. To model that the
adversary can not flip more bits than are present within each subset, we use a budget vector b N[N]0 [in]
_∈_
with ∀l ∈{1, . . ., Nin} : bl ≤ J[(][l][)], instead of a continuous budget vector b ∈ R[N]+[in][.]

A.4 LINEAR RELAXATION

Combining the previous steps allows us to reduce the number of problem variables and linear constraints fromStill, finding an optimal solution to the mixed-integer linear program may be too expensive. One Din + Dout and Dout + 1 to Nin + Nout · Nbins and Nout · Nbins + 1, respectively.
can obtain a lower bound on the optimal value and thus a valid, albeit more pessimistic, robustness
certificate by relaxing all to be continuous.

When using the general certificate from Section 6, the binary vector t ∈{0, 1}[D][out] can be relaxed to t ∈ [0, 1][D][out] . When using the certificate with quantized base certificate parameters
from Section A.2 or Section A.3, the binary matrix T ∈ [0, 1][N][out][×][N][bins] can be relaxed to T ∈

[0, 1][N][out][×][N][bins] . Conceptually, this means that predictions can be partially certified, i.e. tn (0, 1)
_∈_


-----

or Ti,j (0, 1). In particular, a prediction can be partially certified even if we know that is impos_∈_
sible to attack under the collective perturbation modellike Schuchardt et al. (2021), who encountered the same problem with their collective certificate, Bx = **_x[′]_** _∈_ X[D][in] _| ||x[′]_ _−_ **_x||p ≤_** _ϵ_ . Just

we circumvent this issue by first computing a set L ⊆ T of all targeted predictions in T that are
guaranteed to always be robust:


L = _n_ T max _wd[(][n][)]_ _x[′]d_ _< η[(][n][)]_ (17)

( _∈_ _x∈Bx_ _d=1_ _· |_ _[−]_ _[x][d][|][p]!_ )

X

= _n ∈_ T max max **_w[(][n][)][o]_** _· ϵ[p], 0_ _< η[(][n][)][ o]_ _._ (18)
n  n 

The equality follows from the fact that the most effective way of attacking a prediction is to allocate
all adversarial budget to the least robust dimension, i.e. the dimension with the largest weight –
unless all weights are negative. Because we know that all predictions with indices in L are robust,
we do not have to include them in the collective optimization problem and can instead compute


L =


_n ∈_ T


I **_x[′]_** H[(][n][)][i] _._ (19)
_∈_
_nX∈T\L_ h


L + min
_|_ _|_ **_x[′]∈Bx_**


The r.h.s. optimization can be solved using the general collective certificate from Section 6 or any
of the more efficient, modified certificates from previous sections.

When using the general collective certificate from Section 6 with binary data, the budget variables
**_b ∈{0, 1}[D][in]_** can be relaxed to b ∈ [0, 1][D][in]. When using the modified collective certificate
from Section A.3, the budget variables with b ∈ N[N]0 [in] can be relaxed to b ∈ R[N]+[in][. The additional]
constraint ∀l ∈{1, . . ., Nin} : bl ≤ J[(][l][)] can be kept in order to model that the adversary cannot
flip (or partially flip) more bits than are present within each input subset J[(][l][)].

B BASE CERTIFICATES

In the following, we show why the base certificates presented in Section 5 hold and present alternatives for other collective perturbation models.

B.1 GAUSSIAN SMOOTHING FOR l2 PERTURBATIONS OF CONTINUOUS DATA

**Proposition 1. Given an output gn : R[D][in]** _→_ Y, let fn(x) = argmaxy∈Y Prz∼N (x,Σ) [gn(z) = y]
_be the corresponding smoothed output with Σ = diag (σ)[2]_ _and σ ∈_ R[D]+[in][. Given an input][ x][ ∈] [R][D][in]

_and smoothed prediction yn = fn(x), let q = Prz_ (x,Σ) [gn(z) = yn]. Then, **_x[′]_** H[(][n][)] :

_fn(x[′]) = yn with H[(][n][)]_ _defined as in Eq. 2, wd =_ _σ1d_ [2][,]∼N[ η][ =] Φ[(][−][1)](q) 2 and κ = 2. ∀ _∈_

  

_Proof. Based on the definition of the base certificate interface, we need to show that, ∀x[′]_ _∈_ H :
_fn(x[′]) = yn with_

_Din_

1 2

H = **_x[′]_** R[D][in] _xd_ _x[′]d[|][2][ <]_ Φ[−][1](q) _._ (20)

( _∈_ _d=1_ _σd[2]_ _· |_ _−_ )

X   

Eiras et al. (2021) have shown that under the same conditions as above, but with a general covariance
matrix Σ ∈ R[D]+[in][×][D][in], a prediction yn is certifiably robust to a perturbed input x[′] if


_Din_

_d=1_

X


**_x[′]_** _∈_ R[D][in]


H =


(x **_x[′])Σ[−][1](x_** **_x[′]) <_** [1]
_−_ _−_ 2


Φ[−][1](q) − Φ[−][1](q[′]) _,_ (21)



where q[′] = maxyn′ _[̸][=][y][n]_ [Pr][z][∼N][ (][x][,][Σ][)][ [][g][n][(][z][) =][ y]n[′] []][ is the probability of the second most likely pre-]
diction under the smoothing distribution. Because the probabilities of all possible predictions have
to sum up to 1, we have q[′] _≤_ 1 − _q. Since Φ[−][1]_ is monotonically increasing, we can obtain a
lower bound on the r.h.s. of Eq. 21 and thus a more pessimistic certificate by substituting 1 − _q_


-----

for q[′] (deriving such a ”binary certificate” from a ”multiclass certificate” is common in randomized
smoothing and was already discussed in (Cohen et al., 2019)):


(x **_x[′])Σ[−][1](x_** **_x[′]) <_** [1]
_−_ _−_ 2


Φ[−][1](q) − Φ[−][1](1 − _q)_ _,_ (22)



In our case, Σ is a diagonal matrix diag (σ)[2] with σ ∈ R[D]+[in][. Thus Eq. 22 is equivalent to]


_Din_

(xd _x[′]d[) 1]_ (xd _x[′]d[)][ <][ 1]_
_d=1_ _−_ _σd[2]_ _−_ 2

X


Φ[−][1](q) − Φ[−][1](1 − _q)_ _._ (23)



Finally, using the fact that Φ[−][1](q) − Φ[−][1](1 − _q) = 2Φ[−][1](q) and eliminating the square root shows_
that we are certifiably robust if


_Din_

_d=1_

X


1 2

_xd_ _x[′]d[|][2][ <]_ Φ[−][1](q) _._ (24)
_σd[2]_ _· |_ _−_
  


B.1.1 UNIFORM SMOOTHING FOR l1 PERTURBATIONS OF CONTINUOUS DATA

An alternative base certificate for l1 perturbations is again due to Eiras et al. (2021). Using uniform
instead of Gaussian noise later allows us to collective certify robustness to l1-norm-bound perturbations. In the following U (x, λ) with x ∈ R[D], λ ∈ R[D]+ [refers to a vector-valued random distribution]
in which the d-th element is uniformly distributed in [xd − _λd, xd + λd]._

**Proposition 2. Given an output gn : R[D][in]** _→_ Y, let f (x) = argmaxy∈Y Prz∼U (x,λ) [g(z) = y]
_be the corresponding smoothed classifier with λ_ R[D]+[in][. Given an input][ x][ ∈] [R][D][in][ and smoothed]
_∈_
_prediction y = f_ (x), let p = Prz (x,λ) [g(z) = y]. Then, **_x[′]_** H[(][n][)] : fn(x[′]) = yn with H[(][n][)]
_∼U_ _∀_ _∈_

_defined as in Eq. 2, wd = 1/λd, η = Φ[−][1](q) and κ = 1._


_Proof. Based on the definition of H[(][n][)], we need to prove that ∀x[′]_ _∈_ H : fn(x[′]) = yn with


_Din_

_d=1_

X


**_x[′]_** _∈_ R[D][in] _|_


H =


1

_xd_ _x[′]d[|][ <][ Φ][−][1][(][q][)]_
_λd_ _· |_ _−_


(25)


Eiras et al. (2021) have shown that under the same conditions as above, a prediction yn is certifiably
robust to a perturbed input x[′] if


_Din_ [1] (xd _x[′]d[)][ |][ <][ 1]_

_d=1_ _|_ _λd_ _·_ _−_ 2

X


Φ[−][1](q) − Φ[−][1](1 − _q)_ _,_ (26)



where q[′] = maxyn′ _[̸][=][y][n]_ [Pr][z][∼U] [(][x][,][λ][)][ [][g][n][(][z][) =][ y]n[′] []][ is the probability of the second most likely predic-]
tion under the smoothing distribution. As in our previous proof for Gaussian smoothing, we can obtain a more pessimistic certificate by substituting 1−q for q[′]. Since Φ[−][1](q)−Φ[−][1](1−q) = 2Φ[−][1](q)
and all λd are non-negative, we know that our prediction is certifiably robust if

_Din_


1

_xd_ _x[′]d[|][ <][ Φ][−][1][(][p][)][.]_ (27)
_λd_ _· |_ _−_


_d=1_


B.2 VARIANCE SMOOTHING

We propose variance smoothing as a base certificate for binary data. Variance smoothing certifies
predictions based on the mean and variance of the softmax score associated with a predicted label.
It is in principle applicable to arbitrary data types. We focus on discrete data, but all results can


-----

be generalized from discrete to continuous data by replacing any sum over probability mass functions with integrals over probability density functions. We first derive a general form of variance
smoothing before discussing our certificates for binary data in Section B.2.1 and Section B.2.2.

Variance smoothing assumes that we make predictions by randomly smoothing a base model’s softmax scores. That is, given base model g : X → ∆|Y| mapping from an arbitrary discrete input space
X to scores from the |Y|-dimensional probability simplex ∆|Y|, we define the smoothed classifier
_f_ (x) = argmaxy∈YEz∼Ψ(x) [g(z)y]. Here, Ψ(x) is an arbitrary distribution over X parameterized
by x, e.g a Normal distribution with mean x. The smoothed classifier does not return the most likely
prediction, but the prediction associated with the highest expected softmax score.

Given an input x ∈ X, smoothed prediction y = f (x) and a perturbed input x[′] _∈_ X, we want to
determine whether f (x[′]) = y. By definition of our smoothed classifier, we know that f (x[′]) = y if
_y is the label with the highest expected softmax score. In particular, we know that f_ (x[′]) = y if y’s
softmax score is larger than all other softmax scores combined, i.e.

Ez∼Ψ(x′) [g(z)y] > 0.5 =⇒ _f_ (x[′]) = y. (28)

Computing Ez∼Ψ(x′) [g(z)y] exactly is usually not tractable – especially if we later want to evaluate
robustness to many x[′] from a whole perturbation model B ⊆ X. Therefore, we compute a lower
bound on Ez∼Ψ(x′) [g(z)y]. If even this lower bound is larger than 0.5, we know that prediction y
is certainly robust. For this, we define a set of functionssoftmax score across all functions from H: H with gy ∈ H and compute the minimum

min _f_ (x[′]) = y. (29)
_h∈H_ [E][z][∼][Ψ(][x][′][)][ [][h][(][z][)]][ >][ 0][.][5 =]⇒

For our variance smoothing approach, we define H to be the set of all functions that have a larger
or equal expected value and a smaller or equal variance, compared to our base model g applied to
unperturbed input x. Let µ = Ez Ψ(x) [g(z)y] be the expected softmax score of our base model
_∼_

_g for label y. Let σ[2]_ = Ez Ψ(x) (g(z)y _ν)[2][i]_ be the expected squared distance of the softmax
_∼_ _−_
score from a scalar ν ∈ R. (Choosingh _ν = µ yields the variance of the softmax score. An arbitrary_
_ν is only needed for technical reasons related to Monte Carlo estimation Section C.2). Then, we_
define
H = _h : X_ R Ez Ψ(x) [h(z)] _µ_ Ez Ψ(x) (h(z) _ν)[2][i]_ _σ[2][ o]_ (30)
_→_ _∼_ _≥_ _∧_ _∼_ _−_ _≤_

Clearly, by the definition ofH to the domain [0n, 1], but allow arbitrary real-valued outputs. µ and σ[2], we have gy ∈ H. Note that we do not restrict functions fromh

By evaluating Eq. 28 with H defined as in Eq. 29, we can determine if our prediciton is robust. To
compute the optimal value, we need the following two Lemmata:
**Lemma 1. Given a discrete set X and the set Π of all probability mass functions over X, any two**
_probability mass functions π1, π2_ Π fulfill
_∈_

_π2(z)_ (31)
_π1(z)_ _[π][2][(][z][)][ ≥]_ [1][.]

_zX∈X_

_Proof. For a fixed probability mass function π1, Eq. 31 is lower-bounded by the minimal expected_
likelihood ratio that can be achieved by another ˜π(z) ∈ Π:

_π2(z)_ _π˜(z)_

_π(z)._ (32)

_zX∈X_ _π1(z)_ _[π][2][(][z][)][ ≥]_ [min]π˜∈Π _zX∈X_ _π1(z) [˜]_

The r.h.s. term can be expressed as the constrained optimization problem


_π˜(z)_

_π(z)_ s.t.
_π1(z) [˜]_


_π˜(z) = 1_ (33)
_zX∈X_


min


_π˜_

_z∈X_

with the corresponding dual problem


_π˜(z)_

_π(z) + λ_
_π1(z) [˜]_


(34)


max
_λ∈R_ [min]π˜


_−1 +_


_π˜(z)_
_zX∈X_


_z∈X_


-----

The inner problem is convex in each ˜π(z). Taking the gradient w.r.t. to ˜π(z) for all z ∈ X shows
that it has its minimum at _z_ X : ˜π(z) = 2 . Substituting into Eq. 34 results in
_∀_ _∈_ _−_ _[λπ][1][(][z][)]_


_λ[2]π1(z)[2]_

+ λ
4π1(z)


_λπ1(z)_


(35)


_−1 −_

_π1(z)_


max
_λ∈R_


_z∈X_


_z∈X_

_−_ _λ_ (36)


_π1(z)_

= max _λ_ (36)
_λ∈R_ _[−][λ][2][ X]z∈X_ 4 _−_

= max (37)
_λ∈R_ _[−]_ _[λ]4[2]_ _[−]_ _[λ]_

= 1. (38)


Eq. 37 follows from the fact that π1(z) is a valid probability mass function. Due to duality, the
optimal dual value 1 is a lower bound on the optimal value of our primal problem Eq. 31.

**Lemma 2. Given a probability distribution** _over a R and a scalar ν_ R, let µ = Ez _and_
_D_ _∈_ _∼D_
_ξ = Ez_ (z _ν)[2][i]. Then ξ_ (µ _ν)[2]_
_∼D_ _−_ _≥_ _−_
h

_Proof. Using the definitions of µ and ξ, as well as some simple algebra, we can show:_

_ξ ≥_ (µ − _ν)[2]_ (39)

Ez (z _ν)[2][i]_ _µ[2]_ 2µν + ν[2] (40)
_⇐⇒_ _∼D_ _−_ _≥_ _−_
h

Ez _z[2]_ 2zν + ν[2][] _µ[2]_ 2µν + ν[2] (41)
_⇐⇒_ _∼D_ _−_ _≥_ _−_

_⇐⇒_ Ez∼D z[2] _−_ 2zν + ν[2][] _≥_ _µ[2]_ _−_ 2µν + ν[2] (42)

_⇐⇒_ Ez∼D z[2][] _−_ 2µν + ν[2] _≥_ _µ[2]_ _−_ 2µν + ν[2] (43)
⇐⇒ Ez∼D _z[2][]_ _≥_ _µ[2]_ (44)

It is well known for the variance that Ez (z _µ)[2][]_ = Ez _z[2][]_ _µ[2]. Because the variance is_
_∼D_ _−_ _∼D_ _−_
always non-negative, the above inequality holds.
 

Using the previously described approach and lemmata, we can show the soundness of the following
robustness certificate:

**Theorem 3. Given a model g** : X → ∆|Y| mapping from discrete set X to scores from
_the |Y|-dimensional probability simplex, let f_ (x) = argmaxy∈YEz∼Ψ(x) [g(z)y] be the corre_sponding smoothed classifier with smoothing distribution Ψ(x) and probability mass function_
_πx(z) = Prz˜_ Ψ(x) [[˜]z = z]. Given an input x X and smoothed prediction y = f (x), let
_∼_ _∈_

_µ = Ez_ Ψ(x) [g(z)y] and σ[2] = Ez Ψ(x) (g(z)y _ν)[2][i]_ _with ν_ R. If ν _µ, we know that_
_∼_ _∼_ _−_ _∈_ _≤_

_f_ (x[′]) = y if h


_πx′_ (z)[2]

_πx(z)_ _[<][ 1 +]_


_µ_
_−_ 2[1]


(45)


_σ[2]_ _−_ (µ − _ν)[2]_


**_z∈X_**


_Proof. Following our discussion above, we know that f_ (x[′]) = y if Ez∼Ψ(x′) [g(z)y] > 0.5 with H
defined as in Section 5. We can compute a (tight) lower bound on minh∈H Ez∼Ψ(x′) by following
the functional optimization approach for randomized smoothing proposed by Zhang et al. (2020).
That is, we solve a dual problem in which we optimize the value h(z) for each z ∈ X. By the
definition of the set H, our optimization problem is

min
_h:X→R_ [E][z][∼][Ψ(][x][′][)][ [][h][(][z][)]]

s.t. Ez Ψ(x) [h(z)] _µ,_ Ez Ψ(x) (h(z) _ν)[2][i]_ _σ[2]._
_∼_ _≥_ _∼_ _−_ _≤_
h


-----

The corresponding dual problem with dual variables α, β ≥ 0 is

max
_α,β≥0_ _h[min]:X→R_ [E][z][∼][Ψ(][x][′][)][ [][h][(][z][)]]

(46)
+α _µ_ Ez Ψ(x) [h(z)] + β Ez Ψ(x) (h(z) _ν)[2][i]_ _σ[2][]_ _._
_−_ _∼_ _∼_ _−_ _−_
    h

We first move move all terms that don’t involve h out of the inner optimization problem:

= max (h(z) _ν)[2][i]_ (47)
_α,β≥0_ _[αµ]_ _[−]_ _[βσ][2][ + min]h:X→R_ [E][z][∼][Ψ(][x][′][)][ [][h][(][z][)]] _[−]_ _[α][E][z][∼][Ψ(][x][)][ [][h][(][z][)]+]_ _[β][E][z][∼][Ψ(][x][)]_ _−_
h

Writing out the expectation terms and combining them into one sum (or – in the case of continuous
X – one integral), our dual problem becomes


= max _h(z)πx′_ (z) _αh(z)πx(z) + β (h(z)_ _ν)[2]_ _πx(z)_ (48)
_α,β≥0_ _[αµ][ −]_ _[βσ][2][ + min]h:X→R_ **_zX∈X_** _−_ _−_


(recall that πx[′] and πx[′] refer to the probability mass functions of the smoothing distributions). The
inner optimization problem can be solved by finding the optimal h(z) in each point z:


min (49)
**_zX∈X_** _h(z)∈R_ _[h][(][z][)][π][x][′]_ [(][z][)][ −] _[αh][(][z][)][π][x][(][z][) +][ β][ (][h][(][z][)][ −]_ _[ν][)][2][ π][x][(][z][)]_


= max
_α,β_ 0 _[αµ][ −]_ _[βσ][2][ +]_
_≥_


Because β ≥ 0, each inner optimization problem is convex in h(z). We can thus find the optimal
_h[∗](z) by setting the derivative to zero:_

_d_ !

= 0 (50)
_dh(z)_ _[h][(][z][)][π][x][′]_ [(][z][)][ −] _[αh][(][z][)][π][x][(][z][) +][ β][ (][h][(][z][)][ −]_ _[ν][)][2][ π][x][(][z][)]_

!
_πx′_ (z) _απx(z) + 2β (h(z)_ _ν) πx(z)_ = 0 (51)
_⇐⇒_ _−_ _−_

= _h[∗](z) =_ (52)
_⇒_ _−_ 2[π]βπ[x][′]x[(][z](z[)]) [+][ α]2β [+][ ν.]


Substituting into Eq. 48 and simplifying leaves us with the dual problem


max
_α,β≥0_ _[αµ][ −]_ _[βσ][2][ −]_ 4[α]β[2] [+][ α]2β 4β

_[−]_ _[αν][ +][ ν][ −]_ [1]


_πx′_ (z)[2] (53)

_πx(z)_


**_z∈X_**


In the following, let us use ρ = **_z∈X_** _ππxx′_ ((zz))[2] [as a shorthand for the expected likelihood ratio. The]

problem is concave in α. We can thus find the optimum α[∗] by setting the derivative to zero, which
gives us α[∗] = 2β(µ _ν) + 1. Because[P]_ _β_ 0 and ou theorem assumes that ν _µ, α[∗]_ is a feasible
_−_ _≥_ _≤_
solution to the dual problem. Substituting into Eq. 53 and simplifying results in

max (54)
_β_ 0 _[α][∗][µ][ −]_ _[βσ][2][ −]_ _[α]4[∗]β[2]_ [+][ α]2β[∗] 4β [ρ]
_≥_

_[−]_ _[α][∗][ν][ +][ ν][ −]_ [1]

= max (µ _ν)[2]_ _σ[2][]_ + µ + [1] (55)
_β_ 0 _[β]_ _−_ _−_ 4β [(1][ −] _[ρ][)][ .]_
_≥_
 

Lemma 1 shows that the expected likelihood ratio ρ is always greater than or equal to 1. Lemma 2
shows that (µ − _ν)[2]_ _−_ _σ[2]_ _≤_ 0. Therefore Eq. 55 is concave in β. The optimal value of β can again
be found by setting the derivative to zero:


1 _ρ_
_−_ (56)

4 ((µ _ν)[2]_ _σ[2])_ _[.]_
_−_ _−_


_β[∗]_ =


Recall that our theorem assumes σ[2] _≥_ (µ − _ν)[2]_ and thus β[∗] is real valued. Substituting into Eq. 56
shows that the maximum of our dual problem is


(1 − _p) ((µ −_ _ν)[2]_ _−_ _σ[2])._ (57)


_µ +_


-----

By duality, this is a lower bound on our primal problem minh∈H Ez∼Ψ(x′) [h(z)]. We know that our
prediction is certifiably robust, i.e. f (x) = y, if minh∈H Ez∼Ψ(x′) [h(z)] > 0.5. So, in particular,
our prediction is robust if


_µ +_ (1 − _ρ) ((µ −_ _ν)[2]_ _−_ _σ[2]) > 0.5_ (58)

2

p 1

_ρ < 1 +_ _µ_ (59)
_⇐⇒_ _σ[2]_ (µ _ν)[2]_ _−_ 2[1]

_−_ _−_  

2

_πx′_ (z)[2] 1

_µ_ (60)

**_zX∈X_** _πx(z)_ _[<][ 1 +]_ _σ[2]_ _−_ (µ − _ν)[2]_  _−_ 2[1] 


The last equivalence is the result of inserting the definition of the expected likelihood ratio ρ.

With Theorem 3 in place, we can certify robustness for arbitrary smoothing distributions, assuming
we can compute the expected likelihood ratio. When we are working with discrete data and the
smoothing distributions factorize (but are not necessarily i.i.d.), this can be done efficiently, as the
two following base certificates for binary data demonstrate.

B.2.1 BERNOULLI VARIANCE SMOOTHING FOR PERTURBATIONS OF BINARY DATA

We begin by proving the base certificate presented in Section 5. Recall that we we use a smoothing
distribution (x, θ) with θ [0, 1][D][in] that independently flips the d’th bit with probability θd,
_F_ _∈_
i.e. for x, z 0, 1 and z (x, θ) we have Pr[zd = xd] = θd.
_∈{_ _}[D][in]_ _∼F_ _̸_

**Theorem 1. Given an output gn : {0, 1}[D][in]** _→_ ∆|Y| mapping to scores from the |Y|-dimensional
_probability simplex, let fn(x) = argmaxy∈YEz∼F_ (x,θ) [gn(z)y] be the corresponding smoothed
_classifier with θ ∈_ [0, 1][D][in]. Given an input x ∈{0, 1}[D][in] _and smoothed prediction yn = fn(x),_
_let µ = Ez_ (x,θ) [gn(z)y] and σ[2] = Varz (x,θ) [gn(z)y]. Then, **_x[′]_** H[(][n][)] : fn(x[′]) = yn
_with H[(][n][)]_ _defined as in Eq. 2,∼F_ _wd = ln_ (1−θθdd∼F)[2] + 1[(][θ][d]θ[)]d[2] _, η = ln_ 1 + ∀σ1[2] _∈µ_ 2 2[] _and κ = 0._

_−_ _−_ [1]

     

_Proof. Based on our definition of the base certificate interface from Section 5, we must show that_
_∀x[′]_ _∈_ H : fn(x[′]) = yn with

_Din_ 2[!)]

(1 _θd)[2]_

H = (x[′] _∈{0, 1}[D][in]_ _d=1_ ln _−θd_ + 1[(] −[θ][d][)]θ[2]d ! _· |x[′]d_ _[−]_ _[x][d][|][0][ <][ ln]_ 1 + σ[1][2] µ − 2[1] 

X

(61)
Because all bits are flipped independently, our probability mass function πx(z) = Prz˜ Ψ(x) [[˜]z = z]
_∼_
factorizes:

_Din_

_πx(z) =_ _πxd_ (zd) (62)

_d=1_

Y


with

_πxd_ (zd) = _θd_ if zd ̸= xd _._ (63)
1 _θd_ else
 _−_

Thus, our expected likelihood ratio can be written as


_Din_

_d=1_

Y


_Din_

_d=1_

Y


_πx[′]d_ [(][z][d][)][2]

_πxd_ (zd) [=]


_πx[′]d_ [(][z][d][)][2]

(64)
_πxd_ (zd) _[.]_


_πx′_ (z)[2]

_πx(z) [=]_


**_z∈{0,1}[D][in]_**


**_z∈{0,1}[D][in]_**


_zd∈{0,1}_


For each dimension d, we can distinguish two cases: If both the perturbed and unperturbed input are

_πx′d_ [(][z][)]
the same in dimension d, i.e. x[′]d [=][ x][d][, then] _πxd_ (z) [= 1][ and thus]


_πx[′]d_ [(][z][d][)][2]

_πxd_ (zd) [=]


_πx[′]d_ [(][z][d][) =][ θ][d][ + (1][ −] _[θ][d][) = 1][.]_ (65)
_zd∈{X0,1}_


_zd∈{0,1}_


-----

If the perturbed and unperturbed input differ in dimension d, then


_πx[′]d_ [(][z][d][)][2] + [(][θ][d][)][2] _._ (66)

_πxd_ (zd) [= (1][ −]θd[θ][d][)][2] 1 _θd_

_−_


_zd∈{0,1}_


Therefore, the expected likelihood ratio is


_|x[′]d[−][x][d][|]_

(1 _θd)[2]_
_−_ + [(][θ][d][)][2]

_θd_ 1 _θd_ !

_−_


_Din_

_d=1_

Y


_Din_

_d=1_

Y


_πx[′]d_ [(][z][d][)][2]

_πxd_ (zd) [=]


(67)


_zd∈{0,1}_


Due to Theorem 3 (and using ν = µ when computing the variance), we know that our prediction is
robust, i.e. fn(x[′]) = yn, if


2
(68)



_πx′_ (z)[2]

_πx(z)_ _[<][ 1 + 1]σ[2]_


_µ_
_−_ 2[1]


**_z∈{0,1}[D][in]_**


_|x[′]d[−][x][d][|]_
!


2

_µ_ (69)
_−_ 2[1]



2[!]

_µ_ _._ (70)

[2] _−_ 2[1]

 

2[!]

_µ_ _._ (71)
_−_ [1]2




_Din_

_d=1_

Y


(1 _θd)[2]_
_−_ + [(][θ][d][)][2]

_θd_ 1 _θd_

_−_


_< 1 + [1]_

_σ[2]_


_µ_
_−_ 2[1]


_Din_

ln

_d=1_

X


(1 _θd)[2]_
_−_ + [(][θ][d][)][2]

_θd_ 1 _θd_

_−_


1 + [1]

_σ[2]_


_x[′]d_
_|_ _[−]_ _[x][d][|][ <][ ln]_


Because xd and x[′]d [are binary, the last inequality is equivalent to]


_Din_

ln

_d=1_

X


(1 _θd)[2]_
_−_ + [(][θ][d][)][2]

_θd_ 1 _θd_

_−_


1 + [1]

_σ[2]_


_x[′]d_
_|_ _[−]_ _[x][d][|][0][ <][ ln]_


B.2.2 SPARSITY-AWARE VARIANCE SMOOTHING FOR PERTURBATIONS OF BINARY DATA

Sparsity-aware randomized smoothing (Bojchevski et al., 2020) is an alternative smoothing approach for binary data. It uses different probabilities for randomly deleting (1 → 0) and adding
(0 → 1) bits to preserve data sparsity. For a random variable z distributed according to the
sparsity-aware distribution S(x, θ[+], θ[−]) with x ∈{0, 1}[D][in] and addition and deletion probabilities θ[+], θ[−] _∈_ [0, 1][D][in], we have:

Pr[zd = 0] = 1 _θd[+]_ 1−xd _θd[−]_ _xd,_
_−_ _·_

Pr[zd = 1] =  θd[+] 1−xd 1  θd[−]xd .
_·_ _−_

The Bernoulli smoothing distribution we discussed in the previous section is a special case of     
sparsity-aware smoothing with θ[+] = θ[−]. The runtime of the robustness certificate derived by
Bojchevski et al. (2020) increases exponentially with the number of unique values in θ[+] and θ[−],
which makes it unsuitable for localized smoothing. Variance smoothing, on the other hand, allows
us to efficiently compute a certificate in closed form.
**Theorem 2. Given an output gn : R[D][in]** _→_ ∆|Y| mapping to scores from the |Y|-dimensional prob_ability simplex, let fn(x) = argmaxy∈YEz∼S(x,θ+,θ−) [gn(z)y] be the corresponding smoothed_
_classifier withyn = fn(x), let θ[+] µ, θ =[−]_ E∈ **_z[0∼S, 1](x[D],[in]θ+._** _,θ−Given an input) [gn(z)y] and x σ ∈{[2]_ = Var0, 1}z[D]∼S[in] (xand smoothed prediction,θ+,θ−) [gn(z)y]. _Then,_
_∀x[′]_ _∈_ H : fn(x[′]) = y for

_Din_

H = **_x[′]_** 0, 1 _γd[+]_ _d[] +][ γ]d[−]_ _d[]][ < η]_ _,_ (72)

( _∈{_ _}[D][in]_ _|_ _d=1_ _[·][ I [][x][d][ = 0][ ̸][=][ x][′]_ _[·][ I [][x][d][ = 1][ ̸][=][ x][′]_ )

X


([θ]d[−][)]2 _d_ [)]
_where γ[+], γ[−]_ _∈_ R[D][in], γd[+] [= ln] 1−θd[+] [+ (][1][−]θ[θ]d[+][−]

ln 1 + _σ1[2]_ _µ_ 2 2[]. 

_−_ [1]

   


([1][−][θ]d[+][)]2 _d_ [)]2
_, γd[−]_ [= ln] _θd[−]_ + [(]1[θ]−[+]θd[−] _[.]_ _and η =_
 


-----

_Proof. Just like with the Bernoulli distribution we discussed in the previous section, all bits are_
flipped independently, meaning our probability mass function πx(z) = Prz˜ Ψ(x) [[˜]z = z] factor_∼_
izes:

_Din_


_πxd_ (zd) (73)
_d=1_

Y


_πx(z) =_


with

_πxd_ (zd) = _θd_ if zd ̸= xd _._ (74)
1 _θd_ else
 _−_

As before, our expected likelihood ratio can be written as


_Din_

_d=1_

Y


_Din_

_d=1_

Y


_πx[′]d_ [(][z][d][)][2]

_πxd_ (zd) [=]


_πx[′]d_ [(][z][d][)][2]

(75)
_πxd_ (zd) _[.]_


_πx[′]_ (z)[2]

_πx(z) [=]_


**_z∈{0,1}[D][in]_**


**_z∈{0,1}[D][in]_**


_zd∈{0,1}_


We can now distinguish three cases. If both the perturbed and unperturbed input are the same in

_πx′d_ [(][z][)]
dimension d, i.e. x[′]d [=][ x][d][, then] _πxd_ (z) [= 1][ and thus]


_πx[′]d_ [(][z][d][)][2]

_πxd_ (zd) [=]


_πx[′]d_ [(][z][d][) = 1][.] (76)
_zd∈{X0,1}_


_zd∈{0,1}_


If x[′]d [= 1][ and][ x][d][ = 0][, i.e. a bit was added, then]


2
_θd[−]_ +

1  − _θd[+]_

2
1 _θd[+]_
_−_
  _θd[−]_ 


2
1 _θd[−]_
_−_ (77)

_θd[+]_ 

2

+ _θd[+]_ _._ (78)

1  − _θd[−]_


_πx[′]d_ [(][z][)][2]

_πxd_ (z) [=]


_π1(zd)[2]_

_π0(zd) [=][ π]π[1]0[(0)](0) [2]_ [+][ π]π[1]0[(1)](1) [2] [=]


_zd∈{0,1}_


_zd∈{0,1}_


If x[′]d [= 0][ and][ x][d][ = 1][, i.e. a bit was deleted, then]


_πx[′]d_ [(][z][)][2]

_zd∈{X0,1}_ _πxd_ (z) [=]


_π0(zd)[2]_

_π1(zd) [=][ π]π[0]1[(0)](0) [2]_ [+][ π]π[0]1[(1)](1) [2] [=]


_zd∈{0,1}_


Therefore, the expected likelihood ratio is

_Din_

_d=1_

Y


_πx[′]d_ [(][z][d][)][2]

(79)
_πxd_ (zd)


_d=1_ _zd∈{0,1}_ _d_

I[xd=0≠ _x[′]d[|][]][  ]_ 2
1 _θd[+]_
_−_
! _θd[−]_ 


I[xd=1≠ _x[′]d[|][]]_
(80)
!


2
_θd[−]_

1 − _θd[+]_


2
1 _θd[−]_
_−_

_θd[+]_ 


2
_θd[+]_

1  − _θd[−]_


_Din_

_d=1_

Y


_Din_

= exp _γd[+]_ I[xd=0≠ _x[′]d[|][]]_ exp _γd[−]_ I[xd=1≠ _x[′]d[|][]]_ _._ (81)

_·_
_d=1_

Y      

In the last equation, we have simple used the shorthands γd[+] [and][ γ]d[−][, as defined in Theorem 2 Due]
to Theorem 3 (and using ν = µ when computing the variance), we know that our prediction is
robust, i.e. fn(x[′]) = yn, if


2
(82)



_πx[′]_ (z)[2]

_πx(z)_ _[<][ 1 + 1]σ[2]_


_µ_
_−_ 2[1]


**_z∈{0,1}[D][in]_**


2

_µ_ (83)
_−_ 2[1]
 

2[!]

_µ_ _._ (84)
_−_ 2[1]




_Din_

_⇐⇒_ exp _γd[+]_ I[xd=0≠ _x[′]d[|][]]_ _· exp_ _γd[−]_ I[xd=1≠ _x[′]d[|][]]_ _< 1 + σ[1][2]_

_d=1_

Y      

_Din_

_γd[+]_ _d[|][]][ ·][ γ]d[−]_ _d[|][]][ <][ ln]_ 1 + σ[1][2]
_d=1_ _[·][ I [][x][d][ = 0][ ̸][=][ x][′]_ _[·][ I [][x][d][ = 1][ ̸][=][ x][′]_

X


-----

It should be noted that this certificate does not comply with our interface for base certificates Section 5, meaning we can not directly use it to certify robustness to norm-bound perturbations using our
collective linear program from Section 6. We can however use it to certify collective robustness to the
more refined threat model used in (Schuchardt et al., 2021): Let the set of admissible perturbed inputs be Bx = **_x[′]_** _∈{0, 1}[D][in]_ _|_ _d=1_ [[][x][d][ = 0][ ̸][=][ x]d[′] _[|][]][ ≤]_ _[ϵ][+][ ∧]_ [P][D]d=1[in] [[][x][d][ = 1][ ̸][=][ x]d[′] _[|][]][ ≤]_ _[ϵ][−][o]_ with

_ϵ[+], ϵ[y]_ _∈_ N0 specifying the number of bits the adversary is allowed to add or delete. We can nown
follow the procedure outlined in Section 6 to combine the per-prediction base certificates into a[P][D][in]
collective certificate for our new collective perturbation model. As discussed in, we can bound the
number of predictions that are robust to simultaneous attacks by minimizing the number of predictions that are certifiably robust according to their base certificates:

min I [fn(x[′]) = yn] min I **_x[′]_** H[(][n][)][i] _._ (85)
**_x[′]∈Bx_** _nX∈T_ _≥_ **_x[′]∈Bx_** _nX∈T_ h _∈_

Inserting the linear inequalities characterizing our perturbation model and base certificates results
in:

_Din_

min I _γd[+]_ _d[] +][ γ]d[−]_ _d[]][ < η][(][n][)]_ (86)
**_x[′]_** 0,1
_∈{_ _}[D][in]_ _nX∈T_ "Xd=1 _[·][ I [][x][d][ = 0][ ̸][=][ x][′]_ _[·][ I [][x][d][ = 1][ ̸][=][ x][′]_ #

_Din_ _Din_

s.t. [xd = 0 ̸= x[′]d[|][]][ ≤] _[ϵ][+][,]_ [xd = 1 ̸= x[′]d[|][]][ ≤] _[ϵ][−][.]_ (87)

_d=1_ _d=1_

X X

Instead of optimizing over the perturbed input x[′], we can define two vectors b[+], b−∈{0, 1}[D][in]
that indicate in which dimension bits were added or deleted. Using these new variables, Eq. 86 can
be rewritten as

min I **_γ[+][][T]_** **_b[+]_** + **_γ[−][][T]_** **_b[−]_** _< η[(][n][)][i]_ (88)
**_b[+],b[−]_** 0,1
_∈{_ _}[D][in]_ _nX∈T_ h   

s.t. sum{b[+]} ≤ _ϵ[+],_ sum{b[−]} ≤ _ϵ[−],_ (89)

_b[+]d_ [= 0][,] _b[−]d_ [= 0][.] (90)
_d|Xxd=1_ _d|Xxd=0_

The last two constraints ensure that bits can only be deleted where xd = 1 and bits can only be
added where xd = 0. Finally, we can use the procedure for replacing the indicator functions with
indicator variables that we discussed in Section 6). Let η[(][n][)] be the minimum values the weighted
sums in the objective function can take on:

_η[(][n][)]_ = min **_γ[+][][T]_** **_b[+]_** + **_γ[−][][T]_** **_b[−]_** (91)
**_b[+],b[−]∈{0,1}[D][in]_**
   

s.t. sum{b[+]} ≤ _ϵ[+],_ sum{b[−]} ≤ _ϵ[−],_ (92)

_b[+]d_ [= 0][,] _b[−]d_ [= 0][.] (93)
_d|Xxd=1_ _d|Xxd=0_

The above problem can be solved by finding all negative entries of γ[+] and γ[−] and then summing up
the min sum 1 **_x_** _, b[+]d_ (or min sum **_x_** _, b[−]d_, respectively) smallest ones. Using these η[(][n][)]
_{_ _−_ _}_ _{_ _}_

and binary indicator variables t 0, 1, we can restate the above problem as the mixed-integer

   _∈{_   _}[D][out]_ 

problem

min _tn_ (94)
**_b[+],b[−]_** 0,1 _,t_ 0,1
_∈{_ _}[D][in]_ _∈{_ _}[D][out]_ _nX∈T_

s.t. **_γ[+][][T]_** **_b[+]_** + **_γ[−][][T]_** **_b[−]_** _tnη[(][n][)]_ + (1 _tn)η[(][n][)],_ (95)
_≥_ _−_
  sum **_b[+]_**   _ϵ[+],_ sum **_b[−]_** _ϵ[−],_ (96)

_{_ _} ≤_ _{_ _} ≤_

_b[+]d_ [= 0][,] _b[−]d_ [= 0][.] (97)
_d|Xxd=1_ _d|Xxd=0_

The first constraint ensures that tn can only be set to 0 if the l.h.s. is greater or equal ηn, i.e. only
when the base certificate can no longer guarantee robustness. The efficiency of the certificate can be
improved by applying any of the techniques discussed in Section A.


-----

C MONTE CARLO RANDOMIZED SMOOTHING

To make predictions and certify robustness, randomized smoothing requires computing certain properties of the distribution of a base model’s output, given an input smoothing distribution. For
example, the certificate of Cohen et al. (2019) assumes that the smoothed model f predicts the
most likely label output by base model g, given a smoothing distribution N (0, σ · 1): f (x) =
argmaxy∈Y Prz∼N (0,σ·1) [g(x + z) = y]. To certify the robustness of a smoothed prediction y =
_f_ (x) for a specific input x, we have to compute the probability q = Prz (0,σ **1) [g(x + z) = y]**
_∼N_ _·_
to then calculate the maximum certifiable radius σΦ[−][1](q) with standard-normal inverse CDF Φ[−][1].
For complicated models like deep neural networks, computing such properties in closed form is
usually not tractable. Instead, they have to be estimated using Monte Carlo sampling. The result are
predictions and certificates that only hold with a certain probability.

Randomized smoothing with Monte Carlo sampling usually consists of three distinct steps. First,
a small number of samples N1 from the smoothing distribution are used to generate a candidate
prediction ˆy, e.g. the most frequently predicted class. Then, a second round of N2 samples is taken
and a statistical test is used to determine whether the candidate prediction is likely to be the actual
prediction of smoothed classifier f, i.e. whether ˆy = f (x) with a certain probability (1 - α1).
If this is not the case, one has to abstain from making a prediction (or generate a new candidate
prediction). To certify the robustness of prediction ˆy, a final round of N3 samples is taken to estimate
all quantities needed for the certificate. In the case of (Cohen et al., 2019), we need to estimate the
probability q = Prz (0,σ **1) [g(x + z) = ˆy] to compute the certificate σΦ[−][1](q), whose strength**
_∼N_ _·_
is monotonically increasing in q. To ensure that the certificate holds with high probability (1 - α2),
we have to compute a probabilistic lower bound q ≤ _q. If one wants to certify robustness for all_
predictions, as we do in our experiments, one can also use the same samples for the abstention test
and the certificates. One particularly simple abstention mechanism is to just compute the Monte
Carlo randomized smoothing certificate to determine whether ∀x[′] _∈{x} : f_ (x[′]) = ˆy with high
probability, i.e. whether the prediction is robust to input x[′] that is the result of ”perturbing” clean
input x with zero adversarial budget.

In the following, we discuss how we perform Monte Carlo randomized smoothing for our base
certificates, as well as the baselines we use for our experimental evaluation. In Section C.4, we
discuss how we account for the multiple comparisons problem, i.e. the fact that we are not just
trying to probabilistically certify a single prediction, but multiple predictions at once.

C.1 MONTE CARLO BASE CERTIFICATES FOR CONTINUOUS DATA

For our base certificates for continuous data, we follow the approach we already discussed in the
previous paragraphs (recall that the certificate of Cohen et al. (2019) is a special case of our certificate with Gaussian noise for l2 perturbations). We are given an input space X[D][in], label space
Y, base model (or – in the case of multi-output classifiers – base model output) g : X[D][in] _→_ Y
and smoothing distribution Ψ(x) (either multivariate Gaussian or multivariate uniform). To generate a candidate prediction, we apply the base classifier to N1 samples from the smoothing
distribution in order to obtain predictions _y[(1)], . . ., y[(][N][1][)][]_ and compute the majority prediction
_yˆ = argmaxy∈Y_ _n | y[(][n][)]_ = ˆy . Recall that for Gaussian and uniform noise, our certificate guar- 
antees **_x[′]_** H : f (x) = ˆy with
_∀_ _∈_ 


_Din_

_wd_ _x[′]d_
_d=1_ _· |_ _[−]_ _[x][d][|][p][ < η]_

X


**_x[′]_** _∈_ X[D][in]


H =


2
with _η_ = Φ[−][1](q) or _η_ = Φ[−][1](q) (depending on the distribution), _q_ =
Prz (0,σ **1) [g(x + z) = ˆy] and standard-normal inverse CDF Φ[−][1]. To obtain a probabilistic cer-**
_∼N_ _·_   
tificate that holds with high probability 1 − _α, we need a probabilistic lower bound on η. Both η are_
monotonically increasing in q, i.e. we can bound them by finding a lower bound q on q. For this, we
take N2 more samples from the smoothing distribution and compute a Clopper-Pearson lower confidence bound (Clopper & Pearson, 1934) on q. For abstentions, we use the aforementioned simple


-----

mechanism: We test whether x ∈ H. Given the definition of H, this is equivalent to testing whether

0 < Φ[−][1](q)

_⇐⇒_ Φ(0) < q

_⇐⇒_ 0.5 < q.


If q ≤ 0.5, we abstain.

C.2 MONTE CARLO VARIANCE SMOOTHING

In variance smoothing, we smooth a model’s softmax scores. That is, we are given an input space
X[D][in], label space Y, base model (or – in the case of multi-output classifiers – base model output)
_g : X[D][in]_ _→_ ∆|Y| with |Y|-dimensional probability simplex ∆|Y| and smoothing distribution Ψ(x)
(Bernoullli or sparsity-aware noise, in the case of binary data). To generate a candidate prediction,
we apply the base classifier to N1 samples from the smoothing distribution in order to obtain vectors
**_s[(1)], . . ., s[(][N][1][)][]_** with s ∈ ∆|Y|, compute the average softmax scores s = _N11_ _Nn=1_ **_[s][ and select]_**

the label with the highest score ˆy = arg maxy sy.

  P

Recall that our certificate guarantees robustness if the optimal value of the following optimization
problem is greater than 0.5:

min (98)
_h:X→R_ [E][z][∼][Ψ(][x][′][)][ [][h][(][z][)]]

s.t. Ez Ψ(x) [h(z)] _µ,_ Ez Ψ(x) (h(z) _ν)[2][i]_ _σ[2],_ (99)
_∼_ _≥_ _∼_ _−_ _≤_
h

with µ = Ez Ψ(x) [g(z)yˆ[]][,][ σ][2][ =][ E]z Ψ(x) (g(z)yˆ and a fixed scalar ν R. To obtain a
_∼_ _∼_ _[−]_ _[ν][)][2][i]_ _∈_
probabilistic certificate, we have to compute a probabilistic lower bound on the optimal value of theh
optimization problem. Because it is a minimization problem, this can be achieved by loosening its
constraints, i.e. computing a probabilistic lower bound µ on µ and a probabilistic upper bound σ[2]

on σ[2].

Like in CDF-smoothing (Kumar et al., 2020), we bound the parameters using CDF-based nonparametric confidence intervals. Let F (s) = Prz Ψ(x) [g(z)yˆ _yˆ_
_∼_ _[≤]_ _[s][]][ be the CDF of the distribution of][ g]_
under the smoothing distribution Ψ(x). Define M thresholds 0τ1 _τ2 . . ., τM_ 1 _τM_ 1
_≤_ _≤_ _−_ _≤_ _≤_
with ∀m : τm ∈ [0, 1]. We then take N2 samples x[(1)], . . ., x[(][N][2][)] from the smoothing distribution
to compute the empirical CDF _F[˜](s) =_ _n=1_ [I] _g(z[(][n][)])yˆ_ _[≤]_ _[s]_ . We can then use the DvoretzkyKeifer-Wolfowitz inequality (Dvoretzky et al., 1956) to compute an upper bound _F[ˆ] and a lower_
 
bound F on the CDF of gyˆ[:] [P][N][2]

_F_ (s) = max _F˜(s) −_ _υ, 0_ _≤_ _F_ (s) ≤ min _F˜(s) + υ, 1_ = F (s), (100)
   

with υ = ln 22·N/α2 [, which holds with high probability][ (1][ −] _[α][)][. Using these bounds on the CDF, we]_

can boundq µ = Ez Ψ(x) [g(z)yˆ[]][ as follows (Anderson, 1969):]
_∼_


_M_ _−1_

(τm+1 _τm) F_ (τm). (101)
_m=1_ _−_

X


_µ_ _τM_ _τ1F_ (τ1) +
_≥_ _−_


The parameter σ[2] = Ez Ψ(x) (g(z)yˆ can be bounded in a similar fashion. Define
_∼_ _[−]_ _[ν][)][2][i]_
_ξ0, . . ., ξM ∈_ R+ with: h

_ξ0 = max_ (κ _ν)[2][]_
_κ_ [0,τ1] _−_
_∈_

 

_ξM =_ _κ_ max[τM _,1]_ (κ − _ν)[2][]_ (102)
_∈_

 

_ξm =_ max (κ _ν)[2][]_ _m_ 1, . . ., M 1 _,_
_κ_ [τm,τm+1] _−_ _∀_ _∈{_ _−_ _}_
_∈_

 


-----

i.e. compute the maximum squared distance to ν within each bin [τm, τm+1]. Then:


_M_ _−1_

_m=1_ _ξm (F_ (τm+1 − _F_ (τm)) (103)

X


_σ[2]_ _≤_ _ξ0F_ (τ1) + ξM (1 − _F_ (τM )) +


_M_ _−1_

_m=1_ (ξm−1 − _ξm) F_ (τm) (104)

X

_M_ _−1_

_m=1_ sgn (ξm−1 − _ξm) F_ (τm) + (1 − sgn (ξm−1 − _ξm)) F_ (τm) (105)

X


= ξM +

_≤_ _ξM +_


with probability (1 − _α). In the first inequality, we bound the expected squared distance from ν by_
assuming that the probability mass in each bin [τm, τm+1] is concentrated at the farthest point from
_ν. The equality is a result of reordering the telescope sum. In the second inequality, we upper-bound_
the CDF where it is multiplied with a non-negative value and lower-bound it where it is multiplied
with a negative value.

With the probabilistic bounds µ and σ[2] we can now – in principle – evaluate our robustness certificate, i.e. check whether

2

_πx′_ (z)[2] 1

**_zX∈X_** _πx(z)_ _[<][ 1 +]_ _σ[2]_ _−_ _µ −_ _ν_ 2 µ − [1]2  _._ (106)

where the π are the probability mass functions of smoothing distributions   Ψ(x) and Ψ(x[′]). But one
crucial detail of Theorem 3 underlying the certificate was that it only holds for ν ≤ _µ, i.e. only when_
this condition is fulfilled can we compute the certificate in closed form by solving the corresponding
dual problem. To use the method with Monte Carlo sampling, one has to ensure that ν ≤ _µ by first_
computing µ and then choosing some smaller ν.

In our experiments, we use an alternative method that allows us to use arbitrary ν: From our proof
of Theorem 3we know that the dual problem of Eq. 98 is


max
_α,β≥0_ _[αµ][ −]_ _[βσ][2][ −]_ 4[α]β[2] [+][ α]2β 4β

_[−]_ _[αν][ +][ ν][ −]_ [1]


_πx′_ (z)[2] (107)

_πx(z)_ _[,]_


_α,β≥0_ **_z∈X_** **_x_**

Instead of trying to find an optimal α (which causes problems in subsequent derivations if ν ≰ _µ),_
we can simply choose α = 1. By duality, the result is still a lower bound on the primal problem,
i.e. the certificate remains valid. The dual problem becomes

_πx′_ (z)[2]

max (108)
_β≥0_ _[µ][ −]_ _[βσ][2][ + 1]4β_ _[−]_ 4[1]β **_zX∈X_** _πx(z)_ _[.]_

The problem is concave in β (because the expected likelihood ratio is ≥ 1). Finding the optimal β,
comparing the result to 0.5 and solving for the expected likelihood ratio, shows that a prediction is
robust if

2

_πx′_ (z)[2]

_µ_ _._ (109)

**_zX∈X_** _πx(z)_ _[<][ 1 + 1]σ[2]_  _−_ 2[1] 

For our abstention mechanism, like in the previous section, we compute the certificate H and then
test whether x ∈ H. In the case of Bernoulli smoothing and sparsity-aware smoothing), this corresponds to testing whether

1 < ln 1 + [1] _µ_ (110)

_σ[2]_ _−_ 2[1]

  

_µ >_ [1] (111)
_⇐⇒_ 2 _[.]_

C.3 MONTE CARLO CENTER SMOOTHING

While we can not use center smoothing as a base certificate, we benchmark our method against it
during our experimental evaluation. The generation of candidate predictions, the abstention mechanism and the certificate are explained in (Kumar & Goldstein, 2021). The authors allow multiple


_πx′_ (z)[2] (108)

_πx(z)_ _[.]_


-----

options for generating candidate predictions. We use the ”β minimum enclosing ball” with β = 2
that is based on pair-wise distance calculations.

C.4 MULTIPLE COMPARISONS PROBLEM

The first step of our collective certificate is to compute one base certificate for each of the Dout
predictions of the multi-output classifier. With Monte Carlo randomized smoothing, we want all of
these probabilistic certificates to simultaneously hold with a high probability (1 − _α). But as the_
number of certificates increases, so does the probability of at least one of them being invalid. To
account for this multiple comparisons problem, we use Bonferroni (Bonferroni, 1936) correction,
i.e. compute each Monte Carlo certificate such that it holds with probability (1 − _[α]n_ [)][.]

For base certificates that only depend on qn = Prz∼Ψ(n) [gn(z) = ˆyn], i.e. the probability of the
base classifier predicting a particular label ˆyn under the smoothing distribution, one can also use the
strictly better Holm correction (Holm, 1979). This includes our Gaussian and uniform smoothing
certificates for continuous data. Holm correction is a procedure than can be used to correct for
the multiple comparisons problem when performing multiple arbitrary hypothesis tests. Given N
hypotheses, their p-values are ordered in ascending order p1, . . ., pN . Starting at i = 1, the i’th
hypothesis is rejected if pi < _N_ +1α _i_ [, until one reaches an][ i][ such that][ p][i][ ≥] _N_ +1α _i_ [.]

_−_ _−_

Fischer et al. (2021) proposed to use Holm correction as part of their procedure for certifying that all
(non-abstaining) predictions of an image segmentation model are robust to adversarial perturbations.
In the following, we first summarize their approach and then discuss how Holm correction can be
used for certifying our notion of collective robustness, i.e. certifying the number of robust predictions. As in Section C.1, the goal is to obtain a lower bound qn on qn = Prz∼Ψ(n) [gn(z) = ˆyn] for
each of the Dout classifier outputs. Assume we take N2 samples z[(1)], . . ., z[(][N][2][)] from the smoothing
distribution. Let νn = _i=1_ [I] _gn(z[(][i][)]) = ˆyn_ and let π : {1, . . ., Dout} →{1, . . ., Dout} be a
bijection that orders the νn in descending order, i.e. νπ(1) _νπ(2)_ _νπ(Dout). Instead of using_
Clopper-Pearson confidence intervals to obtain tight lower bounds on the  _≥_ _· · · ≥_ _qn, Fischer et al. (2021)_

[P][N][2]
define a threshold τ [0.5, 1) and use Binomial tests to determine for which n the bound τ _qn_
_∈_ _≤_
holds with high-probability. Let BinP (νn, N2, _, τ_ ) be the p-value of the one-sided binomial test,
_≤_
which is monotonically decreasing in νn. Following the Holm correction scheme, the authors test
whether
_α_
BinP _νπ(k), N2, ≤, τ_ _<_ _Dout + 1 −_ _k_ (112)

for k = 1, . . ., Dout until reaching a  _k[∗]_ for which the null-hypothesis can no longer be rejected,
i.e. the p-value is g.e.q. _Dout+1α_ _−k[∗]_ [. They then know that with probability][ 1][ −] _[α][, the bound][ τ][ ≤]_ _[q][n]_

holds for all n ∈{π(k) | k ∈{1, . . ., k[∗]}. For these outputs, they use the lower bound τ to compute
robustness certificates. They abstain with all other outputs.

This approach is sensible when one is concerned with the least robust prediction from a set of
predictions. But our collective certificate benefits from having tight robustness guarantees for each
of the individual predictions. Holm correction can be used with arbitrary hypothesis tests. For
instance, we can use a different threshold τn per output gn, i.e. test whether


_α_

(113)
_Dout + 1 −_ _k_


BinP _νπ(k), N2,_ _, τpi(k)_ _<_
_≤_

for k = 1, . . ., Dout. In particular, we can use  

_τn = supt_ s.t. BinP (νn, N2, ≤, t) <


_α_
_τn = sup_ s.t. BinP (νn, N2, _, t) <_ (114)
_t_ _≤_ _Dout + 1 −_ _π[−][1](n)_ _[,]_

i.e. choose the largest threshold such that the null hypothesis can still be rejected. Eq. 114 is the
lower Clopper-Pearson confidence bound with significance _Dout+1α−π[−][1](n)_ [. This means that, instead]

of performing hypothesis tests, we can obtain probabilistic lower bounds qn _qn by computing_
_α_
Clopper-Pearson confidence bounds with significance parameters _Dout_ _[, . . .,][ α]1 ≤[. The][ q]n_ [can then be]

used to compute the base certificates. Due to the definition of the τn, all of the null hypotheses are
rejected, i.e. we obtain valid probabilistic lower bounds on all qn. We can thus use the abstention
mechanism from Section C.1, i.e. only abstain if q 0.5.
_n_
_≤_

A direct consequence of the results above is that using Clopper-Pearson confidence intervals and
Holm correction will yield stronger per-prediction robustness guarantees and lower abstention rates


-----

than the method of Fischer et al. (2021). The Clopper-Pearson-based method only abstains if one
cannot guarantee that qn > 0.5 with high probability, while their method abstains if one cannot
guarantee that qn _τ with τ_ 0.5 (or specific other predictions abstain). For all non-abstaining
predictions, the Clopper-Pearson-based certificate will be at least as strong as that obtained using ≥ _≥_
a single threshold τ, as it computes the tightest bound for which the null hypothesis can still be
rejected (see Eq. 114). Consequently, a na¨ıve collective robustness certificate (i.e. counting the
number of predictions whose robustness are guaranteed by the base certificates) based on ClopperPearson bounds will also be stronger. It should however be noted that this is only relevant for our
notion of collective robustness, not for the one considered by Fischer et al. (2021). It should also
be noted that their method could potentially be used with other methods of family-wise error rate
correction, although they state that “these methods do not scale to realistic segmentation problems”
and do not discuss any further details.

Our above discussion shows that, for our notion of collective robustness, the na¨ıve collective certificate using Clopper-Pearson confidence bounds and Holm correction is at least as strong as that
of Fischer et al. (2021) (or alternatively: the certificate of Fischer et al. (2021) is a na¨ıve collective
certificate with Holm correction, but with weaker per-prediction certificates). Conversely, the certificate based on Clopper-Pearson confidence bounds is at least as strong as that of Fischer et al. (2021)
when certifying their notion of collective robustness, i.e. determining whether all non-abstaining
predictions are robust, given adversarial budget ϵ. To certify this notion of robustness, they simply
iterate over all predictions and determine whether all non-abstaining predictions are certifiably robust, given ϵ. Naturally, as the Clopper-Pearson-based certificates are stronger, any prediction that
is robust according to (Fischer et al., 2021) is also robust acccording to the Clopper-Pearson-based
certificates. The only difference is that, for τ > 0.5, their method will have more abstaining predictions. But, due to the direct correspondence of Clopper-Pearson confidence bounds and Binomial
tests, we can modify our abstention mechanism to obtain exactly the same set of abstaining predictions: We simply have to use qn _τ instead of qn_ 0.5 as our abstention criterion. Finally, it
should be noted that the proposed collective certificate is at least as strong as the na¨ ≤ _≤_ ıve collective certificate (see Section 4). Thus, letting the set of targeted predictions T be the set of all non-abstaining
predictions and checking whether the collective certificate guarantees robustness for all of T will
also result in a certificate that is at least as strong as that of Fischer et al. (2021) in their setting.

In our experiments (see Section 8), we use Holm correction for our na¨ıve collective certificate and
the weaker Bonferroni correction for our base certificates. This is only meant to slightly skew the
results in favor of our baselines. Holm correction can in principle also be used when computing the
base certificates, in order to improve our proposed collective cert.


-----

D COMPARISON TO THE COLLECTIVE CERTIFICATE OF SCHUCHARDT ET AL.

In the following, we first present the collective certificate for binary graph-structured data proposed
by Schuchardt et al. (2021) (see Section D.1. We then show that, when using sparsity-aware smoothing distributions (Bojchevski et al., 2020) – the family of smoothing distributions used both in our
work and that of Schuchardt et al. (2021) – our certificate subsumes their certificate. That is, our
collective robustness certificate based on localized randomized smoothing can provide the same
robustness guarantees (see Section D.2).

D.1 THE COLLECTIVE CERTIFICATE

Their certificate assumes the input space to be G = {0, 1}[N] _[×][D]_ _×{0, 1}[N]_ _[×][N]_ – the set of undirected
attributed graphs with N nodes and D attributes per node. The model is assumed to be a multioutput classifier f : G → Y[N] that assigns a label from label set Y to each of the nodes. Given an
input graph G = (X, A) and a corresponding prediction y = f (G), they want to certify collective
robustness to a set of perturbed graphs B ⊆ G. The perturbation model B is characterized by four
scalar parameters rX[+] _[, r]X[−]_ _[, r]A[+][, r]A[+]_
add (0 → 1) and delete (1 → 0) in the attribute and adjacency matrix, respectively. It can also[∈] [N][0][, specifying the number of bits the adversary is allowed to]
be extended to feature additional constraints (e.g. per-node budgets). We discuss how these can be
integrated after showing our main result. A formal definition of the perturbation model can be found
in Section B of (Schuchardt et al., 2021).

The goal of their work is to certify collective robustness for a set of targeted nodes T ⊆{1, . . ., N _},_
i.e. compute a lower bound on


I [fn(G[′]) = yn] . (115)
_nX∈T_


min
_G[′]∈B_


Their approach to obtaining this lower-bound shares the same high-level idea as ours: Combining
per-prediction base certificates and leveraging some notion of locality. But while our method uses
localized randomized smoothing, i.e. smoothing different outputs with different non-i.i.d. smoothing
distributions, to obtain base certificates that encode locality, their method uses a-priori knowledge
about the strict locality of the classifier f . A model is strictly local if each of its outputs fn only
operates on a well-defined subset of the input data. To encode this strict locality, Schuchardt et al.
(2021) associate each output fn with an indicator vector ψ[(][n][)] and an indicator matrix Ψ[(][n][)] that
fulfill


_d=1_ _ψm[(][n][)][I]_ _Xm,d ̸= Xi,j[′]_

X 


_j=1_ Ψ[(]m[n][)][I] _Am,d ̸= A[′]i,j_ = 0

X  

_fn(X, A) = fn(X_ _[′], A[′])._
_⇒_


(116)


_m=1_


_i=1_


for any perturbed graph G[′] = (X _[′], A[′]). Eq. 116 expresses that the prediction of output fn remains_
unchanged if all inputs in its receptive field remain unchanged. Conversely, it expresses that perturbations outside the receptive field can be ignored. Unlike in our work, Schuchardt et al. (2021)
describe their base certificates as sets in adversarial budget space. That is, some certification procedure is applied to each output fn to obtain a set

K[(][n][)] _⊆_ [rX[+] []][ ×][ [][r]X[−] []][ ×][ [][r]A[+][]][ ×][ [][r]X[−] []] (117)

with [k] = 0, . . ., k . If _c[+]X_ _c[−]X_ _c[+]A_ _c[−]A_ _T_ K(n), then prediction yn is robust to any
_{_ _}_ _∈_
perturbed input with exactly c [+]X [attribute additions,] _[ c]X[−]_ [attribute deletions,][ c]A[+] [edge additions and]
_c[−]A_ [edge deletions. A more detailed explanation can be found in Section 3 of (Schuchardt et al.,]
2021). Note that the base certificates only depend on the number of perturbations, not their location in the input. Only combining them using the receptive field indicators from Eq. 116 makes it
possible to obtain a collective certificate that is better than a na¨ıve combination of the base certificates (i.e. counting how many predictions are certifiably robust to the collective threat model). The


-----

resulting collective certificate is


I **_ψ[(][n][)][][T]_** **_b[+]X_** **_ψ[(][n][)][][T]_** **_b[−]X_**
_nX∈T_ h   


_T_
_i,j_ [Ψ][(]i,j[n][)][B]i,j[−] _∈_ K[(][n][)]
i 

(118)

_N_

_j=1_ _Bi,j[−]_ _[≤]_ _[r]A[−][,]_ (119)

X


_i,j_ [Ψ]i,j[(][n][)][B]i,j[+]

P

_N_

_j=1_ _Bi,j[+]_ _[≤]_ _[r]A[+][,]_

X


min
**_b[+],b[+],B[+],B[−]_**


s.t.


_m=1_ _b[+]m_ _[≤]_ _[r]X[+]_ _[,]_

X


_m=1_ _b[−]m_ _[≤]_ _[r]X[−]_ _[,]_

X


_i=1_


_i=1_


**_b[+], b[−]_** N[N]0 **_B[+], B[−]_** N[N]0 _[×][N]_ _._ (120)
_∈_ _∈_

The variables defined in Eq. 120 model how the adversary allocates their adversarial budget, i.e. how
many attributes are perturbed per node and which edges are modified. Eq. 119 ensures that this
allocation in compliant with the collective threat model. Finally, in Eq. 118 the indicator vector and
matrix ψ[(][n][)] and Ψ[(][n][)] are used to mask out any allocated perturbation budget that falls outside the
receptive field of fn before evaluating its base certificate.

To solve the optimization problem, Schuchardt et al. (2021) replace each of the indicator functions
with binary variables and include additional constraints to ensure that they have value 1 i.f.f. the
indicator function would have value 1. To do so, they define one linear constraint per point separating
(n)
the set of certifiable budgets K[(][n][)] from its complement K in adversarial budget space (the ”pareto
front” discussed in Section 3 of (Schuchardt et al., 2021)).

From the above explanation, the main drawbacks of this collective certificate compared to our localized randomized smoothing approach and corresponding collective certificate should be clear.
Firstly, if the classifier f is not strictly local, i.e. the receptive field indicators ψ and Ψ only have
non-zero entries, then all base certificates are evaluated using the entire collective adversarial budget. It thus degenerates to the na¨ıve collective certificate. Secondly, even if the model is strictly
local, each of the output may assign varying levels of importance to different parts of its receptive
field. Their method is incapable of capturing this additional soft locality. Finally, their means of
evaluating the base certificates may involve evaluating a large number of linear constraints. Our
method, on the other hand, only requires a single constraint per prediction. Our collective certificate
can thus be more efficiently computed.

D.2 PROOF OF SUBSUMPTION

In the following, we show that any robustness certificate obtained by using the collective certificate
of Schuchardt et al. (2021) with sparsity-aware randomized smoothing base certificates can also be
obtained by using our proposed collective certificate with an appropriately parameterized localized
smoothing distribution. The fundamental idea is that, for randomly smoothed models, completely
randomizing all input dimensions outside a receptive field is equivalent to masking out any perturbations outside the receptive field.

First, we derive the certificate of Schuchardt et al. (2021) for predictions obtained via sparsityaware smoothing. Schuchardt et al. (2021) require base certificates that guarantee robustness when
_c[+]X_ _c[−]X_ _c[+]A_ _c[−]A_ _T_ K(n), where the c indicate the number of added and deleted attribute and
_∈_
adjacency bits. That is, the certificates must only depend on the number of perturbations, not on
 
their location. To achieve this, all entries of the attribute matrix and all entries of the adjacency
matrix, respectively, must share the same distribution. For the attribute matrix, they define scalar
distribution parameters p[+]X _[, p][−]A_
random attribute matrices ZX that are distributed according to sparsity-aware smoothing distribu-[∈] [[0][,][ 1]][. Given attribute matrix][ X][ ∈{][0][,][ 1][}][N] _[×][D][, they then sample]_
tion **_X, 1_** _p[+]X_ _[,][ 1][ ·][ p][−]X_ (see Section B.2.2), i.e.
_S_ _·_
  Pr[( _ZX_ )m,d = 0] = 1 _p[+]X_ 1−Xm,d _p[−]X_ _Xm,d,_

_−_ _·_

Pr[(ZX )m,d = 1] =  p[+]X 1−Xm,d 1  p[−]X Xm,d .
_·_ _−_

Given input adjacency matrix A, random adjacency matrices     ZA are sampled from the distribution
**_A, 1_** _p[+]A[,][ 1][ ·][ p][−]A_ . Applying Theorem 2 (to the flattened and concatenated attribute and adja_S_ _·_
cency matrices) shows that smoothed prediction yn = fn(X, A) is robust to the perturbed graph
  


-----

(X _[′], A[′]) if_


_γX[+]_ _Xm,d = 0_ = Xm,d[′] + γX[−] _Xm,d = 1_ = Xm,d[′]

_m=1_ _d=1_ _[·][ I]_ _̸_ _[·][ I]_ _̸_

X X   

_N_ _N_

+ _γA[+]_ _Ai,j = 0_ = A[′]i,j + γA[−] _Ai,j = 1_ = A[′]i,j

_i=1_ _i=1_ _[·][ I]_ _̸_ _[·][ I]_ _̸_

X X    

_< η[(][n][)]_


(121)


([p][−]X[)]2 **_X[)]2_** ([1][−][p][+]X[)]2 **_X[)]2_**
with _γX[+]_ = ln 1−p[+]X [+ (][1][−]p[p][+]X[−], _γX[−]_ = ln _p[−]X_ + [(]1[p]−[+]p[−]X _[.]_, _γA[+]_ =

ln (1[p]−[−]Ap[)][+]A2 [+ (][1][−]p[p][+]AA[−][)]2 , γA[−] [= ln] ([1][−]p[p][−]A[+]A[)]2 + [(]1[p]−A[+]p[)][−]A2 _[.]_ and η[(][n][)] = ln 1 + _σ[(]1[n][)2]_ µ[(][n][)] _−_ 2[1] 2[],

where _µ[(][n][)]_ is the mean and _σ[(][n][)]_ is the variance of the base classifier’s output distribution, given the     
input smoothing distribution. Since the indicator functions for each perturbation type in Eq. 121
share the same weights, Eq. 121 can be rewritten as

_γX[+]_ _[c][+]X_ [+][ γ]X[−] _[c][−]X_ [+][ γ]A[+][c][+]A [+][ γ]A[−][c][−]A _[≤]_ _[η][(][n][)][,]_ (122)

where c[+]X _[, c][−]X_ _[, c][+]A[, c][−]A_ [are the overall number of added and deleted attribute and adjacency bits,]
respectively. Eq. 122 matches the notion of base certificates defined by Schuchardt et al. (2021),
i.e. it corresponds to a set K[(][n][)] in adversarial budget space for which we provably know that prediction yn is certifiably robust if _c[+]X_ _c[−]X_ _c[+]A_ _c[−]A_ _T_ K(n). When this base certificate is used,
_∈_
i.e. we insert the base certificate Eq. 122 into objective function Eq. 118, the collective certificate of
 
Schuchardt et al. (2021) becomes equivalent to


_γX[+]_ **_ψ[(][n][)][][T]_** **_b[+]X_** [+][ γ]X[−] **_ψ[(][n][)][][T]_** **_b[−]X_**
 


min
**_b[+],b[+],B[+],B[−]_**


_n∈T_


Ψ[(]i,j[n][)][B]i,j[+] [+]
_i,j_

X


_i,j_ _γA[−][Ψ]i,j[(][n][)][B]i,j[−]_ _[≤]_ _[η][(][n][)]_

X


+γA[+]

_N_

_i=1_

X


(123)


_m=1_ _b[+]m_ _[≤]_ _[r]X[+]_ _[,]_

X


_m=1_ _b[−]m_ _[≤]_ _[r]X[−]_ _[,]_

X


_j=1_ _B[+]i,j_ _[≤]_ _[r]A[+][,]_

X


_j=1_ _B[−]i,j_ _[≤]_ _[r]A[−][,]_ (124)

X


s.t.


_i=1_


**_b[+], b[−]_** N[N]0 **_B[+], B[−]_** N[N]0 _[×][N]_ _._ (125)
_∈_ _∈_

Next, we show that obtaining base certificates through localized randomized smoothing with appropriately chosen parameters and using these base certificates within our proposed collective certificate (see Section 6) will result in the same optimization problem. Instead of using the same
smoothing distribution for all outputs, we use different distribution parameters for each one. For the
(n) (n)[]
_n’th output, we sample random attributes matrices from distribution_ **_X, Θ[+]X_** _, Θ−X_ with
_S_

**Θ[+]X** (n), Θ−X (n) [0, 1]N _×D. Note that, in order to avoid having to index flattened vectors, we over-_
_∈_
load the definition of sparsity-aware smoothing to allow for matrix-valued parameters. For example,
(n)
the value Θ[+]X _n,d_ [indicates the probability of flipping the value of input attribute][ X][n,d][ from][ 0][ to][ 1]

(n)
and the value Θ[−]X _n,d_ [indicates the probability of flipping the value of input attribute][ X][n,d][ from][ 1][ to]
0. We choose the following values for these parameters:

(n)
Θ[+]X _m,d_ [=][ ψ]m[(][n][)] _[·][ p]X[+]_ [+] 1 − _ψm[(][n][)]_ _· 0.5,_ (126)

(n)  
Θ[−]X _m,d_ [=][ ψ]m[(][n][)] _[·][ p]X[−]_ [+] 1 − _ψm[(][n][)]_ _· 0.5,_ (127)
 

where ψ[(][n][)] is the receptive field indicator vector defined in Eq. 116 and p[+]X _[,][ ·][p][−]X_
the same flip probabilities we used for the certificate of Schuchardt et al. (2021). Due to this[∈] [[0][,][ 1]][ are]
parameterization, attribute bits inside the receptive field are randomized using the same distribution as in the certificate of Schuchardt et al. (2021), while attribute bits outside are set to either


-----

0 or 1 with equal probability. Similarly, we sample random adjacency matrices from distribution
**_A, Θ[+]A(n), Θ−A(n)[]_** with Θ[+]A(n), Θ−A(n) [0, 1]N _×D and_
_S_ _∈_


(n)
Θ[+]Ai,j [= Ψ]i,j[(][n][)] _[·][ p]A[+]_ [+] 1 − Ψ[(]i,j[n][)] _· 0.5,_ (128)

(n)  
Θ[−]Au,j [= Ψ]i,j[(][n][)] _[·][ p]A[−]_ [+] 1 − Ψ[(]i,j[n][)] _· 0.5,_ (129)
 

where Ψ[(][n][)] is the receptive field indicator matrix defined in Eq. 116. Note that, since we only alter
the distribution of bits outside the receptive field, the smoothed prediction yn = fn(X, A) will
be the same as the one obtained via the smoothing distribution used by Schuchardt et al. (2021).
Applying Theorem 2 (to the flattened and concatenated attribute and adjacency matrices) shows that
smoothed prediction yn = fn(X, A) is robust to the perturbed graph (X _[′], A[′]) if_


_m=1_ _d=1_ _τX[+]_ _m,d_ _[·][ I]_ _Xm,d = 0 ̸= Xm,d[′]_ + τX[−] _m,d_ _[·][ I]_ _Xm,d = 1 ̸= Xm,d[′]_

X X   


(130)


_i=1_ _j=1_ _τA[+]i,j_ _[·][ I]_ _Ai,j = 0 ̸= A[′]i,j_ + τA[−] i,j _[·][ I]_ _Ai,j = 1 ̸= A[′]i,j_

X X   


_< η[(][n][)]._

Because we only changed the distribution outside the receptive field, the scalar η[(][n][)], which depends
on the output distribution’s mean and variance µ and σ will be the same as the one obtained via the
smoothing scheme used by Schuchardt et al. (2021) et al. Due to Theorem 2 and the definition of
our smoothing distribution parameters in Eqs. (126) to (129), the scalars τX[+] _m,d[, τ]X[ −]_ _m,d[, τ]A[ +]i,j[, τ]A[ −] i,j_
have the following values:


(1 0.5)[2] 0.5[2]
_−_ +

0.5 1 − 0.5

(1 0.5)[2] 0.5[2]
_−_ +

0.5 1 − 0.5

(1 0.5)[2] 0.5[2]
_−_ +

0.5 1 − 0.5

(1 0.5)[2] 0.5[2]
_−_ +

0.5 1 − 0.5


_τX[+]_ _m,d_ [=][ ψ]m[(][n][)] _[·][ γ]X[+]_ [+] 1 − _ψm[(][n][)]_ _· 2 · ln_
 

_τX[−]_ _m,d_ [=][ ψ]m[(][n][)] _[·][ γ]X[−]_ [+] 1 − _ψm[(][n][)]_ _· 2 · ln_
 

_τA[−] i,j_ [= Ψ][(]i,j[n][)] _[·][ γ]A[+]_ [+] 1 − Ψ[(]i,j[n][)] _· 2 · ln_
 

_τA[−] i,j_ [= Ψ][(]i,j[n][)] _[·][ γ]A[−]_ [+] 1 − Ψ[(]i,j[n][)] _· 2 · ln_
 


(131)

(132)

(133)

(134)


where the γ are the same weights as those of the base certificate Eq. 121 of Schuchardt et al.
(2021). Inserting the above values of τ into the base certificate Eq. 130 and using the fact that
ln (1−00.5.5)[2] + 10−.50[2].5 = ln(1) = 0 results in
 


_ψm[(][n][)]_ **_X_** _Xm,d = 0_ = Xm,d[′] + ψm[(][n][)] **_X_** _Xm,d = 1_ = Xm,d[′]

_m=1_ _d=1_ _[·][ γ][+]_ _[·][ I]_ _̸_ _[·][ γ][−]_ _[·][ I]_ _̸_

X X   


(135)


Ψ[(]i,j[n][)] **_A_** _Ai,j = 0_ = A[′]i,j + Ψ[(]i,j[n][)] **_A_** _Ai,j = 1_ = A[′]i,j
_j=1_ _[·][ γ][−]_ _[·][ I]_ _̸_ _[·][ γ][−]_ _[·][ I]_ _̸_

X   


_i=1_


_< η[(][n][)]._

While our collective certificate derived in Section 6 only considers one perturbation type, we have
already discussed how to certify robustness to perturbation models where there are multiple perturbation types in Section B.2.2: We use a different budget variable per input dimension and perturbation type. Furthermore, the attribute bits of each node share the same noise level. Therefore, we can
use the method discussed in Section A.3, i.e. use a single budget variable per node instead of using


-----

one per node and attribute. Modelling our collective problem in this way, using Eq. 135 as our base
certificates and rewriting the first two sums using inner products results in the optimization problem


_γX[+]_ **_ψ[(][n][)][][T]_** **_b[+]X_** [+][ γ]X[−] **_ψ[(][n][)][][T]_** **_b[−]X_**
 


min
**_b[+],b[+],B[+],B[−]_**


_n∈T_


Ψ[(]i,j[n][)][B]i,j[+] [+]
_i,j_

X


_i,j_ _γA[−][Ψ]i,j[(][n][)][B]i,j[−]_ _[≤]_ _[η][(][n][)]_

X


+γA[+]

_N_

_i=1_

X


(136)


s.t.


_m=1_ _b[+]m_ _[≤]_ _[r]X[+]_ _[,]_

X


_m=1_ _b[−]m_ _[≤]_ _[r]X[−]_ _[,]_

X


_j=1_ _B[+]i,j_ _[≤]_ _[r]A[+][,]_

X


_j=1_ _B[−]i,j_ _[≤]_ _[r]A[−][,]_ (137)

X


_i=1_


**_b[+], b[−]_** N[N]0 **_B[+], B[−]_** N[N]0 _[×][N]_ _._ (138)
_∈_ _∈_

This optimization problem is identical to that of Schuchardt et al. (2021) from Eqs. (123) to (125).
The only difference is in how these problems would be mapped to a mixed-integer linear program.
We would directly model the indicator functions in the objective using a single linear constraint.
Schuchardt et al. (2021) would use multiple linear constraints, each corresponding to one point in
the adversarial budget space.

To summarize: For randomly smoothed models, masking out perturbations using a-priori knowledge about a model’s strict locality is equivalent to completely randomizing (here: flipping bits with
probability 50%) parts of the input. While Schuchardt et al. (2021) only derived their certificate for
binary data, it is conceivable that their approach could be applied to strictly local models for continuous data. Considering our certificates for Gaussian (Proposition 1) and (Proposition 2) uniform
smoothing, where the base certificate weights are _σ1[2][ and][ 1]λ_ [, respectively, it should again be possible]

to perform the same masking operation as Schuchardt et al. (2021) by using σ →∞ and λ →∞.

Finally, it should be noted that the certificate by Schuchardt et al. (2021) allows for additional constraints, e.g. on the adversarial budget per node or the number of nodes controlled by the adversary.
As all of them can be modelled using linear constraints on the budget variables (see Section C of their
paper), they can be just as easily integrated into our mixed-integer linear programming certificate.

E EXPERIMENTAL SETUP AND ADDITIONAL EXPERIMENTS

In the following, we first explain the metrics we use for measuring the strength of certificates, and
how they can be applied to the different types of randomized smoothing certificates used in our
experiments. We then discuss the experimental setup, as well hyperparameters and additional experimental results for our semantic segmentation Section E.2 and node classification Section 8.2
experiments.

E.1 METRICS

We use two metrics for measuring certificate strength: Certified accuracy (i.e. the percentage of
correct and certifiably robust predictions) and certified ratio (i.e. the percentage of certifiably robust
predictions, regardless of correctness). We use Monte Carlo randomized smoothing (see Section C.
Therefore, we may have to abstain from making predictions. Abstentions are counted as non-robust
and incorrect. In the case of center smoothing, either all or none predictions abstain (this is inherent
to the method. In our experiments, center smoothing never abstained). In the following, let Z =
_{d ∈{1, . . ., Dout} | yn = ˆyn} be the indices of correct predictions, given an input x._

**Na¨ıve collective certificate. The na¨ıve collective certificate certifies each prediction independently.**
Like with our base certificates, let H[(][n][)] be the set of perturbed inputs yn is robust to. Let Bx be
the collective perturbation model. Thencertifiably robust predictions. The certified ratio can be computed as L = d ∈{1, . . ., Dout} |D B|Lout|x ⊆[and the certified accuracy]H[(][n][)] is the set of all

as _[|]D[L][∩]out[Z][|]_ [.]

**Center smoothing Center smoothing does not determine which predictions are robust, but only how**
many are. Let k be the number of certifiably robust predictions. The certified ratio can be easily


-----

computed as _Dkout_ [. For the certified accuracy, we have to make the worst-case assumption that the]

non-robust predictions are correct predictions, i.e. compute [max(0][,k][−]D[(]out[D][out][−|][Z][|][))]

**Collective certificate. Let l(T) be the optimal value of our collective certificate and set of targeted**
nodes T. Then both the certified ratio and certified accuracy can be computed via _Dl(outT)_ [with][ T][ =]

_{1, . . ., Dout} and T = Z, respectively._

E.2 SEMANTIC SEGMENTATION

We first list all parameters and details of our training and certification procedures, before discussing
additional experimental results in Section E.2.3.

E.2.1 EXPERIMENTAL SETUP AND HYPERPARAMETERS

**Model. As the base model for the semantic segmentation task, we use a U-Net model (Ronneberger**
et al., 2015) with a ResNet-18 (He et al., 2016) backbone, as implemented by the Pytorch Segmentation Models library (version 0.13) (Yakubovskiy, 2020). We use the library’s default parameters.
In particular, the inputs to the U-Net segmentation head are the features of the ResNet model after
the first convolutional layer and after each ResNet block (i.e. after every fourth of the subsequent
layers). The U-Net segmentation head uses (starting with the original resolution) 16, 32, 64, 128
and 256 convolutional filters for processing the features at the different scales. To avoid dimension
mismatches in the segmentation head, all input images are zero-padded to a height and width that is
the next multiple of 32.

**Data and preprocessing. We evaluate our certificates on the Pascal-VOC 2012 segmentation vali-**
dation set. We do not use the test set, because evaluating metrics like the certified accuracy requires
access to the ground-truth labels. For training, we use the 10582 Pascal segmentation masks extracted from the SBD dataset (Hariharan et al., 2011) (referred to as ”Pascal trainaug” or ”Pascal
augmented training set” in other papers). SBD uses a different data split than the official PascalVOC 2012 segmentation dataset. We avoid data leakage by removing all training images that appear
in the validation set. We downscale both the training and the validation images and ground-truth
masks to 50% of their original height and width, so that we can use larger batch sizes and thus
use our compute time to more thoroughly evaluate a larger range of different smoothing distributions. The segmentation masks are downscaled using nearest-neighbor interpolation, the images are
downscaled using the INTER AREA operation implemented in OpenCV (Bradski, 2000).

**Training and data augmentation. We initialize our model weights using the weights provided by**
the Pytorch Segmentation Models library, which were obtained by pre-training on ImageNet. We
train our models for 256 epochs, using Dice loss with batch size 64 and Adam(lr = 0.001, β1 =
0.9, β2 = 0.999, ϵ = 10[−][8], weight decay = 0. Every 8 epochs, we compute the mean IOU on
the validation set. After training, we use the model that achieved the highest validation mean IOU.
We apply the following train-time augmentations: Each image is randomly shifted by up to 10%
of its height and width, scaled by a factor from [0.5, 2.0] and rotated between −10 and 10 degrees
using the ShiftScaleRotate augmentation implemented by the Albumentations library (version 0.5.2)
(Buslaev et al., 2020). The images are than cropped to a fixed size of 128×160. Where necessary, the
images are padded with zeros. Padded parts of the segmentation mask are ignored during training.
After these operations, each input is randomly perturbed using a Gaussian distribution with a fixed
standard deviation σ ∈{0, 0.01, . . ., 0.5}. All samples are clipped to [0, 1] to retain valid RGBimages.

**Certification. We evaluate all certificates on the first 50 images from the validation set that – after**
downscaling – have a resolution of 166×250. For our experiments, we use Monte Carlo randomized
smoothing (see discussion in Section C). We use 12288 samples for making smoothed predictions
and 153600 samples for certification. We use the significance parameter α to 0.01, i.e. all certificates hold with probability 0.99. For the center smoothing baseline, we use the default parameters
suggested by the authors (∆= 0.05, β = 2, α1 = α2). For the na¨ıve collective certificate baseline,
we use Holm correction to account for the multiple comparisons problem, which yields strictly better results than Bonferroni correction. For our localized smoothing certificates, we use Bonferroni
correction. For our localized smoothing distribution, we partition the input image into a regular grid
of size 4 × 6 and define minimum standard deviation σmin and maximum standard deviation σmax.


-----

Let J[(][k,l][)] be the set of all pixel coordinates in grid cell (k, l). To smooth outputs in grid cell (i, j),
we use a smoothing distribution N (0, diag(σ) with, ∀k ∈{1, . . ., 4}, l ∈{1, . . ., 6}, d ∈ J[(][k,l][)],

_σd = σmin + (σmax_ _σmin)_ [max (][|][i][ −] _[k][|][,][ |][l][ −]_ _[j][|][)]_ _,_ (139)
_−_ _·_ 6

i.e. we linearly interpolate between σmin and σmax based on the l distance of grid cells (i, j)
_∞_
and (k, l). All results are reported for the relaxed linear programming formulation of our collective
certificate (see Section A.4). For each grid cell, we use 241 [=] 416 [of the samples, which corresponds]

_·_
to 512 samples for prediction and 6400 samples for certification. The collective linear program is
solved using MOSEK (version 9.2.46) (MOSEK ApS, 2019) through the CVXPY interface (version
1.1.13) (Diamond & Boyd, 2016).

E.2.2 HARDWARE

All experiments on image segmentation were performed using a Xeon E5-2630 v4 CPU @ 2.20GHz,
an NVIDA GTX 1080TI GPU and 128 GB of RAM.

E.2.3 COMPLETE EXPERIMENTAL EVALUATION

As explained in Section 8.1, the objective of our experiments on semantic segmentation is to verify
our claim that localized smoothing allows for a better accuracy-robustness trade-off than randomized smoothing with i.i.d. distributions, i.e. it allows us to retain a higher prediction quality and
simultaneously provide stronger collective robustness guarantees.

The first step of our evaluation procedure is to evaluate the na¨ıve and the center smoothing baselines
for Gaussian standard deviations σ ∈{0.1, 0.2, 0.3, 0.4, 0.5} and – for each σ – find a combination
of localized smoothing parameters σmin, σout that result in equal or higher accuracy and stronger
certificates, as measured by certified accuracy and certified ratio. Figs. 5 to 8 show the certified
accuracy and certified ratio curve for all 10 pairs of baseline and locally smoothed models (5 per
baseline). The AUC of the certified ratio and certified ratio curves for all smoothed models are
summarized in Tables 1 and 2. Safe for center smoothing with σ = 0.2, which has a slightly higher
certified ratio but lower accuracy, we can outperform all baselines.

The second step is to fix the found localized smoothing parameters and perform a fine-grained search
over σ ∈ [0, 1]. This is to ensure that our previous results were not caused by a particularly poor
choice of baseline parameters. Table 5 summarizes the results. It shows that there is no σ in our
search space for which the baselines outperform the locally smoothed model w.r.t. our metrics of
certificate strength.

Figure Fig. 4 demonstrates why there is no σ that outperforms our baselines, using the certified ratio
of the na¨ıve collective certificate baseline as an example. The certificate strength, as measured by
the certified ratio’s AUC, increases with σ. But the model’s accuracy decreases with σ. Therefore,
strengthening the certificate requires sacrificing some accuracy. But the locally smoothed models
were already as accurate or more accurate than the baselines, which makes it impossible for the baselines to simultaneously attain a higher accuracy and stronger certificates. To summarize: Localized
smoothing appears to indeed offer a better trade-off between certified accuracy and robustness.

In Section 8.1, we mentioned that we compare base classifiers with different trained weights. For
i.i.d. smoothing with σ, we loaded models trained with σtrain = σ. For localized smoothing, we
loaded models trained with σtrain = σmin. To show that this is not the reason why we outperform
the baselines, we compare the same i.i.d. and localized smoothing distributions as before, but use
_σtrain = σmin for all models. The results are summarized in Appendix E.2.3 and Table 4. Overall,_
there is only a single instance in which the baselines are better than the localized smoothing certificate in any of our metrics: For σ = 0.3, the na¨ıve baseline attains an accuracy of 79.5% that is
0.4 p.p. higher than that of the locally smoothed model. The AUC of its certified accuracy and certified ratio curves – 0.492 and 0.5615, respectively – are however much smaller than the 0.5869 and
0.6579 guaranteed by localized smoothing. So, even with this different choice of weights, localized
smoothing appears to guarantee a better trade-off between accuracy and provable robustness.

While it resulted in significantly stronger certificates, the proposed collective certificate added very
little additional computational cost, compared to the na¨ıve collective certificate. The na¨ıve collective certificate requires taking Monte Carlo samples and then performing a small number of vector


-----

operations to compute the per-prediction Gaussian smoothing certificates. The proposed collective
certificates requires taking Monte Carlo samples and computing the same type of per-prediction
certificate, but then solves a linear program on top. The sampling dominated the runtime, with an
average 460 s per image (both for the baseline and the proposed certificate, as we used the same number of samples for both). Averaged over all experiments and images, computing the per-prediction
certificates took 15.85 s for the baseline and 0.15 s for the proposed certificate (the increased cost
for the baseline is due to the more complicated Holm correction – with Bonferroni correction both
are equally fast). Averaged over all images and tested adversarial budgets, solving each collective
linear program only took 0.68 s, i.e. much less than the Monte Carlo sampling that is necessary for
both the baseline and our method.


1.0

0.6

0.8

0.4 0.6

0.4

0.2 Accuracy

0.2

Certified ratio AUC

0.0 0.0

0.0 0.1 0.2 0.3 0.4 0.5 0.0 0.1 0.2 0.3 0.4 0.5

_σ_ _σ_


(a) Certificate strength


(b) Accuracy


Figure 4: Trade-off between accuracy and certifiably robustness at the example of the na¨ıve collective certificate and certified ratio AUC. Increasing the standard deviation σ strengthens the certificate, but decreases the accuracy.


-----

|Na¨ıve collective certificate|Col2|Localized smoothing|Col4|
|---|---|---|---|
|σ|AUC of AUC of Acc. Cert. Acc. Cert. Ratio|σ σ min max|AUC of AUC of Acc. Cert. Acc. Cert. Ratio|
|0.1 0.2 0.3 0.4 0.5|87.7% 0.1555 0.1702 83.8% 0.3127 0.3515 79.1% 0.4899 0.548 72.5% 0.5426 0.6067 64.9% 0.5684 0.6444|0.055 0.28 0.16 0.35 0.25 0.7 0.25 1.5 0.3 1.5|87.7% 0.1807 0.193 83.9% 0.3627 0.3925 79.1% 0.5869 0.6579 76.4% 0.7764 0.8999 74.8% 0.7143 0.8091|


Na¨ıve collective certificate Localized smoothing

AUC of AUC of AUC of AUC of
_σ_ Acc. Cert. Acc. Cert. Ratio _σmin_ _σmax_ Acc. Cert. Acc. Cert. Ratio

0.1 87.7% 0.1555 0.1702 0.055 0.28 87.7% **0.1807** **0.193**

0.2 83.8% 0.3127 0.3515 0.16 0.35 **83.9%** **0.3627** **0.3925**

0.3 79.1% 0.4899 0.548 0.25 0.7 79.1% **0.5869** **0.6579**

0.4 72.5% 0.5426 0.6067 0.25 1.5 **76.4%** **0.7764** **0.8999**

0.5 64.9% 0.5684 0.6444 0.3 1.5 **74.8%** **0.7143** **0.8091**

Table 1: Comparison of the na¨ıve collective certificate baseline and localized smoothing for σ ∈
_{0.1, 0.2, 0.3, 0.4, 0.5} and different σmin and σmax. In all cases, the localized smoothing certificate_
retains an equal or higher accuracy and simultaneously provides stronger robustness guarantees.

|Center smoothing|Col2|Localized smoothing|Col4|
|---|---|---|---|
|σ|AUC of AUC of Acc. Cert. Acc. Cert. Ratio|σ σ min max|AUC of AUC of Acc. Cert. Acc. Cert. Ratio|
|0.1 0.2 0.3 0.4 0.5|88.3% 0.1673 0.194 84.7% 0.281 0.3464 80.3% 0.3192 0.4339 75% 0.2316 0.3720 68.3% 0.1498 0.2914|0.08 0.15 0.12 0.4 0.2 0.6 0.25 1.5 0.25 1.5|88.4% 0.1889 0.2004 84.8% 0.3151 0.3427 80.5% 0.4935 0.5533 76.4% 0.7764 0.8999 76.4% 0.7764 0.8999|



Table 2: Comparison of the center smoothing baseline and localized smoothing for σ _∈_
_{0.1, 0.2, 0.3, 0.4, 0.5} and different σmin and σmax. Safe for σ = 0.2, where the baseline has a_
slightly higher certified ratio AUC but a lower accuracy, localized smoothing retains an equal or
higher accuracy and simultaneously provides stronger robustness guarantees.

|Na¨ıve collective certificate|Col2|Localized smoothing|Col4|
|---|---|---|---|
|σ|AUC of AUC of Acc. Cert. Acc. Cert. Ratio|σ σ min max|AUC of AUC of Acc. Cert. Acc. Cert. Ratio|
|0.1 0.2 0.3 0.4 0.5|86.9% 0.1519 0.1686 83.8% 0.3184 0.357 79.5% 0.492 0.5615 73.5% 0.6094 0.711 72.5% 0.7001 0.7981|0.055 0.28 0.16 0.35 0.25 0.7 0.25 1.5 0.3 1.5|87.7% 0.1807 0.193 83.9% 0.3627 0.3925 79.1% 0.5869 0.6579 76.4% 0.7764 0.8999 74.8% 0.7143 0.8091|


Na¨ıve collective certificate Localized smoothing

AUC of AUC of AUC of AUC of
_σ_ Acc. Cert. Acc. Cert. Ratio _σmin_ _σmax_ Acc. Cert. Acc. Cert. Ratio

0.1 86.9% 0.1519 0.1686 0.055 0.28 **87.7%** **0.1807** **0.193**

0.2 83.8% 0.3184 0.357 0.16 0.35 **83.9%** **0.3627** **0.3925**

0.3 **79.5%** 0.492 0.5615 0.25 0.7 79.1% **0.5869** **0.6579**

0.4 73.5% 0.6094 0.711 0.25 1.5 **76.4%** **0.7764** **0.8999**

0.5 72.5% 0.7001 0.7981 0.3 1.5 **74.8%** **0.7143** **0.8091**


Table 3: Comparison of the na¨ıve collective certificate baseline and localized smoothing for
_σ ∈{0.1, 0.2, 0.3, 0.4, 0.5} and different σmin and σmax. Unlike in Table 1, we also load the_
model trained with σmin for the baselines. Safe for σ = 0.3, where the baseline has a higher accuracy but much weaker certificates, localized smoothing again offers a higher accuracy and stronger
certificates.

|Center smoothing|Col2|Localized smoothing|Col4|
|---|---|---|---|
|σ|AUC of AUC of Acc. Cert. Acc. Cert. Ratio|σ σ min max|AUC of AUC of Acc. Cert. Acc. Cert. Ratio|
|0.1 0.2 0.3 0.4 0.5|88.3% 0.1624 0.19 83.9% 0.268 0.3392 80% 0.3721 0.5009 74.9% 0.3494 0.5397 62.9% 0.1656 0.4212|0.08 0.15 0.12 0.4 0.2 0.6 0.25 1.5 0.25 1.5|88.4% 0.1889 0.2004 84.8% 0.3151 0.3427 80.5% 0.4935 0.5533 76.4% 0.7764 0.8999 76.4% 0.7764 0.8999|



Table 4: Comparison of the center smoothing baseline and localized smoothing for σ _∈_
_{0.1, 0.2, 0.3, 0.4, 0.5} and different σmin and σmax. Unlike in Table 2, we also load the model_
trained with σmin for the baselines. In all cases, the locally smoothed model has a higher accuracy
and stronger certificates.


-----

|Localized smoothing|Col2|Baselines|Col4|
|---|---|---|---|
|σ σ Acc. min max|AUC of AUC of Cert. Acc. Cert. Ratio|Baseline|Best Best AUC of AUC of Cert. Acc Cert. Ratio|
|0.055 0.28 87.7% 0.16 0.35 83.9% 0.25 0.7 79.1% 0.25 1.5 76.4% 0.3 1.5 74.8%|0.1807 0.193 0.3627 0.3925 0.5869 0.6579 0.7764 0.8999 0.7143 0.8091|Na¨ıve Collective Certificate|0.1555 0.1702 0.296 0.3293 0.4899 0.548 0.5753 0.6465 0.5753 0.6465|
|0.08 0.15 88.4% 0.12 0.4 84.8% 0.2 0.6 80.5% 0.25 1.5 76.4%|0.1889 0.2004 0.3151 0.3427 0.4935 0.5533 0.7764 0.8999|Center Smoothing|0.1488 0.1704 0.2715 0.3286 0.3306 0.4255 0.3538 0.5029|


Table 5: Optimizing baseline standard deviation σ. For the same (σmin, σmax) as in Tables 1 and 2,
we perform a grid search for σ ∈ [0, 0.5] that yield certified accuracy or certified ratio curves with
higher AUC, and an accuracy that at least equals the locally smoothed models. Such a σ could not
be found. Localized smoothing appears to offer a better accuracy-robustness trade-off.


-----

1.0

0.2

0.0

|Col1|Col2|Col3|Col4|
|---|---|---|---|
||LP|||
||loca Na¨ıv|lized e||
||loca|lized||
||Na¨ıv|e i.i.d.||
|||||
|||||


0.0 0.5 1.0 1.5 2.0


Adversarial budget ϵ

(b) σ = 0.2, σmin = 0.16, σmax = 0.35


1.0

LP

0.8 localized

0.6 Na¨ıve

localized

0.4 Na¨ıve i.i.d.

Certified ratio 0.2

0.0

0.0 0.5 1.0 1.5 2.0

Adversarial budget ϵ


(a) σ = 0.1, σmin = 0.055, σmax = 0.28

1.0

LP

0.8 localized

0.6 Na¨ıve

localized

0.4 Na¨ıve i.i.d.

Certified ratio 0.2

0.0

0.0 0.5 1.0 1.5 2.0

Adversarial budget ϵ


(c) σ = 0.3, σmin = 0.25, σmax = 0.7


1.0

0.2

0.0

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||LP||||
||loca Na¨ıv||lized e||
||loca||lized||
||Na¨ıv||e i.i.d.||
||||||
||||||


0.0 0.5 1.0 1.5 2.0


Adversarial budget ϵ

(d) σ = 0.4, σmin = 0.25, σmax = 1.5


1.0

0.8

0.6

0.4

0.2

0.0

|Col1|Col2|
|---|---|
||LP|
||localize Na¨ıve|
||localize|
||Na¨ıve i.i|
|||
|||


LP
localized
Na¨ıve

localized
Na¨ıve i.i.d.


0.0 0.5 1.0 1.5 2.0

Adversarial budget ϵ


(e) σ = 0.5, σmin = 0.3, σmax = 1.5

Figure 5: Certified ratios of U-Net models under varying adversarial budgets ϵ. We compare the
na¨ıve i.i.d. smoothing baseline (with standard deviation σ) to localized smoothing (with parameters σmin, σmax such that the locally smoothed model has a higher or equal accuracy). Combining the localized smoothing base certificates using the proposed linear program (solid orange
line) instead of evaluating them independently (dotted orange line) yields stronger guarantees. For
_σ ∈{0.3, 0.4, 0.5}, the localized smoothing certificate outperforms the baseline for all ϵ._


-----

1.0

0.2

0.0

|Col1|LP|Col3|
|---|---|---|
||||
||loc Na¨ı|alized ve|
||loc|alized|
||Na¨ı|ve i.i.d.|
||||
||||


LP
localized
Na¨ıve

localized
Na¨ıve i.i.d.

0.0 0.5 1.0 1.5 2.0


1.0

0.2

0.0

|Col1|Col2|Col3|P|Col5|
|---|---|---|---|---|
|||L|P||
|||l N|ocalized a¨ıve||
|||l|ocalized||
|||N|a¨ıve i.i.d.||
||||||
||||||


0.0 0.5 1.0 1.5 2.0


Adversarial budget ϵ

(a) σ = 0.1, σmin = 0.055, σmax = 0.28


Adversarial budget ϵ

(b) σ = 0.2, σmin = 0.16, σmax = 0.35


1.0

0.2

0.0

|Col1|LP|Col3|
|---|---|---|
||||
||loc Na¨ı|alized ve|
||loc|alized|
||Na¨ı|ve i.i.d.|
||||
||||


LP
localized
Na¨ıve

localized
Na¨ıve i.i.d.

0.0 0.5 1.0 1.5 2.0


1.0

0.2

0.0

|Col1|Col2|Col3|P|Col5|
|---|---|---|---|---|
|||L|P||
|||l N|ocalized a¨ıve||
|||l|ocalized||
|||N|a¨ıve i.i.d.||
||||||
||||||


0.0 0.5 1.0 1.5 2.0


Adversarial budget ϵ


Adversarial budget ϵ

(d) σ = 0.4, σmin = 0.25, σmax = 1.5


(c) σ = 0.3, σmin = 0.25, σmax = 0.7

1.0

0.8

0.6

0.4

0.2

Certified accuracy

0.0

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||LP||
|||loc Na¨ı|alized ve|
|||loc|alized|
|||Na¨ı|ve i.i.d.|
|||||
|||||


0.0 0.5 1.0 1.5 2.0

Adversarial budget ϵ


(e) σ = 0.5, σmin = 0.3, σmax = 1.5

Figure 6: Certified accuracies of U-Net models under varying adversarial budgets ϵ. We compare
the na¨ıve collective certificate baseline (with standard deviation σ) to localized smoothing (with
parameters σmin, σmax such that the locally smoothed model has a higher or equal accuracy). Combining the localized smoothing base certificates using the proposed linear program (solid orange
line) instead of evaluating them independently (dotted orange line) yields stronger guarantees. For
_σ ∈{0.3, 0.4, 0.5}, the localized smoothing certificate outperforms the baseline for all ϵ._


-----

1.0

0.8

0.6

0.4

0.2

0.0


1.0

0.2

0.0

|Col1|LP|Col3|Col4|
|---|---|---|---|
|||||
||loc Na¨ı|alized ve||
||loc|alized||
||Cen sm|ter oothing||
|||||


0.0 0.5 1.0 1.5 2.0

|LP|Col2|
|---|---|
|||
|loc Na|alized ¨ıve|
|loc|alized|
|Ce sm|nter oothing|
|||


LP
localized
Na¨ıve

localized
Center
smoothing


0.0 0.5 1.0 1.5 2.0

Adversarial budget ϵ


Adversarial budget ϵ

(b) σ = 0.2, σmin = 0.12, σmax = 0.4


(a) σ = 0.1, σmin = 0.08, σmax = 0.15

1.0

0.2

0.0


1.0

0.2

0.0

|Col1|LP|Col3|Col4|
|---|---|---|---|
|||||
||loc Na¨ı|alized ve||
||loc|alized||
||Cen sm|ter oothing||
|||||


0.0 0.5 1.0 1.5 2.0

|LP|Col2|
|---|---|
|||
|loc Na|alized ¨ıve|
|loc|alized|
|Ce sm|nter oothing|
|||


LP
localized
Na¨ıve

localized
Center
smoothing


0.0 0.5 1.0 1.5 2.0

Adversarial budget ϵ


Adversarial budget ϵ

(d) σ = 0.4, σmin = 0.25, σmax = 1.5


(c) σ = 0.3, σmin = 0.2, σmax = 0.6

1.0

0.8

0.6

0.4

Certified ratio 0.2

0.0


|Col1|Col2|Col3|
|---|---|---|
|||LP|
|||locali Na¨ıv|
|||locali|
|||Cente smoo|
||||


0.0 0.5 1.0 1.5 2.0

Adversarial budget ϵ


(e) σ = 0.5, σmin = 0.25, σmax = 1.5

Figure 7: Certified ratios of U-Net models under varying adversarial budgets ϵ. We compare the
center smoothing baseline (with standard deviation σ) to localized smoothing (with parameters
_σmin, σmax such that the locally smoothed model has a higher or equal accuracy). For ϵ = 0,_
center smoothing has higher certified ratios, i.e. it abstains at a lower rate. For σ = 0.2, the center
smoothing certified accuracy curve has a higher AUC than both the na¨ıve combination of localized
smoothing certificates (dotted line) and the proposed collective certificate (solid line). But for other
_σ localized smoothing outperforms center smoothing. The gap widens with increasing σ._


-----

1.0

0.8

0.6

0.4

0.2

0.0


1.0

0.2

0.0

|Col1|Col2|LP|Col4|
|---|---|---|---|
|||LP||
|||localized Na¨ıve||
|||localized||
|||Center smoothing||
|||||


0.0 0.5 1.0 1.5 2.0

|Col1|LP|Col3|
|---|---|---|
||||
||loc Na|alized ¨ıve|
||loc|alized|
||Ce sm|nter oothing|
||||


LP
localized
Na¨ıve

localized
Center
smoothing


0.0 0.5 1.0 1.5 2.0

Adversarial budget ϵ


Adversarial budget ϵ

(b) σ = 0.2, σmin = 0.12, σmax = 0.4


(a) σ = 0.1, σmin = 0.08, σmax = 0.15

1.0

0.2

0.0


1.0

0.2

0.0

|Col1|Col2|LP|Col4|
|---|---|---|---|
|||LP||
|||localized Na¨ıve||
|||localized||
|||Center smoothing||
|||||


0.0 0.5 1.0 1.5 2.0

|Col1|LP|Col3|
|---|---|---|
||||
||loc Na|alized ¨ıve|
||loc|alized|
||Ce sm|nter oothing|
||||


LP
localized
Na¨ıve

localized
Center
smoothing


0.0 0.5 1.0 1.5 2.0

Adversarial budget ϵ


Adversarial budget ϵ

(d) σ = 0.4, σmin = 0.25, σmax = 1.5


(c) σ = 0.3, σmin = 0.2, σmax = 0.6

1.0

0.8

0.6

0.4

0.2

Certified accuracy

0.0


|Col1|Col2|LP|
|---|---|---|
|||locali Na¨ıve|
|||locali|
|||Cente smoot|
||||


0.0 0.5 1.0 1.5 2.0

Adversarial budget ϵ


(e) σ = 0.5, σmin = 0.25, σmax = 1.5

Figure 8: Certified accuracies of U-Net models under varying adversarial budgets ϵ. We compare
the center smoothing baseline (with standard deviation σ) to localized smoothing (with parameters
_σmin, σmax such that the locally smoothed model has a higher or equal accuracy). Combining the_
localized smoothing base certificates using the proposed linear program (solid orange line) instead
of evaluating them independently (dotted orange line) yields stronger guarantees. The gap between
center smoothing and localized smoothing widens with increasing σ.


-----

E.3 NODE CLASSIFICATION

We first discuss the experimental setup and the necessary parameters for our training and certification
procedure before we show additional experimental results in Section E.3.3.

E.3.1 EXPERIMENTAL SETUP AND HYPERPARAMETERS

**Metric In the node classification setting we are generally only concerned with certified accuracy. To**
calculate this accuracy, we only consider nodes that are correctly classified, where the method does
not abstain, and is certifiably robust for the given perturbation.

**Model We test two different models: 2-layer APPNP (Klicpera et al., 2019) and 6-layer GCN (Kipf**
& Welling, 2017). For both models we use a hidden size of 64 and dropout with a probability of 0.5.
Furthermore, for the propagation step of APPNP we use 10 for the number of interactions and 0.15
as value for α.

**Data and preprocessing. We evaluate our approach on the Cora-ML node classification dataset. We**
perform standard preprocessing, i.e., remove self-loops, make the graph undirected and select the
largest connected component. For the localized smoothing approach we perform Metis clustering
(Karypis & Kumar, 1998) to partition the graph into 5 clusters. We create an affinity ranking by
counting the number of edges which are connecting cluster i and j. This ranking is used to select
the noise parameter for smoothing the attributes of cluster j while classifying a node of cluster i.

**Training and data augmentation All models are trained with a learning rate of 0.001 and weight**
decay of 0.001. The models we use for sparse smoothing are trained with the noise distribution that
is also reported for certification. The localized smoothing models are trained on the their minimal
noise level, i.e., not with localized noise but with only θmin[+] [and][ θ]min[−] [.]

**Certification In the node classification setting the noise parameter space is large. Instead of only**
one noise parameter as used in the image segmentation scenario, we can vary flip probabilities for
addition and deletion. Apart from that we need lower and upper values for each of those. Therefore,
we just selected the minimum parameters and focused on optimising the baseline around these noise
levels as described in Section 8.2. The specific noise parameters and baseline search space regions
can be seen in the captions of the respective figures. The collective linear program is solved using MOSEK (version 9.2.46) (MOSEK ApS, 2019) through the CVXPY interface (version 1.1.13)
(Diamond & Boyd, 2016).

E.3.2 HARDWARE

All experiments on image segmentation were performed using an AMD EPYC 7543 CPU @
2.80GHz, an NVIDA A100 GPU and 32 GB of RAM.

E.3.3 ADDITIONAL EXPERIMENTAL EVALUATION

In Fig. 9, we can see a comparison of our localized smoothing approach to sparse smoothing for a
GCN model with 6-layers. Here, the smoothing distribution is the same as the one used in Fig. 3
of Section 8.2. In Fig. 10, we can see a comparison for an APPNP model but with smaller minimal
noise levels.

We can see that in all experiments for deletion, the localized smoothing approach outperforms the
baselines. In the deletion setting we increase the certified accuracy curve’s AUC from an average
12.28 to 18.93 (i.e. by 54%). In the addition scenario the AUC, averaged over all experiments,
decreases from 6.85 to 6.08 (i.e. by 11.25%). This is mainly due to the results of the experiment
that can be seen in Fig. 10. In the baseline evaluation process both noise parameters for sparse
smoothing are significantly larger than our min-values. This shows that our values may be too small
in this scenario.

Fig. 11 compares the variance certificate to the sparse smoothing certificate of Bojchevski et al.
(2020). That is, we use only one cluster and the same noise levels and models for our approach and
the baseline, i.e., θ[+] = 0.01 and θ[−] = 0.6. We observe that for both models the variance certificate
yields better results for deletion. However, in the addition setting, it is outperformed by the baseline.
If we look at Theorem 2, we can see that we multiply the adversarial budget for addition with γ[+]


-----

1.0

0.8

0.6

0.4

0.2

0.0


1.0

0.8

0.6

0.4

0.2

0.0

|Col1|Col2|Col3|Col4|ed lized .|
|---|---|---|---|---|
||L N N||P localiz a¨ıve loca a¨ıve i.i.d|ed lized .|
||||||
||||||

|Col1|LP locali Na¨ıve loc Na¨ıve i.i.|Col3|Col4|
|---|---|---|---|
|||zed alized d.||
|||||
|||||


LP localized
Na¨ıve localized

Na¨ıve i.i.d.


10 20 30 40 50

Number of adversarial additions


10 20 30 40 50

Number of adversarial deletions


Figure 9: Certified accuracy for varying number of attribute additions (left) and deletions (right)
for the GCN model. We compare localized smoothing (θmin[+] [= 0][.][075][,][ θ]min[−] [= 0][.][6][,][ θ]max[+] [= 0][.][15][,]
_θmax[−]_ [= 0][.][95][) with sparse smoothing with (][θ][+][ = 0][.][085][,][ θ][−] [= 0][.][609][) for addition and deletion.]
This configuration yields the largest certified accuracy curve AUC of 10.41 for addition and 14.78
for deletion compared to all combinations θ[+] _∈{0.007, 0.0085, 0.01} and θ[−]_ _∈_ [0.1, . . ., 0.827].
In the deletion scenario we outperform the baseline with an AUC of 21.76 (15.76 non-collective).
For addition the sparse smoothing performs slightly better than our certificate with AUC of 9.19
(6.39 non-collective). However, the our approach performs significantly better in small perturbation
regions and also in clean accuracy (0 perturbation).

and the one for deletion with γ[−]. As γ[+] has θ[+] in the denominator the small noise level for addition
results in larger values for γ[+]. For these parameters we have γ[+] = 2.795 and γ[−] = 0.491 which
shows why the certificate is less robust to adversarial additions. This is consistent with the other
experiments where our results are on par or worse than the baseline for additions but we outperform
it for deletions.

The Monte Carlo sampling dominated the runtime. Averaged over all experiments, it took 1034 s
(both for the baseline and the proposed certificate, as we used the same number of samples for
both). Averaged over all experiments, the per-prediction certificates took 0.11 s for the baseline
and 0.66 s for the proposed certificate (note that the baseline uses the sparsity-aware smoothing certificate of Bojchevski et al. (2020), while the proposed collective certificate uses our novel variance
smoothing certificate (see Theorem 2), which requires estimating both the mean and variance of softmax scores). Averaged over all tested adversarial budgets, solving each collective linear program
only took 10.9 s, i.e. less than the Monte Carlo sampling that is necessary for both the baseline and
our method. The reason that the linear programs for graphs required more time than those for image,
even though they involve fewer constraints and variables, is that a different, not as well-vectorized
implementation was used.


-----

1.0

0.8

0.6

0.4

0.2

0.0


1.0

0.8

0.6

0.4

0.2

0.0

|Col1|Col2|Col3|ed lized .|
|---|---|---|---|
||L N N|P localiz a¨ıve loca a¨ıve i.i.d||

|LP local Na¨ıve lo Na¨ıve i.i|ized calized .d.|
|---|---|


LP localized
Na¨ıve localized

Na¨ıve i.i.d.


10 20 30 40 50

Number of adversarial additions


10 20 30 40 50

Number of adversarial deletions


Figure 10: Certified accuracy for varying number of attribute additions (left) and deletions (right) for
the APPNP model. We compare localized smoothing (θmin[+] [= 0][.][0075][,][ θ]min[−] [= 0][.][65][,][ θ]max[+] [= 0][.][08][,]
_θmax[−]_ [= 0][.][95][) with sparse smoothing with (][θ][+][ = 0][.][0085][,][ θ][−] [= 0][.][827][) for addition and (][θ][+][ =]
0.007, θ[−] = 0.755) for deletion. These configurations yield the largest certified accuracy curve AUC
of 5.62 for addition and 14.29 for deletion compared to all combinations θ[+] _∈{0.007, 0.0085, 0.01}_
and θ[−] _∈_ [0.1, . . ., 0.827]. In the deletion scenario we outperform the baseline with an AUC of
18.76 (14.86 non-collective). However, the sparse smoothing performs better for addition than our
certificate which only yields a AUC of 3.39 (2.54 non-collective). We observe that the optimal
parameters for addition are both significantly larger than our minimal ones.


1.0

0.8

0.6

0.4

0.2

0.0


1.0

0.8

0.6

0.4

0.2

0.0

|Col1|Col2|Var. cert Sparse,|., del. del.|
|---|---|---|---|
|||Var. cert Sparse,|., add. add.|

|Col1|Col2|
|---|---|
|Var. cert. Sparse, d Var. cert. Sparse, a|, del. el., add. dd.|


Var. cert., del.
Sparse, del.
Var. cert., add.
Sparse, add.


Var. cert., del.
Sparse, del.
Var. cert., add.
Sparse, add.


10 20 30 40

Number of adversarial perturbations


10 20 30 40

Number of adversarial perturbations


Figure 11: Comparison of the variance certificate with sparse smoothing. In this approach we only
used one cluster, i.e., use the same noise levels. Left we can see the results for an APPNP model and
on the right for a 6-layer GCN. The models are trained with the same noise parameters θ[+] = 0.01
and θ[−] = 0.6. We observe that in for both models the variance certificate yields better results for
deletion. However, in the addition setting, it is outperformed by the baseline.


-----

