## LEARNING CURVES FOR CONTINUAL LEARNING IN
### NEURAL NETWORKS: SELF-KNOWLEDGE TRANSFER AND FORGETTING

**Ryo Karakida & Shotaro Akaho**
National Institute of Advanced Industrial Science and Technology (AIST), Japan
{karakida.ryo,s.akaho}@aist.go.jp

ABSTRACT

Sequential training from task to task is becoming one of the major objects in deep
learning applications such as continual learning and transfer learning. Nevertheless,
it remains unclear under what conditions the trained model’s performance improves
or deteriorates. To deepen our understanding of sequential training, this study
provides a theoretical analysis of generalization performance in a solvable case of
continual learning. We consider neural networks in the neural tangent kernel (NTK)
regime that continually learn target functions from task to task, and investigate
the generalization by using an established statistical mechanical analysis of kernel
ridge-less regression. We first show characteristic transitions from positive to
negative transfer. More similar targets above a specific critical value can achieve
positive knowledge transfer for the subsequent task while catastrophic forgetting
occurs even with very similar targets. Next, we investigate a variant of continual
learning which supposes the same target function in multiple tasks. Even for the
same target, the trained model shows some transfer and forgetting depending on the
sample size of each task. We can guarantee that the generalization error monotonically decreases from task to task for equal sample sizes while unbalanced sample
sizes deteriorate the generalization. We respectively refer to these improvement and
deterioration as self-knowledge transfer and forgetting, and empirically confirm
them in realistic training of deep neural networks as well.

1 INTRODUCTION

As deep learning develops for a single task, it enables us to work on more complicated learning frameworks where the model is sequentially trained on multiple tasks, e.g., transfer learning, curriculum
learning, and continual learning. Continual learning deals with the situation in which the learning
machine cannot access previous data due to memory constraints or privacy reasons. It has attracted
much attention due to the demand on applications, and fundamental understanding and algorithms
are being explored (Hadsell et al., 2020). One well-known phenomenon is catastrophic forgetting;
when a network is trained between different tasks, naive training cannot maintain performance on the
previous task (McCloskey & Cohen, 1989; Kirkpatrick et al., 2017).

It remains unclear in most cases under what conditions a trained model’s performance improves
or deteriorates in sequential training. Understanding its generalization performance is still limited
(Pentina & Lampert, 2014; Bennani et al., 2020; Lee et al., 2021). For single-task training, however,
many empirical and theoretical studies have succeeded in characterizing generalization performance
in over-parameterized neural networks and given quantitative evaluation on sample-size dependencies,
e.g., double descent (Nakkiran et al., 2020). For further development, it will be helpful to extend the
analyses on single-task training to sequential training on multiple tasks and give theoretical backing.

To deepen our understanding of sequential training, this study shows a theoretical analysis of its
generalization error in the neural tangent kernel (NTK) regime. In more details, we consider the
NTK formulation of continual learning proposed by Bennani et al. (2020); Doan et al. (2021). By
extending a statistical mechanical analysis of kernel ridge-less regression, we investigate learning
curves, i.e., the dependence of generalization error on sample size or number of tasks. The analysis


-----

focuses on the continual learning with explicit task boundaries. The model learns data generated
by similar teacher functions, which we call target functions, from one task to another. All input
samples are generated from the same distribution in an i.i.d. manner, and each task has output samples
(labels) generated by its own target function. Our main contributions are summarized as follows.
First, we revealed characteristic transitions from negative to positive transfer depending on the target
similarity. More similar targets above a specific critical value can achieve positive knowledge transfer
(i.e., better prediction on the subsequent task than training without the first task). Compared to
this, backward transfer (i.e., prediction on the previous task) has a large critical value, and subtle
dissimilarity between targets causes negative transfer. The error can rapidly increase, which clarifies
that catastrophic forgetting is literally catastrophic (Section 4.1).

Second, we considered a variant of continual learning, that is, learning of the same target function
in multiple tasks. Even for the same target function, the trained model’s performance improves
or deteriorates in a non-monotonic way. This depends on the sample size of each task; for equal
sample sizes, we can guarantee that the generalization error monotonically decreases from task to task
(Section 4.2 for two tasks & Section 5 for more tasks). Unbalanced sample sizes, however, deteriorates
generalization (Section 4.3). We refer to these improvement and deterioration of generalization as
self-knowledge transfer and forgetting, respectively. Finally, we empirically confirmed that selfknowledge transfer and forgetting actually appear in the realistic training of multi-layer perceptron
(MLP) and ResNet-18 (Section 5.1). Thus, this study sheds light on fundamental understanding and
the universal behavior of sequential training in over-parameterized learning machines.

2 RELATED WORK

**Method of analysis. As an analysis tool, we use the replica method originally developed for statistical**
mechanics. Statistical mechanical analysis enables us typical-case evaluation, that is, the average
evaluation over data samples or parameter configurations. It sometimes provides us with novel insight
into what the worst-case evaluation has not captured (Abbaras et al., 2020; Spigler et al., 2020).
The replica method for kernel methods has been developed in Dietrich et al. (1999), and recently
in Bordelon et al. (2020); Canatar et al. (2021). These recent works showed excellent agreement
between theory and experiments on NTK regression, which enables us to quantitatively understand
sample-size dependencies including implicit spectral bias, double descent, and multiple descent.

**Continual learning. Continual learning dynamics in the NTK regime was first formulated by Bennani**
et al. (2020); Doan et al. (2021), though the evaluation of generalization remains unclear. They derived
an upper bound of generalization via the Rademacher complexity, but it includes naive summation
over single tasks and seems conservative. In contrast, our typical-case evaluation enables us to newly
find such rich behaviors as negative/positive transfer and self-knowledge transfer/forgetting. The
continual learning in the NTK regime belongs to so-called single-head setting (Farquhar & Gal, 2018),
and it allows the model to revisit the previous classes (target functions) in subsequent tasks. This is
complementary to earlier studies on incremental learning of new classes and its catastrophic forgetting
(Ramasesh et al., 2020; Lee et al., 2021), where each task includes different classes and does not allow
the revisit. Note that the basic concept of continual learning is not limited to incremental learning
but allows the revisit (McCloskey & Cohen, 1989; Kirkpatrick et al., 2017). Under the limited data
acquisition or resources of memory, we often need to train the same model with the same targets (but
different samples) from task to task. Therefore, the setting allowing the revisit seems reasonable.

3 PRELIMINARIES

3.1 NEURAL TANGENT KERNEL REGIME

We summarize conventional settings of the NTK regime (Jacot et al., 2018; Lee et al., 2019). Let us
consider a fully connected neural network f = uL given by


_Ml_ 1 + σbbl, hl = φ(ul) (l = 1, ..., L), (1)
_−_


_ul = σwWlhl_ 1/
_−_


where we define weight matrices Wl R[M][l][×][M][l][−][1], bias terms bl R[M][l], and their variances σw[2]
and σb[2][. We set random Gaussian initialization] ∈ _[ W][l,ij][, b][l,i][ ∼N]_ [(0][,][ 1)] ∈[ and focus on the mean squared]
error loss: L(θ) = _µ=1_ _[∥][y][µ][ −]_ _[f]_ [(][x][µ][)][∥][2][, where the training samples][ {][x][µ][, y][µ][}]µ[N]=1 [are composed of]

[P][N]


-----

inputs x[µ] _∈_ R[D] normalized by ∥x[µ]∥2 = 1 and labels y[µ] _∈_ R[C] . The set of all parameters is denoted
as θ, and the number of training samples is N . Assume the infinite-width limit for hidden layers
(Ml ), finite sample size and depth. The gradient descent dynamics with a certain learning rate
then converges to a global minimum sufficiently close to the random initialization. This is known as →∞
the NTK regime, and the trained model is explicitly obtained as

_f_ [(][c][)](x[′]) = f0[(][c][)][(][x][′][) + Θ(][x][′][, X][)Θ(][X][)][−][1][(][y][(][c][)][ −] _[f][ (]0[c][)][(][X][))]_ (c = 1, ..., C). (2)
We denote the NTK matrix as Θ, arbitrary input samples (including test samples) as x[′], and the set
of training samples as X. The indices of f mean 0 for the model at initialization and c for the head
of the network. Entries of NTK Θ(x[′], x) are defined by _θf0(x[′])_ _θf0(x)[⊤]. We write Θ(X) as an_
_∇_ _∇_
abbreviation for Θ(X, X). The trained network is equivalent to a linearized model around random
initialization (Lee et al., 2019), that is, f [(][c][)] = f0[(][c][)] + ∇θf0[(][c][)][∆][θ][ with]


_∇θf0[(][c][)][(][X][)][⊤][Θ(][X][)][−][1][(][y][(][c][)][ −]_ _[f][ (]0[c][)][(][X][))][.]_ (3)
_c=1_

X


∆θ = θ − _θ0 =_


While over-parameterized models have many global minima, the NTK dynamics implicitly select
the above θ, which corresponds to the L2 min-norm solution. Usually, we ignore f0 by taking the
average over random initialization. The trained model (2) is then equivalent to the kernel ridge-less
regression (KRR).

NTK regime also holds in various architectures including ResNets and CNNs (Yang & Littwin,
2021), and the difference only appears in the NTK matrix. Although we focus on the fully connected
network in synthetic experiments, the following NTK formulation of sequential training and our
theoretical results hold in any architecture under the NTK regime.

**NTK formulation of Continual learning. We denote the set of training samples in the n-th task as**
(Xn, yn) (n = 1, 2, ..., K), a model trained in a sequential manner from task 1 to task n as fn and its
parameters as θn. That is, we train the network initialized at θn 1 for the n-th task and obtain fn.
_−_
Assume that the number of tasks K is finite. The trained model within the NTK regime is then given
as follows (Bennani et al., 2020; Doan et al., 2021):
_fn(x[′]) = fn−1(x[′]) + Θ(x[′], Xn)Θ(Xn)[−][1](yn −_ _fn−1(Xn)),_ (4)

_θn_ _θn_ 1 = _θf0(Xn)[⊤]Θ(Xn)[−][1](yn_ _fn_ 1(Xn)). (5)
_−_ _−_ _∇_ _−_ _−_
We omit the index c because each head is updated independently. The model fn completely fits
the n-th task, i.e., yn = fn(Xn). The main purpose of this study is to analyze the generalization
performance of the sequentially trained model (4). At each task, the model has an inductive bias of
KRR in the function space and L2 min-norm solution in the parameter space. The next task uses
this inductive bias as the initialization of training. The problem is whether this inductive bias helps
improve the generalization in the subsequent tasks.

**Remark (independence between different heads).** For a more accurate understanding of the
continual learning in the NTK regime, it may be helpful to remark on the heads’ independence, which
previous studies did not explicitly mention. As in Eq. (2), all heads share the same NTK, and f [(][c][)]

depends only on the label of its class y[(][c][)]. While the parameter update (3) includes information of all
classes, the c-th head can access only the information of the c-th class[1]. For example, suppose that
the n-th task includes all classes except 1, i.e., {2, ..., C}. Then, the model update on the the class 1
at the n-th task, i.e., fn[1] _n_ 1[, becomes]

_[−]_ _[f][ 1]−_

_θf0[(1)][(][x][′][)(][θ][n][ −]_ _[θ][n][−][1][) =][ ∇][θ][f][ (1)]0_ [(][x][′][)] _θf0[(][c][)][(][X][n][)][⊤][Θ(][X][n][)][−][1][(][y]n[(][c][)]_ _n_ 1[) = 0]
_∇_ _c=2_ _∇_ _[−]_ _[f][ (][c]−[)]_
X

because _θf0[(][c][)]_ 0 = 0 (c = c[′]) in the infinite-width limit (Jacot et al., 2018; Yang, 2019).
Thus, we can deal with each head independently and analyze the generalization by setting ∇ _[∇][θ][f][ (][c][′][)][⊤]_ _̸_ _C = 1_
without loss of generality. This indicates that in the NTK regime, interaction between different
heads do not cause knowledge transfer and forgetting. One may wonder if there are any non-trivial
knowledge transfer and forgetting in such a regime. Contrary to such intuition, we reveal that when
the subsequent task revisits previous classes (targets), the generalization shows interesting increase
and decrease.

1We use the term “class”, although the regression problem is assumed in NTK theory. Usually, NTK studies
solve the classification problem by regression with a target y[(][c][)] = {0, 1}.


-----

3.2 LEARNING CURVE ON SINGLE-TASK TRAINING

To evaluate the generalization performance of (4), we extend the following theory to our sequential
training. Bordelon et al. (2020) obtained an analytical expression of the generalization for NTK
regression on a single task (2) as follows. Assume that training samples are generated in an i.i.d.
manner ( x[µ] _∼_ _p(x)) and that labels are generated from a square integrable target function_ _f[¯]:_


_∞_

_w¯iψi(x),_ _y[µ]_ = f[¯](x[µ]) + ε[µ] (µ = 1, ..., N ), (6)
_i=0_

X


_f¯(x) =_


where ¯wi are constant coefficients and ε represents Gaussian noise with ⟨ε[µ]ε[ν]⟩ = δµνσ[2]. We define
_ψi(x) :=_ _ηiφi(x) with basis functions φi(x) given by Mercer’s decomposition:_

_[√]_
_dx[′]p (x[′]) Θ (x, x[′]) φi (x[′]) = ηiφi(x)_ (i = 0, 1, . . ., ∞). (7)
Z

Here, ηi denotes NTK’s eigenvalue and we assume the finite trance of NTK _i_ _[η][i][ <][ ∞][. We]_

set η0 = 0 in the main text to avoid complicated notations. We can numerically compute
eigenvalues by using the Gauss-Gegenbauer quadrature. Generalization error is expressed by
2[E] [P]
_E1 :=_ _dxp(x)_ ¯f (x) − _f_ _[∗](x)_ [where][ f][ ∗] [is a trained model and][ ⟨· · · ⟩][D][ is the average]

_D_
over training samples. Bordelon et al. (2020) derived a typical-case evaluation of the generalization

DR   

error by using the replica method: for a sufficiently large N, we have asymptotically

1 _∞_ _κ_ 2 _γ_
_E1 =_ 1 _γ_ _ηi ¯wi[2]_ _κ + Nηi_ + 1 _γ [σ][2][.]_ (8)

_−_ _i=0_   _−_

X

Although the replica method takes a large sample size N, Bordelon et al. (2020); Canatar et al. (2021)
reported that the analytical expression (8) coincides well with empirical results even for small N .
The constants κ and γ are defined as follows and characterize the increase and decrease of E1:


_Nηi[2]_ (9)

(κ + Nηi)[2][ .]


_ηi_

_, γ =_
_κ + Nηi_


1 =


_i_ _i_

_i=0_ _i=0_

The κ is a positive solution of the first equation and obtained by numerical computation. The γ
satisfies 0 < γ < 1 by definition. For N = αD[l] (α > 0, l ∈ N, D ≫ 1), we can analytically solve it
and obtain more detailed evaluation. For example, a positive κ decreases to zero as the sample size
increases and E1 also decreases for σ[2] = 0. For σ[2] _> 0, the generalization error shows multiple_
descent depending on the decay of eigenvalue spectra. Since multiple descent is not a main topic of
this paper, we briefly summarize it in Section A.5 of the Supplementary Materials.

4 LEARNING CURVES BETWEEN TWO TASKS

In this section, we analyze the NTK formulation of continual learning (4) between two tasks (K = 2).
One can also regard this setting as transfer learning. We sequentially train the model from task A to
task B, and each one has a target function defined by


_w¯A,iψi(x),_ _f[¯]B(x) =_


). (10)


_f¯A(x) =_


_w¯B,iψi(x), [ ¯wA,i, ¯wB,i]_ (0, ηi
_∼N_


The target functions are dependent on each other and belong to the reproducing kernel Hilbert
space (RKHS). By denoting the RKHS by H, one can interpret the target similarity ρ as the inner
product ⟨f[¯]A, _f[¯]B⟩H/(∥f[¯]A∥H∥f[¯]B∥H) = ρ. These targets have dual representation such as_ _f[¯](x) =_
_i_ _[α][i][Θ(][x]i[′]_ _[, x][)][ with i.i.d. Gaussian variables][ α][i][ (][Bordelon et al.][,][ 2020][), as summarized in Section]_

E.1. We generate training samples by yA[µ] [= ¯]fA(x[µ]A[) +][ ε][µ]A [(][µ][ = 1][, ..., N][A][) and][ y]B[µ] [= ¯]fB(x[µ]B[) +]

P

_ε[µ]B_ [(][µ][ = 1][, ..., N][B][), although we focus on the noise-less case (][σ][ = 0][) in this section. Input samples]
_x[µ]A_ [and][ x]B[µ] [are i.i.d. and generated by the same distribution][ p][(][x][)][. We can measure the generalization]
error in two ways: generalization error on subsequent task B and that on previous task A:

_EA→B(ρ) =_ _dxp(x)( f[¯]B(x) −_ _fA→B(x))[2]_ _,_ (11)
Z 

_EA[back]B[(][ρ][) =]_ _dxp(x)( f[¯]A(x)_ _fA_ _B(x))[2]_ _,_ (12)
_→_ _−_ _→_
Z 


-----

where we write f2 as fA _B to emphasize the sequential training from A to B. The notation EA[back]B[(][ρ][)]_
_→_ _→_
is referred to as backward transfer (Lopez-Paz & Ranzato, 2017). Large negative backward transfer
is known as catastrophic forgetting. We take the average ⟨· · · ⟩ over training samples of two tasks
_A,_ _B_, and target coefficients ¯w. In fact, we can set ¯w as constants, and it is unnecessary to take
_{D_ _D_ _}_
the average. To avoid complicated notation, we take the average in the main text. We then obtain the
following result.
**Theorem 1. Using the replica method under sufficiently large NA and NB, for σ = 0, we have**


_qA,i[2]_
2(1 _ρ) (1_ _qA,i) +_
_−_ _−_ 1 _γA_

_−_


_EB,i,_ (13)


_EA→B(ρ) =_

_EA[back]B[(][ρ][) =]_
_→_


2(1 _ρ)(1 + FγB_ (qA,i, qB,i))ηi[2] [+] _qA,i[2]_ _EB,i_
_−_ 1 _γA_

_−_


(14)


_where we define qA,i = κA/(κA + NAηi), qB,i = κB/(κB + NBηi), EB,i = qB,i[2]_ _[η]i[2][/][(1][ −]_ _[γ][B][)][ and]_
_FγB_ (a, b) = b(a 2) + b[2](1 _a)/(1_ _γB)._
_−_ _−_ _−_

The constants κA and γA (κB and γB, respectively) are given by setting N = NA(NB) in (9). The
detailed derivation is given in Section A. Technically speaking, we use the following lemma:
**Lemma 2. Denote the trained model (2) on single task A as fA =** _i_ _[w]A,i[∗]_ _[ψ][i][, and define the]_
_following cost function: E =_ _i_ _[φ][i][(][w]A,i[∗]_ _[−]_ _[u][i][)][2]_ _DA_ _[for arbitrary constants][ φ][i][ and][ u][i][. Using the]_

_replica method under a sufficiently large NA, we have_ [P]

P


( ¯wA,i _ui)[2]_ 2 ¯wA,i( ¯wA,i _ui)qA,i +_ _w¯A,i[2]_ [+][ η][i][N][A] (EA + σ[2]) _qA,i[2]_
_−_ _−_ _−_ _κ[2]A_
 


_E =_


_φi,_


_where EA denotes generalization error E1 on single task A._

For example, we can see that EA is a special case of E with φi = ηi and u = ¯wA, and that EA _B(ρ)_
_→_
is reduced to φi = qB,i[2] _[η]i[2][/][(1][ −]_ _[γ][B][)][ and][ u][ = ¯]wB after certain calculation._

**Spectral Bias. The generalization errors obtained in Theorem 1 are given by the summation over**
spectral modes like the single-task case. The EB,i corresponds to the i-th mode of generalization
error (8) on single task B. The study on the single task (Bordelon et al., 2020) revealed that as the
sample size increases, the modes of large eigenvalues decreases first. This is known as spectral
_bias and clarifies the inductive bias of the NTK regression. Put the eigenvalues in descending order,_
i.e.,mode is asymptotically given by λi ≥ _λi+1. When D is sufficiently large and EB,i = 0 (i < l) and NB η =i[2]_ [(] αD[i > l][l][)]([.]α >[ We see that the spectral bias also] 0, l ∈ N), the error of each
holds in EA→B because it is a weighted sum over EB,i. In contrast, EA[back]→B [includes a constant term]
2(1 − _ρ)ηi[2][. This constant term causes catastrophic forgetting, as we show later.]_

4.1 TRANSITION FROM NEGATIVE TO POSITIVE TRANSFER

We now look at more detailed behaviors of generalization errors obtained in Theorem 1. We first
discuss the role of the target similarity ρ for improving generalization. Figure 1(a) shows the
comparison of the generalization between single-task training and sequential training. Solid lines
show theory, and markers show experimental results of trained neural networks in the NTK regime.
We trained the model (1) with ReLU activation, L = 3, and Ml = 4, 000 by using the gradient descent
over 50 trials. More detailed settings of our experiments are summarized in Section E. Because
we set NA = NB = 100, we have EA = EB. The point is that both EA _B(ρ) and EA[back]B[(][ρ][)][ are]_
_→_ _→_
lower than EA(= EB) for large ρ. This means that the sequential training degrades generalization if
the targets are dissimilar, that is, negative transfer. In particular, EA[back]B[(][ρ][)][ rapidly deteriorates for]
_→_
the dissimilarity of targets. Note that both EA→B and EA[back]→B [are linear functions of][ ρ][. Figure 1(a)]
indicates that the latter has a large slope. We can gain quantitative insight into the critical value of ρ
for the negative transfer as follows.

**Knowledge transfer. The following asymptotic equation gives us the critical value for EA** _B:_
_→_

_EA→B(ρ)/EB ∼_ 2(1 − _ρ)_ _for NA ≫_ _NB._ (15)


-----

(a) (b) _ρ_

_ρ_


Figure 1: (a) Transitions from positive to negative transfer caused by target similarity ρ. We set
_NA = NB. (b) Learning curves show negative transfer (EA_ _B/EB) in a highly non-linear way_
_→_
depending on unbalanced sample sizes. We changed NA and set NB = 10[3].

A straightforward algebra leads to this (Section A.3). In the context of transfer learning, it is
reasonable that the target domain has a limited sample size compared to the first task. For ρ > 1/2, we
have EA _B < EB, that is, previous task A contributes to improving the generalization performance_
_→_
on subsequent task B (i.e., positive transfer). For ρ < 1/2, however, negative transfer appears.

The following sufficient condition for the negative transfer is also noteworthy. By evaluating EA _B >_
_→_
_EB, we can prove that for any NA and NB, the negative transfer always appears for_
_ρ < ρ[∗]_ := _γA/(1 +_ _γA)._ (16)

_[√]_ _[√]_
This is just a sufficient condition and may be loose. For example, we asymptotically have the critical
target similarity ρ = 1/2 > ρ[∗] for NA _NB. Nevertheless, this sufficient condition is attractive in_
the sense that it clarifies the unavoidable negative transfer for the small target similarity. ≫

**Backward transfer. The EA[back]B[(0)][ includes the constant term][ P]i** _[η]i[2]_ [independent of sample sizes.]
_→_

Note that qA and qB decrease to zero for large sample sizes (Bordelon et al., 2020), and we have
_EA[back]B[(][ρ][)][ ∼]_ [P]i _[η]i[2][(1][ −]_ _[ρ][)][. In contrast,][ E][A][ converges to zero for a large][ N][A][. Therefore, the]_
_→_

intersection between EA[back]B[(][ρ][)][ and][ E][A][ reach][ ρ][ = 1][ as][ N][A][ and][ N][B][ increase. This means that when]
_→_
we have a sufficient number of training samples, negative backward transfer (EA[back]B[(][ρ][)][ > E][A][) occurs]
_→_
even for very similar targets. Figure 1(a) confirms this result, and Figure 6 in Section A.6 shows more
detailed learning curves of backward transfer. Catastrophic forgetting seems literally “catastrophic”
in the sense that the backward transfer rapidly deteriorates by the subtle target dissimilarity.

4.2 SELF-KNOWLEDGE TRANSFER

We have shown that target similarity is a key factor for knowledge transfer. We reveal that the sample
size is another key factor. To clarify the role of sample sizes, we focus on the same target function
(ρ = 1) in the following analysis. We refer to the knowledge transfer in this case as self-knowledge
_transfer to emphasize the network learning the same function by the same head. As is the same in_
_ρ < 1, the knowledge obtained in the previous task is transferred as the network’s initialization for_
the subsequent training and determines the eventual performance.

**Positive transfer by equal sample sizes. We find that positive transfer is guaranteed under equal**
sample sizes, that is, NA = NB. To characterize the advantage of the sequential training, we compare
it with a model average: (fA + fB)/2, where fA (fB) means the model obtained by a single-task
training on A (B). Note that since the model is a linear function of the parameter in the NTK regime,
this average is equivalent to that of the trained parameters: (θA + θB)/2. After straightforward
calculation in Section D, the generalization error of the model average is given by
_Eave = (1 −_ _γB/2) EB._ (17)
Sequential training and model average include information of both tasks A and B; thus, it is interesting
to compare it with EA→B. We find
**Proposition 3. For ρ = 1 and any NA = NB,**
_EA_ _B(1) < Eave < EA = EB._ (18)
_→_


-----

The derivation is given in Section D. This proposition clarifies the superiority of sequential training
over single-task training and even the average. The first task contributes to the improvement on the
second task; thus, we have positive transfer.

**Negative transfer by unbalanced sample sizes. While equal sample size leads to positive transfer,**
the following unbalanced sample sizes cause the negative transfer of self-knowledge:
_EA→B(1)/EB ∼_ 1/(1 − _γA)_ _for NB ≫_ _NA._ (19)
The derivation is based on Jensen’s inequality (Section A.3). While EA _B and EB asymptotically_
_→_
decrease to zero for the large NB, their ratio remains finite. Because 0 < γA < 1, EA _B(1) > EB._
_→_
It indicates that the small sample size of task A leads to a bad initialization of subsequent training
and makes the training on task B hard to find a better solution.

Figure 1(b) summarizes the learning curves which depend on sample sizes in a highly non-linear
way. Solid lines show theory, and markers show the results of NTK regression over 100 trials. The
figure shows excellent agreement between theory and experiments. Although we have complicated
transitions from positive to negative transfer, our theoretical analyses capture the basic characteristics
of the learning curves. For self-knowledge transfer (ρ = 1), we can achieve positive transfer at
_NA/NB = 1, as shown in Proposition 3, and for large NA/NB, as shown in (15). In contrast,_
negative transfer appears for small NA/NB, as shown in (19). For ρ < 1, large NA/NB produces
positive transfer for ρ < 1/2, as shown in (15).

If we set a relatively large σ > 0, the learning curve may become much more complicated due
to multiple descent. Figure 5 in Section A.5 confirms the case in which multiple descent appears
in EA→B. The curve shape is generally characterized by the interaction among target similarity,
self-knowledge transfer depending on sample size, and multiple descent caused by the noise.

4.3 SELF-KNOWLEDGE FORGETTING

We have shown in Section 4.1 that backward transfer is likely
to cause catastrophic forgetting for ρ < 1. We show that even
for ρ = 1, sequential training causes forgetting. That is, the
training on task B degrades generalization even though both
tasks A and B learn the same target.

We have EA[back]B[(1) =][ E][A][→][B][(1)][ by definition, and Proposi-]
_→_
tion 3 tells us that EA _B < EA. Therefore, no forgetting_
_→_
appears for equal sample sizes. In contrast, we have
_EA→B(1)/EA ∼_ 1/(1 − _γB)_ _for NA ≫_ _NB._ (20)
One can obtain this asymptotic equation in the same manner
as (19) since EA→B(1) is a symmetric function in terms of
indices A and B. Combining (20) with (15), we have EA <
_EA→B(1) ≪_ _EB. Sequential training is better than using_ Figure 2: Self-knowledge forgetonly task B, but training only on the first task is the best. One ting: unbalanced sample sizes decan say that the model forgets the target despite learning the grade generalization.
same one. We call this self-knowledge forgetting. In the
context of continual learning, many studies have investigated the catastrophic forgetting caused
by different heads (i.e., in the situation of incremental learning). Our results suggest that even the
sequential training on the same task and the same head shows such forgetting. Intuitively speaking,
the self-knowledge forgetting is caused by the limited sample size of task B. Note that we have
_EA→B(1) =_ _i_ _[q]A,i[2]_ _[E][B,i][/][(1][ −]_ _[γ][A][)][. The generalization error of single-task training on task B]_

(EB,i) takes a large value for a small NB and this causes the deterioration of EA _B as well. Figure_
_→_
2 confirms the self-knowledge forgetting in NTK regression. We set NA = NB as the red line and

[P]

_NB = 100 as the yellow line. The dashed line shows the point where forgetting appears._

5 LEARNING CURVES OF MANY TASKS


We can generalize the sequential training between two tasks to that of more tasks. For simplicity, let us
focus on the self-knowledge transfer (ρ = 1) and equal sample sizes. Applying Lemma 2 recursively
from task to task, we can evaluate generalization defined by En = _dxp(x)( f[¯](x) −_ _fn(x))[2]_ _D[.]_
R


-----

**Theorem 4.and target y Assume that (i)n =** _i_ _w[¯]iψ(X (n) +Xn ε, ynn. (ii) sample sizes are equal:) (n = 1, ..., K) are given by the same distribution Nn = N_ _. For n = 1, ..., K Xn ∼,_ _P_ (X)

1 _N_
_En+1 =_ _q[⊤]_ _q˜ + Rn+1σ[2],_ := diag _q[2][]_ + _qq˜[⊤],_ (21)

[P] (1 _γ)[2][ ˜]_ _Q[n][−][1]_ _Q_ (1 _γ)κ[2][ ˜]_

_−_ _−_

_where qi = κ/(κ + ηiN_ ), ˜qi = ηiqi[2] _[and][ diag][(][q][2][)][ denotes a diagonal matrix whose entries are] _ _[ q]i[2][.]_
_The noise term Rn is a positive constant. In the noise-less case (σ = 0), the learning curve shows_
_monotonic decrease: En+1_ _En. If all eigenvalues are positive, we have_
_≤_
_En+1 < En_ (n = 1, 2, ...). (22)

See Section B for details of derivation. The learning curve (i.e., generalization error to the number
of tasks) monotonically decreases for the noise-less case. the monotonic decrease comes from
_λmax(_ ) < 1. This result means that the self-knowledge is transferred and accumulated from task to
_Q_
task and contributes in improving generalization. It also ensures that no self-knowledge forgetting
appears. We can also show that Rn converges to a positive constant term for a large n and the
contribution of noise remains as a constant.

**KRR-like expression. The main purpose of this work was to address the generalization of the**
continually trained model fn. As a side remark, we show another expression of fn:

Θ (X1) O O _−1_

_· · ·_ . _y1_

.

[Θ (x[′], X1) · · · Θ (x[′], Xn)]  Θ (X2... _, X1)_ Θ (X2) ... O.   ...yn  _._ (23)

 Θ (Xn, X1) Θ (Xn)   
 _· · ·_   
 

This easily comes from comparing the update (4) with a formula for the inversion of triangular block
matrices (Section C). One can see this expression as an approximation of KRR, where the upper
triangular block of NTK is set to zero. Usual modification of KRR is diagonal, e.g., L2 regularization
and block approximation, and it seems that the generalization error of this type of model has never
been explored. Our result revealed that this model provides non-trivial sample size dependencies
such as self-knowledge transfer and forgetting.

5.1 EXPERIMENTS

Although Theorem 4 guarantees the monotonic learning curve for equal sizes, unbalanced sample
sizes should cause a non-monotonic learning curve. We empirically confirmed this below.

**Synthetic data.** First, we empirically confirm En on synthetic data (10). Figure 3(a1) confirms
the claim of Theorem 4 that the generalization error monotonically decreases for σ = 0 as the
task number increases. Dashed lines are theoretical values calculated using the theorem, and points
with error bars were numerically obtained by fn (4) over 100 trials. For σ > 0, the decrease was
suppressed. We set σ[2] = {0, 10[−][5], 10[−][4], 10[−][3]} and Ni = 100. Figure 3(a2) shows self-knowledge
forgetting. When the first task has a large sample size, the generalization error by the second task can
increase for small subsequent sample sizes Ni. For smaller Ni, there was a tendency for the error to
keep increasing and taking higher errors than that of the first task during several tasks. In practice,
one may face a situation where the model is initialized by the first task training on many samples and
then trained in a continual learning manner under a memory constraint. The figure suggests that if
the number of subsequent tasks is limited, we need only the training on the first task. If we have a
sufficiently large number of tasks, generalization eventually improves.

**MLP on MNIST / ResNet-18 on CIFAR-10. We mainly focus on the theoretical analysis in the**
NTK regime, but it will be interesting to investigate whether our results also hold in more practical
settings of deep learning. We trained MLP (fully-connected neural networks with 4 hidden layers)
and ResNet-18 with stochastic gradient descent (SGD) and cross-entropy loss. We set the number of
epochs sufficient for the training error to converge to zero for each task. We confirmed that they show
qualitatively similar results as in the NTK regime. We randomly divided the dataset into tasks without
overlap of training samples. Figures 3(b1,c1) show the monotonic decrease for an equal sample
size and that the noise suppressed the decrease. We set Ni = 500 and generated the noise by the
label corruption with a corruption probability {0, 0.2, ..., 0.8} (Zhang et al., 2017). The vertical axis
means the error, i.e., 1 − (Test accuracy [%])/100. Figures 3(b2,c2) show that unbalanced sample
sizes caused the non-monotonic learning curve, similar to NTK regression.


-----

NTK MLP ResNet-18

(a1) (b1) (c1)

(a2) (b2) (c2)

Figure 3: (a1)-(c1) Learning curves for equal sample sizes. For noise-less case, they monotonically
decreased (orange lines). For noisy case, decrease was suppressed (grey lines; we plotted learning
curves for several σ[2] and those with larger test errors correspond to larger σ[2]). We trained MLP
on MNIST and ResNet-18 on CIFAR-10. (a2)-(c2) Learning curves for unbalanced sample sizes
were non-monotonic (N1 = 4, 000 for NTK regression, N1 = 10[4] for SGD training of MLP and
ResNet-18). Numbers in the legend mean Nn (n 2).
_≥_

6 DISCUSSION

We provided novel quantitative insight into knowledge transfer and forgetting in the sequential
training of neural networks. Even in the NTK regime, where the model is simple and linearized,
learning curves show rich and non-monotonic behaviors depending on both target similarity and
sample size. In particular, learning on the same target shows successful self-knowledge transfer
or undesirable forgetting depending on the balance of sample sizes. These results indicate that the
performance of the sequentially trained model is more complicated than we thought, but we can still
find some universal laws behind it.

There are other research directions to be explored. While we focused on reporting novel phenomena
on transfer and forgetting, it is also important to develop algorithms to achieve better performance. To
mitigate catastrophic forgetting, previous studies proposed several strategies such as regularization,
parameter isolation, and experience replay (Mirzadeh et al., 2020). Evaluating such strategies with
theoretical backing would be helpful for further development of continual learning. For example,
orthogonal projection methods modify gradient directions (Doan et al., 2021), and replay methods
allow the reuse of the previous samples. We conjecture that these could be analyzed by extending our
calculations in a relatively straightforward way. It would also be interesting to investigate richer but
complicated situations required in practice, such as streaming of non-i.i.d. data and distribution shift
(Aljundi et al., 2019). The current work and other theories in continual learning or transfer learning
basically assume the same input distribution between different tasks (Lee et al., 2021; Tripuraneni
et al., 2020). Extending these to the case with an input distribution shift will be essential for some
applications including domain incremental learning. Our analysis may also be extended to topics
different from sequential training. For example, self-distillation uses trained model’s outputs for the
subsequent training and plays an interesting role of regularization (Mobahi et al., 2020).

While our study provides universal results, which do not depend on specific eigenvalue spectra or
architectures, it is interesting to investigate individual cases. Studies on NTK eigenvalues have made
remarkable progress, covering shallow and deep ReLU neural networks (Geifman et al., 2020; Chen
& Xu, 2020), skip connections (Belfer et al., 2021), and CNNs (Favero et al., 2021). We expect that
our analysis and findings will serve as a foundation for further understanding and development on
theory and experiments of sequential training.


-----

REPRODUCIBILITY STATEMENT

The main contributions of this work are theoretical claims, and we clearly explained their assumptions
and settings in the main text. Complete proofs of the claims are given in the Supplementary Materials.
For experimental results of training deep neural networks, we used only already-known models and
algorithms implemented in PyTorch. All of the detailed settings, including learning procedures and
hyperparameters, are clearly explained in Section E.

ACKNOWLEDGMENTS

This work was funded by JST ACT-X Grant Number JPMJAX190A and JSPS KAKENHI Grant
Number 19K20366.

REFERENCES

Alia Abbaras, Benjamin Aubin, Florent Krzakala, and Lenka Zdeborová. Rademacher complexity
and spin glasses: A link between the replica and statistical theories of learning. In Mathematical
and Scientific Machine Learning (MSML), pp. 27–54. PMLR, 2020.

Rahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars. Task-free continual learning. In IEEE/CVF

Conference on Computer Vision and Pattern Recognition (CVPR), pp. 11254–11263, 2019.

Yuval Belfer, Amnon Geifman, Meirav Galun, and Ronen Basri. Spectral analysis of the neural
tangent kernel for deep residual networks. arXiv preprint arXiv:2104.03093, 2021.

Mehdi Abbana Bennani, Thang Doan, and Masashi Sugiyama. Generalisation guarantees for continual
learning with orthogonal gradient descent. arXiv preprint arXiv:2006.11942, 2020.

Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves
in kernel regression and wide neural networks. In International Conference on Machine Learning
(ICML), pp. 1024–1034. PMLR, 2020.

Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks. Nature
communications, 12(1):1–12, 2021.

Lin Chen and Sheng Xu. Deep neural tangent kernel and Laplace kernel have the same RKHS. In

International Conference on Learning Representations (ICLR), 2020.

Rainer Dietrich, Manfred Opper, and Haim Sompolinsky. Statistical mechanics of support vector
networks. Physical review letters, 82(14):2975, 1999.

Thang Doan, Mehdi Abbana Bennani, Bogdan Mazoure, Guillaume Rabusseau, and Pierre Alquier.
A theoretical analysis of catastrophic forgetting through the NTK overlap matrix. In International
Conference on Artificial Intelligence and Statistics (AISTATS), pp. 1072–1080. PMLR, 2021.

Sebastian Farquhar and Yarin Gal. Towards robust evaluations of continual learning. arXiv preprint

arXiv:1805.09733, 2018.

Alessandro Favero, Francesco Cagnetta, and Matthieu Wyart. Locality defeats the curse of dimensionality in convolutional teacher-student scenarios. arXiv preprint arXiv:2106.08619, 2021.

Amnon Geifman, Abhay Yadav, Yoni Kasten, Meirav Galun, David Jacobs, and Ronen Basri. On
the similarity between the Laplace and neural tangent kernels. In Advances in neural information
processing systems (NeurIPS), pp. 1451–1461, 2020.

Raia Hadsell, Dushyant Rao, Andrei A Rusu, and Razvan Pascanu. Embracing change: Continual
learning in deep neural networks. Trends in cognitive sciences, 2020.

Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in neural information processing systems (NeurIPS),
pp. 8571–8580, 2018.


-----

James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming
catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114
(13):3521–3526, 2017.

Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Jascha Sohl-Dickstein, and Jeffrey
Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. In
Advances in neural information processing systems (NeurIPS), pp. 8572–8583, 2019.

Sebastian Lee, Sebastian Goldt, and Andrew Saxe. Continual learning in the teacher-student setup:
Impact of task similarity. In International Conference on Machine Learning (ICML), pp. 6109–
6119. PMLR, 2021.

David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In

Advances in Neural Information Processing Systems (NIPS), pp. 6467–6476, 2017.

Michael McCloskey and Neal J Cohen. Catastrophic interference in connectionist networks: The
sequential learning problem. In Psychology of learning and motivation, volume 24, pp. 109–165.
Elsevier, 1989.

Seyed Iman Mirzadeh, Mehrdad Farajtabar, Dilan Gorur, Razvan Pascanu, and Hassan Ghasemzadeh.
Linear mode connectivity in multitask and continual learning. In International Conference on
Learning Representations (ICLR), 2020.

Hossein Mobahi, Mehrdad Farajtabar, and Peter L Bartlett. Self-distillation amplifies regularization in
Hilbert space. In Advances in Neural Information Processing Systems (NeurIPS), pp. 3351–3361,
2020.

Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. In International Conference on Learning
Representations (ICLR), 2020.

Anastasia Pentina and Christoph Lampert. A PAC-Bayesian bound for lifelong learning. In

International Conference on Machine Learning (ICML), pp. 991–999. PMLR, 2014.

Vinay Venkatesh Ramasesh, Ethan Dyer, and Maithra Raghu. Anatomy of catastrophic forgetting: Hidden representations and task semantics. In International Conference on Learning Representations
(ICLR), 2020.

Stefano Spigler, Mario Geiger, and Matthieu Wyart. Asymptotic learning curves of kernel methods:
empirical data versus teacher–student paradigm. Journal of Statistical Mechanics: Theory and
Experiment, 2020(12):124001, 2020.

Nilesh Tripuraneni, Michael Jordan, and Chi Jin. On the theory of transfer learning: The importance of
task diversity. In Advances in Neural Information Processing Systems (NeurIPS), pp. 7852–7862,
2020.

Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019.

Greg Yang and Etai Littwin. Tensor programs IIb: Architectural universality of neural tangent kernel
training dynamics. In International Conference on Machine Learning (ICML), pp. 11762–11772.
PMLR, 2021.

Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In International Conference on Learning
Representations (ICLR), 2017.


-----

# Supplementary Materials

A SEQUENTIAL TRAINING BETWEEN TWO TASKS

A.1 NOTATIONS

The kernel ridge(-less) regression is given by


_N_

(f (x[µ]) _y[µ])[2]_ + [1] (S.1)
_−_ 2 _[⟨][f, f]_ _[⟩][H]_
_µ=1_

X


_f_ _[∗]_ = arg min
_f_ _∈H_


2λ


where N is the sample size and H is the reproducing kernel Hilbert space (RKHS) induced by the
neural tangent kernel Θ. We assume that the input samples x[µ] are generated from a probabilistic distribution p(x) and that the NTK has finite trace, i.e., _dxp(x)Θ(x, x) < ∞. Mercer’s decomposition_
of Θ is expressed by
R
_dx[′]p (x[′]) Θ (x, x[′]) φi (x[′]) = ηiφi(x)_ (i = 0, 1, . . ., ∞). (S.2)
Z

In other words, we have Θ(x[′], x) = _i=0_ _[η][i][φ][i][(][x][′][)][φ][i][(][x][)][. Note that a function belonging to RKHS]_
is given by

_∞_

[P][∞]

_f_ (x) = _aiφi(x)_ (S.3)

_i=0_

X

with _f_ [=][ P]i[∞]=0 _[a]i[2][/η][i][ <][ ∞][. Then, using the orthonormal bases][ {][φ][i][}]i[∞]=0[, the solution of the]_
_∥_ _∥H[2]_
regression is given by

_f_ _[∗](x) =_ _wi[∗][ψ][i][(][x][)][, w][∗]_ [=] ΨΨ[⊤] + λI _−1 Ψy_ (S.4)

_i_

X   

where ψi(x) := _ηiφi(x). Each column of Ψ is given by φ(x[µ]) (µ = 1, ..., N_ ). We focus on the

_[√]_
NTK regime and take the ridge-less limit (λ → 0).

The sequentially trained model (4) between tasks A and B is written as

_fA→B(x) −_ _fA(x) = Θ(x, XB)Θ(XB)[−][1]_ (yB − _fA(XB)),_ (S.5)

_fA(x) = Θ(x, XA)Θ(XA)[−][1]yA._ (S.6)
The model fA is trained on single task A and represented by

_fA(x) = wA[∗⊤][ψ][(][x][)]_ (S.7)

with
_wA[∗]_ [= lim] (S.8)
_λ_ 0 [argmin][w][A][ H][A][(][w][A][)][,]
_→_

_NA_

2

_HA(wA) := 2[1]λ_ _wA[⊤][ψ][(][x][µ]A[)][ −]_ _[y]A[µ]_ + 12 _[∥][w][A][∥]2[2][.]_ (S.9)

_µ=1_

X   

Eq. (S.9 ) is the objective function equivalent to (S.1 ) because _fA, fA_ = _wA_ 2[. Similarly, we]
_⟨_ _⟩H_ _∥_ _∥[2]_
can represent fA _B by the series expansion with the bases of RKHS. Note that the right-hand side of_
_→_
We have(S.5 ) is equivalent to the kernel ridge-less regression on input samples XB and labels yB − _fA(B)._

_fA_ _B(x) = (wA[∗]_ [+][ w]B[∗] [)][⊤][ψ][(][x][)] (S.10)
_→_

with
_wB[∗]_ [= lim] _A[)][,]_ (S.11)
_λ_ 0 [argmin][w][B][ H][(][w][B][, w][∗]
_→_

_NB_

2

_H(wB, wA[∗]_ [) := 1] _wB[⊤][ψ][(][x][µ]B[)][ −]_ [(][y]B[µ] _A_ _[ψ][(][x]B[µ]_ [))] + 1 2[.] (S.12)

2λ _µ=1_ _[−]_ _[w][∗⊤]_ 2 _[∥][w][B][∥][2]_

X   


-----

A.2 PROOF OF THEOREM 1.

A.2.1 KNOWLEDGE TRANSFER EA→B

First, we take the average over training samples of task B conditioned by task A, that is,


_EA_ _B_ _A :=_ _dxp(x)( f[¯]B(x)_ (wA[∗] [+][ w]B[∗] [)][⊤][ψ][(][x][))][2] (S.13)
_→_ _|_ _−_ _B_
Z D

= _dxp(x)((wB[∗]_ _[−]_ [( ¯]wB − _wA[∗]_ [))][⊤][ψ][(][x][))][2] _B_ (S.14)
Z D

where the set of task B’s training samples is denoted by _B. We have_
_D_

_EA_ _B =_ _EA_ _B_ _A_ _A_ _._ (S.15)
_→_ _⟨_ _→_ _|_ _⟩D_

Note that the objective function H(wB, wA[∗] [)][ is transformed to]


2
(wB ( ¯wB _wA[∗]_ [))][⊤][ψ][(][x][(]B[µ][)][)] + [1] 2[.] (S.16)
_−_ _−_ 2 _[∥][w][B][∥][2]_



_NB_

_µ=1_

X


_H(wB, wA[∗]_ [) = 1]

2λ


Comparing (S.14 ) and (S.16 ), one can see that the average over task B conditioned by task A is
we immediately obtainequivalent to single-task training with the target function ( ¯wB − _wA[∗]_ [)][⊤][ψ][(][x][)][. Therefore, by using (][8][),]


2
_γB_
+ _σ[2]_ (S.17)

1 _γB_

 _−_


_κB_
_ηi( ¯wB,i_ _wA,i[∗]_ [)][2]
_−_ _κB + NBηi_



_EA_ _B_ _A =_
_→_ _|_

=:


1 − _γB_ _i_ _i_ _B,i −_ _A,i_ _κB + NBηi_ 1 − _γB_

X

_γB_
_φi( ¯wB,i_ _wA,i[∗]_ [)][2][ +] _σ[2]._ (S.18)
_i_ _−_ 1 − _γB_

X


Next, we take the average of (S.18 ) over task A. Only ¯wA[∗] [depends on the task A and is determined by]
the single-task training (S.8 ). This corresponds to Lemma 2 with u = ¯wB and φi = ηiqB,i[2] _[/][(1]_ _[−]_ _[γ][B][)][.]_
We obtain


( ¯wA,i _w¯B,i)[2]_ 2 ¯wA,i( ¯wA,i _w¯B,i)qA,i_
_−_ _−_ _−_


_EA_ _B =_
_→_


_ηiqB,i[2]_

1 _γB_
_−_


1 _ηiNA_

+ _w¯A,i +_

 1 _γA_ _κ[2]A_

_−_



1 1 _NA_
+ σ[2]

1 _γA_ 1 _γB_ _κ[2]A_
_−_ _−_


_ηj ¯wA,j[2]_ _[q]A,j[2]_


_ηj ¯wA,j[2]_ _[q]A,j[2]_ _qA,i[2]_

 # 1



_γB_
_ηi[2][q]A,i[2]_ _[q]B,i[2]_ [+]

1 _γB_
_−_


(S.19)


Although this is a general result that holds for any ¯wA and ¯wB, it is a bit complicated and seems not
easy to give an intuitive explanation. Let us take the average over


). (S.20)



[ ¯wA,i, ¯wB,i] (0, ηi
_∼N_

The generalization error is then simplified to


_EA→B(ρ) = ⟨EA→B⟩w_ (S.21)

= 2(1 _ρ) (1_ _qA,i) +_ _qA,i[2]_ _EB,i_

_i_ " _−_ _−_ 1 − _γA_ #

X


_NA_

_κ[2]A_


_γB_
_ηi[2][q]A,i[2]_ _[q]B,i[2]_ [+]

1 _γB_
_−_


+ σ[2]


(S.22)


1 _γA_
_−_


1 _γB_
_−_


where we used ⟨wA,i[2] _[⟩][w][ =][ ⟨][w]B,i[2]_ _[⟩][w][ =][ η][i][ and][ ⟨][w][A,i][, w][B,i][⟩][w][ =][ ρ][.]_


-----

A.2.2 BACKWARD TRANSFER

Backward transfer is measured by the prediction on the previous task A:

_EA[back]B_ [:=] _dxp(x)( f[¯]A(x)_ _fA_ _B(x))[2]_ (S.23)
_→_ _−_ _→_
Z 

= _dxp(x)((wB[∗]_ _[−]_ [( ¯]wA − _wA[∗]_ [))][⊤][ψ][(][x][))][2] _._ (S.24)
Z 

First, we take the average over task B,


_EA[back]B_ _A_ [=] _dxp(x)((w[∗]_ ( ¯wA _wA[∗]_ [))][⊤][ψ][(][x][))][2]
_→_ _|_ _−_ _−_
Z


_._ (S.25)
_DB_


This corresponds to Lemma 2 with the replacementobjective function of NTK regression was given by (S. φ16i ←). The targetηi and u ← ¯wA in Lemma 2 is replaced asw¯A − _wA[∗]_ [. Recall that the]
_w¯A ←_ _w¯B −_ _wA[∗]_ [. We then have]


"( ¯wB,i − _w¯A,i)[2]_ _−_ 2( ¯wB,i − _wA,i[∗]_ [)( ¯]wB,i − _w¯A,i)qB,i_


_EA[back]B_ _A_ [=]
_→_ _|_


_ηiNB_

_κ[2]B_


1

+ _wB,i_ _wA,i[∗]_ [)][2][ +]

 _−_ 1 _γB_
 _−_

[( ¯]

= γB _ηi( ¯wB,i_ _w¯A,i)[2]_ +

_i_ _−_ _i_

X X


_iNB_

( _ηj( ¯wB,j_ _wA,j[∗]_ [)][2][q]B,j[2] [+][ σ][2][)] _B,i_ _ηi_
_κ[2]B_ _j=0_ _−_  #
X 

(S.26)

_ηiqB,i[2]_  _[q][2]_ 2

_wA,i_ ( ¯wB,i ( ¯wB,i _w¯A,i))_

1 _γB_ _−_ _−_ [1][ −]qB,i[γ][B] _−_
_−_  


_γB_

_σ[2]._ (S.27)
1 _γB_
_−_


Next, we take the average over _A. Since the first and third terms of (S.27 ) are independent of_
_D_
_A, we need to evaluate only the second term. The second term corresponds to Lemma 2 with_
_D_
_φi = ηiqB,i[2]_ _[/][(1][ −]_ _[γ][B][)][ and][ u][i][ = ¯]wB,i −_ (1 − _γB)/qB,i( ¯wB,i −_ _w¯A,i). We obtain_


_EA[back]B_ [=] _EA[back]B_ _A_
_→_ _→_ _|_
D


_DA_


_ηi( ¯wB,i_ _w¯A,i)[2]_ +
_−_


( ¯wA,i _w¯B,i)[2](qB,i_ (1 _γB))[2]_
_−_ _−_ _−_


= γB


2 ¯wA,i( ¯wA,i _w¯B,i)(qB,i_ (1 _γB))qA,iqB,i_
_−_ _−_ _−_ _−_


_ηiNA_

(
_κ[2]A_

_NA_


_ηi_

1 _γB_
_−_


1 _ηi_

+ _wA,i[2]_ [+]

 1 _γA_ _κ_
 _−_

1 1

 [¯]

+ σ[2]

1 _γA_ 1 _γB_
_−_ _−_


_ηjwA,j[2]_ _[q]A,j[2]_ [+][ σ][2][)] _B,i[q]B,i[2]_




_γB_ _[q][2]_
_ηi[2][q]A,i[2]_ _[q]B,i[2]_ [+] _._

1 _γB_ !
_−_


(S.28)


_κ[2]A_


-----

Finally, by taking the average over (S.20 ), we have

_EA[back]→B[(][ρ][) =][ ⟨][E]A[back]→B[⟩][w]_

= 2γBηi[2][(1][ −] _[ρ][) + 2 (][q][B,i]_ _ηi[2]_ (1 _ρ)_

_i_  _[−]_ [(1][ −] _[γ][B][))][2]_ 1 − _γB_ _−_

X


1 1

2 ηi[2][(][q][B,i] (1 _ρ) +_ _ηi[2][q]A,i[2]_ _[q]B,i[2]_
_−_ _[−]_ [1 +][ γ][B][)] _[q]1[B,i][q]γ[A,i]B_ _−_ 1 _γA_ 1 _γB_

_−_ _−_ _−_

1 1 _NA_ _γB_
+ σ[2] _ηi[2][q]A,i[2]_ _[q]B,i[2]_ [+]

1 − _γA_ 1 − _γB_ _κ[2]A_ _i_ 1 − _γB_ !

X


_qB,i[2]_ [(1][ −] _[q][A,i][)]_
1 + qB,i(qA,i 2) +
_−_ 1 _γB_

_−_


_ηi[2]_


= 2(1 − _ρ)_


1 _γA_
_−_


1 _γB_
_−_


_NA_

_κ[2]A_


_γB_
_ηi[2][q]A,i[2]_ _[q]B,i[2]_ [+]

1 _γB_
_−_


_ηi[2][q]A,i[2]_ _[q]B,i[2]_ [+][ σ][2]


(S.29)


1 _γA_
_−_


1 _γB_
_−_


A.2.3 LEMMA 2

**Lemma 2. Suppose training on single task A, the target of which is given by** _f[¯] =_ _i_ _w[¯]A,iψi, and_

_denote the trained model (2) as f_ _[∗]_ = _i_ _[w]A,i[∗]_ _[ψ][i][. Define the following cost function:]_

[P]

[P]

_E =_ _φi(wA,i[∗]_ (S.30)

*Xi _[−]_ _[u][i][)][2]+DA_

_for arbitrary constants φi and ui. Using the replica method under a sufficiently large NA, we have_


( ¯wA,i _ui)[2]_ 2 ¯wA,i( ¯wA,i _ui)qA,i_
_−_ _−_ _−_


_E =_


_i=0_


1 _ηiNA_

+ _wA,i[2]_ [+] ( _ηj ¯wA,j[2]_ _[q]A,j[2]_ [+][ σ][2][)] _A,i_ _φi._ (S.31)

 1 − _γA_ _κ[2]A_ Xj=0  #

 [¯]  _[q][2]_

_Proof. The process of derivation is similar to Canatar et al. (2021), but we have additional constants_
_φi and ui. They cause several differences in the detailed form of equations._


Define

where E(wA) =


_Z[J] =_ _dwA exp(_ _βHA(wA) + J [βN]_ (S.32)
_−_ 2 _[E][(][w][A][))]_
Z

_i_ _[φ][i][(][w][A,i][ −]_ _[u][i][)][2]_ _A_ [. We omit the index A in the following. We have]

_D_

2 _∂_
_E = lim_ _._ (S.33)
_β→∞_ _βN_ _∂J_ _J=0_

_[⟨][log][ Z][[][J][]][⟩][D]_


To evaluate ⟨log Z⟩, we use the replica method (a.k.a. replica trick):

1
log Z = lim (S.34)
_⟨_ _⟩D_ _n_ 0 _n_ [(][⟨][Z] _[n][⟩][D][ −]_ [1)]
_→_

The point of the replica method is that we first calculate Z _[n]_ for n ∈ N then take the limit of n to zero
by treating it as a real number. In addition, we calculate the average ⟨Z _[n]⟩D under a replica symmetric_
ansätz and a Gaussian approximation by following the calculation procedure of the previous works
(Dietrich et al., 1999; Bordelon et al., 2020; Canatar et al., 2021).


-----

We have

_⟨Z_ _[n]⟩D =_ _dWn exp_
Z


_−_ _[β]2_


_n_

_w[a]_ + _[βJN]_
_∥_ _∥[2]_ 2
_a=1_

X


(w[a] _−_ _u)[⊤]_ Φ (w[a] _−_ _u)_
_a=1_

X


_⟨Q⟩[N]{x[A][µ],ε[µ]}_ _[,]_

(S.35)

(S.36)


_n_

_Q := exp_ _−_ 2[β]λ _a=1_ (wA[a] _[−]_ _w[¯]A)[⊤]ψ(x[µ]) −_ _ε[µ][][2]!_ _,_ (S.36)

X  

where we define dWn = _a=1_ _[dw][a][ and][ Φ][ is a diagonal matrix whose diagonal entries given by][ φ][i][.]_
We take the shift of w[a] _→_ _w[a]_ + ¯w. Then,

[Q][n]

_Z_ _[n]_ = _dWn exp(_ _w_ + _[nβJN]_ ( ¯w _u)[⊤]Φ( ¯w_ _u))_
_⟨_ _⟩D_ _−_ _[nβ]2_ _∥[2]_ 2 _−_ _−_
Z _[∥]_ [¯]

=: Z[¯](J)
| {z }

exp( _w[a][⊤]Φw[a]_ _βk[⊤]w[a])_ _Q_ _x[µ],ε[µ]_ _[,]_ (S.37)

_·_ _−_ _[β]2_ 2 _−_ _⟨_ _⟩[N]{_ _}_

_a_

X _[∥][w][a][∥][2][ +][ βJN]_


_Q = exp(_
_−_ 2[β]λ


(w[a][⊤]ψ(x[µ]) − _ε[µ])[2]),_ (S.38)
_a=1_

X


_k¯ := ¯w −_ _JN_ Φ( ¯w − _u)._ (S.39)

First, let us calculate _Q_ _xµ,εµ_ . This term is exactly the same as appeared in the previous works
_⟨_ _⟩{_ _}_
(Bordelon et al., 2020; Canatar et al., 2021). To describe notations, we overview their derivation.
Define q[a] = w[a][⊤]ψ (x) + ε, which are called order parameters. We approximate the probability
distribution of q[a] by a multivariate Gaussian:

1
_P_ ( _q[a]_ ) = exp (q[a] _µ[a]) [C_ _[−][1]]ab_ _q[b]_ _µ[b][]_ _,_ (S.40)
_{_ _}_ (2π)[n] det(C) − 2[1] _−_ _−_ 

_a,b_

X  

with p  
_µ[a]_ := _q[a]_ _x,ε_ = _w[a][⊤]ψ(x)_ _x_ [+][ ⟨][ε][⟩][ε] [=][ √][η][0][w]0[a][,] (S.41)
_⟨_ _⟩{_ _}_

_C_ _[ab]_ := _q[a]q[b]_ _x,ε_ [=][ w][a][⊤][⟨][ψ][(][x][)][ψ][(][x][)][⊤][⟩][x][w][b][ +][ ⟨][ε][a][ε][b][⟩][ε][ =][ w][a][⊤][Λ][w][a][ + Σ][ab][,] (S.42)

_{_ _}_

where Σ[ab] = σδab, ⟨· · · ⟩x denotes the average over p(x) and Λ is a diagonal matrix whose entries
are ηi. We have _η0w0[a]_ [in (][S.41][ ) since][ η][0] [corresponds to the constant shift][ φ][0][(][x][) = 1][. Training]

_[√]_
samples are i.i.d. and we omitted the index µ. We then have

_−1_

_⟨Q⟩{x,ε} = exp_ _−_ [1]2 [log det] I + _[β]λ_ _[C]_ _−_ 2[β]λ _[µ][⊤]_ I + _[β]λ_ _[C]_ _µ!_ _,_ (S.43)

where I denotes the identity matrix. Define conjugate variables {µˆ[a], _C[ˆ][ab]} by the following identity:_

1 = _dµ[a]dµˆ[a]dC_ _[ab]dC[ˆ][ab]_
_C_ 
Z []a≥b

[Y] 

exp _N_ _µˆ[a][  ]µ[a]_ _wA,[a]_ 0√η0 _N_ _Cˆ[ab][  ]C_ _[ab]_ _w[a][⊤]Λw[b]_ Σ[ab][] _,_ (S.44)
_×_ − _−_ _−_ _−_ _−_ 

Xa  Xa≥b

where C denotes an uninteresting constant and we took the conjugate variables on imaginary axes. 

Next, we perform the integral over Wn. Eq. (S.37 ) becomes
_⟨Z_ _[n]⟩D_

= _Z[¯](J)_ _dWn_ _dΩ[ab]_ exp( _N_ ( _µˆ[a]µ[a]_ + _Cˆ[ab](C_ _[ab]_ Σ[ab]))) exp[ _NG_ _GS]_
_C_ _−_ _−_ _−_ _−_
Z _aY≥b_ Xa Xa≥b

(S.45)


-----

where dΩ[ab] = dµ[a]dµˆ[a]dC _[ab]dC[ˆ][ab]_ and define

_−1_

_G = 2 [1]_ [log det] _I +_ _[β]λ_ _[C]_ + 2[β]λ _[µ][⊤]_ _I +_ _[β]λ_ _[C]_ _µ,_ (S.46)

   


(
_−_ _[β]2_ 2

_[∥][w][a][∥][2][ +][ βJN]_


_w[a][⊤]Φw[a]_ _−_ _βk[⊤]w[a]))_


exp( _GS) = exp(_
_−_


_µˆ[a]wA,[a]_ 0√η0 + N


exp _N_
_×_ 

We can represent _dWn exp(−GS) by_
R
_dWn exp(−GS) =_
Z


_Cˆ[ab]w[a][⊤]Λw[b]_ _._ (S.47)



Xa≥b




_dxi exp_ _i_ _Q[ˆ]ixi_ _βb[⊤]i_ _[x][i]_
_−_ _[β]2_ _[x][⊤]_ _−_



(S.48)


(wherei = 0 x, 1i, ... ∈ )R. We defined[n] denotes a vector [wi[1][, ..., w]i[a][, ..., w]i[n][]][⊤] [and][ i][ is the index of kernel’s eigenvalue mode]

_Qˆi = (1_ _φiJN_ )In _C + diag( C[˜])),_ (S.49)
_−_ _−_ _[η][i]β[N]_ [( ˜]


_bi = k[¯]i1n_ (i ≥ 1), (S.50)

_b0 = k[¯]01n_ _µ,ˆ_ (S.51)
_−_ _[N]_ _[√]β_ _[η][0]_

where In is an n × n identity matrix and 1n is an n-dimensional vector whose all entries are 1. The
_GS term includes φ and c that are specific to our study. When φ = η and u = ¯w, it is reduced to_
previous works (Bordelon et al., 2020; Canatar et al., 2021). Taking the integral over _xi_, we have
_{_ _}_
_dWn exp(_ _GS) =_ exp( _[β]_ _i_ _Q[ˆ][−]i_ [1][b][i][)][/] det Q[ˆ]i. (S.52)
_−_ _C_ 2 _[b][⊤]_
Z _i=0_ q
Y


That is,


_GS = [1]_


log det Q[ˆ]i
_i=0_ _−_ _[β]2_

X


_b[⊤]i_ _Q[ˆ][−]i_ [1][b][i][.] (S.53)
_i=0_

X


REPLICA SYMMETRY AND SADDLE-POINT METHOD

Next, we carry out the integral (S.47 ) by the saddle-point method. Assume the replica symmetry:

_µ = µ[a],_ _r = C_ _[aa],_ _c = C_ _[a][̸][=][b],_ (S.54)

_µˆ = ˆµ[a],_ _rˆ = C[ˆ][aa],_ _cˆ = C[ˆ][a][̸][=][b]._ (S.55)

The following three terms are the same as in the previous works:

_n_
_βc_

det _I +_ _[β]_ = 1 + _[β]_ 1 + n _,_ (S.56)

_λ_ _[C]_ _λ_ [(][r][ −] _[c][)]_ _λ + β (r_ _c)_

 _−1_  1   _βc_ _−_ 

_I +_ _[β]_ = _I_ _,_ (S.57)
 _λ_ _[C]_ 1 + _[β]λ_ [(][r][ −] _[c][)]_  _−_ _λ + β (r −_ _c) + nβc_ **[11][⊤]**

_µˆ[⊤]µ +_ _Cˆ[ab](C_ _[ab]_ _−_ Σ[ab]) = n _µµˆ_ + ˆr(r − _σ[2]) −_ 2 [1] _q[ˆ](q −_ _σ[2])_ _,_ (S.58)

_a_ _b_  

X≥

where 11[⊤] denotes a matrix whose all entries are 1. Regarding the leading term of order n (n → 0),

_βC_

log det _I +_ _[β]_ = n log 1 + _[β]_ + n (S.59)

_λ_ _[C]_ _λ_ [(][r][ −] _[c][)]_ _λ + β (r_ _c)_ _[,]_

    _−_

_−1_ _nµ[2]_

_µ[⊤]_ _I +_ _[β]_ _µ =_ _._ (S.60)
 _λ_ _[C]_ 1 + _[β]λ_ [(][r][ −] _[c][)]_


-----

Furthermore. we have


_c_

_Qˆi = (1_ _φiJN_ _r_ _cˆ))In_ 1n1[⊤]n _[.]_ (S.61)
_−_ _−_ _[η][i]β[N]_ [(2ˆ] − _−_ _[η][i]β[N]_ [ˆ]


After straightforward algebra, the leading terms become

_c_
log det Q[ˆ]i = n ln gi _n_ _[η][i][N]_ [ˆ] (S.62)
_−_ _giβ [,]_

1[⊤]n _Q[ˆ][−]i_ [1][1][n][ =][ n] _,_ (S.63)

_gi_


with


_gi := 1_ _φiJN_ _r_ _cˆ)._ (S.64)
_−_ _−_ _[η][i]β[N]_ [(2ˆ] −

Substituting these leading terms into (S.45 ), we obtain ⟨Z _[n]⟩D = C_ _dθ exp(−nNS(θ)) with_
R


_S(θ) = −_ _[βJ]2 [( ¯]w −_ _u)[⊤]Φ( ¯w −_ _u)_

= _µµˆ_ + ˆr(r _σ[2])_ _c(c_ _σ[2])_ + [1]
_−_ _−_ 2[1] [ˆ] _−_ 2
 


log β (r _c) +_ _[c][ +][ µ][2]_
_−_ _r_ _c_
 _−_ 

1

( 2k[¯]0µˆ[√]η0 + Nη0µˆ[2]/β), (S.65)
2g0 _−_


+ [1]

2N


_c_
(ln gi ( _[η][i][N]_ [ˆ]
_i=0_ _−_ _β_

X


+ βk[¯]i[2][)][/g][i][)][ −]


where θ denotes a set of variables {r, ˆr, c, ˆc, µ, ˆµ}. Here, we take N ≫ 1 and use the saddle-point
method. We calculate saddle-point equations ∂S(θ[∗])/∂θ = 0 and obtain

_µ_
_µˆ =_ (S.66)
_−_ _r_ _c_ _[,]_

_−_ 2

_c + µ_ 1

_rˆ = [1]_ _,_ (S.67)

2 (r _c)[2][ −]_ _r_ _c_

 _−_ _−_ 

_cˆ =_ _[c][ +][ µ][2]_ (S.68)

(r _c)[2][,]_
_−_

_µ_ _k¯0_
_µ =_ _[Nη][0]_ [ˆ] _√η0,_ (S.69)

_g0β_ _−_ _g0_


_c_
( _[η][i][N]_ [ˆ]

_β_

_i=0_

X

_c_
( _[η][i][N]_ [ˆ]

_β_

_i=0_

X


+ βk[¯]i[2][)][ η][i] _k0µˆ[√]η0 + Nη0µˆ[2]β) + σ[2],_ (S.70)

_gi[2][β][ +][ η]g[0]0[2][N][β][ (][−][2¯]_


_r =_

_c =_


+ βk[¯]i[2][)][ η][i]

_gi[2][β][ −]_ _β[1]_


_ηi_
+ _[η][0][N]_ _k0µˆ[√]η0 + Nη0µˆ[2]β) + σ[2]._ (S.71)
_gi_ _g0[2][β][ (][−][2¯]_


_i=0_


From (S.67 ) and (S.68 ),

From (S.70 ) and (S.71 ), we also have


1

(S.72)
2ˆr _cˆ[.]_
_−_


_r −_ _c = −_


_ηi_
=: κ. (S.73)
_gi_


_β(r −_ _c) =_


_i=0_


Substituting (S.64 ) and (S.72 ) into (S.73 ), We obtain the implicit function for κ:


_ηi_ (S.74)

_W_ (φi) _[, W]_ [(][φ][i][) :=][ κ][(1][ −] _[φ][i][JN]_ [) +][ η][i][N.]


1 =


_i=0_


-----

Using κ and W, the saddle-point equations become

_µˆ = −_ _[βµ]κ [,]_ (S.75)

_c + µ2_

_rˆ = [1]_ _β[2]_ _,_ (S.76)

2 _κ[2]_ _−_ _[β]κ_

 

_cˆ =_ _[β][2]_ (S.77)

_κ[2][ (][c][ +][ µ][2][)][,]_

_√η0k¯0κ_
_µ =_ (S.78)
_−_ _Nη0 + W_ (φ0) _[,]_

_r = c +_ _β [κ]_ _[.]_ (S.79)


Substituting back these quantities into (S.65 ), we obtain

_S(θ[∗]) =_ _w_ _u)[⊤]Φ( ¯w_ _u) + [1]_
_−_ _[βJ]2 [( ¯] −_ _−_ 2 [ln][ κ][ + 1]2N


_k¯i[2]_

_gi_


ln gi
_i=0_ _−_ 2[β]N

X


_i=0_


_k¯0[2]_ (S.80)

_Nη0 + W_ (φ0) [+][ β]2κ _[σ][2][ +][ const.]_


_βη0κ_

2W (φ0)


Recall _Z_ _[n]_ exp( _nNS(θ[∗])). We have_
_⟨_ _⟩D ≈_ _−_
log Z = _NS(θ[∗])._ (S.81)
_⟨_ _⟩D_ _−_

GENERALIZATION ERROR

We evaluate

2 _∂_ 2 _∂_
_E = lim_ = lim _._ (S.82)
_β→∞_ _βN_ _∂J_ _J=0_ _−_ _β→∞_ _β_ _∂J [S][(][θ][∗][)]_ _J=0_

_[⟨][log][ Z][⟩][D]_

Note that W, g, κ and _k[¯] depend on J. At the point of J = 0,_
_∂W_ (φi)

= _φiNκ +_ _[∂κ]_ (S.83)
_∂J_ _−_ _∂J [,]_

_∂gi_ _∂κ_ (S.84)

_∂J_ [=][ −][(][φ][i][ + 1]κ[2] _∂J [η][i][)][N,]_

_∂κ_ _ηiφi_ (S.85)
_∂J_ [=][ κ]1 [2][N]γ (κ + ηiN )[2][,]

_−_ _i=0_

X

_∂k[¯]i_

_wi_ _ui)._ (S.86)
_∂J_ [=][ −][Nφ][i][( ¯] _−_

Substituting (S.83 -S.86 ) into (S.82 ), we obtain
_E =_

1 _ηiN_

( ¯wi _ui)[2]_ 2 ¯wi( ¯wi _ui)qi +_ _wi[2]_ [+] _ηj ¯wj[2][q]j[2]_ [+][ σ][2][)] _i_ _φi + ξ0,_

Xi=0" _−_ _−_ _−_  1 − _γ_ _κ[2][ (]Xj=0_  #

(S.87)

 [¯]  _[q][2]_

where ξ0 is

_Nη0 + 2W0_ 1 _N_

_ξ0 = η0 ¯w0[2]"κ_ _W0[2][(][Nη][0][ +][ W][0][)][2][ −]_ _W0(Nη0 + W0)_  1 − _γ_ _i_ _ηiqi[2][φ][i]!_

X


_Nη0 + 2W0_
_−_ _φ0Nκ[2]_ _W0[2][(][Nη][0][ +][ W][0][)][2]_


_φ0κN ¯w0( ¯w0_ _u0)_
+ 2η0 _−_ (S.88)

_W0(Nη0 + W0)_


with W0 = κ + Nη0. When η0 = 0, we have ξ0 = 0. Note that the additional term ξ0 is not specific
to our case but also appeared in the previous work (Canatar et al., 2021). We have ξ0 = O(1/η0) for
a large η0 and ξ0 is often negligibly small.


-----

A.3 EQUATIONS FOR UNDERSTANDING NEGATIVE TRANSFER

A.3.1 DERIVATION OF EQUATION 15

First, let us evaluate the term including qA,i = κA/(κA + NAηi). Note that κA is a monotonically
decreasing function of NA because

_−1_

_∂κA_ = _ηi_ _ηi[2]_ (S.89)

_∂NA_ _−_ _i_ (κA + NAηi)[2] ! _i_ (κA + NAηi)[2][ <][ 0][,]

X X

which comes from the implicit function theorem. Let us write κA at a finite NA = c as κA(c). We
then have

_qA,i_ _,_ (S.90)
_≤_ _[κ]N[A]A[(]η[c]i[)]_

for NA > c. Next, we evaluate

_qA,i[2]_

_EA_ _B(ρ) = 2(1_ _ρ)EB_ 2(1 _ρ)_ _qA,iEB,i +_ _EB,i._ (S.91)
_→_ _−_ _−_ _−_ _i_ _i_ 1 − _γA_
X X

The second term is negligibly small for a sufficiently large NA > c because

_qA,iEB,i_ _ηiqB,i[2]_ = _[κ][A][(][c][)]_ _κB._ (S.92)
_i_ _≤_ _[κ]N[A][(]A[c][)]_ _i_ 1 − _γB_ _NA_

X X

Note that we have 1 − _γ = κ_ _i=0_ _[η][i][/][(][κ][ +][ η][i][N]_ [)][2][.][ The third term is]

[P]qA,i[2] _EB,i_ _EA_ = 1 _γA_ _κ[2]A_ _,_ (S.93)

_i_ 1 − _γA_ _≤_ 1 − _γB_ 1 − _γB_ 1 − _γA_ _NA_

X

where we used qB,i 1. It decreases to zero as NA increases. Thus, we have EA _B(ρ)_
2(1 _ρ)EB._ _≤_ _→_ _∼_
_−_


A.3.2 DERIVATION OF EQUATION 19

_EA_ _B(1)_ 1 _i_ _[q]A,i[2]_ _[q]B,i[2]_ _[η]i[2]_
_→_ = (S.94)

_EB_ 1 − _γA_ P _i_ _[q]B,i[2]_ _[η]i[2]_

2

1 P _κA_ (S.95)
_≥_ 1 _γA_ _κA +_ _i_ _[q]B,i[2]_ _[η]i[3][N][A]_ !

_−_

where the second line comes from Jensen’s inequality. Using (S.90 ), we have

[P]

_i_ _qB,i[2]_ _[η]i[3]_ _[≤]_ _[κ][B]N[(]B[2][c][)][2]_ _i_ _ηi,_ (S.96)

X X

(forS.96 N ) is finite and converges to zero for a sufficiently largeB ≥ _c. Since we assumed the finite trance of NTK (i.e., NB. Therefore, we havei_ _[η][i][ <][ ∞][), the left-hand side of]_
_EA_ _B(1)_ 1 [P]
_→_ ≳ _._ (S.97)

_EB_ 1 _γA_

_−_

In contrast, EA→B/EB ≤ 1/(1 − _γA) since qA ≤_ 1. Thus, the ratio is sandwiched by 1/(1 − _γA)._


A.3.3 SUFFICIENT CONDITION FOR THE NEGATIVE TRANSFER

Let us denote the i-th mode of EA _B as EA_ _B,i and that of EB as EB,i. From EA_ _B,i > EB,i,_
_→_ _→_ _→_
we have

_f_ (qi) := 2(1 _ρ)(1_ _qi) +_ _qi[2]_ (S.98)
_−_ _−_ 1 _γ_

_−_ _[−]_ [1][ >][ 0][,]

where f (q) is a quadratic function and takes the minimum at q[∗] = (1 − _ρ)(1 −_ _γ). The condition_
_f_ (q) > 0 holds if and only if

_f_ (q[∗]) = −(1 − _γ)ρ[2]_ _−_ 2γρ + γ > 0. (S.99)
Solving this, we find ρ < _γ/(1 +_ _γ)._

_[√]_ _[√]_


-----

A.4 PROOF OF PROPOSITION 3.

For NA = NB, we have qA,i = qB,i = qi. Then,


_qi[4][η]i[2]_ (S.100)


_EA→B(1) =_


_A→B_ (1 _γ)[2]_

_−_ _i_
X

and the generalization error of single-task training is given by


_qi[2][η]i[2][.]_ (S.101)


_E =_


1 − _γ_


Model average is obtained in Section D and we have Eave = (1 − _γ/2)E < E because 0 < γ < 1._
Thus, we only need to evaluate the relation between Eave and EA _B:_
_→_

1 1

_Eave_ _EA_ _B = (1_ _qi[2][η]i[2]_ _ηi[2][q]i[4]_ (S.102)
_−_ _→_ _−_ _[γ]2 [)]_ 1 − _γ_ _i_ _[−]_ (1 − _γ)[2]_ _i_

X X

1 _κ[2]_

= (1 − _[γ]2 [)]_ 1 − _γ [κ][2][ X]i_ _a[2]i_ _[−]_ (1 − _γ)[2]_ _i_ _a[2]i_ _[q]i[2]_ (S.103)

X

_κ[2]_
= _N_ [2]a[3]i [(2][ −] _[Na][i][)][ −]_ [1] (S.104)

(1 _γ)[2]N_ 2 _[γ][2][(3][ −]_ _[γ][)]_
_−_ _i_ )

(X

=:F

where we defined

| _ηi_ {z }

_ai :=_ (S.105)

_κ + ηiN [,]_

and used qi = 1 _−_ _aiN and γ = N_ _i_ _[a]i[2][. We can provide a lower bound of][ F][ by using the following]_

cubic function:

[P]G(ai) := N [2]a[2]i [(2][ −] _[Na][i][)][.]_

We have 0 _ai_ 1/N by definition. Let us consider a lower bound of G by using its tangent line at
_ai = t/N (0 ≤ ≤ ≤t ≤_ 1), that is, Nt(4 − 3t)ai − 2t[2](1 − _t). Define_

_H(ai) := G(ai)_ (Nt(4 3t)ai 2t[2](1 _t))._
_−_ _−_ _−_ _−_
We have H(0) = 2t[2](1 − _t) ≥_ 0. Since H(1/N ) = −2(t − 1)[2](t − 1/2), we need t ≤ 1/2 to
guaranteefixed points H a(ia =i) ≥ t/N0 for all and (4 0 − ≤3ta)i/ ≤(3N1/N). We can see that for. Here, note that H t is a cubic function of ≤ 1/2, H has the local minimum at ai and has two
_ai = t/N and_

_H(t/N_ ) = t[2](2 − _t) −_ (t[2](4 − 3t) − 2t[2](1 − _t))_
= 0. (S.106)
Therefore, we have H(ai) 0 for all 0 _ai_ 1/N when the fixed constant t satisfies t 1/2.
Thus, we have the lower bound of ≥ _G:_ _≤_ _≤_ _≤_
_G(ai)_ _Nt(4_ 3t)ai 2t[2](1 _t)._ (S.107)
_≥_ _−_ _−_ _−_
Using this lower bound, we have


_aiG(ai)_
_−_ [1]2 _[γ][2][(3][ −]_ _[γ][)]_


_F =_


_γt(4_ 3t) 2t[2](1 _t)_ (S.108)
_≥_ _−_ _−_ _−_ _−_ 2[1] _[γ][2][(3][ −]_ _[γ][)][,]_

where we used γ = N _i_ _[a]i[2]_ [and][ P]i _[a][i][ = 1][. By setting][ t][ =][ γ/][2][, we obtain]_

_F_

[P] ≥ _[γ]2 [2]_ [(4][ −] [3][γ/][2)][ −] [2] _[γ]4 [2]_ [(1][ −] _[γ/][2)][ −]_ [1]2 _[γ][2][(3][ −]_ _[γ][)]_

= 0. (S.109)
Therefore, we have Eave > EA _B._
_→_

A.5 MULTIPLE DESCENT


-----

Figure 5: Theoretical values of EA→B(1). (Left) noise-less case (σ[2] = 0), (Right) noisy case
(σ[2] = 10[−][5]). We set D = 100 and other settings are the same as in Figure 1. In noise-less case,
generalization error monotonically decrease as NA or NB increases. In contrast, when noise is added,
it shows multiple descents.

We overview the multiple descent of E1 investigated in Canatar
et al. (2021). It appears for σ > 0. Let us set NB = αD[l]
(α > 0, l ∈ N, D ≫ 1), which is called the l-th learning
stage. We have EB,i = 0 (i < l), EB,l(α) and ηi[2] [(][i > l][)][.]
_EB,l(α) is a function of α, and becomes a one-peak curve_
depending on the noise and the decay of kernel’s eigenvalue
spectrum. Because each learning stage has a one-peak curve,
the whole learning curve shows multiple descents as the sample size increases. Roughly speaking, the appearance of the
multiple descent is controlled by γ. The γ is determined by the
kernel’s spectrum and sample size N . For example, previous
studies showed that if we assume the l-th learning stage, γ

Figure 4: Typical behaviors of κ

is a non-monotonic function of α, the maximum of which is

and γ.

_γ = 1/(_ _λ¯l +_ _λ¯l + 1)[2]_ for a constant _λ[¯]l =_ _k>l_ _η[¯]k/η¯l,_
where ¯η is a normalized eigenvalue of the kernel (Canatar et al., 2021). This tells us that γ repeats
p p
the increase and decrease depending on the increase of the learning stage as is shown in Figure 4.

[P]
Roughly speaking, this non-monotonic change of γ leads to the multiple descent.

In sequential training, EB,l(α) can similarly cause multiple descent as in single tasks because EA→B
is a weighted summation over EB,i. Figure 5 is an example in which noise causes multiple decreases
and increases depending on the sample sizes. We set ρ = 1 and obtained the theoretical values by
(S.22 ). While EA→B(1) is a symmetric function of indices A and B for σ = 0, it is not for σ > 0.
Depending on NA and NB, the learning “surface” becomes much more complicated than the learning
curve of a single task.

A.6 ADDITIONAL EXPERIMENT ON BACKWARD TRANSFER

Figure 6 shows the learning curves of backward transfer. Solid lines show theory, and markers show
the mean and interquartile range of NTK regression over 100 trials. The figure shows excellent
agreement between theory and experiments. As is shown in Section 4.1, subtle target dissimilarity
(ρ < 1) causes negative backward transfer (i.e., catastrophic forgetting). For a large NA, EA[back]B[(][ρ <]
_→_
1) approaches a non-zero constant while EA approaches zero. Thus, we have EA[back]B _[> E][A][ even for]_
_→_
the target similarity close to 1. When sample sizes are unbalanced (NA _NB), the learning curve_
shows negative transfer even for ρ = 1. This is the self-knowledge forgetting revealed in ( ≫ 20). The
ratio EA[back]B[(1)][/E][A][ takes a constant larger than 1, that is,][ 1][/][(1][ −] _[γ][B][)][.]_
_→_


-----

_ρ_


Figure 6: Negative backward transfer easily appears depending on target similarity and sample sizes.
We changed NA and set NB = 10[2]. We set D = 20 and other experimental details are the same as
in Figure 1(b).

B PROOF OF THEOREM 4.

B.1 LEARNING CURVE OF MANY TASKS

Eq. (4) is written as


_k=1_ Θ(x, Xk)Θ(Xk)[−][1](yk − _fk−1(Xk))._ (S.110)

X


_fn(x) =_


Each term of this summation is equivalent to the kernel ridge-less regression on input samples Xk
and labels yk _fk_ 1(Xk). Therefore, we can represent fn by
_−_ _−_ _n_


_wk[∗⊤][ψ][(][x][)]_ (S.111)
_k=1_

X


_fn(x) =_


with the minimization of the objective function H:
_wn[∗]_ [= lim] 1:(n 1)[)][,] (S.112)
_λ_ 0 [argmin][w][n][ H][(][w][n][, w][∗] _−_
_→_

_Nn_ _n−1_ 2

_H(wn, w1:([∗]_ _n_ 1)[) = 1] _wn[⊤][ψ][(][x][µ][)][ −]_ [(][y]n[µ] _wk[∗⊤][ψ][(][x][µ][))]_ + [1] 2 (S.113)
_−_ 2λ _µ=1_ _[−]_ _k=1_ ! 2 _[∥][w][n][∥][2]_

X X

_Nn_ _n−1_ 2

= [1] (wn ( ¯w _wk[∗][))][⊤][ψ][(][x][µ][)][ −]_ _[ε][µ]n_ + [1] 2[.] (S.114)

2λ _µ=1_ _−_ _−_ _k=1_ ! 2 _[∥][w][n][∥][2]_

X X

The generalization error is


_En+1 :=_ _dxp(x)( f[¯](x) −_ _fn+1(x))[2]_
Z D

_n_

= *Z _dxp(x)((wn[∗]+1_ _[−]_ [( ¯]w − _k=1_ _wk[∗][))][⊤][ψ][(][x][))][2]_

X


(S.115)

(S.116)


where = 1, ..., _n_ .
_D_ _{D_ _D_ _}_

Since the data samples are independent of each other among different tasks, we can apply Lemma 2
sequentially from wn+1 to w1. First, we take the average over the (n + 1)-th task, that is,

_n_

_En+1_ 1:n = _dxp(x)((wn[∗]+1_ _w_ _wk[∗][))][⊤][ψ][(][x][))][2]_ (S.117)
_|_ *Z _[−]_ [( ¯] − _k=1_ + _n+1_

X _D_


-----

This corresponds to Lemma 2 with φ = η and u = ¯wA = ¯w − [P]k[n]=1 _[w]k[∗][. We have]_

_n_

_En+1_ 1:n = _φ1,i( ¯wi_ _wk,i[∗]_ [)][2][ +][ R][1][σ][2] (S.118)
_|_ _i=1_ _−_ _k=1_
X X

with


_φ1,i :=_ _[η][i][q]n[2]+1,i_ _, R1 :=_ _γn+1_ _._ (S.119)

1 _γn+1_ 1 _γn+1_
_−_ _−_

Here, κn, γn and qn,k are determined by Nn. Next, we take the average over the n-th task:

_En+1|1:(n−1) =_ _En+1|1:n_ _Dn,εn_ (S.120)

_n−1_

= _i_ _φ1,i(wn,i[∗]_ _[−]_ [( ¯]wi − _k=1_ _wk,i[∗]_ [))][2]+ _n_ + R1σ[2] (S.121)

*X X _D_

This corresponds to Lemma 2 with φ = φ1 and u = ¯wA = ¯w − [P]k[n]=1[−][1] _[w]k[∗][. We obtain]_

_n−1_

_En+1_ 1:(n 1) = _φ2,i( ¯w_ _wk[∗][)][2][ +][ R][2][σ][2]_ (S.122)
_|_ _−_ _−_

_i=1_ _k=1_

X X

with


_Nn_
_φ2,i = φ1,iqn,i[2]_ [+] _n,i_

_κ[2]n[(1][ −]_ _[γ][n][)]_ _[η][i][q][2]_


_ηjφ1,jqn,j[2]_ _[,]_ (S.123)


_Nn_
_R2 = R1 +_

_κ[2]n[(1][ −]_ _[γ][n][)]_


_ηjφ1,jqn,j[2]_ _[.]_ (S.124)


Similarly, we can take the averages from the (n − 1)-th task to the first task, and obtain


_φn+1,i ¯wi[2]_ [+][ R][n][+1][σ][2][,] (S.125)
_i=1_

X


_En+1 =_


_φm+1,i = φm,iqn[2]_ _m+1,i_ [+] _Nn−m+1_ _n_ _m+1,i_ _ηjφm,jqn[2]_ _m+1,j_ _,_
_−_ _κ[2]n−m+1[(1][ −]_ _[γ][n][−][m][+1][)]_ _[η][i][q][2]−_  _j_ _−_ 

[X] (S.126)

for m = 1, ..., n, and


_Nn−m+1_

_κ[2]n_ _m+1[(1][ −]_ _[γ][n][−][m][+1][)]_
_−_


_ηjφm,jqn[2]_ _m+1,j[.]_ (S.127)
_−_


_Rn+1 =_


_m=1_


They are general results for any Nn. By setting Nn = N for all n, we can obtain a simpler formulation.
In a vector representation, φn+1 is explicitly written as

_n_

1 _N_
_φn+1 =_ diag(q[2]) + _qq˜[⊤]_ _q.˜_ (S.128)

1 _γ_ (1 _γ)κ[2][ ˜]_
_−_  _−_ 

Finally, by taking the average over ¯wi (0, ηi) which is the one-dimensional case of (S.20 ), we
have _∼N_

1 _N_ _n−1_
_En+1 =_ _q[⊤]_ diag(q[2]) + _qq˜[⊤]_ _q˜ + Rn+1σ[2]_ (S.129)

(1 _γ)[2][ ˜]_ (1 _γ)κ[2][ ˜]_
_−_  _−_ 

with


_N_ _m−1_

_qq˜[⊤]_ _q˜ +_
(1 _γ)κ[2][ ˜]_
_−_ 


_q˜[⊤]_ diag(q[2]) +
_m=1_ 

X


(S.130)
1 _γ [.]_
_−_


_Rn+1 =_


_κ[2](1 −_ _γ)[2]_


-----

B.2 MONOTONIC DECREASE OF En

For σ[2] = 0, we have

1
_En+1 =_ _q[⊤]_ _q.˜_ (S.131)

(1 _γ)[2][ ˜]_ _Q[n][−][1]_
_−_

Note that is a positive semi-definite matrix. If the maximum eigenvalue is no greater than 1, En
_Q_
is monotonically non-increasing withBecause the infinite norm bounds the eigenvalues, we have n. We denote the infinite norm as ∥A∥∞ = maxi _j_ _[|][A][ij][|][.]_
P


_ij_ (S.132)
_|Q_ _|_


_λ1(_ ) max
_Q_ _≤_ _i_

= max


2

_κ_

= max 1 + _[N]_ _,_ (S.133)
_i_ _κ [η][i]_ _κ + ηiN_

   

:=G(ηi)

where G(η) is monotonically decreasing for| η 0 and G{z(0) = 1. Therefore, the largest eigenvalue}
_≥_
_λ1(Q) is upper-bounded by 1, and En becomes monotonically decreasing. In particular. if ηi > 0_
for all i, we have λ1(Q) < 1 and obtain En+1 < En.

For λ1(Q) < 1, it is also easy to check that the series (S.130 ) in Rn+1 converges to a constant for
large n.


C DERIVATION OF KRR-LIKE EXPRESSION

KRR-like expression comes from the inverse formula of a block triangular matrix. Let us write (23)
as


_y1:(n−1)_
_yn_


_fn(x[′]) = [Θ(x[′], 1 : (n_ 1)) Θ(x[′], n)]Zn[−][1]
_−_

where Zn is a block triangular matrix recursively defined by

_Zn =_ _Zn−1_ _O_
Θ(n, 1 : (n − 1)) Θ(n)

Then, (S.134 ) is transformed to


(S.134)

(S.135)

(S.136)



[Θ(x[′], 1 : (n 1)) Θ(x[′], n)] _Zn[−]−[1]1_ _O_ _y1:(n−1)_ (S.136)
_−_ Θ(n)[−][1]Θ(n, 1 : (n 1))Zn[−][1]1 Θ(n)[−][1] _yn_
− _−_ _−_   

= Θ(x[′], 1 : (n 1))Zn[−][1]1[y][1:(][n][−][1)] [+ Θ(][x][′][, n][)Θ(][n][)][−][1][(][y][n][ −] [Θ(][n,][ 1 : (][n][ −] [1))][Z]n[−][1]1[y][1:(][n][−][1)][)]
_−_ _−_ _−_
(S.137)


= fn−1(x[′]) + Θ(x[′], n)Θ(n)[−][1](yn − _fn−1(Xn))._ (S.138)

Thus, one can see that the KRR-like expression is equivalent to our continually trained model.

D DERIVATION OF MODEL AVERAGE


For clarity, we set σ = 0, NA = NB, and ρ = 1 (that is, ¯wA = ¯wB). Generalization error of a model
average is expressed by

2[+]

_Eave =_ _dxp(x)_ _f¯B(x)_ (S.139)

_−_ _[f][A][(][x][) +]2_ _[ f][B][(][x][)]_
*Z  

_D_

_⊤_ 2

= _dxp(x)_ _w¯B_ _ψ(x)_ (S.140)

*Z  _−_ _[w][A][ +]2_ _[ w][B]_  ! +

_D_

_⊤_

= _w¯B_ Λ _w¯B_ (S.141)

_−_ _[w][A][ +]2_ _[ w][B]_ _−_ _[w][A][ +]2_ _[ w][B]_
*   [+]

_D_


-----

We can evaluate Eave in a similar way to EA _B. First, take the average over_ _B. We define_
_→_ _D_

_Eave|A = [1]4_ ((wB − (2 ¯wB − _wA))[⊤]Λ(wB −_ (2 ¯wB − _wA))_ _DB_ _[.]_ (S.142)

This average corresponds to Lemma 2 with replacement from the target task A to B, φ ← _η and_
_u_ 2 ¯wB _wA. Then, we have_
_←_ _−_

_Eave_ _A = [1]_ (wA,i _w¯B,i)[2]_ 2 ¯wB,i(wA,i _w¯B,i)qB,i +_ _w¯B,i[2]_ [+][ η][i][N][B] _EB_ _qB,i[2]_ _ηi_
_|_ 4 _i=0_  _−_ _−_ _−_  _κ[2]B_  

X

(S.143)


= [1]


(wA,i (1 + qB,i) ¯wB,i)[2]ηi + _[γ][B]_ (S.144)
_i=0_ _−_ 4 _[E][B][.]_

X


Next, we take the average over _A. The first term of (S.144 ) corresponds to Lemma 2 with_
_D_
replacement φ _η and u_ (1 + qB,i) ¯wB. Then, we get
_←_ _←_

_Eave =_ _Eave_ _A_ _D_ (S.145)
_⟨_ _|_ _⟩_ _A_

= [1] ((1 _qA,i) ¯wA,i_ (1 + qB,i) ¯wB,i)[2]ηi + _[γ][A]_ (S.146)

4 _i=0_ _−_ _−_ 4 _[E][A][ +][ γ]4[B]_ _[E][B][.]_

X

For NA = NB, we have qA = qB, γA = γB, and EA = EB. In addition, since we focused on
_w¯A = ¯wB, we have_


_ηi ¯wB,i[2]_ _[q]B,i[2]_ [+][ γ][B] (S.147)

2 _[E][B]_

_i=0_

X

1 _EB_ (S.148)
_−_ _[γ]2[B]_
 


_Eave =_

=

E EXPERIMENTAL SETTINGS

E.1 DUAL REPRESENTATION


In dual representation, the targets (10) are given by

[αA,i, αB,i] (0,
_∼N_


_/P_ _[′]),_ (S.149)


_P_ _[′]_


_P_ _[′]_


_αA,jΘ(x[′]j[, x][)][,][ ¯]fB(x) =_


_αB,jΘ(x[′]j[, x][)]_ (S.150)


_f¯A(x) =_


for a sufficiently large P _[′]. We sampled x[′]_ from the same input distribution p(x) and set P _[′]_ = 10[4]
in all experiments. It is easy to check that targets (10) and (S.150 ) are asymptotically equivalent for a large P _[′]._ Because Θ(x[′], x) = _i_ _[ψ][i][(][x][′][)][ψ][i][(][x][)][, we have][ P]j[P][ ′]_ _[α][A,j][Θ(][x]j[′]_ _[, x][) =]_

_i_ _Pj_ _[′]_ [(][α][A,j][ψ][i][(][x]j[′] [))][ψ][i][(][x][)][ ∼] [P]i _w[¯]iψi(x), where ¯wi is sampled from N_ (0, ηi).

[P]

P P

E.2 SUMMARIES ON EXPERIMENTAL SETUP

**Figure 1(a). We trained the deep neural network (1) with ReLU, L = 3, and Ml = 4, 000 by the**
gradient descent. We set a learning rate 0.5 and trained the network during 10,000 steps over 50 trials
with different random seeds. The target is given by (S.150 ) with C = 1, D = 10, and Gaussian input
samplesand calculated the generalization error over xi ∼N (0, 1). We initialized the network with 4, 000 samples. Each marker shows the mean and (σw[2] _[, σ]b[2][) = (2][,][ 0)][, set][ N][A][ =][ N][B][ = 100][,]_
interquartile range over the trials. To calculate the theoretical curves, we need eigenvalues ηi of the
NTK. We numerically compute them by the Gauss-Gegenbauer quadrature following the instruction
of Bordelon et al. (2020). Its implementation is also given by Canatar et al. (2021). Since the input


-----

samples are defined on a hyper-sphere S[d][−][1] and the NTK is a dot product kernel (i.e., Θ(x[′], x) can
be represented by Θ(x[′⊤]x)), the NTK can be decomposed into Gegenbauer polynomials:


_ηiN_ (d, i)Qi(z), (S.151)
_i=0_

X


Θ(z) =


where _Qi_ are the Gegenbauer polynomials and N (d, i) are constants depending d and i. Since the
_{_ _}_
Gegenbauer polynomials are orthogonal polynomials, we have

1
_ηi = c_ Θ(z)Qi(z)dτ (z), (S.152)
Z−1

for a certain constant c and dτ (z) = (1 − _z[2])[(][d][−][3)][/][2]dz. The Gauss-Gegenbaur quadrature approxi-_
mates this integral by


_ηi_ _c_ _wjΘ(zj)Qi(zj),_ (S.153)
_≈_ _j=1_

X


where wj are constant weights and zj are the r roots of Qr(z). The definitions of constants (N (d, i),
_c and wj) are given in Section 8 of Bordelon et al. (2020) and we set r = 1000 as is the same in this_
previous work. We computed ηi (i = 1, .., 1000) and substituted them to the analytical expressions.

**Figure 1(b). This figure shows the results of NTK regression; (2) for the single task and (4) for**
sequential training. We set NB = 4, 000, D = 50 and other settings were the same as in Figure 1(a).

**Figure 2. This figure shows the results of NTK regression; we set D = 20, and other settings are the**
same as Figure 1(a).

**Figure 3. (NTK regression) This figure shows the results of NTK regression; We set D = 10 and**
show the means and error bars over 100 trials. Other settings were the same as in Figure 1(a).

**(MLP) We trained the deep neural network (1) with ReLU, L = 5, and Ml = 512 by SGD with a**
learning rate 0.001, momentum 0.9, and mini-batch size 32 over 10 trials with Pytorch seeds 1-10. We
set the number of epochs to 150 and scaled the learning rate ×1/10 at the 100-th epoch, confirming
that the training accuracy reached 1 for each task.

**(ResNet-18) We trained ResNet-18 by SGD with a learning rate 0.01, momentum 0.9, and mini-batch**
size 128 over 10 trials with Pytorch seeds 1-10. We set the number of epochs to 150 and scaled the
learning rate ×1/10 at the 100-th epoch, confirming that the training accuracy reached 1 for each
task.

Note that the purpose of these experiments was not to achieve performance comparable to state-ofthe-art but to confirm self-knowledge transfer and forgetting. Therefore, we did not apply any data
augmentation or regularization method such as weight decay and dropout.


-----

