# WHEN LESS IS MORE: SIMPLIFYING INPUTS AIDS NEU## RAL NETWORK UNDERSTANDING

**Anonymous authors**
Paper under double-blind review

ABSTRACT

How do neural network classifiers behave if you make their inputs simpler? In this
work, we propose SimpleBits, a method to synthesize simplified inputs by reducing information content, and measure the effect of such simplification on learning.
To measure simplicity, we use the finding that the encoding bit size given by a pretrained generative image model correlates well with the visual complexity of the
image. Hence, we minimize the encoding bit size to simplify inputs and investigate the effect of such simplification on neural network behavior in several scenarios: conventional training, dataset condensation and post-hoc explanations. In
all settings, we optimize for simplified inputs that still contain the information to
learn the classification task. We show that SimpleBits successfully removes superfluous information for tasks with injected distractors and investigate the tradeoff
between input simplicity and task performance on real-world datasets. For dataset
condensation, we find that inputs can be simplified with only minimal accuracy
degradation. Finally, applied post-hoc to normally trained classifiers, SimpleBits
provides intuition into reasons for misclassifications.

1 INTRODUCTION

A better understanding of the information deep networks learn can lead to new scientific discoveries (Raghu & Schmidt, 2020), inform our understanding of differences between human and model
behaviors (Makino et al., 2020) and can serve as powerful auditing tools (Geirhos et al., 2020).

Removing information from the input manually is one way to understand what information content is
relevant for learning. For example, researchers have occluded parts of the input or removed specific
frequency ranges from the input to see which input regions and frequency ranges are relevant for
the network’s prediction (Zintgraf et al., 2017; Makino et al., 2020; Banerjee et al., 2021). These
ablation techniques use simple heuristics such as removing at random (Hooker et al., 2019) or exploit
domain knowledge about interpretable aspects of the input (input regions, frequency range) to create
simpler versions of the input and analyze the network’s prediction on these simpler inputs.

What if, instead of using heuristics, one would learn how to simplify inputs that contain predictionrelevant information? This way, one could synthesize simpler inputs and gain intuition into model
behavior without relying on domain knowledge about what information may be relevant for the
network. For this, one needs to define the precise meaning of “simplify an input”, including useful
metrics for the simplification of the inputs and for the retention of task-relevant information.

In this work, we propose SimpleBits – an information-reduction method that learns to synthesize
simplified inputs which contain less information but still remain informative for the task. To measure simplicity, we use a finding initially reported as a problem for density-based anomaly detection
– generative image models tend to assign higher probability densities and hence lower bits to visually simpler inputs (Kirichenko et al., 2020; Schirrmeister et al., 2020). Here, we use this to our
advantage and minimize the encoding bit size given by a generative network trained on a general
image distribution to simplify inputs. At the same time, we optimize that the simplified inputs still
contain the task-relevant information.

We then investigate what information is retained in the simplified inputs and how the simplification
affects network behavior in a variety of settings. We apply SimpleBits both in a per-instance setting,
where each image is processed to be a simplified version of itself, and the size of training set remains


-----

Per-Instance Data Simplification


Data Simplification

with Condensation


_during Training_


_after Training_


Original

dataset


Original

dataset


Original

dataset


Condense



**_SimpleBits_**


_Trained classifier_

**_SimpleBits_**


Train classifier

Sample batches

**_SimpleBits_**

Train classifier

Figure 1: We apply SimpleBits to a variety of tasks to aid neural network understanding. As a
per-image simplifier, applied during training (left), it investigates the trade-off curve between simplification and accuracy. It can also be used as a post-hoc analysis tool after training (center),
illuminating features of images that are crucial to the trained classifier. When combined with data
condensation (right), the original data set can be effectively reduced both in size and in complexity.

unchanged, as well as in a condensation setting, where the dataset is compressed to only a few
samples per class, with the condensed samples simplified at the same time. Applied during training,
_SimpleBits can be used to investigate the trade-off between the information content and the task_
performance. After training, SimpleBits can act as an analysis tool to understand what information
a trained model uses for its decision making. Figure 1 summarizes tasks covered in this paper.

Our evaluation provides the following insights in the investigated scenarios:

1. Per-instance simplification during training. _SimpleBits successfully removes super-_
fluous information for tasks with injected distractors. On natural image datasets, Sim_pleBits highlights plausible task-relevant information (shape, color, texture). Increasing_
simplification leads to accuracy decreases and we report the trade-off between simplifying
inputs and task level performance for different datasets.

2. Dataset simplification with condensation. We evaluate SimpleBits applied to a condensation setting that processes the training data into a much smaller set of synthetic images.
_SimpleBits simplifies these images to drastically reduce the encoding size without substan-_
tial task performance decrease. On a chest radiograph dataset (Johnson et al., 2019a;b),
_SimpleBits can uncover known radiologic features for pleural effusion and gender._

3. Post-training simplification. For a trained model, SimpleBits provides intuition into the
prediction-relevant information in an image. For example, by visualizing simplifications of
mispredicted images, we can form hypotheses of why these images were mispredicted.

2 MEASURING AND REDUCING INSTANCE COMPLEXITY

How to define simplicity? We use the fact that generative image models tend to assign lower encoding bit sizes to visually simpler inputs (Kirichenko et al., 2020; Schirrmeister et al., 2020). Concretely, the complexity of an image x can be quantified as the negative log probability mass given
by a pretrained generative model with tractable likelihood, G, i.e. log pG(x). This log probabil_−_


-----

Figure 2: Visualization of the bits-per-dimension (bpd) measure for image complexity, sorted from
low to high. Image samples are taken from MNIST, Fashion-MNIST, CIFAR10 and CIFAR100,
in addition to a completely black image sample. bpd is calculated from the density produced by a
Glow (Kingma & Dhariwal, 2018) model pretrained on 80 Million Tiny Images.

Side-by-Side MNISTs (ground truth: MNIST) MNIST with Uniform Noise (ground truth: MNIST)

Original

**_SimpleBits_**
output

Interpolated MNIST and CIFAR10 (ground truth: MNIST) CIFAR10 with Stripes (ground truth: Strips)

Original

**_SimpleBits_**
output

Figure 3: Evaluation of distractor removal on four composite datasets. Shown are the composite
original images and the corresponding simplified images produced by SimpleBits trained alongside
the classifier. SimpleBits is able to almost entirely remove task-irrelevant image parts, namely FashionMNIST (top left), random noise (top right), CIFAR10 (bottom left as well as bottom right).

ity mass can be interpreted as the image encoding size in bits per dimension (bpd) via Shannon’s
theorem (Shannon, 1948): bpd(x) = − log2 pG(x)/d where d is the dimension of the flattened x.

The simplification loss for an input x, given a pre-trained generative model G, is as follows:

_Lsim (x) = −_ log pG(x) (1)

Figure 2 visualizes images and their corresponding bits-per-dimension (bpd) values given by a Glow
network (Kingma & Dhariwal, 2018) trained on 80 Million Tiny Images (Torralba et al., 2008) (see
supp. sec. S1 for other models). This is the generative network used across all our experiments. A
visual inspection of Figure 2 suggests that lower bpd corresponds with simpler inputs, as also noted
in prior work (Serr`a et al., 2020). The goal of our approach, SimpleBits, is to minimize bpd of input
images while preserving task-relevant information.

We now explore how SimpleBits affects network behavior in a variety of scenarios. In each scenario, we explain the method to optimize Lsim and the retention of task-relevant information, the
[experimental setup, and results. All code is at https://tinyurl.com/simple-bits.](https://tinyurl.com/simple-bits)

3 PER-INSTANCE SIMPLIFICATION DURING TRAINING

When plugged into the training of a classifier f, SimpleBits simplifies each image such that f can still
learn the original classification task from the simplified batches. We apply backpropagation through
training steps: given a batch of input images Xorig, before updating the classifier f, an image-toimage simplifier network generates a corresponding batch of images Xsim such that: (a) images in
**_Xsim have low bpd as measured per Lsim in Equation (1), and (b) training on the simplified images_**
leads to a reduction of the classification loss on the original images.

We optimize (b) by unrolling one training step of the classifier. So for a single batch Xorig, y, we
first compute an updated classifier f _[′]_ as follows:


-----

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
||||||||||
||||||||||
|3.50 3.25 3.00 2.75 2.50 2.25 2.00 1.75 Bits per dimension|||||||||


Results for Instance Simplification on CIFAR10

90

80

70

Accuracy [%]

3.50 3.25 3.00 2.75 2.50 2.25 2.00 1.75

Bits per dimension


Figure 4: Selection of Simplified Training Images on CIFAR10. Simplified images from settings
with varying simplification loss weight λsim. We observe that at lower bits per dimension color is
retained only for some images such as green color for frog or blue sky for the plane. In this low bit
regime, texture remains discernable for the cat and frog images.

**_Xsim = simplifier(Xorig)_** (2)

_f_ = train step(f, l(f (Xsim), y)) (3)

_[′]_


where simplifier is an image-to-image network, l is the classification loss function, i.e., the crossentropy loss, and f _[′]_ is the classifier f after one optimization step using the classification loss
_l(f_ (Xsim), y) on the simplified data.

To train the simplifier network, we optimize for both (a) (Equation (1)) and for the classification loss on the batch of original images with the updated classifier after the unrolled training step
_l(f_ (Xorig), y) using backpropagation through training (Maclaurin et al., 2015; Finn et al., 2017).

_[′]_
Note that it would not be enough to instead optimize performance on the simplified images as the
simplifier could then to collapse all simplified images of one class to one representative example.

Adding the classification losses on the simplified data both before and after the unrolled training step
ensures the training is not influenced by predictions on the simplified data that are very different from
the classification target and we found that to improve training stability, leading to:

_Lcls_ = l(f (Xorig), y) + l(f (Xsim), y) + l(f (Xsim), y) (4)

_[′]_ _[′]_


Further details to stabilize the training are described in S4. The total loss for the simplifier is

_L = λsim · Lsim + Lcls,_ (5)

where λsim is a hyperparameter to control the trade-off between simplification and task completion.
In subsequent experiments, we vary λsim to flexibly control the extent of simplification.

**Implementation Our training architecture is a normalizer-free classifier architecture to avoid inter-**
pretation difficulties that may arise from normalization layers, such as image values being downscaled by the simplifier and then renormalized again. We use Wide Residual Networks and adapt
them according to the steps outlined in (Brock et al., 2021); additional details are included in the
Supp. Section S2. The normalizer-free architecture reaches 94.0% on CIFAR10 in our experiments,
however we opt for a smaller variant for faster experiments that reaches 91.2%, see Supp. Section
S2. For the simplifier network, we use a UNet architecture (Ronneberger et al., 2015) that we modify to be residual, more details see Supp. Section S3. For both simplifier and classifier networks,
we use AdamW (Loshchilov & Hutter, 2019) with lr = 5 · 10[−][4] and weight decay = 10[−][5].


-----

CIFAR10 SVHN FASHIONMNIST MNIST

90 100 End of

90 Joint Training

80

80 75 Retrained on

80 Simplified

Accuracy [%]70 70 60 50 Finetunedon Simplified

3 2 3 2 1 1.5 1.0 1.0 0.5

Bits per Dimension Bits per Dimension Bits per Dimension Bits per Dimension


Figure 5: Results for Training Image Simplifications on real Datasets. Dots show results for training
with different loss weights for the simplification loss. Images with less bits per dimension lead to
reduced accuracies, this already happens for smaller bpd-reductions for more complex datasets like
CIFAR10 than for less complex ones like SVHN.

3.1 _SimpleBits REMOVES INJECTED DISTRACTORS_

We first evaluate whether our per-instance simplification during training successfully removes superfluous information for tasks with injected distractors. To that end, we construct datasets to contain both useful (ground truth) and redundant (distractor) information for task learning. We create
four composite datasets derived from three conventional datasets: MNIST (LeCun & Cortes, 2010),
FashionMNIST (Xiao et al., 2017) and CIFAR10 (Krizhevsky, 2009). Their construction, samples
and results are described below, and shown in Figure 3.

**Side-by-Side MNISTS constructs each image by concatenating, left and right, one sample from**
Fashion-MNIST and another from MNIST. Each sample is rescaled to 16x32, so the concatenated
image size remains 32x32; the order of concatenation is random. The ground truth target is MNIST
labels, and therefore FashionMNIST acts like a distractor as it is irrelevant for the classification task.
As seen in Figure 3, the simplifier effectively removes the clothes side of the image.

**MNIST with Uniform Noise adds uniform noise to the MNIST digits, preserving the MNIST digit**
as the classification target. Hence the noise is the distractor and is expected to be removed. And
indeed the noise is no longer visible in the simplified outputs shown in Figure 3.

**Interpolated MNIST and CIFAR10 is constructed by interpolating between MNIST and CIFAR10**
images. MNIST digits are the classification target. The expectation is that the simplified images
should no longer contain any of the CIFAR10 image information. The result shows that most of the
CIFAR10 background is removed, leaving only slight traces of colors.

**CIFAR10 with Stripes overlays either horizontal or vertical stripes onto CIFAR10 images, with the**
binary classification label 0 for horizontal and 1 for vertical stripes. With this dataset we observe
the most drastic and effective information removal, where only the tip of vertical strips is retained,
which by itself is sufficient to solve this binary classification task.

3.2 TRADE-OFF CURVES ON CONVENTIONAL DATASETS

Sec 3.1 evaluates the ability of SimpleBits to remove redundant information where ground truth
is known. Now, we evaluate the trade-off between task performance and input simplification on
real-world datasets. We perform instance-level simplification on MNIST, Fashion-MNIST, SVHN
(Netzer et al., 2011) and CIFAR10, comparing results with varying weight λsim for the simplification
loss, including λsim = 0, so without using any simplification loss. At an extreme, when we have
simplified to the black image seen in Figure 2, we expect to see significant erosion of performance.
Understanding the trade-offs inbetween the original image and this extreme can help understand
how sensitive the training is to the removal of more complex features.

In Figure 5, there is a pronounced decay in performance at high levels of simplify (where final bpd
is less than 2). We also observe this decay curve to be more pronounced for relatively more complex
datasets such as CIFAR10, suggesting the classifiers need to be trained with more complex features
present for highest task performance. Baselines for our simplifier can be found in S8.


-----

The visible retention of task-relevant information in some images together with the decreased accuracies suggests another factor: The simplified inputs may lack noise-features that the classifier has to
learn to be invariant to. This is analogous to how removing data augmentation can decrease accuracies even though the augmentations themselves do not contain discriminative information. This view
implies SimpleBits can still be used to visualize task-relevant information and at the same motivates
the adaptation of SimpleBits to analyze regularly trained classifiers after training in Section 5.

4 DATASET SIMPLIFICATION WITH CONDENSATION

Now we investigate how SimpleBits affects training on a small synthetic condensed dataset. Multiple
methods have been proposed to achieve dataset condensation (Zhao et al., 2021; Zhao & Bilen, 2021;
Wang et al., 2018; Maclaurin et al., 2015), via backpropagation through training (Wang et al., 2018;
Maclaurin et al., 2015), gradient matching (Zhao & Bilen, 2021), or kernel based meta-learning
(Nguyen et al., 2021). Due to its small size, one can visualize the full condensed dataset to understand what information is preserved for learning. Our aim here is to combine SimpleBits with dataset
condensation to see if we could obtain a both smaller and simpler training dataset than the original.

In this setting, we jointly condense our training dataset to a smaller number of synthetic training
inputs and simplify the synthetic inputs according to our simplification loss (Equation (1)). Concretely, we add the simplification loss Lsim to the gradient matching loss proposed by Zhao & Bilen
(2021). The gradient matching loss computes the layerwise cosine distance between the gradient
of the classification loss wrt. to the classifier parameters θ produced by a batch of original images
**_Xorig and a batch of synthetic images Xsyn:_**

_Lmatch(Xorig, Xsyn) = D(∇θl(f_ (Xorig), y), ∇θl(f (Xsyn), y)). (6)

where D is the layerwise cosine distance. The matching loss is computed separately per class.

Overall, with our simplification loss, we get:


log pG(xsyn) (7)
_−_
**_xsynX∈Xsyn_**


_Lsyn(Xorig, Xsyn) = Lmatch(Xorig, Xsyn) +_


We perform dataset condensation on MNIST, Fashion-MNIST, SVHN and CIFAR10 with varying
_λsim for the simplification loss. We also apply dataset condensation to the chest radiograph dataset_
MIMIC-CXR-JPG (Johnson et al., 2019a;b) for predicting pleural effusion and gender. We use the
networks from (Zhao et al., 2021), but use Adam (Kingma & Ba, 2015) for optimization.

4.1 _SimpleBits RETAINS CONDENSATION PERFORMANCE WHILE GREATLY SIMPLIFYING DATA_

In Figure 6, we examine the accuracy for each condensed-and-simplified dataset. We observe that
for the natural image datasets, accuracies are mostly retained when decreasing the number of bits
per image. Note that the setting with highest bpd is a reimplementation of Zhao et al. (2021) and
therefore a baseline without simplification loss. We visualize examples in Figure 6 and observe that
the jointly condensed and simplified images look visually smoother, indicating that higher frequency
patterns visible in the original images are not needed to reach the same accuracy. These visualizations are also noticeably more smooth than the results for per-instance simplification Figure 3, which
suggests that data condensation may already favor features that are less complex.

**Evaluation of a medical chest radiograph dataset We also evaluate jointly condensing and sim-**
plifying for a dataset of chest radiograph images (Johnson et al., 2019a;b). This dataset has known
radiologic features for the presence of pleural effusion (Jany & Welte, 2019; Raasch et al., 1982) and
difference in gender (Bellemare et al., 2003). In Figure 7, we visualize both the condensed (top row)
and the jointly condensed and simplified dataset (bottom row). The overlayed shows that a visible
difference between presence of feature. For pleural effusion, a larger white region on the bottom of
the lung occurs in the simplified pathological image, while for gender, lungs appear slightly smaller
for the simplified female image.


-----

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
|8 6 4||||


1 Images per Class 10 Images per Class

100

FashionMNIST

75

75 CIFAR10

SVHN

50

MNIST

Accuracy [%] 50

8 6 4 10 8 6 4 2

Bits per dimension Bits per dimension


Figure 6: Dataset condensation accuracies (when retraining with the condensed dataset) vs. data
simplicity. Top: Each dot represents a data condensation experiment run with a particular weight
for the simplification loss, which results in more or less complex datasets. Accuracies can be retained
even with substantially reduced bits per dimension. In the 1-image-per-class case (top left figure),
arrows highlight the settings that are visualized in the bottom figure. Bottom: Condensed datasets
with varying simplification loss weight. Each row represents the whole condensed dataset (1 image
per class), with high (top row) or low (bottom row) bits per dimension. Lower bits per dimension
datasets are visually simpler and smoother while retaining the accuracy.

Figure 7: Condensed dataset for pleural effusion and gender prediction from chest radiographs in
MIMIC-CXR. Condensed images for the classes look very similar. Color-coded mixed rightmost
images reveal the differences between the classes. Green highlighted region at the lower end of the
lung consistent with typical radiologic features for pleural effusion (white region indicating fluid on
lungs), red highlighted around lung for gender indicate smaller lung volume for the female class.

5 POST-TRAINING SIMPLIFICATION

Per-instance SimpleBits reduces accuracies when applied during training, but can it be used to interpret trained classifiers with high accuracies after training? A trained classifier’s prediction may be
influenced by a lot of information it has seen during training. Here, we use SimpleBits to visualize
some of the information that would help the classifier remember what it has learned for that specific
prediction.


-----

**Algorithm 1 Simplification loss function after training**

1: given generative network G, input x, simplified input xsim, classifier f, parameter scaling factors s < 1
2:3: f hscd = ← f (ScaleParametersx), hsim = f (xsim(f,) s) _▷_ Scale parameters of classifier down by▷ Predict original and simplified with unscaled classifier s to simulate ”forgetting”

4: hscd = fscd(x), hsim,scd = fscd(xsim) _▷_ Predict original and simplified with scaled classifier

5: Lgrad = D _∇sDKL(h∥hscd), ∇sDKL(h∥hsim,scd)_ _▷_ Compute distance of gradients on scaling factors

6:7: L Lpredsim = = − DlogKL  ( phG∥(hxsimsim) +)) _DKL(hscd∥hsim,scd)_  _▷_ Compute needed bits for simplified input▷ Compute prediction differences

8: return Lgrad + Lpred + λsim · Lsim


For synthesizing the prediction-relevant information, we simulate that the classifier forgets knowledge and then synthesize a simplified input that allows the classifier to relearn the relevant knowledge for the prediction of the original input. To simulate forgetting, we scale down all parameters
of the trained classifierscaling removes information from the model by bringing the learned parameter values closer to zero, f by multiplying them with a gating value φscaled,i = φi · s, s < 1. This
is identical to weight decay and also makes the network simpler in terms of model encoding size
(Hinton & van Camp, 1993). To synthesize a simplified input that helps learning to restore the parameter values that are important for a specific input, we compare gradients between the original
and simplified input.

Given an input, we can compute the gradients of the KL-divergence between the rescaled network’s
prediction fscd(x) and the original network’s prediction f (x). At each iteration, the layerwise cosine distance between these two sets of gradients (one for original datapoint and the other for the
simplified) is the basis of the loss used in SimpleBits. We compute this distance only considering
the gradients that are negative for the original input. Combined with the simplification loss Eq. 1,
this amounts to asking what input information is needed to recover parts of the original network to
_restore the original prediction? To ensure that the network is trained towards minimizing the same_
prediction difference on the simplified and original data, we also add a prediction difference loss
_Lpred. Further details about the implementation are included in Alg. 1 and Section S5._

In Figure 8, we visualize both the misclassified images according to the original network f and
produce the corresponding simplified versions. We observe that simplified images may provide
some intuition into the reason for misclassification, highlighting a variety of different features for
different images. We imagine a possible practitioner workflow, where the practitioner derives a set
of possible hypotheses for the misclassification from SimpleBits and tests them on the real data. We
show further post-hoc simplified examples in supplementary Section S13.

6 RELATED WORK

A different approach to reduce input bits while retaining classification performance is to train a compressor that only keeps information that is invariant to predefined label-preserving augmentations.
Dubois et al. (2021) implement this elegant approach in two ways. In their first variant, by training a VAE to reconstruct an unaugmented input from augmented (e.g. rotated, translated, sheared)
versions. In their second variant, building on the CLIP (Radford et al., 2021) model, they take
image-text pairs and view all images with the same text as augmented versions of each other. This
allows to use compressed CLIP encodings for classification and achieves up to 1000x compression
on Imagenet without decreasing classification accuracy. Their approach focuses on achieving maximum compression while our approach is focused on interpretability. Their approach requires access
to predefined label-preserving augmentations and has reduced classification performance in input
space compared to latent/encoding space.

Our approach simplifying individual training images builds on Raghu et al. (2021), where they learn
to inject information into the classifier training. Per-instance simplification during training can be
seen as a instance of their framework combined with the idea of input simplification. In difference
to their methods, SimpleBits explicitly aims for interpretability through input simplification.

Other interpretability approaches that synthesize inputs include generating counterfactual inputs
(Hvilshøj et al., 2021; Dombrowski et al., 2021; Goyal et al., 2019) or inputs with exaggerated
features (Singla et al., 2020). SimpleBits differs in explicitly optimizing the inputs to be simpler.


-----

True class **Ship** **Horse** **Cat** **Bird** **Deer** **Cat**

Original Image


Predicted as

**_SimpleBits output_**

Hypothesis for
one cause of
misclassification

Control Image
(predicted correctly)

Expected
Gradients
on Original Image

Prediction
Differences
on Original Image


Plane Deer Dog Frog Car Frog


_Tree top_
_resembles antler_


_Black pads of paw,_
_red color at_
_bottom_


_Black dots on_
_ship body_


_High frequency_
_texture_


_Circle resembles_
_shape of car_


_Green color_


Figure 8: Post-hoc simplifications of misclassified CIFAR-10 examples. For each, simplified image
reveals plausible causes for the misclassification. We subsequently made alterations to compensate
for the cause (from left to right: removing black dots, removing tree top, removing color, removing
high frequency texture, removing circle, and removing color), and are able to revert the predictions
to the true class. We also show color-coded saliency maps for expected gradients (Erion et al.,
2021) and prediction difference (Zintgraf et al., 2017) for comparison (red: evidence for and blue:
evidence against the predicted class). SimpleBits reveals more information than saliency methods.

Generative models have often been used in various ways for interpretability such as generating realistic-looking inputs (Montavon et al., 2018) and by directly training generative classifiers
(Hvilshøj et al., 2021; Dombrowski et al., 2021), but we are not aware of any work except (Dubois
et al., 2021) (discussed above) to explicitly generate simpler inputs.

7 CONCLUSION

We propose SimpleBits, an information-based method to synthesize simplified inputs. Crucially,
_SimpleBits does not require any domain-specific knowledge to constrain which input components_
should be removed; instead SimpleBits itself learns to remove the components of inputs which are
least relevant for a given task.

As an interpretability tool, we show that SimpleBits is able to remove injected distractors, suggest plausible reasons for misclassification, and recover known radiologic features from condensed
datasets. When combined with data condensation, SimpleBits retains accuracy while greatly reducing the complexity of condensed images.

Our simplification approach sheds light on the information required for a deep network classifier to
learn its task. We find that the tradeoff between task performance and input simplification varies by
dataset and setting - it is more pronounced for more complex datasets.


-----

REPRODUCIBILITY STATEMENT

We provide the following information to ensure reproducibility. Main concepts, algorithms and
basic architectures are described in sections 3, 5 and 4. Further network architecture details are in
supp. sections S2 and S3, further optimization details in supp. sections S4 and S5. Finally, the code
[is available under https://tinyurl.com/simple-bits.](https://tinyurl.com/simple-bits)


-----

REFERENCES

Imon Banerjee, Ananth Reddy Bhimireddy, John L. Burns, Leo Anthony Celi, Li-Ching Chen, Ramon Correa, Natalie Dullerud, Marzyeh Ghassemi, Shih-Cheng Huang, Po-Chih Kuo, Matthew P.
Lungren, Lyle J. Palmer, Brandon J. Price, Saptarshi Purkayastha, Ayis Pyrros, Luke OakdenRayner, Chima Okechukwu, Laleh Seyyed-Kalantari, Hari Trivedi, Ryan Wang, Zachary Zaiman,
Haoran Zhang, and Judy W. Gichoya. Reading race: AI recognises patient’s racial identity in med[ical images. CoRR, abs/2107.10356, 2021. URL https://arxiv.org/abs/2107.10356.](https://arxiv.org/abs/2107.10356)

Franc¸ois Bellemare, Alphonse Jeanneret, and Jacques Couture. Sex differences in thoracic dimensions and configuration. American journal of respiratory and critical care medicine, 168(3):
305–312, 2003.

Andy Brock, Soham De, Samuel L. Smith, and Karen Simonyan. High-performance large-scale
image recognition without normalization. In Marina Meila and Tong Zhang (eds.), Proceedings
_of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual_
_Event, volume 139 of Proceedings of Machine Learning Research, pp. 1059–1071. PMLR, 2021._
[URL http://proceedings.mlr.press/v139/brock21a.html.](http://proceedings.mlr.press/v139/brock21a.html)

Ann-Kathrin Dombrowski, Jan E Gerken, and Pan Kessel. Diffeomorphic explanations with normalizing flows. In ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit
_[Likelihood Models, 2021. URL https://openreview.net/forum?id=ZBR9EpEl6G4.](https://openreview.net/forum?id=ZBR9EpEl6G4)_

Yann Dubois, Benjamin Bloem-Reddy, Karen Ullrich, and Chris J. Maddison. Lossy compression
[for lossless prediction. CoRR, abs/2106.10800, 2021. URL https://arxiv.org/abs/](https://arxiv.org/abs/2106.10800)
[2106.10800.](https://arxiv.org/abs/2106.10800)

Gabriel Erion, Joseph D Janizek, Pascal Sturmfels, Scott M Lundberg, and Su-In Lee. Improving
performance of deep learning models with axiomatic attribution priors and expected gradients.
_Nature Machine Intelligence, pp. 1–12, 2021._

Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th Interna_tional Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017,_
volume 70 of Proceedings of Machine Learning Research, pp. 1126–1135. PMLR, 2017. URL
[http://proceedings.mlr.press/v70/finn17a.html.](http://proceedings.mlr.press/v70/finn17a.html)

Robert Geirhos, J¨orn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel,
Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature
_Machine Intelligence, 2(11):665–673, 2020._

Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, and Stefan Lee. Counterfactual visual
explanations. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
_International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, Cali-_
_fornia, USA, volume 97 of Proceedings of Machine Learning Research, pp. 2376–2384. PMLR,_
[2019. URL http://proceedings.mlr.press/v97/goyal19a.html.](http://proceedings.mlr.press/v97/goyal19a.html)

Jakob Drachmann Havtorn, Jes Frellsen, Søren Hauberg, and Lars Maaløe. Hierarchical vaes know
what they don’t know. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th In_ternational Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, vol-_
ume 139 of Proceedings of Machine Learning Research, pp. 4117–4128. PMLR, 2021. URL
[http://proceedings.mlr.press/v139/havtorn21a.html.](http://proceedings.mlr.press/v139/havtorn21a.html)

Geoffrey E. Hinton and Drew van Camp. Keeping the neural networks simple by minimizing the
description length of the weights. In Lenny Pitt (ed.), Proceedings of the Sixth Annual ACM
_Conference on Computational Learning Theory, COLT 1993, Santa Cruz, CA, USA, July 26-28,_
_[1993, pp. 5–13. ACM, 1993. doi: 10.1145/168304.168306. URL https://doi.org/10.](https://doi.org/10.1145/168304.168306)_
[1145/168304.168306.](https://doi.org/10.1145/168304.168306)

Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. A benchmark for interpretability methods in deep neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d Alch´[´] e-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Sys_[tems, volume 32. Curran Associates, Inc., 2019. URL https://proceedings.neurips.](https://proceedings.neurips.cc/paper/2019/file/fe4b8556000d0f0cae99daa5c5c5a410-Paper.pdf)_
[cc/paper/2019/file/fe4b8556000d0f0cae99daa5c5c5a410-Paper.pdf.](https://proceedings.neurips.cc/paper/2019/file/fe4b8556000d0f0cae99daa5c5c5a410-Paper.pdf)


-----

Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep networks with
stochastic depth. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.), Computer
_Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14,_
_2016, Proceedings, Part IV, volume 9908 of Lecture Notes in Computer Science, pp. 646–661._
[Springer, 2016. doi: 10.1007/978-3-319-46493-0\ 39. URL https://doi.org/10.1007/](https://doi.org/10.1007/978-3-319-46493-0_39)
[978-3-319-46493-0_39.](https://doi.org/10.1007/978-3-319-46493-0_39)

Jonathan J. Hull. A database for handwritten text recognition research. IEEE Trans. Pattern Anal.
_[Mach. Intell., 16(5):550–554, 1994. doi: 10.1109/34.291440. URL https://doi.org/10.](https://doi.org/10.1109/34.291440)_
[1109/34.291440.](https://doi.org/10.1109/34.291440)

Frederik Hvilshøj, Alexandros Iosifidis, and Ira Assent. ECINN: efficient counterfactuals from
[invertible neural networks. CoRR, abs/2103.13701, 2021. URL https://arxiv.org/abs/](https://arxiv.org/abs/2103.13701)
[2103.13701.](https://arxiv.org/abs/2103.13701)

Berthold Jany and Tobias Welte. Pleural effusion in adults—etiology, diagnosis, and treatment.
_Deutsches Arzteblatt International[¨]_, 116(21):377, 2019.

Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-identified publicly
available database of chest radiographs with free-text reports. Scientific data, 6(1):1–8, 2019a.

Alistair EW Johnson, Tom J Pollard, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng,
Yifan Peng, Zhiyong Lu, Roger G Mark, Seth J Berkowitz, and Steven Horng. Mimic-cxr-jpg, a
large publicly available database of labeled chest radiographs. arXiv preprint arXiv:1901.07042,
2019b.

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR
_[2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:](http://arxiv.org/abs/1412.6980)_
[//arxiv.org/abs/1412.6980.](http://arxiv.org/abs/1412.6980)

Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions.
In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.),
_Advances in Neural Information Processing Systems (NeuRIPs), pp. 10215–10224, 2018._

Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Why normalizing flows fail to
detect out-of-distribution data. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, MariaFlorina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems
_33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, De-_
_[cember 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/](https://proceedings.neurips.cc/paper/2020/hash/ecb9fe2fbb99c31f567e9823e884dbec-Abstract.html)_
[2020/hash/ecb9fe2fbb99c31f567e9823e884dbec-Abstract.html.](https://proceedings.neurips.cc/paper/2020/hash/ecb9fe2fbb99c31f567e9823e884dbec-Abstract.html)

Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.

[Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.](http://yann.lecun.com/exdb/mnist/)
[lecun.com/exdb/mnist/.](http://yann.lecun.com/exdb/mnist/)

Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradient descent with warm restarts. In 5th
_International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,_
_[2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.](https://openreview.net/forum?id=Skq89Scxx)_
[net/forum?id=Skq89Scxx.](https://openreview.net/forum?id=Skq89Scxx)

Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In 7th International
_Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019._
[OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.](https://openreview.net/forum?id=Bkg6RiCqY7)

Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimization through reversible learning. In Francis Bach and David Blei (eds.), Proceedings of the
_32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine_
_Learning Research, pp. 2113–2122, Lille, France, 07–09 Jul 2015. PMLR._ [URL https:](https://proceedings.mlr.press/v37/maclaurin15.html)
[//proceedings.mlr.press/v37/maclaurin15.html.](https://proceedings.mlr.press/v37/maclaurin15.html)


-----

Taro Makino, Stanislaw Jastrzebski, Witold Oleszkiewicz, Celin Chacko, Robin Ehrenpreis, Naziya
Samreen, Chloe Chhor, Eric Kim, Jiyon Lee, Kristine Pysarenko, et al. Differences between
human and machine perception in medical diagnosis. arXiv preprint arXiv:2011.14036, 2020.

Gr´egoire Montavon, Wojciech Samek, and Klaus-Robert M¨uller. Methods for interpreting and understanding deep neural networks. Digital Signal Processing, 73:1–15, 2018. ISSN 1051-2004.
doi: https://doi.org/10.1016/j.dsp.2017.10.011. [URL https://www.sciencedirect.](https://www.sciencedirect.com/science/article/pii/S1051200417302385)
[com/science/article/pii/S1051200417302385.](https://www.sciencedirect.com/science/article/pii/S1051200417302385)

Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning
_and Unsupervised Feature Learning, 2011._

Timothy Nguyen, Roman Novak, Lechao Xiao, and Jaehoon Lee. Dataset distillation with infinitely
[wide convolutional networks. CoRR, abs/2107.13034, 2021. URL https://arxiv.org/](https://arxiv.org/abs/2107.13034)
[abs/2107.13034.](https://arxiv.org/abs/2107.13034)

Hieu Pham, Zihang Dai, Qizhe Xie, and Quoc V. Le. Meta pseudo labels. In
_IEEE_ _Conference_ _on_ _Computer_ _Vision_ _and_ _Pattern_ _Recognition,_ _CVPR_ _2021,_ _vir-_
_tual,_ _June_ _19-25,_ _2021,_ pp. 11557–11568. Computer Vision Foundation / IEEE,
2021. URL [https://openaccess.thecvf.com/content/CVPR2021/html/](https://openaccess.thecvf.com/content/CVPR2021/html/Pham_Meta_Pseudo_Labels_CVPR_2021_paper.html)
[Pham_Meta_Pseudo_Labels_CVPR_2021_paper.html.](https://openaccess.thecvf.com/content/CVPR2021/html/Pham_Meta_Pseudo_Labels_CVPR_2021_paper.html)

BN Raasch, EW Carsky, EJ Lane, JP O’Callaghan, and ER Heitzman. Pleural effusion: explanation
of some typical appearances. American Journal of Roentgenology, 139(5):899–904, 1982.

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.

Aniruddh Raghu, Maithra Raghu, Simon Kornblith, David Duvenaud, and Geoffrey E. Hinton.
Teaching with commentaries. In 9th International Conference on Learning Representations,
_ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021._ [URL https:](https://openreview.net/forum?id=4RbdgBh9gE)
[//openreview.net/forum?id=4RbdgBh9gE.](https://openreview.net/forum?id=4RbdgBh9gE)

Maithra Raghu and Eric Schmidt. A survey of deep learning for scientific discovery. arXiv preprint
_arXiv:2003.11755, 2020._

Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for
biomedical image segmentation. In Nassir Navab, Joachim Hornegger, William M. Wells
III, and Alejandro F. Frangi (eds.), Medical Image Computing and Computer-Assisted Inter_vention - MICCAI 2015 - 18th International Conference Munich, Germany, October 5 - 9,_
_2015, Proceedings, Part III, volume 9351 of Lecture Notes in Computer Science, pp. 234–241._
[Springer, 2015. doi: 10.1007/978-3-319-24574-4\ 28. URL https://doi.org/10.1007/](https://doi.org/10.1007/978-3-319-24574-4_28)
[978-3-319-24574-4_28.](https://doi.org/10.1007/978-3-319-24574-4_28)

Robin Schirrmeister, Yuxuan Zhou, Tonio Ball, and Dan Zhang. Understanding anomaly detection with deep invertible networks through hierarchies of distributions and features. In
Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and HsuanTien Lin (eds.), Advances in Neural Information Processing Systems 33: _Annual Con-_
_ference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12,_
_[2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/](https://proceedings.neurips.cc/paper/2020/hash/f106b7f99d2cb30c3db1c3cc0fde9ccb-Abstract.html)_
[f106b7f99d2cb30c3db1c3cc0fde9ccb-Abstract.html.](https://proceedings.neurips.cc/paper/2020/hash/f106b7f99d2cb30c3db1c3cc0fde9ccb-Abstract.html)

Joan Serr`a, David Alvarez, Vicenc[´] ¸ G´omez, Olga Slizovskaia, Jos´e F. N´u˜nez, and Jordi Luque. Input complexity and out-of-distribution detection with likelihood-based generative models. In
_8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia,_
_[April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=](https://openreview.net/forum?id=SyxIWpVYvr)_
[SyxIWpVYvr.](https://openreview.net/forum?id=SyxIWpVYvr)

Claude Elwood Shannon. A mathematical theory of communication. The Bell System Technical
_Journal, 27(3):379–423, July 1948._


-----

Sumedha Singla, Brian Pollack, Junxiang Chen, and Kayhan Batmanghelich. Explanation by
progressive exaggeration. In 8th International Conference on Learning Representations, ICLR
_2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020._ [URL https://](https://openreview.net/forum?id=H1xFWgrFPS)
[openreview.net/forum?id=H1xFWgrFPS.](https://openreview.net/forum?id=H1xFWgrFPS)

Antonio Torralba, Rob Fergus, and William T. Freeman. 80 million tiny images: A large data set for
nonparametric object and scene recognition. IEEE Transactions on Pattern Analysis and Machine
_Intelligence, 30(11):1958–1970, 2008._

Tongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and Alexei A. Efros. Dataset distillation. CoRR,
[abs/1811.10959, 2018. URL http://arxiv.org/abs/1811.10959.](http://arxiv.org/abs/1811.10959)

Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.

Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In Richard C. Wilson, Edwin R.
Hancock, and William A. P. Smith (eds.), Proceedings of the British Machine Vision Conference
_[2016, BMVC 2016, York, UK, September 19-22, 2016. BMVA Press, 2016. URL http://www.](http://www.bmva.org/bmvc/2016/papers/paper087/index.html)_
[bmva.org/bmvc/2016/papers/paper087/index.html.](http://www.bmva.org/bmvc/2016/papers/paper087/index.html)

Bo Zhao and Hakan Bilen. Dataset condensation with differentiable siamese augmentation. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine
_Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine_
_[Learning Research, pp. 12674–12685. PMLR, 2021. URL http://proceedings.mlr.](http://proceedings.mlr.press/v139/zhao21a.html)_
[press/v139/zhao21a.html.](http://proceedings.mlr.press/v139/zhao21a.html)

Bo Zhao, Konda Reddy Mopuri, and Hakan Bilen. Dataset condensation with gradient matching.
In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria,
_May 3-7, 2021. OpenReview.net, 2021._ [URL https://openreview.net/forum?id=](https://openreview.net/forum?id=mSAKhLYLSsl)
[mSAKhLYLSsl.](https://openreview.net/forum?id=mSAKhLYLSsl)

Luisa M. Zintgraf, Taco S. Cohen, Tameem Adel, and Max Welling. Visualizing deep neural network decisions: Prediction difference analysis. In 5th International Conference on Learning
_Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings._
[OpenReview.net, 2017. URL https://openreview.net/forum?id=BJ5UeU9xx.](https://openreview.net/forum?id=BJ5UeU9xx)


-----

Figure S1: Visualization of the bits-per-dimension (bpd) measure for image complexity, sorted from
low to high. Image samples are taken from MNIST, Fashion-MNIST, CIFAR10 and CIFAR100, in
addition to a completely black image sample. bpd is calculated from the density produced by a
Glow (Kingma & Dhariwal, 2018) model pretrained on 80 Million Tiny Images, a PixelCNN model
trained on CIFAR10, and a diffusion model trained on CIFAR10.

SUPPLEMENTARY OUTLINE

This document completes the presentation of the main paper with the following:

_• Bits per dimensions for selected images of other generative models in S1_

_• Details about the classifier and simplifier architectures in sections S2 and S3_

_• Details about the optimization of per-instance simplification during and after training in_
sections S4 and S5

_• Files sizes of the simplified images when compressed with PNG in section S6_

_• Learning curves during retraining on the simplified images in section S7_

_• Baselines for the per-instance simplification during training in section S8_

_• Potentially spurious features revealed by SimpleBits in section S10_

_• More images simplified during and after training in sections S9 and S13, including post-_
hoc-SimpleBits output on the control images



_• More condensed and simplified datasets under varying settings in section S11_

_• Results for the continual learning setting in section S12_

S1 BPDS OF OTHER GENERATIVE MODELS

Figure S1 shows that the bits per dimension produced by other generative models than Glow also
correlate well with visual complexity, validating our measure. This is consistent with prior work
that found bpds of generative models trained on natural image datasets are strongly influenced by
general natural image characteristics independent of any specific dataset (Kirichenko et al., 2020;
Schirrmeister et al., 2020; Havtorn et al., 2021).

S2 CLASSIFIER NETWORK DETAILS

Our classification network built on the Wide ResNet architecture (Zagoruyko & Komodakis, 2016).
We used a version with relatively few parameters with depth = 16 and widen factor = 2 to allow
for fast iteration on experimentation. We used ELU instead of ReLU nonlinearities.


-----

Additionally, we removed batch normalization to avoid interference of normalization layers with the
simplification process. We followed the method from Brock et al. (2021) to create a normalizer-free
Wide ResNet. We reparameterize the convolutional layers using Scaled Weight Standardization:

_Wˆij =_ _[W]√[ij]Nσ[ −]_ _[µ]i_ _[i]_ _,_ (S1)

where µi = (1/N ) _j_ _[W][ij][,][ σ]i[2]_ [= (1][/N] [)][ P]j[(][W][ij][ −] _[µ][i][)][2][, and][ N][ denotes the fan-in. Further]_

as in Brock et al. (2021), ”activation functions are also scaled by a non-linearity specific scalar
gain γ, which ensures that the combination of the γ-scaled activation function and a Scaled Weight

[P]

Standardized layer is variance preserving.” Finally, the output of the residual branch is downscaled
by 0.2, so the function to compute the output becomes hi+1 = hi + 0.2 · fi(hi), where hi denotes
the inputs to the i[th] residual block, and fi denotes the function computed by the i[th] residual branch.
Unlike Brock et al. (2021), we did not multiply scalars βi with the input of the residual branch or
learned zero-initialized scalars to multiply with the output of the residual branch, as we did not find
these two parts helpful in our setting. We also did not attempt to use Stochastic Depth (Huang et al.,
2016), which may further improve upon the accuracies reported here. Due to our small batch sizes
(32), we also did not use adaptive gradient clipping.

S3 SIMPLIFIER NETWORK DETAILS

For the simplifier, we adapted a publicly available implementation of UNet 1. We used
num down = 5 downsampling steps, ELU nonlinearities ngf = 64 filters in the last conv layer
and a simple affine transformation layer instead of a normalization layer. Furthermore, we made the
simplifier residual and ensured the output is within [0, 1] by adding the output of the UNet to the
inverse-sigmoid-transformed input and then reapplying the sigmoid function.

S4 OPTIMIZATION DETAILS PER-INSTANCE SIMPLIFICATION DURING
TRAINING

First, we note that the single train step helps ensure a correspondence between simplified and
original images, and is a technique others have used in meta-learning settings (Pham et al., 2021).

For stabilizing the optimization of the per-instance simplification during training, we found two
further steps helpful. First, we modify:
_Lcls_ = l(f (Xorig), y) + l(f (Xsim), y) + l(f (Xsim), y) (S2)

_[′]_ _[′]_
to
_Lcls_ = 10 _l(f_ (Xorig), y) + l(f (Xsim), y) + l(f (Xsim), y) (S3)
_·_ _[′]_ _[′]_
as (a) the gradient magnitudes are much smaller from the losses after unrolling and (b) we want
to prioritize the classification loss on the original data. Additionally, we dynamically turn off Lsim
during training, for any example x where l(f (xorig), y) > 0.1, which we also found to further

_[′]_
stabilize training.

S5 OPTIMIZATION DETAILS PER-INSTANCE SIMPLIFICATION AFTER
TRAINING

We found it beneficial to optimize the simplified inputs in the latent space of the pre-trained Glow
network and to apply our loss functions to all interpolated inputs on the path between original and
simple input in latent space instead of only to the simplified input itself. The points on the path
include information from the original input and prevent that the optimization is unable to recover
some relevant information from the original input. Additionally, we also ensure that the predictions
are the same for the original and interpolated inputs for both the original and the scaled model. The
complete loss function can be found in Alg. 2, during training we called it with scaling factors
sampled from s U (0.8, 0.95) as these values mostly led to similar but more uniformly distributed
predictions than the unperturbed network.

[1https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)


-----

**Algorithm 2 Simplification loss function after training full algorithm**

1: given generative invertible network G, input x and its latent code z (from G), simplified input’s latent code
**_zsim, classification network f_**, parameter scaling factors s < 1

2:3: f αscaled ∼ _U ←(0,ScaleParameters 1)_ (f, s) _▷_ Scale parameters of classifier down by▷ Sample interpolation factor uniformly between 0 and 1. s to simulate ”forgetting”

4: zmixed = α · zsim + (1 − _α) · z_ _▷_ Interpolate simple and original input in latent space

5: xmix = invert(G, zmix) _▷_ Invert zmix using invertible network

6: h = f (x), hmix = f (xmix) _▷_ Predict original and mixed with classifier

7: hscaled = fscaled(x), hscaled,mix = fscaled(xmix) _▷_ Predict original and mixed with scaled classifier

8: Lgrad = d _∇sDKL(h∥hscaled), ∇sDKL(h∥hmix,scaled)_ _▷_ Compute distance between gradients on
scaling factors.

9: Lpred = D KL(h∥hmix) + DKL(hscaled∥hmix,scaled)  _▷_ Compute prediction differences

10: xsim = invert(G, zsim) _▷_ Invert zsim using invertible network

11: Lsimplification = − log pG(xsim)) _▷_ Compute needed bits for simplified input

12: return Lgrad + Lpred + λsim · Lsimplification


90

80

70



End of Joint Training
Retraining
Orig CIFAR10


T 6.0 5.5 5.0 4.5 4.0

PNG-compressed File Size in BPDs

Figure S2: Tradeoff between PNG storage space and accuracies. Note that the PNG bpd file sizes
do not show the maximally possible savings, these can be seen from the bpd values in the main
manuscript.


S6 PNG-COMPRESSED FILE SIZES OF SIMPLIFIED IMAGES

Figure S2 shows the tradeoff between PNG storage space of the simplified images and the accuracies
achieved when retraining.


S7 LEARNING CURVES FOR RETRAINING

Figure S3 shows learning curves during retraining on the simplified images on CIFAR10. There are
no noticeable differences in training speed for more or less simplified images.



Retraining Learning Curves CIFAR10

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
||||||



20 40 60 80 100

Training Epoch


75

50

25


3.57 BPD
3.56 BPD
3.52 BPD
2.95 BPD
2.81 BPD
2.66 BPD
2.50 BPD
2.26 BPD
1.86 BPD
1.76 BPD


Figure S3: Learning curves for retraining on simplified images on CIFAR10.


-----

Per-Instance Simplification During Training, Retraining Results

CIFAR10 SVHN FASHIONMNIST MNIST

100

SimpleBits

75

75 75 Minimize MSE and BPD

50 50 Gaussian Blurring

50 JPEG Compress

Accuracy [%]25 50

3 2 1 3 2 1 2 1 1.5 1.0 0.5

Bits per dimension Bits per dimension Bits per dimension Bits per dimension

Figure S4: Comparison between SimpleBits and two simpler baselines: In the first one, the simplifier
network is trained to simultaneously reduce bpd of the simplified image and the mean squared error
between the simplified and the original image. In the second one, gaussian blurring is applied to
the input images, different runs vary in the standard deviation used to create the gaussian blurring
kernel. In the third one, we use JPEG compression with varying quality levels. Tradeoff curves are
worse for the baselines than for SimpleBits .

S8 SIMPLIFIER BASELINES

We implemented three simpler baselines to check whether the losses used in SimpleBits during
training help retain task-relevant information. In the first baseline, we train the simplifier to simultaneously reduce bpd of the simplified image and the mean squared error between the simplified and
the original image. Afterwards we train the classifier on the simplified images and evaluate on the
original images the same way as during retraining of SimpleBits. In the second baseline, we blur the
original images with a gaussian kernel, which also reduces their bpd. We vary the sigma/standard
deviation for the gaussian kernel to trade off smoothness and task-informativeness. In the third
baseline, we use lossy JPEG compression with varying quality levels. As in SimpleBits and the
other baselines, we estimate the bits per dimension of the lossy-JPEG-compressed images through
our pretrained Glow network for a fair comparison. The gaussian blurring and JPEG compression each replace the simplifier, so these are fixed simplifier baselines without training a simplifier.
While these three baselines also retain some task-relevant information allowing the classifier to retain above-chance accuracies (see Figure S4), the tradeoff between bpd and accuracy is worse than
for SimpleBits. This shows the losses used in SimpleBits help retain more task-relevant information
compared to these baselines.

S9 MORE IMAGES SIMPLIFIED DURING TRAINING

We show a larger number of images that were simplified on CIFAR10 during training with the largest
simplification loss weight λsim = 2.0 in Figures S5, S6 and S7.

S10 POTENTIALLY SPURIOUS FEATURES UNCOVERED BY SIMPLEBITS

_SimpleBits may have the potential to reveal spurious correlations present in the dataset. We show_
some simplified images that reveal potentially spurious features in Figure S8. These observations
can be used as a starting point to further investigate whether these features also affect regularly
trained classifiers.

S11 MORE CONDENSED DATASETS

We also show some interesting condensed datasets that resulted when we varied the architecture
(Figure S10) or the condensation loss (from gradient matching to either negative gradient product or
single-training-step unrolling, Figure S11).


-----

Figure S5: Uncurated set of simplified images with λsim = 2.0, 6 per class.

S12 EVALUATION OF CONDENSED DATASETS FOR CONTINUAL LEARNING

We also evaluate the simplified condensed datasets in a continual learning setting, following (Zhao
& Bilen, 2021). In this task-incremental continual learning setting, the model is trained on different
classification datasets sequentially. When training on a new dataset, the model is additionally trained
on the condensed versions of the previous datasets.

The continual learning experiment reproduces the setting from (Zhao & Bilen, 2021) to first train
on SVHN, then on MNIST and finally on USPS (Hull, 1994), using the average accuracy across all
three datasets of the classifier at the end of training as the final accuracy (see (Zhao & Bilen, 2021)
for details).

We created a simpler and faster continual training pipeline that achieves comparable results to Zhao
& Bilen (2021). First, we train 3 times for 50 epochs on SVHN, with a cosine annealing learning
rate schedule (Loshchilov & Hutter, 2017) that is restarted at each time with lr = 0.1. Then for each
MNIST and USPS, we train one cosine annealing cycle of 50 epochs for lr = 0.1.

We first verified that we can reproduce the prior continual learning results with our simpler training
pipeline and find that our training pipeline indeed even slightly outperforms the reported final results (96.0% vs. 95.2% with, and 95.4% vs 93.0% without knowledge distillation) despite slightly
inferior performance in the first training stage (before any continual learning, 93.6% vs 94.1%), see
following subsection. When using different SVHN and MNIST condensed datasets, we find that we
can retain the original continual learning accuracies even with condensed datasets with substantially
less (∼9x less) bits per dimension (see Fig.S12).


-----

Figure S6: Uncurated set of correctly predicted simplified images with λsim = 2.0, 6 per class.

S12.1 WITHOUT CONDENSED DATASET

Our training pipeline still exhibits forgetting when not using any condensed datasets of previously
trained-on datasets. As Figure S13 shows, the accuracies are far lower than with just regular sequential training. We performed this ablation to ensure forgetting still occurs in our training pipeline.

S13 MORE IMAGES SIMPLIFIED AFTER TRAINING

We show further examples of post-hoc-simplified images for misclassified original images in Figure S14. We also show the output of SimpleBits when applied to the control images of Figure 8 in
Figure S15. We also show an uncurated set of incorrectly predicted and correctly predicted images
in Figures S16 and S17.


-----

Figure S7: Correctly predicted simplified images with strongest color with λsim = 2.0, 6 per class.


Correctly
Predicted


Incorrectly
Predicted


**Predicted**
**as**

Car

Plane

Horse

Ship


**Spurious**
**Feature**

Red
texture

Blue color

Red Brown
texture

Dark bottom
half, bright
top half


Figure S8: Selected simplified images that highlight potentially spurious features. Two leftmost
images are correctly predicted, two rightmost images are incorrectly predicted.


-----

Figure S9: Dataset condensation results with varying simplification loss weight. Top: Individual
dots represent accuracies for setting with different simplification loss weights. Accuracies can be
retained even with substantially reduced bits per dimension. For 1 image per class, arrows highlight
the settings that are visualized below. Below: Condensed datasets with varying simplification loss
weight. Per dataset, showing condensed datasets with high (top row) and low (bottom row) bits
per dimension. Lower bits per dimension datasets are visually simpler and smoother while mostly
retaining accuracies.


Instance
norm
ReLU

Instance
norm
ELU

No
norm
ReLU

No
norm
ELU


28.7% Acc.
4.35 BPD

27.3% Acc.
4.35 BPD

22.5% Acc.
4.35 BPD

22.7% Acc.
4.31 BPD


Figure S10: Dataset condensation on CIFAR10 with varying architecture.


-----

Grad
Distance,
1 Outer
step

Grad
Product

1-step
Unrolling

Grad
Product,
4 Outer
Steps

Grad
Product,
ReLU,
4 Outer
Steps


28.7% Acc.
4.35 BPD

29.1% Acc.
6.47 BPD

26.4% Acc.
5.24 BPD

27.6% Acc.
1.6 BPD

26.9% Acc.
0.37 BPD


Figure S11: Dataset condensation on CIFAR10 with varying condensation loss and varying outer
loop steps, i.e. how many steps the classifier is trained at each training epoch (default 1 in the 1
image per class setting), after each step the condensation loss is again optimized.

Continual Learning Results

96 ours w. KD SVHN bpd: 11.9, MNIST BPD: 4.3

95 ours w.o. KD

ours w. KD SVHN bpd: 3.9, MNIST BPD: 2.5

94

ours w.o. KD

Accuracy [%] 93 ours w. KD SVHN bpd: 1.0, MNIST BPD: 0.8

92 ours w.o. KD

seen during training) SVHN SVHN+ SVHN+

(average across datasets MNIST MNIST+ Orig w. KD

USPS Orig w.o. KD

Training Stage


Figure S12: Continual Learning Results. Results for first training on SVHN, then MNIST and then
USPS for condensed datasets with varying bits per dimension. Solid lines are with and dashed
lines without knowledge distillation. Note that continual learning accuracies remain similar also
for substantially reduced bits per dimension. Ablations show that accuracies degrade without any
condensed dataset, see supplementary.

Continual Learning Results

ours w. KD SVHN bpd: 11.9, MNIST BPD: 4.3

95

ours w.o. KD

90 ours w. KD SVHN bpd: 3.9, MNIST BPD: 2.5

ours w.o. KD

85 ours w. KD SVHN bpd: 1.0, MNIST BPD: 0.8

ours w.o. KD

Accuracy [%] 80

no condensed dataset

seen during training)75 Orig w. KD

(average across datasets Orig w.o. KD

SVHN SVHN+MNIST SVHN+MNIST+USPS

Training Stage


Figure S13: Continual Learning Results without Condensed Dataset (regular sequential training).
Conventions as in Figure S12. Accuracies substantially worse without any condensed dataset.


-----

Original Images

Predicted as Ship Plane

Simplified


Frog

Higher Freq
Texture

Bird


Deer

Color and antler
-like shapes

Cat


Car

Car and
wheelshape

Deer


Hypothesis for
one cause of
misclassification

Control Image


Ground + plane
looks like ship


Black dots


Predicted as Plane Ship


**(true class)** **(true class)** **(true class)** **(true class)** **(true class)**


Original Images

Predicted as Deer Frog

Simplified


Car

Brighter parts form
car contours

Cat


Deer

Color, ear/tail
antler-like

Dog


Dog

Black-white
dog face

Bird


Hypothesis for
one cause of
misclassification

Control Image


Tree as antler


Color


Predicted as Horse Cat


**(true class)** **(true class)** **(true class)** **(true class)** **(true class)**

Figure S14: Further examples of post-hoc simplifications of originally misclassified images.


-----

True class **Ship** **Horse** **Cat** **Bird** **Deer** **Cat**

Original Image


Predicted as

**_SimpleBits output_**

Hypothesis for
one cause of
misclassification

Control Image
(predicted correctly)

**_SimpleBits output_**
on control


Plane Deer Dog Frog Car Frog


_Tree top_
_resembles antler_


_Black pads of paw,_
_red color at_
_bottom_


_Black dots on_
_ship body_


_High frequency_
_texture_


_Circle resembles_
_shape of car_


_Green color_


Figure S15: Post-hoc simplifications of control images.


-----

Figure S16: Uncurated post-hoc simplifications of incorrectly predicted images.


-----

Figure S17: Uncurated post-hoc simplifications of correctly predicted images.


-----

