# UNDERSTANDING LATENT CORRELATION-BASED MULTIVIEW LEARNING AND SELF-SUPERVISION: AN IDENTIFIABILITY PERSPECTIVE


**Qi Lyu**
School of EECS
Oregon State Univ.


**Xiao Fu[∗]**
School of EECS
Oregon State Univ.


**Weiran Wang**
Google Inc.
Mountain View


**Songtao Lu**
IBM Research
Yorktown Heights


ABSTRACT

Multiple views of data, both naturally acquired (e.g., image and audio) and artificially produced (e.g., via adding different noise to data samples), have proven
useful in enhancing representation learning. Natural views are often handled by
multiview analysis tools, e.g., (deep) canonical correlation analysis [(D)CCA],
while the artificial ones are frequently used in self-supervised learning (SSL)
paradigms, e.g., BYOL and Barlow Twins. Both types of approaches often
involve learning neural feature extractors such that the embeddings of data exhibit
high cross-view correlations. Although intuitive, the effectiveness of correlationbased neural embedding is mostly empirically validated. This work aims to understand latent correlation maximization-based deep multiview learning from a latent
component identification viewpoint. An intuitive generative model of multiview
data is adopted, where the views are different nonlinear mixtures of shared and
private components. Since the shared components are view/distortion-invariant,
representing the data using such components is believed to reveal the identity of
the samples effectively and robustly. Under this model, latent correlation maximization is shown to guarantee the extraction of the shared components across
views (up to certain ambiguities). In addition, it is further shown that the private information in each view can be provably disentangled from the shared using
proper regularization design. A finite sample analysis, which has been rare in nonlinear mixture identifiability study, is also presented. The theoretical results and
newly designed regularization are tested on a series of tasks.

1 INTRODUCTION

One pillar of unsupervised representation learning is multiview learning. Extracting shared information from multiple “views” (e.g., image and audio) of data entities has been considered a major
means to fend against noise and data scarcity. A key computational tool for multiview learning is
_canonical correlation analysis (CCA) (Hotelling, 1936). The classic CCA seeks linear transforma-_
tion matrices such that transformed views are maximally correlated. A number of works studied
nonlinear extensions of CCA; see kernel CCA in (Lai & Fyfe, 2000) and deep learning-based CCA
(DCCA) in (Andrew et al., 2013; Wang et al., 2015). DCCA and its variants were shown to largely
outperform the classical linear CCA in many tasks.

In recent years, a series of self-supervised learning (SSL) paradigms were proposed. These SSL
approaches exhibit a lot of similarities with DCCA approaches, except that the “views” are noisy
data “augmenented” from the original clean data. To be specific, different views are generated by
distorting data—e.g., using rotating, cropping, and/or adding noise to data samples (Dosovitskiy
et al., 2015; Gidaris et al., 2018; Chen et al., 2020; Grill et al., 2020). Then, neural encoders are
employed to map these artificial views to embeddings that are highly correlated across views. This
genre—which will be referred to as artificial multiview SSL (AM-SSL)—includes some empirically
successful frameworks, e.g., BYOL (Grill et al., 2020) and Barlow Twins (Zbontar et al., 2021).

_∗Contact information: Q. Lyu and X. Fu: {lyuqi,xiao.fu}@oregonstate.edu._ W. Wang:
weiranwang@ttic.edu. S. Lu: songtao@ibm.com.


-----

Notably, many DCCA and AM-SSL approaches involve (explicitly or implicitly) searching for
highly correlated representations from multiple views, using neural feature extractors (encoders).
The empirical success of DCCA and AM-SSL bears an important research question: How to under_stand the role of cross-view correlation in deep multiview learning? Furthermore, how to use such_
understanding to design theory-backed learning criteria to serve various purposes?

Intuitively, it makes sense that many DCCA and AM-SSL paradigms involve latent correlation maximization in their loss functions, as such loss functions lead to similar/identical representations
from different views—which identifies view-invariant essential information that is often identityrevealing. However, beyond intuition, theoretical support of latent correlation-based deep multiview
learning had been less studied, until recent works started exploring this direction in both nonlinear
CCA and AM-SSL (see, e.g., (Lyu & Fu, 2020; Von K¨ugelgen et al., 2021; Zimmermann et al.,

2021; Tian et al., 2021; Saunshi et al., 2019; Tosh et al., 2021)), but more insights and theoretical
underpinnings remain to be discovered under more realistic and challenging settings. In this work,
we offer an understanding to the role of latent correlation maximization that is seen in a number of
DCCA and AM-SSL systems from a nonlinear mixture learning viewpoint—and use such understanding to assist various learning tasks, e.g., clustering, cross-view translation, and cross-sample
generation. Our detailed contributions are:

**(i) Understanding Latent Correlation Maximization - Shared Component Identification. We**
start with a concept that has been advocated in many multiview learning works. In particular, the
views are nonlinear mixtures of shared and private latent components; see, e.g., (Huang et al.,
2018; Lee et al., 2018; Wang et al., 2016). The shared components are distortion/view invariant
and identity-revealing. The private components and view-specific nonlinear mixing processes determine the different appearances of the views. By assuming independence between the shared
and private components and invertibility of the data generating process, we show that maximizing
the correlation of latent representations extracted from different views leads to identification of the
ground-truth shared components up to invertible transformations.

**(ii) Imposing Additional Constraints - Private Component Identification. Using the understand-**
ing to latent correlation maximization-type loss functions in DCCA and AM-SSL, we take a step
further. We show that with carefully imposed constraints, the private components in the views can
also be identified, under reasonable assumptions. Learning private components can facilitate tasks
such as cross-view and cross-sample data generation (Huang et al., 2018; Lee et al., 2018).

**(iii) Finite-Sample Analysis. Most existing unsupervised nonlinear mixture identification works,**
e.g., those from the nonlinear independent component analysis (ICA) literature (Hyvarinen &
Morioka, 2016; 2017; Hyvarinen et al., 2019; Khemakhem et al., 2020; Locatello et al., 2020; Gresele et al., 2020), are based on infinite data. This is perhaps because finite sample analysis for
unsupervised learning is generally much more challenging relative to supervised cases—and there
is no existing “universal” analytical tools. In this work, we provide sample complexity analysis for
the proposed unsupervised multiview learning criterion. We come up with a success metric for characterizing the performance of latent component extraction, and integrate generalization analysis and
numerical differentiation to quantify this metric. To our best knowledge, this is the first finite-sample
analysis of nonlinear mixture model-based multiview unsupervised learning.

**(iv) Practical Implementation.** Based on the theoretical understanding, we propose a latent
correlation-maximization based multiview learning criterion for extracting both the shared components and private components. To realize the criterion, a notable innovation is a minimax neural
regularizer that serves for extracting the private components. The regularizer shares the same purpose of some known independence promoters (e.g., Hilbert-Schmidt Independence Criterion (HSIC)
(Gretton et al., 2007)) but is arguably easier to implement using stochastic gradient algorithms.

**Notation. The notations used in this work are summarized in the supplementary material.**

2 BACKGROUND: LATENT CORRELATION IN DCCA AND AM-SSL

In this section, we briefly review some deep multiview learning paradigms that use latent correlation
maximization and its close relatives.


-----

2.1 LATENT CORRELATION MAXIMIZATION IN DCCA

DCCA methods aim at extracting common information from multiple views of data samples. Such
information is expected to be informative and essential in representing the data.

_• DCCA. The objective of DCCA can be summarized as follows (Andrew et al., 2013):_

maximize Tr E **_f_** [(1)][ ]x[(1)][] **_f_** [(2)][ ]x[(2)][][⊤][] _,_ s.t. E **_f_** [(][q][)][ ]x[(][q][)][] **_f_** [(][q][)][ ]x[(][q][)][][⊤][] = I, (1)
**_f_** [(1)],f [(2)]
  

where x[(][q][)] _∈_ R[M][q] _∼Dq is a data sample from view q for q = 1, 2, Dq is the underlying distribution_
of the qth view, f [(1)] : R[M][1] _→_ R[D] and f [(2)] : R[M][2] _→_ R[D] are two neural networks. CCA was found
particularly useful in fending against unknown and strong view-specific (private) interference (see
theoretical supports in (Bach & Jordan, 2005; Ibrahim & Sidiropoulos, 2020)). Such properties were
also observed in DCCA research (Wang et al., 2015), while theoretical analysis is mostly elusive.

An equivalent representation of (1) is as follows


2
minimize **_f_** [(1)](x[(1)]) **_f_** [(2)](x[(2)]) _,_ s.t. E **_f_** [(][q][)][ ]x[(][q][)][] **_f_** [(][q][)][ ]x[(][q][)][][⊤][] = I, (2)
**_f_** [(1)],f [(2)][ E] _−_ 2
  

which is expressed from latent component matching perspective. Both the correlation maximization
form in (1) and the component matching form in (2) are widely used in the literature. As we will
see in our proofs, although the former is popular in the literature (Andrew et al., 2013; Wang et al.,
2015; Chen et al., 2020), the latter is handier for theoretical analysis.

_• Slack Variable-Based DCCA. In (Benton et al., 2017) and (Lyu & Fu, 2020), a deep multiview_
learning criterion is used:

2

2[]

minimize E **_f_** [(][q][)](x[(][q][)]) **_g_** _, s.t. E[_ _gi_ ] = 1, E[gigj] = 0. (3)
**_f_** [(][q][)] _−_ _|_ _|[2]_

_q=1_

X 

The slack variable g represents the common latent embedding learned from the two views. Conceptually, this is also latent correlation maximization (or latent component matching). To see
this, assume that there exists f [(][q][)](x[(][q][)]) = g for all x[(][q][)]. The criterion amounts to learning

[f [(1)](x[(1)])]k = [f [(2)](x[(2)])]k—which has the maximally attainable correlation.

2.2 LATENT CORRELATION MAXIMIZATION/COMPONENT MATCHING IN AM-SSL

Similar to DCCA, the goal of AM-SSL is also to find identity-revealing embeddings of data samples
without using labels. The idea is often realized via intentionally distorting the data to create multiple
artificial views. Then, the encoders are require to produce highly correlated (or closely matched)
embeddings from such views. In AM-SSL, the views x[(1)] and x[(2)] are different augmentations
(e.g., by adding noise, cropping, and rotation) of the sample x.

_• Barlow Twins. The most recent development, namely, the Barlow Twins network (Zbon-_
tar et al., 2021) is appealing since it entails a succinct implementation. Specifically, the Barlow
Twins network aims to learn a single encoder f : R[M] _→_ R[D] for two distorted views. The cost
function is as follows:


_D_ E [f (x[(1)])]i[f (x[(2)])]j

_Cij[2]_ _[,][ where][ C][ij]_ [=]

Xj≠ _i_ E[[f (x[(1)])][2]i []] E[[f (x[(2)])][2]j []]

p q


(1 _Cii)[2]_ + λ
_−_
_i=1_

X


minimize


_i=1_


When the learned embeddings are constrained to have zero mean, i.e., E **_f_** (x[(][q][)]) = 0, Cij is the
cross-correlation between f (x[(1)]) and f (x[(2)]). Note that the normalized representation of cross 
correlation in Cij is equivalent to the objective in (1) with the orthogonality constraints.

_• BYOL. The BYOL method (Grill et al., 2020) uses a cross-view matching criterion that can be_
distilled as follows:


minimize **_f_** (1)(x(1)) **_f_** (2)(x(2))
**_f_** [(1)],f [(2)][ E] _−_



(4)


-----

(q)
where f (·) means that the output of the network is normalized. In BYOL, the networks are
constructed in a special way (e.g., part of f [(2)]’s weights are moving averages of the correspond part
of f [(1)]’s weights). Nonetheless, the cross-view matching perspective is still very similar to that in
latent component matching in (2).

_• SimSiam. The loss function of SimSiam (Chen & He, 2021) has a similar structure as that of_
BYOL, but with a Siamese network, which, essentially, is also latent component matching.

3 UNDERSTANDING LATENT CORRELATION MAXIMIZATION

In this section, we offer understandings to latent correlation maximization (and latent component
matching) from an unsupervised nonlinear multiview mixture identification viewpoint. We will
also show that such understanding can help improve multiview learning criteria to serve different
purposes, e.g., cross-view and cross-sample data generation.

3.1 MULTIVIEW AS NONLINEAR MIXTURES OF PRIVATE AND SHARED COMPONENTS

We consider the following multiview generative model:


**_zℓ_** **_zℓ_**
**_x[(1)]ℓ_** = g[(1)] **_c[(1)]ℓ_** _, x[(2)]ℓ_ = g[(2)] **_c[(2)]ℓ_**
  


_,_ (5)



where x[(]ℓ[q][)] R[M][q] is the ℓth sample of the qth view for q = 1, 2, zℓ R[D] is the shared component
_∈_ _∈_
across views, and c[(]ℓ[q][)] R[D][q] represents the private information of the qth view—which are the ℓth
_∈_
samples of continuous random variables denoted by z ∈ R[D], c[(][q][)] _∈_ R[D][q], respectively. In addition,
**_g[(][q][)](·) : R[D][+][D][q]_** _→_ R[M][q] is an invertible and smooth nonlinear transformation, which is unknown.
Additional notes on (5) and the shared-private component-based modeling idea in the literature can
be found in the supplementary materials (Appendix H). We will use the following assumption:

**Assumption 1 (Group Independence) Under (5), the samples zℓ** _and c[(]ℓ[q][)]_ _are realizations of con-_
_tinuous latent random variables z, c[(][q][)]_ _for q = 1, 2, whose joint distributions satisfy the following:_

**_z_** _p(z), c[(][q][)]_ _p(c[(][q][)]), z_ _, c[(][q][)]_ _q,_ _p(z, c[(1)], c[(2)]) = p(z)p(c[(1)])p(c[(2)]),_ (6)
_∼_ _∼_ _∈Z_ _∈C_

_where Z ⊆_ R[D], Cq ⊆ R[D][q] _are the continuous supports of p(z) and p(c[(][q][)]), respectively._

Assumption 1 is considered reasonable under both AM-SSL and DCCA settings. For AM-SSL,
the private information can be understood as random data augmentation noise-induced components,
and thus it makes sense to assume that such noise is independent with the shared information (which
corresponds to the identity-revealing components of the data sample). In DCCA problems, the private style information can change drastically from view to view (e.g., audio, text, video) without
changing the shared content information (e.g., identity of the entity)—which also shows independence between the two parts. In our analysis, we will assume that D and Dq are known to facilitate
exposition. In practice, these parameters are often selected using a validation set.

**Learning Goals. Our interest lies in extracting zℓ** and c[(]ℓ[q][)] (up to certain ambiguities) from the
views in an unsupervised manner. In particular, we hope to answer under what conditions these
latent components can be identified—and to what extent. As mentioned, zℓ is view/distortioninvariant and thus should be identity-revealing. The ability of extracting it may explain DCCA and
AM-SSL’s effectiveness. In addition, the identification of c[(]ℓ[q][)] and the mixing processes may help
generate data in different views.

3.2 A LATENT CORRELATION-BASED LEARNING CRITERION

Given observations from both views {x[(1)]ℓ _[,][ x][(2)]ℓ_ _[}]ℓ[N]=1_ [generated from (][5][), we aim to understand how]
latent correlation maximization (or latent component matching) helps with our learning goals. To


-----

this end, we consider the following problem criterion:


1 _⊤_

maximize **_fS[(1)]_** **_x[(1)]ℓ_** **_fS[(2)]_** **_x[(2)]ℓ_** (7a)
**_f_** [(1)],f [(2)][ Tr] _N_

_ℓ=1_ !

X    

subject to f [(][q][)] for q = 1, 2 are invertible, (7b)


_N_ _N_

1 _⊤_

_N_ **_fS[(][q][)]_** **_x[(]ℓ[q][)]_** **_fS[(][q][)]_** **_x[(]ℓ[q][)]_** = I, _N[1]_ **_fS[(][q][)]_** **_x[(]ℓ[q][)]_** = 0, q = 1, 2, (7c)

_ℓ=1_ _ℓ=1_

X     X  

**_fS[(][q][)]_** **_x[(]ℓ[q][)]_** **_fP[(][q][)]_** **_x[(]ℓ[q][)]_** _, q = 1, 2,_ (7d)
_⊥⊥_
   

where f [(][q][)] : R[M][q] _→_ R[D][+][D][q] for q = 1, 2 are the feature extractors of view q. We use the notations

**_fS[(][q][)]_** **_x[(]ℓ[q][)]_** = **_f_** [(][q][)][ ]x[(]ℓ[q][)] P **_x[(]ℓ[q][)]_** = **_f_** [(][q][)][ ]x[(]ℓ[q][)] _, q = 1, 2,_

1:D _[,][ f][ (][q][)]_ _D+1:D+Dq_

  h i   h i

to denote the encoder-extracted shared and private components for each view, respectively. Note
that designating the first D dimensions of the encoder outputs to represent the shared information is
without loss of generality, since the permutation ambiguity is intrinsic.

The correlation maximization objective is reminiscent of the criteria of learning paradigms such as
DCCA and Barlow Twins. In addition, under the constraints in (7), the objective function is also
equivalent to shared component matching that is similar to those used by BYOL and SimSiam, i.e.,


1 _⊤_ 1

max **_fS[(1)]_** **_x[(1)]ℓ_** **_fS[(2)]_** **_x[(2)]ℓ_** min **_fS[(1)]_** **_x[(1)]ℓ_** **_fS[(2)]_** **_x[(2)]ℓ_**
**_f_** [(][q][)][ Tr] _N_ _ℓ=1_ ! _⇐⇒_ **_f_** [(][q][)] _N_ _ℓ=1_ _−_ 2 _[.]_

X     X    

To explain the criterion, note that we have a couple of goals that we hope to achieve with fS[(][q][)] and
**_fP[(][q][)][. First, the objective function aims to maximize the latent correlation of the learned shared]_**
components, i.e., fS[(][q][)][. This is similar to those in DCCA and AM-SSL, and is based on the belief]
that the shared information should be identical across views. Second, in (7d), we ask fS[(][q][)] and fP[(][q][)]
to be statistically independent for q = 1, 2. This promotes the disentanglement of the shared and
private parts of each encoder, following in Assumption 1. Third, the invertibility constraint in (7b)
is to ensure that the latent and the ambient data can be constructed from each other—which is often
important in unsupervised learning, for avoiding trivial solutions; see, e.g., (Hyvarinen et al., 2019;
Von K¨ugelgen et al., 2021). The orthogonality and zero-mean constraints in (7c) are used to make
the correlation metric meaningful. In particular, if the learned components are not zero-mean, the
learned embeddings may not capture “co-variations” but dominated by some constant terms.

3.3 THEORETICAL UNDERSTANDING

We have the following theorem in terms of learning the shared components:


**Theorem 1 (Shared Component Extraction) Under the generative model in (5) and Assumption 1,**
_consider the population form in (7) (i.e., N =_ _). Assume that the considered constraints hold over_
_∞_ (q)
_all x[(][q][)]_ _∈Xq for q = 1, 2, where Xq = {x[(][q][)]|x[(][q][)]_ = g[(][q][)]([z[⊤], (c[(][q][)])⊤]⊤), ∀z ∈Z, ∀c _∈Cq}._
_Denote_ **_f_** [(][q][)] _as any solution of (7). Also assume that the first-order derivative of_ **_f_** [(][q][)] _◦_ **_g[(][q][)]_** _exists._
_Then, we have_ **_z =_** **_fS[(][q][)]_** **_x[(][q][)][]_** = γ(z) no matter if (7d) is enforced or not, where γ( ) : R[D] R[D]

_·_ _→_
_is an unknown invertible function.[b]_ [b]
 
b [b]

A remark is that **_z = γ(z) has all the information of z due to the invertibility of γ(·). Theo-_**
rem 1 clearly indicates that latent correlation maximization/latent component matching can identify
the view/distortion-invariant information contained in multiple views under unknown nonlinear disb
tortions. This result may explain the reason why many DCCA and AM-SSL schemes use latent
correlation maximization/latent component matching as part of their objectives. Theorem 1 also indicates that if one only aims to extract z, the constraint in (7d) is not needed. In the next theorem, we
show that our designed constraint in (7d) can help disentangle the shared and private components:


-----

**Theorem 2 (Private Component Extraction) Under the same conditions as in Theorem 1, also**
_assume that (7d) is enforced. Then, we further have_ **_c[(][q][)]_** = **_fP[(][q][)]_** **_x[(][q][)][]_** = δ[(][q][)][  ]c[(][q][)][], where
**_δ[(][q][)](_** ) : R[D][q] R[D][q] _is an unknown invertible function._

_·_ _→_  
b [b]

Note that separating z and c[(][q][)] may be used for other tasks such as cross-view translation (Huang
et al., 2018; Lee et al., 2018) and content/style disentanglement.

The above theorems are based on the so-called population case (with N = ∞ and the Xq observed).
This is similar to the vast majority of provable nonlinear ICA/factor disentanglement literature; see
(Hyvarinen & Morioka, 2016; Hyvarinen et al., 2019; Locatello et al., 2020; Khemakhem et al.,
2020). It is of interest to study the finite sample case. In addition, most of these works assumed that
the learning function f [(][q][)] is a universal function approximator. In practice, considering f [(][q][)] _∈F,_
where F is a certain restricted function class that may have mismatches with g[(][q][)]’s function class,
is meaningful. To proceed, we assume D1 = D2 and M = M1 = M2 for notation simplicity and:

**Assumption 2 Assume the following conditions hold:**

_(a) We have g[(][q][)]_ _∈G and learn f_ [(][q][)] _from F, where the function classes F and G are third-order_
_differentiable and bounded._

_(b) The Rademacher complexity (Bartlett & Mendelson, 2002) of F_ _[′]_ = {fd : R[M] _→_ R|fd(x) =

[f (x)]d, f ∈F} is bounded by RN given N samples.

_(c) Define G[−][1]_ = **_u : R[M]_** _→_ R[D][+][D][1] _|uS(x) = γ(z), uS(x) = [u(x)]1:D_ _∀x ∈Xq and any_
_invertible γ(·). There exists_ **_f ∈F such that supx∈Xq ∥fS(x) −_** **_uS(x)∥2 ≤_** _ν._

_(d) Any third-order partial derivative of [h[(][q][)](x)]d = [f_ [(][q][)] _◦_ **_g[(][q][)](x)]d resides in [−Cd, Cd] for all_**
**_x ∈Xq. In addition, [c[(][q][)]]j ∈_** [−Cp, Cp] with 0 < Cp < ∞ _for j ∈_ [Dq].

Assumption 2 specifies some conditions of the function class F where the learning functions are
chosen from. Specifically, (a) and (d) mean that the learning function is sufficiently smooth (i.e.,
with bounded third-order derivatives); (b) means that the learning function is not overly complex
(i.e., with a bounded Rademacher complexity); and (c) means that the learning function should be
expressive enough to approximate the inverse of the generative function .

**Theorem 3 (Sample Complexity) Under the generative model in (5), Assumption 1 and the suite**
_of conditions in Assumption 2, assume that (x[(1)]ℓ_ _[,][ x][(2)]ℓ_ [)][ for][ ℓ] [= 1][, . . ., N][ are i.i.d. samples of]
(x[(1)], x[(2)]). Denote **_f_** [(][q][)] _as any solution of (7) with the invertibility constraint satisfied. Then, we_
_have the following holds with probability of at least 1 −_ _δ:_

_D_ _Dq[b]_

2

E _∂[fS(x[(][q][)])]i/∂c[(]j[q][)]_ = O _DRN +_ log(1/δ)/N + ν[2][][2][/][3][] (8)

 

_i=1_ _j=1_

X X   [b]   p
 

_for any c[(][q][)]_ _q such that_ _Cp + κj_ _c[(]j[q][)]_ _Cp_ _κj for all j_ [Dq] and all i [D], where
_∈C_ _−_ _≤_ _≤_ _−_ _∈_ _∈_
_κj = Ω(([3]/Cd)[1][/][3](4Cf_ (2DRN + Cf log(1/δ)/2N) + 4ν[2])[1][/][6]).
p

If the metric on the left hand side of (8) is zero, then **_fS_** **_x[(][q][)][]_** is disentangled from c[(][q][)]. The
theorem indicates that with N samples, the metric is bounded by  _O(N_ _[−][1][/][3]). In addition, RN de-_
creases when N increases; e.g., a fully connected neural network with bounded weights satisfies[b]
RN = O(N _[−][1][/][2]) (Shalev-Shwartz & Ben-David, 2014). When RN increases (e.g., by using a more_
complex neural network), the function mismatch ν often decreases (since F can be more expressive
with a higher RN ). In other words, Theorem 3 indicates a tradeoff between the expressiveness of
the function class F and the sample complexity. If F comprises neural networks, the expressiveness is increased (or equivalently, the modeling error is reduced) by increasing the width or depth
of networks. But this in turn increases RN and requires more samples to reduce (8). This makes
sense—one hopes to use a sufficiently expressive learning function, but does not hope to use an
excessively expressive one, which is similar to the case in supervised learning.


-----

4 IMPLEMENTATION

**Enforcing Group Statistical Independence. A notable challenge is the statistical independence**
constraint in (7d), whose enforcement is often an art. Early methods such as (Taleb & Jutten, 1999;
Hyv¨arinen & Oja, 2000) may be costly. The HSIC method in (Gretton et al., 2007) which measures
the correlation of two variables in a kernel space can be used in our framework, but kernels sometimes induce large memory overheads and are sensitive to parameter (e.g., kernel width) selection.

In this work, we provide a simple alternative. Note that if two variables X and Y are statistically
independent, then we have p(X, Y ) = p(X)p(Y ) ⇐⇒ E[ϕ(X)τ (Y )] = E[ϕ(X)]E[τ (Y )] for all
measurable functions ϕ(·) : R → R and τ (·) : R → R (Gretton et al., 2005). Hence, to enforce group
independence between variables **_z[(][q][)]_** and **_c[(][q][)], we propose to exhaust the space of all measurable_**
functions ϕ[(][q][)] : R[D] _→_ R and τ [(][q][)] : R[D][q] _→_ R, such that

**_ϕ[(][q]sup[)],τ_** [(][q][)][ R][(][q][)][ =] **_ϕ[(][q]sup[)],τ_** [(][q] b[)] _|[Cov][[ϕ][(][q][)] b(z[b][(][q][)]),τ_ [(][q][)](c[(][q][)])]|/qV[ϕ[(][q][)](z[(][q][)])]qV[τ [(][q][)](c[(][q][)])] (9)
b b b

is minimized. It is not hard to show the following (see the proof in the supplementary material):

**Proposition 1 In (9), if supϕ(q),τ (q)** = 0 over all measurable functions ϕ[(][q][)] _and τ_ [(][q][)], then,
_R[(][q][)]_
_any_ **_z[(][q][)][]_** **_c[(][q][)][]_**

_i_ _[and]_ _j_ _[for][ i][ ∈]_ [[][D][]][ and][ j][ ∈] [[][D][q][]][ are statistically independent.]

In practice, we use two neural networks to representb b **_ϕ[(][q][)]_** and τ [(][q][)], respectively, which blends well
with the neural encoders for algorithm design.

**Reformulation and Optimization. We use deep neural networks to serve as f** [(][q][)]. We introduce a
slack variable(3). The slack variable can also make orthogonality and zero-mean constraints easier to enforce. A uℓ and change the objective to minimizing Lℓ = _q=1_ _[∥][u][ℓ]_ _[−]_ **_[f][ (]S[q][)][(][x]ℓ[(][q][)][)][∥]2[2]_** [like in]
reconstruction loss Vℓ = _q=1_ _[∥][x]ℓ[(][q][)]_ _−_ **_r[(][q][)](f_** [(][q][)](x[(]ℓ[q][)][))][∥]2[2] [is employed to promote invertibility of][P][2]
**_f_** [(][q][)], where r[(][q][)] is a reconstruction network. Let θ collect the parameters of f [(][q][)] and r[(][q][)], and η
the parameters of ϕ[(][q][)], τ [(][P][q][)]. The overall formulation is:[2]


min + β + λ _,_ s.t. [1]
**_U_** _,θ_ [max]η _L_ _V_ _R_ _N_


_N_

**_uℓu[⊤]ℓ_** [=][ I][,][ 1]

_N_

_ℓ=1_

X


**_uℓ_** = 0, (10)

_ℓ=1_

X


where = [1]/N _ℓ=1_ _ℓ=1_ _q=1_
and β, λ L ≥ 0. We design an algorithm for handling ([L][ℓ][,][ V][ =][ 1][/][N][ P][N] _[V][ℓ][,][ U][ = [][u]10[1][,]) with scalable updates; see the supple-[ · · ·][,][ u][N]_ []][ ∈] [R][D][×][N] [,][ R][ =][ P][2] _[R][(][q][)][,]_
mentary materials for detailed implementation and complexity analysis. We should mention that

[P][N]
using reconstruction to encourage invertibility is often seen in multiview learning; see, e.g., (Wang
et al., 2015). Nonetheless, this needs not to be the only invertibility-encouraging method. Other
realizations such as flow-based (e.g., (Kingma & Dhariwal, 2018)) and entropy regularization-based
approaches (e.g., (Von K¨ugelgen et al., 2021)) are also viable—see more in Appendix I.

5 RELATED WORK - NONLINEAR ICA AND LATENT DISENTANGLEMENT

Other than the DCCA and AM-SSL works, our design for private and shared information separation
also draws insights from two topics in unsupervised representation learning, namely, nonlinear ICA
(nICA) (Hyvarinen & Morioka, 2016; 2017; Hyvarinen et al., 2019; Khemakhem et al., 2020) and
latent factor disentanglement (Higgins et al., 2017; Kim & Mnih, 2018; Chen et al., 2018; Zhao
et al., 2019; Lopez et al., 2018), which are recently offered a unifying perspective in (Khemakhem
et al., 2020). The nICA works aim to separate nonlinearly mixed latent components to an individual
component level, which is in general impossible unless additional information associated with each
sample (e.g., time frame labels (Hyvarinen & Morioka, 2016) and class labels (Hyvarinen et al.,
2019; Khemakhem et al., 2020)) is used. Multiple views are less studied in the context of nICA,
with the recent exception in (Locatello et al., 2020) and (Gresele et al., 2020). Nonetheless, their
models are different from ours and the approaches cannot extract the private information from views
with different nonlinear models. The concurrent work in (Von K¨ugelgen et al., 2021) worked on


-----

Figure 1: t-SNE of the results on multiview MNIST from (Wang et al., 2015). Baselines: DCCA
(Wang et al., 2015), Barlow Twins (Zbontar et al., 2021) and BYOL (Grill et al., 2020).

content-style disentanglement under data augmented SSL settings and considered a similar generative model where both shared and private components are explicitly used. A key difference is that
their model uses an identical nonlinear generative function across the views (i.e., g[(1)] = g[(2)]), while
we consider two possibly different g[(][q][)]’s. In addition, our learning criterion is able to extract the
private information, while the work in (Von K¨ugelgen et al., 2021) did not consider this aspect.
None of the aforementioned works offered finite sample analysis. Our independence promoter is
reminiscent of (Gretton et al., 2005), with the extension to handle group variables.

6 EXPERIMENTS

**Synthetic Data. We first use synthetic data for theory validation; see the supplementary materials.**

6.1 VALIDATING THEOREM 1 - SHARED COMPONENT LEARNING

In this subsection, we show that latent correlation maximization (or latent component matching)
leads to shared component extraction under the model in (5)—which can be used to explain the
effectiveness of a number of DCCA and AM-SSL formulations. This is also the objective of our
formulation (7) if the constraint in (7d) is not enforced.

**Multiview MNIST Data. For proof-of-concept, we adopt a multiview MNIST dataset that was**
used in (Wang et al., 2015). There, the “augmented” view of MNIST contains randomly rotated
digits and the other with additive Gaussian white noise (Wang et al., 2015); see Fig. 1. This is
similar to the data augmentation ideas in AM-SSL. Using this multiview data, we apply different
multiview learning paradigms that match the latent representations (or maximize the correlations
of learned representations) across views. The dataset has 70,000 samples that are 28×28 images
of handwritten digits. Note that without (7d), our method can be understood as a slight variant of
(3). To benchmark our method, we use a number of DCCA and AM-SSL approaches mentioned in
Sec. 2, namely, DCCA (Wang et al., 2015), Barlow Twins (Zbontar et al., 2021) and BYOL (Grill
et al., 2020). We set D = 10, D1 = 20 and D2 = 50 through a validation set. The detailed settings
of our neural networks can be found in the supplementary material.

Since we do not have ground-truth to evaluate the effectiveness of shared information extraction,
we follow the evaluation method in (Wang et al., 2015) and apply k-means to all the embeddings
**_zℓ[(1)]_** = fS[(1)][(][x]ℓ[(1)][)][ and compute the clustering accuracy on the test set (see the visualization of][ b]zℓ[(2)]
in the supplementary materials). In Fig. 1, we show the t-SNE (Van der Maaten & Hinton, 2008)
visualizations ofb **_zℓ[(1)][’s on a test set of 10,000 samples together with the clustering accuracy. All]_**
results are averaged over 5 random initializations.

By Theorem 1, all the methods under test should output identity-revealing representations of the b
data samples. The reason is that the two views share the same identity information of a sample.
Indeed, from Fig. 1, one can see that all latent correlation-maximization-based DCCA and AM-SSL
methods learn informative representations that are sufficiently distinguishable. The “shape” of the
clusters are different, which can be explained by the existence of the invertible function γ(·). These
results corroborate our analysis in Theorem 1.

**CIFAR10 Data. We also observe similar results using the CIFAR10 data (Krizhevsky et al., 2009).**
The results can be found in the supplementary materials, due to page limitations.


-----

ables z and the azimuths that are captured by c[(][q][)], the generation mappings g[(1)] and g[(2)] produce

Figure 2: Evaluation on Cars3D; rows in blue boxes are w/ R; rows in green boxes are w/o R.

6.2 VALIDATING THEOREM 2 - SHARED AND PRIVATE COMPONENT DISENTANGLEMENT

We use data generation examples to support our claim in Theorem 2—i.e., with our designed regularizer R in (10), one can provably disentangle the shared and private latent components.

**Cars3D Data for Cross-sample Data Generation. We use the Cars3D dataset (Reed et al., 2015)**
that contains different car CAD models. For each car image, there are three defining aspects (namely,
‘type’, ‘elevation’ and ‘azimuth’).

We create two views as follows. We assume that given the car type that is captured by shared vari
car images with low elevations and high elevations, respectively (so they must be different mappings). Under our setting, each view has N = 8, 784 car images. We model z with D = 10. For
**_c[(][q][)], we set D1 = D2 = 2. More details about our settings are in the supplementary material._**

We use the idea of cross-sample data generation to evaluate the effectiveness of our method. To
be precise, we evaluate the learned **_f_** [(][q][)]’s by combining **_zℓ[(][q][)]_** = **_fS[(][q][)][(][x]ℓ[(][q][)][)][ and][ b]c[(]j[q][)]_** = **_fP[(][q][)][(][x]j[(][q][)][)]_**

and generating **_x[(]ℓ,j[q][)]_** [=][ b]r[(][q][)]([(zℓ[(][q][)][)]⊤, (c[(]j[q][)][)]⊤]⊤), where **_r[(][q][)]_** is the learned reconstruction network [cf
Eq. (10)]. Under our model, if the shared components and private components are truly disentangled[b] b [b] [b]
_in the latent domain, this generated sample b_ b **_x[(]ℓ,j[q][)]_** _[should exhibit the same ‘type’ and ‘elevation’ as] b_

_those of x[(]ℓ[q][)]_ _and the ‘azimuth’ of x[(]j[q][)][.]_

b

Fig. 2 shows our experiment results. The proposed method’s outputs are as expected. For example,
on the left of Fig. 2, the zℓ[(1)] is extracted from the red convertible, and the **_c[(1)]j_** [’s are extracted from]

the top row. One can see that the generated **_x[(1)]ℓ,j_** [’s under the proposed method with][ R][ are all red]
convertibles with the same elevation, but using the azimuths of the corresponding cars from the top b
row. Note that if is not used, then the learned b **_c[(]j[q][)]_** may still contain the ‘type’ or ‘elevation’
_R_
information; see the example highlighted with yellow background on the right of Fig. 2. More
results are in the supplementary material.
b

**dSprites Data and MNIST Data for Cross-sample/Cross-view Data Generation. We offer two**
extra sets of examples to validate our claim in Theorem 2. Please see the supplementary materials.

7 CONCLUSION

In this work, we provided theoretical understandings to the role of latent correlation maximization
(latent component matching) that is often used in DCCA and AM-SSL methods from an unsupervised nonlinear mixture learning viewpoint. In particular, we modeled multiview data as nonlinear
mixtures of shared and private components, and showed that latent correlation maximization ensures to extract the shared components—which are believed to be identity-revealing. In addition, we
showed that, with a carefully designed constraint (which is approximated by a neural regularizer),
one can further disentangle the shared and private information, under reasonable conditions. We also
analyzed the sample complexity for extracting the shared information, which has not been addressed
in nonlinear component analysis works, to our best knowledge. To realize our learning criterion, we
proposed a slack variable-assisted latent correlation maximization approach, with a novel minimax
neural regularizer for promoting group independence. We tested our method over synthetic and real
data. The results corroborated our design goals and theoretical analyses.


-----

**Acknowledgement. This work is supported in part by the National Science Foundation (NSF) under**
Project NSF ECCS-1808159, and in part by the Army Research Office (ARO) under Project ARO
W911NF-21-1-0227.

**Ethics Statement. This paper focuses on theoretical analysis and provable guarantees of learning**
criteria. It does not involve human subjects or other ethics-related concerns.

**Reproducibility Statement. The authors strive to make the research in this work reproducible.**
The supplementary materials contain rich details of the algorithm implementation and experiment
settings. The source code of our Python-implemented algorithm and two demos with real data are
uploaded as supplementary materials. The details of the proofs of our theoretical claims are also
included in the supplementary materials.

REFERENCES

Galen Andrew, Raman Arora, Jeff Bilmes, and Karen Livescu. Deep canonical correlation analysis.
In International Conference on Machine Learning, pp. 1247–1255. PMLR, 2013.

Raman Arora and Karen Livescu. Multi-view cca-based acoustic features for phonetic recognition
across speakers and domains. In 2013 IEEE International Conference on Acoustics, Speech and
_Signal Processing, pp. 7135–7139. IEEE, 2013._

Francis R Bach and Michael I Jordan. A probabilistic interpretation of canonical correlation analysis.
2005.

Peter L Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463–482, 2002.

Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and Devon Hjelm. MINE: Mutual information neural estimation. In International
_Conference on Machine Learning, volume 80, pp. 531–540. PMLR, 10–15 Jul 2018._

Adrian Benton, Huda Khayrallah, Biman Gujral, Drew Reisinger, Shenmin Zhang, and Raman
Arora. Deep generalized canonical correlation analysis. In RepL4NLP at ACL, 2017.

J Douglas Carroll. Generalization of canonical correlation analysis to three or more sets of variables. In Proceedings of the 76th annual convention of the American Psychological Association,
volume 3, pp. 227–228, 1968.

Ricky TQ Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of disentanglement in variational autoencoders. In Advances in Neural Information Processing Systems,
pp. 2610–2620, 2018.

Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International Conference on Machine Learning,
pp. 1597–1607. PMLR, 2020.

Xinlei Chen and Kaiming He. Exploring simple Siamese representation learning. In Proceedings of
_Conference on Computer Vision and Pattern Recognition, pp. 15750–15758, 2021._

Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999.

G Darmois. Analyse des liaisons de probabilit´e. In Proc. Int. Stat. Conferences 1947, pp. 231, 1951.

Richard A Davis, Keh-Shin Lii, and Dimitris N Politis. Remarks on some nonparametric estimates
of a density function. In Selected Works of Murray Rosenblatt, pp. 95–100. Springer, 2011.

Paramveer Dhillon, Jordan Rodu, Dean Foster, and Lyle Ungar. Two Step CCA: A new spectral method for estimating vector models of words. In Proceedings of the 29th International
_Conference on Machine Learning (ICML-12), pp. 1551–1558, New York, NY, USA, July 2012._
Omnipress.

Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springenberg, Martin Riedmiller, and Thomas
Brox. Discriminative unsupervised feature learning with exemplar convolutional neural networks.
_IEEE Trans. Pattern Anal. Mach. Intell., 38(9):1734–1747, 2015._


-----

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011.

Ronald A Fisher. Frequency distribution of the values of the correlation coefficient in samples from
an indefinitely large population. Biometrika, 10(4):507–521, 1915.

Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by
predicting image rotations. In International Conference on Learning Representations, 2018.

Yunchao Gong, Qifa Ke, Michael Isard, and Svetlana Lazebnik. A multi-view embedding space for
modeling internet images, tags, and their semantics. International Journal of Computer Vision,
106(2):210–233, 2014.

Luigi Gresele, Paul K Rubenstein, Arash Mehrjou, Francesco Locatello, and Bernhard Sch¨olkopf.
The incomplete Rosetta stone problem: Identifiability results for multi-view nonlinear ICA. In
_Proceedings of UAI 2020, pp. 217–227, 2020._

Arthur Gretton, Alexander J Smola, Olivier Bousquet, Ralf Herbrich, Andrei Belitski, Mark Augath,
Yusuke Murayama, Jon Pauls, Bernhard Sch¨olkopf, and Nikos K Logothetis. Kernel constrained
covariance for dependence measurement. In International Conference on Artificial Intelligence
_and Statistics, volume 10, pp. 112–119. Citeseer, 2005._

Arthur Gretton, Kenji Fukumizu, Choon Teo, Le Song, Bernhard Sch¨olkopf, and Alex Smola. A
kernel statistical test of independence. Advances in Neural Information Processing Systems, 20,
2007.

Jean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin Tallec, Pierre Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,
Bilal Piot, Koray Kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent - a
new approach to self-supervised learning. In Advances in Neural Information Processing Systems,
volume 33, pp. 21271–21284, 2020.

Gregory Gundersen, Bianca Dumitrascu, Jordan T Ash, and Barbara E Engelhardt. End-to-end
training of deep probabilistic CCA on paired biomedical observations. In Uncertainty in Artificial
_Intelligence, 2019._

Michael Gutmann and Aapo Hyv¨arinen. Noise-contrastive estimation: A new estimation principle
for unnormalized statistical models. In Proceedings of the 13th International Conference on
_Artificial Intelligence and Statistics, pp. 297–304. JMLR Workshop and Conference Proceedings,_
2010.

Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. Beta-VAE: Learning basic visual concepts with a
constrained variational framework. In International Conference on Learning Representations,
2017.

Harold Hotelling. Relations between two sets of variates. Biometrika, 28(3/4):321–377, 1936.

Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-toimage translation. In Proceedings of the European Conference on Computer Vision, pp. 172–189,
2018.

Aapo Hyvarinen and Hiroshi Morioka. Unsupervised feature extraction by time-contrastive learning
and nonlinear ICA. In Advances in Neural Information Processing Systems, volume 29, 2016.

Aapo Hyvarinen and Hiroshi Morioka. Nonlinear ICA of temporally dependent stationary sources.
In International Conference on Artificial Intelligence and Statistics, volume 54, pp. 460–469,
20–22 Apr 2017.

Aapo Hyv¨arinen and Erkki Oja. Independent component analysis: Algorithms and applications.
_Neural Networks, 13(4-5):411–430, 2000._


-----

Aapo Hyvarinen, Hiroaki Sasaki, and Richard Turner. Nonlinear ICA using auxiliary variables
and generalized contrastive learning. In International Conference on Artificial Intelligence and
_Statistics, pp. 859–868, 2019._

Mohamed Salah Ibrahim and Nicholas D Sidiropoulos. Reliable detection of unknown cell-edge
users via canonical correlation analysis. IEEE Trans. Wireless Commun., 19(6):4170–4182, 2020.

Jon R Kettenring. Canonical analysis of several sets of variables. Biometrika, 58(3):433–451, 1971.

Ilyes Khemakhem, Diederik Kingma, Ricardo Monti, and Aapo Hyvarinen. Variational autoencoders and nonlinear ICA: A unifying framework. In International Conference on Artificial In_telligence and Statistics, pp. 2207–2217. PMLR, 2020._

Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In International Conference on
_Machine Learning, volume 80, pp. 2649–2658. PMLR, 10–15 Jul 2018._

Diederick P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
_Conference on Learning Representations, 2015._

Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions.
_Advances in Neural Information Processing Systems, 31, 2018._

Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.

Pei Ling Lai and Colin Fyfe. Kernel and nonlinear canonical correlation analysis. International
_Journal of Neural Systems, 10(05):365–377, 2000._

Christian Ledig, Lucas Theis, Ferenc Husz´ar, Jose Caballero, Andrew Cunningham, Alejandro
Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolution using a generative adversarial network. In Proceedings of the IEEE
_Conference on Computer Vision and Pattern Recognition, pp. 4681–4690, 2017._

Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Singh, and Ming-Hsuan Yang. Diverse
image-to-image translation via disentangled representations. In Proceedings of the European
_Conference on Computer Vision, pp. 35–51, 2018._

Francesco Locatello, Ben Poole, Gunnar R¨atsch, Bernhard Sch¨olkopf, Olivier Bachem, and Michael
Tschannen. Weakly-supervised disentanglement without compromises. In International Confer_ence on Machine Learning, pp. 6348–6359. PMLR, 2020._

Romain Lopez, Jeffrey Regier, Michael I Jordan, and Nir Yosef. Information constraints on autoencoding variational Bayes. Advances in Neural Information Processing Systems, 31:6114–6125,
2018.

Qi Lyu and Xiao Fu. Nonlinear multiview analysis: Identifiability and neural network-assisted
implementation. IEEE Trans. Signal Process., 68:2697–2712, 2020.

Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning.
MIT press, 2018.

Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.

Pushpendre Rastogi, Benjamin Van Durme, and Raman Arora. Multiview LSA: Representation
learning via generalized CCA. In Proceedings of the 2015 conference of the North American
_chapter of the Association for Computational Linguistics: Human Language Technologies, pp._
556–566, 2015.

Scott E Reed, Yi Zhang, Yuting Zhang, and Honglak Lee. Deep visual analogy-making. Advances
_in Neural Information Processing Systems, 28:1252–1260, 2015._


-----

Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar.
A theoretical analysis of contrastive unsupervised representation learning. In Proceedings of
_the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine_
_Learning Research, pp. 5628–5637. PMLR, 09–15 Jun 2019._

Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo_rithms. Cambridge university press, 2014._

Richard Socher and Li Fei-Fei. Connecting modalities: Semi-supervised segmentation and annotation of images using unaligned text corpora. In 2010 IEEE Computer Society Conference on
_Computer Vision and Pattern Recognition, pp. 966–973. IEEE, 2010._

Anisse Taleb and Christian Jutten. Source separation in post-nonlinear mixtures. IEEE Trans. Signal
_Process., 47(10):2807–2820, 1999._

Yuandong Tian, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning dynamics without contrastive pairs. In Proceedings of the 38th International Conference on Machine
_Learning, volume 139 of Proceedings of Machine Learning Research, pp. 10268–10278. PMLR,_
18–24 Jul 2021.

Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive learning, multi-view redundancy, and linear models. In Algorithmic Learning Theory, pp. 1179–1206. PMLR, 2021.

Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine
_Learning Research, 9(11), 2008._

Julius Von K¨ugelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard Sch¨olkopf, Michel
Besserve, and Francesco Locatello. Self-supervised learning with data augmentations provably
isolates content from style. Advances in Neural Information Processing Systems, 34, 2021.

Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Machine Learning, pp.
9929–9939. PMLR, 2020.

Weiran Wang, Raman Arora, Karen Livescu, and Jeff Bilmes. On deep multi-view representation
learning. In International Conference on Machine Learning, pp. 1083–1092, 2015.

Weiran Wang, Xinchen Yan, Honglak Lee, and Karen Livescu. Deep variational canonical correlation analysis. arXiv preprint arXiv:1610.03454, 2016.

Ka Yee Yeung and Walter L Ruzzo. Details of the Adjusted Rand Index and clustering algorithms,
supplement to the paper an empirical study on Principal Component Analysis for clustering gene
expression data. Bioinformatics, 17(9):763–774, 2001.

Jure Zbontar, Li Jing, Ishan Misra, Yann Lecun, and Stephane Deny. Barlow Twins: Self-supervised
learning via redundancy reduction. In Proceedings of the 38th International Conference on Ma_chine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 12310–12320._
PMLR, 18–24 Jul 2021.

Shengjia Zhao, Jiaming Song, and Stefano Ermon. InfoVAE: Balancing learning and inference in
variational autoencoders. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):
5885–5892, Jul. 2019.

Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation
using cycle-consistent adversarial networks. In Proceedings of the IEEE International Conference
_on Computer Vision, pp. 2223–2232, 2017._

Roland S. Zimmermann, Yash Sharma, Steffen Schneider, Matthias Bethge, and Wieland Brendel.
Contrastive learning inverts the data generating process. In Proceedings of the 38th International
_Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research,_
pp. 12979–12990. PMLR, 18–24 Jul 2021.


-----

**Supplementary Materials**

A NOTATION

The notations used in this work are summarized in Table A.1.

|Col1|Table A.1: Definition of notations.|
|---|---|
|Notation|Definition|
|x, x, X|scalar, vector, and matrix|
|[x], x i i|both represent the ith element of vector x|
|[X] ij|the ith row, jth column of matrix X|
|p(x)|probability density function of random variable x|
|x y ⊥⊥|x and y are statistically independent, i.e., p(x, y) = p(x)p(y)|
|x y ⊥⊥|x ⊥⊥y for all i, j i j|
|x⊤, X⊤|transpose of x, X|
|J f|Jocobian matrix of a vector-valued function f|
|I|identity matrix with a proper size|
|f g ◦|function composition operation|
|detX|determinant of a square matrix X|
|E[ ·]|expectation|
|V[ ·]|variance|
|Cov[ ·, ·]|covariance|
|[N]|the integer set 1, 2, . . ., N { }|



B PROOF OF THEOREM 1

**Theorem 1. (Shared Component Extraction) Under the generative model in (5) and As-**
sumption 1, consider the population form in (7) (i.e., N = ∞). Assume that the considered constraints hold over allg[(][q][)]([z[⊤], (c[(][q][)])⊤]⊤), **_z_** _,_ **_c(q x)_** [(][q][)] _q∈X. Denoteq for qf = 1[(][q][)]_ as any, 2, where solution of ( Xq =7 {). Also assumex[(][q][)]|x[(][q][)] =
_∀_ _∈Z_ _∀_ _∈C_ _}_
that the first-order derivative of **_f_** [(][q][)] **_g[(][q][)]_** exists. Then, we have **_z =_** **_fS[(][q][)]_** **_x[(][q][)][]_** = γ(z) no
_◦_
matter if (7d) is enforced or not, where γ( ) : R[D] bR[D] is a certain invertible functions.

_·_ _→_  

[b] b [b]

We consider the formulation in (7) without (7d). When N = ∞ and x[(][q][)] _∼Xq for q = 1, 2 are_
all available, the sample average version of the formulation in (7) becomes the following expected
value version:


_⊤[]_
maximize E **_fS[(1)]_** **_x[(1)]ℓ_** **_fS[(2)]_** **_x[(2)]ℓ_** (B.1a)
**_f_** [(1)],f [(2)][ Tr]
    []  

subject to f [(][q][)] for q = 1, 2 are invertible, (B.1b)

_⊤[]_
E **_fS[(][q][)]_** **_x[(]ℓ[q][)]_** **_fS[(][q][)]_** **_x[(]ℓ[q][)]_** = I, E **_fS_** **_x[(]ℓ[q][)]_** = 0, q = 1, 2 (B.1c)
     h  i

First, note that under the generative model in (5), the maximum of the objective function in (7) is
_D, which is obtained when every corresponding components of the learned solutions, i.e.,_ **_fS[(1)]_** :
R[M][1] R[D] and **_fS[(1)]_** : R[M][2] R[D], are perfectly correlated, i.e.,
_→_ _→_

[b]

[b] **_fS[(1)]_** **_x[(1)][]_** = **_fS[(2)]_** **_x[(2)][]_** _._ (B.2)

 
b [b]


-----

Indeed, one may rewrite (B.1a) as

_⊤[]_
arg min E **_fS[(1)]_** **_x[(1)]ℓ_** **_fS[(2)]_** **_x[(2)]ℓ_**
**_f_** [(1)],f [(2)][ −] [2Tr]
    []  

_⊤[]_
= arg min E **_fS[(1)]_** **_x[(1)]ℓ_** **_fS[(2)]_** **_x[(2)]ℓ_** + I
**_f_** [(1)],f [(2)][ I][ −] [2Tr]
    [] 2 

= arg min **_fS[(1)]_** **_x[(1)]ℓ_** **_fS[(2)]_** **_x[(2)]ℓ_** (B.3)
**_f_** [(1)],f [(2)][ E] _−_ 2
     

where the second equality holds because the constraint in (B.1c). Note that the criterion in (B.3)
admits the optimal solution in (B.2) under our generative model.


Note that one solution to attain zero cost of (B.3) is

**_fS[(][q][)]_** **_x[(][q][)][]_** = z, q = 1, 2.

b

However, the question lies in “uniqueness”, i.e., can enforcing (B.2) always yield fS[(][q][)] **_x[(][q][)][]_** = z
(up to certain ambiguities)? This is central to learning criterion design, as the expressiveness of
 
function approximators like neural networks may attain zero cost of (B.3) with undesired solutions.

Assume that a solution that satisfies (B.2) is found. Denote **_f_** [(][q][)] for q = 1, 2 as the solution.
Combine the solution with the generative model in (5). Then, following equality can be obtained:

[b]

**_z_** **_z_**

**_h[(1)]S_** **_c[(1)]_** = h[(2)]S **_c[(2)]_** _,_ (B.4)

   


where we have

in which


**_h[(]S[q][)][(][ω][(][q][)][) =]_** **_f_** [(][q][)] **_g[(][q][)][ ]ω[(][q][)][i]_** **_fS[(][q][)]_** **_g[(][q][)][ ]ω[(][q][)][]_**
_◦_ 1:D [=][ b] _◦_
h
b

**_ω[(][q][)]_** = [z[⊤], (c[(][q][)])[⊤]][⊤].


We hope to show that h[(1)]S and h[(2)]S are functions of only z—i.e., the functions **_fS[(][q][)]_** for q = 1, 2
only extract the shared information.

[b]

To show that h[(1)]S is a function of only z but not a function of c[(1)], we consider the first-order
partial derivatives of h[(1)]S w.r.t. z and c[(1)], respectively. Namely, we hope to show that the matrix
consisting of all the partial derivatives of h[(1)]S w.r.t. z is full rank while any partial derivatives of
**_h[(1)]S_** w.r.t. c[(1)] is zero.

Therefore, we investigate the Jacobian of h[(1)] which fully characterizes all the first-order partial
derivatives of the function h[(1)]S and h[(1)]P w.r.t. z and c[(1)]. Let us denote the outputs of h[(1)] =
**_f_** [(1)] _◦_ **_g[(1)](ω[(1)]) as follows:_**
**_z_** **_z_**
b = h[(1)] _._ (B.5)
**_c[(1)]_** **_c[(1)]_**
 b   
b

The Jacobian of h[(1)] can be expressed using the following block form


**_J11[(1)]_** **_J12[(1)]_**
**_J21[(1)]_** **_J22[(1)]_**


**_J_** [(1)] =


-----

where J11[(1)] 12 21 22
defined as follows[∈] [R][D][×][D][,][ J] [(1)] _[∈]_ [R][D][×][D][1] [,][ J] [(1)] _[∈]_ [R][D][1][×][D][ and][ J] [(1)] _[∈]_ [R][D][1][×][D][1][ are Jacobian matrices]


**_h[(1)]S_** [(][ω][(1)][)] 1 _∂_ **_h[(1)]S_** [(][ω][(1)][)] 1

_∂c[(1)]1_ i _· · ·_ h _∂c[(1)]D1_ i



. .
.. ... ..



**_h[(1)]S_** [(][ω][(1)][)] _D_ _∂_ **_h[(1)]S_** [(][ω][(1)][)] _D_ 



_∂c[(1)]1_ i _· · ·_ h _∂c[(1)]D1_ i 



_∂_ **_h[(1)]P_** [(][ω][(1)][)] 1 _∂_ **_h[(1)]P_** [(][ω][(1)][)]
h _∂c.[(1)]1_ i _· · ·_ h _∂c.[(1)]D1_

.. ... ..


**_h[(1)]S_** [(][ω][(1)][)] 1 _∂_ **_h[(1)]S_** [(][ω][(1)][)]

_∂z1_ i h _∂zD_

_· · ·_

. .
.. ... ..

**_h[(1)]S_** [(][ω][(1)][)] _D_ _∂_ **_h[(1)]S_** [(][ω][(1)][)]

_∂z1_ i h _∂zD_ i

_· · ·_


**_J11[(1)]_** [=]

**_J21[(1)]_** [=]


_, J12[(1)]_ [=]
 
 

1

i 

_, J22[(1)]_ [=]


_D1_ 






**_h[(1)]P_** [(][ω][(1)][)] 1 _∂_ **_h[(1)]P_** [(][ω][(1)][)]
h _∂z1_ i h _∂zD_

_· · ·_

. .
.. ... ..

**_h[(1)]P_** [(][ω][(1)][)] _D1_ _∂_ **_h[(1)]P_** [(][ω][(1)][)]

_∂z1_ i h _∂zD_ i

_· · ·_


**_h[(1)]P_** [(][ω][(1)][)] _D1_ _∂_ **_h[(1)]P_** [(][ω][(1)][)]
i h

_∂c[(1)]1_ _· · ·_ _∂c[(1)]D1_


_D1_


What we hope to show is that J12[(1)] **[is an all-zero matrix while the determinant of][ J]11[(1)]** **[is non-zero][.]**

We first show that J12[(1)] [=][ 0][. Note that (][B.4][) holds over the entire domain. Hence, we consider any]
fixed z and c[(2)]. Then for all c[(1)], the following equation holds:

**_z_** **_z_**

**_h[(1)]S_** **_c[(1)]_** = h[(2)]S **_c[(2)]_** (B.6)

   

for all c[(1)] _∈C1 with any fixed z and c[(2)]._

Let us define matrices HS[(1)] and HS[(2)][, where]

_∂_ **_h[(]S[q][)]_** **_ω[(][q][)][i]_**

**_HS[(][q][)]_** _i,j_ [=] h _∂c [(1)]j_ _i_ _, i = 1, · · ·, D, j = 1, · · ·, D1._
h i

By taking partial derivatives of Eq. (B.6) w.r.t. c[(1)]j for j = 1, . . ., D1, we have the following
Jacobian:


_∂z1_ _∂z1_

_∂c.[(1)]1_ _· · ·_ _∂c.[(1)]D1_

.. ... ..
_∂zD_ _∂zD_

_∂c[(1)]1_ _· · ·_ _∂c[(1)]D1_

_∂c[(2)]1_ _∂c[(2)]1_

_∂c.[(1)]1_ _· · ·_ _∂c.[(1)]D1_

.. ... ..

_∂c[(2)]D2_ _∂c[(2)]D2_

_∂c[(1)]1_ _· · ·_ _∂c[(1)]D1_


 00DD2××DD11  = 0D×D1 _,_

(B.7)


(a)
= **_J_** (2)
**_hS_**



(b)
= **_J_** (2)
**_hS_**



**_HS[(1)]_** S
**_z¯,c[(1)][ =][ H]_** [(2)] **_z¯,c¯[(2)]_**


**_z¯,c¯[(2)]_**


**_z¯,c¯[(2)]_**


where Jh(2)S _∈_ R[D][×][(][D][+][D][2][)] is the Jacobian of h[(2)]S [, (a) is by the chain rules and (b) is because]

we take derivatives of constants. The equation above holds for any z and c[(2)]. Hence, the same
derivation holds for all z and c[(2)], which leads to the conclusion that the learned h[(]S[q][)][(][ω][(][q][)][)][ is not]
a function of c[(1)]. Note that another possibility that could lead to (B.7) is that h[(]S[q][)][(][ω][(][q][)][)][ always]
outputs a constant. However, this is not possible because each h[(][q][)] = **_f_** [(][q][)] _◦_ **_g[(][q][)]_** is an invertible
function, which implies that any dimension of h[(][q][)](ω[(][q][)]) cannot be a constant if ω[(][q][)] is not a
constant. To be more precise, note that x[(][q][)] is generated from ω[(][q][)] that has[b] _D + Dq dimensions. If_
there are D dimensions (i.e., h[(]S[q][)][(][ω][(][q][)][)][ ∈] [R][D][) that are constants (and thus no information) in the]
learned generative domain, then x[(][q][)] cannot be reconstructed from that domain, which contradicts
invertibility.


-----

Then, the Jacobian of h[(1)] can be re-expressed by (B.7)


**_J11[(1)]_** **_HS[(1)]_**
**_J21[(1)]_** **_J22[(1)]_**


**_J11[(1)]_** **0D×D1**
**_J21[(1)]_** **_J22[(1)]_**


**_J_** [(1)] =


Next, we show that the determinant of J11[(1)] [is non-zero. By the structure of the Jacobian][ J] [(1)][, one]
can see that **_z is a function of only z but not determined by c[(1)], where we denote as_** **_z = γ(z)._**
Besides, since h[(1)] is invertible, we have
b detJ [(1)] = detJ11[(1)] detJ22[(1)] = 0 b
_̸_

by the property of determinant for block matrix. It further indicates that detJ11[(1)] = 0 (so does
_̸_
detJ22[(1)] ), which implies that γ(·) is an invertible function. This proves Theorem 1. □

C PROOF OF THEOREM 2

**Theorem 2. (Private Component Extraction) Under the same conditions as in Theorem 1,**
also assume that (7d) is enforced, we further have **_c[(][q][)]_** = **_fP[(][q][)]_** **_x[(][q][)][]_** = δ[(][q][)][  ]c[(][q][)][], where
**_δ[(][q][)](_** ) : R[D][q] R[D][q] is a certain invertible function.

_·_ _→_  
b [b]

Following the proof of Theorem 1, we further consider the expected value version with (7d), i.e.,

_⊤[]_
maximize E **_fS[(1)]_** **_x[(1)][]_** **_fS[(2)]_** **_x[(2)][]_** (C.1a)
**_f_** [(1)],f [(2)][ Tr]
   

subject to f [(][q][)] for q = 1, 2 are invertible, (C.1b)

E **_fS[(][q][)]_** **_x[(][q][)][]_** **_fS[(][q][)]_** **_x[(][q][)][][⊤][]_** = I, E **_fS[(][q][)]_** **_x[(][q][)][i]_** = 0, q = 1, 2, (C.1c)
   h 

**_fS[(][q][)]_** **_x[(][q][)][]_** **_fP[(][q][)]_** **_x[(][q][)][]_** _, q = 1, 2,_ (C.1d)
_⊥⊥_
 

Again, under our generative model, any optimal solution (f [(1)], **_f_** [(2)]) satisfies

**_fS[(1)]_** **_x[(1)][]_** = **_fS[(2)]_** **_x[(2)][]_** _._

[b] [b]

We hope to further show that  

b [b]

**_z_**

**_c[(1)]_** = **_fP_** **_x[(1)][]_** = h[(1)]P **_c[(1)]_**
  

is an invertible function-transformed version of c[(1)], where

b [b]

**_h[(]P[q][)][(][ω][(][q][)][) =]_** **_f_** [(][q][)] **_g[(][q][)][ ]ω[(][q][)][i]_** **_fP[(][q][)]_** **_g[(][q][)][ ]ω[(][q][)][]_** _._
_◦_ _D+1:D+Dq_ [=][ b] _◦_
h
b

Recall that we have the following Jacobian matrix for function h[(1)]


**_J11[(1)]_** **0**
**_J21[(1)]_** **_J22[(1)]_**


**_J_** [(1)] =


where the second row corresponding to function h[(1)]P [.]

Since we have shown that both detJ11[(1)] = 0 and detJ22[(1)] = 0 in Section B, we only need to show
_̸_ _̸_

that J21[(1)] [is an all-zero matrix. To show this, we will use the condition (][C.1d][). First, it is not hard]
to see that


**_h[(1)]P_** [(][ω][(1)][)] 1 _∂_ **_h[(1)]P_** [(][ω][(1)][)]
h _∂z1_ i h _∂zD_

_· · ·_

. .

b.. ... b..

**_h[(1)]P_** [(][ω][(1)][)] _D1_ _∂_ **_h[(1)]P_** [(][ω][(1)][)]

_∂z1_ i h _∂zD_ i

_· · ·_
b b


**_J21[(1)]_** [=]


**_J11[(1)][,]_**


_D1_


-----

by chain rules where the first matrix on the right hand side is the Jacobian of **_c[(1)]_** w.r.t. **_z. By (C.1d),_**
we have **_c[(1)]_** _⊥⊥_ **_z, which means that we can observe fixed ˜c[(1)]_** = (c[(1)]1 _[,][ · · ·][, c][i][,][ · · ·][,][ b]c[(1)]D1_ [)][ for any]
fixed ci with any possible **_z. Therefore, at any specific point of ˜c[(1)], the following always holds b_** b
b b b

_∂_ **_c˜[(1)][]_**

b _i_ = _[∂c][i]_ = 0,

_∂zj_ _∂zj_



since the numerator is a constant. Note that the above holds for any ˜c[(1)] with different ci for
_i_ [D1], which further means that we actually haveb b
_∈_

_∂_ **_h[(1)]P_** [(][ω][(1)][)] 1 _∂_ **_h[(1)]P_** [(][ω][(1)][)] 1
h _∂z1_ i h _∂zD_ i

_· · ·_

 . . 

b.. ... b.. = 0

 _∂_ **_h[(1)]P_** [(][ω][(1)][)] _D1_ _∂_ **_h[(1)]P_** [(][ω][(1)][)] _D1_ 
 h _∂z1_ i h _∂zD_ i 
 _· · ·_ 
 

for all h[(1)]P **_ω[(1)][]_** and **_z._** b b

Therefore, J  21[(1)] = 0D b1×DJ11[(1)] = 0D1×D. Consequently, we have the following block diagonal
form for J [(1)] by combing with Theorem 1, i.e.,


**_J11[(1)]_** **0D×D1**
**0D1×D** **_J22[(1)]_**


**_J_** [(1)] =


By invertibility of **_f_** [(][q][)] and g[(][q][)], we have
detJ [(1)] = detJ11[(1)] detJ22[(1)] = 0,

[b] _̸_

which implies that **_z = γ(z) and_** **_c[(1)]_** = δ[(1)][  ]c[(1)][] with invertible functions γ(·) and δ[(1)](·),
respectively. The same proof technique applies to δ[(2)](·). □
b b

D PROOF OF THEOREM 3


**Theorem 3.** Under the generative model in (5) and Assumptions 1 and 2, assume that
(x[(1)]ℓ _[,][ x][(2)]ℓ_ [)][ for][ ℓ] [= 1][, . . ., N][ are i.i.d. samples of][ (][x][(1)][,][ x][(2)][)][. Denote][ b]f [(][q][)] _∈F as any_
solution of (7) with the invertibility constraint satisfied. Then, we have the following holds
with probability of at least 1 − _δ:_

_D_ _Dq_

2

E _∂[fS(x[(][q][)])]i/∂c[(]j[q][)]_ = O _DRN +_ log(1/δ)/N + ν[2][][2][/][3][] (D.1)

 

_i=1_ _j=1_

X X   [b]   p
 

for any c[(][q][)] _q such that_ _Cp + κj_ _c[(]j[q][)]_ _Cp_ _κj for all j_ [Dq] and all i [D], where
_∈C_ _−_ _≤_ _≤_ _−_ _∈_ _∈_
_κj = Ω(([3]/Cd)[1][/][3](4Cf_ (2DRN + Cf log(1/δ)/2N) + 4ν[2])[1][/][6]).
p

D.1 A LEMMA ON RADEMACHER COMPLEXITY

To derive the Rademacher complexity of the loss function, we have the following lemma


**Lemma 1 Consider the following function class**

_D_

= _l_ **_x[(1)], x[(2)][ ]_** **_x[(1)], x[(2)][]_** = **_fd[(1)]_** **_x[(1)][]_** **_fd[(2)]_** **_x[(2)][][2]_**
_H_ ( _d=1_ _−_ )

  X   

_where fd[(1)][,][ f][ (2)]d_ _are as defined in Assumption[l]_ _2. Assume that_ **_fd[(][q][)]_** **_x[(][q][)][]_** _Cf for all_
_∈F_ _[′]_ _≤_

**_fd[(][q][)]_** _, where Cf > 0. Then, the Rademacher complexity of class_ _is bounded by _
_∈F_ _[′]_ _H_

RN ( ) 4DCf RN _._
_H_ _≤_


-----

_Proof: First, we have the function_ **_fd[(1)]_** **_x[(1)][]_** **_fd[(2)]_** **_x[(2)][] bounded within [0, 2Cf_** ]. According
_−_
to the Lipschitz composition property of Rademacher complexity (Bartlett & Mendelson, 2002), we
   
have
RN (ϕ ) _LϕRN_ ( )
_◦F_ _≤_ _F_
where Lϕ is the Lipschitz constant of ϕ. Here ϕ(x) = x[2] and Lϕ = 4Cf _._

Combining with the linearity property of the Rademacher complexity (Bartlett & Mendelson, 2002),
we have
RN ( ) 4DCf RN (D.2)
_H_ _≤_

which completes the proof of the lemma. Note that (D.2) is derived by treating fd[(][q][)] for d [D]
_∈_
as individual functions. However, fd[(][q][)] for d [D] are the first D outputs of the same function
_∈_
**_f_** [(][q][)]—which means that many parameters of these D functions are constrained to be identical.
Nonetheless, this fact does not affect the inequality in (D.2) since adding confining constraints to
the function class H only reduces the Rademacher complexity (Bartlett & Mendelson, 2002). □

D.2 PROOF OF THEOREM 3

First, we bound the true risk on the view matching loss. Consider the regression problem given
**_x[(1)]ℓ_** _[,][ x][(2)]ℓ_ as samples, we have
  _N_

1 2
minimize **_fS[(1)]_** **_x[(1)]ℓ_** **_fS[(2)]_** **_x[(2)]ℓ_**
**_f_** [(1)],f [(2)] _N_ _−_ 2 _[.]_

_ℓ=1_

X    

By Lemma 1 and (Mohri et al., 2018, Theorem 3.3), we have the following hold with probability at
least 1 − _δ_

_N_

2 2
E **_fS[(1)]_** **_x[(1)]ℓ_** **_fS[(2)]_** **_x[(2)]ℓ_** **_fS[(1)]_** **_x[(1)]ℓ_** **_fS[(2)]_** **_x[(2)]ℓ_**
_−_ 2 _≤_ _N[1]_ _−_ 2
      Xℓ=1    

log(1/δ)

+ 2RN (H) + 4Cf[2] 2N _._

r


By Assumption 2(c), the first term on the right hand side can be bounded as

_N_

1 2

**_fS[(1)]_** **_x[(1)]ℓ_** **_fS[(2)]_** **_x[(2)]ℓ_**

_N_ _−_ 2

_ℓ=1_

X    

_N_

2

= [1] **_fS[(1)]_** **_x[(1)]ℓ_** **_u[(1)]S_** **_x[(1)][]_** + u[(2)]S **_x[(2)][]_** **_fS[(2)]_** **_x[(2)]ℓ_**

_N_ _−_ _−_ 2

_ℓ=1_

X      

_N_

(ν + ν)[2] = 4ν[2].

_≤_ _N[1]_

_ℓ=1_

X

where the first equality is because there exist u[(1)] and u[(2)] such that u[(1)]S **_x[(1)][]_**
_∈G[−][1]_ _∈G[−][1]_
**_u[(2)]S_** **_x[(2)][]_** = γ(z) and the second inequality is by the triangle inequality.  

Therefore, by plugging in  RN ( ) we have:
_H_


E **_fS[(1)]_** **_x[(1)]ℓ_** **_fS[(2)]_** **_x[(2)]ℓ_** _ϵ_
_−_ 2 _≤_
     

log(1/δ)

_ϵ := 4ν[2]_ + 8DCf RN + 4Cf[2] 2N

r


with the definition


log(1/δ)

2N


+ 4ν[2].


= 4Cf


2DRN + Cf


-----

Next, we use the bound of true risk to bound the energy of the entries of the Jacobian matrix on the
left hand side of Eq. (D.1). Denote h[(]S[q][)] = fS[(][q][)] **_g[(][q][)]. We define the error for any individual sample_**
_◦_

pair **_x[(1)]ℓ_** _[,][ x][(2)]ℓ_ _p_ **_x[(1)], x[(2)][]_** as
_∼_
    2

**_fS[(1)][(][x]ℓ[(1)][)][ −]_** **_[f][ (2)]S_** [(][x]ℓ[(2)][)]

2 [=][ ε][ℓ][,]

with E[εℓ] ≤ _ϵ._

Define another two pairs of samples, such that

2

**_zℓ_** **_zℓ_**

S **_h[(2)]S_** = εℓ[,]

**_c[(1)]ℓ_** + ∆ej _−_ **_c[(2)]ℓ_** 2
   

2 b

**_zℓ_** **_zℓ_**

**_[h]S[(1)]_** **_h[(2)]S_** = εℓ˜[,]

**_c[(1)]ℓ_** ∆ej _−_ **_c[(2)]ℓ_** 2
 _−_   

where ∆ _> 0 and ej is the unit vector in the[h][(1)]_ _c[(1)]j_ direction. Then by triangle inequality we have

**_zℓ_** **_zℓ_**

S **_h[(1)]S_** _εℓ_ [+][ √][ε]ℓ[˜][.]

**_c[(1)]ℓ_** + ∆ej _−_ **_c[(1)]ℓ_** ∆ej 2 _≤_ _[√]_
   _−_  b

Define **_[h][(1)]_**

_⊤[]_

_ψij_ _c[(1)]j_ := **_h[(1)]S_** **_z¯[⊤],_** _c¯[(1)]1_ _[,][ · · ·][, c]j[(1)][,][ · · ·][,][ ¯]c[(1)]D1_ _,_

_i_

   h  i

which is a scalar function of c[(1)]j with fixed ¯z and ¯c[(1)]k for k = j.
_̸_

Then the element _∂[fS(x[(1)])]i_ can be estimated using the central difference formula as

_∂c[(1)]j_

[b]

**_zℓ_** **_zℓ_**

_∂_ **_fS_** **_x[(1)][i]i_** **_h[(1)]S_** **_c[(1)]ℓ_** + ∆ej _−_ **_h[(1)]S_** **_c[(1)]ℓ_** ∆ej _i_
h b∂c [(1)]j =    2∆  _−_  _−_ [∆]12[2] _ψij[′′′][(][ξ][1][) +][ ψ]ij[′′′][(][ξ][2][)]_

 

**_zℓ_** **_zℓ_**

**_h[(1)]S_** **_c[(1)]ℓ_** + ∆ej _−_ **_h[(1)]S_** **_c[(1)]ℓ_** ∆ej _i_ ∆[2]
    _−_  + _ij_ [(][ξ][′][)] _[,]_

_≤_ 2∆ 6 _[ψ][′′′]_


where ξ1 ∈ _c[(1)]ℓ,j_ _[, c]ℓ,j[(1)]_ [+ ∆], ξ2 ∈ _c[(1)]ℓ,j_ _[−]_ [∆][, c]ℓ,j[(1)] and by intermediate value theorem ξ[′] _∈_
_c[(1)]ℓ,j_ _[−]_ [∆][, c]ℓ,j[(1)] [+ ∆] .   
 

Since |xi| ≤∥x∥∞ _≤∥x∥2, we have_

_∂_ hfbS∂c x[(1)]j [(1)][i]i _≤_ _√εℓb2∆[+][ √][ε]ℓ[˜]_ + ∆6[2] _[ψ]ij[′′′][(][ξ][′][)]_ _[.]_


By taking expectation, we have

E  _∂_ hfbS∂c x[(1)]j [(1)][i]i  _≤_ E[[√]εℓb[] +]2∆[ E][[][√][ε]ℓ[˜][]] + ∆6[2] _[ψ]ij[′′′][(][ξ][′][)]_

  _√ϵ_

_ij_ [(][ξ][′][)][|][,]

_≤_ ∆ [+ ∆]6[2] _[|][ψ][′′′]_

where the second inequality is by Jensen’s inequality

E[[√]εℓ] ≤ pE[εℓ] ≤ _[√]ε,_


-----

due to the concavity of _x._

_[√]_

We aim to find the smallest upper bound, i.e.,

inf
0<∆<min _Cp+c[(1)]ℓ,j_ _[,C][p][−][c]ℓ,j[(1)]_
n


_√ϵ_

_ij_ [(][ξ][′][)][|][.] (D.3)

∆ [+ ∆]6[2]

_[|][ψ][′′′]_


Note that the function in (D.3) is convex and smooth. We have the minimizer


1/3

3[√]ϵ

_ψij[′′′][(][ξ][′][)][|]_ !
_|_


_, min_ _Cp + c[(1)]ℓ,j_ _[, C][p][ −]_ _[c]ℓ,j[(1)]_
n


∆[∗]
_∈_ 



which gives us the minimum






 _[,]_


_|ψij′′′[(][ξ][′][)][|]_

3




1/3
_√ϵ_ _j_
_ϵ[1][/][3],_ + _[κ][2]_ _ij_ [(][ξ][′][)][|]

_κj_ 6

 _[|][ψ][′′′]_


_√ϵ_

_ij_ [(][ξ][′][)][| ≤] [min]

∆ [+ ∆]6[2]

_[|][ψ][′′′]_


inf


where κj = min _Cp + c[(1)]ℓ,j_ _[, C][p][ −]_ _[c]ℓ,j[(1)]_ .
n 1/3 o

3[√]ϵ
If κj ≥ _|ψij[′′′][(][ξ][′][)][|]_, then we can bound
 

_∂_ **_fS_** **_x[(1)][i]_**

E

 h b∂c [(1)]j




_|ψij′′′[(][ξ][′][)][|]_ 1/3 _ϵ[1][/][3]._

3

 


 _≤_ 2[3]




With fixed N, one can choose ϵ = 4Cf


log(1/δ)

2N


+4ν[2], which gives the following


2DRN + Cf


bound

E

if κj
_≥_


**_fS_** **_x[(1)][i]_**
b∂c [(1)]j


1/3

4Cf




1/3
+ 4ν[2]
!


_Cd_

3




 _≤_ 2[3]




log(1/δ)

2N


E _i_ _d_ 4Cf 2DRN + Cf + 4ν[2] _,_

b∂c[(1)]j _≤_ 2  3  r 2N ! !

 1/3  1/6

if κj _C3d_ 4Cf 2DRN + Cf log(12N/δ) + 4ν[2] .
_≥_
  []  q  

Considering all i, j pairs, we have

E _D_ _D1_ _∂_ **_fS_** **_x[(1)][i]i_** _Cd_ 1/3 4Cf 2DRN + Cf log(1/δ) + 4ν[2] 1/3 _._

 _i=1_ _j=1_ h b∂c [(1)]j  _≤_ 2[3] _[DD][1]_  3  r 2N ! !

X X
 

Since ∥· ∥2 is upper bounded by ∥· ∥1, we have

2[]

E  _i=1D_ _jD=11_  _∂_ hfbS∂c x[(1)]j [(1)][i]i  _≤_ 4[9] _[D][2][D]1[2]_  _C3d_ 2/3 4Cf 2DRN + Cf r log(12N/δ) ! + 4ν[2]!2/3

X X

 
   

which completes the proof. The same holds for q = 2 by role symmetry.


E PROOF OF PROPOSITION 1

The claim can be proved by contradiction. Take q = 1 for example. Suppose that there exists two
elements _zi[(1)]_ and _c[(1)]j_ that are dependent but (9) is 0. Let

_ϕ1 = e⊤i_ _[,]_ _τ1 = e⊤j[,]_

b b


-----

which are valid choices. Then, we have

Cov[ϕ1(z[(1)]), τ1(c[(1)])] = E[zi[(1)]c[(1)]j []][ −] [E][[]z[b]i[(1)]]E[c[(1)]j []][.]
Note that by definition of independence, we have
b b b b b

E[zi[(1)]c[(1)]j [] =][ E][[]z[b]i[(1)]]E[c[(1)]j []][ ⇐]⇒ _zi[(1)]_ _⊥⊥_ _c[(1)]j_ _[.]_

Hence, _zi[(1)]_ and _c[(1)]j_ being dependent means that

b b b b b

E[zi[(1)]c[(1)]j []][ −] [E][[]z[b]i[(1)]]E[c[(1)]j []][ ̸][= 0][.]

b b

Consequently, one can see that
b b b

_ϕsup1,τ1_ Cov[ϕ1(z[(1)]), τ1(c[(1)])] ≥ Cov[ϕ1(z[(1)]), τ1(c[(1)])]|ϕ1=e⊤i _[,τ][1][=][e]⊤j_ [=][ Cov][[]z[b]i[(1)]c[(1)]j []][ ̸][= 0][.]

The above is a contradiction to our assumption that holds.

b b b b b


On the other hand, if _zi[(1)]_ and _c[(1)]j_ are independent for all i [D] and j [D1], then we have
_∈_ _∈_

E[ϕ(zi[(1)])τ (c[(1)]j [)]][ −] [E][[][ϕ][(]z[b]i)]E[τ (c[(1)]j [)] = 0][,]

b b

for all i ∈ [D] and j ∈ [D1], for any ϕ : R → R and τ : R → R.

b b b

F DETAILED ALGORITHM IMPLEMENTATION

Recall that the proposed criterion is


1 _⊤_

maximize **_fS[(1)]_** **_x[(1)]ℓ_** **_fS[(2)]_** **_x[(2)]ℓ_** (F.1a)
**_f_** [(1)],f [(2)][ Tr] _N_

_ℓ=1_ !

X    

subject to f [(][q][)] for q = 1, 2 are invertible, (F.1b)


_N_ _N_

1 _⊤_

_N_ **_fS[(][q][)]_** **_x[(]ℓ[q][)]_** **_fS[(][q][)]_** **_x[(]ℓ[q][)]_** = I, _N[1]_ **_fS[(][q][)]_** **_x[(]ℓ[q][)]_** = 0, q = 1, 2, (F.1c)

_ℓ=1_ _ℓ=1_

X     X  

**_fS[(][q][)]_** **_x[(]ℓ[q][)]_** **_fP[(][q][)]_** **_x[(]ℓ[q][)]_** _, q = 1, 2,_ (F.1d)
_⊥⊥_

We will use neural networks to parameterize the functions that we aim to learn. To move forward,   
first, as we have shown in the proof of Theorem 1, Eq. (F.1) is equivalent to the following:


1
minimize **_fS[(1)]_** **_x[(1)]ℓ_** **_fS[(2)]_** **_x[(2)]ℓ_** (F.2a)
**_f_** [(1)],f [(2)] _N_ _−_ 2

_ℓ=1_

X    

_N_ _N_

_⊤_

subject to _N[1]_ **_fS[(][q][)]_** **_x[(]ℓ[q][)]_** **_fS[(][q][)]_** **_x[(]ℓ[q][)]_** = I, _N[1]_ **_fS[(][q][)]_** **_x[(]ℓ[q][)]_** = 0, q = 1, 2 (F.2b)

_ℓ=1_ _ℓ=1_

X     X  

**_f_** [(][q][)] for q = 1, 2 are invertible, (F.2c)


**_fS[(][q][)]_** **_x[(]ℓ[q][)]_** **_fP[(][q][)]_** **_x[(]ℓ[q][)]_** _, q = 1, 2,_ (F.2d)
_⊥⊥_
   

Note that we have manifold constraints on both neural networks fS[(1)] and fS[(2)][. Directly optimizing]
over such manifold constraints may be costly and challenging. To reduce the difficulty of this constrained problem, we introduce a slack variable uℓ and recast the formulation in (F.2) as follows:


minimize = [1]
**_f_** [(1)],f [(2)],uℓ _L_ _N_


_N_

_ℓ_ = [1]
_L_ _N_
_ℓ=1_

X


2

minimize = [1] _ℓ_ = [1] **_uℓ_** **_fS[(][q][)][(][x]ℓ[(][q][)][)]_** (F.3a)

[(1)],f [(2)],uℓ _L_ _N_ _ℓ=1_ _L_ _N_ _ℓ=1_ _q=1_ _−_ 2

X X X

_N_ _N_

subject to [1] **_uℓu[⊤]ℓ_** [=][ I][,][ 1] **_uℓ_** = 0. (F.3b)

_N_ _N_

_ℓ=1_ _ℓ=1_

X X

**_f_** [(][q][)] for q = 1, 2 are invertible, (F.3c)


**_fS[(][q][)]_** **_x[(]ℓ[q][)]_** **_fP[(][q][)]_** **_x[(]ℓ[q][)]_** _, q = 1, 2,_ (F.3d)
_⊥⊥_
   


-----

Ideally, we hope that uℓ = γ(zℓ). Introducing uℓ makes the f [(1)] and f [(2)] subproblems unconstrained. This is a commonly used reformulation in neural network based multiview matching (see
(Benton et al., 2017; Lyu & Fu, 2020)), which is reminiscent of the MAX-VAR formulation of
CCA (Kettenring, 1971; Carroll, 1968; Rastogi et al., 2015). Such reformulations oftentimes make
algorithm design easier, since the constraints are simplified.

The inveritibility and independence constraints in (F.3c) and (F.3d) are also not straightforward to
enforce. Instead of directly enforcing the invertibility constraint in (F.3c), we design a regularization
term. Specifically, we use the idea of autoencoder that reconstructs the samples from their latent
representations f [(][q][)](x[(]ℓ[q][)][)][. We define a regularizer]


= [1]
_V_ _N_


_N_

_ℓ_ = [1]
_V_ _N_
_ℓ=1_

X


2

= [1] _ℓ_ = [1] **_x[(]ℓ[q][)]_** **_r[(][q][)](f_** [(][q][)](x[(]ℓ[q][)][))] (F.4)
_V_ _N_ _V_ _N_ _−_ 2 _[,]_

_ℓ=1_ _ℓ=1_ _q=1_

X X X

as the reconstruction loss, where r[(][q][)]’s are the reconstruction neural networks. Note that the above
term being zero does not necessarily indicate that the function f [(][q][)] is invertible, since this term
is only imposed on limited number of samples. But in practice, this idea is effective in learning
invertible transformations—also see (Wang et al., 2015; Lyu & Fu, 2020).

To promote the statistical independence constraint in (F.3d), we use the designed independence
regularizer, i.e.,

Cov **_ϕ[(][q][)][  ]z[(][q][)][]_** _, τ_ [(][q][)][  ]c[(][q][)][]

sup sup (F.5)
**_ϕ[(][q][)],τ_** [(][q][)][ R][(][q][)][ =] **_ϕ[(][q][)],τ_** [(][q][)] V[ϕ[(][q][)][  ]z[(][q][)][]] V **_τ_** [(][q][)][  ]c[(][q][)][] _[,]_

b b

q q 

where ϕ[(][q][)] and τ [(][q][)] are again represented by neural networks.

b b

Let θ collect the parameters of f [(][q][)] and r[(][q][)], and η the parameters of ϕ[(][q][)] and τ [(][q][)]. Putting all the
terms together, our working cost function is summarized as follows:


_N_ _N_

1
min _ℓ(θ, U_ ) + β [1] _ℓ(θ) + λ_ (θ, η), (F.6a)
**_U_** _,θ_ [max]η _N_ _L_ _N_ _V_ _R_

_ℓ=1_ _ℓ=1_

X X

_N_

subject to _N[1]_ **_UU_** _[⊤]_ = I, _N[1]_ **_[U]_** **[1][ =][ 0][,]** (F.6b)

_ℓ=1_

X


where U = [u1, _, uN_ ] R[D][×][N], and β and λ are nonnegative and = _q=1_
_· · ·_ _∈_ _R_ _[R][(][q][)][.]_

In terms of algorithm design, we propose to handle U, θ and η cyclically when the other two are
fixed, i.e., alternating optimization (AO). [P][2]

First, we use stochastic gradient descent and ascent for the unconstrained θ and η subproblems. To
proceed, we sample a batch of data indexed by B ⊆ [N ]. Then, θ and η can be updated by any
stochastic gradient based optimizers, e.g., the plain-vanilla stochastic gradient descent/ascent,

1

**_θ_** **_θ_** _γ_ ( **_θ_** _ℓ(θ, U_ ) + β **_θ_** _ℓ(θ)) + λ_ [b] **_θ_** (θ, η) _,_
_←_ _−_ _|B|_ Xℓ∈B _∇_ _L_ _∇_ _V_ _∇_ _R_ !

**_η_** **_η + δ_** _λ_ [b] **_η_** (θ, η) _,_
_←_ _∇_ _R_
 

where γ and δ are the step sizes for the updates of θ and η, respectively. The stochastic gradients of
_L, V are defined as follows:_

**_θ_** := [1] **_θ_** _ℓ(θ, U_ ) (F.7a)
_∇_ _L_ _∇_ _L_

_|B|_ Xℓ∈B

b

**_θ_** := [1] **_θ_** _ℓ(θ)._ (F.7b)
_∇_ _V_ _∇_ _V_

_|B|_ Xℓ∈B

b


-----

In addition, the terms **_θ_** and **_η_** are defined similarly. Taking the latter as an example. We
_∇_ _R_ _∇_ _R_
have **_η_** = _q=1_ **_η_**, and **_η_** is estimated by taking gradient w.r.t. η of the following
_∇_ _R_ _∇_ _R[(][q][)]_ _∇_ _R[(][q][)]_

batch-estimated (the same holds for[b] [b] **_θ_** ):
_R[(][q][)]_ _∇_ _R_

[b] [P][2] [b] [b]

:= (F.8a)
_RB[(][q][)]_ [b]
1 **_ϕ[(][q][)][ ]zℓ[(][q][)]_** 1 **_ϕ[(][q][)][ ]zℓ[(][q][)]_** **_τ_** [(][q][)][ ]c[(]ℓ[q][)] 1 **_τ_** [(][q][)][ ]c[(]ℓ[q][)]

_|B|_ _ℓ_ _−_ _|B|_ _ℓ_ _−_ _|B|_ _ℓ_

_∈B_   _∈B_ [ ]  _∈B_ []

P P P

1 **_ϕ[(][q][)]_** **_zℓ[(][q][)]_** b 1 **_ϕ[(][q][)]_** **_zℓ[(][q][)]_** b 1 **_τb[(][q][)]_** **_c[(]ℓ[q][)]_** 1 **_τb[(][q][)]_** **_c[(]ℓ[q][)]_**

s _|B|_ _ℓ_ _−_ _|B|_ _ℓ_ _|B|_ _ℓ_ _−_ _|B|_ _ℓ_

_∈B_    _∈B_  [][2][s] _∈B_    _∈B_  [][2]

P P P P

**_η_** := **_η_** b b b b (F.8b)
_∇_ _R[(][q][)]_ _∇_ _RB[(][q][)]_

**_θ_** := **_θ_** _[.]_ (F.8c)
_∇b_ _R[(][q][)]_ _∇_ _RB[(][q][)]_

It was shown in (Fisher, 1915) that the correlation coefficient is computed using random samples

b

of Gaussian variables, the estimator in (F.8a) for R[(][q][)] is asymptotically unbiased. For other distributions, the estimation also works well in practice; see, e.g., DCCA based works in (Wang et al.,
2015).

Consider more general stochastic optimizers, e.g., Adam (Kingma & Ba, 2015) and Adagrad (Duchi
et al., 2011). Then, the updates can be summarized as follows:

**_θ_** SGD optimizer **_θ,_** **_θ_** + β **_θ_** + λ [b] **_θ_** (F.9)
_←_ _∇_ _L_ _∇_ _V_ _∇_ _R_
 

**_η_** SGD optimizer **_η,_** **_η_** _._ (F.10)
_←_ _−[b]_ _∇[b]_ _R_ [b]
 

where (F.10) uses the negative stochastic gradient since it is an ascending step, while stochastic
optimizers are by default descending the objective function.

The U subproblem consists of (F.3a) and (F.3b). It can be re-expressed as follows by expanding
(F.3a):


2
**_uℓ_** **_fS[(][q][)][(][x]ℓ[(][q][)][)]_**
_−_ 2

_⊤_
2uℓu[⊤]ℓ _[−]_ [2][u][ℓ] **_fS[(1)]_** **_x[(1)]ℓ_** + fS[(2)] **_x[(2)]ℓ_**
    


_ℓ=1_


_q=1_


2

_⊤_
**_fS[(][q][)]_** **_x[(]ℓ[q][)]_** **_fS[(][q][)]_** **_x[(]ℓ[q][)]_**
_q=1_

X    


= [1]


Tr

_ℓ=1_

X


Note that the first term is a constant by (F.3b) and the last term does not involve uℓ. Then, we rewrite
the uℓ subproblem as

maximize Tr **_U_** **_F_** [(1)] + F [(2)][] _,_ (F.11)
**_U_**
 

subject to [1]

_N_ **_[UU][ ⊤]_** [=][ I][,][ 1]N **_[U]_** **[1][ =][ 0][,]**

where F [(][q][)] = **_fS[(][q][)][(][x]1[(][q][)][)][,][ · · ·][,][ f][ (]S[q][)][(][x]N[(][q][)][)]_** . This is an orthogonal projection onto the set of row
zero-mean and orthogonal matrices. It is shown in (h i Lyu & Fu, 2020, Lemma 1) that such a projection problem, although nonconvex, can be solved to optimality via a mean-removed singular value
decomposition (SVD) procedure, i.e.,

**_U ←_** _√N_ **_ST_** _[⊤], with SDT_ _[⊤]_ = SVD **_F_** [(1)] + F [(2)][] **_W_** _,_ (F.12)

 

where W = IN _N_ **[11][⊤]** [removes the mean of][ F][ (1)][ +][ F][ (2)][,][ D][ ∈] [R][D][×][D][ holds the singular values,]
_−_ [1]

**_S ∈_** R[D][×][D] and T ∈ R[N] _[×][D]_ are the left and right orthogonal matrices in the SVD, respectively.

To summarize, an alternating optimization algorithm is summarized in Algorithm 1. Notice that
we use two different batch sizes (denoted as B1 and B2 in line 4 of Algorithm 1) to construct the
gradient estimations for **_θ_** _,_ **_θ_** and **_θ_** _,_ **_η_**, respectively. The reason is that accurately
_∇[b]_ _L_ _∇[b]_ _V_ _∇[b]_ _R_ _∇[b]_ _R_


-----

**Algorithm 1: Proposed Algorithm.**


**Data: x[(]ℓ[q][)]** for ℓ = 1, _, N and q = 1, 2_
_· · ·_

**Result: f** [(][q][)], r[(][q][)]

**1 while stopping criterion is not reached do**

**2** **_U ←_** _√N_ **_ST_** _[⊤]with SDT_ _[⊤]_ = SVD **_F_** [(1)] + F [(2)][] **_W_** ;

**3** **while stopping criterion is not reached do** 

**54** _∇draw a random batchθL ←_ _|B11|_ _ℓ∈B1 B1 and B2;_ // use |B2| > |B1|

1 _[∇][θ][L][ℓ][;]_
**6** _∇b_ **_θV ←_** _|B1|_ Pℓ∈B1 _[∇][θ][V][ℓ][;]_

**7** _∇b_ **_ηR ←_** [P]q[2]=1P[∇][η][R][(]B[q]2[)][;] // using B2 and (F.8a), (F.8b)

**8** _∇b_ **_θR ←_** [P]q[2]=1 _[∇][θ][R][(]B[q]2[)]_ [;] // using B2 and (F.8a), (F.8c)

**9** **_θ_** SGD optimizer **_θ,_** **_θ_** + β **_θ_** + λ [b] **_θ_** ; // descent

b ← _∇_ _L_ _∇_ _V_ _∇_ _R_

**10** **_η ←_** SGD optimizer η, −[b] _λ∇[b]_ **_ηR_** [b];  // ascent

**11** **end**  

**12 end**

Table F.1: Computational complexity of the proposed algorithm in each iteration, where dθ and
_dη denote the parameter dimensions of the encoder/reconstruction and the independence-promoting_
networks, respectively.


Complexity (flops)

Line 2 _O(ND[2])_
Line 5 _O(_ 1 _dθ)_
_|B_ _|_
Line 6 _O(_ 1 _dθ)_
_|B_ _|_
Line 7 _O(_ 2 _dη)_
_|B_ _|_
Line 8 _O(_ 2 _dθ)_
_|B_ _|_
Line 9 _O(dθ)_
Line 10 _O(dη)_

Overall _O_ _ND[2]_ + (|B1| + |B2|)dθ + |B2|dη
  

estimating of R using (F.8a) often requires a relatively large batch size, while small batches may
suffice for the gradient estimations of L and V, according to our extensive simulations.

**Computational Complexity.** Tab. F.1 summarizes the computational complexity of each step.
Specifically, line 2 requires computing a thin SVD, which requires O(ND[2]) flops. Note that this is
linear in the number of samples N, and D is the dimension of the shared component, which is often
relatively small in practice.

Inside the inner loop line 4-10, lines 5 and 6 construct the gradient estimations w.r.t. θ and η.
These two steps cost O( 1 _dθ) flops. Similarly, lines 7 and 8 use O(_ 2 _dη) and O(_ 2 _dθ) flops,_
_|B_ _|_ _|B_ _|_ _|B_ _|_
respectively. Note that we have used dθ and dη to denote the parameter dimensions of the encoder/reconstruction and the independence-promoting networks, respectively. Typically, 1 and
_|B_ _|_
2 are small numbers compared to N (e.g., 1 = 128, 2 = 512 while N could easily exceed
_|B_ _|_ _|B_ _|_ _|B_ _|_
10[5]). For line 9 and 10, when a first-order stochastic optimizer (e.g., plain-vanilla SGD, ADAM,
Adagrad) is used, this step has a computational complexity that is linear in terms of the network
size.

One can see that all the steps scale linearly with size of the neural networks or the sample size, which
makes the algorithm easy to run with large-scale data sets and large-size neural feature extractors.


-----

Figure G.1: Left: z; middle: t-SNE of x[(1)]; right: t-SNE of x[(2)].

G EXPERIMENTS: MORE DETAILS AND ADDITIONAL VALIDATIONS

In this section, we show all the experiment results with greater details, e.g., results under more
metrics, more setting details, and more demonstrations. We also include more real data experiments
using the CIFAR10 (Krizhevsky et al., 2009) and dSprites data (Higgins et al., 2017).

G.1 SYNTHETIC DATA - VALIDATING MAIN THEOREMS

In this subsection, we describe the synthetic data experiments. For synthetic data, we generate the
shared z ∈ R[2] that is uniformly drawn from the unit circle, with noise N (0, 0.02[2]) added to each
dimension. The private components are scalars c[(1)] _∼N_ (0, 2.0) and c[(2)] _∼_ Laplace(0, 4.0). The
shared-to-private energy ratios for the two views are approximately -6 dB and -18 dB. The sample
size is N = 5, 000. And we use two different one-hidden-layer neural networks with 3 neurons
and softplus activation to represent the invertible g[(][q][)]’s. The network parameters are drawn from
standard normal distribution.

The shared component z and the t-SNE of x[(1)] and x[(2)] are shown in Fig. G.1. One can see
that by incorporating strong noise and nonlinear transformations, the shape of circle is hardly to be
identified in both views.

In our simulations, f [(][q][)] is represented by a three-hidden-layer multi-layer perceptrons (MLPs) with
256 neurons in each layer with ReLU activations. In addition, ϕ[(][q][)] and τ [(][q][)] are represented by twohidden-layer MLPs with 128 neurons in each layer. We set batch size to be 1000, β = 1.0, λ = 0.1.
We use the Adam optimizer (Kingma & Ba, 2015) with initial learning rate 0.001 for all the parameters. Besides, we also regularize the network parameters using ∥η∥2[2] [with a regularization parameter]
0.1. This often helps improve numerical stability when optimizing cost functions involving neural
networks. We run lines 4-10 of Algorithm 1 for 10 epochs to update θ and η.

For ablation study, we test the methods with different combinations of L, R and V, i.e.,

(i) the proposed method (L + V + R);

(ii) the proposed without independence regularization (L + V);

(iii) the proposed without reconstruction (L + R);

(iv) the proposed with only latent correlation maximization (L);

(v) we also test the performance with HSIC (Gretton et al., 2007) as the independence regularizer (L + V + HISC);


All methods stop when the average matching loss L reaches 0.01. The learned components by the
proposed method are shown in Fig. G.2. One can see that the estimated shared components are
matched, while the second view exhibits relatively large noise level as expected. For the estimated
private components, one can see that both δ[(1)](·) and δ[(2)](·) are approximately invertible functions.

To evaluate the performance of the synthetic experiment, we compute mutual information (MI) between groups of random variables of interest (measured by the mutual information neural estimation


-----

Figure G.2: (a) Scatter plot of **_z[(1)]; (b)_** _c[(1)]_ as a function of c[(1)]; (c) Scatter plot of **_z[(2)]; (d)_** _c[(2)]_ as
a function of c[(2)].
b b b b

(MINE) (Belghazi et al., 2018) and Gaussian kernel density estimation (KDE) (Davis et al., 2011)).
The results are averaged from 10 random trials.

One can see that all methods successfully extract the information about z in the sense that both
**_z[(][q][)]_** = **_fS[(][q][)][(][x][(][q][)][)][ for][ q][ = 1][,][ 2][ have similarly high MIs with][ z][. Besides, all methods output][ b]z[(][q][)]_** and
_c[(][q][)]_ (c[(][q][)]) that have small MIs—meaning that they are not dependent.

Although most loss functions using latent correlation maximization extract the sharedb [b] **_z’s informa-_**

btion well, the difference is articulated in extracting the private information. The proposed L + _V +_ _R_

objective has the best performance on that regard. The method L + _V +_ HSIC also works reasonably
well since HSIC serves the same purpose as R does—but with a kernel-based implementation.

Moreover, by looking at the last two columns, one can see that the methods with R and V perform
the best in removing the information of z from c[(][q][)]. This corroborates our analysis that both (7b)
(invertibility) and (7d) (independence) are vital to achieve private-shared information disentanglement.

Tab. G.1 shows the results, with all the entries averaged over 10 random trials. One can see that
the results via the Gaussian KDE are consistent with those under MINE. That is, the proposed
_L + V + R exhibits the best performance in terms of extracting and disentangling the shared and_
private information.

G.2 SYNTHETIC DATA - ROBUSTNESS TO STRONG PRIVATE INTERFERENCE

In this subsection, we demonstrate the performance of the proposed method under different levels of
private component energy (which are often considered interference) (Ibrahim & Sidiropoulos, 2020;
Bach & Jordan, 2005; Lyu & Fu, 2020). First, we define the shared-to-private ratio (SPR) for the
_q-th view as_

1 _N_

SPR = 10 log10  _DN1_ _Nℓ=1_ _[∥][z][ℓ][∥]2[2]_  dB.

_DqN_ Pℓ=1 _ℓ_ _[∥]2[2]_

_[∥][c][(][q][)]_

 

For the experiment, we make both views have identical SPRs. We test the performance under SPR=-P
10 dB, -20 dB and -30 dB, respectively.


-----

Table G.1: Mutual information (MI) between groups of variables. “↑”: high score preferred; “↓”:
low score preferred; “n/a”: not applicable; **_z[(][q][)]_** = **_fS[(][q][)]_** for q = 1, 2.

|Col1|z b(1),z (↑)|z b(2),z (↑)|z b(1),c(1) (↓)|z b(2),c(2) (↓)|bc(1),c(1) (↑)|bc(2),c(2) (↑)|bc(1),z (↓)|bc(2),z (↓)|
|---|---|---|---|---|---|---|---|---|
|Metric|MINE-based MI Estimation (Belghazi et al., 2018)||||||||
|L L + V L + R L + V + R L + V+HSIC|2.32±0.06 2.37±0.05 2.32±0.05 2.43±0.04 2.48±0.09|2.36±0.10 2.38±0.08 2.33±0.07 2.39±0.09 2.43±0.08|0.01±0.00 0.01±0.00 0.02±0.01 0.01±0.01 0.01±0.00|0.00±0.00 0.01±0.00 0.00±0.00 0.01±0.00 0.01±0.01|0.45±0.13 0.55±0.09 0.90±0.42 1.22± 0.33 0.68±0.25|0.33±0.07 0.33±0.08 0.22±0.12 0.85±0.23 0.52±0.14|0.39±0.14 0.31±0.06 0.09±0.06 0.04±0.02 0.04±0.01|0.43±0.04 0.43±0.05 0.22±0.12 0.11±0.03 0.07±0.02|
|Metric|Gaussian kernel density estimate (KDE)-based MI Estimation||||||||
|L L + V L + R L + V + R L + V+HSIC|2.73±0.22 2.95±0.21 2.83±0.18 3.27±0.07 3.35±0.16|2.88±0.22 2.91±0.26 2.86±0.15 2.88±0.25 2.95±0.33|0.00±0.00 0.00±0.01 0.00 0.00±0.01 0.00|0.00 0.00 0.00 0.00 0.00|0.33±0.13 0.38±0.11 0.72±0.40 0.99±0.30 0.78±0.22|0.01±0.02 0.01±0.02 0.10±0.10 0.35±0.11 0.06±0.06|0.34±0.13 0.24±0.04 0.08±0.07 0.03±0.04 0.05±0.02|0.37±0.06 0.39±0.07 0.28±0.16 0.08±0.03 0.02±0.01|



Table G.2: Mutual information (MI) between groups of variables under different SPR. “↑”: high
score preferred; “ ”: low score preferred; **_z[(][q][)]_** = **_fS[(][q][)]_** for q = 1, 2.
_↓_

|Col1|z b(1),z (↑)|z b(2),z (↑)|z b(1),c(1) (↓)|z b(2),c(2) (↓)|bc(1),c(1) (↑)|bc(2),c(2) (↑)|bc(1),z (↓)|bc(2),z (↓)|
|---|---|---|---|---|---|---|---|---|

|-10 dB -20 dB -30 dB|2.41±0.07 1.81±0.10 1.16±0.07|2.43±0.05 2.15±0.09 1.55±0.10|0.01±0.01 0.10±0.06 0.11±0.09|0.01±0.01 0.01±0.01 0.07±0.04|1.72±0.41 1.41±0.15 1.77±0.42|0.52±0.10 1.15±0.07 0.73±0.14|0.02±0.01 0.08±0.04 0.03±0.02|0.19±0.03 0.06±0.02 0.14±0.11|
|---|---|---|---|---|---|---|---|---|


SPR MINE-based MI Estimation (Belghazi et al., 2018)

b b b b b b b b


Table G.3: Mutual information (MI) between groups of variables with different β (i.e. reconstruction
term). “ ”: high score preferred; “ ”: low score preferred; **_z[(][q][)]_** = **_fS[(][q][)]_** for q = 1, 2.
_↑_ _↓_

|Col1|z b(1),z (↑)|z b(2),z (↑)|z b(1),c(1) (↓)|z b(2),c(2) (↓)|bc(1),c(1) (↑)|bc(2),c(2) (↑)|bc(1),z (↓)|bc(2),z (↓)|
|---|---|---|---|---|---|---|---|---|


|β = 1e−2 β = 1e−1 β = 1e0 β = 1e1 β = 1e2|2.13±0.26 2.16±0.23 2.41±0.13 2.34±0.19 2.32±0.01|2.11±0.33 2.12±0.21 2.38±0.12 2.28±0.08 1.77±0.07|0.01±0.01 0.01±0.01 0.01±0.00 0.04±0.04 0.07±0.02|0.01±0.00 0.01±0.00 0.01±0.00 0.02±0.02 0.40±0.05|0.77±0.03 1.09±0.37 1.48±0.08 0.86±0.40 0.58±0.04|0.48±0.18 0.70±0.20 0.93±0.17 0.41±0.21 0.21±0.03|0.13±0.02 0.10±0.06 0.02±0.00 0.15±0.10 0.31±0.01|0.17±0.07 0.16±0.07 0.10±0.04 0.52±0.26 0.69±0.05|
|---|---|---|---|---|---|---|---|---|


_λ = 1e[−][1]_ MINE-based MI Estimation (Belghazi et al., 2018)

b b b b b b b b


Tab. G.2 shows the evaluation results averaged from 10 trials. One can see that as SPR decreases, the
MI between the extracted **_z and z decreases—but only gracefully. The slight decline of performance_**
is because the matching of two views becomes harder when the private components get stronger.
However, even if SPR=-30 dB, the extraction and disentanglement of private and shared information
b
are still clearly achieved. Such robustness to strong private interference is considered a key feature
of linear CCA (Ibrahim & Sidiropoulos, 2020) and post-nonlinear CCA (Lyu & Fu, 2020). Our
analysis and evaluation in this work show that such resilience is also inherited by the proposed
approach.

G.3 SYNTHETIC DATA - SENSITIVITY TO HYPERPARAMETERS

In this subsection, we investigate the sensitivity to the key hyperparameters β and λ. The results are
shown in Tab. G.3 and Tab. G.4, respectively. One can see that for the reconstruction regularization
parameter (i.e., β) does not affect the results too much unless it was set to be overly large (i.e., β = 1
or 100 in our simulation). This makes sense, since reconstruction is for preventing trivial degenerate
solutions, and giving this part too much attention may not really help the learning goals (e.g., shared
and private information extraction) reflected in the other parts of the loss function. From Tab. G.4,
one can see that the choice of λ affects the performance slightly more than β. It makes sense, since
_λ reflects the attention that the algorithm puts on the private component extraction part. We should_
mention that, for real data analysis with a downstream task (e.g., classification), one can choose
these hyperparameters using a validation set.


-----

Table G.4: Mutual information (MI) between groups of variables with different λ (i.e. independence
regularizer). “ ”: high score preferred; “ ”: low score preferred; **_z[(][q][)]_** = **_fS[(][q][)]_** for q = 1, 2.
_↑_ _↓_

|Col1|z b(1),z (↑)|z b(2),z (↑)|z b(1),c(1) (↓)|z b(2),c(2) (↓)|bc(1),c(1) (↑)|bc(2),c(2) (↑)|bc(1),z (↓)|bc(2),z (↓)|
|---|---|---|---|---|---|---|---|---|


|λ = 1e−2 λ = 1e−1 λ = 1e0 λ = 1e1 λ = 1e2|2.36±0.09 2.41±0.13 2.04±0.12 1.49±0.18 0.80±0.23|2.34±0.05 2.38±0.12 2.00±0.11 1.55±0.11 0.96±0.36|0.01±0.00 0.01±0.00 0.04±0.01 0.17±0.07 0.46±0.13|0.01±0.00 0.01±0.00 0.02±0.00 0.17±0.10 0.28±0.07|0.56±0.18 1.48±0.08 0.71±0.17 0.61±0.04 0.18±0.08|0.51±0.14 0.93±0.17 0.37±0.10 0.17±0.07 0.22±0.06|0.25±0.07 0.02±0.01 0.15±0.06 0.23±0.02 0.75±0.37|0.36±0.08 0.10±0.04 0.22±0.03 0.16±0.05 0.57±0.18|
|---|---|---|---|---|---|---|---|---|


_β = 1e[0]_ MINE-based MI Estimation (Belghazi et al., 2018)

b b b b b b b b


Table G.5: Network structures for the MNIST experiment.


Encoders Decoders

input: x[(]ℓ[q][)] _∈_ R[28][×][28][×][1] input: f [(][q][)](x[(]ℓ[q][)][)][ ∈] [R][D][+][D][q]
4 × 4 Conv, 64 ReLU, stride 2 FC 256, ReLU
4 × 4 Conv, 32 ReLU, stride 2 FC 7 × 7 × 32, ReLU
FC 256, ReLU 4 × 4 Conv Trans, 64 ReLU, stride 2
FC D + Dq 4 4 Conv Trans, 1, stride 2
_×_

G.4 REAL DATA - MORE ON VALIDATING THEOREM 1

**Multiview MNIST Data. In this subsection, we provide more details and evaluation results on the**
MNIST experiment. The way of generating such two views (as shown in Fig. 1 of the main text) of
MNIST data is similar to the data augmentation ideas used in AM-SSL, e.g., rotation, adding noise,
cropping, flipping (Chen et al., 2020; Grill et al., 2020). We aim to match the two views and try
to learn the shared representations, which should encode the class label information. The dataset
has 70,000 samples that are 28×28 images of handwritten digits. For the latent dimension, we set
_D = 10, D1 = 20 and D2 = 50. In particular, since the second view consists of large random noise_
that are not of interest, we only add the independence regularizer R[(1)] on the first view (i.e., the
rotated digits) to learn the private component of x[(1)]ℓ [. The learned][ b]c[(1)]ℓ was used to generate new
samples; see the illustration in our main text.

The network structure used for all methods is shown in Tab. G.5. The network structure for ϕ[(][q][)] and
**_τ_** [(][q][)] are MLPs with three hidden layers of 64 neurons. For hyperparameters, we set batch size to be
1 = 100 and 2 = 1000, β = 1.0, λ = 100.0. For optimizer, we also use Adam (Kingma &
_|B_ _|_ _|B_ _|_
Ba, 2015) with initial learning rate 0.001 for θ and 1.0 for η. And we add squared ℓ2 regularization
for both θ and η, with different regularization parameters that are 0.0001 and 0.1, respectively. We
also run the SGD optimizer for 10 epochs to update θ and η.

In Tab. G.6, we show more evaluation results on the learned shared information across views. To
be specific, we feed the learned **_zℓ[(1)][’s to a classifier and the][ k][-means algorithm, to observe if the]_**
learned representations improve the performance of supervised and unsupervised learning tasks. For
the classification task, we split the data as 50,000/10,000/10,000 for training/validation/test sets. We
b
train a linear support vector machine (SVM) using the training data. The performance is measured by
classification error (ERR). For the clustering task, we use the standard k-means to cluster on all the
data samples. After clustering, we report the performance on the test set. We use a number of metrics
to measure performance, namely, clustering accuracy (ACC), normalized mutual information (NMI),
and adjusted Rand index (ARI) (Yeung & Ruzzo, 2001). Among these metrics, ARI ranges from
_−1 to +1, with 1 being the best and −1 the worst and NMI range from 0 to 1 with 1 being the best._

The “Baseline” denotes the results of simply applying SVM and k-means onto the raw data of
the first view, i.e., x[(1)]ℓ for ℓ = 1, . . ., N . All the results of algorithms that involve stochastic
methods are averaged over 5 random initializations. One can see that all methods have comparably
good results in terms of learning informative representations across views. The results empirically
validate our Theorem 1 that latent correlation maximization is a useful criterion to extract the shared
information with guarantees. In particular, Barlow Twins performs slightly better in terms of
benefiting downstream classification and clustering tasks. Nonetheless, the proposed method can


-----

Figure G.3: t-SNE of the results on multiview MNIST of the second view. Baselines: DCCA (Wang
et al., 2015), Barlow Twins (Zbontar et al., 2021) and BYOL (Grill et al., 2020) .

Table G.6: The classification error (first row) and clustering results (rows 2 to 4) of the two-view
MNIST dataset. “Baseline _↑”: high score preferred; “L + V + R_ _L + V_ _↓”: low score preferred.DCCA_ DCCAE Barlow Twins BYOL

ERR (↓) 13.40% 2.81%±0.21% 2.95%±0.23% 2.87%±0.16% 2.85%±0.05% 2.05%±0.07% 2.67%±0.22%

ACC (NMI (ARI (↑↑↑))) 37.35%0.3370.216 97.03%0.9220.936±±±0.0030.0030.13% 96.80%0.9230.931±±±0.0070.0090.40% 97.02%%0.9220.935±±±0.0030.0030.12% 96.95%0.9200.934±±±0.0030.0020.12% 98.06%0.9470.958±±±0.0020.0010.06% 95.56%0.8950.904±±±0.0230.0470.41%


Table G.7: Augmentation Used for CIFAR10 to Generate Multiple Views.

Transformation Value Probability

ColorJitter brightness=0.8, contrast=0.8, saturation=0.8, hue=0.2 0.8

GrayScale - 0.2

RandomResizedCrop scale=(0.2, 1.0), ratio=(0.75, 4/3) 
HorizontalFlip - 0.5

GaussianBlur _σ ∼_ _U_ [0.1, 2] 0.5

Solarization - 0.4

Normalization - 

also guarantee extracting private information and facilitate cross-view image generation (see the
experiments in the main text), which is out the reach of Barlow Twins and BYOL.

In addition to showing the visualization of the learned embedding of the firs view in Fig. 1 of the
main paper, we also plot the t-SNE of the learned representation **_zℓ[(2)]_** of the second view (i.e., the
noisy digits). The results are shown in Fig. G.3. One can see that the visualization and clustering
accuracy are similar to those obtained from **_zℓ[(1)][.]_** b

**Augmented CIFAR10 Data for SSL. We also use the CIFAR10 dataset (Krizhevsky et al., 2009)**
to validate Theorem 1. The CIFAR10 dataset contains 50,000 and 10,000 images of size b 32 _×_ 32 for
training and testing, respectively. There are 10 different classes. We use ResNet18 as the backbone
network for learning the representation. Since CIFAR10 images are small, we replace the first 7x7
Conv layer of stride 2 with 3x3 Conv layer of stride 1. We also remove the max pooling layer. We
follow the evaluation method in (Chen & He, 2021) to stop the algorithms after one hundred epochs.
In terms of data augmentation, we use the pipeline of different transformations in Tab. G.7. Note
that for the proposed method, we only impose constraint (7c) since our goal here in this task is to
extract essential shared information.

We evaluate the proposed method and two AM-SSL baselines as mentioned in the main text, namely,
Barlow Twins (Zbontar et al., 2021) and BYOL (Grill et al., 2020) by feeding the learned representations to a linear classifier. We report both the Top-1 linear classification accuracy and the
KNN (with k = 5) accuracy. The results are shown in Tab. G.8. One can see that different methods
achieve comparable results. The proposed method and BYOL attain essentially the same accuracy.
The t-SNE (Van der Maaten & Hinton, 2008) visualizations of the test set is plotted in Fig. G.4.
One can see that all methods extract “identity-revealing” information to a certain extent, as in the
MNIST case.


-----

Table G.8: Evaluation using CIFAR10.

BYOL Barlow Twins Proposed

Classification Acc. (%) 84.2 82.8 84.2
KNN (k = 5) Acc. (%) 80.5 78.7 81.0


Figure G.4: t-SNE of learned representations for CIFAR10.

Figure G.5: Samples of the lower elevations (view1) and higher elevations (view2).

**Remark 1 We should remark that the experiment results suggest that latent correlation maximiza-**
tion (or latent component matching) used in many AM-SSL and DCCA methods works towards the
same ultimate goal under our generative model in (5). However, this does not suggest that different
SSL and DCCA methods are essentially the same in practice—one should not expect that. In fact,
there are many factors that affect DCCA and AM-SSL methods’ results, e.g., model mismatches,
optimization procedure, network construction, and the detailed designs in their loss functions. The
difference between the methods normally are more articulated with larger data sets or more complex
problems. Nonetheless, our interest lies in theoretical understanding of their common properties,
other than the differences in practical implementations. From this perspective, the results in this
section support our theoretical analysis in Theorem 1.

G.5 REAL DATA - MORE ON VALIDATING THEOREM 2

**Cars3D Data for Cross-sample Generation. In this subsection, we provide more detailed settings**
and results of the Cars3D experiment. To create a multiview dataset, we assume that given the car
type that is captured by shared variables z and the azimuths that are captured by c[(][q][)], the generation
mappings g[(1)] and g[(2)] produce car images with low elevations and high elevations, respectively (so
they must be different mappings).

We split the car images as follows. We treat the same car model (e.g., a red convertible) with
lower and higher elevations as x[(1)]ℓ and x[(2)]ℓ [, respectively. The azimuths are randomly shuffled]
with different pairs of x[(1)]ℓ and x[(2)]ℓ [. This way, if our generative model holds,][ z][,][ c][(][q][)][ and][ g][(][q][)][ are]
responsible for ‘type’, ‘azimuth’ and ‘elevation’, respectively. Some samples are shown in Fig. G.5.

Under our splitting, each view has 2×183×24 = 8784 RGB images that all have a size of 64×64×3.
We model the ‘type’ information z using D = 10 latent dimensions since many different factors
(e.g., color and shape) together give rise to a ‘type’. On the other hand, we set D1 = D2 = 2 to
model the ‘azimuth’ information.

Tab. G.9 shows network structures for the encoders and decoders of our formulation in (F.6). In the
table, FC denotes fully connected layer, Conv denotes convolutional layer and Conv Trans denotes
2D transposed convolutional layer. As before, the structures of ϕ[(][q][)] and τ [(][q][)] are MLPs with two

-----

Table G.9: Network structures for the Cars3D experiment.

Encoders Decoders

input: x[(]ℓ[q][)] _∈_ R[64][×][64][×][3] input: f [(][q][)](x[(]ℓ[q][)][)][ ∈] [R][10+2]
4 × 4 Conv, 32 ReLU, stride 2 FC 256, ReLU
4 × 4 Conv, 32 ReLU, stride 2 FC 4 × 4 × 64, ReLU
4 × 4 Conv, 64 ReLU, stride 2 4 × 4 Conv Trans, 64 ReLU, stride 2
4 × 4 Conv, 64 ReLU, stride 2 4 × 4 Conv Trans, 32 ReLU, stride 2
FC 256, ReLU 4 × 4 Conv Trans, 32 ReLU, stride 2
FC 10+2 4 × 4 Conv Trans, 3, stride 2


Figure G.6: Generated samples by fixing **_zℓ_** and varying **_c[(]j[q][)][; rows in blue boxes are w/][ R][; rows in]_**
green boxes are w/o R.
b b

hidden-layer and 256 neurons for each layer. For hyperparameters, we set batch size to be 1 =
_|B_ _|_
100 and 2 = 800, β = 0.1, λ = 1.0. For this real data experiment, we also use Adam (Kingma
_|B_ _|_
& Ba, 2015) as the optimizer with initial learning rate 0.001 for θ and 1.0 for η. We add ∥η∥2[2] [for]
regularization with parameter 0.0001. We limit the inner loops for solving the θ and η subproblems
to 10 epochs as well.

Figs. G.6 and G.7 show more results under the same setting as in Fig. 2 in the main text.

**dSprites Data for Cross-sample data Generation. We present the results on an additional dataset,**
i.e., dSprites (Higgins et al., 2017). In the dSprites dataset, 64 × 64 images are generated based on
five factors: 3 shapes (square, ellipse, heart), 6 scales, 40 orientations, and 32 different horizontal
and vertical coordinates.

In particular, we take a subset that contains squares and hearts as the two views of data. In this
subset, all the data samples are with the same scale. We assume the generating functions g[(1)]

and g[(2)] are responsible for the shape of square and heart, respectively. We treat the orientation
and horizontal positions as the shared information, i.e., z, and the vertical position as the private
information c[(][q][)]. The vertical coordinates are random and not matched between different pairs of
**_x[(1)]ℓ_** and x[(2)]ℓ [. Overall, we have][ 40][,][ 960][ samples for each view. We set][ D][ = 2][ and][ D][1][ =][ D][2][ = 1][.]

We use the same neural network structure as in Tab. G.9. The only differences lie in the input and
latent dimensions. The network structure for ϕ[(][q][)] and τ [(][q][)] are MLPs with two hidden layers of 128
neurons, and we set 1 = 100, 2 = 500, β = 0.1, and λ = 100.0. Similar as before, we use
_|B_ _|_ _|B_ _|_
the Adam (Kingma & Ba, 2015) optimizer with initial learning rate 0.001 for θ. As before, we add
a squared ℓ2 norm regularization on the network parameters η, and set the regularization parameter
to 0.1. We let the inner loop stochastic optimizers run for 10 epochs.


-----

Figure G.7: Generated samples by fixing **_c[(]j[q][)]_** and varying **_zl; rows in blue boxes are w/_** ; rows in
_R_
green boxes are w/o R.
b b

We conduct the same cross-sample data generation experiment as in the Cars3D case. Fig. G.8
shows the results. To be specific, we extract **_zℓ[(][q][)]_** = **_fS(x[(]ℓ[q][)][)][ that represents the rotation and]_**
horizontal coordinate information and **_c[(]j[q][)]_** = **_fP(x[(]ℓ[q][)][)][. And we combine this information to-]_**

gether to generate synthetic samples **_x[(]ℓ,j[q][)]_** [with the learned reconstruction network] b b **_[ r][(][q][)][, i.e.,][ b]x[(]ℓ,j[q][)]_** [=]

b b

**_r[(][q][)]([(zℓ[(][q][)][)]⊤, (c[(]j[q][)][)]⊤]⊤)._**

b

The observations are similar to that in the Cars3D experiments. Ideally, the generated samples

b b

_should have the rotation and horizontal position of x[(]ℓ[q][)]_ _(contained in_ **_zℓ[(][q][)][) while the vertical po-]_**
_sition of x[(]j[q][)]_ _(contained in_ **_c[(]j[q][)][).][ One can see that without using the independence regularizer][ R][,]_**

the generated samples may have rotation change, shape deformation compared to b **_x[(]ℓ[q][)]_** or simply the
vertical position is not exactly replicated from the sample b **_x[(]j[q][)][. However, with the][ R][ regularization,]_**
the results are exactly what one expects to see. This again verifies our claim in Theorem 2.

**Multiview MNIST Data for Cross-view Generation. Using the multiview MNIST data, we also**
show the cross-view generation results in Fig. G.9. Here, we extract **_zℓ[(2)]_** = **_fS[(2)][(][x]ℓ[(2)][)][ from the]_**
second view and **_c[(1)]j_** = **_fP[(1)][(][x]j[(1)][)][ from the first. Then, we generate][ b]x[(1)]ℓ,j_** [=][ b]r[(1)]([(zℓ[(2)][)]⊤, (c[(1)]j [)]⊤]⊤)
shown in the blue and green boxes. Ideally, the generated samples should have the digit information b [b]
_of x[(2)]ℓ_ _(contained in b_ **_zℓ[(2)][b][) while the style information of][ x]j[(1)]_** _(contained in_ **_c[(1)]j_** _[).][ Clearly, using]b_ _[ R]_
attains the desired results. Note that this dataset is challenging as the noise in view 2 is very high,
making it hard to achieve perfect matching and reconstruction—but our result is still plausible.
b b

**Remark 2 We would like to mention that multiview data and pertinent learning tasks are perva-**
_sive in the real world. For example, acoustic features and articulatory recordings are two views of_
_speech signals, and multiview based representation learning can be used to enhance speech recog-_
_nition (Arora & Livescu, 2013; Wang et al., 2015). Another example is cross-media information_
_retrieval (Gong et al., 2014). There, a data entity has a text view and an image view, and the task_
_is to retrieve one view from another. This task can be efficiently done in the learned shared do-_
_main. In natural language processing, multilingual word embedding can also be formulated as a_
_CCA-type shared information learning problem; see (Socher & Fei-Fei, 2010; Dhillon et al., 2012)._
_In computer vision, there are a number of important tasks such as image style translation (e.g.,_
_sketch to picture and picture to cartoon) (Zhu et al., 2017; Huang et al., 2018; Lee et al., 2018) and_
_super-resolution (Ledig et al., 2017) can be considered as multiview learning problems. In particu-_
_lar, image style translation will benefit from our method’s guaranteed shared (content) and private_
_(style) disentanglement._


-----

Figure G.8: Generated samples by fixing **_zℓ_** (rotation and horizontal position) and varying **_c[(]j[q][)]_**
(vertical position). Top: the square view; bottom: the heart view; rows in blue boxes are w/ R; rows
in green boxes are w/o .
_R_ b b

Figure G.9: Cross-view generation from x[(2)]ℓ to x[(1)]ℓ [.]

H ADDITIONAL NOTES ON SHARED-PRIVATE MODELING

Regarding the generative model in (5), some remarks are as follows. The intuition that multiview
data consists of shared and private components are widely used; see, e.g., (Huang et al., 2018; Lee
et al., 2018; Wang et al., 2016; Gundersen et al., 2019). However, explicit generative models were
only considered in limited theory-oriented works.


-----

The model in (5) can be understood as a nonlinear generalization of the linear CCA model in
(Ibrahim & Sidiropoulos, 2020), where the views are modeled as

**_x[(]ℓ[q][)]_** = A[(][q][)][z[⊤]ℓ _[,][ (][c][(]ℓ[q][)][)]⊤]⊤_

for q = 1, 2. In (Lyu & Fu, 2020), a special type of nonlinear model, namely, the post-nonlinear
mixture model, was analyzed. There, the model is

**_x[(]ℓ[q][)]_** = g[(][q][)](A[(][q][)][z[⊤]ℓ _[,][ (][c][(]ℓ[q][)][)]⊤]⊤),_

where g[(][q][)](y) applies a nonlinear distortion to each element of y individually. However, postnonlinear models are much less general compared to our model in (5)—where g[(][q][)](y) nonlinearly
distorts all elements of y jointly in an unknown way. More recently, under the context of AM-SSL,
the work in (Von K¨ugelgen et al., 2021) considered a multiview generative model that is similar to
our model, but the views share the same generative nonlinear function, i.e.,

**_x[(]ℓ[q][)]_** = g([z[⊤]ℓ _[,][ (][c][(]ℓ[q][)][)]⊤]⊤)._

This assumption restricts the applicability of the model to scenarios where the two views are generated using exactly the same nonlinear distortions, which may be less flexible. Our model in (5)
subsumes the models in (Ibrahim & Sidiropoulos, 2020; Lyu & Fu, 2020; Von K¨ugelgen et al., 2021)
as its special cases.

I AVOIDING RECONSTRUCTION USING ENTROPY REGULARIZATION

I.1 ENTROPY REGULARIZATION AND SHARED COMPONENT IDENTIFIABILITY

If we ignore private information extraction, our formulation for shared information extraction is as
follows:

2[]
minimize **_fS[(1)]_** **_x[(1)][]_** **_fS[(2)]_** **_x[(2)][]_** (I.1a)
**_f_** [(1)],f [(2)][ E] _−_
  

subject to f [(][q][)] for q = 1, 2 are invertible, (I.1b)

E **_fS[(][q][)]_** **_x[(][q][)][]_** **_fS[(][q][)]_** **_x[(][q][)][][⊤][]_** = I, E **_fS_** **_x[(][q][)][i]_** = 0, q = 1, 2, (I.1c)
   h 

with the latent variables satisfying:

_p(z, c[(1)], c[(2)]) = p(z)p(c[(1)])p(c[(2)])._

We hope to encourage invertibility of f [(][q][)] without using a decoder reconstruction network. To this
end, we generalize the idea in Theorem 4.4 in (Von K¨ugelgen et al., 2021). Note that (Von K¨ugelgen
et al., 2021) deals with the case where only one f is learned (i.e., f [(1)] = f [(2)]). Here, we show that
this idea can be used under our case as well. To see this, let us consider the following formulation:

2[]
minimize E **_fS[(1)]_** **_x[(1)][]_** **_fS[(2)]_** **_x[(2)][]_** _H_ **_fS[(1)]_** **_x[(1)][]_** (I.2a)
**_fS[(1)],fS[(2)]_** _−_ _−_
    

subject to fS[(][q][)] : R[M][q] (0, 1)[D]. (I.2b)
_→_

where H(·) computes the differential entropy of its argument. The formulation still aims to match
the latent representations of the two views, but at the same time maximizes the entropy of the learned
features of a the first view. The proof of this case consists of three major steps.

**Step 1. It is straightforward to see that the optimal solution of (I.2) is**


**_z = fS[(1)][(][x][(1)][) =][ f][ (2)]S_** [(][x][(2)][)][,][ b]z ∼ Uniform(0, 1)[D]

since the first term has optimal value 0 when two view are perfectly matched, and the differential entropy of a random variable is maximized when the distribution onb (0, 1)[D] is uniform (Cover, 1999).
Next, following the idea in (Von K¨ugelgen et al., 2021), by the Darmois construction (Darmois,


-----

1951), there exists d(·) : Z → (0, 1)[D] which maps the ground-truth z to a uniform random variable
on (0, 1)[D]. Thus, one can construct an optimal solution of (I.2) as:

**_fS[(][q][)]_** = d **_g[(][q][)][][−][1][]_**
_◦_ 1:D


where the first D dimensions of the output of **_g[(][q][)][][−][1]_** are fed to d(·).

**Step 2. By using our proof technique in Theorem ** 1, employing the equation **_z = fS[(1)][(][x][(1)][) =]_**
**_fS[(2)][(][x][(2)][)][, it can be shown that][ b]z only depends on the shared component z but does not depend_**
on either c[(1)] or c[(2)], which we denote as **_z = γ(z). Note that the proof of this part holds since b_**
it only uses the latent correlation maximization (or fS[(][q][)] matching). Using the same derivation as in
Theorem 1, we have the Jacobian of h[(1)] as follows: b


**_J11[(1)]_** **_HS[(1)]_**
**_J21[(1)]_** **_J22[(1)]_**


**_J11[(1)]_** **0D×D1**
**_J21[(1)]_** **_J22[(1)]_**


**_J_** [(1)] =


which indicates that **_z only depends on z but not c[(1)]. The above also holds for the second view._**
Note that the possibility of fS[(][q][)] being a trivial constant solution (and thus making HS[(1)] = 0) is
ruled out since fS[(][q][)][’s entropy is maximized.] b

**Step 3. The last step in Theorem 1 is to use rank(J** [(1)]) = D + D1 to show that J11[(1)]

has full rank. There, rank(J [(1)]) = D + D1 is natural since f [(1)] is constructed to be invertible[∈] [R][D][×][D]
using an autoencoder (and thus f [(1)] _◦_ **_g[(1)]_** is also invertible). Here, we could not use this argument.
However, similar to Theorem 4.4 in (Von K¨ugelgen et al., 2021), by applying Proposition 5 of
(Zimmermann et al., 2021), one can show that **_z = γ(z) where γ(·) is an invertible function, if_**
_p(z) is a regular density, i.e., 0 < p(z) < ∞_ everywhere. Note that under our generative model,
**_f_** [(1)](x[(1)]) = f [(2)](x[(2)]) for all x[(][q][)]. Hence, the above derivations can be repeated for f [(2)]. This
b
concludes the proof.

I.2 REALIZATION AND CONNECTION TO CONTRASTIVE SSL

To implement the formulation (I.2), following the idea in (Von K¨ugelgen et al., 2021), one can use
the idea of InfoNCE (Gutmann & Hyv¨arinen, 2010; Oord et al., 2018), which it has interesting
connections to contrastive SSL (Wang & Isola, 2020). In particular, the formulation of InfoNCE is
as follows:

_K_

exp sim(zi, **_zi[′][)][/τ]_** _[}]_

E{x(1)ℓ _,x[(2)]ℓ_ _}ℓ[K]=1[∼][p][(][x][(1)][,][x][(2)][)]_ "− _i=1_ log _Kj=1_ [exp]{ _[{][sim][(]z[b]i,_ **_zj[′]_** [)][/τ] _[}]_ # (I.3)

X b b

P

where **_zℓ_** and **_zℓ[′]_** [are the learned representations of the two corresponding samples] b **_[ x][(1)]ℓ_** and x[(2)]ℓ [,]
respectively, sim(a, b) computes the similarity of its arguments, τ is a temperature hyperparameter
and there are K samples of each batch where K 1 of them are negative.
b b _−_

Note that in (Von K¨ugelgen et al., 2021), only one generative function g(·) is considered. In their
implementation, given sample pairs {x[(1)]ℓ _[,][ x][(2)]ℓ_ _[}]ℓ[K]=1[, the above InfoNCE objective can be rewritten]_
with τ = 1 and sim(a, b) = −∥a − **_b∥2[2]_** [as]


_._



[]






**_f_** (x[(1)]i [)][ −] **_[f]_** [(][x]i[(2)][)]


2
exp **_f_** (x[(1)]i [)][ −] **_[f]_** [(][x]j[(2)][)]
_−_ 2
_j=1_ 

X

(I.4)


E{x(1)ℓ _,x[(2)]ℓ_ _}ℓ[K]=1[∼][p][(][x][(1)][,][x][(2)][)]_


2 [+ log]


_i=1_


The second term is a non-parametric entropy estimator of the representation as K →∞ (Wang &
Isola, 2020). The above nicely connects AM-SSL with contrastive learning when g[(1)] = g[(2)] and
only one encoder is used, i.e., f [(1)] = f [(2)].


-----

However, in our problem the generative functions are different in each view. Hence, the formulation
above is not directly applicable. Nonetheless, one can use the slack variable based design as in (10).
Then, the problem can be reformulated as


2
**_ui −_** **_f_** [(][q][)](x[(]i[q][)][)] + log


_j=1_ exp{−∥ui − **_uj∥2[2][}]_**

X


_._



(I.5)


E{x(1)ℓ _,x[(2)]ℓ_ _}ℓ[K]=1[∼][p][(][x][(1)][,][x][(2)][)]_


_i=1_


_q=1_


Note that the entropy regularization is imposed on the slack variable u—which indirectly promotes
high entropy of f [(][q][)]’s. This way, one can handle Q views with different generative functions g[(][q][)]’s.


-----

