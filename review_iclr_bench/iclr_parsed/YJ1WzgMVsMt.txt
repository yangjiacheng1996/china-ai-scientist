# REINFORCEMENT LEARNING WITH SPARSE REWARDS
## USING GUIDANCE FROM OFFLINE DEMONSTRATION

**Desik Rengarajan, Gargi Vaidya, Akshay Sarvesh, Dileep Kalathil & Srinivas Shakkottai**
Department of Electrical and Computer Engineering, Texas A&M University
_{desik,gargivaidya,sarvesh,dileep.kalathil,sshakkot}@tamu.edu_

ABSTRACT

A major challenge in real-world reinforcement learning (RL) is the sparsity of
reward feedback. Often, what is available is an intuitive but sparse reward function
that only indicates whether the task is completed partially or fully. However,
the lack of carefully designed, fine grain feedback implies that most existing RL
algorithms fail to learn an acceptable policy in a reasonable time frame. This is
because of the large number of exploration actions that the policy has to perform
before it gets any useful feedback that it can learn from. In this work, we address
this challenging problem by developing an algorithm that exploits the offline
demonstration data generated by a sub-optimal behavior policy for faster and
efficient online RL in such sparse reward settings. The proposed algorithm, which
we call the Learning Online with Guidance Offline (LOGO) algorithm, merges
a policy improvement step with an additional policy guidance step by using the
offline demonstration data. The key idea is that by obtaining guidance from - not
imitating - the offline data, LOGO orients its policy in the manner of the sub-optimal
policy, while yet being able to learn beyond and approach optimality. We provide a
theoretical analysis of our algorithm, and provide a lower bound on the performance
improvement in each learning episode. We also extend our algorithm to the even
more challenging incomplete observation setting, where the demonstration data
contains only a censored version of the true state observation. We demonstrate
the superior performance of our algorithm over state-of-the-art approaches on
a number of benchmark environments with sparse rewards and censored state.
Further, we demonstrate the value of our approach via implementing LOGO on
a mobile robot for trajectory tracking and obstacle avoidance, where it shows
excellent performance.

1 INTRODUCTION

Reinforcement Learning (RL) is being considered for application to a variety of real-world settings
that have very large state-action spaces, but even under the availability of accurate simulators
determining how best to explore the environment is a major challenge. Oftentimes, the problem
setting is such that intuitively obvious reward functions are quite sparse. Such rewards typically
correspond to the attainment of a particular task, such as a robot attaining designated way points, with
little fine-grain reward feedback on the intermediate steps taken to complete these tasks. The sparsity
of rewards implies that existing RL approaches often do not make much headway into learning viable
policies unless provided with carefully hand-tuned reward feedback by a domain expert.

Many of these problems correspond to systems where data has been gathered over time using an
empirically determined (sub-optimal) behavior policy, which contains information important to help
bootstrap learning. An additional caveat is that this behavior data might only contain measurements of
a subset of the true state. Is it possible to design an algorithm that has principled policy optimization
for efficient learning, while also being able to explore by utilizing the data derived from a behavior
policy? A natural candidate to begin with is the framework of policy gradient algorithms (Schulman
et al., 2015; Lillicrap et al., 2016; Haarnoja et al., 2018), which performs really well in the dense
reward setting. Can such a policy gradient algorithm be aligned with a behavior policy for guided
exploration that is critical in the sparse reward setting?


-----

In this work, we propose a principled approach for using the well known trust region policy optimization (TRPO) algorithm (Schulman et al., 2015) with offline demonstration data for guidance in a
sparse reward setting. Our choice of the TRPO approach is motivated by its analytical tractability and
superior performance. Our key insight is that we can utilize the trust region approach while being
guided by the behavior data by virtue of two steps. The first step is identical to traditional TRPO to
generate a candidate policy. In the second step, the objective is to find a policy closest to the behavior
policy, subject to it being in the trust region of the candidate generated in the first step. Thus, the
second step ensures that the policy chosen is always guided by the behavior policy, but the level of
alignment with the behavior policy can be reduced by shrinking the trust region over time. We call
our approach Learning Online with Guidance Offline (LOGO) to capture this two-step procedure.
This principled approach enables us to provide analytical performance guarantees while achieving
superior performance in a number of benchmark problems.

Our main results are as follows: (i) LOGO can guarantee a performance improvement as long as
the behavior policy is able to offer an advantage. Reducing the alignment with the behavior policy
over time allows us to extract this advantage and then smoothly move beyond to find near-optimal
policies, (ii) we provide a generalized version of the Performance Difference Lemma (Kakade &
Langford, 2002) wherein the stage reward function can depend on the policy itself, and use this result
to determine a surrogate objective function for step 2 of LOGO that is straightforward to evaluate.
This allows us to implement LOGO in the manner of two TRPO-like steps, enabling us to leverage
the TRPO code base, (iii) we show on standard MuJoCo environments that LOGO trained with sparse
rewards can attain nearly the same performance as an optimal algorithm trained with dense rewards.
We go further and show that this excellent performance carries over to waypoint tracking by a robot
over Gazebo simulations and real-world TurtleBot experiments, (iv) finally, we show that LOGO
can also be used in the case where the demonstration data only contains a censored version of the
true state (incomplete state information) by simply adding a projection to the available subset of the
state space in step 2 of the algorithm. Again, we provide supporting evidence via excellence in both
MuJoCo simulations, as well as obstacle avoidance by a TurtleBot that is trained with a behavior
policy without a Lidar (censored state), but is tested with it (full state).

1.1 RELATED WORK

Our work is mainly related to two RL research areas:
**Imitation Learning (IL): The goal of an IL algorithm is to imitate an (expert) policy using the**
demonstration data generated by that policy. Behavior cloning (BC) is a simple IL approach where
the expert policy is estimated from the demonstration data using supervised learning. BC algorithms,
however, suffer from the problem of distribution shift (Ross et al., 2011). Inverse reinforcement
learning (IRL) algorithms (Ng & Russell, 2000; Ziebart et al., 2008) estimate a reward function
from the demonstration data and solve a forward RL problem using this reward function. Generative
adversarial imitation learning (GAIL) (Ho & Ermon, 2016) avoids the reward estimation problem by
formulating the IL problem as a distribution matching problem, and provides an implicit reward for
the RL algorithm using a discriminator (Goodfellow et al., 2014). Most IL algorithms do not use
reward feedback from the environment, and hence are restricted to the performance of the policy that
generated the demonstration data. Our approach is different from pure IL, and we leverage online RL
with (sparse) reward feedback for efficient learning.

**Learning from Demonstration (LfD): The key idea of the LfD algorithms is to use demonstration**
data to aid online learning (Schaal et al., 1997). Many works propose to exploit demonstration data by
adding it to the replay buffer with a prioritized replay mechanism to accelerate learning (Hester et al.,
2018; Vecerik et al., 2017; Nair et al., 2018). Rajeswaran et al. (2018) combine a policy gradient
algorithm with demonstration data by using a mix of behavior cloning and online RL fine tuning. Nair
et al. (2020) propose AWAC algorithm to accelerate online RL by leveraging large amounts of offline
data with associated rewards. Different from this, LOGO does not need the reward observations.
Moreover, we give provable guarantees on the performance of LOGO whereas AWAC does not have
any such provable guarantee (further details are provided in Appendix G). Kim et al. (2013) propose
a framework to integrate LfD and approximate policy iteration by formulating a coupled constraint
convex optimization problem, where the expert demonstrations define a set of linear constraints.
This approach is, however, limited to small problems with a discrete action space. The closest to
our work is the PofD algorithm proposed by Kang et al. (2018). PofD modifies the reward function
by taking a weighted combination of the sparse reward from the online interaction and an implicit


-----

reward obtained from the demonstration data using a discriminator. Very different from this, we
propose an intuitive and principled approach of using the offline demonstration data for guiding the
online exploration during the initial phase of learning. Our two step approach enables us to provide a
rigorous performance guarantee for the proposed LOGO algorithm, and to leverage trust region-based
approaches to solve high dimensional problems with even sparser settings. We provide an extensive
comparison between our algorithm and PofD in Section 5.

**Offline RL: Recently, there has been many interesting works in the area of offline RL (Kumar et al.,**
2019; Fujimoto et al., 2019; Siegel et al., 2020; Wu et al., 2019a) which focus on learning a policy
using only the offline data without any online learning or online fine-tuning. Different from these,
LOGO is an online RL algorithm (further details are provided in Appendix G).

2 PRELIMINARIES AND PROBLEM SETTING

2.1 PRELIMINARIES

A Markov Decision Process (MDP) is denoted as a tuple < S, A, R, P, γ >, where S is the state
space, A is the action space, R : S × A → R is the reward function, P : S × A × S → [0, 1] is the
transition probability function with P (s[′]|s, a) giving the probability of transitioning to state s[′] when
action a is taken at state s, and γ ∈ (0, 1) is the discount factor. A policy π is a mapping from S
to probability distribution over A, with π(s, a) specifying the probability of taking action a in state
_s. A policy π can generate state-action trajectory τ_, where τ = (s0, a0, s1, a1, . . .), s0 _µ, at_
_π(st,_ ), st+1 _P_ ( _st, at) and µ is the initial state distribution. The infinite horizon discounted ∼_ _∼_
return of policy · _∼ π is defined as·|_ _JR(π) = Eτ_ _∼π [[P][∞]t=0_ _[γ][t][R][(][s][t][, a][t][)]][. The goal of a reinforcement]_
learning algorithm is to learn the optimal policy π[⋆] = arg maxπ JR(π).

The value function of a policy π defined as VR[π][(][s][) =][ E][τ] _[∼][π][ [][P]t[∞]=0_ _[γ][t][R][(][s][t][, a][t][)][|][s][0][ =][ s][]][, is]_
the expected cumulative discounted reward obtained by following the policy π starting from
the state s. The action-value function of a policy π is defined similarly as Q[π]R[(][s, a][)] =
Eτ _∼π [[P][∞]t=0_ _[γ][t][R][(][s][t][, a][t][)][|][s][0][ =][ s, a][0][ =][ a][]][.]_ The advantage function is defined as A[π]R[(][s, a][) =]
_Q[π]R[(][s, a][)][ −]_ _[V][ π]R_ [(][s][)][. The discounted state visitation distribution for the policy][ π][, denoted as][ d][π][,]
is defined as d[π](s) = (1 − _γ)_ _t=0_ _[γ][t][P][(][s][t][ =][ s][|][π][)][, where the probability is defined with respect to]_
the randomness induced by π, P and µ.

**Definitions and Notations: The Kullback-Leibler (KL) divergence between two distribution[P][∞]** _p_
and q is defined as DKL(p, q) = _x_ _[p][(][x][) log][ p]q([(]x[x])[)]_ [. The average KL divergence between two po-]

lices π1 and π2 with respect to d[π][1] is defined as DKL[π][1] [(][π][1][, π][2][) =][ E][s][∼][d][π][1][ [][D][KL][(][π][1][(][s,][ ·][)][, π][2][(][s,][ ·][))]][.]
The maximum KL divergence between two polices[P] _π1 and π2 is defined as DKL[max][(][π][1][, π][2][) =]_
maxs DKL(π1(s, ·), π2(s, ·)). The total variation (TV) distance between two distribution p and q is
defined as DTV(p, q) = (1/2) _x_

tance between two polices π1 and π[|]2[p] are defined as[(][x][)] _[−]_ _[q][(][x][)][|][. The average TV distance and the maximum TV dis-] DTV[π][1]_ [(][π][1][, π][2][) =][ E][s][∼][d][π][1][ [][D][TV][(][π][1][(][s,][ ·][)][, π][2][(][s,][ ·][))]]
and DTV[max][(][π][1][, π][2][) = max][s][ D][P][TV][(][π][1][(][s,][ ·][)][, π][2][(][s,][ ·][))][, respectively.]

2.2 PROBLEM SETTING


As described in the introduction, our goal is to develop an algorithm that can exploit offline demonstration data generated using a sub-optimal behavior policy for faster and efficient online reinforcement
learning in a sparse reward setting. Formally, we assume that the algorithm has access to the demonstration data generated by a sub-optimal behavior policy πb. We first consider the setting in which
the demonstration data has complete state observation. In this setting, the demonstration data has the
form D = {τ _[i]}i[n]=1[, where][ τ][ i][ = (][s]1[i]_ _[, a][i]1[, . . ., s][i]T_ _[, a]T[i]_ [)][,][ τ][i][ ∼] _[π][b][. Later, we also propose an extension]_
to the incomplete observation setting in which only a censored version of the true state is available in
demonstration data. More precisely, instead of the complete state observation s[i]t[, the demonstration]
data will only contain ˜s[i]t [=][ o][(][s]t[i][)][, where][ o][(][·][)][ is a projection to a lower dimensional subspace. We]
represent the incomplete demonstration data as _D[˜] = {τ˜[i]}i[n]=1[, where][ ˜]τ_ _[i]_ = (˜s[i]1[, a][i]1[, . . .,][ ˜]s[i]T _[, a]T[i]_ [)][.]

We make the following assumption about the behavior policy πb.

**Assumption 1. In the initial episodes of learning, Ea∼πb [A[π]R[k]** [(][s, a][)]][ ≥] _[β >][ 0][,][ ∀][s,][ where][ π][k][ is the]_
_learning policy employed by the algorithm in the kth episode of learning._


-----

Intuitively, the above assumption implies that taking action according to πb will provide a higher
advantage than taking actions according to πk because Ea∼πk [A[π]R[k] [(][s, a][)] = 0][. This is a reasonable]
assumption, since the behavior policies currently in use in many systems are likely to perform much
better than an untrained policy. We also note that a similar assumption is made by Kang et al. (2018)
to formalize the notion of a useful behavior policy. We emphasize that πb need not be the optimal
policy, and πb could be such that JR(πb) < JR(π[⋆]). Our proposed algorithm learns a policy that
performs better than πb through online learning with guided exploration.

3 ALGORITHM AND PERFORMANCE GUARANTEE

3.1 ALGORITHM

In this section, we describe our proposed LOGO algorithm. Each iteration of our algorithm consists
of two steps, namely a policy improvement step and a policy guidance step. In the following, πk
denotes the policy after k iterations.

**Step 1: Policy Improvement: In this step, the LOGO algorithm performs a one step policy improve-**
ment using the Trust Region Policy Optimization (TRPO) approach (Schulman et al., 2015). This
can be expressed as

_πk+1/2 = arg maxπ_ Es∼d[π]k,a∼π [A[π]R[k] [(][s, a][)]] s.t. _DKL[π][k]_ [(][π, π][k][)][ ≤] _[δ.]_ (1)

The TRPO update finds the policy πk+1/2 that maximizes the objective while constraining this
maximizing policy to be within the trust region around πk defined as {π : DKL[π][k] [(][π, π][k][)][ ≤] _[δ][}][.]_
The TRPO approach provides a provable guarantee on the performance improvement, as stated in
Proposition 1. We omit the details as this is a standard approach in the literature.

**Step 2: Policy Guidance: While the TRPO approach provides an efficient online learning strategy**
in the dense reward setting, it fails when the reward structure is sparse. In particular, it fails to achieve
any significant performance improvement for a very large number of episodes in the initial phase
of learning due to the lack of useful reward feedback that is necessary for efficient exploration. We
propose to overcome this challenge by providing policy guidance using offline demonstration data
after each step of the TRPO update. The policy guidance step is given as

_πk+1 = arg min_ _DKL[π]_ [(][π, π][b][)] s.t. _DKL[max][(][π, π]k+[1]/[2][)][ ≤]_ _[δ][k][.]_ (2)
_π_

Intuitively, the policy guidance step aids learning by selecting the policy πk+1 in the direction of the
behavior policy πb. This is achieved by finding a policy that minimizes the KL divergence w.r.t. πb,
but at the same time lies inside the trust region around πk+1/2 defined as {π : DKL[max][(][π, π][k][+][1][/][2][)][ ≤] _[δ][k][}][.]_
This trust region-based policy guidance is the key idea that distinguishes LOGO from other approaches.
In particular, this approach gives LOGO two unique advantages over other state-of-the-art algorithms.

First, unlike imitation learning that tries to mimic the behavior policy by directly minimizing the
distance (typically KL/JS divergence) between it and the current policy, LOGO only uses the behavior
policy to guide initial exploration. This is achieved by starting the guidance step with a large value
of the trust region δk, and gradually decaying it according to an adaptive update rule (specified in
Appendix F) as learning progresses. This novel approach of trust region based policy improvement
and policy guidance enables LOGO to both learn faster in the initial phase by exploiting demonstration
data, and to converge to a better policy than the sub-optimal behavior policy.

Second, from an implementation perspective, the trust region based approach allows to approximate
the objective by a surrogate function that is amenable to sample based learning (see Proposition 2).
This allows us to implement LOGO in the manner of two TRPO-like steps, enabling us to leverage
the TRPO code base. Details are given Section 4.

3.2 PERFORMANCE GUARANTEE

We derive a lower bound on the performance improvement, JR(πk+1) _JR(πk) for LOGO in each_
_−_
learning episode. We analyze the policy improvement step and policy guidance step separately. We
begin with the performance improvement due to the TRPO update (policy improvement step). The
following result and its analysis are standard in the literature and we omit the details and the proof.


-----

**Proposition 1 (Proposition 1, (Achiam et al., 2017)). Let πk and πk+1/2 are related by (1). Then,**


2δγϵR,k/(1 _γ)[2],_ (3)
_−_


_JR(πk+1/2)_ _JR(πk)_
_−_ _≥−_

_where ϵR,k = maxs,a |A[π]R[k]_ [(][s, a][)][|][.]


We now give the following result which can be used to obtain a lower bound on the performance
improvement due to the policy guidance step.
**Lemma 1. Let πk+1/2 be a policy that satisfies Assumption 1. Then, for any policy π,**


_JR(π)_ _JR(πk+1/2)_ (1 _γ)[−][1]β_ (1 _γ)[−][1]ϵR,k+1/2_
_−_ _≥_ _−_ _−_ _−_


2DKL[π] [(][π, π][b][)][,] (4)


_πk+1/2_
_Where ϵR,k+1/2 = maxs,a_ _AR_ (s, a) _._
_|_ _|_

_Remark 1. Lemma 1 also gives an intuitive explanation for our proposed policy guidance step. The_
policies used in the initial phase of the learning can be far from the optimal. So, it is reasonable
to assume that πk (and hence πk+1/2) satisfies Assumption 1 in the initial phase of learning. Then,
minimizing DKL[π] [(][π, π][b][)][ can get a non-negative lower bound in][ (4)][, which will imply that the]
performance of πk+1 is better than πk+1/2. This is indeed the idea behind the policy guidance step.

Combining the results of Proposition 1 and Lemma 1, and with some more analysis, we get the
following performance improvement guarantee for the LOGO algorithm.
**Theorem 1. Let πk and πk+1/2 are related by (1) and let πk+1/2 and πk+1 are related by 2. Let ϵR,k**
(andi) If ϵ πR,kk++11//22 satisfies Assumption 1, then be as defined in Proposition 1 and Lemma 1, respectively. Let Rmax = maxs,a |R(s, a)|.


_√2δγϵR,k_
_JR(πk+1)_ _JR(πk)_ +
_−_ _≥_ _[−](1_ _γ)[2]_

_−_

(ii) If πk+1/2 does not satisfy Assumption 1, then


_β_

(1 − _γ)_ _[−]_ _[ϵ](1[R,k] −[+]γ[1][/])[2]_


2DKL[π] [(][π][k][+1][, π][b][)][.] (5)


_JR(πk+1) −_ _JR(πk) ≥−(√2δγϵR,k + 3Rmaxδk)/(1 −_ _γ)[2]._ (6)

_Remark 2. In the initial phase of learning when the baseline policy is better than the current policy,_
the policy guidance step can add a non-negative term to the standard lower bound obtained by the
TRPO approach, as shown in (5). This indicates faster learning in the initial phase as compared to
the naive TRPO approach. The policy improvement guarantee during the later phase of learning
is given by (6), which shows that LOGO achieves similar performance guarantee as TRPO when
_δk = O(√δ). We ensure this by decreasing the value of δk as the learning progresses. Thus, Theorem_

1 clearly shows the key advantage of the LOGO algorithm achieved by the novel combination of the
policy improvement step and the policy guidance step.

4 PRACTICAL ALGORITHM

We first develop an approximation to the policy guidance step (2) that is amenable to sample-based
learning and can scale to policies paramaterized by neural networks. This step involves minimizing
_DKL[π]_ [(][π, π][b][)][ under a trust region constraint. However, this is not easy to solve directly by a sample-]
based learning approach, because estimating it requires samples generated according to any possible
_π, which is clearly infeasible. To overcome this issue, inspired by the surrogate function idea used in_
the TRPO algoirthm (Schulman et al., 2015), we derive a surrogate function for DKL[π] [(][π, π][b][)][ that can]
be estimated using only the samples from the policy πk+1/2.

For deriving a surrogate function for DKL[π] [(][π, π][b][)][ that is amenable to sample-based learning, we first]
define the policy dependent reward function Cπ as Cπ(s, a) = log(π(s, a)/πb(s, a)). Using Cπ, we
can also define the quantities JCπ (˜π), VCπ[˜]π _[, Q]πC[˜]_ _π_ [and][ A]πC[˜] _π_ [for any policy][ ˜]π, exactly as defined in
Section 2.1 by replacing R by Cπ. Using these notations, we now present an interesting result which
we call the performance difference lemma for policy dependent reward function.
**Lemma 2. For any policies π and ˜π,**

_π_
_JCπ_ (π) _JCπ˜_ [(˜]π) = (1 _γ)[−][1]_ Es _dπ,a_ _π(s,_ )[AC[˜] _π˜_ [(][s, a][)] + (1][ −] _[γ][)][−][1][D]KL[π]_ [(][π,][ ˜]π). (7)
_−_ _−_ _∼_ _∼_ _·_


-----

Note that the above result has an additional term, (1 − _γ)[−][1]DKL[π]_ [(][π,][ ˜]π), compared to the standard
performance difference lemma (Kakade & Langford, 2002). In addition to being useful for analyzing
our algorithm, we believe that the above result may also be of independent interest.

We now make an interesting observation that JCπ (π) = (1 − _γ)[−][1]DKL[π]_ [(][π, π][b][)][ (proof is given in the]
Appendix), which can be used with Lemma 2 to derive the surrogate function given below.

**Proposition 2. Let πk+1/2 be as given in (1). Then, for any policy π that lies in the trust region**
_around πk+1/2 defined as {π : DKL[max][(][π, π][k][+][1][/][2][)][ ≤]_ _[δ][k][}][, we have]_

_DKL[π]_ [(][π, π][b][)][ ≤] _[α][k]_ [+][ E]s∼dπk+1/2 _,a∼π(s,·)[[][A]Cπkπk+1+1/2/2_ [(][s, a][)] +][ γ][(1][ −] _[γ][)][−][1][ϵ][π,k]_ 2δk + δk, (8)

_πk+1/2_ _πk+1/2_ p
_where αk = DKL_ (πk+1/2, πb), ϵπ,k = maxs,a |ACπk+1/2 [(][s, a][)][|][.]

Now, instead of minimizing DKL[π] [(][π, π][b][)][, we will minimize the upper bound as a surrogate objective]
function. Note that only one term in this surrogate objective function depends on π, and that term can
be estimated using only samples from πk+1/2. Thus, we will approximate the policy guidance step as

_πk+1 = arg minπ_ Es∼dπk+1/2 _,a∼π(s,·)[[][A]πCkπk+1+1/2/2_ [(][s, a][)]] s.t. _DKL[max][(][π, π]k+[1]/[2][)][ ≤]_ _[δ][k][.]_ (9)

Since DKL[max] [is difficult to implement in practice, we will further approximate it by average KL]πk+1/2
divergence DKL (π, πk+1/2). We note that this is a standard approach used in the TRPO algorithm.

We can now put steps (1) and (9) together to obtain the full algorithm. However, as the policies are
represented by large neural networks, solving them exactly is challenging. Hence, for sufficiently
small δ, δk, we can further approximate the objective functions and constraints by a Taylor series
expansion to obtain readily implementable update equations. This is a standard approach (Schulman
et al., 2015; Achiam et al., 2017), and we only present the final result below.

Consider the class of policies {πθ : θ ∈ Θ} where θ is the parameter of the policy. Let θk be the
parameter corresponding to policy πk. Then, the Taylor series expansion-based approximate solution
of (1) and (9) yields the final form of LOGO as follows:


2δ

_Fk[−][1][g][k][,]_ _θk+1 = θk+1/2_
_gk[T]_ _[F][ −]k_ [1][g][k] _−_


2δk

_L[−]k_ [1][h][k][,] (10)
_h[T]k_ _[L]k[−][1][h][k]_


_θk+1/2 = θk +_


where gk = _θ_ Es _d[π]k,a_ _πθ(s,_ ) [A[π]R[k] [(][s, a][)]][,] _Fk_ = _θ_ _DKL[π][k]_ [(][π][θ][, π][k][)][,] and hk =
_∇_ _∼πk+1/2∼_ _·_ _πk+1/2_ _∇[2]_
_∇θ Es∼dπk+1/2_ _,a∼πθ(s,·)[[][A]Cπk+1/2_ [(][s, a][)]][,][ L][k][ =][ ∇]θ[2] _[D]KL_ (πθ, πk+1/2).

While it is straightforward to compute A[π]Cπ [for any policy when the form of the baseline policy][ π][b]
is known, it is more challenging when only the demonstration data D generated according to πb is
available. We overcome this challenge by training a discriminator using the demonstration data D and
the data Dk+1/2 generated by the policy πk+1/2 (Goodfellow et al., 2014; Ho & Ermon, 2016; Kang
et al., 2018) that will approximate the policy dependent reward function Cπk+1/2 [. Further details on]
training this discriminator and a concise form of the algorithm are given in Appendix D.

4.1 EXTENSION TO INCOMPLETE OBSERVATION SETTING

We now discuss how to extend LOGO to the setting where the behavior policy data contains only
incomplete state observations. For instance, consider the problem of learning a policy for a mobile
robot to reach a target point without colliding with any obstacles. Collision avoidance requires
sensing the presence of obstacles, which is typically achieved by camera/Lidar sensors. Learning an
RL policy for this problem requires a high fidelity simulator that models various sensors and their
interaction with the dynamics. Such high fidelity simulators are, however, typically slow and difficult
to parallelize, and training an RL policy using such a simulator in a sparse reward environment can
be very time consuming and computationally expensive. Often, it is much easier to train an RL policy
for the trajectory tracking problem using only a simple kinematics simulator model which only has
a lower dimensional state space compared to the original problem. In particular, such a kinematics
simulator will only have the position and velocity of the robot as the state instead of the true state with


-----

high dimensional camera/Lidar image. Can we use the demonstration data or behavior policy from
this low dimensional simulator to accelerate the RL training in a high dimensional/fidelity simulator
in sparse reward environments? We answer this question affirmatively using a simple idea to extend
LOGO into such incomplete observation settings.

Since the behavior policy appears in LOGO only through a policy dependent reward function
_Cπ(s, a) = log(π(s, a)/πb(s, a)), we propose to replace this with a form that can handle the_
incomplete observation setting. For any state s ∈S, let ˜s = o(s) be its projection to a lower
dimensional state space _S[˜]. As explained in the mobile robot example above, a behavior policy_
_π˜b in the incomplete observation setting can be interpreted as mapping from_ _S[˜] to the set of prob-_
ability distributions over . We can then replace Cπ(s, a) in the LOGO algorithm with _C[˜]π(s, a)_
_A_
defined as _C[˜]π(s, a) = log(π(s, a)/π˜b(o(s), a)). When only the demonstration data with incomplete_
observation is available instead of the representation of the behavior policy, we propose to train a
discriminator to estimate _C[˜]π. Let_ _D[˜] = {τ˜[i]}i[n]=1_ [be the demonstration with incomplete observation,]
where ˜τ _[i]_ = (˜s[i]1[, a]1[i] _[, . . .,][ ˜]s[i]T_ _[, a]T[i]_ [)][. Then we train a discriminator using][ ˜]D and Dπ to estimate _C[˜]π._


5 EXPERIMENTS

We now evaluate LOGO from two perspectives: (i) Can LOGO learn near-optimally in a sparse reward
environment when guided by demonstration data generated by a sub-optimal policy? (ii) Can LOGO
retain near-optimal performance when guided by sub-optimal and incomplete demonstration data with
sparse rewards? We perform an exhaustive performance analysis of LOGO, first through simulations
under four standard (sparsified) environments on the widely used MuJoCo platform (Todorov et al.,
2012). Next, we conduct simulations on the Gazebo simulator (Koenig & Howard, 2004) using
LOGO for way-point tracking by a robot in environments with and without obstacles, with the
only reward being attainment of way points. Finally, we transfer the trained models to a real-world
TurtleBot robot (Amsters & Slaets, 2019) to demonstrate LOGO in a realistic setting.

0.05


0.00

0.05


LOGO PofD TRPO BC-TRPO GAIL DAPG Behavior Expert

Hopper-v20.04 0.02HalfCheetah-v2 0.00 Walker-v2 0.02 InvertedDoublePendulum-v20.04

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|

|Col1|Col2|
|---|---|
|||


0.0 0.5 1.0 Samples1.5 2.0 2.5 3.01e7


0.0 0.5 1.0 Samples1.5 2.0 2.5 3.01e7


0.0 0.5 1.0Samples1.5 2.0 2.5 3.01e7


0.0 0.5 Samples1.0 1.5 1e52.0


(a) Evaluation on MuJoCo with full offline observation.


Hopper-v2

3000

2000

1000

Average episodic reward

0

0.0 0.5 1.0 Samples1.5 2.0 2.5 3.01e7


HalfCheetah-v2

5000

4000

3000

2000

1000

0

-1000

-2000 0 1 Samples2 3 41e7


Walker-v2

4000

3000

2000

1000

0

0.0 0.5 1.0Samples1.5 2.0 2.5 3.01e7


InvertedDoublePendulum-v2

10000

8000

6000

4000

2000

0

0.0 0.5 Samples1.0 1.5 2.01e5


(b) Evaluation on MuJoCo with incomplete offline observation.

Figure 1: Evaluation of algorithms on four sparse reward MuJoCo environments with complete (1a)
and incomplete (1b) offline data. The solid line corresponds to the mean over five trials with different
seeds and the shaded region corresponds to the standard deviation over the trials.


In what follows, we present a summary of our experiments[1] and results. We remark that LOGO is
relatively easy to implement and train, since its two TRPO-like steps imply that we can utilize much
of a TRPO code base.

1code base and a video of the TurtleBot experiments: https://github.com/DesikRengarajan/LOGO


-----

5.1 MUJOCO SIMULATIONS

In our first set of experiments, we consider four standard environments using the MuJoCo platform.
We introduce sparsity by reducing the events at which reward feedback is provided. Specifically, for
Hopper, HalfCheetah and Walker2d, we provide a reward of +1 at each time only after the agent
moves forward over 2, 20, and 2 units from its initial position, respectively. For InvertedDoublePendulum, we introduce sparsity by providing a reward only at the end of the episode.

Besides LOGO, our candidate algorithms are as follows: (i) Expert: We train TRPO in the dense
reward environment to provide the optimal baseline, (ii) Behavior: We use a partially trained expert
that is still at a sub-optimal stage of learning to provide behavior data, (iv) GAIL: We use Generative
Adversarial Imitation Learning (Ho & Ermon, 2016), which attempts to imitate the Behavior policy
(iii) TRPO: We directly use TRPO in the sparse reward setting without any guidance from behavior
data (iv) POfD: We use Policy Optimization from Demonstration (Kang et al., 2018) as a heuristic
approach to exploiting behavior data. (v) BC-TRPO: We warm start TRPO by performing behavior
cloning (BC) on the sub-optimal behavior data. (vi) DAPG: We use Demo Augmented Policy
Gradient (Rajeswaran et al., 2018) which warm starts Natural Policy Gradient (NPG) algorithm using
BC, and fine tunes it online using behavior data in a heuristic manner. Note that for all algorithms, we
evaluate the final performance in the corresponding dense reward environment provided by OpenAI
Gym, which provides a standardized way of comparing their relative merits.

**Setting 1: Sparse Rewards. We compare the performance of our candidate algorithms in the sparse**
reward setting in Figure 1a, which illustrates their rewards during training. As expected, TRPO
fails to make much meaningful progress during training, while GAIL can at best attain the same
performance as the sub-optimal behavior policy. While BC-TRPO benefits from warm starting, it
fails to learn later due to the absence of online guidance as in the case of LOGO. POfD and LOGO
both use the behavior data to boot strap learning. POfD suffers from the fact that it is influenced
throughout the learning process by the behavior data, which prevents it from learning the best policy.
However, LOGO’s nuanced exploration using the demonstration data only as guidance enables it to
quickly attain optimality. LOGO outperforms DAPG in all but one environment.

**Setting 2: Sparse Rewards and Incomplete State. We next consider sparse rewards along with**
reducing the revealed state dimensions in the behavior data of Setting 1. These state dimensions are
selected by eliminating state dimensions revealed until the expert TRPO with dense rewards starts
seeing reduced performance. This ensures that we are pruning valuable information. Since GAIL,
POfD and LOGO all utilize the behavior data, we use the approach of projecting the appropriate
reward functions that depend on the behavior data into a lower dimensional state space described in
Section 4.1. We emphasize that BC-TRPO and DAPG cannot be extended to this setting as they
require full state information for warm starting (in BC-TRPO and DAPG) and online fine tuning (in
DAPG). We see from Figure 1b that LOGO is still capable of attaining good performance, although
training duration is increased. We will further explore the value of being able to utilize behavior data
with such incomplete state information in robot experiments in the next subsection.

5.2 TURTLEBOT EXPERIMENTS

We now evaluate the performance of LOGO in a real-world using TurtleBot, a two wheeled differential
drive robot (Amsters & Slaets, 2019). We train policies for two tasks, (i) Waypoint tracking and (ii)
Obstacle avoidance, on Gazebo, a high fidelity 3D robotics simulator.

**Task 1: Waypoint Tracking: The goal is to train a policy that takes the robot to an arbitrary**
waypoint within 1 meter of its current position in an episode of 20 seconds. The episode concludes
when the robot either reaches the waypont or the episode timer expires. The state space of the
agent are its relative x, y, coordinates and orientation φ to the waypoint. The actions are its linear
and angular velocities. The agent receives a sparse reward of +1 if it reaches the waypoint, and 0
otherwise. We created a sub-optimal Behavior policy by training TRPO with dense rewards on our
own low fidelity kinematic model Python-based simulator with the same state and action space. While
it shows reasonable waypoint tracking, the trajectories that it generates in Gazebo are inefficient,
and its real-world waypoint tracking is poor (Figures 2 (d)-(f)). As expected, TRPO shows poor
performance in this sparse reward setting and often does not attain the desired waypoint before the
episode concludes, giving the impression of aimless circling seen in Figure 2(f). LOGO is able to
effectively utilize the Behavior policy and shows excellent waypoint tracking seen in Figures 2 (d)-(f).


-----

(a) Gazebo setup (b) Real-world setup (c) Real-world 2D Lidar scan


Waypoint Tracking


1.50

1.25

1.00

0.75

0.50

0.25

0.00

0.25

0.50


1.0

0.8

Reward0.6

0.4

0.2


LOGO
TRPO

Behavior



1.4

1.2

1.0

0.8

0.6

0.4

0.2

0.0


Goal


|Col1|Col2|Col3|Col4|
|---|---|---|---|


LOGO
TRPO
Behavior
Waypoint

Goal

0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5

m

(f) Real-world evaluation


Waypoint


|Start|Col2|
|---|---|


0.0 0.5 1.0 1.5 2.0 2.5 3.0

(e) Gazebo evaluation

|Col1|LOGO TRPO Behavior|
|---|---|


0 1 2Samples3 4 5 1e4

(d) Waypoint tracking training


Obstacle Avoidance

1.00 LOGO

TRPO

0.75 Behavior

0.50

0.25

Reward 0.00

0.25

0.50

0.75

0.0 0.5 1.0 Samples1.5 2.0 2.5 3.01e5


1.00

0.75

0.50

0.25

Start

0.00

0.25 Goal

0.50

0.75 LOGO

TRPO

1.00 Behavior

0.0 0.5 1.0 1.5 2.0


0.8

0.6

0.4

0.2

0.0

Start Goal

0.2

LOGO

0.4 TRPO

Behavior

0.6 0.2 0.0 0.2 0.4 0.6 0.8 1.0

m


(g) Obstacle avoidance training


(h) Gazebo evaluation


(i) Real-world evaluation


Figure 2: Training and evaluation under Gazebo simulations and real-world experiments. (a)-(c) show
the experiment setup, with the TurtleBot added for visual reference in (c). (d)-(f) show waypoint
tracking performance. (g)-(i) show waypoint tracking with obstacle avoidance performance.

**Task 2: Obstacle Avoidance: The goal and rewards are the same as in Task 1, with a penalty of −1**
for collision with the obstacle, shown in Figure 2 (a)-(c). The complete state space is now augmented
by a 2D Lidar scan in addition to coordinates and orientation described in Task 1. However, the
Behavior policy is still generated via the low fidelity kinematic simulator without the obstacle, i.e., it
is created on a lower dimensional state space. As seen in Figures 2 (g)-(i), this renders the Behavior
policy impractical for Task 2, since it almost always hits the obstacle in both Gazebo and the realworld. However, it does possess information on how to track a waypoint, and when combined with
the full state information, this nugget is utilized very effectively by LOGO to learn a viable policy as
seen in Figures 2 (g)-(i). Further, TRPO in this sparse reward setting does poorly and often collides
with the obstacle in real-world experiments as seen in Figure 2 (i).


6 CONCLUSION

In this paper, we studied the problem of designing RL algorithms for problems in which only sparse
reward feedback is provided, but offline data collected from a sub-optimal behavior policy, possibly
with incomplete state information is also available. Our key insight was that by dividing the training
problem into two steps of (i) policy improvement and (ii) policy guidance using the offline data, each
using the concept of trust region based policy optimization, we can both obtain principled policy
improvement and desired alignment with the behavior policy. We designed an algorithm entitled
LOGO around this insight and showed how it can be instantiated in two TRPO-like steps. We both
proved analytically that LOGO can exploit the advantage of the behavior policy, as well as validated
its performance through both extensive simulations and illustrative real-world experiments.


-----

7 ACKNOWLEDGEMENT

This work was supported in part by the National Science Foundation (NSF) grants NSF-CRII-CPS1850206, NSF-CAREER-EPCN-2045783, NSF-CPS-2038963 and NSF-CNS 1955696, and U.S.
Army Research Office (ARO) grant W911NF-19-1-0367. Any opinions, findings, and conclusions or
recommendations expressed in this material are those of the authors and do not necessarily reflect the
views of the sponsoring agencies.

REFERENCES

Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In
_Proceedings of the 34th International Conference on Machine Learning, ICML 2017, pp. 22–31._
PMLR, 2017.

Robin Amsters and Peter Slaets. Turtlebot 3 as a robotics education platform. In Robotics in Education

_- Current Research and Innovations, Proceedings of the 10th RiE, Advances in Intelligent Systems_
and Computing, pp. 170–181, 2019.

Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without
exploration. In International Conference on Machine Learning, pp. 2052–2062. PMLR, 2019.

Yang Gao, Huazhe Xu, Ji Lin, Fisher Yu, Sergey Levine, and Trevor Darrell. Reinforcement learning
from imperfect demonstrations. arXiv preprint arXiv:1802.05313, 2018.

Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural
_Information Processing Systems, pp. 2672–2680, 2014._

C¸ aglar Gul¨ c¸ehre, Tom Le Paine, Bobak Shahriari, Misha Denil, Matt Hoffman, Hubert Soyer, Richard
Tanburn, Steven Kapturowski, Neil C. Rabinowitz, Duncan Williams, Gabriel Barth-Maron, Ziyu
Wang, Nando de Freitas, and Worlds Team. Making efficient use of demonstrations to solve hard
exploration problems. In International Conference on Learning Representations, ICLR, 2020.

Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy
learning: Solving long-horizon tasks via imitation and reinforcement learning. In Conference on
_Robot Learning, pp. 1025–1037, 2019._

Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Conference
_on Machine Learning, ICML, pp. 1856–1865, 2018._

Todd Hester, Matej Vecer´ık, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Horgan,
John Quan, Andrew Sendonaris, Ian Osband, Gabriel Dulac-Arnold, John P. Agapiou, Joel Z.
Leibo, and Audrunas Gruslys. Deep q-learning from demonstrations. In AAAI Conference on
_Artificial Intelligence, pp. 3223–3230, 2018._

Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural
_Information Processing Systems, pp. 4565–4573, 2016._

Mingxuan Jing, Xiaojian Ma, Wenbing Huang, Fuchun Sun, Chao Yang, Bin Fang, and Huaping
Liu. Reinforcement learning from imperfect demonstrations under soft expert guidance. In AAAI
_Conference on Artificial Intelligence, volume 34, pp. 5109–5116, 2020._

Sham M. Kakade and John Langford. Approximately optimal approximate reinforcement learning.
In International Conference Machine Learning, pp. 267–274, 2002.

Bingyi Kang, Zequn Jie, and Jiashi Feng. Policy optimization with demonstrations. In International
_Conference on Machine Learning, pp. 2469–2478, 2018._

Beomjoon Kim, Amir-massoud Farahmand, Joelle Pineau, and Doina Precup. Learning from limited
demonstrations. In Advances in Neural Information Processing Systems, pp. 2859–2867, 2013.


-----

Nathan P. Koenig and Andrew Howard. Design and use paradigms for gazebo, an open-source
multi-robot simulator. In IEEE/RSJ International Conference on Intelligent Robots and Systems,
pp. 2149–2154, 2004.

Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy
q-learning via bootstrapping error reduction. Advances in Neural Information Processing Systems,
32:11784–11794, 2019.

Gabriele Libardi, Gianni De Fabritiis, and Sebastian Dittert. Guided exploration with proximal policy
optimization using a single demonstration. In International Conference on Machine Learning, pp.
6611–6620, 2021.

Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In
_International Conference on Learning Representations ICLR, 2016._

Ashvin Nair, Bob McGrew, Marcin Andrychowicz, Wojciech Zaremba, and Pieter Abbeel. Overcoming exploration in reinforcement learning with demonstrations. In IEEE International Conference
_on Robotics and Automation (ICRA), pp. 6292–6299, 2018._

Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement
learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.

Andrew Y. Ng and Stuart J. Russell. Algorithms for inverse reinforcement learning. In International
_Conference on Machine Learning (ICML), pp. 663–670, 2000._

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep
learning library. In Advances in Neural Information Processing Systems, pp. 8024–8035. 2019.

Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel
Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement
learning and demonstrations. In Robotics: Science and Systems, 2018.

Stephane Ross, Geoffrey J. Gordon, and Drew Bagnell. A reduction of imitation learning and´
structured prediction to no-regret online learning. In International Conference on Artificial
_Intelligence and Statistics, AISTATS, pp. 627–635, 2011._

Stefan Schaal et al. Learning from demonstration. Advances in neural information processing
_systems, pp. 1040–1046, 1997._

John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889–1897, 2015.

Noah Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Neunert,
Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing what worked:
Behavior modelling priors for offline reinforcement learning. In International Conference on
_[Learning Representations (ICLR), 2020. URL https://openreview.net/forum?id=](https://openreview.net/forum?id=rke7geHtwH)_
[rke7geHtwH.](https://openreview.net/forum?id=rke7geHtwH)

[Stanford Artificial Intelligence Laboratory et al. Robotic operating system. URL https://www.](https://www.ros.org)
[ros.org.](https://www.ros.org)

Umar Syed, Michael Bowling, and Robert E Schapire. Apprenticeship learning using linear programming. In International Conference on Machine Learning, pp. 1032–1039, 2008.

Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026–5033, 2012.

Mel Vecerik, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nicolas Heess,
Thomas Rothorl, Thomas Lampe, and Martin Riedmiller. Leveraging demonstrations for deep¨
reinforcement learning on robotics problems with sparse rewards. arXiv preprint arXiv:1707.08817,
2017.


-----

Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.
_arXiv preprint arXiv:1911.11361, 2019a._

Yueh-Hua Wu, Nontawat Charoenphakdee, Han Bao, Voot Tangkaratt, and Masashi Sugiyama. Imitation learning from imperfect demonstration. In International Conference on Machine Learning, pp.
6818–6827, 2019b.

Brian D. Ziebart, Andrew L. Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy inverse
reinforcement learning. In AAAI Conference on Artificial Intelligence, pp. 1433–1438, 2008.


-----

A USEFUL TECHNICAL RESULTS

We will use the well known Performance Difference Lemma in our analysis (Kakade & Langford,
2002)

**Lemma 3 (Performance Difference Lemma). For any two policies π and ˜π,**

_π_
_JR(π)_ _JR(˜π) = (1_ _γ)[−][1]_ Es _dπ,a_ _π(s,_ )[AR[˜] [(][s, a][)]]
_−_ _−_ _∼_ _∼_ _·_

The following result gives a bound on the difference between discounted state visitation distributions
of two policies in terms of their average total variation difference.

**Lemma 4 (Lemma 3, (Achiam et al., 2017)). For any two policies π and π[′],**
_d[π]_ _d[π][′]_ TV[(][π, π][′][)]
_−_ 1 _[≤]_ [2][γ][(1][ −] _[γ][)][−][1][ D][π]_

We will use the following well known result.


**Lemma 5. For any two policies π and π[′], DTV[π]** [(][π, π][′][)][ ≤]


_DKL[π]_ [(][π, π][′][)][/][2][.]


_Proof. From Pinsker’s inequality, DTV(p, q)_ _DKL(p, q)/2 for any two distributions p, q. Com-_
_≤_

bining this with Jensen’s inequality, we get DTV[π] [(][π, π][′][)][ ≤] _DKL[π]_ [(][π, π][′][)][/][2][.]

p

p

We will use the following result which is widely used in literature. We provide the proof for
completeness.

**Lemma 6. Given any policy π and a function fπ : S × A →** R,


Eτ _π[_ _γ[t]fπ(st, at)] = (1_ _γ)[−][1]Es_ _dπ,a_ _π(s,_ )[fπ(s, a)]
_∼_ _−_ _∼_ _∼_ _·_

_t=0_

X


_Proof._

_∞_

Eτ _π[_ _γ[t]fπ(st, at)] =_
_∼_

_t=0_

X


_γ[t]P(st = s, at = a|π)fπ(s, a)_
_t=0_

X


_s,a_


= _γ[t]P(st = s|π)π(s, a)fπ(s, a) =_ _π(s, a)fπ(s, a)_ _γ[t]P(st = s|π)_

_s,a_ _t=0_ _s,a_ _t=0_

X X X X

= (1 _γ)[−][1][ X]_ _d[π](s)π(s, a)fπ(s, a) = (1_ _γ)[−][1]Es_ _dπ,a_ _π(s,_ )[fπ(s, a)].
_−_ _−_ _∼_ _∼_ _·_

_s,a_

B PROOF OF THE RESULTS IN SECTION 3

B.1 PROOF LEMMA 1

_Proof. Starting from the performance difference lemma,_


_πk+1/2_
_JR(π) −_ _JR(πk+1/2) = (1 −_ _γ)[−][1]_ Es∼dπ,a∼π(s,·)[AR (s, a)]

= (1 _γ)[−][1][ X]_ _d[π](s)_ _πb(s, a)AπRk+1/2_ (s, a)
_−_

_s_ _a_

+ (1 _γ)X[−][1][ X]_ _d[π](s)_ (π(s, a) _πb(s, a))AπRk+1/2_ (s, a)
_−_ _−_

_s_ _a_
X

(a) (1 _γ)[−][1]β + (1_ _γ)[−][1][ X]_ _d[π](s)_ (π(s, a) _πb(s, a))ARπk+1/2_ (s, a)
_≥_ _−_ _−_ _−_

_s_ _a_
X


-----

(b)
_≥_ (1 − _γ)[−][1]β −_ (1 − _γ)[−][1]_ _ϵR,k+1/2 2 DTV[π]_ [(][π, π][b][)]

(c)
_≥_ (1 − _γ)[−][1]β −_ (1 − _γ)[−][1]_ _ϵR,k+1/2_ 2DKL[π] [(][π, π][b][)][,]
q

where (a) follows from the premise that πk+1/2 satisfies Assumption 1, (b) is obtained by denoting
_πk+1/2_
_ϵR,k+1/2 = maxs,a_ _AR_ (s, a), and (c) follows from Lemma 5.
_|_ _|_

B.2 PROOF THEOREM 1

We will first prove the following result.
**Lemma 7. For any two policies π and ˜π,**


_JR(π)_ _JR(˜π)_ TV [(][π,][ ˜]π),
_|_ _−_ _| ≤_ (1[3][R][max]γ)[2][ D][max]

_−_

_where Rmax = maxs,a |R(s, a)|._

_Proof. We have,_

_JR(π)_ _JR(˜π) = (1_ _γ)[−][1](_ _d[π](s)π(s, a)R(s, a)_ _dπ[˜](s)˜π(s, a)R(s, a))_
_−_ _−_ _−_

_s,a_ _s,a_

X X

= (1 − _γ)[−][1](_ _d[π](s)π(s, a)R(s, a) −_ _dπ[˜](s)π(s, a)R(s, a))_

_s,a_ _s,a_

X X


+ (1 − _γ)[−][1](_


_dπ[˜](s)π(s, a)R(s, a) −_
_s,a_

X


_dπ[˜](s)˜π(s, a)R(s, a))_

_s,a_

X


_≤_ (1 − _γ)[−][1]Rmax_ _d[π]_ _−_ _dπ[˜]_ 1 [+ (1][ −] _[γ][)][−][1][R][max][D]TV[max][(][π,][ ˜]π)_

(a)
_≤_ (1 − _γ)[−][1]Rmax2γ(1 −_ _γ)[−][1]DTV[max][(][π,][ ˜]π) + (1 −_ _γ)[−][1]RmaxDTV[max][(][π,][ ˜]π)_

TV [(][π,][ ˜]π),

_≤_ (1[3][R][max]γ)[2][ D][max]

_−_

where (a) follows from Lemma 4. The lower bound can also be obtained in a similar way. Combining
both, we obtain the desired result.

_Proof of Theorem 1. (i) If πk+1/2 satisfies Assumption 1, then (5) follows immediately by combining_
the results of Proposition 1 and Lemma 1.
(ii) If πk+1/2 does not satisfy Assumption 1, we can directly bound JR(πk+1) − _JR(πk+1/2) without_
invoking Lemma 1 (and hence the implicit assumption that moving towards πb improves the policy).

Use Lemma 7, we get

_JR(πk+1)_ _JR(πk+1/2)_ TV [(][π][k][+1][, π]k+[1]/[2][)][ ≥−] [3][R][max] (11)
_−_ _≥−_ (1[3][R][max]γ)[2][ D][max] (1 _γ)[2][ δ][k][,]_

_−_ _−_

where the last inequality follows from the fact DTV[max][(][π][k][+1][, π][k][+][1][/][2][)][ ≤] _[δ][k][ according to the policy]_
guidance step (2).

Now, combining the result of Proposition 1 and the above inequality, we obtain (6).

C PROOF OF THE RESULTS IN SECTION 4

C.1 PROOF OF LEMMA 2

_Proof. We follow the proof technique of the performance difference lemma (PDL) with some_
modifications to get the result. For any given initial state s,

_∞_

_VC[π]π_ [(][s][)][ −] _[V][ ˜]Cππ˜_ [(][s][) =][ E][τ] _[∼][π][|][s][0][=][s][[]_ _γ[t]Cπ(st, at)] −_ _VCπ[˜]π˜_ [(][s][)]

_t=0_

X


-----

= Eτ _π_ _s0=s[_ _γ[t](Cπ(st, at) + VCπ[˜]π˜_ [(][s][t][)][ −] _[V][ ˜]Cππ˜_ [(][s][t][))]][ −] _[V][ ˜]Cππ˜_ [(][s][)]
_∼_ _|_

_t=0_

X

(=a) Eτ _π_ _s0=s[_ _∞_ _γ[t](Cπ(st, at) + γVCπ[˜]π˜_ [(][s][t][+1][)][ −] _[V][ ˜]Cππ˜_ [(][s][t][))]]
_∼_ _|_

_t=0_

X


=Eτ _π_ _s0=s[_ _γ[t](Cπ˜[(][s]t[, a]t[) +][ γV]C[ ˜]ππ˜_ [(][s][t][+1][)][ −] _[V][ ˜]Cππ˜_ [(][s][t][))]]
_∼_ _|_

_t=0_

X


+ Eτ _π_ _s0=s[_ _γ[t](Cπ(st, at)_ _Cπ˜[(][s]t[, a]t[))]][,]_ (12)
_∼_ _|_ _−_

_t=0_

X


where (a) is obtained by rearranging the summation and telescoping. The first summation in (12) can
now be handled as in the proof of the PDL, and we omit the details. So, we get

_∞_

Eτ _π[_ _γ[t](Cπ˜[(][s]t[, a]t[) +][ γV]C[ ˜]ππ˜_ [(][s][t][+1][)][ −] _[V][ ˜]Cππ˜_ [(][s][t][))] = (1][ −] _[γ][)][−][1][E][s][∼][d][π][,a][∼][π][(][s,][·][)][[][A]πC[˜]_ _π˜_ [(][s, a][)]][ (13)]
_∼_

_t=0_

X

The second summation can be written as

_∞_ _∞_ _π(st, at)_

Eτ _π[_ _γ[t](Cπ(st, at)_ _Cπ˜[(][s]t[, a]t[))] =][ E]τ_ _π[[]_ _γ[t](log_ _[π][(][s][t][, a][t][)]_
_∼_ _−_ _∼_ _πb(st, at)_ _πb(st, at)_ [)]]

_t=0_ _t=0_

X X _[−]_ [log ˜]

_∞_ (b)

= Eτ _π[_ _γ[t]_ log _[π][(][s][t][, a][t][)]_ = (1 _γ)[−][1][ X]_ _d[π](s)_ _π(s, a) log_ _[π][(][s][t][, a][t][)]_
_∼_ _π˜(st, at)_ []] _−_ _π˜(st, at)_

_t=0_ _s_ _a_

X X

= (1 − _γ)[−][1]DKL[π]_ [(][π,][ ˜]π), (14)

where (b) is obtained by using Lemma 6.

Now, by taking expectation on the both sides of (12) w.r.t. the initial state distribution µ, and then
substituting (13) and (14) there, we get the desired result.

C.2 PROOF OF PROPOSITION 2

Lemma 2 provides an approach to evaluate the infinite horizon return JCπ (π) of policy π using
the infinite horizon return JCπ˜ [(˜]π) and advantage AπC[˜] _π˜_ [of policy][ ˜]π. Unfortunately, the RHS of (7)
contains two terms that requires taking expectation w.r.t. d[π], which means that empirical estimation
requires samples according to π (that are unavailable). This is a well known issue in the policy
gradient literature and it is addressed by considering an approximation by replacing the expectation
w.r.t. d[π] by dπ[˜] (Kakade & Langford, 2002; Schulman et al., 2015). We follow the same approach
with minor modifications in the context of our problem.

We will first prove the following lemmas.

**Lemma 8. For any two policies π and ˜π,**


1 _π_

_π,a_ _π(s,_ )[AC[˜] _π˜_ [(][s, a][)]]
(1 − _γ)_ [E][s][∼][d][˜] _∼_ _·_

+ _√2γϵπ˜_ _DKLπ[˜]_ [(][π,][ ˜]π) + 1 KL [(][π,][ ˜]π),

(1 _γ)[2]_ (1 _γ)_ _[D][max]_
_−_ q _−_


_JCπ_ (π) − _JCπ˜_ [(˜]π) ≤

_where ϵπ˜_ [= max]s,a _[|][A]πC[˜]_ _π˜_ [(][s, a][)][|][.]


_Proof. We will separately bound the two terms on the RHS of (7) in Lemma 2. Firstly,_


_π_
Es _dπ,a_ _π(s,_ )[AC[˜] _π˜_ [(][s, a][)] =]
_∼_ _∼_ _·_


_d[π](s)_


_π(s, a)AπC[˜]_ _π˜_ [(][s, a][)]
_a_

X

(d[π](s) − _dπ[˜](s))_
X


_dπ[˜](s)_


_π(s, a)AπC[˜]_ _π˜_ [(][s, a][) +]


_π(s, a)AπC[˜]_ _π˜_ [(][s, a][)]


-----

_π_ _π_
_≤_ Es∼dπ˜ _,a∼π(s,·)[AC[˜]_ _π˜_ [(][s, a][)] +] _d[π]_ _−_ _d[˜]_ 1 _[ϵ]π[˜]_

(a)
_π_ _π_
_≤_ Es∼dπ˜ _,a∼π(s,·)[AC[˜]_ _π˜_ [(][s, a][)] + 2][γ][(1][ −] _[γ][)][−][1][ ϵ]π[˜]_ _[D]TV[˜]_ [(][π,][ ˜]π)

(b)
Es _dπ˜_ _,a_ _π(s,_ )[AπC[˜] _π˜_ [(][s, a][)] +] _√2γ(1_ _γ)[−][1]_ _ϵπ˜_ _DKLπ[˜]_ [(][π,][ ˜]π), (15)
_≤_ _∼_ _∼_ _·_ _−_

q


where (a) follows from Lemma 4 and (b) follows from Lemma 5.

Secondly, DKL[π] [(][π,][ ˜]π) ≤ _DKL[max][(][π,][ ˜]π). Using this fact and the inequality (15) in (7), we get the desired_
result.

We now make an interesting observation that JCπ (π) is a scaled version of the average KL divergence
between π and πb. We formally state this result below.

**Lemma 9. For any arbitrary policy π,**

_∞_

_JCπ_ (π) = Eτ _π[_ _γ[t]Cπ(st, at)] = (1_ _γ)[−][1]DKL[π]_ [(][π, π][b][)] (16)
_∼_ _−_

_t=0_

X


_Proof. We have,_

_∞_ (a)

_JCπ_ (π) = Eτ _π[_ _γ[t]Cπ(st, at)]_ = (1 _γ)[−][1][ X]_ _d[π](s)π(s, a)Cπ(s, a)_
_∼_ _−_

_t=0_ _s,a_

X

= (1 _γ)[−][1][ X]_ _d[π](s)_ _π(s, a) log_ _[π][(][s, a][)]_ KL[(][π, π][b][)][,]
_−_ _πb(s, a) [= (1][ −]_ _[γ][)][−][1][D][π]_

_s_ _a_
X

where (a) follows from Lemma 6.


_Proof of Proposition 2. Using the result of Lemma 9 in the inequality given by Lemma 8, for any_
policy π and ˜π, we get


_√2γϵπ˜_

(1 − _γ)_


_π_ _π_
_DKL[π]_ [(][π, π][b][)][ ≤] _[D]KL[˜]_ [(˜]π, πb) + Es _dπ˜_ _,a_ _π(s,_ )[AC[˜] _π˜_ [(][s, a][)] +]
_∼_ _∼_ _·_

Now, replacing ˜π by πk+1/2, we get


_DKLπ[˜]_ [(][π,][ ˜]π) + DKL[max][(][π,][ ˜]π).


_DKL[π]_ [(][π, π][b][)][ ≤] _[α][k]_ [+][ E]s∼dπk+1/2 _,a∼π(s,·)[[][A]Cπkπk+1+1/2/2_ [(][s, a][)]]


2γϵπ,k


_πk+1/2_
KL (π, πk+1/2) + DKL[max][(][π, π]k+[1]/[2][)][,] (17)


(1 − _γ)_ KL _k+1/2_

_πk+1/2_ _πk+1/2_
where αk = DKL (πk+1/2, πb), ϵπ,k = maxs,a |ACπk+1/2 [(][s, a][)][|][.]


Now, for any π that lies in the trust region _π_ : _DKL[max][(][π, π][k][+][1][/][2][)]_ _δk_, we have
_πk+1/2_ _{_ _≤_ _}_
_DKL_ (π, πk+1/2) ≤ _DKL[max][(][π, π][k][+][1][/][2][)][ ≤]_ _[δ][k][. Using this in (17) gives the final result.]_

D DETAILS OF THE PRACTICAL ALGORITHM

As explained in Section 4, when πb is known, obtaining Cπ(s, a) = log(π(s, a)/πb(s, a)) and
estimating A[π]Cπ [is straightforward. Hence, LOGO can perform the policy improvement and policy]
guidance step according to the update equations given in (10) by directly using πb. When the form of
_πb is unknown and we only have access to the demonstration data D generated according to πb, we_
have to estimate Cπ(s, a) using D and Dπ, where Dπ is the trajectory data generated according to π.

Instead of estimating log(π(s, a)/πb(s, a)) directly, we make use of the one-to-one correspondence between a policy and its discounted state-action visitation distribution defined as ρ[π](s, a) =
_d[π](s)π(s, a) (Syed et al., 2008). More precisely, we estimate log(ρ[π](s, a)/ρ[π][b]_ (s, a)) using D and
_Dπ instead of estimating log(π(s, a)/πb(s, a)) directly. We can then use the powerful framework of_


-----

generative adversarial networks (GAN) (Goodfellow et al., 2014) to estimate this quantity. This can
be achieved by training a discriminator function B : S × A → [0, 1] as

max E(s,a) _ρ[π]b_ [log B(s, a)] + E(s,a) _ρπ_ [log(1 _B(s, a))]._ (18)
_B_ _∼_ _∼_ _−_

We note that the GAN-based approach to estimate a distance metric between ρ[π] and ρ[π][b] is popular in
the imitation learning literature (Ho & Ermon, 2016; Kang et al., 2018).

The optimal discriminator for the above problem is given by B[∗](s, a) = _ρ[π][b]_ (ρs,a[π][b])+(s,aρ[π])(s,a) [(Goodfellow]

et al., 2014, Proposition 1). Hence, given the discriminator function B obtained after training to solve
(18), we use Cπ(s, a) = log B(s, a) in the LOGO algorithm, which provides an approximation to
_−_
the quantity of interest. As shown in Section 5, this approximation yields excellent results in practice.

We summarize the LOGO algorithm below.

**Algorithm 1 LOGO Algorithm**

1: Initialization: Initial policy π0, Demonstration data or behavior policy πb
_D_
2: for k = 0, 1, 2, .. do
3: Collect (s, a, r, s[′]) ∼ _πk and store in Dπk_

4: **if πb is known then**

5: _Cπk_ (s, a) = log(πk(s, a)/πb(s, a))

6: **else**

7: Train a discriminator B(s, a) according to (18) using and _πk_
_D_ _D_

8: _Cπk_ (s, a) = log B(s, a)
_−_

9: **end if**

10: Estimate gk and Fk using Dπk

2δ

11: Perform policy improvement step: θk+1/2 = θk + _Fk[−][1][g][k]_

s _gk[T]_ _[F][ −]k_ [1][g][k]

12: Decay δk (according to the adaptive rule described in Appendix F)

13: Estimate hk and Lk using Cπk and Dπk+1/2

2δk

14: Perform policy guidance step: θk+1 = θk+ 12 _[−]_ s _h[T]k_ _[L]k[−][1][h][k]_ _L[−]k_ [1][h][k]

15: end for


E DETAILS OF TURTLEBOT EXPERIMENTS

We evaluate the performance of LOGO in a real-world setting using TurtleBot, a two wheeled
differential drive robot (Amsters & Slaets, 2019). We consider two different simulators for training
our policy. The first one is a simple kinematics-based low fidelity simulator. The second one is a
sophisticated physics-based Gazebo simulator (Koenig & Howard, 2004) with an accurate model of
the TurtleBot.

We use the low fidelity simulator to get a sub-optimal Behavior policy by training TRPO with dense
rewards. The details are given in Section E.1. Gazebo simulator is used for implementing the LOGO
algorithm in a sparse reward setting with complete and incomplete observations. The details are
given in Section E.2.

E.1 LOW FIDELITY KINEMATICS SIMULATOR

We design the low fidelity simulator using the OpenAI gym framework where the dynamics are
governed by the following equations

_xt+1 = xt + vt cos(θt)∆, yt+1 = yt + vt sin(θt)∆, θt+1 = θt + ωt, ∆,_

where xt, yt are the coordinates of the bot, vt is the linear velocity, ωt is the angular velocity,
and θt is its yaw calculated from the quaternion angles, all calculated at time t. ∆ is the time
discretization factor. We define the state st to be the normalized relative position w.r.t. the waypoint,


-----

i.e., st = ((xt _xw)/G, (yt_ _yw)/G, θt_ _θt[w][)][, where][ x][w][, y][w]_ [are the target coordinates of the]
waypoint, and θ −t[w] [is the target heading to the waypoint at time] − _−_ _[ t][. we define the action][ a][t]_ [to be the]
linear and angular velocities, i.e., at = [vt, ωt]. We discretize the action space into 15 actions.

We handcraft a dense reward function as follows,

+10 if _xt_ _xw_ 0.05 and _yt_ _yw_ 0.05,
_|_ _−_ _| ≤_ _|_ _−_ _| ≤_

_rt =_ 1 if _xt_ _G_ or _yt_ _G,_

− _|_ _| ≥_ _|_ _| ≥_
 0.166dt 0.3184 _θt[w]_ otherwise,

_−_ _−_ _|_ _[−]_ _[θ][t][|]_

where dt is a combination of cross track and along track error defined as follows,

_dt =_ (xt _xw)[2]_ + (yt _yw)[2][]_ sin[2](θt[w]
_−_ _−_ _[−]_ _[θ][t][) +][ |][x][t]_ _[−]_ _[x][w][|][ +][ |][y][t]_ _[−]_ _[y][w][|][.]_
 

E.2 HIGH FIDELITY GAZEBO SIMULATOR


Gazebo is a physics-based simulator with rendering capabilities and can be used for realistic simulation of the environment. It is configurable and can be used to model robots with multiple joint
constraints, actuator physics, gravity and frictional forces and a wide range of sensors in indoor as
well as outdoor settings. Gazebo facilitates close-to-real-world data collection from the robots and
sensor models. Since it runs in real-time, it takes millions of simulation frames for an RL agent to
learn simple tasks. Gazebo can also speed up simulation by increasing step size. This can however
lead to loss of precision. For this project, we ran the training scheme on Gazebo in almost real time
with a simulation step-size of ∆T = 0.001.

The Gazebo simulation setup consists of the differential drive robot (TurtleBot3 Burger) model
spawned in either an empty space or a custom space with obstacles at a predefined location. A
default coordinate grid is setup with respect to the base-link of the robot model. A ROS (Stanford
Artificial Intelligence Laboratory et al.) framework is instantiated using a custom-built OpenAI Gym
environment which acts as an interface between the proposed RL algorithm and the simulation model.
ROS topics are used to capture the state update information of the robot asynchronously in a callback
driven mechanism. For the purpose of our experiments we use the following ROS topics,

1. /odom (for xt, yt, θt): Odometry information based on the wheel encoder
2. /cmd vel (for vt, ωt): Linear and angular velocity commands
3. /scan (for obstacle avoidance): Scan values from the Lidar

The /odom and /scan topics are used to determine st while /cmd vel is used to output the
required action at. The state space and action space for waypoint tracking task is similar to state
and action space in section E.1. For obstacle avoidance tasks, we also include the distance values
obtained form the Lidar as a part of the state.

E.3 ROBOT PLATFORM: TURTLEBOT3

We use TurtleBot 3 (Amsters & Slaets, 2019), an open source differential drive robot equipped with
a RP Lidar as our robotic platform for real-world experiments. We use ROS as the middleware to
setup the communication framework. The bot transmits its state (position and Lidar information)
information over a network to a intel NUC, which transmits back the corresponding action according
to the policy being executed.

E.4 TRAINING: WAYPOINT TRACKING

In navigation problems, we have a global planner that uses a high level map of the bot’s surroundings
for planning a trajectory using waypoints a unit-meter distance from each other, while the goal of the
bot is to achieve these waypoints. To obtain a waypoint tracking scheme, we first train a behavior
policy on the low fidelity simulator using TRPO to reach a waypoint approximately one unit distance
(1m in the real world) away from its initial position. In each training episode, the bot is reset to
the origin and a waypoint is randomly chosen asterminated if the robot reaches the waypoint, or if it crosses the training boundary or exceeds the xw ∼ uniform([−1, 1]), yw = 1. The episode is
maximum episode length. The behavior policy obtained after training in the low fidelity simulator


-----

is then used in the LOGO algorithm training on Gazebo. LOGO is trained in Gazebo with sparse
sparse rewards, where a reward of +1 is provided if the bot reaches the waypoint, and 0 otherwise.
We evaluate the trained policy in Gazebo and the real world, by providing it a series of waypoints to
track in order to reach its final goal.

E.5 TRAINING: OBSTACLE AVOIDANCE

We train our bot for obstacle avoidance in Gazebo using the behavior policy described in the section
above. Our goal is to use the skills of waypoint navigation from the behavior policy to guide and learn
the skills of obstacle avoidance. The state space includes the Lidar scan values in addition to the state
space described in section E.1. The /scan provides 360 values, each of these indicate the distance
to the nearest object in a 1[◦] sector. For the purpose of our experiments, we use the minimum distance
in each 60[◦] sector. This reduces the Lidar data to 6 values. We train our algorithm on Gazebo with
a fixed obstacle for random waypoints. In each training episode, the bot is reset to the origin and a
waypoint is generated similar to the previous section. The episode is terminated if same conditions
in the previous section are satisfied or if a collision with the obstacle occurs. We demonstrate the
performance of our algorithms both in Gazebo as well as the real-world.

F IMPLEMENTATION DETAILS

We implement all the algorithms in this paper using PyTorch (Paszke et al., 2019). For all our
experiments, we have a two layered (128 × 128) fully connected neural network with tanh activation
functions to parameterize our policy and value functions. We use a learning rate of 3 × 10[−][4], a
discount factor γ = 0.99, and TRPO parameter δ = 0.01. We decay the influence of the behavior
policy by decaying δk. We start with δ0, and we do not decay δk for the first Kδ iterations. For
_k > Kiteration is greater than the average return in the pastδ, we geometrically decay δk as δk ←_ _αδk 10, whenever the average return in the current iterations. The rest of the hyperparameters_
for MuJoCo simulations, Gazebo simulation, and real-world experiments are given in table 1. In table
2 we provide details on the demonstration data collected using the behavior policy.

We implement LOGO based on a publicly available TRPO code base. We implement PofD, TRPO,
BC-TRPO, and GAIL by modifying the same code base as used by LOGO. We run DAPG using
code base and hyperparameters provided by the authors. For consistency, we run all algorithms using
the same batch size.

Environment _δ0_ _α_ _Kδ_ Batch Size
Complete Incomplete
Observation Observation

Hopper-v2 0.01 0.02 0.95 50 20000
HalfCheetah-v2 0.2 0.1 0.95 50 20000
Walker2d-v2 0.03 0.05 0.95 50 20000
InvertedDoublePendulum-v2 0.2 0.1 0.9 5 5000

Waypoint tracking 0.01 0.95 5 2048
Obstacle avoidance 0.008 0.9 5 2048

Table 1: Hyperparameters


-----

Average
Environment CompleteOfflineIncomplete S Online S _A_ Samples Episodic Reward
Observation Observation

Hopper-v2 R[11] R[7] R[11] R[3] 3869 1369.81
HalfCheetah-v2 R[17] R[14] R[17] R[6] 5000 2658.34
Walker2d-v2 R[17] R[10] R[17] R[6] 4584 2449.21
InvertedDoublePendulum-v2 R[11] R[7] R[11] R 183 340.73

Waypoint traking R[3] R[3] 15 Policy 1
Obstacle avoidance R[3] R[9] 15 Policy _−0.88_

Table 2: Demonstration data details

G RELATED WORK

**Offline RL: Recently, there have been many interesting works in the area of offline RL which use**
offline data to learn a policy. In particular, offline RL algorithms such as BEAR (Kumar et al., 2019),
BCQ (Fujimoto et al., 2019), ABM (Siegel et al., 2020), and BRAC (Wu et al., 2019a) focus on
learning a policy using only the offline data without any online learning or online fine-tuning. The
key idea of these offline RL algorithms is to learn a policy that is ‘close’ to the behavior policy
that generated the data via imposing some constraints, to overcome the problem of distribution
shift. This approach, however, often results in conservative policies, and hence it is difficult to
guarantee significant performance improvement over the behavior policy. Moreover, standard offline
RL algorithms are not immediately amenable to online fine-tuning, as observed in Nair et al. (2020).
LOGO is different from the offline RL algorithms in the following two key aspects. First, unlike the
_offline RL algorithms mentioned before, LOGO is an online RL algorithm which uses the offline_
demonstration data for guiding the online exploration during the initial phase of learning. By virtue
of this clever online guidance, LOGO is able to converge to a policy that is significantly superior than
the sub-optimal behavior policy that generated the demonstration data. Second, offline RL algorithms
typically require large amount of state-action data with the associated rewards. LOGO only requires
a small amount demonstration data since it is used only for the guiding the online exploration.
Moreover, LOGO requires only the state-action observation and does not need the associated reward
data. In many real-world application applications, it may be possible to get state-action demonstration
data from human demonstration or using a baseline policy. However, it is difficult to assign reward
values to these observation.

Advantage Weighted Actor-Critic (AWAC) algorithm (Nair et al., 2020) propose to accelerate online
RL by leveraging offline data. This work is different from our approach in four crucial aspects. (i)
AWAC requires offline data with associated rewards whereas LOGO requires only the state-action
observations (not the reward data). In many real-world applications, it may be possible to get stateaction demonstration data from human demonstration or using a baseline policy. However, it may be
difficult to assign reward values to these observations, especially in the sparse reward setting. (ii)
AWAC explicitly mentions that it leverages large amounts of offline data whereas LOGO relies on
small amount of sub-optimal demonstration data. (iii) LOGO gives a novel and theoretically sound
approach using the double trust region structure that provides provable guarantees on its performance.
AWAC algorithm does not give any such provable guarantees. (iv) LOGO can be easily extended to
the setting with incomplete state information where as AWAC is not immediately amenable to such
extension.

There are also many recent works on addressing multiple aspects of IL and LfD, including combining
IL and RL for long horizon tasks (Gupta et al., 2019), learning from imperfect (Gao et al., 2018;
Jing et al., 2020; Wu et al., 2019b) and incomplete (Libardi et al., 2021; Gul¨ c¸ehre et al., 2020)
demonstrations. Their approaches, performance guarantees and experimental settings are different
from that of our problem and proposed solution approach.


-----

Hopper-v2 Hopper-v2

3000 3000

2000 2000

1000 1000

Average episodic reward Average episodic reward

0 0

0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0

Samples 1e7 Samples 1e7

= 0.85 = 0.95 = 0.99 Behavior K = 1 K = 100 K = 1000 Behavior
= 0.9 = 0.97 = 1 Expert K = 50 K = 500 K = 1500 Expert

(a) Sensitivity of α

SENSITIVITY ANALYSIS OF α AND Kδ


(b) Sensitivity of Kδ


We run LOGO for different values of α and Kδ by keeping the other parameters fixed on the Hopperv2 environment. We observe from Figure 3a that the performance of LOGO is not sensitive to
perturbations in α. When α = 1, we do not decay the influence of the behavior data, this results in a
performance close to the behavior data which is inline with our intuition. We observe from Figure 3b
that LOGO is not sensitive to perturbations in Kδ as well. We observe that the performance of LOGO
is similar for Kδ = 1, 50, 100. This because we only decay δk whenever the average return in the
current iteration is greater than the average return in the past 10 iterations. We further note that Kδ
controls the iteration from which we begin decaying the influence of the behavior data, this can be
clearly seen for Kδ = 500, 1000 where the performance rises as soon as the decaying begins.


-----

