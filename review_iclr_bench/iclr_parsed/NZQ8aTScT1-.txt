# EIGENSPACE RESTRUCTURING: A PRINCIPLE OF SPACE AND FREQUENCY IN NEURAL NETWORKS

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Understanding the fundamental principles behind the massive success of neural
networks is one of the most important open questions in deep learning. However,
due to the highly complex nature of the problem, progress has been relatively slow.
In this note, through the lens of infinite-width networks, a.k.a. neural kernels, we
present one such principle resulting from hierarchical locality. It is well-known
that the eigenstructure of infinite-width multilayer perceptrons (MLPs) depends
solely on the concept frequency, which measures the order of interactions. We
show that the topologies from convolutional networks (CNNs) restructure the associated eigenspaces into finer subspaces. In addition to frequency, the new structure also depends on the concept space — the distance among interaction terms,
defined via the length of a minimum spanning tree containing them. The resulting fine-grained eigenstructure dramatically improves the network’s learnability,
empowering them to simultaneously model a much richer class of interactions,
including long-range-low-frequency interactions, short-range-high-frequency interactions, and various interpolations and extrapolations in-between. Finally, we
show that increasing the depth of a CNN can improve the inter/extrapolation resolution and, therefore, the network’s learnability.

1 INTRODUCTION

Learning in high dimensions is commonly believed to suffer from the curse of dimensionality, in
which the number of samples required to solve the problem grows rapidly (often polynomially) with
the dimensionality of the input. Nevertheless, modern neural networks often exhibit an astonishing
power to tackle a wide range of highly complex and high-dimensional real-world problems, many of
which were thought to be out-of-scope of known methods (Krizhevsky et al., 2012; Vaswani et al.,
2017; Devlin et al., 2018; Silver et al., 2016; Senior et al., 2020; Kaplan et al., 2020). What are
the mathematical principles that govern the astonishing power of neural networks? This question
perhaps is the most crucial research question in the theory of deep learning because such principles
are also the keys to resolve fundamental questions in the practice of machine learning such as (outof-distribution) generalization (Zhang et al., 2021), calibration (Ovadia et al., 2019), interpretability
(Montavon et al., 2018), robustness (Goodfellow et al., 2014).

Unarguably, there can be more than one of such principles. They are related to one or more of the
three basic ingredients of machine learning methods: the data, the model and the inference algorithm. Among them, the models, a.k.a. architectures of neural networks are the most crucial innovation in deep learning that set it apart from classical machine learning methods. More importantly,
the current revolution in machine learning is initialized by the (re-)introduction of convolution-based
architectures (Krizhevsky et al., 2012; Lecun, 1989), and subsequent breakthroughs are often driven
by the discovery or application of novel architectures (Vaswani et al. (2017); Devlin et al. (2018)).
As such, identifying and understanding fundamental roles of architectures are of great importance.

In this paper, we take a step forwards by leveraging recent developments in overparameterized
networks (Poole et al. (2016); Daniely et al. (2016); Schoenholz et al. (2017); Lee et al. (2018);
Matthews et al. (2018); Xiao et al. (2018); Jacot et al. (2018); Du et al. (2018); Novak et al. (2019a);
Lee et al. (2019) and many others.) These developments have discovered an important connection
between neural networks and kernel machines: the Neural Network Gaussian Process (NNGP) kernels and the neural tangent kernels (NTKs). Under certain scaling limits, the former describes the


-----

|Order of Lea Long-Range- Beyond Budg|Col2|rning Low-Frequency et|
|---|---|---|
||||
||||
||||
||||
|||(a)|
||||
|Space|||

|Col1|Col2|Short-Range-High-Frequency|
|---|---|---|
||||
||||
||||
||||
||||
|||(b)|
||||
|Space|||

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|||||||
|||||(c)||
|||||||
|Space||||||

|Col1|F U|iner ltra-|Inter Long|polati er-Ra|on nge-|Ultra|-Low|-Fre|quen|cy|Col12|
|---|---|---|---|---|---|---|---|---|---|---|---|
|||||||||||||
|||||||||||||
|||||||||||||
|||||||||||||
||||||||||(d)|||
|||||||||||||
|Space||||||||||||


MLP Shallow CNN Deep CNN High Resolution CNN

Space + Frequency = Budget Short-Range-High-Frequency Median-Range-Median-Frequency Ultra-Short-Range-Ultra-High-Frequency
Order of LearningLong-Range-Low-Frequency Finer InterpolationFiner InterpolationUltra-Longer-Range-Ultra-Low-Frequency
Beyond Budget

Frequency

(a) (b) (c) (d)

Space Space Space Space


Figure 1: Architectural Inductive Biases. An demonstration of learnable functions vs architectures for four families of architectures. Each shaded box indicates the maximum learnable
eigenspaces within a given compute budget (Dashed Line.) From left to right: (a) MLPs can
model Long-Range-Low-Frequency interactions; (b) S-CNNs can model Short-Range-High**Frequency interactions; (c) Additionally, D-CNNs can also model interactions between LRLF**
and SRHF, a.k.a., Median-Range-Median-Frequency interactions. (d) Finally, HS-CNNs can additionally model interactions of Ultra-Short-Range-Ultra-High-Frequency, Ultra-Long-Range**Ultra-Low-Frequency, and finer interpolations in-between. The Green Arrow indicates the di-**
rection of expansion of learnable functions when increasing the compute budget.

distribution of the outputs of a randomly initialized network (a.k.a. prior), and the latter can describe
the network’s gradient descent dynamics. Although recent work (Ghorbani et al., 2019; Yang & Hu,
2020) has identified several limitations of using them in studying the feature learning dynamics of
practical networks, we show that they do capture several crucial aspects of the architectural inductive
biases.

Our main contribution is an eigenspace restructuring theorem. It characterizes a mathematical connection between a network’s architecture and its learnability through a trade-off between space and
_frequency, providing novel insights behind the mystery power of deep CNNs (more generally, hi-_
erarchical locality (Deza et al., 2020; Vasilescu et al., 2021).) By frequency, we mean the degree
(order) of the eigenfunction and by space, we mean the spatial distance among the eigenfunction’s
interaction terms. We summarize our main contribution below; see Fig. 1.

1. The learning order (see Green Arrow in Fig. 1) of eigen-functions is governed by the
learning index (LI), the sum of the frequency index (FI) and the spatial index (SI), which
can be characterized precisely by the network’s topology.

2. There is a trade-off between space and frequency: within a fixed (compute/data) budget, it is
impossible to model generic Long-Range-High-Frequency interactions. MLPs can model
**Long-Range-Low-Frequency (LRHF) interactions but fail to model Short-Range-High-**
**Frequency (SRHF), while shallow CNNs (S-CNNs) are the opposite. Remarkably, deep**
CNNs (D-CNNs) can simultaneously model both and various interpolating interactions
between them (e.g., Median-Range-Median-Frequency (MRMF).)

3. In addition, high-resolution CNNs (HS-CNNs, EfficientNet-type of model scaling) further
broaden the class of learnable functions to contain (1) extrapolation: Ultra-Long-Range**Ultra-Low-Frequency and the dual interactions and (2) finer interpolations interactions.**

4. Finally, we verify the above claims empirically for neural kernel methods and finite-width
networks using SGD + Momentum for dataset and networks of practical sizes.

2 LINEAR AND LINEARIZED MODELS

As a warm up exercise, we briefly go through the training dynamics of linear models. Let (X _, Y)_
denote the inputs and labels, where X ⊆ R[d] and Y ⊆ R. Assume J : R[d] _→_ R[n] is a feature
map and the task is to learn a linear function f (x, θ) = J(x)θ to minimize the MSE objective
1

Then the gradient flow dynamics can be written as2 (x,y)∈(X _,Y)_ _[|][f]_ [(][x, θ][)][ −] _[y][|][2][. Let][ ℛ][(][X]_ _[, θ][) =][ f]_ [(][X] _[, θ][)][ −Y][ be the residual of the predictions of][ X]_ [.]

P

_d_

(1)
_dt_ [ℛ][(][X] _[, θ][) =][ −][J][(][X]_ [)][J] _[T][ (][X]_ [)][ℛ][(][X] _[, θ][)][ ≡−K][(][X]_ _[,][ X]_ [)][ℛ][(][X] _[, θ][)]_


-----

Since the feature kernel K(X _, X_ ) = J(X )J _[T]_ (X ) is constant in time, the above ODE can be solved
in closed form. Let m = |X| the cardinality of X and _K[ˆ](j)/uj be the j-th eigenvalue/eigenvector_
of K(X _, X_ ) in descending order. By initializing θ = 0 at time t = 0 and denoting the projection by
_ηj = u[T]j_ [ℛ][(][X] _[,][ 0)][, the dynamics of the residual and the loss can be reduced to]_

ℛ(X _, θt) =_ _e[−]K[ˆ](j)tηjuj,_ _L(θt) = [1]2_ _e[−][2 ˆ]K(j)tηj2_ (2)

_jX∈[m]_ _jX∈[m]_

Therefore, to make the residual in uj smaller than some ϵ > 0, the amount of time needed is
_t ≥_ _K[ˆ](j)[−][1]_ log _η[2]j[2][ϵ]_ _[/][2][. The larger][ ˆ]K(j) is, the shorter amount of time it takes to learn uj._

Although simple, linear models provide us with the most useful intuition behind the relation between
“eigenstructures" and learning dynamics.

2.1 LINEARIZED NEURAL NETWORKS: NNGP KERNELS AND NT KERNELS

Let f (θ, x) be a general function, e.g. f is neural network parameterized by θ. Similarly,
_d_

(3)
_dt_ [ℛ][(][X] _[, θ][) =][ −][J][(][X]_ [;][ θ][)][J] _[T][ (][X]_ [;][ θ][)][ℛ][(][X] _[, θ][)][ ≡−K][(][X]_ _[,][ X]_ [;][ θ][)][ℛ][(][X] _[, θ][)][ .]_

However, the kernel K(X _, X_ ; θ) depends on θ via the Jacobian J(X ; θ) of f (X ; θ) and evolves with
time. The above system is unsolvable in general. However, under certain parameterization methods
(e.g. Sohl-Dickstein et al. (2020)) and when the network is sufficient wide, this kernel does not
change much during training and converges to a deterministic kernel called the NTK (Jacot et al.,
2018),
_K(X_ _, X_ ; θ) → Θ(X _, X_ ) as width →∞. (4)

The residual dynamics becomes a constant coefficient ODE again ℛ[˙] (X _, θ) = −Θ(X_ _, X_ )ℛ(X _, θ)._
To solve this system, we need the initial value of ℛ(X _, θ). Since the parameters θ are often initial-_
ized using iid standard Gaussian variables, as the width approach infinity, the logits f (X ; θ) converge
to a Gaussian process (GP), known as the neural network Gaussian process (NNGP). Specifically,
_f_ (X ; θ) ∼N (0; 풦(X _, X_ )), where 풦 is the NNGP kernel. Note that one can also treat infinite-width
networks as Bayesian models, a.k.a. Bayesian Neural Networks, and apply Bayesian inference to
compute the posteriors. This approach is equivalent to training only the network’s classification
layer (Lee et al., 2019) and the gradient descent dynamics is described by the kernel 풦.

As such, there are two natural kernels, the NTK Θ and the NNGP kernel 풦, associated to infinitewidth networks, whose training dynamics are governed by constant coefficient ODEs. To make
progress, it is tempting to apply Mercer’s Theorem to eigendecompose Θ and 풦, e.g.,

풦(x, ¯x) = ˆ풦(j)φj(x)φj(¯x) and Θ(x, ¯x) = ˆΘ(j)ψj(x)ψj(¯x) (5)

One advantage of applying this decomposition is that it has almost no constraint on the kernelsX X
and the inputs. However, this decomposition is too coarse to be useful since it can hardly provide
fine-grained information about the eigenstructures. E.g, it is not clear what are the corrections to
Eq. (5) when changing the architecture from a 2-layer CNN to a 4-layer CNNs. For this reason,
we choose to work on the product space of hyperspheres, which has richer mathematical structures.
Our primary goal is to characterize the analytical dependence of the decomposition Eq. (5) on the
network’s topology in the high-dimensional limit.

3 NEURAL COMPUTATIONS ON DAGS

Notations will become heavier starting from this section. In particular, we rely crucially on the
directed acyclic graphs (DAGs) and minimal spanning trees (MSTs) to define the spatial complexity
of eigenfunctions. In Sec. B, we provide a toy example to help understand the motivation.

For a positive integer p, let Sp 1 denote the unit sphere in R[p] and Sp 1 = _pSp_ 1, the sphere of
_−_ _−_ _[√]_ _−_
radius _p in R[p]. We introduce the normalized sum (integral)_

_[√]_

_f_ (x) _X_ _f_ (x) _f_ (x) _µ(X)[−][1]_ _f_ (x)µ(dx) (6)
_x∈X_ _≡|_ _|[−][1][ X]x∈X_  _x∈X_ _≡_ ˆx∈X 


-----

where X is a finite set (a measurable set with a finite positive measure µ).

We find it more convenient to express the computations in neural networks, and in neural kernels
via DAGs (Daniely et al., 2016), as both computations are of recursive nature. The associated DAG
of a network can be thought of as the same network by setting all its widths (or the number of
channels for CNNs) to 1. Let G = (N _, E) denote a DAG, where N and E are the nodes and edges,_
resp. We always assume the graph to have a unique output node o and is an ancestor of all other
_G_
nodes. Denote 0 the set of input nodes (leaves) of, i.e., the collection nodes without a
child. Each node N u ⊆N ∈N is associated with a pointwise function G _φu : R →_ R, which is normalized
in the sense Ez (0,1)φ[2]u[(][z][) = 1][ .][ It induces a function][ φ][∗]u [:][ I][ ≡] [[][−][1][,][ 1]][ →] _[I][ defined to be]_
_∈N_
_φ[∗]u[(][t][) =][ E](z1,z2)_ _t_ _[φ][u][(][z][1][)][φ][u][(][z][2][)][ .][ Here][ N][t]_ [denotes a pair of standard Gaussians with correlation]
_∈N_
_t. We associate each u ∈N a finite-dimensional Hilbert space Hu, and each uv ∈E a bounded_
linear operator Luv : Hv → Hu. Let

**_X ≡_** _uY∈N0_ **_X u ≡_** _uY∈N0_ Sdim(Hu)−1 ⊆ _uY∈N0_ Hu and **_I = I_** _[|N][0][|]_

be the input tensors and the input correlations to the graph G, resp. We associate two types of
computations to a DAG: finite-width neural network computation and kernel computation,

풩 : Ho and 풦 : I _I,_ (7)
_G_ **_X →_** _G_ _G_ _→_

resp. They are defined recursively as follows


if _u /_ 0 else 풩u(x) = xu (8)
_∈N_


풩u(x) = φu

풦u(t) = φ[∗]u


_uv(풩v(x))_
_L_

Xv:uv∈E


풦u(t) = φ[∗]u 풦v(t) if _u /_ 0 else 풦u(t) = tu (9)

_v:uv_ _∈N_

 _∈E_ 

where x and t **_I. The outputs of the computations are 풩_** (x) = 풩o (x) and 풦 (t) =
_∈_ **_X_** _∈_ _G_ _G_ _G_
풦oG (t). Note that 풦G is indeed the NNGP kernel. The NTK can also be written recursively as

Θu(t) = φ[˙] _[∗]u_ _v:uv_ 풦v(t) _v:uv_ (풦v(t) + Θv(t)) with ΘG = ΘoG _._ (10)

 _∈E_  _∈E_

Here, Θu = 0 if u ∈N0 and _φ[˙]_ _[∗]u_ [is the derivative of][ φ]u[∗] [.]

3.1 THREE EXAMPLES: MLPS, S-CNNS AND D-CNNS.

To unpack the notation, we consider three concrete examples: an L-hidden layer MLP, a shallow
convolutional network (S-CNN) that contains only one convolutional layer and a deep convolutional
network (D-CNN) that contains (1 + L) convolutional layers. The architectures are

**MLP:** [Input] → [Dense-Act][⊗][L] _→_ [Dense] (11)
**S-CNN:** [Input] → [Conv(p)-Act] → [Flatten-Dense] (12)

**D-CNN:** [Input] → [Conv(p)-Act] → [Conv(k)-Act][⊗][L] _→_ [Flatten-Dense-Act] → [Dense] (13)

where p/k is the filter size of the first/hidden layers and Act means an activation layer. We choose
the stride to be the same as the size of the filter for all convolutional layers and choose flattening as
the readout strategy rather than pooling. See Fig. 2 (a, b, c) for the DAGs associated to a (1+3)-layer
1 1
CNN (with p = k = d 4 ), a (1+1)-layer CNN (with p = k = d 2 ) and a 4-layer MLP.

**MLPs.R[n][u][×][n][v]**, whereLet G be a linked list with nu/v = dim(Hu/v) ( and the activations of the input/output nodes be the identityL + 2) nodes, including the input/output nodes. Let Luv ∈
function. Then 풩 represents a L-hidden-layer MLP. In addition, let _uv be initialized iid as_
_G_ _L_

1
_uv =_ (ωuv,ij)i [nu],j [nv], _ωuv,ij_ (0, 1) . (14)
_L_ _√nv_ _∈_ _∈_ _∼N_

converge weakly to the GPLet tx,x′ = x[T] **_x[′]/nu for u ∈N(0, 풦0 and(tx n,xv′ →∞)x,x′_** for all hidden nodes, then the outputs of ) and Θ (tx,x′ )x,x′ is the NTK in the sense 풩(X )
_GP_ _G_ _∈X_ _G_ _∈X_

in prob. in prob.
E풩G(x)풩G(x[′]) _−−−−→_ 풦G(tx,x′ ) and _⟨∇풩G(x), ∇풩G(x[′])⟩_ _−−−−→_ ΘG(tx,x′ ). (15)


Θu(t) = φ[˙] _[∗]u_


-----

L(Y2) = 0 + 2 = 2Output Node L(Y2) = [1]2 [+][ 2]2 [=][ 6]4 [, deg(Y][2][)=2] Ultra-Short-Range-Low-Frequency: L(Y2) = [3]4 [+][ 2]4 [=][ 5]4 0.8 MSE Residual vs Training Set Size: Y2

Frequency: k [= 0] Spatial: k [=][ 1]2 Spatial: k [= 0] Spatial: k [=][ 1]4 0.6

0.4

CNN(p) 4

0.20.0 CNN(pMLPMLP 41[2]) 2

Input Node 10[1] 10[2] 10[3] 10[4] 10[5]


(a) MLP


(b) CNN(p[2])[⊗][2]


(c) CNN(p)[⊗][4]


(d) MSE Residual


Figure 2: Architectures/DAGs. vs Eigenfunctions vs Learning Indices. Left to right: DAGs associated to (a) a four-layer MLP; (b) CNN(p[2])[⊗][2], a“D"-CNN that has two convolutional layer (c)
CNN(p)[⊗][4], a “HR"-CNN that has four convolutional layers; and (d) MSE (Y-axis) vs training set
size (X-axis) for Y2 obtained by NTK-regression for 4 architectures. Here Y2 is a linear combination of eigenfunctions of Short-Range-Low-Frequency interactions (deg(Y2) = 2); see Sec. D for
the expression. The DAGs are generated with p = 4. In each DAG, the Dashed Lines represent the
edges with zero weights. The Solid Lines have weights 0, [1]2 [and][ 1]4 [in (a), (b) and (c), resp. The][ col-]

**ored path represents the minimum spanning tree used to compute the spatial indices of Y2. Under**
architectures (a), (b) and (c), the spatial indices are1 0, 1[1]2 [and][ 3]4 [, resp. Each input node represents an]

input patch of dimension p[4] = d, p[2] = d 2 and p = d 4 and the frequency indices are 2, 2 × [1]2 [and]

2 × [1]4 [in (a), (b) and (c), resp.]

Indeed, note that deg(u) = 1 for all u / 0. Eq. (9) and Eq. (10) become
_∈N_

풦u(tx,x′ ) = φ[∗]u[(][풦][v][(][t][x][,][x][′] [))] and Θu(tx,x′ ) = φ[˙] _[∗]u[(][풦][v][(][t][x][,][x][′]_ [))(][풦][v][(][t][x][,][x][′] [) + Θ][v][(][t][x][,][x][′] [))][ (16)]

which are the recursive formulas for the NNGP kernel and NTK; see e.g. Sec.E in Lee et al. (2019).

**S-CNN.** The input = (Sp 1)[1][×][w] R[d], where p is the patch size, w is the number of patches,
**_X_** _−_ _⊆_
_d = pw is the dimension of the inputs. Here, the inputs have been pre-processed by a patch extractor_
and then by a normalization operator. In words, the S-CNN has one convolutional layer with filter
size p, followed by an activation function φ (e.g., Relu), and finally by a flatten-dense readout layer.
Mathematically, by letting n ∈ N be the number of channels in the hidden layer, the output (i.e.,
logit) is given by


**Convolution + Activation:** _zij(x) = φ_ _p[−]_ [1]2 _ω1,j,βxβ,i_ for _i_ [w], j [n] (17)

  _∈_ _∈_

_β∈[p]_

**Flatten + Dense:** _f_ (x) = (wn)[−] 2[1] [X] _ω2,ijzij(x),_ (18)

_i∈[wX],j∈[n]_

where ω1,i,β and ω2,ij are the parameters of the first and readout layers, resp.

We can associate a DAG G = (N _, E) to the above S-CNN. Let the input, hidden and output nodes_
be 0 = [1] [w], 1 = [w] and 2 = _o_ =, resp. and = 0 1 2. Moreover,
_uv N_ if u = × o and N v 1 or u N = (i, { ) _G}_ 1 and {∅} v = (0, i) _N_ 0 N. Let ∪N Hv = ∪N R[p] for v 0,
_∈E_ _G_ _∈N_ _∈N_ _∈N_ _∈N_
Hu = R[n] if u ∈N1 and HoG = R. The associated linear operators are given by

_uv = p[−]_ [1]2 (ω1,j,β)j [n],β [p] R[n][×][p] for (u, v) 1 0 (19)
_L_ _∈_ _∈_ _∈_ _∈N_ _× N_

_o_ _v = (wn)[−]_ [1]2 (ω2,ij)j [n] R[n] if _v = (i, )_ 1 (20)
_L_ _G_ _∈_ _∈_ _∈N_

Note that the weights are shared in the first layer but not in the readout layer (i.e., the network has no
pooling layer). To compute the NNGP kernel and NTK, we initalize all parameters ω1,i,β and ω2,ij
with iid Gaussian N (0, 1). Letting n →∞ and denoting tv = x[T]v **_[x]v[′]_** _[/p][ and][ t][ = (][t][v][)][v][∈N]0_ [, we have]


_φ[∗](tv) + φ[˙]_ _[∗](tv)_ (21)
_v∈N0_


_v∈N0_ _φ[∗](tv)_ and ΘG(t) =


풦 (t) =
_G_


**D-CNN.** The input space is = (Sp 1)[k][L][×][w] R[p][×][1][×][k][L][×][w], where p is the patch size of the
**_X_** _−_ _⊆_
input convolutional layer, k is the filter size in hidden layers, L is the number of hidden convolution


-----

layers and w is the spatial dimension of the penultimate layer. The total dimension of the input is
_d = p_ _k[L]_ _w, and the number of input nodes is_ 0 = k[L] _w. Since the stride is equal to the filter_
_·_ _·_ _|N_ _|_ _·_
size for all convolutional layers, the spatial dimension is reduced by a factor of p in the first layer, a
factor of k by each hidden layer, and is reduced to 1 by the Flatten-Dense layer. Similar to S-CNNs,
one can associate a DAG to a D-CNN. Briefly, the input layer has k[L] _× w nodes and is reduced by_
a factor of k by each convolutional layer. The penultimate and output layers have w and 1 nodes,
resp.

4 MAIN RESULTS

The goal is to obtain a precise charaterization of the relation between the eigenstructures of 풦 / Θ
and the DAG associated to the network’s architectures in the large input dimension setting. As such
we consider a sequence of graphs G = _G[(][d][)][]d∈N[, where][ G][(][d][)][ = (][N][ (][d][)][,][ E]_ [(][d][)][)][. We associate a][ finite]

set of non-negative numbers Λ to, which is called the shape parameters of,
**_G_** **_G_**   **_G_**

0 Λ [0, 1] and Λ _<_ _._ (22)
_∈_ **_G ⊆_** _|_ **_G|_** _∞_

We need several technical assumptions on G regarding the asymptotic shapes of G[(][d][)], which are
summarized as Assumption-G in Sec.G of the appendix. We list two of them which are the most
crucial ones. (1) For each non-input node u [(][d][)], there is αu Λ with deg(u) _d[α][u]_ .The
_∈N_ _∈_ **_G_** _∼_
weight associated to the edge uv is defined to be πuv _αu. (2) For each input node v, there_
is 0 < αv Λ so that the input dimension ∈E [(][d][)] _dv_ _d[α][v]_ . Here ≡ _a_ _b means a/b_ [1/C, C] for
some C > ∈ 0 independent ofG _d. The main purpose of making these two assumptions is to remove ∼_ _∼_ _∈_
non-leading terms when computing the spectra.

We say φ[∗] is semi-admissible if, for all r ≥ 1, the r-th derivative of φ[∗] at zero is non-vanishing, i.e.,
_φ[∗][(][r][)](0) > 0. If, in addition, φ[∗](0) = 0 (i.e., the activation is centered), then we say φ is admissible._
An activation φ is (semi-)admissible if φ[∗] is (semi-)admissible. Note that if φ[∗] is (semi-)admissible,
then _φ[˙]_ _[∗]_ is semi-admissible.

**Assumption-φ. We make the following assumptions on the activations. (a.) If u** 0[(][d][)], φu is the
_∈N_
identity function. (b.) If u /∈N0[(][d][)] _∪{oG}, φu is admissible. (c.) If u = oG, φu semi-admissible._

Next, we introduce the key concept which defines the spatial distance among nodes. It is the length
of the minimum spanning tree (MST) of the nodes.

**Definition 1 (Spatial Index of Nodes). Let 퓃** _⊆N_ [(][d][)]. The spatial index of 퓃 _is defined to be_

풮(퓃) = 퓃⊆T ≤Gmin [(][d][)] _uvX∈E(T )_ _πuv =_ 퓃⊆T ≤Gmin [(][d][)] _uvX∈E(T )_ deg(u; T )αu (23)

_where 퓃_ _⊆T ≤G[(][d][)]_ _means T is a sub-graph containing 퓃_ _and deg(u; T ) is the degree of u in T ._
_By default, 풮(퓃) = 0 if 퓃_ _contains only one or zero node._

Let t[r] : I _[|N][ (]0[d][)]|_ _I be a monomial, where r :_ 0[(][d][)] N[|N][ (]0[d][)]|. We use 퓃(r) = _v_ 0[(][d][)] : rv =
0 to denote the support of → **_r and 퓃(r; o_** ) = 퓃 N(r) _→o_ . _{_ _∈N_ _̸_
_}_ _G_ _∪{_ _G}_

**Definition 2 (Spatial, Frequency and Learning Indices of t[r]). We say r** N[|N][ (]0[d][)]| is (d)-learnable,
_∈_ _G_
_or learnable for short, if there is a common ancestor node u of 퓃(r) such that φu is semi-admissible._
_We use 풜(_ ) **_r_** N[N][ (]0[d][)] : r is learnable _. For r_ 풜( ), the spatial index, frequency
_G[(][d][)]_ _≡{_ _∈_ _}_ _∈_ _G[(][d][)]_
_index and the learning index are defined to be,_


_rvαv_ _and_ ℒ(r) := 풮(r) + ℱ(r), (24)


풮(r) := 풮(퓃(r; o )), ℱ(r) :=
_G_


_v_ 0[(][d][)]
_∈N_


_resp. If r /∈_ 풜(G[(][d][)]), we set 풮(r) = ℱ(r) = ℒ(r) = +∞. Let ℒ(G[(][d][)]) denote the sequence of
_learning indices in non-descending order, i.e._

ℒ( ) ℒ(r) : r 풜( ) ( _rj_ _rj+1_ _. . . )_ (25)
_G[(][d][)]_ _≡_ _∈_ _G[(][d][)]_ _≡_ _· · · ≤_ _≤_ _≤_
 


-----

Finally, for each u ∈N0[(][d][)], let {Y r,l}l∈[N (du,r)],r∈N be the family of normalized spherical harmonics in Sdu 1, where N (du, r) is the number of degree r spherical harmonics in Sdu 1. Define
_−_ _−_



[N (du, ru)] (26)


**_Y r,l(ξ) =_**


_Y ru,lu_ (ξu), **_l = (lu)u∈N (0d)_** _∈_ [N (d, r)] ≡


_u_ 0[(][d][)]
_∈N_


_u_ 0[(][d][)]
_∈N_


for ξ = (ξu) . The following is our main theorem. It describes a connection between the
_∈_ **_X_**
architecture of a network and the eigenstructure of its inducing kernels.
**Theorem 1 (Eigenspace Restructuring). Assume Assumption-G and Assumption-φ. We have**
_the following eigen-decomposition for_ = 풦 (d) or Θ (d) _. For ξ, η_
_K_ _G_ _G_ _∈_ **_X_**

_K(ξ, η) =_ _λK(r)_ **_Y r,l(ξ)Y r,l(η),_** _where_ _λK(r) ∼_ _d[−][ℒ][(][r][)]_ _if r ̸= 0 . (27)_

**_l_** **_N_** (d,r)

**_r_** NX[|N][ (]0[d][)] _|_ _∈_ X
_∈_

Coupling with the observation in Eq. (2), Theorem 1 implies that within t ∼ _d[r]_ amount of time
for gradient flow, when d is sufficiently large, only the eigenfunctions Y r,l with L(r) ≤ _r can be_
learned. By leveraging an analytical result from Mei et al. (2021a) (Sec. 3 Theorem 4), which says
under certain regularity conditions on the eigenstructure, the corresponding kernel regression acts as
a projection, Theorem 1 establishes a connection between architectures and generalization bounds
of NNGP kernel/NTK.

Let σ be the uniform (product) probability measure on X and denote L[p](X ) ≡ _L[p](X_ _, σ). For_
_X ⊆_ **_X and r /∈L(G[(][d][)]), define the regressor and the projection operator to be_**

**_RX_** (f )(x) = (x, X) (X, X)[−][1]f (X) and P>r(f ) = _f, Y r,l_ _L2(_ )Y r,l .
_K_ _K_ _⟨_ _⟩_ **_X_**

**_r:LX(r)>r_** **_l∈NX(d,r)_**

**Theorem 2. Let G = {G[(][d][)]}d, where each G[(][d][)]** _is a DAG associated to the D-CNN in Eq. (13). Let_
_r /∈L(G[(][d][)]) be fixed. Let f ∈_ _L[2](X_ ) with Eσf = 0. Then for ϵ > 0,
_∥RX_ (f ) − _f_ _∥L[2]_ [2](X ) _[−∥][P][>r][(][f]_ [)][∥]L[2] [2](X ) = cd,ϵ∥f _∥L[2]_ [2+][ϵ](X )[,] (28)

_where cd,ϵ_ 0 in probability as d _over X_ **_σ[[][d][r][]]._**
_→_ _→∞_ _∼_

In words, with [d[r]] many training samples where r /∈ ℒ(G[(][d][)]), the NNGP kernel and the NTK are
able to learn all Y r,l with L(r) < r but not any eigenfunctions with L(r) > r. In Sec. C, we show
that the number of training samples can be reduced by a factor of w if the readout layer Fattening is
replaced by the global average pooling.

5 INTERPRETATION OF THE MAIN RESULTS

We say r is the budget index if (1) (Finite Training Set) the training set size m ∼ _d[r], or (2) (Finite_
**Compute Time) the training set X = X and the total number of training steps/time t ∼** _d[r]._

**MLPs and LRLF Interactions Fig. 1 (a).** Each G[(][d][)] is a linked list. Let v be the input node.
Clearly, we have dv = d, i.e., αv = 1 and αu = 0 (since deg(u) = 1) for all other nodes u. As such

풜( ) = N 0 _, ℱ(r) =_ **_r_** _, 풮(r) = 0, ℒ(r) =_ **_r_** and ℒ (d) = ( **_r_** : r N 0 ). (29)
_G[(][d][)]_ _\{_ _}_ _|_ _|_ _|_ _|_ _G_ _|_ _|_ _∈_ _\{_ _}_

There isn’t any spatial structure since 풮(r) = 0. Given a budget index r, the kernels can only learn
span{Y r,l : r < r, r ∈ N\{0}}. Therefore, MLPs are good at modeling LRLF interactions.

**S-CNNs and SRHF Interactions Fig. 1 (b).** For one-hidden layer convolutional networks, we
have d = p × k[0] _× w, (i.e. L = 0). The number of input nodes is w ≡_ _d[α][w]_ and for each input
node v we have dv = p ≡ _d[α][p]_ . We have αp + αw = 1. Since the activation function of the
last layer is the identity function, there is no non-linear interactions between different patches and
풜( ) = **_r_** N[|N][ (]0[d][)]| **0** _,_ 퓃(r) = 1 . Therefore for r 풜( (d)),
_G[(][d][)]_ _{_ _∈_ _\{_ _}_ _|_ _|_ _}_ _∈_ _G_

풮(r) = αw, ℱ(r) = **_r_** _αp,_ ℒ = _αw +_ **_r_** _αp, r_ 풜( ) (30)
_|_ _|_ _G_ _{_ _|_ _|_ _∈_ _G[(][d][)]_ _}_


-----

Given a budget index r, the learnable r are the ones r ∈ 풜(G[(][d][)]) with

_r > αw + |r|αp = 1 + (|r| −_ 1)αp ⇒|r| < 1 + (r − 1)/αp . (31)

When αp = 1 (and thus αw = 0), this S-CNN is essentially a shallow MLP and we have |r| < r. On
the other hand, if αp is small (say αp = 0.1), this network can model interactions with much higher
frequencies, at the cost of giving up long-range interactions. Therefore, there is a trade-off between
space and frequency, and S-CNNs with small patch size are good at modeling SRHF interactions.

**D-CNNs and Interterpolation Fig. 1 (c).** Recall that d = p × k[L] _× w. Let p = d[α][p]_ _, k = d[α][k]_ and
_w = d[α][w]_ . Then we have the constraint αp + Lαk + αw = 1. The learnable terms are the ones with

_r > 풮(r) + ℱ(r) = 풮(r) +_ **_r_** _αp._ (32)
_|_ _|_

D-CNNs can simultaneously model interpolations (including the two ends) between LRLF and
SRHF, i.e. MRMF interactions. Consider two extreme cases. (i.) |퓃(r)| = 1, i.e. there is an
input node v s.t. rv = |r| and thus the non-linear interaction happens within one patch. In this
case, the length of the MST reaches its infimum 풮(r) = αw + Lαk = 1 − _αp and Eq. (32) implies_
_|r| < (r −_ 1)/αp + 1, which is exactly Eq. (31). Thus D-CNNs can model SRHF interactions; see
Fig. 6 Y5[∗][. (ii.)][ |][퓃][(][r][)][|][ =][ |][r][|][, i.e.][ r][v] [= 0][ or][ 1][ for all][ v][ ∈N]0[ (][d][)]. Then 풮(r) = **_r_** (1 _αp) and_
_|_ _|_ _−_
Eq. (32) implies |r| < r, which is the constraint for MLPs. In particular, D-CNNs can model LRLF
interactions; see Y5. By varying **_r_** and then varying 퓃(r) in [1, **_r], D-CNNs can model various_**
_|_ _|_ _|_ _|_ _|_
interpolating interactions between LRLF and SRHF.

**HR-CNNs Extrapolations and Finer Interpolations Fig. 1 (d).** The resolution of the learning
indices ℒ( ) can be improved by decreasing αk, αp and increasing L accordingly. E.g., changing
_αpG[(][d][)]_
_αresolution of the range of spatial/frequency/learning indices is doubled. This empowers the networkp →_ 2 [,][ α][k][ →] _[α][k][/][2][ and doubling the number of convolutional layers accordingly, then the]_

to model finer-grained interpolating modes, and extrapolating modes. E.g., the last equation in
Eq. (31) becomes |r| < 1 + 2(r − 1)/αp (almost doubles the upper bound of |r|) and the network
can additionally model Ultra-Short-Range-Ultra-High-Frequency interactions (see Y5[∗] [in Fig. 3)]
without sacrificing its expressivity, which isn’t the case for S-CNNs due to the space-frequency
trade-off. We refer to such networks as high-resolution CNNs (HR-CNNs). For practical networks,
e.g. ResNet (He et al., 2016), the filters/patches are already quite small and there isn’t much room to
reduce them. Equivalently, one increases the resolution of the input images instead, i.e. increasing d.
From this point of view, HR-CNNs and, therefore, our theorems justify the additional performance
gain from EfficientNet-type (Tan & Le, 2019) of model scaling.

For more details regarding computing the learning index, see Sec. D.

6 EXPERIMENTS

There are many practical consequences due to Theorem 1 and Theorem 2. We focus on two of them
which are about the impact of architectures to learning / generalization:

1. Order of Learning (Fig. 1 Green Arrow.) The order of learning is restructured from
frequency-based (MLPs) to space-and-frequency-based (CNNs).

2. Learnability (Fig. 1 Cells under the budget line.) With the same budget index, MLPLearnable ⊊ D-CNN-Learnable ⊊ HR-CNN-Learnable. Moreover, the set differences between these learnable sets are captured as in Sec.5.

Overall, we see excellent agreements between predictions from our theorems and experimental results from both practical-size networks and kernel methods using NNGP/NT kernels, even when
_d = 256 is moderate-size. We detail the setup, results, corrections, etc. for the experiments below._

**Setup.** Set d = p[4] and the input = (Sp 1)[p][3] R[p][4], where p N. Note that αp = 1/4. The
**_X_** _−_ _⊆_ _∈_
task is learning a function Y ∈ _L[2](X_ ) by minimizing the MSE, where

_Y = Y1 + Y2 + Y3 + Y4 + Y5 + Y5[∗]_ [+][ Y][6] [+][ Y][7] (33)


-----

and each eigenfunction Yi is normalized so that ∥Yi∥2[2] [=][ ∥][Y][ ∥]2[2][/][8 = 1][. The exact expressions of the]
functions can be found in Sec.D.

We optimize finite-width networks by SGD+Momentum and infinite-width networks (NNGP and
NTK) by kernel regression. We investigate three types of architectures: (1) Dense[⊗][4], a four hidden
layer MLP; (2) Conv(p[2])[⊗][2], a “deep" CNN with filter size/stride k = p[2], and (3) Conv(p)[⊗][4], a
“HS"-CNN with filter size/stride k = p. See Fig. 2 and Fig. 6 for a visualization of the associated
DAGs, and Sec. F for the code of the architectures. There is an activation φ in each hidden layer,
which is chosen so that φ[∗] is the Gaussian kernel. For the CNNs, the readout layer(s) is Flatten_Dense-Act-Dense. We carefully chose the eigenfunctions_ _Yi_ so that they cover a wide range of
_{_ _}_
space-frequency combinations (풮(Yi), ℱ(Yi)) w.r.t. Conv(p)[⊗][4]. Under Conv(p)[⊗][4], the corresponding learning indices are ℒ(Yi) = 풮(Yi) + ℱ(Yi) = 3αp + iαp = (3 + i)/4. For the learning
indices of Yi under Conv(p[2])[⊗][2] or Dense[⊗][4], see the legends in Fig.3. The purpose of doing so
is to create a “separation of learning" under Conv(p)[⊗][4], since in the large p limit, learning Yi requires d[(3+][i][)][/][4+][ϵ] examples/SGD steps. The relation between architectures and learning indices can
be “visualized" in Fig. 2 for Y2, which is short-range-low-frequency ( deg(Y2) = 2). Fig. 2 (d)
plots the test residual of Y2 vs training set size for NTK regression, as one example to showcase
the relation among architectures, learning indices and generalization. See Fig. 6 in the appendix for
other eigenfunctions.

In the experiments, the width/number of channels is set to 512 for all networks. We sample mt =
32 × 10240 (mv = 10240) data points randomly from X as training (test) set with p = 4 and d =
256. The SGD training configurations (batch size (=10240), learning rate (=1.), momentum (=0.9)
etc.) are identical across architectures. To compute the kernels, we rely crucially on NeuralTangents
(Novak et al., 2020) which is based on JAX (Bradbury et al., 2018).

In Fig.3, for each eigenfunction Yi, we plot [1]2 [E][|][ ˆ]Yi(x, t) − _Yi(x)|2[2]_ [against][ t][, where][ ˆ]Yi(x, t) is the

projection of the prediction onto Yi and t is either the training steps (SGD) or training set size
(kernels). The expectation is taken over the test set. The budget index r = log(mt)/ log(d) 2.28.
_≈_
As d = 256 (p = 4) is far from the asymptotic limit, we expect r = 2.28 being a soft cut-off between
learnable and non-learnable indices. Although the theorems assume d, p →∞, they do provide good
predictions even when d and p are far from ∞. We summarize several key observations below.

1. Dense[⊗][4] (1nd Row.) This architecture can capture all low-frequency interactions (deg =
1, 2, Y1, Y2, Y3, Y5) but fail to learn deg 3 interactions, as expected. For MLPs,
_≥_
making the network deeper won’t improve its learnability much; see Fig. 7.

2. Conv(p[2])[⊗][2] (2nd Row.) Learning curves of Y2/Y3 are separated from Y5 because the
spatial indices of them are different. Higher-frequency (deg = 3, 4) shorter-range interactions (Y4, Y6) become (partially) learnable, as L(Y4) = [8]4 [,][ L][(][Y][6][) =][ 10]4 _[< r][ ≈]_ [2][.][28][.]

3. Conv(p)[⊗][4] (3rd Row). We see L(Yi) capture the order of learning very/reasonably well
in the kernel/SGD setting. To test the ability of Conv(p)[⊗][4] in modeling ultra-shortrange-ultra-high-frequency interactions, we trace the learning progress of Y5[∗] [(][deg(][Y]5[∗][) =]
5, L(Y5[∗][) =][ 8]4 [.) As expected, while other architectures completely fail to make progress,]

the NTK/NNGP of Conv(p)[⊗][4] makes good progress and the SGD even completes the learning process. Interestingly, Y5[1] is learned faster than Y5[∗] [in the kernel setting but slower in]
the SGD setting (even slower than L(Y6) = [10]4 [), which is unexpected. We suspect it might]

be due to certain “implicit" effect of SGD. Further investigation is needed to understand it.

7 CONCLUSION

We establish a precise relation among networks’ architectures, eigenstructures of the inducing kernels, and generalization of the corresponding kernel machines. We show that deep convolutional
networks restructure the eigenspaces of the inducing kernels, which empowers them to learn a dramatically broader class of functions, covering a wide range of space-frequency combinations. We
believe our framework can be extended to study architectural inductive biases for other families of
topologies, such as RNNs, GNNs, and self-attention. However, we have not covered the learning

1Ultra-Long-Range-Low-Frequency under Conv(p)⊗4, with L(Y5) = 8/4 = L(Y5∗[)]


-----

MLP 4: NNGP Regression

|0.8|Col2|MLP|: NNG|GP Regression|Col6|n|Col8|Col9|MLP|P : NTK|K Regression M|MLP : SGD Test ML|LP : SGD Train|Col15|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|0.8 0.6 MSE 0.4 0.2|||||||||||||L(Y1) L(Y2)|= 4/4 = 8/4|
||||||||||||||L(Y3) L(Y4) *|= 8/4 = 12/4|
||||||||||||||L(Y 5) L(Y5) L(Y6)|= 20/4 = 8/4 = 16/4|
||||||||||||||L(Y7) MSE/|= 16/4 8|



0.5 0.75 1 1.25 1.5 1.75 2 2.25

log(TrainingSetSize)/ log(d)

CNN(p[2]) 2: NNGP Regression


MLP 4: NTK Regression

0.5 0.75 1 1.25 1.5 1.75 2 2.25

log(TrainingSetSize)/ log(d)

CNN(p[2]) 2: NTK Regression


MLP 4: SGD Test

0.5 0.75 1 1.25 1.5 1.75 2 2.25

log(SGDSteps)/ log(d)

CNN(p[2]) 2: SGD Test


MLP 4: SGD Train

L(Y1) = 4/4

L(Y2) = 8/4

L(Y3) = 8/4

L(Y4) = 12/4

L(Y5[*] [) = 20/4]

L(Y5) = 8/4

L(Y6) = 16/4

L(Y7) = 16/4

MSE/8

0.5 0.75 1 1.25 1.5 1.75 2 2.25

log(SGDSteps)/ log(d)

CNN(p[2]) 2: SGD Train


0.8

0.6

0.4

0.2

0.0


|0.8|Col2|CNN(p|) : NN|NGP Regressi|Col6|ion|Col8|Col9|CNN(p|p) : N|NTK Regression CNN|N(p) : SGD Test CNN|N(p) : SGD Train|Col15|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|0.8 0.6 MSE 0.4 0.2|||||||||||||L(Y1) L(Y2)|= 4/4 = 6/4|
||||||||||||||L(Y3) L(Y4) *|= 6/4 = 8/4|
||||||||||||||L(Y 5) L(Y5) L(Y6)|= 12/4 = 8/4 = 10/4|
||||||||||||||L(Y7) MSE/|= 12/4 8|


0.5 0.75 1 1.25 1.5 1.75 2 2.25

log(TrainingSetSize)/ log(d)

CNN(p) 4: NNGP Regression

|0.8|Col2|CNN(p|p) : NN|NGP Regressio|Col6|on|Col8|Col9|CNN(p|p) : N|NTK Regression CN|NN(p) : SGD Test CNN|N(p) : SGD Train|Col15|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|0.8 0.6 MSE 0.4 0.2|||||||||||||L(Y1) L(Y2)|= 4/4 = 5/4|
||||||||||||||L(Y3) L(Y4) *|= 6/4 = 7/4|
||||||||||||||L(Y 5) L(Y5) L(Y6)|= 8/4 = 8/4 = 9/4|
||||||||||||||L(Y7) MSE/|= 10/4 8|



0.5 0.75 1 1.25 1.5 1.75 2 2.25

log(TrainingSetSize)/ log(d)


0.5 0.75 1 1.25 1.5 1.75 2 2.25

log(TrainingSetSize)/ log(d)

CNN(p) 4: NTK Regression

0.5 0.75 1 1.25 1.5 1.75 2 2.25

log(TrainingSetSize)/ log(d)


0.5 0.75 1 1.25 1.5 1.75 2 2.25

log(SGDSteps)/ log(d)

CNN(p) 4: SGD Test

0.5 0.75 1 1.25 1.5 1.75 2 2.25

log(SGDSteps)/ log(d)


0.5 0.75 1 1.25 1.5 1.75 2 2.25

log(SGDSteps)/ log(d)

CNN(p) 4: SGD Train

L(Y1) = 4/4

L(Y2) = 5/4

L(Y3) = 6/4

L(Y4) = 7/4

L(Y5[*] [) = 8/4]

L(Y5) = 8/4

L(Y6) = 9/4

L(Y7) = 10/4

MSE/8

0.5 0.75 1 1.25 1.5 1.75 2 2.25

log(SGDSteps)/ log(d)


Figure 3: Learning Dynamics vs Architectures vs Learning Indices. We plot the learning/training
dynamics of each eigenfunction Yi. From top to bottom: a 4-layer MLP, a 2-layer CNN and a 4-layer
CNN. From left to right: residual MSE (per eigenfunction) of NNGP/NTK regression, test/training
MSE of SGD. The learning indices of Yi in each architecture is shown in the legends.

dynamics of SGD. In addition, it is of great interest and importance to study the combined effect of
SGD and architectures in the future.

REFERENCES

William Beckner. Inequalities in fourier analysis. Annals of Mathematics, 102(1):159–182, 1975.

William Beckner. Sobolev inequalities, the poisson semigroup, and analysis on the sphere sn. Pro_ceedings of the National Academy of Sciences, 89(11):4816–4819, 1992._

Alberto Bietti. Approximation and learning with deep convolutional models: a kernel perspective,
2021.

Alberto Bietti and Francis Bach. Deep equals shallow for relu networks in kernel regimes. arXiv
_preprint arXiv:2009.14397, 2020._

James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, and Skye Wanderman-Milne. JAX: composable transformations of Python+NumPy
[programs, 2018. URL http://github.com/google/jax.](http://github.com/google/jax)

Amit Daniely, Roy Frostig, and Yoram Singer. Toward deeper understanding of neural networks:
The power of initialization and a dual view on expressivity. In Advances In Neural Information
_Processing Systems, 2016._

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,
pp. 248–255, 2009. doi: 10.1109/CVPR.2009.5206848.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.


-----

Arturo Deza, Qianli Liao, Andrzej Banburski, and Tomaso Poggio. Hierarchically compositional
tasks and deep convolutional networks. arXiv preprint arXiv:2006.13915, 2020.

Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018.

Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2019.

Alessandro Favero, Francesco Cagnetta, and Matthieu Wyart. Locality defeats the curse of dimensionality in convolutional teacher-student scenarios, 2021.

Christopher Frye and Costas J. Efthimiou. Spherical harmonics in p dimensions. 2012.

Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Limitations of lazy
training of two-layers neural networks. arXiv preprint arXiv:1906.08899, 2019.

Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-layers
neural networks in high dimension, 2020.

Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Conference on Computer Vision and Pattern Recognition, pp. 770–778, 2016.

Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas
Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2020. URL
[http://github.com/google/flax.](http://github.com/google/flax)

Wei Hu, Lechao Xiao, Ben Adlam, and Jeffrey Pennington. The surprising simplicity of the earlytime learning dynamics of neural networks. arXiv preprint arXiv:2006.14599, 2020.

Wei Huang, Weitao Du, and Richard Yi Da Xu. On the neural tangent kernel of deep networks with
orthogonal initialization. arXiv preprint arXiv:2004.05867, 2020.

Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and generalization in neural networks. arXiv preprint arXiv:1806.07572, 2018.

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language
models. arXiv preprint arXiv:2001.08361, 2020.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pp. 1097–1105,
2012.

Yann Lecun. Generalization and network design strategies. In Connectionism in perspective. Elsevier, 1989.

Jaehoon Lee, Yasaman Bahri, Roman Novak, Sam Schoenholz, Jeffrey Pennington, and Jascha Sohldickstein. Deep neural networks as gaussian processes. In International Conference on Learning
_Representations, 2018._

Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha SohlDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in Neural Information Processing Systems, 2019.

Zhiyuan Li, Yi Zhang, and Sanjeev Arora. Why are convolutional nets more sample-efficient than
fully-connected nets? _[CoRR, abs/2010.08515, 2020. URL https://arxiv.org/abs/2010.](https://arxiv.org/abs/2010.08515)_
[08515.](https://arxiv.org/abs/2010.08515)


-----

Eran Malach and Shai Shalev-Shwartz. Computational separation between convolutional and fullyconnected networks. _CoRR, abs/2010.01369, 2020._ [URL https://arxiv.org/abs/2010.](https://arxiv.org/abs/2010.01369)
[01369.](https://arxiv.org/abs/2010.01369)

Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahramani. Gaussian process behaviour in wide deep neural networks. In International Conference on
_Learning Representations, 2018._

Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Generalization error of random features and kernel methods: hypercontractivity and kernel matrix concentration. arXiv preprint
_arXiv:2101.10588, 2021a._

Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Learning with invariances in random
features and kernel models. arXiv preprint arXiv:2102.13219, 2021b.

Ashley Montanaro. Some applications of hypercontractive inequalities in quantum information theory. Journal of Mathematical Physics, 53(12):122206, 2012.

Grégoire Montavon, Wojciech Samek, and Klaus-Robert Müller. Methods for interpreting and understanding deep neural networks. Digital Signal Processing, 73:1–15, 2018.

Preetum Nakkiran, Gal Kaplun, Dimitris Kalimeris, Tristan Yang, Benjamin L. Edelman, Fred
Zhang, and Boaz Barak. SGD on neural networks learns functions of increasing complexity.
_[CoRR, abs/1905.11604, 2019. URL http://arxiv.org/abs/1905.11604.](http://arxiv.org/abs/1905.11604)_

Roman Novak, Lechao Xiao, Yasaman Bahri, Jaehoon Lee, Greg Yang, Daniel A. Abolafia, Jeffrey
Pennington, and Jascha Sohl-dickstein. Bayesian deep convolutional networks with many channels are gaussian processes. In International Conference on Learning Representations, 2019a.
[URL https://openreview.net/forum?id=B1g30j0qF7.](https://openreview.net/forum?id=B1g30j0qF7)

Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A. Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with
many channels are gaussian processes. In International Conference on Learning Representations,
2019b.

Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A. Alemi, Jascha Sohl-Dickstein,
and Samuel S. Schoenholz. Neural tangents: Fast and easy infinite neural networks in python.
[In International Conference on Learning Representations, 2020. URL https://github.com/](https://github.com/google/neural-tangents)
[google/neural-tangents.](https://github.com/google/neural-tangents)

Yaniv Ovadia, Emily Fertig, J. Ren, Zachary Nado, D. Sculley, Sebastian Nowozin, Joshua V. Dillon,
Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? evaluating
predictive uncertainty under dataset shift. In NeurIPS, 2019.

Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. In Advances In Neural
_Information Processing Systems, 2016._

Meyer Scetbon and Zaid Harchaoui. Harmonic decompositions of convolutional networks. In Inter_national Conference on Machine Learning, pp. 8522–8532. PMLR, 2020._

Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information
propagation. International Conference on Learning Representations, 2017.

Andrew W Senior, Richard Evans, John Jumper, James Kirkpatrick, Laurent Sifre, Tim Green,
Chongli Qin, Augustin Žídek, Alexander WR Nelson, Alex Bridgland, et al. Improved protein
structure prediction using potentials from deep learning. Nature, 577(7792):706–710, 2020.

Vaishaal Shankar, Alex Chengyu Fang, Wenshuo Guo, Sara Fridovich-Keil, Ludwig Schmidt,
Jonathan Ragan-Kelley, and Benjamin Recht. Neural kernels without tangents. In International
_Conference on Machine Learning, 2020._


-----

David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. Nature, 529(7587):484–489, 2016.

Jascha Sohl-Dickstein, Roman Novak, Samuel S. Schoenholz, and Jaehoon Lee. On the infinite
width limit of neural networks with a standard parameterization, 2020.

Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine Learning, pp. 6105–6114. PMLR, 2019.

M. Alex O. Vasilescu, Eric Kim, and Xiao S. Zeng. Causalx: Causal explanations and block multilinear factor analysis, 2021.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762,
2017.

Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
_arXiv:1011.3027, 2010._

Lechao Xiao and Jeffrey Pennington. What breaks the curse of dimensionality in deep learning?,
[2021. URL https://openreview.net/forum?id=KAV7BDCcN6.](https://openreview.net/forum?id=KAV7BDCcN6)

Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Pennington.
Dynamical isometry and a mean field theory of cnns: How to train 10,000-layer vanilla convolutional neural networks. In International Conference on Machine Learning, pp. 5393–5402,
2018.

Greg Yang and Edward J Hu. Feature learning in infinite-width neural networks. arXiv preprint
_arXiv:2011.14522, 2020._

Greg Yang and Hadi Salman. A fine-grained spectral perspective on neural networks, 2020.

Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107–
115, 2021.


-----

A ADDITIONAL RELATED WORK

Bietti (2021) studies the approximation and learning properties of CNN kernels via the lens of
RKHS. Impressively, they demonstrate that a 2-layer CNN kernel can reach 88.3% validation accuracy on CIFAR-10, matching the performance of a 10-layer Myrtle kernel Shankar et al. (2020).
Malach & Shalev-Shwartz (2020) and Li et al. (2020) study the algorithmic benefits of shallow
CNNs and show that they outperform MLPs in certain tasks. Xiao & Pennington (2021) and Favero
et al. (2021) study the benefits of locality in S-CNNs and argue that locality is the key ingredient
to defeat the curse of dimensionality. Mei et al. (2021b) and several papers mentioned above study
the benefits of pooling in (S-)CNNs in terms of data efficiency. Their conclusion is similar to that
of Theorem 4 (b): pooling improves data efficiency by a factor of the pooling size. In addition,
we show that (Theorem 4 (a)) pooling does not improve training efficiency for D-CNNs, extending
a result from Xiao & Pennington (2021) which concerns S-CNNs. Finally, Scetbon & Harchaoui
(2020) also study the eigenstructures of certain CNN kernels without pooling. Their kernels can be
considered as a particular case of the NNGP kernels, where the associated networks have only one
convolutional layer and multiple dense layers. The key contribution that sets the current work apart
from existing work is a precise mathematical characterization of the fundamental role of architectures in (infinite-width) networks through a space-frequency analysis.

B A TOY EXAMPLE AND MOTIVATION

In this section, we provide a toy example to help understand the motivation and ideas of the paper.
Let’s consider learning the following polynomials in Sd 1 _x_ R[d] : _x_ 2 = 1 using (in)finitewidth neural networks and for concreteness we have set− d = 10 ≡{ : ∈ _∥_ _∥_ _}_

_f1(x) = x9, f2(x) = x0x1, f3(x) = x0x8, f4(x) = x6x7(x[2]6_ 7[)][, f][5][(][x][) =][ x][2][x][3][x][5] (34)

_[−]_ _[x][2]_

Which architectures (e.g. MLPs, CNNs) can efficiently learn fi or the sum of fi? More precisely, (1)
if we have sufficiently amount of training data, how much time (compute) is required to learn fi for
a given architecture? (2) Alternatively, if we have sufficiently amount of compute, how much data is
needed to learn fi? To answer these questions, one crucial step is to provide a meaningful definition
of “learning complexity" of a function fi under an architecture M. Denote this complexity associated to compute and to data by _C(fi;_ ) and _D(fi;_ ), resp. With such the complexity properly
_C_ _M_ _C_ _M_
defined, the questions are reduced to solving the min-max problem min maxi _C/D(fi;_ ), if
_M_ _{C_ _M_ _}_
the task is, e.g, to learn the sum of fi.

Let’s focus on the complexity. For infinite-width MLPs, aka, inner product kernels, it is wellknown that they have the inductive biases (Yang & Salman, 2020; Ghorbani et al., 2020) (known
as the frequency biases) that the model prioritizes learning low-frequency modes (i.e., low degree
polynomials) over high-frequency modes. In addition, the models require ∼ _d[r]_ many data points to
learn any degree r polynomials in R[d]. The frequency biases of MLPs are the consequence of the fact
that the eigenspaces of inner product kernels are structured based only on frequencies. Specific to our
example, for MLP, the order of learning is f1/f2, f3/f5/f4 and it requires about 10/10[2], 10[2]/10[4]
many data points to learn the functions. Clearly, the model is very inefficient in learning f4, the
high-frequency modes.

To improve the learning efficiency, we must take the modality of the task into account, which is
overlooked by MLPs. We observe that: (1) although of high frequency, f4 depends only on two consecutive terms x6 and x7, which are spatially close; (2) in contrast, f3(x) = x0x8 is of low frequency
but the spatial distance between the two interaction terms x0 and x8 are “far" from each other; (3) the
function f5(x) = x2x3x5 is somewhere in-between: the order of interaction is 3 (lower than that of
_f4) and the spatial distance (not yet defined) among interaction terms is conceptually “closer" than_
that of f3(x) = x0x8, but “farther" than that of f5. Using the terminologies from the introduction,
the functions f2/f3/f5/f4 model interactions of types: Short-Range-Low-Frequency/Long-RangeLow-Frequency/Median-Range-Median-Frequency/Short-Range-High-Frequency. By Range we
mean the distance among interaction terms and by Frequency we mean the order(=degree) of interactions. Clearly, the MLPs are inefficient since they totally ignore the “spatial structure" of the
functions. As such, a good architecture must balance the "spatial structure" and the “frequency
structure" of the functions. For the same reason, a good complexity measure must (1) be able to
capture both the frequency of the functions and the spatial distance among interaction terms; (2)


-----

be able to precisely characterize the data and the computation efficiency of learning and their dependence on architectures. The learning index mentioned in the introduction satisfies these two
conditions. It is the sum of the frequency index and the spatial index. The former measures the
order (=degree=frequency) of interactions, which depends on how the network partitions the input
into patches. The latter measures the spatial distance among the interaction terms, which depends on
how the network organizes these patches hierarchically. Later we show that, in the high-dimensional
setting, the learning index provides a sharp characterization for the learnability of eigenfunctions,
and certain CNNs can perfectly balance the learning of f3 and f4, i.e., informally
_C/D(f3; CNN)_ _C/D(f4; CNN)_ _C/D(f2/3; MLP) <<_ _C/D(f3; MLP)_ (35)
_C_ _≈C_ _≈C_ _C_
See Fig. 3 in the experiment section for more details.

C GLOBAL AVERAGE POOLING (GAP) VS FLATTENING

In this section, we compare two readout strategies for CNNs: GAP and Flatten

C.1 INFINITE-TRAINING DATA.

First, let’s state a theorem regarding training efficiency in the infinite-training-data-finite-trainingtime regime, which follows directly from the same arguments in Sec. 2. The theorem requires
knowing only the eigenvalues of the kernels.

Let F : L[2](X ) → _L[2](X_ ) be the solution operator to the kernel descent _h[˙]_ = −K(h _−_ _f_ ) with initial
value ht=0 = 0, i.e. ht ≡ **_F_** _t(f_ ) ≡ (Id − _e[−K][t])f_ . Then we have for t ∼ _d[r], F_ _t ≈_ **_P<r._**

**Theorem 3. Assume Assumption-** _and Assumption-φ and_ = 풦 (d) or Θ (d) _. Let r /_ ( )
**_G_** _K_ _G_ _G_ _∈L_ _G[(][d][)]_
_and t ∼_ _d[r]. Then for 0 < ϵ < inf{|r −_ _r¯| : ¯r ∈L(G[(][d][)])} and f ∈_ _L[2](X_ ) with Eσf = 0 we have

_∥F_ _t(P<rf_ ) − **_P<rf_** _∥2[2]_ [≲] _[e][−][d][ϵ]_ _[∥][P][<r][f]_ _[∥]2[2]_ _and_ _∥F_ _t(P>rf_ ) − **_P>rf_** _∥2[2]_ [≳] _[e][−][d][−][ϵ]_ _[∥][P][>r][f]_ _[∥]2[2]_
(36)
_for d sufficiently large._

In words, in the infinite-training-data-finite-training-time regime, within t ∼ _d[r]_ amount of time only
the eigenfunctions Y r,l with L(r) < r are learnable.

C.2 GAP VS FLATTENING.

Let a CNN with and without a global average pooling (GAP) be defined as

**CNN+GAP** [Input] → [Conv(p)-Act] → [Conv(k)-Act][⊗][L] _→_ [GAP] → [Dense] (37)

**CNN+Flatten** [Input] → [Conv(p)-Act] → [Conv(k)-Act][⊗][L] _→_ [Flatten] → [Dense] (38)
resp. Note that there isn’t any activation after the GAP/Flatten layer. The DAGs associated to
_CNN+GAP and CNN+Flatten are identical, and the associated kernel and network computations in_
each layer are also identical but the last layer; see Sec. K for more details. Our last theorem states
that the GAP improves the data efficiency by a factor of w, the window size of the pooling, but
does not improve the training efficiency in the infinite-training-data-finite-training-time regime. The
former is due to the dimension reduction effect of GAP in the eigenspaces and the latter is due to
the fact that the GAP layer does not change the eigenvalues of the associated eigenspaces.

Let KSym = 풦Sym or ΘSym be the NNGP kernel or NTK associated to Eq. (37) and L[p]Sym[(][X] [)][ ≤]
_L[p](X_ ) be the subspace of “translation-invariant" functions, whose co-dimension is w. Let F [Sym],
**_P_** [Sym] and R[Sym] be the solution operator, projection operator and regressor associated to KSym, resp.
**Theorem 4 (Informal). For the architectures defined as in by Eq. (37), we have**

_(a) Theorem 3 holds with L[2](X_ ), F and P replaced by L[2]Sym[(][X] [)][,][ F] _[Sym][ and][ P][ Sym][, resp.]_

_(b) Eq. (28) holds with L[p](X_ ), R and P replaced by L[p]Sym[(][X] [)][,][ R][Sym][ and][ P][ Sym][, resp and]

_with X ∼_ **_σ[[][d][r][−][αw][ ]]_** _under the assumptions that all activations in the hidden layers are_
_poly-admissible._


-----

**Remark 1. Several remarks are in order.**

_1. In terms of order of learning, our results say that (infinite-width) neural networks pro-_
_gressively learn more complex functions, where complexity is defined to be the learning_
_index L(r). This is consistent with the empirical observation from Nakkiran et al. (2019)._
_Regardless of architectures (MLPs vs CNNs), linear functions have the smallest learn-_
_ing index[2]_ _L(r) = 1 and are always learned first, which was first proved in Hu et al._
_(2020) for MLPs. After learning the linear functions, the learning dynamics of MLPs_
_and CNNs diverge. MLPs will start to learn quadratic functions (L(r) = 2), then cu-_
_bic functions (L(r) = 3) and so on. While for CNNs, L(r) can be fractional numbers (e.g._
_L(r) = 5/4, 6/4) and it is possible for the network to learn higher order functions before_
_lower order functions. See Sec.D and Sec. 6 for more details._

_2. By NTK-style convergent arguments (e.g., Du et al. (2019)), the above kernel regression_
_results could be extended to the over-parameterized network setting as long as the learning_
_rate of gradient descent is small and the number of channels is sufficiently large (polyno-_
_mially in d (Huang et al., 2020))._

_3. Theorem 4 states that CNN+GAP is better than CNN+Flatten in terms of data efficiency but_
_not training efficiency. In particular, when the dataset size is sufficiently large, CNN+GAP_
_and CNN+Flatten perform equally well. Therefore, in the large (infinite) data set regime,_
_CNN+Flatten may be preferable since the associated function class is less restricted._

C.3 EXPERIMENTAL RESULTS: GAP VS FLATTEN.

We compare the SGD learning dynamics of two convolutional architectures: Conv(p)[⊗][3]-Flatten and
Conv(p)[⊗][3]-GAP. Both networks have three convolutional layers with filter size and stride equal to
_p. The spatial dimension is reduced to p after the convolutional layers. The only difference is that_
the architecture Conv(p)[⊗][3]-Flatten ( Conv(p)[⊗][3]-GAP) uses a Flatten-Dense ( GAP-Dense) to map
the penultimate layer to the logit layer.

The experimental setup is almost the same as that of Sec. 6 except the eigenfunctions _Yi_ are chosen
_{_ _}_
to be in the RKHS of the NNGP kernel/NTK of Conv(p)[⊗][3]-GAP and thus of Conv(p)[⊗][3]-Flatten,
i.e., they are shifting-invariant (the invariant group is of order p). Moreover, we still have ℒ(Yi) =
(i + 3)/4. For each Yi, we plot the validation MSE of the residual vs SGD steps in Fig. 4. Overall,
the predictions from Theorem 4 gives excellent agreement with the empirical result. With training
set size mt = 32 × 10240, the residuals of Yi for GAP and Flatten are almost indistinguishable
from each other for i 6 (recall that ℒ(Y6) = 9/4 = 2.25 < r 2.28). However, when i = 7
_≤_ _≈_
the dataset size (r 2.28) is relatively small compared to the learning index (ℒ(Y7) = 2.5), GAP
_≈_
outperforms Flatten in learning Y7

To test the robustness of the prediction from Theorem 4 (a) on more practical datasets and models,
we perform an additional experiment on ImageNet (Deng et al., 2009) using ResNet (He et al.,
2016). We compare the performance of the original ResNet50, denoted by ResNet50-GAP, and a
modified version ResNet50-Flatten, in which the GAP readout layer is replaced by Flatten. We
use the ImageNet codebase from FLAX[3](Heek et al., 2020). In order to see how the performance
difference between ResNet50-GAP and ResNet50-Flatten evolves as the training set size increases,
we make a scaling plot, namely, we vary the training set sizes[4] _mi = [m × 2[−][i/][2]] for i = 0, . . ., 11,_
where m = 1281167 is the total number of images in the training set of ImageNet. The networks
are trained for 150 epochs with batch size 128. We plot the validation accuracy and loss (averaged
over 3 runs) as a function of training set size mi in Fig. 5. Overall, we see that the performance gap
between ResNet50-GAP and ResNet50-Flatten shrink substantially as the training set size increases.
E.g, using 1/8 of the training set (i.e. i = 6), the top 1 accuracy between the two is 19.3% (57.7%
GAP vs 38.4% Flatten). However, with the whole training set (i.e., m0), this gap is reduced to
2% (76.5% GAP vs 74.5% Flatten). To demonstrate the robustness of this trend, we additionally
generate the same plots for ResNet34 and ResNet101; see Fig. 8 in Sec. E.

2Ignoring the constant functions.
3https://github.com/google/flax/blob/main/examples/imagenet/README.md
4Standard data-augmentation is applied for each i; see input_pipeline.py.


-----

Valid Loss: GAP vs Flatten


0.5 0.75 1 1.25 1.5 1.75 2 2.25

|0.4 MSE 0.2 0.0|Col2|Col3|Y1:G Y1:F Y7:G|AP latten AP|
|---|---|---|---|---|
||||Y7:F|latten|
||||||
||||||


log(SGDSteps)/ log(d)

Figure 4: Learning Dynamics: GAP vs Flatten. We plot the validation MSE of the residual of
each Yi (left → **right: i = 1 →** 7) for GAP (Solid lines) and Flatten (Dashed lines). The mean/std
in each curve is obtained by 5 random initializations. Clearly, the residual dynamics of GAP and
Flatten are almost indistinguishable for Yi with i ≤ 6. However, GAP outperforms Flatten when
learning Y7.

|ResNet50-G ResNet50-F|AP latten|Col3|
|---|---|---|
||||
||||
||||

|Col1|ResNet50-GAP ResNet50-Flatt|en|
|---|---|---|
||||
||||
||||


Accuracy: GAP vs Flatten Loss: GAP vs Flatten

ResNet50-GAP ResNet50-GAP
ResNet50-Flatten ResNet50-Flatten

6

0.6

4

0.4

Top 1 Acc ImageNet

2

0.2 Validation Cross-Entropy

10[5] 10[6] 10[5] 10[6]

Training Set Size Training Set Size


Figure 5: ResNet50-GAP vs ResNet50-Flatten. As the training set size increases the performance
(accuracy and loss) gap between the two shrinks.

D DAGS, EIGENFUNCTIONS, SPATIAL INDEX, AND FREQUENCY INDEX.


In this section, we provide more details regarding the DAGs and the eigenfunctions used in the
experiments, and how the spatial, frequency and learning indices are computed.

Let (Sp 1)[p][3] R[p][4] be the input space, where d = p[4] is the input dimension. We use x
_−_ _⊆_ _≡_
(xk)k [p]4 (Sp 1)[p][3] to denote one input (an image), where k = [k1, k2, k3, k4] [p][4]. In
_∈_ _∈_ _−_ _∈_
addition, we treat [p][4] as a group (i.e. with circular boundaries) and let e1 = [1, 0, 0, 0], e2 =

[0, 1, 0, 0], e3 = [0, 0, 1, 0] and e4 = [0, 0, 0, 1] be a set of generator/basis of the group. Note that
each input xk is partitioned into p[3] many patches: {xk1,k2,k3,: : k1, k2, k3 ∈ [p]}.


-----

The eigenfunctions used in the experiments and the associated space/frequency indices (will be
explained momentarily) are given as follow

**Eigenfunction** **degree** **Space/Freq Index**

MLP CNN(p[2])[⊗][2] CNN(p)[⊗][4]


**1**
_c[(1)]k_ **_[x][k]_** **1** **0/1**
**2** _[/]_ **2[1]**
**_k∈X[p−1][4]_**

**1**
_c[(2)]k_ **_[x][k][x][k][+][e][4]_** **2** **0/2** **2** _[/]_ **2[2]**
**_k∈X[p−1][4]_**

**1**
_c[(3)]k_ **_[x][k][+][e][3]_** **_[x][k][+][e][4]_** **2** **0/2** **2** _[/]_ **2[2]**
**_k∈X[p−1][4]_**

**1**
_c[(4)]k_ **_[x][k][+][e][3][+][e][4]_** **_[x][k][+][e][4]_** **_[x][k]_** **3** **0/3**
**2** _[/]_ **2[3]**
**_k∈X[p−1][4]_**

**1**
**_k∈X[p−1][4]_** _c[(5]k_ _[∗][)]xkxk+e4_ **_xk+2e4_** (x[2]k _[−]_ **_[x]k[2]_** +e4 [)] **5** **0/5** **2** _[/]_ **2[5]**

**2**
_c[(5)]k_ **_[x][k][x][k][+][e][1]_** **2** **0/2** **2** _[/]_ **2[2]**
**_k∈X[p−1][4]_**

**1**
_c[(6)]k_ **_[x][k][x][k][+][e][3]_** [(3][x]k[2] **_e3_** **_k[)]_** **4** **0/4**
_−_ **2** _[/]_ **2[4]**
**_k∈X[p−1][4]_** _[−]_ **_[x][2]_**

**1**
_c[(7)]k_ **_[x][k][−][e][3][+][e][4]_** **_[x][k][+][e][2]_** [(3][x]k[2] **_k_** **_e3+e4_** [)] **4** **0/4**
**_k∈X[p−1][4]_** _[−]_ **_[x][2]_** _−_ **2** _[/]_ **2[4]**


**3**
**4** _[/]_ **[1]4**

**3**
**4** _[/]_ **[2]4**

**4**
**4** _[/]_ **[2]4**

**4**
**4** _[/]_ **[3]4**

**3**
**4** _[/]_ **[5]4**

**6**
**4** _[/]_ **[2]4**

**5**
**4** _[/]_ **[4]4**

**6**
**4** _[/]_ **[4]4**


**Y1(x) =**

**Y2(x) =**

**Y3(x) =**

**Y4(x) =**

**Y5[∗][(][x][) =]**

**Y5(x) =**

**Y6(x) =**

**Y7(x) =**


Each (eigen)function Yi is a linear combination of basis eigenfunction of the same type, in the sense
they have the same eigenvalue and the same spatial/frequency/learning indices. The coefficients c[(]k[i][)]
are first sampled from standard Gaussians iid and then multiplied by an i-dependent constant so that
_Yi has unit norm[5]. In the experiments, p is chosen to be 4 and the target is defined to be sum of them_

_Y =_ _Yi_ (39)

Since they are orthogonal to each other, ∥Y ∥2[2][/][8 =]X[ ∥][Y][i][∥][2]2 [= 1][ where the][ L][2][-norm is taken over the]
uniform distribution on (Sp 1)[p][3] . In the experiment when we compare Flatten and GAP ( Fig. 4),
_−_
we set all ck[(][i][)] [to be the same (note that we also remove][ Y][5][ from the target function), so that the]
functions are learnable by convolutional networks with a GAP readout layer.

We compare three architectures. Fig. 6 Column (a) MLP[⊗][4], a (four-layer) MLP, the most coarse
architecture used in the paper. Fig. 6 Column (b), CNN(p[2])[⊗][2], a“D"-CNN that contains two convolutional layers with filter size/stride equal to p[2]. Fig. 6 Column (c), CNN(p)[⊗][4], a “HR"-CNN,
the finest architecture used in the experiments, that contains four convolutional layers with filter
size/stride equal to p. In all experiments except the one in Fig. 4, we use Flatten as the readout layer
for the convolutional networks and add a Act-Dense layer after Flatten to improve the expressivity
of the function class. However, in the Flatten vs GAP experiments, Fig. 4, we have only one dense
layer after GAP/Flatten.

We show how to compute the frequency index, the spatial index and the learning index through
three examples. We focus on Y2 / Y3 / Y5[∗] [which have degree 2 / 2 / 5, resp. The indices of other]
eigenfunctions can be computed using the same approach. We use Dashed Lines to represent either
an edge connecting an input node to a node in the first-hidden layer or an edge associated to a dense
layer. In either case, the corresponding output node of the edge has degree O(1) and thus the weights
(of the DAGs) of such edges are always 0. Only Solid Lines are relevant in computing the spatial
_index. Since each Yi is a linear combination of basis eigenfunctions of the same type, we only need_
to compute the indices of one component. We use the k = 0 component, which corresponds to the

5In our experiments, they are normalized over the test set.


-----

colored path in each DAG. Recall that p = 4, d = p[4] = 256 and mt = 32 _×_ 10240 ∼ _d[2][.][28]_ (training
set size), i.e. the budget index is roughly r = 2.28.

(a.) MLP Column (a) Fig. 6. The NTK and NNGP kernels are inner product kernels and
the associated DAGs are linked lists. The corresponding DAG has only one input node
whose dimension is equal to d = p[4]. The spatial index is always 0 since the degree of
each hidden node is 1 (since 1 = d[0]) and the frequency index is equal to the degree of
the eigenfunctions. Thus ℒ(Y2) = ℒ(Y3) = 2 and ℒ(Y5[∗][) = 5][. Changing the number]
of layers won’t change the learning indices. In sum, learning Y2/Y3/Y5[∗] [using infinite-]
width MLP requires d[2][+] _/d[2][+]_ _/d[5][+]_ many samples /SGD steps. Clearly, Y5[∗] [is completely]
unlearnable as r = 2.28 << 5. In the MSE plot Y5[∗] [(5-th row in Fig. 6), the][ Red Lines]

does not make any progress

(b.) CNN(p[2])[⊗][2] Column (b) Fig. 6. The input image is partitioned into p[2] patches and each
patch has dimension p[2]. The second layer of the DAG has p[2] many nodes, each node
represents one pixel (with many channels) in the first hidden layer of a finite-width ConvNet. After one more convolutional layer with filter size/stride p[2], the number of node
(pixel) is reduced to one. The remaining part of the DAG is essentially a linked list
(Dashed Line) with length equal to 1, which corresponds to the Act-Dense layer. The
frequency index ℱ(Y2) = 2 [1]2 [= 1][. This is because the degree of][ Y][2][ is 2 and the input]

dimension of a node is p[2] = d[1][/][2]. The spatial index is equal to 1/2, since the minimum tree containing xkxk+e4 has only one non-zero edge (Solid Lines) whose weight
is equal to 1/2 (since the degree of the output node is p[2] = d[1][/][2]); see the colored paths
in Fig. 6 Column (b). Therefore the learning index of ℒ(Y2) = 1 + 1/2 = 3/2. Similarly ℒ(Y3) = 1 + 1/2 = 3/2, as the term xk+e3 **_xk+e4 are lying in the same patch_**
of size p[2] for all k, and ℒ(Y5[∗][) = 5][/][2 + 1][/][2 = 3][. In sum, learning][ Y][2][/][Y][3][/][Y]5[∗] [using]
infinite-width CNN(p[2])[⊗][2] requires d[1][.][5][+] _/d[1][.][5][+]_ _/d[3][+]_ many samples /SGD steps. While
neither infinite-width CNN(p[2])[⊗][2] nor MLP[⊗][4] distinguishes Y2 from Y3, CNN(p[2])[⊗][2]

does improve the learning efficiency: d[2][+] _→_ _d[1][.][5][+]_ . Note that Y5[∗] [is still unlearnable as]
_r = 2.28 < 3 = ℒ(Y5[∗][)][. In the MSE plot][ Y]5[∗]_ [(5-th row in Fig. 6), the][ Orange Line][ does]
not make any progress.

(c.) CNN(p)[⊗][4] Column (c) Fig. 6. The input image is partitioned into p[3] patches and each
patch has dimension p. The second/third/fourth/output layer of the DAG has p[3]/p[2]/p/1
many nodes. The frequency indices are: ℱ(Y2) = ℱ(Y3) = 2 [1]4 [= 1][/][2][ and][ ℱ][(][Y]5[∗][) =]

5 [1]4 [= 5][/][4][. This is because the size of input nodes is reduced to][ p][ =][ d][1][/][4][. Unlike the]

above cases, the spatial indices become different. The two interacting terms in xkxk+e4
while the two interacting terms inand the three interacting terms in x xkk++ee43xxkk+2+ee44 and are in two different input nodes. As a(x[2]k _[−]_ **_[x]k[2]_** +e4 [)][ are in the same input node]
consequence, the minimum spanning tree (MST) that contains xk and xk+e4 and the one
contains xk, xk+e4 and xk+2e4 are the same. They have 3 solid lines. However, the MST
containing xk+e3 and xk+e4 has 4 solid lines. Therefore 풮(Y2) = 풮(Y5[∗][) = 3][ ×][ 1]4 [and]

풮(Y3) = 4 × 4[1] [. As such,][ ℒ][(][Y][2][) =][ 5]4 [,][ ℒ][(][Y][3][) =][ 6]4 [and][ ℒ][(][Y]5[∗][) =][ 8]4 [. In sum, learning]

**Y2/Y3/Y5[∗]** [using infinite-width CNN][(][p][)][⊗][4][ requires][ d][1][.][25][+] [/][ d][1][.][5][+] [/][ d][2][+][ many samples]
/SGD steps, resp. Now ℒ(Y5[∗][) = 2][. <][ 2][.][28][ and in the MSE plot][ Y]5[∗] [(5-th row in Fig. 6),]
the Blue Line does make significant progress (the MSE is reduced to ∼ 0.2.)


-----

L(Y1) = 0 + 1 = 1Output Node L(Y1) = [1]2 [+][ 1]2 [=][ 4]4 [, deg(Y][1][)=1] Ultra-Low-Frequency: L(Y1) = [3]4 [+][ 1]4 [=][ 4]4 0.8 MSE Residual vs Training Set Size: YCNN(p)1 4

Frequency: k [= 0] Spatial: k [=][ 1]2 Spatial: k [= 0] Spatial: k [=][ 1]4 0.6 CNN(pMLPMLP 41[2]) 2

0.4

0.2

0.0

Input Node 10[1] 10[2] 10[3] 10[4] 10[5]

L(Y2) = 0 + 2 = 2Output Node L(Y2) = [1]2 [+][ 2]2 [=][ 6]4 [, deg(Y][2][)=2] Ultra-Short-Range-Low-Frequency: L(Y2) = [3]4 [+][ 2]4 [=][ 5]4 0.8 MSE Residual vs Training Set Size: Y2

Frequency: k [= 0] Spatial: k [=][ 1]2 Spatial: k [= 0] Spatial: k [=][ 1]4 0.6

0.4

CNN(p) 4

0.20.0 CNN(pMLPMLP 41[2]) 2

Input Node 10[1] 10[2] 10[3] 10[4] 10[5]

L(Y3) = 0 + 2 = 2Output Node L(Y3) = [1]2 [+][ 2]2 [=][ 6]4 [, deg(Y][3][)=2] Short-Range-Low-Frequency: L(Y3) = [4]4 [+][ 2]4 [=][ 6]4 0.8 MSE Residual vs Training Set Size: Y3

Frequency: k [= 0] Spatial: k [=][ 1]2 Spatial: k [= 0] Spatial: k [=][ 1]4 0.6

0.4

CNN(p) 4

0.20.0 CNN(pMLPMLP 41[2]) 2

Input Node 10[1] 10[2] 10[3] 10[4] 10[5]

L(Y4) = 0 + 3 = 3Output Node L(Y4) = [1]2 [+][ 3]2 [=][ 8]4 [, deg(Y][4][)=3] Short-Range-Median-Frequency: L(Y4) = [4]4 [+][ 3]4 [=][ 7]4 0.8 MSE Residual vs Training Set Size: Y4

Frequency: k [= 0] Spatial: k [=][ 1]2 Spatial: k [= 0] Spatial: k [=][ 1]4 0.6

0.4 CNN(p) 4

0.20.0 CNN(pMLPMLP 41[2]) 2

Input Node 10[1] 10[2] 10[3] 10[4] 10[5]

L(Y5[*] [) = 0 + 5 = 5]Output Node L(Y5[*] [) =][ 1]2 [+][ 5]2 [=][ 12]4 [, deg(Y][5][)=5] Ultra-Short-Range-Ultra-High-Frequency: L(Y5[*] [) =][ 3]4 [+][ 5]4 [=][ 8]4 0.8 MSE Residual vs Training Set Size: Y5

Frequency: k [= 0] Spatial: k [=][ 1]2 Spatial: k [= 0] Spatial: k [=][ 1]4 0.6

0.4 CNN(p) 4

0.2 CNN(pMLPMLP 41[2]) 2

Input Node 0.010[1] 10[2] 10[3] 10[4] 10[5]

L(Y5) = 0 + 2 = 2Output Node L(Y5) = [2]2 [+][ 2]2 [=][ 8]4 [, deg(Y][5][)=2] Utlra-Long-Range-Low-Frequency: L(Y5) = [6]4 [+][ 2]4 [=][ 8]4 0.8 MSE Residual vs Training Set Size: Y8

Frequency: k [= 0] Spatial: k [=][ 1]2 Spatial: k [= 0] Spatial: k [=][ 1]4 0.6

0.4

CNN(p) 4

0.20.0 CNN(pMLPMLP 41[2]) 2

Input Node 10[1] 10[2] 10[3] 10[4] 10[5]

L(Y6) = 0 + 4 = 4Output Node L(Y6) = [1]2 [+][ 4]2 [=][ 10]4 [, deg(Y][6][)=3] Long-Range-High-Frequency: L(Y6) = [5]4 [+][ 4]4 [=][ 9]4 0.8 MSE Residual vs Training Set Size: Y6

Frequency: k [= 0] Spatial: k [=][ 1]2 Spatial: k [= 0] Spatial: k [=][ 1]4 0.6

0.4 CNN(p) 4

0.2 CNN(pMLPMLP 41[2]) 2

Input Node 0.010[1] 10[2] 10[3] 10[4] 10[5]

L(Y7) = 0 + 4 = 4Output Node L(Y7) = [2]2 [+][ 4]2 [=][ 12]4 [, deg(Y][7][)=4] Utlra-Long-Range-High-Frequency: L(Y7) = [6]4 [+][ 4]4 [=][ 10]4 0.8 MSE Residual vs Training Set Size: Y7

Frequency: k [= 0] Spatial: k [=][ 1]2 Spatial: k [= 0] Spatial: k [=][ 1]4 0.6

0.4 CNN(p) 4

0.2 CNN(pMLPMLP 41[2]) 2

Input Node 0.010[1] 10[2] 10[3] 10[4] 10[5]


(a) MLP


(b) CNN(p[2])[⊗][2]


(c) CNN(p)[⊗][4]


(d) MSE Residual


Figure 6: Eigenfunction vs Learning Index vs Architecture/DAG. Rows: eigenfunctions Yi with
various space-frequency combinations. Columns: DAGs associated to (1) a four-layer MLP; (b)
CNN(p[2])[⊗][2], a“D"-CNN; (c) CNN(p)[⊗][4] a “HR"-CNN. Column (d) is the MSE, as a function of
training set size (X-axis), of the residual of the corresponding eigenfunction Yi obtained by NTKregression. The colored path in each DAG corresponds to the minimum spanning tree that contains
all interacting terms in the k = 0 component of Yi.


-----

MLP: 1 vs 4 Layers (NTK)


CNN: 1 vs 4 Layers (NTK)


0.8

0.6

0.4

0.2

0.0


0.8

0.6

0.4

0.2

0.0


0.5 1 1.5

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
||||||
||1 - Laye 4 - Laye|r r|||
||||||


log(TrainingSetSize)/ log(d)

MLP: 1 vs 4 Layers (SGD)


0.5 1 1.5

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
||1 - Laye 4 - Laye|r r|||
||||||


1 - Layer

4 - Layer

log(TrainingSetSize)/ log(d)

CNN: 1 vs 4 Layers (SGD)



|0.8|Col2|MLP|P: 1 vs 4 Lay|yers (SGD|D)|0.8|Col8|CNN|Col10|N: 1 vs 4 Lay|yers (SGD|D)|
|---|---|---|---|---|---|---|---|---|---|---|---|---|
|0.8 0.6 MSE 0.4 0.2||||||0.8 0.6 0.4 0.2||||||1 - Layer 4 - Layer|
|||||||||||||MSE/8|
||||||1 - Layer||||||||
||||||4 - Layer MSE/8||||||||


0.5 1 1.5

log(SGDSteps)/ log(d)


0.5 1 1.5

log(SGDSteps)/ log(d)


Figure 7: MLPs do not benefits from having more layers. We plot the learning dynamics vs
training set size / SGD steps for each eigenfunction Yi. Top: NTK regression and bottom: SGD
+ Momentum. Left: MLP; right: CNN. Dashed lines / Solid lines correspond to one-hidden/fourhidden layer networks. For both finite-width SGD training and infinite-width kernel regression,
having more layers does not essentially improve performance of a MLP. This is in stark contrast to
CNNs (right). By having more layers, the eigenstuctures of the kernels are refined.

E FIGURE ZOO


E.1 MLPS: DEPTH ̸= HIERARCHY

We compare a one hidden layer MLP and a four hidden layer MLP in Fig. 7. Unlike CNNs, increasing the number of layers does not improve the performance of MLPs much for both NTK regression
and SGD. This is consistent with a theoretical result from Bietti & Bach (2020), which says the
NTKs of Relu MLPs are essentially the same for any depth.


E.2 IMAGENET PLOTS


-----

|ResNet34-G ResNet34-F|AP latten|Col3|
|---|---|---|
||||
||||
||||

|Col1|ResNet34-GAP ResNet34-Flatt|en|
|---|---|---|
||||
||||
||||

|ResNet101- ResNet101-|GAP Flatten|Col3|
|---|---|---|
||||
||||
||||

|Col1|ResNet101-GAP ResNet101-Flatt|en|
|---|---|---|
||||
||||
||||


Accuracy: GAP vs Flatten Loss: GAP vs Flatten

ResNet34-GAP ResNet34-GAP
ResNet34-Flatten 6 ResNet34-Flatten

0.6

4

0.4

Top 1 Acc ImageNet 0.2 Validation Cross-Entropy 2

10[5] 10[6] 10[5] 10[6]

Training Set Size Training Set Size

Accuracy: GAP vs Flatten Loss: GAP vs Flatten

0.8

ResNet101-GAP ResNet101-GAP
ResNet101-Flatten 6 ResNet101-Flatten

0.6

4

0.4

Top 1 Acc ImageNet 2

0.2 Validation Cross-Entropy

10[5] 10[6] 10[5] 10[6]

Training Set Size Training Set Size


Figure 8: ResNet-GAP vs ResNet-Flatten. As the training set size increases the performance
(accuracy and loss) gap between the two shrinks substantially. Top/bottom ResNet34/ResNet101

F ARCHITECTURE SPECIFICATIONS

In this section, we provide the details of the architectures used in the experiments.

G ASSUMPTIONS

**Assumption-G. Let G = (G[(][d][)])d. There are absolute constants c, C > 0 such that**

(a.) For each non-input node u [(][d][)], there is αu Λ such that
_∈N_ _∈_ **_G_**

_cd[α][u]_ _≤_ deg(u) ≤ _Cd[α][u]_ _._ (40)

For each edge uv, its weight is defined to be ωuv _αu._
_∈E_ [(][d][)] _≡_

(b.) For each input node v, there are dv N and 0 < αv Λ such that
**_G_**
_∈_ _∈_

_cd[α][v]_ _≤_ _dv ≤_ _Cd[α][v]_ and _αv = 1._ (41)

_v∈NX0[(][d][)]_

(c.) Let 1[(][d][)] _u :_ _v_ 0[(][d][)] s.t. uv be the collection of nodes in the first hidden
_N_ _≡{_ _∃_ _∈N_ _∈E_ [(][d][)]}
layer. We assume that for every u 1[(][d][)], αu = 0 and all children of u are input nodes.
_∈N_

(d.) For every v ∈N [(][d][)], |{u : uv ∈E [(][d][)]}| ≤ _C. Moreover, the number of layers is uniformly_
bounded, namely, for any node u, any path from u to o contains at most C edges.
_G_


The first two assumptions help to create spectral gaps between eigenspaces. When d is not large,
“finite-width" effect is no longer negligible and we expect that the spectra decay more smoothly.
Assumption (c.) says there is no “skip" connections from the input layer to other layers except to
the first hidden layer.


-----

**from neural_tangents import stax**

**def MLP(width=2048, depth=1, W_std=0.5, activation=stax.Rbf()):**

layers = []

**for _ in range(depth):**

layers += [stax.Dense(width, W_std=W_std), activation]

layers += [stax.Dense(1)]
**return stax.serial(*layers)**

**def CNN(width=512, ksize=4, depth=1, W_std=0.5, activation=stax.Rbf(), readout=stax.Flatten(),**

act_after_readout=True):

layers = []
conv_op = stax.Conv(width, (ksize, 1), strides=(ksize, 1), W_std=W_std, padding='VALID')

**for _ in range(depth):**

layers += [conv_op, activation]

layers += [readout]

**if act_after_readout:**

layers += [stax.Dense(width * 4, W_std=W_std), activation]

layers += [stax.Dense(1)]
**return stax.serial(*layers)**

p = 4 # input shape: (-1, p**4, 1, 1)

S_MLP = MLP(depth=1) # Shallow MLP
D_MLP = MLP(depth=4) # Deep MLP

S_CNN = CNN(ksize=p, depth=1, act_after_readout=False) # Shallow CNN
# One layer CNN with an additional activation-dense layer
S_CNN_plus_Act = CNN(ksize=p, depth=1, act_after_readout=True)

D_CNN = CNN(ksize=p**2, depth=2) # Deep CNN
HR_CNN = CNN(ksize=p, depth=4) # High-resolution CNN
# High-resolution CNN with global average pooling readout
HR_CNN_GAP = CNN(ksize=p, depth=4, readout=stax.GlobalAvgPool(), act_after_readout=False)
# High-resolution CNN with flattening as readout
HR_CNN_Flatten = CNN(ksize=p, depth=4, act_after_readout=False)


Listing 1: Definitions of MLPs, S-CNN, D-CNN and HR-CNN used in the experiments. The architectures used in the experiments of Fig. 3 are defined as follows. Dense[⊗][4] = D_MLP, Conv(p[2])[⊗][2]
= D_CNN, Conv(p)[⊗][4] = HR_CNN. The one layer MLP and one layer CNN used in Fig. 7 are
S_MLP and S_CNN_plus_Act, resp.

H PROOF OF THE EIGENSPACE RESTRUCTURING THEOREM.

**Lemma 1. Let r** 0[(][d][)]. Then
_∈N_

풦 (0), Θ (0) = 0 (42)
_G_ _G_

_and for r ̸= 0, for d large enough,_

풦[(][r][)][(0)][,] Θ[(][r][)][(0)][ ∼] _[d][−][풮][(][r][)]_ _and_ 풦[(][r][)] (43)
_G_ _G_ _∥_ _G_ _[∥][∞][,][ ∥][Θ]G[(][r][)][∥][∞]_ [≲] _[d][−][풮][(][r][)]_


-----

_Proof. Recall that the recursion formulas for 풦_ and Θ are


풦u(t) = φ[∗]u

Θu(t) = φ[˙] _[∗]u_


풦v(t) (44)
_v:uv_
_∈E_ 


풦v(t)
_v:uv∈E_


(풦v(t) + Θv(t)) (45)
_v:uv∈E_


For convenience, define 풦, Θ, 풦[˙] as follows

풦u(t) =

Θu(t) =


풦v(t) (46)
_v:uv∈E_


Θu(t) = Θv(t) (47)

_v:uv∈E_

풦˙ _u(t) = φ[˙]_ _[∗]u_ (48)

_[◦]_ [풦][u][(][t][)][ .]

Note that
풦u(t) = φ[∗]u and Θ(t) = 풦[˙] _u(t)(풦u(t) + Θu(t)) ._

_[◦]_ [풦][u][(][t][)]

The equalities 풦u(0) = Θu(0) = 0 follow easily from recursion and the fact φ[∗]u[(0) = 0][ for all]
_u ∈N_ [(][d][)].

We induct on the tuple (h, |r|), where h ≥ 0 is the number of hidden layers in G[(][d][)] and |r| is the
total degree of r. We begin with the proof of the NNGP kernel 풦.

**Base Case I: |r| = 1 and h ≥** 0 is any integer. Let u ∈N0 be such that r = eu, where {eu}u∈N (0d)
is the standard basis. Then


deg(v)[−][1][ ˙]φ[∗]v
_v∈Ypath_ _[◦]_ [풦][v][(][t][)][ ∼]


_d[−][α][v][ ˙]φ[∗]v_
_v∈Ypath_ _[◦]_ [풦][v][(][t][)]

(49)


_∂tu_ 풦G(t) =


path (u _o_ )
_∈P_ _→_ _G_


path (u _o_ )
_∈P_ _→_ _G_


Here P(u → _u[′]) represents the set of paths from u to u[′]. By Assumption-G and Assumption-φ,_
(u _o_ ) is uniformly bounded and _φ[˙]_ _[∗]v[(0)][ >][ 0][ for all hidden nodes][ v][. Therefore]_
_|P_ _→_ _G_ _|_

_∂tu_ 풦G(0) ∼ path (u _o_ ) _d[−]_ [P]v∈path _[α][v]_ _φ˙_ _[∗]v_ _[◦]_ [풦][v][(][0][)]

_∈PX→_ _G_

= _d[−]_ [P]v∈path _[α][v]_ _φ˙_ _[∗]v[(0)]_

path (u _o_ )
_∈PX→_ _G_

max _v∈path_ _[α][v]_
_∼_ path (u _o_ ) _[d][−]_ [P]
_∈P_ _→_ _G_

= d[−][풮][(][e][u][)] = d[−][풮][(][r][)]

In the above, we have used 풦v(0) = 0 (which is due to φ[∗](0) = 0.) The second estimate
_∂tu_ 풦 ≲ _d[−][풮][(][r][)]_ follows from _φ[˙]_ _[∗]v_
_∥_ _G∥∞_ _|_ _[◦]_ [풦][v][(][t][)][|][ ≲] [1][.]

**Base Case II: h = 0 and |r| ≥** 1 is any number. Note that G[(][d][)] has no hidden layer and all input
nodes are linked to the output node oG. The case when the activation φoG is the identity function is
obvious and we assume φoG is admissible.


_∂t[r][풦][G][(][t][) = deg(][o][G][)][−|][r][|][φ]o[∗]G_ (|r|)


**_tu_**
_u_ 0[(][d][)]
_∈N_


This implies Eq. (43) since 풮(r) = 0, deg(o ) ≲ 1 by Assumption- and φ[∗]o (|r|)(0) > 0 by
_G_ **_G_** _G_
**Assumption-φ.**

**Induction: |r| ≥** 2, h ≥ 1 and r ∈ 풜(G[(][d][)]). We only prove the first estimate in Eq. (43) since the
other one can be proved similarly. WLOG, we assume φoG is not the identity function and hence is


-----

semi-admissible. Let u 0[(][d][)] be such that ru 1 and denote ¯r = r **_eu. Then_**
_∈N_ _≥_ _−_

_∂t[r][풦][G][(][t][)]_ **_t=0_** [=][ ∂]tr[¯][(][∂]t[e][u] [풦][G][(][t][))] **_t=0_** [=] deg(oG)[−][1]∂tr[¯] 풦˙ _oG_ (t)∂t[e][u] [풦][v][(][t][)] **_t=0_** (50)

_∂t[e]o[u]GX풦v∈Ev=0_  
_̸_

= deg(oG)[−][1][ ]∂tr[¯]1 풦˙ _oG_ (t) ∂tr[¯]2+eu 풦v(t) **_t=0_** (51)

_∂t[e]o[u]GX풦v∈Ev=0_ **_r¯1+¯Xr2=¯r_** 
_̸_

deg(o )[−][1]d[−][풮][(¯]r1)d−풮(퓃(¯r2+eu;v)) (52)

_∼_ _G_

_∂t[e]o[u]GX풦v∈Ev=0_ **_r¯1+¯Xr2=¯r_**
_̸_

_∼_ _d[−][풮][(¯]r1)d−(αoG +풮(퓃(¯r2+eu;v)))_ (53)

_∂t[e]o[u]GX풦v∈Ev=0_ **_r¯1+¯Xr2=¯r_**
_̸_

sup sup **_r1)+αoG +풮(퓃(¯r2+eu;v)))_** (54)
_∼_ _o_ _v_ **_r¯1+¯r2=¯r_** _[d][−][(][풮][(¯]_
_G_ _∈E_
_∂t[e][u]_ 풦v=0
_̸_

We have applied induction twice in Eq. (52): one to obtain the estimate ∂tr[¯]1 풦˙ _[∗]oG_ [(][0][)][ ∼] _[d][−][풮][(¯]r1) (with_
_|r¯1| < |r| and_ _φ[˙]_ _[∗]oG_ [semi-admissible) and one to][ ∂]tr[¯]2+eu 풦v(t) ∼ _d[−][풮][(][퓃][(¯]r2+eu;v)), in which the sub-_
graph with v as the output node has depth at most (h − 1). The last line follows from that both the
cardinality of the tuple (¯r1, ¯r2) with ¯r1, ¯r2 **0 and ¯r1 + ¯r2 = ¯r and the cardinality of v** 0[(][d][)]
with oGv ∈E and ∂t[e][u] [풦][v][ ̸][= 0][ are finite and independent of] ≥ _[ d][. From the definition of MST, it is] ∈N_
clear that for all (¯r1, ¯r2)

풮(¯r1) + αoG + 풮(퓃(¯r2 + eu; v)) ≥ 풮(¯r1) + 풮(¯r2 + eu) ≥ 풮(r) (55)

It remains to show that there exists at least one pair (¯r1, ¯r2) such that the above can be an equality.
Let be a MST containing all nodes in 퓃(r). If o has only one child v in, then we
_T ⊆G[(][d][)]_ _G_ _T_
choose ¯r1 = 0 and notice that

풮(¯r1) + αoG + 풮(퓃(¯r2 + eu; v)) = 0 + 풮(r2 + eu) = 풮(r) (56)

since 풮(¯r1) = 0 and ¯r2 + eu = r. Else, T contains at least two children and therefore at least two
disjoint branches split fromsuch that all the nodes ofcontained in _u. Clearly (¯r2 o +G e. Letu) are contained in Tu ⊆T be the branch that contains Tu and all the nodes of u ¯r and choose1 ≡_ **_r −_** (¯r ¯2r +2 ≤ eu)r¯ are be
_T \T_

풮(r) = 풮(¯r1) + 풮(¯r2 + eu) = 풮(¯r1) + 풮(퓃(¯r2 + eu; v)) + αo _,_ (57)
_G_

where v is the unique child of o in _u._
_G_ _T_

This completes the proof of the NNGP kernel 풦. As the proof of the NTK part is quite similar, we
will be brief and focus only on the induction step.

**Induction Step of Θ: |r| ≥** 2, h ≥ 1 and r ∈ 풜(G[(][d][)]). Recall that the formula of Θ is


Θu(t) = φ[˙] _[∗]u_


(풦v(t) + Θv(t)) (58)
_v:uv∈E_


풦v(t)
_v:uv∈E_


For r 0[(][d][)],
_∈N_

_∂t[r][Θ][o]G_ [(][t][) =]


_∂tr[¯]1_ _φ˙_ _[∗]oG_
**_r¯1+¯r2=r_**

X


풦v(t) _∂tr[¯]2_
_v:oG_ _v∈E_ 


(풦v(t) + Θv(t)) (59)
_v:oG_ _v∈E_


Note that _φ[˙]_ _[∗]oG_ [is semi-admissible. We apply the result of][ 풦] [to conclude that]


_∂tr[¯]1_ _φ˙_ _[∗]oG_

**_tr1_** _φ˙_ _[∗]oG_

_[∂]_ [¯]


풦v(t) _d[−][풮][(][r][1][)]_ (60)
_v:o_ _v_ _∼_
_G_ _∈E_  **_t=0_**

풦v(t) ≲ _d[−][풮][(¯]r1)_ (61)
_v:oG_ _v∈E_ ∞


-----

and the inductive step to conclude that


_∂tr[¯]2_

_∂tr[¯]2_


_d[−][풮][(¯]r2)_ if **_r¯2_** = 0 else 0 (62)
_̸_


(풦v(t) + Θv(t))
_v:oG_ _v∈E_


_∂tr[¯]2_ [(]v[풦]:o[v]G[+Θ]v∈E[v][)][̸≡][0]


**_t=0_**


_d[−][풮][(¯]r2) ._ (63)


(풦v(t) + Θv(t)) ≲
_v:oG_ _v∈E_


_∂tr[¯]2_ [(]v[풦]:o[v]G[+Θ]v∈E[v][)][̸≡][0]


Note that
_v : o_ _v_ and ∂tr[¯]2 [(][풦][v][ + Θ][v][)][ ̸≡] [0][}|][ ≲] [1][ .]
_|{_ _G_ _∈E_ [(][d][)]
Thus


_d[−][풮][(¯]r1)−풮(¯r2) ≲_ _d−풮(r)._ (64)
**_r¯1+¯r2=r_**

X


_∥∂t[r][Θ][o]G_ [(][t][)][∥]∞ [≲]


To control the lower bound, let be a MST containing 퓃(r). If deg(o ; ) = 1, then we can choose
**_r¯1 = 0 and ¯r2 = r_** = 0. Notice that there is at least one child node T _v ofG_ _o T_ with ∂tr[¯]2 [(][풦][v][ +Θ][v][)][ ̸≡] [0][.]
_̸_ _G_
Therefore

_d[−][풮][(¯]r2) ≳_ _d−풮(¯r2) = d−풮(r)_ (65)
_v:∂tr[¯]2_ [(][풦]X[v][+Θ][v][)][̸≡][0]

Combining with


_φ˙_ _[∗]oG_


_v:oG_ _v∈E_ 풦v(0) = φ[˙] _[∗]oG_ [(0)][ >][ 0] (66)

_∂t[r][Θ][o]G_ [(][0][)][ ≳] _[d][−][풮][(][r][)]_


we have


It remains to handle the deg(oG; T ) > 1 case. We choose (¯r1, ¯r2) such that one branch of T is the
MST that contains 퓃(¯r2) and the o and the remaining branch(es) is a MST that contains 퓃(¯r1) and
_G_
the o . Then
_G_

_∂t[r][Θ][o]G_ [(][0][)][ ≳][∂]tr[¯]1 _φ˙_ _[∗]oG_ _v:o_ _v_ 풦v(t) _∂tr[¯]2_ _v:o_ _v_ (풦v(t) + Θv(t))

 _G_ _∈E_  _G_ _∈E_ **_t=0_**

≳d[−][풮][(][r][1][)][−][풮][(][r][2][)] = d[−][풮][(][r][)]


H.1 LEGENDRE POLYNOMIALS, SPHERICAL HARMONICS AND THEIR TENSOR PRODUCTS.

Our notation follows closely from (Frye & Efthimiou, 2012).

**Legendre Polynomials.** Let din ∈ N[∗] and ωdin be the measure defined on the interval I = [−1, 1]

_ωdin_ (t) = (1 _t[2])[(][d][in][−][3)][/][2]_ (67)
_−_

The Legendre polynomials[6] _{Pr(t) : r ∈_ N} is an orthogonal basis for the Hilbert space L[2](I, ωdin ),
i.e.

Sdin 1
_Pr(t)Pr′_ (t)ωdin (t)dt = 0 if _r_ = r[′] else _N_ (din, r)[−][1] _|_ _−_ _|_ (68)

ˆI _̸_ Sdin 2

 _|_ _−_ _|_ 

Here Pr(t) is a degree r polynomials with Pr(1) = 1 that satisfies the formula below, N (din, r) is
the cardinality of degree r spherical harmonics in R[d][in] and Sdin 1 is the measure of Sdin 1.
_|_ _−_ _|_ _−_

[6More accurate, this should be called Gegenbauer Polynomials. However, we decide to stick to the termi-](https://en.wikipedia.org/wiki/Gegenbauer_polynomials)
nology in (Frye & Efthimiou, 2012)


-----

**Lemma 2 (Rodrigues Formula. Proposition 4.19 (Frye & Efthimiou, 2012)).**

_r_

_d_
_Pr(t) = crωd[−]in[1][(][t][)]_ _dt_ (1 − _t[2])[r][+(][d][in][−][3)][/][2]_ _,_ (69)
 

_where_

( 1)[r]
_cr =_ _−_ (70)

2[r](r + (din − 3)/2)r


In the above lemma, (x)l denotes the falling factorial

(x)l _x(x_ 1) (x _l + 1)_ (71)
_≡_ _−_ _· · ·_ _−_
(x)0 1 (72)
_≡_

**Spherical Harmonics.** Let dSdin 1 define the (un-normalized) uniform measure on the unit sphere
_−_
Sdin 1. Then
_−_


_dSdin_ 1 = [2][π][d][in][/][2] (73)
Sdin−1 _−_ Γ( _[d]2[in]_ [)][ .]


Sdin 1
_|_ _−_ _| ≡_


The normalized measure on this sphere is defined to be

1
_dσdin =_ and _dσdin = 1 ._ (74)

_|Sdin−1|_ _[d][S][d][in][−][1]_ ˆSdin _−1_

The spherical harmonics {Yr,l}r,l in R[d][in] are homogeneous harmonic polynomials that form an
orthonormal basis in L[2](Sdin 1, σdin )
_−_


_Yr,l(ξ)Yr′,l′_ (ξ)dσdin = δ(r,l)=(r′,l′) . (75)

ˆξ∈Sdin _−1_

Here Yr,l denotes the l-th spherical harmonic whose degree is r, where r ∈ N, l ∈ [N (din, r)] and

_din + r_ 3

_N_ (din, r) = [2][r][ +][ d][in][ −] [2] _−_ (din)[r]/r! as _din_ _._ (76)

_r_ _r_ 1 _∼_ _→∞_

 _−_ 

The Legendre polynomials and spherical harmonics are related through the addition theorem.
**Lemma 3 (Addition Theorem. Theorem 4.11 (Frye & Efthimiou, 2012)).**


_Pr(ξ[T]_ _η) =_


_Yr,l(ξ)Yr,l(η),_ _ξ, η_ Sdin 1 . (77)
_∈_ _−_
_l∈[NX(din,r)]_


_N_ (din, r)


**Tensor Products.** Let p = |N0[(][d][)]|, d = (du)|u∈N (0d)|[,][ r][ ∈] [N][|N][ (]0[d][)]| ≃ Np, I _|N0[(][d][)]| ≃_ _I_ _p = [−1, 1]p_

and ω = _u_ 0[(][d][)] _ωd[p]u_ [be the product measure on][ I] _[p][. Then the (product of) Legendre polynomials]_

_∈N_

[N] **_Pr(t) =_** _Pru_ (tu), **_t = (tu)u∈N (0d)_** _∈_ _I_ _[p]_ _,_ (78)

_u∈NY0[(][d][)]_

which form an orthogonal basis for the Hilbert space L[2](I _[p], ω) =_ _u_ 0[(][d][)] _L[2](I, ωdu_ ). Similarly,

_∈N_
the tensor product of spherical harmonics

[N]

**_Yr,l =_** _Yru,lu_ _,_ **_l = (lu)u∈N (0d)_** _∈_ [N (d, r)] ≡ [N (du, r)] (79)

_u∈NY0[(][d][)]_ _u∈NY0[(][d][)]_

form an orthonormal basis for the product space


_L[2](X_ _, σ) ≡_


_L[2](Sdu_ 1, σdu ) (80)
_−_


_u_ 0[(][d][)]
_∈N_


Elements in the set **_Yr,l_** **_l_** [N (d,r)] are called degree (order) r spherical harmonics in L[2]( _, σ)_
_{_ _}_ _∈_ **_X_**
and also degree r spherical harmonics if |r| = r ∈ N.


-----

**Theorem 5. We have the following, for** = 풦 (d) or = Θ (d)
_K_ _G_ _K_ _G_


ˆ(r)Pr(t) _with_ ˆ(r) _d[−][풮][(][r][)]_ _if_ **_r_** = 0 _else_ 0. (81)
_K_ _K_ _∼_ _̸_


_K(t) =_


**_r_** N[N][ (]0[d][)]
_∈_


Note that Theorem 1 follows from this theorem and the addition theorem.

_Proof of Theorem 1. Assume r ̸= 0. Indeed, setting_

**_ξ = (ξu)u∈N (0d)_** _∈_ **_X_** _,_ **_η = (ηu)u∈N (0d)_** _∈_ **_X and_** **_t = (tu)u∈N (0d)_** = (ξu[T] **_[η][u][/][d][u][)]u∈N0[(][d][)]_**

we have


_N_ (du, ru)[−][1]


**_Pr(t) =_**


_Pru_ (tu) =


_Yru,lu_ (ξu/
**_lu∈NX(du,ru)_**


**_du)Yru,lu_** (ηu/


**_du)_**

(82)


_u_ 0[(][d][)]
_∈N_

= N (d, r)[−][1]


_u_ 0[(][d][)]
_∈N_


**_Y r,l(ξ)Y r,l(η) ._** (83)


**_l∈[N_** (d,r)]

Then Theorem 1 follows by noticing

_K(r)N_ (d, r)[−][1] _∼_ _d[−][풮][(][r][)]d−_ [P]u∈N0[(][d][)] **_ruαu = d−ℒ(r)_** (84)

_Proof of Theorem 5. From the orthogonality,_

ˆ(r) = _, Pr_ _/_ **_Pr_** _L2(I_ _p,ω)[2]_ (85)
_K_ _⟨K_ _⟩_ _∥_ _∥_

We begin with the denominator. Note that


**_Pr_** _L2(I_ _p,σ)[2]_ =
_∥_ _∥_


_Pru_ _L[2](I,ωdu_ ) [=][ N] [(][d][;][ r][)][−][1]
_∥_ _∥[2]_


( Sdu 1 _/_ Sdu 2 ) (86)
_|_ _−_ _|_ _|_ _−_ _|_


_u_ 0[(][d][)]
_∈N_


_u_ 0[(][d][)]
_∈N_


By applying Lemma 2, integration by parts and continuity of K[(][r][)] on the boundary ∂I _[p]_

**_r_**

_d_

_, Pr_ _L2(I_ _p,ω) = cr_ 1 **_t[2][][r][+(][d][−][3)][/][2]_** _dt_ (87)
_⟨K_ _⟩_ ˆI _[p][ K][(][t][)]_ _dt_ _−_

   

= ( 1)[r]cr 1 **_t[2][][r][+(][d][−][3)][/][2]_** _dt_ (88)
_−_ ˆI _[p][ K][(][r][)][(][t][)]_ _−_

= (−1)[r]cr (M(K, d) +  ϵ(K, d)) (89)

where K[(][r][)] is the r derivative of K, the coefficient cr is given by Lemma 2

( 1)[r][u]

_cr =_ _cru =_ _−_ ( 1)[r][u] **_d[−]u_** **_[r][u]_** = ( 1)[r]d[−][r] (90)

2[r][u](ru + (d 3)/2)ru _∼_ _−_ _−_

_u∈NY0[(][d][)]_ _u∈NY0[(][d][)]_ _−_ _u∈NY0[(][d][)]_

and the major and error terms are given by


S2ru+du 1

_M(K, d) ≡K[(][r][)](0)_ ˆI _[p]_ 1 − **_t[2][][r][+(][d][−][3)][/][2]_** _dt = K[(][r][)](0)_ _|S2ru+du−2|_ (91)

  _u∈NY0[(][d][)]_ _|_ _−_ _|_

_ϵ(_ _, d)_ 1 **_t[2][][r][+(][d][−][3)][/][2]_** _dt_ (92)
_K_ _≡_ ˆI _[p]_ [(][K][(][r][)][(][t][)][ −K][(][r][)][(][0][))] _−_

 

The mean value theorem gives


_|K[(][r][)](t) −K[(][r][)](0)| ≤_


_L∞(I_ _p)_ **_tu_** (93)
_∥K[(][r][+][e][u][)]∥_ _|_ _|_


_u_ 0[(][d][)]
_∈N_


-----

and the error term |ϵ(K, d)| is bounded above by


_I_ 1 **_t[2]u_** **_ru+(du−3)/2 dtu_**

´ _[|][t][u][|]_ _−_ (94)

´I [(1] [ −] **_[t]u[2]_** [)][r][u][+(][r][u][−][3)][/][2][ d][t][u] !

S2ru+du 1 _−1_
_|_ _−_ _|_ _._ (95)

S2ru+du 2

 _|_ _−_ _|_ 


1 − **_t[2][][r][+(][d][−][3)][/][2]_** _dt_


_L∞(I_ _p)_
_∥K[(][r][+][e][u][)]∥_


_I_ _[p]_


_u_ 0[(][d][)]
_∈N_


S2ru+du 1
_|_ _−_ _|_
S2ru+du 2
_|_ _−_ _|_


_L∞(I_ _p)d[−]u_ [1]
_∥K[(][r][+][e][u][)]∥_


_u_ 0[(][d][)]
_∈N_


_u_ 0[(][d][)]
_∈N_


Since for any α ∈ N, as du →∞,

_|Sα+du−1|_ 21 Γ((α + du 1)/2)/Γ((α + du)/2) _π_ 12 (du/2)[−] 2[1] (du)[−] 2[1], (96)
Sα+du 2 [=][ π] _−_ _∼_ _∼_
_|_ _−_ _|_

we have


S2ru+du 1
_|_ _−_ _|_ (97)
S2ru+du 2 _[.]_
_|_ _−_ _|_


_L∞(I_ _p)d−u_ 2[1]
_∥K[(][r][+][e][u][)]∥_

_u∈NY0[(][d][)]_


_|ϵ(K, d)| ≲_


_u_ 0[(][d][)]
_∈N_


We claim that (which will be proved later)


_∥K[(][r][+][e][u][)]∥L∞(I_ _p) ≲_ _d[−][풮][(][r][)]_ (98)


_u_ 0[(][d][)]
_∈N_


which implies

_⟨K, Pr⟩L2(I_ _p,ωd[p]in_ [)][ =][ c][r]


_d[−][풮][(][r][)]( min_ **_du)[−]_** [1]2
_u_ 0[(][d][)] !!
_∈N_


S2ru+du 1
_|_ _−_ _|_ (99)
S2ru+du 2
_|_ _−_ _|_


_K[(][r][)](0) + O_


_u_ 0[(][d][)]
_∈N_


Plugging back to Eq. (85), we have


!! []

u∈NY0[(][d][)]



Sdu 1
_|_ _−_ _|_

Sdu 2

 _|_ _−_ _|_

(100)


_−1_



S2ru+du 1
_|_ _−_ _|_
S2ru+du 2
_|_ _−_ _|_


_d[−][풮][(][r][)]( min_ **_du)[−]_** [1]2
_u_ 0[(][d][)]
_∈N_


ˆ(r) = ( 1)[r]crN (d, r)
_K_ _−_


_K[(][r][)](0) + O_


Since, for r fixed and as du for all u 0[(][d][)]
_→∞_ _∈N_

_cr_ **_N_** (d, r)

and 1 and
( 1)[r]d[−][r][ →] [1] **_d[r]/r!_** _→_
_−_


Sdu 1
_|_ _−_ _|_

Sdu 2

 _|_ _−_ _|_

!!


_−1_



S2ru+du 1
_|_ _−_ _|_
S2ru+du 2
_|_ _−_ _|_


_→_ 1

 (101)

(102)


_u_ 0[(][d][)]
_∈N_


and thus


_d[−][풮][(][r][)]( min_ **_du)[−]_** 2[1]
_u_ 0[(][d][)]
_∈N_


_Kˆ(r) ∼_ **_r![−][1]_**


_K[(][r][)](0) + O_


It remains to verify Eq. (98). By Lemma 1, we only need to show that

_d[−][풮][(][r][+][e][u][)]_ ≲ _d[−][풮][(][r][)]_ (103)

_u∈NX0[(][d][)]_

We prove this by induction on the number of hidden layers of G[(][d][)]. The base case is obvious. Now
suppose the depth of is h. Let (r) be the set of children of o that are ancestors of at least one
_G[(][d][)]_ _C_ _G_
node of 퓃(r). We split 0[(][d][)] into two disjoint sets
_N_

(r) _u_ 0[(][d][)] : _v_ (r) s.t. (u _v)_ = and 0[(][d][)] 0(r) .
_Q_ _≡{_ _∈N_ _∃_ _∈C_ _P_ _→_ _̸_ _∅}_ _N_ _\Q_


-----

For u / (r), we have 풮(r + eu) = 풮(r) + 풮(eu) and hence
_∈Q_

_d[−][풮][(][r][+][e][u][)]_ = _d[−][풮][(][r][)][−][풮][(][e][u][)]_ = d[−][풮][(][r][)] _d[−][풮][(][e][u][)]_ ≲ _d[−][풮][(][r][)]_ _._ (104)
_u/∈QX(r)_ _u/∈QX(r)_ _u/∈QX(r)_

In the last inequality above, we have used

_d[−][풮][(][e][u][)]_ _≤_ _d[−][풮][(][e][u][)]_ _∼_ 1 . (105)
_u/∈QX(r)_ _u∈NX0[(][d][)]_

To estimate the remaining, we use induction. Note that |C(r)| is finite and independent of d. Then

_d[−][풮][(][r][+][e][u][)]_ _≤_ _d[−][α][o][G][ −][풮][(][퓃][(][r][+][e][u][;][v][))]_ (106)
_u∈QX(r)_ _v∈CX(r)_ _u∈QX(r)_

= d[−][α][o][G] _d[−][풮][(][퓃][(][r][+][e][u][;][v][))]_ (107)

_v∈CX(r)_ _u∈QX(r)_

≲ (r) _d[−][α][o][G]_ max (108)
_|C_ _|_ _v_ (r) _[d][−][풮][(][퓃][(][r][;][v][))][ ∼]_ _[d][−][풮][(][r][)]_
_∈C_

We have used induction on the sub-graph with v as the output node.

I PROOF OF THEOREM 2

Let G[(][d][)] be a DAG associated to the convolutional networks whose filter sizes in the l-th layer is
_kl = [d[α][l]_ ], for 0 ≤ _l ≤_ _L_ +1, in which we treat the flatten-dense readout layer as a convolution with
filter size [d[α][L][+1] ]. Note that we have set αp = α0 and αw = αL+1. We also assume an activation
layer after the flatten-dense layer, which does not essentially alter the topology of the DAG.

We need the following dimension counting lemma.


**Lemma 4. Let r ∈L(G[(][d][)]). Then**

dim span


**_Y r,l :_** _L(r) = r, l ∈_ **_N_** (d, r) _∼_ _d[r]_ (109)



To prove Theorem 2, we only need to verify the assumptions of Theorem 4 in Mei et al. (2021a).
For convenience, we briefly recap the assumptions and results from Mei et al. (2021a) in Sec.L.

It is convenient to group the eigenspaces together according to the learning indices L(G[(][d][)]). Recall
that L(G[(][d][)]) = (r1 ≤ _r2 ≤_ _r3 . . . ). Let_

_Ei = span{Y r,l : L(r) = ri}_ (110)

Then by Theorem 1 and Lemma 4,

dim(Ei) _d[r][i]_ and _λ(g)_ _d[−][r][i]_ _g_ _Ei, g_ = 0, (111)
_∼_ _∼_ _∀_ _∈_ _̸_

where λ(g) denote the eigenvalue of g. We proceed to verify Assumptions 4 and 5 in Sec. L.
They follow directly from Theorem 1, Lemma 4 and the hypercontractivity of spherical harmonics
Beckner (1992).

I.1 VERIFYING Assumption 4

We need the following.


**Proposition 1. For 0 < s ∈** R, let Ds = span{Y r,l : |L(r)| < s}. Then for f ∈ _Ds,_

**_f_** _q_ 2 (112)
_∥_ _∥[2]_ _[≤]_ [(][q][ −] [1)][s/α][0] _[∥][f]_ _[∥][2]_

_Proof of Proposition 1. The lemma follows from the tensorization of hypercontractivity. Let f =_

_k_ 0 _[Y][k][ ∈]_ _[L][2][(][S][n][)][ where][ Y][k][ is a degree][ k][ spherical harmonics in][ S][n][. Define the Poisson semi-]_
_≥_
group operator

P


_ϵ[k]Yk(x)_ (113)
_kX≥0_


_Pϵf_ (x) =


-----

_pq−−11_


Then we have the hypercontractivity inequality (Beckner, 1992), for 1 ≤ _p ≤_ _q and ϵ ≤_


_∥Pϵf_ _∥Lq(Sn) ≤∥f_ _∥Lp(Sn)_ (114)

One can then tensorize (Beckner, 1975) it to obtain the same bound in the tensor space.

**Lemma 5 (Corollary 11 Montanaro (2012)). Let f : (Sn)[k]** _→_ R. If 1 ≤ _p ≤_ _q and ϵ ≤_ _pq−−11_ _[,]_

_then_ q

_∥Pϵ[⊗][k]f_ _∥Lq((Sn)k) ≤∥f_ _∥Lp((Sn)k) ._ (115)


Let f = **_r,l_** _[a][r][,][l][Y][ r][,][l][ ∈]_ _[D][s][. Choosing][ ϵ][ =]_ _q_ 1 1 [and][ p][ = 2][ in the above lemma, we have]

_−_

q

[P]

_∥f_ _∥q[2]_ [=][ ∥] _ar,lY r,l∥q[2]_ (116)

**_r,l_**

X


0[(][d][)]
= _Pϵ⊗|N_
_∥_


_ar,lϵ[−][r]Y r,l_ _q_ (117)
_∥[2]_
**_r,l_**

X


_ar,lϵ[−][r]Y r,l_ 2 (118)
_∥[2]_
**_r,l_**

X


_≤∥_


= _a[2]r,l[ϵ][−][2][r][∥][Y][ r][,][l][∥][2]2_ (119)

**_r,l_**

X

_ϵ[−][2 max][ |][r][|][ X]_ _a[2]r,l[∥][Y][ r][,][l][∥][2]2_ (120)
_≤_

**_r,l_**

= (q 1)[max][ |][r][|] **_f_** 2 2 (121)
_−_ _∥_ _∥[2]_ _[≤]_ [(][q][ −] [1)][s/α][0] _[∥][f]_ _[∥][2]_

Since r /∈ ℒ(G[(][d][)]), there is a j such that rj < r < rj+1. Let n(d) = d[r] and


_Ei)_ (122)
_i[≤j_


_m(d) = dim_ span{Y r,l : ℒ(r) ≤ _rj}_ = dim(span
  


Clearly, m(d) _d[r][j]_ . We list all eigenvalues of in non-ascending order as _λd,i_ . In particular,
_∼_ _K_ _{_ _}_
we have

_λd,m(d) ∼_ _d[−][r][j]_ _> d[−][r]_ _> d[−][r][j][+1]_ _∼_ _λd,m(d)+1 ._ (123)

**Assumption 4 (a). We choose u(d) to be**


_u(d) = dim_ span





**Assumption 4 (a) follows from Proposition 1.**


_._ (124)






_Ei_
_i:ri≤[2r+100_


**Assumptions 4 (b).** Let s = inf{r¯ ∈ ℒ(G[(][d][)]) : ¯r > 2r + 100}. For l > 1, we have

_λ[l]d,j_ (d[−][r][i] )[l] dim(Ei) _d[−][s][(][l][−][1)]_ (125)
_j=uX(d)+1_ _[∼]_ _riX:ri≥s_ _∼_

which also holds for l = 1 since

_λd,j_ 1 (126)
_∼_
_j=u(d)+1_

X


-----

Thus
([P]j=u(d)+1 _[λ]d,j[l]_ [)][2]

(127)

_j=u(d)+1_ _[λ]d,j[2][l]_ _∼_ _d[d][−][−][s][2][(2][s][(][l][l][−][−][1)][1)][ =][ d][s][ > d][2][r][+100][ > n][(][d][)][2+][δ][ ∼]_ _[d][(2+][δ][)][r][.]_

as long as δ < 100/r.

P

**Assumption 4 (c). Denote**

_Kd,>m(d)(ξ, η) =_ _λK(r)Y r,l(ξ)Y r,l(η)_ (128)

**_r,l:LX(r)>r_**

We have
Eη _d,>m(d)(ξ, η)[2]_ =Eη _λ_ (r)[2] **_Y r,l(ξ)Y r,l(η)_** (129)
_K_ _K_ _|_ _|[2]_

**_r,l:LX(r)>r_**


_λK(r)[2]|Y r,l(ξ)|[2]_ (130)
**_r,l:LX(r)>r_**

_λK(r)[2][ X]_ _|Y r,l(ξ)|[2]_ (131)
**_r:LX(r)>r_** **_l_**

_λK(r)[2]N_ (d, r)Pr(1) = _λK(r)[2]N_ (d, r) . (132)
**_r:LX(r)>r_** **_r:LX(r)>r_**


Similarly,


_λK(r)[2]|Y r,l(ξ)Y r,l(η)|[2]_ =
**_r,l:LX(r)>r_**


Eξ,η


_λ_ (r)[2]N (d, r) . (133)
_K_
**_r:LX(r)>r_**


Thus
Eη


Eη _λ_ (r)[2] **_Y r,l(ξ)Y r,l(η)_** Eξ,η

_K_ _|_ _|[2]_ _−_
**_r,l:LX(r)>r_**

For the diagonal terms,


_λK(r)[2]|Y r,l(ξ)Y r,l(η)|[2]_ = 0 . (134)
**_r,l:LX(r)>r_**


_d,>m(d)(ξ, ξ) =_ _λ_ (r) **_Y r,l(ξ)_** = _λ_ (r)N (d, r) = Eξ _d,>m(d)(ξ, ξ)_
_K_ _K_ _|_ _|[2]_ _K_ _K_

**_r,l:LX(r)>r_** **_r:LX(r)>r_**

(135)
which is deterministic.

I.2 VERIFYING Assumption 5.


Recall that n(d) ∼ _d[r]_ and rj < r < rj+1 . Assumption 5(a) follows from Eq. (111). Indeed, for
_l > 1_
_λ[−]d,m[l]_ (d)+1 _λ[l]d,k_ dim(Ei)d[−][lr][i] (136)

_k=mX(d)+1_ _[∼][(][d][−][r][j][+1]_ [)][−][l] _i:riX≥rj+1_

_∼d[lr][j][+1]_ _d[r][i]_ _d[−][lr][i]_ (137)

_i:riX≥rj+1_

=d[r][j][+1] _> n(d)[1+][δ]_ (138)
for some δ > 0 since r < rj+1. Similarly, for l = 1, since


_λd,k_ 1 (139)
_∼_
_k=m(d)+1_

X

_λd,k_ _d[r][j][+1]_ _> n(d)[1+][δ]_ _._ (140)
_∼_
_k=m(d)+1_

X


we have
(d[−][r][j][+1] )[−][1]


**Assumption 5(b) follows from r > rj.**
**Assumption 5(c). Note that**

_λ[−]d,m[1]_ (d) _λd,k_ _λ[−]d,m[1]_ (d) (141)

_k=m(d)+1_ _∼_ _[∼]_ _[d][r][j][ < n][(][d][)][(1][−][δ][)][ ∼]_ _[d][r][(1][−][δ][)]_

X

for some δ > 0 since rj < r.


-----

J PROOF OF LEMMA 4

_Proof. The lemma can be proved by induction. Base case: L = 0. The network is a S-CNN._
WLOG, assume α0 = 0 and α1 = 0. For r ( ), we know that r can be written as a
combination ofr-feasible if in addition, there exists α0 and ̸ _α1, i.e. r = ̸_ **_r k with0α0 + 풮 k(r1) =α ∈L1 for some k1αG1[(] and[d][)]_** _k ℱ0, k(r1) = ≥_ _k0. We say a tuple0α0. Consider the set of all (k0, k1) is_
_r-feasible tuple_

_F_ (r) (k0, k1) : r-feasible _._ (142)
_≡{_ _}_

Clearly, F (r) is finite. It suffices to prove that for each r-feasible tuple (k0, k1),

dim span **_Y r,l : ℱ(r) = k0α0_** 풮(r) = k1α1, l ∈ **_N_** (d, r) _∼_ _d[r]_ (143)

Note that there are  ∼ (d[α][1] )[k][1] many ways to choose k1 nodes in the penultimate layer. Then the 
dimension of the above set is about


_k1_

_N_ (d[α][0] _, k0,j)_ (d[α][1] )[k][1]
_∼_
_j=1_

Y


_k1_

(d[α][0] )[k][0][,j] (144)

_j=1_

Y


(d[α][1] )[k][1]

(k0,1,...,k0,k1 )
_k0,1+···X+k0,k1_ =k0

since the cardinality of the set


(k0,1,...,k0,k1 ) _j=1_
_k0,1+···+k0,k1_ =k0

_∼_ _d[k][0][α][0][+][k][1][α][1]_ = d[r] _,_ (145)


_{(k0,1, . . ., k0,k1_ ) : k0,1 + · · · + k0,k1 = k0, _k0,j ≥_ 1 _k0,j ∈_ N}


is finite.


**Induction step: L ≥** 1. For r with L(r) = r, let k be the number of children of a MST of
퓃(r; o ). Clearly, k [1, [r/αL+1]]. Then we can classify Y r,l into at most [r/αL+1] bins:
_G_ _∈_
Ωk _k=1,...,[r/αL+1] depending on the number of children of o_ in a MST. Let Ωk be non-empty.
_{_ _}_ _G_
We only need to prove the number of Y r,l in Ωk is d[r]. Note that there are ∼ (d[α][L][+1] )[k] many ways
to choose k children from o . Let _uj_ _j=1,...,k be one fixed choice and_ _j_ be the subgraphs with
_G_ _{_ _}_ _{G_ _}_
_uj_ as the output nodes. Next, we partition (r _kαL+1) into k components,_
_{_ _}_ _−_

_r −_ _kαL+1 = r1 + · · · + rk_

so that each rj is a combination of _αj_ 0 _j_ _L. The cardinality of such partition is also finite. We fix_
_{_ _}_ _≤_ _≤_
one of such partition (r1, . . ., rk) so that each rj is a learning index of Gj. We can apply induction to
each (Gj, rj) to conclude that the cardinality of Y rj _,lj with LGj_ (rj) = rj is ∼ _d[r][j]_, where LGj (rj)
is the learning index of rj of Gj. Therefore, we have

dim span **_Y r,l :_** _L(r) = r, l ∈_ **_N_** (d, r) _∼_ (d[α][L][+1] )[k][ Y] _d[r][j]_ = d[r] _._ (146)
    _j∈[k]_

K CNN-GAP: CNNS WITH GLOBAL AVERAGE POOLING

Consider convolutional networks whose readout layer is a global average pooling (GAP) and a
flattening layer (namely, without pooling), resp.

**CNN + GAP:** [Conv(p)-Act] → [Conv(k)-Act][⊗][L] _→_ [GAP] → [Dense] (147)

**CNN:** [Conv(p)-Act] → [Conv(k)-Act][⊗][L] _→_ [Flatten] → [Dense] (148)

Concretely, the input space is = (Sp 1)[k][L][×][w] R[p][×][1][×][k][L][×][w], where p is the patch size of the
**_X_** _−_ _⊆_
input convolutional layer, k is the filter size in hidden layers, L is the number of hidden convolution
layers and w is the spatial dimension of the penultimate layer. The total dimension of the input
is d = p _k[L]_ _w, and the number of input nodes is_ 0 = k[L] _w. Since the stride is equal to_
_·_ _·_ _|N_ _|_ _·_
the filter size for all convolutional layers, the spatial dimension is reduced by a factor of p in the
first layer, a factor of k by each hidden layer. The penultimate layer (before pooling/flattening)


-----

has spatial dimension w and is reduced to 1 by the GAP-dense layer or the Flatten-dense layer.
The DAGs associated to these two architectures are identical which is denoted by G. However,
the kernel/neural network computations are slightly different. If the penultimate layer has n many
channels and fpen : X → R[n][×][w] is the mapping from the input layer to the penultimate layer, then
the outputs of the CNN-GAP and CNN-Flatten are

_fCNN-GAP(x) = n[−]_ 2[1] _ωj_ _fpen(x)j,i_ (149)

_i_ [w]

_j_ [n] _∈_
_∈_

_fCNN-Flatten(x) = (nw)[−][X][1]2_ _ωjifpen(x)j,i,_ (150)

_j∈[nX],i∈[w]_


resp., where wj and wji are parameters of the last layer and are usually initialized with standard iid
Guassian wj, wji ∼N (0, 1). Let N−1 ⊆N be the nodes in the penultimate layer, then |N−1| = w.
Let{ξu,i ξ} = (i∈[kLξ]v. Define)v∈N−1 ∈ **_X_**, where ξv ∈ (Sp−1)[k][L] . Thus, each ξv contains k[L] many input nodes

**_tuv = (_** **_ξu,i, ηv,i_** _/p)i_ [kL] [ 1, 1][k][L] _._ (151)
_⟨_ _⟩_ _∈_ _∈_ _−_

Recall that in the case without pooling


풦u(t) = φ[∗](


풦v(t) (152)
_v∈N−1_


풦v(t)), 풦 =
_uv∈E_ _G_


풦v(t) =
_oG_ _v∈E_


where t ∈ [−1, 1][k][L][×][w], which is usually obtained by t = (⟨ξu,i, ηu,i⟩/p)u∈N−1,i∈[kL]. Indeed, for
each v 1, 풦v depends only on the diagonal terms tvv = ( **_ξv,i, ηv,i_** _/p)i_ [kL] [ 1, 1][k][L] .
We can find a function ∈N− _⟨_ _⟩_ _∈_ _∈_ _−_

풦pen : [ 1, 1][k][L] [ 1, 1] s.t. 풦v(t) = 풦pen(tvv) _v_ 1 (153)
_−_ _→_ _−_ _∀_ _∈N−_

Therefore, without pooling the NNGP kernel is


풦pen(tvv) = [1]
_v∈N−1_ _w_


풦pen(tvv) (154)
_v∈NX−1_


풦CNN(t) =


Note that the kernel does not depend on any off-diagonal terms tuv with u ̸= v because there isn’t
weight-sharing in the last layer. Let dpen = (p, p, . . ., p) ∈ N[k][L] . Then ∥dpen∥1 = pk[L] is the
effective dimension of the input to any node u ∈N−1. Assume k = d[α][k], p = d[α][p] and w = d[α][w]
and αk, αp, αw > 0. Applying Theorem 1 to 풦pen, we have


**_Y r,l(ξv), Y r,l(ηv)_** (155)


풦CNN(t) =


_w_ _[λ][풦][pen]_ [(][r][)]


**_r∈N[kL]_**


_v∈N−1_


**_l∈N_** (dpen,r)


Clearly, the eigenfunctions are **_Y r,l(ξv)_** **_r,l,v_** and the corresponding eigenvalues are
_{_ _}_

[1]
_{_ _w_ _[λ][풦][pen]_ [(][r][)][}][r][,][l][,v][ . Note that]

1

(156)
_w_ _[λ][풦][pen]_ [(][r][) =][ λ][풦][G] [(][r][)][ ∼] _[d][−][ℒ][(][r][)]_

Here and in what follows, we also consider r ∈ N[k][L] as an element in N[wk][L]

When the readout layer is a GAP, the weights of the penultimate layer are shared across different
spatial locations, namely, all nodes in 1 use the same weight. As such, the kernel correspond_N−_
ing to the CNN-GAP depends on both the diagonal and off-diagonal terms t = (tuv)u,v∈N−1
_∈_

[−1, 1][k][L][×][k][L], which can be written as (Novak et al., 2019b)


풦pen(tuv) = [1]
_u,v∈N−1_ _w[2]_


풦pen(tuv) (157)
_u,vX∈N−1_


풦CNN-GAP(t) =


-----

Applying Theorem 1 to 풦pen, we have


풦CNN-GAP(t) = [1]

_w[2]_


**_Y r,l(ξu), Y r,l(ηv)_** (158)


_λ풦pen_ (r)
**_r∈XN[kL]_**


_u,v∈N−1_


**_l∈N_** (dpen,r)

_w[−]_ [1]2

ˆ


_w[−]_ [1]2


_w_ _[λ][풦][pen]_ [(][r][)]

1

_w_ _[λ][풦][pen]_ [(][r][)]


**_Y r,l(ξu)_**


**_Y r,l(ηu)_**

(159)


_u∈N−1_


_u∈N−1_


**_r∈N[kL]_**

**_r∈XN[kL]_**


**_l∈N_** (dpen,r)

**_l∈NX(dpen,r)_**


Sym Sym
**_r,l_** [(][ξ][)][Y] **_r,l_** [(][η][)] (160)


where


Sym
**_r,l_** [(][ξ][)][ ≡] _[w][−]_ 2[1]


**_Y r,l(ξu)_** _,_ **_r_** N[k][L] and **_l_** **_N_** (dpen, r) (161)
_∈_ _∈_


_u∈N−1_


Sym
That is the eigenfunctions and eigenvalues of 풦CNN-GAP are {Y **_r,l_** [(][ξ][)][}][r][,][l] [and][ {][ 1]w _[λ][풦][pen]_ [(][r][)][}][r][,][l][ resp.]

In sum, the eigenvalues of 풦CNN and 풦CNN-GAP are the same (up to the multiplicity factor w). Each
eigenspace of 풦CNN-GAP is given by symmetric polynomials of the form Eq. (161). We can see that
the GAP reduces the dimension of each eigenspace by a factor of w. Same arguments can also be
applied to the NTKs


(풦pen(tvv) + Θpen(tvv)) (162)
_v∈N−1_

1

**_Y r,l(ξv), Y r,l(ηv)_**

_w_ _[λ][풦][pen]_ [(][r][) + 1]w _[λ][Θ][pen]_ [(][r][)]

**_r∈XN[kL]_**   Xv∈N−1 **_l∈NX(dpen,r)_**

(163)

(풦pen(tuv) + Θpen(tuv)) (164)
_u,v∈N−1_

1 Sym Sym

**_Y_** **_r,l_** [(][ξ][)][,][ Y] **_r,l_** [(][η][)] (165)

_w_ _[λ][풦][pen]_ [(][r][) + 1]w _[λ][Θ][pen]_ [(][r][)]

**_r∈XN[kL]_**   Xv∈N−1


ΘCNN(t) =

=

ΘCNN-GAP(t) =


where Θpen is the NTK of the penultimate layer which is the same for all nodes in 1. Since
1 _N−_

_w_ _[λ][Θ][pen]_

_[∼]_ _[d][−][ℒ][(][r][)][, we have]_

1

(166)

_w_ _[λ][풦][pen]_ [(][r][) + 1]w _[λ][Θ][pen]_ [(][r][)][ ∼] _[d][−][ℒ][(][r][)]_

K.1 GENERALIZATION BOUND OF CNN-GAP

We show that GAP improves the data efficiency of D-CNNs by a factor of w ∼ _d[α][w]_ under a stronger
assumptions on activations φ.
**Assumption Poly-φ: There is a sufficiently large J ∈** N such that for all hiddens nodes u

_φ[∗]u(j)(0)_ = 0 for 1 _j_ _J_ and _φ[∗]u(j)(0) = 0_ otherwise (167)
_̸_ _≤_ _≤_

This assumption implies that there are 0 < J1 < J2 ∈ R such that for all 0 ̸= r with |r| < J1

_d[r]_ _d[r]_

and (168)
_dt_ [풦][pen][(][0][)][ ̸][= 0] _dt_ [Θ][pen][(][0][)][ ̸][= 0]


and for all r with **_r_** _> J2_
_|_ _|_

Moreover, J1 as J .
_→∞_ _→∞_


_d[r]_ _d[r]_

and (169)
_dt_ [풦][pen][ ≡] **[0]** _dt_ [Θ][pen][ ≡] **[0]**


-----

Let L[p]Sym[(][X] [)][ ≤] _[L][p][(][X]_ [)][ be the close subspace spanned by symmetric eigenfunctions Eq. (161). Let]
_KSym = 풦CNN-GAP or ΘCNN-GAP._

For X ⊆ **_X and r /∈L(G[(][d][)]), define the regressor and the projection operator to be_**

**_RX[Sym][(][f]_** [)(][x][) =][ K][Sym][(][x, X][)][K][Sym][(][X, X][)][−][1][f] [(][X][)]

**_P>r[Sym][(][f]_** [) =] _f, Y_ Symr,l _L[2](_ )[Y] Symr,l _[.]_

**_r:LX(r)>r_** **_l∈NX(dpen,r)⟨_** _[⟩]_ **_X_**

**Theorem 6. Let G = {G[(][d][)]}d, where each G[(][d][)]** _is a DAG associated to the D-CNN in Eq. (147)_
_with αk, αp, αw > 0. Let r /∈L(G[(][d][)]) be fixed and the activations satisfy Assumption Poly-φ for_
_J = J(r) sufficiently large. Let f ∈_ _L[2]Sym[(][X]_ [)][ with][ E][σ][f][ = 0][. Then for][ ϵ >][ 0][,]

**_RX[Sym][(][f]_** [)][ −] _[f]_ 2L[2]Sym[(][X] [)][ −] **_P>r[Sym][(][f]_** [)] 2L[2]Sym[(][X] [)] [=][ c][d,ϵ][∥][f] _[∥]L[2]_ [2+]Sym[ϵ][(][X] [)][,] (170)

_where cd,ϵ_ 0 in probability as d _over X_ **_σ[[][d][r][−][αw][ ]]._**
_→_ _→∞_ _∼_

_Proof of Theorem 6. We need the following dimension counting lemma, which follows directly_
from Lemma 4.


**Lemma 6. Let r ∈L(G[(][d][)]). Then**

_Sym_

dim span **_Y_** **_r,l_** [:] _L(r) = r, l ∈_ **_N_** (d, r) _∼_ _d[r][−][α][w]_ (171)
 n o

Recall that ℒ( ) = _rj_ in non-descending order. Similarly, let
_G[(][d][)]_ _{_ _}_

Sym
_Ei[Sym]_ = span{Y **_r,l_** [:] ℒ(r) = ri} (172)

From the above lemma, we have

dim(Ei[Sym]) _d[r][i][−][α][w]_ (173)
_∼_

Since r /∈ ℒ(G[(][d][)]), there is a j such that rj < r < rj+1. Let n(d) = d[r][−][α][w] and

Sym
_m(d) = dim_ span{Y **_r,l_** [:][ ℒ][(][r][)][ ≤] _[r][j][}]_ = dim(span _Ei[Sym])_ (174)

_i_ _j_

  [≤

Clearly, m(d) ∼ _d[r][j]_ _[−][α][w]_ . We list all eigenvalues of KSym in non-ascending order as {λd,i}. In
particular, we have

_λd,m(d) ∼_ _d[−][r][j]_ _> d[−][r]_ _> d[−][r][j][+1]_ _∼_ _λd,m(d)+1 ._ (175)

We proceed to verify Assumptions 4 and 5 in Sec. L.

**Assumptions 4 (a)** We choose u(d) to be


_u(d) = dim_ span _Ei[Sym]_ _._ (176)

 

_i:ri≤[2r+100_

 

Let s = inf{r¯ ∈ ℒ(G[(][d][)]) : ¯r > 2r + 100}. Assumption 4 (a) follows from Proposition 1.

**Assumptions 4 (b)** For l > 1, we have


(d[−][r][i] )[l] dim(Ei[Sym]) _d[−][s][(][l][−][1)][−][α][w]_ (177)
_∼_
_riX:ri≥s_


_λ[l]d,j_
_j=u(d)+1_ _[∼]_

X

which also holds for l = 1 since


_d[α][w]_


_λd,j_ 1 (178)
_∼_
_j=u(d)+1_

X


-----

Thus

([P]j=u(d)+1 _[λ]d,j[l]_ [)][2]

_j=u(d)+1_ _[λ]d,j[2][l]_ _∼_ _[d]d[−][−][2][s][s][(2][(][l][l][−][−][1)][1)][−][−][2][α][α][w][w][ =][ d][s][−][α][w][ > d][2][r][+100][−][α][w][ > n][(][d][)][2+][δ][ ∼]_ _[d][(2+][δ][)(][r][−][α][w][)][.]_

(179)

P

**Assumption 4 (c) This requires some work and is verified in Sec. K.2.**

**Assumption 5 (a) Since m(d) = dim(span** _i_ _j_ _[E][i][)][ and][ r][j][+1][ > r > r][j][, we have]_

_≤_

1 1

_λd,m(d)+1_ _j≥mX(d)+1_ _λ[l]d,j_ [S][∼] (d[−][r][j][+1] )[l] Xi>j (d[−][r][i] )[l] dim(Ei[Sym]) (180)

_∼_ dim(Ej[Sym]+1[) =][ d][r][j][+1][−][α][w] (181)

_> n(d)[1+][δ]_ = d[(][r][−][α][w][)(1+][δ][)] (182)

as long as δ < (rj+1 _αw)/(r_ _αw)_ 1.
_−_ _−_ _−_

**Assumption 5 (b) This is obvious since m(d) ∼** _d[r][j]_ _[−][α][w]_, n(d) ∼ _d[r][−][α][w]_ and r > rj.

**Assumption 5 (c) This follows from rj < r. Indeed,**


1 1

_λd,m(d)_ _j≥mX(d)+1_ _λd,j ∼_ (d[−][r][j] ) Xi>j (d[−][r][i] ) dim(Ei[Sym]) ∼ _d[r][j]_ _[−][α][w]_ (183)

_≤_ _n(d)[1][−][δ]_ = d[(][r][−][α][w][)(1][−][δ][)] (184)

as long as 0 < δ < 1 (rj _αw)/(r_ _αw)._
_−_ _−_ _−_

K.2 VERIFICATION OF ASSUMPTIONS 4(C).

We begin with proving Eq. (227). Let Xi define the random variable


_Xi_ Eξ **_σd_** Sym,>m(d)(ξi, ξ)[2] and ∆i _Xi_ EXi (185)
_≡_ _∼_ _K_ _≡_ _−_

We need to show that


supi∈[n(d)] |∆i|

EXi

By Markov’s inequality, it suffices to show that


in prob.
(186)
_−d−−−→_
_→∞_ [0][.]


(EXi)[−][1]E sup ∆i (187)
_i∈[n(d)]_ _|_ _| −d−→∞−→_ [0]

By orthogonality and treating r ∈ N[k][L] as an element of N[wk][L], we have

EXi = Eξ, ¯ξ **_σd_** _[K]Sym,>m(d)[(][ξ][,][ ¯]ξ)[2]_ (188)
_∼_


Sym Sym
**_r,l_** [(][ξ][)][Y] **_r,l_** [(¯]ξ)


= Eξ, ¯ξ∼σd

= Eξ, ¯ξ∼σd


(189)


_KSym(r)_
**_r:ℒX(r)>r_** **_l∈NX(dpen,r)_**
b


Sym Sym 2
**_r,l_** [(][ξ][)][Y] **_r,l_** [(¯]ξ) (190)


_KSym[2]_ [(][r][)]
**_r:ℒ(r)>r_**

X

b


**_l∈N_** (dpen,r)


Sym Sym 2
Eξ, ¯ξ **_σd_** **_Y_** **_r,l_** [(][ξ][)][Y] **_r,l_** [(¯]ξ) (191)
_∼_
**_l∈NX(dpen,r)_**

1 (192)
**_l∈NX(dpen,r)_**


_KSym[2]_ [(][r][)]
**_r:ℒ(r)>r_**

X

b

_KSym[2]_ [(][r][)]
**_r:ℒ(r)>r_**

X

b


_KSym[2]_ [(][r][)][|][N] [(][d][pen][,][ r][)][|] (193)
**_r:ℒ(r)>r_**

X

b


-----

and
_Xi = Eξ_ **_σd_** Sym,>m(d)(ξi, ξ)[2] (194)
_∼_ _K_


Sym Sym
**_r,l_** [(][ξ][i][)][Y] **_r,l_** [(][ξ][)]


= Eξ **_σd_**
_∼_

= Eξ **_σd_**
_∼_


(195)


_KSym(r)_
**_r:ℒX(r)>r_** **_l∈NX(dpen,r)_**
b


Sym Sym 2
**_r,l_** [(][ξ][i][)][Y] **_r,l_** [(][ξ][)] (196)


_KSym[2]_ [(][r][)]
**_r:ℒX(r)>r_** **_l∈NX(dpen,r)_**
b


Sym Sym 2
Eξ **_σd_** **_Y_** **_r,l_** [(][ξ][i][)][Y] **_r,l_** [(][ξ][)] (197)
_∼_
**_l∈NX(dpen,r)_**

Sym
_|Y_ **_r,l_** [(][ξ][i][)][|][2] (198)
**_l∈NX(dpen,r)_**


_KSym[2]_ [(][r][)]
**_r:ℒ(r)>r_**

X

b

_KSym[2]_ [(][r][)]
**_r:ℒ(r)>r_**

X

b

_KSym[2]_ [(][r][)]
**_r:ℒ(r)>r_**

X

b


 _w_

 [1]


**_Y r,l(ξi,u)[2]_** + [1]


= Sym[(][r][)] **_Y r,l(ξi,u)[2]_** + [1] **_Y r,l(ξi,u)Y r,l(ξi,v)_** (199)

_K[2]_  _w_ _w_ 
**_r:ℒX(r)>r_** **_l∈NX(dpen,r)_** Xu _uX≠_ _v_
b  [1] 

= Eξ, ¯ξ **_σd_** _[K]Sym,>m(d)[(][ξ][,][ ¯]ξ)[2]_ + Sym[(][r][)] **_Y r,l(ξi,u)Y r,l(ξi,v)_**
_∼_ _K[2]_  _w_

**_r:ℒX(r)>r_** **_l∈NX(dpen,r)_** _uX≠_ _v_
b  [1] (200)

Let


 _w_

 [1]


(201)


_Xr,i =_


**_Y r,l(ξi,u)Y r,l(ξi,v)_**


**_l_** **_N_** (dpen,r) _u=v_
_∈_ _̸_

then  
∆i = Sym[(][r][)][X][r][,i] (202)

_K[2]_
**_r:ℒ(r)>r_**

X

We replace the maximal function by the l[q]-norm for1 b q ≥ 1, 1 1 1

E _i_ sup[n(d)] _|∆i| ≤_ E( _|∆i|[q])_ _q ≤_ (E _|∆i|[q])_ _q = n(d)_ _q (E|∆i|[q])_ _q_ (203)
_∈_ _i_ [n(d)] _i_ [n(d)]

_∈X_ _∈X_

where the first three expectations are taken over ξ1, . . ., ξn(d) ∼ **_σd and the last one is taken over_**
**_ξi_** **_σd. Then we replace the L[q]-norm by the L[2]-norm via Hypercontractivity, in which we used_**
**Assumption Poly- ∼** _φ which implies that ∆i is a polynomial of bounded degree_


1 1 1 1 1 1
E _i∈sup[n(d)]_ _|∆i| ≤_ _n(d)_ _q (E|∆i|[q])_ _q ≤_ _Cqn(d)_ _q (E|∆i|[2])_ 2 = Cqn(d) _q (E∆[2]i_ [)] 2 (204)

We expand the L[2]-norm and use orthogonality to “erase" the off-diagonal terms twice: first for
**_r ̸= ¯r_**
Eξi **_σd_** _Xr,iXr¯,i_ [= 0] (205)
_∼_
and second for l ̸= l[′] or u ̸= v

Eξi **_σd_** _Xr[2],i_ [= 1] **_Y r,l(ξi,u)Y r,l(ξi,v)_** **_Y r,l′_** (ξi,u′ )Y r,l′ (ξi,v′ )
_∼_ _w[2][ E][ξ][i][∼][σ][d]_   

**_l_** **_N_** (dpen,r) _u=v_ **_l[′]_** _u[′]=v[′]_
_∈_ X _̸_ _̸_

[X]  [X]  [X] (206)

= [2] **_Y r,l(ξi,u)[2]Y r,l(ξi,v)[2]_** = [2] _w(w_ 1)

_w[2][ E][ξ][i][∼][σ][d]_   _w[2]_ _−_

**_l_** **_N_** (dpen,r) _u=v_ **_l_** **_N_** (dpen,r)
_∈_ X _̸_ _∈_ X

[X]  (207)


= [2(][w][ −] [1)]


1 = [2(][w][ −] [1)]


**_N_** (dpen, r) 2 **_N_** (dpen, r) (208)
_|_ _| ≤_ _|_ _|_


-----

Combining this estimate with Eq. (193), Eq. (202) and Eq. (204) yields

(EXi)[−][1](E _i∈sup[n(d)]_ _|∆i|) ≤_ _Cqn(d)_ _q1_ [(2][ P]rr:ℒ:ℒ((rr))>r>rKKSym[4]Sym[2] [(][(][r][r][)][)][|][N][|][N][(][(][d][d][pen][pen][,][ r][,][ r][)][)][|][)][|] [1][/][2] (209)

[b]

_√2Cqn(d)_ _q1_ Pr:ℒ(r)>r _K[b]Sym[2]_ [(][r][)][|][N] [(][d][pen][,][ r][)][|][1][/][2] (210)
_≤_

P **_r:ℒ(r)>r_** Sym[(][r][)][|][N] [(][d][pen][,][ r][)][|]

_K[2]_

[b]

_√2Cqn(d)_ _q1_ Psup _KSym[2]_ [b][(][r][)][|][N] [(][d][pen][,][ r][)][|][1][/][2] (211)
_≤_ _r_ **_r:ℒ(r)>r_** bKSym[2] [(][r][)][|][N] [(][d][pen][,][ r][)][|]

2Cqd _q_ sup **_N_** (dpen, r) 2 (212)
_∼_ **_r:ℒ(r)>r_** _|_ b _|[−]_ [1]


_r_
_d_ _q_ sup _d[−|][r][|][α][p]_ (213)
_∼_ **_r:ℒ(r)>r_** _−d−→∞−→_ [0]

by choosing q (independent of d) sufficiently large.

The proof of Eq. (228) is similar. Let Xi denote the random variable


_Xi ≡KSym,>m(d)(ξi, ξi)_ and ∆i ≡ _Xi −_ EXi (214)

E supi [n(d)] ∆i
_∈_ _|_ _|_ 0 (215)

EXi _−d−→∞−→_


and it suffices to prove

We have


EXi = Eξi **_σd_** Sym,>m(d)(ξi, ξi) (216)
_∼_ _K_

Sym Sym

= Eξi **_σd_** Sym(r) **_Y_** **_r,l_** [(][ξ][i][)][Y] **_r,l_** [(][ξ][i][)] (217)
_∼_ _K_

**_r:ℒX(r)>r_** **_l∈NX(dpen,r)_**
b

= _KSym(r)|N_ (dpen, r)| (218)

**_r:ℒ(r)>r_**

X

b


and


∆i = Xi − EXi = w[−][1]


**_Y r,l(ξi,u)Y r,l(ξi,v), ._** (219)


_KSym(r)_
**_r:ℒ(r)>r_**

X

b


**_l∈N_** (dpen,r)


_u≠_ _v_


The remaining steps (replacing the maximal function by the l[q]-norm, and then the L[q]-norm by the
_L[2]-norm using hypercontractivity) are similar to that of the proof of Eq. (227), which are omitted_
here.

L KERNEL CONCENTRATION, HYPERCONTRACTIVITY AND
GENERALIZATION OF MEI ET AL. (2021A)

For convenience, we briefly recap the analytical results regarding generalization bounds of kernel
machines from Mei et al. (2021a) Sec 3.

Let (Xd, σd) be a probability space and ℋd be a compact self-adjoint positive definite operator
from L[2](Xd, σd) _L[2](Xd, σd). We assume ℋd_ _L[2](Xd_ **_Xd). Let_** _ψd,j_ and _λd,j_ be the
eigenfunctions and eigenvalues associated to → ℋd, i.e. ∈ _×_ _{_ _}_ _{_ _}_

ℋdψd,j(x) ℋd(x, y)ψd,j(y)σd(y) = λd,jψd,j(x). (220)
_≡_ ˆy∈Xd

We assume the eigenvalues are in non-ascending order, i.e. λd,j+1 _λd,j_ 0. Note that
_≥_ _≥_

_λ[2]d,j_ [=][ ∥][ℋ][d][∥]L[2] [2](Xd **_Xd)_** _[<][ ∞][.]_ (221)
_×_

X


-----

The associated reproducing kernel Hilbert space (RKHS) is defined to be functions f _L[2](Xd, σd)_
_∈_

with ℋd− 2[1] _f_ _L2(Xd,σd) <_ . Given a finite training set X **_Xd and observed labels f_** (X)
_∥_ _∥_ _∞_ _⊆_ _∈_

R[|][X][|], the regressor is an extension operator defined to be

ℛX _f_ (x) = ℋd(x, X)ℋd(X, X)[−][1]f (X) . (222)


Intuitively, when “X **_Xd" in some sense, we expect the following_**
_→_

ℛX _f_ (x) = ℋd(x, X)ℋd(X, X)[−][1]f (X) → ℛXd _f_ (x) = ℋd(ℋd[−][1][f] [)(][x][) =][ f] [(][x][)][,] (223)

namely, “ℛX **_IXd_** " in some sense.
_→_

Using tools from the non-asymptotic analysis of random matrices (Vershynin, 2010), the work Mei
et al. (2021a) provides a very nice answer to the above question in terms of the decay property
of the eigenvalues _λd,j_ and the hypercontractivity property of the eigenfunctions _ψd,j_ . They
_{_ _}_ _{_ _}_
show that ℛX is essentially a projection operator onto the low eigenspace under certain regularity
assumptions on the operator ℋd. These assumptions are stated via the relationship between the
number of (training) samples n = n(d), the tail behavior of the eigenvalues with index ≥ _m = m(d)_
and the tail behavior of the operator ℋd


ℋd,>m(d)(x, ¯x) _λjψj(x)ψj(¯x)_ (224)
_≡_

_j>m(d)_

X


as the "input dimension" d becomes sufficiently large.

**Assumption 4. We say that the the sequence of operator** ℋd _d_ 1 satiesfies the Kernel Concen_{_ _}_ _≥_
tration Property (KCP) with respect to the sequence _n(d), m(d)_ _d_ 1 if there exsts a sequence of
_{_ _}_ _≥_
integers _u(d)_ _d_ 1 with u(d) _m(d) such that the following holds_
_{_ _}_ _≥_ _≥_

(a) (Hypercontractivity.) Let Du(d) = span{ψj : 1 ≤ _j ≤_ _u(d)}. Then for any fixed q ≥_ 1,
and C = C(q) such that for f _Du(d)_
_∈_

**_f_** _Lq(Xd,σd)_ _C_ **_f_** _L2(Xd,σd)_ (225)
_∥_ _∥_ _≤_ _∥_ _∥_

(b) (Eigen-decay.) There exists δ > 0, such that, for all d large enough, for l = 1 and 2,

([P]j _u(d)+1_ _[λ]d,j[l]_ [)][2]
_n(d)[2+][δ]_ _≤_ _j≥_ _u(d)+1_ _[λ]d,j[2][l]_ (226)

_≥_
P


(c) (Concentration of Diagonals.) For {xi}i∈[n(d)] ∼ **_σd[n][(][d][)], we have:_**

supi∈[n(d)] Ex∼σd ℋd,>m(d)(xi, x)[2] _−_ Ex,x¯∼σd [ℋ]d,>m(d)[(][x,][ ¯]x)[2] in Prob. 0 (227)

Ex,x¯∼σd [ℋ]d,>m(d)[(][x,][ ¯]x)[2] _−d−→∞−−→_

supi∈[n(d)] ℋd,>m(d)(xi, xi) − Ex∼σd ℋd,>m(d)(x, x) in Prob. 0 (228)

Ex∼σd ℋd,>m(d)(x, x) _−d−→∞−−→_

where cd 0 in probability as d .
_→_ _→∞_

**Assumption 5. Let ℋd and** _m(d), n(d)_ _d_ 1 be the same as above.
_{_ _}_ _≥_

(a) For l = 1 and 2, there exists δ > 0 such that


_n(d)[1+][δ]_ _≤_

(b) There exists δ > 0 such that


_λ[l]d,k_ (229)
_k=λm(d)+1_

X


_λ[l]d,m(d)+1_


_m(d) ≤_ _n(d)[1][−][δ]_ (230)

(c) (Spectral Gap.) There exists δ > 0 such that


_n(d)[1][−][δ]_ _≥_


_λd,k_ (231)
_k≥mX(d)+1_


_λd,m(d)_


-----

Let 풫>k (similarly for 풫k, 풫 _k, etc.) denote the projection operator_
_≤_


_f, ψj_ _ψj_ (232)
_⟨_ _⟩_
_j>k_

X


풫>kf =


**Theorem 7 (Mei et al. (2021a)). Assume ℋd satisfy Assumptions 4 and 5. Let** _fd_ _d_ 1 be a
_{_ _}_ _≥_
_sequence of functions and let X_ **_σd[n][(][d][)]. Then for every ϵ > 0,_**
_∼_

_∥ℛX_ (fd) − _fd∥L[2]_ [2](Xd,σd) [=][ ∥][풫][>m][(][d][)][f][d][∥]L[2] [2](Xd,σd) [+][ c][d,ϵ][∥][f][d][∥]L[2] [2+][ϵ](Xd,σd) (233)

_where cd,ϵ_ 0 in probability as d _._
_→_ _→∞_

The theorem says, ℛX is essentially the projection operator 풫 _m(d) in the sense that when restricted_
_≤_
to L[2+][ϵ](Xd, σd),

ℛX = 풫 _m(d) + Errord,ϵ_ (234)
_≤_


-----

