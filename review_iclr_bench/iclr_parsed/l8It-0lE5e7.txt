# IMPLICIT BIAS OF ADVERARIAL TRAINING FOR DEEP NEURAL NETWORKS


**Bochen Lyu**
DataCanvas Lab
DataCanvas, Beijing, China
lvbc@zetyun.com


**Zhanxing Zhu**
The University of Edinburgh, UK
zhanxing.zhu@gmail.com

ABSTRACT


We provide theoretical understandings of the implicit bias imposed by adversarial
training for homogeneous deep neural networks without any explicit regularization.
In particular, for deep linear networks adversarially trained by gradient descent on
a linearly separable dataset, we prove that the direction of the product of weight
matrices converges to the direction of the max-margin solution of the original
dataset. Furthermore, we generalize this result to the case of adversarial training
for non-linear homogeneous deep neural networks without the linear separability
of the dataset. We show that, when the neural network is adversarially trained
with ℓ2 or ℓ FGSM, FGM and PGD perturbations, the direction of the limit
_∞_
point of normalized parameters of the network along the trajectory of the gradient
flow converges to a KKT point of a constrained optimization problem that aims to
maximize the margin for adversarial examples. Our results theoretically justify the
longstanding conjecture that adversarial training modifies the decision boundary
by utilizing adversarial examples to improve robustness, and potentially provides
insights for designing new robust training strategies.

1 INTRODUCTION

Deep neural networks (DNNs) have achieved great success in many fields such as computer vision
(Krizhevsky et al., 2012; He et al., 2015) and natural language processing (Collobert & Weston, 2008)
and other applications. These breakthroughs also lead to the importance of research on the robustness
and security issues of DNNs, among which those about adversarial examples are especially prevalent.
Adversarial examples are obtained by adding craftily designed imperceptible perturbations to the
original examples, which can sharply change the predictions of the DNNs with high confidence
(Szegedy et al., 2014; Nguyen et al., 2015). Such vulnerability of DNNs renders security concerns
about deploying them in security-critical systems including vision for autonomous cars and face
recognition. It is therefore crucial to develop defense mechanisms against these adversarial examples
for deep learning.

To this end, many defense strategies have been proposed to make DNNs resistant to adversarial
examples, such as adding a randomization layer before the input to the classifier (Xie et al., 2018),
input transformations (Guo et al., 2018), adversarial training (Madry et al., 2018), etc. However,
Athalye et al. (2018) pointed out that most of the defense techniques are ineffective and give a false
sense of security due to obfuscated gradients except for adversarial training—a method which has
not been comprehensively attacked yet. Given a C-class training dataset {(xi, yi)}i[n]=1 [with natural]
examplesolving the following minimax optimization problem xi ∈ R[d] and corresponding label yi ∈{1, ..., C}, adversarial training is formulated as


max (1)
_δi_ (0,ϵ) _[ℓ]_ [(][x][i][ +][ δ][i][(][W] [)][, y][i][;][ W] [)][,]
_i=1_ _∈B_

X


min ˜(W ) = min
_W_ _L_ _W_


where f is the DNN function, ℓ is the loss function and δi(W ) is the adversarial perturbation
generated by some adversary (xi, yi; W ) typically depending on model parameters W, within
_A_
the set (0, ϵ) = _δ :_ _δ_ _p_ _ϵ_ around the natural example xi. Commonly, adversarial training
conducts a two-player game at each iteration: the inner maximization is to attack the model and B _{_ _∥_ _∥_ _≤_ _}_


-----

find the corresponding perturbation of the original example that maximizes the classification loss ℓ;
the outer minimization, on the other hand, is to update the model parameters W with the gradient
descent method such that the loss ℓ is minimized on adversarial examples generated by the inner
maximization (see Algorithm 1 for details).

To have a better theoretical understanding of the fact that adversarial training empirically improves
the robustness of DNNs against adversarial examples, Zhang et al. (2019) decomposed the prediction
error for adversarial examples and identified a trade-off between robustness and accuracy while Li
et al. (2020) studied the inductive bias of gradient descent based adversarial training for logistic
regression on linearly separable data. Faghri et al. (2021) studied adversarial robustness of linear
neural networks by exploring the optimization bias of different methods. Yu et al. (2021) studied
adversarial training through the bias-variance decomposition and showed that its generalization error
on clean examples mainly comes from the bias. However, even for the simplest DNN—the deep linear
network, we notice that there exists no work to theoretically understand such robustness achieved by
adversarial training through exploring its implicit bias.

On the other hand, for the standard training, recent works extensively explored the implicit bias
imposed by gradient descent or its variants for DNNs in different settings. For a simple setting of
linear logistic regression on linearly separable data, Soudry et al. (2018); Ji & Telgarsky (2018);
Nacson et al. (2019b) derived that the direction of the model parameter converges to that of the
max-ℓ2-margin with divergent norm. Shamir (2021) concluded that gradient methods never overfit
when training linear predictors over separable dataset. Viewing the above model as a single-layer
network, a natural but more complicated extension is that for the deep linear network on linearly
separable data: Ji & Telgarsky (2019) proved that the gradient descent aligns the weight matrices
across layers and the product of weight matrices also converges to the direction of the max-ℓ2-margin
solution; Gunasekar et al. (2018) showed the implicit bias of gradient descent on linear convolution
networks. Nacson et al. (2019a); Lyu & Li (2020) further promoted the study of implicit bias of
gradient descent for standard training to the case of more general homogeneous non-linear neural
networks and proved that the limit point of the optimization path is along the direction of a KKT
point of the max-margin problem. Wei et al. (2019) studied the regularization path for homogeneous
DNNs and also proved the convergence to the max-margin direction in this setting. Banburski et al.
(2019) showed that gradient descent induces a dynamics of normalized weights converging to an
equilibrium which corresponds to a minimal norm solution.

It is therefore our goal in this paper to theoretically understand the resistance to adversarial examples
for adversarially trained DNNs, linear and non-linear ones, through the lens of implicit bias imposed
by adversarial training. Due to the inner maximization, adversarial training differs a lot from standard
training and one should be careful to analyze the perturbed training dynamics. For the adversarial
training objective Eq. (1), various approaches have been proposed to solve the inner maximization,
such as fast gradient sign method (FGSM (Goodfellow et al., 2015)) and its stronger version projected
gradient descent (PGD (Madry et al., 2018)). The widely used adversarial training adopts PGD to
attack the model, while recent work (Wong et al., 2020) also suggested that a weaker adversary
can also surprisingly yield a model with satisfying robustness. Thus to conduct a comprehensive
study about the implicit bias of adversarial training for DNNs, we will use ℓ2-fast gradient method
(FGM (Miyato et al., 2016)), ℓ∞ FGSM, ℓ2-PGD and ℓ∞-PGD to solve the inner maximization of
the adversarial training objective.

1.1 OUR CONTRIBUTION

In this paper, we devote to answering two questions. First, is there any implicit bias imposed by
_adversarial training for DNNs without explicit regularization? Second, if there exists such an implicit_
bias, what are the convergence properties of the model parameters along the adversarial training
_trajectory?_

To this end, we first investigate the adversarial training with ℓ2 adversarial perturbations for deep
linear networks on linear separable data where the allowed Euclidean distances from the adversarial
examples to their corresponding original examples _x[′]i_
the original dataset. Despite the simplicity of this setting, this problem is meaningful due to its ∥ _[−]_ _[x][i][∥][2][ are less than the max-][ℓ][2][-margin of]_
non-convexity and the introduction of adversarial examples, which heavily depend on the model
parameters, during the training process. We prove that gradient descent for adversarial training


-----

implicitly aligns weight matrices across layers and the direction of the product of weight matrices
also surprisingly converges to that of the max-ℓ2-margin solution of the original dataset—similar to
that of standard training for deep linear network (Ji & Telgarsky, 2019). Our results significantly
generalize those in Li et al. (2020) for adversarial training of logistic regression. This simple yet
insightful case positively answers our first question but partially answers the second one because it
still remains unclear why such convergence property can improve robustness considering its similarity
to that for the standard training. In fact, adversarial training differs from standard training in the
way how they impose such convergence property for parameters: the first one is to maximize the
margin of adversarial examples while the latter is maximizing that of the original dataset, and these
two optimization problems happen to possess solutions along the same direction.

We then move forward to explore a more general situation, adversarial training for homogeneous
non-linear DNNs without the linear separability of the dataset. We study the limit point of the
normalized model parameters along the adversarial training trajectory and show that

**Theorem 1 (Informal). When the deep neural network is adversarially trained with one of the**
_ℓ2-FGM, FGSM, ℓ2-PGD and ℓ∞-PGD perturbations, the limit point of the normalized model_
_parameters is along the direction of a KKT point of a constrained optimization problem which aims_
_to maximize the margin of adversarial examples._

This indicates that adversarial training is implicitly maximizing the margin of adversarial examples
rather than that of original dataset. Thus Theorem 1 provides another view for the high bias error
on clean examples of adversarial training in Yu et al. (2021) since distributions of adversarial and
clean examples are different. To the best of knowledge, these results are the first attempt to analyze
the implicit bias of adversarial training for DNNs. We believe our results provide a theoretical
understanding on the effectiveness of adversarial training for improving robustness against adversarial
examples. It could potentially shed light on how to enhance the robustness of adversarially trained
models or even further inspire more effective defense mechanisms.

**Organization.** This paper is organized as follows. Section 2 is about notations and settings. Section
3 presents our main results on the implicit bias of adversarial training for DNNs. Section 4 provides
numerical experiments to support our claims. We conclude this work in Section 5 and discuss future
directions. Some technical proofs are deferred to supplementary materials.

**Algorithm 1 Adversarial Training**

**Input: Training set S = {(xi, yi)}i[n]=1[, Adversary][ A][ to solve the inner maximization, learning]**
rate η, initialization Wk for k ∈{1, . . ., L}
**for t = 0 to T −** 1 do

_S[′](t) = ∅_
**for i = 1 to n do**

_x[′]i[(][t][) =][ A][(][x][i][, y][i][, W]_ [(][t][))]

_S[′](t) = S[′](t)_ (x[′]i[(][t][)][, y][i][)]

**end for**
**for k = 1 to L do**

_Wk(t + 1) = W[S]k(t)_ _η(t)_ _[∂]L[˜](S∂W[′](tk);W )_
_−_

**end for**

**end for**


2 PRELIMINARIES

**Notations.** For any matrix A ∈ R[m][×][n], we denote its i-th row j-th column entry by Aij. Let A[T]
denote the transpose of A. ∥·∥F represents the Frobenius norm and ∥·∥p is the ℓp norm. The training
set is {(xi, yi)}i[n]=1 [where][ x][i][ ∈] [R][d][,][ ∥][x][i][∥][2][ ≤] [1][ and][ y][i][ ∈{][1][,][ −][1][}][. For a scalar function][ f][ :][ R][d][ 7→] [R][,]
we denote its gradient by ∇f . Furthermore, tr (A) = _i_ _[A][ii][ denotes the trace for the matrix][ A][.]_

We study the adversarial training for the L-layer positively homogeneous deep neural network

[P]

_f_ (x; W ) = WLϕL (WL 1 _ϕ3(W2ϕ2(W1x))_ ) (2)
_−_ _· · ·_ _· · ·_


-----

where Wk is the k-th layer weight matrix and ϕk is the activation function of the k-th layer [1]. The
multi-c-homogeneity of the network is defined by


_a[c]k[f]_ [(][x][;][ W] [)] (3)
_k=1_

Y


_f_ (x; a1W1, _, aLWL) =_
_· · ·_


for any positive constants ak’s and c 1, where W = (W1, _, WL) is the collection of the_
_≥_ _· · ·_
parameters of the network. For example, deep ReLU networks are multi-1-homogeneous. For
convenience, we also adopt the following notations for a multi-c-homogeneous DNN:

_ρk =_ _Wk_ _F, ρ = ρ[c]1_ _L_ and _f_ (x; W ) = ρf (x; _W_ ), (4)
_∥_ _∥_ _[· · ·][ ρ][c]_

where _Wk = Wk/_ _Wk_ _F with_ _Wk_ _F = 1 for k_ 1, . . ., L .
_∥_ _∥_ _∥[c]_ _∥_ _∈{_ _}_ [c]

We use δi(W ) to represent the adversarial perturbation of the original example xi within the perturbation setthe scale invariant adversarial perturbations[c] B(0, ϵ) : {δ : ∥δ∥p ≤ _ϵ} around the original example defined as follows for adversarial training in this paper. xi for f_ (x; W ). Furthermore, we use
**Definition 1 (Scale invariant adversarial perturbation). An adversarial perturbation is said to be a**
_scale invariant adversarial perturbation for f_ (xi; W1, . . ., WL) and loss function ℓ _if it satisfies_

_δi(a1W1, . . ., aLWL) = δi(W1, . . ., WL)_ (5)

_for any positive constants ak’s._

We will show in Section 3.2 that FGSM, ℓ2-FGM, ℓ2-PGD and ℓ∞-PGD perturbations for homogeneous DNNs are all scale invariant perturbations, which are important for analyzing different types of
perturbation in a unified manner. The empirical adversarial training loss with the perturbation δi(W )
is given by


˜(W ) = [1]
_L_ _n_


_n_

_ℓ˜(yi, xi; W_ ) = [1]

_n_

_i=1_

X


_ℓ_ (yif (xi + δi(W ); W )) . (6)
_i=1_

X


For ease of notation, we denote _ℓ[˜]i(W_ ) = ℓ[˜](yi, xi; W ). The loss function ℓ is continuously differentiable and satisfies
**Assumption 1 (Loss function). ℓ> 0, ℓ[′]** _< 0, limx_ _ℓ(x) = 0 and ℓx_ (x) = _._
_→∞_ _→−∞_ _∞_

Many widely used loss functions satisfy the above assumption such as ℓ(x) = e[−][x] and the logistic
loss ℓ(x) = ln(1 + e[−][x]). Furthermore, we make the following common assumptions about the
smoothness of f (x; W ) and the adversarial perturbation δ(W ):
**Assumption 2 (Smoothness). With respect to W** _, yf_ (x; W ) is locally Lipschitz for any fixed x;
_yif_ (xi; W ) further have locally Lipschitz gradients and δi(W ) are locally Lipschitz for all training
_examples xi._

**Remark.** Our results can also be generalized to non-smooth homogeneous neural networks straightforwardly (Appendix B.4). Assuming Lipschitzness about perturbations is because we focus on
popular perturbations such as ℓ2-FGM and PGD perturbations which have explicit forms and depend
on gradients of the network, whose Lipschitzness assumptions are quite common.

3 MAIN RESULTS

We present our main theoretical results on the implicit bias of the gradient flow/gradient descent for
adversarial training in this section. For the deep linear neural network, a special kind of homogeneous
models, in Section 3.1 we further restrict the dataset to be linearly separable and focus on ℓ2 adversarial
perturbations. We prove that the singular vectors corresponding to the largest singular values of
weight matrices get aligned across layers with the progress of adversarial training[2]. Based on this key
result, the product of weight matrices converges to the direction of the maximum margin solution

1ϕk is a vector in our notations.
2This phenomenon for standard training has been studied by Ji & Telgarsky (2019) first.


-----

under the original data. Furthermore, we study a much more general scenario in Section 3.2: without
the assumption of linearly separability of data and norm constraints on adversarial perturbations,
we show that the gradient flow of adversarial training with scale invariant adversarial perturbations
for the homogeneous nonlinear neural networks implicitly performs margin maximization under
the adversarial data. Due to the space limit, some technical proofs of Section 3.1 are deferred to
Appendix A, and we present proofs of Section 3.2 in Appendix B.

3.1 ADVERSARIAL TRAINING FOR DEEP LINEAR NETWORK

In this section we restrict the dataset {(xi, yi)}i[n]=1 [to be linearly separable in the sense that there]
exists a unit vector u such that _i_ 1, . . ., n _, yi_ _u, xi_ _> 0 and we explore the adversarial_
training dynamics of gradient descent for deep linear networks, i.e., with identity activation function ∀ _∈{_ _}_ _⟨_ _⟩_
_ϕk(x) = x for all layers,_
_f_ (x, W ) = WL _W1x._ (7)
_· · ·_
Now let
_u¯ = arg max_ min
_u_ =1 _i_ 1,...,n _[y][i][ ⟨][x][i][, u][⟩]_
_∥_ _∥_ _∈{_ _}_

be the max-margin solution given by the hard-margin SVM and γm = max _u_ =1 ym _xm, u_ be
_∥_ _∥_ _⟨_ _⟩_
the max-margin, where m = arg mini 1,...,n _yi_ _xi, u_ . We only consider the ℓ2-norm adversarial
_∈{_ _}_ _⟨_ _⟩_
perturbations in this section, i.e., (0, ϵ) = _δ :_ _δ_ 2 _ϵ_ . The allowed Euclidean distance from an
adversarial example x[′] to its original example B _{_ _x ∥ is assumed to satisfy∥_ _≤_ _}_

_x[′]_ _x_ 2 _ϵ_ _γm._ (8)
_∥_ _−_ _∥_ _≤_ _≤_

We denote the product of all weight matrices bymodel is adversarially trained by gradient descent WΠ = WL · · · W1, which is simply a vector. Our

(t)
_Wk(t + 1) = Wk(t)_ _η(t)_ _[∂]L[˜]_ (9)
_−_ _∂Wk(t)_ _[.]_

The inner maximization of the adversarial training objective Eq. (1) can be solved exactly in this case,
where the adversarial perturbations are taken as

_WΠ(t)_
_δi(W_ (t)) = _yiϵ_ (10)
_−_ _WΠ(t)_ 2

_∥_ _∥_

for the t-th iteration of adversarial training. For any layer k, the weight matrix can be decomposed as

_Wk = UkΣkVk[T]_
where Uk and Vk are the left and right singular matrices, respectively. Furthermore, we use uk and vk
to represent the left and right singular vectors corresponding to the largest singular value, respectively.
We emphasize that the training dynamics abruptly changes due to this perturbation and one should
be careful to analyze its effects. It can be easily seen that Eq. (10) is a scale invariant adversarial
perturbations by our definition. Note that with this kind of perturbation, the adversarial data during
the training are still linearly separable, since we have required that the perturbation is smaller than
the max-margin of the original dataset described in Eq. (8). We further assume that ℓ satisfies
**Assumption 3. The loss function ℓ** _is β-smooth and |ℓ[′]| ≤_ _α._

such as logistic loss. We are now ready to explore the implicit bias of the gradient descent for
adversarial training. Inspired by the idea of studying the smoothness of loss function from Ji &
Telgarsky (2019) for standard training, here we first examine whether our adversarial training loss
Eq. (1) possesses smoothness. Let the set S(r) denote the set of weights with bounded norm

(r) = _W_ _Wk_ _F_ _r,_ _WΠ_ 2 _r, k¯_ = 1, . . ., L _,_ (11)
_S_ _∥_ _∥_ _≤_ _∥_ _∥_ _≥_

where r 1 and ¯r is a small constant to avoid trivial solution. To simplify the notation, we let
_≥_

_β(r, ¯r, ϵ) = r[3][L]L[2]_ _α + β +_ _[ϵ]_ 2α + β + _[αβ]_ _._ (12)

_r¯[L]_ _r¯[L]_

  

The following lemma provides us a first view of the overall trends of gradient descent for adversarial
training in deep linear network on linear separable dataset.


-----

**Lemma 1 (Overall trends of adversarial training for deep linear network). With adversarial pertur-**
_bation Eq. (10), under Assumption 1, Assumption 3 and the requirement Eq. (8), for the deep linear_
_network adversarially trained by gradient descent on linear separable data, the adversarial training_
_objective (1) is β(r, ¯r, ϵ)-smooth within the given set S(r) for a constant r. However, with constant_
_step size for gradient descent, the weight matrices will eventually leave the set S(r) if r remains_
_unchanged during the training_

max _for some t1 > 0._ (13)
_k∈{1,...,L}_ _[∥][W][k][(][t][1][)][∥][F][ > r]_

The first part of this lemma implies the smoothness of the adversarial training loss Eq. (1) while also
leading the weight matrices to leave the set S(r). In fact, this is saying that maxk∈{1,...,L} ∥Wk∥F is
divergent along the adversarial training trajectory since maxk∈{1,...,L} ∥Wk∥F will be larger than r
for any given r. We can keep W (t + 1) inside S(r(t)) at every step t by delicately adjusting the step
size of gradient descent and treating r as a function of the step t.

**Lemma 2 (Smoothness of the adversarial training loss). If the learning rate is taken as η(t) =**
min{1, 1/β(r, ¯r, ϵ)}, then the adversarial training loss _L[˜](W_ ) is β(r, ¯r, ϵ)-smooth where r(t + 1) is
_taken as_

_r(t)_ _if W_ (t + 1) (r(t) _µ(t)),_
_r(t + 1) =_ _∈S_ _−_
_r(t) + µ(t)_ _otherwise_


_during the training, where µ = r[L][−][1](1 + ϵ)α/β(r, ¯r, ϵ)._

1, and now we can further derive that the Frobenius norms of weight matrices for all layers areWe show the divergence of maxk∈{1,...,L} ∥Wk∥F along the adversarial training trajectory in Lemma
divergent with the smoothness of the adversarial training loss in hand, considering that the differences
between any two layers during the adversarial training are always bounded (Lemma 6). Due to this
divergence of weight norms, only the direction of the product of weight matrices is important for the
model to make predictions.

Now we present our main theorem to provide insights on the theoretical understanding of the implicit
bias of gradient descent for adversarial training of deep linear network.

**Theorem 2 (Convergence to the direction of the max-margin solution). With adversarial perturbation**
_Eq. (10), under Assumption 1, Assumption 3 and requirement (8), for gradient descent of the_
_adversarial training objective Eq. (6) with logistic loss, the singular vectors of the largest singular_
_values of adjacent layers get aligned if the learning rate is taken as that in Lemma 2:_

_k_ 1, . . ., L : _uk, vk+1_ 1 (14)
_∀_ _∈{_ _}_ _|⟨_ _⟩| →_

_as t →∞. As a result, the direction of the product of weight matrices converges to that of the_
_max-margin solution_
_WΠ_

_u.¯_ (15)
_WΠ_ 2 _→_
_∥_ _∥_

**Remark.** The alignment phenomenon for standard training has been showed in the previous work
Ji & Telgarsky (2019), here we further extend this result to the case of adversarial training, where the
training objective is different from that of the standard training due to the introduction of adversarial
perturbation dependent on the parameters of the network. Our work is also a significant generalization
of Li et al. (2020) which studied the convergence to max-margin solution of adversarial training for
logistic regression, a single-layer linear network.

Furthermore, although the direction of the solution Eq. (15) given by adversarial training does not
differ from that of standard training, i.e., the direction of ¯u, these two solutions are not same: the
first is maximizing the margin of adversarial examples while the latter is maximizing that of original
dataset—which happens to be the same one in this setting. The similarity of these two solutions
comes from the fact that the adversarial data are also linearly separable under the requirement Eq. (8),
whose max-margin solution has the same direction as that of ¯u. The above reasoning will be more
clear in the next section where we show the solution of adversarial training as a max-margin solution
of adversarial examples more explicitly.


-----

3.2 ADVERSARIAL TRAINING FOR HOMOGENEOUS DEEP NEURAL NETWORK

**Additional definitions** For simplicity, we will use exponential loss ℓ(x) = e[−][x] in this section.
For an original example xi, the margin for its adversarial example xi + δi(W ) is defined as ˜γi =
_yif_ (xi + δi(W ); W ) while for the whole dataset {(xi, yi)}i[n]=1[, the margin for their corresponding]
adversarial examples is denoted by ˜γm where m = arg minm 1,...,n _yif_ (xi + δi(W ); W ). We
_∈{_ _}_
introduce the normalized margin as ˆγi = yif (xi + δi(W[c]); _W_ ) to explore the convergence properties,
where _Wk = Wk/ ∥Wk∥F such that ∥W[c]k∥F = 1 for any layer k . Furthermore, the margin for the_
original example xi is defined to be γi = yif (xi; W ) [c]γ˜i. To relieve the headaches of picking
_≥_
suitable learning rates, we consider gradient flow, gradient descent with infinitesimal step, of the

[c]
training loss Eq. (6), which leads to


_T_
!


_∂L[˜]_

_∂Wk_


_dWk_

=
_dt_ _−_


(16)


for k ∈{1, . . ., L}. Note that the introduction of adversarial perturbations (relying on the network
parameters) abruptly perturbs the training dynamics. In this section, to conduct a comprehensive study
of adversarial training, we do not restrict the perturbation used for solving the inner maximization of
the adversarial training objective in advance while only require it to be a scale invariant adversarial
perturbation. By noting the following lemma, we can easily generalize our results to the commonly
used adversarial attacks:
**Lemma 3. Under Assumption 1, ℓ2-FGM, FGSM, ℓ2-PGD and ℓ** _-PGD perturbations for homoge-_
_∞_
_neous DNNs are all scale invariant adversarial perturbations._

The RHS of Eq. (16) is different from that of standard training due to the inner maximization. Our
key observation is the following property of adversarial training which can drastically simplify our
analysis despite of the complexity brought by the adversarial examples.
**Lemma 4. For the exponential loss, along the gradient flow trajectory of adversarial training with**
_scale invariant adversarial perturbations for homogeneous DNN f_ (x; W ), we have


_∂_ [˜]
_L_ _Wk_

_∂Wk_


=
_−_ _n[c]_


_tr_


_e[−]γ[˜]i_ _γ˜i._ (17)
_i=1_

X


In the case of deep linear network, we adopt linearly separable dataset with the requirement Eq. (8),
yielding the fact that the adversarial data are also linearly separable. In this section we do not impose
such conditions but only apply a weaker assumption in a more general scenario.
**Assumption 4. There exists a separability of adversarial examples of the dataset[3]: there exists a**
_time t0 such that yif_ (xi + δi(W (t0)); W (t0)) > 0 for all i ∈{1, . . ., n}.

Theoretically, Zhang et al. (2020); Gao et al. (2019) proved the convergence to low robust training
loss for heavily overparametrized nets. Moreover, adversarial training can typically achieve this
separability in practice, i.e., the model can fit adversarial examples of the training dataset, which
makes the above assumption a reasonable one. In Section 3.1, we obtain divergent weight norms for
adversarial training. Thanks to Lemma 4, we can further generalize it to the case of homogeneous
network without linearly separability of dataset and the requirement Eq. (8) for adversarial examples.
This divergence of model parameters is necessary for the convergence of the adversarial training loss.
To begin with, recalling our definition oftheorem regarding the overall trend for gradient flow of adversarial training as follows. ρ = _k=1_ _[ρ]k[c]_ [=][ Q]k[L]=1 _[∥][W][k][∥]F[c]_ [, we state our first main]
**Theorem 3 (Divergence of weight norms). For the adversarial training objective (6) of the binary[Q][L]**
_classification task with exponential loss and scale invariant adversarial perturbations, under Assump-_
_tion 2 and Assumption 4, the product of the Frobenius norms of weight matrices diverges to infinity_
_along the trajectory of the gradient flow_

_ρ = Ω(ln t) →∞,_

_and, as a result, the training loss_ _L[˜] converges to zero as t →∞._

3Similar assumptions are previously used in Lyu & Li (2020); Ji & Telgarsky (2020)


-----

**Remark.** The divergence of weight norms for standard training of homogeneous DNNs has been
studied by, for example, Lyu & Li (2020); Wei et al. (2019). The above theorem considers adversarial
_training, which implies that the margin for an adversarial example, ˜γi for any i ∈{1, . . ., n}, also_
goes to infinity along the adversarial training trajectory. This makes the normalized margin ˆγi
necessary to understand the convergence properties of the adversarial training solution as t →∞.
Built upon this result, we first shed some lights on the implicit bias of the gradient flow for the
adversarial training through the following theorem, which discusses the training dynamics of the
normalized margins for the adversarial examples.
**Theorem 4 (Non-decreasing adversarial margin). For adversarial training with scale invariant**
_adversarial perturbations, when the separability of adversarial examples is achieved at t0, the_
_weighted sum of the changing rates of normalized margins for the adversarial examples are non-_
_negative for all t_ _t0:_
_≥_ _n_

_γi_ _[d]γ[ˆ]i_

_t_ _t0 :_ _e[−][ρ][ˆ]_ (18)
_∀_ _≥_ _dt_

_i=1_ _[≥]_ [0;]

_there exists a sufficiently large time t1 > t0 such thatX_ _[d]dtγ[ˆ]m_ _[≥]_ [0][ if][ ∀][i][ : ˆ]γi − _γˆm = ω(_ _ρ[1]_ [)][ for all][ t][ ≥] _[t][1][.]_

The above theorem reveals a relation for the normalized margins for adversarial examples. According
to the above lemma, since ρ →∞ as t →∞, the term with weight e[−][ρ]γ[ˆ]m, roughly speaking,
dominates the LHS of (18) after some time t1 _t0. Then dγˆm/dt[4]_ will never be negative for _t_ _t1._
This directly implies that the gradient flow is actually maximizing the margin for the adversarial ≥ _∀_ _≥_
examples of the original dataset. Applying Lemma 4 and Theorem 3, in the following theorem, we
further strengthen the above observation for adversarial training for homogeneous DNNs.
**Theorem 5 (Convergence to the direction of a KKT point). Under Assumption 2 and 4, for ex-**
_ponential loss and multi-c-homogeneous DNNs, the limit point of normalized weight parameters_
_{W/∥W_ _∥2 : t ≥_ 0} of the gradient flow for the adversarial training objective Eq. (6) with scale
_invariant adversarial perturbations is along the direction of a KKT point of the constrained norm-_
_minimization problem_

1
min 2 _s.t. ˜γi_ 1 _i_ 1, . . ., n _._ (19)
_W1,...,WL_ 2 _[∥][W]_ _[∥][2]_ _≥_ _∀_ _∈{_ _}_

**Remark 1.** According to Lemma 3, the above theorem can be directly applied to ℓ2-FGM, FGSM,
_ℓ2 and ℓ_ -PGD perturbations. The requirement Eq. (8) in Section 3.1 is not required since the dataset
_∞_
in this section is no longer linearly separable. It is well-known that the norm-minimization problem
is closely related to the margin-maximization problem in the sense that
Optimization problem(19) ⇐⇒ _W1max,...,WL_ _γ[˜]m,_ _s.t. ∥W_ _∥2 = 1._

**Remark 2.** Theorem 5 is the key theorem of this section. It shows that, for adversarial training,
the gradient flow is implicitly maximizing the margin of adversarial examples. Therefore, for
perturbations with norm constraints other than ℓ2-norm, (19) is no longer an ℓ2-norm optimization
problem but a mixed-norm one due to the perturbation δ(W ) in ˜γm, which was first observed by
Li et al. (2020) for adversarial training of one-layer neural network while our results are for more
general deep non-linear homogeneous neural networks. This is one of the major differences between
adversarial training and standard training—they are formulated from solving different optimization
problem. Theorem 5 provides theoretical understandings of the folklore that adversarial training can
modify the decision boundary against adversarial examples to improve robustness.

**Remark 3.** In the context of standard training, Lyu & Li (2020) studied the properties of the normalized margin through its approximation, the smooth-normalized margin. One can try to generalize
the analysis method in Lyu & Li (2020) to adversarial training with various kinds of adversarial
perturbations by further assuming the local smoothness regarding gradients when considering nonsmooth analysis. Generalizinig our results to non-smooth case can be found in Appendix B.4. To
easily adapt to adversarial training, our new strategy in this work is to directly analyze the normalized
_margin itself for adversarial examples instead and utilizes the alignment phenomena observed by_
Ji & Telgarsky (2020). Most importantly, what we seek to push forward in the current work are the
theoretical understandings of the adversarial training for DNNs through its implicit bias.

4Strictly speaking, this is not well-defined since m may not be unique. However, we can use this definition
in an approximately correct way for t sufficiently large.


-----

0.01

0.00

−0.01

−0.02

−0.03

−0.04

−0.05


0.02

0.00

−0.02

−0.04

−0.06

−0.08

−0.10

10000 12500 15000 17500 20000 22500 25000 27500 30000

AT wi h FGSM per urba iona

epochs


AT with PGD perturbation
standard training


|AT with PGD perturbation standard training|Col2|Col3|
|---|---|---|
|||AT with PGD perturbation standard training|
|0|50 100 150 200 250||


(a) FGSM adversarial examples (b) ℓ -PGD adversarial examples (c) FGSM adversarial examples

_∞_

Figure 1: Adversarial training for the 3-layer neural network on MNIST. Adversarial normalized
margin for (a) FGSM adversarial examples; (b) ℓ -PGD adversarial examples; (c) FGSM adversarial
_∞_
examples after 10000 epochs.

**Trade-off between robustness and accuracy.** More significantly, Theorem 5 also implies that
adversarial training leads to the trade-off between robustness and accuracy as in Zhang et al. (2019).
This can be seen as follows. Let f (·, W ) denote the classifier obtained from adversarial training.
Then for a clean training example (x, y) with y = 1 for simplicity, we have yf (x + δ(W ); W ) =
_f_ (x + δ(W ); W ) > 0 since f (x; W ) is a max-margin classifier over adversarial data. This means
that, for any clean test example (x[′], y[′]) around x in the sense that x[′] = x + τ with _τ_ _p_ _ϵ, we will_
have _∥_ _∥_ _≤_
_f_ (x[′], W ) = f (x + τ ; W ) ≥ _f_ (x + δ(W ); W ) > 0 (20)
due to the definition of δ(W ) when y = 1: _δ(W_ ) = maxv (x,ϵ) ℓ(yf (x + v; W )) =
_∈B_
minv (x,ϵ) f (x + v; W ) for ℓ[′] _< 0. Therefore, as a result of forcing our model to correctly_
_∈B_
classify the adversarial example x + δ(W ), it will classify any such x[′] = x + τ as having labels 1
even there may be some of them having true labels -1, which leads to the drop of accuracy at the cost
of increasing robustness. Besides, since adversarial training maximizes the margin of the specific
adversarial perturbation used for training, the trained model performs worse on other perturbations.

4 NUMERICAL EXPERIMENTS


In this section, we conduct numerical experiments on MNIST dataset to support our claims. We
adversarially trained a 3-layer neural network using SGD with constant learning rate and batch-size 80.
The model has the architecture of input layer-1024-ReLU-64-ReLU-output layer. We present results
for adversarial training with: (1) FGSM perturbations with ϵ = 16/255; (2) ℓ -PGD perturbations,
_∞_
where the PGD is ran for 5 steps with step size 6/255 and ϵ = 16/255. As a comparison, we
also standardly trained a model with the same architecture to evaluate the normalized margin for
adversarial examples by attacking it with FGSM and PGD during its training process.

Fig. 1(a) shows that the normalized margin for FGSM adversarial examples keeps increasing during
the process of adversarial training with FGSM perturbations. Meanwhile, the standardly trained
model has lower normalized margins for FGSM adversarial examples. Similar results exist for
_ℓ_ -PGD adversarial examples as showed by Fig. 1(b). To see that adversarial normalized keeps
_∞_
increasing more clearly, we also plot the adversarial training with FGSM perturbations after 10000
epochs in Fig. 1(c). More detailed experiments are in Appendix C. These empirical findings support
our claims that adversarial training implicitly maximizes the margin of adversarial examples.

5 CONCLUSION


In this paper, we have studied the implicit bias of adversarial training for the deep linear networks and,
more generally, the homogeneous DNNs. We proved that adversarial training with scale invariant
adversarial perturbations implicitly performs margin-maximization for adversarial data during its
training process. Intuitively, such implicit bias strongly implies that adversarial training encourages
the neural networks to utilize adversarial examples more to improve its robustness. It will be an
interesting future direction to improve the effect of adversarial training by promoting it to enlarge
the margins of adversarial examples more effectively. It is also possible to design new defense
mechanisms by explicitly requiring the standard training to have the bias of maximizing the margin
of adversarial examples. We will leave these potential benefits of our theoretical analysis as future
work.


-----

ACKNOWLEDGEMENT

This project is supported by Beijing Nova Program (No. 202072) from Beijing Municipal Science
Technology Commission.

REFERENCES

Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In arXiv:1802.06509, 2018.

A. Athalye, N. Carlini, and D. Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In International Conference on Machine learning(ICML),
2018.

A. Banburski, Q. Liao, B. Miranda, T. Poggio, L. Rosasco, F. D. L. Torre, and J. Hidary. Theory iii:
Dynamics and generalization in deep networks. In arXiv:1903.04991, 2019.

Frank H Clarke. Generalized gradients and applications. 1975.

Frank H Clarke. Optimization and nonsmooth analysis. 1983.

R. Collobert and J. Weston. A unified architecture for natural language processing: Deep neural
networks with multitask learning. In Proceedings of the 25th international conference on Machine
_learning(ICML), 2008._

Damek Davis, Dmitriy Drusvyatskiy, Sham Kakade, and Jason D. Lee. Stochastic subgradient
method converges on tame functions. In arXiv:1804.07795, 2018.

S. S. Du, W. Hu, and J. D. Lee. Algorithmic regularization in learning deep homogeneous models:
Layers are automatically balanced. In arXiv: 1806.00900, 2018.

J. Dutta, K. Deb, R. Tulshyan, and R. Arora. Approximate kkt points and a proximity measure for
termination. Journal of Global Optimization, pp. 1463–1499, 2013.

Fartash Faghri, Sven Gowal, Cristina Vasconcelos, David J. Fleet, Fabian Pedregosa, and Nicolas Le Roux. Bridging the gap between adversarial robustness and optimization bias. In arXiv:
_2102.08868, 2021._

R. Gao, T. Cai, H. Li, L. Wang, C. Hsieh, and J. D. Lee. Convergence of adversarial training in
overparametrized neural networks. In arXiv:1906.07916, 2019.

I. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In
_International Conference on Learning Representations (ICLR), 2015._

Suriya Gunasekar, Jason D. Lee, Daniel Soudry, and Nathan Srebro. Implicit bias of gradient descent
on linear convolutional networks. In arXiv: 1806.00468, 2018.

C. Guo, M. Rana, M. Cisse, and L. van der Maaten. Countering adversarial images using input
transformations. In International Conference on Learning Representations(ICLR), 2018.

K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In International conference on computer vision (ICCV),
2015.

Z. Ji and M. Telgarsky. Risk and parameter convergence of logistic regression. In arXiv:1803.07300,
2018.

Z. Ji and M. Telgarsky. Gradient descent aligns the layers of deep linear networks. In International
_Conference on Learning Representations(ICLR), 2019._

Z. Ji and M. Telgarsky. Directional convergence and alignment in deep learning. In arXiv:2006.06657,
2020.


-----

A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenent classification with deep convolutional neural
networks. In Advances in Neural Information Processing Systems(NeurIPS), 2012.

Y. Li, E. X.Fang, H. Xu, and T. Zhao. Inductive bias of gradient descent based adversarial training on
separable data. In International Conference on Machine Learning(ICLR), 2020.

K. Lyu and J. Li. Gradient descent maximizes the margin of homogeneous neural networks. In
_International Conference on Learning Representations(ICLR), 2020._

A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant
to adversarial attacks. In International Conference on Learning Representations(ICLR), 2018.

T. Miyato, A. M. Dai, and I. Goodfellow. Adversarialtraining methods for semi-supervised text
classification. In arXiv:1605.07725, 2016.

M. S. Nacson, S. Gunasekar, J. Lee, N. Srebro, and D. Soudry. Lexicographic and depth-sensitive margins in homogeneous and non-homogeneous deep models. In Proceedings of the 36th International
_Conference on Machine Learning, 2019a._

M. S. Nacson, N. Srebro, and D. Soudry. Stochastic gradient descent on separable data: Exact
convergence with a fixed learning rate. In Proceedings of Machine Learning Research, 2019b.

A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks are easily fooled: High confidence
predictions for unrecognizable images. In Conference on computer vision and pattern recognition
_(CVPR), 2015._

Ohad Shamir. Gradient methods never overfit on separable data. In Journal of Machine Learning
_Research, 2021._

D. Soudry, E. Hoffer, M. S. Nacson, S. Gunasekar, and N. Srebro. The implicit bias of gradient
descent on separable data. Journal of Machine Learning Research, pp. 1–57, 2018.

C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing
properties of neural networks. In International Conference on Learning Representations (ICLR),
2014.

C. Wei, J. D Lee, Q. Liu, and T. Ma. Regularization matters: Generalization and optimization of neural
nets v.s. their induced kernel. In Advances in Neural Information Processing Systems(NeurIPS),
2019.

E. Wong, L. Rice, and J. Z. Kolter. Fast is better than free: Revisiting adversarial training. In
_International Conference on Learning Representations (ICLR), 2020._

C. Xie, J. Wang, Z. Zhang, Z. Ren, and A. Yuille. Mitigating adversarial effects through randomization.
In International Conference on Learning Representations (ICLR), 2018.

Yaodong Yu, Zitong Yang, Edgar Dobriban, Jacob Steinhardt, and Yi Ma. Understanding generalization in adversarial training via the bias-variance decomposition. In arXiv: 2103.09947,
2021.

H. Zhang, Y. Yu, J. Jiao, E. P. Xing, L. E. Ghaoui, and M. I. Jordan. Theoretically principled trade-off
between robustness and accuracy. In International Conference on Machine Learning(ICLR), 2019.

Y. Zhang, O. Plevrakis, S. S. Du, X. Li, Z. Song, and S. Arora. Over-parameterized adversarial
training: An analysis overcoming the curse of dimensionality. In arXiv: 2002.06668, 2020.

A PROOFS OF SECTION 3.1

In Section A.1, we discuss the smoothness of the adversarial training loss. In Section A.2, we prove
the alignment phenomenon of adversarial training for deep linear networks. We make the common
assumption that _L[˜](W_ (0)) ≤ _L[˜](0) = ℓ(0) and ∇L[˜](W_ (0)) ̸= 0 to start adversarial training.


-----

A.1 SMOOTHNESS OF THE ADVERSARIAL TRAINING LOSS

A.1.1 PROOF OF LEMMA 1

_Proof. We first prove that the adversarial training loss for the deep linear network is smooth within_
_S(r). For any W, V ∈S(r), let δW and δV denote the adversarial perturbations of the network_
_f_ (x; W ) and f (x; V ), respectively. Then we have, by taking derivatives of L w.r.t W and using that
_∥A∥F + ∥B∥F ≥∥A −_ _B∥F,_


_∂L[˜]_ _L_

_∂W1_ _−_ _∂V[∂]_ [˜]1


_n_

_W2[T]_ _L_ _[x]i[T]_ _[y][i]ℓ[˜][′]i[(][W][Π][)][ −]_ _[V][ T]2_ _L_ _[x]i[T]_ _[y][i]ℓ[˜][′]i[(][V][Π][)]_

_≤_ _n[1]_ _i=1_ _[· · ·][ W][ T]_ _[· · ·][ V][ T]_

X

(a)

_n_

| _∂_ _WΠ_ {z

+ _[ϵ]_ _∥_ _∥_ _ℓ˜[′]i[(][W][Π][)][ −]_ _[∂]_ _[∥][V][Π][∥]_ _ℓ˜[′]i[(][V][Π][)]_ _,_

_n_ _i=1_ _∂W1_ _∂V1_ _F_

X

(b)
| {z }


(21)


where the term (b) abruptly changes the training dynamics. For simplicity, we use x, y and _ℓ[˜][′]_ without
the subscript i because the analysis can be done similarly for any i ∈{1, . . ., n}. We begin with the
term (a) of Eq. (21):

(a) ≤ _W2[T]_ _[· · ·][ W][ T]L_ _[x][T][ y]ℓ[˜][′](WΠ) −_ _V2[T]_ _[W][ T]3_ _[· · ·][ W][ T]L_ _[x][T][ y]ℓ[˜][′](WΠ)_

+ _V2[T]_ _[W][ T]3_ _[· · ·][ W][ T]L_ _[x][T][ y]ℓ[˜][′](WΠ) −_ _V2[T]_ _[· · ·][ V][ T]L_ _[x][T][ y]ℓ[˜][′](VΠ)_

_≤∥W2 −_ _V2∥_ _W3[T]_ _[· · ·][ W][ T]L_ _[x][T][ y]ℓ[˜][′](WΠ)_

+ ∥V2∥ _W3[T]_ _[· · ·][ W][ T]L_ _[x][T][ y]ℓ[˜][′](WΠ) −_ _V3[T]_ _[· · ·][ V][ T]L_ _[x][T][ y]ℓ[˜][′](VΠ)_

c
_≤∥W2 −_ _V2∥_ _r[L][−][2]α + r_ _W3[T]_ _[· · ·][ W][ T]L_ _[x][T][ y]ℓ[˜][′](WΠ) −_ _V3[T]_ _[· · ·][ V][ T]L_ _[x][T][ y]ℓ[˜][′](VΠ)_

d _L_

_r[L][−][2]α_ _Wk_ _Vk_ + r[L][−][2] _VL_ _ℓ[′](WΠ)_ _ℓ[′](VΠ)_

_≤_ _k=2_ _∥_ _−_ _∥_ _∥_ _∥_ _−_ [˜]

X

[˜]


e
_≤_ _r[L][−][2]α(L −_ 1) + βLr[2][L][−][2][] _∥W −_ _V ∥_ _,_ (22)
 

where c follows from _x_ 1, _Wk_ _r and ℓ[′]_ _α; repeating the procedure of c until k = L_
_∥_ _∥≤_ _∥_ _∥≤_ _≤_
gives us d; e is because _Wk_ _Vk_ _W_ _V_ for any k and
_∥_ _−_ _∥≤∥_ _−_ _∥_

_ℓ[′](WΠ)_ _ℓ[′](VΠ)_ _β_ _WΠ_ _VΠ_ _βLr[L][−][1]_ _W_ _V_ (23)

[˜] _−_ [˜] _≤_ _∥_ _−_ _∥≤_ _∥_ _−_ _∥_

considering that ℓ is a β-smooth function. It now remains us to bound the term (b) of Eq. (21). Note
that


_∂_ _WΠ_ 2 2 _L_ _[W][L][ · · ·][ W][1]_
_∥_ _∥_ = _[W][ T]_ _[· · ·][ W][ T]_ _,_ (24)

_∂W1_ _WΠ_ 2

_∥_ _∥_


-----

then we have

(b) _ϵ_ _W2[T]_ _[· · ·][ W][ T]L_ _[W][L][ · · ·][ W][1]_ _ℓ˜[′](WΠ)_ 2 _[W][ T]3_ _[· · ·][ W][ T]L_ _[W][L][ · · ·][ W][1]_ _ℓ˜[′](WΠ)_
_≤_ _WΠ_ _−_ _[V][ T]_ _WΠ_

_∥_ _∥_ _∥_ _∥_

+ ϵ _V2[T]_ _[W][ T]3_ _[· · ·][ W][ T]L_ _[W][L][ · · ·][ W][1]_ _ℓ˜[′](WΠ)_ 2 _[V][ T]3_ _[· · ·][ V][ T]L_ _[V][L][ · · ·][ V][1]_ _ℓ˜[′](VΠ)_

_WΠ_ _−_ _[V][ T]_ _VΠ_
_∥_ _∥_ _∥_ _∥_

c

_W2_ _V2_

_≤_ _[ϵαr]r¯[2][L][L][−][2]_ _∥_ _−_ _∥_

+ ϵr _W3[T]_ _[· · ·][ W][ T]L_ _[W][L][ · · ·][ W][1]_ _ℓ˜[′](WΠ)_ 3 _[· · ·][ V][ T]L_ _[V][L][ · · ·][ V][1]_ _ℓ˜[′](VΠ)_

_WΠ_ _−_ _[V][ T]_ _VΠ_
_∥_ _∥_ _∥_ _∥_

d _ℓ˜[′](WΠ)_ _ℓ˜[′](VΠ)_
(2L 1) _[ϵαr][2][L][−][2]_ _W_ _V_ + ϵr[2][L][−][2]
_≤_ _−_ _r¯[L]_ _∥_ _−_ _∥_ _WΠ_ _VΠ_

_∥_ _∥_ _[−]_ _∥_ _∥_

e
(2L 1) _[ϵαr][2][L][−][2]_ _W_ _V_ + _[ϵr][2][L][−][2]_ _ℓ[′](WΠ)_ _ℓ[′](VΠ)_ + _[ϵαr][2][L][−][2]_ _WΠ_ _VΠ_
_≤_ _−_ _r¯[L]_ _∥_ _−_ _∥_ _r¯[L]_ _−_ [˜] _r¯[2][L]_ _∥_ _−_ _∥_

f

(2L 1)α + βLr[L][−][1] + _[αβLr][˜][L][−][1]_ _W_ _V_ _,_ (25)

_≤_ _[ϵr][2]r¯[L][L][−][2]_ _−_ _r¯[L]_ _∥_ _−_ _∥_

 


where c follows from _Wk_ _r and_ _ℓ[˜][′]_ _α; repeating c for 2L_ 1 times gives d; e follows from
_∥_ _∥≤_ _≤_ _−_
_WΠ_ _VΠ_ _WΠ_ _VΠ_ ; invoking Eq. (23) gives us f. Now we can bound Eq. (21) by
_∥∥combing the above results∥−∥_ _∥∥≤∥_ _−_ _∥_
_∂L[˜]_ _L_

_∂W1_ _−_ _∂V[∂]_ [˜]1

_F_

_r[L][−][2]α(L_ 1) + βr[2][L][−][2]L + _[ϵr][2][L][−][2]_ (2L 1)α + βLr[L][−][1] + _[αβLr][L][−][1]_ _W_ _V_
_≤_ _−_ _r¯[L]_ _−_ _r¯[L]_ _∥_ _−_ _∥_
  

_r[3][L]L_ _α + β +_ _[ϵ]_ 2α + β + _[αβ]_ _W_ _V_ _,_ (26)
_≤_ _r¯[L]_ _r¯[L]_ _∥_ _−_ _∥_
  

where the last inequality is because we choose r ≥ 1. The above analysis is for the first layer. Now
we can apply the same technique to other layers, which finally gives us

(W ) (V ) _r[3][L]L[2]_ _α + β +_ _[ϵ]_ 2α + β + _[αβ]_ _W_ _V_ _,_ (27)
_∥∇L[˜]_ _−∇L[˜]_ _∥≤_ _r¯[L]_ _r¯[L]_ _∥_ _−_ _∥_
  

thus _L[˜] is β(r, ¯r, ϵ)-smooth inside S(r) where_

_β(r, ¯r, ϵ) = r[3][L]L[2]_ _α + β +_ _[ϵ]_ 2α + β + _[αβ]_ _._ (28)

_r¯[L]_ _r¯[L]_

  

We now prove the second statement of Lemma 1, which says that W will leave S(r) with constant r
and learning rate η. Let W (t), W (t + 1) ∈S(r) and η(t) = min{1, 1/β(r, ¯r, ϵ)}.

**Lemma 5 (Adapted from Lemma 6 in Ji & Telgarsky (2019)). Under Assumption 1 and Assumption**
_3, suppose gradient descent for adversarial training is run with a constant step size 1/β(r, ϵ, ¯r). Then_
_there exists a time t when maxk ∥Wk(t)∥F > r._


Eq. (27) states that _L[˜] is β(r, ¯r, ϵ)-smooth, therefore_

_r, ϵ)η(t)[2]_
˜(W (t + 1)) (W (t)) (W (t)), _η(t) [˜](W_ (t)) + _[β][(][r,][ ¯]_
_L_ _−_ _L[˜]_ _≤_ _∇L[˜]_ _−_ _L_ 2
D E


_∥∇L[˜](W_ (t))∥[2]


= (W (t)) _,_ (29)
_−_ _[η][(]2[t][)]_ _[∥∇]L[˜]_ _∥[2]_

which also implies that _L[˜](W_ (t)) never increases during the training

˜(W (t + 1)) (W (t)) (W (t)) _._ (30)
_L_ _≤_ _L[˜]_ _−_ _[η][(]2[t][)]_ _[∥∇]L[˜]_ _∥[2]_

By repeatedly applying Lemma 5 when the learning rate is constant, we know W will eventually
leave S(r), which also implies that maxk ∥Wk∥F is unbounded in this case.


-----

A.1.2 PROOF OF LEMMA 2

_Proof. For any step t with the learning rate in the Lemma 2, suppose that W_ (t) ∈S(r(t) − _µ(t)),_
we then have


_∂L[˜]_

_∂Wk(t)_


_∥Wk(t + 1)∥F ≤∥Wk(t)∥F + η(t)_


_≤∥Wk(t)∥F + µ(t),_


where the last inequality follows from
_∂L[˜]_ _Wk+1_ _WL[T]_ _[x][T][ W][ T]1_ _k_ 1[y]ℓ[˜][′](WΠ) + ϵ _∂_ _∥WΠ∥2_ _ℓ˜[′](WΠ)_

_∂Wk(t)_ _[≤]_ _· · ·_ _[· · ·][ W][ T]−_ _∂Wk_

(1 + _[ϵ]_ (31)
_≤_ _r¯[L][ )][αr][2][L][.]_

Therefore ∥Wk(t + 1)∥F will always stay in S(r(t)) and the smoothness is preserved. If
_∥Wk(t + 1)∥F is still less than r(t) −_ _µ(t), then we keep r(t + 1) = r(t) while we increase_
_r(t + 1) to_
_r(t + 1) = r(t) + µ(t)_

if r(t) _µ(t)_ _Wk(t + 1)_ _F_ _r(t) such that_ _Wk(t + 1)_ _F_ _r(t)_ _r(t + 1)_ _µ(t + 1),_
which means that − _≤∥ W_ (t + 2) will stay inside∥ _≤_ _S(r(t + 1)) ∥_ . The overall smoothness can then be proved∥ _≤_ _≤_ _−_
by induction.

A.2 ALIGNMENT PHENOMENON

We start with the following lemma for the adversarial training to prove the alignment phenomenon[5]:

**Lemma 6 (Bounded differences between weight norms). The difference of Frobenius norms for any**
_two layers is bounded during the adversarial training process_
_Wk(t)_ _F_ _F_ 2 ˜(W (0)) _Wk(0)_ _F_ _F_ _,_ _k, j_ 1, . . ., L _._
_∥_ _∥[2]_ _[−∥][W][j][(][t][)][∥][2]_ _−_ _L_ _≤_ _∥_ _∥[2]_ _[−∥][W][j][(0)][∥][2]_ _∀_ _∈{_ _}_

_Proof. If there is not adversarial perturbation, then we can easily obtain_

_∂_
_Wk[T]+1_ _∂WLk+1_ = _∂W[∂][L]k_ _Wk[T]_ (32)


by conducting some algebra, where L stands for the empirical loss without adversarial perturbation.
Then invoking Eq. (24) for k by noting that tr (A) = tr _A[T][ ]_ and tr (ABC) = tr (CAB), we have

_yϵℓ[˜](WΠ)Wk[T]+1_ _∂_ _∥WΠ∥2_ = _[yϵ]ℓ[˜](WΠ)_ _Wk[T]+1 _ _L_ _[W][L]_ 1 _k_

_∂Wk+1_ _WΠ_ 2 _[· · ·][ W][ T]_ _[· · ·][ W][1][W][ T]_ _[· · ·][ W][ T]_

_∥_ _∥_

= yϵℓ[˜](WΠ) _[∂]_ _[∥][W][Π][∥][2]_ _Wk[T]_ _[.]_ (33)

_∂Wk_

Combining Eq. (32), for the adversarial empirical loss, we have


_∂_ [˜]
_Wk[T]+1_ _L_ = _[∂]L[˜]_ _Wk[T]_ _[,]_ (34)

_∂Wk+1_ _∂Wk_

which will give us the following relation


_T_

_∂_ [˜] _∂_ [˜]
_Wk[T]+1[(][t][ + 1)][W][k][+1][(][t][ + 1) =][ W][ T]k+1[(][t][)][W][k][+1][(][t][) +][ η][(][t][)][2]_ _L_ _L_

_∂Wk+1(t)_ ! _∂Wk+1(t)_ !

_T_

_∂_ [˜] _∂_ [˜]

_η(t)_ _Wk[T]+1[(][t][)]_ _L_ + _L_ _Wk+1(t)_ _,_
_−_  _∂Wk+1(t)_ ! _∂Wk+1(t)_ ! 

 

5This property for standard training has previously been studied by Ji & Telgarsky (2019); Arora et al. (2018).


-----

which can be easily seen by writing out the gradient descent update explicitly. One can derive a
similar relation for Wk(t + 1)Wk[T] [(][t][ + 1)][. Combing Eq. (32), we have]

_T_

_∂_ [˜] _∂_ [˜]
_Wk[T]+1[(][t][ + 1)][W][k][+1][(][t][ + 1)][ −]_ _[W][ T]k+1[(][t][)][W][k][+1][(][t][)][ −]_ _[η][(][t][)][2]_ _L_ _L_

_∂Wk+1(t)_ ! _∂Wk+1(t)_ !

_T_

_∂_ [˜] _∂_ [˜]
= Wk(t + 1)Wk[T] [(][t][ + 1)][ −] _[W][k][(][t][)][W][ T]k_ [(][t][)][ −] _[η][(][t][)][2]_ _L_ _L_ _._ (35)

_∂Wk(t)_ ! _∂Wk(t)_ !

Summing up the above equation from 0 to t will give us

_Wk[T]+1[(][t][)][W][k][+1][(][t][)][ −]_ _[W][ T]k+1[(0)][W][k][+1][(0)][ −]_ _[A][k][+1][(][t][) =][ W][k][(][t][)][W][ T]k_ [(][t][)][ −] _[W][k][(0)][W][ T]k_ [(0)][ −] _[B][k][(][t][)]_
(36)

where symmetric matrices Ak and Bk are defined by


_t−1_ _∂_ [˜] _T_ _∂_ [˜]

_η(τ_ )[2] _L_ _L_
_τ_ =0 _∂Wk(τ_ ) ! _∂Wk(τ_ )

X


_Ak(t) =_

_Bk(t) =_


and

Also notice that

where


_T_

_∂L[˜]_

_∂Wk(τ_ ) !


_t−1_ _∂_ [˜]

_η(τ_ )[2] _L_

_∂Wk(τ_ )

_τ_ =0

X


tr (Ak(t)) = tr (Bk(t)) (37)


![]




_T_
_∂L[˜]_
! _∂Wk[′]_ (τ )


_t−1_ _∂_ [˜]

_η(τ_ )[2] _L_

_∂Wk[′]_ (τ )

_τ_ =0

X


tr (Ak(t))
_≤_


tr
_k[′]=1_

X


_t−1_ _∂_ [˜]

_η(τ_ )[2] _L_
_τ_ =0 _∂Wk′_ (τ )

X


_k[′]=1_


_t_ 1

a _−_ 2
_≤_ _η(τ_ ) _∇L[˜](W_ (τ ))

_τ_ =0

X

b _t−1_
_≤_ 2 _η(τ_ ) _L˜(W_ (τ )) − _L[˜](W_ (τ + 1))

_τ_ =0

X h i

= 2 c _L[˜](W_ (0)) − 2 L[˜](W (t)) ≤ 2 L[˜](W (0)), (38)

whereL˜ > 0 a give us is because c. For any two layers η < 1; b is due to Eq. k < j, taking trace of both sides of (36) and summing from (29); summing up from τ = 0 to t − 1 and noting that k to
_j, we have that the differences between the Frobenius norms for any two layers_

_Wk(t)_ _F_ _F_ = tr (Aj(t)) + _Wj(0)_ _F_ _F_
_∥_ _∥[2]_ _[−∥][W][j][(][t][)][∥][2]_ _∥_ _∥[2]_ _[−]_ [tr][ (][B][k][(][t][))][ −∥][W][k][(0)][∥][2]

2 [˜](W (0)) + _Wj(0)_ _F_ _F_
_≤_ _L_ _∥_ _∥[2]_ _[−∥][W][k][(0)][∥][2]_

for allare always bounded. Since k. max1≤k≤L ∥Wk∥F →∞, the above result implies that ∥Wk∥F →∞


A.2.1 PROOF OF THEOREM 2

Intuitively, Wk can be decomposed as

_Wk(t) = Uk(t)Σk(t)Vk[T]_ [(][t][)][.]


-----

For Wk, denote the first singular value, the corresponding left singular vector and right singular
vector byto Eq. (36), this will lead to σk, uk and vk. Since σk →∞, the initialization becomes negligible as t →∞. According

_Uk(t)Σk(t)Σ[T]k_ [(][t][)][U][ T]k [(][t][)][ →] _[V][k][+1][(][t][)Σ]k[T]+1[(][t][)Σ][k][+1][(][t][)][V][ T]k+1[(][t][)]_

asrank t →∞ 1 because they get aligned with. On the other hand, since W WLLW andL[T] [has rank]WWkk _F_ [ 1][ for][ W]k [.][L][ being a row vector, all layers have]

_∥_ _∥_

_[→]_ _[u][k][v][T]_

To further elaborate the above reasoning, we provide a more exact proof starting with the definition
of the alignment phenomenon.

**Definition 2 (Alignment phenomenon for deep linear networks.). For deep linear network f** (x; W ) =
_WL_ _W1x, the alignment phenomenon is defined as_
_· · ·_

_k_ 1, . . ., k : _uk, vk+1_ 1 (39)
_∀_ _∈{_ _}_ _| ⟨_ _⟩| →_

_andright singular vectors correspond to the largest singular value of Wk/∥Wk∥F →_ _ukvk[T]_ _[as][ t][ →∞]_ _[along the training trajectory, where] Wk._ _[ u][k][, v][k][ are the left and]_

The proof for the alignment phenomenon of adversarially trained deep linear networks in our setting
is as follows [6].

_Proof. Note that_

_⟨uk, vk+1⟩[2]_ _σk[2]+1_ [=][ u]k[T] _[W][ T]k+1[W][k][+1][u][k]_ [+][ u][T]k [(][v][k][+1][σ]k[2]+1[v]k[T]+1 _[−]_ _[W][ T]k+1[W][k][+1][)][u][k][,]_ (40)

we have
_u[T]k_ _[W][ T]k+1[W][k][+1][u][k]_ 2 _F_

+ _[−∥][W][k][+1][∥][2]_ _uk, vk+1_ 1 (41)
_σk[2]+1_ _[∥][W][k][+1][∥][2]σk[2]+1_ _≤⟨_ _⟩[2]_ _≤_

by utilizing the definition of matrix norm and


_∥Wk+1∥F[2]_ _[≥]_ _[u]k[T]_ _[W][ T]k+1[W][k][+1][u][k][.]_ (42)


According to Eq. (36), let


Γk = Wk[T]+1[(0)][W][k][+1][(0)][ −] _[W][k][(0)][W][k][(0)][T][ +][ A][k][+1]_ (43)

_[−]_ _[B][k][,]_

and replace Wk[T]+1[W][k][+1][ with][ Γ][k][ +][ W][k][W][ T]k [in Eq. (41), we have]

_σk[2]_ _k_ [Γ][k][u][k][ +][ ∥][W][k][+1][∥]2[2] _F_
_uk, vk+1_ + _[u][T]_ _[−∥][W][k][+1][∥][2]_ _._ (44)
_⟨_ _⟩[2]_ _≥_ _σk[2]+1_ _σk[2]+1_


To prove the alignment phenomenon, we now bound the RHS of Eq. (44) with 1 − _α where α →_ 0
when t →∞. We are going to bound 3 terms, namely u[T]k [Γ][k][u][k][,][ ∥][W][k][∥]2[2] _[−∥][W][k][∥]F[2]_ [and][ σ]k[2][/σ]k[2]+1[.]

1. Bound u[T]k [Γ][k][u][k][.]

_u[T]k_ [Γ][k][u][k] _k_ _[W][k][(0)][W][k][(0)][T][ u][k]_ _k_ _[B][k][u][k]_

_[≥−][u][T]_ _[−]_ _[u][T]_
_Wk(0)_ 2
_≥−∥_ _∥[2]_ _[−]_ [tr][ (][B][k][)]

_Wk(0)_ 2 (W (0)), (45)
_≥−∥_ _∥[2]_ _[−]_ [2 ˜]L

where we used Eq. (38) in the third inequality.


2. Bound ∥Wk∥2[2] _[−∥][W][k][∥]F[2]_ [. According to the definition of singular values, we have]

_Wk_ 2 [=][ σ]k[2] _k+1[W][k][W][ T]k_ _[v][k][+1]_
_∥_ _∥[2]_ _[≥]_ _[v][T]_
= vk[T]+1[(][W][ T]k+1[W][k][+1]

_[−]_ [Γ][k][)][v][k][+1]
_σk[2]+1_ _k+1[∆][W][k][(0)][v][k][+1]_ (46)
_≥_ _[−]_ _[v][T]_ _[−]_ [tr][ (][B][k][)]

6Some parts of the proof is inspired by Ji & Telgarsky (2019)


-----

where ∆Wk(0) = Wk(0)Wk(0)[T] _Wk[T]+1[(0)][W][k][+1][(0)][ and we used that][ tr][ (][B][k][)][ ≥∥][B][k][∥][2]_
_−_
in the second inequality. On the other hand, by taking trace of both sides of Eq. (36), we
have

_∥Wk∥F[2]_ [=][ ∥][W][k][+1][∥]F[2] _[−∥][W][k][+1][(0)][∥]F[2]_ [+][ ∥][W][k][(0)][∥]F[2] _[−]_ [tr][ (][A][k][+1][) +][ tr][ (][B][k][)][ .] (47)

Combined with Eq. (46), we have

_Wk_ 2 _F_ 2 _F_
_∥_ _∥[2]_ _[−∥][W][k][∥][2]_ _[≥∥][W][k][+1][∥][2]_ _[−∥][W][k][+1][∥][2]_
+ (∥Wk+1(0)∥F[2] _[−∥][W][k][(0)][∥]F[2]_ [)][ −] _[v]k[T]+1[∆][W]_ [(0)][v][k][+1]
+ (tr (Ak+1) tr (Bk)) tr (Bk)
_−_ _−_

a
_Wk+2_ 2 _F_
_≥∥_ _∥[2]_ _[−∥][W][k][+2][∥][2]_
+ (∥Wk+2(0)∥F[2] _[−∥][W][k][(0)][∥]F[2]_ [)][ −] [(][v]k[T]+1[∆][W][k][(0)][v][k][+1] [+][ v]k[T]+2[∆][W][k][+1][(0)][v][k][+2][)]

+ (tr (Ak+2) tr (Bk)) (tr (Bk) + tr (Bk+1))
_−_ _−_
_. . ._


_L−1_

_vk[T][′]+1[∆][W][k][′]_ [(0)][v][k][′][+1]
_k[′]=k_ _[−]_

X


_L−1_

tr (Bk′ )
_k[′]=k_

X


_WL(0)_ _F_ _F_ _vk[T][′]+1[∆][W][k][′]_ [(0)][v][k][′][+1] tr (Bk′ )
_≥∥_ _∥[2]_ _[−∥][W][k][(0)][∥][2]_ _[−]_ _k[′]=k_ _[−]_ _k[′]=k_

X X

+ tr (AL) tr (Bk)
_−_

c
_≥_ _M −_ 2LL[˜](W (0)), (48)

where a follows from Eq. (37); b follows from summing a from k to L − 1 and that
_∥WL∥2 = ∥WL∥F ; c is because Eq. (38) and that_

_L−1_

_M = min_ _WL(0)_ _F_ _F_ _vk[T][′]+1[∆][W][k][′]_ [(0)][v][k][′][+1] (49)
_∥_ _∥[2]_ _[−∥][W][k][(0)][∥][2]_ _[−]_ _k[′]=k_

X


is finite.

3. Bound σk[2][/σ]k[2]+1[. Eq. (46) gives us that]


_σk[2]_ _k+1[∆][W][k][(0)][v][k][+1][ −]_ [tr][ (][B][k][)] _k+1[∆][W][k][(0)][v][k][+1][ + 2 ˜](W_ (0))

1 1 _L_ _, (50)_
_σk[2]+1_ _≥_ _−_ _[v][T]_ _σk[2]+1_ _≥_ _−_ _[v][T]_ _σk[2]+1_

where vk[T]+1[∆][W][k][(0)][v][k][+1][ + 2 ˜]L(W (0)) is finite.

Putting the bounds obtained from the above 1, 2 and 3 back to Eq. (44) will give us


_uk, vk+1_ 1 2 [+ 2(][L][ + 2) ˜]L(W (0)) − _M + vk[T]+1[∆][W][k][(0)][v][k][+1]_ _._ (51)
_⟨_ _⟩[2]_ _≥_ _−_ _[∥][W][k][(0)][∥][2]_ _σk[2]+1_

Considering that σk and the numerator is finite, we thus conclude that the RHS of Eq. (51) will
converge to 1. Therefore, we have that →∞

_uk, vk+1_ 1 (52)
_⟨_ _⟩[2]_ _→_

as t . Furthermore, since _Wk+1_ _F_ 2 [is finite (see Eq.][ (48)][) and][ σ][k]
_Wk →∞2[/][∥][W][k][∥][2]F_ _∥WWkk_ _F_ _∥[2]_ _[−∥][W]k_ [because other singular values are negligible. We][k][+1][∥][2] _[→∞]_ [(i.e.,]
_∥now have the alignment phenomenon.∥[2]_ _[→]_ [1][), we have] _∥_ _∥_ _[→]_ _[u][k][v][T]_

Furthermore, we have
_WΠ_

_WL_ _F_ _W1_ _F_ _→_ _v1u[T]1_ _[· · ·][ v][L][u]L[T]_ _[→]_ _[v][1][.]_ (53)
_∥_ _∥_ _· · · ∥_ _∥_

We now move forward to prove the alignment phenomenon for the first layer. We assume that the
support vectors span the data space R[d] and denote the orthogonal complement of span(¯u) by ¯u[⊥].
Let denote the projection onto ¯u[⊥]. Under this assumption, Ji & Telgarsky (2019) showed the
_P⊥_
following lemma[7]

7This lemma is based on Lemma 12 in Soudry et al. (2018).


-----

**Lemma 7 (Lemma 3 in Ji & Telgarsky (2019)). Let S denote the set of indices of support vectors,**
_then with probability 1,_
_κ :=_ min (54)
_ξ_ =1,ξ _u¯_ [max]i _S_
_|_ _|_ _⊥_ _∈_ _[⟨][ξ, x][i][y][i][⟩]_ _[>][ 0]_

_if the data is sampled from absolutely continuous distribution._

To begin with, we first introduce the following lemma.
**Lemma 8. For the logistic loss function ℓ** = ln(1+ _e[−][x]), if_ _WΠ, ¯u_ 0 and _WΠ_ 2n/eκ =
_⟨_ _⟩≥_ _∥P⊥_ _∥≥_
_ϕ, then_


*P⊥W1, ∂W[∂]L[˜]1

_Proof. We first decompose Eq. (55) as two parts:_


_≥_ 0. (55)


_P⊥W1, ∂W[∂]L[˜]1_


= [1]


_P⊥W1, [∂f]_ [(]∂W[x][i][;]1[ W] [)]


_−_ _ϵ_ _P⊥W1, [∂]_ _∂W[∥][W][Π]1_ _[∥]_


_ℓ˜[′]_

_i=1_

X


(56)


_yi_


where
_P⊥W1, [∂]_ _∂W[∥][W][Π]1_ _[∥]_


Since _ℓ[˜][′]_ _< 0, we have_


1

_WΠ_ _P⊥W1, W1[T]_ _[· · ·][ W][ T]L_ _[W][L]_ _[· · ·][ W][2]_
_∥_ _∥_

1

_WΠ_
_∥_ _∥_ _[⟨P][⊥][W][Π][,][ P][⊥][W][Π][⟩≥]_ [0][.]


_P⊥W1, ∂W[∂]L[˜]1_


_≥_ _n[1]_


_P⊥W1, [∂f]_ [(]∂W[x][i][;]1[ W] [)]


= [1]


_ℓ˜[′]i[y][i]_
_i=1_

X


_P⊥WΠ, xiyiℓ[˜][′](WΠ)_ _._ (57)
E


_i=1_


Let xjyj ∈ arg maxi∈S ⟨−P⊥WΠ, xiyi⟩ which means that
_⟨−P⊥WΠ, xjyj⟩≥_ _κ∥P⊥WΠ∥._
Then we have


_P⊥WΠ, xiyiℓ[˜][′](WΠ)_
D E

_e[−⟨][W][Π][,][(][x][i][+][δ][i][)][y][i][⟩]_

1 + e[−⟨][W][Π][,][(][x][i][+][δ][i][)][y][i][⟩] _[⟨−P][⊥][W][Π][, x][i][y][i][⟩]_


_i=1_

_n_

_i=1_

Xn

_i=1_

Xn

_i=1_

X


= [1]

_n_

= [1]

_n_

_≥_ _n[1]_

_≥_ _n[1]_


_e[−⟨][W][Π][,][(][x][i][+][δ][i][)][y][i][⟩]_

1 + e[−⟨][W][Π][,][(][x][i][+][δ][i][)][y][i][⟩] _[⟨−P][⊥][W][Π][,][ P][⊥][x][i][y][i][⟩]_

_e[−⟨][W][Π][,][(][x][i][+][δ][i][)][y][i][⟩]_

1 + e[−⟨][W][Π][,][(][x][i][+][δ][i][)][y][i][⟩] _[⟨−P][⊥][W][Π][,][ P][⊥][x][i][y][i][⟩]_


_e[−⟨][W][Π][,][(][x][j]_ [+][δ][j] [)][y][j] _[⟩]_

1 + e[−⟨][W][Π][,][(][x][j] [+][δ][j] [)][y][j] _[⟩]_ _[⟨−P][⊥][W][Π][,][ P][⊥][x][i][y][i][⟩]_ [+ 1]n


_e[−⟨][W][Π][,][(][x][k][+][δ][k][)][y][k][⟩]_

1 + e[−⟨][W][Π][,][(][x][k][+][δ][k][)][y][k][⟩] _[⟨−P][⊥][W][Π][,][ P][⊥][x][k][y][k][⟩]_ _[,]_

(58)


_k∈Ck_


where Ck = _k :_ _WΠ,_ _xkyk_ 0 . Considering that xjyj is a support vector, the first term
_{_ _⟨−P⊥_ _P⊥_ _⟩≤_ _}_
of (58) can be bounded as follows:
1 _e[−⟨][W][Π][,][(][x][j]_ [+][δ][j] [)][y][j] _[⟩]_
_n_ 1 + e[−⟨][W][Π][,][(][x][j] [+][δ][j] [)][y][j] _[⟩]_ _[⟨−P][⊥][W][Π][,][ P][⊥][x][i][y][i][⟩]_

_u⟩eϵ∥WΠ∥_ _e[−⟨][W][Π][,x][j]_ _[y][j]_ _[−][γ]u[¯]⟩_

_≥_ _n[κ]_ _[∥P][⊥][W][Π][∥][e][−⟨][W][Π][,γ]_ [¯] 1 + e[−⟨][W][Π][,][(][x][j] [+][δ][j] [)][y][j] _[⟩]_


= _[κ]_ _u⟩eϵ∥WΠ∥_ _e[−⟨P][⊥][W][Π][,][P][⊥][x][j]_ _[y][j]_ _[⟩]_

_n_ _[∥P][⊥][W][Π][∥][e][−⟨][W][Π][,γ]_ [¯] 1 + e[ϵ][∥][W][Π][∥]e[−⟨P][⊥][W][Π][,][P][⊥][x][j] _[y][j]_ _[⟩]e[−⟨][W][Π][,γ]u[¯]⟩_

_u⟩eϵ∥WΠ∥,_ (59)

_≥_ 2[κ]n _[∥P][⊥][W][Π][∥][e][−⟨][W][Π][,γ]_ [¯]


-----

where we use that ϵ∥WΠ∥, ⟨WΠ, γu¯⟩ _, ⟨P⊥WΠ, P⊥xjyj⟩_ _> 0. Each term of the second part of (58)_
can be bounded by

_e[−⟨][W][Π][,][(][x][k][+][δ][k][)][y][k][⟩]_

1 + e[−⟨][W][Π][,][(][x][k][+][δ][k][)][y][k][⟩] _[⟨−P][⊥][W][Π][,][ P][⊥][x][k][y][k][⟩]_


_≥_ _e[ϵ][∥][W][Π][∥]e[−⟨][W][Π][,x][k][y][k][⟩]_ _⟨−P⊥WΠ, P⊥xkyk⟩_

= e[ϵ][∥][W][Π][∥]e[−⟨][W][Π][,γ]u[¯]⟩e−⟨WΠ,xkyk−γu¯⟩ _⟨−P⊥WΠ, P⊥xkyk⟩_

_≥_ _e[ϵ][∥][W][Π][∥]e[−⟨][W][Π][,γ]u[¯]⟩e−⟨−P⊥WΠ,P⊥xkyk⟩_ _⟨−P⊥WΠ, P⊥xkyk⟩_

_e[ϵ][∥][W][Π][∥]e[−⟨][W][Π][,γ]u[¯]⟩_ _,_
_≥_ _−_ [1]e
 

where the second inequality follows from


(60)

(61)


_⟨WΠ, xkyk −_ _γu¯⟩_ = ⟨WΠ, P⊥xkyk⟩ + ⟨WΠ, xkyk −P⊥xkyk − _γu¯⟩_
_≥⟨WΠ, P⊥xkyk⟩_ = ⟨P⊥WΠ, P⊥xkyk⟩

while the last inequality comes from f (x) = −xe[−][x] _≥−1/e for x ≥_ 0. Finally, we have
_u_ _ϵ_ _WΠ_ _κ_ _WΠ_
_WΠ, (xi + δi)yiℓ[˜][′](WΠ)_ _e[−⟨][W][Π][,γ]_ [¯]⟩e _∥_ _∥_ _∥P⊥_ _∥_ _._
_P⊥_ _≥_ 2n _−_ [1]e
D E  

Therefore, _WΠ, (xi + δi)yiℓ[˜][′](WΠ)_ 0 if
_P⊥_ _≥_
D E


_∥P⊥WΠ∥≥_ [2]eκ[n] [=][ ϕ.] (62)

We are now ready to prove the alignment phenomenon for the first layer. Let _W1 denote the_
_P⊥_
projection of rows of W1 onto ¯u[⊥]. We start from exploring the asymptotic behavior of _[∥P][⊥]W[W]1_ [1]F[∥][F]

_∥_ _∥_
when t →∞. The update of P⊥W1(t + 1) can be written explicitly as

_∥P⊥W1(t + 1)∥F[2]_

2

_∂_ [˜] _∂_ [˜]

= ∥P⊥W1(t)∥F[2] _[−]_ [2][η][(][t][)] *P⊥W1(t), P⊥ _∂WL1(t)_ + + η[(][t][)2] _∂WL1(t)_ _F_

2

_∂_ [˜] _∂_ [˜]

_≤∥P⊥W1(t)∥F[2]_ _[−]_ [2][η][(][t][)] *P⊥W1(t), P⊥ _∂WL1(t)_ + + η[(][t][)2] _[P]∂W[⊥]_ _L1(t)_ _F_

_∂_ [˜]

_≤∥P⊥W1(t)∥F[2]_ _[−]_ [2][η][(][t][)] *P⊥W1(t), _∂WL1(t)_ + + 2 _L˜(W_ (t + 1)) − _L[˜](W_ (t)) (63)

 

where the last inequality follows from Eq. (30). We define a large enough step t0 as follows: for any
_t_ _t0, we have_
_≥_


_∂_ [˜]
_W1(t),_ _L_

_∂W1(t)_


_∂_ [˜]
+ η(t)[2] _L_

_∂W1(t)_


_∥W1(t + 1)∥F[2]_ [=][ ∥][W][1][(][t][)][∥]F[2] _[−]_ [2][η][(][t][)]

where ”large enough t0” means that


_≥∥W1(t)∥F[2]_ _[,]_

(64)


_ϕ[2]_ + [˜](t0)
_L_ 0 (65)

_W1(t0)_ _F_ _→_
_∥_ _∥_

as _W1_ _F_ . Suppose that there exists a t1 _t0 such that_
_∥_ _∥_ _→∞_ _≥_

_WΠ(t1_ 1) _F < ϕ and_ _WΠ(t1)_ _F_ _ϕ,_ (66)
_∥P⊥_ _−_ _∥_ _∥P⊥_ _∥_ _≥_


-----

which is to say (recalling Lemma 8)

_∂_ [˜]

*P⊥W1(t), _∂WL1(t)_ +


_≥_ 0


=⇒∥P⊥W1(t + 1)∥F[2] _[≤∥P][⊥][W][1][(][t][)][∥]F[2]_ [+ 2] _L˜(W_ (t)) − _L[˜](W_ (t + 1)) _,_ (67)
 

for t0 _t1_ _t_ _t2 and t2 =_ if we never have _WΠ(t)_ _F < ϕ after t1. If there does not_
_≤_ _≤_ _≤_ _∞_ _∥P⊥_ _∥_
exist such a t1, then we directly conclude that _[∥P]∥[⊥]W[W]1∥[1]F[∥][F]_ _→_ 0 since ∥W1∥F →∞. On the other

hand, for any t0 _t1_ _t_ _t2_
_≤_ _≤_ _≤_

_∥P⊥W1(t)∥F[2]_ _∥P⊥W1(t1)∥F[2]_ [+ 2] _L˜(W_ (t1)) − _L[˜](W_ (t))

_W1(t)_ _F_ _≤_ _W1(t1)_ _F_ 
_∥_ _∥[2]_ _∥_ _∥[2]_

_F_ [+ 2 ˜]L(W (t1))
_≤_ _[∥P][⊥][W][1][(][t][1]W[)][∥]1[2](t1)_ _F_

_∥_ _∥[2]_

(W (t1))
_L_
_≤_ _[ϕ][2][ + 2][µ][(][t][1][)][ϕ][ +]W[ µ]1([2]t[(]1[t])[1][) + 2 ˜]F_

_∥_ _∥[2]_

_Φ(ϕ, t1)_
= (68)

_W1(t1)_ _F_
_∥_ _∥[2]_

where the last inequality follows from Eq. (31) and Φ(ϕ, t1) is defined by

_Φ(ϕ, t1) = ϕ[2]_ + 2µ(t1)ϕ + µ[2](t1) + 2 [˜](W (t1)) _Φ(ϕ, t[′])_ (69)
_L_ _≥_

for any t[′] _≥_ _t1 because the loss and µ never increase. Therefore we can conclude that_

_∥P⊥W1(t)∥F[2]_ _Φ(ϕ, t)_ _Φ(ϕ, t1)_ 0 (70)

_W1(t)_ _F_ _≤_ _W1(t)_ _F_ _≤_ _W1(t1)_ _F_ _→_
_∥_ _∥[2]_ _∥_ _∥[2]_ _∥_ _∥[2]_

for any t _t1. As a result,_
_≥_
_v1(t), ¯u_ 1. (71)
_| ⟨_ _⟩| →_

B PROOFS OF SECTION 3.2

We present some useful properties and proofs of Lemma 3 and Lemma 4 in Section B.1. Section
B.2 focuses on the divergences of weight norms. Section B.3 is about the convergence to KKT
points(Theorem 5).

**Additional notations** We use Wij;k to represent the i-th row j-th column entry of the k-th layer
weight martrix.

B.1 USEFUL PROPERTIES AND PROOFS OF LEMMA 3 AND LEMMA 4

For any m × n matrix A, we denote v(A), an mn-dimensional vector, as the vectorized version of it:


_A11_
_A21_
_. . ._
_Am1_
_. . ._
_Amn_


(72)


_v(A) =_

then the trace operator can be represented by


tr _A[T]_ _B_ = v(A)[T] _v(B) = v(B)[T]_ _v(A)._ (73)

We first introduce the Euler’s theorem on homogeneous functions.  


-----

**Lemma 9 (Euler’s theorem on homogeneous functions). If f** (W1, . . ., WL) is a positive multi-ck_homogeneous function_


_f_ (ρ1W1, . . ., ρLWL) = _ρ[c]k[k]_ _[f]_ [(][W][1][, . . ., W][L][)] (74)

_k=1_

Y


_for positive constants ρk’s and ck_ 1, then we have:
_≥_

_∂f_ (W1, . . ., WL)
_tr_ _Wk_ = ckf (W1, . . ., WL). (75)

_∂Wk_

 

_Proof. Taking derivatives of ρk on both sides of Eq. (74) for any given k ∈{1, . . ., L}, we have_


_Wk, [∂f]_

_∂Wk_


_∂f_
= tr _Wk_

_∂Wk_




_ρ[c]k[k][′][ f][′]_ [(][x][;][ W] [)][c][k][ρ][c]k[k][−][1]. (76)
_kY[′]≠_ _k_


Since ρk is arbitrary, we let ρL = · · · = ρk = · · · = ρ1 = 1, then the above equation becomes


_∂f_ (W1, . . ., WL)
tr _Wk_

_∂Wk_




= ckf (W1, . . ., WL) (77)


for any k ∈{1, . . ., L}.

Furthermore, we have

_∂f_ (x; W )

_, W_
_∂W_



_L_

_∂f_ (x; W )

= tr _Wk_

_∂Wk_

_k=1_  

X

= Kf (x, W ) (78)


where K = _k=1_ _[c][k][. For the deep neural networks defined in Eq.][ (2)][ with homogeneous property,]_
we can then apply the above lemma to them.

We examine whether Assumption 2 can still be applied to adversarial margin under Assumption 1.[P][L]

**Lemma 10 (Assumption 2 for adversarial training). For any fixed x,**

-  if yf (x; W ) is locally Lipschitz, then so does yf (x + δ(W ); W ) with perturbation δ(W ).

_Proof. For fixed x, suppose yf_ (W ) is locally Lipschitz on Y, then for each W ⊂ _Y there_
is a ZW containing W such that yf (W ) is Lipschitz on ZW :

_∥yf_ (W ) − _yf_ (V )∥≤ _LW ∥W −_ _V ∥_ for W, V ⊂ _ZW ._ (79)

For the adversarial perturbation, by definition δ(W ) and δ(V ) are solutions of the inner
maximization for the adversarial training, in other words,

_yf_ (x + δ(W ); W ) ≤ _yf_ (x + δ(V ); W ) (80)
_yf_ (x + δ(V ); V ) ≤ _yf_ (x + δ(W ); V ) (81)

because then we will have ℓ(x + δ(W ); W ) > ℓ(x + δ(V ); W ) since ℓ is non-increasing
under Assumption 1. As a result,


_yf_ (x + δ(W ); W ) − _yf_ (x + δ(V ); V ) ≤ _yf_ (x + δ(V ); W ) − _yf_ (x + δ(V ); V )
_LW_ _W_ _V_ (82)
_≤_ _∥_ _−_ _∥_

for yf (x + δ(W ); W ) > yf (x + δ(V ); V ). The result for yf (x + δ(W ); W ) < yf (x +
_δ(V ); V ) is similar._

-  if ∀i : yif (xi; W ) have locally Lipschitz gradients, then so do yif (xi + δi(W ); W ).


-----

_Proof. For fixed x, suppose y∇f_ (W ) is locally Lipschitz on Y, then for each W ⊂ _Y there_
is a ZW containing W such that y∇f (W ), δ(W ) are Lipschitz on ZW, for W, V ⊂ _ZW :_

_y_ _f_ (x; W ) _y_ _f_ (x; V ) _LW_ _W_ _V_ (83)
_∥_ _∇_ _−_ _∇_ _∥≤_ _∥_ _−_ _∥_
_y_ _f_ (x + δ(W ); W ) _y_ _f_ (x + δ(V ); W ) _LW x_ _δ(W_ ) _δ(V )_
_∥_ _∇_ _−_ _∇_ _∥≤_ _∥_ _−_ _∥_
_LW xLW δ_ _W_ _V_ (84)
_≤_ _∥_ _−_ _∥_

Then

_∥yi∇f_ (xi + δi(W ); W ) − _yi∇f_ (xi + δi(V ); V )∥
_≤∥yi∇f_ (xi + δi(W ); W ) − _yi∇f_ (xi + δi(W ); V )∥
+ ∥yi∇f (xi + δi(W ); V ) − _yi∇f_ (xi + δi(V ); V )∥
_≤_ (LW + LW xLW δ) ∥W − _V ∥._ (85)

B.1.1 PROOF OF LEMMA 3

_Proof. The proof can be done for multi-ck-homogeneous networks as defined in Lemma 9. We first_
note that


_ρ[c]k[k]_ (86)
_k=1_ _[∇][x][f]_ [(][x][;][ W][1][, . . ., W][L][)]

Y


_xf_ (x; ρ1W1, . . ., ρLWL) =
_∇_


by taking derivatives with respect to x on both sides of Eq. (74). Therefore _xf_ (x; W1, . . ., WL) is
_∇_
also positive homogeneous. Under Assumption 1, we prove Lemma 3 in the following cases:

1. ℓ2-FGM perturbation. This perturbations is taken as

_ℓ[′]_ _xf_ (x; W1, . . ., WL)
_δFGM(W_ ) = _[ϵy]_ [˜] _∇_ = (87)
_yℓ[˜][′]_ _xf_ (x; W1, . . ., WL) _−_ _∥∇[ϵy][∇]x[x]f[f](x[(][x]; W[;][ W]1, . . ., W[1][, . . ., W]L[L])∥[)]_
_∇_

because _ℓ[˜][′]_ _< 0. Invoking Eq. (86), we know that_


_k=1_ _[ρ]k[c][k]_
_δFGM(ρ1W1, . . ., ρLWL) =_ _L_ _[∇][x][f]_ [(][x][;][ W][1][, . . ., W][L][)]
_−_ _[ϵy][ Q][L]_

_k=1_ _[ρ]k[c][k]_ _[∥∇][x][f]_ [(][x][;][ W][1][, . . ., W][L][)][∥]

= Q
_−_ _[ϵy][∇]x[x]f[f](x[(][x]; W[;][ W]1, . . ., W[1][, . . ., W]L[L])_ [)]

_∥∇_ _∥_
= δFGM(W1, . . ., WL). (88)

2. FGSM perturbation.


_δFGSM(W_ ) = ϵsgn _yℓ[˜][′]∇xf_ (x; W1, . . ., WL) _,_ (89)
 

where sgn() denotes the sign function. Then we have


_δFGSM(ρ1W1, . . ., ρLWL) = ϵsgn_ _yℓ[˜][′]_ _ρ[c]k[k]_ = δFGSM(W )

_k=1_ _[∇][x][f]_ [(][x][;][ W][1][, . . ., W][L][)]!

Y

(90)
since ρk > 0 for any k thus will not affect the sign operation.


3. ℓ2 and ℓ -PGD perturbation. We only prove the case for ℓ2-PGD and the case for ℓ -PGD
_∞_ _∞_
is similar.


_δPGD[j]_ [(][W] [)][ −] _[ξ y][∇][x][f]_ [(][x][;][ W][1][, . . ., W][L][)]

_xf_ (x; W1, . . ., WL)
_∥∇_ _∥_


_δPGD[j][+1][(][W]_ [) =][ P][B][(0][,ϵ][)]


(91)


-----

where (0,ϵ) denotes the projection operator, (0, ϵ) is the perturbation set, j is the step
_PB_ _B_
indices of PGD and ξ is the step size. We prove by induction. For j = 0, we have


_δPGD[1]_ [(][ρ][1][W][1][, . . ., ρ][L][W][L][) =][ P] (0,ϵ) _ξ [y][∇][x][f]_ [(][x][;][ W][1][, . . ., W][L][)]
_B_ _−_ _xf_ (x; W1, . . ., WL)

 _∥∇_ _∥_ 

= δPGD[1] [(][W][1][, . . ., W][L][)][.] (92)


If we have δPGD[j] [(][ρ][1][W][1][, . . ., ρ][L][W][L][) =][ δ]PGD[j] [(][W][1][, . . ., W][L][)][, then for][ j][ + 1][, we have]


_k=1_ _[ρ][k][∇][x][f]_ [(][x][;][ W][1][, . . ., W][L][)]

_δPGD[j][+1][(][ρ][1][W][1][, . . ., ρ][L][W][L][) =][ P][B][(0][,ϵ][)]_ _δPGD[j]_ [(][ρW] [)][ −] _[ξ y][ Q]L_ _[L]_

" _k=1_ _[ρ][k][ ∥∇][x][f]_ [(][x][;][ W][1][, . . ., W][L][)][∥] #

= (0,ϵ) _δPGD[j]_ [(][W] [)][ −] _[ξ y]Q[∇][x][f]_ [(][x][;][ W][1][, . . ., W][L][)]
_PB_ _xf_ (x; W1, . . ., WL)

 _∥∇_ _∥_ 

= δPGD[j][+1][(][W][1][, . . ., W][L][)][.] (93)

Therefore, these four adversarial perturbations are all scale invariant adversarial perturbations by our
definition.

B.1.2 PROOF OF LEMMA 4

_Proof. This can be easily proved by noting that, for any multi-ck-homogeneous functions,_


_f_ (xi + _δi(ρ1W1, . . ., ρLWL); ρ1W1, . . ., ρLWL) =_ _ρ[c]k[k]_ _[f]_ [(][x][i][ +] _[δ][i][(][W][1][, . . ., W][L][);][ W][1][, . . ., W][L][)]_

_k=1_

Y

(94)
because δ(W ) is a scale invariant adversarial perturbations. Then f (x + δ(W ); W ) is still a homogeneous function and Lemma 9 can be applied to f (x + δ(W ); W1, . . ., WL) and we have


_n_

_ℓ˜[′]i[y][i][tr]_ _∂f_ (xi + δi(W ); W1, . . ., WL) _Wk_

_∂Wk_

_i=1_ 

X


_∂_ [˜]
_L_ _Wk_

_∂Wk_


= [1]


tr


=
_−_ _[c]n[k]_

=
_−_ _[c]n[k]_


_e[−]γ[˜]i_ _yif_ (xi + δi(W ); W1, . . ., WL)
_i=1_

Xn

_e[−]γ[˜]i_ _γ˜i._ (95)
_i=1_

X


Taking c1 = · · · = cL gives us Lemma 4.

B.2 DIVERGENCES OF FROBENIUS NORMS OF WEIGHT MATRICES

B.2.1 DIVERGENCES OF ALL LAYERS

**Lemma 11. The Frobenius norms of all layers grow at approximately the same rate along the**
_trajectory of gradient flow for adversarial training with scale invariant adversarial perturbations for_
_multi-ck-homogeneous DNNs_


_d_ _WL_ _F_
_∥_ _∥[2]_

_dt_


_d∥WL−1∥F[2]_ = = [1]

_dt_ _· · ·_ _c1_


_d∥W1∥F[2]_ (96)

_dt_


_cL_


_cL−1_


**Remark.** Note that a similar conclusion as that of Lemma 11 exists for gradient flow of the standard
training for multi-1-homogeneous networks Du et al. (2018). We generalize this property to the
adversarial training of multi-ck-homogeneous neural networks. When c1 = · · · = cL, we have that
all layer grow at the same rate.


-----

_Proof. For any Wk,_

_d_ _Wk_ _F_
_∥_ _∥[2]_

_dt_


_T_
!


_Wij;kWji[T];k_

= 2
_dt_  _−_


_∂L[˜]_

_∂Wij;k_


_Wji[T];k_


_i,j_

= [2][c][k]


_i,j_


_e[−]γ[˜]i_ _γ˜i,_ (97)
_i=1_

X


_T_

where we use _[dW]dt_ [=][ −] _∂W∂L[˜]k_ and Lemma 4. One can then immediately notice that the above

equation does not depend on any specific  _k, thus we have_


1 _d_ _WL_ _F_

_∥_ _∥[2]_

_cL_ _dt_


_d∥WL−1∥F[2]_ = = [1]

_dt_ _· · ·_ _c1_


_d∥W1∥F[2]_ _._ (98)

_dt_


_cL−1_


B.2.2 PROOF OF THEOREM 3

We prove this theorem by exploring the time evolution of ρ.

_Proof. We start from a multi-ck-homogeneous function then take c1 = · · · = cL. Recalling_
_f_ (x; W ) = ρf (x; _W_ ) where _Wk = Wk/ ∥Wk∥F and_

_L_ _L_

[c] [c] _ρ =_ _ρ[c]k[k]_ [=] _∥Wk∥F[c][k]_ _[,]_ (99)

_k=1_ _k=1_

Y Y

the adversarially trained predictor with scale invariant adversarial perturbation is still homogeneous
(see Eq. (94)):
_f (x + δ(W_ ); W ) = ρf _x + δ(W[c]);_ _W_ _._ (100)

For ρk = ∥Wk∥F, we have  [c]


_dρk_

_dt_ [=][ c]nρ[k]k


_e[−]γ[˜]i_ _γ˜i_ (101)
_i=1_

X


by invoking Eq. (97). The dynamical evolution of ρ will then be


_L_

_dρ_ _ρ[c]1[1]_ _k_ _L_ [=]

_dt_ [=] _k=1_ _[· · ·][ dρ]dt[c][k]_ _[· · ·][ ρ][c][L]_

X


_c[2]k[ρ][2]_

_ρ[2]k_


_e[−]γ[˜]i_ _γˆi_


(102)


_k=1_


Let t0 denote the time such that all worst case adversarial examples are correctly classified. We study
the trends for ρ after t0. On one hand, the empirical adversarial loss does not increase:


_L_ _L_ _dW_ _dW_
_−_ _[d]dt[ ˜]_ [=][ −] _∂W[∂]_ [˜] _dt_ [=] _dt_


_≥_ 0. (103)


On the other hand, recalling our definition of normalized adversarial margin, let

_m = arg_ min _γi = arg_ min _γi_ (104)
_i_ 1,...,n [˜] _i_ 1,...,n _[ρ][ˆ]_
_∈{_ _}_ _∈{_ _}_


and note the definition of t0, we have

_e[−]γ[˜]m ≤_ _n[1]_ Xi _e[−]γ[˜]i = ˜L =⇒_ _γ˜m ≥_ ln  1L˜(t)


_> 0_ (105)


_≥_ ln


˜(t0)
_L_


because _L[˜] does not increase and_ _L[˜](t) ≤_ _L[˜](t0) < 1. This also implies that ˜γm, thus ˜γi for all_
_i ∈{1, . . ., n}, can not be arbitrarily close to 0. Otherwise one would conclude that_ _L[˜](t) may_
be arbitrarily close to 1 which is a contradiction. Therefore we can immediately conclude that
_dρ/dt > 0 in Eq. (102), which implies that ρ may diverge as t →∞. To have a clearer view_


-----

about the convergence property of ρ, we study the following relation regarding the time evolution of
_e[ρy][i][f]_ [(][x][i][+][δ][i][(]W[c] )) after t0:

_de[ρy][i][f]_ [(][x][i][+][δ][i][(]W[c] );W[c] ) = _e[ρ]γ[ˆ]i_ _γˆi_ _dρ_ + _e[ρ]γ[ˆ]i_ _ρyi_ _df_ (xi + δi(W[c]); _W_ ) _._ (106)

_dt_ _dt_ _dt_

_i_ _i_ _i_

X X X [c]

(a) (b)

The term (a) can be computed by invoking Eq. (102)| {z } | {z }

_L_

(a) = _c[2]k[ρ][2]_ _e[ρ]γ[ˆ]i_ _γˆi_ _e[−][ρ]γ[ˆ]j ˆγj_

_k=1_ _nρ[2]k_ _i_  _j_ 

X X

_L_ _n_ [X]n 

= _c[2]k[ρ][2]_ _γˆi[2]_ [+ 1] _γˆiγˆj_ _e[ρ][(ˆ]γi−γˆj_ ) + e−ρ(ˆγi−γˆj )[]

_k=1_ _nρ[2]k_  _i=1_ 2 _i=j_ 

X X X̸ 

_L_  _n_ 2 _L_ 

_c[2]k[ρ][2]_ _ρ[2]ζ_ [2]

_γˆi_ _,_ (107)

_≥_ _k=1_ _nρ[2]k_ _i=1_ ! _≥_ _k=1_ _nρ[2]k_

X X X

where the first inequality follows from x + 1/x ≥ 2 for x ≥ 0 and we denote the minimum of _i_ _γ[ˆ]i_

for t ≥ _t0 as ζ > 0. The existence of such ζ has been discussed below Eq. (105)._

[P]

The term (b) needs more analysis. For any fixed adversarial example xi + δ(W[c]), we have

_L_

_df_ (xi + δi(W[c]); _W_ ) _∂f_ (xi + δi(W[c]); _W_ ) _dW[c]k_
_yi_ = yi tr _,_ (108)

_dt_ _k=1_ _∂W[c]k_ _dt_ !

[c] X [c]

where dW[c]k/dt can be computed as follows

_dW[c]jl;k_ _∂W[c]jl;k_ _dWmn;k_

=
_dt_ _∂Wmn;k_ _dt_

_m,n_

X

_∂_ _Wjl;k_

=

_m,n_ _∂Wmn;k_  tr _WkWk[T]_  _dt_

X

 q _n_  _[dW][mn][;][k]_

= [1] _dWjl;k_ _Wjl; k_ _yie[−]γ[˜]i_ _γ˜i_ _._ (109)

_ρk_ _dt_ _−_ _nρk_ _i=1_ !

c X

On the other hand, we note that

_T_

_dWk_ _∂_ [˜] _∂f_ (xi + δi(W ); W )

= _L_
_dt_ _−_ _i_ _∂f_ (xi + δi(W ); W )  _∂Wk_ 
X

_T_

= _ρ_ _yje[−]γ[˜]j_ _∂f_ (xj + δj(W[c]); _W_ ) (110)

_nρk_ _j_ _∂W[c]k_ !

X [c]

because _[∂f]∂W[(][W]k[ )]_ = _ρρk_ _∂f∂W[c](W[c]k )_ [. Combing Eq. (109) and Eq. (110), we have]

_T_

_dW[c]k_ = _ρ_ _yje[−]γ[˜]j_ _∂f_ (xj + δj(W[c]); _W_ ) _Wkf_ (xj + δj(W[c]); _W_ ) _._ (111)
_dt_ _nρ[2]k_ _j_  _∂W[c]k_ ! _−_ [c] 

X [c]

Therefore  [c] 

_T_

_v_ _dW[c]k_ = _ρ_ _yje[−]γ[˜]j_ _Akv_ _∂f_ (xj + δj(W[c]); _W_ ) _,_ (112)

 _dt_ ! [] _nρ[2]k_ _j_ _∂W[c]k_ !

X [c]

 


-----

where

_T_

can be seen as a projector operator. Putting Eq. (112) into Eq. (108) will give us the expressionAk = I − _[v]_  Wk∥[T]W _vk∥ F[2]Wk[T]_  = I − _v_ Wk[T]  _v_ Wk[T] T (113)

c c

_L_ _T_

_yi_ _df_ (xi + δi(W[c]); _W_ ) = _ρ_ _e[−]γ[˜]j_ _v_ _∂γˆj(W[c])_ _A[T]k_ _[v]_ _∂γˆi(W[c])_ _._ (114)

_dt_ _k=1_ _nρ[2]k_ _j_ _∂W[c]k_ ! _∂W[c]k_ !

[c] X X

Now the term (b) in Eq. (106) is, by putting Eq. (114) back into it,

_L_ _T_

(b) = _ρ[2]_ _e[ρ]γ[ˆ]i_ _e[−][ρ]γ[ˆ]j_ _v_ _∂γˆj_ _A[T]k_ _[v]_ _∂γˆi_

_k=1_ _nρ[2]k_ _i_  _j_  _∂W[c]k_   _∂W[c]k_ []

X X

_L_ _n_ [X] _T_ 

_ρ[2]_ _∂γˆi_ _∂γˆj_ _γi_ _γˆj_ ) _ρ(ˆγi_ _γˆj_ )[i]

= [1] _v_ _A[T]k_ _[A][k][v]_ _e[ρ][(ˆ]_ _−_ + e− _−_ _._ (115)

2 _kX=1_ _nρ[2]k_ _i,jX=1_  _∂W[c]k_   _∂W[c]k_  h

(c)

Rearranging e[ρ][(ˆ]γi−γˆj ) +| e−ρ(ˆγi−γˆj ) as follows {z }
_e[ρ][(ˆ]γi−γˆj_ ) + e−ρ(ˆγi−γˆj ) = eρ(ˆγi+ˆγj ) + e−ρ(ˆγi+ˆγj ) − (eργˆj − _e−ργˆj_ )(eργˆi − _e−ργˆi_ )
will allow us to rewrite (c) as

(c) = _e[ρ]γ[ˆ]i_ _Akv_ _∂γˆi_ 2 + _e[−][ρ]γ[ˆ]i_ _Akv_ _∂γˆi_ 2

_i_  _∂W[c]k_  2 _i_  _∂W[c]k_  2

X X

_γi_ _ργˆi_ [] _∂γˆi_ 2
_e[ρ][ˆ]_ _e−_ _Akv_

_−_ _i_ _−_  _∂W[c]k_  2

= Xe[ρ]γ[ˆ] i _Akv_ _∂γˆi_ 2 + _e[−][ρ]γ[ˆ]i_ _Akv_ _∂γˆi_ 2

_i_  _∂W[c]k_  2 _i_  _∂W[c]k_  2

X X

_e[ρ]γ[ˆ]i_ _Akv_ _∂γˆi_ _e[−][ρ]γ[ˆ]i_ _Akv_ _∂γˆi_ 2

_−_ _i_  _∂W[c]k_  _−_ _i_  _∂W[c]k_  2

X X

_≥_ 0. (116)
Combing Eq. (107) and Eq. (116) with Eq. (106), we will have


_de[ρy][i][f]_ [(][x][i][+][δ][i][(]W[c] );W[c] )

_dt_


_ρ[2](t)ζ_ [2]

(117)
_n[2]ρ[2]k[(][t][)]_ _[.]_


_k=1_


According to Eq. (101), we know that ∀k : dρk/dt > 0 after t0, therefore ∀k : ρ[2]/ρ[2]k [is lower]
bounded by its value at t0 for t ∈ [t0, ∞) because

_d_ _ρ2_ _L_

_dt_  _ρ[2]k_  = _dt[d]_ ρ[2]k[c][k][−][2] _k[′]=k_ _ρ[2]k[c][′]_ _[k][′]_ 

Y̸

 _L_ _L_ _L_

= 2(ck 1)ρk[2][c][k][−][3]ρ˙k _ρ[2]k[c][′]_ _[k][′]_ + ρ[2]k[c][k][−][2] 2ck′ _ρ[2]k[c][′]_ _[k][′]_ _[−][1]ρ˙k′_ _ρ[2]k[c][′′][k][′′]_ _> 0,_
_−_ _kY[′]≠_ _k_ _kX[′]≠_ _k_ k[′′]Y≠ _k[′],k_ 

  (118)

where ˙ρk denotes dρk/dt > 0 for any k due to Eq. (101). Integrating both sides of Eq. (117) from
_t0 to t gives us_

_L_ _t_

1 _e[ρ][(][t][)][y][i][f]_ [(][x][i][+][δ][i][(]W[c] );W[c] (t)) _eρ(t0)yif_ (xi+δi(W[c] );W[c] (t0))[] _ρ[2](τ_ )ζ [2]
_n_ Xi  _−_ _≥_ _kX=1_ Zt0 _n[2]ρ[2]k[(][τ]_ [)] _[dτ]_

_L_

_ρ[2](t0)ζ_ [2] (119)

_≥_ _k=1_ _n[2]ρ[2]k[(][t][0][)(][t][ −]_ _[t][0][)][,]_

X


-----

where the last inequality follows from Eq. (118). Let t →∞ in the above equation, we will have

_ρ = Ω(ln t) →∞_ (120)

since yif (xi + δi(W[c]); _W_ ) is upper bounded, which can be easily deduced considering that all
weights have norm 1, and ζ is lower bounded as discussed before. Taking c1 = · · · = cL gives us the
conclusion.

[c]

(Lemma 11), we immediately haveNote that ρ = ρ[c]1[1] _[· · ·][ ρ]L[c][L]_ [and][ ρ][k][ for any][ k][ grows at the same rate for multi-][c][-homogeneous functions]
**Corollary 1. ∀k ∈{1, . . ., L} : ρk →∞** _as t →∞._

B.3 CONVERGENCE TO KKT POINT

B.3.1 PROOF OF THEOREM 4

_Proof. Most calculations needed for the proof of this theorem have been done in Section B.2.2._
According to Eq. (114), we have

_L_ _T_

_e[−]γ[˜]i_ _yi_ _df_ (x + δi(W[c]); _W_ ) 1 _e[−]γ[˜]i_ _e[−]γ[˜]j_ _v_ _∂γˆj(W[c])_ _A[T]k_ _[v]_ _∂γˆi(W[c])_
_i_ _dt_ _∝_ _k=1_ _ρ[2]k_ _i_  _j_ _∂W[c]k_ ! _∂W[c]k_ ![]

X [c] XL X [X] 2 

= 1 _e[−]γ[˜]i_ _Akv_ _∂γˆi_ 0. (121)

_k=1_ _ρ[2]k_ _i_  _∂W[c]k_  2 _≥_

X X

B.3.2 PROOF OF THEOREM 5

The KKT conditions for the optimization problem Eq. (19) are


_k :_ 1 _∂ρ[2]k_
_∀_ 2 _∂Wk_


_n_

_∂γ˜i_
_λi_ = 0 (122)

_∂Wk_

_i=1_

X


_i_ 1, . . ., n : λi(˜γi 1) = 0. (123)
_∀_ _∈{_ _}_ _−_

Following Dutta et al. (2013); Lyu & Li (2020), we define the approximate KKT point in our settings
**Definition 3 (Adapted from Definition C.4 in Lyu & Li (2020)). The approximate KKT points of the**
_optimization problem (19) are those feasible points which satisfy the following condtions:_

_∂_ _W_ _∂γ˜i_

_1._ 2[1] _∥∂W ∥[2]_ _i=1_ _[λ][i]_ _∂W_

_−_ [P][n] 2 _[≤]_ _[χ][;]_

_2._ _i_ 1, . . ., n : λi(˜γi 1) _ξ,_
_∀_ _∈{_ _}_ _−_ _≤_


_where χ, ξ > 0 and λi_ 0. These points are said to be (χ, ξ)-approximate KKT points.
_≥_

We now present the proof of Theorem 5.

_Proof. Let_


_ρ[2]k[,]_ (124)

uk=1
uX
tW

_,_ (125)
_γ˜m[1][/K]_


_∥W_ _∥_ =

_V =_


where K = _k=1_ _[c][ =][ cL][. Since]_

[P][L]


_cf_ (x; W ) = cγ˜mf (x; V ) (126)


-----

according to the definition of the multi-c-homogeneous functions and


_cf_ (x; W ) = _Wk, [∂f]_ [(][x][;][ W] [)]

_∂Wk_


we have
_∂f_ (x; V )


_, cf_ (x; V ) = _Vk, [∂f]_ [(][x][;][ V][ )]

_∂Vk_



(127)


_∂f_ (x; W )

_._ (128)
_∂W_


_γ˜m[(][K][−][1)][/K]_


_∂V_


Specifically, in the following, we will show that V is a (χ, ξ)-approximate KKT points of the
optimization problem (19) with χ, ξ → 0 along the training trajectory. Because the homogeneous
property of the network trained with scale invariant adversarial perturbations, it satisfies the MFCQ
condition by simply noting that

_∂γ˜i_
_i_ 1, . . ., n : tr _Wk_ = cγ˜i > 0. (129)
_∀_ _∈{_ _}_ _∂Wk_
 

This makes a point being a KKT point a necessary condition for it to be the optimal solution.

We now show that the limit point of V along the adversarial training trajectory is an (χ, ξ)-approximate
KKT point with χ, ξ → 0 starting with the first condition. Let

_−1_

_λi =_ _∂W∂L[˜]_ ! _∥W_ _∥γ˜m[(][K][−][2)][/K]e[−]γ[˜]i_ _,_ (130)


then we have

_[V][ −]_


_∂f_ (xi + δi(V ); V )
_λiyi_

_∂V_


_λiyi_

_γ˜m[(][K][−][1)][/K]_


_∂f_ (xi + δi(W ); W )

_∂W_


_γ˜m[1][/K]_


2([P]k _[ρ]k[2][)]_ _W,_ _∂W[∂]L[˜]_
= ([Q]k _[ρ]k[2][c][)][1][/K]γ[ˆ]m[2][/K]_ 1 − _WD_ _∂W[∂]L[˜]E_ 2 

 _∥_ _∥_ 
 

_≤_ _ρ[2]k2[′′]Lργ[ˆ]m[2][2]k[/K][′]_ 1 − _WDW,_ _∂W[∂]∂W[∂]L[˜]L[˜]E_ 2  = χ(t) (131)

 _∥_ _∥_ 
 

where a follows from (128); k[′] = arg maxk ρk and k[′′] = arg mink ρk. This is the first condition
for the limit point of V being an approximate KKT point. We can further show that χ → 0 as t → 0
by noting the following alignment phenomenon which was originally observed by Ji & Telgarsky
(2020) and intended for standard training with fixed training examples:

**Lemma 12 (Adapted from Theorem 4.1 in Ji & Telgarsky (2020)). Under Assumption 2 and**
_Assumption 4 for exponential loss and homogeneous deep neural networks, along the trajectory of_
_adversarial training with scale invariant adversarial perturbations, we have_

_W_ (t), _∂W[∂]L[˜]_ [(][t][)]

_k_ 1, . . ., k : lim = 1. (132)
_∀_ _∈{_ _}_ _t_ D E _−_
_→∞_ _∥W_ _∥_ _∂W[∂]L[˜]_ [(][t][)]

The extension of Theorem 4.1 in Ji & Telgarsky (2020), which was intended for fixed training
examples, to our settings is because, by our construction for adversarial training with scale invariant
adversarial perturbations, the adversarial training margin are locally Lipschitz with locally Lipschitz
gradients and the prediction function f (x + _δ(W_ ); W ) is positively (multi)homogeneous with respect
to W (see Lemma 10 and Eq. (4)). Invoking Lemma 12 and considering that ˆγm can not be arbitrarily
close to 0[8] and ∀k : ρk →∞ at the same rate according to Lemma 11 thus _ρ[ρ]k[k]′′[′]_ [can not be infinite,]

we have,
lim (133)
_t→∞_ _[χ][(][t][)][ →]_ [0][.]


8See Eq. (105). In fact, it keeps increasing after some time according to Theorem 4.


-----

On the other hand, the second condition for the limit point of V being an approximate KKT point is


_−1_
!


_∂L[ˆ]_

_∂W_


_n_

_γi_
_λi( [˜]_ 1) =

_γ˜m_ _−_

_i=1_

X


_∥W_ _∥_

_γ˜m[2][/K]_


_e[−]γ[˜]i_ (˜γi _γ˜m)_
_−_


_i=1_


a _k_ _[ρ]k[2]_ _n_ _e[−]γ[˜]i_ (˜γi − _γ˜m)_
_≤_ _ρ[2]P[/K]γˆm[2][/K]_ _i=1_ _W,_ _∂W[∂]L[˜]_

X

b _k_ _[ρ]k[2]_ _n_ D _neE[−]γ[˜]i_ (˜γi − _γ˜m)_
_≤_ _ρ[2]P[/K]γˆm[2][/K]_ _i=1_ _Kρe[−]γ[˜]m_ _j_ _[y][j][f]_ [(][x][j][ +][ δ][j][(]W[c]); _W_ )

c _n_ _k_ _[ρ]k[2]X_ _n_ _γi_ _γ˜m)_ _γ[˜]i_ _γ˜m_

_e[−][(˜]_ [P]− _−_ [c]

_≤_ _Kρ[1+2][/K]γˆm[2][/K]_ _i=1_ _γˆm_

d _n[P]_ _k_ _[ρ]k[2]_ X
_≤_ _Keρ[1+2][/K]γˆm[1+2][/K]_

[P]

_≤_ _KeρρnLρ[2]k[′′]_ _γ[ˆ][2]km[1+2][′]_ _[/K]_ = ξ, (134)

where a follows from _W,_ _∂W[∂]L[˜]_ _W_ _∂W[∂]L[˜]_ ; b is due to Lemma 4 and _i : ˜γi_ _γ˜m; c uses_

_≤∥_ _∥_ _∀_ _≥_

_∀∀ik : ˜ : limγi ≥t→∞γ˜mρ again;k(t) →∞ dD is due toat the same rate, we conclude thatE e[−][x]x is upper bounded by its value at limt→∞_ _ρρ[2]k[2]k x[′′][′]_ [ can not be infinite thus] = 1 for x ≥ 0. Since

limt _ξ(t)_ 0 because ˆγm can not be arbitrarily close to 0 and limt _ρ_ . Therefore, the
_→∞_ _→_ _→∞_ _→∞_
limit point of V is an (χ, ξ)-approximate KKT point of the mini-norm problem along the trajectory
of adversarial training with scale invariant adversarial perturbations where limt→∞ _χ(t), ξ(t) →_ 0.
Restating the theorem in Dutta et al. (2013) regarding the relation between (χ, ξ)-approximate KKT
point and KKT point in our setting:

**Theorem 6 (Theorem 3.6 in Dutta et al. (2013) and Theorem C.4 in Lyu & Li (2020)). Let {xt : t ∈**
N} be a sequence of feasible point of the optimization problem 19. xt is an (χt, ξt)-approximate
_KKT poiint for allxt_ _x as t_ _tand MFCQ holds at with two sequences x {, thenχt > x 0 : is a KKT point of this optimization problem. t ∈_ N} and {ξt > 0 : t ∈N} and χt, ξt → 0. If
_→_ _→∞_

We then immediately conclude that the limit point of {W (t)/∥W _∥_ : t ≥ 0} of gradient flow for
the adversarial training objective Eq. (6) is along the direction of a KKT point of the optimization
problem (19).

B.4 GENERALIZATION TO NON-SMOOTH CASE

It is not hard to generalize the current analysis to non-smooth case, which will include the deep ReLU
network, because, in our main steps, there are counterparts of our conclusions for non-smooth case
for that sub-differential (Clarke, 1975) is a generalization of gradient. The non-smooth analysis can
be done by first replacing the gradient flow equation Eq. (16) with its generalization,

_dW_

(W (t)) (135)
_dt_ _[∈−][∂]L[˜]_

where ∂L[˜](W (t)) is the sub-differential. Then one can follow similar procedures as our approach to
make the generalization.

_Proof. For simplicity, we only consider multi-1-homogeneous networks here, i.e., c1 = · · · = cL = 1,_
which include the deep ReLU neural networks. Lemma 9 can be generalized to non-smooth case
according to Lemma C.1 in Ji & Telgarsky (2020) or Theorem B.2 in Lyu & Li (2020):

**Lemma 13 (Lemma C.1 in Ji & Telgarsky (2020)). Suppose f : R[n]** _→_ R is locally Lipschitz and
_L-positively homogeneous, then for any W ∈_ R[n] _and any W_ _[∗]_ _∈_ _∂f_ (W ),

_⟨W, W_ _[∗]⟩_ = Lf (W ). (136)


-----

The starting point of the proof for Lemma 3 is the homogeneity of f (x; W ) and can be easily
promoted to handle the non-smooth case.

With generalizations of Lemma 3 and Euler’s theorem on homogeneous functions for non-smooth
case, the proof of Lemma 4 can also be generalized to non-smooth case accordingly because it
is based on the Euler’s theorem on homogeneous functions and Lemma 3 for adversarial training.
Specifically, it will become

_n_

_dWk_ = [1] _e[−]γ[˜]i_ _γ˜i._ (137)

_dt [, W][k]_ _n_

 _i=1_

X

The proof of Theorem 3 adopts Lemma 4 and can be generalized to non-smooth case by combining
the chain rule (Clarke, 1983) for gradient flow to ensure Eq. (103)


_L_ _∂_ [˜](W (t)), [dW]
_−_ _[d]dt[ ˜]_ [=][ −] _L_ _dt_



_dW_

_dt_


_≥_ 0 (138)


with similar approach as in Lemma B.9 in Ji & Telgarsky (2020) or Lemma 5.2 in Davis et al. (2018).
Then generalizing the rest of the proof of Theorem 3 is straightforward.

Finally, the generalization of Theorem 5 to non-smooth case can also be done because it adopts
Theorem 3, Lemma 4, the chain rule, and the gradient alignments Lemma 12 which holds for
non-smooth case.

B.5 DEEP LINEAR NETWORKS ADVERSARIALLY TRAINED WITH ℓ PERTURBATIONS
_∞_

Following the settings of Section 3.1, the ℓ -perturbation for deep linear networks f (x; W ) =
_∞_
_WL_ _W1x can be given exactly as_
_· · ·_

_δ(W_ (t)) = _ϵyisgn(WΠ(t))._ (139)
_−_

According to Theorem 5, the adversarial training solution is formulated from solving the following
optimization problem:

max 2[1] _[∥][W]_ _[∥]2[2]_ (140)

_s.t._ min (141)
_i_ 1,...,n _[y][i][W][Π][x][i][ −]_ _[ϵ][∥][W][Π][∥][1][ ≥]_ [1][.]
_∈{_ _}_

It can be seen that this optimization problem, a mixed-norm optimization problem, will have different
solutions when compared to the margin-maximization problem of the original data, which will lead
to a different decision boundary distinguishing adversarial training methods from the standard ones.

Besides, the constraint on ϵ (Eq. (8) for ℓ2 perturbation) should also be changed accordingly such
that the adversarial training examples are still linear separable:


_ϵ_ _γm,1_ (142)
_≤_


where γm,1 is the max-ℓ1-norm margin:


_γm,1 = max_ min (143)
_u_ 1=1 _i_ 1,...,n _[y][i][ux][i][.]_
_∥_ _∥_ _∈{_ _}_

C SUPPLEMENTED EXPERIMENTS

C.1 ADVERSARIAL TRAINING WITH DIFFERENT LOSS FUNCTIONS, ARCHITECTURES AND
VARYING PERTURBATION SIZES

We present the results for adversarial training with different loss functions, architectures and varying
perturbation sizes in this section to further verify our theorems and assumptions. We use two different
models: one is a 3-layer neural network with the same architecture as that in the Section 4; the other
is a CNN with architecture Input-Conv-ReLU-Pooling-Conv-ReLU-Pooling-FC-FC,
where Conv stands for convolution layer, FC stands for fully connected layer and Pooling is
max-pooling layer.


-----

0.04

0.03

0.02

0.01

0.00


−0.03

|FGSM, ε = 8/255 FGSM, ε = 12/255 FGSM, ε = 24/255 PGD, ε = 8/255 PGD, ε = 12/255 PGD, ε = 24/255|Col2|
|---|---|
|0|500 1000 1500 2000 2500 epochs|



(a) Adversarial normalized margins for varying perturbation sizes


100

98

96

94

92

90

88

86

|FGSM, ε = 8/255 FGSM, ε = 12/255 FGSM, ε = 24/255 PGD, ε = 8/255 PGD, ε = 12/255 PGD, ε = 24/255|Col2|
|---|---|
|0|100 200 300 400 500 epochs|


(b) Adversarial training accuracy for varying perturbation sizes

Figure 2: Adversarial training for the 3-layer neural network on MNIST to perform binary classification. The perturbations used for training are FGSM and ℓ -PGD perturbations with varying
_∞_
perturbation sizes ϵ = 8/255, 12/255 and 24/255. The adversarial normalized margins and adversarial training accuracies are evaluated with the corresponding perturbations used for training.


For the 3-layer network, we conduct adversarial training using both FGSM and ℓ -PGD perturbations
_∞_
with perturbation sizes ϵ = 8/255, 12/255 and 24/255 to perform binary classification where we
choose all examples with labels ”3” and ”8” from MNIST to be our dataset. The loss function is
logistic loss. Fig. 2(a) shows that for both FGSM and ℓ -PGD perturbations with varying perturbation
_∞_
sizes, the adversarial normalized margins during training keep increasing after some step. Fig. 2(b)
reveals that the adversarial training accuracy can achieve 100% (after ∼ 200 epochs), which supports
Assumption 4.

For the multi-classification task, let f (x; W )[j] denote the j-th output. Then the margin for an
adversarial example xi + δi corresponding to the original example xi is defined by

_γ˜i = f_ (xi + δi; W )[yi] max
_−_ _j=yi_ _[f]_ [(][x][i][ +][ δ][i][;][ W] [)[][j][]][,]
_̸_


-----

0.2

0.0


−0.2

−0.4

−0.6

|FGSM, ε = 16/255 FGSM, ε = 32/255 PGD, ε = 16/255 PGD, ε = 32/255|Col2|Col3|
|---|---|---|
|0|100|200 300 400 500 epochs|



(a) Adversarial normalized margins for varying perturbation sizes


100

98


96

94


92

90

|FGSM, ε = 16/255 FGSM, ε = 32/255 PGD, ε = 16/255 PGD, ε = 32/255|Col2|
|---|---|
|0|100 200 300 400 500 epochs|


(b) Adversarial normalized margins for varying perturbation sizes

Figure 3: Adversarial training for the CNN on all training examples of MNIST to perform multiclassification. The perturbations used for training are FGSM and ℓ -PGD perturbations with varying
_∞_
perturbation sizes ϵ = 16/255 and 32/255. The adversarial normalized margins and adversarial
training accuracies are evaluated with the corresponding perturbations used for training.


-----

and the margin for the adversarial data is defined to be

_γ˜m =_ min _γi._
_i_ 1,...,n [˜]
_∈{_ _}_

Then for the CNN, we conduct adversarial training using both FGSM and ℓ -PGD perturbations with
_∞_
perturbation sizes ϵ = 16/255 and 32/255 to perform multi-classification on all training examples of
MNIST to verify our claims and assumptions when the perturbation sizes are large. The loss function
is cross-entropy loss. It can be seen from Fig. 3(a) that the adversarial normalized margins keep
increasing after about 250 epochs, i.e., after the model fits all adversarial training examples, for all
adversarial perturbations. Fig. 3(b) clearly shows that the adversarial training accuracy can reach
100% for all perturbation types and sizes, even the large size ϵ = 32/255.

These experiments show that adversarial normalized margins are different whenever perturbation
types or sizes are different. All models adversarially trained with all perturbations and loss functions
can achieve 100% adversarial training accuracy, which verifies Assumption 4. Moreover, the trends
that adversarial normalized margins all keep increasing are clear, even long after the separation of
adversarial training examples. These results well support the claims of Theorem 5.

C.2 EXPERIMENTS ABOUT ℓ2-FGM PERTURBATIONS FOR BINARY CLASSIFICATION


1.0

adversarial training

0.5 standard training

0.0

−0.5

−1.0

−1.5

normalized margin for AEs −2.0

−2.5

−3.0

0 100 200 300 400
epochs


100

90

80

70

adversarial training accuracy 60

50

0 100 200 300 400
epochs


(a) ℓ2-FGM adversarial examples


(b) Adversarial training accuracy


Figure 4: Adversarial training for the 3-layer neural network on MNIST. The attack method is
_ℓ2-FGM with ϵ = 8. (a) Adversarial normalized margin for ℓ2-FGM adversarial examples during_
adversarial training and standard training. (b) Adversarial training accuracy for ℓ2-FGM adversarial
training.

In the MNIST dataset, we adversarially trained a 3-layer neural network with the same architecture as
that in Section 4 using SGD with constant learning rate and batch-size 64. To have a clear view about
the difference between the implicit bias for standard training and adversarial training, the adversarial
perturbation used for training is ℓ2-FGM perturbation with ϵ = 8 since even the standard model can
fit ℓ2-FGM adversarial examples easily in the MNIST dataset when the perturbation size ϵ is not
large. As a comparison, we also standardly trained a model with the same architecture to evaluate the
normalized margin for adversarial examples by attacking it with ℓ2-FGM during its training process.

As showed in Fig.4(b), the adversarilly training accuracy reaches 100% very quickly, which supports
Assumption 4. It can be seen in Fig.4(a) that the adversarial normalized margin keeps increasing during the adversarial training process while the standardly trained model maintains a lower adversarial
normalized margin.

C.2.1 EXPERIMENTS ABOUT EXPONENTIAL LOSS

For completeness, we also conduct experiment about the binary classification where the model is
adversarially trained with exponential loss (Fig.5) for ℓ2-FGM perturbations. We select the examples
with label ”2” and ”8” from MNIST and adversarially trained a 2-layer neural network using SGD
with constant learning rate 10[−][5]. The hidden layer is of size 10000 and the activation function is
ReLU. The normalized margin for adversarial data is defined exactly as in Section 3.2. The adversarial


-----

0.0005

adversarial training

0.0000

−0.0005

−0.0010

−0.0015

−0.0020

−0.0025

−0.0030

0 2000 4000 6000 8000 10000
epochs


Figure 5: Adversarial normalized margin during adversarial training for binary classification. The
loss function during training is taken as exponential loss.


1.6

1.4

1.2

1.0

0.8

0.6


1.8

1.6

1.4

1.2

1.0

0.8

0.6


5000 10000 15000 20000 25000 30000

||W1||F
||W2||F
||W3||F
||W4||F

epochs


5000 10000 15000 20000 25000 30000

||W1||F
||W2||F
||W3||F

epochs

(b) 3-layer deep linear network


(a) 4-layer deep linear network

1.8

1.6

1.4

1.2

1.0

0.8

0.6


5000 10000 15000 20000 25000 30000

||W1||F
||W2||F

epochs


(c) 2-layer deep linear network

Figure 6: Frobenius norms for weights of deep linear networks with different layers during adversarial
training. The perturbations are ℓ2 perturbations.


perturbations during the adversarial training are given by ℓ2-FGM with ϵ = 3. As showed in Fig.5,
although extremely slowly, the normalized margin for adversarial data gradually increases during the
adversarial training.

C.3 DIVERGENCE OF WEIGHT NORMS FOR DEEP LINEAR NETWORKS DURING ADVERSARIAL
TRAINING

We conduct adversarial training of deep linear networks with ℓ2 perturbation on a linearly separable
dataset. The loss function is logistic loss. Specifically, we use deep linear networks with layers 2, 3
and 4, respectively. Since the divergence is slow ( ln t), it is hard to observe that _Wk_ _F_ .
_∼_ _∥_ _∥_ _→∞_


-----

However, as showed in Fig. 6(a), Fig. 6(b) and Fig. 6(c), the trends that ∥Wk∥F for all layers of deep
linear networks with different layers keep increasing are clear. Furthermore, we can see that the ratios
_∥Wk∥2/∥Wk∥F for all layers of deep linear networks with different layers also keep increasing, as_
showed by Fig. 7(a), Fig. 7(b) and Fig. 7(c).


0.85

0.80

0.75

0.70

0.65


1.0

0.9

0.8

0.7

0.6


5000 10000 15000 20000 25000 30000

||W1||2/||W1||F
||W2||2/||W2||F
||W3||2/||W3||F
||W4||2/||W4||F

epochs

(a) 4-layer deep linear network

1.00

0.95

0.90

0.85

0.80


5000 10000 15000 20000 25000 30000

||W1||2/||W1||F
||W2||2/||W2||F
||W3||2/||W3||F

epochs

(b) 3-layer deep linear network


0.75


5000 10000 15000 20000 25000 30000

||W1||2/||W1||F
||W2||2/||W2||F

epochs

(c) 2-layer deep linear network


Figure 7: Ratios of 2-norm and Frobenius norm for weights of deep linear networks with different
layers during adversarial training. The perturbations are ℓ2 perturbations.


-----

