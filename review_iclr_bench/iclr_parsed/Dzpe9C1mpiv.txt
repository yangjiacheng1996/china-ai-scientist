# A UNIFIED WASSERSTEIN DISTRIBUTIONAL ROBUST## NESS FRAMEWORK FOR ADVERSARIAL TRAINING

Tuan Anh Bui[1], Trung Le[1], Quan Hung Tran[2], He Zhao[1], and Dinh Phung[1, 3]

1Monash University 2Adobe Research 3VinAI Research

ABSTRACT

It is well-known that deep neural networks (DNNs) are susceptible to adversarial attacks, exposing a severe fragility of deep learning systems. As the result,
adversarial training (AT) method, by incorporating adversarial examples during
training, represents a natural and effective approach to strengthen the robustness
of a DNN-based classifier. However, most AT-based methods, notably PGD-AT
and TRADES, typically seek a pointwise adversary that generates the worst-case
adversarial example by independently perturbing each data sample, as a way to
‚Äúprobe‚Äù the vulnerability of the classifier. Arguably, there are unexplored benefits
in considering such adversarial effects from an entire distribution. To this end, this
paper presents a unified framework that connects Wasserstein distributional robustness with current state-of-the-art AT methods. We introduce a new Wasserstein cost
function and a new series of risk functions, with which we show that standard AT
methods are special cases of their counterparts in our framework. This connection
leads to an intuitive relaxation and generalization of existing AT methods and
facilitates the development of a new family of distributional robustness AT-based
algorithms. Extensive experiments show that our distributional robustness AT
algorithms robustify further their standard AT counterparts in various settings.[1]

1 INTRODUCTION

Despite remarkable performances of DNN-based deep learning methods, even the state-of-the-art
(SOTA) models are reported to be vulnerable to adversarial attacks (Biggio et al., 2013; Szegedy
et al., 2014; Goodfellow et al., 2015; Madry et al., 2018; Athalye et al., 2018; Zhao et al., 2019b;
2021a), which is of significant concern given the large number of applications of deep learning in
real-world scenarios. Usually, adversarial attacks are generated by adding small perturbations to
benign data but to change the predictions of the target model. To enhance the robustness of DNNs,
various adversarial defense methods have been developed, recently Pang et al. (2019); Dong et al.
(2020); Zhang et al. (2020b); Bai et al. (2020). Among a number of adversarial defenses, Adversarial
Training (AT) is one of the most effective and widely-used approaches (Goodfellow et al., 2015;
Madry et al., 2018; Shafahi et al., 2019; Tram√®r & Boneh, 2019; Zhang & Wang, 2019; Xie et al.,
2020). In general, given a classifier, AT can be viewed as a robust optimization process (Ben-Tal
et al., 2009) of seeking a pointwise adversary (Staib & Jegelka, 2017) that generates the worst-case
adversarial example by independently perturbing each data sample.

Different from AT, Distributional Robustness (DR) (Delage & Ye, 2010; Duchi et al., 2021; Gao
et al., 2017; Gao & Kleywegt, 2016; Rahimian & Mehrotra, 2019) looks for a worst-case distribution
that generates adversarial examples from a known uncertainty set of distributions located in the
ball centered around the data distribution. To measure the distance between distributions, different
kinds of metrics have been considered in DR, such as f -divergence (Ben-Tal et al., 2013; Miyato
et al., 2015; Namkoong & Duchi, 2016) and Wasserstein distance (Shafieezadeh-Abadeh et al., 2015;
Blanchet et al., 2019; Kuhn et al., 2019), where the latter has shown advantages over others on
efficiency and simplicity (Staib & Jegelka, 2017; Sinha et al., 2018). Therefore, adversary in DR
does not look for the perturbation of a specific data sample, but moves the entire distribution around
the data distribution, thus, is expected to have better generalization than AT on unseen data samples

1Our code is available at https://github.com/tuananhbui89/Unified-Distributional-Robustness


-----

(Staib & Jegelka, 2017; Sinha et al., 2018). Conceptually and theoretically, DR can be viewed as
a generalization and better alternative to AT and several attempts (Staib & Jegelka, 2017; Sinha
et al., 2018) have shed light on connecting AT with DR. However, to the best of our knowledge,
practical DR approaches that achieve comparable peformance with SOTA AT methods on adversarial
robustness have not been developed yet.

To bridge this gap, we propose a unified framework that connects distributional robustness with
various SOTA AT methods. Built on top of Wasserstein Distributional Robustness (WDR), we
introduce a new cost function of the Wasserstein distances and propose a unified formulation of the
risk function in WDR, with which, we can generalize and encompass SOTA AT methods in the DR
setting, including PGD-AT (Madry et al., 2018), TRADES (Zhang et al., 2019), MART (Wang et al.,
2019) and AWP (Wu et al., 2020). With better generalization capacity of distributional robustness,
the resulted AT methods in our DR framework are shown to be able to achieve better adversarial
robustness than their standard AT counterparts.

The contributions of this paper are in both theoretical and practical aspects, summarized as follows:
**1) Theoretically, we propose a general framework that bridges distributional robustness and standard**
robustness achieved by AT. The proposed framework encompasses the DR versions of the SOTA AT
methods and we prove that these AT methods are special cases of their DR counterparts. 2) Practically,
motivated by our theoretical study, we develop a novel family of algorithms that generalize the AT
methods in the standard robustness setting, which have better generalization capacity. 3) Empirically,
we conduct extensive experiments on benchmark datasets, which show that the proposed AT methods
in the distributional robustness setting achieve better performance than standard AT methods.

2 PRELIMINARIES

2.1 DISTRIBUTIONAL ROBUSTNESS

Distributional Robustness (DR) is an emerging framework for learning and decision-making under
uncertainty, which seeks the worst-case expected loss among a ball of distributions, containing all
distributions that are close to the empirical distribution (Gao et al., 2017). Wasserstein DR has been
one of the most widely-used variant of DR, which has rich applications in (semi)-supervised learning
(Blanchet & Kang, 2020; Chen & Paschalidis, 2018; Yang, 2020), generative modeling (Huynh et al.,
2021; Dam et al., 2019), transfer learning and domain adaptation (Lee & Raginsky, 2018; Duchi et al.,
2019; Zhao et al., 2019a; Nguyen et al., 2021a;b; Le et al., 2021b;a), topic modeling (Zhao et al.,
2021b), and reinforcement learning (Abdullah et al., 2019; Smirnova et al., 2019; Derman & Mannor,
2020). For more comprehensive review, please refer to the surveys of Kuhn et al. (2019); Rahimian
& Mehrotra (2019). Here we consider a generic Polish space S endowed with a distribution P. Let
_f : S ‚Üí_ R be a real-valued (risk) function and c : S √ó S ‚Üí R+ be a cost function. Distributional
robustness setting aims to find the distribution Q in the vicinity of P and maximizes the risk in the E
form (Sinha et al., 2018; Blanchet & Murthy, 2019):

sup EQ [f (z)], (1)
Q:Wc(P,Q)<œµ

where œµ > 0 and Wc denotes the optimal transport (OT) cost, or a Wasserstein distance if c is a
metric, defined as:

_c (P, Q) :=_ inf _cdŒ≥,_ (2)
_W_ _Œ≥‚ààŒì(P,Q)_ Z

where Œì (P, Q) is the set of couplings whose marginals are P and Q. With the assumption that
_f ‚àà_ _L[1]_ (P) is upper semi-continuous and the cost c is a non-negative lower semi-continuous
satisfying c(z, z[‚Ä≤]) = 0 iff z = z[‚Ä≤], Sinha et al. (2018); Blanchet & Murthy (2019) show that the dual
form for Eq. (1) is:

inf _Œªœµ + E_ sup _._ (3)
_Œª‚â•0_  _z‚àºP_  _z[‚Ä≤][ {][f][ (][z][‚Ä≤][)][ ‚àí]_ _[Œªc][ (][z][‚Ä≤][, z][)][}]_

Sinha et al. (2018) further employs a Lagrangian for Wasserstein-based uncertainty sets to arrive at a
relaxed version with Œª ‚â• 0:


sup _._ (4)
_z[‚Ä≤][ {][f][ (][z][‚Ä≤][)][ ‚àí]_ _[Œªc][ (][z][‚Ä≤][, z][)][}]_



sup EQ [f (z)] _Œª_ _c (P, Q)_ = E
Q _{_ _‚àí_ _W_ _}_ _z‚àºP_


-----

2.2 ADVERSARIAL ROBUSTNESS WITH ADVERSARIAL TRAINING

In this paper, we are interested in image classification tasks and focus on the adversaries that add
small perturbations to the pixels of an image to generate attacks based on gradients, which are the
most popular and effective. FGSM (Goodfellow et al., 2015) and PGD (Madry et al., 2018) are
the most representative gradient-based attacks and PGD is the most widely-used one, due to its
effectiveness and simplicity. Now we consider a classification problem on the space S = X √ó Y
where X is the data space, Y is the label space. We would like to learn a classifier that predicts the
label of a datum well hŒ∏ : X ‚ÜíY. Learning of the classifier can be done by minimising its loss:
_‚Ñì_ (hŒ∏ (x), y), which can typically be the the cross-entropy loss. In addition to predicting well on
benign data, an adversarial defense aims to make the classifier robust against adversarial examples.
As the most successful approach, adversarial training is a straightforward method that creates and
then incorporates adversarial examples into the training process. With this general idea, different AT
methods vary in the way of picking which adversarial examples one should train on. Here we list
three widely-used AT methods.

**PGD-AT (Madry et al., 2018) seeks the most violating examples to improve model robustness:**

inf _Œ≤_ sup _CE_ _hŒ∏_ _x[‚Ä≤][]_ _, y_ + CE (hŒ∏ (x), y) _,_ (5)
_Œ∏_ [E][P] " _x[‚Ä≤]‚ààBœµ(x)_ #

    

where Bœµ (x) = _x[‚Ä≤]_ : c (x, x[‚Ä≤]) _œµ_, Œ≤ > 0 is the trade-off parameter and cross-entropy loss CE.
_{_ _X_ _‚â§_ _}_

**TRADES (Zhang et al., 2019) seeks the most divergent examples to improve model robustness:**

inf _Œ≤ sup_ _hŒ∏_ _x[‚Ä≤][]_ _, hŒ∏ (x)_ + CE (hŒ∏ (x), y) _,_ (6)
_Œ∏_ [E][P] _x[‚Ä≤][ D][KL]_

 
    

where x[‚Ä≤] _‚àà_ _Bœµ (x) and DKL is the usual Kullback-Leibler (KL) divergence._

**MART (Wang et al., 2019) takes into account prediction confidence:**


inf
_Œ∏_ [E][P]


inf
_Œ∏_ [E][P]


1 [hŒ∏ (x)]y sup _DKL_ _hŒ∏_ _x[‚Ä≤][]_ _, hŒ∏ (x)_ + BCE (hŒ∏ (x), y) _,_ (7)
_‚àí_  _x[‚Ä≤]‚ààBœµ(x)_      


inf
_Œ∏_ [E][P]


where BCE (hŒ∏ (x), y) is defined as: ‚àí log [hŒ∏ (x)]y _‚àí_ log (1 ‚àí maxk=Ã∏ _y [hŒ∏ (x)]k)._
 

2.3 CONNECTING DISTRIBUTIONAL ROBUSTNESS TO ADVERSARIAL TRAINING

To bridge distributional and adversarial robustness, Sinha et al. (2018) proposes an AT method,
named Wasserstein Risk Minimization (WRM), which generalizes PGD-AT through the principled
lens of distributionally robust optimization. For smooth loss functions, WRM enjoys convergence
guarantees similar to non-robust approaches while certifying performance even for the worst-case
population loss. Specifically, assume that P is a joint distribution that generates a pair z = (x, y)
where x and y . The cost function is defined as: c (z, z[‚Ä≤]) = c (x, x[‚Ä≤]) + **1** _y_ = y[‚Ä≤]
_‚ààX_ _‚ààY_ _X_ _‚àû√ó_ _{_ _Ã∏_ _}_
where z[‚Ä≤] = (x[‚Ä≤], y[‚Ä≤]), c : R+ is a cost function on, and 1 is the indicator function.
_X_ _X √ó X ‚Üí_ _X_ _{¬∑}_
One can define the risk function f as the loss of the classifier, i.e., f (z) := ‚Ñì (hŒ∏ (x), y). Together
with Eq. (1), attaining a robust classifier is to solve the following min-max problem:

inf sup EQ [‚Ñì (hŒ∏ (x), y)] . (8)
_Œ∏_ Q:Wc(P,Q)<œµ

The above equation shows the generalisation of WRM to PGD-AT. With Eq. (3) and Eq. (4), one can
arrive at Eq. (9) as below where Œª ‚â• 0 is a trade-off parameter:


inf
_Œ∏_ [E][P]


sup _._ (9)
_x[‚Ä≤][ {][‚Ñì]_ [(][h][Œ∏][ (][x][‚Ä≤][)][, y][)][ ‚àí] _[Œªc][X][ (][x][‚Ä≤][, x][)][}]_



3 PROPOSED UNIFIED DISTRIBUTION ROBUSTNESS FRAMEWORK

Although WRM (Sinha et al., 2018) sheds light on connecting distributional robustness with adversarial training, its framework and formulation is limited to PGD-AT, which cannot encompass more


-----

advanced AT methods including TRADES and MART. In this paper, we propose a unified formulation
for distributional robustness, which is a more general framework connecting state-of-the-art AT and
existing distributional robustness approaches where they become special cases.

Let P[d] be the data distribution that generates instance x ‚àº P[d] and P[l].|x [the conditional to generate]
label y ‚àº P[l].|x [given][ x][ where][ x][ ‚ààX] _[, y][ ‚ààY][. For our purpose, we consider the space][ S][ =][ X √óX √óY]_
and a joint distribution P on S consisting of samples (x, x, y) where x P[d] and y P[l]. _x[. Now]_
_‚ñ≥_ _‚àº_ _‚àº_ _|_
consider a distribution Q on S such that _c (Q, P_ ) < œµ. A draw z P will take the form
_W_ _‚ñ≥_ _‚àº_ _‚ñ≥_
_z = (x, x, y) whereas z[‚Ä≤]_ _‚àº_ Q will be z[‚Ä≤] = (x[‚Ä≤], x[‚Ä≤‚Ä≤], y[‚Ä≤]). We propose cost function c(z, z[‚Ä≤]) defined as:
_c(z, z[‚Ä≤]) = c_ (x, x[‚Ä≤]) + _c_ (x, x[‚Ä≤‚Ä≤]) + **1** _y_ = y[‚Ä≤] _,_ (10)
_X_ _‚àû√ó_ _X_ _‚àû√ó_ _{_ _Ã∏_ _}_
where we note that this cost function is non negative, satisfies c(z, z) = 0 and lower semi-continuous,
i.e., lim
_z[‚Ä≤]_ _z0_ [inf][ c][(][z, z][‚Ä≤][)][ ‚â•] _[c][(][z, z][0][)][.]_
_‚Üí_

With our new setting, it is useful to understand the ‚Äúvicinity‚Äùof P‚àÜ via the distribution OT-ball
condition _c (Q, P_ ) < œµ. Since there exists a transport plan Œ≥ Œì (P _, Q) s.t._ _cdŒ≥ < œµ and_
_W_ _‚ñ≥_ _‚àà_ _‚ñ≥_
_cand(z, z y[‚Ä≤][‚Ä≤])= is finite a.s. y, and second, Œ≥, this implies that if x[‚Ä≤]_ tends to be close to (z, z x[‚Ä≤]). To see why the later is the case, since ‚àº _Œ≥, then first, it is easy to see thatR_ _x P[‚Ä≤‚Ä≤]_ _[d]=is a x_
marginal of P on the first x in (x, x, y), therefore if Q[d] is the marginal of Q on x[‚Ä≤] in (x[‚Ä≤], x[‚Ä≤‚Ä≤], y[‚Ä≤])
_‚ñ≥_
then _d_ Q[d], P[d][] _d (Q, P_ ) < œµ, which explains the closeness between of x and x[‚Ä≤].
_W_ _‚â§W_ _‚ñ≥_

Given z[‚Ä≤]  = (x[‚Ä≤], x[‚Ä≤‚Ä≤], y[‚Ä≤]) Q where _c (Q, P_ ) < œµ, we define a unified risk function gŒ∏ (z[‚Ä≤]) w.r.t
_‚àº_ _W_ _‚ñ≥_
a classifier hŒ∏ that encompasses the unified distributional robustness (UDR) version for PGD-AT,
TRADES, and MART (cf Section 2.2):

-  UDR-PGD: gŒ∏ (z[‚Ä≤]) := CE (hŒ∏ (x[‚Ä≤‚Ä≤]), y[‚Ä≤]) + Œ≤CE (hŒ∏ (x[‚Ä≤]), y[‚Ä≤]).

-  UDR-TRADES: gŒ∏ (z[‚Ä≤]) := CE (hŒ∏ (x[‚Ä≤‚Ä≤]), y[‚Ä≤]) + Œ≤DKL (hŒ∏ (x[‚Ä≤]), hŒ∏ (x[‚Ä≤‚Ä≤])).

-  UDR-MART: gŒ∏ (z[‚Ä≤]) := BCE (hŒ∏ (x[‚Ä≤‚Ä≤]), y[‚Ä≤]) + Œ≤(1 [hŒ∏ (x[‚Ä≤‚Ä≤])]y)DKL (hŒ∏ (x[‚Ä≤]), hŒ∏ (x[‚Ä≤‚Ä≤])) .[2]
_‚àí_

Now we derive the primal and dual objectives for the proposed UDR framework. With the UDR risk
function gŒ∏(z[‚Ä≤]) defined previously, following Eq. (1) and Eq. (3), the primal (left) and dual (right)
forms of our UDR objective are:

infŒ∏ Q:Wc(supQ,P‚ñ≥)<œµ EQ [gŒ∏ (z[‚Ä≤])] = infŒ∏ _Œª[inf]‚â•0_ Œªœµ + EP‚ñ≥ supz[‚Ä≤][ {][g][Œ∏][ (][z][‚Ä≤][)][ ‚àí] _[Œªc][ (][z][‚Ä≤][, z][)][}]_ _._ (11)

With the cost function c defined in Eq. (10), the dual form in (11) can be rewritten as:

_Œ∏,Œªinf‚â•0_ Œªœµ + EP‚àÜ x[‚Ä≤],x[‚Ä≤‚Ä≤]sup=x,y[‚Ä≤]=y _{gŒ∏ (z[‚Ä≤]) ‚àí_ _ŒªcX (x[‚Ä≤], x)}_ =

inf _Œªœµ + EP_ sup (12)
_Œ∏,Œª_ 0 _x[‚Ä≤][ {][g][Œ∏][ (][x][‚Ä≤][, x, y][)][ ‚àí]_ _[Œªc][X][ (][x][‚Ä≤][, x][)][}]_
_‚â•_   

where we note that P is a distribution over pairs (x, y) for which x ‚àº P[d] and y ‚àº P[l].|x[. The min-max]
problem in Eq. (12) encompasses the PGD-AT, TRADES, and MART distributional robustness
counterparts on the choice of the function gŒ∏ (x[‚Ä≤], x, y) by simply choosing an appropriate gŒ∏ (x[‚Ä≤], x, y)
as shown in Section 2.3.

In what follows, we prove that standard PGD-AT, TRADES, and MART presented in Section 2 are
specific cases of their UDR counterparts by specifying corresponding cost functions. Given a cost
function c (e.g., L1, L2, and L ), we define a new cost function Àúc as:
_X_ _‚àû_ _X_

_c_ (x, x‚Ä≤) if c (x, x[‚Ä≤]) _œµ_
_cÀú_ (x, x[‚Ä≤]) = _X_ _X_ _‚â§_ (13)
_X_ otherwise.
‚àû

The cost function _cÀú_ is lower semi-continuous. By defining the ball Bœµ (x) :=
_X_
_x[‚Ä≤]_ : c (x, x[‚Ä≤]) _œµ_ = _x[‚Ä≤]_ : Àúc (x, x[‚Ä≤]) _œµ_, we achieve the following theorem on the relation
_{_ _X_ _‚â§_ _}_ _{_ _X_ _‚â§_ _}_
between distributional and standard robustness.

2To encompass MART with our framework, we assume a classifier is adversarially trained by Eq. (7) with
adversarial examples generated by supx‚Ä≤‚ààBœµ(x) DKL (hŒ∏ (x[‚Ä≤]), hŒ∏ (x))+ _BCE (hŒ∏ (x), y). This is slightly dif-_
ferent from the original MART, where the adversarial examples are generated by supx‚Ä≤‚ààBœµ(x) CE (hŒ∏ (x[‚Ä≤]), y).


-----

**Theorem 1. With the cost function Àúcx defined as above, the optimization problem:**

_Œ∏,Œªinf_ 0 _Œªœµ + EP_ supx[‚Ä≤][ {][g][Œ∏][ (][x][‚Ä≤][, x, y][)][ ‚àí] _[Œª]c[Àú]X (x[‚Ä≤], x)}_ (14)
_‚â•_   

_is equivalent to the optimization problem:_


sup _gŒ∏ (x[‚Ä≤], x, y)_
_x[‚Ä≤]‚ààBœµ(x)_


(15)


inf
_Œ∏_ [E][P]

_Proof. See Appendix A for the proof._


**Theoretical contribution and comparison to previous work. Theorem 1 says that the standard**
PGD-AT, TRADES, and MART are special cases of their UDR counterparts, which indicates that our
UDR versions of AT have a richer expressiveness capacity than the standard ones. Different from
WRM (Sinha et al., 2018), our proposed framework is developed based on theoretical foundation of
(Blanchet & Murthy, 2019). It is worth noting that the theoretical development is not trivial because
theory developed in Blanchet & Murthy (2019) is only valid for a bounded cost function, while the
cost function Àúc is unbounded. More specifically, the transformation from primal to dual forms in
Eq. (11) requires the cost function c to be bounded. In Theorem 2 in Appendix A, we prove this
primal-dual form transformation for the unbounded cost function Àúc, which is certainly not trivial.
_X_

Moreover, our UDR is fundamentally distinctive from WRM in its ability to adapt and learn Œª, while
this is a hyper-parameter in WRM. As a result of a fixed Œª, WRM is fundamentally same as PGD in
the sense that these methods can only utilize local information of relevant benign examples when
crafting adversarial examples. In contrast, our UDR can leverage both local and global information
of multiple benign examples when crafting adversarial examples due to the fact that Œª is adaptable
and captures the global information when solving the outer minimization in (14). Further explanation
can be found in Appendix B.

4 LEARNING ROBUST MODELS WITH UDR

In this section we introduce the details of how to learn robust models with UDR. To do this, we
first discuss the induced cost function Àúc defined as in Eq (13), which assists us in understanding
_X_
the connection between distributional and standard robustness approaches. We note that Àúc is non_X_
differential outside the perturbation ball (i.e., c (x[‚Ä≤], x) _œµ). To circumvent this, we introduce a_
_X_ _‚â•_
smoothed version ÀÜc to approximate Àúc as follows:
_X_ _X_

_cÀÜ_ (x, x[‚Ä≤]) := 1 _c_ (x, x[‚Ä≤]) < œµ _c_ (x, x[‚Ä≤]) + 1 _c_ (x, x[‚Ä≤]) _œµ_ _œµ +_ _[c][X][ (][x, x][‚Ä≤][)][ ‚àí]_ _[œµ]_ _,_ (16)
_X_ _{_ _X_ _}_ _X_ _{_ _X_ _‚â•_ _}_ _œÑ_
 

where œÑ > 0 is the temperature to control the growing rate of the cost function when x[‚Ä≤] goes out
of the perturbation ball. It is obvious that ÀÜc (x, x[‚Ä≤]) is continuous and approaches Àúc (x, x[‚Ä≤]) when
_X_ _X_
_œÑ_ 0. Using the smoothed function ÀÜc (x, x[‚Ä≤]) from Eq. (16), the final object of our UDR becomes:
_‚Üí_ _X_

_Œ∏,Œªinf_ 0 _Œªœµ + EP_ supx[‚Ä≤][ {][g][Œ∏][ (][x][‚Ä≤][, x, y][)][ ‚àí] _[Œª]c[ÀÜ]X (x[‚Ä≤], x)}_ _._ (17)
_‚â•_   

With this final objective, our training strategy involves three iterative steps at each iteration w.r.t. a
batch of data examples, which are shown in Algorithm 1.

**1. Craft adversarial examples w.r.t. the current model and the parameter Œª. Given the current**
model Œ∏ and the parameter Œª, we find the adversarial examples by solving:

_x[a]_ = argmaxx‚Ä≤ {gŒ∏(x[‚Ä≤], x, y) ‚àí _ŒªcÀÜX (x[‚Ä≤], x)},_ (18)

where different methods (i.e., UDR-PGD, UDR-TRADES, etc.) specifies gŒ∏(x[‚Ä≤], x, y) differently.

Similar to other AT methods like PGD-AT, we employ iterative gradient ascent update steps to
optimise to find x[a]. Specifically, we start from a random example inside the ball Bœµ and update in k
steps with the step size Œ∑ > 0. Since the magnitude of the gradient _x‚Ä≤_ _gŒ∏(x[‚Ä≤], x, y) is significantly_
_‚àá_
smaller than that of ‚àáx‚Ä≤ _cÀÜ X (x[‚Ä≤], x), we use sign (‚àáx‚Ä≤_ _cÀÜX (x[‚Ä≤], x)) in the update formula rather than_
_‚àáx‚Ä≤_ _cÀÜX (x[‚Ä≤], x). These steps are shown in 2(a) to 2(c) of Algorithm 1._


-----

An important difference from ours to
other AT methods is that at each update step, we do not apply any explicit
projecting operations onto the ball Bœµ.
Indeed, the parameter Œª controls how
distant x[a] to its benign counterpart
_x. Thus, this can be viewed as im-_
plicitly projecting onto a soft ball governed by the magnitude of the parameter Œª and the temperature œÑ . Specifically, when Œª becomes higher, the
crafted adversarial examples x[a] stay
closer to their benign counterparts x
and vice versa. When œÑ is set closer to
0, the smoothed cost function ÀÜc ap_X_
proximates the cost function Àúc more
_X_
tightly. Thus, our soft-ball projection
is more identical to the hard ball projection as in projected gradient ascent.


**Algorithm 1 The pseudocode of our proposed method.**
**Input: training set D, number of iterations T**, batch size N,
adversary parameters {k, œµ, Œ∑}
**for t = 1 to T do**

1. Sample mini-batch _xi, yi_ _i=1_
_{_ _}[N]_ _[‚àºD]_

2. Find adversarial examples {x[a]i _[}]i[N]=1_ [using Eq. (18)]

(a) Initialize randomly: x[0]i [=][ x][i][ +][ noise][ where]
_noise ‚àºU(‚àíœµ, œµ)_

(b) for n = 1 to k do

i. x[inter]i = x[n]i [+][ Œ∑][sign][ (][‚àá][x][g][Œ∏][(][x]i[n][, x][i][, y][i][))]

ii. x[n]i [+1] = x[inter]i _Œ∑Œª_ _xcÀÜ(x[inter]i_ _, xi)_
_‚àí_ _‚àá_

(c) Clip to valid range: x[a]i [=][ clip][(][x]i[k][,][ 0][,][ 1)]


3. Update parameter Œª using Eq. (19)

4. Update model parameter Œ∏ using Eq. (20)


**Output: model parameter Œ∏**

**2. Update the parameter Œª. Given**
current model Œ∏, we craft a batch of adversarial examples {x[a]i _[}]i[N]=1_ [corresponding to the benign]
examples {xi}i[N]=1 [crafted as above. Inspired by the Danskin‚Äôs theorem, we update][ Œª][ as follows:]


_œµ_
_‚àí_ _N[1]_


_cÀÜ_ (x[a]i _[, x][i][)]_
_X_
_i=1_

X


(19)


_Œªn = Œª ‚àí_ _Œ∑Œª_


where Œ∑Œª > 0 is a learning rate and Œªn represents the new value of Œª.

The proposed update of Œª is intuitive: if the adversarial examples stay close to their benign examples,
_i.e.,_ _i=1_ _c[ÀÜ]X (x[a]i_ _[, x][i][)][ < œµ][,][ Œª][ decreases to make them more distant to the benign examples and vice]_
_versa. Therefore the adversarial examples are crafted more diversely, which can further strengthen_
the robustness of the model.

[P][N]

**3. Update the model parameter Œ∏. Given the set of adversarial examples {x[a]i** _[}]i[N]=1_ [crafted as above]
and their benign examples {xi}i[N]=1 [with the labels][ {][y][i][}]i[N]=1[, we update the model parameter][ Œ∏][ to]
minimize EP [‚àágŒ∏(x[a], x, y)] using the current batches of adversarial and benign examples:


_Œ∏n = Œ∏_
_‚àí_ _[Œ∑]N[Œ∏]_


_‚àáŒ∏gŒ∏(x[a]i_ _[, x][i][, y][i][)][,]_ (20)
_i=1_

X


where Œ∑Œ∏ > 0 is a learning rate and Œ∏n specifies the new model parameter.

5 EXPERIMENTS

We use MNIST (LeCun et al., 1998), CIFAR10 and CIFAR100 (Krizhevsky et al., 2009) as the
benchmark datasets in our experiment. The inputs were normalized to [0, 1]. We apply padding 4
pixels at all borders before random cropping and random horizontal flips as used in Zhang et al.
(2019). We use both standard CNN architecture (Carlini & Wagner, 2017) and ResNet architecture
(He et al., 2016) in our experiment. The architecture and training setting are provided in Appendix D.

We compare our UDR with the SOTA AT methods, i.e., PGD-AT (Madry et al., 2018), TRADES
(Zhang et al., 2019) and MART (Wang et al., 2019). Because TRADES and MART performances
are strongly dependent on the trade-off ratio (i.e., Œ≤ in Eq. (6) and (7)) between natural loss
and robust loss, we use the original setting in their papers (CIFAR10/CIFAR100: Œ≤ = 6 for
TRADES/UDR-TRADES, Œ≤ = 5 for MART/UDR-MART; MNIST: Œ≤ = 1 for all the methods). We
also tried with the distributional robustness method WRM (Sinha et al., 2018). However, WRM
did not seem to obtain reasonable performance in our experiments. Its results can be found in
Appendix F. For all the AT methods, we use {k = 40, œµ = 0.3, Œ∑ = 0.01} for the MNIST dataset,


-----

Table 1: Comparisons of natural classification accuracy (Nat) and adversarial accuracies against
different attacks. Best scores are highlighted in boldface.

MNIST CIFAR10 CIFAR100

Nat PGD AA B&B Nat PGD AA B&B Nat PGD AA B&B
PGD-AT 99.4 94.0 88.9 91.3 **86.4** 46.0 42.5 44.2 72.4 41.7 39.3 39.6
UDR-PGD **99.5** **94.3** **90.0** **91.4** **86.4** **48.9** **44.8** **46.0** **73.5** **45.1** **41.9** **42.3**

TRADES **99.4** 95.1 90.9 92.2 80.8 51.9 49.1 50.2 68.1 49.7 46.7 47.2
UDR-TRADES **99.4** **96.9** **92.2** **95.2** **84.4** **53.6** **49.9** **51.0** **69.6** **49.9** **47.8** **48.7**

MART **99.3** 94.7 90.6 92.9 **81.9** 53.3 48.2 49.3 **68.1** 49.8 44.8 45.4
UDR-MART **99.3** **96.0** **92.3** **94.4** 80.1 **54.1** **49.1** **50.4** 67.5 **52.0** **48.5** **48.6**

Table 2: Robustness evaluation under different PGD attack
strengths œµ. Avg represents for the average improvement of our
DR methods over their counterparts.

MNIST
_œµ_ 0.3 0.325 0.35 0.375 0.4 0.425 Avg

PGD-AT 94.0 67.8 21.1 6.8 2.3 1.2 - 
UDR-PGD **94.3** **92.9** **90.1** **79.2** **22.3** **3.8** 31.57

TRADES 95.5 85.2 34.4 5.8 0.6 0.1 - 
UDR-TRADES **96.9** **96.9** **95.8** **95.1** **94.5** **88.5** 57.68


MART 94.7 66.1 9.4 0.9 0.2 0.1 - 
UDR-MART **96.0** **95.0** **94.1** **92.8** **88.8** **37.7** 55.5
CIFAR10


(a) Natural/robust accuracy trade-off

(b) Robustness in correlation with œÑ

Figure 1: Further analysis on parameter sensitivity.


_œµ_ 2558 25510 25512 25514 25516 25520 Avg

PGD-AT 46.0 33.7 23.7 15.2 9.5 3.6 - 
UDR-PGD **48.9** **36.4** **26.3** **18.5** **13.0** **7.1** 3.08

TRADES 51.9 42.5 33.7 25.7 18.9 9.1 - 
UDR-TRADES **53.6** **43.6** **35.2** **27.5** **20.7** **10.9** 1.62


MART 53.3 43.2 34.1 25.5 18.4 9.0 - 
UDR-MART **54.1** **46.0** **37.3** **29.7** **22.9** **12.2** 3.12
CIFAR100
_œµ_ 100010 100012.5 100015 100017.5 100020 100025 Avg

PGD-AT 41.7 34.5 27.8 22.6 18.2 11.7 - 
UDR-PGD **45.1** **38.3** **31.9** **26.2** **21.4** **14.2** 3.43

TRADES 49.7 44.3 39.9 35.2 31.2 23.5 - 
UDR-TRADES **49.9** **44.8** **40.3** **35.7** **31.7** **24.2** 0.47

MART 49.8 45.3 41.0 36.6 32.4 25.1 - 
UDR-MART **52.0** **47.8** **44.1** **40.2** **36.2** **29.4** 3.25


_{k = 10, œµ = 8/255, Œ∑ = 2/255} for the CIFAR10 dataset and {k = 10, œµ = 0.01, Œ∑ = 0.001} for_
the CIFAR100 dataset, where k is number of iteration, œµ is the distortion bound and Œ∑ is the step size
of the adversaries.

We use different SOTA attacks to evaluate the defense methods including: 1) PGD attack (Madry
et al., 2018) which is one of the most widely-used gradient based attacks. For PGD, we set k = 200
and œµ = 0.3, Œ∑ = 0.01 for MNIST, œµ = 8/255, Œ∑ = 2/255 for CIFAR10, and œµ = 0.01, Œ∑ = 0.001
for CIFAR100, which are the standard settings. 2) B&B attack (Brendel et al., 2019) which is a
decision based attack. Following Tramer et al. (2020), we initialized with the PGD attack with k = 20
and corresponding {œµ, Œ∑} then apply B&B attack with 200 steps. 3) Auto-Attack (AA) (Croce &
Hein, 2020b) which is an ensemble methods of four different attacks. We use œµ = 0.3, 8/255, 0.01,
for MNIST, CIFAR10, and CIFAR100, respectively. The distortion metric we use in our experiments
is l for all measures. We use the full test set for PGD and 1000 test samples for the other attacks.
_‚àû_


-----

Table 3: Adversarial accuracy in the blackbox settings. Avg represents for the average improvement
of our DR methods over their counterparts.

Source
PGD-AT UDR-P TRADES UDR-T MART UDR-M Avg
Target


PGD-AT -  -  61.6 61.6 61.7 62.4 - 
UDR-PGD -  -  **63.6** **63.4** **64.0** **64.1** 2.0

TRADES 61.2 61.3 -  -  58.9 59.8 - 
UDR-TRADES **62.7** **62.8** -  -  **61.1** **61.6** 1.8

MART 61.4 61.4 58.9 59.5 -  -  - 
UDR-MART **62.3** **62.1** **60.1** **60.5** -  -  1.0

5.1 MAIN RESULTS

**Whitebox Attacks with fixed œµ.** First, we compare the natural and robust accuracy of the AT
methods and their counterparts under our UDR framework, against several SOTA attacks. Note
that in this experiment, the attacks are with their standard settings. The result of this experiment
is shown in Table 1. It can be observed that for all the AT methods, our UDR versions are able to
boost the model robustness significantly against all the strong attack methods in comparison on all
the three datasets. These improvements clearly show that our UDR empowered AT methods achieve
the SOTA adversarial robustness performance. Specifically, our UDR-PGD‚Äôs improvement over PGD
on both CIFAR10 and CIFAR100 is over 3% against all the attacks. Similarly, our UDR-MART also
improves over MART with a 3% gap on CIFAR100.

**Whitebox Attacks with varied œµ. Recall that UDR is designed to have better generalization capacity**
than standard adversarial robustness. In this experiment, we exam the generalization capacity by
attacking the AT methods (including our UDR variants) with PGD with varied attack strength œµ while
keeping other parameters of PGD attack the same. This is a highly practical scenario where attackers
may use various attack strengths that are different from that the model is trained with. The results
of this experiment are shown in Table 2. We have the following remarks of the results: 1) All AT
methods perform reasonably well (our UDR variants are better than their counterparts) when PGD
attacks with the same œµ that these methods are trained on. This is shown in the first column on all the
datasets, whose results are in line with these in Table 1. 2) With increased œµ, the performance of all
the AT methods deteriorates, which is natural. However, the advantage of our UDR methods over
their counterparts becomes more and more significant. For example, when œµ = 0.375, all of our UDR
methods can achieve at least 80% robust accuracy on MNIST, while others can barely defend. This
clearly demonstrates the benefit of our UDR framework on generalization capacity.


**Blackbox Attacks.** To further exam the generalization of the UDR framework, we conduct
the experiment with the blackbox setting via
transferred attacks. Specifically, we use PGD
to generate adversarial examples according to
the model trained with a specific AT method,
i.e., the source method. Next, we use the generated adversarial examples to attack another
AT method, i.e., the target method. This is to
see whether an AT method can defend against
attacks generated from other models. We report
the results in Table 3. It can be seen that with
better generalization capacity, our UDR methods also outperform their standard counterparts
with a margin of 2% in the blackbox setting.


Table 4: Robustness evaluation against AutoAttack and PGD (k = 100) with WRN-34-10 on
the full test set of CIFAR10 dataset. (*) Omit the
cross-entropy loss of natural images. Detail can be
found in Appendix D.


erated adversarial examples to attack another Nat PGD AA C&W
AT method, i.e., the target method. This is to PGD-AT* 84.93 55.04 52.12 40.85
see whether an AT method can defend against UDR-PGD*TRADES 84.6085.70 55.7156.97 52.9853.82 47.3147.65
attacks generated from other models. We report UDR-TRADES 84.93 57.35 54.45 49.14
the results in Table 3. It can be seen that with AWP-AT 85.57 57.78 53.91 49.91
better generalization capacity, our UDR meth- UDR-AWP-AT 85.51 58.65 54.40 54.44

Zhang et al. (2020a) 84.52 -  53.51 - 

ods also outperform their standard counterparts Huang et al. (2020) 83.48 -  53.34 - 
with a margin of 2% in the blackbox setting. Zhang et al. (2019) 84.92 -  53.08 - 

Cui et al. (2021) 88.22 -  52.86 - 

**Results with WideResNet architecture.** We
would like to provide further experimental results on the CIFAR10 dataset with WideResNet (WRN-34-10) as shown in Table 4. It can be seen
that our distributional frameworks consistently outperform their standard AT counterparts in both
metrics. More specifically, our improvement over PGD-AT against Auto-Attack is around 0.8%,
while that for TRADES is 0.5%. To make a more concrete conclusion, we deploy our framework on a
recent SOTA standard AT which is AWP-AT Wu et al. (2020). The result shows that our distributional


-----

Table 5: Average norm L1 and L‚àû of the perturbation Œ¥ = |x[a] _‚àí_ _x|p_

_L1_ _L‚àû_ _p(Œ¥ ‚â§_ 0.9œµ) _p(Œ¥ ‚â§_ _œµ)_ _p(Œ¥ ‚â§_ 1.1œµ)

PGD 0.0270 0.031 19.7% 100% 100%
UDR-PGD at epoch 0th 0.0278 0.031 18.9% 100% 100%
UDR-PGD at epoch 200th 0.0301 0.034 19.5% 22.1% 100%

Table 6: Comparison to PGD-AT with different perturbation limitations.


2558 25510 25512 25514 25516 25520 Avg

PGD-AT at œµ = 0.031 46.0 33.7 23.7 15.2 9.5 3.6 - 
PGD-AT at œµ = 0.034 46.7 34.8 24.7 16.2 10.1 3.7 0.75
PGD-AT at œµ = 0.037 44.9 33.3 23.7 15.6 10.0 3.8 -0.07
UDR-PGD at œµ = 0.031 48.9 36.4 26.3 18.5 13.0 7.1 3.08

robustness version (UDR-AWP-AT) also improves its counterpart by 0.5%. With the same setting
(i.e., same architecture and without additional data), our UDR-TRADES and UDR-AWP-AT achieve
better robustness than recently listed methods on RobustBench (Croce et al., 2020).[3] Remarkably, the
additional experiment with C&W (L2) attack shows a significant improvement of our distributional
methods over standard AT by around 5%. More discussion can be found in Appendix F.

5.2 ANALYTICAL RESULTS

**Benefit of the soft-ball projection.** Here we would like to analytically study why our UDR
methods are better than standard AT methods, by taking UDR-PGD and PGD-AT as examples.
The visualization on the synthetic dataset can be found in Appendix E. Recall that one of the key
differences between UDR-PGD and PGD-AT is that the former uses the soft-ball projection and the
later use the hard-ball one, discussed in the second paragraph under Eq. (18). More specifically, Table
5 reports the average norm (L1 and L‚àû) of the perturbation Œ¥ = |x[a] _‚àí_ _x|p in PGD and our UDR-PGD._
It can be seen that: (i) At the beginning of the training process, there is no difference between the
norms of the perturbations generated by PGD and our UDR-PGD. More specifically, most of the pixels
lie on the edge of the hard-ball projection (i.e., p(0.9œµ ‚â§ _Œ¥ ‚â§_ _œµ) = p(Œ¥ ‚â§_ _œµ) ‚àí_ _p(Œ¥ ‚â§_ 0.9œµ) > 80%).
(ii) When our model converges, there are 77.9% pixels lying slightly beyond the hard-ball projection
(i.e., p(Œ¥ > œµ)). It is because our soft-ball projection can be adaptive based on the value of . This
flexibility helps the adversarial examples reach a better local optimum of the prediction loss, therefore,
benefits the adversarial training.

Next, we show that doing PGD adversarial training with larger œµ cannot achieve the same defence
performance as our methods with the soft-ball projection. We conduct more experiments with
PGD-AT with œµ = 0.034 (the final when our model converages) and œµ = 0.037 to show that simply
extending the hard-ball projection doesn‚Äôt benefit adversarial training. More specifically, the average
robustness improvement with œµ = 0.034 is 0.75%, while there is no improvement with œµ = 0.037.

**Parameter sensitivity of œÑ** **. Figure 1a and 1b show the our framework‚Äôs sensitivity to œÑ on CIFAR10**
under the PGD attack. It can be observed that overly small values of œÑ can hardly improve adversarial
robustness while overly big values of œÑ may hurt the natural performance (accnat = 68.7% with
_œÑ = 1.0). Empirically, we find that œÑ = 2Œ∑ performs well in our experiments._
6 CONCLUSIONS

In this paper, we have presented a new unified distributional robustness framework for adversarial
training, which unifies and generalizes standard AT approaches with improved adversarial robustness.
By defining a new family of risk functions, our framework facilitates the development of the distributional robustness counterparts of the SOTA AT methods including PGD-AT, TRADES, MART
and AWP. Moreover, we introduce a new cost function, which enables us to bridge the connections
between standard AT methods and their distributional robustness counterparts and to show that the
former ones can be viewed as the special cases of the later ones. Extensive experiments on the
benchmark datasets including MNIST, CIFAR10, CIFAR100 show that our proposed algorithms are
able to boost the model robustness against strong attacks with better generalization capacity.

3RobustBench reported a robust accuracy of 56.17% for AWP-TRADES version from Wu et al. (2020) which
is higher than ours but might not be used as a reference.


-----

ACKNOWLEDGEMENT

This work was partially supported by the Australian Defence Science and Technology (DST) Group under the Next Generation Technology Fund (NGTF) scheme. The
authors are grateful to the anonymous (meta) reviewers for their helpful comments.

REFERENCES

Mohammed Amin Abdullah, Hang Ren, Haitham Bou Ammar, Vladimir Milenkovic, Rui Luo,
Mingtian Zhang, and Jun Wang. Wasserstein robust reinforcement learning. arXiv preprint
_arXiv:1907.13196, 2019. 2.1_

Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. In International Conference on Machine
_Learning, pp. 274‚Äì283, 2018. 1_

Yang Bai, Yuyuan Zeng, Yong Jiang, Shu-Tao Xia, Xingjun Ma, and Yisen Wang. Improving
adversarial robustness via channel-wise activation suppressing. In International Conference on
_Learning Representations, 2020. 1_

Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Nemirovski. Robust optimization. Princeton
university press, 2009. 1

Aharon Ben-Tal, Dick Den Hertog, Anja De Waegenaere, Bertrand Melenberg, and Gijs Rennen.
Robust solutions of optimization problems affected by uncertain probabilities. Management
_Science, 59(2):341‚Äì357, 2013. 1_

Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic, Pavel Laskov, Giorgio
Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. In Joint European
_conference on machine learning and knowledge discovery in databases, pp. 387‚Äì402. Springer,_
2013. 1

Jose Blanchet and Yang Kang. Semi-supervised learning based on distributionally robust optimization.
_Data Analysis and Applications 3: Computational, Classification, Financial, Statistical and_
_Stochastic Methods, 5:1‚Äì33, 2020. 2.1_

Jose Blanchet and Karthyek Murthy. Quantifying distributional model risk via optimal transport.
_Mathematics of Operations Research, 44(2):565‚Äì600, 2019. 2.1, 2.1, 3, A, A, C_

Jose Blanchet, Yang Kang, and Karthyek Murthy. Robust wasserstein profile inference and applications to machine learning. Journal of Applied Probability, 56(3):830‚Äì857, 2019. 1

Wieland Brendel, Jonas Rauber, Matthias K√ºmmerer, Ivan Ustyuzhaninov, and Matthias Bethge.
Accurate, reliable and fast robustness evaluation. In Advances in Neural Information Processing
_Systems, pp. 12861‚Äì12871, 2019. 5, C_

Anh Bui, Trung Le, He Zhao, Paul Montague, Olivier deVel, Tamas Abraham, and Dinh Phung. Improving adversarial robustness by enforcing local and global compactness. In European Conference
_on Computer Vision, pp. 209‚Äì223. Springer, 2020. C_

Anh Bui, Trung Le, He Zhao, Paul Montague, Seyit Camtepe, and Dinh Phung. Understanding and
achieving efficient robustness with adversarial supervised contrastive learning. arXiv preprint
_arXiv:2101.10027, 2021a. C_

Anh Tuan Bui, Trung Le, He Zhao, Paul Montague, Olivier deVel, Tamas Abraham, and Dinh Phung.
Improving ensemble robustness by collaboratively promoting and demoting adversarial robustness.
In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pp. 6831‚Äì6839,
2021b. C

N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In 2017 ieee
_symposium on security and privacy (sp), pp. 39‚Äì57. IEEE, 2017. 5, D, F_


-----

Ruidi Chen and Ioannis C Paschalidis. A robust learning approach for regression models based on
distributionally robust optimization. Journal of Machine Learning Research, 19(13), 2018. 2.1

Francesco Croce and Matthias Hein. Minimally distorted adversarial examples with a fast adaptive
boundary attack. In International Conference on Machine Learning, pp. 2196‚Äì2205. PMLR, 2020a.
C

Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
of diverse parameter-free attacks. In International conference on machine learning, pp. 2206‚Äì2216.
PMLR, 2020b. 5

Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial
robustness benchmark. arXiv preprint arXiv:2010.09670, 2020. 5.1, D

Jiequan Cui, Shu Liu, Liwei Wang, and Jiaya Jia. Learnable boundary guided adversarial training.
_International Conference on Computer Vision, 2021. 4_

Nhan Dam, Quan Hoang, Trung Le, Tu Dinh Nguyen, Hung Bui, and Dinh Phung. Three-player
wasserstein gan via amortised duality. In International Joint Conference on Artificial Intelligence
_2019, pp. 2202‚Äì2208. Association for the Advancement of Artificial Intelligence (AAAI), 2019._
2.1

Erick Delage and Yinyu Ye. Distributionally robust optimization under moment uncertainty with
application to data-driven problems. Operations research, 58(3):595‚Äì612, 2010. 1

Esther Derman and Shie Mannor. Distributional robustness and regularization in reinforcement
learning. arXiv preprint arXiv:2003.02894, 2020. 2.1

Yinpeng Dong, Zhijie Deng, Tianyu Pang, Jun Zhu, and Hang Su. Adversarial distributional training
for robust deep learning. Advances in Neural Information Processing Systems, 33:8270‚Äì8283,
2020. 1, C

John C Duchi, Tatsunori Hashimoto, and Hongseok Namkoong. Distributionally robust losses against
mixture covariate shifts. Under review, 2019. 2.1

John C Duchi, Peter W Glynn, and Hongseok Namkoong. Statistics of robust optimization: A
generalized empirical likelihood approach. Mathematics of Operations Research, 2021. 1

Rui Gao and Anton J Kleywegt. Distributionally robust stochastic optimization with wasserstein
distance. arXiv preprint arXiv:1604.02199, 2016. 1

Rui Gao, Xi Chen, and Anton J Kleywegt. Wasserstein distributionally robust optimization and
variation regularization. arXiv preprint arXiv:1712.06050, 2017. 1, 2.1

Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In Yoshua Bengio and Yann LeCun (eds.), 3rd International Conference on Learning
_Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings,_
[2015. URL http://arxiv.org/abs/1412.6572. 1, 2.2, C](http://arxiv.org/abs/1412.6572)

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770‚Äì778, 2016. 5

Quan Hoang, Trung Le, and Dinh Phung. Parameterized rate-distortion stochastic encoder. In
Hal Daum√© III and Aarti Singh (eds.), Proceedings of the 37th International Conference on
_Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp. 4293‚Äì4303._
PMLR, 13‚Äì18 Jul 2020. C

Lang Huang, Chao Zhang, and Hongyang Zhang. Self-adaptive training: beyond empirical risk
minimization. Advances in Neural Information Processing Systems, 33, 2020. 4

Viet Huynh, Dinh Phung, and He Zhao. Optimal transport for deep generative models: State of the
art and research challenges. In The 30th International Joint Conference on Artificial Intelligence
_(IJCAI), pp. 4450‚Äì4457, 2021. 2.1_


-----

Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009. 5

Daniel Kuhn, Peyman Mohajerin Esfahani, Viet Anh Nguyen, and Soroosh Shafieezadeh-Abadeh.
Wasserstein distributionally robust optimization: Theory and applications in machine learning. In
_Operations Research & Management Science in the Age of Analytics, pp. 130‚Äì166. INFORMS,_
2019. 1, 2.1

Trung Le, Dat Do, Tuan Nguyen, Huy Nguyen, Hung Bui, Nhat Ho, and Dinh Phung. On label shift
in domain adaptation via wasserstein distance. arXiv preprint arXiv:2110.15520, 2021a. 2.1

Trung Le, Tuan Nguyen, Nhat Ho, Hung Bui, and Dinh Phung. Lamda: Label matching deep
domain adaptation. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International
_Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp._
6043‚Äì6054. PMLR, 18‚Äì24 Jul 2021b. 2.1

Trung Le, Anh Bui, Tue Le, He Zhao, Paul Montague, Quan Tran, and Phung Dinh. On globalview based defense via adversarial attack and defense risk guaranteed bounds. In International
_Conference on Artificial Intelligence and Statistics. PMLR, 2022. C_

Yann LeCun, L√©on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278‚Äì2324, 1998. 5

Jaeho Lee and Maxim Raginsky. Minimax statistical learning with wasserstein distances. In NeurIPS,
pp. 2692‚Äì2701, 2018. 2.1

Alexander Levine and Soheil Feizi. Wasserstein smoothing: Certified robustness against wasserstein
adversarial attacks. In International Conference on Artificial Intelligence and Statistics, pp.
3938‚Äì3947. PMLR, 2020. C

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
_Learning Representations, 2018. 1, 2.2, 5, C_

Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, Ken Nakae, and Shin Ishii. Distributional
smoothing with virtual adversarial training. arXiv preprint arXiv:1507.00677, 2015. 1

Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a
regularization method for supervised and semi-supervised learning. IEEE transactions on pattern
_analysis and machine intelligence, 41(8):1979‚Äì1993, 2018. C_

Amir Najafi, Shin-ichi Maeda, Masanori Koyama, and Takeru Miyato. Robustness to adversarial
perturbations in learning from incomplete data. Advances in Neural Information Processing
_Systems, 32:5541‚Äì5551, 2019. C_

Hongseok Namkoong and John C Duchi. Stochastic gradient methods for distributionally robust
optimization with f-divergences. In NIPS, volume 29, pp. 2208‚Äì2216, 2016. 1

Tuan Nguyen, Trung Le, Nhan Dam, Quan Hung Tran, Truyen Nguyen, and Dinh Phung. Tidot: A
teacher imitation learning approach for domain adaptation with optimal transport. In Zhi-Hua
Zhou (ed.), Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence,
_IJCAI-21, pp. 2862‚Äì2868. International Joint Conferences on Artificial Intelligence Organization,_
8 2021a. Main Track. 2.1

Tuan Nguyen, Trung Le, He Zhao, Quan Hung Tran, Truyen Nguyen, and Dinh Phung. Most:
Multi-source domain adaptation via optimal transport for student-teacher learning. In Uncertainty
_in Artificial Intelligence, pp. 225‚Äì235. PMLR, 2021b. 2.1_

Tianyu Pang, Kun Xu, Chao Du, Ning Chen, and Jun Zhu. Improving adversarial robustness via
promoting ensemble diversity. In International Conference on Machine Learning, pp. 4970‚Äì4979,
2019. 1, C

Tianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, and Jun Zhu. Bag of tricks for adversarial training.
In International Conference on Learning Representations, 2020. D


-----

Hamed Rahimian and Sanjay Mehrotra. Distributionally robust optimization: A review. arXiv
_preprint arXiv:1908.05659, 2019. 1, 2.1_

Leslie Rice, Eric Wong, and Zico Kolter. Overfitting in adversarially robust deep learning. In
_International Conference on Machine Learning, pp. 8093‚Äì8104. PMLR, 2020. D_

A. Shafahi, M. Najibi, M A. Ghiasi, Z. Xu, J. Dickerson, C. Studer, L S. Davis, G. Taylor, and
T. Goldstein. Adversarial training for free! In Advances in Neural Information Processing Systems,
pp. 3353‚Äì3364, 2019. 1

Soroosh Shafieezadeh-Abadeh, Peyman Mohajerin Esfahani, and Daniel Kuhn. Distributionally robust
logistic regression. In Proceedings of the 28th International Conference on Neural Information
_Processing Systems-Volume 1, pp. 1576‚Äì1584, 2015. 1_

Aman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with
principled adversarial training. In International Conference on Learning Representations, 2018. 1,
2.1, 2.1, 2.1, 2.3, 3, 3, 5, C, F

Elena Smirnova, Elvis Dohmatob, and J√©r√©mie Mary. Distributionally robust reinforcement learning.
_arXiv preprint arXiv:1902.08708, 2019. 2.1_

Matthew Staib and Stefanie Jegelka. Distributionally robust deep learning as a generalization of
adversarial training. NIPS workshop on Machine Learning and Computer Security, 2017. 1, C

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In Yoshua Bengio and Yann LeCun
(eds.), 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada,
_[April 14-16, 2014, Conference Track Proceedings, 2014. URL http://arxiv.org/abs/](http://arxiv.org/abs/1312.6199)_
[1312.6199. 1](http://arxiv.org/abs/1312.6199)

Nguyen-Duc Thanh, Le Trung, Zhao He, Cai Jianfei, and Phung Dinh. Particle-based adversarial local
distribution regularization. In International Conference on Artificial Intelligence and Statistics.
PMLR, 2022. C

Florian Tram√®r and Dan Boneh. Adversarial training and robustness for multiple perturbations. In
_Advances in Neural Information Processing Systems, pp. 5858‚Äì5868, 2019. 1_

Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to
adversarial example defenses. Advances in Neural Information Processing Systems, 33, 2020. 5

Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving
adversarial robustness requires revisiting misclassified examples. In International Conference on
_Learning Representations, 2019. 1, 2.2, 5, D_

Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust generalization. Advances in Neural Information Processing Systems, 33, 2020. 1, 5.1, 3, D

Cihang Xie, Mingxing Tan, Boqing Gong, Alan Yuille, and Quoc V Le. Smooth adversarial training.
_arXiv preprint arXiv:2006.14536, 2020. 1, C_

Insoon Yang. Wasserstein distributionally robust stochastic control: A data-driven approach. IEEE
_Transactions on Automatic Control, 2020. 2.1_

Haichao Zhang and Jianyu Wang. Defense against adversarial attacks using feature scattering-based
adversarial training. In Advances in Neural Information Processing Systems, pp. 1829‚Äì1839, 2019.
1, C

Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
Theoretically principled trade-off between robustness and accuracy. In Proceedings of the 36th
_International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning_
_Research, pp. 7472‚Äì7482, 2019. 1, 2.2, 5, 4, C_

Jingfeng Zhang, Xilie Xu, Bo Han, Gang Niu, Lizhen Cui, Masashi Sugiyama, and Mohan Kankanhalli. Attacks which do not kill training make adversarial learning stronger. In International
_Conference on Machine Learning, pp. 11278‚Äì11287. PMLR, 2020a. 4_


-----

Jingfeng Zhang, Jianing Zhu, Gang Niu, Bo Han, Masashi Sugiyama, and Mohan Kankanhalli.
Geometry-aware instance-reweighted adversarial training. In International Conference on Learning
_Representations, 2020b. 1_

Han Zhao, Remi Tachet Des Combes, Kun Zhang, and Geoffrey Gordon. On learning invariant
representations for domain adaptation. In International Conference on Machine Learning, pp.
7523‚Äì7532. PMLR, 2019a. 2.1

He Zhao, Trung Le, Paul Montague, Olivier De Vel, Tamas Abraham, and Dinh Phung. Perturbations are not enough: Generating adversarial examples with spatial distortions. arXiv preprint
_arXiv:1910.01329, 2019b. 1_

He Zhao, Thanh Nguyen, Trung Le, Paul Montague, Olivier De Vel, Tamas Abraham, and Dinh
Phung. Learning to attack with fewer pixels: A probabilistic post-hoc framework for refining
arbitrary dense adversarial attacks. arXiv preprint arXiv:2010.06131, 2021a. 1

He Zhao, Dinh Phung, Viet Huynh, Trung Le, and Wray Buntine. Neural topic model via optimal
[transport. In International Conference on Learning Representations (ICLR), 2021b. https:](https://openreview.net/forum?id=Oos98K9Lv-k)
[//openreview.net/forum?id=Oos98K9Lv-k. 2.1](https://openreview.net/forum?id=Oos98K9Lv-k)

A THEORETICAL DEVELOPMENT

**Theorem 1. With the cost function Àúc** _defined as above, the optimization problem:_
_X_

_Œ∏,Œªinf_ 0 _Œªœµ + EP_ supx[‚Ä≤][ {][g][Œ∏][ (][x][‚Ä≤][, x, y][)][ ‚àí] _[Œª]c[Àú]X (x[‚Ä≤], x)}_ (21)
_‚â•_   

_is equivalent to the optimization problem:_


sup _gŒ∏ (x[‚Ä≤], x, y)_
_x[‚Ä≤]‚ààBœµ(x)_


(22)

(23)

(24)

(25)


inf
_Œ∏_ [E][P]


_Proof. We need to prove that_


supx[‚Ä≤][ {][g][Œ∏][ (][x][‚Ä≤][, x, y][)][ ‚àí] _[Œª]c[Àú]X (x[‚Ä≤], x)}_ = EP



sup _gŒ∏ (x[‚Ä≤], x, y)_
_x[‚Ä≤]‚ààBœµ(x)_


_Œªœµ + EP_


inf
_Œª‚â•0_


By the definition of the cost function Àúc, the LHS of (23) can be rewritten as:
_X_

min (Œª>inf0 (Œªœµ + EP "x[‚Ä≤]‚ààsupBœµ(x) _gŒ∏_ _x[‚Ä≤], x, y_ _‚àí_ _ŒªcX_ _x[‚Ä≤], x_ #) _, EP_ supx[‚Ä≤][ g][Œ∏] _x[‚Ä≤], x, y_ _._

         [)]

Given any Œª > 0 and x[‚Ä≤] _‚àà_ _Bœµ (x), we have_

_Œªœµ + gŒ∏ (x[‚Ä≤], x, y) ‚àí_ _ŒªcX (x[‚Ä≤], x) = gŒ∏ (x[‚Ä≤], x, y) + Œª (œµ ‚àí_ _cX (x[‚Ä≤], x)) ‚â•_ EP [gŒ∏ (x[‚Ä≤], x, y)] .

Hence, we arrive at

_Œªœµ +_ sup _gŒ∏ (x[‚Ä≤], x, y)_ _Œªc_ (x[‚Ä≤], x) sup _gŒ∏ (x[‚Ä≤], x, y) ._
_x[‚Ä≤]‚ààBœµ(x)_ _{_ _‚àí_ _X_ _} ‚â•_ _x[‚Ä≤]‚ààBe(x)_


sup _gŒ∏ (x[‚Ä≤], x, y)_ _Œªc_ (x[‚Ä≤], x)
_x[‚Ä≤]‚ààBœµ(x)_ _{_ _‚àí_ _X_ _}_


sup _gŒ∏ (x[‚Ä≤], x, y)_
_x[‚Ä≤]‚ààBe(x)_


_Œªœµ + EP_

which follows that


_‚â•_ EP


_Œªœµ + EP_ "x[‚Ä≤] supBœµ(x) _{gŒ∏ (x[‚Ä≤], x, y) ‚àí_ _ŒªcX (x[‚Ä≤], x)}#)_

_‚àà_

_‚â•_ EP "x[‚Ä≤] supBe(x) EP [gŒ∏ (x[‚Ä≤], x, y)]# _._

_‚àà_


inf
_Œª>0_


-----

We now prove the inequality


_Œªœµ + EP_ "x[‚Ä≤] supBœµ(x) _{gŒ∏ (x[‚Ä≤], x, y) ‚àí_ _ŒªcX (x[‚Ä≤], x)}#)_

_‚àà_

= EP sup EP [gŒ∏ (x[‚Ä≤], x, y)] _._

"x[‚Ä≤] _Be(x)_ #

_‚àà_


lim
_Œª‚Üí0[+]_


Take a sequence _Œªn_ _n_ 1 0[+]. Given a feasible pair (x, y), we define
_{_ _}_ _‚â•_ _‚Üí_

_fn (x[‚Ä≤]; x, y) := gŒ∏ (x[‚Ä≤], x, y) + Œªn [œµ_ _c_ (x[‚Ä≤], x)], _x[‚Ä≤]_ _Bœµ (x) ._
_‚àí_ _X_ _‚àÄ_ _‚àà_

It is evident that fn (x[‚Ä≤]; x, y) converges pointwise to gŒ∏ (x[‚Ä≤], x, y) over the compact set Bœµ (x). Therefore, fn (x[‚Ä≤]; x, y) converges uniformly to gŒ∏ (x[‚Ä≤], x, y) on this set. This follows that

_‚àÄŒ± > 0, ‚àÉn0 = n (Œ±) : |fn (x[‚Ä≤]; x, y) ‚àí_ _gŒ∏ (x[‚Ä≤], x, y)| < Œ±, ‚àÄx[‚Ä≤]_ _‚àà_ _Bœµ (x), n ‚â•_ _n0._


Hence, we obtain for all x[‚Ä≤] _‚àà_ _Bœµ (x) and n ‚â•_ _n0:_

_gŒ∏ (x[‚Ä≤], x, y) ‚àí_ _Œ± < fn (x[‚Ä≤]; x, y) < gŒ∏ (x[‚Ä≤], x, y) + Œ±._

This leads to the following for all n _n0:_
_‚â•_

_x[‚Ä≤]‚ààsupBœµ(x)_ _gŒ∏ (x[‚Ä≤], x, y) ‚àí_ _Œ± ‚â§_ _x[‚Ä≤]‚ààsupBœµ(x)_ _fn (x[‚Ä≤]; x, y) ‚â§_ _x[‚Ä≤]‚ààsupBœµ(x)_ _gŒ∏ (x[‚Ä≤], x, y) + Œ±._

Therefore, we obtain:

lim sup _fn (x[‚Ä≤]; x, y) =_ sup _gŒ∏ (x[‚Ä≤], x, y)_
_n‚Üí‚àû_ _x[‚Ä≤]‚ààBœµ(x)_ _x[‚Ä≤]‚ààBœµ(x)_

for all feasible pairs (x, y), which further means that


sup _fn (x[‚Ä≤]; x, y)_
_x[‚Ä≤]‚ààBœµ(x)_


sup _gŒ∏ (x[‚Ä≤], x, y)_
_x[‚Ä≤]‚ààBœµ(x)_


lim
_n‚Üí‚àû_ [E][P]


= EP


or equivalently


sup _gŒ∏ (x[‚Ä≤], x, y)_ _Œªnc_ (x[‚Ä≤], x)
_x[‚Ä≤]_ _Bœµ(x)_ _{_ _‚àí_ _X_ _}##_
_‚àà_


lim
_n‚Üí‚àû_ [E][P]


_Œªnœµ + EP_


= EP


sup _gŒ∏ (x[‚Ä≤], x, y)_
_x[‚Ä≤]‚ààBœµ(x)_


(26)

(27)

(28)


Because Eq. (26) holds for every sequence _Œªn_ _n_ 1 0[+], we reach
_{_ _}_ _‚â•_ _‚Üí_


sup _gŒ∏ (x[‚Ä≤], x, y)_ _Œªc_ (x[‚Ä≤], x)
_x[‚Ä≤]_ _Bœµ(x)_ _{_ _‚àí_ _X_ _}#)_
_‚àà_


_Œªœµ + EP_


lim
_Œª‚Üí0[+]_

= EP


sup EP [gŒ∏ (x[‚Ä≤], x, y)]

"x[‚Ä≤] _Be(x)_

_‚àà_


By combining (25) and (27), we reach


_Œªœµ + EP_ "x[‚Ä≤] supBœµ(x) _{gŒ∏ (x[‚Ä≤], x, y) ‚àí_ _ŒªcX (x[‚Ä≤], x)}#)_

_‚àà_

= EP sup EP [gŒ∏ (x[‚Ä≤], x, y)] _._

"x[‚Ä≤] _Be(x)_ #

_‚àà_


inf
_Œª>0_


-----

Finally, we have

inf
_Œª‚â•0_ 


sup _c_ (x[‚Ä≤], x)
_x[‚Ä≤][ {][g][Œ∏][ (][x][‚Ä≤][, x, y][)][ ‚àí]_ _[Œª][Àú]X_ _}_



_Œªœµ + EP_


_x[‚Ä≤]_

+ EP "x[‚Ä≤] supBœµ(x) _{gŒ∏ (x[‚Ä≤], x, y) ‚àí_ _ŒªcX (x[‚Ä≤], x)}#)_ _, EP_

_‚àà_

sup EP [gŒ∏ (x[‚Ä≤], x, y)] _, EP_ sup
_x[‚Ä≤]‚ààBe(x)_ #  _x[‚Ä≤][ g][Œ∏][ (][x][‚Ä≤][, x, y][)][)]_


sup
_x[‚Ä≤][ g][Œ∏][ (][x][‚Ä≤][, x, y][)]_
[)]


_Œªœµ + EP_


= min


inf
_Œª>0_


EP


= min

(

= EP


sup EP [gŒ∏ (x[‚Ä≤], x, y)]
_x[‚Ä≤]‚ààBe(x)_


That concludes our proof.

One of most technical challenge we need to bypass in our work is that in theory developed in Blanchet
& Murthy (2019), to equivalently transform the primal form to the dual form, it requires the cost
function to be finite. In the following theorem, we reprove the equivalence of the primal and dual
forms in our context.
**Theorem 2. Assume that the function g is upper-bounded by a number L. We have the following**
_equality between the primal form and dual form_

Q:Wc(supQ,P‚ñ≥)<œµ EQ [g (z[‚Ä≤])] = infŒª‚â•0 Œªœµ + EP‚ñ≥ supz[‚Ä≤][ {][g][ (][z][‚Ä≤][)][ ‚àí] _[Œªc][ (][z][‚Ä≤][, z][)][}]_ _,_

_where z = (x, x, y), z[‚Ä≤]_ = (x[‚Ä≤], x[‚Ä≤‚Ä≤], y[‚Ä≤]), and we have defined


_c (z, z[‚Ä≤]) = Àúc_ (x, x[‚Ä≤]) + Àúc (x, x[‚Ä≤‚Ä≤]) + **1** _y_ = y[‚Ä≤] _,_
_X_ _‚àû√ó_ _X_ _‚àû√ó_ _{_ _Ã∏_ _}_

_for which we have defined_

_c_ (x, x‚Ä≤) _if c_ (x, x[‚Ä≤]) _œµ_
_cÀú_ (x, x[‚Ä≤]) = _X_ _X_ _‚â§_
_X_ _otherwise._
‚àû

_Proof. Given a positive integer number n > 0, we define the following metrics:_

_c[n]_ (z, z[‚Ä≤]) = Àúc[n] [(][x, x][‚Ä≤][) +][ ‚àû√ó][ Àú]c[n] [(][x, x][‚Ä≤‚Ä≤][) +][ ‚àû√ó][ 1][ {][y][ Ã∏][=][ y][‚Ä≤][}][,]
_X_ _X_

_c_ (x, x‚Ä≤) if c (x, x[‚Ä≤]) < œµ.
_cÀú[n]_ [(][x, x][‚Ä≤][) =] _X_ _X_
_X_ _n_ otherwise.



We have Àúc[n] _c_ and c[n] _c. We now prove that_
_X_ _[‚Üó]_ [Àú]X _‚Üó_

Q:W(supQ,P‚ñ≥)<œµ EQ [g (z[‚Ä≤])] = infn Q:Wcnsup(Q,P‚ñ≥)<œµ EQ [g (z[‚Ä≤])] .

In fact, for each n, we have c[n] _c._ Therefore, _cn (Q, P_ ) _c (Q, P_ ), hence
Q : _c (Q, P_ ) < œµ Q : _cn (Q‚â§, P_ ) < œµ, implying that W _‚ñ≥_ _‚â§_ _W_ _‚ñ≥_
_{_ _W_ _‚ñ≥_ _} ‚äÇ{_ _W_ _‚ñ≥_ _}_

Q:W(supQ,P‚ñ≥)<œµ EQ [g (z[‚Ä≤])] ‚â§ Q:Wcnsup(Q,P‚ñ≥)<œµ EQ [g (z[‚Ä≤])] .

Q:W(supQ,P‚ñ≥)<œµ EQ [g (z[‚Ä≤])] ‚â§ infn Q:Wcnsup(Q,P‚ñ≥)<œµ EQ [g (z[‚Ä≤])] .

Let us define

_A =_ (x,y) (z, z[‚Ä≤]) : z = (x, x, y), z[‚Ä≤] = (x[‚Ä≤], x[‚Ä≤‚Ä≤], y[‚Ä≤]), c (x, x[‚Ä≤]) < œµ, x[‚Ä≤‚Ä≤] = x, y[‚Ä≤] = y _,_
_‚à™_ _‚ààD {_ _X_ _}_

_B =_ (x,y) (z, z[‚Ä≤]) : z = (x, x, y), z[‚Ä≤] = (x[‚Ä≤], x[‚Ä≤‚Ä≤], y[‚Ä≤]), c (x, x[‚Ä≤]) _œµ, x[‚Ä≤‚Ä≤]_ = x, y[‚Ä≤] = y _._
_‚à™_ _‚ààD {_ _X_ _‚â•_ _}_


-----

To simplify our proof, without generalization ability, for each n, we denote Qn as the distribution in
which admits{Q : Wcn (Q, P P‚ñ≥)and < œµ Q}n that peaks as its marginals. Note that because EQ [gŒ∏ (z[‚Ä≤])] and Œ≥n as the optimal transport plan ofcn (Qn, P ) < œµ, the support of Wcn (Qn, P Œ≥‚ñ≥n)
_‚ñ≥_ _W_ _‚ñ≥_
almost surely determines on A ‚à™ _B.We then have_

_cn (Qn, P_ ) = _c[n]_ (z, z[‚Ä≤]) dŒ≥n (z, z[‚Ä≤])
_W_ _‚ñ≥_
Z


_c[n]_ (z, z[‚Ä≤]) dŒ≥n (z, z[‚Ä≤]) +

_c_ (x, x[‚Ä≤]) dŒ≥n (z, z[‚Ä≤]) +
_X_


_c[n]_ (z, z[‚Ä≤]) dŒ≥n (z, z[‚Ä≤])

_ndŒ≥n (z, z[‚Ä≤])_


= _c_ (x, x[‚Ä≤]) dŒ≥n (z, z[‚Ä≤]) + nŒ≥n (B) < œµ.

_A_ _X_

Z

Therefore, we obtain: Œ≥n (B) < _n[œµ]_ [. We now define][ ¬Ø]Œ≥n as a restricted measure of Œ≥n on A, meaning

that ¬ØŒ≥n (C) = _[Œ≥][n][(][A]Œ≥n[)+](A[Œ≥][n])_ [(][B][)] _Œ≥n (C) =_ 1 + o _n[‚àí][1][]_ _Œ≥n (C) for any measure set C_ _A, where_

1 [] _‚äÇ_
_‚àí_
limn _o_ _n_ = 0. Let Pn as marginal distribution of    Qn corresponding to the dimensions of z[‚Ä≤].
_‚Üí‚àû_
It appears that


_c (Pn, P_ )
_W_ _‚ñ≥_ _‚â§_

(1)


_c (z, z[‚Ä≤]) dŒ≥¬Øn (z, z[‚Ä≤]) +_
Z

_c_ (x, x[‚Ä≤]) dŒ≥¬Øn (z, z[‚Ä≤]) <
_X_


_c (z, z[‚Ä≤]) dŒ≥¬Øn (z, z[‚Ä≤])_

_œµdŒ≥¬Øn (z, z[‚Ä≤]) = œµ._

Z


(1)
Note that we have = because ¬ØŒ≥n (B) = 0.

This implies that Pn Q : _c (Q, P_ ) < œµ, which follows that
_‚àà{_ _W_ _‚ñ≥_ _}_

Q:W(supQ,P‚ñ≥)<œµ EQ [g (z[‚Ä≤])] ‚â• EPn [gŒ∏ (z[‚Ä≤])] = EŒ≥¬Øn [[][g][ (][z][‚Ä≤][)]]


_g (z[‚Ä≤]) dŒ≥¬Øn (z, z[‚Ä≤]) +_


_g (z[‚Ä≤]) dŒ≥¬Øn (z, z[‚Ä≤])_


_g (z[‚Ä≤]) dŒ≥¬Øn (z, z[‚Ä≤]) =_ _[Œ≥][n][ (][A][) +][ Œ≥][n][ (][B][)]_ _g (z[‚Ä≤]) dŒ≥n (z, z[‚Ä≤])_
_A_ _Œ≥n (A)_ _A_

Z

1 [ Z]
_‚àí_
1 + o n _A‚à™B_ _g (z[‚Ä≤]) dŒ≥n (z, z[‚Ä≤]) ‚àí_ ZB _g (z[‚Ä≤]) dŒ≥n (z, z[‚Ä≤])_

1 [ Z]
_‚àí_
1 + o _n_ _A_ _B_ _g (z[‚Ä≤]) dQn (z[‚Ä≤]) ‚àí_ _B_ _g (z[‚Ä≤]) dŒ≥n (z, z[‚Ä≤])_
 _‚à™_ Z 

1 [ "]
_‚àí_
1 + o n Q:Wcnsup(Q,P‚ñ≥)<œµ EQ [gŒ∏ (z[‚Ä≤])] ‚àí ZB _LdŒ≥n (z, z[‚Ä≤])_


(1)


1 [ "]
_‚àí_
Q:W(supQ,P‚ñ≥)<œµ EQ [g (z[‚Ä≤])] ‚â• 1 + o n Q:Wcnsup(Q,P‚ñ≥)<œµ EQ [gŒ∏ (z[‚Ä≤])] ‚àí _LŒ≥n (B)#_

(2) 1 [ "]
_‚àí_
1 + o _n_ sup EQ [gŒ∏ (z[‚Ä≤])] _._
_‚â•_   Q:Wcn (Q,P‚ñ≥)<œµ _‚àí_ _[Lœµ]n_ #

Note that we have (1)= due to ¬ØŒ≥n (B) = 0 and (2)‚â• due to Œ≥n (B) < _nœµ_ [. Therefore, we reach the]

conclusion
Q:W(supQ,P‚ñ≥)<œµ EQ [gŒ∏ (z[‚Ä≤])] = infn Q:Wcnsup(Q,P‚ñ≥)<œµ EQ [gŒ∏ (z[‚Ä≤])] .


-----

Next, we apply primal-dual form in Blanchet & Murthy (2019) for the finite metric Àúc[n] [to reach]
_X_

_Wcn_ (supQ,P‚ñ≥)<œµ EQ [gŒ∏ (z[‚Ä≤])] = infŒª‚â•0 Œªœµ + EP‚ñ≥ supz[‚Ä≤][ {][g][Œ∏][ (][z][‚Ä≤][)][ ‚àí] _[Œªc][n][ (][z][‚Ä≤][, z][)][}]_ _._

Finally, taking n ‚Üí‚àû and noting that c[n] _‚Üó_ _c, we reach the conclusion._

B FURTHER EXPLANATION WHY OUR UDR CAN UTILIZE GLOBAL
INFORMATION AND THE ADVANTAGE OF SOFT-BALL

**Algorithm 1 The pseudocode of our proposed method.**
**Input: training set D, number of iterations T**, batch size N, adversary parameters {k, œµ, Œ∑}
**for t = 1 to T do**


1. Sample mini-batch _xi, yi_ _i=1_
_{_ _}[N]_ _[‚àºD]_

2. Find adversarial examples {x[a]i _[}]i[N]=1_ [using Eq. (18)]

(a) Initialize randomly: x[0]i [=][ x][i][ +][ noise][ where][ noise][ ‚àºU][(][‚àí][œµ, œµ][)]

(b) for n = 1 to k do

i. x[inter]i = x[n]i [+][ Œ∑][sign][ (][‚àá][x][g][Œ∏][(][x]i[n][, x][i][, y][i][))]

ii. x[n]i [+1] = x[inter]i _Œ∑Œª_ _xcÀÜ(x[inter]i_ _, xi)_
_‚àí_ _‚àá_

(c) Clip to valid range: x[a]i [=][ clip][(][x]i[k][,][ 0][,][ 1)]

3. Update parameter Œª using Eq. (19)

4. Update model parameter Œ∏ using Eq. (20)

**Output: model parameter Œ∏**

The advantage of our soft ball comes from the adaptive capability of Œª, which is controlled by a
global effect regarding how far adversarial examples x[a]i [from benign examples][ x][i][. Let us revisit]
Algorithm 1. In the step 2.(b).i, we update

_x[inter]i_ = x[n]i [+][ Œ∑][sign][ (][‚àá][x][g][Œ∏][(][x]i[n][, x][i][, y][i][))]

with the aim to find x[inter]i that can maximize gŒ∏( _, xi, yi) as in the standard versions._

_¬∑_

Furthermore, in the step 2.(b).ii, we update


_x[n]i_ [+1] = x[inter]i _Œ∑Œª_ _xcÀÜ(x[inter]i_ _, xi) = x[inter]i_ _Œ∑Œª_ _x[inter]i_ _xi_
_‚àí_ _‚àá_ _‚àí_ _‚àí_

= (1 _Œ∑Œª) x[inter]i_ + Œ∑Œªxi,    (29)
_‚àí_

where we assume L2 cost c(x, x[‚Ä≤]) = [1]2 _[‚à•][x][ ‚àí]_ _[x][‚Ä≤][‚à•][2][ is used. It is evident that][ x]i[n][+1]_ is an interpolation

point of x[inter]i and xi, hence x[n]i [+1] is drawn back to xi wherein the drawn-back amount is proportional
to Œ∑Œª.

We now revisit the formula to update Œª as Eq. (19)


_œµ_
_‚àí_ _N[1]_


_cÀÜ_ (x[a]i _[, x][i][)]_
_X_
_i=1_

X


_Œªn = Œª ‚àí_ _Œ∑Œª_


which indicates that Œª is globally controlled. More specifically, if average distance from x[a]i [to][ x][i][ (i.e.,]

_N1_ _Ni=1_ _c[ÀÜ]X (x[a]i_ _[, x][i][)][) is less than][ œµ][ (i.e., adversarial examples are globally close to benign examples),]_
_Œª is adapted decreasingly. Linking with the formula in Eq. (29), in this case, x[n]i_ [+1] gets back to xi
P
less aggressively to maintain the distance between x[a]i [and][ x][i][. Otherwise, adversarial examples are]
globally far from benign examples, Œª is adapted increasingly. In this case, x[n]i [+1] gets back to xi more
aggressively to reduce more the distance between x[a]i [and][ x][i][.]


-----

C RELATED WORK

**Adversarial Attacks. In this paper, we are interested in image classification tasks and focus on**
the adversaries that add small perturbations to the pixels of an image to generate attacks based on
gradients, which are the most popular and effective. FGSM (Goodfellow et al., 2015) and PGD
(Madry et al., 2018) are the most representative gradient-based attacks and PGD is the most widelyused one, due to its effectiveness and simplicity. Recently, there are several variants of PGD that
achieve improved performance, for example, Auto-Attack by ensembling PGD with other attacks
(Croce & Hein, 2020a) and the B&B method (Brendel et al., 2019) by attacking with decision-based
boundary initialized with PGD. Along with PGD, these attacks have been considered as benchmark
attacks for adversarial robustness.

**Adversarial defenses. Among various kinds of defense approaches, Adversarial Training (AT),**
originating in Goodfellow et al. (2015), has drawn the most research attention. Given its effectiveness
and efficiency, many variants of AT have been proposed with (1) different types of adversarial
examples (e.g., the worst-case examples as in Goodfellow et al. (2015) or most divergent examples as
in Zhang et al. (2019)), (2) different searching strategies (e.g., non-iterative FGSM and Rand FGSM
(Madry et al., 2018)), (3) additional regularizations (e.g., adding constraints in the latent space (Zhang
& Wang, 2019; Bui et al., 2020; 2021a; Hoang et al., 2020)), and (4) different model architectures
(e.g., activation function (Xie et al., 2020) or ensemble models (Pang et al., 2019; Bui et al., 2021b)).

**Distributional robustness. There have been a few works attempting to connect DR with adversarial**
machine learning or improve adversarial robustness based on the ideas of DR (Sinha et al., 2018;
Staib & Jegelka, 2017; Miyato et al., 2018; Zhang & Wang, 2019; Najafi et al., 2019; Levine &
Feizi, 2020; Le et al., 2022; Thanh et al., 2022). A recent work of Dong et al. (2020) proposes a
new AT algorithm by constructing a distribution over each data sample to model the adversarial
examples around it, which is still in the category of pointwise adversary (Sinha et al., 2018) and has
no relations to DR. Although its aim of enhancing adversarial robustness is visually related ours, its
mythology is different from ours. Therefore, we consider Sinha et al. (2018); Staib & Jegelka (2017)
as the most relevant ones to ours. Specifically, both works leverage the dual form of Wasserstein
DR (Blanchet & Murthy, 2019) for searching worst-case perturbations for AT, where Sinha et al.
(2018) (WRM) focuses on certified robustness with comprehensive study on the tradeoffs between
complexity, generality, guarantees, and speed, while Staib & Jegelka (2017) (FDRO) points out that
Wasserstein robust optimization can be viewed as the generalization to standard AT.

Although our study is inspired by the two works, there are significant differences and new results of
ours: 1) We introduce a new Wasserstein cost function and a new series of risk functions in WDR,
which facilitate our framework to generalize and encompass many SOTA AT methods. While WRM
can be viewed as the generalization to PGD-AT only. 2) Most importantly, although WDR has been
demonstrated to have superior properties over standard AT in the two papers, unfortunately, WRM
and FDRO have not been observed to outperform standard AT methods. For example, the experiments
of FDRO show that adversarial robustness on MNIST of WRM and FDRO is worse than that of AT
with PGD and iterative-FGSM (Staib & Jegelka, 2017). Moreover, WRM and FDRO‚Äôs effectiveness
either on more complex colored images (e.g., CIFAR10) or against more advanced attacks (e.g.,
Auto-Attack) has not been carefully studied yet. On the contrary, we conduct extensive experiments
to show the SOTA performance of our proposed algorithms.

D EXPERIMENTAL SETTINGS

**For MNIST dataset.** We use a standard CNN architecture for the MNIST dataset which is identical
with that in Carlini & Wagner (2017). We use the SGD optimizer with momentum 0.9, starting
learning rate 1e-2 and reduce the learning rate (√ó0.1) at epoch {55, 75, 90}. We train with 100
epochs.

**For CIFAR10 and CIFAR100 dataset with ResNet18 architecture.** We use the ResNet18 for
the CIFAR10 and CIFAR100 dataset. We use the SGD optimizer with momentum 0.9, weight decay
3.5e-3 as in the official implementation from Wang et al. (2019).[4] The starting learning rate 1e-2 and
reduce the learning rate (√ó0.1) at epoch {75, 90, 100}. We train with 200 epochs.

4https://github.com/YisenWang/MART


-----

**For hard/soft-ball projection experiments.** For PGD-AT, we use the following three adhoc strategies for œµ: 1) Fixing œµ = 8/255; 2) Fixing œµ = 16/255; 3) Gradually increasing/decreasing œµfrom 8/255 to 16/255, from epoch 20 to epoch 70, with the changing rate
_Œ¥ = 8/255/50 per epoch. For example, the perturbation bound of the increasing strategy at epoch_
_i is: œµi = min(_ 255[16] _[,][ max(][ 8]255_ _[,]_ 2558 [+ (][i][ ‚àí] [20)][Œ¥][))][; the perturbation bound for decreasing strategy is:]

_œµi = max(_ 255[8] _[,][ min(][ 16]255_ _[,][ 16]255_

_[‚àí]_ [(][i][ ‚àí] [20)][Œ¥][))][.]

**For CIFAR10 with WideResNet architecture.** We follow the setting in Pang et al. (2020) for the
additional experiments on CIFAR10 with WideResNet-34-10 architecture. More specifically, we
train with 200 epochs with SGD optimizer with momentum 0.9, weight decay 5e-4. The learning rate
is 0.1 and reduce at epoch 100th and 150th with rate 0.1 (Rice et al., 2020; Wu et al., 2020). More
importantly, to match the performance as reported in Croce et al. (2020), we omit the cross-entropy
loss of the natural images in PGD-AT and UDR-PGD. More specifically, the objective function
of PGD-AT in Eq. (5) has been replaced by: inf _Œ∏ EP_ _Œ≤ supx‚Ä≤_ _Bœµ(x) CE (hŒ∏ (x[‚Ä≤]), y)_ while the
_‚àà_
unified risk function for UDR-PGD to be: gŒ∏ (z[‚Ä≤]) := Œ≤CEh (hŒ∏ (x[‚Ä≤]), y[‚Ä≤]). We also switch Batchi
Normalization layer to evaluation stage when crafting adversarial examples as adviced in Pang et al.
(2020).

E VISUALIZING THE BENEFIT OF DISTRIBUTIONAL ROBUSTNESS

**Synthetic dataset setting.** We conduct an experiment on a synthetic dataset with a simple MLP
model to visualize the benefit of our UDR framework over the standard AT methods, by taking UDRPGD and PGD-AT as examples. The synthetic dataset consists of three clusters A, B1, B2 where A,
B are two classes as shown in Figure 2c. The data points are sampled from normal distributions, i.e.,
_A ‚àºN ((‚àí2, 0), Œ£), B1 ‚àºN ((2, 0), Œ£) and B2 ‚àºN ((6, 0), Œ£) where Œ£ = 0.5 ‚àó_ _I with I is the_
identity matrix. There are total 10k training samples and 2k testing samples with densities of three
clusters are 10%, 50% and 40%, respectively. We use a simple model of 4 Fully-Connected (FC)
layers as follows: Input ‚Äì> ReLU(FC(10)) ‚Äì> ReLU(FC(10)) ‚Äì> ReLU(FC(10)) ‚Äì> Softmax(FC(2)),
where FC(k) represents for FC with k hidden units. We use Adam optimizer with learning rate 1e-3
and train with 30 epochs. We use {k = 20, œµ = 1.0, Œ∑ = 0.1} for adversarial training (either PGD-AT
or UDR-PGD) and PGD attack with {k = 200, œµ = 2.0, Œ∑ = 0.1} for evaluation.

It is a worth noting that while the distance between clusters is 2, we limit the perturbation œµ = 1 for
the adversarial training to show the advantage on the flexibility of the soft-ball projection on the
same/limited perturbation budget. Intuitively, cluster A has the lowest density (10%), therefore, the
ideal decision boundary should be surrounded cluster A which sacrifices the robustness of the cluster
A but increases the overall robustness eventually.

**Comparison between UDR-PGD and PGD-AT.** First, we visualize the trajectory of adversarial
example from PGD and our UDR-PGD as in Figures 2b,2a to compare behaviors of two adversaries
on the same pre-trained model. It can be seen that: (i) the PGD‚Äôs adversarial examples and ours are
pushed toward the lower confident region to maximize the prediction loss gŒ∏(x[‚Ä≤], x, y); (ii) however,
while the adversarial examples of PGD are limited on the hard-projection ball, our adversarial
examples have more flexibility. Specifically, those are close to the decision boundary (cluster A, B1)
can go further, while those are distant to the decision boundary (cluster B2) stay close to the original
input. This flexibility helps the adversarial examples reach better local optimum of the prediction
loss, hence, benefits the adversarial training. Consequently, as shown in Figure 2c the final decision
boundary of our UDR-PGD is closer to the ideal decision boundary than that of PGD-AT, hence,
achieving a better robustness. Quantitative result shows that the robust accuracy of our UDR-PGD is
82.6%, while that of PGD-AT is 74.5% with the same PGD attack {k = 200, œµ = 2.0, Œ∑ = 0.1}.

**Comparison among UDR-PGD settings.** Here we would like to provide more understanding
about our framework through the experiment with PGD-AT as shown in Figure 3. First, we compare
the trajectories of the adversarial examples of UDR-PGD with different Œª as shown in Figures 3a,3b.
It can be seen that the crafted adversarial examples stay closer to their benign counterparts when Œª
becomes higher (i.e., Œª = 0.1 in Figure 3a). In contrast, the soft-projection ball is extended when Œª
becomes smaller (i.e., Œª = 0.01 in Figure 3b). On the other hand, with the same Œª but smaller œÑ as


-----

(a) UDR-PGD (b) PGD (c) Decision Boundary

Figure 2: (a)/(b): Trajectory of PGD and UDR-PGD adversarial examples. Each trajectory includes
20 intermediate steps. For better visualization, we do not use random initialization. The model is the
natural training at epoch 1. (c) The final decision boundary comparison.

shown in Figure 3c, the soft-ball projection is more identical to the hard ball projection as shown in
Figure 2b. These behaviors concur with the theoretical expectation as discussed in Section 4.1 in the
main paper.

Figure 3d shows the learning progress of parameter Œª. It can be observed that (i) the Œª converges to 0
regardless of its initialization value and (ii) the convergence rate of Œª depends on the parameter œÑ (i.e.,
smaller œÑ slower convergence). We choose œÑ = 2Œ∑ for the experiments on real-world image datasets.

(a) Œª = 0.1, œÑ = 1.0 (b) Œª = 0.01, œÑ = 1.0 (c) Œª = 0.01, œÑ = 0.01 (d) Changing of Œª

Figure 3: (a)/(b)/(c): Trajectory of UDR-PGD adversarial examples with different settings. Each
trajectory includes 20 intermediate steps. For better visualization, we do not use random initialization.
The model is the natural training at epoch 1. (d) The changing of parameter Œª.

**Further results of soft-ball projection.** In Figure 4, we compare our UDR-PGD with the soft-ball
projection to PGD-AT with the hard-ball projection with different settings against the PGD attack on
CIFAR10. For PGD-AT, we use the following three ad-hoc strategies for œµ: 1) Fixing œµ = 8/255; 2)
Fixing œµ = 16/255; 3) Gradually increasing/decreasing œµ from 8/255 to 16/255 (Refer to Appendix
D for details). It can be seen that it is hard to find an effective strategy of the perturbation boundary
of the hard-ball projection for PGD-AT, which can outperform ours. This demonstrates the benefit of
our soft-project operation.

F MORE RESULTS AND ANALYSIS

**Further results with C&W (L2) attack.** We enrich the comprehensiveness of the experiments by
further evaluating the defense methods with C&W (L2) attack (Carlini & Wagner, 2017) which is a
very strong optimization based attack. The experiment has been conducted on the CIFAR10 dataset
with WideResNet architecture. The hyper-parameters are c ‚àà{0.5, 0.7, 1.0}, kappa = 0, steps =
1000, lr = 0.01 where kappa is the confidence coefficient and c is box-constraint coefficient.[5] As
shown in Table 7, our distributional robustness version significantly outperform the standard ones
in term of robust accuracy. For example, against C&W (c=0.5) attack, the robust accuracy gap
between UDR-PGD and PGD-AT is 6% while that for UDR-AWP-AT and AWP-AT is around 5%.
The average improvement of robust accuracies against different levels of attack strengths is around

5We use the implementation from https://github.com/Harry24k/adversarial-attacks-pytorch


-----

Figure 4: Hard/soft-ball projections

Table 7: Robustness evaluation against C&W attack with WRN-34-10 on the full test set of the
CIFAR10 dataset (10K test images). c is box-constraint coefficient. (*) Omit the cross-entropy loss
of natural images.

Nat _c = 0.5_ _c = 0.7_ _c = 1.0_ Avg-Gap

PGD-AT* 84.93 40.85 25.90 12.95 - 
UDR-PGD* 84.60 **47.31** **31.58** **16.57** 5.25

TRADES 85.70 47.65 34.30 21.03 - 
UDR-TRADES 84.93 **49.14** **36.33** **23.28** 1.92

AWP-AT 85.57 49.91 34.31 18.97 - 
UDR-AWP-AT 85.51 **54.44** **39.86** **23.61** 4.91

5%. This result strongly emphasizes the contribution of our distributional robustness and the soft-ball
projection over the standard adversarial training.

**Experimental results of WRM (Sinha et al., 2018).** The performance of WRM highly depends
on the Lagrange dual parameter Œ≥ (or œµ = 0.5/Œ≥ in their implementation[6]), which controls the
robustness level. As mentioned in their paper, with large Œ≥, the method is less robust but more
tractable. Generally, decreasing Œ≥ will reduce the natural accuracy but increase the robustness of the
model as shown in Table 8. We obtained the best performance on MNIST with Œ≥ = 0.05 (CNN),
while on CIFAR10 and CIFAR100 with Œ≥ = 0.5 (ResNet18). The best results with three benchmark
datasets have been reported as in Table 9 (recall results from Table 1). It is a worth mentioning that
while we could obtain a similar performance as reported Sinha et al. (2017) on the MNIST dataset
with their architecture (3 Convolution layers + 1 FC layer), however, WRM seems much less effective
with larger architectures.

Table 8: Result of WRM with different œµ = 0.5/Œ≥ on the CIFAR10 dataset.

Nat PGD AA B&B
_œµ = 0.1_ 90.9 15.3 13.7 15.8
_œµ = 0.5_ 86.7 33.9 32.6 35.4
_œµ = 1.0_ 83.7 40.9 39.8 41.4
_œµ = 2.0_ 79.4 45.4 43.6 45.5
_œµ = 5.0_ 71.6 47.5 45.2 46.2
_œµ = 10.0_ 65.0 46.6 43.4 44.4

6https://github.com/duchi-lab/certifiable-distributional-robustness/blob/master/attacks_tf.py


-----

Table 9: Comparisons of natural classification accuracy (Nat) and adversarial accuracies against
different attacks. Recall results from Table 1 with additional results of WRM. Best scores are
highlighted in boldface.

MNIST CIFAR10 CIFAR100

Nat PGD AA B&B Nat PGD AA B&B Nat PGD AA B&B
WRM 91.8 27.1 4.5 8.2 83.7 40.9 39.8 41.4 56.6 24.7 21.3 22.9
PGD-AT 99.4 94.0 88.9 91.3 **86.4** 46.0 42.5 44.2 72.4 41.7 39.3 39.6
UDR-PGD **99.5** **94.3** **90.0** **91.4** **86.4** **48.9** **44.8** **46.0** **73.5** **45.1** **41.9** **42.3**

**Further results of whitebox attacks with varied œµ.** Here we would like to provide more results on
defending against whitebox attacks with a bigger range of œµ as shown in Figure 5. It can be seen that
in a wide range of attack strengths our DR methods consistently outperform their AT counterparts.

(a) MNIST (b) CIFAR10 (c) CIFAR100

Figure 5: Robustness evaluation against multiple attack strengths.

**The convergence of the algorithm.** During the training, we observed that while adversarial examples distribute inside/outside the hard ball œµ differently (i.e., as shown in Figure 2a ), but generally the
average distance to original input is less than œµ. Therefore, according to the update formulation in
Eq. (19), Œª tends to decrease to 0 and eventually is stable at 0 because of very small learning rate
as shown in Figure 3d. In addition, we visualize the training progress as shown in Figure 6 to show
the convergence of our method. It can be seen that, the error-rate reduces over training progress and
converges at the end of the training progress.

(a) MNIST (b) CIFAR10 (c) CIFAR100

Figure 6: Training progress of our UDR-PGD on different datasets, evaluating on the full training set
(e.g., 50k images) and the full testing set (e.g., 10k images). Robust accuracy is against PGD attack
with k = 20.

**Further experiment result on CIFAR100.** We would like to provide additional experiment result
on CIFAR100 dataset such that all defenses are adversarially trained with œµ = 2558 [. Our UDR-PGD]

outperforms PGD 3.7% at œµ = 2558 [and 2.3% on average, while our UDR-TRADES and UDR-MART]

outperform their counterparts by around 0.5% and 0.7%, respectively. It is worth noting that, in our
experiment, MART is quite sensitive with changes of (MART‚Äôs natural accuracy drops to a lower


-----

Table 10: Robustness evaluation on CIFAR100 dataset. The last column ‚ÄúAvg‚Äù represents the average
gap of robust accuracy between our methods and their standard AT counterparts.

Nat 2558 25510 25512 25514 25516 25520 Avg

PGD-AT 63.7 22.8 16.1 11.4 7.8 5.1 2.4 - 
UDR-PGD 64.5 26.5 18.9 13.7 9.8 7.0 3.5 2.30

TRADES 60.2 30.3 24.5 18.8 14.8 11.5 6.7 - 
UDR-TRADES 60.1 30.8 25.1 19.3 15.5 12.2 7.5 0.52

MART 54.1 32.0 26.8 21.9 17.4 13.8 7.6 - 
UDR-MART 54.4 32.3 27.4 22.5 18.4 14.4 8.5 0.67


Table 11: Distance function and its gradient


_L1_ _di=1cX (x, xxi_ _[‚Ä≤])x‚Ä≤i_ 1,‚àáx‚Ä≤ic(x, x[1, d[‚Ä≤])]

_L2_ 12P _di=1_ [(][x][i] ‚àí[ ‚àí] _[x]i[‚Ä≤]_ [)][2] _di ‚àÄ=1[(] ‚àà[x]i[‚Ä≤]_

_L‚àû_ maxP _i_ _xi ‚àí_ _x‚Ä≤i_ (10, i, otherwise =P argmaxi _[‚àí]x[x]i ‚àí[i][)]_ _x‚Ä≤i_

performance than that of TRADES); that might explain the lower gap between UDR-MART and
MART with the new œµ.

G CHOOSING THE COST FUNCTION

In this section, we provide the technical details of our learning algorithm in Section 4 in the main
paper, especially, the important of choosing cost function ÀÜc(x, x[‚Ä≤]). Given the current model Œ∏ and the
parameter Œª, we find the adversarial examples by solving:

_x[a]_ = argmaxx‚Ä≤ {gŒ∏(x[‚Ä≤], x, y) ‚àí _ŒªcÀÜX (x[‚Ä≤], x)}_

We employ multiple gradient ascent update steps without projecting onto the hard ball Bœµ. Specifically,
the updated adversarial at step t + 1 as follows:

_x[t][+1]_ = x[t] + Œ∑ (‚àáx‚Ä≤ _gŒ∏(x[‚Ä≤], x, y) ‚àí_ _Œª ‚àáx‚Ä≤_ _cÀÜX (x[‚Ä≤], x))_

Given the smoothed cost function as in Equation (19), the updating step is as follows:

_x[t][+1]_ = _xt + Œ∑ (‚àáx‚Ä≤_ _gŒ∏(x‚Ä≤, x, y) ‚àí_ _Œª ‚àáx‚Ä≤_ _cX (x‚Ä≤, x)),_ if cX (x[‚Ä≤], x) < œµ
_x[t]_ + Œ∑ _x‚Ä≤_ _gŒ∏(x[‚Ä≤], x, y)_ _œÑ_ _,_ otherwise.
 _‚àá_ _‚àí_ _[Œª]_ _[‚àá][x][‚Ä≤]_ _[c][X][ (][x][‚Ä≤][, x][)]_

It shows that, the pixels that are out-of-perturbation ball  _Bœµ will be traced back with a longer step,_
depending on the parameter œÑ . We consider three popular distance functions of c (x[‚Ä≤], x) with their
_X_
gradient as Table 11. It is worth noting that, while the norm L1, L2 have gradient in all pixels, the
_L_ has gradient in only one pixel per image. It means that, when using L norm as the cost function
_‚àû_ _‚àû_
_c_ (x, x[‚Ä≤]), only single pixel has been traced back at each iteration. In contrast, using L2 will project
_X_
all pixels toward the original input x with the step size of each. As in the discussion in Section F,
only small part of an MNIST image contributes to the prediction, while in contrast, most of pixels of
a CIFAR10 image affect to the prediction. Based on this observation, we use the L for the MNIST
_‚àû_
dataset and L2 for the CIFAR10 dataset in the updating step. However, the perturbation strength œµ
has been measured in L, therefore, we still use L in the Equation (22) to update Œª.
_‚àû_ _‚àû_

We also visualize the histogram of gradient ‚àáx‚Ä≤ _gŒ∏(x[‚Ä≤], x, y) and ‚àáx‚Ä≤_ _cÀÜX (x[‚Ä≤], x) as shown in Figure_
7. It can be seen that the strength of gradient grad1 = _x‚Ä≤_ _gŒ∏(x[‚Ä≤], x, y) is much smaller than_
_‚àá_
_grad2 = ‚àáx‚Ä≤_ _cÀÜX (x[‚Ä≤], x), for example, on the MNIST dataset, grad1 ‚àà_ [‚àí5 √ó 10[‚àí][4], 5 √ó 10[‚àí][4]] while
_grad2 ‚àà_ [‚àí0.3, 0.3] which is 600 times larger. Therefore, if using single update step, the gradient
_‚àáx‚Ä≤_ _cÀÜX (x[‚Ä≤], x) dominates the other and pulls the adversarial examples close to the natural input._
These adversarial examples are weaker and do not helps to improve the robustness. Alternatively,


-----

we break single update step for solving Equation (21) to two sub-steps as shown in Algorithm 1 to
balance between push/pull steps. It also can be seen that the grad2 corresponds with the perturbation
boundary œµ and the step size Œ∑. For example, on the MNIST dataset, grad2 has the range from

[‚àí0.3, 0.3] and has the highest density around [‚àí0.01, 0.01] where {0.3, 0.01} are the perturbation
boundary and step size in the experiment.

(a) CIFAR10, grad1 (b) MNIST, grad2 (c) CIFAR10, grad1 (d) CIFAR10, grad2

Figure 7: Histogram of gradient strength of grad1 = ‚àáx‚Ä≤ _gŒ∏(x[‚Ä≤], x, y) and grad2 = ‚àáx‚Ä≤_ _cÀÜX (x[‚Ä≤], x)_
on MNIST and CIFAR10 dataset. We use L2 norm for the cost function c (x[‚Ä≤], x), œÑ = Œ∑ and Œª = 1
_X_


-----

