# GRAND++: GRAPH NEURAL DIFFUSION WITH A SOURCE TERM

**Matthew Thorpe*[1], Hedi Xia*[2], Tan Nguyen*[2], Thomas Strohmer[3]**

**Andrea L. Bertozzi[2], Stanley J. Osher[2]** **& Bao Wang[4][ ∗]**

1Department of Mathematics, University of Manchester, Manchester M13 9PL, UK
2Department of Mathematics, UCLA, Los Angeles, CA, 90095, USA
3Department of Mathematics, UC Davis, Davis, CA 95616, USA
4Department of Mathematics and Scientific Computing and Imaging (SCI) Institute
University of Utah, Salt Lake City, UT, 84102, USA


ABSTRACT

We propose GRAph Neural Diffusion with a source term (GRAND++) for graph
deep learning with a limited number of labeled nodes, i.e., low-labeling rate.
GRAND++ is a class of continuous-depth graph deep learning architectures whose
theoretical underpinning is the diffusion process on graphs with a source term.
The source term guarantees two interesting theoretical properties of GRAND++:
(i) the representation of graph nodes, under the dynamics of GRAND++, will not
converge to a constant vector over all nodes even as the time goes to infinity,
which mitigates the over-smoothing issue of graph neural networks and enables
graph learning in very deep architectures. (ii) GRAND++ can provide accurate
classification even when the model is trained with a very limited number of labeled training data. We experimentally verify the above two advantages on various graph deep learning benchmark tasks, showing a significant improvement over
many existing graph neural networks.

1 INTRODUCTION

Graph neural networks (GNNs) are the backbone for deep learning on graphs. Recent GNN architectures include graph convolutional networks (GCNs) [30], ChebyNet [16], GraphSAGE [29],
neural graph fingerprints [20], message passing neural networks [28], and graph attention networks
(GATs) [54]. These graph deep networks have achieved success in many applications, including
computational physics and computational chemistry [20, 28, 3], recommender systems [41, 62], and
social networks [63, 47]. Hyperbolic GNNs have also been proposed to enable certain kinds of
data embedding with much smaller distortion [11, 37]. See [6] for some recent advances of GNN
algorithm development and applications.

A well-known problem of GNNs is that increasing the depth of GNNs often results in a significant
drop in performance on various graph learning tasks. This performance degradation has been widely
interpreted as the over-smoothing issue of GNNs [35, 44, 12]. Intuitively, GNN layers update the
node representation by taking a weighted average of its neighbors’ features, making representations
for neighboring nodes to be similar. As the GNN architecture gets deeper, all nodes’ representation will become indistinguishable resulting in over-smoothing. In Sec. 2, we briefly show that
certain GNNs have a diffusive nature which makes over-smoothing inevitable. Another interesting
interpretation of the GNN performance degradation is via a bottleneck [1], since a GNN tends to
represent exponentially growing information from neighbors with fixed-size vectors. Several algorithms have been proposed to mitigate the over-smoothing of GNNs, including skip connection and
dilated convolution [33], Jumping Knowledge [60], DropEdge [49], PairNorm [64], graph neural
diffusion (GRAND) [10], and wave equation motivated GNNs [21]. Nevertheless, developing deep
GNN architectures is still in its infancy compared to the development of other deep networks.

Besides suffering from over-smoothing, we notice that the accuracy of existing GNNs drops severely
when they are trained with a limited labeled data. As illustrated in Fig. 1, the test accuracy of several

_∗Correspond to wangbaonj@gmail.com or matthew.thorpe-2@manchester.ac.uk_


-----

celebrated GNN architectures, including GCN, GAT, and GraphSage, drops rapidly when they are
trained with fewer labeled data. Moreover, the variance of classification accuracy grows significantly
as number of labeled nodes drops. Indeed, semi-supervised graph learning with very low-labeling
rates has been studied in the Laplace learning and graph deep learning settings, see, e.g., [36, 9, 23];
one question is can we develop new GNN architectures to improve the performance of graph deep
_learning in low-labeling rate regimes?_


80

70

60

50


80

75

70

65

60


80

70

60

50


50 55

30 40

20 15 10 5 0 20 15 10 5 0 20 15 10 5 0

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|GC|N|||||
|GA Gra|T phSage|||||


|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
||N|||||
|GC|N|||||
|GA|T|||||
|Gra|phSage|||||


|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|||||||
|GC|N|||||
|GA|T|||||
|Gra|phSage|||||


GCN
GAT
GraphSage

GCN
GAT
GraphSage

GCN
GAT
GraphSage

#Labeled Nodes Per Class #Labeled Nodes Per Class #Labeled Nodes Per Class

CORA CiteSeer PubMed

Figure 1: Test accuracy of GCN, GAT, and GraphSage vs. the number of labeled nodes per class. All networks
have 2 layers, and each experiment is run with 100 splits and 20 random seeds following [10]. The accuracy
drops rapidly with fewer labeled data for training. CORA, CiteSeer, and PubMed have 2485, 2120, and 19717
nodes in total respectively. Results on more benchmark GNN architectures are in Appendix D.4.


1.1 OUR CONTRIBUTION

With the above GNN problems in mind, we focus on developing new continuous-depth GNNs to
overcome over-smoothing and boost the accuracy of GNNs with a limited number of labeled data.
We first present a random walk interpretation of the GRAND model [10], revealing a potentially inevitable over-smoothing phenomenon when GRAND is implicitly very deep. Based on the random
walk viewpoint of GRAND, we then propose graph neural diffusion with a source term (GRAND++)
that corrects the bias arising from the diffusion process, see Sec. 5 for details. GRAND++ theoretically guarantees that: (i) under GRAND++ dynamics, the graph node features do not converge to
a constant vector over all nodes even as the time goes to infinity, and (ii) GRAND++ can provide
accurate prediction even when it is trained with a limited number of labeled nodes. Moreover, these
theoretical results resonate with the practical advantages of GRAND++. We summarize the major
practical advantages of GRAND++ below.

-  GRAND++ can effectively overcome the over-smoothing issue; it is remarkably more accurate
than existing GNNs when the architecture is very deep.

-  GRAND++ is suitable for graph deep learning when only a few nodes are labeled as training data.
Moreover, in the low-labeling rates, GRAND++ can be more accurate when the network is deeper.

-  GRAND++ inherits the continuous-depth merit from GRAND, which defines the network depth
implicitly and enables memory-efficient training by using the adjoint method.

1.2 RELATED WORK
**Diffusion on graphs and continuous-depth graph neural networks.** Diffusion has been defined
on graphs, see, e.g., [25, 24], and used in various applications, including data clustering and dimension reduction [15, 4], image processing [27, 22, 17, 38], and semi-supervised graph nodes
classification [67, 65]. From the numerical viewpoint, fast algorithms have been proposed for using
diffusion on graphs to solve penalized graph cut problems [26]. The connection between GNNs
and diffusion on graphs has been studied substantially. For instance, GNN has been interpreted as a
diffusion process on graphs, which performs low-pass filtering on the input features [43]. Moreover,
insights from the diffusion process on graphs have been used to improve the performance of GNNs,
see, e.g., [2, 36, 31, 57].

Leveraging neural ordinary differential equations (ODEs) [13], continuous-depth GNNs have been
proposed, see, e.g., [45, 58, 68]. One recent work is GRAND [10], which parameterizes the diffusion
equation on graphs with a neural network. See Sec. 3 for a brief review of GRAND.
**Neural ODEs.** Neural ODEs [13] are a class of continuous-depth neural networks whose depth is
defined implicitly. Training neural ODEs using the adjoint method [46] is more memory efficient
than training other neural networks using backpropagation. We provide a brief review of neural
ODEs and the adjoint method in Appendix C. GRANDs [10] are a class of neural partial differential equations (PDEs) on graphs that can also be considered as a coupled system of neural ODEs.
Furthermore, GRANDs are also trained by using the adjoint method.


-----

**Laplace learning and Poisson learning.** Laplace learning has been used for semi-supervised data
classification [67, 65, 56], image processing [7, 27], etc. Direct application of Laplace learning
with Gaussian weights [5] or locally linear embedding weights [50] for the above tasks may cause
inference inconsistency when only a limited number of graph nodes are labeled, resulting in poor
performance. Several algorithms address the inference inconsistency at low labeling rate. They
include up-weighting the weights of the labeled data [52] and the p-Laplacian [8, 48, 66]. In [9], the
authors have proposed Poisson learning for improving Laplace learning at extremely low-labeling
rate regimes. Poisson learning augments Laplace learning with a Green’s function at each labeled
data, enabling accurate node classification when only a few labeled data are available. Compared to
Laplace learning, Poisson learning adds Green’s function to the label of each labeled node and then
performs label propagation to predict the label for unlabeled graph nodes. GRAND and GRAND++
both learn graph node representations and perform prediction by activating the node representations,
which are fundamentally different from Laplace and Poisson learning.

1.3 NOTATION
We denote scalars by lower- or upper-case letters and vectors and matrices by lower- and upper-case
boldface letters, respectively. For a matrix A, we denote its transpose as A[⊤] and its Hadamard
product with another matrix B as A ⊙ **_B, i.e., the entrywise multiplication of A and B. We write_**
the set {1, 2, · · ·, n} as [n]. We denote the probability and expectation of a given random variable
**_x as P(x) and E[x], respectively. The meaning of other notations can be inferred from the context._**

1.4 ORGANIZATION
The paper is organized as follows: In Sec. 2, we review diffusion equation on graphs and its connection to GNNs. In Secs. 3 and 4, we briefly review GRAND and present a random walk interpretation of GRAND, respectively. Leveraging the random walk viewpoint of GRAND, we propose
GRAND++ for deep graph learning with theoretical guarantees in Sec. 5. We verify the efficacy of
GRAND++ in Sec. 6. Technical proofs and more validations are provided in the appendix.

2 BACKGROUND
**Diffusion equation on graphs.** Let G = (X, W ) represent an undirected graph with n nodes,
where X = [x[(1)]][⊤], · · ·, [x[(][n][)]][⊤][][⊤] _∈_ R[n][×][d] with each row x[(][i][)] _∈_ R[d] a feature vector and
**_W :=_** _Wij_ a n _n matrix with Wij representing the similarity (edge weight) between the i[th]_
  _×_

and j[th] feature vectors, and we assume Wij = Wji. Consider the following diffusion process that

  

evolves the feature matrix X on the graph (see Appendix A for a brief review of calculus on graphs):

_∂X(t)_

= div **_G(X(t), t)_** **_X(t)_** _,_ (1)
_∂t_ _⊙∇_
  

where X(t) = [x[(1)](t)][⊤], · · ·, [x[(][n][)](t)][⊤][][⊤] _∈_ R[n][×][d] with x[(][i][)](0) = x[(][i][)], ∇ and div are the
gradient and divergence operators, respectively. The matrix G(X(t), t) is chosen such that W **_G_**
  _⊙_
is right-stochastic, i.e., each row of W ⊙ **_G summing to 1. In the machine learning setting, we can_**
parameterize G with learnable parameters θ which we denote by G(X(t), t, θ). The initial features
are evolved under the diffusion dynamics (1) from t = 0 to T to learn the final representation X(T )
for further machine learning tasks.

In the simplest case when G(X(t), t) is only dependent on the initial node features X, i.e., G
is time-independent, right-stochasticity implies _j_ _[W][ij][G][ij][ = 1][ for all][ i][, and so we focus on the]_

particular case when Gij = 1/di with di = _j=1_ _[W][ij][. In this case the right-hand side of (1)]_
reduces to the negative of the random-walk Laplacian applied to[P] **_X(t) and (1) becomes_**

_∂X(t)_ [P][n]

= div **_G(X(t), t)_** **_X(t)_** = **_LX(t),_** (2)
_∂t_ _⊙∇_ _−_

where L = I − **_D[−][1]W := I −_** **_A (A  := A(X)) is the random walk Laplacian and_** **_D is diagonal_**
with Dii = di. See [14, 25, 24] for more about random walk Laplacian and diffusion on graphs.

**Graph neural networks. Applying forward Euler discretization, with step size δt < 1, of (2) gives**

**_X(kδt) = X((k_** 1)δt) _δtLX((k_ 1)δt) := LX[˜] ((k 1)δt), for k = 1, 2, _, K,_ (3)
_−_ _−_ _−_ _−_ _· · ·_


-----

_T = Kδt, and X(0) = X. Note that the matrix_ **_L[˜] is the discretization of the diffusion operator,_**
which is a special low-pass filter. Equation (3) is a prototype for motivating GNNs: by introducing
weights W [(][k][)] _∈_ R[d][×][d] and a nonlinearity σ, e.g., ReLU, into (3), we have

**_X((k + 1)δt) = σ_** ˜LX(kδt)W [(][k][)][]. (4)

The model in (4) is similar to the well-established GCN architecture proposed in [30]. The diffusive 
nature of the GNN architecture in (4) further explains the over-smoothing issue of training deep
GNNs; the deeper the network architecture is, the more the node features diffuse. Eventually, all
nodes share similar features and become indistinguishable. See Sec. 5 for a detailed analysis.

3 A BRIEF REVIEW OF GRAND
GRAND is a new continuous-depth GNN proposed in [10]. It integrates a learnable encoder function
_φ and a learnable decoder function ψ with the neural network parameterized graph diffusion process,_
resulting in the prediction Y = ψ(X(T )), where X(T ) is computed as

_T_

_∂X(t)_

**_X(T_** ) = X(0) + _dt, with X(0) = φ(X),_ (5)

0 _∂t_

Z

where ∂X(t)/∂t is given by the graph diffusion equation (2). From the neural ODE perspective, we
can perform forward propagation of GRAND, i.e., we solve (5), using numerical ODE solvers.


In the simplest case, when G is only dependent on the initial node features, we can rewrite (1) as

_∂X(t)_

= **_A(X)_** **_I_** **_X(t),_** (6)
_∂t_ _−_
  


GRAND models the diffusivity A(X) in (6) by the multi-head self-attention mechanism; potential
choices of the attention function include the ones proposed in [53, 54]. More precisely, in GRAND
**_A(X) =_** _h1_ _hl=1_ **_[A][l][(][X][)][ with][ h][ being the number of heads and the attention matrix][ A][l][(][X][) =]_**
(a[l](xi, xj)), for l = 1, _, h, is computed as follows:_
P _· · ·_

exp LeakyReLU(a[l][⊤][W _[l]xi_ **_W_** _[l]xj])_
_a[l](xi, xj) =_ _∥_ _,_ (7)

_k∈Ni_ [exp]  LeakyReLU(a[l][⊤][W _[l]xi∥W_ _[l]xk])_

where W _[l]_ and a[l] are learned, ∥Pis the concatenation operator, and  _Ni is the index set of the nodes_
that are connected to the i[th] node in the graph. GRAND with the attention in (7) is called GRAND-l,
that is, GRAND-l is a special case of GRAND when the diffusivity is dependent only on the initial
graph node features. Time-dependent attention and graph rewiring can be integrated into GRAND,
resulting in GRAND-nl and GRAND-nl-rw, respectively [10]. From the ODE viewpoint, GRAND
and its variants are a class of coupled neural ODEs defined on an unweighted graph. Their merits
include continuous-depth and memory-efficient training using the adjoint method [46, 13].

4 RANDOM WALK VIEWPOINT OF GRAND


In this section, we present a random walk interpretation of GRAND. The connection between graph
random walks and the diffusion equation has been extensively studied, but we recap the key idea
here to motivate the new GRAND with a source term architecture. Let {B[(][i][)](k)}k∈N be the random
walk on {x[(][j][)](0)}j[n]=1 [defined by, for][ δ][t][ ∈] [[0][,][ 1]][,]

**_B[(][i][)](0) = x[(][i][)](0)_**

1 _δt_ if ℓ = j (8)
P **_B[(][i][)](k + 1) = x[(][ℓ][)](0)_** **_B[(][i][)](k) = x[(][j][)](0)_** = _δt −Wjℓ_
_|_ _dj_ if ℓ = j
 _̸_
  

where dj = _ℓ=1_ _[W][jℓ]_ [(assume][ W][ℓℓ] [= 0][ for all][ ℓ][). Proposition 1 below is well-known, see [67].]
We provide the proof of Proposition 1 and all the subsequent theoretical results in Appendix B.

[P][n]

**Proposition 1Then** _Let X solve (3) and B[(][i][)]_ _be the random walk determined by (8) where δt ∈_ [0, 1].
**_x[(][i][)](δtk) = E_** **_B[(][i][)](k)_** _._
 


-----

Proposition 2 below gives the stationary distribution of the random walk {B[(][i][)](k)}k∈N.

**Proposition 2 Assume the graph G = (X, W ) is connected. Then, the stationary distribution of**
_{B[(][i][)](k)}k∈N is_

_d1_ _dn_
_π =_ _n_ _, . . .,_ _n_ _,_ (9)
_j=1_ _[d][j]_ _j=1_ _[d][j]_
 

_which is independent of the starting positionP_ **_x[(][i][)]._** P

Furthermore, we have the following theoretical result on the asymptotic behavior of graph node
features under the GRAND dynamics given by (3).

**Proposition 3 Assume the graph G = (X, W ) is connected. Then for all i = 1, · · ·, n, we have**

_n_

**_x[(][i][)](kδt)_** **_x :=_** **_x[(][j][)](0)πj,_** _as k_ _._
_→_ _→∞_

_j=1_

X

e

Hence, for the case of (3), i.e., GRAND-l, we expect the output to be approximately independent of
the input, due to over-smoothing. Of course, once we reintroduce the X(t) dependence back into G
in (1) and (2) or into the operator A in (6) then the above arguments no longer hold. Nevertheless,
the GRAND architectures are built on a principle that is ill-suited to deep networks. In the next
section we introduce a source term and perform a similar random walk analysis that illustrates how
the new architecture can be better suited for deep GNN architectures.


5 GRAND++: GRAPH NEURAL DIFFUSION WITH A SOURCE TERM

5.1 ALGORITHM AND FORMULATION

At the core of GRAND++ is the introduction of a source term into GRAND, leveraging the random
walk viewpoint of the diffusion process. We take a small subset of feature vectors, indexed by
_I ⊆_ [n], believed to be “trustworthy” for use as a source term. In particular, we use the features of
labeled data. The GRAND++ dynamics are defined by a diffusion equation with a source term (we
use the variable z for GRAND++-related dynamics and x for GRAND dynamics)


_∂z[(][i][)](t)_

= div [G(Z(t), t) **_Z(t)][(][i][)]_** +
_∂t_ _⊙∇_


_δijCj_ (10)

Xj∈I


where Cj is the source at feature vector of node j. Below we motivate a particular choice of Cj.

The key idea is to first characterise the bias that arises from the diffusion and use that to propose
a correction via the choice of source terms Cj. Following the simplifications in (2), our diffusion
equation (without the source term) follows the approximate dynamics when t ≫ 1

_n_

_∂x[(][i][)](t)_

= [LX(t)][(][i][)] = **_x[(][i][)](t)_** + [1] _Wij x[(][j][)](t)_ 0.
_∂t_ _−_ _−_ _di_ _≈_

_j=1_

**_x_** X **_x_**
_≈_ _≈_

For i ∈I, it transpires that choosing Ci = x| {z }[(][i][)] _−e_ **_xˆ (where ˆx is defined below) gives rise to| {z }e_**
a random walk interpretation that allows us to prove that the oversmoothing seen in the GRAND
model is avoided.

One can in fact choose **_x with a certain degree of freedom. If we initialise X(0) = X then we obtain_**
**_x =_** _j=1_ **_[x][(][j][)][π][j][ (as is usual in the GRAND model). However, as the similarities are encoded in]_**
the graph weights, and the diffusion dynamics will drive it towards a non-trivial state, we can choose e
a different initialization than X(0) = X. Through connections with random walks we, in the next
e [P][n]
subsection, motivate an alternative initialisation


**_x[(][i][)]_** **_xˆ_**
_−_ _, where ˆx = [1]_

_di_

_|I|_


**_z[(][i][)](0) =_**

_i=1_

X


**_x[(][j][)]_** (11)

Xj∈I


_i∈I_


-----

with the dynamics

_∂z[(][i][)](t)_

= div [G(Z(t), t) **_Z(t)][(][i][)]_** +
_∂t_ _⊙∇_

For example, we could choose


_δij_ **_x[(][i][)]_** **_xˆ_** _._ (12)
_−_
_j_

X∈I  


**_z[(][i][)](0) =_** _d1i_ **_x[(][i][)]_** _−_ **_xˆ_** if i ∈I or z[(][i][)](0) = x[(][i][)] _c,_

**0** otherwise, _−_

   

where c = _n[1]_ _ni=1_ **_[x][(][i][)]_** _[−][P]j_ **_x[(][j]d[)]j−xˆ_** is chosen such that (11) holds. We do not believe that the

_∈I_

constant c (that shifts by a constant) is particularly important but it is included to provide a random P 
walk interpretation which helps to understand the deep architecture (when T is big) behaviour of
the model GRAND++. The justification for this choice will be made in Sec. 5.2. To summarize, the
GRAND++ model in (12), with initial condition satisfying (11), simply adds a source term to the
original GRAND model and uses a different initial condition. Therefore, the nonlinear diffusivity
and graph rewiring tricks used by GRAND can be easily integrated into GRAND++. In terms of
implementation, since GRAND++ merely changes the right-hand side of GRAND, which again can
be regarded as a system of coupled first-order neural ODEs; we can leverage neural ODE training,
testing, and inference for GRAND++ similar to GRAND.

In the next subsection we explore the random walk connection of the above model, suggesting that
building a graph neural network based on the diffusion with source model does not suffer from the
same degeneracy as we observed in Sec. 4 and is therefore better suited to build deep GNNs. In
particular, we can write the diffusion with source model as the short time expected behaviour of a
random walk and therefore we do not have the issue of reaching the stationary state (in other words
passing the mixing time). Our experiments in Sec. 6 suggest the formal motivation holds and we are
able to design deep GNNs.

5.2 THE RANDOM WALK PERSPECTIVE OF GRAND++

Let us continue to consider the simplified model in the previous subsection, i.e., assume the dynamics are governed by
_∂z[(][i][)](t)_

= [LZ(t)][(][i][)] + _δij_ **_x[(][i][)]_** **_xˆ_** (13)
_∂t_ _−_ _−_

_j_

X∈I  

where the initial condition satisfies (11). Using the forward Euler discretisation of the above dynamics we have


**_z[(][i][)](δtk) = z[(][i][)](δt(k −_** 1)) − _δt [LZ(δt(k −_ 1))][(][i][)] + δt

for k = 1, 2, . . ., K where again T = Kδt.


_δij_ **_x[(][i][)]_** _−_ **_xˆ_** _,_ (14)
_j_

X∈I  


We use the same random walk as that introduced in Sec. 4, i.e. the random walk defined by (8), but
we will now only consider random walks that are initialised on the nodes indexed by I.

**Proposition 4 Let Z solve (14) with the initial condition satisfying (11), and let B[(][i][)]** _be the random_
_walk determined by (8). Then,_

_k_

1

 Xs=0 _di_ Xj∈I x[(][j][)] _−_ **_xˆ_** 1B(j)(s)=x(i)  _[→]_ [0][ as][ k][ →∞][.]

**Remark 1 In the limit[z][(][i][)][(][kδ][t][)] k[ −] →∞[E]** _the termk_

1

E **_x[(][j][)]1B(j)(s)=x(i)_**

_di_

 _s=0_ _j_ 
X X∈I

_is formally a function of the random walk at all times. Whilst if k is very large (i.e. in comparison to_
_the mixing time) we still have that_

1
E **_x[(][j][)]1B(j)(k)=x(i)_** = [1] **_x[(][j][)]_** P **_B[(][j][)](k) = x[(][j][)][]_** **_x[(][j][)]_** (15)

_di_ _di_ _≈_ _[π]di[i]_

 Xj∈I  Xj∈I  _≈πi_ Xj∈I

| {z }


_di_


_s=0_


-----

_and on the other hand_

1 **_x_** **_x_**
E **_xˆ1B(j)(k)=x(i)_** = [ˆ] P **_B[(][j][)](k) = x[(][j][)][]_** _._ (16)

_di_ _di_ _≈_ _[π][i][|I|]di_ [ˆ]

 Xj∈I  Xj∈I  _≈πi_

_From the definition of ˆx we see that (15) and (16) are approximately equal. And therefore we can|_ {z }
_understand E[_ _d[1]i_ _j_ **_x[ˆ]1B(j)(k)=x(i)_** ] as the long time behaviour of E[ _d[1]i_ _j_ **_[x][(][j][)][1]B[(][j][)](k)=x[(][i][)]_** []]

_∈I_ _∈I_

_Very formally we can see that subtracting the long-time behaviour from the all-time behaviour leaves_

P P

_us with the short time behaviour. This provides one explanation as to why we do not expect the deep_
_layers to be determined by the stationary state of the random walk (at which point there is little_
_dependence on the initial layers, causing the deep layers to be approximately constant)._


**Remark 2 The random walk interpretation**


**_x[(][j][)]_** **_xˆ_** 1B(j)=x(i) (17)
_−_
 


_di_


_s=0_


_j∈I_


_can be considered to be dual to the random walk interpretation in Sec. 4: in Sec. 4 we released_
_the random walker from the node of interest, whilst now we release the random walkers from nodes_
_indexed by I and see how many of them hit the node of interest. We note also that we do not require_
_a lower bound on the size of the set I. Indeed, if |I| is fixed whilst one takes the number of feature_
_vectors n →∞_ _we still expect many properties of GRAND++, in particular Proposition 5 below, to_
_hold. This is due to the asymptotic well-posedness of the dual random walk in low labeling rates [9]._

Proposition 3 reveals that in the simple setting of (2), GRAND converges to a constant when its
depth goes to infinity. However, this is not true for GRAND++ since the graph node features will
not converge to a constant vector driven by the GRAND++, as shown in Proposition 5 below.

**Proposition 5 Assume the graph G = (X, W ) is connected. Then z[(][i][)](kδt) that was defined in**
(14) does not converge to a constant vector as a function of i as k →∞. That is, the node features
_will not become the same across graph nodes under the GRAND++ dynamics._

**Remark 3 Proposition 5 guarantees GRAND++ is less likely to suffer from over-smoothing than**
_GRAND, and in particular it shows that we have a non-constant deep layer limit, i.e., as t →_
_∞. Analysing the limit is beyond the scope of the paper but we have seen one characterisation in_
_Proposition 4. By construction we have ∂z[(][i][)](t)/∂t ≈_ **0 for i ∈I so one should expect that the**
_deep layer limit is (close to) a smooth interpolation of the feature vectors labeled by I._

The continuous time model (10) is, in the special case of (13), the mean field limit of the probabilistic
formulation (17). Our proposed algorithm is formulated from the mean-field limit.

6 EXPERIMENTS

In this section, we compare the performance of GRAND++ with GRAND and several other popular GNNs on various graph node classification tasks. We aim to show the practical advantages of
GRAND++ in learning with limited labeled data and using deep architectures. Without mentioning
clearly, we use the same hyperparameters that that used for GRAND in [10] for GRAND++. We
provide detailed descriptions of experimental settings and datasets that are omitted in the main text
in Appendix D.1. For all experiments, we run 100 splits for each dataset with 20 random seeds for
each split, which are conducted on a server with four NVIDIA RTX 3090 graphics cards.

We compare the performance of GRAND++ and its nonlinear and graph rewiring variants with several popular GNNs on various graph node classification benchmarks. Except for the integration time,
which measures the implicit depth of GRAND and GRAND++, we adopt the experimental settings
of GRAND in [10] for GRAND++ include numerical differential equation solvers. Following [10],
we study seven graph node classification datasets, namely CORA, CiteSeer, PubMed, CoauthorCS,
Computer, Photo, and ogbn-arxiv; we describe these datasets in Appendix D.1.


-----

6.1 GRAND++ IS MORE RESILIENT TO DEEP ARCHITECTURES

We first show that our introduced source term in (12) can improve the accuracy of GRAND-l when
the architecture is deep, i.e., the integration time T in (5) is big. We denote GRAND-l with the source
term as GRAND++-l. For each node classification task, we train all models using the same number
of labeled nodes as in [10]. Figure 2 contrasts the performance of GRAND-l and GRAND++-l
with different depths, or T, on CORA, CiteSeer, Computer, and Photo datasets. We provide the
detailed results on PubMed and CoauthorCS, together with more comparisons of GRAND++-l with
GRAND-l and several other celebrated GNNs include GCN, GAT, and GraphSage in Table 5 in
Appendix D.2. The results in Fig. 2 and Appendix D.2 confirm that GRAND-l suffers less from
over-smoothing compared to GCN, GAT, and GraphSage. Moreover, GRAND++-l performs on
par with GRAND-l when the depth (T ) of the network is small, but GRAND++-l significantly
outperforms GRAND-l when T is large. As T increases, the margin becomes wider, indicating
that GRAND++-l can overcome over-smoothing much more effectively than GRAND-l. Note that
we did not use uniform depth for GRAND-l and GRAND++-l on all datasets because the adaptive
step-size ODE solver fails when T is large for some tasks.


85

80

75

70

65

60


78

76

74

72

70

68

66


85

80

75

70


94

93

92

91

90

89


60 16 64 128 256 4 16 64 65 1 4 16 32 88 1 4 16 32

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
|||||
|||||
|G G|RAND+ RAND-l|+-l||


|Col1|Col2|Col3|
|---|---|---|
||||
||||
||||
||||
||||
|G G|RAND++-l RAND-l||


|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
|||||
||GRAND++ GRAND|||


|Col1|Col2|
|---|---|
|||
|||
|||
|||
|GRAND++ GRAND||


GRAND++-l
GRAND-l

GRAND++-l
GRAND-l

GRAND++
GRAND

Depth (T) Depth (T) Depth (T) Depth (T)

CORA CiteSeer Computer Photo

Figure 2: Test accuracy vs. the “depth” (T in (5)) of GRAND-l and GRAND++-l on the four graph node
classification tasks. We see that GRAND++-l is much more resilient to deep architectures than GRAND-l.
These results show that GRAND++ is better suited for learning with a very deep architecture than GRAND.

Next, we compare GRAND-l and GRAND++-l on the ogbn-arxiv node classification task, which
is a large-scale benchmark. We train two models using labeling rates of 3.0% and 5.0%, respectively; the corresponding test accuracy for GRAND-l/GRAND++-l are 65.26%/66.64% and
67.42%/67.77%, respectively. GRAND++-l outperforms GRAND-l in both labeling rates.We further compare GRAND and GRAND++ with different depth on the ogbn-arxiv task in Appendix D.6.


6.2 GRAND++ IS MORE ACCURATE WITH LIMITED LABELED TRAINING DATA

Besides helping to overcome over-smoothing, 65 65
our theory shows that the source term can 60 60

55

boost the accuracy of GRAND-l with low- 50 55
labeling rates. Table 1 compares the accuracy 45 50
of GRAND++-l with GRAND-l, GCN, GAT, 35
GraphSage, and MoNet, trained with different 30 GRAND++-lGRAND-l 40 GRAND++-lGRAND-l
numbers of labeled data. Here, we slightly tune 4 16 Depth (T)64 128 1 4 16 Depth (T) 64

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
|||||
|||||
||GRAND++|-l||
||GRAND-l|||


|Col1|Col2|Col3|
|---|---|---|
||||
||||
||||
|G|RAND++-l||
|G|RAND-l||


GRAND++-l
GRAND-l

GRAND++-l
GRAND-l

_T for GRAND++ based on the optimal value_ CORA CiteSeer
for GRAND, see Table 4 in the Appendix for Figure 3: Accuracy of GRAND++-l and GRAND-l for
their values. We see that with few labeled data, CORA and CiteSeer, where both models, with different
in most tasks GRAND++-l is significantly more depth (T ), are train with 1 labeled node per class. These
accurate than the other GNNs include GRAND- results show that GRAND++ is more effective in learn
ing with low-labeling rates than GRAND.

l, confirming our theoretical insight. For CoauthorCS task, both GRAND-l and GRAND++-l are worse than GCN and GraphSage. Moreover,
increasing the depth of GRAND++-l can improve the classification accuracy with limited training
data, but this is not the case for GRAND-l, see Fig. 3. We perform the t-test in Appendix D.5 to
confirm the statistical significance of the accuracy gain of GRAND++ over GRAND in Table 1.

6.3 TIME-DEPENDENT ATTENTION AND GRAPH REWIRING
The previous experimental results show that GRAND++-l enhances the accuracy of GRAND-l in the
cases when the labeled training data is limited and when the network is deep. Here, we explore the
same strategy for GRAND-nl and GRAND-nl-rw; we name the corresponding models with the new
source term GRAND++-nl and GRAND++-nl-rw, respectively. Table 2 compares GRAND-nl and


-----

Model #per class CORA CiteSeer PubMed CoauthorCS Computer Photo

GRAND++-l(ours) 1020521 **66.9254.9482.9580.8677.80 ± ± ± ± ± 10.04 16.09 1.37 2.99 4.46** **64.9858.9572.3470.0373.53 ± ± ± ± ± 2.42 3.31 3.63 8.31 9.59** 71.9969.3165.9475.1379.16 ± ± ± ± ± 1.91 3.88 1.37 4.87 4.87 86.9490.8076.5360.3084.83 ± ± ± ± ± 0.34 0.46 0.84 1.85 1.50 **85.7382.9982.6467.6576.47 ± ± ± ± ± 0.50 0.81 0.56 1.48 0.37** **93.5583.1290.6588.3383.71 ± ± ± ± ± 0.38 1.19 1.21 0.90 0.78**

GRAND-l[10] 1020512 52.5364.8276.0780.2582.86 ± ± ± ± ± 16.40 11.16 2.39 3.40 5.08 50.0659.5573.0271.9068.37 ± ± ± ± ± 17.98 10.89 5.89 7.66 5.00 62.1169.0078.7676.3373.98 ± ± ± ± ± 10.58 1.69 3.41 7.55 5.08 91.0373.8387.8185.2959.15 ± ± ± ± ± 1.36 0.47 2.19 5.58 5.73 82.4284.5480.7274.7748.67 ± ± ± ± ± 1.10 0.90 1.09 1.85 1.66 93.5388.2782.1381.2590.98 ± ± ± ± ± 0.93 0.47 1.94 3.27 2.50

GCN [30] 1020215 60.8547.7273.8678.8282.07 ± ± ± ± ± 14.01 15.33 7.97 5.38 2.03 48.9458.0667.2472.1874.21 ± ± ± ± ± 10.24 4.19 3.47 9.76 2.90 58.6160.4568.6972.5976.89 ± ± ± ± ± 16.20 12.83 7.93 3.19 3.27 86.6688.6091.0983.6165.22 ± ± ± ± ± 1.49 0.43 0.50 0.35 2.25 49.4676.9082.4782.5382.94 ± ± ± ± ± 1.49 0.97 0.74 1.65 1.54 90.4182.9491.9583.6188.86 ± ± ± ± ± 0.71 1.56 0.35 2.17 0.11

GAT [54] 1020125 47.8658.3071.0476.3179.92 ± ± ± ± ± 15.38 13.55 5.74 4.87 2.28 50.3155.5567.3771.3573.22 ± ± ± ± ± 14.27 9.19 5.08 4.92 2.90 58.8460.2468.5472.4475.55 ± ± ± ± ± 12.81 14.44 5.75 3.50 4.11 71.6574.7179.9551.1363.12 ± ± ± ± ± 5.24 6.09 4.53 3.35 2.88 37.1465.0771.4376.0480.05 ± ± ± ± ± 7.81 8.86 7.34 0.35 1.81 73.5876.8983.0187.4289.38 ± ± ± ± ± 8.15 4.89 3.64 2.38 2.48

GraphSage[29] 1020125 43.0453.9668.1475.0480.04 ± ± ± ± ± 14.01 12.18 2.54 5.03 6.95 48.8154.3972.0268.9064.79 ± ± ± ± ± 11.45 11.37 2.82 5.08 5.16 55.5358.9774.5570.7466.07 ± ± ± ± ± 12.71 12.65 3.09 3.11 6.16 **91.3389.6889.0661.3576.51 ± ± ± ± ± 0.36 0.39 0.69 1.35 1.31** 27.6542.6379.9874.6664.83 ± ± ± ± ± 0.96 1.29 1.62 2.39 4.29 91.2984.3878.2645.3651.93 ± ± ± ± ± 0.67 1.75 1.93 7.13 4.21

MoNet[40] 1020512 47.7260.8573.8678.8282.07 ± ± ± ± ± 14.01 15.53 7.97 5.38 2.03 39.1348.5261.6668.0871.52 ± ± ± ± ± 11.37 9.52 6.61 6.29 4.11 56.4761.0367.9271.2476.49 ± ± ± ± ± 6.93 2.50 1.54 1.75 4.67 76.5787.0288.7690.3158.99 ± ± ± ± ± 4.06 1.67 0.49 0.41 5.17 38.1959.3868.6673.6623.78 ± ± ± ± ± 3.72 4.73 3.30 2.87 7.57 43.0371.8078.6688.6134.72 ± ± ± ± ± 8.22 5.02 3.17 1.18 8.18

Table 1: Classification accuracy of different GNNs trained with different number of labeled data per class
(#per class) on six benchmark graph node classification tasks. The highest accuracy is highlighted in bold for
each number of labeled data per class. These results show that GRAND++ is more effective in learning with
low-labeling rates than GRAND. (Unit: %)

GRAND-nl-rw with the corresponding model with a source term. We see that overall GRAND++-nl
(GRAND++-nl-rw) outperforms GRAND-nl (GRAND-nl-rw) when the network is deep, i.e., T is
big. We further study the low-labeling rate regimes in Appendix D.3.

Model Depth (T ) GRAND-nl [10] GRAND-nl-rw [10] GRAND++-nl (ours) GRAND++-nl-rw (ours)

CORA 163214 82.3182.1179.4279.70 ± ± ± ± 0.91 1.88 1.42 0.64 82.4782.0581.0179.07 ± ± ± ± 1.32 3.05 1.31 0.81 79.2481.2182.6483.24 ± ± ± ± 1.48 0.89 0.20 0.37 79.2482.2381.4882.20 ± ± ± ± 1.48 1.14 1.07 1.15

CiteSeer 12816641 65.1972.6570.2971.84 ± ± ± ± 2.98 2.42 2.58 6.77 73.0669.6565.4571.84 ± ± ± ± 2.66 2.98 2.50 7.18 72.4872.6470.4574.24 ± ± ± ± 1.10 0.93 2.12 0.70 71.7473.2973.3874.23 ± ± ± ± 1.37 1.37 0.95 0.70

PubMed 1641 77.9576.5177.93 ± ± ± 1.27 1.28 2.73 77.9378.0276.88 ± ± ± 1.26 1.14 2.57 **78.0178.4178.43 ± ± ± 0.68 0.88 0.78** 78.0178.1778.12 ± ± ± 0.68 0.93 0.87

Table 2: Classification accuracy of GRAND and GRAND++ variants of different depth trained 20 labeled data
per class. The highest accuracy is highlighted in bold for each of the depths T = 1, 4, 16, 32, 64, and 128. We
test T only up to 16 for PubMed and up to 32 for 32 since the neural ODE solver failed for larger T . (Unit: %)

7 CONCLUDING REMARKS

We propose GRAND++, which augments graph neural diffusion with a source term. We present
some theory that connects the model to a random walk formulation on graphs. GRAND++ outperforms many existing GNNs for graph deep learning with very deep architectures and when the
number of labeled data is limited. GRAND++ can be regarded as coupled ODE system in which
each ODE has an external force term. As such, it is natural to consider if advanced techniques in
accelerating training, test, and inference of neural ODEs can be leveraged to improve the efficiency
and accuracy of GRAND++, in particular high-order neural ODEs [19, 61, 42, 59] and noise injection [55]. It is interesting to note that the second-order neural ODE can be connected to the wave
equation in the graph setting, which can automatically bypass over-smoothing. We leave studying
the second-order neural ODE on graphs as future work.


-----

8 ACKNOWLEDGEMENT

This material is based on research sponsored by NSF grants DMS-1924935, DMS-1952339, DMS2027248 and NSF CCF-1934568, DOE grant DE-SC0021142, and ONR grant N00014-18-1-2527
and the MURI grant N00014-20-1-2787. MT would like to thank the Isaac Newton Institute for
Mathematical Sciences for support and hospitality during the programme Mathematics of Deep
_Learning when work on this paper was undertaken (EPSRC grant number EP/R014604/1) and ac-_
knowledge support from the European Union Horizon 2020 research and innovation programmes
under the Marie Skłodowska-Curie grant agreement No. 777826 (NoMADS). MT also holds a Turing Fellowship at the Alan Turing Institute.

REFERENCES

[1] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In International Conference on Learning Representations, 2021.

[2] James Atwood and Don Towsley. Diffusion-convolutional neural networks. In Advances in
_neural information processing systems, pages 1993–2001, 2016._

[3] Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction
networks for learning about objects, relations and physics. In Advances in Neural Information
_Processing Systems, pages 4502–4510, 2016._

[4] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data
representation. Neural Computation, 15(6):1373–1396, 2003.

[5] Mikhail Belkin and Partha Niyogi. Semi-supervised learning on Riemannian manifolds. Ma_chine learning, 56(1-3):209–239, 2004._

[6] Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Veliˇckovi´c. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478, 2021.

[7] Antoni Buades, Bartomeu Coll, and Jean M. Morel. Neighborhood filters and PDE’s. Nu_merische Mathematik, 105(1):1–34, 2006._

[8] Jeff Calder. The game theoretic p-Laplacian and semi-supervised learning with few labels.
_Nonlinearity, 32(1), 2018._

[9] Jeff Calder, Brendan Cook, Matthew Thorpe, and Dejan Slepcev. Poisson learning: Graph
based semi-supervised learning at very low label rates. In Hal Daum´e III and Aarti Singh,
editors, Proceedings of the 37th International Conference on Machine Learning, volume 119
of Proceedings of Machine Learning Research, pages 1306–1316. PMLR, 13–18 Jul 2020.

[10] Ben Chamberlain, James Rowbottom, Maria I Gorinova, Michael Bronstein, Stefan Webb,
and Emanuele Rossi. GRAND: Graph neural diffusion. In Marina Meila and Tong Zhang,
editors, Proceedings of the 38th International Conference on Machine Learning, volume 139
of Proceedings of Machine Learning Research, pages 1407–1418. PMLR, 18–24 Jul 2021.

[11] Ines Chami, Zhitao Ying, Christopher R´e, and Jure Leskovec. Hyperbolic graph convolutional
neural networks. Advances in neural information processing systems, 32:4868–4879, 2019.

[12] Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the
over-smoothing problem for graph neural networks from the topological view. In Proceedings
_of the AAAI Conference on Artificial Intelligence, volume 34, pages 3438–3445, 2020._

[13] Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.

[14] Fan RK Chung and Fan Chung Graham. Spectral graph theory. Number 92. American Mathematical Soc., 1997.


-----

[15] Ronald R Coifman, Stephane Lafon, Ann B Lee, Mauro Maggioni, Boaz Nadler, Frederick
Warner, and Steven W Zucker. Geometric diffusions as a tool for harmonic analysis and structure definition of data: Diffusion maps. Proceedings of the National Academy of Sciences,
102(21):7426–7431, 2005.

[16] Micha¨el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks
on graphs with fast localized spectral filtering. Advances in neural information processing
_systems, 29:3844–3852, 2016._

[17] Xavier Desquesnes, Abderrahim Elmoataz, and L´ezoray, Olivier. Eikonal equation adaptation
on weighted graphs: fast geometric diffusion process for local and non-local image and data
processing. Journal of Mathematical Imaging and Vision, 46(2):238–257, 2013.

[18] John R Dormand and Peter J Prince. A family of embedded Runge-Kutta formulae. Journal
_of Computational and Applied Mathematics, 6(1):19–26, 1980._

[19] Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. Augmented neural odes. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett, editors, Advances in
_Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019._

[20] David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel,
Alan Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning
molecular fingerprints. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett,
editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates,
Inc., 2015.

[21] Moshe Eliasof, Eldad Haber, and Eran Treister. PDE-GCN: Novel architectures for graph
neural networks motivated by partial differential equations. arXiv preprint arXiv:2108.01938,
2021.

[22] Abderrahim Elmoataz, Olivier Lezoray, and S´ebastien Bougleux. Nonlocal discrete regularization on weighted graphs: A framework for image and manifold processing. IEEE Transactions
_on Image Processing, 17(7):1047–1060, 2008._

[23] Francesco Farina and Emma Slade. Data efficiency in graph networks through equivariance.
_arXiv preprint arXiv:2106.13786, 2021._

[24] Mark Freidlin and Shuenn-Jyi Sheu. Diffusion processes on graphs: stochastic differential
equations, large deviation principle. Probability theory and related fields, 116(2):181–220,
2000.

[25] Mark I. Freidlin and Alexander D. Wentzell. Diffusion Processes on Graphs and the Averaging
Principle. The Annals of Probability, 21(4):2215 – 2245, 1993.

[26] Cristina Garcia-Cardona, Ekaterina Merkurjev, Andrea L. Bertozzi, Arjuna Flenner, and Allon G. Percus. Multiclass data segmentation using diffuse interface methods on graphs. IEEE
_Transactions on Pattern Analysis and Machine Intelligence, 36(8):1600–1613, 2014._

[27] Guy Gilboa and Stanley Osher. Nonlocal operators with applications to image processing.
_Multiscale Modeling & Simulation, 7(3):1005–1028, 2008._

[28] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl.
Neural message passing for quantum chemistry. In Doina Precup and Yee Whye Teh, editors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of
_Proceedings of Machine Learning Research, pages 1263–1272. PMLR, 06–11 Aug 2017._

[29] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran
Associates, Inc., 2017.

[30] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional
networks. In Proceedings of the 5th International Conference on Learning Representations,
ICLR ’17, 2017.


-----

[31] Johannes Klicpera, Stefan Weißenberger, and Stephan G¨unnemann. Diffusion improves graph
learning. Advances in Neural Information Processing Systems, 32:13354–13366, 2019.

[32] Guohao Li, Matthias M¨uller, Bernard Ghanem, and Vladlen Koltun. Training graph neural
networks with 1000 layers. arXiv preprint arXiv:2106.07476, 2021.

[33] Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as
deep as cnns? In Proceedings of the IEEE/CVF International Conference on Computer Vision,
pages 9267–9276, 2019.

[34] Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem. Deepergcn: All you need to
train deeper gcns. arXiv preprint arXiv:2006.07739, 2020.

[35] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for semi-supervised learning. In Thirty-Second AAAI conference on artificial intelli_gence, 2018._

[36] Renjie Liao, Zhizhen Zhao, Raquel Urtasun, and Richard Zemel. Lanczosnet: Multi-scale
deep graph convolutional networks. In International Conference on Learning Representations,
2019.

[37] Qi Liu, Maximilian Nickel, and Douwe Kiela. Hyperbolic graph neural networks. Advances
_in Neural Information Processing Systems, 32:8230–8241, 2019._

[38] Franc¸ois Lozes, Abderrahim Elmoataz, and Olivier L´ezoray. Partial difference operators on
weighted graphs for image processing on surfaces and point clouds. IEEE Transactions on
_Image Processing, 23(9):3896–3909, 2014._

[39] Stefano Massaroli, Michael Poli, Jinkyoo Park, Atsushi Yamashita, and Hajime Asama. Dissecting neural odes. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin,
editors, Advances in Neural Information Processing Systems, volume 33, pages 3952–3963.
Curran Associates, Inc., 2020.

[40] Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and
Michael M Bronstein. Geometric deep learning on graphs and manifolds using mixture model
cnns. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages
5425–5434, Los Alamitos, CA, USA, jul 2017. IEEE Computer Society.

[41] Federico Monti, Michael M Bronstein, and Xavier Bresson. Geometric matrix completion with
recurrent multi-graph neural networks. In Proceedings of the 31st International Conference on
_Neural Information Processing Systems, pages 3700–3710, 2017._

[42] Alexander Norcliffe, Cristian Bodnar, Ben Day, Nikola Simidjievski, and Pietro Li´o. On second order behaviour in augmented neural odes. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33,
pages 5911–5921. Curran Associates, Inc., 2020.

[43] Hoang Nt and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass
filters. arXiv preprint arXiv:1905.09550, 2019.

[44] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for
node classification. In International Conference on Learning Representations, 2020.

[45] Michael Poli, Stefano Massaroli, Junyoung Park, Atsushi Yamashita, Hajime Asama, and
Jinkyoo Park. Graph neural ordinary differential equations. arXiv preprint arXiv:1911.07532,
2019.

[46] Lev Semenovich Pontryagin. Mathematical theory of optimal processes. CRC press, 1987.

[47] Jiezhong Qiu, Jian Tang, Hao Ma, Yuxiao Dong, Kuansan Wang, and Jie Tang. Deepinf:
Social influence prediction with deep learning. In Proceedings of the 24th ACM SIGKDD
_International Conference on Knowledge Discovery & Data Mining, pages 2110–2119, 2018._


-----

[48] Mauricio Flores Rios, Jeff Calder, and Gilad Lerman. Algorithms for lp-based semi-supervised
learning on graphs. arXiv preprint arXiv:1901.05031, 2019.

[49] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep
graph convolutional networks on node classification. In International Conference on Learning
_Representations, 2020._

[50] Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear
embedding. science, 290(5500):2323–2326, 2000.

[51] Yulia Rubanova, Ricky T. Q. Chen, and David K Duvenaud. Latent ordinary differential
equations for irregularly-sampled time series. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alch´e-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing
_Systems, volume 32. Curran Associates, Inc., 2019._

[52] Zuoqiang Shi, Stanley J. Osher, and Wei. Zhu. Weighted nonlocal Laplacian on interpolation
from sparse data. Journal of Scientific Computing, 73(2-3):1164–1177, 2017.

[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in
_Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017._

[54] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and
Yoshua Bengio. Graph attention networks. In International Conference on Learning Rep_resentations, 2018._

[55] Bao Wang, Binjie Yuan, Zuoqiang Shi, and Stanley Osher. Resnets ensemble via the FeynmanKac formalism to improve natural and robust accuracies. In Advances in Neural Information
_Processing Systems, pages 1655–1665, 2019._

[56] Fei Wang, Changshui Zhang, Helen C Shen, and Jingdong Wang. Semi-supervised classification using linear neighborhood propagation. In 2006 IEEE Computer Society Conference on
_Computer Vision and Pattern Recognition (CVPR’06), volume 1, pages 160–167. IEEE, 2006._

[57] Yifei Wang, Yisen Wang, Jiansheng Yang, and Zhouchen Lin. Dissecting the diffusion process
in linear graph convolutional networks. arXiv preprint arXiv:2102.10739, 2021.

[58] Louis-Pascal Xhonneux, Meng Qu, and Jian Tang. Continuous graph neural networks. In
Hal Daum´e III and Aarti Singh, editors, Proceedings of the 37th International Conference on
_Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 10432–_
10441. PMLR, 13–18 Jul 2020.

[59] Hedi Xia, Vai Suliafu, Hangjie Ji, Tan Minh Nguyen, Andrea Bertozzi, Stanley Osher, and
Bao Wang. Heavy ball neural ordinary differential equations. In A. Beygelzimer, Y. Dauphin,
P. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Sys_tems, 2021._

[60] Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and
Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks. In
_International Conference on Machine Learning, pages 5453–5462. PMLR, 2018._

[61] Cagatay Yildiz, Markus Heinonen, and Harri Lahdesmaki. ODE2VAE: deep generative second
order odes with Bayesian neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d'Alch´e-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing
_Systems, volume 32. Curran Associates, Inc., 2019._

[62] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure
Leskovec. Graph convolutional neural networks for web-scale recommender systems. In Pro_ceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data_
_Mining, pages 974–983, 2018._


-----

[63] Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. In Proceed_ings of the 32nd International Conference on Neural Information Processing Systems, pages_
5171–5181, 2018.

[64] Lingxiao Zhao and Leman Akoglu. PairNorm: Tackling oversmoothing in GNNs. In Interna_tional Conference on Learning Representations, 2020._

[65] Dengyong Zhou, Olivier Bousquet, Thomas N Lal, Jason Weston, and Bernhard Sch¨olkopf.
Learning with local and global consistency. In Advances in neural information processing
_systems, pages 321–328, 2004._

[66] Dengyong Zhou and Bernhard Sch¨olkopf. Regularization on discrete spaces. In 27th DAGM
_Conference on Pattern Recognition, pages 361–368, 2005._

[67] Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using Gaussian fields and harmonic functions. In Proceedings of the 20th International Conference on
_Machine learning, pages 912–919, 2003._

[68] Juntang Zhuang, Nicha Dvornek, Xiaoxiao Li, and James S. Duncan. Ordinary differential
equations on graph networks. https://openreview.net/forum?id=SJg9z6VFDr, 2020.


-----

A BACKGROUND ON GRAPH DIFFERENTIAL OPERATORS

Let (X, W ) represent a graph where X = ([x[(1)]][⊤], . . ., [x[(][n][)]][⊤])[⊤] _∈_ R[n][×][d] is the matrix where
each row x[(][i][)] _∈_ R[d] is a feature vector and W = (Wij)[n]i,j=1 [is a][ n] _[×]_ _[n][ matrix with][ W][ij][ representing]_
the similarity (edge weight) between the ith and jth feature vector. We assume that we are dealing
with an undirected graph, i.e., Wij = Wji. A R[k][1] -valued function on the nodes of the graph can
be represented as a matrix U ∈ R[n][×][k][1] by U = ([u[(1)]][⊤], . . ., [u[(][n][)]][⊤])[⊤] and we define the inner
product


**_u[(][i][)]_** _· v[(][i][)]._
_i=1_

X


_⟨U_ _, V ⟩_ =


Similarly, a R[k][2] -valued function on the edges can be represented as a third-order tensor U ∈
R[n][×][n][×][k][2] which we write as


_U_ [(1][,][1)] _· · ·_ _U_ [(1][,n][)]
. .
.. ... ..
_U_ [(][n,][1)] _· · ·_ _U_ [(][n,n][)]


_U =_


and U [(][i,j][)] _∈_ R[k][2] . On edge functions we use the inner product


_,_ = [1]
_⟨U_ _V⟩_ 2


_Wij_ _._
_U_ [(][i,j][)] _· V_ [(][i,j][)]
_i,j=1_

X


Multiplication between a matrix and an edge function is usually defined pointwise and we use the
notation [A ⊙U][(][i,j][)] = AijU [(][i,j][)] _∈_ R[k][2], for a matrix A ∈ R[n][×][n] and an edge function U ∈
R[n][×][n][×][k][2], to make this clear. Similarly, pointwise multiplication between two matrices A, B ∈
R[n][×][n] is defined by [A ⊙ **_B]ij = AijBij ∈_** R. When a matrix is acting as a linear operator on a
node function we use the usual matrix-vector notation and write [AU ][(][i][)] = _j=1_ _[A][ij][u][(][j][)][ ∈]_ [R][k][1]

for a matrix A ∈ R[n][×][n] and node function U = ([u[(1)]][⊤], . . ., [u[(][n][)]][⊤])[⊤] _∈_ R[n][×][k][1] . In the sequel
we will have k1 = k2 = d. [P][n]

The gradient of a node-function U = ([u[(1)]][⊤], . . ., [u[(][n][)]][⊤])[⊤] _∈_ R[n][×][d] is defined as the edgefunction ∇U _∈_ R[n][×][n][×][d] with [∇U ][(][i,j][)] = u[(][j][)] _−_ **_u[(][i][)]_** _∈_ R[d]. The divergence divV =
([[divV][(1)]][⊤], . . ., [[divV][(][n][)]][⊤])[⊤] _∈_ R[n][×][d] of an edge-function V ∈ R[n][×][n][×][d] is defined as



[divV][(][i][)] =


_Wij_
_V_ [(][i,j][)]
_j=1_

X


for all i = 1, . . ., n. For anti-symmetric edge functions, i.e. V [(][i,j][)] = −V [(][j,i][)] for all i, j, we have
that the divergence is the negative adjoint to the gradient, i.e.

_⟨divV, U_ _⟩_ = −⟨V, ∇U _⟩._

B TECHNICAL PROOFS

**Proof:** [Proof of Proposition 1] For notational convenience let us assume that x[(][i][)](0) = x[(][i][)].
Clearly

E **_B[(][i][)](0)_** = x[(][i][)] = x[(][i][)](0)
h i

for all i = 1, . . ., n. Assume that

E **_B[(][i][)](k)_** = x[(][i][)](δtk)
h i


-----

for all i = 1, . . ., n. Then,

E **_B[(][i][)](k + 1)_**
h _n_ i

= **_x[(][j][)]P_** **_B[(][i][)](k + 1) = x[(][j][)][]_**

_j=1_

X 

_n_ _n_

= **_x[(][j][)]P_** **_B[(][ℓ][)](k) = x[(][j][)]|B[(][i][)](1) = x[(][ℓ][)][]_** P **_B[(][i][)](1) = x[(][ℓ][)][]_**

_j=1_ _ℓ=1_

X X  

_n_ _n_

= **_x[(][j][)]_** (1 _δt) 1i=ℓ_ + _[δ][t][W][iℓ]_ P **_B[(][i][)](1) = x[(][ℓ][)][]_**

_−_ _di_

Xj=1 Xℓ=1   

_n_ _n_ _n_

= (1 _δt)_ **_x[(][j][)]P_** **_B[(][i][)](1) = x[(][ℓ][)][]_** + _[δ][t]_ _Wiℓ_ **_x[(][j][)]P_** **_B[(][ℓ][)](k) = x[(][j][)][]_**
_−_ _di_

_j=1_ _ℓ=1_ _j=1_

X  X X 


= (1 _δt)E_ **_B[(][i][)](k)_** + _[δ][t]_
_−_ _di_
h i


_WiℓE_ **_B[(][ℓ][)](k)_**
_ℓ=1_

X h


= (1 _δt)x[(][i][)](δtk) +_ _[δ][t]_
_−_ _di_


_Wiℓx[(][ℓ][)](δtk)_

_ℓ=1_

X


= x[(][i][)](δtk) + _[δ][t]_

_di_


_WiℓWiℓ_ **_x[(][ℓ][)](δtk)_** **_x[(][i][)](δtk)_**
_−_
_ℓ=1_

X 


= x[(][i][)](δtk) − _δt [LX(δtk)][(][i][)]_

= x[(][i][)](δt(k + 1)),
as required.

**Proof:** [Proof of Proposition 2] Let P = _Pij_ _∈_ R[n][×][n] be the probability transition kernel, so
  1  _δt_ if i = j,

_Pij =_ _δt −Wij_

_di_ if i = j.

 _̸_

We have


_n_ _n_

_πiPij =_ _ndi_ (1 _δt1i=j +_ _[δ][t][W][ij]_
_i=1_ _i=1_ _k=1_ _[d][k]_  _−_ _di_ 

X X _n_

P _i=1_ _[W][ij]_

= _[d][j][(1]n[ −]_ _[δ][t][)]_ + _[δ][t]_ _n_
_k=1_ _[d][k]_ Pk=1 _[d][k]_

_dj_
= Pn P
_k=1_ _[d][k]_
= πj,
P
as required. □

**Proof:** [Proof of Proposition 3] The proof follows from a simple application of Propositions 1
and 2. Namely, for any i ∈{1, . . ., n}

_n_ _n_

**_x[(][i][)](kδt) = E_** **_B[(][i][)](k)_** = **_x[(][j][)](0)P_** **_B[(][i][)](k) = x[(][j][)][]_** _→_ **_x[(][j][)](0)πj =_** **_x_**

_j=1_ _j=1_

  X   X

as k →∞. e □


**Proof:** [Proof of Proposition 4] Let

_k_

**_y[(][i][)](k) = E_**



_s=0_

X



**_x[(][j][)]_** **_xˆ_** 1B(j)(s)=x(i) _._
_−_ 





_di_


_j∈I_


-----

Notice that

_k_

E 1B(j)(s)=x(i)

"s=0
X


_k_

= P **_B[(][j][)](s) = x[(][i][)][]_**

_s=0_

X 

_k_

= P **_B[(][j][)](0) = x[(][i][)][]_** + P **_B[(][j][)](s) = x[(][i][)][]_**

_s=1_

 _δij_ X 

_k_ _n_

| {z }

= δij + P **_B[(][j][)](s) = x[(][i][)]|B[(][j][)](s −_** 1) = x[(][ℓ][)][] P **_B[(][j][)](s −_** 1) = x[(][ℓ][)][]

_s=1_ _ℓ=1_

X X  

_k_ _n_

= δij + (1 _δt)δℓi +_ _[δ][t][W][ℓi]_ P **_B[(][j][)](s_** 1) = x[(][ℓ][)][]

_−_ _dℓ_ _−_

Xs=1 Xℓ=1   

_k_ _n_ _k_

_Wℓi_

= δij + (1 _δt)_ P **_B[(][j][)](s_** 1) = x[(][i][)][] + δt P **_B[(][j][)](s_** 1) = x[(][ℓ][)][]
_−_ _−_ _dℓ_ _−_

_s=1_ _ℓ=1_ _s=1_

X  X X 

_k−1_ _n_ _Wℓi_ _k−1_

= δij + (1 _δt)_ P **_B[(][j][)](s) = x[(][i][)][]_** + δt P **_B[(][j][)](s) = x[(][ℓ][)][]_**
_−_ _dℓ_

_s=0_ _ℓ=1_ _s=0_

X  X X 

= δij + (1 _δt)E_ _k−1_ 1B(j)(s)=x(i) + δt _n_ _Wℓi_ E _k−1_ 1B(j)(s)=x(ℓ) _._
_−_ "s=0 # _ℓ=1_ _dℓ_ "s=0 #

X X X

From the definition of Y and the above recursive relationship we have


_k_

1B(j)(s)=x(i)

"s=0
X


**_y[(][i][)](k) = [1]_**

_di_

= [1]

_di_


**_x[(][j][)]_** _−_ **_xˆ_**


_j∈I_

Xj∈I


_k−1_

1B(j)(s)=x(ℓ)

"s=0
X


**_x[(][j][)]_** **_xˆ_** _δij + (1_ _δt) [1]_
_−_ _−_ _di_



**_x[(][j][)]_** _−_ **_xˆ_**


_j∈I_ _s_

_k−1_

E 1B(j)(s)=x(ℓ)

"s=0
X


+ _[δ][t]_

_di_


_Wℓi_

_dℓ_


**_x[(][j][)]_** _−_ **_xˆ_**


_j∈I_


_ℓ=1_


_n_

= [1] **_x[(][j][)]_** **_xˆ_** _δij + (1_ _δt)y[(][i][)](k_ 1) + _[δ][t]_ _Wℓiy[(][ℓ][)](k_ 1)

_di_ _−_ _−_ _−_ _di_ _−_

_j_ _ℓ=1_

X∈I   X

= y[(][i][)](k 1) + [1] **_x[(][j][)]_** **_xˆ_** _δij_ _δt [LY (k_ 1)][(][i][)] _._
_−_ _di_ _j_ _−_ _−_ _−_

X∈I  


Now we let

so that W satisfies


**_w[(][i][)](k) = di_** **_z[(][i][)](kδt)_** **_y[(][i][)](k)_**
_−_



**_w[(][i][)](k) = w[(][i][)](k −_** 1) − _δt [LW (k −_ 1)][(][i][)] = [P W (k − 1)][(][i][)] _._

Hence, W (k) = P _[k]W (0). Since the stationary distribution of the random walk with transition_
kernel is π, we have limk→∞ **_P_** _[k]_ = 1π[⊤]. Hence, as k →∞, w[(][i][)](k) → [P]j[n]=1 _[π][j][w][(][j][)][(0) = 0]_
since w[(][j][)](0) = 0 by the choice in the initialisation of Z. □


-----

**Proof:** [Proof of Proposition 5] We prove the result by contradiction. Assume that there exists z
such that z[(][i][)](kδt) **_z for all i = 1, . . ., n as k_** . Then LZ(kδt) 0. Since we can write
_→_ _→∞_ _→_

**_z[(][i][)](kδt)_** **_z[(][j][)](kδt) = z[(][i][)]((k_** 1)δt) **_z[(][j][)]((k_** 1)δt)
_−_ _−_ _−_ _−_



[LZ((k 1)δt)][(][i][)] [LZ((k 1)δt)][(][j][)]
_−_ _−_ _−_


_δt_
_−_

+ δt


_δiℓ(x[(][i][)]_ **_xˆ)_** _δjℓ(x[(][j][)]_ **_xˆ)_**
_−_ _−_ _−_


_ℓ∈I_


then taking the limit k →∞ implies

_δiℓ(x[(][i][)]_ **_xˆ) =_**
_−_

Xℓ∈I

for all i, j which is clearly not true.


_δjℓ(x[(][j][)]_ **_xˆ)_**
_−_

Xℓ∈I


C NEURAL ODES AND TRAINING NEURAL ODES WITH ADJOINT METHOD

Neural ODEs [13] are a class of continuous-depth (-time) neural networks that are particularly suitable for learning complex dynamics from irregularly sampled sequential data, see, e.g.,

[13, 51, 19, 39, 42]. Mathematically, a neural ODE is the first-order ODE:

_dh(t)_

= f (h(t), t, θ), (18)
_dt_

where f (h(t), t, θ) ∈ R[d] is specified by a neural network parameterised by θ, e.g., a two-layer feedforward neural network. Starting from the input h(0), neural ODEs learn the representation and
perform prediction by solving (18) from t = 0 to T using a numerical integrator with a given error
tolerance, often with an adaptive step size solver (or adaptive solver for short) [18]. Solving (18)
from t = 0 to T in a single pass with an adaptive solver requires evaluating f (h(t), t, θ) at various
timesteps, with computational complexity counted by the number of forward function evaluations
(forward NFEs) [13].

The adjoint sensitivity method (or adjoint method) [46], is a memory-efficient method for training
neural ODEs. We regard the output h(T ) as the prediction and denote the loss between h(T ) and
the ground truth as L. Let a(t) := ∂L/∂h(t) be the adjoint state, then we have (see [13, 46] for
details)

_T_

_d_
_L_ **_a(t)[⊤]_** _[∂f]_ [(][h][(][t][)][, t, θ][)] _dt,_ (19)

_dθ_ [=] 0 _∂θ_
Z

with a(t) satisfying the following adjoint ODE

_da(t)_

= **_a(t)[⊤]_** _[∂]_ (20)
_dt_ _−_ _∂h_ _[f]_ [(][h][(][t][)][, t, θ][)][,]


which is solved numerically from t = T to 0 and also requires the evaluation of the right-hand side
of (20) at various timestamps, and the backward NFEs measure the computational complexity.

D EXPERIMENTAL DETAILS AND MORE EXPERIMENTAL RESULTS

D.1 DATASETS AND EXPERIMENTAL SETTINGS

**Graph node classification dataset.** Following [10], we consider the largest connected component
of seven graph node classification datasets, including CORA, CiteSeer, PubMed, coauthor graph
CoauthorCS, and Amazon co-purchasing graphs Computer and Photo, and a large scale ogbn-arxiv
dataset. For completeness, we list the number of classes, the number of features, and the number of
nodes and edges of each dataset in Table 3. More detailed information can be found in [10].

**Depth of GRAND and GRAND++ for the results in Table 1.** Table 4 lists the fine-tuned T for
the results in Table 1. Due to the limited time, we only search around the value of optimal T for
GRAND with grid spacing 0.1.


-----

Dataset Classes Features #Nodes #Edges

CORA 7 1433 2485 5069
CiteSeer 6 3703 2120 3679
PubMed 3 500 19717 44324
CoauthorCS 15 6805 18333 81894
Computer 10 767 13381 245778
Photo 8 745 7487 119043
ogbn-arxiv 40 128 169343 1166243

Table 3: Summary of the graph node classification datasets.

Model CORA CiteSeer PubMed CoauthorCS Computer Photo

GRAND++-l 18.3 8.0 13.0 4.0 3.2 3.6

GRAND-l 18.2948 7.8741 12.9423 3.2490 3.5824 3.6760

Table 4: The value of the fine-tuned T, i.e. depth of the continuous-depth GNNs, for GRAND and GRAND++
in learning with different labeling results, and the corresponding accuracy are reported in Table 1. The values
of T for GRAND++ are adopted from the paper [10].

D.2 CLASSIFICATION ACCURACY OF GNNS WITH DIFFERENT DEPTHS

In this subsection, we provide detailed numbers that correspond to Fig. 1. We further compare
GRAND++-l with several other GNN architectures, including GCN, GAT, and GraphSage, with
different depths on a few benchmark datasets. Table 5 lists the classification accuracy of GRAND++,
GRAND, and three benchmark GNNs with different depths on six graph node classification tasks.
Again, we see that GRAND++ is better than the other GNN models when the networks are deep.

Model depth CORA CiteSeer PubMed CoauthorCS Computer Photo

GRAND++-l(ours) 12825664321614 77.4881.9882.4980.9982.4880.2979.04 ± ± ± ± ± ± ± 1.37 1.43 1.42 1.76 0.71 1.98 2.94 71.2372.5872.8173.2973.84NANA ± ± ± ± ± 2.66 3.47 3.79 1.29 2.18 **79.4979.8178.1179.20NANANA ± ± ± ± 1.47 0.74 0.84 1.61** **90.2490.4290.89NANANANA ± ± ± 0.76 0.36 0.30** 84.1978.9784.1176.01NANANA ± ± ± ± 0.51 0.93 2.33 1.33 **92.9392.6992.9493.54NANANA ± ± ± ± 0.84 0.38 0.61 0.90**

GRAND-l[10] 12825664321614 77.2267.7978.5982.8082.7580.8782.19 ± ± ± ± ± ± ± 1.17 1.62 1.17 1.73 2.28 2.88 3.10 69.8473.8772.6571.9672.61NANA ± ± ± ± ± 2.74 2.12 2.42 3.15 2.66 77.9378.7978.7078.71NANANA ± ± ± ± 1.26 1.19 0.93 1.08 90.7987.6690.94NANANANA ± ± ± 0.93 0.21 1.70 69.5683.4184.2377.67NANANA ± ± ± ± 0.69 1.05 1.94 2.20 92.6692.3789.6192.47NANANA ± ± ± ± 0.42 0.53 0.27 1.33

GCN[30] 163214 76.9221.8681.3519.70 ± ± ± ± 0.56 1.27 7.06 6.09 70.5424.7824.2372.80 ± ± ± ± 1.69 6.61 1.45 1.65 77.1541.3640.6672.78 ± ± ± ± 1.80 3.00 1.77 1.86 91.5387.8414.4912.14 ± ± ± ± 0.45 0.96 0.91 1.64 21.1581.4475.7312.86 ± ± ± ± 13.10 0.24 1.02 2.39 24.3091.3190.1123.11 ± ± ± ± 0.19 0.66 0.73 1.76

GAT[54] 163241 72.4929.7529.1480.95 ± ± ± ± 2.03 2.28 1.02 1.57 71.8372.3124.8424.83 ± ± ± ± 1.53 2.82 1.45 1.45 77.2477.3739.2139.02 ± ± ± ± 0.72 1.32 0.43 0.12 79.2278.0524.2022.73 ± ± ± ± 0.60 1.10 2.22 2.08 73.9776.6737.0732.53 ± ± ± ± 1.20 2.79 3.09 2.99 87.0887.9525.5729.97 ± ± ± ± 0.37 1.76 4.03 3.68

GraphSage[29] 163214 73.4729.1479.8325.52 ± ± ± ± 2.43 1.02 1.98 6.45 50.0028.3871.9424.84 ± ± ± ± 14.27 2.54 1.45 1.45 39.2172.4237.5576.01 ± ± ± ± 2.35 4.39 0.61 3.92 87.9410.1291.747.91 ± ± ± ± 3.15 0.23 0.26 2.21 22.7937.0775.6275.95 ± ± ± ± 10.77 13.22 0.70 2.85 25.5720.0988.1090.68 ± ± ± ± 0.87 2.11 3.31 5.67

Table 5: Classification accuracy of different GNN models with different depths on six benchmark graph node
classification tasks. NA: neural ODE solver failed. These results show that GRAND++ is better suited for
learning with a very deep architecture than GRAND. (Unit: %)

D.3 MORE RESULTS ON TIME-DEPENDENT ATTENTION AND GRAPH REWIRING

We further explore the effects of the source term for GRAND-nl and GRAND-nl-rw in the lowlabeling rate regimes. Table 6 compares GRAND-nl and GRAND-nl-rw with the corresponding
model with a source term. We see that GRAND-nl and GRAND-nl-rw are almost always worse than
the vanilla GRAND-l, consistent with the results reported in [10]. GRAND++-nl and GRAND++nl-rw cannot help learning at low labeling rates anymore. However, when the labeling rates are
not low, GRAND++-nl or GRAND++-nl-rw can outperform GRAND-nl and GRAND-nl-rw, even
outperform GRAND++.


-----

Model #per class GRAND-nl [10] GRAND-nl-rw [10] GRAND++-nl (ours) GRAND++-nl-rw (ours)

CORA 1020512 50.5579.6082.2265.0676.93 ± ± ± ± ± 15.68 9.35 3.10 2.69 1.93 61.2450.6376.5079.3882.14 ± ± ± ± ± 17.71 16.19 3.91 3.25 2.49 48.8974.0180.1483.2459.96 ± ± ± ± ± 11.51 7.90 1.73 0.69 0.20 58.2547.9474.2580.1881.48 ± ± ± ± ± 11.06 11.97 1.99 0.40 1.07

CiteSeer 1020521 50.2559.8771.8872.8468.21 ± ± ± ± ± 10.89 17.66 5.08 6.94 6.61 59.9550.2068.0571.9272.72 ± ± ± ± ± 10.48 17.90 5.48 7.34 6.85 66.1368.8472.5259.1649.65 ± ± ± ± ± 5.45 8.13 2.09 2.84 1.24 53.1060.2667.8171.4573.87 ± ± ± ± ± 5.51 5.10 1.97 1.64 1.35

PubMed 1020215 66.9776.0378.5569.1772.56 ± ± ± ± ± 10.07 2.46 3.36 3.73 1.59 69.4272.6875.3278.3067.69 ± ± ± ± ± 7.89 2.13 2.52 3.45 1.43 63.8566.9871.4974.9478.41 ± ± ± ± ± 4.86 5.30 1.53 2.15 0.99 67.4569.1172.0575.0979.44 ± ± ± ± ± 3.88 1.80 3.67 2.88 0.56

Table 6: Classification accuracy of the variants of GRAND and GRAND++ models trained with different
numbers of labeled data per class (#per class) on graph node classification tasks. (Unit: %)

D.4 CLASSIFICATION ACCURACY OF GNNS WITH FEWER NUMBER OF TRAINING DATA

Besides the results shown in Fig. 1, we further test the classification accuracy of more benchmark
GNN architectures trained with fewer numbers of labeled data per class. Tables 7-9 list the classification accuracy, on the test set, of different benchmark GNN models when they are trained with
different numbers of labeled nodes per class.

#labeled nodes per class 1 2 5 10 20

GRAND-nl-rw (two-hop) [10]GRAND-nl-rw (gdc) [10]AdaLanczos [36]GRAND-nl [10]GraphSage [29]GRAND-l [10]Lanczos [36]MoNet [40]GCNN [58]GCN [30]GAT [54] 43.0443.3147.7253.7947.7247.4152.5348.2347.8652.6840.97 ± ± ± ± ± ± ± ± ± ± ± 17.72 12.48 11.82 14.87 15.53 11.82 11.95 16.40 15.38 14.01 15.53 64.5065.5450.5960.8560.2864.8258.3053.9660.8560.9461.46 ± ± ± ± ± ± ± ± ± ± ± 11.88 10.01 13.25 14.01 12.89 11.16 13.55 12.18 14.01 4.00 4.96 74.2874.2474.3374.9465.1373.8672.7576.0771.0468.1473.86 ± ± ± ± ± ± ± ± ± ± ± 6.28 7.04 9.14 7.97 3.07 4.21 5.08 5.74 6.95 3.25 7.97 79.6180.6472.5578.8276.1278.9280.2576.3175.0477.6178.82 ± ± ± ± ± ± ± ± ± ± ± 4.47 6.19 6.65 5.38 0.93 1.32 3.40 4.87 5.03 1.36 5.38 82.3782.4777.7682.0779.8581.8982.8679.9280.0481.0382.07 ± ± ± ± ± ± ± ± ± ± ± 1.98 1.93 4.21 2.03 1.82 1.12 2.39 2.28 2.54 1.56 2.03

Table 7: Classification accuracy of different GNNs trained with different numbers of labeled nodes per class.
Dataset: CORA.

#labeled nodes per class 1 2 5 10 20

GRAND-nl-rw (two-hop) [10]GRAND-nl-rw (gdc) [10]AdaLanczos [36]GRAND-nl [10]GraphSage [29]GRAND-l [10]Lanczos [36]MoNet [40]GCNN [58]GCN [30]GAT [54] 48.9448.8140.5839.1350.2050.0650.3150.3549.9649.1650.32 ± ± ± ± ± ± ± ± ± ± ± 14.27 10.24 11.45 11.37 15.32 17.98 17.90 18.62 17.74 3.63 7.42 54.3951.7159.5559.5759.9859.9557.6558.3558.0655.5548.52 ± ± ± ± ± ± ± ± ± ± ± 11.37 13.87 10.89 11.03 10.32 10.48 9.76 9.19 9.52 7.60 7.97 63.1667.2467.3761.6666.7267.3964.7968.3768.2168.3968.05 ± ± ± ± ± ± ± ± ± ± ± 12.26 4.19 5.08 5.16 6.61 9.38 8.20 5.00 7.08 5.81 5.49 72.1871.3568.9068.0871.0172.1571.9071.8871.8371.9267.06 ± ± ± ± ± ± ± ± ± ± ± 3.47 4.92 5.65 5.08 6.29 4.90 4.85 7.66 6.94 7.26 7.34 74.2173.2269.8472.0271.5272.1474.3373.0272.8472.8172.72 ± ± ± ± ± ± ± ± ± ± ± 2.90 2.90 1.77 2.82 4.11 2.00 2.83 5.89 6.61 6.94 6.85

Table 8: Classification accuracy of different GNNs trained with different numbers of labeled nodes per class.
Dataset: CiteSeer.

D.5 T-TEST OF THE ACCURACY IMPROVEMENT OF GRAND++ OVER GRAND

To confirm the statistical significance of the accuracy improvement of GRAND++ over GRAND in
Table 1, in this subsection, we conduct t-test experiments at 0.95 confidence to compare GRAND and
GRAND++ on six different benchmark graph node classification tasks. We first perform unpaired
t-tests to show the improvement of GRAND++ over GRAND on low labeled datasets using the
following t-score


**t-score =** _[µ][GRAND++][ −]_ _[µ][GRAND]_

_σGRAND++[2]_ _n_ + _[σ]GRAND++[2]_ _n_

q


(21)


-----

#labeled nodes per class 1 2 5 10 20

GRAND-nl-rw (two-hop) [10]GRAND-nl-rw (gdc) [10]AdaLanczos [36]GRAND-nl [10]GraphSage [29]GRAND-l [10]Lanczos [36]MoNet [40]GCNN [58]GCN [30]GAT [54] 58.6155.5360.7861.6562.1158.8461.7061.7556.4760.1261.07 ± ± ± ± ± ± ± ± ± ± ± 12.81 12.83 12.71 20.64 10.58 12.09 11.12 10.74 4.67 6.37 5.16 60.4560.2458.9765.1461.0363.6564.1169.0069.1669.4268.49 ± ± ± ± ± ± ± ± ± ± ± 16.20 14.44 12.65 19.45 6.93 6.97 6.88 7.55 8.46 8.21 8.99 72.7267.9270.6169.0573.9872.3572.3972.6868.6968.5466.07 ± ± ± ± ± ± ± ± ± ± ± 10.82 7.93 5.75 6.16 2.50 4.50 3.00 5.08 5.35 5.25 5.92 72.5972.4470.7471.2473.0172.7976.3376.0375.3275.7276.47 ± ± ± ± ± ± ± ± ± ± ± 3.19 3.50 6.03 3.11 1.54 3.27 2.74 3.41 3.72 3.45 3.50 76.8975.5579.2474.5576.4978.3578.1078.7678.5578.3078.77 ± ± ± ± ± ± ± ± ± ± ± 3.27 4.11 3.45 3.09 1.75 1.84 1.91 1.69 1.59 1.43 1.88

Table 9: Classification accuracy of different GNNs trained with different numbers of labeled nodes per class.
Dataset: PubMed.

where µ and σ[2] are the mean and variance of the performances of each model, and n is the number
of runs for each model. The t-test score are shown in Table 10.

#per class CORA CiteSeer PubMed CoauthorCS Computer Photo

1 1.05 **4.36** **3.28** **1.95** **111.60** **45.33**
2 1.39 **3.96** 0.34 **4.59** **7.18** **4.65**
5 **2.55** **2.68** -3.67 -1.96 **15.67** 0.26

Table 10: Unpaired t-test scores of GRAND++ v.s. GRAND on six different benchmark graph node classification tasks. With n = 100, over 0.95 confidence is equivalent to exceed roughly 1.66 t-test scores. Highlighted
are the ones passing the test.

For some entries in Table 10 that are not significant enough, we further conduct paired t-test between
GRAND++ and GRAND on these specific datasets as shown in Table 11. Since a large portion of
variance comes from splitting of the datasets, we pair up tests of GRAND and GRAND++ with the
same splitting in this experiment. In this case, a sample of difference of size n is computed, and
t-test score can be computed using the equation

**t-score =** _[µ][diff][ −]_ [0] (22)

_σdiff/[√]n_ _[.]_

Dataset #per class Accuracy Difference # splits t-score p-score

CORACORA 12 1.061.45 ± ± 6.24 5.23 100100 1.802.78 0.0440.003

Table 11: Paired t-test scores of GRAND++ v.s. GRAND on datasets where unpaired t-test scores are not
significant enough.

D.6 TASKS FOR FURTHER EVALUATING DEEP GRAND AND GRAND++

**Open graph benchmark with paper citation network (ogbn-arxiv).** Ogbn-arxiv consists of 169,
343 nodes and 1, 166, 243 directed edges. Each node is an arxiv paper represented by a 128dimensional features and each directed edge indicates the citation direction. This dataset is used
for node property prediction and has been a popular benchmark to test the advantage of deep graph
neural networks over shallow graph neural networks [34, 32]. Compared to the GRAND model used
in [10], we reduce the hidden dimension from 162 to 81 to fit the model into the GPU in our lab.

Model depth (T ) GRAND-l [10] GRAND++-l (ours) Improvement from GRAND++-l (ours)

OGBN-arXiv 6496321468 63.4755.9568.5069.5369.4669.4467.44 ± ± ± ± ± ± ± 0.76 0.21 0.43 0.30 0.28 1.24 0.59 **68.7969.6869.7169.6169.4168.0567.26 ± ± ± ± ± ± ± 0.35 0.38 0.24 0.28 0.53 0.73 0.61** 11.310.290.150.250.171.974.58

Table 12: Classification accuracy of the linear GRAND and GRAND++ models trained with different depth
on the OGBN-arXiv graph node classification task. Compared to the GRAND model used in [10], we reduce
the hidden dimension from 162 to 81 to fit the model into the GPU in our lab. (Unit: %)


-----

