# HYBRID LOCAL SGD FOR FEDERATED LEARNING
## WITH HETEROGENEOUS COMMUNICATIONS


**Yuanxiong Guo**
The University of Texas at San Antonio
San Antonio, Texas 78249 USA
yuanxiong.guo@utsa.edu

**Rui Hu & Yanmin Gong**
The University of Texas at San Antonio
San Antonio, Texas 78249 USA
_{rui.hu@my.,yanmin.gong@}utsa.edu_

ABSTRACT


**Ying Sun**
Pennsylvania State University
State College, PA 16801 USA
ysun@psu.edu


Communication is a key bottleneck in federated learning where a large number
of edge devices collaboratively learn a model under the orchestration of a central
server without sharing their own training data. While local SGD has been proposed to reduce the number of FL rounds and become the algorithm of choice
for FL, its total communication cost is still prohibitive when each device needs to
communicate with the remote server repeatedly for many times over bandwidthlimited networks. In light of both device-to-device (D2D) and device-to-server
(D2S) cooperation opportunities in modern communication networks, this paper
proposes a new federated optimization algorithm dubbed hybrid local SGD (HLSGD) in FL settings where devices are grouped into a set of disjoint clusters with
high D2D communication bandwidth. HL-SGD subsumes previous proposed algorithms such as local SGD and gossip SGD and enables us to strike the best
balance between model accuracy and runtime. We analyze the convergence of
HL-SGD in the presence of heterogeneous data for general nonconvex settings.
We also perform extensive experiments and show that the use of hybrid model
aggregation via D2D and D2S communications in HL-SGD can largely speed up
the training time of federated learning.

1 INTRODUCTION

Federated learning (FL) is a distributed machine learning paradigm in which multiple edge devices
or clients cooperate to learn a machine learning model under the orchestration of a central server,
and enables a wide range of applications such as autonomous driving, extended reality, and smart
manufacturing (Kairouz et al., 2021). Communication is a critical bottleneck in FL as the clients are
typically connected to the central server over bandwidth-limited networks. Standard optimization
methods such as distributed SGD are often not suitable in FL and can cause high communication
costs due to the frequent exchange of large-size model parameters or gradients. To tackle this issue,
local SGD, in which clients update their models by running multiple SGD iterations on their local
datasets before communicating with the server, has emerged as the de facto optimization method in
FL and can largely reduce the number of communication rounds required to train a model (McMahan
et al., 2017; Stich, 2019).

However, the communication benefit of local SGD is highly sensitive to non-iid data distribution as
observed in prior work (Rothchild et al., 2020; Karimireddy et al., 2020). Intuitively, taking many
local iterations of SGD on local dataset that is not representative of the overall data distribution will
lead to local over-fitting, which will hinder convergence. In particular, it is shown in (Zhao et al.,
2018) that the convergence of local SGD on non-iid data could slow down as much as proportionally
to the number of local iteration steps taken. Therefore, local SGD with a large aggregation period


-----

can converge very slow on non-iid data distribution, and this may nullify its communication benefit
(Rothchild et al., 2020).

Local SGD assumes a star network topology where each device connects to the central server for
model aggregation. In modern communication networks, rather than only communicating with the
server over slow communication links, devices are increasingly connected to others over fast communication links. For instance, in 5G-and-beyond mobile networks, mobile devices can directly
communicate with their nearby devices via device-to-device links of high data rate (Asadi et al.,
2014; Yu et al., 2020). Also, edge devices within the same local-area network (LAN) domain can
communicate with each other rapidly without traversing through slow wide-area network (WAN)
(Yuan et al., 2020). This gives the potential to accelerate the FL convergence under non-iid data
distribution by leveraging fast D2D cooperation so that the total training time can be reduced in FL
over bandwidth-limited networks.

Motivated by the above observation, this paper proposes hybrid local SGD (HL-SGD), a new distributed learning algorithm for FL with heterogeneous communications, to speed up the learning
process and reduce the training time. HL-SGD extends local SGD with fast gossip-style D2D communication after local iterations to mitigate the local over-fitting issue under non-iid data distribution
and accelerate convergence. A hybrid model aggregation scheme is designed in HL-SGD to integrate both fast device-to-device (D2D) and slow device-to-server (D2S) cooperations. We analyze
the convergence of HL-SGD in the presence of heterogeneous data for general nonconvex settings,
and characterize the relationship between the optimality error bound and algorithm parameters. Our
algorithm and analysis are general enough and subsume previously proposed SGD variations such
as distributed SGD, local SGD and gossip SGD.

Specifically, we consider the FL setting in which all devices are partitioned into disjoint clusters,
each of which includes a group of connected devices capable of communicating with each other
using fast D2D links. The clustering can be a natural result of devices belonging to different LAN
domains so that those devices connected to the same LAN domain are considered as one cluster. In
another example, clustering is based on the geographic locations of mobile devices so that devices
in a cluster are connected to each other through D2D communication links.

In summary, the paper makes the following main contributions:

_• We propose a novel distributed learning algorithm for FL called HL-SGD to address the commu-_
nication challenge of FL over bandwidth-limited networks by leveraging the availability of fast D2D
links to accelerate convergence under non-iid data distribution and reduce training time.

_• We provide the convergence analysis of HL-SGD under general assumptions about the loss func-_
tion, data distribution, and network topology, generalizing previous results on distributed SGD, local
SGD, and gossip SGD.

_• We conduct extensive empirical experiments on two common benchmarks under realistic network_
settings to validate the established theoretical results of HL-SGD. Our experimental results show
that HL-SGD can largely accelerate the learning process and speed up the runtime.

2 BACKGROUND AND RELATED WORK

Large-scale machine learning based on distributed SGD has been well studied in the past decade, but
often suffers from large network delays and bandwidth limits (Bottou et al., 2018). Considering that
communication is a major bottleneck in federated settings, local SGD has been proposed recently to
reduce the communication frequency by running SGD independently in parallel on different devices
and averaging the sequences only once in a while (Stich, 2019; Lin et al., 2019; Haddadpour et al.,
2019; Yu et al., 2019; Wang et al., 2021). However, they all assume the client-server architecture and
do not leverage the fast D2D communication capability in modern communication networks. Some
studies (Liu et al., 2020; Abad et al., 2020; Castiglia et al., 2021) develop hierarchical FL algorithms
that first aggregate client models at local edge servers before aggregating them at the cloud server or
with neighboring edge servers, but they still rely on D2S communication links only and suffer from
the scalability and fault-tolerance issues of centralized setting. On the other hand, while existing
works on decentralized or gossip SGD consider D2D communications (Tsitsiklis, 1984; Boyd et al.,
2006), they assume a connected cluster with homogeneous communication links and will converge


-----

very slow on the large and sparse network topology that is typically found in FL settings. Unlike
previous works, HL-SGD leverages both D2S and D2D communications in the system.

Some recent studies aim to encapsulate variants of SGD under a unified framework. Specifically,
a cooperative SGD framework is introduced in (Wang & Joshi, 2021) that includes communication reduction through local SGD steps and decentralized mixing between clients under iid data
distribution. A general framework for topology-changing gossip SGD under both iid and non-iid
data distributions is proposed in (Koloskova et al., 2020). Note that all of the above works assume
undirected network topology for communications in every iteration. In comparison, our proposed
HL-SGD is different: the D2S communication is asymmetric due to the use of device sampling
and model broadcasting in each global aggregation round and cannot be modeled in an undirected
graph. Therefore, the convergence analysis of HL-SGD does not fit into the prior frameworks and
is much more challenging. Moreover, our major focus is on the runtime of the algorithm rather than
its convergence speed in iterations.

3 SYSTEM MODEL

In this section, we introduce the FL system model, problem formulation, and assumptions we made.

**Notation. All vectors in this paper are column vectors by default. For convenience, we use 1 to**
denote the all-ones vector of appropriate dimension, 0 to denote the all-zeros vector of appropriate
dimension, and [n] to denote the set of integers {1, 2, . . ., n} with any positive integer n. Let ∥·∥
denote the ℓ2 vector norm and Frobenius matrix norm and ∥·∥2 denote the spectral norm of a matrix.

We consider a FL system consisting of a central server and K disjoint clusters of edge devices.
Devices in each cluster k ∈ [K] can communicate with others across an undirected and connected
graph Gk = (V, Ek), where Vk denotes the set of edge devices in the cluster, and edge (i, j) ∈Ek
denotes that the pair of devices i, j ∈Vk can communicate directly using D2D as determined by the
communication range of D2D links. Besides, each device can directly communicate with the central
server using D2S links. Denote the set of all devices in the system as := _k_ [K]
_V_ _∈_ _[V][k][, the number]_

of devices in each cluster k [K] as n := _k_, and the total number of devices in the system as
_∈_ _|V_ _|_
_N :=_ _k_ [K] _[n][ 1][.]_ [S]

_∈_

The FL goal of the system is to solve an optimization problem of the form:

[P]

min _fi(x) := [1]_ _f¯k(x),_ (1)
_x_ R[d][ f] [(][x][) := 1]N _K_
_∈_ Xi∈V _kX∈[K]_

where fi(x) := Ez _i_ [ℓi(x; z)] is the local objective function of device i, _f¯k(x)_ :=
_∼D_
(1/n) _i_ _k_ _[f][i][(][x][)][ is the local objective function of cluster][ k][, and][ D][i][ is the data distribution of]_
_∈V_
device i. Here ℓi is the (non-convex) loss function defined by the learning model and z represents a
data sample from data distribution _i._

[P] _D_

When applying local SGD to (1) in FL with heterogeneous communications, the communications
between the server and devices in FL are all through D2S links that are bandwidth-limited, particularly for the uplink transmissions. Therefore, the incurred communication delay is high. Due to the
existing of high-bandwidth D2D links that are much more efficient than low-bandwidth D2S links,
it would be highly beneficial if we can leverage D2D links to reduce the usage of D2S links such
that the total training time can be reduced. This motivates us to design a new learning algorithm for
FL with heterogeneous communications.

4 HYBRID LOCAL SGD

In this section, we present our HL-SGD algorithm suitable for the FL setting with heterogeneous
communications. Algorithm 1 provides pseudo-code for our algorithm.

At the beginning of r-th global communication round, the server broadcasts the current global model
_x[r]_ to all devices in the system via cellular links (Line 4). Note that in typical FL systems, the down
1For presentation simplicity, we assume each cluster contains the same number of devices here. The results
of this paper can be extended to the case of clusters with different device numbers as well.


-----

**Algorithm 1 HL-SGD: Hybrid Local SGD**
**Input: initial global model x[0], learning rate η, communication graph Gk and mixing matrix Wk for**
all clusters k ∈ [K], and fraction of sampled devices in each cluster p.
**Output: final global model x[R]**

1: for each round r = 0, . . ., R − 1 do
2: **for each cluster k ∈** [K] in parallel do

3: **for each device i ∈Vk in parallel do**

4: _x[r,]i_ [0] = x[r]

5: **for s = 0, . . ., τ −** 1 do

6:7:8: Compute a stochastic gradientxxir,s[r,s]i +[+1][1]2 == x[r,s]ij∈N−i[k]ηg[(][W]i([k]x[)][r,s]i[i,j][)][x]r,sj + g[1]2i over a mini-batch ξi sampled from▷ gossip averaging▷ local update Di

9: **end for**

10: **end for** [P]

11: **end for**

12: **for each cluster k ∈** [K] do

13: _m ←_ max(p · n, 1)

14: _Sk[r]_ _▷_ device sampling

15: **end for[←]** [(][random set of][ m][ clients in][ V][k][)]

16: _x[r][+1]_ = _K[1]_ _k∈[K]_ _m1_ _i∈Sk[r]_ _[x]i[r,τ]_ _▷_ global aggregation

17: end for

P P

18: return x[R]

link communication is much more efficient than uplink communication due to the larger bandwidth
allocation and higher data rate. Therefore, devices only consume a smaller amount of energy when
receiving data from the server compared with transmitting data to the server.

After that, devices in each cluster initialize their local models to be the received global model and
run τ iterations of gossip-based SGD via D2D links to update their local models in parallel (lines 5–
9). Let x[r,s]i denote the local model of device i at the r-th local iteration of s-th round. Here
each gossip-based SGD iteration consists of two steps: (i) SGD update, performed locally on each
device (lines 6–7), followed by a (ii) gossip averaging, where devices average their models with
their neighbors (line 8). In the gossip averaging protocol, Ni[k] [denotes the neighbors of device][ i][,]
including itself, on the D2D communication graphthe mixing matrix of cluster k with each element (W Gkk) of clusteri,j being the weight assigned by device k, and Wk ∈ [0, 1][n][×][n] denotes i to
device j. Note that (Wk)i,j > 0 only if devices i and j are directly connected via D2D links.

Next, a set Sk[r] [of][ m][ devices are sampled uniformly at random (u.a.r.) with probability][ p][ without]
replacement from each cluster k ∈ [K] by the server (lines 13–14), and their final updated local
models {x[r,τ]i _, ∀i ∈_ _Sk[r][}][ are sent to the server via D2S links. After that, the server updates the global]_
model x[r][+1] by averaging the received local models from all sampled devices (line 16). Note that
only m devices per cluster will upload their models to the server in each round to save the usage of
expensive D2S uplink transmissions. The intuition is that after multiple iterations of gossip-based
SGD, devices have already reached approximate consensus within each cluster, and the sampled
average can well represent the true average. By trading D2D local aggregation for D2S global
aggregation, the total communication cost can be reduced. We will empirically validate such benefits
later in the experiments.

It is worth noting that HL-SGD inherits the privacy benefits of classic FL schemes by keeping the
raw data on device and sharing only model parameters. Moreover, HL-SGD is compatible with
existing privacy-preserving techniques in FL such as secure aggregation (Bonawitz et al., 2017; Guo
& Gong, 2018), differential privacy (McMahan et al., 2018; Hu et al., 2020; 2021), and shuffling
(Girgis et al., 2021) since only the sum rather than individual values is needed for the local and
global model aggregation steps.

**Runtime analysis of HL-SGD. We now present a runtime analysis of HL-SGD. Here we ignore**
the communication time of downloading models from the server by each device since the download
bandwidth is often much larger than upload bandwidth for the D2S communication in practice (?).
In each round of HL-SGD, we denote the average time taken by a device to compute a local update,


-----

perform one round of D2D communication and one round of D2S communication as ccp, cd2d and
_cd2s, respectively. Assume the uplink bandwidth between the server and devices is fixed and evenly_
shared among the sampled devices in each round, then cd2s is linearly proportional to the sampling
ratio p. Similarly, ccp depends on the D2D network topology Gk and typically increases with the
maximum node degree ∆( _k). The total runtime of HL-SGD after R communication rounds is_
_G_

_R × [τ × (ccp + cd2d) + cd2s] ._ (2)

The specific values of ccp, cd2d and cd2s depend on the system configurations and applications. In
comparison, the total runtime of local SGD after R communication rounds is R × [τ × ccp + cd2s].

**Previous algorithms as special cases. When devices do not communicate with each other, i.e.,**
_Wk = I, ∀k ∈_ [K], and sampling ratio p = 1, HL-SGD reduces to distributed SGD (when τ = 1)
or local SGD (when τ > 1) where each device only directly communicates with the server with D2S
links. Also, when τ →∞, HL-SGD reduces to gossip SGD where devices only cooperate with their
neighboring devices through a gossip-based communication protocol with D2D links to update their
models without relying on the server. Therefore, HL-SGD subsumes existing algorithms and enables
us to strike the best balance between runtime and model accuracy by tuning τ, Wk, and p. However,
due to the generality of HL-SGD, there exist significantly new challenges in its convergence analysis,
which constitutes one of the main contributions of this paper as elaborated in the following section.

5 CONVERGENCE ANALYSIS OF HL-SGD

In this section, we analyze the convergence of HL-SGD with respect to the gradient norm of the
objective function f (·), specifically highlighting the effects of τ and p. Before stating our results,
we make the following assumptions:

**Assumption 1 (Smoothness). Each local objective function fi : R[d]** _→_ R is L-smooth for all i ∈V,
_i.e., for all x, y ∈_ R[d],
_fi(x)_ _fi(y)_ _L_ _x_ _y_ _,_ _i_ _._
_∥∇_ _−∇_ _∥≤_ _∥_ _−_ _∥_ _∀_ _∈V_

**Assumption 2 (Unbiased Gradient and Bounded Variance). The local mini-batch stochastic gra-**
_dient in Algorithm 1 is unbiased, i.e., Eξi_ [gi(x)] = ∇fi(x), and has bounded variance, i.e.,
Eξi _∥gi(x) −∇fi(x)∥[2]_ _≤_ _σ[2], ∀x ∈_ R[d], i ∈V, where the expectation is over all the local mini_batches._

**Assumption 3 (Mixing Matrix). For any cluster k ∈** [K], the D2D network is strongly connected
_We also assumeand the mixing matrix ||Wk − W(1k ∈/n)[011, 1][⊤][n]||[×]2[n] ≤satisfiesρk for some Wk1 ρ =k 1 ∈, 1[0[⊤], 1)W.k = 1[⊤], null(I −_ _Wk) = span(1)._

**Assumption 4(1/n)** _i∈Vk_ (Bounded Intra-Cluster Dissimilarity)fk(x)∥[2] _≤_ _ϵ[2]k_ _[for any][ x][ ∈]. There exists a constant[R][d][ and][ k][ ∈]_ [[][K][]][. If local functions are] ϵk ≥ 0 such that
_identical to each other within a cluster, then we have[∥∇][f][i][(][x][)][ −∇]_ [¯] _ϵk = 0._

**Assumption 5[P]** (Bounded Inter-Cluster Dissimilarity). There exist constants α ≥ 1, ϵ ≥ 0 such that
(1each other across all clusters, then we have/K) _k∈[K]_ _[∥∇]f[¯]k(x)∥[2]_ _≤_ _α[2]_ _∥∇f_ (x)∥[2] α+ = 1 ϵ[2]g _[for any], ϵg = 0[ x].[ ∈]_ [R][d][. If local functions are identical to]

[P]

Assumptions 1–3 are standard in the analysis of SGD and decentralized optimization (Bottou et al.,
2018; Koloskova et al., 2019). Assumptions 4–5 are commonly used in the federated optimization
literature to capture the dissimilarities of local objectives (Koloskova et al., 2020; Wang et al., 2020).

5.1 MAIN RESULTS

We now provide the main theoretical results of the paper in Theorem 1 and Theorem 2. The detailed
proofs are provided in the appendices. Define the following constants:


1

_, τ_ _,_ _ϵ¯[2]L_ [= 1]
1 _ρmax_ _K_
_−_ 


_ϵ[2]k_ (3)
_k=1_

X


_ρmax = max_ _Dτ,ρ = min_
_k_ [K] _[ρ][k][,]_
_∈_


-----

and let

_σ2_
_r0 = 8(f_ (x[0]) _f_ (x[⋆])), r1 = 16L _,_
_−_ _N_
  1 (4)

_r2 = 16C1L[2]τ_ [2]ϵ[2]g [+ 16][C][1][L][2] _τρ[2]max[D][τ,ρ]ϵ[¯][2]L_ [+][ τσ][2] max _._

_n_ [+][ ρ][2]

  

**Theorem 1 (Full device participation). Let Assumptions 1–5 hold, and let L, σ, ¯ϵL, ϵg, Dτ,ρ, ρmax,**
_r0, r1, and r2 be as defined therein. If the learning rate η satisfies_


1

2




1

3




1 _r0_

4C1α _τL_ _[,]_ _r1τR_

_[·][ 1]_ 


_r0_

_r2τR_


(5)


_η = min_


_then for any R > 0, the iterates of Algorithm 1 with full device participation for HL-SGD satisfy_


 τ [2]ϵ[2]g [+][ τρ]max[2] _[D]τ,ρ(ϵ[¯]τR[2]L_ [+]) 23[ τ]   1n [+][ ρ]max[2]  _σ[2][][ 1]3_ + R[1]


min _x[r,s])_ = O
_r,s_ [E][∥∇][f] [(¯] _∥[2]_


_,_ (6)






_NτR_


_where ¯x[r,s]_ = _N[1]_ _Ni=1_ _[x]i[r,s][.]_

In the following, we analyze the iteration complexity of HL-SGD and compare it with those ofP
some classic and state-of-the-art algorithms relevant to our setting in Table 1. First, we consider
two extreme cases of HL-SGD where ρmax = 0 and ρk = 1, ∀k ∈ [K], and show that our analysis
recovers the best known rate of local SGD.

**Fully Connected D2D networks. In this case, ρmax = 0, and each cluster can be viewed as a single**
device, and thus HL-SGD reduces to local SGD with K devices. Substuting ρmax = 0 into (6),

1/3 2/3

the iteration complexity of HL-SGD reduces to O(σ/√NτR + _τ_ [2]ϵ[2]g [+][ τ][ ·][ (][σ][2][/n][)] _/(τR)_ +

1/R). This coincides with the complexity of local SGD provided in Table 1 with device number K
  
and stochastic gradient variance σ[2]/n thanks to the fully intra-cluster averaging.

**Disconnected D2D networks.** In this case, HL-SGD reduces to local SGD with N devices.
Substituting ρmax = 1 into (6), the iteration complexity of HL-SGD becomes O(σ/√NτR +

_τ_ [2](ϵ[2]g [+ ¯]ϵ[2]L[) +][ τσ][2][][1][/][3][ /][(][τR][)][2][/][3][ + 1][/R][)][. This coincides with the complexity of local SGD with]
_N devices, stochastic gradient variance σ[2], and gradient heterogeneity of order ϵ[2]g_ [+ ¯]ϵ[2]L[.]
 


Table 1: Comparison of Iteration Complexity. [2]


_σ_

_NτR_ [+ (][τ][ 2][ϵ]([2]τR[+][τσ]) 23 [2][)]


Local SGD

Gossip SGD

Gossip PGA (Chen et al., 2021)

HL-SGD (this work)


_τR_


_σ_ _ρ_ 3 ϵ 3 _ρ_ 3 σ 3

_NτR_ [+] (τR1 ) 23 (11 _−ρ)_ 23 [+] (τR) 32 (1−ρ) 13 [+]


(1−ρ)τR


_NτRσ_ [+] _Cτ,ρ3_ _[D](τRτ,ρ3_ )[′]23[ρ] 3 ϵ 3 + _[C]τ,ρ(13τR[ρ]_ )32 σ23 32 + _ρDτRτ,ρ′_

!

1

_σ_ _g[+][τρ]max[2]_ _[D][τ,ρ]ϵ[¯][2]L[)]_ 3 _n_ [+][ρ]max[2] [)][σ][2][)]

_NτR_ [+ (][τ][ 2][ϵ][2] (τR) 23 + [(][τ][(][ 1] (τR) 23


_τR_


Next, we compare the complexities of HL-SGD, local SGD, gossip SGD and gossip PGA.

**Comparison to Local SGD. Comparing (6) and the complexity of local SGD, we can see the intra-**
cluster D2D communication provably improves the iteration complexity by reducing the transient
iterations. This is reflected in the smaller coefficient associated with the O((τR)[−][2][/][3]) term. In particular, improving D2D communication connectivity will lead to a smaller ρmax and consequently,
mitigate the impact of both local data heterogeneity and stochastic noise on the convergence rate.

1The convergence rates for gossip SGD and local SGD are from (Koloskova et al. (2020)). The parameters
in the table are given by the following: σ[2]: stochastic gradient variance; ρ: network connectivity; ϵ[2]: data
heterogeneity of order ϵ[2]g [+ ¯]ϵ[2]L[;][ C][τ,ρ] [≜] [P][τ]k=0[−][1] _[ρ][k][,][ D][τ,ρ][′][ = min][{][1][/][(1][ −]_ _[ρ][)][, τ]_ _[}][. Note that][ D][τ,ρ][ ̸][=][ D][τ,ρ][′]_ [.]


-----

**Comparison to Gossip SGD. Under the condition that ρ = ρmax, i.e., the connectivity of D2D**
network in gossip SGD is the same as that of HL-SGD, Table 1 shows HL-SGD outperforms gossip
SGD when τ/n ≤ _ρ[2]/(1−ρ). In other words, HL-SGD is beneficial for weakly connected networks,_
which is the case in FL settings where a large number of devices are often loosely connected or
disconnected into several disjoint clusters via D2D communications only.

**Comparison to Gossip PGA. Gossip PGA improves local SGD by integrating gossiping among all**
devices in one round using a connected network. Compared to gossip SGD, gossip PGA has one
extra full averaging step with period τ . The complexity of gossip PGA improves both by reducing the transient iterations. HL-SGD (full participation) differs from gossip PGA in the sense that
gossiping is performed within multiple clusters instead of a single one. The benefit comes from the
fact that for many commonly used D2D network topologies, the spectral gap 1 − _ρ decreases as the_
network size decreases, see Table 2. Therefore, when employing the same D2D network topology,
HL-SGD enjoys a smaller connectivity number ρmax than ρ. Considering the scenario where τ and
_n are fixed while the cluster number K grows, the total device number N = nK grows and hence_
_ρ_ 1 for gossip PGA. In the case when τ = Dτ,ρ[′] _Cτ,ρ, the fastest decaying O(1/τR) terms_
_→_ _≈_
are comparable for both algorithms. However, the O((τR)[−][2][/][3]) term of gossip GPA can be larger
than that of HL-SGD since ρ increases with N . This observation shows for large-scale networks,
it is advantageous to use HL-SGD with multiple connected clusters instead of gossip GPA with a
single cluster under the D2D network topology.

Our next result shows the iteration complexity of HL-SGD with partial device participation. We
assume the devices participate in synchronizing their models at the end of each FL round following
the sampling rule given by Assumption 6.
**Assumption 6 (Sampling strategy). Each Sk[r]** _[contains a subset of][ m][ indices uniformly sampled from]_
_{1, . . ., n} without replacement. Furthermore, Sk[r]_ _[is independent of][ S]k[r][′][′][ for all][ (][k, r][)][ ̸][= (][k][′][, r][′][)][.]_

**Theorem 2 (Partial device participation). Let Assumptions 1–6 hold, and let L, σ, ¯ϵL, ϵg, Dτ,ρ,**
_ρmax, r0, r1, and r2 be as defined therein. If the network connectivity satisfies_
_ρmax_ 1 1/τ, (7)
_≤_ _−_
_then for suitably chosen learning rate η, the iterates of Algorithm 1 with partial device participation_
_for HL-SGD satisfy_

min _x[r,s])_
_r,s_ [E][∥∇][f] [(¯] _∥[2]_

_ϵL, σ, ρmax)_ _τ_ [2]ϵ[2]g [+][ τρ]max[2] _[D]τ,ρϵ[¯][2]L_ [+][τ] 1n [+][ ρ]max[2] _σ[2][][ 1]3_

=O _√NτR_ +   (τR) 23    + [max][{][1][, G][p]R[D][τ,ρ][ρ][max][}]  _,_

 _[σ][ +][ E][(][ϵ][g][,][ ¯]_ (8)

_where ¯x[r,s]_ = _N[1]_ _Ni=1_ _[x]i[r,s][,]_

(ϵg, ¯ϵL, σ, ρPmax) = (ϵ[2]g[D][τ,ρ] [+][ ρ][max][D][τ,ρ]ϵ[¯][2]L [+][ σ][2][)][ ·][ G]p[′] _[D][τ,ρ][ρ][max][N][ +][ n]_ max[σ][2][.] (9)
_E_ [2] _m_ _[·][ 1]τ [ρ][2]_

_and_

_n_ _m_
_Gp =_ _m( −n −_ 1) _[,]_ _G[′]p_ [=][ G][p] [+ 1]τ [2][ .] (10)

Compared to Theorem 1, Theorem 2 shows partial device participation deteriorates the rate
by O( (ϵg, ¯ϵL, σ, ρmax)/√NτR). From the expression of, we observe that as ρmax 0,

(ϵg, ¯ϵEL, σ, ρmax) vanishes, which indicates that the loss caused by device sampling can be com- E _→_
_E_
pensated by increasing network connectivity uniformly for all clusters.

The next corollary finds the critial ρmax so that E [2] = O(1), and the order of convergence rate of
partial device participation matches that of the full participation case.
**Corollary 1. Under the same assumptions as Theorem 2, if the network connectivity satisfies**


1

(11)
4N [min][{][m, τ][ −] [1][}][.]


_ρmax_
_≤_

 _√NτRϵL_ +

 _[σ][ +][ ϵ][g][ + ¯]_


_then_

min _x[r,s])_ = O
_r,s_ [E][∥∇][f] [(¯] _∥[2]_


_τ_ [2]ϵ[2]g [+][ τρ]max[2] _[D]τ,ρ(ϵ[¯]τR[2]L_ [+]) 23[ τ]   1n [+][ ρ]max[2]  _σ[2][][ 1]3_ + R[1]


_._ (12)






-----

Corollary 1 reveals the tradeoff between sampling intensity and network connectivity. More connected D2D networks result in smaller ρmax, and thus (11) can be satisfied by a smaller m. This
means we can sample fewer devices at the end of each round and reduce the D2S communication
delay when the D2D network is more connected.

6 EXPERIMENTAL EVALUATION

6.1 EXPERIMENTAL SETTINGS

We use two common datasets in FL literature (McMahan et al., 2017; Reddi et al., 2021; Wang et al.,
2020): Federated Extended MNIST (Caldas et al., 2019) (FEMNIST) and CIFAR-10 (Krizhevsky
et al., 2009). The 62-class FEMNIST is built by partitioning the data in Extended MNIST (Cohen
et al., 2017) based on the writer of the digit/character and has a naturally-arising device partitioning. CIFAR-10 is partitioned across all devices using a Dirichlet distribution Dir(0.1) as done in
(Hsu et al., 2019; Yurochkin et al., 2019; Reddi et al., 2021; Wang et al., 2020). We evaluate our
algorithms by training CNNs on both datasets, and the CNN models for FEMNIST and CIFAR-10
were taken from (Caldas et al., 2019) and (McMahan et al., 2017) with around 6.5 and 1 million
parameters, respectively. For each dataset, the original testing set (without partitioning) is used to
evaluate the generalization performances of the trained global model.

We consider a FL system consisting of a central server and 32 devices. The devices are evenly
divided into four clusters, and each cluster has a ring topology by default, which provides a conservative estimation for the cluster connectivity and convergence speed. In our experiments, the
mixing matrix of each cluster Wk is set according to the Metropolis-Hastings weights (Nedi´c et al.,
2018). According to the real-world measurements in (Yuan et al., 2020; Yang et al., 2021), we set
the average time for a device to perform a local update, a round of D2D communication under ring
topology, and a round of D2S communication with one device sampled per cluster to be ccp = 0.01h,
_cd2d(∆= 2) = 0.005h and cd2s(p = 1/8) = 0.05h, respectively, in the runtime model (2). For ar-_
bitrary device sampling ratio and D2D network topology, we consider a linear-scaling rule (Wang
et al., 2019) and let cd2d(∆) = (∆/2) × 0.005h and cd2s(p) = 8p × 0.05h.

We compare HL-SGD with local SGD in the experiments. For local SGD, devices will only communicate with the central server periodically. In all experiments, we let the local iteration period
_τ to be the same for both local SGD and HL-SGD to have a fair comparison. On the FEMNIST_
dataset, we fix the batch size as 30 and tune the learning rate η from {0.005, 0.01, 0.02, 0.05, 0.08}
for each algorithm separately. On the CIFAR-10 dataset, we fix the batch size as 50 and tune η from
_{0.01, 0.02, 0.05, 0.08, 0.1} for each algorithm separately. We run each experiment with 3 random_
seeds and report the average. All experiments in this paper are conducted on a Linux server with 4
NVIDIA RTX 8000 GPUs. The algorithms are implemented by PyTorch. More details are provided
in Appendix F.

6.2 EXPERIMENTAL RESULTS

(a) FEMNIST (b) FEMNIST (c) CIFAR-10 (d) CIFAR-10

Figure 1: Convergence rate and runtime comparisons of HL-SGD and local SGD under ring topology when
_τ = 50 and p = 1 for FEMNIST and CIFAR-10 datasets. (a) and (c) show how the accuracy changes over_
communication round; (b) and (d) show how the accuracy changes over runtime.

We first compare the convergence speed and runtime of HL-SGD and local SGD while fixing τ = 50
and p = 1. We measure the best test accuracy of the global model on the server in every FL round.
Figure 1 shows the convergence process. From the figure, we can observe that HL-SGD can largely
accelerate the model convergence while improving model accuracy in FL. On FEMNIST, the best
accuracy of HL-SGD achieved over 100 rounds is 4.78% higher than that of local SGD (i.e., 83.76%


-----

vs. 79.94%), and its runtime necessary to achieve a target test accuracy of 75% is only 17.64% of
that of the baseline (i.e., 5.67× speedup). On CIFAR-10, the best accuracy of HL-SGD achieved
over 100 rounds is 9.32% higher than that of local SGD (i.e., 68.71% vs. 63.68%), and its runtime
necessary to achieve a target test accuracy of 60% is 15.67% less than that of local SGD (i.e., 1.186×
speedup).

Figure 2: Convergence rate (left) and runtime (right) comparisons of HL-SGD and local SGD on CIFAR-10
under different τ and ring topology when p = 1.

Next, to give a more comprehensive analysis on the runtime benefits of HL-SGD, we vary τ from
_{5, 10, 20, 50} and compare the performances of HL-SGD and local SGD on CIFAR-10 in Figure 2._
From the figure, we can observe that HL-SGD can consistently outperform local SGD across a wide
range of τ . In particular, on CIFAR-10, the best accuracy of HL-SGD achieved over 100 rounds is
2.49%, 3.99%, 4.05%, and 7% higher than that of local SGD, respectively, as τ increases from 5 to
50. At the same time, the runtime of HL-SGD needed to achieve a target test accuracy of 60% is
9.66%, 19.76%, 33.46%, and 45.88% less than that of local SGD, respectively.

(a) FEMNIST (b) FEMNIST (c) CIFAR-10 (d) CIFAR-10

Figure 3: Effect of sampling ratio p on the convergence rate and runtime of HL-SGD under ring topology when
_τ = 50 for FEMNIST and CIFAR-10 datasets. (a) and (c) show how the accuracy changes over communication_
round in HL-SGD; (b) and (d) show how the accuracy changes over runtime.

Finally, we investigate how the sampling ratio p affects the performance of HL-SGD. We select
_p from {0.125, 0.25, 0.5, 1}, corresponding to sampling {1, 2, 4, 8} devices from each cluster to_
upload models to the server. Figure 3 depicts the best value of test accuracy achieved over all prior
rounds. As can be observed from the figures, sampling one device per cluster only results in slightly
lower model accuracy, e.g., neligible and 1.92% drop compared to full participation on FEMNIST
and CIFAR-10, respectively. This matches the theoretical result in Corollary 1 that device sampling
does not affect the order of convergence rate under certain conditions. However, decreasing p can
lead to faster training speed due to its shorter D2S communication delay as observed in Figures 3b
and 3d. In practice, the optimal value of p needs to be tuned to strike a good balance between model
accuracy and runtime.

7 CONCLUSION

In this paper, we have proposed a new optimization algorithm called HL-SGD for FL with heterogeneous communications. Our algorithm leverages the D2D communication capabilities among edge
device to accelerate the model convergence while improving model accuracy in FL. We have provided the theoretical convergence analysis of HL-SGD and conducted experiments to demonstrate
the benefits of HL-SGD. In the future, we plan to extend HL-SGD to handle straggler issues under
device heterogeneity and provide rigorous privacy protection for HL-SGD.


-----

ACKNOWLEDGMENTS

The work of Y. Guo was supported in part by NSF under the Grant CNS-2106761. The work of Y.
Sun was partially supported by the Office of Naval Research under the Grant N00014-21-1-2673.
The work of R. Hu and Y. Gong was supported in part by NSF under the Grants CNS-2047761 and
CNS-2106761.

REFERENCES

Mehdi Salehi Heydar Abad, Emre Ozfatura, Deniz Gunduz, and Ozgur Ercetin. Hierarchical federated learning across heterogeneous cellular networks. In IEEE International Conference on
_Acoustics, Speech and Signal Processing (ICASSP), pp. 8866–8870. IEEE, 2020._

Arash Asadi, Qing Wang, and Vincenzo Mancuso. A survey on device-to-device communication in
cellular networks. IEEE Communications Surveys & Tutorials, 16(4):1801–1819, 2014.

Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar
Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacypreserving machine learning. In Proceedings of the 2017 ACM SIGSAC Conference on Computer
_and Communications Security, pp. 1175–1191, 2017._

L´eon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. SIAM Review, 60(2):223–311, 2018.

Stephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah. Randomized gossip algorithms.
_IEEE Transactions on Information Theory, 52(6):2508–2530, 2006._

Sebastian Caldas, Sai Meher Karthik Duddu, Peter Wu, Tian Li, Jakub Koneˇcn`y, H Brendan McMahan, Virginia Smith, and Ameet Talwalkar. LEAF: A benchmark for federated settings. In Work_shop on Federated Learning for Data Privacy and Confidentiality, 2019._

Timothy Castiglia, Anirban Das, and Stacy Patterson. Multi-Level Local SGD: Distributed SGD for
heterogeneous hierarchical networks. In International Conference on Learning Representations,
2021.

Yiming Chen, Kun Yuan, Yingya Zhang, Pan Pan, Yinghui Xu, and Wotao Yin. Accelerating gossip
SGD with periodic global averaging. In Proceedings of the 38th International Conference on
_Machine Learning, pp. 1791–1802. PMLR, 2021._

Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. EMNIST: Extending
MNIST to handwritten letters. In 2017 International Joint Conference on Neural Networks
_(IJCNN), pp. 2921–2926. IEEE, 2017._

Antonious Girgis, Deepesh Data, Suhas Diggavi, Peter Kairouz, and Ananda Theertha Suresh. Shuffled model of differential privacy in federated learning. In International Conference on Artificial
_Intelligence and Statistics, pp. 2521–2529. PMLR, 2021._

Yuanxiong Guo and Yanmin Gong. Practical collaborative learning for crowdsensing in the internet
of things with differential privacy. In 2018 IEEE Conference on Communications and Network
_Security (CNS), pp. 1–9. IEEE, 2018._

Farzin Haddadpour, Mohammad Mahdi Kamani, Mehrdad Mahdavi, and Viveck Cadambe. Local
SGD with periodic averaging: Tighter analysis and adaptive synchronization. In Advances in
_Neural Information Processing Systems, pp. 11082–11094, 2019._

Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data
distribution for federated visual classification. arXiv preprint arXiv:1909.06335, 2019.

Rui Hu, Yuanxiong Guo, Hongning Li, Qingqi Pei, and Yanmin Gong. Personalized federated
learning with differential privacy. IEEE Internet of Things Journal, 7(10):9530–9539, 2020.

Rui Hu, Yanmin Gong, and Yuanxiong Guo. Federated learning with sparsification-amplified privacy and adaptive optimization. In Proceedings of the Thirtieth International Joint Conference
_on Artificial Intelligence, IJCAI-21, pp. 1463–1469, 2021._


-----

Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur´elien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. Foundations and Trends® in Machine Learning,
14(1–2):1–210, 2021.

Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. SCAFFOLD: Stochastic controlled averaging for federated learning.
In International Conference on Machine Learning, pp. 5132–5143. PMLR, 2020.

Anastasia Koloskova, Sebastian Stich, and Martin Jaggi. Decentralized stochastic optimization and
gossip algorithms with compressed communication. In International Conference on Machine
_Learning, pp. 3478–3487, 2019._

Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. A unified
theory of decentralized SGD with changing topology and local updates. In International Confer_ence on Machine Learning, pp. 5381–5393. PMLR, 2020._

Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.

Tao Lin, Sebastian U Stich, Kumar Kshitij Patel, and Martin Jaggi. Don’t use large mini-batches,
use local SGD. In International Conference on Learning Representations, 2019.

Lumin Liu, Jun Zhang, SH Song, and Khaled B Letaief. Client-edge-cloud hierarchical federated
learning. In IEEE International Conference on Communications, pp. 1–6. IEEE, 2020.

Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial intelli_gence and statistics, pp. 1273–1282. PMLR, 2017._

H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private
recurrent language models. In International Conference on Learning Representations, 2018.

Angelia Nedi´c, Alex Olshevsky, and Michael G Rabbat. Network topology and communicationcomputation tradeoffs in decentralized optimization. Proceedings of the IEEE, 106(5):953–976,
2018.

Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Koneˇcn´y,
Sanjiv Kumar, and Hugh Brendan McMahan. Adaptive federated optimization. In International
_Conference on Learning Representations, 2021._

Daniel Rothchild, Ashwinee Panda, Enayat Ullah, Nikita Ivkin, Ion Stoica, Vladimir Braverman,
Joseph Gonzalez, and Raman Arora. FetchSGD: Communication-efficient federated learning
with sketching. In International Conference on Machine Learning, pp. 8253–8265. PMLR, 2020.

Sebastian Urban Stich. Local SGD converges fast and communicates little. In International Con_ference on Learning Representations, 2019._

John Nikolas Tsitsiklis. Problems in decentralized decision making and computation. Technical
report, Massachusetts Inst of Tech Cambridge Lab for Information and Decision Systems, 1984.

Jianyu Wang and Gauri Joshi. Cooperative SGD: A unified framework for the design and analysis
of local-update SGD algorithms. Journal of Machine Learning Research, 22(213):1–50, 2021.

Jianyu Wang, Anit Kumar Sahu, Zhouyi Yang, Gauri Joshi, and Soummya Kar. MATCHA: Speeding up decentralized SGD via matching decomposition sampling. In 2019 Sixth Indian Control
_Conference, pp. 299–300. IEEE, 2019._

Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective
inconsistency problem in heterogeneous federated optimization. Advances in Neural Information
_Processing Systems, 33:7611–7623, 2020._

Jianyu Wang, Zachary Charles, Zheng Xu, Gauri Joshi, H Brendan McMahan, Maruan Al-Shedivat,
Galen Andrew, Salman Avestimehr, Katharine Daly, Deepesh Data, et al. A field guide to federated optimization. arXiv preprint arXiv:2107.06917, 2021.


-----

Chengxu Yang, Qipeng Wang, Mengwei Xu, Zhenpeng Chen, Kaigui Bian, Yunxin Liu, and Xuanzhe Liu. Characterizing impacts of heterogeneity in federated learning upon large-scale smartphone data. In Proceedings of the Web Conference 2021, pp. 935–946, 2021.

Bicheng Ying, Kun Yuan, Yiming Chen, Hanbin Hu, Pan Pan, and Wotao Yin. Exponential graph
is provably efficient for decentralized deep training. Advances in Neural Information Processing
_Systems, 34, 2021._

Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted SGD with faster convergence and less
communication: Demystifying why model averaging works for deep learning. In Proceedings of
_the AAAI Conference on Artificial Intelligence, volume 33, pp. 5693–5700, 2019._

Zhe Yu, Yanmin Gong, Shimin Gong, and Yuanxiong Guo. Joint task offloading and resource
allocation in UAV-enabled mobile edge computing. IEEE Internet of Things Journal, 7(4):3147–
3159, 2020.

Jinliang Yuan, Mengwei Xu, Xiao Ma, Ao Zhou, Xuanzhe Liu, and Shangguang Wang. Hierarchical
federated learning through LAN-WAN orchestration. arXiv preprint arXiv:2010.11612, 2020.

Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and
Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In Interna_tional Conference on Machine Learning, pp. 7252–7261. PMLR, 2019._

Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. Federated
learning with non-iid data. arXiv preprint arXiv:1806.00582, 2018.

A PRELIMINARIES

**Intra-cluster dynamics.R[n][×][d]** constructed by stacking respectively To facilitate the analysis, we introduce matrices xi and gi for i ∈Vk row-wise. Similarly, we define Xk ∈ R[n][×][d] and Gk ∈
the pseudo-gradient ∇Fk(Xk) ∈ R[n][×][d] associated to cluster k by stacking ∇fi(xi) for i ∈Vk
row-wise. In addition, define the following intra-cluster averages for each cluster k:

_x¯k ≜_ [1] _xi_ and _g¯k ≜_ [1] _gi._ (13)

_n_ _n_

_iX∈Vk_ _iX∈Vk_

The update within each cluster then can be written compactly in matrix form as

_Xk[r,s][+1]_ = Wk(Xk[r,s] _−_ _ηG[r,s]k_ [)][,] _∀k = 1, . . ., K._ (14)

Since each Wk is bi-stochastic, we obtain the following update of the intra-cluster average

_x¯[r,s]k_ [+1] = ¯x[r,s]k _−_ _η · ¯gk[r,s][.]_ (15)

We proceed to derive the update of the intra-cluster consensus error. Define the averaging matrix

_J = [1]_ with **1 = [1, . . ., 1].** (16)

_n_ **[1][ ·][ 1][⊤]**
_n_

Multiplying both sides of (14) from the left by (I _J) leads to the following update of the consensus_

| {z }

_−_
error:


(I _J)Xk[r,s][+1]_
_−_

_Xk,[r,s]⊥[+1]_

| {z }


= (I − _J)Wk(Xk[r,s]_ _−_ _ηG[r,s]k_ [)]

= (Wk − _J)(Xk[r,s]_ _−_ _ηG[r,s]k_ [)]

= (Wk _J)(Xk,[r,s]_ _k_ [)][.]
_−_ _⊥_ _[−]_ _[ηG][r,s]_


(17)


**Global average dynamics. Define the global average among all xi’s as**


_x¯ ≜_ [1]


_xi._ (18)
_i=1_

X


-----

Then accordingly to (15) we have the following update of ¯x for all s = 0, . . ., τ − 1:


_x¯[r,s][+1]_ = [1]

_N_

= [1]


_K_

_nx¯[r,s]k_ [+1] = N[1]
_k=1_

X


_n (¯x[r,s]k_ _−_ _ηg¯k[r,s][)]_
_k=1_

X


(19)


_N_

(x[r,s]i _ηgi[r,s][) = ¯]x[r,s]_ _η_ [1]
_−_ _−_ _N_
_i=1_

X


_gi[r,s][.]_
_i=1_

X


**Filtration. Let G = [G1; . . . ; GK] ∈** R[N] _[×][d]_ be the matrix constructed by stacking all the stochastic
gradients. We introduce the following filtration

_r,s = σ_ _G[0][,][0], . . ., G[0][,τ]_ _[−][1], S0, G[1][,][0], . . ., G[1][,τ]_ _[−][1], . . ., Sr_ 1, G[r,][0], . . ., G[r,s][]
_F_ _−_ (20)

_Fr = σ_  G[0][,][0], . . ., G[0][,τ] _[−][1], S0, G[1][,][0], . . ., G[1][,τ]_ _[−][1], . . ., Sr−1_ _._
  

Therefore we have x[r,]i [0] = x[r] _∈Fr for r ≥_ 1, and x[r,s]i _∈Fr,s−1 for 1 ≤_ _s ≤_ _τ_ . For simplicity
the conditional expectation E( · |Fr,s) is denoted as Er,s, and we define the noise in the stochastic
gradient as

_ξi[r,s]_ ≜ _gi[r,s]_ _−∇fi(x[r,s]i_ [)][.] (21)

Since at the end of round r all nodes are picked with equal probability, the sampling procedure
preserves average in expectation:

Er,τ 2x[r][+1] = E(E(x[r][+1] _r,τ_ 1) _r,τ_ 2)
_−_ _|F_ _−_ _|F_ _−_


 _K_

 [1]


E






_x[r,τ]i_ _|Fr,τ_ _−1_ _|Fr,τ_ _−2_
_iX∈Sk[r]_



I(i ∈ _Sk[r][)][x][r,τ]i_ _|Fr,τ_ _−1_
_iX∈Vk_


= E

= E


_k=1_

_K_

_k=1_

X


(22)


_|Fr,τ_ _−2_


= Er,τ 2(¯x[r,τ] )
_−_


where the last equality holds since P _i ∈_ _Sk[r][|][i][ ∈V][k]_ = _[m]n_ [.]
  

B CONVERGENCE ANALYSIS

To prove the convergence we first establish in Sec. B.1 that the objective value Ef (x[r]) is descending
at each round r, up to some consensus error terms. Subsequently, bounds on the error terms are
provided in Sec. B.2-B.4. Based on these results, the proof of convergence of Algorithm 1 with full
and partial device participation are given in Sec. B.5 and B.6, respectively. The proofs of the main
propositions are given in Sec. C and that of the supporting lemmas are deferred to Sec. D.


-----

B.1 OBJECTIVE DESCENT

**Lemma 1. Let** _x[r,s]i_
_then the following inequality holds for all {_ _[}][ be the sequence generated by Algorithm 1 under Assumptions 1-6. If] r ∈_ N+: _[ η >][ 0][,]_

Ef (x[r][+1])


_τ_ _−1_

E _f_ (¯x[r,s])
_∥∇_ _∥[2]_ _−_ _[η]4_
_s=0_

X

_K_

2

[1] _fk(¯x[r,s]k_ [)]

_K_ _∇_ [¯]

_k=1_

X


_τ_ _−1_

_s=0_

X


Ef (x[r])
_≤_ _−_ _[η]4_


_∇fi(x[r,s]i_ [)]
_i=1_

X


_τ_ _−1_

_s=0_

X


_−_ _[η]4_

+ _[η]_

4

+ _[η]_


_τ_ 1 _N_ _τ_ 1
_−_ 2 _−_

_[η]_ E _f_ (¯x[r,s]) _fi(x[r,s]i_ [)] + _[η]_ E _f_ (¯x[r,s])

4 _∇_ _−_ _N[1]_ _∇_ 4 _∇_ _−_ _K[1]_

_s=0_ _i=1_ _s=0_

X X X

_τ_ 1 _N_ _K_
_−_ 2

_[η]_ E [1] _fi(x[r,s]i_ [)][ −] [1] _fk(¯x[r,s]k_ [)]

4 _N_ _∇_ _K_ _∇_ [¯]

_s=0_ _i=1_ _k=1_

X X X

_τ_ _−2_ _L_ _L_

E _x[r,s][+1]_ _x¯[r,s]_ + E _x[r,τ]_ _[−][1]_ _._

2 _−_ _∥[2]_ 2 _∥[2]_

_s=0_  _[∥][¯]_   _[∥][x][r][+1][ −]_ [¯] 

X


(23)


_∇f[¯]k(¯x[r,s]k_ [)]
_k=1_

X


_Proof. The proof is a standard application of the descent lemma and the sampling rule applied at_
iteration τ to obtain x[r][+1]. See Appendix D.1.

Lemma 1 shows the objective value f (x[r]) is descending in expectation up to the following error
terms:


_N_

2

_T1 = E_ _f_ (¯x[r,s]) _fi(x[r,s]i_ [)] _, T2 = E_ _f_ (¯x[r,s])
_∇_ _−_ _N[1]_ _∇_ _∇_ _−_ _K[1]_

_i=1_

X

_K_ _N_

2

_T3 = E_ [1] _fk(¯x[r,s]k_ [)][ −] [1] _fi(x[r,s]i_ [)]

_K_ _∇_ [¯] _N_ _∇_

_k=1_ _i=1_

X X

_T4 = E∥x¯[r,s][+1]_ _−_ _x¯[r,s]∥[2], T5 = E∥x[r][+1]_ _−_ _x¯[r,τ]_ _[−][1]∥[2]._


_∇f[¯]k(¯x[r,s]k_ [)]
_k=1_

X


(24)


In the sequel, we will show these quantities can be bounded by the optimality gap measured in terms
of the gradient norms ∥∇f (¯x[r,s])∥[2], ∥(1/K) _k=1_ _[∇]f[¯]k(¯x[r,s]k_ [)][∥][2][, and][ ∥][(1][/N] [)][ P]i[N]=1 _[∇][f][i][(][x]i[r,s][)][∥][2][.]_

[P][K]

B.2 BOUNDING T1, T2 AND T3.

Define

_ρmax =_ max (25)
_k=1,...,K_ _[ρ][k][.]_

Therefore it holds 0 _ρmax_ 1 by Assumption 3.
_≤_ _≤_


-----

Since each fi is L-smooth by Assumption 1, we have _f[¯]k and f are also L-smooth. Using this fact_
and the convexity of ∥· ∥[2] we can bound T1, T2 and T3 as


_K_

_T1 = E_ _f_ (¯x[r,s]) [1] _fk(¯x[r,s]k_ [)][ −] [1]
_∇_ _±_ _K_ _∇_ [¯] _N_

_k=1_

X

_K_ _K_

1

_≤_ 2 K[1] _k=1_ _L[2]E∥x¯[r,s]_ _−_ _x¯[r,s]k_ _[∥][2][ + 2]_ _k=1_ _N_

X X


_∇fi(x[r,s]i_ [)]
_i=1_

X

_L[2]E_ _x¯[r,s]k_ _x[r,s]i_
_iX∈Vk_ _∥_ _−_ _[∥][2][,]_


(26)


_K_

2

_T2 = E_ _f_ (¯x[r,s]) _fk(¯x[r,s]k_ [)]
_∇_ _−_ _K[1]_ _∇_ [¯] _≤_ _K[1]_

_k=1_

X

_K_ _N_

_T3 = E_ [1] _fk(¯x[r,s]k_ [)][ −] [1] _fi(x[r,s]i_ [)]

_K_ _∇_ [¯] _N_ _∇_

_k=1_ _i=1_

X X


_L[2]E_ _x¯[r,s]_ _x¯[r,s]k_
_−_
_k=1_

X


_L[2]E_ _x¯[r,s]k_ _x[r,s]i_
_iX∈Vk_ _∥_ _−_ _[∥][2][.]_


_k=1_


Clearly, in order to bound T1,2,3 we first need to bound the inter-cluster consensus error _x¯[r,s]_ _x¯[r,s]k_
and the intra-cluster consensus error _x¯[r,s]k_ _x[r,s]i_ _∥_ _−_ _[∥]_
_∥_ _−_ _[∥][.]_

**Lemma 2 (Inter-Cluster Consensus Error Bound). Let** _x[r,s]i_
_rithm 1 under Assumptions 1, 2, 3, and 5. If the learning rate {_ _η >[}][ be the sequence generated by Algo-] 0 satisfies_


_η[2]_ _≤_


1

(27)
24τ (4τ 1)L[2][,]
_−_


_then for all s = 0, . . ., τ −_ 1 it holds

_K_

1

_K_ E∥x¯[r,s][+1] _−_ _x¯[r,s]k_ [+1]∥[2] _≤_ _Cτ_

_k=1_

X


E _x¯[r,s]_ _x¯[r,s]k_
_k=1_ _∥_ _−_ _[∥][2]_

X

! + 12τη[2][  ]α[2]E∥∇f (¯x[r,s])∥[2] + ϵ[2]g + η[2][ K][ −]N [1] _σ[2]_ (28)




E _Xk,[r,s]⊥_
_K=1_

X


+ 12τη[2]L[2]


_where_


_Cτ ≜_ 1 + [3]

2

_[·]_


1

(29)
4τ 1 _[.]_
_−_


_Proof. See Appendix D.2._

**Lemma 3 (Intra-Cluster Consensus Error Bound). Let** _x[r,s]i_
_rithm 1 under Assumptions 1-5. If η > 0, then for all s = 0 {_ _, . . ., τ[}][ be the sequence generated by Algo-] −_ 1 it holds


_K_

1
E _Xk,[r,s][+1]_ max _k[(1 +][ ζ]k[−][1][) +][ η][2][ρ][L][ ·][ 4][L][2]_
_∥_ _⊥_ _∥[2]_ _≤_ _k_ [K] _[ρ][2]_ _N_
_k=1_  _∈_ 

X


E _Xk,[r,s]_
_∥_ _⊥[∥][2]_
_k=1_

X


+ 4η[2]ρLL[2][ 1]


E∥x¯[r,s]k _−_ _x¯[r,s]∥[2]_ + 4η[2]ρL(α[2]E∥∇f (¯x[r,s])∥[2] + ϵ[2]g[) + 4][η][2][ρ][L]ϵ[¯][2]L [+][ η][2][σ][2][ρ]max[2] _[,]_
_k=1_

X

(30)


_where ρmax is defined in (25) and_

_ρL ≜_ _k=1max,...,K_ _ρ[2]k[(1 +][ ζ][k][)]_ _,_ _ϵ¯[2]L_ [≜] _K[1]_



_ϵ[2]k_ (31)
_k=1_

X


_with ζk > 0 being a free parameter to be chosen properly for all k = 1, . . ., K._

_Proof. See Appendix D.3._


-----

Combining Lemma 2 and 3 we can obtain the following bound on the sum of intra- and interconsensus errors using gradient ∥∇f (¯x[r,s])∥[2].

**Proposition 1. Let** _x[r,s]i_
_learning rate η > 0 { satisfies[}][ be the sequence generated by Algorithm 1 under Assumptions 1-5. If the]_


_η[2]_ _≤_


1

(32)
24τ (4τ 1)L[2][,]
_−_


_then for all s = 0, . . ., τ −_ 1 it holds


E _x¯[r,s][+1]_ _x¯[r,s]k_ [+1] +
_∥_ _−_ _∥[2]_
_k=1_ _k=1_

X X


1

_N_ _k,⊥_ _∥[2]_

_[∥][X]_ _[r,s][+1]_


_s_

_C1η[2][  ]τ + ρ[2]max[D][τ,ρ]_ (α[2]E _f_ (¯x[r,ℓ]) + ϵ[2]g[) +][ C][1][η][2][τρ][2]max[D][τ,ρ]ϵ[¯][2]L
_∥∇_ _∥[2]_
_ℓ=0_

X 

+ C1τη[2]ρ[2]max[σ][2][ +][ C][1][(][τ][ +][ D]τ,ρ[2] _[τ][ −][1][ρ][2]max[)][η][2][ 1]_

_n_ _[σ][2]_


(33)

(34)

(35)

(36)


_where_

_Dτ,ρ ≜_ min _τ,_


_and C1 > 0 is some universal constant._

_Proof. See Appendix C.1._


1 _ρmax_
_−_


Notice that according to (26) the gradient difference terms in Lemma 1 can be bounded as


_N_

_η_ 2
_f_ (¯x[r,s]) _fi(x[r,s]i_ [)] + _[η]_ _f_ (¯x[r,s])
4 [E] _∇_ _−_ _N[1]_ _∇_ 4 [E] _∇_ _−_ _K[1]_

_i=1_

X

_N_ _K_

2

+ _[η]_ [1] _fi(x[r,s]i_ [)][ −] [1] _fk(¯x[r,s]k_ [)]

4 [E] _N_ _∇_ _K_ _∇_ [¯]

_i=1_ _k=1_

X X


_∇f[¯]k(¯x[r,s]k_ [)]
_k=1_

X


_≤_ _[η]4_

+ _[η]_

4

_≤ηL[2]_


2

_kX=1_ _L[2]E∥x¯[r,s]_ _−_ _x¯[r,s]k_ _[∥][2][ +]_ _kX=1_ _N_ _iX∈Vk_ _L[2]E∥x¯[r,s]k_ _−_ _x[r,s]i_ _[∥][2]!_

_K_ _K_

1 2 1

_K_ _kX=1_ _L[2]E_ _x¯[r,s]_ _−_ _x¯[r,s]k_ + _kX=1_ _N_ _iX∈Vk_ _L[2]E∥x¯[r,s]k_ _−_ _x[r,s]i_ _[∥][2]_

_K_ _K_

1 1

_K_ _k=1_ E∥x¯[r,s] _−_ _x¯[r,s]k_ _[∥][2][ +]_ _k=1_ _N_ _[∥][X]k,[r,s]⊥[∥][2]!_

X X


for all s = 1, . . ., τ . Therefore Proposition 1 immediately leads to the following result.

**Corollary 2. Under the same setting as Proposition 1, it holds**


_τ_ _N_

_η_
_f_ (¯x[r,s]) _fi(x[r,s]i_ [)]
4 [E] _∇_ _−_ _N[1]_ _∇_

_s=0_ _i=1_

X X

_τ_ _η_ _τ_ _−1_ _N_

+ E [1] _fi(x[r,s]i_ [)][ −] [1]

4 _N_ _∇_ _K_

_s=0_ _s=0_ _i=1_

X X X


_τ_

2 _η_
+ _f_ (¯x[r,s])

4 [E] _∇_ _−_ _K[1]_

_s=0_

X

_K_

2
_∇f[¯]k(¯x[r,s]k_ [)]
_k=1_

X


_∇f[¯]k(¯x[r,s]k_ [)]
_k=1_

X


_τ_ _−1_

_C1L[2]η[3][  ]τ_ [2] + τρ[2]max[D][τ,ρ] (α[2]E _f_ (¯x[r,s]) + ϵ[2]g[) +][ C][1][L][2][η][3][τ][ 2][ρ][2]max[D][τ,ρ]ϵ[¯][2]L
_∥∇_ _∥[2]_
_s=0_

X 

+ C1τ [2]L[2]η[3]ρ[2]max[σ][2][ +][ C][1][L][2][(][τ][ 2][ +][ D]τ,ρ[2] _[ρ][2]max[)][η][3][ σ][2]_

_n [.]_


-----

We conclude this section by providing a separate bound on the consensus error

_N1_ _Kk=1_ [E][r][∥][X]k,[r,τ]⊥[−][1]∥[2] that will be useful in bounding T5.

**Proposition 2.P** _Under the same setting as Proposition 1, if ρmax ≤_ 1 − _τ[1]_ _[,][ then we have]_

1 _K_ _τ_ _−2_

_N_ [E] _k=1_ _∥Xk,[r,τ]⊥[−][1]∥[2]_ _≤_ 2C2 _s=0_ _Dτ,ρ[2]_ _[ρ][2]max[η][2][(][α][2][∥∇][f]_ [(¯]x[r,s])∥[2] + ϵ[2]g[)]

X X

1 _Dτ,ρ_

+ C2Dτ,ρ[2] _[τρ][2]max[η][2]ϵ[¯][2]L_ [+][ C][2] + 1 _ρ[2]max[τD][τ,ρ][η][2][σ][2][.]_ (37)

_n_ _τ_

 

_for some universal constant C2 > 0._

_Proof. See Appendix C.2._

Proposition 2 shows that the average intra-cluster consensus error _N[1]_ _Kk=1_ _k,_ decreases as

the network connectivity improves, and vanishes if ρmax goes to zero. _[∥][X]_ _[r,τ]⊥[−][1]∥[2]_
P

B.3 BOUNDING T4

**Proposition 3. Under the same setting as Lemma 1, we have**


+ η[2][ σ][2]


E∥x¯[r,s][+1] _−_ _x¯[r,s]∥[2]_ = η[2]E

_for s = 0, . . ., τ −_ 1 and r ∈ N+.


_∇fi(x[r,s]i_ [)]
_i=1_

X


(38)


_Proof. Recall the algorithmic update at iteration s for all s = 0, . . ., τ −_ 1:

_Xk[r,s][+1]_ = WkXk[r,s] _ηWkG[r,s]k_
_−_ (39)
_x¯[r,s]k_ [+1] = ¯x[r,s]k _−_ _ηg¯k[r,s][.]_

Therefore, it holds under Assumption 2 that

E∥x¯[r,s][+1] _−_ _x¯[r,s]∥[2]_


(40)

+ η[2][ σ][2]

_N [.]_


(gi[r,s] _± ∇fi(x[r,s]i_ [)]
_i=1_

X


_∇fi(x[r,s]i_ [)]
_i=1_

X


=E

B.4 BOUNDING T5


= E


We provide the bound on T5 separately for the full device participation and partial participation
cases.

**Full participation.**

When the sampling probability p = 1, we have


_x[r][+1]_ = [1]

_N_

In this case, it follows from Proposition 3 that

E∥x[r][+1] _−_ _x¯[r,τ]_ _[−][1]∥[2]_ = η[2]E

**Partial participation.**


_x[r,τ]i_ = ¯x[r,τ] _._
_i=1_

X

_N_

1

_N_ _∇fi(xi[r,τ]_ _[−][1]_

_i=1_

X


+ η[2][ σ]N [2] _[.]_ (41)


-----

We proceed to bound T5 for

1 ≤ _m ≤_ _n −_ 1. (42)

Define p = m/n. Recall the algorithmic update at iteration τ − 1:

_Xk[r,τ]_ = WkXk[r,τ] _[−][1]_ _ηWkGk[r,τ]_ _[−][1]_ (43)
_−_

and


_x[r][+1]_ = [1]


_x[r,τ]i_
_iX∈Sk[r]_


_x[r,τ]i_ _._ (44)
_iX∈Sk[r]_


_Np_


_k=1_


_k=1_


Therefore, with (Wk)i,j being the ij-th element of matrix Wk we have under Assumption 2:


E∥x[r][+1] _−_ _x¯[r,τ]_ _[−][1]∥[2]_


2[]




_x[r,τ]i_ _x¯[r,τ]_ _[−][1]_
_−_
_iX∈Sk[r]_


=E

=E

= E


_Np_


_k=1_


(Wk)i,j(xj[r,τ] _[−][1]_ _ηgj[r,τ]_ _[−][1])_ _x¯[r,τ]_ _[−][1]_
_−_ _−_

  j[X]∈Vk 


_Np_

1

_Np_


_k=1_ _i∈Sk[r]_ _j∈Vk_

_K_ 2 (45)

1

= E _Np_ I(i ∈ _Sk[r][)]_ (Wk)i,j(x[r,τ]j _[−][1]_ _−_ _η∇fj(xj[r,τ]_ _[−][1])_ _−_ _x¯[r,τ]_ _[−][1]_

_kX=1_ _iX∈Vk_   j[X]∈Vk 

_A2,1_

2

| _K_ {z }

1

+ η[2] E _Np_ I(i ∈ _Sk[r][)]_ (Wk)i,j(∇fj(x[r,τ]j _[−][1]) −_ _gj[r,τ]_ _[−][1])_ _._

_kX=1_ _iX∈Vk_   j[X]∈Vk 

_A2,2_

**Proposition 4. Let|** _x[r,s]i_ {z }
_learning rate η > 0 { satisfies[}][ be the sequence generated by Algorithm 1 under Assumptions 1-6. If the]_


1
_η[2]_ (46)
_≤_ 24τ (4τ 1)L[2][,]

_−_

_then we have the following bounds on A2,1:_


+ 8 _Gp + [1]_

_τ_ [2]




_fk(¯x[r,τ]k_ _[−][1]_
_∇_ [¯]
_k=1_

X


E _Xk,[r,τ]_ _[−][1]_
_∥_ _⊥_ _∥[2]_
_k=1_

X


1

_A2,1_ 2η[2]E
_≤_ _K_

_where_

_Proof. See Appendix C.3._


(47)


_n_ _m_
_Gp ≜_ _−_ (48)

_m(n_ 1) _[.]_
_−_


**Proposition 5. Under the same setting as Proposition 4, A2,2 can be bounded as**


_A2,2_
_≤_ _[σ]N[2]_


2 + _m[n]_ max _._ (49)

_[·][ ρ][2]_ 


_Proof. See Appendix C.4_


-----

B.5 PROOF OF THEOREM 1 (FULL PARTICIPATION)

We first prove the descent of the objective value under suitable choice of η.
**Proposition 6. If the learning rate satisfies**


1

(50)

4C1α _τL_ _[,]_

_[·][ 1]_


_η ≤_


_then we have_


_τ_ _−1_

E∥∇f (¯x[r,s])∥[2] + Rfull(η), (51)
_s=0_

X


Ef (x[r][+1]) Ef (x[r])
_≤_ _−_ _[η]8_


_where_

1
full(η) =C1L[2]η[3]τ [2][  ]τ + ρ[2]max[D][τ,ρ] _ϵ[2]g_ [+ 2][C][1][L][2][η][3] _τ_ [2]ρ[2]max[D][τ,ρ]ϵ[¯][2]L [+][ τ][ 2][σ][2] max
_R_ _n_ [+][ ρ][2]
  


+ η[2]Lτ [σ][2]

_N [.]_

(52)

_C1 > 0 is some universal constant._

_Proof. See Appendix C.5._

To attain the expression of the convergence rate, we sum (51) over r = 0, . . ., R:

min min _x[r,s])_
_r∈[R]_ _s=0,...,τ_ _−1_ [E][∥∇][f] [(¯] _∥[2]_

+ [8][R][full][(][η][)]

_≤_ [8(][f]ητ[(][x][0]([)]R[ −] + 1)[f] [(][x][⋆][))] _ητ_

= [8(][f] [(][x][0][)][ −] _[f]_ [(][x][⋆][))] + 16ηL _[σ][2]_ (53)

_ητ_ (R + 1) _N_

centralized SGD

1

|+ 16C1L[2]τ [2]η{z[2]ϵ[2]g [+ 16][C][1][L][2][η]}[2] _τρ[2]max[D][τ,ρ]ϵ[¯][2]L_ [+][ τσ][2] max _._

_n_ [+][ ρ][2]

  

network effect

The first two terms of (53) corresponds to the impact of stochastic noise and is of the same order| {z }
as the centralized SGD algorithm. The last term is of order η[2] and corresponds to the deterioration
of convergence rate due to the fact that we are not computing the average gradients of all devices at
each iteration.


Denote

_σ2_
_r0 = 8(f_ (x[0]) _f_ (x[⋆])), r1 = 16L _,_
_−_ _N_
  1 (54)

_r2 = 16C1L[2]τ_ [2]ϵ[2]g [+ 16][C][1][L][2] _τρ[2]max[D][τ,ρ]ϵ[¯][2]L_ [+][ τσ][2] max _._

_n_ [+][ ρ][2]

  

The rest of the proof follows the same argument as (?, Appendix B.5) and thus we omit the details.


B.6 PROOF OF THEOREM 2 AND COROLLARY 1 (PARTIAL PARTICIPATION)

**Proposition 7. Let** _x[r,s]i_
_learning rate η and the network connectivity satisfies {_ _[}][ be the sequence generated by Algorithm 1 under Assumption 1-5. If the]_


_and_ _ρmax_ 1 (55)
_≤_ _−_ _τ [1]_ _[,]_


_η ≤_


1,
_C3ατL_

_[·][ min]_ 


_αGpDτ,ρ ρmax_


-----

_then_


_τ_ _−1_

E∥∇f (¯x[r,s])∥[2] + Rpart[(1)] [(][η][) +][ R][(2)]part[(][η][)] (56)
_s=0_

X


Ef (x[r][+1]) Ef (x[r])
_≤_ _−_ _[η]8_


_with_

1
part[(][η][) =][C][1][L][2][η][3][τ][ 2][  ]τ + ρ[2]max[D][τ,ρ] _ϵ[2]g_ [+ 2][C][1][L][2][η][3] _τ_ [2]ρ[2]max[D][τ,ρ]ϵ[¯][2]L [+][ τ][ 2][σ][2] max
_R[(1)]_ _n_ [+][ ρ][2]
 


+ Lτη[2][ σ][2]

_N [,]_




(57)


part[(][η][) =] 8C2G[′]p[LD]τ,ρ[2] _[τρ][2]max_ _η[2]ϵ[2]g_
_R[(2)]_

1

  

+ 4C2LG[′]p[η][2] _Dτ,ρ[2]_ _[τρ][2]maxϵ[¯][2]L_ [+]

_n_

 

_C1, C3 > 0 are some universal constants, and_


_n_

_m_ _[ρ]max[2]_ _η[2][ σ]N [2]_ _[.]_

 


_Dτ,ρ_


+ 1 _ρ[2]max[τD][τ,ρ][σ][2]_ + _[L]_
 


_n_ _m_

_G[′]p_ [=][ G][p] [+ 1]τ [2][,] _with_ _Gp =_ _m( −n_ 1) _[.]_ (58)

_−_

_Proof. See Appendix C.6_

Comparing (56) to (51) we can see that Rpart[(1)] [(][η][)][ is of the same order as][ R][(1)]full[(][η][)][, while][ R]part[(2)] [(][η][)][ is]
an extra loss term introduced by sampling.

Following the same steps as the proof of Theorem 1 gives


min min _x[r,s])_
_r∈[R]_ _s=0,...,τ_ _−1_ [E][∥∇][f] [(¯] _∥[2]_

1 3

_σ +_ (ϵg, ¯ϵL, σ, ρmax) _τ_ [2]ϵ[2]g [+][ τρ]max[2] _[D][τ,ρ]ϵ[¯][2]L_ [+][ τ] _n_ [+][ ρ]max[2] _σ[2][][ 1]_
=O _E_ + 2
 _√NτR_   (τR) 3   

+ [1] _,_

_R_ [max][{][1][, G][p][D][τ,ρ][ρ][max][}]



(59)


where

(ϵg, ¯ϵL, σ, ρmax) = (ϵ[2]g[D][τ,ρ] [+][ D][τ,ρ]ϵ[¯][2]L [+][ σ][2][)][ ·][ G]p[′] _[D][τ,ρ][ρ]max[2]_ _[N][ +][ n]_ max[σ][2][.] (60)
_E_ [2] _m_ _[·][ 1]τ [ρ][2]_


Our last step simplifies the overall conditions on ρmax so that E [2](ϵg, ¯ϵL, σ, ρmax) = O(1):

_ρmax_ 1 _G[′]p[D]τ,ρ[2]_ _[ρ][2]max_ _ρmax_ (61)
_≤_ _−_ _τ [1]_ _[,]_ _[≤]_ _N [1]_ _[,]_ _≤_ _[m]n_

_[·][ τ.]_

We claim to fulfill (61) it suffices to require

1
_ρmax_ (62)
_≤_ 4N [min][{][m, τ][ −] [1][}][.]

When τ = 1, the condition trivially requires ρmax = 0. We then consider the case for τ ≥ 2. By
definition, it can be verified that


_G[′]p_ _[≤]_ _m[1]_ [+ 1]τ [2][ .] (63)


First notice that


_m_

4N 4 _τ_

_[≤]_ [1] _[≤]_ [1][ −] [1]

Therefore, it remains to prove (62) implies


_τ_

(64)

4N _n_

_[≤]_ _[m]_ _[·][ τ.]_


and


_G[′]p[D]τ,ρ[2]_ _[ρ][2]max_ (65)

_[≤]_ _N [1]_ _[.]_


-----

Using the fact that under (62) ρmax 1/4 we have
_≤_


1
_G[′]p[D]τ,ρ[2]_ _[ρ][max]_ [=][ G][′]p[ρ][max]

_[·]_ (1 _ρmax)[2]_

1 _− 1_ 1 (66)

_≤_ [16]9 _[ρ][max]_ _m_ [+ 1]τ [2] _≤_ [16]9 _m_ [+ 1]τ [2] 4N [min][{][m, τ] _[} ≤]_ _N [1]_ _[.]_

   

This proves the claim.

C PROOF OF MAIN PROPOSITIONS

C.1 PROOF OF PROPOSITION 1

Denote for short

_K_

1

_N_ E∥Xk,[r,s]⊥[∥][2]

_M_ _[r,s]_ ≜  _KkX=1_  _._ (67)

1

 
 E _x¯[r,s]k_ _x¯[r,s]_ 
 _K_ _k=1_ _∥_ _−_ _∥[2]_
 X 
 

Invoking Lemma 2 and Lemma 3 we obtain that under the condition that the learning rate η > 0
satisfies

1
_η[2]_ (68)
_≤_ 24τ (4τ 1)L[2][,]

_−_

the following inequality is satisfied for all s = 0, . . ., τ − 1:

_M_ _[r,s][+1]_ _≤_ _G · M_ _[r,s]_ + B[r,s], (69)

where


_G =_ maxk∈[K] ρ[2]k[(1 +][ ζ]k[−][1][) +][ η][2][ρ][L][ ·][ 4][L][2] _η[2]ρL · 4L[2]_

12τη[2]L[2] _Cτ_

 

_B[r,s]_ = 4ρLη2(α2E∥∇f (¯x[r,s])∥[2] + ϵ[2]g[) + 4][η][2][ρ][L]ϵ[¯][2]L [+][ η][2][ρ]max[2] _[σ][2]_
 12τη[2](α[2]E∥∇f (¯x[r,s])∥[2] + ϵ[2]g[) +][ η][2][ σ]n[2] _[.]_


(70)

(71)


The inequality in (69) is defined elementwise.

Unrolling (69) yields

_M_ _[r,s][+1]_ _≤_


_G[ℓ]B[r,s][−][ℓ],_ (72)

_ℓ=0_

X


where we have used the fact that M _[r,][0]_ = 0 due to full synchronization of the xi’s at the beginning
of each round r.

We first provide a bound on the sum of the two elements of G[ℓ]B[r,s][−][ℓ]. For simplicity we omit the
round index r in the superscript for the rest of this section.

**Lemma 4. Let b[s]1[−][ℓ]** _and b[s]2[−][ℓ]_ _be the first and second element of B[s][−][ℓ], respectively. Suppose the_
_learning rate η > 0 then_

(1, 1)G[ℓ]B[s][−][ℓ] _λ[ℓ]2[(][b][s]1[−][ℓ]_ + b[s]2[−][ℓ]) + _[λ]2[ℓ]_ _[−]_ _[λ]1[ℓ]_ _η[2]_ 12τL[2]b[s]1[−][ℓ] + 4ρLL[2]b[s]2[−][ℓ] (73)
_≤_ _λ2_ _λ1_ _·_

_−_
  

_where λ1 ≤_ _λ2 are the eigenvalues of G; and ρL is defined in (31)._

_Proof. See Appendix D.4._


-----

From Lemma 4 we immediately get

_s_

(1, 1) · G[ℓ]B[s][−][ℓ]
_ℓ=0_

X


(74)


_s_ (74)

_λ[ℓ]2[(][b][s]1[−][ℓ]_ + b[s]2[−][ℓ]) + _[λ]2[ℓ]_ _[−]_ _[λ]1[ℓ]_ _η[2]_ 12τL[2]b[s]1[−][ℓ] + 4ρLL[2]b[s]2[−][ℓ] _._

_≤_ _λ2_ _λ1_ _·_

Xℓ=0  _−_   []

Since λ2 ≥ _Cτ > 1, we have_

_λ[ℓ]2_ _[−]_ _[λ]1[ℓ]_ = λ[ℓ]2[−][1] _ℓ−1_ _λ1_ _s_ _λ[ℓ]2[−][1]_ min _λ2_ _, ℓ_ _λ[ℓ]2_ [min] 1 _, ℓ_ (75)

_λ2_ _λ1_ _λ2_ _≤_ _λ2_ _λ1_ _≤_ _λ2_ _λ1_
_−_ Xs=0    _−_   _−_ 


and thus

_s_

(1, 1) · G[ℓ]B[s][−][ℓ]
_ℓ=0_

X _s_

_λ[ℓ]2[(][b][s]1[−][ℓ]_ + b2[s][−][ℓ]) +

_≤_

_ℓ=0_

X


(76)


1

_λ2_ _λ1_ _, ℓ_ _η[2]_ _·_ 12τL[2]b[s]1[−][ℓ] + 4ρLL[2]b[s]2[−][ℓ]
_−_ 
 


_λ[ℓ]2_ [min]


_ℓ=0_


Recall the definition of ρL given by (31):

_ρL =_ max _k[(1 +][ ζ][k][)][.]_ (77)
_k=1,...,K_ _[ρ][2]_

By the Gershgorin’s theorem, since η > 0, we can upperbound λ2 as


_λ2_ max max _k[(1 +][ ζ]k[−][1][) +][ η][2][ρ][L][ ·][ 8][L][2][, C][τ][ + 12][τη][2][L][2]_
_≤_ k∈[K] _[ρ][2]_


(78)


_ρL_
max max _k[(1 +][ ζ]k[−][1][) +]_
_≤_ _k_ [K] _[ρ][2]_ (4τ 1)3τ [,][ 1 +]
 _∈_ _−_

where the last inequality is due to the bound on η:


4τ − 1


_η[2]_ _≤_


1

(79)
24τ (4τ 1)L[2][ .]
_−_


Define constant

We consider two cases.

_• Case 1:_


(80)


_Dτ,ρ = min_ _τ,_



1 _ρmax_
_−_


1

_ρmax_ 1 _τ._ (81)
_≤_ _−_ _τ[1]_ _[⇒]_ 1 − _ρmax_ _≤_

Thus Dτ,ρ = 1/(1 − _ρmax). We let ζk = ρk/(1 −_ _ρk) and it gives_


_ρ[2]k_

1 _ρk_
_−_


_ρ[2]max_ = ρ[2]max[D][τ,ρ][.] (82)

1 _ρmax_
_−_


_kmax[K]_ _[ρ]k[2][(1 +][ ζ]k[−][1][) =][ ρ][max][,]_ _ρL =_ _k=1max,...,K_
_∈_

Substituting into the bound of λ2 [cf. (78)] gives


_λ2_ max _ρmax +_ _ρ[2]max_ 2
_≤_ (1 _ρmax)3τ_ (4τ 1) _[,][ 1 +]_ 4τ 1
 _−_ _−_ _−_ 

2 (83)

1 _τ_ 2 3
_−_ [1]

max 1 _< 1 +_
_≤_  _−_ _τ[1]_ [+] 3(4τ 1) _[,][ 1 +]_ 4τ 1  4τ 1 _[,]_

 _−_ _−_  _−_


-----

where in the second inequality we used the condition (81).

Since s _τ and λ2_ 1, we obtain the following bound
_≤_ _≥_


_s_

_λ[ℓ]2[b][s]1[−][ℓ]_ 1 +
_≤_
_ℓ=0_ 

X


_τ_
 


_b[ℓ]1_
_ℓ=0_

X


_b[ℓ]1_
_ℓ=0_

X


(84)


_≤_ 3 ·


4τ − 1


Moreover, since

_ρmax + η[2]ρL_ 4L[2] _ρmax +_ _ρ[2]max_
_·_ _≤_ (1 _ρmax)(4τ_ 1)6τ

_−_ _−_

we can bound λ2 − _λ1 as_


2

(81) 1 _τ_

_−_ [1]

1 (85)
_≤_ _−_ _τ[1]_ [+] 6(4τ 1)

_−_ _[≤]_ _[C][τ]_ _[,]_


_λ2_ _λ1_ _Cτ_ _ρmax_ _η[2]ρL_ 4L[2]
_−_ _≥_ _−_ _−_ _·_

_Cτ_ _ρmax +_ _ρ[2]max_
_≥_ _−_ (1 _ρmax)(4τ_ 1)6τ
 _−_ _−_

(81) 1 _τ_
_Cτ_ _ρmax + ρmax_ _−_ [1]
_≥_ _−_ _·_ 6(4τ 1)
 _−_ 


(86)

(87)


1 + _ρmax + ρmax_
_≥_ 4τ 1 _·_

_−_ _[−]_ 


4τ − 1


= (1 _ρmax)_ 1 +
_−_


Collecting (84) and (86) we can bound (76) as

_s_

(1, 1) · G[ℓ]B[s][−][ℓ]
_ℓ=0_

X


1 _ρmax._
_≥_ _−_


4τ − 1


1

_, τ_
_λ2_ _λ1_
_−_ 


_η[2]_ _·_ 12τL[2]b[ℓ]1 [+ 4][ρ][L][L][2][b]2[ℓ] _· 3_ min
_ℓ=0_ 

X   


(b[ℓ]1 [+][ b]2[ℓ] [)][ ·][ 3 +]
_ℓ=0_

Xs

(b[ℓ]1 [+][ b]2[ℓ] [)][ ·][ 3 +]
_ℓ=0_

X


_η[2]_ _·_ 12τL[2]b[ℓ]1 [+][ D][τ,ρ][ρ]max[2] [4][L][2][b][ℓ]2 _· 3Dτ,ρ_
_ℓ=0_

X   


(79)


(b[ℓ]1 [+][ b]2[ℓ] [)][ ·][ 3 +]
_ℓ=0_

X


1
12τb[ℓ]1 [+][ D][τ,ρ][ρ]max[2] [4][b][ℓ]2 3Dτ,ρ

_·_ (4τ 1)24τ

_−_




_ℓ=0_


12τb[ℓ]1 [+][ D][τ,ρ][ρ]max[2] [4][b][ℓ]2 [1]

_·_ 8


12τb[ℓ]1 [+][ D][τ,ρ][ρ]max[2] [4][b][ℓ]2 [1]

_·_ 8



_Dτ,ρ_

_τ_ [2]

_Dτ,ρ_

_τ_ [2][ .]


(b[ℓ]1 [+][ b]2[ℓ] [)][ ·][ 3 +]
_ℓ=0_

Xs

(b[ℓ]1 [+][ b]2[ℓ] [)][ ·][ 3 +]
_ℓ=0_

X


_ℓ=0_

_s_

_ℓ=0_

X


-----

Substituting the expression of b[ℓ]1 [and][ b]2[ℓ] [gives]


_Dτ,ρ[2]_

_τ_ [2] _· ρ[2]max[b][ℓ]2_


(1, 1) · G[ℓ]B[s][−][ℓ] _≤_
_ℓ=0_

X


(b[ℓ]1 [+][ b]2[ℓ] [)][ ·][ 5 +]
_ℓ=0_

X


_ℓ=0_


_s_

5 4η[2](ρL + 3τ )(α[2]E _f_ (¯x[r,ℓ]) + ϵ[2]g[) + 4][η][2][ρ][L]ϵ[¯][2]L [+][ η][2][ρ]max[2] _[σ][2][ +][ η][2][ 1]_
_∥∇_ _∥[2]_ _n_ _[σ][2]_
_ℓ=0_ 

X


_Dτ,ρ[2]_

_τ_ [2] _· ρ[2]max_


12τη[2](α[2]E _f_ (¯x[r,ℓ]) + ϵ[2]g[) +][ η][2][ 1]
_∥∇_ _∥[2]_ _n_ _[σ][2]_


_ℓ=0_


(88)


_C1_

2 _[η][2][(][ρ][L][ +][ τ][ +][ ρ]max[2]_ _[D]τ,ρ[2]_ _[τ][ −][1][)(][α][2][E][∥∇][f]_ [(¯]x[r,ℓ])∥[2] + ϵ[2]g[) +][ C]2[1] _[η][2][τρ][L]ϵ[¯][2]L_


_ℓ=0_


+ _[C][1]_ max[σ][2][ +][ C][1] _τ,ρ[τ][ −][1][ρ][2]max[)][η][2][ 1]_

2 _[τη][2][ρ][2]_ 2 [(][τ][ +][ D][2] _n_ _[σ][2]_


_s_

_C1η[2][  ]τ + ρ[2]max[D][τ,ρ]_ (α[2]E _f_ (¯x[r,ℓ]) + ϵ[2]g[) +][ C][1][η][2][τρ][2]max[D][τ,ρ]ϵ[¯][2]L

_≤_ _∥∇_ _∥[2]_

_ℓ=0_

X 

+ C1τη[2]ρ[2]max[σ][2][ +][ C][1][(][τ][ +][ D]τ,ρ[2] _[τ][ −][1][ρ]max[2]_ [)][η][2][ 1]

_n_ _[σ][2]_

where C1 is some universal constant. The last inequality holds since ρL = ρ[2]max[D][τ,ρ] [and][ D][τ,ρ]

_[≤]_ _[τ]_ [.]

_• Case 2:_

_ρmax > 1_ (89)
_−_ _τ[1]_

_[⇒]_ _[D][τ,ρ][ =][ τ.]_

In such a case, we let ζk = (4τ − 1) and thus

max _k[(1 +][ ζ]k[−][1][) =][ ρ]max[2]_ [(1 + (4][τ][ −] [1)][−][1][)][,] _ρL = 4τρ[2]max_ [= 4][ρ]max[2] _[D][τ,ρ][.]_ (90)
_k_ [K] _[ρ][2]_
_∈_

Substituting into the bound of λ2 given in (78), applying again the learning rate condition (79) and
using the fact that Dτ,ρ = τ :

_λ2_ max _ρ[2]max[(1 + (4][τ][ −]_ [1)][−][1][) +] 4ρ[2]max 2
_≤_ 3(4τ 1) _[,][ 1 +]_ (4τ 1)
 _−_ _−_  (91)

3
1 +
_≤_ 4τ 1 _[.]_

_−_

Therefore by (76), (79), (84), and the fact that


1

_, ℓ_ _τ = Dτ,ρ_ (92)
_λ2_ _λ1_ _≤_
_−_ 


min

_s_

(1, 1) · G[ℓ]B[s][−][ℓ]
_ℓ=0_

X


we obtain


_η[2]_ _·_ 12τL[2]b[ℓ]1 [+ 16][ρ]max[2] _[D][τ,ρ][L][2][b][ℓ]2_ _· 3Dτ,ρ_
_ℓ=0_

X   


(b[ℓ]1 [+][ b]2[ℓ] [)][ ·][ 3 +]
_s=0_

Xs

(b[ℓ]1 [+][ b]2[ℓ] [)][ ·][ 3 +]
_ℓ=0_

Xs

(b[ℓ]1 [+][ b]2[ℓ] [)][ ·][ 5 +]
_ℓ=0_

X


(93)


_s_ 1

12τb[ℓ]1 [+ 16][ρ]max[2] _[D][τ,ρ][b][ℓ]2_

8

_ℓ=0_

Xs   

_τ,ρ_
2 _[D][2]_ max[b][ℓ]2[.]

_τ_ [2][ ρ][2]

_ℓ=0_

X


_Dτ,ρ_

_τ_ [2]


Substituting the expression of b1 and b2 and using the fact that

_ρL = 4τρ[2]max_ [= 4][ρ]max[2] _[D][τ,ρ]_
we arrive at the same bound as in Case 1, possibly with a different constant C1.


-----

C.2 PROOF OF PROPOSITION 2

We are in Case 1 described in the proof of Proposition 1. By letting ζk = ρk/(1 − _ρk) we have_


_ρmax +_ 1−ρ[2]maxρmax 1−ρ[2]maxρmax

12τη[2]L[·][2][ η][2][ ·][ 4][L][2] _C[·][ η]τ_ [2][ ·][ 4][L][2]


(94)


_G =_


and the following bound on the difference of the eigenvalues of G:

_λ2_ _λ1_ 1 _ρmax._ (95)
_−_ _≥_ _−_

Notice that according to (72) and (148)


_τ_ _−2_

_t[τ]1[−][2][−][ℓ]_
_ℓ=0_

X


_Xk,[r,τ]_ _[−][1]_ =
_∥_ _⊥_ _∥[2]_
_k=1_

X


det(T )


Therefore

_K_

1

_N_ _∥Xk,[r,τ]⊥[−][1]∥[2]_

_k=1_

X

1
=

12τη[2]L[2](λ2 _λ1)_
_−_


_τ_ _−2_

_ℓ=0_

X


1 _τ_ _−2_
= 12τη[2]L[2](λ2 _λ1)_ _ℓ=0_ (Cτ − _λ1)_ _λ[ℓ]1[12][τη][2][L][2][b][τ]1[−][2][−][ℓ]_ _−_ _λ[ℓ]1[(][λ][2]_ _[−]_ _[C][τ]_ [)][b]2[τ] _[−][2][−][ℓ]_

1 − Xτ −2   
+ 12τη[2]L[2](λ2 _λ1)_ _ℓ=0_ (λ2 − _Cτ_ ) _λ[ℓ]2[12][τη][2][L][2][b][τ]1[−][2][−][ℓ]_ + λ[ℓ]2[(][C][τ] _[−]_ _[λ][1][)][b]2[τ]_ _[−][2][−][ℓ]_ (96)

1 _−_ _τ_ _−X2_     
_≤_ 12η[2]L[2]τ (1 − _ρmax)_ _ℓ=0_ (Cτ − _λ1) λ[ℓ]1_ 12τη[2]L[2]b[τ]1[−][2][−][ℓ] _−_ (λ2 − _Cτ_ )b[τ]2[−][2][−][ℓ]

1 Xτ −2   
+ 12η[2]L[2]τ (1 − _ρmax)_ _ℓ=0_ (λ2 − _Cτ_ ) λ[ℓ]2 12τη[2]L[2]b[τ]1[−][2][−][ℓ] + (Cτ − _λ1)b[τ]2[−][2][−][ℓ]_ _._

X     

In the following, we boundthe subscript of ρmax in the rest of the proof. Further, we introduce the following shorthand notation λ1 and λ2 − _Cτ as a function of ρmax. For notation simplicity we omit_
for the elements of G :

_ρ[2]_ _ρ[2]_
_f_ (ρ) = ρ + _g(ρ) =_ and h(τ ) = 12τη[2]L[2]. (97)

1 − _ρ_ _[·][ η][2][ ·][ 4][L][2][,]_ 1 − _ρ_ _[·][ η][2][ ·][ 4][L][2][,]_

Applying the Gershgorin’s theorem we obtain

_λ1_ min _ρ, Cτ_ 12τη[2]L[2] _ρ_ 0, (98)
_≥_ _−_ _≥_ _≥_

and 

_λ2_ max _f_ (ρ) + h(ρ), g(ρ) + Cτ _._ (99)
_≤_ _{_ _}_

Under the learning rate condition (79) we can show


_g(ρ) + Cτ_ (f (ρ) + h(ρ))
_−_

_ρ[2]_

= _[ρ][2]_

1 − _ρ_ _[·][ η][2][ ·][ 4][L][2][ +][ C][τ][ −]_ _[ρ][ −]_ 1 − _ρ_ _[·][ η][2][ ·][ 4][L][2][ −]_ [12][τη][2][L][2]

=1 + [3] 1

2 4τ 1

_[·]_ _−_ _[−]_ _[ρ][ −]_ [12][τη][2][L][2][ ≥] [0][.]

Therefore,
_λ2_ _Cτ_ _g(ρ)._
_−_ _≤_


(100)


-----

Substituting the bounds into (96) gives

_K_

1

_N_ _∥Xk,[r,τ]⊥[−][1]∥[2]_

_k=1_

X


_τ_ _−2_

_ℓ=0_

X


1 _τ_ _−2_
_≤_ 12η[2]L[2]τ (1 − _ρ)_ _ℓ=0_ _Cτ_ _λ[ℓ]1_ 12τη[2]L[2]b[τ]1[−][2][−][ℓ] _−_ (λ2 − _Cτ_ )b[τ]2[−][2][−][ℓ]

1 Xτ −2   
+ 12η[2]L[2]τ (1 − _ρ)_ _ℓ=0_ (λ2 − _Cτ_ ) λ[ℓ]2 12τη[2]L[2]b[τ]1[−][2][−][ℓ] + Cτ _b[τ]2[−][2][−][ℓ]_

1 _τ_ _−2X_     
_≤_ 12η[2]L[2]τ (1 _ρ)_ _Cτ_ _f_ (ρ)[ℓ] [ ]12τη[2]L[2]b[τ]1[−][2][−][ℓ]

_−_ _ℓ=0_

1 Xτ −2 
+ 12η[2]L[2]τ (1 _ρ)_ _g(ρ)λ[ℓ]2_ 12τη[2]L[2]b[τ]1[−][2][−][ℓ] + Cτ _b[τ]2[−][2][−][ℓ]_

_−_ _ℓ=0_

1 _τ_ _−2_ X  1   _τ_ _−2_ 
_≤_ 1 − _ρ_ _[C][τ]_ _ℓ=0_ _b[ℓ]1!_ + 1 − _ρ_ _[g][(][ρ][)][λ]2[τ]_ _ℓ=0_ _b[ℓ]1!_

X X

_Cτ_ _τ_ _−2_
+ 12η[2]L[2]τ (1 _ρ)_ _[g][(][ρ][)][λ]2[τ]_ _b[ℓ]2_

_−_ _ℓ=0_ !

X

_τ_ _−2_ _τ_ _−2_ _τ_ _−2_

_Dτ,ρCτ_ _b[ℓ]1_ + 12Dτ,ρ[2] _[η][2][L][2][ρ][2]_ _b[ℓ]1_ + _[C][τ]_ _τ,ρ_ _b[ℓ]2_ _._
_≤_ _ℓ=0_ ! _ℓ=0_ ! _τ [ρ][2][D][2]_ _ℓ=0_ !

X X X


12η[2]L[2]τ (1 − _ρ)_


(101)


where we have used the bound λ1 ≤ _f_ (ρ) < 1, λ2 > 1 and λ[τ]2 _[<][ 3][.]_

Plug in the expression of b1 and b2 and using the fact that Cτ < 2, ρL = ρ[2]Dτ,ρ gives

_K_

1

_N_ _∥Xk,[r,τ]⊥[−][1]∥[2]_

_k=1_

X


_τ_ _−2_
2Dτ,ρ + 12Dτ,ρ[2] _[η][2][L][2][ρ][2][]ρ[2]_ 4η[2]Dτ,ρ(α[2] _f_ (¯x[r,s]) + ϵ[2]g[) + 4][η][2][D][τ,ρ]ϵ[¯][2]L [+][ η][2][σ][2][]

_∥∇_ _∥[2]_

_s=0_

  _τ,ρ[C][τ]_ _τ_ _−2_ X  

+ _[D][2]_ _ρ[2]_ (12τη[2](α[2] _f_ (¯x[r,s]) + ϵ[2]g[) +][ η][2][ σ][2]

_τ_ _∥∇_ _∥[2]_ _n_ [)]

_s=0_

X

2Dτ,ρ + _[D]τ,ρ[2]_ _ρ[2][  ]4η[2]Dτ,ρ(α[2]_ _f_ (¯x[r,s]) + ϵ[2]g[) + 4][η][2][D][τ,ρ]ϵ[¯][2]L [+][ η][2][σ][2][]

_τ_ [2][ ρ][2][][ τ] _[−][2]_ _∥∇_ _∥[2]_

_s=0_

 + 2 _[D]τ,ρ[2]_ _ρ[2]_ _τ_ _−2(12τηX[2](α[2]_ _f_ (¯x[r,s]) + ϵ[2]g[) +][ η][2][ 1]

_τ_ _∥∇_ _∥[2]_ _n_ _[σ][2][)]_

_s=0_

X


_τ_ _−2_
3Dτ,ρρ[2] 4η[2]Dτ,ρ(α[2] _f_ (¯x[r,s]) + ϵ[2]g[) + 4][η][2][D][τ,ρ]ϵ[¯][2]L [+][ η][2][σ][2][]
_≤_ _∥∇_ _∥[2]_

_s=0_

X   _τ_ _−2_

+ 2Dτ,ρ[2] _[ρ][2][τ][ −][1]_ (12τη[2](α[2] _f_ (¯x[r,s]) + ϵ[2]g[) +][ η][2][ 1]

_∥∇_ _∥[2]_ _n_ _[σ][2][)][.]_
_s=0_

X


(102)


-----

Tidy up the expression gives

1 _K_ _τ_ _−2_

_Xk,[r,τ]_ _[−][1]_ 3Dτ,ρρ[2][  ]4η[2]Dτ,ρ(α[2] _f_ (¯x[r,s]) + ϵ[2]g[) + 4][η][2][D][τ,ρ]ϵ[¯][2]L [+][ η][2][σ][2][]

_N_ _∥_ _⊥_ _∥[2]_ _≤_ _∥∇_ _∥[2]_

_k=1_ _s=0_

X X

_τ_ _−2_

+ 2Dτ,ρ[2] _[ρ][2][τ][ −][1][(12][τη][2][(][α][2][∥∇][f]_ [(¯]x[r,s]) + ϵ[2]g[) +][ η][2][ 1]

_∥[2]_ _n_ _[σ][2][)]_
_s=0_

X


_τ_ _−2_

_s=0_

X


_τ_ _−2_

_C2_ _Dτ,ρ[2]_ _[ρ][2][]_ _η[2](α[2]∥∇f_ (¯x[r,s])∥[2] + ϵ[2]g[)]

_s=0_

X  

+ C2(Dτ,ρ)[2]τρ[2]η[2]ϵ¯[2]L [+][ C][2][τD][τ,ρ][ρ][2][η][2][σ][2][ +][ C][2][(][D][τ,ρ][)][2][ρ][2][η][2][ 1]

_n_ _[σ][2]_


_C2_
_≤_


(103)

(104)


_τ_ _−2_

2C2 _Dτ,ρ[2]_ _[ρ][2][η][2][(][α][2][∥∇][f]_ [(¯]x[r,s]) + ϵ[2]g[) +][ C][2][(][D][τ,ρ][)][2][τρ][2][η][2]ϵ[¯][2]L
_≤_ _∥[2]_

_s=0_

X

1 _Dτ,ρ_

+ C2 + 1 _ρ[2]τDτ,ρη[2]σ[2]_

_n_ _τ_

 

for some C2 > 0.

C.3 PROOF OF PROPOSITION 4

We prove (47) by splitting the terms A2,1 follows:


I(i _Sk[r][)]_ (Wk)i,j(xj[r,τ] _[−][1]_ _η_ _fj(xj[r,τ]_ _[−][1])_ _x¯k[r,τ]_ _[−][1]_
_∈_ _−_ _∇_ _−_
_iX∈Vk_   j[X]∈Vk


(a)
_A2,1_ = E

_≤2E_

where


_Np_


_k=1_


1

_Np_ _p · nη∇f[¯]k(¯x[r,τ]k_ _[−][1]_

_k=1_

X


I(i _Sk[r][)][r][ik]_
_∈_
_iX∈Vk_


+ 2E


_Np_


_k=1_


_rik ≜_ (Wk)i,j(x[r,τ]j _[−][1]_ _−_ _x¯k[r,τ]_ _[−][1]_ _−_ _η(∇fj(xj[r,τ]_ _[−][1]) −∇f[¯]k(¯x[r,τ]k_ _[−][1]))._ (105)

_jX∈Vk_

Equality (a) holds since


(Wk)i,j ¯x[r,τ]k _[−][1]_
_jX∈Vk_


_x¯k[r,τ]_ _[−][1]_
_iX∈Sk[r]_


_m_ ¯xk[r,τ] _[−][1]_
_·_
_k=1_

X


_k=1_ _i∈Sk[r]_


_k=1_

_K_

_k=1_

X


(106)


_x[r,τ]i_ _[−][1]_ = Npx¯[r,τ] _[−][1],_
_iX∈Vk_


and similarly,


(Wk)i,j _fk(¯xk[r,τ]_ _[−][1]) = np_ _fk(¯xk[r,τ]_ _[−][1])._ (107)
_∇_ [¯] _∇_ [¯]
_jX∈Vk_


_i∈Sk[r]_


Since samples are taken according to the rule specified by Assumption 6, the following probabilities
hold:

P _i ∈_ _Sk[r]_ _[|][ i][ ∈V][k]_ = p, P(i, j ∈ _Sk[r]_ _[|][, i, j][ ∈V][k][) =][ p][ ·][ np]n_ _[ −]1[1]_ _[,]_ (108)

_−_

P (i ∈ _Sk[r][, j][ ∈]_ _[S]ℓ[r]_ [|][ i][ ∈V][k][, j][ ∈V][ℓ][, k][ ̸][=][ ℓ][) =][ p][2][.] (109)

Consequently, we can evaluate the second term in (104) and obtain


_η_ _fk(¯x[r,τ]k_ _[−][1]_
_∇_ [¯]
_k=1_

X


_A2,1 =2E_


-----

_K_

2

_p_ _rik_ + p _[np][ −]_ [1]

(Np)[2][ E]  _∥_ _∥[2]_ _·_ _n_ 1

_kX=1_ _iX∈Vk_ _−_

2 

E(rik[⊤] _[r][jℓ][)]_

(Np)[2][ ·][ p][2][ X]

_k≠_ _ℓ_ _iX∈Vk_ _jX∈Vℓ_

2

_K_

1

_K_ _η∇f[¯]k(¯x[r,τ]k_ _[−][1])_

_k=1_

X


_rik[⊤]_ _[r][jk]_
_i,jX∈Vk_


_k=1_


=2E

+

+

_≤2E_

+

+

_≤2E_


_p_ _[np][ −]_ [1]
_·_ _n_ 1

_−_


2
_rik_ + _[p][(1][ −]_ _[p][)][n]_

_n_ 1

_iX∈Vk_ _−_


2

(Np)[2][ E]


_rik_
_∥_ _∥[2]_
_iX∈Vk_

_rik_
_∥_ _∥[2]_
_iX∈Vk_

_rik_
_∥_ _∥[2]_
_iX∈Vk_


_k=1_


_k=1_

_K_

_k=1_

X




_k≠_ _ℓ_

[X]


2 _p(1 −_ _p)_

(Np)[2] _n_ 1 [E]

_−_


_rik[⊤]_ _[r][jℓ]_
_jX∈Vℓ_


_i∈Vk_


_η_ _fk(¯x[r,τ]k_ _[−][1]_
_∇_ [¯]
_k=1_

X


_p_ _[np][ −]_ [1]
_·_ _n_ 1

_−_


2
_rik_ + _[p][(1][ −]_ _[p][)][n]_

_n_ 1

_iX∈Vk_ _−_


2

(Np)[2][ E]


_k=1_




_k≠_ _ℓ_

[X]


2 _p(1 −_ _p)_

(Np)[2] _n_ 1 [E]

_−_


1
2 _[∥][r][ik][∥][2][ + 1]2_ _[∥][r][jℓ][∥][2]_

_jX∈Vℓ_


_i∈Vk_


_η_ _fk(¯x[r,τ]k_ _[−][1]_
_∇_ [¯]
_k=1_

X


_p_ _[np][ −]_ [1]
_·_ _n_ 1

_−_


2
_rik_ + _[p][(1][ −]_ _[p][)][n]_

_n_ 1

_iX∈Vk_ _−_


2

(Np)[2][ E]


_k=1_


_k=1_


_K_

2 _p(1_ _p)_
+ _−_ E _rik_ _._ (110)

(Np)[2] _n_ 1 [(][K][ −] [1)][n] _∥_ _∥[2]_

_−_ _kX=1_ _iX∈Vk_

ByK substituting the expression of _rik_ we can bound terms _∥_ [P]k[K]=1 _i∈Vk_ _[r][ik][∥][2]_ and

_k=1_ _i∈Vk_ P

_[∥][r][ik][∥][2][ as]_

P _K_ P 2 _K_ 2

_rik_ = (Wk)i,j(xj[r,τ] _[−][1]_ _x¯k[r,τ]_ _[−][1]_ _η(_ _fj(xj[r,τ]_ _[−][1])_ _fk(¯x[r,τ]k_ _[−][1]))_

_−_ _−_ _∇_ _−∇_ [¯]

_kX=1_ _iX∈Vk_ _kX=1_ _iX∈Vk_ _jX∈Vk_

2

_K_

= η[2] (Wk)i,j( _fj(x[r,τ]j_ _[−][1])_ _fk(¯xk[r,τ]_ _[−][1]))_

_∇_ _−∇_ [¯]

_kX=1_ _iX∈Vk_ _jX∈Vk_

2

_K_

= η[2] (Wk)i,j( _fj(x[r,τ]j_ _[−][1])_ _fj(¯xk[r,τ]_ _[−][1]))_

_∇_ _−∇_

_kX=1_ _iX∈Vk_ _jX∈Vk_


(Wk)i,jL[2] _xj[r,τ]_ _[−][1]_ _x¯[r,τ]k_ _[−][1]_
_∥_ _−_ _∥[2]_
_jX∈Vk_


_≤_ _η[2]N_

_≤_ _η[2]N_


_i∈Vk_


_k=1_

_K_

_k=1_

X


_L[2]_ _x[r,τ]i_ _[−][1]_ _x¯k[r,τ]_ _[−][1]_ = η[2] _NL[2]_ _Xk,[r,τ]_ _[−][1]_
_∥_ _−_ _∥[2]_ _·_ _∥_ _⊥_ _∥[2]_
_iX∈Vk_ _kX=1_


(111)


-----

and


_rik_
_∥_ _∥[2]_
_iX∈Vk_


_k=1_


(Wk)i,j(x[r,τ]j _[−][1]_ _x¯[r,τ]k_ _[−][1]_ _η(_ _fj(x[r,τ]j_ _[−][1])_ _fk(¯xk[r,τ]_ _[−][1]))_
_−_ _−_ _∇_ _−∇_ [¯]
_jX∈Vk_


_i∈Vk_

_iX∈Vk_

_iX∈Vk_


_k=1_

_K_

_k=1_

X

_K_

_k=1_

X

_K_

_k=1_

X


2
(Wk)i,j _x[r,τ]j_ _[−][1]_ _x¯k[r,τ]_ _[−][1]_ _η(_ _fj(x[r,τ]j_ _[−][1])_ _fj(¯xk[r,τ]_ _[−][1]))_
_−_ _−_ _∇_ _−∇_

_kX=1_ _iX∈Vk_ _jX∈Vk_

_K_

2 2[]

(Wk)i,j 2 _xj[r,τ]_ _[−][1]_ _x¯k[r,τ]_ _[−][1]_ + 2η[2] _fj(x[r,τ]j_ _[−][1])_ _fj(¯xk[r,τ]_ _[−][1])_

_−_ _∇_ _−∇_

_k=1_ _i_ _k_ _j_ _k_ 

X X∈V X∈V

_K_ _K_

2
2 _x[r,τ]i_ _[−][1]_ _x¯[r,τ]k_ _[−][1]_ + 2η[2]L[2] _xi[r,τ]_ _[−][1]_ _x¯k[r,τ]_ _[−][1]_
_−_ _∥_ _−_ _∥[2]_

_kX=1_ _iX∈Vk_ _kX=1_ _iX∈Vk_

_K_

2(1 + η[2]L[2]) _Xk,[r,τ]_ _[−][1]_ _._ (112)
_∥_ _⊥_ _∥[2]_
_k=1_

X


Tidy up the expression leads to the following bound of A2,1:


_p_ _[np][ −]_ [1]
_·_ _n_ 1 [E]

_−_


_η_ _fk(¯x[r,τ]k_ _[−][1]_
_∇_ [¯]
_k=1_

X


_A2,1 ≤2E_

+

_≤2E_


_rik_
_iX∈Vk_


(Np)[2]


_k=1_


E∥rik∥[2]
_iX∈Vk_


(Np)[2][ p][(1][ −] _[p][)]_


_n −_ 1


_k=1_


_p_ _[np][ −]_ [1]
_·_ _n_ 1 _[η][2][ ·][ NL][2]_

_−_


_η_ _fk(¯x[r,τ]k_ _[−][1]_
_∇_ [¯]
_k=1_

X


E _Xk,[r,τ]_ _[−][1]_
_∥_ _⊥_ _∥[2]_
_k=1_

X


(Np)[2]


2(1 + η[2]L[2])E _Xk,[r,τ]_ _[−][1]_
_∥_ _⊥_ _∥[2]_
_k=1_

X


(Np)[2][ p][(1][ −] _[p][)]_


_n −_ 1


+ [2]

_N [η][2][L][2]_


_fk(¯x[r,τ]k_ _[−][1]_
_∇_ [¯]
_k=1_

X


E _Xk,[r,τ]_ _[−][1]_
_∥_ _⊥_ _∥[2]_
_k=1_

X


_≤2η[2]E_

+ [4]

_N_

_≤2η[2]E_


1 − _p_

_p(n −_ 1)


(1 + η[2]L[2])E _Xk,[r,τ]_ _[−][1]_
_∥_ _⊥_ _∥[2]_
_k=1_

X


_K_

E _Xk,[r,τ]_ _[−][1]_
_∥_ _⊥_ _∥[2]_

 _k=1_
X


+ [8]


1 − _p_

_p(n −_ 1) [+ 1]τ [2]


_fk(¯x[r,τ]k_ _[−][1]_
_∇_ [¯]
_k=1_

X


(113)


where the last inequality holds under the learning rate condition


_η[2]_ _≤_


1

(114)
24τ (4τ 1)L[2][ .]
_−_


This completes the proof of (47).


-----

C.4 PROOF OF PROPOSITION 5

We bound A2,2 in following the same rationale as Proposition 4.

_K_

_A2,2 =E_ [1] I(i _Sk[r][)]_ (Wk)i,j( _fj(x[r,s]j_ [)][ −] _[g]j[r,s][)]_

_Np_ _∈_ _∇_

_kX=1_ _iX∈Vk_   j[X]∈Vk 

_eik_

_K_ _K_

1 | 2 {z _n_ }
= _p_ _[np][ −]_ [1] _eik_ + p(1 _p)_

(Np)[2] _·_ _n −_ 1 [E] _kX=1_ _iX∈Vk_ _−_ _n −_ 1 _kX=1_

2 _p(1_ _p)_
+ _−_ _e[⊤]ik[e][jℓ]_

(Np)[2] _n −_ 1 [E] k≠ _ℓ_ _iX∈Vk_ _jX∈Vℓ_ 

1 [X]K 2  _n_ _K_
= _p_ _[np][ −]_ [1] _eik_ + p(1 _p)_

(Np)[2] _·_ _n −_ 1 [E] _kX=1_ _iX∈Vk_ _−_ _n −_ 1 _kX=1_


E∥eik∥[2]
_iX∈Vk_

E∥eik∥[2]
_iX∈Vk_


(115)


where the last equality is due to fact that the inter-cluster stochastic noise is zero mean and independent.

Recall the definition ξi[r,s] ≜ _gi[r,s]_ _−∇fi(x[r,s]i_ [)][. Using again the independence of the][ ξ][i][’s we get]


(Wk)i,jξj[r,s] = E

_kX=1_ _iX∈Vk_ _jX∈Vk_

_K_

2

E (Wk)i,jξj[r,s]

_kX=1_ _iX∈Vk_ _jX∈Vk_


_ξi[r,s]_ = Nσ[2], (116)
_i=1_

X

_K_

_Wk_ _σ[2]_
_∥_ _∥[2]_
_k=1_

X

(117)


_eik_ = E
_iX∈Vk_


_k=1_


and


E∥eik∥[2] =
_iX∈Vk_


_k=1_


_σ[2]_

_k=1_

X


_d[2]j_
_j=1_

X


_K_

_≤_ _σ[2][]1 +_ _n −_ 1 _ρ[2]k_ _≤_ _Kσ[2]_ + (n − 1) Kρ[2]max[σ][2]

_k=1_

X    

= 1 + (n 1) ρ[2]max _Kσ[2]._
_−_
  

where d1 ≤ _d2 ≤· · · ≤_ _dn = 1 are the singular values of Wk. Therefore,_


1 _n_
_A2,2 ≤_ (Np)[2] _p ·_ _[np]n_ _[ −]1[1]_ _n_ 1 1 + (n − 1) ρ[2]max _Kσ[2]_

 _−_ _[·][ Nσ][2][ +][ p][(1][ −]_ _[p][)]_ _−_

1 _np_ 1 1 1 _p_   
= _Np_ _n_ _−1_ _[σ][2]_ + _Np_ _n −_ 1 1 + (n − 1) ρ[2]max _σ[2]_

 _−_   _−_ 

_p[−][1]_ 1   

_≤_ _[σ]N[2]_ [+][ σ]N[2] _n_ _−1_ 1 + (n − 1) ρ[2]max _≤_ _[σ]N[2]_ 2 + p[−][1]ρ[2]max _._

_−_
     


(118)


The last inequality is due to p ≥ 1/n.


-----

C.5 PROOF OF PROPOSITION 6

Invoking the descent inequality Lemma 1 and the error bound for T1-T5 given by Corollary 2,
Proposition 3 and Eq. (41):

Ef (x[r][+1])


_τ_ _−1_

E _f_ (¯x[r,s])
_∥∇_ _∥[2]_ _−_ _[η]4_
_s=0_

X

_K_

2

[1] _fk(¯x[r,s]k_ [)]

_K_ _∇_ [¯]

_k=1_

X


_τ_ _−1_

_s=0_

X


Ef (x[r])
_≤_ _−_ _[η]4_


_∇fi(x[r,s]i_ [)]
_i=1_

X


_τ_ _−1_

_s=0_

X


_−_ _[η]4_



_[τ]_ _[−][1]_
+ C1L[2]η[3]τ _τ + ρ[2]max[D][τ,ρ]_ (α[2] _f_ (¯x[r,s]) + ϵ[2]g[) +][ C][1][L][2][η][3][τ][ 2][ρ][2]max[D][τ,ρ]ϵ[¯][2]L

_∥∇_ _∥[2]_
_s=0_

   X

+ C1τ [2]L[2]η[3]ρ[2]max[σ][2][ +][ C][1][L][2][(][τ][ 2][ +][ D]τ,ρ[2] _[ρ][2]max[)][η][3][ σ][2]_


(119)


_τ_ _−1_

_s=0_

X


+ _[η][2][L]_


+ _[η][2][L]_

2 _[τ σ]N[2]_


1

E _fi(x[r,s]i_ [)]

_N_ _∇_

_i=1_

X

_τ_ _−1_

E∥∇f (¯x[r,s])∥[2]
_s=0_

X


Ef (x[r])
_≤_ _−_ _[η]4_



_[τ]_ _[−][1]_
+ C1L[2]η[3]τ _τ + ρ[2]max[D][τ,ρ]_ (α[2]E _f_ (¯x[r,s]) + ϵ[2]g[) +][ C][1][L][2][η][3][τ][ 2][ρ]max[2] _[D][τ,ρ]ϵ[¯][2]L_

_∥∇_ _∥[2]_
_s=0_

   X

+ C1τ [2]L[2]η[3]ρ[2]max[σ][2][ +][ C][1][L][2][(][τ][ 2][ +][ D]τ,ρ[2] _[ρ]max[2]_ [)][η][3][ σ][2]

_n_ [+][ η][2][Lτ σ]N [2] _[.]_


The last inequality holds under the condition that

_η_ (120)
_≤_ 2[1]L _[.]_


If we further enforce

_C1L[2]η[3]τ_ _τ + ρ[2]max[D][τ,ρ]_ _α[2]_
   _≤_ _[η]8_

then
Ef (x[r][+1])


_η[2]_ _≤_


1

(121)
8C1L[2]τ (τ + ρ[2]max[D][τ,ρ][)][ α][2][,]


_τ_ _−1_

Ef (x[r]) E _f_ (¯x[r,s])
_≤_ _−_ _[η]8_ _∥∇_ _∥[2]_

_s=0_

X

+ C1L[2]η[3]τ [2][  ]τ + ρ[2]max[D][τ,ρ] _ϵ[2]g_ [+][ C][1][L][2][η][3][τ][ 2][ρ]max[2] _[D][τ,ρ]ϵ[¯][2]L_

+ C1τ [2]L[2]η[3]ρ[2]max[σ][2][ +][ C][1][L][2][(][τ][ 2][ +][ D]τ,ρ[2] _[ρ][2]max[)][η][3][ σ][2]_

_n_ [+][ η][2][Lτ σ]N[2]


_τ_ _−1_

=Ef (x[r]) − _[η]8_ E∥∇f (¯x[r,s])∥[2] + C1L[2]η[3]τ [2][  ]τ + ρ[2]max[D][τ,ρ] _ϵ[2]g_

_s=0_

X 

+ C1L[2]η[3] _τ_ [2]ρ[2]max[(][D][τ,ρ]ϵ[¯][2]L [+][ σ][2][) + (][τ][ 2][ +][ D]τ,ρ[2] _[ρ][2]max[)]_ _[σ][2]_ + η[2]Lτ [σ][2]

_n_ _N_

 

_τ_ _−1_

Ef (x[r]) E _f_ (¯x[r,s]) + η[2]Lτ [σ][2]
_≤_ _−_ _[η]8_ _∥∇_ _∥[2]_ _N_

_s=0_

X

1
+ C1L[2]η[3]τ [2][  ]τ + ρ[2]max[D][τ,ρ] _ϵ[2]g_ [+ 2][C][1][L][2][η][3] _τ_ [2]ρ[2]max[D][τ,ρ]ϵ[¯][2]L [+][ τ][ 2][σ][2] max

_n_ [+][ ρ][2]

  

the last inequality is due to Dτ,ρ  _τ and ρmax_ 1.
_≤_ _≤_


(122)


-----

C.6 PROOF OF PROPOSITION 7

Invoking the descent inequality Lemma 1 and the error bound for T1-T5 given by Corollary 2,
Proposition 3, and 4:

Ef (x[r][+1])


_τ_ _−1_

E _f_ (¯x[r,s])
_∥∇_ _∥[2]_ _−_ _[η]4_
_s=0_

X

_K_

2

[1] _fk(¯x[r,s]k_ [)]

_K_ _∇_ [¯]

_k=1_

X


_τ_ _−1_

_s=0_

X


Ef (x[r])
_≤_ _−_ _[η]4_


_∇fi(x[r,s]i_ [)]
_i=1_

X


_τ_ _−1_

_s=0_

X


_−_ _[η]4_

+ _[η]_

4

+ _[η]_


_τ_ 1 _N_ _τ_ 1
_−_ 2 _−_

_[η]_ E _f_ (¯x[r,s]) _fi(x[r,s]i_ [)] + _[η]_ E _f_ (¯x[r,s])

4 _∇_ _−_ _N[1]_ _∇_ 4 _∇_ _−_ _K[1]_

_s=0_ _i=1_ _s=0_

X X X

_τ_ 1 _N_ _K_
_−_ 2

_[η]_ E [1] _fi(x[r,s]i_ [)][ −] [1] _fk(¯x[r,s]k_ [)]

4 _N_ _∇_ _K_ _∇_ [¯]

_s=0_ _i=1_ _k=1_

X X X

_τ_ _−2_ _L_ _L_

E _x[r,s][+1]_ _x¯[r,s]_ + E _x[r,τ]_ _[−][1]_

2 _−_ _∥[2]_ 2 _∥[2]_

_s=0_  _[∥][¯]_   _[∥][x][r][+1][ −]_ [¯] 

X


_∇f[¯]k(¯x[r,s]k_ [)]
_k=1_

X


_τ_ _−1_

E _f_ (¯x[r,s])
_∥∇_ _∥[2]_ _−_ _[η]4_
_s=0_

X

_K_

2

[1] _fk(¯x[r,s]k_ [)]

_K_ _∇_ [¯]

_k=1_

X


_τ_ _−1_

_s=0_

X


Ef (x[r])
_≤_ _−_ _[η]4_


_∇fi(x[r,s]i_ [)]
_i=1_

X


_τ_ _−1_

_s=0_

X


_−_ _[η]4_



_[τ]_ _[−][1]_
+ C1L[2]η[3]τ _τ + ρ[2]max[D][τ,ρ]_ (α[2] _f_ (¯x[r,s]) + ϵ[2]g[) +][ C][1][L][2][η][3][τ][ 2][ρ]max[2] _[D][τ,ρ]ϵ[¯][2]L_

_∥∇_ _∥[2]_
_s=0_

   X

+ C1τ [2]L[2]η[3]ρ[2]max[σ][2][ +][ C][1][L][2][(][τ][ 2][ +][ D]τ,ρ[2] _[ρ]max[2]_ [)][η][3][ σ][2]


_τ_ _−2_

+ _[L]_

2 _[η][2]_

_s=0_

X

1

+ Lη[2]E


+ _[L]_

2 _[η][2][(][τ][ −]_ [1)] _[σ]N[2]_


1

_fi(x[r,s]i_ [)]

_N_ _∇_

_i=1_

X

_K_

_fk(¯x[r,τ]k_ _[−][1])_
_∇_ [¯]
_k=1_

X


+ _[L]_

2 _[η][2][ σ]N[2]_


+ _[L]_


_Gp + [1]_

_τ_ [2]


2 + _m[n]_ _[ρ]max[2]_ _._ (123)




E _Xk,[r,τ]_ _[−][1]_
_∥_ _⊥_ _∥[2]_
_k=1_

X


Denote for short

_G[′]p_ [≜] _[G][p]_ [+ 1]τ [2][ .] (124)

Further applying the bounds on the consensus error derived in Proposition 2:

Ef (x[r][+1])


_τ_ _−1_

E _f_ (¯x[r,s])
_∥∇_ _∥[2]_ _−_ _[η]4_
_s=0_

X

_K_

2

[1] _fk(¯x[r,s]k_ [)]

_K_ _∇_ [¯]

_k=1_

X


_τ_ _−1_

_s=0_

X


Ef (x[r])
_≤_ _−_ _[η]4_


_∇fi(x[r,s]i_ [)]
_i=1_

X


_τ_ _−1_

_−_ _[η]4_

_s=0_

X



_[τ]_ _[−][1]_
+ C1L[2]η[3]τ _τ + ρ[2]max[D][τ,ρ]_ (α[2] _f_ (¯x[r,s]) + ϵ[2]g[) +][ C][1][L][2][η][3][τ][ 2][ρ][2]max[D][τ,ρ]ϵ[¯][2]L

_∥∇_ _∥[2]_
_s=0_

   X


-----

+ C1τ [2]L[2]η[3]ρ[2]max[σ][2][ +][ C][1][L][2][(][τ][ 2][ +][ D]τ,ρ[2] _[ρ][2]max[)][η][3][ σ][2]_


_τ_ _−2_

+ _[L]_

2 _[η][2]_

_s=0_

X


+ _[L]_

2 _[η][2][(][τ][ −]_ [1)] _[σ]N[2]_


1

_fi(x[r,s]i_ [)]

_N_ _∇_

_i=1_

X

_K_

_fk(¯x[r,τ]k_ _[−][1])_
_∇_ [¯]
_k=1_

X


+ Lη[2]E

+ 4LG[′]p

+ 4LG[′]p

+ _[L]_

2 _[η][2][ σ]N[2]_


_τ_ _−2_

2C2 _s=0_ _Dτ,ρ[2]_ _[ρ][2]max[η][2][(][α][2][∥∇][f]_ [(¯]x[r,s])∥[2] + ϵ[2]g[)]!

X

1 _Dτ,ρ_

_C2Dτ,ρ[2]_ _[τρ][2]max[η][2]ϵ[¯][2]L_ [+][ C][2] + 1 _ρ[2]max[τD][τ,ρ][η][2][σ][2]_

_n_ _τ_

  

2 + _m[n]_ _[ρ]max[2]_ (125)
 


Rearranging terms and tidy up the expression we have

Ef (x[r][+1])


_τ_ _−1_

E _f_ (¯x[r,s])
_∥∇_ _∥[2]_ _−_ _[η]4_
_s=0_

X


_τ_ _−1_

_s=0_

X


Ef (x[r])
_≤_ _−_ _[η]4_


_∇fi(x[r,s]i_ [)]
_i=1_

X

_N_

1

_fi(x[r,s]i_ [)]

_N_ _∇_

_i=1_

X


_τ_ _−1_

_s=0_

X


_K_ _τ_ 2

2 _−_

[1] _fk(¯x[r,s]k_ [)] + _[L]_

_∇_ [¯] 2 _[η][2]_
_k=1_ _s=0_

X X

2

_K_

_fk(¯x[r,τ]k_ _[−][1])_
_∇_ [¯]
_k=1_

X


_−_ _[η]4_


+ Lη[2]E



_[τ]_ _[−][1]_
+ C1L[2]η[3]τ _τ + ρ[2]max[D][τ,ρ]_ (α[2] _f_ (¯x[r,s]) + ϵ[2]g[)] (126)

_∥∇_ _∥[2]_
_s=0_

  _τ_ _−2X_

+ 8C2G[′]p[LD]τ,ρ[2] _[ρ][2]max[η][2]_ _s=0(α[2]∥∇f_ (¯x[r,s])∥[2] + ϵ[2]g[)]!
X

+ C1L[2]η[3] _τ_ [2]Dτ,ρρ[2]maxϵ[¯][2]L [+][ τ][ 2][ρ]max[2] _[σ][2][ + (][τ][ 2][ +][ D]τ,ρ[2]_ _[ρ][2]max[)]_ _[σ][2]_
 


+ _[L]_ 2 + _[n]_ max _η[2][ σ][2]_

2 [(][τ][ −] [1)][η][2][ σ]N[2] [+][ L]2 _m_ _[ρ][2]_ _N_

 1 _D_ _τ,ρ_

+ 4C2LG[′]p[η][2] _Dτ,ρ[2]_ _[τρ][2]maxϵ[¯][2]L_ [+] + 1 _ρ[2]max[τD][τ,ρ][σ][2]_

_n_ _τ_

  


Notice that if the following conditions on the learning rate are satisfied

_η_

_η4_ _[≥]_ _[η][2][L,]_ (127)
_τ + ρ[2]max[D][τ,ρ]_ _α[2]_ + 8C2G[′]p[LD]τ,ρ[2] _[ρ][2]max[η][2][α][2][,]_
8

_[≥]_ _[C][1][L][2][η][3][τ]_
  


-----

then the terms associated to the gradients will be negative and

Ef (x[r][+1])

_τ_ _−1_

Ef (x[r]) E _f_ (¯x[r,s])
_≤_ _−_ _[η]8_ _∥∇_ _∥[2]_

_s=0_

X

1
+ C1L[2]η[3]τ [2][  ]τ + ρ[2]max[D][τ,ρ] _ϵ[2]g_ [+ 2][C][1][L][2][η][3] _τ_ [2]ρ[2]max[D][τ,ρ]ϵ[¯][2]L [+][ τ][ 2][σ][2] max

_n_ [+][ ρ][2]

  


+ Lτη[2][ σ]N[2] [+] 8C2G[′]p[LD]τ,ρ[2] _[τρ][2]max_ _η[2]ϵ[2]g_

  1 _Dτ,ρ_ _n_

+ 4C2LG[′]p[η][2] _Dτ,ρ[2]_ _[τρ][2]maxϵ[¯][2]L_ [+] + 1 _ρ[2]max[τD][τ,ρ][σ][2]_ + _[L]_ max _η[2][ σ][2]_

_n_ _τ_ 2 _m_ _[ρ][2]_ _N [.]_

     


In the last step we clean the condition on the learning rate η. Collecting all the conditions on η:


1
_η[2]_ (128)
_≤_ 24L[2]τ (4τ 1) _[,]_

_−_

_η_
(129)
4

_η_ _[≥]_ _[η][2][L,]_
_τ + ρ[2]max[D][τ,ρ]_ _α[2]_ + 8C2G[′]p[LD]τ,ρ[2] _[ρ]max[2]_ _[η][2][α][2][.]_ (130)
8

_[≥]_ _[C][1][L][2][η][3][τ]_
  

Clearly, (128) implies (129). To ensure (130) it suffices to require


_η_

_τ + ρ[2]max[D][τ,ρ]_ _α[2]_
16η _[≥]_ _[C][1][L][2][η][3][τ]_ (131)
  

_p[LD]τ,ρ[2]_ _[ρ]max[2]_ _[η][2][α][2][.]_
16

_[≥]_ [8][C][2][G][′]

_n_ _m_

_G[′]p_ [=][ G][p] [+ 1]τ [2][,] and _Gp =_ _m( −n_ 1) _[.]_ (132)

_−_


Recall the definition of G[′]p[:]

It can be verified that if


1

(133)
_C3[′]_ _[α][2][τL.]_


_η ≤_


for some C3[′] _[>][ 0][ large enough, then]_

_η_

_τ + ρ[2]max[D][τ,ρ]_ _α[2]_
16

_[≥]_ _[C][1][L][2][η][3][τ]_

_η_ 1   

_LDτ,ρ[2]_ _[ρ][2]max[η][2][α][2][.]_

32 _τ_ [2]

_[≥]_ [8][C][2]  


It remains to guarantee

_η_

_τ,ρ[ρ][2]max[η][2][α][2][.]_ (134)
32

_[≥]_ [8][C][2][G][p][LD][2]

Rearranging terms gives the condition


1

(135)
256C2GpDτ,ρ[2] _[ρ]max[2]_ _[α][2][L.]_


_η ≤_


Combining with (133) and using the fact that Dτ,ρρmax _τ provides the final condition on η as_
_≤_


1

_._ (136)
_αGpDτ,ρ ρmax_



_η ≤_


1,
_C3ατL_

_[·][ min]_ 


-----

D SUPPORTING LEMMAS

D.1 PROOF OF LEMMA 1

Since the global average of the local copies follows the update [cf. (19)]:


_x¯[r,s][+1]_ = ¯x[r,s] _η_ [1]
_−_ _N_


_gi[r,s][,][ ∀][s][ = 0][, . . ., τ][ −]_ [1][.] (137)
_i=1_

X


Under Assumption 1, we can apply the descent lemma at points ¯x[r,s][+1] and ¯x[r,s] for s = 0, . . ., τ −2,
conditioned on Fr,s−1:

_L_

Er,s 1f (¯x[r,s][+1]) _f_ (¯x[r,s]) + _f_ (¯x[r,s])[⊤]Er,s 1 _x¯[r,s][+1]_ _x¯[r,s][]_ + Er,s 1 _x[r,s][+1]_ _x¯[r,s]_
_−_ _≤_ _∇_ _−_ _−_ _−_ 2 _−_ _∥[2]_

_N_   _[∥][¯]_

(a) _L_
= _f_ (¯x[r,s]) − _η∇f_ (¯x[r,s])[⊤][][ 1]N _∇fi(x[r,s]i_ [)] + Er,s−1 2 _x[r,s][+1]_ _−_ _x¯[r,s]∥[2]_

Xi=1   _[∥][¯]_ 

_N_

=f (¯x[r,s]) _x[r,s])[⊤][][ 1]_ _fi(x[r,s]i_ [)]
_−_ [1]2 _[η][∇][f]_ [(¯] _N_ _∇_

_i=1_

X 

_N_ _K_

_L_

_−_ 2[1] _[η][∇][f]_ [(¯]x[r,s])[⊤][][ 1]N _∇fi(x[r,s]i_ [)][ ±][ 1]K _∇f[¯]k(¯x[r,s]k_ [)] + Er,s−1 2 _x[r,s][+1]_ _−_ _x¯[r,s]∥[2]_

Xi=1 _kX=1_   _[∥][¯]_ 


(b)
_f_ (¯x[r,s])
_≤_ _−_ _[η]2_


_N_ _N_

(b) 1 2
_f_ (¯x[r,s]) _x[r,s])_ + [1] [1] _fi(x[r,s]i_ [)] _f_ (¯x[r,s]) _fi(x[r,s]i_ [)]
_≤_ _−_ _[η]2_ 2 _[∥∇][f]_ [(¯] _∥[2]_ 2 _N_ _∇_ _−_ [1]2 _∇_ _−_ _N[1]_ _∇_

_i=1_ _i=1_

X X

_K_ _K_

1 2 2
_x[r,s])_ + [1] [1] _fk(¯x[r,s]k_ [)] _f_ (¯x[r,s]) _fk(¯x[r,s]k_ [)]

_−_ _[η]2_ 2 _[∥∇][f]_ [(¯] _∥[2]_ 2 _K_ _k=1_ _∇_ [¯] _−_ [1]2 _∇_ _−_ _K[1]_ _k=1_ _∇_ [¯] !

X X

_N_ _K_

+ _[η]_ _x[r,s])_ [1] _fi(x[r,s]i_ [)][ −] [1] _fk(¯x[r,s]k_ [)]

2 _[∥∇][f]_ [(¯] _∥_ _N_ _∇_ _K_ _∇_ [¯]

_i=1_ _k=1_

X X

_L_

+ Er,s 1 _x[r,s][+1]_ _x¯[r,s]_
_−_ 2 _−_ _∥[2]_

 _[∥][¯]_ 

_N_ _K_

(c) 2 2
_f_ (¯x[r,s]) _x[r,s])_ [1] _fi(x[r,s]i_ [)] [1] _fk(¯x[r,s]k_ [)]
_≤_ _−_ _[η]4_ _[∥∇][f]_ [(¯] _∥[2]_ _−_ _[η]4_ _N_ _∇_ _−_ _[η]4_ _K_ _∇_ [¯]

_i=1_ _k=1_

X X


1
_x[r,s])_ + [1]
2 _[∥∇][f]_ [(¯] _∥[2]_ 2


_N_

2
_fi(x[r,s]i_ [)]
_∇_ _−_ [1]2
_i=1_

X


_N_ _K_

2 2

+ _[η]_ _f_ (¯x[r,s]) _fi(x[r,s]i_ [)] + _[η]_ _f_ (¯x[r,s]) _fk(¯x[r,s]k_ [)]

4 _∇_ _−_ _N[1]_ _∇_ 4 _∇_ _−_ _K[1]_ _∇_ [¯]

_i=1_ _k=1_

X X

_N_ _K_

2 _L_

+ _[η]4_ _N[1]_ _∇fi(x[r,s]i_ [)][ −] _K[1]_ _∇f[¯]k(¯x[r,s]k_ [)] + Er,s−1 2 _x[r,s][+1]_ _−_ _x¯[r,s]∥[2]_ _,_

_i=1_ _k=1_  _[∥][¯]_ 

X X

where (a) is due to Assumption 2, (b) is due to 2ab = ∥a∥[2] + ∥b∥[2] _−∥a −_ _b∥[2]_ and ab ≤∥a∥∥b∥,
and (c) is due to ∥a∥∥b∥≤ [1]2 _[∥][a][∥][2][ +][ 1]2_ _[∥][b][∥][2][. Notation][ a][ ±][ b][ stands for adding and subtracting, i.e.,]_

_a ± b = a + b −_ _b._

For the pair (¯x[r,τ] _[−][1], x[r][+1]) we have according to (19) and (22):_


_x¯[r,τ]_ _[−][1]_ _η_ [1]
_−_ _N_


Er,τ 2x[r][+1] = Er,τ 2(¯x[r,τ] ) = Er,τ 2
_−_ _−_ _−_


_gi[r,τ]_ _[−][1]_
_i=1_

X


Applying the descent lemma in the same way as before yields

Er,τ 2f (x[r][+1])
_−_

_f_ (¯x[r,τ] _[−][1]) +_ _f_ (¯x[r,τ] _[−][1])[⊤]Er,τ_ 2(x[r][+1] _x¯[r,τ]_ _[−][1]) +_ _[L]_ _x[r,τ]_ _[−][1]_
_≤_ _∇_ _−_ _−_ 2 [E][r,τ] _[−][2][∥][x][r][+1][ −]_ [¯] _∥[2]_


-----

_f_ (¯x[r,τ] _[−][1])_ _x[r,τ]_ _[−][1])_
_≤_ _−_ _[η]4_ _[∥∇][f]_ [(¯] _∥[2]_ _−_ _[η]4_


_N_

2
_∇fi(xi[r,τ]_ _[−][1])_ _−_ _[η]4_
_i=1_

X


_fk(¯x[r,τ]k_ _[−][1]_
_∇_ [¯]
_k=1_

X

_K_

_fk(¯xk[r,τ]_ _[−][1]_
_∇_ [¯]
_k=1_

X


+ _[η]_

4

+ _[η]_


_N_ _K_

2

_∇f_ (¯x[r,τ] _[−][1]) −_ _N[1]_ _∇fi(xi[r,τ]_ _[−][1])_ + _[η]4_ _∇f_ (¯x[r,τ] _[−][1]) −_ _K[1]_ _∇f[¯]k(¯xk[r,τ]_ _[−][1]_

_i=1_ _k=1_

X X

_N_ _K_

2 _L_

_N[1]_ _∇fi(x[r,τ]i_ _[−][1]) −_ _K[1]_ _∇f[¯]k(¯xk[r,τ]_ _[−][1])_ + Er,τ _−2_ 2 _x[r,τ]_ _[−][1]∥[2]_

_i=1_ _k=1_  _[∥][x][r][+1][ −]_ [¯]

X X


Taking expectation, summing over the iterations in round r over s = 0, . . ., τ − 1 and using the fact
that x[r] = ¯x[r,][0] completes the proof.

D.2 PROOF OF LEMMA 2

Recall the average update of the k-th cluster and that of the global average given by (15) and (19),
respectively, for s = 0, . . ., τ − 1:

_x¯[r,s]k_ [+1] = ¯x[r,s]k _η_ ¯gk[r,s] (138)
_−_ _·_


_x¯[r,s][+1]_ = ¯x[r,s] _η_ [1]
_−_ _·_ _N_


_gi[r,s][.]_ (139)
_i=1_

X


Taking the difference gives

E _x¯[r,s][+1]_ _x¯[r,s]k_ [+1]
_∥_ _−_ _∥[2]_

1

=E _x[r,s]_ _x¯[r,s]k_ [)][ −] _[η]_

_−_ _n_

[(¯] 1

+ η[2]E _n_ _ξi[r,s]_ _−_ _N[1]_

_iX∈Vk_


_fi(x[r,s]i_ [)][ −] [1]
_∇_ _N_
_iX∈Vk_

2

_N_

_ξi[r,s]_
_i=1_

X


_∇fi(x[r,s]i_ [)]
_i=1_

X


(140)


_fi(x[r,s]i_ [)][ −] [1]
_∇_ _N_
_iX∈Vk_


(1 + ϵ)E _x¯[r,s]_ _x¯[r,s]k_
_≤_ _∥_ _−_ _[∥][2][ + (1 +][ ϵ][−][1][)][η][2][E]_


_∇fi(x[r,s]i_ [)]
_i=1_

X


_ξi[r,s]_ _−_ _N[1]_
_iX∈Vk_


_ξi[r,s]_
_i=1_

X


+ η[2]E


where ϵ > 0 is some constant to be chosen.

Averaging over k = 1, . . ., K:

_K_

1

_K_ E∥x¯[r,s][+1] _−_ _x¯[r,s]k_ [+1]∥[2]

_k=1_

X


(1 + ϵ) [1]
_≤_ _K_

+ η[2][ 1]


_K_

_k=1_ E∥x¯[r,s] _−_ _x¯[r,s]k_ _[∥][2][ + (1 +][ ϵ][−][1][)][η][2][ 1]K_

X


1

_fi(x[r,s]i_ [)][ −] [1]

_n_ _∇_ _N_

_iX∈Vk_

_N_

2
_i=1_ _∇fi(x[r,s]i_ [)] !

X


_∇fi(x[r,s]i_ [)]
_i=1_

X


_k=1_


_ξi[r,s]_ _−_ _N[1]_
_iX∈Vk_


_ξi[r,s]_
_i=1_

X


_k=1_


(a)
=(1 + ϵ) [1]


E _x¯[r,s]_ _x¯[r,s]k_
_k=1_ _∥_ _−_ _[∥][2]_

X


2
_∇fi(x[r,s]i_ [)] _−_ E
_iX∈Vk_


+ (1 + ϵ[−][1])η[2]


_k=1_


-----

1
+ η[2]

_K_

(1 + ϵ) [1]
_≤_ _K_

=(1 + ϵ) [1]


_ξi[r,s]_ E
_−_
_iX∈Vk_


_ξi[r,s]_
_i=1_

X


_k=1_


+ η[2][ K][ −] [1]


E _x¯[r,s]_ _x¯[r,s]k_
_k=1_ _∥_ _−_ _[∥][2][ + (1 +][ ϵ][−][1][)][η][2]_

X

_K_

E _x¯[r,s]_ _x¯[r,s]k_
_k=1_ _∥_ _−_ _[∥][2]_

X


_∇fi(x[r,s]i_ [)]
_iX∈Vk_


_σ[2]_


_k=1_


+ η[2][ K][ −] [1]


_∇fi(x[r,s]i_ [)][ ± ∇]f[¯]k(¯x[r,s]k [)][ ± ∇]f[¯]k(¯x[r,s])
 


+ (1 + ϵ[−][1])η[2]


_σ[2]_


_k=1_


_i∈Vk_


(1 + ϵ) [1]
_≤_ _K_


E _x¯[r,s]_ _x¯[r,s]k_
_k=1_ _∥_ _−_ _[∥][2][ + (1 +][ ϵ][−][1][)][η][2]_

X


_∇fi(x[r,s]i_ [)][ −∇]f[¯]k(¯x[r,s]k [)]


_k=1_


_i∈Vk_


+ (1 + ϵ[−][1])η[2][ 3]


_K_

2
E _fk(¯x[r,s]k_ [)][ −∇]f[¯]k(¯x[r,s]) + (1 + ϵ[−][1])η[2][ 3]
_∇_ [¯] _K_
_k=1_

X


E∥∇f[¯]k(¯x[r,s])∥[2]
_k=1_

X


+ η[2][ K][ −] [1] _σ[2]_


(b)
(1 + ϵ) [1]
_≤_ _K_


E _x¯[r,s]_ _x¯[r,s]k_
_k=1_ _∥_ _−_ _[∥][2][ + (1 +][ ϵ][−][1][)][η][2]_

X


3

_≤_ (1 + ϵ) K[1] _k=1_ E∥x¯[r,s] _−_ _x¯[r,s]k_ _[∥][2][ + (1 +][ ϵ][−][1][)][η][2]_ _N_ _k=1_ _L[2]E_ _Xk,[r,s]⊥_ !

X X

_K_ _K_

2 2

+ (1 + ϵ[−][1])η[2][ 3]K _L[2]E_ _x¯[r,s]k_ _−_ _x¯[r,s]_ + (1 + ϵ[−][1])η[2][ 3]K E _∇f[¯]k(¯x[r,s])_ + η[2][ K][ −]N [1] _σ[2]_

_k=1_ _k=1_

X X

_K_ _K_

1 3 2
= 1 + ϵ + 3L[2]η[2](1 + ϵ[−][1]) _K_ _k=1_ E∥x¯[r,s] _−_ _x¯[r,s]k_ _[∥][2][ + (1 +][ ϵ][−][1][)][η][2]_ _N_ _k=1_ _L[2]E_ _Xk,[r,s]⊥_ !
   X X

_K_

2

+ (1 + ϵ[−][1])η[2][ 3] E _fk(¯x[r,s])_ + η[2][ K][ −] [1] _σ[2]._ (141)

_K_ _∇_ [¯] _N_

_k=1_

X

In (a) we used the fact that


_fi(x[r,s]i_ [) = 1]
_∇_ _N_
_iX∈Vk_

_K_

_xi_ _x¯_ =
_i=1_ _∥_ _−_ _∥[2]_

X


_ξi[r,s]_ = N[1]
_iX∈Vk_


_∇fi(x[r,s]i_ [)][,]
_i=1_

X


_ξi[r,s][.]_ (142)
_i=1_

X


_k=1_


_k=1_


and


_K_

_xi_ _K_ _x¯_ with _x¯ = [1]_
_∥_ _∥[2]_ _−_ _∥_ _∥[2]_ _K_
_i=1_

X


_xi._ (143)

_k=1_

X

_K_

1 2

_N_ _k=1_ E _Xk,[r,s]⊥_ !

X


In (b) we applied the L-smoothness of fi and _f[¯]k._


Choosing ϵ =

we have


1

4τ 1 [and using the condition that]
_−_


_η[2]_ _≤_


24τ (4τ − 1)L[2]


E _x¯[r,s][+1]_ _x¯[r,s]k_ [+1]
_∥_ _−_ _∥[2]_
_k=1_

X

1 1 1
1 +

4τ 1 [+] 2(4τ 1) _K_
_−_ _−_ 


E _x¯[r,s]_ _x¯[r,s]k_
_k=1_ _∥_ _−_ _[∥][2][ + 12][τη][2][L][2]_

X


-----

_K_

2

+ 12τη[2][ 1] E _fk(¯x[r,s])_ + η[2][ K][ −] [1] _σ[2]_

_K_ _∇_ [¯] _N_

_k=1_

X

_K_ _K_

1 1 2
_≤Cτ_ _K_ _k=1_ E∥x¯[r,s] _−_ _x¯[r,s]k_ _[∥][2][ + 12][τη][2][L][2]_ _N_ _k=1_ E _Xk,[r,s]⊥_ !

X X

+ 12τη[2][  ]α[2]E∥∇f (¯x[r,s])∥[2] + ϵ[2]g + η[2][ K][ −]N [1] _σ[2]._

In the last inequality we applied Assumption 5 on the inter-cluster heterogeneity.

D.3 PROOF OF LEMMA 3

We follow the perturbed average consensus analysis. Recall the update equation of the consensus
error given in (17):

_Xk,[r,s][+1]_ = (Wk _J)(Xk,[r,s]_ _k_ [)][.] (144)
_⊥_ _−_ _⊥_ _[−]_ _[ηG][r,s]_


Squaring both sides and conditioning:

E _Xk,[r,s][+1]_ = E E (Wk _J)(Xk,[r,s]_ _k_ [)][ −] _[ηG]k[r,s][)][∥][2][|F][r,s][−][1]_
_∥_ _⊥_ _∥[2]_ _∥_ _−_ _⊥_ _[±][ η][∇][F][k][(][X]_ _[r,s]_

_≤_ E∥(W k − _J)(Xk,[r,s]⊥_ _[−]_ _[η][∇][F][k][(][X]k[r,s][))][∥][2][ +][ η][2][ρ]k[2][nσ][2]_ []

_≤_ _ρ[2]k[(1 +][ ζ]k[−][1][)][ ·][ E][∥][X]k,[r,s]⊥[∥][2][ +][ ρ]k[2][(1 +][ ζ][k][)][η][2][E][∥∇][F][k][(][X]k[r,s][)][∥][2][ +][ η][2][ρ]k[2][nσ][2][,]_

where ζk > 0 is some free parameter to be properly chosen. Next, we bound the norm of the
pseudo-gradient ∇Fk(Xk[r,s][)][.]

_Fk(Xk[r,s][)][∥][2][ =]_ _fi(x[r,s]i_ [)][∥][2]
_∥∇_ _∥∇_

_iX∈Vk_

= _∥∇fi(x[r,s]i_ [)][ ± ∇][f][i][(¯]x[r,s]k [)][ ± ∇]f[¯]k(¯x[r,s]k [)][ ± ∇]f[¯]k(¯x[r,s])∥[2]

_iX∈Vk_

_≤_ (4∥∇fi(x[r,s]i [)][ −∇][f][i][(¯]x[r,s]k [)][∥][2][ + 4][∥∇][f][i][(¯]x[r,s]k [)][ −∇]f[¯]k(¯x[r,s]k [)][∥][2][ + 4][∥∇]f[¯]k(¯x[r,s]k [)][ −∇]f[¯]k(¯x[r,s])∥[2])

_iX∈Vk_

+ 4 _fk(¯x[r,s])_ (145)

_∥∇_ [¯] _∥[2]_
_iX∈Vk_

_≤_ 4∥∇fi(¯x[r,s]k [)][ −∇]f[¯]k(¯x[r,s]k [)][∥][2][ + 4][L][2][∥][x]i[r,s] _−_ _x¯[r,s]k_ _x[r,s]k_ _−_ _x¯[r,s]∥[2]_ + 4∥∇f[¯]k(¯x[r,s])∥[2][]

_iX∈Vk_   _[∥][2][ + 4][L][2][∥][¯]_

4L[2] _Xk,[r,s]_ _x[r,s]k_ _x¯[r,s]_ + 4n _fk(¯x[r,s])_ + 4nϵ[2]k[.]
_≤_ _∥_ _⊥[∥][2][ + 4][L][2][n][∥][¯]_ _−_ _∥[2]_ _∥∇_ [¯] _∥[2]_

The last inequality is due to Assumption 4 on the intra-cluster heterogeneity.

Averaging over k = 1, . . ., K clusters:

_K_

1

_N_ E∥Xk,[r,s]⊥[+1]∥[2]

_k=1_

X


_≤_ _N[1]_

_≤_ _N[1]_


_K_

_ρ[2]k[(1 +][ ζ]k[−][1][)][E][∥][X]k,[r,s]_
_⊥[∥][2][ + 1]N_
_k=1_

X


_ρ[2]k[(1 +][ ζ][k][)][η][2][E][∥∇][F][k][(][X]k[r,s][)][∥][2][ +][ η][2]_
_k=1_

X


_ρ[2]k_
_k=1_

X


_σ[2]_


_K_

_ρ[2]k[(1 +][ ζ]k[−][1][)][E][∥][X]k,[r,s]_
_⊥[∥][2][ +][ η][2][ 1]N_
_k=1_

X


_ρ[2]k[(1 +][ ζ][k][)][ ·][ 4][L][2][E][∥][X]k,[r,s]⊥[∥][2]_
_k=1_

X


+ η[2][ 1]

_K_

+ η[2][ 1]


_K_

_ρ[2]k[(1 +][ ζ][k][)4][L][2][E][∥]x[¯][r,s]k_ _−_ _x¯[r,s]∥[2]_ + η[2][ 1]K
_k=1_

X


_ρ[2]k[(1 +][ ζ][k][)4][E][∥∇]f[¯]k(¯x[r,s])∥[2]_
_k=1_

X


_ρ[2]k[(1 +][ ζ][k][)4][ϵ][2]k_ [+][ η][2][ρ]max[2] _[σ][2]_ (146)
_k=1_

X


-----

1

_N_




max _k[(1 +][ ζ]k[−][1][) +][ η][2][ ·][ 4][L][2][ max]_
_k_ [K] _[ρ][2]_ _k_ [K]
_∈_ _∈_


_ρ[2]k[(1 +][ ζ][k][)]_


E _Xk,[r,s]_
_∥_ _⊥[∥][2]_
_k=1_

X


_ρL_

_K_

{z }

E _x¯[r,s]k_ _x¯[r,s]_
_∥_ _−_ _∥[2]_
_k=1_

X


_ρ[2]k[(1 +][ ζ][k][)]_ 4L[2][ 1]

_·_ _K_


+ η[2] max
_k∈[K]_

+ η[2] max
_k∈[K]_


_ρ[2]k[(1 +][ ζ][k][)]_ 4(α[2]E _f_ (¯x[r,s]) + ϵ[2]g[) +][ η][2][ 1]

_·_ _∥∇_ _∥[2]_ _K_


_ρ[2]k[(1 +][ ζ][k][)4][ϵ][2]k_ [+][ η][2][ρ]max[2] _[σ][2][.]_
_k=1_

X


D.4 PROOF OF LEMMA 4

To simplify the notation we omit the superscript in B[r,s][−][ℓ] in this section.

Let Λ = diag(λ1, λ2) and the eigendecomposition of G = T ΛT, we can obtain the closed form

_[−][1]_
expression of T as

_T =_ 12λ1τη −[2]CLτ[2] 12λ2τη −[2]CLτ[2]
 

and


12τη[2]L[2] _−λ2 + Cτ_
12τη[2]L[2] _λ1_ _Cτ_
_−_ _−_


_T_ _[−][1]_ =


det(T )


where

Consequently


det(T ) = 12τη[2]L[2](λ1 _λ2)._ (147)
_−_


det(T ) · G[ℓ]B = det(T ) · T Λ[ℓ]T _[−][1]B_

_λ1_ _Cτ_ _λ2_ _Cτ_ _λ[ℓ]1_ 0 12τη[2]L[2] _λ2 + Cτ_ _b1_
12τη −[2]L[2] 12τη −[2]L[2] 0 _λ[ℓ]2_ 12τη[2]L[2] _−λ1_ _Cτ_ _b2_
    − _−_  

_λ1_ _Cτ_ _λ2_ _Cτ_ _λ[ℓ]1_ 0 12τη[2]L[2]b1 + ( _λ2 + Cτ_ )b2
12τη −[2]L[2] 12τη −[2]L[2] 0 _λ[ℓ]2_ 12τη[2]L[2]b1 + (−λ1 _Cτ_ )b2
    − _−_ 

_λ1_ _Cτ_ _λ2_ _Cτ_ _λ[ℓ]1[12][τη][2][L][2][b][1]_ [+][ λ][ℓ]1[(][−][λ][2] [+][ C][τ] [)][b][2]
12τη −[2]L[2] 12τη −[2]L[2] _λ[ℓ]2[12][τη][2][L][2][b][1]_ [+][ λ][ℓ]2[(][λ][1]
  − _[−]_ _[C][τ]_ [)][b][2]

_t1_
_t2_
 


(148)


with
_t1 = (λ1 −_ _Cτ_ ) _λ[ℓ]1[12][τη][2][L][2][b][1]_ [+][ λ][ℓ]1[(][−][λ][2] [+][ C][τ] [)][b][2] (149)

+ (λ2  Cτ ) _λ[ℓ]2[12][τη][2][L][2][b][1]_ [+][ λ][ℓ]2[(][λ][1]  _,_
_−_ _−_ _[−]_ _[C][τ]_ [)][b][2]

_t2 = 12τη[2]L[2][  ]λ[ℓ]1[12] [τη][2][L][2][b][1]_ [+][ λ][ℓ]1[(][−][λ][2] [+][ C][τ] [)][b][2] 

(150)
+ 12τη[2]L[2][  ] _λ[ℓ]2[12][τη][2][L][2][b][1]_ [+][ λ][ℓ]2[(][λ][1]  _._
_−_ _[−]_ _[C][τ]_ [)][b][2]

Therefore 
det(T )(1, 1)T Λ[ℓ]T _[−][1]B = t1 + t2_
= (λ1 _Cτ_ ) _λ[ℓ]1[L][2][12][τη][2][b][1]_ [+][ λ][ℓ]1[(][−][λ][2] [+][ C][τ] [)][b][2]
_−_

+ (λ2 _C τ_ ) _λ[ℓ]2[L][2][12][τη][2][b][1]_ [+][ λ][ℓ]2[(][λ][1]  (151)
_−_ _−_ _[−]_ _[C][τ]_ [)][b][2]

+ L[2]12τη[2][  ]λ [ℓ]1[L][2][12][τη][2][b][1] [+][ λ][ℓ]1[(][−][λ][2] [+][ C][τ] [)][b][2] 

+ L[2]12τη[2][  ] _λ[ℓ]2[L][2][12][τη][2][b][1]_ [+][ λ][ℓ]2[(][λ][1]  _._
_−_ _[−]_ _[C][τ]_ [)][b][2]

Substituting the expression of det(T ) and dividing both sides of the equality by 12τη[2](λ1 _λ2) we_
have _−_

_L[2](1, 1)T_ Λ[ℓ]T _[−][1]B_


-----

1
= _λ[ℓ]1[L][2][12][τη][2][b][1]_ [+][ λ][ℓ]1[(][−][λ][2] [+][ C][τ] [)][b][2]

12τη[2](λ1 _λ2) [(][λ][1][ −]_ _[C][τ]_ [)]
_−_

1   
+ _λ[ℓ]2[L][2][12][τη][2][b][1]_ [+][ λ][ℓ]2[(][λ][1]

12τη[2](λ1 − _λ2) [(][λ][2][ −]_ _[C][τ]_ [)] _−_ _[−]_ _[C][τ]_ [)][b][2]

1   
+ _λ[ℓ]1[L][2][12][τη][2][b][1]_ [+][ λ][ℓ]1[(][−][λ][2] [+][ C][τ] [)][b][2]

(λ1 _λ2)_ _[L][2][  ]_
_−_

1 
+ _λ[ℓ]2[L][2][12][τη][2][b][1]_ [+][ λ][ℓ]2[(][λ][1]

(λ1 − _λ2)_ _[L][2][  ]−_ _[−]_ _[C][τ]_ [)][b][2]

1 1 
= 1[L][2][b][1] [+] _−_ 2[L][2][b][1]

(λ1 _λ2) [(][λ][1][ −]_ _[C][τ]_ [)][ λ][ℓ] (λ1 _λ2) [(][λ][2][ −]_ _[C][τ]_ [)][ λ][ℓ]
_−_ _−_

1 1
+ 1[12][τη][2][b][1] 2[12][τη][2][b][1]

(λ1 _λ2)_ _[L][4][λ][ℓ]_ _[−]_ (λ1 _λ2)_ _[L][4][λ][ℓ]_
_−_ _−_

1 1
+ 1[(][−][λ][2] [+][ C][τ] [)][b][2] [+] 2[(][λ][1]

12τη[2](λ1 − _λ2) [(][λ][1][ −]_ _[C][τ]_ [)][ λ][ℓ] 12τη[2](λ1 − _λ2) [(][λ][2][ −]_ _[C][τ]_ [)][ λ][ℓ] _[−]_ _[C][τ]_ [)][b][2]

1 1
+ 1[(][−][λ][2] [+][ C][τ] [)][b][2] [+] 2[(][λ][1]

(λ1 − _λ2)_ _[L][2][λ][ℓ]_ (λ1 − _λ2)_ _[L][2][λ][ℓ]_ _[−]_ _[C][τ]_ [)][b][2]

1 1
= _λ[ℓ]2[(][λ][2]_ 1[(][C][τ] _L[2]b1 +_ 1 2[)][L][4][b][1]

(λ1 _λ2)_ _−_ _[−]_ _[C][τ]_ [)][ −] _[λ][ℓ]_ _[−]_ _[λ][1][)]_ (λ1 _λ2)_ [12][τη][2][(][λ][ℓ] _[−]_ _[λ][ℓ]_
_−_ _−_

1   1
+ 1[(][−][λ][2] [+][ C][τ] [)][b][2] [+] 2[(][λ][1]

12τη[2](λ1 − _λ2) [(][λ][1][ −]_ _[C][τ]_ [)][ λ][ℓ] 12τη[2](λ1 − _λ2) [(][λ][2][ −]_ _[C][τ]_ [)][ λ][ℓ] _[−]_ _[C][τ]_ [)][b][2]

1 1
+ 1[(][−][λ][2] [+][ C][τ] [)][b][2] [+] 2[(][λ][1]

(λ1 − _λ2)_ _[L][2][λ][ℓ]_ (λ1 − _λ2)_ _[L][2][λ][ℓ]_ _[−]_ _[C][τ]_ [)][b][2]

_λ[ℓ]2[L][2][b][1]_ [+][ λ]2[ℓ] _[−]_ _[λ]1[ℓ]_ 12τη[2]L[4]b1 + _[λ]2[ℓ]_ _[−]_ _[λ]1[ℓ]_ (λ2 _Cτ_ )(Cτ _λ1)_ 1 2[L][2][b][2][,] (152)
_≤_ _λ2_ _λ1_ _·_ _λ2_ _λ1_ _−_ _−_ 12τη[2][ b][2][ +][ λ][ℓ]

_−_ _−_

where in the last inequality we used the fact that λ1 _Cτ_ _λ2._
_≤_ _≤_

Note that


(λ2 _Cτ_ )(Cτ _λ1)_
_−_ _−_

= _Cτ[2]_ [+ (][λ][1] [+][ λ][2][)][C][τ]
_−_ _[−]_ _[λ][1][λ][2]_
= _Cτ[2]_
_−_ _[−]_ [det(][G][) +][ Tr][(][G][)][C][τ]
= _Cτ[2]_ _Cτ_ (max _k[(1 +][ ζ]k[−][1][) +][ η][2][ρ][L][ ·][ 4][L][2][)][ −]_ [48][ρ][L][τη][4][L][4][]
_−_ _[−]_ _k∈[K]_ _[ρ][2]_
 

+ Cτ (max _k[(1 +][ ζ]k[−][1][) +][ η][2][ρ][L][ ·][ 4][L][2][ +][ C][τ]_ [)]
_k_ [K] _[ρ][2]_
_∈_

= 48ρLτη[4]L[4].

Therefore, we further obtain


(153)


_L[2](1, 1)T_ Λ[ℓ]T _[−][1]B_

(154)

_λ[ℓ]2[L][2][(][b][1]_ [+][ b][2][) +][ λ]2[ℓ] _[−]_ _[λ]1[ℓ]_ _η[2]_ 12τL[4]b1 + 4ρLL[4]b2
_≤_ _λ2_ _λ1_ _·_

_−_
  

Dividing both sides by L[2] completes the proof.

E NETWORK CONNECTIVITY CONDITIONS IN THEOREMS AND COROLLARY

Both Theorem 2 and Corollary 1 impose some sufficient conditions on the network connectivity
_ρmax for convergence. This can be satisfied in practice as follows. For Theorem 2, as long as_
_ρmax < 1, we can choose τ large enough so that (7) is fulfilled. Corollary 1 strengthens the result of_
Theorem 2 by requiring no loss in the order of convergence rate compared to full device participation. This naturally leads to a more stringent condition on ρmax given by (11). For any given D2D
network topology, this can be satisfied by running multiple D2D gossip averaging steps per SGD


-----

update in Algorithm 1. Since the right hand side of (11) depends only on the algorithmic parameters, we can choose the suitable gossip averaging steps to fulfill this condition before launching the
algorithm.

F MORE EXPERIMENTAL DETAILS

In this section, we provide additional experimental results on CIFAR-10 dataset. We follow the
same CNN model and non-iid data partition strategy as before and run each experiments for 3 times
with different random seeds to report the mean values of best test accuracy. Instead of using a
constant learning rate, we decay the local learning rate η by half after finishing 50% and 75% of the
communication rounds and tune the initial learning rate from {0.01, 0.02, 0.05, 0.08, 0.1} for each
algorithm.

(a) (b)

Figure 4: Convergence rate and runtime comparisons of HL-SGD and local SGD on CIFAR-10 under ER
random D2D network topology. The device sampling ratio p = 1/8 and local iteration period τ = 50.

(a) (b)

Figure 5: Convergence rate and runtime comparisons of HL-SGD and local SGD on CIFAR-10 under ring
topology and multiple SGD updates before gossip averaging. The device sampling ratio p = 1, and the local
iteration period τ = 50.

First, we evaluate the convergence processes of HL-SGD and local SGD under varying D2D network
topologies in Figure 4. We generate random network topologies by Erd˝os-R´enyi model with edge
probability from 0.2, 0, 5, 0.8, 1 and use Metropolis-Hastings weights to set Wk, corresponding to
_{_ _}_
spectral norm ρmax = {0.9394, 0.844, 0.5357, 0}. As observed in Figure 4a, a more connected D2D
network topology (i.e., a smaller value of ρmax) generally accelerates the convergence and leads to
a higher model accuracy achieved over 100 communication rounds in HL-SGD. However, in terms
of runtime, a more connected D2D network topology corresponds to a larger D2D communication
delay cd2d per round, and hence the total runtime is larger as well, which can be clearly observed
in Figure 4b. Therefore, to achieve a target level of model accuracy within the shortest time in HLSGD, a sparse D2D network topology could work better than the fully connected one in practice.


-----

Second, we consider an extension of HL-SGD by allowing each device to perform multiple SGD
updates before the gossip averaging step in Algorithm 1 and empirically evaluate its performance.
Specifically, each device performs l = {1, 5, 10} steps of SGD update before aggregating models
with their neighbors in the same cluster. Note that l = 1 corresponds to the original version of HLSGD in Algorithm 1. As observed in Figure 5a, when communicating and aggregating models with
neighbors more frequently, HL-SGD with l = 1 has the best convergence speed and will converge
to the highest level of test accuracy. In terms of runtime, choosing a value of l > 1 might be
favorable in some cases due to the reduced D2D communication delay per round. For instance, to
achieve a target level of 60% test accuracy, HL-SGD with l = 5 needs 5.22% less amount of time
than l = 1. It is an interesting direction to rigorously analyze the convergence properties of HLSGD with arbitrary l and find the best hyperparameter tuning method for minimizing the runtime to
achieve a target level of model accuracy in the future.

G RELATIONSHIP BETWEEN SPECTRAL GAP AND NETWORK TOPOLOGY

ring/path 2D-grid 2-D torus Erd˝os-R´enyi exponential

1 − _ρ_ _O(1/n[2])_ _O(1/(n log n))_ _O(1/n)_ _O(1)_ _O(1/ log(n))_

Table 2: Spectral gap of some commonly used graphs, where n denotes the number of nodes. Results are taken
from (Nedi´c et al. (2018); Ying et al. (2021)).


-----

