# UNDERSTANDING INTRINSIC ROBUSTNESS USING LABEL UNCERTAINTY


**Xiao Zhang**
Department of Computer Science
University of Virginia
shawn@virginia.edu


**David Evans**
Department of Computer Science
University of Virginia
evans@virginia.edu

ABSTRACT


A fundamental question in adversarial machine learning is whether a robust classifier exists for a given task. A line of research has made some progress towards this
goal by studying the concentration of measure, but we argue standard concentration
fails to fully characterize the intrinsic robustness of a classification problem since
it ignores data labels which are essential to any classification task. Building on a
novel definition of label uncertainty, we empirically demonstrate that error regions
induced by state-of-the-art models tend to have much higher label uncertainty than
randomly-selected subsets. This observation motivates us to adapt a concentration
estimation algorithm to account for label uncertainty, resulting in more accurate
intrinsic robustness measures for benchmark image classification problems.

1 INTRODUCTION

Since the initial reports of adversarial examples against deep neural networks (Szegedy et al., 2014;
Goodfellow et al., 2015), many defensive mechanisms have been proposed aiming to enhance the
robustness of machine learning classifiers. Most have failed, however, against stronger adaptive
attacks (Athalye et al., 2018; Tramer et al., 2020). PGD-based adversarial training (M ˛adry et al., 2018)
and its variants (Zhang et al., 2019; Carmon et al., 2019) are among the few heuristic defenses that
have not been broken so far, but these methods still fail to produce satisfactorily robust classifiers, even
for classification tasks on benchmark datasets like CIFAR-10. Motivated by the empirical hardness
of adversarially-robust learning, a line of theoretical works (Gilmer et al., 2018; Fawzi et al., 2018;
Mahloujifar et al., 2019a; Shafahi et al., 2019) have argued that adversarial examples are unavoidable.
In particular, these works proved that as long as the input distributions are concentrated with respect
to the perturbation metric, adversarially robust classifiers do not exist. Recently, Mahloujifar et al.
(2019b) and Prescott et al. (2021) generalized these results by developing empirical methods for
measuring the concentration of arbitrary input distributions to derive an intrinsic robustness limit.
(Appendix A provides a more thorough discussion of related work.)

We argue that the standard concentration of measure problem, which was studied in all of the aforementioned works, is not sufficient to capture a realistic intrinsic robustness limit for a classification
problem. In particular, the standard concentration function is defined as an inherent property regarding
the input metric probability space that does not take account of the underlying label information.
We argue that such label information is essential for any supervised learning problem, including
adversarially robust classification, so must be incorporated into intrinsic robustness limits.

**Contributions. We identify the insufficiency of the standard concentration of measure problem and**
demonstrate why it fails to capture a realistic intrinsic robustness limit (Section 3). Then, we introduce
the notion of label uncertainty (Definition 4.1), which characterizes the average uncertainty of label
assignments for an input region. We then incorporate label uncertainty in the standard concentration
measure as an initial step towards a more realistic characterization of intrinsic robustness (Section 4).
Experiments on the CIFAR-10 and CIFAR-10H (Peterson et al., 2019) datasets demonstrate that error
regions induced by state-of-the-art classification models all have high label uncertainty (Section 6.1),
which validates the proposed label uncertainty constrained concentration problem.


-----

(a) ℓ perturbations (b) ℓ2 perturbations
_∞_

Figure 1: Intrinsic robustness estimates for classification tasks on CIFAR-10 under (a) ℓ pertur_∞_
bations with ϵ = 8/255 and (b) ℓ2 perturbations with ϵ = 0.5. Orange dots are intrinsic robustness
estimates using the method in Prescott et al. (2021), which does not consider labels; green dots
show the results using our methods that incorporate label uncertainty; blue dots are results achieved
by the state-of-the-art adversarially-trained models in RobustBench (Croce et al., 2020). Three
fundamental causes behind the adversarial vulnerability can be summarized as imperfect risk (red
region), concentration of measure (orange region) and existence of uncertain inputs (green region).

By adapting the standard concentration estimation method in Mahloujifar et al. (2019b), we propose an
empirical estimator for the label uncertainty constrained concentration function. We then theoretically
study the asymptotic behavior of the proposed estimator and provide a corresponding heuristic
algorithm for typical perturbation metrics (Section 5). We demonstrate that our method is able to
produce a more accurate characterization of intrinsic robustness limit for benchmark datasets than
was possible using prior methods that do not consider labels (Section 6.2). Figure 1 illustrates the
intrinsic robustness estimates resulting from our label uncertainty approach on two CIFAR-10 robust
classification tasks. The intrinsic robustness estimates we obtain by incorporating label uncertainty
are much lower than prior limits, suggesting that compared with the concentration of measure
phenomenon, the existence of uncertain inputs may explain more fundamentally the adversarial
vulnerability of state-of-the-art robustly-trained models. In addition, we also provide empirical
evidence showing that both the clean and robust accuracies of state-of-the-art robust classification
models are largely affected by the label uncertainty of the tested examples, suggesting that adding an
abstain option based on label uncertainty is a promising avenue for improving adversarial robustness
of deployed machine learning systems (Section 6.3).

**Notation.set**, We use denotes its cardinality, [k] to denote {1 pow, 2, . . ., k( ) is all its measurable subsets, and} and use In to denote the n × n identity matrix. For any 1 ( ) is the indicator
_A_ _|A|_ _A_ _A_ _·_
function of . Consider metric probability space ( _, µ, ∆), where ∆:_ R 0 is a distance
_A_ _X_ _X × X →_ _≥_
metric on X . Define the empirical measure of µ with respect to a data set S sampled from µ as
_µS_ (A) = **_x∈S_** [1][A][(][x][)][/][|S|][. Denote by][ B][ϵ][(][x][,][ ∆)][ the ball around][ x][ with radius][ ϵ][ measured by][ ∆][.]

The ϵ-expansion of is defined as _ϵ(∆) =_ **_x_** : **_x[′]_** _ϵ(x, ∆)_ . When ∆ is free of
_A_ _A_ _{_ _∈X_ _∃_ _∈B_ _∩A}_
context, we simply writeb [P] _ϵ(x) =_ _ϵ(x, ∆) and_ _ϵ =_ _ϵ(∆)._
_B_ _B_ _A_ _A_

2 PRELIMINARIES

**Adversarial Risk. Adversarial risk captures the vulnerability of a classifier against adversarial**
perturbations. In particular, we adopt the following adversarial risk definition, which has been studied
in several previous works, such as Gilmer et al. (2018); Bubeck et al. (2019); Mahloujifar et al.
(2019a;b); Zhang et al. (2020b); Prescott et al. (2021).
**Definition 2.1 (Adversarial Risk). Let (X** _, µ, ∆) be a metric probability space of instances and Y be_
the set of possible class labels. Assume c : X →Y is a concept function that gives each instance a
label. For any classifier f : X →Y and ϵ ≥ 0, the adversarial risk of f is defined as:

AdvRiskϵ(f, c) = Pr **_x[′]_** _ϵ(x) s.t. f_ (x[′]) = c(x[′]) _._
**_x_** _µ_ _∃_ _∈B_ _̸_
_∼_

The adversarial robustness of f is defined as: AdvRobϵ(f, c) = 1 AdvRisk _ϵ(f, c)._
_−_


-----

When ϵ = 0, adversarial risk equals to the standard risk. Namely, AdvRisk0(f, c) = Risk(f, c) :=
Prx∼µ[f (x) ̸= c(x)] holds for any classifier f . Other definitions of adversarial risk have been
proposed, such as the one used in M ˛adry et al. (2018). These definitions are equivalent to the one we
use, as long as small perturbations preserve the labels assigned by c(·).

**Intrinsic Robustness. The definition of intrinsic robustness was first introduced by Mahloujifar et al.**
(2019b) to capture the maximum adversarial robustness with respect to some set of classifiers:
**Definition 2.2 (Intrinsic Robustness). Consider the input metric probability space (X** _, µ, ∆) and the_
set of labels Y. Let c : X →Y be a concept function that gives a label to each input. For any set of
classifiers F ⊆{f : X →Y} and ϵ ≥ 0, the intrinsic robustness with respect to F is defined as:

AdvRobϵ( _, c) = 1_ inf AdvRiskϵ(f, c) = sup AdvRobϵ(f, c) _._
_F_ _−_ _f_ _∈F_ _f_ _∈F{_ _}_


According to the definition of intrinsic robustness, there does not exist any classifier in F with
adversarial robustness higher than AdvRobϵ( _, c) for the considered task. Prior works, including_
_F_
Gilmer et al. (2018); Mahloujifar et al. (2019a;b); Zhang et al. (2020b), selected F in Definition 2.2
as the set of imperfect classifiers Fα = {f : Risk(f, c) ≥ _α}, where α ∈_ (0, 1) is set as a small
constant that reflects the best classification error rates achieved by state-of-the-art methods.

**Concentration of Measure. Concentration of measure captures a ‘closeness’ property for a metric**
probability space of instances. More formally, it is defined by the concentration function:
**Definition 2.3 (Concentration Function). Let (X** _, µ, ∆) be a metric probability space. For any α ∈_
(0, 1) and ϵ ≥ 0, concentration function is defined as: h(µ, α, ϵ) = inf _E∈pow(X_ ){µ(Eϵ) : µ(E) ≥ _α}._

The standard notion of concentration function considers a special case of Definition 2.3 with α = 1/2
(e.g., Talagrand (1995)). For some special metric probability spaces, one can prove the closed-form
solution of the concentration function. The Gaussian Isoperimetric Inequality (Borell, 1975; Sudakov
& Tsirelson, 1974) characterizes the concentration function for spherical Gaussian distribution and
_ℓ2-norm distance metric, and was generalized by Prescott et al. (2021) to other ℓp norms._

3 STANDARD CONCENTRATION IS INSUFFICIENT

We first explain a fundamental connection between the concentration of measure and the intrinsic
robustness with respect to imperfect classifiers shown in previous work, and then argue that standard
concentration fails to capture a realistic intrinsic robustness limit because it ignores data labels.

**Connecting Intrinsic Robustness with Concentration of Measure. Let (X** _, µ, ∆) be the consid-_
ered input metric probability space, Y be the set of possible labels, and c : X →Y be the concept
function that gives each input a label. Given parameters 0 < α < 1 and ϵ ≥ 0, the standard
concentration problem can be cast into an optimization problem as follows:

minimize subject to _µ(_ ) _α._ (3.1)
pow( ) _[µ][(][E][ϵ][)]_ _E_ _≥_
_E∈_ _X_

For any classifier f, let Ef = {x ∈X : f (x) ̸= c(x)} be its induced error region with respect to c(·).
By connecting the risk of f with the measure of Ef and the adversarial risk of f with the measure
of the ϵ-expansion of _f_, Mahloujifar et al. (2019a) proved that the standard concentration problem
_E_
(3.1) is equivalent to the following optimization problem regarding risk and adversarial risk:

minimize AdvRiskϵ(f, c) subject to Risk(f, c) _α._
_f_ _≥_

To be more specific, the following lemma characterizes the connection between the standard concentration function and the intrinsic robustness limit with respect to the set of imperfect classifiers:
**Lemma 3.1 (Mahloujifar et al. (2019a)). Let α ∈** (0, 1) and Fα = {f : Risk(f, c) ≥ _α} be the set_
of imperfect classifiers. For any ϵ 0, it holds that AdvRobϵ( _α, c) = 1_ _h(µ, α, ϵ)._
_≥_ _F_ _−_

Lemma 3.1 suggests that the concentration function of the input metric probability space h(µ, α, ϵ)
can be translated into an adversarial robustness upper bound that applies to any classifier with risk at


-----

least α. If this upper bound is shown to be small, then one can conclude that it is impossible to learn
an adversarially robust classifier, as long as the learned classifier has risk at least α.

**Concentration without Labels Mischaracterizes Intrinsic Robustness. Despite the appealing**
relationship between concentration of measure and intrinsic robustness, we argue that solving the
standard concentration problem is not enough to capture a meaningful intrinsic limit for adversarially
robust classification. The standard concentration of measure problem (3.1), which aims to find the
optimal subset that has the smallest ϵ-expansion with regard to the input metric probability space
(X _, µ, ∆), does not involve the concept function c(·) that determines the underlying class label of_
each input. Therefore, no matter how we assign the labels to the inputs, the concentration function
_h(µ, α, ϵ) will remain the same for the considered metric probability space. In sharp contrast, learning_
an adversarially-robust classifier depends on the joint distribution of both the inputs and the labels.

Moreover, when the standard concentration function is translated into an intrinsic limit of adversarial
robustness, it is defined with respect to the set of imperfect classfiers Fα (see Lemma 3.1). The only
restriction imposed by Fα is that the classifier (or equivalently, the measure of the corresponding
error region) has risk at least α. This fails to consider whether the classifier is learnable or not
under the given classification problem. Therefore, the intrinsic robustness limit implied by standard
concentration AdvRobϵ(Fα, c) could be much higher than AdvRobϵ(Flearn, c), where Flearn denotes
the set of classifiers that can be produced by some supervised learning method. Hence, it is not
surprising that Mahloujifar et al. (2019b) found that the adversarial robustness attained by state-of-theart robust training methods for several image benchmarks is much lower than the intrinsic robustness
limit implied by standard concentration of measure. In this work, to obtain a more meaningful
intrinsic robustness limit we restrict the search space of the standard concentration problem (3.1) by
considering both the underlying class labels and the learnability of the given classification problem.

**Gaussian Mixture Model. We further illustrate the insufficiency of standard concentration under**
a simple Gaussian mixture model. Let X ⊆ R[n] be the input space and Y = {−1, +1} be the label
space. Assume all the inputs are first generated according to a mixture of 2-Gaussian distribution:
**_x ∼_** _µ =_ [1]2 _[N]_ [(][−][θ][, σ][2][I][n][) +][ 1]2 _[N]_ [(][θ][, σ][2][I][n][)][, then labeled by a concept function][ c][(][x][) = sgn(][θ][⊤][x][)][,]

where θ ∈ R[n] and σ ∈ R are given parameters (this concept function is also the Bayes optimal
classifier, which best separates the two Gaussian clusters). Theorem 3.2, proven in Appendix C.1,
characterizes the optimal solution to the standard concentration problem under this assumed model.
**Theorem 3.2. Consider the above Gaussian mixture model with ℓ2 perturbation metric. The optimal**
solution to the standard concentration problem (3.1) is a halfspace, either

= **_x_** : θ[⊤]x + b **_θ_** 2 0 or + = **_x_** : θ[⊤]x _b_ **_θ_** 2 0 _,_
_H−_ _{_ _∈X_ _· ∥_ _∥_ _≤_ _}_ _H_ _{_ _∈X_ _−_ _· ∥_ _∥_ _≥_ _}_

where b is a parameter depending on α and θ such that µ(H−) = µ(H+) = α.
**Remark 3.3. Theorem 3.2 suggests that for the Gaussian mixture model, the optimal subset achieving**
the smallest ϵ-expansion under ℓ2-norm distance metric is a halfspace, which is far away from the
_H_
boundary between the two Gaussian classes for small α. When translated into the intrinsic robustness
problem, the corresponding optimal classifier f has to be constructed by treating H as the only error
region, or more precisely f (x) = c(x) if x /∈H; f (x) ̸= c(x) otherwise. This optimally constructed
classifier f, however, does not match our intuition of what a predictive classifier would do under
the considered Gaussian mixture model. In particular, since all the inputs in H and their neighbours
share the same class label and are also far away from the boundary, examples that fall into H should
be easily classified correctly using simple decision rule, such as k-nearest neighbour or maximum
margin, whereas examples that are close to the boundary should be more likely to be misclassified as
errors by supervisedly-learned classifiers. This confirms our claim that standard concentration is not
sufficient for capturing a meaningful intrinsic robustness limit.

4 INCORPORATING LABEL UNCERTAINTY IN INTRINSIC ROBUSTNESS

In this section, we first propose a new concentration estimation framework by imposing a constraint
based on label uncertainty (Definition 4.1) on the search space with respect to the standard problem
(3.1). Then, we explain why this yields a more realistic intrinsic robustness limit.

Let (X _, µ) be the input probability space and Y = {1, 2, . . ., k} be the set of labels. η : X →_ [0, 1][k]
is said to capture the full label distribution (Geng, 2016; Gao et al., 2017), if [η(x)]y corresponds


-----

to the description degree of y to x for any x ∈X and y ∈Y, and _y∈[k][[][η][(][x][)]][y][ = 1][ holds for any]_

**_x ∈X_** . For classification tasks that rely on human labeling, one can approximate the label distribution
for any input by collecting human labels from multiple human annotators. Our experiments use the

[P]

CIFAR-10H dataset that did this for the CIFAR-10 test images (Peterson et al., 2019).

For any subset E ∈ pow(X ), we introduce label uncertainty to capture the average uncertainty level
with respect to the label assignments of the inputs within E:
**Definition 4.1 (Label Uncertainty). Let (X** _, µ) be the input probability space and Y = {1, 2, . . ., k}_
be the complete set of class labels. Suppose c : X →Y is a concept function that assigns each input
**_x a label y_** . Assume η : [0, 1][k] is the underlying label distribution function, where [η(x)]y
_∈Y_ _X →_
represents the description degree of y to x. For any subset E ∈ pow(X ) with measure µ(E) > 0, the
_label uncertainty (LU) of E with respect to (X_ _, µ), c(·) and η(·) is defined as:_


LU(E; µ, c, η) =


1 − _η(x)_



_c(x)_ [+ max]y[′]≠ _c(x)_


_η(x)_


_dµ._
_y[′]_
o


_µ(E)_


We define LU(E; µ, c, η) as the average label uncertainty for all the examples that fall into E, where
1 − [η(x)]c(x) + maxy′≠ _c(x)[η(x)]y′ represents the label uncertainty of a single example {x, c(x)}._
The range of label uncertainty is [0, 2]. For a single input, label uncertainty of 0 suggests the assigned
label fully captures the underlying label distribution; label uncertainty of 1 means there are other
classes as likely to be the ground-truth label as the assigned label; label uncertainty of 2 means the
input is mislabeled and there is a different label that represents the ground-truth label. Based on the
notion of label uncertainty, we study the following constrained concentration problem:
minimize subject to _µ(_ ) _α and LU(_ ; µ, c, η) _γ,_ (4.1)
pow( ) _[µ][(][E][ϵ][)]_ _E_ _≥_ _E_ _≥_
_E∈_ _X_

where γ ∈ [0, 2] is a constant. When γ is set as zero, (4.1) simplifies to the standard concentration of
measure problem. In this work, we set the value of γ to roughly represent the label uncertainty of the
error region of state-of-the-art classifiers for the given classification problem.

Theorem 4.2, proven in Appendix C.2, shows how (4.1) captures the intrinsic robustness limit with
respect to the set of imperfect classifiers whose error region label uncertainty is at least γ.
**Theorem 4.2. Define Fα,γ = {f : Risk(f, c) ≥** _α, LU(Ef_ ; µ, c, η) ≥ _γ}, where α ∈_ (0, 1),
_γ ∈_ (0, 2) and Ef = {x ∈X : f (x) ̸= c(x)} is the error region of f . For any ϵ ≥ 0, it holds that

inf
pow( )[{][µ][(][E][ϵ][) :][ µ][(][E][)][ ≥] _[α,][ LU(][E][;][ µ, c, η][)][ ≥]_ _[γ][}][ = 1][ −]_ [AdvRob][ϵ][(][F][α,γ][, c][)][.]
_E∈_ _X_

Compared with standard concentration, (4.1) aims to search for the least expansive subset with
respect to input regions with high label uncertainty. According to Theorem 4.2, the translated intrinsic
robustness limit is defined with respect to Fα,γ and is guaranteed to be no greater than AdvRobϵ(Fα).
Although both AdvRobϵ( _α) and AdvRobϵ(_ _α,γ) can serve as valid robustness upper bounds for_
_F_ _F_
any f _α,γ, the latter one would be able to capture a more meaningful intrinsic robustness limit,_
_∈F_
since state-of-the-art classifiers are expected to more frequently misclassify inputs with large label
uncertainty, as there is more discrepancy between their assigned labels and the underlying label
distribution (Section 6 provides supporting empirical evidence for this on CIFAR-10).

**Need for Soft Labels. The proposed approach requires label uncertainty information for training**
examples. The CIFAR-10H dataset provided soft labels from humans that enabled our experiments,
but typical machine learning datasets do not provide such information. Below, we discuss possible
avenues to estimating label uncertainty when human soft labels are not available and are too expensive
to acquire. A potential solution is to estimate the set of examples with high label uncertainty using the
predicted probabilities of a classification model. Confident learning (Natarajan et al., 2013; Lipton
et al., 2018; Huang et al., 2019; Northcutt et al., 2021b;a) provides a systematic method to identify
label errors in a dataset based on this idea. If the estimated label errors match the examples with high
human label uncertainty, then we can directly extend our framework by leveraging the estimated error
set. Our experiments on CIFAR-10 (see Appendix G), however, suggest that there is a misalignment
between human recognized errors and errors produced by confident learning. The existence of such
misalignment further suggests that one should be cautious when combining the estimated set of label
errors into our framework. As the field of confident learning advances to produce a more accurate
estimator of label error set, it would serve as a good alternative solution for applying our framework
to the setting where human label information is not accessible.


-----

5 MEASURING CONCENTRATION WITH LABEL UNCERTAINTY CONSTRAINTS

Directly solving (4.1) requires the knowledge of the underlying input distribution µ and the groundtruth label distribution function η(·), which are usually not available for classification problems. Thus,
we consider the following empirical counterpart of (4.1):

minimize _µS_ (Eϵ) subject to _µS_ (E) ≥ _α and LU(E;_ _µS_ _, c,_ _η) ≥_ _γ,_ (5.1)
_E∈G_

where the search space is restricted to some specific collection of subsets pow( ), µ is replaced

b b b b

_G ⊆_ _X_
by the empirical distribution _µ_ with respect to a set of inputs sampled from µ, and the empirical
_S_
label distribution _η(x) is considered as an empirical replacement of η(x) for any given input x ∈S._

Theorem 5.1, proven in Appendix C.3, characterizes a generalization bound regarding the proposed b
label uncertainty estimate. It shows that if b _G is not too complex and_ _η is close to the ground-truth_
label distribution function η, the empirical estimate of label uncertainty LU( ; _µ_ _, c,_ _η) is guaranteed_
_E_ _S_
to be close to the actual label uncertainty LU( ; µ, c, η). The formal definition of the complexity
_E_ b
penalty with respect to a collection of subsets is given in Appendix B.
b b
**Theorem 5.1 (Generalization of Label Uncertainty). Let (X** _, µ) be a probability space and G ⊆_
pow(X ) be a collection of subsets of X . Assume φ : N × R → [0, 1] is a complexity penalty for G. If
_η(_ ) is close to η( ) in L[1]-norm with respect to µ, i.e. _η(x)_ 1dµ _δη, where δη_ (0, 1)

is a small constant, then for any· _·_ _α, δ_ (0, 1) such thatX δ < α[∥][η][(][x], we have[)][ −] [b] _∥_ _≤_ _∈_
_∈_ R

b

Pr and µ( ) _α :_ LU( ; µ, c, η) LU( ; _µ_ _, c,_ _η)_ _φ(m, δ)._
_µ[m]_ _∃E ∈G_ _E_ _≥_ _E_ _−_ _E_ _S_ _≤_ [4]α[δ][ +][ δ]δ[η] _≤_
_S←_  _−_ 

**Remark 5.2. Theorem 5.1 implies the generalization of concentration under label uncertainty b** b
constraints (see Theorem C.3 for a formal argument of this and its proof in Appendix C.5). If
we choose G and the collection of its ϵ-expansions, Gϵ = {Eϵ : E ∈G}, in a careful way that both of
their complexities are small, then with high probability, the empirical label uncertainty constrained
concentration will be close to the actual concentration when the search space is restricted to G.

Moreover, define h(µ, c, η, α, γ, ϵ, G) = inf _E∈G{µ(Eϵ) : µ(E) ≥_ _α, LU(E; µ, c, η) ≥_ _γ} as the_
generalized concentration function under label uncertainty constraints. Then, based on a similar proof
technique used for Theorem 3.5 in Mahloujifar et al. (2019b), we can further show that if G also
satisfies a universal approximation property, then with probability 1,

_h(µ, c, η, α, γ_ _δη/α, ϵ)_ lim _η, α, γ, ϵ,_ (T )) _h(µ, c, η, α, γ + δη/α, ϵ),_ (5.2)
_−_ _≤_ _T_ _G_ _≤_
_→∞_ _[h][(][µ][S][T][, c,][ b]_

where T stands for the complexity of G and ST denotes a set of samples of size m(T ). Appendix
C.4 provides a formal argument and proof for (5.2). It is worth noting that (5.2) suggests that if we
increase both the complexity of the collection of subsets G and the number of samples used for the
empirical estimation, the optimal value of the empirical concentration problem (5.1) will converge to
the actual concentration function with an error limit of δη/α on parameter γ. When the difference
between the empirical label distribution _η(·) and the underlying label distribution η(·) is negligible, it_
is guaranteed that the optimal value of (5.1) asymptoptically converges to that of (4.1).
b

**Concentration Estimation Algorithm. Although Remark 5.2 provides a general idea how to choose**
_G for measuring concentration, it does not indicate how to solve the empirical concentration problem_
(5.1) for a specific perturbation metric. This section presents a heuristic algorithm for estimating the
least-expansive subset for optimization problem (5.1) when the metric is ℓ2-norm or ℓ∞-norm. We
choose as a union of balls for the ℓ2-norm distance metric and set as a union of hypercubes for
_G_ _G_
_ℓ∞-norm (see Appendix B for the formal definition of union of ℓp-balls). It is worth noting that such_
choices of G satisfy the condition required for Theorem C.2, since they are universal approximators
for any set and the VC-dimensions of both G and Gϵ are both bounded (see Eisenstat & Angluin
(2007) and Devroye et al. (2013)).

The remaining task is to solve (5.1) based on the selected G. Following Mahloujifar et al. (2019b),
we place the balls for ℓ2 (or the hypercubes for ℓ ) in a sequential manner, and search for the
_∞_
best placement that satisfies the label uncertainty constraint using a greedy approach. Algorithm 1
in Appendix D gives pseudocode for the search algorithm. It initializes the feasible set of the
hyperparmeters Ω as an empty set for each placement of balls (or hypercubes), then enumerates all


-----

(a) Illustration of CIFAR-10 and CIFAR-10H (b) Label Uncertainty Distribution

Figure 2: (a) Visualization of the CIFAR-10 test images with the soft labels from CIFAR-10H,
the original assigned labels from CIFAR-10 and the label uncertainty scores computed based on
Definition 4.1. (b) Histogram of the label uncertainty distribution for the CIFAR-10 test dataset.

the possible initial placements, init(u, k), such that its empirical label uncertainty exceeds the given
_S_
threshold γ. Finally, among all the feasible ball (or hypercube) placements, it records the one that
has the smallest ϵ-expansion with respect to the empirical measure _µ_ . In this way, the input region
_S_
produced by Algorithm 1 serves as a good approximate solution to the empirical problem (5.1).
b

6 EXPERIMENTS

We conduct experiments on the CIFAR-10H dataset (Peterson et al., 2019), which contains soft labels
reflecting human perceptual uncertainty for the 10,000 CIFAR-10 test images (Krizhevsky & Hinton,
2009). These soft labels can be regarded as an approximation of the label distribution function η(·) at
each given input, whereas the original CIFAR-10 test dataset provides the class labels given by the
concept function c(·). We report on experiments showing the connection between label uncertainty
and classification error rates (Section 6.1) and that incorporating label uncertainty enables better
intrinsic robustness estimates (Section 6.2). Section 6.3 demonstrates the possibility of improving
model robustness by abstaining for inputs in high label uncertainty regions.

6.1 ERROR REGIONS HAVE LARGER LABEL UNCERTAINTY

Figure 2(a) shows the label uncertainty scores for several images with both the soft labels from CIFAR10H and the original class labels from CIFAR-10 (see Appendix F for more illustrations). Images
with low uncertainty scores are typically easier for humans to recognize their class category (first row
of Figure 2(a)), whereas images with high uncertainty scores look ambiguous or even misleading
(second and third rows). Figure 2(b) shows the histogram of the label uncertainty distribution for
all the 10, 000 CIFAR-10 test examples. In particular, more than 80% of the examples have label
uncertainty scores below 0.1, suggesting the original class labels mostly capture the underlying label
distribution well. However, around 2% of the examples have label uncertainty scores exceeding 0.7,
and some 400 images appear to be mislabeled with uncertainty scores above 1.2.

We hypothesize that ambiguous or misleading images should also be more likely to be misclassified
as errors by state-of-the-art machine learning classifiers. That is, their induced error regions should
have larger that typical label uncertainty. To test this hypothesis, we conduct experiments on
CIFAR-10 and CIFAR-10H datasets. More specifically, we train different classification models,
including intermediate models extracted at different epochs, using the CIFAR-10 training dataset, then
empirically compute the standard risk, adversarial risk, and label uncertainty of the corresponding
error region. The results are shown in Figure 3 (see Appendix E for experimental details).

Figures 3(a) and 3(b) demonstrate the relationship between label uncertainty and standard risk for
various classifiers produced by standard training and adversarial training methods under ℓ perturba_∞_
tions with ϵ = 8/255. In addition, we plot the label uncertainty with error bars of randomly-selected
images from the CIFAR-10 test dataset as a reference. As the model classification accuracy increases,
the label uncertainty of its induced error region increases, suggesting the misclassified examples
tend to have higher label uncertainty. This observation holds consistently for both standard and


-----

(a) Standard Training (b) Adversarial Training (c) RobustBench

Figure 3: Visualizations of error region label uncertainty versus standard risk and adversarial risk
with respect to classifiers produced by different machine learning methods: (a) Standard-trained
classifiers with different network architecture; (b) Adversarially-trained classifiers using different
learning algorithms; (c) State-of-the-art adversarially robust classification models from RobustBench.

(a) ℓ perturbations (ϵ = 8/255) (b) ℓ2 perturbations (ϵ = 0.5)
_∞_

Figure 4: Estimated intrinsic robustness based on Algorithm 1 with γ = 0.17 under (a) ℓ pertur_∞_
bations with ϵ = 8/255; and (b) ℓ2 perturbations with ϵ = 0.5. For comparison, we plot baseline
estimates produced without considering label uncertainty using a half-space searching method
(Prescott et al., 2021) and using union of hypercubes or balls (Algorithm 1 with γ = 0). Robust
accuracies achieved by state-of-the-art RobustBench models are plotted in green.

adversarially trained models with any tested network architecture. Figure 3(c) summarizes the error
region label uncertainty with respect to the state-of-the-art adversarially robust models documented
in RobustBench (Croce et al., 2020). Regardless of the perturbation type or the learning method,
the average label uncertainty of their misclassified examples all falls into a range of (0.17, 0.23),
whereas the mean label uncertainty of all the testing CIFAR-10 data is less than 0.1. This supports
our hypothesis that error regions of state-of-the-art classifiers tend to have larger label uncertainty,
and our claim that intrinsic robustness estimates should account for labels.

6.2 EMPIRICAL ESTIMATION OF INTRINSIC ROBUSTNESS

In this section, we apply Algorithm 1 to estimate the intrinsic robustness limit for the CIFAR-10
dataset under ℓ perturbations with ϵ = 8/255 and ℓ2 perturbations with ϵ = 0.5. We set the label
_∞_
uncertainty threshold γ = 0.17 to roughly represent the error region label uncertainty of state-ofthe-art classification models (see Figure 3). In particular, we adopt a 50/50 train-test split over the
original 10, 000 CIFAR-10 test images (see Appendix E for experimental details).

Figure 4 shows our intrinsic robustness estimates with γ = 0.17 when choosing different values of
_α. We include the estimates of intrinsic robustness defined with Fα as a baseline, where no label_
uncertainty constraint is imposed (γ = 0). Results are shown both for our ℓp-balls searching method
and the half-space searching method in Prescott et al. (2021). We also plot the standard error and
the robust accuracy of the state-of-the-art adversarially robust models in RobustBench (Croce et al.,
2020). For concentration estimation methods, the plotted values are the empirical measure of the
returned optimally-searched subset (x-axis) and 1− the empirical measure of its ϵ-expansion (y-axis).


-----

(a) Carmon et al. (2019) (b) Wu et al. (2020)

Figure 5: Accuracy curves for different adversarially-trained classifiers, varying the abstaining ratio
of CIFAR-10 images with high label uncertainty score: (a) Carmon et al. (2019) for ℓ perturbations
_∞_
with ϵ = 8/255; (b) Wu et al. (2020) for ℓ2 perturbations with ϵ = 0.5. Corresponding cut-off values
of label uncertainty are marked on the x-axis with respect to percentage values of {0.02, 0.1, 0.2}.

Compared with the baseline estimates, our label-uncertainty constrained intrinsic robustness estimates
are uniformly lower across all the considered settings (similar results are obtained under other
experimental settings, see Table 1 in Appendix F). Although both of these estimates can serve as
legitimate upper bounds on the maximum achievable adversarial robustness for the given task, our
estimate, which takes data labels into account, being closer to the robust accuracy achieved by stateof-the-art classifiers indicates it is a more accurate characterization of intrinsic robustness limit. For
instance, under ℓ perturbations with ϵ = 8/255, the best adversarially-trained classification model
_∞_
achieves 66% robust accuracy with approximately 8% clean error, whereas our estimate indicates
that the maximum robustness one can hope for is about 82% as long as the classification model has at
least 8% clean error. In contrast, the intrinsic robustness limit implied by standard concentration is as
high as 90% for the same setting, which again shows the insufficiency of standard concentration.

6.3 ABSTAINING BASED ON LABEL UNCERTAINTY

Based on the definition of label uncertainty, and our experimental results in the previous subsections,
we expect classification models to have higher accuracy on examples with low label uncertainty.
Figure 5 shows the results of experiments to study the effect of abstaining based on label uncertainty
on both clean and robust accuracies using adversarially-trained CIFAR-10 classification models from
Carmon et al. (2019) (ℓ∞, ϵ = 8/255) and Wu et al. (2020) (ℓ2, ϵ = 0.5). We first sort all the test
CIFAR-10 images based on label uncertainty, then evaluate the model performance with respect to
different abstaining ratios of top uncertain inputs. The accuracy curves suggest that a potential way
to improve the robustness of classification systems is to enable the classifier an option to abstain on
examples with high uncertainty score.

For example, if we allow the robust classifier of Carmon et al. (2019) to abstain on the 2% of the
test examples whose label uncertainty exceeds 0.7, the clean accuracy improves from 89.7% to
90.3%, while the robust accuracy increases from 59.5% to 60.4%. This is close to the maximum
robust accuracy that could be achieved with a 2% abstention rate (0.595/(1 − 0.02) = 0.607). This
result points to abstaining on examples in high label uncertainty regions as a promising path towards
achieving adversarial robustness.

7 CONCLUSION

Standard concentration fails to sufficiently capture intrinsic robustness since it ignores data labels.
Based on the definition of label uncertainty, we observe that the error regions induced by state-ofthe-art classification models all tend to have high label uncertainty. This motivates us to develop an
empirical method to study the concentration behavior regarding the input regions with high label
uncertainty, which results in more accurate intrinsic robustness measures for benchmark image
classification tasks. Our experiments show the importance of considering labels in understanding
intrinsic robustness, and further suggest that abstaining based on label uncertainty could be a potential
method to improve the classifier accuracy and robustness.


-----

AVAILABILITY

An implementation of our method, and code for reproducing our experiments, is available under an
[open source license from: https://github.com/xiaozhanguva/intrinsic_rob_lu.](https://github.com/xiaozhanguva/intrinsic_rob_lu)

ACKNOWLEDGEMENTS

This work was partially funded by an award from the National Science Foundation (NSF) SaTC
program (Center for Trustworthy Machine Learning, #1804603).

ETHICS STATEMENT

Our work is primarily focused on deepening our understanding of intrinsic adversarial robustness
limit and the main contributions in this paper are theoretical. Our work could potentially enable
construction of more robust classification systems, as suggested by the results in Section 6.3. For
most applications, such as autonomous vehicles and malware detection, improving the robustness of
classifiers is beneficial to society. There may be scenarios, however, such as face recognition where
uncertainty and the opportunity to confuse classifiers with adversarial perturbations may be useful, so
enabling more robust classifiers in these domains may have negative societal impacts.

REPRODUCIBILITY STATEMENT

Details of our experimental setup and methods are provided in Appendix E, and all of the datasets we
use are publicly available. In addition, we state the assumptions for our theoretical results in each
theorem. Detailed proofs of all the presented theorems are provided in Appendix C.

REFERENCES

Jean-Baptiste Alayrac, Jonathan Uesato, Po-Sen Huang, Alhussein Fawzi, Robert Stanforth, and
Pushmeet Kohli. Are labels required for improving adversarial robustness? In NeurIPS, 2019.

Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. In International Conference on Machine
_Learning, 2018._

Arjun Nitin Bhagoji, Daniel Cullina, and Prateek Mittal. Lower bounds on adversarial robustness
from optimal transport. In NeurIPS, 2019.

Christer Borell. The Brunn-Minkowski inequality in Gauss space. Inventiones mathematicae, 30(2):
207–216, 1975.

Sebastien Bubeck, Yin Tat Lee, Eric Price, and Ilya Razenshteyn. Adversarial examples from
computational constraints. In International Conference on Machine Learning, 2019.

Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Unlabeled
data improves adversarial robustness. In NeurIPS, 2019.

Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In International Conference on Machine Learning, 2019.

Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble
of diverse parameter-free attacks. In International Conference on Machine Learning, 2020.

Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Nicolas Flammarion, Mung Chiang,
Prateek Mittal, and Matthias Hein. RobustBench: a standardized adversarial robustness benchmark.
_arXiv preprint arXiv:2010.09670, 2020._

Luc Devroye, László Györfi, and Gábor Lugosi. A Probabilistic Theory of Pattern Recognition.
Springer, 2013.


-----

Elvis Dohmatob. Generalized no free lunch theorem for adversarial robustness. In International
_Conference on Machine Learning, 2019._

David Eisenstat and Dana Angluin. The VC dimension of k-fold union. Information Processing
_Letters, 101(5):181–184, 2007._

Alhussein Fawzi, Hamza Fawzi, and Omar Fawzi. Adversarial vulnerability for any classifier. In
_NeurIPS, 2018._

Bin-Bin Gao, Chao Xing, Chen-Wei Xie, Jianxin Wu, and Xin Geng. Deep label distribution learning
with label ambiguity. IEEE Transactions on Image Processing, 26(6):2825–2838, 2017.

Xin Geng. Label distribution learning. IEEE Transactions on Knowledge and Data Engineering, 28
(7):1734–1748, 2016.

Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S Schoenholz, Maithra Raghu, Martin Wattenberg,
and Ian Goodfellow. Adversarial spheres. arXiv:1801.02774, 2018.

Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In ICLR, 2015.

Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan
Uesato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. Scalable verified training for
provably robust image classification. In International Conference on Computer Vision, 2019.

Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens van der Maaten. Countering adversarial
images using input transformations. In ICLR, 2018.

Jinchi Huang, Lie Qu, Rongfei Jia, and Binqiang Zhao. O2U-Net: A simple noisy label detection
approach for deep neural networks. In International Conference on Computer Vision, 2019.

Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Technical report, University of Toronto, 2009.

Ryen Krusinga, Sohil Shah, Matthias Zwicker, Tom Goldstein, and David Jacobs. Understanding
the (un)interpretability of natural image distributions using generative models. arXiv:1901.01499,
2019.

Michel Ledoux. Isoperimetry and Gaussian analysis. In Lectures on Probability Theory and Statistics.
Springer, 1996.

Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Certified adversarial robustness with
additive noise. In NeurIPS, 2019.

Zachary Lipton, Yu-Xiang Wang, and Alexander Smola. Detecting and correcting for label shift with
black box predictors. In International Conference on Machine Learning, 2018.

Saeed Mahloujifar, Dimitrios Diochnos, and Mohammad Mahmoody. The curse of concentration in
robust learning: Evasion and poisoning attacks from concentration of measure. In AAAI Conference
_on Artificial Intelligence, 2019a._

Saeed Mahloujifar, Xiao Zhang, Mohammad Mahmoody, and David Evans. Empirically measuring
concentration: Fundamental limits on intrinsic robustness. In NeurIPS, 2019b.

Aleksander M ˛adry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In ICLR, 2018.

Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with
noisy labels. In NeurIPS, 2013.

Curtis Northcutt, Anish Athalye, and Jonas Mueller. Pervasive label errors in test sets destabilize
machine learning benchmarks. In NeurIPS (Datasets and Benchmarks Track), 2021a.

Curtis Northcutt, Lu Jiang, and Isaac Chuang. Confident learning: Estimating uncertainty in dataset
labels. Journal of Artificial Intelligence Research, 70:1373–1411, 2021b.


-----

Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a
defense to adversarial perturbations against deep neural networks. In IEEE Symposium on Security
_and Privacy, 2016._

Joshua C Peterson, Ruairidh M Battleday, Thomas L Griffiths, and Olga Russakovsky. Human
uncertainty makes classification more robust. In International Conference on Computer Vision,
2019.

Jack Prescott, Xiao Zhang, and David Evans. Improved estimation of concentration under ℓp-norm
distance metrics using half spaces. In ICLR, 2021.

Aditi Raghunathan, Jacob Steinhardt, and Percy Liang. Certified defenses against adversarial
examples. In ICLR, 2018.

Ali Shafahi, W. Ronny Huang, Christoph Studer, Soheil Feizi, and Tom Goldstein. Are adversarial
examples inevitable? In ICLR, 2019.

Vladimir N Sudakov and Boris S Tsirelson. Extremal properties of half-spaces for spherically
invariant measures. Zapiski Nauchnykh Seminarov Leningrad Otdel Mathematical Institute Steklov
(LOMI), 41:14–24, 1974.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In ICLR, 2014.

Michel Talagrand. Concentration of measure and isoperimetric inequalities in product spaces.
_Publications Mathématiques de l’Institut des Hautes Études Scientifiques, 81(1):73–205, 1995._

Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to
adversarial example defenses. In NeurIPS, 2020.

Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In International Conference on Machine Learning, 2018.

Eric Wong, Frank R Schmidt, Jan Hendrik Metzen, and Zico Kolter. Scaling provable adversarial
defenses. In NeurIPS, 2018.

Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust generalization. In NeurIPS, 2020.

Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille. Mitigating adversarial effects
through randomization. In ICLR, 2018.

Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
Theoretically principled trade-off between robustness and accuracy. In International Conference
_on Machine Learning, 2019._

Huan Zhang, Hongge Chen, Chaowei Xiao, Sven Gowal, Robert Stanforth, Bo Li, Duane Boning,
and Cho-Jui Hsieh. Towards stable and efficient training of verifiably robust neural networks. In
_ICLR, 2020a._

Xiao Zhang, Jinghui Chen, Quanquan Gu, and David Evans. Understanding the intrinsic robustness of
image distributions using conditional generative models. In International Conference on Artificial
_Intelligence and Statistics (AISTATS), 2020b._

A RELATED WORK

This section summarizes the work related to ours, beyond the brief background provided in the
Introduction. First, we discuss the line of research aiming to develop robust classification models
against adversarial examples. Then, we introduce the line of works which focus on understanding the
intrinsic robustness limit.


-----

A.1 TRAINING ADVERSARIALLY ROBUST CLASSIFIERS

Witnessing the vulnerability of modern machine learning models to adversarial examples, extensive
studies have been carried out aiming to build classification models that can be robust against adversarial perturbations. Heuristic defense mechanisms (Goodfellow et al., 2015; Papernot et al., 2016;
Guo et al., 2018; Xie et al., 2018; M ˛adry et al., 2018) had been most popular until many of them
were broken by stronger adaptive adversaries (Athalye et al., 2018; Tramer et al., 2020). The only
scalable defense which seems to hold up well against adaptive adversaries is PGD-based adversarial
training (M ˛adry et al., 2018). Several variants of PGD-based adversarial training have been proposed,
which either adopt different loss function (Zhang et al., 2019; Wu et al., 2020) or make use of
additional training data (Carmon et al., 2019; Alayrac et al., 2019). Nevertheless, the best current
adversarially-trained classifiers can only achieve around 65% robust accuracy on CIFAR-10 against
_ℓ_ perturbations with strength ϵ = 8/255, even with additional training data (see the leaderboard in
_∞_
Croce et al. (2020)).

To end the arms race between heuristic defenses and newly designed adaptive attacks that break them,
certified defenses have been developed based on different approaches, including linear programming
(Wong & Kolter, 2018; Wong et al., 2018), semidefinite programming (Raghunathan et al., 2018),
interval bound propagation (Gowal et al., 2019; Zhang et al., 2020a) and randomized smoothing
(Cohen et al., 2019; Li et al., 2019). Although certified defenses are able to train classifiers with
robustness guarantees for input instances, most defenses can only scale to small networks and they
usually come with sacrificed empirical robustness, especially for larger adversarial perturbations.

A.2 THEORETICAL UNDERSTANDING ON INTRINSIC ROBUSTNESS

Given the unsatisfactory status quo of building adversarially robust classification models, a line of
research (Gilmer et al., 2018; Fawzi et al., 2018; Mahloujifar et al., 2019a; Shafahi et al., 2019;
Dohmatob, 2019; Bhagoji et al., 2019) attempted to explain the adversarial vulnerability from a
theoretical perspective. These works proved that as long as the input distribution is concentrated
with respect to the perturbation metric, adversarially robust classifiers cannot exist. At the core of
these results is the fundamental connection between the concentration of measure phenomenon and
an intrinsic robustness limit that capture the maximum adversarial robustness with respect to some
specific set of classifiers. For instance, Gilmer et al. (2018) showed that for inputs sampled from
uniform n-spheres, a model-independent robustness upper bound under the Euclidean distance metric
can be derived using the Gaussian Isoperimetric Inequality (Sudakov & Tsirelson, 1974; Borell,
1975). Mahloujifar et al. (2019a) generalized their result to any concentrated metric probability
space of inputs. Nevertheless, it is unclear how to apply these theoretical results to typical image
classification tasks, since whether or not natural image distributions are concentrated is unknown.

To address this question, Mahloujifar et al. (2019b) proposed a general way to empirically measure the
concentration for any input distribution using data samples, then employed it to estimate an intrinsic
robustness limit for typical image benchmarks. By showing the existence of a large gap between the
limit implied by concentration and the empirical robustness achieved by state-of-the-art adversarial
training methods, Mahloujifar et al. (2019b) further concluded that concentration of measure can
only explain a small portion of adversarial vulnerability of existing image classifiers. More recently,
Prescott et al. (2021) further strengthened their conclusion by using the set of half-spaces to estimate
the concentration function, which achieves enhanced estimation accuracy. Other related works (Fawzi
et al., 2018; Krusinga et al., 2019; Zhang et al., 2020b) proposed estimating lower bounds on the
concentration of measure by approximating the underlying distribution using generative models.
None of these works, however, consider data labels. Our main results show that data labels are
essential for understanding intrinsic robustness limits.

B FORMAL DEFINITIONS

In this section, we introduce the formal definitions of complexity penalty and union of ℓp-balls
that are used in Section 5. To begin with, we lay out the definition of complexity penalty that is
defined for some collection of subsets G ∈ pow(X ). VC dimension and Rademacher complexity are
commonly-used examples of such a complexity penalty.


-----

**Definition B.1 (Complexity Penalty). Let G ⊆** pow(X ). We say φ : N × R → [0, 1] is a complexity
penalty for G, if for any δ ∈ (0, 1), it holds that

Pr : _µ_ ( ) _µ(_ ) _δ_ _φ(m, δ)._
_µ[m]_ _∃E ∈G_ _|_ _S_ _E_ _−_ _E_ _| ≥_ _≤_
_S←_
 

Next, we provide the formal definition of union ofb _ℓp-balls as follows:_
**Definition B.2 (Union of ℓp-Balls). Let p ≥** 1. For any T ∈ Z[+], define the union of T ℓp-balls as

(T ; ℓp) = _t=1_ **_rt_** [(][u]t[):][ ∀][t][ ∈] [[][T] []][,][ (][u]t[,][ r]t[)][ ∈] [R][n][ ×][ R][n] 0 _,_
_B_ _∪[T]_ _[B][(][ℓ][p][)]_ _≥_
n o

When p =, (T ; ℓ ) corresponds to the union of T hypercubes.
_∞_ _B_ _∞_

C PROOFS OF MAIN RESULTS

In this section, we provide detailed proofs of our main results, including Theorem 3.2, Theorem 4.2,
Theorem 5.1 and the argument presented in Remark 5.2.

C.1 PROOF OF THEOREM 3.2

In order to prove Theorem 3.2, we make use of the Gaussian Isoperimetric Inequality (Sudakov &
Tsirelson, 1974; Borell, 1975). The proof of such inequality can be found in Ledoux (1996).
**Lemma C.1 (Gaussian Isoperimetric Inequality). Let (R[n], µ) be the n-dimensional Gaussian space**
equipped with the ℓ2-norm distance metric. Consider an arbitrary subset E ∈ pow(R[n]), suppose H is
a half space that satisfies µ(H) = µ(E). Then for any ϵ ≥ 0, we have

_µ(_ _ϵ)_ _µ(_ _ϵ) = Φ_ Φ[−][1][ ]µ( ) + ϵ _,_
_E_ _≥_ _H_ _E_

where Φ( ) is the cumulative distribution function of  (0, 1) and Φ[−][1]( ) is its inverse function.

_·_ _N_ _·_

Now we are ready to prove Theorem 3.2.

_Proof of Theorem 3.2. To begin with, we introduce the following notations. Let µ_ be the probability
_−_
measure for N (−θ, σ[2]In) and µ+ be the probability measure for N (θ, σ[2]In), then by definition, we
have µ = µ−/2 + µ+/2. Consider the optimal subset E _[∗]_ = argminE∈pow(X ){µϵ(E) : µ(E) ≥ _α}._

Note that the standard concentration function h(µ, α, ϵ) is monotonically increasing with respect to
_α, thus µ(_ ) = α holds for any continuous µ. Let α = µ ( ) and α+ = µ+( ). According to
_E_ _[∗]_ _−_ _−_ _E_ _[∗]_ _E_ _[∗]_
the Gaussian Isoperimetric Inequality Lemma C.1, it holds for any ϵ ≥ 0 that

_µ(_ _ϵ_ [) = 1] _ϵ_ [) + 1] _ϵ_ [)][ ≥] [1] (C.1)
_E_ _[∗]_ 2 _[µ][−][(][E]_ _[∗]_ 2 _[µ][+][(][E]_ _[∗]_ 2 [Φ(Φ][−][1][(][α][−][) +][ ϵ][) + 1]2 [Φ(Φ][−][1][(][α][+][) +][ ϵ][)][.]

Note that the equality of (C.1) can be achieved if and only if E _[∗]_ is a half space.

Next, we show that there always exists a half space pow( ) such that µ ( ) = α and
_H ∈_ _X_ _−_ _H_ _−_
_µ+(_ ) = α+. Let f ( ), f+( ) be the PDFs of µ and µ+ respectively. For any x, f (x) and
_H_ _−_ _·_ _·_ _−_ _∈X_ _−_
_f+(x) are always positive, thus we have_

_ff−+((xx)) [= exp]exp_ _−−_ 22σσ11[2][2][ (][ (][x][x][ −][ +][ θ][θ][)][)][⊤][⊤][(][(][x][x][ −][ +][ θ][θ][)][)] = exp 2θσ⊤[2]x .

This implies that the ratio of f+(x)/f−(x) is monotonically increasing with respect to θ[⊤]x.

Consider the following extreme half spaceµ(H−) = α. We are going to prove µ−(H H−) ≥− =µ {−(xE ∈X[∗]) = α : θ− and[⊤]x + µ+ b( · ∥H−θ)∥ ≤2 ≤µ+0(}E such that[∗]) = α+.
Consider the sets ( )[∁] and ( )[∁], we have
_E_ _[∗]_ _∩_ _H−_ _E_ _[∗]_ _∩H−_

_µ+_ ( )[∁][] 2θ⊤x 2θ⊤x ( )[∁]
_E_ _[∗]_ _∩_ _H−_ inf sup _E_ _[∗]_ _∩H−_ _._

_µ− E_ _[∗]_ _∩_ (H−)[∁][][ ≥] **_x∈E_** _[∗]∩(H−)[∁]_ [exp] _σ[2]_  _≥_ **_x∈(E_** _[∗])[∁]∩H−_  _σ[2]_  _≥_ _µ[µ]−[+] (E_ _[∗])[∁]_ _∩H−_

(C.2)

    


-----

Note that we also have

_µ+_ _E_ _[∗]_ _∩_ (H−)[∁][] + µ− _E_ _[∗]_ _∩_ (H−)[∁] = µ+ (E _[∗])[∁]_ _∩H−_ + µ− (E _[∗])[∁]_ _∩H−_ _._ (C.3)

Thus, combining (C.2) and (C.3), we have         

_µ+_ _E_ _[∗]_ _∩_ (H−)[∁][] _≥_ _µ+_ (E _[∗])[∁]_ _∩H−_ and _µ−_ _E_ _[∗]_ _∩_ (H−)[∁][] _≤_ _µ−_ (E _[∗])[∁]_ _∩H−_ _,_

Adding the term  _µ+_   or µ  on both sides, we further have    
_E_ _[∗]_ _∩H−_ _−_ _E_ _[∗]_ _∩H−_

_µ+ _ ( ) µ+( ) =  _α+_ and _µ_ ( ) _µ_ ( ) = α _._
_H−_ _≤_ _E_ _[∗]_ _−_ _H−_ _≥_ _−_ _E_ _[∗]_ _−_

On the other hand, consider the half spaceµ( +) = α. Based on a similar technique, we can prove H+ = {x ∈X : θ[⊤]x − _b · ∥θ∥2 ≥_ 0} such that
_H_

_µ−(H+) ≥_ _µ+(E_ _[∗]) = α+_ and _µ−(H+) ≤_ _µ−(E_ _[∗]) = α−._

In addition, let H = {x ∈X : w[⊤]x + b ≤ 0} be any half space such that µ(H) = α. Since both
_µ+ and µ_ are continuous, as we rotate the half space (i.e., gradually increase the value of w[⊤]θ),
_−_
_µ−(H) and µ+(H) will also change continuously. Therefore, it is guaranteed that there exists a half_
space H ∈ pow(X ) such that µ−(H) = α− and µ+(H) = α+. This further implies that the lower
bound of (C.1) can be always be achieved.

Finally, since we have proved the optimal subset has to be a half space, the remaining task is to solve
the following optimization problem:

1
_H∈minpow(X_ ) 2 [Φ] Φ[−][1][ ]µ−(H) + ϵ + [1]2 [Φ] Φ[−][1][ ]µ+(H) + ϵ (C.4)
       

s.t. H = {x ∈X : w[⊤]x + b ≤ 0} and _µ(H) = α._

Construct function g(u) = Φ Φ[−][1](u) + ϵ + Φ Φ[−][1](2α − _u) + ϵ_, where u ∈ [0, 2α]. Based on
the derivative of inverse function formula, we compute the derivative of g with respect to u as follows
     


1

2π [exp] _−_ [(Φ][−][1][(][u]2[) +][ ϵ][)][2]




[dΦ][−][1][(][u][)]

_·_ du


dg(u)

du


+ _√12π_ [exp] _−_ [(Φ][−][1][(2][α][ −]2 _[u][) +][ ϵ][)][2]_ _·_ [dΦ][−][1][(2]du[α][ −] _[u][)]_

 

(Φ−1(u))2

= exp exp
_−_ [(Φ][−][1][(][u]2[) +][ ϵ][)][2] _·_ 2
   

(Φ−1(2α _u))2_

exp exp _−_
_−_ _−_ [(Φ][−][1][(2][α][ −]2 _[u][) +][ ϵ][)][2]_ _·_ 2
   

= exp(−ϵ[2]/2) · exp _−_ _ϵΦ[−][1](u)_ _−_ exp _−_ _ϵΦ[−][1](2α −_ _u)_ _._

     []

Noticing the term exp(−ϵΦ[−][1](u)) is monotonically decreasing with respect to u, we then know
that g(u) is monotonically increasing in [0, α] and monotonically decreasing in [α, 2α]. Therefore,
this suggests that the optimal solution to (C.4) is achieved when µ ( ) reaches its maximum or
_−_ _H_
its minimum. According to the previous argument regarding the range of α− and α+, we can
immediately prove the optimality results of Theorem 3.2.

C.2 PROOF OF THEOREM 4.2

In this section, we prove Theorem 4.2 based on techniques used in Mahloujifar et al. (2019a) for
proving the connection between the standard concentration function and intrinsic robustness with
respect to the set of imperfect classifiers.

_Proof of Theorem 4.2. Let E_ _[∗]_ be the optimal solution to (4.1), then µ(Eϵ[∗][)][ corresponds to the optimal]
value of (4.1). We are going to show 1 − AdvRobϵ(Fα,γ, c) = µ(Eϵ[∗][)][ by proving both directions.]

First, we prove 1 − AdvRobϵ(Fα,γ, c) ≥ _µ(Eϵ[∗][)][. Let][ f][ be any classifier within][ F][α,γ][, and][ E][(][f]_ [)][ be]
the corresponding error region of f . According to the definitions of risk and adversarial risk, we have

Risk(f, c) = µ( (f )) and AdvRiskϵ(f, c) = µ( _ϵ(f_ )),
_E_ _E_


-----

where _ϵ(f_ ) represents the ϵ-expansion of (f ). Since f _α,γ, we have_
_E_ _E_ _∈F_

Risk(f, c) = µ(E(f )) ≥ _α and LU(E(f_ ); µ, c, η) ≥ _γ._

Thus, by (4.1), we obtain that

1 − AdvRobϵ(f, c) = AdvRiskϵ(f, c) = µ(Eϵ(f )) ≥ _µ(Eϵ[∗][)][.]_

By taking the infimum over f over Fα,γ on both sides, we prove 1 − AdvRobϵ(Fα,γ, c) ≥ _µ(Eϵ[∗][)][.]_

Next, we show that 1 − AdvRobϵ(Fα,γ, c) ≤ _µ(Eϵ[∗][)][. We construct a classifier][ f][ ∗]_ [such that]

_f_ _[∗](x) = c(x) if x ̸∈E_ _[∗]; f_ _[∗](x) ̸= c(x) otherwise._

Note that by construction, E _[∗]_ corresponds to the error region of f _[∗]. Thus according to the definitions_
of risk and adversarial risk, we know

Risk(f _[∗], c) = µ(E_ _[∗]) ≥_ _α and AdvRiskϵ(f_ _[∗], c) = µ(Eϵ[∗][)][.]_

Since LU(E _[∗]; µ, c, η) ≥_ _γ, we know the error region label uncertainty of f_ _[∗]_ is at least γ. Thus, by
definition of intrinsic robustness, we know 1 − AdvRobϵ(Fα,γ, c) ≤ AdvRiskϵ(f _[∗], c) = µ(Eϵ[∗][)][.]_

Finally, putting pieces together, we complete the proof.


C.3 PROOF OF THEOREM 5.1

_Proof of Theorem 5.1. For simplicity, denote by lu(x; c, η) = 1_ _η(x)_ _c(x)_ [+max][y][′][̸][=][c][(][x][)] _η(x)_ _y[′]_
_−_

the label uncertainty of a given input x with respect to c( ) and η( ). Let be a subset in such that

_·_ ·  E _G_  
_µ(E) ≥_ _α and |µ(E) −_ _µ(E)| ≤_ _δ, where δ is a constant much smaller than α. Then according to_
Definition 4.1, we can decompose the estimation error of label uncertainty as:
b 1 1

LU( ; µ, c, η) LU( ; _µ_ _, c,_ _η) =_ lu(x; c, η) dµ lu(x; c, _η) dµ_
_E_ _−_ _E_ _S_ _µ(_ ) _−_ _µ_ ( ) _S_

_E_ ZE _S_ _E_ ZE

1 1

b b = lu(x; c, η) dµ b b

_µ(_ ) _µ_ ( ) _·_ b

 _E_ _[−]_ _S_ _E_  ZE

_I1_

1 b

| + lu({zx; c, η) lu(x; c,}η) _dµ_

_µ_ ( ) _−_
_S_ _E_ ZE

 
_I2_
b

b 1

+ | lu(x; c,{zη) dµ lu(x; c,} _η) dµ_ _._

_µ_ ( ) _−_ _S_
_S_ _E_  ZE ZE 

_I3_

b b b

b

Next, we upper bound the absolute value of the three components, respectively.| {z }

Consider the first term I1. Note that 0 lu(x; c, η) 2 for any x, thus we have
_≤_ _≤_ _∈X_
_|_ _E_ [lu(][x][;][ c, η][)][ dµ][| ≤] [2][µ][(][E][)][. Therefore, we have]
R


_|I1| ≤_ _µ(E)_ _[−]_ _µS_ (E) _[·][ 2][µ][(][E][)][ ≤]_ _µS_ (E) _[· |][µ][(][E][)][ −]_ _µ[b]S_ (E)|.

As for the second term I2, the following inequality holds for any x
_∈X_

b b

lu(x; c, η) lu(x; c, _η)_ _η(x)_ _η(x)_ _η(x)_ _η(x)_
_|_ _−_ _| ≤_ _−_ _c(x)_ [+] _y[′][max]=c(x)_ _y[′][ −]_ _y[′][max]=c(x)_ _y[′]_

_̸_ _̸_

     

_η(x)_ _η(x)_

b _≤_ _−_ b 1[,] b

where the second inequality holds becauseTherefore, we can upper bound _I2_ by _|b maxi ai −_ maxi bi| ≤ maxi |ai − _bi| for any a, b ∈_ R[n].
_|_ _|_

1 1 _δη_
_|I2| ≤_ _µ_ ( ) _η(x) −_ _η(x)_ 1 _[dµ][ ≤]_ _µ_ ( ) _η(x) −_ _η(x)_ 1 _[dµ][ ≤]_ _µ_ ( ) _[.]_

_S_ _E_ ZE _S_ _E_ ZX _S_ _E_

b b

b b b


-----

For the last term I3, since 0 lu(x; c, η) 2 holds for any x, we have
_≤_ _≤_ _∈X_


2

_µ(_ ) _µ_ ( )
_µ_ ( ) _E_ _−_ _S_ _E_
_S_ _E_ _[·]_ b
b


_I3_
_|_ _| ≤_

Finally, putting pieces together, we have


4 _δη_
LU( ; µ, c, η) LU( ; _µ_ _, c,_ _η)_ _µ(_ ) _µ_ ( ) +
_|_ _E_ _−_ _E_ _S_ _| ≤_ _µ_ ( ) _E_ _−_ _S_ _E_ _µ_ ( ) _α_ _δ [,]_

_S_ _E_ _[·]_ _S_ _E_ _[≤]_ [4][δ][ +] −[ δ][η]

provided µ( ) _α and_ _µ(_ ) bµ ( b) _δ. Making use of the definition of complexity penalty forb_
_G completes the proof of Theorem 5.1.E_ _≥_ _|_ _E_ _−_ _S_ _E_ _| ≤_ b b
b

C.4 PROOF OF REMARK 5.2

Before presenting the proofs, we first lay out the formal statement of Remark 5.2 in Theorem C.2.
The proof technique of Theorem C.2 is inspired by Theorem 3.5 in Mahloujifar et al. (2019b).
**Theorem C.2 (Formal Statement of Remark 5.2). Consider the input metric probability space**
(X _, µ, ∆), the concept function c and the label distribution function η. Let {G(T_ )}T ∈N be a series of
collection of subsets over X . For any T ∈ N, assume φ[T] and φ[T]ϵ [are complexity penalties for][ G][(][T] [)]
and _ϵ(T_ ) respectively, and _η is a function such that_ _η(x)_ _η(x)_ 1dx _δη._
_G_ _X_ _[∥][b]_ _−_ _∥_ _≤_

Define h(µ, c, η, α, γ, ϵ, ) = inf _µ(_ _ϵ) : µ(_ ) R _α, LU(_ ; µ, c, η) _γ_ to be the constrained
concentration function. We simply write G b _E∈G{_ _hE(µ, c, η, α, γ, ϵE_ _≥_ ) whenE _G = pow ≥(X}). Given a sequence of_
datasets{δ(T )}T { ∈SNT with }T ∈ δN, where(T ) ∈ (0 ST, α/ consists of2), if the following assumptions holds: m(T ) i.i.d. samples from µ and a sequence of real numbers

1. _T =1_ _[φ][T][ (][m][(][T]_ [)][, δ][(][T] [))][ <][ ∞]

2. _T =1_ _[φ]ϵ[T]_ [(][m][(][T] [)][, δ][(][T] [))][ <][ ∞]

[P][∞]

3. lim[P][∞]T →∞ _δ(T_ ) = 0

4. limT →∞ _h(µ, c, η, α, γ, ϵ, G(T_ )) = h(µ, c, η, α, γ, ϵ) [1]

5. h is locally continuous w.r.t. α and γ at (µ, c, η, α, γ _δη/α, ϵ, pow(_ )),
_±_ _X_

then with probability 1, we have

_h(µ, c, η, α, γ_ _δη/α, ϵ)_ lim _η, α, γ, ϵ,_ (T )) _h(µ, c, η, α, γ + δη/α, ϵ)._
_−_ _≤_ _T_ _G_ _≤_
_→∞_ _[h][(][µ][S][T][, c,][ b]_

To prove Theorem C.2, we use the following theorem regarding the generalization of concentration
under label uncertainty constraints. The proof of Theorem C.3 is provided in Appendix C.5.
**Theorem C.3 (Generalization of Concentration). Let (X** _, µ, ∆) be a metric probability space and_
_G ⊆_ pow(X ). Define h(µ, c, η, α, γ, ϵ, G) = inf _E∈G{µ(Eϵ) : µ(E) ≥_ _α, LU(E; µ, c, η) ≥_ _γ} as the_
generalized concentration function under label uncertainty constraints. Then, under the same setting
of Theorem 5.1, for any γ, ϵ ∈ [0, 1], α ∈ (0, 1] and δ ∈ (0, α/2), we have

Pr _h(µ, c, η, α_ _δ, γ_ _δ[′], ϵ,_ ) _δ_ _h(µ_ _, c,_ _η, α, γ, ϵ,_ )
_µ[m]_ _−_ _−_ _G_ _−_ _≤_ _S_ _G_
_S←_
 _h(µ, c, η, α + δ, γ + δ[′], ϵ,_ ) + δ 1 6φ(m, δ) 2φϵ(m, δ),

_≤_ _G_ b b≥ _−_ _−_

where δ[′] = (4δ + δη)/(α − 2δ) and φϵ is the complexity penalty for _Gϵ._

In addition, we also make use of the Borel-Cantelli Lemma to prove Theorem C.2.
_∞Lemma C.4. Then with probability (Borel-Cantelli Lemma) 1, only finite number of events will occur.. Let {ET }T ∈N be a series of events such that_ _T =1_ [Pr[][E][T][ ]][ <]

[P][∞]

Now we are ready to prove Theorem C.2.

1It is worth nothing that this assumption is satisfied for any family of collections of subsets that is a universal
approximator, such as kernel SVMs and decision trees.


-----

_Proof of Theorem C.2. Let ET be the event such that_

_h_ _µ, c, η, α_ _δ(T_ ), γ _δ[′](T_ ), ϵ, (T ) _δ(T_ ) > h _µ_ _T, c,_ _η, α, γ, ϵ,_ (T ) or
_−_ _−_ _G_ _−_ _S_ _G_

_h µ, c, η, α + δ(T_ ), γ + δ[′](T ), ϵ, G(T ) + δ(T ) < h µST, c, _η, α, γ, ϵ, G(T_ ),

b b

_δC.3, for any[′](T_ ) = (4 δ T(T ∈) +N δ, we haveη)/(α − 2δ(T )) for any T ∈ N. Since δ(T )b < α/2 b, thus according to Theorem

Pr[ET ] ≤ 6φ[T] (m(T ), δ(T )) + 2φ[T]ϵ [(][m][(][T] [)][, δ][(][T] [)))][.]

By Assumptions 1 and 2, this further implies


_∞_

_φ[T]_ (m(T ), δ(T )) + 2

_T =1_

X


_φ[T]ϵ_ [(][m][(][T] [)][, δ][(][T] [)))][ <][ ∞][.]
_T =1_

X


Pr[ET ] ≤ 6
_T =1_

X


Thus according to Lemma C.4, we know that there exists some j ∈ N such that for all T ≥ _j,_

_h_ _µ, c, η, α_ _δ(T_ ), γ _δ[′](T_ ),ϵ, (T ) _δ(T_ ) _h(µ_ _T, c,_ _η, α, γ, ϵ)_
_−_ _−_ _G_ _−_ _≤_ _S_
  _h_ _µ, c, η, α_ + δ(T ), γ + δ[′](T ), ϵ, (T ) + δ(T ), (C.5)

_≤_ _G_

b b

holds with probability 1. In addition, by Assumptions 3, 4 and 5, we have  

lim _µ, c, η, α_ _δ(T_ ), γ _δ[′](T_ ), ϵ, (T )
_T_ _−_ _−_ _G_
_→∞_ _[h]_

= lim  _µ, c, η, α_ _δ(T1), γ_ _δ[′](T1), ϵ,_ (T2)
_T1_ _T2_ _−_ _−_ _G_
_→∞_ [lim]→∞ _[h]_

= lim _µ, c, η, α _ _δ(T1), γ_ _δ[′](T1), ϵ_ 
_T1_ _−_ _−_
_→∞_ _[h]_

= h _µ, c, η, α, γ _ _δη/α, ϵ_ _,_ 
_−_
  

where the second equality is due to Assumption 4 and the last equality is due to Assumptions 3 and 5.
Similarly, we have

lim _µ, c, η, α + δ(T_ ), γ + δ[′](T ), ϵ, (T ) = h _µ, c, η, α, γ + δη/α, ϵ_ _._
_T_ _G_
_→∞_ _[h]_

Therefore, let T goes to  _∞_ in (C.5), we have    

_h_ _µ, c, η, α, γ_ _δη/α, ϵ_ lim _µ_ _T, c,_ _η, α, γ, ϵ,_ (T ) _h_ _µ, c, η, α, γ + δη/α, ϵ_ _._
_−_ _≤_ _T_ _S_ _G_ _≤_
_→∞_ _[h]_
        
b b


C.5 PROOF OF GENERALIZATION OF CONCENTRATION THEOREM

_Proof of Theorem C.3. First, we introduce some notation. Let h(µ, c, η, α, γ, ϵ, G) be the optimal_
value and g(µ, c, η, α, γ, ϵ, G) be the optimal solution with respect to the following generalized
concentration of measure problem with label uncertainty constraint:

minimize _µ(_ _ϵ)_ subject to _µ(_ ) _α and LU(_ ; µ, c, η) _γ._ (C.6)
_E_ _E_ _≥_ _E_ _≥_
_E∈G_

Note that the difference between (C.6) and (4.1) is that the feasible set of E is restricted to some collection of subsets pow( ). Correspondingly, we let h(µ _, c,_ _η, α, γ, ϵ,_ ) and g(µ _, c,_ _η, α, γ, ϵ,_ )
_G ⊆_ _X_ _S_ _G_ _S_ _G_
be the optimal value and optimal solution with respect to the empirical optimization problem (5.1).

Let = g(µ, c, η, α + δ, γ + δ[′], ϵ, ) and = g(µ _, c,b_ _η, α, γ, ϵ, b_ ), where δ[′]bwill be specified b
_E_ _G_ _E_ _S_ _G_
later. Note that when these optimal sets do not exist, we can select a set for which the expansion is
arbitrarily close to the optimum, then every step of the proof will apply to this variant. According to

[b] b b
the definition of complexity penalty, we have

Pr _µ_ ( [b]) _µ(_ [b]) _δ_ _φ(m, δ)._ (C.7)
_µ[m]_ _|_ _S_ _E_ _−_ _E_ _| ≥_ _≤_
_S←_
 

Since _µ_ ( [b]) _α by definition, (C.7) implies thatb_
_S_ _E_ _≥_

Pr _µ(_ [b]) _α_ _δ_ _φ(m, δ)._ (C.8)

b _S←µ[m]_ _E_ _≤_ _−_ _≤_

 


-----

In addition, according to Theorem 5.1, for any δ ∈ (0, α/2), we have

Pr LU( ; µ, c, η) LU( [b]; _µ_ _, c,_ _η)_ 2φ(m, δ), (C.9)
_µ[m]_ _E_ _−_ _E_ _S_ _≤_ [4]α[δ][ +][ δ]2δ[η] _≤_
_S←_ _−_ 



where the inequality holds because ofb (C.8) and the union bound. Since b b LU( [b]; _µ_ _, c,_ _η)_ _γ by_
_E_ _S_ _≥_
definition, (C.9) implies that
b b

Pr LU( [b]; µ, c, η) _γ_ 2φ(m, δ). (C.10)
_µ[m]_ _E_ _≤_ _−_ [4]α[δ][ +][ δ]2δ[η] _≤_
_S←_  _−_ 

Based on the definition of the concentration function h, combining (C.8) and (C.10) and making use
of the union bound, we have

Pr _µ(_ [b]ϵ) _h(µ, c, η, α_ _δ, γ_ _δ[′], ϵ,_ ) 3φ(m, δ), (C.11)
_µ[m]_ _E_ _≤_ _−_ _−_ _G_ _≤_
_S←_
 

where we set δ[′] = [4]α[δ][+]2[δ]δ[η] [. Note that according to the definition of][ φ][ϵ][, we have]

_−_

Prµ[m] _|µ(E[b]ϵ) −_ _µS_ (E[b]ϵ)| ≤ _δ_ _≤_ _φϵ(m, δ),_ (C.12)
_S←_
 

thus combining (C.11) and (C.12) by union bound, we have

Prµ[m] _µS_ (E[b]ϵ) ≤ _h(µ, c, η, α −_ _δ, γ −_ _δ[′], ϵ, G) −_ _δ_ _≤_ 3φ(m, δ) + φϵ(m, δ). (C.13)
_S←_
 

This completes the proof of one-sided inequality of Theorem C.3. The other side of Theorem C.3 canb
be proved using the same technique. In particular, we have

Prµ[m] _µS_ (E[b]ϵ) ≥ _h(µ, c, η, α + δ, γ + δ[′], ϵ, G) + δ_ _≤_ 3φ(m, δ) + φϵ(m, δ). (C.14)
_S←_
 

Combining (C.13) and (C.14) by union bound completes the proof.b


D HEURISTIC SEARCH ALGORITHM

The pseudocode of the heuristic search algorithm for the empirical label uncertainty constrained
concentration problem (5.1) is shown in Algorithm 1.

**Algorithm 1: Heuristic Search for Robust Error Region under ℓp(p ∈{2, ∞})**

**Input** **: a set of labeled inputs** **_x, c(x),_** _η(x)_ **_x_**, parameters α, γ, ϵ, T
_{_ _}_ _∈S_

**1**, init, exp ;

**2 forE ←{} t = 1, 2S, . . ., T ←{} do** _S_ _←{}_ b

**3** [b] _klower_ b (α initb )/(T _t + 1)_, _kupper_ (α init );

**4** Ω _←{} ←⌈;_ _|S| −|S_ _|_ _−_ _⌉_ _←_ _|S| −|S_ _|_

**5** **for u** **do**
_∈S_ [b] [b]

**6** **for k ∈** [klower, kupper] do

**7** _rk(u) ←_ compute the ℓp distance from u to the k-th nearest neighbour in S \ _Sinit;_

**8** _Sinit(u, k) ←{x ∈S \_ _Sinit : ∥x −_ **_u∥2 ≤_** _rk(u)};_

**109** _Sifexp LU((u, kinit) ←{(u, kx), ∈S \µ_ _, c,_ _Sη[b])exp :γ ∥ thenx −_ **_u∥2 ≤_** _rk(u) + ϵ};_ [b]
_S_ _S_ _≥_

**11** insert (u, k) into Ω

[b]

**12** (u, _k)_ argmin(u,k) Ω b exp( bu, k) init(u, k) ;
_←_ _∈_ _{|S_ _| −|S_ _|}_

**13** _E ←_ _E ∪[b]_ Ball(u, rk[(]u[b]));

**14** binit[b] init init(u, _k),_ exp exp exp(u, _k);_
**Output :Sb** _←E_ _S[b]_ _∪Sb_ b _S_ _←_ _S[b]_ _∪S_
b b [b] b b [b]

[b]


-----

E DETAILED EXPERIMENTAL SETTINGS

In this section, we specify the details of the experiments presented in Section 6. The robustness
results of all the adversarially-trained models from RobustBench (Croce et al., 2020) are evaluated
using the auto attack (Croce & Hein, 2020). All of our experiments are conducted using a GPU server
with a NVIDIA GeForce RTX 2080 Ti Graphics card.

**Error Region Label Uncertainty. We explain the experimental details of Figure 3. For standard**
trained classifiers, we implemented five neural network architecture, including a 4-layer neural net
with two convolutional layers and two fully-connected layers (small), a 7-layer neural net with four
convolutional layers and three fully-connected layers (large), a ResNet-18 architecture (resnet18),
ResNet-50 architecture (resnet50) and a WideResNet-34-10 architecture (wideresnet). We trained the
_small and large model using a Adam optimizer with initial learning rate 0.005, whereas we trained_
the resnet18, resnet50 and wideresnet model using a SGD optimizer with initial learning rate 0.01.
All models are trained using a piece-wise learning rate schedule with a decaying factor of 10 at epoch
50 and epoch 75, respectively. For Figure 3(a), we plotted the label uncertainty and standard risk
for the intermediate models obtained at epochs 5, 10, . . ., 100 for each architecture. In addition, we
also randomly selected different subsets of inputs with empirical measure of 0.05, 0.10, . . . 0.95 and
plotted their corresponding label uncertainty with error bars.

For adversarially trained classifiers, we implemented the vanilla adversarial training method (M ˛adry
et al., 2018) and the adversarial training method with adversarial weight perturbation (Wu et al., 2020),
which are denoted as AT and AT-AWP in Figure 3(b) respectively. Both ResNet-18 (resnet18) and
WideResNet-34-10 (wideresnet) architecture are implemented for each training method. A 10-step
PGD attack (PGD-10) with step size 2/255 and maximum perturbation size 8/255 is used for each
model during training. In addition, each model is trained for 200 epochs using a SGD optimizer with
initial learning rate 0.1 and piece-wise learning rate schedule with a decaying factor of 10 at epoch
100 and epoch 150. We record the intermediate models at epoch 10, 20, . . ., 200 respectively.

**Estimation of Intrinsic Robustness. For Figure 4, we first conduct a 50/50 train-test split over the**
10, 000 CIFAR-10 test images, then run Algorithm 1 for each setting on the training dataset to obtain
the optimal subset. Here, we choose the value of α ∈{0.01, 0.02, . . ., 0.15} and tune the parameter
_T for each α parameter. Next, we evaluate the empirical measure of the optimally-searched subset_
(denoted by empirical risk in Figure 4) and the empirical measure of its ϵ-expansion using the testing
dataset, and translate it into an intrinsic robustness estimate. Finally, we plot the empirical risk and
the estimated intrinsic robustness for each parameter setting in Figure 4.

F ADDITIONAL EXPERIMENTS

This appendix provides additional experimental results, supporting our arguments in Section 6.

**Visualization of label uncertainty. Figure 6 shows some CIFAR-10 images with the original CIFAR-**
10 labels and the CIFAR-10H human uncertainty labels. The label uncertainty score is computed
based on Definition 4.1 and provided under each image.

There are a few examples with high label uncertainty, whose CIFAR-10 label contradicts with the
CIFAR-10H soft label (see the first two images in Figure 6(a)), indicating they are actually mislabeled.
The images with uncertainty scores around 1.0 do appear to be images that are difficult for human to
recognize, whereas images with lowest uncertainty scores look clearly representative of the labeled
class. These observations show the usefulness of the proposed label uncertainty definition.

**Estimation of Intrinsic Robustness. Table 1 summarizes our estimated intrinsic robustness limits**
produced by Algorithm 1 for different hyperparameter settings. In particular, we set α = 0.05 and
_γ = 0.17 to roughly reflect the standard error and the label uncertainty of the error regions with_
respect to the state-of-the-art classification models (see Figure 3), use ϵ ∈{4/255, 8/255, 16/255}
for ℓ∞ and ϵ ∈{0.5, 1.0, 1.5} for ℓ2. Note that we also compare our estimate with the intrinsic
robustness limit implied by the standard concentration by setting γ = 0 for each setting.

We perform a 50/50 train-test split on the CIFAR-10 test images: we obtain the optimal subset with
the smallest ϵ-expansion on the training dataset based on Algorithm 1 and evaluate it on the testing


-----

(a) Highest Uncertainty Score (b) Lowest Uncertainty Score

(c) Uncertainty Score around 1.0 (d) Uncertainty Score around 1.0

Figure 6: Illustration of human uncertainty labels and label uncertainty of CIFAR-10 test images.
Each subfigure shows a group of images with a certain level of uncertainty score.

dataset. We report both the empirical measure of the optimally-found subset (Empirical Risk in Table
1), and the translated intrinsic robustness estimate. These results show that our estimation of intrinsic
robustness generalizes from the training data to the testing data, and support the argument that our
estimate is a more accurate characterization of intrinsic robustness compared with standard one.

G ESTIMATING LABEL ERRORS USING CONFIDENT LEARNING

The proposed concentration estimation framework relies on the knowledge of human soft labels to
determine which example has label uncertainty exceeding a certainty threshold. Since most machine
learning datasets do not provide such information like CIFAR-10H, this raises the question of how to
extend our method to the setting where human soft labels are unavailable.

We make an initial attempt to address the aforementioned issue using the confident learning approach
of Northcutt et al. (2021b). Their goal was to identify label errors for a dataset, which is closely related
to label uncertainty. The method first computes a confidence joint matrix based on the predicted
probabilities of a pretrained classifier, then selects the top examples based on a ranking rule, such as
self-confidence or max margin. If we are able to approximate human label uncertainty from the raw


-----

Table 1: Summary of the main results using our method for different settings on CIFAR-10 dataset.
We conduct 5 repeated trials for each setting to record the mean statistics and its standard deviation.

**Empirical Risk (%)** **Intrinsic Robustness (%)**
**Metric** **_α_** **_ϵ_** _γ_ **_T_**

training testing training testing


0.0 5 5.80 0.04 4.50 0.21 93.48 0.10 93.86 0.26
4/255 _±_ _±_ _±_ _±_
0.17 5 5.84 ± 0.10 5.06 ± 0.82 92.03 ± 0.45 92.61 ± 1.12

0.0 10 5.77 0.01 4.76 0.27 92.89 0.11 92.36 0.33
8/255 _±_ _±_ _±_ _±_
0.17 10 5.77 ± 0.02 4.85 ± 0.58 90.91 ± 0.53 90.98 ± 1.03

0.0 5 5.68 0.04 5.30 0.33 88.44 0.47 87.89 1.24
16/255 _±_ _±_ _±_ _±_
0.17 5 5.67 ± 0.25 4.79 ± 0.75 81.96 ± 1.69 83.83 ± 2.37

0.0 5 5.76 0.00 5.41 0.60 93.78 0.10 93.51 0.67
0.5 _±_ _±_ _±_ _±_
0.17 5 5.76 ± 0.00 5.36 ± 0.14 91.89 ± 0.38 91.70 ± 0.49

0.0 5 5.76 0.00 6.00 0.50 92.93 0.06 92.22 0.55
1.0 _±_ _±_ _±_ _±_
0.17 5 5.76 ± 0.00 5.32 ± 0.29 87.86 ± 0.79 87.75 ± 0.58

0.0 5 5.76 0.00 5.67 0.56 91.98 0.13 91.82 0.65
1.5 _±_ _±_ _±_ _±_
0.17 5 5.76 ± 0.00 5.69 ± 0.45 83.33 ± 2.04 82.87 ± 2.50


_ℓ_ 0.05
_∞_

_ℓ2_ 0.05


(a) Error versus Non-error (b) Precision Recall Curve

Figure 7: Illustration of misalignment label errors recognized by human and those identified by
confident learning (a) Distribution of human label uncertainty between errors and non-errors estimated
using confident learning; (b) Precision-recall curve for estimating the set of examples with human
label uncertainty exceeding 0.5.

inputs and labels, or identify the set of examples with high label uncertainty, then we can immediately
adapt our proposed framework by leveraging such estimated results. However, we observe only a
weak correlation between the set of label errors that are produced by confident learning and the set of
examples with high human label uncertainty.

We conduct the experiments on CIFAR-10 and identify the set of label errors based on confident
learning. We train a ResNet-50 based classification model on the CIFAR-10 training data, and select
examples in the CIFAR-10 test dataset as labeling errors using the best ranking method suggested in
Northcutt et al. (2021b). Figure 7(a) compares the distribution of human label uncertainty (based
on the human soft labels from CIFAR-10H) between the set of estimated label error and non-errors.
Although the set of examples estimated as label error have relative higher human label uncertainty
compared with non-errors, there exist over 30% of estimated label errors have 0 label uncertainty for
human annotators. It implies that there is a mismatch between label errors identified by human and
that estimated using confident learning techniques. This is further confirmed by the precision-recall
curve presented in Figure 7(b). We treat examples with human label uncertainty exceeding 0.5 as the
‘ground-truth’ uncertain images, and vary the size of produced set of label errors to plot the precision
and recall curve. The fact that precision rate is uniformly lower than 0.25, indicating that over 75%
of the estimate error examples have human label uncertainty less than 0.5.


-----

Figure 8: Visualization of label distribution of top uncertain CIFAR-10 test images estimated using a
confident learning approach. Both human and estimated label distribution are plotted in each figure.
The corresponding label uncertainty scores are computed and provided under each image, while the
original CIFAR-10 label is highlighted in blue above each image.

Figure 8 visualizes the human label distribution and estimated label distribution on CIFAR-10. We
compute the estimated label uncertainty of each CIFAR-10 testing examples by replacing the human
label distribution with the predicted probabilities of the trained model. It can be seen that there exists
a misalignment between the human label distribution and the distribution estimated using some neural
network. This again confirms that label errors produced by confident learning are not guaranteed to
be examples that are difficult for humans.


-----

