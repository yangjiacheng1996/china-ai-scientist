# THE GEOMETRY OF MEMORYLESS STOCHASTIC POL## ICY OPTIMIZATION IN INFINITE-HORIZON POMDPS

**Johannes M¨uller**
Max Planck Institute for Mathematics in the Sciences, Leipzig, Germany
jmueller@mis.mpg.de

**Guido Mont´ufar**
Department of Mathematics and Department of Statistics, UCLA, CA, USA
Max Planck Institute for Mathematics in the Sciences, Leipzig, Germany
montufar@math.ucla.edu

ABSTRACT

We consider the problem of finding the best memoryless stochastic policy for an
infinite-horizon partially observable Markov decision process (POMDP) with finite state and action spaces with respect to either the discounted or mean reward
criterion. We show that the (discounted) state-action frequencies and the expected
cumulative reward are rational functions of the policy, whereby the degree is determined by the degree of partial observability. We then describe the optimization
problem as a linear optimization problem in the space of feasible state-action frequencies subject to polynomial constraints that we characterize explicitly. This
allows us to address the combinatorial and geometric complexity of the optimization problem using recent tools from polynomial optimization. In particular, we
estimate the number of critical points and use the polynomial programming description of reward maximization to solve a navigation problem in a grid world.

1 INTRODUCTION

Markov decision processes (MDPs) were introduced by Bellman (1957) as a model for sequential
decision making and optimal planning (see, e.g., Howard, 1960; Derman, 1970; Puterman, 2014).
Many algorithms in reinforcement learning rely on the ideas and methods developed in the context of
MDPs (see, e.g., Sutton & Barto, 2018). Often in practice, the decisions need to be made based only
on incomplete information of the state of the system. This setting is modeled by partially observable
Markov decision processes (POMDPs) introduced by Astr¨[˚] om (1965) (for a historical discussion see
Monahan, 1982), which have become an important model for planning under uncertainty. In this
work we pursue a geometric characterization of the policy optimization problem in POMDPs over
the class of stochastic memoryless policies and its dependence on the degree of partial observability.

It is well known that acting optimally in POMDPs may require memory ( Astr¨[˚] om, 1965). A POMDP
with unlimited memory policies can be modeled as a belief state MDP, where the states are replaced
by probability distributions that serve as sufficient statistics for the previous observations (Kaelbling
et al., 1998; Murphy, 2000). Finding an optimal policy in this class is PSPACE-complete for finite
horizons (Papadimitriou & Tsitsiklis, 1987) and undecidable for infinite horizons (Madani et al.,
2003; Chatterjee et al., 2016). Therefore, it is of interest to consider POMDPs with constrained
policy classes. A natural class to consider are memoryless policies, also known as reactive or Markov
policies, which select actions based solely on the current observations. In this case, it is useful to
allow the actions to be selected stochastically, which not only allows for better solutions but also
provides a continuous optimization domain (Singh et al., 1994).

Although they are more restrictive than policies with memory, memoryless policies are attractive as
they are easier to optimize and are versatile enough for certain applications (Tesauro, 1995; Loch &
Singh, 1998; Williams & Singh, 1999; Kober et al., 2013). In fact, finite-memory policies can be
modeled in terms of memoryless policies by supplementing the state of the system with an external
memory (Littman, 1993; Peshkin et al., 1999; Icarte et al., 2021). Hence theoretical advances on
memoryless policy optimization are also of interest to finite-memory policy optimization. Theoret

-----

ical aspects and optimization strategies over the class of memoryless policies have been studied in
numerous works (see, e.g., Littman, 1994; Singh et al., 1994; Jaakkola et al., 1995; Loch & Singh,
1998; Williams & Singh, 1999; Baxter et al., 2000; Baxter & Bartlett, 2001; Li et al., 2011; Azizzadenesheli et al., 2018). However, finding exact or approximate optimal memoryless stochastic
policies for POMDPs is still considered an open problem (Azizzadenesheli et al., 2016), which is
NP-hard in general (Vlassis et al., 2012). One reason for the difficulties in optimizing POMDPs
is that, even in a tabular setting, the problem is non-convex and can exhibit suboptimal strict local
optima (Bhandari & Russo, 2019). For memoryless policies the expected cumulative reward is a
linear function of the corresponding (discounted) state-action frequencies. In the case of MDPs the
feasible set of state-action frequencies is known to form a polytope, so that the optimization problem
can be reduced to a linear program (Manne, 1960; De Ghellinck, 1960; d’Epenoux, 1963; Hordijk
& Kallenberg, 1981). On the other hand, to the best of our knowledge, for POMDPs the specific
structure of the feasibility constraints and the optimization problem have not been studied, at least
not in the same level of detail (see related works below).

**Related works** In MDPs, the (discounted) state-action frequencies form a polytope resp. a compact convex set in the finite resp. countable state-action cases, whereby the extreme points are
given by the state-action frequencies of deterministic stationary policies (Derman, 1970; Altman
& Shwartz, 1991). Further, Dadashi et al. (2019) showed that the set of state value functions in finite
state-action MDPs is a finite union of polytopes. The set of stationary state-action distributions of
POMDPs has been studied by Mont´ufar et al. (2015) highlighting a decomposition into infinitely
many convex subsets whose dimensions depend on the degree of observability. Although this decomposition can be used to localize optimal policies to some extent, a description of the pieces in
combination is still missing, needed to capture the properties of the optimization problem. We will
obtain a detailed description in terms of finitely many polynomial constraints with closed form expressions and bound their degrees in terms of the observation mechanism. This yields a polynomial
programming formulation of POMDPs generalizing the linear programming formulation of MDPs.
This is different to the formulation as a quadratically constrained problem by Amato et al. (2006),
where the number and degree of constraints do not depend on the observability. Finally, Cohen &
Parmentier (2018) described finite horizon POMDPs as a mixed integer linear program.

Grinberg & Precup (2013) showed that the expected mean reward is a rational function and obtained
bounds on the degree of this function. We generalize this to the setting of discounted rewards and
refine the result by relating the rational degree to the degree of observability. For both MDPs and
POMDPs the expected cumulative reward is known to be a non-convex function of the policy even
for tabular policy models (Bhandari & Russo, 2019). Nonetheless, for MDPs critical points can be
shown to be global maxima under mild conditions. In contrast, for POMDPs or MDPs with linearly
restricted policy models, it is known that non-global local optimizers can exist (Baxter et al., 2000;
Poupart et al., 2011; Bhandari & Russo, 2019). However, nothing is known about the number of local
optimizers. We will present bounds on the number of critical points building on our computation of
the rational degree and feasibility constraints.

The structure of the expected cumulative reward has been studied in terms of the location of the
global optimizers and the existence of local optimizers. Most notably, it is well known that in MDPs
there always exist optimal policies which are memoryless and deterministic (see Puterman, 2014). In
the case of POMDPs, optimal memoryless policies may need to be stochastic (see Singh et al., 1994).
Mont´ufar et al. (2015); Mont´ufar & Rauh (2017); Mont´ufar et al. (2019) obtained upper bounds on
the number of actions that need to be randomized by these policies, which in the worst case is equal
to the number of states that are compatible with the observation. Although we do not improve these
results (which are indeed tight in some cases), our description of the expected cumulative reward
function leads to a simpler proof of the bounds obtained by Mont´ufar et al. (2015).

Neyman (2003) considered stochastic games as semialgebraic problems showing that the minmax
and maxmin payoffs in an n-player game are semialgebraic functions of the discount factor. Although this is not directly related to our work, we take a similar philosophy. We pursue a semialgebraic description of the feasible set of discounted state-action frequencies in POMDPs, which
is closely related to the general spirit of semialgebraic statistics, where this is usually referred to
as the implitization problem (Zwiernik, 2016). Based on this we characterize the properties of the
optimization problem by its algebraic degree, a concept that has been advanced in recent works on
polynomial optimization (Bajaj, 1988; Nie & Ranestad, 2009; Ozl¨[¨] um C¸ elik et al., 2021).


-----

**Contributions** We obtain results for infinite-horizon POMDPs with memoryless stochastic policies under the mean or discounted reward criteria which can be summarized as follows.

1. We show that the state-action frequencies and the expected cumulative reward can be written as
fractions of determinantal polynomials in the entries of the stochastic policy matrix. We show that
the degree of these polynomials is directly related to the degree of observability (see Theorem 4).

2. We describe the set of feasible state-action frequencies as a basic semialgebraic set, i.e., as the
solution set to a system of polynomial equations and inequalities, for which we also derive closed
form expressions (see Theorem 16 and Remark 18).

3. We reformulate the expected cumulative reward optimization problem as the optimization of
a linear function subject to polynomial constraints (see Remark 19), which we use to solve a
navigation problem in a grid world (see Appendix F). This is a POMDP generalization of the
dual linear programming formulation of MDPs (Kallenberg, 1994; Puterman, 2014).

4. We present two methods for computing the number of critical points, which rely, respectively, on
the rational degree of the expected cumulative reward function and the geometric description of
the feasible set of state-action frequencies (see Theorem 20, Proposition 21 and Appendix D).

2 PRELIMINARIES

We denote the simplex of probability distributions over a finite set by ∆ . An element µ ∆
_X_ _X_ _∈_ _X_
is a vector with non-negative entries µx = µ(x), x ∈X adding to one. We denote the set of Markov
kernels from a finite set to another finite set by ∆[X] [. An element][ Q][ ∈] [∆][X] [is a][ |X| × |Y|][ row]
_X_ _Y_ _Y_ _Y_
stochastic matrix with entries Qxy = Q(y _x), x_, y . Given Q[(1)] ∆[X] [and][ Q][(2)][ ∈] [∆][Y]
_|_ _∈X_ _∈Y_ _∈_ _Y_ _Z_
we denote their composition into a kernel from to by Q[(2)] _Q[(1)]_ ∆[X]
_X_ _Z_ _◦_ _∈_ _Z_ [. Given][ p][ ∈] [∆][X]
and Q ∆[X] [we denote their composition into a joint probability distribution by][ p][ ∗] _[Q][ ∈]_ [∆][X×Y] [,]
_∈_ _Y_
(p ∗ _Q)(x, y) := p(x)Q(y|x). The support of v ∈_ R[X] is the set supp(v) = {x ∈X : vx ̸= 0}.

A partially observable Markov decision process or shortly POMDP is a tuple (S, O, A, α, β, r). We
assume that S, O and A are finite sets which we call state, observation and action space respectively.
We fix a Markov kernel α ∆[S×A] which we call transition mechanism and a kernel β ∆[S] [which]
_∈_ _S_ _∈_ _O_
we call observation mechanism. Further, we consider an instantaneous reward vector r ∈ R[S×A].
We call the system fully observable if β = id[1], in which case the POMDP simplifies to a Markov
_decision process or shortly MDP._

As policies we consider elements π ∆[O] [and call the Markov kernel][ τ][ =][ π][ ◦] _[β][ ∈]_ [∆][S] [its]
_∈_ _A_ _A_
corresponding effective policy. A policy induces transition kernels Pπ ∆[S×A] [and][ p][π][ ∈] [∆][S] [by]
_∈_ _S×A_ _S_

_Pπ(s[′], a[′]_ _s, a) := α(s[′]_ _s, a)(π_ _β)(a[′]_ _s[′])_ and _pπ(s[′]_ _s) :=_ (π _β)(a_ _s)α(s[′]_ _s, a)._
_|_ _|_ _◦_ _|_ _|_ _◦_ _|_ _|_

_aX∈A_

For any initial state distribution µ ∈ ∆S, a policy π ∈ ∆[O]A [defines a Markov process on][ S × A][ with]
transition kernel Pπ which we denote by P[π,µ]. For a discount rate γ ∈ (0, 1) and γ = 1 we define

_∞_ 1 _T −1_

_Rγ[µ][(][π][) :=][ E][P][π,µ]_ (1 _γ)_ _γ[t]r(st, at)_ and _R1[µ][(][π][) := lim]_ _r(st, at)_ _,_
_−_ _T_ _T_
 _t=0_  _→∞_ [E][P][π,µ]  _t=0_ 

X X

called the expected discounted reward and the expected mean reward, respectively. The goal is to
maximize this function over the policy polytope ∆[O]
_A[. For a policy][ π][ we define the][ value function]_
_Vγ[π]_ _γ_ [(][s][) :=][ R]γ[δ][s] [(][π][)][,][ s][ ∈S][, where][ δ][s] [is the Dirac distribution concentrated at][ s][. A]
short calculation shows that[∈] [R][S][ via][ V][ π] _Rγ[µ][(][π][) =][ P]s,a_ _[r][(][s, a][)][η]γ[π,µ](s, a) = ⟨r, ηγ[π,µ]⟩S×A (Zahavy et al., 2021),_

where

(1 _γ)_ _t=0_ _[γ][t][P][π,µ][(][s][t][ =][ s, a][t][ =][ a][)][,]_ if γ (0, 1)

_ηγ[π,µ](s, a) :=_ (lim −T →∞ _T[1]_ _Tt=0 −1_ [P][π,µ][(][s][t][ =][ s, a][t][ =][ a][)][,] if γ = 1 ∈ _._ (1)

Here, ηγ[π,µ] is an element of ∆S×A called[P][∞]P expected (discounted) state-action frequency (Derman,
1970), (discounted) visitation/occupancy measure or on-policy distribution (Sutton & Barto, 2018).
Denoting the state marginal of ηγ[π,µ] by ρ[π,µ]γ ∆ we have ηγ[π,µ](s, a) = ρ[π,µ]γ (s)(π _β)(a_ _s). We_
_∈_ _S_ _◦_ _|_
recall the following well-known facts.

1More generally, the system is fully observable if the supports of _β(_ _s)_ _s_ are disjoint subsets of .
_{_ _·|_ _}_ _∈S_ _O_


-----

**Proposition 1 (Existence of state-action frequencies and rewards). Let (S, O, A, α, β, r) be a**
_POMDP, γ ∈_ (0, 1] and µ ∈ ∆S _. Then ηγ[π,µ], ρ[π,µ]γ_ _and Rγ[µ][(][π][)][ exist for every][ π][ ∈]_ [∆][O]A _[and][ µ][ ∈]_ [∆][S]
_and are continuous in γ ∈_ (0, 1] for fixed π and µ.

For γ = 1 we work under the following standard assumption in the (PO)MDP literature[2].

**Assumption 2 (Uniqueness of stationary disitributions). If γ = 1, we assume that for any policy**
_π_ ∆[O] _[there exists a unique stationary distribution][ η][ ∈]_ [∆][S×A][ of][ P][π][.]
_∈_ _A_

The following proposition shows in particular that for any initial distribution µ, the infinite time
horizon state-action frequency ηγ[π,µ] is the unique discounted stationary distribution of Pπ.

**Proposition 3 (State-action frequencies are discounted stationary). Let (S, O, A, α, β, r) be a**
_POMDP, γ ∈_ (0, 1] and µ ∈ ∆S _. Then ηγ[π,µ]_ _is the unique element in ∆S×A satisfying the dis-_
_counted stationarity equation ηγ[π,µ]_ = γPπ[T] _[η]γ[π,µ]_ + (1 − _γ)(µ ∗_ (π ◦ _β)). Further, ρ[π,µ]γ_ _is the unique_
_element in ∆_ _satisfying ρ[π,µ]γ_ = γp[T]π _[ρ]γ[π,µ]_ + (1 _γ)µ._
_S_ _−_

We denote the set of all state-action frequencies in the fully and in the partially observable case by

_γ[µ]_ [:=] _ηγ[π,µ]_ ∆ _π_ ∆[S] and _γ[µ,β]_ := _ηγ[π,µ]_ ∆ _π_ ∆[O] _._
_N_ _∈_ _S×A |_ _∈_ _A_ _N_ _∈_ _S×A |_ _∈_ _A_

We have seen that the expected cumulative reward function _Rγ[µ][: ∆][O]_
_A_ _[→]_ [R][ factorises according to]

∆[O]A _−f→β_ ∆[S]A _−Ψ−→N[µ]γ_ _γ[µ,β]_ _→_ R, _π 7→_ _π ◦_ _β 7→_ _ηγ[π,µ]_ _7→⟨r, ηγ[π,µ]⟩S×A._

This is illustrated in Figure 1. We make use of this decomposition in two different ways. First,
in Section 3 we study the algebraic properties of the parametrization Ψ[µ]γ [of the set of state-action]
frequencies _γ[µ,β]. In Section 4 we derive a description of_ _γ[µ,β]_ via polynomial inequalities.
_N_ _N_

|Col1|Col2|Col3|
|---|---|---|
|1|||
||π ∆O A||
||||


|Col1|State policies|Col3|
|---|---|---|
||||
|1|||
||∆S A τ π ∆S,β A||
||||


1 1

_π_ ∆[S]A _τπ_ ∆S×A

∆[O] _fβ_ ∆[S][,β] Ψ[µ]γ
_A_ _−linear−−→_ _A_ _−rational−−−→_ _ηγ[π,µ]_

_γ[µ,β]_ _γ[µ]_

0 1 0 1 _N_ _N_

1 _πrational 7→_ _R_ 1 _τrational 7→_ _R_ 1 _η 7→linearR_

0.5 0.5 0.5

0 0 0

0 0 1

0.5 0.5 0

1 1 0.5 0 1 1 0.5 0 -1 -1 0 1


Figure 1: Geometry of a POMDP with two states, two actions and two observations. The top shows
the observation policy polytope ∆[O] [(yellow) along with]
_A[; the associated state policy polytope][ ∆][S]A_
its subset of effective policies ∆[S][,β] (blue); and the corresponding sets of discounted state-action
_A_
frequencies in the simplex ∆ (a tetrahedron in this case). The bottom shows the graph of the
_S×A_
expected cumulative discounted reward R as a function of the observation policy π; the state policy
_τ_ ; and the discounted state-action frequencies η. We characterize the parametrization and geometry
of these domains and the structure of the expected cumulative reward function.

2Assumption 2 is weaker than ergodicity, for which well known criteria exist. For γ < 1 the assumption is
not required, since the discounted stationary distributions are always unique.


-----

3 THE PARAMETRIZATION OF DISCOUNTED STATE-ACTION FREQUENCIES

In this section we show that the discounted state-action frequencies, the value function and the
expected cumulative reward of POMDPs are rational functions and relate their rational degree, which
can be interpreted as a measure of their complexity, to the degree of observability. Here, we say that
a function is a rational function of degree at most k if it is the fraction of two polynomials of degree
at most k. By Cramer’s rule (see Appendix B.2), it holds that

_π_ [)]s[µ]
_ηγ[π,µ](s, a) = (π ◦_ _β)(a|s)ρ[π,µ]γ_ (s) = (π ◦ _β)(a|s) · (1 −_ _γ) ·_ [det(]det([I]I[ −] _[γp]γp[T][T]π_ [)][,]

_−_

where (I _γp[T]π_ [)]s[µ] [denotes the matrix obtained by replacing the][ s][-row of][ I][ −] _[γp]π[T]_ [with][ µ][. Since][ p][π]
_−_
depends linearly on π and the determinant is a polynomial, this is a rational function in the entries
of π. For the degree, we show the following result in Appendix B.1.
**Theorem 4 (Degree of POMDPs). Let (** _,_ _,_ _, α, β, r) be a POMDP, µ_ ∆ _be an initial dis-_
_S_ _O_ _A_ _∈_ _S_
_tribution and γ_ (0, 1) a discount factor. The state-action frequencies ηγ[π,µ] _and ρ[π,µ]γ_ _, the value_
_∈_
_function Vγ[π]_ _[and the expected cumulative reward][ R]γ[µ][(][π][)][ are rational functions with common de-]_
_nominator in the entries of the policy π. Further, if they are restricted to the subset Π_ ∆[O] _[of]_
_⊆_ _A_
_policies which agree with a fixed policy π0 on all states outside of O ⊆O, they have degree at most_
_s ∈S | β(o|s) > 0 for some o ∈_ _O_ _._

Hence, the number of states that are compatible with[] _o determines the algebraic complexity of the_
discounted state-action frequencies, the value function and the reward function. Various refinements
of the theorem are presented in Appendix B.1. For the mean reward case and under an ergodicity
assumption, Grinberg & Precup (2013) showed that the stationary distributions are a rational function of degree of most |S| of the policy. From Theorem 4 we can derive multiple implications (see
also Appendix D.5.2 for implications on the optimization landscape):
**Corollary 5 (Feasible state-action frequencies and value functions form semialgebraic sets). Con-**
_sider a POMDP (_ _,_ _,_ _, α, β, r) and let µ_ ∆ _be an initial distribution and γ_ (0, 1) a
_S_ _O_ _A_ _∈_ _S_ _∈_
_discount factor. The set of discounted state-action frequencies and the set of value functions are_
_semialgebraic sets[3]._

_Proof. By Theorem 4, both sets possess a rational and thus a semialgebraic parametrization and are_
semialgebraic by the Tarski-Seidenberg theorem (Neyman, 2003).

We compute the defining linear and polynomial (in)equalities of the set of feasible state-action frequencies in Section 4 for MDPs and POMDPs respectively, which shows in particular that also
in the mean case the state-action frequencies form a semialgebraic set. The special properties of
degree-one rational functions, which we elaborate in the Appendix B.3, imply the following results.
The first one is a refinement of Dadashi et al. (2019, Lemma 4), stating that linear interpolation
between two policies that differ on a single state leads to a linear interpolation of the corresponding
value functions. We generalize this to state-action frequencies, explicitly compute the interpolation
speed and describe the curves obtained by interpolation between arbitrary policies. Further, our
formulation extends to the mean reward case (see Remark 43).
**Proposition 6. Let (** _,_ _, α, r) be an MDP and γ_ (0, 1). Further, let π0, π1 ∆[S] _[be two]_
_S_ _A_ _∈_ _∈_ _A_
_policies that differ on at most k states. For any λ_ [0, 1] let Vλ R[S] _and ηλ[µ]_
_the discount factorvalue function and state-action frequency belonging to the policy γ, the initial distribution µ and the instantaneous reward ∈_ _∈ π0 + λ(π1 − r[∈]. Then the rationalπ[∆]0)[S×A] with respect to[ denote the]_
_degrees of λ 7→_ _Vλ and λ 7→_ _ηλ are at most k. If they differ on at most one state ˜s ∈S then_

_Vλ = V0 + c(λ) · (V1 −_ _V0)_ _and_ _ηλ[µ]_ [=][ η]0[µ] [+][ c][(][λ][)][ ·][ (][η]1[µ] _[−]_ _[η]0[µ][)]_ _for all λ ∈_ [0, 1],

_where_

det(I _γp1)λ_ _λ[(˜]s)_

_c(λ) = [det(][I][ −]_ _[γp][1][)][λ]_ _−_

3A semialgebraic set is a set defined by a number of polynomial inequalities or a finite union of such sets;det(I − _γpλ) [=]_ (det(I − _γp1) −_ det(I − _γp0))λ + det(I −_ _γp0) [=][ λ][ ·][ ρ]ρ[µ]1[µ][(˜]s)_ _[.]_
for details see Appendix A.2.


-----

In particular, for a blind controller with two actions the set of feasible value functions and the set of
feasible state-action frequencies are pieces of curves with rational parametrization of degree at most
_k = |S|. By Theorem 4, the cumulative reward of (PO)MDPs is a degree-one rational function in_
every row of the (effective) policy. Since degree-one rational functions attain their maximum in a
vertex (Corollary 39), we immediately obtain the existence of an optimal policy which is deterministic on every observation from which the state can be reconstructed, which has been shown using
other methods by Mont´ufar et al. (2015).

**Proposition 7 (Determinism of optimal policies). Let (** _,_ _,_ _, α, β, r) be a POMDP, µ_ ∆ _be_
_S_ _O_ _A_ _∈_ _S_
_an initial distribution and γ_ (0, 1) a discount factor and let π ∆[O] _[be an arbitrary policy and]_
_∈_ _∈_ _A_
_denote the set of observations o such that |{s ∈S | β(o|s) > 0}| ≤_ 1 by O. Then there is a policy
_π˜, which is deterministic on every o ∈_ _O such that Rγ[µ][(˜]π) ≥_ _Rγ[µ][(][π][)][.]_

_Proof. For o ∈_ _O, the reward function restricted to the o-component of the policy is a rational_
function of degree at most one. By Corollary 39 (see Appendix B.3.2), there is a policy ˜π, which is
deterministic on o and satisfies Rγ[µ][(˜]π) ≥ _Rγ[µ][(][π][)][. Iterating over][ o][ ∈]_ _[O][ yields the result.]_

On observations which can be made from more than one state, bounds on the required stochasticity
were established by Mont´ufar & Rauh (2017); Mont´ufar et al. (2019).

4 THE SET OF FEASIBLE DISCOUNTED STATE-ACTION FREQUENCIES

In Corollary 5, we have seen that the state-action frequencies form a semialgebraic set. Now we
aim to describe its defining polynomial inequalities. In the case of full observability, the feasible
state-action freqencies are known to form a polytope (Derman, 1970; Altman & Shwartz, 1991)
which is closely linked to the dual linear programming formulation of MDPs (Hordijk & Kallenberg, 1981), see also Figure 1. We first describe the combinatorial properties of this polytope (see
Appendix C.1.2) and extend the result to the partially observable case, for which we obtain explicit
polynomial inequalities induced by the partial observability under a mild assumption. Most proofs
are postponed to Appendix C. In Section 5 we discuss how the degree of these defining polynomials
allows us to upper bound the number of critical points of the optimization problem. We use the following explicit version of the classic characterization of the state-action frequencies as a polytope
(see Appendix C.1).

**Proposition 8 (Characterization of** _γ[µ][)][.][ Let][ (][S][,][ A][, α, r][)][ be an MDP,][ µ][ ∈]_ [∆][S] _[be an initial distri-]_
_N_
_bution and γ ∈_ (0, 1]. It holds that

_γ[µ]_ [= ∆][S×A] _η_ R[S×A] _wγ[s]_ _[, η][⟩][S×A]_ [= (1][ −] _[γ][)][µ][s]_ _[for][ s][ ∈S]_ (2)
_N_ _[∩]_ _∈_ _| ⟨_

_where wγ[s]_ [:=][ δ][s]  _[can be replaced by][ [0][,][ ∞][)][S×A][ in][ (2)][.]_

_[⊗]_ [1][A] _[−]_ _[γα][(][s][|·][,][ ·][)][. For][ γ][ ∈]_ [(0][,][ 1)][,][ ∆][S×A]

Now we turn towards the partially observable case and introduce the following notation.

**Definition 9 (Effective policy polytope). We call the set of effective policies τ = π** _β_ ∆[S] [the]
_◦_ _∈_ _A_
_effective policy polytope and denote it by ∆[S][,β][.]_
_A_

Note that ∆[S][,β] is indeed a polytope since it is the image of the polytope ∆[O] [under the linear]
_A_ _A_
mapping π _π_ _β = βπ. Hence, we can write it as an intersection ∆[S][,β]_ = ∆[S]
where U _, C ⊆ 7→R[S×A] ◦_ are an affine subspace and a polyhedral cone and describe a finite set of linearA _A_ _[∩U ∩C][,]_
equalities and a finite set of linear inequalities respectively.

**Defining linear inequalities of the effective policy polytope** Obtaining inequality descriptions of
the images of polytopes under linear maps is a fundamental problem that is non-trivial in general. It
can be approached algorithmically, e.g., by Fourier-Motzkin elimination, block elimination, vertex
approaches, and equality set projection (Jones et al., 2004). In the special case where the linear map
is injective, one can give the defining inequalities in closed form as we show in Appendix C.2.1.
Hence, for the purpose of obtaining closed-formulas for the effective policy polytope we make the
following assumption. However, our subsequent analysis in Section 4 can handle any inequalities.

**Assumption 10. The matrix β** ∆[S]
_∈_ _O_ _[⊆]_ [R][S×O][ has linearly independent columns.]


-----

**Remark 11. The assumption above does not imply that the system is fully observable. Recall**
that if β has linearly independent columns, the Moore-Penrose takes the form β[+] = (β[T] _β)[−][1]β[T]_ .
An interesting special case is when β is deterministic but may map several states to the same
observation (this is the partially observed setting considered in numerous works). In this case,
_β[+]_ = diag(n[−]1 [1][, . . ., n][−][1]
_|O|[)][β][T][, where][ n][o][ denotes the number of states with observation][ o][. In this]_
case, βso[+] [agrees with the conditional distribution][ β][(][s][|][o][)][ with respect to a uniform prior over the]
states; however, this is not in general the case since β[+] can have negative entries.

**Theorem 12 (H-description of the effective policy polytope). Let (S, O, A, α, β, r) be a POMDP**
_and let Assumption 10 hold. Then it holds that_

∆[S][,β] = ∆[S] (3)
_A_ _A_ _[∩U ∩C][ =][ U ∩C ∩D][,]_

_where U = {π ◦_ _β | π ∈_ R[S×O]} = ker(β[T] )[⊥] _is a subspace, C = {τ ∈_ R[S×A] _| β[+]τ ≥_ 0}
_is a pointed polyhedral cone and D = {τ ∈_ R[S×A] _|_ _a[(][β][+][τ]_ [)][oa][ = 1][ for all][ o][ ∈O}][ an affine]

_subspace. Further, the face lattices of ∆[O]_ _[and][ ∆][S][,β]_ _are isomorphic._
_A_ _A_

[P]

**Defining polynomial inequalities of the feasible state-action frequencies** In order to transfer
inequalities in ∆[S] [to inequalities in the set of state-action frequencies][ N][ µ]γ [, we use that the inverse]
_A_
of π 7→ _η[π]_ is given through conditioning (see Proposition 46) under the following assumption.

**Assumption 13 (Positivity). Let ρ[π,µ]γ** _> 0 hold entrywise for all policies π ∈_ ∆[S]A[.]

This assumption holds in particular, if either α > 0 and γ > 0 or γ < 1 and µ > 0 entrywise
(see Appendix C.1). Assumption 13 is standard in linear programming approaches and necessary
for the convergence of policy gradient methods in MDPs (Kallenberg, 1994; Mei et al., 2020). By
conditioning, we can translate linear inequalities in ∆[S] [into polynomial inequalities in][ N][ µ]γ [.]
_A_

**Proposition 14 (Correspondence of inequalities). Let (** _,_ _, α, r) be an MDP, τ_ ∆[S] _[and let]_
_S_ _A_ _∈_ _A_
_η_ ∆ _denote its corresponding discounted state-action frequency for some µ_ ∆ _and_
_∈_ _S×A_ _∈_ _S_
_γ ∈_ (0, 1]. Let c ∈ R, b ∈ R[S×A] _and set S := {s ∈S | bsa ̸= 0 for some a ∈A}. Then_


_bsaτsa_ _c_ _implies_
_s,a_ _≥_

X


_ηs′a′_ _c_
_−_
_a[′]_

X


_ηs′a′_ 0,
_≥_
_a[′]_

X


_bsaηsa_


_s[′]∈S_


_s∈S_


_s[′]∈S\{s}_


_where the right is a multi-homogeneous polynomial[4]_ _in the blocks (ηsa)a_ R[A] _with multi-degree_
1S ∈ N[S] _. If further Assumption 13 holds, the inverse implication also holds.∈A ∈_

The preceding proposition shows that the state-action frequencies of a linearly constrained policy
model, where the constraints only address the policy in individual states form a polytope. However,
the effective policy polytope is almost never of this box type (see Remark 55).

**Example 15 (Blind controller). For a blind controller the linear equalities defining the effective**
policy polytope in ∆[S]A [are][ τ][s]1[a][ −] _[τ][s]2[a][ =][ τ]_ [(][a][|][s][1][)][ −] _[τ]_ [(][a][|][s][2][) = 0][ for all][ a][ ∈A][, s][1][, s][2][ ∈S][. They]
thattranslate into the polynomial equalities = _a1, a2_, we obtain _ηs1aρs2 −_ _ηs2aρs1 = 0 for all a ∈A, s1, s2 ∈S. In the case_
_A_ _{_ _}_

0 = ηs1a1 (ηs2a1 + ηs2a1 ) − _ηs2a1_ (ηs1a1 + ηs1a1 ) = ηs1a1 _ηs2a2 −_ _ηs1a2_ _ηs2a1_ for all s1, s2 ∈S,

which is precisely the condition that all 2 × 2 minors of η vanish. Hence, in this case the set of
state-action frequencies Nγ[µ,β] is given as the intersection of Nγ[µ] [of state-action frequencies of the]
associated MDP and the determinantal variety of rank one matrices.

The following result describes the geometry of the set of feasible state-action frequencies.

**Theorem 16. Let (** _,_ _,_ _, α, β, r) be a POMDP, µ_ ∆ _and γ_ (0, 1] and assume that As_S_ _O_ _A_ _∈_ _S_ _∈_
_sumption 13 holds. Then we have_ _γ[µ,β]_ = _γ[µ]_
_multi-homogeneous polynomial equations and N_ _N B is a basic semialgebraic set described by multi-[∩V ∩B][, where][ V][ is a variety described by]_
_homogeneous polynomial inequalities. Further, the face lattices of ∆[S]A[,β]_ _and Nγ[µ,β]_ _are isomorphic._

4A polynomial p : Rn1 _×· · ·×Rnk →_ R is called multi-homogeneous with multi-degree (d1, . . ., dk) ∈ Nk,
if it is homogeneous of degree dj in the j-th block of variables for j = 1, . . ., k.


-----

**Remark 17. The variety V corresponds to the subspace U and the basic semialgebraic set B to the**
cone C from (3). Further, closed form expressions for the defining polynomials can be computed
using Proposition 14 (see also Remark 18). The statement about isomorphic face lattices is in the
sense that ∆[S]A[,β] and Nγ[µ,β] have the same number of surfaces of a given dimension with the same
neighboring properties. This can be seen in Figure 1, where the effective policy polytope and the set
of state-action frequencies both have four vertices, four edges, and one two-dimensional face.

**Remark 18. By Theorem 12 and Proposition 14, the defining polynomials of the basic semialge-**
braic set B from Theorem 16 are indexed by a ∈A, o ∈O and are given by

_pao(η) :=_ _βos[+]_ _[η][sa]_ _ηs′a′_ = _βos[+]_ _[′]_ _ηsf_ (s) 0, (4)

_sX∈So_  _s[′]∈SYo\{s}_ Xa[′]  _f : SXo→A_  _s[′]∈fX[−][1]({a})_  Ys∈So _≥_

where So := _s_ _βos[+]_
have |So||A|[|S] {[o][|−] ∈S |[1] monomials of degree[̸][= 0][}][. The polynomials depend only on] |So| of the form _s∈So_ _[η][sf][ β][(][s][ and not on][)][ for some][ f][ γ][ :][,][ S][ µ][o][ nor][ →A][ α][, and][. In]_

particular, we can read of the multi-degree of pao with respect to the blocks (ηsa)a which is given
by 1So (see also Proposition 14). A complete description of the set[Q] _γ[µ,β]_ via (in)equalities follows∈A
_N_
from the description of Nγ[µ] [via linear (in)equalities given in (2). In Section 5 we discuss how the]
degree of these polynomials controls the complexity of the optimization problem.
**Remark 19 (Planning in POMDPs as a polynomial optimization problem). The semialgebraic de-**
scription of the set _γ[µ,β]_ of feasible state-action distributions allows us to reformulate the reward
_N_
maximization as a polynomially constrained optimization problem with linear objective (see also
Remark 59 and Algorithm 1). This reformulation allows the use of constrained optimization algorithms, which we demonstrate in Appendix F on the toy example of Figure 1 and a grid world. Note
that this polynomial program is different to the quadratic program obtained by Amato et al. (2006).

5 NUMBER AND LOCATION OF CRITICAL POINTS

Although the reward function of MDPs is non convex, it still exhibits desirable properties from a
standpoint of optimization. For example, without any assumptions, every policy can be continuously
connected to an optimal policy by a path along which the reward is monotone (see Appendix D.5).
Under mild conditions, all policies which are critical points of the reward function are globally
optimal (Bhandari & Russo, 2019). In partially observable systems, the situation is fundamentally
different. In this case, suboptimal local optima of the reward function can exist as can be seen
in Figure 1 (see also Poupart et al., 2011; Bhandari & Russo, 2019). In the following we use the
geometric description of the discounted state-action frequencies to study the number and location of
critical points. These are important properties of the optimization problem and have implications on
the required stochasticity of optimal policies. In Appendix D we discuss the mean reward case and
an example and describe the sublevelsets as semialgebraic sets.

We regard the reward as a linear function p0 over the set of feasible state-action frequencies Nγ[µ,β].
Under Assumption 13 π 7→ _η[π]_ is injective and has a full-rank Jacobian everywhere (see Appendix C.1.1). Hence, the critical points in the policy polytope ∆[O] [correspond to the critical points]
_A_
of p0 on Nγ[µ,β] (see Trager et al., 2019). In general, critical points of this linear function can occur on
every face of the semialgebraic set _γ[µ,β]. The optimization problem thus has a combinatorial and a_
_N_
geometric component, corresponding to the number of faces of each dimension and the number of
critical points in the relative interior of any given face. We have discussed the combinatorial part in
Theorem 16 and focus now on the geometric part. Writing _γ[µ,β]=_ _η_ R[S×A] _pi(η)_ 0, i _I_,
_N_ _{_ _∈_ _|_ _≤_ _∈_ _}_
we are interested in the number of critical points on the interior of a face,

int(FJ ) = _η_ _γ[µ,β]_ _pj(η) = 0 for j_ _J, pi(η) > 0 for i_ _I_ _J_ _._
_{_ _∈N_ _|_ _∈_ _∈_ _\_ _}_

Note that a point η is critical on int(FJ ), if and only if it is a critical point on the variety VJ :=
_{η ∈_ R[S×A] _| pj(η) = 0 for j ∈_ _J}. For the sake of notation we write J = {1, . . ., m}. We can_
bound the number of critical points in the interior of the face by the number of critical points of the
polynomial optimization problem of optimizing p0(η) subject to p1(η) = = pm(η) = 0. This
_· · ·_
number is upper bounded by the algebraic degree of the problem which controls also the (algebraic)
complexity of optimal policies (see Appendix D.1 for details). Using Theorem 12, Proposition 14
and an upper bound on the algebraic degree of polynomial optimization by Nie & Ranestad (2009)
yields the following result.


-----

**Theorem 20. Consider a POMDP (S, O, A, α, β, r), γ ∈** (0, 1), assume that r is generic, that
_β ∈_ R[S×O] _is invertible, and that Assumption 13 holds. For any given I ⊆A × O consider the_
_following set of policies, which is the relative interior of a face of the policy polytope:_

int(F ) = _π_ ∆[O] _._
_∈_ _A_ _[|][ π][(][a][|][o][) = 0][ if and only if][ (][a, o][)][ ∈]_ _[I]_

_Let O :=_ _o_ (a, o) _I for some a_ _and set ko :=_ _a_ (a, o) _I_ _as well as do :=_ _s_
_{_ _∈O |_ _∈_ _}_ _|{_ _|_ _∈_ _}|_ _|{_ _|_
_βos[−][1]_

_[̸][= 0][}|][. Then, the number of critical points of the reward function on][ int(][F]_ [)][ is at most]


_d[k]o[o]_

Yo∈O


(do 1)[i][o] _,_ (5)
_oY∈O_ _−_


_o∈O_ _[i][o][=][l]_


_where l = |S|(|A| −_ 1) −|I|. If α and µ are generic, this bound can be refined by computing the
_polar degrees of multi-homogeneous varieties (see Proposition 21 for a special case). The same_
_bound holds in the mean reward case γ = 1 for l given in Remark 61._

By results from Mont´ufar & Rauh (2017) a POMDP has optimal memoryless stochastic policies with
_| supp π(·|o)| ≤_ _lo, where lo = |supp β(o|·)| ≥_ 1. Hence, we may restrict attention to optimization
over Nγ[µ,β] with k = _o∈O_ _[k][o][ active inequalities (zeros in the policy), where][ k][o][ = max][{|A|−][l][o][,][ 0][}][.]_

Over these faces of the feasible set, the algebraic degree of the reward maximization problem is
upper bounded by [P]o∈O _[d]o[k][o]_ _i1+···+io=|S|(|A|−1)−k_ _o∈O[(][d][o][ −]_ [1)][i][o][ due to Theorem 20.]

In the special case of MDPs the bound shows that for MDPs only deterministic policies can becritical points of the reward function (see Corollary 62). Setting[Q] P Q _I := ∅_ shows that there are no
critical points in the interior of the policy polytope ∆[O]
_A[. This requires the assumption that][ β][ is]_
invertible (see Appendix D.4). The bound in Theorem 20 neglects the specific algebraic structure of
the problem, and can be refined by considering polar degrees of determinantal varieties. This yields
the following tighter upper bound for a blind controller with two actions (see Appendix D.3).
**Proposition 21 (Number of critical points in a blind controller). Let (S, O, A, α, β, r) be a POMDP**
_describing a blind controller with two actions, i.e.,_ = _o_ _and_ = _a1, a2_ _and let r, α and µ_
_O_ _{_ _}_ _A_ _{_ _}_
_be generic and let γ ∈_ (0, 1). Then the reward function Rγ[µ] _[has at most][ |S|][ critical points in the]_
_interior int(∆[O]_ = (0, 1) of the policy polytope and hence at most + 2 critical points.
_A[)][ ∼]_ _|S|_

In Appendix D.4 we provide examples of blind controllers which have several critical points in
the interior (0, 1) = int(∆[O] = ∆[O]

_[∼]_ _A[)][ and strict maxima at the two endpoints of the interval][ [0][,][ 1]][ ∼]_ _A_
respectively. Such points are called smooth and non-smooth critical points respectively.

6 CONCLUSION

We described geometric and algebraic properties of POMDPs and related the rational degree of the
discounted state-action frequencies and the expected cumulative reward function to the degree of
observability. We described the set of feasible state-action frequencies as a basic semialgebraic
set and computed explicit expressions for the defining polynomials. In particular, this yields a
polynomial programming formulation of POMDPs extending the linear programming formulation
of MDPs. Based on this we use polynomial optimization theory to bound the number of critical
points of the reward function over the polytope of memoryless stochastic policies. Our analysis
also yields insights into the optimization landscape, such as the number of connected components
of superlevel sets of the expected reward. Finally, we use a navigation problem in a grid world
to demonstrate that the polynomial programming formulation can offer a computationally feasible
approach to the reward maximization problem.

Our analysis focuses on infinite-horizon problems and memoryless policies with finite state, observation, and action spaces. Continuous spaces are interesting avenues, since they occur in real world
application like robotics. The general bound on the number of critical points in Theorem 20 does
not exploit the special multi-homogeneous structure of the problem, which could allow for tighter
bounds as illustrated in Proposition 21 for blind controllers. Computing polar degrees is a challenging problem that remains to be studied using more sophisticated algebraic tools. Possible extensions
of our work include the generalization to policies with finite memories as sketched in Appendix E.1.
Further, we believe that it is interesting to explore to what extent our results can be used to identify
policy classes guaranteed to contain maximizers of the reward in POMDPs.


-----

ACKNOWLEDGMENTS

The authors thank Alex Tong Lin and Thomas Merkh for valuable discussions on POMDPs, Bernd
Sturmfels for sharing his expertise on algebraic degrees and Mareike Dressler, Marina GarroteL´opez and Kemal Rose for their discussions on polynomial optimization. The authors acknowledge
support by the ERC under the European Union’s Horizon 2020 research and innovation programme
(grant agreement no 757983). JM received support from the International Max Planck Research
School for Mathematics in the Sciences and the Evangelisches Studienwerk Villigst e.V..

REFERENCES

Eitan Altman and Adam Shwartz. Markov decision problems and state-action frequencies. SIAM
_journal on control and optimization, 29(4):786–809, 1991._

Christopher Amato, Daniel S Bernstein, and Shlomo Zilberstein. Solving pomdps using quadratically constrained linear programs. In Proceedings of the fifth international joint conference on
_Autonomous agents and multiagent systems, pp. 341–343, 2006._

Miguel F Anjos and Jean B Lasserre. Handbook on semidefinite, conic and polynomial optimization,
volume 166. Springer Science & Business Media, 2011.

Kamyar Azizzadenesheli, Alessandro Lazaric, and Animashree Anandkumar. Open problem: Approximate planning of POMDPs in the class of memoryless policies. In Conference on Learning
_Theory, pp. 1639–1642. PMLR, 2016._

Kamyar Azizzadenesheli, Yisong Yue, and Animashree Anandkumar. Policy Gradient in Partially
Observable Environments: Approximation and Convergence. arXiv:1810.07900, 2018.

Chanderjit Bajaj. The Algebraic Degree of Geometric Optimization Problems. Discrete & Compu_tational Geometry, 3(2):177–191, 1988._

Serguei Barannikov, Alexander Korotin, Dmitry Oganesyan, Daniil Emtsev, and Evgeny Burnaev.
Barcodes as summary of loss function’s topology. arXiv:1912.00043, 2019.

Saugata Basu. Different Bounds on the Different Betti Numbers of Semi-Algebraic Sets. Discrete
_and Computational Geometry, 30(1):65–85, 2003._

Saugata Basu. Algorithms in Real Algebraic Geometry: A Survey. arXiv:1409.1534, 2014.

Saugata Basu, Richard Pollack, and Marie-Franc¸oise Roy. Algorithms in Real Algebraic Geometry
_(Algorithms and Computation in Mathematics). Springer-Verlag, Berlin, Heidelberg, 2006._

Jonathan Baxter and Peter L Bartlett. Infinite-Horizon Policy-Gradient Estimation. _Journal of_
_Artificial Intelligence Research, 15:319–350, 2001._

Jonathan Baxter, Peter L Bartlett, et al. Reinforcement Learning in POMDP’s via Direct Gradient
Ascent. In ICML, pp. 41–48. Citeseer, 2000.

Richard Bellman. A Markovian decision process. Journal of mathematics and mechanics, 6(5):
679–684, 1957.

Jalaj Bhandari and Daniel Russo. Global Optimality Guarantees For Policy Gradient Methods.
_arXiv:1906.01786, 2019._

Jacek Bochnak, Michel Coste, and Marie-Franc¸oise Roy. Real Algebraic Geometry, volume 36.
Springer Science & Business Media, 2013.

Paul Breiding, T¨urk¨u Ozl¨[¨] um C¸ elik, Timothy Duff, Alexander Heaton, Aida Maraj, Anna-Laura
Sattelberger, Lorenzo Venturello, and O˘guzhan Y¨ur¨uk. Nonlinear Algebra and Applications.
_arXiv:2103.16300, 2021._

Michael J Catanzaro, Justin M Curry, Brittany Terese Fasy, J¯anis Lazovskis, Greg Malen, Hans
Riess, Bei Wang, and Matthew Zabka. Moduli spaces of morse functions for persistence. Journal
_of Applied and Computational Topology, 4(3):353–385, 2020._


-----

Krishnendu Chatterjee, Martin Chmel´ık, and Mathieu Tracol. What is decidable about partially
observable Markov decision processes with ω-regular objectives. Journal of Computer and System
_[Sciences, 82(5):878–911, 2016. URL https://www.sciencedirect.com/science/](https://www.sciencedirect.com/science/article/pii/S0022000016000246)_
[article/pii/S0022000016000246.](https://www.sciencedirect.com/science/article/pii/S0022000016000246)

Victor Cohen and Axel Parmentier. Linear Programming for Decision Processes with Partial Information. arXiv:1811.08880, 2018.

Robert Dadashi, Adrien Ali Taiga, Nicolas Le Roux, Dale Schuurmans, and Marc G Bellemare.
The value function polytope in reinforcement learning. In International Conference on Machine
_Learning, pp. 1486–1495. PMLR, 2019._

Guy De Ghellinck. Les problemes de decisions sequentielles. _Cahiers du Centre d’Etudes de_
_Recherche Op´erationnelle, 2(2):161–179, 1960._

Francois d’Epenoux. A Probabilistic Production and Inventory Problem. Management Science, 10
(1):98–108, 1963.

Cyrus Derman. Finite state Markovian decision processes. Academic Press, 1970.

Joseph Leo Doob. Stochastic processes, volume 10. New York Wiley, 1953.

Jan Draisma, Emil Horobet¸, Giorgio Ottaviani, Bernd Sturmfels, and Rekha R. Thomas. The Euclidean distance degree of an algebraic variety. Foundations of Computational Mathematics, 16
[(1):99–149, 2016. URL https://doi.org/10.1007/s10208-014-9240-x.](https://doi.org/10.1007/s10208-014-9240-x)

Dean Gillette. 9. stochastic games with zero stop probabilities. In Contributions to the Theory of
_Games (AM-39), Volume III, pp. 179–188. Princeton University Press, 1958._

D Yu Grigor’ev and NN Vorobjov. Counting connected components of a semialgebraic set in subexponential time. Computational Complexity, 2(2):133–186, 1992.

Yuri Grinberg and Doina Precup. Average Reward Optimization Objective In Partially Observable
Domains. In International Conference on Machine Learning, pp. 320–328. PMLR, 2013.

J William Helton and Victor Vinnikov. Linear matrix inequality representation of sets. Communica_tions on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathemat-_
_ical Sciences, 60(5):654–674, 2007._

A Hordijk and LCM Kallenberg. Linear Programming Methods for Solving Finite Markovian Decision Problems . In DGOR, pp. 468–482. Springer, 1981.

Ronald A Howard. Dynamic programming and Markov processes. MIT Press, 1960.

Jeffrey J. Hunter. Chapter 2 - generating functions. In Jeffrey J. Hunter (ed.), Mathematical
_[Techniques of Applied Probability, pp. 24–67. Academic Press, 1983. URL https://www.](https://www.sciencedirect.com/science/article/pii/B9780123618016500083)_
[sciencedirect.com/science/article/pii/B9780123618016500083.](https://www.sciencedirect.com/science/article/pii/B9780123618016500083)

Rodrigo Toro Icarte, Richard Valenzano, Toryn Q. Klassen, Phillip Christoffersen, Amir massoud Farahmand, and Sheila A. McIlraith. The act of remembering: A study in partially observable reinforcement learning, 2021. [URL https://openreview.net/forum?id=](https://openreview.net/forum?id=uFkGzn9RId8)
[uFkGzn9RId8.](https://openreview.net/forum?id=uFkGzn9RId8)

Tommi Jaakkola, Satinder Singh, and Michael Jordan. Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems. In G. Tesauro, D. Touretzky, and T. Leen (eds.), Advances in Neural Information Processing Systems, volume 7.
MIT Press, 1995. [URL https://proceedings.neurips.cc/paper/1994/file/](https://proceedings.neurips.cc/paper/1994/file/1c1d4df596d01da60385f0bb17a4a9e0-Paper.pdf)
[1c1d4df596d01da60385f0bb17a4a9e0-Paper.pdf.](https://proceedings.neurips.cc/paper/1994/file/1c1d4df596d01da60385f0bb17a4a9e0-Paper.pdf)

Colin Jones, E. C. Kerrigan, and Jan Maciejowski. Equality Set Projection: A new algorithm for
the projection of polytopes in halfspace representation. Technical report, Cambridge, 2004. URL
[http://publications.eng.cam.ac.uk/327023/.](http://publications.eng.cam.ac.uk/327023/)

Leslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. Planning and acting in
partially observable stochastic domains. Artificial intelligence, 101(1-2):99–134, 1998.


-----

Lodewijk CM Kallenberg. Survey of linear programming for standard and nonstandard Markovian
control problems. Part I: Theory. Zeitschrift f¨ur Operations Research, 40(1):1–42, 1994.

Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The
_International Journal of Robotics Research, 32(11):1238–1274, 2013._

HT Kung. The computational complexity of algebraic numbers. In Proceedings of the fifth annual
_ACM symposium on Theory of computing, pp. 152–159, 1973._

Amy N. Langville and William J. Stewart. The Kronecker product and stochastic automata networks.
_[Journal of Computational and Applied Mathematics, 167(2):429–447, 2004. URL https://](https://www.sciencedirect.com/science/article/pii/S0377042703009312)_
[www.sciencedirect.com/science/article/pii/S0377042703009312.](https://www.sciencedirect.com/science/article/pii/S0377042703009312)

Jean Bernard Lasserre. An introduction to polynomial and semi-algebraic optimization, volume 52.
Cambridge University Press, 2015.

Yanjie Li, Baoqun Yin, and Hongsheng Xi. Finding optimal memoryless policies of POMDPs
under the expected average reward criterion. European Journal of Operational Research, 211(3):
[556–567, 2011. URL https://www.sciencedirect.com/science/article/pii/](https://www.sciencedirect.com/science/article/pii/S0377221710008805)
[S0377221710008805.](https://www.sciencedirect.com/science/article/pii/S0377221710008805)

Michael L. Littman. An optimization-based categorization of reinforcement learning environments. In Jean-Arcady Meyer, Herbert L. Roitblat, and Stewart W. Wilson (eds.), From Ani_[mals to Animats 2, pp. 262–270. MIT Press, 1993. URL http://www.cs.rutgers.edu/](http://www.cs.rutgers.edu/~mlittman/papers/sab92.giveout.ps)_
[˜[mlittman/papers/sab92.giveout.ps][.]](http://www.cs.rutgers.edu/~mlittman/papers/sab92.giveout.ps)

Michael L. Littman. Memoryless policies: Theoretical limitations and practical results. In Proceed_ings of the Third International Conference on Simulation of Adaptive Behavior: From Animals to_
_Animats 3: From Animals to Animats 3, SAB94, pp. 238–245. MIT Press, 1994._

John Loch and Satinder P. Singh. Using Eligibility Traces to Find the Best Memoryless Policy in
Partially Observable Markov Decision Processes. In Proceedings of the Fifteenth International
_Conference on Machine Learning, ICML ’98, pp. 323–331, San Francisco, CA, USA, 1998._
Morgan Kaufmann Publishers Inc.

Omid Madani, Steve Hanks, and Anne Condon. On the undecidability of probabilistic planning and related stochastic optimization problems. _Artificial Intelligence, 147(1):_
5–34, 2003. [URL https://www.sciencedirect.com/science/article/pii/](https://www.sciencedirect.com/science/article/pii/S0004370202003788)
[S0004370202003788. Planning with Uncertainty and Incomplete Information.](https://www.sciencedirect.com/science/article/pii/S0004370202003788)

Alan S Manne. Linear Programming and Sequential Decisions. Management Science, 6(3):259–267,
1960.

Peter McMullen and Egon Schulte. Abstract regular polytopes, volume 92. Cambridge University
Press, 2002.

Jincheng Mei, Chenjun Xiao, Csaba Szepesvari, and Dale Schuurmans. On the global convergence
rates of softmax policy gradient methods. In International Conference on Machine Learning, pp.
6820–6829. PMLR, 2020.

George E. Monahan. A survey of partially observable Markov decision processes: Theory, models,
[and algorithms. Management Science, 28(1):1–16, 1982. URL http://www.jstor.org/](http://www.jstor.org/stable/2631070)
[stable/2631070.](http://www.jstor.org/stable/2631070)

Guido Mont´ufar and Johannes Rauh. Geometry of Policy Improvement. In International Conference
_on Geometric Science of Information, pp. 282–290. Springer, 2017._

Guido Mont´ufar, Keyan Ghazi-Zahedi, and Nihat Ay. Geometry and Determinism of Optimal Stationary Control in Partially Observable Markov Decision Processes. arXiv:1503.07206, 2015.

Guido Mont´ufar, Johannes Rauh, and Nihat Ay. Task-agnostic constraining in average reward
[POMDPs. In Task-agnostic reinforcement learning Workshop at ICLR 2019. 2019. URL https:](https://tarl2019.github.io/assets/papers/montufar2019taskagnostic.pdf)
[//tarl2019.github.io/assets/papers/montufar2019taskagnostic.pdf.](https://tarl2019.github.io/assets/papers/montufar2019taskagnostic.pdf)


-----

Kevin P. Murphy. A Survey of POMDP Solution Techniques. Environment, 2, 10 2000.

Tim Netzer and Andreas Thom. Polynomials with and without determinantal representations. Linear
_algebra and its applications, 437(7):1579–1595, 2012._

Abraham Neyman. Real Algebraic Tools in Stochastic Games. In Stochastic games and applica_tions, pp. 57–75. Springer, 2003._

Jiawang Nie and Kristian Ranestad. Algebraic Degree of Polynomial Optimization. SIAM Journal
_[on Optimization, 20(1):485–502, 2009. URL https://doi.org/10.1137/080716670.](https://doi.org/10.1137/080716670)_

Christos H Papadimitriou and John N Tsitsiklis. The complexity of Markov decision processes.
_Mathematics of operations research, 12(3):441–450, 1987._

Leonid Peshkin, Nicolas Meuleau, and Leslie Pack Kaelbling. Learning Policies with External
Memory. In Proceedings of the 16th International Conference on Machine Learning, pp. 307–
314. Morgan Kaufmann, 1999.

Pascal Poupart, Tobias Lang, and Marc Toussaint. Analyzing and escaping local optima in planning as inference for partially observable domains. In Joint European Conference on Machine
_Learning and Knowledge Discovery in Databases, pp. 613–628. Springer, 2011._

Martin L Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John
Wiley & Sons, 2014.

Johannes Rauh, Nihat Ay, and Guido Mont´ufar. A continuity result for optimal memoryless planning
in pomdps. 2021.

Jesus M Ruiz. Semialgebraic and semianalytic sets. _Cahiers du s´eminaire d’histoire des_
_math´ematiques, 1:59–70, 1991._

Satinder P Singh, Tommi Jaakkola, and Michael I Jordan. Learning without state-estimation in
partially observable Markovian decision processes. In Machine Learning Proceedings 1994, pp.
284–292. Elsevier, 1994.

Lucca Sodomaco. The Distance Function from the Variety of partially symmetric rank-one Tensors.
PhD thesis, University of Florence, Department of Mathematics and Computer Science, 2020.

Pierre-Jean Spaenlehauer. Solving multi-homogeneous and determinantal systems: algorithms, com_plexity, applications. PhD thesis, Universit´e Pierre et Marie Curie (Univ. Paris 6), 2012._

Richard S Sutton and Andrew G Barto. Reinforcement Learning: An Introduction. MIT press, 2018.

Richard S Sutton, David A McAllester, Satinder P Singh, Yishay Mansour, et al. Policy Gradient
Methods for Reinforcement Learning with Function Approximation. In NIPs, volume 99, pp.
1057–1063. Citeseer, 1999.

Gerald Tesauro. Temporal Difference Learning and TD-Gammon. Commun. ACM, 38(3):58–68,
[March 1995. URL https://doi.org/10.1145/203330.203343.](https://doi.org/10.1145/203330.203343)

Sascha Timme. Numerical Nonlinear Algebra. PhD thesis, Technische Universit¨at Berlin (Germany), 2021.

Matthew Trager, Kathl´en Kohn, and Joan Bruna. Pure and Spurious Critical Points: a Geometric
Study of Linear Networks. In International Conference on Learning Representations, 2019.

Nikos Vlassis, Michael L Littman, and David Barber. On the Computational Complexity of Stochastic Controller Optimization in POMDPs. ACM Transactions on Computation Theory (TOCT), 4
(4):1–8, 2012.

Robert Vrabel. A note on the matrix determinant lemma. International Journal of Pure and Applied
_Mathematics, 111(4):643–646, 2016._

Stephan Wilhelm Weis. Exponential Families with Incompatible Statistics and Their Entropy Dis_tance. Friedrich-Alexander-Universit¨at Erlangen-N¨urnberg (Germany), 2010._


-----

John Williams and Satinder Singh. Experimental Results on Learning Stochastic Memoryless Policies for Partially Observable Markov Decision Processes. In M. Kearns, S. Solla,
and D. Cohn (eds.), Advances in Neural Information Processing Systems, volume 11.
MIT Press, 1999. [URL https://proceedings.neurips.cc/paper/1998/file/](https://proceedings.neurips.cc/paper/1998/file/1cd3882394520876dc88d1472aa2a93f-Paper.pdf)
[1cd3882394520876dc88d1472aa2a93f-Paper.pdf.](https://proceedings.neurips.cc/paper/1998/file/1cd3882394520876dc88d1472aa2a93f-Paper.pdf)

Tom Zahavy, Brendan O’Donoghue, Guillaume Desjardins, and Satinder Singh. Reward is enough
for convex MDPs. arXiv:2106.00661, 2021.

G¨unter M Ziegler. Lectures on Polytopes, volume 152. Springer Science & Business Media, 2012.

Piotr Zwiernik. Semialgebraic statistics and latent tree models. _Monographs on Statistics and_
_Applied Probability, 146:146, 2016._

Karl Johan Astr¨[˚] om. Optimal Control of Markov Processes with Incomplete State Information. Jour_[nal of Mathematical Analysis and Applications, 10:174–205, 1965. URL https://lup.lub.](https://lup.lub.lu.se/search/ws/files/5323668/8867085.pdf)_
[lu.se/search/ws/files/5323668/8867085.pdf.](https://lup.lub.lu.se/search/ws/files/5323668/8867085.pdf)

T¨urk¨u Ozl¨[¨] um C¸ elik, Asgar Jamneshan, Guido Mont´ufar, Bernd Sturmfels, and Lorenzo Venturello. Wasserstein distance to independence models. Journal of Symbolic Computation, 104:
[855–873, 2021. URL https://www.sciencedirect.com/science/article/pii/](https://www.sciencedirect.com/science/article/pii/S0747717120301152)
[S0747717120301152.](https://www.sciencedirect.com/science/article/pii/S0747717120301152)


-----

APPENDIX

The Sections A–D of the Appendix correspond to the Sections 2–5 of the main body. We present the
postponed proofs and elaborate various remarks in more detail. In Appendix E we discuss possible
extensions of our results to memory policies and polynomial POMDPs. In Appendix F we provide
details on the example plotted in Figure 1 and provide a plot of a three dimensional state-action
frequency set.

**A Details on the Preliminaries** **15**

A.1 Partially observable Markov decision processes . . . . . . . . . . . . . . . . . . . 15

A.2 Semialgebraic sets and their face lattices . . . . . . . . . . . . . . . . . . . . . . . 17

**B** **Details on the Parametrizaton of Discounted State-Action Frequencies** **17**

B.1 The degree of determinantal polynomials . . . . . . . . . . . . . . . . . . . . . . 17

B.2 The degree of POMDPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19

B.3 Properties of degree-one rational functions . . . . . . . . . . . . . . . . . . . . . . 20

**C Details on the Geometry of State-Action Frequencies** **22**

C.1 The fully observable case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

C.2 The partially observable case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26

**D Details on the Optimization** **30**

D.1 Introduction to algebraic degrees . . . . . . . . . . . . . . . . . . . . . . . . . . . 31

D.2 General upper bound on the number of critical points . . . . . . . . . . . . . . . . 32

D.3 Number of critical points in a two-action blind controller . . . . . . . . . . . . . . 34

D.4 Examples with multiple smooth and non-smooth critical points . . . . . . . . . . . 36

D.5 (Super)level sets of (PO)MDPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

**E** **Possible Extensions** **38**

E.1 Application to finite memory policies . . . . . . . . . . . . . . . . . . . . . . . . 38

E.2 Polynomial POMDPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38

**F** **Examples** **39**

F.1 Toy example of Figure 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

F.2 Navigation in a grid world . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

F.3 A three dimensional example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44

A DETAILS ON THE PRELIMINARIES

We elaborate the proofs that where ommited or only sketched in the main body.

A.1 PARTIALLY OBSERVABLE MARKOV DECISION PROCESSES

The statement of Proposition 1 can found in the work by Howard (1960) and we quickly sketch
the proof therein. In order to show that the expected state-action frequencies exist without any


-----

assumptions, we recall that for a (row or column) stochastic matrix P, the Ces`aro mean is defined
by


_T −1_

_P_ _[t]_

_t=0_

X


_P_ _[∗]_ := limT →∞


and exists without any assumptions. Further, P _[∗]_ is the projection onto the subspace of stationary
distribution (Doob, 1953). For γ ∈ (0, 1), the matrix


_γ[t]P_ _[t]_ = (1 − _γ)(I −_ _γP_ )[−][1]
_t=0_

X


_Pγ[∗]_ [:= (1][ −] _[γ][)]_


is known as the Abel mean of P, where we used the Neumann series. By the Tauberian theorem, it
holds that Pγ[∗] _[→]_ _[P][ ∗]_ [for][ γ][ ↗] [1][ (Gillette, 1958; Hunter, 1983).]

**Proposition 1 (Existence of state-action frequencies and rewards). Let (S, O, A, α, β, r) be a**
_POMDP, γ ∈_ (0, 1] and µ ∈ ∆S _. Then ηγ[π,µ], ρ[π,µ]γ_ _and Rγ[µ][(][π][)][ exist for every][ π][ ∈]_ [∆][O]A _[and][ µ][ ∈]_ [∆][S]
_and are continuous in γ ∈_ (0, 1] for fixed π and µ.

_Proof. The existence of the state-action frequencies as well as the continuity with respect to the_
discount parameter follows directly from the general theory since

_ηγ[π,µ]_ = (Pπ[T] [)]γ[∗] [(][µ][ ∗] [(][π][ ◦] _[β][))]_

for γ ∈ (0, 1) and η1[π,µ] = (Pπ[T] [)][∗][(][µ][ ∗] [(][π][ ◦] _[β][))][. With an analogue argument, the statement follows]_
for the state frequencies and for the reward.

For γ = 1 we work under the following standard assumption in the (PO)MDP literature[5].

**Assumption 2 (Uniqueness of stationary disitributions). If γ = 1, we assume that for any policy**
_π_ ∆[O] _[there exists a unique stationary distribution][ η][ ∈]_ [∆][S×A][ of][ P][π][.]
_∈_ _A_

The following proposition shows in particular that for any initial distribution µ, the infinite time
horizon state-action frequency η1[π,µ] is the unique stationary distribution of Pπ.

**Proposition 3 (State-action frequencies are discounted stationary). Let (S, O, A, α, β, r) be a**
_POMDP, γ ∈_ (0, 1] and µ ∈ ∆S _. Then ηγ[π,µ]_ _is the unique element in ∆S×A satisfying the dis-_
_counted stationarity equation ηγ[π,µ]_ = γPπ[T] _[η]γ[π,µ]_ + (1 − _γ)(µ ∗_ (π ◦ _β)). Further, ρ[π,µ]γ_ _is the unique_
_element in ∆_ _satisfying ρ[π,µ]γ_ = γp[T]π _[ρ]γ[π,µ]_ + (1 _γ)µ._
_S_ _−_

_Proof. By the general theory of Ces`aro means, (Pπ[T]_ [)][∗] [projects onto the space of stationary distribu-]
tions and hence the η1[π,µ] = (Pπ[T] [)][∗][(][µ][ ∗] [(][π][ ◦] _[β][))][ is stationary. Hence, by Assumption 2,][ η]1[π,µ]_ is the
unique stationary distribution. For γ ∈ (0, 1) we have

_ηγ[π,µ]_ = (Pπ[T] [)]γ[∗] [(][µ][ ∗] [(][π][ ◦] _[β][)) = (][I][ −]_ _[γP]π[ T]_ [)][−][1][(][µ][ ∗] [(][π][ ◦] _[β][))][,]_

which yields the claim. For the state distributions ρ[π,µ]γ the claim follows analogously or by marginalisation.

Since the state-action frequencies satisfy this generalized stationarity condition, we sometimes refer
to them as discounted stationary distributions.

5Assumption 2 is weaker than ergodicity and is satisfied whenever the Markov chain with transition kernel
_P_ _[π]_ is irreducible and aperiodic for every policy π, e.g., when the transition kernel satisfies α > 0. For
_γ ∈_ (0, 1) the assumption is not required, since the discounted stationary distributions are always unique since
_I −_ _γPπ is invertible because the spectral norm of Pπ is one._


-----

A.2 SEMIALGEBRAIC SETS AND THEIR FACE LATTICES

We recall the definition of semialgebraic sets, which are fundamental objects in real algebraic geometry (Bochnak et al., 2013). A basic (closed) semialgebraic set A is a subset of R[d] defined by
finitely many polynomial inequalities as

_A = {x ∈_ R[d] _| pi(x) ≥_ 0 for i ∈ _I},_

where I is a finite index set and the pi are polynomials. A semialgebraic set is a finite union
of basic (not necessarily closed) semialgebraic sets, and a function is called semialgebraic if its
graph is semialgebraic. By the Tarski-Seidenberg theorem the range of a semialgebraic function
is semialgebraic. A simple algebraic set has a lattice associated to it, induced by the set of active
inequalities. More precisely, for a subset J ⊆ _I we set FJ := {x ∈_ _A | pj(x) = 0 for j ∈_ _J} and_
endow the set := _FJ_ _J_ _I_ with the partial order of inclusion. We call the elements of
the faces of A F. The faces described above are a generalization of the faces of a classical polytope { _|_ _⊆_ _}_ _F_
and a special case of the faces of an abstract polytope and we refer to McMullen & Schulte (2002)
for more details. Next, we want to endow this partially ordered set with more structure. A lattice F
carries two operations, the join ∧ and the meet ∨, which satisfy the absortion laws F ∨ (F ∧ _G) = F_
and F ∧ (F ∨ _G) = F for all F, G ∈F. In the face lattice of a basic semialgebraic set, the join and_
meet are given by


_F ∧_ _G := F ∩_ _G_ and _F ∨_ _G :=_


_H._
_H∈F :\ F,G⊆H_


A morphism between two lattices F and G is a mapping ϕ : F →G that respects the join and the
meet, i.e., such that ϕ(F ∧ _G) = ϕ(F_ ) ∧ _ϕ(G) and ϕ(F ∨_ _G) = ϕ(F_ ) ∨ _ϕ(G) for all F, G ∈F._
A lattice isomorphism is a bijective lattice morphism where the inverse is also a morphism. We say
that two basic semialgebraic sets with isomorphic face lattice are combinatorially equivalent.

B DETAILS ON THE PARAMETRIZATON OF DISCOUNTED STATE-ACTION
FREQUENCIES

B.1 THE DEGREE OF DETERMINANTAL POLYNOMIALS

Determinantal representation of polynomials play an important role in convex geometry (see for
example Helton & Vinnikov, 2007; Netzer & Thom, 2012), but often the emphasis is put on symmetric matrices. We adapt those arguments to the general case and present them here. We call p a
_determinantal polynomial if it admits a representation_


for all x ∈ R[m], (6)


_p(x) = det_


_A0 +_


_xiAi_
_i=1_

X


for some A0, . . ., Am ∈ R[n][×][n]. Let us use the notations


_xiAi_ and B(x) :=
_i=1_

X


_A(x) := A0 +_


_xiAi._
_i=1_

X


**Proposition 22 (Degree of monic univariate determinantal polynomials). Let A, B ∈** R[n][×][n] _and A_
_algebraic multiplicity. Then,be invertible and let λ1, . . ., λn ∈_ C denote the eigenvalues of A[−][1]B if repeated according to their
_p_ : R → R, _t 7→_ det(A + tB)

_is a polynomial of degree_

deg(p) = _j_ 1, . . ., n _λj_ = 0 rank(B).
_∈{_ _} |_ _̸_ _≤_

_The roots of p are given by_ _λ[−]j_ [][1] _j_ _J_ C. If further A[−][1]B is symmetric, then we have
_{−_ _|_ _∈_ _} ⊆_
deg(p) = rank(B).


-----

_Proof. Let J_ 1, . . ., n denote the set of indices j such that λj = 0. For x = 0 we have[6]
_⊆{_ _}_ _̸_ _̸_

_p(t) = det(A) det(I + tA[−][1]B) = x[n]_ det(A) det(A[−][1]B + t[−][1]I) = x[n] det(A)χA−1B( _t[−][1])_
_−_


( _t[−][1]_ _λi) = (_ 1)[n][−|][J][|]
_−_ _−_ _−_ _·_
_i=1_

Y


= t[n]


_t + λ[−]j_ [1]


( _λj)_
_−_ _·_
_jY∈J_


_j∈J_


which is a polynomial of degree |J|. Note that |J| is upper bounded by the complex rank of A[−][1]B.
Since the rank over C and R agree for a real matrix, we have deg(p) ≤ rank(A[−][1]B) = rank(B).
Assume now that A[−][1]B is symmetric, then the rank of A[−][1]B coincides with the number |J| of non
zero eigenvalues. Further, the rank of B and A[−][1]B is the same.

**Remark 23. Note that the degree of p can be lower than rank(B), for example if**


1 1
_A = I_ and B = _−_
1 _−1_

Then we have rank(B) = 1, but


(1 _−1) ._


1 + λ _λ_
_p(λ) = det_ _−_
 _λ_ 1 − _λ_


= (1 + λ)(1 − _λ) + λ[2]_ = 1


and therefore deg(p) = 0. Note that in this case A[−][1]B = B has no non-zero eigenvalues.

Now we show that the degree of p is still bounded by rank(B) even if A is not invertible. However,
we loose an explicit description of the degree in this case.

**Proposition 24 (Degree of univariate determinantal polynomials). Let A, B ∈** R[n][×][n] _and consider_
_the polynomial_
_p_ : R → R, _t 7→_ det(A + tB).

_repeated according to their algebraic multiplicity, thenThen either p = 0 or if p(t0) ̸= 0 and λ1, . . ., λn ∈_ C p denote the eigenvalues of has degree (A + t0B)[−][1]B


deg(p) = _j_ 1, . . ., n _λj_ = 0 rank(B).
_∈{_ _} |_ _̸_ _≤_

_In particular, it always holds that deg(p)_ rank(B).

[] _≤_

_Proof. Let without loss of generality p(t0)_ = 0, then C = A + _t0B is invertible. By Proposition 22,_
_̸_
the degree of q(t) = det(C + tB) is precisely _j_ _λj_ = 0 . Noting that p(t) = q(t _t0) yields_
the claim. _|{_ _|_ _̸_ _}|_ _−_

The following result generalizes Proposition 24 to multivariate determinantal polynomials.

**Proposition 25 (Degree of determinantal polynomials). Let p** : R[m] _→_ R be a determinantal poly_nomial with the representation (6). Then_

deg(p) ≤ max rank(B(x)) | x ∈ R[m] _._


_Proof. Let us fix x ∈_ R[m] and for t ∈ R set f (t) := p(tx) = det(A0 + tB(x)). By the next
proposition it suffices to show that deg(f ) ≤ rank(B(x)). However, this is precisely the statement
of Proposition 24.

**Proposition 26 (Degree of polynomials). Let p** : R[n] _→_ R be a polynomial. Then there is a direction
_x ∈_ R[n] _such that t 7→_ _p(tx) is a polynomial of degree deg(p). Moreover, for any x ∈_ R[n], the
_univariate polynomial t 7→_ _p(tx) has degree at most deg(p)._

_Proof. Let without loss of generality p be non trivial. Decompose p into its leading and lower order_
terms p = p1 + p2 and choose x ∈ R[n] such that p1(x) ̸= 0. Let k := deg(p), then we have
_p1(tx) = t[k]p1(x) for all µ ∈_ R. Since the degree of t 7→ _p2(tx) is at most k −_ 1, the degree of
_t_ _p(tx) = p1(tx) + p2(tx) is k._
_7→_

6Here, χC (λ) = det(C − _λI) denotes the characteriztic polynomial of a matrix C._


-----

**Remark 27. Analogue to the univariate case, it is possible to give a precise statement on the degree,**
Rwhich is the following. If[n][×][n] is invertible. Writing p λ is not vanishing and1(x), . . ., λn(x) ∈ _xC for the eigenvalues of0 ∈_ R[m] is such that p A(x0(x) ̸0= 0)[−][1], thenB(x) and A(x J0)( ∈x)
for the indices j such that λj(x) = 0 we obtain
_̸_

deg(p) = max _|J(x)| | x ∈_ R[m] _._


B.2 THE DEGREE OF POMDPS

The general bounds on the degree of determinantal polynomials directly implies Theorem 4, which
we state again for the sake of convenience here.
**Theorem 4 (Degree of POMDPs). Let (** _,_ _,_ _, α, β, r) be a POMDP, µ_ ∆ _be an initial dis-_
_S_ _O_ _A_ _∈_ _S_
_tribution and γ_ (0, 1) a discount factor. The state-action frequencies ηγ[π,µ] _and ρ[π,µ]γ_ _, the value_
_∈_
_function Vγ[π]_ _[and the expected cumulative reward][ R]γ[µ][(][π][)][ are rational functions with common de-]_
_nominator in the entries of the policy π. Further, if they are restricted to the subset Π_ ∆[O] _[of]_
_⊆_ _A_
_policies which agree with a fixed policy π0 on all states outside of O ⊆O, they have degree at most_
_s ∈S | β(o|s) > 0 for some o ∈_ _O_ _._

In fact, the results from the preceding pararaph imply the following sharper versions.[]
**Theorem 28 (Degree of discounted state-action frequencies of POMDPs). Let (S, O, A, α, β, r) be**
_a POMDP, µ_ ∆ _be an initial distribution and γ_ (0, 1) a discount factor. Then the discounted
_∈_ _S_ _∈_
_state-action distributions can be expressed as_

_ηγ[π,µ](s, a) =_ _[q][s,a][(][π][)]_ _for every π_ ∆[O] _[and][ s][ ∈S][,]_ (7)

_q(π)_ _∈_ _A_


_where_

_qs,a(π) := (π ◦_ _β)(a|s) · (1 −_ _γ) · det(I −_ _γp[T]π_ [)]s[µ] _and_ _q(π) := det(I −_ _γp[T]π_ [)]

_are polynomials in the entries of the policy. Further, if qs,a and q are restricted to the subset Π ⊆_
∆[O] _[of policies which agree with a fixed policy][ π][0][ on all states outside of][ O][ ⊆O][ and if we set]_
_A_
_S := {s ∈S | β(o|s) > 0 for some o ∈_ _O}, then they have degree at most_

deg(qs,a) max _π_ _π0_ [)]s[0] [+][ 1][S][(][s][)][ ≤|][S][|] (8)
_≤_ _π∈Π_ [rank(][p][T] _[−]_ _[p][T]_

_and_
deg(q) max _π_ _π0_ [)][ ≤|][S][|][ .] (9)
_≤_ _π∈Π_ [rank(][p][T] _[−]_ _[p][T]_

_Proof. Recall that we have_
_ηγ[π,µ](s, a) = (π_ _β)(a_ _s)ρ[π,µ]γ_ (s).
_◦_ _|_

Further, by Proposition 3, the state distribution is given by ρ[π,µ]γ = (1 − _γ)(1 −_ _γp[T]π_ [)][−][1][µ][. Applying]
Cramer’s rule yields

_π_ [)]s[µ]
_ρ[π,µ]γ_ (s) = [det(]det([I]I[ −] _[γp]γp[T][T]π_ [)][,]

_−_

where (I − _γp[T]π_ [)]s[µ] [denotes the matrix obtained by replacing the][ s][-th row of][ I][ −] _[γp]π[T]_ [with][ µ][, which]
shows (7). For the estimates on the degree, we note that

deg(qs,a) ≤ deg(I − _γp[T]π_ [)]s[µ] [+][ 1][S][(][s][)][.]

Further, we can use Proposition 25 to estimate the degree over a subset Π ∆[S] [of policies which]
_⊆_ _A_
agree with a fixed policy π0 on all states outside of O ⊆O. We obtain

deg(I _γp[T]π_ [)]s[µ] _π0_ [)]s[µ] _π_ [)]s[µ][) = max] _π_ _π0_ [)]s[0][,]
_−_ _[≤]_ [max]π∈Π [rank((][I][ −] _[γp][T]_ _[−]_ [(][I][ −] _[γp][T]_ _π∈Π_ [rank][ γ][(][p][T] _[−]_ _[p][T]_

which shows the first estimate in (8). To see the second inequality, we first assume that s ∈ _S. Then_
(p[T]π _[−]_ _[p]π[T]0_ [)]s[0] [has at most][ |][S][| −] [1][ non zero columns and hence rank at most][ |][S][| −] [1][. If][ s /]∈ _S, then_
in (8) holds in both cases. The estimates in (9) follow with completely analoguous arguments.with the same argument, the rank of (p[T]π _[−]_ _[p]π[T]0_ [)]s[0] [=][ p]π[T] _[−]_ _[p]π[T]0_ [is at most][ |][S][|][ and the second estimate]


-----

**Remark 29. The polynomial q is independent of the initial distribution µ, whereas the polynomi-**
als qs,a and therefore also their degrees depend on µ. Further, Proposition 24 contains an exact
expressions for the degree of the polynomials qs,a and q depending on the eigenvalues of certain
matrices.
**Corollary 30 (Degree of the reward and value function). Theorem 28 also yields the rational degree**
_of the reward and value function. Indeed, it holds that[7]_

_s,a_ _[r][(][s, a][)][q][s,a][(][π][)]_

_Rγ[µ][(][π][) =]_ = (1 _γ)_ [det(][I][ −] _[γp][π][ +][ r][π][ ⊗]_ _[µ][)]_ 1 + γ,

_q(π)_ _−_ _·_ det(I _γpπ)_ _−_

P _−_

_where we used the matrix determinant lemma (Vrabel, 2016), and_


_Vγ[π][(][s][) = det(][I][ −]_ _[γp][π][ +][ r][π][ ⊗]_ _[δ][s][)]_ 1 + γ.

det(I _γpπ)_ _−_
_−_

_The degree of their denominator is bounded by (9). The degree of the numerator of Vγ[π][(][s][)][ is bounded]_
_by (8). Finally, the degree of the numerator of the reward Rγ[µ]_ _[is bounded by the maximum degree of]_
_the numerators of Vγ[π][(][s][)][ over the support of][ µ][. An explicit formula for the rational degrees can be]_
_deduced from Remark 27._
**Corollary 31 (Degree of curves). Let (** _,_ _, α, r) be an MDP, µ_ ∆ _be an initial distribution,_
_S_ _A_ _∈_ _S_
_γ_ (0, 1) a discount factor and r R[S×A]. Further, let π0, π1 ∆[S] _[be two policies that disagree]_
_on ∈ k states. Let ηλ and Vλ denote the discounted state-action frequencies and the value function ∈_ _∈_ _A_
_functions of degree at mostbelonging to the policy πλ k :=._ _π0 + λ(π1 −_ _π0). Then both λ 7→_ _ηλ and λ 7→_ _V_ _[λ]_ _are rational_

B.3 PROPERTIES OF DEGREE-ONE RATIONAL FUNCTIONS

B.3.1 A LINE THEOREM FOR DEGREE-ONE RATIONAL FUNCTIONS


First, we notice that certain degree-one rational functions map lines to lines which implies that they
map polytopes to polytopes. Further, the extreme points of the range lie in the image of the extreme
points which implies that degree-one rational functions are maximized in extreme points – just like
linear functions.
**Definition 32. We say that a function f : Ω** _→_ R[m] is a rational function of degree at most k with
_common denominator if it admits a representation of the form fi = pi/q for polynomials pi and q_
of degree at most k.
**Remark 33. We have seen that the state-action frequencies, the reward function and the value**
function of POMDPs are rational functions of degree at most |S| with common denominator. In
the case of MDPs and if a policy is fixed on all but k states, it is a rational function with common
denominator of degree at most k.
**Proposition 34. Let Ω** _⊆_ R[d] _be convex and f : Ω_ _→_ R[m] _be a rational function of degree at most_
_one with common denominator and the representation fi(x) = pi(x)/q(x) for affine linear functions_
_pi, q. Then, f maps lines to lines. More precisely, if x0, x1_ Ω, then
_∈_

_q(x1)λ_

_c_ : [0, 1] [0, 1], _λ_
_→_ _7→_ _[q]q[(]([x]x[1]λ[)]) [λ]_ [=] (q(x1) _q(x0))λ + q(x0)_

_−_

_is strictly increasing and satisfies_

_f_ ((1 − _λ)x0 + λx1) = (1 −_ _c(λ))f_ (x0) + c(λ)f (x1) = f (x0) + c(λ)(f (x1) − _f_ (x0)). (10)

_Further, c is strictly convex if_ _q(x1)_ _<_ _q(x0)_ _, strictly concave if_ _q(x1)_ _>_ _q(x0)_ _and linear if_
_|_ _|_ _|_ _|_ _|_ _|_ _|_ _|_
_q(x0)_ = _q(x1)_ _._
_|_ _|_ _|_ _|_

_Proof. We set xλ := (1 −_ _λ)x0 + λx1 and by explicit computation we obtain_


_f_ (xλ) = _[p][(][x][λ][)]_ = [(1][ −] _[λ][)][q][(][x][0][)]_ _f_ (x0) + _[λq][(][x][1][)]_

_q(xλ) [= (1][ −]_ _[λ][)][p]q[(][x](x[0][) +]λ)_ _[ λp][(][x][1][)]_ _q(xλ)_ _·_ _q(xλ)_

_[·][ f]_ [(][x][1][)][.]

7Here, u _v := uvT denotes the Kronecker product._
_⊗_


-----

Noting that
_λq(x1)_ _λq(x1)_

_q(xλ) [=]_ (1 _λ)q(x0) + λq(x1) [=][ c][(][λ][)]_

_−_

and
(1 _λ)q(x0)_
_−_ + _[λq][(][x][1][)]_ = 1

_q(xλ)_ _q(xλ) [= (1][ −]_ _[λ][)][q]q[(][x](x[0][) +]λ)_ _[ λq][(][x][1][)]_

yields (10). Finally, we differentiate and obtain


_c[′](λ) =_ _[q][(][x][0][)][q][(][x][1][)]_ _._ (11)

_q(xλ)[2]_

Since q has no root in Ω it follows that q(x0) and q(x1) have the same sign and hence c[′](λ) > 0.
Differentiating a second time yields

_c[′′](λ) =_ 2q(x0)q(x1)(q(x1) _q(x0))_ _q(xλ)[−][3]._
_−_ _−_ _·_

Using that sgn(q(xλ)) = sgn(q(x0)) = sgn(q(x1)) yields the assertion.

**Remark 35. The formula (10) holds for all λ ∈** R for which xλ = λx0 + (1 − _λ)x1 ∈_ Ω.

**Proposition 36 (Level sets of degree one rational functions). Let Ω** _⊆_ R[d] _be convex and f : Ω_ _→_ R
_be a rational function of degree at most one. Then, Lα := {x ∈_ Ω _| f_ (x) = α} is the intersection
_of an affine space with Ω._

_Proof. For x, y ∈_ _Lα the ray {x_ + _t(y −_ _x) | t ∈_ R}∩ Ω is contained in Lα by the line theorem.

B.3.2 EXTREME POINTS OF DEGREE-ONE RATIONAL FUNCTIONS

It is well known that linear functions obtain their maxima on extreme points. We show that this is
also the case for rational functions of degree at most one.

**Definition 37. Let Ω** _⊆_ R[d]. Then we call x ∈ Ω an extreme point of Ω if x is not the strict convex
impliescombination of two other points in x0 = x1 = x. We denote the set of extreme points of Ω, i.e., if x = (1 − _λ)x0 + Ω λxby extr(Ω)1 for x0, x._ 1 ∈ Ω and λ ∈ (0, 1)

**Proposition 38. Let Ω** _⊆_ R[d] _be convex and f : Ω_ _→_ R[m] _be a rational function of degree at most_
_one with common denominator. Then f_ (Ω) is convex and we have extr(f (Ω)) ⊆ _f_ (extr(Ω)).

_Proof. Let y0 = f_ (x0), y1 = f (x1) ∈ _f_ (Ω). Then by the line theorem, the line connecting y0 and
_y1 agrees with the image of the line connecting x0 and x1 under f_, in particular, it is contained in
_f_ (Ω) which shows the convexity of f (Ω). Pick now an extreme point y = f (x) ∈ extr(f (Ω)). If
_x ∈_ extr(Ω), there is nothing to show, so let x /∈ extr(Ω). Then by the Carath´eodory theorem we
can write x as a strict convex combination _i=1_ _[λ][i][x][i][, λ][i][ >][ 0][, n][ ≥]_ [2][ for some extreme points][ x][i][ ∈]
extr(Ω). In particular, it is possible to write x as the strict convex combination x = (1 _−_ _λ)x0 +_ _λx1_
by setting x0 := _i=2_ _[λ][i][x][i][. Now, by the line theorem we have][P][n]_

_y = (1_ _c(λ))f_ (x0) + c(λ)f (x1),

[P][n] _−_

where c(λ) (0, 1). Since y is an extreme point, this implies f (x0) = f (x1) = y. In particular,
_∈_
this shows that y = f (x1) _f_ (extr(Ω)).
_∈_

**Corollary 39 (Maximizers of degree-one rational functions). Let Ω** _⊆_ R[d] _be a convex and compact_
_set and let f : Ω_ _→_ R be a rational function of degree at most one with common denominator. Then
_f is maximized in at least one extreme point of Ω. In particular, if Ω_ _is a polytope, f is maximized_
_in at least one vertex._

_Proof. Since Ω_ is compact and f is continuous, f (Ω) is a compact interval, lets say f (Ω) = [α, β].
By the preceding proposition we have {α, β} = extr(f (Ω)) ⊆ _f_ (extr(Ω)), which shows that f is
maximized in at least one extreme point.

**Corollary 40. Let P ⊆** R[d] _be a polytope and f : Ω_ _→_ R[m] _be a rational function of degree at most_
_one with common denominator. Then f_ (P ) is a polytope and we have vert(f (P )) ⊆ _f_ (vert(P )).


-----

_Proof. By the preceding proposition, f_ (P ) is convex. Further, f (P ) has finitely many extreme
points since extr(f (P )) ⊆ _f_ (extr(P )) = f (vert(P )), which implies the assertion.

**Proposition 41.polytopes, which is a degree-one rational function with common denominator whenever all but one Let f : P →** R[m] _be defined on the Cartesian product P = P1 × · · · × Pk of_
_components are fixed. Then f_ (P ) has finitely many extreme points and it holds that

extr(f (P )) _f_ (vert(P )) = f (vert(P1) vert(Pk)).
_⊆_ _× · · · ×_

_In particular, if m = 1 this shows that f is maximized in at least one vertex of P_ _._

_Proof. Let now x = (x[(1)], . . ., x[(][k][)]) ∈_ _P1 × · · · × Pk be such that f_ (x) ∈ extr(f (P )). If x[(][i][)] _∈_
vert(Pi), there is nothing to show. Hence, we assume that x[(][i][)] _/_ vert(Pi). Let us denote the
_∈_
restriction of f onto Pi by g, where we keep the other components fixed to be x[(][j][)]. Then we have
_g(x[(][i][)])_ extr(g(Pi)) and hence by Proposition 38 there is ˜x[(][i][)] vert(Pi) such that g(˜x[(][i][)]) =
_∈_ _∈_
_g(x[(][i][)]) = f_ (x). Replacing x[(][i][)] by ˜x[(][i][)] and iterating over i yields the claim.

**Remark 42. We have seen that both the value function as well as the discounted state-action fre-**
quencies are degree-one rational functions in the rows of the policy in the case of full observability.
Hence, the extreme points of the set of all value functions and of the set of discounted state-action
frequencies are described by the proposition above. In fact we will see later that the discounted
state-action frequencies form a polytope; further, one can show that the set of value functions is a
finite union of polytopes (see Dadashi et al., 2019).

B.3.3 IMPLICATIONS FOR POMDPS

**Proposition 6. Let (** _,_ _, α, r) be an MDP and γ_ (0, 1). Further, let π0, π1 ∆[S] _[be two]_
_S_ _A_ _∈_ _∈_ _A_
_policies that differ on at most k states. For any λ_ [0, 1] let Vλ R[S] _and ηλ[µ]_
_the discount factorvalue function and state-action frequency belonging to the policy γ, the initial distribution µ and the instantaneous reward ∈_ _∈ π0 + λ(π1 − r[∈]. Then the rationalπ[∆]0)[S×A] with respect to[ denote the]_
_degrees of λ 7→_ _Vλ and λ 7→_ _ηλ are at most k. If they differ on at most one state ˜s ∈S then_

_Vλ = V0 + c(λ) · (V1 −_ _V0)_ _and_ _ηλ[µ]_ [=][ η]0[µ] [+][ c][(][λ][)][ ·][ (][η]1[µ] _[−]_ _[η]0[µ][)]_ _for all λ ∈_ [0, 1],

_where_

det(I _γp1)λ_ _λ[(˜]s)_

_c(λ) = [det(][I][ −]_ _[γp][1][)][λ]_ _−_

det(I − _γpλ) [=]_ (det(I − _γp1) −_ det(I − _γp0))λ + det(I −_ _γp0) [=][ λ][ ·][ ρ]ρ[µ]1[µ][(˜]s)_ _[.]_


_Proof. This is a direct consequence of Proposition 34 and Theorem 4._

**Remark 43. The proposition above describes the interpolation speed λ · ρ[µ]λ[(˜]s)/ρ[µ]1** [(˜]s) in terms of
the discounted state distribution in ˜s. This expressions extends to the case of mean rewards – note
that the determinants vanish – and the theorem can be shown to hold in this case as well, if we set
0/0 := 0. Note that the interpolation speed does not depend on the initial condition µ.
**Remark 44. Refinements on the upper bound of the rational degree of λ 7→** _Vλ and λ 7→_ _ηλ can_
be obtained using Proposition 24. Indeed, if we write ηλ(s, a) = qsa(λ)/q(λ) like in Theorem 28
those degrees can be upper bounded by

deg(qsa) rank(p1 _p0)[0]s_ [+][ 1][S][(][s][)][ ≤] [rank(][p][1] and deg(q) rank(p1 _p0),_
_≤_ _−_ _[−]_ _[p][0][)]_ _≤_ _−_

where S ⊆S is the set of states on which the two policies differ; see also the proof of Theorem 28
for more details on an analogue argument. Hence, the degree of the two curves λ 7→ _Vλ and λ 7→_ _ηλ_
is upper bounded by rank(p1 _p0)._
_−_

C DETAILS ON THE GEOMETRY OF STATE-ACTION FREQUENCIES

The set of all state-action frequencies is known to be a polytope in the fully observable case (Derman,
1970) and we show that it is combinatorially equivalent to the conditional probability polytope ∆[S]
_A[.]_
We show that in the partially observable case the set of feasible state-action frequencies is cut out
from this polytope by a finite set of polynomial inequalities. We discuss the special structure of
those polynomials and give closed form expressions for them.


-----

C.1 THE FULLY OBSERVABLE CASE

Let νγ[π,µ] ∆ denote the expected number of transitions from s to s[′] given by
_∈_ _S×S_

_∞_ 1 _T −1_

(1 _γ)_ _γ[t]P[π,µ](st = s, st+1 = s[′])_ and lim P[π,µ](st = s, st+1 = s[′])
_−_ _T_ _T_

_t=0_ _→∞_ _t=0_

X X


respectively. Note that we have

_νγ[π,µ](s, s[′]) =_ _ηγ[π,µ](s, a)α(s[′]_ _s, a),_ (12)

_|_
_aX∈A_

hence νγ[π,µ] is the image of ηγ[π,µ] under the linear transformation


_fα : ∆_ ∆ _,_ _η_ _η(s, a)α(s[′]_ _s, a)_ _._ (13)
_S×A →_ _S×S_ _7→_ Xa∈A _|_ !s,s[′]∈S

Therefore, we can hope to obtain a characterization of Nγ[µ] [using this mapping. In order to do so, we]
would like to understand the structural properties of νγ[π,µ]. For γ = 1 those distributions have equal
marginals since we can compute


_ν1[π,µ](s, s[′])_ _ν1[π,µ](s[′], s) = lim_
_−_ _T_
_sX[′]∈S_ _sX[′]∈S_ _→∞_

In the discounted case, we compute similarly


P[π,µ](s0 = s) − P[π,µ](sT +1 = s) = 0. (14)



_νγ[π,µ](s, s[′])_ _γ_ _νγ[π,µ](s[′], s) = (1_ _γ)_ _γ[t]P[π,µ](st = s)_
_−_ _−_ _−_
_sX[′]∈S_ _sX[′]∈S_ Xt=0

= (1 − _γ)µ(s)._
If we perceive νγ[π,µ] ∆ as a matrix, we have shown that
_∈_ _S×S_

(νγ[π,µ])[T] 1S = γ(νγ[π,µ])1S + (1 − _γ)µ,_
which motivates the following definition.


_γ[t][+1]P[π,µ](st+1 = s)_
_t=0_

X


We will see that the set of state-action frequencies is the pre-image of the following polytope under
a linear map.
**Definition 45 (Discounted Kirchhoff polytopes). For a distribution µ** ∆ and γ (0, 1] we
_∈_ _S_ _∈_
define the discounted Kirchhoff polytope (this is a generalization of a definition by Weis 2010)
Ξ[µ]γ [:=] _ν_ ∆ R[S×S] _ν[T]_ 1 = γν1 + (1 _γ)µ_ _,_
_∈_ _S×S ⊆_ _|_ _S_ _S_ _−_

where 1S ∈ R[S] is the all one vector.

So far, we have observed that fα(ηγ[π,µ]) ∈ Ξ[µ]γ [, i.e., that]

Ψ[µ]γ [: ∆][S] _π_ _ηγ[π,µ]_
_A_ _[→]_ [∆][S×A][,] _7→_
maps to fα[−][1][(Ξ]γ[µ][)][. In order to see that this mapping is surjective on][ f][ −]α [1][(Ξ]γ[µ][)][ we show that its right]
inverse is given through conditioning. The following proposition uses the ergodicity assumption.
**Proposition 46. Let γ ∈** (0, 1] and η ∈ _fα[−][1][(Ξ]γ[µ][)][ and let][ ρ][ ∈]_ [∆][S] _[denote the state marginal of][ η][.]_
_Set_

_η(_ _s) = η(s,_ )/ρ(s) _if ρ(s) > 0_
_π(_ _s) :=_ _·|_ _·_

_·|_ _arbitrary element in ∆_ _if ρ(s) = 0,_
 _A_
_then we have ηγ[π,µ]_ = η.


_Proof. We calculate_

_γ(P_ _[π])[T]_ _η(s, a) = γ_


_α(s|s[′], a[′])π(a|s)η(s[′], a[′]) = γπ(a|s)_
_s[′],a[′]_

X


_α(s|s[′], a[′])η(s[′], a[′])_
_s[′],a[′]_

X


_ν(s[′], s) = π(a|s)_
_s[′]_

X


_ν(s, s[′]) −_ (1 − _γ)µ(s)_
_s[′]_


= γπ(a|s)


= π(a|s)ρ(s) − (1 − _γ)π(a|s)µ(s) = η(s, a) −_ (1 − _γ)(µ ∗_ _π)(s, a)._
The unique characterization from Proposition 3 of ηγ[π,µ] yields the assertion.


-----

The proposition states that we can reconstruct the policy from the state-action frequencies by conditioning and is well known in the context of the dual linear programming formulation of MDPs
(Kallenberg, 1994). Hence, it will be convenient later to work under the following assumption in
which ensures that policies in ∆[S] [are one-to-one with state-action frequencies.]
_A_

**Assumption 13 (Positivity). Let ρ[π,µ]γ** _> 0 hold entrywise for all policies π ∈_ ∆[S]A[.]

Note that this positivity assumption holds in particular, if either α > 0 and γ > 0 or γ < 1 and
_µ > 0 or entrywise. Indeed, if α > 0, then the transition kernel pπ is strictly positive for any policy_
since
_pπ(s[′]_ _s) =_ (π _β)(a_ _s)α(s[′]_ _s, a) > 0,_
_|_ _◦_ _|_ _|_

_a_

X

since (π _β)(a_ _s) > 0 for some a_ . Since ρ[π,µ]γ is discounted stationary with respect to pπ
_◦_ _|_ _∈A_
(Proposition 3), it holds that

_ρ[π,µ]γ_ (s) = γ _ρ[π,µ]γ_ (s[′])pπ(s _s[′]) + (1_ _γ)µ(s) > 0_

_|_ _−_
_s[′]_

X

since ρ[π,µ]γ (s[′]) > 0 for some s[′] . If µ > 0 and γ < 1, then ρ[π,µ]γ (s) (1 _γ)µ(s) > 0. As a_
_∈S_ _≥_ _−_
consequence of Proposition 46, we obtain the following characterization of Nγ[µ][.]

**Proposition 8 (Characterization of** _γ[µ][)][.][ Let][ (][S][,][ A][, α, r][)][ be an MDP,][ µ][ ∈]_ [∆][S] _[be an initial distri-]_
_N_
_bution and γ ∈_ (0, 1]. It holds that

_γ[µ]_ [= ∆][S×A] _η_ R[S×A] _wγ[s]_ _[, η][⟩][S×A]_ [= (1][ −] _[γ][)][µ][s]_ _[for][ s][ ∈S]_ (2)
_N_ _[∩]_ _∈_ _| ⟨_

_where wγ[s]_ [:=][ δ][s]  _[can be replaced by][ [0][,][ ∞][)][S×A][ in][ (2)][.]_

_[⊗]_ [1][A] _[−]_ _[γα][(][s][|·][,][ ·][)][. For][ γ][ ∈]_ [(0][,][ 1)][,][ ∆][S×A]

Instead of proving this proposition directly, we first present the following version of it.
**Proposition 47. Let (A, S, α, r) be an MDP and γ ∈** (0, 1]. It holds that Nγ[µ] [=][ f][ −]α [1][(Ξ]γ[µ][)][.]

_Proof. By (13) and (14), it holds that fα(Nγ[µ][)][ ⊆]_ [Ξ]γ[µ] [and thus][ N][ µ]γ _[⊆]_ _[f][ −]α_ [1][(Ξ]γ[µ][)][. However, by]
Proposition46, for every η _fα(Ξ[µ]γ_ [)][ there is a policy][ π][ ∈] [∆][S] [such that][ η]γ[π,µ] = η and hence it
_∈_ _A_
holds that fα[−][1][(Ξ]γ[µ][)][ ⊆] _[f][ −]α_ [1][(Ξ]γ[µ][)][.]

_Proof of Proposition 8. By the preceding proposition η_ _γ[µ]_ [is equivalent to][ η][ ∈] [∆][S×A] [and]
_∈N_
_ν := fα(η) ∈_ Ξ[µ]γ [. Using the definition of][ Ξ][µ]γ [this equivalent to]

_ν(s, s[′]) = γ_ _ν(s[′], s) + (1 −_ _γ)µ(s)_
_s[′]_ _s[′]_

X X

for all s ∈S. Plugging in the definition of fα we see that the term on the left hand side is equivalent
to

_s[′]_ _a_ _η(s, a)α(s[′]|s, a) =_ _a_ _η(s, a) = ⟨δs ⊗_ 1A, η⟩S×A.

X X X

The first term of the right hand side is precisely


_η(s[′], a)α(s_ _s[′], a) =_ _γα(s_ _,_ ), η _._
_|_ _⟨_ _|·_ _·_ _⟩S×A_


_s[′]_


Hence, we have seen that fα(η) ∈ Ξ[µ]γ [is equivalent to the condition]

_wγ[s]_ _[, η][⟩][S×A]_ [= (1][ −] _[γ][)][µ][(][s][)]_ for all s _._ (15)
_⟨_ _∈S_

Note that

_η_ _wγ[s]_ _[, η][⟩][S×A]_ [= (1][ −] _[γ][)][µ][(][s][)][ for all][ s][ ∈S}][ =][ η][0]_ [+][ {][w]γ[s]
_{_ _| ⟨_ _[|][ s][ ∈S}][⊥]_

for an arbitrary element η0 satisfying (15). This shows the first equation in (2). The second equation
follows from the observation that _s_ _[w]γ[s]_ [= (1][ −] _[γ][)][1][S]_ [. Hence, for][ γ <][ 1][ it holds that]

_η0 + {wγ[s]_ _[|][ s][ ∈S}][⊥][]_ _∩_ [P]∆S×A = _η0 +_ _{wγ[s]_ _[|][ s][ ∈S} ∪{][1][S]_ _⊥[]_ _∩_ [0, ∞)[S×A]
  = η0 +  wγ[s] [0,  )[S×A].

  _{_ _[|][ s][ ∈S}][⊥][]_ _∩_ _∞_


-----

C.1.1 DERIVATIVE OF THE DISCOUNTED STATE-ACTION FREQUENCIES

In this section we discuss the Jacobian of the parametrization π 7→ _η[π]_ of the discounted state-action
frequencies. One motivation for this is that this Jacobian plays an important role in the relation
of critical points in the policy space and the space of discounted state-action frequencies. Note that
Ψ[µ]γ [(][π][) = (1][−][γ][)(1][−][γP]π[ T] [)][−][1][(][µ][∗][π][)][ is well defined, whenever][ ∥][P][π][∥][2] _[< γ][−][1][. Hence, we can extend]_
Ψ[µ]γ [onto the neighborhood] _π_ R[S×A] _Pπ_ 2 < γ[−][1] of ∆[S]
_∈_ _| ∥_ _∥_ _A[, which enables us to compute the]_
Jacobian of Ψ[µ]γ [.]


**Proposition 48 (Jacobian of Ψ[µ]γ** [)][.][ For any policy][ π][ ∈] [∆][S] _[and][ s][ ∈S][, a][ ∈A][ it holds that]_
_A_

_∂(s,a)Ψ[µ]γ_ [(][π][) = (][I][ −] _[γP]π[ T]_ [)][−][1][(][ρ]γ[π,µ] _∂(s,a)π) = ρ[π,µ]γ_ (s)(I _γPπ[T]_ [)][−][1][(][δ][s] (16)
_∗_ _−_ _[⊗]_ _[δ][a][)][,]_

_where_
(ρ[π,µ]γ _∂(s,a)π)(s[′], a[′]) = ρ[π,µ]γ_ (s[′])∂(s,a)π(a[′] _s[′]) = ρ[π,µ]γ_ (s)(δs _δa)(s[′], a[′])._
_∗_ _|_ _⊗_

_Hence, ∂(s,a)Ψ[µ]γ_ [(][π][)][ is identical to the][ (][s, a][)][-th column of][ (][I][ −] _[γP]π[ T]_ [)][−][1][ up to the scaling factor of]
_ρ[π,µ]γ_ (s). In particular, if ρ[π,µ]γ (s) > 0 for all s ∈S, the Jacobian DΨ[µ]γ _[has full rank.]_

_Proof. Recall that for invertible matrices A(t), it holds that ∂tA(t)[−][1]_ = _A(t)[−][1](∂tA(t))A(t)[−][1]._
_−_
We compute


(1 − _γ)[−][1]∂(s,a)Ψ[µ]γ_ [(][π][) =][ ∂](s,a)[(][I][ −] _[γP]π[ T]_ [)][−][1][(][µ][ ∗] _[π][)]_

= (∂(s,a)(I − _γPπ[T]_ [)][−][1][)(][µ][ ∗] _[π][) + (][I][ −]_ _[γP]π[ T]_ [)][−][1][∂](s,a)[(][µ][ ∗] _[π][)]_

= −(1 − _γ)[−][1](I −_ _γPπ[T]_ [)][−][1][∂](s,a)[(][I][ −] _[γP]π[ T]_ [)][η]γ[π,µ]
+ (I − _γPπ[T]_ [)][−][1][(][µ][ ∗] _[∂](s,a)[π][)]_

= (I _γPπ[T]_ [)][−][1][  ](1 _γ)[−][1]γ(∂(s,a)Pπ[T]_ [)][η]γ[π,µ] + µ _∂(s,a)π_
_−_ _−_ _∗_

Further, direct computation shows 


((∂(s,a)Pπ[T] [)][η]γ[π,µ])(s, a) = ∂(s,a)π(a|s)


_α(s_ _s[′], a[′])π(a[′]_ _s[′])ρ[π,µ]γ_ (s[′])
_|_ _|_
_s[′],a[′]_

X


= (p[T]π _[ρ]γ[π,µ]_ _∗_ _∂(s,a)π)(s, a)._

Using the fact that ρ[π,µ]γ is the discounted stationary distribution, yields

(1 − _γ)[−][1]γ(∂(s,a)Pπ[T]_ [)][η]γ[π,µ] + µ ∗ _∂(s,a)π = ((1 −_ _γ)[−][1]γp[T]π_ _[ρ]γ[π,µ]_ + µ) ∗ _∂(s,a)π_

= (1 _γ)[−][1]ρ[π,µ]γ_ _∂(s,a)π,_
_−_ _∗_


which shows (16). Note that (I _γPπ[T]_ [)][−][1][(][δ][s]
matrix (I − _γPπ[T]_ [)][−][1][. Those columns are linearly independent, and so are the partial derivatives] − _[⊗]_ _[δ][a][)][ is precisely the][ (][s][0][, a][0][)][-th column of the]_
_∂(s,a)Ψ[µ]γ_ [(][π][)][, given that the discounted stationary distribution][ ρ][π,µ]γ vanishes nowhere.

**Corollary 49 (Dimension of** _γ[µ][)][.][ Assume that][ ρ]γ[π,µ]_ _> 0 entrywise for some policy π_ int(∆[S]
_N_ _∈_ _A[)][.]_
_Then we have_
dim( _γ[µ][) = dim(∆][S]_
_N_ _A[) =][ |S|][(][|A| −]_ [1)][.]

_Proof. By Proposition 48 the mapping Ψ[µ]γ_ [is full rank in a neighborhood of][ π][ and hence, we have]

dim( _γ[µ][) = dim(Ψ]γ[µ][(∆][S]_
_N_ _A[)) = dim(∆][S]A[)][.]_

_θLet us consider a parametrized policy modelπθ._ ΠΘ = {πθ | θ ∈ Θ} with differentiable parametrization
_7→_

**Proposition 50 (Parameter derivatives of discounted state-action frequencies). It holds that**

_∂θi_ _ηγ[π][θ][,µ]_ = (I − _γPπ[T]θ_ [)][−][1][(][ρ]γ[π][θ][,µ] _∗_ _∂θi_ _πθ),_ _where (ρ[π]γ[θ][,µ]_ _∗_ _∂θi_ _πθ)(s, a) = ρ[π]γ[θ][,µ](s)∂θi_ _πθ(a|s)._


-----

_Proof. This follows directly from the application of the chain rule and (16)._

Using this expression, we can compute the parameter gradient with respect to the discounted reward
_F_ (θ) := Rγ[µ][(][π][θ][)][ and recover the well known policy gradient theorem, see Sutton et al. (1999).]

**Definition 51 (state-action value function). We call Q[π]** := (I − _γPπ)[−][1]r ∈_ R[S×A] the state-action
_value function or the Q-value function of the policy π._

**Corollary 52 (Policy gradient theorem). It holds that**


_ρ[π]γ[θ][,µ](s)_


_∂θi_ _πθ(a_ _s)Q[π][θ]_ (s, a) =
_|_


_ηγ[π][θ][,µ](s, a)∂θi log(πθ(a|s))Q[π][θ]_ (s, a).
_s,a_

X


_∂θi_ _F_ (θ) =


_Proof. Using the preceding proposition, we compute_

_∂θi_ _F_ (θ) = ⟨ρ[π]γ[θ][,µ] _∗_ _∂θi_ _πθ, Q[π][θ]_ _⟩S×A =_
X


_ρ[π]γ[θ][,µ](s)_


_∂θi_ _πθ(a_ _s)Q[π][θ]_ (s, a)
_|_


= _ηγ[π][θ][,µ](s, a)∂θi log(πθ(a|s))Q[π][θ]_ (s, a).

_s,a_

X

**Remark 53 (POMDPs as parametrized policy models). The case of partial observability can some-**
times be regarded as a special case of parametrized policies. In fact the observation mechanism β
induces a linear map π 7→ _π ◦_ _β. This interpretation together with the preceding proposition can be_
used to calculate policy gradients in partially observable systems.

C.1.2 THE FACE LATTICE IN THE FULLY OBSERVABLE CASE

So far, we have seen that the set of state-action frequencies form a polytope in the fully observable
case. However, not all polytopes are equally complex and thus we aim to describe the face lattice of
_Nγ[µ][, which describes the combinatorial properties of a polytope, see Ziegler (2012).]_

**Theorem 54 (Combinatorial equivalence of** _γ[µ]_ [and][ ∆][S]
_N_ _A[)][.][ Let][ (][A][,][ S][, α, r][)][ be an MDP and][ γ][ ∈]_
(0, 1]. Then π _ηγ[π,µ]_ _induces an order preserving surjective morphism between the face lattices of_
_7→_
∆[S] _[and][ N][ µ]γ_ _[, such that for every][ I][ ⊆S × A][ it holds that]_
_A_
_π ∈_ ∆[S]A _[|][ π][(][a][|][s][) = 0][ for all][ (][s, a][)][ ∈]_ _[I]_ _7→_ _η ∈Nγ[µ]_ _[|][ η][(][s, a][) = 0][ for all][ (][s, a][)][ ∈]_ _[I]_ _._

_If additionally Assumption 13 holds, this is an isomorphism and preserves the dimension of the faces._ 

_Proof. First, we note that the faces of both ∆[S]_ [and][ N][ µ]γ [have the structure of the left and right hand]
_A_
side of (54) respectively, which follows from (2). Denote now the left and right hand side in (54) by
_F and G respectively, then we need to show that Ψ[µ]γ_ [(][F] [) =][ G][. For][ π][ ∈] _[F][ it holds that]_

_ηγ[π,µ](s, a) = ρ[π,µ]γ_ (s)π(a _s) = 0_ for all (s, a) _I_
_|_ _∈_

and hence ηγ[π,µ] _G. On the other hand for η_ _G we can set π(_ _s) := η(_ _s) whenever defined and_
_∈_ _∈_ _·|_ _·|_
any other element such that π(a|s) = 0 for all (s, a) ∈ _I otherwise. Then we surely have π ∈_ _F_
and by Proposition 46 also ηγ[π,µ] = η. In the case that ρ[π,µ]γ _> 0 holds entrywise for all policies_
_π_ int(∆[S] _γ_ [, which shows that the mapping]
_∈_ _A[)][, the mapping][ η][ 7→]_ _[η][(][·|·][)][ defines an inverse to][ Ψ][µ]_
defined in (54) is injective. The assertion on the dimension follows from basic dimension counting,
from the fact that the rank is preserved by a lattice isomorphism or by virtue of Proposition 48.

C.2 THE PARTIALLY OBSERVABLE CASE

In Corollary 5, we have seen that the discounted state-action frequencies form a semialgebraic set.
Now we aim to describe its defining polynomial inequalities. In Section 5 we will discuss how the
degree of these polynomials allows us to upper bound the number of critical points of the optimization problem.


-----

**Definition 9 (Effective policy polytope). We call the set of effective policies τ = π** _β_ ∆[S] [the]
_◦_ _∈_ _A_
_effective policy polytope and denote it by ∆[S][,β][.]_
_A_

Note that ∆[S][,β] is indeed a polytope since it is the image of the polytope ∆[O] [under the linear]
_A_ _A_
mapping π 7→ _π ◦_ _β. Hence, we can write it as an intersection_

∆[S][,β] = ∆[S] (17)
_A_ _A_ _[∩U ∩C][,]_

where U _, C ⊆_ R[S×A] are an affine subspace and a polyhedral cone and describe a finite set of linear
equalities and a finite set of linear inequalities respectively. In the following we will compute those
sets explicitely under mild conditions and see that they do not carry an affine part.

C.2.1 DEFINING LINEAR INEQUALITIES OF THE EFFECTIVE POLICY POLYTOPE

Obtaining inequality descriptions of the images of polytopes under linear maps is a fundamental
problem that is non-trivial in general. It can be approached algorithmically, e.g., by Fourier-Motzkin
elimination, block elimination, vertex approaches, and equality set projection (Jones et al., 2004).
We discuss the special case where the linear map is injective, corresponding to the case where the
associated matrix B has linearly independent columns. As a polytope is a finite intersection of
closed half spaces H [+] = {x | n[T] _x ≥_ _α}, it suffices to characterize the image BH_ [+]. It holds that

_BH_ [+] = {y ∈ range B | n[T] _B[+]y ≥_ _α} = {y | ((B[+])[T]_ _n)[T]_ _y ≥_ _α} ∩_ ker(B[T] )[⊥], (18)

where B[+] is a pseudoinverse and where we have used that B[+]y consists of at most one element by
the injectivity of B. Let us now come back to the mapping π 7→ _π ◦_ _β = βπ. By the “vec-trick”,_
this map corresponds to vec(βπI) = (I _[T]_ _⊗_ _β) vec(π). Hence the linear map is represented by the_
matrix B = I ⊗ _β. We observe that (I ⊗_ _β)[+]_ = I ⊗ _β[+]_ (see Langville & Stewart, 2004, Section
2.6.3). Notice that B = I ⊗ _β has linearly independent columns if and only if β does. By the_
above discussion, if β has linearly independent columns, then an inequality ⟨π, n⟩≥ 0 in the policy
polytope ∆[O] [corresponds to an inequality][ ⟨][τ,][ (][β][+][)][T][ n][⟩≥] [0][ in the polytope][ ∆][S]
_A_ _A[.]_

**Assumption 10. The matrix β** ∆[S]
_∈_ _O_ _[⊆]_ [R][S×O][ has linearly independent columns.]

**Remark 11. The assumption above does not imply that the system is fully observable. Recall**
that if β has linearly independent columns, the Moore-Penrose takes the form β[+] = (β[T] _β)[−][1]β[T]_ .
An interesting special case is when β is deterministic but may map several states to the same
observation (this is the partially observed setting considered in numerous works). In this case,
_β[+]_ = diag(n[−]1 [1][, . . ., n][−][1]
_|O|[)][β][T][, where][ n][o][ denotes the number of states with observation][ o][. In this]_
case, βso[+] [agrees with the conditional distribution][ β][(][s][|][o][)][ with respect to a uniform prior over the]
states; however, this is not in general the case since β[+] can have negative entries.

**Theorem 12 (H-description of the effective policy polytope). Let (S, O, A, α, β, r) be a POMDP**
_and let Assumption 10 hold. Then it holds that_

∆[S][,β] = ∆[S] (3)
_A_ _A_ _[∩U ∩C][ =][ U ∩C ∩D][,]_

_where U = {π ◦_ _β | π ∈_ R[S×O]} = ker(β[T] )[⊥] _is a subspace, C = {τ ∈_ R[S×A] _| β[+]τ ≥_ 0}
_is a pointed polyhedral cone and D = {τ ∈_ R[S×A] _|_ _a[(][β][+][τ]_ [)][oa][ = 1][ for all][ o][ ∈O}][ an affine]

_subspace. Further, the face lattices of ∆[O]_ _[and][ ∆][S][,β]_ _are isomorphic._
_A_ _A_

[P]

_Proof. First, we recall the defining linear (in)equalities of the policy polytope ∆[O]_
_A[, which are given]_
by

_π(a|o) = ⟨δo ⊗_ _δa, π⟩O×A ≥_ 0 for all a ∈A, o ∈O and

_π(a_ _o) =_ _δo_ 1 _, π_ = 1 for all o _._
_a_ _|_ _⟨_ _⊗_ _A_ _⟩O×A_ _∈O_

X

Hence, by the general discussion from above, namely by (18), it holds that


∆[S][,β] = ker(β[T] )[⊥] _τ_ _β[+]τ_ 0 _τ_
_A_ _∩{_ _|_ _≥_ _} ∩_ _|_
n


(β[+]τ )oa = 1 for all o ∈O


-----

Note that the linear inequalities _a[(][β][+][τ]_ [)][oa][ = 1][ are redundant in][ ∆][S]

_β[+]1_ = 1 by the injectivity of β and β1 = 1 . Now we can check thatA[. To see this, we note that]
_S_ _O_ _O_ _S_

[P]

(β[+]τ )oa = _βos[+]_ _[τ][sa]_ [=] _βos[+]_ _τsa =_ _βos[+]_ [= 1][.]
_a_ _a_ _s_ _s_ _a_ _s_

X X X X X X

This together with β(∆[O] [shows that]
_A[)][ ⊆]_ [∆][S]A

∆[S][,β] = ∆[S]
_A_ _A_ _[∩]_ [ker(][β][T][ )][⊥] _[∩{][τ][ |][ β][+][τ][ ≥]_ [0][}][.]

The reformulation of the sets C and D for deterministic observation mechanisms β follows from the
preceding remark.

C.2.2 DEFINING POLYNOMIAL INEQUALITIES OF THE FEASIBLE STATE-ACTION
FREQUENCIES

Using that the inverse of π 7→ _η[π]_ is given through conditioning (see Proposition 46), we can translate
linear inequalities in ∆[S] [into polynomial inequalities in][ N][ µ]γ [. More precisely, we have the following]
_A_
result, which can easily be extended to more general inequalities.

**Proposition 14 (Correspondence of inequalities). Let (** _,_ _, α, r) be an MDP, τ_ ∆[S] _[and let]_
_S_ _A_ _∈_ _A_
_η_ ∆ _denote its corresponding discounted state-action frequency for some µ_ ∆ _and_
_∈_ _S×A_ _∈_ _S_
_γ ∈_ (0, 1]. Let c ∈ R, b ∈ R[S×A] _and set S := {s ∈S | bsa ̸= 0 for some a ∈A}. Then_


_bsaτsa_ _c_ _implies_
_s,a_ _≥_

X


_ηs[′]a[′]_ _c_
_−_
_a[′]_

X


_ηs[′]a[′]_ 0,
_≥_
_a[′]_

X


_bsaηsa_


_s[′]∈S_


_s∈S_


_s[′]∈S\{s}_


_where the right is a multi-homogeneous polynomial[8]_ _in the blocks (ηsa)a_ R[A] _with multi-degree_
1S ∈ N[S] _. If further Assumption 13 holds, the inverse implication also holds.∈A ∈_

_Proof. Let τ_ ∆[S] [and let][ η][ denote its corresponding discounted stationary distribution and][ ρ][ the]
_∈_ _A_
state marginal. Assuming that the left hand side holds, we compute


_ηs′a′ =_
_a[′]_

X


_bsaηsa_


_bsaτsaρs_


_ρs[′]_
_s[′]∈YS\{s}_


_s∈S_


_s[′]∈S\{s}_


_s∈S_ _a_

_bsaτsa_
_s,a_

X


_ρs′_ _c_ _ρs′_ _,_
_≥_
_sY[′]∈S_ _sY[′]∈S_


which shows the first implication. If further Assumption 13 holds, the product over the marginals is
strictly positive, which shows the other implication.

**Remark 55. According to the preceding proposition, a linear inequality in the state policy polytope**
∆[S] [involving actions of][ k][ different states yields a polynomial inequality of degree][ k][ in the set]
_A_
of state-action frequencies _γ[µ][. In particular, for a linearly constrained policy model][ Π][ ⊆]_ [∆][S]
_N_ _A[,]_
where every constraint only addresses a single state, the set of state-action frequencies induced
by these policies will still form a polytope. This shows that this type of box constraints are well
aligned with the algebraic geometric structure of the problem. The linear constraints arising from
partial observability never exhibit this box type structure – unless the system is equivalent to its
fully observable version. This is because the projection of the effective policy polytope ∆[S][,β] onto
_A_
a single state always gives the entire probability simplex ∆, which is never the case, if there is a
_A_
non trivial linear constraint concerning only this state.

**Theorem 16. Let (** _,_ _,_ _, α, β, r) be a POMDP, µ_ ∆ _and γ_ (0, 1] and assume that As_S_ _O_ _A_ _∈_ _S_ _∈_
_sumption 13 holds. Then we have_ _γ[µ,β]_ = _γ[µ]_
_multi-homogeneous polynomial equations and N_ _N B is a basic semialgebraic set described by multi-[∩V ∩B][, where][ V][ is a variety described by]_
_homogeneous polynomial inequalities. Further, the face lattices of ∆[S]A[,β]_ _and Nγ[µ,β]_ _are isomorphic._

8A polynomial p : Rn1 _×· · ·×Rnk →_ R is called multi-homogeneous with multi-degree (d1, . . ., dk) ∈ Nk,
if it is homogeneous of degree dj in the j-th block of variables for j = 1, . . ., k.


-----

|Col1|(In)equalities of state policies|(In)equalities of state-action frequencies|
|---|---|---|
|MDPs|∆S is described by A|γµ is described by N|
||τ(as) 0 | ≥ Row normalization: P τ(a |s) −1 = 0 a – –|η(s, a) 0 ≥ – Discounted stationarity: ws, η (1 γ)µ(s) = 0 ⟨ γ ⟩− − For γ = 1: P η −1 = 0 s,a sa|
|POMDPs|∆S,β is described in ∆S by A A|γµ,β is described in γµ by N N|
||Linear (in)equalities See Section 4 Closed form under Assumption 10: See Theorem 12 Closed form for deterministic observ.: See Remark 57|Polynomial (in)equalities See Section 4, Proposition 14 Closed form under Assumption 10: See Remark 18 for inequalities See Remark 56 for equalities Closed form for deterministic observ.: See Remark 57|


(In)equalities of state policies (In)equalities of state-action frequencies

∆[S] [is described by] _γ[µ]_ [is described by]
_A_ _N_

_τ_ (a|s) ≥ 0 _η(s, a) ≥_ 0

Row normalization:
MDPs –
_a_ _[τ]_ [(][a][|][s][)][ −] [1 = 0]

Discounted stationarity:

P –

_⟨wγ[s]_ _[, η][⟩−]_ [(1][ −] _[γ][)][µ][(][s][) = 0]_

For γ = 1:
–

_s,a_ _[η][sa][ −]_ [1 = 0]

P

∆[S][,β] is described in ∆[S] [by] _γ[µ,β]_ is described in _γ[µ]_ [by]
_A_ _A_ _N_ _N_

Linear (in)equalities Polynomial (in)equalities
See Section 4 See Section 4, Proposition 14
Closed form under Assumption 10: Closed form under Assumption 10:

POMDPs See Theorem 12 See Remark 18 for inequalities

See Remark 56 for equalities

Closed form for deterministic observ.: Closed form for deterministic observ.:
See Remark 57 See Remark 57


Table 1: Correspondence of the defining linear and polynomial inequalities of the (effective) state
policies and the (feasible) state-action frequencies for MDPs and POMDPs respectively.

_Proof. The equation_ _γ[µ,β]_ = _γ[µ]_
_N_ _N_ _[∩V ∩B][ is a direct consequence of (3) and Proposition 14. Further,]_
it is clear from Proposition 14 that the mapping Ψ: ∆[S] _γ_ _[, π][ 7→]_ _[η]γ[π,µ]_ induces a bijection of the
_A_ _[→N][ µ]_
face lattices of ∆[S]A[,β] and Nγ[µ,β]. In order to see that the join and meet are respected, we note that for
_F, G_ (∆[S][,β][)][ it holds that][ Ψ(][F][ ∧] _[G][) = Ψ(][F][ ∩]_ _[G][) = Ψ(][F]_ [)][ ∩] [Ψ(][G][) = Ψ(][F] [)][ ∧] [Ψ(][G][)][. Further,]
_∈F_ _A_
Ψ(F _G) is a face of_ _γ[µ,β]_ containing Ψ(F ) and Ψ(G) and hence by definition Ψ(F ) Ψ(G)
_∨_ _N_ _∨_ _⊆_
Ψ(F _G). Further, for any face I of_ _γ[µ,β]_ containing Ψ(F ) and Ψ(G) it holds that Ψ[−][1](I) is a
_∨_ _N_
face of ∆[S][,β] containing F and G and hence Ψ[−][1](I) _F_ _G or equivalently Ψ(F_ _G)_ _I._
_A_ _⊇_ _∨_ _∨_ _⊆_

Comparing (17) and Theorem 16 we see that the linear space U corresponds to the variety V, where
the cone C corresponds to the basic semialgebraic set B. In general, every linear (in)equality cutting
out the effective policy polytope ∆[S][,β] from the state policy polytope ∆[S] [of the associated MDP]
_A_ _A_
corresponds to a polyomial (in)equality cutting out the feasible state-action frequencies _γ[µ,β]_ from
_N_
all state-action frequencies Nγ[µ] [of the corresponding MDP, see also Table 1. This correspondence]
arises by relating state-action frequencies to state policies via conditioning. Hence, the problem
of computing the defining polynomial inequalities of the feasible state-action frequencies reduces
to computing the defining linear inequalities of the effective policy polytope. This can be done in
closed form if β has linearly independent columns or if it deterinistic, see Remark 18, 56 and 57.
**Remark 18. By Theorem 12 and Proposition 14, the defining polynomials of the basic semialge-**
braic set B from Theorem 16 are indexed by a ∈A, o ∈O and are given by

_pao(η) :=_ _βos[+]_ _[η][sa]_ _ηs′a′_ = _βos[+]_ _[′]_ _ηsf_ (s) 0, (4)

_sX∈So_  _s[′]∈SYo\{s}_ Xa[′]  _f : SXo→A_  _s[′]∈fX[−][1]({a})_  Ys∈So _≥_

where So := _s_ _βos[+]_
have |So||A|[|S] {[o][|−] ∈S |[1] monomials of degree[̸][= 0][}][. The polynomials depend only on] |So| of the form _s∈So_ _[η][sf][ β][(][s][ and not on][)][ for some][ f][ γ][ :][,][ S][ µ][o][ nor][ →A][ α][, and][. In]_

particular, we can read of the multi-degree of pao with respect to the blocks (ηsa)a which is given
by 1So (see also Proposition 14). A complete description of the set[Q] _γ[µ,β]_ via (in)equalities follows∈A
_N_
from the description of Nγ[µ] [via linear (in)equalities given in (2). In Section 5 we discuss how the]
degree of these polynomials controls the complexity of the optimization problem.


-----

**Remark 56 (Defining polynomial equalities). Analogously to the defining inequalities, we can**
compute the defining polynomial equalities in the following way. First, we need to compute a
basis _b[j]_ _j_ _J of_ _βπ_ _π_ R[O×A] = ker(β[T] ) R[S×A], which can easily be done using the
_{_ _}_ _∈_ _{_ _|_ _∈_ _}[⊥]_ _⊆_
Gram-Schmidt process. Note that the defining linear equalities of the effective policy polytope (in
the policy polytope) are given by _b[j], τ_ = 0. Hence, by Proposition 14 the corresponding
_⟨_ _⟩S×A_
polynomial equality is given by


_b[j]sa[η][sa]_
_aX∈A_


_ηs′a′ = 0,_ (19)
_aX[′]∈A_


_qj(η) :=_

_sX∈Sj_


_s[′]∈Sj_ _\{s}_


where Sj := _s_ _b[j]sa_
_{_ _∈S |_ _[̸][= 0][ for some][ a][ ∈A}][.]_

**Remark 57 (Polynomial constraints for deterministic observations). In the case, where β corre-**
sponds to a determinstic mapping we can compute all polynomial constraints in closed form. Let us
assume that β(o|s) = δog(s) for some mapping g : S →O and write So := g[−][1]({o}) ⊆S, then
_τ_ ∆[S] [belongs to the effective policy polytope][ ∆][S][,β] if and only if
_∈_ _A_ _A_

_τ_ (a _s1) = τ_ (a _s2)_ for all s1, s2 _So, a_ _, o_ _._ (20)
_|_ _|_ _∈_ _∈A_ _∈O_

Note that this can be encoded in _o[|A|][(][|][S][o][| −]_ [1) =][ |A|][(][|S| −|O|][)][ linear equations; indeed if we]
fix so _So, then (20) is equivalent to_
_∈_

_τ_ (a _s)_ _τ_ (a [P]so) = 0 for all s _So_ _so_ _, a_ _, o_ _._ (21)
_|_ _−_ _|_ _∈_ _\ {_ _}_ _∈A_ _∈O_

Another way to derive these linear equalities is by noticing thatcompare also Remark 56. By Proposition 14 for η ∈Nγ[µ] [it is equivalent to lie in] esa − _esoa form a basis of[ N][ µ,β]γ_ or to satisfy ker(β[T] ),


_ηsa′ = 0_ for all s _So_ _so_ _, a_ _, o_ _._ (22)
_a[′]_ _∈_ _\ {_ _}_ _∈A_ _∈O_

X


_ηsa_


_ηsoa′_ _ηsoa_
_−_
_a[′]_

X


Note that in this case, there are no polynomial inequalities; this can also be seen from Remark 11
and Remark 18. Indeed, it holds that β[+] = β[T] diag(n1, . . ., n ) 0, where no := _So_ . Hence,
_|O|_ _≥_ _|_ _|_
the polynomial inequalities pao(η) 0 are redundant on the cone [0, )[S×A].
_≥_ _∞_

**Remark 58. In the fully observable case we have |So| = 1 for each o. Hence, each of the polynomial**
inequalities has a single term of degree 1. Indeed, in this case the inequalities are simply ηsa 0,
for each a, for each s. In the case of a deterministic β, we have βos[+] [=][ 1][s][∈][S]o _[/][|][S][o][|][. For each] ≥[ o, a][,]_
there is an inequality _f : So_ _s_ _So_ _[η][sf]_ [(][s][)][ ≥] [0][ of degree][ |][S][o][|][ equal to the number of]

states that are compatible with→A o. _[|][f][ −][1][(][a][)][|][ Q]_ _∈_

**Remark 59 (Reformulation of reward maximization as a polynomial program)[P]** **. By the theorem**
above and Proposition 14, reward maximization is equivalent to the maximization of a linear function
subject to polynomial constraints. This enables the use of any (approximate) solution technique of
polynomial optimization problems in order to solve POMDPs. Such methods have been developed
for a long time and have been applied to a variety of problems (Anjos & Lasserre, 2011; Lasserre,
2015). As meta algorithm, this is presented in Algorithm 1. Once, a solution η[∗] is obtained, the corresponding state policy τ _[∗]_ _∈_ ∆[S]A [can be computed by conditioning, i.e.][ τ] [(][a][|][s][) :=][ η][sa][/][(][P]a[′][ η][sa][′] [)][.]

Then, every π[∗] ∆[O] [with][ βπ][∗] [=][ τ][ ∗] [is an optimal policy. Such a policy can be computed by solv-]
_∈_ _A_
ing a system of linear equations, which are βπ = τ and π ∆[O]
_∈_ _A[, which is standard. In particular,]_
if β has linearly independent columns, it holds that π[∗] := β[+]τ . We demonstrate that this offers a
computationally feasible approach to planning of POMPDs in Section F on the toy example used for
Figure 1 and a grid world.

D DETAILS ON THE OPTIMIZATION

Let us quickly recall how we can reformulate the reward maximization problem as a polynomial
optimization problem, which then leads us to the mighty tools of algebraic degrees. We perceive
the reward maximization problem again as the maximization of a linear function p0 over the set of
feasible state-action frequencies _γ[µ,β]. Since under Assumption 13 the parametrization π_ _η[π]_
_N_ _7→_
is injective and has a full-rank Jacobian everywhere (see Appendix C.1.1), the critical points in the
policy polytope ∆[O] [correspond to the critical points of][ p][0][ on][ N][ µ,β]γ (Trager et al., 2019). In general,
_A_


-----

**Algorithm 1 Polynomial programming for POMDPs**
**Require: α, β, γ, µ**

**for sw ∈S[s]** _← doδs ⊗_ 1A − _γα(s|·, ·)_

**end for**
**for a ∈A, o ∈O do**

Define pao according to Equation (4)

**end for**
Compute a basis _b[j]_ _j_ _J of_ _βπ_ _π_ R[O×A] R[S×A]
_{_ _}_ _∈_ _{_ _|_ _∈_ _}[⊥]_ _⊆_
**for j ∈** _J do_

Define qj according to Equation (19)

**end for**
_η[∗]_ _←_ arg max⟨r, η⟩ sbj to η ≥ 0, ⟨w[s], η⟩ = (1 − _γ)µs, ⟨1S×A, η⟩_ = 1, pao(η) ≥ 0, qj(η) = 0
_R[∗]_ _←⟨r, η[∗]⟩_
_τ_ _[∗]_ _←_ _η[∗](·|·) ∈_ ∆[S]A

_π[∗]_ _←_ solution of βπ = τ _[∗]_
**return maximizer η[∗], optimal value R[∗], optimal policy π[∗]**

critical points of this linear function can occur on every face of the semialgebraic set _γ[µ,β]. The_
_N_
optimization problem thus has a combinatorial and a geometric component, corresponding to the
number of faces of each dimension and the number of critical points in the interior of any given
face. We have discussed the combinatorial part in Theorem 16 and focus now on the geometric part.
Writing _γ[µ,β]=_ _η_ R[S×A] : pi(η) 0, i _I_, we are interested in the number of critical points
_N_ _{_ _∈_ _≤_ _∈_ _}_
on the interior of a face

int(FJ ) = _η_ _γ[µ,β]_ _pj(η) = 0 for j_ _J, pi(η) > 0 for i_ _I_ _J_ _._
_{_ _∈N_ _|_ _∈_ _∈_ _\_ _}_

Note that a point η int(FJ ) is critical, if and only if it is a critical point on the variety
_∈_

_VJ := {η ∈_ R[S×A] _| pj(η) = 0 for j ∈_ _J}._

For the sake of notation, let us assume that J = {1, . . ., m} from now on. We can upper bound the
number of critical points in the interior of the face by the number of critical points of the polynomial
optimization problem

maximize p0(η) subject to _pj(η) = 0 for j = 1, . . ., m,_ (23)

where the polynomials have n variables. The number of critical points of this problems is upper
bounded by the algebraic degree of the problem as we discuss now.

D.1 INTRODUCTION TO ALGEBRAIC DEGREES

We try to present the results from the mighty theory of algebraic degrees that we use here and refer
the interested reader to the excellent low level introduction by Breiding et al. (2021) and to the
references therein. Let us consider the polynomial optimization problem (23), where we do not
require p0 to be linear. Further, denote the number of variables by n (in the case of state-action
frequencies n = |S||A|) and denote the degrees of p0, . . ., pm by d0, . . ., dm. We call a point
_critical, if it satisfies the KKT conditions (∇p0(x) +_ _i=1_ _[λ][i][∇][p][i][(][x][) = 0][,][ p][1][(][x][) =][ · · ·][ =][ p][m][(][x][) =]_
0), which can be phrased as a system of polynomial equations (see Nie & Ranestad, 2009). The
number of complex solutions to those criticality equations, when finite, is called the algebraic degree

[P][m]
of the problem. The algebraic degree is determined by the nature of the polynomials p0, . . ., pm and
captures the computational complexity of the optimization problem (Kung, 1973; Bajaj, 1988).[9]
A special case of (23) is when m = n and the polynomials p1, . . ., pm are generic. Then by
the polynomialsB´and all of them are critical and hence the algebraic degree is preciselyezout’s theorem there are exactly p0, . . ., pm define a complete intersection, i.e., the co-dimension of their induced d1 · · · dn isolated points satisfying the polynomial constraints d1 · · · dn (Timme, 2021). If

9The coordinates of critical points can be shown to be roots of some univariate polynomials whose degree
equals the algebraic degree and whose coefficients are rational functions of the coefficients of p0, . . ., pm.


-----

variety is m + 1, the algebraic degree of (23) is upper bounded by


_d1_ _dm_ (d0 1)[i][0] (dm 1)[i][m] _,_ (24)
_· · ·_ _i0+···+Xim=n−m_ _−_ _· · ·_ _−_

and this bound is attained for generic polynomials (Nie & Ranestad, 2009; Breiding et al., 2021).
For non-complete intersections, the expression (24) does not need to yield an upper bound if some
constraints are redundant. However, we can modify the expression to obtain a valid upper bound.
Indeed, if l and c = n − _l denote the dimension and co-dimension of_
:= _x_ _p1(x) =_ = pm(x) = 0
_V_ _{_ _|_ _· · ·_ _}_
upper bounded byand if p0 is generic and if the degrees are ordered, i.e., d1 ≥· · · ≥ _dm, then the algebraic degree is_
_d1_ _dc_ (d0 1)[i][0] (dc 1)[i][c] _._ (25)
_· · ·_ _i0+···X+ic=l_ _−_ _· · ·_ _−_

To see this, fix a subset J 1, . . ., m of cardinality c, such that = _x_ _pj(x) = 0 for j_ _J_ .
_⊆{_ _}_ _V_ _{_ _|_ _∈_ _}_
Then we can apply the bound from (24) and evaluate it to be

_dj_ (d0 1)[i][0] (dj 1)[i][j] _,_
_jY∈J_ _i0+[P]jX∈J_ _[i][j]_ [=][n][−][c] _−_ _·_ _jY∈J_ _−_

which is clearly upper bounded by (25). If p0 is linear, then d0 = 1 and the expression simplifies to

_d1_ _dc_ (d1 1)[i][0] (dc 1)[i][c] _._
_· · ·_ _i1+···X+ic=l_ _−_ _· · ·_ _−_

If further di = 1 for i ≥ _k for some k ≤_ _c, then we obtain_

_d1_ _dk_ (d1 1)[i][1] (dk 1)[i][k] _._ (26)
_· · ·_ _i1+···X+ik=l_ _−_ _· · ·_ _−_

If pk+1, . . ., pm are affine linear (and in general position relative to p1, . . ., pk, the algebraic degree
of (23) is given by the (m − _k)-th polar degree δm−k(V) of the variety_
:= _η_ _pk+1(η) =_ = pm(η) = 0 _,_
_V_ _{_ _|_ _· · ·_ _}_

see Draisma et al. (2016); Ozl¨[¨] um C¸ elik et al. (2021). This relation is particularly useful, since
for state-action frequencies there are always active linear equations as described in (2). The polar degrees of certain interesting cases (Segre-Veronese varieties) have been recently computed by
Sodomaco (2020, Section 5) and our proof of Proposition 21 builds on those formulas and their
presentation by Ozl¨[¨] um C¸ elik et al. (2021).

**Remark 60 (Genericity assumptions). In the case, where the polynomials p0, . . ., pm are not**
generic, there might be infinitely many critical points. Indeed, even for a linear program, i.e., when
all polynomials are linear, there might be infinitely many and even a non-trivial face of global optima. This is however not the case if p0 is generic. Hence, the genericity assumptions on the reward
vector r and also other elements of the POMDP are not surprising. For example, they prevent the
reward vector to be identical to zero or to be perpendicular on all vectorswhich cases the reward function would be constant and every policy would be a global optimum. δs ⊗ 1A − _γα(s|·, ·) in_

D.2 GENERAL UPPER BOUND ON THE NUMBER OF CRITICAL POINTS

**Theorem 20. Consider a POMDP (S, O, A, α, β, r), γ ∈** (0, 1), assume that r is generic, that
_β ∈_ R[S×O] _is invertible, and that Assumption 13 holds. For any given I ⊆A × O consider the_
_following set of policies, which is the relative interior of a face of the policy polytope:_
int(F ) = _π_ ∆[O] _._
_∈_ _A_ _[|][ π][(][a][|][o][) = 0][ if and only if][ (][a, o][)][ ∈]_ _[I]_

_Let O :=_ _o_ (a, o) _I for some a_ _and set ko :=_ _a_ (a, o) _I_ _as well as do :=_ _s_
_{_ _∈O |_ _∈_ _}_ _|{_ _|_ _∈_ _}|_ _|{_ _|_
_βos[−][1]_

_[̸][= 0][}|][. Then, the number of critical points of the reward function on][ int(][F]_ [)][ is at most]

_d[k]o[o]_ (do 1)[i][o] _,_ (5)

Yo∈O ! _·_ _o∈XO_ _[i][o][=][l]_ _oY∈O_ _−_

_where l = |S|(|A| −_ 1) −|I|. If α and µ are generic, this bound can be refined by computing theP
_polar degrees of multi-homogeneous varieties (see Proposition 21 for a special case). The same_
_bound holds in the mean reward case γ = 1 for l given in Remark 61._


_d1_ _dm_
_· · ·_


_dj_
_jY∈J_


_d1_ _dc_
_· · ·_


-----

_Proof. The face G of the effective policy polytope corresponding to F is given by_

int(G) = _τ_ ∆[S][,β] (β[+]τ )oa = 0 (a, o) _I_ _._
_∈_ _A_ _|_ _⇔_ _∈_
n o

In order to describe the corresponding set of discounted state-action frequencies, we use the notation

_pao(η) :=_ _βos[+]_ _[η][sa]_ _ηs′a′_ _,_

 

_s_ _So_ _s[′]_ _So_ _s_ _a[′]_

X∈ _∈Y\{_ _}_ X

 

then it holds that
_γ[µ,β]_ = _η_ _γ[µ]_
_N_ _{_ _∈N_ _[|][ p][ao][(][η][)][ ≥]_ [0][ for all][ a][ ∈A][, o][ ∈O}][.]

Then, F and G correspond to the face


int(H) = _η_ _γ[µ,β]_ _pao(η) = 0_ (a, o) _I_
_∈N_ _|_ _⇔_ _∈_

= η _γ[µ]_ _._
_∈N_ _[|][ p][ao][(][η][)][ ≥]_ [0][ and equality if and only if][ (][a, o][)][ ∈] _[I]_

1In order to use the explicit description ofγα(s _,_ ). Then, it holds that _Nγ[µ]_ [given in (2), we remind the reader that][ w]γ[s] [:=][ δ][s] _[⊗]_
_A −_ _|·_ _·_

int(H) = _η_ [0, )[S×A] _pao(η)_ 0 and equality if and only if (a, o) _I_
_∈_ _∞_ _|_ _≥_ _∈_
 _wγ[s]_ _[, η][⟩][S×A]_ [= (1][ −] _[γ][)][µ][s]_ [for][ s][ ∈S] _,_

_⟨_

where we used Proposition 8. Since the discounted state distributions are all positive by assumption,
for η ∈ int(H) it holds ηsa = 0 if and only if τ (a|s) := η(a|s) = 0. Note that τ = π ◦ _β for some_
_π_ ∆[O] [by assumption and thus for][ η][ ∈] [int(][H][)][ it holds that][ η][sa][ = 0][ if and only if]
_∈_ _A_


0 = τ (a|s) =


_β(o|s)π(a|o),_


which holds if and only if (a, o) ∈ _I for every o ∈O with β(o|s) > 0. Hence, if we write_
_J := {(s, a) | (a, o) ∈_ _I for all o ∈O with β(o|s) > 0}, we obtain_

int(H) = _η ∈_ R[S×A] _| ηsa ≥_ 0 and equality if and only if (s, a) ∈ _J,_
n _⟨wγ[s]_ _[, η][⟩][S×A]_ [= (1][ −] _[γ][)][µ][s]_ [for][ s][ ∈S][,]

_pao(η)_ 0 and equality if and only if (a, o) _I_ _._
_≥_ _∈_
o

The number of critical points over this surface is upper bounded by the number of critical points
over

_V = {η ∈_ R[S×A] _| ηsa = 0 for (s, a) ∈_ _J,_
_⟨wγ[s]_ _[, η][⟩][S×A]_ [= (1][ −] _[γ][)][µ][s]_ [for][ s][ ∈S][, p][ao][(][η][) = 0][ for][ (][a, o][)][ ∈] _[I][}][.]_

Now we want to apply (26) and note that the objective p0 = r is generic. Further, we see that
there are |I| non-linear constraints and hence in the notation of (26) have k = |I|. Further, we can
calculate to dimension and co-dimension of V as follows. Note that F →V, π 7→ _η[π]_ is a local
parametrization of V (meaning it parametrizes a full dimensional subset of V), which is injective
and has full rank Jacobian everywhere. Hence, we have

_l = dim(V) = dim(F_ ) = |S|(|A| − 1) −|I| = |S||A| −|S| −|I|.

The co-dimension of V is given by |S||A|− dim(V) = |S| + _|I| and with the notation from (26), we_
have c = |S| + |I| ≥ _k. Further, it holds that deg(pao) ≤_ _do and using (26) yields an upper bound_
of


(do 1)[j][ao] =
_−_

(a,o)X∈I _[j][ao][=][m]_ (a,oY)∈I


_d[k]o[o]_
_oY∈O_ _[·]_


(do 1)[i][o] _._
_oY∈O_ _−_


_do_
_·_
(s,oY)∈I


_o∈O_ _[i][o][=][m]_


-----

**Remark 61 (The mean reward case). Theorem 20 can be generalized to the mean reward case, i.e.,**
to the case of γ = 1 with some adjustments. Indeed, the proof can be carried out analogously,
however, the characterization of N1[µ] [has the extra linear condition that][ P]sa _[η][sa][ = 1][, see also]_

Proposition 8. Indeed, in the mean reward case we have with the notation from the proof above

int(H) = _η ∈_ R[S×A] _| ηsa ≥_ 0 and equality if and only if (s, a) ∈ _J,_
n

_wγ[s]_ _[, η][⟩][S×A]_ [= 0][ for][ s][ ∈S][,] _ηsa = 1,_
_⟨_

_sa_

X

_pao(η)_ 0 and equality if and only if (a, o) _I_ _._
_≥_ _∈_
o

Hence, the upper bound in (5) remains valid if we set

_l := dim_ _η_ R[S×A] _ηsa = 0 for (s, a)_ _J,_ _wγ[s]_ _[, η][⟩][S×A]_ [= 0][ for][ s][ ∈S][,]
_∈_ _|_ _∈_ _⟨_
n

_ηsa = 1, pao(η) = 0 for (a, o) ∈_ _I_ _._ (27)
_sa_

X o

In the discounted case we obtained an explicit formulation for l. In the mean case the value obeys
a case distinction depending, in particular, on whether the all ones vector 1 lies in the span of
_S_
the vectors wγ[s] [. However, the value can be computed from the above expression (27) in any given]
specific case.

**Corollary 62 (Critical points of MDPs). Consider an MDP (S, A, α, r), γ ∈** (0, 1), assume that r
_is generic, that β ∈_ R[S×O] _is invertible, and that Assumption 13 holds. Then, every critical point_
_π_ ∆[O] _[of the discounted expected reward function is deterministic.]_
_∈_ _A_

_Proof. We evaluate the bound of Equation (5). If the face is not a vertex, then the corresponding_
index set I ⊆A × O satisfies |I| < |O|(|A| − 1) and thus in the notation from Theorem 20 it holds
that l > 0. Note that do = 1 for every o ∈O and hence there is at least one factor in the product
in (5) that vanishes and so does the whole expression in (5).

**Remark 63 (Geometry around the critical points). The key argument in the proof of Theorem 20**
is that a critical point π ∆[O] [of the reward function corresponds to a critical point][ η][ of a linear]
_∈_ _A_
function over a multi-homogeneous variety V, where the defining polynomials can be computed by
the means of Proposition 8 and Proposition 14. A closer study of this variety would shed light into
the geometry of the loss landscape around the critical points, which has important implications for
gradient based methods.

**Remark 64 (Efficient design of observation mechanisms). The bound (5) could be used to design**
observation mechanisms in such a way that the reward function has the least critical points, which
would potentially make the system more approachable for gradient based methods. Rauh et al.
(2021) showed that planning in POMDPs is stable under perturbations of the observation kernel β.
More precisely, consider two observation kernels β, β[′] ∆[S] [satisfying][ ∥][β][(][·|][s][)][ −] _[β][′][(][·|][s][)][∥][T V][ =]_
_∈_ _O_

_o[|][β][(][o][|][s][)][ −]_ _[β][′][(][o][|][s][)][|][/][2][ ≤]_ _[ε][ for every][ s][ ∈S][.]_ Then if π ∆[O] [is an optimal policy of]
_∈_ _A_
( _,_ _,_ _, α, β[′], r), then it is a 2εγ_ _r_ _/(1_ _γ)-optimal policy of (_ _,_ _,_ _, α, β, r). Hence, if_

PβS does not fulfill the invertability assumption made in Theorem 20 an arbitrary small perturbation A _O_ _∥_ _∥∞_ _−_ _S_ _A_ _O_

of it does (given that β is a square matrix) and hence Theorem 20 provides an upper bound on the
number of critical points of an approximate problem. Further, note that the faces, which are guaranteed to contain an optimal policy by Mont´ufar & Rauh (2017) might be considerably fewer for
the POMDP (S, A, O, α, β[′], r). The bound (5) could be used to identify the best perturbations of a
given magnitude to obtain a problem with a minimal number of critical points.

**Remark 65 (Design of policy models). Knowledge about the location of critical points of the reward**
function can be used to design policy models, which provably include those critical points and
therefore also the optimal policy.

D.3 NUMBER OF CRITICAL POINTS IN A TWO-ACTION BLIND CONTROLLER

This subsection is devoted to the proof of Proposition 21 that we restate here for convenience.


-----

**Proposition 21 (Number of critical points in a blind controller). Let (S, O, A, α, β, r) be a POMDP**
_describing a blind controller with two actions, i.e.,_ = _o_ _and_ = _a1, a2_ _and let r, α and µ_
_O_ _{_ _}_ _A_ _{_ _}_
_be generic and let γ ∈_ (0, 1). Then the reward function Rγ[µ] _[has at most][ |S|][ critical points in the]_
_interior int(∆[O]_ = (0, 1) of the policy polytope and hence at most + 2 critical points.
_A[)][ ∼]_ _|S|_

Before we present the proof of this result, we discuss how the bound on the rational degree of the
reward function leads to am upper bound on the number of critical points. We consider a blind
controller and restrict ourselves to the discounted case γ ∈ (0, 1). We associate the policy polytope
∆[O] [with][ [0][,][ 1]][ and for][ p][ ∈] [[0][,][ 1]][ we write][ π][p][ and][ η][p][ for the associated policy and the state-action]
_A_
frequency. From Theorem 4 we know that the reward function R = f/g : [0, 1] → R is a rational
function of degree at most k := |S|, which is well known to possess at most 2k − 2 critical points.
Hence, there are at most this many critical points in the interior (0, 1) if the reward function is not
constant. Now we use the geometric description of the set of state-action frequencies and yields a
refined bound.

_Proof of Proposition 21. First, we note that since µ is generic and γ < 1 Assumption 13 is sat-_
isfied. In this case, the combinatorial part is simple, since there are only two zero-dimensional
faces of the state-action frequencies (corresponding to the endpoints of the unit interval) and one
one-dimensional face (corresponding to the interior of the unit interval). Let us set

_U = {η ∈_ R[S×A] _| ⟨wγ[s]_ _[, η][⟩][S×A]_ [= (1][ −] _[γ][)][µ][s]_ [for all][ s][ ∈S}][,]

where wγ[s] [:=][ δ][s]
action frequencies is given by[⊗] [1][A] _[−]_ _[γα][(][s][|·][,][ ·][)][. By Proposition 8 and Example 15 the set of discounted state-]_

_γ[µ,β]_ = _γ[µ]_ [= [0][,][ ∞][)][S×A][ ∩U ∩D][1][.]
_N_ _N_ _[∩D][1]_

Like above, we associate the policy polytope ∆[O] [with][ [0][,][ 1]][ and for][ p][ ∈] [[0][,][ 1]][ we write][ π][p][ and][ η][p][ for]
_A_
the associated policy and the state-action frequency. We aim to bound the number of critical points
of the reward function over (0, 1) or equivalently the number of critical over {η[p] _| p ∈_ (0, 1)} where
we used that Assumption 13 holds. Further, recall that η[p](a|s) = ηsa[p] _[/ρ][p]s[, we have that]_

_η[p]_ _p_ (0, 1) = _η_ _γ[µ,β]_ _η(a_ _s) > 0 for all s_ _, a_
_{_ _|_ _∈_ _}_ _{_ _∈N_ _|_ _|_ _∈S_ _∈A}_

= _η_ _γ[µ,β]_ _ηsa > 0 for all s_ _, a_
_{_ _∈N_ _|_ _∈S_ _∈A}_

= (0, )[S×A] 1.
_∞_ _∩U ∩D_

Thus the number of critical points over {η[p] _| p ∈_ (0, 1)} are upper bounded by the number of critical
points on 1. Note that if α and µ are generic, the subspace is in general position. Further, its
_U ∩D_ _U_
dimension is |S||A| −|S| = |S|, where we used |A| = 2. Hence, the number of complex solutions
to the KKT conditions over U ∩D1 are given by the k-th polar degree δk(D1), where k := |S|,
where we also used the genericity of the reward vector. We can compute the polar degree using the
formula presented by Ozl¨[¨] um C¸ elik et al. (2021) to obtain


_k_ 2

(k _s)!_ _i_ _j_
_−_  (k 1 _i)!_ (2 1 _j)!_
 _i+j=s_ _− _ _−_ _[·]_ _− _ _−_

 [X]k 2

_i_ _j_ _._

 (k 1 _i)!_ (2 1 _j)!_ 

_i+j=s_ _− _ _−_ _[·]_ _− _ _−_

 [X] 


_k−2k+k+2_ _k_ _s + 1_

( 1)[s] _−_
_−_ 2k (k + 1)
_s=0_  _−_

X


_δk(_ 1) =
_D_


2

_k_ _s + 1_
( 1)[s] _−_
_−_ _k_ 1
_s=0_  _−_

X


(k − _s)!_


We calculate the three individual terms to be
_k + 1_ _k!_ _ki_ 2j = [(][k][ + 1)][k] _k!_ _k0_ 20 _,_
_k_ 1  (k 1 _i)!_ (2 1 _j)!_  2 _·_ _·_ (k 1)! 1! [= (][k][ + 1)]2 _[k][2]_
 _−_  i+[X]j=0 _− _ _−_ _[·]_ _− _ _−_  _− _  _[·]_   

and

_k_ (k 1)! _k1_ 21 = _k!_ _k_ 2 = _k[2](k_ 1) 2k,
_−k −_ 1 _−_ (k −  2)! [+] (k −  1)! ! _−_  (k − 2)! [+] (k − 1)!  _−_ _−_ _−_


-----

and
_k −_ 1
_k_ 1
 _−_


_k_ 2
1 1

(k 2)! [+]

  _− _ 


_k_
2

(k 3)!
_− _ 


2k
= (k 2)!
_−_ (k 2)! [+][ k]2([(]k[k][ −]3)![1)]
 _−_ _−_

= 2k + _[k][(][k][ −]_ [1)(][k][ −] [2)] _._


(k − 2)!


Adding those three summands we obtain

_δk(_ 1) = _[k][3][ +][ k][2][ +][ k][3][ −]_ [3][k][2][ + 2][k]
_D_ 2


_−_ _k[3]_ + k[2] _−_ 2k + 2k = k.


Note that there is also a more structural argument to obtain this polar degree. In fact, the polar
degree δl( 1) = 0 for l > dim( 1[)][ −] [1][, where][ D]1[∗] [denotes the dual variety of][ D][1] [( ¨]Ozl¨um C¸ elik
_D_ _D[∗]_
et al., 2021). Note that in the case of k 2 matrices 1 [=][ D][1] [(Draisma et al., 2016) and hence]
_×_ _D[∗]_
it holds that δl( 1) = 0 for l > dim( 1) 1 = k (Spaenlehauer, 2012). The largest non-zero
_D_ _D_ _−_
polar degree is equal to the degree of the dual variety (Draisma et al., 2016) and hence we obtain
_δk(D1) = degree(D1[∗][) = degree(][D][1][) =][ k][ (Spaenlehauer, 2012).]_

Note that this bound is not necessarily sharp, since it is exactly the number of complex solutions of
the criticality equations over 1. Overall, we have seen that the study of the algebraic properties
_U ∩D_
of the reward function provided an upper bound on the number of critical points of the problem,
which can be improved using the description of the state-action frequencies as a basic semialgebraic
set and employing tools from algebraic geometry.

D.4 EXAMPLES WITH MULTIPLE SMOOTH AND NON-SMOOTH CRITICAL POINTS

It is the goal of this example to demonstrate that for a blind controller multiple critical points can
occur in the interior (0, 1) = int(∆[O] = ∆[O] [of the policy]

_[∼]_ _A[)][ as well as at the two endpoints of][ [0][,][ 1]][ ∼]_ _A_
polytope. We refer to such points as smooth and non-smooth critical points. We consider a blind
controller with one observation, two actions a1, a2 and three states s1, s2, s3 and a deterministic
transition kernel α and reward described by the graph shown in Figure 2.

_a1, 5_

_s1_

_a1, −30_ _a1, 0_ _a2, 0_

_s2_ _s3_

_a2,_ 5
_−_


Figure 2: Graph describing the deterministic transition kernel α and the associated instantaneous
rewards.

We make the usual identification [0, 1] = ∆[O]

_[∼]_ _A[, where we associate][ p][ with][ π][(][a][1][|][o][)][. In Figure 3, the]_
reward function is plotted on the left for the three initial conditions µ = δs1 _, δs2_ _, δs3_ . It is apparent
that the reward has two critical points in the interior of the policy polytope ∆[O]A = [0, 1] for the
two initial conditions µ = δs1 = δs3 . For µ = δs2, there are two strict local maxima on the two[∼]
endpoints of the interval. In this example, the bound from Proposition 21 ensures that there are at
most |S| = 3 critical points in the interior and at most |S| +2 = 5 critical points in the whole policy
polytope. We see that those bounds are not sharp in this specific setting. Note that this example is
stable under small perturbations of the transition kernel and reward vector and hence can occur for


-----

generic α and r. The right hand side of Figure 3 shows a three dimensional random projection of the
set of feasible discounted state-action frequencies. By Theorem 4 they are a curve in R[S×A][ ∼]= R[6]

Figure 3: Plot of the reward function for initial distributions δs on the left and of a three-dimensional

with an injective rational parametrization of degree at most |S| = 3.

starting at s_3
starting at s_2
starting at s_1

Discounted reward

0.0 0.2 0.4 0.6 0.8 1.0

Probability of selecting first action

random projection of the set of feasible discounted state-action frequencies on the right.

D.5 (SUPER)LEVEL SETS OF (PO)MDPS

D.5.1 CONNECTEDNESS OF SUPERLEVEL SETS IN MDPS

**Theorem 66 (Existence of improvement paths in MDPs). For every policy π** ∆[S]
_∈_ _A[, there is a]_
_continuous path connecting π to an optimal policy along which the reward is monotone. If further_
_π 7→_ _η[π]_ _is injective, the reward is strictly monotone along this path, if π is suboptimal. In particular,_
_the superlevel sets of MDPs are connected._

_Proof. Let us fix π_ ∆[S] [and set][ η][0][ :=][ η][π][ and][ η][1][ be a global optimum and][ η][t][ be the linear]
_∈_ _A_
interpolation and ρt be the corresponding state marginal. Note that for s ∈S it holds that either
_ρt(s) > 0 for all t_ (0, 1) or ρt(s) = 0 for all t [0, 1]. In the latter case, we can set πt( _s) to_
_∈_ _∈_ _·|_
be an arbitrary element in ∆ . For the other states and t (0, 1) we can define the policy through
_A_ _∈_
conditioning by πt(a _s) := ηt(s, a)/ρt(s) and will continuously extend the definition to t_ 0, 1
_|_ _∈{_ _}_
in the following. If ρ0(s) > 0 or ρ1(s) > 0, then the definition extends naturally. Suppose that
_ρ0(s) = 0, then we now that ρ1(s) > 0 since otherwise ρt(s) = 0 for all t_ [0, 1]. Now for t > 0
_∈_
it holds that

_πt(s, a) =_ _[η][t][(][s, a][)]_ = [(1][ −] _[t][)][η][0][(][s, a][) +][ tη][1][(][s, a][)]_ = _[tη][1][(][s, a][)]_ = _[η][1][(][s, a][)]_

_ρt(s)_ (1 _t)ρ0(s) + tρ1(s)_ _tρ1(s)_ _ρ1(s)_ _[,]_

_−_

which extends continuously to t = 0. If ρ1(s) = 0, then like before, πt( _s) does not depend on t_

_·|_
and we can extend it to t = 1. Now we have constructed a continuous path πt, such that η[π][t] = ηt
and thus
_R(πt) =_ _r, ηt_ = (1 _t)_ _r, η0_ + t _r, η1_ = R(π0) + t(R[∗] _R(π0)),_
_⟨_ _⟩_ _−_ _⟨_ _⟩_ _⟨_ _⟩_ _−_
which is strictly increasing if π0 is suboptimal. It remains to construct a continuous path between
_π0 and π. Note that if ρ0(s) > 0, the policies π0 and π agree on the state s and so does the_
linear interpolation between the two policies. Now, by Proposition 46 we see that every linear
interpolation between π0 and π has the state-action distribution η0. Gluing the two paths, we obtain
a path that first leaves the state-action distribution unchanged and then increases the reward strictly
up to optimality.

D.5.2 THE SEMIALGEBRAIC STRUCTURE OF LEVEL AND SUPERLEVEL SETS FOR POMDPS

Consider a POMDP (S, A, O, α, β, r) and fix a discount rate γ ∈ (0, 1) as well as an initial condition
_µ_ ∆ . The levelset
_∈_ _S_
_La :=_ _π_ ∆[O] _γ_ [(][π][) =][ a][}]
_{_ _∈_ _A_ _[|][ R][µ]_


-----

of the reward function is the intersection of a variety generated by one determinantal polynomial
of degree at most |S| with the policy polytope ∆[O]A[. Indeed, by Theorem 4 the reward function][ R]γ[µ]
is the fraction f/g of two determinantal polynomials f and g of degree at most |S|. The level set
consists of all policies, such that f (π) = ag(π). Thus, the levelset is given by

_La = ∆[O]_ _x_ R[O×A] _f_ (x) _ag(x) = 0_ _._
_A_ _[∩]_ _∈_ _|_ _−_

Analogously, a superlevel set is the intersection


∆[O] _x_ R[O×A] _f_ (x) _ag(x)_ 0
_A_ _[∩]_ _∈_ _|_ _−_ _≥_

of a basic semialgebraic generated by one determinantal polynomial of degree at most _|S| with the_
policy polytope ∆[O]
_A[. In particular, both the levelset and superlevel sets of POMDPs are semialge-]_
braic sets defined by linear inequalities and equations (corresponding to the conditional probability
polytope ∆[O]
_A[) and a determinantal (in)equality of degree at most][ |S|][. This description can be used]_
to bounds the number of connected components, which captures important properties of the loss
landscape of an optimization problem (Barannikov et al., 2019; Catanzaro et al., 2020). By a theorem due to Łojasiewicz, level and superlevel sets possess finitely many connected (semialgebraic)
components (Ruiz, 1991; Basu et al., 2006) and there exist algorithmic approaches to computing the
number of connected components (Grigor’ev & Vorobjov, 1992) as well as explicits upper bounds,
which involve the dimension, the number of defining polynomials as well as their degrees (Basu,
2003; 2014). Those results are generalizations of the classic result due to Milnor and Thom which
bounds the sum of all Betti numbers of a variety. If we apply the Milnor-Thom theorem to the variety V we obtain that there are at most |S|(2|S|− 1)[|O||A|−][1] many connected components of V. This
bound neglects the determinantal nature of the defining polynomial and might therefore be coarse.
Using an analogue approach, we can also study the level and superlevel sets of the reward function
in the space of feasible state-action frequencies. Indeed, they are the intersections of the hyperplane
_η_ R[S×A] _r, η_ = a and halfspace _η_ R[S×A] _r, η_ _a_ with the semialgebraic
set{ _∈_ _γ[µ,β]_ of state-action frequencies.| ⟨ _⟩S×A_ _}_ _{_ _∈_ _| ⟨_ _⟩S×A ≥_ _}_
_N_

E POSSIBLE EXTENSIONS

E.1 APPLICATION TO FINITE MEMORY POLICIES

In general, it is possible to reduce POMDPs with finite memory policies to a POMDP with memoryless policies by augmenting the state and observation space with the memory. Say we consider
policies with a memory that stores the last k observations that were made. Then we could set
_S˜ := S × O[k][−][1]_ and _O[˜] := O[k]. If the first state is s0 and the first observation that is being made is_
_o0, then we will associate it with ˜s0 := (s0, o0, . . ., o0) ∈_ _S[˜] and ˜o0 := (o0, . . ., o0) ∈_ _O[˜] respec-_
tively. If after t steps, the current state is ˜st = (st, ot _k, . . ., ot_ 1) and the next observation is ot,
_−_ _−_
then we set ˜ot := (ot _k, . . ., ot). An analogue strategy can be taken when the memory does consist_
_−_
of more than the history of observations and for example includes the history of decision. It remains
open to explore the implications of the translation of our results to policies with internal memory
with this identification.

E.2 POLYNOMIAL POMDPS

Zahavy et al. (2021) consider MDPs, where the objective is a convex function of the state-action frequency, i.e., where Rγ[µ][(][π][) =][ f] [(][η]γ[π,µ]) for some convex function f : R[S×A] R and coin the name
_→_
of convex MDPs. In analogy, we refer to the case where f is a polyomial function as polynomial
_(PO)MDPs. In polynomial POMDPs, the problem of reward maximization is by definition an op-_
timization problem of a polynomial function over the set _γ[µ,β]_ of feasible state-action frequencies.
_N_
Since the feasible state-action frequencies form a basic semialgebraic set, the problem of reward
maximization in polynomials is a polynomial optimization problem. Hence, the method of bounding the number of critical points as discussed in Section 5 generalizes to the case of polynomial
reward criteria. If f is a polynomial of degree d, the upper bound (5) from Theorem 20 takes the
form

_d[k]o[o]_ (d 1)[i][ Y] (do 1)[i][o] _._
_oY∈O_ _[·]_ _i+[P]oX∈O_ _[i][o][=][m]_ _−_ _o∈O_ _−_


-----

The use of polar degrees does not extend in general to the case of polynomial POMDPs, since they
require a linear objective function, but can still be related to the algebraic degree for a quadratic
objective as it is the case for the Euclidean distance function (Draisma et al., 2016).

F EXAMPLES

Here, we provide examples, which illustrate our findings. In particular, we compute the defining
polynomial inequalities of the set of feasible state-action frequencies for the example from Figure 1
and a navigation problem in a grid world. We use an interior point method to solve the constrained
optimization problem corresponding to the polynomial programming formulation of the respective
POMPDs and see that in this offers a computationally feasible approach to the reward maximization
problem.

F.1 TOY EXAMPLE OF FIGURE 1

We discuss in detail a toy POMDP which we used to generate the plots in Figure 1. We consider
state, observation, and action spaces with two elements each, as well as following deterministic
transition mechanism α, observation mechanism β, and instantaneous reward r:

_S = {s1, s2},_ _β =_ 1 0 _,_ _r(s, a) = δs1,sδa1,a + δs2,sδa2,a,_
_O = {o1, o2},_ 1/2 1/2 _γ = 0.5._
= _a1, a2_ _,_ _α(si_ _sj, ak) = δik,_
_A_ _{_ _}_ _|_

The transitions, instantaneous rewards, and observations are shown in Figure 4. As an initial distribution we take the uniform distribution µ = (δs1 + δs2 )/2 over the states.


_a2_

_a1_



_s1_ _s2_


1

_s1_ _o1_

1/2

_s2_ _o2_

1/2


Figure 4: The left shows the transition graph of the toy example. The right shows the observation
mechanism; the numbers on the edges indicate the observation probabilities.

**Polynomial programming formulation** To illustrate Theorem 16 (and Proposition 14), we derive step-by-step the explicit polynomial program for the reward maximization in this toy example.
For this, we first compute the defining inequalities of the set of feasible state-action frequencies.
We begin with the linear constraints that define the set Nγ[µ] [of state-action frequencies of the asso-]
ciated MDP, given in general form in Proposition 8. In the remainder, we denote the state-action
frequencies as matrices


_η =_ _η(s1, a1)_ _η(s1, a2)_
_η(s2, a1)_ _η(s2, a2)_



_η11_ _η12_
_η21_ _η22_


_∈_ R[S×A].


Following Proposition 8, the linear inequalities are ηij 0 for all i, j 1, 2, and the linear
_≥_ _∈{_ _}_
equations are _wγ[i]_ _[, η][⟩][S×A]_ [= (1][ −] _[γ][)][µ][i]_ [= 1][/][4][ for][ i][ ∈{][1][,][ 2][}][, whereby here]
_⟨_

1 1 1 0 1 2
_wγ[1]_ [=][ δ][1] _[⊗]_ [1][A] _[−]_ _[γα][(1][|·][,][ ·][) =]_ 0 0 _−_ 2[1] 1 0 = 2[1] 1 0
    − 

and

0 0 0 1 0 1
_wγ[2]_ [=][ δ][2] _[⊗]_ [1][A] _[−]_ _[γα][(2][|·][,][ ·][) =]_ 1 1 _−_ 2[1] 0 1 = 2[1] 2 _−1_ _._
     

Thus the two linear equations are
2η11 + 4η12 2η21 = 1
_−_ (28)
_−2η12 + 4η21 + 2η22 = 1._


-----

It remains to compute the polynomial inequalities, which can be done using Remark 18. We invert
the matrix β and obtain

1 0
_β[+]_ = β[−][1] = R[S×O].
1 2 _∈_
− 

Using the notation from Remark 18 we have So1 = {s1} and So2 = {s1, s2}, and thus the polynomial inequalities are

_η11_ 0
_≥_
_η12_ 0
_≥_
_−η11(η21 + η22) + 2η21(η11 + η12) ≥_ 0
_−η12(η21 + η22) + 2η22(η11 + η12) ≥_ 0.

The first two inequalities can be seen to be redundant and can be discarded. Finally, note that the
objective function is given by
_r, η_ = η11 + η22.
_⟨_ _⟩S×A_
Hence, we have obtained the following explicit formulation of the reward maximization problem as
a polynomial optimization problem:


_−22ηη1211 + 4 + 4η11ηη2112, η + 2 −12, η2ηη212221, η − −221 = 01 = 00_
_ηη1112ηη2122 + 2 + 2ηη2111ηη2212 − −_ _ηη1211ηη2122 ≥ ≥ ≥_ 00.


maximize η11 + η22 subject to


(29)


**Solution with constrained and polynomial optimization tools** The formulation (29) allows us
to use polynomial optimization algorithms, semi-definite programming (SDP) solvers, or relaxation
hierarchies such as the popular Sum Of Squares (SOS). Using the modeling language JuMP and the
interior point solver Ipopt we directly obtained the globally optimal[10] solution to problem (29)
(rounded to three digits)

0.667 0
_η[∗]_ = _._
0.167 0.167
 

The corresponding optimal state policy τ _[∗]_ is obtained simply by conditioning on states, and any
pre-image under the observation kernel is an optimal observation policy, in this case simply π[∗] =
_β[−][1]τ_ _[∗],_


_π[∗]_ =


∆[O]
_∈_ _A[.]_


This policy achieves a reward of Rγ[µ][(][π][∗][) =][ ⟨][r, η][∗][⟩][S×A] [= 0][.][833][ (rounded to three digits). The]
computations took 0.01s (on a 2 GHz Quad-Core Intel Core i5 processor). The command in JuMP
to call the optimizer Ipopt is simply:

model = Model(optimizer_with_attributes(Ipopt.Optimizer)
@variable(model, \eta[1:2, 1:2]>=0)
@constraint(model, 2\eta[1, 1] + 4\eta[1, 2] - 2\eta[2, 1] == 1)
@constraint(model, -2\eta[1, 2] + 4\eta[2, 1] + 2\eta[2, 2] == 1)
@constraint(model, \eta[1, 1]\eta[2, 1] + 2\eta[2, 1]\eta[1, 2]

-  \eta[1, 1]\eta[2, 2] >= 0)
@constraint(model, \eta[1, 2]\eta[2, 2] + 2\eta[1, 1]\eta[2, 2]

-  \eta[1, 2]\eta[2, 1] >= 0)
@NLobjective(model, Max, \eta[1, 1] + \eta[2, 2])
optimize!(model)

For completeness, we also provide the command to solve a relaxation in Python SumOfSquares,
which is the following, although we found this to run a bit slower depending on the selected degree. Here we negate the objective in order to obtain a minimization problem and square the search
variables (which are required to be non-negative) in order to obtain polynomials of even degree:

10The SOS relaxation provides a certificate for global optimality in this case.


-----

e11, e12, e21, e22 = sp.symbols(’e11 e12 e21 e22’)
prob = poly_opt_prob([e11, e12, e21, e22], - e11**2 - e22**2,
eqs=[+ 2 * e11**2 + 4 * e12**2 - 2 * e21**2 - 1,

-  2 * e12**2 + 4 * e21**2 + 2 * e22**2 - 1,
+ e11**2 + e12**2 + e21**2 + e22**2 - 1],
ineqs=[e11**2 * e21**2 + 2 * e21**2 * e12**2 - e11**2 * e22**2,
e12**2 * e22**2 + 2 * e11**2 * e22**2 - e12**2 * e21**2], deg=2)
prob.solve()
print(prob.value)

**Policy gradient methods may not find a global optimum** We want to demonstrate an important
problem of policy gradient methods, which is the well known possibility to get stuck in local optima,
in the case of the toy example. For this, we used a tabular softmax policy model to represent the
interior of the policy polytope ∆[O]
_A[, i.e. used the following parametric policy model]_

exp(θoa)
_πθ(a|o) :=_ _a[′][ exp(][θ][oa][′]_ [)] for θ ∈ R[O×A].

We computed 15 policy gradient trajectories, where we used the policy gradient theorem (see Corol-P
lary 52) to compute the update directions. The starting positions where generated randomly, such
that the initial conditions in the policy polytope ∆[O] [are uniformly random. The trajectories in the]
_A_
policy polytope ∆[O] = [0, 1][2] are shown in Figure 5, which also shows a heat map of the reward
function. We observe that 5 of the trajectories converge to a suboptimal strict local minimum. NoteA _[∼]_
that this is not artefact of the parametrization, but of the fact that there is a strict local minimum
and hence every naive local optimization method will suffer from this problem. The reward of the
suboptimal local minimum


∆[O]
_∈_ _A_


_π =_


is 0.747 if rounded to 3 digits.


_R_

1.0

0.7848

0.8 0.7308

0.6768

0.6 0.6228

_|1)(2_ 0.5688
_π_ 0.4 0.5148

0.4608

0.2 0.4068

0.3528

0.0 0.2988

0.0 0.2 0.4 0.6 0.8 1.0

_π(2|2)_

Figure 5: Policy gradient optimization trajectories (shown as black curves) in the observation policy
polytope. As expected, since the problem is non-convex and has several distinct local optimizers,
the trajectories converge to different local optimizers depending on the initial policy.


**Number of critical points** We evaluate the bound of Theorem 20 for this toy problem. First note
that in this example the observation matrix β is invertible with β[−][1] = 1−1 20 . Further, Assumption 13 is satisfied for initial distributions µ with full support. Hence, we can apply Theorem 20.
Here, we have |S| = |A| = |O| = 2 and in the notation of Theorem 20 we have   _do1 = 1 and_
_do2 = 2. As discussed in the main body, the bound evaluates to zero if we consider the inte-_
rior of the policy polytope, which corresponds to I = ∅. This means that there are no critical


-----

points in the interior of the policy polytope, in other words, all optimal policies lie at the boundary
and hence have one or more zero entries. The one-dimensional faces correspond to the index sets
(a1, o1) _,_ (a2, o1) _,_ (a1, o2) _,_ (a2, o2) . The choices I = (a1, o1) _,_ (a2, o1) correspond to
_{_ _}_ _{_ _}_ _{_ _}_ _{_ _}_ _{_ _}_ _{_ _}_
the two edges on the left and right of the policy polytope ∆[O] [as shown in the top left corner of]
_A_
Figure 1 or alternatively to the two straight faces of the set _γ[µ,β]_ of state-action distributions shown
_N_
in the top right corner. The bound (5) evaluates to zero for those choices. This can also be seen in
the bottom row in Figure 1, where it is apparent that there are no critical points on the respective
faces. For the choices I = (a1, o2) _,_ (a2, o2) the bound (5) evaluates to two. Indeed, these faces
_{_ _}_ _{_ _}_
contain critical points. The bound is not sharp in this case since the actual number of critical points
in any of the two faces of the policy polytope ∆[O]
_A[, which correspond to the two non-linear faces of]_
_γ[µ,β]_ is one. Nonetheless, this illustrates how the theorem allows us to discard most faces of the
_N_
polytope and focus the search for an optimal policy on just two faces.

F.2 NAVIGATION IN A GRID WORLD

|1 2|Col2|1|2|3|4|5|
|---|---|---|---|---|---|---|
|||||6|||
|7|8|9|10|11|||
||12||||||
||13||||||



Figure 6: Depiction of a grid world; the reward of R is obtained in state 1, the actions are
_{R, L, U, D} corresponding to movements to the right, left, up and down; observed are the pos-_
sible directions that the agent can move in. Once the agent transitions to state 1, she transfers to
state 7 and 13 uniformly.

We consider the grid world depicted in Figure 6 with 13 states and 7 observations, where it
is the goal to reach state 1. The four actions are {R, L, U, D} corresponding to the directions
right, left, up and down on the grid. The transitions are deterministic and lead to the cell right,
left, above or below the current cell, if this cell is admissible; from the goal state 1 one transitions uniformly to the states 7 and 13 independently of the chosen action. Further, we consider deterministic observations, which correspond to the agent being able to observe its immediate four neighboring positions. This observation mechanism partitions the state space into the
seven subsets {1, 7}, {2, 4, 9, 10}, {3, 8}, {5}, {6, 12}, {11}, {13}, which lead to the observations
_o1, o2, o3, o4, o5, o6 and o7 respectively. Hence, by Remark 57 the polynomial constraints are given_
by

_η7aρ1 −_ _η1aρ7 = 0_ for all a ∈A
_η4aρ2 −_ _η2aρ4 = 0_ for all a ∈A
_η8aρ2 −_ _η2aρ8 = 0_ for all a ∈A
_η10aρ2 −_ _η2aρ10 = 0_ for all a ∈A
_η9aρ3 −_ _η3aρ9 = 0_ for all a ∈A
_η12aρ6 −_ _η6aρ12 = 0_ for all a ∈A,


-----

where ρs = _a_ _[η][sa][. The linear constraints apart from][ η][ ≥]_ [0][ can be computed to be]

[P] _ρ1 −_ _γ(η12 + η13 + η14 + η22) = µ1_

_ρ2 −_ _γ(η11 + η23 + η24 + η32) = µ2_
_ρ3 −_ _γ(η21 + η33 + η42) = µ3_
_ρ4 −_ _γ(η31 + η43 + η44 + η52) = µ4_
_ρ5 −_ _γ(η41 + η53 + η54) = µ5_
_ρ6 −_ _γ(η34 + η61 + η62 + η11,3) = µ6_
_ρ7 −_ _γ(ρ1/2 + η72 + η73 + η74 + η82) = µ7_
_ρ8 −_ _γ(η71 + η83 + η84 + η92) = µ8_
_ρ9 −_ _γ(η81 + η93 + η10,2) = µ9_
_ρ10 −_ _γ(η91 + η10,3 + η10,4 + η11,2) = µ10_
_ρ11 −_ _γ(η10,1 + η11,1 + η11,4) = µ11_
_ρ12 −_ _γ(η94 + η12,1 + η12,2 + η13,3) = µ12_
_ρ13 −_ _γ(ρ1/2 + η12,4 + η13,1 + η13,2 + η13,4) = µ13._

Further, the objective function is given by

_r, η_ = η11 + η12 + η13 + η14.
_⟨_ _⟩S×A_

Let us now consider the uniform distribution µs = 1/13 for s ∈S as an initial distribution and
_γ = 0.999 as a discount factor. Like for the toy problem we used the interior point method Ipopt_
implemented in the Julia packages JuMP and Ipopt to solve this polynomial optimization problem.
The solver took around 0.03s consistently (on a 2 GHz Quad-Core Intel Core i5 processor). The
found solution is (rounded to three digits)


1 0.033 0.000 0.000 0.000
2 0.044 0.033 0.000 0.000
3  0.049 0.077 0.000 0.000
4 0.066 0.049 0.000 0.000



5  0.000 0.066 0.000 0.000



6  0.000 0.000 0.033 0.000
7  0.133 0.000 0.000 0.000



8  0.075 0.117 0.000 0.000



9  0.057 0.042 0.000 0.000



10  0.033 0.024 0.000 0.000



11  0.000 0.000 0.033 0.000



12  0.000 0.000 0.017 0.000
13  0.000 0.000 0.016 0.000





_η[∗]_ =


∆ R[S×A]
_∈_ _S×A ⊆_


and has the objective value ⟨r, η[∗]⟩S×A = 0.033. The corresponding optimal state policy τ _[∗]_ is
obtained simply by conditioning on states, and any pre-image under the observation kernel is an
optimal observation policy, in this case simply π[∗] = β[+]τ _[∗]_ which is (rounded to two digits)


_o1_ 1.00 0.00 0.00 0.00
_o2_ 0.53 0.47 0.00 0.00
_o3_  0.48 0.52 0.00 0.00
_o4_ 0.00 1.00 0.00 0.00



_o5_  0.00 0.00 0.99 0.00



_o6_  0.00 0.00 1.00 0.00
_o7_  0.00 0.00 0.99 0.00





_π[∗]_ =


∆[O]
_∈_ _A[.]_


This policy

1. moves right on observation 1 corresponding to the states 1 and 7,
2. moves right and left with probability close to 1/2 on observation 2 corresponding to states
2, 4, 9 and 10,


-----

3. moves right and left with probability close to 1/2 on observation 3 corresponding to the
states 3 and 8,

4. moves left on observation 4 corresponding to the state 5,

5. moves up on observation 5 corresponding to the states 6 and 12,

6. moves up on observation 6 corresponding to the states 11,

7. moves up on observation 7 corresponding to the state 13.

The action choices of the policy π[∗] are also shown in Figure 7. Note that the policy ˆπ selects
the best action in the states 1, 5, 6, 11, 12 and 13. Those are the states that are either identifiable
from its observation (this is the case for 5, 11 and 13) or where the optimal actions of all states
leading to the same observation agree (this is the case for the pairs {1, 7} and {6, 12}). In the other
states, where the corresponding observation is ambiguous, the policy randomizes among the two
actions, which are optimal for the compatible states. This is for example the case for the states
2, 4, 9 and 10, which all lead to observation 2. The optimal MDP policy would move left in state
2 and 4 and move right in the states 9 and 10. The POMDP policy has to randomize between
moving left and right, since otherwise the agent could never reach the goal state if starting in 7 or
13. The same consideration applies to the states 3 and 8, which both lead to observation 3. The Julia
[code is available in the supplements and under https://github.com/muellerjohannes/](https://github.com/muellerjohannes/geometry-POMDPs-ICLR-2022)
[geometry-POMDPs-ICLR-2022.](https://github.com/muellerjohannes/geometry-POMDPs-ICLR-2022)

|→ ↔|Col2|→|↔|↔|↔|←|
|---|---|---|---|---|---|---|
|||||↑|||
|→|↔|↔|↔|↑|||
||↑||||||
||↑||||||



Figure 7: Depiction of the policy ˆπ, which is found using the polynomial programming formulation
of the POMDP and applying the interior point method Ipopt implemented in the Julia libraries JuMP
and Ipopt; an arrow into two direction indicates that the agent moves into those two directions with
probability close to 1/2.

F.3 A THREE DIMENSIONAL EXAMPLE

Let us now discuss an example where the set of discounted state-action distributions is threedimensional and not two-dimensional as before. For this, we consider a generalization of the previous example where |S| = 3, |A| = 2 and |O| = 3 such that

dim(∆[S]
_A[) = dim(∆][O]A[) = 3][ ·][ (2][ −]_ [1) = 3][.]

The observation mechanism used is


1/2 1/2 0
1/3 1/3 1/3


_β =_


Further, the action mechanism α and the initial distribution µ are sampled randomly and the used
discount factor is 0.5. Since the initial distribution is generic and β is invertible, the set of stateaction frequencies Nγ[µ] [and the set of feasible state-action frequencies][ N][ µ,β]γ are three-dimensional
and are in fact combinatorially equivalent to the three-dimensional cube ∆[S] = ∆[O] = [0, 1][3] (see
Theorem 16). In Figure 8 we plot a random three-dimensional projection of the sets. More precisely,A _[∼]_ _A_ _[∼]_
we plot their one-dimnesional faces dashed and solid for the MDP and POMDP respectively. The
combinatorial equivalence to the three-dimensional cube can be see in this plot.


-----

three dimensional cube.


Figure 8: A random three-dimensional projection of the set of discounted state-action frequencies of
the MDP is the polytope defined by the dashed straight edges. The same random three-dimensional
projection of the set of feasible discounted state-action frequencies is the basic semialgebraic set
where the edges are shown by the solid lines. Both of those sets are combinatorially equivalent to a


-----

