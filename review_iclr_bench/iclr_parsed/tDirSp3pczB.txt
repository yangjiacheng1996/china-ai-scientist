# SHARP LEARNING BOUNDS FOR CONTRASTIVE UNSU## PERVISED REPRESENTATION LEARNING

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Contrastive unsupervised representation learning (CURL) encourages data representation to make semantically similar pairs closer than randomly drawn negative
samples, which has been successful in various domains such as vision, language,
and graphs. Although recent theoretical studies have attempted to explain its success by upper bounds of a downstream classification loss by the contrastive loss,
they are still not sharp enough to explain an experimental fact: larger negative samples improve the classification performance. This study establishes a downstream
classification loss bound with a tight intercept in the negative sample size. By
regarding the contrastive loss as a downstream loss estimator, our theory not only
improves the existing learning bounds substantially but also explains why downstream classification empirically improves with larger negative samples—because
the estimation gap of the downstream loss decays with larger negative samples.
We verify that our theory is consistent with experiments on synthetic, vision, and
language datasets.

1 INTRODUCTION

The contrastive loss (Chopra et al., 2005) is one of the popular loss functions in metric learning (Kulis,
2012) and representation learning (Bengio et al., 2013). The contrastive loss forces data representation
of semantically similar pairs closer in some metric space than multiple random samples, called
negative samples. Many state-of-the-art representation learning algorithms use a type of contrastive
losses in natural language processing (Mikolov et al., 2013; Logeswaran & Lee, 2018), vision (Chopra
et al., 2005; He et al., 2019; Chen et al., 2020), and graph (Lirong et al., 2021) domains. A simple
model built on top of the learned representation can achieve almost the same accuracy as supervised
learning does. See surveys (Le-Khac et al., 2020; Schmarje et al., 2021) and references therein for
recent empirical progress.

Compared to its empirical success, we still do not know much about the theoretical perspective of the
contrastive loss function. Since the first theoretical analysis of contrastive unsupervised representation
learning (CURL) (Arora et al., 2019), a number of follow-up researches (Nozawa et al., 2020; Chuang
et al., 2020; Nozawa & Sato, 2021; Ash et al., 2021) improved theoretical understanding of CURL.
They provided upper bounds of a downstream classification loss by the contrastive loss. While they
partially explain why CURL provides good representation in terms of downstream classification, a
controversy has been held on the impact of the negative sample size, denoted by K in this work. From
the empirical viewpoint, it has been widely shown that downstream classification accuracy tends to be
better with the larger K (He et al., 2020; Chen et al., 2020).[1] From the theoretical viewpoint, although
Ash et al. (2021) and Nozawa & Sato (2021) addressed the issue of the earlier analysis (Arora et al.,
2019)—the bound grows exponentially in K—their improved bounds are still so huge that they
may not capture the actual behavior of CURL related to K, as we will see in Figure 1. In addition,
Ash et al. (2021) claims that the optimal K exists in smaller K, which does not agree well with
the empirical observations. Henceforth, the relationship between the performance of downstream
classification and K needs to be investigated carefully.

This study aims to clarify the relationship between the performance of downstream classification
and the negative sample size K by deriving an upper bound of the downstream classification loss

1The empirical evidence has also not reached a consensus yet. Chen et al. (2021) reported that SimCLR with
the smaller K could be competitive in linear evaluation than K used by the original SimCLR (Chen et al., 2020).


-----

(Theorem 1). By deriving the lower bound (Theorem 2), we found that the intercept of our upper
bound is tight in K. Notably, such a lower bound has yet to be known in CURL so far. Our bounds
support that the larger K reduces estimation gap of the downstream classification loss (Section 3.2),
while the smaller K could perform well. This finding is evidence for the empirical success of
the larger K. In addition, our bounds are compared with the existing bounds to demonstrate that
there is no optimal point in K and how accurately the derived bounds capture the downstream loss
(Section 4). Finally, we empirically verify our theory by experiments (Section 6) on a synthetic
dataset, CIFAR-10/100 (Krizhevsky, 2009) datasets, and Wiki-3029 dataset (Arora et al., 2019).

2 FORMULATION OF CONTRASTIVE LEARNING


First, this section briefly summarizes the problem setup and formulation of CURL.


10[4]

10[3]

10[2]

10[1]


**Notation.** The C-dimensional vector whose
elements are all one is denoted by 1C :=

[1 1 _. . ._ 1][⊤]. When it is clear from context, the subscript is abbreviated. For a vector
**a ∈** R[p], a(i) denotes the i-th largest element
of a, namely, a(1) _a(2)_ _a(p). Like-_
wise, a( _i) denotes the ≥_ _i-th smallest element of ≥· · · ≥_
_−_
the vector a. The indicator function is denoted
by 1 _A_ for a predicate A. Let := **p**
_{_ _}_ _△[C]_ _{_ _∈_

[0, 1][C] _| p[⊤]1 = 1} be the C-dimensional prob-_
ability simplex. For p ∈△[C], the Shannon entropy is denoted by H (p) := − [P]c∈[C] _[p][c][ ln][ p][c][.]_

Let Hn be the n-th harmonic number.

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
|||Ou|rs||||||
|||Aro No|ra et al. zawa & Sa|to|||||
|||Ash Sup|et al. ervised lo|ss|||||
||||||||||
||||||||||
||||||||||
||||||||||


16 32 64 128 512


Figure 1: Empirical comparison of our upper

**Supervised classification.** One of the goals bound and the existing bounds on CIFAR-10 (C =
in machine learning is supervised classification, 10). Arora et al.’s and Nozawa & Sato’s bounds
while we consider the setup where supervision is are valid only at K + 1 ≥ _C. Note that Arora_
unavailable. Here, we first formulate the multi- et al.’s and Ash et al.’s bounds become infinity at
class classification problem. Assume that there _K = 512. As can be seen, our bound is the closest_
exists an unknown number of latent classes de- approximator of the true mean supervised losses.
noted by C ∈ N. Let X be d-dimensional fea- The detailed experimental setup is stated in Secture space and Y := [C] be the supervised label tion 6.2.
set.[2] In the supervised setup, we are interested in the following risk quantity, the supervised loss, for
a multi-class classifier g : X → R[C]:


exp(gy(x))
_−_ ln _c_ [exp(][g][c][(][x][))]

_∈Y_

P


(1)


_Rsupv(g) :=_ E
**x,y∼P**


which is specialized for the softmax cross-entropy loss. The expectation is taken over the unknown
underlying joint distribution P. The classifier is eventually used for prediction by argmaxy _gy(_ ).
_∈Y_ _·_

**Contrastive unsupervised representation learning (CURL).** In the CURL framework (Arora
et al., 2019), we target to learn meaningful data representation in an unsupervised manner. By doing so,
a similarity model is trained to make the representation of positive pairs more similar than randomly
drawn K negative samples using a contrastive loss. The class-conditional distribution is denoted by
_c := P(X_ _Y = c) for each c_ and the class-prior distribution by π := [P(Y = c)]c .
The data generating process is described as follows: (i) draw latent positive/negative classes:D _|_ _∈Y_ _∈Y ∈△ c[+][C],_

2Strictly speaking, latent classes should be distinguished from the supervised labels as they may contain
concepts that human annotators are not interested in. In the CURL framework (Arora et al., 2019), latent classes

[C] and the supervised label set Y are allowed to be different, i.e., Y ⊆ [C]. To focus on the relationship
between the supervised loss and contrastive loss, we analyze under the assumption Y = [C] for simplicity.


-----

_c[−]k_ _k=1_

(iv) draw{ _[}][K]_ _K[∼] negative samples[π][K][+1][ (ii) draw an anchor sample] x[−]k_ _[∼D]c[−]k_ [(for each][ x][ ∼D][ k][ ∈][c][[][+][K][ (iii) draw a positive sample][]][).] **[ x][+][ ∼D][c][+]** [3]

Now we specify our model. A multi-class classifier g : X → R[C] consists of representation
**f : X →** R[h] and linear parameters W ∈ R[C][×][h] as g(·) := Wf (·), where h ∈ N denotes the
dimensionality of the representation given in advance. In CURL, the representation is learned through
minimization of the following contrastive loss

exp(f (x)[⊤]f (x[+]))

_Rcont(f_ ) := _c[+]∼,{πcE[−]k[K][}][+1]k[K]=1_ **xx,x[−]k[+][∼D]E∼Dc[−]kc[2][+]** "− ln exp(f (x)[⊤]f (x[+])) + _k∈[K]_ [exp(][f] [(][x][)][⊤][f] [(][x]k[−][))] # _._ (2)

[P]

In the downstream task, the representation f shall be frozen, and the parameters W are to be learned
via minimizing the supervised loss Rsupv.

**Evaluation of representations.** For the sake of evaluation, a specific linear classifier called mean
_classifier is introduced. Given representation f_, the mean classifier W[µ] is defined as W[µ] :=

[µ1 _, µC][⊤], where µc := Ex_ _c_ [f (x)]. This will later be used for evaluating the representation f
combined with the supervised loss, which is denoted by · · · _∼D_ _Rµ-supv(f_ ) := Rsupv(W[µ]f ). We call it the
_mean supervised loss. If the mean supervised loss is successfully bounded from above, we end up_
a bound on the supervised loss through inf **W∈RC×h Rsupv(Wf** ) ≤ _Rµ-supv(f_ ). For this reason, an
upper bound on Rµ-supv is an intermediate milestone that we seek throughout this paper.

3 LEARNING BOUNDS FOR CONTRASTIVE LEARNING

In this section, our main theoretical results are provided. We aim at showing that the contrastive loss
_Rcont(f_ )—the computable functional in CURL—serves as a good estimator of the mean supervised
loss Rµ-supv(f )—the inaccessible target in CURL—for any f . We show this by establishing upper
and lower bounds of Rµ-supv(f ) by Rcont(f ). Eventually, the minimization of Rcont(f ) may lead to
a good minimizer of Rµ-supv(f ). All proofs are provided in Appendix B.

3.1 MAIN RESULTS

First, we show a sharp upper bound of the mean supervised loss. Unlike any of the existing learning
bounds of CURL, the upper bound obtained here has a constant coefficient in the contrastive loss and
is applicable for all C and K (see discussions in Section 4).[4]

**Theorem 1. For all f such that ∥f** (x)∥2 ≤ _L (∀x ∈X_ _), the following inequality holds._

_Rµ-supv(f_ ) _Rcont(f_ ) + ∆U, (3)
_≤_

_where ∆U := 2 ln((CKπ(_ 1) + 1) cosh(L[2])) 2 ln(1 + K) ln Kπ( 1).
_−_ _−_ _−_ _−_

Next, the first lower bound of the mean supervised loss is provided. While the existing theoretical
analyses only provided upper bounds, they often entail a huge coefficient in the contrastive loss. The
lower bound provided below has the same constant coefficient and intercept (∆U and ∆L) rate as our
upper bound, ensuring the tightness of our analysis.

**Theorem 2. For all f such that ∥f** (x)∥2 ≤ _L (∀x ∈X_ _), the following inequality holds._

_Rµ-supv(f_ ) _Rcont(f_ ) + ∆L, (4)
_≥_


_where ∆L := H (π) + ln_


_K_

(K+1)[2][ −] [2 ln cosh(][L][2][)][.]


3In self-supervised learning, data augmentation (DA) is commonly used to obtain a positive sample. While

Nozawa & Sato (2021) handled DA in this formulation, we omit it for simplicity to focus on the effect of K.

4Even if Y = [C] is assumed in Section 2, Theorem 1 can be extended to the case Y ⊆ [C] (subset) and Y is
a coarse-grained set of [C]. In contrast, Theorem 2 can only be extended to the coarse-grained Y. The details
are discussed in their proofs.


-----

Our proofs leverage that the contrastive loss and mean supervised loss share the similar log-sum-exp
functional form, which leads to the direct relationships between the two losses. This is in contrast to
the existing works including Arora et al. (2019), which approximate the mean supervised loss with
the contrastive loss by taking the expectation over latent classes, leading to an exponentially large
coefficient.

In Theorems 1 and 2, the size of the representation ∥f (x)∥2 is assumed to be bounded. This assumption is reasonable from the experimental perspective since it is common to normalize representation
to employ the cosine similarity as the similarity metric. Several works reported that the normalized
embeddings improve the performance (Chen et al., 2020; Wang & Isola, 2020). Unlike the existing
analyses (reviewed in Section 4), we take advantage of this assumption to derive the sharp bounds.

As we see in Section 3.2, ∆U and ∆L are the same order in K under the uniform class prior
assumption. By applying either the high-probability bound (Arora et al., 2019) or PAC-Bayesian
analysis (Nozawa et al., 2020), Theorem 1 (Theorem 2 as well) can be naturally extended to the form
_Rµ-supv([b]f_ ) ≤ _Rcont(f_ ) + ∆U + χ with a complexity term χ, where **f is the empirical minimizer of**
the contrastive loss. Since this is a routine, we omit the high-probability bounds.

[b]

3.2 DISCUSSION


Subsequently, we discuss implications of our
main results on the relationship between the
mean supervised loss and the negative sample
size K. For the sake of simplicity, we assume
_πc =_ [1]/C for all c ∈ [C] (the uniform class
prior) in this section.

**Gap between learning upper and lower**
**bounds.** Both of our upper (Theorem 1) and
lower (Theorem 2) bounds draw the linear relationship between the mean supervised loss
_Rµ-supv and the contrastive loss Rcont, with the_
additional intercept terms ∆U and ∆L. Under
the uniform class prior assumption, the intercepts are in the same order:



_Rµ-supv(f_ )

_Rcont[∗]_ [+ ∆][U]

_Rµ[∗]-supv_


_Rµ[∗]-supv_

_[−]_ [∆][L]


_Rcont(f_ )


|Rµ-supv(f) ≤Rcont(f) + ∆U|Col2|Col3|
|---|---|---|
|) Rµ-supv(f) ≥Rcont(f|||
||||
||||
||||


_Rcont[∗]_


Figure 2: The learning bounds and feasible region. The point ⋆, (Rcont[∗] _[, R]µ[∗]-supv[)][, is the opti-]_
mal point in the feasible region. The points and

■ are mentioned in the texts. _•_


_CK_

∆U = ln _[C]_

_K_ [+ 2 ln cosh(][L][2][) =][ O][(ln 1]K [)][,][ ∆][L][ = ln] (K + 1)[2][ −] [2 ln cosh(][L][2][) =][ O][(ln 1]K [)][,][ (5)]


and the gap between two bounds is ∆U ∆L = 4 ln cosh(L[2]) + 2 ln 1 + _K[1]_, meaning that the gap
_−_

shrinks to 4 ln cosh(L[2]) as K increases. Hence, our bounds have the tight intercepts, and the larger

  

_K is beneficial for CURL from the viewpoint of the estimation gap of the mean supervised loss._

**Learning bounds and feasible region.** Next, we consider the (Rcont, Rµ-supv)-plot, in which a
point indicates (Rcont(f ), Rµ-supv(f )) for some f (see Figure 2). Here, let us focus on the feasible
region in the (Rcont, Rµ-supv)-plot by assuming **f** 2 _L for any f (same as Theorems 1 and 2)._
_∥_ _∥_ _≤_
Then, the mean supervised loss and contrastive loss are essentially lower-bounded by the constants[5]

_Rµ[∗]-supv_ [:= ln] 1 + (C − 1) exp(−2L[2]) _,_ (6)

_K_   _K_ 1 _m_ _K_ _−m_

_Rcont[∗]_ [:=] 1 ln 1 + m + (K _m) exp(_ 2L[2]) _,_ (7)

_m_ _C_ _−_ _C[1]_ _{_ _−_ _−_ _}_

_mX=0_      

respectively. Hence, the feasible region is

_Rµ-supv(f_ ) _Rcont(f_ ) + ∆U, _Rµ-supv(f_ ) _Rcont(f_ ) + ∆L, (8a)
_≤_ _≥_
_Rµ-supv(f_ ) ≥ _Rµ[∗]-supv[,]_ _Rcont(f_ ) ≥ _Rcont[∗]_ _[,]_ (8b)

as illustrated in Figure 2. The first two bounds (8a) restrict the mean supervised loss by the contrastive
loss. We specifically refer to these bounds as learning bounds. The remaining two bounds (8b)

5The derivations of Rµ∗ -supv [and][ R]cont[∗] [are detailed in][ Appendix C][.]


-----

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
||||||Rc∗ont + ∆ ∗|U (C = 50)|||
||||||||||
||||||Rµ-supv ( Rc∗ont + ∆ Rµ∗-supv (|C = 50) U (C = 100) C = 100)|||
||||||||||
||||||||||
||||||||||
||||||||||
||||||||||

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|||||||||
|||||Rµ∗-supv −∆ Rc∗ont (C =|L (C = 50) 50)|||
|||||||||
|||||||||
|||||Rµ∗-supv −∆ Rc∗ont (C =|L (C = 100) 100)|||
|||||||||
|||||||||


_Rcont[∗]_ [+ ∆][U] [(][C][ = 50][)]

_Rµ[∗]-supv_ [(][C][ = 50][)]

_Rcont[∗]_ [+ ∆][U] [(][C][ = 100][)]

_Rµ[∗]-supv_ [(][C][ = 100][)]


20 40 60 80 100

_K_

(a) The feasible region at Rcont(f ) = Rcont[∗] [.]


0 20 40 60 80 100

_K_

(b) The feasible region at Rµ-supv(f ) = Rµ[∗] -supv[.]


Figure 3: Visualization of the smallest possible value of Rµ-supv in the feasible region (8) for different
_K and C. The dotted lines show the essential lower bounds which come from each loss separately._
The solid lines show the learning upper bounds in which Rµ-supv and Rcont restrict each other.

Table 1: Learning bounds of the existing works. Remark that Arora et al.’s and Nozawa & Sato’s
bounds are valid only K + 1 ≥ _C. The detailed derivations are discussed in Appendix D._

UPPER BOUND REFERENCE


(1−τK1)vK+1 Arora et al. (2019)

_vK1+1_ _[{][R][cont][(][f]_ [)][ −] [E][ ln(Col + 1)][}] Nozawa & Sato (2021)

_[{][2][R][cont][(][f]_ [)][ −] [E][ ln(Col + 1)][}]

1 2τK 2(C−1)KHC−1 _Rcont(f_ ) E ln(Col + 1) Ash et al. (2021)
_−_ _{_ _−_ _}_

l m


_Rµ-supv(f_ )
_≤_


represent the achievable limits for each loss separately. One of the important questions is how the
smallest possible value of Rµ-supv in the feasible region (8) changes as K and C change. In other
words, we are interested in whether the optimal point (Rcont[∗] _[, R]µ[∗]-supv[)][ (][⋆]_ [in][ Figure 2][) is always]
achievable regardless of the values of K and C. To investigate it, we check the following two points.

-  The feasible region at Rcont(f ) = Rcont[∗] [(][Figure 3a][):][ We plot the value][ R]cont[∗] [+ ∆][U] [(solid line;]
the Rµ-supv-value of the point in Figure 2) and the minimum possible Rµ-supv (Rµ[∗]-supv[; dotted]
line) numerically. These two curves do not cross for all • _K, which means Rµ-supv(f_ ) = Rµ[∗]-supv [is]
attainable no matter the values K and C. In addition, the bound becomes sharper as K increases,
but the gap between the upper bound and Rµ[∗]-supv [does remain even at the limit][ K][ ↗∞][.]

-  The feasible region at Rµ-supv(f ) = Rµ[∗]-supv [(][Figure 3b][):][ When][ R][µ][-supv][(][f] [) =][ R]µ[∗]-supv[, the con-]
trastive lossFigure 2). The curve of this value does not cross Rcont(f ) is upper-bounded by Rµ[∗]-supv Rcont[∗] _[−][, which tells us that the lower bound does][∆][L]_ [(the][ R][cont][-value of the point][ ■] [in]

not exclude the optimal point (Rcont[∗] _[, R]µ[∗]-supv[)][ from the feasible region][ (][8][)][ at any][ K][. Note that]_
the gap betweeneasier to attain R Rµ[∗]-supvcont[∗] [and][as][ K][ R][ increases.]µ[∗]-supv _[−]_ [∆][L] [gradually increases, meaning that it becomes much]

Hence, the optimal point ⋆ stays in the feasible region (8) no matter the value K. From this
viewpoint, smaller K is not necessarily disadvantageous because the optimal point ⋆ _remains in the_
_feasible region. Note again that the estimation of Rµ-supv may become harder with the smaller K_
because of the bound gap ∆U − ∆L = O(ln [1]/K), even if the optimal solution is unaffected by K.

**Summary.** We draw a connection between the mean supervised loss and the negative sample size
_K by the following claim: if we regard CURL as an estimation process of the mean supervised loss_
by the contrastive loss, the small K does not necessarily have the estimation bias, but the larger K
helps reduce the estimation gap.


-----

4 COMPARISON WITH EXISTING LEARNING BOUNDS

This section discusses the difference between our main results and the existing theoretical results on
CURL. The learning bound analysis of CURL has been first established by Arora et al. (2019) and
later extended by Nozawa & Sato (2021) and Ash et al. (2021). Here, we briefly review these results.
For ease of the comparison, we assume the uniform class prior, namely, πc = [1]/C for all c ∈ [C]. We
introduce a notation Col := _k∈[K]_ [1]{c[+]=c[−]k _[}][. Let][ v][K][ be the probability that sampled][ K][ negative]_
classes contains all classes c ∈ [C].

[P]K _C−1_ _C_ 1 _n−1_

_vK :=_ _−_ ( 1)[m] 1 _._ (9)

_m_ _−_ _−_ _[m][ + 1]C_

_n=1_ _m=0_    

X X

The value vK is often referred to as the coupon collector’s probability. Let τK be the probability that
at least one of the negative classes c[−]k [is the same as the positive class][ c][+][. Under the uniform class]
prior, τK = 1 − (1 − [1]/C)[K]. The learning bounds are summarized in Table 1.[6]


**Comparison.** First, the coefficient of Rcont(f )
in the upper bounds are numerically compared in
Figure 4a. While Theorem 1 has the coefficient
1 for all K, the existing bounds have different
nature—Arora et al.’s and Ash et al. (2021)’s
coefficients have unique minima, and Nozawa &
Sato’s coefficient has monotonically decreasing
nature. Since the larger K was shown advantageous in the several experimental work (Chen
et al., 2020; He et al., 2020), the behaviors of
Arora et al.’s and Ash et al.’s bounds do not
explain it well. In Section 6, we also experimentally confirm the benefits of the large K.

Figure 4b compares the upper bound values
at Rcont(f ) = Rcont[∗] [, namely, the best possi-]
ble mean supervised loss in terms of the upper
bounds. The tendencies are slightly different
from Figure 4a—Ash et al.’s bound is monotonically increasing, Arora et al.’s and Nozawa
& Sato’s bounds have a unique minimum, and
ours is monotonically decreasing. Among the
compared bounds, only ours agrees well with
the experimental fact that the larger K is better.[7]


10[2]

10[1]

10[0]


(a) Coefficient of Rcont(f ).

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||||||||
|||||Ours|||
|||||Arora Noza|et al. wa & Sato||
|||||Ash e|t al.||
||||||||
||||||||
|0||20 40 60 80 100 K|||||



(b) Upper bound at Rcont(f ) = Rcont[∗] [.]


10[2]

10[1]

10[0]

|Col1|Col2|Col3|Col4|cont|Col6|Col7|
|---|---|---|---|---|---|---|
||||||||
||||||||
||||||||
||||||||
|0||20 40 60 80 100 K|||||


ours is monotonically decreasing. Among the Figure 4: Theoretical comparison of our upper
compared bounds, only ours agrees well with bound and the existing bounds (C = 10). Arora
the experimental fact that the larger K is better.[7] et al.’s and Nozawa & Sato’s bounds are valid only

Note that the coefficients of the existing theories at K + 1 ≥ _C (the dotted vertical lines)._
are so large—the vertical axes are log-scaled—that the upper bounds are vacuous in both situations.
In contrast, our theory can bound the downstream classification loss most sharply in all ranges of K.
This sharpness can be empirically observed as well in Figure 1; the details are stated in Section 6.2.


5 RELATED WORK

While our work and the existing works reviewed in Section 4 attempt to understand CURL by
connecting the contrastive loss to the mean supervised loss, there are other approaches; Wang &
Isola (2020) showed that the contrastive loss asymptotically favors data representation uniformly
distributed over the unit sphere yet aligning across semantically similar samples. Li et al. (2021)
proposed an alternative loss function to the contrastive loss based on a kernel metric, following the

6More precisely, Arora et al. (2019) bound the averaged supervised loss over a part of the latent classes rather
than Rµ-supv. Thus we can obtain a slightly better upper bound than Arora et al.’s bound shown in Table 1.
Nevertheless, the scale of the upper bound is dominated by the coefficient (1 _τK_ )vK+1.

7 _−_
Note that Nozawa & Sato (2021)’s bound also implies larger K is better. Still, its mechanism and resulting
explainability are different from ours. See Appendix D for the further discussions.


-----

similar idea to Wang & Isola (2020). Tosh et al. (2021) showed that a (linear) mean classifier learned
in CURL can approximate the (potentially nonlinear) Bayes classifier well.

While our work does not handle data augmentation (DA), several works analyzed how DA affects the
performance of downstream classification. Wen & Li (2021) showed that DA is necessary to recover
sparse signals under a specific assumption on the model architecture. HaoChen et al. (2021) introduces
a notion of the augmentation graph, representing how likely the nearby samples are generated via
DA and showed that a type of contrastive loss could be viewed as a low-rank approximation of the
adjacency matrix of the augmentation graph. von Kügelgen et al. (2021) proposed a loss function
that enables the model to identify invariant factors across DA.

We mention a few works analyzing the other types of self-supervised learning; Garg & Liang (2020)
analyzed masked self-supervised learning, Wei et al. (2021) analyzed the input consistency loss for
unsupervised learning, and Saunshi et al. (2021) analyzed auto-regressive language models.

As a final remark, multi-sample estimators (Oord et al., 2018; Poole et al., 2019; Song & Ermon,
2020) popularly used in mutual information (MI) estimation are substantially related to the contrastive
loss. Although the multi-sample estimators have high bias and low variance compared to variational
estimators in general (Poole et al., 2019; Song & Ermon, 2020; Guo et al., 2021), the quantitative
analysis of the bias-variance trade-off of the multi-sample MI estimators has yet to be clearly known.[8]
Note that Tian et al. (2020) and Tschannen et al. (2020) experimentally showed that maximizing
_tighter MI bound does not necessarily lead to good representation; there is no guarantee that the_
model can achieve higher MI by the tighter bound.

6 EXPERIMENTS

We verified our theoretical findings with experiments on synthetic datasets (Section 6.1), vision, and
language datasets (Section 6.2). The details of the experimental setup are provided in Appendix F.

6.1 SMALL-SCALE EXPERIMENTS ON SYNTHETIC DATASET

**Dataset and learning setups.** We create a synthetic dataset circle, which is a 2D dataset created
as follows: for each class c ∈ [C], 1 000 samples are drawn from Uniform([−0.5 − 0.5], [0.5, 0.5]),
normalized, and multiplied by _[c][+1]/2. The generated samples are nonlinear and require disentangle-_
ment to be linearly separable. We treated 60% of the generated samples as a training dataset and the
rest of the samples as a test dataset. We did not use data augmentation.

As a feature extractor f, we used a multi-layer perceptron (the number of units 2-256-256-256) with
the ReLU activation functions following after each hidden layer. During the training, the extracted
feature representations are normalized. For negative samples, we sampled K ∈{1, 4, 16, 64, 256}
samples without replacement from 2B − 2 points included in the same mini-batch to avoid the
influence of mini-batch size B, inspired by Ash et al. (2021).[9]

**Results.** Figure 5 shows a single trajectory in the (Rcont, Rµ-supv)-plot and the feasible region
(confer Figure 2) for each K. We plotted the trajectories by tracking (Rcont(f [(][t][)]), Rµ-supv(f [(][t][)]))
at each epoch t computed with the test dataset. All trajectories were located in between the upper
(Rcont + ∆U) and lower (Rcont + ∆L) bounds as a matter of course. Given that the existing learning
bounds provide the much larger upper bounds (Figure 4), our learning bounds provide the finest
estimate of the mean supervised loss. In addition, it is remarkable that all trajectories have nearly
the same slopes as our learning bounds, which constitutes solid evidence that our learning bounds
capture the learning dynamics well.

In Figure 6, the mean supervised loss and accuracy are compared with the different K. For each K,
we conducted the same experiments with eight different random seeds, and the standard deviations
are shown in the figures. From these figures, it can be concluded that the contrastive loss performance

8There are some quantitative analyses such as the bias of general MI estimators (Gao et al., 2015; McAllester
& Stratos, 2020) and the variance of the NWJ/MINE estimators (Song & Ermon, 2020). We will further discuss
the implication of the bias analysis in Appendix E.

9Each mini-batch consists of B pairs of positive pairs. The candidates of the negative samples are the 2B − 2
samples excluding the anchor and its paired point.


-----

|Col1|K = 1|Col3|
|---|---|---|
||||
||||
||||
||Rcont|+ ∆U|
||||
||Rcont|+ ∆L|


_K = 1_ _K = 4_ _K = 16_ _K = 64_ _K = 256_

6 300

5

100

4

_Rcont + ∆U_ 20

3

mean supervised loss 2 _Rcont + ∆L_ 0 Trajectory (epoch)

0 2 4 6 0 2 4 6 0 2 4 6 0 2 4 6 0 2 4 6

contrastive loss contrastive loss contrastive loss contrastive loss contrastive loss


Figure 5: Learning trajectories of the circle dataset in the (Rcont, Rµ-supv)-plot. The trajectories
are plotted with gradient color lines, indicating the epochs.

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
||||||K = 1 K = 4|
||||||K = 16 K = 64 K = 256|
|||||||


4.4 _K = 1_ 100

_K = 4_ 3.80

4.2 _K = 16_ 75

4.0 _K = 64_ 50 3.75

_K = 256_

mean supervised loss 3.8 25 3.70

3.6 mean supervised accuracy 0 best mean supervised loss

0 50 100 150 200 250 300 0 50 100 150 200 250 300 1 4 16 64 256

epoch epoch _K_


Figure 6: For each K, eight runs on the circle dataset are averaged with the standard deviations
plotted. (Left) the test mean supervised losses at each epoch with the different negative sample sizes
_K. (Middle) the test mean supervised accuracy at each epoch with the different negative sample_
sizes K. (Right) the best test mean supervised loss with the different negative sample sizes K.

becomes better with the larger K in the sense that the supervised loss improved and the variance
shrank. The variance improvement can be theoretically observed from Figure 5 as well; the larger K
is, the smaller the gap between upper and lower bounds becomes.

6.2 LARGE-SCALE EXPERIMENTS ON VISION AND LANGUAGE DATASETS

We used the same datasets as Arora et al. (2019): CIFAR-100 (Krizhevsky, 2009) and Wiki3029 (Arora et al., 2019) datasets. In addition, we used CIFAR-10 (Krizhevsky, 2009) dataset.

**Learning setups.** To create positive pairs for contrastive learning, we treated the supervised
classes as latent classes as in Arora et al. (2019) and Ash et al. (2021). We treated the original
supervised classes of CIFAR-10/100 as [C]; C = 10 and C = 100, respectively. We used K in
_{4, 16, 32, 64, 128, 512} and in {4, 64, 128, 512} for CIFAR-10/100, respectively. For Wiki-3029,_
we used C ∈{500, 1 000, 2 000, 3 029} and K ∈{8, 64, 256, 1 024}. For each different (C, K)
pair, we trained the feature extractor f on the training dataset. We then evaluated its supervised
performance on the test dataset with mean and linear classifiers. We used ResNet-18 (He et al.,
2016)-based feature extractor f for CIFAR-10/100 and the fasttext (Joulin et al., 2017)-based feature
extractor for Wiki-3029.

**Results.** Figure 1 shows the comparison between the estimated upper bounds using Theorem 1 and
actual supervised loss on CIFAR-10 for different K on the test dataset. We estimated the bounds
by substituting the actual Rcont(f ) to the equations shown in Theorem 1 and Table 1. Our learning
bound gave the closest upper bound to the experimental value of the supervised loss. The existing
learning bounds of Arora et al. (2019) and Ash et al. (2021) were not sharp enough to explain the
classification performance. Although Nozawa & Sato’s bound was comparable with our bound, it
was valid only in K + 1 ≥ _C and tended to increase near K + 1 = C, as we explained in Section 4._

We investigated how K affects the test accuracy for different C in Figure 7. The test accuracy
improved or was saturated with the larger K for all C on Wiki-3029. In contrast, it was degraded as
_K increased in mean and linear classifiers on CIFAR-10/100. This behavior could be partly because of_
the gap between the cross-entropy loss and the supervised accuracy—the theory of CURL, including
the existing studies, usually focuses on the cross-entropy loss only. Figure 9 in Appendix F.6 revealed
that the supervised loss was not significantly worse with the larger K on CIFAR-10/100.

With the smaller K and large C, we found that long epochs were more effective to improve classification accuracy than increasing the negative sample size K (Figure 8b). While similar experimental
results were reported by Chen et al. (2020, Figure 9), it is important to remark that we randomly


-----

linear classifier

41.70 43.00 43.26 43.30
(0.14) (0.15) (0.14) (0.21)

45.13 46.33 46.46 46.37
(0.29) (0.14) (0.15) (0.16)

50.86 51.21 51.06 50.94
(0.25) (0.33) (0.24) (0.23)

55.41 55.41 55.52 55.69
(0.39) (0.43) (0.51) (0.47)


64 256 1024

|Col1|Col2|Col3|Col4|m lin|ean classifier ear classifier|
|---|---|---|---|---|---|
|||||||
|||||||


(a) CIFAR-10 (Top)/100 (Bottom).


(b) Wiki-3029.


mean classifier

5. mean classifier 37.86 40.36 40.85 40.94
0 92. linear classifier 3029 (0.19) (0.13) (0.09) (0.13)
5 92.
0 91.91 2000 (042..17)09 (044..16)06 (044..09)38 (044..23)24

4 16 32 64 128 512 _C_

48.07 48.77 48.75 48.66

supervised accuracy 1000 (0.12) (0.23) (0.05) (0.10)

52.66 52.66 53.04 53.72

500 (0.71) (0.69) (0.67) (0.63)

64 65 66 67 68 69

4 64 128 512 8 64 256 1024

_K_ _K_


Figure 7: Mean and linear classifier’s test accuracy on CIFAR-10/100 and Wiki-3029 when varying
the negative samples size K. For Wiki-3029, we also change the number of latent classes C. The
error bars in (a) and parenthesized number in (b) indicate the standard deviation of three runs.


70

65

60

55

50

45

40

35


90

85


80

75


70

200 400 600 800 1000 1200 1400 1600 1800 2000 200 400 600 800 1000 1200 1400 1600 1800 2000

epoch epoch

(a) CIFAR-10. (b) CIFAR-100.

Figure 8: Test accuracy of mean classifier at every 200 epochs on CIFAR-10/100. In CIFAR-100,
the accuracy of K = 4 and the others’ have a large gap at smaller epochs at epoch 200, but the gap
become smaller when epochs increase. The error bars indicate the standard deviation of three runs.


drew K negative samples from the 2B − 2 samples in the given mini-batch at each iteration as in

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||||||||
||||||||
||||||||
|||||||K|
||||||||
||||||||
||||||||
|00 40 epoc at sm dicat|0 60 hs o aller e the|0 80 (b) n CI epo stan|0 10 CIF FAR chs dard|00 1200 epoch AR-100 -10/10 at epoc devia|1400 16 . 0. In h 200, tion of|00 1800 CIFAR but th three r|


_K_

4
64
128
512

200 400 600 800 1000 1200 1400 1600 1800 2000

epoch

(b) CIFAR-100.

epochs on CIFAR-10/100. In CIFAR-100,
200, but the gap

Ash et al. (2021)—a different approach was used by Chen et al. (2020) to regard the all samples
in the mini-batch except an anchor sample as negative samples. Under our experimental setup, a
learner may encounter less diverse samples with the smaller K even if the mini-batch size B is the
same, which could make the performance of downstream classification worse—the longer epochs are
necessary to mitigate the issue. Since the CIFAR-10 dataset has a smaller C and is simpler than the


In this study, we established novel theoretical bounds for contrastive unsupervised representation
learning. We derived sharp upper and lower bounds on a downstream classification loss that are
tight in the negative sample size. They suggest that the contrastive loss can be used as a downstream
loss estimator and its estimation gap decays with larger negative samples. In the experiments, we
verified that our bounds well explained learning dynamics on the synthetic dataset, and the estimation
gap was large with smaller negative samples. In addition, the empirical downstream classification
loss was best explained by our learning bound compared to the existing ones. It is left open to
fill the gap between the supervised loss and accuracy. In addition, we may understand contrastive
unsupervised representation learning better by incorporating the optimization perspective into the

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|||||||||
|||||||||
||||||||K|
||||||||4 16 32|
||||||||64 12 51|
|200 4 8: T urac e sm neg al. ( mini- may which ary to -100 ONC stud g. W the timat d that s lar as be gap rvise ical a|00 6 est a y of aller ativ 2021 batc enc cou mit , all LU y, w e d nega or a our ge w st e betw d re naly|00 80 (a) ccur K = whe e sa )—a h ex ount ld m igate accu SIO e est erive tive s nd it boun ith s xplai een pres sis.|0 10 CIF acy 4 a n ep mple diff cept er le ake the raci N ablis d sh amp s est ds w mall ned the entat|00 1200 epoch AR-10. of mea nd the ochs in s from erent an an ss dive the per issue. es were hed no arp up le size imatio ell exp er neg by our supervi ion lea|1400 16 n class others’ crease the 2 approa chor sa rse sa forman Since satura vel th per an . They n gap lained ative s learn sed lo rning|00 18 ifier hav . Th B − ch w mpl mple ce o the C ted eoret d low sugg deca lear ampl ing b ss an bette|00 200 at e e a la e erro 2 sam as u e as s wit f dow IFA with ical er b est t ys w ning es. I oun d ac r by|


_K_

4
16
32
64
128
512

200 400 600 800 1000 1200 1400 1600 1800 2000

epoch

(a) CIFAR-10.

Figure 8: Test accuracy of mean classifier at every
the accuracy of K = 4

_K negative samples from the 2B −_ 2
(2021)—a different approach was used by

learner may encounter less diverse samples with the smaller

necessary to mitigate the issue. Since the CIFAR-10 dataset has a smaller
CIFAR-100, all accuracies were saturated with similar epochs for all

CONCLUSION

loss estimator and its estimation gap

theoretical analysis.


-----

ETHICS STATEMENT

Since this study is theoretical in nature, we do not have specific ethical concerns.

REPRODUCIBILITY STATEMENT

All experimental codes are available in the supplementary material. In addition, the details of
experimental datasets and hyper-parameters of learning algorithms are described in Appendix F.

All theorems’ proofs are described in Appendix B. Appendix A provides Lemmas used in Appendix B.
Appendices C and D provide how to derive equations used in Section 3.2 and Section 4, respectively.


-----

## «APPENDIX» SHARP LEARNING BOUNDS FOR CONTRASTIVE UNSUPERVISED REPRESENTATION LEARNING

As an additional notation, the d-dimensional ball of radius r associated with the Lp-norm is denoted
by B[d]p[(][r][) :=][ {][x][ ∈] [R][d][ | ∥][x][∥][p]

_[≤]_ _[r][}][. For][ z][ ∈]_ [R][C][, the log-sum-exp function is denoted by]
LSE (z) := ln([P]c [C] [exp(][z][c][))][.]

_∈_

A USEFUL LEMMAS


In this section, a few lemmas are introduced in order to prove the main results.
**Lemma 3. For z ∈** R[N] _,_


ln 1 + exp( _zi)_ ln 1 +

 _−_  _≥−_ 

_nX∈[N_ ]

  

_where ∆0 := 2 ln(1 + N_ ).


exp(zk) + ∆0, (10)



_nX∈[N_ ]




_Proof. Define H(z) := LSE (−[0 z1 . . . zN_ ]) + LSE ([0 z1 . . . zN ]). Then, ∆0 is a lower bound
of H. Since

_∂H_ exp( _zi)_ exp(zi)

= _−_ (11)
_∂zi_ _−_ _n_ [N ] [exp(][−][z][n][) +] _n_ [N ] [exp(][z][n][)]
_∈_ _∈_

for all i ∈ [N ], z = 0N satisfies the first-order optimality condition ofP P _H. By noting that H is convex_
due to the convexity of the log-sum-exp functions, H is minimized at z = 0N . Hence, we can choose
∆0 = H(0N ) = 2 ln(1 + N ).

**Lemma 4. For z0 ∈** [−L[2], L[2]], z ∈ [−L[2], L[2]][N] _, and ρ > 0,_

exp( _z0)_ exp(z0)
ln _−_

(1 + ρ) exp( _z0) + ρ_ _n_ [N ] [exp(][−][z][n][)][ ≥−] [ln] (1 + ρ) exp(z0) + ρ _n_ [N ] [exp(][z][n][)][ −] [∆][1][,]
_−_ _∈_ _∈_

(12)

[P] [P]

_where ∆1 := 2 ln_ _ρ_ _N + 1 +_ _ρ[1]_ cosh(L[2]) _._
   

_Proof. The goal is to find a tight upper bound ∆1 of_


exp(z0) exp( _z0)_
ln _−_ (13)
_−_ (1 + ρ) exp(z0) + ρ _n∈[N_ ] [exp(][z][n][)][ −] [ln] (1 + ρ) exp(−z0) + ρ _n∈[N_ ] [exp(][−][z][n][)]

= ln [P] exp(zn) [P] exp( _zn)_

   _−_ 

_n_ [N ] _n_ [N ]

 X∈   X∈ 

(14)

[(1 +][ ρ][) exp(][z][0][) +][ ρ]  [+ ln] [(1 +][ ρ][) exp(][−][z][0][) +][ ρ] 

= 2 ln ρ + ln 1 + _ρ[1]_ exp(z0) + exp(zn) + ln 1 + _ρ[1]_ exp( _z0) +_ exp( _zn)_

_n_ ) _−_ _n_ _−_ )

(  X (  X

:=H _[′](˜z)_

(15)

| {z }

where ˜z := [z0 z[⊤]][⊤].


-----

Equivalently, we aim at an upper bound of H _[′](˜z) for ˜z_ B[N] [+1](L[2]). Observe that H _[′](˜z) =_
_∈_ _∞_

_⊤[]_ _⊤[]_

LSE **z˜ +** ln 1 + _ρ[1]_ 0 . . . 0 + LSE **z˜ +** ln 1 + _ρ[1]_ 0 . . . 0 is the sum of the

_−_

two log-sum-exp functions after adding the constant vector hence it is convex in h   i  h   i ˜z. In addition,
the domain B[N] [+1](L[2]) is a compact convex polytope. Henceforth, every vertex of the polytope,
_∞_
**z˜ ∈{−L[2], L[2]}[N]** [+1], is a local maximizer because maximizing H _[′](˜z) is concave minimization over a_
convex polytope (Pardalos & Rosen, 1986). Since H _[′](˜z) is symmetric in every zn except z0, we need_
to divide the cases depending on either z0 = L[2] or z0 = −L[2] and compare the maximum values of
_H_ _[′](˜z)._

If z0 = L[2]: In order to maximize H _[′](˜z), it is sufficient to test the vertices ˜z_ **z** B[N] [+1](L[2])
_∈{_ _∈_ _∞_ _|_
_z0 = L[2]} and see the difference between_ _H[˜]_ _[′](m) and_ _H[˜]_ _[′](m + 1), where_

_H˜_ _[′](m) := H_ _[′][]L[2], L[2], . . ., L[2], −L[2], . . ., −L[2]_ (16)
_m_ _N_ _−m_ 

= ln _m + 1 + |_ {z [1] }exp(| _L{z[2]) + (N}_ _m) exp(L[2])_

_ρ_ _−_ _−_

  

+ ln _m + 1 + [1]_ exp(L[2]) + (N _m) exp(_ _L[2])_ (17)

_ρ_ _−_ _−_

  

for m ∈{0, . . ., N _} to seek the maximum. Here, the definition of_ _H[˜]_ _[′]_ can be naturally extended over
the real domain m ∈ R. A simple algebra shows

2

exp _H˜_ _[′](m)_ = 2 (exp(2L[2]) + exp( 2L[2])) _m_ _N −_ 1 − _ρ[1]_ + Const. (18)
_−_ _−_ _−_ 2 !
    _<0 (AM-GM inequality)_ 

1
Since _H[˜]_ _[′]_ is concave over| _m_ R, _H[˜]_ _[′]_ is maximized at{z _m}_ = _N_ _−21−_ _ρ_ _N1_ 1 [, then]
_∈_ + [(][< N] [)][. If][ ρ <] _−_

the maximum value is h i


= 2 ln _N + 1 + [1]_

_ρ_




_H˜_ _[′]_ _N −_ 1 − _ρ[1]_


cosh(L[2]) _._ (19)



_N1_ 1 [,][ ˜]H _[′]_ is maximized at m = 0 and its value is
_−_

2

_H˜_ _[′](0) = ln_ _N_ [2] + 1 + [1] + N 1 + [1]

_ρ_ _ρ_

  


If ρ ≥



[1] (exp(2L[2]) + exp( 2L[2]))

_ρ_  _−_ !

2

_N_ [2] + 1 + [1] + 2N 1 + [1]

_ρ_ _ρ_

   [!!]


(20)

(21)


2

exp(2L[2]) + exp( 2L[2])

ln _−_ _N_ [2] + 1 + [1] + 2N 1 + [1] (21)
_≤_ 2 _ρ_ _ρ_

   [!!]

= ln _N + 1 + [1]_ cosh(2L[2]) _,_ (22)

_ρ_

  


where the inequality follows from the AM-GM inequality [exp(2][L][2][)+exp(]2 _[−][2][L][2][)]_

If z0 = −L[2]: Similarly, we define


_≥_ 1.


_H˜_ _[′](m) := H_ _[′]_ _L[2], L[2], . . ., L[2],_ _L[2], . . .,_ _L[2]_ (23)

− _−_ _−_ 

_m_ _N_ _−m_
 

= ln _m exp(|_ _L[2]{z) +_ }N| + 1 + {z[1] } exp(L[2])
_−_ _ρ_
  _[−]_ _[m]_ 

+ ln _m exp(L[2]) +_ _N + 1 + [1]_ exp( _L[2])_ _,_ (24)

_ρ_ _−_

  _[−]_ _[m]_ 


-----

and extends its definition over the real line m ∈ [0, N ]. Since

exp _H˜_ _[′](m)_ = 2 (exp(2L[2]) + exp( 2L[2])) _m_ _N + 1 +_ _ρ[1]_ 2 + Const, (25)
_−_ _−_ _−_ 2 !
    _<0 (AM-GM inequality)_ 

_H˜_ _[′]_ is maximized at m = min| _N_ +1+2 1ρ{z, N (> 0). If ρ} _N1_ 1 [,][ ˜]H _[′]_ is maximized at m = _N_ +1+2 _ρ[1]_

_≥_ _−_

and its value is n o

_H˜_ _[′]_ _N + 1 +_ _ρ[1]_ = 2 ln _N + 1 + [1]_ cosh(L[2]) _._ (26)

2 _ρ_

!   


_N1_ 1 [,][ ˜]H _[′]_ is maximized at m = N and its value is
_−_

2

_H˜_ _[′](N_ ) = ln _N_ [2] + 1 + [1] + N 1 + [1] (exp(2L[2]) + exp( 2L[2])) (27)

 _ρ_   _ρ_  _−_ !

ln _N + 1 + [1]_ cosh(2L[2]) _._ (28)
_≤_ _ρ_
  


If ρ <


After all, _H[˜]_ _[′]_ is bounded from above by either

2 ln _N + 1 + [1]_ cosh(L[2]) or ln _N + 1 + [1]_

_ρ_ _ρ_

   

These two values can be compared as follows.


cosh(2L[2]) _._ (29)


cosh(2L[2]) (30)



2 ln _N + 1 + [1]_

_ρ_



= ln _N + 1 + [1]_

_ρ_



= ln _N + 1 + [1]_

_ρ_



ln _N + 1 + [1]_
_≥_ _ρ_



cosh(L[2]) ln _N + 1 + [1]_
_−_ _ρ_
 


+ ln cosh[2](L[2]) − ln cosh(2L[2]) (31)


1 +
+ ln


cosh(2L[2])


(32)

(33)


_> 0,_ (34)

where the first inequality is due to the AM-GM inequality [exp(2][L][2][)+exp(]2 _[−][2][L][2][)]_ 1. Hence, regardless

_≥_

of ρ, 2 ln _N + 1 +_ _ρ[1]_ cosh(L[2]) is a tight upper bound of _H[˜]_ _[′]._
  

**Lemma 5. For all z0 ∈** R and z ∈ R[K] _such that z0, zk ∈_ [−L[2], L[2]] (∀k ∈ [K]),

exp( _z0)_
ln _−_

exp(−z0) + _k∈[K]_ [exp(][−][z][k][)]

exp(z0)
ln (K + 1) cosh(L[2]) _._ (35)
_≥−_ exp(z[P]0) + _k_ [K] [exp(][z][k][)][ −] [2 ln]

_∈_


_Proof. We write ˜z := [z0 z[⊤]][⊤]. Let H[P](˜z) be a function such that_


exp(z0) exp( _z0)_
_H(˜z) :=_ ln _−_ (36)
_−_ exp(z0) + _k∈[K]_ [exp(][z][k][)][ −] [ln] exp(−z0) + _k∈[K]_ [exp(][−][z][k][)]

_K+1_ _K+1_

= ln exp(˜zk) + ln[P] exp( _z˜k)._ [P] (37)

_−_

_k=1_ _k=1_

X X


-----

Our goal is to find a tight upper bound of H(˜z) for ˜z B[K][+1](L[2]).
_∈_ _∞_

Observe that H(˜z) is the sum of the two log-sum-exp functions hence it is convex in ˜z. In addition,
the domain B[K][+1](L[2]) is a compact convex polytope. Henceforth, every vertex of the polytope,
_∞_
**z˜ ∈{−L[2], L[2]}[K][+1], is a local maximizer because maximizing H(˜z) is concave minimization over**
a convex polytope (Pardalos & Rosen, 1986). Since H(˜z) is symmetric in every element ˜zk, it is
sufficient to test the vertices and see the difference between


_H_ (L[2], . . ., L[2], _L[2], . . .,_ _L[2])_ and H (L[2], . . ., L[2], _L[2], . . .,_ _L[2])_

 _−_ _−_   _−_ _−_ 

# = j # = j + 1
   
| {z := }H[˜] (j) | {z := H}[˜] (j+1)

| {z } | {z }

for j ∈{0, . . ., K} to seek out the global maximum. For 0 ≤ _j ≤_ _K, a simple algebra shows_


(38)

(39)


exp _H˜_ (j) _−_ exp _H˜_ (j + 1) = (K − 2j) 2 − exp(2L[2]) + exp(−2L[2]) _,_ (39)
      because of AM-GM inequality≤0 

| {z }

from which we can tell that _H[˜]_ (j) is maximized at j = K/2 when K is even and j = (K + 1)/2
when K is odd. In addition, it is confirmed that


_K_
exp _H˜_

2

 


_K + 1_ exp(2L[2]) + exp( 2L[2])
exp _H˜_ = [2][ −] _−_ (40)
_−_ 2 4
      

_≤_ 0, (41)


where the AM-GM inequality is invoked at the last line. Eventually, _H[˜] ((K + 1)/2) turns out to_
be a tight upper bound of H(˜z) for ˜z B[K][+1](L[2]). It is elementary to confirm _H[˜] ((K + 1)/2) =_
_∈_ _∞_
2 ln (K + 1) cosh(L[2]) .


B PROOFS OF MAIN RESULTS

In this section, we provide proofs for the main results, Theorems 1 and 2.

**Theorem 1. For all f such that ∥f** (x)∥2 ≤ _L (∀x ∈X_ _), the following inequality holds._

_Rµ-supv(f_ ) _Rcont(f_ ) + ∆U, (3)
_≤_

_where ∆U := 2 ln((CKπ(_ 1) + 1) cosh(L[2])) 2 ln(1 + K) ln Kπ( 1).
_−_ _−_ _−_ _−_

_Proof of Theorem 1. Define the two constants_

∆0 := 2 ln(1 + K), (42)

∆1 := 2 ln((CKπ( 1) + 1) cosh(L[2])). (43)
_−_

The proof begins with the Jensen’s inequality applied on the contrastive loss:


1 +






exp(f (x)[⊤](f (x[−]k [)][ −] **[f]** [(][x][+][)))]
_kX∈[K]_


(44)

(45)


_Rcont(f_ ) = E ln
_c[+],{c[−]k_ _[}][,][x][,][x][+][,][{][x]k[−][}]_


1 +






exp(f (x)[⊤](µc−k
_k_ [K] _[−]_ **_[µ][c][+]_** [))]

X∈


E ln
_c[+],{c[−]k_ _[}][,][x]_


1 +






exp(f (x)[⊤](µc+ − **_µc−k_** [))] + ∆0, (46)
_kX∈[K]_




_≥−_ _c[+],{Ec[−]k_ _[}][,][x]_ ln


-----

where the second inequality follows from Lemma 3. The first term, the origin-symmetric transform
of the contrastive loss, is further bounded as follows.


E ln 1 +
_c[+],{c[−]k_ _[}][,][x]_ 




exp(f (x)[⊤](µc+ − **_µc−k_** [))]
_kX∈[K]_


1 +






E
_c_ [exp(][f] [(][x][)][⊤][(][µ][c][+][ −] **_[µ][c][))]_**
_kX∈[K]_


(47)


_≥−_ _c[+]E,x_ [ln]

_≥−_ _c[+]E,x_ [ln]


E 1 + Kπ( 1) 1 + exp(f (x)[⊤](µc+ **_µc))_** _,_ (48)
_≥−_ _c[+],x_ [ln]  _−_  _−_ 

_c∈[CX]\{c[+]}_

  

where the first inequality is the Jensen’s inequality and the second inequality holds by noting that
Ec[A] = _c_ _[π][c][A][ ≥]_ [P]c _[π][(][−][1)][A][. In order to proceed further, the origin-symmetric transform is]_

needed to be applied on the lower-bounding term, resulting in a term aligning to the mean supervised
loss.

[P]


1 + Kπ(−1) 1 +

 


exp(f (x)[⊤](µc+ **_µc))_**
_−_
_c∈[CX]\{c[+]}_


_−_ _c[+]E,x_ [ln]


1 + Kπ(−1)




1 +






exp(f (x)[⊤](µc **_µc+_** ))
_−_
_c∈[CX]\{c[+]}_


∆1 (49)

 _−_


_≥_ _c[+]E,x_ [ln]

= E
_c[+],x_ [ln]

_≥_ _c[+]E,x_ [ln]


exp(f (x)[⊤](µc **_µc+_** )) ∆1 + ln Kπ( 1) (50)
_−_  _−_ _−_
_c∈[CX]\{c[+]}_




+ 1 +
_Kπ(−1)_


E 1 + exp(f (x)[⊤](µc **_µc+_** )) ∆1 + ln Kπ( 1) (51)
_≥_ _c[+],x_ [ln]  _−_  _−_ _−_

_c∈[CX]\{c[+]}_

= Rµ-supv(f ) ∆1 + ln Kπ( 1),  (52)
_−_ _−_

where the first inequality is a consequence of Lemma 4 and the second inequality is simply observed
by ln( _Kπ1(−1)_ [+][ x][)][ ≥] [ln][ x][ for any][ x >][ 0][ since] _Kπ1(−1)_

final bound Rµ-supv(f ) _Rcont(f_ ) ∆0 + ∆1 ln Kπ[≥]( [0]1)[. Combining all the above, we get the].
_≤_ _−_ _−_ _−_

Even if Theorem 1 assumes Y = [C], it can be extended to the cases Y ⊆ [C] (subset) and Y is a
coarse-grained set of [C]. When Y ̸= [C], Rµ-supv is defined over the class set Y, while Rcont is
defined over the class set [C]. In the subset case, we can replace Eq. (52) with the subset class bound:


1 +






exp(f (x)[⊤](µc **_µc+_** ))
_−_
_c∈[CX]\{c[+]}_


E
_c[+],x_ [ln]


E 1 + exp(f (x)[⊤](µc **_µc+_** )) = Rµ-supv(f ). (53)
_≥_ _c[+],x_ [ln]  _−_ 

_c∈Y\{Xc[+]}_

 

In the coarse-grained case, Eq. (52) holds without modification.
**Theorem 2. For all f such that ∥f** (x)∥2 ≤ _L (∀x ∈X_ _), the following inequality holds._

_Rµ-supv(f_ ) _Rcont(f_ ) + ∆L, (4)
_≥_


_where ∆L := H (π) + ln_ (KK+1)[2][ −] [2 ln cosh(][L][2][)][.]


_Proof of Theorem 2. The proof is essentially a consequence of the Fenchel’s inequality and the_
Jensen’s inequality. First, by noting that the convex conjugate of the log-sum-exp function is the


-----

negative Shannon entropy, the following identity is obtained.


_−f_ (x)[⊤]µy + LSE (W[µ]f (x)) (54)



_Rµ-supv(f_ ) = E
**x,y**

= E
**x,y**


_−f_ (x)[⊤]µy + supp∈△[C]


**p[⊤](W[µ]f** (x)) + H (p)


(55)


If we choose an arbitrary p, Rµ-supv(f ) is lower bounded (Fenchel’s inequality). Our choice
_∈△[C]_
is p = π. Recall that K is the number of negative samples. Then,

_Rµ-supv(f_ ) E **f** (x)[⊤]µc+ + _πc−_ **f** (x)[⊤]µc− + H (π) (56)
_≥_ _c[+],x_ − 

_cX[−]∈Y_

 

= Ec[+] **x,x[+]E∼Dc[2][+]** "−f (x)[⊤]f (x[+]) + f (x)[⊤] _cE[−]_ **x[−]∼DE** _c−_ **f** (x[−]) + H (π) (57)

  [#]


= E **f** (x)[⊤]f (x[+]) + [1] E E [f (x)[⊤]f (x[−]k [)]] + H (π) (58)
_c[+][ E]x,x[+]_ − _K_ _kX∈[K]_ _c[−]k_ **x[−]k** 

 

= E E **f** (x)[⊤]f (x[+]) **f** (x)[⊤]f (x[−]k [)] + H (π) (59)
_c[+],{c[−]k_ _[}][k]_ **x,x[+],{x[−]k** _[}][k]_ − _K[1]_ _kX∈[K]_   _−_ 

 

= E E ln exp(f (x)[⊤](f (x[+]) **f** (x[−]k [)))] + H (π) . (60)
_c[+],{c[−]k_ _[}][k]_ **x,x[+],{x[−]k** _[}][k]_ − _K[1]_ _kX∈[K]_ _−_ 

 

Here, we can proceed with the Jensen’s inequality to lower bound the first term: for a non-negative
vector z R[N]0[, the inequality][ −][N][ −][1][ P]i [N ] [ln][ z][i][ ≥−] [ln(][N][ −][1][ P]i [N ] _[z][i][)][ holds. If we set]_
_∈_ _≥_ _∈_ _∈_

_zk = exp(f_ (x)[⊤](f (x[+]) − **f** (x[−]k [)))][ for][ k][ ∈] [[][K][]][,]

_Rµ-supv(f_ ) − H (π)

_k_ [K] [exp(][f] [(][x][)][⊤][(][f] [(][x][+][)][ −] **[f]** [(][x]k[−][)))]

E ln _∈_ (61)
_≥_ **xc,x[+][+],{,c{[−]kx[}][−]k[k][}][,][k]** "− P _K_ #

E ln exp(f (x)[⊤](f (x[+]) − **f** (x[+]))) + _k∈[K]_ [exp(][f] [(][x][)][⊤][(][f] [(][x][+][)][ −] **[f]** [(][x]k[−][)))]
_≥_ **xc,x[+][+],{,c{[−]kx[}][−]k[k][}][,][k]** "− [P]K #

(62)

exp( **f** (x)[⊤]f (x[+]))

= E ln _−_ + ln K. (63)
**xc,x[+][+],{,c{[−]kx[}][−]k[k][}][,][k]** " exp(−f (x)[⊤]f (x[+])) + _k∈[K]_ [exp(][−][f] [(][x][)][⊤][f] [(][x]k[−][))] #

[P]

Finally, by using Lemma 5,


_Rµ-supv(f_ ) − H (π)

exp(f (x)[⊤]f (x[+]))

_≥_ _c[+],{Ec[−]k_ _[}][k]_ **x,x[+],E{x[−]k** _[}][k]_ "− ln exp(f (x)[⊤]f (x[+])) + _k∈[K]_ [exp(][f] [(][x][)][⊤][f] [(][x]k[−][))] # + ln K (64)

_−_ 2 ln (K + 1) cosh(L[2]) [P] (65)

= Rcont(f ) + ln K 2 ln(K + 1) 2 ln cosh(L[2]), (66)
_−_ _−_


which concludes the proof.


-----

Unlike Theorem 1, Theorem 2 can only be extended to the coarse-grained Y from the case Y = [C].
Indeed, the first expectation term on the right-hand side of Eq. (56) can be transformed as

E **f** (x)[⊤]µc+ + _πc−_ **f** (x)[⊤]µc− = E **f** (x)[⊤]µc+ + _πc−_ **f** (x)[⊤]µc− _, (67)_
_c[+],x_ −  _c[+],x_ − 

_cX[−]∈Y_ _c[−]X∈[C]_

   

because of the linearity of the expectation. On the other hand, it is not straightforward to lower-bound
Eq. (56) by the expectation over the label set [C] when Y is a strict subset of [C].


ESSENTIAL BOUNDS OF MEAN SUPERVISED AND CONTRASTIVE LOSSES


This section provides a supplementary explanation of the essential lower bounds of the mean supervised and contrastive losses. The common approaches of CURL applies the normalization on
representation, in order to employ the cosine similarity _∥f_ (fx()x∥)2[⊤]·∥ff((xx[′][′]))∥2 [as the similarity metric. Then,]

it is reasonable to assume **f** (x) 2 _L for all x with our data representation f_ . The normalized
representation corresponds to the case ∥ _∥_ _≤ L = 1._

When we introduce the constraint **f** (x) 2 _L, the mean supervised loss and contrastive loss are_
restricted as well. As for the mean supervised loss, ∥ _∥_ _≤_

_Rµ-supv(f_ ) inf (68)
_≥_ _∥f_ _[′]∥2≤L_ _[R][µ][-supv][(][f][ ′][)]_

= inf ln 1 + exp(f (x)[⊤](µc **_µy))_** (69)
_∥f_ _[′]∥2≤L_ [E]   Xc≠ _y_ _[′]_ _−_ 

  

= ln 1 + (C − 1) exp(−2L[2]) (70)

(:= R µ[∗]-supv[)][.]  (71)


As for the contrastive loss,

_Rcont(f_ ) inf (72)
_≥_ _∥f_ _[′]∥2≤L_ _[R][cont][(][f][ ′][)]_


ln






1 +






exp(f _[′](x)[⊤](f_ _[′](x[−]k_ [)][ −] **[f][ ′][(][x][+][)))]**
_kX∈[K]_


(73)

(74)


inf E E
_∥f_ _[′]∥2≤L_ _c[+],{c[−]k_ _[}][,][x]_ **x[+],{x[−]k** _[}]_


_≥_ _∥f_ _[′]inf∥2≤L_ _c[+],{Ec[−]k_ _[}][,][x]_ ln 1 + _kX∈[K]_ exp(f _[′](x)[⊤](µc−k_ _[−]_ **_[µ][c][+]_** [))] (74)

_K_ _K_ 1 _m_  _K−m_ 

= 1 ln 1 + m + (K _m) exp(_ 2L[2]) (75)

_m_ _C_ _−_ _C[1]_ _{_ _−_ _−_ _}_

_mX=0_      

(:= Rcont[∗] [)][,] (76)


where the Jensen’s inequality is applied in the second inequality.

D DISCUSSION OF EXISTING LEARNING BOUNDS

In this section, we describe the existing learning bounds in details to make them comparable with
our main results. Then, we further discuss the detailed comparison between our theory and existing
works. Before the discussion, we need to introduce the sub-class loss (of the mean classifier), which
is the supervised classification loss over a subset of classes:


exp(µ[⊤]y **[f]** [(][x][))]
_−_ ln _c_ _T_ [exp(][µ]c[⊤][f] [(][x][))]

_∈_

P


_Rsub(f_ _, T_ ) := E
**x,y**


(77)


where T ⊆ [C] is a subset of classes and y is drawn from the subset of π with respect to T .


-----

**Arora et al.’s bound.** We introduce additional notation that Arora et al. (2019) use. For a subset of
classes T,

-  Q ⊆ [C] is the set of distinct classes in c[+], c[−]1 _[, . . ., c]K[−]_

-  I [+] := {k ∈ [K] | c[−]k [=][ c][+][}]

-  Col := _k∈[K]_ [1]{c[+]=c[−]k _[}][ =][ |][I]_ [+][|]

-  ρmax(T ) := maxc _T πc_

[P] _∈_

-  ρ[+]min[(][T] [) := min][c][∈][T][ P]c[+],{c[−]k _[}][k][∼][π][K][+1]_ [(][c][+][ =][ c][ |][ Q][ =][ T, I] [+][ =][ ∅][)]

-  τK := P(I [+] ≠ _∅)_


Arora et al. (2019) prove a finite-sample learning bound in Theorem B.1. In its proof, Eq. (26) is a
learning bound established for a fixed f . For the comparison, we focus on their Eq. (26):


+
_ρmin[(][T]_ [)]

(1 _τK)_ E
_−_ _T ∼π[K][+1]_  _ρmax(T_ ) _[R][sub][(][f]_ _[, T]_ [)]

_≤_ _Rcont(f_ ) − _τK_ _c[+],{c[−]k_ _[}]E[k][∼][π][K][+1]_


ln(Col + 1) _I_ [+] ≠ _∅_ _._ (78)



We split the expectation term in the left-hand side as follows.


+
_ρmin[(][T]_ [)]

E

_ρmax(T_ ) _[R][sub][(][f]_ _[, T]_ [)]

 

+
_ρmin[(][T]_ [)]

= P(T covers [C]) E

_·_ _ρmax(T_ ) _[R][sub][(][f]_ _[, T]_ [)]

=vK+1  

+

| {z } _ρmin[(][T]_ [)][T][ covers][ [][C][]]

+ P(T does not cover [C]) E (79)
_·_ _ρmax(T_ ) _[R][sub][(][f]_ _[, T]_ [)]

 

_ρ[+]min[([][C][])]_
_vK+1_ _._ _[T][ does not cover][ [][C][]]_ (80)
_≥_ _ρmax([C])_ _[R][sub][(][f]_ _[,][ [][C][])]_

=Rµ-supv(f )

Under the uniform class prior assumption (| {z } **_π =_** [1]/C **1), ρmax([C]) =** [1]/C, and we can pick any class
_·_
_c0 ∈_ [C] by the symmetry and ρ[+]min[([][C][]) =][ P][(][c][+][ =][ c][0][ |][ Q][ = [][C][]][, I] [+][ =][ ∅][) =][ 1][/][C][. In addition,]

_τK_ _c[+],{c[−]k_ _[}]E[k][∼][π][K][+1]_ ln(Col + 1) _I_ [+] ≠ _∅_ = E [ln(Col + 1)] − (1 − _τK) E_ ln(Col + 1) _I_ [+] = ∅

   (81)

= E [ln(Col + 1)] . (82)

As a result, we obtain the following simplified expression in Table 1:


1

_Rcont(f_ ) E[ln(Col + 1)] _._ (83)
(1 _τK)vK+1_ _{_ _−_ _}_
_−_


_Rµ-supv(f_ )
_≤_


**Nozawa & Sato’s bound.** The learning bound provided by Nozawa & Sato (2021, Theorem 8)
involves a factor resulting from data augmentation and self-supervised learning setting. By dropping
this (negative) factor, the learning bound is


_Rcont(f_ )
_≥_ 2[1]


_vK+1Rµ-supv(f_ ) + (1 − _vK+1)_ _T_ **_πE[K][+1][[][R][sub][(][f]_** _[, T]_ [)] +][ E][[ln(Col + 1)]] (84)
_∼_ 


(85)

_≥_ [1]2

_[{][v][K][+1][R][µ][-supv][(][f]_ [) +][ E][[ln(Col + 1)]][}][,]

resulting in the bound in Table 1. The sub-class loss may be safely dropped because it has the
coefficient 1 _vK+1, which is expected to be exponentially small in K._
_−_


-----

**Ash et al.’s bound.** Ash et al. (2021, Theorem 5) provides the following learning bound

_Rµ-supv(f_ ) ≤ 2 l 2(1(1− −Kππ(π−((1)1)−)1)H)[K]C−1 m (Rcont(f ) − _τK_ _c[+],{c[−]k_ _[}]E[k][∼][π][K][+1]_ ln(Col + 1) _I_ [+] ≠ _∅_ ) _._

  (86)

By substituting π = [1]/C · 1 and τK E[ln(Col + 1) | I [+] ≠ _∅] = E[ln(Col + 1)], the bound in Table 1_
is obtained.

**Detailed comparisons.** As we stated in Section 4 of the main text, only our bound agrees well with
the experimental fact that the larger K is better for all K regions:

-  Arora et al. (2019): Large K degrades the performance because of the label collision.

-  Nozawa & Sato (2021): Large K improves the performance for K > C.

-  Ash et al. (2021): The optimal K exists by the collision-coverage trade-off.

Even though the claim by Nozawa & Sato (2021) is similar with ours, we discovered a different
underlying mechanism to support this idea, which leads to better explainability of empirical facts.

The proof of Nozawa & Sato (2021) is based on the idea of label coverage: The more negative
samples we draw (larger K), the more likely the negative samples can cover all class labels. The
upper bound based on this idea is only activated when K > C because label coverage is impossible
with K ≤ _C. This inability contradicts the real experiments including Chen et al. (2021), which_
showed that CURL exhibits reasonable performance even with small K.

Our proof leverages the idea that Rcont and Rµ-supv have the similar log-sum-exp functional forms.
This similarity casts Rcont as a surrogate estimator of Rµ-supv and its estimation gap is reduced
with larger K. Even with small K, the upper bound of Rµ-supv is loose but not vacuous thereby the
estimation of Rµ-supv is still possible. Our theoretical claim reveals that the surrogate gap improves
in O(ln [1]/K) for all K regions, which is in good agreement with the real experiments. Eventually,
our theory provides practical feedback such that one may reduce K (even smaller than C) to trade off
the downstream performance with the computational cost.

E RELATIONSHIP TO MUTUAL INFORMATION (MI) ESTIMATION

The contrastive loss Rcont we studied in this paper is also known as the InfoNCE loss (Oord et al.,
2018), which is known to be deeply related to the multi-sample estimation of mutual information (MI)
(Oord et al., 2018; Poole et al., 2019; Song & Ermon, 2020). Recently, the theoretical limitations
of sample-based MI estimation have been analyzed (Gao et al., 2015; McAllester & Stratos, 2020).
These studies revealed that a particular type of sample-based estimator of MI (Gao et al., 2015) or
its lower bound (McAllester & Stratos, 2020) can be upper bounded by O(ln N ) for the number of
samples N . In this section, we discuss the implications of these limitations in the CURL setting.

Given two random variables X and Y, suppose that we have K + 1 randomly drawn pairs
(xi, yi) _i=1_ from these random variables such that for all (i, j), (xi, yj) can be regarded as a
_{_ _}[K][+1]_
positive pair when i = j, and otherwise can be regarded as a negative pair. Poole et al. (2019,
Equation 10) derived the following lower bound for MI:

_K+1_

1 exp(s(xi, yi))

_I(X; Y ) ≥_ _INCE[K][+1]_ [:=][ E] " _K + 1_ _i=1_ ln _K1+1_ _Kj=1+1_ [exp(][s][(][x][i][, y][j][))] # _,_ (87)

X

where I(X; Y ) is the MI between X and Y, and s(x, y) is a critic function. This lower boundP
estimator can be rewritten using Rcont as follows:


_K+1_

exp(s(xi, yi))
ln 1 _K+1_ (88)
_i=1_ _K+1_ _j=1_ [exp(][s][(][x][i][, y][j][))] #

X

_K+1_

exp(Ps(xi, yi))
ln _K+1_ + ln(K + 1) (89)
_i=1_ _j=1_ [exp(][s][(][x][i][, y][j][))] #

X

P


_INCE[K][+1]_ [:=][ E]

= E


_K + 1_

1

_K + 1_


-----

_K+1_

1 exp(s(xi, yi))

= E ln + ln(K + 1) (90)

" _K + 1_ _i=1_ exp(s(xi, yi)) + _j≠_ _i_ [exp(][s][(][x][i][, y][j][))] #

X

exp(s(x, x[+]))

= E ln [P] + ln(K + 1) (91)

" exp(s(x, x[+])) + _j=1_ [exp(][s][(][x, x]j[−][))] #

= _Rcont(f_ ) + ln(K + 1). (92)
_−_

[P][K]

The first equality is obtained by putting the constant in the denominator outside. The third equality
comes by replacing the notation (xi, yi) with (x, x[+]) under the assumption that all (xi, yi) come
from the same iid distribution. By setting s(x, y) := f (x)[⊤]f (y), we obtain the last equation.

Here, McAllester & Stratos (2020) gave the following theorem for the sample-based estimator of the
lower bound on MI.

**Theorem 6 (McAllester & Stratos (2020) Theorem 1.1, informal). Let** _I_ _[N]_ _be any mapping from N_
_samples of (X, Y ) to R that satisfies_

[b]

_I(X; Y ) ≥_ _I[b][N]_ ({(xi, yi)}i[N]=1[)] (93)

_in high probability, then the following relationship holds in high probability:_

_Ib[N]_ ({(xi, yi)}i[N]=1[)][ ≤] [2 ln][ N][ + 5][.] (94)


Since INCE[K][+1] [satisfies the condition for][ b]I _[K][+1], we now have the following:_

_Rcont(f_ ) + ln(K + 1) 2 ln(K + 1) + 5 = _Rcont(f_ ) ln(K + 1) 5. (95)
_−_ _≤_ _⇒_ _≥−_ _−_

However, the right-hand statement always holds by the construction of Rcont for all f (∀f _, Rcont(f_ ) ≥
0). In other words, in the case of the CURL setting, McAllester & Stratos (2020)’s theorem does not
restrict Rcont, which means that the large K effect investigated in our paper comes from a completely
different mechanism from the above theorem. The existing studies on sample-based MI estimation
are worthwhile in the sense that these works revealed the O(ln N ) effect on the non-trivial estimators
such as k-NN based estimator (Gao et al., 2015) or any kind of lower bound estimator (McAllester &
Stratos, 2020).

F EXPERIMENTAL DETAILS

F.1 SYNTHETIC DATASET

We used Adam (Kingma & Ba, 2015) optimizer with the weight decay of coefficient 0.01 to all
parameters. The mini-batch size was set to B = 1 024 and the number of epochs was 300. The
learning rate was set to 0.01 with ReduceLROnPlateau scheduler (patience: 10 epochs) provided
by PyTorch (Paszke et al., 2019).

F.2 CIFAR-10/100

We treated 10% training samples as a validation dataset by sampling class uniformly. We used the
original test dataset for testing. We used the same data-augmentation as in the CIFAR-10 experiment
by Chen et al. (2020) during contrastive learning and linear supervised training of the linear classifier.

As a feature extractor f, we modified the ResNet-18 (He et al., 2016) by following the convention of
self-supervised representation learning (Chen et al., 2020, B.9); replacement of the first convolutional
layer with a smaller one, removal of the first max-pooling layer, and replacement of the final fullyconnected layer with a nonlinear projection head whose dimensional is 32.[10]

Since we need to enlarge the negative samples size K that depends on the size of mini-batches, we
followed a large mini-batch training setting used in recent self-supervised learning (Chen et al., 2020;

10Unlike the reported results by Chen et al. (2021), smaller dimensionality, i.e., 32 gives better downstream
accuracy on CIFAR-100 than 64 or 128. This difference might come from the differences in the loss function
and positive pair’s generation process.


-----

Caron et al., 2020). We used LARC (You et al., 2017) optimizer wrapping the momentum SGD,
whose momentum term was 0.9. We applied weights decay of coefficient 10[−][4] to all parameters
except for all bias terms and batch norm’s parameters. The base learning rate was initialized at
lr × _√B, where lr ∈{2, 4, 6} ×_ [1]/64 and mini-batch size B = 1 024 inspired by SimCLR’s squared

learning rate scaling. As a learning rate scheduler for each iteration, we used linear warmup during
the first 10 epochs and cosine annealing without restart (Loshchilov & Hutter, 2017) during the rest
epochs. The number of epochs was 2 000.

We implemented our experimental code by using PyTorch (Paszke et al., 2019)’s distributed dataparallel training (Li et al., 2020) on 8 NVIDIA A100 GPUs provided by the internal cluster. Therefore
we replaced the all batch normalization layer with SyncBatchNorm module provided by PyTorch.[11]
To accelerate contrastive learning, we used automatic mixed-precision training provided by PyTorch.

F.3 WIKI-3029

Wiki-3029 contains 3 029 English Wikipedia article pages. Each page consists of 200 sentences.
Since the dataset does not have the explicit train/validation/test splits, we split the dataset into
70%/10%/20% train/validation/test datasets, respectively. As a pre-processing, we tokenized the
dataset using torchtext’s basic_english tokenizer. After tokenization, we removed the tokens
whose frequency is less than 5 in the training dataset. We did not use data augmentation.

We used fasttext (Joulin et al., 2017)’s based feature extractor.[12] In our preliminary experiments,
only using a word embedding layer and average pooling among words perform better than either
additional linear or nonlinear projection heads. A similar model to ours is also used in Ash et al.
(2021). The dimensionality of the word embedding layer was 256.

We mainly followed the same optimization setting as our CIFAR-10/100 experiments. We note that
the mini-batch size B = 2 048; the initial learning rate lr was selected in {1, 2, 3, 4} × [1]/40; no
weights decay; the number of epochs was 90; and perform linear warmup during the first 3 epochs.
When we decrease C, the number of epochs is multiplied by [3 000]/C for simplicity.[13]

F.4 CONTRASTIVE LEARNING

By following the data generation process in contrastive representation learning and existing
work (Arora et al., 2019; Ash et al., 2021), we treated the supervised classes Y as latent classes

[C]. After obtaining training/validation/test datasets as described above, we carefully constructed
positive pairs for contrastive learning before training[14] as follows; We treated each sample in the
training data as an anchor sample. We drew a different sample from the same latent class of each
anchor sample as a positive sample in the training dataset. For negative samples, we drew K negative
samples from other samples in the same mini-batch by following the convention of self-supervised
representation learning such as SimCLR (Chen et al., 2020). Since Chen et al. (2020) used all other
samples as negative samples, the negative samples size and the size of mini-batches depend on each
other: K = 2B − 2. To relax the effect of the difference of the mini-batch size when we change K,
we drew K samples without replacement from 2B − 2 inspired by Ash et al. (2021). In this sampling,
we guaranteed to draw at most one sample from each positive pair because we are concerned about
the relation between the number of latent classes and K. We did not use validation and test datasets
during contrastive representation learning.

F.5 MEAN AND LINEAR CLASSIFIERS’ EVALUATION

For evaluation, we reported the test accuracy values of mean and linear classifiers. For a linear
classifier, we used Nesterov’s momentum SGD, whose momentum coefficient was 0.9 without weight

11See Wu & Johnson (2021, Sec. 6.2) for more detailed discussion of this replacement for contrastive learning.
12Arora et al. (2019) uses GRU-based feature encoder with frozen word embeddings of GloVe (Pennington
et al., 2014) trained on commonCrawl.

13We found the contrastive learning did not yield good feature representations for a downstream task without
this longer training.

14We can create the labeled dataset, especially with non-overlapped latent classes, if we draw positive samples
at each iteration or epoch during optimization using stochastic gradient descent.


-----

3.4

3.2

3.0

2.8

2.6

2.4

2.2

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
|||||Ours Supe|rvised loss||
||||||||
||||||||
||||||||
||||||||
||||||||
||||||||


Ours
Supervised loss


4 16 32 64 128 512

_K_

(a) CIFAR-10.


5.4

5.2

5.0

4.8

4.6

4.4

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||Ours Supervised loss||
||||||
||||||
||||||
||||||
||||||
||||||


Ours
Supervised loss


4 64 128 512

_K_

(b) CIFAR-100.


Figure 9: Enlarged Figure 1 for the detailed comparison between the proposed bound and the
supervised loss on CIFAR-10/100 datasets. All value is an averaged value among three runs with a
different random seed. Error bar indicates the standard deviation.

decay. We set the mini-batch size B = 256 and B = 512 for CIFAR-10/100 and Wiki-3029,
respectively. We used cosine annealing without restart as a learning rate scheduler for each iteration.
We set 100 and 30 epochs for CIFAR-10/100 and Wiki-3029 datasets, respectively. For CIFAR-10/100,
we set learning rate as 0.03. For Wiki-3029, we searched the learning rate in {0.5, 1, 5, 10, 50}× [1]/10[3].
The learning rate was scaled by using squared learning rate scaling. For linear evaluation of CIFAR10/100, we used PyTorch’s distributed data-parallel training. We calculated the test accuracy by
using the best combination of the contrastive model and the hyper-parameter of a linear classifier
that maximizes the validation accuracy. We repeated contrastive learning and downstream task’s
evaluation three times with different random seeds and reported the averaged values.

F.6 DETAILS OF FIGURE 1

Before computing the upper bounds and supervised loss, we normalized feature representations f (x)
learned in Appendix F.4 to ensure L = 1, which is the upper bound of **f** (x) 2, **x. For each random**
_∥_ _∥_ _∀_
seed and the number of negative samples K, we selected learned feature encoder f that got the highest
validation mean supervised accuracy in different learning rates of the optimizer of the contrastive
learning. Then we calculated the test supervised loss value by using the selected contrastive models.

Using the same feature encoder with L2 normalization, we calculated the contrastive loss on the test
dataset. To do so, we created positive pairs by the same procedure on the test dataset as described
in Appendix F.4. Negative samples were also drawn from the other samples in the mini-batches as
the contrastive learning step described in Appendix F.4. To calculate the contrastive loss, we used
the same batch size as the contrastive learning step and only one epoch. Since this contrastive loss
calculation was stochastic due to the sampling of positive and negative samples, we repeated the
contrastive loss calculation 25 times and averaged them to create plot Figure 1. Note that we used the
theoretical values of τK, vK+1, E ln(Col + 1) that are shown in the existing upper bounds on Table 1
rather than the simulated values.

Figure 9 shows the enlarged version of Figure 1 and the same plot using CIFAR-100. This figure
focuses on the detailed comparison between the test datasets’ empirical supervised loss values and
theoretical bounds. For both CIFAR-10/100 datasets, there were almost no changes in the supervised
loss as K varied, and the losses were slightly larger in the region where K was small. These results
are consistent with the theoretical estimation of the upper bounds (solid lines).

F.7 DETAILS OF FIGURE 8

During minimization of the contrastive loss to learn f in Appendix F.4, we saved the model’s weight at
every 200 epochs. We reported the test mean supervised accuracy using f that maximized validation
accuracy among different learning rate values.


-----

F.8 ADDITIONALLY USED LIBRARIES

In our experiments, we also used scikit-learn (Pedregosa et al., 2011) for train/val/test data splits. We
created all plots by using matplotlib (Hunter, 2007) and seaborn (Waskom, 2021) via pandas (Reback
et al., 2020) except for Figure 2. We managed our experiments’ configuration using hydra (Yadan,
2019) and experimental results using Weights & Biases (Biewald, 2020). For effective parallelized
execution of our experimental codes, we use GNU Parallel (Tange, 2021).


-----

