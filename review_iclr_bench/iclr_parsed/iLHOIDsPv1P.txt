# PAC-BAYES INFORMATION BOTTLENECK

**Zifeng Wang** _[∗]_ **Shao-Lun Huang** **Ercan E. Kuruoglu**
UIUC Tsinghua University Tsinghua University

**Jimeng Sun** **Xi Chen** **Yefeng Zheng**
UIUC Tencent Tencent

ABSTRACT

Understanding the source of the superior generalization ability of NNs remains
one of the most important problems in ML research. There have been a series
of theoretical works trying to derive non-vacuous bounds for NNs. Recently, the
compression of information stored in weights (IIW) is proved to play a key role
in NNs generalization based on the PAC-Bayes theorem. However, no solution
of IIW has ever been provided, which builds a barrier for further investigation of
the IIW’s property and its potential in practical deep learning. In this paper, we
propose an algorithm for the efficient approximation of IIW. Then, we build an
IIW-based information bottleneck on the trade-off between accuracy and information complexity of NNs, namely PIB. From PIB, we can empirically identify the
fitting to compressing phase transition during NNs’ training and the concrete connection between the IIW compression and the generalization. Besides, we verify
that IIW is able to explain NNs in broad cases, e.g., varying batch sizes, overparameterization, and noisy labels. Moreover, we propose an MCMC-based algorithm to sample from the optimal weight posterior characterized by PIB, which
fulfills the potential of IIW in enhancing NNs in practice.

1 INTRODUCTION

Understanding the behavior of neural networks (NNs) learned through stochastic gradient descent
(SGD) is a prerequisite to revealing the source of NN’s generalization in practice. Information bottleneck (IB) (Tishby et al., 2000) was a promising candidate to reveal the principle of NNs through
the lens of information stored in encoded representations of inputs. Drawn from the conception of
representation minimality and sufficiency in information theory, IB describes the objective of NNs
as a trade-off, where NNs are abstracted by a Markov chain Y ↔ _X ↔_ _T_, as

max _I(T_ ; Y ) _βI(T_ ; X), (1)
_T_ _−_

where I(T ; X) and I(T ; Y ) are the mutual information of representation T towards inputs X and
labels Y, respectively. IB theory claimed (Tishby & Zaslavsky, 2015) and then empirically corroborated (Shwartz-Ziv & Tishby, 2017) that NNs trained by plain cross entropy loss and SGD all
confront an initial fitting phase and a subsequent compression phase. It also implied that the representation compression is a causal effect to the good generalization capability of NNs. IB theorem
points out the importance of representation compression and ignites a series of follow-ups to propose new learning algorithms on IB to explicitly take compression into account (Burgess et al., 2018;
Achille & Soatto, 2018b; Dai et al., 2018; Li & Eisner, 2019; Achille & Soatto, 2018a; Kolchinsky
et al., 2019; Wu et al., 2020; Pan et al., 2020; Goyal et al., 2019; Wang et al., 2019; 2020a).

However, recent critics challenged the universality of the above claims. At first, Saxe et al. (2019)
argued that the representation compression phase only appears when double-sided saturating nonlin_earities like tanh and sigmoid are deployed. The boundary between the two phases fades away_
with other nonlinearities, e.g., ReLU. Second, the claimed causality between compression and generalization was also questioned (Saxe et al., 2019; Goldfeld et al., 2019), i.e., sometime networks that
do not compress still generalize well, and vice versa. To alleviate this issue, Goldfeld et al. (2019)

_∗Correspondence at zifengw2@illinois.edu_


-----

proposed that the clustering of hidden representations concurrently occurs with good generalization
ability. However, this new proposal still lacks solid theoretical guarantee. Third, mutual information
becomes trivial in deterministic cases (Shwartz-Ziv & Alemi, 2020). Other problems encountered
in deterministic cases were pointed out by Kolchinsky et al. (2018). Although several techniques
(Shwartz-Ziv & Tishby, 2017; Goldfeld et al., 2019), e.g., binning and adding noise, are adopted to
make stochastic approximation for the information term, they might either violate the principle of
IB or be contradictory to the high performance of NNs. Motivated by these developments, we focus
on the following questions:

-  Does a universal two-phase training behavior of NNs exist in practice? If this claim is invalid
with the previous information measure I(T ; X), can we achieve this two-phase property through
another information-theoretic perspective?

-  As I(T ; X) was unable to fully explain NN’s generalization, can we find another measure with
theoretical generalization guarantee? Also, how do we leverage it to amend the IB theory for deep
neural network?

-  How do we utilize our new IB for efficient training and inference of NNs in practice?

In this work, we propose to handle the above questions through the lens of information stored in
weights (IIW), i.e., I(w; S) where S = {Xi, Yi}i[n]=1 [is a finite-sample dataset. Our main contri-]
butions are four-fold: (1) we propose a new information bottleneck under the umbrella of PACBayes generalization guarantee, namely PAC-Bayes Information Bottleneck (PIB); (2) we derive
an approximation of the intractable IIW; (3) we design a Bayesian inference algorithm grounded
on stochastic gradient Langevin dynamics (SGLD) for sampling from the optimal weight posterior specified by PIB; and (4) we demonstrate that our new information measure covers the
wide ground of NN’s behavior. Crediting to the plug-and-play modularity of SGD/SGLD, we
can adapt any existing NN to a PAC-Bayes IB augmented NN seamlessly. Demo code is at

[https://github.com/RyanWangZf/PAC-Bayes-IB.](https://github.com/RyanWangZf/PAC-Bayes-IB)

2 A NEW BOTTLENECK WITH PAC-BAYES GUARANTEE

In this section, we present the preliminaries of PAC-Bayes theory and then introduce our new information bottleneck. A loss function ℓ(f **[w](X), Y ) is a measure of the degree of prediction accuracy**
_f_ **[w](X) compared with the ground-truth label Y . Given the ground-truth joint distribution p(X, Y ),**
the expected true risk (out-of-sample risk) is taken on expectation as

_L(w) ≜_ Ep(w _S)Ep(X,Y )[ℓ(f_ **[w](X), Y )].** (2)
_|_

Note that we take an additional expectation over p(w|S) because we are evaluating risk of the
learned posterior instead of a specific value of parameter w. In addition, we call p(w|S) posterior
here for convenience while it is not the Bayesian posterior that is computed through Bayes theorem
_p(w_ _S) =_ _[p][(][w]p[)][p](S[(][S])_ _[|][w][)]_ . The PAC-Bayes bounds hold even if prior p(w) is incorrect and posterior
_|_

_p(w|S) is arbitrarily chosen._

In practice, we only own finite samples S. This gives rise to the empirical risk as


_LS(w) = Ep(w_ _S)_
_|_


_ℓ(f_ **[w](Xi), Yi)**
_i=1_

X


(3)


With the above Eqs. (2) and (3) at hand, the generalization gap of the learned posterior p(w|S) in
out-of-sample test is ∆L(w) ≜ _L(w) −_ _LS(w). Xu & Raginsky (2017) proposed a PAC-Bayes_
bound based on the information contained in weights I(w; S) that

2σ[2]

Ep(S)[L(w) − _LS(w)] ≤_ _n [I][(][w][;][ S][)][,]_ (4)

r

when ℓ(f **[w](X), Y ) is σ-sub-Gaussian.[1]** A series of follow-ups tightened this bound and verified it
is an effective measure of generalization capability of learning algorithms (Mou et al., 2018; Negrea

1A easy way to fulfill this condition is to clip the lost function to ℓ _∈_ [0, a] hence it satisfies a2 [-sub-Gaussian]

(Philippe, 2015; Xu & Raginsky, 2017).


-----

et al., 2019; Pensia et al., 2018; Zhang et al., 2018). Therefore, it is natural to build a new information
bottleneck grounded on this PAC-Bayes generalization measure, namely the PAC-Bayes information
bottleneck (PIB), as
min (5)
_p(w|S)_ _[L][PIB][ =][ L][S][(][w][) +][ βI][(][w][;][ S][)][.]_

For classification term, the loss term LS(w) becomes the cross-entropy between the prediction
_p(Y |X, w) and the label p(Y |X), hence PIB in Eq. (5) is equivalent to_

max (6)
_p(w_ _S)_ _[I][(][w][;][ Y][ |][X, S][)][ −]_ _[βI][(][w][;][ S][)][,]_
_|_

which demonstrates a trade-off between maximizing the sufficiency (the information of label Y contained in w) and minimizing the minimality of learned parameters w (the information of dataset
_S contained in w). Unlike previous IB based on representations, our PIB is built on weights that_
are not directly influenced by inputs and selected activation functions. Likewise, the trade-off described by PIB objective is more reasonable since its compression term is explicitly correlated to
generalization of NNs.

3 ESTIMATING INFORMATION STORED IN WEIGHTS

In this section, we present a new notion of IIW I(w; S) built on the Fisher information matrix that
relates to the flatness of the Riemannian manifold of loss landscape. Unlike Hessian eigenvalues of
loss functions used for identifying flat local minima and generalization but can be made arbitrarily
large (Dinh et al., 2017), this notion is invariant to re-parameterization of NNs (Liang et al., 2019).
Also, our measure is invariant to the choice of activation functions because it is not directly influenced by input X like I(T ; X). We leverage it to monitor the information trajectory of NNs trained
by SGD and cross entropy loss and verify it is capable of reproducing the two-phase transition for
varying activations (e.g., ReLU, linear, tanh, and sigmoid) in §5.1.

3.1 CLOSED-FORM SOLUTION WITH GAUSSIAN ASSUMPTION

By deriving a new information bottleneck PIB, we can look into how IIW I(w; S) and LS(w)
evolve during the learning process of NNs optimized by SGD. Now the key challenge ahead is how
to estimate the IIW I(w; S), as

_I(w; S) = Ep(S)[KL(p(w|S) ∥_ _p(w))]_ (7)

is the expectation of Kullback-Leibler (KL) divergence between p(w|S) and p(w) over the distribution of dataset p(S). And, p(w) is the marginal distribution of p(w|S) as p(w) ≜ Ep(S)[p(w|S)].
When we assume both p(w) = (w **_θ0, Σ0) and p(w_** _S) =_ (w **_θS, ΣS) are Gaussian distribu-_**
_N_ _|_ _|_ _N_ _|_
tions, the KL divergence term in Eq. (7) has closed-form solution as

KL(p(w _S)_ _p(w)) = [1]_ log [det Σ][S] _D + (θS_ **_θ0)[⊤]Σ[−]0_** [1][(][θ][S][ −] **_[θ][0][) +][ tr]_** Σ[−]0 [1][Σ][S] _._ (8)
_|_ _∥_ 2 det Σ0 _−_ _−_



  []

Here det A and tr(A) are the determinant and trace of matrix A, respectively; D is the dimension
of parameter w and is a constant for a specific NN architecture; θS are the yielded weights after
SGD converges on the dataset S. If the covariances of prior and posterior are proportional,[2] the
logarithmic and trace terms in Eq. (8) both become constant. Therefore, the mutual information
term is proportional to the quadratic term as

_I(w; S) ∝_ Ep(S) (θS − **_θ0)[⊤]Σ[−]0_** [1][(][θ][S][ −] **_[θ][0][)]_** = Ep(S) **_θS[⊤][Σ]0[−][1][θ][S]_** _−_ **_θ0[⊤][Σ]0[−][1][θ][0][.]_** (9)

In the next section, we will see how to set prior covariance  Σ0.  

2Assuming the same covariance for the Gaussian randomization of posterior and prior is a common practice
of building PAC-Bayes bound for simplification, see (Dziugaite & Roy, 2018; Rivasplata et al., 2018).


-----

3.2 BOOTSTRAP COVARIANCE OF ORACLE PRIOR

Since the computation of exact oracle prior Σ0 needs the knowledge of p(S) [3], we propose to
approximate it by bootstrapping from S as

Σ0 = Ep(S) (θS **_θ0)(θS_** **_θ0)[⊤][]_** (θSk **_θS)(θSk_** **_θS)[⊤],_** (10)
_−_ _−_ _≃_ _K[1]_ _k_ _−_ _−_
 X

is still a valid sample followingwhere Sk is a bootstrap sample obtained by re-sampling from the finite data p(S). Now we are closer to the solution but the above term is S, and Sk ∼ _p(S)_
still troublesome to calculate. For getting {θSk _}k[K]=1[, we need to optimize on a series of (][K][ times)]_
bootstrapping datasets {Sk}k[K]=1 [via SGD until it converges, which is prohibitive in deep learning]
practices. Therefore, we propose to approximate the differencefrom robust statistics literature (Cook & Weisberg, 1982; Koh & Liang θS −θ0, by 2017 influence functions; Wang et al., 2020c drawn;b).
_the parameterLemma 1 (Influence Function (θ[ˆ]S ≜_ argminθ LCook & WeisbergS(θ) = argminθ, 1982n1 _ni))=1. Given a dataset[ℓ][i][(][θ][)][4][ that optimizes the empirical loss] S = {(Xi, Yi)}i[n]=1_ _[and]_
_function, if we drop sample (Xj, Yj) in S to get a jackknife sample S_ _j and retrain our model, the_
_new parameters are_ **_θ[ˆ]S\j = argminθ LS\j_** (θ) = argminP **_θ_** _n1_ _ni=1_ _[ℓ][i]\[(][θ][)][ −]_ _n[1]_ _[ℓ][j][(][θ][)][. The approxi-]_

_mation of parameter difference_ **_θ[ˆ]S_** _j_ **_θS is defined by influence function ψ, as_**
_\_ P
_−_ [ˆ]

**_θˆS\j −_** **_θ[ˆ]S ≃−_** _n[1]_ **_[ψ][j][,][ where][ ψ][j][ =][ −][H]θ[−]ˆS[1][∇][θ][ℓ][j][(ˆ]θS) ∈_** R[D], (11)

_and Hθ ˆS_ _n_ _ni=1_ **_θ[ℓ][i][(ˆ]θS)_** R[D][×][D] _is Hessian matrix._

[≜] [1] _[∇][2]_ _∈_

The application of influence functions can be further extended to the case when the loss functionP
is perturbed by a vector ξ = (ξ1, ξ2, . . ., ξn)[⊤] _∈_ R[n] as **_θ[ˆ]S,ξ = argminθ_** _n1_ _ni=1_ _[ξ][i][ℓ][i][(][θ][)][. In this]_
scenario, the parameter difference can be approximated by
P

_n_

**_θˆS,ξ_** **_θS_** (ξi 1) ψi = [1] (12)
_−_ [ˆ] _≃_ _n[1]_ _i=1_ _−_ _n_ [Ψ][⊤] [(][ξ][ −] **[1][)][,]**

X

where Ψ = (ψ1, ψ2, . . ., ψn)[⊤] _∈_ R[n][×][D] is a combination of all influence functions ψ; 1 =
(1, 1, . . ., 1)[⊤] is an n-dimensional all-one vector. Lemma 1 gives rise to the following lemma on
approximation of oracle prior covariance in Eq. (10):
**Lemma 2 (Approximation of Oracle Prior Covariance). Given the definition of influence functions**
_(Lemma 1) and Poisson bootstrapping (Lemma A.2), the covariance matrix of the oracle prior can_
_be approximated by_

_K_

Σ0 = Ep(S) (θS − **_θ0)(θS −_** **_θ0)[⊤][]_** _≃_ _K[1]_ _k=1_ **_θˆξk −_** **_θ[ˆ]_** **_θˆξk −_** **_θ[ˆ]_** _⊤_ _≃_ _n[1]_ **[H][−]θˆ** [1][F][ ˆ]θ[H]θ[−]ˆ [1] _≃_ _n[1]_ **[F][−]θˆ** [1][,]
 X     (13)

_where F ˆθ_ _[is Fisher information matrix (FIM); we omit the subscript][ S][ of][ ˆ]θS and_ **_θ[ˆ]S,ξ for notation_**
_conciseness, and ξ[k]_ _is the bootstrap resampling weight in the k-th experiment._


Please refer to Appendix A for the proof of this lemma.

3.3 EFFICIENT INFORMATION ESTIMATION ALGORITHM

After the approximation of oracle prior covariance, we are now able to rewrite the IIW term I(w; S)
in Eq. (9) to

_I(w; S) ∝_ _nEp(S)_ (θS − **_θ0)[⊤]F ˆθ[(][θ][S][ −]_** **_[θ][0][)]_** _≃_ _n(θ[¯]S −_ **_θ0)[⊤]F ˆθ[(¯]θS −_** **_θ0) =_** _I(w; S)._ (14)

3
_p(w4) =ΣNote0 is called oracle prior because it minimizes the term E Lp(SS()θ[p)( is not the expected empirical riskw|S)] (Dziugaite et al., 2021)._ _LS(w) in Eq. ( I(w; S3) =); instead, it is the deterministic empir- Ep(S)[KL(p(w|S)[e] ∥_ _p(W_ )] when
ical risk that only relates to the mean parameter θ. We also denote ℓ(f **_[θ](Xi), Yi) by ℓi(θ) for the notation_**
conciseness.


-----

**Algorithm 1: Efficient approximate information estimation of I(w; S)**
**Data: Total number of samples n, batch size B, number of mini-batches in one epoch T0,**
number of information estimation iterations T1, learning rate η, moving average
hyperparameters ρ and K, a sequence of gradients set ∇L = ∅

**Result: Calculated approximate information** _I(w; S)_

**1 Pretrain the model by vanilla SGD to obtain the prior mean θ0 ;**

**2 for t=1:T0 do** [e]

**3** _∇Lt ←∇θ_ _B[1]_ _b_ _[ℓ][b][(ˆ]θt−1),_ **_θ[ˆ]t ←_** **_θ[ˆ]t−1 −_** _η∇Lt ;_ /* Vanilla SGD */

**4** _∇L ←∇L_ _{∇PLt} ;_ /* Store gradients */

**5** **_θ¯t_** _ρθ[¯]t[2]_ 1 [+][ 1]K[−][ρ] _Kk=0−1_ **_θ[ˆ]t[2]_** _k_ [;] /* Moving average */
_←_ [S]− _−_

**6 end** q

P

**7 ∆θ** **_θT0_** **_θ0, ∆F0_** 0 ;

**8 for t=1: ←** [¯]T1 do − _←_

**9** ∆Ft ∆Ft 1 + (∆θ[⊤] _Lt)[2]_ ; /* Storage-friendly computation */

**10 end** _←_ _−_ _∇_

_n_
**11** _I[e](w; S) ←_ _T1_ [∆][F][T][1] [;]

**Algorithm 2: Optimal Gibbs posterior inference by SGLD.**
**Data: Total number of samples n, batch size B, learning rate η, temperature β**
**Result: A sequence of weights {wt}t≥kˆ** [following][ p][(][w][|][S][∗][)]

**1 repeat**

/* Mini-batch gradient of energy function *[/]

**2** _∇U[e]S∗_ (wt−1) ←∇ _−_ _[B]n_ _b_ [log][ p][(][Y][b][|][X][b][,][ w][t][−][1][)][ −] _[β][t][−][1][ log][ p][(][w][t][−][1][)]_ ;

/* SGLD by gradient plus isotropic Gaussian noise *[/]

  P 

**3** _εt_ (ε **0, ID), wt** **wt** 1 _ηt_ 1 _US∗_ (wt 1) + 2ηt 1βt 1εt ;
_←N_ _|_ _←_ _−_ _−_ _−_ _∇_ [e] _−_ _−_ _−_

/* Learning rate & temperature decay *[/]

**54 untilη the weight sequencet ←** _φη(ηt−1), βt ← {φwβ(tβ}tt≥−k1ˆ)[becomes stable], t ←_ _t + 1 ;_ [;] p

We define the approximate information by _I(w; S) where we approximate the expectation_

Ep(S)[θS[⊤][F][ ˆ]θ[θ][S][]][ by][ ¯]θS = _K1_ _Kk=1_ **_θ[ˆ]k[2]_** [=] _K1_ _Kk=1_ _θ[ˆ]1[2],k[, . . .,]_ _K1_ _Kk=1_ _θ[ˆ]D,k[2]_ _⊤.[5]_ In Eq.

[e]

q q q 

(14), the information consists of two major components:P P ∆θ = θ[¯]S **_θ0_** PR[D] and F ˆθ
which can easily cause out-of-memory error due to the high-dimensional matrix product operations. − _∈_ _[∈]_ [R][D][×][D][,]
We therefore hack into FIM to get

_T_ _T_

1 2
_I(w; S) = n∆θ[⊤]_ **_θℓt(θ[ˆ])_** **_θℓ[⊤]t_** [(ˆ]θ) ∆θ = _[n]_ ∆θ[⊤] **_θℓt(θ[ˆ])_** _,_ (15)
" _T_ _t=1_ _∇_ _∇_ # _T_ _t=1_ _∇_

X X h i

e

such that the high dimensional matrix vector product reduces to vector inner product. We encapsulate the algorithm for estimating IIW during vanilla SGD by Algorithm 1.

4 BAYESIAN INFERENCE FOR THE OPTIMAL POSTERIOR

Recall that we designed a new bottleneck on the expected generalization gap drawn from PAC-Bayes
theory in §2, and then derived an approximation of the IIW in §3. The two components of PACBayes IB in Eq. (6) are hence tractable as a learning objective. We give the following lemma on
utilizing it for inference.

5The quadratic mean is closer to the true value than arithmetic mean because of the quadratic term within
the expectation function.


-----

**Fitting**

**Compressing**

**Inflection point**


Figure 1: IIW (up), loss and accuracy (down) of different activation functions (linear, tanh,
ReLU, and sigmoid) NNs. There is a clear boundary between the initial fitting and the compres**sion phases identified by IIW. Meanwhile, the train loss encounters the inflection point that keeps**
decreasing with slower slope. Note that the learning rate is set small (1e-4) except for sigmoid-NN
for the better display of two phases.

**Lemma 3 (Optimal Posterior for PAC-Bayes Information Bottleneck). Given an observed dataset**
_S[∗], the optimal posterior p(w|S[∗]) of PAC-Bayes IB in Eq. (5) should satisfy the following form that_


1

_Z(S)_ _[p][(][w][) exp]_ _−_ _β[1]_



1

_,_ (16)

_Z(S) [exp]_ _−_ _β [1]_ _[U][S][∗]_ [(][w][)]
 


_p(w|S[∗]) =_


_LˆS∗_ (w)


_where US∗_ (w) is the energy function defined as US∗ (w) = L[ˆ]S∗ (w) _β log p(w), and Z(S) is the_
_−_
_normalizing constant._

Please refer to Appendix B for the proof. The reason why we write the posterior in terms of an
exponential form is that it is a typical Gibbs distribution (Kittel, 2004) (also called Boltzmann distribution) with energy function US∗ (w) and temperature β (the same β of PIB appears in Eq. (5)).
Crediting to this formula, we can adopt Markov chain Monte Carlo (MCMC) for rather efficient
Bayesian inference. Specifically, we propose to use stochastic gradient Langevin dynamics (SGLD)
(Welling & Teh, 2011) that has been proved efficient and effective in large-scale posterior inference.
SGLD can be realized by a simple adaption of SGD as

**wk+1 = wk −** _ηkgk +_ 2ηkβεk, (17)

whereased estimate of energy function gradient ηk is step size, εk ∼N (ε|0, ID) is a standard Gaussian noise vector, andU (wk). SGLD can be viewed as a discrete Langevinp **gk is an unbi-**
_∇_
diffusion described by stochastic differential equation (Raginsky et al., 2017; Borkar & Mitter,
1999): dw(t) = _U_ (w(t))dt + 2βdB(t), where _B(t)_ _t_ 0 is the standard Brownian mo_−∇_ _[√]_ _{_ _}_ _≥_
tion in R[D]. The Gibbs distribution π(w) ∝ exp(− _β[1]_ _[U]_ [(][w][))][ is the unique invariant distribution]

of the Langevin diffusion. And, distribution of wt converges rapidly to π(w) when t →∞ with
sufficiently small β (Chiang et al., 1987). Similarly for SGLD in Eq. (17), under the conditions
that _t_ _[η][t][ →∞]_ [and][ P]t[∞] _[η]t[2]_ _k_ _k_

_[→]_ [0][, and an annealing temperature][ β][, the sequence of][ {][w][k][}] _≥[ˆ]_
converges to Gibbs distribution with sufficiently large _k[ˆ]._

[P][∞]

As we assume the oracle prior p(w) = (w **_θ0, Σ0), log p(w) satisfies_**
_N_ _|_

_−_ log p(w) ∝ (w − **_θ0)[⊤]Σ[−]0_** [1][(][w][ −] **_[θ][0][) + log(det Σ][0][)][.]_** (18)

The inference of the optimal posterior is then summarized by Algorithm 2. φη( ) and φβ( ) are

_·_ _·_
learning rate decay and temperature annealing functions, e.g., cosine decay, respectively. Our SGLD
based algorithm leverages the advantage of MCMC such that it is capable of sampling from the optimal posterior even for very complex NNs. Also, it does need to know the groundtruth distribution
of S while still theoretically allows global convergence avoiding local minima. It can be realized
with a minimal adaptation of common auto-differentiation packages, e.g., PyTorch (Paszke et al.,
2019), by injecting isotropic noise in the SGD updates. Please refer to Appendix C for the details of
computation for PIB object.


-----

5 EXPERIMENTS

In this section, we aim to verify the intepretability of the proposed notion of IIW by Eq. (15). We
monitor the information trajectory when training NNs with plain cross entropy loss and SGD for
the sake of activation functions (§5.1), architecture (§5.2), noise ratio (§5.3), and batch size (§5.4).
We also substantiate the superiority of optimal Gibbs posterior inference based on the proposed
Algorithm 2, where PIB instead of plain cross entropy is used as the objective function (§5.5).
We conclude the empirical observations in §5.6 at last. Please refer to Appendix D for general
experimental setups about the used datasets and NNs.


5.1 INFORMATION WITH DIFFERENT ACTIVATION FUNCTIONS

We train a 2-layer MLP (784-512-10) with
plain cross-entropy loss by Adam on the _×10[−][2]_ IIW of 1-layer MLP _×10[−][2]_ IIW of 2-layer MLP
MNIST dataset, meanwhile monitor the trajec- 1.0
tory of the IIW I(w; S). Results are illustrated IIW 1 IIW 0.5
in Fig. 1 where different activation functions layer 1
(linear, tanh, ReLU, and sigmoid) are 0 0 1000 2000 0.0 0 200 400

layer 1
layer 2

layer 1

testified. We identify that there is a clear bound- iteration iteration
ary between fitting and compression phases for _×10[−][3]_ IIW of 3-layer MLP _×10[−][4]_ IIW of 4-layer MLP
all of them. For example, for the linear ac- layer 1
tivation function on the first column, the IIW IIW 2 layer 3 IIW 5
_I(w; S) surges within the first several itera-_
tions then drops slowly during the next itera- 0 100 200 300 400 0 100 200 300 400

|Col1|Col2|
|---|---|


layer 1
layer 2
layer 3

tions. Simultaneously, we could see that the iteration iteration
training loss reduces sharply at the initial stage,

Figure 2: Information compression with varying

then keeps decreasing during the information

number of layers (1, 2, 3, and 4) for ReLU MLPs.

compression. At the last period of compres
All follow the general trend of fitting to compres
sion, we witness the information fluctuates near

sion phase transition. And, deeper layers can ac
zero with the recurrence of memorization phe
celerate both the fitting and compressing phases.

nomenon (IIW starts to increase). This implies
that further training is causing over-fitting. IIW shows great universality on representing the information compression of NNs.


5.2 INFORMATION WITH DEEPER AND WIDER ARCHITECTURE

Having identified the phase transition of the 2-layer MLP corresponding to IIW I(w; S), we test it
under more settings: different architectures and different batch sizes. For the architecture setting,
we design MLPs from 1 to 4 layers. Results are shown in Fig. 2. The first and the last figures show
the information trajectory of the 1-layer/4-layer version of MLP-Large (784-10/784-512-100-80-10)
where clear two-phase transitions happen in all these MLPs.

The 1-layer MLP is actually a softmax regression model. It can be identified that this model fits
and compresses very slowly w.r.t. IIW compared with deeper NNs. This phenomenon demonstrates
that deep models have overwhelmingly learning capacity than shallow models, because deep layers
can not only boost the memorization of data but also urges the model to compress the redundant
information to gain better generalization ability. Furthermore, when we add more layers, the fitting
phase becomes shorter. Specifically, we observe the incidence of overfitting at the end of the 4-layer
MLP training as IIW starts to increase.

We also examine how IIW explains the generalization w.r.t. the number of hidden units, a.k.a. the
width of NNs, by Fig. 3. We train a 2-layer MLP without any regularization on MNIST. The left
panel shows the training and test errors for this experiment. Notably, the difference of test and train
acc can be seen an indicator of the generalization gap in Eq. (4). IIW should be aligned to this gap
by definition. While 32 units are (nearly) enough to interpolate the training set, more hidden units
still achieve better generalization performance, which illustrates the effect of overparameterization.
In this scenario, weights ℓ2-norm keeps increasing with more units while IIW decays, similar to the
test error. We identify that more hidden units do not render much increase of IIW, which is contrast


-----

5 _×10[−][2][ IIW to # of random-label data]_


_×10[−][2]_ IIW to corrupted labels

0.0 0.2 0.4 0.6 0.8 1.0

IIW
test acc
train acc

noise ratio


Train/Test Acc to Width

number of hidden units


1e 4 IIW/l2 to Width


1.0

0.8

0.6

0.4

0.2

0.0


1.0



1.0

0.9


2.00

1.75

1.50

1.25

1.00

0.75


1.4

1.2



0.8


32 512 4096 8192

_×10_

IIW
_ℓ2-norm_

number of hidden units


010000 20000 30000 400000.0

0.96 0.93 0.92 0.90

train acc
test acc
IIW

0.10 0.10 0.10 0.10

data size


train acc
test acc

32 512 4096 8192


Figure 3: Left: Training and test accuracy
w.r.t. # units; Right: Complexity measure
(IIW and ℓ2-norm) w.r.t. # units. Blue dash line
shows the gap between train/test acc (generalization gap). We find ℓ2-norm keeps increasing with more hidden units. Instead, IIW keeps
pace with the generalization gap: the larger the
gap, the larger the IIW.


Figure 4: Left: IIW, train, and test accuracy
when noise ratio in labels changes. IIW rises
when noise ratio grows. Right: IIW with varying size of random-label data. Test acc keeps
constant while train acc decays. Hence, more
data causes lower IIW because of the shrinking
gap between the train and test accuracy.

IIW of VGG


train/test acc to batch size

16 64 256

train acc
test acc

number of batch size


_×10[−][5]_ IIW to batch size

4 16 64 256

IIW

number of batch size


1.0

0.9


0.0075

0.0050

0.0025

0.0000


0.8


9 10


vanilla
l2
dropout
PIB


epoch


Figure 5: Left: Training and test accuracy
w.r.t. # batch size; Right: IIW w.r.t. # batch
size. We find IIW keeps pace with the generalization gap: the larger the gap, the larger the
IIW. From IIW we can specify that 16 is the
best which reaches the least generalization gap
without the need of having the model tested.


Figure 6: The tracked IIW of the VGG net during the training by four ways: vanilla, ℓ2-norm
regularization, dropout, and PIB training. We
can identify that: first, all of them still follow a
fitting-compressing paradigm specified by IIW;
second, vanilla VGG reaches the largest IIW far
above the others; third, PIB regularizes IIW directly thus yielding the smallest IIW.


to the intuition that wider NNs always have larger information complexity. More importantly, we
find IIW is consistent to the generalization gap on each width.

5.3 RANDOM LABELS VS. TRUE LABELS


According to the PAC-Bayes theorem, the IIW is a promising measure to explain/predict the generalization capability of NNs. NNs are often over-parameterized thus can perfectly fit even the
random labels, obviously without any generalization capability. For example, 2-layer MLP has
15, 728, 640 parameters that are much larger than the sample number of CIFAR-10 (50,000). That
causes the number of parameters an unreliable measure of NN complexity in overparameterization
settings (Neyshabur et al., 2015). Alternatively, ℓ2-norm is often used as a complexity measure to
be imposed on regularizing model training in practices.

We investigate the model trained with different levels of label corruption, as shown by the left panel
of Fig. 4. We train a 2-layer MLP on CIFAR-10 and find that the increasing noise causes sharp test
acc decay while train acc does not change much. Meanwhile, IIW keeps growing with the fall of
test acc and expansion of generalization gap. This demonstrates IIW’s potential in identifying the
noise degree in datasets or the mismatch between the test and train data distributions.

We further build a random-label CIFAR-10, results are on the right of Fig. 4. Although the model
can still nearly interpolate the train data, with the rise of random-label data, the model keeps 10%
test acc but has less train acc, which renders larger generalization gap. This phenomenon is also
captured by IIW.

5.4 INFORMATION IN WEIGHTS W.R.T. BATCH SIZE


We also consider how batch size influences IIW and generalization gap. Recent efforts on bounding
_I(w; S) of iterative algorithms (e.g., SGD and SGLD) (Mou et al., 2018; Pensia et al., 2018) imply_
that the variance of gradients is a crucial factor. For the ultimate case where batch size equals


-----

Table 1: Test performance of the proposed PIB algorithm compared with two other common regularization techniques: ℓ2-norm and dropout, on VGG-net (Simonyan & Zisserman, 2014). The 95%
confidence intervals are shown in parentheses. Best values are in bold.

|Test ACC (%)|CIFAR10|CIFAR100|STL10|SVHN|
|---|---|---|---|---|
|vanilla SGD SGD+ℓ -norm 2 SGD+dropout SGD+PIB|77.03(0.57) 77.13(0.53) 78.95(0.60) 80.19(0.42)|52.07(0.44) 50.84(0.71) 52.34(0.66) 56.47(0.62)|54.31(0.65) 55.30(0.68) 56.35(0.78) 58.83(0.75)|93.57(0.67) 93.60(0.68) 93.61(0.76) 93.88(0.88)|


**Test ACC (%)** **CIFAR10** **CIFAR100** **STL10** **SVHN**

vanilla SGD 77.03(0.57) 52.07(0.44) 54.31(0.65) 93.57(0.67)

SGD+ℓ2-norm 77.13(0.53) 50.84(0.71) 55.30(0.68) 93.60(0.68)

SGD+dropout 78.95(0.60) 52.34(0.66) 56.35(0.78) 93.61(0.76)

SGD+PIB **80.19(0.42)** **56.47(0.62)** **58.83(0.75)** **93.88(0.88)**


full sample size, the gradient variance is zero, and the model is prone to over-fitting grounded on
empirical observations. When batch size equals one, the variance becomes tremendously large.

We conjecture there is an optimal batch size that reaches the minimum generalization gap, in other
word, the minimum IIW. This conjecture is raised on our empirical findings, displayed in Fig. 5
where we test IIW on model with varying batch size. Each model is updated with the same total
number of iterations and the same learning rate. We identify that when batch size is 16, the model
reaches the best test acc and the least generalization gap, which means this optimal batch size should
fall into (4,16) or (16, 64). On the left, the model reaches the minimum IIW when batch size is 16.

5.5 BAYESIAN INFERENCE WITH VARYING ENERGY FUNCTIONS

To confirm the superiority of the proposed PIB in §4, we compare it with vanilla SGD and two
widely used regularizations: ℓ2-norm and dropout. We train a large VGG network (Simonyan &
Zisserman, 2014) on four open datasets: CIFAR10/100 (Krizhevsky et al., 2009), STL10 (Coates
et al., 2011), and SVHN (Netzer et al., 2011), as shown in Table 1, where we find PIB consistantly
outperforms the baselines. We credit the improvement to the explicit consideration of information
regularization during the training, which forces the model to forget the training dataset to regularize
the generalization gap. This is verified by Fig. 6 where PIB helps restrict IIW in the lowest level.
Please refer to Appendix D for experimental setups.

5.6 SUMMARY OF EXPERIMENTS

We made the following observations from our experiments:

1. We can clearly identify the fitting-compression phase transition during training through our
new information measure, i.e., information stored in weights (IIW). Unlike the representationbased information measure I(X; Z), IIW applies to various activation functions including ReLU,
sigmoid, tanh, and linear.

2. We further identify that the phase transition applies to deeper and wider architecture. More
importantly, deeper model is proved to reach faster fitting and compression than shallow models.

3. Unlike ℓ2-norm of weights that rise together with wider models, IIW better illustrates the true
model complexity and its generalization gap.

4. IIW can explain the performance drop w.r.t. the degree of label noise. Also, IIW can even identify
the generalization gap for models learned from random labels.

5. There might exist an optimal batch size for the minimum generalization gap, which is empirically
demonstrated by our experiments.

6. Adopting SGD based on the energy function derived from PAC-Bayes IB enables good inference
to the optimal posterior of NNs. This works for practical large networks in the literature.

6 CONCLUSION

In this paper, we proposed PAC-Bayes information bottleneck and the corresponding algorithm for
measuring information stored in weights of NNs and training NNs with information principled regularization. Empirical results show the universality of our information measure on explaining NNs,
which sheds light on understanding NNs through information bottleneck. We aim to further investigate its performance and develop this into practical NNs for production in the future.


-----

REFERENCES

Alessandro Achille and Stefano Soatto. Emergence of invariance and disentanglement in deep representations.
_The Journal of Machine Learning Research, 19(1):1947–1980, 2018a._

Alessandro Achille and Stefano Soatto. Information dropout: Learning optimal representations through noisy
computation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(12):2897–2905, 2018b.

Vivek S Borkar and Sanjoy K Mitter. A strong approximation theorem for stochastic recursive algorithms.
_Journal of Optimization Theory and Applications, 100(3):499–513, 1999._

Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, and
Alexander Lerchner. Understanding disentangling in β-VAE. arXiv preprint arXiv:1804.03599, 2018.

Nicholas Chamandy, Omkar Muralidharan, Amir Najmi, and Siddartha Naidu. Estimating uncertainty for
massive data streams. Technical report, Google, 2012.

Tzuu-Shuh Chiang, Chii-Ruey Hwang, and Shuenn Jyi Sheu. Diffusion for global optimization in R[n]. SIAM
_Journal on Control and Optimization, 25(3):737–753, 1987._

Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature
learning. In International Conference on Artificial Intelligence and Statistics, pp. 215–223, 2011.

R Dennis Cook and Sanford Weisberg. Residuals and influence in regression. New York: Chapman and Hall,
1982.

Bin Dai, Chen Zhu, Baining Guo, and David Wipf. Compressing neural networks using the variational information bottleneck. In International Conference on Machine Learning, pp. 1135–1144. PMLR, 2018.

Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets.
In International Conference on Machine Learning, pp. 1019–1028. PMLR, 2017.

Gintare Karolina Dziugaite and Daniel M Roy. Data-dependent PAC-Bayes priors via differential privacy. In
_Advances in Neural Information Processing Systems, pp. 8440–8450, 2018._

Gintare Karolina Dziugaite, Kyle Hsu, Waseem Gharbieh, Gabriel Arpino, and Daniel Roy. On the role of data
in PAC-Bayes. In International Conference on Artificial Intelligence and Statistics, pp. 604–612. PMLR,
2021.

Bradley Efron. Bootstrap methods: another look at the jackknife. In Breakthroughs in Statistics, pp. 569–593.
Springer, 1992.

Ziv Goldfeld, Ewout van den Berg, Kristjan Greenewald, Igor Melnyk, Nam Nguyen, Brian Kingsbury, and
Yury Polyanskiy. Estimating information flow in deep neural networks. In International Conference on
_Machine Learning, 2019._

Anirudh Goyal, Riashat Islam, DJ Strouse, Zafarali Ahmed, Hugo Larochelle, Matthew Botvinick, Yoshua Bengio, and Sergey Levine. InfoBot: transfer and exploration via the information bottleneck. In International
_Conference on Learning Representations, 2019._

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint_
_arXiv:1412.6980, 2014._

Charles Kittel. Elementary statistical physics. Courier Corporation, 2004.

Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In International
_Conference on Machine Learning, pp. 1885–1894. PMLR, 2017._

Artemy Kolchinsky, Brendan D Tracey, and Steven Van Kuyk. Caveats for information bottleneck in deterministic scenarios. In International Conference on Learning Representations, 2018.

Artemy Kolchinsky, Brendan D Tracey, and David H Wolpert. Nonlinear information bottleneck. Entropy, 21
(12):1181, 2019.

Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical
report, University of Toronto, 2009.

Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.


-----

Xiang Lisa Li and Jason Eisner. Specializing word embeddings (for parsing) by information bottleneck. In
_Conference on Empirical Methods in Natural Language Processing, pp. 2744–2754, 2019._

Tengyuan Liang, Tomaso Poggio, Alexander Rakhlin, and James Stokes. Fisher-Rao metric, geometry, and
complexity of neural networks. In International Conference on Artificial Intelligence and Statistics, pp.
888–896. PMLR, 2019.

James Martens. New insights and perspectives on the natural gradient method. Journal of Machine Learning
_Research, 21:1–76, 2020._

Wenlong Mou, Liwei Wang, Xiyu Zhai, and Kai Zheng. Generalization bounds of SGLD for non-convex
learning: Two theoretical viewpoints. In Conference on Learning Theory, pp. 605–638. PMLR, 2018.

Jeffrey Negrea, Mahdi Haghifam, Gintare Karolina Dziugaite, Ashish Khisti, and Daniel M Roy. Informationtheoretic generalization bounds for SGLD via data-dependent estimates. arXiv preprint arXiv:1911.02151,
2019.

Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading digits in
natural images with unsupervised feature learning. 2011.

Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of
implicit regularization in deep learning. In International Conference on Learning Representations Workshop,
2015.

Ziqi Pan, Li Niu, Jianfu Zhang, and Liqing Zhang. Disentangled information bottleneck. _arXiv preprint_
_arXiv:2012.07372, 2020._

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,
Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative style, high-performance deep
learning library. In Advances in Neural Information Processing Systems, pp. 8024–8035, 2019.

Ankit Pensia, Varun Jog, and Po-Ling Loh. Generalization error bounds for noisy, iterative algorithms. In IEEE
_International Symposium on Information Theory, pp. 546–550. IEEE, 2018._

Rigollet Philippe. High-Dimensional Statistics, chapter 1. Massachusetts Institute of Technology: MIT OpenCourseWare, 2015.

Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic gradient
Langevin dynamics: a nonasymptotic analysis. In Conference on Learning Theory, pp. 1674–1703. PMLR,
2017.

Omar Rivasplata, Emilio Parrado-Hern´andez, John Shawe-Taylor, Shiliang Sun, and Csaba Szepesv´ari. PACBayes bounds for stable algorithms with instance-dependent priors. In Advances in Neural Information
_Processing Systems, pp. 9234–9244, 2018._

Andrew M Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Brendan D Tracey, and
David D Cox. On the information bottleneck theory of deep learning. Journal of Statistical Mechanics:
_Theory and Experiment, 2019(12):124020, 2019._

Ravid Shwartz-Ziv and Alexander A Alemi. Information in infinite ensembles of infinitely-wide neural networks. In Symposium on Advances in Approximate Bayesian Inference, pp. 1–17. PMLR, 2020.

Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information. arXiv
_preprint arXiv:1703.00810, 2017._

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.
_arXiv preprint arXiv:1409.1556, 2014._

Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In IEEE Informa_tion Theory Workshop, pp. 1–5. IEEE, 2015._

Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv preprint
_physics/0004057, 2000._

Qi Wang, Claire Boudreau, Qixing Luo, Pang-Ning Tan, and Jiayu Zhou. Deep multi-view information bottleneck. In Proceedings of the SIAM International Conference on Data Mining, pp. 37–45. SIAM, 2019.

Zifeng Wang, Xi Chen, Rui Wen, Shao-Lun Huang, Ercan E. Kuruoglu, and Yefeng Zheng. Information
theoretic counterfactual learning from missing-not-at-random feedback. In Advances in Neural Information
_Processing Systems, 2020a._


-----

Zifeng Wang, Rui Wen, Xi Chen, Shao-Lun Huang, Ningyu Zhang, and Yefeng Zheng. Finding influential
instances for distantly supervised relation extraction. arXiv preprint arXiv:2009.09841, 2020b.

Zifeng Wang, Hong Zhu, Zhenhua Dong, Xiuqiang He, and Shao-Lun Huang. Less is better: Unweighted
data subsampling via influence function. In AAAI Conference on Artificial Intelligence, volume 34, pp.
6340–6347, 2020c.

Max Welling and Yee W Teh. Bayesian learning via stochastic gradient Langevin dynamics. In International
_Conference on Machine Learning, pp. 681–688, 2011._

Tailin Wu, Hongyu Ren, Pan Li, and Jure Leskovec. Graph information bottleneck. _arXiv preprint_
_arXiv:2010.12811, 2020._

Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learning algorithms. In Advances in Neural Information Processing Systems, pp. 2521–2530, 2017.

Jingwei Zhang, Tongliang Liu, and Dacheng Tao. An information-theoretic view for deep learning. arXiv
_preprint arXiv:1804.09060, 2018._


-----

A PROOF OF LEMMA A.3

Before the proof of Lemma A.3, we need to introduce a lemma by Martens (2020) as:
**Lemma A.1 (Approximation of Hessian Matrix in NNs (Martens, 2020)). The Hessian matrix of**
_NNs on a local minimum_ **_θ[ˆ] can be decomposed based on Fisher information matrix as_**

_n_ _C_

**H ˆθ** [=][ F][ ˆ]θ [+ 1] [ **yˆ[ℓ]i[(ˆ]θ)]cH[f** ]c _,_ (A.1)

_n_ _∇_

_i=1_ _c=1_

X X

_where C is the total number of classes, ˆy is the output (or prediction) of the network given input x,_
_and H[f_ ]c is the Hessian of the c-th component of ˆy. Specifically, for a well-trained NN, we could
_have ∇yˆ[ℓ]i[(ˆ]θ) ≃_ 0 thus Hθ ˆ _[≃]_ **[F][ ˆ]θ[.]**

We further introduce a lemma of Poisson bootstrapping as
**Lemma A.2 (Poisson Bootstrapping (Efron, 1992; Chamandy et al., 2012)). Given an infinite num-**
_ber of samples, the bootstrap resampling weight ξ has the property that limn→∞_ _Binomial_ _n,_ _n[1]_ =

_Poisson(1)._ _This approximation becomes precise in practice when n_ 100. _Also, we know_
_≥_   
E[ξi] = 1 and Var[ξi] = 1 by the definition of Poisson distribution when n is large enough.

When bootstrap resampling from dataset S, each individual sample Zi = (Xi, Yi) has a probability
of _n[1]_ [being picked, causing the weight][ ξ][i][ to follow a binomial distribution as][ ξ][i][ ∼] [Binomial] _n,_ _n[1]_ .

As a result, all weights ξ follow a multinomial distribution as ξ Multinomial _n,_ _n[1]_ _[,][ 1]n_ _[, . . .,][ 1]n_
_∼_   

with the total number of samples constrained to be n. When it comes to big data, i.e., n is pro
  

hibitively large, this multinomial resampling thus becomes rather slow. Based on Lemma A.2, we
can now start to prove our lemma of approximating oracle prior covariance.
**Lemma A.3 (Approximation of Oracle Prior Covariance). Given the definition of influence functions**
_(Lemma 1) and Poisson bootstrapping (Lemma A.2), the covariance matrix of the oracle prior can_
_be approximated by_

_K_

Σ0 = Ep(S) (θS − **_θ0)(θS −_** **_θ0)[⊤][]_** _≃_ _K[1]_ _k=1_ **_θˆξk −_** **_θ[ˆ]_** **_θˆξk −_** **_θ[ˆ]_** _⊤_ _≃_ _n[1]_ **[H][−]θˆ** [1][F][ ˆ]θ[H]θ[−]ˆ [1] _≃_ _n[1]_ **[F][−]θˆ** [1][,]
 X     (A.2)

_where F ˆθ_ _[is Fisher information matrix (FIM); we omit the subscript][ S][ of][ ˆ]θS and_ **_θ[ˆ]S,ξ for notation_**
_conciseness, and ξ[k]_ _is the bootstrap resampling weight in the k-th experiment._


_Proof. Recall that in the k-th bootstrap resampling process, the loss function is reweighted by ξk =_
(ξk,1, ξk,2, . . ., ξk,n)[⊤]. Also, we have an influence matrix Ψ = (ψ1, ψ2, . . ., ψn)[⊤] _∈_ R[n][×][D]. The
original risk minimizer on the full dataset S is

_n_

1

**_θˆS ≜_** argmin _ℓi(θ),_ (A.3)
**_θ_** _n_

_i=1_

X

and the reweighted empirical risk minimizer (after bootstrapping) is defined by

_n_

1

**_θˆS,ξ ≜_** argmin _ξiℓi(θ),_ (A.4)
**_θ_** _n_

_i=1_

X

where we omit the subscript k for the sake of conciseness. Given the definition of influence function
from Lemma 1, the difference between the two risk minizers above, **_θ[ˆ]S and_** **_θ[ˆ]S,ξ, can be written as_**

**_θˆS,ξ_** **_θS_** (ξi 1)ψi = [1] (A.5)
_−_ [ˆ] _≃_ _n[1]_ _i=1_ _−_ _n_ [Ψ][⊤][(][ξ][ −] **[1][)][.]**

X

As a result, the oracle prior can be transformed as

Σ0 ≃ Ep(S) (θ[ˆ]S,ξ − **_θ[ˆ]S)(θ[ˆ]S,ξ −_** **_θ[ˆ]S)[⊤][i]_** (A.6)
h

1 1 _⊤[#]_

Ep(S) (A.7)
_≃_ _n_ [Ψ][⊤][(][ξ][ −] **[1][)]** _n_ [Ψ][⊤][(][ξ][ −] **[1][)]**

"   

= [1] Ψ[⊤](ξ **1)(ξ** **1)[⊤]Ψ** _._ (A.8)

_n[2][ E][p][(][S][)]_ _−_ _−_
 


**_θˆS ≜_** argmin


**_θˆS,ξ ≜_** argmin


**_θˆS,ξ_** **_θS_**
_−_ [ˆ] _≃_ _n[1]_


-----

Furthermore, based on the definition of influence function, we know Ψ[⊤]1 = 1[⊤]Ψ = 0. The term
in Eq. (A.8) can be further writen to

Σ0 Ψ[⊤](ξ **1)(ξ** **1)[⊤]Ψ** = [1] (A.9)

From Lemma A.2 we know ≃ _n[1][2][ E] E[p][(][[S]ξ[)]i] = 1_ and Var − [ξ −i] = 1 when _nn ≥[2][ Ψ]100[⊤][E]. We also know that[p][(][S][)][[][ξξ][⊤][]Ψ][.]_

Ep(S)[ξi]Ep(S)[ξj] = 1, _i_ = j,
Ep(S)[ξiξj] = _̸_
Ep(S)[ξi[2][] =][ Var][[][ξ][i][] +][ E][2][[][ξ][i][] = 2][,] _i = j._


This gives rise to the final solution that

Ψ[⊤]Ep(S)[ξξ[⊤]]Ψ = Ψ[⊤] [ ]11[⊤] + In Ψ (A.10)

_n_



= **_ψiψi[⊤]_** (A.11)

_i=1_

Xn

= **H[−]θˆ** [1] **_θ)ℓ[⊤]i_** [(ˆ]θ)H[−]θˆ [1] (A.12)

_i=1_ _[∇][θ][ℓ][i][(ˆ]_

X


= nH[−]ˆ [1]


_∇θℓi(θ[ˆ])∇θℓ[⊤]i_ [(ˆ]θ)
_i=1_

X


**H[−]ˆ** [1] (A.13)


= nH[−]θˆ [1][F][ ˆ]θ[H][−]θˆ [1] (A.14)

_≃_ _nF[−]θˆ_ [1][.] (A.15)

**In in Eq. (A.10) is an identity matrix with size n × n; Eq. (A.11) is true by re-applying the property**
of influence functions that Ψ[⊤]1 = 1[⊤]Ψ = 0; and Eq. (A.15) is achieved by the result from Lemma
A.1. Summarizing all the above results, the oracle prior covariance can be approximated by


Σ0
_≃_ _n[1][2]_


= n[1] **[F]θ[−]ˆ** [1][.] (A.16)


_nF[−]ˆ_ [1]


B PROOF OF LEMMA 3

**Lemma B.1 (Optimal Posterior for PAC-Bayes Information Bottleneck). Given an observed dataset**
_S[∗], the optimal posterior p(w|S[∗]) of PAC-Bayes IB in Eq. (5) should satisfy the following form that_


1

_Z(S)_ _[p][(][w][) exp]_ _−_ _β[1]_



1

_,_ (B.1)

_Z(S) [exp]_ _−_ _β[1]_ [(][U][S][∗] [(][w][))]
 


_p(w|S[∗]) =_


_LˆS[∗]_ (w)


_where US∗_ (w) is the energy function defined by

_US∗_ (w) = L[ˆ]S∗ (w) _β log p(w),_ (B.2)
_−_

_and Z(S) is the normalizing constant._

_Proof. Recap that the PAC-Bayes information bottleneck in Eq. (5) is_

min (B.3)
_p(w|S)_ _[L][PIB][ =][ L][S][(][w][) +][ βI][(][w][;][ S][)][.]_

Given an observed dataset S[∗], our object of interest is to find the optimal posterior p(w|S[∗]) that
minimizes the LPIB. Consider a constraint of posterior distribution that
_p(w|S)dw = 1, ∀S ∼_ _p(X, Y )[⊗][n],_ (B.4)
Z

we can formulate the problem by


min
_p(w|S)_ _[L][PIB][ =][ L][S][(][w][) +][ βI][(][w][;][ S][)][,]_ (B.5)

s.t. _p(w|S)dw = 1._
Z


-----

A Lagrangian can hence be built to relax the above optimization problem by


min PIB = LS(w) + βI(w; S) + _αS_
_p(w_ _S)_ _L_
_|_ Z
e


(p(w|S) − 1) dwdS


(B.6)


_p(w_ _S)_ _LˆS(w)_ _dw + β_ _p(w, S) [log p(w_ _S)_ log p(w)] dwdS
_|_ _|_ _−_
h i Z


_αS_


(p(w|S) − 1) dwdS,


with α = _αS_ _S_ _p(X, Y )[⊗][n]_ corresponding to Lagrange multipliers; we denote the empirical
risk by _L[ˆ]S {(w) =|∀_ _∼n[1]_ _ni=1_ _[ℓ][i][(][w][)][.]}_

Differentiating _LPIB w.r.t.P_ _p(w|S[∗]) results in_

_∇[e]p(w|S∗)LPIB = L[ˆ]S[∗]_ (w) + β log p(w|S[∗]) − _β log p(w) + β + αS[∗]_ _._ (B.7)

Setting ∇p(w|S∗)LPIB = 0[e] and solving for p(w|S[∗]) yields

log p(w _S[∗]) =_ _LˆS∗_ (w) + log p(w) 1

[e] _|_ _−_ _β[1]_ _−_ _−_ _[α]β[S][∗]_

(B.8)

_p(w_ _S[∗]) = p(w) exp_ _LˆS∗_ (w) exp 1 _._
_|_ _−_ _β[1]_ _−_ _−_ _[α]β[S][∗]_
   

The second exponential term exp 1 _β_ is the partition function that normalizes the poste_−_ _−_ _[α][S][∗]_

rior distribution. Denoting the normalization term asn o _Z(S), we hence obtain the optimal posterior_
solution as


1
_p(w_ _S[∗]) =_ _LˆS[∗]_ (w)
_|_ _Z(S)_ _[p][(][w][) exp]_ _−_ _β[1]_

  (B.9)

1
= _LˆS∗_ (w) _β log p(w)_ _._

_Z(S) [exp]_ _−_ _β[1]_ _−_
 h i[]

COMPUTATION FOR PIB OBJECT


Our Alg. 2 presents the training process based on the proposed PIB objective function. It differs
from vanilla SGD on two aspects: (1) the objective function (also called energy function U ) consists
of the negative log-likelihood plus a regularization term log p(w) (line 2); (2) the parameter w gets
update by the gradient of the energy function plus an isotropic Gaussian noise (line 3). Since (2)
has no additional computational cost, the major change is the regularization term log p(w) in (1),
described in Eq. (18) as

_−_ log p(w) ∝ (w − **_θ0)[⊤]Σ[−]0_** [1][(][w][ −] **_[θ][0][) + log(det Σ][0][)][.]_** (C.1)

The first term is just matrix-vector product based on the approximation of Σ0 in Eq. (13). And the
second term can be written by


log λi, (C.2)
_i=1_

X


log(det Σ0) =


where λi is the eigenvalue of Σ0, which can be obtained by efficient eigen-decomposition techniques.

D EXPERIMENTAL PROTOCOL

All experiments are conducted on MNIST (LeCun et al., 1998) or CIFAR-10 (Krizhevsky et al.,
2009). We design two multi-layer perceptron (MLPs): MLP-Small and MLP-Large, where MLPSmall is a two-layer NN, i.e., 784(3072)-512-10, and MLP-Large is a five-layer NN, i.e., 784(3072)100-80-60-40-10. The number of input units is 784 with permutation MNIST inputs or 3072 with


-----

permutation CIFAR-10 inputs. For the performance comparison in §5.5, we use a reduced version
of VGG-Net where two blocks are cut due to the memory constraint. In the general setting, we pick
Adam optimizer (Kingma & Ba, 2014) to accelerate the convergence of NN training. We use one
RTX 3070 GPU for all experiments.

Specifically for the Bayesian inference experiment, the batch size is picked within
8, 16, 32, 64, 128, 256, 512 ; learning rate is in 1e[−][4], 1e[−][3], 1e[−][2], 1e[−][1] ; weight decay of ℓ2_{_ _}_ _{_ _}_
norm is in {1e[−][3], 1e[−][4], 1e[−][5], 1e[−][6]}; noise scale of SGLD is in {1e[−][4], 1e[−][6], 1e[−][8], 1e[−][10]}; β of
PAC-Bayes IB is in {1e[−][1], 1e[−][2], 1e[−][3]}; and the dropout rate is fixed as 0.1 for the dropout regularization.


-----

