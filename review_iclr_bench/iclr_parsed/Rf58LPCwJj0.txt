## OPTIMAL REPRESENTATIONS FOR COVARIATE SHIFT

**Yangjun Ruan[∗]** [12], Yann Dubois[∗] [2], Chris J. Maddison[12]

1University of Toronto & 2Vector Institute
{yjruan,yanndubois,cmaddis}@cs.toronto.edu

ABSTRACT

Machine learning systems often experience a distribution shift between training
and testing. In this paper, we introduce a simple variational objective whose optima
are exactly the set of all representations on which risk minimizers are guaranteed to
be robust to any distribution shift that preserves the Bayes predictor, e.g., covariate
shifts. Our objective has two components. First, a representation must remain
discriminative for the task, i.e., some predictor must be able to simultaneously
minimize the source and target risk. Second, the representation’s marginal support
needs to be the same across source and target. We make this practical by designing
self-supervised objectives that only use unlabelled data and augmentations to train
robust representations. Our objectives give insights into the robustness of CLIP, and
further improve CLIP’s representations to achieve SOTA results on DomainBed.

1 INTRODUCTION

It is hard to build machine learning (ML) systems that are robust to distribution shifts between a
source (train) and target (test) domain. One promising approach to domain generalization (DG)
is learning robust representations from which predictors trained on source must perform well on
target. In practice, however, no current DG methods for learning representation uniformly outperform
empirical source-risk minimizers (ERM) (Gulrajani & Lopez-Paz, 2021). Furthermore, our theoretical
understanding of DG is still lacking. Specifically, while previous work have studied properties that
would or would not imply robust representations (Ben-David et al., 2007; 2010a; Zhao et al., 2019;
Johansson et al., 2019), the minimal set of achievable requirements for perfect DG is not yet known.

We introduce the first, simple, variational objective whose optima are exactly the set of all representations on which source risk minimizers are guaranteed to generalize across distribution shifts that
preserve the Bayes predictor. We work in an idealized DG (IDG) setting; we assume that a learner has
access to the source population risk. Our variational characterization implies that it is both sufficient
and necessary for optimal IDG that a representation: (a) remains discriminative for the learning task,
i.e., there must exist predictors from the representation to the labels that can simultaneously minimize
_both source and target risk; and (b) keeps the support of its marginal distribution invariant to shifts._

This means that any optimal representation learning method must seek discriminative information
about the target. Even worse, we prove that without access to some knowledge about the target, any
representation learning algorithm cannot uniformly (over all target domains) outperform a constant
representation, which may explain why DG methods struggle to outperform ERM.

We show, in theory and practice, how to overcome these challenges using only a large set of unlabeled
examples and particular data augmentations that retain all discriminative information but minimal
domain-specific information. Text descriptions of images are examples of such augmentations, as they
are informative for many downstream classification tasks, but they remove a lot of domain-specific
information. With such augmentations, we design practical self-supervised learning (SSL) objectives
for learning robust representations. Our objectives give insights into the robustness of CLIP (Radford
et al., 2021) over other SSL methods, and lead to improved CLIP-based representations that achieve
state-of-the-art (SOTA) results on DomainBed (Gulrajani & Lopez-Paz, 2021). To summarize, we:

-  provide minimal sufficient objectives whose optima achieve optimal DG under covariate shift;

-  prove that it is impossible to learn useful representations without accessing target information;

-  provide practical objectives to learn optimally robust representations using specific augmentations;

-  obtain SOTA results on typical domain generalization benchmarks.[1]

_∗Authors contributed equally._
[1Our implementation is released at https://github.com/ryoungj/optdom.](https://github.com/ryoungj/optdom)


-----

2 BACKGROUND: DOMAIN GENERALIZATION AND REPRESENTATIONS

We are interested in predictions that are robust across distribution shifts. We formalize this using
domain generalization (DG) language. Given a distribution pX,Y | ds over inputs x ∈X and labels
_y_ from the source domain ds, we select a predictor f : Γ. The predictions γ Γ
could for example be labels or distributions over labels. Despite being selected on the source domain, ∈Y _∈D_ _X →_ _∈_
we would like f to achieve a small expected risk with respect to a loss function ℓ : Γ R 0,
_Y ×_ _→_ _≥_

R[d]f [[][Y][ |][ X][] :=][ E][p]X,Y | d [[][ℓ][(][Y, f] [(][X][))]][,] (1)

on a distribution pX,Y _d from a target domain d = dt_, which is somehow related to ds.
_|_ _∈D_

A common strategy for DG is to learn robust representations, which splits the problem into two.
First, learn an encoder pZ | X, which maps inputs X to representations Z. Then, learn a predictor
_h : Z →_ Γ from representations Z to labels Y using standard risk minimization. The goal is to
design a robust representation Z, so that predictors h trained to minimize the source risk R[d]h[s] [[][Y][ |][ Z][]]
also achieve low target risk R[d]h[t] [[][Y][ |][ Z][]][. Many methods have been proposed to try to learn such][ Z][,]
e.g., by enforcing domain invariance of the marginal pZ _d (e.g., Ganin et al., 2016). Still, many of_
_|_
these proposals are not sound (Zhao et al., 2019; Johansson et al., 2019). Furthermore, they rarely
outperform source empirical risk minimization (ERM) in practice (Gulrajani & Lopez-Paz, 2021).


3 OPTIMAL REPRESENTATIONS FOR DOMAIN GENERALIZATION

To separate domain generalization from finite sample generalization, we consider an idealized DG
(IDG), where the predictor h is selected on the source population risk rather than empirical risk. We
assume sample spaces X _, Z, Y, D are discrete; formal statements and proofs are in Appxs. A and B._

3.1 DEFINING OPTIMAL REPRESENTATIONS FOR IDEALIZED DOMAIN GENERALIZATION

We want to evaluate the quality of a representation Z of X. In our IDG, the learner is given a random
source Ds; she selects any source risk minimizer; and is scored according to her risk on a random
target domain Dt. To give uniform guarantees while reflecting the uncertainty over the source-target
pair (Ds, Dt), we measure the quality of Z as the expected risk of the learner’s worst-case choice.

**Definition. The idealized domain generalization risk (IDG risk) of an encoder pZ** _X is the expected_
_|_
(over domains) worst-case (over source risk minimizers) target risk, i.e.,


sup R[D]h _[t]_ [[][Y][ |][ Z][]]
_h∈HDs[∗]_


RIDG [Y _Z] := EpDs,Dt_
_|_


(2)


where HD[∗] _s_ [:= arg min]h [R]h[D][s] [Y | Z] are the source risk minimizers, and pDs,Dt is any joint distribution that has full support over D × D. We call a representation Z _[∗]_ (or its encoder) optimal for IDG if
it minimizes the IDG risk.

3.2 CHARACTERIZING OPTIMAL REPRESENTATIONS FOR IDG UNDER COVARIATE SHIFT

The IDG risk is useful to evaluate representations, but gives few insights into IDG and is impractical to
optimize due to the supremum in Eq. (2). Under mild assumptions, we provide a simplified, equivalent
objective, which is easier to optimize. For convenience, we assume that there is a unique Bayes predictor f _[∗], which minimizes the expected risk over domains, i.e., f_ _[∗]_ = arg minf EpDt [Rf[D][t] [Y | X]].
This is satisfied by standard ML tasks pY,X and losses ℓ. More importantly, we assume the following
domain structure, which ensures the existence of optimal encoders and allows our simplification.

**Assumptions. All domains d ∈D we consider are related by the following assumptions:**

1. Generalized covariate shift. All domain-specific risk minimizers f ∈ arg minf [R[d]f [[][Y][ |][ X][]]][ are]
equal to the Bayes predictor f on their support, i.e., f (x) = f (x) for all x supp(pX _d)._

_[∗]_ _[∗]_ _∈_ _|_

2. Invariance of Bayes predictions. The set of Bayes predictions is the same for all domains, i.e.,
_f_ (x) _x_ supp(pX _d)_ = _f_ (x) _x_ .

_[∗]_ _|_ _∈_ _|_ _{_ _[∗]_ _|_ _∈X}_



-----

|h rcep Z|ds Y = 0 et p Z|dt|Y = 1|
|---|---|


_pZ|ds_

_Y = 0_ _Y = 1_

_pZ|dt_

(a) discriminative & support match (b) only support match (c) only discriminative

Figure 1: (a) Optimal representations for IDG must have invariant supports while being simultaneously
discriminative on all domains: (b) without the discriminative requirement, a source-risk minimizer
can mispredict the target, and (c) without support match, some risk minimizer will perform poorly.

Generalized covariate shift (GCS) ensures that f _[∗]_ is simultaneously optimal on all domains. For
log-loss ℓ it recovers standard covariate shift, i.e., pY _x,d = pY_ _x. For other losses, GCS is weaker,_
_|_ _|_
e.g., it only requires invariance of most likely labels for 0-1 loss, and of conditional expectations for
MSE. Invariance of Bayes predictors is necessary to learn useful predictors using a single domain.
For example, for 0-1 loss it ensures that each label is seen at least once in each domain.

The intuition behind our objective is that under GCS any source risk minimizer will make optimal
predictions on target samples x that are also in the source. Thus, IDG optimal representations are
exactly those that (a) have the same support in Z for all domain, and (b) retain GCS from Z without
sacrificing the ability to predict Y, which can be ensured by minimizing the risk from Z. See Fig. 1.

**Theorem 1. Under our assumptions, an encoder pZ∗** _| X is optimal for IDG if and only if it minimizes_
_the risk R [Y | Z] := inf_ _h EpDt_ R[D]h _[t]_ [[][Y][ |][ Z][]] _while matching the support of Z across domains, i.e.,_

_pZ∗_ _| X ∈_ arg minpZ | X  R [Y | Z] s.t. _∀_ _d ∈D, supp(pZ | d) = supp(pZ)_ (3)

_Moreover, such encoders exist and their IDG risk is the Bayes risk RIDG [Y | Z_ _[∗]] = R [Y | X]._

Theorem 1 provides an objective to learn representations on which performing risk minimization
using a single domain and Z _[∗]_ is as good as performing risk minimization on the target domain
from inputs X. Other sufficient conditions have previously been hinted towards, e.g., matching
the marginal pZ _d instead of its support (e.g., Ben-David et al., 2010a) which is the focus of most_
_|_
DG methods (e.g., Ganin et al., 2016). Note that previous conditions are nevertheless generally
not necessary and could be too stringent to be achievable. To our knowledge, Thm. 1 is the first
characterization of necessary and sufficient conditions, which gives better insights into the essential
goal for optimal IDG and provides a guide for deriving the least stringent objectives in practice.

The risk minimization (Eq. (3)) shows that one must have some knowledge about the target domains
to learn optimal representations for IDG. Access to targets might seem unrealistic, but without such
knowledge or additional assumptions it is provably impossible to beat even constant representations.

**Proposition 1 (No free lunch for IDG). Let ds be any source domain, Zds be any representation**
_chosen on source ds, and C_ _be a constant representation. Under minor assumptions, for every_
_∈Z_
_“good” target domain outside the source’s support on which Zds outperforms C for IDG, there are_
_many “bad” target domains on which Zds is strictly worse than C. Formal statement in Appx. B.3._

Proposition 1 shows that target knowledge is necessary for learning useful representations in IDG.
This may explain why previous DG methods have been unable to outperform ERM in standard
benchmarks (Gulrajani & Lopez-Paz, 2021): the knowledge they have access to is insufficient to
generalize. Taken together, Prop. 1 and Thm. 1 say that either you have access to target domains dt,
in which case you can achieve an IDG risk that matches supervised learning, or you do not access dt,
in which case any representation learning algorithm can achieve worse IDG risk than a constant.

4 LEARNING REPRESENTATIONS UNDER COVARIATE SHIFT

4.1 SELF-SUPERVISED LEARNING USING DOMAIN-AGNOSTIC AUGMENTATIONS

Our characterization of optimal representations for IDG (Thm. 1) requires labeled data from all
domains, which is impractical. We show how this can be overcome with self-supervised learning


-----

(a) standard augmentations (b) supervised augmentations


(c) image-text augmentations


“A dog with floppy ears.” “A pointy-eared dog.”


Figure 2: Image-text augmentations are practical domain-agnostic augmentations. Arrows denote
augmenters. Bubbles denote inputs that have the same representations, as induced by predicting the
augmentations. (a) Standard augmentations are not domain-agnostic. (b) Supervised augmentations
uniformly augment inputs inside their label class, irrespective of domains. (c) Image-text augmentations are (nearly) domain-agnostic as they map images across domains to similar descriptions.

(SSL), which is a technique for training representations without direct access to labels, and a particular
class of data augmentations. E.g, in CLIP, images are augmented with alt-text collected on the internet
and invariance is enforced between the representations of the image and its text pair (Radford et al.,
2021). Representations learned like this preserve discriminative information about all downstream
tasks Y whose label information is preserved by the augmentation (e.g., Dubois et al., 2021).

More precisely, an augmentation A is a random variable sampled conditionally from the input X. The
key requirement is that augmentations retain task information. Specifically, if any samples x, x[′] _∈X_
have the same augmentation conditional pA | x = pA | x′, then their Bayes predictions must be the
same f _[∗](x) = f_ _[∗](x[′]). With such A, one can learn an encoder that minimizes the risk R [Y | Z] by_
instead maximizing mutual information I[A; Z]. Intuitively, if Z has all augmentation information,
then it must have information about the conditional pA _X_, and thus the Bayes prediction f (X).
_|_ _[∗]_

This suggests learning optimal representations for IDG by replacing Eq. (3) with a maximization of I[A; Z]. Unfortunately, fully optimizing I[A; Z] w.r.t. pZ _X is not generally possible un-_
_|_
der the support constraint Eq. (3). This can be overcome under a domain-agnostic assumption,
which requires that the set of possible augmentation distributions is the same across domains, i.e.,
_pA | x | x ∈_ supp(pX | d) = _pA | x | x ∈X_ .

**Proposition 2.** _Let pA | X be a domain-agnostic augmenter. Then any optimal solution_ _pZ∗_ _| X of the_
_following objective is optimal for IDG:_

_pZ∗_ _| X ∈_ arg maxpZ | X I[A; Z] s.t. _∀_ _d ∈D, supp(pZ | d) = supp(pZ)_ (4)

Proposition 2 shows that we can still learn IDG optimal representations without labels if we have
access to the right augmentations. How realistic are those augmentations? For 0-1 loss ℓ, the most
likely label should be preserved, which is satisfied by standard image augmentations like rotations
and color jittering. Those augmentations are nevertheless not domain-agnostic for typical domains
(e.g. sketches and photos), since outputs A are correlated with the input’s domain D. See Fig. 2a.

A practical choice of augmentation that is nearly domain-agnostic, is a mapping from images to
text descriptions, as with CLIP (Radford et al., 2021) which uses text-image pairs. Image-text
augmentations have many advantages. First, text augmentations preserve label information for many
downstream tasks. Second, they are close to being domain-agnostic, since images from different
domains (e.g., sketches and photos) but similar semantics are often mapped to similar descriptions.[2]
(Fig. 2c). This gives insights into the open question (Radford et al., 2021) about why CLIP’s
representations are so robust compared to other SSL methods. Finally, image-text pairs are easy to
access in practice given their abundance on the internet. Many other multi-modal augmentations, e.g.,
audio-video (Wang et al., 2021), are also likely domain-agnostic and can be explored in practice.

In practice, even the domain information D is usually unknown. One can nevertheless still optimize
(Eq. (4)) by replace the support constraint with a stronger one that does not rely on D e.g., minimizing
I[Z; X] (see Sec. 4.2.2), . This highlights the potential of Prop. 2: if one can find a large source of
inputs X and domain-agnostic augmentations A (e.g., the 400M image-text pairs of CLIP) then one
can, in principle, learn optimal representations for IDG on any downstream task Y that A preserves.

2Although text descriptions might contain domain information (e.g., referring to “sketch”), they are still
much better than standard augmentations that rarely map together images from different domains.


-----

4.2 PRACTICAL OBJECTIVES

We now design practical objectives for learning optimal representations without labels. Proposition 2
does provide an objective but it is impractical as it involves constrained optimization. We can
nevertheless convert it to the following unconstrained objective by using a Lagrangian relaxation and
introducing a domain bottleneck B[Z, D] that enforces support match,
arg minpZ | X _−_ I[A; Z] + λ B[Z, D], (5)

Eq. (5) is a valid reformulation of Prop. 2 as long as minimizing B[Z, D] while maximizing I[A; Z]
enforces the support constraint in Eq. (4). Below, we provide different choices of such B[Z, D]
each of which results in a different SSL objective. In practice, however, terms in Eq. (5) are hard to
estimate from finite samples. We now discuss two variational bounds that can be efficiently estimated
and optimized with stochastic gradient descent (Bottou, 2010). For simplicity, we use a deterministic
encoder eϕ : X →Z for the rest of the paper. Detailed derivations are in Appx. C.

For both practical objectives we use a contrastive variational lower bound on I[A; Z] based on
InfoNCE (Oord et al., 2018), which is standard in SSL. Specifically, for a sample X, we first obtain
the augmented ‘positive’n _A by sampling from pA | X_ . We then obtain n augmented ‘negatives’n
_A[−]i_ _i=1_ [i.i.d. from the marginal][ p][A][ by first independently sampling][ X][ :=] _Xi[−]_ _i=1_ [from][ p][X][ and]
then sampling _A[−]i_ [from][ p]A | Xi[−] [. We denote][ A][ :=] _A, A[−]1_ _[, . . ., A]n[−]_ . InfoNCE then uses a critic _sψ_
to score how likely each A[′] **A is to be positive, resulting in the following variational bound,**
_∈_ 

I[A; Z] ≥ log(n + 1) + EpA,X,Z log _A[′]expA_ _s[exp]ψ([ s]A, Z[ψ][(][A])[′][, Z][)]_ _._ (6)

 _∈_ 

When =, one can tie the parameters of the critic and the encoder by passing augmentations
_A_ _X_ P
through the encoder and taking an inner product, i.e., sψ(A, Z) := eϕ(A)[T] _Z._

Many previous DG regularizers (e.g., Ganin et al., 2016; Li et al., 2018b;a) could be valid domain
bottlenecks. In the following, we discuss two possible B[Z, D], the first of which is novel.

4.2.1 CONTRASTIVE ADVERSARIAL DOMAIN BOTTLENECK (CAD)


Our first domain bottleneck minimizes B[Z, D] =
I[Z; D], which enforces support match using a KL
divergence. Dropping constants w.r.t. Z we thus
aim to maximize H[D | Z]. Domain-adversarial
neural network (DANN, Ganin et al., 2016) does
so by ensuring that a domain classifier qφ cannot
predict domains from representations, i.e., it maximizes EpD,Z [− log qφ(D | Z)] ≥ H[D | Z] w.r.t.
encoder parameter ϕ but minimizes it w.r.t. φ.
However, DANN suffers from two issues: (i) it
maximizes an upper bound on the desired term;
(ii) it requires adversarial training, which is challenging in practice.


**Algorithm 1 CAD objective**

**Require: eϕ, sψ, D, X, n**

1: Z ← _eϕ(X)_
2: A ← sample(pA | Xn ) i.i.d.

3: (Di[−][, X]i[−][, A]i[−][)] _i=1_ sample(pD,X,A)

4: X _, A ←_ _Xi[−]_ _ni=1[,][{][A]←[} ∪]−−_ _A[−]i_ _ni=1_

5:6: X Laug¬D ←− ← logXi[−] _[|][ D]A[′]i[−]exp∈A[̸][=] s[exp][ D, i]ψ_ ([ s]A,Z[ψ][ ∈][(][A]) _[′][[][,Z][n][]][)]_ _▷_ _−_ I[A; Z]

7: supp log P _X[′]_ _∈X¬D_ [exp][ e][ϕ][(][X][′][)][T][ Z] _▷_ I[Z; D]
_L_ _←−_ P _X[′′]_ _∈X_ [exp][ e][ϕ][(][X][′′][)][T][ Z]

8: return LCAD = LPaug + λLsupp


To overcome these issues, we construct q(D | Z) without introducing additional parameters and with
a bound that is tight with enough samples. In short, using the equality pD _Z = EpX_ _Z_ _pD_ _X_, we
_|_ _|_ _|_
set our variational distribution to q(D _Z) = Eqϕ,X_ [ˆp(D _X)], where qϕ,X(X_ _Z) is a contrastive_
_|_ _|_ _|_  
variational distribution of pX _Z constructed with samples X and a critic eϕ(X)[T]_ _Z tied with the_
_|_
encoder, ˆp is a count estimate of pD | X . Detailed derivations and explanations are in Appx. C.3. The
resulting contrastive adversarial domain (CAD) objective is in Algorithm 1. First, sample domains
**Dcurrent domain :=** _{Di−[}]i[n]=1 D[for each], i.e., X[ X]¬D[′][ ∈] :=[X]{[. Then collect inputs associated with a different domain from the]Xi−_ _[|][ D]i−_ _[̸][=][ D, i][ ∈]_ [[][n][]][}][. Ignoring constants, the final loss is]

_LCAD(ϕ, ψ) := EpD,X,A,Z_ "− log _A[′]exp∈A_ _s[exp]ψ([ s]A, Z[ψ][(][A])[′][, Z][)][ −]_ _[λ][ log]_ _X_ _[′]X∈X¬D_ _qϕ,X(X_ _[′]_ _| Z)!#_ _._ (7)

In Appx. C.4, we also derive a conditional variation of CAD that minimizesP I[Z; D _Y ], which can_
_|_
be used when labels are available and supervised augmentations are used.


-----

4.2.2 ENTROPY BOTTLENECK (ENT)

Our second domain bottleneck is the entropy bottleneck (Ent) that minimizes H[Z] = I[Z; X] ≥
I[Z; D], where the first equality uses the encoder’s determinism. Ent enforces support match by
removing all information that is not needed to maximize I[Z; A]. In particular, minimizing I[Z; X] is
more stringent than I[Z; D], as it also matches the representations inside a domain. The advantage of
Ent is that it does not require domain samples D, which are rarely accessible in SSL. We consider
the standard variational bound used in neural compression (Ballé et al., 2016; Theis et al., 2017),
H[Z] ≤ EpZ [− log qθ(Z)], where an entropy model qθ(Z) is used. This leads to


_−_ log _A[′]expA_ _s[exp]ψ([ s]A, Z[ψ][(][A])[′][, Z][)][ −]_ _[λ][ log][ q][θ][(][Z][)]_ _._ (8)
_∈_ 
P


_LEnt(ψ, ϕ, θ) := EpX,A,Z_

RELATED WORK


**Provably robust representations under covariate shift. Previous work mostly focuses on domain**
generalization bounds for robust representations. Ben-David et al. (2007; 2010a) bound the target
risk using the source risk, a divergence between source and target distributions, and the joint optimal
risk over source and target domains. Mansour et al. (2009) generalizes these results from 0-1 loss to
more general losses. Johansson et al. (2019) takes this further by deriving a support-based bound.
In our setting, these bounds only hint towards a sufficient condition for optimality, i.e., matching
the marginal pZ _d or its support while minimizing R [Y_ _Z]. However, these bounds can often be_
_|_ _|_
loose and the implied sufficient conditions are neither necessary nor generally achievable. Ben-David
et al. (2010b) suggests that separately minimizing R [Y | Z] or matching the marginal is not sufficient,
while Zhao et al. (2019) also proves minimizing only the source risk R[d][s] [Y | Z] is not sufficient; but
none of them proves the desired necessary condition. Our work distinguishes from previous work on
three key aspects: (i) we are the first to study and formalize optimally robust representations, and
provide the achievable sufficient and necessary conditions; (ii) we prove that one can practically
learn optimal Z _[∗]_ with SSL using domain-agnostic augmentations; (iii) we consider a more general
framework with any standard losses and a less stringent generalized covariate shift assumption, Still,
our work is more specific than others, as we consider idealized DG and unrestricted predictors H.

**Practical objectives for DG. The most popular DG methods aim to learn domain-invariant represen-**
tation by minimizing various divergernces between the marginal distributions pZ _d and pZ (Long_
_|_
et al., 2015; Ganin et al., 2016; Sun & Saenko, 2016; Long et al., 2017; Li et al., 2018a; Shen et al.,
2018; Nguyen et al., 2021). Others propose matching the conditional pZ _y,d across domains instead_
_|_
(Gong et al., 2016; Li et al., 2018b; Tachet des Combes et al., 2020). These regularizers would all be
valid domain bottlenecks B[Z, D] . Another line of work aims at learning Z with invariant predictors
_pY_ _z,d across domains (e.g., Arjovsky et al., 2019; Krueger et al., 2021; Li et al., 2021). However,_
_|_
none of these methods outperform ERM with fair model selections (Gulrajani & Lopez-Paz, 2021).

6 EXPERIMENTS

In our experiments, we aimed to: (i) verify our theoretical results in practice; (ii) investigate our
proposed representation learning objectives in practical DG; (iii) take advantage of pretrained SSL
models (in particular, CLIP) to achieve powerful models for DG. Unless stated otherwise, we consider
a two-stage training setup. First, the representation learner (“the representor”) trains an encoder pZ | X
using a specified objective and freezes it. Then, the person performing predictions (“the learner”)
trains her predictor h from Z by minimizing the risk on source data. Finally, the representation Z
and predictor h are evaluated on target data. In all experiments, the learner uses a linear classifier
for h. For the Ent bottleneck, we used Ballé et al.’s (2018) entropy model. For the CAD bottleneck
we used its conditional version whenever labels were available. When a model contains no domain
bottleneck, we label it as “Base”. For experimental details and additional results see Appxs. E and F.

6.1 SCIENTIFIC SETTING: EXPLORING OPTIMAL REPRESENTATIONS FOR WORST-CASE DG

To validate our theory, we studied optimal representations in a scientific setup that is as close to our
IDG framework as possible with log-loss ℓ. In particular, we used the PACS dataset (Li et al., 2017)


-----

-2

-4

-6

-8

-10


_R[Y |Z]_

_H[A|Z]_


10 2 10[0] 10[2] 10[4]

|-5.1±0.3 -0.4±0.1 -0.7±0.1 -3.5±0.1 -0.0±0.0 -0.8±0.2|0 likelihood 2 4 Log 6|source target|
|---|---|---|


source
target

2 [0] [2] [4]


(b) Effect of λ


DA Approx. Non-DA

Supervised
SingleDom
ApproxDA
IntraDom
Standard

Augmentation type


-5.1±0.3 -0.4±0.1 -0.7±0.1

-3.5±0.1 -0.0±0.0 -0.8±0.2


Base Ent CAD

(a) Effect of different objectives


(c) Effect of augmentations


Figure 3: (a) Adding bottlenecks significantly improves the worst-case DG performance and using
domain-agnostic (DA) augmentations (H[A | Z]) performs as well as with labels (R [Y | Z]). (b) Increasing the domain bottleneck weight λ will improve target performance until it decreases source
performance. (c) DA augmentations are crucial but approx. DA aug. might be also be sufficient.

and approximated the idealized DG by treating the dataset as the population distribution, i.e., we
did not split datasets into train and test sets. To approximate the worst-case source predictor, we
followed Dubois et al. (2020) by incorporating the wrongly labeled target data to the source domain.
The experimental setup goes as follows: (i) the representor trains a ResNet-18 (He et al., 2016) to
minimize the objective on labeled data from all domains; (ii) the learner trains a worst-case source
classifier h on every possible pair of (source, target); (iii) the negative target risk (log likelihood)
for each h is evaluated. We reported the log likelihood averaged over 5 seeds. For more realistic
scenarios (i.e. non-idealized average-case DG) see Appx. F.2 which replicates the following results.

**Do our domain bottlenecks improve worst-case DG?** In Fig. 3a, we compare IDG performance
of representations trained with (Ent, CAD) and without (Base) domain bottlenecks. We see that
both bottlenecks significantly improve the worst-case DG, and nearly achieve the source-domain
performance (0 log likelihood). This shows the importance of support match (Thm. 2) and the
effectiveness of our bottlenecks to enforce it. In Appx. F.2, we show that bottlenecks also helps in
practical scenarios, i.e., non-idealized average-case DG evaluated with accuracy (95.9% → 96.7%).

**What is the effect of λ?** Fig. 3b shows the effect of the bottleneck weight λ on the worst-case
target and source performance. We see that increasing λ will decrease the DG gap. As a result the
target performance improves until λ ≈ 10[2], where source performance starts to decrease.

**What if the representor has access to domain-agnostic augmentations instead of labels?** In
Sec. 4.2, we provide a contrastive objective for using augmentations. To show the effectiveness of the
objective, we compared minimizing H[A | Z] using Eq. (6) to standard supervised risk minimization
R [Y | Z] and used the domain-agnostic supervised augmentations (Fig. 2b). The 1[st] and 2[nd] row of
Fig. 3a show that our objective performs similarly to direct label prediction.

**How important is the choice of augmentations?** Prop. 2 shows that domain-agnostic (DA) augmentations are sufficient for achieving IDG, but it does not give necessary conditions. Here we
investigate the effect of using our loss with different choices of augmentations. Specifically, we used
_LCAD with five augmentations. The first two are DA. ‘Supervised’: augment inputs inside the label_
class across all domains as in Fig. 2b; ‘SingleDom’: augment inputs to same label samples from a
fixed domain. The second two are not DA. ‘Standard’: standard SSL augmentations (Chen et al.,
2020) as in Fig. 2a; ‘IntraDom’: augment inputs to same label and same domain samples. Finally, we
consider ‘ApproxDA’, which is approximately DA by augmenting 10% of the time with ‘Supervised‘
and 90% of the time with ‘IntraDom‘. Fig. 3c shows that the non-DA augmentations give terrible
results compared to DA. Interestingly, ‘ApproxDA’ also performs very well, which suggests that
approximately DA augmentations might be sufficient to learn optimal representations in practice.

**What if the representor does not have access to target domains?** Prop. 1 shows that DG without
access to target domains is generally impossible. We empirically verified this by excluding a
predefined target dt domain from the representor’s training set, i.e., LCAD is optimized on 3 of the 4
domains. The learner then trains a predictor h on each source. We finally evaluate each h on the target
domain dt, and average over choices of dt. The resulting worst-case log likelihood was 4.2 0.2,
_−_ _±_
which is significantly worse than when the representor had access to all domains (−0.8 ± 0.2).


-----

6.2 APPROXIMATING OPTIMAL REPRESENTATIONS BY EXPLOITING PRETRAINED SSL

As discussed in Sec. 4.1, one can learn optimal representations for IDG by performing SSL with
a domain bottleneck on a large sample of inputs X and domain-agnostic augmentations A. This
is nearly how CLIP was pretrained (SSL with 400M image-text pairs) except it did not include a
domain bottleneck. In this section, we investigate how to take advantage of CLIP to approximate
optimal representations for IDG. We did so in two simple steps. First, we froze the pretrained CLIP
and added a multi-layer perceptron (MLP) that could effectively finetune CLIP’s representations.
Then, we trained the MLP by minimizing our CAD bottleneck and R [Y | Z] on the available data.

In all experiments, we used the standard DomainBed benchmark (with non-MNIST datasets) and
protocol (Gulrajani & Lopez-Paz, 2021). In particular, we left out a target domain for evaluation
and used the union of other domains for training both the encoder and the classifier. Contrary to our
scientific setting, the representor does not get access to the target domain. All our representations
were evaluated by fitting a linear classifier on source domains with source validation selection. As
in DomainBed we selected the encoder based on ‘oracle selection’ over 10 hyperparameters, and
reported the target accuracy averaged over all choices of targets and 5 random seeds with standard
errors. Note that using ‘oracle selection’ is more consistent with our theory since it gets access to the
necessary target information (for model selection), as discussed in Appx. F.3. Due to space limit,
we only included as baselines ‘ERM’ and ‘DomainBed SOTA’ which for each dataset is the best
result over all baselines. The extended results and baselines are in Table 4. Details in Appx. E.3. We
investigated two pretrained CLIP models with different number of parameters. The larger ViT-B/32
denoted ‘CLIP L’ and the smaller ResNet-50 denoted ‘CLIP S’.

Table 1: CLIP significantly outperforms the previous SOTA result on DomainBed, as supported
by our theoretical analysis. Finetuning CLIP with our CAD bottleneck consistently improves the
robustness of its representations and achieves SOTA performance.

|Algorithm|VLCS PACS OfficeHome TerraIncognita DomainNet|
|---|---|


|ERM DomainBed SOTA|77.6 ± 0.3 86.7 ± 0.3 66.4 ± 0.5 53.0 ± 0.3 41.3 ± 0.1 79.9 ± 0.2 87.2 ± 0.1 68.4 ± 0.2 54.4 ± 0.3 41.8 ± 0.1|
|---|---|


|DINO + CAD|69.6 ± 0.6 76.1 ± 0.1 56.9 ± 0.5 25.9 ± 1.2 33.6 ± 0.1|
|---|---|


|CLIP S CLIP S + Base CLIP S + CAD|81.1 ± 0.5 90.3 ± 0.2 70.6 ± 0.1 29.6 ± 0.8 47.7 ± 0.0 81.3 ± 0.5 91.2 ± 0.3 70.6 ± 0.1 36.4 ± 0.7 46.8 ± 0.2 82.3 ± 0.3 92.0 ± 0.2 71.9 ± 0.2 36.2 ± 0.8 48.8 ± 0.1|
|---|---|


|CLIP L CLIP L + CAD|80.7 ± 0.4 93.7 ± 0.8 79.6 ± 0.1 36.9 ± 0.6 52.8 ± 0.1 81.6 ± 0.1 94.9 ± 0.3 80.0 ± 0.2 40.6 ± 1.1 53.7 ± 0.1|
|---|---|


|Approx. Optimal Z∗|86.8 ± 0.6 97.2 ± 0.6 86.3 ± 1.6 76.5 ± 4.1 66.7 ± 0.2|
|---|---|


**Can we approximate optimal representations by exploiting pretrained CLIP?** The row ‘CLIP
L + CAD’ in Table 1 shows that finetuning a large pretrained CLIP model with our CAD achieves
SOTA on nearly all DomainBed benchmarks by a very large margin (see 2[nd] row). Note that the
poor performance on TerraIncognita is likely because CLIP’s dataset does not cover such images
(camera traps monitoring animals). The last row essentially shows an optimal representation, which
we approximate by finetuning CLIP L with our CAD on all domains including the target. The gap
between CLIP L + CAD and the upper-bound suggests that one can still learn better representations.
We hypothesize that end-to-end training of our objective would greatly shrink this gap.

**Are gains due to the architectural differences?** DomainBed’s baselines finetuned an ImageNet
pretrained ResNet-50. In contrast, CLIP L pretrained a larger ViT. To decouple gains due to our
objective from architectural gains, we evaluated ResNet-50 pretrained CLIP S. Table 1 shows that
CLIP S + CAD still significantly outperforms DomainBed baselines. Note that our theory does not
constrain the encoder and so we expect larger encoders to be better as seen in Table 1.

**What is the effect of domain bottlenecks?** In the “CLIP” rows of Table 1, we investigated the
effect of finetuning CLIP with our CAD bottleneck. We see that for both CLIP L and CLIP S, it
consistently improves results by around 1 ∼ 2%. These gains are due to the bottleneck, rather than
finetuning on source data as seen by ‘CLIP S + Base’. We believe the gains could potentially be much
larger if CLIP was trained end-to-end with our bottleneck. Note that raw CLIP S already significantly


-----

outperforms baselines. We hypothesize that this is because SGD acts as an information bottleneck
that naturally favors support match (Shwartz-Ziv & Tishby, 2017).

**Which pretrained SSL model to use?** Our theory suggests that we can exploit pretrained SSL
models as long as their augmentations are domain-agnostic and their training set covers desired
domains. We investigated adaption of SSL models that do not satisfy those properties by finetuning
DINO (Caron et al., 2021), the current SOTA on SSL ImageNet. DINO is pretraiend using standard
augmentations. As a result, Table 1 shows that the finetuned DINO + CAD significantly underperforms compared to CLIP S and DomainBed baselines. This supports our hypothesis that CLIP is
much more robust than other SSL methods due to its domain-agnostic augmentations.

6.3 TOWARDS GENERIC ROBUST REPRESENTATIONS WITH SSL

In the previous section, we finetuned CLIP in a task specific fashion by optimizing R [Y | Z] and our
CAD bottleneck. To get generic (task agnostic) robust representations, one should instead directly
use our objectives on a sufficiently large dataset with image-text augmentations. Unfortunately, we
cannot fully train CLIP with our bottlenecks as we do not have access to CLIP’s original dataset and
sufficient compute. In this section, we aim to emulate such training of generic robust representations.

To do so we used LAION-400M (Schuhmann et al., 2021) that is a public dataset that contains 400M
web-crawled image-text pairs. Due to our computational budget, we again froze the pretrained CLIP
L and only finetuned an additional MLP with our LEnt. We used LEnt as it only requires access to paired
image X and text A but no prior information about domain D. As in CLIP’s paper, we evaluated the
learned representation Z in Taori et al.’s (2020) realistic setting, where a linear classifier h from Z is
trained on ImageNet and tested on 7 natural distribution shift datasets. Details in Appx. E.4.

**Would training CLIP with a bottleneck have improved its robustness?** As shown in the last
2 rows of Table 2, finetuning CLIP L on LAION with LEnt (Tuned w/ Ent) outperforms finetuning
without bottleneck (Tuned w/o Ent) on all 7 distribution shift datasets. This suggests that directly
training CLIP with our Ent bottleneck would improve the robustness of learned representations. We
hypothesize that the gains could be larger if SSL models trained LEnt end-to-end. In Appx. F.4, we
show similar results on DomainBed. Note that both models underperform the original CLIP L, likely
due to non-end-to-end training and LAION data with (possibly) lower quality than CLIP’s data.

Table 2: Finetuning CLIP L on LAION with an entropy bottleneck improves its robustness compared
to finetuning without on 7 distribution shift datasets. The pretrained CLIP L is still better likely due
to end-to-end training with higher quality data. IN denotes ImageNet.

|Col1|IN|IN-V2 IN-S YT-BB IN-Vid ObjectNet IN-A IN-R Avg.|
|---|---|---|


|CLIP L|75.2|64.2 41.0 58.4 71.6 42.8 27.5 62.9 52.6|
|---|---|---|


|Tuned w/o Ent Tuned w/ Ent|73.8 74.2|62.1 37.0 56.9 68.8 41.3 26.0 58.1 50.0 62.7 38.9 58.1 70.1 42.1 26.2 60.8 51.3|
|---|---|---|



7 CONCLUSION

We gave a simple variational characterization of all representations on which source-risk minimizers
are guaranteed to generalize to target domains that preserve the Bayes predictor. Similar to previous
work, our theory strongly implies the need for target information when learning representations for
domain generalization. Nevertheless, we identified a domain-agnostic property of data augmentations
that make it possible to learn optimal representations from unlabelled data. Thus, we showed that it is
possible to learn robust representations using only large sources of inputs X and augmentations A.

There are caveats that need to be addressed in future work. First, we studied an idealized DG,
which assumes access to the population distributions. This gives insights into the challenges that are
specific to DG, rather than finite sample challenges faced throughout ML. Second, we considered risk
minimizers from an unconstrained hypothesis class. The support constraint can likely be weakened,
if the hypothesis class is constrained. Finally, we focus only on optimal representations, but it would
be interesting to characterize approximately optimal representations. Nevertheless, in this idealized
setting, our characterization is a springboard from which all future objectives can be derived, and, in
general, it brings us closer to the goal of robust machine learning systems.


-----

**Acknowledgement** We would like to thank Elliot Creager, Roger Grosse, Elan Rosenfeld, Guodong
Zhang, Han Zhao, and anonymous reviewers for their helpful feedbacks and encouragements. Resources used in preparing this research were provided, in part, by the Province of Ontario, the
Government of Canada through CIFAR, and companies sponsoring the Vector Institute. We acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC),
RGPIN-2021-03445.

**Reproducibility** For our theoretical results, we include formal assumptions, statements, and proofs
in Appxs. A and B. We include the detailed derivations of our algorithms in Appx. C. For our
experiments, we include experimental details for reproducing our results in Appx. E and have
[released our code at https://github.com/ryoungj/optdom.](https://github.com/ryoungj/optdom)

REFERENCES

Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information
bottleneck. arXiv preprint arXiv:1612.00410, 2016.

Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.
_arXiv preprint arXiv:1907.02893, 2019._

Johannes Ballé, Valero Laparra, and Eero P Simoncelli. End-to-end optimized image compression.
_arXiv preprint arXiv:1611.01704, 2016._

Johannes Ballé, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston. Variational
image compression with a scale hyperprior. arXiv preprint arXiv:1802.01436, 2018.

Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Danny Gutfreund,
Joshua Tenenbaum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing
the limits of object recognition models. 2019.

Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Proceedings of the
_European conference on computer vision (ECCV), pp. 456–473, 2018._

Shai Ben-David, John Blitzer, Koby Crammer, Fernando Pereira, et al. Analysis of representations
for domain adaptation. Advances in neural information processing systems, 19:137, 2007.

Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman
Vaughan. A theory of learning from different domains. Machine Learning, 79(1):151–175, 2010a.

Shai Ben-David, Tyler Lu, Teresa Luu, and David Pal. Impossibility theorems for domain adaptation.
In Yee Whye Teh and Mike Titterington (eds.), Proceedings of the Thirteenth International
_Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings of Machine Learning_
_Research, pp. 129–136, Chia Laguna Resort, Sardinia, Italy, 13–15 May 2010b. PMLR. URL_
[https://proceedings.mlr.press/v9/david10a.html.](https://proceedings.mlr.press/v9/david10a.html)

Léon Bottou. Large-scale machine learning with stochastic gradient descent. In Proceedings of
_COMPSTAT’2010, pp. 177–186. Springer, 2010._

Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and
Armand Joulin. Emerging properties in self-supervised vision transformers. arXiv preprint
_arXiv:2104.14294, 2021._

Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning, pp.
1597–1607. PMLR, 2020.

Yann Dubois, Douwe Kiela, David J Schwab, and Ramakrishna Vedantam. Learning optimal representations with the decodable information bottleneck. In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33,
[pp. 18674–18690. Curran Associates, Inc., 2020. URL https://proceedings.neurips.](https://proceedings.neurips.cc/paper/2020/file/d8ea5f53c1b1eb087ac2e356253395d8-Paper.pdf)
[cc/paper/2020/file/d8ea5f53c1b1eb087ac2e356253395d8-Paper.pdf.](https://proceedings.neurips.cc/paper/2020/file/d8ea5f53c1b1eb087ac2e356253395d8-Paper.pdf)

Yann Dubois, Benjamin Bloem-Reddy, Karen Ullrich, and Chris J. Maddison. Lossy compression for
lossless prediction. arXiv preprint arXiv:2106.10800, 2021.


-----

Chen Fang, Ye Xu, and Daniel N Rockmore. Unbiased metric learning: On the utilization of multiple
datasets and web images for softening bias. In Proceedings of the IEEE International Conference
_on Computer Vision, pp. 1657–1664, 2013._

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, François
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks.
_The journal of machine learning research, 17(1):2096–2030, 2016._

Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation.
_Journal of the American statistical Association, 102(477):359–378, 2007._

Mingming Gong, Kun Zhang, Tongliang Liu, Dacheng Tao, Clark Glymour, and Bernhard Schölkopf.
Domain adaptation with conditional transferable components. In International conference on
_machine learning, pp. 2839–2848. PMLR, 2016._

Ian Goodfellow. Nips 2016 tutorial: Generative adversarial networks. _arXiv preprint_
_arXiv:1701.00160, 2016._

Ishaan Gulrajani and David Lopez-Paz. In search of lost domain generalization. In International
_[Conference on Learning Representations, 2021. URL https://openreview.net/forum?](https://openreview.net/forum?id=lQdXeXDoWtI)_
[id=lQdXeXDoWtI.](https://openreview.net/forum?id=lQdXeXDoWtI)

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770–778, 2016.

Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul
Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical
analysis of out-of-distribution generalization. arXiv preprint arXiv:2006.16241, 2020.

Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
_Recognition, pp. 15262–15271, 2021._

Fredrik D Johansson, David Sontag, and Rajesh Ranganath. Support and invertibility in domaininvariant representations. In The 22nd International Conference on Artificial Intelligence and
_Statistics, pp. 527–536. PMLR, 2019._

Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. _arXiv preprint_
_arXiv:2004.11362, 2020._

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
_arXiv:1412.6980, 2014._

Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On convergence and stability of gans.
_arXiv preprint arXiv:1705.07215, 2017._

David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai
Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In International Conference on Machine Learning, pp. 5815–5826. PMLR, 2021.

Bo Li, Yifei Shen, Yezhen Wang, Wenzhen Zhu, Colorado J Reed, Jun Zhang, Dongsheng Li, Kurt
Keutzer, and Han Zhao. Invariant information bottleneck for domain generalization. arXiv preprint
_arXiv:2106.06333, 2021._

Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain
generalization. In Proceedings of the IEEE international conference on computer vision, pp.
5542–5550, 2017.

Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adversarial feature learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern
_Recognition, pp. 5400–5409, 2018a._


-----

Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao.
Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the
_European Conference on Computer Vision (ECCV), pp. 624–639, 2018b._

Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with
deep adaptation networks. In International conference on machine learning, pp. 97–105. PMLR,
2015.

Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Deep transfer learning with joint
adaptation networks. In International conference on machine learning, pp. 2208–2217. PMLR,
2017.

Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds
and algorithms. arXiv preprint arXiv:0902.3430, 2009.

A Tuan Nguyen, Toan Tran, Yarin Gal, Philip HS Torr, and Atılım Güne¸s Baydin. Kl guided domain
adaptation. arXiv preprint arXiv:2106.07780, 2021.

Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. arXiv preprint arXiv:1807.03748, 2018.

Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching
for multi-source domain adaptation. In Proceedings of the IEEE/CVF International Conference on
_Computer Vision, pp. 1406–1415, 2019._

Ben Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational
bounds of mutual information. In International Conference on Machine Learning, pp. 5171–5180.
PMLR, 2019.

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.
Learning transferable visual models from natural language supervision. In Marina Meila and Tong
Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume
139 of Proceedings of Machine Learning Research, pp. 8748–8763. PMLR, 18–24 Jul 2021. URL
[https://proceedings.mlr.press/v139/radford21a.html.](https://proceedings.mlr.press/v139/radford21a.html)

Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers
generalize to imagenet? In International Conference on Machine Learning, pp. 5389–5400. PMLR,
2019.

Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust
neural networks for group shifts: On the importance of regularization for worst-case generalization.
_arXiv preprint arXiv:1911.08731, 2019._

Nikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar.
A Theoretical Analysis of Contrastive Unsupervised Representation Learning. In Kamalika
Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference
_on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97_
[of Proceedings of Machine Learning Research, pp. 5628–5637. PMLR, 2019. URL http:](http://proceedings.mlr.press/v97/saunshi19a.html)
[//proceedings.mlr.press/v97/saunshi19a.html.](http://proceedings.mlr.press/v97/saunshi19a.html)

Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis,
Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of
clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.

Ohad Shamir, Sivan Sabato, and Naftali Tishby. Learning and generalization with the information
bottleneck. Theor. Comput. Sci., 411(29-30):2696–2711, 2010. doi: 10.1016/j.tcs.2010.04.006.
[URL https://doi.org/10.1016/j.tcs.2010.04.006.](https://doi.org/10.1016/j.tcs.2010.04.006)

Vaishaal Shankar, Achal Dave, Rebecca Roelofs, Deva Ramanan, Benjamin Recht, and Ludwig
Schmidt. Do image classifiers generalize across time? arXiv preprint arXiv:1906.02168, 2019.

Jian Shen, Yanru Qu, Weinan Zhang, and Yong Yu. Wasserstein distance guided representation
learning for domain adaptation. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.


-----

Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information.
_[CoRR, abs/1703.00810, 2017. URL http://arxiv.org/abs/1703.00810.](http://arxiv.org/abs/1703.00810)_

Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In
_European conference on computer vision, pp. 443–450. Springer, 2016._

Remi Tachet des Combes, Han Zhao, Yu-Xiang Wang, and Geoffrey J Gordon. Domain adaptation
with conditional distribution matching and generalized label shift. Advances in Neural Information
_Processing Systems, 33, 2020._

Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig
Schmidt. Measuring robustness to natural distribution shifts in image classification. _arXiv_
_preprint arXiv:2007.00644, 2020._

Lucas Theis, Wenzhe Shi, Andrew Cunningham, and Ferenc Huszár. Lossy image compression with
compressive autoencoders. arXiv preprint arXiv:1703.00395, 2017.

Naftali Tishby, Fernando C Pereira, and William Bialek. The information bottleneck method. arXiv
_preprint physics/0004057, 2000._

Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep
hashing network for unsupervised domain adaptation. In Proceedings of the IEEE conference on
_computer vision and pattern recognition, pp. 5018–5027, 2017._

Haohan Wang, Songwei Ge, Eric P Xing, and Zachary C Lipton. Learning robust global representations by penalizing local predictive power. arXiv preprint arXiv:1905.13549, 2019.

Luyu Wang, Pauline Luc, Adria Recasens, Jean-Baptiste Alayrac, and Aaron van den Oord. Multimodal self-supervised learning of general audio representations. arXiv preprint arXiv:2104.12807,
2021.

Aolin Xu and Maxim Raginsky. Minimum excess risk in bayesian learning. _arXiv preprint_
_arXiv:2012.14868, 2020._

Shen Yan, Huan Song, Nanxiang Li, Lincan Zou, and Liu Ren. Improve unsupervised domain
adaptation with mixup training. arXiv preprint arXiv:2001.00677, 2020.

Han Zhao, Remi Tachet Des Combes, Kun Zhang, and Geoffrey Gordon. On learning invariant
representations for domain adaptation. In International Conference on Machine Learning, pp.
7523–7532. PMLR, 2019.


-----

# Appendix

### Table of Contents

**A Preliminaries** **15**
A.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15

A.2 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15

A.3 Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

**B** **Proofs** **19**
B.1 Lemmas for general losses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19

B.2 Proof of Theorem 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19

B.3 Impossibility results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

B.4 Augmentations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25

**C Practical objectives** **27**
C.1 Mutual information bottleneck B[Z, X, Y, D] = I[Z; X] . . . . . . . . . . . . 27

C.2 Entropy bottleneck B[Z, X, Y, D] = H[Z] . . . . . . . . . . . . . . . . . . . . 28

C.3 Contrastive adversarial domain bottleneck B[Z, X, Y, D] = I[Z; D] . . . . . . . 29

C.4 Conditional CAD B[Z, X, Y, D] = I[Z; D | Y ] . . . . . . . . . . . . . . . . . 30

**D Extended Related Work** **32**

**E** **Experimental Details** **32**
E.1 Scientific . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32

E.2 Bridge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33

E.3 DomainBed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34

E.4 LAION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35

**F** **Additional Experimental Results** **36**
F.1 Scientific . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36

F.2 Bridge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36

F.3 DomainBed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

F.4 LAION . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38


-----

A PRELIMINARIES

A.1 NOTATION

For the most part, we will assume that all spaces are discrete probability spaces. A full list of
assumptions is found at Appx. A.3.

**General** The image of a set A ⊆X under a function f : X →Y is denoted f _[→](A) =_
_{f_ (x) | x ∈ _A}. The pre-image is denoted f_ _[←](B) =_ _{x ∈X | f_ (x) ∈ _B} for B ⊆Y._

**Probability** Random variables (r.v.) are denoted by uppercase letters (e.g., X), and their sample space and realizations are denoted by the corresponding calligraphic (e.g., X ) and lowercase
letters (e.g., x) respectively. The probability mass function (pmf) of a random variable X is
denoted as pX . We use capital P instead of p to denote the measure under p. The support
supp(pX ) of a discrete distribution is the set of all points x with positive probability, i.e.,
_∈X_
supp(pX ) = _x_ _pX_ (x) > 0 . The space of all probability distributions on is denoted
_{_ _∈X |_ _}_ _X_
_P(X_ ) = _pX | pX_ (x) ≥ 0 and _x∈X_ _[p][X]_ [(][x][) = 1] .

When it is necessary to be explicit, we will denote ‘ _X is distributed as pX_ ’ using the notation
d
_X_ _∼_ _pX_ . Expectations are written as:[P] EpX [f (X)], independence of two r.v. as ·⊥⊥·, conditional
independence as ·⊥⊥· | ·.

For jointly distributed random variables (X, Y ) taking value in (t.v.i.) X × Y, the conditional
distribution is denoted as pY _X :_ [0, 1]. For convenience, let pY _x = pY_ _X_ ( _x) be the_
_|_ _Y × X →_ _|_ _|_ _· |_
conditional distribution of Y given x. All random variables are independently distributed, unless an
explicit joint distribution or coupling is given.

A.2 DEFINITIONS

We are interested in prediction problems with domain shift. There are three random variables: the
target domain Dt, the input X, the label Y . They have the following joint distribution:
(Dt, X, Y ) _∼d_ _pDt · pX,Y | Dt_ (9)
where we drop the arguments of the probability densities for clarity. We make a variety of convenience
assumptions on these random variables (Assumption 6). Crucially, we will be making the Bayes
invariance assumption on pDt,X,Y that can be thought of as a generalized covariate shift assumption
(Assumption 4).

We will be studying the effect of changing the representation of the data. This is done by “encoding”
_X into a representation Z using a conditional distribution pZ | X_ .

**Definition 1 (Encoder). An encoder is a conditional distribution pZ** _X :_ [0, 1] from the
_|_ _Z × X →_
input space X to the representation space Z.

The data together with the representation has the following joint:
(Dt, X, Y, Z) _∼d_ _pDt · pX,Y | Dt · pZ | X_ (10)
The key thing to notice here is that Z is conditionally independent of Y, Dt given X. In particular,
the same encoder is used across all domains.

A.2.1 RISK MINIMIZATION

Our ultimate goal is to predict Y from the representation Z of X in a manner that is robust to changes
in the domain.

We formalize this in the standard way by making predictions γ ∈ Γ in a space of predictions or
actions. For example the prediction space may be the set of all possible labels Γ = Y, in which case
we would be predicting deterministic labels. Or we may predict a distribution over labels, in which
case the prediction space would be the set of all probability distributions on Y, i.e. Γ = P(Y).

A predictor is a function mapping inputs to predictions, i.e., f : X → Γ, or representations to
predictions, i.e., h : Z → Γ. For example, f may be a neural network that takes as input a sample x
and outputs a vector of logits that parameterize a softmax distribution over finitely many labels.


-----

We select predictors according to the risk defined via a loss function ℓ : Γ R 0 :
_Y ×_ _→_ _≥_ _∪{∞}_

Rf [Y | X] := EpX,Y [ℓ(Y, f (X))] . (11)

In particular, we are interested in the Bayes (minimum) risk over all predictors:

R [Y _X] := inf_ (12)
_|_ _f_ [R][f][ [][Y][ |][ X][]][,]

We denote the set of all optimal predictors from X as

_F_ _[∗]_ := _{f | Rf [Y | X] = R [Y | X]}_ (13)

Similarly, we define the risk Rh [Y | Z], the Bayes risk R [Y | Z], and the set of optimal predictors

_Z_ [:=] _[{][h][ |][ R][h]_ [[][Y][ |][ Z][] = R [][Y][ |][ Z][]][}] (14)
_H[∗]_

from Z, all of which vary as a function of the encoder pZ | X . Note, in the main body of the paper,
we omitted the subscript Z from HZ[∗] [for clarity, but we will keep it in the Appendices. We assume]
that together our loss and prediction space always admit optima (Item 2 of Assumption 2), and thus
_F_ _[∗], HZ[∗]_ [are always non-empty.]

We will be assuming that the risk admits unique optimal prediction when predicting from X (Item 3
of Assumption 2). Thus it makes sense to define the following:

**Definition 2 (The Bayes predictor). The Bayes predictor f** _[∗]_ : X → Γ is the unique predictor that is
optimal for all x ∈X :
_f_ _[∗](x) = arg minγ∈Γ_ [E][p][Y][ |][ x] [[][ℓ][(][Y, γ][)]] (15)

**Definition 3 (The Bayes image). The image of all the inputs under the Bayes predictor will be**
denoted as Γ[∗] = f _[∗]_ _[→](X_ ) and called the Bayes image.

Note that F _[∗]_ becomes a singleton {f _[∗]}, but it is not necessarily the case for HZ[∗]_ [since we will not be]
making any uniqueness assumption on optimal prediction from Z.

A.2.2 DOMAIN GENERALIZATION

We are interested in controlling the risk in a domain generalization setting, and so we define the
_domain-conditional risk,_

R[d]f [[][Y][ |][ X][] :=][ E][p]X,Y | d [[][ℓ][(][Y, f] [(][X][))]][ .] (16)

R[d] [Y | X], Fd[∗] [are defined as Eqs. (12) and (13), respectively, but with respect to][ R]f[d] [. Similarly,]
define the Bayes image for domain d as

Γ[∗]d [:=][ f][ ∗] _[→][ ]supp(pX_ _d)_ _._ (17)
_|_

We also define domain-conditional quantities for prediction from a representation _Z. The most_
important term which we will be investigating is an idealization of the domain generalization worstcase risk.

**Definition 4 (IDG risk). Given an encoder pZ** _X and a distribution pDt,Ds over a target domain Dt_
_|_
and source domain Ds, the idealized domain generalization worst-case risk, IDG risk for short, is the
expected worst-case target risk taken over source minimizers, i.e.,


sup R[D]h _[t]_ [[][Y][ |][ Z][]]
_h∈HZ,Ds[∗]_


RIDG [Y _Z] := EpDt,Ds_
_|_


(18)


Note that the IDG risk is well-defined because HZ,Ds[∗] [is non-empty by Assumption 2. The desired]
optimal representations, are then those that minimize the IDG risk.

**Definition 5 (Optimal representations for IDG). An encoder pZ∗** _X is optimal for idealized domain_
_|_
generalization if and only if it minimizes the IDG risk, i.e.,

RIDG [Y _Z_ _[∗]] = inf_ (19)
_|_ _pZ | X_ [R][IDG][ [][Y][ |][ Z][]]


-----

A.3 ASSUMPTIONS

We make a the following assumptions throughout the paper. All these assumptions should hold for
**practical settings.**

**Assumption 1 (Convenience: discrete probability spaces). All data spaces (D, X** _, Y, Z, A) are_
discrete spaces. Because the distributions of X, Y, D are fixed, we assume for convenience that
supp(pX ) = X, supp(pY ) = Y, and supp(pDt ) = D.

Assumption 1 is a convenience assumption to avoid measure theory for the sake of clarity. It always
holds in practice due to finiteness of computers, i.e., all spaces will be finite but arbitrarily large. We
believe that our claims can nevertheless be generalized to typical continuous spaces with some minor
technical assumptions.

**Assumption 2 (Losses admit optima). We assume that our risk always admits optimal predictions:**

1. |Γ| > 1.

2. For all pΥ ( ), there exists γ[∗] Γ, such that
_∈P_ _Y_ _∈_

EpΥ [ℓ(Υ, γ[∗])] ≤ EpΥ [ℓ(Υ, γ)] _∀_ _γ ∈_ Γ. (20)

3. For all x ∈X, there exist γ[∗] _∈_ Γ, such that

EpY _x_ [ℓ(Y, γ[∗])] < EpY _x_ [ℓ(Y, γ)] _γ_ = γ[∗]. (21)
_|_ _|_ _∀_ _̸_

Note that for log-loss ℓ(y, γ) = − log γ(y) and finite Y, these assumptions are satisfied if Γ = P(Y)
where the optimal prediction for Item 3 is γ[∗] = pY _x by strict properness (Gneiting & Raftery,_
_|_
2007). If we consider the 0-1 loss (reverse accuracy) ℓ(y, γ) = 1 − 1[y = γ] with Γ = Y and a finite
label space where the optimal prediction for Item 3 is γ[∗] = arg maxy _pY_ _x(y), this assumption is_
_∈Y_ _|_
mostly satisfied, except we assume that pY _x has a unique mode._
_|_

Assumption 2 serves two purposes: Item 2 ensures that for any representation the optimal predictors
from Z exists such that the IDG risk is well-defined as in Def. 5; Item 3 ensures a unique Bayes
predictor from X, which simplifies the analysis and is satisfied by common losses as described above.

**Assumption 3 (Cardinalities). We assume that**

_|Z| ≥|Γ[∗]| ≥_ 2 (22)

Assumption 3 is very weak and ensures that optimal representations always exists (Prop. 3).

**Assumption 4 (Generalized covariate shift). The Bayes predictor is optimal for all domains. I.e., for**
all (x, d) ∈ supp(pX,Dt ), γ ∈ Γ such that γ ̸= f _[∗](x), we have_

EpY | x,d [ℓ(Y, f _[∗](x))] < EpY | x,d_ [ℓ(Y, γ)] . (23)

For example, in the case of strictly proper scoring rules, e.g. log loss, covariate shift pY _X,D = pY_ _X_
_|_ _|_
is equivalent to the invariance of the Bayes predictor. For the 0-1 loss, this is guaranteed by invariance
of the most likely label. For MSE it is guaranteed by the invariance of the expected label. In the latter
two cases, Assumption 4 is less stringent than the typical covariate shift assumption.

Assumption 4 is the core assumption for our theoretical results. It ensures that source and target
domains are related in a useful way that can be utilized by the representation.

**Assumption 5 (Constant Bayes image). The Bayes image is invariant across domains, i.e., for all**
_d ∈D,_
Γ[∗]d [= Γ][∗][.] (24)
For the case of 0-1 loss, this simply means that the label set for all domains is the same, which is
trivial. For log-loss, this means that the set of possible conditional distributions Γ[∗]d [=][ {][p][Y][ |][ x][ |][ x][ ∈]
supp(pX | d)} is the same across domains.


-----

Assumption 5 is crucial to be able to learn. Without it, in the extreme case, one could set each domain
to be all examples associated with a single element from the label set (or the Bayes image set) in
which case it is impossible to generalize across different domains. Assumption 5 is also necessary to
guarantee the existence of optimal representations as in Prop. 3.

**Assumption 6 (Domain joint). pDt,Ds is any distribution such that supp(pDt,Ds** ) = D × D.

In a simplified scenario, one could define the source Ds and target Dt as i.i.d. r.v. from pDt, where
_pDt,Ds = pDt · pDs = pDt · pDt and Assumption 6 is trivially satisfied._


-----

B PROOFS

B.1 LEMMAS FOR GENERAL LOSSES

An important result that we will be using is the generalized data processing inequality of Bayes risk
(Xu & Raginsky, 2020; Dubois et al., 2021). We include it here for completeness.

**Lemma 1 (Generalized DPI (Xu & Raginsky, 2020; Dubois et al., 2021)). Let Z −** _X −_ _Y be a_
_Markov chain of random variables. For any loss function ℓ,_

R [Y | X] ≤ R [Y | Z] . (25)

For the case of strictly proper losses (Assumption 2) we can go one step further.

**Lemma 2. Let Z −** _X −_ _Y be a Markov chain of random variables. Then, under Assumptions 1_
_and 2 we have that_

R [Y | Z] = R [Y | X] _⇐⇒_ _∀h[∗]_ _∈HZ[∗]_ _[,][ ∀][(][x, z][)][ ∈]_ [supp(][p][X,Z][)][, h][∗][(][z][) =][ f][ ∗][(][x][)][.] (26)

_Proof. Suppose that for all h[∗]_ _∈HZ[∗]_ [we have][ h][∗][(][z][) =][ f][ ∗][(][x][)][ on the support of][ p][X,Z][. Then,]

R [Y | X] = EpX,Y [ℓ(Y, f _[∗](X))]_ (27)

= EpX,Y pZ | X [ℓ(Y, f _[∗](X))]_ (28)

= EpX,Y pZ | X [ℓ(Y, h[∗](Z))] (29)

= EpZ,Y [ℓ(Y, h[∗](Z))] (30)

= R [Y | Z] . (31)

Now suppose there exists a h[∗] _∈HZ[∗]_ [and a pair][ (][x][′][, z][′][)][ ∈] [supp(][p][X,Z][)][ such that][ h][∗][(][z][′][)][ ̸][=][ f][ ∗][(][x][′][)][.]
Then

R [Y | Z] (32)
= EpX,Z _pY_ _X_ [ℓ(Y, h[∗](Z))] (33)
_|_


= pX,Z(x[′], z[′]) EpY | x′ [ℓ(Y, h[∗](z[′]))] +

_≥_ _pX,Z(x[′], z[′]) EpY | x′ [ℓ(Y, h[∗](z[′]))] +_

_> pX,Z(x[′], z[′]) EpY | x′ [ℓ(Y, f_ _[∗](x[′]))] +_


_pX,Z(x, z) EpY_ _x_ [ℓ(Y, h[∗](z))] (34)
_|_
(x,z)X=(̸ _x[′],z[′])_

_pX,Z(x, z) EpY | x_ [ℓ(Y, f _[∗](x))]_ (35)
(x,z)X=(̸ _x[′],z[′])_


_> pX,Z(x[′], z[′]) EpY | x′ [ℓ(Y, f_ _[∗](x[′]))] +_ _pX,Z(x, z) EpY | x_ [ℓ(Y, f _[∗](x))]_ (36)

(x,z)X=(̸ _x[′],z[′])_

= R [Y | X] (37)

Eq. (35) follows by Item 3 of Assumption 2 along with the definition of f _[∗]. Eq. (36) follows by Item 3_
of Assumption 2 and the fact that h[∗](z[′]) ̸= f _[∗](x[′]). This completes the proof, because Lemma 1_
prevents R [Y | Z] < R [Y | X].

B.2 PROOF OF THEOREM 1

First we will show that the desired representation exists by taking all inputs for which the Bayes
predictor predicts similarly and “bucketing” them to the same representation. This is a direct extension
of the example from Dubois et al.’s (2020) Proposition 6, to the case of proper losses.

**Proposition 3 (Existence of optimal representations). Under Assumptions 1 to 5, there exists an**
_encoder pZ∗_ _| X that is optimal for Eq. (3), i.e.,_

_pZ∗_ _X_ arg min s.t. _d_ _, supp(pZ_ _d) = supp(pZ)._ (38)
_|_ _∈_ _pZ | X_ [R [][Y][ |][ Z][]] _∀_ _∈D_ _|_

_Moreover, we have that_
R [Y | X] = R [Y | Z _[∗]] ._ (39)


-----

_Proof. Because we assume arbitrary encoders pZ | X_, the essence of this construction is simple: we
embed the Bayes image into Z. Indeed, let φ : Γ[∗] _→Z be any one-to-one function, which exists due_
to Assumption 3 (here we use deterministic one-to-one function for simplicity, the construction can
be easily extended to stochastic case). Then let Z _[∗]_ = φ(f _[∗](X)). We now verify the properties of_
_pZ∗_ _| X_ .

1. Z _[∗]_ satisfies R [Y | X] = R [Y | Z _[∗]]. Indeed,_

R [Y | X] = EpX,Y [ℓ(Y, f _[∗](X))]_ (40)

= EpX,Y pZ∗| X [ℓ(Y, f _[∗](X))]_ (41)

= EpZ∗ _,Y_ _ℓ(Y, φ[−][1](Z_ _[∗]))_ (42)

R [Y _Z_ _[∗]] ._ (43)

 

_≥_ _|_

Eq. (42) is by our construction of Z _[∗]_ and Eq. (43) is by the definition of the Bayes risk.
Due to the data processing inequality of Bayes risk (Lemma 1) we also have R [Y | X] ≤
R [Y | Z _[∗]], from which we conclude that R [Y | X] = R [Y | Z_ _[∗]] and that Eq. (39) holds._

2. Recall that Γ[∗] = f _[∗]_ _[→](X_ ) and Γ[∗]d [=][ f][ ∗] _[→][ ]supp(pX | d)_ . Now let us compute the desired
support for all d ∈D: 

supp(pZ∗ _| d) = φ→(Γ∗d[)]_ (44)

= φ→(Γ∗) (45)
= supp(pZ∗ ). (46)


Eq. (45) is by Assumption 5.

Because R [Y | X] is the minimum achievable risk by any encoder regardless of constraint (this is by
Lemma 1), this implies that pZ∗ _X is an optimal encoder for Eq. (3)._
_|_

The following lemma essentially says that when R [Y | Z] is minimized, then the optimal predictors
for each domain all agree on the intersection of their support.

**Lemma 3. Let pZ** _X be an encoder such that R [Y_ _Z] = R [Y_ _X]. Under Assumptions 1 and 2,_
_|_ _|_ _|_
_we have that for all z_ supp(pZ), there exists γ[∗] Γ such that
_∈_ _∈_

EpY _z_ [ℓ(Y, γ[∗])] < EpY _z_ [ℓ(Y, γ)] _γ_ = γ[∗]. (47)
_|_ _|_ _∀_ _̸_

_In other words, the restriction of any h[∗]_ _∈HZ[∗]_ _[to][ supp(][p][Z][)][ is unique. If, in addition, Assumption 4]_
_holds, then for all (z, d)_ supp(pZ,Dt ), γ Γ such that γ = h[∗](z),
_∈_ _∈_ _̸_

EpY _z,d_ [ℓ(Y, h[∗](z)] < EpY _z,d_ [ℓ(Y, γ)] . (48)
_|_ _|_

_In other words, the restriction of any h_ _Z,d_ _[to][ supp(][p]Z_ _d[)][ is unique and equal to][ h][∗][.]_
_∈H[∗]_ _|_

_Proof. For the first result, let z ∈_ supp(pZ) and consider x ∈ supp(pX | z). By Lemma 2, it must be
the case that f is constant on supp(pX _z). Thus, we can pick γ[∗]_ = f (x). Now, let γ = γ[∗]. We

_[∗]_ _|_ _[∗]_ _̸_
have that,

EpY _z_ [ℓ(Y, γ[∗])] = EpX _zpY_ _X_ [ℓ(Y, γ[∗])] (49)
_|_ _|_ _|_

= EpX | zpY | X [ℓ(Y, f _[∗](X))]_ (50)

_< EpX_ _zpY_ _X_ [ℓ(Y, γ)] (51)
_|_ _|_

= EpY _z_ [ℓ(Y, γ)] . (52)
_|_

Eq. (49) is due to the conditional independence of Y and Z given X. Eq. (51) is due to Assumption 2
and the definition of the Bayes predictor. Let h[∗] : supp(pZ) Γ be the unique Bayes predictor
_→_
from Z.

Now, for the second result, note that

R [Y | X] = Rf _[∗]_ [Y | X] (53)


-----

_pDt_ (d) R[d]f _[∗]_ [[][Y][ |][ X][]] (54)
_dX∈D_

_pDt_ (d) R[d] [Y _X],_ Assumption 4 (55)
_|_
_dX∈D_


and

R [Y | Z] = Rh[∗] [Y | Z] (56)

= _pDt_ (d) R[d]h[∗] [[][Y][ |][ Z][]] (57)

_dX∈D_

_pDt_ (d) R[d] [Y _Z],_ (58)

_≥_ _|_

_dX∈D_

where Eq. (58) is due to the definition of (domain-conditional) Bayes risk. Then


R [Y _Z]_ R [Y _X]_ _pDt_ (d) R[d] [Y _Z]_ R[d] [Y _X]_ (59)
_|_ _−_ _|_ _≥_ _|_ _−_ _|_

_d_

X∈D  

_≥_ 0. Lemma 1 conditioned on d
(60)


Thus, any encoder that achieves R [Y | Z] = R [Y | X] also satisfies R[d] [Y | Z] = R[d] [Y | X] for
all d since we assume that supp(pDt ) = in Assumption 1. Now, let d . An argument
_∈D_ _D_ _∈D_
analogous to Lemma 2 gives us,

_h_ _Z,d[,][ ∀][(][x, z][)][ ∈]_ [supp(][p]X,Z _d[)][, h][(][z][) =][ f][ ∗][(][x][) =][ h][∗][(][z][)][.]_ (61)
_∀_ _∈H[∗]_ _|_

Eq. (61) is derived from R[d] [Y | Z] = R[d] [Y | X] using Assumption 4 in place of Item 3 of Assumption 2 for a specific domain d. Let z ∈ supp(pZ | d) and γ ∈ Γ such that γ ̸= h[∗](z). Since
supp(pX _z,d)_ supp(pX _z), f_ is a constant on supp(pX _z,d) and equal to h[∗]. Now, as above,_
_|_ _⊆_ _|_ _[∗]_ _|_
we have that

EpY _z,d_ [ℓ(Y, h[∗](z))] = EpX _z,dpY_ _X,d_ [ℓ(Y, h[∗](z))] (62)
_|_ _|_ _|_

= EpX | z,dpY | X,d [ℓ(Y, f _[∗](X))]_ (63)

_< EpX_ _z,dpY_ _X,d_ [ℓ(Y, γ)] (64)
_|_ _|_

= EpY _z,d_ [ℓ(Y, γ)] . (65)
_|_

Eq. (64) is due to Assumption 4.

**Corollary 1. Let pZ** _X be an encoder such that R [Y_ _Z] = R [Y_ _X]. Under Assumptions 1, 2_
_|_ _|_ _|_
_and 4 we have that_ _Z_ _Z,d_ _[for all][ d][ ∈D][ and that for all][ d][s][, d][t]_
_H[∗]_ _[⊆H][∗]_ _[∈D]_

inf R[d]h[t] [[][Y][ |][ Z][] = R][d][t][ [][Y][ |][ Z][]] (66)
_h∈HZ,ds[∗]_

_Proof.the result follows by taking any HZ[∗]_ _[⊆H]Z,d[∗]_ [is immediate from Lemma 3. Now, we have that] h ∈HZ[∗] _[⊆H]Z,ds[∗]_ [in the][ inf][ of Eq. (66).][ R]h[d][t] [[][Y][ |][ Z][]][ ≥] [R][d][t][ [][Y][ |][ Z][]][. So,]

**Theorem 2 (Characterizing optimal representations for IDG, equiv. Theorem 1). Under Assump-**
_tions 1 to 6, an encoder pZ_ _X is optimal for idealized domain generalization if and only if it minimizes_
_|_
_the Bayes risk while matching the support of pZ_ _d and pZ for all d_ _, i.e.,_
_|_ _∈D_

_pZ_ _X_ arg min (67)
_|_ _∈_ _pZ | X_ [R [][Y][ |][ Z][]]

s.t. _∀_ _d ∈D, supp(pZ | d) = supp(pZ)_ (68)

_Proof. The IDG risk is lower bounded by R [Y | X]:_


inf R[D]h _[t]_ [[][Y][ |][ Z][]]
_h∈HZ,Ds[∗]_


RIDG [Y _Z]_ EpDs,Dt
_|_ _≥_


(69)


-----

EpDs,Dt R[D][t] [Y _Z]_ (70)
_≥_ _|_

EpDs,Dt R[D][t] [Y _X]_ Lemma 1 (71)
_≥_ _|_

= R [Y _X]_ Assumption 4 (72)

 

_|_

We will now show that this lower bound is achieved by an encoder if and only if it satisfies Eqs. (67)
and (68), which exist by Prop. 3.

**Sufficiency (** = ): Let pZ _X be an encoder that satisfies Eqs. (67) and (68). Note that R [Y_ _Z] =_
_⇐_ _|_ _|_
R [Y | X] by Prop. 3. Let h[∗] _∈HZ[∗]_ [, then we have the following IDG risk]

RIDG [Y | Z] (73)


= EpDs,Dt

= EpDs,Dt


_h∈HsupZ,Ds[∗]_ EpZ,Y | Dt [ℓ(Y, h(Z))]#

_h∈HsupZ,Ds[∗]_ EpZ,Y | Dt [ℓ(Y, h[∗](Z))]


(74)

Lemma 3 under matching support (75)


= EpDt EpZ,Y | Dt [ℓ(Y, h[∗](Z))] constant w.r.t Ds (76)

= R [Y |h Z] = R [Y | X] i (77)

**Necessity ( =⇒** ): If the IDG risk is R [Y | X], then it must be the case that

R [Y | Z] = R [Y | X] (78)

_h∈HsupZ,ds[∗]_ R[d]h[t] [[][Y][ |][ Z][] = R][d][t][ [][Y][ |][ Z][]] _∀(ds, dt) ∈_ supp(pDs,Dt ) (79)

We will prove by contrapositive that Eq. (79) implies support match (Eq. (68)). Suppose that the
support match does not hold. Since supp(pZ) = ∪d∈Dsupp(pZ | d) and supp(pDs,Dt ) = D × D
(Assumption 6), there must exist (ds, dt) ∈ supp(pDs,Dt ) such that supp(pZ | ds ) ̸= supp(pZ | ds ).

Define the set S = supp(pZ | ds ) ∩ supp(pZ | dt ) and _S[¯] = supp(pZ | dt_ ) \ supp(pZ | ds ), let ρ =
_PZ | dt_ (S), and let h[∗] _∈HZ[∗]_ [. Then,]

sup R[d]h[t] [[][Y][ |][ Z][]] (80)
_h∈HZ,ds[∗]_

= _h∈HsupZ,ds[∗]_ _ρ EpY,Z | S,dt [ℓ(Y, h(Z))] + (1 −_ _ρ) EpY,Z | ¯S,dt_ [[][ℓ][(][Y, h][(][Z][))]] (81)

= _h∈HsupZ,ds[∗]_ _ρ EpY,Z | S,dt [ℓ(Y, h[∗](Z))] + (1 −_ _ρ) EpY,Z | ¯S,dt_ [[][ℓ][(][Y, h][(][Z][))]] Lem. 3 (82)

= ρ EpY,Z | S,dt [ℓ(Y, h[∗](Z))] + (1 − _ρ)_ _h∈HsupZ,ds[∗]_ EpY,Z | ¯S,dt [[][ℓ][(][Y, h][(][Z][))]] (83)

= R[d][t] [Y | Z] + (1 − _ρ)_ _h∈HsupZ,ds[∗]_ EpY,Z | ¯S,dt [[][ℓ][(][Y, h][(][Z][))][ −] _[ℓ][(][Y, h][∗][(][Z][))]]_ (84)

_> R[d][t]_ [Y | Z] Lem. 3 (85)

Eq. (85) uses the following reasoning. 1 − _ρ > 0 due to support mismatch. For any h ∈HZ,ds[∗]_ [such]
that h ̸= h[∗] on _S[¯] (such an h exists by Item 1 of Assumption 2), we have that_

EpY,Z | ¯S,dt [[][ℓ][(][Y, h][(][Z][))][ −] _[ℓ][(][Y, h][∗][(][Z][))]][ >][ 0]_ (86)

by Lemma 3.

As a corollary from the proof strategy we directly have that the optimal DG risk is simply R [Y | X].
This means that using the optimal encoder one can actually perform just as well by training on the
source as if you were to directly train on the target using the raw data.

**Corollary 2 (Optimal IDG Risk). Under Assumptions 1 to 6, inf** _pZ | X RIDG [Y | Z] = R [Y | X]._


-----

B.3 IMPOSSIBILITY RESULTS

As a direct corollary of Thm. 2 we know that it is impossible to learn an optimal representation
without knowledge or assumptions on the target domain. We can actually prove the following much
stronger negative result, which essentially states that it is impossible to find a useful representation
without having some information about the target domain. Specifically, we prove that if there exists a
non-trivial target domain on which the representation is advantageous then there exists an infinite
amount of target domains on which it is disadvantageous compared to predicting from a constant.

For clarity, we will focus on the proof for the standard accuracy (0-1 loss) which is much shorter
and simpler to understand, but note that we can generalize the proof to all losses with the right
assumptions.

The key is that outside of the source domain, the label distribution is unconstrained because generalized covariate shift has no effect. In other words, for any domain which gives some probability mass
on an example that has not been seen during training, then all possible labels for that example gives
a valid domain. Furthermore, if there exists one domain on which the representation is good, then
one can construct a domain on which the representation is bad simply by labelling this point as the
constant prediction.

**Proposition 4 (No free lunch for learning representations for IDG, equiv. Proposition 1). Let ℓ** _be_
_the 0-1 loss with prediction space Γ = Y. Let Rep : P(X_ _, Y) →P(Z|X_ ) be any algorithm for
_choosing an encoder pZ_ _X from the data distribution pX,Y, C be any constant r.v. that t.v.i._ _, and_
_|_ _Z_
_pX,Y | ds be any desired source distribution such that_

-  there is a unique constant prediction γC = arg miny∈Y EpY | ds [ℓ(Y, y)],

-  and |X \ supp(pX | ds )| > 1.

_Let pZds_ _X := Rep(pX,Y_ _ds_ ) be the chosen source encoder. If there exists a target domain pX,Y _d[g]t_
_|_ _|_ _|_
_such that_

-  (Non-trivial support) ∅̸= supp(pX | d[g]t [)][ ⊆X \][ supp(][p][X][ |][ d][s] [)][;]

-  (Satisfies Bayes image invariance) Γ[∗]d[g]t [=][ Y][, i.e., there is at least one example for every]
_possible label;_

-  (Source encoder is useful) pZds | X performs better than a constant representation,

sup R[d]ht[g] [[][Y][ |][ Z]ds []][ <] sup R[d]ht[g] [[][Y][ |][ C][]][,] (87)
_h∈HZds,ds[∗]_ _h∈HC,ds[∗]_


_Then there exist multiple target domains d[b]t_ _[such that][ p]Zds_ _X_ _[underperforms a constant encoder,]_
_|_

sup R[d]ht[b] [[][Y][ |][ Z]ds []][ >] sup R[d]ht[b] [[][Y][ |][ C][]][ .] (88)
_h∈HZds,ds[∗]_ _h∈HC,ds[∗]_

_Proof. Let h[∗]_ _Zds,ds_ [be any source Bayes predictor corresponding to our encoder. Partition][ Z]
_∈H[∗]_
according to whether h[∗] predicts like the constant or not:

_C :=_ _z_ _h[∗](z) = γC_ =C := _C._ (89)
_Z_ _{_ _∈Z |_ _}_ _Z_ _̸_ _Z \ Z_

We know by assumption that d[g]t [is s.t.]

sup R[d]ht[g] [[][Y][ |][ Z]ds []][ <] sup R[d]ht[g] [[][Y][ |][ C][]][,] (90)
_h∈HZds,ds[∗]_ _h∈HC,ds[∗]_

which is clearly only possible if
_PZds_ _d[g]t_ [(][Z] _[̸][=][C][)][ >][ 0][.]_ (91)
_|_

In other words, there exists some input x=C supp(pX _ds_ ) that will get represented outside of
the constant region, i.e., _̸_ _∈X \_ _|_
_PZds | x≠_ _C_ (Z ≠ _C) > 0._ (92)


-----

We will now construct the desired bad domain d[b]t [by giving nearly all mass to this][ x][̸][=][C][, specifically,]
let pX _dbt_ [(][x][̸][=][C][) = 1][ −] _[δ][ for some][ 0][ < δ <][ 1][. We assign this example to the constant label, i.e.,]_
_|_
_pY | x≠_ _C_ _,dbt_ [(][γ][C][) = 1][. The rest of the target domain mass][ δ][ is distributed as with the source domain,]
i.e., pX,Y _dbt_ [(][x, y][) =][ δ][ ·][ p][X,Y][ |][ d][s] [(][x, y][)][ for all][ x, y][ ∈] [supp(][p][X,Y][ |][ d][s] [)][. Importantly, the constructed]
_|_
domain d[b]t [is valid. Indeed, the Bayes image is the same as the source’s (Assumption 5), because]
we removed no prediction γ from the source’s Bayes image (δ > 0). We added no new prediction γ,
because f _[∗](x≠_ _C) = γC ∈Y which must already have been in Γ[∗]_ due to the validity of d[g]t [.]

Now let us compute the desired risk for that “bad” domain and show that the desired encoder performs
worse than a constant encoder.

sup R[d]ht[b] [[][Y][ |][ Z]ds []] (93)
_h_
_∈HZds,ds[∗]_

= _h_ sup (1 − _δ) EpZds | x≠_ _C [1 −_ 1[γC = h(Zds )]] + δ R[d]h[s] [[][Y][ |][ Z][d]s []] (94)
_∈HZds,ds[∗]_

_≥_ (1 − _δ)(1 −_ _PZds | x≠_ _C_ (Z _C))_ (95)

= (1 − _δ)PZds | x≠_ _C_ (Z ≠ _C)_ (96)

In contrast, it is easy to show that suph∈H∗C,ds [R]h[d]t[b] [[][Y][ |][ C][]][ ≤] _[δ][ because the constant predictor would]_

be perfect for x≠ _C. So any choice of 0 < δ <_ 1+PPZdsZds | x | x≠ ≠C (CZ (Z≠ ≠CC) ) [, would satisfy Eq. (88). We]

conclude the proof by noting that there are infinitely many such choices of δ, and any choice of those
would result in a different valid bad domain d[b]t [.]

Note that representations can often be much worse than using a constant r.v. Specifically, if an
encoder pZ _X maps an x outside of the source support then there exists an infinite number of target_
_|_
domains where that representation is the worst possible representation.

**Proposition 5 (Worst representation). Let Rep, pY,X | ds** _, pZds | X_ _, ℓ_ _be as in Prop. 4, and ϵ > 0. If_
_there exists an example xb ∈X \ supp(pX | ds_ ) that is mapped outside of the source support, i.e.,
supp(pZds | xb ) ∩ supp(pZ | ds ) = ∅, then there exist many target domains pX,Y | dt s.t. pZds | X is ϵ
_close to the worst possible loss, i.e.,_

sup R[d]h[t] [[][Y][ |][ Z][d]s []][ ≥] [1][ −] _[ϵ.]_ (97)
_h_
_∈HZds,ds[∗]_

_Proof. By assumption there exists an xb whose support is outside the source support. Then similarly_
to Prop. 4 we construct a bad target domain dt by giving nearly all mass to that example pX _dt_ (xb) =
_|_
1 − _δ where δ > 0 and assign with probability 1 to some label that is in the source Bayes image,_
i.e., pY _xb,dt_ (γb) = 1 for some γb Γ[∗]ds[. The rest of the target domain mass][ δ][ is distributed as in]
Prop. 4 to the source inputs. As in Prop. 4, such a target domain | _∈_ _dt satisfies our assumptions. Now let_
us compute the risk for that dt and show that the desired encoder performs arbitrarily bad.

sup R[d]h[t] [[][Y][ |][ Z][d]s []] (98)
_h_
_∈HZds,ds[∗]_

= _h_ sup (1 − _δ) EpZds | xb [1 −_ 1[γb = h(Zds )]] + δ R[d]h[s] [[][Y][ |][ Z][d]s []] Eq. (94) (99)
_∈HZds,ds[∗]_

_≥_ _h_ sup (1 − _δ) EpZds | xb [1 −_ 1[γb = h(Zds )]] (100)
_∈HZds,ds[∗]_

= 1 − _δ_ (101)
Eq. (101) uses the fact that _Zds,ds_ [is unconstrained outside of the source support and that by]
_H[∗]_
assumption supp(pZds _xb_ ) supp(pZds _ds_ ) = . To achieve the sup 1 _δ it then suffices to predict_
anproof by noting that there is an infinite possible choices of γ ̸= γb ∈ Γ. We thus see that Eq. (97) holds for | _∩_ _|_ _∅_ _dt as long as δ each of which give rise to a bad target 0 < δ < ϵ −_ . We conclude the
domain.


-----

B.4 AUGMENTATIONS

Proposition 2 shows that the optimal representations for IDG can be learned with augmentations in a
self-supervised fashion. Here, we provide formal definitions, assumptions, and proofs.

**Definition 6 (Augmenter). An augmenter is a conditional distribution pA** _X :_ [0, 1] from
_|_ _A × X →_
the input space X to an augmentation space A. For example, in CLIP X is the space of images and A
is the space of text. In standard SSL, A is typically the same as X (e.g., both X and A are the space
of images).

**Definition 7 (Augmentation conditional set). Given an augmenter pA | X**, define the augmentation
conditional set as the set of conditionals of A given X:

_P_ _[∗](A | X) :=_ _pA | x | x ∈X_ (102)

Similarly, we can define the augmentation conditional set for domain d:



_d_ [(][A][ |][ X][) :=] _pA_ _x_ _x_ supp(pX _d)_ (103)
_P_ _[∗]_ _|_ _|_ _∈_ _|_

These sets are clearly countable. Note that the augmentation conditional set can be seen as a special



case of the Bayes image (Def. 3) if we view the augmentation A as the label and consider the log-loss
where the conditional distribution is the Bayes optimal predictor due to its strict properness (Gneiting
& Raftery, 2007).

**Assumption 7 (Finite augmentation entropy). We consider the augmenter pA** _X such that the entropy_
_|_
of the augmentation A is finite, i.e., H[A] < ∞.

**Assumption 8 (Cardinalities). We assume that**

_|Z| ≥|P_ _[∗](A | X)|_ (104)

This is a similar assumption as Assumption 3, which ensures the existence of optimal representations.

**Assumption 9 (Domain-agnostic augmentation). We assume that the augmentation A is domain-**
_agnostic, i.e., the augmentation conditional set is invariant across domains,_

_Pd[∗][(][A][ |][ X][) =][ P]_ _[∗][(][A][ |][ X][)][,]_ _∀d ∈D_ (105)

This assumption is generalized from the constant Bayes image assumption (Assumption 5), which
guarantees the existence of optimal representations.

Domain-agnostic augmentations essentially ensures that each augmentation conditional pA | x ∈
_P_ _[∗](A | X) is seen at least once in all domains. If we introduce an equivalence relation ∼_ as x ∼ _x[′]_
iff pA | x = pA | x′ and the equivalence class [x] := {x[′] _∈X | x[′]_ _∼_ _x}. Under this relation, it is_
easy to see that the above assumption is satisfied if and only if, for all possible equivalence classes

[x] ∈{[x[′]] | x[′] _∈X}, we have that [x] has intersections with all domains:_

[x] ∩ supp(pX | d) ̸= ∅, _∀d ∈D_ (106)

Not all augmentations are domain-agnostic. In particular, the standard image augmentations used by
typical SSL models like SimCLR are not domain-agnostic, but the text-image augmentations of CLIP
nearly are, as discussed in the main body (Sec. 4).

**Assumption 10 (Bayes-preserving augmentation). We assume that the augmentation A is Bayes-**
_preserving, i.e., ∀x, x[′]_ _∈X_,

_pA | x = pA | x′ =⇒_ _f_ _[∗](x) = f_ _[∗](x[′])._ (107)

Under the notion of equivalence relation in Assumption 9, this means that for each equivalence class

[x], all x[′] _∈_ [x] have the same Bayes prediction. Note that most augmentations used in practice like
standard image augmentations are Bayes-preserving.

Next, we show that under the above assumptions, we can learn optimal representations by maximizing
the mutual information I[A; Z] (in the case of log-loss ℓ) under the support match constraint. We use
log-loss simply because it is typically the loss used for training in practice. Note that the learned
representations are optimal for any strict proper losses.


-----

**Proposition 6 (Learning optimal representations without labels, equiv. Proposition 2). Let pA** _X be_
_|_
_an augmenter. Under Assumptions 1 to 10, any encoder pZ_ _X such that_
_|_

_pZ_ _X_ arg max (108)
_|_ _∈_ _pZ | X_ [I[][A][;][ Z][]]

s.t. _∀_ _d ∈D, supp(pZ | d) = supp(pZ)_ (109)

_is optimal for idealized domain generalization._

_Proof. The support match constraint Eq. (109) is equivalent to the support match constraint Eq. (68)._
Thus, Prop. 3 and Thm. 2 state that we only need to prove that maximizing the mutual information of
_A and Z under the support constraint implies that_

R [Y | Z] = R [Y | X] . (110)

We will prove this by constructing an optimal predictor h[∗].

Since H[A] < ∞ (Assumption 7) we have that

arg max (111)
_pZ | X_ [I[][A][;][ Z][] = arg min]pZ | X [H[][A][ |][ Z][]][ .]

Note the fact that the conditional entropy is the Bayes risk under the log-loss (Gneiting & Raftery,
2007), i.e., H[A | Z] = R [A | Z]. By construction, A satisfies covariate shift w.r.t. X (thus Bayes
invariant) since A − _X −_ _D forms a Markov chain. Together with Assumptions 1 and 7 to 9, it means_
that the optimization problem in Eqs. (108) and (109) satisfies the assumptions of Prop. 3, with A in
place of Y . Thus, an optimal encoder satisfies R [A | Z] = R [A | X], which leads to

H[A | Z] = H[A | X] . (112)

By Assumption 7, we can invoke Lemma 2 with the fact that A − _X −_ _Z forms a Markove chain to_
show that for all (x, z) supp(pX,Z)
_∈_

_pA_ _z = pA_ _x,_ (113)
_|_ _|_

as the conditional distributions are the Bayes optimal predictors due to strict properness of log-loss.

Now, define the following equivalence relation on X,

_x ∼_ _x[′]_ _⇐⇒_ _pA | x = pA | x′_ _._ (114)

Because the number of equivalence classes under ∼ is countable, there exists a maximal invariant
_M : X →_ N from X to the natural numbers (for our definition of a maximal invariant see Definition
2, Dubois et al., 2021). By Assumption 10, f _[∗]_ is invariant on the equivalence classes [x] := {x[′] _∈_
_X | x[′]_ _∼_ _x} for all x ∈X_ . Thus, there exists a function g : N →A such that f _[∗]_ = g ◦ _M_
(Lemma 5, Dubois et al., 2021). Given z supp(pZ), we construct h[∗] in the following way. Let
_∈_
_xz ∈_ supp(pX | z) be any input point that could have led to this representation z and define

_h[∗](z) = g(M_ (xz)). (115)

By Eq. (113) we are guaranteed that all x supp(pX _z) share the same value for f_ since they are
_∈_ _|_ _[∗]_
in the same equivalence class. Thus, by the definition of M we have that


_M_ (xZ) = M (X) for (X, Z) _pX,Z._ (116)
_∼_

Rh[∗] [Y | Z] = EpY,Z [ℓ(Y, h[∗](Z)] (117)

= EpY _X_ _pX,Z_ [ℓ(Y, h[∗](Z)] (118)
_|_

= EpY _X_ _pX,Z_ [ℓ(Y, g(M (xZ)))] Eq. (115) (119)
_|_

= EpY _X_ _pX,Z_ [ℓ(Y, g(M (X)))] Eq. (116) (120)
_|_

= EpY | X _pX,Z_ [ℓ(Y, f _[∗](X))] = R [Y | X] ._ (121)


Therefore,


-----

C PRACTICAL OBJECTIVES

Proposition 6 provides an objective to obtain the desired optimal representations, compared to
Thm. 2 it is more practical in that it does not require direct access to the labels and in that it can use
augmentations under appropriate assumptions. There are nevertheless multiple remaining issues for
deriving objectives that can be trained with in practice. Specifically, (i) the support constraint is hard
to satisfy in practice; (ii) mutual information I[A; Z] is hard to estimate from samples (Poole et al.,
2019); (iii) the objective is constrained which is harder to optimize. We will now show different
objectives and variational bounds of them that do not suffer from these issues, and could still recover
the desired encoders in their optima. In contrast to the proofs of main theoretical results (previous
section), here the derivations will be less formal.

As we have seen in Proposition 6, the optimal representation achieves I[A; Z] = I[A; X]. In the
following, we will rewrite the objective as the constrained optimization:

_pZ_ _X_ arg min (122)
_|_ _∈_ _pZ | X_ [B[][Z, X, Y, D][]]

s.t. I[A; Z] = I[A; X] (123)

where we introduce the domain bottleneck B[Z, X, Y, D] as the objective for enforcing support
match (which we denote as B[Z, D] in the main body for simplicity). The requirement on the domain
bottleneck objective is that minimizing Eq. (122) under Eq. (123) implies that the support match
constraint holds (and can be achieved by some encoder), which leads to optimal representations for
IDG. Different domain bottlenecks will be derived later this section. We can then use Lagrangian
relaxation to get the following unconstrained objectives.

arg min I[A; Z] + λ B[Z, X, Y, D] (124)
_pZ | X_ _−_

The first term can be easily optimized using variational bounds on MI. Throughout the paper, we will
use a contrastive variational lower bound which is based on InfoNCE (Oord et al., 2018). Namely, let
_X be the input sample and A be the ‘positive’ augmentation sampled fromn_ _n pA | X_ . We then obtain n
‘negative’ augmentations _A[−]i_ _i=1_ [by first independently sampling] _Xi[−]_ _i=1_ [from the marginal][ p][X]
and then sampling A[−]i [from] _[ p]A | Xi[−][. It is easy to see that the negatives]_ _[ A]i[−]_ [follow the marginal][ p][A][.]
We construct A := _A, A[−]1_ _[, . . ., A]n[−]_ . Let Z be the representation of X by passing it through the
encoder pϕ := pZ _X parameterized by ϕ and sψ the critic function parametrized by ψ used to score_
_|_ 
which A[′] _∈_ **A is the positive augmentation. Then we have the following variational lower bound**
(Poole et al., 2019):


log exp sψ(A, Z)

_A[′]_ **A** [exp][ s][ψ][(][A][′][, Z][)]
_∈_

P


(125)


I[A; Z] ≥ log(n + 1) + EpA,X,Z


In the case of unconstrained variational families sψ, pϕ and infinite samples (n →∞), the above
variational bound recovers I[A; Z] up to a constant (see Oord et al. (2018); Dubois et al. (2021)).
Typically the critic is separable, i.e., sψ(A, Z) := gψ(A)[T] _hψ(Z). As discussed in the main body, it_
can be tied with the encoder pϕ when A = X .

In the following we focus on the second term B[Z, X, Y, D] and discuss several choices.

Throughout this section, the function M : X → N is the maximal invariant defined in Prop. 6 via the
equivalence relation defined in Eq. (114).

C.1 MUTUAL INFORMATION BOTTLENECK B[Z, X, Y, D] = I[Z; X]

The first bottleneck we consider is so called mutual information (MI) bottleneck B[Z, X, Y, D] =
I[Z; X], which was introduced by Tishby et al. (2000) to achieve a tradeoff between the predictive
power and the complexity of representations. Intuitively, it tries to remove all information of Z that
is not needed for maximizing I[Z; A]. In particular, using the fact that Z − _X −_ _D forms a Markov_
chain and the chain rule of MI, we have I[Z; X] = I[Z; X, D] = I[Z; D] + I[Z; X | D]. Thus, it
not only minimizes I[Z; D], i.e., matches the representations’ distribution across domains, but also
minimizes I[Z; X | D], i.e., matches the representations’ distribution inside domains.


-----

**Why** The key to show is that minimizing Eq. (122), i.e., arg minpZ | X I[Z; X] under I[A; Z] =
I[A; X], implies the support match constraint. This can be seen as a specific subcase of Dubois
et al.’s (2021) Corollary 15 with A in place of Y and M (X) induced by pA _X as in the proof of_
_|_
Prop. 6. From the corollary, we know that minpZ | X I[Z; X] = H[M (X)] which can be achieved
by any Z s.t. pZ | x = pZ | x′ _⇐⇒_ _M_ (x) = M (x[′]). With the assumption of domain-agnostic
augmentations (Assumption 9), we have that the set of maximal invariant {M (x) | x ∈ supp(pX | d)}
is invariant across domains. Then we directly have supp(pZ | d) = ∪x∈supp(pX | d)supp(pZ | x) =
_∪x∈supp(pX_ )supp(pZ | x) = supp(pZ), where we use the fact that x within the same equivalence
class has the the same pZ | x.

**How** Essentially, we can use any variational upper bound of mutual information. We consider the
one used by Variational Information Bottelenck (Alemi et al., 2016), i.e.,


log _[p][ϕ][(][Z][ |][ X][)]_

_pZ(Z)_

log _[p][ϕ][(][Z][ |][ X][)]_

_qθ(Z)_

log _[p][ϕ][(][Z][ |][ X][)]_

_qθ(Z)_


I[Z; X] = EpX,Z

= EpX,Z

_≤_ EpX,Z


(126)

_−_ DKL[pZ(Z)∥qθ(Z)] (127)

(128)


= EpX [DKL[pϕ(Z | X)∥qθ(Z)]] (129)

where a variational distribution qθ is used to approximate pZ and is jointly optimized with pϕ to
minimize the bound. The approximation gap of the bound is DKL[pZ(Z)∥qθ(Z)]. Ignoring the
constant, the final loss becomes

_LMI(ψ, ϕ, θ) := EpX,A,Z_ _−_ log _A[′]expA_ _s[exp]ψ([ s]A, Z[ψ][(][A])[′][, Z][) +][ λ][ D][KL][[][p][ϕ][(][Z][ |][ X][)][∥][q][θ][(][Z][)]]_ (130)

 _∈_ 

which recovers the optimal encoder in the case of unconstrained variational families forP _pϕ, qθ, sψ,_
infinite samples n →∞, and any λ > 1 (Dubois et al., 2021).

C.2 ENTROPY BOTTLENECK B[Z, X, Y, D] = H[Z]


The entropy (Ent) bottleneck introduced in the main body is a special case of the MI bottleneck,
where the encoder is a deterministic mapping, i.e., pϕ(Z | x) is a dirac delta function for all x ∈X
and we denote by eϕ(x) the deterministic encoder s.t. pϕ(eϕ(x) | x) = 1.

**Why** In the deterministic case, the MI bottleneck becomes the entropy bottleneck because I[X; Z] =
H[Z] − H[Z | X] = H[Z], where we use the fact that H[Z | X] = 0. Importantly, considering only
deterministic encoders does not constrain our ability to learning optimal encoders. Indeed, just as
with the MI bottleneck optimizing the objective with the entropy bottleneck under I[A; Z] = I[A; X]
will recover encoders s.t. eϕ(x) = eϕ(x[′]) ⇐⇒ _M_ (x) = M (x[′]), which also satisfies the support
match constraint as discussed before.

**How** Using the same derivation as the MI bottleneck, we can derive the variational upper bound on
entropy

H[Z] ≤ EpZ [− log qθ(Z)] (131)

which is the standard variational bound used in neural compression (Ballé et al., 2016; Theis et al.,
2017). Putting all together, we have

_LEnt(ψ, θ, ϕ) := EpX,A,Z_ _−_ log _A[′]expA_ _s[exp]ψ([ s]A, Z[ψ][(][A])[′][, Z][)][ −]_ _[λ][ log][ q][θ][(][Z][)]_ (132)

 _∈_ 

which also recovers the optimal encoder with unconstrained variational families, infinite samples, andP
_λ > 1 as with the MI bottleneck. The detialed algorithm is provided in Algorithm 2. Note that the_
discreteness of Z could lead to difficulty of gradient-based optimization, and we follow Ballé et al.
(2016) to add uniform noise to Z as a differentiable substitute for rounding during training. In our
experiments, we will mostly use the Ent bottleneck instead of the MI bottleneck to avoid introducing
stochastic encoders.


-----

**Algorithm 2 Ent objective**


**Require: eϕ, sψ, qθ, X, n**

1: Z _eϕ(X)_
_←_
2: A ← sample(pA |i.i.d. X )

3: {(Xi−[, A]−i [)][}]i[n]=1 _←−−_ sample(pX,A)

4: A _A_ _A−i_ _i=1_
_←{_ _} ∪{_ _[}]exp[n]_ _sψ(A,Z)_

5: aug log
_L_ _←−_ _A[′]_ _∈A_ [exp][ s][ψ][(][A][′][,Z][)][ ▷] _[−]_ [I[][A][;][ Z][]]

6: supp log qPθ(Z) _▷_ H[Z]

7: return L _←− LEnt = Laug + λLsupp_

C.3 CONTRASTIVE ADVERSARIAL DOMAIN BOTTLENECK B[Z, X, Y, D] = I[Z; D]

The previous two bottlenecks require removing the information of Z (about X) as much as possible,
which seems to be unnecessary since our ultimate goal is to match the support of Z across domains.
Now we introduce a bottleneck B[Z, X, Y, D] = I[Z; D] which we only seek to remove the information of Z about the domain D. This is very related to the work on invariant representation learning
for domain generalization/adaptation (e.g., Ganin et al., 2016; Li et al., 2018a). We derive a new
variational bound called the contrastive adversarial domain (CAD) bottleneck that is more stable to
train and leads to better empirical performance. For simplicity we consider the deterministic encoder
_eϕ(x) as with the main body._

**Why** Similar to the previous analysis, we aim to show that arg minpZ | X I[Z; D] under I[A; Z] =
I[A; X] leads to the support match constraint. Using Eq. (116) we have I[Z; D] = I[Z, M (XZ); D] =
I[Z, M (X); D] = I[M (X); D] + I[Z; D | M (X)] where the last equality uses the chain rule of
mutual information. Due to the non-negativity of (conditional) mutual information, we have that
the minimum of I[Z; D] under I[A; Z] = I[A; X] is I[M (X); D]. Then we show the minimum is
achievable by constructing the same optimal encoder eϕ(X) as the Ent bottleneck which clearly
satisfies I[Z; D | M (X)] = 0. It is then easy to show that the support match constraint has to hold
when I[Z; D | M (X)] = 0 by contrapositive. Indeed, suppose that the support constraint does not
hold then it must be true that I[Z; D | M (X)] > 0 and so the encoder cannot be optimal.

**How** The typical way of minimizing I[Z; D] is to derive the variational bound as

I[Z; D] = H[D] − H[D | Z] (133)

= (const) EpD,Z log pD _Z(D_ _Z)_ (134)
_−_ _−_ _|_ _|_

(const) EpD,Z [ log qφ(D _Z)]_  (135)
_≥_ _−_ _−_ _|_

where a variational distribution (or domain classifier) qφ is used to approximate pD _Z and jointly_
_|_
trained to maximize the bound. This recovers the domain-adversarial training method as introduced
in Ganin et al. (2016). However, this has two potential issues: 1) it gives a lower bound instead of
the desired upper bound on I[Z; D]; 2) it requires adversarial training which is not stable in practice
(Goodfellow, 2016; Kodali et al., 2017).

We propose the contrastive adversarial domain (CAD) bottleneck, which is based on the above explicit
version but uses a variational distribution qφ(D | Z) that is tied with other parts of the model, thus no
need to learn a domain classifier. Suppose we have access to a set of inputs X, we first introduce a
contrastive variational distribution qϕ,X(X _Z) of pX_ _Z as_
_|_ _|_

_qϕ,X(X | Z) :=_ _X_ _[′]expX_ _s[exp]ϕ(X, Z[ s][ϕ][(][X])[′][, Z][)]_ (136)

_∈_

where sϕ(X, Z) := eϕ(X)[T] _Z is tied with the encoderP_ _eϕ. Note that qϕ,X has support over X, and_
equals pX _Z when sϕ(X, Z)_ log pX,Z(X, Z) and X recovers . In practice, we use a variety of
_|_ _∝_ _X_
crude approximations. Our first crude approximation is that we use the minibatch of samples, i.e., the
_n_
independently sampled _Xi[−]_ _i=1_ [as][ X][. Now, since][ p][D][ |][ Z][ can be rewritten as][ E][p][X][ |][ Z] _pD | X_ using
the fact that D − _X −_ _Z forms a Markov chain, we obtain the following variational distribution:_ 

_qϕ,X(D_ _Z) = Eqϕ,X_ _pD_ _X_ (D _X)_ (137)
_|_ _|_ _|_
 


-----

which recovers pD _Z when qϕ,X = pX_ _Z. Note that pD_ _X is still not available. For our second_
_|_ _|_ _|_
crude approximation, we use a count estimate ˆpD,X. In particular, we obtain a collection D by taking
each X _[′]_ **X and independently sampling D[′]** from pD _X_ _′ to get D :=_ _Di−_ _i=1[. In other words,]_
_{(Di−[, X]i∈−[)][}]i[n]=1_ [are all i.i.d. sampled from][ p][D,X] [. Then we use a count estimate] | _{_ _[}][n]_

_n_ _−_ _−_
_pˆD,X(d | x) =_ Pi=1 [I]ni[ (]=1[X]i[I][ (][=][X][ x, D]i− [=][ x]i [)][=][ d][)] (138)

which is an accurate estimate with infinite samples. This leads to our final variational distribution:P


_qϕ,X(X_ _[′]_ _Z)ˆpD,X(D_ _X_ _[′])_ (139)
_|_ _|_
_XX[′]∈X_


_qϕ,X,D(D_ _Z) =_
_|_

Putting all together we get that the loss:


_LCAD(ϕ, ψ) := EpD,X,A.Z_ "− log _A[′]exp∈A_ _s[exp]ψ([ s]A, Z[ψ][(][A])[′][, Z][) +][ λ][ log] XX_ _[′]∈X_ _qϕ,X(X_ _[′]_ _| Z)ˆpD,X(D | X_ _[′])!#_

(140)

P

In practice, ˆpD,X(D _X) is typically a dirac delta function since it is rare to have the same samples_
_|_
in a minibatch. Thus, in Eq. (139) we only need to sumthe same domain labelleads to the simplified loss: D as X, i.e., XD := _{Xi−_ _[|][ D]i−_ [=][ D, i] qϕ,[ ∈]X(X[[][n][′][]]|[}] Z[ where]) over those associated with[ [][n][] :=] _[{][1][, . . ., n][}][. This]_

_LCAD(ϕ, ψ) := EpD,X,A,Z_ "− log _A[′]exp∈A_ _s[exp]ψ([ s]A, Z[ψ][(][A])[′][, Z][) +][ λ][ log] XX_ _[′]∈XD_ _qϕ,X(X_ _[′]_ _| Z)!#_ _. (141)_

In practice, we find that the second term that minimizes the log probability leads to numericalP
instability. Intuitively, this could be seen by the exploding gradient of the function log(p) when
_p →_ 0. We thus replace it with − log(1 − _p) which has the same optima. I.e. in practice we_
maximize the log of the probablity summed over X _D := X_ **XD. This reduces Eq. (141) to Eq. (7)**
_¬_ _\_
described in the main body with a detailed algorithm in Algorithm 1. Note that it is easy to generalize
Algorithm 1 to parallel computation within a batch of samples. Indeed, for each sample in the batch,
we can view all other samples in the batch as negatives and compute the loss efficiently in parallel.

C.4 CONDITIONAL CAD B[Z, X, Y, D] = I[Z; D | Y ]

The analysis of the CAD bottleneck also implies that we can minimize the conditional mutual
information I[Z; D | M (X)] if we have access to M (X). However, since M (X) is typically not
available in practice, we consider the special case where M (X) = Y . In particular, this is the case
where the labels are available and the supervised augmentations are used (see Fig. 2b). This reduces
the bottleneck to B[Z, X, Y, D] = I[Z; D | Y ] which is related to the conditional version of the
domain-adversarial neural network (Li et al., 2018b). In practice, minimizing I[Z; D | Y ] could be
easier for optimization than I[Z; D], as it does not require to remove the information that D has about
_Y . In the following, we derive the conditional CAD (C[2]AD) bottleneck using a similar idea as CAD._

**How** In this case, we want to minimize


_LCAD(ϕ, ψ) := EpD,X,A.Z_


I[Z; D | Y ] = H[D | Y ] − H[D | Z, Y ] (142)
= (const) − H[D | Z, Y ] (143)
_≥_ (const) − EpD,Z,Y [− log q(D | Z, Y )] (144)

where q(D _Z, Y ) is a variational distribution of pD_ _Z,Y . Similar to the unconditional case, we_
_|_ _|_
also aim to use a non-parametric approximation that is tied with other parts of the model, and we
obtain it using the fact pD _Z,Y = EpX_ _Z,Y_ _pD_ _X_ . Specifically, let Y be the label of input X
_|_ _|_ _|_
sampled from pY _X and Y :=_ _Y1−[, . . ., Y]n[ −]_
sampling the label fromi.e., XY := _Xi− |_ _i−_ = p Y, iY | X _′{ for each[n]_ and obtain a variational distribution of X _[′]_ _[}]∈[ be the collection of labels obtained by independently]X. We collect samples associated with the label_ _pX_ _Z,Y :_ _Y,_
_{_ _[|][ Y]_ _∈_ _}_ _|_

_qϕ,X,Y(X | Z, Y ) :=_ _X_ _[′]_ expXY s[exp]ϕ(X, Z[ s][ϕ][(][X]) _[′][, Z][)]_ (145)

_∈_

P


-----

where we use the same critic sϕ(X, Z) := eϕ(X)[T] _Z that is tied with the encoder eϕ as before, but_
only take softmax over those samples with the same label Y . For the term pD | X, we use the same
count estimate ˆpD,X in Eq. (138). Then we obtain the variational distribution of pD _Z,Y :_
_|_


_qϕ,X,Y(X_ _[′]_ _Z, Y )ˆpD,X(D_ _X_ _[′])_ (146)
_|_ _|_
_XX[′]∈XY_


_qϕ,X,D,Y(D_ _Z, Y ) =_
_|_


Putting all together we get that the final loss:


exp sψ(A, Z)

_LC2_ AD(ϕ, ψ) := EpD,X,A,Y,Z "− log _A[′]∈A_ [exp][ s][ψ][(][A][′][, Z][) +][ λ][ log] XX _[′]∈XY_ _qϕ,X,Y(X_ _[′]_ _| Z, Y )ˆpD,X(D | X_ _[′])!#_

(147)

P

Again, since in practice ˆpD,X(D _X) is typically a dirac delta function, the summation in Eq. (146)_
_|_
can be done only over those associated with the same label Y and the same domain label D as X,
i.e.,probability summed over XY,D := {Xi− _[|][ Y]i−_ = X Y, DY,D, we maximize the log of the probability summed overi− [=][ D, i][ ∈] [[][n][]][}][. Similarly, instead of minimizing the log of the] XY, _D :=_
_¬_
**XY** **XY,D =** _Xi−_ _i−_ = Y, Di−
_\_ _{_ _[|][ Y]_ _[̸][=][ D, i][ ∈]_ [[][n][]][}][. Finally we obtaine the simplified loss:]


exp sψ(A, Z)

C2 AD(ϕ, ψ) := EpD,X,A,Y,Z log _qϕ,X,Y(X_ _[′]_ _Z, Y )_ _._
_L_ − _A[′]∈A_ [exp][ s][ψ][(][A][′][, Z][)][ −] _[λ][ log]X_ _[′]∈XXY,¬D_ _|_ 

 P  (148)

A detailed algorithm is in Algorithm 3.


**Algorithm 3 conditional CAD (C[2]AD) objective**

**Require: eϕ, sψ, D, X, Y, n**

1: Z ← _eϕ(X)_
2: A ← sample(pA | X ) _n_ i.i.d.

3: (Di[−][, X]i[−][, A]i[−][, Y][ −]i [)] _i=1_ sample(pD,X,A,Y )

4: X _, A ←_ _Xi[−]_ _ni=1[,][{][A][} ∪]←A−−[−]i_ _ni=1_

5: XY _Xi[−]_ _i_ = Y, i [n]
_←_  _[|][ Y][ −]_ _∈_

7:6: L XaugY, ←−¬D ← logXi[−]A[|][ Y][′]exp∈[ −]iA _s[exp]=ψ Y, D([ s]A,Z[ψ]_ [(][A]) _i[−][′][,Z][̸][=][)]_ _[ D, i][ ∈]_ [[][n]▷[]] _−_ I[A; Z]

8: Lsupp ←− log PPXX[′] _∈[′′]X∈Y,XY¬D[exp][exp][ e][ϕ][ e][(][ϕ][X][(][X][′′][)][′][T][)][T][ Z][ Z]_ _▷_ I[Z; D | Y ]

9: return LC2 AD =P Laug + λLsupp


-----

D EXTENDED RELATED WORK

**Provably optimal representations. Many previous work have theoretically studied advantages of**
representations in various two-stage settings (representation learning followed by standard training of
predictors) by bounding downstream performance (e.g., Ben-David et al., 2007; Shamir et al., 2010;
Saunshi et al., 2019). As learning theoretical bounds can be loose, it is hard to know whether they give
the right insights into the problem. Our work instead proves the properties of optimal representations,
which ensure best downstream performance. Those properties need to be approximated but give the
right insights into what to aim for. This perspective and our proofs were inspired by Dubois et al.
(2020) who gives sufficient conditions for optimal representations in supervised learning.

E EXPERIMENTAL DETAILS

E.1 SCIENTIFIC

In both the scientific setting and the following bridge setting, we consider rather unrealistic setups
for verifying our theory where we have access to labels from all domains. We can choose to directly
minimize the risk R [Y | Z] with the cross-entropy loss (denoted as CE henceafter), or minimize
H[A | Z] (i.e., maximize I[A; Z]) with supervised augmentations as in Fig. 2b detailed below.

**Implementation of supervised augmentations** When using supervised augmentations, for each
sample we obtain its augmentations from within its label class across all domains. A constrastive
loss with such augmentations will essentially reduce to the supervised contrast loss (SupCon, Khosla
et al., 2020). In particular, for a single sample in a batch, all samples in the batch with the same
labels can be used as the positives (could come from the same domain or different domains) and
others as the negatives. In Khosla et al. (2020), two variants of SupCon loss were introduced for
solving the issue of multi-positives depending on whether the summation over multi-positives was
located inside (SupCon-In, Eq. (3) in Khosla et al. (2020)) or outside (SupCon-Out, Eq. (2) in Khosla
et al. (2020)) the log. Though Khosla et al. (2020) chose SupCon-Out because it worked better
than SupCon-In, we hypothesized that this is because SupCon-Out has an implicit bottleneck effect.
Intuitively, SupCon-Out upper bounds SupCon-In and achieves its optima only if the logits with
positive samples are all the same by Jensen’s inequality, which may encourage positive samples from
different domains to get clustered. Since this might confound with the effect of our bottlenecks,
we chose to use SupCon-In though it performed slightly worse in out initial experiments. For the
implementation of SupCon, we followed Khosla et al. (2020) except that no projection was used.
Specifically, the temperature was set to 0.1, and normalization was applied when computing the
logits.

In the scientific setting, we tried to simulate our theory to the greatest extent. In particular, we had
two special considerations as detailed below:

**Eliminating empirical generalization** As our theory focuses on the idealized domain generalization that assumes access to population distribution, we considered the setup where the empirical
generalization was eliminated. Specifically, we treated the dataset as the population distribution and
used the same dataset for training the encoder and training/evaluating the predictor. The ResNet-18
encoder was trained to 300 epochs without any regularization, using the Adam optimizer (Kingma &
Ba, 2014) with a learning rate of 5e-5, a batch size of 192 (48 for each domain), and a cosine learning
rate decay schedule.

**Worst-case approximation** To approximate the worst-case source predictor, we included the target
data with randomly assigned wrong labels to the training set for training the source predictor. The
target data samples were down-weighted with a sample weight that maximizes the target risk while
keeping the source risk close to optima (which is 0). We selected the sample weight by sweeping
over [10[−][10], 1] with a logarithmic scale using CE-Base and SupCon-Base, as shown in Fig. 4. As
the sample weight increases, the target log likelihood (neg. risk) first decreases and then increases.
We hypothesized that the increasing trend was due to that the source performance was already not
optimal (though not visible from the figure), thus we selected the weight close to the turning point and
10[−][5] seemed to be reasonable for both CE-Base and SupCon-Base. Although we did not adaptively
select the sample weight for each setup due to the computational cost, the pre-specified sample turned


-----

10 9 10 7 10 5 10 3 10

sample weight

(a) CE-Base


10 9 10 7 10 5 10 3 10

source
target

sample weight

(b) SupCon-Base


Figure 4: Sweeping the sample weight using CE-Base and SupCon-Base. We selected 10[−][5] which
seemed to be reasonble for both cases.

out to be reasonable for all other losses and different λ combinations. Furthermore, we also removed
regularization when training the linear classifier and initialized the linear weight i.i.d. from N (0, 1).

Next, we provide other experimental details for reproducibility:

**Implementation of standard augmentations** We followed SimCLR (Chen et al., 2020) for implementing standard image augmentations. For a fair comparison between the cases when using standard
augmentations (SimCLR) and supervised augmentations (SupCon), we kept the total batch size the
same and also used the same configurations for computing the SupCon loss, i.e., temperature set to
0.1, no projection, and normalization applied.

**Details of Fig. 3c** In Fig. 3c, we considered different choices of augmentations. The ‘Standard‘
augmentation implementation is described above (Appx. E.1). The ‘Supervised’ augmentation was
essentially implemented using the SupCon loss as described in Appx. E.1. For other augmentations
considered, we implemented them by dropout inter-domain supervised augmentations in SupCon.
Specifically, for each sample in the batch, we randomly masked the samples from different domains
(i.e., both inter-domain positives and negatives) i.i.d. with the specified dropout probability, while
samples within the same domain were always kept. ‘IntraDom’ and ‘ApproxDA’ correspond to
dropout probability 1 and 0.9, respectively. ‘SingleDom’ were implemented by dropout all interdomain samples with probability 1 except for a fixed domain (the ‘A’ domain of PACS in our
case).


E.2 BRIDGE

In the bridge setting (see Appx. F.2), we aimed to bridge the gap between our theoretical setup to the
practical setup. The main differences from the scientific setups are that the empirical generalization
gap is considered and the average-case source predictor is used, as detailed below:

**Incorporating empirical generalization** In practice, empirical-generalization gap should also be
considered besides the source-target generalization gap. Thus, we randomly split the PACS dataset
to 80% training and 20% validation splits for each domain. The training splits were used to train
both the encoder and the source predictor, and the validation splits were used for encoder and source
predictor selection as well as evaluation on target domains. We used the ResNet-50 model as the
encoder and initialized it from ImageNet pretrained model. The encoder was trained to a maximum of
50 epochs with a 1e-5 weight decay, using the Adam optimizer (Kingma & Ba, 2014) with a learning
rate of 5e-5, a batch size of 112 (28 for each domain), and a cosine learning rate decay schedule.

**Using average-case source predictor** Instead of approximating the worst-case source predictor in
the scientific setting, we considered the average-case[3] source predictor which is closer to the common
practice. Specifically, we freezed the encoder and trained a SVM classifier with L2 regularization on

3Here we have a slight abuse use of the phrase ‘average-case’ to distinguish from the ‘worst-case’ that we
use in the scientific setting. In fact, the source predictor could be close to the ‘best-case’ since the max-margin
classifier (SVM) was used.


-----

the source training split. The regularization parameter was tuned over {1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1,
1e2, 1e3} with the source validation accuracy.

Next, we provide other experimental details for reproducibility:

**Selection of λ** For all different setups considered in bridge settings, the CAD bottleneck was used
and the λ was tuned over {1e-3, 1e-2, 1e-1, 1, 1e1} independently for each.

E.3 DOMAINBED

**Datasets** We used non-MNIST datasets on DomainBed that were non-synthetic, including VLCS
(Fang et al., 2013), PACS (Li et al., 2017), OfficeHome (Venkateswara et al., 2017), TerraIncognita
(Beery et al., 2018), and DomainNet (Peng et al., 2019). For each dataset, we split it to 80%/20%
training/validation set according to DomainBed.

**SSL-based models & Training** For all models based on pretrained SSL models (either CLIP-based
or DINO-based) with finetuning in this experiment, we freezed the pretrained SSL model and added
on top a 1-layer MLP with hidden size 1024, and residual connection. We used CLIP ResNet-50
(CLIP S) to obtain the best possible fair comparison with baselines from DomainBed, and CLIP
ViT-B/32 (CLIP L) to achieve the best results. Note that the ResNet-50 model of CLIP S was
modified as described in Radford et al. (2021) and contained 38M parameters (more than 23M of
the original CLIP). The model was trained to 300 epochs for DomainNet and 50 epochs on other
datasets (an epoch is defined as a single pass over the smallest domain according to DomainBed). No
data augmentation was used and the temperature for scaling the logits in CAD was fixed to 0.05. We
used the Adam optimizer with a 1e-5 weight dacay, and a cosine learning rate decay schedule. The
hyperparameter search space is:

-  Learning rate: discrete set {1e-4, 3e-4, 1e-3, 3e-3}

-  Batch size: discrete set {128, 256, 512} for DomainNet and OfficeHome, and {64, 128,
256} for other datasets

-  MLP dropout: discrete set {0., 0.1, 0.5}

-  Learning rate warmup: discrete set {True, False}

**End-to-end models & Training** In Table 1, we also included an end-to-end trained model without
any pretrained SSL models. We used exactly the same model architecture (the original ResNet50, initialized from ImageNet pretrained model), training procedure and evaluation protocal as
baselines on DomainBed. Importantly, the linear classifier was jointly trained with the encoder, and
no refitting was applied. The model was trained to a maximum of 5000 steps on each dataset, and
data augmentations were applied. The Adam optimizer was used without any particular learning
rate schedule. The hyperparameter search space is (same as DomainBed except we added the
temperature):

-  Learning rate: log-uniform over [1e-5, 1e-3.5]

-  Batch size: log-uniform over [8, 64] for DomainNet, and [8, 2[5][.][5]] for other datasets

-  MLP dropout: discrete set {0., 0.1, 0.5}

-  Weight decay: log-uniform over [1e-6, 1e-2]

-  Temperature: discrete set {0.05, 0.1}

**Linear Probe Evaluation** In all the experiments except for the end-to-end training setup, we
always followed the procedure of two-stage training, where we first trained the encoder with specified
objectives, and then refit the classifier. For datasets except DomainNet, we fitted the SVM classifier
and tuned the regularization parameter over {1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3} with source
validation selection. Since DomainNet was too large and SVM cannot fit it efficiently, we used the
logistic regression classifier which was trained with a batch size 512, the Adam optimizer with a
learning rate 5e-4 and early stopping. Note that an alternative was to just use the linear head fitted
when training the representor (as we used CE loss with source labels), and we found this could work
better than refitting since the classifier was less overfitted to the source domain. However, we didn’t
do that since we wanted to stick to the representation learning protocol with two-stage training. We


-----

did that in our end-to-end training setup since we wanted it to be compeletely comparable to baselines
on DomainBed (which did not do refitting).

**Selection of λ** In our experiments, we treated λ as a special hyperparamter. For each model, we
used the same λ selected on PACS on all datasets except DomainNet, because our bottleneck is fairly
robust to the choice of λ. For the large-scale DomainNet dataset, we selected its λ individually. The
_λ values chosen for each model were:_

-  CLIP S: 1 on DomainNet and 1e-2 on other datasets

-  CLIP L: 1e-1 on DomainNet and 1e-2 on other datasets

-  DINO: 1e-1 on all datasets

-  End-to-end ResNet-50: 1e-5 on all datasets

E.4 LAION

**Model** We used the CLIP L model (i.e., CLIP ViT-B/32) with an additional network on top for
finetuning. The additional network were two blocks of 2-layer MLP, each with hidden size 2048,
pre-activation batch normalization, residual connection, and dropout probability 0.1. Note that the
original CLIP L model was frozen and only the additional network was trained.

**Dataset** We used the LAION-400M dataset which contained 400 million image-text pairs for
training. Though the dataset might not be as clean as the original CLIP training data (as evidenced by
our experimental results), it was the largest publicly available image-text-pair dataset that we could
get access to. As we froze the CLIP L model and only did finetuning, we used the 1TB preprocessed
embeddings provided by LAION-400M[4]. No further preprocessing was applied.

**Training** We used the image-text contrastive loss as introduced in Radford et al. (2021) for training
model. The temperature was learnable which was initialized as 0.07 and clipped with a minimum
0.01. The model was trained for 1 epoch using the Adam optimizer with a batch size of 16384 and a
cosine learning rate decay schedule. The learning rate was tuned over the set {3e-5, 1e-4, 3e-4, 1e-3,
3e-3, 1e-2} and the λ value for the Ent bottleneck was tuned over {1e-3, 1e-2, 1e-1, 1, 1e1}.

**Evaluation** For the evaluation on the ImageNet-related datasets, we followed a similar procedure in
Radford et al. (2021), where a linear classifier was fitted on ImageNet using the model representations
and evaluated on 7 natural distribution shift datasets. In particular, we fitted a logistic regression
classifier with 1e-5 L2 regularization on ImageNet training set which was trained with a batch size
512, the Adam optimizer with a learning rate 3e-4 and early stopping. Note that this was different
from Radford et al. (2021), where a logistic regression classifier was fitted using full-batch data
with decent hyperparameter tuning, due to our computational budget. For evaluation on natural
distribution shift datasets, we followed Taori et al. (2020) and used their released testbed[5]. The
evaluation datasets and their abbreviations used in Table 2 were: ImageNetV2 (IN-V2, Recht et al.,
2019), ImageNet-Sketch (IN-S, Wang et al., 2019), Youtube-BB (YT-BB, Shankar et al., 2019),
ImageNet-Vid (IN-Vid, Shankar et al., 2019), ObjectNet (Barbu et al., 2019), ImageNet Adversarial
(IN-A, Hendrycks et al., 2021), and ImageNet Rendition (IN-R Hendrycks et al., 2020).

[4See https://laion.ai/laion-400-open-dataset/ for details.](https://laion.ai/laion-400-open-dataset/)
[5https://github.com/modestyachts/imagenet-testbed](https://github.com/modestyachts/imagenet-testbed)


-----

ADDITIONAL EXPERIMENTAL RESULTS


F.1 SCIENTIFIC

1

3

4

Log likelihood

5

6 10 2 10[0] 10[2] 10[4]

|Base CAD|0 d|Base CAD|
|---|---|---|
|CAD Ent|1 likelihood 2 3 Log 4|CAD Ent|
||||
||||



(a) CE (R [Y | Z]) objectives


10 2 10[0] 10[2] 10[4]

Base
CAD
Ent


(b) SupCon (H[A | Z]) objectives


Figure 5: The worst-case DG performance of Ent bottleneck is more sensitive to λ than CAD

**What’s the effect of λ for different objectives on the worst-case DG performance?** In Fig. 5,
the worst-case target log likelihood versus λ values for different objectives is shown. We found that
Ent is much more sensitive to the choice of λ than CAD, which was part of the reason why we used
the latter in most of our experiments. Note that for SupCon-Ent with small λ values, it was worse
than SupCon-Base because of the discretization introduced by the Ent bottleneck, which we verified
by observing that setting λ = 0 lead to similar results.


F.2 BRIDGE

The scientific setup is closer to our theory than what we do in practice in that worst-case predictor
was considered and empirical generalization gap was ignored. Here we bridged these gaps with a
more practical setup. In particular, we split the PACS dataset to training and validation splits for each
domain and considered the setting: the representor trains the encoder on all-domain training splits
with a validation loss selection; the learner trains the SVM predictor (average-case) on the source
training split which is selected over the source validation split, and evaluates on the validation splits
of other target domains. The target validation accuracy averaged over all (source, target) setups was
reported. For simplicity, we will use CE to denote the objective with the cross-entropy loss that uses
labels to minimize R [Y | Z], and SupCon for the contrastive loss that uses supervised augmentations
to minimize H[A | Z]. We will use CAD in following experiments unless otherwise specified (chosen
with initial experiments). Details in Appx. E.2.


Table 3: We repeated most empirical analysis (in the scientific setting) in the more practical bridge
setting and observed similar results.

|Setup|Avg. target acc.|
|---|---|


|CE-Base CE-CAD CE-CAD (partial domains)|95.9 0.5 ± 96.7 0.2 ± 82.6 0.5 ±|
|---|---|

|SupCon-CAD SupCon-CAD (SingleDom) SupCon-CAD (ApproxDA) SupCon-CAD (IntraDom) SimCLR-CAD|96.7 0.4 ± 96.7 0.3 ± 96.6 0.3 ± 96.2 0.7 ± 61.7 0.8 ±|
|---|---|


-----

**Does domain bottleneck improve the average-case DG performance?** Though our theory focuses on the worst-case DG, we empirically showed that adding bottlenecks to enforce support match
can also improve the average-case DG performance by comparing CE-Base and CE-CAD in Table 3.

**What if the representor only has access to source domains?** Similar to what we did in the
scientific setting, we considered the setup where one single domain is specified as the target domain
and excluded from the training set of the representor and used for evaluation with source predictors
trained on other domains. This is denoted as CE+CAD (partial domains) in Table 3, which is much
worse then CE-CAD. This shows the necessity of getting access to target domain information for DG.

**What if the representor only has access to domain-agnostic augmentations?** In Table 3, we
also compared SupCon-CAD which used supervised augmentations through the labels with CECAD and they achieved the same performance. This shows that the representor can still learn good
representations without labels but only domain-agnostic augmentations in practice.

**Can we use standard augmentations?** In Fig. 2, we point out that standard augmentations are not
domain-agnostic and thus not suitable for SSL with our objectives. We empirically showed this by
using augmentations of SimCLR (see Appx. E.1 for details) with our objectives (SimCLR-CAD). In
Table 3, we indeed observed that using standard augmentations performed much worse than using
desired augmentations (SupCon-CAD).

**How do augmentations matter?** Besides investigating the ‘Supervised’ augmentations (SupConCAD) and ‘Standard’ augmentations (SimCLR-CAD) above, we also compared other three augmentations as in the scientific section. Specifically, we considered the ‘SingleDom’, ‘IntraDom’,
and ‘ApproxDA’ augmentations. As shown in Table 3, SupCon-CAD (SingleDom) and (ApproxDA)
maintained the DG performance but SupCon-CAD (IntraDom) was slightly worse (0.5 accuracy
drop). We assumed the small gap was due to the specific dataset that we used (PACS). We did
the same analysis on VLCS, and SupCon-CAD with ‘Supervised’, ‘SingleDom’, and ‘IntraDom’
augmentations gave 84.7 ± 0.4, 83.2 ± 0.3, and 77.5 ± 2.3, respectively. This shows the importance
of using domain-agnostic augmentations in practice.

**Do standard augmentations affect source performance?** Previously, we showed that using standard augmentations hurt the DG performance measured by the average target accuracy. It is natural to
ask whether using standard augmentations also hurt the source performance since we should also be
interested in the ‘effective robustness’ (Taori et al., 2020). Thus we also reported the average source
accuracy of SupCon-CAD and SimCLR-CAD which were 96.9 ± 0.2 and 90.1 ± 0.2, respectively.
The source performance using standard augmentations was indeed worse, but if we consider the
source-target gap which was 0.2 for SupCon-CAD and 28.4 for SimCLR-CAD, which still verified
that the non-domain-agnostic standard augmentations were harder to force support match. To be
even more convincing, we did the same analysis on VLCS, and the average source accuracy of
SupCon-CAD and SimCLR-CAD were 86.6 ± 0.1 and 84.6 ± 0.5 which were fairly close, but the
average target accuracy were 84.7 ± 0.4 and 57.5 ± 1.7, respectively.

F.3 DOMAINBED

**Full result of Table 1** We included the full result of Table 1 with all baselines on DomainBed as in
Table 4. We considered most representative baselines from DomainBed, most of which considered
learning invariant representations or optimal classifiers across domains. Specifically, we included
IRM (Arjovsky et al., 2019), GroupDRO (Sagawa et al., 2019), Mixup (Yan et al., 2020), CORAL
(Sun & Saenko, 2016), MMD (Li et al., 2018a), DANN (Ganin et al., 2016), CDANN (Li et al.,
2018b), and VREx (Krueger et al., 2021). We also included the result pretrained CLIP S model with
a zero-shot classifier using text representations (CLIP S Zero Shot), which demonstrated better DG
performance than CLIP S with linear probe. But we observed that it was outperformed by our CLIP
S + CAD.

**What is the impact of CLIP pretraining?** To ensure that our gains are not only due to a novel
CAD bottleneck, but the synergy between enforcing support constraint and using desired SSL models,
we investigated CAD using the standard DomainBed protocol denoted as CAD in the table. It
shows that CAD on its own performs similarly with DomainBed baselines (see Table 4 for a full
comparison).


-----

Table 4: Full results on DomainBed with ‘oracle selection’ method.

|Algorithm|VLCS PACS OfficeHome TerraIncognita DomainNet|
|---|---|


|ERM IRM GroupDRO Mixup CORAL MMD DANN CDANN VREx|77.6 0.3 86.7 0.3 66.4 0.5 53.0 0.3 41.3 0.1 ± ± ± ± ± 76.9 0.6 84.5 1.1 63.0 2.7 50.5 0.7 28.0 5.1 ± ± ± ± ± 77.4 0.5 87.1 0.1 66.2 0.6 52.4 0.1 33.4 0.3 ± ± ± ± ± 78.1 ± 0.3 86.8 ± 0.3 68.0 ± 0.2 54.4 ± 0.3 39.6 ± 0.1 77.7 0.2 87.1 0.5 68.4 0.2 52.8 0.2 41.8 0.1 ± ± ± ± ± 77.9 0.1 87.2 0.1 66.2 0.3 52.0 0.4 23.5 9.4 ± ± ± ± ± 79.7 0.5 85.2 0.2 65.3 0.8 50.6 0.4 38.3 0.1 ± ± ± ± ± 79.9 0.2 85.8 0.8 65.3 0.5 50.8 0.6 38.5 0.2 ± ± ± ± ± 78.1 0.2 87.2 0.6 65.7 0.3 51.4 0.5 30.1 3.7 ± ± ± ± ±|
|---|---|


|CAD|78.0 0.1 87.3 0.2 67.0 0.5 53.5 0.9 41.5 0.1 ± ± ± ± ±|
|---|---|


|DINO + CAD|69.6 0.6 76.1 0.1 56.9 0.5 25.9 1.2 33.6 0.1 ± ± ± ± ±|
|---|---|


|CLIP S CLIP S (Zero-Shot) CLIP S + Base CLIP S + CAD|81.1 0.5 90.3 0.2 70.6 0.1 29.6 0.8 47.7 0.0 ± ± ± ± ± 80.9 0.1 91.8 0.1 70.4 0.2 19.1 0.1 46.9 0.0 ± ± ± ± ± 81.3 0.5 91.2 0.3 70.6 0.1 36.4 0.7 46.8 0.2 ± ± ± ± ± 82.3 ± 0.3 92.0 ± 0.2 71.9 ± 0.2 36.2 ± 0.8 48.8 ± 0.1|
|---|---|


|CLIP L CLIP L + CAD|80.7 0.4 93.7 0.8 79.6 0.1 36.9 0.6 52.8 0.1 ± ± ± ± ± 81.6 ± 0.1 94.9 ± 0.3 80.0 ± 0.2 40.6 ± 1.1 53.7 ± 0.1|
|---|---|



**Why ‘oracle’ selection?** In the main body, we provided the results with ‘oracle selection’ which
was the closest to our theory among the model selection methods in DomainBed (in the sense that
we needed target domain information to achieve IDG). Here, we also provided results with ‘source
validation’ selection in Table 5. Source validation selection relies on the assumption that source
and target data follow similar distributions (Gulrajani & Lopez-Paz, 2021) thus source and target
accuracy are highly correlated, which is not really true in practice. We found some issues with source
validation selection results:

-  The selected model with the highest source validation accuracy tends to overfit the source
domain, thus possibly leads to worse performance on the target domain. This can be probed
by the fact that the finetuned CLIP models (CLIP + Base or CLIP + CAD) were generally
worse than the original CLIP model;

-  Selecting model with source validation accuracy tends to diminish the effect of bottlenecks.
This can be seen by the fact that the gap between CLIP + Base and CLIP + CAD of source
validation selection is much smaller than that of oracle selection;

-  The source accuracy is not a good indicator of target accuracy thus its result has a larger
variance.

F.4 LAION

**Evaluation results on DomainBed** We included the evaluation results of trained models on DomainBed in Table 6, where we followed exactly the same linear evaluation protocal discussed in
Appx. E.3. We observed similar results as Table 2: the CLIP L model trained with the Ent bottleneck on LAION (Tuned w/ Ent) outperformed the one without (Tuned w/o Ent) on all DomainBed
datasets, but slightly underperformed the original CLIP L model (which might be due to quality of
the LAION-400M dataset).


-----

Table 5: Results on DomainBed with ‘source validation’ selection. Source validation selected model
tends to overfit more to the source domain and diminish the effect of bottlenecks.

|Algorithm|VLCS PACS OfficeHome TerraIncognita DomainNet|
|---|---|

|ERM IRM GroupDRO Mixup CORAL MMD DANN CDANN VREx|77.5 0.4 85.5 0.2 66.5 0.3 46.1 1.8 40.9 0.1 ± ± ± ± ± 78.5 0.5 83.5 0.8 64.3 2.2 47.6 0.8 33.9 2.8 ± ± ± ± ± 76.7 0.6 84.4 0.8 66.0 0.7 43.2 1.1 33.3 0.2 ± ± ± ± ± 77.4 0.6 84.6 0.6 68.1 0.3 47.9 0.8 39.2 0.1 ± ± ± ± ± 78.8 0.6 86.2 0.3 68.7 0.3 47.6 1.0 41.5 0.1 ± ± ± ± ± 77.5 0.9 84.6 0.5 66.3 0.1 42.2 1.6 23.4 9.5 ± ± ± ± ± 78.6 0.4 83.6 0.4 65.9 0.6 46.7 0.5 38.3 0.1 ± ± ± ± ± 77.5 0.1 82.6 0.9 65.8 1.3 45.8 1.6 38.3 0.3 ± ± ± ± ± 78.3 0.2 84.9 0.6 66.4 0.6 46.4 0.6 33.6 2.9 ± ± ± ± ±|
|---|---|

|CAD|78.0 0.5 85.2 0.9 67.4 0.2 47.3 2.2 41.0 0.1 ± ± ± ± ±|
|---|---|

|DINO + CAD|68.9 0.9 75.4 0.5 56.4 0.7 23.6 1.2 31.0 2.3 ± ± ± ± ±|
|---|---|

|CLIP S CLIP S (Zero-Shot) CLIP S + Base CLIP S + CAD|81.1 0.5 90.3 0.2 70.6 0.1 29.6 0.8 47.7 0.0 ± ± ± ± ± 80.9 0.1 91.8 0.1 70.4 0.2 19.1 0.1 46.9 0.0 ± ± ± ± ± 81.4 0.4 89.6 0.7 70.4 0.2 30.9 2.2 44.6 1.6 ± ± ± ± ± 81.2 0.6 90.0 0.6 70.5 0.3 30.3 0.9 45.5 2.1 ± ± ± ± ±|
|---|---|

|CLIP L CLIP L + CAD|80.6 0.7 93.5 0.8 79.4 0.2 37.5 0.7 50.1 1.1 ± ± ± ± ± 80.8 0.7 93.5 0.7 79.7 0.2 37.4 1.2 51.7 1.4 ± ± ± ± ±|
|---|---|


Table 6: Finetuning CLIP L on LAION with an entropy bottleneck performs better on DomainBed
than finetuning without.

|Algorithm|VLCS PACS OfficeHome TerraIncognita DomainNet|
|---|---|

|CLIP L|80.7 0.4 93.7 0.8 79.9 0.1 36.9 0.6 52.8 0.1 ± ± ± ± ±|
|---|---|

|Tuned w/o Ent Tuned w/ Ent|79.2 0.7 93.4 0.3 77.2 0.5 36.1 0.4 51.2 0.1 ± ± ± ± ± 80.7 0.4 94.3 0.8 78.2 0.2 36.8 0.4 52.2 0.1 ± ± ± ± ±|
|---|---|


-----

