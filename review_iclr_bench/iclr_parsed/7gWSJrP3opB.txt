# A GENERAL ANALYSIS OF EXAMPLE-SELECTION FOR STOCHASTIC GRADIENT DESCENT

**Yucheng Lu[∗], Si Yi Meng[∗], Christopher De Sa**
Department of Computer Science
Cornell University
Ithaca, NY 14853, USA
_{yl2967,sm2833,cmd353}@cornell.edu_

ABSTRACT

Training example order in SGD has long been known to affect convergence rate.
Recent results show that accelerated rates are possible in a variety of cases for
permutation-based sample orders, in which each example from the training set
is used once before any example is reused. In this paper, we develop a broad
condition on the sequence of examples used by SGD that is sufficient to prove
tight convergence rates in both strongly convex and non-convex settings. We show
that our approach suffices to recover, and in some cases improve upon, previous
state-of-the-art analyses for four known example-selection schemes: (1) shuffle
once, (2) random reshuffling, (3) random reshuffling with data echoing, and (4)
Markov Chain Gradient Descent. Motivated by our theory, we propose two new
example-selection approaches. First, using quasi-Monte-Carlo methods, we achieve
unprecedented accelerated convergence rates for learning with data augmentation.
Second, we greedily choose a fixed scan-order to minimize the metric used in our
condition and show that we can obtain more accurate solutions from the same
number of epochs of SGD. We conclude by empirically demonstrating the utility
of our approach for both convex linear-model and deep learning tasks. Our code is
[available at: https://github.com/EugeneLYC/qmc-ordering.](https://github.com/EugeneLYC/qmc-ordering)

1 INTRODUCTION

To minimize a differentiable function f : R[d] _→_ R, stochastic gradient descent (SGD) iteratively
updates a parameter vector w ∈ R[d] starting at some w0 by running

_wt+1 = wt −_ _αt∇f_ (wt; xt), (1)

where αt is the step size at iteration t, xt is a data example (often a minibatch of data) chosen
by some process—typically by subsampling a training dataset—for SGD to use at iteration t, and
_f_ (wt; xt) is an example gradient, which we hope will be a good approximation for the gradient
_∇_
of the objective _f_ (wt). In the standard setup, the x’s are drawn from a dataset of size n, and
_f_ (w) = _n[1]_ _x∈D ∇[f]_ [(][w][;][ x][)][, which is often referred to as Empirical Risk Minimization (ERM). The] D

order in which the example sequence x0, x1, . . . are chosen is known to affect the convergence of SGD.
For instance, compare so-called “random reshuffling” to with-replacement sampling: optimizing aP
strongly convex objective withreplacement and achieves an accelerated T total iterations, random reshuffling samples the convergence rate of [1]/T [2], while with-replacement sampling xt from D without
yields a convergence rate of [1]/T (Bottou, 2012; Recht & Re´, 2012; Gurb¨ uzbalaban et al.¨, 2021).
Similar accelerated rates have been shown in other settings and for other example orders, such as
shuffling the dataset once (Nguyen et al., 2020; Ahn et al., 2020; Mishchenko et al., 2020). However,
these analyses have mostly focused only on specific example-selection schemes, and study only the
case of ERM-type finite-sum objectives. This does not help us understand how new example orders
(such as the data echoing method of Choi et al. (2019)) affect the convergence of SGD.

This paper develops a general condition on the example gradients themselves that is sufficient to
provide a convergence rate for SGD. Intuitively, our main result is: the convergence rate of SGD

_∗Equal Contribution._


-----

_depends on how fast the averages of consecutive example gradients_ _f_ (w; xt) converge to the full
_∇_
_objective gradient_ _f_ (w). Using with-replacement sampling, the average of m consecutive gradient
examples starting at any timestep ∇ _τ_, _m1_ _τt=+τm−1_ _f_ (w; xt), converges to _f_ (w) at a rate of ([1]/m)

_∇_ _∇_ _O_

in terms of the norm squared: we show SGD with any example sequence that fulfills this condition
will converge at the same asymptotic rate as with-replacement sampling. Alternatively, if that averageP
converges at the faster _O[˜]([1]/m[2]) rate typical of Quasi-Monte-Carlo (QMC) (Caflisch, 1998), then we_
show SGD enjoys the accelerated rate that random reshuffling gets. Our contributions are as follows:

-  We propose a new condition on the example gradients—average gradient error—and provide
convergence analysis using this general condition for both non-convex and strongly-convex
problems. We justify the validity of this condition on synthetic experiments (Section 3).

-  We show that many commonly used example orderings—shuffle once, random reshuffling, data
echoing, Markov Chain Gradient Descent—can be analyzed as special cases under our theoretical
results, which match or improve upon their existing rates in the literature (Section 4).

-  We propose two new algorithms: (1) QMC-based data augmentation that transforms examples via
a low-discrepancy sequence, improving generalization; and (2) a greedy algorithm that sorts the
examples before each epoch based on our new average gradient error metric (Section 5).

-  Empirically, we evaluate our two algorithms on several image classification benchmarks including
MNIST, CIFAR10/100 and ImageNet. We show with QMC-based data augmentation, a higher
validation accuracy can be achieved without hyperparameter tuning—this suggests that QMC may
be a good default driver to use with data augmentation for deep learning in general. Meanwhile,
the greedy algorithm converges faster both in terms of iteration and wall-clock time (Section 6).


2 RELATED WORK

**Example ordering in stochastic optimization.** Traditional example ordering in SGD is carried
out in a with-replacement fashion, which is used to ensure unbiased estimation of the full gradient
(Robbins & Monro, 1951; Bach & Moulines, 2011; Zhang, 2004; Bottou et al., 2018; Drori & Shamir,
2020). Significant attention has been paid to importance sampling with respect to various measures,
such as Lipschitz constants (Schmidt et al., 2017; Needell et al., 2014), example gradient norms
and bounds (Zhao & Zhang, 2015; Alain et al., 2015; Papa et al., 2015; Lee et al., 2019), individual
losses (Kawaguchi & Lu, 2020; Loshchilov & Hutter, 2015), and data heterogeneity (Lu et al., 2021).
Without-replacement sampling, however, is more common in practice and empirically allows faster
convergence (Bottou, 2012). Among the most popular without-replacement approaches are shuffle
once (SO) (Bertsekas, 2011; Gurb¨ uzbalaban et al.¨, 2019) and random reshuffling (RR) (Ying et al.,

2017). In theory, Recht & Re´ (2012) undertook the first investigation on convergence of RR via
the noncommutative arithmetic-geometric mean conjecture, to which subsequent works provide
counter examples (Yun et al., 2021; De Sa, 2020). HaoChen & Sra (2019) performed an epoch-wise
acceleration analysis on RR while Gurb¨ uzbalaban et al.¨ (2021) considered its convergence over
infinite epochs. Safran & Shamir (2020) analyzed the lower bounds for both SO and RR methods,
and their results are further polished by Mishchenko et al. (2020) via the Bregman divergence bound.
Aside from manual ordering, another line of research focuses on perturbed example ordering from
data echoing (Choi et al., 2019; Agarwal et al., 2020).

**Quasi-Monte Carlo.** Quasi-Monte Carlo (QMC) is a variant of Monte Carlo (MC) methods that
uses a low-discrepancy sequence instead of a pseudorandom sequence. QMC has been successfully
applied in a wide variety of domains including computer graphics (Keller, 1995), finance (Joy et al.,

1996), and computational biology (Cieslak et al., 2008). In machine learning, using QMC in place
of MC can significantly improve many techniques including variational inference (Buchholz et al.,
2018; Liu & Owen, 2021), feature mapping (Yang et al., 2014; Avron et al., 2016), normalizing
flows (Wenzel et al., 2018), deep learning based PDE (Chen et al., 2019), and time series analysis
(Philipson et al., 2020). For stochastic optimization, early works like Homem-de Mello (2008) and
Pennanen (2005) established the asymptotic convergence of a QMC sequence in terms of the training
set size, while Jank (2005) proposed replacing MC with QMC in computing the E-step of the EM
algorithm. Similar to our motivation, Buchholz et al. (2018) analyzed the convergence of SGD when
samples are drawn using QMC. Their approach differs significantly from ours in that their method
draws an independent unbiased length-b QMC sequence for each minibatch, while our examples
come from contiguous subsequences of a length-T QMC sequence that is used across all iterations.


-----

3 EXAMPLE-GRADIENT AVERAGES AND SGD CONVERGENCE

In this section, we describe our setup, define the average gradient error condition we are proposing,
and state our main result. Our objective is to minimize a continuously differentiable function
_f : R[d]_ _→_ R using examples xt from some set X retrieved at each iteration t. We make the usual
assumption that both the loss gradient and the example gradients are L-Lipschitz continuous.
**Assumption 1 (L-Smoothness). For some L < ∞, for any u, v ∈** R[d] _and any example x ∈X_ _,_
_∥∇f_ (u; x) −∇f (v; x)∥≤ _L · ∥u −_ _v∥_ _and ∥∇f_ (u) −∇f (v)∥≤ _L · ∥u −_ _v∥._

We propose a new condition on the average gradient error. Informally, this condition bounds how
averages of consecutive example gradients approximate the objective gradient. Formally,
**Assumption 2. In the context of Equation (1), we say that the example sequence x0, x1, . . . is**
(γ, C, Φ)-concentrating for γ ∈ [1, 2], C > 0, Φ ≥ 0, if for any timestep τ ≥ 0 and any m > 0,

2

1 _τ_ +m−1 1

_f_ (wτ ; xt) _f_ (wτ ) _C_ [2] + Φ[2] _f_ (wτ ) _,_ (2)

_m_ _∇_ _−∇_ _≤_ _m[γ]_ _∥∇_ _∥[2][]_

_t=τ_

X 

_where wτ is the weight parameter vector arrived at after τ SGD update steps in Equation (1)._

The constants C and Φ here may depend on the total number of iterations T : specifically, they may
absorb logarithmic factors like log(τ + m)[2][s] typical in a QMC error bound, where s is the dimension
of the sample space. In addition, when the _xt_ ’s are random, we will show that this assumption
_{_ _}_
holds with high probability. Furthermore, it suffices to show that the inequality holds for any w:
our requirement that it holds only for the specific wτ arrived at by SGD is weaker. We can develop
intuition about Assumption 2 by considering familiar cases:

-  For general with-replacement sampling, the sum in (2) is a sum of independent random variables;
so, a concentration argument would yield Assumption 2 with γ = 1 with high probability.

-  For general without-replacement sampling from a dataset of size n, any whole-epoch subsequences
from the sum in (2) will cancel to zero, so we expect that sum to have magnitude O(n) independent
of m, and thus Assumption 2 should hold with γ = 2 (Propositions 1 to 4).

-  For low-discrepancy sampling, we would expect Assumption 2 to hold with γ = 2, along with a
multiplicative log(τ + m)[2][s] term: this is the classic error rate for QMC (Proposition 6).


In Section 4, we will make this intuition rigorous with high probability under the bounded gradient
error assumption. This intuition illustrates both how Assumption 2 can cover previously analyzed
settings and how it can generalize to cases not previously studied, such as QMC data augmentation.
Also observe that Assumption 2 is easily adapted to minibatch SGD: if it holds for the single-example
case, it should also hold with modified constants for minibatches of size b (consisting of averages
of b consecutive example gradients) by substituting m 7→ _mb.[1]_ Since this is straightforward, for
simplicity of presentation our theory focuses on the batch-size-1 case. Our analysis is based on the
following key lemma, which bounds the evolution of SGD over an “analysis phase” of m steps.
**Lemma 1. Suppose our setup satisfies Assumptions 1 and 2 and that we use a constant step size α.**
_For all timesteps τ ≥_ 0, let m > 0 be some integer such that 3αm[1][−][γ][/][2]Φ ≤ _αm ≤_ 61L _[. Then the]_

_objective at timestep τ + m is bounded by_

_f_ (wτ +m) ≤ _f (wτ_ ) − [1]4 _[αm][∥∇][f]_ [(][w][τ] [)][∥][2][ + 2][αm][1][−][γ][C] [2][.]

By applying this lemma inductively and choosing the step size appropriately, we can derive convergence rates for SGD in a variety of settings. Note that while here for simplicity we state results for a
fixed step size, Lemma 1 can also be used to get essentially the same rates for diminishing step size
schemes, which we analyze in Appendices A.2 and A.3. In what follows, we let f _[∗]_ be the global
minimum of f, let ∆:= f (w0) _f_, and use [˜] to hide logarithmic terms in the problem parameters
_−_ _[∗]_ _O_
such as C, Φ, L, ∆, and ϵ, while treating γ as a constant.
**Theorem 1 (Non-convex case). Suppose that our setup satisfies Assumptions 1 and 2, and let ϵ > 0**
_be any target error. Using SGD (1) with a constant step size α =_ 61L (4C/ϵ + 3Φ)2/γ[][−][1], the

_number of stepsT = T needed to achieve 48ϵL[2]∆_ 4ϵC [+ 3Φ] min2/γt=0[m] =,···,T[˜] _−1Cϵ∥∇2[2+2]/γ_ _Lf/γ∆(w[+]t)[ Φ]∥22 ≤/γϵ[2]Lϵ∆2 is at most+_ _[C]ϵ[2]2/γ/γ_ [+ Φ]2/γ[] _._

_·_ _O_

1
The only technical subtlety is that the iterates  l   _wτ of SGD would vary with minibatch sizes._


-----

10

10

10

10

10

10

10

10

|Offline|Col2|
|---|---|
|IID Uniform Sobol RR SO||
|03 104 105 106 m|107|


1


1m


Online Offline

10 1 10 1

10 3

2 *wwt 101010 579 IID Uniform 2 *wwtn101010 357 IID Uniform

10 11 Sobol1/t 10 9 SobolRR

1/t[2] SO

10[2] 10[3] 10[4] 10[5] 10[6] 10[7] 10[2] 10[3] 10[4] 10[5] 10[6] 10[7]

iterations t iterations t


Online

2)(wR0 1010 24
11);(m xwR0 = 0tmt101010 1068 IID UniformSobol

10[3] 10[4] 10[5] 10[6] 10[7]

m


(a) Distance to optimum over iterations


(b) Average gradient error (LHS of Assumption 2)


Figure 1: Comparison of sampling schemes on a synthetic least squares problem.

Observe that when γ = 1, this gives the standard ϵ[−][4] rate expected for non-convex SGD, and for
_γ = 2, this gives us the accelerated ϵ[−][3]_ rate of shuffling methods. We can get an even faster rate if f
satisfies the µ-Polyak-Łojasiewicz (PL) condition ∥∇f (w)∥[2] _≥_ 2µ(f (w) − _f_ _[∗]), which generalizes_
strong convexity and has been applied before in the study of sample orders (Mishchenko et al., 2020;
Ahn et al., 2020): in particular the following theorem holds for µ-strongly convex functions.

**Theorem 2. Suppose that f satisfies the µ-PL condition and our setup satisfies Assumptions 1 and 2.**
_Let ϵ > 0 be any target error, and κ =_ _[L]/µ be the condition number of the problem. Using SGD_
_(1) with a constant step size α =_ 61L (8C [2]/(µϵ[2]) + 9Φ[2])1/γ _−1, the number of steps T needed to_

_guarantee f_ (wT ) − _f_ _[∗]_ _≤_ _ϵ[2]_ _is at most_

[]

_T =_ 12κ log 2∆ϵ[2] _·_ 8µϵC[2][2][ + 9Φ][2][][1][/][γ][] = O[˜] _µC[1][/γ][2][/γ]ϵ[2]κ[/γ][ +][ κ][Φ][2][/γ][ +][ κ]_ _._
      

Observe that when γ = 1, this recovers the ordinary T = κϵ[−][2] rate we usually get for strongly
convex SGD, and when γ = 2 we get a faster rate of κϵ[−][1]. These theorems together show that our
Assumption 2 is sufficient to show the convergence of SGD, and the convergence-rate parameter γ
of the assumption translates to affect the convergence rate of SGD. This validates our intuition that
_faster convergence of averages of consecutive example gradients to the full gradient ∇f_ (w) leads to
_faster convergence of SGD._

**Synthetic experiments.** We quickly validate these results on a synthetic 10-dimensional
strongly-convex problem. The first setting we consider is the expected risk minimization of
_R(w) = E [(x[T]w −_ _y)[2]], where x ∼_ _N_ (0, Id) and y | x ∼ _N_ (x[T]w[∗], 1) for some optimal value
_w[∗]. We run SGD in an online fashion—at iteration t we draw samples (xt, yt) from the underlying_
distribution to compute _R(wt; xt) used in the update. We compare drawing these samples indepen-_
_∇_
dently at random against drawing using a QMC sequence (Sobol in [0, 1][d]) via an inverse transform,
using for both cases the same diminishing step size scheme selected to minimize the expected risk
for the random-sampling case—the optimal step size scheme for vanilla SGD. The online plot of
Fig. 1(a) shows that the convergence rate is strictly superior with QMC, which achieves a O([1]/t[2]) rate
compared to the O([1]/t) rate of random sampling, which is what Theorem 2 predicts.

We also evaluate the offline setting, where we draw n independent examples from the same distribution
to form a training set, and minimize the empirical risk Rn(w) = _n1_ _ni=1[(][x]i[T][w][ −]_ _[y][i][)][2][. This]_
corresponds to a least squares problem with optimal solution wn[∗] [. We run SGD epoch-wise for]
_K epochs, in which we compare sampling from the training set uniformly with replacement (IIDP_
Uniform), random reshuffling (RR), shuffle once (SO), and sampling using one QMC sequence in

[0, 1] of length T = nK followed by a mapping to example indices (Sobol). In the offline plot of
Fig. 1(a), we see that the low-discrepancy methods all yield an accelerated rate compared to IID
Uniform, which again validates our theory. Additional details can be found in Appendix A.1.3.

With the same synthetic setup, we also verified our bound in Assumption 2 by measuring the average
gradient error over a sequence of examples. The fixed point wτ is arbitrarily set to the origin. For the
online setting (left of Fig. 1(b)), we use the same set of examples as in the SGD experiments above
starting at t = 0. Similarly, in the offline setting, we go through the examples epoch-wise. As we can
see, the sample orderings given by QMC, RR and SO (offline only) indeed give us an accelerated rate
of decrease in the average gradient error as we increase m (γ ≈ 2), justifying our main assumption.


-----

4 ANALYSIS OF EXISTING SCAN ORDERS

In this section, we illustrate the power of our approach by proving convergence rates for exampleselection methods proposed and analyzed in previous literature. For each method, we show Assumption 2 holds with high probability, and then by applying Theorems 1 and 2 to these results, we show
how existing rates for these methods can be recovered and in some ways improved upon. To the best
of our knowledge, these are the first high-probability results for shuffle once and random reshuffle for
general non-convex optimization. When using a finite training set of examples, we let n denote its
size, let the examples be indexed as x[(0)], x[(1)], . . ., x[(][n][−][1)] (to avoid confusion with xt, the example
used by SGD at step t), and let T = nK denote the total number of iterations after K epochs. We
will also require the following standard assumption that bounds the error of a single example gradient.

**Assumption 3. For all examples x ∈X and points w ∈** R[d], there exists A, B ≥ 0 such that the
_gradient errors satisfy ∥∇f_ (w; x) −∇f (w)∥[2] _≤_ _A[2]_ + B[2]∥∇f (w)∥[2].

**Shuffle once (SO).** In the shuffle-once variant of SGD, a single permutation σ of {0, . . ., n − 1} is
chosen uniformly at random at the start, and the examples are used repeatedly in that order: explicitly,
_xt = x[(][σ][(][t][ mod][ n][))]. One way to analyze shuffle-once is to prove Assumption 2 for permutation-based_
methods generally.
**Proposition 1. Let Assumptions 1 and 3 hold, and suppose that we are using a permutation-based**
_method of sampling, that is, any method such that xkn, xkn+1, . . ., xkn+n_ 1 is a permutation of
_−_
_x[(0)], x[(1)], . . ., x[(][n][−][1)]_ _for all epochs k ≥_ 0. Any such method satisfies Assumption 2 with γ = 2,
_C_ [2] = n[2]A[2] _and Φ[2]_ = n[2]B[2].

This immediately lets us recover previous rates up to constant factors, but we can do better. In the
case where we are learning over a bounded region, we can prove a stronger result for shuffle once.
**Proposition 2. Suppose that we are using the shuffle once variant of SGD to learn over a region**
_B ∈_ R[d] _of radius at most R, such that the iterates wt are guaranteed to remain within this region._
_Assume that for all w ∈B and all examples x in the training set of size n, Assumption 1 (L-_
_Smoothness) and Assumption 3 hold. Then with probability at least 1 −_ _p, Assumption 2 holds with_
_γ = 2, C_ [2] = O[˜](dA[2](n + B[2])), and Φ[2] = O[˜](ndB[2]).[2]

In comparison to previous results and to our rate implied by Proposition 1, this improves the
dependence from n to _√nd, which is a significant improvement over the best rates for shuffle once_

available in the literature when the dimension is small relative the training set size. In particular, if
we set B = 0 and consider small ϵ, our rate in the non-convex case becomes

_T = [˜]_ _AL∆ϵ[3]√nd_ _,_ which implies _ϵ[2]_ (AL∆)T2/[2]3/3(nd)1/3 = [˜] _nKd_ [2] 1/3[].
_O_ _≤_ _O[˜]_ _O_

This rate matches that for shuffle once achieved in  Nguyen et al. (2020, Corollary 1) in terms of    _ϵ,_
up to logarithmic factors. In the µ-PL (or strongly convex) case for B = 0, we again obtain a rate
matching that of Nguyen et al. (2020), Ahn et al. (2020), and Mishchenko et al. (2020):


which implies _ϵ[2]_ _κ[2]µTA[2][2]nd_ = [˜] _κµnK[2]A[2][2]d_
_≤_ _O[˜]_ _O_
  


_T = [˜]_ _κAϵ_
_O_



_nd_


**Random reshuffling (RR).** Random reshuffling is similar to shuffle once, except that a new
ordering is chosen at each epoch: sampling the dataset without replacement. Concretely, if σk denotes
the permutation used by random reshuffling at the kth epoch, then xt = x[(][σ][⌊][t/n][⌋][(][t][ mod][ n][))].
**Proposition 3. Suppose that we are using the random reshuffling variant of SGD. Assume that for all**
_w_ R[d] _and all examples, Assumption 1 (L-Smoothness) and Assumption 3 hold. For some p_ (0, 1),
_set the constant step size to satisfy ∈_ _α ≤_ max 1460BnL · log 4e[2]T/p _, 2nL_ _−1. Then with ∈_
_probability at least 1_ _p, Assumption 2 holds with γ = 2, C_ [2] = O[˜](nA[2]) and Φ[2] = O[˜](nB[2]).
_−_       


Setting B = 0, our rate in the non-convex case now becomes

_T = [˜]_ _ALϵ∆[3][√]n_ _,_ which implies _ϵ[2]_ (AL∆)T2[2]//33(n)1/3 = [˜] _nK1_ [2] 1/3[].
_O_ _≤_ _O[˜]_ _O_

2       
The probability p is only present in log terms in these expressions, so it does not appear in the ˜O.


-----

Here we match the best rate obtained by Mishchenko et al. (2020, Corollary 3) in the small ϵ setting.
In the µ-PL (or strongly-convex) case, we get


_κ[2]A[2]n_ _κ[2]A[2]_
which implies _ϵ[2]_ _µT_ [2] = [˜] _µnK[2]_
_≤_ _O[˜]_ _O_
  


_T = [˜]_ _κAϵ_
_O_



As in shuffle once, here our rate for random reshuffling matches that obtained by Nguyen et al. (2020)
and Ahn et al. (2020), as well as Mishchenko et al. (2020) albeit with a slightly worse dependency
on κ. It’s worth noting that for simple quadratics, RR and SO are only faster than with-replacement
SGD when K ≳ [1]/µ (Safran & Shamir, 2021).

**Random reshuffling with data echoing.** Data echoing is a technique that can be easily implemented in a machine learning training pipeline to increase throughput and improve performance.
It was first introduced and tested empirically by Choi et al. (2019) and analyzed by Agarwal et al.
(2020). The idea is to perform multiple SGD updates on each example xi (or minibatch) before
proceeding to the next. By “echoing” examples we allow more time for upstream data loading and
preprocessing, and consequently decrease downstream GPU idle time for gradient computation. For
simplicity, we also use a fixed number of echos c as in Agarwal et al. (2020). Concretely, a c-echoed
version of a sample order ˆx is given by xt = ˆx _t/c_ . Data echoing can essentially be applied to
_⌊_ _⌋_
any example-ordering scheme, and here we provide one analysis under random reshuffling. The
justification for Assumption 2 under random reshuffling with data echoing follows essentially without
modification from the c = 1 version in the previous subsection.

**Proposition 4. Suppose that we are using the random reshuffling variant of SGD, where each example**
_is echoed c times using the same step size in RR. Under the same assumptions as in Proposition 3,_
_with probability at least 1 −_ _p, Assumption 2 holds with γ = 2, C_ [2] = O[˜](cnA[2]) and Φ[2] = O[˜](cnB[2]).

It immediately follows that data echoing should get the same convergence rates we showed in
Section 4 with A[2] and B[2] multiplied by c. Although Agarwal et al. (2020) also provided an analysis
for data echoing, they require that the examples are sampled independently, rather than the random
reshuffling setting that is more commonly-used: as a result, their analysis did not achieve the
accelerated ϵ[−][3] rate that shuffled methods enjoy. Another advantage of our analysis is that the proof
follows exactly from that of vanilla random reshuffling, from which the constant c simply propagates.

**Markov chain gradient descent (MCGD).** To illustrate the versatility of Assumption 2, we show
how it can be satisfied by a problem where the objective is not a finite sum and where γ ̸= 2. Consider
_f_ (w) = Eξ Ξ[f (w; ξ)] with some underlying distribution Ξ. Running SGD then requires that at
_∼_
each iteration t, we draw _f_ (wt; ξt) where ξt Ξ; however, sampling from Ξ can be intractable.
The method of Markov Chain Gradient Descent addresses this problem by sampling the ∇ _∼_ _ξt from the_
trajectory of a single Markov chain with stationary distribution Ξ (Sun et al., 2018). The intuition
is that although at early iterations the ξ’s have not converged to their true distribution, the iterates
visited by SGD are also far from the optimum, thus larger approximation error in the early ξ’s is
rather harmless. As we continue iterating, the Markov chain will mix as SGD converges. We show
that the convergence of MCGD can be bounded in terms of the mixing time of that Markov chain.

**Proposition 5. Suppose that we use samples xt from a Markov chain with mixing time tmix. Assume**
_that for all w ∈_ R and all examples xt, Assumption 3 holds. Then with probability at least 1 − _p,_
_Assumption 2 holds with γ = 1, C_ [2] = O[˜](A[2]t[2]mix[)][, and][ Φ][2][ = ˜]O(B[2]t[2]mix[)][.]

It follows that for non-convex optimization in the B = 0 case, our convergence rate is given by

_T = [˜]_ _A[2]Lϵ∆[4]_ _t[2]mix_ _,_ which implies _ϵ[2]_ _AtmixT_ [1]√/2L∆ _._
_O_ _≤_ _O[˜]_
   


Sun et al. (2018, Theorem 2) use a diminishing step size O([1]/t[q]) to obtain T = O(ϵ−2/1−q), where
_q ∈_ ([1]/2, 1). In contrast, our rate is faster and holds with high probability instead of in expectation.

5 NEW EXAMPLE-SELECTION METHODS FOR FASTER CONVERGENCE

The analysis in Section 4 focused on recovering the convergence rates for SGD with known exampleordering algorithms. In this section, we propose two new example-selection approaches that allow
faster convergence: QMC-based data augmentation and greedily minimizing the metric in (2).


-----

**Algorithm 1 Example-Ordered SGD via Greedily Minimizing Average Gradient Error**
**Input: step size α, number of iterations T**, random projection matrix Π, buffer for gradients estimation: gi 0, _i_ 0, _, n_ 1, g 0, initial weights w0, initial permutation σ0.

1: for t = 0 ←, · · · ∀, T/n ∈{ − · · ·1 do _−_ _}_ _←_
2: Initialize : gi 0, _i_ 0, _, n_ 1 ; g 0; .

3: **for i = 0, · · · ←, n − ∀1 do ∈{** _· · ·_ _−_ _}_ _←_ _I ←∅_

4: Update the model parameters: wtn+i+1 _wtn+i_ _α_ _f_ (wtn+i; x[(][σ][t][(][i][))]).
_←_ _−_ _∇_

5: Update the buffers: gσt(i) Π _f_ (wtn+i; x[(][σ][t][(][i][))]); g _g + gσt(i)._

6: **end for** _←_ _∇_ _←_

7: **for i = 0,** _, n_ 1 do
_· · ·_ _−_ 2[]

8: _σt+1(i)_ arg min (gj _g/n)_ ; _σt+1(i)_ .
_←i∈{0,···,n−1}\I_ j∈I∪{i} _−_ _I ←I ∪{_ _}_

9: **end for** P

10: end for
11: return wT

**QMC-based data augmentation.** In many scenarios where only limited examples are given, we
want to augment the dataset for better generalization. More formally, given a transform function
_A that takes example x and a random variable ζ uniformly distributed in [0, 1][s]_ as input, where s
denotes the dimension of augmentation space, the augmented objective for a dataset D of size n is

_f_ (w) = _n[1]_ _x∈DEζ∼U_ [0,1][s] _f_ (w; A(x, ζ)) = _n[1]_ _x∈D_ R[s][ f] [(][w][;][ A][(][x, ζ][))][ dζ.] (3)

P P R

The rationale is that by performing some reasonable random transformation on a given example, we
assume the output would be another example that is identically distributed, and the expected value
models an infinitely-large training set consisting of such transformed examples. For example, in an
image classification task, we could set s = 1 and have A(x, ζ) output the image x rotated by an angle
of 20[◦](2ζ − 1), modeling that a slight rotation of an image should preserve its label.

QMC can approximate this expectation with a low-discrepancy sequence of ζt drawn from the
_s-dimensional unit cube [0, 1][s]. Examples of such sequences include the Halton and Sobol sequences_
(Drmota & Tichy, 2006). QMC is especially favorable for data augmentation because s is usually
small in most data augmentation methods. We propose to use QMC for data augmentation together
with random reshuffling. Concretely, if ζ0, ζ1, . . . is our low-discrepancy sequence and σk denotes
the permutation used by random reshuffling in the kth epoch, then we propose to use the example
_xt = A(x_ (σ⌊t/n⌋ (t mod n)), ζ⌊t/n⌋+σ⌊t/n⌋ (t mod n)). That is, when we sample example i in epoch k, we
use the (k + i)th element of the low-discrepancy sequence. This is not the only reasonable way of
combining QMC and RR: it is just one way we found to work well. In theory we would expect the
approximation error here to decay at the rate _O[˜]([1]/m[2]), instead of the O([1]/m) of the random sampling_
that is standard for data augmentation. To prove this rigorously, we make two additional assumptions
which are commonly used in analyzing QMC sequences (Aistleitner & Dick, 2014).
**Assumption 4 (Bounded gradient variation). There exists a constant V > 0 such that for any fixed**
_w ∈_ R[d] _and x ∈X_ _, the example gradients as a function of ζ under QMC data augmentation, F_ (ζ) =
_∇f_ (w; A(x, ζ)), has Hardy-Krause variation (Tezuka, 2000) at most V, that is VHK(F ) ≤ _V ._
**Assumption 5. The QMC sequence {ζt}t≥0 has low star-discrepancy (Owen, 2003): for all m > 0,**

sup _m1_ _m−11_ _ζt_ [0, a) _s_ _aj_ _m_ _,_

_a∈[0,1][s]_ _t=0_ _{_ _∈_ _} −_ _j=1_ _[≤C][QMC][ ·][ log(][m][)][s]_

P Q

_where [0, a) = {x ∈_ [0, 1][s] _| 0 ≤_ _xj < aj, j = 1, . . ., d} for some constant CQMC._

**Proposition 6. Suppose that we are using the random reshuffling variant of SGD with QMC data**
_augmentation as described. Assume that for all w ∈_ R[d] _and all examples, Assumptions 1, 3,_
_4 and 5 hold for Equation 3. For some p ∈_ (0, 1), set the step size to be a constant such that
_α ≤_ max{1460BnL · log(4e[2]T/p _, 2nL})[−][1]. Then with probability at least 1 −_ _p, Assumption 2_

_holds with _ _γ = 2, C_ [2] = _O[˜](n[2]V_ [2]CQMC[2] [log(][T] [)][2][s][ +][ nA][2][)][ and][ Φ][2][ =][ ˜]O(nB[2]).

Comparing it with Proposition 3, this QMC variant enjoys the same _O[˜]([1]/T_ [2]) rate we get for vanilla
random reshuffling: to our knowledge, this is the first accelerated rate for learning with data augmen

-----

(a) Validation of ResNet20 on CIFAR10 (b) Validation of ResNet20 on CIFAR100

Figure 2: Data augmentation with IID-uniform (standard) and QMC-based methods on CIFAR.

tation. In addition to the theoretical advance, we demonstrate in Section 6 that our QMC variant can
achieve better validation performance in practice in multiple applications.

**Better example ordering via greedy selection.** Taking a closer look at Assumption 2, the
magnitude of the left-hand side plays a crucial role in the convergence. In the ERM setting, this
motivates us to select a permuted example order that minimizes this expression, following which
SGD would converge with minimized average gradient error. However, naively constructing such a
sequence is tedious as iterating over all the τ, m > 0 and all permutations can be computationally
intensive. In light of this, we apply several approximation techniques into the construction and
formulate it into Algorithm 1. The first technique is to use stale gradients, i.e., using the gradients
computed at each epoch to estimate the sequence used in the next epoch (line 5 in Algorithm 1).
The intuition is that based on the smoothness of loss function (Assumption 1), we would expect
the stale gradients to approximate the current gradients with tolerant approximation error as long as
the step size is reasonably small. The second technique is random projection (line 9): we search
a lower-dimensional space, which allows faster construction and reduces memory use (see similar
strategies of using smoothness and dimension reduction techniques in (Caflisch, 1998)). Because this
is a selection-by-permutation method, under Assumption 3 our greedy selection method will trivially
satisfy Assumption 2 with γ = 2, C = nA, and Φ = nB (see Proposition 1).

6 EXPERIMENTS Table 1: Top1 validation accuracy (%) of
ResNet18 on ImageNet. The original one is the
standard benchmark provided by PyTorch.

In this section we evaluate our new algorithms
on several deep learning benchmarks. First, we

Original Uniform (tuned) QMC (untuned)

compare QMC-based data augmentation against
IID-uniform augmentation on CIFAR10/100 and 69.76 70.19 **70.48**
ImageNet datasets. Second, we compare greedy
ordering (Algorithm 1) with RR and SO, and show how to further accelerate it with randomly
projected sorting. Other details on the experimental setup can be found in Appendix A.1.1. Each
experiment is repeated 10 times with consistent seeds among the algorithms.

**QMC-based data augmentation.** We start by training ResNet20 on CIFAR10 and CIFAR100,
where discrete and continuous data augmentations are applied, respectively. Specifically, CIFAR10
uses random crop and random horizontal flip while CIFAR100 uses an additional random rotation
of 15 degrees. To apply the QMC-based data augmentation, we first generate a Sobol sequence of
appropriate dimension using the qmcpy package (Choi et al., 2020+), and then replace the pseudorandom sequence used in the original random augmentation pipelines with that Sobol sequence.
We run the baseline IID-uniform method with finetuned hyperparameters (weight decay 10[−][4]),
which reproduces the result from He et al. (2016) with an error rate 8.4%. Then we run QMC-base
augmentation with the same hyperparameter (untuned) and finetuned counterparts, with a grid search
over weight decay values in {r · 10[−][4]}r[4]=1[. From Figure][ 2][ we observe the QMC-based augmentation]
consistently outperforms the baseline methods, even without hyperparameter tuning. Comparing
Figure 2 with He et al. (2016), we observe the QMC-based augmentation allows ResNet20 to reach
comparable validation accuracy as ResNet44 while requiring only 40% as many parameters (0.27M
vs 0.66M). We run a t-test on these results (in Appendix A.1.2) to show the validation accuracy
from the two augmentation methods are different statistically significantly (p-value p = 7 · 10[−][7] on


-----

(a) Logistic Regression on MNIST. The greedy algorithm reuses the hyperparameters finetuned on RR.

(b) ResNet20 on CIFAR10. The greedy algorithm is able to converge faster when optimizing the same
loss function while achieving SOTA validation accuracy with mild tuning.

Figure 3: Comparison between Algorithm 1 and RR/SO on MNIST and CIFAR10.

CIFAR10 and p=0.036 on CIFAR100). Importantly, this improvement comes essentially for free, as
generating low discrepancy sequences in low dimension has very little overhead.

We also evaluate our method on fine-tuning ResNet18 on ImageNet. We apply different augmentation
methods on a pre-trained model and train for 5 additional epochs with step size 10[−][4]. We report their
Top1 accuracies in Table 6, which shows that our QMC method improves the validation accuracy by
0.3% compared to the same number of epochs of fine-tuning using random sampling.

**Better example ordering via greedy selection.** In this section we evaluate Algorithm 1 on two
benchmarks: Logistic Regression on MNIST and ResNet20 on CIFAR10. As discussed, sorting the
stale gradients naively could incur substantial overhead on memory and computation. To mitigate
this, we adopt two methods: random projection and QR decomposition. The former is mainly
to reduce storage: we obtain a gradient computed at some time in an epoch and project it into a
lower-dimensional space before storing it. Classic ways of projection include Gaussian projection or
random sparsification: we adopt the latter as it does not require storing the projection matrix, which
minimizes the storage cost. After we obtain all the stale gradients, we concatenate them into a matrix
and perform QR decomposition before sorting, which allows us to sort in a low-dimensional space
while preserving the order of gradients since the inner products between any two tensors will remain
the same Gander (1980). We set the target dimension to be of 10% size of the original space. In the
spirit of evaluating the applicability of Algorithm 1, we do not perform hyperparameter tuning in this
section but reuse the ones tuned in the literature on Random Reshuffling.

We plot the results in Figure 3. In Figure 3(a) we observe greedy algorithms can consistently converge
faster than RR and SO epoch-wise with optimizing the same loss function. When QR is used without
projection, the algorithm is able to reach higher validation accuracy but converges slower with respect
to the wall-clock time. On the other hand, when we use random sparsification additionally, the
algorithm converges faster with respect to both epoch and wall-clock time without compromising
the validation accuracy. For CIFAR10, we observe the greedy method can converge faster when
optimizing the same loss as other baselines, and achieves higher validation accuracy when fine-tuned.

7 CONCLUSION

We present a unified analysis on example orderings used in SGD, which generalizes several widelyused orderings in the literature. We propose a greedy algorithm that allows faster convergence via
constructing a better example order with approximate sorting techniques, as well as QMC-based
augmentation that achieves higher validation accuracy on multiple benchmarks. One potential future
direction is designing example orderings to more efficiently minimize the average gradient errors.


-----

ACKNOWLEDGMENTS

The authors would like to thank A. Feder Cooper and anonymous reviewers from ICLR 2022 for their
valuable feedbacks on earlier versions of this paper.

REFERENCES

Naman Agarwal, Rohan Anil, Tomer Koren, Kunal Talwar, and Cyril Zhang. Stochastic optimization
with laggard data pipelines. In Advances in Neural Information Processing Systems, 2020.

Kwangjun Ahn, Chulhee Yun, and Suvrit Sra. SGD with shuffling: optimal rates without component
convexity and large epoch requirements. In Advances in Neural Information Processing Systems,
2020.

Christoph Aistleitner and Josef Dick. Functions of bounded variation, signed measures, and a general
Koksma-Hlawka inequality. arXiv:1406.0230, 2014.

Guillaume Alain, Alex Lamb, Chinnadhurai Sankar, Aaron C. Courville, and Yoshua Bengio. Variance
reduction in SGD by distributed importance sampling. arXiv:1511.06481, 2015.

Haim Avron, Vikas Sindhwani, Jiyan Yang, and Michael W. Mahoney. Quasi-Monte Carlo Feature
Maps for Shift-Invariant Kernels. The Journal of Machine Learning Research, 17:120:1–120:38,
2016.

Francis R. Bach and Eric Moulines. Non-asymptotic analysis of stochastic approximation algorithms
for machine learning. In Advances in Neural Information Processing Systems, pp. 451–459, 2011.

Bernard Bercu, Bernard Delyon, and Emmanuel Rio. Concentration inequalities for sums and
_martingales. SpringerBriefs in Mathematics. Springer, 2015._

Dimitri P. Bertsekas. Incremental Gradient, Subgradient, and Proximal Methods for Convex Optimization: A Survey. In Optimization for Machine Learning. The MIT Press, 2011.

Leon Bottou. Stochastic gradient descent tricks. In´ _Neural networks: Tricks of the trade, pp. 421–436._
Springer, 2012.

Leon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine´
learning. SIAM Review, 60(2):223–311, 2018.

Alexander Buchholz, Florian Wenzel, and Stephan Mandt. Quasi-Monte Carlo Variational Inference.
In Proceedings of the International Conference on Machine Learning, volume 80 of Proceedings
_of Machine Learning Research, pp. 667–676. PMLR, 2018._

Russel E. Caflisch. Monte Carlo and quasi-Monte Carlo methods. Acta Numerica, 7:1–49, 1998.

Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A library for support vector machines. ACM
_transactions on intelligent systems and technology (TIST), 2(3):27, 2011._

Jingrun Chen, Rui Du, Panchi Li, and Liyao Lyu. Quasi-Monte Carlo sampling for machine-learning
partial differential equations. arXiv:1911.01612, 2019.

Dami Choi, Alexandre Passos, Christopher J. Shallue, and George E. Dahl. Faster neural network
training with data echoing. arXiv:1907.05550, 2019.

S.-C. T. Choi, F. J. Hickernell, M. McCourt, and A. Sorokin. QMCPy: A quasi-Monte Carlo Python
[library, 2020+. URL https://github.com/QMCSoftware/QMCSoftware.](https://github.com/QMCSoftware/QMCSoftware)

Mikolaj Cieslak, Christiane Lemieux, Jim Hanan, and Przemyslaw Prusinkiewicz. Quasi-Monte
Carlo simulation of the light environment of plants. Functional Plant Biology, 35(10):837–849,
2008.

Christopher De Sa. Random reshuffling is not always better. In Advances in Neural Information
_Processing Systems, 2020._


-----

Michael Drmota and Robert F/ Tichy. Sequences, discrepancies and applications. Springer, 2006.

Yoel Drori and Ohad Shamir. The complexity of finding stationary points with stochastic gradient
descent. In Proceedings of the International Conference on Machine Learning, volume 119, pp.
2658–2667. PMLR, 2020.

Walter Gander. Algorithms for the QR decomposition. Seminar fur Angewandte Mathematik:¨
_Research report, 80(02):1251–1268, 1980._

Mert Gurb¨ uzbalaban, Asuman E. Ozdaglar, and Pablo A. Parrilo. Convergence rate of incremental¨
gradient and incremental Newton methods. SIAM Journal on Optimization, 29(4):2542–2565,
2019.

Mert Gurb¨ uzbalaban, Asuman E. Ozdaglar, and Pablo A. Parrilo. Why random reshuffling beats¨
stochastic gradient descent. Mathematical Programming, 186(1):49–84, 2021.

Jeff Z. HaoChen and Suvrit Sra. Random shuffling beats SGD after finite epochs. In Proceedings of
_the International Conference on Machine Learning, volume 97, pp. 2624–2633, 2019._

Thomas P. Hayes. A large-deviation inequality for vector-valued martingales. Combinatorics,
_Probability and Computing, 2005._

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770–778, 2016.

Tito Homem-de Mello. On rates of convergence for stochastic optimization problems under non–
independent and identically distributed sampling. SIAM Journal on Optimization, 19(2):524–551,
2008.

Wolfgang Jank. Quasi-Monte Carlo sampling to improve the efficiency of Monte Carlo EM. Compu_tational statistics & data analysis, 48(4):685–701, 2005._

Corwin Joy, Phelim P. Boyle, and Ken Seng Tan. Quasi-Monte Carlo methods in numerical finance.
_Management Science, 42(6):926–938, 1996._

Kenji Kawaguchi and Haihao Lu. Ordered SGD: A new stochastic optimization framework for
empirical risk minimization. In The 23rd International Conference on Artificial Intelligence and
_Statistics, volume 108, pp. 669–679, 2020._

Alexander Keller. A Quasi-Monte Carlo Algorithm for the Global Illumination Problem in the
Radiosity Setting. In Monte Carlo and Quasi-Monte Carlo Methods in Scientific Computing, pp.
239–251. Springer New York, 1995.

Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with
differentiable convex optimization. In Proceedings of the IEEE/CVF Conference on Computer
_Vision and Pattern Recognition, pp. 10657–10665, 2019._

David A. Levin and Yuval Peres. Markov chains and mixing times, volume 107. American Mathematical Society, 2017.

Sifan Liu and Art B. Owen. Quasi-Newton Quasi-Monte Carlo for variational Bayes.
_arXiv:2104.02865, 2021._

Ilya Loshchilov and Frank Hutter. Online batch selection for faster training of neural networks.
_arXiv:1511.06343, 2015._

Yucheng Lu, Youngsuk Park, Lifan Chen, Yuyang Wang, Christopher De Sa, and Dean Foster.
Variance reduced training with stratified sampling for forecasting models. In Proceedings of the
_International Conference on Machine Learning, pp. 7145–7155. PMLR, 2021._

Konstantin Mishchenko, Ahmed Khaled, and Peter Richtarik. Random reshuffling: Simple analysis´
with vast improvements. In Advances in Neural Information Processing Systems, 2020.


-----

Deanna Needell, Rachel Ward, and Nathan Srebro. Stochastic Gradient Descent, Weighted Sampling,
and the Randomized Kaczmarz algorithm. In Advances in Neural Information Processing Systems,
pp. 1017–1025, 2014.

Lam M. Nguyen, Quoc Tran-Dinh, Dzung T. Phan, Phuong Ha Nguyen, and Marten van Dijk. A
unified convergence analysis for shuffling-type gradient methods. arXiv:2002.08246, 2020.

Art B. Owen. Quasi-Monte Carlo sampling. Monte Carlo Ray Tracing: SIGGRAPH, 1:69–88, 2003.

Guillaume Papa, Pascal Bianchi, and Stephan Cl´ emen´ c¸on. Adaptive sampling for incremental
optimization using stochastic gradient descent. In Algorithmic Learning Theory, volume 9355 of
_Lecture Notes in Computer Science, pp. 317–331. Springer, 2015._

Teemu Pennanen. Epi-convergent discretizations of multistage stochastic programs. Mathematics of
_Operations Research, 30(1):245–256, 2005._

Pete Philipson, Graeme L. Hickey, Michael J. Crowther, and Ruwanthi Kolamunnage-Dona. Faster
Monte Carlo estimation of joint models for time-to-event and multivariate longitudinal data.
_Computational Statistics & Data Analysis, 151:107010, 2020._

Benjamin Recht and Christopher Re. Toward a noncommutative arithmetic-geometric mean inequality:´
Conjectures, case-studies, and consequences. In Conference on Learning Theory, volume 23, pp.
11.1–11.24, 2012.

Herbert Robbins and Sutton Monro. A Stochastic Approximation Method. The Annals of Mathemati_cal Statistics, 22(3):400 – 407, 1951._

Itay Safran and Ohad Shamir. How good is SGD with random shuffling? In Conference on Learning
_Theory, volume 125 of Proceedings of Machine Learning Research, pp. 3250–3284. PMLR, 2020._

Itay Safran and Ohad Shamir. Random shuffling beats SGD only after many epochs on ill-conditioned
problems. arXiv:2106.06880, 2021.

Mark Schmidt, Nicolas Le Roux, and Francis R. Bach. Minimizing finite sums with the stochastic
average gradient. Mathematical Programming, 162(1-2):83–112, 2017.

Tao Sun, Yuejiao Sun, and Wotao Yin. On Markov chain gradient descent. In Advances in Neural
_Information Processing Systems, pp. 9918–9927, 2018._

Shu Tezuka. Discrepancy theory and its application to finance. In IFIP International Conference on
_Theoretical Computer Science, pp. 243–256. Springer, 2000._

Florian Wenzel, Alexander Buchholz, and Stephan Mandt. Quasi-Monte Carlo Flows. In Proceedings
_of the 3rd Workshop on Bayesian Deep Learning, 2018._

Jiyan Yang, Vikas Sindhwani, Haim Avron, and Michael W. Mahoney. Quasi-Monte Carlo Feature
Maps for Shift-Invariant Kernels. In Proceedings of the International Conference on Machine
_Learning, volume 32, pp. 485–493, 2014._

Bicheng Ying, Kun Yuan, Stefan Vlaski, and Ali H. Sayed. On the performance of random reshuffling
in stochastic learning. In 2017 Information Theory and Applications Workshop (ITA), pp. 1–5.
IEEE, 2017.

Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Open problem: Can single-shuffle SGD be better than
reshuffling SGD and GD? In Conference on Learning Theory, 2021.

Tong Zhang. Solving large scale linear prediction problems using stochastic gradient descent
algorithms. In Proceedings of the International Conference on Machine Learning, volume 69, pp.
116, 2004.

Peilin Zhao and Tong Zhang. Stochastic optimization with importance sampling for regularized
loss minimization. In Proceedings of the 32nd International Conference on Machine Learning,
volume 37, pp. 1–9, 2015.


-----

A APPENDIX

TABLE OF CONTENTS

A.1 Experiment details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14

A.1.1 Additional details for Section 6 . . . . . . . . . . . . . . . . . . . . . . . 14

A.1.2 _t-test for data augmentation results . . . . . . . . . . . . . . . . . . . . . ._ 14

A.1.3 Small experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14

A.1.4 Optimal step size derivation . . . . . . . . . . . . . . . . . . . . . . . . . 16

A.2 Convergence analysis: diminishing step size . . . . . . . . . . . . . . . . . . . . . 18

A.2.1 Non-convex case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18

A.2.2 Strongly-convex case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

A.3 Proof for Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

A.4 Convergence analysis: constant step size . . . . . . . . . . . . . . . . . . . . . . . 28

A.4.1 Non-convex case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28

A.4.2 Strongly-convex case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29

A.5 Justifications for Assumption 2 under various example orderings . . . . . . . . . . 30

A.5.1 Arbitrary permutation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30

A.5.2 Shuffle once . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30

A.5.3 Random reshuffling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33

A.5.4 Random reshuffling with data echoing . . . . . . . . . . . . . . . . . . . . 37

A.5.5 Markov chain gradient descent . . . . . . . . . . . . . . . . . . . . . . . . 38

A.5.6 QMC-based data augmentation with random reshuffling . . . . . . . . . . 40

A.6 Miscellaneous lemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42


-----

A.1 EXPERIMENT DETAILS

A.1.1 ADDITIONAL DETAILS FOR SECTION 6

In Section 6, all the training scripts are implemented via PyTorch1.6 and run on a single machine
configured with an 2.6GHz 4-core Intel (R) Xeon(R) CPU, 16GB memory and NVIDIA GeForce
GTX 1080Ti with CUDA 10.1.

In the example ordering comparison, we use the same seed among different algorithms in the same
run so as to guarantee every algorithm works with the same loss function.

In the ImageNet experiment, the standard step size schedule for ImageNet training is starting at
0.1 and decaying by 10 every 30 epochs. The pre-trained model is the trained model at epoch 90.
Naturally, our learning rate should be 1e-4 by the same schedule. Other hyperparameters are adopted
[by the open source implementation: https://github.com/pytorch/examples/tree/](https://github.com/pytorch/examples/tree/master/imagenet)
[master/imagenet.](https://github.com/pytorch/examples/tree/master/imagenet)

In the data augmentation comparison, obviously other augmentation techniques can be used.
[The strategies we used are taken from open source implementation https://github.com/](https://github.com/akamaster/pytorch_resnet_cifar10)
[akamaster/pytorch_resnet_cifar10 and https://github.com/weiaicunzai/](https://github.com/akamaster/pytorch_resnet_cifar10)
[pytorch-cifar100 that can reproduce the validation accuracy in He et al. (2016), so that our](https://github.com/weiaicunzai/pytorch-cifar100)
comparison can be consistent with the correct benchmarks. Below we include the convergence plot
for the experiment on ImageNet with ResNet18, where each algorithm is repeated three times with
seeds uniformly selected from [0, 1000].

Figure 4: Data augmentation with IID-uniform (standard) and QMC-based methods on ImageNet.

A.1.2 _t-TEST FOR DATA AUGMENTATION RESULTS_

We now perform t-test on the validation accuracy on two data augmentation results, to show the
improvement from QMC is statistically significant. We include validation accuracy at epoch 200
for CIFAR10 and best accuracy for CIFAR100 in Table A.1.2. We compute the p-value between
IID-Uniform and tuned QMC, and found the p-value on CIFAR10 and CIFAR100 to be 7e-7 and
0.036, respectively. Since they are both smaller than 0.05, we reject the null hypothesis which
concludes they are of the same mean.

A.1.3 SMALL EXPERIMENTS

For both experiments in Section 3, we generated w[∗] from a standard Normal distribution. The
minibatch size is 1, and the total number of iterations is 10[7], with n = 10[4] over 1000 epochs for the
offline setting. For all variants, we use the theoretical step size optimized for SGD with replacement
(corresponding to IID Uniform) on this particular problem, given by

_αt(1_ _αt)_ _w[∗]_
_αt+1 =_ _−_ with _α0 =_ _∥_ _∥[2]_ _,_

1 − _αt[2][(][d][ + 2)]_ _∥w[∗]∥[2](d + 2) + d_

which are derived in Appendix A.1.4 below. To obtain low-discrepancy samples from the Gaussian
distribution, we use the inverse transform method. First we obtain the QMC sequences (Sobol in our


-----

Table 2: Validation accuracy for different data augmentation methods on CIFAR datasets.

CIFAR10 CIFAR100
Runs IID-Uniform QMC QMC (tuned) IID-Uniform QMC QMC (tuned)


1 91.87 92.05 92.45 67.92 68.91 68.21
2 91.67 92.13 92.53 68.01 68.92 68.12
3 91.68 92.03 92.33 68.03 68.53 68.42
4 91.88 91.9 92.35 67.82 68.03 68.71
5 92.05 92.03 92.41 68.89 68.41 67.99
6 91.79 91.93 92.15 66.99 68.66 68.51
7 91.91 92.79 92.59 68.31 68.92 68.53
8 91.53 91.73 92.44 68.14 68.09 68.53
9 91.3 91.96 92.21 68.02 67.91 68.12
10 91.82 92.21 92.16 67.73 68.31 68.53

case) ζt [0, 1][s] where s is the appropriate dimension, and then use that in the inverse CDF function
of a Gaussian distribution to obtain the corresponding Gaussian sample. ∈

In addition to using synthetic data, we also performed an offline version of the experiment Figure 1(b)
on a real dataset, a6a from the LIBSVM repository (Chang & Lin, 2011). The dataset contains
_n=11220 examples with d=124 features (including bias), and all labels are binary. We use logistic_
regression with ℓ2 regularization (with λ=1e-4) as the empirical risk:


_Rn(w) = [1]_


_n_

log(1 + exp( _yixTi_ _[w][)) +][ λ]_
_−_ 2
_i=1_ _[∥][w][∥][2][.]_

X


To measure the left hand side of Assumption 2, we again fix wτ to be at the origin, and we take the
number of epochs to be 1000. The results are in Figure 5 for one run with an arbitrarily-set seed, and
we draw similar conclusions as observed with synthetic data (see Figure 1(b) in the main paper).


10

10

10

10

10

10


1m


|Offline (a6a)|Col2|
|---|---|
|IID Uniform Sobol RR SO||
|03 104 105 10 m|6 107|


Figure 5: Comparison of sampling schemes on ℓ2-regularized logistic regression with real data.


-----

A.1.4 OPTIMAL STEP SIZE DERIVATION

In this section, we derive the optimal step-size sequence used in our synthetic toy example from
Section 3. Recall our setup
_x_ (0, Id), _y = xTw∗_ + ϵ
_∼_ _N_
where w[∗] is the true model parameters that we are trying to recover, and ϵ ∼ _N_ (0, 1) is some intrinsic
error. The goal is to minimize the expected risk

T 2[]

_f_ (w) = [1] (x _w_ _y)_ _._

2 [E] _−_

By using the tower rule of expectation conditioning on _x, this objective can be written as_


_f_ (w) = [1]

2 [(][∥][w][ −] _[w][∗][∥][2][ + 1)][.]_

The full gradient and the example gradient given a pair (x, y) drawn from the above distribution are


_∇f_ (w∇;f x, y(w) = () = wx −Tww −[∗],y)x = (xT(w − _w∗) −_ _ϵ)x._

Define the gradient error to be

∆[g] := _f_ (w; x, y) _f_ (w) = (xxT _Id)(w_ _w∗)_ _ϵx._ (4)
_∇_ _−∇_ _−_ _−_ _−_

Let us first analyze the expected gradient error conditioned on any randomness in w that might arise
from the sampling history:

E _∥∆[g]∥[2][i]_ = (w − _w[∗])TE_ (xxT − _Id)2[]_ _−_ 2E[ϵxT(xxT − _Id)](w −_ _w∗) + E[ϵ2xTx]._

The last term is simplyh E[ϵ[2]x[T]x] = d since ϵ is independent of x. The middle term cancels to 0 again
by independence and zero-mean of ϵ. For the first term,

T 2[] T T T
E (xx _−_ _Id)_ = E[xx _xx_ _−_ 2xx + I]
 = (d + 2)Id 2Id + Id = (d + 1)Id.

_−_

Together, we have

E _∥∆[g]∥[2][i]_ = (d + 1)∥w − _w[∗]∥[2]_ + d. (5)
h

Now let us derive an suboptimality gap recursion for the SGD update step using the step size αt:

_wt+1_ _w[∗]_ = wt _αt_ _f_ (wt; x, y) _w[∗]_
_−_ _−_ _∇_ _−_

= (wt _w[∗]) + αt_ _f_ (wt) _αt_ _f_ (wt) _αt_ _f_ (wt; x, y)
_−_ _∇_ _−_ _∇_ _−_ _∇_

= (1 − _αt)(wt −_ _w[∗]) −_ _αt∆[g]t_ _[,]_

where ∆[g]t [is the expression in Eq. (][4][) with][ w][t] [as input. The expected suboptimality gap conditioned]
on all randomness up to timestep t is then

Eh∥wt+1 − _w[∗]∥[2][i]_ = (1 − _αt)[2]Eh∥wt −_ _w[∗]∥[2][i]_ _−_ 2(1 − _αt)E[((wt −_ _w[∗])T∆gt_ [] +][ α]t[2][E]h∥∆[g]t _[∥][2][i]._

Observe that under iid uniform sampling, E[∆[g]t [] = 0][, and using Eq. (][5][) gives us]

E _∥wt+1 −_ _w[∗]∥[2][i]_ = (1 − _αt)[2]∥wt −_ _w[∗]∥[2]_ + αt[2][(][d][ + 1)][∥][w][t] _[−]_ _[w][∗][∥][2][ +][ α]t[2][d.]_
h

Taking an expectation over the entire history to remove conditioning and let ρt := E _∥wt −_ _w[∗]∥[2][i],_
h

_ρt+1 = (1 −_ _αt)[2]ρt + αt[2][(][d][ + 1)][ρ][t]_ [+][ α]t[2][d]

= (1 − 2αt + αt[2][(][d][ + 2))][ρ][t] [+][ α]t[2][d.] (6)

Note that the RHS is convex in αt, we can differentiate to minimize the expected suboptimality at
every iteration ρt+1 = E _∥wt+1 −_ _w[∗]∥[2][i]:_
h _ρt_

0 = ( 2 + 2αt(d + 2))ρt + 2αtd _αt =_
_−_ _⇒_ (d + 2)ρt + d _[.]_


-----

This also gives us an expression for ρt in terms of αt:


_ρt =_


_αt[−][1]_ (d + 2) _[,]_
_−_


which combined with Eq. (6) gives us


_ρt+1 = (1 −_ _αt)ρt = (1 −_ _αt)_ _αt[−][1]_ (d + 2) _[.]_

_−_

Finally, the optimal step-size sequence can be implemented via the following recursion


1 = d + 2 + _d_ = d + 2 + _[α]t[−][1]_ _−_ (d + 2)

_αt+1_ _ρt+1_ 1 _αt_

_−_
1 _d_

= d + 2 + _[d]_ = d + 2 +
_α0_ _ρ0_ _w0_ _w[∗]_

_∥_ _−_ _∥[2][ .]_

It is easy to verify that this is indeed a decreasing sequence. If we initialize at w0 = 0, then our step
sizes are given by

_αt(1_ _αt)_ _w[∗]_
_αt+1 =_ _−_ with _α0 =_ _∥_ _∥[2]_ _._

1 − _αt[2][(][d][ + 2)]_ _∥w[∗]∥[2](d + 2) + d_


-----

A.2 CONVERGENCE ANALYSIS: DIMINISHING STEP SIZE

In the main text, our convergence results focused on the constant step size regime for simplicity of
presentation and ease of comparison to prior works, as they often lack diminishing step size results
for permutation-based SGD variants. Here we formally present the convergence rate under our main
Assumption 2 using a diminishing step size sequence. In Theorems 3 and 4 that we present below,
the rate is of the same order as what we obtained under a constant step size (Theorems 1 and 2) up to
a factor of log(T ) that typically arises when using a diminishing step size.

Our analysis for the diminishing step size setting is accomplished by breaking the total number of
iterations into phases. Suppose we want to run a total of T iterations of SGD updates. We will break
the analysis into I number of phases. In each phase i = 0, . . . I − 1, we run K [(][i][)]m[(][i][)] number of
iterations with constant step size α[(][i][)]. The step size is decayed at the beginning of each phase. For
this, we define the triplets (α[(][i][)], m[(][i][)], K [(][i][)]), where _m[(][i][)]_ is an increasing sequence.

Recall that γ and Φ are constants in Assumption 2. In the _non-convex case, the inner interval length_
is chosen to be

_m[(][i][)]_ = ⌈3(Φ + 1)2/γ⌉2i,

where as for the strongly-convex setting it is

_m[(][i][)]_ = ⌈3(Φ + 1)2/γ · ei/γ⌉.

For both function classes the diminishing step-size is set to


_α[(][i][)]_ =


1

(7)
6Lm[(][i][)][ .]


A.2.1 DIMINISHING STEP SIZE: NON-CONVEX CASE

**Theorem 3 (Diminishing step size: non-convex case). Suppose that our setup satisfies Assumptions 1**
_and 2, and let ϵ > 0 be any target error. After T iterations of SGD (Eq. (1)) with a diminishing_
_step-size (Eq. (7)), we can obtain_

16 _L∆+ C_ [2](3(Φ + 1))[−][2] log(T )

_t=0min,...,T −1[∥∇][f]_ [(][w][t][)][∥][2][ ≤] [12 log][2] 3(Φ + 1) _[T]_  _T_ _γ/1+γ(12(Φ + 1)2)−γ/1+γ −_ 1 [=][ O] _T_ _γ/1+γ_ .

_This implies the number of gradient evaluations to achieve mint_ _f_ (wt) _ϵ[2]_ _is_
_∥∇_ _∥[2]_ _≤_


_T = O[˜](ϵ_ _−2(1+γ_ _γ)_


).


_Proof. To apply Lemma 1 to the iterations within a particular phase i, we need_


3Φ ≤ _mγ/2_ and _α ≤_


6Lm


where we have temporarily dropped the index (i) for convenience. Lemma 1 gives us the following
bound on the objective value between any length-m number of iterations within that phase: for
_τ ∈{0, m −_ 1, 2m − 1, . . ., (K − 1)m − 1},

_f_ (wτ +m) _f_ (wτ )
_≤_ _−_ _[η]4_ _[∥∇][f]_ [(][w][τ] [)][∥][2][ + 2][C]η [2] _[α][2][m][2][−][γ]_

_f_ (wτ )
_≤_ _−_ _[αm]4_

_[∥∇][f]_ [(][w][τ] [)][∥][2][ + 2][C] [2][αm][1][−][γ][,]

since the step size is constant throughout one phase. Summing over K such intervals from τ = 0
followed by a telescope,


_K−1_

_∥∇f_ (wmk)∥[2] _≤_ _f_ (w0) − _f_ (wmK−1) + 2C [2]Kαm[1][−][γ].
_k=0_

X


_αm_


-----

We now restore our phase index,


_K[(][i][)]−1_

_∥∇f_ (wmk)∥[2] _≤_ _f_ (w0) − _f_ (wm(i)K(i)−1) + 2C [2]K [(][i][)]α[(][i][)](m[(][i][)])[1][−][γ]
_k=0_

X


_α[(][i][)]m[(][i][)]K_ [(][i][)]


_K_ [(][i][)]


= f (w0) − _f_ (wm(i)K(i)−1) + _[C]3L[2]_ _[K]_ [(][i][)][(][m][(][i][)][)][−][γ][,]

where we have chosen the largest possible α[(][i][)] = [1]/6Lm[(][i][)] for all i. Summing over I phases and
letting ∆:= f (w0) _f_,
_−_ _[∗]_


_K[(][i][)]−1_ _I−1_

_∥∇f_ (wm(i)k)∥[2] _≤_ _I∆+_ _[C]3L[2]_ _K_ [(][i][)](m[(][i][)])[−][γ]
_k=0_ _i=0_

X X


_I−1_

_i=0_

X


_α[(][i][)]m[(][i][)]K_ [(][i][)]


_K_ [(][i][)]


where the left hand side can be further lower bounded with


_I−1_

_i=0_

X


_I−1_

min _K_ [(][i][)](m[(][i][)])[−][γ],
_t=0,...,T −1[∥∇][f]_ [(][w][t][)][∥][2][ ≤] _[I][∆+][ C]3L[2]_ _i=0_

X


_α[(][i][)]m[(][i][)]K_ [(][i][)]


using the inclusion that 0, m[(][i][)] 1, . . ., K [(][i][)]m[(][i][)] 1 _Ii=0−1_
_−_ _−_ _[⊂{][0][, . . ., T][ −]_ [1][}][. Now let us choose]
 2/γ _i_

_m[(][i][)]_ = ⌈(3(Φ + 1)) _⌉2_

_K_ [(][i][)] = ⌈2[iγ]⌉.

Here the choice of m[(][i][)] guarantees that

_γ/2_ 2/γ _i[][γ][/][2]_
(m[(][i][)]) = _⌈(3(Φ + 1))_ _⌉2_
 2/γ _γ/2_ _i_ _γ/2_

=≥ ⌈(3(Φ + 1))2(3(Φ + 1)) _iγ/2⌉ ≥_ (23Φ).

Next, observe that for the last term in the previous sum,


_I−1_

_K_ [(][i][)](m[(][i][)])[−][γ] =

_i=0_

X


_I−1⌈2[iγ]⌉(⌈(3(Φ + 1))2/γ⌉2i)−γ_

_i=0_

X


_I−1_

_i=0_

X


2[iγ] + 1 ((3(Φ + 1))2/γ2i)−γ


_I−1_ 2(2[iγ])((3(Φ + 1))2/γ2i)−γ

_i=0_

X


_I−1_

= 2(3(Φ + 1))[−][2]2[iγ]2[−][iγ]

_i=0_

X

= 2I(3(Φ + 1))[−][2].

min (8)
_t=0,...,T −1[∥∇][f]_ [(][w][t][)][∥][2][ ≤] _[I][∆+ 2]3[C]L[2]_

_[·][ I][(3(Φ + 1))][−][2][.]_


Combining with the above,


_I−1_

_i=0_

X


_α[(][i][)]m[(][i][)]K_ [(][i][)]


Furthermore, using our choice of α[(][i][)]


_I−1_

_i=0_

X


_I−1_

_K_ [(][i][)].

_i=0_

X


_α[(][i][)]m[(][i][)]K_ [(][i][)]


= [1]

6L


-----

Re-arranging Eq. (8),

3 _[C]_ [2][ ·][ I][(3(Φ + 1))][−][2]

_t=0min,...,T −1[∥∇][f]_ [(][w][t][)][∥][2][ ≤] [6][IL][∆+][ 2] _Ii=0−1_ _[K]_ [(][i][)] _._ (9)

The total number of iterations is given by P


_I−1_

_K_ [(][i][)]m[(][i][)]

_i=0_

X

_I−1⌈2[iγ]⌉· ⌈(3(Φ + 1))2/γ⌉2i_

_i=0_

X


_T =_


_≥_ (3(Φ + 1))2/γ _I−1_ 2[(][γ][+1)][i]

_i=0_

X

2/γ [2][(][γ][+1)][I][ −] [1] 2/γ [2][(][γ][+1)][I][ −] [1]
= (3(Φ + 1)) (3(Φ + 1))

2[γ][+1] 1 _≥_ 2[γ][+1]
_−_

Solving for I and using lg to denote log2, we obtain


2[γ][+1]
lg (3(Φ + 1))2/γ _[T][ + 1]_



2[γ][+2]
lg (3(Φ + 1))2/γ _[T]_



_I ≤_


_γ + 1_


_γ + 1_


Moreover, we can also upper bound T using similar arguments,


_T =_ _I−1⌈2[iγ]⌉· ⌈(3(Φ + 1))2/γ⌉2i_

_i=0_

X

_≤_ _I−1_ 2(2[iγ]) · 2(3(Φ + 1))2/γ2i

_i=0_

X


= 4(3(Φ + 1))2/γ _I−1_ 2[(][γ][+1)][i]

_i=0_

X

2/γ [2][(][γ][+1)][I]
= 4(3(Φ + 1))

2[γ][+1] 1 _[,]_
_−_


which gives

We are left to bound


4(3(Φ + 1))2[γ][+1] _−_ 1 2/γ

=:Γ
| {z


_I ≥_


lg


_γ + 1_


_I−1_ _I−1_

= 2[iγ] 2[iγ] = [2][γI][ −] [1]

_⌈_ _⌉≥_ 2[γ] 1
_i=0_ _i=0_ _−_

Xγ/1+γ lg(ΓT )X1

_−_
_≥_ [2] 2[γ]

= [(Γ][T] [)]γ/1+γ − 1

2[γ]

_γ/1+γ_
4(3(Φ+1))2[γ][+1]−1[2]/γ _[T]_ 1
_−_

=

 2[γ] 


_I−1_

_K_ [(][i][)] =

_i=0_

X


_I−1_

_⌈2[iγ]⌉≥_
_i=0_

X


-----

using our lower bound for I. Substituting this and the upper bound for I into Eq. (9),

3 _[C]_ [2][ ·][ I][(3(Φ + 1))][−][2]

_t=0min,...,T −1[∥∇][f]_ [(][w][t][)][∥][2][ ≤] [6][IL][∆+][ 2] _Ii=0−1_ _[K]_ [(][i][)]

1 P 2[γ][+2] 6L∆+ 23 _[C]_ [2][(3(Φ + 1))][−][2]
_≤_  _γ + 1_  lg (3(Φ + 1))2/γ _[T]_  4(3(Φ+1))2[γ][+1] _−1[2]/γ_ _[T]_ _γ/1+γ_ _−1_

 

2[γ]

16 _L∆+ C_ 2(3(Φ + 1))−2
_≤_ 12 lg 3(Φ + 1) _[T]_ 2[γ][+1] 1 _γ/1+γ_
  4(3(Φ+1))− [2]/γ _[T]_ 1

_−_

16  _L∆+ C_ [2](3(Φ + 1)) _[−][2]_
_≤_ 12 lg 3(Φ + 1) _[T]_ _T_ _γ/1+γ(12(Φ + 1)2)−γ/1+γ_ 1
  _−_

log(T )
= O _T_ _γ/1+γ_ _,_
 


which yields our convergence rate in the nonconvex setting.

A.2.2 DIMINISHING STEP SIZE: STRONGLY-CONVEX CASE

**Theorem 4 (Diminishing step size: strongly-convex case). Suppose f is µ-Polyak-Łojasiewicz (PL),**
_and that our setup satisfies Assumptions 1 and 2 Let ϵ > 0 be any target error, and κ =_ _[L]/µ be the_
_condition number of the problem. After T iterations of SGD (Eq. (1)) with a diminishing step-size_
_(Eq. (7)), we can obtain_

_f_ (wT ) − _f_ _[∗]_ _≤_ 2 12κ _e(3(Φ + 1))1/γ −_ 1 2/γ _−γ_ ∆+ [4][C]µ [2] _e(3(Φ + 1))(1_ _ρ)[⌈]12[−]κ[2]_ = O 1T _[γ]_ _._
 _⌈_ _⌉_   _−_ _⌉_   

_This implies the number of gradient evaluations to achieve f_ (wT ) − _f_ _[∗]_ _≤_ _ϵ[2]_ _is_

_T = O(ϵ−2/γ)._

_Proof. We will begin with the same analysis technique as used in the non-convex case. To apply_
Lemma 1 to the iterations within a particular phase i, we need


3Φ ≤ _mγ/2_ and _α ≤_


6Lm


dropping the index (i) for convenience and will re-introduce it later when appropriate. Lemma 1
gives us the following bound on the objective value between any length-m number of iterations
within that phase: for τ ∈{0, m − 1, . . ., (K − 1)m − 1},

_f_ (wτ +m) _f_ (wτ )
_≤_ _−_ _[η]4_ _[∥∇][f]_ [(][w][τ] [)][∥][2][ + 2][C]η [2] _[α][2][m][2][−][γ]_

_f_ (wτ )
_≤_ _−_ _[αm]4_

_[∥∇][f]_ [(][w][τ] [)][∥][2][ + 2][C] [2][αm][1][−][γ][,]


since the step size is constant throughout one phase. Strong-convexity (or the Polyak-Łojasiewicz
(PL) inequality) of f implies ∥∇f (w)∥[2] _≥_ 2µ(f (w) − _f_ _[∗]) for all w ∈_ R[d]. Using this while
subtracting f _[∗]_ on both sides leads to


_f_ (wτ +m) _f_ 1
_−_ _[∗]_ _≤_ _−_ _[αmµ]2_



(f (wτ ) _f_ ) + 2C [2]αm[1][−][γ].
_−_ _[∗]_


-----

Applying this recursively K times gives

_K_ 1
_K_ _−_

_f_ (wmK) _f_ 1 (f (w0) _f_ ) + 2C [2]αm[1][−][γ]
_−_ _[∗]_ _≤_ _−_ _[αmµ]2_ _−_ _[∗]_

_k=0_

  X

_K_ _∞_

1 (f (w0) _f_ ) + 2C [2]αm[1][−][γ]
_≤_ _−_ _[αmµ]2_ _−_ _[∗]_

_k=0_

  X

_µ_ _K_ 2
= 1 (f (w0) _f_ ) + 2C [2]αm[1][−][γ]
_−_ 12L _−_ _[∗]_ _αmµ_
  _K_

1
= 1 (f (w0) _f_ ) + 4C [2]m[−][γ]µ[−][1],
_−_ 12κ _−_ _[∗]_
 

where we have used


_k_

1
_−_ _[αmµ]2_
 

_k_

1
_−_ _[αmµ]2_




1 _µ_
_α_ as µ _L._
_≤_ 6Lm [=]⇒ _[αmµ]2_ _≤_ 12L [<][ 1] _≤_

We now restore the phase index. Letting Ti := _i[′]=0_ _[m][(][i][′][)][K]_ [(][i][′][)][ be the total number of iterations]
passed after i phases so that T = TI, and ∆t := f (wt) − _f_ _[∗],_

[P][i][−]K[1][(][i][)]

1
_f_ (wTi+1 ) − _f_ _[∗]_ _≤_ 1 − 12κ ∆Ti + [4][C]µ [2] [(][m][(][i][)][)][−][γ][.]
 

Now let us choose

_m[(][i][)]_ = ⌈(3(Φ + 1))2/γ · ei/γ⌉

1
_K_ [(][i][)] = K = 12κ = _−_
_⌈_ _⌉_ _⌈_ log(1 _/12κ)_ _[⌉≥]_ [12][.]

_−_ [1]

Here the choice of m[(][i][)] guarantees that


_γ/2_
(m[(][i][)])γ/2 = _⌈(3(Φ + 1))2/γ · ei/γ⌉_ _≥_ (3(Φ + 1))2/γ[][γ][/][2]ei/2 ≥ 3Φ.
  

Using the constant K across all phases, our recursion can be simplified to


_K_

1
_f_ (wTi+1 ) − _f_ _[∗]_ _≤_ 1 − 12κ ∆Ti + [4][C]µ [2] [(][m][(][i][)][)][−][γ][.]
 

Since this holds for any i = 0, . . ., I − 1, applying recursion over I phases yields,

_I−1_

_f_ (wT ) − _f_ _[∗]_ _≤_ (1 − _ρ)[KI]_ ∆0 + [4][C]µ [2] (1 − _ρ)[Ki](m[(][I][−][i][)])[−][γ]_ (10)

_i=0_

X


_f_ (wTi+1 ) − _f_ _[∗]_ _≤_ 1 −



12κ


where we defined ρ := [1]/12κ. Observe that for our choice of m[(][i][)] and K,


_I−1_

_K_ [(][i][)]m[(][i][)] = K

_i=0_

X


_I−1⌈(3(Φ + 1))2/γ · ei/γ⌉_

_i=0_

X


_T =_


_I−1_ 2(3(Φ + 1))2/γ _ei/γ_

_·_
_i=0_

X


_≤_ _K_


= 2K(3(Φ + 1))2/γ _I−1_ _ei/γ_

_i=0_

2/γ X 2/γ

_I/γ_ _I/γ_

= [2][K][(3(Φ + 1))]e1/γ 1 (e _−_ 1) ≤ [2][K][(3(Φ + 1))]e1/γ 1 _e_ _,_

_−_ _−_


which implies


_e1/γ_ 1
_I ≥_ _γ log_ 2K(3(Φ + 1)) − 2/γ _[T]_



-----

and so

note that

This gives us


_e1/γ_ 1
(1 − _ρ)[KI]_ _≤_ exp(−ρKI) ≤ exp _−ρKγ · log_ 2K(3(Φ + 1)) − 2/γ _[T]_
  

1
_ρKγ = γρ_ _−_
_⌈_ log(1 _ρ)_ _[⌉≥]_ _[γ.]_

_−_


_e1/γ_ 1 _−γ_
(1 − _ρ)[KI]_ _≤_ 2K(3(Φ + 1)) − 2/γ _T_ _[−][γ]._ (11)
 


Substituting this into Eq. (10),


_−γ_
_T_ + [4][C] [2]

_[−][γ]_ _µ_



_e1/γ_ 1

2K(3(Φ + 1)) − 2/γ


_I−1_

(1 − _ρ)[Ki](m[(][I][−][i][)])[−][γ]._ (12)
_i=0_

X


_f_ (wT ) − _f_ _[∗]_ _≤_ ∆0

For the last term,


_I−1_

(1 − _ρ)[Ki](m[(][I][−][i][)])[−][γ]_ =
_i=0_

X


_I−1(1 −_ _ρ)[Ki]⌈(3(Φ + 1))2/γ · eI−i/γ⌉−γ_

_i=0_

X


_I−1_
_≤_ (3(Φ + 1))[−][2] (1 − _ρ)[Ki]e[i][−][I]_

_i=0_

X


_I−1_
= (3(Φ + 1))[−][2]e[−][I] (e(1 − _ρ)[K])[i]_

_i=0_

X

= (3(Φ + 1))[−][2]e[−][I][ (][e][(1][ −] _[ρ][)][K][)][I]_

_e(1 −_ _ρ)[K]_

= (3(Φ + 1))[−][2][ (1][ −] _[ρ][)][KI]_

_e(1_ _ρ)[K][ .]_
_−_

Substituting these into Eq. (12), and using Eq. (11) to bound (1 − _ρ)[KI]_ gives us the convergence rate
for strongly-convex functions using a diminishing step size:


_f_ (wT ) − _f_ _[∗]_ _≤_ ∆(1 − _ρ)[KI]_ + [4][C]µ [2] [(3(Φ + 1))][−][2][ (1]e(1[ −] _[ρ]ρ[)][KI])[K]_

_−_

(3(Φ + 1))−2

(1 _ρ)[KI]_ ∆+ [4][C] [2]
_≤_ _−_ _µ_ _e(1_ _ρ)[K]_
  _−_

_≤_ 2 12κ _e(3(Φ + 1))1/γ −_ 1 2/γ _−γ_ ∆+ [4][C]µ [2] _e(3(Φ + 1))(1_ _ρ)[⌈]12[−]κ[2]_
 _⌈_ _⌉_   _−_ _⌉_

1
= _._
_O_ _T_ _[γ]_
 


_T_ _[−][γ]_


-----

A.3 PROOF FOR LEMMA 1

We now prove the lemma that bounds the evolution of SGD over a length-m analysis phase. This
lemma is the key to our convergence analyses in both the diminishing and constant step size regimes.
Before we proceed, we first state and prove the following bound on the gradient error when scaled by
a nonincreasing sequence.
**Lemma 2. Suppose Assumption 2 holds. If {ρt} is a deterministic, nonincreasing, and nonnegative**
_sequence, then_


_ρ[2]τ_ _C_ [2] + Φ[2] _f_ (wτ )
_≤_ _[·][ m][2][−][γ][ ·]_ _∥∇_ _∥[2][]_



_τ_ +m−1

_ρt (∇f_ (wτ ; xt) −∇f (wτ ))
_t=τ_

X


_Proof. Let βt = ρt_ _ρt+1 for t_ _τ, τ + 1, . . ., τ + m_ 2, and let βτ +m 1 = ρt+m 1. Observe
that these are all nonnegative, and − _∈{_ _−_ _}_ _−_ _−_

2

_τ_ +m−1

_ρt (∇f_ (wτ ; xt) −∇f (wτ ))
_t=τ_

X


_τ_ +m−1

_t=τ_

X

_τ_ +m−1

_k=τ_

X


_τ_ +m−1

_βk (∇f_ (wτ ; xt) −∇f (wτ ))
_k=t_

X

2

_k_

_βk (∇f_ (wτ ; xt) −∇f (wτ ))
_t=τ_

X


_τ_ +m−1 _βk_ _k_

= ρ[2]τ _ρτ_ (∇f (wτ ; xt) −∇f (wτ ))

_k=τ_ _t=τ_

X X

Applying Jensen’s inequality using _k=τ_ _βk = ρτ_,

2

_τ_ +m−1

_ρt (∇f_ (wτ ; xt) −∇[P][τ] [+]f[m](w[−][1]τ ))
_t=τ_

X


_τ_ +m−1

_k=τ_

X

_τ_ +m−1

_k=τ_

X


_βk_
_ρτ_


_ρ[2]τ_
_≤_

= ρ[2]τ


( _f_ (wτ ; xt) _f_ (wτ ))
_∇_ _−∇_
_t=τ_

X


_βk_
(k _τ + 1)[2]_
_ρτ_ _−_


( _f_ (wτ ; xt) _f_ (wτ ))
_∇_ _−∇_
_t=τ_

X


_τ_ _ρτ_ _−_ _k_ _τ + 1_

_k=τ_ _−_

X

Applying Assumption 2 on the squared norm,


2

_τ_ +m−1

_ρt (∇f_ (wτ ; xt) −∇f (wτ ))
_t=τ_

X

_τ_ +m−1 _βk_ 1

_≤_ _ρ[2]τ_ _ρτ_ (k − _τ + 1)[2]_ _·_ (k _τ + 1)[γ]_ _C_ [2] + Φ[2]∥∇f (wτ )∥[2][]

_kX=τ_ _−_ 

_τ_ +m−1 _βk_

= ρ[2]τ _ρτ_ _· (k −_ _τ + 1)[2][−][γ]_ _·_ _C_ [2] + Φ[2]∥∇f (wτ )∥[2][]

_k=τ_

X 

_τ_ +m−1 _βk_

_≤_ _ρ[2]τ_ _ρτ_ _· m[2][−][γ]_ _·_ _C_ [2] + Φ[2]∥∇f (wτ )∥[2][]

_k=τ_

X 

= ρ[2]τ _C_ [2] + Φ[2] _f_ (wτ ) _,_

_[·][ m][2][−][γ][ ·]_ _∥∇_ _∥[2][]_


which is what we wanted to show.


-----

We now recall the statement for Lemma 1. In the main paper, we state the lemma with constant step
size. In the following proof, we prove it holds for non-decreasing step size, where constant step size
will hold naturally as a special case. For this, we define η := _t=τ_ _αt._

**Lemma 1. Suppose our setup satisfies Assumptions 1 and 2 and that we use a constant step size**
_α For all timesteps τ ≥_ 0, let m > 0 be some integer such that[P][τ] 3[+]α[m]τ[−]m[1][1][−][γ][/][2]Φ ≤ _η ≤_ 61L _[. Then the]_

_objective at timestep τ + m is bounded by_

_f_ (wτ +m) _f (wτ_ ) _τ_ _[m][2][−][γ][C]_ [2][.]
_≤_ _−_ _[η]4_ _[∥∇][f]_ [(][w][τ] [)][∥][2][ + 2]η [α][2]


_Proof. Let η and g denote_


_τ_ +m−1

_αt_ and _g := [1]_

_η_

_t=τ_

X


_τ_ +m−1

_αt_ _f_ (wt; xt).
_∇_
_t=τ_

X


_η :=_


This means that wτ +m = wτ − _ηg. From Assumption 1 (L-Smoothness),_

_f_ (wτ +m) = f (wτ _ηg)_
_−_

_f (wτ_ ) _η_ _f_ (wτ ), g + _[η][2][L]_
_≤_ _−_ _⟨∇_ _⟩_ 2 _[∥][g][∥][2]_

= f (wτ )
_−_ _[η]2_ _[∥∇][f]_ [(][w][τ] [)][∥][2][ −] _[η]2_ _[∥][g][∥][2][ +][ η]2_ _[∥][g][ −∇][f]_ [(][w][τ] [)][∥][2][ +][ η][2]2[L]

_[∥][g][∥][2]_

_f (wτ_ )
_≤_ _−_ _[η]2_ _[∥∇][f]_ [(][w][τ] [)][∥][2][ +][ η]2 _[∥][g][ −∇][f]_ [(][w][τ] [)][∥][2][,]

where the last inequality follows because we assumed ηL ≤ [1]/6 (< 1). Next, observe that
_η_
2 _[∥][g][ −∇][f]_ [(][w][τ] [)][∥][2]

= [1]

2η

_[∥][ηg][ −]_ _[η][∇][f]_ [(][w][τ] [)][∥][2] 2

_τ_ +m−1

= [1] _αt (_ _f_ (wt; xt) _f_ (wτ ))

2η _∇_ _−∇_

_t=τ_

X


_τ_ +m−1

_αt (∇f_ (wτ ; xt) −∇f (wτ ))
_t=τ_

X


_τ_ +m−1

_αt (∇f_ (wt; xt) −∇f (wτ ; xt))
_t=τ_

X


_≤_ _η[1]_


+ [1]

_η_

= η


To bound the second term,

1 _τ_ +m−1

_αt (_ _f_ (wt; xt) _f_ (wτ ; xt))

_η_ _∇_ _−∇_

_t=τ_

X

Combining the above,


_τ_ +m−1

_t=τ_

X


_αt_

_η_ [(][∇][f] [(][w][t][;][ x][t][)][ −∇][f] [(][w][τ] [;][ x][t][))]


_τ_ +m−1 _αt_

_η_
_≤_ _η_

_t=τ_

_[∥][(][∇][f]_ [(][w][t][;][ x][t][)][ −∇][f] [(][w][τ] [;][ x][t][))][∥][2]

X

_τ_ +m−1

_αtL[2]_ _wt_ _wτ_ _._

_≤_ _t=τ_ _∥_ _−_ _∥[2]_

X


_τ_ +m−1

_αt (∇f_ (wτ ; xt) −∇f (wτ ))
_t=τ_

X


_τ_ +m−1

_αtL[2]_ _wt_ _wτ_ _. (13)_
_t=τ_ _∥_ _−_ _∥[2]_

X


_η_
2 _[∥][g][ −∇][f]_ [(][w][τ] [)][∥][2][ ≤] _η[1]_


Applying Lemma 2 on the first term in Eq. (13) gives


_τ_ _[m][2][−][γ][ ]C_ [2] + Φ[2] _f_ (wτ )

_≤_ _η [1]_ _[α][2]_ _∥∇_ _∥[2][]_


_τ_ +m−1

_αt (∇f_ (wτ ; xt) −∇f (wτ ))
_t=τ_

X


-----

For the second term in Eq. (13),

_τ_ +m−1

_αtL[2]_ _wt_ _wτ_ =
_t=τ_ _∥_ _−_ _∥[2]_

X


_τ_ +m−1

_αtL[2]_
_t=τ_

X


2

_t−1_

_αu_ _f_ (wu; xu)
_∇_
_u=τ_

X

_t−1_

_αu (∇f_ (wu; xu) −∇f (wτ ; xu))
_u=τ_

X


_τ_ +m−1

_αtL[2]_
_t=τ_

X


_≤_ 3

_≤_ 3


_τ_ +m−1

_αtL[2]_
_t=τ_

X


_t−1_

_αu (∇f_ (wτ ; xu) −∇f (wτ ))
_u=τ_

X


+ 3

+ 3


2

_τ_ +m−1 _t−1_

+ 3 _αtL[2]_ _αu_ _f_ (wτ )

_∇_

_t=τ_ _u=τ_

X X

_τ_ +m−1 _t−1_ _t−1_

_αtL[4]_ _αu_ _αu_ _wu_ _wτ_
_t=τ_ _u=τ_ ! _u=τ_ _∥_ _−_ _∥[2]_

X X X


_τ_ +m−1

_αtL[2]_ _ατ[2][m][2][−][γ][ ]C_ [2] + Φ[2] _f_ (wτ )

_·_ _∥∇_ _∥[2][]_
_t=τ_

X 2

_τ_ +m−1 _t−1_

_αtL[2]_ _αu_ _f_ (wτ ) _,_
_t=τ_ _u=τ_ ! _∥∇_ _∥[2]_

X X


+ 3

+ 3


where the second term in the last inequality follows from Lemma 2. Continuing,


_τ_ +m−1

_αtL[2]_ _wt_ _wτ_ 3
_t=τ_ _∥_ _−_ _∥[2]_ _≤_

X


_τ_ +m−1

_αtL[4]η_
_t=τ_

X


_t−1_

_αu_ _wu_ _wτ_
_u=τ_ _∥_ _−_ _∥[2]_

X


_τ_ +m−1

+ 3 _αtL[2]ατ[2][m][2][−][γ][ ]C_ [2] + Φ[2] _f_ (wτ )

_∥∇_ _∥[2][]_
_t=τ_

X

+ 3η[3]L[2] _f_ (wτ )
_∥∇_ _∥[2]_

_τ_ +m−1
= 3η[2]L[2] _αtL[2]_ _wt_ _wτ_

_t=τ_ _∥_ _−_ _∥[2]_

X

+ 3L[2]ηατ[2][m][2][−][γ][ ]C [2] + Φ[2] _f_ (wτ )
_∥∇_ _∥[2][]_

+ 3η[3]L[2] _f_ (wτ ) _._
_∥∇_ _∥[2]_


This implies that

_τ_ +m−1

(1 − 3η[2]L[2]) _t=τ_ _αtL[2]∥wt −_ _wτ_ _∥[2]_ _≤_ 3L[2]C [2]ηατ[2][m][2][−][γ]

X

+ 3η[3]L[2] + 3L[2]ηατ[2][m][2][−][γ][Φ][2][] _f_ (wτ ) _._
_∥∇_ _∥[2]_
 

And because by our assumption, ατ[2][m][2][−][γ][Φ][2][ ≤] _[η]9[2]_ [, it follows that]


_τ_ +m−1

_αtL[2]_ _wt_ _wτ_ 3L[2]C [2]ηατ[2][m][2][−][γ][ + 10]
_t=τ_ _∥_ _−_ _∥[2]_ _≤_ 3 _[η][3][L][2][∥∇][f]_ [(][w][τ] [)][∥][2][.]

X


(1 − 3η[2]L[2])


Since we also assumed η ≤ 61L [, it follows that][ 3][η][2][L][2][ ≤] 121 [, and so]

_τ_ +m−1

1 _αtL[2]_ _wt_ _wτ_ 3L[2]C [2]ηατ[2][m][2][−][γ][ + 5]
 _−_ 12[1]  _t=τ_ _∥_ _−_ _∥[2]_ _≤_ 54 _[η][∥∇][f]_ [(][w][τ] [)][∥][2][,]

X


-----

which gives

_τ_ +m−1

_αtL[2]_ _wt_ _wτ_ _τ_ _[m][2][−][γ][ + 12]_
_t=τ_ _∥_ _−_ _∥[2]_ _≤_ 11[12] 11 54 _[η][∥∇][f]_ [(][w][τ] [)][∥][2]

X _[·][ 3][L][2][C]_ [2][ηα][2] _[·][ 5]_

= [36] _τ_ _[m][2][−][γ][ + 10]_

11 _[L][2][C]_ [2][ηα][2] 99 _[η][∥∇][f]_ [(][w][τ] [)][∥][2][.]


So, putting this all together,

_f_ (wτ +m) _f (wτ_ ) _τ_ _[m][2][−][γ][ ]C_ [2] + Φ[2] _f_ (wτ )
_≤_ _−_ _[η]2_ _[∥∇][f]_ [(][w][τ] [)][∥][2][ + 1]η [α][2] _∥∇_ _∥[2][]_

+ [36] _τ_ _[m][2][−][γ][ + 10]_

11 _[L][2][C]_ [2][ηα][2] 99 _[η][∥∇][f]_ [(][w][τ] [)][∥][2]


= f (wτ ) _τ_ _[m][2][−][γ][C]_ [2]
_−_ _[η]2_ _[∥∇][f]_ [(][w][τ] [)][∥][2][ + 1]η [α][2]

+ η [1] _[α]τ[2][m][2][−][γ][Φ][2][∥∇][f]_ [(][w][τ] [)][∥][2]


+ [36] _τ_ _[m][2][−][γ][ + 10]_

11 _[L][2][C]_ [2][ηα][2] 99 _[η][∥∇][f]_ [(][w][τ] [)][∥][2]

_f (wτ_ ) _τ_ _[m][2][−][γ][C]_ [2]
_≤_ _−_ _[η]2_ _[∥∇][f]_ [(][w][τ] [)][∥][2][ + 1]η [α][2]

+ [1]

9 _[η][∥∇][f]_ [(][w][τ] [)][∥][2]

+ 4L[2]C [2]ηατ[2][m][2][−][γ][ + 10]

99 _[η][∥∇][f]_ [(][w][τ] [)][∥][2]

= f (wτ ) _τ_ _[m][2][−][γ][C]_ [2]
_−_ _[η]2_ _[∥∇][f]_ [(][w][τ] [)][∥][2][ + 1]η [α][2]

+ 4L[2]C [2]ηατ[2][m][2][−][γ][ + 21]

99 _[η][∥∇][f]_ [(][w][τ] [)][∥][2]

_f (wτ_ ) _τ_ _[m][2][−][γ][C]_ [2]
_≤_ _−_ _[η]2_ _[∥∇][f]_ [(][w][τ] [)][∥][2][ + 1]η [α][2]


+ [1] _τ_ _[m][2][−][γ][ + 1]_

4η [C] [2][α][2] 4 _[η][∥∇][f]_ [(][w][τ] [)][∥][2]


_f (wτ_ ) _τ_ _[m][2][−][γ][C]_ [2][.]
_≤_ _−_ _[η]4_ _[∥∇][f]_ [(][w][τ] [)][∥][2][ + 2]η [α][2]

This is what we wanted to show.


-----

A.4 CONVERGENCE ANALYSIS: CONSTANT STEP SIZE

A.4.1 CONSTANT STEP SIZE: NON-CONVEX CASE

**Theorem 1 (Non-convex case). Suppose that our setup satisfies Assumptions 1 and 2, and let ϵ > 0**
_be any target error. Using SGD (1) with a constant step size α =_ 61L (4C/ϵ + 3Φ)2/γ[][−][1], the

_number of steps T needed to achieve mint=0,···,T −1∥∇f_ (wt)∥2 ≤ _ϵ2 is at most_

_T =_ 48ϵL[2]∆ 4ϵC [+ 3Φ] 2/γ[m] = [˜] _Cϵ2[2+2]/γ_ _L/γ∆_ [+][ Φ]2/γϵ[2]L∆ + _[C]ϵ[2]2/γ/γ_ [+ Φ]2/γ[] _._

_·_ _O_

  l   

_Proof. To satisfy the requirements of Lemma 1 in the constant step size, constant m case, we need_


3αm[1][−][γ][/][2]Φ _mα_
_≤_ _≤_ 6[1]L _[.]_

This breaks apart into first a constraint on only m:


3Φ ≤ _mγ/2,_ (14)


and then a constraint on α in terms of m:


_α ≤_


6Lm _[.]_


In the nonconvex setting, we invoke Lemma 1 followed by summing up over K phases and telescoping,


_K−1_

_f_ (wmk) _f_ (w0) _f_ + [2][K]
_∥∇_ _∥[2]_ _≤_ _−_ _[∗]_ _αm_ _[α][2][m][2][−][γ][C]_ [2]
_k=0_

X

= f (w0) _f_ + 2Kαm[1][−][γ]C [2],
_−_ _[∗]_


_αm_


which gives a rate of


1 _K−1_

_f_ (wmk) + 8m[−][γ]C [2].

_K_ _∥∇_ _∥[2]_ _≤_ [4(][f] [(]αmK[w][0][)][ −] _[f][ ∗][)]_

_k=0_

X

If we want to minimize the right side, observe that we will always want to set α as large as possible,
i.e. set α = [1]/6Lm. This gives


1 _K−1_

_f_ (wmk)

_K_ _∥∇_ _∥[2]_ _≤_ [24][L][(][f] [(][w]K[0][)][ −] _[f][ ∗][)]_

_k=0_

X

Now, for this to all be less than ϵ[2], it suffices for


+ 8m[−][γ]C [2].


24L(f (w0) _f_ )
_−_ _[∗]_


_≤_ _[ϵ]2[2]_


and 8m[−][γ]C [2]
_≤_ _[ϵ]2[2]_ _[.]_


The former occurs when

while the latter happens when


_K_
_≥_ [48][L][(][f] [(][w]ϵ[2][0][)][ −] _[f][ ∗][)]_


4C


_≤_ _mγ/2._ (15)


If we let ∆= f (w0) _f_, using Eqs. (14) and (15) and taking the minimum m and K required, we
_−_ _[∗]_
can bound the number of iterations as

2/γ[']

48L∆ 4C
_T = mK_ _._
_≥_ _ϵ[2]_ _·_ _ϵ_ [+ 3Φ]
  & 


So,

_T = O_ _Cϵ2[2+]/γL[2][/][γ]∆_ + _[L][∆Φ]ϵ[2]_ 2/γ + _[C]ϵ22//γγ_ [+ Φ]2/γ


where we suppose that γ acts as a constant.


-----

A.4.2 CONSTANT STEP SIZE: STRONGLY-CONVEX CASE

**Theorem 2. Suppose that f satisfies the µ-PL condition and our setup satisfies Assumptions 1 and 2.**
_Let ϵ > 0 be any target error, and κ =_ _[L]/µ be the condition number of the problem. Using SGD_
_(1) with a constant step size α =_ 61L (8C [2]/(µϵ[2]) + 9Φ[2])1/γ _−1, the number of steps T needed to_

_guarantee f_ (wT ) − _f_ _[∗]_ _≤_ _ϵ[2]_ _is at most_

[]

_T =_ 12κ log 2∆ϵ[2] _·_ 8µϵC[2][2][ + 9Φ][2][][1][/][γ][] = O[˜] _µC[1][/γ][2][/γ]ϵ[2]κ[/γ][ +][ κ][Φ][2][/γ][ +][ κ]_ _._
      

_Proof. The strongly convex setting has the same constraints on m and α as the non-convex setting._
Recall that µ-strong convexity or µ-PL of f implies that for all x, ∥∇f (x)∥[2] _≥_ 2µ(f (x) − _f_ _[∗])._
Applying this to the result of Lemma 1 gives


_f_ (wτ +m) _f (wτ_ )
_≤_ _−_ _[αmµ]2_

which is equivalent to

_f_ (wτ +m) _f_ 1
_−_ _[∗]_ _≤_ _−_ _[αmµ]2_



(f (wτ ) _f_ ) + 2αm[1][−][γ]C [2],
_−_ _[∗]_

(f (wτ ) _f_ ) + 2αm[1][−][γ]C [2].
_−_ _[∗]_



Applying this recursively over K intervals of length m starting from τ = 0,


_K_ 1
_K_ _−_
(f (w0) _f_ ) + 2αm[1][−][γ]C [2]
_−_ _[∗]_

_k=0_

 X


_K_ 1
_−_ _k_
(w0) _f_ ) + 2αm[1][−][γ]C [2] 1
_−_ _[∗]_ _−_ _[αmµ]2_

_k=0_

X  

_∞_ _k_
(f (w0) _f_ ) + 2αm[1][−][γ]C [2] 1
_−_ _[∗]_ _−_ _[αmµ]2_

_k=0_

X  

_−1_

(f (w0) _f_ ) + 2αm[1][−][γ]C [2][ ]1 1
_−_ _[∗]_ _−_ _−_ _[αmµ]2_
 

2
(f (w0) _f_ ) + 2αm[1][−][γ]C [2]
_−_ _[∗]_ _·_ _αmµ_


_f_ (wmK) _f_ 1
_−_ _[∗]_ _≤_ _−_ _[αmµ]2_



1
_−_ _[αmµ]2_


exp
_≤_ _−_ _[αmµK]2_


= exp
_−_ _[αmµK]2_


= exp
_−_ _[αmµK]2_


= exp
_−_ _[αmµK]2_



(f (w0) _f_ ) + [4][C] [2]
_−_ _[∗]_ _µ [m][−][γ][.]_


As in the non-convex case, it best advantages us to set α as large as possible. Setting α = [1]/6Lm gives


(f (w0) _f_ ) + [4][C] [2]
_−_ _[∗]_ _µ [m][−][γ][.]_


_f_ (wmK) _f_ exp
_−_ _[∗]_ _≤_ _−_ 12[µK]L



For this to be less than ϵ[2], it suffices for each of these two terms to be less than _[ϵ][2]/2. In the latter case,_
we would need
8C [2]

(16)
_µϵ[2][ ≤]_ _[m][γ][.]_

The former case holds when

2∆ 2∆

_K_ log = 12κ log _,_
_≥_ [12]µ[L] _ϵ[2]_ _ϵ[2]_

   

where κ = _[L]/µ. Applying a ceiling, using the minimum requirement on K and m from Eq. (16)_
above and Eq. (14) required in Lemma 1, we can lower bound the total number of iterations as

2∆ 8C 2 1/γ[']
_T = mK_ 12κ log _._
_≥_ _ϵ[2]_ _·_ _µϵ[2][ + 9Φ][2]_
   & 

Ignoring logarithmic terms gives


2 1/γ
_C_ + κΦ2/γ

_µϵ[2]_

 


_T = O[˜]_


-----

A.5 JUSTIFICATIONS FOR ASSUMPTION 2 UNDER VARIOUS EXAMPLE ORDERINGS

A.5.1 PROOF FOR PROPOSITION 1 (ARBITRARY PERMUTATION)

**Proposition 1. Let Assumptions 1 and 3 hold, and suppose that we are using a permutation-based**
_method of sampling, that is, any method such that xkn, xkn+1, . . ., xkn+n_ 1 is a permutation of
_−_
_x[(0)], x[(1)], . . ., x[(][n][−][1)]_ _for all epochs k ≥_ 0. Any such method satisfies Assumption 2 with γ = 2,
_C_ [2] = n[2]A[2] _and Φ[2]_ = n[2]B[2].

_Proof. This result follows trivially from Assumption 3. Consider an arbitrary sum of gradient errors_
going from τ to τ + m − 1:

_τ_ +m−1

_f_ (wτ ; xσ(t)) _f_ (wτ ),
_∇_ _−∇_
_t=τ_

X

where σ(t) is the index into the training set given by the permutation σ used at time step t. Since the
interval {τ, τ + 1, . . ., τ + m − 1} is arbitrary, this m can potentially be greater than n, the epoch
size. We can split this interval up as follows. Let τ1 be the first epoch boundary in the interval, such
that allepoch boundary in the interval. Let t going from τ to τ1 − 1 are within the same epoch as τ2 be the last epoch boundary in the interval, such that all wτ, or else τ1 = τ + m if there is no t going
from τ2 to τ + _m_ _−_ 1 are within a later epoch than wτ (it may be the case that τ1 = τ2). Then the sum

_τ2−1_

_f_ (wτ ; xσ(t)) _f_ (τ )
_∇_ _−∇_
_t=τ1_

X

must be zero, since this interval goes over full epochs. This leaves us with

_τ_ +m−1

_f_ (wτ ; xσ(t)) _f_ (wτ )
_∇_ _−∇_
_t=τ_

X


_τ1−1_ _τ_ +m−1

_f_ (wτ ; xσ(t)) _f_ (wτ ) _f_ (wτ ; xσ(t)) _f_ (wτ )

_≤_ _∇_ _−∇_ [+] _∇_ _−∇_

_t=τ_ _t=τ2_

X X

_n_ _A[2]_ + B[2] _f_ (wτ ) _._
_≤_ _∥∇_ _∥[2][][1][/][2]_


where we have used the fact that the intervals _τ, . . ., τ1_ and _τ2, τ + m_ 1 are of length at
_{_ _}_ _{_ _−_ _}_
most ⌊n/2⌋ (if such interval is of length greater than ⌊n/2⌋, then bounding its norm is equivalent to
bounding the sums of the remaining terms, for which there are at most ⌊n/2⌋ of them). Therefore


2

1 _τ_ +m−1 1

_m_ _∇f_ (wτ ; xσ(t)) −∇f (wτ ) _≤_ _m[2][ n][2][]A[2]_ + B[2]∥∇f (wτ )∥[2][],

_t=τ_

X

and so Assumption 2 is satisfied with γ = 2, C [2] = n[2]A[2], and Φ[2] = n[2]B[2].

A.5.2 PROOF FOR PROPOSITION 2 (SHUFFLE ONCE)

**Proposition 2. Suppose that we are using the shuffle once variant of SGD to learn over a region**
_B ∈_ R[d] _of radius at most R, such that the iterates wt are guaranteed to remain within this region._
_Assume that for all w ∈B and all examples x in the training set of size n, Assumption 1 (L-_
_Smoothness) and Assumption 3 hold. Then with probability at least 1 −_ _p, Assumption 2 holds with_
_γ = 2, C_ [2] = O[˜](dA[2](n + B[2])), and Φ[2] = O[˜](ndB[2]).[3]

_Proof. We begin by invoking Lemma 6 (Permuted vector Hoeffding inequality) with_


_f_ (w; xi,j) _f_ (w)
_Xi,j =_ _∇_ _−∇_

_A[2]_ + B[2]∥∇f (w)∥[2][][1][/][2]

3The probability p is only present in log terms in these expressions, so it does not appear in the ˜O.


-----

where we use xi,j to denote the j-th example after time step i, for all i ∈{1, . . . m}, j ∈{1, . . ., n},
with n ≥ _m > 0. Note that here we are using a fixed w ∈B that does not depend on the permutation_
_σ used in shuffle once. Clearly,_ _j=1_ _[X][i,j][ = 0][ for all][ i][ due to the periodicity in shuffle once, and]_
_Xi,j_ 1 by our assumption. Therefore the requirements of Lemma 6 are satisfied, which gives
_∥_ _∥≤_
the following high probability bound for some[P][n] _ϵ > 0 on any sequence drawn without replacement:_

_m_ 2

**P**  _∇f_ (w; xσ(t)) −∇f (w) _≥_ _ϵ[2][ ]A[2]_ + B[2]∥∇f (w)∥[2][] _≤_ 2e[2] exp _−_ 32[ϵ][2]m _._

_t=1_  

X

 

Using the periodicity property of shuffle once, i.e. if σ is the drawn permutation at the beginning of
training then σ(t + n) = σ(t), which allows us to arbitrarily shift the starting time step in the above:

2

_τ_ +m−1

**P**  _∇f_ (w; xσ(t)) −∇f (w) _≥_ _ϵ[2][ ]A[2]_ + B[2]∥∇f (w)∥[2][] _≤_ 2e[2] exp _−_ 32[ϵ][2]m _._

_t=τ_  

X

 

Consider an arbitrary interval of length m out of the permutation. As all of the _[n][(][n][−][1)]/2 intervals are_
equivalent to an interval of size at most ⌊[n]/2⌋, by the sum to 0 property, we only need to consider
intervals with m ≤ _[n]/2. This gives us_


2

_τ_ +m−1

**P**  _∇f_ (w; xσ(t)) −∇f (w) _≥_ _ϵ[2][ ]A[2]_ + B[2]∥∇f (w)∥[2][] _≤_ 2e[2] exp _−_ 16[ϵ][2]n

_t=τ_ 

X

 

By a union bound across all intervals we have


_ϵ[2][ ]A[2]_ + B[2] _f_ (w)
_≥_ _∥∇_ _∥[2][]_



_e[2]n[2]_ exp
_≤_ _−_ 16[ϵ][2]n



_τ_ +m−1

_f_ (w; xσ(t)) _f_ (w)
_∇_ _−∇_
_t=τ_

X


_τ, m_ Z,

∃ _∈_


Unfortunately, we cannot take a union bound overthe set may contain iterates visited by shuffle once that depend on R[n] or B (or even just over σ, and thus Lemma {w0, . . ., w 6 may not hold.T }), since
Instead, suppose that we cover the whole region B with balls of radius δ. We will be able to do this
with (1 + [2][R]/δ)[d] balls by Lemma 7 (ϵ-net lemma). So, if the centers of the balls form a set W, then


_τ_ +m−1

_f_ ( ˆw; xσ(t)) _f_ ( ˆw)
_∇_ _−∇_
_t=τ_

X


2

_ϵ[2][ ]A[2]_ + B[2] _f_ ( ˆw)
_≥_ _∥∇_ _∥[2][]_

_d_ 

_e[2]n[2]_ 1 + [2][R] exp
_≤_ _δ_ _−_ 16[ϵ][2]n
  


_τ, m_ Z, ˆw _,_

∃ _∈_ _∈W_


Note that the centers we use to cover the region is completely independent of the running algorithm,
therefore the union bound over Lemma 6 can be applied. Next, consider some wτ that is not
necessarily the center of a ball. The function

_τ_ +m−1

_f_ (wτ ; xσ(t)) _f_ (wτ )
_∇_ _−∇_
_t=τ_

X

is Ln-Lipschitz continuous, because each of the components is L-Lipschitz, and we can sum up only
at most _[n]/2 of them. Let ˆw_ be such that _wτ_ _wˆ_ _δ. Adding and subtracting_ _f_ ( ˆw; σ(i))
_∈W_ _∥_ _−_ _∥≤_ _∇_
and ∇f ( ˆw), applying triangle inequality followed by Lipschitz continuity, we have

_τ_ +m−1 _τ_ +m−1

_f_ (wτ ; xσ(t)) _f_ (wτ ) _f_ ( ˆw; xσ(t)) _f_ ( ˆw)
_∇_ _−∇_ _∇_ _−∇_
_t=τ_ _[≤]_ [2][Lnδ][ +] _t=τ_

X X

_≤_ 2Lnδ + ϵ _A[2]_ + B[2]∥∇f ( ˆw)∥[2]. (w.h.p.)
q


-----

Adding and subtracting _f_ (wτ ) from _f_ ( ˆw) and bound using δ and Lipschitz continuity again,
_∇_ _∇_


_τ_ +m−1

_f_ (wτ ; xσ(t)) _f_ (wτ )
_∇_ _−∇_
_t=τ_ _[≤]_ [2][Lnδ][ +][ ϵ]

X

_≤_ 2Lnδ + ϵ

and consequently


_A[2]_ + B[2] _f_ ( ˆw _f_ (wτ ) + _f_ (wτ ))
_∥∇_ _−∇_ _∇_ _∥[2]_

_A[2]_ + 2B[2]L[2]δ[2] + 2B[2] _f_ (wτ ) _,_
_∥∇_ _∥[2]_


2

_τ_ +m−1

_f_ (wτ ; xσ(t)) _f_ (wτ ) 4L[2]n[2]δ[2] + 2ϵ[2][ ]A[2] + 2B[2]L[2]δ[2] + 2B[2] _f_ (wτ )
_∇_ _−∇_ _≤_ _∥∇_ _∥[2][]_
_t=τ_

X

Note that this inequality will fail to hold with probability at most


_d_

_e[2]n[2]_ 1 + [2][R] exp

_δ_ _−_ 16[ϵ][2]n

  


If we want this to fail with probability less than some p, it suffices to set


_e2n2_
_ϵ[2]_ = 16n log

_p_

 





+ d log 1 + [2][R]



This gives

2

_τ_ +m−1

_f_ (wτ ; xσ(t)) _f_ (wτ ) 4L[2]n[2]δ[2]
_∇_ _−∇_ _≤_
_t=τ_

X _e2n2_

+ 32n log + d log 1 + [2][R] _A[2]_ + 2B[2]L[2]δ[2] + 2B[2] _f_ (wτ )
_·_   _p_   _δ_  _·_  _∥∇_ _∥[2][]_

If we set δ such that L[2]nδ[2] = A[2], then we can bound this with


2

_τ_ +m−1

_f_ (wτ ; xσ(t)) _f_ (wτ ) 4nA[2]
_∇_ _−∇_ _≤_
_t=τ_

X _e2n2_

+ 32n log + d log 1 + [2][RL][√][n] _A[2]_ + [2][A][2][B][2] + 2B[2] _f_ (wτ ) _,_
_·_ _p_ _A_ _·_ _n_ _∥∇_ _∥[2]_
      

which gives

2

1 _τ_ +m−1

_m_ _∇f_ (wτ ; xσ(t)) −∇f (wτ )

_t=τ_

X _e3n2_

log + d log 1 + [2][RL][√][n] _A[2]_ + [2][A][2][B][2] + 2B[2] _f_ (wτ ) _._

_≤_ [32]m[n][2][ ·] _p_ _A_ _·_ _n_ _∥∇_ _∥[2]_

      

Hiding logarithmic terms involving n, p, R, L, A, the shuffle once setup satisfies the requirement of
Assumption 2 that with probability at least 1 − _p,_

2

1 _τ_ +m−1 _nd_

_m_ _∇f_ (wτ ; xσ(t)) −∇f (wτ ) = O[˜] _m[2]_ _A[2]_ + [2][A]n[2][B][2] + 2B[2]∥∇f (wτ )∥[2]

_t=τ_   

X

1
= [˜] _ndA[2]_ + 2dA[2]B[2] + 2ndB[2] _f_ (wτ )
_O_  _m[2]_  _∥∇_ _∥[2][]_

1
= [˜] _dA[2](n + 2B[2]) + 2ndB[2]_ _f_ (wτ ) _,_
_O_  _m[2]_  _∥∇_ _∥[2][]_

and so the parameters are γ = 2, C [2] = O[˜](dA[2](n + B[2])), and Φ[2] = O[˜](ndB[2]).


-----

A.5.3 PROOF FOR PROPOSITION 3 (RANDOM RESHUFFLING)

We first introduce a lemma that bounds the distance between iterates obtained from SGD with random
permutation and that obtained from deterministic gradient descent, over one epoch.
**Lemma 3. Consider a single epoch of SGD with constant step size α > 0, where the examples come**
_from a random permutation over the training set. Let Assumption 1 (L-Smoothness) and the bounded_
_gradient error Assumption 3 be satisfied. Without loss of generality, assume that the epoch starts at_
_time t = 0 at w0. Let the sequence ut be defined by u0 = w0 and_


_ut+1 = ut −_ _α∇f_ (ut),


_while_


_wt+1 = wt −_ _α∇f_ (wt; xt)

_for some xt chosen from the permutation. With probability at least (1 −_ _δ) it will hold that for all_
_T ∈{1, . . ., n}, if we set_


_α ≤_ min ( 64e(e[2] + 1)BnL log 2eδ2n _,_ 2nL ) _,_

  

2e2n
_wT_ _uT_ 128α[2]n(e[2] + 1)[2] log _A[2]_ + B[2]e[2] _f_ (wT )
_∥_ _−_ _∥[2]_ _≤_ _·_ _δ_ _∥∇_ _∥[2][]_
  


_then_


_Proof. Given our setup, observe that we can write the difference between these sequences as_

_wt+1 −_ _ut+1 = wt −_ _ut −_ _α (∇f_ (wt; xt) −∇f (ut))
= wt _ut_ _α (_ _f_ (wt; xt) _f_ (ut; xt)) _α (_ _f_ (ut; xt) _f_ (ut)),
_−_ _−_ _∇_ _−∇_ _−_ _∇_ _−∇_

such that summing this up and using w0 = u0 gives


_T −1_

_wT −_ _uT = −α_ _t=0_ (∇f (wt; xt) −∇f (ut; xt)) − _α_

X


_T −1_

( _f_ (ut; xt) _f_ (ut)),
_∇_ _−∇_
_t=0_

X

_T −1_

( _f_ (ut; xt) _f_ (ut))
_∇_ _−∇_
_t=0_

X


and so

_wT_ _uT_ _α_
_∥_ _−_ _∥≤_


_T −1_

_f_ (wt; xt) _f_ (ut; xt) + α
_∥∇_ _−∇_ _∥_
_t=0_

X


_T −1_

_wt_ _ut_ + α
_t=0_ _∥_ _−_ _∥_

X


_T −1_

( _f_ (ut; xt) _f_ (ut))
_∇_ _−∇_
_t=0_

X


_≤_ _αL_


Recall the update rule of ut, with Assumption 1 (L-Smoothness), we obtain

_f_ (ut+1) _f_ (ut) _αL_ _f_ (ut) _._
_∥∇_ _−∇_ _∥≤_ _∥∇_ _∥_

By the reverse triangle inequality, this implies

_f_ (ut) (1 _αL)[−][1]_ _f_ (ut+1)
_∥∇_ _∥≤_ _−_ _∥∇_ _∥_
_f_ (ut+1) (1 + αL) _f_ (ut) _,_
_∥∇_ _∥≤_ _∥∇_ _∥_

which further implies for any T ∈{1, · · ·, n} and t ∈{0, · · ·, T − 1},

_∥∇f_ (ut)∥≤(1 − _αL)[−][n]∥∇fn(uT )∥_

_αL_
= 1 + _f_ (uT )

1 _αL_ _∥∇_ _∥_

 _−_ 

_αLn_
exp _f_ (uT )
_≤_ 1 _αL_ _∥∇_ _∥_
 _−_ 

_≤_ exp (2αLn) ∥∇f (uT )∥
_≤e∥∇f_ (uT )∥,


-----

while for any t ∈{T + 1, · · ·, n},
_∥∇f_ (ut)∥≤(1 + αL)[n]∥∇f (uT )∥
_≤e∥∇f_ (uT )∥,
where we apply the condition that α ≤ 1/(2nL). For a given T, consider the following vector set

_f_ (uj; xj) _f_ (uj)
_Xj =_ _∇A[2]_ + B[2]e −∇[2]∥∇f (uT )∥ _, ∃j ∈{0, · · ·, n},_

it can be easily verified that they sum to zero.

p

Now we apply Lemma 6 (Permuted vector Hoeffding inequality), for any γ ≥ 0,

_T −1_

**P** ( _f_ (ut; xt) _f_ (ut)) _A[2]_ + B[2]e[2] _f_ (uT ) 2e[2] exp

_t=0_ _∇_ _−∇_ _[≥]_ _[γ]_ _∥∇_ _∥[2]!_ _≤_ − 32[γ][2]T 

X p

2e[2] exp
_≤_ _−_ 32[γ][2]n
 


as T ≤ _n. Now, this holds for just one T_ . By a union bound,

_T −1_

**P** _T_ 1, . . ., n _,_ ( _f_ (ut; xt) _f_ (ut)) 2e[2]n exp

_∃_ _∈{_ _}_ _t=0_ _∇_ _−∇_ _[≥]_ _[γQ][T]_ ! _≤_ − 32[γ][2]n

X

where QT = _A[2]_ + B[2]e[2]∥∇f (uT )∥[2]. Now, if we set γ such that

2e2n

p

2e[2]n exp = δ _γ[2]_ = 32n log _,_
_−_ 32[γ][2]n _⇒_ _δ_
   

then we get that


_T −1_

**P** _T_ 1, . . ., n _,_ ( _f_ (ut; xt) _f_ (ut))

_∃_ _∈{_ _}_ _∇_ _−∇_

_t=0_ _[≥]_ _[γQ][T]_

X

In this case, we will have that with probability at least (1 − _δ),_

_T −1_

_∥wT −_ _uT ∥≤_ _αL_ _t=0_ _∥wt −_ _ut∥_ + αγQT .

X

If we let ρ0 = 0 and

_T −1_

_ρT = αL_ _ρt + αγQT,_

_t=0_

X

then ∥wT − _uT ∥≤_ _ρT . Here, if T > 0,_
_ρT +1 −_ _ρT = αLρT + αγ(QT +1 −_ _QT ),_
on the other hand, obviously ρ1 = αγQ1,


_≤_ _δ._


_T −1_

(1 + αL)[T][ −][k][−][1](Qk+1 _Qk)_
_k=0_ _−_

X


_ρT =αγ_

=αγ

_≤αγ_

_≤αγ_


_T −1_

(1 + αL)[T][ −][k][−][1]Qk

_k=0_

X


(1 + αL)[T][ −][k]Qk
_k=1_ _−_

X


_T −1_

(1 + αL)[T][ −][k][−][1]Qk
_k=0_ _−_

X


_T −1_

(1 + αL)[T][ −][k][−][1]Qk + QT
_k=0_

X


(1 + αL)


_T −1_

_≤αγ_ _αLe[1][/][2]_ _k=0_ _Qk + QT_ !

X

_αγ(e[2]_ + 1)QT
_≤_

=αγ(e[2] + 1) _A[2]_ + B[2]e[2]∥∇f (wT )∥[2]
q

_≤αγ(e[2]_ + 1)(A + Be∥∇f (wT )∥).


-----

Put it back we obtain


_∥wT −_ _uT ∥≤αγ(e[2]_ + 1) (A + Be∥∇f (uT )∥)

_≤αγ(e[2]_ + 1) (A + BeL∥wT − _uT ∥_ + Be∥∇f (wT )∥),

(1 − _αγ(e[2]_ + 1)BeL)∥wT − _uT ∥≤_ _αγ(e[2]_ + 1) (A + Be∥∇f (wT )∥) .


which gives

If we require


_α_ 2e2n
_≤_ 64e(e[2] + 1)BnL log _δ_

Squaring and substituting the value of γ gives   

2e2n
_wT_ _uT_ 128α[2]n(e[2] + 1)[2] log _A[2]_ + B[2]e[2] _f_ (wT )
_∥_ _−_ _∥[2]_ _≤_ _·_ _δ_ _∥∇_ _∥[2][]_
  

with probability at least (1 − _δ)._

We now provide justifications to Assumption 2 for the random reshuffling scheme.
**Proposition 3. Suppose that we are using the random reshuffling variant of SGD. Assume that for all**
_w_ R[d] _and all examples, Assumption 1 (L-Smoothness) and Assumption 3 hold. For some p_ (0, 1),
_set the constant step size to satisfy ∈_ _α ≤_ max 1460BnL · log 4e[2]T/p _, 2nL_ _−1. Then with ∈_
_probability at least 1_ _p, Assumption 2 holds with γ = 2, C_ [2] = O[˜](nA[2]) and Φ[2] = O[˜](nB[2]).
_−_       

_Proof. For some γ > 0 (but different from the γ of Lemma 3, consider the event that for some_
specific epoch k, for some wτ _w0, w1, . . ., wn(k_ 1) _u0, u1, . . ., un_, where the ui are the u
_∈{_ _−_ _} ∪{_ _}_
from Lemma 3 for epoch k, and for some τ and mk such that n(k − 1) ≤ _τ < τ + mk ≤_ _nk,_

_τ_ +mk−1

_f_ (wτ ; xσ(t)) _f_ (wτ ) _A[2]_ + B[2] _f_ (wτ ) _._
_∇_ _−∇_ _∥∇_ _∥[2]_
_t=τ_ _[≤]_ _[γ]q_

X

Since all the xσ(t) are from the k-th epoch, but wτ is independent of any randomness in the k-th
epoch (as it is either a point visited in a previous epoch, or a value from the u sequence which
depends only on the position at the start of the k-th epoch and not on any k-th epoch randomness), it
follows that we can apply Lemma 6 (Permuted vector Hoeffding inequality) on either this sum or,
alternatively, the terms from epoch k but not in the sum (the terms left out) to get that


_τ_ +mk−1

**P** _f_ (wτ ; xσ(t)) _f_ (wτ ) _A[2]_ + B[2] _f_ (wτ )

_t=τ_ _∇_ _−∇_ _[≥]_ _[γ]q_ _∥∇_ _∥[2]!_

X

_γ[2]_
2e[2] exp _._
_≤_ _−_ 32 min(mk, n _mk)_
 _−_ 

As mk _n, it follows that_
_≤_

_τ_ +mk−1

**P** _f_ (wτ ; xσ(t)) _f_ (wτ ) _A[2]_ + B[2] _f_ (wτ )

_t=τ_ _∇_ _−∇_ _[≥]_ _[γ]q_ _∥∇_ _∥[2]!_

X

2e[2] exp _._
_≤_ _−_ 16[γ][2]n
 

Now, by a union bound the probability that there exists some τ, wτ, and mk such that the average
gradient error is large is bounded by


_τ_ +m−1

_f_ (wτ ; xσ(t)) _f_ (w) _A[2]_ + B[2] _f_ (wτ )
_∇_ _−∇_ _∥∇_ _∥[2]_
_t=τ_ _[≥]_ _[γ]q_

X

2e[2]nT [2] exp
_≤_ _−_ 16[γ][2]n
 


-----

where T is the total number of iterations across all epochs (and we assume that we finish all epochs
so n divides T ). This follows from the fact that there are at most T such τ that we could take on, at
most T values of wτ that can be adopted for each, and at most n values mk can take on. If we set γ
such that

4e2nT 2

2e[2]nT [2] exp = _[p]_ _γ[2]_ = 16n log _,_ (17)
_−_ 16[γ][2]n 2 _⇒_ _p_
   

then with probability at least (1 _/2), it will hold that for all epochs, wτ_, τ, and mk,
_−_ _[p]_


_τ_ +mk−1

_f_ (wτ ; xσ(t)) _f_ (wτ ) _A[2]_ + B[2] _f_ (wτ ) _._ (18)
_∇_ _−∇_ _∥∇_ _∥[2]_
_t=τ_ _[≤]_ _[γ]q_

X

Additionally, set the δ in Lemma 3 to be _[pn]/2T_ . By a union bound on the result of Lemma 3, across
all _[T]/n epochs, with probability at least (1 −_ _[p]/2), it must follow that_

4e2T
_wT_ _uT_ 128α[2]n(e[2] + 1)[2] log _A[2]_ + B[2]e[2] _f_ (wT ) _,_ (19)
_∥_ _−_ _∥[2]_ _≤_ _·_ _p_ _∥∇_ _∥[2][]_
  

for the corresponding ut sequence for all epochs. Therefore, both of these inequalities Eqs. (18)
and (19) hold together with probability at least (1 − _p)._

Now, consider an arbitrary sum of gradient errors going from τ to τ + m − 1. Note since the interval
here is arbitrary, this m can be different from mk and potentially greater than n. We can split this
interval up as follows. Let τ1 be the first epoch boundary in the interval, such that all t going from τ
tointerval. Let τ1 − 1 are within the same epoch as τ2 be the last epoch boundary in the interval, such that all wτ, or else τ1 = τ + m if there is no epoch boundary in the t going from τ2 to τ + m − 1
are within a later epoch than wτ (it may be the case that τ1 = τ2). It follows that

_τ_ +m−1

_f_ (wτ ; xσ(t)) _f_ (wτ )
_∇_ _−∇_
_t=τ_

X


_τ1−1_

_f_ (wτ ; xσ(t)) _f_ (wτ )
_∇_ _−∇_
_t=τ_

X

_τ2−1_

_f_ (wτ ; xσ(t)) _f_ (wτ )
_∇_ _−∇_
_t=τ1_

X


_τ_ +m−1

_f_ (wτ ; xσ(t)) _f_ (wτ )
_∇_ _−∇_
_t=τ2_

X


Observe that since the second of these sums must go over some number of full epochs, its value must
be 0. Therefore,

_τ_ +m−1

_f_ (wτ ; xσ(t)) _f_ (wτ )
_∇_ _−∇_
_t=τ_

X


_τ1−1_

_f_ (wτ ; xσ(t)) _f_ (wτ )
_∇_ _−∇_
_t=τ_

X


_τ_ +m−1

_f_ (wτ ; xσ(t)) _f_ (wτ )
_∇_ _−∇_
_t=τ2_

X


Observe that the term in the first sum must be _[nL]/2-Lipschitz continuous in w, because it can be_
written as the sum of at most ⌊[n]/2⌋ terms each of which is L-Lipschitz (either as the actual terms of
the sum, or else the terms left out of the sum). So, add and subtract _f_ (uτ ; xσ(t)) and _f_ (uτ ),
_∇_ _∇_

_τ_ +m−1

_f_ (wτ ; xσ(t)) _f_ (wτ )
_∇_ _−∇_
_t=τ_

X


_nL_ _wτ_ _uτ_
_≤_ _∥_ _−_ _∥_

_τ1−1_

+ _f_ (uτ ; xσ(t)) _f_ (uτ )

_∇_ _−∇_
_t=τ_

X


_τ_ +m−1

_f_ (wτ ; xσ(t)) _f_ (wτ )
_∇_ _−∇_
_t=τ2_

X


-----

Now applying Eq. (19) on the first term and Eq. (18) on the last two terms gives


_τ_ +m−1

_f_ (wτ ; xσ(t)) _f_ (wτ )
_∇_ _−∇_
_t=τ_

X

4e2T

_nL_ 128α[2]n(e[2] + 1)[2] log
_≤_ _·_ s _·_ _p_



+ 2γ _A[2]_ + B[2] _f_ (wτ ) _._

_∥∇_ _∥[2]_

q


_A[2]_ + B[2]e[2] _f_ (wτ )
  _∥∇_ _∥[2][]_


Squaring both sides for simplicity, we can bound this with

2

_τ_ +m−1

_f_ (wτ ; xσ(t)) _f_ (wτ )
_∇_ _−∇_
_t=τ_

X 4e2T

256α[2]n[3]L[2](e[2] + 1)[2] log _A[2]_ + B[2]e[2] _f_ (wτ )
_≤_ _·_ _p_ _∥∇_ _∥[2][]_
  

+ 8γ[2](A[2] + B[2] _f_ (wτ ) ),
_∥∇_ _∥[2]_

where we have also used Eq. (17) for γ. If we apply our requirement that αLn ≤ 1/2, we get

2

_τ_ +m−1 4e2T

_∇f_ (wτ ; xσ(t)) −∇f (wτ ) _≤_ 64n(e[2] + 1)[2] _· log_ _p_ _A[2]_ + B[2]e[2]∥∇f (wτ )∥[2][]

Xt=τ    4e2nT 2

+ 128n(A[2] + B[2] _f_ (wτ ) ) log _._
_∥∇_ _∥[2]_ _·_ _p_
 

It follows that for random reshuffling with probability 1 − _p, if we set_


_α_ min _,_
_≤_ 4e[2]T 2nL

 64(e[2] + 1)BenL log _p_ 

 4e2T  _−1_

= max 1460BnL log _, 2nL_ _,_

_p_

    

random reshuffling satisfies the requirement of Assumption 2 with γ = 2, C [2] = O[˜](nA[2]), and
Φ[2] = O[˜](nB[2]), where _O[˜](·) hides logarithmic terms in n, T, p._

A.5.4 PROOF FOR PROPOSITION 4 (RANDOM RESHUFFLING WITH DATA ECHOING)

The proof for Proposition 4 is nearly a repeat of the random reshuffling proof in Proposition 3. The
trick is to re-define an epoch when data echoing is used. Instead of referring to an epoch as a random
permutation of the n examples in the training set as in vanilla random reshuffling, here we define the
_cn samples as an epoch, where each example σ(i) is repeated c times. For instance, let c = 3, then_
the k-th epoch with permutation σk is given by the sequence

_. . ., xσk(1), xσk(1), xσk(1), xσk(2), . . ., xσk(n−1), xσk(n), xσk(n), xσk(n), . . . ._
examples used in epoch k

As a result of this redefinition, Lemma| 3 needs to be modified such that wherever{z _n} appears, we now_
need cn. We omit the proof for the following lemma as it is a trivial adaptation from that of Lemma 3.


**Lemma 4. Consider a single epoch of SGD with constant step size α > 0, where the examples come**
_from a random permutation over the training set, each echoed c times. Without loss of generality,_
_assume that the epoch starts at time t = 0 at w0. Let the sequence ut be defined by u0 = w0 and_

_ut+1 = ut −_ _α∇f_ (ut),


-----

_while_

_wt+1 = wt −_ _α∇f_ (wt; xt)

_for some xt chosen from the permutation. Under the same assumptions as Lemma 3, with probability_
_at least (1 −_ _δ) it will hold that for all T ∈{1, . . ., cn},_

2e2cn
_wT_ _uT_ 128α[2]cn(e[2] + 1)[2] log _A[2]_ + B[2]e[2] _f_ (wT ) _._
_∥_ _−_ _∥[2]_ _≤_ _·_ _δ_ _∥∇_ _∥[2][]_
  

The justifications for Assumption 2 for the random reshuffling with data echoing scheme can also be
obtained from Appendix A.5.3 similarly. At appropriate places one should invoke Lemma 4 instead
of Lemma 3, and replace n with cn. We omit the proof as it is again a trivial modification.

A.5.5 PROOF FOR PROPOSITION 5 (MARKOV CHAIN GRADIENT DESCENT (MCGD))

To justify Assumption 2 for MCGD, we will first need the following lemma.
**Lemma 5. Let F be any vector-valued measurable function, and let x0, x1, . . . be a sequence of**
_samples from a Markov chain with mixing time tmix and stationary distribution π starting from an_
_arbitrary initial distribution. If the function is constrained such that ∥F_ (x)∥≤ 1 for all x, and if we
_also have EX∼π[F_ (X)] = 0, then for any δ ∈ (0, 1),

_m−1_ 2e2

**P** _F_ (xi) 2m log _δ,_

_i=0_ _[≥]_ [5][t][mix]s  _δ_ [!] _≤_

X

_Proof. Consider the Doob martingale_

_m−1_

_Wk = E_ " _i=0_ _F_ (xi) | Fk# _,_

X

where Fk contains all randomness up to timestep k, i.e. x0, x1, . . ., xk, and so (as usual for a Doob
martingale) the martingale property is trivially satisfied using repeated conditioning:

_m−1_ _m−1_

**E [Wk+1** _k+1] = E_ **E** _F_ (xi) _k+1_ _k_ = E _F_ (xi) _k_ = Wk.
_| F_ " " _i=0_ _| F_ # _| F_ # " _i=0_ _| F_ #

X X

Observe that the sum we want is Wm = E _mi=0−1_ _[F]_ [(][x][i][)][ | F][m] = _i=0_ _[F]_ [(][x][i][)][, and that this sum]
has increments hP i

_m_ 1 _m_ 1 [P][m][−][1]
_−_ _−_

_Wk+1 −_ _Wk = E_ " _i=0_ _F_ (xi) | Fk+1# _−_ **E** " _i=0_ _F_ (xi) | Fk#

X X

_k+1_ _m−1_

= E _F_ (xi) _k+1_ + E _F_ (xi) _k+1_

" _i=0_ _| F_ # "i=k+2 _| F_ #
X X

_k_ _m−1_

**E** _F_ (xi) _k_ **E** _F_ (xi) _k_
_−_ " _i=0_ _| F_ # _−_ "i=k+1 _| F_ #

X X


_k+1_

_F_ (xi)
_−_
_i=0_

X


_m−1_

**E [F** (xi) _k+1]_
_| F_ _−_
_i=k+2_

X


_m−1_

**E [F** (xi) _k]_
_| F_
_i=k+1_

X


_F_ (xi) +
_i=0_

X


_m−1_

**E [F** (xi) _k+1]_
_| F_ _−_
_i=k+2_

X


_m−1_

**E [F** (xi) _k] ._
_| F_
_i=k+1_

X


= F (xk+1) +


Now, observe that since the mixing time of the Markov chain is tmix, for any i ≥ _k, if µ denotes the_
distribution of xi conditioned on Fk, then using results from Levin & Peres (2017, Section 4.5)

_µ_ _π_ _T V_ 2[−⌊][(][i][−][k][)][/t][mix][⌋].
_∥_ _−_ _∥_ _≤_


-----

It follows from this and the fact that F is bounded that


**E [F** (xi) _k]_ 2[−⌊][(][i][−][k][)][/t][mix][⌋].
_∥_ _| F_ _∥≤_


Therefore,


_m−1_

**E [F** (xi) _k+1]_
_| F_
_i=k+2_

X

and similarly for the last term. It follows that


_m−1_

**E [F** (xi) _k+1]_
_∥_ _| F_ _∥_
_i=k+2_

X

_m−1_

2[−⌊][(][i][−][k][−][1)][/t][mix][⌋]

_i=k+2_

X

_∞_

_i=k+2_ 2[−⌊][(][i][−][k][−][1)][/t][mix][⌋] = 2tmix − 1,

X


_∥Wk+1 −_ _Wk∥≤_ 4tmix.

Therefore, by the vector Azuma’s inequality of Hayes (2005, Theorem 1.8), for any a > 0,

**P (** _Wm_ _W0_ 4tmixa) 2e[2] exp _._
_∥_ _−_ _∥≥_ _≤_ _−_ 2[a]m[2]
 

On the other hand, by the same reasoning as before,

_m−1_

_W0_ = _F_ (xi) 0
_∥_ _∥_ " _i=0_ _| F_ #

X

_m−1_

_F[E](x0)_ + _F_ (xi) 0
_≤∥_ _∥_ " _i=1_ _| F_ #

X

**[E]**


_≤_ 1 + 2tmix − 1 = 2tmix.

**P (** _Wm_ 2tmix + 4tmixa) 2e[2] exp
_∥_ _∥≥_ _≤_ _−_ 2[a]m[2]



So,

Now, setting a such that


2e2
= δ _a[2]_ = 2m log
_⇒_ _δ_



2e[2] exp
_−_ 2[a]m[2]


and noting that this makes a ≥ 2, we get


[!]


2e2
2m log

_δ_




_m−1_

**P** _F_ (xi)

_i=0_ _[≥]_ [5][t][mix]

X

which is what we wanted to show.


_≤_ _δ,_


The justification for Assumption 2 then follows straightforwardly.

**Proposition 5. Suppose that we use samples xt from a Markov chain with mixing time tmix. Assume**
_that for all w ∈_ R and all examples xt, Assumption 3 holds. Then with probability at least 1 − _p,_
_Assumption 2 holds with γ = 1, C_ [2] = O[˜](A[2]t[2]mix[)][, and][ Φ][2][ = ˜]O(B[2]t[2]mix[)][.]

_Proof. Observe that we need Lemma 5 to hold for all subintervals of examples, of which there are_
only at most T [2]. So, setting δ to be _[p]/T_ [2] in Lemma 5, we can show that for all τ, m, the probability


2e2T 2
25 _A[2]_ + B[2] _f_ (wτ ) _t[2]mix[2][m][ log]_
_≥_  _∥∇_ _∥[2][]_  _p_


_τ_ +m−1

( _f_ (wτ ; xt) _f_ (wτ ))
_∇_ _−∇_
_t=τ_

X


-----

is at least 1 − _p. Equivalently,_

2

1 _τ_ +m−1 2e2T 2

( _f_ (wτ ; xt) _f_ (wτ )) mix _A[2]_ + B[2] _f_ (wτ ) log _._

_m_ _∇_ _−∇_ _≤_ [50]m [t][2] _∥∇_ _∥[2][]_ _·_ _p_

Xt=τ   

It follows that MCGD satisfies the requirements of Assumption 2 with γ = 1,

2e2T 2 2e2T 2
_C_ [2] = 50A[2]t[2]mix [log] and Φ[2] = 50B[2]t[2]mix [log] _._

_p_ _p_

   

The big-O[˜] expressions in the proposition statement immediately follow.

A.5.6 JUSTIFICATION FOR QMC-BASED DATA AUGMENTATION WITH RANDOM RESHUFFLING

Before we begin with the proof, let us first present the introductory material necessary on quasi-Monte
Carlo methods. Similar to Monte Carlo integration, QMC is also used for numerical integration
but using low-discrepancy sequences instead of pseudorandom number sequences. Concretely, the
problem is to approximate the integral of a function f over some s-dimensional hypercube [0, 1][s],


_I(f_ ) :=



[0,1][s][ f] [(][x][)][dx]


using the average of the function evaluated at a sequence of points x1, . . ., xm,


_Im(f_ ) := [1]


_f_ (xi).
_i=1_

X


The approximation error rate is defined as ϵ = _I(f_ ) _Im(f_ ), and it is well-known that in the case
_|_ _−_ _|_
of Monte Carlo integration where the xi’s are drawn uniformly at random from [0, 1][s], the error rate
is ϵ[2] = O([1]/m). If the sequence of xi’s has low star-discrepancy, which is defined as

_m_

1

_Dm[∗]_ [:= sup] 1 _xi_ _U_ Volume(U )
_U_ _m_ _i=1_ _{_ _∈_ _} −_

X

where U = _j=1[[0][, b][i][]][ for][ b][i][ ∈]_ [[0][,][ 1)][, and the volume is measured using the][ s][-dimensional Lebesgue]
measure. Intuitively, the smaller this quantity is the more evenly the sequence of points covers the
space. Some popular low-discrepancy sequences include the Halton sequence, Sobol sequence, van

[Q][s]
der Corput sequence, etc., which are essentially deterministic sequences that are cleverly constructed
to mimic random numbers but in fact have low star-discrepancy. For instance, the Halton sequence
satisfies Dm[∗] [=][ O][(][(log][ m][)][s][/][m][)][.]

To ensure fast convergence of Im to I as we increase m, in addition to using low discrepency
sequences we also need f to be relatively well-behaved. For this, the Hardy-Krause variation VHK is
often used, for which we refer the reader to Drmota & Tichy (2006, Definition 1.13) for its detailed
characterization. Most importantly, if f has finite VHK on [0, 1][s], then the Koksma-Hlawka inequality
guarantees that

_ϵ ≤_ _VHKDm[∗]_ _[.]_

This implies that using quasi-Monte Carlo with, for instance, the Halton sequence, to integrate
a function f with bounded Hardy-Krause variation, the error rate would be ϵ[2] = O([(log][ m][)][2][s]/m[2]).
In comparison to the Monte Carlo error rate, the QMC error rate can be much faster when the
dimensionality s is relatively small. For an in-depth exposition of the related materials we recommend
Owen (2003) and Drmota & Tichy (2006).

We are now ready to restate our proposition from Section 5, in which we describe our QMC data
augmentation setup.
**Proposition 6. Suppose that we are using the random reshuffling variant of SGD with QMC data**
_augmentation as described. Assume that for all w ∈_ R[d] _and all examples, Assumptions 1, 3,_
_4 and 5 hold for Equation 3. For some p ∈_ (0, 1), set the step size to be a constant such that
_α ≤_ max{1460BnL · log(4e[2]T/p _, 2nL})[−][1]. Then with probability at least 1 −_ _p, Assumption 2_

_holds with _ _γ = 2, C_ [2] = _O[˜](n[2]V_ [2]CQMC[2] [log(][T] [)][2][s][ +][ nA][2][)][ and][ Φ][2][ =][ ˜]O(nB[2]).


-----

_Proof. Recall that the example used in the t-th iteration is being transformed as_

_xt = A(x[(][σ][⌊][t/n][⌋][(][t][ mod][ n][))], ζ⌊t/n⌋+σ⌊t/n⌋(t mod n))._

We start from the average gradient error term, that is,

2

1 _τ_ +m−1

_f (wτ_ ; (xt, ζt)) _f_ (wτ )

_m_ _∇_ _A_ _−∇_

_t=τ_

X


_τ_ +m−1

_f (wτ_ ; (xt, ζt))
_∇_ _A_ _−_ _n[1]_
_t=τ_

X


**E** _x[(][i][)]; ζ_
_ζ_ [0,1][s][∇][f] [(][w][τ] [;][ A]
_i=1_ _∼_

X 


_τ_ +m−1

_f (wτ_ ; (xt, ζt))
_∇_ _A_ _−_ _m[1]_
_t=τ_

X


_τ_ +m−1

**E**
_ζ_ [0,1][s][∇][f] [(][w][τ] [;][ A][ (][x][t][;][ ζ][))]
_t=τ_ _∼_

X


_≤_ 2

+ 2


_τ_ +m−1

**E**
_ζ_ [0,1][s][∇][f] [(][w][τ] [;][ A][ (][x][t][;][ ζ][))][ −] _n[1]_
_t=τ_ _∼_

X


**E** _x[(][i][)]; ζ_
_ζ_ [0,1][s][∇][f] [(][w][τ] [;][ A]
_i=1_ _∼_

X 


where the first step follows from the definition of f in the data augmentation setup (Eq. (3)). In the
second step, the first norm relates to the QMC approximation error while the second norm relates
to the RR analysis. From the analysis in Section A.5.3 we know the second norm can be bounded
asymptotically (and probabilistically) by [˜](nA[2]) + [˜](nB[2]) _f_ (wτ ) _/m[2]. Now we analyze the_
_O_ _O_ _∥∇_ _∥[2]_
first term. Since we use a contiguous QMC subsequence on each individual example, for the period
of [τ, τ + m − 1], we define τj and mj as the starting point and length of example x[(][j][)] being chosen
during this period, such that _j=1_ _[m][j][ =][ m][. With this notation, we can now rewrite the first norm as]_

2

1 _τ_ +m−1 [P][n]

_f (wτ_ ; (xt, ζt)) **E**

_m_ _∇_ _A_ _−_ _ζ_ [0,1][s][∇][f] [(][w][τ] [;][ A][ (][x][t][;][ ζ][))]

_t=τ_ _∼_

X


_τj_ +mj _−1_

_f_ _wτ_ ; _x[(][j][)], ζt_ **E** _wτ_ ; _x[(][j][)]; ζ_
_∇_ _A_ _−_ _ζ_ [0,1][s][∇][f] _A_
_t=τj_ _∼_

X      


_j=1_

_n_

_j=1_

X


_n_ _τj_ +mj _−1_

_f_ _wτ_ ; _x[(][j][)], ζt_ **E** _wτ_ ; _x[(][j][)]; ζ_

_≤_ _m[n][2]_ _∇_ _A_ _−_ _ζ_ [0,1][s][∇][f] _A_

_j=1_ _t=τj_ _∼_

X X      

_n_ 1 _τj_ +mj _−1_

_f_ _wτ_ ; _x[(][j][)], ζt_ **E** _wτ_ ; _x[(][j][)]; ζ_

_≤_ [2][n][(][τ][j]m[ +][2][ m][j][)][2] _j=1_ _τj + mj_ _t=0_ _∇_ _A_ _−_ _ζ∼[0,1][s][∇][f]_ _A_

X X      

2

_j_ _n_ 1 _τj_ _−1_
+ [2][nτ][ 2] _f_ _wτ_ ; _x[(][j][)], ζt_ **E** _wτ_ ; _x[(][j][)]; ζ_ _._

_m[2]_ _τj_ _∇_ _A_ _−_ _ζ_ [0,1][s][∇][f] _A_

_j=1_ _t=0_ _∼_

X X      


Now we can use the Koksma–Hlawka inequality (Aistleitner & Dick, 2014) to bound these norms,
which gives us

2

1 _τ_ +m−1

_f (wτ_ ; (xt, ζt)) **E**

_m_ _∇_ _A_ _−_ _ζ_ [0,1][s][∇][f] [(][w][τ] [;][ A][ (][x][t][;][ ζ][))]

_t=τ_ _∼_

X


QMC[V][ 2] _n_ log(τj + mj)[2][s] + log(τj)[2][s][] _n[2]CQMC[2]_ _[V][ 2][ log(][T]_ [)][2][s] _._

_≤O_  _m[2]_ _j=1_ _·_  _≤O_ _m[2]_ !

X  

 _[n][C][2]_ 

Putting it together, we have shown that random reshuffling with QMC data augmentation satisfies
Assumption 2 with γ = 2, C [2] = [˜] _n[2]_ QMC[V][ 2][ log(][T] [)][2][s][ +][ nA][2][], and Φ[2] = [˜] _nB[2][]._
_O_ _C[2]_ _O_
   


-----

A.6 MISCELLANEOUS LEMMAS

We now collect some technical lemmas used in our analyses.

The following lemma is a Hoeffding-type concentration bound on the sums of random permutations
of vectors. This lemma is particularly useful for simplifying the logic of the proofs of our shuffling
propositions above, as it frees us from having to use Doob-martingale-type arguments throughout.

**Lemma 6 (Permuted vector Hoeffding inequality). Let n ≥** _m > 0 be some integers, and let Xi,j_
_for i ∈{1, . . ., m} and j ∈{1, . . ., n} be vectors in R[d]. We also require for all i, j, ∥Xi,j∥≤_ 1,
_and that for all i,_

_n_

_Xi,j = 0._
_j=1_

X

_Then, if σ :_ 1, . . ., n 1, . . ., n _is a random permutation and that the individual Xi,j’s do not_
_{_ _} →{_ _}_
_depend on σ,_


2e[2] exp
_≤_ _−_ 32[a][2]m



_Xi,σ(i)_
_i=1_

X



_[≥]_ _[a]_


Note that this lemma trivially also applies to random subsamples of vector sums, by just letting
_Xi,j = Yj for the desired vector sequence Yj._

_Proof. This proof is adapted from the proof of Theorem 4.3 in Bercu et al. (2015). Their approach_
is more sophisticated and tends to a Bernstein-type inequality, but is specialized to only scalars.
Consider sampling the elements of the permutation one at a time, and let Fj be the filtration containing
the random variables σ(1), σ(2), . . ., σ(j) but not σ(j + 1), . . ., σn. For k _n, define the process_
_≤_


_m_

_Xi,σ(i)_ _k_

" _i=1_ _| F_
X


_Wk = E_


Observe that this must be a (vector) martingale process, as it is a Doob martingale. Explicitly, we can
write it as


_k_

_Xi,σ(i) +_

" _i=1_
X

_k_

_Xi,σ(i)_

 _i=1_ _−_

X



_Wk = E_

= E


_Xi,j_ _k_
_j /∈{σ(1),...,σ(k)} | F_

X


_n −_ _k_

1

_n −_ _k_


_i=k+1_

_m_

_i=k+1_

X


_,_






_Xi,σ(j)_ _k_
_j=1_ _| F_

X


-----

where here we used the fact that the Xi,j’s sum to 0 over j = [n]. Thus, the increments of this process
will have


_Wk_ _Wk_ 1 = E
_−_ _−_


_Xi,σ(i)_
_i=1_ _−_

X


_Xi,σ(j)_ _k_

_i=k+1_ _j=1_ _| F_

X X


_n −_ _k_


_k−1_ 1

_i=1_ _Xi,σ(i) −_ _n −_ _k + 1_

X


_k−1_

_Xi,σ(j)_ _k−1_
_j=1_ _| F_

X


_−_ **E**


_i=k_


= Xk,σ(k)
_−_


_n_ _k_ **[E]**
_−_


_Xi,σ(j)_ _k_
_j=1_ _| F_

X


_i=k+1_


_m_ _k−1_

_Xi,σ(j)_ _k−1_

_i=k_ _j=1_ _| F_

X X


_n_ _k + 1_ **[E]**
_−_


= Xk,σ(k)
_−_


_Xi,σ(k)_
_i=k+1_

X


_n −_ _k_


_k−1_

_Xi,σ(j) +_
_j=1_

X


_k−1_

_Xk,σ(j),_
_j=1_

X


_n −_ _k_ _[−]_


_n −_ _k + 1_


_n −_ _k + 1_


_i=k+1_


where we have repeatedly applied the martingale property. Now to bound all of these, first by our
assumption
_Xk,σ(k)_ 1.
_≤_

Also,

_m_

1

_n_ _k_ _Xi,σ(k)_ _n_ _k_
_−_ _i=k+1_ _[≤]_ _[m] −[ −]_ _[k]_ _[≤]_ [1][.]

X

Next, again using the sum to 0 assumption, we have


_k−1_

_Xk,σ(j)_
_j=1_

X


_≤_ _[n]n[ −]_ _[k]k[ + 1] + 1 [= 1][,]_

_−_


_Xk,σ(j)_
_j=k_

X


_n −_ _k + 1_


_n −_ _k + 1_


and finally, by a combination of these bounds,


_m_ _k−1_

_Xi,σ(j)_

_i=k+1_ _j=1_

X X


_k−1_

_Xi,σ(j)_
_j=1_

X


_n −_ _k_ _[−]_


_n −_ _k + 1_


(n − _k)(n −_ _k + 1)_


_i=k+1_


_≤_ (n _k)(n_ _k + 1)_

_−_ _−_ _[·][ (][m][ −]_ _[k][)][ ·][ (][n][ −]_ _[k][ + 1)]_

_≤_ 1.


It follows that the increment satisfies


_∥Wk −_ _Wk−1∥≤_ 4

with probability 1. We can now apply the vector Azuma’s inequality of Hayes (2005, Theorem 1.8).
Applying this to Wm gives the result that, for any a,

**P (** _Wm_ _a)_ 2e[2] exp _._
_∥_ _∥≥_ _≤_ _−_ 32[a][2]m
 


This proves the lemma.

The next lemma we state is a standard result showing that we can cover a region of radius R with a
bounded number of balls of radius ϵ.


-----

**Lemma 7 (ϵ-net lemma). For any region D of radius R in d-dimensional space, and any ϵ > 0, there**
_exists a subset S ⊆D such that S is of size at most_

_d_

1 + [2][R] _,_
_|S| ≤_ _ϵ_
 

_and for every point x ∈D, there exists a ˆx ∈S such that ∥x −_ _xˆ∥≤_ _ϵ._

_Proof. Consider the following procedure. As long as there are points in D that are not within a_
distance ϵ of an existing point in S, choose one such point arbitrarily and add it to S. Observe that
with this construction, any two points in S must be at a distance greater than ϵ from each other. This
means that if we center a ball with radius ϵ/2 at each of the points in S, these balls will be disjoint.

The total volume of these balls will bein d-dimensional space. However, the centers of all these balls must lie in |S| · V1 · (ϵ/2)d, where V1 is the volume of the unit ball D, and so the balls
themselves must all lie within a slightly larger region of radius R + _ϵ/2. This region will have volume_
_V1_ (R + ϵ/2)d. This shows that our process must eventually stop, implying that must exist, and
gives us a bound on the size of · _S as_ _S_

_d_

= 1 + [2][R] _,_

_|S| ≤_ [(][R]([ +]ϵ/[ ϵ/]2)[d][2)][d] _ϵ_

 

and the proof is complete.


-----

