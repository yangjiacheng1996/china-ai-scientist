Under review as a conference paper at ICLR 2022
SCALING-UP
DIVERSE
ORTHOGONAL
CONVOLU-
TIONAL NETWORKS BY A PARAUNITARY FRAMEWORK
Anonymous authors
Paper under double-blind review
ABSTRACT
Enforcing orthogonality in convolutional neural networks is an antidote for gra-
dient vanishing/exploding problems, sensitivity to perturbation, and bounding
generalization errors. Many previous approaches for orthogonal convolutions
enforce orthogonality on its ﬂattened kernel, which, however, do not lead to the
orthogonality of the operation. Some recent approaches consider orthogonality
for standard convolutional layers and propose speciﬁc classes of their realizations.
In this work, we propose a theoretical framework that establishes the equivalence
between diverse orthogonal convolutional layers in the spatial domain and the
paraunitary systems in the spectral domain. Since 1D paraunitary systems admit
a complete factorization, we can parameterize any separable orthogonal convolu-
tion as a composition of spatial ﬁlters. As a result, our framework endows high
expressive power to various convolutional layers while maintaining their exact
orthogonality. Furthermore, our layers are memory and computationally efﬁcient
for deep networks compared to previous designs. Our versatile framework, for the
ﬁrst time, enables the study of architectural designs for deep orthogonal networks,
such as choices of skip connection, initialization, stride, and dilation. Consequently,
we scale up orthogonal networks to deep architectures, including ResNet and Shuf-
ﬂeNet, substantially outperforming their shallower counterparts. Finally, we show
how to construct residual ﬂows, a ﬂow-based generative model that requires strict
Lipschitzness, using our orthogonal networks.
1
INTRODUCTION
Convolutional neural networks (CNNs), whose deployment has witnessed extensive empirical success,
still exhibit a range of limitations that are not thoroughly studied. Firstly, deep convolutional networks
are in general difﬁcult to learn, and their high performance heavily relies on techniques that are not
fully understood, such as skip-connections (He et al., 2016a), batch normalization (Ioffe & Szegedy,
2015), specialized initialization (Glorot & Bengio, 2010). Secondly, they are notoriously sensitive
to imperceptible perturbations, including adversarial attacks (Goodfellow et al., 2014) or geometric
transformations (Azulay & Weiss, 2019). Finally, a precise characterization of their generalization
property is still under active investigation (Neyshabur et al., 2018; Jia et al., 2019).
Orthogonal networks, which have a “ﬂat” spectrum with all singular values of the layer being 1 (each
layer’s output norm ∥y∥equals its input norm ∥x∥, ∀x), alleviate all problems above. As shown in
recent works, by enforcing orthogonality in neural nets, we obtain (1) easier optimization (Zhang
et al., 2018a; Qi et al., 2020): since each orthogonal layer preserves the gradient norm during
backpropagation, an orthogonal network is free from gradient vanishing/ exploding problems; (2)
robustness against adversarial perturbation (Anil et al., 2019; Li et al., 2019b; Trockman & Kolter,
2021): since each orthogonal layer is 1-Lipschitz, an orthogonal network can not amplify any
perturbation to the input to ﬂip the output prediction; (3) better generalizability as proved in (Jia et al.,
2019): a network’s generalization error is positively related to the standard deviation of each linear
layer’s singular values, thus encouraging orthogonality in the network lowers its generalization error.
Despite the beneﬁts, enforcing orthogonality in convolutional networks is challenging. To avoid the
strict constraint, orthogonal initialization (dynamical isometry) (Pennington et al., 2017; Xiao et al.,
2018) and orthogonal regularization (Wang et al., 2019; Qi et al., 2020) are adopted to address the
gradient vanishing/exploding problems. However, these methods are not suitable for applications that
1
Under review as a conference paper at ICLR 2022
require strict Lipschitzness, such as adversarial robustness (Anil et al., 2019) and residual ﬂows (Chen
et al., 2019), as they do not enforce strict orthogonality (and Lipschitzness) during training.
Our goal is to enforce exact orthogonality in state-of-the-art convolutional networks without expensive
computations. We identify three main challenges in doing so. Challenge I: Achieving exact orthog-
onality throughout the training process. Prior works such as orthogonal regularization (Wang
et al., 2019) and reshaped kernel orthogonality (Jia et al., 2017; Cisse et al., 2017), while enjoying
algorithmic simplicity, do not meet the requirement of exact orthogonality. Note that enforcing
orthogonality during training is necessary as a post-training orthogonalization of the weights can
ruin the performance. Challenge II: Avoiding expensive computations. An efﬁcient algorithm is
crucial for scalability to large networks. Existing work based on projected gradient descent (Sedghi
et al., 2019), however, requires expensive projection after each update. For instance, the projection
step in Sedghi et al. (2019) computes an SVD and ﬂattens the spectrum to enforce orthogonality,
which costs O(size(feature) · channels3) for a convolutional layer. Challenge III: Scaling-up to
state-of-the-art deep convolutional networks. There are many variants to the standard convolu-
tional layer, including dilated, strided, group convolutions, which are essential for state-of-the-art
deep convolutional networks. However, none of the existing methods proposes mechanisms to
orthogonalize these variants. The lack of techniques, as a result, limits the broad applications of
orthogonal convolutional layers to state-of-the-art deeper convolutional networks.
We resolve challenges I, II, and III by proposing complete parameterizations for orthogonal 1D-
convolutions and separable orthogonal 2D-convolutions. First, using the convolution theorem (Op-
penheim et al., 1996) (convolution in the spatial domain is equivalent to multiplication in the spectral
domain), we reduce the problem of designing orthogonal convolutions to constructing unitary ma-
trices for all frequencies, i.e., a paraunitary system (Vaidyanathan, 1993). Therefore, we obtain a
parameterization for all separable orthogonal 2D-convolutions, guaranteeing exact orthogonality and
high expressive power. Note that there is no previous approach that achieves exact orthogonality (up
to machine precision). For the ﬁrst time, our versatile framework enables the study in the designs of
deep orthogonal networks, such as skip connection, initialization, stride, and dilation. Consequently,
we scale orthogonal networks to deep architectures, including ResNet and ShufﬂeNet, substantially
outperforming their shallower counterparts. Our proposed method serves as gaining insight into the
trade-off between orthogonality and expressiveness. We observe that exact orthogonality is essential
in these deeper architectures with more than 10 layers (detailed discussion of exact orthogonality in
Appendix E.3). Finally, we show how to deploy our orthogonal networks in Residual Flow (Chen
et al., 2019), a ﬂow-based generative model that requires strict Lipschitzness (Appendix F).
Summary of Contributions:
1. We establish the equivalence between orthogonal convolutions in the spatial domain and paraunitary
systems in the spectral domain. Consequently, we can interpret existing approaches for orthogonal
convolutions as implicit designs of paraunitary systems.
2. Based on a complete factorization of 1D paraunitary systems, we propose the ﬁrst exact and
complete design for orthogonal 1D-convolutions as well as separable orthogonal 2D-convolutions,
ensuring exact orthogonality and high expressive power. None of the existing works achieve a
complete parameterization of all orthogonal 2D-convolutions.
3. We prove that orthogonality for various convolutional layers (strided, dilated, group) are also
entirely characterized by paraunitary systems. Consequently, our design easily extends to these
variants, ensuring both completeness and exactness of the orthogonal convolutions.
4. We study the design considerations for orthogonal networks (choices of skip connection, initializa-
tion, depth, width, and kernel size), and show that orthogonal networks can scale to deep architectures
including ResNet, WideResNet, and ShufﬂeNet.
2
ORTHOGONAL CONVOLUTIONS VIA PARAUNITARY SYSTEMS
Designing orthogonal convolutional layer {ht,s : yt = ht,s ∗xs}T,S
t=1,s=1 (s, t index input/output
channels) in the spatial domain is challenging. When a convolution layer is written as a matrix-vector
product, where the matrix is block-circulant and its (t, s)th takes a circulant form Cir (ht,s) as:
Cir (ht,s) =




ht,s[1]
ht,s[N]
· · ·
ht,s[2]
ht,s[2]
ht,s[1]
ht,s[N]
· · ·
.
.
.
...
...
.
.
.
ht,s[N]
· · ·
ht,s[2]
ht,s[1]



∈RN×N.
(2.1)
2
Under review as a conference paper at ICLR 2022
Therefore, the layer is orthogonal if the block-circulant matrix [Cir (ht,s)]T,S
t=1,s=1 is orthogonal.
However, it is not obvious how to enforce orthogonality in a block-circulant matrix.
2.1
ACHIEVING ORTHOGONAL CONVOLUTIONS BY PARAUNITARY SYSTEMS
We propose a novel design of orthogonal convolutions from a spectral domain perspective, motivated
by the convolution theorem (Theorem 2.1). For simplicity, we group the entries at the same locations
into a vector/matrix, e.g., we denote {xs[n]}S
s=1 as x[n] ∈RS and {ht,s[n]}T,S
t=1,s=1 as h[n] ∈RT ×S.
Theorem 2.1 (Convolution theorem (Oppenheim et al., 1996)). For a standard convolution layer
h: y[i] = P
n h[n]x[i −n], the convolution in the spatial domain is equivalent to a matrix-
vector multiplication in the spectral domain, i.e., Y (z) = H(z)X(z), ∀z ∈C. Here, X(z) =
PN−1
n=0 x[n]z−n, Y (z) = PN−1
n=0 y[n]z−n, H(z) = PL
n=−L h[n]z−n denote the z-transforms of
input, output, kernel respectively, where N is the length of x, y and [−L, L] is the span of the ﬁlter h.
The convolution theorem states that a standard convolution layer is a matrix-vector multiplication in
the spectral domain. As long as the transfer matrix H(z) is unitary at z = ejω for all frequencies
∀ω ∈R (j is the imaginary unit), the convolutional layer h is orthogonal.
Therefore, as a major novelty of our paper, we design orthogonal convolutions through designing
unitary transfer matrix H(ejω) at all frequencies ω ∈R, which is known as a paraunitary sys-
tems (Vaidyanathan, 1993; Strang & Nguyen, 1996). We prove in Theorem B.5 that a convolutional
layer is orthogonal in the spatial domain if and only if it is paraunitary in the spectral domain.
Beneﬁts through paraunitary systems. (1) The spectral representation simpliﬁes the designs of
orthogonal convolutions and avoids the analysis of block-circulant structure. (2) Since paraunitary
systems are necessary and sufﬁcient for orthogonal convolutions, it is impossible to ﬁnd an orthogonal
convolution whose transfer matrix is not paraunitary. (3) There exists a complete factorization of
paraunitary systems: any paraunitary H(z) can be realized through multiplications in the spectral
domain, as will show in Equation (2.2a). (4) Since multiplications in spectral domain correspond to
convolutions in spatial domain, any orthogonal convolution can be realized as cascaded convolutions
of multiple sub-layers, each parameterized by an orthogonal matrix. (5) There are mature methods that
represent orthogonal matrices via unconstrained parameters. Consequently, we can learn orthogonal
convolutions using standard optimizers on a model parameterized via our design.
Interpretation of existing methods. Since paraunitary system is a necessary and sufﬁcient condition
for orthogonal convolution, all existing approaches, including singular value clipping and masking
(SVCM) (Sedghi et al., 2019), block convolution orthogonal parameterization (BCOP) (Li et al.,
2019b), Cayley Convolution (CayleyConv) (Trockman & Kolter, 2021), skew orthogonal convolution
(SOC) design paraunitary systems implicitly. We discuss these interpretations in Appendix C.3.3, and
show that our SC-Fac has the lowest computational complexity among all approaches.
2.2
REALIZING PARAUNITARY SYSTEMS VIA RE-PARAMETERIZATION
After reducing the problem of orthogonal convolutions to paraunitary systems, we are left with the
question of how to realize paraunitary systems. We use a complete factorization form of paraunitary
systems to realize any paraunitary systems. According to Theorem C.7, any paraunitary system H(z)
can be written as the form in Equation (2.2a) and any H(z) in this form is a paraunitary system:
H(z) = V (z; U (−L)) · · · V (z; U (−1))QV (z−1; U (1)) · · · V (z−1; U (L)),
(2.2a)
where V (z; U (ℓ)) = (I −U (ℓ)U (ℓ)⊤) + U (ℓ)U (ℓ)⊤z, ∀ℓ∈{−L, · · · , −1} ∪{1, · · · , L}.
(2.2b)
Here Q is an orthogonal matrix and each U (ℓ) is a column-orthogonal matrix whose number of
columns is sampled uniformly from {1, · · · , T}. As multiplications in the spectral domain are
equivalent to convolutions in the spatial domain, this complete spectral factorization of paraunitary
systems in Equation (2.2a) allows us to parameterize any orthogonal convolution in the spatial domain
as cascaded convolutions of V (z; U (ℓ))’s spatial counterparts and the orthogonal matrix Q.
Model design in the spatial domain. We obtain a complete design of orthogonal 1D-convolutions:
using learnable (column)-orthogonal matrices ({U (ℓ)}−1
ℓ=−L, Q, {U (ℓ)}L
ℓ=1), we parameterize a size
(L + L + 1) convolution as cascaded convolutions of the following ﬁlters in the spatial domain
nh
I −U (ℓ)U (ℓ)⊤, U (ℓ)U (ℓ)⊤io−1
ℓ=−L , Q,
nh
U (ℓ)U (ℓ)⊤, I −U (ℓ)U (ℓ)⊤ioL
ℓ=1 .
(2.3)
3
Under review as a conference paper at ICLR 2022
Figure 1 provides a visualization of our proposed design of orthogonal convolution layers; each block
denotes a convolution and the form of the ﬁlter is displayed in each of the block. In practice, we
compose all (L + L + 1) ﬁlters into one for orthogonal convolution, which not only increases the
computational parallelism but also avoids storing intermediate outputs between ﬁlters.
Q
[ I−U
(1)(U
(1))
Τ
,U
(1)(U
(1))
Τ]
[ I−U
(L)(U
(L))
Τ
,U
(L)(U
(L))
Τ]
…
x
y
[U
(−1)(U
(−1))
Τ ,
I −U
(−1)(U
(−1))
Τ]
…
[U
(−L)(U
(−L))
Τ ,
I−U
(−L)(U
(−L))
Τ]
Figure 1: Complete design of orthogonal convolutional layer as a cascade of convolutions, whose ﬁlter
coefﬁcients are depicted in each block. The matrix Q is orthogonal and U (ℓ)’s are column-orthogonal.
Using this 1D-convolution, we obtain a complete design for separable orthogonal 2D-convolutions,
which can be represented as a convolution of two orthogonal 1D-convolutional layers. With separabil-
ity, we obtain a design of orthogonal 2D-convolutions by composing two complete designs of orthogo-
nal 1D-convolutions. As a result, we can parameterize separable orthogonal 2D-convolution with ﬁlter
size (L1+L1+1)×(L2+L2+1) as a convolution of two orthogonal 1D-convolutions with learnable
(column-)orthogonal matrices ({U (ℓ)
1 }−1
ℓ=−L1, Q1, {U (ℓ)
1 }L1
ℓ=1) and ({U (ℓ)
2 }−1
ℓ=−L2, Q2, {U (ℓ)
2 }L2
ℓ=1).
With a complete factorization of paraunitary systems (1D and separable 2D), we reduce the problem
of designing orthogonal convolutions to the one for orthogonal matrices.
Parameterization for orthogonal matrices. In Appendix C.4, we perform a comparative study on
different parameterizations of orthogonal matrices, including the Björck orthogonalization (Anil
et al., 2019; Li et al., 2019b), the Cayley transform (Helfrich et al., 2018; Maduranga et al., 2019),
and the exponential map (Lezcano-Casado & Martínez-Rubio, 2019). In our implementation, we
adopt a modiﬁed exponential map due to its efﬁciency, exactness, and completeness. The exponential
map is a surjective mapping from a skew-symmetry matrix A to a special orthogonal matrix U (i.e.,
det(U) = 1) with U = exp(A) = I + A + A2/2 + · · · , where the inﬁnite series is computed up
to machine-precision (Higham, 2009). To parameterize all orthogonal matrices, we introduce an
additional orthogonal matrix Q in U = Q exp(A), where Q is (randomly) generated at initialization
and ﬁxed during training (Lezcano Casado, 2019) .
Now we have an end-to-end pipeline for orthogonal convolutions as shown in Figure 2 (See Algorithm
1 in Appendix E for a pseudo implementation). Since our method relies on separability and complete
factorization for 1D paraunitary systems, we call it Separable Complete Factorization (SC-Fac).
Ortho. Conv.
Paraunitary
Ortho. Factors
Q ,{U
(l)}
H (z)
h[n]
A ,{A
(l)}
H (z)=∑h[n] z
−n
H (z)= f (Q ,{U
(l)})
Q=exp(A)
U
(l)=exp(A
(l))
Figure 2: SC-Fac: A pipeline for designing orthogonal convolutional layer. (1) An orthogonal
convolution h[n] is equivalent a paraunitary system H(z) in the spectral domain (Theorem 2.1). (2)
The paraunitary system H(z) is multiplications of factors characterized by (column-)orthogonal ma-
trices ({U (ℓ)}−1
ℓ=−L, Q, {U (ℓ)}L
ℓ=1) (Equation (2.2a), Theorem B.5). (3) These orthogonal matrices
are parameterized by skew-symmetric matrices using exponential map.
3
UNIFYING ORTHOGONAL CONVOLUTIONS VARIANTS
Various convolutional layers (strided, dilated, and group convolution) are widely used in neural
networks. However, it is not apparent how to enforce their orthogonality, as the convolution theorem
(Theorem 2.1) only holds for standard convolutions. Previous approaches only deal with standard
convolutions (Sedghi et al., 2019; Li et al., 2019b; Trockman & Kolter, 2021), thus orthogonality for
state-of-the-art architectures has never been studied before.
We address this limitation by modifying convolution theorem for each variant of convolution layer,
which allows us to design these variants using paraunitary systems.
Theorem 3.1 (Convolution and paraunitary theorems for various convolutions). Strided, dilated,
and group convolutions can be uniﬁed in the spectral domain as Y (z) = H(z)X(z), where Y (z),
H(z), X(z) are modiﬁed Z-transforms of y, h, x. We instantiate H(z) for strided convolutions in
Proposition C.4, dilated convolution in Proposition C.5, and group convolution in Proposition C.6.
Furthermore, a convolution is orthogonal if and only if H(z) is paraunitary.
4
Under review as a conference paper at ICLR 2022
0
1
2
3
4
5
……
0
1
2
3
4
5
6
7
8
9
10
x[n]
x
↑2[n]
n
n
(a) Up-sample
0
1
2
3
4
5
0
1
2
3
x[n]
x 0∣2[n]
n
n
6
7
……
0
1
2
3
x1∣2[n]
n
…
…
(b) Down-sample
Figure 3: Up and down sampling. In (a), the sequence x[n] is
up-sampled into x↑2[n]. In (b), x[n] is down-sampled into x0|2[n]
with even entries (red) and x1|2[n] with odd entries(blue).
In Table 1, we formulate strided, di-
lated, and group convolutions in the
spatial domain, interpreting them as
up-sampled or down-sampled variants
of a standard convolution. Now, we
introduce the concept of up-sampling
and down-sampling precisely below.
Given a sequence x, we introduce its
up-sampled sequence x↑R with sam-
pling rate R as x↑R[n] ≜x[n/R] for
n ≡0 (mod R). On the other hand,
its (r,R)-polyphase component xr|R
indicates the r-th down-sampled se-
quence with sampling rate R, deﬁned
as xr|R[n] ≜x[nR + r]. We illustrated an example of x↑R and xr|R in Figure 3 when sampling
rate R = 2. The Z-transforms of x↑R, xr|R are denoted as X↑R(z), Xr|R(z) respectively. Their
relations to X(z) are studied in Appendix C.1.
Table 1: Variants of convolutions. We present the modiﬁed Z-transforms, Y (z), H(z), and X(z) for each
convolution such that Y (z) = H(z)X(z) holds. In the table, X[R](z) ≜[X0|R(z)⊤, . . . , XR−1|R(z)⊤]⊤
and f
X[R](z) = [X−0|R(z), . . . , X−(R−1)|R(z)]. For group convolution, hg is the ﬁlter for the gth group with
Hg(z) being its Z-transform, and blkdiag (·) stacks multiple matrices into a block-diagonal matrix.
Convolution Type
Spatial Representation
Spectral Representation
Y (z)
H(z)
X(z)
Standard
y[i] = P
n∈Z h[n]x[i −n]
Y (z)
H(z)
X(z)
R-Dilated
y[i] = P
n∈Z h↑R[n]x[i −n]
Y (z)
H(zR)
X(z)
↓R-Strided
y[i] = P
n∈Z h[n]x[Ri −n]
Y (z)
f
H[R](z)
X[R](z)
↑R-Strided
y[i] = P
n∈Z h[n]x↑R[i −n]
Y [R](z)
H[R](z)
X(z)
G-Group
y[i] = P
n∈Z blkdiag ({hg[n]})x[i −n]
Y (z)
blkdiag ({Hg(z)})
X(z)
Now we are ready to interpret convolution variants. (1) Strided convolution is used to adjust the
feature resolution: a strided convolution (↓R-strided) decreases the resolution by down-sampling
after a standard convolution, while a transposed strided convolutional layer (↑R-strided) increases
the resolution by up-sampling before a standard convolution. (2) Dilated convolution increases the
receptive ﬁeld of a convolution without extra parameters: an R-dilated convolution up-samples its
ﬁlters before convolution with the input. (3) Group convolution reduces the parameters and compu-
tations, thus widely used by efﬁcient architectures: a G-group convolution divides the input/output
channels into G groups and restricts the connections within each group. In Appendix C.2, we prove
that a convolution is orthogonal if and only if its modiﬁed Z-transform H(z) is paraunitary.
4
LEARNING DEEP ORTHOGONAL NETWORKS WITH LIPSCHITZ BOUNDS
In this section, we switch our focus from layer design to network design. In particular, we aim to
study how to scale-up deep orthogonal networks with Lipschitz bounds.
Lipschitz networks (Anil et al., 2019; Li et al., 2019b; Trockman & Kolter, 2021), whose Lipschitz
bounds are imposed by their architectures, are proposed as competitive candidates to guarantee
robustness in deep learning. A Lipschitz network consists of orthogonal layers and GroupSort
activations (Anil et al., 2019) — both are 1-Lipschitz and gradient norm preserving. See Appendix D
for more discussions on the properties of GroupSort and Lipschitz networks. Given a Lipschitz
constant L, a Lipschitz network f can compute a certiﬁed radius for each input from its output margin.
Formally, denote the output margin of an input x with label c as
Mf(x) ≜max(0, f(x)c −max
i̸=c f(x)i),
(4.1)
i.e., the difference between the correct logit and the second largest logit. Then the output is robust to
perturbation such that f(x + ϵ) = f(x) = c, ∀ϵ : ∥ϵ∥< Mf(x)/
√
2L.
5
Under review as a conference paper at ICLR 2022
Despite the beneﬁt, existing architectures for Lipschitz networks remain simple and shallow, and a
Lipschitz network is typically an interleaving cascade of orthogonal layers and GroupSort activa-
tions (Li et al., 2019b). More advanced architectures, such as ResNet and ShufﬂeNet, are still out of
reach. While orthogonal layers supposedly substitute the role of batch normalization (Pennington
et al., 2017; Xiao et al., 2018; Qi et al., 2020), other critical factors, including skip-connections (He
et al., 2016a;b) and proper initialization (Glorot & Bengio, 2010) are lacking. In this section, we
explore skip-connections and initialization methods toward addressing this problem.
Skip-connections. Two general types of skip-connections are widely used in deep networks, one
based on addition and another on concatenation. The addition-based connection is proposed in
ResNet (He et al., 2016a), and adopted in SE-Net (Hu et al., 2018) and EffcientNet (Tan & Le,
2019). The concatenation-based connection is proposed in ﬂow-based generative models (Dinh
et al., 2014; 2016; Kingma & Dhariwal, 2018), and adopted in DenseNet (Huang et al., 2017)
and ShufﬂeNet (Zhang et al., 2018b; Ma et al., 2018). In what follows, we propose Lipschitz
skip-connections with these two mechanisms, illustrated in Figure 6 (in Appendix D).
Proposition 4.1 (Lipschitzness of residual blocks). Suppose f 1, f 2 are L-Lipschitz (for resid-
ual/shortcut branches) and α ∈[0, 1] is a learnable scalar, then an additive residual block
f : f(x) ≜αf 1(x)+(1−α)f 2(x) is L-Lipschitz. Alternatively, suppose g1, g2 are L-Lipschitz and
P is a channel permutation, then a concatenative residual block g : g(x) ≜P

g1(x1); g2(x2)

is L-
Lipschitz, where [·; ·] denotes channel concatenation, and x is split into x1 and x2, i.e., x = [x1, x2].
Initialization. Proper initialization is crucial in training deep networks (Glorot & Bengio, 2010;
He et al., 2016a). Various methods are proposed to initialize orthogonal matrices, including the
identical/permutation and torus initialization in the context of orthogonal RNNs (Henaff et al.,
2016; Helfrich et al., 2018; Lezcano-Casado & Martínez-Rubio, 2019). However, initialization of
orthogonal convolutions was not systematically studied, and all previous approaches inherit the
initialization from the underlying parameterization (Li et al., 2019b; Trockman & Kolter, 2021). In
Proposition D.1, we study the condition when a paraunitary system (represented in Equation (2.2a))
reduces to an orthogonal matrix. This reduction allows us to apply the initialization methods for
orthogonal matrices (e.g., uniform, torus) to orthogonal convolutions.
In the experiments, we will evaluate the impact of different choices of skip-connections and initializa-
tion methods to the performance of deep Lipschitz networks.
5
RELATED WORK
Dynamical isometry (Pennington et al., 2017; 2018; Chen et al., 2018; Pennington et al., 2018)
aims to address the gradient vanishing/exploding problems in deep vanilla networks with orthogonal
initialization. These works focus on understanding the interplay between initialization and various
nonlinear activations. However, these approaches do not guarantee Lipschitzness after training, thus
are unsuitable for applications that require strict characterization of Lipschitz constants, such as
adversarial robustness (Anil et al., 2019) and residual ﬂows (Behrmann et al., 2019).
Learning orthogonality has three typical approaches: regularization, parameterization (representing
the feasible set with unconstrained parameters), and projected gradient descent (PGD) / Riemannian
gradient descent (RGD). While regularization is approximate, the latter two learn exact orthog-
onality. (1) For orthogonal matrices, various regularizations are proposed in Xie et al. (2017);
Bansal et al. (2018). Alternatively, numerous parameterizations exist, including Householder reﬂec-
tions (Mhammedi et al., 2017), Given rotations (Dorobantu et al., 2016), Cayley transform (Helfrich
et al., 2018), matrix exponential (Lezcano-Casado & Martínez-Rubio, 2019), and algorithmic un-
rolling (Anil et al., 2019; Huang et al., 2020). Lastly, Jia et al. (2017) propose PGD via singular
value clipping, and Vorontsov et al. (2017); Li et al. (2019a) consider RGD. (2) For orthogonal
convolutions, some existing works learn orthogonality for the ﬂattened matrix (Jia et al., 2017; Cisse
et al., 2017; Bansal et al., 2018) or each output channel (Liu et al., 2021). However, these methods
do not lead to orthogonality (norm preserving) of the operation. Sedghi et al. (2019) propose to
use PGD via singular value clipping and masking, which is expensive and can result in inexact
orthogonality. Recent works adopt parameterizations, using block convolutions (Li et al., 2019b),
Cayley transform (Trockman & Kolter, 2021), or convolution exponential (Singla & Feizi, 2021).
Note that network deconvolution (Ye et al., 2020) aims to whiten the activations (i.e., orthogonalize
their distribution), but the added whitening operations do not necessarily preserve orthogonality.
6
Under review as a conference paper at ICLR 2022
Paraunitary systems are extensively studied in ﬁlter banks and wavelets (Vaidyanathan, 1993; Strang
& Nguyen, 1996; Lin & Vaidyanathan, 1996). Classic theory shows that 1D-paraunitary systems
are fully characterized by a spectral factorization (see Chapter 14 of Vaidyanathan (1993)), but not
all MD-paraunitary systems admit a factorized form (see Chapter 8 of Lin & Vaidyanathan (1996)).
While the complete characterization of MD-paraunitary systems is known in theory (which incurs a
difﬁcult problem of solving a system of nonlinear equations) (Venkataraman & Levy, 1995; Zhou,
2005), most practical constructions use separable paraunitary systems (Lin & Vaidyanathan, 1996)
and special classes of non-separable paraunitary systems (Hurley & Hurley, 2012). The equivalence
between orthogonal convolutions and paraunitary systems thus opens the opportunities to apply these
classic theories in designing orthogonal convolutions.
6
EXPERIMENTS
In the experiments, we achieve the following goals. (1) We demonstrate in Section 6.1 that our
separable complete factorization (SC-Fac) achieves precise orthogonality (up to machine-precision),
resulting in more accurate orthogonal designs than previous ones (Sedghi et al., 2019; Li et al., 2019b;
Trockman & Kolter, 2021). (2) Despite the differences in preciseness, we show in Section 6.2 that
different realizations of paraunitary systems only have a minor impact on the adversarial robustness
of Lipschitz networks. (3) Due to the versatility of our convolutional layers and architectures, in
Section 6.3, we explore the best strategy to scale Lipschitz networks to wider/deeper architectures.
(4) In Appendix F, we further demonstrate in a successful application of orthogonal convolutions in
residual ﬂows (Chen et al., 2019). Training details are provided in Appendix E.1.
6.1
EXACT ORTHOGONALITY
We evaluate the orthogonality of our SC-Fac layer verse previous approaches, including Cayley-
Conv (Trockman & Kolter, 2021), BCOP (Li et al., 2019b), SVCM (Sedghi et al., 2019), RKO (Cisse
et al., 2017), OSSN (Miyato et al., 2018). Our experiments are based on a convolutional layer with 64
input channels and 16 × 16 input size. We orthogonalize the layer using each approach, and evaluate
it with Gaussian inputs. For our SC-Fac layer, We initialize all orthogonal matrices uniformly, while
we use built-in initialization for others. We evaluate the difference between 1 and the ratio of the
output norm to the input norm — a layer is exactly orthogonal if the number is close to 0.
Table 2: (Left) Orthogonality evaluation of different designs for standard convolution. The number
∥Conv(x)∥/∥x∥−1 indicates the difference between the output and input norms of a layer. A layer is more
precisely orthogonal if the number is closer to 0. As shown, our SC-Fac achieves orders of magnitude more
orthogonal on standard convolution. (Right) Orthogonality evaluation of our SC-Fac design for various
convolutions. The numbers ∥Conv(x)∥/∥x∥−1 displayed are in the magnitude of 10−8. As shown, our
SC-Fac achieves machine epsilon orthogonality on variants of convolution.
Conv.
∥Conv(x)∥/∥x∥−1
SC-Fac
(+3.14 ± 7.38) × 10−8
CayleyConv
(+2.88 ± 1.90) × 10−4
BCOP
(+2.59 ± 6.14) × 10−3
SVCM
−0.429 ± 3.31 × 10−3
RKO
−0.666 ± 1.74 × 10−3
OSSN
−0.422 ± 3.44 × 10−3
Type
Groups
1
4
16
R-Dilated
1
+3.14 ± 7.38
+1.94 ± 6.87
+1.44 ± 6.29
2
+3.65 ± 7.87
+1.41 ± 6.77
+1.02 ± 6.46
4
+3.18 ± 7.46
+1.79 ± 6.87
+1.54 ± 6.21
↓R-Strided
2
−4.69 ± 5.10
+4.38 ± 6.30
+1.79 ± 5.78
4
+10.39±5.15
+6.35 ± 6.04
+3.05 ± 5.79
↑R-Strided
2
+3.67 ± 7.96
+1.38 ± 6.70
+1.43 ± 6.23
4
+3.86 ± 7.09
+1.12 ± 6.81
N/A
(1) Standard convolution. We show in Table 2 (Left) that our SC-Fac is orders of magnitude more
precise than all other approaches. The SC-Fac layer is in fact exactly orthogonal up to machine
epsilon, which is 2−24 ≈5.96 × 10−8 for 32-bits ﬂoats. While RKO and OSSN are known not
to be orthogonal, we surprisingly ﬁnd that SVCM is far from orthogonal due to its masking step.
(2) Convolutions variants. In Section 3, we show that various orthogonal convolutions can be
constructed using paraunitary systems. We verify our theory in Table 2 (Right): our SC-Fac layers
are exactly orthogonal (up to machine precision) for various convolutions.
6.2
ADVERSARIAL ROBUSTNESS
In this subsection, we evaluate the adversarial robustness of Lipschitz networks. Following the setup
in (Trockman & Kolter, 2021), we adopt KW-Large, ResNet9, WideResNet10-10 as the backbone
architectures, and evaluate their robust accuracy on CIFAR-10 with different designs of orthogonal
7
Under review as a conference paper at ICLR 2022
convolutions. We extensively perform a hyper-parameter search and choose the best hyper-parameters
for each approach based on the robust accuracy. The details of the hyper-parameter search is in
Appendix E. We run each model with 5 different seeds and report the best accuracy.
(1) Certiﬁed robustness. Following Li et al. (2019b), we use the raw images (without normalization)
for network input to achieve the best certiﬁed accuracy. As shown in Table 3 (Top), different
realizations of paraunitary systems, SC-Fac, CayleyConv and BCOP achieve comparable performance
— CayleyConv performs < 1% better in clean accuracy, but the difference in robust accuracy are
negligible. (2) Practical robustness. Trockman & Kolter (2021) shows that the certiﬁed accuracy is
too conservative, and it is possible to increase the practical robustness (against PGD attacks) with a
standard input normalization. Notice that the normalization increases the Lipschitz bound, thus lower
the certiﬁed accuracy. Our experiments in Table 3 (Bottom) are based on ResNet9, WideResNet10-
10 (Trockman & Kolter, 2021) and a deeper WideResNet22. For the shallow architectures (ResNet9,
WideResNet10-10), our SC-Fac, CayleyConv, and BCOP again achieve comparable performance —
CayleyConv is slightly ahead in robust accuracy. For the deeper architecture, our SC-Fac has a
clear advantage in both clean and robustness accuracy, and the clean accuracy to only 5% lower
than a traditional ResNet 32 trained with batch normalization. Surprisingly, we ﬁnd that RKO also
performs well in robust accuracy while not exactly orthogonal. In summary, our experiments show
that various paraunitary realizations provide different impacts on certiﬁed and practical robustness.
While exact orthogonality provides tight Lipschitz bound, there is a trade-off between the exact
orthogonality and the practical robustness (especially with the shallow architectures).
Table 3: (Top) Certiﬁed robustness for plain convolutional networks (without input normalization). We
use KW-Large introduced by Wong et al. (2018). The results for RKO, OSSN, and SVCM are produced by
Trockman & Kolter (2021). (Bottom) Practical robustness for residual networks (with input normalization).
For 22 layers, the width of SC-Fac is multiplied with 10, CayleyConv with 6, and BCOP and RKO with 8. We
are unable to scale CayleyConv, BCOP, and RKO due to memory constraint. As shown, deeper architectures
perform better than shallow ones for all orthogonal convolution types, and our SC-Fac has a clear advantage.
KW-Large
ϵ
Test Acc. SC-Fac CayleyConv BCOP RKO OSSN SVCM
0
Clean
74.69
75.57
74.81
74.47 71.69
72.43
36
255
Certiﬁed
58.68
59.03
58.83
57.50 55.71
52.11
PGD
67.72
67.78
67.47
68.32 65.13
66.43
ResNet9
WideResNet10-10
WideResNet22-max
ϵ
Test Acc. SC-Fac CayleyConv BCOP RKO SC-Fac CayleyConv BCOP RKO SC-Fac CayleyConv BCOP RKO
0
Clean
82.19
84.26
83.20 84.07
84.09
82.99
84.29 84.51
87.82
85.85
84.50 84.55
36
255
PGD
71.21
73.47
73.05 75.03
74.29
76.02
74.60 77.14
76.46
74.81
75.00 76.41
6.3
SCALING-UP DEEP ORTHOGONAL NETWORKS WITH LIPSCHITZ BOUNDS
All previously proposed Lipschitz networks (Li et al., 2019b; Trockman & Kolter, 2021) consider
only shallow architectures (≤10 layers). In this part, we investigate various factors to scale Lipschitz
networks to deeper architectures: skip-connection, depth-width, receptive ﬁeld, and down-sampling.
(1) Skip-connections. Conventional wisdom suggests that skip-connections mainly address the
gradient vanishing/exploding problem; thus, they are not needed for orthogonal networks. To
understand their role, we perform an experiment that trains deep Lipschitz networks without skip-
connection and with skip-connections based on addition/concatenation (see Section 4). As shown in
Table 4 (left), the network with additive skip-connection substantially outperforms the other two, and
the one without skip-connections performs the worst. Therefore, we empirically show that (additive)
skip-connection is crucial in deep Lipschitz networks. (2) Depth and width. Exact orthogonality
is criticized for harming the expressive power of neural networks. We show that the decrease of
expressive power can be alleviated by increasing the network depth/width. In Table 3 (Bottom)
and Table 7 (Appendix E), we observe that deeper/wider architectures increase both the clean and
robust accuracy. (3) Initialization methods. We try different initialization methods, including
identical, permutation, uniform, and torus (Henaff et al., 2016; Helfrich et al., 2018). We ﬁnd that
identical initialization works the best for deep Lipschitz networks (> 10 layers), while all methods
perform similarly in shallow networks as shown in Table 6 (Appendix E). (4) Receptive ﬁeld and
down-sampling. Previous works (Li et al., 2019b; Trockman & Kolter, 2021) use larger kernel
8
Under review as a conference paper at ICLR 2022
Table 4: (Left) Comparisons of various skip connection types on WideResNet22-10 (kernel size 5). (Right)
Comparisons of various receptive ﬁeld and down-sampling types on WideResNet10-10. The symbols ,
 indicate whether average pooling or strided convolution is used for down-sampling. For “slim” in strided
convolution, we set kernel_size = stride; and for for “wide”, kernel_size = stride * kernel_size’ (where kernel_
size’ is the kernel size for the main branch.
Skip type
Test Acc.
Clean
PGD
ConvNet (w/o skip)
69.59
59.22
ShufﬂeNet (concat)
75.21
66.00
ResNet (add)
87.82
76.46
Receptive Field
Down-Sampling
Test Acc.
Kernel
Dilation
Pool
Stride
Clean
PGD
3
1

slim
80.70
68.81
3
1

wide
82.36
70.36
3
1


84.54
71.71
3
2


81.53
70.07
5
1


84.09
74.29
5
2


81.28
70.58
size and no stride for Lipschitz networks. In Table 4 (Right), we perform a study on the effects
of kernel/dilation size and down-sampling types for the orthogonal convolutions. We ﬁnd that an
average pooling as down-sampling consistently outperforms strided convolutions. Furthermore, a
larger kernel size helps to boost the performance. (5) Run-time and memory comparison. We ﬁnd
that previously proposed orthogonal convolutions such as CayleyConv, BCOP, and RKO require
more GPU memory and computation time than SC-Fac. Therefore, we could not to scale them due
to memory constraints (for 22 and 32 layers using Tesla V100 32G). In order to scale up Lipschitz
networks, economical implementation of orthogonal convolution is crucial. As shown in Figure 4, for
deep and wide architectures, our SC-Fac is the most computationally and memory efﬁcient method
and the only method that scales to a width increase of 10 on WideResNet22. Missing numbers in
Figure 4 and Table 7 (Appendix E) are due to the large memory requirement.
1
3
6
8
10
0
100
200
300
400
500
Width
Train Time (s)
Normal
SC-Fac
Cayley
BCOP
RKO
1
3
6
8
10
0
5
10
15
20
25
30
Width
Inference Time (s)
1
3
6
8
10
0
5
10
15
Width
Memory (GB)
Figure 4: Run-time and memory comparison using WideResNet22 on Tesla V100 32G. x-axis indicates the
width factor (channels = base_channels × factor). Our SC-Fac is the most computationally and memory-efﬁcient
for wide architectures and is the only method that scales to width factor to 10 on WideResNet22. We also
compare with an ordinary network with regular convolutions and ReLU activations. Note that SC-Fac has the
same inference speed as a regular convolution — the overhead is from the GroupSort activations.
In summary, additive skip-connections are still essential for learning deep orthogonal networks. Due
to the orthogonal constraints, it is helpful to increase the depth/width of the network. However,
this signiﬁcantly increases the memory requirement; thus, a cheap implementation (like SC-Fac) is
desirable. Finally, we ﬁnd that a larger kernel size and down-sampling based on average pooling is
helpful, unlike standard practices in deep networks.
7
CONCLUSION
In this paper, we present a paraunitary framework for orthogonal convolutions. Speciﬁcally, we estab-
lish the equivalence between orthogonal convolutions in the spatial domain and paraunitary systems
in the spectral domain. Therefore, any design for orthogonal convolutions is implicitly constructing
paraunitary systems. We further show that the orthogonality for variants of convolution (strided,
dilated, and group convolutions) is also fully characterized by paraunitary systems. In summary,
paraunitary systems are all we need to ensure orthogonality for diverse types of convolutions.
Based on the complete factorization of 1D paraunitary systems, we develop the ﬁrst exact and
complete design of separable orthogonal 2D-convolutions. Our versatile design allows us to study
the design principles for orthogonal convolutional networks. Consequently, we scale orthogonal
networks to deeper architectures, substantially outperforming their shallower counterparts. In our
experiments, we observe that exact orthogonality plays a crucial role in learning deep Lipschitz
networks. In the future, we plan to investigate other use cases that exact orthogonality is essential.
9
Under review as a conference paper at ICLR 2022
ETHICS STATEMENT
Our work lies in the foundational research on neural information processing. Speciﬁcally, we establish
the equivalence between orthogonal convolutions in neural networks and paraunitary systems in signal
processing. Furthermore, our presented orthogonal convolutional layers are plug-and-play modules
that can replace various convolutional layers in neural networks. Consequently, our modules are
applicable in Lipschitz networks for adversarial robustness, recurrent networks for learning long-term
dependency, or ﬂow-based networks for effortless reversibility.
The vulnerability of neural networks raises concerns about their deployment in security-sensitive
scenarios, such as healthcare systems or self-driving cars. In our experiment, we demonstrate a
successful application of orthogonal convolutions in learning robust networks. Furthermore, these
networks achieve higher robust accuracy without additional techniques such as adversarial training or
randomized smoothing. Therefore, our research contributes to the robust learning of neural networks
and potentially leads to their broader deployment.
Furthermore, we reduce the inference cost of our layer to be the same as a traditional convolution
layer, signiﬁcantly lowering the expense for deployment compared with other methods for orthogonal
networks. However, our layers are memory and computationally more expensive during training than
traditional layers. The overhead to the already expensive training cost exacerbates the concerns on
the efﬁcacy of learning neural networks. Therefore, balancing between robustness and efﬁciency
is an important research topic that requires more research in the future. We develop more efﬁcient
implementation than previous approaches, narrowing the gap between these two conﬂicting goals.
REPRODUCIBILITY STATEMENT
We provide detailed proofs to our theoretical claims in Appendices B to D. We describe experimental
setups and training details of our models in Appendix E. We include our code for experiments in the
supplementary material.
REFERENCES
Cem Anil, James Lucas, and Roger Grosse. Sorting out lipschitz function approximation. In
International Conference on Machine Learning, pp. 291–301, 2019.
Aharon Azulay and Yair Weiss. Why do deep convolutional networks generalize so poorly to small
image transformations? Journal of Machine Learning Research, 20(184):1–25, 2019.
Nitin Bansal, Xiaohan Chen, and Zhangyang Wang. Can we gain more from orthogonality regular-
izations in training deep cnns? In Proceedings of the 32nd International Conference on Neural
Information Processing Systems, pp. 4266–4276. Curran Associates Inc., 2018.
Jens Behrmann, Will Grathwohl, Ricky TQ Chen, David Duvenaud, and Jörn-Henrik Jacobsen.
Invertible residual networks. In International Conference on Machine Learning, pp. 573–582.
PMLR, 2019.
Åke Björck and Clazett Bowie. An iterative algorithm for computing the best estimate of an
orthogonal matrix. SIAM Journal on Numerical Analysis, 8(2):358–364, 1971.
Minmin Chen, Jeffrey Pennington, and Samuel Schoenholz. Dynamical isometry and a mean ﬁeld
theory of rnns: Gating enables signal propagation in recurrent neural networks. In International
Conference on Machine Learning, pp. 873–882. PMLR, 2018.
Ricky TQ Chen, Jens Behrmann, David K Duvenaud, and Joern-Henrik Jacobsen. Residual ﬂows for
invertible generative modeling. Advances in Neural Information Processing Systems, 32, 2019.
Artem Chernodub and Dimitri Nowicki. Norm-preserving orthogonal permutation linear unit activa-
tion functions (oplu). arXiv preprint arXiv:1604.02313, 2016.
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier. Parseval
networks: Improving robustness to adversarial examples. In International Conference on Machine
Learning, pp. 854–863. PMLR, 2017.
10
Under review as a conference paper at ICLR 2022
Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components
estimation. arXiv preprint arXiv:1410.8516, 2014.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
preprint arXiv:1605.08803, 2016.
Victor Dorobantu, Per Andre Stromhaug, and Jess Renteria. Dizzyrnn: Reparameterizing recurrent
neural networks for norm-preserving backpropagation. arXiv preprint arXiv:1612.04035, 2016.
Xavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artiﬁcial intelligence and
statistics, pp. 249–256. JMLR Workshop and Conference Proceedings, 2010.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord:
Free-form continuous dynamics for scalable reversible generative models.
arXiv preprint
arXiv:1810.01367, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770–778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European Conference on Computer Vision, pp. 630–645. Springer, 2016b.
Kyle Helfrich, Devin Willmott, and Qiang Ye. Orthogonal recurrent neural networks with scaled
cayley transform. In International Conference on Machine Learning, pp. 1969–1978. PMLR,
2018.
Mikael Henaff, Arthur Szlam, and Yann LeCun. Recurrent orthogonal networks and long-memory
tasks. In International Conference on Machine Learning, pp. 2034–2042, 2016.
Nicholas J Higham. The scaling and squaring method for the matrix exponential revisited. SIAM
review, 51(4):747–764, 2009.
Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pp. 7132–7141, 2018.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In CVPR, volume 1, pp. 3, 2017.
Lei Huang, Li Liu, Fan Zhu, Diwen Wan, Zehuan Yuan, Bo Li, and Ling Shao. Controllable
orthogonalization in training dnns. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 6429–6438, 2020.
Barry Hurley and Ted Hurley. Paraunitary matrices. arXiv preprint arXiv:1205.0703, 2012.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning, pp. 448–456.
PMLR, 2015.
Kui Jia, Dacheng Tao, Shenghua Gao, and Xiangmin Xu. Improving training of deep neural networks
via singular value bounding. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 4344–4352, 2017.
Kui Jia, Shuai Li, Yuxin Wen, Tongliang Liu, and Dacheng Tao. Orthogonal deep neural networks.
IEEE transactions on pattern analysis and machine intelligence, 2019.
Li Jing, Yichen Shen, Tena Dubcek, John Peurifoy, Scott Skirlo, Yann LeCun, Max Tegmark, and
Marin Soljaˇ
ci´
c. Tunable efﬁcient unitary neural networks (eunn) and their application to rnns. In
International Conference on Machine Learning, pp. 1733–1741. PMLR, 2017.
11
Under review as a conference paper at ICLR 2022
Jaroslav Kautsky and Radka Turcajová. A matrix approach to discrete wavelets. In Wavelet Analysis
and Its Applications, volume 5, pp. 117–135. Elsevier, 1994.
Durk P Kingma and Prafulla Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolutions. In
Advances in neural information processing systems, pp. 10215–10224, 2018.
Ivan Kobyzev, Simon Prince, and Marcus Brubaker. Normalizing ﬂows: An introduction and review
of current methods. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.
Mario Lezcano Casado. Trivializations for gradient-based optimization on manifolds. Advances in
Neural Information Processing Systems, 32:9157–9168, 2019.
Mario Lezcano-Casado and Davidd Martínez-Rubio. Cheap orthogonal constraints in neural networks:
A simple parametrization of the orthogonal and unitary group. In International Conference on
Machine Learning, pp. 3794–3803, 2019.
Jun Li, Fuxin Li, and Sinisa Todorovic. Efﬁcient riemannian optimization on the stiefel manifold via
the cayley transform. In International Conference on Learning Representations, 2019a.
Qiyang Li, Saminul Haque, Cem Anil, James Lucas, Roger B Grosse, and Jörn-Henrik Jacobsen.
Preventing gradient attenuation in lipschitz constrained convolutional networks. In Advances in
neural information processing systems, pp. 15390–15402, 2019b.
Yuan-Pei Lin and PP Vaidyanathan. Theory and design of two-dimensional ﬁlter banks: A review.
Multidimensional Systems and Signal Processing, 7(3-4):263–330, 1996.
Sheng Liu, Xiao Li, Yuexiang Zhai, Chong You, Zhihui Zhu, Carlos Fernandez-Granda, and Qing
Qu. Convolutional normalization: Improving deep convolutional network robustness and training.
Advances in Neural Information Processing Systems, 34, 2021.
Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufﬂenet v2: Practical guidelines for
efﬁcient cnn architecture design. In Proceedings of the European conference on computer vision
(ECCV), pp. 116–131, 2018.
Kehelwala DG Maduranga, Kyle E Helfrich, and Qiang Ye. Complex unitary recurrent neural
networks using scaled cayley transform. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, volume 33, pp. 4528–4535, 2019.
Alexander Mathiasen, Frederik Hvilshøj, Jakob Rødsgaard Jørgensen, Anshul Nasery, and Davide
Mottin. What if neural networks had svds? arXiv preprint arXiv:2009.13977, 2020.
Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey. Efﬁcient orthogonal
parametrisation of recurrent neural networks using householder reﬂections. In Proceedings of the
34th International Conference on Machine Learning-Volume 70, pp. 2401–2409, 2017.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. In International Conference on Learning Representations, 2018.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to spectrally-
normalized margin bounds for neural networks. In International Conference on Learning Repre-
sentations, 2018.
Alan V. Oppenheim, Alan S. Willsky, and S. Hamid Nawab. Signals & Systems (2nd Ed.). Prentice-
Hall, Inc., 1996.
George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji
Lakshminarayanan. Normalizing ﬂows for probabilistic modeling and inference. arXiv preprint
arXiv:1912.02762, 2019.
Jeffrey Pennington, Samuel S Schoenholz, and Surya Ganguli. Resurrecting the sigmoid in deep
learning through dynamical isometry: theory and practice. In Proceedings of the 31st International
Conference on Neural Information Processing Systems, pp. 4788–4798, 2017.
12
Under review as a conference paper at ICLR 2022
Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli. The emergence of spectral universality in
deep networks. In International Conference on Artiﬁcial Intelligence and Statistics, pp. 1924–1932.
PMLR, 2018.
Haozhi Qi, Chong You, Xiaolong Wang, Yi Ma, and Jitendra Malik. Deep isometric learning for
visual recognition. In International Conference on Machine Learning, pp. 7824–7835. PMLR,
2020.
Hanie Sedghi, Vineet Gupta, and Philip M Long. The singular values of convolutional layers. In
International Conference on Learning Representations, 2019.
Sahil Singla and Soheil Feizi. Skew orthogonal convolutions. arXiv preprint arXiv:2105.11417,
2021.
Gilbert Strang and Truong Nguyen. Wavelets and ﬁlter banks. SIAM, 1996.
Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks.
In International Conference on Machine Learning, pp. 6105–6114. PMLR, 2019.
Asher Trockman and J Zico Kolter. Orthogonalizing convolutional layers with the cayley transform.
In International Conference on Learning Representations, 2021.
PP Vaidyanathan. Multirate systems and ﬁlter banks. Prentice-Hall, Inc., 1993.
Rianne Van Den Berg, Leonard Hasenclever, Jakub M Tomczak, and Max Welling. Sylvester
normalizing ﬂows for variational inference. In 34th Conference on Uncertainty in Artiﬁcial
Intelligence 2018, UAI 2018, pp. 393–402. Association For Uncertainty in Artiﬁcial Intelligence
(AUAI), 2018.
Shankar Venkataraman and Bernard C Levy. A comparison of design methods for 2-d ﬁr orthogonal
perfect reconstruction ﬁlter banks. IEEE Transactions on Circuits and Systems II: Analog and
Digital Signal Processing, 42(8):525–536, 1995.
Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality and learning
recurrent networks with long term dependencies. In International Conference on Machine Learning,
pp. 3570–3578, 2017.
Jiayun Wang, Yubei Chen, Rudrasis Chakraborty, and Stella X Yu. Orthogonal convolutional neural
networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.
Eric Wong, Frank R Schmidt, Jan Hendrik Metzen, and J Zico Kolter. Scaling provable adversarial
defenses. In Proceedings of the 32nd International Conference on Neural Information Processing
Systems, pp. 8410–8419, 2018.
Eric Wong, Leslie Rice, and J Zico Kolter. Fast is better than free: Revisiting adversarial training. In
International Conference on Learning Representations, 2020.
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Pennington.
Dynamical isometry and a mean ﬁeld theory of cnns: How to train 10,000-layer vanilla convolu-
tional neural networks. In International Conference on Machine Learning, pp. 5393–5402. PMLR,
2018.
Di Xie, Jiang Xiong, and Shiliang Pu. All you need is beyond a good init: Exploring better solution
for training extremely deep convolutional neural networks with orthonormality and modulation. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6176–6185,
2017.
Chengxi Ye, Matthew Evanusa, Hua He, Anton Mitrokhin, Tom Goldstein, James A Yorke, Cornelia
Fermuller, and Yiannis Aloimonos. Network deconvolution. In International Conference on
Learning Representations, 2020.
Jiong Zhang, Qi Lei, and Inderjit Dhillon. Stabilizing gradients for deep neural networks via efﬁcient
svd parameterization. In International Conference on Machine Learning, pp. 5806–5814, 2018a.
13
Under review as a conference paper at ICLR 2022
Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufﬂenet: An extremely efﬁcient
convolutional neural network for mobile devices. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 6848–6856, 2018b.
Jianping Zhou. Multidimensional Multirate Systems: Characterization, Design, and Applications.
Citeseer, 2005.
14
Under review as a conference paper at ICLR 2022
Appendices: Scaling-up Diverse Orthogonal Convolutional
Networks by a Paraunitary Framework
Notations. We use non-bold letters for scalar (e.g., x) and bold ones for vectors or matrices (e.g., x).
We denote sequences in the spatial domain using lower-case letters (e.g., x[n], x[n]) and their spectral
representations using upper-case letters (e.g., X(z), X(z)). For a positive integer, say R ∈Z+, we
abbreviate the set {0, 1, · · · , R −1} as [R], and whenever possible, we use its lower-case letter, say
r ∈[R], as the corresponding iterator.
Assumptions. For simplicity, we assume all sequences are L2 with range Z = {0, ±1, ±2, · · · } (a
sequence x = {x[n], n ∈Z} is L2 if P
n∈Z ∥x[n]∥2 < ∞). Such assumption is common in the
literature, which avoids boundary conditions in signal analysis. To deal with periodic sequences
(ﬁnite sequences with circular padding), one can either adopt the Dirac function in the spectral domain
or use discrete Fourier transform to compute the spectral representations. In our implementation, we
address the boundary condition case by case for each convolution type, with which we achieve exact
orthogonality in the experiments (Section 6.1).
A
PSEUDO CODE FOR OUR SC-FAC ALGORITHM
We include the pseudo-code for separable complete factorization (Section 2) in Algorithm 1 and
diverse orthogonal convolutions (Section 3) in Algorithm 2. The pseudo-code in Algorithm 1 consists
of three parts: (1) First, we obtain orthogonal matrices from skew-symmetric matrices using matrix
exponential. We use GeoTorch library (Lezcano Casado, 2019) for the function matrix_exp in our
implementation; (2) Subsequently, we construct two 1D paraunitary systems using these orthogonal
matrices; (3) Lastly, we compose two 1D paraunitary systems to obtain one 2D paraunitary systems
The pseudo-code in Algorithm 2 consists of two parts: (1) First, we reshape each paraunitary system
into an orthogonal convolution depending on the stride; and (5) second, we concatenate the orthogonal
kernels for different groups and return the output.
B
ORTHOGONAL CONVOLUTIONS VIA PARAUNITARY SYSTEMS
In this section, we prove the convolution theorem and Parseval’s theorem for standard convolutional
layers. Subsequently, we prove the paraunitary theorem which establishes the equivalence between
orthogonal convolutional layers and paraunitary systems.
B.1
SPECTRAL ANALYSIS OF STANDARD CONVOLUTION LAYERS
Standard convolutional layers are the default building blocks for convolutional neural networks.
One such layer consists of a ﬁlter bank with T × S ﬁlters h = {hts[n], n ∈Z}t∈[T ],s∈[S], where
S, T are the number of input and output channels respectively. The layer maps an S-channel input
x = {xs[i], i ∈Z}s∈[S] to a T-channel output y = {yt[i], i ∈Z}t∈[T ] according to
yt[i] =
X
s∈[S]
X
n∈Z
hts[n]xs[i −n],
(B.1)
where i indexes the output location to be computed, and n indexes the ﬁlter coefﬁcients. Alternatively,
we can rewrite Equation (B.1) in matrix-vector form as
y[i] =
X
n∈Z
h[n]x[i −n],
(B.2)
where each h[n] ∈RT ×S is a matrix, and each x[i −n] ∈RS or y[i] ∈RT is a vector.
Notice that in Equation (B.2), we group entries from all channels into a vector or matrix (e.g.,
from {x0[n]}s∈[S] to x[n]), different from a common notation that groups entries from all locations
into a vector or matrix (e.g., from {xs[n], n ∈Z} into xs). In the matrix-vector form, a standard
convolutional layer computes a vector sequence y = {y[i] ∈RT , i ∈Z} with a convolution between
a matrix sequence h = {h[n] ∈RT ×S, n ∈Z} and a vector sequence x = {x[i] ∈RS, i ∈Z}.
15
Under review as a conference paper at ICLR 2022
Algorithm 1: Separable Complete Factorization (SC-Fac)
Input: Number of channels C, kernel size K = 2L + 1, and
Skew-symmetric matrices {A(ℓ)
d } with A(ℓ)
d
∈RC×C, ∀ℓ∈[−L, L], d ∈{1, 2}.
Output: A paraunitary system H ∈RC×C×K×K.
Initialization: Sample N (ℓ)
d
from {1, · · · , C} uniformly ∀ℓ∈[−L, L], d ∈{1, 2}
/* Iterate for vertical/horizontal dimensions
*/
for d = 1 to 2 do
/* 1) Compute orthogonal matrices from skew-symmetric matrices
*/
/* Iterate for filter locations
*/
for ℓ= −L to L do
if ℓ= 0 then
Qd ←matrix_exp(A(0)
d ) // use matrix_exp() in GeoTorch
(Lezcano Casado, 2019)
else
U (ℓ)
d
←select(matrix_exp(A(ℓ)
d ), cols = N (ℓ)
d ) // selects the first cols
columns of the matrix
end if
end for
/* 2) Compose 1D paraunitary systems from orthogonal matrices
*/
Hd ←Qd
for ℓ= 1 to L do
Hd ←conv1d(Hd,

U (ℓ)
d U (ℓ)
d
⊤, I −U (ℓ)
d U (ℓ)
d
⊤
)
Hd ←conv1d(

I −U (−ℓ)
d
U (−ℓ)
d
⊤, U (−ℓ)
d
U (−ℓ)
d
⊤
, Hd)
end for
end for
/* 3) Compose a 2D paraunitary systems from two 1D paraunitary
*/
H ←Compose(H1, H2) // i.e., H:,:,i,j = (H2):,:,j(H1):,:,i where the 1D
paraunitary systems H1 and H2 are of size C × C × K
return H
Algorithm 2: Construct Diverse Orthogonal Convolutions from Paraunitary Systems
Input: Number of base channels C, kernel size K = R(2L + 1),
stride R, dilation D, number of groups G
Output: An orthogonal kernel W ∈RT ×S×K×K
Set K′ ←K/R, number of input channels S ←GC/R2 and output channels T ←GC
for g = 0 to G −1 do
/* 1) Construct orthogonal convolutions from paraunitary systems
*/
Initialize skew-symmetric matrices {{A(ℓ,g)
d
}L
ℓ=−L}2
d=1 for the current g
Hg ←Algorithm 1: SC-Fac(C, K′, {{A(ℓ,g)
d
}L
ℓ=−L}2
d=1)
Hg ←reshape(Hg, (C, C, K′, K′) →(C/R2, C, K, K))
end for
/* 2) Concatenate orthogonal convolutions from different groups
*/
W ←concatenate({Hg}G−1
g=0 , dim = 0)
return W (where the ﬁlter for input channel s and output channel t is Wt,s,:,: ∈RK×K)
Let us ﬁrst deﬁne the Z-transform and various types of Fourier transform in Deﬁnition B.1 before
proving the convolution theorem (Theorem 2.1).
16
Under review as a conference paper at ICLR 2022
Deﬁnition B.1 (Z-transform and Fourier transforms). For a sequence (of scalars, vectors, or matrices)
x = {x[n], n ∈Z}, its Z-transform X(z) is deﬁned as
X(z) =
X
n∈Z
x[n]z−n,
(B.3)
where z ∈C is a complex number such that the inﬁnite sum is convergent. If z is restricted to the
unit circle z = ejω (i.e., |z| = 1), the z-transform X(z) reduces to a discrete-time Fourier transform
(DTFT) X(ejω). If ω is further restricted to a ﬁnite set ω ∈{2πk/N, k ∈[N]}, the DTFT X(ejω)
reduces to an N-points discrete Fourier transform (DFT) X(ej2πk/N).
The celebrated convolution theorem states that the convolution in spatial domain (Equation (B.2)) is
equivalent to a multiplication in the spectral domain, i.e., Y (z) = H(z)X(z), ∀z ∈C.
Proof of Theorem 2.1. The proof follows directly from the deﬁnitions of standard convolution (Equa-
tion (B.2)) and Z-transform (Equation (B.3)).
Y (z) =
X
i∈Z
y[i]z−i
(B.4)
=
X
i∈Z
 X
n∈Z
h[n]x[i −n]
!
z−i
(B.5)
=
X
n∈Z
h[n]z−n
 X
i∈Z
x[i −n]z−(i−n)
!
(B.6)
=
 X
n∈Z
h[n]z−n
!  X
k∈Z
x[k]z−k
!
(B.7)
= H(z)X(z),
(B.8)
where Equations (B.4) and (B.8) use the deﬁnition of Z-transform, Equation (B.5) uses the deﬁnition
of convolution, and Equation (B.7) makes a change of variable k = i −n.
Next, we introduce the concepts of inner product and Frobenius norm for sequences. We then
prove Parseval’s theorem, which allows us to compute the sequence norm in the spectral domain.
Deﬁnition B.2 (Inner product and norm for sequences). Given two sequences x = {x[n], n ∈Z}
and y = {y[n], n ∈Z} with x[n], y[n] having the same dimension for all n, the inner product of
these two sequences is deﬁned as
⟨x, y⟩≜
X
n∈Z
⟨x[n], y[n]⟩
(B.9)
where ⟨x[n], y[n]⟩denotes the Frobenius inner product between x[n] and y[n]. Subsequently, we
can deﬁne the Frobenius norm of a sequence using inner product as
∥x∥≜
p
⟨x, x⟩
(B.10)
Theorem B.3 (Parsavel’s theorem). Given a sequence x = {x[n], n ∈Z}, its sequence norm ∥x∥
can be computed by X(ejω) in the spectral domain as
∥x∥2 =
X
n∈Z
∥x[n]∥2 = 1
2π
Z π
−π
∥X(ejω)∥2dω,
(B.11)
where ∥X(ejω)∥2 = X(ejω)†X(ejω) is an inner product between two complex arrays.
17
Under review as a conference paper at ICLR 2022
Proof of Theorem B.3. The theorem follows from the deﬁnitions of convolution and discrete-time
Fourier transform (DTFT).
1
2π
Z π
−π

X(ejω)

2 dω = 1
2π
Z π
−π

X(ejω), X(ejω)

dω
(B.12)
= 1
2π
Z π
−π
*X
n∈Z
x[n]e−jωn,
X
m∈Z
x[m]e−jωm
+
dω
(B.13)
=
X
n∈Z
X
m∈Z
⟨x[n], x[m]⟩1
2π
Z π
−π
e−jω(m−n)dω
(B.14)
=
X
n∈Z
X
m∈Z
⟨x[n], x[m]⟩1m=n
(B.15)
=
X
n∈Z
⟨x[n], x[n]⟩=
X
n∈Z
∥x[n]∥2 ,
(B.16)
where Equation (B.14) is due to the bi-linearity of inner products, and Equation (B.15) makes uses of
the fact that
R π
−π e−jωkdω = 0 for k ̸= 0 and
R π
−π e−jωkdω =
R π
−π dω = 2π for k = 0.
B.2
EQUIVALENCE BETWEEN ORTHOGONAL CONVOLUTIONS AND PARAUNITARY SYSTEMS
With the sequence norm introduced earlier, we formally deﬁne orthogonality for convolutional layers.
Deﬁnition B.4 (Orthogonal convolutional layer). A convolution layer is orthogonal if the input norm
∥x∥is equal to the output norm ∥y∥for arbitrary input x, that is
∥y∥≜
sX
n∈Z
∥y[n]∥2 =
sX
n∈Z
∥x[n]∥2 ≜∥x∥,
(B.17)
where ∥x∥(or ∥y∥) is deﬁned as the squared root of P
n∈Z ∥x[n]∥2 (or P
n∈Z ∥y[n]∥2).
This deﬁnition of orthogonality not only applies to standard convolutions in Equation (B.2) but also
variants of convolutions in Appendix C.3. In this section, however, we ﬁrst establish the equivalence
between orthogonality for standard convolutions and paraunitary systems.
Theorem B.5 (Paraunitary theorem). A standard convolutional layer (in Equation (B.2)) is orthogo-
nal (by Deﬁnition B.4) if and only if its transfer matrix H(z) is paraunitary, i.e.,
H(z)†H(z) = I, ∀|z| = 1 ⇐
⇒H(ejω)
†H(ejω) = I, ∀ω ∈R.
(B.18)
In other words, the transfer matrix H(ejω) is unitary for all frequencies ω ∈R.
Proof of Theorem B.5. We ﬁrst prove that a convolutional layer is orthogonal if its transfer matrix
H(z) is paraunitary (i.e., H(ejω) is unitary for any frequency ω ∈R).
∥y∥2 = 1
2π
Z π
−π

Y (ejω)

2 dω
(B.19)
= 1
2π
Z π
−π

H(ejω)X(ejω)

2 dω
(B.20)
= 1
2π
Z π
−π

X(ejω)

2 dω
(B.21)
= ∥x∥2
(B.22)
where Equations (B.19) and (B.22) are due to Parseval’s theorem (Theorem B.3), Equations (B.19)
and (B.20) follows from the convolution theorem (Theorem 2.1), and Equation (B.21) utilizes that
H(ejω) is unitary for any frequency ω ∈R (thus ∥H(ejω)X(ejω)∥= ∥X(ejω)∥for any X(ejω)).
The ‘only if’ part also holds in practice. We here prove by contradiction using periodic inputs (e.g.,
ﬁnite inputs with circular padding). Suppose there exists a frequency ω and H(ejω) is not unitary.
18
Under review as a conference paper at ICLR 2022
Since H(ejω) is continuous (due to h ∈L2 and dominated convergence theorem), there exist integers
N, k, such that ω ≈2kπ/N and H(ej2πk/N) is also not unitary. As a result, there exists a complex
vector u such that v = H(ej2πk/N)u while ∥v∦= ∥u∥. Therefore, we can construct two periodic
sequences x = {x[n], n ∈[N]} and y = {y[n], n ∈[N]} such that
x[n] = uej2πnk/N =
⇒y[n] = vej2πnk/N.
(B.23)
Now the input norm ∥x∥=
qP
n∈[N] ∥x[n]∥2 =
√
N∥u∥is not equal to the output norm ∥y∥=
qP
n∈[N] ∥y[n]∥2 =
√
N∥v∥, i.e., the layer is not orthogonal, which leads to a contradiction.
C
A PARAUNITARY FRAMEWORK FOR ORTHOGONAL CONVOLUTIONS
Orthogonal
Conv. Layers
Paraunitary
Systems
Orthogonal
Matrices
Unconstrained
Parameters
●
Standard Conv.
●
Dilated Conv.
●
Strided Conv.
●
Group Conv.
●
Mat. Exp. 
●
Cayley
●
Bjȍrck
●
…
●
Separable
●
BCOP
Spectral 
Transformation
Spectral
Factorization
Parameterization
Section B.2
Section B.3
Section B.4
Parameterization
●
CayleyConv
●
SOC
Figure 5: A framework for designing orthogonal convolutional layers. In Appendix C.2, we unify
variants of orthogonal convolutions in the spectral domain and show that their designs reduce to con-
structing paraunitary systems. In Appendix C.3, we show that a paraunitary system can be constructed
with different approaches: our approach and BCOP (Li et al., 2019b) represent the paraunitary using
orthogonal matrices, while CayleyConv (Trockman & Kolter, 2021) and SOC (Singla & Feizi, 2021)
directly parameterizes it using unconstrained parameters. In Appendix C.4, we investigate various
parameterizations for orthogonal matrices, such as matrix exponential, Cayley transform, and Björck
orthogonalization.
C.1
MULTI-RESOLUTION ANALYSIS
Multi-resolution operations are essential in various convolutional layers, in particular strided and
dilated convolutions. In order to deﬁne and analyze these convolutions rigorously, we ﬁrst review the
concepts of up-sampling, down-sampling, and polyphase components.
(1) Up-sampling. Given a sequence (of scalars, vectors, matrices) x = {x[n], n ∈Z}, its up-
sampled sequence x↑R = {x↑R[n], n ∈Z} is deﬁned as
x↑R[n] ≜
x[n/R]
n ≡0 (mod R)
0
otherwise
,
(C.1)
where R ∈Z+ is the up-sampling rate. Accordingly, we denote the Z-transform of x↑R as
X↑R(z) =
X
n∈Z
x↑R[n]z−n.
(C.2)
The following proposition shows that X↑R(z) is easily computed from X(z).
Proposition C.1 (Z-transform of up-sampled sequence). Given a sequence x and its up-sampled
sequence x↑R, their Z-transforms X(z) and X↑R(z) are related by
X↑R(z) = X(zR).
(C.3)
19
Under review as a conference paper at ICLR 2022
Proof of Proposition C.1. The proof makes use of the deﬁnition of Z-transform (Equation (B.3)).
X↑R(z) =
X
n∈Z
x↑R[n]z−n
(C.4)
=
X
m∈Z
x↑R[mR]z−mR
(C.5)
=
X
m∈Z
x[m](zR)−m = X(zR),
(C.6)
where Equation (C.5) makes a change of variables m = n/R since x↑R[n] = 0, ∀n ̸= mR.
(2) Down-sampling and polyphase components. Different from the up-sampled sequence, there
exist multiple down-sampled sequences, depending on the phase of down-sampling. These sequences
are known as the polyphase components. Speciﬁcally, given a sequence (of scalars, vectors, or
matrices) x = {x[n], n ∈Z}, its rth polyphase component xr|R = {xr|R[n], n ∈Z} is deﬁned as
xr|R[n] ≜x[nR + r],
(C.7)
where R ∈Z+ is the down-sampling rate. We further denote the Z-transform of xr|R as
Xr|R(z) =
X
n∈Z
xr|R[n]z−n,
(C.8)
Note that r ∈Z is an arbitrary integer, which does not necessarily take values from [R]. In fact, we
have x(r+kR)|R[n] = xr|R[n + k] and Xr+kR|R(z) = zkXr|R(z). In Proposition C.2, we establish
the relation between H(z) and {Hr|R(z)}r∈[R], i.e., to represent H(z) in terms of {Hr|R(z)}r∈[R].
Proposition C.2 (Polyphase decomposition). Given a sequence x and its polyphase components
xr|R’s, the Z-transform X(z) can be represented by {Xr|R(z)}r∈[R] as
X(z) =
X
r∈[R]
Xr|R(zR)z−r.
(C.9)
Proof of Proposition C.2. We start with X(z), and try to decompose it into its polyphase components
X0|R(z), · · · , XR−1|R(z).
X(z) =
X
n∈Z
x[n]z−n
(C.10)
=
X
r∈[R]
X
m∈Z
x[mR + r]z−(mR+r)
(C.11)
=
X
r∈[R]
 X
m∈Z
x[mR + r]z−mR
!
z−r
(C.12)
=
X
r∈[R]
 X
m∈Z
xr|R[m](zR)−m
!
z−r
(C.13)
=
X
r∈[R]
Xr|R(zR)z−r,
(C.14)
where Equation (C.11) makes a change of variables n = mR+r, and Equation (C.13) is the deﬁnition
of polyphase components xr|R[m] = x[mR + r].
For simplicity, we stack R consecutive polyphase components into polyphase matrices as
X[R](z) =



X0|R(z)
.
.
.
XR−1|R(z)


,
f
X[R](z) =
h
X−0|R(z); · · · ; X−(R−1)|R(z)
i
.
(C.15)
The following proposition extends the Parseval’s theorem in Theorem B.3 and shows that the sequence
norm ∥x∥can also be computed in terms of the polyphase matrix X[R](z) (or f
X[R](z)).
20
Under review as a conference paper at ICLR 2022
Proposition C.3 (Parseval’s theorem for polyphase matrices). Given a sequence x, its sequence
norm ∥x∥can be computed by X[R](ejω) (or f
X[R](ejω)) in the spectral domain as
∥x∥2 = 1
2π
Z π
−π


X[R](ejω)



2
dω = 1
2π
Z π
−π


f
X[R](ejω)



2
dω.
(C.16)
Proof of Proposition C.3. The proof follows the standard Parseval’s theorem in Theorem B.3. We
only prove the ﬁrst part of the proposition (using X[R](ejω)) as follows.
∥x∥2 =
X
n∈Z
∥x[n]∥2
(C.17)
=
X
r∈[R]
X
m∈Z
∥x[mR + r]∥2
(C.18)
=
X
r∈[R]
X
m∈Z


xr|R[m]



2
(C.19)
= 1
2π
X
r∈[R]
Z π
−π


Xr|R(ejω)



2
dω
(C.20)
= 1
2π
Z π
−π


X[R](ejω)



2
dω,
(C.21)
where Equation (C.17) follows the deﬁnition of sequence norm, Equation (C.18) changes variables
as n = mR + r, Equation (C.19) is the deﬁnition of polyphase components, and Equation (C.20)
applies Parseval’s theorem to xr|R’s. The second part (using f
X[R](ejω)) can be proved similarly.
C.2
UNIFYING VARIOUS CONVOLUTIONAL LAYERS IN THE SPECTRAL DOMAIN
In Appendix B, the convolution theorem states that a standard convolutional layer is a matrix-vector
product Y (z) = H(z)X(z) in the spectral domain, and the layer is orthogonal if and only if H(z)
is paraunitary (Theorem B.5). However, the canonical convolution theorem does not hold for variants
of convolutions, thus enforcing a paraunitary H(z) may not lead to orthogonal convolution. In this
subsection, we address this limitation by showing that various convolutions can be uniformly written
as Y (z) = H(z)X(z), where Y (z), H(z), X(z) are some spectral representations of y, h, x.
Subsequently, we prove that any of these layers is orthogonal if and only if its H(z) is paraunitary.
(1) Strided convolutional layers are widely used in neural networks to adjust the feature resolution:
a strided convolution layer decreases the resolution by down-sampling after a standard convolution,
while a transposed convolution increases the resolution by up-sampling before a standard convolution.
Formally, a strided convolutional layer with stride R (abbrev. as ↓R-strided convolution) computes
its output following
y[i] =
X
n∈Z
h[n]x[Ri −n].
(C.22)
In contrast, a transposed strided convolutional layer with stride R (abbrev. as ↑R-strided convolution)
computes its output according to
y[i] =
X
n∈Z
h[n]x↑R[i −n].
(C.23)
Proposition C.4 (Orthogonality of strided convolutional layers). For a ↓R-strided convolution, the
spatial convolution in Equation (C.22) leads to the following spectral representation:
Y (z) = f
H[R](z)X[R](z)
(C.24)
And for an ↑R-strided convolution, the spatial convolution is represented in spectral domain as:
Y [R](z) = H[R](z)X(z)
(C.25)
Furthermore, a ↓R-strided convolution is orthogonal if and only if f
H[R](z) is paraunitary, and an
↑R-strided convolution is orthogonal if and only if H[R][z] is paraunitary.
21
Under review as a conference paper at ICLR 2022
Proof of Proposition C.4. (1a) ↓R-strided convolutions. We ﬁrst prove the spectral representation
of ↓R-strided convolution in Equation (C.24).
Y (z) =
X
i∈Z
y[i]z−i =
X
i∈Z
 X
n∈Z
h[n]x[Ri −n]
!
z−i
(C.26)
=
X
i∈Z

X
r∈[R]
X
m∈Z
h[mR −r]x[(i −m)R + r]

z−i
(C.27)
=
X
r∈[R]
 X
m∈Z
h[mR −r]z−m
 X
i
x[(i −m)R + r]z−(i−m)
!!
(C.28)
=
X
r∈[R]
 X
m∈Z
h[mR −r]z−m
!  X
i′∈Z
x[i′R + r]z−i′
!
(C.29)
=
X
r∈[R]
H−r|R(z)Xr|R(z),
(C.30)
where Equation (C.26) follows from the deﬁnitions of the ↓R-strided convolution (Equation (C.22))
and the Z-transform (Equation (B.3)), Equation (C.27) makes a change of variables n = mR −r,
Equation (C.29) further changes i′ = i −m, and Equation (C.30) is due to the deﬁnition of polyphase
components (Equation (C.7)). Now We rewrite the last equation concisely as
Y (z) =

H−0|R(z); · · · ; H−(R−1)(z)

|
{z
}
f
H[R](z)



X0|R(z)
.
.
.
XR−1|R(z)



|
{z
}
X[R](z)
,
(C.31)
which is the spectral representation of ↓R-strided convolutions in Equation (C.24).
Now we prove the orthogonality condition for ↓R-strided convolutions.
∥y∥2 = 1
2π
Z π
−π

Y (ejω)

2 dω
(C.32)
= 1
2π
Z π
−π


f
H[R](ejω)X[R](ejω)



2
dω
(C.33)
= 1
2π
Z π
−π


X[R](ejω)



2
dω
(C.34)
= ∥x∥2 ,
(C.35)
where Equations (C.32) and (C.35) are due to Parseval’s theorems (Theorem B.3 and Proposition C.3),
Equation (C.33) follows from the spectral representation of the ↓R-strided convolution (Equa-
tion (C.24)), and Equation (C.34) utilizes that the transfer matrix is unitary at each frequency. The
“only if” part can be proved by contradiction similar to Theorem B.5.
(1b) ↑R-strided convolutions. According to Proposition C.1, the Z-transform of x↑R is X(zR).
Therefore, an application of the convolution theorem (Theorem 2.1) on Equation (C.23) leads us to
Y (z) = H(z)X(zR)
(C.36)
Expanding Y (z) and H(z) using polyphase decomposition (Proposition C.2), we have
X
r∈[R]
Y r|R(zR)z−r =

X
r∈[R]
Hr|R(zR)z−r

X(zR)
(C.37)
X
r∈[R]
Y r|R(zR)z−r =
X
r∈[R]

Hr|R(zR)X(zR)

z−r
(C.38)
Y r|R(zR) = Hr|R(zR)X(zR), ∀r ∈[R]
(C.39)
Y r|R(z) = Hr|R(z)X(z), ∀r ∈[R],
(C.40)
22
Under review as a conference paper at ICLR 2022
where Equation (C.39) is due to the uniqueness of Z-transform, and Equation (C.40) changes the
variables from zR to z. Again„we can rewrite the last equation in concisely as





Y 0|R(z)
Y 1|R(z)
.
.
.
Y R−1|R(z)





|
{z
}
Y [R](z)
=





H0|R(z)
H1|R(z)
.
.
.
HR−1|R(z)





|
{z
}
H[R](z)
X(z),
(C.41)
which is the spectral representation of ↑R-strided convolutions in Equation (C.25).
Lastly, we prove the orthogonality condition for ↑R-strided convolutions.
∥y∥2 = 1
2π
Z π
−π


Y [R](ejω)



2
dω
(C.42)
= 1
2π
Z π
−π


H[R](ejω)X(ejω)



2
dω
(C.43)
= 1
2π
Z π
−π

X(ejω)

2 dω
(C.44)
= ∥x∥2 ,
(C.45)
where Equations (C.42) and (C.45) are due to Parseval’s theorems (Theorem B.3 and Proposition C.3),
Equation (C.43) follows from the spectral representation of the ↑R-strided convolution (Equa-
tion (C.25)), and Equation (C.44) uses the fact that the transfer matrix is unitary for each frequency.
The “only if” part can be proved by contradiction similar to Theorem B.5.
(2) Dilated convolutional layer is proposed to increase the receptive ﬁeld of a convolutional layer
without extra parameters and computation. The layer up-samples its ﬁlter bank before convolution
with the input. R-dilated convolutional layer) computes its output with the following equation:
y[i] =
X
n∈Z
h↑R[n]x[i −n]
(C.46)
Proposition C.5 (Orthogonality of dilated convolutional layer). For an R-dilated convolution, the
spatial convolution in Equation (C.46) leads to a spectral representation as
Y (z) = H(zR)X(z),
(C.47)
Furthermore, an R-dilated convolutional layer is orthogonal if and only if H(zR) is paraunitary.
Proof of Proposition C.5. According to Proposition C.1, the Z-transform of h↑R is H(zR). There-
fore, the “if” part follows directly from the convolution theorem. The “only if” part can be proved by
constructing a counterexample similar to Theorem B.5.
Notice that H(zR) is paraunitary if and only if H(ejω) is unitary for all frequency ω ∈R, which is
the same as XHz being paraunitary. In other words, any ﬁlter bank that is orthogonal for a standard
convolution is also orthogonal for a dilated convolution and vice versa.
(3) Group convolutional layer is proposed to reduce the parameters and computations and used in
many efﬁcient architectures, including MobileNet, ShufﬂeNet. The layer divides both input/output
channels into multiple groups and restricts the connections within each group.
Formally, a group convolutional layer with G groups (abbrev. as G-group convolutions) is parame-
terized by G ﬁlter banks {hg}g∈[G], each consists of (T/G) × (S/G) ﬁlters. The layer maps an S
channels input x to a T channels output y according to
y[i] =
X
n∈Z
blkdiag
 {hg[n]}g∈[G]

x[i −n],
(C.48)
where blkdiag({·}) computes a block diagonal matrix from a set of matrices.
23
Under review as a conference paper at ICLR 2022
Proposition C.6 (Orthogonality of group convolutional layer). For a G-group convolution, the
spatial convolution in Equation (C.48) leads a spectral representation as , their z-transforms satisfy
Y (z) = blkdiag
 {Hg(z)}g∈[G]

X(z),
(C.49)
Furthermore, a G-group convolutional layer is orthogonal if and only if the block diagonal matrix is
paraunitary, i.e., each hg(z) is paraunitary.
Proof of Proposition C.6. Due to the convolution theorem, it sufﬁces to prove that the Z-transform
of a sequence of block diagonal matrices is also block diagonal in the spectral domain.
X
n∈Z



h0[n]
...
hG−1[n]



|
{z
}
blkdiag
 {hg[n]}g∈[G]

z−n =



P
n∈Z h0[n]z−n
...
P
n∈Z hG−1[n]z−n



(C.50)
=



H0(z)
...
HG−1(z)



|
{z
}
blkdiag
 {Hg(z)}g∈[G]

.
(C.51)
As a result, we can write the orthogonality condition as



H0(z)
...
hG−1(z)



† 


H0(z)
...
HG−1(z)



=



H0(z)†H0(z)
...
HG−1(z)†HG−1(z)


= I, ∀|z| = 1.
(C.52)
The equation implies Hg(z)†Hg(z) = I, ∀|z| = 1, ∀g ∈[G], i.e., each Hg(z) is paraunitary.
C.3
REALIZATIONS OF PARAUNITARY SYSTEMS
In this subsection, we ﬁrst prove that all ﬁnite-length 1D-paraunitary systems can be represented in a
factorized form. Next, we show how we can construct MD-paraunitary systems using 1D systems.
Lastly, we study the relationship of existing approaches to paraunitary systems.
C.3.1
COMPLETE FACTORIZATION OF 1D-PARAUNITARY SYSTEMS
The classic theorem for spectral factorization of paraunitary systems is traditionally developed for
causal systems (Vaidyanathan, 1993; Kautsky & Turcajová, 1994). Given a causal paraunitary system
of length L (i.e., polynomial in z−1), there always exists a factorization such that
H(z) = QV (z−1; U (1)) · · · V (z−1; U (L−1)),
(C.53)
where Q is an orthogonal matrix, U (ℓ) is a column-orthogonal matrix, and V (z; U) is deﬁned as
V (z; U) = (I −UU ⊤) + UU ⊤z.
(C.54)
In Theorem C.7, we extends this theorem from causal systems to ﬁnite-length (but non-causal) ones.
Theorem C.7 (Complete factorization for 1D-paraunitary systems). Suppose that a paraunitary
system H(z) is ﬁnite-length, i.e., it can be written as P
n h[n]z−n for some sequence {h[n], n ∈
[−L, L]}, then it can be factorized in the following form:
H(z) = V (z; U (−L)) · · · V (z; U (−1))QV (z−1; U (1)) · · · V (z−1; U (L)),
(C.55)
where Q is an orthogonal matrix, U (ℓ) is a column-orthogonal matrix, and V (z; U) is deﬁned
in Equation (C.54). Consequently, the paraunitary system H(z) is parameterized by L + L + 1
(column-)orthogonal matrices Q and U (ℓ)’s.
24
Under review as a conference paper at ICLR 2022
Proof for Theorem C.7. Given a non-causal paraunitary system H(z), we can always ﬁnd a causal
counterpart ˆ
H(z) such that H(z) = zL ˆ
H(z) (This can be done by shifting the causal system
backward by L steps, which is equivalent to multiplying zL in the spectral domain). Since the causal
system ˆ
H(z) admits a factorization in Equation (C.55), we can write the non-causal system H(z) as
H(z) = zLQV (z−1; ˆ
U (1)) · · · V (z−1; ˆ
U (L+L)).
(C.56)
Therefore, it sufﬁces to show that for an orthogonal matrix Q and any column-orthogonal matrix ˆ
U,
we can always ﬁnd another column-orthogonal matrix U such that
zQV (z−1; ˆ
U) = V (z; U)Q.
(C.57)
If the equation above is true, we can set U (ℓ) = ˆ
U (ℓ−1−L) for ℓ< 0 and U (ℓ) = ˆ
U (ℓ−L) for ℓ> 0,
which will convert Equation (C.56) into Equation (C.55).
Now we start to prove Equation (C.57). Note that any column-orthogonal ˆ
U has a complement ¯
U
such that [ ˆ
U, ¯
U] is orthogonal and I = ˆ
U ˆ
U ⊤+ ¯
U ¯
U ⊤. We then rewrite Equation (C.57) as
zQV (z−1; ˆ
U) = zQ(I −ˆ
U ˆ
U ⊤+ ˆ
U ˆ
U ⊤z−1)
(C.58)
= Q(I −¯
U ¯
U ⊤+ ¯
U ¯
U ⊤z)
(C.59)
= (I −Q ¯
U ¯
U ⊤Q⊤+ Q ¯
U ¯
U ⊤Q⊤z)Q
(C.60)
= (I −UU ⊤+ UU ⊤z)Q
(C.61)
= V (z; U)Q,
(C.62)
where in Equation (C.61) we set U = Q ¯
U. This completes the proof.
C.3.2
MULTI-DIMENSIONAL (MD) PARAUNITARY SYSTEMS
If the data are multi-dimensional (MD), we will need MD-convolutional layers in neural networks.
Analogously, we can prove the equivalence between orthogonal MD-convolutions in the spatial
domain and MD-paraunitary systems in the spectral domain, i.e.,
H(z)†H(z) = I, z = (z1, · · · , zD), |zd| = 1, ∀d ∈[D],
(C.63)
where D is the data dimension. In this work, we adopt a parameterization based on separable systems.
Deﬁnition C.8 (Separable MD-paraunitary system). A MD-paraunitary system H(z) is separable if
there exists D 1D-paraunitary systems H1(z1), · · · , HD(zD) such that
H(z) = H(z1, · · · , zD) ≜H1(z1) · · · HD(zD).
(C.64)
Therefore, we can construct an MD-paraunitary system with D number of 1D-paraunitary systems,
each of which is represented in Equation (C.55). Notice that not all MD-paraunitary systems are
separable, thus the parameterization in Equation (C.64) is not complete (see Section 5 for a discussion).
However, we can guarantee that our parameterization realizes all separable MD-paraunitary systems
— each separable paraunitary system admits a factorization in Equation (C.64), where each 1D-system
admits a factorization in Equation (C.55).
C.3.3
INTERPRETATIONS OF PREVIOUS APPROACHES
In Theorem B.5, we have shown that a paraunitary transfer matrix is both necessary and sufﬁcient for
a convolution to be orthogonal. Therefore, we can interpret all approaches for orthogonal convolutions
as implicit constructions of paraunitary systems, including singular value clipping and masking
(SVCM) (Sedghi et al., 2019), block convolution orthogonal parameterization (BCOP) (Li et al.,
2019b), Cayley convolution (CayleyConv) (Trockman & Kolter, 2021), skew orthogonal convolution
(SOC) (Singla & Feizi, 2021). Furthermore, we prove how orthogonal regularization (Wang et al.,
2019; Qi et al., 2020) encourages the transfer matrix to be unitary for all frequencies.
(1) Singular value clipping and masking (SVCM) (Sedghi et al., 2019) clips all singular values
of H(ejω) to ones for each frequency ω after gradient update. Since the clipping step can arbitrarily
25
Under review as a conference paper at ICLR 2022
enlarge the ﬁlter length, SVCM subsequently masks out the coefﬁcients outside the ﬁlter length.
However, the masking step breaks the orthogonality, as we have seen in the experiments (Section 6.1).
(2) Block convolution orthogonal parameterization (BCOP) (Li et al., 2019b) tries to generalize
the spectral factorization of 1D-paraunitary systems in Equation (C.55) to 2D-paraunitary systems.
H(z1, z2) = z
K−1
2
1
z
K−1
2
2
QV (z1, z2; U (1)
1 , U (1)
2 ) · · · V (z1, z2; U (K−1)
1
, U (K−1)
2
),
(C.65)
where V (z1, z2; U (ℓ)
1 , U (ℓ)
2 ) = V (z1; U (ℓ)
1 )V (z2; U (ℓ)
2 ). In other words, this approach makes
each V -block, instead of the whole paraunitary system, separable. This factorization in BCOP is
incomplete for 2D-paraunitary system — unlike 1D-paraunitary systems, not every 2D-paraunitary
system admits a factorized form (Lin & Vaidyanathan, 1996).
(2) Calyey convolution (CayleyConv) (Trockman & Kolter, 2021) aims to generalize the Cayley
transform for orthogonal matrices in Equation (C.88) to 2D-paraunitary systems H(z1, z2):
H(z1, z2) = (I −A(z1, z2)) (I + A(z1, z2))−1 ,
(C.66)
where A(z1, z2) is a skew-Hermitian matrix for |z1| = 1, |z2| = 1 (i.e., A(ejω1, ejω2)† =
A(ejω1, ejω2) for any ω1, ω2). Since Cayley transform cannot parameterize a matrix with singu-
lar value −1 for any frequency, the CayleyConv is not a complete parameterization.
(4) Skew orthogonal convolution (SOC) (Singla & Feizi, 2021) aims to generalize the matrix
exponential for orthogonal matrices (Equation (C.90)) to convolution exponential for 2D-paraunitary
systems H(z1, z2):
H(z1, z2) = exp (A(z1, z2)) ≜
∞
X
k=0
A(z1, z2)
k!
= I + A(z1, z2) + A(z1, z2)2
2
+ · · · ,
(C.67)
where A(z1, z2) is skew-Hermitian matrix for any matrix for |z1| = 1, |z2| = 1 (in other words,
A(ejω1, ejω2)† = A(ejω1, ejω2) for any ω1, ω2). It is not resolved whether all 2D-paraunitary systems
can be represented in terms of convolution exponential.
(5) Orthogonal regularization (Ortho-Reg) (Wang et al., 2019; Qi et al., 2020) is developed
to encourage orthogonality in convolutional layers. We show that such orthogonal regularization
is equivalent to a unitary regularization of the paraunitary system with uniform weights on all
frequencies.
X
i∈Z





X
n∈Z
h[n]h[n −Ri]⊤−δ[i]





2
= 1
2π
Z π
−π


H[R](ejω)
†H[R](ejω) −I



2
dω,
(C.68)
where δ[0] = I is an identity matrix, δ[n] = 0 is a zero matrix for n ̸= 0. We prove the equivalence
more generally in Proposition C.9. However, this approach cannot enforce exact orthogonality and in
practice requires hyperparameter search for a proper regularizer coefﬁcient.
Proposition C.9 (Parseval’s theorem for ridge regularization). Given a sequence of matrices h =
{h[n], n ∈Z}, the following four expressions are equivalent:
1
2π
Z π
−π


H[R](ejω)
†H[R](ejω) −I



2
dω,
(C.69a)
X
i∈Z





X
n∈Z
h[n]⊤h[n −Ri] −δ[i]





2
,
(C.69b)
1
2π
Z π
−π


H[R](ejω)H[R](ejω)
† −I



2
dω,
(C.69c)
X
i∈Z





X
n∈Z
h[n]h[n −Ri]⊤−δ[i]





2
,
(C.69d)
where ∥· ∥denotes the Frobenius norm of a matrix.
26
Under review as a conference paper at ICLR 2022
Proof of Proposition C.9. We ﬁrst prove the equivalence between Equations (C.69a) and (C.69c).


H[R](ejω)
†H[R](ejω) −I



2
= tr

H[R](ejω)
†H[R](ejω) −I
† 
H[R](ejω)
†H[R](ejω) −I

(C.70)
= tr

H[R](ejω)
†H[R](ejω)H[R](ejω)
†H[R](ejω)

−2tr

H[R](ejω)
†H[R](ejω)

+ I
(C.71)
= tr

H[R](ejω)H[R](ejω)
†H[R](ejω)H[R](ejω)
†
−2tr

H[R](ejω)H[R](ejω)
†
+ I
(C.72)
= tr

H[R](ejω)H[R](ejω)
† −I
† 
H[R](ejω)H[R](ejω)
† −I

(C.73)
=


H[R](ejω)H[R](ejω)
† −I



2
,
(C.74)
where Equations (C.70) and (C.74) make use of ∥A∥2 = tr(A†A), Equations (C.71) and (C.73) are
due to the linearity of tr(·), and Equation (C.72) utilizes tr(AB) = tr(BA).
Next, we prove the equivalence between Equations (C.69a) and (C.69b).
H[R](ejω)
†H[R](ejω) −I =
X
r∈[R]
Hr|R(ejω)
†Hr|R(ejω) −I
(C.75)
=
X
r∈[R]
X
i∈Z
 X
m∈Z
hr|R[m]⊤hr|R[m −i] −δ[i]
!
e−jωi
(C.76)
=
X
r∈[R]
X
i∈Z
 X
m∈Z
h[Rm + r]⊤h[R(m −i) + r] −δ[i]
!
e−jωi
(C.77)
=
X
i∈Z

X
r∈[R]
X
m∈Z
h[Rm + r]⊤h[Rm + r −Ri] −δ[i]

e−jωi
(C.78)
=
X
i∈Z
 X
n∈Z
h[n]⊤h[n −Ri] −δ[i]
!
e−jωi,
(C.79)
where Equation (C.75) follows from the deﬁnition of polyphase matrix in Equation (C.15). Equa-
tion (C.76) uses a number of properties of Fourier transform: a Hermitian in the spectral domain is a
transposed reﬂection in the spatial domain, a frequency-wise multiplication in the spectral domain is
a convolution in the spatial domain, and an identical mapping in the spectral domain is an impulse
sequence in the spatial domain. Equation (C.77) follows from the deﬁnition of polyphase components
in Equation (C.7), and Equation (C.79) makes a change of variables n = Rm + r. In summary, we
show that the LHS (denoted D(ejω)) is a Fourier transform of the RHS (denoted as d[i]):
H[R](ejω)
†H[R](ejω) −I
|
{z
}
D(ejω)
=
X
i∈Z
 X
n∈Z
h[n]⊤h[n −Ri] −δ[n]
!
|
{z
}
d[i]
e−jωi.
(C.80)
Applying Parseval’s theorem (Theorem B.3) to the sequence d = {d[i], i ∈Z}, we have
1
2π
Z π
−π


H[R](ejω)
†H[R](ejω) −I



2
dω =
X
i∈Z





X
n∈Z
h[n]⊤h[n −Ri] −δ[i]





2
,
(C.81)
which proves the equivalence between Equations (C.69a) and (C.69b). With almost identical argu-
ments, we can prove the equivalence between Equations (C.69c) and (C.69d), that is
1
2π
Z π
−π


H[R](ejω)H[R](ejω)
† −I



2
dω =
X
i∈Z





X
n∈Z
h[n]h[n −Ri]⊤−δ[i]





2
,
(C.82)
which completes the proof.
27
Under review as a conference paper at ICLR 2022
Table 5: Computational complexities of different approaches for orthogonal convolutions.
Approach
Computational Complexity
Normal
O
 L2N 2C2
SVCM
O
 L2N 2C2 + N 2C3
Ortho-Reg
O
 L2N 2C2 + L4C2
CayleyConv
O
 N 2 log(N)C2 + N 2C3
SOC
O
 KL2N 2C2
BCOP
O
 L2N 2C2 + KL2C3
SC-Fac
O
 L2N 2C2 + L2C3
In Table 5, we compare the computational complexities (forward pass) of orthogonal convolutions
against normal convolution. For simplicity, we assume the feature maps have size N × N, the
convolution ﬁlters have size L × L, and the maximum of input/output channels is C. We use K to
denote the number of iterations for Björck’s algorithm in BCOP, or Taylor’s series order in SOC.
• For SC-Fac, BCOP, SVCM, or Ortho-Reg, the ﬁrst term O(L2N 2C2) is the base cost of a normal
convolution, and the second term is the overhead for reconstruction, projection, or regularization.
Note that the overhead in SC-Fac is comparable to Ortho-Reg, which is lower than SVCM or
BCOP. If C < N 2, the overhead in SC-Fac is negligible compared to the base cost.
• For CayleyConv, the ﬁrst term O(N 2 log(N)C2) is the cost for fast Fourier transform (FFT), and
the second term O(N 2C3) is the cost for matrix inversion for all frequencies. The cost of SOC
is exactly K times as a normal convolution. The computational complexities of CayleyConv and
SOC are signiﬁcantly higher than the one of normal convolution.
• For those approaches whose ﬁlters can be explicitly obtained (SC-Fac, BCOP, SVCM, and Ortho-
Reg), the inference time of an orthogonal convolution is no different from a normal convolution,
i.e., O(L2N 2C2). On the other hand, for those approaches whose ﬁlters are implicitly deﬁned
(CayleyConv and SOC), the inference time is the same as the forward pass in training.
C.4
CONSTRAINED OPTIMIZATION OVER ORTHOGONAL MATRICES
In Theorem C.7, we have shown how orthogonal matrices characterize paraunitary systems. Our
remaining goal, therefore, is to parameterize orthogonal matrices using unconstrained parameters. In
this part, we review popular parameterization methods: Householder reﬂections (Mhammedi et al.,
2017; Mathiasen et al., 2020), Givens rotations (Dorobantu et al., 2016; Jing et al., 2017), Björck
orthogonalization (Anil et al., 2019), Cayley transform (Helfrich et al., 2018; Maduranga et al., 2019),
and exponential map (Lezcano-Casado & Martínez-Rubio, 2019; Lezcano Casado, 2019).
(1) Householder reﬂections (Mhammedi et al., 2017; Mathiasen et al., 2020). The parameteriza-
tion represents an orthogonal matrix using a product of Householder matrices. Given N ∈N and
n ∈{1, · · · , N}, we deﬁne H(n) as a mapping from a vector v ∈Rn (a scalar v ∈R for n = 1) to
a block-diagonal matrix H(n)(v) ∈RN×N:
H(n)(v) =
IN−n
In −2 vv⊤
∥v∥2

, H(1)(v) =

IN−1
v

,
(C.83)
where In ∈Rn×n denotes an identity matrix. For n ≥2, H(n)(v) is the Householder matrix
that represents the reﬂection with respect to the hyperplane orthogonal to concat(0N−n, v) ∈RN
and passing through the origin. For n = 1, the scalar v takes values {−1, +1}, which makes
H(1)(v) either an identity matrix (for v = 1) or a Householder matrix (for v = −1). This method
parameterizes an orthogonal matrix as a product of H(n)’s:
U = H(n)(v(n)) · · · H(2)(v(2))H(1)(v(1)),
(C.84)
where each v(n) ∈Rn is a learnable vector for n ≥2, and v(1) is a ﬁxed constant that is generated
at initialization and ﬁxed afterward. Observing that Equation (C.84) is serial (unfriendly to parallel
computing), Mathiasen et al. (2020) proposes to increase its parallelism using WY transform.
28
Under review as a conference paper at ICLR 2022
(2) Givens rotations (Dorobantu et al., 2016; Jing et al., 2017). The parameterization represents
a special orthogonal matrix using a product of Given rotations matrices. Given N ∈N and i, j ∈
{1, · · · , N} with i ̸= j, we deﬁne G(i,j) as a mapping from an angle θ ∈R to the corresponding
rotation matrix G(i,j) ∈RN×N:
G(i,j)(θ) =













1
· · ·
0
· · ·
0
· · ·
0
.
.
.
...
.
.
.
.
.
.
.
.
.
0
· · ·
cos θ
· · ·
−sin θ
· · ·
0
.
.
.
.
.
.
...
.
.
.
.
.
.
0
· · ·
sin θ
· · ·
cos θ
· · ·
0
.
.
.
.
.
.
.
.
.
...
.
.
.
0
· · ·
0
· · ·
0
· · ·
1













.
(C.85)
This method parameterizes a special orthogonal matrix U as a product of G(i,j)’s:
U = G(0,1)(θ(0,1)) G(0,2)(θ(0,2)) G(1,2)(θ(1,2)) · · · G(N−2,N−1)(θ(N−2,N−1)),
(C.86)
where θ(i,j)’s are N(N −1) unconstrained parameters. Since the determinant of each rotation
matrix is +1, their product U also has +1 determinant, thus is a special orthogonal matrix. Again,
Equation (C.86) is highly serial, thus both Dorobantu et al. (2016) and Jing et al. (2017) propose to
group N/2 Given rotations into a packed rotation to increase the parallelism.
(2) Björck orthogonalization (Anil et al., 2019). The algorithm was ﬁrst introduced in Björck &
Bowie (1971) to compute the closest orthogonal matrix of a given matrix. Given an initial matrix U0,
this algorithm iteratively approaches its closest orthogonal matrix as:
Uk+1 = Uk

I + 1
2Pk + · · · + (−1)p

−1
2
p

P p
k

, ∀k ∈[K],
(C.87)
where K is the iterative steps, Pk = I −U ⊤
k Uk, and p controls the trade-off between efﬁciency and
accuracy at each step. When the algorithm is used for parameterization, it maps an unconstrained
matrix U0 to an approximately orthogonal matrix UK in K steps. Although Björck parameterization
is complete (since any orthogonal matrix Q can be represented by U0 = Q), it is inexact due to the
iterative approximation.
(3) Cayley transform (Helfrich et al., 2018; Maduranga et al., 2019). The transform provides
a bijective parameterization of orthogonal matrices without −1 eigenvalue with skew-symmetric
matrices (i.e., A⊤= −A)
U = (I −A)(I + A)−1,
(C.88)
where the skew-symmetric matrix A is represented by its upper-triangle entries. Since orthogonal
matrices with −1 eigenvalue are out of consideration, the parameterization is incomplete. Helfrich
et al. (2018); Maduranga et al. (2019) overcome this difﬁculty by a scaled Cayley transform:
U = D(I −A)(I + A)−1,
(C.89)
where D is a diagonal matrix with ±1 non-zero entries, which is (randomly) generated at initialization
and ﬁxed during training.
(4) Exponential map (Lezcano-Casado & Martínez-Rubio, 2019; Lezcano Casado, 2019). The
mapping provides a surjective parameterization of all special orthogonal matrices (with +1 determi-
nant) with using skew-symmetry matrices (i.e., A⊤= −A).
U = exp(A) ≜
∞
X
k=0
Ak
k! = I + A + 1
2A2 + · · · ,
(C.90)
where the inﬁnite sum can be computed exactly up to machine-precision (Higham, 2009). Lezcano-
Casado & Martínez-Rubio (2019) derives an efﬁcient backpropagation algorithm for the mapping,
making it an exact and efﬁcient parameterization in neural networks. To support a complete parame-
terization for all orthogonal matrices, Lezcano Casado (2019) extends the mapping as:
U = Q exp(A),
(C.91)
where Q is an orthogonal matrix, which is generated at initialization and ﬁxed during training.
In principle, we can use any of these approaches to parameterize the orthogonal matrices. In this
work, we choose exponential map due to its exactness, efﬁciency, and completeness.
29
Under review as a conference paper at ICLR 2022
D
LEARNING DEEP ORTHOGONAL NETWORKS WITH LIPSCHITZ BOUNDS
In this section, we ﬁrst discuss the properties of GroupSort and Lipschitz networks. Subsequently, we
prove Proposition 4.1, which exhibits two approaches to construct Lipschitz residual blocks. Lastly,
we prove in Proposition D.1 when a paraunitary system (represented by a complete factorization as in
Theorem C.7) reduces to an orthogonal matrix. The reduction allows us to apply the initialization
methods for orthogonal matrices to paraunitary systems.
GroupSort and orthogonality. The GroupSort activation separates inputs into groups and sorts
each group into ascending order (Anil et al., 2019). It guarantees two properties: (1) The activation
is norm preserving in the forward pass — a sorting function does not change the norm of any
input vector. (2) The activation is gradient norm preserving in the backward pass — a sorting
function acts as a permutation, which is orthogonal. GroupSort with the group size of two is a spacial
case that we use in the paper followed by (Chernodub & Nowicki, 2016; Anil et al., 2019).
A Lipschitz network with orthogonal layers and GroupSort activations is locally orthogonal. Given
any input x to the network, there exists a neighborhood N(x, r) with radius r such that the sorting
order in each activation does not change. In this neighborhood N(x, r), each GroupSort layer operates
as a constant permutation (thus orthogonal); consequently, the whole network is locally orthogonal.
Lipschitz residual blocks. In Proposition 4.1, we prove the Lipschitzness of two types of residual
blocks, one based on additive skip-connection and another based on concatenative one (See Figure 6).
Proof for Proposition 4.1. We ﬁrst prove the Lipschitzness for the additive residual block f : f(x) ≜
αf 1(x) + (1 −α)f 2(x). Let x, x′ be two inputs to f and f(x), f(x′) be their outputs, we have
∥f(x′) −f(x)∥=

 αf 1(x′) + (1 −α)f 2(x′)

−
 αf 1(x) + (1 −α)f 2(x)


(D.1)
=

α
 f 1(x′) −f 1(x)

+ (1 −α)
 f 2(x′) −f 2(x)


(D.2)
≤α

f 1(x′) −f 1(x)

 + (1 −α)

f 2(x′) −f 2(x)


(D.3)
≤αL ∥x′ −x∥+ (1 −α)L ∥x′ −x∥
(D.4)
= L∥x′ −x∥,
(D.5)
where Equation (D.3) makes uses of the triangle inequality, and Equation (D.4) is due to the L-
Lipschitzness of both f 1, f 2. Therefore, we have shown that ∥f(x′) −f(x)∥≤L∥x′ −x∥.
Similarly, we prove the Lipschitzness for the concatenative residual block g
:
g(x)
≜
P

g1(x1); g2(x2)

. Let x, x′ be two inputs to g and g(x), g(x′) be their outputs, we have
∥g(x′) −g(x)∥2 =


P

[g1(x1′); g2(x2′)] −[g1(x1); g2(x2)]



(D.6)
=

[g1(x′); g2(x′)] −[g1(x); g2(x)]

2
(D.7)
=


g1(x1′) −g1(x)



2
+


g2(x2′) −g2(x2)



2
(D.8)
≤L2 

x1′ −x1


2
+ L2 

x2′ −x2


2
(D.9)
= L2 

[x1′; x2′] −[x1; x2]



2
(D.10)
= L2 ∥x′ −x∥2 ,
(D.11)
where Equation (D.7) utilizes ∥P x∥= ∥x∥, ∀x, and Equation (D.9) is due to the L-Lipschitzness of
g1, g2. The equations above implies that ∥g(x′) −g(x)∥≤L∥x′ −x∥.
Reduction of a paraunitary system to an orthogonal matrix. In Proposition D.1, we prove a
special case when a paraunitary system reduces to an orthogonal matrix. The reduction allows us to
apply the initialization methods for orthogonal matrices to paraunitary systems.
Proposition D.1 (Reduction of a paraunitary matrix to an orthogonal matrix). Suppose a paraunitary
system H(z) takes the complete factorization in Equation (C.55), and assume L = L with U (−ℓ) =
QU (ℓ) for all ℓ, then the paraunitary matrix H(z) reduces to an orthogonal matrix Q,
H(z) = V (z; U (−L)) · · · V (z; U (−1))QV (z−1; U (1)) · · · V (z−1; U (L)) = Q.
(D.12)
30
Under review as a conference paper at ICLR 2022
Add
Ortho. Conv.
Ortho. Conv.
GroupSort
GroupSort
α
1−α
(a) Basic additive block.
Add
Ortho. Conv.
(Stride = 2)
Ortho. Conv.
GroupSort
GroupSort
α
1−α
Ortho. Conv.
(Stride = 2)
(b) Strided additive block.
Channel Split
Concatenate
Channel Shuffle
Ortho. Conv.
Ortho. Conv.
GroupSort
GroupSort
(c) Basic shufﬂing block.
Channel Split
Concatenate
Channel Shuffle
Ortho. Conv.
(Stride = 2)
Ortho. Conv.
GroupSort
GroupSort
Ortho. Conv.
(Stride = 2)
(d) Strided shufﬂing block.
Figure 6: Variants of residual blocks. In our experiments, we combine (a) & (b) to construct an
orthogonal ResNet, and (c) & (d) to construct an orthogonal ShufﬂeNet. In Proposition 4.1, we prove
the Lipschitzness of these building blocks. Since composition of Lipschitz functions is still Lipschitz,
it implies that a network constructed by these building blocks is also Lipschitz.
Proof for Proposition D.1. In order to prove Equation (D.12), it sufﬁce to show that
V (z; U (−ℓ))QV (z−1; U (ℓ)) = Q,
(D.13)
and Equation (D.12) will reduce recursively to the orthogonal matrix Q. For simplicity, we rewrite
U (−ℓ) as L and U (ℓ) as R, by which we have L = QR (or R = Q⊤L) and we aim to prove
V (z; L)QV (z−1; R) = Q. By the deﬁnition of V (z; ·) in Equation (C.54), we expand it as
V (z; L)QV (z−1; R) =
h
I −LL⊤
+ LL⊤z
i
Q
h
I −RR⊤
+ RR⊤z−1i
=
LL⊤Q(I −RR⊤)
|
{z
}
c[−1]
z + (I −LL⊤)Q(I −RR⊤) + LL⊤QRR⊤
|
{z
}
c[0]
+ (I −LL⊤)QRR⊤
|
{z
}
c[1]
z−1
(D.14)
Therefore, we will need to show that c[−1] = 0, c[1] = 0 and c[0] = Q.
We ﬁrst show that both c[−1] for z and c[1] for z−1 are zero matrices.
c[−1] = LL⊤Q
 I −RR⊤
(D.15)
= LL⊤Q −LL⊤QRR⊤
(D.16)
= L(Q⊤L)⊤−L(Q⊤L)⊤RR⊤
(D.17)
= LR⊤−L(R⊤R)R⊤
(D.18)
= LR⊤−LR⊤= 0,
(D.19)
c[1] = (I −LL⊤)QRR⊤
(D.20)
= QRR⊤−LL⊤QRR⊤
(D.21)
= (QR)R⊤−LL⊤(QR)R⊤
(D.22)
= LR⊤−L(L⊤L)R⊤
(D.23)
= LR⊤−LR⊤= 0.
(D.24)
31
Under review as a conference paper at ICLR 2022
Table 6: Comparisons of various initialization methods on WideResNet (kernel size 5).
Initialization
WideResNet10-10
WideResNet22-10
Clean (%)
PGD (%)
Clean (%)
PGD (%)
uniform
83.58
73.20
87.55
75.71
torus
82.40
72.50
88.12
75.43
permutation
83.18
73.16
87.82
76.46
identical
83.29
73.49
87.82
75.49
Lastly, we show that the constant coefﬁcient c[0] is equal to Q.
c[0] = (I −LL⊤)Q(I −RR⊤) + LL⊤QRR⊤
(D.25)
= Q −LL⊤Q −QRR⊤+ 2LL⊤QRR⊤
(D.26)
= Q −LR⊤−LR⊤+ 2LR⊤= Q
(D.27)
which completes the proof.
E
SUPPLEMENTARY MATERIALS FOR EXPERIMENTS
E.1
EXPERIMENTAL SETUP
Network architectures. For fair comparisons, we follow the architectures by Trockman & Kolter
(2021) for KW-Large, ResNet9, WideResNet10-10 (i.e., shallow networks). We set the group size for
GroupSort activations as 2 in all experiments. For networks deeper than 10 layers, we implement their
architectures modifying from the Pytorch ofﬁcial implementation of ResNet. It is crucial to replace
the global pooling before fully-connected layers with an average pooling with a window size of 4.
For the average pooling, we multiply the output with the window size to maintain its 1-Lipschitzness.
Other architectures, including ShufﬂeNet and plain convolutional network (ConvNet), are further
modiﬁed from the ResNet, where only the skip-connections are changed or removed. We use the
widen factor to indicate the channel number: we set the number of channels at each layer as base
channels multiplied by the widen factor. The base channels are 16, 32, 64 for three groups of residual
blocks. More details of the ResNet architecture can be found in the ofﬁcial Pytorch implementation.1
Learning strategies. We use the CIFAR-10 dataset for all our experiments. We normalize all
input images to [0, 1] followed by standard augmentation, including random cropping and horizontal
ﬂipping. We use the Adam optimizer with a maximum learning rate of 10−2 coupled with a piece-wise
triangular learning rate scheduler. We initialize all our SC-Fac layers as permutation matrices: (1) we
select the number of columns for each pair U (ℓ), U (−ℓ) uniformly from {1, · · · , T} at initialization
(the number is ﬁxed during training); (2) for ℓ> 0, we sample the entries in U (ℓ) uniformly with
respect to the Haar measure; (3) for ℓ< 0, we set U (−ℓ) = QU (ℓ) according to Proposition D.1.
E.2
ADDITIONAL EMPIRICAL RESULTS ON HYPER-PARAMETERS SELECTION
Multi-class hinge loss. Following previous works on Lipschitz networks (Anil et al., 2019; Li et al.,
2019b; Trockman & Kolter, 2021), we adopt the multi-class hinge loss in training. For each model,
we perform a grid search on different margins ϵ0 ∈{1 × 10−3, 2 × 10−3, 5 × 10−3, 1 × 10−2, 2 ×
10−2, 5 × 10−2, 0.1, 0.2, 0.5} and report the best performance in terms of robust accuracy. Notice
that the margin ϵ0 controls the trade-off between clean and robust accuracy, as shown in Figure 7.
Initialization methods. In Proposition D.1, we have shown how to initialize our orthogonal convolu-
tional layers as orthogonal matrices. In Table 6, we perform a study on different initialization methods,
including identical, permutation, uniform, and torus (Henaff et al., 2016; Helfrich et al., 2018). We
ﬁnd that permutation works the best for WideResNet22-10, while all methods are similar in shallower
WideResNet10-10. Therefore, we use permutation initialization for all other experiments.
1 https://github.com/pytorch/vision/blob/master/torchvision/models/
32
Under review as a conference paper at ICLR 2022
500 200 100
50
20
10
5
2
1
60
80
100
Margin ϵ0 (×10−3)
Accuracy (%)
Train
Test
PGD
Figure 7: Effect of the Lipschitz margin ϵ0 for WideResNet22-10. It shows a trade-off between
clean and robust accuracy with different margins for multi-class hinge loss. As shown, the training
and test accuracy become higher with larger margin, but the robust accuracy decreases after ϵ0 = 0.1.
Table 7: Comparison of different depth and width on WideResNet (kernel size 5). Some numbers
are missing due to the large memory requirement (on Tesla V100 32G). The notation width factor
indicates (channels = base channels × factor).
10 layers
Width
1
3
6
8
10
1
3
6
8
10
Clean (%)
PGD with ϵ = 36/255 (%)
Ours
79.96 84.17 84.96
84.61
84.09
65.92
69.70
72.18
72.51
74.29
Cayley
77.88 82.14 82.56
85.53
85.01
66.65
73.06
74.33
75.66
76.13
RKO
81.37 83.55 84.67
85.18
84.62
70.55
74.44
76.41
76.65
77.02
22 layers
Width
1
3
6
8
10
1
3
6
8
10
Clean (%)
PGD with ϵ = 36/255 (%)
Ours
79.90 82.22 87.21
88.10
87.82
67.95
70.88
74.30
75.12
76.46
Cayley
79.11 84.82 85.85
-
-
69.79
65.61
74.81
-
-
RKO
82.71 84.19 84.33
84.55
-
72.40
74.36
75.66
76.41
-
34 layers
Width
1
3
6
8
10
1
3
6
8
10
Clean (%)
PGD with ϵ = 36/255 (%)
Ours
81.24 88.17
88.92
-
-
69.21
71.85
75.09
-
-
Cayley
82.46 84.29
-
-
-
71.27
74.73
-
-
-
RKO
81.51 83.24 83.92
-
-
71.38
73.84
75.03
-
-
Network depth and width. Exact orthogonality is criticized for harming the expressive power
of neural networks, and we ﬁnd that increasing network depth/width can partially compensate for
such loss. In Table 7, we perform a study on the impact of network depth/width on the predictive
performance. As shown, deeper/wider architectures consistently improve both the clean and robust
accuracy for our implementation. However, the best robust accuracy is achieved by a 22-layer network
since we can afford a wide architecture for 34-layer architecture.
Comparison against normal convolutional networks. In Table 8, We perform a comparison
between our orthogonal networks and normal convolutional networks. Their architecture are identical
except for the activation function (GroupSort for ours and ReLU for normal convolutional networks).
Since batch normalization is common in normal convolutional networks but not in Lipschitz networks,
33
Under review as a conference paper at ICLR 2022
Table 8: Comparison of orthogonal convolutions and normal convolutions on WideResNet (ker-
nel size 5). The notation width factor indicates (channels = base channels × factor).
22 layers
Width
1
3
6
8
10
1
3
6
8
10
Clean (%)
PGD with ϵ = 36/255 (%)
Ortho. (SC-Fac)
79.90 82.22
87.21
88.10
87.82
67.95
70.88
74.30
75.12
76.46
Normal (w/o BN)
88.81 90.71
91.59
91.64
91.57
54.33
66.36
69.35
69.94
74.10
Normal (with BN)
88.52 91.74
91.20
92.29
92.40
51.53
66.90
73.18
73.89
73.02
Training (s)
Inference (s)
Ortho. (SC-Fac)
145.3 173.4
250.1
323.1
434.0
1.47
4.35
9.72
12.65
17.56
Normal (w/o BN)
13.77 30.71
69.35
99.94
153.5
1.01
3.03
6.77
9.12
13.03
Normal (with BN)
16.56 34.16
87.49
106.9
167.4
1.14
3.34
7.33
9.69
13.39
Table 9: Practical robustness against ℓ∞adversarial examples (WideResNet kernel size 5, ℓ∞
perturbation radius of ϵ = 8/255). BCOP+ and SOC (Singla & Feizi, 2021) results with ResNet-18
are reported by Singla & Feizi (2021).
Model
Method
Clean (%)
PGD (%)
Resnet-18
BCOP+
79.26
34.85
SOC
82.24
43.73
WideResNet22-max
BCOP
77.57
46.35
Cayley
78.27
45.21
Ours
76.28
46.27
we provide both results for normal convolutional networks with or without batch normalization. In
the table, we report the clean/robust accuracy, train time for epoch, and inference time for the test set.
Robustness against ℓ∞attacks using adversarial training. Since orthogonality only guarantees ℓ2
Lipschitzness, Lipschitz networks with orthogonal layers are not naturally robust to ℓ∞perturbations.
To further guard Lipschitz networks against ℓ∞attacks, we follow the approach of adversarial training
in Wong et al. (2020). For training, we use a FGSM variant with step size 10/255; for evaluation,
we use 50 PGD iterations with step size 2/255 and 10 random restarts. We report the experimental
results in Table 9. We observe that different orthogonal convolution methods achieve similar ℓ∞
robustness on WideResNet-22. Furthermore, the Lipschitz networks with WideResNet22 architecture
is consistently better than ResNet-18, which is previously used in Singla & Feizi (2021).
E.3
ON THE NECESSITY OF EXACT ORTHOGONALITY
Due to the beneﬁts like generalizability and robustness, achieving exact orthogonality in convolutions
is the primary goal of a current research line (Sedghi et al., 2019; Li et al., 2019b; Trockman & Kolter,
2021; Singla & Feizi, 2021). However, until our work, no previous approach achieves orthogonality
up to machine precision. Therefore, our proposed method serves as an extreme case in gaining insight
into the trade-off between orthogonality and expressiveness.
In Section 6, we have seen that exact orthogonality is not critical in shallow Lipschitz networks
for robustness, and various orthogonal convolutions (with different precision) achieve comparable
results. However, our method is more favorable in deeper networks (with more than 10 layers) — we
show the results in Table 7. It indicates that exact orthogonality is crucial in learning deep Lipschitz
networks. However, without our implementation of exact orthogonality (which does not exist before),
it is unclear whether exact orthogonality up to machine precision is needed in Lipschitz networks.
Moreover, exact orthogonality is essential for other important and timely applications. For example,
reversible networks/normalizing ﬂows (Kingma & Dhariwal, 2018; Van Den Berg et al., 2018) require
exact orthogonality to compute inverse transform and determinants accurately.
34
Under review as a conference paper at ICLR 2022
F
ORTHOGONAL CONVOLUTIONS FOR RESIDUAL FLOWS
In this section, we ﬁrst review the class of ﬂow-based generative models (Papamakarios et al., 2019;
Kobyzev et al., 2020). We focus on invertible residual network (Behrmann et al., 2019), a ﬂow-based
model that relies on Lipschitz residual block, and its extended version Residual Flow (Chen et al.,
2019). We then show how to construct improved Residual Flow using our orthogonal convolutions.
Flow-based models. Given an observable vector x ∈RD and a latent vector z ∈RD, we deﬁne a
bijective mapping f : RD →RD from the latent vector z to an observation x = f(z). We further
deﬁne the inverse of f as F = f −1, with which we represent the likelihood of x by the one of z as:
ln pX(x) = ln pZ(z) + ln|det JF (x)|,
(F.1)
where pX is the data distribution, pZ is the base distribution (usually a normal distribution), and
JF (x) is the Jacobian of F at x. In practice, the bijective mapping f is composed by a sequence
of K bijective mapping such that f = fK ◦· · · ◦f1, where each fk is named as a ﬂow. Since the
inverse mapping F = F1 ◦· · · FK transforms the data distribution pX into a normal distribution pZ,
ﬂow-based models are also known as normalizing ﬂows. Accordingly, we rewrite Equation (F.1) as:
ln pX(x) = ln pZ(z) +
K
X
k=1
ln|det JFk(x)|,
(F.2)
In a practical ﬂow-based model, we require efﬁcient computations of (a) each bijective mapping fk,
(b) its inverse mapping Fk = f −1
k , and (c) the corresponding log-determinant ln|det JF (·)|.
Invertible residual networks (i-ResNets). Behrmann et al. (2019) proposes a ﬂow-based model
based on residual network (ResNet). Note that a block in ResNet is deﬁned as F(x) = x + g(x),
where g is a convolutional network. In (Behrmann et al., 2019), the authors prove that F is a bijective
mapping if g is 1-Lipschitz, and its inverse mapping can be computed by ﬁxed-point iterations:
xk+1 = y −g(xk),
(F.3)
where y = g(x) is the output of F and the initialization of the iterative algorithm is x0 := y. From
the Banach ﬁxed-point theorem, we have
∥x −xk∥2 =
Lip(g)k
1 −Lip(g) ∥x1 −x0∥,
(F.4)
i.e., the convergence rate is exponential in the number of iterations and smaller Lipschitz constant
will yield faster convergence. Furthermore, the log-determinant can be computed as:
ln pX(x) = ln pZ(z) + tr (ln (I + Jg(x)))
(F.5)
= ln pZ(z) + tr
 ∞
X
k=1
(−1)k+1
k
[Jg(x)]k
!
,
(F.6)
where the inﬁnite sum is approximated by truncation and the trace is efﬁciently estimated using the
Hutchinson trace estimator tr(A) = Ev∼N (0,I)[v⊤Av].
To constrain the Lipschitz constant, i-ResNet uses spectral normalization on each linear layer in the
block. Moreover, to improve optimization stability, i-ResNet changes the activation function from
ReLU to ELU, ensuring nonlinear activations have continous derivatives.
As summarized in the conclusion of Behrmann et al. (2019), there are two remaining problems in this
model: (1) The estimator of the log-determinant is biased and inefﬁcient; (2) Designing and learning
networks with a Lipschitz constraint are challenging — one needs to constrain each linear layer in
the block instead of being able to control the Lipschitz constant of a block.
Residual Flow. Chen et al. (2019) address problem {(1) by proposing an unbiased Russian roulette
estimator for Equation (F.6):
tr (ln (I + Jg(x))) = En,v
" n
X
k=1
(−1)k
k
v

Jg(x)k
v
P(N ≥k)
#
,
(F.7)
35
Under review as a conference paper at ICLR 2022
Table 10: Comparisons of various ﬂow-based models on the MNIST dataset. We report the
performance in bits per dimension (bpm), where a smaller number indicates a better performance.
Model
MNIST
Glow (Kingma & Dhariwal, 2018)
1.05
FFJORD (Grathwohl et al., 2018)
0.99
i-ResNet (Behrmann et al., 2019)
1.05
Residual Flow (Chen et al., 2019)
0.97
SC-Fac Residual Flow (Ours)
0.896
where n ∼p(N) and v ∼N(0, I). Residual Flow further changes the activation function from ELU
to LipSwish. The LipSwich activation avoids derivative saturation, which occurs when the second
derivative is zero in a large region. However, problem (2) remains unresolved.
Residual ﬂows with orthogonal convolutions. We propose to address problem (2) by replacing
the spectral normalized layers by our orthogonal convolutional layers (SC-Fac). Note that orthogonal
convolutions directly control the Lipschitz constant of a ResNet block. We keep all other components
unchanged — in particular, we use LipSwish activation instead of GroupSort, as GroupSort suffers
from derivative saturation. We experiment our model on MNIST dataset. As shown in Table 10,
our model substantially improve the performance over the original Residual Flow. We display some
images generated by our model in Figure 8.
Figure 8: Random samples from SC-Fac Residual Flow trained on MNIST.
36
