# NON-DENOISING FORWARD-TIME DIFFUSIONS

**Anonymous authors**
Paper under double-blind review

ABSTRACT

The scope of this paper is generative modeling through diffusion processes. An
approach falling within this paradigm is the work of Song et al. (2021), which
relies on a time-reversal argument to construct a diffusion process targeting the
desired data distribution. We show that the time-reversal argument, common to all
denoising diffusion probabilistic modeling proposals, is not necessary. We obtain
diffusion processes targeting the desired data distribution by taking appropriate
mixtures of diffusion bridges. The resulting transport is exact by construction,
allows for greater flexibility in choosing the dynamics of the underlying diffusion,
and can be approximated by means of a neural network via novel training objectives.
We develop a unifying view of the drift adjustments corresponding to our and
to time-reversal approaches and make use of this representation to inspect the
inner workings of diffusion-based generative models. Finally, we leverage on
scalable simulation and inference techniques common in spatial statistics to move
beyond fully factorial distributions in the underlying diffusion dynamics. The
methodological advances contained in this work contribute toward establishing a
general framework for generative modeling based on diffusion processes.

1 INTRODUCTION

Denoising diffusion probabilistic modeling (DDPM) (Sohl-Dickstein et al., 2015; Ho et al., 2020;
Song et al., 2021) is a recent generative modeling paradigm exhibiting strong empirical performance.
Consider a dataset of N samples D = {x[(][n][)]}n[N]=1 [with empirical distribution][ P][D][. The unifying]
key steps underlying DDPM approaches are: (i) the definition of a stochastic process with initial
distribution, whose forward-time (noising) dynamics progressively transform toward a simple
_PD_ _PD_
data-independent distribution _Z; (ii) the derivation of the backward-time (denoising / sampling)_
_P_
dynamics transforming _Z toward_ ; (iii) the approximation of the backward-time transitions by
_P_ _PD_
means of a neural network. Following the training step (iii), a sample whose distribution approximates
is drawn by (iv) simulating from the approximated backward-time transitions starting with a
_PD_
sample from _Z. Both discrete-time (Ho et al., 2020) and continuous-time (Song et al., 2021)_
_P_
formulations of DDPM have been pursued. This work focuses on the latter case, to which we refer
as diffusion time-reversal transport (DTRT). As in DTRT, dynamics are specified through diffusion
processes, i.e. solutions to stochastic differential equations (SDE) with associated drift f ( _·_ ) and
diffusion g( _·_ ) coefficients. A number of approximations are involved in the aforementioned steps.
Firstly, as the dynamics are defined on a finite time interval, a dependency from is retained
_PD_
through the noising process. Hence, starting with a sample from the data-independent distribution
_PZ in (iv) introduces an approximation. Secondly, while the backward-time dynamics of (ii) are_
directly available for diffusions, they are approximated by means of a neural network in (iii). Thirdly,
sampling in (iv) is achieved through a discretization on a time-grid, which introduces a discretization
error. De Bortoli et al. (2021, Theorem 1) links these approximations to the total variation distance
between the distribution of the generated samples from (iv) and .
_PD_

In our first methodological contribution we develop a procedure for constructing diffusion processes
targeting without relying on time-reversal arguments. The proposed transport (coupling) between
_PD_
_Z and_ is achieved by: (1) specifying a diffusion process X on [0, τ ] starting from a generic x0;
_P_ _PD_
(2) conditioning X on hitting a generic xτ at time τ, thus obtaining a diffusion bridge; (3) taking a
bivariate mixture Π0,τ of diffusion bridges over (x0, xτ ) with marginals Π0 = _Z and Πτ =_,
_P_ _PD_
obtaining a mixture process M ; (4) matching the marginal distribution of M over [0, τ ] with a
diffusion process, resulting in a diffusion with initial distribution _Z and terminal distribution_ .
_P_ _PD_


-----

The realized diffusion bridge mixture transport (DBMT) between _Z and_ is exact by construction.
_P_ _PD_
We thus sidestep the approximation common to all DDPM approaches due to the dependency from
retained through the noising process. Moreover, the DBMT can be realized for almost arbitrary
_PD_
_Z, f_ ( ) and g( ). This increased flexibility is a departure from the DTRT where f ( ) and g( )
_P_ _·_ _·_ _·_ _·_
need to be chosen to obtain convergence toward a simple distribution _Z._
_P_

Similarly to the DTRT, achieving the DBMT requires the computation of a drift adjustment term
which depends on D. For a SDE class of interest, we develop a unified and interpretable representation
of DTRT and DBMT drift adjustments as simple transformations of conditional expectations over
_D. This novel result provides insights on the target mapping that we aim to approximate and on_
the quality of approximation achieved by the trained score models of Song et al. (2021). Having
defined for the DBMT a Fisher divergence objective similarly to Song et al. (2021), we leverage on
this unified representation to define two additional training objectives featuring appealing properties.

In our last methodological contribution we extend the class of SDEs that can be realistically employed
in computer vision applications. Specifically, computational considerations have so far restricted the
transitions of the stochastic processes employed in DDPM to be fully factorials. We view images
at a given resolution as taking values over a 2D lattice which discretizes the continuous coordinate
system [0, 1][2] representing heights and widths. Diffusion processes are viewed as spatio-temporal
processes with spatial support [0, 1][2]. Doing so, it is possible to leverage on scalable simulation and
inference techniques from spatial statistics and consider more realistic diffusion transitions.

This paper is structured as follows. In Section 2 we review the DTRT of Song et al. (2021) and in
Section 3 we introduce the DBMT. In order to implement the DTRT and the DBMT it is necessary
to specify the underlying SDE, i.e. the coefficients f ( _·_ ) and g( _·_ ). We study a class of interest in
Section 4. The unified view of drift adjustments is introduced in Section 5. Section 6 develops the
training objectives and Section 7 reviews the obtained results and finalizes the DBMT construction.
In Section 8 we establish the connection with spatio-temporal processes. We conclude in Section 9.
Appendices A to D contain the theoretical framework, assumptions, proofs, and additional material.

_Notation and conventions: we use uppercase notation for probability distributions (measures, laws)_
and lowercase notation for densities; each probability distribution, and corresponding density, is
uniquely identified by its associated letter not by its arguments (which are muted); for example
_P_ (dx) is a distribution, p(x) is its corresponding density; random elements are always uppercase (an
exception is made for times, always lowercase for typographical reasons); if P is the distribution of a
stochastic process, we use subscript notation to refer to its finite dimensional distributions (densities
with p), conditional or not, for some collection of times; for example pt′ _t denotes a transition density,_
_|_
which is understood to be a function of four arguments pt′|t(y|x) = f (t, t[′], x, y); δx is the delta
distribution at x and ⊗ is used for product distributions; we refer directly to a given SDE instead of
referring to the diffusion process satisfying such SDE when no ambiguity arises; we use [a]i and

[A]i,j for vector and matrix indexing, A[⊤] for matrix transposition.

2 DIFFUSION TIME-REVERSAL TRANSPORT

The starting point of Song et al. (2021) is a diffusion process Y satisfying a generic D-dimensional
time-inhomogenous SDE with initial distribution Y0 ∼PD
_dYr = f_ (Yr, r)dr + g(Yr, r)dWr, (1)
over noising time r ∈ [0, τ ]. Thorough this paper we denote with Q the law of the diffusion solving
(1) and with q the corresponding densities. Thus, let qr′|r(y|x), 0 ≤ _r < r[′]_ _≤_ _τ_, be the transition
density of (1), and let qr(y), 0 < r ≤ _τ_, be the marginal density of (1). As Y0 ∼PD, we have

_N_

_qr(y) = N[1]_ _qr|0(y|x[(][n][)])._ (2)

_n=1_

X

The dynamics of (1) over the reversed, i.e. sampling, time t = τ − _r, t ∈_ [0, τ ], are given by
(Anderson, 1982; Haussmann & Pardoux, 1986; Millet et al., 1989)
_dXt = [−f_ (Xt, r) + ∇· G(Xt, r) + G(Xt, r) ∇Xt [ln][ q]r[(][X]t[)]][ dt][ +][ g][(][X]t[, r][)][dW]t[,] (3)
where r = τ − _t is the remaining sampling time, G(x, r) = g(x, r)g(x, r)[⊤]_ and the D-dimensional
vector _G(x, r) is defined by [_ _G(x, r)]i =_ _j=1_ _xj_ [[][G][(][x, r][)]]i,j[. That is the processes][ X]t
_∇·_ _∇·_ _[∇]_

[P][D]


-----

and Yr = Yτ _t have the same distribution. Approximating the terminal distribution Qτ of (1), i.e._
_−_
the initial distribution of (3), with PZ, X0 is sampled from PZ and (3) is discretized and integrated
over t to produce a sample Xτ approximately distributed as .
_PD_

The computation of the multiplicative drift adjustment ∇y [ln][ q]r[(][y][)][ entering (][3][), i.e. the score of the]
marginal density (2), requires in principle (N ) operations. Let sφ(y, r) be a neural network for
_O_
which we would like sφ(y, r) ≈∇y [ln][ q]r[(][y][)][. It remains to find a suitable training objective for which]
unbiased gradients with respect to φ can be obtained at O(1) cost with respect to the dataset size N .
As qr(y) has a mixture representation, the identity of Vincent (2011) for Fisher divergences provides
us with the desired objective for a fixed r ∈ (0, τ ]

LFD,DTRT(φ, r) = E _Yr_ [ln][ q]r[(][Y]r[)][ −] _[s]φ[(][Y]r[, r][)]_
_Yr_ _Qr_ _∇_
_∼_

h

= E _Yr_ [ln][ q]r 0[(][Y]r[|][Y]0[)][ −] _[s][2]φ[i][(][Y]r[, r][)]_ _._ (4)
(Y0,Yr) _Q0,r_ _∇_ _|_
_∼_

h

The key point is that an unbiased, (1) with respect to N, mini-batch Monte Carlo (MC) estimator[2][i]
_O_
for the expectation (and evaluating the average loss over the batch. In order to achieve a global approximation over the4) can be trivially obtained by sampling a batch Y0 ∼PD, Yr ∼ _Qr|0(dyr|Y0),_
whole time interval (0, τ ], Song et al. (2021) proposes uniform sampling of time r

LFD,DTRT(φ) = E _r_ _Yr_ [ln][ q]r 0[(][Y]r[|][Y]0[)][ −] _[s]φ[(][Y]r[, r][)]_ _,_ (5)
_r_ (0,τ ],(Y0,Yr) _Q0,r_ _R_ _∇_ _|_
_∼U_ _∼_

where _r = E[_ _Yr_ [ln][ q]r 0[(][Y]r[|][Y]0[)][∥][2][]]−1 is a regularization term. A MC estimator for (h [2][i] 5) is con_R_ _∥∇_ _|_
structed by augmenting the MC estimator for (4) with the additional sampling step r ∼U(0, τ ].

3 DIFFUSION BRIDGE MIXTURE TRANSPORT

Our starting point is a generic D-dimensional time-inhomogenous SDE which, in contrast to Song
et al. (2021), is directly defined on the sampling time t ∈ [0, τ ]
_dXt = f_ (Xt, t)dt + g(Xt, t)dWt. (6)
We reserve P 0( _x0) to denote the law of the diffusion solving (6) for a given starting value x0 and_

_·|_ _·|_
_p_ _·|0(_ _·|x0) to denote the corresponding densities._

3.1 DIFFUSION BRIDGES

Diffusion bridges are central to the proposed methodology, in this Section we cover their basic theory.
A diffusion bridge is a diffusion process starting from a given value which is conditioned on hitting a
terminal value. It is a deep result, and consequence of Doob h-transforms (Särkkä & Solin (2019,
Chapter 7.9), Rogers & Williams (2000, Chapter IV.6.39)), that a diffusion processes pinned down on
both ends is still a diffusion process. In particular the Markov property is preserved. More precisely,
(6) with initial value x0 conditioned on hitting a terminal value xτ at time τ is characterized the
following SDE on [0, τ ] with initial value x0 (Särkkä & Solin, 2019, Theorem 7.11)

_dXt =_ _f_ (Xt, t) + G(Xt, t) _Xt_ [ln][ p]τ _t[(][x]τ_ _[|][X]t[)]_ _dt + g(Xt, t)dWt,_ (7)
_∇_ _|_

where G(x, t) = g(x, t)g(x, t)[⊤]. The multiplicative adjustment factor _xt_ [ln][ p]τ _t[(][x]τ_ _[|][x]t[)][ forces the]_
_∇_ _|_
process to hit xτ at time τ and the diffusion process solving (7) is known as the diffusion bridge from
(x0, 0) to (xτ _, τ_ ). As previously noted, pτ _|t(xτ_ _|xt) in (7) refers to the transition density of (6)._

3.2 DIFFUSION MIXTURES

The proposed transport construction relies on a representation result for diffusion mixtures. We
present here an informal version and report the precise statement, the required assumptions, and the
proof in Appendix A.
**Theorem 1 (Diffusion mixture representation — informal). Let {X** _[λ]}, λ ∈_ Λ be a collection of
_diffusions with associated SDEs_ _dXt[λ]_ _t_
_on Λ, πt be the_ _-mixture of_ _π {t[λ]_ _[}][ and marginal densities][ {][π][λ][}][. Let][ L][ be a mixing distribution]_
_follows a SDE whose drift and diffusion coefficients are weighted averages of the corresponding L_ _{_ _[}][. Then there exists a diffusion process][ X][ with marginal][ π][t][.][ X]_
_coefficients in_ _dXt[λ]_ _t_
_{_ _[}][, where the weights are proportional to][ {][π][λ][}][ and to the mixing density.]_


-----

Theorem 1 is first established in Brigo (2002, Corollary 1.3) limitedly to finite mixtures and 1dimensional diffusions. The proof of Theorem 1 in Appendix A is more direct and extends the result
to the required multivariate setting. In Section 3.1 we introduced diffusion bridges mapping arbitrary
initial values x0 to arbitrary final values xτ . Let Π0,τ denote a generic bivariate distribution on
R[D] _× R[D]_ with marginals Π0, Πτ . We define the diffusion mixture M as the mixture of diffusion
bridges corresponding to (X0, Xτ ) Π0,τ . That is, we apply Theorem 1 to the collection of
_∼_
diffusion bridges (7) indexed by their initial and terminal values, λ = (x0, xτ ), Λ = R[D] _× R[D], with_
mixing distribution (dλ) = Π0,τ (dx0, dxτ ). By Theorem 1 the following SDE on [0, τ ] with initial
_L_
distribution Π0 has the same marginal distribution as M, in particular its terminal distribution is Πτ
_dXt = µ(Xt, t)dt + g(Xt, t)dWt,_


_µ(xt, t) = f_ (xt, t) + G(xt, t) _xt_ [ln][ p]τ _t[(][x]τ_ _[|][x]t[)]_ _[p][t][|][0][,τ]_ [(][x][t][|][x][0][, x][τ] [)] Π0,τ (dx0, dxτ )
_∇_ _|_ _πt(xt)_
Z

_A(xt,t)_

_πt(xt) =_ _pt|0,τ_ (xt|x0, xτ )Π|0,τ (dx0, dxτ ). {z }
Z


(8)


In (8), A(xt, t) gives the multiplicative drift adjustment factor for (6). A case of particular interest
occurs when Π0 puts all the mass on a single value x0. In the following we refer to A(xt, t, x0)
in stance of A(xt, t), and to πt|0(xt|x0) in stance of πt(xt), when it is necessary to distinguish
this specific case. We also extend the scope of Π to indicate the law of M . Indeed, we already
denoted with Π0,τ its initial-terminal distribution, and with πt its marginal density. Accordingly,
_A(xt, t) = EXτ_ Π(dxτ _xt)[_ _xt_ [ln][ p]τ _t[(][X]τ_ _[|][x]t[)]][. The transport from][ P]Z_ [to][ P] [is then achieved by]
_∼_ _|_ _∇_ _|_ _D_
Πτ = and _Z = Π0 (_ _Z can be arbitrarily defined). As_ is an empirical distribution, the
_PD_ _P_ _P_ _PD_
integral in (8) with respect to xτ reduces to averages over D. In summary, the diffusion X solution
of (8) realizes the proposed transport from _Z to_ by matching the marginal distribution of M .
_P_ _PD_

4 SDE CLASS

The starting point of the proposed transport is the unconstrained SDE (6). In this Section we define
SDEs which are realized through a time-change of simpler SDEs and which are general enough to
subsume the SDEs introduced in Song et al. (2021). Consider the D-dimensional SDEs

_dZt = Γ[1][/][2]dWt,_ (9)

_dZt = αtZtdt + Γ[1][/][2]dWt,_ (10)
where αt = 0 is a scalar function and G(Xt, t) = Γ introduces an arbitrary covariance structure. (9)
is the SDE of a correlated and scaled Brownian motion and ( ̸ 10) is the SDE of an Ornstein-Uhlenbeck
process driven by a correlated and scaled Brownian motion. The transition densities of (9) and (10)
are Gaussian (Appendix B). We denote both with _pt′|t, informally (9) is a special case of (10) with_
_αt = 0. SDE (10) in the time-homogenous case αt =_ _/2 has stationary distribution_ _D(0, Γ). We_
_−[1]_ _N_ _t_
now introduce the time-change. Let βt > 0 be a continuous function on e [0, τ ]. Then bt = 0 _[β][u][du]_
defines a monotonically (strictly) increasing function bt : [0, τ ] [0, bτ ]. The following SDEs on
_→_ R

[0, τ ] represent the class of dynamics for (1) and (6) on which we focus on the rest of this paper


_βtΓ[1][/][2]dWt,_ (11)


_dXt =_


_dXt = αtβtXtdt +_ _βtΓ[1][/][2]dWt,_ (12)

and G(x, t) = βtΓ. The standard time-change result for diffusions (p Øksendal, 2003, Theorem 8.5.1)
establishes that the processes Xt respectively from (11) and (12) are equivalent in law to their timescaled counterparts Zbt from (9) and (10). That is, SDEs (11) and (12) correspond to the evolution of
the simpler SDEs (9) and (10) under a non-linear time wrapping where time flows with instantaneous
intensity βt. For both (11) and (12) the time-change argument yields pτ _|t(y|x) =_ _pbτ |bt_ (y|x) for the
transition density of (6), and equivalently for the transition density qτ _t of (1). We thus obtain_
_|_

_pτ_ _|t(xτ_ _|xt) = ND(xτ_ ; xta(t, τ ), Γv(t, τ )) e (13)

for appropriate scalar functions a(t, τ ), v(t, τ ) with v(t, τ ) > 0 (Appendix B). By direct computation

_xt_ [ln][ p]τ _t[(][x]τ_ _[|][x]t[) = Γ][−][1]_ _xτ_ _a2(t, τ_ ) (14)
_∇_ _|_ _a(t, τ_ ) _v(t, τ_ ) _[,]_
 _[−]_ _[x][t]_


-----

_xτ_ [ln][ p]τ _t[(][x]τ_ _[|][x]t[) = Γ][−][1]_ _xta(t, τ_ ) _xτ_
_∇_ _|_ _−_


From Bayes theorem and the Markov property we have


1

(15)
_v(t, τ_ ) _[.]_


_pt|0,τ_ (xt|x0, xτ ) = ND (xt; x0abr(0, t, τ ) + xτ _abr(0, t, τ_ ), Γvbr(0, t, τ )), (16)

where once again abr(0, t, τ ), abr(0, t, τ ) and vbr(0, t, τ ) > 0 are scalar functions given in Appendix B. Finally, by direct computation

_xt_ [ln][ p]t 0,τ [(][x]t[|][x]0[, x]τ [) = Γ][−][1][ x][0][a][br][(0][, t, τ] [) +][ x][τ] _[a][br][(0][, t, τ]_ [)][ −] _[x][t]_ _._ (17)
_∇_ _|_ _vbr(0, t, τ_ )


These results provide all the analytical formulas required for the computation of the adjustment
factors A(xt, t), A(xt, t, x0) and of the training objectives used to approximate them (Section 6).

4.1 INTERPRETATION OF DENOISING TIME-REVERSED SDES

Song et al. (2021) introduces two specifications of (1), named VESDE and VPSDE, which are
respectively given by


_βve,rdWr,_ (18)


_dYr =_


_dYr =_
_−_ 2[1] _[β][vp][,r][Y][r][dr][ +]_


_βvp,rdWr._ (19)


See Appendix B for the functional form of βve,r and βvp,r. We thus recover (18) and (19) from (11)
and (12) with Γ = I and αt = −[1]/2. That is, VESDE and VPSDE correspond to a time change of
the much simpler SDEs for the standard Brownian motion and for the standard Langevin SDE

_dZr = dWr,_

_dZr =_
_−_ [1]2 _[Z][r][dr][ +][ dW][r][.]_

5 UNIFIED VIEW OF DRIFT ADJUSTMENTS

The linearity of SDEs (11) and (12), underlying our and Song et al. (2021) works, has the important
consequence that (14) and (15) are linear in xt. This in turn allow us to derive an alternative
representation for the drift adjustment in (8). Indeed, substituting (14) in (8) gives (Appendix A)

1 _a2(t, τ_ )

_G(x, t)A(x, t) = βt_ E (20)

 _a(t, τ_ ) _Xτ ∼Πτ_ _|t(dxτ |x)[[][X][τ]_ []][ −] _[x]_ _v(t, τ_ ) _[.]_

Similarly, for the time-reversal drift adjustment term in (3) we have (Appendix A)


1

(21)
_v(0, r)_ _[.]_


_G(x, r) ∇x_ [ln][ q]r[(][x][) =][ β]r


_a(0, r)_ E
_Xτ ∼Q0|r(dxτ |x)[[][X][τ]_ []][ −] _[x]_


The relations (20) and (21) provide a unified view of the inner workings of the DTRT and of the
DBMT targeting . In the following we always refer to sampling time t. Remember that r = τ _t_
_PD_ _−_
is the remaining sampling time. For ease of exposition we assume βt = 1, as shown in Section 4 the
term βt corresponds to a time-warping. The terms a(t, τ ), a(0, r) are “integrated scalings”. They are
equal to 1 for (11) and the same holds for (12) as r → 0. The terms v(t, τ ), v(0, r) are “integrated
variances”. They are equal to r for (11) and the same holds for (12) as r → 0. We commonly refer
to E[Xτ _x, t] for expectation terms in (20) and (21). Both drift adjustments (20) and (21) are thus_
_|_
essentially of the form (E[Xτ _x, t]_ _x)vr[−][1]_ where the term vr[−][1] diverges as r 0.
_|_ _−_ _→_

The expectations E[Xτ _x, t] are convex linear combinations of the samples x[(][n][)]_ from . Explicitly,
_|_ _D_
_E[Xτ_ _x, t] =_ _n=1_ _[ω][(][x, t][)][(][n][)][x][(][n][)][, where the weights][ ω][(][x, t][)][(][n][)][ are the probabilities, under the]_
_|_
distributions Q (time-reversal sampling process (3)) and Π (mixture of diffusions process M from
Section 3.2), of reaching each state x[(][n][)] at terminal time τ from x at time t. By construction, the

[P][N]
initial weights entering expectation (20) are all equal to [1]/N when X starts from a fixed value x0,
and are so on average when X0 is stochastic. The initial weights entering expectation (21) are


-----

on average approximately equal to [1]/N, depending on the quality of the approximation _Z_ _Qτ_ .
_P_ _≈_
Thus, E[Xτ _X0, 0] is an averaging of many samples x[(][n][)]. As time progresses, changes in Xt_
_|_
correspond to changes in E[Xτ _Xt, t] through changes in the weights ω(x, t)[(][n][)]. Eventually all_
_|_
mass concentrates on a single weight ω(x, t)[(][∗][)] corresponding to a dataset sample x[(][∗][)]. Ultimately,
the attractor dynamics implied by (E[Xτ _|x, t] −_ _x)vr[−][1]_ drive Xt to x[(][∗][)]. We provide an inspection
in Figure 1, where D(CIFAR) stands for the training portion of the CIFAR10 dataset, and Euler(T)
corresponds to the Euler scheme (Kloeden & Platen, 1992) applied with T discretization steps.

In the VESDE and VPSDE of Song et al. (2021) we have G(x, r) = βrI and reversing (21) gives

_x_ [ln][ q]r[(][x][) +][ x]
E _,_ (22)
_Xτ ∼Q0|r(dxτ |x)[[][X][τ]_ [] =][ v][(0][, r][)][ ∇]a(0, r)

where ∇y [ln][ q]r[(][y][)][ is the][ true score][. We can thus take a][ trained score model][ s]φ[(][x, r][)][ ≈∇]x [ln][ q]r[(][y][)][,]
plug it in (22), and verity the extent to which E[Xτ _x, t] has been approximated, see Figure 1._
_|_

1.0

0.8

0.6

0.4

0.2

0.0

0.0 0.2 0.4 0.6 0.8 1.0

Figure 1: VPSDE model — 2[nd] cells’ row: evolution of a trajectory of X over sampling time (its
terminal value Xτ is the generated sample) via the Euler(1000) discretization of (3) using the true
_score ∇y_ [ln][ q]r[(][y][)][ for][ D][(][CIFAR][)][; line-plot: weights’ evolution][ ω][(][X]t[, t][)][(][n][)][ for all][ x][(][n][)][ in][ D][(][CIFAR][)]
for the same X (cyclical palette, many weights cannot be distinguished as they remain close to 0); 1[st]
cells’ row: E[Xτ _Xt, t] evolution for the same X; 3[rd]_ and 4[th] cells’ rows: same as 1[st] and 2[nd] cells’
_|_
rows for another trajectory X, using the trained score model; 5[th] and 6[th] cells’ rows: same as 3[rd] and
4[th] cells’ rows for another trajectory X, using Euler(100).

We pause for a moment to review the findings of Figure 1 (see Appendix D for additional related
plots). Firstly, we can classify the dynamics of E[Xτ _Xt, t] and of the associated weights in three_
_|_
stages. In the 1[st] stage the weights do not move much. During the 2[nd] stage, roughly t ∈ [0.4, 0.6],
the weights’ mass gets distributed over a limited number of samples. Interestingly, the weights’
dynamics are not monotonic. As time progresses the weights’ mass shifts between different objects
from different classes. From the beginning of the 3[rd] stage all mass gets allocated to a single weight,
the terminal image is decided well in advance of the terminal time. These dynamics are suboptimal.
We would like to shorten the 1[st] stage, but it is associated with large values of βt (i.e. quick time
passing) which are required to decouple Qτ from . This is an intrinsic limitation of time-reversal
_PD_
approaches. It is also dubious that (partially) sampling multiple objects over t is beneficial for efficient
generative modeling when we make use only of the terminal sample. This issue applies to trained
models as well, as the 3[rd] row of Figure 1 shows. An interesting open question is how to obtain more
suitable dynamics, where perhaps class transitions happen rarely. Secondly, E[Xτ _Xt, t] provides_
_|_
a denoised representation of Xt across the whole 3[rd] stage. An alternative to the noise removal
step applied to Xτ in Song et al. (2021) is to consider E[Xτ _|Xt, t] as the sampling process instead._
Thirdly, Figure 1 makes it clear that lowering the number of discretization steps affects generative


-----

sampling in multiple ways. On the one hand the terminal sample Xτ is more noisy. This is not very
surprising: close to τ the drift adjustment is approximately (x[(][∗][)] _−_ _Xt)vr[−][1][, which is the drift of]_
a Brownian bridge. Bridge sampling is notoriously problematic (Bladt et al., 2016). On the other
hand larger discretization errors also significantly affect the dynamics of E[Xτ _Xt, t] resulting in_
_|_
less coherent samples. We remark that none of these insights could have been gained by observing
_Xt alone, i.e. the even cells’ rows of Figure 1. To conclude, (20) and (21) give an additional meaning_
to “denoising”. Neural network approximators need to map from a noisy input Xt to an adjustment
toward a smoother superimposition of samples. The desire to minimize the discrepancy between the
smoothness properties of Xt and that of E[Xτ _|Xt, t] motivates the developments of Section 8._

6 TRANSPORTS APPROXIMATION

As in Song et al. (2021), computing the multiplicative drift adjustment A(xt, t) requires (N )
_O_
operations. In this Section we introduce three training objectives for which unbiased and scalable, i.e.
_O(1) with respect to N_, MC estimators can be immediately derived.

The first training objective applies only to A(xt, t, x0). It relies on the identity (Appendix A)

_A(xt, t, x0) =_ _xt_ [ln][ π]t 0[(][x]t[|][x]0[)][ −∇]xt [ln][ p]t 0[(][x]t[|][x]0[)][.] (23)
_∇_ _|_ _|_

It is advantageous to consider the right-hand side of (23) because from (8) we know that πt|0(xt|x0)
has mixture representation. As in Song et al. (2021), we can rely on Vincent (2011) to obtain a
scalable objective to train a neural network approximator sφ(xt, t) _xt_ [ln][ π]t 0[(][x]t[|][x]0[)][, i.e.]
_≈∇_ _|_

LFD,DBMT(φ) = E _t_ _Xt_ [ln][ π]t 0[(][X]t[|][x]0[)][ −] _[s]φ[(][X]t[, t][)]_
_t_ (0,τ ),Xt Πt 0 _J_ _∇_ _|_
_∼U_ _∼_ _|_

h

= E _t_ _Xt_ [ln][ p]t 0,τ [(][X]t[|][x]0[, X]τ [)][ −][2][i][s]φ[(][X]t[, t][)] _,_ (24)
_t∼U_ (0,τ ),(Xt,Xτ )∼Πt,τ _|0_ _J_ _∇_ _|_

h

where _t = E[_ _Xt_ [ln][ p]t 0,τ [(][X]t[|][x]0[, X]τ [)][∥][2][]]−1 is a regularization term. [2][i]
_J_ _∥∇_ _|_

The remaining training objectives rely on the identities (20) and (21). The goal is directly approximate
the expectations of (20) and (21) which, as in Section 5, we denote with a generic E[Xτ _|x, t]. That is,_
we aim to train a neural network approximator sφ(x, t) ≈ E[Xτ _|x, t]. As conditional expectations_
are mean squared error minimizers, suitable objectives for the expectation terms of (20) and (21) are

LCE,DBMT(φ) = E _Xτ_ _sφ(Xt, t)_ _,_ (25)
_t∼U_ [0,τ ),(Xt,Xτ )∼Πt,τ _−_

h

LCE,DTRT(φ) = E _Y0_ _sφ(Yr, r)_ [2].[i] (26)
_r∼U_ [0,τ ),(Y0,Yr)∼Q0,r _−_

h

[2][i]

In Table 1 we summarize the operations needed to implement the plain MC estimators for the four
objectives considered in this work. We reference where to find the required quantities for SDEs
(11) and (12). The MC estimators for the Fisher divergence losses LFD, involve multiplications
_∗_
by Γ[−][1] (by (15) and (17)). Moreover, computing the drift adjustment at generation time requires
multiplications by Γ. In Section 8 we discuss how to manage the computational burden. An appealing
property of LCE, is that computing the drift adjustment only requires the application of simple
_∗_
scalar functions (see (20) and (21)), and that their MC estimators only requires sampling operations.
A further advantage of LCE, is that no regularization is required. In contrast, in the absence of
_∗_
regularization terms, LFD,∗ are divergent for t ≈ _τ due to the term vr[−][1]_ (Section 5).

L Sampling (r∼U (0,τ ],t∼U [0,τ )) Evaluation

LFD,DTRT _Y0∼PD, Yr∼Qr|0(dyr|Y0)(13)_ _∇Yr_ [ln][ q]r|0[(][Y]r[|][Y]0[)][(][15][)]
LFD,DBMT _Xτ ∼PD, (X0=x0), Xt∼Pt|0,τ (dxt|x0,Xτ )(16)_ _∇Xt_ [ln][ p]t|0,τ [(][X]t[|][X]0[,X]τ [)][(][17][)]
LCE,DTRT _Y0∼PD, Yr∼Qr|0(dyr|Y0)(13)_
LCE,DBMT _Xτ ∼PD, X0∼Π0|τ (dx0|Xτ ), Xt∼Pt|0,τ (dxt|X0,Xτ )(16)_

Table 1: Sampling and evaluation operations required to implement the proposed MC estimators.


-----

7 DBMT OVERVIEW AND NUMERICAL EXPERIMENT

In this section we finalize the DBMT construction, putting together the results of Sections 3, 4 and 6.
The unconstrained SDE follows (11) or (12). It remains to choose the mixing distribution Π0,τ . The
marginal Πτ needs to match, but there is flexibility in the choice of Π0 _τ_ . Song et al. (2021)
_PD_ _|_
derived a random ordinary differential equation (RODE) matching the marginal distribution of a
generative SDE, leading to faster sampling and to likelihood evaluation. RODE-matching requires
Π0 to have density. A natural implementation is given by the factorial distribution Π0,τ = _Z_
with PZ = ND(0, Γ) and the unconstrained SDE following (12) with αt = [1]/2 which preserves P _⊗PD_
_Z. If instead the DBMT starts from a fixed value x0, i.e. Π0,τ = δx0_, we can choose
_P_ _⊗PD_
_x0 =_ [1]/N _n=1_ _[x][(][n][)][a][(0][, τ]_ [)][−][1][ to remove the drift adjustment at][ t][ = 0][ (see (][20][)) and reduce the]
work required to transport x0 to . Finally, the use of non-factorial distributions can lead to a more
_PD_
efficient implementation, by linking the initial distribution to .

[P][N] _PD_

The training steps for the simplest objective (25) of Section 6 are reported in Algorithm 1. Batch
size is assumed to be 1 to ease the description. It is also assumed that Π0,τ is factorial, otherwise
the obvious modification applies to line 2 (Algorithm 2 is unaffected) where the endpoints are
sampled. At line 3 a random central time and the corresponding state are sampled. The function
optimizationstep implements a step of stochastic gradient descent update based on the loss L.
The corresponding sampling algorithm is reported in Algorithm 2 where the Euler(T) discretization is
assumed in line 6. Pt 0,τ (dxt _X0, Xτ_ ), a(t, τ ), v(t, τ ), βt are defined in Section 4. Section 8 shows
_|_ _|_
how to sample efficiently from Pt|0,τ (dxt|X0, Xτ ) and ND(0, Γ) in computer vision applications.


**Algorithm 1 DBMT training (LCE,DBMT)**
**Input:**, _Z, SDE (11) or (12), NN sφ(x, t)_
_PD_ _P_
**Output: trained sφ(x, t)**

1: repeat

3:2: _tX ∼Uτ ∼P[0, τD,) X, X0 ∼Pt ∼_ _PZt|0,τ_ (dxt|X0, Xτ )

4: _Xτ_ _sφ(Xt, t)_
_L ←_ _−_

5: _φ ←_ optimizationstep(φ, L)

6: until convergence [2]


**Algorithm 2 DBMT sampling (LCE,DBMT)**
**Input: PZ, SDE (11) or (12), trained sφ(x, t)**
**Output: Discretized path X0:T**

1: X0 _Z_
2: for ∼P s = 1, . . ., T do
3: _t_ (s 1) _T[τ]_ _[, x][ ←]_ _[X][s][−][1]_
_←_ _−_

4: _us_ _βt_ _a(t,τ1_ ) _[s][φ][(][x, t][)][ −]_ _[x]_ _av[2]((t,τt,τ))_
_←_

5: _s_ _D(0, Γ)_ 

6: _EXs ∼Nx+(f_ (x, t)+us) _T[τ]_ [+][g][(][x, t][)][p][ τ]T

7: end for ← _[E][s]_


We consider a toy numerical example with _Z =_ = [1]/3(δ 2 + δ0 + δ2), D = τ = 1. The
_P_ _PD_ _−_
unconstrained SDE follows the standard Brownian motion. We consider two mixing distributions:
independent mixing Π[⊥]0⊥,1 [where][ X][0] [and][ X][1] [are independent and fully dependent mixing][ Π]0[=],1 [where]
_X0 = X1. The results are reported in Figure 2. For both couplings the correct terminal distribution_
is recovered, as can be seen by taking the row-wise sum of the transition matrices. The initial_PD_
terminal distribution of X solving (8), which realizes the DBMT, is different from the corresponding
mixing distribution Π0,1 which is realized by the mixture process M . Π[⊥]0⊥,1 [results in a transition]
matrix of equal entries [1]/9, Π[⊥]0⊥,1 [results in a diagonal transition matrix of equal diagonal entries][ 1][/][3][.]


0.0 0.2 0.4 0.6 0.8 1.0


0.0 0.2 0.4 0.6 0.8 1.0


-2.0 0.0 2.0

0.24 0.07 0.01

0.08 0.19 0.08

0.01 0.08 0.25

x0


-2.0 0.0 2.0

0.30 0.03 0.00

0.03 0.28 0.03

0.00 0.03 0.30

x0


Figure 2: (1[st] (Π[⊥]0⊥,1[), 2][nd][ (][Π][=]0,1[) plots): marginal density of the diffusion mixture][ M][ in yellow, which]
matches the marginal density of X solving (8), 5 sample paths of X started at 0 in black; (3[rd] (Π[⊥]0⊥,1[),]
4[th] (Π[=]0,1[) plots): transition matrix of][ X][ from][ t][ = 0][ to][ t][ = 1][ estimated from 2000 samples.]


-----

8 NON-DENOISING DIFFUSIONS

In computer vision applications, images of resolution H×W corresponds to D = 3HW . The use of
an arbitrary covariance matrix Γ in (11) and (12) requires its Cholesky (or equivalent) decomposition
with cost O(D[3]). As the resolution increases the computational burden gets intractable very quickly.
Indeed, to the best of the authors’ knowledge, all prior DDPM literature only considers independent
transitions, that is Γ = I. We suggest to view SDEs (11) and (12) as corresponding to the space
discretization on an H×W grid of a spatio-temporal process defined over the spatial domain [0, 1][2].
Consider the Euler discretization of (the idea is to adopt a functional perspective:12): Xt X+∆(tt =+∆ Xt, st +) =αtβ XtX(tt, s∆t)++√α∆tβtEtXt, where(t, s)∆ Ett+ ∼N√∆Dt(0(,t, s Γ)),

_E_
where s ∈ [0, 1][2] defines space coordinates. That is, both X(t) and E(t) at each time t are random
processes over [0, 1][2]. We assume the innovations E(t) to be a Gaussian process (GP) for each t.

As the GPs E(t) are defined on a 2D domain we can leverage on scalable inference techniques from
spatial statistics. As an example, we consider the circulant embedding method (CEM) (Wood &
Chan, 1994; Dietrich & Newsam, 1997) which exploits a connection with the fast Fourier transform
(FFT). See Appendix C for a cursory review of the CEM. Consider an H×W uniform grid S of
size S = HW discretizing [0, 1][2], i.e. the support of images. For a stationary covariance function
the CEM samples E(t) on S with cost O(D ln(D)). This is close to the O(D) cost of sampling
from a pure white-noise process, and compares very favorably to the O(D[3]) cost of a Cholesky
decomposition. One limitation of CEM is that generated samples, while always Gaussian, might not
have the correct covariances. Whether this happens, and in that case the quality of the approximation,
depends on the covariance function. In Appendix C we select and fit an isotropic GP to the microscale
properties of D(CIFAR). For this estimated GP sampling is exact. Figure 3 shows samples from a
pure white-noise GP, i.e. Γ = I, (1[st] row) and from the fitted GP using CEM (2[nd] row). As noted in
Section 6, sampling is enough to implement the MC estimators for LCE,, but the MC estimators and
_∗_
drift adjustments for LCE, involve additional matrix multiplications by Γ and Γ[−][1]. The CEM allows
_∗_
to compute these at the same O(D ln(D)) cost if we define the GP E(t) on a 2D torus (Rue & Held,
2005, Chapter 2.1). This corresponds to introducing dependencies between “opposing” boundaries of

[0, 1][2]. Figure 3 (3[rd] row) shows some samples, in the highlighted patch the opposing-boundaries
dependency is evident. Either way, all samples from the 2[nd] and 3[rd] rows of Figure 3 match the
smoothness properties of D(CIFAR).


Figure 3: Spatial GP samples, see the main text for the description.

9 CONCLUSIONS

The DBMT construction of Section 3 is exact. The SDE class of Section 4 is tractable as it results in
linear diffusion bridges. The time-space factorization of the diffusion coefficient g(x, t) = _βtΓ[1][/][2]_

_[√]_
separates modeling concerns: βt corresponds to a time-wrapping, Γ can be efficiently modeled
by fitting the microscale properties of . Availability of GPU-accelerated FFT implementations
_PD_
motivates our focus on the CEM. Alternative scalable approaches abound, from Gaussian Markov
Random Fields (Rue & Tjelmeland, 2002; Rue, 2001) to Karhunen–Loève expansions (Betz et al.,
2014). It remains to apply the results of this work to perform an empirical benchmarking. Section 6
develops three novel training objectives, two of which with desirable properties compared to the
objective of Song et al. (2021), especially for non-factorial transitions. We remark the simplicity of the
proposed DBMT approach (Algorithms 1 and 2) compared to alternatives grounded in the Schrödinger
bridge problem (De Bortoli et al., 2021; Wang et al., 2021; Vargas et al., 2021). The understanding
of the target mappings ((20) and (21)) can guide the development of neural networks more closely
[matching the target structure compared to the U-Net default choice. https://github.com/?](https://github.com/?)
links to the code accompanying this paper which is made available under the MIT license.


-----

REFERENCES

Brian D.O. Anderson. Reverse-Time Diffusion Equation Models. Stochastic Processes and their
_Applications, 12(3):313–326, May 1982._

Wolfgang Betz, Iason Papaioannou, and Daniel Straub. Numerical Methods for the Discretization
of Random Fields by Means of the Karhunen–Loève Expansion. Computer Methods in Applied
_Mechanics and Engineering, 271:109–129, April 2014._

Mogens Bladt, Samuel Finch, and Michael Sørensen. Simulation of Multivariate Diffusion Bridges.
_Journal of the Royal Statistical Society. Series B (Statistical Methodology), 78(2):343–369, 2016._

Damiano Brigo. The General Mixture-Diffusion SDE and Its Relationship with an Uncertain-Volatility
Option Model with Volatility-Asset Decorrelation, December 2002.

Noel Cressie. Statistics for Spatial Data. John Wiley & Sons, 1993.

Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion Schrödinger
Bridge with Applications to Score-Based Generative Modeling, June 2021.

C. R. Dietrich and G. N. Newsam. Fast and Exact Simulation of Stationary Gaussian Processes
through Circulant Embedding of the Covariance Matrix. SIAM Journal on Scientific Computing,
18(4):1088–1107, July 1997.

U. G. Haussmann and E. Pardoux. Time Reversal of Diffusions. The Annals of Probability, 14(4):
1188–1205, October 1986.

Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural
_Information Processing Systems, volume 33, pp. 6840–6851, 2020._

Ioannis Karatzas and Steven E. Shreve. Brownian Motion and Stochastic Calculus. Number 113
in Graduate Texts in Mathematics. Springer, New York, 2nd ed edition, 1996. ISBN 978-0-38797655-6 978-3-540-97655-4.

Peter E. Kloeden and Eckhard Platen. Numerical Solution of Stochastic Differential Equations.
Springer Berlin Heidelberg, Berlin, Heidelberg, 1992. ISBN 978-3-642-08107-1 978-3-662-126165.

NV Krylov. Introduction to the Theory of Diffusion Processes, volume 142. Providence, 1995.

Annie Millet, David Nualart, and Marta Sanz. Integration by Parts and Time Reversal for Diffusion
Processes. The Annals of Probability, pp. 208–238, 1989.

L Chris G Rogers and David Williams. Diffusions, Markov Processes and Martingales: Volume 2:
_Itô Calculus, volume 2. Cambridge university press, 2000._

Havard Rue. Fast Sampling of Gaussian Markov Random Fields. Journal of the Royal Statistical
_Society. Series B (Statistical Methodology), 63(2):325–338, 2001._

Håvard Rue and Leonhard Held. Gaussian Markov Random Fields: Theory and Applications.
Number 104 in Monographs on Statistics and Applied Probability. Chapman & Hall/CRC, Boca
Raton, 2005. ISBN 978-1-58488-432-3.

Hååvard Rue and Hååkon Tjelmeland. Fitting Gaussian Markov Random Fields to Gaussian Fields.
_Scandinavian Journal of Statistics, 29(1):31–49, 2002._

Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep Unsupervised
Learning using Nonequilibrium Thermodynamics. In Proceedings of the 32nd International
_Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp._
2256–2265. PMLR, 2015.

Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and
Ben Poole. Score-Based Generative Modeling through Stochastic Differential Equations. In
_International Conference on Learning Representations, 2021._


-----

Simo Särkkä and Arno Solin. Applied Stochastic Differential Equations. Cambridge University Press,
first edition, April 2019. ISBN 978-1-108-18673-5 978-1-316-51008-7 978-1-316-64946-6.

Francisco Vargas, Pierre Thodoroff, Austen Lamacraft, and Neil Lawrence. Solving Schrödinger
Bridges via Maximum Likelihood. Entropy, 23(9):1134, September 2021.

Pascal Vincent. A Connection Between Score Matching and Denoising Autoencoders. Neural
_Computation, 23(7):1661–1674, July 2011._

Gefei Wang, Yuling Jiao, Qian Xu, Yang Wang, and Can Yang. Deep Generative Learning via
Schrödinger Bridge, June 2021.

Andrew T. A. Wood and Grace Chan. Simulation of Stationary Gaussian Processes in [0,1]d. Journal
_of Computational and Graphical Statistics, 3(4):409–432, 1994._

B. K. Øksendal. Stochastic Differential Equations: An Introduction with Applications. Universitext.
Springer, Berlin ; New York, 6th ed. edition, 2003. ISBN 978-3-540-04758-2.


-----

A THEORETICAL FRAMEWORK

A.1 ASSUMPTIONS

**Assumption 1 (SDE solution). A given D-dimensional SDE(f, g) with associated initial distribution**
_V0 and integration interval [0, τ_ ] admits a unique strong solution on [0, τ ].

Assumption 1 can be checked through the application of the standard existence and uniqueness
theorems for SDE solutions. Of particular relevance to our setting is the formulation of Krylov (1995,
Chapter 5, Theorem 1) that limits the monotonic requirement to, informally speaking, drifts that pull
the process toward infinities.

**Assumption 2 (SDE density). A given D-dimensional SDE(f, g) with associated initial distribution**
_V0 and integration interval [0, τ_ ] admits a marginal / transition density on (0, τ ) with respect to the
_D-dimensional Lebesgue measure that uniquely satisfies the Fokker-Plank / Kolmogorov-forward_
_partial differential equation (PDE)._

We refer to Särkkä & Solin (2019, Chapter 5) and to Karatzas & Shreve (1996, Chapter 5.7) for
connections between SDEs and PDEs.

All theoretical results of this work rely on simple algebraic manipulations and re-arrangements of
quantities of interest. The main complication stems from the need to justify differentiation and
integration exchanges, i.e. exchange of limits.

**Assumption 3 (exchange of limits). We assume that limits exchanges are justified in the steps marked**
_with (⋆) and (⋆⋆)._

Similarly, various steps in the derivations involve considering fractional quantities with densities
appearing in the denominators.

**Assumption 4 (positivity). For a given stochastic process, all finite-dimensional densities, condi-**
_tional or not, are strictly positive._

Assumption 4 is easy to verify. We resorted to the practical but somewhat unsatisfactory formulation
of Assumption 3 because in full generality it is complicated to give easy to check conditions. We just
note that when ΠT =, the limit exchange marked with (⋆) is always justified. So are the limits
_PD_
exchanges marked with (⋆⋆) when in addition Π0 puts all the mass to a fixed initial value, or (by
direct verification) when Π0 is Gaussian for the SDE class of Section 4. Thorough this paper, both in
the main text and in the proofs that follow, it is supposed that Assumptions 1, 2 and 4 are satisfied by
SDEs (1), (6) and (7). This is the case for the SDE class of Section 4, i.e. (11) and (12), for any Π0
with finite variance.

_Remark: For ease of exposition it is assumed thorough this paper that all diffusions take values in the_
state space R[D]. There is no impediment in extending the presented results to the case of diffusions
taking values in a subset X ⊂ R[D]. The obvious changes to Assumptions 1 to 4 apply, the proofs
carry over without substantial modifications. This extension could be of practical interest as images
are often represented as floating point values in [0, 1].

A.2 STATEMENT AND PROOF OF DIFFUSION MIXTURE REPRESENTATION THEOREM

**Theorem 2 (Diffusion mixture representation). Consider the family of D-dimensional SDEs on**
_t ∈_ [0, τ ] indexed by λ ∈ Λ

_dXt[λ]_ [=][ µ][λ][(][X]t[λ][, t][)][dt][ +][ σ][λ][(][X]t[λ][, t][)][dW]t[ λ][,]

(27)
_X0[λ]_ _[∼V]0[λ][,]_

_where the initial distributions V0[λ]_ _[and the BMs][ W][ λ]t_ _[are all independent. Let][ ν]t[λ][, t][ ∈]_ [(0][, τ] [)][ denote]
_the marginal density of Xt[λ][. For a generic mixing distribution][ L][ on][ Λ][, define the mixture marginal]_
_density νt for t ∈_ (0, τ ) and the mixture initial distribution V0 by


_νt[λ][(][x][)][L][(][dλ][)][,]_ _V0(dx) =_


_V0[λ][(][dx][)][L][(][dλ][)][.]_ (28)


_νt(x) =_


-----

_Consider the D-dimensional SDE on t ∈_ [0, τ ] defined by


Λ _[µ][λ][(][x, t][)][ν]t[λ][(][x][)][L][(][dλ][)]_

_νt(x)_

Λ _[σ][λ][(][x, t][)][ν]t[λ][(][x][)][L][(][dλ][)]_

_νt(x)_


_µ(x, t) =_

_σ(x, t) =_


(29)


_dXt = µ(Xt, t)dt + σ(Xt, t)dWt,_
_Y0_ 0.
_∼V_

_It is assumed that all diffusion processes X_ _[λ]_ _and the diffusion process X solving (29) satisfy the_
_regularity assumptions Assumptions 1, 2 and 4 and that Assumption 3 holds. Then the marginal_
_distribution of the diffusion X is νt._

_Proof of Theorem 2. We start by establishing that the law of X is indeed given by the solution of_
(29). In this proof we make use of the following notation: for f scalar-valued (f )t = _dtd_ _[f]_ [, for][ a]

_d_ _d[2]_
vector-valued (a)x = _i=1_ _dxi_ _[a][, for][ A][ matrix-valued][ (][A][)]xx_ [=][ P]i,j[D] =1 _dxidxj_ _[A][. This notation]_

allows for a compact representation of PDEs reminiscent of the 1-dimensional setting. Then, for
0 < t < τ we have that[P][D]

(ν(x, t))t = Λ _ν[λ](x, t)L(dλ)_ _t_
Z 


_ν[λ](x, t)_


_t[L][(][dλ][)]_ (⋆⋆)


_x_ [+ 1]2


_µ[λ](x, t)ν[λ](x, t)_


_σ[λ](x, t)ν[λ](x, t)_


= _µ[λ](x, t)ν[λ](x, t)_ _σ[λ](x, t)ν[λ](x, t)_

Λ _x_ [+ 1]2 _xx[L][(][dλ][)]_

Z

  _µλ(x, t)νλ(x, t)_   _σλ(x, t)νλ(x, t)_

= _ν(x, t)_ + [1] _ν(x, t)_ (dλ)

Λ _ν(x, t)_ _x_ 2 _ν(x, t)_ _xxL_

Z    

_µ[λ](x, t)ν[λ](x, t)_ _σ[λ](x, t)ν[λ](x, t)_

= (dλ)ν(x, t) + [1] (dλ)ν(x, t) _._ (⋆⋆)

Λ _ν(x, t)_ _L_ _x_ 2 Λ _ν(x, t)_ _L_ _xx_

Z  Z 

The second line is an exchange of limits, the third line is the application of the Fokker-Plank PDEs
for the collection of processes X _[λ], the fourth line is a rewriting in terms of ν(y, t), the last line is_
another exchange of limits. The result follows by noticing that the last line gives the Fokker-Plank
representation of (29).

A.3 DRIFT ADJUSTMENTS

Limitedly to this section, we lighten the notation by removing subscripts from probability measures
and densities. The missing time points can be inferred without ambiguity from the variables.

A.3.1 DRIFT ADJUSTMENT IDENTITIES FOR CONSTANT INITIAL VALUE

First identity:


_p(xτ_ _xt)_
_xt_ [ln] _|_

_p(xτ_ _x0)_ [Π(][dx][τ] [)]

Z _|_

_xt_ _[p][(][x]τ_ _[|][x]t[)]_ _p(xτ_ _xt)_
= _∇_ _p(xt_ _x0)Π(dxτ_ ) _|_ (⋆)
_p(xτ_ _x0)_ _|_ _p(xτ_ _x0)_ _[p][(][x][t][|][x][0][)Π(][dx][τ]_ [)]
Z _|_ Z _|_

_p(xτ_ _, xt_ _x0)_

= _xt_ [ln][ p][(][x]τ _[|][x]t[)]_ _[p][(][x][τ]_ _[, x][t][|][x][0][)]_ Π(dxτ ) _|_ Π(dxτ )
_∇_ _p(xτ_ _x0)_ _p(xτ_ _x0)_
Z _|_ Z _|_

= _∇xt_ [ln][ p][(][x]τ _[|][x]t[)][p][(][x]t[|][x]0[, x]τ_ [)Π(][dx]τ [)] _π(xt|x0)_
Z 

= A(xt, t, x0).


-----

Second identity:


_p(xτ_ _xt)_
_xt_ [ln] _|_

_p(xτ_ _x0)_ [Π(][dx][τ] [)]

Z _|_

_p(xτ_ _xt)_
= _xt_ [ln] _|_ _xt_ [ln][ p][(][x]t[|][x]0[)]
_∇_ _p(xτ_ _x0)_ _[p][(][x][t][|][x][0][)Π(][dx][τ]_ [)][ −∇]
Z _|_

= ∇xt [ln][ π][(][x]t[|][x]0[)][ −∇]xt [ln][ p][(][x]t[|][x]0[)][.]


A.3.2 DRIFT ADJUSTMENTS AS EXPECTATIONS

To establish (20) notice that from (14) we have

_G(xt, t)A(xt, t)_

_xτ_ _a2(t, τ_ ) _p(xt_ _x0, xτ_ )
= βtΓΓ[−][1] _|_ Π0,τ (dx0, dxτ )

_a(t, τ_ ) _v(t, τ_ ) _π(xt)_

Z  _[−]_ _[x][t]_

1 _π(xt_ _x0, xτ_ ) _a2(t, τ_ )

= βt _xτ_ _|_ Π0,τ (dx0, dxτ ) _xt_

_a(t, τ_ ) _π(xt)_ _−_ _v(t, τ_ )

 Z 

1 _a2(t, τ_ )

= βt E

_a(t, τ_ ) _Xτ_ Π(dxτ _xt)[[][X][τ]_ []][ −] _[x][t]_ _v(t, τ_ ) _[.]_

 _∼_ _|_ 


To establish (21) note that from (15) we have

_G(yr, r)_ _yr_ [ln][ q][(][y]r[) = Γ] _yr_ [ln][ q][(][y]r[|][y]0[)] _[q][(][y][r][|][y][0][)]_
_∇_ _∇_ _q(yr)_
Z _[P][D][(][dy][0][)]_

_a(0, r)y0_ _yr_ _q(yr_ _y0)_
= βrΓΓ[−][1] _−_ _|_

_v(0, r)_ _q(yr)_

Z   _[P][D][(][dy][0][)]_

_q(yr_ _y0)_ 1

= βr _a(0, r)_ _y0_ _|_

_q(yr)_ _v(0, r)_

 Z _[P][D][(][dy][0][)][ −]_ _[y][r]_

1

= βr _a(0, r)_ E

_Y0_ _Q(dx0_ _yr)[[][Y][0][]][ −]_ _[y][r]_ _v(0, r)_ _[.]_
 _∼_ _|_ 


B SDES CLASS FORMULAS

The transition densities of (9) and (10) are given respectively by

_pbm,τ_ _t(zτ_ _zt) =_ _D (zτ_ ; zt, Γ(τ _t)),_
_|_ _|_ _N_ _−_

1 1

_pou,τ_ _t(zτ_ _zt) =_ _D_ _zτ_ ; zte[α][t][:][τ][ (][τ] _[−][t][)], Γ_ _e[2][α][t][:][τ][ (][τ]_ _[−][t][)]_ _._

e _|_ _|_ _N_ 2αt _−_ 2ατ

  

Here we used the notatione _f t:τ =_ _τ_ 1 _t_ _τt_ _[f][u][du][, i.e.][ f][ t][:][τ][ is the average value of a function][ f][u][ on the]_

_−_
interval [t, τ ]. The time-homogenous case of (10), where αt:τ = α, is thus immediately recovered.
R

The scalar functions abm(t, τ ), aou(t, τ ), vbm(t, τ ) and vou(t, τ ) are given by


_abm(t, τ_ ) = 1, _vbm(t, τ_ ) = bτ _bt,_
_−_

1
_aou(t, τ_ ) = e[α][bt] [:][bτ][ (][b][τ][ −][b][t][)], _vou(t, τ_ ) = _e[2][α][bt]_ [:][bτ][ (][b][τ][ −][b][t][)]

2αbt _−_


2αbτ


The scalar functions vbr(0, t, τ ), abr(0, t, τ ) and abr(0, t, τ ) are given by

_v(0, t)v(t, τ_ )
_vbr(0, t, τ_ ) =

_v(0, t)a[2](t, τ_ ) + v(t, τ ) _[,]_

_v(t, τ_ )a(0, t)
_abr(0, t, τ_ ) = _v(0, t)a[2](t, τ_ ) + v(t, τ ) _[,]_

_v(0, t)a(t, τ_ )
_abr(0, t, τ_ ) =

_v(0, t)a[2](t, τ_ ) + v(t, τ ) _[.]_


-----

The scalar functions βve,r and βvp,r are given by

2r

_βve,r = σmin[2]_ _σσmaxmin_ 2 log _[σ]σ[max]min_ _,_ _βvp,r =_ _β¯min + r_ _β¯max −_ _β[¯]min_ _._

 

    

The constants σmin, σmax, _β[¯]min,_ _β[¯]max depend in part on the dataset considered, but are consistently_
chosen to have βve,r, βvp,r small for r ≈ 0 and large for r ≈ _τ_ .

The approximating distributions PZ in Song et al. (2021) are PZ[ve] [=][ N][D][(0][, Iσ]max[2] [)][ for VESDE,]
_PZ[vp]_ [=][ N][D][(0][, I][)][ for VPSDE.]

C ADDITIONAL MATERIAL

C.1 CLOSELY RELATED WORK

A work closely related to the present paper is that of Wang et al. (2021) as it similarly avoids the
time-reversal construction. Wang et al. (2021) construct a 2-stages diffusion process from a constant
initial value x0 to by relying on the theory of Schrödinger bridges. The most notable differences
_PD_
with respect to the DBMT transport are: (i) the dynamics considered in Wang et al. (2021) are less
general, in our notation they correspond to f ( _·_ ) = 0, g( _·_ ) = σI for a fixed scalar σ; (ii) the transport
proposed in Wang et al. (2021) necessarily starts from x0, the general result of (8) allows for (almost)
arbitrary initial distributions and initial-terminal dependencies. For an initial x0, i.e. for case of
_A(xt, tx0) in Section 3.2 and for the more limited dynamics considered in Wang et al. (2021), the_
achieved transport is the same. In this sense the DBMT generalizes the first stage diffusion of Wang
et al. (2021). It is interesting to note that in the case of a constant x0 the DBMT can also be obtained
by an application of Doob h-transforms as we show in the following section.

We now review two additional works grounded in the Schrödinger bridge problem: De Bortoli et al.
(2021); Vargas et al. (2021). Both works rely on the Iterative Proportional Fitting (IPF) procedure
to solve the (dynamic) Schrödinger bridge problem. Both works leverage on time-reversal results
to carry out the alternated Schrödinger half-bridge IPF iterations. The main difference between
the two works is that De Bortoli et al. (2021) estimates the optimal SDE drifts via neural network
approximations and score-matching, while Vargas et al. (2021) relies on Gaussian Processes and
maximum likelihood fitting. The work of De Bortoli et al. (2021) can be seen as an extension of Song
et al. (2021), and similarly to our work allows the use of shorter time intervals. Compared to our
proposal, it solves a harder problem but also presents additional difficulties. Training is more involved
as all the neural network approximations, one for each IPF iterate, need to converge. Moreover, there
is limited guidance on how to optimally choose the number of integration steps over the number of
IPF iterates.

C.2 CONNECTION WITH DOOB H-TRANSFORMS

The previously established identity

_A(xt, t, x0) =_ _xt_ [ln] _pτ_ _|t(xτ_ _|xt)_
_∇_ Z _pτ_ _|0(xτ_ _|x0)_ [Π][τ] [(][dx][τ] [)][,]

shows that the drift adjustment can be equivalently expressed as


_µ(xt, t) = f_ (xt, t) + G(xt, t) _xt_ _[h][(][x]t[, t][)][,]_ _h(xt, t) = ln_ _pτ_ _|t(xτ_ _|xt)_
_∇_ Z _pτ_ _|0(xτ_ _|x0)_ [Π][τ] [(][dx][τ] [)][,]

as x0 is a constant. It can be verified that the h function satisfies the required space-time regularity
property (Särkkä & Solin, 2019, Eq. (7.73)). As such, it is a genuine Doob h-transform. That
_p[h]t[′]_ _t[(][x][t][′]_ _[|][x][t][) =][ p][t][′][|][t][(][x][t][′]_ _[|][x][t][)][h][(][x][t][′]_ _[, t][′][)][/h][(][x][t][, t][)][ is the transition density of the DBMT transport from]_
_|_
_δx0 to Πτ follows by direct computation._

C.3 GP MODELLING ON CIFAR10

For simplicity, we assume a factorial distribution over the channels and an isotopic stationary
covariance function. We rely on the semivariogram approach (Cressie, 1993) to compare how


-----

different covariance functions fit D(CIFAR). A semivariogram is a measure of dependency across
space. In the case of an isotropic stationary covariance it simplifies to a scalar function of the
Euclidean distance between points:rate of decrease of γ(∥∆s∥) toward γ 0 as(∥∆ ∥s∆∥s) =∥→ E0[(∆ gives a measure of the infinitesimal spatialxs)[2]]/2 with ∆xs = xs+∆s − _xs. The_
dependency, i.e. the smoothness of the spatial process. Semivariograms corresponding to different
covariance functions are here fitted to their empirical counterparts via a weighted minimum-leastsquares procedure (Cressie, 1993). Figure 4 illustrates the exponential and RBF semivariogram
fits for two images of D(CIFAR). The exponential covariance, which corresponds to rougher paths,
provides a much better fit to the shown samples. This result is consistent across D(CIFAR). We
remark that Γ = I corresponds to a pure white-noise process with a perfectly flat semivariogram
which would clearly result in a very poor fit to the empirical semivariograms shown in Figure 4.
Based on these findings, we model the innovations of each image channel as a GP with exponential
covariance function with length-scale θ = 0.205, the median estimated value (Figure 4 (right)). We
match the marginal variance to that of D(CIFAR), σ[2] = 0.063.

1.4

1.2

1.0

( )p 0.8

0.6

0.4

0.2


0.0

0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4

Figure 4: Empirical semivariograms (dots) and fitted exponential (solid lines) and RBF (doted lines)
semivariograms for the 1[st] (left) and 2[nd] (center) image of D(CIFAR); histograms (bins) and medians
(lines) of the distributions of the length-scale parameters in the exponential variogram model over
_D(CIFAR) (right); colors represent the RGB channels._


C.4 CIRCULANT EMBEDDING METHOD

Figure 5: Circulant embedding covariance matrices, see the appendix’s main text for the description.


We start by providing a cursory explanation leading to efficient sampling in the 1D case, before
giving the intuition behind the extension to the 2D case. We refer to Wood & Chan (1994); Dietrich
& Newsam (1997) for a complete explanation and to Rue & Held (2005) for the results underlying
efficient density (likelihood) computation. Let [0, 1] be the spatial domain of interest. Let S =
_{si}i[M]=1_ [be a uniform grid (regular lattice) discretizing][ [0][,][ 1]][, where the points][ s][i][ are assumed to be]
ordered. The first key observation is that for a stationary covariance function ρ( _·_ _, ·_ ) the covariance
matrix C with entries Ci,j = ρ(sj, sj) is symmetric and Toeplitz, i.e. with constant-diagonals. See
Figure 5 (leftmost) for an example where M = 9. A property of symmetric Toeplitz matrices is
that they can always be embedded in larger symmetric circulant matrices. A circulant matrix of size
_M_ _[′]×M_ _[′]_ is defined by the property that all its rows (and columns) are obtained by cycling through
the same M _[′]-dimensional vector. Circulant matrices correspond to covariance matrices of GPs_
defined on a (here 1D) torus (Rue & Held, 2005, Chapter 2.1). The circulant embedding matrix just
introduced corresponds to an artificial enlargement of the spatial domain [0, 1] to a larger interval
leading to a torus. See Figure 5 (2[nd] from left) for a circulant embedding of C. The second key
observation is that a circulant matrix is diagonalized by the 1D FFT matrix. Having obtained the
eigenvalues of C, efficient sampling on the enlarged domain is achieved by the 1D FFT applied to


-----

complex standard random numbers multiplied by the (square root of the) eigenvalues. The real and
imaginary part of the generated samples are independent. The main issue with the CEM is that the
circulant matrix embedding might fail to be positive definite. The issue can be avoided by considering
progressively larger embeddings, see the theoretical and empirical findings of Dietrich & Newsam
(1997). Otherwise, a level of approximation can be accepted by modifying the covariance function or
by truncating the eigenvalues to be positive.

The development of the 2D CEM follows very similar steps. The domain of interest is now [0, 1][2],
the uniform grid is S = {si,j}i,j[M] =1 [and the points][ s][i,j][ are assumed to be lexicographically ordered.]
The stationarity of the covariance functions results in a symmetric block-Toeplitz covariance matrix,
as shown in Figure 5 (3[rd] from left). Again, symmetric block-Toeplitz matrices can be embedded
in symmetric block-circulant matrices, as exemplified by Figure 5 (rightmost, zooming might be
required to see the block structure). Block-circulant matrices can be shown to be diagonalized by the
2D FFT matrix, and efficient sampling follows from similar steps to the ones seen in the 1D case.

D ADDITIONAL FIGURES

1.0

0.8

0.6

0.4

0.2

0.0

0.0 0.2 0.4 0.6 0.8 1.0

Figure 6: Same as Figure 1 for VESDE model.


-----

Figure 7: Additional samples from the trained VPSDE model of Song et al. (2021), E[Xτ _Xt, t] and_
_|_
_Xt (interleaved rows) over sampling time t, Euler(1000) (top 16 rows) and Euler(100) (bottom 16_
rows).


-----

Figure 8: Additional samples from the trained VESDE model of Song et al. (2021), E[Xτ _Xt, t] and_
_|_
_Xt (interleaved rows) over sampling time t, Euler(1000) (top 16 rows) and Euler(100) (bottom 16_
rows).


-----

