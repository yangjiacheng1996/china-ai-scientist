# HOW DOES THE TASK LANDSCAPE AFFECT MAML PERFORMANCE?

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Model-Agnostic Meta-Learning (MAML) has become increasingly popular for
training models that can quickly adapt to new tasks via one or few stochastic
gradient descent steps. However, the MAML objective is significantly more difficult to optimize compared to standard non-adaptive learning (NAL), and little
is understood about how much MAML improves over NAL in terms of the fast
adaptability of their solutions in various scenarios. We analytically address this
issue in a linear regression setting consisting of a mixture of easy and hard tasks,
where hardness is related to the rate that gradient descent converges on the task.
Specifically, we prove that in order for MAML to achieve substantial gain over
NAL, (i) there must be some discrepancy in hardness among the tasks, and (ii)
the optimal solutions of the hard tasks must be closely packed with the center far
from the center of the easy tasks optimal solutions. We also give numerical and
analytical results suggesting that these insights apply to two-layer neural networks.
Finally, we provide few-shot image classification experiments that support our
insights for when MAML should be used and emphasize the importance of training
MAML on hard tasks in practice.

1 INTRODUCTION

Large-scale learning models have achieved remarkable successes in domains such as computer vision
and reinforcement learning. However, their high performance has come at the cost of requiring huge
amounts of data and computational resources for training, meaning they cannot be trained from
scratch every time they are deployed to solve a new task. Instead, they typically must undergo a
single pre-training procedure, then be fine-tuned to solve new tasks in the wild.

_Meta-learning aims to address this problem by extracting an inductive bias from a large set of_
pre-training, or meta-training, tasks that can be used to improve test-time adaptation. Model-agnostic
_meta-learning (MAML) (Finn et al., 2017), one of the most popular meta-learning frameworks,_
formulates this inductive bias as an initial model for a few gradient-based fine-tuning steps. Given
that for the model can be fine-tuned on any meta-test task before evaluating its performance, MAML
aims to learn the best initialization for fine-tuning among a set of meta-training tasks. To do so, it
executes an episodic meta-training procedure in which the current initialization is adapted to specific
tasks in an inner loop, then the initialization is updated in an outer loop. This procedure has led to
impressive few-shot learning performance in many settings (Finn et al., 2017; Antoniou et al., 2018).

However, in some settings MAML’s inner loop adaptation and the second derivative calculations
resulting thereof may not justify their added computational cost compared to traditional, no-inner-loop
pre-training. Multiple works have suggested that MAML’s few-shot learning performance may not
drop when executing the inner loop adaptation for only a fraction of the model parameters (Raghu
et al., 2019; Oh et al., 2020) or, in some cases, by ignoring the inner loop altogether (Bai et al.,
2020; Chen et al., 2019). Unfortunately, the underlying reasons for MAML’s behavior are still not
well-understood, making it difficult to determine when inner loop adaptation during meta-training
yields meaningful benefit.

In this work, we investigate when and why MAML’s inner loop updates provide significant gain
for meta-test time adaptation. To achieve this, we focus on how the meta-training loss landscape
induced by the MAML objective affects adaptation performance compared to the loss landscape
induced by the classical objective without inner loop adaptation, which we term the Non-Adaptive


-----

(II), not (I): Large R, Small rH, pH/pE = 1 (I), not (II): pH/pE ≪1, Large R, rH (I), not (II): pH/pE ≪1, Small R, rh (I) and (II): pH/pE ≪1, Large R, Small rH

Hard task solutions
Easy task solutions
_wMAML_
_wNAL_

NAL NAL NAL NAL

MAML MAML MAML MAML

0 1 2 3 4 5 0 2 4 6 8 10 12 0.00 0.05 0.10 0.15 0 2 4 6

Excess Risk Excess Risk Excess Risk Excess Risk


(a)


(b)


(c)


(d)


Figure 1: MAML, NAL excess risks and optimal solutions in various environments.

Learning (NAL) method. Notably, MAML should always perform at least as well as NAL because its
meta-training procedure aligns with the meta-test time evaluation of performance after a few steps of
task-specific SGD. Thus, our goals are to quantify the gain provided by MAML and determine the
scenarios in which this gain is most significant. We start by studying the multi-task linear regression
setting since it allows us to compare the exact solutions and excess risks for MAML and NAL.
In doing so, we obtain novel insights on how MAML deals with hard tasks differently than NAL,
and show that MAML achieves significant gain over NAL only if certain conditions are satisfied
relating to the hardness and geography of the tasks. These observations provide a new understanding
of MAML that is distinct from the representation learning perspective considered in recent works
(Raghu et al., 2019; Du et al., 2020; Tripuraneni et al., 2020). We then give theoretical and empirical
evidence generalizing these insights to neural networks.

In particular, our main observations are best captured in the setting in which tasks are either “hard” or
“easy”. We let ρH be the hardness parameter for the hard tasks, and ρE be the hardness parameter
for the easy tasks, where ρE > ρH (smaller hardness parameter means more hard). As we measure
the hardness of a task by the rate at which gradient descent converges for the task, in this case,
the hardness parameter is the task loss function’s strong convexity parameter. For a particular task
environment, we let R be the dimension-normalized distance between the average of easy tasks’
optimal solutions and the average of hard tasks’ optimal solutions, and let rH quantify the variance,
or dispersion, of the hard tasks’ optimal solutions. Assuming that _[ρ]ρ[H]E_ [(1][ −] _[ρ]ρ[H]E_ [)][2][ ≫] _m[d]_ [, where][ d][ is]

the problem dimension and m is the number of samples used for adaptation, then the ratio of the
expected excess risks after task-specific adaptation of the NAL and MAML solutions is approximately
(Corollary 2):task environment satisfies (informal):EEmm(w(wMAMLERM )) _[≈]_ [1 +][ R]rH[2] [. Thus, the largest gain for MAML over NAL occurs when the]

I. Hardness discrepancy: the hard tasks are significantly more difficult than the easy tasks
_ρH_
_ρE_

II. Benign geography:[≪] [1][, without being impossibly hard (] the optimal solutions of the hard tasks have small variance[ρ][H][ ≥] _[c >][ 0][), and]_ _rH_, and the
distance between the hard and easy task centers R is large.


Figure 1 summarizes observations (I) and (II) by plotting locations of the easy and hard tasks’ optimal
solutions sampled from four distinct task environments and the corresponding solutions and excess
risks for NAL and MAML. The environment in subfigure (a) violates (I) since _[ρ]ρ[H]E_ [= 1][. Subfigures]

(b)-(c) show environments with either small R or large rh, so (II) is not satisfied. In contrast, the
environment in subfigure (d) has small rH, large R, and _[ρ]ρ[H]E_ [=0][.][2][, in which case as expected MAML]

achieves the largest gain over NAL, _EEmm(w(wMAMLNAL)_ ) _[≈]_ [3][.]

**Summary – Why MAML. We show theoretically and empirically that MAML outperforms standard**
training (NAL) in linear and nonlinear settings by finding a solution that excels on the more-difficultto-optimize (hard) tasks, as long as the hard tasks are sufficiently similar. Our work thus highlights
the importance of task hardness and task geography to the success of MAML.

1.1 RELATED WORK

A few works have explored why MAML is effective. Raghu et al. (2019) posed two hypotheses
for MAML’s success in training neural networks: rapid learning, meaning that MAML finds a
set of parameters advantageous for full adaptation on new tasks, and feature reuse, meaning that


-----

MAML learns a set of reusable features, among the tasks. The authors’ experiments showed that
MAML learns a shared representation, supporting the feature reuse hypothesis. Motivated by this
idea, Saunshi et al. (2020) proved that Reptile (Nichol et al., 2018), a similar algorithm to MAML,
can learn a representation that reduces the new-task sample complexity in a two-task, linear setting.
Conversely, Oh et al. (2020) gave empirical evidence that MAML performance does not degrade
when forced to learn unique representations for each task, and Goldblum et al. (2020) showed that
while some meta-learning algorithms learn more clustered features compared to standard training, the
same cannot be said of MAML. Moreover, a series of works have shown that removing the inner loop
and learning a distinct classifier for each class of images in the environment can yield representations
as well-suited for few-shot image classification as MAML’s (Chen et al., 2019). In this work, we
take a more general, landscape-based perspective on the reasons for MAML’s success, and show that
MAML can still achieve meaningful gain without necessarily learning a representation.

Much of the theory surrounding MAML and meta-learning more broadly has focused on linear
settings, specifically multi-task linear regression or the related linear centroid problem (Denevi et al.,
2018). Some works have studied how to allocate data amongst and within tasks (Cioba et al., 2021;
Bai et al., 2020; Saunshi et al., 2021). Denevi et al. (2018) considered meta-learning a common mean
for ridge regression, and Kong et al. (2020) studied whether many small-data tasks and few large-data
tasks is sufficient for meta-learning. Other works have examined the role of the inner loop SGD
step size in meta-learning approaches (Bernacchia, 2021; Charles & Konecnˇ y, 2020; Wang et al.,`
2020b), while Gao & Sener (2020) studied the trade-off between the accuracy and computational
cost of the NAL and MAML training solutions. However, unlike our work, these works either did not
consider or did not provide any interpretable characterization of the types of task environments in
which MAML is effective. Other theoretical studies of MAML and related methods have focused on
convergence rates in general settings (Fallah et al., 2020; Rajeswaran et al., 2019; Zhou et al., 2019;
Ji et al., 2020a;b; Collins et al., 2020; Wang et al., 2020a) and excess risk bounds for online learning
(Finn et al., 2019; Balcan et al., 2019; Khodak et al., 2019). Like the current work, Fallah et al. (2020)
noticed that the MAML solution should be closer than the NAL solution to the hard tasks’ global
minima, but the authors neither quantified this observation nor further compared MAML and NAL.

2 PROBLEM SETUP: TRAINING TO ADAPT

We aim to determine when and why MAML yields models that are significantly more adaptable
compared to models obtained by solving the traditional NAL problem in multi-task environments. To
this end, we consider the gradient-based meta-learning setting in which a meta-learner tries to use
samples from a set of tasks observed during meta-training to compute a model that performs well
after one or a few steps of SGD on a new task drawn from the same environment at meta-test time.

Specifically, we follow Baxter (1998) by considering an environment p which is a distribution over
tasks. Each task Ti is composed of a data distribution µi over an input space X and a label space
_Y. We take our model class to be the family of functions {hw : X →Y : w ∈_ R[D]} where hw is
a model parameterized by w. The population loss fi(w) on task i is the expected value of the loss
of hw on samples drawn from µi, namely fi(w) := E(x,y) _µi_ [ℓ(hw(x), y)], where ℓ is some loss
_∼_
function, such as the squared or cross entropy loss.

During training, the meta-learner sampleseach sampled task _i. The meta-learner uses this data to compute an initial model T tasks from p and n points {(x[j]i_ _[, y]i[j][)][}]j[n]=1 w, which is[∼]_ _[µ]i[n]_ [for]
_T_
then evaluated as follows. First, the meta-learner samples a new task _i_ _p and m labeled points_
_T_ _∼_
_{(xˆ[j]i_ _[,][ ˆ]yi[j][)][}]j[m]=1_ _[∼]_ _[µ]i[m][. Next, it updates][ w][ with one step of stochastic gradient descent (SGD) on the]_
loss functiondenote the matrix and vector containing the fi using those m samples and step size m feature vectors and their labels, respectively, the update α. Namely, letting **X[ˆ]** _i ∈_ R[m][×][d] and ˆyi ∈ R[m]
of w is given by wi := w − _α∇wf[ˆ]i(w;_ **X[ˆ]** _i, ˆyi), where_ _f[ˆ]i(w;_ **X[ˆ]** _i, ˆyi):=_ _m[1]_ _mj=1_ _[ℓ][(][h][w][(]x[ˆ][j]i_ [)][,][ ˆ]yi[j][)][ is]

the empirical average of the loss on the m samples in (X[ˆ] _i, ˆyi). The test loss of w is the expected_
P
population loss of wi, where the expectation is taken over tasks and the m samples, specifically

_Fm(w) := EiE( ˆXi,yˆi)_ _fi(w_ _α_ _fi(w;_ **X[ˆ]** _i, ˆyi))_ (1)
_−_ _∇_ [ˆ]
 


-----

where we have used the shorthand Ei := ETi∼p and E( ˆXi,yˆi) [:=] [E](X[ˆ] _i,yˆi)∼µ[m]i_ [. For fair evaluation, we]
measure solution quality by the excess risk

_m(w) := Fm(w)_ inf (2)
_E_ _−_ **w[′]** R[D][ F][m][(][w][′][)]
_∈_

The excess risk is the difference between the average performance of w after one step of task-specific
adaptation from the performance of the best possible initialization for fast adaptation. To find a model
with small excess risk, one can solve one of two problems during training, NAL or MAML.

**NAL. NAL minimizes the loss fi(w) on average across the training tasks, which may yield small**
excess risk _m(w) with less computational cost than MAML (Gao & Sener, 2020). Denoting the n_
_E_
training examples for the i-th task as Xi ∈ R[n][×][d] and their labels as yi ∈ R[n], NAL solves

min _NAL[(][w][) :=][ 1]T_ _Ti=1_ _f[ˆ]i(w; Xi, yi),_ (3)
**w** R[D][ F][ tr]
_∈_

which is a surrogate for the expected risk minimization problem defined asP minw∈RD Ei [fi(w)].
We let wNAL[∗] [denote the unique solution of the expected risk minimization problem, and let][ w][NAL]
denote the unique solution to (3). We emphasize that in our study we evaluate the solution of NAL
by its expected error after running one step of SGD using the m samples from a new task that are
released at test time, and this error is captured by the excess risk _m(wNAL), defined in (2)._
_E_

**MAML. In contrast to NAL, MAML minimizes a surrogate loss of (1) during training. According**
to the MAML framework, the n samples per task are divided into τ training “episodes”, each with
_n2 “inner” samples for the inner SGD step and n1 “outer” samples for evaluating the loss of the_
fine-tuned model. Thus, we have that τ (n1 + n2) = n. We denote the matrices that contain the outer
samples for the j-th episode of the i-th task as X[out]i,j _i,j_

_[∈]_ [R][n][1][×][d][ and][ y][out] _[∈]_ [R][n][1] [, and the matrices that]
contain the inner samples as X[in]i,j _i,j_

_[∈]_ [R][n][2][×][d][ and][ y][in] _[∈]_ [R][n][2] [. The MAML objective is then]


_fˆi(w −_ _α∇f[ˆ]i(w; X[in]i,j[,][ y]i,j[in]_ [);][ X]i,j[out][,][ y]i,j[out][)][.] (4)
_j=1_

X


min _MAML[(][w][) :=]_
**w** R[D][ F][ tr]
_∈_


_T τ_


_i=1_


We denote the unique solution to (4) by wMAML and the unique solution to the population version (1)
by wMAML[∗] [. We expect][ E][m][(][w][MAML][)][ ≤E][m][(][w][NAL][)][ since (4) is a surrogate for the true objective]
of minimizing (1). However, the gain of MAML over NAL may not be significant enough to justify
its added computational cost (Gao & Sener, 2020), necessitating a thorough understanding of the
relative behavior of MAML and NAL.

3 MULTI-TASK LINEAR REGRESSION

We first explore the relative behaviors of MAML and NAL in a setting in which we can obtain
closed-form solutions: multi-task linear regression. Here, the model hw maps inputs x to predicted
labels by taking the inner product with w, i.e. hw(x)= **w, x** . The loss function ℓ is the squared
_⟨_ _⟩_
loss, therefore fi(w)= [1]2 [E][(][x][i][,y][i][)][∼][µ][i] [[(][⟨][w][,][ x][⟩−][y][i][)][2][]][ and][ ˆ]fi(w; Xi, yi)= [1]2 _[∥][X][i][w][−][y][i][∥]2[2]_ [for all][ i][. We]

consider a realizable setting in which the data for the i-th task is Gaussian and generated by a ground
truth model wi, R[d]. That is, points (xi, yi) are sampled from µi by first sampling xi (0, Σi),
_∗_ _∈_ _∼N_
then sampling yi ( **wi,** _, xi_ _, ν[2]), i.e. yi =_ **wi,** _, xi_ +zi where zi (0, ν[2]). In this setting
the population-optimal solutions for NAL and MAML are given by: ∼N _⟨_ _∗_ _⟩_ _⟨_ _∗_ _⟩_ _∼N_

**wNAL[∗]** [=][ E][i][[][Σ][i][]][−][1][E][i][[][Σ][i][w]i[∗][]][,] and **wMAML[∗]** [=][ E][i][[][Q][(]i[n][2][)]][−][1]Ei[Q[(]i[n][2][)]wi[∗][]][,] (5)


where for any s ∈ N, we define Q[(]i[s][)] := (Id _−αΣi)Σi(Id_ _−αΣi) +_ _[α]s[2]_ [(][tr][(][Σ]i[2][)][Σ][i] [+][Σ]i[3][)][. Note that]

these Q[(]i[s][)] matrices are composed of two terms: a preconditioned covariance matrix Σi (Id _αΣi)[2],_
and a perturbation matrix due to the stochastic gradient variance. We provide expressions for the −
empirical solutions wNAL and wMAML for this setting and show that they converge to wNAL[∗] [and]
**wMAML[∗]** [as][ n, T, τ][ →∞] [in Appendix D. Since our focus is ultimately on the nature of the solutions]
sought by MAML and NAL, not on their non-asymptotic behavior, we analyze wNAL[∗] [and][ w]MAML[∗]
in the remainder of this section, starting with the following result.

It is most helpful to interpret the solutions wMAML[∗] [and][ w]NAL[∗] [and their corresponding excess risks]
through the lens of task hardness. In this strongly convex setting, we naturally define task hardness


-----

**as the rate at which gradient descent converges to the optimal solution for the task, with harder**
tasks requiring more steps of gradient descent to reach an optimal solution. For step size α fixed
across all tasks, the rate with which gradient descent traverses each fi is determined by the minimum
eigenvalue of the Hessian of fi, namely λmin(Σi). So, for ease of interpretation, we can think of
the easy tasks as having data with large variance in all directions (all the eigenvalues of their Σi are
large), while the hard tasks have data with small variance in all directions (all λ(Σi) are small).

Note that both wMAML[∗] [and][ w]NAL[∗] [are normalized weighted sums of the task optimal solutions,]
with weights being functions of the Σi’s. For simplicity, consider the case in which m and n2 are
large, thus theproportional to the Qi Σ matrices are dominated byi matrices, wNAL[∗] **[is closer to the easy task optimal solutions] Σi(Id −** _αΣi)[2]. Since the weights for[, as][ Σ] w[i][ has large]NAL[∗]_ [are]
energy for easy tasks and small energy for hard tasks. Conversely, the MAML weights are determined
**to the hard task optimal solutionsby Σi(Id −** _αΣi)[2], which induces a relatively larger weight on the hard tasks, so._ **wMAML[∗]** **[is closer]**

Note that easy tasks can be approximately solved after one step of gradient descent from far away,
which is not true for hard tasks. We therefore expect (I) MAML to perform well on both hard and
easy tasks since wMAML[∗] [is closer to the optimal solutions of hard tasks, and (II) NAL to perform]
well on easy tasks but struggle on hard tasks since wNAL[∗] [is closer to the optimal solutions of easy]
tasks. We explicitly compute the excess risks of wNAL[∗] [and][ w]MAML[∗] [as follows.]

**Proposition 1. The excess risks for wNAL[∗]** _[and][ w]MAML[∗]_ _[are:]_

2

_m(wNAL[∗]_ [) =][ 1]2 [E][i] Ei[′] []Σi′ [][−][1]Ei′ []Σi′ (wi[∗][′][ −][w]i[∗][)] (6)
_E_ **Q[(]i[m][)]**

_m(wMAML[∗]_ [) =][ 1]2 [E][i] Ei[′] []Q[(]i[′][n][2][)] _−1Ei′_ **Q[(]i[′][n][2][)](wi[∗][′][ −]** **[w]i[∗][)]** 2 (7)
_E_ **Q[(]i[m][)]**

  

We next formally interpret these excess risks, and develop intuitions (I) and (II), by focusing on two
key properties of the task environment: task hardness discrepancy and task geography.

3.1 HARDNESS DISCREPANCY

We analyze the levels of task hardness that confer a significant advantage for MAML over NAL in
this section. To do so, we compare wMAML[∗] [and][ w]NAL[∗] [in an environment with two tasks of varying]
hardness. We let n2 = _m, Σ1 =_ _ρH_ **Id, and Σ2 =** _ρEId, where ρH <ρE, thus task 1 is the hard task.[1]_

In this setting, the NAL and MAML solutions defined in (5) can be simplified as


_ρH_ _ρE_ _aH_ _aE_
**wNAL[∗]** [=] **w1[∗]** [+] **w2[∗][,]** **wMAML[∗]** [=] **w1[∗]** [+] **w2[∗][,]** (8)

_ρH + ρE_ _ρH + ρE_ _aH + aE_ _aH + aE_


where aE := (ρE(1 − _αρE)[2]_ + _[d]m[+1]_ _[α][2][ρ]E[3]_ [)][ and][ a][H][ := (][ρ][H] [(1][ −] _[αρ][H]_ [)][2][ +][ d]m[+1] _[α][2][ρ]H[3]_ [)][. Note that the]

natural choice of α is the inverse of the largest task smoothness parameter ( _ρ[1]E_ [in this case). Setting]

_α =_ _ρ1E_ [yields][ a][E][ =][ d]m[+1] _[ρ][E][ and][ a][H][ =][ ρ][H]_ [(1][ −] _[ρ]ρ[H]E_ [)][2][ +][ (][d]mρ[+1)][3]E[ρ]H[3] . As a result, we can easily see that

for sufficiently large values of m, we have aH > aE. This observation shows that the solution of
MAML is closer to the solution of the harder task, i.e., w1[∗][. On the other hand,][ w][NAL] [is closer to]
**w2[∗][, the solution to the easy task, since][ ρ][H]** _[< ρ][E][.]_

Considering these facts, we expect the performance of MAML solution after adaptation to exceed
that of NAL. Using Proposition 1, the excess risks for NAL and MAML in this setting are

_H_ [+][ a][H] _[ρ]E[2]_ _aEaH_
_m(wNAL) =_ _[a][E][ρ][2]_ _,_ _m(wMAML) =_ _._ (9)
_E_ (ρE + ρH )[2] _E_ _aE + aH_


Recalling that aE 0 for m _d, we conclude that MAML achieves near-zero excess risk in the_
_≈_ _≫_
case of large m. In particular, we require that ρH (1 − _[ρ]ρ[H]E_ [)][2][ ≫] _[dρ]m[E]_ [, otherwise the][ O][(][d/m][)][ terms]

1The effects of task hardness could be removed by scaling the data so that it would have covariance α−1Id.
However, the current setting is useful to build intuition. Further, one can imagine a similar setting in which the
first dimension has variance α[−][1] and the rest have variance ρH, in which scaling would not be possible (as it
would result in gradient descent not converging in the first coordinate).


-----

1.000.750.500.25 Location of Solutions: m = n2 = 2000 _)w(m_ 1.00.80.6 Excess Risk: m = n2 = 2000wwMAMLNAL * * 1.000.750.500.25 Location of Solutions: m = n2 = 100 _)w(m_ 1.00.80.6 Excess Risk: m = n2 = 100

0.00 0.00

−0.25 0.4 −0.25 0.4

−0.50−0.75 _www12MAML * : Hard Task * : Easy Task *_ Excess Risk 0.2 −0.50−0.75 Excess Risk 0.2

Coordinate in first dimension −1.00 0.0 0.2 0.4 _pH_ 0.6 _w0.8NAL *_ 1.0 0.0 0.0 0.2 0.4 _pH_ 0.6 0.8 1.0 Coordinate in first dimension −1.00 0.0 0.2 0.4 _pH_ 0.6 0.8 1.0 0.0 0.0 0.2 0.4 _pH_ 0.6 0.8 1.0


(a)


(b)


(c)


(d)


Figure 2: First coordinates of wMAML[∗] [,][ w]NAL[∗] [and their excess risks for various][ ρ][H][ for large][ m][ (in]
subplots (a)-(b) for m =2000) and small m (in subplots (c)-(d) for m =100).

in aE and aH are non-negligible. Meanwhile, for NAL we have _m(wNAL) =_ (ρEaH+ρρ[2]EH )[2][, which]
_E_

may be significantly larger than zero if _[ρ]ρ[H]E_

easy task. Importantly, the error for NAL is dominated by poor performance on the hard task, which[≪] [1][, i.e. the harder task is significantly harder than the]
MAML avoids by initializing close to the hard task solution. Thus, these expressions pinpoint the
level of hardness discrepancy needed for superior MAML performance: _[ρ]ρ[H]E_ _[must be much smaller]_

_than 1, but also larger than 0, such that_ _[ρ]ρ[H]E_ [(1][ −] _[ρ]ρ[H]E_ [)][2][ ≫] _m[d]_ _[.]_

Figure 2 visualizes these intuitions in the case that w1[∗] [=][ 1][d][,][ w]2[∗] [=][ −][1][d][,][ d][ = 10][ and][ σ][2][ = 0][.][01][.]
Subfigures (a) and (c) show the locations of the first coordinates of wNAL[∗] [and][ w]MAML[∗] [for varying]
_ρH_, and m = 2000 and m = 500, respectively. Subfigures (b) and (d) show the corresponding excess
risks. We observe that unlike NAL, MAML initializes closer to the optimal solution of the harder
task as long as _[ρ]ρ[H]E_ [is not close to zero or one, which results in significantly smaller excess risk for]

MAML compared to NAL in such cases, especially for large m.

Figure 2 further shows that the MAML and NAL excess risks go to zero with ρH . This is also shown
in (9) and the definition of aH, and points to the fact that too much hardness discrepancy causes no
gain for MAML. The reason for this is that ρH 0 corresponds to the hard task data going to zero
(its mean), in which case any linear predictor has negligible excess risk. Consequently, both NAL →
and MAML ignore the hard task and initialize at the easy task to achieve near-zero excess risk here.

**Remark 1. The condition** _[ρ]ρ[H]E_ [(1][ −] _[ρ]ρ[H]E_ [)][2][ ≫] _m[d]_ _[requires that][ m][ ≫]_ _[d][. However, this condition arises]_

_due to the simplification tr(Σi) = O(d), where tr(Σi) can be thought of as the effective problem_
_dimension (Kalan et al., 2020). In realistic settings, the effective dimension may be o(d), which would_
_reduce the complexity of m accordingly._


3.2 TASK GEOGRAPHY

The second important property of the task environment according to Proposition 1 is the location, i.e.
geography, of the task optimal solutions. In this section, we study how task geography affects the
MAML and NAL excess risks by considering a task environment with many tasks. In particular, the
task environment µ is a mixture over distributions of hard and easy tasks, with mixture weights 0.5.
The optimal solutions wi[∗] _i_
for easy tasks are sampled as[∈] w[R][d]i[∗][ for hard tasks are sampled according to][ w][∗] _[∼N]_ [(][R][1][d][, r][H] **[I][d][)][ and]**
between the centers of the hard and easy tasks’ optimal solutions, and[∼N] [(][0][d][, r][E][I][d][)][. Therefore][ R][ is the dimension-normalized distance] rH and rE capture the spread
of the hard and easy tasks’ optimal solutions, respectively. The data covariance is Σi = ρH **Id for the**
hard tasks and Σi = ρEId for the easy tasks, recalling that ρH and ρE parameterize hardness, with
smaller ρH meaning harder task. In this setting the following corollary follows from Proposition 1.

**Corollary 1. In the setting described above, the excess risks of wNAL[∗]** _[and][ w]MAML[∗]_ _[are:]_


_Em(wNAL[∗]_ [) =]

_Em(wMAML[∗]_ [) =]


4(ρE +d _ρH_ )[2] (aEρ[2]E [+ 2][a][E][ρ][E][ρ][H] [)][r][E] [+][ a][E][ρ][2]H [(][r][E] [+][ R][2][)]
h

+ (aH _ρ[2]H_ [+ 2][a][H] _[ρ][E][ρ][H]_ [)][r][H] [+][ a][H] _[ρ][2]E[(][r][H]_ [+][ R][2][)] (10)

4(aE +d _aH_ )[2] (a[3]E [+ 2][a]E[2] _[a][H]_ [)][r][E] [+ (][a][3]H [+ 2][a][E][a]H[2] [)][r][H] i
h

+ aEa[2]H [(][r][E] [+][ R][2][) +][ a][2]E[a][H] [(][r][H] [+][ R][2][)] (11)
i


-----

25

20

15

10

5

|Model Performance|Col2|
|---|---|
|||


Model Performance 4.0 Ratio m(wNAL)/m(wMAML) 25 Model Performance 4.0 Ratio m(wNAL)/m(wMAML)

3.53.0 _)w(m_ 2015 _wwwMAMLMAMLNAL[*]_ 3.53.0

Ratio 2.5 10 _wNAL[*]_ Ratio 2.5

2.0 2.0

1.5 Excess Risk 5 1.5 EmpiricalTheoretical

0 1 2 _R_ 3 4 5 1.0 1 2 _R_ 3 4 5 0 0 1 2 _R_ 3 4 5 1.0 1 2 _R_ 3 4 5


(a) _r[R]H[2]_ [=0][.][5][,] _RrE[2]_ [=10]


(b) _r[R]H[2]_ [=0][.][5][,] _RrE[2]_ [=10]


(c) _r[R]H[2]_ [=10][,] _RrE[2]_ [=0][.][5]


(d) _r[R]H[2]_ [=10][,] _RrE[2]_ [=0][.][5]


Figure 3: Theoretical and empirical excess risks for NAL and MAML and ratios of the NAL to
MAML excess risk in the setting described in Section 3.2.

_where aE := ρE(1 −_ _αρE)[2]_ + _[d]m[+1]_ _[α][2][ρ]E[3]_ _[and][ a][H][ :=][ ρ][H]_ [(1][ −] _[αρ][H]_ [)][2][ +][ d]m[+1] _[α][2][ρ]H[3]_ _[.]_

Each excess risk is a normalized weighted sum of the quantities rE, rH and R[2]. So, the comparison
between MAML and NAL depends on the relative weights each algorithm induces on these task
environment properties. If _[ρ]ρ[H]E_ [(1][ −] _[ρ]ρ[H]E_ [)][2][ ≫] _md_ [, then][ a][H][ ≫] _[a][E][ and the dominant weight in]_

_m(wMAML[∗]_ [)][ is on the][ r][H][ term, while the dominant weights in][ E][m][(][w]NAL[∗] [)][ are on the][ r][H][ +][ R][2][ and]
_E_
_rH terms. This observation leads us to obtain the following corollary of Proposition 1._
**Corollary 2. In the above setting, with α = 1/ρE and** _[ρ]ρ[H]E_ [(1][ −] _[ρ]ρ[H]E_ [)][2][ ≫] _m[d]_ _[, the relative excess risk]_

_for NAL compared to MAML satisfies_
_m(wNAL[∗]_ [)]
_E_ _._ (12)

_m(wMAML[∗]_ [)][ ≈] [1 +][ R]rH[2]
_E_

Corollary 2 shows that MAML achieves large gain over NAL when: (i) the hard tasks’ solutions are
_closely packed (rH is small) and (ii) the hard tasks’ solutions are far from the center of the easy tasks’_
_solutions (R is large). Condition (i) allows MAML to achieve a small excess risk by initializing in_
the center of the hard task optimal solutions, while condition (ii) causes NAL to struggle on the hard
tasks since it initializes close to the easy tasks’ solutions. These conditions are reflected by the fact
that the MAML excess risk weighs rH (the spread of the hard tasks) most heavily, whereas the NAL
excess risk puts the most weight on R[2] (distance between hard and easy task solutions) as well as rH .

Note that the above discussion holds under the condition that aH _aE. In order for aH_ _aE, we_
must havehardness discrepancy. Now, we see that even with appropriate hardness discrepancy, the hard tasks[ρ]ρ[H]E [(1][ −] _[ρ]ρ[H]E_ [)][2][ ≫] _m[d]_ [, i.e.][ m][ ≫] _[d][ and][ 0][ <][ ρ]ρ[H]E_ _[≪]_ [1][, as we observed in our discussion on] ≫ _≫_

must be both closely packed and far from the center of the easy tasks in order for MAML to achieve
significant gain over NAL. This conclusion adds greater nuance to prior results (Balcan et al., 2019;
Jose & Simeone, 2021), in which the authors argued that gradient-based meta-learning (e.g. MAML)
is only effective when all of the tasks’ optimal solutions are close to each other (in our case, all of
_rE, rH and R are small). Crucially, our analysis shows that NAL would also perform well in this_
scenario, and neither rE nor R need to be small for MAML to achieve small excess risk.

To further explain these insights, we return to Figure 1 in which easy and hard task optimal solutions
are each sampled from 10-dimensional Gaussian distributions of the form described above, and
100 random task optimal solutions are shown, along with the population-optimal MAML and NAL
solutions. In subfigures (b), (c) and (d), we set ρE = 0.9, ρH = 0.1, and m = 500. Among these
plots, the largest gain for MAML is in the case that the hard tasks’ optimal solutions are closely
packed (small rH ), and their centers are far from each other (large R), demonstrating the the primary
_dependence of relative performance on R[2]/rH_ _._

We plot more thorough results for this setting in Figure 3. Here, we vary R and compare the
performance of wNAL[∗] [and][ w]MAML[∗] [in settings with relatively large][ r][H][ and small][ r][E][, specifically]
choosing rH and rE such that _r[R]H[2]_ [= 0][.][5][ and][ R]rE[2] [= 10][ in (a)-(b), and][ R]rH[2] [= 10][ and][ R]rE[2] [= 0][.][5][ in]

(c)-(d). Here we also plot the empirical solutions wNAL and wMAML. Again, MAML significantly
_outperforms NAL when R[2]/rH is large (subfigures (c)-(d)), but not otherwise._


4 TWO-LAYER NEURAL NETWORK

In this section, we consider a non-linear setting in which each task is a regression problem with a
two-layer neural network with a fixed second layer. The k-th neuron in the network maps R[d] _→_ R


-----

via σ(⟨w[k], x⟩), where σ : R → R is some activation function and w[k] _∈_ R[d] is the parameter vector.
The network contains M neurons for a total of D = Md parameters, which are contained in the
matrix W := [w[1], . . ., w[M] ] ∈ R[d][×][M] . The predicted label for the data point x is the sum of
the neuron outputs, namely hW(x) := _k=1_ _[σ][(][⟨][w][k][,][ x][⟩][)][. The loss function is again the squared]_
loss, i.e. fi(W) = [1]2 [E][(][x][i][,y][i][)][∼][µ][i] [[(][σ][(][P]k[M]=1[⟨][w][k][,][ x][i][⟩][)][ −] _[y][i][)][2][]][. Ground-truth models][ W][i,][∗]_ [generate]

the data for task i. We sample (xi, yi) ∼[P][M]µi by first sampling xi ∼N (0d, Σi) then computing
_yi =_ _k=1_ _[σ][(][⟨][w]i,[k]_
_∗[,][ x][i][⟩][)][. The following result demonstrates an important property of the MAML]_
objective function in the two-task version of this setting.

**Theorem 1.[P][M]** _Suppose that in the setting described above, the task environment is the uniform_
_distribution over two tasks, and σ is the ReLU, Softplus, Sigmoid, or tanh function. Let Σi = Id_
_and define si,k as the k-th singular value of Wi,∗, κi := ∥Wi,∗∥2/si,M_ _, and λi :=_ [Π]k[M]s=1[M]i,M[s][i,k] _for_

_i_ 1, 2 _. Further, define the regions_ _i :=_ **W :** **W** **Wi,** 2 _c1/(s[c]i,1[λ][i][κ][2]i_ _[M][ 2][)][}][ and the]_
_parameters ∈{_ _}_ _βi :=_ _λciκ2_ [2]i _[and][ L][i][ :=][ c][3][Ms] S_ _i,[2][c]1_ _[for] {[ i][ ∈{] ∥[1][,][ 2] −[}][ and absolute constants]∗∥_ _≤_ _[ c, c][1][, c][2][,][ and][ c][3][.]_

_Let β1 <β2 and L1 <L2. For any stationary point WMAML[∗]_ _[of the population MAML objective (1)]_
_with full inner gradient step (m =_ _), such that WMAML[∗]_
_∞_ _[∈S][1][ ∩S][2][, must satisfy for all][ α][ ≤]_ [1][/L][2][:]

1 2αβ2
_f1(WMAML)_ 2 _−_ (13)
_∥∇_ _∥_ _≤_ (1 _αL1)[2][ ∥∇][f][2][(][W][MAML][)][∥][2][ +][ O][(][α][2][)][.]_

_−_


**Interpretation: MAML prioritizes hard tasks. In the setting above, βi and Li are strong convexity**
and smoothness parameters, respectively, of the function fi on the region Si. Here, task 1 is
the harder task since β1 and β2 control the rate with which gradient descent converges to the
ground-truth solutions (Zhong et al., 2017), with smaller βi implying slower convergence, and
_βhas smaller gradient norm on the hard task than on the easy task as long as there is sufficient1 < β2. Thus, Theorem 1 shows that, any stationary point of the MAML objective in S1 ∩S2_
hardness discrepancy, specifically β2 > L1. Physically, this condition means that the curvature of
the loss function for the easy task is more steep than the curvature of the hard task in all directions
around their ground-truth solutions. The smaller gradient norm of MAML stationary points on the
harder task suggests that MAML solutions prioritize hard-task performance. In contrast, we can
easily see that any stationary point wNAL of the NAL population loss must satisfy the condition
_∥∇f1(wNAL)∥2 = ∥∇f2(wNAL)∥2, meaning that NAL has no such preference for the hard task._

Figure 4 demonstrates the importance of task hardness in comparing NAL and MAML in this setting.
Here, for ease of visualization we consider the case in which M = 1 and d = 2, i.e. the learning
model consists of one neuron (with Softplus activation) mapping from R[2] _→_ R. Each subfigure plots
the NAL or MAML population loss landscape for different task environments with m =250, as well
as the ground-truth neuron for each task. In light of prior work showing that the number of gradient
steps required to learn a single neuron diminishes with the variance of the data distribution (Theorem
3.2 in Yehudai & Ohad (2020)), we again control task hardness via the data variance. In all plots,
**Σi = 0.5I2 for hard tasks and Σi = I2 for easy tasks. Note that the MAML loss (evaluated after one**
step of adaptation) is the evaluation metric we ultimately care about.

We observe that when all tasks are equally hard, the MAML and NAL solutions are identical
(subfigures (a)-(b)), whereas when one task is hard, MAML initializes closer to the hard task and
achieves significantly better post-adaptation performance than the NAL solution, which is closer to
the centroid of the easy tasks (c)-(d). This supports our intuition that MAML achieves significant
gain over NAL in task environments in which it can leverage improved performance on hard tasks.

5 EXPERIMENTS

We next experimentally study whether our observations generalize to problems closer to those seen
in practice. We consider image classification on the Omniglot (Lake et al., 2019) and FS-CIFAR100
(Oreshkin et al., 2018) datasets. Following convention, tasks are N -way, K-shot classification
problems, i.e., classification problems among N classes where K labeled images from each class
are available for adapting the model. For both the Omniglot and FS-CIFAR100 experiments, we use
the five-layer CNN used in Finn et al. (2017); Vinyals et al. (2016). NAL is trained equivalently to
MAML with no inner loop and n = _n1_ +n2. Further details and error bounds are in Appendix C.


-----

(a)


(b)


(c)


(d)


2.0 [NAL Loss Landscape - Softplus]Easy Tasks 2.0 [MAML Loss Landscape - Softplus]NAL Solution 10[0] 2.0 [NAL Loss Landscape - Softplus]Easy Tasks 10[1] 2.0 [MAML Loss Landscape - Softplus]NAL Solution

1.5 10[1] 1.5 MAML Solution 1.5 Hard Task 1.5 MAML Solution

1.0 1.0 1.0 1.0

0.5 6 × 10[0] 0.5 0.5 6 × 10[0] 0.5 10[0]

0.0 0.0 0.0 4 × 10[0] 0.0

−0.5 4 × 10[0] −0.5 −0.5 −0.5

3 × 10[0]

−1.0 −1.0 −1.0 −1.0

3 × 10[0]

−1.5 −1.5 10[−1] −1.5 2 × 10[0] −1.5

−2.0 −2.0 −2.0 −2.0

−2 −1 0 1 2 −2 −1 0 1 2 −2 −1 0 1 2 −2 −1 0 1 2


Figure 4: Loss landscapes and ground-truth neurons for NAL (a,c) and MAML (b,d) for two distinct,
four-task environments and Softplus activation.


Table 1: Omniglot accuracies.

Setting Train Tasks Test Tasks

_rH_ Alg. Easy Hard Easy Hard

Large MAML 99.2 96.0 98.0 81.2
Large NAL-1 69.4 41.5 57.8 45.2
Large NAL-10 70.0 45.3 67.2 47.9

Small MAML 99.2 99.1 98.1 95.4
Small NAL-1 69.2 46.0 55.8 45.8
Small NAL-10 70.2 44.0 67.8 48.9


Table 2: FS-CIFAR100 accuracies.

Setting Train Tasks Test Tasks

_p_ Alg. Easy Hard Easy Hard

NAL 78.2 9.9 74.8 9.5
0.99
MAML 92.9 50.1 84.6 21.7

NAL 81.9 9.5 76.5 8.6
0.5
MAML 93.6 41.1 84.3 17.9

NAL 82.4 7.6 76.9 8.2
0.01
MAML 94.0 15.7 85.0 11.8


**Omniglot. Omniglot contains images of 1623 handwritten characters from 50 different alphabets.**
Characters from the same alphabet typically share similar features, so are harder to distinguish
compared to characters from differing alphabets. We thus define easy tasks as classification problems
among characters from distinct alphabets, and hard tasks as classification problems among characters
from the same alphabet, consistent with prior definitions of semantic hardness (Zhou et al., 2020).
Here we use N =5 and K =1. For NAL, we include results for K = 10 (NAL-10) in addition to
_K = 1 (NAL-1). We split the 50 alphabets into four disjoint sets: easy train (25 alphabets), easy test_
(15), hard train (5), and hard test (5). During training, tasks are drawn by first choosing ‘easy’ or
‘hard’ with equal probability. If ‘easy’ is chosen, 5 characters from 5 distinct alphabets among the
easy train alphabets are selected. If ‘hard’ is chosen, a hard alphabet is selected from the hard train
alphabets, then N =5 characters are drawn from that alphabet. After training, we evaluate the models
on new tasks drawn analogously from the test (unseen) alphabets as well as the train alphabets.

Table 1 gives the average errors after completing training for two experiments: the first (Large) when
the algorithms use all 5 hard train and test alphabets, and the second (Small) when the algorithms use
only one hard train alphabet (Sanskrit) and one hard test (Bengali). The terms ‘Large’ and ‘Small’
describe the hypothesized size of rH in each experiment: the optimal solutions of hard tasks drawn
from ten (train and test) different alphabets are presumably more dispersed than hard tasks drawn
from only two (train and test) similar alphabets. By our previous analysis, we expect MAML to
achieve more gain over NAL in the Small rH setting. Table 1 supports this intuition, as MAML’s
performance improves significantly with more closely-packed hard tasks (Small), while NAL’s does
not change substantially. Hence, MAML’s relative gain over NAL increases with smaller rH .

**FS-CIFAR100. FS-CIFAR100 has 100 total classes that are split into 80 training classes and 20**
testing classes. We further split the 600 images in each of the training classes into 450 training
images and 150 testing images in order to measure test accuracy on the training classes in Table 2.
Here we use N and K as proxies for hardness, with larger N and smaller K being more hard as the
agent must distinguish more classes with fewer training samples/class. Specifically, easy tasks have
(N, K) = (2, 10), and the hard tasks have (N, K) = (20, 1). During training, hard tasks are sampled
with probability p and easy tasks with probability 1−p. Observe that the largest performance gains
for MAML in Table 2 are on the hard tasks, consistent with our conclusion that MAML outperforms
NAL primarily by achieving relatively high accuracy on the hard tasks. However, the improvement
by MAML on the hard tasks disappears when the hard tasks are scarce (p = 0.01), supporting the
idea of oversampling hard tasks during training in order to improve MAML performance (Zhou et al.,
2020; Sun et al., 2020).


-----

6 ETHICS STATEMENT

We believe that our paper does not have any potential ethical concerns.

7 REPRODUCIBILITY STATEMENT

The proofs for all theoretical results are in the Appendix. Experimental details can also be found in
the Appendix, and we have provided our code, as well as instructions on how to use it, in a zip file as
part of the supplementary material.

REFERENCES

Antoniou, A., Edwards, H., and Storkey, A. How to train your maml. In International Conference on
_Learning Representations, 2018._

Bai, Y., Chen, M., Zhou, P., Zhao, T., Lee, J. D., Kakade, S., Wang, H., and Xiong, C. How important
is the train-validation split in meta-learning? arXiv preprint arXiv:2010.05843, 2020.

Balcan, M.-F., Khodak, M., and Talwalkar, A. Provable guarantees for gradient-based meta-learning.
In International Conference on Machine Learning, pp. 424–433. PMLR, 2019.

Baxter, J. Theoretical models of learning to learn. In Learning to learn, pp. 71–94. Springer, 1998.

Bernacchia, A. Meta-learning with negative learning rates. arXiv preprint arXiv:2102.00940, 2021.

Charles, Z. and Konecnˇ y, J. On the outsized importance of learning rates in local update methods.`
_arXiv preprint arXiv:2007.00878, 2020._

Chen, W.-Y., Liu, Y.-C., Kira, Z., Wang, Y.-C. F., and Huang, J.-B. A closer look at few-shot
classification. arXiv preprint arXiv:1904.04232, 2019.

Cioba, A., Bromberg, M., Wang, Q., Niyogi, R., Batzolis, G., Shiu, D.-s., and Bernacchia, A. How to
distribute data across tasks for meta-learning? arXiv preprint arXiv:2103.08463, 2021.

Collins, L., Mokhtari, A., and Shakkottai, S. Task-robust model-agnostic meta-learning. In Advances
_in Neural Information Processing (NeurIPS), 2020._

Denevi, G., Ciliberto, C., Stamos, D., and Pontil, M. Learning to learn around a common mean.
_Advances in Neural Information Processing Systems, 31:10169–10179, 2018._

Du, S. S., Hu, W., Kakade, S. M., Lee, J. D., and Lei, Q. Few-shot learning via learning the
representation, provably. arXiv preprint arXiv:2002.09434, 2020.

Fallah, A., Mokhtari, A., and Ozdaglar, A. On the convergence theory of gradient-based modelagnostic meta-learning algorithms. In International Conference on Artificial Intelligence and
_Statistics, pp. 1082–1092, 2020._

Finn, C., Abbeel, P., and Levine, S. Model-agnostic meta-learning for fast adaptation of deep
networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pp. 1126–1135. JMLR. org, 2017.

Finn, C., Rajeswaran, A., Kakade, S., and Levine, S. Online meta-learning. In International
_Conference on Machine Learning, pp. 1920–1930, 2019._

Gao, K. and Sener, O. Modeling and optimization trade-off in meta-learning. Advances in Neural
_Information Processing Systems, 33, 2020._

Goldblum, M., Reich, S., Fowl, L., Ni, R., Cherepanova, V., and Goldstein, T. Unraveling meta-learning: Understanding feature representations for few-shot tasks. arXiv preprint
_arXiv:2002.06753, 2020._

Ji, K., Lee, J. D., Liang, Y., and Poor, H. V. Convergence of meta-learning with task-specific
adaptation over partial parameters. arXiv preprint arXiv:2006.09486, 2020a.


-----

Ji, K., Yang, J., and Liang, Y. Theoretical convergence of multi-step model-agnostic meta-learning.
_arXiv e-prints, pp. arXiv–2002, 2020b._

Jose, S. T. and Simeone, O. An information-theoretic analysis of the impact of task similarity on
meta-learning. arXiv preprint arXiv:2101.08390, 2021.

Kalan, S. M. M., Fabian, Z., Avestimehr, A. S., and Soltanolkotabi, M. Minimax lower
bounds for transfer learning with linear and one-hidden layer neural networks. arXiv preprint
_arXiv:2006.10581, 2020._

Khodak, M., Balcan, M.-F. F., and Talwalkar, A. S. Adaptive gradient-based meta-learning methods.
In Advances in Neural Information Processing Systems, pp. 5915–5926, 2019.

Kong, W., Somani, R., Song, Z., Kakade, S., and Oh, S. Meta-learning for mixed linear regression.
In International Conference on Machine Learning (ICML), 2020.

Lake, B. M., Salakhutdinov, R., and Tenenbaum, J. B. The omniglot challenge: A 3-year progress
report. Current Opinion in Behavioral Sciences, 29:97–104, 2019.

Nichol, A., Achiam, J., and Schulman, J. On first-order meta-learning algorithms. arXiv preprint
_arXiv:1803.02999, 2018._

Oh, J., Yoo, H., Kim, C., and Yun, S.-Y. Does maml really want feature reuse only? arXiv preprint
_arXiv:2008.08882, 2020._

Oreshkin, B. N., Rodriguez, P., and Lacoste, A. Tadam: task dependent adaptive metric for improved
few-shot learning. In Proceedings of the 32nd International Conference on Neural Information
_Processing Systems, pp. 719–729, 2018._

Raghu, A., Raghu, M., Bengio, S., and Vinyals, O. Rapid learning or feature reuse? towards
understanding the effectiveness of maml. In International Conference on Learning Representations,
2019.

Rajeswaran, A., Finn, C., Kakade, S. M., and Levine, S. Meta-learning with implicit gradients. In
_Advances in Neural Information Processing Systems, pp. 113–124, 2019._

Saunshi, N., Zhang, Y., Khodak, M., and Arora, S. A sample complexity separation between nonconvex and convex meta-learning. In International Conference on Machine Learning (ICML),
2020.

Saunshi, N., Gupta, A., and Hu, W. A representation learning perspective on the importance of
train-validation splitting in meta-learning. In International Conference on Machine Learning, pp.
9333–9343. PMLR, 2021.

Schudy, W. and Sviridenko, M. Concentration and moment inequalities for polynomials of independent random variables. _Proceedings of the Twenty-Third Annual ACM-SIAM Sympo-_
_sium on Discrete Algorithms, Jan 2012._ doi: 10.1137/1.9781611973099.37. [URL http:](http://dx.doi.org/10.1137/1.9781611973099.37)
[//dx.doi.org/10.1137/1.9781611973099.37.](http://dx.doi.org/10.1137/1.9781611973099.37)

Sun, Q., Liu, Y., Chen, Z., Chua, T.-S., and Schiele, B. Meta-transfer learning through hard tasks.
_IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020._

Tripuraneni, N., Jin, C., and Jordan, M. I. Provable meta-learning of linear representations. arXiv
_preprint arXiv:2002.11684, 2020._

Tropp, J. A. An introduction to matrix concentration inequalities. arXiv preprint arXiv:1501.01571,
2015.

Vershynin, R. High-Dimensional Probability: An Introduction with Applications in Data Science,
volume 47. Cambridge University Press, 2018.

Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., and Wierstra, D. Matching networks for
one shot learning. arXiv preprint arXiv:1606.04080, 2016.


-----

Wang, H., Sun, R., and Li, B. Global convergence and induced kernels of gradient-based metalearning with neural nets. arXiv preprint arXiv:2006.14606, 2020a.

Wang, X., Yuan, S., Wu, C., and Ge, R. Guarantees for tuning the step size using a learning-to-learn
approach. arXiv preprint arXiv:2006.16495, 2020b.

Yehudai, G. and Ohad, S. Learning a single neuron with gradient methods. In Conference on Learning
_Theory, pp. 3756–3786. PMLR, 2020._

Zhong, K., Song, Z., Jain, P., Bartlett, P. L., and Dhillon, I. S. Recovery guarantees for one-hiddenlayer neural networks. In International conference on machine learning, pp. 4140–4149. PMLR,
2017.

Zhou, P., Yuan, X., Xu, H., Yan, S., and Feng, J. Efficient meta learning via minibatch proximal
update. In Advances in Neural Information Processing Systems, pp. 1532–1542, 2019.

Zhou, Y., Wang, Y., Cai, J., Zhou, Y., Hu, Q., and Wang, W. Expert training: Task hardness aware
meta-learning for few-shot classification. arXiv preprint arXiv:2007.06240, 2020.


-----

APPENDIX

A PROOFS FOR SECTION 3

A.1 PROOF OF PROPOSITION 1

For all i, let yi[in] = X[in]i **[w][i,][∗]** [+][ z]i[in] [and][ y]i[out] = X[out]i **wi,∗** + z[out]i . In other words, z[in]i _∈_ R[n][2] is the
vector containing the additive noise for the inner samples and z[out]i R[n][1] is the vector containing the
_∈_
additive noise for the outer samples.

From (41) and (43), we have that

**wNAL[∗]** [=][ E][i][[][Σ][i][]][−][1][E][i][[][Σ][i][w]i[∗][]] (14)

**wMAML[∗]** [=][ E][i][[][Q]i[(][n][2][)]][−][1]Ei[Q[(]i[n][2][)]wi[∗][]][,] (15)

Next, we compute a closed-form expression for Fm(w) (defined in (1)). For all i, let ˆyi = **X[ˆ]** _iwi,_ +ˆzi
_∗_
and yi = x[⊤]i **[w][i,][∗]** [+][ z][i][. In other words,][ z]i[in] _∈_ R[m] is the vector containing the additive noise for the
we haveinner samples and zi ∈ R is the scalar additive noise containing the outer (evaluation) sample. Then

2[]

_F_ (w) = [1] **Xi,yˆi)** **xi, w** _m_ **X[ˆ]** _[⊤]i_ [(]X[ˆ] _iw_ **yˆi)** _yi_

2 [E][i][E][(][x][i][,y][i][)][E][(][ ˆ] _−_ _[α]_ _−_ _−_

D E  2[]

= [1]2 [E][i][E][(][x][i][,y][i][)][E][(][ ˆ]Xi,yˆi) _⟨xi, Pi(w −_ **wi[∗][)][⟩]** [+ (][z][i] [+][ α]m **[x]i[⊤]X[ˆ]** _[⊤]i_ **[ˆ]zi)**

 2[]

= [1]2 [E][i][E][(][x][i][,y][i][)][E][(][ ˆ]Xi,yˆi) _⟨xi, Pi(w −_ **wi[∗][)][⟩][2][ +]** _zi +_ _m[α]_ **[x]i[⊤]X[ˆ]** _[⊤]i_ **[ˆ]zi**

  

= [1]2 [E][i][E][(][x][i][,y][i][)][E][(][ ˆ]Xi,yˆi) _⟨xi, Pi(w −_ **wi[∗][)][⟩][2][ +][ σ][2][ +][ α]m[2][2][ tr][(][x]i[⊤]X[ˆ]** _[⊤]i_ **[ˆ]ziˆz[⊤]i** **X[ˆ]** _ixi)_

 


E ˆXi _[∥][P][i][(][w][ −]_ **[w]i[∗][)][∥]Σ[2]** _i_ [+][ σ][2][ +][ α]m[2][σ][2] [tr][(][Σ]i[2][)]


= [1]

2 [E][i]


where Pi := Id − _m[α]_ **X[ˆ]** _[⊤]i_ **X[ˆ]** _i. Hence,_

inf _i_ [)] (16)
**w** R[d][ F][m][(][w][) =][ σ][2][ +][ α]m[2][σ][2] [tr][(][Σ][2]
_∈_


Thus we have


_σ[2]_ + _[α][2][σ][2]_ _i_ [)]

_m_ [tr][(][Σ][2]


_m(wNAL) = Fm(wNAL)_
_E_ _−_ 2[1] [E][i]


= EiE ˆXi _i_ [)][∥]Σ[2] _i_

_[∥][P][i][(][w][NAL][ −]_ **[w][∗]**
= EiE ˆXi (wNAL **wi[∗][)][⊤][P]i[⊤][Σ][i][P][i][(][w][NAL]** _i_ [)]
_−_ _[−]_ **[w][∗]**

= EiE ˆXi  Ei[′] [Σi[′] ][−][1]Ei[′] [Σi[′] (wi[∗][′][ −] **[w]i[∗][)]]** _⊤_ 
  P[⊤]i **[Σ][i][P][i][E][i][′]** [[][Σ][i][′] []][−][1][E][i][′] [[][Σ][i][′] [(][w]i[∗][′][ −] **[w]i[∗][)]**

= Ei Ei′ [Σi′ ][−][1]Ei′ [Σi′ (wi[∗][′][ −] **[w]i[∗][)]]** _⊤_ 

Eh   ˆXi **P[⊤]i** **[Σ][i][P][i]** Ei′ [Σi′ ][−][1]Ei′ [Σi′ (wi[∗][′][ −] **[w]i[∗][)]]**

= Ei Ei′ [Σi′ ][−][1]E  i′ [Σi′ (wi[∗][′][ −] **[w]i[∗][)]]** _⊤_  [i]
h   **Q[(]i[m][)]** Ei′ [Σi′ ][−][1]Ei′ [Σi′ (wi[∗][′][ −] **[w]i[∗][)]]** (17)

= Ei Ei′ [Σi′ ][−][1]Ei′ [Σi′ (wi[∗][′][ −] **[w]i[∗][)]]** **Q[(]i[m][)]**  [i] (18)

[ ] [2]


-----

where (17) follows from Lemma 2. Similarly, for MAML we have the following chain of equalities
for _m(wMAML) :_
_E_


_m(wMAML)_
_E_

= Fm(wMAML)
_−_ [1]2 [E][i]


_σ[2]_ + _[α][2][σ][2]_ _i_ [)]

_m_ [tr][(][Σ][2]


= EiE ˆXi _i_ [)][∥]Σ[2] _i_

_[∥][P][i][(][w][MAML][ −]_ **[w][∗]**
= EiE ˆXi (wMAML **wi[∗][)][⊤][P]i[⊤][Σ][i][P][i][(][w][MAML]** _i_ [)]
_−_ _[−]_ **[w][∗]**
 _⊤_ 

= EiE ˆXi Ei′ [Q[(]i[′][n][2][)]][−][1]Ei′ [Q[(]i[′][n][2][)]wi[∗][′] []][ −] **[w]i[∗]** **P[⊤]i** **[Σ][i][P][i]** Ei′ [Q[(]i[′][n][2][)]][−][1]Ei′ [Q[(]i[′][n][2][)]wi[∗][′] []][ −] **[w]i[∗]**

   []

_⊤_

= EiE ˆXi Ei′ [Q[(]i[′][n][2][)]][−][1]Ei′ [Q[(]i[′][n][2][)](wi[∗][′][ −] **[w]i[∗][)]]**

  

**P[⊤]i** **[Σ][i][P][i]** Ei′ [Q[(]i[′][n][2][)]][−][1]Ei′ [Q[(]i[′][n][2][)](wi[∗][′][ −] **[w]i[∗][)]]**
  []

_⊤_

= Ei Ei′ [Q[(]i[′][n][2][)]][−][1]Ei′ [Q[(]i[′][n][2][)](wi[∗][′][ −] **[w]i[∗][)]]**

  

E ˆXi **P[⊤]i** **[Σ][i][P][i]** Ei′ [Q[(]i[′][n][2][)]][−][1]Ei′ [Q[(]i[′][n][2][)](wi[∗][′][ −] **[w]i[∗][)]]**
  []  []

_⊤_

= Ei Ei′ [Q[(]i[′][n][2][)]][−][1]Ei′ [Q[(]i[′][n][2][)](wi[∗][′][ −] **[w]i[∗][)]]** **Q[(]i[m][)]** Ei′ [Q[(]i[′][n][2][)]][−][1]Ei′ [Q[(]i[′][n][2][)](wi[∗][′][ −] **[w]i[∗][)]]**

   [] (19)


= Ei Ei′ [Q[(]i[′][n][2][)]][−][1]Ei′ [Q[(]i[′][n][2][)](wi[∗][′][ −] **[w]i[∗][)]]**


where (19) follows from Lemma 2.

A.2 PROOF OF COROLLARY 1


(20)
**Q[(][m][)]**


We first write out the dimension-wise excess risks for NAL and MAML, using the results in Proposition 1. Let wi,l[∗] [denote the][ l][-th element of the vector][ w]i[∗][. Since each][ Σ][i][ is diagonal, we have]

that Q[(]i[m][)] is diagonal. Let λi,l denote the l-th diagonal element of the matrix Σi and let qi,l denote

the l-th diagonal element of the matrix Q[(]i[m][)], thus qi,l = λi,l (1 _αλi,l)[2]_ + _[α][2]_ _i_ [)][λ][i,l][ +][ λ]i,l[3] [)][.]
_−_ _m_ [(][tr][(][Σ][2]

Recall that we assume n2 = m. The error for NAL in the l-th dimension is:


2[]




Ei′ _λi′,l_ _wi[∗][′],l_ _i,l_

_[−]_ _[w][∗]_
h Ei′ [λi′,l] i


_m_ [(][w]NAL[∗] [) = 1]
_E_ [(][l][)] 2 [E][i]


_m_ [(][w]NAL[∗] [) = 1] _qi,l_
_E_ [(][l][)] 2 [E][i] Ei′ [λi′,l]

 
1   

= Ei′ Ei′′ _qi,lλi′,lλi′′,l_ _wi[∗][′],l_ _i,l_ _wi[∗][′′],l_ _i,l_

2Ei [λi,l][2][ E][i] _[−]_ _[w][∗]_ _[−]_ _[w][∗]_
 [] []      

Likewise, the error for MAML in the l-th dimension is:


2[]




Ei[′] _qi′,l_ _wi[∗][′],l_ _i,l_

_[−]_ _[w][∗]_
h Ei[′] [qi[′],l] i


_m_ [(][w]MAML[∗] [) = 1]
_E_ [(][l][)] 2 [E][i]


_qi,l_

2 [E][i] Ei[′] [qi[′],l]

 

1   

Ei′ Ei′′ _qi,lqi′,lqi′′,l_ _wi[∗][′],l_ _i,l_ _wi[∗][′′],l_ _i,l_
2Ei [qi,l][2][ E][i] _[−]_ _[w][∗]_ _[−]_ _[w][∗]_
 [] []      


-----

Next we use the definitions of λi,l for the easy and hard tasks. For the easy tasks, note that

_qi,l[easy]_ = λi,l (1 _αλi,l)[2]_ + _[α][2]_ _i_ [)][λ][i,l] [+][ λ][3]i,l[)]
_−_ _m_ [(][tr][(][Σ][2]

= ρE(1 _αρE)[2]_ + [(][d][ + 1)][α][2][ρ]E[3]
_−_ _m_

=: aE (21)

and similarly for the hard tasks, namely


_qi,l[hard]_ = ρH (1 − _αρH_ )[2] + [(][d][ + 1)]m[α][2][ρ][3]


=: aH (22)


Next, recall that the task distribution is a mixture of the distribution of the hard tasks and the
distribution of the easy tasks, with mixture weights (0.5, 0.5). Let iH be the index of a task drawn
from the distribution of hard tasks, and iE be the index of a task drawn from the distribution of easy
tasks. Then using the law of total expectation, the excess risk for MAML in the l-th dimension can
then be written as:


_Em[(][l][)][(][w][MAML][)]_


2[]




Ei′ _qi′,l_ _wi[∗][′],l_ _i,l_

_[−]_ _[w][∗]_
h Ei′ [qi′,l] i


= [1]

2 [E][i]


_qi,l_




1 2 2

_qi,lEi[′]H_ _aH_ _wi[∗][′]H_ _[,l][ −]_ _[w]i,l[∗]_ + qi,lEiE _aE_ _wi[∗][′]E_ _[,l][ −]_ _[w]i,l[∗]_

8Ei [qi,l][2][ E][i]"

h  i h  i

+ 2qi,lEiH _aH_ _wi[∗][′]H_ _[,l][ −]_ _[w]i,l[∗]_ EiE _aE_ _wi[∗][′]E_ _[,l][ −]_ _[w]i,l[∗]_
h  i h  i [#]

1 2 2

_aH_ Ei[′]H _aH_ _wi[∗][′]H_ _[,l][ −]_ _[w]i[∗]H_ _,l_ + aH Ei[′]E _aE_ _wi[∗][′]E_ _[,l][ −]_ _[w]i[∗]H_ _,l_

16Ei [qi,l][2][ E][i][H] "

h  i h  i

+ 2aH Ei[′]H _aH_ _wi[∗][′]H_ _[,l][ −]_ _[w]i[∗]H_ _,l_ Ei[′]E _aE_ _wi[∗][′]E_ _[,l][ −]_ _[w]i[∗]H_ _,l_

h  i h  i [#]

1 2 2
+ _aEEi[′]H_ _aE_ _wi[∗][′]H_ _[,l][ −]_ _[w]i[∗]E_ _,l_ + aEEi[′]E _aE_ _wi[∗][′]E_ _[,l][ −]_ _[w]i[∗]E_ _,l_

16Ei [qi,l][2][ E][i][E] "

h  i h  i

+ 2aEEi[′]H _aH_ _wi[∗][′]H_ _[,l][ −]_ _[w]i[∗]E_ _,l_ Ei[′]E _aE_ _wi[∗][′]E_ _[,l][ −]_ _[w]i[∗]E_ _,l_

h  i h  i [#]


1

8Ei [qi,l][2][ E][i]


1

_a[3]H_ _[r][H]_ [+][ a][H] _[a][2]E[(][r][H]_ [+][ R][2][) + 2][a][2]H _[a][E][r][H]_
16Ei [qi,l][2]


+ aEa[2]H [(][r][E] [+][ R][2][) +][ a][3]E[r][E] [+ 2][a][2]E[a][H] _[r][E]_ (23)



_a[3]H_ _[r][H]_ [+][ a][H] _[a][2]E[(][r][H]_ [+][ R][2][) + 2][a][2]H _[a][E][r][H]_

+ aEa[2]H [(][r][E] [+][ R][2][) +][ a][3]E[r][E] [+ 2][a][2]E[a][H] _[r][E]_


4(aE + aH )[2]


-----

where (23) follows by the properties of the distributions of the hard and easy tasks. Likewise, for
NAL we have


_Em[(][l][)][(][w][NAL][)]_


2[]




Ei′ _λi′,l_ _wi[∗][′],l_ _i,l_

_[−]_ _[w][∗]_
h Ei′ [λi′,l] i


= [1]

2 [E][i]


_qi,l_

2 [E][i]


 

1

16Ei [λi,l][2][ E][i][H]


1 2 2

_aH_ Ei[′]H _ρH_ _wi[∗][′]H_ _[,l][ −]_ _[w]i[∗]H_ _,l_ + aH Ei[′]E _ρE_ _wi[∗][′]E_ _[,l][ −]_ _[w]i[∗]H_ _,l_

Ei [λi,l][2][ E][i][H] "

h  i h  i

+ 2aH Ei[′]H _ρH_ _wi[∗][′]H_ _[,l][ −]_ _[w]i[∗]H_ _,l_ Ei[′]E _ρE_ _wi[∗][′]E_ _[,l][ −]_ _[w]i[∗]H_ _,l_

h  i h  i [#]

1 2 2

_aEEi[′]H_ _ρE_ _wi[∗][′]H_ _[,l][ −]_ _[w]i[∗]E_ _,l_ + aEEi[′]E _ρE_ _wi[∗][′]E_ _[,l][ −]_ _[w]i[∗]E_ _,l_

16Ei [λi,l][2][ E][i][E] "

h  i h  i

+ 2aEEi[′]H _aH_ _wi[∗][′]H_ _[,l][ −]_ _[w]i[∗]E_ _,l_ Ei[′]E _aE_ _wi[∗][′]E_ _[,l][ −]_ _[w]i[∗]E_ _,l_

h  i h  i [#]


_aH_ _ρ[2]H_ _[r][H]_ [+][ a][H] _[ρ][2]E[(][r][H]_ [+][ R][2][) + 2][a][H] _[ρ][H]_ _[ρ][E][r][H]_ [+]

_aEρ[2]H_ [(][r][E] [+][ R][2][) +][ a][E][ρ][2]E[r][E] [+ 2][a][E][ρ][H] _[ρ][E]_



4(aE + aH )[2]


Note that in the this setting the excess risks are symmetric across dimension. Thus multiplying by d
completes the proof.

A.3 PROOF OF COROLLARY 2

For the case that α = 1/ρE and _[ρ]ρ[H]E_ [(1][ −] _[ρ]ρ[H]E_ [)][2][ ≫] _md_ [, we can compare MAML and NAL by the]

weights that are placed on rH, rE, rH + R[2] and rE + R[2] in their excess risks. Using the fact that
_α = 1/ρE, the expressions in (21) and (22) can be simplified as_

_aE = [(][d][ + 1)][ρ][E]_


_aH = ρH_ (1 − _[ρ]ρ[H]E_ [)][2][ + (][d][ + 1)]mρ[2]E[ρ]H[3] (24)

Under the assumption that _[ρ]ρ[H]E_ [(1][ −] _[ρ]ρ[H]E_ [)][2][ ≫] _m[d]_ [, the bias term][ ρ][H] [(1][ −] _[ρ]ρ[H]E_ [)][2][ dominates the gradient]

variance term [(][d][+1)]m _[ρ][E]_, thus aH dominates aE. As a result, we can ignore aE in the expressions.

Furthermore, ρH (1 − _[ρ]ρ[H]E_ [)][2][ dominates][ (][d]mρ[+1)][2]E[ρ]H[3] within the expression for aH, meaning we can also

drop the latter term. Thus we have


_m(wMAML[∗]_ [)] _[≈]_ _[da]H[3]_ _rH =_ _dρH_ (1 − _[ρ]ρ[H]E_ [)][2]
_E_ 4a[2]H 4


_rH_ (25)


and


_H_ [+ 2][da][H] _[ρ][E][ρ][H]_ _daH_ _ρ[2]E_
_m(wNAL[∗]_ [)][ ≈] _[da][H]_ _[ρ][2]_ _rH +_
_E_ 4(ρH + ρE)[2] 4(ρH + ρE)[2][ (][r][H][ +][ R][2][)]


_dρH_ (1 − _[ρ]ρ[H]E_ [)][2]


_dρH_ _ρ[2]E[(1][ −]_ _[ρ]ρ[H]E_ [)][2]
_rH +_ _R[2]_ (26)

4(ρH + ρE)[2]


-----

Dividing (26) by (25) yields

_Em(wNAL[∗]_ [)] _dρH_ (1 − _[ρ]ρ[H]E_ [)][2] _rH +_ _dρH_ _ρ[2]E[(1][ −]_ _[ρ]ρ[H]E_ [)][2] _R[2]_ _dρH_ (1 − _[ρ]ρ[H]E_ [)][2]

_m(wMAML[∗]_ [)][ ≈] 4 4(ρH + ρE)[2] !, 4
_E_

= _rH +_ _ρ[2]E_ _rH_

(ρH + ρE)[2][ R][2]

  [,]

= 1 + _ρ[2]E_ _R[2]_

(ρH + ρE)[2] _rH[2]_


_rH_


1 + _[R][2]_ (27)
_≈_ _rH[2]_

where the last approximation follows since [1]4 (ρE +ρ[2]EρH )[2][ ≤] [1][.]

_[≤]_


B PROOF OF THEOREM 1

In this section we prove Theorem 1. We first show a general result.
_Hessians of the task loss functions are bounded and positive definite, i.e.Lemma 1. Let wMAML ∈_ R[D] _be a stationary point of the MAML objective 1 with wMAML satisfies m = ∞_ _whose_
_β1ID_ _f1(wMAML)_ _L1ID,_ _β2ID_ _f2(wMAML)_ _L2ID_ (28)
_⪯∇[2]_ _⪯_ _⪯∇[2]_ _⪯_
1αfor some/∇ max(f1(wL LMAML1, L1 ≥2):)β and1 > w 02 and := w LMAML2 ≥ _β2 − >α 0∇. Suppose thatf2(wMAML). Then the following holds for any β2 > β1. Define w1 := wMAML α < −_

_f1(w1)_ 2 ≲ [1][ −] _[αβ][2]_ _f2(w2)_ + O(α[2]). (29)
_∥∇_ _∥_ 1 _αL1_ _∥∇_ _∥_

_−_


_Proof. For some vector wMAML_ _∈_ R[D], let H1(wMAML) := _∇[2]f1(wMAML) and_
**H2(wMAML) =** _f2(wMAML) and g1(wMAML) =_ _f1(w1) and g2(wMAML) =_ _f2(w2),_
_∇[2]_ _∇_ _∇_
recalling that w1 = wMAML − _α∇f1(wMAML) and w2 = wMAML −_ _α∇f2(wMAML)._

Recall that the MAML objective in this case is given by

_F_ (w) = [1] (30)
_∞_ 2 _[f][1][(][w][ −]_ _[α][∇][f][1][(][w][)) + 1]2_ _[f][2][(][w][ −]_ _[α][∇][f][2][(][w][))][.]_

After setting the gradient of F∞(.) equal to zero at wMAML, we find that any stationary point
**wMAML of the MAML objective must satisfy**
(ID _αH1(wMAML))g1(w1) =_ (ID _αH2(wMAML))g2(w2)_ (31)
_−_ _−_ _−_
with minimum eigenvalue at leastSince H2(wMAML) ⪯ _L2Id and 1 α − ≤αL1/2 > max( 0. Thus we haveL1, L2), Id −_ _αH2(wMAML) is positive definite_
(ID _αH2(wMAML))[−][1](ID_ _αH1(wMAML))g1(w1) =_ **g2(w2)** (32)
_−_ _−_ _−_
Taking the norm of both sides and using the fact thatvectorL2ID by assumption, yields v, along with the facts that β1ID ⪯ **H1(wMAML ∥Av∥) ⪯2 ≥Lσ1minID and(A) β∥v2∥I2D for any matrix ⪯** **H2(wMAML A and) ⪯**
_∥g2(w2)∥2 = ∥(ID −_ _αH2(wMAML))[−][1](ID −_ _αH1(wMAML))g1(w1)∥2_
_σmax[−][1]_ [(][I][D]
_≥_ _[−]_ _[α][H][2][(][w][MAML][))][σ][min][(][I][D]_ _[−]_ _[α][H][1][(][w][MAML][))][∥][g][1][(][w][1][)][∥][2]_

**g1(w1)** 2 (33)

_≥_ [1]1[ −] _[αL]αβ2[1]_ _∥_ _∥_

_−_

Next, note that using Taylor expansion,
**g1(w1) =** _f1(wMAML)_ _α_ _f1(wMAML)_ _f1(wMAML) + O(α[2])_
_∇_ _−_ _∇[2]_ _∇_
= (ID _αH1(wMAML))g1(wMAML)_ (34)
_−_

**g2(w2) =** _f2(wMAML)_ _α_ _f2(wMAML)_ _f2(wMAML) + O(α[2])_
_∇_ _−_ _∇[2]_ _∇_
= (ID _αH2(wMAML))g2(wMAML)_ (35)
_−_


-----

(a)


(b)


(c)


(d)


2.0 Task 1 Loss Landscape - Softplus 10[0] 2.0 Task 2 Loss Landscape - Softplus 2.0 Task 3 Loss Landscape - Softplus 2.0[Task 4 Loss Landscape - Softplus]

1.5 1.5 1.5 1.5

10[0] 10[0] 10[0]

1.0 10 1 1.0 1.0 1.0

0.5 0.5 10 1 0.5 10 1 0.5 10 1

0.0 10 2 0.0 0.0 0.0

0.5 0.5 0.5 0.5

1.0 10 3 1.0 10 2 1.0 10 2 1.0 10 2

1.5 1.5 1.5 1.5

2.0 2.0 10 3 2.0 10 3 2.0 10 3

2 1 0 1 2 2 1 0 1 2 2 1 0 1 2 2 1 0 1 2


Figure 5: Task loss functions f1, f2, f3, f4 for task environment in Figure 4 (a-b).

Therefore

_∥g2(wMAML)∥2 + O(α[2])_

≳ _σmax[−][1]_ [(][I][D] **g1(w1)** 2

_[−]_ _[α][H][2][(][w][MAML][))][σ][min][(][I][D]_ _[−]_ _[α][H][1][(][w][MAML][))1]1[ −]_ _[αL]αβ2[1]_ _∥_ _∥_

2 _−_

1 _αL1_
≳ _−_ **g1(wMAML)** 2, (36)

1 _αβ2_ _∥_ _∥_

 _−_ 

2

i.e., ∥∇f1(wMAML)∥2 ≲ 11−−αLαβ21 _∥∇f2(wMAML)∥2 + O(α[2])._
 

Now we are ready to prove Theorem 1.

_Proof.∇[2]fi( We first show thatW) ∈_ R[Nd][×][Nd] is the Hessian of the loss of the vectorized βiINd ⪯∇[2]fi(W) ⪯ _LiINd for all W W and each ∈Si for β ii and ∈{ L1,i 2 is defined}, where_
in Theorem 1. In other words, we will show that each fi is βi-strongly convex and Li-smooth within
_Sof Lemma 1.i. This will show that any stationary point WMAML that lies within S1 ∩S2 satisfies the conditions_

For any i ∈{1, 2} and for any W ∈ R[d][×][N] we have by Weyl’s Inequality

_∇[2]fi(Wi,∗) −∥∇[2]fi(W) −∇[2]fi(Wi,∗)∥2INd_
_⪯∇[2]fi(W) ⪯∇[2]f1(Wi,∗) + ∥∇[2]fi(W) −∇[2]fi(Wi,∗)∥2INd_ (37)

Thus, we will control the spectrum of ∇[2]fi(W) by controlling the spectrum of ∇[2]fi(Wi,∗) and by
upper bounding ∥∇[2]fi(W) −∇[2]fi(Wi,∗)∥2.

To control the spectrum of ∇[2]fi(Wi,∗) we use Lemma D.3 from Zhong et al. (2017), which shows
that for absolute constants c1 and c2 and each of the possible σ functions listed in Theorem 1,

(c1/(κ[2]λ))INd _f1(Wi,_ ) (c2Ns[2]i,[c]1[)][I][Nd][.] (38)
_⪯∇[2]_ _∗_ _⪯_

Next, we apply Lemma D.10 from Zhong et al. (2017), which likewise applies for all the mentioned
_σ, to obtain_

_∥∇[2]fi(W) −∇[2]fi(Wi,∗)∥2 ≤_ _c3N_ [2]s[c]i,1[∥][W][ −] **[W][i,][∗][∥][2]** (39)

for a constant c3. Next, for any W _i, we have_ **W** **Wi,** 2 = O(1/(s[c]i,1[λ][i][κ][2]i _[N][ 2][))][. Therefore,]_
_∈S_ _∥_ _−_ _∗∥_
by combining (37), (38) and (39), we obtain that βiINd ⪯∇[2]fi(W) ⪯ _LiINd for all W ∈Si._

Finally we apply Lemma 1 to complete the proof.

Note that the constant c used in Theorem 1 is the homogeneity constant for which σ satisfies Property
3.1 from Zhong et al. (2017), namely, 0 ≤ _σ[′](z) ≤_ _a|z|[c]_ _∀z ∈_ R.


-----

(a)


(b)


(c)


(d)


2.0 NAL Loss Landscape - Sigmoid 2.0 MAML Loss Landscape - Sigmoid 2.0 NAL Loss Landscape - Sigmoid 2.0 MAML Loss Landscape - Sigmoid 10[0]

1.5 6 × 10[−1] 1.5 1.5 10[0] 1.5

1.0 1.0 1.0 1.0

0.5 0.5 0.5 0.5

0.0 4 × 10[−1] 0.0 10[−1] 0.0 0.0

10[−1]

−0.5 3 × 10[−1] −0.5 −0.5 −0.5

−1.0 −1.0 −1.0 −1.0

−1.5 Easy Tasks −1.5 NAL Solution −1.5 Easy Tasks −1.5 NAL Solution

−2.0 Hard Tasks 2 × 10[−1] −2.0 MAML Solution −2.0 Hard Tasks 10[−1] −2.0 MAML Solution

−2 −1 0 1 2 −2 −1 0 1 2 −2 −1 0 1 2 −2 −1 0 1 2


Figure 6: Loss landscapes for NAL (a,c) and MAML (b,d) for two distinct task environments and
Sigmoid activation.

Table 3: Effect of up-weighting hard tasks for NAL.

NAL NAL, ν = 2 NAL, ν = 5 NAL, ν = 10 MAML

Avg. coord. 0.18 ± .02 0.33 ± .02 0.61 ± .03 0.87 ± .03 1.58 ± 0.01
Test error 0.78 ± .11 0.64 ± .08 0.51 ± 04 0.39 ± .05 0.26 ± 0.10

C ADDITIONAL EXPERIMENTS AND DETAILS

C.1 LINEAR REGRESSION

In all of the linear regression experiments, to run SGD on the MAML and NAL objectives, we sample
one task from the corresponding environment on each iteration for 5,000 iterations. Each task has
_n1 = 25 outer loop samples and varying n2 inner loop samples for MAML, and n = n1 +_ _n2 samples_
for NAL. We appropriately tuned the ‘meta-learning rates’, i.e. the learning rate with which wMAML
and wNAL are updated after each full iteration, and used n2/10000 for MAML and 0.025 for NAL.
After 5,000 iterations, the excess risks of the final iterates were estimated using 3,000 randomly
samples from the environment. We repeated this procedure ten times to obtain standard deviations.

We also ran an experiment to test whether up-weighting the hard tasks improves NAL performance,
in light of our observation that MAML achieves performance gain by initializing closer to the hard
task solutions. We use a similar environment as in Section 3.2. Tasks are 10-dimensional linear
regression problems with n2 = 25 inner loop samples and n1 = 500 outer loop samples, and noise
variance 0.01. To implement up-weighting for NAL, we introduce a parameter ν which is the ratio of
the weight placed on the hard tasks to the weight placed on the easy tasks within each batch of tasks.
We normalize the weights to sum to 1. For example, if a task batch consists of 6 hard tasks and 4
easy tasks, then NAL with ν = 2 places a weight of 6νν+4 [=][ 1]8 [on the hard task loss functions and]

1 1 1

6ν+4 [=] 16 [on the easy task loss functions (as opposed to] 10 [on all tasks for standard NAL).]

Easy tasks have hardness parameter ρE = 1 and optimal solution drawn from N (0d, Id), and hard
tasks have hardness parameter ρH = 0.1 and optimal solution drawn from N (21d, Id). We run NAL
and MAML for 4000 iterations and use a task-batch size of 10 tasks per iteration, sampling easy
and hard tasks with equal probability. We report the average coordinate value for the final solutions
**wNAL and wMAML and their test error (averaged across randomly sampled hard and easy tasks),**
plus or minus standard deviation over 5 independent random trials.

Note that average coordinate value closer to 2 means the solution is closer to optimal solutions of
the hard tasks, while closer to 0 means it is closer to the optimal solutions of the easy tasks. Indeed
we see that when NAL places more emphasis on the hard tasks, i.e. ν is large, its performance
correspondingly increases and approaches that of MAML. Indeed, it illustrates that MAML can be
interpreted as a reweighing of tasks based on their level of hardness for GD.


-----

Figure 7: Logistic regression results in analogous setting to m = 500 column in Figure 2 (T = 2
tasks, d = 10 dimensions). Recall that ρH is the strong convexity parameter (data variance) for the
hard task, which determines its hardness, while the strong convexity parameter of the easy task is 1.
MAML again initializes closer to the harder task, and has smaller excess risk for appropriate ρH .

_m = n2 = 2000_ _m = n2 = 500_ _m = 50, n2 = 50_

1.0 1.0

1.0

0.5 0.5

0.5 _wNAL_

0.0 _wwMAML1 * : Hard Task_ 0.0 0.0

_w2 * : Easy Task_

−0.5 −0.5

−0.5

Coordinate in first dimension

−1.0 −1.0 −1.0

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

4.0 4.0 4.0

3.5 _E(wMAML) (emp.)_ 3.5 3.5

)w( 3.0 _EE((wwMAMLNAL) (emp.) * ) (theory)_ 3.0 3.0
_m_ 2.5 _E(wNAL * ) (theory)_ 2.5 2.5

2.0 2.0 2.0

1.5 1.5 1.5

Excess Risk 1.0 1.0 1.0

0.5 0.5 0.5

0.0 0.0 0.0

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0


Figure 8: Version of Figure 2 with corresponding empirical results, including 95% confidence
intervals. The hardness parameter ρH varies along the x-axis.

C.2 LOGISTIC REGRESSION

We also experimented with logistic regression, please see Figure 7 for details.

C.3 ONE-LAYER NEURAL NETWORKS

We approximate loss landscapes for two types of activations: Softplus (Figures 4 and 5) and Sigmoid
(Figure 6). To approximate each landscape, we sample Gaussian data as specified in Section 4 and
compute the corresponding empirical losses as in equation (3) for NAL and (4) for MAML. We use
_n = 500 for NAL and n1 = 20 and τ = 25 for MAML in all cases. We use n2 = m = 250 for_
Softplus and n2 = m = 80 for Sigmoid. Figure 6 shows that when the hard and easy task solutions
_have similar centroids (R is small, as in subfigures (a)-(b)), then the NAL and MAML solutions are_
_close and achieve similar post-adaptation loss (b). On the other hand, if the centroids are spread_
_and the hard tasks are close (large R, small rH as in subfigures (c)-(d)), then the NAL and MAML_
_solutions are far apart and MAML obtains significantly smaller post-adaptation loss (d)._


-----

SETTING TRAIN TASKS TEST TASKS

_rH_ ALG EASY HARD EASY HARD

LARGE MAML 99.2±.2 96.0±.6 98.0 ± .2 81.2 ± .3
LARGE NAL-1 69.4 ± .4 41.5 ± .3 57.8 ± .4 45.2 ± .8
LARGE NAL-10 70.0 ± .8 45.3 ± .9 67.2 ± .3 47.9 ± .7

SMALL MAML 99.2 ± .5 99.1 ± .2 98.1 ± .3 95.4 ± .3
SMALL NAL-1 69.2 ± .5 46.0 ± .6 55.8 ± .5 45.8 ± 1.0
SMALL NAL-10 70.2 ± .6 44.0 ± .8 67.8 ± .8 48.9 ± .8

Table 4: Omniglot accuracies with 95% confidence intervals.

Setting Train Tasks Test Tasks

_p_ Alg. Easy Hard Easy Hard

NAL 78.2 _.4_ 9.9 1.0 74.8 _.4_ 9.5 _.6_
0.99 _±_ _±_ _±_ _±_
MAML 92.9 ± .3 50.1 ± 1.0 84.6 ± .4 21.7 ± .3

NAL 81.9 _.3_ 9.5 1.0 76.5 _.4_ 8.6 _.3_
0.5 _±_ _±_ _±_ _±_
MAML 93.6 ± .5 41.1 ± .3 84.3 ± .6 17.9 ± .4

NAL 82.4 1.4 7.6 _.2_ 76.9 _.9_ 8.2 _.4_
0.01 _±_ _±_ _±_ _±_
MAML 94.0 ± 1.2 15.7 ± .3 85.0 ± .2 11.8 ± .3

Table 5: FS-CIFAR100 accuracies with 95% confidence intervals.

C.4 OMNIGLOT

The full version of Table 1, with error bounds, is given in Table 4.

We use the same 4-layer convolutional neural network architecture as in (Finn et al., 2017), using
code adapted from the code that implements in PyTorch the experiments in the paper (Antoniou et al.,
2018). We ran SGD on the MAML and NAL objectives using 10 target samples per class for MAML,
i.e. n1 = 5 _×_ 10. Likewise, n = n1 +5 _×_ _n2 samples were used in each task to update wNAL on each_
iteration, for n2 = 5 × K. Eight tasks were drawn on each iteration for a total of 20,000 iterations.
The outer-loop learning rate for MAML was tuned in {10[−][2], 10[−][3], 10[−][4], 10[−][5], 10[−][6]} and selected
as 10[−][3]. Similarly, the learning rate for NAL was tuned in {10[−][2], 10[−][3], 10[−][4], 10[−][5], 10[−][6]} and
selected as 10[−][6]. Both MAML and NAL used a task-specific adaptation (inner) learning rate of 10[−][1]
as in Antoniou et al. (2018). To select the alphabets corresponding to hard tasks, we ran MAML on
tasks drawn from all 50 Omniglot alphabets, and took the 10 alphabets with the lowest accuracies.
The train/test split for the other (easy) alphabets was random among the remaining alphabets.

To compute the accuracies in Table 1, we randomly sample 500 tasks from the training classes and
500 from the testing classes, with easy tasks and hard tasks being chosen with equal probability, and
take the average accuracies after task-specific adaption from the fixed, fully trained model for each
set of sampled tasks. MAML uses 1 step of SGD for task-specific adaptation during both training
and testing, and NAL uses 1 for testing. The entire procedure was repeated 5 times with different
random seeds to compute the average accuracies in Table 1 and the confidence bounds in Figure 4.

C.5 FS-CIFAR100

First, we note a typo from the main body: we used 5x as many samples for task-specific adaptation
as stated in the main body for both easy and hard tasks, that is, (N, K) = (2, 50) for easy tasks and
(N, K) = (20, 5) for hard tasks.

The full version of Table 2 with error bounds is given in Table 5.

We use the same CNN as for Omniglot but with a different number of input nodes and channels
to account for the larger-sized CIFAR images, which are 32-by-32 RGB images (Omniglot images
are 28-by-28 grayscale). We ran MAML and NAL (SGD on the MAML and NAL objectives,
respectively) using 10 target (outer loop) samples per class per task for MAML, and 15 samples per
class per task for NAL (recalling that NAL has no inner loop samples). Eight tasks were drawn on


-----

each iteration for a total of 20,000 iterations. The outer-loop learning rate for MAML was tuned
in {10[−][2], 10[−][3], 10[−][4], 10[−][5], 10[−][6]} and selected as 10[−][3]. Similarly, the learning rate for NAL was
tuned in {10[−][2], 10[−][3], 10[−][4], 10[−][5], 10[−][6]} and selected as 10[−][6]. Both MAML and NAL used a
task-specific adaptation (inner loop) learning rate of 10[−][1] as in Antoniou et al. (2018).

We trained for 10,000 iterations with a task batch size of 2, with hard tasks being chosen with
probability p and easy tasks with probability 1−p. As in the Omniglot experiment, after completing
training we randomly sample 500 tasks from the training classes and 500 from the testing classes, with
easy tasks and hard tasks being chosen with equal probability. We then take the average accuracies
after task-specific adaption from the fixed, fully trained model for each set of sampled tasks. NAL
uses 5 steps of task-specific adaptation for testing and MAML uses 1 for both training and testing.
The entire procedure was repeated 5 times with different random seeds to compute average accuracies
in Table 2 and the confidence bounds in Figure 5.

D MULTI-TASK LINEAR REGRESSION CONVERGENCE RESULTS

We motivate our analysis of the NAL and MAML population-optimal solutions for multi-task linear
regression by showing that their respective empirical training solutions indeed converge to their
population-optimal values.

First note that the empirical training problem for NAL can be written as

_T_

1
min **Xi(w** **wi,** ) **zi** 2[,] (40)
**w** **R[d]** _T_ _∥_ _−_ _∗_ _−_ _∥[2]_
_∈_ _i=1_

X

where thederivative with respect to j-element of z wi ∈ and setting it equal to zero yields that the NAL training solution is:R[n] contains the noise for the j-th sample for task Ti. Taking the

_T_ _T_
**wNAL =** **X[⊤]i** **[X][i]** _−1_ **X[⊤]i** **[X][i][w]i[∗]** [+][ X]i[⊤][z][i] _,_ (41)

_i=1_ _i=1_

  X  X   

assuming _i=1_ **[X]i[⊤][X][i][ is invertible. Similarly, using][ z]i[out]** and z[in]i [to denote noise vectors, as well]
as **Q[ˆ]** _i,j =_ **P[ˆ]** _i,j(X[out]i,j_ [)][⊤][X]i,j[out]P[ˆ] _i,j where_ **P[ˆ]** _i,j = Id −_ _nα2_ [(][X]i,j[in] [)][⊤][X][in]i,j[, we have that the empirical]

training problem for MAML is[P][T]


_τ_

**X[out]i** **Pˆ** _i,j(w_ **wi,** ) **z[out]i,j** **X[out]i,j** [(][X]i,j[in] [)][⊤][z]i,j[in] _[∥]2[2][,]_ (42)
_j=1_ _∥_ _−_ _∗_ _−_ _[−]_ _n[α]2_

X


min
**w∈R[d]**


_Tτ_


_i=1_


therefore

**wMAML**


_τ_

1 _[T]_
**Qˆ** _i,j_ _−_
_j=1_ _i=1_

X  X


**Qˆ** _i,jwi[∗]_ [+][ ˆ]Pi,j(X[out]i,j [)][⊤][z]i,j[out] _[−]_ _n[α]2_ **P[ˆ]** _i,j(X[out]i,j_ [)][⊤][X]i,j[out][(][X]i,j[in] [)][⊤][z][in]i,j



(43)


_i=1_


_j=1_


To show that wNAL and wMAML indeed converge to their population-optimal values as
_T, n, n1, τ_, we first make the following regularity assumptions.
_→∞_
**Assumption 1. There exists B > 0 s.t.** **wi[∗]**
_∥_ _[∥≤]_ _[B][ ∀][i][.]_

**Assumption 2. There exists β, L > 0 s.t. βId ⪯** **Σi ⪯** _LId ∀i._

Assumption 1 ensures that the task optimal solutions have bounded norm and Assumption 2 ensures
that the data covariances are positive definite with bounded spectral norm.
**Remark 2. We would like to note that Gao & Sener (2020) achieve a similar results as our Theorem**
_2 and 3. However, we arrive at our results using distinct techniques from theirs. Moreover, our MAML_
_convergence result (Theorem 3) accounts for convergence over task instances to the population-_
_optimal solution for MAML when a finite number of samples are allowed for task-specific adaptation_
_(a stochastic gradient step), whereas the analogous result in Gao & Sener (2020) (Theorem 2) does_


-----

_not: it assumesit shows convergence to the population-optimal MAML solution when an τ = 1 and shows convergence as n1 = n2 →∞. Since their result relies on infinite amount of samples n2 →∞,_
_are allowed for the inner task-specific update, i.e. a full gradient step. Our dimension-dependence_
_is significantly worse, than theirs, which suggests the extra complexity of the MAML objective with_
_finite samples allowed for task-specific adaptation._

D.1 NAL CONVERGENCE

Define Var(Σi) := ∥Ei[(Σi − Ei′ [Σi′ ])[2]]∥ and Var(Σiwi,∗) := ∥Ei[(Σiwi,∗ _−_ Ei′ [Σi′ **wi,∗])[2]]∥.**

**Theorem 2. (NAL Convergence) Under Assumptions 1 and 2, the distance of the NAL training**
_solution (41) to its population-optimal value (5) is bounded as_


_d + β_ _dKc[′′][ log(200][n][) +][ K]c[′′][ log(200][n][)]_ log(200T )L[2]B

+

 q _β[2][√]n_ _β[2]n_

p

 _[c][′][√]_

+ _Var(Σiwi,∗) log(200d)_ + _[LB]_ _Var(Σi) log(200d)_

_β√T_ _β[2][√]T_

p p


**wNAL** **wNAL[∗]** _[∥][2]_
_∥_ _−_ _[≤]_


(44)


_with probability at least 0.96, where K is the maximum sub-exponential norm of pair-_
_wise products of data and noise samples, for some absolute constants c, c[′], and n_

2 _≥_

_cL√d+_ _c[2]L[2]d+4βcL[√]log(200T )_

4 q 2β _and T > L[2]B[2](Var(Σi) + Var(Σiwi[∗][))][/][9][. Informally,]_

!


**wNAL** **wNAL[∗]** _[∥][2]_
_∥_ _−_ _[≤]_ _O[˜]_


(45)


_√n +_


_with probability at least 1 −_ _o(1), as long as n = Ω([˜]_ _d) and T = Ω([˜]_ _Var(Σi) + Var(Σiwi[∗][))][, where]_
_O˜ and_ Ω[˜] _exclude log factors._

_Proof. We first introduce notation to capture the dependency of the empirical training solution on T_
and n. In particular, for any T, n ≥ 1, we define

1

_T_ _T_ _T_ _−_ _T_

1 1

**wNAL[(][T,n][)]** [:= ( 1] **X[⊤]i** **[X][i][)][−][1][ 1]** **X[⊤]i** **[X][i][w]i[∗]** [+] **X[⊤]i** **[X][i]** **X[⊤]i** **[z][i][.]**

_Tn_ _Tn_ _Tn_ _Tn_

_i=1_ _i=1_ _i=1_ ! _i=1_

X X X X

We next fix the number of tasks T and define the asymptotic solution over T tasks as the number of
samples approaches infinity, i.e., n →∞, namely we define


**wNAL[(][T][ )][∗]** [:= (]

Using the triangle inequality we have


**Σi)[−][1]**
_i=1_

X


**Σiwi[∗][.]**
_i=1_

X


_∥wNAL[(][T,n][)]_ _[−]_ **[w]NAL[∗]** _[∥]_ [=][ ∥][w]NAL[(][T,n][)] _[−]_ **[w]NAL[(][T][ )][∗]** [+][ w]NAL[(][T][ )][∗] _[−]_ **[w]NAL[∗]** _[∥]_

**wNAL[(][T,n][)]** _NAL[∥]_ [+][ ∥][w]NAL[(][T][ )][∗] _NAL[∥]_ (46)
_≤∥_ _[−]_ **[w][(][T][ )][∗]** _[−]_ **[w][∗]**

We will first bound the first term in (46), which we denote asFor this part we implicitly condition on the choice of T training tasks to obtain a bound of the θ = ∥wNAL[(][T,n][)] _[−]_ **[w]NAL[(][T][ )][∗]** _[∥]_ [for convenience.]
form P(∥θ∥≥ _ϵ|{Ti}i) ≤_ 1 − _δ. Since this holds for all {Ti}i, we will obtain the final result_
P(∥θ∥≥ _ϵ) ≤_ 1 − _δ by the Law of Total Probability. We thus make the conditioning on {Ti}i_
implicit for the rest of the analysis dealing with θ.


-----

We use the triangle inequality to separate θ into a term dependent on the data variance and a term
dependent on the noise variance as follows:


_−1_

1

_i[′][ X][i][′]_ **X[⊤]i** **[X][i]**
_Tn_ **[X][⊤]** ! _[−]_


1

_T_ _−_

**Σi′**
_i[′]=1_ !

X


**wi[∗]**


_θ =_


_T_ **[Σ][i]**


_Tn_

1

_Tn_

_i_

1

_Tn_


_i[′]=1_


_i=1_

+

_T_

_i=1_

X


1
_−_ _T_
1

_[⊤][X][i]_ **X[⊤]i** **[z][i]**

_Tn_

! _i=1_

X

_−1_

1

_i[′][ X][i][′]_ **X[⊤]i** **[X][i]**
_Tn_ **[X][⊤]** ! _[−]_


**X[⊤]i** **[X][i]**
_i=1_

X


1

_T_ _−_

1

**Σi′**

_T_ **[Σ][i]**

_i[′]=1_ !

X


**wi[∗]**


_i[′]=1_


1

_T_ _−_

1

**X[⊤]i** **[X][i]**

_Tn_

_i=1_ !

X


1

_i_ **[z][i]**
_Tn_ **[X][⊤]**

_i=1_

X


(47)


where (47) follows from the triangle inequality. We analyze the two terms in (47) separately, starting
with the first term, which we denote by ϑ, namely


1

_T_ _T_ _−_

1 1

_ϑ :=_ _i[′][ X][i][′]_ **X[⊤]i** **[X][i]**

_i=1_ " _Tn_ _i[′]=1_ _Tn_ **[X][⊤]** ! _[−]_

X X

We define the matrix Ai for each i:

1

_T_ _−_

1 1
**Ai =** _i[′][ X][i][′]_ **X[⊤]i** **[X][i]**

_Tn_ _i[′]=1_ _Tn_ **[X][⊤]** ! _[−]_

X


1

_T_ _−_

**Σi′**
_i[′]=1_ !

X

1

_T_ _−_

**Σi′**
_i[′]=1_ !

X


**wi[∗]**


(48)


_T_ **[Σ][i]**


(49)
_T_ **[Σ][i][,]**


with high probability. Note that if we definewhich implies that ϑ := ∥ [P]i[T]=1 **[A][i][w]i[∗][∥][. We proceed to bound the maximum singular value of] C as** **[ A][i]**


**X[⊤]i[′][ X][i][′]**
_i[′]=1_

X


**C :=**


_Tn_


then Ai can be written as


1

_T_ _T_ _−_

1 1

**Ai = C[−][1]** _i_ **[X][i]** **X[⊤]i[′][ X][i][′]** **Σi′**

 _Tn_ **[X][⊤]** _[−]_ _Tn_ _i[′]=1_ ! _T_ _i[′]=1_ !

X X

 [1]

where, defining Λ := _T[1]_ _Ti[′′]=1_ **[Σ][i][′′][ for notational convenience,]**

P _T_

1 1

**Bi =** _i_ **[X][i]** **X[⊤]i[′][ X][i][′][,j]** **Λ[−][1][ 1]**

_Tn_ **[X][⊤]** _[−]_ _Tn_ _i[′]=1_ ! _T_ **[Σ][i]**

X


1

= C[−][1]Bi (50)

_T_ **[Σ][i]**




**Λ[−][1][ 1]**

_T_ **[Σ][i]**


1

_i_ **[X][i][Λ][−][1][Λ][ −]**
_Tn_ **[X][⊤]**


**X[⊤]i[′][ X][i][′]**
_i[′]=1_

X


_Tn_


1

**X[⊤]i** **[X][i][Λ][−][1][Σ][i][′][ −]**

_T_ [2]n

_i[′]=1_

X


1

**X[⊤]i[′][ X][i][′]**

_T_ [2]n

_i[′]=1_

X


**Λ[−][1]Σi**


**X[⊤]i** **[X][i][Λ][−][1][Σ][i][′][ −]** **[X]i[⊤][′][ X][i][′]** **[Λ][−][1][Σ][i]** (51)



_T_ [2]n


_i[′]=1_


-----

Adding and subtracting terms, we have


**X[⊤]i** **[X][i][Λ][−][1][Σ][i][′][ −]** _[n][Σ][i][Λ][−][1][Σ][i][′][ +][ n][Σ][i][′]_ **[Λ][−][1][Σ][i]** _i[′][ X][i][′]_ **[Λ][−][1][Σ][i]**

_[−]_ **[X][⊤]**


**Bi =**


_T_ [2]n


_i[′]=1_


+ [1]

_T_ [2]


**ΣiΛ[−][1]Σi′** **Σi′** **Λ[−][1]Σi**
_−_


_i[′]=1_


1

_i_ **[X][i]**
_n_ **[X][⊤]** _[−]_ **[Σ][i]**




= [1]

_T_

= [1]


+ [1]

_T_ [2]


**Σi′** _i[′][ X][i][′]_ **Λ[−][1]Σi**
_−_ _n[1]_ **[X][⊤]**




**Λ[−][1]**


_T_

**Σi′**
_i[′]=1_

X


**Σi′**

_T_

_i[′]=1_

X

1

_−_ _T[1]_ _T_


_i[′]=1_


+ [1]

_T_ **[Σ][i][Λ][−][1]**


**Λ[−][1]Σi**


**Σi′**
_i[′]=1_

X


_T_

1

= T[1]  _n_ **[X]i[⊤][X][i]** _[−]_ **[Σ][i]** + T[1][2] _i[′]=1_ Σi′ − _n[1]_ **[X]i[⊤][′][ X][i][′]**  **Λ[−][1]Σi**

X

where the last line follows by the definition of Λ.


Next, define


_n_

**Zi =** **Σi** _i_ **[X][i]** = [1] **Σi** **x[(]i[h][)](x[(]i[h][)])[⊤][]**
 _−_ _n[1]_ **[X][⊤]**  _n_ _hX=1_  _−_

for all i = 1, ..., T, where x[(]i[h][)] R[d] is the h-th sample for the i-th task (the h-th row of the matrix
_∈_
**Xi). Using this expression we can write**


**Zi =** **Σi** _i_ **[X][i]**
_−_ _n[1]_ **[X][⊤]**



**Bi = [1]**


**Zi + [1]**
_−_ _T_


**Zi′** **Λ[−][1]Σi**
_i[′]=1_

X


Note that each Zi is the sum of n independent random matrices with mean zero.

Using Lemma 27 from Tripuraneni et al. (2020) with each ai = 1, we have that

P _∥Zi∥≤_ _c1λi,max max_ _c2(_ _d/n + ti/n), c[2]2[(]_ _d/n + ti/n)[2][]_ _≥_ 1 − 2 exp(−t[2]i [)] (52)

for any ti > 0, and for each i ∈ [np]. p

Next, considering the expressions for ϑ, Ai, Bi, C, and Zi, we can write that


_ϑ =_ **Aiwi[∗]** **C[−][1]Biwi[∗]** **Biwi[∗]**

[=] [=]

_i=1_ _i=1_ _i=1_

X X X

_T_ _T_

1

= _i=1_ _T_ _−Zi + T[1]_ _i[′]=1_ **Zi′** **Λ[−][1]Σ[C]i[−]![1]wi[∗]**

Next replace Λ by its definitionXT[1] _Ti[′′]=1_ **[Σ][i][′′][ to obtain]X**

**[C][−][1]** 1

_T_ _T_ _T_ _−_

P

_ϑ =_ **C[−][1][ 1]T** _i=1_ −Zi + _i[′]=1_ **Zi′** _i[′′]=1_ **Σi′′** ! **Σi** **wi[∗]**

X X X

_T_  _T_ _−1_ _T_ 

= **C[−][1][ 1]T** _i[′]=1_ Zi′ −wi[∗][′][ +] _i[′′]=1_ **Σi′′** ! _i=1_ **Σiwi[∗]**

X X X

where in equation 54 we have swapped the summations. This implies  

1

_T_ _T_ _−_ _T_

_ϑ =_ **C[−][1][ 1]** **Zi′** **Σi** **Σi(wi[∗]** _i[′]_ [)]

_T_ _i[′]=1_  _i=1_ ! _i=1_ _[−]_ **[w][∗]** 

X X X

_T_ _T_ _−1_ _T_ 

**C[−][1]** [1] **Zi′** **Σi** **Σi(wi[∗]** _i[′]_ [)]
_≤_ _T_ _i[′]=1_ _∥_ _∥_ _i=1_ ! _i=1_ _[−]_ **[w][∗]**

X X X


**Aiwi[∗]**
_i=1_

X


**C[−][1]Biwi[∗]**
_i=1_

X


(53)

(54)

(55)


-----

where (55) follows by the Cauchy-Schwarz and triangle inequalities. Using these inequalities and
Assumptions 1 and 2 we can further bound ϑ as:

1

_T_ _T_ _−_ _T_

_ϑ_ **C[−][1]** **Zi′** **Σi** **Σi(wi[∗]** _i[′]_ [)][∥]
_≤∥_ _∥_ _T[1]_ _i[′]=1_ _∥_ _∥_ _i=1_ ! _i=1_ _∥_ _[−]_ **[w][∗]**

X X X

1

_T_ _T_ _−_ _T_

**C[−][1]** **Zi′** **Σi** **Σi** **wi[∗]** _i[′]_ _[∥]_
_≤∥_ _∥_ _T[1]_ _i[′]=1_ _∥_ _∥_ _i=1_ ! _i=1_ _∥_ _∥∥_ _[−]_ **[w][∗]**

X X X

1

_T_ _T_ _−_

**C[−][1]** **Zi′** **Σi** 2TLB
_≤∥_ _∥_ _T[1]_ _i[′]=1_ _∥_ _∥_ _i=1_ !

X X

Next, by the dual Weyl inequality for Hermitian matrices, we have λmin _Ti=1_ **[Σ][i]**
_Ti=1_ _[λ][min]_ **Σi** . Thus by Assumption 2, we have λmin _Ti=1_ **[Σ][i]** _≥_ _Tβ, so_   P  _≥_
P    _T_   P  _T_

_ϑ_ **C[−][1]** **Zi** 2LB/β = **C[−][1]** **Zi** (56)
_≤∥_ _∥_ _T[1]_ _∥_ _∥_ _∥_ _∥_ [2]Tβ[LB] _∥_ _∥_

_i=1_ _i=1_

X X

By a union bound and Lemma 27 from Tripuraneni et al. (2020) with each ai = 1, the probability
that any
_∥Zi∥≥_ _c1λmax(Σi) max_ _c2(_ _d/n + ti/n), c[2]2[(]_ _d/n + ti/n)[2][]_ (57)
 p p

is at most 2 _i=1_ [exp(][−][t]i[2][)][. Thus, with probability at least][ 1][ −] [2][ P]i[T]=1 [exp(][−][t]i[2][)]

_T_

_ϑ_ [P][T]C[−][1] _c1λmax(Σi) max_ _c2(_ _d/n + ti/n), c[2]2[(]_ _d/n + ti/n)[2][]_
_≤∥_ _∥_ [2]Tβ[LB]

_i=1_

X  p p

Let t := maxi ti, then using Assumption 2 we have

_ϑ_ **C[−][1]** _c1 max_ _c2(_ _d/n + t/n), c[2]2[(]_ _d/n + t/n)[2][]_ (58)
_≤∥_ _∥_ [2][L]β[2][B]

 p p

with probability at least 1 − 2T exp (−t[2]) for some absolute constants c1 and c2 and any t > 0.

Next we bound ∥C[−][1]∥, where C is the random matrix


**X[⊤]i** **[X][i]** (59)
_i=1_

X


**C =**


_Tn_


Using the dual Weyl inequality again, we have


1

_i_ **[X][i]**
_n_ **[X][⊤]**




_λmin(C)_
_≥_ _T[1]_


(60)


_λmin_
_i=1_

X


Next, using again using Lemma 27 from Tripuraneni et al. (2020) with each ai = 1, as well as Weyl’s
Inequality (Theorem 4.5.3 in Vershynin (2018)), we have

1

_λmin_ _i_ **[X][i]** _λmin (Σi)_ _c1_ **Σi** max _c2(_ _d/n + s/n), c[2]2[(]_ _d/n + t/n)[2][]_

_n_ **[X][⊤]** _≥_ _−_ _∥_ _∥_

   p p

_≥_ _β −_ _c1L max_ _c2(_ _d/n + t/n), c[2]2[(]_ _d/n + t/n)[2][]_

= β − _cL(_ _d/n + t/np_ ) =: φ p (61)

with probability at least 1 − 2 exp (p−t[2]) for any t > 0 and sufficiently large n such that φ > 0,
where c1, c2, and c are absolute constants (note that since L _β, in order for φ to be positive n_
_≥_
must be such that c2( _d/n + t/n)_ 1 assuming c1 1, so we can eliminate the maximization. In

_≤_ _≥_

p


-----

2
. Now combining (61) with (60) and using a



_d+[√]c[2]L[2]d+4βcLt_

2β


_cL_
particular, we must have n ≥


union bound over i, we have


1 _T_
_∥C[−][1]∥_ = _λmin(C)_ _[≤]_ _Ti_ _[β][ −]_ _[cL][(]_ _d/n + s/n)_

1
= P p (62)

_β −_ _cL(_ _d/n + t/n)_

for any t > 0 and n sufficiently large, where c is an absolute constant, with probability at leastp
1 − 2T exp (−t[2]). Using (62) with (58), and noting that both inequalities are implied by the samecL√d+[√]c[2]L[2]d+4βcLt 2

event so no union bound is necessary, and n 2β sufficiently large, we have
_≥_
 

_c[′](_ _d/n + t/n)_ _L[2]B_
_ϑ_ (63)
_≤_ _β_ _cL(_ _d/n + t/n)_ _β_
p

_−_

with probability at least 1 − 2T exp (−t[2]) for some absolute constantsp _c and c[′]_ and any t > 0.

So far, we derived an upper bound for the first term in (47) which we denoted by ϑ. Next,
we consider the second term of (47), which is due to the effect on the additive noise on
the empirical solution. To be more precise, we proceed to provide an upper bound for

1

1 _T_ _−_ _T_ 1

_T n_ _i=1_ **[X]i[⊤][X][i]** _i=1_ _T n_ **[X]i[⊤][z][i]** _[.][ Using the Cauchy-Schwarz inequality, we can bound]_

this term as P  P 

1

_T_ _−_ _T_

1 1

**X[⊤]i** **[X][i]** **X[⊤]i** **[z][i]**

_T_ _T_

_i=1_ ! _i=1_ !

X X

1

_T_ _−_ _T_ _T_

1 1 1 1 1

_i_ **[X][i]** _i_ **[z][i]** _i_ **[z][i]** (64)

_≤_ _T_ Xi=1 _n_ **[X][⊤]** ! _T_ Xi=1 _n_ **[X][⊤]** _[≤∥][C][−][1][∥]_ _T[1]_ Xi=1 _n_ **[X][⊤]**

We have already boundedeach element of the vector ∥ XC[⊤]i _[−][z][1][i][ is the sum of products of a Gaussian random variable with another]∥, so we proceed to bound the term_ _T[1]_ _Ti=1_ _[∥]_ _n[1]_ **[X]i[⊤][z][i][∥][. Note that]**

P

Gaussian random variable. Namely, denoting the (h, s)-th element of the matrix Xi as x[(]i[h][)](s) and
the s-th element of the vector zi as zi(s), then the s-th element of X[⊤]i **[z][i][ is][ P]s[d]=1** _[x][(]i[h][)](s)zi(s). The_
products x[(]i[h][)](s)zi(s) are each sub-exponential, since the products of subgaussian random variables
is sub-exponential (Lemma 2.7.7 in Vershynin (2018)), have mean zero, and are independent from
each other. Thus by the Bernstein Inequality,

_d_

_b[2]_ _b_

P _s=1_ _x[(]i[h][)](s)zi(s)_ _[≥]_ _[b]!_ _≤_ 2 exp _−c[′′]_ min( _ds=1_ _[K][s]_ _,_ maxs Ks )! (65)

X

for some absolute constant c[′′] and any b > 0, where Ks is the sub-exponential norm of the randomP
variable x[(]i[h][)](s)zi(s) (for any h, since the above random variables indexed by h are i.i.d.). Define
_K := maxs Ks Using a union bound over h ∈_ [n], we have

2

_n_ _d_

1
P  _√n_ _∥X[⊤]i_ **[z][i][∥≥]** _[b]_ = P  _l=1_ _s=1_ _x[(]i[h][)](s)zi(s)!_ _≥_ _nb[2]_

X X
 


P _x[(]i[h][)](s)zi(s)_

_≤_

_l=1_ _s=1_ _[≥]_ _[b]_

X X

2
_b_
_≤_ 2n exp _−c[′′]_ min _dK [, b]K_
  


-----

for any b _>_ 0. Thus we have _n1_ _[∥][X]i[⊤][z][i][∥]_ _≤_ _b/[√]n with probability at least 1 −_

2n exp _−c[′′]_ min( _dK[b][2]_ _[,][ b]K_ [)] . Combining this result with (64) with (47) and (63), we obtain
 

_c[′](_ _d/n + t/n)_ _L[2]B_ _b_ 1
_θ ≤_ _β_ _cL(_ _d/n + t/n)_ _β_ + _√n_ _β_ _cL(_ _d/n + t/n)_
p

_−_ _−_
p1 _c[′][√]d + βb_ p

= + _[tL][2][B]_ (66)

_β_ _cL(_ _d/n + t/n)_ _β[√]n_ _βn_ !
_−_

with probability at least p

2
_b_
1 − 2T exp (−t[2]) − 2n exp _−c[′′]_ min _dK [, b]K_ (67)
  

_cL√d+[√]c[2]L[2]d+4βcLt_ 2
for any t, b > 0 and n 2β .
_≥_
 

Now that we have bounds for the terms in (47), we proceed to bound the second term in (46). We
have

**wNAL[(][T][ )][∗]** _NAL[∥]_
_∥_ 1 _[−]T[w][∗]_ _−1 1_ _T_

= **Σi** **Σiwi[∗]** _i_ []]

 _T_ _i=1_  _T_ _i=1_ _[−]_ [E][i][[][Σ][i][]][−][1][E][i][[][Σ][i][w][∗]

X X

1 _T_ _−1 1_ _T_ 1 _T_ _−1_

= **Σi** **Σiwi[∗]** **Σi** Ei[Σiwi[∗][]]

 _T_ _i=1_  _T_ _i=1_ _[−]_  _T_ _i=1_ 

X X X

1 _T_ _−1_
+ **Σi** Ei[Σiwi[∗][]][ −] [E][i][[][Σ][i][]][−][1][E][i][[][Σ][i][w]i[∗][]]

_T_

 _i=1_ 

X

1 _T_ _−1 1_ _T_ _T_ _−1_

**Σi** **Σiwi[∗]** **Σi** Ei[Σiwi[∗][]]

_≤_  _T_ _i=1_  _T_ _i=1_ _[−]_  _i=1_ 

X X X

1 _T_ _−1_

+ **Σi** Ei[Σiwi[∗][]][ −] [E][i][[][Σ][i][]][−][1][E][i][[][Σ][i][w]i[∗][]] (68)

_T_

 _i=1_ 

X

1 _T_ _−1_ 1 _T_

**Σi** **Σiwi[∗]** _i_ []]

_≤_  _T_ _i=1_  _T_ _i=1_ _[−]_ [E][i][[][Σ][i][w][∗]

X X

1 _T_ _−1_

+ **Σi** Ei[Σi][−][1] _i_ []][∥] (69)

_T_ _−_

 _i=1_ 

X

1 _T_ 1 _T_ _[∥][E]−[i][[]1[Σ][i][w][∗]_

= [1] **Σiwi[∗]** _i_ []] [+] **Σi** Ei[Σi][−][1] (70)

_β_ _T_ _i=1_ _[−]_ [E][i][[][Σ][i][w][∗]  _T_ _i=1_  _−_

X X

where in equations (68) and (69) we have used the triangle and Cauchy-Schwarz inequalities,

_[LB]_

respectively, and in (70) we have used the dual Weyl’s inequality and Assumption 2. We first consider
the second term in (70). We can bound this term as


1

_T_




_T_ _−1_

**Σi** _−_ Ei[Σi][−][1]
_i=1_ 

X


1

[=]

_T_



1

_≤_ _T_



_≤_ _β[1][2]_


_−1_ 1

_T_

 

_−1_



(Σi Ei′ [Σi′ ]) Ei[Σi][−][1]
_i=1_ _−_ 

X


**Σi**
_i=1_

X

_T_

**Σi**
_i=1_

X


1

**Σi** **Σi** Ei′ [Σi′ ]
=1  _T_ _i=1_  _−_ 

X X

_T_

[E][i][[][Σ][i][]][−][1]

(Σi Ei′ [Σi′ ]) (71)
_i=1_ _−_

X


-----

using Assumptions 1 and 2.

We use the matrix Bernstein inequality (Theorem 6.5 in Tropp (2015)) to bound _T_ _Ti=1[(][Σ][i][ −]_
_∥_ [1]

EEi[′]i[[(ΣΣi[′] ])i _∥, noting that eachEi′_ [Σi′ ])[2]] . By matrix Bernstein and equation (71) we obtain Σi − Ei[′] [Σi[′] ] is an iid matrix with mean zero. Recall thatP Var(Σi) :=
_∥_ _−_ _∥_

1 _T_ _−1_ _δ Var(Σi)_
 _T_ _i=1_ **Σi** _−_ Ei[Σi][−][1] _[≤]_ _β[1][2]_ _√T_ (72)

X

with probability at least 1 2d exp 1+Lδ/(3√T ), for any δ > 0. Similarly, we have that
_−_ _−_ _[δ][2][ Var][(][Σ][i][)][/][2]_
 

_T_

_β1_ _T1_ _i=1_ **Σiwi[∗]** _[−]_ [E][i][[][Σ][i][w]i[∗][]] _[≤]_ _β[1]_ _δ Var(√ΣTiwi,∗)_ (73)

X

with probability at least 1 2d exp 1+LBδ/(3√i [)]T[/] )[2] .
_−_ _−_ _[δ][2][ Var][(][Σ][i][w][∗]_
 

Thus (70) reduces to:

**wNAL[(][T][ )][∗]** _NAL[∥≤]_ [1] _δ Var(Σiwi,∗)_ + _[LB]_ _δ Var(Σi)_ (74)
_∥_ _[−]_ **[w][∗]** _β_ _√T_ _β[2]_ _√T_

with probability at least 1 2d exp 1+Lδ/(3√T ) 2d exp 1+LBδ/(3√i [)]T[/] )[2] .
_−_ _−_ _[δ][2][ Var][(][Σ][i][)][/][2]_ _−_ _−_ _[δ][2][ Var][(][Σ][i][w][∗]_
   

We combine this result with (47) and (66) via a union bound to obtain

1 _c[′][√]d + βb_
**wNAL** **wNAL[∗]** _[∥≤]_ + _[tL][2][B]_
_∥_ _−_ _β_ _cL(_ _d/n + t/n)_ _β[√]n_ _βn_ !

_−_
p

+ _[δ][ Var]β[(]√[Σ]T[i][w][i,][∗][)]_ + _[δ][ Var]β[2][(][LB][√]T[Σ][i][)]_ (75)

with probability at least

2
_b_
1 − 2T exp(−t[2]) − 2n exp _−c[′′]_ min _dK [, b]K_ _−_ 2d exp _−_ 1 +[δ][2][ Var] Lδ/[(][Σ](3[i]√[)][/]T[2])
    

_i_ [)][/][2]
_−_ 2d exp _−_ 1 +[δ][2][ Var] LBδ/[(][Σ][i](3[w]√[∗] _T_ ) (76)
 

_cL√d+[√]c[2]L[2]d+4βcLt_ 2
as long as n 2β . Finally, choose t = log(200T ), b =
_≥_
 

_dK_ _K_ p

_c[′′][ log(200][n][) +]_ _c[′′][ log(200][n][)][ and][ δ]_ = ( Var(Σi) + Var(Σiwi[∗][))][−][1][ log(200][d][)][ and re-]

2

qstrict n _≥_ 4 _cL√d+[√]c2[2]βL[2]d+4βcLt_ such thatp _β−cL([√]1d/n+t/n)_ _≤_ _β2_ and T _>_

 

_L[2]B[2]_ log(200d)/(9(Var(Σi) + Var(Σiwi[∗][)))][. This ensures that each negative term in the high]
probability bound (124) is at most 0.01 and thereby completes the proof.


-----

D.2 MAML CONVERGENCE

We first state and prove the following lemma.

**Lemma 2. Let A be a fixed symmetric matrix in R[d][×][d], and let X be a random matrix in R[n][×][d]**
_whose rows are i.i.d. multivariate Gaussian random vectors with mean 0 and diagonal covariance Σ._
_Then_

1 1

**EX** **A** = ΣAΣ + [1] (77)

_n_ **[X][⊤][X]** _n_ **[X][⊤][X]** _n_ [(][tr][(][ΣA][)][I][d][ +][ ΣA][)][ Σ]

   


_Proof. Letting xk denote the k-th row of X, we have_

1 1

**EX** **A**

_n_ **[X][⊤][X]** _n_ **[X][⊤][X]**

   


!#


**x[⊤]k** **[x][k]**
_k=1_

X


**x[⊤]k** **[x][k]**
_k=1_

X


= EX

= Exk


_n_ _n_ _n_

1

= Exk **x[⊤]k** **[x][k][Ax]k[⊤][′]** **[x][k][′]** + Exk **x[⊤]k** **[x][k][Ax]k[⊤][x][k]**

 _n[2]_ _kX=1_ _k[′]=1X,k[′]≠_ _k_  " _n[2]_ _kX=1_ #

 [1] _n_ 

= _[n][ −]_ [1] **ΣAΣ + [1]** **Exk** **x[⊤]k** **[x][k][Ax]k[⊤][x][k]** (78)

_n_ _n_

_k=1_

X  


Let Ck = x[⊤]k **[x][k][Ax]k[⊤][x][k][ for][ k][ ∈]** [[][n][]][, and let][ λ][i][ be the][ i][-th diagonal element of][ Σ][ for][ i][ ∈] [[][d][]][. Then]
for any k, using the fact that the elements of xk are independent, have all odd moments equal to 0,
and have fourth moment equal to 3λ[2]i [for the corresponding][ i][, it follows that]


_λr_ _dj=1_ _[λ][j][a][j,j][ + 2][λ]r[2][A][r,r][,]_ if r = s
(79)
_λrλs(Ar,s + As,r),_ otherwise
P


E[Cr,s] =

Using (79), we can write


E[Ck] = tr(ΣA)Σ + Σ(A + A[⊤])Σ
= tr(ΣA)Σ + 2ΣAΣ (80)

where (80) follows by the symmetry of A. Plugging (80) into (78) completes the proof.

Now we have the main convergence result. Analogously to Theorem 2, we define Var(Qi) :=
Ei[(Qi Ei′ [Qi′ ])[2]] 2 and Var(Qiwi, ) := Ei[(Qiwi, Ei′ [Qi′ **wi,** ])[2]] 2.
_∥_ _−_ _∥_ _∗_ _∥_ _∗_ _−_ _∗_ _∥_

**Theorem 3. (MAML Convergence, General Statement) Define** _β[ˆ] := β(1_ _αL)[2]_ + _[α][2][β][3]n[(]2[d][+1)]_ _and_
_−_

_Lˆ := L(1_ _αβ)[2]_ + _[α][2][L][3]n[(]2[d][+1)]_ _. If Assumptions 1 and 2 hold, the distance of the MAML training_
_−_

_solution (4) to its population-optimal value (5) is bounded as_


16LBd[ˆ]
_∥wMAML −_ **wMAML[∗]** _[∥≤]_


_α[4]L[6]_


log[3](100Td)


_βˆ[2][√]τ_


_Var(Qiwi[∗][) log(200][d][)]_


_Var(Qi) log(200d)_

_βˆ[2][√]T_


_LBˆ_


(81)


_with probability at least 0.96, for some absolute constants c and c[′], and any τ_ _>_
32d[3]α[4]L[6] log[6](100Td)/(cβ[ˆ]) and T > _L[ˆ][2]B[2]_ log(200d)/(9(Var(Qi) + Var(Qiwi[∗][)))][.]

_Proof. The proof follows the same form as the proof of Theorem 2. Here the empirical covariance_
matrices _n[1]_ **[X]i[⊤][X][i][ and their means][ Σ][i][ are replaced by the empirical][ preconditioned][ covariance]**

matrices **Q[ˆ]** _i,j and their means Qi, respectively. As before, a critical aspect of the proof will be to_


-----

show concentration of the empirical (preconditioned) covariance matrix to its mean. To do so, we
re-define the perturbation matrices Zi:


**Qˆ** _i,j_ **Qi** (82)
_j=1_ _−_

X


**Zi :=**


_τn1_


where
**Qˆ** _i,j := P[⊤]i,j[(][X][out]i,j_ [)][⊤][X]i,j[out][P][i,j]
and

_τ_

**Qi := E** **Qˆ** _i,j_

 _τn1_ 

_j=1_

X

 [1] _τ_ 

= E[P[⊤]i,j[(][X][out]i,j [)][⊤][X]i,j[out][P][i,j][]] (83)

 _τn1_ 

_j=1_

X

 [1] _τ_ 

1
= E **Id** (X[in]i,j[)][⊤][X]i,j[in] **Σi** **Id** (X[in]i,j[)][⊤][X]i,j[in]

_τn1_ _j=1_  _−_ _n[α]2_   _−_ _n[α]2_ 

X

_τ_

1
= **Σi** 2αΣ[2]i [+][ α][2] E (X[in]i,j[)][⊤][X]i,j[in] [(][X]i,j[out][)][⊤][X]i,j[out][(][X]i,j[in] [)][⊤][X]i,j[in]

_τn1_ _j=1_  _−_ _n[2]2_

X  []

= (Id _αΣi)Σi(Id_ _αΣi) +_ _[α][2]_ (tr(Σ[2]i [)][Σ][i] [+][ Σ][3]i [)] (84)
_−_ _−_ _n2_


where (84) follows by Lemma 2.

Note that here Zi has higher-order matrices than previously. Lemma 4 nevertheless gives the key
concentration result for the Zi’s, which we will use later. For now, we argue as in Theorem 2: for any
_T, τ ≥_ 1, we define

**wMAML[(][T,τ]** [)]

_T_ _τ_ _T_ _τ_

:= _T τn1_ 1 _i=1_ _j=1_ **Qˆ** _i,j_ _−1_ _T τn1_ 1 _i=1_ _j=1(Q[ˆ]_ _i,jwi[∗]_ [+][ P][i,j][X]i,j[⊤] **[z]i,j[out]** _[−]_ _n[α]2_ **Pi,jX[⊤]i,j[X][i,j]X[ˆ]** _[⊤]i,j[ˆ]z[in]i,j[)][.]_
  X X  X X

(We next fixT[1] _Ti=1_ **[Q][i] T[)][−] and define the asymptotic solution over[1 1]T** _Ti=1_ **[Q][i][w]i[∗][. Again using the triangle inequality we have] T tasks as τ →∞, namely wMAML[(][T][ )][∗]** [:=]

P _∥wMAML[(][T,τ]_ [)] P[−] **[w]MAML[∗]** _[∥]_ [=][ ∥][w]MAML[(][T,τ] [)] _[−]_ **[w]MAML[(][T][ )][∗]** [+][ w]MAML[(][T][ )][∗] _[−]_ **[w]MAML[∗]** _[∥]_

**wMAML[(][T,τ]** [)] _MAML[∥]_ [+][ ∥][w]MAML[(][T][ )][∗] _MAML[∥]_ (85)
_≤∥_ _[−]_ **[w][(][T][ )][∗]** _[−]_ **[w][∗]**

The first term in (85) captures the error due to have limited samples per task during training, and the
second term captures the error from having limited tasks from the environment during training. We
first bound the first term in (85), which we denote asNote that by Assumption 2, we have _θ = ∥wMAML[(][T,τ]_ [)] _[−]_ **[w]MAML[(][T][ )][∗]** _[∥]_ [for convenience.]
_βˆId_ **Qi** _LId_ _i_ (86)
_⪯_ _⪯_ [ˆ] _∀_

where _β[ˆ] := β(1_ _αL)[2]_ + _[α][2][β][3]n[(]2[d][+1)]_ and _L[ˆ] := L(1_ _αβ)[2]_ + _[α][2][L][3]n[(]2[d][+1)]_ . Thus, using the argument

from (47) to (56) in the proof of Theorem 2, with − _−n[1]_ **[X]i,j[⊤]** **[X][i,j][ replaced by]** _τn1_ 1 _τj=1_ **Q[ˆ]** _i,j and Σi_

replaced by Qi, we obtain
P

_T_ _τ_ _−1_


_θ ≤_ _ϑ +_


**Qˆ** _i,j_

_i=1_ _j=1_

X X

_T_ _τ_ 

**Pi,j(X[out]i,j** [)][⊤][z]i,j[out] **Pi,j(X[out]i,j** [)][⊤][X]i,j[out][(][X]i,j[in] [)][⊤][z][in]i,j

_i=1_ _j=1_  _[−]_ _n[α]2_

X X


_Tτn1_

1

_Tτn1_


(87)


-----

where


_−1_
1

_T_ **[Q][i]** **wi[∗]**

!





_−1_


_T_ _T_ _τ_

1 1

_ϑ :=_ **Qˆ** _i[′],j_ **Qˆ** _i,j_

_i=1_  _T_ _i[′]=1_ _n1τ_ _j=1_  _T_ _−_

X X X



Defining C := _T[1]_ _Ti[′]=1_ [1]n11τ _τj=1_ **Q[ˆ]** _i[′],j, we have,_

1

_TP_ P _T_ _−_

1 1 1

_ϑ =_ **Qi,j** **Qi′**

_i=1_ " _T_ **[C][−][1][ ˆ]** _−_ _T_ _i[′]=1_ ! _T_ **[Q][i]#**

X X


**Qi[′]**
_i[′]=1_

X


_LB_
_≤∥C[−][1]∥_ [2ˆ]T _β[ˆ]_


**wi[∗]**


**Zi** (88)
_∥_ _∥_
_i=1_

X


where to obtain the inequality we have swapped the order of the summations as in (54). Next we
bound each **Zi** with high probability, by first showing element-wise convergence to 0.
_∥_ _∥_

**Lemma 3. Consider a fixed i ∈** [T ], k ∈ [d], and s ∈ [d], and let _Q[ˆ]i,j(k, s) be the (k, s)-th element_
_of the matrix_ **Q[ˆ]** _i,j defined in (93). The following concentration result holds for the random variable_

_τn1_ 1 _τj=1_ _Q[ˆ]i,j(k, s) Q[ˆ]i,j(k, s):_

_τ_ 1/6[!]

P

1 _τγ[2]_

P _Qˆi,j(k, s)_ _Qi(k, s)_ _γ_ _e[2]_ exp (89)

 _τn1_ _−_ _≤_  _≥_ _−_ _cα[4]d[2]L[6]_

_j=1_  

X

 1 _τ_ 

_for some any γ > 0, where Qi = E[_ _τn1_ _j=1_ **Q[ˆ]** _i,j] as defined in e.g. (83)._

P

_Proof. We start by computing the (k, s)-th element of Zi. First we compute the (k, l)-th element_
of the matrixn1-dimensional vector of the Di,j := (Id − r-th-dimensional elements from all of thenα2 [(][X]i,j[in] [)][⊤][X][in]i,j[)(][X][out]i,j [)][⊤][X]i,j[out][. Here, we define] n1 outer samples for the[ x]i,j[(][r][)] _[∈]_ [R][n][1][ as the] j-th

instance of task i, and ˆx[(]i,j[k][)]
from all of the n2 outer samples for task[∈] [R][n][2][ as the] i[ n]. Then we have[2][-dimensional vector of the][ k][-th-dimensional elements]


_d_

(1r=k **xˆ[(]i,j[k][)][,][ ˆ]x[(]i,j[r][)]** _i,j_ _[,][ x]i,j[(][l][)][⟩]_
_r=1_ _−_ _n[α]2_ _⟨_ _[⟩][)][⟨][x][(][r][)]_

X


_Di,j(k, l) =_


_α_

= **x[(]i,j[k][)][,][ x]i,j[(][l][)][⟩−]** **xˆ[(]i,j[k][)][,][ ˆ]x[(]i,j[r][)]** _i,j_ _[,][ x]i,j[(][l][)][⟩]_ (90)
_⟨_ _r=1_ _n2_ _⟨_ _[⟩⟨][x][(][r][)]_

X

Then, the (k, s)-th element of **Q[ˆ]** _i,j is_
**Qˆ** _i,j(k, s)_

= **P[⊤]i,j[X][⊤]i,j[X][i,j][P][i,j]** (k, s)

" #

_d_

= _l=1_ _Di,j(h, l)(1l=s −_ _n[α]2_ _⟨xˆ[(]i,j[l][)][,][ ˆ]x[(]i,j[s][)][⟩][)]_

X

_d_ _d_

_α_

= (1l=s **xˆ[(]i,j[l][)][,][ ˆ]x[(]i[s][)]** **x[(]i,j[k][)][,][ x]i[(][l][)]** **xˆ[(]i,j[k][)][,][ ˆ]x[(]i,j[r][)]** _i,j_ _[,][ x]i,j[(][l][)][⟩]_

_l=1_ _−_ _n[α]2_ _⟨_ _[⟩][)]_ _⟨_ _[⟩−]_ _r=1_ _n2_ _⟨_ _[⟩⟨][x][(][r][)]_ !

X X


_⟨x[(]i,j[k][)][,][ x]i,j[(][s][)][⟩−]_


_α_

**xˆ[(]i,j[k][)][,][ ˆ]x[(]i[r][)]** _i,j_ _[,][ x][(]i,j[s][)]_
_n2_ _⟨_ _[⟩⟨][x][(][r][)]_ _[⟩]_


_r=1_


_−_ _n[α]2_


**xˆ[(]i,j[l][)][,][ ˆ]x[(]i,j[s][)]**
_l=1⟨_ _[⟩]_

X


_⟨x[(]i,j[k][)][,][ x]i,j[(][l][)][⟩−]_


_α_

**xˆ[(]i,j[k][)][,][ ˆ]x[(]i,j[r][)]** _i,j_ _[,][ x]i,j[(][l][)][⟩]_
_n2_ _⟨_ _[⟩⟨][x][(][r][)]_


_r=1_


_d_

**xˆ[(]i,j[k][)][,][ ˆ]x[(]i,j[r][)]** _i,j_ _[,][ x][(]i,j[s][)]_
_r=1⟨_ _[⟩⟨][x][(][r][)]_ _[⟩]_ [+][ α]n[2]2[2]

X


= **x[(]i,j[k][)][,][ x]i,j[(][s][)]**
_⟨_ _[⟩−]_ [2]n[α]2


_r=1⟨xˆ[(]i,j[l][)][,][ ˆ]x[(]i,j[s][)][⟩⟨]x[ˆ][(]i,j[k][)][,][ ˆ]x[(]i[r][)][⟩⟨][x]i,j[(][r][)][,][ x]i,j[(][l][)][⟩]_

X

(91)


_l=1_


-----

Therefore the (k, s) element of _τn1_ 1 _τj=1_ **Q[ˆ]** _i,j is_

_τ_ _τP_

1 1

**Qˆ** _i,j_ (k, s) = **x[(]i,j[k][)][,][ x][(]i,j[s][)]**

_τn1_ _j=1_ _τn1_ _j=1_ _⟨_ _[⟩−]_ [2]n[α]2

h X i X h


_r=1⟨xˆ[(]i,j[k][)][,][ ˆ]x[(]i,j[r][)][⟩⟨][x]i,j[(][r][)][,][ x][(]i,j[s][)][⟩]_

X


+ _[α][2]_

_n[2]2_


_r=1⟨xˆ[(]i,j[l][)][,][ ˆ]x[(]i,j[s][)][⟩⟨]x[ˆ][(]i,j[k][)][,][ ˆ]x[(]i,j[r][)][⟩⟨][x]i,j[(][r][)][,][ x]i,j[(][l][)][⟩]_ (92)

X i


_l=1_


As we can see, this is a polynomial in the independent Gaussian random variables
_xˆ[(]i,j[r][)][(][q][)][}][j][∈][[][τ]_ []][,r][∈][[][d][]][,q][∈][[][n]2[]] [and][ {][x][(]i,j[r][)][(][q][)][}][j][∈][[][τ] []][,r][∈][[][d][]][,q][∈][[][n]1[]][, for a total of][ dτ] [(][n][1][ +][ n][2][)][ random vari-]
_{ables. Moreover,_ _τn1_ 1 _τj=1_ _Q[ˆ]i,j(k, s) is the average of τ i.i.d. random variables indexed by j._

Define these random variables as uj for j [τ ], i.e.,
P _∈_


2α

_uj = [1]_ **x[(]i,j[k][)][,][ x]i,j[(][s][)]**

_n1_ _⟨_ _[⟩−]_ _n1n2_


_r=1⟨xˆ[(]i,j[k][)][,][ ˆ]x[(]i,j[r][)][⟩⟨][x]i,j[(][r][)][,][ x][(]i,j[s][)][⟩]_

X


_α[2]_

_n1n[2]2_

1

_τn1_


_r=1⟨xˆ[(]i,j[l][)][,][ ˆ]x[(]i,j[s][)][⟩⟨]x[ˆ][(]i,j[k][)][,][ ˆ]x[(]i,j[r][)][⟩⟨][x]i,j[(][r][)][,][ x]i,j[(][l][)][⟩]_ (93)

X


_l=1_


such that

Then the variance of


_τ_

_Qˆi,j(k, s) = [1]_

_τ_

_j=1_

X


_uj_ (94)
_j=1_

X


_τn1_ 1 _τj=1_ _Q[ˆ]i,j(k, s) decreases linearly with τ_, since

_τ_ P _τ_

_Qˆi,j(k, s)_ = E  _τn1_ _Qˆi,j(k, s) −_ _Qi(k, s)_
_j=1_ _j=1_

X X



  _τ[1]_ _τ_ 

= E[( [1] _uj_ E[uj])[2]]

_τ_ _j=1_ _−_ _τ[1]_ _j=1_

X X


2[]




 _τn1_

 [1]


Var


= [1]

_τ_ [2]


_j=1_ E[(uj − E[uj])[2]]

X


+ [1]

_τ_ [2]


E[(uj E[uj])]E[(uj′ E[uj])] (95)
_j[′]=1X,j[′]≠_ _j_ _−_ _−_


_j=1_


= [1]

_τ_ [E][[(][u][1][ −] [E][[][u][1][])][2][]]

1[]]

_≤_ _τ[1]_ [E][[][u][2]


where (95) follows from the independence of the uj’s. Next, recall the definition of u1 given in (93):


2α

_u1 = [1]_ **x[(]i,[k]1[)][,][ x]i,[(][s]1[)][⟩−]**

_n1_ _⟨_ _n1n2_


_⟨xˆ[(]i,[k]1[)][,][ ˆ]x[(]i,[r]1[)][⟩⟨][x][(]i,[r]1[)][,][ x][(]i,[s]1[)][⟩]_
_r=1_

X


_α[2]_

_n1n[2]2_


_⟨xˆ[(]i,[l]1[)][,][ ˆ]x[(]i,[s]1[)][⟩⟨]x[ˆ][(]i,[k]1[)][,][ ˆ]x[(]i,[r]1[)][⟩⟨][x][(]i,[r]1[)][,][ x][(]i,[l]1[)][⟩]_ (96)
_r=1_

X


_l=1_


The second moment of u1 is dominated by the higher-order terms in u[2]1[, since these terms have]
the largest expectation (being of the highest order) and are the most populous. To see that they are
the most populous, note that u1 has d[2]n1n[2]2 [monomials with six random variables, and only][ dn][1][n][2]
monomials with four random variables and n1 monomials with two random variables. Thus E[u[2]1[]]
is at most a constant times the expectation of the d[4]n[2]1[n][4]2 [monomials of 12 variables in][ u]1[2][. Each of]


-----

these monomials in 12 variables is the product of 12 one-dimensional, mean-zero Gaussian random
variables, 8 corresponding to inner-loop samples and 4 corresponding to outer-loop samples. The
maximum expected value of each of these monomials is thus the eighth moment of the inner loop
random variable with maximum variance times the fourth moment of the outer-loop random variable
with maximum variance. Since the variables are Gaussian with maximum variance L, the maximum
expected value of each monomial is 105L[4] _× 3L[2]_ = 315L[6]. This yields our upper bound on the
variance. The following analysis formalizes this argument:

_τ_

Var _Qˆi,j(k, s)_

 _τn1_ 

_j=1_

X

 [1] 

1[]]

_≤_ _τ[1]_ [E][[][u][2]

_n1_ _d_

1

= [1] _x[(]i,[k]1[)][(][h][)][x]i,[(][s]1[)][(][h][)][ −]_ [2][α] _x[(]i,j[r][)][(][h][)][x][(]i,j[s][)][(][h][)][⟨]x[ˆ][(]i,j[k][)][,][ ˆ]x[(]i,j[r][)]_

_τ_ [E] _n1_ _h=1_ _n2_ _r=1_ _[⟩]_

" X  X

_d_ _d_

2

+ _[α][2]_ _x[(]i,j[r][)][(][h][)][x]i,j[(][l][)][(][h][)][⟨]x[ˆ][(]i,j[h][)][,][ ˆ]x[(]i,j[s][)]_ **x[(]i,j[k][)][,][ ˆ]x[(]i,j[r][)]**

_n[2]2_ _l=1_ _r=1_ _[⟩⟨][ˆ]_ _[⟩]_ #

X X 

2[]

_c_ _α[2]_ _n1_ _d_ _d_

_x[(]i,j[r][)][(][h][)][x]i,j[(][l][)][(][h][)][⟨]x[ˆ][(]i,j[l][)][,][ ˆ]x[(]i,j[s][)]_ **x[(]i,j[k][)][,][ ˆ]x[(]i,j[r][)]**

_≤_ _n[2]1[τ][ E]_  _n[2]2_ _h=1_ _l=1_ _r=1_ _[⟩⟨][ˆ]_ _[⟩]!_

X X X

_cα[4]_  _n1_ _n1_ _d_ _d_ _d_ _d_ 
= _x[(]i,j[r][)][(][h][)][x]i,j[(][l][)][(][h][)][⟨]x[ˆ][(]i,j[l][)][,][ ˆ]x[(]i,j[s][)]_ **x[(]i,j[k][)][,][ ˆ]x[(]i,j[r][)]**

_n[2]1[n][4]2[τ][ E]"_ _h=1_ _h[′]=1_ _l=1_ _r=1_ _l[′]=1_ _r[′]=1_ _[⟩⟨][ˆ]_ _[⟩]_

X X X X X X

_x[(]i,j[r][′][)][(][h][′][)][x]i,j[(][l][′][)][(][h][′][)][⟨]x[ˆ][(]i,j[l][′][)][,][ ˆ]x[(]i,j[s][)][⟩⟨]x[ˆ][(]i,j[k][)][,][ ˆ]x[(]i,j[r][′][)][⟩]#_

_cα[4]_ _n1_ _n1_ _d_ _d_ _d_ _d_ _n2_
= _x[(]i,j[r][)][(][h][)][x]i,j[(][l][)][(][h][)]_ _xˆ[(]i,j[l][)][(][q][)ˆ]x[(]i,j[s][)][(][q][)]_

_n[2]1[n][4]2[τ][ E]"_ _h=1_ _h[′]=1_ _l=1_ _r=1_ _l[′]=1_ _r[′]=1_ _q=1_ !

X X X X X X X

_n2_ _n2_ _n2_

_xˆ[(]i,j[k][)][(][q][)ˆ]x[(]i,j[r][)][(][q][)]_ _x[(]i,j[r][′][)][(][h][′][)][x]i,j[(][l][′][)][(][h][′][)]_ _xˆ[(]i,j[l][′][)][(][q][)ˆ]x[(]i,j[s][)][(][q][)]_ _xˆ[(]i,j[k][)][(][q][)ˆ]x[(]i,j[r][′][)][(][q][)]_
_q=1_ ! _q=1_ ! q=1 ! #

X X X

(97)


By independence, each term in the above summation has expectation zero unless (i) r = l and r[′] = l[′],
or (ii) r = r[′], l = l[′] and h = h[′]. There are n[2]1[d][2][ distinct values of][ (][h, h][′][, r, r][′][, l, l][′][)][ that satisfy (i),]
and n1d[2] _≤_ _n[2]1[d][2][ distinct values of][ (][h, h][′][, r, r][′][, l, l][′][)][ that satisfy (ii). For simplicity we treat each]_
term in the summations over n2 as having non-zero mean, although this can tightened. Since the
expectation of each non-zero-mean monomial is at most 315L[6], we have

_τ_

Var _Qˆi,j(k, s)_ (98)

 _τn1_  _≤_ [730][cd]τ [2][α][4]

_j=1_

X

for some absolute constant c. With this bound on [1] Var ( _τn1_ 1 _τj=1_ _Q[ˆ]i,j(k, s)), the result directly_

follows from Theorem 1.9 in Schudy & Sviridenko (2012), noting thatP _τn1_ 1 _τj=1_ _Q[ˆ]i,j(k, s)−Qi(k, s)_

is a degree 6 polynomial in centered independent Gaussian random variables. Note that by the
P
argument in Schudy & Sviridenko (2012), the bound on the elements of Zi given in Lemma 3 is tight
up to logarithmic factors.

Next, we show that this result implies a high probability bound on each **Zi** .
_∥_ _∥_

**Lemma 4. For all task indices i, any γ > 0, and some absolute constant c, the following holds:**

1/6[!]

_τγ[2]_

P ( **Zi** _γ)_ 1 _e[2]d[2]_ exp (99)
_∥_ _∥≤_ _≥_ _−_ _−_ _cα[4]d[2]L[6]_

 


-----

_Proof. By Lemma 3, we have a high-probability bound on the size of the elements of Zi._

Using this bound, we can bound the spectral norm of Zi using the Gershgorin Circle Theorem, which
says that every eigenvalue of Zi lies within one of the Gershgorin disks Dk defined by

Dk := {λ : λ ∈ [Zi(k, k) − _Rk, Zi(k, k) + Rk]}_ (100)

where Rk = _s=1,s≠_ _k_ _[|][Z][i][(][k, s][)][|][. This implies that]_

**Zi** max (101)

[P][d] _∥_ _∥≤_ _k∈d_ _[Z][i][(][k, k][) +][ R][k]_

so we attempt to bound the RHS of the above. By a union bound over k, we have that for any γ > 0,

P (Rk (d 1)γ) P( _s=k_ _Q[˜]i(k, s)_ _Q(k, s)_ _γ_ )
_≥_ _−_ _≤_ _∪_ _̸_ _{|_ _−_ _| ≥_ _}_

_d_ 1/6[!]

_τγ[2]_

_e[2]_ exp

_≤_ _−_ _cα[4]d[2]L[6]_

_s=1,s=k_  

X̸

1/6[!]

_τγ[2]_

= (d 1)e[2] exp (102)
_−_ _−_ _cα[4]d[2]L[6]_

 

where (102) follows from Lemma 3. We also have that

1/6[!]

_τγ[2]_

P ( _Zi(k, k)_ _γ)_ _e[2]_ exp (103)
_|_ _| ≥_ _≤_ _−_ _cα[4]d[2]L[6]_

 

again using Lemma 3. Combining (102) and (103) via a union bound yields for some particular s


P (Zi(k, k) + Rk ≤ _γ) ≥_ P ({Rk ≤ (d − 1)γ} ∩{Zi(k, k) ≤ _γ})_
= 1 − P ({Rk ≥ (d − 1)γ} ∪{Z(k, k) ≥ _γ})_
_≥_ 1 − P ({Rk ≥ (d − 1)γ}) − P ({Z(k, k) ≥ _γ})_

1/6[!]

_τγ[2]_

= 1 _e[2]d exp_
_−_ _−_ _cα[4]d[2]L[6]_

 

Therefore, via a union bound over k, we have

P max = 1 P _k_ [d] _Zi(k, k) + Rk_ _γ_
k∈[d][{][Z][i][(][k, k][) +][ R][k][} ≤] _[γ]_ _−_ _∪_ _∈_ _{_ _≥_ _}_
  1/6[!]

_τγ[2]_

1 _e[2]d[2]_ exp
_≥_ _−_ _−_ _cα[4]d[2]L[6]_

 

for any γ > 0. As a result, using (101) we have

1/6[!]

_τγ[2]_

P ( **Zi** _γ)_ 1 _e[2]d[2]_ exp
_∥_ _∥≤_ _≥_ _−_ _−_ _cα[4]d[2]L[6]_

 


(104)

(105)

(106)


We continue with the proof of Theorem 3 by bounding ∥C[−][1]∥. Using an analogous argument as in
the proof of Theorem 2 based on Weyl’s Inequality (Theorem 4.5.3 in (Vershynin, 2018)), we have


_λmin(C)_ _λmin( [1]_
_≥_ _T_


**Qi)** **Zi**
_−∥_ _∥_
_i=1_

X


_≥_ _β[ˆ] −_ _γ_ (107)

where (107) follows by (86) and Lemma 4, with high probability, for any γ > 0. Thus choosing
_γ <_ _β[ˆ], we have_


_∥C[−][1]∥≤_


1

(108)
_βˆ −_ _γ_


-----

Using this with (88) and Lemma 4, and using a union bound over i ∈ [T ], we have

1/6[!]

_LBγ_ 1 _τγ[2]_

P _ϑ_ _e[2]Td[2]_ exp

_≥_ [2ˆ]βˆ _βˆ −_ _γ_ ! _≤_ _−_  _cα[4]d[2]L[6]_ 


(109)


Next, we turn to bounding the second term in (87), which is the error term due to the additive noise in
the linear regression setting. We will do this by again making an argument based on the concentration
of a polynomial in independent, centered Gaussian random variables around its mean. First, note that
the term we must bound is


_−1_



**Pi,j(X[out]i,j** [)][⊤][X]i,j[out][P][i,j]
_j=1_

X


_Tn1τ_

1

_Tn1τ_


_i=1_

_T_

_i=1_

X


_τ_

**Pi,j(X[out]i,j** [)][⊤][z]i,j[out] **Pi,j(X[out]i,j** [)][⊤][X]i,j[out][(][X]i,j[in] [)][⊤][z][in]i,j
_j=1_ _[−]_ _n[α]2_

X

_T_ _τ_ _−1_

**Pi,j(X[out]i,j** [)][⊤][X]i,j[out][P][i,j]

_i=1_ _j=1_ 

X X


_Tn1τ_

1

_Tn1τ_


_τ_

**Pi,j(X[out]i,j** [)][⊤][z]i,j[out] **Pi,j(X[out]i,j** [)][⊤][X]i,j[out][(][X]i,j[in] [)][⊤][z]i,j[in]
_j=1_ _[−]_ _n[α]2_

X


1

**Pi,j(X[out]i,j** [)][⊤][z]i,j[out] **Pi,j(X[out]i,j** [)][⊤][X]i,j[out][(][X]i,j[in] [)][⊤][z]i,j[in] (110)

_Tn1τ_  _i=1_ _j=1_ _[−]_ _n[α]2_ 

X X

_T_ _τ_

1

= **C[−][1]** **Pi,j(X[out]i,j** [)][⊤][z]i,j[out] **Pi,j(X[out]i,j** [)][⊤][X]i,j[out][(][X]i,j[in] [)][⊤][z]i,j[in] (111)

_Tn1τ_  _i=1_ _j=1_ _[−]_ _n[α]2_ 

X X

where (110) follows from the Cauchy-Schwarz Inequality. Define

_T_ _τ_

1

_g(k) :=_ **Pi,j(X[out]i,j** [)][⊤][z]i,j[out] **Pi,j(X[out]i,j** [)][⊤][X]i,j[out][(][X]i,j[in] [)][⊤][z]i,j[in] (k) (112)

" _Tn1τ_  _i=1_ _j=1_ _[−]_ _n[α]2_ [#]

X X

for eachT n11τ _kTi ∈=1[d], i.e.τj=1_ _g[P]([i,j]k)[(] is the[X]i,j[out][)] k[⊤][z]-th element of thei,j[out]_ _[−]_ _n[α]2_ **[P][i,j][(][X]i,j[out] d-dimensional vector[)][⊤][X]i,j[out][(][X]i,j[in]** [)][⊤][z][in]i,j, and let _g_ :=
[g(k)]1 _k_ _d._ Note that each g(k) is a degree-6 polynomial in independent, centered Gaus-

_≤P≤_ P
sian random variables. One can see that it is degree-6 because after expanding the Pi,j matrices,
there are five data matrices and a noise vector (six total matrices) in the highest-order product. Also
note that this polynomial has mean zero since the noise has mean zero and is independent of the data.
Its variance E[(g(k) − E[g(k)])[2]] can be upper bounded using a similar argument as in Lemma 3.
We will not write the full calculations and argument since they are very similar to those in Lemma 3.
We again use the fact that the polynomial in question is the sum of n1τ i.i.d. random variables to
obtain variance decreasing linearly in n1τ . The differences are that in the highest-order monomial of
_g(k)[2], there are there are six inner-loop samples, four outer-loop samples, and two noise samples,_
for a maximum expectation of 15L[3] _× 3L[2]_ _× σ[2]. There are T_ [2]τ [2]n[2]1[d][4][n][4]2 [of these terms, but as]
before, only T [2]τd[4]n[2]1[n][4]2 [of these terms have nonzero expectation due to independence, and the][ n]1[2][n][4]2
coefficient cancels. Thus, the variance is upper bounded as

_α[4]d[2][ P][T]i=1_ _τj=1_ _[L][3][σ][2]_
Var(g(k)) _c_ = c _[α][4][d][2][L][5][σ][2]_ (113)
_≤_ _T_ [2]τ [2] _τ_
P

for some absolute constant c, which implies, via Theorem 1.9 in Schudy & Sviridenko (2012), that

1/6[!]

_τb[2]_

P ( _g(k)_ _b)_ _e[2]_ exp (114)
_|_ _| ≥_ _≤_ _−_ _cα[4]d[2]L[5]σ[2]_

 

thus we have via a union bound over k ∈ [d],

1/6[!]

P _g_ _√db_ 1 _e[2]d exp_ _τb[2]_ (115)
_∥_ _∥≤_ _≥_ _−_ _−_ _cα[4]d[2]L[5]σ[2]_
   


-----

for any b > 0. Combining this with (111) via a union bound, we have that


_−1_



**Pi,j(X[out]i,j** [)][⊤][X]i,j[out][P][i,j]
_j=1_

X


_Tn1τ_

1

_Tn1τ_


_i=1_

_T_

_i=1_

X


_τ_

**Pi,j(X[out]i,j** [)][⊤][z]i,j[out] **Pi,j(X[out]i,j** [)][⊤][X]i,j[out][(][X]i,j[in] [)][⊤][z][in]i,j
_j=1_ _[−]_ _n[α]2_

X


_√db_

(116)

_≤_ _βˆ −_ _γ_

with probability at least

1/6[!] 1/6[!]

_τb[2]_ _τγ[2]_

1 _e[2]d exp_ _e[2]Td[2]_ exp (117)
_≥_ _−_ _−_ _cα[4]d[2]L[5]σ[2]_ _−_ _−_ _cα[4]d[2]L[6]_

   

This completes the bound on θ. Next, we must bound **wMAML[(][T][ )][∗]** _MAML[∥][. We have]_
_∥_ _[−]_ **[w][∗]**

**wMAML[(][T][ )][∗]** _MAML[∥]_
_∥_ 1 _T[−]_ **[w][∗]** _−1 1_ _T_

= **Qi** **Qiwi[∗]** _i_ []]

 _T_ _i=1_  _T_ _i=1_ _[−]_ [E][i][[][Q][i][]][−][1][E][i][[][Q][i][w][∗]

X X

1 _T_ _−1 1_ _T_ 1 _T_ _−1_

= **Qi** **Qiwi[∗]** **Qi** Ei[Qiwi[∗][]]

 _T_ _i=1_  _T_ _i=1_ _[−]_  _T_ _i=1_ 

X X X

1 _T_ _−1_
+ **Qi** Ei[Qiwi[∗][]][ −] [E][i][[][Q][i][]][−][1][E][i][[][Q][i][w]i[∗][]]

_T_

 _i=1_ 

X

1 _T_ _−1 1_ _T_ _T_

**Qi** **Qiwi[∗]** **Qi)[−][1]Ei[Qiwi[∗][]]**

_≤_  _T_ _i=1_  _T_ _i=1_ _[−]_ [(] _i=1_

X X X

1 _T_ _−1_

+ **Qi** Ei[Qiwi[∗][]][ −] [E][i][[][Q][i][]][−][1][E][i][[][Q][i][w]i[∗][]] (118)

_T_

 _i=1_ 

X

1 _T_ _−1_ 1 _T_ 1 _T_ _−1_

**Qi** **Qiwi[∗]** _i_ []] [+] **Qi** Ei[Qi][−][1] _i_ []][∥]

_≤_  _T_ _i=1_  _T_ _i=1_ _[−]_ [E][i][[][Q][i][w][∗]  _T_ _i=1_  _−_

X X X

(119)

_[∥][E][i][[][Q][i][w][∗]_

1 _T_ 1 _T_ _−1_

= β[1]ˆ _T_ _i=1_ **Qiwi[∗]** _[−]_ [E][i][[][Q][i][w]i[∗][]] [+]  _T_ _i=1_ **Qi** _−_ Ei[Qi][−][1] (120)

X X

where in equations (118) and (119) we have used the triangle and Cauchy-Schwarz inequalities,[LB]
respectively, and in (120) we have used the dual Weyl’s inequality and Assumption 2. We first
consider the second term in (120). Continuing to argue as in the proof of Theorem 2, we have


_∥wMAML[(][T][ )][∗]_ _[−]_ **[w]MAML[∗]** _[∥≤]_ _β[1]ˆ_


_T_ _LBˆ_

_i=1_ **Qiwi[∗]** _[−]_ [E][i][[][Q][i][w]i[∗][]] [+] _βˆ[2]_

X


_i=1_ **Qi −** Ei[Qi] (121)

X


As in the proof of Theorem 2, will use the matrix Bernstein inequality (Theorem 6.1.1 in Tropp (2015))
to upper boundDoing so yields ∥ _T[1]_ _Ti=1_ **[Q][i][w]i[∗]** _[−]_ [E][i][[][Q][i][w]i[∗][]][∥] [and][ ∥] _T[1]_ _Ti=1_ **[Q][i][ −]** [E][i][[][Q][i][]][∥] [with high probability.]

P P

_δ Var(Qiwi[∗][)]_ _LBˆ_ _δ Var(Qi)_

_∥wMAML[(][T][ )][∗]_ _[−]_ **[w]MAML[∗]** _[∥≤]_ _β[1]ˆ_ _√T_ + _βˆ[2]_ _√T_ (122)

with probability at least 1 2d exp 1+Lδ/[ˆ] (3√T ) 2d exp 1+LBδ/[ˆ] (3√T ) for any δ > 0.
_−_ _−_ _[δ][2][ Var][(][Q][i][w][i,][∗][)][/][2]_ _−_ _−_ _[δ][2][ Var][(][Q][i][)][/][2]_
   


-----

Combining (122) with our bound on θ (from (87), (109), and (116)) via a union bound and rescaling
_γ = γnew = γold√τ yields_


_LBγ_
**wERM** **wERM[∗]** _[∥≤]_ [2ˆ]
_∥_ _−_ _µˆ[√]τ_

with probability at least

1 − _e[2]Td[2]_ exp


_δ Var(Qiwi[∗][)]_


_LBˆ_ _δ Var(Qi)_

_βˆ[2]_ _√T_

(123)


_√db_

_√τ_


1

_µˆ −_ _γ/[√]τ_ [+ 1]βˆ

_−_ _e[2]d exp_ _−_


_µˆ_ _γ/[√]τ_ [+]
_−_


1/6[!]

_cγ[2]_
_−_ _α[4]d[2]L[6]_
 

_−_ _[δ][2]1 + [ Var]Lδ/[ˆ][(][Q][i](3[w]√[i,][∗]T[)][/])[2]_ !


1/6[!]

_c[′]b[2]_

_α[4]d[2]L[5]σ[2]_



_δ[2]_ Var(Qi)/2

!


_cγ[2]_

_α[4]d[2]L[6]_


(124)


_−_ 2d exp


_−_ 2d exp


1 + LBδ/[ˆ] (3


_T_ )


for some absolute constants c and c[′], and any b and δ > 0, and γ ∈ (0, _[√]τ ˆµ)._


Finally, choose γ = 4d _α[4]cL[6]_ log[3](100Td), b = 2d _α[4]Lc[′][5]σ[2]_ log[3](100d), and

_δ = (_ Var(Qi) + Var(qQiwi[∗][))][−][1][p]log(200d) and restrictq _τ > 32d[2]α[4]L[6]_ log[6](100Td)/(cβ[ˆ]) such

that _βˆpγ/1_ _[√]τ_ _β_ [and][ T >][ ˆ]L[2]B[2] log(200d)/(9(Var(Qi) + Var(Qiwi[∗][)))][. This ensures that each]

negative term in the high probability bound (124) is at most 0.01 and thereby completes the proof.− _[≤]_ [2]


-----

