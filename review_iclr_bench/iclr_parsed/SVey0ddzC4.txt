# CONNECTING GRAPH CONVOLUTION & GRAPH PCA

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Graph convolution operator of the GCN model is originally motivated from a
localized first-order approximation of spectral graph convolutions. This work
stands on a different view; establishing a mathematical connection between graph
_convolution and graph-regularized PCA (GPCA). Based on this connection, the_
GCN architecture, shaped by stacking graph convolution layers, shares a close relationship with stacking GPCA. We empirically demonstrate that the unsupervised
embeddings by GPCA paired with a 1- or 2-layer MLP achieves similar or even
better performance than many sophisticated baselines on semi-supervised node
classification tasks across five datasets including Open Graph Benchmark. This
suggests that the prowess of graph convolution is driven by graph based regularization. In addition, we extend GPCA to the (semi-)supervised setting and show
that it is equivalent to GPCA on a graph extended with “ghost” edges between
nodes of the same label. Finally, we capitalize on the discovered relationship to
design an effective initialization strategy based on stacking GPCA, enabling GCN
to converge faster and achieve robust performance at large number of layers.

1 INTRODUCTION

Graph neural networks (GNNs) are neural networks designed for the graph domain. Since the
breakthrough of GCN (Kipf & Welling, 2017), which notably improved performance on the semisupervised node classification problem, many GNN variants have been proposed; including GAT
(Velickoviˇ c et al., 2018), GraphSAGE (Hamilton et al., 2017), DGI (Veli´ ckoviˇ c et al., 2019), GIN´
(Xu et al., 2019), PPNP and APPNP (Klicpera et al., 2019), to name a few.

Despite the empirical successes of GNNs in both node-level and graph-level tasks, they remain not
well understood due to limited systematic and theoretical analysis of GNNs. For example, researchers
have found that GNNs, unlike their non-graph counterparts, suffer from performance degradation
with increasing depth, their expressive power decaying exponentially in number of layers (Oono &
Suzuki, 2020). Such behavior is only partially explained by the oversmoothing phenomenon (Li
et al., 2018; Zhao & Akoglu, 2020). Another surprising observation shows that a Simplified Graph
Convolution model, named SGC (Wu et al., 2019), can achieve similar performance to various more
complex GNNs on a variety of node classification tasks. Moreover, a simple baseline that does not
utilize the graph structure altogether performs similar to state-of-the-art GNNs on graph classification
tasks (Errica et al., 2020). These observations call attention to studies for a better understanding of
GNNs (NT & Maehara, 2019; Morris et al., 2019; Xu et al., 2019; Oono & Suzuki, 2020; Loukas,
2020; Srinivasan & Ribeiro, 2020). (See Sec. 2 for more on understanding GNNs.)

Toward a systematic analysis and better understanding of GNNs, we establish a connection between
the graph convolution operator of GCN (and PPNP) and Graph-regularized PCA (GPCA) (Zhang &
Zhao, 2012), and show the similarity between GCN and stacking GPCA. This connection provides
a deeper understanding of GCN’s power and limitation. Empirically, we also find that GPCA
performance matches that of many GNN baselines on benchmark semi-supervised node classification
tasks. We argue that the simple GPCA should be a strong baseline in future. What is more, the
unsupervised stacking GPCA can be viewed as “unsupervised GCN” and provides a straightforward,
yet systematic way to initialize GCN training. We summarize our contributions as follows:

_• Connection between Graph Convolution and GPCA: We establish the connection between the_
graph convolution operator of GCN (also PPNP) and the closed-form solution of graph-regularized
PCA (GPCA) formulation. We demonstrate that a simple graph-regularized PCA paired with 1or 2-layer MLP can achieve similar or even better results than state-of-the-art GNN baselines over


-----

several benchmark datasets. We further extend GPCA to (semi-)supervised setting which can generate
embeddings using information of labels, which yields better performance on 3 out of 5 datasets. The
outstanding performance of simple GPCA supports that the prowess of GCN on node classification
task comes from graph based regularization. This motivates the study and design of other graph
regularization techniques in the future.

_• GPCANET: New Stacking GPCA model: Capitalizing on the connection between GPCA and_
graph convolution, we design a new GNN model called GPCANET shaped by (1) stacking multiple
GPCA layers and nonlinear transformations, and (2) fine-tuning end-to-end via supervised training.
GPCANET is a generalized GCN model with adjustable hyperparameters that control the strength of
graph regularization of each layer. We show that with stronger regularization, we can train GPCANET
with fewer (1–3) layers and achieve comparable performance to much deeper GCNs.

_• First initialization strategy for GNNs: Capitalizing on the connection between GCN and GP-_
CANET, we design a new strategy to initialize GCN training based on stacking GPCA, outperforming
the popular Xaiver initialization (Glorot & Bengio, 2010). We show that the GPCANET-initialization
is extremely effective for training deeper GCNs, that significantly improves the convergence speed,
performance, and robustness. Notably, GPCANET-initialization is general-purpose and also applies
to other GNNs. To our knowledge, it is the first initialization method specifically designed for GNNs.

[We open-source code at http://bit.ly/GPCANet. All datasets are public-domain.](http://bit.ly/GPCANet)

2 RELATED WORK

**Understanding GNNs. Our work concerns learning on a single graph, hence we limit discussion**
of related work to node-level GNNs. GCN’s graph convolution is originally motivated from the
approximation of graph filters in graph signal processing (Kipf & Welling, 2017). NT & Maehara
(2019) show that graph convolution only performs low-pass filtering on original feature vectors, and
also state a connection between graph filtering and Laplacian regularized least squares. Motivated by
the oversmoothing phenomenon of graph convolution, Oono & Suzuki (2020) theoretically prove that
GCN can only preserve information of node degrees and connected components when the number
of layers goes to infinity, under some conditions of GCN weights. Recently several papers revisited
the connection of graph convolution to graph-regularized optimization problem (Li et al., 2019; Ma
et al., 2020; Pan et al., 2021; Zhao & Akoglu, 2020; Zhu et al., 2021), which is originally discussed
in graph signal processing (Shuman et al., 2013). More specifically, both Ma et al. (2020) and
Zhu et al. (2021) relate graph-regularization optimization to several GNNs such as GCN (Kipf &
Welling, 2017), APPNP (Klicpera et al., 2019), and GAT (Velickoviˇ c et al., 2018). However, all´
previous work study these connections while ignoring the learnable parameters, which are essential
for high-performance deep learning. Our work differs from these by establishing a stronger and closer
connection to graph-regularized PCA that also takes learnable parameters into account.

**Graph-regularized PCA. PCA and its variants are standard linear dimensionality reduction ap-**
proaches. Several work extend PCA to graph-structured data, such as Graph-Laplacian PCA (Jiang
et al., 2013) and Manifold-regularized Matrix Factorization (Zhang & Zhao, 2012). For other variants,
see Shahid et al. (2016).

**Stacking Models and Deep Learning. The connection between CNN and stacking PCA has been**
explored in PCANet (Chan et al., 2015), which demonstrated that the (unsupervised) simple stacking
PCA works as well as supervised CNN over a large variety of vision tasks. The original PCANet is
shallow and does not have nonlinear transformations, while PCANet+ (Low et al., 2017) overcomes
these limitations and pushes the architecture much deeper. The idea of layerwise stacking for feature
extraction is not new and was empirically observed to exhibit better representation ability in terms of
classification. For a comprehensive review, we refer to Bengio et al. (2013).

**Initialization. Traditionally, neural networks (NNs) were initialized with random weights generated**
from Gaussian distribution with zero mean and a small standard deviation (Krizhevsky et al., 2012).
As training deeper NNs became extremely difficult due to vanishing gradient and activation functions,
Glorot & Bengio (2010) provided a specific weight initialization formula, named Xavier initialization,
based on variance analysis without considering activation function. Xavier initialization is widely used
for any type of NN even today, and it is the main initialization strategy used for GNNs. Later, He et al.
(2015) adapted Xavier initialization to ReLU activation by considering a multiplier. Taking another


-----

direction, Saxe et al. (2013) analyzed the dynamics of training deep NNs and proposed random
orthonormal initialization. Mishkin & Matas (2015) further improved orthonormal initialization for
batch normalization (Ioffe & Szegedy, 2015). Different from these data-independent approaches,
others (Krahenb¨ uhl et al., 2016; Seuret et al., 2017; Wagner et al., 2013) have employed data-¨
dependent techniques, like PCA, to initialize deep NNs. Although initialization has been widely
studied for general NNs, no specific initialization has been proposed for GNNs. In this work, we
propose a data-driven initialization technique (based on GPCA), specific to GNNs for the first time.

3 GRAPH CONVOLUTION AND GPCA

3.1 GRAPH CONVOLUTION

Consider a node-attributed input graph G = (V, E, X) with |V | = n nodes and |E| = m edges,
where X ∈ R[n][×][d] denotes the node feature matrix with d features. Broadly, graph convolution
operation convolves the features (or representations) over the graph structure.

**GCN. Similar to other neural networks stacked with repeated layers, GCN contains multiple graph**
convolution layers each of which is followed by a nonlinear activation. Let H [(][l][)] be the l-th hidden
layer representation, then, each GCN layer performs
_H_ [(][l][+1)] = σ( A[˜]symH [(][l][)]W [(][l][)]) (1)

where _A[˜]sym = D[˜]_ _[−]_ 2[1] (A + _I) D[˜]_ _[−]_ [1]2 denotes the n _×_ _n symmetrically normalized adjacency matrix with_

self-loops, _D[˜] is the diagonal degree matrix where_ _D[˜]_ _ii = 1 +_ _j=1_ _[A][ij][,][ W][ (][l][)][ depicts the][ l][-th layer]_
parameters (to be learned), and σ is the nonlinear activation function. Formally, graph convolution is
parameterized with W and maps an input X to a new representation Z as

[P][n]
_Z = A[˜]symXW ._ (2)

**PPNP. For PPNP (Klicpera et al., 2019), the features are first transformed by an MLP before**
convolving over the graph. Formally, the operation is revised as

_−1_ _−1_
_Z = µ_ _I −_ (1 − _µ) A[˜]sym_ MLPW (X) = _I + αL[˜]_ MLPW (X) (3)

where we replace µ with _α = (1 −_ _µ)/µ,_ _L[˜] := I −_ _A[˜]sym denotes the normalized graph Laplacian,_ 
and W depicts the learnable MLP parameters. As matrix inverse is expensive, an approximate version
called APPNP that employs the power method (Golub & Van Loan, 1989) is often used in practice.

3.2 GRAPH-REGULARIZED PCA (GPCA)

As stated by Bengio et al. (2013), “Although depth is an important part of the story, many other
_priors are interesting and can be conveniently captured when the problem is cast as one of learning a_
representation.” GPCA is one such representation learning technique with a graph-based prior.

Standard PCA learns k-dimensional projections Z ∈ R[n][×][k] of feature matrix X ∈ R[n][×][d], aiming to
minimize the reconstruction error
_∥X −_ _ZW_ _[T]_ _∥F[2]_ _[,]_ (4)
subject to W ∈ R[d][×][k] being an orthonormal basis. GPCA extends this formalism to graph-structured
data by additionally assuming either smoothing bases (Jiang et al., 2013) or smoothing projections
(Zhang & Zhao, 2012) over the graph. In this work we consider the latter case where low-dimensional
projections are smooth over the input graph G, where _L[˜] = I −_ _A[˜]sym denotes its normalized Laplacian_
matrix. The objective formulation of GPCA is then given as
min _X_ _ZW_ _[T]_ _F_ [+][ α][ Tr(][Z] _[T][ ˜]LZ)_ s.t. _W_ _[T]_ _W = I_ (5)
_Z,W_ _∥_ _−_ _∥[2]_

where α is a hyperparameter that balances reconstruction error and the variation of the projections
over the graph. Note that the first part of Eq. equation 5, along with the constraint, corresponds
to the objective of the original PCA, while the second part is a graph regularization term that aims
to “smooth” the learned representations Z over the graph structure. As such, GPCA becomes the
standard PCA when α = 0.

Similar to PCA, the problem (5) is non-convex but has a closed-form solution (Zhang & Zhao, 2012).
Surprisingly, as we show, it has a close connection with the graph convolution formulation in Eq.
equation 2. In the following, we give the GPCA solution and then detail its connection to graph
convolution.


-----

**Theorem 3.1. GPCA with formulation shown in (5) has the optimal solution (Z** _[∗], W_ _[∗]) following_
_Z_ _[∗]_ = (I + αL[˜])[−][1]XW _, and_ _W_ = (w1, w2, ..., wk) (6)

_[∗]_ _[∗]_

_where w1, ..., wk are the eigenvectors of X_ _[T]_ (I +αL[˜])[−][1]X corresponding to the largest k eigenvalues.

_Proof. The proof can be found in Appendix. A.1._

3.3 CONNECTION BETWEEN GCN AND GPCA

Let Φα := I + αL[˜]. The normalized Laplacian matrix _L[˜] has absolute eigenvalues bounded by 1, thus,_
all its positive powers have bounded operator norm. When α 1, Φ[−]α [1] can be decomposed into
_≤_
Taylor series as (I + αL[˜])[−][1] = I − _αL[˜] + . . . + (−α)[t][ ˜]L[t]_ + . . .. The first-order truncated form (i.e.
approximation) of the series is
(I + αL[˜])[−][1] _≈_ _I −_ _αL[˜] = (1 −_ _α)I + αA[˜]sym ._ (7)
When α = 1, the first-order approximation of Z _[∗]_ in Theorem 3.1 follows
_Z_ _[∗]_ _AsymXW_ _._ (8)
_≈_ [˜] _[∗]_
The (approximate) solution to GPCA in Eq. equation 8 matches the form of graph convolution
operation in Eq. equation 2, with W _[∗]_ plugged in as the eigenvectors of the matrix X _[T]_ Φ[−]α [1][X][.][ In]
other words, there exists some parameter W _[∗]_ with which GCN becomes the first-order approximation
of GPCA.

To reiterate, a key contribution of this work is to show that the graph convolution operation in GCN
can be viewed as the first-order approximation of GPCA with α = 1 with a learnable W . Put
differently, the first-order approximation of (unsupervised) GPCA with α = 1 can be viewed as a
graph convolution with a fixed, data-driven W . In other words, Notably, for α < 1, Eq. equation 7
shows the connection between GPCA and graph convolution equipped with 1-step (scaled) residual
connection.

3.4 CONNECTION BETWEEN PPNP AND GPCA

Replacing the MLP in Eq. equation 3 with a single linear layer without activation results in Z =
_−1_
_I + αL[˜]_ _XW_, which has exactly the same formulation as the solution Z _[∗]_ in Theorem 3.1 Eq.
equation 6. The connection states that the graph convolution in PPNP can be viewed as the GPCA 
solution with a learnable W . Interestingly, the empirical performance improvement of PPNP over
GCN (see Table 2 in Klicpera et al. (2019)) may be explained through these connections that they
have to GPCA; where PPNP relates to the exact solution of GPCA while GCN is related to its
(first-order) approximation.

3.5 SUPERVISED GPCA

The standard GPCA problem in (5) is unsupervised. Motiviated from LDA (Balakrishnama &
Ganapathiraju, 1998) and PLS (Geladi & Kowalski, 1986), in this section we show how to extend it
to the supervised setting, by learning embeddings that not only (1) provide good reconstruction and
(2) vary smoothly over the graph structure, but also (3) highly correlate with the response variable(s).
For simplicity of presentation, let z ∈ R[d] be a 1-d embedding and Y denote the response matrix (in
the general case of multiple responses). We write the additional, i.e. (3)rd objective above, as[1]

max corr(Y, z) _T_ corr(Y, z) var(z) max **z[T]** _Y Y_ _[T]_ **z** (9)
**z** _≡_ **z**

The form of equation 9 (See Appendix. A.3) and the variance-maximizing term    var(z) are for
mathematical convenience. Despite agnostic to labels, including var(z) is intuitive since an implicit
objective of data projection (embedding) is to ensure that inherent variation in data is captured as much
as possible. In general, we would aim to maximize the trace of Z _[T]_ _Y Y_ _[T]_ _Z for multi-dimensional_
embeddings.

**Interpretation. For semi-supervised node classification with c classes, let L ⊂** _V denote the set_
of labeled nodes. For this task, Y ∈{0, 1}[n][×][c] would encode the node labels where the v-th row
of Y, denoted Yv, depicts the one-hot encoded label for each v **_L. For u_** _V_ **_L with unknown_**
_∈_ _∈_ _\_
labels, Yu = 0, set as the c-dimensional all-zero vector. Then, (Y Y _[T]_ )ij is simply equal to 1 when

1For the optimization to be well-posed, constraints on z are required, omitted for simplicity of presentation.


-----

nodes i and j share the same label, and otherwise 0 (either because they have different labels or
labels are unknown). This term simply enforces the representations Zi and Zj of two same-labeled
nodes to be similar. In a sense, Y Y _[T]_ adds “ghost” edges between the same-label nodes, further
guiding the smoothness of their representations over this extended graph structure. We remark that
earlier work (Gallagher et al., 2008) has heuristically introduced edges between same-label nodes to
enhance a given graph for the node classification task. In this work, we have derived the theoretical
underpinning for this strategy.

**Supervised formulation. We have shown that requiring the embeddings to correlate with the known**
labels can be interpreted as additional smoothing over “ghost” edges between the same-label nodes in
the graph. As such, we extend the GPCA problem in (5) to the (semi-)supervised setting as
min _X_ _ZW_ _[T]_ _F_ [+][ α][ Tr(][Z] _[T][ ˜]LsprZ)_ s.t. _W_ _[T]_ _W = I_ ; (10)
_Z,W_ _∥_ _−_ _∥[2]_

where _L[˜]spr = I −_ _A[˜]spr,_ _A˜spr = (1 −_ _β) A[˜]sym + βD[−]_ 2[1] (Y Y _[T]_ )D[−] 2[1] (11)

In Eq. equation 11, β is an additional hyperparameter for trading-off the graph-based regularization
(i.e. smoothing) due to the actual input graph edges versus the ones introduced through Y Y _[T]_ between
the nodes of the same label, and D is the diagonal matrix with Dii = _j=1[(][Y Y][ T][ )][ij][.]_

**Theorem 3.2. Supervised GPCA, as shown in (10) has the optimal solution (Z** _[∗], W_ _[∗]) following_

[P][n]

_Z_ _[∗]_ = (I + αL[˜]spr)[−][1]XW _, and_ _W_ = (w1, w2, ..., wk) (12)

_[∗]_ _[∗]_

_where w1, . . ., wk are the top eigenvectors of the matrix X_ _[T]_ (I + αL[˜]spr)[−][1]X, equivalently X _[T][  ](1 +_

_α)I −_ _α(1 −_ _β) A[˜]sym + αβD[−]_ 2[1] Y Y _[T]_ _D[−]_ [1]2 _X, corresponding to the largest k eigenvalues._
 [][−][1]

_Proof. The proof is similar to that of Theorem 3.1._

3.6 APPROXIMATION AND COMPLEXITY ANALYSIS

According to formulations in Theorems 3.1 and 3.2, obtaining Z _[∗]∈R[n][×][k]_ and W _[∗]∈R[d][×][k]_ requires
two demanding computations (1) the inverse of Φα = (I + αL) ∈ R[n][×][n], or in the supervised case
Φα = (I +αL[˜]spr); and (2) top k eigenvectors of the matrix X _[T]_ Φ[−]α [1][X][ ∈] [R][d][×][d][. Eigen-decomposition]
takes O(d[3]) (Pan & Chen, 1999), which is scalable as d is usually small. Computing matrix inverse,
on the other hand, can take O(n[3]) and require O(n[2]) memory, which would be infeasible for very
large graphs.

To reduce computation and memory complexity, we instead approximately compute F := ϕ[−]α [1][X][,]
which is a common term for both Z _[∗]_ and W _[∗]. We can equivalently write_

_α_ 1
(I + αL)F = X = _F + αF = αPF + X =_ _F =_
_⇒_ _⇒_ 1 + α _[PF][ +]_ 1 + α _[X]_

for P = A[˜]sym in the unsupervised case and P = (1 _−_ _β) A[˜]sym +_ _βD[−]_ 2[1] (Y Y _[T]_ )D[−] [1]2 when supervised.

Then, we can iteratively (with total T iterations) use the power method (Golub & Van Loan, 1989) to
compute F as

_α_ 1
_F_ [(][t][+1)] (13)
_←_ 1 + α _[PF][ (][t][)][ +]_ 1 + α _[X]_

where t ∈{0, ..., T _} depicts the iteration and F_ [(0)] _∈_ R[n][×][d] is initialized as X (or randomly). For the
supervised case, PF [(][t][)] is computed through a series of (from right to left) matrix-matrix products.
This avoids the explicit construction of matrix Y Y _[T]_ in memory. Overall, solving for F takes
_O(T_ (m + n)d) where m is the number of edges in the graph. The supervised case has an additional
term O(Td|L|c) with c being the number of classes and |L| ≤ _n be the number of labeled nodes,_
which can also be upper-bounded by O(T (m + n)d) when treating c as constant.

Having solved for F, we perform the matrix-matrix product Z _[∗]_ = FW _[∗]_ in O(ndk) and then the
eigen-decomposition of X _[T]_ _F in O(d[3]_ + nd[2]) = O(nd[2]) (for n ≥ _d). Assuming O(d) = O(k),_
overall complexity for computing the 1-layer GPCA is given as O(Tmd + Tnd + nd[2]), which is
_linear in the number of nodes and edges. Note that empirically we found 5 ≤_ _T ≤_ 10 to be sufficient.


-----

**Algorithm 1 GPCANET Forward Pass and Pre-training**

1: Input: graph G = (V, E, X), GPCA hyper-parameter(s) α (and β if supervised, β = 0 otherwise), #layers
_L, hidden layer sizes {d1, . . ., dL}, activation function σ(·), #approximation steps T_

2: Output: pre-set layer-wise parameters {W [(1)], . . ., W [(][L][)]}
3: Initialize H [(0)] := X
4: for l = 1 to L do
5: Center H [(][l][−][1)] by subtracting mean of row vectors

6: _F ←−_ _H_ [(][l][−][1)]

7: **for t = 1 to T do**

9:8: _FPF ←− ←−1+α(1α −[PF]β[ +]) A[˜]sym1+1αF[H] +[(] βD[l][−][1)]_ _[−]_ 2[1] (Y Y _[T]_ )D[−] 2[1] F

10: **end for**

11: _W_ [(][l][)] _←−_ top dl eigenvectors of H [(][l][−][1)][T] _F_

12: _H_ [(][l][)] _←−_ _σ(FW_ [(][l][)])

13: end for

4 GPCANET: A STACKING GPCA MODEL

4.1 GPCANET

Thus far, we drew a connection between the geometrically motivated, manifold-based GPCA and the
graph convolution operation of deep NN based GCN. Next we leverage this connection to design a
new model called GPCANET that takes advantage of the relative strengths of each paradigm; namely,
GPCA’s ability to capture data variation and structure, and GCN’s ability to capture multiple levels of
abstraction (i.e. high-level concepts) through stacked layers and non-linearity.

In a nustshell, GPCANET is a stacking of multiple (unsupervised or supervised) GPCA layers and
nonlinear transformations, which shares the same architecture as a multi-layer GCN. It consists of
two main stages: (1) Pre-training, which initializes the layer-wise parameters through closed-form
GPCA solutions, and (2) End-to-end-training, which refines these parameters through end-to-end
gradient-based minimization of a global supervised loss criterion at the output layer.

We remark that GPCANET is not the same as GCN, as each layer uses the formulation in Thm.s
3.1 and 3.2 (with approximation shown in Sec. 3.6). In fact, when α = 1 and β = 0, GPCANET
is the GCN model initialized with GPCANET-initialization, which we discuss more in Sec. 4.2. In
other words, GPCANET is a generalized GCN model with additional hyperparameters, α and β,
controlling the strength of graph regularization based on the existing or “ghost” edges, respectively.

**Forward Pass and Pre-training stage.** During pre-training, weights of the l-th layer, denoted
as W [(][l][)] _∈_ R[d][l][−][1][×][d][l], are pre-set (i.e. initialized) as the leading dl eigenvectors of the matrix
_H_ [(][l][−][1)][T] Φ[−]α [1][H] [(][l][−][1)][,][2][ where][ H] [(][l][−][1)][ is the representation as output by the][ (][l][ −] [1)][-th layer (with]
_H_ [(0)] := X), and Φα can be the unsupervised (I +αL) or the supervised (I +αL[˜]spr). The pre-training
stage takes a single forward pass. Algo. 1 shows both forward pass during end-to-end-training and
the pre-training procedure, where line 11 in blue is a step used only for pre-training.

_Additional treatment for ReLU: Nonlinear transformations like ReLU improves model capacity,_
however at pre-training stage, it causes information loss as all negative values are truncated to 0. This
hinders the advantage of using the leading dl eigenvectors to initialize the weights so as to convey
maximum variance (i.e. information) to the next layers. To address this issue, we instead use the
leading dl/2 eigenvectors {wi}[d]i=1[l][/][2] [and their negatives][ {−][w][i][}]i[d]=1[l][/][2] [to initialize][ W][ (][l][)][. Empirically]
we observe this always improves performance when using ReLU activation.

**End-to-end training stage. Pre-training can be seen as an information-preserving initialization, as**
compared to an uninformative random initialization, after which we refine the layer-wise parameters
via gradient-based optimization w.r.t. a supervised loss criterion at the output layer. Specifically for
semi-supervised node classification, we perform an end-to-end training w.r.t. the cross-entropy loss
on the labeled nodes. All parameters are updated jointly through backpropagation during this stage,
with forward computation shown in Algo.1 (excluding line 11).

2If d(l) is greater than the number of eigenvectors, all eigenvectors are used, with additional vectors generated
from random projection of eigenvectors.


-----

4.2 GPCANET-INITIALIZATION FOR GCN

When we set α = 1, β = 0, and approximate the matrix inverse (I + αL)[−][1] via first-order truncated
Taylor expansion as shown in Eq. equation 7, GPCANET has the same architecture with GCN. As
such, we can use the pre-training stage of GPCANET to initialize GCN with only minor modification.
Specifically, we replace lines 6 through 10 in Algo. 1 with the following single line:
_F ←−_ _A[˜]symH_ [(][l][−][1)] (14)
The modified initialization is for GCN and is driven by the mathematical connection between
GPCANET and GCN that we established. We expect that adapting it for other GNNs is also possible
although we do not pursue this direction here.

5 EXPERIMENTS

In this section we design extensive experiments to answer the following questions. (Q1) How does
the simple, unsupervised and shallow GPCA compare to its multi-layer extension GPCANET, as well
as to existing GNNs? (Q2) How does our extended, semi-supervised GPCA compare to the original,
unsupervised GPCA? (Q3) Does GPCANET-initialization improve GCN accuracy and robustness?

5.1 EXPERIMENTAL SETUP

**Datasets. We focus on semi-supervised node classification (SSNC) and use 5 benchmark datasets:**
First three, CORA, CITESEER, PUBMED (Sen et al., 2008), are relatively small (2K to 10K nodes)
but widely-used citation graphs. For these we use the data splits in Kipf & Welling (2017). The
others, ARXIV and PRODUCTS, are newest and much larger (100K to 2000K) node classification
benchmarks from Open Graph Benchmark (Hu et al., 2020), for which we use the official data splits.
Data statistics can be found in Appendix. A.4.
**Baselines. We compare (unsupervised & semi-supervised) GPCA and GPCANET to state-of-the-art**
(SOTA) GNNs, including GCN (Kipf & Welling, 2017), APPNP (Klicpera et al., 2019), GAT
(Veliˇckovi´c et al., 2018), and GraphSAGE (G-SAGE) (Hamilton et al., 2017).
**Model configuration and training. For each dataset, we define a separate pool of values for the**
hyperparameters (HPs): learning rate, weight decay, number of layers, hidden size, dropout rate, and
regularization trade-off terms α, β. For fair comparison, all models share the same HP pools during
training. See Appendix. A.5 for HP configurations and other details.

5.2 Q1: PERFORMANCE OF (UNSUPERVISED) GPCA AND GPCANET

**GPCA. Having proved the mathematical connection between GPCA, GCN, and PPNP, we expect**
unsupervised GPCA (β = 0) to generate comparable representations. We perform GPCA with
different α ∈{1, 5, 10, 20, 50} (Appendix. Table 5) to obtain node representations and pass those to
a 1- or 2-layer MLP. We compare to GCN, APPNP, as well as other GNNs; GAT and G-SAGE.

The performance results are given in Table 1. Due to the scale of the largest two datasets, ARXIV and
PRODUCTS, we list the reported performance at OGB-leaderboard[3] (depicted by _[∗]) for G-SAGE on_
both datasets, and that of (Cluster-)GAT on PRODUCTS.

We find that the simple 1-layer GPCA paired with MLP performs consistently better than the multilayer GCN model across all 5 datasets. GPCA’s performance is also comparable to or better than
other SOTA GNNs. This is quite notable, given that GPCA is not only shallow but also unsupervised,
whereas all other baselines are trained end-to-end, and with the exception of APPNP, they exhibit
a multi-layer architecture. By carefully looking at the performance of GPCA with varying α (see
Appendix. A.6), we find that different datasets have different best selected α[∗] (in Table 1 top to
bottom: α[∗] = {50, 5, 10, 20, 20}) but in general a relatively larger α (compared to graph convolution
of GCN that is equivalent to α = 1) is preferable for all datasets. Larger α implies stronger graphregularization on the representations. The outstanding performance of the simple GPCA empirically
confirms that the power of GNNs on the SSNC problem is mainly driven by graph regularization.

GPCANET. Compared to the 1-layer GPCA, GPCANET has a deeper architecture along with
nonlinear activation function. Moreover, it employs hyperparameter α at every layer to control the
degree of graph regularization. As each graph convolution has fixed level of graph regularization,
one may hypothesize that increasing the number of layers (L) corresponds to increasing the degree

[3https://ogb.stanford.edu/docs/leader_nodeprop/](https://ogb.stanford.edu/docs/leader_nodeprop/)


-----

Table 1: Comparison btwn. unsupervised GPCA (β = 0), GPCANET, and existing (supervised)
SOTA GNNs on 5 datasets, w.r.t. mean test accuracy and standard deviation (in parentheses) over 5
different seeds. Those marked with _[∗]_ are reported values at the OGB-leaderboard[3]. Highest mean
performance is in bold and the second highest is underlined.

|Col1|GPCA|GPCANET|GCN APPNP GAT G-SAGE|
|---|---|---|---|


|CORA CITESEER PUBMED ARXIV PRODUCTS|81.10 (0.00) 71.80 (0.75) 78.78 (0.36) 71.86 (0.18) 79.23 (0.14)|80.64 (0.33) 71.36 (0.21) 78.52 (0.17) 72.20 (0.15) 80.05 (0.29)|80.62 (0.90) 81.35 (0.18) 79.27 (0.50) 81.48 (0.83) 71.25 (0.05) 70.33 (0.75) 69.65 (0.59) 71.20 (0.92) 78.42 (0.25) 78.95 (0.36) 78.23 (0.54) 77.78 (0.29) 70.64 (0.17) 70.55 (0.27) 71.11 (0.11) 71.49∗(0.27) 77.90 (0.33) 77.96 (0.34) 79.23∗(0.78) 78.29∗(0.16)|
|---|---|---|---|



of graph regularization. We empirically test this hypothesis using GPCANET, by varying both L (2
to 10) and α (0.1 to 10) to show their connection (hidden size is fixed as 128). The result is shown
in Figure 1. The diagonal pattern (in dark blue) empirically suggests that increasing the number of
layers has the same effect as increasing graph regularization via α.

The corresponding interaction between α and number of layers
suggests that we can train a GPCANET with fewer number of
layers yet achieve similar regularization by increasing α. Such
a shallow model that in fact behaves like a deep one has the
advantage of less memory requirement and faster training due
to fewer parameters.


To this end, we train 1–3-layer GPCANET with varying α
(higher α’s for fewer layers, see Appendix. A.7), and select
the best α and number of layers using validation set. We report
test set performance in Table 1. We do not observe much improvement by GPCANET over other models on smaller datasets
CORA, CITESEER, PUBMED, but notable gains on the larger
ARXIV and PRODUCTS. As such, GPCANET enables shallow model training via tunable hyperparameter α, achieving
comparable or better performance.

5.3 Q2:UNSUPERVISED VS. SEMI-SUPERVISED GPCA


Figure 1: GPCANET performance
(avg. over 5 seeds) with varying
number of layers (L) and α on
CORA. Increasing L has similar
effect as increasing α. Results also
hold for the other datasets.


The representations generated by unsupervised GPCA does not use any label information from
training data. In this work, we have extended GPCA to (semi-)supervised setting with an additional
HP, namely β ∈ [0, 1] that trades-off graph regularization due to the actual input graph edges versus
the “ghost” ones added through Y Y _[T]_ . Overfitting can hurt performance when β is too large or
when there is a distribution shift between the training and test sets. For ARXIV and PRODUCTS, we
empirically observe that β > 0 always degrades performance, possibly because of the distribution
difference between the training and test sets as described in OGB (Hu et al., 2020). Therefore we
only study the effect of β on CORA, CITESEER and PUBMED. The pool for β > 0 is {0.1, 0.2}.

Table 2: Comparison btwn. Supervised (S-)GPCA (β>0) and Results are shown in Table 2,
Unsupervised (U-)GPCA (β=0), w.r.t. mean test accuracy and where (ALL β>0) depicts the sestandard deviation (in parentheses) over 5 different seeds. Also lected configuration for which Sshown (bottom row) is the performance by the best method in GPCA achieves highest validaTable 1. Highest mean performance is highlighted in bold. tion accuracy. The performance

of the best method in Table 1,

CORA CITESEER PUBMED respectively of G-SAGE, (unsu
U-GPCA 81.10 (0.00) 71.80 (0.75) 78.78 (0.36) pervised) GPCA, and APPNP,

is also shown for comparison.

S-GPCA (ALL β>0) 81.17 (0.27) 73.20 (0.71) 79.40 (0.69)

Notably, supervised GPCA pro
S-GPCA β=0.1 81.17 (0.27) 72.07 (0.37) 79.40 (0.69) vides a slight gain over unsuperS-GPCA β=0.2 **81.90 (0.00) 73.20 (0.71) 78.73 (0.59)** vised GPCA across all 3 datasets,

TABLE 1 BEST 81.48 (0.83) 71.80 (0.75) 78.95 (0.36) which also improves over the

competing baseline methods.


-----

5.4 Q3: GPCANET-INITIALIZATION FOR GCN

Finally, we evaluate the effectiveness of GPCANET-initialization for GCN in terms of performance
and robustness under different model sizes, i.e. number of layers L or number of training parameters.
For comparison, Xavier initialization (Glorot & Bengio, 2010) is also used to initialize GCN.


Table 3: Test set performance of GCN with Xaiver- versus
GPCANET-initialization, w.r.t. varying number of layers (L)
across all datasets. Each reported value is based on the best selected configuration on validation data. GPCANET-init. enables
higher performance that is also stable with increasing depth.

DATASET _L=2_ _L=3_ _L=5_ _L=10_ _L=15_

CORA XAIVER-INIT 80.62 80.62 79.40 76.37 66.07
CORA GPCANET-INIT **81.67 79.50 80.90 79.82 78.00**

CITESEER XAIVER-INIT 71.25 70.15 71.10 61.90 57.40
CITESEER GPCANET-INIT **71.27 69.27 70.15 68.67 67.87**

PUBMED XAIVER-INIT **78.42 77.90 77.07 77.00 45.80**
PUBMED GPCANET-INIT 78.05 77.25 78.07 77.80 78.03


We report the test set performance (averaged over 5 seeds)
of the GCN model using both
initializations in Table 3. The
results show that GPCANETinitialization tends to outperform the widely-used Xavier
initialization. The improvement grows with increasing number of layers, which is significant at large depths. Notably, GCN with GPCANETinitialization exhibits stable performance across all layers.


ARXIV XAIVER-INIT 69.61 70.64 70.33 68.32 61.68
ARXIV GPCANET-INIT **69.76 70.72 70.52 69.77 66.28** Besides looking at the av
erage performance, we fur
PRODUCTS XAIVER-INIT 77.90 78.65 78.08 76.27 74.70

ther study whether GPCANET
PRODUCTS GPCANET-INIT 78.13 78.71 78.22 77.47 75.90

initialization improves the training robustness, by reducing performance variation across different seeds. To this end, we first choose
the best configuration for each initialization method based on validation performance, and train the
GCN model with the chosen configuration using 100 random seeds.

In Figure 2 we present the Arxiv, 2-Layer GCN Arxiv, 15-Layer GCN

Xavier-init 10 Xavier-init

over 100 runs with different 6

8

seeds for ARXIV. (For results on other datasets, see Ap
4

pendix. A.8.) For both 2-layer 2

2

and 15-layer GCN, GPCANETinitialization not only outper- 69.4 69.6 69.8 70.0 56 58 60 62 64 66 68

|GPCANet-init Xavier-init|Col2|
|---|---|


|Col1|GPCANe Xavier-i|
|---|---|


GPCANet-init
Xavier-init

GPCANet-init
Xavier-init

Test accuracy Test accuracy

forms Xavier-initialization w.r.t.

Figure 2: Comparison between Xavier-init. and GPCANET-init.

average performance, but also

in terms of test accuracy robustness over 100 seeds on ARXIV.

in terms of robustness, achiev
GPCANET-init. enables robust training especially at larger depth.

ing much lower performance
variation and few bad outliers, especially for deeper GCN. As such, it acts as a strong data-driven
prior, facilitating the training of numerous parameters across many layers by identifying a promising
region of the parameter space from which supervised fine-tuning is initiated.


6 CONCLUSION

In this work we have (1) discovered a mathematical connection between GPCA and graph convolution
of GCN and PPNP; (2) extended GPCA to the (semi-)supervised setting; (3) proposed GPCANET,
by stacking GPCA and nonlinear activation, which is a generalized GCN model with an additional
hyperparameter to control the degree of graph regularization, and (4) introduced the GPCANETinitialization based on the established connection. Accordingly, we designed extensive experiments
demonstrating that (i) the unsupervised shallow GPCA achieves comparable or better performance
than GCN, APPNP, as well as other modern GNNs which suggests that graph convolution’s power is
mainly driven by graph regularization; (ii) semi-supervised GPCA helps improve performance and
should be a powerful yet simple baseline in future research; (iii) GPCANET enables the training of
shallow models with competitive performance via increasing the degree of graph regularization at
each layer, with reduced memory and training time cost; and finally (iv) GPCANET-initialization
acts as a strong data-driven prior for GCN training, enabling robust performance. Our methodological
contributions ( 3) & 4) above) capitalize on the discovery of our theoretical findings ( 1) & 2) ),
shedding new light toward a better understanding and design of GNNs.


-----

REFERENCES

Suresh Balakrishnama and Aravind Ganapathiraju. Linear discriminant analysis-a brief tutorial.
_Institute for Signal and information Processing, 18(1998):1–8, 1998._

Y. Bengio, A. Courville, and P. Vincent. Representation learning: A review and new perspectives.
_IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798–1828, August 2013._
[ISSN 0162-8828. doi: 10.1109/TPAMI.2013.50. URL http://ieeexplore.ieee.org/](http://ieeexplore.ieee.org/document/6472238/)
[document/6472238/. Zu bearbeitendes Review.](http://ieeexplore.ieee.org/document/6472238/)

Tsung-Han Chan, Kui Jia, Shenghua Gao, Jiwen Lu, Zinan Zeng, and Yi Ma. PCANet: A simple
deep learning baseline for image classification? IEEE Transactions on Image Processing, 24(12):
5017–5032, 2015.

Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An
efficient algorithm for training deep and large graph convolutional networks. In Proceedings of
_the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp._
257–266, 2019.

Federico Errica, Marco Podda, Davide Bacciu, and Alessio Micheli. A fair comparison of graph
neural networks for graph classification. In International Conference on Learning Representations
_[(ICLR), 2020. URL https://openreview.net/forum?id=HygDF6NFPB.](https://openreview.net/forum?id=HygDF6NFPB)_

Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In
_ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019._

Brian Gallagher, Hanghang Tong, Tina Eliassi-Rad, and Christos Faloutsos. Using ghost edges for
classification in sparsely labeled networks. In International Conference on Knowledge Discovery &
_[Data Mining, pp. 256–264. ACM, 2008. URL http://dblp.uni-trier.de/db/conf/](http://dblp.uni-trier.de/db/conf/kdd/kdd2008.html#GallagherTEF08)_
[kdd/kdd2008.html#GallagherTEF08.](http://dblp.uni-trier.de/db/conf/kdd/kdd2008.html#GallagherTEF08)

Paul Geladi and Bruce R Kowalski. Partial least-squares regression: a tutorial. Analytica chimica
_acta, 185:1–17, 1986._

Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the 13th International Conference on Artificial Intelligence and
_Statistics, pp. 249–256, 2010._

G.H. Golub and C.F. Van Loan. Matrix Computations. Johns Hopkins University Press, 1989.

Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In
_Advances in neural information processing systems, pp. 1024–1034, 2017._

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
_Conference on Computer Vision, pp. 1026–1034, 2015._

Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,
and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv
_preprint arXiv:2005.00687, 2020._

Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning, pp. 448–456.
PMLR, 2015.

Bo Jiang, Chris Ding, Bio Luo, and Jin Tang. Graph-laplacian PCA: Closed-form solution and
robustness. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 3492–3498, 2013.

Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
In International Conference on Learning Representations (ICLR), 2017.


-----

Johannes Klicpera, Aleksandar Bojchevski, and Stephan Gunnemann. Predict then propagate:¨
Graph neural networks meet personalized pagerank. In International Conference on Learning
_Representations (ICLR), 2019._

Philipp Krahenb¨ uhl, Carl Doersch, Jeff Donahue, and Trevor Darrell. Data-dependent initializations¨
of convolutional neural networks. In International Conference on Learning Representations (ICLR),
2016.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097–1105,
2012.

Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for
semi-supervised learning. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.

Qimai Li, Xiao-Ming Wu, Han Liu, Xiaotong Zhang, and Zhichao Guan. Label efficient semisupervised learning via graph filtering. In Proceedings of the IEEE/CVF Conference on Computer
_Vision and Pattern Recognition, pp. 9582–9591, 2019._

Andreas Loukas. What graph neural networks cannot learn: depth vs width. In International
_[Conference on Learning Representations, 2020. URL https://openreview.net/forum?](https://openreview.net/forum?id=B1l2bp4YwS)_
[id=B1l2bp4YwS.](https://openreview.net/forum?id=B1l2bp4YwS)

Cheng-Yaw Low, Andrew Beng-Jin Teoh, and Kar-Ann Toh. Stacking PCANet+: An overly simplified
convnets baseline for face recognition. IEEE Signal Processing Letters, 24(11):1581–1585, 2017.

Yao Ma, Xiaorui Liu, Tong Zhao, Yozen Liu, Jiliang Tang, and Neil Shah. A unified view on graph
neural networks as graph signal denoising. arXiv preprint arXiv:2010.01777, 2020.

Dmytro Mishkin and Jiri Matas. All you need is a good init. arXiv preprint arXiv:1511.06422, 2015.

Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav
Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In
_Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 4602–4609, 2019._

Hoang NT and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass filters.
_arXiv preprint arXiv:1905.09550, 2019._

Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node
classification. In International Conference on Learning Representations (ICLR), 2020. URL
[https://openreview.net/forum?id=S1ldO2EFPr.](https://openreview.net/forum?id=S1ldO2EFPr)

Victor Y Pan and Zhao Q Chen. The complexity of the matrix eigenproblem. In Proceedings of the
_31th annual ACM Cymposium on Theory of Computing, pp. 507–516, 1999._

Xuran Pan, Shiji Song, and Gao Huang. A unified framework for convolution-based graph neural
[networks, 2021. URL https://openreview.net/forum?id=zUMD--Fb9Bt.](https://openreview.net/forum?id=zUMD--Fb9Bt)

Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics
of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.

Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3):93–93, 2008.

Mathias Seuret, Michele Alberti, Marcus Liwicki, and Rolf Ingold. Pca-initialized deep neural
networks applied to document image analysis. In 2017 14th IAPR International Conference on
_Document Analysis and Recognition, volume 1, pp. 877–882. IEEE, 2017._

Nauman Shahid, Nathanael Perraudin, Vassilis Kalofolias, Gilles Puy, and Pierre Vandergheynst.
Fast robust PCA on graphs. IEEE Journal of Selected Topics in Signal Processing, 10(4):740–756,
2016.

David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst.
The emerging field of signal processing on graphs: Extending high-dimensional data analysis to
networks and other irregular domains. IEEE Signal Processing magazine, 30(3):83–98, 2013.


-----

Balasubramaniam Srinivasan and Bruno Ribeiro. On the equivalence between positional node
embeddings and structural graph representations. In International Conference on Learning Repre_[sentations, 2020. URL https://openreview.net/forum?id=SJxzFySKwH.](https://openreview.net/forum?id=SJxzFySKwH)_

Petar Velickoviˇ c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li´ o, and Yoshua`
Bengio. Graph Attention Networks. In International Conference on Learning Representations
_(ICLR), 2018._

Petar Velickoviˇ c, William Fedus, William L. Hamilton, Pietro Li´ o, Yoshua Bengio, and R Devon`
Hjelm. Deep graph infomax. In International Conference on Learning Representations (ICLR),
[2019. URL https://openreview.net/forum?id=rklz9iAcKQ.](https://openreview.net/forum?id=rklz9iAcKQ)

Raimar Wagner, Markus Thom, Roland Schweiger, Gunther Palm, and Albrecht Rothermel. Learning¨
convolutional neural networks from few samples. In The 2013 International Joint Conference on
_Neural Networks (IJCNN), pp. 1–7. IEEE, 2013._

Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph convolutional networks. In International Conference on Machine Learning, pp.
6861–6871, 2019.

Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
[networks? In International Conference on Learning Representations (ICLR), 2019. URL https:](https://openreview.net/forum?id=ryGs6iA5Km)
[//openreview.net/forum?id=ryGs6iA5Km.](https://openreview.net/forum?id=ryGs6iA5Km)

Zhenyue Zhang and Keke Zhao. Low-rank matrix approximation with manifold regularization. IEEE
_Transactions on Pattern Analysis and Machine Intelligence, 35(7):1717–1729, 2012._

Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In International
_[Conference on Learning Representations (ICLR), 2020. URL https://openreview.net/](https://openreview.net/forum?id=rkecl1rtwB)_
[forum?id=rkecl1rtwB.](https://openreview.net/forum?id=rkecl1rtwB)

Meiqi Zhu, Xiao Wang, Chuan Shi, Houye Ji, and Peng Cui. Interpreting and unifying graph neural
networks with an optimization framework. arXiv preprint arXiv:2101.11859, 2021.


-----

A APPENDIX

A.1 PROOF OF THEOREM 3.1

_Proof. We give the proof in two steps._

_Step 1: For a fixed W_ _, Solve optimal Z_ _[∗]_ _as a function of W_ _: When fixing W as constant, the_
problem becomes quadratic and convex. There is a unique solution, given by first-order optimal
condition. Let ℓ denote the objective function as given in equation 5. Its gradient can be calculated as
_∂ℓ_

_L)Z_ 2XW . (15)
_∂Z_ [= 2(][I][ +][ α] [˜] _−_

Setting equation 15 to 0 leads to the solution Z _[∗]_ = (I + αL[˜])[−][1]XW .

_Step 2: Replace Z with Z_ _[∗], Solve optimal W_ _[∗]:_ Substituting Z in objective ℓ with Z _[∗]_ = (I +
_αL[˜])[−][1]XW_, we reduce the optimization to

min _L)[−][1]XWW_ _[T]_ _F_ [+][ α][ Tr] _W_ _[T]_ _X_ _[T]_ (I + _αL[˜])[−][1][ ˜]L(I +_ _αL[˜])[−][1]XW_ _._ (16)
_W,W_ _[T]_ _W =I_ _[∥][X][ −]_ [(][I][ +] _[α]_ [˜] _∥[2]_

For this part only, let M = (I + αL[˜])[−][1] to simplify notation. We can show that equation 16 is 
equivalent to
min Tr(XX _[T]_ + MXWW _[T]_ _WW_ _[T]_ _X_ _[T]_ _M_ )
_W,W_ _[T]_ _W =I_

_−_ 2 Tr(MXWW _[T]_ _X_ _[T]_ ) + α Tr(W _[T]_ _X_ _[T]_ _M_ _LMXW[˜]_ ) (17)

Using the cyclic property of (Tr)ace (and plugging (I + αL[˜])[−][1] for M back), we can write it as (see
Supp. A.2 for detailed derivation.)
max Tr _W_ _[T]_ _X_ _[T]_ (I + αL[˜])[−][1]XW _._ (18)
_W,W_ _[T]_ _W =I_

Based on the spectral theorem of PSD matrices, the optimal solution  W _[∗]_ of problem equation 18
is the combination of eigenvectors, associated with the largest c eigenvalues of the graph-revised
covariance matrix X _[T]_ (I + αL[˜])[−][1]X.

A.2 DERIVATION FROM EQ. EQUATION 17 TO EQ. EQUATION 18

For this part only, let A = (I + αL[˜])[−][1] to simplify the notation. We can show that equation 16 is
equivalent to
min Tr(XX _[T]_ ) 2 Tr(AXWW _[T]_ _X_ _[T]_ )
_W,W_ _[T]_ _W =I_ _−_

+ Tr(AXWW _[T]_ _WW_ _[T]_ _X_ _[T]_ _A) + α Tr(W_ _[T]_ _X_ _[T]_ _ALAXW[˜]_ )

max 2 Tr(AXWW _[T]_ _X_ _[T]_ ) Tr(AXWW _[T]_ _X_ _[T]_ _A)_
_≡_ _W,W_ _[T]_ _W =I_ _−_

_−_ _α Tr(W_ _[T]_ _X_ _[T]_ _ALAXW[˜]_ ) (19)
Using the cyclic property of (Tr)ace, we can write
max 2 Tr(W _[T]_ _X_ _[T]_ _AXW_ ) Tr(W _[T]_ _X_ _[T]_ _AAXW_ )
_W,W_ _[T]_ _W =I_ _−_


_−_ _α Tr(W_ _[T]_ _X_ _[T]_ _ALAXW[˜]_ )

max Tr _W_ _[T]_ _X_ _[T]_ (2A _AA_ _A(αL[˜])A)XW_
_W,W_ _[T]_ _W =I_ _−_ _−_

max Tr W _[T]_ _X_ _[T][  ]A +_ _I_ _A(I + αL[˜])_ _A_ _XW_
_W,W_ _[T]_ _W =I_ _{_ _−_ _}_

max Tr W _[T]_ _X_ _[T]_ (I + αL[˜])[−][1]XW   (20)
_W,W_ _[T]_ _W =I_

where the objective simplifies upon replacing _A with (I + αL[˜])[−]_ [1].


-----

A.3 DERIVATION OF EQUIVALENCE IN EQ. EQUATION 9

_T_
max corr(Y, z) corr(Y, z) var(z)
**z**

_T_
max var(Y ) corr(Y,  **z)** corr(Y, z) var(z) (21)
_≡_ **z**

_T_
max cov(Y, z) cov(Y,  z)  (22)
_≡_ **z**
where cov(Y,  z) = var (Y )corr(Y, z) var(z)

max _Y_ _[T]_ **z** _T_ _Y_ _[T]_ **z** p p (23)
_≡_ **z**

max **z** _[T]_ _Y Y[T] z_  (24)
_≡_ **z**


Note that in equation 21 we added the term var(Y ) without affecting the optimization problem as it
is with respect to z.

A.4 DATASET STATISTICS

Table 4: Statistics of used datasets.

DATASET #NODES #EDGES #FEATURES #CLASSES TRAIN/VAL./TEST

CORA 2,708 5,429 1,433 7 5.2%/18.5%/36.9%
CITESEER 3,327 4,732 3,703 6 3.6%/15%/30%
PUBMED 19,717 44,338 500 3 0.3%/2.5%/5%
ARXIV 169,343 1,166,243 128 40 54%/18%/28%
PRODUCTS 2,449,029 61,859,140 100 47 8%/2%/90%

Datasets used in the experiments are presented in Table 4. Cora, CiteSeer, and PubMed can be
downloaded in Pytorch Geometric Library Fey & Lenssen (2019). Arxiv and Products can be
[accessed in https://ogb.stanford.edu/.](https://ogb.stanford.edu/)

A.5 HYPERPARAMETER CONFIGURATIONS

We setup hyperparameters pool for each dataset, presented in Table 5. All methods use the same pool.
The only exception is GPCA, as GPCA is just a 1-layer shallow model which can be trained with
lager learning rate; we use 0.1 learning rate for it on all datasets.

Table 5: Hyperparameters pool for each dataset, includes learning rate (LR), weight decay (WD),
number of layers (#Layers), hidden size, dropout, α, and β. For ARXIV and PRODUCTS, weight
decay is set as 0 because the dataset is large and no overfit happened. Same reason for choosing
smaller dropout rate for them.

DATASET LR WD #LAYERS HIDDEN

CORA 0.001 [0.0005, 0.005, 0.05] [2, 3, 5, 10, 15] [128, 256]
CITESEER 0.001 [0.0005, 0.005, 0.05] [2, 3, 5, 10, 15] [128, 256]
PUBMED 0.001 [0.0005, 0.005, 0.05] [2, 3, 5, 10, 15] [128, 256]
ARXIV 0.005 0 [2, 3, 5, 10, 15] [128, 256]
PRODUCTS 0.001 0 [2, 3, 5, 10, 15] [128, 256]

DATASET DROPOUT _α_ _β_

CORA [0, 0.5] [1, 5, 10, 20, 50] [0, 0.1, 0.2]
CITESEER [0, 0.5] [1, 5, 10, 20, 50] [0, 0.1, 0.2]
PUBMED [0, 0.5] [1, 5, 10, 20, 50] [0, 0.1, 0.2]
ARXIV [0, 0.2] [1, 5, 10, 20, 50] 0
PRODUCTS [0, 0.1] [1, 5, 10, 20, 50] 0

Models are trained on every configuration across HP pools and picked based on validation performance. We use the Adam optimizer for all models. Learning rate is first manually tuned for each
dataset to achieve stable training, and the same learning rate is fixed for all models—we empirically


-----

observed that learning rate is sensitive to datasets but insensitive to models. For GPCA and GPCANET, number of power iterations in Eq. equation 13 is always set to 5. All experiments use the
maximum training epoch as 1000 and repeat 5 times. Detailed configuration of HPs can be found
in Supp. A.5. We mainly use a single GTX-1080ti GPU for small datasets CORA, CITESEER, and
PUBMED. RTX-3090 GPU is used for ARXIV and PRODUCTS.

**Mini-batch training. As nodes are not independent, GNN is mostly trained in full-batch under**
semi-supervised setting. We use full-batch training for all datasets except PRODUCTS, which is too
large to fit into GPU memory during training. ClusterGCN Chiang et al. (2019), a subgraph based
mini-batch training algorithm, is used to train GCN and GPCANET. For evaluation, we still use
full-batch since a single forward pass can be conducted without memory issues. Initialization is also
employed in full-batch.

**Fair evaluation. Instead of picking the hyperparameter configurations manually, reported (test)**
performance is based on the best configuration selected using validation performance, where all
models leverage the same hyperparameter pools. Further, each configuration from the pool is
conducted 5 times to reduce randomness.

A.6 GPCA WITH VARYING α

Table 6: Performance of unsupervised GPCA (β = 0) for varying α w.r.t. mean test accuracy and
standard deviation (in parentheses). GPCA (best α) selects α ∈{1, 5, 10, 20, 50} based on validation,
whereas GPCA with specific α uses the specified fixed α.

CORA CITESEER PUBMED ARXIV PRODUCTS

81.10 71.80 78.78 71.86 79.23
GPCA (BEST α)
(0.00) (0.75) (0.36) (0.18) (0.14)

72.57 70.90 76.92 65.47 73.65
GPCA-α=1
(0.79) (0.58) (0.30) (0.26) (0.07)

80.95 71.80 **79.40** 70.69 78.66
GPCA-α=5
(0.17) (0.75) (0.29) (0.11) (0.09)

**82.23** 71.65 78.78 71.37 **79.24**
GPCA-α=10
(0.58) (0.53) (0.36) (0.09) (0.09)

82.05 **72.15** 78.15 **71.86** 79.23
GPCA-α=20
(0.54) (0.47) (0.50) (0.18) (0.14)

81.10 71.50 78.00 71.48 78.92
GPCA-α=50
(0.00) (0.32) (0.19) (0.15 (0.10)

A.7 CONFIGURATIONS FOR EXPERIMENTS OF 1∼3-LAYER GPCANET

To train a shallow GPCANET with tunable α (β=0 is used), we setup different α pool for different
number of layers, because the effect of increasing α is the same to increasing number of layers
(shown in Figure 1). We report the pool for α for each layer in Table 7. For other parameters we use
the same setting mentioned in Table 5.

Table 7: Pool of α for 1∼3-layer GPCANET, same across all datasets.

# LAYERS POOL OF α

1-LAYER [10, 20, 30]
2-LAYER [3, 5, 10]
3-LAYER [1, 2, 3, 5]

A.8 GPCANET-INIT’S ROBUSTNESS FOR ADDITIONAL DATASETS

Histogram of test set accuracy over 100 runs for GCN initialized by Xavier-initialization and
GPCANET-initialization in CORA (Figure 3), CITESEER (Figure 4), and PUBMED (Figure 5).
We have ignored PRODUCTS as it takes too long to run 100 times, but the result should be similar.


-----

Cora, 2-Layer GCN

79 80 81 82

GPCANet-init
Xavier-init

Test accuracy


Cora, 15-Layer GCN

30 40 50 60 70 80

GPCANet-init
Xavier-init

Test accuracy


20

15

10


15

10


Figure 3: Comparison between Xavier-init and GPCANET-init in terms of test accuracy robustness
over 100 seeds on CORA.


CiteSeer, 2-Layer GCN


CiteSeer, 15-Layer GCN


20

15

10


70.25 70.50 70.75 71.00 71.25 71.50 71.75 72.00 50 55 60 65 70

GPCANet-init
Xavier-init

GPCANet-init
Xavier-init

Test accuracy Test accuracy

Figure 4: Comparison between Xavier-init and GPCANET-init in terms of test accuracy robustness
over 100 seeds on CITESEER.


PubMed, 2-Layer GCN


PubMed, 15-Layer GCN


60

40


15

10


77.5 78.0 78.5 79.0 40 50 60 70 80

GPCANet-init
Xavier-init

GPCANet-init
Xavier-init

Test accuracy Test accuracy

Figure 5: Comparison between Xavier-init and GPCANET-init in terms of test accuracy robustness
over 100 seeds on PUBMED.


-----

A.9 TRAINING CURVE COMPARISON FOR GPCANET-INIT

Figure 6: Training curve of 10-layer GCN initialized with Xavier initialization and GPCANET-Init
on CORA.

Figure 7: Training curve of 10-layer GCN initialized with Xavier initialization and GPCANET-Init
on ARXIV.

A.10 RUNTIME COMPARISON

We have analyzed the runtime complexity of GPCA, GPCANET, and GPCANET-Init in Sec.3.6 and
show their runtime is linear in number of nodes. Table 8 presents the practical runtime comparison
among all methods, measured in seconds/epoch for all models, and total initialization seconds for
GPCANET-Init, which verified the complexity analysis. Besides, GPCA is a extremely fast method
with strong performance, and should be used as a strong baseline in future research.

Table 8: Runtime comparison for different methods over all datasets.

|Col1|CORA CITESEER PUBMED ARXIV PRODUCTS|
|---|---|

|Num Nodes n Features d|2,708 3,327 19,717 169,343 2,449,029 1,433 3,708 500 128 100|
|---|---|

|GCN (seconds/epoch) GPCA (seconds/epoch) GPCANET (seconds/epoch)|0.0025 0.0025 0.0040 0.0469 30.9544 0.0010 0.0010 0.0010 0.0072 0.0443 0.0062 0.0101 0.0202 0.2172 31.3664|
|---|---|

|GPCANET-Init (seconds)|0.836 1.614 0.659 0.657 2.477|
|---|---|


-----

