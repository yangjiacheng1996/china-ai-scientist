# CENTROID APPROXIMATION FOR BOOTSTRAP

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Bootstrap is a principled and powerful frequentist statistical tool for uncertainty
quantification. Unfortunately, standard bootstrap methods are computationally
intensive due to the need of drawing a large i.i.d. bootstrap sample to approximate
the ideal bootstrap distribution; this largely hinders their application in large-scale
machine learning, especially deep learning problems. In this work, we propose
an efficient method to explicitly optimize a small set of high quality “centroid”
points to better approximate the ideal bootstrap distribution. We achieve this by
minimizing a simple objective function that is asymptotically equivalent to the
Wasserstein distance to the ideal bootstrap distribution. This allows us to provide
an accurate estimation of uncertainty with a small number of bootstrap centroids,
outperforming the naive i.i.d. sampling approach. Empirically, we show that our
method can boost the performance of bootstrap in a variety of applications.

1 INTRODUCTION

Bootstrap is a simple and principled frequentist uncertainty quantification tool and can be flexibly
applied to obtain data uncertainty estimation with strong theoretical guarantees (Hall et al., 1988;
Austern & Syrgkanis, 2020; Chatterjee et al., 2005; Cheng et al., 2010). In particular, when combined
with the maximum likelihood estimator or more general M-estimators, bootstrap provides a generalpurpose, plug-and-play non-parametric inference framework for general probabilistic models without
case-by-case derivations; this makes it a promising frequentist alternative to Bayesian inference.

However, the standard bootstrap inference is highly expensive in both computation and memory as it
typically requires drawing a large number[1] of i.i.d. bootstrap particles (samples) to obtain an accurate
uncertainty estimation. In the context of this paper, as each bootstrap particle/sample/centroid is a
machine learning model, we might directly call a model as particle/sample/centroid. With a small
number of particles, bootstrap may perform poorly. As a consequence, when applied to deep learning,
we need to store a large number of neural networks and feed the input into a tremendous number
of networks every time we make inference, which can be quite expensive and even unaffordable
for deep learning problems with huge models[2]. For example, in autonomous driving applications,
our device can only store a limited number of models and we need to make decisions within a short
time, which makes the standard bootstrap with a large number of models no more feasible. Typical
ensemble methods in deep learning, such as Lakshminarayanan et al. (2017); Huang et al. (2017);
Vyas et al. (2018); Maddox et al. (2019); Liu & Wang (2016), can only afford to use a small number
(e.g., less than 20) of models.

Therefore, to make bootstrap more accessible in modern machine learning, it is essential to develop
new approaches that break the key computation and memory barriers mentioned above. This paper
aims to improve the bootstrap when the resource at inference is limited. We are motivated to consider
the following problem:

_How to improve the accuracy of bootstrap inference when the number of particles is limited?_

We attack this challenge by presenting an efficient centroid approximation for bootstrap. Our method
replaces the i.i.d. bootstrap particles with a set of carefully optimized centroid particles that are

1For example, thousands of, as suggested by Statistics textbooks such as Wasserman (2013).
2While training cost is an extra burden, it is small compared with the cost of making prediction as we only
need to train the model once but make countless predictions at deployment.


-----

guaranteed to provide an accurate and compact approximation to the ideal bootstrap distribution so
that only a smaller number of particles is needed to obtain good performance.

Our method is based on minimizing a specially designed objective function that is asymptotically
equivalent to the Wasserstein distance between the ideal bootstrap distribution and the particle
distribution formed by the learned centroids. During the training, each centroid adjusts its location
being aware of the locations of the others so that centroids are diversified and well distributed on
the domain. Our method is similar to doing K-means on the ideal bootstrap distribution, finding
K representative centroids that well represent K separate parts of the target distribution’s domain
in an optimal way. As centroids are optimized to better approximate the distribution, our approach
naturally improves over the vanilla bootstrap with i.i.d. particles. See Figure 1 for illustration.

Empirically, we apply the centroid approximation method to various applications, including confidence interval estimation (DiCiccio et al., 1996), bootstrap method for contextual bandit (Riquelme
et al., 2018), bootstrap deep Q-network (Osband et al., 2016) and bagging method (Breiman, 1996)
for neural networks. We find that our method consistently improves over the standard bootstrap.


**Notation** We use ∥·∥ to represent the ℓ2 norm
for a vector and the operator norm for a matrix.
We denote the integer set {1, 2, ...., N _} by [N_ ].
Given any m, we define the probability simplex
_C[m[m]] and:= {[vi1, ..., v[m]_ _[v][i]m[ = 1]] ∈[}][. For a symmetric ma-]R[m]_ : vi ≥ 0, ∀i ∈

_∈_
trix M, we denote its minimal eigenvalue by
_λmin(M_ )[P]. For a positive-definite matrix M, if

_C_

[

_M = A[⊤]A, then we denote A by M_ [1][/][2]. We denote the Wasserstein distance between two distribution ρ1 and ρ2 by W2[ρ1, ρ2]. We use O and
_o to denote the conventional big-O and small-_
o notation and use Op to denote the stochastic

boundedness. We use _→d_ to denote convergence
in distribution.


Figure 1: The solid lines represent the density of
the target distribution. Left figure: Typical i.i.d.
particles that are randomly distributed on the domain. Right figure: The learned diversified centroids that are well distributed on the domain. The
centroids partition the domain into several disjoint
regions (separated by the dashed lines in the figure)
and each centroid can be viewed as the ‘K-means’
center of the region it belongs to.

2 BACKGROUND


Suppose we have a model fθ parameterized by θ in a parameter space Θ R[d]. Let _xi_ _i=1_
a training set with n data points on . Assume ℓ(x, fθ) is the negative log-likelihood of data point ⊆ _{_ _}[n]_ _[⊂X][ be] x_
_X_
with model fθ. A standard approach to estimate θ is maximum likelihood estimator (MLE), which
minimizes the negative log-likelihood function (loss) over the training set

_θˆ = arg min_ (θ) = _i=1[ℓ][(][x][i][, f][θ][)][/n.]_
_θ_ Θ _L_
_∈_ _[L][(][θ][)][,]_

Here the MLE _θ[ˆ] provides a point estimation without any information on the data uncertainty. Bootstrap[P][n]_
is a simple and effective frequentist method to quantify the uncertainty. The bootstrap loss is a
randomly perturbed loss defined as

_Lw(θ) =_ _i=1[w][i][ℓ][(][x][i][, f][θ][)][/n,]_

where w = [w1, ..., wn][⊤] is a set of random weights of data points drawn from some distribution π.

[P][n]

A typical choice of π is the multinomial distribution with uniform probability, which corresponds to
resampling on the training set with replacement. Given w, one can calculate its associated bootstrap
particle by minimizing the bootstrap loss:

_θˆw = arg min_ (1)
_θ∈Θ_ _[L][w][(][θ][)][.]_

Let ρπ be the distribution of _θ[ˆ]w when w ∼_ _π. Bootstrap theory indicates that we can quantify the_
data uncertainty of θ or any function g(θ) using ρπ. We call ρπ the ideal bootstrap distribution and it
is the main object we want to approximate.


-----

Denote δθ as the delta measure centered at θ. Standard bootstrap method approximates ρπ by
the particle distribution ˆρπ(·) = _j=1[δ]θ[ˆ]wj_ [(][·][)][/m][ formed by][ m][ i.i.d. particles][ {]θ[ˆ]wj _}j[m]=1[, which]_

(can be obtained by drawing1). However, for deep learning applications, as discussed in the introduction, storing and making m i.i.d. weights[P][m] _{wj}j[m]=1_ [from][ π][ and calculating each][ ˆ]θwj based on
inference using a large number m of bootstrap particles can be quite expensive. On the other hand,
if m is small, ˆρπ tends to be a poor approximation of ρπ. In this paper, we aim to improve the
approximation of the particle distribution when m is small.

3 METHOD

Our idea is simple. Instead of using i.i.d. particles, in which the location of each particle is
independent from that of the others, we try to actively optimize the location of each particle so
that particles are diversified, better distributed and eventually providing a particle distribution with
improved approximation accuracy. A natural way to achieve this goal is to explicitly optimize a set
of points {θj}j[m]=1 [(called centroids) jointly such that the Wasserstein distance between][ ρ][π][ and the]
induced particle distribution is minimized:

_θj[∗][, v]j[∗]_ _j=1_ [= arg] min _mj=1[v][j][δ][θ]j_ _[, ρ][π]_ _._ (2)
_{_ _[}][m]_ _θ1,...,θm∈Θ, [v1,...,vm]∈C[m][ W][2]_
hP i

Here we consider a Wasserstein distance W2 equipped with a special data-dependent distance metric
_|| · ||D that will be introduced later in (5). We note that here we also optimize the probability weights_
_{vj}j[m]=1_ [of the centroids. Finding the optimal centroids and probability weights can be decomposed]
into two steps: the centroid learning phase and the probability weights learning phase, based on the
facts in (3,4).

_W2[2]_ _j∈[m][v]j[∗][δ][θ]j[∗]_ _[, ρ][π]_ = Jπ({θj[∗][}]j[m]=1[)][,][ where][ J][π][(][{][θ][j][}]j[m]=1[):=][E][w][∼][π] minj∈[m]||θj − _θ[ˆ]w||D[2]_ _. (3)_

P  h i

Here (3) implies that, to find the optimal particle distribution in (2), we can start with the centroid
learning phase where we only need to optimize the centroids. It can be achieved by minimizing
_Jπ({θj}j[m]=1[)][, which is the averaged distance of bootstrap particles to their closest centroid. After we]_
obtain the optimal centroids, the optimal probability weights can be learned by (4):

_vj[∗]_ [= ˜]vj[∗][/][P]s∈[m] _v[˜]s[∗][,][ where][ ˜]vj[∗]_ [=][ P][w][∼][π] _j = arg minj∈[m]||θj[∗]_ _[−]_ _θ[ˆ]w||D[2]_ _._ (4)

 

Here vj[∗] [is the proportion of bootstrap particles that are closest to the centroid][ j][. We emphasize that]
the optimal solution to two-stage learning is guaranteed to be the global minimizer of the loss in (2)
(see Lemma 3.1 and 3.2 in Canas & Rosasco (2012)).

However, the key issue is that the losses in both (2, 3) can not be computed in practice, as they require
us to access ρπ (i.e., obtain _θ[ˆ]w first in order to calculate the loss). To handle this issue, we seek an_
easy-to-compute surrogate loss. Our idea is based on the following observation. Assuming the size
of training data is large, which is usually the case in deep learning, we can expect that θw will be
centered around a small region[3]. It implies that we should search the centroid in this small region.
Notice that when θ is close to _θ[ˆ]w, based on Taylor approximation, we have_

_Lw(θ) ≈Lw(θ[ˆ]w) + ∇θ[⊤][L][w][(ˆ]θw)(θ −_ _θ[ˆ]w) + 1/2(θ −_ _θ[ˆ]w)[⊤]∇θ[2][L][w][(ˆ]θw)(θ −_ _θ[ˆ]w)_

_≈L∞(θ0) + 1/2||θ −_ _θ[ˆ]w||D[2]_ _[,][ where][ ∥][V][ ∥][2]D_ [:=][ V][ ⊤][∇]θ[2][L][∞][(][θ][0][)][V.] (5)

Here (θ) := Exℓ(x, fθ) denotes the population loss; θ0 is the minimizer of (θ). In (5), we use
_L∞_ _L∞_
the facts[4] that _θ_ _θw) = 0; and with large training set, the empirical distribution_ _i=1[δ][x]i_ _[/n]_
well approximates the whole data population, and hence the bootstrap resampling distribution, i.e., ∇[⊤][L][w][(ˆ]
_n_
_i=1[w][i][δ][x]i_ _[/n][ on the empirical distribution also well approximates the whole data population. This][P][n]_
implies that **_w(_** ) ( ) and _θ[L][w][(][·][)][ ≈∇][2]θ[L][∞][(][·][)][. As the loss are close to each other, their]_
P _L_ _·_ _≈L∞_ _·_ _∇[2]_
minimizers are also close _θ[ˆ]w ≈_ _θ0. Since L∞(θ0) is some (unknown) constant independent with θ,_
we can replace the ||θj − _θ[ˆ]w||D[2]_ [in (][3][) by][ L][w][(][θ][j][)][ as it only adds some constant into the loss.]

3This can be formally characterized by central limit theorem as discussed in Section 4.
4We defer the detailed analysis to Section 4.


-----

Intuitively, we can expect that the centroid closest to _θ[ˆ]w is the one that gives the smallest loss on Lw._
It motivates us to learn the centroids via the modified centroid learning phase:

_θj[∗]_ _j=1_ [= arg] min minj [m] **_w(θj)_** _._ (6)
_{_ _[}][m]_ _θ1,...,θm∈Θ_ [E][w][∼][π] _∈_ _L_
 

Similarly, the optimal probability weights can be learned via the modified weight learning phase:

_vj[∗]_ [= ˜]vj[∗][/][P]s∈[m] _v[˜]s[∗][,][ where][ ˜]vj[∗]_ [=][ P][w][∼][π] _j ∈_ arg minj∈[m]Lw(θj[∗][)] _._ (7)

We note that here we slightly abuse the notation of θj[∗] [and]  _[ v]j[∗]_ [in (][3][,][4][) and (][6][,][7][) for simplification. In]
the later context, θj[∗] [and][ v]j[∗] [are used based on their definitions in (][6][,][7][).]

**Connection to K-means By viewing the target distribution as a set of particles that we want to**
cluster, in K-means clustering, each centroid (i.e., K-means center) represents one of the K disjoint
groups[5] of particles, which is formed by assigning each particle in the whole set to the closest centroid
among all the K centroids. K-means learns the optimal K centroids in the way that they can best
approximate the whole set. The ‘closeness’ for assigning the particles is measured by the distance
between the two points. As pointed out by Canas & Rosasco (2012), K-means essentially searches
the optimal particle distribution formed by the K centroids that minimizes its Wasserstein distance to
the target distribution. Our centroid approximation idea follows the same fashion of clustering but
our key innovation is to measure the ‘closeness’ by examining the bootstrap loss of the centroids so
that we can still learn the optimal centroids without obtaining the i.i.d. bootstrap particles first. We
also point out that, while we share the same objective as K-means, the optimization algorithms differ.
The Expectation-Maximization type of algorithm used by K-means is not applicable to our scenario.

**Comparing with Other Particle Improving Approach Intuitively, from a high level abstracted**
perspective, we provide an approach to use K-means type of idea to improve the particle quality
_without accessing to the true target distribution. This is the key differentiator of this work to other_
approaches that improve the particle quality, as they all require to access the target distribution. For
example Claici et al. (2018) requires that sampling from target distribution is cheap and easy. Chen
et al. (2012; 2018a); Campbell & Beronov (2019) need to access the logarithm of the probability
density function of the target distribution. In our problem, neither sampling from the target distribution
is cheap nor the logarithm of the probability density function is available, making those approaches
not more applicable.

3.1 TRAINING

The optimization of (6) can be solved by gradient descent. Suppose θj[∗][(][t][)][ is the][ j][-th centroid at]
iteration t. We initialize {θj[∗][(0)][}]j[m]=1 [by sampling from][ ρ][π][ and at iteration][ t][, we update][ θ]t[∗] [by applying]
the gradient descent on the loss in (6), which yields

_θj[∗][(][t][ + 1)][ ←]_ _[θ]j[∗][(][t][)][ −]_ _[ϵ][t][g][(][θ]j[∗][(][t][))][,]_

(8)
_g(θj[∗][(][t][)) =][ ∇][θ][E][w][∼][π]_ I{j ∈ _uw(t)}Lw(θj[∗][(][t][))]_ _/vj[∗][(][t][)][,]_

where we define the index of the closest centroid to particle _θ[ˆ]w as uw(t) = arg minj_ [m] **_w(θj[∗][(][t][))]_**
_∈_ _L_
and vj[∗][(][t][) =][ P][w][∼][π][ (][j][ ∈] _[u][w][(][t][))][ denotes the probability that centroid][ j][ is the one that gives the lowest]_
bootstrap loss. The denominator vj[∗][(][t][)][ in][ g][(][θ]j[∗][(][t][))][ is optional. However, notice that the magnitude of]
numerator in g(θk[∗][(][t][))][ decays with larger][ m][, which might require an adjustment of the learning rate]
when m changes. This adjustment can be avoided by rescaling with vk[∗][(][t][)][.]

We note that {θj[∗][(0)][}]j[m]=1 [is just][ m][ i.i.d. bootstrap particles which is not optimal for approximation]
and our algorithm can be viewed as an approach for refining the m particles by solving (6). In
practice, we find that we can simply use random initialization (e.g., draw θ from some Gaussian
distribution) instead.

**Centroid Degeneration Phenomenon Naively applying the updating rule (8) may cause a degenera-**
tion phenomenon: When a centroid happens to give considerably worse performance than others,
which can be caused by the stochasticity of gradient or worse initialization, the performance of this
centroid will remain considerably worse throughout the optimization. The reason is simple. As this

5i.e. the regions separated by the dashed lines in the right plot of Figure 1.


-----

centroid (e.g. θj[∗][(][t][)][) gives a considerably worse performance, the probability that it gives the lowest]
bootstrap loss, i.e., vj[∗][(][t][)][, is small. As a consequence, the gradient that updates this centroid is only]
based on aggregating information from a small low-density region of π and hence can be unstable and
further degrades this centroid. Note that this mechanism is self-reinforced since when this centroid
cannot be effectively improved in the current iteration, it faces the same issue in the next one. As a
result, this centroid is always significantly worse than the others.

We call this undesirable phenomenon centroid degeneration and we want to prevent this phenomenon
because when it happens, we have a centroid that is not representative and contributes less to
approximating ρπ. We solve this issue with a simple solution and here is the intuition. The reason
that a centroid degenerates lies in that this centroid is far from the good region where it gives a good
performance. And when this happens, we should push the centroid to move towards this good region,
which can be achieved by using the common gradient over the whole training data. Specifically, we
define a threshold γ, indicating centroid j is degenerated if vj[∗][(][t][)][ ≤] _[γ][. And when it happens, we]_
update using the common gradient over the whole data:

_θj[∗][(][t][ + 1)][ ←]_ _[θ]j[∗][(][t][)][ −]_ _[ϵ][t][∇][θ][L][(][θ]j[∗][(][t][))][.]_ (9)

In section 4, we give a theoretical analysis on why this modification is important and is able to solve
the centroid degeneration issue.

**Practical Algorithm In practice, we estimate the gradient by replacing the expectation over w ∼** _π_
in (8) with averaging over M i.i.d. samples {wh}h[M]=1 [drawn from][ π][:]

_gˆ(θj[∗][(][t][)) =]_ _Mh=1_ I{j ∈M _uwh_ (t)}∇θLwh (θj[∗][(][t][))] _._ (10)

P  _h=1_ [I][{][j][ ∈] _[u][w]h_ [(][t][)][}] 

We emphasize that here uwh and Lwh for allP wh can be computed very cheaply, enabling us to use a
very large M to reduce the error of gradient estimation. Specifically, at iteration t, for each j ∈ [m],
we first calculate

**L(θj[∗][(][t][)) = [][ℓ][(][x][1][, f]θj[∗][(][t][)][)][, ...ℓ][(][x][n][, f][θ]j[∗][(][t][)][)]][⊤]** _[∈]_ [R][n][,] (11)

which is the vector encodes the loss of centroid j at each data point. This procedure does not introduce
any extra computational overhead compared with standard gradient descent. After that, the bootstrap
loss of centroid j can be computed cheaply by Lw(θj[∗][(][t][)) =][ w][⊤][L][(][θ]j[∗][(][t][))][. Similarly, it is cheap to]
obtain uwh by

_uwh_ (t) = arg minj [m] **_[w]h[⊤][L][(][θ]j[∗][(][t][))][.]_** (12)
_∈_

Taking the modified updating rule introduced to prevent the centroid degeneration phenomenon into
account, we update θj[∗][(][t][)][ by][ θ]j[∗][(][t][ + 1)][ ←] _[θ]j[∗][(][t][)][ −]_ _[ϵ][t][φ][(][θ]j[∗][(][t][))][, where]_

_φ(θj[∗][) =]_ _gˆ(θj[∗][(][t][))]_ if _h∈[M_ ] [I][{][u][w]h [(][t][) =][ j][}][/M > γ] (13)

( _θ_ (θj[∗][(][t][))] otherwise.

_∇_ _L_

[P]

Notice that as as L(θj[∗][(][t][))][ is pre-computed, calculating][ L][w]h [(][θ]j[∗][(][t][))][ for many (e.g.,][ M] [) different]
**_wh is almost free, since it only requires a simple matrix multiplication with O(nM_** ) complexity.
Similarly, calculating uwh (t) is also very cheap. In practical implementation, as uwh (t) do not
change much within a few iterations, we can update uwh (t) every a few iterations (e.g., every epoch).
We can also replace the ∇θLwh (θj[∗][(][t][))][ or][ ∇][θ][L][(][θ]j[∗][(][t][))][ in (][13][) using a mini-batch of data instead of]
the whole data, which leads to a stochastic gradient version of our algorithm. We refer readers to
Algorithm 1 for the ideal updating and Algorithm 2 for the practical implementation in Appendix B
for more details.

4 THEORY

Recall that, as discussed in (5), our approach relies on the intuition that bootstrap particles are nested
in a small region so that we can approximate the distance between the centroid and a bootstrap particle
by the bootstrap loss of that centroid. The main goal of this section is to give a formal theoretical
justification of this intuition.


-----

Before we proceed, we clarify several important setups for establishing and interpreting the theoretical
result. As discussed in the introduction, we are mainly interested in the scenerio that the number of
available particles/centroids m is small while the number of training data n is large, which motivates
us to establish theoretical result in the region of small m and large n. This is significantly different
from conventional asymptotic analysis in which we aim to show the behavior when m →∞. We
consider the setting that the parameter dimension d is fixed and does not scale with n.

_We are mainly interested in characterizing the approximation of the proposed loss in (6) to the ideal_
_loss in (3), given any small and fixed number m of centroids when n →∞. This justifies why the_
_proposed centroid approximation method can be viewed as minimizing the Wasserstein distance_
_between the particle distribution ρ[∗]π_ _[and the target bootstrap distribution][ ρ][π][.]_

For simplicity, we build our analysis assuming the ideal update rule (8,9) is used. We start with the
following main assumptions.

**Assumption 1 (Smoothness and boundedness) Assume that the following quantities are upper**
_bounded by some constant c < ∞:_

_∂[3]ℓ(x, fθ)_ _θ[ℓ][(][x, f][θ]1_ [)][ −∇][2]θ[ℓ][(][x, f][θ]2 [)][||]

1. max sup ; 2. sup sup _||∇[2]_ ;
_i,j,k∈[d]_ _θ∈Θ,x∈X_ _∂iθi∂θj∂θk_ _θ1,θ2∈Θ_ _x∈X_ _||θ1 −_ _θ2||_

3. _x∈Xsup,θ∈Θ_ _∇θ[2][ℓ][(][x, f][θ][)]_ ; 4. supθ∈Θ _∥θ∥_ _._

Assumption 1 is a standard regularity condition on the boundness and smoothness of the problem.

**Assumption 2 (Asymptotic normality) Assume** _n_ _θˆw_ _θ_ _d_ (0, A) and _n_ _θˆ_ _θ0_ _d_

_[√]_ _−_ [ˆ] _→N_ _[√]_ _−_ _→_

(0, A) as n _, where A is a positive-definite matrix with the largest eigenvalue bounded._   
_N_ _→∞_


Assumption 2 is a higher level assumption on the asymptotic normality of the estimators. Such result
is classic and can be derived with some weak and technical regularity conditions. See examples in
Chatterjee et al. (2005); Cheng et al. (2010).

**Assumption 3 (On the global minimizer) Suppose that λmin** _∇θ[2][L][∞][(][θ][0][)]_ _> 0._

Assumption 3 is also standard showing the locally strongly convexity of the loss around the truth   _θ0._

**Assumption 4 (On the learning rate) Suppose that maxt ϵt = O(n[−][1]).**

Assumption 4 assumes that the learning rate of the algorithm is sufficiently small such that its induced
discretization error is not the dominating term.

The key challenge of our analysis is to show that our dynamics is (θ0, r)-stable (defined below in
_B_
Definition 1) for some small r, saying that {θj[∗][(][t][)][}]j[m]=1 [stay in a small region that is close to][ θ][0][ for any]
_iteration t. Combined with the property[6]_ that _θ[ˆ]w are also close to θ0, the centroids and the bootstrap_
particles are close to each other and thus our approximation in (5) holds for all t ≥ 0. In this way,
optimizing the centroids by minimizing our loss is almost equivalent to optimizing the centroids by
minimizing the Wasserstein distance.

**Definition 1 (B(θ, r)-stable) Given some θ ∈** Θ and r ≥ 0, we say our dynamics is B(θ, r)-stable
_if ∀t ≥_ 0 and ∀j ∈ [m], θj[∗][(][t][)][ ∈B][(][θ, r][)][, where][ B][(][θ, r][) :=][ {][θ][′][ :][ ∥][θ][′][ −] _[θ][∥≤]_ _[r, θ][′][ ∈]_ [Θ][}][ is the ball]
_with radius r centered at θ._

The key intuition to establish such (θ0, r)-stable result is to characterize that our optimization
_B_
dynamics is implicitly self-controlled: when some centroid approaches the boundary of (θ0, r), the
_B_
updating mechanism automatically start to push the centroid to move towards the center of the region.
Thus, if all the centroids are within (θ0, r) at initialization, they will alway stay in this region.
_B_

Thanks to assumption 2, 3, when the dataset is large, the landscape of our loss is locally strongly
convex around θ0. When a centroid j is at the boundary of B(θ0, r), it has vj[∗][(][t][)][ < γ][ and thus the]

6This is implied by the asymptotic normality in assumption 2.


-----

|Col1|Col2|Col3|m = 20 m = 50 m = 100 m = 200|
|---|---|---|---|
|α = 0.9|Normal|Bootstrap Centroid|0.029 ± 0.010 0.031 ± 0.011 0.021 ± 0.010 0.017 ± 0.010 0.027 ± 0.010 0.001 ± 0.009 0.012 ± 0.010 0.016 ± 0.010|
||Percentile|Bootstrap Centroid|0.101 ± 0.013 0.036 ± 0.011 0.021 ± 0.010 0.014 ± 0.010 0.081 ± 0.012 0.021 ± 0.010 0.020 ± 0.010 0.015 ± 0.010|
||Pivotal|Bootstrap Centroid|0.106 ± 0.013 0.045 ± 0.011 0.025 ± 0.010 0.023 ± 0.010 0.046 ± 0.011 0.013 ± 0.009 0.011 ± 0.010 0.020 ± 0.010|


Table 1: Centroid approximation for confidence interval. The numbers in the table represent |α − _αˆ|,_
where ˆα is the estimated coverage probability. The errors bar is the standard deviation.

updating direction is the gradient of loss L. By the convexity, such gradient will push the centroid
move towards the center of (θ0, r) where the empirical minimizer locates at. On the other hand, for
_B_
centroid j with vj[∗][(][t][)][ ≥] _[γ][, its updating direction aggregates information from sufficient data point]_
and thus behaves similarly to that of the common gradient, pushing centroid to move towards the
center with the centroid is not close to the center.

**Theorem 1 Under Assumptions 1-4 and suppose that we initialize θj[∗][(0)][,][ j][ ∈]** [[][m][]][ by sampling from]
_ρπ, given any m <_ _and γ > 0, when n is sufficiently large, we have_
_∞_

max _θj[∗][(][t][)][ −]_ _[θ][0]_ = Op( (log n)/n).
_j∈[m]_ [sup]t≥0

p

_Here the probability is taken w.r.t. training data._

Theorem 1 implies our dynamics is B(θ0, rn)-stable with rn = O( log n/n). The condition that

_θj[∗][(0)][ ∼]_ _[ρ][π][ i.i.d. can be replaced by the condition that][ θ]j[∗][(0)][ is sufficiently close to][ θ][0][. We need such]_

p

condition as we uniformly bound the distance between θj[∗][(][t][)][ and][ θ][0][ at any iteration including the first]
one. Theorem 1 implies that the approximation stated in (5) holds with high probability and hence
the proposed loss in (6) is ‘almost as good as’ the ideal loss in (3).

**Theorem 2 Under the same assumptions as Theorem 1, given any m < ∞** _and γ > 0, when n is_
_sufficiently large, we have_

supt≥0 _j∈[m]_ _[L][w][(][θ]j[∗][(][t][))]][ −]_ _[B][ −]_ [E][w][∼][π][[ min]j∈[m] _[||][θ]j[∗][(][t][)][ −]_ _θ[ˆ]w||D[2]_ []][/][2] [=][ O][p][(]q(log n)/n[3][/][2]).

_Here the probability is taken w.r.t. training data and[E][w][∼][π][[ min]_ _B is some constant independent from θj[∗][(][t][)][ for]_
_any t ≥_ 0 and j ∈ [m].

**Asymptotics when m also grows** Although our main interest is the asymptotics with a small, fixed
_m and growing n, we discuss here on asymptotics when m also grows. As shown in Section 3 and_
introduction, our method can be viewed as an ‘approximated’ K-means on the target distribution. From
Theorem 5.2 in Canas & Rosasco (2012), the particle distribution formed by the optimal centroids
learned by K-means gives improved O(m[−][1][/d]) convergence to any general target distribution in
terms of Wasserstein distance, where d is data dimension. In comparison, the particle distribution of
i.i.d. sample only gives O(m[−][1][/][(2][d][+4)]) from Theorem 5.1 in Canas & Rosasco (2012). This implies
that our approach potentially also has such a rate improvement. Note that the results in Canas &
Rosasco (2012) are for general target distribution without any n involves. To rigorously establish the
large m asymptotic result for our problem, we need to study the joint limit of n and m. This is indeed
very non-trivial: as discussed in Weed et al. (2019) (i.e. Proposition 14), when n →∞, the target
distribution ρπ becomes a sharp Gaussian and the convergence rate of i.i.d. bootstrap particles will
gradually improve to O(m[−][1][/][2]) (in a way that depends on n). It implies that when n ≫ _m →∞,_
our improvement may become only constant level. We find establishing such a theory is out the scope
of this conference paper and leave it as future work.

5 EXPERIMENT

As discussed in the introduction, our main goal is to improve the quality of the particle distribution
when only a limited number of particles/centroids is allowed, so that we can use less particles at


-----

|Col1|Col2|m = 3 m = 4 m = 5 m = 10|
|---|---|---|
|Mushroom|Bootstrap Centroid|3282.1 ± 72.82 3307.9 ± 69.2 3311.6 ± 79.3 3397.4 ± 51.4 3702.7 ± 89.76 3723.1 ± 78.7 3799.6 ± 84.2 3796.9 ± 36.1|
|Statlog|Bootstrap Centroid|1864.3 ± 6.4 1869.2 ± 5.2 1877.2 ± 4.1 1877.0 ± 2.7 1893.6 ± 6.0 1892.6 ± 3.6 1891.3 ± 3.5 1892.6 ± 2.8|
|Financial|Bootstrap Centroid|2255.77 ± 58.45 2265.42 ± 58.17 2269.33 ± 56.36 2281.35 ± 56.65 2313.29 ± 56.45 2315.32 ± 56.75 2323.88 ± 56.73 2325.54 ± 56.05|


Table 2: Results on the contextual bandit experiment. The numbers in the table represent the averaged
reward with its standard deviation.

deployment, which reduces the memory consumption and the computational cost for making prediction. Thus, our experiment design will be focusing on comparing the testing performance of vanilla
bootstrap and our centroid based approach when the same and a small number of particles/centroids is
used. We apply our method to four applications: confidence interval construction, bootstrap method
in contextual bandit, bootstrapped deep Q-network and bagging. Due to space limit, we refer to
Appendix C.4 for the bagging experiment, Appendix C.5 for ablation study on the importance of
modifying the gradient to overcome the centroid degeneration phenomenon. Although we are less
interested in the computational cost of training, as discussed in Section 3, our method actually only
introduces a little training computation overhead, which is another advantage of our method. We
draw analysis on this aspect in Appendix C.6.


0.20

0.15

0.10

0.05


5.1 BOOTSTRAP CONFIDENCE INTERVAL

We start with a classic application of bootstrap:
confidence interval estimation for linear model
with parameter θ. Fix confidence level α, we
consider three ways to construct (two-sided)
bootstrap confidence interval of θ: the Normal
interval, the percentile interval and the pivotal
interval. And we test m = 20, 50, 100, 200. For
all experiments, we repeat with 1000 independent random trials. We consider the standard
bootstrap as baseline. Detailed experimental
setup are included in Appendix C.1.


20 50 100 200

|Col1|Col2|Col3|Bootstrap Centroid|Col5|
|---|---|---|---|---|
||||||
||||||
||||||


Num particles


Figure 2: Wasserstein distance between the particle
distribution and the true bootstrap distribution w.r.t.
the number of particles.


Figure 2 shows the Wasserstein distance between the true target distribution ρπ and the empirical distributions obtained by (a) i.i.d. sampling
_ρˆπ, (b) the proposed centroid approximation ρ[∗]π[. The centroid approximation significantly reduces]_
the Wasserstein distance by a large margin. We then compare the quality of obtained confidence
intervals, which is measured by the difference between the estimated coverage probability and the
true confidence level, i.e., |αˆ − _α| (the lower the better). Here we only consider confidence intervals_
of the first coordinate of θ: θ1. Table 1 summarizes the result with α = 0.9. We see that using more
particles is generally able to improve the constructed confidence intervals. We also compare with
two variants of standard bootstrap: Bayesian bootstrap (Rubin, 1981) and residual bootstrap (Efron,

1992). And we consider varying α = 0.8, 0.95. These results are included in Appendix C.1.

5.2 CENTROID APPROXIMATION FOR BOOTSTRAP METHOD IN CONTEXTUAL BANDIT


Contextual bandit is a classic task in sequential decision making, in which accurately quantifying
the model uncertainty is important in order to achieve good exploration-exploitation trade-off. As
shown in Riquelme et al. (2018), tracking the model uncertainty using bootstrap is a strong method
for contextual bandit. However, it is costly to maintain a large number of bootstrap models and thus
the number of models is typically within 10 (Osband et al., 2016). We find that applying the proposed
centroid approximation here can significantly improve the performance. Riquelme et al. (2018)
uses m = 3 bootstrap models and we give a more comprehensive evaluation with m = 3, 4, 5, 10.
We consider three datasets: Mushroom, Statlog and Financial. We set γ = 0.5/m. We randomly
generate 20 different context sequences, apply all the methods and report the averaged cumulative
reward and its standard deviation. Table 2 summarizes the result and note that a large part of variance


-----

250

200

150

100

50


50

40

30

20

10


50

100

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|
|---|---|---|---|---|---|---|---|---|
||||||||||
||||||||||
||||||||||
||||||Bootstra|p 2-head|||
||||||Centroid Bootstra|-2-head p-5-head|||
||||||Centroid|-5-head|||

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
|||||
||Bootstrap 2-hea Centroid-2-head|d||
||Bootstrap 5-hea|d||
||Centroid-5-head|||


100 150 200 250 300 350 400

Bootstrap 2-head
Centroid-2-head
Bootstrap-5-head
Centroid-5-head

Episodes


30 40 50 60 70 80 90

Bootstrap 2-head
Centroid-2-head
Bootstrap 5-head
Centroid-5-head

Episodes


Figure 3: Results for Bootstrap DQN with centroid approximation experiment. Left: LunarLander-v2;
Right: Catcher-v0.

can be explained by different context sequences. All results in Table 2 are statistically significant
under significant level 5% using matched pair t-test. Table 2 shows that using more bootstrap models
generally improves the accumulated reward. And when using the same number of models, the
proposed centroid approximation method consistently improves over standard bootstrap method. We
refer readers to appendix C.2 for more information on the background and experiment.

5.3 CENTROID APPROXIMATION FOR BOOTSTRAP DQN


“Efficient exploration is a major challenge for reinforcement learning (RL). Common dithering strategies such as ϵ-greedy do not carry out temporally-extended exploration, which leads to exponentially
larger data requirements” (Osband et al., 2016). To tackle this issue, Osband et al. (2016) proposes
the Bootstrapped Deep Q-Network (DQN). We apply our centroid approximation to improve Bootstrapped DQN. We consider m = 2, 5 and similar to the experimental setting in contextual bandit,
we set γ = 0.5/m. We consider two benchmark environments: LunarLander-v2 and Catcher-v0
from GYM (Brockman et al., 2016) and PyGame learning environment (Tasfi, 2016). We conduct
the experiment with 5 independent random trails and report the averaged result with its standard
deviation. We refer readers to Appendix C.3 for more background and other experiment details.
Figure 3 summarizes the result. For LunarLander-v2, Bootstrap DQN with 2 and 5 heads give similar
performance but both converge to a less optimal model compared with the centroid approximation
method. Centroid approximation method with 2 and 5 heads performs similarly at convergence but
centroid approximation method with 5 heads is able to converge faster than 2-head model and thus
has lower regret. For Catcher-v0, adding more heads to the model is able to improve the performance
for both methods. The proposed centroid approximation consistently improves over baselines.

6 RELATED WORK


**Bootstrap is an classical statistical inference method, which was developed by Efron (1992) and**
generalized by, i.e., Mammen (1993); Shao (2010); Efron (2012) (just to name a few). Bootstrap can
be widely applied to various statistical inference problem, such as confidence interval estimation
(DiCiccio et al., 1996), model selection (Shao, 1996), high-dimensional inference (Chen et al., 2018b;
El Karoui & Purdom, 2018; Nie & Rockovˇ a´, 2020), off-policy evaluation (Hanna et al., 2017),
distributed inference (Yu et al., 2020) and inference for ensemble model (Kim et al., 2020), etc.

Despite its wide applications and nice theoretical properties, there has been very few works on
discussing and improving the approximation efficiency in the region of small bootstrap sample size,
beyond the i.i.d. sampling paradigm. While the m-out-of-n bootstrap (Bickel et al., 2012) and the
bag of little bootstrap (Kleiner et al., 2014) are designed to reduce the computational cost with the
subsampling techniques in the big data settings (large n), they still require a large bootstrap sample
size and thus are still not scalable to large deep learning applications.

**Bayesian Inference is a different approach to quantify the model uncertainty. Different from**
frequentists’ method, Bayesian assumes a prior over the model and the uncertainty can be captured by
the posterior. Bayesian inference have been largely popularized in machine learning, largely thanks
to the recent development in scalable sampling method (Welling & Teh, 2011; Chen et al., 2014;
Seita et al., 2018; Wu et al., 2020), variational inference (Blei et al., 2017; Liu & Wang, 2016), and
other approximation methods such as Gal & Ghahramani (2016); Lee et al. (2018). In comparison,


-----

bootstrap has been much less widely used in modern machine learning and deep learning. We believe
this is largely attributed to the lack of similarly efficient computational methods in the small sample
_size m region, which is the very problem that we aim to address with our new centroid approximation_
method.

**Uncertainty in Deep Learning In additional to the applications considered in this paper, uncertainty**
in deep learning model can also be applied to problems including calibration (Guo et al., 2017)
and out-of-distribution detection (Nguyen et al., 2015). The definition of uncertainty of neural
network is quite generalized (e.g., Gal & Ghahramani (2016); Ovadia et al. (2019); Maddox et al.
(2019); Van Amersfoort et al. (2020)) and can be quite different from the uncertainty that bootstrap
inference want to quantify and can be approached with various methods including drop out (Gal &
Ghahramani, 2016; Durasov et al., 2020), label smoothing (Qin et al., 2020), designing new modules
in the model (Kivaranovic et al., 2020), adversarial training (Lakshminarayanan et al., 2017) and
Bayesian modeling (Blundell et al., 2015), etc. This paper focuses on improving the bootstrap method
and thus is orthogonal to those previous works. Pearce et al. (2018); Salem et al. (2020) also try to
refine the ensemble models to improve the quality of prediction interval. Compare with our method,
their method can only be applied to prediction interval and does not have theoretical guarantee.

7 CONCLUSION

We propose a centroid approximation method to learn an improved particle distribution that better
approximates the target bootstrap distribution, especially in the region with small particle size.
Theoretically, when the size of training data is large, our objective function is surrogate to the
Wasserstein distance between the particle distribution and target distribution. Thus, compared with
standard bootstrap, the proposed centroid approximation method actively optimizes the distance
between particle distribution and target distribution. The proposed method is simple and can be
flexibly used for applications of bootstrap with negligible extra computational cost.


-----

REFERENCES

Morgane Austern and Vasilis Syrgkanis. Asymptotics of the empirical bootstrap method beyond
asymptotic normality. arXiv preprint arXiv:2011.11248, 2020.

Peter J Bickel, Friedrich Gotze, and Willem R van Zwet. Resampling fewer than n observations:¨
gains, losses, and remedies for losses. In Selected works of Willem van Zwet, pp. 267–297. Springer,
2012.

David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisticians.
_Journal of the American statistical Association, 112(518):859–877, 2017._

Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural network. In International Conference on Machine Learning, pp. 1613–1622. PMLR, 2015.

Leo Breiman. Bagging predictors. Machine learning, 24(2):123–140, 1996.

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.

Trevor Campbell and Boyan Beronov. Sparse variational inference: Bayesian coresets from scratch.
_Advances in Neural Information Processing Systems, 32:11461–11472, 2019._

Guillermo Canas and Lorenzo Rosasco. Learning probability measures with respect to optimal
transport metrics. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.), Advances
_in Neural Information Processing Systems, volume 25. Curran Associates, Inc., 2012._

Snigdhansu Chatterjee, Arup Bose, et al. Generalized bootstrap for estimating equations. The Annals
_of Statistics, 33(1):414–436, 2005._

Tianqi Chen, Emily Fox, and Carlos Guestrin. Stochastic gradient hamiltonian monte carlo. In
_International conference on machine learning, pp. 1683–1691. PMLR, 2014._

Wilson Ye Chen, Lester Mackey, Jackson Gorham, Franc¸ois-Xavier Briol, and Chris Oates. Stein
points. In International Conference on Machine Learning, pp. 844–853. PMLR, 2018a.

Xiaohui Chen et al. Gaussian and bootstrap approximations for high-dimensional u-statistics and
their applications. The Annals of Statistics, 46(2):642–678, 2018b.

Yutian Chen, Max Welling, and Alex Smola. Super-samples from kernel herding. arXiv preprint
_arXiv:1203.3472, 2012._

Guang Cheng, Jianhua Z Huang, et al. Bootstrap consistency for general semiparametric m-estimation.
_Annals of Statistics, 38(5):2884–2915, 2010._

Sebastian Claici, Aude Genevay, and Justin Solomon. Wasserstein measure coresets. arXiv preprint
_arXiv:1805.07412, 2018._

Thomas J DiCiccio, Bradley Efron, et al. Bootstrap confidence intervals. Statistical science, 11(3):
189–228, 1996.

Nikita Durasov, Timur Bagautdinov, Pierre Baque, and Pascal Fua. Masksembles for uncertainty
estimation. arXiv preprint arXiv:2012.08334, 2020.

Bradley Efron. Bootstrap methods: another look at the jackknife. In Breakthroughs in statistics, pp.
569–593. Springer, 1992.

Bradley Efron. Bayesian inference and the parametric bootstrap. The annals of applied statistics, 6
(4):1971, 2012.

Noureddine El Karoui and Elizabeth Purdom. Can we trust the bootstrap in high-dimensions? the
case of linear models. The Journal of Machine Learning Research, 19(1):170–235, 2018.

Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertainty in deep learning. In international conference on machine learning, pp. 1050–1059.
PMLR, 2016.


-----

Alex Graves. Practical variational inference for neural networks. In Advances in neural information
_processing systems, pp. 2348–2356. Citeseer, 2011._

Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. In International Conference on Machine Learning, pp. 1321–1330. PMLR, 2017.

Peter Hall et al. Rate of convergence in bootstrap approximations. The Annals of Probability, 16(4):
1665–1684, 1988.

Josiah Hanna, Peter Stone, and Scott Niekum. Bootstrapping with models: Confidence intervals for
off-policy evaluation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31,
2017.

Botao Hao, Yasin Abbasi Yadkori, Zheng Wen, and Guang Cheng. Bootstrapping upper confidence
bound. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett´
(eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.,
2019.

Jiri Hron, Alexander G de G Matthews, and Zoubin Ghahramani. Variational gaussian dropout is not
bayesian. arXiv preprint arXiv:1711.02989, 2017.

Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, and Kilian Q Weinberger.
Snapshot ensembles: Train 1, get m for free. International Conference on Learning Representations,
2017.

Byol Kim, Chen Xu, and Rina Foygel Barber. Predictive inference is free with the jackknife+-afterbootstrap. Advances in Neural Information Processing Systems, 33, 2020.

Danijel Kivaranovic, Kory D Johnson, and Hannes Leeb. Adaptive, distribution-free prediction
intervals for deep networks. In International Conference on Artificial Intelligence and Statistics,
pp. 4346–4356. PMLR, 2020.

Ariel Kleiner, Ameet Talwalkar, Purnamrita Sarkar, and Michael I Jordan. A scalable bootstrap
for massive data. Journal of the Royal Statistical Society: Series B: Statistical Methodology, pp.
795–816, 2014.

Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive
uncertainty estimation using deep ensembles. Advances in neural information processing systems,
30, 2017.

Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and
Yasaman Bahri. Deep neural networks as gaussian processes. In International Conference on
_Learning Representations, 2018._

Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference
algorithm. Advances in Neural Information Processing Systems, 29, 2016.

Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson.
A simple baseline for bayesian uncertainty in deep learning. Advances in Neural Information
_Processing Systems, 32:13153–13164, 2019._

Enno Mammen. Bootstrap and wild bootstrap for high dimensional linear models. The annals of
_statistics, pp. 255–285, 1993._

Benedict C May, Nathan Korda, Anthony Lee, and David S Leslie. Optimistic bayesian sampling in
contextual-bandit problems. Journal of Machine Learning Research, 13:2069–2106, 2012.

Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidence
predictions for unrecognizable images. In Proceedings of the IEEE conference on computer vision
_and pattern recognition, pp. 427–436, 2015._

Lizhen Nie and Veronika Rockovˇ a.´ Bayesian bootstrap spike-and-slab lasso. _arXiv preprint_
_arXiv:2011.14279, 2020._


-----

Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via
bootstrapped dqn. Advances in neural information processing systems, 29:4026–4034, 2016.

Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D Sculley, Sebastian Nowozin, Joshua Dillon,
Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? evaluating
predictive uncertainty under dataset shift. Advances in Neural Information Processing Systems, 32:
13991–14002, 2019.

Tim Pearce, Alexandra Brintrup, Mohamed Zaki, and Andy Neely. High-quality prediction intervals
for deep learning: A distribution-free, ensembled approach. In International Conference on
_Machine Learning, pp. 4075–4084. PMLR, 2018._

Yao Qin, Xuezhi Wang, Alex Beutel, and Ed H Chi. Improving uncertainty estimates through the
relationship with adversarial robustness. arXiv preprint arXiv:2006.16375, 2020.

Carlos Riquelme, George Tucker, and Jasper Snoek. Deep bayesian bandits showdown: An empirical
comparison of bayesian deep networks for thompson sampling. In International Conference on
_Learning Representations, 2018._

Donald B Rubin. The bayesian bootstrap. The annals of statistics, pp. 130–134, 1981.

Tarik S Salem, Helge Langseth, and Heri Ramampiaro. Prediction intervals: Split normal mixture´
from quality-driven deep ensembles. In Conference on Uncertainty in Artificial Intelligence, pp.
1179–1187. PMLR, 2020.

Daniel Seita, Xinlei Pan, Haoyu Chen, and John Canny. An efficient minibatch acceptance test
for metropolis-hastings. In Proceedings of the 27th International Joint Conference on Artificial
_Intelligence, pp. 5359–5363, 2018._

Jun Shao. Bootstrap model selection. Journal of the American statistical Association, 91(434):
655–665, 1996.

Xiaofeng Shao. The dependent wild bootstrap. Journal of the American Statistical Association, 105
(489):218–235, 2010.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.

Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
_learning research, 15(1):1929–1958, 2014._

Norman Tasfi. Pygame learning environment. [https://github.com/ntasfi/](https://github.com/ntasfi/PyGame-Learning-Environment)
[PyGame-Learning-Environment, 2016.](https://github.com/ntasfi/PyGame-Learning-Environment)

William R Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrika, 25(3/4):285–294, 1933.

Joost Van Amersfoort, Lewis Smith, Yee Whye Teh, and Yarin Gal. Uncertainty estimation using a
single deep deterministic neural network. In International Conference on Machine Learning, pp.
9690–9700. PMLR, 2020.

Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double qlearning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 30, 2016.

Apoorv Vyas, Nataraj Jammalamadaka, Xia Zhu, Dipankar Das, Bharat Kaul, and Theodore L
Willke. Out-of-distribution detection using an ensemble of self supervised leave-out classifiers. In
_Proceedings of the European Conference on Computer Vision (ECCV), pp. 550–564, 2018._

Larry Wasserman. All of statistics: a concise course in statistical inference. Springer Science &
Business Media, 2013.

Jonathan Weed, Francis Bach, et al. Sharp asymptotic and finite-sample rates of convergence of
empirical measures in wasserstein distance. Bernoulli, 25(4A):2620–2648, 2019.


-----

Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In
_Proceedings of the 28th international conference on machine learning (ICML-11), pp. 681–688._
Citeseer, 2011.

Tung-Yu Wu, YX Rachel Wang, and Wing H Wong. Mini-batch metropolis–hastings with reversible
sgld proposal. Journal of the American Statistical Association, pp. 1–9, 2020.

Jeremy Wyatt. Exploration and inference in learning from reinforcement. 1998.

Yang Yu, Shih-Kang Chao, and Guang Cheng. Simultaneous inference for massive data: Distributed
bootstrap. In International Conference on Machine Learning, pp. 10892–10901. PMLR, 2020.


-----

A PROOF

We also show Theorem 3, which gives a formal characterization of the Taylor approximation intuition
introduced in (5).

**Theorem 3 Under Assumption 1 and 2, when n is sufficiently large, we have**


_⊤_

**_w(θ) =_** **_w(θ[ˆ]w) +_** [1]2 _θ_ _θw_ _θ[L][∞][(][θ][0][)]_ _θ_ _θw_
_L_ _L_ _−_ [ˆ] _∇[2]_ _−_ [ˆ]

  

+Op _θ_ _θw_ (n[−][1][/][2] + _θ_ _θw_ ) _._
_||_ _−_ [ˆ] _||[2]_ _||_ _−_ [ˆ] _||_
 


_Here the stochastic boundedness is taken w.r.t. the training data and w._

In the proof, we may use c to represent some absolute constant, which may vary in different lines.

A.1 PROOF OF THEOREM 3

With the fact that **_w_** (θ[ˆ]w) = 0 and under assumption 1, using Taylor expansion, we have
_∇_ _L_

3[]
_⊤_

**_w(θ) =_** **_w(θ[ˆ]w) + [1]_** _θ_ _θw_ _θ[L][w][(ˆ]θw)_ _θ_ _θw_ + O _θ_ _θw_ _._
_L_ _L_ 2 _−_ [ˆ] _∇[2]_ _−_ [ˆ] _−_ [ˆ]

    

Notice that
_⊤_
_θ −_ _θ[ˆ]w_ _∇θ[2][L][w][(ˆ]θw) −∇θ[2][L][∞][(][θ][0][)]_ _θ −_ _θ[ˆ]w_
 2    

_≤_ _θ −_ _θ[ˆ]w_ _∇θ[2][L][w][(ˆ]θw) −∇θ[2][L][∞][(][θ][0][)]_

2
_≤_ _θ −_ _θ[ˆ]w_ _∇θ[2][L][w][(ˆ]θw) −∇θ[2][L][w][(][θ][0][)]_ + _∇θ[2][L][w][(][θ][0][)][ −∇]θ[2][L][∞][(][θ][0][)]_

2 [] 
_θ_ _θw_ _C_ _θw_ _θ0_ + _θ[L][w][(][θ][0][)][ −∇]θ[2][L][∞][(][θ][0][)]_ _F_ _,_
_≤_ _−_ [ˆ] _−_ _∇[2]_

[] 

where we denote the Frobenius norm as[ˆ] _∥·∥F . With assumption 2, we have_ _θw −_ _θ0_ = Op(n[−][1][/][2]).

By applying centroid limit theorem and delta method to _∇θ[2]ij_ _[L][w][(][θ][0][)][ −∇]θ[2]ij[ˆ][L][∞][(][θ][0][)]_ for every pair

_i, j ∈_ [d], we have _∇θ[2][L][w][(][θ][0][)][ −∇][2]θ[L][∞][(][θ][0][)]_ _F_ [=][ O][p][(][n][−][1][/][2][)][. Thus we obtained the desired result.]

A.2 PROOF OF THEOREM 1


Given any radius r and ϵ > 0, with sufficiently large n, we have

P _θw −_ _θ0_ _≥_ _r_ _≤_ exp(−λnr[2])+ϵ/m,
 

for λ = 4[1] _[λ][max][(][A][)][−][1][. Here the probability is the jointly probability of bootstrap weight and training][ˆ]_

data. Thus, given any r, under the assumption that θj[∗][(0)][ is initialized via sampling][ ˆ]θw, then we have


P _∪j∈[m]_ _θj[∗][(0)][ −]_ _[θ][0]_ _≥_ _r_ _≤_ P _θj[∗][(0)][ −]_ _[θ][0]_ _≥_ _r_ _≤_ _m exp(−λnr[2]) + ϵ._
    _jX∈[m]_   

We proof by induction. Given any {θj}j[m]=1[, define]

_Rk,r = I_ _w_ supp(π) : arg min _θw_ _θ0_ _r_ _._
 _∈_ _j∈[m]_ _[L][w][(][θ][j][) =][ k][ and]_ _−_ _≤_ 

_n_ [ˆ]

Suppose at iteration t, we have _θk[∗][(][t][)][ −]_ _[θ][0][∥≤]_ _[cα][√]λ0[log]γ_ _[ n]_ for some constant c and λ0, which we
_∥_

denote as the minimum eigenvalue of _θ[L][∞][(][f][θ]0_ [)][. Now at iteration][ t][, we have two cases.]
_∇[2]_


-----

**Case 1: EπRk,** _γ_ Suppose that at iteration t, for k such that EπRk, _γ, and_ _θk[∗][(][t][)][ −]_ _[θ][0][∥]_ [=]
_∞_ _≥_ _∞_ _≥_ _∥_
_qk, we have the following property:_

2

_ϵt_
_θk[∗][(][t][ + 1)][ −]_ _[θ][0][∥][2][ =]_ _k[(][t][)][ −]_ Eπ [ _θ_ **_w(θk[∗][(][t][))][R][k,][∞][]][ −]_** _[θ][0]_
_∥_ EπRk, _∇_ _L_

_∞_

2ϵt
= _θ[θ]k[∗][∗][(][t][)][ −]_ _[θ][0][∥][2][ −]_ _θk[∗][(][t][)][ −]_ _[θ][0][,][ E][π]_ [[][∇][θ][L][w][(][θ]k[∗][(][t][))][R][k,][∞][]][⟩] [+][ ϵ][2]t [[][∇][θ][L][w][(][θ]k[∗][(][t][))][R][k,][∞][]][∥][2][ .]
_∥_ EπRk,∞ _⟨_ _[∥][E][π]_


Notice that


(1)
Eπ [ _θ_ **_w(θk[∗][(][t][))][R][k,q]k_** []] = Eπ _θ[L][w][(ˆ]θw)(θk[∗][(][t][)][ −]_ _θ[ˆ]w)Rk,qk_ + o _qk[2]_
_∇_ _L_ _∇[2]_

(2) h i   
= Eπ _θ[L][w][(ˆ]θw)(θk[∗][(][t][)][ −]_ _[θ][0][)][R][k,q]k_ + O _qk[2]_
_∇[2]_

(3) h i   
= Eπ _θ[L][w][(][θ][0][)(][θ]k[∗][(][t][)][ −]_ _[θ][0][)][R][k,q]k_ + O _qk[2]_
_∇[2]_

Here (1) is obtained via applying Taylor expansion on _∇θLw(θk[∗][(][t][))][ at][ ˆ]θw. (2) is by assumption _  1

and 2. (3) is by assumption 1. We thus have


_θk[∗][(][t][)][ −]_ _[θ][0][,][ E][π]_ [[][∇][θ][L][w][(][θ]k[∗][(][t][))][R][k,][∞][]][⟩]
_−⟨_
_θk[∗][(][t][)][ −]_ _[θ][0][,][ E][π]_ [[][∇][θ][L][w][(][θ]k[∗][(][t][))][R][k,q]k []][⟩] [+][ ∥][θ]k[∗][(][t][)][ −] _[θ][0][∥∥][E][π][∇][θ][L][w][(][θ]k[∗][(][t][))(1][ −]_ _[R][k,q]k_ [)][∥]
_≤−⟨_

_θk[∗][(][t][)][ −]_ _[θ][0][,][ E][π]_ [[][∇][θ][L][w][(][θ]k[∗][(][t][))][R][k,q]k []][⟩] [+][ cq][k] [exp(][−][λnq]k[2][)]
_≤−⟨_

_≤−_ EπRk,qk (θk[∗][(][t][)][ −] _[θ][0][)][⊤][∇]θ[2][L][w][(][θ][0][)(][θ]k[∗][(][t][)][ −]_ _[θ][0][) +][ cq][k]_ [exp(][−][λnq]k[2][) +][ O] _qk[3]_ _._

Notice that with sufficiently large n, with central limit theorem, we have   

_−_ EπRk,qk (θk[∗][(][t][)][ −] _[θ][0][)][⊤][∇]θ[2][L][w][(][θ][0][)(][θ]k[∗][(][t][)][ −]_ _[θ][0][)]_

_θk[∗][(][t][)][ −]_ _[θ][0][∥][2][ E][π]_ _θ[L][w][(][θ][0][)][ −∇]θ[2][L][w][(][θ][0][)]_ EπRk,qk (θk[∗][(][t][)][ −] _[θ][0][)][⊤][∇]θ[2][L][∞][(][θ][0][)(][θ]k[∗][(][t][)][ −]_ _[θ][0][)]_
_≤∥_ _∇[2]_ _−_

= − EπRk,qk (θk[∗][(][t][)][ −] _[θ][0][)][⊤][∇]θ[2][L][∞][(][θ][0][)(][θ]k[∗][(][t][)][ −]_ _[θ][0][) +][ O][(][n][−][1][/][2][)][.]_

This gives that


_θk[∗][(][t][)][ −]_ _[θ][0][,][ E][π]_ [[][∇][θ][L][w][(][θ]k[∗][(][t][))][R][k,][∞][]][⟩]
_−⟨_

EπRk,qk (θk[∗][(][t][)][ −] _[θ][0][)][⊤][∇]θ[2][L][∞][(][θ][0][)(][θ]k[∗][(][t][)][ −]_ _[θ][0][) +][ cq][k]_ [exp(][−][λnq]k[2][) +][ O] _qk[3]_ [+][ q][k][n][−][1][/][2][]
_≤−_


_λ0EπRk,qk_ _θk[∗][(][t][)][ −]_ _[θ][0][∥][2][ +][ cq][k]_ [exp(][−][λnq]k[2][) +][ O] _qk[3]_ [+][ q][k][n][−][1][/][2][] _._
_≤−_ _∥_


Use the above estimation, we have

_∥θk[∗][(][t][ + 1)][ −]_ _[θ][0][∥][2]_

EπRk,qk
_θk[∗][(][t][)][ −]_ _[θ][0][∥][2][ −]_ [2][ϵ][t][λ][min] _θk[∗][(][t][)][ −]_ _[θ][0][∥][2]_
_≤∥_ EπRk, _∥_
_∞_

+2cϵtqk exp(−λnqk[2][)][/][E][π][R][k,][∞] [+][ O] _ϵt(qk[3]_ [+][ q][k][n][−][1][/][2][)][/][E][π][R][k,][∞] [+][ ϵ]t[2]

_ϵt_  
_θk[∗][(][t][)][ −]_ _[θ][0][∥][2][ +]_ 2λ0EπRk,qk _θk[∗][(][t][)][ −]_ _[θ][0][∥][2][ −]_ [2][cq][k] [exp(][−][λnq]k[2][) +][ O] _qk[3]_ [+][ ϵ][t] [+][ q][k][n][−][1][/][2][]
_≤∥_ EπRk,∞ _−_ _∥_

 

Notice that by choosing α > 1/(2λ) and ϵt = O(n[−][1]), with sufficiently large n, when
p _cα_ logn n

_θk[∗][(][t][)][ −]_ _[θ][0][∥≥]_
_∥_ _λ0EqπRk,qk_

for some constant c, we have


_∥θk[∗][(][t][ + 1)][ −]_ _[θ][0][∥≤∥][θ]k[∗][(][t][)][ −]_ _[θ][0][∥]_ _[.]_

Thus ∥θk[∗][(][t][ + 1)][ −] _[θ][0][∥≤]_ _λcα0E[√]πR[log]k,n[ n]∞_ _[≤]_ _[cα][√]λ0[log]γn[ n]_ for some constant c.


-----

**Case 2: EπRk,** _γ_ In this case, we have
_∞_ _≤_

2
_θk[∗][(][t][ + 1)][ −]_ _[θ][0][∥][2][ =]_ _θk[∗][(][t][)][ −]_ _[ϵ][t][∇][θ][L][(][f]θk[∗][(][t][)][)][ −]_ _[θ][0]_
_∥_

2
= ∥θk[∗][(][t][)][ −] _[θ][0][∥][2][ −]_ [2][ϵ][t] _[⟨][θ]k[∗][(][t][)][ −]_ _[θ][0][,][ ∇][θ][L][(][θ]k[∗][(][t][))][⟩]_ [+][ ϵ][2]t _∇θL(fθk[∗][(][t][)][)]_ _._

Notice that
_−⟨θk[∗][(][t][)][ −]_ _[θ][0][,][ ∇][θ][L][(][θ]k[∗][(][t][))][⟩≤−]_ _θk[∗][(][t][)][ −]_ _[θ][0][,][ ∇][2]θ[L][(][θ][0][) (][θ]k[∗][(][t][)][ −]_ _[θ][0][)]_ _−⟨θk[∗][(][t][)][ −]_ _[θ][0][,][ ∇][θ][L][(][f][θ]0_ [)][⟩] [+][ o][(][||][θ]k[∗][(][t][)][ −] _[θ][0][||][3][)]_

= − (θk[∗][(][t][)][ −] _[θ][0][)][⊤]_ _[∇][2]θ[L][∞][(][θ][0][) (][θ]k[∗][(][t][)][ −]_ _[θ][0][) +][ o][(][||][θ]k[∗][(][t][)][ −]_ _[θ][0][||][3][) +][ O][p][(][n][−][1][/][2][)][||][θ]k[∗][(][t][)][ −]_ _[θ][0][||][.]_
This gives that


_∥θk[∗][(][t][ + 1)][ −]_ _[θ][0][∥][2][ ≤∥][θ]k[∗][(][t][)][ −]_ _[θ][0][∥][2][−][2][ϵ][t][λ][0]_ _[∥][θ]k[∗][(][t][)][ −]_ _[θ][0][∥][2][+][o][(][ϵ][t][||][θ]k[∗][(][t][)][−][θ][0][||][3][+][ϵ][2]t_ [)+][O][p][(][n][−][1][/][2][)][ϵ][t][||][θ]k[∗][(][t][)][−][θ][0][||][.]

With ϵt = O(n[−][1]) and sufficiently large n, when


log n

_n_

qλ0γ


_cα_
_∥θk[∗][(][t][)][ −]_ _[θ][0][∥≥]_


we have ∥θk[∗][(][t][ + 1)][ −] _[θ][0][∥][2][ ≤∥][θ]k[∗][(][t][)][ −]_ _[θ][0][∥][2][.]_

Combine this two cases, we conclude that _θk[∗][(][t][ + 1)][ −]_ _[θ][0][∥≤]_ _cα[√]λ0[log]γn[ n]_ for any t, when
_∥_

_θk[∗][(0)][ −]_ _[θ][0][∥≤]_ _cα[√]λ0[log]γn[ n]_ . We thus conclude that, for any α > 1/(2λ) and ϵ > 0, when n
_∥_

is sufficiently large, with probability at least 1 − _m exp_ _−λ_ _[cα]λ[2][2]0[ log][γ][2]_ _[ n]p_ _−_ _ϵ, we have_
 


log n

_n_

qλ0γ


_cα_
_θj[∗][(][t][)][ −]_ _[θ][0]_
_≤_


max
_j∈[m]_ [sup]t

A.3 PROOF FOR THEOREM 2


Notice that

_⊤_ 3

**_w(θj[∗][(][t][))][ −L][w][(ˆ]θw) = [1]_** _θj[∗][(][t][)][ −]_ _θ[ˆ]w_ _θ[L][w][(ˆ]θw)_ _θj[∗][(][t][)][ −]_ _θ[ˆ]w_ + O( _θj[∗][(][t][)][ −]_ _θ[ˆ]w_ )
_L_ 2 _∇[2]_

    3 2

_⊤_

= [1] _θj[∗][(][t][)][ −]_ _θ[ˆ]w_ _θ[L][w][(][θ][0][)]_ _θj[∗][(][t][)][ −]_ _θ[ˆ]w_ + O( _θj[∗][(][t][)][ −]_ _θ[ˆ]w_ ) + O( _θj[∗][(][t][)][ −]_ _θ[ˆ]w_ _θw_ _θ0_

2 _∇[2]_ _−_

 2   2 3

= [1] _θj[∗][(][t][)][ −]_ _θ[ˆ]w_ _θ[L][w][(][θ][0][)][ −∇]θ[2][L][∞][(][θ][0][)]_ _θj[∗][(][t][)][ −]_ _θ[ˆ]w_ + O( _θj[∗][(][t][)][ −]_ _θ[ˆ]w_ ) [ˆ]

2 _D_ [+] _∇[2]_

2
+ O( _θj[∗][(][t][)][ −]_ _θ[ˆ]w_ _θw −_ _θ0_ )

2

[ˆ]

Given w, we define uw = arg minj [m] _θj[∗][(][t][)][ −]_ _θ[ˆ]w_ 1/(2λ) and ϵ > 0, when n
_∈_ _D[. For any][ α >]_

is sufficiently large, with probability at least 1 − _m exp_ _−λ_ _[cα]λ[2][2]0[ log][γ][2]_ _[ n]_ p− _ϵ, we have_
 


1

min _θj[∗][(][t][)][ −]_ _θ[ˆ]w_

2 [E][w][∼][π] _j_ [m] _D_

 _∈_ 

2

= [1]2 [E][w][∼][π] _θu[∗]w_ _θw_ _D_

 _[−]_ [ˆ]  2

Ew _π_ **_w(θu[∗]w_** [)][ −L][w][(ˆ]θw) _c_ _θu[∗]w_ _θw_ _θw_ _θ0_ + _θ[L][w][(][θ][0][)][ −∇][2]θ[L][∞][(][θ][0][)]_ + _θu[∗]w_ _θw_
_≥_ _∼_ L _−_ _[−]_ [ˆ] [] _−_ _∇[2]_ _[−]_ [ˆ] []

_α√log n_
Ew _π_ **_w(θu[∗]w_** [)] Ew _π_ **_w(θ[ˆ]w)_** _c_ [ˆ]
_≥_ _∼_ _L_ _−_ _∼_ _L_ _−_ _λ0γn[3][/][2]_
  h i  _α√log n_

=Ew _π_ min _j_ [(][t][))] Ew _π_ **_w(θ[ˆ]w)_** _c_ _._
_∼_ j∈[m] _[L][w][(][θ][∗]_  _−_ _∼_ hL i _−_  _λ0γn[3][/][2]_ 


-----

**Algorithm 1 Ideal algorithm for centroid approximation with full-batch gradient used and wh**
updated every iteration.

1: Initialize θj[∗][(0)][,][ j][ ∈] [[][m][]][ by i.i.d. sampling from][ ρ][π][ or other distribution such as Gaussian.]

2: for t ∈ iterations do
3: _∀j ∈_ [m], calculate L(θj[∗][(][t][))][ defined in (][11][)]

4: Sample {wh}h[M]=1[, i.i.d. from][ π][.]

5: _∀h ∈_ [M ] and j ∈ [m], calculate Lwh (θj[∗][(][t][)) =][ w]h[T] **[L][(][θ]j[∗][(][t][))][.]**

6: _∀h ∈_ [M ], calculate uwh defined in (12) for each h.

7: _∀j ∈_ [m], update θj[∗] [by (][13][).]

8: end for


Similarly, we also have, with probability at least 1 − _m exp_ _−λ_ _[cα]λ[2][2]0[ log][γ][2]_ _[ n]_,
 

2 _α√log n_

Ew _π_ min _j_ [(][t][))] Ew _π_ **_w(θ[ˆ]w)_** min _θj[∗][(][t][)][ −]_ _θ[ˆ]w_ _c_ _._
_∼_ j∈[m] _[L][w][(][θ][∗]_ − _∼_ hL i _≥_ 2[1] [E][w][∼][π] j∈[m] _D−_  _λ0γn[3][/][2]_ 

Notice that the above bound holds uniformly for all j ∈ [m] and any iteration t, which implies that

with probability at least 1 − 2m exp _−λ_ _[cα]λ[2][2]0[ log][γ][2]_ _[ n]_ _−_ 2ϵ, we have
 

_α√log n_

sup _j_ [(][t][))]][ −] _[B][ −]_ [E][w][∼][π][[ min] _j_ [(][t][)][ −] _θ[ˆ]w_ _D[]][/][2]_ _._
_t≥0_ [E][w][∼][π][[ min]j∈[m] _[L][w][(][θ][∗]_ _j∈[m]_ _[||][θ][∗]_ _||[2]_ _[≤]_ _[c]_  _λ0γn[3][/][2]_ 

B ALGORITHM BOX

We provide pseudo algorithm for the ideal centroid approximation algorithm in Algorithm 1. In
practical implementation, we do not need to update wh every iteration and can also replace the
ˆ
full-batch gradient by stochastic gradient. Specifically, notice that _g(θj[∗][)][ defined in (][10][) can be]_
alternative represented as


_M_ _n_
_h=1_ _i=1_ [[][I][{][j][ ∈] _[u][w]h_ [(][t][)][}][]][ w][h,i][∇][θ][ℓ][(][x][i][, f][θ]j[∗][(][t][)][)][/n]

_gˆ(θj[∗][(][t][)) =]_ _M_ = n[1]

P P _h=1_ [[][I][{][j][ ∈] _[u][w]h_ _[}][]]_

P

where qi,j is defined by


_qi,j∇θℓ(xi, fθj[∗][(][t][)][)][,]_
_i=1_

X

(14)


_M_ _n_
_h=1_ _i=1_ [[][I][{][j][ ∈] _[u][w]h_ [(][t][)][}][]][ w][h,i]
_qij :=_ _M_ _._ (15)
P Ph=1 [[][I][{][j][ ∈] _[u][w]h_ [(][t][)][}][]]

This allows us to use a stochastic gradient version of gradientP


_gˆsgd(θj[∗][) =]_


_qi,j∇θℓ(xi, fθj[∗]_ [)][,] (16)
_iX∈[B]_


_|B|_


where B is the set of mini-batch data. The detailed algorithm is summarized in Algorithm

C ADDITIONAL EXPERIMENT DETAILS

C.1 BOOTSTRAP CONFIDENCE INTERVAL

Given a model fθ parameterized by θ and a training set with n data points i.i.d. sampled from
population, our goal is to construct confidence interval for θ. Let ˜ρπ be an empirical distribution
approximating ρπ, which could be obtained by i.i.d. sampling, or by our centroid method. Denote
by Q[α, ˜ρπ] the α-quantile function of ˜ρπ with some α ∈ [0, 1]. We consider the following three
ways to construct (two-sided) bootstrap confidence interval of θ with confidence level α: the Normal
interval, the percentile interval and the pivotal interval which are defined below.


-----

**Algorithm 2 Practical implementation of centroid approximation with less frequent updating of wh**
and stochastic gradient enabled.

1: Initialize θj[∗][(0)][,][ j][ ∈] [[][m][]][ by i.i.d. sampling from][ ρ][π][ or other distribution such as Gaussian.]

2: for t ∈ iterations do
3: // Update wh only every a few iterations.

4: **if t mod freq == 0 then**

5: _∀j ∈_ [m], calculate L(θj[∗][(][t][))][ defined in (][11][)]

6: Sample {wh}h[M]=1[, i.i.d. from][ π][.]

7: _∀h ∈_ [M ] and j ∈ [m], calculate Lwh (θj[∗][(][t][)) =][ w]h[T] **[L][(][θ]j[∗][(][t][))][.]**

8: _h_ [M ], calculate uwh (t) defined in (12) for each h.
_∀_ _∈_

9: **else**

10: _uwh_ (t) = uwh (t 1)
_−_

11: **end if**

12: _∀j ∈_ [m], update θj[∗][(][t][)][ by (][13][). (May use mini-batch gradient defined in (][16][)).]

13: end for


**Methods to construct confidence interval** The methods we used to construct confidence interval
are – The Normal interval:

[θ[ˆ] − _z((1 + α)/2) ˆseboot,_ _θ[ˆ] + z((1 + α)/2) ˆseboot],_

where z(·) is the inverse cumulative distribution function of standard Normal distribution. And ˆseboot
is the standard deviation estimated from ˜ρπ.

– The percentile intervals:

[Q[(1 _α)/2, ˜ρπ], Q[(1 + α)/2, ˜ρπ]]._
_−_

– The pivotal interval:

[2θ[ˆ] _Q[(1 + α)/2, ˜ρπ], 2θ[ˆ]_ _Q[(1_ _α)/2, ˜ρπ]]._
_−_ _−_ _−_

We consider the following simple linear regression: x ∼N (0, I), y | x ∼N (θ[⊤]x, I), where the
features x ∈ R[4] and we set the true parameter to be θ0 = [1, −1, 1, −1]. We consider n = 50 and the
number of particles m = 20, 50, 100, 200. We compare the coverage probability and the confidence
level α to measure the quality:

**Measuring the quality of confidence interval** With a large number N of independently generated
training data (we use N = 1000), we are able to obtain the corresponding confidence intervals
_{CI(α)s}s[N]=1[and thus obtain the probability that the true parameter falls into the confidence intervals,]_
which is the estimated coverage probability


_αˆ = [1]_


_s=1_ I{θ0 ∈ CI(α)s}.

X


A good confidence interval should have ˆα close to α. Thus we measure the performance by calculating
the difference |α − _αˆ|._

As _θ[ˆ]w is the least square estimator of the bootstrapped dataset, it has analytic solution and thus can_
be obtained via some matrix multiplications. θw[∗] [is initialized using][ ˆ]θw and then updated for 2000
steps. For this experiment, we find that adding the threshold γ does not gives further improvement
for this experiment and thus we simply set γ = 0 and use M = 1. We approximate the true bootstrap
distribution by sampling 10000 i.i.d. samples.

**More experiment result** Table 3 all the result we have varying α = 0.8, 0.9, 0.95, m =
20, 50, 100, 200 and three different approaches for constructing confidence interval. As we can
see, centroid approximation gives the best performance in most cases compared with the other three
baselines.


-----

|Num Particle|Col2|Col3|20 50 100 200|
|---|---|---|---|
|α = 0.8|Normal|Bootstrap Bayesian Residual Centroid|0.033 ± 0.013 0.028 ± 0.013 0.026 ± 0.013 0.031 ± 0.013 0.084 ± 0.014 0.076 ± 0.014 0.082 ± 0..014 0.086 ± 0.014 0.033 ± 0.013 0.037 ± 0.013 0.029 ± 0.013 0.024 ± 0.013 0.036 ± 0.013 0.003 ± 0.012 0.017 ± 0.013 0.030 ± 0.013|
||Percentile|Bootstrap Bayesian Residual Centroid|0.096 ± 0.014 0.050 ± 0.014 0.044 ± 0.013 0.024 ± 0.013 0.114 ± 0.015 0.079 ± 0.014 0.074 ± 0.014 0.071 ± 0.014 0.079 ± 0.014 0.032 ± 0.013 0.017 ± 0.013 0.010 ± 0.013 0.066 ± 0.014 0.008 ± 0.013 0.019 ± 0.013 0.020 ± 0.013|
||Pivotal|Bootstrap Bayesian Residual Centroid|0.101 ± 0.015 0.053 ± 0.014 0.045 ± 0.014 0.033 ± 0.013 0.158 ± 0.015 0.110 ± 0.110 0.088 ± 0.014 0.078 ± 0.014 0.087 ± 0.014 0.044 ± 0.013 0.030 ± 0.013 0.023 ± 0.013 0.026 ± 0.013 0.030 ± 0.012 0.018 ± 0.013 0.030 ± 0.013|
|α = 0.9|Normal|Bootstrap Bayesian Residual Centroid|0.029 ± 0.010 0.031 ± 0.011 0.021 ± 0.010 0.017 ± 0.010 0.076 ± 0.012 0.054 ± 0.011 0.048 ± 0.011 0.045 ± 0.011 0.043 ± 0.011 0.023 ± 0.010 0.025 ± 0.010 0.020 ± 0.010 0.027 ± 0.010 0.001 ± 0.009 0.012 ± 0.010 0.016 ± 0.010|
||Percentile|Bootstrap Bayesian Residual Centroid|0.101 ± 0.013 0.036 ± 0.011 0.021 ± 0.010 0.014 ± 0.010 0.129 ± 0.013 0.077 ± 0.012 0.059 ± 0.012 0.054 ± 0.011 0.098 ± 0.013 0.030 ± 0.011 0.033 ± 0.011 0.025 ± 0.010 0.081 ± 0.012 0.021 ± 0.010 0.020 ± 0.010 0.015 ± 0.010|
||Pivotal|Bootstrap Bayesian Residual Centroid|0.106 ± 0.013 0.045 ± 0.011 0.025 ± 0.010 0.023 ± 0.010 0.149 ± 0.014 0.093 ± 0.012 0.073 ± 0.012 0.056 ± 0.011 0.100 ± 0.013 0.044 ± 0.011 0.030 ± 0.011 0.023 ± 0.010 0.046 ± 0.011 0.013 ± 0.009 0.011 ± 0.010 0.020 ± 0.010|
|α = 0.95|Normal|Bootstrap Bayesian Residual Centroid|0.018 ± 0.008 0.014 ± 0.008 0.012 ± 0.008 0.006 ± 0.007 0.053 ± 0.010 0.038 ± 0.009 0.031 ± 0.009 0.037 ± 0.009 0.036 ± 0.009 0.019 ± 0.008 0.011 ± 0.008 0.008 ± 0.007 0.018 ± 0.008 0.005 ± 0.006 0.009 ± 0.007 0.005 ± 0.007|
||Percentile|Bootstrap Bayesian Residual Centroid|0.081 ± 0.010 0.047 ± 0.009 0.030 ± 0.008 0.017 ± 0.008 0.126 ± 0.012 0.072 ± 0.010 0.056 ± 0.010 0.042 ± 0.009 0.100 ± 0.011 0.040 ± 0.009 0.037 ± 0.009 0.021 ± 0.008 0.077 ± 0.010 0.029 ± 0.008 0.020 ± 0.008 0.016 ± 0.008|
||Pivotal|Bootstrap Bayesian Residual Centroid|0.089 ± 0.011 0.043 ± 0.009 0.027 ± 0.008 0.015 ± 0.008 0.127 ± 0.012 0.091 ± 0.011 0.064 ± 0.010 0.056 ± 0.010 0.085 ± 0.011 0.051 ± 0.009 0.036 ± 0.009 0.029 ± 0.008 0.046 ± 0.009 0.002 ± 0.007 0.014 ± 0.008 0.009 ± 0.007|


Table 3: Complete result on comparing centroid approximation with various bootstrap methods. The
bold number shows the best approach.


-----

**Algorithm 3 Algorithm for Centroid Approximation Applied to Contextual Bandit.**

1: Obtain a randomly initialized θj[∗][(0)][,][ j][ ∈] [[][m][]][.]

2: Initialize a common replay buffer Rc = ∅ recording all the observed contexts.
3: For each model, initialize its own replay buffer Rj = ∅ that is used for training.
4: for t ∈ number of total steps do
5: Obtain the t-th context xt.

6: Sampling one model based on probability {vj[∗][(][t][)][}]j[m]=1 [to make action][ a][t][ and get reward][ r][t][.]

7: Update the common replay buffer by Rc _Rc_ (xt, at, rt)

8: // Update wh and Rj and model every a few iterations. ← _∪{_ _}_

9: **if t mod freq == 0 then**

10: _∀j ∈_ [m], calculate L(θj[∗][(][t][))][ defined in (][11][) for all the contexts in][ R][c][. //][ L][(][θ]j[∗][(][t][))][ ∈] [R][|][Rc] _[|][.]_

11: Generate M sets of random weights {wh}h[M]=1 [of contexts in][ R][c][ from][ π][.]

12: _∀h ∈_ [M ] and j ∈ [m], calculate Lwh (θj[∗][(][t][)) =][ w]h[T] **[L][(][θ]j[∗][(][t][))][.]**

13: _h_ [M ], calculate uwh (t) defined in (12) for each h.
_∀_ _∈_

14: _∀i ∈_ [|Rc|] and j ∈ [m], calculate qi,j by (15)

15: _∀j ∈_ [m], update vj[∗][(][t][)][ based on (][7][).]

16: _∀j ∈_ [m], if vj[∗][(][t][)][ ≤] _[γ][, construct][ R][j][ =][ R][c][, else, construct][ R][j][ by sample][ |][R][c][|][ contexts]_

in Rc. The probability that context i is being sampled is qi,j/ _i=1_ _[q][i,j][.]_

17: _∀j ∈_ [m], train model j using the data in Rj for several iterations.

18: **end if**

[P][|][R][c][|]

19: end for


C.2 CENTROID APPROXIMATION FOR BOOTSTRAP METHOD IN CONTEXTUAL BANDIT

C.2.1 MORE BACKGROUND

Contextual bandit is a classic task in sequential decision making problem in which at time t = 1, ..., n,
a new context xt arrives and is observed by an algorithm. Based on its internal model, the algorithm
selects an actions at and receives a reward rt(xt, at) related to the context and action. During this
process, the algorithm may update its internal model with the newly received data. At the end of
this process, the cumulative reward of the algorithm is calculated by r = _t=1_ _[r][t][ and the goal for]_
the algorithm is to improve the cumulative reward r. The exploration-exploitation dilemma is a
fundamental aspect in sequential decision making problem such as contextual bandit: the algorithm

[P][n]
needs to trade-off between the best expected action returned by the internal model at the moment
(i.e., exploitation) with potentially sub-optimal exploratory actions. Thompson sampling (Thompson,
1933; Wyatt, 1998; May et al., 2012) is an elegant and effective approach to tackle the explorationexploitation dilemma using the model uncertainty, which can be approached with various methods
including Bayesian posterior (Graves, 2011; Welling & Teh, 2011), dropout uncertainty (Srivastava
et al., 2014; Hron et al., 2017) and Bootstrap (Osband et al., 2016; Hao et al., 2019). The ability
to accurately assess the uncertainty is a key to improve the cumulative reward. Bootstrap method
for contextual bandit maintains m bootstrap models trained with different bootstrapped training set.
When conducting an action, the algorithm uniformly samples a model and then selects the best action
returned by the sampled model.

C.2.2 MORE EXPERIMENT SETUP DETAILS

We set all the experimental setting including data preprocessing, network architecture and training
pipeline exactly the same as the one used in Riquelme et al. (2018) and adopt its open source code
repository.

**Network architecture Following Riquelme et al. (2018), we consider a fully connected feed forward**
network with two hidden layers with 50 hidden units and ReLU activations. The input and output
dimensions depends on the dimension of context and number of possible actions.

**Training For each dataset, we randomly generate 2000 contexts, and for each algorithm, we update**
the replay memory buffer for each model every 50 contexts, and each model is also updated every
50 contexts. For the standard bootstrap, when updating the replay buffer of each model, we sample
50 i.i.d. contexts with uniform probability from the latest 50 contexts (each model have different


-----

realizations of the samples) and add the newly sampled contexts to each model’s replay buffer. For
the centroid approximation, we update the replay buffer of each model by applying resampling on all
the observed contexts up to the current steps. The resampling probability of each context for each
model is different and determined by the algorithm. We refer readers to Algorithm 3 for the detailed
procedures. Here we choose freq = 50 and M = 100. When at model updating, each model is trained
for 100 iterations with batch size 512 using the data from its replay buffer. Following Riquelme et al.
(2018), we use RMSprop optimizer with learning rate 0.1 for optimizing. When making actions,
we sample the prediction head according to vk[∗][(][t][)][ obtained using the examples between the last two]
model updates.

Notice that in the implementation, we only need to maintain one common replay buffer and the
replay buffer for each model can be implemented by maintaining the number of each context. Thus
when sampling batches of context, we simply need to sample the index of the context and refer to the
common replay buffer to get the actual data.

C.3 CENTROID APPROXIMATION FOR BOOTSTRAP DQN

C.3.1 MORE BACKGROUND

Similar to the bootstrap method in contextual bandit problem, Bootstrap DQN explores using the
model uncertainty, which can be assessed via maintaining several models trained with bootstrapped
training set. Maintaining several independent models can be very expensive in RL and to reduce the
computational cost, Bootstrap DQN uses a multi-head network with a shared base. Each head in the
network corresponds to a bootstrap model and the common shared base is thus trained via the union
of the bootstrap training set of each head. We train the Bootstrap DQN with standard updating rule
for DQN and use Double-DQN (Van Hasselt et al., 2016) to reduce the overestimate issue. Notice
that our centroid approximation method only changes the memory buffer for each head and thus
introduces no conflict to other possible techniques that can be applied to Bootstrap DQN.

C.3.2 MORE EXPERIMENT SETUP DETAILS

**Network Architecture Following Osband et al. (2016), we considered multi-head network structure**
with a shared base layer to save the memory. Specifically, we use a fully connected layer with 256
hidden neurons as the shared base and stack two fully connected layers each with 256 hidden neurons
as head. Each head in the model can be viewed as one bootstrap particles and in computation, all the
bootstrap particles use the same base layer.

**Training and Evaluation For LunarLander-v2, we train the model for 450 episodes with the first**
50 episodes used to initialize the common memory buffer. The maximum number of steps within
each episode is set to 1000 and we report the moving average reward with window width 100. For
Catcher-v0, we train the model for 100 episodes with the first 10 episodes used to initialize the
common memory buffer. We set the maximum number of steps within each episodes 2000 and report
the moving average reward with window width 25.

For training the Bootstrap DQN, given the current state xt, we sample one particle based on _vj[∗]_ _j=1_
and use its policy network to make an action at and get the reward rt and next state xt {+1. The[}][m]
Q-value of the state action pair Q(xt, at) is estimated by rt + λ ∗ _Q[ˆ](xt+1), where_ _Q[ˆ](xt+1) is the_
predicted state value by the target network of the sampled particle and λ is the discount factor set
to be 0.99. At each step, the policy network of all particles are updated using one step gradient
descent with Adam optimizer (β = (0.9, 0.999) and learning rate 0.001) and mini-batch data (size
64) sampled from its replay buffer. We update target model, each particle’s replay buffer and vj[∗][s]
every 1000 steps for LunarLander-v2 and every 200 steps for Catcher-v0. The update scheme for
replay buffers of each particles and vj[∗][s is the same as the one in contextual bandit experiment. As]
the model see significantly larger number of contexts than that in the contextual bandit experiment, to
reduce the memory consumption, we set the max capacity of the common replay buffer to 50000
(the oldest data point will be pop out when the size reaches maximum and new data comes in). For
training the shared base, following Osband et al. (2016), we adds up all the gradient comes from each
head and normalizes it by the number of heads. Algorithm 4 summarizes the whole training pipeline.


-----

**Algorithm 4 Algorithm for Centroid Approximation Applied to DQN.**

1: Obtain a randomly initialized θj[∗][(0)][,][ j][ ∈] [[][m][]][. (For the][ j][-th particle, both of its target and policy]
network use the same initialization.)

2: Initialize a common replay buffer Rc = ∅ recording all the observed contexts.
3: For each head, initialize its own replay buffer Rj = ∅ that is used for training.
4: for t ∈ number of total episodes do
5: **while not at terminal state and the number of steps does not exceed the threshold do**

6: Obtain the t-th context xt.

7: Sample an head based on _vj[∗]_
and the prediction of the corresponding target network. { _[}][ to make action][ a][t][ and get][ Q][(][x][t][, a][t][)][ using the reward][ r][t]_

8: Update the common replay buffer by Rc _Rc_ (xt, Q(xt, at))

9: _∀j ∈_ [m], update its policy network by one step gradient descent using the data from its ← _∪{_ _}_
reply buffer.

10: // Update wh and Rj and target network every a few iterations.

11: **if t mod freq == 0 then**

12: _∀j ∈_ [m], calculate L(θj[∗][(][t][))][ defined in (][11][) for all the contexts in][ R][c][.]

13: Generate M sets of random weights {wh}h[M]=1 [of contexts in][ R][c][ from][ π][.]

14: _∀h ∈_ [M ] and j ∈ [m], calculate Lwh (θj[∗][(][t][)) =][ w]h[T] **[L][(][θ]j[∗][(][t][))][.]**

15: _h_ [M ], calculate uwh (t) defined in (12) for each h.
_∀_ _∈_

16: _∀i ∈_ [|Rc|] and j ∈ [m], calculate qi,j by (15)

17: _∀j ∈_ [m], update vj[∗][(][t][)][ based on (][7][).]

18: _∀j ∈_ [m], if vj[∗][(][t][)][ ≤] _[γ][, construct][ R][j][ =][ R][c][, else, construct][ R][j][ by sample][ |][R][c][|]_

contexts in Rc. The probability that context i is being sampled is qi,j/ _i=1_ _[q][i,j][.]_

19: _∀j ∈_ [m], update the j-th target network by loading the weights of the j-th policy
network.

[P][|][R][c][|]

20: **end if**

21: **end while**

22: end for


76

75

74

73

72

71


92.5

92.0

91.5

91.0


10


10

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|
|---|---|---|---|---|---|---|---|---|---|
|||||||||||
|||||||||||
|||||||Ce|ntro|id||
|||||||||||
|||||||Bo|otst|rap||
|||||||||||

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|
|---|---|---|---|---|---|---|---|---|---|
|||||||||||
|||||||||||
|||||||Ce|ntro|id||
|||||||||||
|||||||Bo|otst|rap||
|||||||||||


Num particles


Num particles


Figure 4: Results on ensemble modeling with bootstrap using Vgg16 on CIFAR-100.

C.4 BOOTSTRAP ENSEMBLE MODEL


Ensemble of deep neural networks have been successfully used to boost predictive performance
(Lakshminarayanan et al., 2017). In this experiment, we consider using an ensemble of deep neural
network trained on different bootstrapped training set, which is also known as a popular strategy
called bagging.

We consider image classification task on CIFAR-100 and use standard VGG-16 (Simonyan &
Zisserman, 2014) with batch normalization. We apply a standard training pipeline. We train the
bootstrap model for 160 epochs using SGD optimizer with 0.9 momentum and batchsize 128. The
learning rate is initialized to be 0.1 and is decayed by a factor of 10 at epoch 80 and 120. We start
to apply the centroid approximation at epoch 120 (thus the centroid is initialized with 120 epochs’
training). We generate the bootstrap training set for each centroid every epoch using the proposed
centroid approximation method. We consider m = 3, 4, 5, 10 ensembles and use γ = 0.5/m. We


-----

|#Particle|γ = 0 γ = 0.5 γ = m|
|---|---|
|3 4 5 10|3480.0 120 3702.7 89.8 3467.7 115 ± ± ± 3461.92 126 3723.1 78.7 3600.0 69.3 ± ± ± 3586.5 64.5 3799.6 84.2 3647.3 64.5 ± ± ± 3785.0 59.1 3796.9 36.1 3742.7 86.8 ± ± ±|


Table 4: Ablation study.

repeat the experiment for 3 random trials and report the averaged top1 and top5 accuracy with the
standard deviation of the mean estimator. Algorithm

Figure 4 summarizes the result. Overall, increasing m is able to improve the predictive performance
and with the same number of models, our centroid approximation consistently improves over standard
bootstrap ensembles.

C.5 ABLATION STUDY

We study the effectiveness of using (8) to modify the gradient of centroid with vk[∗][(][t][)][ ≤] _[γ][. We]_
consider the setting γ = 0 (no modification) and γ = m (always modify, equivalent to no bootstrap
uncertainty) and applied the method on the mushroom dataset in the contextual bandit problem. Table
4 shows that (i) modifying the gradient of centroid with small vk[∗][(][t][)][ using do improve the overall]
result; (2) bootstrap uncertainty is important for exploration.

C.6 COMPUTATION OVERHEAD

Our main goal is not to decrease the training cost but improve the quality of bootstrap partical
distribution so that we can use less models at deployment and hence reduce the memory cost and
the computational cost for inference. Actually, as discussed in Section 3, our method actually only
introduces a little computation overhead while much improves the quality of the particles, which
is another advantage of our method. For example, in bandit problem on mushroom dataset, when
_m = 3, 10, vanilla bootstrap takes 33s, 101s while our approach takes 35s, 108s per run. For the_
bagging, when m = 3, 10, vanilla bootstrap takes 11200s, 34240s while ours take 12000s, 36200s.
Results are based on the average of 3 runs. Our method only introduces about 7% computational
overhead even with an naive implementation.


-----

