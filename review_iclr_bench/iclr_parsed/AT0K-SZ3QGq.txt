# ACHIEVING PERSONALIZED FEDERATED LEARNING
## WITH SPARSE LOCAL MODELS

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Federated learning (FL) is vulnerable to heterogeneously distributed data, since
a common global model in FL may not adapt to the heterogeneous data distribution of each user. To counter this issue, personalized FL (PFL) was proposed
to produce dedicated local models for each individual user. However, PFL is far
from its maturity, because existing PFL solutions either demonstrate unsatisfactory generalization towards different model architectures or cost enormous extra
computation and memory. In this work, we propose federated learning with personalized sparse mask (FedSpa), a novel PFL scheme that employs personalized
sparse masks to customize sparse local models on the edge. Instead of training an
intact (or dense) PFL model, FedSpa only maintains a fixed number of active parameters throughout training (aka sparse-to-sparse training), which enables users’
models to achieve personalization with cheap communication, computation, and
memory cost. We theoretically show that with the rise of data heterogeneity, setting a higher sparsity of FedSpa may potentially result in a better convergence
on its personalized models, which also coincides with our empirical observations.
Comprehensive experiments demonstrate that FedSpa significantly saves communication and computation costs, while simultaneously achieves higher model accuracy and faster convergence speed against several state-of-the-art PFL methods.

1 INTRODUCTION

Data privacy raises increasingly intensive concerns, and
governments have enacted legislation to regulate the privacy
intrusion behavior of mobile users, e.g., the General Data
Protection Regulation (Voigt & Von dem Bussche, 2017).
Traditional distributed learning approaches, requiring massive users’ data to be collected and transmitted to a central
server for training model, soon may no longer be realistic
under the increasingly stringent regulations on users’ private
data. On this ground, federated learning (FL), a distributed
training paradigm emerges as a successful solution to cope
with privacy concerns, which allows multiple clients to per- Figure 1: Performance of FedSpa and
form model training within the local device without the ne- several baselines w.r.t. communication

cost in Non-IID setting. Numbers above

cessity to exchange the data to other entities. In this way, the

FedSpa and Sub-FedAvg are sparsity.

data privacy leakage problem could be potentially relieved.
Despite the promising prospect, several notorious issues are afflicting practical performance of FL:

-  The global model produced by weight average (or FedAvg and its non-personalized variants)
_exhibits unsatisfactory performance in a Non-IID data distribution setting. To alleviate this prob-_
lem, the most popular idea is to integrate personalized features into the global model, and produce
dedicated model for each local distribution. However, how to make this integration is an open
problem that remains unresolved. Prior works on personalized FL (PFL) zero in this issue, but the
existing methods either demonstrate weak generalization towards different model architectures
(Arivazhagan et al., 2019), or require extra computation and storage (Li et al., 2021).

-  The communication and training overhead is prohibitively high for both the FL and PFL. Clients
in FL/PFL responsible for model training are mostly edge-devices with limited computation capacity and low bandwidth, and may not be powerful enough to fulfill a modern machine learning


-----

task with large deep neural networks. Existing studies (Li et al., 2020; Vahidian et al., 2021) integrate model compression into FL/PFL to save communication and computation overhead. However, both methods embrace the technique of dense-to-sparse training, which still requires a large
amount of communication at the beginning of training. In addition, how to effectively aggregate
the dynamic sparse models is another challenging problem that remains unresolved.

In this work, we propose FedSpa (see Figure 2), which **Parameter Server**
model, but allows each client to own its unique sparsemodel masked by a personalized mask, which success-has two key features to counter the above two chal-lenges: (i) FedSpa does not deploy a single global 𝑤𝑤𝑤𝑤2534 𝑤𝑤16  **Aggregation & Apply** 00𝑤𝑤𝑤052′′3′((𝑤𝑤𝑤𝑤𝑤1𝑤′0652′′′0-3′6-′ 𝑤𝑤𝑤061)1)′
fully alleviates the Non-IID challenge. (ii) FedSpa al- **Dense global model** **Sparse local updates**
lows each client to train over an evolutionary sparse  **Mask & distribute sparse models…**  **Upload local updates & next masks**
erated training process, which consistently alleviatesmodel with constant sparsity[1] throughout the whole fed- Client KClient K-1Client K-20 0𝑤3𝑤0𝑤01 3𝑤1 𝑤1 0 0𝑤𝑤3′10′𝑤0𝑤3′1′ 𝑤1′
the computation overhead of clients. Besides, all thelocal models in FedSpa are sparse models, which re- 𝑤𝑤053𝑤05𝑤𝑤6 5𝑤6 𝑤6  **Local Training** **Local Training** **Local Training𝑤𝑤05′** 3′𝑤05𝑤′ 𝑤6′ 5𝑤′ 6′ 𝑤6′  **next maskProduce next maskProduce next maskProduce**
quires a smaller amount of communication cost in each
communication round. Theoretically, we conclude that

**Parameter Server**

𝑤𝑤𝑤𝑤2534 𝑤𝑤16  **Aggregation & Apply** 00𝑤𝑤𝑤052′′3′((𝑤𝑤𝑤𝑤𝑤1𝑤′0652′′′0-3′6-′ 𝑤𝑤𝑤061)1)′

**Dense global model** **Sparse local updates**

 **Mask & distribute sparse models…**  **Upload local updates & next masks**

Client KClient K-1Client K-20 0𝑤3𝑤0𝑤01 3𝑤1 𝑤1 0 0𝑤𝑤3′10′𝑤0𝑤3′1′ 𝑤1′

𝑤𝑤053𝑤05𝑤𝑤6 5𝑤6 𝑤6  **Local Training** **Local Training** **Local Training𝑤𝑤05′** 3′𝑤05𝑤′ 𝑤6′ 5𝑤′ 6′ 𝑤6′  **next maskProduce next maskProduce next maskProduce**

with the rise of Non-IID extent, setting a higher spar
Figure 2: Overview of FedSpa. Firstly,

sity may result in a better convergence on the personal
the server masks and distributes the

ized models of FedSpa. Empirically, in the Non-IID set
sparse weights. Secondly, clients do lo
ting, we demonstrate that FedSpa accelerates the con- cal training on a constantly sparse model.
vergence (respectively 76.2% and 38.1% less commu
Thirdly, clients upload the sparse updates

nication rounds to reach the best accuracy of FedAvg

and being aggregated by the server.

(McMahan et al., 2016) and Ditto (Li et al., 2021)), in**creases the final accuracy (up to 21.9% and 4.4% higher accuracy than FedAvg and Ditto, respec-**
tively), reduces the communication overhead (50% less parameters communicated than the dense
solutions), and lowers the computation (15.3% lower floating-point operations (FLOPs) than algorithms trained with fully dense model). To the end, we summarize our contribution as:

-  We present a novel formulation of the sparse personalized FL (SPFL) problem, which can be
applied to various network architectures by enforcing personalized sparse masks to a global model.

-  We propose a solution dubbed as FedSpa to solve the SPFL problem. By our novel design, FedSpa
reduces the communication and computation overhead of the general FL solution.

-  Two sparse-to-sparse mask searching techniques are integrated as plugins of our solution. To
adapt our PFL training context, we modify the DST-based mask searching technique to enable a
warm-start of the searching process, which achieves superior performance.

-  We theoretically present the convergence analysis of the personalized models. Experimental results demonstrate the superiority of FedSpa and also coincides with the theoretical conclusion –
with the rise of data heterogeneity, setting a higher sparsity of FedSpa may potentially result in a
better convergence on its personalized models.

2 RELATED WORKS

Federated learning (FL) (McMahan et al., 2016) is seriously afflicted by the issue of heterogeneously
distributed (or Non-IID) data. Personalized FL (PFL), initiated by recent literature (Li et al., 2021;
Arivazhagan et al., 2019), is shown to be effective to counter this issue of FL. In this work, we
propose an alternative yet effective way to enhance PFL with personalized sparse models.

2.1 PERSONALIZED FEDERATED LEARNING

We categorize PFL into five genres. Firstly, PFL via layer partition, e.g., FedPer (Arivazhagan
et al., 2019), LG-FedAvg (Liang et al., 2020), FedRep (Collins et al., 2021), is to divide the global
model layers into shared layers and personalized layers. For the shared layers, weights average as
in FedAvg is adopted, while for personalized layers, models are trained only locally and will not

1Sparsity specifies the ratio of parameters that are set to 0 (or inactive) in a model.


-----

be exchanged with others. Secondly, PFL via regularization, e.g., Ditto (Li et al., 2021), L2GD
(Hanzely & Richt´arik, 2020) is to add a proximal term on the local model to force the local model
and global model closely in the local model fine-tuning stage. Similarly, Sarcheshmehpour et al.
(2021); SarcheshmehPour et al. (2021) propose a total variation (TV) regularization to form the
network lasso (nLasso) problem, and primal-dual methods adapted from (Jung, 2020) are proposed
to solve the nLasso problems. Thirdly, PFL via model interpolation, e.g., MAPPER (Mansour et al.,
2020), APFL (Deng et al., 2020) achieves personalization by linearly interpolating the weights of
the cluster (global) model and local model as the personalized model. Fourthly, PFL via transfer
learning, e.g., FedMD (Li & Wang, 2019), FedSteg (Yang et al., 2020), and Fedhealth (Chen et al.,
2020), is to either use model and domain-specific local fine-tuning or knowledge distillation to adapt
the global model into the personalized model. Finally, PFL via model compression, e.g., LotteryFL
(Li et al., 2020) and Sub-FedAvg (Vahidian et al., 2021), achieves personalization via employing
principle model compression techniques, such as weight pruning and channel pruning, over the
shared global model.

2.2 SPARSE DEEP NEURAL NETWORKS

Methods to Sparsify neural networks can be classified into two genres: dense-to-sparse methods
and sparse-to-sparse methods. Dense-to-sparse methods train from a dense model, and compress
the model along the training process. Iterative pruning, first proposed by Frankle & Carbin (2018),
shows promising performance in dynamically searching for a sparse yet accurate network. Recently,
sparse-to-sparse methods have been proposed to pursue training efficiency. Among them, dynamic
sparse training (DST) (Bellec et al., 2018; Evci et al., 2020; Liu et al., 2021) is the most successful technique that allows sparse networks, trained from scratch, to match the performance of their
dense equivalents. Stemming from the first work – sparse evolutionary training (Mocanu et al.,
2018; Liu et al., 2020), DST has evolved as a class of sparse training methods absorbing many advanced techniques, e.g., weight redistribution (Mostafa & Wang, 2019; Dettmers & Zettlemoyer,
2019), gradient-based regrowth (Dettmers & Zettlemoyer, 2019; Evci et al., 2020), and extra weight
exploration (Jayakumar et al., 2020; Liu et al., 2021).

Our work also achieves personalization via model compression. We emphasize that three main
progresses are made towards SOTA compression-based PFL: (i) We rigorously formulate the sparse
personalized FL problem, filling the gap left by the prior works. (ii) While prior works either vaguely
describe their model aggregation as ”aggregating the Lottery Ticket Network via FedAvg” (Li et al.,
2020), or ”taking the average on the intersection of unpruned parameters in the network” (Vahidian
et al., 2021), we explicitly formulate the aggregation as averaging the sparse update from clients.
(iii) Both the two prominent prior works use the idea of iterative pruning to prune the network
from dense to sparse. We instead provide two sparse-to-sparse training alternatives to plug in our
solution, which largely reduces the costs of communication at the beginning of the training process,
and exhibits remarkable performance.

3 PROBLEM FORMULATION

We assume a total number of K clients within our FL system, and we consistently use k to index a
specific client. First, we give a preliminary introduction on the general FL and PFL problem.

**General FL problem. Let w ∈** R[d] be the global weight. General FL takes the formulation as below


(P1) min _f˜(w) = [1]_


_F˜k(w) := E[_ (x,y) _k_ (w; (x, y))]
_L_ _∼D_


_k=1_


whereis uniformly sampled according to distribution D = D1 ∪· · ·∪DK is the joint distribution of Dk wrt. client k local heterogeneous distributions, data k, and L(·; ·) is the loss function. (x, y)

**Ultimate PFL Problem. Let {wk} be the personalized models. The ultimate PFL is defined as:**


(P2) min _fˆ(w1,_ _, wK) = [1]_
**_w1,_** _,wK_ _· · ·_ _K_
_{_ _···_ _}_


_Fˆk (wk) := E[_ (x,y) _k_ (wk; (x, y))]
_L_ _∼D_


_k=1_


-----

according to (Zhang et al., 2020; Hanzely et al., 2021). The problem could be separately solved by
individual client with no communication. However, if the local data is insufficient, poor performance
could be observed by this direct solution, since the local models cannot be boosted by other clients.

**Regularized PFL problem. Regularized PFL can ease the heterogeneous challenges encountered**
by general FL, while escaping the curse of insufficient samples encountered by the ultimate PFL
problem. Inspired by (Chen & Chao, 2021), we formulate the Regularized PFL problem as follows:

_K_

(P3) min _f¯(w1,_ _, wK) = [1]_ ¯Fk (wk) := E[ (x,y) _k_ (wk; (x, y))] + ( ),
**_w1,_** _,wK_ _· · ·_ _K_ _L_ _∼D_ _R_ _·_
_{_ _···_ _}_ _k=1_

X 

where wk denote the personalized models and R(·) is the regularizer that enables information exchange between clients, making the problem tractable even the local samples are insufficient. However, it remains controversial about how to define the regularizer. Also, the gap between regularized
PFL problem (P3) and the ultimate PFL problem (P2) still remains unspecified in existing PFL study.


(P3) min _f¯(w1,_ _, wK) = [1]_
**_w1,_** _,wK_ _· · ·_ _K_
_{_ _···_ _}_


_k=1_


In this work, our ultimate goal is to solve problem (P2), which requires information exchange between clients to ensure effective solution. Below, instead of utilizing regularizer as in (P3), we
alternatively propose a novel Sparse PFL (SPFL) problem to reach the same goal.

**Sparse PFL problem. By introducing personalized masks into FL, we derive the SPFL problem as**


(P4) min
**_w_** _[f]_ [(][w][) = 1]K


_Fk(m[∗]k_ (x,y)∼Dk [(][m][∗]k

_[⊙]_ **_[w][) :=][ E][[][L]_** _[⊙]_ **_[w][; (][x][, y][))]]_**


_k=1_


where m[∗]k
product for two given vectors. Our ultimate goal is to find a global model[∈{][0][,][ 1][}][d][ is a personalized sparse binary mask for][ k][-th client.][ ⊙] **_w[denotes the Hadamard], such that the per-_**
sonalized model for k-th client can be extracted from the global model by personalized mask m[∗]k[,]
i.e.,k-th personalized model, otherwise, remains dormant. Thus, the information exchange between all m[∗]k _[⊙]_ **_[w][. The element of][ m]k[∗]_** [being][ 1][ means that the weight in the global model is active for]
personalized models is enforced by a shared global model w.

Compared with existing PFL algorithms, solving our SPFL problem (P4) does not sacrifice additional computation and storage overhead of clients, since we do not maintain both personalized
local models and global model in clients as in (Li et al., 2021; Mansour et al., 2020). On contrary,
the solution to our problem could potentially lower the communication and computation overhead.
Moreover, SPFL problem (P4) can be applied to most of the model architectures without modelspecific hyper-parameter tuning, since we do not make model-specific separation of the public and
personalized layer in (Arivazhagan et al., 2019; Liang et al., 2020; Collins et al., 2021), or domainspecific fine-tuning in (Chen et al., 2020; Yang et al., 2020).

4 FEDSPA: SOLUTION FOR SPFL

In this section, we first introduce our proposed FedSpa in Algorithm 1. Then, we specify the update
rule of global model, and two sparse-to-sparse mask searching methods that can be plug in the
update process. At last, we give a theoretical analysis on evaluating the quality of the iterates of
FedSpa with respect to the ultimate PFL problem (P2).

4.1 GLOBAL MODEL UPDATE RULE FOR FEDSPA

**Data Parallel-based Update. We first propose the following iterative update to solve problem (P4)**


**_wt+1 = wt_**
_−_ _K[η]_


**_m[∗]k_** **_wk,t_** _[L][( ˜]wk,t; ξk,t),_ (1)
_k=1_ _[⊙∇]_ [˜]

X


where ξk,t is a batch of data that is uniformly sampled from the k-th client’s local distribution Dk,
_η is the learning rate, and ˜wk,t = m[∗]k_ _[⊙]_ **_[w][t][ is the sparse weights sparsified by mask][ m]k[∗][. However,]_**


-----

the optimal personalized masks {m[∗]k[}][ are generally not accessible to us in the solution process. Let]
**_mk,t be an intermediate surrogate personalized mask of m[∗]k[, we rewrite Eq. (][1][) as:]_**


**_wt+1 = wt_**
_−_ _K[η]_


_k=1_ **_mk,t ⊙∇w˜k,t_** _[L][( ˜]wk,t; ξk,t)._ (2)

X


For our proposed update rule, it is worth mentioned that: (i) Some coordinates of the model weights
have been made zero before doing the forward process, i.e., not all the parameters have to be involved
when calculating ( ˜wk,t; ξk,t). This means that the computation overhead in the forward process
_L_
could be potentially saved. (ii) In the backward process, the stochastic gradient ∇w˜k,t _[L][( ˜]wk,t; ξk,t)_
is masked again by mk,t, which means that we do not need to backward the gradient for those sparse
coordinates. Thus, the computation cost can be largely saved.

**FL-adapted Update. To save the communication overhead, we integrate the idea from local SGD**
(Stich, 2018) and partial participation to our solution. Let ˜wk,t,τ denote the weights before doing
(τ +1)-th step of local SGD and set ˜wk,t,0 = mk,t _⊙wt (i.e., the local weights will be synchronized_
every N steps with the global weights). Then, each client k ∈ _St updates its model as below:_

**_w˜k,t,τ_** +1 = ˜wk,t,τ − _ηmk,t ⊙∇w˜k,t,τ_ _[L][( ˜]wk,t,τ_ ; ξk,t,τ ), τ = 0, 1, . . ., N − 1, (3)

where ξk,t,τ is a batch of sampled data in τ -th step at round t. After the local training is finished, the
models of participated clients are updated and aggregated to the global model in server as follows:

1
**_wt+1 = wt_** ( ˜wk,t,0 **_w˜k,t,N_** ), (4)
_−_ _|St|_ _kX∈St_ _−_

where St is the set of clients selected to be participant in round t. According to Eq. (3), the update
synchronized to the server (i.e., ˜wk,t,0 **_w˜k,t,N_** ), and the model distributed to clients (i.e., ˜wk,t,0)
are all sparse with a constant sparsity. Therefore, the communication overhead over synchronization −
could be largely saved. At last, we summarize our proposed FedSpa in Algorithm 1.

**Algorithm 1 FedSpa**

**Input Training iteration T** ; Learning rate η; Local Steps N ; Random seed seed;

1: procedure SERVER’S MAIN LOOP
2: Randomly initialize global model w0

3: **_mk,0 = Initialization(seed) for k_** =0, 1, . . ., K _▷_ Initialize masks via a mask-searching
solution

4: **for t = 0, 1, . . ., T −** 1 do

5: Uniformly sample a fraction of client into St

6: **for each client k /∈** _St do_

7: **_mk,t+1 = mk,t_** _▷_ Inherit masks for round t + 1 if not chosen


**_wt+1 = wt −_**


_St_
_|_ _|_


8: **for k ∈** _St do_

10:9: SendCall Client ˜wk,t,0 k =’s main loop and receive mk,t ⊙ **_wt to client k Uk,t and mk,t+1_**

11: **_wt+1 = wt −_** _S[1]_ _k∈St_ **_[U][k,t]_** _▷_ Average and apply the update

12: procedure CLIENT’S MAIN LOOP

P

13: **for τ = 0, 1, . . ., N −** 1 do

14: Sample a batch of data ξk,t,τ from local dataset

15: **_gk,t,τ_** ( ˜wk,t,τ ) = **_w˜k,t,τ_** **_wk,t,τ_** ; ξk,t,τ )

16: **_w˜k,t,τ_** +1 = ˜wk,t,τ ∇ − _ηm[L]k,t[( ˜] ⊙_ **_gk,t,τ_** ( ˜wk,t,τ ) _▷_ Local update with fixed mask mk,t

18:17: **_mUk,tk,t = ˜+1 = Next Masks(wk,t,0 −_** **_w˜k,t,N·)_** _▷_ Plug in a mask-searching solution to produce next masks

19: Send back Uk,t and mk,t+1


4.2 SPARSE-TO-SPARSE MASK SEARCHING TECHNIQUE

The framework of FedSpa is extensible. Specifically, we use the mask surrogate mk,t in Eq. (3)
to perform the update, which allows us to plug in arbitrary mask searching techniques to determine


-----

the iterating process of mk,t. In this work, we nominate two kinds of sparse-to-sparse training
techniques: modified dynamic sparse training (DST) (Liu et al., 2021) and Random Static Masks
(RSM), into FedSpa to reduce the computation and communication cost.

**Modified DST for FedSpa. Our modified DST solution (see Algorithm 2) for FL follows these**
procedures. Firstly, randomly initialize the same mask for each client based on Erd´os-R´enyi Kernel
(ERK) (Evci et al., 2020). Secondly, after local training, each client prunes out a number of unpruned
weights with the smallest magnitude, and the number of weights being pruned is determined by a
decayed pruned rate. Thirdly, recover the same amount of weights pruned in the last step. We follow
the recovery process as in (Evci et al., 2020) by utilizing the gradient information to do the recovery.
By our DST method, the number of sparse weights (aka. sparse volume) remains a constant (i.e., β)
throughout the whole training process.

**Algorithm 2 Modified DST for FedSpa**

**Input Initial pruning rate α0; Set of Model layers J ;**

1: procedure INITIALIZATION(seed)
2: Randomly initialize mk,0 using the same random seed seed.

3: procedure NEXT MASKS(w ˜k,t,N )

4: Decay αt using cosine annealing with initial pruning rate α0

5: Sample a batch of data and backward the dense gradient g( ˜wk,t,N )

6: **for layer j ∈J do**

7: Update mask m[(]k,t[j][)]+ [1]2 [by zeroing out][ α][t][-proportion of weights with magnitude pruning]

8: Update mask m[(]k,t[j][)]+1 [via recovering weights with gradient information][ g][( ˜]wk,t,N )

9: Return mk,t+1

_Remark. We highlight our main modification over traditional DST techniques like Rigl (Evci et al.,_
2020) and Set (Mocanu et al. (2018)) to an FL context exist in two main aspects: (i) The pruning is
performed individually by each client based on their local models, and the gradient used for weights
recovery is derived using the client’s local training data. (ii) Once the next masks are generated,
existing DST solutions immediately apply them to the local model weight. Indicated by Liu et al.
(2021), by doing so, the recovered coordinate may need extra training steps to grow from 0 to a dense
value. Our solution relieves this problem by applying the new mask on the global weights (which
are dense), such that the recovered coordinates could have a dense initial value to warm-start.


**RSM for FedSpa. RSM (shown in Algorithm 3) is basi-**
cally fixing mk,t for all k ∈ [K] to the same randomly initialized mask, which remains unchanged during the whole
training session. This solution also ensures the same
sparse volume for all the clients throughout the training
process, and could also reduce the computation and communication overhead as DST. Interestingly, within the setting of the homogeneous data distribution, we empirically
show that RSM is more effective than DST in FedSpa.

4.3 THEORETICAL ANALYSIS


**Algorithm 3 RSM for FedSpa**

1: procedure INITIALIZATION(seed)
2: Randomly initialize mk,0 using the same random seed seed

3: procedure NEXT MASKS
4: **_mk,t+1 = mk,t_**

5: Return mk,t+1


In this section, we shall introduce the convergence property of personalized models produced by
FedSpa. We first give the following assumptions.
**Assumption 1 (Coordinate-wise bounded gradient dissimilarity). For any ˜w ∈** R[d], there ex_ists a constantFk( ˜w)_ _K G ≥Kk[′]=10 bounding the coordinate-wise gradient dissimilarity over all clients, i.e.,w)_
_∇_ _−_ [1] _[∇][F][k][′]_ [( ˜] _∞_ _[≤]_ _[G][.]_

**Assumption 2 (Bounded variance)P** **. Assume that gk,t,τ** ( ˜w):= _∇L( ˜w; ξk,t,τ_ ) is an unbiased estima
_tor ofAssumption 3 ∇Fk( ˜w) with bounded variance, i.e., (L-smoothness). We assume L-smoothness over the client’s loss function, i.e., E_ h∥gk,t,τ ( ˜w)−∇Fk( ˜w)∥[2][i] _≤_ _σ[2], ∀k, t, τ, ˜w ∈_ R[d].

_∥∇Fk( ˜w1) −∇Fk( ˜w2)∥≤_ _L∥w˜1 −_ **_w˜2∥_** _holds for arbitrary ˜w1, ˜w2 ∈_ R[d].


-----

**Assumption 4 (Coordinate-wise bounded gradient). Suppose the coordinate-wise gradient of each**
_client is upper-bounded, i.e.,_ **_w˜_** _[F]k[( ˜]w)_ _B._
_∥∇_ _∥∞_ _≤_

Assumptions 2 and 3 are commonly used for characterizing the convergence of FL algorithms. We
modify the other two assumptions slightly from their counterparts in existing FL literature (Xu et al.,
2021, see Assumptions 3 and 5) by replacing the L2 norm with L norm. These modifications are
_∞_
made to reveal the coordinate-wise gradient information in our analysis.
**Theorem 1 (Convergence of personalized models). Given the above assumptions, suppose the**
_learning rate satisfiestionary masks||1 −_ **_mk,t||0 = m βk,t, the personalized sparse models ∈{ η ≤0, 116}[d]LN1_** _maintain the same sparse volume constraint, i.e.,[, the optimal personalized masks] {w ˜k,t} generated via FedSpa exhibits the fol-[ m]k[∗]_ _[∈{][0][,][ 1][}][d] ||[ and the evolu-]1 −_ **_m[∗]k[||][0][ =]_**

_lowing convergence property towards the optimal solution of ultimate PFL problem (P2):_

1 _T −1_ _K_ E _Fk ( ˜wk,t)_ + [3] _T −1_ _ρt + Υ,_ (5)

_TK_ _∥∇_ _∥[2][]_ _≤_ [3(][f][ (][w]TηNκ[0][)][ −] _[f][ (][w][∗][))]_ _T_

_t=0_ _k=1_ _t=0_

_where_ _κ_ = X12 X  _and_ _ρt_ X= (25N [3]η[4]L[3] +

5N [2]η[3]L[2] _[−]_ [150][N][ 3][η]4[3]N[L][2][3]η[ −][2]L+Nη[15][N][ 2][η][2][L][2][ −] [5][NηL] _Nη[2]Lσ[2]_

2 )(σ[2] + 18N Φt) + 2K _k_ _[dist][(][m][k,t][,][ m]k[∗][)][B][2][ + 9][N][ 2][η][2][L][Φ][t][ +]_ _S_ _,_

Φt = _K1_ _k[((][d][ −]_ _[β][)][G][2]_ + _K[1]_ _k[′][ B][2][(][dist][(][m][k,t][,][ m][k][′][,t]) + dist(mk′,t, m[∗]k[′]_ [)])), Υ =

P

P _≈0 if β→d_ P _≈0 if mk,t→mk′_ _,t_ _≈0 if mk′_ _,t→m[∗]k[′]_

3(d _β)G[2]+ 3βB|[2]_ +{zK[3][2] } _k_ _k[′][ dist][(][m]|k[∗][,][ m]k[∗][′]_ [)]{z[B][2], dist}(m1, m| 2) is the hamming distance{z } [2]
_−_
_≈0 if β→d_ _≈0 if β→0_ P P _≈0 if m[∗]k[→][m][∗]k[′]_

_between masks m1 and m2._

| {z } | {z } | {z }

Below, we give several comments on the above theorem. We focus our analysis on the on the second
and third residuals, which do not vanish as communication round T →∞

-  Impact of sparse volume. The result in Eq. (5) is highly related to setting of sparse volume,
i.e., β. The term βB[2] would vanish as β → 0, while the term (d − _β)G[2]_ would disappear as
_β →_ _d. When the data is highly heterogeneous, i.e., gradient dissimilarity is drastic, G could_
be prohibitively large to dominate the error. Then setting the sparsity level to a relatively large
value, which results in a lower (d−β)G[2], would potentially give a smaller non-vanished error. On
contrary, when the data heterogeneity is mild, the error could be dominated by βB[2]. Then setting
an excessively high sparsity might even hurt the performance. This conclusion is also evidenced
by our experimental results (see Figure 1 in Section 1, where the optimal sparsity setting of FedSpa
that achieves the highest accuracy is 0.5).

-  Error by evolutionary mask searching. Recall that FedSpa uses evolutionary masks as a surrogate of optimal masks to perform update on the global model wt (see Eq. (3)). This replacement
may bring additional errors to the convergence bound, as the hamming distance between the evolutionary masks and optimal masks (i.e., the term dist(mk,t, m[∗]k[)][) exists in our bound.]

-  Error by dissimilar evolutionary masks. The theoretical result on the term dist(mk,t, mk′,t) in
Φt indicates that the dissimilarity among the evolutionary masks may also play a role in optimizing
the ultimate PFL problem (P2). This term could be minimized if the evolutionary masks remain
the same throughout the training process, and would dominate the error if the masks are too
distinct. This observation exactly motivates us to present the naive mask searching technique
RSM in Algorithm 3, which forces all the clients to share the same random masks.

5 EXPERIMENTS

In this section, we conduct extensive experiments on verifying the efficacy of the proposed FedSpa.
Our implementation of FedSpa is based on an open-source FL simulator FedML (He et al., 2020).

5.1 EXPERIMENTAL SETUP

**Dataset. We evaluate the efficacy of FedSpa on EMNIST-Letter (EMNIST-L henceforth), CIFAR10,**
and CIFAR100 datasets. We simulate the client’s data distribution on Non-IID and IID setting. We

2Hamming distance measures the number of positions at which the two masks are different.


-----

Figure 3: Test Accuracy vs. Communication Rounds

simulate two groups of Non-IID settings via γ-Dirichlet distribution, named setting A and setting B.
Setting A and setting B respectively specify γ = 0.2, 0.1 for both EMNIST-L and CIFAR100, while
specify γ = 0.5, 0.3 for CIFAR10. Details of our simulation setting are available in Appendix B.1.

**Baselines. We compare our proposed FedSpa with four baselines, including FedAvg (McMahan**
et al. (2016)), Sub-FedAvg (Vahidian et al. (2021)), Ditto (Li et al. (2021)) and Local. We tune the
hyper-parameters of the baselines to their best states. Specifically, the regularization factor of Ditto
is set to 0.5. The prune rate each round, distance threshold, and accuracy threshold of Fed-Subavg
are fixed to 0.05, 0.0001, 0.5, respectively. We ran 3 random seeds in our comparison.

**Models and hyper-parameters.** We use LeNet5 for EMNIST-L, VGG11 for CIFAR10, and
ResNet18 for CIFAR100 in our experiment. We use a SGD optimizer with weight decayed parameter 0.0005. The learning rate is initialized with 0.1 and decayed with 0.998 after each communication
round. We simulate 100 clients in total, and in each round 10 of them are picked to perform local
training (the setting follows McMahan et al. (2016)). For all the methods except Ditto, local epochs
are fixed to 5. For Ditto, in order to ensure a fair comparison, each client uses 3 epochs for training
of the local model, and 2 epochs for global model training. The batch size of all the experiments is
fixed to 128. For FedSpa, the pruning rate (i.e., αt) is decayed using cosine annealing with an initial
pruned rate 0.5. The initial sparsity of layers is initialized by ERK with scale parameter 1.

5.2 MAIN PERFORMANCE EVALUATION

We fix the dense ratio of FedSpa (DST), FedSpa (RSM), and the final dense ratio of Fed-SubAvg
both to 0.5 (i.e., 50% of parameters are pruned) in our main evaluation. Other hyper-parameters are
fixed as default. Figure 3 and Table 1 illustrate the training performance of different algorithms on
three datasets. We evaluate the performance based on the following metrics:
**Final Accuracy. In the Non-IID setting, we show that FedSpa (DST) achieves remarkable per-**
formance. Specifically, in Non-IID setting B of CIFAR100, FedSpa (DST) achieves respectively
4.4%, 11.9% and 21.9% higher final model accuracy, compared with Ditto, Sub-FedAvg and FedAvg. FedSpa (DST) seems to achieve better performance as the FL tasks becoming difficult (since
better performance is observed in a higher Non-IID extent, and in datasets that are intrinsically
more difficult). Interestingly, in the IID setting, we show that all the personalized solutions exhibit
some extents of performance degradation, which become more significant as the dataset becomes
challenging. The compression-based methods seem to be especially vulnerable in this setting. Our
interpretation for this phenomenon is that: since the information exchange between clients would
be limited by employing different sub-networks for training, the clients could not efficiently make
an effective fusion on their models through parameter averaging. This hypothesis is substantiated
by our experiment on FedSpa (RSM), an alternative implementation of FedSpa, which forces all
the masks to maintain the same sub-network. FedSpa (RSM) achieves commensurate performance
with FedAvg in the IID setting, outperforming the personalized solutions. The reason FedSpa (DST)


-----

Table 1: Table illustrating performance of different methods.

Non-IID

|Task Method IID|Non Setting A|n-IID Setting B|
|---|---|---|
|Acc Comm Cost FLOPs (GB) (1e14)|Acc Comm Cost FLOPs (GB) (1e16)|Acc Comm Cost FLOPs (GB) (1e14)|
|FedSpa (DST) 92.2±0.1 7.0 2.0 FedSpa (RSM) 92.9±0.1 7.0 2.0 EMNIST-L Ditto 92.9±0.1 14.1 3.5 (LeNet) FedAvg 93.5±0.2 14.1 3.5 Sub-FedAvg 90.7±0.2 9.5 1.9 Local 77.6±0.3 - 3.5 Subsampling 93.3±0.2 10.5 3.5|95.3±0.1 7.0 2.0 91.9±0.2 7.0 2.0 95.9±0.1 14.1 3.5 92.3±0.3 14.1 3.5 94.9±0.2 9.4 1.9 87.8±0.1 - 3.5 92.0±0.4 10.5 3.5|96.5±0.2 7.0 2.0 90.6±0.9 7.0 2.0 97.0±0.2 14.1 3.5 90.9±0.8 14.1 3.5 96.4±0.2 9.4 1.9 91.6±0.5 - 3.5 91.3±0.6 10.5 3.5|
|FedSpa (DST) 83.4±0.1 369.2 172.9 FedSpa (RSM) 84.5±0.1 369.2 172.9 CIFAR-10 Ditto 83.5±0.2 738.5 229.3 (VGG11) FedAvg 84.8±0.3 738.5 229.3 Sub-FedAvg 71.8±0.3 410.2 121.4 Local 42.5±0.2 - 229.3 Subsampling 83.0±0.4 553.9 229.3|86.6±0.5 369.2 173.3 82.1±0.2 369.2 173.3 86.4±0.6 738.5 229.8 82.0±0.4 738.5 229.8 78.3±1.0 424.7 120.6 63.6±0.6 - 229.8 78.9±0.5 553.9 229.8|88.2±0.4 369.2 173.5 80.9±0.2 369.2 173.5 87.8±0.3 738.5 230.0 81.4±0.4 738.5 230.0 79.6±0.6 416.9 119.8 69.4±0.2 - 230.0 76.7±1.0 553.9 230.0|
|FedSpa (DST) 41.5±0.5 448.8 705.1 FedSpa (RSM) 54.6±1.1 448.8 705.1 CIFAR-100 Ditto 51.9±1.1 897.6 833.2 (ResNet18) FedAvg 55.7±1.3 897.6 833.2 Sub-FedAvg 38.3±0.8 616.5 494.1 Local 10.3±0.3 - 833.2 Subsampling 49.8±1.3 673.2 833.2|59.0±1.0 448.8 704.9 48.7±0.5 448.8 704.9 56.8±0.6 897.6 833.0 49.3±0.4 897.6 833.0 49.2±0.7 624.4 508.4 28.8±0.1 - 833.0 42.3±0.8 673.2 833.0|66.9±0.2 448.8 704.8 44.6±0.5 448.8 704.8 62.5±0.2 897.6 832.9 45.0±0.9 897.6 832.9 55.0±0.7 612.8 496.1 40.5±0.4 - 832.9 37.6±1.1 673.2 832.9|



performing better in Non-IID setting can also be explained by Theorem 1, which concludes that in a
Non-IID setting, setting proper sparsity for personalized models may promise a better convergence.

**Convergence. As shown in Table 2, FedSpa achieves significantly faster convergence, which poten-**
tially saves the communication rounds to train a model from scratch to a specific accuracy.

Table 2: Communication rounds to a fixed accuracy.

Non-IID

|CIFAR10 IID|Non Setting A|n-IID Setting B|
|---|---|---|
|Acc@70 Acc@75 Acc@80|Acc@70 Acc@75 Acc@80|Acc@70 Acc@75 Acc@80|
|FedSpa (DST) 134.0±2.9 183.3±6.8 312.3±16.2 FedSpa (RSM) 101.3±1.7 141.3±6.2 237.0±6.4 Ditto 284.7±8.1 370.3±9.3 549.3±22.6 FedAvg 105.0±2.2 140.3±4.7 228.7±23.5 Sub-FedAvg 197.7±22.9 > 1000 > 1000 Subsampling 198.0±4.5 268.3±4.5 457.0±13.5|167.3±4.0 210.3±4.2 281.3±19.1 195.3±10.7 271.3±16.2 471.7±19.8 242.3±12.7 334.0±16.5 466.3±30.3 198.3±14.8 256.7±10.8 474.7±31.4 151.7±10.6 235.0±17.1 > 1000 365.3±23.8 523.0±43.4 > 1000|164.3±5.0 206.3±4.1 270.0±5.1 252.0±12.7 339.0±20.6 614.0±72.8 190.3±6.1 278.0±22.0 417.7±10.2 241.0±3.7 327.3±8.5 583.7±65.5 137.3±1.7 191.7±6.6 > 1000 466.3±15.0 722.7±105.1 > 1000|
|CIFAR100 Acc@40 Acc@50 Acc@55|Acc@40 Acc@50 Acc@55|Acc@40 Acc@50 Acc@55|
|FedSpa (DST) 536.3±35.9 > 1000 > 1000 FedSpa (RSM) 239.3±4.1 435.7±25.8 > 1000 Ditto 545.7±19.4 868.7±56.1 > 1000 FedAvg 245.0±5.1 436.3±25.3 > 1000 Sub-FedAVG > 1000 > 1000 > 1000 Subsampling 460.7±16.6 > 1000 > 1000|236.3±12.3 442.0±16.9 595.0±41.3 460.7±12.5 > 1000 > 1000 455.3±6.9 724.0±20.2 894.0±25.0 470.7±25.4 > 1000 > 1000 280.7±2.5 > 1000 > 1000 845.3±59.2 > 1000 > 1000|181.3±7.8 314.7±17.4 407.7±16.7 594.0±10.7 > 1000 > 1000 301.3±10.8 534.0±11.3 678.7±7.9 589.7±46.7 > 1000 > 1000 246.3±10.1 335.3±14.1 511.0±85.9 > 1000 > 1000 > 1000|



**Training FLOPs and Communication. From Table 1, FedSpa (DST) achieves 15.4%∼42.9%**
lower FLOPs than the dense solutions (e.g., Ditto, FedAvg), 13.0%∼28.2% lower communication
overhead than another model compression solution Sub-FedAvg, and 50% lower communication
than the dense solution. The edge of FedSpa (DST) stems from its training pattern – it is trained from
a sparse model, with constant sparsity throughout the training process. However, it is interesting to
see that the training FLOPS of Fed-SubAvg is considerably lower than FedSpa, even under the same
sparsity setting. This phenomenon stems from our ERK initialization, which is essential for the high
performance of our solution, for which we will have a further discussion in Appendix B.4.

6 CONCLUSIONS

In this paper, we propose FedSpa, a personalized FL solution that enables sparse-to-sparse training and efficient sub-model aggregation. As demonstrated by our experiments, FedSpa exhibits
outstanding performance in the Non-IID setting, outperforming other existing solutions in terms of
accuracy, convergence speed as well as communication overhead. Additionally, we present theoretical analysis to evaluate the error bound of FedSpa towards the ultimate PFL problem. Future
direction includes designing new model aggregation solutions for the sparse sub-network, and new
mask searching techniques specifically targeting on federated learning process.


-----

7 REPRODUCIBILITY STATEMENT

For sake of reproducibility of our solution, we make the following efforts: (i) In Appendix B.1, we
clearly state the data splitting method for IID and Non-IID data distribution. (ii) In Appendix B.2, the
network models we used in our experiment are clearly described. (iii) In Appendix B.4, the proposed
FedSpa with various hyper-parameters during the implementation are also systemically tested to
demonstrate its stability and superiority. (iv) In Appendix C, we give the self-contained proof of
Theorem 1. (v) At last, the source-code of FedSpa is enclosed in the supplementary material.

REFERENCES

Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, and Sunav Choudhary. Federated learning with personalization layers. arXiv preprint arXiv:1912.00818, 2019.

Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein. Deep rewiring: Training
very sparse deep networks. In International Conference on Learning Representations, 2018.

Hong-You Chen and Wei-Lun Chao. On bridging generic and personalized federated learning. arXiv
_preprint arXiv:2107.00778, 2021._

Yiqiang Chen, Xin Qin, Jindong Wang, Chaohui Yu, and Wen Gao. Fedhealth: A federated transfer
learning framework for wearable healthcare. IEEE Intelligent Systems, 35(4):83–93, 2020.

Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared representations for personalized federated learning. arXiv preprint arXiv:2102.07078, 2021.

Yuyang Deng, Mohammad Mahdi Kamani, and Mehrdad Mahdavi. Adaptive personalized federated
learning, 2020.

Tim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing
performance. arXiv preprint arXiv:1907.04840, 2019.

Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery:
Making all tickets winners. In International Conference on Machine Learning, pp. 2943–2952.
PMLR, 2020.

Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. arXiv preprint arXiv:1803.03635, 2018.

Filip Hanzely and Peter Richt´arik. Federated learning of a mixture of global and local models. arXiv
_preprint arXiv:2002.05516, 2020._

Filip Hanzely, Boxin Zhao, and Mladen Kolar. Personalized federated learning: A unified framework and universal optimization techniques. arXiv preprint arXiv:2102.09743, 2021.

Chaoyang He, Songze Li, Jinhyun So, Xiao Zeng, Mi Zhang, Hongyi Wang, Xiaoyang Wang, Praneeth Vepakomma, Abhishek Singh, Hang Qiu, et al. Fedml: A research library and benchmark
for federated machine learning. arXiv preprint arXiv:2007.13518, 2020.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778, 2016.

Kevin Hsieh, Amar Phanishayee, Onur Mutlu, and Phillip Gibbons. The non-iid data quagmire of
decentralized machine learning. In International Conference on Machine Learning, pp. 4387–
4398. PMLR, 2020.

Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data
distribution for federated visual classification. arXiv preprint arXiv:1909.06335, 2019.

Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International conference on machine learning, pp. 448–456.
PMLR, 2015.


-----

Siddhant Jayakumar, Razvan Pascanu, Jack Rae, Simon Osindero, and Erich Elsen. Top-kast: Top-k
always sparse training. Advances in Neural Information Processing Systems, 33, 2020.

Alexander Jung. Networked exponential families for big data over networks. IEEE Access, 8:
202897–202909, 2020.

Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In
_International Conference on Machine Learning, pp. 5132–5143. PMLR, 2020._

Jakub Koneˇcn`y, H Brendan McMahan, Felix X Yu, Peter Richt´arik, Ananda Theertha Suresh, and
Dave Bacon. Federated learning: Strategies for improving communication efficiency. _arXiv_
_preprint arXiv:1610.05492, 2016._

Yann LeCun et al. Lenet-5, convolutional neural networks. URL: http://yann. lecun. com/exdb/lenet,
20(5):14, 2015.

Ang Li, Jingwei Sun, Binghui Wang, Lin Duan, Sicheng Li, Yiran Chen, and Hai Li. Lotteryfl:
Personalized and communication-efficient federated learning with lottery ticket hypothesis on
non-iid datasets. arXiv preprint arXiv:2008.03371, 2020.

Daliang Li and Junpu Wang. Fedmd: Heterogenous federated learning via model distillation, 2019.

Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated
learning through personalization, 2021.

Paul Pu Liang, Terrance Liu, Liu Ziyin, Nicholas B Allen, Randy P Auerbach, David Brent, Ruslan
Salakhutdinov, and Louis-Philippe Morency. Think locally, act globally: Federated learning with
local and global representations. arXiv preprint arXiv:2001.01523, 2020.

Shiwei Liu, Decebal Constantin Mocanu, Amarsagar Reddy Ramapuram Matavalam, Yulong Pei,
and Mykola Pechenizkiy. Sparse evolutionary deep learning with over one million artificial neurons on commodity hardware. Neural Computing and Applications, pp. 1–16, 2020.

Shiwei Liu, Lu Yin, Decebal Constantin Mocanu, and Mykola Pechenizkiy. Do we actually need
dense over-parameterization? in-time over-parameterization in sparse training. In Proceedings of
_the 39th International Conference on Machine Learning, pp. 6989–7000. PMLR, 2021._

Yishay Mansour, Mehryar Mohri, Jae Ro, and Ananda Theertha Suresh. Three approaches for
personalization with applications to federated learning. arXiv preprint arXiv:2002.10619, 2020.

H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et al. Communication-efficient
learning of deep networks from decentralized data. arXiv preprint arXiv:1602.05629, 2016.

Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu,
and Antonio Liotta. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science. Nature Communications, 9(1):2383, 2018.

Hesham Mostafa and Xin Wang. Parameter efficient training of deep convolutional neural networks
by dynamic sparse reparameterization. International Conference on Machine Learning, 2019.

Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Koneˇcn`y,
Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. _arXiv preprint_
_arXiv:2003.00295, 2020._

Yasmin Sarcheshmehpour, M Leinonen, and Alexander Jung. Federated learning from big data over
networks. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal
_Processing (ICASSP), pp. 3055–3059. IEEE, 2021._

Yasmin SarcheshmehPour, Yu Tian, Linli Zhang, and Alexander Jung. Networked federated multitask learning. arXiv preprint arXiv:2105.12769, 2021.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.


-----

Sebastian U Stich. Local sgd converges fast and communicates little. _arXiv preprint_
_arXiv:1805.09767, 2018._

Saeed Vahidian, Mahdi Morafah, and Bill Lin. Personalized federated learning by structured and
unstructured pruning under data heterogeneity. arXiv preprint arXiv:2105.00562, 2021.

Paul Voigt and Axel Von dem Bussche. The eu general data protection regulation (gdpr). A Practical
_Guide, 1st Ed., Cham: Springer International Publishing, 10:3152676, 2017._

Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on
_computer vision (ECCV), pp. 3–19, 2018._

Jing Xu, Sen Wang, Liwei Wang, and Andrew Chi-Chih Yao. Fedcm: Federated learning with
client-level momentum. arXiv preprint arXiv:2106.10874, 2021.

Hongwei Yang, Hui He, Weizhe Zhang, and Xiaochun Cao. Fedsteg: A federated transfer learning
framework for secure image steganalysis. IEEE Transactions on Network Science and Engineer_ing, 2020._

Michael Zhang, Karan Sapra, Sanja Fidler, Serena Yeung, and Jose M Alvarez. Personalized federated learning with first order model optimization. arXiv preprint arXiv:2012.08565, 2020.


-----

A CRITICAL COMPONENTS OF FEDSPA (DST)

**ERK initialization. In Algorithm 2, we use Erd´os-R´enyi Kernel (ERK) originally proposed by**

Evci et al. (2020) to initialize the sparsity of each layer. Specifically, the active parameters of the
convolutional layer initialized by ERK is proportional to 1 _[n]n[1][−][l][−][1][1][+][n]n[l][l][+]w[w][l][l][+]h[h][l][, where][l]_ _[ n][l][−][1][,][ n][l][ w][l][ and]_

_∗_ _∗_ _∗_
_h[l]_ respectively specify number of input channels, output channels and kernel’s width and height in
the l-th layer. For the linear layer, the number of active parameters scale with 1 n[n][l][l][−][−][1][1][+n]n[l][l][ where][ n][l][−][1]

_÷_

and n[l] are the number of neurons in the l − 1-th and l-th layer. This initialization basically allows
the layer with less parameters have more proportion of active parameters.

**Cosine annealing. Recall that we set the initial pruning rate as α0 and gradually decay it to 0**
with cosine annealing Liu et al. (2021). The update of pruning rate with cosine annealing can be
formalized as: αt = 0.5 × α0 × 1 + cos _T −t_ 1 _[π]_ . We perform this decay in order to ensure that

the network (specifically, its active coordinates) would not experience drastic change on the later  
stage of training while ensuring that the mask searching is effective on the early stage of training.


B DETAILED SETTING AND ADDITIONAL EXPERIMENTS

B.1 DATA SPLITTING SETTING

In our implementation, we first split the training data (60k pieces of data for CIFAR10 and CIFAR100, and 145.6k for EMNIST-L, respectively) to clients for IID setting and Non-IID setting.
For the IID setting, data are uniformly sampled for each client. For the Non-IID setting, we use γDirichlet distribution on the label ratios to ensure uneven label distributions among devices as (Hsu
et al., 2019). The lower the distribution parameter γ is, the more uneven the label distribution will
be, and would be more challenging for FL. After the initial splitting of training data, we sample 100
pieces of testing data from the testing set to each client. To simulate the personalized setting, each
client’s testing data has the same proportion of labels as its training data. Testing of the personalized
model is performed by each client based on their personalized data, and the overall testing accuracy
is calculated as the average of all the client’s testing accuracy. In our experiment, we simulate different Non-IID settings. For CIFAR10, Non-IID setting A and B respectively specify γ = 0.5 and
_γ = 0.3. For EMNIST-L and CIFAR100, since the number of the total labels are bigger[3], we use_
smaller γ, wherein setting A and B respectively specify γ = 0.2 and γ = 0.1.

B.2 NETWORK ARCHITECTURES

We follow the Caffe’s implementation of LeNet5 [4] (LeCun et al., 2015), VGG11 (Simonyan &
Zisserman, 2014) and ResNet18 (He et al., 2016) to do the evaluation. Suggested by Hsieh et al.
(2020), DNNs with batch normalization layers (Ioffe & Szegedy, 2015) are particularly vulnerable
to the Non-IID setting, suffering significant model quality loss in the FL process. Following the
recommendation from Hsieh et al. (2020), we use group normalization (Wu & He, 2018) to substitute
the original batch normalization layer in both ResNet18 and VGG11.

B.3 BASELINE DESCRIPTION

Below, we give a brief introduction of the baselines compared in our evaluations:

-  FedAvg (McMahan et al., 2016) is the vanilla solution of FL. It utilizes weights average to enable
all the clients to collaboratively train a global model, which efficiently absorbs knowledge from
personal data resided in clients.

-  Ditto (Li et al., 2021) is a personalized FL solution aiming to smooth the tension brought by the
data heterogeneity problem of FL. Ditto achieves personalization via maintaining both the local
models and global model. Specifically, within each round of iteration, each client first trains the
global model based on its local empirical loss (which shares the same procedure as FedAvg).

326 and 100 labels respectively in EMNIST-L and CIFAR100, while only 10 labels in CIFAR10.
4Available in https://github.com/mi-lad/snip/blob/master/train.py


-----

After the global model is updated, each client additionally trains its local model based on a loss
function involving its local empirical loss and the proximal term towards the global model. This
local training phase is used to extract the global knowledge into each client’s local model. Since
each client has to maintain and train both local model and global model, Ditto might need extra
computation and storage overhead to achieve its personalization.

-  Local is the direct solution to the ultimate PFL problem (P2). Each client performs SGD based
on its local data, and there is no communication between clients. To mimic the FL setting, we
sample 10 out of 100 clients to do the local update on its local model after every 5 epochs of
training (same with the number of local epochs in a communication round that is performed by
other solutions). For sake of consistency, we still use 1 communication round to represent 5 local
epochs of Local in our evaluation.

-  Sub-FedAvg (Vahidian et al., 2021) is a prominent model compression-based PFL. Sub-FedAvg
maintains personalized sub-networks for each client. Training of Sub-FedAvg starts from a fully
dense model, and this solution iteratively prunes out the parameters and channels as the training
progresses. Finally, the commonly shared parameters of each layer are removed, and only the
personalized parameters that can represent the features of local data are kept.

-  Subsampling (Koneˇcn`y et al., 2016) is a gradient-compression solution aiming to reduce the communication overhead of FL. The local training procedure is the same with FedAvg. The difference
is that Subsampling does not communicate the intact model for aggregation, but only communicates the sparse gradient update to the server for aggregation. Explicitly, in each round, the
sparse gradient update is produced through element-wisely multiplying a random mask. Different
from FedSpa, the randomized mask is independently generated in each round, and would only
be used to compress the gradient when uploading the gradient update (which means in the model
distribution phase, the model distributed would not be sparsified, and therefore would not save the
downlink communication cost).

B.4 ABLATION STUDY

In this sub-section, we focus on the ablation study of FedSpa. Specifically, we study the impacts of
dense ratio, different mask initialization methods, and the gradient-involved weight recovery procedure. Additionally, we present an interesting observation on the performance of the global model
trained by our personalized solution. Our ablation study is done with ResNet-18 on CIFAR100.

**Impact of sparsity (aka. sparse ratio). Fixing other components and hyper-parameters to the**
default value in our setup, we change the sparsity of FedSpa to 0.2, 0.4, 0.5, 0.6 and 0.8, to show
its impact on the algorithm performance. Experimental results are available in Figure 4 and Table
3. By our report, we observe that sparsity may impact learning performance under different data
distribution settings. For the IID setting, a higher sparsity seems to seriously degrade the training
performance, while for the Non-IID setting, properly sparsifying the model may even enhance the
final accuracy, and with a higher Non-IID extent, the benefit of sparsification reinforces. This observation surprisingly coincides with our theoretical conclusion given in Theorem 1, that setting the
sparse volume to a proper value when gradient dissimilarity is large (i.e., under a high extent of
Non-IID) could foster a better performance of personalized models. But too much sparsity, even in
the highly non-iid setting (e.g. γ = 0.1) leads to performance degradation. On the contrary, while
it is iid, the convergence could be dominated by the errors brought by sparsification, and setting the
mask to a higher sparsity could possibly enlarge these existing errors.

Another more intuitive interpretation for the impact of sparse ratio is from the perspective of information exchange. Too much sparsification may limit the information exchange between the local
sparse models. If all the clients maintain an extremely high sparsity, the intersected coordinates
between clients’ local sparse models (or identically, their masks) would be small. Then the local
update averaging process (see Eq. (4)), the only way to extract global knowledge into the local
models, would not be effective. On contrary, while the sparsity is set to an extremely low value, the
personalized features of local models could be eliminated, since only limited coordinates in their
models are different.

**ERK vs. Uniform sparsity initialization. Recall our mask initialization procedure in Algorithm 3**
that the layer-wise sparsity is initialized by ERK. This in essence ensures that the layer-wise sparsity
of a model is scaled with the number of parameters in a layer. Liu et al. (2021) confirms the out

-----

Figure 4: FedSpa (DST) under different sparsity. Numbers in the labels are sparsity.

Table 3: Performance of FedSpa (DST) under different sparsity settings.


Non-iid
iid
Sparsity _γ=0.2_ _γ=0.1_

Acc Comm Cost FLOPs Acc Comm Cost FLOPs Acc Comm Cost FLOPs
(GB) (1e16) (GB) (1e16) (GB) (1e16)

0.2 **51.5±0.8** 718.1 8.2 **62.9±0.4** 718.1 8.2 65.5±0.5 718.1 8.2
0.4 45.5±0.9 538.6 7.6 61.4±0.6 538.6 7.5 **67.2±0.4** 538.6 7.5
0.5 41.5±0.5 448.8 7.1 59.0±1.0 448.8 7.0 66.9±0.2 448.8 7.0
0.6 38.4±0.6 359.0 6.5 57.3±1.5 359.0 6.5 65.2±0.2 359.0 6.5
0.8 32.0±0.7 **179.5** **4.6** 49.2±1.8 **179.5** **4.6** 56.7±0.8 **179.5** **4.6**

standing effect of ERK initialization in improving overall training performance over the centralized
training primitive, but it remains unexplored how it performs in our proposed distributed training
framework. Below, we show in Figure 5 how accuracy evolves with communication rounds under
ERK and Uniform [5] initialization. As shown, a drastic drop of accuracy is observed by replacing
ERK with Uniform, by which we conclude that ERK is an essential component for FedSpa (DST).

Figure 5: Layer-wise sparsity initialized by ERK or Uniform. Sparsity of FedSpa is fixed to 0.5.

However, though a significant accuracy enhancement is observed, we note that integrating ERK may
sacrifice potentially more FLOPS reduction. This observation can be found in Table 6, wherein our
results show that initialization with Uniform can save 34.3% FLOPs of that with ERK.

Table 4: Performance of FedSpa under ERK and Uniform initialization.

Non-iid
iid
Methods _γ=0.2_ _γ=0.1_

Acc Comm Cost FLOPs Acc Comm Cost FLOPs Acc Comm Cost FLOPs
(GB) (1e16) (GB) (1e16) (GB) (1e16)

ERK **41.5±0.5** 448.8 7.1 **59.0±1.0** 448.8 7.0 **66.9±0.2** 448.8 7.0
Uniform 33.0±1.4 448.8 **4.6** 49.3±0.9 448.8 **4.6** 56.3±1.1 448.8 **4.6**

**Different or same mask initialization. Recall that based on the layer-wise sparsity calculated by**
ERK, FedSpa uses the same random seed to initialize the mask, so as to make the mask exploration

5Uniform enforces the same sparsity for all the layers in a model.


-----

of all clients started from the same mask. In the following, we give another implementation that
allows each client to share different masks in the beginning.

As shown in Figure 6, we surprisingly find that for FedSpa (DST), maintaining different masks in
initialization may slightly enhance its training performance in IID and Non-IID (γ = 0.2) setting.
We hypothesize that by different mask initialization, each client could more efficiently search for
their optimal masks that better represents the features and labels of the personal data.

Figure 6: Initialization based on same or different masks. Sparsity of FedSpa is fixed to 0.5.

For FedSpa (RSM), compared with initialization using the same mask, different mask initialization
may result in a drastic performance loss in the IID setting, and a significant improvement in the NonIID setting. With the same mask initialization of RSM, each client consistently trains based on the
same sub-network, which completely eliminates personalization. So this setting shares a similar performance with FedAvg – with satisfactory performance in IID setting and rather weak performance
in Non-IID setting. On contrary, by initializing different masks in the beginning, FedSpa (RSM) reserves some degrees of personalization, since only the intersected coordinates in their local models
are shared and updated by the information exchange (i.e., average) process. Consequently. FedSpa
(RSM) with different mask initializations has a similar performance pattern with FedSpa (DST).

Another interesting observation is that FedSpa (RSM) with different mask initialization cannot outperform FedSpa (DST) in both the two groups of Non-IID settings. This indicates that the DST
mask searching process is effective to achieve a superior performance of FedSpa in Non-IID setting.

**Weight recovery w/ or w/o gradient information. Recall that in FedSpa (DST), we proposed to**
use gradient information to recover the pruned weights, which is empirically proven in (Evci et al.,
2020) to outperform its random recovery counterpart in Set (Mocanu et al., 2018). Specifically,
for gradient information-based recovery, the weight coordinates with the top-αt magnitude of the
gradient would be recovered, while for random recovery, the coordinates are recovered randomly.
To demonstrate the impact of the weight recovery method over FedSpa (DST), in Figure 7, we
compare the gradient information-based recovery with random recovery. Our experimental result
demonstrates that recovery with gradient information could slightly accelerate the convergence and
enhance the final accuracy in our FedSpa framework.

Figure 7: Recovery with gradient information or random recovery. Sparsity is fixed to 0.5.

**Global model vs. Personalized model. In our main experimental result, all the testings are con-**
ducted by clients based on their own personalized models. But it is interesting to evaluate whether


-----

Table 5: Wall time of FedSpa (DST) for local training and mask searching.

Wall Time Wall Time Ratio
Task
(Train) (Mask Search) (Mask Search/Train)

EMNIST-LeNet (CPU) 1.03±0.04s 0.09±0.0s 8.92%±0.6
CIFAR10-VGG11 (CPU) 11.4±0.25s 2.19±0.11s 19.19%±1.09
CIFAR100-Resnet18 (CPU) 28.61±0.38s 3.7±0.28s 12.93%±1.08
EMNIST-LeNet5 (GPU) 0.39±0.01s 0.02±0.0s 5.66%±0.25
CIFAR10-VGG11 (GPU) 1.56±0.03s 0.22±0.01s 14.3%±0.48
CIFAR100-Resnet18 (GPU) 2.71±0.01s 0.33±0.01s 12.06%±0.2

the global model trained by FedSpa itself could converge, or even could achieve commensurate performance with the global model trained by general FL solution (e.g. FedAvg). As demonstrated
by Figure 8, we empirically find that in the IID setting, the global model trained by FedSpa cannot
recover the performance of that trained by FedAvg, and a considerable performance drop is also observed in the Non-IID setting. Another observation is that the global model of FedSpa surprisingly
maintains roughly the same performance as its personalized models in the IID setting, but conceivably suffers significant performance loss in the Non-IID setting. This corroborates our conclusion
that sub-networks extracted from a global model may potentially outperform the full model, under
the condition that the data distributions of clients are skewed (or heterogeneous).

Figure 8: Global model vs. Personalized models. Sparsity of FedSpa is fixed to 0.5.

**Wall time. Recall that we do an additional mask searching procedure in FedSpa (DST), which might**
possibly induce extra wall time on the local devices. We show in Table 5 the wall time used for
training and mask searching in one single local round. The sparsity used for this experiment is fixed
to 0.5, while other parameters remain the default setting (see Section 5.1). We use one single 1080Ti
to perform training for the GPU-based experiment, while the CPU-based experiment is conducted on
an Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz with 8 cores. Our experimental results confirm
that the mask searching process only accounts for a small portion of wall time (approximately 5% _−_
20%) for the entire computation time on the local devices.

C PROOF OF THEOREM 1

In this section, for sake of readability, we first clarify the notations and reiterate several facts that we
use in our proof. Then we present several lemmas that are commonly used in the FL literature (see
(Karimireddy et al., 2020; Xu et al., 2021)). Later, several key lemmas are listed with exhaustive
proof, and finally, the proof of our main theorem is given by exploiting the listed lemmas and facts.

C.1 NOTATIONS AND FACTS

Throughout the proof, we assume _τ_ [equivalent to][ P]τ[N]=0[−][1][,][ P]k [equivalent to][ P]k[K]=1[, and][ P]k,τ

equivalent to _k=1_ _Nτ_ =0−1 [unless otherwise specified. In our proof, we reuse most of the notations]

[P]

[P][K] P


-----

from our problem formulation part. We use gk,t,τ ( ˜wk,t,τ ) = **_w˜k,t,τ_** **_wk,t,τ_** ; ξk,t,τ ) to denote the
stochastic gradient of client k in round t and at step τ . _∇_ _[L][( ˜]_

Then as per our formulation in Section 4.1, we reiterate the following facts, which would be heavily
used in our proof.
**Fact 1 (Local step). As per Eq. (3), one local step of client’s update can be formalized as follows:**

**_w˜k,t,τ_** +1 = ˜wk,t,τ − _ηmk,t ⊙_ **_gk,t,τ_** ( ˜wk,t,τ ), (6)

_where gk,t,τ_ ( ˜wk,t,τ ) is the stochastic gradient over the sparse model weights ˜wk,t,τ _, and ˜wk,t,0 =_
**_mk,t ⊙_** **_wt is the synchronized local weights at the beginning of a communication round._**
**Fact 2 (Local update from client k). The local update of clients can be formalized as follows:**


**_Uk,t = ˜wk,t,0 −_** **_w˜k,t,N = η_**


**_mk,t ⊙_** **_gk,t,τ_** ( ˜wk,t,τ ). (7)


**Fact 3 (Server’s update). The server aggregates the sparse update by averaging, which can be**
_formalized as follows:_

**_wt+1 = wt_** **_Uk,t = wt_** **_mk,t_** **_gk,t,τ_** ( ˜wk,t,τ ). (8)
_−_ _S[1]_ _kX∈St_ _−_ _S[η]_ _k∈XSt,τ_ _⊙_

**Fact 4 (Global and local loss). The local loss of a client is denoted by Fk( ˜wk), and is formulated**
_as:_
_Fk( ˜wk) = E[_ (x,y) _k_ ( ˜wk; (x, y))] (9)
_L_ _∼D_
_where ˜wk = m[∗]k_

_[⊙]_ **_[w][, and the global loss can be formalized as follows:]_**


_f_ (w) = [1]


_K_

_Fk( ˜wk) = [1]_

_K_

_k=1_

X


_Fk(m[∗]k_ (10)
_k=1_ _[⊙]_ **_[w][)]_**

X


_and {m[∗]k[}][ in the function are viewed as optimal personalized masks which satisfies the sparse]_
_volume constraint ∥1 −_ **_m[∗]k[∥][0][ =][ β][ for all][ k][.]_**

C.2 AUXILIARY LEMMAS

In the following. we shall present several common lemmas that are heavily used in the FL literature.
**Lemma 1 (Cauchy-Schwarz). Assume arbitrary vector sequences {ak}k=1,...,K and {bk}k=1,...,K** _,_
_Cauchy-Schwarz inequality implies:_


_K_ _K_

**_ak_** **_bk_**
_k=1_ _∥_ _∥[2]! k=1_ _∥_ _∥[2]_

X X


(11)

(12)


**_akbk_**

_≤_

_k=1_

X

_by taking bk = 1, we also have:_

_K_

**_ak_**

_k=1_

X


**_ak_**
_∥_ _∥[2]_
_k=1_

X


_≤_ _K_


**Lemma 2 (Separating mean and variance, Lemma B.3 (Xu et al., 2021)). Let {a1, . . ., aτ** _} be_
_τ random vectors in R[d]_ _._ _Suppose that {ai −_ **_ξi} form a martingale difference sequence, i.e._**

E [ai **_ξi_** **_a1, . . ., ai_** 1] = 0, and suppose that their variance is bounded by E **_ai_** **_ξi_**
_−_ _|_ _−_ _∥_ _−_ _∥[2][i]_ _≤_

_σ[2]. Then, the following inequality holds:_ h

_τ_ 2[] _τ_ 2

E **_ai_** 2 **_ξi_** + 2τσ[2].

 _≤_

_i=1_ _i=1_

X X

 

**Lemma 3 (Relaxed triangle inequality, Lemma 3 (Karimireddy et al., 2020)). Let vi and vj be**
_vectors in R[d]. Then the following inequality holds true for any a > 0:_


**_vi + vj_** (1 + a) **_vi_** + 1 + [1]
_∥_ _∥[2]_ _≤_ _∥_ _∥[2]_ _a_



**_vj_** _._ (13)
_∥_ _∥[2]_


-----

**Lemma 4. For random vector v1 satisfying E[v1] =**

**_v2 is independent with v1, we have:_**


_, and assume another random vector_




E[∥v1 + v2∥[2]] = E[∥v1∥[2]] + E∥v2∥[2] (14)


_Proof._


E[∥v1 + v2∥[2]] = E⟨v1 + v2, v1 + v2⟩

= E∥v1∥[2] + E∥v2∥[2] + 2E⟨v1, v2⟩

= E∥v1∥[2] + E∥v2∥[2] + 2⟨Ev1, Ev2⟩

= E∥v1∥[2] + E∥v2∥[2]


(15)


This completes the proof.

C.3 KEY LEMMAS

In this section, we present several important lemmas that would be used in our formal proof. All the
presented claims are rigorously proved.

**Lemma 5 (Smoothness of f** (w)). Assume ˜wk = m[∗]k _k_
_smoothness for f_ (w) = _K[1]_ _k_ _[F][k][( ˜]wk), i.e., for any w1, w[⊙]_ 2[w] ∈[ for any]R[d], we have:[ m][∗] _[∈{][0][,][ 1][}][d][, we have][ L][-]_

P

_f_ (w1) _f_ (w2) _L_ **_w1_** **_w2_** _._ (16)
_∥∇_ _−∇_ _∥≤_ _∥_ _−_ _∥_

_Proof._


(m[∗]k _k_ _k_ _k_

_[⊙∇][F][k][(][m][∗]_ _[⊙]_ **_[w][1][)][ −]_** **_[m][∗]_** _[⊙∇][F][k][(][m][∗]_ _[⊙]_ **_[w][2][))]_**

**_m[∗]k_** _k_ _k_
_∥_ _[⊙]_ [(][∇][F][k][(][m][∗] _[⊙]_ **_[w][1][)][ −∇][F][k][(][m][∗]_** _[⊙]_ **_[w][2][))][∥]_**

_Fk(m[∗]k_ _k_
_∥∇_ _[⊙]_ **_[w][1][)][ −∇][F][k][(][m][∗]_** _[⊙]_ **_[w][2][)][∥]_**

**_m[∗]k_**
_k_ _∥_ _[⊙]_ [(][w][1] _[−]_ **_[w][2][)][∥]_**

X


_f_ (w1) _f_ (w2) =
_∥∇_ _−∇_ _∥_ _K_

_≤_ _K[1]_

_≤_ _K[1]_

(a)
_L_ [1]
_≤_ _K_


(17)


_L_ **_w1_** **_w2_**
_≤_ _∥_ _−_ _∥_

where the first equality holds by the Fact 4 and inequality (a) is due to Assumption 3. This completes
the proof.

**Lemma 6following bounding for the expected average gradient: (Separating mean and variance of stochastic gradient). For mk,t ∈{0, 1}[d], We have the**

2[] 2[]

1 1

Et  _S_ _k∈XSt,τ_ **_mk,t ⊙_** **_gk,t,τ_** ( ˜wk,t,τ ) _≤_ 2Et  _S_ _k∈XSt,τ_ **_mk,t ⊙∇Fk( ˜wk,t,τ_** ) + [2][Nσ]S [2] _._

   
    (18)

_where Et[·] denotes the expectation over all the randomness of round t._


-----

_Proof. We view_ _S[1]_ **_[m][k,t][ ⊙]_** **_[g][k,t,τ]_** [( ˜]wk,t,τ ) for all k and τ as stochastic vectors. By the unbiasedness

of stochastic gradient, we know their variance satisfies:



[1] **_wk,t,τ_** ) E[ [1] **_wk,t,τ_** ) ]
_||_ _S_ **_[m][k,t][ ⊙]_** **_[g][k,t,τ]_** [( ˜] _−_ _S_ **_[m][k,t][ ⊙]_** **_[g][k,t,τ]_** [( ˜] _||[2]_

= [1] **_wk,t,τ_** ) _Fk( ˜wk,t,τ_ ))
_||_ _S_ **_[m][k,t][ ⊙]_** [(][g][k,t,τ] [( ˜] _−∇_ _||[2]_

**_wk,t,τ_** ) _Fk( ˜wk,t,τ_ )

_≤_ _S[1][2][ ||][g][k,t,τ]_ [( ˜] _−∇_ _||[2]_

_≤_ _S[σ][2][2]_


(19)


where the first inequality holds since mk,t 0, 1 and the last inequality is due to Assumption 2.
_∈{_ _}[d]_

As per the variance given above, and directly apply Lemma 2, the claim immediately shows.

**Lemma 7. We have the following error bound by introducing personalized masks:**


_∥mk,t ⊙∇Fk( ˜wk,t,τ_ ) − **_m[∗]k[∇][F][k][( ˜]wk,t)∥[2]_** _≤∥∇Fk( ˜wk,t,τ_ ) −∇Fk( ˜wk,t)∥[2] + dist(mk,t, m[∗]k[)][B][2]

(20)

_and,_


**_mk,t_** _Fk( ˜wk,t)_ _f_ (wt) 3(d _β)G[2]+ [3]_ _B[2](dist(mk,t, mk′,t)+dist(mk′,t, m[∗]k[′]_ [))]
_∥_ _⊙∇_ _−∇_ _∥[2]_ _≤_ _−_ _K_

_k[′]_

X

(21)
_where dist(m1, m2) is the hamming distance between two masks._


_Proof. Let_ _Fk( ˜wk,t,τ_ ) and _Fk( ˜wk,t) be the derivative over the j-th weight coordinates._
_∇[(][j][)]_ _∇[(][j][)]_
Define the support of a vector v ∈ R[d] as supp(v) = {j : v[(][j][)] = 0̸ _}. The left hand side of the first_
claim can be rewritten and bounded as follows:

**_mk,t_** _Fk( ˜wk,t,τ_ ) **_m[∗]k_** **_wk,t)_**
_∥_ _⊙∇_ _−_ _[⊙∇][F][k][( ˜]_ _∥[2]_

= _Fk( ˜wk,t,τ_ ) _Fk( ˜wk,t)_

_∥∇[(][j][)]_ _−∇[(][j][)]_ _∥[2]_
_j∈supp(mk,tX)∩supp(m[∗]k[)]_

+ _Fk( ˜wk,t,τ_ ) + _Fk( ˜wk,t)_

_∥∇[(][j][)]_ _∥[2]_ _∥∇[(][j][)]_ _∥[2]_
_j∈supp(mk,tX)andj /∈supp(m[∗]k[)]_ _j /∈supp(mk,tX)andj∈supp(m[∗]k[)]_


(a)
_≤_

(b)


_Fk( ˜wk,t,τ_ ) _Fk( ˜wk,t)_ +
_∥∇[(][j][)]_ _−∇[(][j][)]_ _∥[2]_
_j∈supp(mk,tX)∩supp(m[∗]k[)]_


_B[2]_
_j∈supp(Xmk,t⊕m[∗]k[)]_


_∥∇[(][j][)]Fk( ˜wk,t,τ_ ) −∇[(][j][)]Fk( ˜wk,t)∥[2] + dist(mk,t, m[∗]k[)][B][2]
_j∈supp(mk,tX)∩supp(m[∗]k[)]_

_∥∇[(][j][)]Fk( ˜wk,t,τ_ ) −∇[(][j][)]Fk( ˜wk,t)∥[2] + dist(mk,t, m[∗]k[)][B][2]

X


_≤∥∇Fk( ˜wk,t,τ_ ) −∇Fk( ˜wk,t)∥[2] + dist(mk,t, m[∗]k[)][B][2]


(22)


where inequality (a) is true since we have _Fk( ˜wk,t,τ_ ) _B[2]_ and _Fk( ˜wk,t)_ _B[2]_ by
_∥∇[(][j][)]_ _∥[2]_ _≤_ _∥∇[(][j][)]_ _∥[2]_ _≤_
coordinate-wise bounded gradient Assumption 4. Inequality (b) is due to the definition of hamming
distance, dist(v1, v2) = _j∈supp(v1⊕v2)[. This shows our first claim.]_

[P]


-----

Following the same technique, we expand the left hand side of the second claim using Cauchy
Schwarz inequality in Lemma 1, as follows:

**_mk,t_** _Fk( ˜wk,t)_ _f_ (wt)
_∥_ _⊙∇_ _−∇_ _∥[2]_

= **_mk,t_** _Fk( ˜wk,t)_ **_m[∗]k[′][ ⊙∇][F][k][′]_** [( ˜]wk′,t)
_∥_ _⊙∇_ _−_ _K[1]_ _k[′]_ _∥[2]_

X

3 **_mk,t_** _Fk( ˜wk,t)_ **_mk,t_** _Fk′_ ( ˜wk′,t)
_≤_ _∥_ _⊙∇_ _−_ _K[1]_ _k[′]_ _⊙∇_ _∥[2]_

X


+ 3
_∥_ _K[1]_


(mk,t _Fk′_ ( ˜wk′,t) **_mk′,t_** _Fk′_ ( ˜wk′,t))
_k[′]_ _⊙∇_ _−_ _⊙∇_ _∥[2]_

X


+ 3
_∥_ _K[1]_


_k[′]_ (mk′,t ⊙∇Fk′ ( ˜wk′,t) − **_m[∗]k[′][ ⊙∇][F][k][′]_** [( ˜]wk′,t))∥[2]

X


(23)


3 **_mk,t_** _Fk( ˜wk,t)_
_≤_ _∥_ _⊙∇_ _−_ _K[1]_


**_mk,t_** _Fk′_ ( ˜wk′,t)
_k[′]_ _⊙∇_ _∥[2]_

X


+ [3]


**_mk,t_** _Fk′_ ( ˜wk′,t) **_mk′,t_** _Fk′_ ( ˜wk′,t)
_k[′]_ _∥_ _⊙∇_ _−_ _⊙∇_ _∥[2]_

X


+ [3]


_k[′]_ _∥(mk[′],t ⊙∇Fk[′]_ ( ˜wk[′],t) − **_m[∗]k[′][ ⊙∇][F][k][′]_** [( ˜]wk[′],t))∥[2]

X


3(d _β)G[2]_ + [3]
_≤_ _−_ _K_

=3(d _β)G[2]_ + [3]
_−_ _K_


_dist(mk,t, mk′,t)B[2]_ + [3]

_K_

_k[′]_

X


_dist(mk′,t, m[∗]k[′]_ [)][B][2]
_k[′]_

X


_B[2](dist(mk,t, mk′,t) + dist(mk′,t, m[∗]k[′]_ [))]
_k[′]_

X


For the last inequality, we use the same technique as in the first claim to bound the second term and
the third term, and the first term is bounded by Lemma 8.

**Lemma 8. Let ∥1 −** **_m∥0 = β be the sparse volume of a mask m, and suppose m ∈{0, 1}[d], by_**
_assumption 4, we have the following relation:_


**_m_** ( _Fk( ˜wk,t)_
_∥_ _⊙_ _∇_ _−_ _K[1]_


_Fk′_ ( ˜wk′,t)) (d _β)G[2]_ (24)
_∇_ _∥[2]_ _≤_ _−_
_k[′]_

X


_and,_


_Fk( ˜wk,t)_
_∥∇_ _−_ _K[1]_


**_m_** _Fk′_ ( ˜wk′,t) (d _β)G[2]_ + βB[2] (25)
_⊙∇_ _∥[2]_ _≤_ _−_
_k[′]_

X


_where d is the dimension of the models (or identically, the dimension of the mask m)._

_Proof. The proof of this lemma follows a similar technique in Lemma 7. Let_ _Fk( ˜wk,t) be the_
_∇[(][j][)]_
derivative over the j-th weight coordinate. Define the support of a vector v ∈ R[d] as supp(v) = {j :
**_v[(][j][)]_** = 0̸ _}. The left hand side (L.H.S) of our first claim can be bounded by:_


**_m_** ( _Fk( ˜wk,t)_
_∥_ _⊙_ _∇_ _−_ _K[1]_


_Fk′_ ( ˜wk′,t))
_∇_ _∥[2]_
_k[′]_

X


= _Fk( ˜wk,t)_

_∥∇[(][j][)]_ _−_ _K[1]_
_j∈suppX(m)_

_≤(d −_ _β)G[2]_


(26)


_Fk′_ ( ˜wk′,t)
_∇[(][j][)]_ _∥[2]_
_k[′]_

X


where the last inequality is obtained from the sparsity constraint and the coordinate-wise gradient
dissimilarity Assumption 1. This shows our first claim.


-----

For our second claim, we expand its L.H.S as follows:


_Fk( ˜wk,t)_
_∥∇_ _−_ _K[1]_


**_m_** _Fk′_ ( ˜wk′,t)
_⊙∇_ _∥[2]_
_k[′]_

X


= _Fk( ˜wk,t)_ _Fk′_ ( ˜wk′,t) + _Fk( ˜wk,t)_

_∥∇[(][j][)]_ _−_ _K[1]_ _∇[(][j][)]_ _∥[2]_ _∥∇[(][j][)]_ _∥[2]_
_j∈suppX(m)_ Xk[′] _j /∈suppX(m)_

_≤(d −_ _β)G[2]_ + βB[2],

where the last inequality is due to Assumption 1 and Assumption 4. It completes the proof.


(27)


**Lemma 9 (Drift towards Synchronized Point). For any t** 1, . . ., T _, τ_ 0, . . ., N _, and_
1 _∈{_ _}_ _∈{_ _}_
_learning rate satisfies η ≤_ 16LN _[, we have the following claim:]_

1

Et **_w˜k,t,τ_** **_w˜k,t_** 5Nη[2](σ[2] + 18N Φt) + 30N [2]η[2] _f_ (wt) (28)

_K_ _k_ _∥_ _−_ _∥[2][]_ _≤_ _∥∇_ _∥[2]_

_where Φt =_ _KX1_ _k[((][d][ −]_ _[β][)][G][2][ +]_ _K1_ _k[′][ B][2][(][dist][(][m][k,t][,][ m][k][′][,t][) +][ dist][(][m][k][′][,t][,][ m]k[∗][′]_ [)))][ and][ E][t][[][·][]]

_denotes the expectation over all the randomness of round t._

P P

_Proof. We follow the basic techniques from (Reddi et al., 2020) to prove this lemma._

We first assume that:



-  O = mk,t ⊙ **_gk,t,τ_** _−1( ˜wk,t,τ_ _−1) −_ **_mk,t ⊙∇Fk( ˜wk,t,τ_** _−1)_

-  P = mk,t ⊙∇Fk( ˜wk,t,τ _−1) −_ **_mk,t ⊙∇Fk( ˜wk,t)_**

-  Q = mk,t _Fk( ˜wk,t)_ _f_ (wt)
_⊙∇_ _−∇_

We can expand ˜wk,t,τ as follows:


_k_ Et _∥w˜k,t,τ −_ **_w˜t∥[2][]_**

X 

Et **_w˜k,t,τ_** 1 **_w˜t_** _ηmk,t_ **_gk,t,τ_** 1( ˜wk,t,τ 1)
_k_ _∥_ _−_ _−_ _−_ _⊙_ _−_ _−_ _∥[2][]_

X 

Et **_w˜k,t,τ_** 1 **_w˜t_** _η(O + P + Q +_ _f_ (wt))
_k_ _∥_ _−_ _−_ _−_ _∇_ _||[2][]_

X1 

2N _−1_ Et **_w˜k,t,τ_** 1 **_w˜t_** + _[η][2]_ Et _O_ + [6][Nη][2]

_K_ _∥_ _−_ _−_ _∥[2]_ _K_ _∥_ _∥[2]_ _K_

X X


Fact 1 1
=

_K_

= [1]

_K_

1 +


(29)


Et∥P _∥[2]_


+ [6][Nη][2]


Et _Q_ + [6][Nη][2]
_∥_ _∥[2]_ _K_


_f_ (wt)
_∥∇_ _∥[2]_


where the last inequality follows from Lemma 3 and Lemma 4. Explicitly, we use Lemma 4 to treat
the stochastic term with Et∥O∥[2], and then we use Lemma 3 with a = 2N − 1 to separate the other
four terms.

Then we proceed by separately bounding the components in the above inequality.


**Bounding the second term:**
_η[2]_

Et _O_ = _[η][2]_

_K_ _∥_ _∥[2]_ _K_

_k_ _k_

X X

= _[η][2]_

_K_

_k_

X

_≤_ _[η]K[2]_

_k_

X

_≤_ _η[2]σ[2]_


Et **_mk,t_** **_gk,t,τ_** 1( ˜wk,t,τ 1) **_mk,t_** _Fk( ˜wk,t,τ_ 1)
_∥_ _⊙_ _−_ _−_ _−_ _⊙∇_ _−_ _∥[2]_

Et **_mk,t_** (gk,t,τ 1( ˜wk,t,τ 1) _Fk( ˜wk,t,τ_ 1))
_∥_ _⊙_ _−_ _−_ _−∇_ _−_ _∥[2]_

Et **_gk,t,τ_** 1( ˜wk,t,τ 1) _Fk( ˜wk,t,τ_ 1)
_∥_ _−_ _−_ _−∇_ _−_ _∥[2]_


(30)


-----

where the last inequality holds by Assumption 2.

**Bounding the third term:**


6Nη[2]


Et _P_ = [6][Nη][2]
_∥_ _∥[2]_ _K_

_≤_ [6][Nη]K [2]


Et **_mk,t_** _Fk( ˜wk,t,τ_ 1) **_mk,t_** _Fk( ˜wk,t)_
_∥_ _⊙∇_ _−_ _−_ _⊙∇_ _∥[2]_

Et _Fk( ˜wk,t,τ_ 1) _Fk( ˜wk,t)_
_∥∇_ _−_ _−∇_ _∥[2]_


(31)

(32)


(6Nη[2]L[2]) [1]
_≤_ _K_

**Bounding the fourth term:**


Et **_w˜k,t,τ_** 1 **_w˜k,t_**
_∥_ _−_ _−_ _∥[2]_


6Nη[2]

_K_

= [6][Nη][2]

_K_

_≤_ [18]K[Nη][2]


Et∥Q∥[2]

Et∥mk,t ⊙∇Fk( ˜wk,t) −∇f (wt)∥[2]


((d _β)G[2]_ + [1]
_−_ _K_


_B[2](dist(mk,t, mk′,t) + dist(mk′,t, m[∗]k[′]_ [)))][,]
_k[′]_

X


where we use the second claim in Lemma 7 in the last inequality.

Let Φt = _K[1]_ _k[((][d]_ _[−]_ _[β][)][G][2][ +][ 1]K_ _k[′][ B][2][(][dist][(][m][k,t][,][ m][k][′][,t][)+]_ _[dist][(][m][k][′][,t][,][ m]k[∗][′]_ [)))][. The bound can]

be simplified as follows:

P P


6Nη[2]


Et∥Q∥[2] _≤_ 18Nη[2]Φt. (33)


**Putting together: Plugging all the components into Eq.(29), the following result immediately**
follows:

1

Et **_w˜k,t,τ_** **_w˜k,t_**

_K_ _k_ _∥_ _−_ _∥[2][]_

X 

1
(1 + Et **_w˜k,t,τ_** 1 **_w˜k,t_** + η[2]σ[2] + 18Nη[2]Φt + 6Nη[2] _f_ (wt)
_≤_ 2N − 1 [+ 6][Nη][2][L][2][) 1]K _k_ _∥_ _−_ _−_ _∥[2]_ _∥∇_ _∥[2]_

X

1
(1 + Et **_w˜k,t,τ_** 1 **_w˜k,t_** + η[2](σ[2] + 18N Φt) + 6Nη[2] _f_ (wt) _,_
_≤_ _N −_ 1 [) 1]K _k_ _∥_ _−_ _−_ _∥[2]_ _∥∇_ _∥[2]_

X

(34)

where the last inequlity holds by our assumption η ≤ 16LN1 [.]

**Unrolling the recursion, we obtain the following results:**

1

Et **_w˜k,t,τ_** **_w˜k,t_**

_K_ _k_ _∥_ _−_ _∥[2][]_

X 

_N_ _−1_ 1

_≤_ (1 + _N_ 1 [)][τ][ ]η[2](σ[2] + 18N Φt) + 6Nη[2]∥∇f (wt)∥[2][] (35)

_τ_ =0

_−_

X

1
(N 1) (1 + _η[2](σ[2]_ + 18N Φt) + 6Nη[2] _f_ (wt)
_≤_ _−_ _×_ _N_ 1 [)][N][ −] [1] _∥∇_ _∥[2][]_
 _−_

5Nη[2](σ[2] + 18N Φt) + 30N [2]η[2]  f (wt)
_≤_ _∥∇_ _∥[2]_


_N1−1_ [)][N][ −] [1] _≤_ 5 for N ≥ 1. This completes the proof.



The last inequality holds since (1 +



-----

C.4 FORMAL PROOF

We start our proof by expanding f (wt+1) under its smoothness condition (see Lemma 5), which
indicates that:

Et [f (wt+1) | wt]

_f_ (wt) _f_ (wt), Et[wt+1 **_wt]_** + _[L]_
_≤_ _−⟨∇_ _−_ _⟩_ 2 [E][t][||][w][t][+1][ −] **_[w][t][||][2]_**

2

Fact 3 1
= f (wt) _ηEt_ _f (wt),_ [1] **_Uk,t_** + _[L]_ **_Uk,t_**
_−_ "*∇ _S_ _kX∈St_ +# 2 [E][t] _S_ _kX∈St_

2

Fact 2 1
= f (wt) _N_ _f (wt), Et_ **_mk,t_** _Fk( ˜wk,t,τ_ ) + _[L]_ **_Uk,t_**
_−_ _N[η]_ -  _∇_  _K_ Xk,τ _⊙∇_ + 2 [E][t] _S_ _kX∈St_

 [1] 

_f (wt)_ **_mk,t_** _Fk( ˜wk,t,τ_ ) _N_ _f (wt)_
_≤_ _−_ _[ηN]2_ _[||∇][f][ (][w][t][)][ ||][2][ +][ η]2N_ [E][t][||][ 1]K Xk,τ _⊙∇_ _−_ _∇_ _||[2]_

_T1_

2

| {z 1 }

+ _[L]_ **_Uk,t_**

2 [E][t] _S_ _kX∈St_

_T2_
(36)
| {z }

where the last inequality holds since −ab ≤ [1]2 [((][b][ −] _[a][)][2][ −]_ _[a][2][)][, and][ E][t][[][·][]][ is the expectation over all]_

the randomness in round t.

In the following, we shall separately bound T1 and T2.

**Bounding T1:**


_T1 =_ _[η]_

2N [E][t][||][ 1]K


**_mk,t_** _Fk( ˜wk,t,τ_ ) _N_ _f (wt)_
_k,τ_ _⊙∇_ _−_ _∇_ _||[2]_

X


_η_

2N [E][t][||][ 1]K


**_mk,t_** _Fk( ˜wk,t,τ_ ) _N_ [1]
_k,τ_ _⊙∇_ _−_ _K_

X


Fact 4


**_m[∗]k_** **_wk,t)_**

_[⊙∇][F][k][( ˜]_ _||[2]_


= _[η]_

2N [E][t][||][ 1]K


(mk,t _Fk( ˜wk,t,τ_ ) **_m[∗]k_** **_wk,t))_**
_k,τ_ _⊙∇_ _−_ _[⊙∇][F][k][( ˜]_ _||[2]_

X


(37)


(a)
_≤_ 2[η]K

(b)
_≤_ 2[η]K

_≤_ _[ηL]2K[2]_


Et **_mk,t_** _Fk( ˜wk,t,τ_ ) **_m[∗]k_** **_wk,t)_**
_k,τ_ _||_ _⊙∇_ _−_ _[⊙∇][F][k][( ˜]_ _||[2]_

X

(∥∇Fk( ˜wk,t,τ ) −∇Fk( ˜wk,t)∥[2] + dist(mk,t, m[∗]k[)][B][2][)]
_k,τ_

X


Et ˜wk,t,τ **_w˜k,t_** + _[ηN]_
_k,τ_ _||_ _−_ _||[2]_ 2K

X


_dist(mk,t, m[∗]k[)][B][2]_


where inequality (a) is due to Cauchy-Schwarz inequality (i.e., Lemma 1), (b) follows from the first
claim in Lemma 7, and the last inequality holds by Assumption 3.

Plugging the results of Lemma 9, we obtain that:


_T1_ (5Nη[2](σ[2] + 18N Φt) + 30N [2]η[2] _f_ (wt) ) + _[ηN]_
_≤_ _[ηL]2[2][N]_ _∥∇_ _∥[2]_ 2K


_dist(mk,t, m[∗]k[)][B][2]_

(38)
_dist(mk,t, m[∗]k[)][B][2]_


_≤_ [5][N][ 2]2[η][3][L][2]


(σ[2] + 18N Φt) + 15N [3]η[3]L[2] _f_ (wt) + _[ηN]_
_∥∇_ _∥[2]_ 2K


-----

**Bounding T2:**


_T2 =_ _[L]_

2 [E][t]

Fact 2
= _[Lη][2]_

2 [E][t]


**_Uk,t_**
_kX∈St_

2[]

1

**_mk,t_** **_gk,t,τ_** ( ˜wk,t,τ )

_S_ _k∈XSt,τ_ _⊙_



2[]

1

**_mk,t_** _Fk( ˜wk,t,τ_ ) + _[NLη][2][σ][2]_

_S_ _k∈XSt,τ_ _⊙∇_ _S_


 2[]

1

Et **_mk,t_** _Fk( ˜wk,t,τ_ ) + _[NLη][2][σ][2]_

 _S_ _kX∈St_ _⊙∇_ _S_

 

_T3_
| {z }


(39)


(a)
_≤_ _Lη[2]Et_

_≤Lη[2]N_


where (a) is obtained as per Lemma 6.

**Bounding T3:**


_T3_

= [1]

_S[2][ E][t]_


I _i_ _St_ **_mi,t_** _Fi( ˜wi,t,τ_ ),
_{_ _∈_ _}_ _⊙∇_

-  Xi∈[K]


I _j_ _St_ **_mj,t_** _Fj( ˜wj,t,τ_ )
_{_ _∈_ _}_ _⊙∇_
_jX∈[K]_


= [1]

_S[2][ E][t]_

= [1]

_S[2][ E][t]_


ESt [I _i_ _St_ _j_ _St_ ] **_mi,t_** _Fi( ˜wi,t,τ_ ), mj,t _Fj( ˜wj,t,τ_ )
_{_ _∈_ _∩_ _∈_ _}_ _⟨_ _⊙∇_ _⊙∇_ _⟩_
_i,j∈[KX],j≠_ _i,τ_

+ ESt [I _i_ _St_ ] **_mi,t_** _Fi( ˜wi,t,τ_ )

_i_ _{_ _∈_ _}_ _∥_ _⊙∇_ _∥[2]#_

X


_S(S_ 1)
_−_ **_wi,t,τ_** ), mj,t _Fj( ˜wj,t,τ_ ) +

_K(K_ 1) _⊙∇_ _⟩_
_−_ _[⟨][m][i,t][ ⊙∇][F][i][( ˜]_


_S_

**_wi,t,τ_** )
_K_ _[∥][m][i,t][ ⊙∇][F][i][( ˜]_ _∥[2]_


_i,j∈[K],j≠_ _i_




_i,j∈[K]_

 [X]


_S(S_ 1)
_−_ **_wi,t,τ_** ), mj,t _Fj( ˜wj,t,τ_ )

_K(K_ 1) _⊙∇_ _⟩_
_−_ _[⟨][m][i,t][ ⊙∇][F][i][( ˜]_

_S(K_ _S)_

+ _−_ **_wi,t,τ_** )

_i_ _K(K −_ 1) _[∥][m][i,t][ ⊙∇][F][i][( ˜]_ _∥[2]#_

X


= [1]

_S[2][ E][t]_


(K − _S)_

_SK(K −_ 1)


**_mk,t_** _Fk( ˜wk,t,τ_ )
_⊙∇_ _∥[2]_

_T4_
{z }


**_mk,t_** _Fk( ˜wk,t,τ_ )
_∥_ _⊙∇_ _∥[2]_

_T5_
{z }


_≤Et_


_K_ [2][ ∥]


(40)


where the last inequality holds since _SKS(−K1_ 1) [=] _SKS[2]−1SK_

_−_ _−_ _[≤]_


1

_K[2][ .]_


_S_

_SK[2][ =]_


-----

**Bounding T4:**


_K_

1

2 [( 1]


_T4 =_

Lemma 1


**_mk,t_** _Fk( ˜wk,t,τ_ ) _f_ (wt)) + _f_ (wt))
_⊙∇_ _−∇_ _∇_


+ 2 _f_ (wt)
_∥∇_ _∥[2]_


**_mk,t_** _Fk( ˜wk,t,τ_ ) _f_ (wt)
_⊙∇_ _−∇_


Fact 2 1
= 2

_K_

Lemma 1 2

_≤_ _K_

Lemma 7 2

_≤_ _K_

_≤_ [2]K[L][2]


+ 2 _f_ (wt)
_∥∇_ _∥[2]_


(mk,t _Fk( ˜wk,t,τ_ ) **_m[∗]k_** **_wk,t))_**
_⊙∇_ _−_ _[⊙∇][F][k][( ˜]_


(41)

(42)


_∥mk,t ⊙∇Fk( ˜wk,t,τ_ ) − **_m[∗]k[∇][F][k][( ˜]wk,t)∥[2]_** + 2 ∥∇f (wt)∥[2]

(∥∇Fk( ˜wk,t,τ ) −∇Fk( ˜wk,t)∥[2] + dist(mk,t, m[∗]k[)][B][2][) + 2][ ∥∇][f] [(][w][t][)][∥][2]


**_w˜k,t,τ_** **_w˜k,t_** + [2]
_∥_ _−_ _∥[2]_ _K_


_dist(mk,t, m[∗]k[)][B][2][ + 2][ ∥∇][f]_ [(][w][t][)][∥][2][,]


where the last inequality is obtained by the L-smoothness Assumption 3.

**Bounding T5:**


(K _S)_
_T5 =_ _−_

_SK(K −_ 1)

_≤_ _SK[3(][K](K[ −]_ _[S]1)[)]_

_−_

(a)
_≤_ [3]SK[L][2][(]([K]K[ −] _[S]1)[)]_

_−_


**_mk,t_** _Fk( ˜wk,t,τ_ )
_∥_ _⊙∇_ _∥[2]_

( **_mk,t_** _Fk( ˜wk,t,τ_ ) **_mk,t_** _Fk( ˜wk,t)_
_∥_ _⊙∇_ _−_ _⊙∇_ _∥[2]_

+ **_mk,t_** _Fk( ˜wk,t)_ _f_ (wt) + _f_ (wt) )
_∥_ _⊙∇_ _−∇_ _∥[2]_ _∥∇_ _∥[2]_

3(K _S)_
**_w˜k,t,τ_** **_w˜k,t_** + _−_ 3(d _β)G[2]_
_∥_ _−_ _∥[2]_ _S(K −_ 1)K _k_ _−_

X  


+ [3]

_K_

_≤_ [3]K[L][2] X


_B[2](dist(mk,t, mk′,t) + dist(mk′,t, m[∗]k[′]_ [))) + 3(][K][ −] _[S][)]_

_S(K_ 1) _[∥∇][f]_ [(][w][t][)][∥][2]

_k[′]_ _−_

X

_∥w˜k,t,τ −_ **_w˜k,t∥[2]_** + 9Φt + 3∥∇f (wt)∥[2],


where the last inequality holds sinceobtained by Assumption 3 and the second claim in LemmaSK(K−−S1) _[≤]_ [1][ under the condition] 7 . _[ S][ ≥]_ [1][. Inequality (a) is]

**Summing T4 and T5, we have the following bounding for T3:**

_T3_ **_w˜k,t,τ_** **_w˜k,t_** + [2] _dist(mk,t, m[∗]k[)][B][2][ + 9Φ][t]_ [+ 5][∥∇][f] [(][w][t][)][∥][2] (43)
_≤_ [5]K[L][2] _k_ _∥_ _−_ _∥_ _K_ _k_

X X

Plugging Lemma 9 into the above inequality, we have:
_T3_


5L[2][  ]5Nη[2](σ[2] + 18N Φt) + 30N [2]η[2] _f_ (wt) + [2] _dist(mk,t, m[∗]k[)][B][2][ + 9Φ][t]_ [+ 5][∥∇][f] [(][w][t][)][∥][2]
_≤_ _∥∇_ _∥[2][]_ _K_

_k_

X

=25η[2]L[2]N (σ[2] + 18N Φt) + (150N [2]η[2]L[2] + 5) _f_ (wt) + [2] _dist(mk,t, m[∗]k[)][B][2][ + 9Φ][t]_
_∥∇_ _∥[2]_ _K_

_k_

X

(44)
**Plugging T3 into Inequality (39), we bound T2 as follows:**
_T2_ (150N [4]η[4]L[3] + 5Lη[2]N [2]) _f_ (wt) + 25η[4]L[3]N [3](σ[2] + 18N Φt)
_≤_ _∥∇_ _∥[2]_

+ [2][η][2][N][ 2][L] _dist(mk,t, m[∗]k[)][B][2][ + 9][η][2][N][ 2][L][Φ][t]_ [+][ NLη][2][σ][2] _._ (45)

X


-----

**Plugging T2 and T1 into R.H.S of Inequality (36), we obtain that:**

E [f (wt+1 **_wt)]_** _f (wt)_ _ηN_ ( [1]
_|_ _≤_ _−_ 2 _[−]_ [150][N][ 3][η][3][L][3][ −] [5][LηN][ −] [15][N][ 2][η][2][L][2][)][||∇][f][ (][w][t][)][ ||][2]

+(25η[4]L[3]N [3] + [5][N][ 2][η][3][L][2] )(σ[2] + 18N Φt) + [4][η][2][N][ 2][L][ +][ ηN] _dist(mk,t, m[∗]k[)][B][2]_

2 2K

X


+ 9η[2]N [2]LΦt + _[NLη][2][σ][2]_ _._

_S_

(46)

Taking expectation over the randomness before round t towards both sides of the inequality, it yields:

E [f (wt+1)] ≤ E[f (wt)] − _ηNκE[||∇f (wt) ||[2]] + ρt._ (47)


where κ = 12 2 )(σ[2] +

_[−]_ [150][N][ 3][η][3][L][3][ −] [15][N][ 2][η][2][L][2][ −] [5][NηL][ and][ ρ][t][ = (25][N][ 3][η][4][L][3][ +][ 5][N][ 2][η][3][L][2]

18N Φt) + [4][N][ 2][η]2[2]K[L][+][Nη] _k_ _[dist][(][m][k,t][,][ m]k[∗][)][B][2][ + 9][N][ 2][η][2][L][Φ][t][ +][ Nη][2]S[Lσ][2]_ .

P


By rearanging, and summing from t = 0, . . ., T − 1, the following result immediately shows:


_T −1_

E[ _f (wt)_ + [1]
_||∇_ _||[2]_ _≤_ [E][ [][f][ (][w][0]TηNκ[)]][ −] [E][ [][f][ (][w][T][ )]] _T_
_t=0_

X


_T −1_

_ρt_
_t=0_

X


(48)

_K_

E∥∇f (wt)∥[2]
_k=1_

X


_T −1_

_ρt_
_t=0_

X


+ [1]

_≤_ _[f][ (][w][0]TηNκ[)][ −]_ _[f][ (][w][∗][)]_ _T_


Now we bound the local gradient over the personalized sparse model:


_T −1_

_t=0_

X

_T −1_

_t=0_

X

_T −1_

_t=0_

X


E[||∇Fk ( ˜wk,t) ||[2]
_k=1_

X

_K_

E[||∇Fk ( ˜wk,t) −∇f (wt) + ∇f (wt)||[2]]
_k=1_

X


_TK_

= [1]

_TK_

Lemma 1 3

_≤_ _TK_


_Fk( ˜wk,t)_
_∥∇_ _−_ _K[1]_


**_m[∗]k_** **_wk′,t)_**
_k[′]_ _[⊙∇][F][k][′]_ [( ˜] _∥[2]_

X


_k=1_


+
_∥_ _K[1]_


(m[∗]k **_wk[′],t)_** **_m[∗]k[′][ ⊙∇][F][k][′]_** [( ˜]wk′,t)) + _f_ (wt)
_k[′]_ _[⊙∇][F][k][′]_ [( ˜] _−_ _∥[2]_ _∥∇_ _∥[2]_

X


_T −1_

_t=0_

X


_T −1_

_t=0_

X


(a)


_K_

((d _β)G[2]_ + βB[2] + [1]
_−_ _K_
_k=1_

X


_dist(m[∗]k[,][ m][∗]k[′]_ [)][B][2][) +]
_k[′]_

X


_TK_


_TK_


_T −1_

_t=0_

X


3(d _β)G[2]_ + 3βB[2] + [3]
_≤_ _−_ _K_ [2]


_dist(m[∗]k[,][ m][∗]k[′]_ [)][B][2][ +]
_k[′]_

X


E∥∇f (wt)∥[2]
_k=1_

X


_TK_


(49)


In inequality (a), we use the second claim in Lemma 8 to treat the first term, and the second term is
bounded using a similar technique as in the first claim in Lemma 8.

Let Υ = 3(d _β)G[2]_ + 3βB[2] + _K3[2]_ _k_ _k[′][ dist][(][m]k[∗][,][ m]k[∗][′]_ [)][B][2][ and plugging inequality (][48][) into]
_−_

the above inequality, it further implies that:

P P


_T −1_

_t=0_

X


_T −1_

_ρt + Υ_ (50)
_t=0_

X


_K_

E[ _Fk ( ˜wk,t)_ + [3]
_||∇_ _||[2]_ _≤_ [3(][f][ (][w]TηNκ[0][)][ −] _[f][ (][w][∗][))]_ _T_
_k=1_

X


_TK_


This shows the claim.


-----

D DISCUSSION ON VARIOUS BOUNDS IN EXISTING STUDY

Under the non-convex setting, Theorem 1 bounds the squared gradient norm of the personalized
iterates over the local loss. Existing studies on PFL zero in various different kinds of convergence
bound, and under different assumptions. Below are some concrete examples:

-  Li et al. (2021) study the upper bound of E **_w˜k,T_** **_wk[∗][∥][2][i]_** for any device k (see their Corollary
_∥_ _−_
1), where wk,T is the personalized weights at iterationh _T and wk[∗]_ [is the optimal weights under the]
client k’s local loss. Under the assumption of strongly convex, smoothness, bounded gradient, and
gradient dissimilarity, as well as an additional assumption on the distance between personalized
models and the optimal global model, it is theoretically proved that the proposed solution, named
Ditto, achieves O(1/T ) convergence rate. Their derived convergence rate is on the same scale
as the convergence rate of the global model produced by FedAvg. However, it is important to
note that an extra assumption on the distance between personalized models and the optimal global
model is required to achieve this rate.

-  Hanzely et al. (2021) study the convergence of the proposed regularization problem (see
their Theorem 4.5). Specifically, they propose to bound E _Kk=1_ **_wk,T_** **_wk[∗][(][λ][)][∥][2][i]_** where

_[∥]_ [˜] _−_
**_w˜k,T is the personalized model, and wk[∗][(][λ][)][ is the optimal model for their defined regular-]hP_**
ization problem under the regularized hyper-parameter λ. Under the strongly convex and
smoothness assumption, the authors derive the bound as E _Kk=1_ **_wk,T_** **_wk[∗][(][λ][)][∥][2][i]_**
1 _n_ _T_ _Kk=1_ **_wk,T_** **_wk[∗][(][λ][)][∥][2][ +][ 2][nασ]µ_** [2] . Further, by applying the variance reduction tech-hP _[∥]_ [˜] _−_ _≤_

nique on their proposed L2GD solution, they eliminate the constant term (i.e., the second term) − _[αµ]_ _[∥]_ [˜] _−_
in the above bound. However, their convergence analysis are made towards a special case of the   P
regularized problem. It remains unspecified about how and whether the personalized iteration
produced by L2GD can achieve convergence towards the optimum for each client’s local loss
(i.e., the optimum of the ultimate PFL problem (P2)). In other words, the gap between ˜wk,T and
**_wk[∗]_** [= min][w]k _F[ˆ]k(wk) remains unknown._

-  Under the non-convex setting, Deng et al. (2020) study the upper bound of squared gradient norm,
or formally, _T[1]_ _Tt=1_ [E] _∥∇Fk ( ˜wk,t)∥[2][i]_ (see their Theorem 6), where ˜wk,t is the personalized

model for client k and hFk( ) is the gradient of a model over the local loss. Under the smoothness

P _∇_ _·_

and bounded variance conditions, they derive a bound with rate O( _√[1]T_ [)][ to convergence. Similar]

to our results, a non-vanished residual presents in their bound. This non-vanished residual is
related to: i) the gradient dissimilarity between clients’ local loss function; and (ii) the gradient
discrepancy, The form of this non-vanished term is highly similar to our derived residual term (i.e.,
Υ in our Theorem 5), since both the gradient dissimilarity factor (i.e., G in Assumption 1) and
gradient bound (i.e., B in Assumption 4) would present in the non-vanished residual. Additionally,
when the personalized factor is completely eliminated i.e., αk 0 in their formulation or β _d_
in our formulation, the non-vanished residual (i.e., Γ in their formulation and → Υ in ours) would →
both become significant to dominate the bound.


-----

