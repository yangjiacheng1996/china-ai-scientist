# QUANTIFYING THE CONTROLLABILITY OF COARSELY
## CHARACTERIZED NETWORKED DYNAMICAL SYSTEMS

**Anonymous authors**
Paper under double-blind review

ABSTRACT

We study the controllability of large-scale networked dynamical systems when
complete knowledge of network structure is unavailable. In particular, we establish
the power of learning community-based representations to understand the ability
of a group of control nodes to steer the network to a target state. We are motivated by abundant real-world examples, ranging from power and water systems to
brain networks, in which practitioners do not have access to fine-scale knowledge
of the network. Rather, knowledge is limited to coarse summaries of network
structure. Existing work on "model order reduction" starts with full knowledge
of fine-scale structure and derives a coarse-scale (lower-dimensional) model that
well-approximates the fine-scale system. In contrast, in this paper the controllability aspects of the coarse system are derived from coarse summaries without
knowledge of the fine-scale structure. We study under what conditions measures of
controllability for the (unobserved) fine-scale system can be well approximated by
measures of controllability derived from the (observed) coarse-scale system. To
accomplish this, we require knowledge of some inherent parametric structure of the
fine-scale system that makes this type of inverse problem feasible. To this end, we
assume that the underlying fine-scale network is generated by the stochastic block
model (SBM) often studied in community detection. We quantify controllability
using the “average controllability” metric and bound the difference between the
controllability of the fine-scale system and that of the coarse-scale system. Our
analysis indicates the necessity of underlying structure to make possible the learning of community-based representations, and to be able to quantify accurately the
controllability of coarsely characterized networked dynamical systems.

1 INTRODUCTION

In this paper we study controllability for networked dynamical systems when our knowledge of
system structure is limited to coarse summaries. We are motivated by myriad real-world settings
where system identification must be performed based upon measurements taken by low-resolution
instruments unable to probe fine-scale structure. Our motivating example is the human brain. While
efforts are under way to produce a canonical human brain map, our knowledge of the brain as an
interconnected, network system is not yet to the level of the whole-brain individual neuron (Betzel
and Bassett, 2017). And yet, motivated by emerging medical technologies, there are important control
tasks we would like to tackle. For example, novel brain implants designed for epilepsy patients aim
to “steer” the brain away from states that correspond to seizures (Heck et al., 2014; Muldoon et al.,
2016). Our goal is to quantify the controllability of a fine-scale networked dynamical system given
access only to coarse knowledge of network structure. Generally, without parametric structure, this
is impossible. But real networks do have structure and so in our model we assume the fine-scale
network has a connectivity induced by an underlying stochastic block model (SBM).

Approximation of high-dimensional (fine-scale) dynamical systems by lower-dimensional (coarsescale) ones is known as “model order reduction” (MOR) in the controls literature. There is a key
difference in assumptions that differentiate our setting from that literature. In MOR the starting point
is a complete description of the high-dimensional system. The task is to formulate a lower-dimension
system, the dynamics of which well-approximate those of the full system. In contrast, we start from
coarse summaries of the fine-scale system. We do not have access to the fine-scale dynamics and
must exploit parametric knowledge (via the assumption of a generative SBM). One might think of the


-----

distinction as akin to “active” versus “passive” MOR. Traditional MOR is active in that it actively
decides how to coarsen the system to yield the best reduction. But for us, our knowledge is limited by
the precision of our instrumental observations, so passively collected data is our starting point.

Controllability is a function both of system dynamics and how we actuate the system (Pasqualetti
et al., 2014; Yuan et al., 2013). Herein we assume that we both measure and actuate a system only
coarsely. A control question we study is which coarse-level actuations are most “influential” in
controlling the underlying fine-scale system. Such knowledge can assist with actuation selection;
e.g., in our motivating epilepsy application, where best to position devices to be able to collapse the
unstable brain-state oscillations that lead to seizures. (O’Leary et al., 2018; Pazhouhandeh et al.,
2019; Kassiri et al., 2017; Shulyzki et al., 2015) To accomplish our goal we characterize the average
controllability of a vector of systems, each corresponding to a different coarse-scale actuation input.
By comparing these vectors, and because these vectors well approximate the corresponding vectors
for the fine-scale system, we aim (in the long term) to produce clinically-usable information for the
neurologist.

**Contribution: Our work is the first of its kind that proposes a learning-based framework for inferring**
the controllability of fine networks from coarse measurements, and characterizes the mismatch
between the controllability of the coarse and fine-scale networks. We study two approaches.

1. In Section 5, we build from MOR. We define an auxiliary, fictitious, reduced-order system
based on the coarse data, and use the average controllability vector of this system to approximate that of the fine-scale system. We derive a tight upper bound on the “approximationerror” which is the sum of two terms. One term goes to zero as the coarse network size
increases and the network becomes dense. The second term is a function of the synchronization between the coarse summary data and the underlying community structure. If
synchronization is not sufficiently high, this term may not approach zero even as the network
size increases.

2. In Section 6 we learn the fine-scale system’s average controllability vector directly from the
coarse data. This learning-based algorithm builds on the mixed-membership algorithm of
Mao et al. (2017) for unsupervised learning of the parameters and the community structure of
a SBM. We derive a tight upper bound on estimation error and characterize its convergence.
Although the error bound implicitly depends on synchronization, unlike in the MOR-based
approach, the error of this approach converges to zero as the coarse network size and its
density increases.

2 BACKGROUND / RELATED WORK

_Coarsened SBM as a generative process: The study of extracting community structure from coarse_
summaries is recent. The authors in (Ghoroghchian et al., 2021) used the stochastic block model
(SBM), developed in the community detection literature (Abbe, 2017), to lay out a framework for a
coarsened and weighted variant of the SBM. We build off those results in this paper. The structure of
many real-world networks, including brain networks is, at least empirically, known to have community
structure across various spatial scales (Sporns and Betzel, 2016; Pavlovi´c et al., 2020). The SBM and
its variants provide a powerful modeling framework to facilitate fundamental understanding of graph
community organization and have found applications in many domains, including social and power
networks. (Dulac et al., 2020; Funke and Becker, 2019; Abbe, 2017).

_Complex networks controllability: The development of control methods for complex networks is a_
major effort in network science (Scheid et al., 2020). Coupling traditional notions of controllability
with graph theory reveals several insights into the role of network structure (e.g., presence of
communities, diameter, and sparsity), size, and edge weight strength in controlling large-scale
networks (Wu-Yan et al., 2018; Kim et al., 2018; Constantino et al., 2019; Sun, 2015). Further one
may want to understand which group of nodes, when actuated as inputs, can be used to steer the
network to an arbitrary target state, and at what cost (Cortesi et al., 2014; Gu et al., 2015). Recent
works in network neuroscience Gu et al. (2015) have popularized the notion of average controllability.
This scalar metric associate a measure the relative control influence of a group of nodes. In this paper
we consider a vector of such scalar measures to study the comparative influence of different sets of
nodes. To the best of our knowledge ours is the first work that characterizes this type of error bounds
for the controllability of coarse graphs.


-----

3 PRELIMINARY NOTIONS

**Notation: We denote vectors and matrices using bold faced small and upper case letters. The n**
dimensional all-ones and -zero vectors are denoted by∥mM =∥ n∞, then define the spectral radius by= max1≤u≤n _mv=1_ _[|][M][uv][|][;][ ∥][M][∥][max] ρ([ = max]M) = max[u,v] 1n[ |] andi[M]λ[uv] 0i_ _[|]n[; and];. For diag([ ∥] M[M]M = [[∥]) = [[2][ =]MMuvλ]11 ∈max, . . .,R(M[n][×] M[T][m]M, definenn). Let][T]_
P _{|_ _|}_ p _∈_

R[n]; and Diag(M) sets the off-diagonal entries of M to zero. For matrices M[′]i[s][ with arbitrary]
dimensions, BlkDiag(M1, . . ., Md) denotes the block diagonal matrix. The inequality M1 **M2**
implies element wise inequality. We write f (n) = O(h(n)) iff there exist positive reals c0 and ≤ _n0_
such that _f_ (n) _c0h(n) for all n_ _n0. The support of a vector, supp(m), is the set of indices i_
_|_ _| ≤_ _≥_
such that mi = 0. The cardinality of a set is denoted by . For a positive integer m, we denote
_̸_ _V_ _|V|_

[m] ≜ _{1, . . ., m}. 1(m) returns a vector of same size with non-zero replaced by 1._

**Networks: A network is defined by an un-directed graph G ≜** (V, E), where the node set V ≜
_{1, . . ., n} and edge set E ⊆V × V. For an edge (u, v) ∈E, assign the weight Auv = Avu ∈_ R,
and define the weighted symmetric adjacency matrix of G as A ≜ [Auv], where Auv = Auv = 0
whenever Auv /∈E. A random network is an un-directed graph with a random adjacency matrix.

3.1 LINEAR DYNAMICAL SYSTEM ON RANDOM NETWORK

For a network G with n nodes and the symmetric adjacency matrix A, associate a state xi[k] ∈ R to
the i-th node, and let the nodes evolve with the linear and time-invariant (LTI) dynamics [1]:

1
**x[t + 1] =** _t = 0, 1, . . . ._ (1)

_c · tr(A)_ **[Ax][[][t][] +][ Bu][[][t][]][,]** _∀_

The state x[t] = [x1[t], . . ., xn[t]][T] is steered to an arbitrary value by an input u[t] ∈ R[n]. Here, the
input matrix B = Diag(b) ∈ R[n][×][n], where b ∈{0, 1}[n] determines which components of u[t] enters
the network[2]. For e.g., for B = Diag(1n1 _, 0n−n1_ ), the input enters the network through control
nodes set = 1, . . ., n1 . The normalization c tr(A) factor, with appropriately chosen constant
_c > 0, ensures that system in Eq. 1 is asymptotically stable. Finally, we define K_ _{_ _}_ _·_ **Anom ≜** _c_ tr1(A) **[A][ for]**

_·_
the normalized matrix, and use this convention throughout the paper.

For fixed system matrix Anom, a necessary and sufficient condition for the asymptotic stability[3] of
Eq. 1 is that ρ(Anom) ≤ 1. For random Anom, we consider the probabilistic stability: P[ρ(Anom) ≤
1]—the greater the value, the greater the chance that Anom is stable. For SBM generated random
symmetric matrices, we provide sharp non-asymptotic lower bounds on P[ρ(Anom) ≤ 1].

The networked LTI system in Eq. 1 is T-step controllable if x[0] = 0 can be steered to any target
state x ∈ R[n] for some inputs: u[0], . . ., u[T − 1]. The T -step controllability Gramian of Eq. 1 given
below, among other things, allows us to study if Eq. 1 is controllable or not.

**_CT (Anom, B) =_** _t=0_ [(][A][nom][)][t][BB][T][(][A][nom][)][t][.] (2)

By definitionn-step controllable; or equivalently, CT (Anom, B) ⪰ 0, and it is well known that CT (Anom[P],[T] B[ −])[1] ≻ 0. For other interesting properties of Eq. 2 we G with n nodes is T -step controllable if
refer to (Chen, 1999). For the simplicity of exposition, we let T →∞ and consider the infinite time
horizon Gramian: (Anom, B) = limT _T (Anom, B), which exists with 1_ P[ρ(Anom) 1];
**_C_** _→∞_ **_C_** _−_ _≥_
see also Pasqualetti et al. (2014). We drop the notation (Anom, B) in when the context is clear.
**_C_**

_Average energy: A widely used metric to measure how hard or easy it is to control the network is_
average energy: **x** 2=1 **[x][T][C][†][x][ d][x][/]** **x** 2=1 _[d][x][, which evaluates to][ n][−][1][tr(][C][†][)][ (Cortesi et al., 2014).]_

_∥_ _∥_ _∥_ _∥_

Here, where is the pseudo inverse, and x[T] **x is the minimum control energy needed to steer**

R R

**_C[†]_** **_C[†]_**
**x[0] = 0 to an arbitrary target state x ∈** R[n]. Thus, average energy measures the minimum control
energy required to steer x[0] = 0 to an arbitrary state uniformly distributed over the unit sphere.

1 One may think our LTI model as the linearized system of an underlying non-linear system. Controllability
of non-linear systems require a case by case analysis and we leave this topic for future research.
2Alternatively, Bu[t] = B **u** [t], where B is the sub-matrix of B whose columns are indexed by [n].
_K_ _K_ _K_ _K ⊂_
However, we stick with notation in Eq. 1 to make our analysis less cumbersome.
3The LTI system Eq. 1 is asymptotically stable if ∥x[t]∥2 → 0 as t →∞, for u[t] = 0 and x[0] ̸= 0.


-----

_Average controllability: Numerical computation of C[†]_ for large-scale networks is demanding. Owing
to the fact that tr(C[†]) ≥ 1/tr(C), one uses tr(C)—called the average controllability—as a proxy for
average energy Gu et al. (2015). The higher the average controllability is for a given set of control
nodes defined by B, the smaller their average energy, thus higher their influence on the network.

3.2 STOCHASTIC BLOCK MODELS

Stochastic block models (SBMs) are probabilistic models that produce random graphs with planted
communities. Formally, let Gfine ≜ (V, E) be the un-directed graph (also referred as fine graph) with
_n nodes and random edge weights generated according to the general SBM(n, Q, p):_
**Definition 1. (General SBM) In the general SBM(n, Q, p), the graph Gfine is partitioned to K**
disjoint sub-graphs (or communities) of relative sizes p = [p1, . . ., pK] such that V = ∪k[K]=1[V][k][. Two]
nodesprobability u ∈V Qk andkk′ independently from other edges, for all v ∈Vk′ are joined by an edge with the weight k, k[′] **A[Kuv]. ∈{0, 1}, which is drawn with**
_∈_

In General SBM, the probability distribution of weights Auv is common for all u ∈Vk and u ∈Vk′ .
The general SBM(n, Q, p) thus generates a weighted symmetric graph with K communities with
non-identical in- and cross-edge connection probabilities given by Q ∈ [0, 1][K][×][K]. Alternatively,

**Auv** Bernoulli(Qk,k′ ) if k, k[′] [K] : Pku > 0, Pk′v > 0, (3)
_∼_ _∈_

where the community membership matrix P = [Pkv] ∈ R[K][×][n] is given by

**PP[T]** = Diag ( 1 _, . . .,_ _K_ ) with Pkv = 1 if v ∈Vk, (4)
_|V_ _|_ _|V_ _|_ 0 otherwise.


We define D ≜ _n[1]_ **[PP][T][ which is a diagonal matrix of][ relative][ community sizes.]**

**Definition 2. (Coarse SBM Ghoroghchian et al. (2021)) Define a coarse-scale summary to A as**


**A ≜** **WAW[T]** _∈_ R[m][×][m], (5)

where the coarsening matrix W R[m,n] is (a) r-homogeneous, for all i [m]; that is, each i-th row of
_∈_ e _∈_
**W (say wi) has r non-zero terms and all rows have constant row sum and (b) (WW[T]** = [1]r **[I][m][).]**

Here, **A can be interpreted as the symmetric adjacency matrix of an un-directed graph Gcoarse with**
_m nodes— referred to as coarse graph. This interpretation is helpful when we discuss LTI system_
associated with[e] **A in Section 5. We refer the nodes in** coarse to as c-nodes[4] as opposed to the fine
_G_
nodes in Gfine. Note that r ≤ _m[n]_ [, and][ r][ ≪] _[n][ indicating that][ c][-nodes can cover the fine graph only]_

sparsely. In other words, there may exist (several) fine nodes that do not contribute to A (see Fig. 1).

[e]

The main goal of our paper is to quantify controllability of Gfine, with community structure, using the
coarsely inferred network coarse. Importantly, we do not have access to the way the coarse graph is
_G_
acquired at the time of decision making though the results depend on them.

From Eq. 8 and Eq. 3, the expected quantities of **A[¯]** ≜ E[A] and **A[¯]** ≜ E[A] can be computed as

**A¯** = P[T]QP and **A¯** = (WP[T]) **Q(eWP[T])[T][e],** (6)
**Φ**
e

where Φ ∈ R[m][×][K] is the coarse community membership matrix| {z }, and Φik captures the extent to which
the i-th c-node overlaps with the k-th community. Let us also define the resolution parameter.

_ν ≜_ min (7)
_i_ [m],k [K]:Φik>0 **[Φ][ik][.]**
_∈_ _∈_

By definition 1/r ≤ _ν ≤_ 1. In what follows, we assume that a c-node has a constant minimum
overlap (i.e. ν) with each community that independent of other system parameters.

The example below will highlight the structural differences among matrices P, Φ, and W.
**Example 1. For fine network shown in Fig. 1, the following hold**

4“c-” stands for compound or coarse.


-----

_1. P = BlkDiag(1[T]12[,][ 1][T]18[,][ 1][T]6_ [)][. Here,][ 1][d] _[is the d-dimensional all-ones column vector.]_


_2. Φ = [Φ[T]1_ _[,][ Φ]2[T][,][ · · ·][,][ Φ]6[T][]][T][, where][ Φ][1]_ [= [1][,][ 0][,][ 0]][;][ Φ][2] [= [][ 2]3 _[,][ 1]3_ _[,][ 0]][;][ Φ][3][ =][ Φ][4][ = [0][,][ 1][,][ 0]][;]_

**Φ5 = [0,** [1]3 _[,][ 2]3_ []][; and][ Φ][6][ = [0][,][ 0][,][ 1]][.]

_3. Finally, each row of the coarsening matrix W has three non-zero entries, all equal to 1/3._

_Each c-node in {1, 3, 4, 6} covers one community; each c-nodes in {2, 5} overlap with two._

**Assumption 1. (SBM scaling Abbe (2017)) For all k** [K], we have 0 < cmin _n_ _cmax < 1,_
_∈_ _≤_ _[|V][k][|]_ _≤_

_where cmin, cmax are constants. There exists a ρn ∈_ (0, 1) and a non-negative matrix Q[(][c][)], such that

**Q = ρnQ[(][c][)],** (8)

**Assumption 2. (Fully-Synchronized c-node): The coarse graph has at least one pure node per**
_community k ∈_ [K]. Formally, for all k ∈ [K], there exist a coarse node i ∈ [m] such that Φik = 1.
**Assumption 3. (Uniform Coarsening): There exist ˜cmin and ˜cmax independent of m such that**
_c˜min1K ≤_ **1[T]m[Φ][/m][ ≤]** _c[˜]max1K._

We also assume that communities have self-connections, that is, tr(Q[(][c][)]) > 0, and Q[c] _≤_ **1K×K.**
Assumption 1 helps us uniformly control the sparsity of connection in and cross communities. For
several real-world networks, ρn typically decreases with n Abbe (2017). Assumption 2 states that
for each community there exist at least one c-node that is fully inside one community. We call such
c-node fully synchronized (see Remark 1). Finally, Assumption 3 ensures that the relative coverage
of each community measured by coarse nodes scales linearly with respect to the graph size.


Figure 1: Schematic of MOR and learning-based approaches for estimating θgroup,A. (a) For Gfine consisting
of n = 36 nodes, we have K = 3 communities (V1, V2, V3) with m = 6 coarse (c)-nodes (K1,...,K6), each
having a coverage size r = 3. The control node set is K3. (b) In MOR approach, we infer θgroup,A via reduced
order dynamical system Scoarse. (c) Learning based approach capitalizes on mixed membership (MM) Algorithm
2 to estimate θgroup,A directly from **A, thereby avoiding the need to do consider Scoarse.**

**Remark 1. (Synchronization of community and coarsening): The coarsening operation is oblivi-**

[e]

_ous to the community structure in Gfine. Thus, the i-th c-node contains information about multiple_
_communities if supp(Wi,:)_ supp(Pi,:) = _(the subscript denotes the i-th row), for i_ = j. Perfect
_∩_ _̸_ _∅_ _̸_
_synchronization: A special case where the intersection is non-empty only when i = j. This happens_
_when all the communities have the same size and each c-node covers only one whole community._

4 PROBLEM STATEMENT


Consider the LTI system on the network Gfine, defined by the symmetric adjacency matrixin A Eq. 3:
fine : x[t + 1] = Anomx[t] + B **u[t],** (9)
_S_ _K_

where Anom ≜ _c_ tr1(A) **[A][,][ A][ ∈]** [R][n][×][n][, and][ B][K][ ∈] [R][n][×][n][ selects a set of control nodes][ K ⊂G][fine][.]

_·_
Depending on the controllability properties of Gfine (see, Section 3.1), the inputs at these |K| control
nodes may or may not effect all the states in x[k]. Let Ki = supp(wi), be the set (hereafter, group)
of r nodes coarsened by wi. The following assumption states that Gfine is controllable from all
_i_ fine; see Remark 2.
_K_ _⊂G_


-----

**Assumption 4. Let BKi = Diag(wi[T][)][. For any][ i][ ∈]** [[][m][]][, the Gramian][ C][(][A][nom][,][ B][K]i [)][ is full rank.]

As ρ(Anom) < 1 holds with high probability (see Lemma 1), it follows that C(Anom, BKi ) exits, and
hence, the infinite time Gramian C(Anom, BKi ) exists. Assumption 4 ensures that average energy
(see Section 3.1) is finite; however, average controllability need not be finite.

For _i-th control node set with input matrix B_ _i = Diag(wi[T][)][, associate the average controllabity]_
_K_ _K_
measure: θgroup[(][i][)] _,A_ [≜] [tr[][C][(][A][nom][,][ B][K]i [)]][, for all][ i][ ∈] [[][m][]][. Since][ C][(][·][)][ is a p.s.d matrix, it follows that]

**_θgroup[(][i][)]_** _,A_

_[≥]_ [0][. Accordingly, define the group average controllability vector for][ S][fine][:]

**_θgroup,A ≜_** **_θgroup[(1)]_** _,A[, . . .,][ θ]group[(][m][)]_ _,A_ _∈_ R[m], (10)
 

which summarizes the average controllability measure (see Sec 3.1) for all control nodes sets
_{K1, . . ., Km}. Thus, θgroup,A helps infer (i) Kis that drive the network to the desired target state_
with least control effort, and (ii) if such control node sets should have a community structure.

In this paper, using only the knowledge of **A in Eq. 5, we want to estimate the random vector**
**_θgroup,A in Eq. 10. We consider two contrasting approaches: (i) the traditional model order reduction_**
(MOR), where we rely upon the reduced order auxiliary system coarse (see Eq. 11) governed by

[e] _S_
**A to infer θgroup,A; and (ii) the learning based approach, where we directly estimate θgroup,A**
using a clustering based mixed membership community learning algorithm; see Section 6. Fig. 1
provides a nice graphical illustration of our approaches. Broadly, our analysis highlights the role ofe
community structure, coarsening process, and graph sparsity conditions on the performance of these
both approaches. Our numerical simulations show that both approaches can outperform each other;
however, learning based approach outperforms its counterpart in several parametric regions Finally,
our main results in Sections 5 and 6 are probabilistic in nature because A is a random matrix.
**Remark 2. (Group nodes controllability) Assumption 4 demands that a group of nodes should be**
_able to control Gfine, which holds true for brain networks (Pasqualetti et al., 2019). This assumption is_
_a very weaker condition than asking Gfine to be controllable from every single node. Moreover, if Gfine_
_is not controllable from the control nodes, we can decompose the state-space of Gfine into controllable_
_and uncontrollable sub-spaces (Chen, 1999), and adapt our analysis to the controllable sub-space._

5 MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY

We provide a tight upper bound on the element wise error between θgroup,A in Eq. 10 and θcoarse,A
in Eq. 13. The latter quantity is the group average controllability vector of the reduced order system:
e

_Scoarse :_ **x[t + 1] =** **Anomx[t] +** **Bu[t],** (11)

where **Anom ≜** _c˜·tr1(A)_ **A,** **A = WAW e[T]** is given by Eq. 5 and the normalization factor[e] e [e] ˜c · tr(A) is

used for stability purposes, where[e] ˜c > 0. Importantly, the state **x[t] ∈** R[m] is not a compression
of the true state[e] **x[t] in** fine[e] and m _n (hence the name MOR). Rather,_ **x[t] is a fictitious state[e]**

[e] S _≪_

that describes the dynamics of the (scaled) matrix **A. This fictitious state is controlled by the input e**
**Bu[t] ∈** R[m]. e

[e]

The following lemma states thate _Sfine and Scoarse are asymptotically stable with very high probability._
Thus, θgroup,A in 10 and θcoarse,A [in 13 are well defined. Let][ c,][ ˜]c, β ≥ 0, and define the stability

indices: κnom := (cβ tr( A[¯] ) **A** )/(n(1 _cβ)[2]) and_ _κnom := (˜cβ tr( A[¯]_ ) **A** )/(m(1 _rcβ˜_ )[2]),
_−∥_ [¯] _∥ e∞_ _−_ _−∥_ [¯] _∥∞_ _−_
whereκLemma 1.nom are strictly positive for some constantsA[¯] _∈ (RProbabilistic stability of[n][×][n]_ and **Ae[¯]** _∈_ R[m][×][m] are given in Eq. 6 and Sfine and c, ˜ Sc >coarse 0, and): Suppose that the stability indices e 0 r ≤ is the coverage size.β < 1. Then,e e _κnom and_

e P [ρ(Anom) ≥ _β] ≤_ _n exp (−2κnom)_ _and_ P[ρ(Anom) ≥ _β] ≤_ _m exp (−2κnom) ._ (12)

For m = n, we have r = 1, and hence, the probabilistic inequalities in Eq. 12 coincide with each

[e] e

other. Further, the higher κnom and _κnom, the higher the chance that Sfine and Scoarse are stable._
Interestingly, κnom and _κnom do not explicitly scale with n and m. In fact, κnom ≥_ _κnom, lb, where_
e
e


-----

_κnom, lb := (cβ cmin tr(Q)_ _cmax_ **Q** )/(1 _cβ)[2]_ is independent of n. Thus, stability is not
guaranteed for larger networks modeled using − _∥_ _∥∞_ _S −fine with c · trA normalization._

what conditionsInstead, κnom, lb κ →∞nom >as 0 cβ for → cβ1, thereby = 1? One such condition is P [ρ(Anom) ≥ _β] ≤_ _n cβ exp(−2cκmaxnom, lbQ_ ) →/(0c. So, undermin tr(Q)).
Let cmax = cmin . For diagonally dominant probability matrix Q (i.e., the community structure is ≥ _∥_ _∥∞_
assortative—more in-community edges than across-community edgess), tr(Q) **Q**, and hence,
_≥∥_ _∥∞_
we can choose c and β such that cβ = 1. However, non diagonally dominant matrices can satisfy
tr(Q) **Q** . For example, a symmetric matrix Q R[3][×][3], with Qii = 0.2, Q12 = 0.25, and
_≥∥_ _∥∞_ _∈_
**Q13 = 0.01 is not diagonally dominant because Qii** _j=i_ **[Q][ij][, for all][ i][, but][ tr(][Q][)][ ≥∥][Q][∥][∞][.]**
_≤_ [P] _̸_

We now bound the difference between θgroup,A in 10 and θcoarse,A [in 13. Let][ e]Anom ≜ **A[e]** _/(˜c · tr(A))_

and **Bi = rWdiag(wi[T][)][—the coarsened input matrix. Let the average controllability for the][ i][-th]**

e

c-node be θ[(][i][)] **Anom,** **Bi)]** 0. Define the average controllability vector for coarse: [e]
group,A [≜] [tr[][C][(][ e] _≥_ _S_

[e]

[e] **_θcoarse,A[e]_** [≜] **_θcoarse[(1)]_** _,A[, . . .,][ θ]coarse[(][m][)]_ _,A_ _∈_ R[m]. (13)

 

In what follows, when the synchronization holds, we show that e [e] **_θ[e]_** coarse,A [associated with][ S][coarse][ can]
well approximate θgroup,A associated with Sfine. Define the error metric: e

_rθgroup[(][i][)]_ _,A_ **_θcoarse[(][i][)]_** _,A_

∆i(A, **A) ≜** _m_ _[−]_ [1] _m_ _[−]_ [1] _,_ for all i [m]. (14)

_−_ _∈_
_i=1[[][r][θ]group[(][i][)]_ _,A_ _i=1[[][θ]coarse[(][i][)]_ [e] _,A_

_[−]_ [1]] _[−]_ [1]]

The proposed error metric[e] P ∆i(A, **A) allows us to do a fair shifting- and scaling-free comparisonP** [e]
between the vectors θgroup,A and θcoarse,A[. The shift factor "-1" accounts for the inherent "+1" shift]
in the average controllability definition and the scale factor[e] _r discounts the 1/r factor in θgroup,A._
e

Akin to ∆i(A, **A) in Eq. 14, define ∆i( A[¯]** _,_ **A¯** ) associated with the expected quantities **A[¯]** and **A¯** .

**Theorem 1. (Element wise bound on[e]** **_θgroup[e],A−θcoarse,A[): Let][ ∆][i][(][A][,][ e]A) and ∆i( A[¯]_** _,_ **A¯** ) be defined[e]

_as above. Then, under the assumptions in Lemma 1, ∆i( eA,_ **A) ≤** ∆i( A[¯] _,_ **A¯** ) + O _ρn1[e]m_ [+] _√mρr[2]_ 3n

_with probability at least 1_ 6 exp( 2κ(Q[(][c][)], D, ν, m, n, ρn)), where, for constants 0 < δ, ζ < 1,
_−_ _−_

[e] [e]

_κ(Q[(][c][)], D, ν, m, n, ρn) = min_ _nρ[2]n[(tr(][DQ][(][c][)][)][ζ][)][2][, m][(][ρ][n][ν][2][tr(][Q][(][c][)][)][δ][)][2]_
n

_ρ[2]n[mn][(˜]cmincmin_ **Q[(][c][)]** 1,1ζ)[2], ρ[2]n[m][2][(˜]c[2]min[||][Q][(][c][)][||][1][,][1][δ][)][2][o] _,_
_||_ _||_

_with ν, D, and Q[(][c][)]_ _given by Eq. 7, Eq. 4, and Eq. 8, and ˜cmin is defined in Assumption 3._

Theorem 1 says that ∆i(A, **A) ≈O** _ρn1m_ [+] _√mρr[2]_ 3n provided ∆i( A[¯] _,_ **A¯** ) is small. Thus, for fixed n,

MOR based estimate θcoarse,A [approximates] **_[ θ][group][,][A][ if the graph density][ ρ][n][ or number of][ c][-nodes]_**
is large, which we validate using numerical simulations as well. It can be shown that the bias term[e] [e]
∆i( A[¯] _,_ **A¯** ) is exactly zero if the communities are synchronized with the coarsening process (see e
Remark 1) and that Qii = p > 0 and Qij = q > 0, for i ̸= j.

[e]

6 LEARNING APPROACH FOR GROUP AVERAGE CONTROLLABILITY

We present our learning based approach to estimate θgroup,A in Eq. 10. Unlike the MOR based
approach that relies on Sfine, we directly estimate elements in θgroup,A based on the popular mixed
membership (MM) community learning algorithms (Mao et al., 2020; Huang et al., 2019; Mao et al.,
2018; 2017; Aicher et al., 2015). Specifically, we work with the MM algorithm by (Mao et al., 2020)
which is not only numerically efficient but also has strong theoretical guarantees.
**Lemma 2. Let θgroup, ¯A** _[be obtained by replacing][ A][ with the expected matrix][ ¯]A in θgroup,A, given_
_by Eq. 10. Let P and Φ be as in Eq. 4 and Eq. 6. Suppose that Assumption 1 hold. Then,_

**_θgroup, ¯A_** [= (][1][m] [+][ d][Φ][diag(Υ))][ /r,] (15)


-----

_where d = 1/(ntr[DQ[(][c][)]]), D = (1/n)PP[T], and Q[(][c][)]_ _is given by Eq. 8, and_

Υ ≜ (nd)Q[(][c][)]DQ[(][c][)][I − ((nd)DQ[(][c][)])[2]][−][1]. (16)

Lemma 2 gives us a formula to compute the group average controllability vector associated with the
expected matrix **A[¯]** (see Supplemental material for complementary explanation and interpretation).
Theorem 3 (see Appendix) shows that ∆i(A, **A)** _ϵ, for arbitrary ϵ > 0, hold with high probability._
_≤_
In view of this fact, we propose our candidate estimator as
**_θˆgroup ≜_** [e]1m + Φ[ˆ] diag(Υ)[ˆ] _,_ (17)

where Υ[ˆ] ≜ trQˆ( DD[ˆ][ˆ]QQ[ˆ][ˆ]) [(][I][ −] [(] tr( DˆD[ˆ]Q[ˆ]Q[ˆ] ) [)][2][)][−][1][. The hatted quantities][ ˆ]Φ and **Q[ˆ]** are obtained from Algorithm[5]

2, which takes as input **A and number of communities K. Instead, we obtain** **D[ˆ]** from Algorithm 1.
Importantly, **_θ[ˆ]group in Eq. 17 is obtained from coarsened matrix_** **A but not the fine scale matrix A.**

**Theorem 2. (Component wise error bound between[e]** **_θ[ˆ]group and θgroup,A): Suppose that there exist_**
_constants ˜cmin and ˜cmax that satisfy Assumption 3. Then, under the hypotheses stated in Lemma 1,[e]_
_rθgroup[(][i][)]_ _,A_ **_θˆgroup[(][i][)]_** 1

_mi=1[(][r][θ]group[(][i][)]_ _[−],A[1]_ _−_ _mi=1[(ˆ]θgroup[(][i][)]_ _[−]_ [1] = _O_  _m_ [[ 1]ρn + ||EΦ||max + ||EQ||max + ||ED||max]

_[−]_ [1)] _[−]_ [1)]
P ≜∆[b] _i(A)P_

|holds with probability at least{z [e] 1−3 exp(−2ˆκ(Q}[(][c][)], D, m, n, ρn)), where EΦ ≜ **Φ[ˆ]** _−Φ, EQ ≜_ **Q[ˆ]** _−Q,_

_and ED ≜_ **D[ˆ]** _−_ **D are the error matrices. Further, for a constant 0 < ζ < 1, the exponent**
_κˆ(Q[(][c][)], D, m, n, ρn) = min_ _nρ[2]n[(tr(][DQ][(][c][)][)][ζ][)][2][, mnρ][2]n[(˜]cmincmin||Q[(][c][)]||1,1ζ)[2][o]_ _._
n

Theorem 2 suggests that for sufficiently large m, the estimate **_θ[ˆ]group approximates θgroup,A to an_**
arbitrary precision if: (a) the graph is dense enough (larger ρn), (b) the coarse community membership
matrix is well estimated (smaller[6] _||EΦ||max), (c) the cross-community probability estimation do_
not suffer from high error (smaller ||EQ||max), and the relative community sizes estimated from
the coarse graph are close the ones in the fine graph (smaller **ED** ). The result of Theorem 2 is
_||_ _||_
important because one can directly infer the most influential control node set Ki (one with high
average controllability) via the most influential i-the c-nodes, and vice versa.

**Algorithm 1: Direct Inference of the Group Average Controllability**


**Require: estimates** **Φ[ˆ]** and **Q[ˆ]** from Algorithm 2, and the number of communities K

**Φˆ** [T]1m
1: compute **D[ˆ]** = diag(

**1K** **Φ[ˆ]** [T]1m [)]

2: return **_θ[ˆ]group = 1m + Φ[ˆ]_** diag ˆtrQ( DD[ˆ][ˆ]QQ[ˆ][ˆ]) [(][I][ −] [(] tr( DˆD[ˆ]Q[ˆ]Q[ˆ] ) [)][2][)][−][1][)]
 

**Algorithm 2: Mixed Membership Community Estimation Algorithm (Mao et al., 2020)**


**Require: Coarse adjacency matrix** **A, number of communities K**

1: compute the highest K eigen-decomposition of **A as** _V[ˆ]_ Λ [ˆ]V[ˆ] [T] and set Spruned = Prune( V[ˆ] )

2: set X = V[ˆ] ([m]\Sp.runed, :) and compute[e] _Spure = Successive Projection Algorithm(X_ [T])

3: set Xpure = X( pure, :) and compute un-normalized[e] **Φ[ˆ]** [un-nom] = V X[ˆ] pure[−][1]
_S_

4: **Φ[ˆ]** [un-nom]ik 0 if **Φ[ˆ]** [un-nom]ik _< e[−][12],_ _i_ [m], k [K]
_←_ _∀_ _∈_ _∈_

5: return **Φ[ˆ]** = Diag[−][1]( Φ[ˆ] [un-nom]1K) Φ[ˆ] [un-nom] and **Q[ˆ]** = XpureΛ[ˆ]Xpure[T]

5This MM algorithm adapted from Mao et al. (2020) is a type of spectral clustering method that first performs
eigen decomposition of **A to find the overlapping membership (Φik) of the fine nodes. A pruning step is also**
included (see steps 4 and 5 in Algorithm 2) to speed up the algorithm performance.
6 (Mao et al., 2020) showed that[e] _||EΦ||max and ||EQ||max stated in Theorem 2 approach zero under some_
conditions, as m →∞. But these conditions might not be applicable to our setup because of our coarsening
operation. However, our simulations show that ∆i(A) decreases with m. The behavior of ||EΦ||max and
_||EQ||max with respect to graph scaling is left for future work._

[b] [e]


-----

7 SIMULATIONS

We validate our theoretical results by plotting[7] the errors _m[1]_ _mi=1_ [∆][i][(][A][,][ e]A) and _m[1]_ _mi=1_ ∆i(A) and

show that these errors are comparable to the bounds we obtained in Theorems 1 and 2. We generate

P P

**A ∼** SBM(n, Q, p) and then determine **A = WAW[T]** (see Supplemental material for generating[b] [e]
**W). We set number of fine nodes n = 5000, the overlap parameter η = 0.1, and the number of**
communities K = 5. Finally, for Q R[K][×][K], we set Qkk = p = 0.05 and Qkk′ = q = 0.01 (for
_∈_ [e]
_k ̸= k[′]). If not specified, the number of c-nodes m = 100 and coverage size per c-node r = 4._
Fig. 2(a)-2(d) illustrate the qualitative behavior of the errors with respect to changes in m, ρn, and
the degree of (non-)synchronization in coarse nodes (i.e., η), and r. To fairly compare these errors,

we also consider a base line error: _i=1_ _[|]_ Pmi=1µi[(]−[µ]1[i][−][1)][ −] Pmi=1rθ[(]group[(][r][i][θ][)] group[(][i][)],A[−],A[1][−][1)] _[|][/m,][ where][ µ][ ∈]_ [[1][,][ 2]][m]

contains i.i.d. uniform random variable drawn independently of **A.**

[P][m]



[e]


(a) w.r.t. no. coarse nodes m. (b) w.r.t. ρn for p = 5q = 0.005.

(c) w.r.t. overlap extent η. (d) w.r.t. coverage size r.

Figure 2: Estimation error based on MOR and Learning approaches. MOR Error=[P][m]i=1 [∆][i][(][A][,][ e]A)/m and
Learning Error=[P][m]i=1 ∆i(A)/m. The shaded region in the figures represent one standard deviation computed
for 20 independent realizations. We make the following observations. First, both the learning and MOR based
errors are consistently better than the random baseline. Second, the learning based approach has consistently

[b] [e]
smaller error than that of the MOR approach for large parametric regimes. Third, (a) shows that all errors
monotonically decrease as m increases. This is consistent with our bounds in Theorems 1 and 2. Fourth, (b)
shows that errors decrease as ρn increases. This is expected because larger values of ρn result in more distant
in- and cross-community edge densities. This makes community representation extraction and controllability
estimation easier. Fifth, (c) demonstrates the higher tolerance of the Learning approach to situations whenin
coarse measurements are less synchronized, in comparison to the MOR method. Finally, (d) shows that error
decreases with r. This should be the case as larger r means more fine nodes are sampled during coarsening.
8 CONCLUSION AND FUTURE WORK

We introduced a learning-based framework that exploits the power of community-based representation
learning to infer average controllability of fine graphs from coarse summary data. We compared the
performance of this approach with that of MOR approach. For both these methods, we derived high
probability error bounds on the deviation between the error estimate and ground truth, and validated
the theory with numerical simulations. Our results highlight the role of fine- and coarse-network
sizes, graph density, and community synchronization bias (see Remark 1 in modulating the estimation
errors. Interestingly, for the latter approach, we show that the estimation error decreases with network
size albeit the synchronization bias, which is not the case with the MOR-based approach. For
future, we plan to implement our theory to study the role of coarsening, community structures, and
synchronization aspects on the controllability of brain networks.

7The Python code to reproduce the results is attached to the submitted file.


-----

9 ETHICS STATEMENT

Although our work mainly takes a theoretical perspective to the controllability of coarse graphs
motivated by therapeutic neuroscience applications, our results can potentially involve negative
impacts if employed in other applications. For instance, identifying the most influential groups of
nodes (or equivalently individuals) in a social network as a result of estimating the network group
average controllability, may motivate manipulative actions; i.e. the most influential node group
may be selected for control, in order to steer the whole network towards an unethical goal (like
manipulating individuals in a social network to vote in favour of a particular election candidate). The
negative impact may also result in bias against less-influential nodes, as they will be ignored when it
comes to the selection of node groups for control actuation.

10 REPRODUCIBILITY STATEMENT

All the results presented in this paper are reproducible. The theoretical findings are annotated and
step-by-step elaborated in the appendix. The data generation process and the parameter values used
for numerical simulations are fully explained. In addition, the Python code from which the simulation
figures are generated is attached to the submission files. The code is also on Github and the repository
will go public upon submission acceptance.


-----

REFERENCES

Emmanuel Abbe. Community detection and stochastic block models: recent developments. The Journal of
_Machine Learning Research, 18(1):6446–6531, 2017._

Christopher Aicher, Abigail Z Jacobs, and Aaron Clauset. Learning latent block structure in weighted networks.
_Journal of Complex Networks, 3(2):221–248, 2015._

Richard F Betzel and Danielle S Bassett. Multi-scale brain networks. Neuroimage, 160:73–83, 2017.

Chi-Tsong Chen. Linear system theory and design. 1999.

Pedro H Constantino, Wentao Tang, and Prodromos Daoutidis. Topology effects on sparse control of complex
networks with laplacian dynamics. Scientific reports, 9(1):1–9, 2019.

Fabrizio L. Cortesi, Tyler H. Summers, and John Lygeros. Submodularity of energy related controllability
metrics. In 53rd IEEE Conference on Decision and Control, pages 2883–2888, 2014. doi: 10.1109/CDC.
2014.7039832.

Adrien Dulac, Eric Gaussier, and Christine Largeron. Mixed-membership stochastic block models for weighted
networks. In Conference on Uncertainty in Artificial Intelligence, pages 679–688. PMLR, 2020.

Thorben Funke and Till Becker. Stochastic block models: A comparison of variants and inference methods.
_PloS one, 14(4):e0215296, 2019._

Nafiseh Ghoroghchian, Gautam Dasarathy, and Stark Draper. Graph community detection from coarse measurements: Recovery conditions for the coarsened weighted stochastic block model. In International Conference
_on Artificial Intelligence and Statistics, pages 3619–3627. PMLR, 2021._

Shi Gu, Fabio Pasqualetti, Matthew Cieslak, Qawi K Telesford, B Yu Alfred, Ari E Kahn, John D Medaglia,
Jean M Vettel, Michael B Miller, Scott T Grafton, et al. Controllability of structural brain networks. Nature
_communications, 6(1):1–10, 2015._

Christianne N Heck, David King-Stephens, Andrew D Massey, Dileep R Nair, Barbara C Jobst, Gregory L
Barkley, Vicenta Salanova, Andrew J Cole, Michael C Smith, Ryder P Gwinn, et al. Two-year seizure
reduction in adults with medically intractable partial onset epilepsy treated with responsive neurostimulation:
final results of the rns system pivotal trial. Epilepsia, 55(3):432–441, 2014.

Ling Huang, Chang-Dong Wang, and Hongyang Chao. ocomm: Overlapping community detection in multi-view
brain network. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 2019.

Hossein Kassiri, Sana Tonekaboni, M Tariqus Salam, Nima Soltani, Karim Abdelhalim, Jose Luis Perez
Velazquez, and Roman Genov. Closed-loop neurostimulators: A survey and a seizure-predicting design
example for intractable epilepsy treatment. IEEE trans. on biomedical circuits and systems, 11(5):1026–1040,
2017.

[Fritz Keinert. Course notes: Applied linear algebra. https://orion.math.iastate.edu/keinert/](https://orion.math.iastate.edu/keinert/math507/notes/chapter5.pdf)
[math507/notes/chapter5.pdf.](https://orion.math.iastate.edu/keinert/math507/notes/chapter5.pdf)

Jason Z Kim, Jonathan M Soffer, Ari E Kahn, Jean M Vettel, Fabio Pasqualetti, and Danielle S Bassett. Role of
graph architecture in controlling dynamical networks with applications to neural systems. Nature physics, 14
(1):91–98, 2018.

Xueyu Mao, Purnamrita Sarkar, and Deepayan Chakrabarti. On mixed memberships and symmetric nonnegative
matrix factorizations. In International Conference on Machine Learning, pages 2324–2333. PMLR, 2017.

Xueyu Mao, Purnamrita Sarkar, and Deepayan Chakrabarti. Overlapping clustering models, and one (class) svm
to bind them all. In Advances in Neural Information Processing Systems, pages 2126–2136, 2018.

Xueyu Mao, Purnamrita Sarkar, and Deepayan Chakrabarti. Estimating mixed memberships with sharp
eigenvector deviations. Journal of the American Statistical Association, (just-accepted):1–24, 2020.

Sarah Feldt Muldoon, Fabio Pasqualetti, Shi Gu, Matthew Cieslak, Scott T Grafton, Jean M Vettel, and Danielle S
Bassett. Stimulation-based control of dynamic brain networks. PLoS computational biology, 12(9):e1005076,
2016.

Gerard O’Leary, David M Groppe, Taufik A Valiante, Naveen Verma, and Roman Genov. NURIP: Neural
interface processor for brain-state classification and programmable-waveform neurostimulation. IEEE Journal
_of Solid-State Circuits, 53(11):3150–3162, 2018._


-----

Fabio Pasqualetti, Sandro Zampieri, and Francesco Bullo. Controllability metrics, limitations and algorithms for
complex networks. IEEE Transactions on Control of Network Systems, 1(1):40–52, 2014.

Fabio Pasqualetti, Shi Gu, and Danielle S Bassett. Re: Warnings and caveats in brain controllability. NeuroImage,
197:586–588, 2019.

Dragana M Pavlovi´c, Bryan RL Guillaume, Emma K Towlson, Nicole MY Kuek, Soroosh Afyouni, Petra E
Vértes, BT Thomas Yeo, Edward T Bullmore, and Thomas E Nichols. Multi-subject stochastic blockmodels
for adaptive analysis of individual differences in human brain network cluster structure. NeuroImage, 220:
116611, 2020.

M Reza Pazhouhandeh, Gerard O’Leary, Iliya Weisspapir, David Groppe, Xuan-Thuan Nguyen, Karim Abdelhalim, Hamed Mazhab Jafari, Taufik A Valiante, Peter Carlen, Naveen Verma, et al. 22.8 adaptively
clock-boosted auto-ranging responsive neurostimulator for emerging neuromodulation applications. In 2019
_IEEE International Solid-State Circuits Conference-(ISSCC), pages 374–376. IEEE, 2019._

Brittany H Scheid, Arian Ashourvan, Jennifer Stiso, Kathryn A Davis, Fadi Mikhail, Fabio Pasqualetti, Brian
Litt, and Danielle S Bassett. Time-evolving controllability of effective connectivity networks during seizure
progression. arXiv preprint arXiv:2004.03059, 2020.

Ruslana Shulyzki, Karim Abdelhalim, Arezu Bagheri, M Tariqus Salam, Carlos M Florez, Jose Luis Perez
Velazquez, Peter L Carlen, and Roman Genov. 320-channel active probe for high-resolution neuromonitoring
and responsive neurostimulation. IEEE trans. on biomedical circuits and systems, 9(1):34–49, 2015.

Olaf Sporns and Richard F Betzel. Modular brain networks. Annual review of psychology, 67:613–640, 2016.

Peng Gang Sun. Controllability and modularity of complex networks. Information Sciences, 325:20–32, 2015.

Elena Wu-Yan, Richard F Betzel, Evelyn Tang, Shi Gu, Fabio Pasqualetti, and Danielle S Bassett. Benchmarking
measures of network controllability on canonical graph models. Journal of Nonlinear Science, pages 1–39,
2018.

Zhengzhong Yuan, Chen Zhao, Zengru Di, Wen-Xu Wang, and Ying-Cheng Lai. Exact controllability of complex
networks. Nature communications, 4(1):1–9, 2013.


-----

A APPENDIX

In this appendix, we first provide further detailed explanation for the numerical simulations that was
missing due to space constraints. Next we have a remark that adds complementary interpretation of
the group controllability formula estimated from the learning approach. Finally, we provide proofs
for all results stated in our paper.

A.1 MORE ON NUMERICAL SIMULATIONS: HOW TO GENERATE W

For a realization of **A, we obtain the support of each row of W independently by first generating a**
random vector π ∈{η}[K] using a Dirichlet distribution; larger positive η result in greater overlap and
more communities. For each community[e] _k where πk > 0,_ _r_ _πk_ fine nodes are randomly chosen
_⌊_ _·_ _⌋_
from _k_ (i.e., non-selected fine indices in community k) and set as the support of wi. The
_V_ [non][−][sel]
chosen indices are removed from _k_ .This process continues until the support of all wis are
_V_ [non][−][sel]
selected.

A.2 MORE ON THE ESTIMATED GROUP CONTROLLABILITY USING THE LEARNING APPROACH

_suppRemark 3.(wi)_ _/r (Synchronization versus controllability) (for all k_ [K], i [m]) is the fraction of c-node Recall that Φ i’s overlap with communityi,k = |{v : v ∈Vk ∩
_}|_ _∈_ _∈_
_k. Thus, from Eq. 15, it follows that θgroup[(][i][)]_ _,A[¯]_ _k∈[K]_ **[Φ][i,k][Υ][kk][. In view of this observation and]**

_Lemma 2, we observe that c-nodes that have the largest overlap with communities of strongest[∝]_ [P] Υkk
_are the most controllable._

**Additional notation: For n × m dimensional real matrix M, denote ∥M∥F =** _u_ _v_ **[M]uv[2]** [=]

tr(M[T]M). For a symmetric matrix M, denote λmax(M) to be the maximum eigenvalue.

pP P

diag(M) = [M11, . . ., Mnn][T] R[n], and Diag(M) sets the off-diagonal entries of M to zero.

p _∈_

**Useful matrix norm bounds:**

1. Let Z = XY. Then, ∥Z∥F ≤∥X∥2∥Y∥F ≤∥X∥F ∥Y∥F .
3. For any norm: If2. Cauchy-Schwartz inequality ||M|| < 1 ⇒||: tr(X(I −[T]YM) ≤∥)[−][1]X|| <∥F ∥1−||Y1∥MF .|| [(Keinert).]

4. ∥M∥∞ _≤_ _[√]n∥M∥2_
5. **M** 1 _n_ **M** 2
_∥_ _∥_ _≤_ _[√]_ _∥_ _∥_
6. For any m × n matrix M, we have ∥M∥2 ≤ _[√]m∥M∥∞._
7. ∥MXY∥max ≤∥M∥∞∥X∥max∥Y∥1
8. ||M[2] _−_ **M[′][2]|| ≤||M −** **M[′]||(||M|| + ||M[′]||)**


_Proof. ||M[2]−M[′][2]|| = ||M[2]−MM[′]+MM[′]−M[′][2]|| = ||M(M−M[′])+(M−M[′])M[′]|| ≤_
_||M −_ **M[′]||(||M|| + ||M[′]||)**

9. M[−][1] _−_ **M[′−][1]** = M[−][1](M[′] _−_ **M)M[′−][1]**

**Lemma 3. (Lower-Bound Probability of Joint Events:) For the intersection of two events ℧1 and**
℧2 we have: P {℧1 ∩ ℧2} ≥ P {℧1} + P {℧2} − 1.

A.3 PROOFS FOR SECTION 5: MOR APPROACH FOR GROUP AVERAGE CONTROLLABILITY

A.3.1 PROOF OF LEMMA 1

_Proof. We prove only the left inequality in Eq. 12. The right inequality in Eq. 12 can be proved using_
similar steps, and the details are omitted. Because Anom = A/(c · tr(A)) is symmetric, it follows
that ρ(Anom) = ∥Anom∥2. From this observation and the fact that ∥Anom∥2 ≤∥Anom∥∞, we have
P [ρ(Anom) _β] = P [_ **Anom** 2 _β]_ P [ **Anom** _β]_
_≥_ _∥_ _∥_ _≥_ _≤_ _∥_ _∥∞_ _≥_
= P [ **A** _cβtr(A)_ 0] . (18)
_∥_ _∥∞_ _−_ _≥_


-----

Let F = **A** _cβtr(A) and note that E[F] = E_ **A** _cβtr( A[¯]_ ) **A** _cβtr( A[¯]_ ).
_∥_ _∥∞_ _−_ _n∥_ _∥∞_ _−_ _≥∥_ [¯] _∥∞_ _−_
The inequality follows becausemax E[X1], . . ., E[Xt] . From these observations, inequality in Eq. 18 can be further bounded as ∥A∥∞ = maxi∈[n] _j=1_ _[|][a][ij][|][ and that][ E][[max][{][X][1][, . . ., X][t][}][]][ ≥]_
_{_ _}_ P

P [F ≥ 0] = P [F − E[F] ≥−E[F]]

P **F** E[F] _cβtr( A[¯]_ ) **A** _._ (19)
_≤_ _−_ _≥_ _−∥_ [¯] _∥∞_

We show that F is a sub-Gaussian random variable and then bound the right term in Eq. 19 using the 
well-known concentration inequality results. Rewrite F as follows


**F = max**

= max

= max


_a1j_ _,_
_|_ _|_
_j=1_

X


_a2j_ _, . . .,_
_|_ _|_
_j=1_

X


_anj_
_|_ _|_
_j=1_

X 

_n_

 _[−]_ _[cβ][tr(][A][)]_

_a2j_ _cβtr(A), . . .,_
_|_ _| −_
_j=1_

X


_a1j_ _cβtr(A),_
_|_ _| −_
_j=1_

X


_anj_ _cβtr(A)_
_|_ _| −_
_j=1_

X


= max (a1j _cβajj),_ (a2j _cβajj), . . .,_ (anj _cβajj)_ (20)

j=1 _−_ _j=1_ _−_ _j=1_ _−_ 
X X X 

In the last equality, we drop the absolute values because _aij_ [0, 1]. From the latter fact, we _[.]_
also note that (akl _akk)_ [ _cβ, 1]. Thus, for all k_ = l, ( ∈akl _akk) is bounded, and hence,_
_−_ _∈_ _−_ _̸_ _−_
sub-Gaussian with parameter at most 1 + (cβ)[2]/2. Instead, for k = l, (akl _akk) is sub-Gaussian_

with parameter at most 1 _cβ_ _/2. Finally, from Definition 1, notice that each term in the summation −_
_n_ _|_ _−_ _|_ p
_j=1[(][a][kj][ −]_ _[cβa][jj][)][ is independent. From these facts and the linearity of sub-Gaussians, we note]_
that, for all k [n], summand _j=1[(][a][kj][ −]_ _[cβa][jj][)][ is sub-Gaussian with parameter at most:]_
P _∈_

_σ = [P][n][1]_ (n 1)(1 + (cβ)[2]) + (1 _cβ)[2]._

2 _−_ _−_

p

Putting all these pieces together in conjunction with the facts that maxima of sub-Gaussians concentrates near its expectation and cβtr( A[¯] ) **A** 0 (by assumption), from Eq. 19, we have
_−∥_ [¯] _∥∞_ _≥_

P **F** E[F] _cβtr( A[¯]_ ) **A** _n exp_ (cβtr( A[¯] ) **A** )[2]/2σ[2][]
_−_ _≥_ _−∥_ [¯] _∥∞_ _≤_ _−_ _−∥_ [¯] _∥∞_
  _≤_ _n exp_  −2(cβtr( A[¯] ) −∥A[¯] _∥∞)[2]/(n(1 −_ _cβ)[2])_ _._ (21)

  

The last inequality follows because 2σ[2] _≥_ _n/2(1 −_ _cβ)[2]. The left inequality in Eq. 12 follows by_
combining inequalities in Eq. 18 and Eq. 21. The proof is now complete.

A.3.2 LEMMA 4:UPPER AND LOWER BOUNDS OF THE TRACE OF THE FINE- AND
COARSE-SCALE MATRICES

**Lemma 4. (Upper and Lower bounds of the trace of the fine- and coarse-scale matrices). For**
_constants 0 < δ, ζ < 1:_


P _nρnζ(l)_ _tr(A)_ _nρnζ(u)_ 1 2 exp 2nρ[2]n[(][tr][(][DQ][(][c][)][)][ζ][)][2][],
_≤_ _≤_ _≥_ _−_ _−_

P _mρ_ _nν[2]δ(l)_ _tr(A)_ _mρnδ(u)_ 1 2 exp (  2m(ρnν[2]tr(Q[(][c][)])δ)[2]),
_≤_ _≤_ _≥_ _−_ _−_

P **1[T]WAWh** [T]1 _c˜[2]min[ρ][n][m][2][||][Q][(][c][)][||][1][,][1][(][δ][ + 1)]i_ 1 exp 2ρ[2]n[m][2][(˜]c[2]min[||][Q][(][c][)][||][1][,][1][δ][)][2][],
_≥_ _≥_ _−_ _−_
P **1[T]m[WA1][n]** _cmincminρnmn[e]_ **Q[(][c][)]** 1,1(ζ + 1) 1 exp 2ρ[2]n[mn][(˜]cmincmin **Q[(][c][)]** 1,1ζ)[2][]
 _[≥]_ [˜] _||_ _||_ ≥ _−_  − _||_ _||_

(22)

   

_where ν is the coarsening resolution parameter (c.f. Eq. 7), and_

_ζ(l) ≜_ _tr(DQ[(][c][)])(1 −_ _ζ), ζ(u) ≜_ _tr(DQ[(][c][)])(1 + ζ)_ (23)
_δ(l) ≜_ _tr(Q[(][c][)])(1 −_ _δ), δ(u) ≜_ _tr(Q[(][c][)])(1 + δ)_


-----

A.3.3 PROOF OF LEMMA 4

_Proof. From definition, E[tr(A)] = tr(P[T]QP) = nρntr(DQ[(][c][)]) and E[tr(A)] = ρntr(ΦQ[(][c][)]Φ[T])._
Using the Hoeffding’s inequality for tail bounding independent random variables, for a constant
0 < ζ < 1 we have:

[e]


**P** _|tr(A) −_ _nρntr(DQ[(][c][)])| ≥_ _nρntr(DQ[(][c][)])ζ_ _≤_ 2 exp ( _[−][2(][nρ][n][tr][(]n[DQ][(][c][)][)][ζ][)][2]_
 

Hence


) = 2 exp _n[(][tr][(][DQ][(][c][)][)][ζ]_

 = _ζ(u)_ _−2_ _ζ(l)_

[−][2][nρ](24)[2] | {z }


)[2]


_nρn tr(DQ[(][c][)])(1 −_ _ζ)_ _≤_ tr(A) ≤ _nρn tr(DQ[(][c][)])(1 + ζ)_ (25)
_ζ(l)_ _ζ(u)_
| {z } | {z }

We follow similar steps for **A. For all i ∈** [m]:

**ΦikQ[(]kk[c][)][′]** **[Φ][ik][′]** [Φ[2]ik[Q][(]kk[c][)] [+ 2][Φ][ik] **Q[(]kk[c][)][′]** **[Φ][ik][′]** []]

[e] _≥_

_k,kX[′]∈[K]_ _k∈[KX]:Φik>0_ _kX[′]>k_ (26)

_≥_ _ν[2]tr(Q[(][c][)]) + 2ν[2]||Q[(][c][)]||min_
_≥_ _ν[2]tr(Q[(][c][)])_

based on which the following upper and lower bounds on tr(A¯ ) can be found.

_m_

_mρnν[2]tr(Q[(][c][)]) ≤_ tr(A¯ ) = ρntr(ΦQ[(][c][)]Φ[T]) = ρn [e] **ΦikQ[(]kk[c][)][′]** **[Φ][ik][′][ ≤]** _[mρ][n][tr][(][Q][(][c][)][)]_

Xi=1 _k,kX[′]∈[K]_

[e] (27)

We can now use Hoeffding’s inequality to obtain the tail bound for a constant 0 < δ < 1:


**P** _|tr(A) −_ _ρntr(ΦQ[(][c][)]Φ[T])| ≥_ _ρntr(ΦQ[(][c][)]Φ[T])δ_ _≤_ 2 exp ( _[−][2(][ρ][n][tr][(][ΦQ]m_ [(][c][)][Φ][T][))][2] ) (28)
h i 2 exp ( 2m(ρnν[2]tr(Q[(][c][)])δ)[2]).

_≤_ _−_

[e]

The event in Eq. 28 leads to the following upper and lower bound on tr(A):

_mρnν[2]_ tr(Q[(][c][)])(1 − _δ)_ _≤_ _ρntr(ΦQ[(][c][)]Φ[T])(1 −_ _δ) ≤_ tr(A) ≤ _ρntr(ΦQ[e][(][c][)]Φ[T])(1 + δ) ≤_ _mρn tr(Q[(][c][)])(1 + δ)_
_δ(l)_ _δ(u)_

[e] (29)

| {z } | {z }

We then replacing δ(l), δ(u) defined in Eq. 29 into Eq. 28.

Similarly, event

**1[T]WAW[T]1 ≥** **1[T]Φ** **QΦ[T]1(1 + δ) ≥** _c˜[2]min[ρ][n][m][2][||][Q][(][c][)][||][1][,][1][(1 +][ δ][)]_ (30)
_≥c˜minm1K_

happens with probability at least|{z}


_c[2]min[ρ][n][m][2][||][Q][(][c][)][||][1][,][1][δ][)][2]_
1 − P **1[T]WAW[T]1 −** **1[T]ΦQΦ[T]1 < ˜c[2]min[ρ][n][m][2][||][Q][(][c][)][||][1][,][1][δ]** _≥_ 1 − exp ( _[−][2(˜]_ _m[2]_ )

1 exp 2ρ[2]n[m][2][(˜]c[2]min[||][Q][(][c][)][||][1][,][1][δ][)][2][]

  _≥_ _−_ _−_

(31)

 

using Assumption 3 and following using one-sided Hoeffding’s concentration inequality.

Finally, similar arguments can be applied to the following event


**1[T]m[WA1][n]** **1mΦ**

_[≥]_
_≥c˜minm1K_
| {z }


_≥_ _c˜mincminρnmn||Q[(][c][)]||1,1(1 + ζ)_ (32)


**P1n**
=ndiag(D)(1+ζ)≥ncmin1K
|{z}


-----

that occurs with probability at least

1 P **1[T]m[WA1][n]** _[<][ ˜]cmincminρnmn_ **Q[(][c][)]** 1,1ζ 1 exp 2 [(˜]cmincminρnmnmn||Q[(][c][)]||1,1ζ)[2]
_−_ _[−]_ **[1][m][ΦQP1][n]** _||_ _||_ _≥_ _−_ _−_
  1 exp  2ρ[2]n[mn][(˜]cmincmin **Q[(][c][)]** 1,1ζ)[2][]

_≥_ _−_ _−_ _||_ _||_

(33)

 

using one-sided Hoeffding’s inequality. This concludes the proof.

A.3.4 LEMMA: ERROR BETWEEN THE GRAMIANS OF RANDOM AND EXPECTED LTI SYSTEMS


Let S fine and S coarse denote the expected dynamics of LTI Eq. 9 when A and **A are replaced with the**
expected quantities **A[¯]** and **A¯** . The following result provides an error bound between the difference of
Gramians of Sfine and S fine and that of Scoarse and S coarse. [e]

**Lemma 5. (Error between the Gramians of random and expected LTI systems[e]** _):_ _Under_
_the assumptions stated in Lemma 1, the following holds with probability at least 1 −_
2 exp 2nρ[2]n[(][tr][(][DQ][(][c][)][)][ζ][)][2][]:
_−_
 

[1 + _tr[∥][Q](DQ[(][c][)][∥][(][c][max][)])_ [][][ 1]ζ(l) [+][ ρ]tr[n]([∥]DQ[Q][(][c][(][)][c][∥][)][max]) []] 1
_αn ≜_ _∥C(Anom, In×n) −_ **_C( A[¯]_** _nom, In×n)∥max ≤_ _c[2](1_ _β[2])ζ(l)_ 1 _c[2]_ _tr[2](DQ1_ [(][c][)]) _ρ[2]n[n][ =][ O][( 1]ρ[2]n[n]_ [)][,]

_−_ _−_ _·_
h i (34)

_and with probability at least 1_ 2 exp ( 2m(ρnν[2]tr(Q[(][c][)])δ)[2]):
_−_ _−_

[1 + _ν[∥][2][Q]tr[(]([c]Q[)][∥][(][max][c][)])_ [][][ 1]δ(l) [+][ ρ][n]tr[∥]([Q]Q[(][(][c][c][)][)][∥])[max] ] 1
_αn ≜_ _∥C(Anom, Im×m) −_ **_C( A[¯]_** _nom, Im×m)∥max ≤_ _c˜[2](1 −_ _β[2])δ(l)(1 −_ ( _cν˜_ [2]tr(1Q[(][c][)]) [)][2][)] _mρ[2]n[ν][4][ =][ O]_ 
e [e] e (35)

_where r =_ _supp(wi)_ _is the homogeneous coarsening parameter._
_|_ _|_

Note that (D, I) = (I **D[2])[−][1], where D can take Anom,** **A[¯]** nom, **Anom, or** **A[¯]** nom. Thus, Theorem 5
**_C_** _−_
is effectively bounding the difference of resolvents (zI − **D[2])[−][1]** evaluated at z = 1. For n = m, we
have r = 1, ν = 1 and both αn and _αn coincide. Lemma 5 is basically a concentration result for the[e]_ e
Gramians of Sfine (or Scoarse) and S fine (or S coarse); however the rate at which the difference goes to
zero is different for fine- and coarse systems. Further, smaller the stability margin 1 _β, looser are_
e _−_
the bounds in Eq. 34 and Eq. 35.

A.3.5 PROOF OF LEMMA 5

We begin by proving the inequality in Eq. 34. From Eq. 2 and Lemma 1, the limit below exists with
the probability stated in the statement of lemma.

**_C(Anom, In×n)_** ≜ _Tlim_ _Tt=0 −1[(][A][nom][)][t][(][A]nom[T]_ [)][t]

= limT →∞ →∞ **_[C][T]tT[ (]=0 −[A]1[nom][(][A][,][nom][ I][n][×][)][2][n][t][ = (][) = lim][I][ −][T][A][ →∞]nom[2]_** P[)][−][1][.] (36)

The last but one equality follows becauseP **A is a symmetric matrix,** **Anom** 2 = _c_ trA(A) _[∥][2][ ≤]_ _[β <][ 1]_
_∥_ _∥_ _∥_ _·_

(follows from the lemma’s hypothesis), the last one from the Neumann series formula. Also, we have


_mρ[2]n[ν][4]_


**A¯** = **P[T]Q[(][c][)]P**

_c_ tr( A[¯] ) _[∥][2]_ _c_ tr(P[T]Q[(][c][)]P) _[∥][2][ ≤]_

_·_ _∥_ _·_


_∥P[T]Q[(][c][)]P∥∞∥P[T]Q[(][c][)]P∥1_

_c·ntr(DQ[(][c][)])_


_∥A[¯]_ nom∥2 = ∥


1

_c_ tr(DQ[(][c][)]) _[<][ 1][,]_

_·_
(37)


since [P[T]Q[(][c][)]P]ℓv 1 for all ℓ, v [n]. Similarly
_≤_ _∈_

( A[¯] nom, In _n)_ ≜ lim **Anom, In** _n) = (I_ **A[2]nom[)][−][1][.]**
**_C_** _×_ _T_ _×_ _−_ [¯]
_→∞_ **_[C][T][ ( ¯]_**


-----

From these observations, we establish the following identity (explanations for each step succeeds the
equations):


_αn_ = ∥C(Anom, In×n) − **_C( A[¯]_** nom, In×n)∥max
(=a) ∥(I − (Anom)[2])[−][1] _−_ (I − ( A[¯] nom)[2])[−][1]∥max
(=b) ∥(I − **Anom)[−][1](A[2]nom** _[−]_ **A[¯]** [2]nom[)(][I][ −] **A[¯]** [2]nom[)][−][1][∥][max]
(I **A[2]nom[)][−][1][∥][∞][∥][A][2]nom** **A[2]nom[∥][max][∥][(][I][ −]** **A[¯]** [2]nom[)][−][1][∥][1]
(≤∥c) _−_ _[−]_ [( ¯]
_n_ (I **A[2]nom[)][−][1][∥][2][∥][A][2]nom** **A[2]nom[∥][max]√n** (I **A¯** [2]nom[)][−][1][∥][2]
(≤d) _[√]_ _∥_ _−_ _[−]_ [¯] _∥_ _−_

(≤e) _n_ 1−∥A1[2]nom[∥][2][ ∥][A]nom[2] _[−]_ **A[¯]** [2]nom[∥][max] 1−∥A[¯]1[2]nom[∥][2]
_n_ 1 1β[2][ ∥][A]nom[2] **A[2]nom[∥][max]** 1 1 1
_≤_ _−_ _[−]_ [¯] _−_ _c[2]_ _·tr[2]_ (DQ[(][c][)] )

(f )
_n_ 1 1β[2][ ∥][A][nom][ −] **A[¯]** nom max[ **Anom** max + **Anom** max] 1 1 1
_≤_ _−_ _∥_ _∥_ _∥_ _∥_ [¯] _∥_ _−_ _c[2]_ _·tr[2]_ (DQ[(][c][)] )

_≤_ _n_ 1−1β[2][ ∥] _c·trA(A)_ _[−]_ _c·trA¯( A[¯]_ ) _[∥][max][[]_ _cnρn1_ _ζ(l)_ [+][ cρ]nρ[n]n[∥]tr[Q](DQ[(][c][)][∥][(][max][c][)]) []] 1− _c[2]_ _·tr[2]1(DQ1_ [(][c][)] )

_≤_ _n_ 1−1β[2] _c·tr1(A)_ _[∥][A][ −]_ **A[¯]** _∥max[1 +_ _[n][∥]trA( [¯]A[¯]∥max)_ []] _cnρ1_ _n_ [[][ 1]ζ(l) [+][ ρ][n]tr([∥]DQ[Q][(][c][(][)][c][∥][)][max]) []] 1− _c[2]_ _·tr[2]1(DQ1_ [(][c][)] )

(g)
_≤_ _n_ 1−1β[2] _cnρn1_ _ζ(l)_ [[1 +][ nρ]ρnn[n]tr[∥][Q](DQ[(][c][)][∥][(][c][max][)]) []] _cnρ1_ _n_ [[][ 1]ζ(l) [+][ ρ][n]tr([∥]DQ[Q][(][c][(][)][c][∥][)][max]) []] 1− _c[2]_ _·tr[2]1(DQ1_ [(][c][)] )

[1 + tr[∥][Q](DQ[(][c][)][∥][(][c][max][)]) [][][ 1]ζ(l) [+][ ρ][n]tr([∥]DQ[Q][(][c][(][)][c][∥][)][max]) []] 1
_≤_ 1 _ρ[2]n[n]_

_c[2](1_ _β[2])ζ(l)_ 1 _c[2]_ tr[2](DQ[(][c][)])
_−_ _−_ _·_
h i

= O| ( _ρ[2]n1[n]_ [)][.] _O{z(1)_ }


(38)


where (a)-(d),(f) follow the inequalities itemized at the beginning of the appendix; (e) is because of
the assumption in Lemma 1, Eq. 37, and that:

(I **A[2]nom[)][−][1][∥][2]** nom[∥][2][)][ ≤] [1][/][(1][ −∥][A][nom][∥]2[2][)][ ≤] [1][/][(1][ −] _[β][2][);]_ (39)
_∥_ _−_ _[≤]_ [1][/][(1][ −∥][A][2]

and the rest of inequalities follow from definitions of Anom, **A[¯]** nom in the paragraphs processing Eq. 1
and Eq. 11.

The proof for the inequality in Eq. 35 follows similar lines as above, and hence, we provide a sketch,
but not the full details. From Eq. 2, note that the following

_t_
**_C(Anom, Im×m)_** ≜ _Tlim →∞_ **_[C][T]T[ (][ e]A1nom, Im×m) = limT →∞_** _Tt=0 −1_ **A[t]nom** **A[T]nom** (40)

= lim _t=0 −_ **A[2]nom[t]** [= (][I][ −] **A[e]** [2]nom[)][−][1][,]P  

[e] _T →∞_ [e] e

P

[e]

where ∥A[e] nom∥2 = ∥ _c˜·trAe(A2)_ _[∥][2][ ≤]_ _[β <][ 1][ (from the lemma’s hypothesis). Moreover, we have]_

( A[¯] nom, Im _m) = (I_ **A[e]nom[)][−][1][, where]**
**_C_** e _×_ _−_ e[¯]


_∥ΦQ[(]c˜[c]tr[)]Φ(ΦQ[T]∥∞[(][c]∥[)]ΦΦQ[T])[(][c][)]Φ[T]∥1_ 2 _cmν˜_ _√[2]mmtr(Q[(][c][)])_

_∥_ _≤_ _[≤]_


_∥Ae[¯]_ nom∥2 = ∥


1 (41)

_cν˜_ [2]tr(Q[(][c][)])


**A¯**

_c˜tr(A¯_ ) _[∥][2][ =][ ∥]_

e

[e]


following the fact that [ΦQ[(][c][)]Φ[T]]ij 1.
_≤_


-----

Using these observations, we obtain the following inequality by taking similar steps as those for αn
in Eq. 38 (inner steps are removed due to redundancy) :


_α˜n_ ≜ (Anom, Im _n)_ ( A[¯] nom, Im _m)_ max
_≤_ _∥mC1−∥[e]_ (A1 [2]nom[∥][2] _×c˜tr(1A −)_ _[∥]CA[e]_ _−e_ **A[e]¯** _∥max[1 +×_ _∥[m]tr[∥]A([e]¯A¯∥)max_ ][∥A[e] nom∥max + ∥A[¯] nom∥max] 1−∥A[¯]12nom[∥][2]

_≤_ _m_ 1−1β[2][e]mρnν1[2]δ(l)[e][[1 +][ mρ]mρn[n]ν[∥][2][Q]tr[(]([c]Q[)][∥][(][max][c][)]) [][] _cmρ˜_ _n[e]1ν[2]δ(l)_ [+] _cmρ˜_ _ρnn∥Qν[2][(]tr[c][)](∥Qmax[(][c]e[)])_ []] 1−∥( tr(1A¯eeA¯ ) [)][2][∥]e[2]

_≤_ _m_ 1−1β[2] _mρnν1[2]δ(l)_ [[1 +][ ∥]ν[2][Q]tr[(]([c]Q[)][∥][(][max][c][)]) [][] _cmρ˜_ _n1ν[2]δ(l)_ [+] _cmρ˜_ _ρnn∥Qν[2][(]tr[c][)](∥Qmax[(][c][)])_ []] 1−( _cν˜_ [2] tr1(1Q[(][c][e][)] ) [)][2]

[1 + _ν[∥][2][Q]tr[(]([c]Q[)][∥][(][max][c][)])_ [][][ 1]δ(l) [+][ ρ][n]tr[∥]([Q]Q[(][(][c][c][)][)][∥])[max] ] 1
_≤_ _c˜[2](1_ _β[2])δ(l)(1_ ( _cν˜_ [2]tr(1Q[(][c][)]) [)][2][)] _mρ[2]n[ν][4]_

_−_ _−_

_O(1)_

= O| _mρ1[2]n[ν][4]_ _._ {z }
 


(42)


The proof is now complete.

A.3.6 PROOF OF THEOREM 1

The proof of the theorem makes use of Lemma 5. Let i ∈ [m], and recall that Bi = diag(wi[T][)][ and]
**Bi = rWdiag(wi[T][)][. From Eq. 10 and Eq. 13, consider the following bound]**

_rθgroup[(][i][)]_ _,A[−][1]_ **_θcoarse[(][i][)]_** _,A[−][1]_

e ∆i(A, **A) ≜** _|_ _m_ _−_ _m_ [e] _|_

(rθgroup[(][i][)] _,A_ (θcoarse[(][i][)] _,A_

[e] _i=1_ _[−]_ [1)] _i=1_ _[−]_ [1)]

X X

_rθgroup[(][i][)]_ _,A_ _rθgroup[(][i][)]_ [e],A[¯]

_m_ _[−]_ [1] _m_ _[−]_ [1]

_≤|_ _−_ _|_

(rθgroup[(][i][)] _,A_ (rθgroup[(][i][)] _,A[¯]_
_i=1_ _[−]_ [1)] _i=1_ _[−]_ [1)]

X X

=∆i(A,A[¯] )
| **_θcoarse[(][i][)]_** _,A_ {z **_θcoarse[(][i][)]_** _,A¯_ }

+ | _m_ _[−]_ [1] _−_ _m_ _[−]_ [1] _|_ (43)

(θ[(][i][)] [e] (θ[(][i][)] [e]
coarse,A coarse,A¯
_i=1_ _[−]_ [1)] _i=1_ _[−]_ [1)]

X X

[e] =∆i(A,A¯ ) [e]

| _rθgroup[(][i][)]_ _,A[¯]_ {z[e] [e] **_θcoarse[(][i][)]_** _,A[¯]_ }

+ _m_ _[−]_ [1] _m_ _[−]_ [1]
_|_ _−_ _||_

e

(rθ[(][i][)] (θ[(][i][)]
group,A[¯] coarse,A[¯]
_i=1_ _[−]_ [1)] _i=1_ _[−]_ [1)]

X X

e

bias=∆i( A[¯] ),A¯ )

_≤_ ∆i|(A, **A[¯]** ) + ∆i(A, **A¯** ) +{z bias[e] _._ }

The complete proof of the bound on ∆i(A, **A[¯]** ) will be elaborated in Thm. 3 and it follows:

[e] [e]

∆i(A, **A[¯]** )

_ζ([2]u)_ [1 + _[∥]tr[Q](DQ[(][c][)][∥][(][c][max][)])_ [][] _ζ(1l)_ [+][ ρ][n]tr([∥]DQ[Q][(][c][(][)][c][∥][)][max]) []] _K_ **Q[(][c][)]DQ[(][c][)]** max

1 + _||_ _||_

_≤_ _c˜mincmin||Q[(][c][)]||1,1(1 + ζ)_ (1 − _β[2])ζ(l)_ 1 − _c[2]·tr[2](DQ1_ [(][c][)])  _||Q[(][c][)]DQ[(][c][)]||min(1 −_ ( _[||]tr[DQ](DQ[(][c][(][)][c][||][)][F])_ [)][2][)]

h i 

= | ( _ρn1m_ [)] _O{z(1)_
_O_

(44)


_ρnm_


-----

We now derive an upper bound on ∆i(A, **A¯** ).

∆i(A, **A¯** ) = _m_ **_θcoarse[(][i][)]_** _,A[−][1]_ [e]m [e]θcoarse[(][i][)] _,A¯_ _[−][1]_
_|_ [e] _−_ [e] _|_

(θ[(][i][)] (θ[(][i][)]
coarse,A coarse,A¯

[e] [e] _i=1_ _[−]_ [1)] _i=1_ _[−]_ [1)]

X X

[e] [e]

_≤_ _m_ (θ[(][i][)] 1 _|θcoarse[(][i][)]_ _,A_ _[−]_ **_[θ]coarse[(][i][)]_** _,A¯_ [+]

coarse,A
_i=1_ _[−]_ [1)] [e] [e]

X

[e]


(θ[(][i][)]
coarse,A coarse,A¯ [)]
_i=1_ _[−]_ **_[θ][(][i][)]_**

X _m_ [e] [e] [θcoarse[(][i][)] _,A¯_

(θ[(][i][)] _[−]_ [1]][|]

coarse,A¯

_i=1_ _[−]_ [1)] [e]

X

[e]




_m_ _m_ [θcoarse[(][i][)] _,A¯_

(θ[(][i][)] _[−]_ [1]][|]

coarse,A¯

_i=1_ _[−]_ [1)] [e]

X

[e]

(45)


_m_ 1 _||θcoarse,A_ _[−]_ **_[θ]coarse,A¯_** _[||][max]_

(θ[(][i][)]
coarse,A e [e]
_i=1_ _[−]_ [1)]

X

[e]




1 + |


_m||θcoarse,A¯_ _[−][1][||][max]_
1 + _m_
e

(θ[(][i][)]

coarse,A¯

_i=1_ _[−]_ [1)]

X

[e]


_m_ 1 _α˜n_

_≤_

(θ[(][i][)]
coarse,A
_i=1_ _[−]_ [1)]

X

[e]

From the equality after Eq. 40, we have


**_θcoarse[(][i][)]_** _,A¯_ [= diag][i] (I − **A[¯]** [2]nom[)][−][1][] (46)

 

Furthermore, since ΦQ[(][c][)]Φ[T] _≤_ **11[T]** then[e] (ΦQ[(][c][)]Φ[T])[2] _≤_ **11[T]11[T]** = m11[T]

**_θcoarse,A¯_** = (I **A[2]nom[)][−][1][ ¯]A[2]nom[∥][max]**
_||_ _[−]_ **[1][||][max]** _∥tr[2](ΦQ −1[(][¯][c][)]Φ[T])_ _[∥][(][I][ −]_ **A[¯]** [2]nom[)][−][1][m][11][T][∥][max]
e _≤_ tr[2](ΦQm[(][c][)]Φ[T]) _[∥][(][I][ −]_ **A[¯]** [2]nom[)][−][1][∥][∞]

_≤_

_m[√]m_ (47)

tr[2](ΦQ[(][c][)]Φ[T]) _[∥][(][I][ −]_ **A[¯]** [2]nom[)][−][1][∥][2]

_≤_

(mρnmν[2][√]tr(mQ[(][c][)])[2][ ∥][(][I][ −] **A[¯]** [2]nom[)][−][1][∥][2]

_≤_ 1

_≤_ _√mρ2n[ν][4][tr][2][(][Q][(][c][)][)(1][−][(]_ _cν˜_ [2] tr(1Q[(][c][)] ) [)][2][)] _[.]_

Similarly

_m_

2

(θcoarse[(][i][)] _,A¯_ = tr[(I − **A[¯]** nom[)][−][1][ −] **[I][]]**
_i=1_ _[−]_ [1)]

X 2 2

[e] = tr[ A[¯] nom[(]e[I][ −] **A[¯]** nom[)][−][1][]]

tr[ A[¯] 2nom[]] (48)
_≥_ tr[(eΦQ[(][c][)]Φ[T])2]e

tr[2][ΦQ[(][c][)]Φ[T]]

_≥_ e

_cmintr[(Q[(][c][)])[2]]_

(˜cmtr[Q[(][c][)]])[2]

_≥_ _[m][2][ν][3]_ [˜]

= _[ν][3]c[˜]mintr[(Q[(][c][)])[2]]_

_c˜[2]tr[2][Q[(][c][)]]_

where all inequalities follow well-known matrix norm axiom. The last line in Eq. 48 is based on
Assumption 2, and

tr[ **ΦQ[(][c][)]Φ[T][][2]]** _≥_ _νc˜minmtr[Φ(Q[(][c][)])[2]Φ[T]]_
_νc˜minmmν[2]tr[(Q[(][c][)])[2]]_ (49)
  _≥_
= m[2]ν[3]c˜mintr[(Q[(][c][)])[2]]


-----

similar to the derivation of Eq. 27 since for all k ∈ [K]:



[Φ[T]Φ]kk =


**Φ[2]ik**
_iX∈[m]_ _[≥]_ _[ν]_


**Φik** _νc˜minm_ (50)
_≥_
_iX∈[m]_


We use the definition of θcoarse,A [in Eq. 13:]

_m_

e

(θ[(][i][)]

_|_ coarse,A

_i=1_ _[−]_ [1)][|]

X

[e]


= diagi (I − **A[e]** [2]nom[)][−][1][ −] **[I]**

_i=1_

X 

= tr (I − **A[e]** [2]nom[)][−][1][ −] **[I]**

_≥_ tr(AA[2][2]nom) [)] 
_≥_ tr[tr][2][(][ e](A)

(a) [e]

_c˜[2]min[(1+][δ][)]_

[e] _r[2]_ (mρnδ(u))[2]
_≥_ _[m][2][ρ][n][||][Q][(][c][)][||][1][,][1]_

= _ρn1r[2]_ _c˜[2]min[(1+]δ[δ]([)]u[||])[Q])[2][(][c][)][||][1][,][1]_ _,_


(51)

(52)

(53)

(54)


where the inequality (a) in Eq. 51 comes from


tr(A[e] [2])


**A[2]ij**
_i,j_

X

e


**Avℓ**

 _r[2]_ 

_v∈KXi,ℓ∈Kj_

 [1] **AvℓAv[′]ℓ[′]**

Xi,j _v,v[′]∈KXi,ℓ,ℓ[′]∈Kj_


_i,j_

1

_r[4]_

1

_r[4]_

1

_r[2]_


**Avℓ** +

_v≠_ _v[′]∈_ _∪iKi orX ℓ_ ≠ _ℓ[′]_ _∈∪jKj_

(1 + δ)


**AvℓAv′ℓ′**


1 v∈ _∪iKi, ℓ_ _∈∪jKj_ _v≠_ _v[′]∈_ _∪iKi or ℓ_ ≠ _ℓ[′]_ _∈∪jKj_

_r[2]_  **wi[T][Aw][j]**

_≥_

_i,j_

= _r1[2][ 1]X[T][WAW][T][1]_

_r1[2]_ **1[T]Φ** **Q** **Φ[T]1** (1 + δ)

_≥_

_≥c˜minm1K_ _≥c˜minm1[T]K_
|{z}r[2] _c˜[2]min|{z}[(1 +][ δ][)][,]_

_≥_ _[m][2][ρ][n][||][Q][(][c][)][||][1][,][1]_

where the event defined in Eq. 30 is used whose corresponding probability is Eq. 31.


Replacing Eq. 47, Eq. 48, Eq. 51, and Eq. 42 into Eq. 45 yields:


_√mρ2n_ _[ν][4]_ [tr][2(][Q][(][c][))(1][−][(]


1 )[2] )

_cν˜_ [2] tr(Q[(][c][)] )


∆i(A, **A¯** ) _≤_ _ρnr[2]_ _c˜[2]min[(1+]δ[δ]([)]u[||])[Q])[2][(][c][)][||][1][,][1][ ˜]αn_

[e] [e] = O _ρmρnr[2][2]n[√][ρ][2]nm_

= O  _√mρr[2]_ 3n 
 


1 +






_ν[3 ]c˜mintr[(Q[(][c][)]_ )[2] ]

_c˜[2]_ tr[2] [Q[(][c][)] ]


The statement of the theorem follows by invoking Eq. 44 and Eq. 53 into Eq. 43:


1 _r[2]_

_ρnm_ [+] _√mρ3n_


1 1 1 _r[2]_
∆i( tr(A) **[A][,]** tr(A) **A)** = bias + O _ρnm_ [+] _√mρ3n_ _._ (54)

 

with a joint probability of at least [e]

[e]

1 − 2 exp _−2nρ[2]n[(][tr][(][DQ][(][c][)][)][ζ][)][2][]_ _−_ exp _−2ρ[2]n[mn][(˜]cmincmin||Q[(][c][)]||1,1ζ)[2][]_
 

2 exp ( 2m(ρnν[2]tr(Q[(][c][)])δ)[2]) exp 2ρ[2]n[m][2][(˜]c[2]min[||][Q][(][c][)][||][1][,][1][δ][)][2][] (55)
_−_ _−_ _−_ _−_


per Lemma 3. We then get the minimum of the four exponents in Eq. 70. The proof is now complete.


-----

A.4 PROOFS FOR SECTION 6: LEARNING APPROACH FOR GROUP AVERAGE
CONTROLLABILITY

A.4.1 PROOF OF LEMMA 2

We start by defining θfine,M similar to Eq. 10 and Eq. 13 (M will be later substituted with A and **A[¯]** ),

**_θfine[T]_** _,M_ [≜] [[tr[][C][(][M][nom][,][ e][1][)]] _. . ._ tr[C(Mnom, en)]] ∈ R[n][×][1]. (56)
Using the Gramian definition in Eq. 2, we have:

**_θfine[(][i][)]_** _,M_ = tr [ (Mnom, ei)]
**_C_**

_∞_

= tr **M[τ]nom[e][i][e]i[T][M]nom[τ]**

"τ =0 #
X


tr(M[τ]nom[e][i][e]i[T][M]nom[τ] [)]
_τ_ =0

X

_∞_

tr(e[T]i **[M]nom[2][τ]** **[e][i][)]**
_τ_ =0

X

_∞_

diagi(M[2]nom[τ] [)]
_τ_ =0

X


(57)


= diagi **M[2]nom[τ]**

_τ_ =0 !

X

= diagi (I **M[2]nom[)][−][1][]** _._
_−_
 

We first simplify the term θfine, ¯A [in Eq. 60, by substituting][ ¯]A with P[T]QP introduced prior to Eq. 6:

**_θfine, ¯A_** = tr[C( A[¯]. . .nom, e1)] = diag _∞_ **A¯** [2]nom[τ]

tr[ ( A[¯] nom, en)] _τ_ =0 !

**_C_** X
 _∞_ 1  _∞_ 1

= diag( ( (

tr(P[T]QP) **[P][T][QP][)][2][τ]** [) = diag(] _ntr(DQ[(][c][)])_ **[P][T][Q][(][c][)][P][)][2][τ]** [)]

_τ_ =0 _τ_ =0

X 1 X 1

= diag(I + (

_ntr(DQ[(][c][)])_ [)][2][P][T][Q][(][c][)][PP][T][Q][(][c][)][P][ + (] _ntr(DQ[(][c][)])_ [)][4][P][T][Q][(][c][)][PP][T][Q][(][c][)][P][ +][ · · ·][ )]

= diag(I + _n[1]_ [(] tr(DQ1 [(][c][)]) [)][2][P][T][Q][(][c][)][ 1]n **[PP][T]** **Q[(][c][)]P**

≜D

+ _n[1]_ [(] tr(DQ1 [(][c][)]) [)][4][P][T][Q]| {z }[(][c][)][ 1]n **[PP][T]** **Q[(][c][)][ 1]n** **[PP][T]** **Q[(][c][)][ 1]n** **[PP][T]** **Q[(][c][)]P + · · · )**

= 1n + _ntr(DQ1_ [(][c][)]) [diag(][P][T][Q][(][c][)] tr(DQDQ[(][(][c][c][)][)]) **[P][ +]** tr(1A) **[P][T][Q][(][c][)]** tr(DQDQ[(][(][c][c][)][)]) tr(DQDQ[(][(][c][c][)][)]) tr(DQDQ[(][(][c][c][)][)]) **[P][ +][ · · ·][ )]**

= 1n + _ntr(DQ1_ [(][c][)]) [diag(][P][T][ Q]tr[(]([c]DQ[)][DQ][(][c][(][)][c]) [)]| {z }[[][I][ + (] tr(DQDQ| {z }[(][(][c][c][)][)]) [)][2][ +][ · · ·][ ]]| {z }P)

≜Υ(Q[(][c][)],D)

= 1n + _ntr(DQ1_ [(][c][)]) [diag(][P][T][Υ(]| **[Q][(][c][)][,][ D][)][P][)]** {z }

=(a) 1n + _ntr(DQ1_ [(][c][)]) [(][P][ ◦] **[P][)][T][diag(Υ(][Q][(][c][)][,][ D][))]**

(b)= 1n + _ntr(DQ1_ [(][c][)]) **[P][T][diag(Υ)][,]**

(58)
where (a) is due to the special structure of P Eq. 4 since for an arbitrary matrix M of appropriate
size:
diagi(P[T]MP) = **PkiMk,k′** **Pk′i =** **PkiMk,k′** **Pk′i**

_k,kX[′]∈[K]_ _k,kX[′]∈[K]_

= **PkiMk,kPki =** **P[2]ki[M][k,k]** (59)

_kX∈[K]_ _kX∈[K]_

= (Pi **Pi)diag(M),**
_◦_


-----

and (b) is true because P is binary. Replacing Eq. 58 into Eq. 60 yields:


**_θgroup, ¯A_** = [1]r **[W]** **1n +**



The proof is now complete.


_ntr(DQ1_ [(][c][)]) **[P][T][diag(Υ))]** = [1]r



_ntr(DQ1_ [(][c][)]) **[Φ][diag(Υ))]** (60)



**1m +**


A.4.2 THEOREM 3 AND PROOF

Define the error metric similar to Eq. 14:

_rθgroup[(][i][)]_ _,A_ _rθgroup[(][i][)]_ _,A_

∆i(A, **A[¯]** ) ≜ _m_ _[−]_ [1] _m_ _[−]_ [1] _,_ for all i [m]. (61)

_−_ _∈_
_i=1[[][r][θ]group[(][i][)]_ _,A_ _i=1[[][r][θ]group[(][i][)]_ [e] _,A[¯]_

_[−]_ [1]] _[−]_ [1]]
P P

**Theorem 3. (Component wise error bound between θgroup,A and θgroup, ¯A[): Let][ ∆][i][(][A][,][ ¯]A) be**
_defined as above and ν be the resolution parameter given by Eq. 7. Under the assumptions stated in_
_Section 3 and Lemma 1, the following holds:_



[1 +

_[∥]tr[Q](DQ[(][c][)][∥][(][c][max][)])_ [][]


1

_ζ(l)_ [+][ ρ][n]tr([∥]DQ[Q][(][c][(][)][c][∥][)][max]) []]


∆i(A, **A[¯]** ) _ζ([2]u)_
_≤_ _c˜mincmin_ **Q[(][c][)]** 1,1(1 + ζ)

_||_ _||_


_K_ **Q[(][c][)]DQ[(][c][)]** _max_
1 + _||_ _||_

 _||Q[(][c][)]DQ[(][c][)]||min(1 −_ ( _[||]tr[DQ](DQ[(][c][(][)][c][||][)][F])_ [)][2][)]



(62)


_ρnm_


(1 _β[2])ζ(l)_ 1
_−_ _−_
h


_c[2]·tr[2](DQ[(][c][)])_

_O(1)_
{z


_with probability at least 1_ 3 exp 2ˆκ(Q[(][c][)], D, m, n, ρn) _, where Further, for a constant 0 <_
_−_ _−_
_ζ < 1, the exponent is ˆκ(Q[(][c][)], D, m, n, ρn) = min_ _nρ[2]n[(tr(][DQ][(][c][)][)][ζ][)][2][, mnρ][2]n[(˜]cmincmin_ **Q[(][c][)]** 1,1ζ)[2][o] _._
   _||_ _||_
n

A.4.3 PROPOSITION 4 AND PROOF

**Proposition 4. (Group Average Controllability for** **A[¯]** **_) The group average controllability vector for_**
**A¯** _is_

**_θgroup[(][i][)]_** _,M_ [= 1] (63)

_r_ **[w][i][θ][fine][,][M][.]**

_Proof. We begin by simplifying the group average controllability using its definition in Eq. 10 and_
the definition of Gramian in Eq. 2, for a general matrix notation M which can be replaced by either
**A or** **A[¯]** :

**_θgroup[(][i][)]_** _,M_ = tr **_C(Mnom, diag(wi[T][))]_**

_∞_

= tr  **M[τ]nom[diag(][w]i[T][)diag(]** **[w]i[T][)][T][M]nom[τ]**

"τ =0 #
X

_∞_

= tr **M[τ]nom[diag((][w][i]** nom

"τ =0 _[◦]_ **[w][i][)][T][)][M][τ]** #
X


(a)


=(a) _r1[2][ tr]_  **M[τ]nom[(]** **eve[T]v** [)][M]nom[τ]  (64)

_τX=0_ _v∈suppX(wi)_

 _∞_ 

= _r1[2]_ tr **M[τ]nom[e][v][e]v[T][M]nom[τ]**

_v∈suppX(wi)_ "τX=0 #

_∞_

= _r1[2]_ tr **M[τ]nom[e][v][e]v[T][M]nom[τ]** _,_

_v∈suppX(wi)_ "τX=0 #

**_θfine[(][v][)],M_**

| {z }

where (a) is due to the assumption of r-homogeneous W, ◦ denotes the Hadamard product, and we
have already defined θfine,M in Eq. 56. Putting Eq. 64 in vector form concludes the proof.


1

_r[2][ tr]_


-----

A.4.4 PROOF OF THEOREM 3

_Proof. We start by substituting M into Eq. 63, from Proposition 4, with A and_ **A[¯]**, yields :

_rθgroup[(][i][)]_ _,A[−][1]_ _rθgroup[(][i][)]_ _,A[¯]_ _[−][1]_
∆i(A, **A[¯]** ) = _m_ _m_
_|_ _−_ _|_

(rθgroup[(][i][)] _,A_ (rθgroup[(][i][)] _,A[¯]_
_i=1_ _[−]_ [1)] _i=1_ _[−]_ [1)]

X= _m_ **wiθfine,A−1** X _m_ **wiθfine, ¯A[−][1]**

_|_ _−_ _|_

(wiθfine,A 1) (wiθfine, ¯A
_i=1_ _−_ _i=1_ _[−]_ [1)]

X X


Xi=1m (wiθfine,A − 1) [wiθfine, ¯A

_[−]_ [1]][|]
(wiθfine, ¯A
_i=1_ _[−]_ [1)]

X


_m_ 1 **wiθfine,A** 1

_|_ _−_ _−_
_i=1(wiθfine,A −_ 1)

X


Xi=1m (wiθfine,A − 1) 1

_−_
(wiθfine, ¯A
_i=1_ _[−]_ [1)]

X


_m_ 1 **wi(θfine,A** **_θfine, ¯A[)][ −]_**

_|_ _−_
_i=1(wiθfine,A −_ 1)

X



[wiθfine, ¯A

_[−]_ [1]][|]


**wi(θfine,A** **_θfine, ¯A[)]_**

Xi=1 _m_ _−_ [wiθfine, ¯A

_[−]_ [1]][|]
(wiθfine, ¯A
_i=1_ _[−]_ [1)]

X


**wi(θfine,A** **_θfine, ¯A[)][|][ +][ |]_**
_|_ _−_

**wi(θfine,A** **_θfine, ¯A[)][|][ +][ |]_**
_|_ _−_

**wi(θfine,A** **_θfine, ¯A[)][|][ +][ |]_**
_|_ _−_


(wiθfine,A 1) (wiθfine, ¯A
_i=1_ _−_  _i=1_ _[−]_ [1)] 

X  Xm 

 

**wi(θfine,A** **_θfine, ¯A[)]_**

_m_ 1  **wi(θfine,A** **_θfine, ¯A[)][|][ +][ |]_** _mXi=1_ _−_ _ntr(DQ1_ [(][c][)]) **[Φ][(][i][)][diag(Υ)][|]**

_|_ _−_ 1

(wiθfine,A 1) 



_i=1_ _−_  _i=1_ _ntr(DQ[(][c][)])_ **[Φ][(][i][)][diag(Υ)]**

X  Xm



**wi(θfine,A** **_θfine, ¯A[)]_**

_m_ 1  **wi(θfine,A** **_θfine, ¯A[)][|][ +][ |]_** Xi=1 _m_ _−_ **Φ[(][i][)]diag(Υ)** 

_|_ _−_ _|_

(wiθfine,A 1)  **Φ[(][i][)]diag(Υ)** 

 

_i=1_ _−_  _i=1_ 

Xm 1  **_θfine,A_** **_θfine, ¯A[||][max]_** [+] _m||θfinemX,A−Υθfinemin, ¯A[||][max]_ Υ max 

_||_ _−_ _||_ _||_ _||_ _||_

(wiθfine,A 1) h i
_i=1_ _−_

Xm 1 _||θfine,A −_ **_θfine, ¯A[||][max]_** 1 + _[||]||[Υ]Υ[||]||[max]min_

(wiθfine,A 1) h i
_i=1_ _−_

X


_m_ 1 _αn_ 1 + _[||][Υ]Υ[||][max]min_

_||_ _||_

(wiθfine,A 1) h
_i=1_ _−_

X


(65)


where (a) is due to Cauchy-Schwartz inequality, αn is defined in Eq. 34, θfine, ¯A [is substituted from]
Eq. 58, and (b) is the result of the properties of the coarsening matrix in Definition 2. We use the


-----

definition of θfine,A in Eq. 57:

_m_

_i=1(wiθfine,A −_ 1)

X


= **widiag** (I − **A[2]nom[)][−][1][ −]** **[I]**

_i=1_

= 1XmWdiag  (I − **A[2]nom[)][−][1][ −]** **[I]**

= 1mWdiag **A[2]nom**

 

= 1mWdiag **A[2]nom**

= _c[2]_ tr1[2](A) **[1][m] [W][diag]** **A[2][]**

= _c[2]··tr1[2](A)_ **[1][m] [WA1][n]**  

(a)
_c˜mincminρnmn||Q[(][c][)]||1,1(1+ζ)_

_c[2](ζ(u)ρnn)[2]_

_≥_

= _c[˜]mincmin||Qc[2][(]ζ[c]([2][)]u||)1,1(1+ζ)_ _ρmnn_ _[.]_


(66)


inequality (a) uses the event defined in Eq. 32 is used whose corresponding probability is Eq. 33.
Using the definition of Υ in Eq. 16


_||Υ||max_ _≤||_ **[Q]tr[(]([c]DQ[)][DQ][(][c][(][)][c])[)]** [(][I][ −] [(] tr(DQDQ[(][(][c][c][)][)]) [)][2][)][−][1][||][max]

_K||Q[(][c][)]DQ[(][c][)]||max_
_≤_ tr(DQ[(][c][)])(1−( _[||]tr[DQ](DQ[(][c][(][)][c][||][)]_ )[F] [)][2][)]

= O(1)

_||Υ||min_ _≥||_ **[Q]tr[(]([c]DQ[)][DQ][(][c][(][)][c])[)]** [(][I][ −] [(] tr(DQDQ[(][(][c][c][)][)]) [)][2][)][−][1][||][min]

tr(DQ[(][c][)])

_≥_ _[||][Q][(][c][)][DQ][(][c][)][||][min]_

= Ω(1),


(67)

(68)


where Ω(.) is the opposite scaling of O(.); we write f (m) = Ω(h(m)) iff there exist positive reals
_c0 and m0 such that |f_ (n)| ≥ _c0h(m) for all m ≥_ _m0._

We substitute Eq. 66 and Eq. 34 (with probability Eq. 24) into Eq. 65



[1+ _[∥]tr[Q](DQ[(][c][)][(][∥][c][max][)]_ ) [][] _ζ(1l)_ [+][ ρn]tr([∥]DQ[Q][(][c][(][)][c][∥][)][max])


∆i(A, **A[¯]** ) _c˜mincmin_ **Qζ[(]([2][c]u[)])** 1,1(1+ζ)
_≤_ _||_ _||_


1 + _K||Q[(][c][)]DQ[(][c][)]||max_

_||Q[(][c][)]DQ[(][c][)]||min(1−(_ _[||]tr[DQ](DQ[(][c][(][)][c][||][)]_ )[F] [)][2][)]

(69)


_ρnm_


(1 _β[2])ζ(l)_
_−_


1−


_c[2]_ _·tr[2]_ (DQ[(][c][)] )


with a joint probability of at least

1 2 exp 2nρ[2]n[(][tr][(][DQ][(][c][)][)][ζ][)][2][] exp 2ρ[2]n[mn][(˜]cmincmin **Q[(][c][)]** 1,1ζ)[2][] (70)
_−_ _−_ _−_ _−_ _||_ _||_
 

per Lemma 3. We then get the minimum of the two exponents in Eq. 70 which concludes the proof.

A.4.5 PROOF OF THEOREM 2

We start by using the triangle inequality for absolute values:


**_θˆgroup[(][i][)]_** _[−][1]_ _rθgroup[(][i][)]_ _,A[−][1]_
∆i(A) = _m_ _m_
_|_ _−_ _|_

(θ[ˆ]group[(][i][)] (rθgroup[(][i][)] _,A_

b [e] _i=1_ _[−]_ [1)] _i=1_ _[−]_ [1)]

X X

_rθgroup[(][i][)]_ _,A[¯]_ _rθgroup[(][i][)]_ _,A_

_m_ _[−]_ [1] _m_ _[−]_ [1]

_≤|_ _−_

(rθgroup[(][i][)] _,A[¯]_ (rθgroup[(][i][)] _,A_
_i=1_ _[−]_ [1)] _i=1_ _[−]_ [1)]

X X

∆i(A,A[¯] )
| {z


_rθ[(][i][)]_
group,A[¯]

_[−]_ [1]


**_θˆgroup[(][i][)]_**
+ _m_ _[−]_ [1]
_|_

(θ[ˆ]group[(][i][)]
_i=1_ _[−]_ [1)]

X


(rθ[(][i][)]
group,A[¯]

[1)]

_i=1_ _[−]_ [1)]

X

≜∆[b] _i( A[˜]_ _,A[¯]_ )
(71)

{z


-----

The first term on the RHS of Eq. 71 has already been bounded in Thm.3. Next, we bound the second
term in Eq. 71 and combine the two bounds at the end.

We substitute **_θ[ˆ]group[(][i][)]_** [from the output of Algorithm 1, and][ θ]group[(][i][)] _,A[¯]_ [from Eq. 60, for][ Υ][ defined in]

Eq. 16. For notation simplicity we write Υ(Q[(][c][)], D) as Υ and Υ( Q[ˆ] [(][c][)], **D[ˆ]** ) as Υ[ˆ] yields


**_θˆgroup[(][i][)]_** _[−][1]_ _rθgroup[(][i][)]_ _,A[¯]_ _[−][1]_
∆i( A[˜] _,_ **A[¯]** ) = _m_ _m_ =
_|_ _−_ _|_

(θ[ˆ]group[(][i][)] (rθgroup[(][i][)] _,A[¯]_

b _i=1_ _[−]_ [1)] _i=1_ _[−]_ [1)]

X X


**Φˆ** [(][i][)]diag( Υ)[ˆ] **Φ[(][i][)]diag(Υ)**

_m_ _m_

_−_
**Φˆ** [(][i][)]diag(Υ)[ˆ] **Φ[(][i][)]diag(Υ)**

_i=1_ _i=1_

X X


**Φˆ** [(][i][)]diag(Υ)[ˆ]

Xi=1m **Φ[(][i][)]diag(Υ)**

**Φ[(][i][)]diag(Υ)**

_i=1_

X


**Φˆ** [(][i][)]diag(Υ)[ˆ] _−_


**Φˆ** [(][i][)]diag(Υ)[ˆ]

_i=1_

X


(72)


We set **Φ[ˆ]** [(][i][)] = Φ[(][i][)] + EΦ, **Q[ˆ]** [(][c][)] = Q[(][c][)] + EQ, **D[ˆ]** = D + ED, and Υ = Υ + [ˆ] _E[¯] where EΦ, EQ and_
**ED are error matrices of appropriate sizes. Substitution of theses error matrices into Eq. 72, as well**
as multiple applications of the triangle inequality, gives:
∆i( A[˜] _,_ **A[¯]** )


(Φ[(][i][)] + EΦ)diag(Υ + E[¯])

Xi=1 _m_ **Φ[(][i][)]diag(Υ)**

**Φ[(][i][)]diag(Υ)**

_i=1_

X


(Φ[(][i][)] + EΦ)diag(Υ + E[¯])
_−_


**Φˆ** [(][i][)]diag(Υ)[ˆ]

_i=1_

X

1

_m_

**Φˆ** [(][i][)]diag(Υ)[ˆ]

_i=1_

X

1

_m_

**Φˆ** [(][i][)]diag(Υ)[ˆ]

_i=1_

X

1

_m_

**Φˆ** [(][i][)]diag(Υ)[ˆ]

_i=1_

X


(Φ[(][i][)] + EΦ)[diag(Υ) + diag( E[¯])]

Xi=1 _m_ **Φ[(][i][)]diag(Υ)**

**Φ[(][i][)]diag(Υ)**

_i=1_

X


(Φ[(][i][)] + EΦ)[diag(Υ) + diag( E[¯])]
_−_


**Φ[(][i][)]diag( E[¯]) + mEΦ[diag(Υ) + diag( E[¯])]**

Xi=1 _m_ **Φ[(][i][)]diag(Υ)**

**Φ[(][i][)]diag(Υ)**

_i=1_

X


**Φ[(][i][)]diag( E[¯]) + EΦ[diag(Υ) + diag( E[¯])]**
_−_


**Φ[(][i][)]diag( E[¯]) + mEΦ[diag(Υ) + diag( E[¯])]**

Xi=1 _m_ **Φ[(][i][)]]diag(Υ)**

**Φ[(][i][)]diag(Υ)**

_i=1_

X



[Φ[(][i][)] + EΦ]diag( E[¯]) + [EΦ
_−_


**Φ[(][i][)]diag( E[¯]) + mEΦ[diag(Υ) + diag( E[¯])]**

_m_ 1 (1 + **EΦ** 1) _E[¯]_ max + [ **EΦ** 1 + Xi=1 _m_ ] Υ max

_≤_ _||_ _||_ _||_ _||_ _||_ _||_ _||_ _||_

**Φˆ** [(][i][)]diag(Υ)[ˆ]  **Φ[(][i][)]diag(Υ)**



_i=1_  _i=1_

X  X

 (73)

To further simplify Eq. 73, we find upper bounds on the terms inside. The two terms ||Υ||max and
_||Υ||min have already been bounded in Eq. 67 and Eq. 68 and we have:_


_m||diag( Υ)[ˆ]_ _||min_


**Φˆ** [(][i][)]diag(Υ)[ˆ]

_i=1_

X


(74)


_≤_ _m||Υ[ˆ]_ _||min_

= O( _m[1]_ [)]

The following inequality holds from the definition of norms:
_||EΦ||1 ≤_ _K||EΦ||max_ (75)


-----

Using Eq. 74 and Eq. 75, the inner term in the last line of Eq. 73 is simplified as:


**Φ[(][i][)]diag( E[¯]) + mEΦ[diag(Υ) + diag( E[¯])]**
_i=1_

X _m_

**Φ[(][i][)]diag(Υ)**

_i=1_

X


**Φ[(][i][)]diag( E[¯])**

= Xi=1m + m **[E][Φ]m[[diag(Υ)+diag( ¯]E)]**

**Φ[(][i][)]diag(Υ)** **Φ[(][i][)]diag(Υ)**

_i=1_ _i=1_

X _E)||max_ X _E)||max]_

_≤_ _[m]mE[||]||||[diag( ¯]diag(Υ)max_ _||min_ [+][ mK][ ||][E][Φ][||][max][[]E[||][diag(Υ)]||mmax||]diag(Υ)[||][max][+]||min[||][diag( ¯]

_≤_ _[||]||[ ¯]Υ||min_ [+][ K][ ||][E][Φ][||][max][[][||]||[Υ]Υ[||]||[max]min[+][||][ ¯]

= O(||EΦ||max + ||E[¯]||max)
(76)


Replacing Eq. 67, Eq. 68, Eq. 74, Eq. 75, and Eq. 76 into Eq. 73 simplifies it as:

**_θˆgroup[(][i][)]_** _[−][1]_ _rθgroup[(][i][)]_ _,A[¯]_ _[−][1]_
_|_ _m_ _−_ _m_ _| = O(_ _m[1]_ [[][||][E][Φ][||][max][ +][ ||][ ¯]E||max]). (77)

(θ[ˆ]group[(][i][)] (rθgroup[(][i][)] _,A[¯]_
_i=1_ _[−]_ [1)] _i=1_ _[−]_ [1)]

X X


We now simplify the term ||E[¯]||max in Eq. 79:

_||E[¯]||max_ = ||ΥQ[ˆ]ˆ _−[(][c][)][ ˆ]DΥQ[ˆ]||[(]max[c][)]_ **Dˆ** **Q[ˆ]** [(][c][)] **DQ[(][c][)]**
=
_||_ tr( D[ˆ] **Q[ˆ]** [(][c][)]) [(][I][ −] [(] tr( D[ˆ] **Q[ˆ]** [(][c][)]) [)][2][)][−][1][ −] **[Q]tr[(]([c]DQ[)][DQ][(][c][(][)][c])[)]** [(][I][ −] [(] tr(DQ[(][c][)]) [)][2][)][−][1][||][max]

= 1 **Q[(][c][)][ ˆ]DQ[ˆ]** [(][c][)](I ( **Dˆ** **Q[ˆ]** [(][c][)] **DQ[ˆ]** [(][c][)]) **DQ[(][c][)]**

tr( D[ˆ] **Q[ˆ]** [(][c][)]) _[||][ ˆ]_ _−_ tr( D[ˆ] **Q[ˆ]** [(][c][)]) [)][2][)][−][1][ −] [tr]tr[( ˆ](DQ[(][c][)]) **[Q][(][c][)][DQ][(][c][)][(][I][ −]** [(] tr(DQ[(][c][)]) [)][2][)][−][1][||][max][.]

(78)

To continue the simplification of ||E[¯]||max, we bring the two error matrices EQ and ED introduced in
the statement of the theorem into play:

(Q[(][c][)] + EQ) (D + ED)(Q[(][c][)] + EQ) = (Q[(][c][)] + EQ)(DQ[(][c][)] + DEQ + EDQ[(][c][)] + EDEQ)
= Q[(][c][)]DQ[(][c][)] + Γ,
(79)

where we define

**Γ ≜** **Q[(][c][)]DEQ + Q[(][c][)]EDQ[(][c][)]** + Q[(][c][)]EDEQ + EQDQ[(][c][)] + EQDEQ + EQEDQ[(][c][)] + EQEDEQ
(80)

and


(D + ED) (Q[(][c][)] + EQ) = DQ[(][c][)] + DEQ + EDQ[(][c][)] + EDEQ

E

Replacing Eq. 79 and Eq. 81 into Eq. 78 yields:

| {z }


(81)


= 1 **Dˆ** **Q[ˆ]** [(][c][)] **DQ[(][c][)]**

tr( D[ˆ] **Q[ˆ]** [(][c][)]) _[||][(][Q][(][c][)][DQ][(][c][)][ +][ Γ][)(][I][ −]_ [(] tr( D[ˆ] **Q[ˆ]** [(][c][)]) [)][2][)][−][1][ −] [tr][(]tr[DQ](DQ[(][c][(][)][c][+][)])[E][)] **[Q][(][c][)][DQ][(][c][)][(][I][ −]** [(] tr(DQ[(][c][)]) [)][2][)][−][1][||][max]

= 1 **Dˆ** **Q[ˆ]** [(][c][)] **DQ[(][c][)]**

tr( D[ˆ] **Q[ˆ]** [(][c][)]) _[||][Q][(][c][)][DQ][(][c][)][[(][I][ −]_ [(] tr( D[ˆ] **Q[ˆ]** [(][c][)]) [)][2][)][−][1][ −] [(][I][ −] [(] tr(DQ[(][c][)]) [)][2][)][−][1][]]

**Dˆ** **Q[ˆ]** [(][c][)] tr(E) **DQ[(][c][)]**
+Γ(I (
_−_ tr( D[ˆ] **Q[ˆ]** [(][c][)]) [)][2][)][−][1][ −] tr(DQ[(][c][)]) **[Q][(][c][)][DQ][(][c][)][(][I][ −]** [(] tr(DQ[(][c][)]) [)][2][)][−][1][||][max]

= 1 _∞_ ( **Dˆ** **Q[ˆ]** [(][c][)] )[2][ℓ] ( **DQ[(][c][)]** ]

tr( D[ˆ] **Q[ˆ]** [(][c][)]) [[][||][Q][(][c][)][DQ][(][c][)][[]ℓ=1 tr( D[ˆ] **Q[ˆ]** [(][c][)]) _−_ tr(DQ[(][c][)]) [)][2][ℓ]

X

≜ϵ1

**Dˆ** **Q[ˆ]** [(][c][)] tr(E) **DQ[(][c][)]**
+Γ(I (
_−_ tr|( D[ˆ] **Q[ˆ]** [(][c][)]) [)][2][)][−][1][ −] tr{z(DQ[(][c][)]) **[Q][(][c][)][DQ][(][c]}[)][(][I][ −]** [(] tr(DQ[(][c][)]) [)][2][)][−][1][||][max][]]

_K_ **Dˆ** **Q[ˆ]** [(][c][)]
_≤_ tr( D[ˆ] **Q[ˆ]** [(][c][)]) [[][||][Q][(][c][)][DQ][(][c][)][||][max][||][ϵ][1][||][max][ +][ ||][Γ][||][max][||][(][I][ −] [(] tr( D[ˆ] **Q[ˆ]** [(][c][)]) [)][2][)][−][1][||][max]

tr(E) **DQ[(][c][)]**
+

tr(DQ[(][c][)]) _[||][Q][(][c][)][DQ][(][c][)][||][max][||][(][I][ −]_ [(] tr(DQ[(][c][)]) [)][2][)][−][1][||][max][]]

= O(||ϵ1||max + ||Γ||max + tr(E))
= O(||ϵ1||max + ||Γ||max + ||E||max)
(82)


_||E[¯]||max_


-----

We define another error term:

2ℓ
**DQ[(][c][)]** + E = (DQ[(][c][)])[2][ℓ] +
  

We can then simplify ϵ1 for ℓ _≥_ 1 as:


2ℓ

_· · ·_
_ν=1_

X

≜R=O(E)

| {z }


(83)


**DQ[ˆ]** [(][c][)])
( D[ˆ] **Q[ˆ]** [(][c][)])[2][ℓ] ( [tr][( ˆ]
_−_ tr(DQ[(][c][)]) [)][2][ℓ][(][DQ][(][c][)][)][2][ℓ]


_||ϵ1||max_ = ||

= ||

= ||


_||max_


(tr( D[ˆ] **Q[ˆ]** [(][c][)]))[2][ℓ]

1

(tr( D[ˆ] **Q[ˆ]** [(][c][)]))[2][ℓ]

1

(tr( D[ˆ] **Q[ˆ]** [(][c][)]))[2][ℓ]


_ℓ=1_

_∞_

_ℓ=1_

X

_∞_

_ℓ=1_

X


tr( )
(DQ[(][c][)])[2][ℓ] + (1 + _R_ max
_R −_ tr(DQ[(][c][)]) [)][2][ℓ][(][DQ][(][c][)][)][2][ℓ] _||_



tr( )
( _R_ max
_R −_ tr(DQ[(][c][)]) [)][2][ℓ][(][DQ][(][c][)][)][2][ℓ] _||_




= ||[(1 − ( tr( D[ˆ] **Q1[ˆ]** [(][c][)]) [)][2][)][−][1][ −] [1]][R −] _ℓ∞=1_ "( tr(DQ[(]tr[c][)]()Rtr( )D[ˆ] **Q[ˆ]** [(][c][)]) )[2][ℓ](DQ[(][c][)])[2][ℓ]# _||max_

= [(1 ( 1 X tr(R)
_||_ _−_ tr( D[ˆ] **Q[ˆ]** [(][c][)]) [)][2][)][−][1][ −] [1]][R −] [[(][I][ −] [(] tr(DQ[(][c][)])tr( D[ˆ] **Q[ˆ]** [(][c][)]) **[DQ][(][c][)][)][2][)][−][1][ −]** **[I][]][||][max]**

[(1 ( 1 tr(R)
_≤_ _−_ tr( D[ˆ] **Q[ˆ]** [(][c][)]) [)][2][)][−][1][ −] [1]][||R||][max][ +][ ||][[(][I][ −] [(] tr(DQ[(][c][)])tr( D[ˆ] **Q[ˆ]** [(][c][)]) **[DQ][(][c][)][)][2][)][−][1][ −]** **[I][]][||][max]**

= O(||R||max)
= O(||E||max)
(84)


1

tr( D[ˆ] **Q[ˆ]** [(][c][)]) [)][2][)][−][1][ −] [1]][R −]


= ||[(1 − (


Similarly, we can rewrite an upper bound on ∥Γ∥max as:

_∥Γ∥max_ = K [2](∥Q[(][c][)]D∥max∥EQ∥max + ∥Q[(][c][)]∥max∥ED∥max∥Q∥max + ∥Q[(][c][)]∥max∥ED∥max∥EQ∥max
+∥EQ∥max∥DQ[(][c][)]∥max + ∥EQ∥max∥D∥max∥EQ∥max
+∥EQ∥max∥ED∥max∥Q[(][c][)]∥max + ∥EQ∥max∥ED∥max∥EQ∥max)
= K [2]( **EQ** max + **ED** max + **ED** max **EQ** max + **EQ** max + **EQ** max
_∥_ _∥_ _∥_ _∥_ _∥_ _∥_ _∥_ _∥_ _∥_ _∥_ _∥_ _∥[2]_
+∥EQ∥max∥ED∥max + ∥EQ∥max[2] _[∥][E][D][∥][max][)]_
= O(||EQ||max + ||ED||max),
(85)

and


_∥E∥max_ = ∥DEQ + EDQ[(][c][)] + EDEQ∥max
= K(∥D∥max∥EQ∥max + ∥ED∥max∥Q[(][c][)]∥max + ∥ED∥max∥EQ∥max)
= K(∥EQ∥max + ∥ED∥max + ∥ED∥max∥EQ∥max)
= O(||EQ||max + ||ED||max).

By substituting Eq. 84, Eq. 86, and Eq. 85 into Eq. 82, we get:


(86)


_||E[¯]||max_ = O(||EQ||max + ||ED||max). (87)

Replacing Eq. 87 into the original error in Eq. 79 yields:

**_θˆgroup[(][i][)]_** _[−][1]_ _rθgroup[(][i][)]_ _,A[¯]_ _[−][1]_ 1
_|_ _m_ _−_ _m_ _| = O_ _m_ [[][||][E][Φ][||][max][ +][ ||][E][Q][||][max][ +][ ||][E][D][||][max][]] _._ (88)

(θ[ˆ]group[(][i][)] (rθgroup[(][i][)] _,A[¯]_   
_i=1_ _[−]_ [1)] _i=1_ _[−]_ [1)]

X X


which happens with the same lower bound probability as in Eq. 70. The proof is now complete.


-----

