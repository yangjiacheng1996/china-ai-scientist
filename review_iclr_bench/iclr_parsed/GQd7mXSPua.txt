# META LEARNING LOW RANK COVARIANCE FACTORS
## FOR ENERGY-BASED DETERMINISTIC UNCERTAINTY

**Jeffrey Ryan Willette[1], Hae Beom Lee[1], Juho Lee[1,2], & Sung Ju Hwang[1,2]**

KAIST[1], AITRICS[2]
{jwillette,haebeom.lee,juholee,sjhwang82}@kaist.ac.kr

ABSTRACT

Numerous recent works utilize bi-Lipschitz regularization of neural network layers
to preserve relative distances between data instances in the feature spaces of each
layer. This distance sensitivity with respect to the data aids in tasks such as
uncertainty calibration and out-of-distribution (OOD) detection. In previous works,
features extracted with a distance sensitive model are used to construct feature
covariance matrices which are used in deterministic uncertainty estimation or OOD
detection. However, in cases where there is a distribution over tasks, these methods
result in covariances which are sub-optimal, as they may not leverage all of the
meta information which can be shared among tasks. With the use of an attentive set
encoder, we propose to meta learn either diagonal or diagonal plus low-rank factors
to efficiently construct task specific covariance matrices. Additionally, we propose
an inference procedure which utilizes scaled energy to achieve a final predictive
distribution which is well calibrated under a distributional dataset shift.

1 INTRODUCTION

Accurate uncertainty in predictions (calibration) lies at the heart of being able to trust decisions
made by deep neural networks (DNNs). However, DNNs can be miscalibrated when given out-ofdistribution (OOD) test examples (Ovadia et al., 2019; Guo et al., 2017). Hein et al. (2019) show that
the problem can arise from ReLU non-linearities introducing linear polytopes into decision boundaries
which lead to arbitrary high confidence regions outside of the domain of the training data. Another
series of works (van Amersfoort et al., 2021; Liu et al., 2020a; Mukhoti et al., 2021; van Amersfoort
et al., 2021) link the problem to feature collapse, whereby entire regions of feature space collapse into
singularities which then inhibits the ability of a downstream function to differentiate between points
in the singularity, thereby destroying any information which could be used to differentiate them.
When these collapsed regions include areas of OOD data, the model loses any ability to differentiate
between in-distribution (ID) and OOD data.

A solution to prevent feature collapse is to impose bi-Lipschitz regularization into the network,
enforcing both an upper and lower Lipschitz bound on each function operating in feature space
(van Amersfoort et al., 2021; Liu et al., 2020a), preventing feature collapse. Such features from biLipschitz regualarized extractors are then used to improve downstream tasks such as OOD detection
or uncertainty quantification. Broadly speaking, previous works have done this by constructing
covariance matrices from the resulting features in order to aid in uncertainty quantification (Liu et al.,
2020a; Van Amersfoort et al., 2020) or OOD detection (Mukhoti et al., 2021). Intuitively, features
from a Lipschitz regularized extractor make for more expressive covariances, due to the preservation
of identifying information within different features.

However, empirical covariance estimation is limited when there are few datapoints on hand, such as in
few-shot learning. A key aspect of meta-learning is to learn meta-knowledge over a task distribution,
but as we show, empirical covariance estimation methods are not able to effectively encode such
knowledge, even when the features used to calculate the covariance come from a meta-learned feature
extractor (see Figure 6). As a result, the empirical covariance matrices are not expressive given
limited data and thus the model loses its ability to effectively adapt feature covariances to each task.

Another obstacle, highlighted by Mukhoti et al. (2021), is that plain softmax classifiers cannot
accurately model epistemic uncertainties. We identify a contributing factor to this, which is the shift


-----

|ttreasitn 01 0|Col2|
|---|---|

|ttreasitn 01 0|Col2|
|---|---|

|ccuracy: 0.97 ttreasitn NLL 0.25 D/OOD: 0.50 / 0.50 D/OOD 0.19 / 0.28|Col2|
|---|---|

|Accuracy: 0.81 ttreasitn NLL 0.54 T ID/OOD: 0.37 / 0.16 E ID/OOD 0.08 / 0.83|Col2|
|---|---|


Accuracy: 0.97 traintest

NLL 0.25

ENT ID/OOD: 0.50 / 0.50

ECE ID/OOD 0.19 / 0.28


Accuracy: 0.81 traintest

NLL 0.54

ENT ID/OOD: 0.37 / 0.16

ECE ID/OOD 0.08 / 0.83


Accuracy: 1.00 traintest Accuracy: 1.00 traintest

NLL 0.00 NLL 0.00

ENT ID/OOD: 0.00 / 0.01 ENT ID/OOD: 0.01 / 0.01

ECE ID/OOD 0.00 / 0.50 ECE ID/OOD 0.00 / 0.50


(a) ProtoDDU

|ttreasitn 55 0|Col2|
|---|---|


Accuracy: 1.00 traintest

NLL 0.04

ENT ID/OOD: 0.07 / 0.55

ECE ID/OOD 0.03 / 0.20


(e) Proto Mahalanobis


(b) Protonet


(c) ProtoSNGP


(d) Protonet




|ccuracy: 0.96 ttreasitn NLL 0.10 D/OOD: 0.12 / 0.55 D/OOD 0.04 / 0.19|Col2|
|---|---|

|Accuracy: 0.87 ttreasitn NLL 0.38 T ID/OOD: 0.43 / 1.14 E ID/OOD 0.08 / 0.44|Col2|
|---|---|

|Energy ttreasitn|Col2|
|---|---|


Accuracy: 0.96 traintest

NLL 0.10

ENT ID/OOD: 0.12 / 0.55

ECE ID/OOD 0.04 / 0.19


Accuracy: 0.87 traintest

NLL 0.38

ENT ID/OOD: 0.43 / 1.14

ECE ID/OOD 0.08 / 0.44


Energy traintest


(f) Proto Mahalanobis


(g) Proto Mahalanobis


(h) Proto Mahalanobis


Figure 1: Top row: Examples of the learned entropy surface of baseline networks. Bottom row: our Proto
Mahalanobis models. Each pixel in the the background color represents the entropy given to that coordinate in
input space. Baseline networks exhibit high confidence in areas where there has been no evidence, leading to
higher calibration error when presented with OOD data.

invariance property of the softmax function. Specifically, even if an evaluation point comes from an
OOD area and is assigned low logit values (high energy), this alone is insufficient for a well calibrated
prediction. Small variations in logit values can lead to arbitrarily confident predictions due to the
shift invariance. From the perspective of Prototypical Networks (Snell et al., 2017), we highlight this
problem in Figure 3, although it applies to linear softmax classifiers as well.

In the following work, we first propose a method of meta-learning class-specific covariance matrices
that is transferable across the task distribution. Specifically, we meta-learn a function that takes a set
of class examples as an input and outputs a class-specific covariance matrix which is in the form of
either a diagonal or diagonal plus low-rank factors. By doing so, the resulting covariance matrices
remain expressive even with limited amounts of data. Further, in order to tackle the limitation
caused by the shift invariance property of the softmax function, we propose to use scaled energy
to parameterize a logit-normal softmax distribution which leads to better calibrated softmax scores.
We enforce its variance to increase as the minimum energy increases, and vice versa. In this way,
the softmax prediction can become progressively more uniform between ID and OOD data, after
marginalizing the logit-normal distribution (see example in Figure 1).

By combining those two components, we have an inference procedure which achieves a well calibrated
probabilistic model using a deterministic DNN. Our contributions are as follows:

-  We show that existing approaches fail to generalize to the meta-learning setting.

-  We propose a meta learning framework which predicts diagonal or low-rank covariance
factors as a function of a support set.

-  We propose an energy-based inference procedure which leads to better calibrated uncertainty
on OOD data.

2 RELATED WORK

**Mahalanobis Distance.** Mahalanobis distance has been used in previous works for OOD detection
(Lee et al., 2018) which also showed that there is a connection between softmax classifiers and
Gaussian discriminant analysis, and that the representation space in the latent features of DNN’s
provides for an effective multivariate Gaussian distribution which can be more useful in constructing
class conditional Gaussian distributions than the output space of the softmax classifier. The method
outlined in Lee et al. (2018) provides a solid groundwork for our method, which also utilizes


-----

(a) ProtoSNGP (b) ProtoDDU (c) Proto Mahalanobis

Figure 2: Comparison between covariances learned in SNGP (Liu et al., 2020a), DDU (Mukhoti et al., 2021) and
Proto Mahalanobis (Ours) in the few shot setting (half-moons 2-way/5-shot). Covariance generated from SNGP
are close to a multiple of the identity matrix, while that of ProtoMahalanobis contains significant contributions
from off-diagonal elements.

Mahalanobis distance in the latent space, and adds a deeper capability to learn meta concepts which
can be shared over a distribution of tasks.

**Post Processing.** We refer to post-processing as any method which applies some function after
training and before inference in order to improve the test set performance. In the calibration literature,
temperature scaling (Guo et al., 2017) is a common and effective post-processing method. As the
name suggests, temperature scaling scales the logits by a constant (temperature) before applying
the softmax function. The temperature is tuned such that the negative log-likelihood (NLL) on a
validation set is minimized. Previous works which utilize covariance (Lee et al., 2018; Mukhoti
et al., 2021; Liu et al., 2020a) have also applied post-processing methods to construct latent feature
covariance matrices after training. While effective for large single tasks, these post-processing
methods make less expressive covariances in the meta learning setting, as demonstrated in Figure 1.

**Bi-Lipschitz Regularization.** Adding a regularizer to enforce functional smoothness of a DNN is
a useful tactic in stabilizing the training of generative adversarial networks (GANs) (Miyato et al.,
2018; Arjovsky et al., 2017), improving predictive uncertainty (Liu et al., 2020a; Van Amersfoort
et al., 2020), and aiding in OOD detection (Mukhoti et al., 2021). By imposing a smoothness
constraint on the network, distances which are semantically meaningful w.r.t. the feature manifold
can be preserved in the latent representations, allowing for downstream tasks (such as uncertainty
estimation) to make use of the preserved information. (Van Amersfoort et al., 2020) showed that
without this regularization, a phenomena known as feature collapse can map regions of feature space
onto singularities (Huang et al., 2020), where previously distinct features become indistinguishable.
For both uncertainty calibration and OOD detection, feature collapse can map OOD features onto the
same feature spaces as ID samples, adversely affecting both calibration and OOD separability.

**Meta Learning.** The goal of meta learning (Schmidhuber, 1987; Thrun & Pratt, 1998) is to
leverage shared knowledge which may apply across a distribution of tasks. In the few shot learning
scenario, models leverage general meta-knowledge gained through episodic training over a task
distribution (Vinyals et al., 2016; Ravi & Larochelle, 2017), which allows for effective adaptation and
inference on a task which may contain only limited amounts of data during inference. The current
meta-learning approaches are roughly categorized into metric-based (Vinyals et al., 2016; Snell et al.,
2017) or optimization-based approaches (Finn et al., 2017; Nichol et al., 2018). In this work, our
model utilizes a metric-based approach as they are closely related to generative classifiers, which
have been shown to be important for epistemic uncertainty (Mukhoti et al., 2021).

3 APPROACH

We start by introducing a task distribution p(τ ) which randomly generates tasks containing a support
set S = {(˜xi, ˜yi)}i[N]=1[s] [and a query set][ Q][ =][ {][(][x][i][, y][i][)][}]i[N]=1[q] [. Then, given randomly sampled task]
_τ = (S, Q), we meta-learn a generative classifier that can estimate the class-wise distribution of_
query examples, p(x|y = c, S) conditioned on the support set S, for each class c = 1, . . ., C. A
generative classifier is a natural choice in our setting due to fact that it utilizes feature space densities
which has been shown to be a requirement for accurate epistemic uncertainty prediction (Mukhoti
et al., 2021). Under the class-balanced scenario p(y = 1) = · · · = p(y = C) we can easily predict


-----

the class labels as follows.


_p(x_ _y = c,_ )
_p(y = c_ **x,** ) = _C_ _|_ _S_ _._ (1)
_|_ _S_
_c[′]=1_ _[p][(][x][|][y][ =][ c][′][,][ S][)]_
P

3.1 LIMITATIONS OF EXISTING GENERATIVE CLASSIFIERS

Possibly one of the simplest forms of deep generative classifier is Prototypical Networks (Snell
et al., 2017). In Protonets we assume a deep feature extractor fθ that embeds x to a common metric
space such that z = fθ(x). We then explicitly model the class-wise distribution p(z _y = c,_ ) of the
_|_ _S_
embedding z instead of the raw input x. Under the assumption of a regular exponential family distribution for pθ(z _y = c,_ ) and a Bregman divergence d such as Euclidean or Mahalanobis distance,
_|_ _S_ 1
we have pθ(z|y = c, S) ∝ exp(−d(z, µc)) (Snell et al., 2017), where µc = _|Sc|_ **x˜∈Sc** _[f][θ][(˜]x) is the_

class-wise embedding mean computed from _c, the set of examples from class c. In Protonets, d is_
_S_ P
squared Euclidean distance, resulting in the following likelihood of the query embedding z = fθ(x)
in the form of a softmax function.

exp( **z** **_µc_** )
_pθ(y = c_ **z,** ) = _C_ _−∥_ _−_ _∥[2]_ _._ (2)
_|_ _S_
_c[′]=1_ [exp(][−∥][z][ −] **_[µ][c][′]_** _[∥][2][)]_
P

**1. Limitations of fixed or empirical covariance.** Unfortunately, Eq. (2) cannot capture a nontrivial
class-conditional distribution structure, as Euclidean distance in Eq. (2) is equivalent to Mahalanobis
distance with fixed covariance I for all classes, such that pθ(z _y = c,_ ) = (z; µc, I). For this
_|_ _S_ _N_
reason, many-shot models such as SNGP (Liu et al., 2020b) and DDU (Mukhoti et al., 2021) calculate
empirical covariances from data after training to aid in uncertainty quantification. However, such
empirical covariance estimations are limited especially when the dataset size is small. If we consider
the few-shot learning scenario where we have only a few training examples for each class, empirical
covariances can provide unreliable estimates of the true class covariance. Unreliable covariance leads
to poor estimation of Mahalanobis distances and therefore unreliable uncertainty estimation.

**2. Shift invariant property of softmax and OOD cal-** 𝝁! 𝝁" 𝒛
**ibration.** Another critical limitation of Eq. (2) is that
it produces overconfident predictions in areas distant
from the class prototypes. The problem can arise fromthe shift invariance property of the softmax function whileFigure 3: z travels along the line, making a pre- ∥µ2 − **_µ1∥_** remains the same

𝝁! 𝝁" 𝒛

diction with unnecessarily low entropy.

_σ(ω) = e[ω]/_ _ω[′][ e][ω][′][ with][ ω][ denoting the logits, such]_

that σ(ω + s) = e[ω][+][s]/ _ω[′][ e][ω][′][+][s][ =][ e][ω][/][ P]ω[′][ e][ω][′][ =][ σ][(][ω][)][ for any shift][ s][. More specifically,]_

suppose we have two classes[P] _c = 1, 2, and z moves along the line extrapolating the prototypes µ1_
and µ2 such that z = µ1 + c(µ2 **_µ1) for c_** 0 or c 1. Then, we can easily derive the following
equality based on the shift invariant property of the softmax function:[P] _−_ _≤_ _≥_

1
_pθ(y = 1_ **z,** ) = (3)
_|_ _S_ 1 + exp( **_µ2_** **_µ1_** )

_±∥_ _−_ _∥_

where ± corresponds to the sign of c. Note that the expression is invariant to the value of c except for
its sign. Therefore, even if z is OOD, residing somewhere distant from the prototypes µ1 and µ2
with extreme values of c, we still have equally confident predictions. See Figure 3 for illustration.

3.2 META-LEARNING OF THE CLASS-WISE COVARIANCE

In order to remedy the limitations of empirical covariance, and capture a nontrivial structure of the
class-conditional distribution even with a small support set, we propose to meta-learn the class-wise
covariances over p(τ ). Specifically, we meta-learn a set encoder gφ that takes a class set Sc as
input and outputs a covariance matrix corresponding to the density p(z|y = c, S), for each class
_c = 1, . . ., C. We expect gφ to encode shared meta-knowledge gained through episodic training over_
tasks from p(τ ), which, as we will demonstrate in section 4, fills a key shortcoming of applying
existing methods such as DDU (Mukhoti et al., 2021) and SNGP (Liu et al., 2020a). We denote the
set-encoder gφ for each class c as

Λc, Φc = gφ(Zc), _Zc = {z˜ −_ **_µc|z˜ = fθ(˜x) and ˜x ∈Sc}._** (4)


-----

**Algorithm 1 Proto Mahalanobis – Training**

1: Input: Task distribution p(τ ), initial θ and φ
2: Output: Meta-learned θ and φ
3: while not converged do
4: Sample a task τ = (S, Q)

5: **for c = 1 to C do**

6: **_µc ←_** _|S1c|_ **x˜∈Sc** _[f][θ][(˜]x)_

7: Λc, Φc _gφ(_ _c)_ _▷_ Eq. 4
_←_ P _Z_

9:8: **ΣComputec ←** Λc Σ + Φ[−]c [1] _candΦ[⊤]c_ **Σc** _▷_ _▷Eq.Eq. 8, 59_
_|_ _|_

10: **end for**

11: _Lτ ←_ _|Q|1_ (x,y)∈Q _[−]_ [log][ p][θ,φ][(][y][|][E][[][ω][])] _▷_ Eq. 10

12: (θ, φ) (θ, φ) _α_ _θ,φ_ _τ_
_←_ P _−_ _∇_ _L_

13: end while


**Algorithm 2 Proto Mahalanobis – Inference**

1: Input: Task τ, meta-learned θ and φ
2: for c = 1 to C do
3: **_µc ←_** _|S1c|_ **x˜∈Sc** _[f][θ][(˜]x)_

4: Λc, Φc _gφ(_ _c)_ _▷_ Eq. 4
_←_ P _Z_

5:6: ComputeΣc ← Λc Σ + Φ[−]c [1] _candΦ[⊤]c_ **Σc** _▷_ _▷Eq.Eq. 8, 59_

_|_ _|_

7: end for
8: Eval. pθ,φ(y **z,** ) for (y, x) _▷_ Eq. 11
_|_ _S_ _∈Q_


wherecovariance matrix or empirical covariance estimation, we have the meta-learnable covariance matrix Λc ∈ R[d][×][d] is a diagonal matrix and Φc ∈ R[d][×][r] is a rank-r matrix. Now, instead of the identity
consisting of the strictly positive diagonal and low-rank component for each class c = 1, . . ., C.

**Σc = Λc + ΦcΦ[⊤]c** _[.]_ (5)

It is easy to see that Σc is a valid positive semi-definite covariance matrix for positive Λc. Note
that the covariance becomes diagonal when r = 0. A natural choice for gφ is the Set Transformer
(Lee et al., 2019) which models pairwise interactions between elements of the input set, an implicit
requirement for covariance matrices.

Now, we let pθ,φ(z _y = c,_ ) = (z; µc, Σc). From Bayes’ rule (see Appendix A.1), we compute
_|_ _S_ _N_
the predictive distribution in the form of softmax function as follows,

_pθ,φ(z_ _y = c,_ )
_pθ,φ(y = c_ **z,** ) = _C_ _|_ _S_ (6)
_|_ _S_
_c[′]=1_ _[p][θ,φ][(][z][|][y][ =][ c][′][,][ S][)]_

= PC exp(− 2[1] [(][z][ −] **_[µ][c][)][⊤][Σ]c[−][1][(][z][ −]_** **_[µ][c][)][ −]_** 2[1] [log][ |][Σ][c][|][)] (7)

_c[′]=1_ [exp(][−] 2[1] [(][z][ −] **_[µ][c][′]_** [)][⊤][Σ]c[−][′][ (][1] **[z][ −]** **_[µ][c][′]_** [)][ −] [1]2 [log][ |][Σ][c][′] _[|][)]_
P

**Covariance inversion and log-determinant.** Note that the logit of the softmax function in Eq. (7)
involves the inverse covariance Σ[−]c [1] and the log-determinant log **Σc** . In contrast to both DDU and
_|_ _|_
SNGP which propose to calculate and invert an empirical feature covariance during post-processing,
the meta-learning setting requires that this inference procedure be performed on every iteration during
meta-training, which may be cumbersome if a full O(d[3]) inversion is to be performed. Therefore, we
utilize the matrix determinant lemma (Ding & Zhou, 2007) and the Sherman-Morrison formula in the
following recursive forms for both the inverse and the log determinant in Equation 7.

_i_ [Φ][i][+1][Φ]i[⊤]+1[Σ][−]i [1]
(Σi + Φi+1Φ[⊤]i+1[)][−]i+1[1] [=][ Σ]i[−][1] _−_ **[Σ]1 + Φ[−][1]** _[⊤]i+1[Σ][−]i_ [1][Φ][i][+1] (8)


det(Σi + Φi+1Φ[⊤]i+1[)][i][+1] [= (1 + Φ][⊤]i+1[Σ][−]i [1][Φ][i][+1][)][det][(][Σ][i][)] (9)

3.3 OUT-OF-DISTRIBUTION CALIBRATION WITH SCALED ENERGY

Next, in order to tackle the overconfidence problem caused softmax shift invariance (Figure 3), we propose incorporating a positive constrained function of energy h(E) = max(ϵ, _T_ [log][ P]c [exp(][−][E][c][))][,]
_−_ [1]

with temperature T, into the predictive distribution. Energy has been used for OOD detection (Liu
et al., 2020b) and density estimation (Grathwohl et al., 2019), and the success of energy in these tasks
implies that it can be used to calibrate the predictive distribution (example in Figure 1h). Results in
Grathwohl et al. (2019) show improvements in calibration, but their training procedure requires a
full input space generative model during training, adding unwanted complexity if the end goal does
not require input space generation. Our method makes use of our logit values ω = (ω1, . . ., ωC)
to parameterize the mean of a logit-normal distribution with the variance given by h(E). In this


-----

Table 1: OOD ECE on models trained on variations of the Omniglot and MiniImageNet datasets. The OOD
distribution for these models are random classes from the test set which are not present in the support set.

MAML 63.14±0.67 53.90±0.77 56.60±5.98 48.39±1.09 29.00±0.67 42.43±0.51

|Omniglot OOD Class ECE ↓ MiniImageNet OOD Class ECE ↓ Model 5-way 5-shot 5-way 1-shot 20-way 5-shot 20-way 1-shot 5-way 1-shot 5-way 5-shot|Col2|
|---|---|
|||
|||

Reptile 48.01±0.76 41.84±0.98 46.31±0.30 **35.62±0.49** 29.86±0.73 38.35±0.93
Protonet 68.50±0.69 67.64±0.63 77.58±0.37 72.07±0.63 33.23±1.20 47.06±1.30
Protonet-SN 69.43±0.57 67.67±0.70 77.84±0.44 72.36±0.58 33.24±2.14 46.76±1.40
ProtoDDU 69.16±0.63 66.61±1.15 78.14±0.19 71.39±0.74 35.31±2.09 46.82±1.28

Ours (Diag) 33.95±0.98 40.52±0.68 **40.00±0.23** 50.39±1.84 **17.19±1.80** **32.22±3.12**

|± ± ± ±|± ±|
|---|---|

Ours (Rank 1) **33.19±0.94** **39.62±2.02** 40.04±0.40 49.28±1.21 18.78±1.72 34.44±0.64

way, the logit-normal distribution variance rises in conjunction with the energy magnitude, making
predictions more uniform over the simplex for higher magnitude energies.

_pθ,φ(ωc_ **z,** ) = (ωc; ˜µc, ˜σ), where ˜µc = _c_ [(][z][ −] **_[µ][c][)][ −]_** [1]
_|_ _S_ _N_ _−_ [1]2 [(][z][ −] **_[µ][c][)][⊤][Σ][−][1]_** 2 [log][ |][Σ][c][|][,]

[(10)]

_σ˜ =_ exp (z **_µc′_** )[⊤]Σ[−]c[′][ (][1] **[z][ −]** **_[µ][c][′]_** [)]
_−_ _T[1]_ [log] _−_ _−_

_c[′]_

X   

Intuitively, h(E) is dominated by minc( _Ec_ ) thereby acting as a soft approximation to the minimum
_|_ _|_
energy magnitude (shortest Mahalanobis distance), which only becomes large when the energy is
high for all classes represented in the logits. Then, the predictive distribution becomes

_pθ,φ(y = c_ **z,** ) = _p(y = c_ **_ω)pθ,φ(ω_** **z,** )dω (11)
_|_ _S_ _|_ _|_ _S_
Z


exp(ωc[(][m][)])

_,_ _ωc[(][m][)]_ _p(ωc_ **z,** ). (12)
_c[′][ exp(][ω]c[(][m][′]_ [)]) _∼_ _|_ _S_


_≈_ _M[1]_


_m=1_


**Meta-training** At training time, we do not sample ω and use the simple deterministic approximation1 _pθ,φ(y|z, S) ≈_ _pθ,φ(y|E[ω]). Therefore, the loss for each task becomes Lτ_ (θ, φ) =

_|Q|_ (x,y)∈Q _[−]_ [log][ p][θ,φ][(][y][|][E][[][ω][])][. We then optimize][ θ][ and][ φ][ by minimizing the expected loss]

Ep(τ )[ _τ_ (θ, φ)] over the task distribution p(τ ) via episodic training.

PL

**Energy scaling.** Inference with equation 11 can still benefit from temperature scaling of ˜σ in 10.
Therefore, in order properly scale the variance to avoid underconfident ID performance, we tune the
temperature parameter T after training. Specifically, we start with T = 1 and iteratively increase T
by 1 until E [ log p(y **z,** )] E [ log p(y E[ω])], where log p(y E[ω]) is the NLL evaluated
_D_ _−_ _|_ _S_ _≤_ _D_ _−_ _|_ _−_ _|_
by using only the deterministic logits E[ω].

3.4 SPECTRAL NORMALIZATION.


Lastly, we enforce a bi-Lipschitz regularization fθ by employing both residual connections and
spectral normalization on the weights (Liu et al., 2020a), such that Equation 13 is satisfied. Using
features Z, the calculation of covariance (Z **_µc)(Z_** **_µc)[⊤]_** and the subsequent mean and variance
_−_ _−_
of 10 both implicitly utilize distance, therefore we require bi-Lipschitz regularization of fθ. We
choose spectral normalization via the power iteration method, also known as the Von Mises Iteration
(Mises & Pollaczek-Geiringer, 1929), due to its low memory and computation overhead as compared
to second order methods such as gradient penalties (Arjovsky et al., 2017). Specifically, for features
at hidden layer h(·), at depth l, and for some constants α1, α2, for all zi and zj, we enforce:

_α1_ **z[(]i[l][)]** **z[(]j[l][)]** _h(z[(]i[l][−][1)])_ _h(z[(]j[l][−][1)])_ _α2_ **z[(]i[l][)]** **z[(]j[l][)]** (13)
_||_ _−_ _[||]_ _≤_ _||_ _−_ _||_ _≤_ _||_ _−_ _[||][.]_

4 EXPERIMENTS

The goal of our experimental evaluation is to answer the following questions. 1) What is the benefit
of each component of our proposed model? 2) Does gφ produce more expressive covariances than


-----

0.5 ProtoDDU Method MAML

Protonet Reptile

0.4 ProtonetSN Ours (Diag)

ProtoSNGP Ours (Rank-1)

ECE0.3

0.2

0.1

0.0

Test 1 2 3 4 5

Shift Intensity: Omniglot-C (5-way/5-shot)

0.5

0.4

ECE0.3

0.2

0.1

0.0

Test 1 2 3 4 5

Shift Intensity: Omniglot-C (5-way/1-shot)

0.6

0.4

ECE

0.2

0.0

Test 1 2 3 4 5

Shift Intensity: Omniglot-C (20-way/5-shot)

0.6

0.5

0.4

ECE0.3

0.2

0.1

0.0

Test 1 2 3 4 5

Shift Intensity: Omniglot-C (20-way/1-shot)


Figure 4: ECE results for all models on different variants of the Omniglot dataset. ProtoMahalanobis models
show comparable in distribution ECE while significantly improving ECE over the baselines on corrupted
instances from the dataset.

empirical features? 3) How does the ID/OOD calibration and accuracy compare with other popular
baseline models?

**Datasets. For few shot learning, we evaluate our model on both the Omniglot (Lake et al., 2015)**
and MiniImageNet (Vinyals et al., 2017) datasets. We utilize corrupted versions (Omniglot-C and
MiniImageNet-C) which consists of 17 corruptions at 5 different intensities (Hendrycks & Dietterich,
2019). We follow the precedent set by Snell et al. (2017) and test Omniglot for 1000 random episodes
and MiniImageNet for 600 episodes. For corruption experiments, the support set is uncorrupted, and
corruption levels 0-5 are used as the query set (0 being the uncorrupted query set). We also experiment
with multiple toy datasets which include half-moons, and concentric circles for binary classification
and random 2D multivariate Gaussian distributions for multiclass classification (Figure 1). On the toy
datasets, we create task distributions by sampling random tasks with biased support sets, applying
random class shuffling and varying levels of noise added to each task. Randomly biasing each task
ensures that no single task contains information from the whole distribution and therefore, the true
distribution must be meta-learned through the episodic training over many such tasks. For a detailed
explanation of the exact toy dataset task creation procedure, see the appendix section A.2.

**Baselines. We compare our model against Protonets (Snell et al., 2017), A spectral normalized**
version of Protonets (Protonet-SN), MAML (Finn et al., 2017), Reptile (Nichol et al., 2018), and


-----

Method

0.4 ProtoDDU MAML

Protonet Reptile
ProtonetSN Ours (Diag)

0.3 ProtoSNGP Ours (Rank-1)

ECE0.2

0.1

0.0

Test 1 2 3 4 5

Shift Intensity: MiniImageNet-C (5-way/5-shot)

0.4

0.3

0.2

ECE

0.1

0.0

Test 1 2 3 4 5

Shift Intensity: MiniImageNet-C (5-way/1-shot)


Figure 5: ECE for different variants of the MiniImageNet dataset. ProtoMahalanobis models show improved
ECE on corrupted data instances while maintaining comparable performance on in-distribution data.

straightforward few-shot/protonet adaptations of Spectral Normalized Neural Gaussian Processes
(ProtoSNGP) (Liu et al., 2020a) and Deep Deterministic Uncertainty (ProtoDDU) (Mukhoti et al.,
2021). These models represent a range of both metric based, gradient based, and covariance based
meta learning algorithms. All baseline models are temperature scaled after training, with the
temperature parameter optimized via LBFGS for 50 iterations with a learning rate of 0.001. This
follows the temperature scaling implementation from Guo et al. (2017).

**Calibration Error. We provide results for Expected Calibration Error (ECE) (Guo et al., 2017) on**
various types of OOD data in Figures 4 and 5 as well as Table 1. Accuracy and NLL are reported
in Appendix A.8. Meta learning generally presents a high correlation between tasks, but random
classes from different tasks which are not in the current support set S should still be treated as
OOD. In Table 1 we provide results where the query set Q consists of random classes not in S.
ProtoMahalanobis models perform the best in every case except for Omniglot 20-way/1-shot, where
Reptile showed the lowest ECE. The reason for this can be seen in Figure 4, where Reptile shows
poor ID performance relative to all other models. Under-confidence on ID data can lead to better
confidence scores on OOD data, even though the model is poorly calibrated. Likewise we also
evaluate our models on Omniglot-C and MiniImageNet-C in Figures 4 and 5. As the corruption
intensity increases, ProtoMahalanobis models exhibit lower ECE in relation to baseline models while
maintaining competitive ID performance. Overall, Reptile shows the strongest calibration of baseline
models although it can be underconfident on ID data as can be seen in Figure 4.

In our experiments, transductive batch normalization used in MAML/Reptile led to suboptimal results,
as the normalization statistics depend on the query set which is simultaneously passed through the
network. Passing a large batch of corrupted/uncorrupted samples caused performance degradation on
ID data and presented an unrealistic setting. We therefore utilized the normalization scheme proposed
by Nichol et al. (2018) which creates batch normalization statistics based on the whole support set
plus a single query instance.

**Eigenvalue Distribution. In Figure 6, we evaluate the effectiveness of meta learning the low rank**
covariance factors with gφ by analyzing the eigenvalue distribution of both empirical covariance
from DDU/SNGP and the encoded covariance from gφ (Equation 5). The empirically calculated
covariances exhibit lower diversity in eigenvalues, which implies that the learned Gaussian distribution
is more spherical and uniform for every class. ProtoMahalanobis models, on the other hand, exhibit a
more diverse range of eigenvalues, leading to non-trivial ellipsoid distributions. We also note that in
addition to more diverse range of eigenvalues, the differences between the distributions of each class
in S are also amplified in ProtoMahalanobis models, indicating a class specific variation between


-----

10




10


0
0.00 0.25 0.50 0.75 1.00

|Col1|Col2|Col3|Col4|
|---|---|---|---|
||||class 0 1|
||||2 3|
||||4|
|||||
|||||


class

0
1
2
3
4


(b) ProtoDDU Σ[−][1] Eigenvals


0
0 50 100 150 200

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
|||||class 0 1 2|
||||3 4|3 4|
||||||
||||||


class

0
1
2
3
4


(c) ProtoSNGP Σ[−][1] Eigenvals

|Col1|Col2|class 0|
|---|---|---|
|||1 2|
|||3 4|
||||
||||
||||


class

0
1
2
3
4


(a) Ours Σ[−][1] Eigenvals


Figure 6: Precision matrix eigenvalue distribution for various meta learning model variants. A diverse
distribution of eigenvalues which varies by class, indicates a class specific, non-spherical Gaussian distribution
is learned. Data comes from Omniglot 5-way/5-shot experiments.

learned covariance factors. Extra figures are reported in the Appendix A.7, where it can be seen that
the eigenvalue distribution becomes less diverse for ProtoMahalanobis models in the one-shot setting.

**Architectures. For both Omniglot and MiniImageNet experiments, we utilize a 4 layer convolutional**
neural network with 64 filters, followed by BatchNorm and ReLU nonlinearities. Each of the four
layers is followed by a max-pooling layer which results in a vector embedding of size 64 for Omniglot
and 1600 for MiniImageNet. Exact architectures can be found in Appendix A.9. Protonet-like models
use BatchNorm with statistics tracked over the training set, and MAML-like baselines use Reptile
Norm (Nichol et al., 2018). As spectral normalized models require residual connections to maintain
the lower Lipschitz bound in equation 13, we add residual connections to the CNN architecture in all
Protonet based models.


4.1 IMPLEMENTATION DETAILS

**ProtoSNGP & ProtoDDU Both ProtoSNGP and ProtoDDU baselines are adapted to meta learning**
by using the original backbone implementation plus the addition of a positive constrained meta
parameter for the first diagonal term in Equation 8 which is shared among all classes. This provides
meta knowledge and a necessary first step in applying the recursive formula for inversion to make
predictions on each query set seen during during training.

**Covariance Encoder gφ. We utilize the Set Transformer (Lee et al., 2019), as the self-attention**
performed by the transformer is an expressive means to encode pairwise information between inputs.
We initialize the seeds in the pooling layers (PMA), with samples from N (0, 1). We do not use
any spectral normalization in gφ, as it should be sufficient to only require that the input to the
encoder is composed of geometry preserving features. Crucially, we remove the residual connection
_Q + σ(QK_ _[⊤])V as we found that this led to the pooling layer ignoring the inputs and outputting an_
identical covariance for each class in each task. In the one-shot case, we skip the centering about the
centroid in Equation 4 because it would place all class centroids at the origin.


5 CONCLUSION

It is widely known that DNNs can be miscalibrated for OOD data. We have shown that existing
covariance based uncertainty quantification methods fail to calibrate well when given a limited
amounts of data for class-specific covariance construction for meta learning. In this work, we have
proposed a novel method which meta-learns a diagonal or diagonal plus low rank covariance matrix
which can be used for downstream tasks such as uncertainty calibration. Additionally, we have
proposed an inference procedure and energy tuning scheme which can overcome miscalibration
due to the shift invariance property of softmax. We further enforce bi-Lipschitz regularization of
neural network layers to preserve relative distances between data instances in the feature spaces. We
validated our methods on both synthetic data and two benchmark few-shot learning datasets, showing
that the final predictive distribution of our method is well calibrated under a distributional dataset
shift when compared with relevant baselines.


-----

6 ACKNOWLEDGEMENTS

This work was supported by the Institute of Information & communications Technology Planning
& Evaluation (IITP) grant funded by the Korea government(MSIT) (No.2019-0-00075, Artificial
Intelligence Graduate School Program(KAIST)), the Engineering Research Center Program through
the National Research Foundation of Korea (NRF) funded by the Korean Government MSIT (NRF2018R1A5A1059921), the Institute of Information & communications Technology Planning &
Evaluation (IITP) grant funded by the Korea government (MSIT) No. 2021-0-02068 (Artificial
Intelligence Innovation Hub), and the National Research Foundation of Korea (NRF) funded by the
Ministry of Education (NRF2021R1F1A1061655).

REFERENCES

Bruno Andreis, Jeffrey Willette, Juho Lee, and Sung Ju Hwang. Mini-batch consistent slot set encoder
for scalable set encoding. arXiv preprint arXiv:2103.01615, 2021.

Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein gan, 2017.

Jiu Ding and Aihui Zhou. Eigenvalues of rank-one updated matrices with some applications. Applied
_Mathematics Letters, 20(12):1223–1226, 2007._

Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In International Conference on Machine Learning, pp. 1126–1135. PMLR, 2017.

Will Grathwohl, Kuan-Chieh Wang, Jörn-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi,
and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like
one. arXiv preprint arXiv:1912.03263, 2019.

Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural
networks, 2017.

Matthias Hein, Maksym Andriushchenko, and Julian Bitterwolf. Why relu networks yield highconfidence predictions far away from the training data and how to mitigate the problem, 2019.

Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common
corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019.

Haiwen Huang, Zhihan Li, Lulu Wang, Sishuo Chen, Bin Dong, and Xinyu Zhou. Feature space
singularity for out-of-distribution detection. arXiv preprint arXiv:2011.14654, 2020.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
_arXiv:1412.6980, 2014._

Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning
through probabilistic program induction. Science, 350(6266):1332–1338, 2015.

Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks. In International
_Conference on Machine Learning, pp. 3744–3753. PMLR, 2019._

Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting
out-of-distribution samples and adversarial attacks. arXiv preprint arXiv:1807.03888, 2018.

Jeremiah Zhe Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax-Weiss, and Balaji Lakshminarayanan. Simple and principled uncertainty estimation with deterministic deep learning via
distance awareness. arXiv preprint arXiv:2006.10108, 2020a.

Weitang Liu, Xiaoyun Wang, John D Owens, and Yixuan Li. Energy-based out-of-distribution
detection. arXiv preprint arXiv:2010.03759, 2020b.

RV Mises and Hilda Pollaczek-Geiringer. Praktische verfahren der gleichungsauflösung. ZAMM_Journal of Applied Mathematics and Mechanics/Zeitschrift für Angewandte Mathematik und_
_Mechanik, 9(1):58–77, 1929._


-----

Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.

Jishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort, Philip H. S. Torr, and Yarin Gal. Deterministic
neural networks with appropriate inductive biases capture epistemic and aleatoric uncertainty,
2021.

Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms, 2018.

Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua V
Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty?
evaluating predictive uncertainty under dataset shift. arXiv preprint arXiv:1906.02530, 2019.

Massimiliano Patacchiola, Jack Turner, Elliot J. Crowley, and Amos Storkey. Bayesian meta-learning
for the few-shot setting via deep kernels. In Advances in Neural Information Processing Systems,
2020.

Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In ICLR, 2017.

Jürgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to learn:
_the meta-meta-... hook. PhD thesis, Technische Universität München, 1987._

Jake Snell, Kevin Swersky, and Richard S Zemel. Prototypical networks for few-shot learning. arXiv
_preprint arXiv:1703.05175, 2017._

Sebastian Thrun and Lorien Pratt (eds.). Learning to Learn. Kluwer Academic Publishers, Norwell,
MA, USA, 1998. ISBN 0-7923-8047-9.

Joost Van Amersfoort, Lewis Smith, Yee Whye Teh, and Yarin Gal. Uncertainty estimation using a
single deep deterministic neural network. In International Conference on Machine Learning, pp.
9690–9700. PMLR, 2020.

Joost van Amersfoort, Lewis Smith, Andrew Jesson, Oscar Key, and Yarin Gal. Improving deterministic uncertainty estimation in deep learning for classification and regression, 2021.

Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching Networks for One
Shot Learning. In NIPS, 2016.

Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning, 2017.

Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Ruslan Salakhutdinov, and
Alexander Smola. Deep sets. arXiv preprint arXiv:1703.06114, 2017.


-----

A APPENDIX

A.1 LOSS DERIVATION (EQUATION 7)

The full derivation of Equation 7 can be achieved by first applying Bayes’ Rule, assuming a simple
uniform prior over the class labels, p(yi **xi) can be proportionately expressed as,**
_|_

_p(yi_ **xi) =** _[p][(][x][i][|][y][k][)][p][(][y][k][)]_ _p(xi_ _yk)p(yk)_ (14)
_|_ _p(xi)_ _∝_ _|_

In which case the objective of the model becomes raising the class conditional p(xi _yi), while_
_|_
simultaneously lowering p(xi _yj)_ _j_ = i. This is in fact equivalent to a softmax + cross entropy
_|_ _∀_ _̸_
loss over the class conditional densities which are output from our model. In the softmax case,
maximizing p(yi **xi) for a given class can be done by,**
_|_

_e[z][i]_
_p(yi_ **xi) =** (15)
_|_ _z[′][ e][z][′]_

Which them implies that the loss to be minimized is the following, commonly known as the negativeP
log likelihood of the data, or the empirical cross entropy between the true data distribution and the
predictive distribution of the model.

_NLL = E_ [ log p(y **x)]**
_L_ _D_ _−_ _|_


= [1]

_N_

= [1]

_N_

_LCE = −_

_≈_ _N[1]_

= [1]


log p(yi **xi)**
_−_ _|_
_i=0_

X


exp(zj[′] [)]
_zj[′]_

X


_−_ _zj + log_


_i=0_


(16)


_px(x) log pθ(y_ **x)dx**
**x** _|_

_N_

log pθ(yi **xi)**
_−_ _|_
_i=0_

X


exp(zj[′] [)]
_zj[′]_

X


_−_ _zj + log_


_i=0_


In our case, assuming a uniform prior over the classes, we can analogously formulate the loss as,

= E [ log p(x _y)p(y)]_
_L_ _D_ _−_ _|_


= [1]

_N_

= [1]

_N_

= [1]


_p(x[′]i[|][y]k[)][p][(][y]k[)]_
**x[′]**

X


log p(xi _yk)p(yk) + log_
_−_ _|_


_i=0_

_N_

_i=0_

X

_N_

_i=0_

X


(17)


_p(x[′]i[|][y]k[)]_
**x[′]**

X


log p(xi _yk)_ log p(yk) + log p(yk) + log
_−_ _|_ _−_


_p(x[′]i[|][y]k[)]_
**x[′]**

X


log p(xi _yk) + log_
_−_ _|_


A.2 TOY DATASETS

To add bias to each samples task from our 2D toy datasets, we first randomly choose an axis (X or Y)
for each class and then slice the datapoints in half randomly. We then sample the support set from the


-----

chosen biased subset and leave the rest of the remaining points for the query set. Each sampled task
calculates the mean and variance from the support set, which are then used to normalize all instances
in S and Q.

Dataset N-Way K-Shot

Circles 2 5
Moons 2 5
Gaussians 10 10

A.2.1 META MOONS

For the Meta Moons dataset, we randomly invert the classes to make sure that the class indices appear
in a random order for each task. We add a random amount of Gaussian noise to each moon with a
uniform standard deviation in the range of (0, 0.25].

Figure 7: Random task samples from the Meta Moons dataset.

A.2.2 META CIRCLES

For the Meta Circles dataset, we randomly invert the order of the classes so that the inner circle and
the outer circle are not guaranteed to appear in the same order on every task. We inject a random
amount of Gaussian noise into the data, with a uniformly random standard deviation in the range of
(0, 0.25]. We also randomly choose the scale factor between the size of the inner circle and the outer
circle, which is uniformly random in the range of (0, 0.8]

Figure 8: Random task samples from the Meta Circles dataset.

A.2.3 META GAUSSIANS

The task construction of the Meta Gaussians dataset requires that we construct random positive
semidefinite covariance matrices for each class. We first uniformly sample N 2 × 2 matrices in
the range U (−1, 1) and perform a QR decomposition to extract orthonormal matrices Q. We then
sample a random diagonal D ∼ _U_ (0, 1), and construct the final matrix as QDQ[⊤] which is positive


-----

Table 2: Accuracy, ECE, NLL, and OOD AUPR for different n-way k-shot classification problems on the
Omniglot-C dataset which contains 17 different corruptions at 5 different intensity levels. MAML/Reptile both
utilize ‘Reptile Norm’ instead of transductive BatchNorm

Accuracy ↑ NLL ↓

Model 5-way 5-shot 5-way 1-shot 20-way 5-shot 20-way 1-shot 5-way 5-shot 5-way 1-shot 20-way 5-shot 20-way 1-shot


|MAML 65.02±17.94 64.72±16.96 48.17±23.93 44.35±22.27 Reptile 61.29±18.61 60.01±17.63 46.55±23.75 43.42±22.13|3.658±2.421 1.528±0.869 5.962±3.822 3.668±1.873 2.300±1.345 1.571±0.796 3.937±2.357 2.864±1.380|
|---|---|
|Reptile 61.29±18.61 60.01±17.63 46.55±23.75 43.42±22.13 Protonet 60.91±19.13 58.15±19.68 46.29±25.21 43.11±25.55 Protonet-SN 60.08±19.47 57.64±19.89 46.19±25.22 43.47±25.44 ProtoDDU 60.31±19.19 58.03±19.57 45.75±25.33 43.90±25.22 ProtoSNGP 59.18±19.66 57.03±19.91 46.49±25.12 44.37±24.85|2.300±1.345 1.571±0.796 3.937±2.357 2.864±1.380 6.526±3.800 6.539±4.029 10.706±5.689 9.119±4.890 7.189±4.261 6.541±3.960 11.200±6.168 8.446±4.590 10.945±7.143 10.428±7.186 18.014±9.740 17.039±9.935 2.534±1.302 2.015±0.967 6.409±3.196 4.151±1.982|
|Ours (Diag) 60.77±19.12 57.71±19.88 45.98±25.31 43.05±25.59 Ours (Rank 1) 59.88±19.55 58.15±19.59 45.47±25.57 43.12±25.57 Ours (Rank 2) 59.68±19.61 56.61±20.21 45.53±25.52 43.28±25.45 Ours (Rank 4) 60.28±19.51 59.14±19.32 45.87±25.44 42.38±25.90 Ours (Rank 8) 59.69±19.69 58.21±19.63 45.93±25.39 43.53±25.42|1.010±0.476 1.205±0.547 2.466±1.141 3.854±1.775 1.020±0.481 1.312±0.595 2.541±1.212 3.564±1.624 1.045±0.495 1.314±0.594 2.561±1.194 3.787±1.760 1.068±0.510 1.263±0.580 2.528±1.168 4.019±1.877 1.055±0.501 1.329±0.613 2.486±1.156 3.474±1.607|
|ECE ↓|OOD AUPR|


ProtonetProtonet-SNProtoDDUProtoSNGPMAMLReptile 34.0835.1227.8429.2628.1024.85±±±±±±13.1215.2517.0217.4614.1914.53 19.6919.9635.6936.2733.7928.70±±±±±±17.6217.8516.6114.0710.2911.74 26.3343.5743.7537.6240.4233.64±±±±±±13.3320.8920.8918.1619.4617.35 43.4742.0640.8336.4327.9519.98±±±±±±20.6920.1019.5517.3613.129.77 0.6170.8670.8690.6750.8750.602±±±±±±0.1690.1690.0830.1720.0810.060 0.8530.8570.5520.8550.6720.653±±±±±±0.0980.1630.1640.0440.1630.075 0.6460.8760.8750.6470.8780.440±±±±±±0.0870.1720.1710.0740.1730.032 0.7200.8630.8620.5790.8580.484±±±±±±0.1130.1660.1660.0620.1640.098

Ours (Rank 8)Ours (Rank 4)Ours (Rank 2)Ours (Diag)Ours (Rank 1) 7.057.106.625.876.10±±±±±2.872.583.423.403.09 11.8111.1512.8911.1712.38±±±±±5.114.964.935.604.95 11.4911.3010.6110.879.81±±±±±4.364.844.755.125.48 22.9021.9920.5621.4220.26±±±±±10.149.458.689.659.22 0.8670.8690.8690.8680.868±±±±±0.1690.1690.1690.1690.169 0.8510.8510.8530.8550.850±±±±±0.1620.1620.1630.1620.164 0.8760.8740.8740.8740.876±±±±±0.1720.1710.1710.1710.172 0.8630.8640.8620.8630.863±±±±±0.1660.1670.1660.1660.166

_±_ _±_ _±_ _±_ _±_ _±_ _±_ _±_


semi-definite. This leads to the distribution of each class being an elliptical multivariate Gaussian
distribution.

Figure 9: Random task samples from the Meta Gaussians dataset.

A.3 EXTRA RESULTS

We provide extra results on the MiniImageNet-C and Omniglot-C dataset here. Tables 2 and 3 contain
results averaged over the whole corrupted dataset, including the natural test set and all 5 levels of
corruption

A.4 SET ENCODING RELATED WORKS

Set encoding functions require special end-to-end design considerations such as obeying permutation
invariance w.r.t. the input set f ( **X1, X2, ..., Xn** ) = f ( **Xπ(1), Xπ(2), ..., Xπ(n)** ) for any random
_{_ _}_ _{_ _}_
permutation of indices π(.). Likewise, the intermediate latent representations must satisfy permutation
equivariance such that f ( **Xπ(1), Xπ(2), ..., Xπ(n)** ) = _fπ(1)(X), fπ(2)(X), ..., fπ(1)(X)_ .
_{_ _}_ _{_ _}_

Deepsets (Zaheer et al., 2017) first proposed basic adaptations of linear and convolutional neural
networks which obey the above required properties and have the addition of a sum decomposable
(permutation invariant) pooling function and decoder to match the requirements of the given task.
As sets can have complex interactions between elements, it may be beneficial to model pairwise
interactions between set elements. The Set Transformer (Lee et al., 2019) uses a transformer
architecture with self attention to model such pairwise interactions between set elements. As


-----

Table 3: Accuracy, ECE, NLL, and AUPR for different n-way k-shot classification problems on the
MiniImageNet-C dataset which contains 17 different corruptions at 5 different intensity levels. MAML/Reptile
both utilize ‘Reptile Norm’ instead of transductive BatchNorm

Accuracy ↑ NLL ↓

Model 5-way 1-shot 5-way 5-shot 5-way 1-shot 5-way 5-shot


|MAML 31.94±7.85 39.30±12.92 Reptile 33.07±7.85 39.18±12.84|1.720±0.242 1.748±0.462 1.580±0.150 1.581±0.339|
|---|---|
|Reptile 33.07±7.85 39.18±12.84 Protonet 33.43±8.49 41.35±14.03 Protonet-SN 32.79±8.56 40.95±14.21 ProtoDDU 33.62±8.92 41.46±14.35 ProtoSNGP 33.56±8.65 41.20±13.66|1.580±0.150 1.581±0.339 1.801±0.323 2.123±0.798 1.836±0.341 2.112±0.802 1.906±0.465 2.180±0.932 1.699±0.267 1.889±0.614|
|Ours (Diag) 33.21±8.68 40.69±13.66 Ours (Rank 1) 33.19±8.45 40.89±13.90 Ours (Rank 2) 33.03±8.54 40.90±13.87 Ours (Rank 4) 32.52±8.45 41.24±13.84 Ours (Rank 8) 32.41±8.55 40.33±13.77|1.556±0.158 1.630±0.423 1.575±0.171 1.699±0.484 1.571±0.174 1.659±0.456 1.591±0.191 1.696±0.486 1.581±0.175 1.644±0.442|
|ECE ↓|AUPR ↑|


|MAML 19.62±9.02 24.03±11.29|0.536±0.083 0.628±0.106|
|---|---|
|Reptile 12.32±3.84 17.60±8.98 Protonet 20.78±8.98 27.75±13.62 Protonet-SN 21.66±9.28 27.99±13.90 ProtoDDU 22.70±9.69 27.13±13.71 ProtoSNGP 20.30±7.28 25.96±12.19|0.756±0.131 0.749±0.124 0.637±0.094 0.579±0.071 0.629±0.085 0.572±0.068 0.530±0.041 0.620±0.061 0.636±0.080 0.653±0.089|
|Ours (Diag) 8.13±3.72 15.57±7.32 Ours (Rank 1) 9.02±3.83 17.27±8.42 Ours (Rank 2) 9.27±4.02 16.27±7.33 Ours (Rank 4) 9.87±5.29 16.71±7.85 Ours (Rank 8) 9.27±4.38 15.93±7.50|0.636±0.092 0.578±0.065 0.625±0.087 0.574±0.067 0.632±0.087 0.581±0.064 0.629±0.085 0.569±0.058 0.637±0.092 0.574±0.069|


Table 4: Accuracy, ECE, NLL, and AUPR for different n-way k-shot classification problems on the Omniglot
dataset. All metrics are measured on the natural test set except AUPR/AUROC which is measured using random
classes which are different from the classes in the support set. Our model maintains competitive performance for
ID data on all metrics. MAML/Reptile both utilize ‘Reptile Norm’ instead of transductive BatchNorm


Accuracy ↑ NLL ↓

Model 5-way 5-shot 5-way 1-shot 20-way 5-shot 20-way 1-shot 5-way 5-shot 5-way 1-shot 20-way 5-shot 20-way 1-shot

|MAML 99.51±0.06 96.55±0.23 97.96±0.28 91.97±0.27 Reptile 98.55±0.07 95.72±0.38 96.50±0.07 90.95±0.47|0.015±0.002 0.104±0.006 0.078±0.015 0.289±0.013 0.054±0.002 0.150±0.010 0.142±0.002 0.365±0.015|
|---|---|
|Reptile 98.55±0.07 95.72±0.38 96.50±0.07 90.95±0.47 Protonet 99.65±0.02 98.24±0.16 99.29±0.05 97.47±0.09 Protonet-SN 99.67±0.04 98.26±0.12 99.26±0.06 97.51±0.16 ProtoDDU 99.70±0.05 98.37±0.11 99.28±0.05 97.54±0.16 ProtoSNGP 99.65±0.07 98.23±0.08 99.23±0.06 97.41±0.13|0.054±0.002 0.150±0.010 0.142±0.002 0.365±0.015 0.013±0.002 0.059±0.007 0.027±0.006 0.087±0.008 0.013±0.003 0.061±0.007 0.029±0.006 0.086±0.011 0.010±0.002 0.058±0.010 0.027±0.005 0.085±0.010 0.012±0.003 0.054±0.004 0.029±0.006 0.085±0.006|
|Ours (Diag) 99.64±0.06 98.21±0.23 99.26±0.01 97.49±0.09 Ours (Rank 1) 99.63±0.06 98.21±0.12 99.29±0.03 97.61±0.14 Ours (Rank 2) 99.62±0.06 98.30±0.23 99.30±0.06 97.56±0.10 Ours (Rank 4) 99.66±0.04 98.42±0.17 99.28±0.07 97.56±0.17 Ours (Rank 8) 99.64±0.02 98.35±0.16 99.32±0.04 97.63±0.16|0.020±0.002 0.064±0.005 0.032±0.002 0.089±0.006 0.020±0.002 0.067±0.005 0.031±0.002 0.086±0.007 0.020±0.002 0.064±0.010 0.031±0.003 0.087±0.007 0.019±0.002 0.060±0.005 0.032±0.003 0.088±0.007 0.019±0.001 0.059±0.004 0.030±0.003 0.084±0.008|
|ECE ↓|OOD AUPR ↑|



ProtoSNGPProtoDDUProtonet-SNProtonetReptileMAML 0.090.070.090.091.640.05±±±±±±0.040.020.040.020.080.03 0.150.420.510.543.951.06±±±±±±0.040.150.170.090.410.12 0.140.140.210.193.211.39±±±±±±0.050.030.040.030.110.87 0.200.330.398.730.354.95±±±±±±0.130.040.110.120.090.68 0.9940.4820.9940.9940.8560.831±±±±±±0.0010.0030.0010.0000.0060.019 0.4750.9770.9770.8130.9770.799±±±±±±0.0030.0040.0010.0020.0150.010 0.9890.9900.4960.9900.5910.622±±±±±±0.0010.0020.0010.0000.0020.023 0.9720.4960.9740.9750.5790.578±±±±±±0.0020.0020.0010.0020.0030.007

Ours (Rank 8)Ours (Rank 4)Ours (Rank 2)Ours (Rank 1)Ours (Diag) 1.031.001.071.061.07±±±±±0.080.110.100.090.12 1.882.002.112.142.02±±±±±0.170.450.380.300.12 1.101.161.131.141.13±±±±±0.100.080.060.050.05 1.471.521.711.471.58±±±±±0.110.120.270.280.26 0.9940.9940.9940.9940.994±±±±±0.0000.0010.0000.0010.001 0.9780.9770.9760.9770.976±±±±±0.0020.0040.0020.0020.003 0.9900.9900.9900.9900.990±±±±±0.0010.0010.0010.0010.000 0.9740.9750.9750.9740.974±±±±±0.0010.0020.0010.0010.001


_±_ _±_ _±_ _±_ _±_ _±_ _±_ _±_


-----

Table 5: Accuracy, NLL, ECE, and AUPR for different n-way k-shot classification problems on the MiniImageNet dataset. All metrics are measured on the natural test set except AUPR/AUROC which is measured
using random classes which are different from the classes in the support set. Our model maintains competitive
performance for ID data on all metrics. MAML/Reptile both utilize ‘Reptile Norm’ instead of transductive
BatchNorm

Accuracy ↑ NLL ↓

Model 5-way 1-shot 5-way 5-shot 5-way 1-shot 5-way 5-shot


|MAML 46.13±1.19 64.71±0.50 Reptile 47.79±1.21 62.89±0.88|1.297±0.015 0.921±0.017 1.297±0.023 0.967±0.017|
|---|---|
|Reptile 47.79±1.21 62.89±0.88 Protonet 48.61±0.91 67.57±0.55 Protonet-SN 47.47±0.90 68.03±0.79 ProtoDDU 49.57±0.53 68.31±0.59 ProtoSNGP 49.55±0.90 66.89±0.88|1.297±0.023 0.967±0.017 1.245±0.019 0.832±0.008 1.279±0.017 0.820±0.016 1.246±0.006 0.816±0.016 1.232±0.012 0.841±0.020|
|Ours (Diag) 48.31±0.39 66.12±1.76 Ours (Rank 1) 48.57±0.96 66.54±0.66 Ours (Rank 2) 48.08±0.99 67.17±0.56 Ours (Rank 4) 47.76±0.62 66.73±0.37 Ours (Rank 8) 48.91±0.87 66.58±1.67|1.277±0.023 0.887±0.044 1.267±0.013 0.859±0.016 1.271±0.021 0.853±0.017 1.274±0.020 0.868±0.010 1.272±0.039 0.873±0.038|
|ECE ↓|AUPR ↑|


|MAML 3.39±1.09 2.79±0.45|0.508±0.009 0.544±0.005|
|---|---|
|Reptile 4.65±0.76 1.64±0.23 Protonet 5.62±1.08 4.09±0.86 Protonet-SN 6.77±1.81 3.22±0.96 ProtoDDU 7.33±1.44 3.46±0.55 ProtoSNGP 7.21±1.26 4.06±0.84|0.517±0.006 0.542±0.003 0.609±0.010 0.596±0.006 0.608±0.011 0.602±0.002 0.473±0.006 0.479±0.003 0.621±0.007 0.687±0.004|
|Ours (Diag) 9.40±1.82 8.32±2.46 Ours (Rank 1) 8.09±1.60 6.77±0.53 Ours (Rank 2) 7.50±2.30 7.92±1.34 Ours (Rank 4) 7.57±2.45 8.02±2.19 Ours (Rank 8) 10.03±3.64 8.80±1.53|0.607±0.011 0.602±0.013 0.605±0.007 0.611±0.008 0.610±0.008 0.611±0.003 0.603±0.005 0.609±0.005 0.609±0.010 0.606±0.005|


transformers have a quadratic complexity w.r.t. input set length, it may not be possible to process a
large set with a transformer and maintain permutation invariance, if the set will not fit into memory.
Therefore, recent works have also further explored how to make an attentive set encoder which
can process sets in batches (Andreis et al., 2021) while maintaining the above requirements of set
functions.

For our model, we chose to use the set transformer architecture, as it models pairwise interactions
between elements which is an implicit requirement of construction a Gaussian covariance matrix.
Therefore, it has the proper inductive biases needed to satisfy our requirement of predicting low rank
covariance factors given an input set of features.

A.5 EXTRA TOY RESULTS

In Figures 10, 11, 12, 13, 14, 15, and 16 we provide extra qualitative results on toy dataset covariances
and entropy surfaces. In Tables 6, 7, and 8 we provide tabular results of all toy experiments,
showcasing the differences between in-distribution data and random uniform OOD noise.

In Distribiution Out of Distribiution
Model Accuracy ↑ NLL ↓ ECE ↓ ECE ↓ AUPR ↑ AUROC ↑

|Protonet 97.02±1.60 0.171±0.110 2.21±1.40 ProtonetSN 97.31±1.54 0.140±0.088 2.38±1.38 Proto DDU 96.04±2.74 0.158±0.077 2.43±1.04 Proto SNGP 97.22±1.19 0.138±0.046 4.81±2.63|48.16±0.91 0.999±0.000 0.931±0.012 48.40±0.64 0.999±0.000 0.932±0.006 49.43±0.37 0.976±0.001 0.119±0.013 45.74±4.19 0.995±0.001 0.684±0.064|
|---|---|
|Ours (Diag) 96.82±1.09 0.167±0.056 5.09±1.18 Ours (Rank-1) 96.86±1.42 0.157±0.049 4.21±1.77 Ours (Rank-2) 96.90±1.55 0.162±0.042 5.13±1.39 Ours (Rank-4) 96.90±1.15 0.157±0.033 4.69±1.32 Ours (Rank-8) 96.69±1.36 0.171±0.047 4.74±1.48 Ours (Rank-16) 96.73±1.28 0.161±0.057 3.80±1.87 Ours (Rank-32) 96.73±1.28 0.170±0.042 4.49±1.01 Ours (Rank-64) 96.73±1.55 0.159±0.045 4.43±0.99|15.66±2.65 0.999±0.000 0.937±0.006 20.60±2.24 0.999±0.000 0.934±0.008 17.74±2.76 0.999±0.000 0.939±0.006 18.49±2.62 0.999±0.000 0.937±0.006 19.75±3.53 0.999±0.000 0.935±0.007 18.47±2.30 0.999±0.000 0.939±0.007 18.22±3.26 0.999±0.000 0.939±0.005 17.90±2.49 0.999±0.000 0.938±0.005|



Table 6: Tabular results from the meta-moons toy experiment


-----

|ttreasitn 55 0|Col2|
|---|---|


Accuracy: 1.00 traintest

NLL 0.04

ENT ID/OOD: 0.07 / 0.55

ECE ID/OOD 0.03 / 0.20


(a) Meta Circles (ProtoMahalanobisFC). From left to right: entropy surface, covariances for class 1-2

|ccuracy: 0.96 ttreasitn NLL 0.10 D/OOD: 0.12 / 0.55 D/OOD 0.04 / 0.19|Col2|
|---|---|


Accuracy: 0.96 traintest

NLL 0.10

ENT ID/OOD: 0.12 / 0.55

ECE ID/OOD 0.04 / 0.19


(b) Meta Gaussians (ProtoMahalanobisFC) From left to right: entropy surface, covariances for class 1-2

Figure 10: ProtoMahalanobisFC model performance on the meta-moons and meta-circles toy datasets.

In Distribiution Out of Distribiution
Model Accuracy ↑ NLL ↓ ECE ↓ ECE ↓ AUPR ↑ AUROC ↑

|Protonet 89.12±3.92 0.343±0.091 4.33±0.83 ProtonetSN 88.93±4.12 0.342±0.098 4.23±0.71 Proto DDU 90.45±2.67 0.267±0.061 3.52±0.95 Proto SNGP 89.63±3.30 0.323±0.063 7.39±2.71|81.84±0.78 0.999±0.000 0.950±0.009 82.42±0.87 0.999±0.000 0.951±0.008 84.48±0.41 0.969±0.001 0.175±0.003 62.60±6.57 0.999±0.000 0.916±0.014|
|---|---|
|Ours (Diag) 90.61±3.54 0.279±0.075 7.24±1.57 Ours (Rank-1) 91.01±2.65 0.271±0.062 7.12±1.00 Ours (Rank-2) 91.31±2.87 0.269±0.064 7.59±1.34 Ours (Rank-4) 90.96±2.76 0.272±0.068 6.68±0.56 Ours (Rank-8) 90.99±2.52 0.268±0.065 6.74±1.02 Ours (Rank-16) 91.01±2.88 0.264±0.065 6.96±1.23 Ours (Rank-32) 90.45±3.22 0.267±0.069 6.35±0.27 Ours (Rank-64) 90.83±2.99 0.264±0.064 6.79±0.83|42.55±0.98 0.999±0.000 0.954±0.005 42.77±1.62 0.999±0.000 0.954±0.005 43.13±1.90 0.999±0.000 0.955±0.004 43.33±1.80 0.999±0.000 0.955±0.004 43.14±1.49 0.999±0.000 0.955±0.004 42.93±1.94 0.999±0.000 0.955±0.005 42.89±1.48 0.999±0.000 0.956±0.004 42.67±1.67 0.999±0.000 0.956±0.004|



Table 7: Tabular results from the meta-Gaussians toy experiment

In Distribiution Out of Distribiution
Model Accuracy ↑ NLL ↓ ECE ↓ ECE ↓ AUPR ↑ AUROC ↑

|Protonet 94.45±3.18 0.195±0.106 3.49±1.67 ProtonetSN 94.53±2.49 0.185±0.098 3.03±1.58 Proto DDU 95.02±1.79 0.165±0.088 3.62±2.27 Proto SNGP 94.49±2.09 0.192±0.071 6.05±3.56|49.19±0.25 1.000±0.000 0.952±0.007 49.17±0.21 1.000±0.000 0.952±0.007 48.87±0.18 0.972±0.001 0.072±0.008 45.14±3.08 0.992±0.001 0.683±0.055|
|---|---|
|Ours (Diag) 94.24±3.85 0.215±0.139 4.11±0.62 Ours (Rank-1) 94.08±4.62 0.214±0.158 4.34±1.46 Ours (Rank-2) 94.53±4.27 0.192±0.148 3.54±1.52 Ours (Rank-4) 94.12±4.68 0.209±0.158 4.27±1.83 Ours (Rank-8) 94.00±4.69 0.194±0.134 3.80±1.45 Ours (Rank-16) 94.12±4.40 0.205±0.148 4.12±1.93 Ours (Rank-32) 93.84±4.66 0.193±0.141 3.48±1.54 Ours (Rank-64) 94.16±4.61 0.196±0.146 3.46±1.31|14.64±4.32 1.000±0.000 0.954±0.011 19.04±8.21 1.000±0.000 0.953±0.013 18.22±3.72 1.000±0.000 0.954±0.013 19.61±4.52 1.000±0.000 0.954±0.013 19.37±4.42 1.000±0.000 0.955±0.014 20.59±5.47 1.000±0.000 0.955±0.013 20.50±5.56 1.000±0.000 0.954±0.014 19.53±6.30 1.000±0.000 0.955±0.014|



Table 8: Tabular results from the meta-circles toy experiment


-----

Accuracy: 0.87 traintest

NLL 0.38

ENT ID/OOD: 0.43 / 1.14

ECE ID/OOD 0.08 / 0.44


Figure 11: ProtoMahalanobisFC model performance on the meta Gaussians toy dataset. From the top
left: Entropy surface, covariances for clases 1-10

|ttreasitn 01 0|Col2|
|---|---|



covariances for class 1-2


Accuracy: 1.00 traintest Accuracy: 1.00 traintest

NLL 0.00 NLL 0.19

ENT ID/OOD: 0.00 / 0.01 ENT ID/OOD: 0.46 / 0.50

ECE ID/OOD 0.00 / 0.50 ECE ID/OOD 0.18 / 0.30

(a) Meta Circles (DDU). From left to right: entropy surface (distance), entropy surface (softmax sample),



|ccuracy: 0.97 ttreasitn NLL 0.09 D/OOD: 0.07 / 0.02 D/OOD 0.02 / 0.50|Col2|
|---|---|


Accuracy: 0.97 traintest

NLL 0.09

ENT ID/OOD: 0.07 / 0.02

ECE ID/OOD 0.02 / 0.50


Accuracy: 0.97 traintest

NLL 0.25

ENT ID/OOD: 0.50 / 0.53

ECE ID/OOD 0.18 / 0.25


(b) Meta Moons (DDU). From left to right: entropy surface (distance), entropy surface (softmax sample),
covariances for class 1-2

Figure 12: Proto DDU model performance on the two toy meta learning datasets.


-----

|train|Col2|
|---|---|
|Accuracy: 0.84 ttreasitn NLL 0.44 T ID/OOD: 0.36 / 0.16 E ID/OOD 0.08 / 0.83 13: Proto DDU e (distance), ent||
|||
|||
|||
|||
|||
|||
|||
|||
|||
||r|


|test 01 50|Col2|
|---|---|


|ccuracy: 0.96 test NLL 0.16 D/OOD: 0.03 / 0.05 D/OOD 0.04 / 0.49|Col2|
|---|---|


|Accuracy: 0.81 test NLL 0.54 T ID/OOD: 0.37 / 0.16 E ID/OOD 0.08 / 0.83|Col2|
|---|---|


Accuracy: 0.84 traintest Accuracy: 0.82 traintest

NLL 0.44 NLL 1.84

ENT ID/OOD: 0.36 / 0.16 ENT ID/OOD: 2.26 / 2.29

ECE ID/OOD 0.08 / 0.83 ECE ID/OOD 0.64 / 0.04

Figure 13: Proto DDU model performance on the Meta Gaussians dataset. From the top left: Entropy
surface (distance), entropy surface (softmax sample), covariances for clases 1-10

Accuracy: 1.00 traintest Accuracy: 0.96 traintest Accuracy: 0.81 traintest

NLL 0.00 NLL 0.16 NLL 0.54

ENT ID/OOD: 0.01 / 0.01 ENT ID/OOD: 0.03 / 0.05 ENT ID/OOD: 0.37 / 0.16

ECE ID/OOD 0.00 / 0.50 ECE ID/OOD 0.04 / 0.49 ECE ID/OOD 0.08 / 0.83

Figure 14: Protonet model performance on the three meta-toy datasets. From left to right: MetaCircles, Meta-Moons, Meta-Gaussians.

|ttreasitn 47 2|Col2|
|---|---|


Accuracy: 0.99 traintest Accuracy: 0.99 traintest

NLL 0.21 NLL 0.03

ENT ID/OOD: 0.47 / 0.47 ENT ID/OOD: 0.04 / 0.04

ECE ID/OOD 0.18 / 0.32 ECE ID/OOD 0.01 / 0.49


(a) Meta Circles (SNGPProtoFC). From left to right: entropy surface (softmax sample), entropy surface (distance),
covariances for class 1-2

NLL 0.25 NLL 0.10

ENT ID/OOD: 0.50 / 0.50 ENT ID/OOD: 0.18 / 0.19

ECE ID/OOD 0.19 / 0.28 ECE ID/OOD 0.05 / 0.44

|ccuracy: 0.97 ttreasitn NLL 0.25 D/OOD: 0.50 / 0.50 D/OOD 0.19 / 0.28|Col2|
|---|---|


Accuracy: 0.97 traintest

NLL 0.25

ENT ID/OOD: 0.50 / 0.50

ECE ID/OOD 0.19 / 0.28

Accuracy: 0.97 traintest

NLL 0.10

ENT ID/OOD: 0.18 / 0.19

ECE ID/OOD 0.05 / 0.44


(b) Meta Moons (SNGPProtoFC). From left to right: entropy surface (softmax sample), entropy surface (distance),
covariances for class 1-2

Figure 15: SNGPProtoFC model performance on the meta-moons and meta-circles toy datasets.


-----

Accuracy: 0.83 traintest Accuracy: 0.82 traintest

NLL 1.84 NLL 0.48

ENT ID/OOD: 2.27 / 2.29 ENT ID/OOD: 0.48 / 0.70

ECE ID/OOD 0.65 / 0.03 ECE ID/OOD 0.07 / 0.69

|Accuracy: 0.83 ttreasitn NLL 1.84 T ID/OOD: 2.27 / 2.29 E ID/OOD 0.65 / 0.03|Col2|
|---|---|


|train|Col2|
|---|---|
|Accuracy: 0.82 ttreasitn NLL 0.48 T ID/OOD: 0.48 / 0.70 E ID/OOD 0.07 / 0.69||
|||
|||
|||
|||


Accuracy: 0.83 traintest

NLL 1.84

ENT ID/OOD: 2.27 / 2.29

ECE ID/OOD 0.65 / 0.03

Accuracy: 0.82 traintest

NLL 0.48

ENT ID/OOD: 0.48 / 0.70

ECE ID/OOD 0.07 / 0.69


Figure 16: SNGPProtoFC model performance on the meta-Gaussians toy dataset. From the top left:
Entropy surface (softmax sample), entropy surface (distance), covariances for clases 1-10

A.6 FURTHER IMPLEMENTATION DETAILS

**Positive Diagonal Constraint** In order to constrain the diagonal Λ of Proto Mahalanobis models
(Equation 5) to be positive as mentioned in Section 3.2, we utilize a truncated sigmoid function
Λ = max(0.1, σ(z)). We truncate the values in order to avoid extreme values during the inversion.

**SNGP & DDU** Both SNGP (Liu et al., 2020a) and DDU (Mukhoti et al., 2021) were originally
designed under the assumption that an entire dataset would be used in the final pass to construct a
feature covariance matrix. Given that few-shot-learning contains a limited number of samples for
each task, we compose the feature covariance as a diagonal + low-rank factor Λ + ΦΦ[⊤], where Λ is
a positive constrained (via softplus) meta learned parameter. Λ can be seen as a shrinkage estimation
(δΛ + (1 − _δ)ΦΦ[⊤]) for low sample size, with a meta learned mixing coefficient δ._

In order to extend SNGP to work in the few shot learning scenario under the prototypical network
Snell et al. (2017) framework, we had to modify the original algorithm by replacing the last linear
layer with the embedding layer and centroids used by prototypical networks. Empirically, we found
that using the SNGP logit-normal inference procedure led to a severe performance decrease, therefore
our results utilized Mahalanobis distance instead.


**OOD AUPR/AUROC** In order to evaluate the OOD AUPR/AUROC metrics in the supplementary
tables, we utilize the method proposed by Liu et al. (2020b). Specifically, we use the total energy in
the logits log _i_ [exp(][z][i][)][ as the score when evaluating AUPR/AUROC.]

**Optimizers** [P]All models are trained with the Adam (Kingma & Ba, 2014) optimizer

A.7 ADDITIONAL EIGENVALUE DISTRIBUTIONS

The eigenvalue distributions highlighted in section 4 exhibit the most diverse case of eigenvalues.
However, the eigenvalues of ProtoMahalanobis precision matrices become less diverse in the one-shot
setting which is also where we are unable to mean center the respective features by class.


-----

1.0

0.9

0.8

0.7

0.6

0.5

0.4



Test

|Col1|Method ProtoDDU MAML|
|---|---|
||Protonet Reptile ProtonetSN Ours (Diag)|
|ProtoSNGP Ours (Rank-1)||
|||


2 3

Shift Intensity: Omniglot-C (5-way/5-shot)


1.0

0.9

0.8

0.7

0.6

Accuracy

0.5

0.4

Test 1 2 3 4 5

Shift Intensity: Omniglot-C (5-way/1-shot)

1.0

0.8

0.6

Accuracy

0.4

0.2

Test 1 2 3 4 5

Shift Intensity: Omniglot-C (20-way/5-shot)

1.0

0.8

0.6

Accuracy

0.4

0.2

Test 1 2 3 4 5

Shift Intensity: Omniglot-C (20-way/1-shot)


Figure 18: Accuracy boxplots for different variations of the Omniglot dataset

50


10


40

30


10

11

12

13

14

15

16

17

18

19

|Col1|Col2|Col3|Col4|Col5|clas|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|||||||



10


20

10

|Col1|Col2|class|
|---|---|---|
|||0 1 2 3 4|
|||5 6 7 8 9 10|
|||11 12 13 14 15|
|||16 17 18 19|
||||


0.0 0.5 1.0 1.5 2.0 2.5 3.0


Figure 17: From left to right: covariance, precision, and eigenvalue distribution for ProtoMahalanobis
precision matrix on Omniglot 20-way/1-shot (left) and 20-way/5-shot (right) experiments.


-----

20 Method

ProtoDDU MAML
Protonet Reptile

15 ProtonetSN Ours (Diag)

ProtoSNGP Ours (Rank-1)

NLL10

5

0

Test 1 2 3 4 5

Shift Intensity: Omniglot-C (5-way/5-shot)

20

15

NLL10

5

0

Test 1 2 3 4 5

Shift Intensity: Omniglot-C (5-way/1-shot)

25

20

NLL15

10

5

0

Test 1 2 3 4 5

Shift Intensity: Omniglot-C (20-way/5-shot)

30

25

20

NLL15

10

5

0

Test 1 2 3 4 5

Shift Intensity: Omniglot-C (20-way/1-shot)


Figure 19: NLL boxplots for different variations of the Omniglot dataset


-----

0.7 Method

ProtoDDU MAML

0.6 Protonet Reptile

ProtonetSN Ours (Diag)
ProtoSNGP Ours (Rank-1)

0.5

Accuracy0.4

0.3

Test 1 2 3 4 5

Shift Intensity: MiniImageNet-C (5-way/5-shot)

0.50

0.45

0.40

0.35

Accuracy

0.30

0.25

Test 1 2 3 4 5

Shift Intensity: MiniImageNet-C (5-way/1-shot)


Figure 20: Accuracy boxplots for different variations of the MiniImageNet dataset


Test

|Method ProtoDDU MAML Protonet Reptile ProtonetSN Ours (Diag) ProtoSNGP Ours (Rank-1)|Col2|
|---|---|
|||
|||


Shift Intensity: MiniImageNet-C (5-way/5-shot)


3.0

2.5

NLL2.0

1.5

Test 1 2 3 4 5

Shift Intensity: MiniImageNet-C (5-way/1-shot)


Figure 21: NLL boxplots for different variations of the MiniIMageNet dataset


-----

Table 9: Convolutional architecture used for MAML/Reptile Omniglot

Layers

Conv2d(1, 64, pad=1, stride=2) → BatchNorm(reptilenorm=True) → ReLU
Conv2d(64, 64, pad=1, stride=2) → BatchNorm(reptilenorm=True) → ReLU
Conv2d(64, 64, pad=1, stride=2) → BatchNorm(reptilenorm=True) → ReLU
Conv2d(64, 64, pad=1, stride=2) → BatchNorm(reptilenorm=True) → ReLU
AveragePool(2)
FC(64, nway)

Table 10: Convolutional architecture used for MAML/Reptile MiniImageNet. Reptile uses 64 filters
instead of 32.

Layers

Conv2d(1, 32, pad=1, stride=1) → BatchNorm(reptilenorm=True) → ReLU → MaxPool2d(2)
Conv2d(32, 32, pad=1, stride=2) → BatchNorm(reptilenorm=True) → ReLU → MaxPool2d(2)
Conv2d(32, 32, pad=1, stride=2) → BatchNorm(reptilenorm=True) → ReLU → MaxPool2d(2)
Conv2d(32, 32, pad=1, stride=2) → BatchNorm(reptilenorm=True) → ReLU → MaxPool2d(2)
Flatten
FC(1600, nway)

A.8 ADDITIONAL BOXPLOT RESULTS

Figures 18, 20 show extra boxplot results for accuracy while Figures 19, 21 show negative log
likelihood.

A.9 ARCHITECTURE DETAILS

Tables 9, and 10 show the backbone architectures for MAML/Reptile or Omniglot and MiniImageNet
respectively. Table 11 shows the backbone architecture for all Protonet based models.

A.10 RUNTIME ANALYSIS

In Tables 12, and 13 we provide a runtime analysis of different variants of our models and baselines.
Linear models are evaluated by using the mean and standard deviations from 50 iterations of both
training and inference on the MetaMoons dataset. Convolutional models are likewise evaluated on
50 iterations of the Omniglot dataset. All models were evaluated on a single GeForce GTX 1080 Ti
GPU. SNGP/DDU also utilize the matrix inversion outlined in Equation 8.

Mahalanobis models show slightly better (Linear) or similar (CNN) latency to SNGP/DDU for
diagonal and rank-1 variants. Latency increases as the rank goes higher due to more factors and more
iterations required for inversion and log-determinant calculations. Comparing Protonet, Protonet-SN,
and other variants which need to construct a covariance, we can see that constructing the covariance
matrix adds a cost which is roughly equivalent to spectral normalization.

Table 11: Convolutional architecture used for Protonet Models. Plain Protonets use no spectral
normalization

Layers

SpectralNorm(Conv2d(1, 64, pad=1, stride=1), residual=True, c=3) → BatchNorm() → ReLU → Dropout() → AveragePool2d(2)
SpectralNorm(Conv2d(1, 64, pad=1, stride=1), residual=True, c=3) → BatchNorm() → ReLU → Dropout() → AveragePool2d(2)
SpectralNorm(Conv2d(1, 64, pad=1, stride=1), residual=True, c=3) → BatchNorm() → ReLU → Dropout() → AveragePool2d(2)
SpectralNorm(Conv2d(1, 64, pad=1, stride=1), residual=True, c=3) → BatchNorm() → ReLU → Dropout() → AveragePool2d(2)
Flatten()
FC(features, nway)


-----

Model Train Iteration (ms) Eval Iteration (ms)

ProtoMahalanobis-FC diag 10.33±0.40 2.95±0.20
ProtoMahalanobis-FC Rank-1 10.96±0.37 3.11±0.22
ProtoMahalanobis-FC Rank-5 13.06±0.44 3.84±0.24
ProtoMahalanobis-FC Rank-10 15.65±0.35 4.63±0.20
ProtoDDU-FC 11.86±0.44 3.97±0.21
ProtoSNGP-FC 12.32±0.38 4.02±0.22
Protonet-FC 2.83±0.39 0.86±0.09
Protonet-FC SN 6.78±0.34 1.91±0.08

Table 12: Runtime analysis of linear variants of models

Model Train Iteration (ms) Eval Iteration (ms)

ProtoMahalanobis Diag 11.84±0.69 3.61±0.35
ProtoMahalanobis Rank-1 12.35±0.75 3.85±0.31
ProtoMahalanobis Rank-5 15.04±0.69 4.55±0.36
ProtoMahalanobis Rank-10 17.72±0.71 5.42±0.45
ProtoDDU 11.39±0.81 3.80±0.29
ProtoSNGP 11.43±0.65 3.68±0.30
Protonet 3.55±0.24 1.14±0.20
Protonet SN 8.03±0.40 2.42±0.18

Table 13: Runtime analysis of CNN variants of models

A.11 FURTHER EIGENVALUE EXPERIMENTS

In Table 14, we perform further experiments and analysis into the behavior of the low rank covariance
encoder outlined in Section 3, we analyze the significance of the eigenvalues of the precision matrix.
In this experiment, we first obtain the predicted precision matrix and perform an eigendecomposition
_A = QΛQ[−][1]_ _∈_ R[N] _[×][N]_ . We then construct a set of alternate precision matrices S = {A[′]}i[N]=1[, where]
each set element is a recomposition A[′]i [=][ Q][Λ]i[′] _[Q][−][1][, where][ Λ]i[′]_ [has one eigenvalue reset to 1. We then]
compute the final Accuracy, NLL, and ECE once for each matrix in S. If the predicted eigenvalues
are due to arbitrary error or noise, then we would expect to see that the test statistics would arbitrarily
improve for some precision matrices in S.

Instead, in Table 14 we see that the precision matrix which is predicted from the Set Transformer
gives the best results on the test set in all cases, showing that all of the predicted values are necessary
for the given solution. This experiment utilizes Omniglot 5-way/5-shot and the ProtoMahalanobis
Rank-1 variant.


-----

1.0

0.8

0.6

0.4



Test

|Method DKT-cos Ours (Diag) DKT-bncos Ours (Rank-1)|Method DKT-cos Ours (Diag) DKT-bncos Ours (Rank-1)|
|---|---|
|||
|||
|||


Shift Intensity: Omniglot-C (5-way/5-shot)


0.30

0.25

0.20

ECE0.15

0.10

0.05

0.00

Test 1 2 3 4 5

Shift Intensity: Omniglot-C (5-way/5-shot)

1.0

0.9

0.8

0.7

0.6

Accuracy

0.5

0.4

Test 1 2 3 4 5

Shift Intensity: Omniglot-C (5-way/1-shot)

0.5

0.4

ECE0.3

0.2

0.1

0.0

Test 1 2 3 4 5

Shift Intensity: Omniglot-C (5-way/1-shot)


Figure 22: Extra results comparing to Deep Kernel Transfer (Patacchiola et al., 2020) on the Omniglot
dataset. In our experiments, DKT showed a large variance in performance between tasks. In the
5-way/1-shot case, calibration on corrupted data comes at the expense of underconfidence on in
distribution data.


-----

**Σ[−][1]** Matrix Accuracy NLL ECE better%


predicted level 0 Σ[−][1] **99.55±0.04** **0.02±0.00** **1.21±0.16** **100%/100%/100%**
modified level 0 Σ[−][1] 97.98±0.23 0.09±0.01 3.31±0.60 0%/0%/0%

predicted level 1 Σ[−][1] **63.43±1.49** **0.97±0.05** **5.22±0.87** **100%/100%/100%**
modified level 1 Σ[−][1] 58.44±2.11 1.30±0.17 9.06±1.73 0%/0%/0%

predicted level 2 Σ[−][1] **56.31±1.59** **1.14±0.04** **6.92±1.43** **100%/100%/100%**
modified level 2 Σ[−][1] 51.17±2.12 1.54±0.37 11.92±2.04 0%/0%/0%

predicted level 3 Σ[−][1] **52.45±1.33** **1.21±0.04** **6.62±1.45** **100%/100%/100%**
modified level 3 Σ[−][1] 46.39±1.76 1.72±0.74 13.51±2.03 0%/0%/0%

predicted level 4 Σ[−][1] **45.07±1.04** **1.37±0.02** **8.34±1.26** **100%/100%/100%**
modified level 4 Σ[−][1] 39.70±1.36 1.97±1.13 15.85±2.11 0%/0%/0%

predicted level 5 Σ[−][1] **40.73±0.70** **1.46±0.02** **10.25±1.16** **100%/100%/100%**
modified level 5 Σ[−][1] 36.54±0.91 2.09±1.42 17.34±2.06 0%/0%/0%

Table 14: Analyzing the predicted precision matrix against a set of modified precision matrices with
perturbed eigenvalues. The predicted precision matrix performs better in every instance, showing that
the precision matrix is not arbitrary. This data comes from Omniglot 5-way/5-shot and utilizes the
ProtoMahalanobis Rank-1 variant


-----

