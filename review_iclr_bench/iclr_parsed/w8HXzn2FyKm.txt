# FINITE-TIME ERROR BOUNDS FOR DISTRIBUTED LIN## EAR STOCHASTIC APPROXIMATION

ABSTRACT

This paper considers a novel multi-agent linear stochastic approximation algorithm driven by Markovian noise and general consensus-type interaction, in which
each agent evolves according to its local stochastic approximation process which
depends on the information from its neighbors. The interconnection structure
among the agents is described by a time-varying directed graph. While the convergence of consensus-based stochastic approximation algorithms when the interconnection among the agents is described by doubly stochastic matrices (at least
in expectation) has been studied, less is known about the case when the interconnection matrix is simply stochastic. For any uniformly strongly connected graph
sequences whose associated interaction matrices are stochastic, the paper derives
finite-time bounds on the mean-square error, defined as the deviation of the output
of the algorithm from the unique equilibrium point of the associated ordinary differential equation. For the case of interconnection matrices being stochastic, the
equilibrium point can be any unspecified convex combination of the local equilibria of all the agents in the absence of communication. Both the cases with
constant and time-varying step-sizes are considered. In the case when the convex
combination is required to be a straight average and interaction between any pair
of neighboring agents may be uni-directional, so that doubly stochastic matrices
cannot be implemented in a distributed manner, the paper proposes a push-type
distributed stochastic approximation algorithm and provides its finite-time bounds
for the performance by leveraging the analysis for the consensus-type algorithm
with stochastic matrices.

1 INTRODUCTION

The use of reinforcement learning (RL) to obtain policies that describe solutions to a Markov decision process (MDP) in which an autonomous agent interacting with an unknown environment aims
to optimize its long term reward is now standard Sutton & Barto (1998). Multi-agent or distributed
reinforcement learning is useful when a team of agents interacts with an unknown environment or
system and aims to collaboratively accomplish tasks involving distributed decision-making. Distributed here implies that agents exchange information only with their neighbors according to a
certain communication graph. Recently, many distributed algorithms for multi-agent RL have been
proposed and analyzed (Zhang et al., 2019). The basic result in such works is of the type that
if the graph describing the communication among the agents is bi-directional (and hence can be
represented by a doubly stochastic matrix), then an algorithm that builds on traditional consensus
algorithms converges to a solution in terms of policies to be followed by the agents that optimize
the sum of the utility functions of all the agents; further, both finite and infinite time performance of
such algorithms can be characterized (Doan et al., 2019; Zhang et al., 2018b).

This paper aims to relax the assumption of requiring bi-directional communication among agents
in a distributed RL algorithm. This assumption is arguably restrictive and will be violated due to
reasons such as packet drops or delays, differing privacy constraints among the agents, heterogeneous capabilities among the agents in which some agents may be able to communicate more often
or with more power than others, adversarial attacks, or even sophisticated resilient consensus algorithms being used to construct the distributed RL algorithm. A uni-directional communication
graph can be represented through a (possibly time-varying) stochastic – which may not be doubly
stochastic – matrix being used in the algorithm. As we discuss in more detail below, relaxing the
assumption of a doubly stochastic matrix to simply a stochastic matrix in the multi-agent and distributed RL algorithms that have been proposed in the literature, however, complicates the proofs
of their convergence and finite time performance characterizations. The main result in this paper is
to provide a finite time bound on the mean-square error for a multi-agent linear stochastic approximation algorithm in which the agents interact over a time-varying directed graph characterized by
a stochastic matrix. This paper, thus, extends the applicability of distributed and multi-agent RL


-----

algorithms presented in the literature to situations such as those mentioned above where bidirectional communication at every time step cannot be guaranteed. As we shall see, this extension is
technically challenging and requires new proof techniques that may be of independent interest.

**Related Work** (See Appendix B for more related work) Many distributed reinforcement learning
algorithms have now been proposed in the literature. In this setting, each agent can receive information only from its neighbors, and no single agent can solve the problem alone or by ‘taking the lead’.
A backbone of almost all distributed RL algorithms proposed in the literature is the consensus-type
interaction among the agents, dating back at least to Tsitsiklis (1984). Many works have analyzed
asymptotic convergence of such RL algorithms using ODE methods (Zhang & Zavlanos, 2019;
Zhang et al., 2018b; Suttle et al., 2020; Zhang et al., 2018a). This can be viewed as an application
of ideas from distributed stochastic approximation (Kushner & Yin, 1987; Stankovi´c et al., 2010;
Huang, 2012; Stankovi´c & Stankovi´c, 2016; Bianchi et al., 2013; Stankovi´c et al., 2016). Finite-time
performance guarantees for distributed RL have also been provided in works, most notably in Doan
et al. (2019; 2021); Wang et al. (2020); Zhang et al. (2021); Sun et al. (2020); Zeng et al. (2020).

The assumption that is the central concern of this paper and is made in all the existing finite-time
analyses for distributed RL algorithms is that the consensus interaction is characterized by doubly
stochastic matrices (Doan et al., 2019; 2021; Wang et al., 2020; Zhang et al., 2021; Sun et al., 2020;
Zeng et al., 2020) at every time step, or at least in expectation, i.e., W **1 = 1 and 1[⊤]E(W** ) = 1[⊤]
(Bianchi et al., 2013). Intuitively, doubly stochastic matrices imply symmetry in the communication graph, which almost always requires bidirectional communication graphs. More formally, the
assumption of doubly stochastic matrices is restrictive since distributed construction of a doubly
stochastic matrix needs to either invoke algorithms such as the Metropolis algorithm (Xiao et al.,
2005) which requires bi-directional communication of each agent’s degree information; or to utilize
an additional distributed algorithm (Gharesifard & Cort´es, 2012) which significantly increases the
complexity of the whole algorithm design. Doubly stochastic matrices in expectation can be guaranteed via so-called broadcast gossip algorithms which still requires bi-directional communication
for convergence (Bianchi et al., 2013). In a realistic network, especially with mobile agents such
as autonomous vehicles, drones, or robots, uni-directional communication is inevitable due to various reasons such as asymmetric communication and privacy constraints, non-zero communication
failure probability between any two agents at any given time, and application of resilient consensus in the presence of adversary attacks (Vaidya et al., 2012; LeBlanc et al., 2013), all leading to
an interaction among the agents characterized by a stochastic matrix, which may further be timevarying. The problem of design of distributed RL algorithms with time-varying stochastic matrices
and characterizing either their asymptotic convergence or finite time analysis remains open.

As a step towards solving this problem, we propose a novel distributed stochastic approximation
algorithm and provide its convergence analyses when a time-dependent stochastic matrix is being
used due to uni-directional communication in a dynamic network. One of the first guarantees to be
lost as the assumption of doubly stochastic matrices is removed is that the algorithm converges to
a “policy” that maximizes the sum of reward functions of all the agents. Instead, the convergence
is to a set of policies that optimize a convex combination of the network-wise accumulative reward,
with the exact combination depending on the limit product of the infinite sequence of stochastic
matrices. Nonetheless, by defining the error as the deviation of the output of the algorithm from
the eventual equilibrium point, we derive finite-time bounds on the mean-square error. We consider
both the cases with constant and time-varying step sizes. In the important special case where the
goal is to optimize the average of the individual accumulative rewards of all the agents, we provide
a distributed stochastic approximation algorithm, which builds on the push-sum idea (Kempe et al.,
2003) that has been used to solve distributed averaging problem over strongly connected graphs, and
characterize its finite-time performance. Thus, this paper provides the first distributed algorithm that
can be applied (e.g., in Temporal difference (TD) learning, see Appendix D) to converge to the policy
maximizing the team objective of the sum of the individual utility functions over time-varying, unidirectional, communication graphs, and characterizes the finite-time bounds on the mean-square
error of the algorithm output from the equilibrium point under appropriate assumptions.

**Technical Innovation and Contributions** There are two main technical challenges in removing
the assumption of doubly stochastic matrices being used in the analysis of distributed stochastic
approximation algorithms. The first is in the direction of finite-time analysis. For distributed RL algorithms, finite-time performance analysis essentially boils down to two parts, namely bounding the
consensus error and bounding the “single-agent” mean-square error. For the case when consensus


-----

interaction matrices are all doubly stochastic, the consensus error bound can be derived by analyzing
the square of the 2-norm of the deviation of the current state of each agent from the average of the
states of the agents. With consensus in the presence of doubly stochastic matrices, the average of the
states of the agents remains invariant. Thus, it is possible to treat the average value as the state of a
fictitious agent to derive the mean-square consensus error bound with respect to the limiting point.
More formally, this process relies on two properties of a doubly stochastic matrix W, namely that
where(1) 1[⊤] σW2 =(W 1) denotes the second largest singular value of[⊤], and (2) if xt+1 = Wxt, then ∥xt+1 − ( W1[⊤] (which is strictly less than one ifxt+1)1∥2 ≤ _σ2(W_ )∥xt − (1[⊤]xt W)1 is∥2
irreducible). Even if the doubly stochastic matrix is time-varying (denoted by Wt), property (1) still
holds and property (2) can be generalized as in Nedi´c et al. (2018). Thus, the square of the 2-norm
stochastic matrices in expectation can be treated in the same way by looking at the expectation. This∥xt − (1[⊤]xt)1∥2[2] [is a quadratic Lyapunov function for the average consensus processes. Doubly]
is the core on which all the existing finite-time analyses of distributed RL algorithms are based.

However, if each consensus interaction matrix is stochastic, and not necessarily doubly stochastic,
the above two properties may not hold. In fact, it is well known that quadratic Lyapunov functions
for general consensus processes xt+1 = Stxt, with St being stochastic, do not exist (Olshevsky
& Tsitsiklis, 2008). This breaks down all the existing analyses and provides the first technical
challenge that we tackle in this paper. Specifically, we appeal to the idea of quadratic comparison
functions for general consensus processes. This was first proposed in Touri (2012) and makes use of
the concept of “absolute probability sequences”. We provide a general analysis methodology and results that subsume the existing finite-time analyses for single-timescale distributed linear stochastic
approximation and TD learning as special cases (see Appendix D).

The second technical challenge arises from the fact that with stochastic matrices, the distributed RL
algorithms may not converge to the policies that maximize the average of the utility functions of
the agents. To regain this property, we propose a new algorithm that utilizes a push-sum protocol
for consensus. However, finite-time analysis for such a push-based distributed algorithm is challenging. Almost all, if not all, the existing push-based distributed optimization works build on the
analysis in Nedi´c & Olshevsky (2014); however, that analysis assumes that a convex combination
of the entire history of the states of each agent (and not merely the current state of the agent) is
being calculated. This assumption no longer holds in our case. To obtain a direct finite-time error
bound without this assumption, we propose a new approach to analyze our push-based distributed
algorithm by leveraging our consensus-based analyses to establish direct finite-time error bounds for
stochastic approximation. Specifically, we tailor an “absolute probability sequence” for the pushbased stochastic approximation algorithm and exploit its properties. Such properties have never
been found in the existing literature and may be of independent interest for analyzing any push-sum
based distributed algorithm.

We now list the main contributions of our work. We propose a novel consensus-based distributed
linear stochastic approximation algorithm driven by Markovian noise in which each agent evolves
according to its local stochastic approximation process and the information from its neighbors. We
assume only a (possibly time-varying) stochastic matrix being used during the consensus phase,
which is a more practical assumption when only unidirectional communication is possible among
agents. We establish both convergence guarantees and finite-time bounds on the mean-square error,
defined as the deviation of the output of the algorithm from the unique equilibrium point of the
associated ordinary differential equation. The equilibrium point can be an “uncontrollable” convex
combination of the local equilibria of all the agents in the absence of communication. We consider
both the cases of constant and time-varying step-sizes. Our results subsume the existing results on
convergence and finite-time analysis of distributed RL algorithms that assume doubly stochastic matrices and bi-directional communication as special cases. In the case when the convex combination
is required to be a straight average and interaction between any pair of neighboring agents may be
uni-directional, we propose a push-type distributed stochastic approximation algorithm and establish its finite-time performance bound. It is worth emphasizing that it is straightforward to extend
our algorithm from the straight average point to any pre-specified convex combination. Since it is
well known that TD algorithms can be viewed as a special case of linear stochastic approximation
(Tsitsiklis & Roy, 1997), our distributed linear stochastic approximation algorithms and their finitetime bounds can be applied to TD algorithms in a straight-forward manner; (see distributed TD(λ)
in Appendix D).


-----

**Notation** We use Xt to represent that a variable X is time-dependent and t ∈{0, 1, 2, . . .} is
the discrete time index. The ith entry of a vector x will be denoted by x[i] and, also, by (x)[i] when
convenient. The ijth entry of a matrix A will be denoted by a[ij] and, also, by (A)[ij] when convenient.
We use 1n to denote the vectors in R[n] whose entries all equal to 1’s, and I to denote the identity
matrix, whose dimension is to be understood from the context. Given a set S with finitely many
elements, we use |S| to denote the cardinality of S. We use ⌈·⌉ to denote the ceiling function.

A vector is called a stochastic vector if its entries are nonnegative and sum to one. A square nonnegative matrix is called a row stochastic matrix, or simply stochastic matrix, if its row sums all
equal one. Similarly, a square nonnegative matrix is called a column stochastic matrix if its column
sums all equal one. A square nonnegative matrix is called a doubly stochastic matrix if its row sums
and column sums all equal one. The graph of an n × n matrix is a direct graph with n vertices
and a directed edge from vertex i to vertex j whenever the ji-th entry of the matrix is nonzero. A
directed graph is strongly connected if it has a directed path from any vertex to any other vertex.
For a strongly connected graph G, the distance from vertex i to another vertex j is the length of the
shortest directed path from i to j; the longest distance among all ordered pairs of distinct vertices i
and j in G is called the diameter of G. The union of two directed graphs, Gp and Gq, with the same
the union of the edge set ofvertex set, written Gp ∪ Gq, is meant the directed graph with the same vertex set and edge set being Gp and Gq. Since this union is a commutative and associative binary
operation, the definition extends unambiguously to any finite sequence of directed graphs.

2 DISTRIBUTED LINEAR STOCHASTIC APPROXIMATION

Consider a network consisting of N agents. For the purpose of presentation, we label the agents
from 1 through N . The agents are not aware of such a global labeling, but can differentiate between
their neighbors. The neighbor relations among the N agents are characterized by a time-dependent
directed graph Gt = (V, Et) whose vertices correspond to agents and whose directed edges (or arcs)
depict neighbor relations, where V = {1, . . ., N _} is the vertex set and Et = V × V is the edge set_
at time t. Specifically, agent j is an in-neighbor of agent i at time t if (j, i) _t, and similarly,_
_∈E_
agent k is an out-neighbor of agent i at time t if (i, k) _t. Each agent can send information to its_
_∈E_
out-neighbors and receive information from its in-neighbors. Thus, the directions of edges represent
the directions of information flow. For convenience, we assume that each agent is always an in- and
out-neighbor of itself, which implies that Gt has self-arcs at all vertices for all time t. We use Nt[i]
and _t[i][−]_ to denote the in- and out-neighbor set of agent i at time t, respectively, i.e.,
_N_

_Nt[i]_ [=][ {][j][ ∈V][ : (][j, i][)][ ∈E][t][}][,] _Nt[i][−]_ = {k ∈V : (i, k) ∈Et}.

It is clear that Nt[i] [and][ N][ i]t _[−]_ are nonempty as they both contain index i.

We propose the following distributed linear stochastic approximation over a time-varying neighbor
graph sequence {Gt}. Each agent i has control over a random vector θt[i] [which is updated by]

_θt[i]+1_ [=] _wt[ij][θ]t[j]_ [+][ α][t] _A(Xt)_ _wt[ij][θ]t[j]_ [+][ b][i][(][X][t][)] _,_ _i ∈V,_ _t ∈{0, 1, 2, . . .},_ (1)

_j_ _t[i]_  _j_ _t[i]_ 

X∈N X∈N

where wt[ij] [are consensus weights,][ α][t] [is the step-size at time][ t][,][ A][(][X][t][)][ is a random matrix and][ b][i][(][X][t][)]
is a random vector, both generated based on the Markov chain _Xt_ with state spaces . It is worth
_{_ _}_ _X_
noting that the update of each agent only uses its in-neighbors’ information and thus is distributed.

**Remark 1 The work of Kushner & Yin (1987) considers a different consensus-based networked**
_linear stochastic approximation as follows:_

_θt[i]+1_ [=] _wt[ij][θ]t[j]_ [+][ α][t] _A(Xt)θt[i]_ [+][ b][i][(][X][t][)] _,_ _i ∈V,_ _t ∈{0, 1, 2, . . .},_ (2)

_j_ _t[i]_

X∈N   

_whose state form is Θt+1 = WtΘt+αtΘtA(Xt)[⊤]+αtB(Xt), and mainly focuses on asymptotically_
_weakly convergence for the fixed step-size case (i.e., αt = α for all t). Under the similar set of_
_conditions, with its condition (C3.4’) being a stochastic analogy for Assumption 6, Theorem 3.1 in_
_Kushner & Yin (1987) shows that equation 2 has a limit which can be verified to be the same as θ[∗],_
_the limit of equation 1. How to apply the finite-time analysis tools in this paper to equation 2 has_
_so far eluded us. The two updates equation 1 and equation 2 are analogous to the “combine-then-_
_adapt” and “adapt-then-combine” diffusion strategies in distributed optimization (Chen & Sayed,_
_2012)._ □


-----

We impose the following assumption on the weights wt[ij] [which has been widely adopted in consen-]
sus literature (Jadbabaie et al., 2003; Olfati-Saber et al., 2007; Nedi´c & Liu, 2017).

**Assumption 1 There exists a constant β > 0 such that for all i, j** _and t, wt[ij]_
_j ∈Nt[i][. For all][ i][ ∈V][ and][ t][,][ P]j∈Nt[i]_ _[w]t[ij]_ [= 1][.] _∈V_ _[≥]_ _[β][ whenever]_

Let Wt be the N × N matrix whose ijth entry equals wt[ij] [if][ j][ ∈N][ i]t [and zero otherwise. From]
Assumption 1, each Wt is a stochastic matrix that is compliant with the neighbor graph Gt. Since
each agent i is always assumed to be an in-neighbor of itself, all diagonal entries of Wt are positive.
Thus, if Gt is strongly connected, Wt is irreducible and aperiodic. To proceed, define


(θt[1][)][⊤]
.
.
.
(θt[N] [)][⊤]


(b[1](Xt))[⊤]
.
.
.
(b[N] (Xt))[⊤]


Θt = .. _,_ _B(Xt) =_ .. _._

(θt[N] [)][⊤] (b[N] (Xt))[⊤]

   
   

Then, the N linear stochastic recursions in equation 1 can be combined and written as


Θt =


Θt+1 = WtΘt + αtWtΘtA(Xt)[⊤] + αtB(Xt), _t ∈{0, 1, 2, . . .}._ (3)

The goal of this section is to characterize the finite-time performance of equation 1, or equivalently equation 3, with the following standard assumptions, which were adopted e.g. in Srikant &
Ying (2019); Doan et al. (2019).

**Assumption 2 There exists a matrix A and vectors b[i], i ∈V, such that**

lim lim _i_ _._
_t_ _t_ _∈V_
_→∞_ **[E][[][A][(][X][t][)] =][ A,]** _→∞_ **[E][[][b][i][(][X][t][)] =][ b][i][,]**


_∥DefineA∥2 ≤ bmaxAmax = max and ∥bi[i]∈V∥2 ≤ supbmaxx∈X, ∥ i ∈Vb[i](x).∥2 < ∞_ _and Amax = supx∈X ∥A(x)∥2 < ∞. Then,_

**Assumption 3 Given a positive constant α, we use τ** (α) to denote the mixing time of the Markov
_chain_ _Xt_ _for which_
_{_ _}_

_∥E[A(Xt) −_ _A|X0 = X]∥2 ≤_ _α,_ _∀X, ∀t ≥_ _τ_ (α),



**E[b[i](Xt)** _b[i]_ _X0 = X]_ 2 _α,_ _X,_ _t_ _τ_ (α), _i_ _._

 _∥_ _−_ _|_ _∥_ _≤_ _∀_ _∀_ _≥_ _∀_ _∈V_

_The Markov chain_ _Xt_ _mixes at a geometric rate, i.e., there exists a constant C such that τ_ (α)
_{_ _}_ _≤_
_−C log α._

**Assumption 4 All eigenvalues of A have strictly negative real parts, i.e., A is a Hurwitz matrix.**
_Then, there exists a symmetric positive definite matrix P_ _, such that A[⊤]P + PA =_ _I. Let γmax_
_−_
_and γmin be the maximum and minimum eigenvalues of P_ _, respectively._

**Assumption 5 The step-size sequence {αt} is positive, non-increasing, and satisfies** _t=0_ _[α][t][ =][ ∞]_
_and_ _t=0_ _[α]t[2]_ _[<][ ∞][.]_

[P][∞]

To state our first main result, we need the following concepts.

[P][∞]

**Definition 1 A graph sequence {Gt} is uniformly strongly connected if there exists a positive integer**
_L such that for any t_ 0, the union graph _k=t_ Gk is strongly connected. If such an integer exists,
_≥_ _∪[t][+][L][−][1]_
_we sometimes say that {Gt} is uniformly strongly connected by sub-sequences of length L._

**Remark 2 Two popular joint connectivity definitions in consensus literature are “B-connected”**
_(Nedi´c et al., 2009) and “repeatedly jointly strongly connected” (Cao et al., 2008)._ _A graph_
_sequence {Gt} is B-connected if there exists a positive integer B such that the union graph_
_t=kB_ Gt is strongly connected for each integer k 0. Although the uniformly strongly con_∪[(][k][+1)][B][−][1]_ _≥_
_nectedness looks more restrictive compared with B-connectedness at first glance, they are in fact_
_equivalent. To see this, first it is easy to see that if {Gt} is uniformly strongly connected, {Gt} must_


-----

_be B-connected; now supposing_ Gt _is B-connected, for any fix t, the union graph_ _k=t_ Gk
_{_ _}_ _∪[t][+2][B][−][1]_
_must be strongly connected, and thus {Gt} is uniformly strongly connected by sub-sequences of_
_length 2B. Thus, the two definitions are equivalent. It is also not hard to show that the uniformly_
_strongly connectedness is equivalent to “repeatedly jointly strongly connectedness” provided the_
_graphs under consideration all have self-arcs at all vertices, as “repeatedly jointly strongly con-_
_nectedness” is defined upon “graph composition”._ □

**Definition 2 Let {Wt} be a sequence of stochastic matrices. A sequence of stochastic vectors {πt}**
_is an absolute probability sequence for {Wt} if πt[⊤]_ [=][ π]t[⊤]+1[W][t] _[for all][ t][ ≥]_ [0][.]

This definition was first introduced by Kolmogorov (Kolmogoroff, 1936). It was shown by Blackwell (Blackwell, 1945) that every sequence of stochastic matrices has an absolute probability sequence. In general, a sequence of stochastic matrices may have more than one absolute probability
sequence; when the sequence of stochastic matrices is “ergodic”, it has a unique absolute probability
sequence (Nedi´c & Liu, 2017). It is easy to see that when Wt is a fixed irreducible stochastic matrix
_W_, πt is simply the normalized left eigenvector of W for eigenvalue one. More can be said.

**Lemma 1 Suppose that Assumption 1 holds. If {Gt} is uniformly strongly connected, then there**
_exists a unique absolute probability sequence_ _πt_ _for the matrix sequence_ _Wt_ _and a constant_
_{_ _}_ _{_ _}_
_πmin_ (0, 1) such that πt[i] _[for all][ i][ and][ t][.]_
_∈_ _[≥]_ _[π][min]_

Let ⟨θ⟩t = _i=1_ _[π]t[i][θ]t[i][, which is a column vector and convex combination of all][ θ]t[i][. It is easy to]_
see that ⟨θ⟩t = (πt[⊤][Θ][t][)][⊤] [= Θ]t[⊤][π][t][. From Definition 2 and equation 3, we have][ π]t[⊤]+1[Θ][t][+1] [=]
_πt[⊤]+1[W][t][Θ][t]_ [+][P][ α][N][t][π]t[⊤]+1[W][t][Θ][t][A][(][X][t][)][⊤] [+][ α][t][π]t[⊤]+1[B][(][X][t][) =][ π]t[⊤][Θ][t] [+][ α][t][π]t[⊤][Θ][t][A][(][X][t][)][⊤] [+][ α][t][π]t[⊤]+1[B][(][X][t][)][,]
which implies that

_⟨θ⟩t+1 = ⟨θ⟩t + αtA(Xt)⟨θ⟩t + αtB(Xt)[⊤]πt+1._ (4)

Asymptotic performance of equation 1 with any uniformly strongly connected neighbor graph sequence is characterized by the following two theorems.

**Theorem 1 Suppose that Assumptions 1, 2 and 5 hold. Let {θt[i][}][,][ i][ ∈V][, be generated by equation 1.]**
_If {Gt} is uniformly strongly connected, then limt→∞_ _∥θt[i]_ _[−⟨][θ][⟩][t][∥][2]_ [= 0][ for all][ i][ ∈V][.]

Theorem 1 only shows that all the sequences {θt[i][}][,][ i][ ∈V][, generated by equation 1 will finally]
reach a consensus, but not necessarily convergent or bounded. To guarantee the convergence of the
sequences, we further need the following assumption, whose validity is discussed in Remark 3.

**Assumption 6 The absolute probability sequence {πt} for the stochastic matrix sequence {Wt}**
_has a limit, i.e., there exists a stochastic vector π_ _such that limt_ _πt = π_ _._
_∞_ _→∞_ _∞_

**Theorem 2 Suppose that Assumptions 1–6 hold. Let {θt[i][}][,][ i][ ∈V][, be generated by equation 1 and]**
_θ[∗]_ _be the unique equilibrium point of the ODE_


_π[i]_ (5)
_∞[b][i][,]_
_i=1_

X


_θ˙ = Aθ + b,_ _b =_


_where A and b[i]_ _are defined in Assumption 2 and π_ _is defined in Assumption 6. If_ Gt _is uniformly_
_∞_ _{_ _}_
_strongly connected, then all θt[i]_ _[will converge to][ θ][∗]_ _[both with probability 1 and in mean square.]_

**Remark 3 Though Assumption 6 may look restrictive at first glance, simple simulations show that**
_the sequences {θt[i][}][,][ i][ ∈V][, do not converge if the assumption does not hold. It is worth empha-]_
_sizing that the existence of π∞_ _does not imply the existence of limt→∞_ _Wt, though the converse_
_is true. Indeed, the assumption subsumes various cases including (a) all Wt are doubly stochastic_
_matrices, and (b) all Wt share the same left eigenvector for eigenvalue 1, which may arise from_
_the scenario when the number of in-neighbors of each agent does not change over time (Olshevsky_
_& Tsitsiklis, 2013). An important implication of Assumption 6 is when the consensus interaction_
_among the agents, characterized by_ _Wt_ _, is replaced by resilient consensus algorithms such as_
_{_ _}_
_Vaidya et al. (2012); LeBlanc et al. (2013) in order to attenuate the effect of unknown malicious_


-----

_agents, the resulting dynamics of non-malicious agents, in general, will not converge, because the_
_resulting interaction stochastic matrices among the non-malicious agents depend on the state val-_
_ues transmitted by the malicious agents, which can be arbitrary, and thus the resulting stochastic_
_matrix sequence, in general, does not have a convergent absolute probability sequence; of course,_
_in this case, the trajectories of all the non-malicious agents will still reach a consensus as long as_
_the step-size is diminishing, as implied by Theorem 1. Further discussion on Assumption 6 can be_
_found in Appendix C._ □

We now study the finite-time performance of the proposed distributed linear stochastic approximation equation 1 for both fixed and time-varying step-size cases. Its finite-time performance is
characterized by the following theorem.

Let ηt = _πt_ _π_ 2 for all t 0. From Assumption 6, ηt converges to zero as t .
_∥_ _−_ _∞∥_ _≥_ _→∞_

**Theorem 3 Let the sequences {θt[i][}][,][ i][ ∈V][, be generated by equation 1. Suppose that Assump-]**
_tions 1–4, 6 hold and {Gt} is uniformly strongly connected by sub-sequences of length L. Let qt_
_and mt be the unique integer quotient and remainder of t divided by L, respectively. Let δt be the_
_diameter of ∪[t]k[+]=[L]t_ _[−][1]Gk, δmax = maxt≥0 δt, and_


_ϵ =_ 1 + [2][b][max]

_Amax_ _−_ _[π]2[min]δmax[β][2][L]_




(1 + αAmax)[2][L] (1 + αAmax)[L], (6)
_−_ [2]A[b]max[max]


_where 0 < α < min_ _K1,_ _Amaxlog 2τ_ (α) _[,]_ _K20γ.max1_
_{_ _[}][.]_

**1) Fixed step-size: Let αt = α for all t ≥** 0. For all t ≥ _T1,_


_N_ _t−T1_
_≤_ 2ϵ[q][t] Xi=1 _πm[i]_ _t_ **[E]** hθm[i] _t_ _[−⟨][θ][⟩][m]t_ 2i + C11 − _γ[0]max[.][9][α]_  + C2

_t−T1_ [2] _k_

+ _[γ]γ[max]min_ 2αζ4 _ηt+1−k_ 1 − _γ[0]max[.][9][α]_ _._ (7)

_k=0_  

X


_N_

_i=1_ _πt[i][E]_ _θt[i]_ _[−]_ _[θ][∗]_

X h


**2) Time-varying step-size: Let αt =** _tα+10_ _[with][ α][0][ ≥]_ _[γ]0[max].9_ _[. For all][ t][ ≥]_ _[LT][2][,]_


_N_ _N_

_πt[i][E]_ _θt[i]_ 2 2ϵ[q][t][−][T][2] _πLT[i]_ 2+mt **[E]** _θLT[i]_ 2+mt 2[+][m]t 2
_i=1_ _[−]_ _[θ][∗]_ _≤_ _i=1_ _[−⟨][θ][⟩][LT]_

X h i X h _t_ i

+ C3 _α[2]0ϵ_ _qt2−1_ + α⌈ _qt2−1_ _⌉L_ + [1]t _C4 log[2][ ][ t]α0_ + C5 _ηk +[2]_ _C6_ _._ (8)
    _k=XLT2_ 

_Here T1, T2, K1, K2, C1 −_ _C6 are finite constants whose definitions are given in Appendix A.1._

Since πt[i] [is uniformly bounded below by][ π][min]
above bound holds for each individualthe following remark. **E[∥θt[i]** _[−]_ _[θ][∗][∈][∥]2[2][(0][]][. To better understand the theorem, we provide][,][ 1)][ from Lemma 1, it is easy to see that the]_

**Remark 4 In Appendix E.2.1, we show that both ϵ and (1 −** _γ[0]max[.][9][α]_ [)][ lie in the interval][ (0][,][ 1)][. It is]

_easy to show that ϵ is monotonically increasing for δmax and L, monotonically decreasing for β and_
_πmin. Also, limt→∞_ _kt−=0T1_ _[η][t][+1][−][k][(1][ −]_ _γ[0]max[.][9][α]_ [)][k][ ≤] [lim][t][→∞] _γ0max.9α_ [[][η]⌈ _[t][−]2[T][1]_ _⌉_ [+][ η][1][(1][ −] _γ[0]max[.][9][α]_ [)] _t−2T1_ ] =

0. Therefore, the summands in the finite-time bound equation 7 for the fixed step-size case are
_exponentially decaying except forP_ _the constant C2, which implies that lim supt_ _Ni=1_ _[π]t[i][E][[][∥][θ]t[i]_
_θ[∗]_ 2[]][ ≤] _[C][2][, providing a constant limiting bound. From Appendix A,][ C][2]_ _[is monotonically increasing]→∞_ _[−]_
_∥[2]_ P
_for γmax, δmax, bmax and L, and monotonically decreasing for γmin, πmin and β. In Appendix E.2.2,_
_we show that limt→∞_ [1]t _tk=1_ _[η][k][ = 0][, which implies that the finite-time bound equation 8 for the]_

_definingtime-varying step-size case converges to zero asϵ and the feasible set of α. Actually, we can replace αP, with the latter becoming 0.1 with any constant t →∞ 0 < α <. We next comment on c ∈ min(0, 1)K, which will affect the value of1,_ _Amaxlog 2τ 0(α.)1[,] in the inequalityK2γcmax_
_{_ _[}][.][ Thus,]_


-----

_the smaller the value of c is, the smaller is the feasible set of α, though the feasible set is always_
_nonempty. For convenience, we simply pick c = 0.1 in this paper; that is why we also have 0.9 in_

_purpose of getting a cleaner expression of the finite-time bound. Forequation 7. Lastly, we comment on α0 in the time-varying step-size case. We set α0 <_ _[γ]0[max].9_ _[, our approach still] α0 ≥_ _[γ]0[max].9_ _[for the]_

_works, but will yield a more complicated expression. The same is true for Theorem 5._ □

**Technical Challenge and Proof Sketch** As described in the introduction, the key challenge of
analyzing the finite-time performance of the distributed stochastic approximation equation 1 lies
in the condition that the consensus-based interaction matrix is time-varying and stochastic (not
necessarily doubly stochastic). To tackle this, we appeal to the absolute probability sequence
_πt of the time-varying interaction matrix sequence and introduce the quadratic Lyapunov com-_
parison function _i=1_ _[π]t[i][E][[][∥][θ]t[i]_ _[−]_ _[θ][∗][∥]2[2][]][. Then, using the inequality][ P][N]i=1_ _[π]t[i][E][[][∥][θ]t[i]_ _[−]_ _[θ][∗][∥]2[2][]][ ≤]_
2 _i=1_ _[π]t[i][E][[][∥][θ]t[i]_ 2[] + 2][E][[][∥⟨][θ][⟩][t] 2[]][, the next step is to find the finite-time bounds of]
_N_ _[−⟨][θ][⟩][t][∥][2]_ _[−]_ _[θ][∗][∥][2]_
agent” mean-square error. Our main analysis contribution here is to bound the former term for both[P]i=1[N] _[π]t[i][E][[][∥][θ]t[i]_ _[−⟨][P][θ][⟩][N][t][∥]2[2][]][ and][ E][[][∥⟨][θ][⟩][t]_ _[−]_ _[θ][∗][∥]2[2][]][, respectively. The latter term is essentially the “single-]_
fixed and time-varying step-size cases.P

3 PUSH-SA

The preceding section shows that the limiting state of consensus-based distributed stochastic approximation depends on π, which leads to a convex combination of the local equilibria of all the agents
_∞_
in the absence of communication, but the convex combination is in general “uncontrollable”. Note
that this convex combination will correspond to a convex combination of the network-wise accumulative rewards in applications such as distributed TD learning. In an important case when the convex
combination is desired to be the straight average, the existing literature e.g. Doan et al. (2019; 2021)
relies on doubly stochastic matrices whose corresponding π∞ = (1/N )1N . As mentioned in the
introduction, doubly stochastic matrices implicitly require bi-directional communication between
any pair of neighboring agents; see e.g. gossiping (Boyd et al., 2006) and the Metropolis algorithm (Xiao et al., 2005). A popular method to achieve the straight average target while allowing
uni-directional communication between neighboring agents is to appeal to the idea so-called “pushsum” (Kempe et al., 2003), which was tailored for solving the distributed averaging problem over
directed graphs and has been applied to distributed optimization (Nedi´c & Olshevsky, 2014). In this
section, we will propose a push-based distributed stochastic approximation algorithm tailored for
uni-directional communication and establish its finite-time error bound.

Each agent i has control over three variables, namely yt[i][,][ ˜]θt[i] [and][ θ]t[i][, in which][ y]t[i] [is scalar-valued]
with initial value 1, _θ[˜]t[i]_ [can be arbitrarily initialized, and][ θ]0[i] [= ˜]θ0[i] [. At each time][ t][ ≥] [0][, each agent][ i]
sends its weighted current values ˆwt[ji][y]t[i] [and][ ˆ]wt[ji][(˜]θt[i] [+][ α][t][A][(][X][t][)][θ][t] [+][ α][t][b][i][(][X][t][))][ to each of its current]
out-neighbors j _t[i][−], and updates its variables as follows:_
_∈N_

_yt[i]+1_ [=] _wˆt[ij][y]t[j][,]_ _y0[i]_ [= 1][,]

 _jX∈Nt[i]_
θ˜t[i]+1 [=] _jX∈Nt[i]_ _wˆt[ij]_ hθ˜t[j] [+][ α][t] A(Xt)θt[j] [+][ b][j][(][X][t][)]i _,_ (9)

where ˆwt[ij] = 1/ _t[j][−]. It is worth noting that the algorithm is distributed yet requires that eachθt[i]+1_ [=] _yθ˜tt[i][i]+1+1_ _,_ _θ0[i]_ [= ˜]θ0[i] _[,]_
_|N_ _|_
agent be aware of the number of its out-neighbors.

Asymptotic performance of equation 9 with any uniformly strongly connected neighbor graph sequence is characterized by the following theorem.

**Theorem 4 Suppose that Assumptions 2–5 hold. Let {θt[i][}][,][ i][ ∈V][, be generated by equation 9 and]**
_θ[∗]_ _be the unique equilibrium point of the ODE_


_θ˙ = Aθ + [1]_


_b[i],_ (10)

_i=1_

X


-----

_where A and b[i]_ _are defined in Assumption 2. If {Gt} is uniformly strongly connected, then θt[i]_ _[will]_
_converge to θ[∗]_ _in mean square for all i ∈V._

In this section, we define ⟨θ[˜]⟩t = _N1_ _Ni=1_ _θ[˜]t[i]_ [and][ ⟨][θ][⟩][t] [=] _N1_ _Ni=1_ _[θ]t[i][. To help understand these]_

definitions, let _W[ˆ]_ _t be the N × N matrix whoseP_ _ij-th entry equalsP_ ˆwt[ij] [if][ j][ ∈N][ i]t [, otherwise equals]
zero. It is easy to see that each _W[ˆ]_ _t is a column stochastic matrix whose diagonal entries are all_
positive. Then, πt = _N1_ **[1][N][ for all][ t][ ≥]** [0][ can be regarded as an absolute probability sequence of]

_{W[ˆ]_ _t}. Thus, the above two definitions are intuitively consistent with ⟨θ⟩t in the previous section._

Finite-time performance of equation 9 with any uniformly strongly connected neighbor graph sequence is characterized by the following theorem.

Letas t µt = ∥, so doesA(Xt)( µ⟨θt⟩.t −⟨θ[˜]⟩t)∥2. In Appendix E.3, we show that ∥⟨θ⟩t −⟨θ[˜]⟩t∥2 converges to zero
_→∞_

**Theorem 5 Suppose that Assumptions 2–4 hold and** Gt _is uniformly strongly connected by sub-_
_{_ _}_ _α0_
_sequences of lengthThen, there exists a nonnegative L. Let {θt[i][}][,] ¯[ i]ϵ_ _[ ∈V](1[, be generated by equation 9 with]N_ _[NL]1_ [ )] _L1 such that for all t_ _T[ α],_ _[t]_ [=] _t+1_ _[and][ α][0][ ≥]_ _[γ]0[max].9_ _[.]_

_≤_ _−_ _≥_ [¯]

_N_

_t_

_i=1_ **E** _θt[i]+1_ _[−]_ _[θ][∗]_ 2 _≤_ _C7ϵ¯[t]_ + C8 _α0ϵ¯2 + α⌈_ 2t _[⌉]_ + C9αt

X h i   _t_

[2]

+ [1] _C10 log[2][ ][ t]_ + C11 _µk + C12_ _,_ (11)

_t_ _α0_

  _kX= T[¯]_ 

_where_ _T[¯] and C7 −_ _C12 are finite constants whose definitions are given in Appendix A.2._

In Appendix E.3, we show that limt→∞ [1]t _tk=1_ _[µ][k][ = 0][, which implies that the finite-time bound]_

equation 11 converges to zero asthe fixed step-size case, as our current analysis approach cannot t →∞. It is worth mentioning that the theorem does not considerP be directly applied for this case.

**Proof Sketch and Technical Challenge** Using the inequality _i=1_ **[E][[][∥][θ]t[i]+1** _[−]_ _[θ][∗][∥]2[2][]]_ _≤_
2 _Ni=1_ **[E][[][∥][θ]t[i]+1** _[−⟨]θ[˜]⟩t∥2[2][] + 2][N]_ **[E][[][∥⟨]θ[˜]⟩t −** _θ[∗]∥2[2][]][, our goal is to derive the finite-time bounds of]_
_i=1_ **[E][[][∥][θ]t[i]+1** _[−⟨]θ[˜]⟩t∥2[2][]][ and][ E][[][∥⟨]θ[˜]⟩t −_ _θ[∗]∥2[2][]][, respectively. Although this looks similar to the proof][P][N]_
of Theorem 3, the derivation is quite different. First, the iteration of[P][N] _θ_ _t is a single-agent SA plus_
P _⟨[˜]⟩_
a disturbance term _θ_ _t_ _θ_ _t, so we cannot directly apply the existing single-agent SA finite-_
_⟨_ _⟩_ _−⟨[˜]⟩_
time analyses to bound E[∥⟨θ[˜]⟩t − _θ[∗]∥2[2][]][; instead, we have to show that][ ⟨][θ][⟩][t]_ _[−⟨]θ[˜]⟩t will diminish_
and quantify the diminishing “speed”. Second, both the proof of showing diminishing _θ_ _t_ _θ_ _t_
_⟨_ _⟩_ _−⟨[˜]⟩_
_{and derivation of boundingθt[i][}][ generated from the Push-SA equation 9 is bounded almost surely. To tackle this, we introduce a]i=1_ **[E][[][∥][θ]t[i]+1** _[−⟨]θ[˜]⟩t∥2[2][]][ involve a key challenge: to prove the sequence]_
novel way to constructing an absolute probability sequence for the Push-SA as follows. From equa
[P][N]

tion 9, θt[i]+1 [=][ P]j[N]=1 _w[˜]t[ij][[][θ]t[j]_ [+][ α][t][A][(][X][t][)][ θ]ytt[j][j] [+][ α][t] _b[j]_ (yXt[j] _t)_ ], where ˜wt[ij] = ( ˆwt[ij][y]t[j][)][/][(][P]k[N]=1 _w[ˆ]t[ik][y]t[k][)][.]_

We show that each matrix _W[˜]_ _t = [ ˜wt[ij][]][ is stochastic, and there exists a unique absolute probability]_
sequence {π˜t} for the matrix sequence {W[˜] _t} such that ˜πt[i]_ _[≥]_ _π[˜]min for all i ∈V and t ≥_ 0, with
the constant ˜πmin (0, 1). Most importantly, we show two critical properties of _W[˜]_ _t_ and _π˜t_,
namely limt→∞(Π ∈[t]s=0W[˜] _s) =_ _N1_ **[1][N]** **[1]N[⊤]** [and][ ˜]πyt[i]t[i] [=] _N1_ [for all][ i, j][ ∈V][ and][ t][ ≥] [0][, which have never] { _}_ _{_ _}_

been reported in the literature though push-sum based algorithms have been extensively studied.

4 CONCLUDING REMARKS

In this paper, we have established both asymptotic and non-asymptotic analyses for a consensusbased distributed linear stochastic approximation algorithm over uniformly strongly connected
graphs, and proposed a push-based variant for coping with uni-directional communication. Both
algorithms and their analyses can be directly applied to TD learning. One limitation of our finitetime bounds is that they involve quite a few constants which are well defined and characterized
but whose values are not easy to compute. Future directions include leveraging the analyses for
resilience in the presence of malicious agents and extending the tools to more complicated RL.


-----

REFERENCES

P. Bianchi, G. Fort, and W. Hachem. Performance of a distributed stochastic approximation algorithm. IEEE Transactions on Information Theory, 59(11):7405–7418, 2013.

D. Blackwell. Finite non-homogeneous chains. Annals of Mathematics, 46(4):594–599, 1945.

S. Boyd, A. Ghosh, B. Prabhakar, and D. Shah. Randomized gossip algorithms. IEEE Transactions
_on Information Theory, 52(6):2508–2530, 2006._

M. Cao, A. S. Morse, and B. D. O. Anderson. Reaching a consensus in a dynamically changing
environment: a graphical approach. SIAM Journal on Control and Optimization, 47(2):575–600,
2008.

J. Chen and A. H. Sayed. Diffusion adaptation strategies for distributed optimization and learning
over networks. IEEE Transactions on Signal Processing, 60(8):4289–4305, 2012.

T.T. Doan, S.T. Maguluri, and J. Romberg. Finite-time analysis of distributed TD(0) with linear
function approximation on multi-agent reinforcement learning. In 36th International Conference
_on Machine Learning, pp. 1626–1635, 2019._

T.T. Doan, S. T. Maguluri, and J. Romberg. Finite-time performance of distributed temporaldifference learning with linear function approximation. SIAM Journal on Mathematics of Data
_Science, 3(1):298–320, 2021._

B. Gharesifard and J. Cort´es. Distributed strategies for generating weight-balanced and doubly
stochastic digraphs. European Journal of Control, 18(6):539–557, 2012.

M. Huang. Stochastic approximation for consensus: a new approach via ergodic backward products.
_IEEE Transactions on Automatic Control, 57(12):2994–3008, 2012._

A. Jadbabaie, J. Lin, and A. S. Morse. Coordination of groups of mobile autonomous agents using
nearest neighbor rules. IEEE Transactions on Automatic Control, 48(6):988–1001, 2003.

D. Kempe, A. Dobra, and J. Gehrke. Gossip-based computation of aggregate information. In 44th
_IEEE Symposium on Foundations of Computer Science, pp. 482–491, 2003._

A. Kolmogoroff. Zur theorie der markoffschen ketten. Mathematische Annalen, 112(1):155–160,
1936.

H.J. Kushner and G. Yin. Asymptotic properties of distributed and communicating stochastic approximation algorithms. SIAM Journal on Control and Optimization, 25(5):1266–1290, 1987.

H. J. LeBlanc, H. Zhang, X. Koutsoukos, and S. Sundaram. Resilient asymptotic consensus in robust
networks. IEEE Journal on Selected Areas in Communications, 31(4):766–781, 2013.

A. Nedi´c and J. Liu. On convergence rate of weighted-averaging dynamics for consensus problems.
_IEEE Transactions on Automatic Control, 62(2):766–781, 2017._

A. Nedi´c and A. Olshevsky. Distributed optimization over time-varying directed graphs. IEEE
_Transactions on Automatic Control, 60(3):601–615, 2014._

A. Nedi´c, A. Olshevsky, A. Ozdaglar, and J. N. Tsitsiklis. On distributed averaging algorithms and
quantization effects. IEEE Transactions on automatic control, 54(11):2506–2517, 2009.

A. Nedi´c, A. Olshevsky, and M. G. Rabbat. Network topology and communication-computation
tradeoffs in decentralized optimization. Proceedings of the IEEE, 106(5):953–976, 2018.

R. Olfati-Saber, J. A. Fax, and R. M. Murray. Consensus and cooperation in networked multi-agent
systems. Proc. IEEE, 95(1):215–233, 2007.

A. Olshevsky and J. N. Tsitsiklis. On the nonexistence of quadratic lyapunov functions for consensus
algorithms. IEEE Transactions on Automatic Control, 53(11):2642–2645, 2008.

A. Olshevsky and J. N. Tsitsiklis. Degree fluctuations and the convergence time of consensus algorithms. IEEE Transactions on Automatic Control, 58(10):2626–2631, 2013.


-----

R. Srikant and L. Ying. Finite-time error bounds for linear stochastic approximation and TD learning. In 32nd Conference on Learning Theory, volume 99, pp. 2803–2830. Proceedings of Machine
Learning Research, 25–28 Jun 2019.

M. S. Stankovi´c, N. Ili´c, and S. S. Stankovi´c. Distributed stochastic approximation: weak convergence and network design. IEEE Transactions on Automatic Control, 61(12):4069–4074, 2016.

M.S. Stankovi´c and S.S. Stankovi´c. Multi-agent temporal-difference learning with linear function
approximation: Weak convergence under time-varying network topologies. In American Control
_Conference, pp. 167–172, 2016._

S. S. Stankovi´c, M. S. Stankovi´c, and D. Stipanovi´c. Decentralized parameter estimation by consensus based stochastic approximation. IEEE Transactions on Automatic Control, 56(3):531–543,
2010.

J. Sun, G. Wang, G. B. Giannakis, Q. Yang, and Z. Yang. Finite-time analysis of decentralized
temporal-difference learning with linear function approximation. In International Conference on
_Artificial Intelligence and Statistics, pp. 4485–4495. PMLR, 2020._

W. Suttle, Z. Yang, K. Zhang, Z. Wang, T. Bas¸ar, and J. Liu. A multi-agent off-policy actor-critic
algorithm for distributed reinforcement learning. In 21st IFAC World Congress, 2020.

R.S. Sutton and A.G. Barto. Reinforcement Learning: An Introduction. Cambridge: MIT press,
1998.

B. Touri. Product of Random Stochastic Matrices and Distributed Averaging. Springer Science &
Business Media, 2012.

J. N. Tsitsiklis. Problems in Decentralized Decision Making and Computation. PhD thesis, Department of Electrical Engineering and Computer Science, MIT, 1984.

J.N. Tsitsiklis and B. Van Roy. An analysis of temporal-difference learning with function approximation. IEEE Transactions on Automatic Control, 42(5):674–690, 1997.

N. H. Vaidya, L. Tseng, and G. Liang. Iterative approximate byzantine consensus in arbitrary directed graphs. In Proceedings of the 2012 ACM symposium on Principles of distributed comput_ing, pp. 365–374, 2012._

G. Wang, S. Lu, G. Giannakis, G. Tesauro, and J. Sun. Decentralized TD tracking with linear
function approximation and its finite-time analysis. In 34th Conference on Neural Information
_Processing Systems, 2020._

L. Xiao, S. Boyd, and S. Lall. A scheme for robust distributed sensor fusion based on average
consensus. In Proceedings of the 4th International Conference on Information Processing in
_Sensor Networks, pp. 63–70, 2005._

S. Zeng, T.T. Doan, and J. Romberg. Finite-time analysis of decentralized stochastic approximation
with applications in multi-agent and multi-task learning. arXiv preprint arXiv:2010.15088, 2020.

K. Zhang, Z. Yang, and T. Bas¸ar. Networked multi-agent reinforcement learning in continuous
spaces. In 57th IEEE Conference on Decision and Control, pp. 2771–2776, 2018a.

K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Bas¸ar. Fully decentralized multi-agent reinforcement
learning with networked agents. In 35th International Conference on Machine Learning, pp.
5872–5881, 2018b.

K. Zhang, Z. Yang, and T. Bas¸ar. Multi-agent reinforcement learning: A selective overview of
theories and algorithms. arXiv preprint arXiv:1911.10635, 2019.

K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Bas¸ar. Finite-sample analysis for decentralized batch
multi-agent reinforcement learning with networked agents. IEEE Transactions on Automatic Con_trol, 2021._

Y. Zhang and M.M. Zavlanos. Distributed off-policy actor-critic reinforcement learning with policy
consensus. In 58th IEEE Conference on Decision and Control, pp. 4674–4679, 2019.


-----

