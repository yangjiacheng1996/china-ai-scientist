# WHO IS THE STRONGEST ENEMY? TOWARDS OPTI## MAL AND EFFICIENT EVASION ATTACKS IN DEEP RL

**Yanchao Sun[1]** **Ruijie Zheng[2]** **Yongyuan Liang[3]** **Furong Huang[4]**

1,2,4 University of Maryland, College Park 3 Sun Yat-sen University
1,2,4{ycs,rzheng12,furongh}@umd.edu 3liangyy58@mail2.sysu.edu.cn

ABSTRACT

Evaluating the worst-case performance of a reinforcement learning (RL) agent under the strongest/optimal adversarial perturbations on state observations (within
some constraints) is crucial for understanding the robustness of RL agents. However, finding the optimal adversary is challenging, in terms of both whether we
can find the optimal attack and how efficiently we can find it. Existing works on
adversarial RL either use heuristics-based methods that may not find the strongest
adversary, or directly train an RL-based adversary by treating the agent as a part of
the environment, which can find the optimal adversary but may become intractable
in a large state space. This paper introduces a novel attacking method to find the
optimal attacks through collaboration between a designed function named “actor”
and an RL-based learner named “director”. The actor crafts state perturbations for
a given policy perturbation direction, and the director learns to propose the best
policy perturbation directions. Our proposed algorithm, PA-AD, is theoretically
optimal and significantly more efficient than prior RL-based works in environments with large state spaces. Empirical results show that our proposed PA-AD
universally outperforms state-of-the-art attacking methods in various Atari and
MuJoCo environments. By applying PA-AD to adversarial training, we achieve
state-of-the-art empirical robustness in multiple tasks under strong adversaries.

1 INTRODUCTION

Deep Reinforcement Learning (DRL) has achieved incredible success in many applications. However, recent works (Huang et al., 2017; Pattanaik et al., 2018) reveal that a well-trained RL agent
may be vulnerable to test-time evasion attacks, making it risky to deploy RL models in high-stakes
applications. As in most related works, we consider a state adversary which adds imperceptible
noise to the observations of an agent such that its cumulative reward is reduced during test time.

In order to understand the vulnerability of an RL agent and to improve its certified robustness, it
is important to evaluate the worst-case performance of the agent under any adversarial attacks with
certain constraints. In other words, it is crucial to find the strongest/optimal adversary that can
minimize the cumulative reward gained by the agent with fixed constraints, as motivated in a recent
paper by Zhang et al. (2021). Therefore, we focus on the following question:

**Given an arbitrary attack radius (budget) ϵ for each step of the deployment, what is the worst-**
**case performance of an agent under the strongest adversary?**

Finding the strongest adversary in RL is challenging. Many myopic
existing attacks (Huang et al., 2017; Pattanaik et al., 2018) are
based on heuristics, crafting adversarial states at every step in- strongest
dependently, although steps are interrelated in contrast to im- Victim
age classification tasks. These heuristic methods can often effectively reduce the agent’s reward, but are not guaranteed to
achieve the strongest attack under a given budget. This type of myopic
attack is “myopic” since it does not plan for the future. Fig- **Figure 1: An example that a myopic**

|myopic|Col2|Col3|Col4|
|---|---|---|---|
||str|ongest||
||Victim|||
|myopic||||

ure 1 shows an intuitive example, where myopic adversaries adversary is not the strongest.
only prevent the agent from selecting the best action in the current step, but the strongest adversary
can strategically “lead” the agent to a trap, which is the worst event for the agent.


-----

Achieving computational efficiency arises as another challenge in practice, even if the strongest
adversary can be found in theory. A recent work (Zhang et al., 2020a) points out that learning the
optimal state adversary is equivalent to learning an optimal policy in a new Markov Decision Process
(MDP). A follow-up work (Zhang et al., 2021) shows that the learned adversary significantly outperforms prior adversaries in MuJoCo games. However, the state space and the action space of the new
MDP are both as large as the state space in the original environment, which can be high-dimensional
in practice. For example, video games and autonomous driving systems use images as observations.
In these tasks, learning the state adversary directly becomes computationally intractable.

To overcome the above two challenges, we propose a novel attack method called Policy Adver**sarial Actor Director (PA-AD), where we design a “director” and an “actor” that collaboratively**
finds the optimal state perturbations. In PA-AD, a director learns an MDP named Policy Adversary
_MDP (PAMDP), and an actor is embedded in the dynamics of PAMDP. At each step, the director_
proposes a perturbing direction in the policy space, and the actor crafts a perturbation in the state
space to lead the victim policy towards the proposed direction. Through a trail-and-error process, the
director can find the optimal way to cooperate with the actor and attack the victim policy. Theoretical analysis shows that the optimal policy in PAMDP induces an optimal state adversary. The size
of PAMDP is generally smaller than the adversarial MDP defined by Zhang et al. (2021) and thus is
easier to be learned efficiently using off-the-shelf RL algorithms. With our proposed director-actor
_collaborative mechanism, PA-AD outperforms state-of-the-art attacking methods on various types_
of environments, and improves the robustness of many DRL agents by adversarial training.

**Summary of Contributions**
**(1) We establish a theoretical understanding of the optimality of evasion attacks from the perspective**
of policy perturbations, allowing a more efficient implementation of optimal attacks.
**(2) We introduce a Policy Adversary MDP (PAMDP) model, whose optimal policy induces the op-**
timal state adversary under any attacking budget ϵ.
**(3) We propose a novel attack method, PA-AD, which efficiently searches for the optimal adversary**
in the PAMDP. PA-AD is a general method that works on stochastic and deterministic victim policies, vectorized and pixel state spaces, as well as discrete and continuous action spaces.
**(4) Empirical study shows that PA-AD universally outperforms previous attacking methods in vari-**
ous environments, including Atari games and MuJoCo tasks. PA-AD achieves impressive attacking
performance in many environments using very small attack budgets,
**(5) Combining our strong attack PA-AD with adversarial training, we significantly improve the ro-**
bustness of RL agents, and achieve the state-of-the-art robustness in many tasks.

2 PRELIMINARIES AND NOTATIONS

**The Victim RL Agent** In RL, an agent interacts with an environment modeled by a Markov
Decision Process (MDP) denoted as a tuple M = ⟨S, A, P, R, γ⟩, where S is a state space with
cardinality |S|, A is an action space with cardinality |A|, P : S × A → ∆(S) is the transition
function [1], R : S × A → R is the reward function, and γ ∈ (0, 1) is the discount factor. In this
paper, we consider a setting where the state space is much larger than the action space, which arises
in a wide variety of environments. For notation simplicity, our theoretical analysis focuses on a finite
MDP, but our algorithm applies to continuous state spaces and continuous action spaces, as verified
in experiments. The agent takes actions according to its policy, π : S → ∆(A). We suppose the
victim uses a fixed policy π with a function approximator (e.g. a neural network) during test time.
We denote the space of all policies as Π, which is a Cartesian product of |S| simplices. The value
of a policy π ∈ Π for state s ∈S is defined as V _[π](s) = Eπ,P [[P][∞]t=0_ _[γ][t][R][(][s][t][, a][t][)][|][s][0][ =][ s][]][.]_

**Evasion Attacker** Evasion attacks are test-time attacks that aim to reduce the expected total
reward gained by the agent/victim. As in most literature (Huang et al., 2017; Pattanaik et al., 2018;
Zhang et al., 2020a), we assume the attacker knows the victim policy π (white-box attack). However,
the attacker does not know the environment dynamics, nor does it have the ability to change the
environment directly. The attacker can observe the interactions between the victim agent and the
environment, including states, actions and rewards. We focus on a typical state adversary (Huang
et al., 2017; Zhang et al., 2020a), which perturbs the state observations returned by the environment
before the agent observes them. Note that the underlying states in the environment are not changed.

1∆(X) denotes the the space of probability distributions over X.


-----

Formally, we model a state adversary by a function h which perturbs state s ∈S into ˜s := h(s), so
that the input to the agent’s policy is ˜s instead of s. In practice, the adversarial perturbation is usually
under certain constraints. In this paper, we consider the common ℓp threat model (Goodfellow et al.,
2015): ˜s should be in Bϵ(s), where Bϵ(s) denotes an ℓp norm ball centered at s with radius ϵ ≥ 0, a
constant called the budget of the adversary for every step. With the budget constraint, we define the
_admissible state adversary and the admissible adversary set as below._

**Definition 1 (Set of Admissible State Adversaries Hϵ). A state adversary h is said to be admissible**
_if_ _s_ _, we have h(s)_ _ϵ(s). The set of all admissible state adversaries is denoted by Hϵ._
_∀_ _∈S_ _∈B_

Then the goal of the attacker is to find an adversary h[∗] in Hϵ that maximally reduces the cumulative
reward of the agent. In this work, we propose a novel method to learn the optimal state adversary
through the identification of an optimal policy perturbation defined and motivated in the next section.

3 UNDERSTANDING OPTIMAL ADVERSARY VIA POLICY PERTURBATIONS

In this section, we first motivate our idea of interpreting evasion attacks as perturbations of policies,
then discuss how to efficiently find the optimal state adversary via the optimal policy perturbation.

**Evasion Attacks Are Perturbations of Policies** Although
existing literature usually considers state-attacks and actionattacks separately, we point out that evasion attacks, either applied to states or actions, are essentially equivalent to perturbing the agent’s policy π into another policy πh in the policy

**Figure 2: Equivalence between eva-**

space Π. For instance, as shown in Figure 2, if the adversary

sion attacks and policy perturbations.

_h alters state s into state ˜s, the victim selects an action ˜a based_
on π( _s˜). This is equivalent to directly perturbing π(_ _s) to πh(_ _s) := π(_ _s˜). (See Appendix A for_

_·|_ _·|_ _·|_ _·|_
more detailed analysis including action adversaries.)

In this paper, we aim to find the optimal state adversary through the identification of the “optimal
policy perturbation”, which has the following merits. (1) πh(·|s) usually lies in a lower dimensional
space than h(s) for an arbitrary state s ∈S. For example, in Atari games, the action space is discrete
and small (e.g. |A| = 18), while a state is a high-dimensional image. Then the state perturbation
_h(s) is an image, while πh(·|s) is a vector of size |A|. (2) It is easier to characterize the optimality of_
a policy perturbation than a state perturbation. How a state perturbation changes the value of a victim
policy depends on both the victim policy network and the environment dynamics. In contrast, how
a policy perturbation changes the victim value only depends on the environment. Our Theorem 4 in
Section 3 and Theorem 12 in Appendix B both provide insights about how V _[π]_ changes as π changes
continuously. (3) Policy perturbation captures the essence of evasion attacks, and unifies state and
action attacks. Although this paper focuses on state-space adversaries, the learned “optimal policy
perturbation” can also be used to conduct action-space attacks against the same victim.

**Characterizing the Optimal Policy Adversary** As depicted in Figure 3, the policy perturbation
serves as a bridge connecting the perturbations in the state space and the value space. Our goal is
to find the optimal state adversary by identifying the optimal “policy adversary”. We first define
an Admissible Adversarial Policy Set (Adv-policy-set) Bϵ[H] [(][π][)][ ⊂] [Π][ as the set of policies perturbed]
from π by all admissible state adversaries h _Hϵ. In other words, when a state adversary perturbs_
_∈_
states within an ℓp norm ball Bϵ(·), the victim policy is perturbed within Bϵ[H] [(][π][)][.]

**Definition 2 (Admissible Adversarial Policy Set (Adv-policy-set) Bϵ[H]** [(][π][)][)][.][ For an MDP][ M][, a]
_fixed victim policy π, we define the admissible adversarial policy set (Adv-policy-set) w.r.t. π, de-_
_noted by Bϵ[H]_ [(][π][)][, as the set of policies that are perturbed from][ π][ by all admissible adversaries, i.e.,]

_ϵ_ [(][π][) :=][ {][π][h] _[s.t][ ∀][s, π][h][(][·|][s][) =][ π][(][·|][h][(][s][))][}][.]_ (1)
_B[H]_ _[∈]_ [Π :][ ∃][h][ ∈] _[H][ϵ]_

**Remarks** (1) Bϵ[H] [(][π][)][ is a subset of the policy space][ Π][ and it surrounds the victim][ π][, as shown]
in Figure 3(middle). In the same MDP, Bϵ[H] [(][π][)][ varies for different victim][ π][ or different attack]
budget ϵ. (2) In Appendix B, we characterize the topological properties of Bϵ[H] [(][π][)][. We show that]
for a continuous function π (e.g., neural network), Bϵ[H] [(][π][)][ is connected and compact, and the value]
functions generated by all policies in the Adv-policy-set Bϵ[H] [(][π][)][ form a polytope (Figure][ 3][(right)),]
following the polytope theorem by Dadashi et al. (2019).


-----

**End-to-end** An RL problem in a large MDP
**SA-RL:**

𝑉["]

𝑠

**Victim 𝝅** 𝜋 **Environment** 𝑉["][!] 𝑉

ℎ(𝑠)

(Policy network) 𝜋!

𝑉[!]["∗]

𝑩𝝐(𝒔) 𝑩𝑯𝝐 (𝝅) 𝑉

## 𝑆 Π

**PA-AD** An optimization problem An RL problem in a small MDP
**(ours):**


The Actor The Director


**Figure 3: A state adversary h perturbs s into h(s) ∈Bϵ(s) in the state space; hence, the victim’s policy π is**
perturbed intobecomes V _[π][h] πinstead ofh within the Adv-policy-set V_ _[π]. A prior work SA-RL ( Bϵ[H]_ [(][π][)][; as a result, the expected total reward the victim can gain]Zhang et al., 2021) directly uses an RL agent to learn
the best state adversary h[∗], which works for MDPs with small state spaces, but suffers from high complexity
in larger MDPs. In contrast, we find the optimal state adversary h[∗] efficiently through identifying the optimal
policy adversary πh[∗] . Our proposed attack method called PA-AD contains an RL-based “director” which learns
to propose policy perturbation πh in the policy space, and a non-RL “actor”, which targets at the proposed πh
and computes adversarial states in the state space. Through this collaboration, the director can learn the optimal
policy adversary πh[∗] using RL methods, such that the actor executes h[∗] as justified in Theorem 7.

Given that the Adv-policy-set Bϵ[H] [(][π][)][ contains all the possible policies the victim may execute under]
admissible state perturbations, we can characterize the optimality of a state adversary through the
lens of policy perturbations. Recall that the attacker’s goal is to find a state adversary h[∗] _Hϵ_
_∈_
that minimizes the victim’s expected total reward. From the perspective of policy perturbation, the
attacker’s goal is to perturb the victim’s policy to another policy πh∗ _∈Bϵ[H]_ [(][π][)][ with the lowest value.]
Therefore, we can define the optimal state adversary and the optimal policy adversary as below.

**Definition 3 (Optimal State Adversary h[∗]** **and Optimal Policy Adversary πh∗** ). For an MDP
_M, a fixed policy π, and an admissible adversary set Hϵ with attacking budget ϵ,_
_(1) an optimal state adversary h[∗]_ _satisfies h[∗]_ _∈_ argminh∈Hϵ _V_ _[π][h]_ (s), ∀s ∈S, which leads to
_(2) an optimal policy adversary πh∗_ _satisfies πh∗_ _∈_ argminπh∈BϵH [(][π][)][V][ π][h] [(][s][)][,][ ∀][s][ ∈S][.]
_Recall that πh is the perturbed policy caused by adversary h, i.e., πh(·|s) = π(·|h(s)), ∀s ∈S._

Definition 3 implies an equivalent relationship between the optimal state adversary and the optimal
policy adversary: an optimal state adversary leads to an optimal policy adversary, and any state
adversary that leads to an optimal policy adversary is optimal. Theorem 19 in Appendix D.1 shows
that there always exists an optimal policy adversary for a fixed victim π, and learning the optimal
policy adversary is an RL problem. (A similar result have been shown by Zhang et al. (2020a) for
the optimal state adversary, while we focus on the policy perturbation.)

Due to the equivalence, if one finds an optimal policy adversary πh∗, then the optimal state adversary
can be found by executing targeted attacks with target policy πh∗ . However, directly finding the
optimal policy adversary in the Adv-policy-set Bϵ[H] [(][π][)][ is challenging since][ B]ϵ[H] [(][π][)][ is generated by]
all admissible state adversaries in Hϵ and is hard to compute. To address this challenge, we first get
insights from theoretical characterizations of the Adv-policy-set Bϵ[H] [(][π][)][. Theorem][ 4][ below shows]
that the “outermost boundary” of Bϵ[H] [(][π][)][ always contains an optimal policy adversary. Intuitively, a]
policy π[′] is in the outermost boundary of Bϵ[H] [(][π][)][ if and only if no policy in][ B]ϵ[H] [(][π][)][ is farer away from]
_π than π[′]_ in the direction π[′] _−_ _π. Therefore, if an adversary can perturb a policy along a direction,_
it should push the policy as far away as possible in this direction under the budget constraints.
Then, the adversary is guaranteed to find an optimal policy adversary after trying all the perturbing
directions. In contrast, such a guarantee does not exist for state adversaries, justifying the benefits
of considering policy adversaries. Our proposed algorithm in Section 4 applies this idea to find the
optimal attack: an RL-based director searches for the optimal perturbing direction, and an actor is
_responsible for pushing the policy to the outermost boundary of Bϵ[H]_ [(][π][)][ with a given direction.]

**Theorem 4. For an MDP M, a fixed policy π, and an admissible adversary set Hϵ, define the**
**_outermost boundary of the admissible adversarial policy set Bϵ[H]_** [(][π][)][ w.r.t][ π][ as]

_∂πBϵ[H]_ [(][π][) :=][ {][π][′][ ∈B]ϵ[H] [(][π][) :][ ∀][s][ ∈S][, θ >][ 0][,][ ∄]π[ˆ] ∈Bϵ[H] [(][π][)][ s.t.][ ˆ]π(·|s) = π[′](·|s) + θ(π[′](·|s) − _π(·|s))}._ (2)
_Then there exists a policy ˜π ∈_ _∂πBϵ[H]_ [(][π][)][, such that][ ˜]π is the optimal policy adversary w.r.t. π.

Theorem 4 is proven in Appendix B.3, and we visualize the outermost boundary in Appendix B.5.


-----

4 PA-AD: OPTIMAL AND EFFICIENT EVASION ATTACK

In this section, we first formally define the optimality of an attack algorithm and discuss some existing attack methods. Then, based on the theoretical insights in Section 3, we introduce our algorithm,
_Policy Adversarial Actor Director (PA-AD) that has an optimal formulation and is efficient to use._

Although many attack methods for RL agents have been proposed (Huang et al., 2017; Pattanaik
et al., 2018; Zhang et al., 2020a), it is not yet well-understood how to characterize the strength and
the optimality of an attack method. Therefore, we propose to formulate the optimality of an attack
algorithm, which answers the question “whether the attack objective finds the strongest adversary”.

**Definition 5 (Optimal Formulation of Attacking Algorithm). An attacking algorithm Algo is said to**
_have an optimal formulation iff for any MDP M, policy π and admissible adversary set Hϵ under_
_attacking budget ϵ, the set of optimal solutions to its objective, Hϵ[Algo], is a subset of the optimal_
_adversaries against π, i.e., Hϵ[Algo]_ _Hϵ[∗]_ [:=][ {][h][∗][|][h][∗] _[∈]_ [argmin]h _Hϵ_ _[V][ π][h]_ [(][s][)][,][ ∀][s][ ∈S}][.]
_⊆_ _∈_

Many heuristic-based attacks, although are empirically effective and efficient, do not meet the requirements of optimal formulation. In Appendix D.3, we categorize existing heuristic attack methods into four types, and theoretically prove that there exist scenarios where these heuristic methods
may not find the strongest adversary. A recent paper (Zhang et al., 2021) proposes to learn the optimal state adversary using RL methods, which we will refer to as SA-RL in our paper for simplicity.
SA-RL can be viewed as an “end-to-end” RL attacker, as it directly learns the optimal state adversary
such that the value of the victim policy is minimized. The formulation of SA-RL satisfies Definition 5 and thus is optimal. However, SA-RL learns an MDP whose state space and action space
are both the same as the original state space. If the original state space is high-dimensional (e.g.
images), learning a good policy in the adversary’s MDP may become computationally intractable,
as empirically shown in Section 6.

Can we address the optimal attacking problem in an efficient manner? SA-RL treats the victim and
the environment together as a black box and directly learns a state adversary. But if the victim policy
is known to the attacker (e.g. in adversarial training), we can exploit the victim model and simplify
the attacking problem while maintaining the optimality. Therefore, we propose a novel algorithm,
_Policy Adversarial Actor Director (PA-AD), that has optimal formulation and is generally more ef-_
ficient than SA-RL. PA-AD decouples the whole attacking process into two simpler components:
policy perturbation and state perturbation, solved by a “director” and an “actor” through collaboration. The director learns the optimal policy perturbing direction with RL methods, while the actor
crafts adversarial states at every step such that the victim policy is perturbed towards the given direction. Compared to the black-box SA-RL, PA-AD is a white-box attack, but works for a broader
range of environments more efficiently. Note that PA-AD can be used to conduct black-box attack
based on the transferability of adversarial attacks (Huang et al., 2017), although it is out of the scope
of this paper. Appendix F.2 provides a comprehensive comparison between PA-AD and SA-RL in
terms of complexity, optimality, assumptions and applicable scenarios.

**A Heuristic Attacker:** **Our Method: Policy Adversarial Actor Director:**

**Efficient But Non-optimal** **Optimal And Efficient**

**State** State Environment Reward Environment

**Adversary** **Director** State Action

Reward

Environment

State Action

Policy Perturbing

Direction

**Actor**

Victim Policy


The actor's task: similar to a (targeted) evasion attack in supervised learning.
_Can be solved by optimization methods (FGSM, PGD, etc)._

Victim Policy The director's task: minimize the total reward gained from the environment.

_Can be solved by RL methods (PPO, DQN, etc)._

**A Heuristic Attacker:**

**Efficient But Non-optimal**

Action

**State** State Environment

**Adversary**

Victim Policy

**An End-to-end RL Attacker (SA-RL):**

**Optimal But Inefficient**

**State** Reward Environment Action

**Adversary** State

Victim Policy


**Figure 4: An overview of PA-AD compared with a heuristic attacker and an end-to-end RL attacker. Heuristic**
attacks are efficient, but may not find the optimal adversary as they do not learn from the environment dynamics.
An end-to-end RL attacker directly learns a policy to generate state perturbations, but is inefficient in large-statespace environments. In contrast, our PA-AD solves the attack problem with a combination of an RL-based
director and a non-RL actor, so that PA-AD achieves both optimality and efficiency.


-----

Formally, for a given victim policy π, our proposed PA-AD algorithm solves a Policy Adversary
_MDP (PAMDP) defined in Definition 6. An actor denoted by g is embedded in the dynamics of the_
PAMDP, and a director searches for an optimal policy ν[∗] in the PAMDP.

_fixedDefinition 6 stochastic (Policy Adversary MDP (PAMDP) victim policy π, an attack budget ϵM ≥). Given an MDP0, we define a Policy Adversarial MDP M = ⟨S, A, P, R, γ⟩, a_
_M = ⟨S,_ _A,_ _P,_ _R, γ⟩, where the action space is_ _A[c] := {d ∈[−1, 1][|A|],_ _i=1_ _[d][i][ = 0][}][, and][ ∀][s, s][′][ ∈]_
_S, ∀a ∈_ _A[b],_
c _P_ (s[b][′] _s,[b]a) =[b]_ _a, s))P_ (s[′] _s, a),_ [b]R(s, _a) =_ [P][|A|] _a, s))R(s, a),_
_|_ _a∈A_ _[π][(][a][|][g][(][b]_ _|_ _−_ _a∈A_ _[π][(][a][|][g][(][b]_

_whereb_ _g is the actor function defined asX_ X

b b b b

_T_
_g(a, s) = argmaxs˜∈Bϵ(s)[∥][π][(˜]s) −_ _π(s)∥_ _subject to_ _π(˜s) −_ _π(s)_ _a = ∥π(˜s) −_ _π(s)∥∥a∥._ (G)

_If the victim policy isb_ **_deterministic, i.e., πD := argmax _** _aπ(a|s), (subscript_ b _D stands for determin-b_
_istic), the action space of PAMDP is_ _AD :=_ _A, and the actor function gD is_

_gD(a, s) = argmaxs˜_ _ϵ(s)_ _π(a_ _s˜)_ maxa _,a=a[π][(][a][|]s[˜])_ _._ (GD)
_∈B_ _|_ _−_ _∈A_ _̸_

[b]

_Detailed definition of the deterministic-victim version of PAMDP is in Appendix _ b  _C.1._

b b

A key to PA-AD is the director-actor collaboration mechanism. The input to director policy ν is the
current state s in the original environment, while its output _a is a signal to the actor denoting “which_
the policy space. That is,direction to perturb the victim policy into”.a _A, there exists a constantA is designed to contain all “perturbing directions” in θ b0_ 0 such that _θ_ _θ0, π(_ _s) + θ_ _aa_
_∀_ _∈_ [b] _≥_ _∀_ _≤_ _·|_ _∥_ _∥_

belongs to the simplex ∆(A). The actor g takes in the state[b] _s and director’s direction_ _a and then_

[b]

b
computes a state perturbation within the attack budget. Therefore, the director and the actor togetherb
induce a state adversary: h(s) := g(ν(s), s), _s_ . The definition of PAMDP is slightly different
_∀_ _∈S_ b
for a stochastic victim policy and a deterministic victim policy, as described below.
_For a stochastic victim π, the director’s action_ _a ∈_ _A[b] is designed to be a unit vector lying in the_
policy simplex, denoting the perturbing direction in the policy space. The actor, once receiving the
perturbing direction _a, will “push” the policy as far as possible by perturbing s to g(a, s)_ _ϵ(s),_
b _∈B_
as characterized by the optimization problem (G). In this way, the policy perturbation resulted by
the director and the actor is always in the outermost boundary of b _Bϵ[H]_ [(][π][)][ w.r.t. the victim]b _[ π][, where]_
the optimal policy perturbation can be found according to Theorem 4.
_For a deterministic victim πD, the director’s action_ _a ∈_ _A[b]D can be viewed as a target action in the_
original action space, and the actor conducts targeted attacks to let the victim execute _a, by forcing_
the logit corresponding to the target action to be larger than the logits of other actions.
b

In both the stochastic-victim and deterministic-victim case, PA-AD has an optimal formulation as b
stated in Theorem 7 (proven in Appendix D.2).
**Theorem 7 (Optimality of PA-AD). For any MDP M, any fixed victim policy π, and any attack**
_budget ϵ ≥_ 0, an optimal policy ν[∗] _in_ _M induces an optimal state adversary against π in M. That_
_is, the formulation of PA-AD is optimal, i.e., H_ _[PA-AD]_ _⊆_ _Hϵ[∗][.]_

[c]

**Algorithm 1: Policy Adversarial Actor Director (PA-AD)**


**1 Input: Initialization of director’s policy ν; victim policy π; budget ϵ; start state s0**

**2 for t = 0, 1, 2, ... do**

|Col1|Director samples a policy perturbing directiona ν( s ) bt ∼ ·| t Actor perturbs s to s˜ = g D( ba t, s t) if Victim is deterministic, otherwise to s˜ = g( ba t, s t) t t t Victim takes action a π( s˜), proceeds to s, receives r t ∼ ·| t t+1 t Director saves (s,a, r, s ) to its buffer t bt − t t+1 Director updates its policy ν using any RL algorithm|
|---|---|



**Efficiency of PA-AD** As commonly known, the sample complexity and computational cost of
learning an MDP usually grow with the cardinalities of its state space and action space. Both SARL and PA-AD have state space S, the state space of the original MDP. But the action space of
SA-RL is also S, while our PA-AD has action space R[|A|] for stochastic victim policies, or A for
deterministic victim policies. In most DRL applications, the state space (e.g., images) is much larger
than the action space, then PA-AD is generally more efficient than SA-RL as it learns a smaller MDP.


-----

The attacking procedure is illustrated in Algorithm 1. At step t, the director observes a state st, and
proposes a policy perturbation _at, then the actor searches for a state perturbation to meet the policy_
perturbation. Afterwards, the victim acts with the perturbed state ˜st, then the director updates its
policy based on the opposite value of the victim’s reward. Note that the actor solves a constrained
b
optimization problem, (GD) or (G). Problem (GD) is similar to a targeted attack in supervised
learning, while the stochastic version (G) can be approximately solved with a Lagrangian relaxation.
In Appendix C.2, we provide our implementation details for solving the actor’s optimization, which
empirically achieves state-of-the-art attack performance as verified in Section 6.

**Extending to Continuous Action Space** Our PA-AD can be extended to environments with continuous action spaces, where the actor minimizes the distance between the policy action and the
target action, i.e., argmins′∈Bϵ(s)∥π(s[′]) − _a∥. More details and formal definitions of the variant of_
PA-AD in continuous action space are provided in Appendix C.3. In Section 6, we show experimental results in MuJoCo tasks, which have continuous action spaces.
b

5 RELATED WORK

**Heuristic-based Evasion Attacks on States** There are many works considering evasion attacks
on the state observations in RL. Huang et al. (2017) first propose to use FGSM (Goodfellow et al.,
2015) to craft adversarial states such that the probability that the agent selects the “best” action is
minimized. The same objective is also used in a recent work by Korkmaz (2020), which adopts a
Nesterov momentum-based optimization method to further improve the attack performance. Pattanaik et al. (2018) propose to lead the agent to select the “worst” action based on the victim’s Q
function and use gradient descent to craft state perturbations. Zhang et al. (2020a) define the concept of a state-adversarial MDP (SAMDP) and propose two attack methods: Robust SARSA and
Maximal Action Difference. The above heuristic-based methods are shown to be effective in many
environments, although might not find the optimal adversaries, as proven in Appendix D.3.

**RL-based Evasion Attacks on States** As discussed in Section 4, SA-RL (Zhang et al., 2021)
uses an end-to-end RL formulation to learn the optimal state adversary, which achieves state-ofthe-art attacking performance in MuJoCo tasks. For a pixel state space, an end-to-end RL attacker
may not work as shown by our experiment in Atari games (Section 6). Russo & Proutiere (2021)
propose to use feature extraction to convert the pixel state space to a small state space and then learn
an end-to-end RL attacker. But such feature extractions require expert knowledge and can be hard
to obtain in many real-world applications. In contrast, our PA-AD works for both pixel and vector
state spaces and does not require expert knowledge.

**Other Works Related to Adversarial RL** There are many other papers studying adversarial RL
from different perspectives, including limited-steps attacking (Lin et al., 2017; Kos & Song, 2017),
multi-agent scenarios (Gleave et al., 2020), limited access to data (Inkawhich et al., 2020), and
etc. Adversarial action attacks (Xiao et al., 2019; Tan et al., 2020; Tessler et al., 2019; Lee et al.,
2021) are developed separately from state attacks; although we mainly consider state adversaries,
our PA-AD can be extended to action attacks as formulated in Appendix A. Poisoning (Behzadan
& Munir, 2017; Huang & Zhu, 2019; Sun et al., 2021; Zhang et al., 2020b; Rakhsha et al., 2020) is
another type of adversarial attacks that manipulates the training data, different from evasion attacks
that deprave a well-trained policy. Training a robust agent is the focus of many recent works (Pinto
et al., 2017; Fischer et al., 2019; L¨utjens et al., 2020; Oikarinen et al., 2020; Zhang et al., 2020a;

2021). Although our main goal is to find a strong attacker, we also show by experiments that our
proposed attack method significantly improves the robustness of RL agents by adversarial training.

6 EXPERIMENTS

In this section, we show that PA-AD produces stronger evasion attacks than state-of-the-art attack
algorithms on various OpenAI Gym environments, including Atari and MuJoCo tasks. Also, our
experiment justifies that PA-AD can evaluate and improve the robustness of RL agents.

**Baselines and Performance Metric** We compare our proposed attack algorithm with existing
evasion attack methods, including MinBest (Huang et al., 2017) which minimizes the probability that
the agent chooses the “best” action, MinBest +Momentum (Korkmaz, 2020) which uses Nesterov
momentum to improve the performance of MinBest, MinQ (Pattanaik et al., 2018) which leads


-----

**Natural** **MinBest +** **PA-AD**
**Environment** **_ϵ_** **Random** **MinBest** **MinQ** **MaxDiff** **SA-RL**
**Reward** **Momentum** **(ours)**

**Boxing** 96 ± 4 0.001 95 ± 4 53 ± 16 52 ± 18 88 ± 7 95 ± 5 94 ± 6 **19 ± 11**

**Pong** 21 ± 0 0.0002 21 ± 0 _−10 ± 4_ _−14 ± 2_ 14 ± 3 15 ± 4 20 ± 1 **_−21 ± 0_**

**RoadRunner 46278 ± 4447 0.0005 44725 ± 6614 17012 ± 6243 15823 ± 5252 5765 ± 12331 36074 ± 6544 43615 ± 7183** **0 ± 0**

**Freeway** 34 ± 1 0.0003 34 ± 1 12 ± 1 12 ± 1 15 ± 2 22 ± 3 34 ± 1 **9 ± 1**

**Seaquest** 10650 ± 2716 0.0005 8177 ± 2962 3820 ± 1947 2337 ± 862 6468 ± 2493 5718 ± 1884 8152 ± 3113 2304 ± 838

**Alien** 1623 ± 252 0.00075 1650 ± 381 819 ± 486 775 ± 648 938 ± 446 869 ± 279 1693 ± 439 256 ± 210

**Tutankham** 227 ± 29 0.00075 221 ± 65 30 ± 13 26 ± 16 88 ± 74 130 ± 48 202 ± 65 **0 ± 0**

**Breakout** 356 ± 79 0.0005 355 ± 79 86 ± 104 74 ± 95 N/A 304 ± 111 353 ± 79 **44 ± 62**

**Seaquest** 1752 ± 70 0.005 1752 ± 73 356 ± 153 179 ± 83 N/A 46 ± 52 1752 ± 71 **4 ± 13**

**Pong** 20 ± 1 0.0005 20 ± 1 _−4 ± 8_ _−11 ± 7_ N/A 18 ± 3 20 ± 1 **_−13 ± 6_**

**Alien** 1615 ± 601 0.001 1629 ± 592 1062 ± 610 940 ± 565 N/A 1482 ± 633 1661 ± 625 507 ± 278

**Tutankham** 258 ± 53 0.001 260 ± 54 139 ± 26 134 ± 28 N/A 196 ± 34 260 ± 54 **71 ± 47**

**RoadRunner 34367 ± 6355 0.002 35851 ± 6675 9198 ± 3814 5410 ± 3058** N/A 31856 ± 7125 36550 ± 6848 2773 ± 3468


**DQN**

**A2C**


**Table 1: Average episode rewards ± standard deviation of vanilla DQN and A2C agents under different evasion**
attack methods in Atari environments. Results are averaged over 1000 episodes. Note that RS works for
continuous action spaces, thus is not included. MinQ is not applicable to A2C which does not have a Q
network. In each row, we bold the strongest (best) attack performance over all attacking methods.

the agent to select actions with the lowest action values based on the agent’s Q network, Robust
_SARSA (RS) (Zhang et al., 2020a) which performs the MinQ attack with a learned stable Q network,_
_MaxDiff (Zhang et al., 2020a) which maximizes the KL-divergence between the original victim_
policy and the perturbed policy, as well as SA-RL (Zhang et al., 2021) which directly learns the state
adversary with RL methods. We consider state attacks with ℓ norm as in most literature (Zhang
_∞_
et al., 2020a; 2021). Appendix E.1 provides hyperparameter settings and implementation details.

**PA-AD Finds the Strongest Adversaries in Atari Games** We first evaluate the performance of
PA-AD against well-trained DQN (Mnih et al., 2015) and A2C (Mnih et al., 2016) victim agents
on Atari games with pixel state spaces. The observed pixel values are normalized to the range of

[0, 1]. SA-RL and PA-AD adversaries are learned using the ACKTR algorithm (Wu et al., 2017)
with the same number of steps. (Appendix E.1 shows hyperparameter settings.) Table 1 presents
the experiment results, where PA-AD significantly outperforms all baselines against both DQN and
A2C victims. In contrast, SA-RL does not converge to a good adversary in the tested Atari games
with the same number of training steps as PA-AD, implying the importance of sample efficiency.
Surprisingly, using a relatively small attack budget ϵ, PA-AD leads the agent to the lowest possible
**reward in many environments such as Pong, RoadRunner and Tutankham, whereas other attackers**
may require larger attack budget to achieve the same attack strength. Therefore, we point out that
_vanilla RL agents are extremely vulnerable to carefully learned adversarial attacks. Even if an RL_
agent works well under naive attacks, a carefully learned adversary can let an agent totally fail with
the same attack budget, which stresses the importance of evaluating and improving the robustness
of RL agents using the strongest adversaries. Our further investigation in Appendix F.3 shows that
RL models can be generally more vulnerable than supervised classifiers, due to the different loss
and architecture designs. In Appendix E.2.1, we show more experiments with various selections of
the budget ϵ, where one can see PA-AD reduces the average reward more than all baselines over
_varying ϵ’s in various environments._

**PA-AD Finds the Strongest Adversaries MuJoCo Tasks** We further evaluate PA-AD on MuJoCo games, where both state spaces and action spaces are continuous. We use the same setting with
Zhang et al. (2021), where both the victim and the adversary are trained with PPO (Schulman et al.,
2017). During test time, the victim executes a deterministic policy, and we use the deterministic

**State** **Natural** **PA-AD**
**Environment** **_ϵ_** **Random** **MaxDiff** **RS** **SA-RL**
**Dimension** **Reward** **(ours)**

**Hopper** 11 3167 ± 542 0.075 2101 ± 793 1410 ± 655 794 ± 238 636 ± 9 **160 ± 136**

**Walker** 17 4472 ± 635 0.05 3007 ± 1200 2869 ± 1271 1336 ± 654 1086 ± 516 **804 ± 130**

**HalfCheetah** 17 7117 ± 98 0.15 5486 ± 1378 1836 ± 866 489 ± 758 **_−660 ± 218_** _−356 ± 307_

**Ant** 111 5687 ± 758 0.15 5261 ± 1005 1759 ± 828 268 ± 227 _−872 ± 436_ **_−2580 ± 872_**


**Table 2: Average episode rewards ± standard deviation of vanilla PPO agent under different evasion attack**
methods in MuJoCo environments. Results are averaged over 50 episodes. Note that MinBest and MinQ do
not fit this setting, since MinBest works for discrete action spaces, and MinQ requires the agent’s Q network.


-----

version of PA-AD with a continuous action space, as discussed in Section 4 and Appendix C.3. We
use the same attack budget ϵ as in Zhang et al. (2021) for all MuJoCo environments. Results in Table 2 show that PA-AD reduces the reward much more than heuristic methods, and also outperforms
SA-RL in most cases. In Ant, our PA-AD achieves much stronger attacks than SA-RL, since PA-AD
is more efficient than SA-RL when the state space is large. Admittedly, PA-AD requires additional
knowledge of the victim model, while SA-RL works in a black-box setting. Therefore, SA-RL is
more applicable to black-box scenarios with a relatively small state space, whereas PA-AD is more
applicable when the attacker has access to the victim (e.g. in adversarial training as shown in Table 3). Appendix E.2.3 provides more empirical comparison between SA-RL and PA-AD, which
shows that PA-AD converges faster, takes less running time, and is less sensitive to hyperparameters
_than SA-RL by a proper exploitation of the victim model._

**Environment** **Model** **RewardNatural** **Random** **MaxDiff** **RS** **SA-RL** **PA-AD(ours)** **Average rewardacross attacks**


SA-PPO 3705 ± 2 2710 ± 801 2652 ± 835 1130 ± 42 1076 ± 791 **856 ± 21** 1684.8

ATLA-PPO 3291 ± 600 3165 ± 576 2814 ± 725 2244 ± 618 1772 ± 802 **1232 ± 350** 2245.4

**PA-ATLA-PPO (ours) 3449 ± 237 3325 ± 239** 3145 ± 546 3002 ± 129 **1529 ± 284** 2521 ± 325 2704.4

SA-PPO 4487 ± 61 4867 ± 39 3668 ± 1789 3808 ± 138 2908 ± 1136 **1042 ± 153** 3258.6

ATLA-PPO 3842 ± 475 3927 ± 368 3836 ± 492 3239 ± 894 3663 ± 707 **1224 ± 770** 3177.8

**PA-ATLA-PPO (ours) 4178 ± 529** 4129 ± 78 4024 ± 572 3966 ± 307 3450 ± 478 **2248 ± 131** 3563.4

SA-PPO 3632 ± 20 3619 ± 18 3624 ± 23 3283 ± 20 3028 ± 23 **2512 ± 16** 3213.2

ATLA-PPO 6157 ± 852 6164 ± 603 5790 ± 174 4806 ± 603 5058 ± 718 **2576 ± 1548** 4878.8

**PA-ATLA-PPO (ours) 6289 ± 342 6215 ± 346** 5961 ± 53 5226 ± 114 4872 ± 79 **3840 ± 673** 5222.8

SA-PPO 4292 ± 384 4986 ± 452 4662 ± 522 3412 ± 1755 2511 ± 1117 −1296 ± 923 2855.0

ATLA-PPO 5359 ± 153 5366 ± 104 5240 ± 170 4136 ± 149 3765 ± 101 **220 ± 338** 3745.4

**PA-ATLA-PPO (ours) 5469 ± 106 5496 ± 158** 5328 ± 196 4124 ± 291 3694 ± 188 **2986 ± 864** 4325.6


**Hopper**
(state-dim: 11)
_ϵ: 0.075_

**Walker**
(state-dim: 17)
_ϵ: 0.05_

**Halfcheetah**
(state-dim: 17)
_ϵ: 0.15_

**Ant**
(state-dim: 111)
_ϵ: 0.15_


**Table 3: Average episode rewards ± standard deviation of robustly trained PPO agents under different attack**
methods. Results are averaged over 50 episodes. In each row corresponding to a robust agent, we bold the
strongest attack. The gray cells are the most robust agents with the highest average rewards across attacks.
Our PA-AD achieves the strongest attack against robust models, and our PA-ATLA-PPO achieves the most
robust performance under multiple attacks. The attack budget ϵ’s are the same as in Zhang et al. (2021).

**Training and Evaluating Robust Agents** A natural application of PA-AD is to evaluate the robustness of a known model, or to improve the robustness of an agent via adversarial training, where
the attacker has white-box access to the victim. Inspired by ATLA (Zhang et al., 2021) which alternately trains an agent and an SA-RL attacker, we propose PA-ATLA, which alternately trains an
agent and a PA-AD attacker. In Table 3, we evaluate the performance of PA-ATLA for a PPO agent
(namely PA-ATLA-PPO) in MuJoCo tasks, compared with state-of-the-art robust training methods,
SA-PPO (Zhang et al., 2020a) and ATLA-PPO (Zhang et al., 2021) [2]. From the table, we make
the following observations. (1) Our PA-AD attacker can significantly reduce the reward of previ_ous “robust” agents. Take the Ant environment as an example, although SA-PPO and ATLA-PPO_
agents gain 2k+ and 3k+ rewards respectively under SA-RL, the previously strongest attack, our PAAD still reduces their rewards to about -1.3k and 200+ with the same attack budget. Therefore, we
emphasize the importance of understanding the worst-case performance of RL agents, even robustlytrained agents. (2) Our PA-ATLA-PPO robust agents gain noticeably higher average rewards across
_attacks than other robust agents, especially under the strongest PA-AD attack. Under the SA-RL_
attack, PA-ATLA-PPO achieves comparable performance with ATLA-PPO, although ATLA-PPO
agents are trained to be robust against SA-RL. Due to the efficiency of PA-AD, PA-ATLA-PPO
requires fewer training steps than ATLA-PPO, as justified in Appendix E.2.4. The results of attacking and training robust models in Atari games are in Appendix E.2.5 and E.2.6, where PA-ATLA
improves the robustness of Atari agents against strong attacks with ϵ as large as 3/255.

7 CONCLUSION

In this paper, we propose an attack algorithm called PA-AD for RL problems, which achieves optimal attacks in theory and significantly outperforms prior attack methods in experiments. PA-AD can
be used to evaluate and improve the robustness of RL agents before deployment. A potential future
direction is to use our formulation for robustifying agents under both state and action attacks.

2We use ATLA-PPO(LSTM)+SA Reg, the most robust method reported by Zhang et al. (2021).


-----

ACKNOWLEDGMENTS

This work is supported by National Science Foundation IIS-1850220 CRII Award 030742-00001
and DOD-DARPA-Defense Advanced Research Projects Agency Guaranteeing AI Robustness
against Deception (GARD), and Adobe, Capital One and JP Morgan faculty fellowships.

ETHICS STATEMENT

Despite the rapid advancement of interactive AI and ML systems using RL agents, the learning agent
could fail catastrophically in the presence of adversarial attacks, exposing a serious vulnerability
in current RL systems such as autonomous driving systems, market-making systems, and security
monitoring systems. Therefore, there is an urgent need to understand the vulnerability of an RL
model, otherwise, it may be risky to deploy a trained agent in real-life applications, where the
observations of a sensor usually contain unavoidable noise.

Although the study of a strong attack method may be maliciously exploited to attack some RL
systems, it is more important for the owners and users of RL systems to get aware of the vulnerability
of their RL agents under the strongest possible adversary. As the old saying goes, “if you know
yourself and your enemy, you’ll never lose a battle”. In this work, we propose an optimal and
efficient algorithm for evasion attacks in Deep RL (DRL), which can significantly influence the
performance of a well-trained DRL agent, by adding small perturbations to the state observations of
the agent. Our proposed method can automatically measure the vulnerability of an RL agent, and
discover the “flaw” in a model that might be maliciously attacked. We also show in experiments
that our attack method can be applied to improve the robustness of an RL agent via robust training.
Since our proposed attack method achieves state-of-the-art performance, the RL agent trained under
our proposed attacker could be able to “defend” against any other adversarial attacks with the same
constraints. Therefore, our work has the potential to help combat the threat to high-stakes systems.

A limitation of PA-AD is that it requires the “attacker” to know the victim’s policy, i.e., PA-AD
is a white-box attack. If the attacker does not have full access to the victim, PA-AD can still be
used based on the transferability of adversarial attacks (Huang et al., 2017), although the optimality guarantee does not hold in this case. However, this limitation only restricts the ability of the
malicious attackers. In contrast, PA-AD should be used when one wants to evaluate the worst-case
performance of one’s own RL agent, or to improve the robustness of an agent under any attacks,
since PA-AD produces strong attacks efficiently. In these cases, PA-AD does have white-box access
to the agent. Therefore, PA-AD is more beneficial to defenders than attackers.

REPRODUCIBILITY STATEMENT

**For theoretical results, we provide all detailed technical proofs and lemmas in Appendix. In Ap-**
pendix A, we analyze the equivalence between evasion attacks and policy perturbations. In Appendix B, we theoretically prove some topological properties of the proposed Adv-policy-set, and
derive Theorem 4 that the outermost boundary of Bϵ[H] [(][π][)][ always contains an optimal policy per-]
turbation. In Appendix D, we systematically characterize the optimality of many existing attack
methods. We theoretically show (1) the existence of an optimal adversary, (2) the optimality of our
proposed PA-AD, and (3) the optimality of many heuristic attacks, following our Definition 5 in
Section 4.
**For experimental results, the detailed algorithm description in various types of environments is**
provided in Appendix C. In Appendix E, we illustrate the implementation details, environment settings, hyperparameter settings of our experiments. Additional experimental results show the performance of our algorithm from multiple aspects, including hyperparameter sensitivity, learning
efficiency, etc. In addition, in Appendix F, we provide some detailed discussion on the algorithm
design, as well as a comprehensive comparison between our method and prior works.
**The source code and running instructions for both Atari and MuJoCo experiments are in our sup-**
plementary materials. We also provide trained victim and attacker models so that one can directly
test their performance using a test script we provide.


-----

REFERENCES

Vahid Behzadan and Arslan Munir. Vulnerability of deep reinforcement learning to policy induction
attacks. In International Conference on Machine Learning and Data Mining in Pattern Recogni_tion, pp. 262–275. Springer, 2017._

Robert Dadashi, Adrien Ali Taiga, Nicolas Le Roux, Dale Schuurmans, and Marc G. Bellemare. The
value function polytope in reinforcement learning. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97
of Proceedings of Machine Learning Research, pp. 1486–1495, Long Beach, California, USA,
09–15 Jun 2019. PMLR.

Marc Fischer, Matthew Mirman, Steven Stalder, and Martin Vechev. Online robustness training for
deep reinforcement learning. arXiv preprint arXiv:1911.00887, 2019.

Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart Russell. Adversarial policies: Attacking deep reinforcement learning. In International Conference on Learning
_Representations, 2020._

Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In International Conference on Learning Representations, 2015.

David Silver Hado Van Hasselt, Arthur Guez. Deep reinforcement learning with double q-learning.
In Thirtieth AAAI Conference on Artificial Intelligence, 2016.

Ashley Hill, Antonin Raffin, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, Rene Traore,
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Rad[ford, John Schulman, Szymon Sidor, and Yuhuai Wu. Stable baselines. https://github.](https://github.com/hill-a/stable-baselines)
[com/hill-a/stable-baselines, 2018.](https://github.com/hill-a/stable-baselines)

Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel. Adversarial attacks
on neural network policies. arXiv preprint arXiv:1702.02284, 2017.

Yunhan Huang and Quanyan Zhu. Deceptive reinforcement learning under adversarial manipulations on cost signals. In International Conference on Decision and Game Theory for Security, pp.
217–237. Springer, 2019.

Matthew Inkawhich, Yiran Chen, and Hai Li. Snooping attacks on deep reinforcement learning.
In Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Sys_tems, AAMAS ’20, pp. 557–565, Richland, SC, 2020. International Foundation for Autonomous_
Agents and Multiagent Systems. ISBN 9781450375184.

Ezgi Korkmaz. Nesterov momentum adversarial perturbations in the deep reinforcement learning domain. In ICML 2020 Inductive Biases, Invariances and Generalization in Reinforcement
_Learning Workshop, 2020._

Jernej Kos and Dawn Song. Delving into adversarial attacks on deep policies. _arXiv preprint_
_arXiv:1705.06452, 2017._

Ilya Kostrikov. Pytorch implementations of reinforcement learning algorithms. [https://](https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail)
[github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail, 2018.](https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail)

Xian Yeow Lee, Yasaman Esfandiari, Kai Liang Tan, and Soumik Sarkar. Query-based targeted
action-space adversarial policies on deep reinforcement learning agents. In Proceedings of the
_ACM/IEEE 12th International Conference on Cyber-Physical Systems, ICCPS ’21, pp. 87–97,_
New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383530.

Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu, and Min Sun.
Tactics of adversarial attack on deep reinforcement learning agents. In Proceedings of the 26th
_International Joint Conference on Artificial Intelligence, IJCAI’17, pp. 3756–3762. AAAI Press,_
2017. ISBN 9780999241103.

Bj¨orn L¨utjens, Michael Everett, and Jonathan P How. Certified adversarial robustness for deep
reinforcement learning. In Conference on Robot Learning, pp. 1328–1337. PMLR, 2020.


-----

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. In International Conference on
_Learning Representations, 2018._

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529, 2015.

Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International conference on machine learning, pp. 1928–1937, 2016.

Tuomas Oikarinen, Tsui-Wei Weng, and Luca Daniel. Robust deep reinforcement learning through
adversarial loss. arXiv preprint arXiv:2008.01976, 2020.

Anay Pattanaik, Zhenyi Tang, Shuijing Liu, Gautham Bommannan, and Girish Chowdhary. Robust
deep reinforcement learning with adversarial attacks. In Proceedings of the 17th International
_Conference on Autonomous Agents and MultiAgent Systems, AAMAS ’18, pp. 2040–2042, Rich-_
land, SC, 2018. International Foundation for Autonomous Agents and Multiagent Systems.

Lerrel Pinto, James Davidson, Rahul Sukthankar, and Abhinav Gupta. Robust adversarial reinforcement learning. In Proceedings of the 34th International Conference on Machine Learning-Volume
_70, pp. 2817–2826. JMLR. org, 2017._

Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John
Wiley & Sons, Inc., USA, 1st edition, 1994. ISBN 0471619779.

Amin Rakhsha, Goran Radanovic, Rati Devidze, Xiaojin Zhu, and Adish Singla. Policy teaching
via environment poisoning: Training-time adversarial attacks against reinforcement learning. In
_International Conference on Machine Learning, pp. 7974–7984, 2020._

Alessio Russo and Alexandre Proutiere. Optimal attacks on reinforcement learning policies. In
_American Control Conference (ACC)., 2021._

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

Yanchao Sun, Da Huo, and Furong Huang. Vulnerability-aware poisoning mechanism for online rl
with unknown dynamics. In International Conference on Learning Representations, 2021.

Kai Liang Tan, Yasaman Esfandiari, Xian Yeow Lee, Soumik Sarkar, et al. Robustifying reinforcement learning agents via action space adversarial training. In 2020 American control conference
_(ACC), pp. 3959–3964. IEEE, 2020._

Chen Tessler, Yonathan Efroni, and Shie Mannor. Action robust reinforcement learning and applications in continuous control. In International Conference on Machine Learning, pp. 6215–6224.
PMLR, 2019.

Ioannis Antonoglou Tom Schaul, John Quan and David Silver. Prioritized experience replay. In
_International Conference on Learning Representations, 2016._

Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable trust-region
method for deep reinforcement learning using kronecker-factored approximation. In Advances in
_neural information processing systems, pp. 5279–5288, 2017._

Chaowei Xiao, Xinlei Pan, Warren He, Jian Peng, Mingjie Sun, Jinfeng Yi, Mingyan Liu, Bo Li,
and Dawn Song. Characterizing attacks on deep reinforcement learning. _arXiv preprint_
_arXiv:1907.09470, 2019._

Huan Zhang, Hongge Chen, Chaowei Xiao, Bo Li, Mingyan Liu, Duane Boning, and Cho-Jui Hsieh.
Robust deep reinforcement learning against adversarial perturbations on state observations. In
H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural
_Information Processing Systems, volume 33, pp. 21024–21037. Curran Associates, Inc., 2020a._


-----

Huan Zhang, Hongge Chen, Duane S Boning, and Cho-Jui Hsieh. Robust reinforcement learning
on state observations with learned optimal adversary. In International Conference on Learning
_Representations, 2021._

Xuezhou Zhang, Yuzhe Ma, Adish Singla, and Xiaojin Zhu. Adaptive reward-poisoning attacks
against reinforcement learning. In International Conference on Machine Learning, 2020b.


-----

## Appendix: Who Is the Strongest Enemy? Towards Optimal and Effi- cient Evasion Attacks in Deep RL

A RELATIONSHIP BETWEEN EVASION ATTACKS AND POLICY
PERTURBATIONS.

As mentioned in Section 2, all evasion attacks can be regarded as perturbations in the policy space.
To be more specific, we consider the following 3 cases, where we assume the victim uses policy π.

_Case 1 (attack on states): define the state adversary as function h such that ∀s ∈S_

_h(s) = ˜s_ _ϵ(s) :=_ _s[′]_ : _s[′]_ _s_ _ϵ_ _._
_∈B_ _{_ _∈S_ _∥_ _−_ _∥≤_ _}_

(For simplicity, we consider the attacks within a ϵ-radius norm ball.)
In this case, for all s, the victim samples action from πh( _s) = π(_ _h(s)) = π(˜s), which is_
_∈S_ _·|_ _·|_
equivalent to the victim executing a perturbed policy πh Π.
_∈_

_Case 2 (attack on actions for a deterministic π): define the action adversary as function h[(][A][)]_ :
_S × A →A, and ∀s ∈S, a ∈A_

_h[(][A][)](a_ _s) = ˜a_ _ϵ(a) :=_ _a[′]_ : _a[′]_ _a_ _ϵ_ _._
_|_ _∈B_ _{_ _∈A_ _∥_ _−_ _∥≤_ _}_

In this case, there exists a policy πh(A) such that πh(A) (s) = h[(][A][)](a _s) = ˜a, which is equivalent to_
the victim executing policy πh(A) Π. _|_
_∈_

_Case 3 (attack on actions for a stochastic π): define the action adversary as function h[(][A][)]_ : S×A →
_A, and ∀s ∈S, a ∈A_

_h[(][A][)](a|s) = ˜a such that {∥π(·|s) −_ _Pr(·|s)∥≤_ _ϵ},_

whereIn this case, there exists a policy Pr(˜a|s) denotes the probability that the action is perturbed into πh(A) such that πh(A) (s) = Pr( _s ˜)a, which is equivalent to the._
victim executing policy πh(A) Π. _·|_
_∈_

Most existing evasion RL works (Huang et al., 2017; Pattanaik et al., 2018; Zhang et al., 2020a;
2021) focus on state attacks, while there are also some works (Tessler et al., 2019; Tan et al., 2020)
studying action attacks. For example, Tessler et al. (Tessler et al., 2019) consider Case 2 and Case 3
above and train an agent that is robust to action perturbations.

These prior works study either state attacks or action attacks, considering them in two different
scenarios. However, the ultimate goal of robust RL is to train an RL agent that is robust to any threat
models. Otherwise, an agent that is robust against state attacks may still be ruined by an action
attacker. We take a step further to this ultimate goal by proposing a framework, policy attack, that
unifies observation attacks and action attacks.

Although the focus of this paper is on state attacks, we would like to point out that our proposed
method can also deal with action attacks (the director proposes a policy perturbation direction, and
an actor perturbs the action accordingly). It is also an exciting direction to explore hybrid attacks
(multiple actors conducting states perturbations and action perturbations altogether, directed by a
single director.) Our policy perturbation framework can also be easily incorporated in robust training
procedures, as an agent that is robust to policy perturbations is simultaneously robust to both state
attacks and action attacks.

B TOPOLOGICAL PROPERTIES OF THE ADMISSIBLE ADVERSARIAL POLICY
SET

As discussed in Section 3, finding the optimal state adversary in the admissible adversary set Hϵ
can be converted to a problem of finding the optimal policy adversary in the Adv-policy-set Bϵ[H] [(][π][)][.]
In this section, we characterize the topological properties of Bϵ[H] [(][π][)][, and identify how the value]
function changes as the policy changes within Bϵ[H] [(][π][)][.]

In Section B.1, we show that under the settings we consider, Bϵ[H] [(][π][)][ is a connected and compact]
subset of Π. Then, Section B.2, we define some additional concepts and re-formulate the notations.


-----

In Section B.3, we prove Theorem 4 in Section 3 that the outermost boundary of Bϵ[H] [(][π][)][ always]
contains an optimal policy perturbation. In Section B.4, we prove that the value functions of policies
in Bϵ[H] [(][π][)][ (or more generally, any connected and compact subset of][ Π][) form a polytope. Section][ B.6]

shows an example of the polytope result with a 2-state MDP, and Section B.5 shows examples of the
outermost boundary defined in Theorem 4.

B.1 THE SHAPE OF ADV-POLICY-SET Bϵ[H] [(][π][)]

It is important to note that Bϵ[H] [(][π][)][ is generally connected and compact as stated in the following]
lemma.

**Lemma 8 (Bϵ[H]** [(][π][)][ is connected and compact][)][.][ Given an MDP][ M][, a policy][ π][ that is a continuous]
_mapping, and admissible adversary set Hϵ := {h : h(s) ∈Bϵ(s), ∀s ∈S} (where ϵ > 0 is a_
_constant), the admissible adversarial policy set Bϵ[H]_ [(][π][)][ is a connected and compact subset of][ Π][.]

_Proof of Lemma 8. For an arbitrary state s ∈S, an admissible adversary h ∈_ _Hϵ perturbs it within_
an ℓp norm ball Bϵ(s), which is connected and compact. Since π is a continuous mapping, we know
_π(s) is compact and connected._

Therefore, Bϵ[H] [(][π][)][ as a Cartesian product of a finite number of compact and connected sets, is com-]
pact and connected.

B.2 ADDITIONAL NOTATIONS AND DEFINITIONS FOR PROOFS

We first formally define some concepts and notations.

For a stationary and stochastic policy π : S → ∆(A), we can define the state-to-state transition
function as
_P_ _[π](s[′]|s) :=_ _π(a|s)P_ (s[′]|s, a), ∀s, s[′] _∈S,_

_aX∈A_

and the state reward function as


_R[π](s) :=_


_π(a|s)R(s, a), ∀s ∈S._
_aX∈A_


Then the value of π, denoted as V _[π], can be computed via the Bellman equation_

_V_ _[π]_ = R[π] + γP _[π]V_ _[π]_ = (I − _γP_ _[π])[−][1]R[π]._

We further use Πsi to denote the projection of Π into the simplex of the i-th state, i.e., the space of
action distributions at state si.

Let fv : Π → R[|S|] be a mapping that maps policies to their corresponding value functions. Let
= fv(Π) be the space of all value functions.
_V_

Dadashi et al. (Dadashi et al., 2019) show that the image of fv applied to the space of policies, i.e.,
_fv(Π), form a (possibly non-convex) polytope as defined below._

**Definition 9 ((Possibly non-convex) polytope). A is called a convex polytope iff there are k ∈** N
_pointsconvex) polytope x1, x2, · · · is defined as a finite union of convex polytopes., xk ∈_ R[n] _such that A = Conv(x1, · · ·, xk). Furthermore, a (possibly non-_

And a more general concept is (possibly non-convex) polyhedron, which might not be bounded.

**Definition 10 ((Possibly non-convex) polyhedron). A is called a convex polyhedron iff it is the**
_intersection of k ∈_ N half-spaces _B[ˆ]1,_ _B[ˆ]2, · · ·,_ _B[ˆ]k, i.e., A = ∩i[k]=1B[ˆ]i. Furthermore, a (possibly_
**_non-convex) polyhedron is defined as a finite union of convex polyhedra._**

In addition, let Ys[π]1,···,sk [be the set of policies that agree with][ π][ on states][ s][1][,][ · · ·][, s][k][. Dadashi et]
al. (Dadashi et al., 2019) also prove that the values of policies that agree on all but one state s, i.e.,
_fv(YS\{[π]_ _s}[)][, form a line segment, which can be bracketed by two policies that are deterministic on]_
_s. Our Lemma 14 extends this line segment result to our setting where policies are restricted in a_
subset of policies.


-----

B.3 PROOF OF THEOREM 4: BOUNDARY CONTAINS OPTIMAL POLICY PERTURBATIONS

Lemma 4 in Dadashi et al. (2019) shows that policies agreeing on all but one state have certain
monotone relations. We restate this result in Lemma 11 below.
**Lemma 11 (Monotone Policy Interpolation). For any π0, π1** _Y_ _[π]_ _s_ _[that agree with][ π][ on all]_
_∈_ _S\{_ _}_
_states except for s ∈S, define a function l : [0, 1] →V as_

_l(α) = fv(απ1 + (1 −_ _α)π0)._

_Then we have_
_(1) l(0) ≽_ _l(1) or l(1) ≽_ _l(0) (≽_ _stands for element-wise greater than or equal to);_
_(2) If l(0) = l(1), then l(α) = l(0), ∀α ∈_ [0, 1];
_(3) If l(0) ̸= l(1), then there is a strictly monotonic rational function ρ : [0, 1] →_ R, such that
_l(α) = ρ(α)l(1) + (1 −_ _ρ(α))l(0)._

More intuitively, Lemma 11 suggests that the value of π[α] := απ1 + (1 − _α)π0 changes (strictly)_
monotonically with α, unless the values of π0, π1 and πα are all equal. With this result, we can
proceed to prove Theorem 4.

_Proof of Theorem 4. We will prove the theorem by contradiction._

Suppose there is a policy ˆπ ∈Bϵ[H] [(][π][)][ such that][ ˆ]π /∈ _∂πBϵ[H]_ [(][π][)][ and][ f][v][(ˆ]π) = V _π[ˆ] < V ˜π, ∀π˜ ∈Bϵ[H]_ [(][π][)][,]
i.e., there is no optimal policy adversary on the outermost boundary of Bϵ[H] [(][π][)][.]

Then according to the definition of ∂πBϵ[H] [(][π][)][, there exists at least one state][ s][ ∈S][ such that we can]
find another policy π[′] _∈Bϵ[H]_ [(][π][)][ agreeing with][ ˆ]π on all states except for s, where π[′](s) satisfies

_πˆ(·|s) = απ(·|s) + (1 −_ _α)π[′](·|s)_

for some scalar α ∈ (0, 1).

Then by Lemma 11, either of the following happens:


(1) fv(π) _fv(ˆπ)_ _fv(π[′])._
_≻_ _≻_
(2) fv(π) = fv(ˆπ) = fv(π[′]);

Note that fv(ˆπ) _fv(π) is impossible because we have assumed ˆπ has the lowest value over all_
_≻_
policies in Bϵ[H] [(][π][)][ including][ π][.]

If (1) is true, then π[′] is a better policy adversary than ˆπ in Bϵ[H] [(][π][)][, which contradicts with the]
assumption.

If (2) is true, then π[′] is another optimal policy adversary. By recursively applying the above process
to π[′], we can finally find an optimal policy adversary on the outermost boundary of Bϵ[H] [(][π][)][, which]
also contradicts with our assumption.

In summary, there is always an optimal policy adversary lying on the outermost boundary of Bϵ[H] [(][π][)][.]

B.4 PROOF OF THEOREM 12: VALUES OF POLICIES IN ADMISSIBLE ADVERSARIAL POLICY
SET FORM A POLYTOPE

We first present a theorem that describes the “shape” of the value functions generated by all admissible adversaries (admissible adversarial policies).
**Theorem 12 (Policy Perturbation Polytope). For a finite MDP M, consider a policy π and an**
_Adv-policy-set Bϵ[H]_ [(][π][)][. The space of values (a subspace of][ R][|S|][) of all policies in][ B]ϵ[H] [(][π][)][, denoted]
_by V_ _[B]ϵ[H]_ [(][π][)], is a (possibly non-convex) polytope.

In the remaining of this section, we prove a more general version of Theorem 12 as below.
**Theorem 13 (Policy Subset Polytope). For a finite MDP M, consider a connected and compact**
_subset of Π, denoted as T . The space of values (a subspace of R[|S|]) of all policies in T, denoted by_
_V_ _[T]_ _, is a (possibly non-convex) polytope._


-----

According to Lemma 8, Bϵ[H] [(][π][)][ is a connected and compact subset of][ Π][, thus Theorem][ 12][ is a]
special case of Theorem 13.

**Additional Notations** To prove Theorem 13, we further define a variant of Ys[π]1,···,sk [as][ T][ π]s1,···,sk [,]
which is the set of policies that are in and agree with π on states s1, _, sk, i.e.,_
_T_ _· · ·_

_s[π]1,_ _,sk_ [:=][ {][π][′][ ∈T][ :][ π][′][(][s][i][) =][ π][(][s][i][)][,][ ∀][i][ = 1][,][ · · ·][, k][}][.]
_T_ _···_

Note that different from Bϵ[H] [(][π][)][,][ T][ is no longer restricted under an admissible adversary set and can]
be any connected and compact subset of Π.

The following lemma shows that the values of policies in T that agree on all but one state form a
line segment.

**Lemma 14. For a policy π** _and an arbitrary state s_ _, there are two policies in ∂π_ _[π]_ _s_
_∈T_ _∈S_ _TS\{_ _}[,]_
_namely πs[−][, π]s[+][, such that][ ∀][π][′][ ∈T][ π]_ _s_
_S\{_ _}[,]_

_fv(πs[−][)][ ≼]_ _[f][v][(][π][′][)][ ≼]_ _[f][v][(][π]s[+][)][,]_ (3)

_wherethe image of ≼_ _denotes element-wise less than or equal to (if fv restricted to TS\{[π]_ _s}_ _[is a line segment.] a ≼_ _b, then ai ≤_ _bi for all index i). Moreover,_

_Proof of Lemma 14. Lemma 5 in Dadashi et al. (2019) has shown that fv is infinitely differentiable_
on Π, hence we know fv( _[π]_ _s_
_TS\{_ _}[)][ is compact and connected. According to Lemma 4 in][ Dadashi]_
et al. (2019), for any two policies π1, π2 ∈ _YS\{[π]_ _s}[, either][ f][v][(][π][1][)][ ≼]_ _[f][v][(][π][2][)][, or][ f][v][(][π][2][)][ ≼]_ _[f][v][(][π][1][)]_
(there exists a total order). The same property applies to TS\{[π] _s}_ [since][ T][ π]S\{s} [is a subset of][ Y][ π]S\{s}[.]

Therefore, there exists πs[−] [and][ π]s[+] [that achieve the minimum and maximum over all policies in]
_TS\{[π]_ _s}[. Next we show][ π]s[−]_ [and][ π]s[+] [can be found on the outermost boundary of][ T][ π]S\{s}[.]

Assume πs[+] _∈[/]_ _∂πTS\{[π]_ _s}[, and for all][ ˜]π ∈TS\{[π]_ _s}[,][ f][v][(˜]π) ≺_ _fv(πs[+][)][. Then we can find another policy]_
_π[′]_ _∈_ _∂πTS\{[π]_ _s}_ [such that][ π]s[+] [=][ απ][ + (1][ −] _[α][)][π][′][ for some scalar][ α][ ∈]_ [(0][,][ 1)][. Then according to]
Lemma 11, fv(π[′]) ≽ _fv(πs[+][)][, contradicting with the assumption. Therefore, one should be able to]_
find a policy on the outermost boundary of TS\{[π] _s}_ [whose value dominates all other policies. And]
similarly, we can also find πs[−] [on][ ∂][π][T][ π] _s_
_S\{_ _}[.]_

Furthermore, fv(TS\{[π] _s}[)][ is a subset of][ f][v][(][Y][ π]S\{s}[)][ since][ T][ π]S\{s}_ [is a subset of][ Y][ π]S\{s}[. Given that]
_fv(YS\{[π]_ _s}[)][ is a line segment, and][ f][v][(][T][ π]S\{s}[)][ is connected, we can conclude that][ f][v][(][T][ π]S\{s}[)][ is also]_
a line segment.

Next, the following lemma shows that πs[+] [and][ π]s[−] [and their linear combinations can generate values]
that cover the set fv( _[π]_ _s_
_TS\{_ _}[)][.]_

**Lemma 15. For a policy π ∈T, an arbitrary state s ∈S, and πs[+][, π]s[−]** _[defined in Lemma][ 14][, the]_
_following three sets are equivalent:_
_(1) fv(_ _[π]_ _s_
_TS\{_ _}[)][;]_
_(2) fv_ _closure(_ _[π]_ _s_ _, where closure(_ ) is the convex closure of a set;
_TS\{_ _}[)]_ _·_
_(3) {f v(απs[+]_ [+ (1][ −] _[α][)][π]_ _s[−][)][|][α][ ∈]_ [[0][,][ 1]][}][;]
_(4) {αfv(πs[+][) + (1][ −]_ _[α][)][f][v][(][π]s[−][)][|][α][ ∈]_ [[0][,][ 1]][}][;]


_Proof of Lemma 15. We show the equivalence by showing (1) ⊆_ (4) ⊆ (3) ⊆ (2) ⊆ (1) as below.

**(2)** **(1): For any π1, π2** _[π]_ _s_
_⊆_ _∈TS\{_ _}[, without loss of generality, suppose][ f][v][(][π][1][)][ ≼]_ _[f][v][(][π][2][)][. According]_
to Lemma 11, for any α ∈ [0, 1], fv(π1) ≼ _απ1 + (1 −_ _α)π2 ≼_ _fv(π2). Therefore, any convex_
combinations of policies in TS\{[π] _s}_ [has value that is in the range of][ f][v][(][T][ π]S\{s}[)][. So the values of]
policies in the convex closure of TS\{[π] _s}_ [do not exceed][ f][v][(][T][ π]S\{s}[)][, i.e., (2)][ ⊆] [(1).]


-----

**(3) ⊆** **(2): Based on the definition, απs[+]** [+ (1][ −] _[α][)][π]s[−]_ _[∈]_ _[closure][(][T][ π]S\{s}[)][, so (3)][ ⊆]_ [(2).]

**(4) ⊆** **(3): According to Lemma 11, there exists a strictly monotonic rational function ρ : [0, 1] →** R,
such that
_l(α) = fv(απs[+]_ [+ (1][ −] _[α][)][π]s[−][) =][ ρ][(][α][)][f][v][(][π]s[+][) + (1][ −]_ _[ρ][(][α][))][f][v][(][π]s[−][)][.]_

Therefore, due to intermediate value theorem, for α ∈ [0, 1], ρ(α) takes all values from 0 to 1. So
(4) = (3).

**(1) ⊆** **(4): Lemma 14 shows that fv(TS\{[π]** _s}[)][ is a line segment bracketed by][ f][v][(][π]s[+][)][ and][ f][v][(][π]s[−][)][.]_
Therefore, for any π[′] _∈TS\{[π]_ _s}[, its value is a convex combination of][ f][v][(][π]s[+][)][ and][ f][v][(][π]s[−][)][.]_

Next, we show that the relative boundary of the value space constrained to Ts[π]1,···,sk [is covered by]
policies that dominate or are dominated in at least one state. The relative interior of set A in B is
defined as the set of points in A that have a relative neighborhood in A _B, denoted as relintBA._
_∩_
The relative boundary of set A in B, denoted as ∂BA, is defined as the set of points in A that are
not in the relative interior of A, i.e., ∂BA = A relintBA. When there is no ambiguity, we omit the
_\_
subscript of ∂ to simplify notations.

In addition, we introduce another notation Fs[π]1, _,sk_ [:=][ V][ π][ +][ span][(][C]k[π]+1[,][ · · ·][, C] _[π]_ _i_

_···_ _|S|[)][, where][ C]_ _[π]_
stands for the i-th column of the matrix (I − _γP_ _[π])[−][1]. Note that Fs[π]1,···,sk_ [is the same with][ H]s[π]1,···,sk
in Dadashi et al. Dadashi et al. (2019), and we change H to F in order to distinguish from the
admissible adversary set Hϵ defined in our paper.

**Lemma 16. For a policy π ∈T, k ≤|S|, and a set of policies Ts[π]1,···,sk** _[that agree with][ π][ on]_
_s1,_ _, sk (perturb π only at sk+1,_ _, s_ _), define_ := fv( _s[π]1,_ _,sk_ [)][. Define two sets of policies]
_· · ·_ _· · ·_ _|S|_ _V_ _[t]_ _T_ _···_
_Xs[+]_ [:=][ {][π][′][ ∈T][ π]s1, _,sk_ [:][ π][′][(][·|][s][) =][ π]s[+][(][·|][s][)][}][, and][ X]s[−] [:=][ {][π][′][ ∈T]s[ π]1, _,sk_ [:][ π][′][(][·|][s][) =][ π]s[−][(][·|][s][)][}][.]

_···_ _···_
_We have that the relative boundary of V_ _[t]_ _in Fs[π]1,···,sk_ _[is included in the value functions spanned by]_
_policies in Ts[π]1,···,sk_ _[∩]_ [(][X]s[+]j _[∪]_ _[X]s[−]j_ [)][ for at least one][ s /]∈{s1, · · ·, sk}, i.e.,


_|S|_

_j=k+1_ _fv(Ts[π]1,···,sk_ _[∩]_ [(][X]s[+]j _[∪]_ _[X]s[−]j_ [))]

[


_∂V_ _[t]_ _⊂_


_Proof of Lemma 16. We first prove the following claim:_

**Claim 1: For a policy π0** _s[π]1,_ _,sk_ [, if][ ∀][j][ ∈{][k][ + 1][,][ · · ·][,][ |S|}][,][ ∄][π][′][ ∈] _[closure][(][T][ π]s1,_ _,sk_ [)][ ∩] [(][X]s[+]j
_Xs[−]j_ [)][ such that][ f][v][(][π][′][) =][ f] ∈T[v][(][π][0][)]···[, then][ f][v][(][π][0][)][ has a relative neighborhood in][ V] _[t][ ∩]_ _[F][ π]s···1,_ _,sk_ [.] _[∪]_

_···_

First, based on Lemma 14 and Lemma 15, we can construct a policy ˆπ _closure(_ _s[π]1,_ _,sk_ [)][ such]
_∈_ _T_ _···_
that fv(ˆπ) = fv(π0) through the following steps:

**Algorithm 2: Constructing ˆπ**


**1 Set π[k]** = π0

**2 for j = k + 1, · · ·, |S| do**

**3** Find πs[+]j _[, π]s[−]j_ _[∈T][ π]S\{[j][−]s[1]j_ _}_

**4** Find π[j] = ˆαjπs[+]j [+ (1][ −] _α[ˆ]j)πs[−]j_ [such that][ f][v][(][π][j][) =][ f][v][(][π][j][−][1][)]

**5 Return ˆπ = π[|S|]**

Denote the concatenation of αj’s as a vector ˆα := [ˆαk+1, · · ·, ˆα|S|].

According to the assumption that _j_ _k + 1,_ _,_, ∄π[′] _closure(_ _s[π]1,_ _,sk_ [)][ ∩] [(][X]s[+]j _sj_ [)]
such that fv(π[′]) = fv(π0), we have ∀ _∈{ ˆαj /∈{0, · · · 1}, ∀ |S|}j = k + 1∈_ _, · · ·, |S|. Then, define a functionT_ _···_ _[∪]_ _[X]_ _[−]_
_φ : (0, 1)[|S|−][k]_ _→V_ _[t]_ such that


_πα(·|sj) = απs[+]j_ [+ (1][ −] _[α][)][π]s[−]j_ if j ∈{k + 1, · · ·, |S|}
_πα(_ _sj) = ˆπ(_ _sj)_ otherwise

_·|_ _·|_


_φ(α) = fv(πα), where_


-----

Then we have that

1. φ is continuously differentiable.

2. φ(ˆα) = fv(ˆπ).

3. _∂α∂φj_ [is non-zero at][ ˆ]α (because of Lemma 11 (3)).


4. _∂α∂φj_ [is along the][ i][-the column of][ (][I][ −] _[γP][ ˆ]π)−1 (see Lemma 3 in Dadashi et al. Dadashi_

et al. (2019)).

Therefore, by the inverse theorem function, there is a neighborhood of φ(α) = fv(ˆπ) in the image
space.

Now we have proved Claim 1. As a result, for any policy π0 _s[π]1,_ _,sk_ [, if][ f][v][(][π][0][)][ is in the relative]
_∈T_ _···_
boundary of in Fs[π]1, _,sk_ [, then][ ∃][j][ ∈{][k][ + 1][,][ · · ·][,][ |S|}][, π][′][ ∈] _[closure][(][T][ π]s1,_ _,sk_ [)][ ∩] [(][X]s[+]j _sj_ [)]
such thatsuch that f fvv(( Vππ[′′][′][t]) =) = f fvv((···ππ00)). So Lemma. Based on Lemma 16 holds. 15, we can also find π[′′] _∈Ts[π]1···,···,sk_ _[∩]_ [(][X]s[+]j _[∪][∪]_ _[X][X]s[−][−]j_ [)]

Now, we are finally ready to prove Theorem 13.

_Proof of Theorem 13. We will show that_ _s1,_ _, sk_, the value = fv( _s[π]1,_ _,sk_ [)][ is a]
_∀{_ _· · ·_ _} ⊆S_ _V_ _[t]_ _T_ _···_
polytope.

We prove the above claim by induction on the cardinality of the number of states k. In the base case
where k =, = _fv(π)_ is a polytope.
_|S|_ _V_ _[t]_ _{_ _}_

Suppose the claim holds for k + 1, then we show it also holds for k, i.e., for a policy π ∈ Π, the
value of Ts[π]1,···,sk _[⊆]_ _[Y][ π]s1,···,sk_ _[⊆]_ [Π][ for a polytope.]

According to Lemma 16, we have


_|S|_

_j=k+1_ _fv(Ts[π]1,···,sk_ _[∩]_ [(][X]s[+]j _[∪]_ _[X]s[−]j_ [)) =]

[


_|S|_

(Fs[+]j _sj_ [))]
_j=k+1_ _V_ _[t]_ _∩_ _[∪]_ _[F][ −]_

[


_∂V_ _[t]_ _⊂_


where ∂V _[t]_ denotes the relative boundary of V _[t]_ in Fs[π]1,···,sk [;][ F][ +]sj [and][ F][ −]sj [are two affine hyperplanes]
of Fs[π]1,···,sk [, standing for the value space of policies that agree with][ π]s[+]j [and][ π]s[−]j [in state][ s][j] [respec-]
tively.

Then we can get

1. V _[t]_ = fv(Ts[π]1,···,sk [)][ is closed as][ T][ π]s1,···,sk [is compact and][ f][v] [is continuous.]

2. ∂ _j=k+1[(][F][ +]sj_ _sj_ [))][, a finite number of affine hyperplanes in][ F][ π]s1, _,sk_ [.]
_V_ _[t]_ _⊂_ [S][|S|] _[∪]_ _[F][ −]_ _···_

3. V _[t]_ _∩_ _Fs[+]j_ [(or][ V] _[t][ ∩]_ _[F][ −]sj_ [) is a polyhedron by induction assumption.]


Hence, based on Proposition 1 by Dadashi et al. Dadashi et al. (2019), we get V _[t]_ is a polyhedron.
Since V _[t]_ _⊆V is bounded, we can further conclude that V_ _[t]_ is a polytope.

Therefore, for an arbitrary connected and compact set of policies T ⊆ Π, let π ∈T be an arbitrary
policy in, then fv( ) = fv( _[π]_
_T_ _T_ _T∅_ [)][ is a polytope.]

B.5 EXAMPLES OF THE OUTERMOST BOUNDARY

See Figure 5 for examples of the outermost boundary for different Bϵ[H] [(][π][)][’s.]


-----

**Figure 5: Two examples of the outermost boundary with |A| = 3 actions at one single state s. The large**
triangle denotes the distributions over the action space at state s, i.e., Πs; π1, π2 and π3 are three policies
that deterministically choose a1, a2 and a3 respectively. π is the victim policy, the dark green area is the
_Bϵ[H]_ [(][π][)]s [:][ B]ϵ[H] [(][π][)][ ∩] [Π]s[. The red solid curve depicts the outermost boundary of][ B]ϵ[H] [(][π][)]s[. Note that a policy is]
in the outermost boundary of Bϵ[H] [(][π][)][ iff it is in the outermost boundary of][ B]ϵ[H] [(][π][)]s [for all][ s][ ∈S][.]

!["][∗]

!["][$]∗


**Figure 6: Value space of an example MDP. The values of the whole policy space Π form a polytope (blue) as**
suggested by Dadashi et al. (2019). The values of all perturbed policies with Hϵ also form a polytope (green)
as suggested by Theorem 12.

B.6 AN EXAMPLE OF THE POLICY PERTURBATION POLYTOPE

An example is given by Figure 6, where we define an MDP with 2 states and 3 actions. We train
an DQN agent with one-hot encodings of the states, and then randomly perturb the states within
an ℓ ball with ϵ = 0.8. By sampling 5M random policies, and 100K random perturbations, we
_∞_
visualize the value space of approximately the whole policy space Π and the admissible adversarial
policy set Bϵ[H] [(][π][)][, both of which are polytopes (boundaries are flat). A learning agent searches for]
the optimal policy π[∗] whose value is the upper right vertex of the larger blue polytope, while the
attacker attempts to find an optimal adversary h[∗], which perturbs a given clean policy π to the worst
perturbed policy πh∗ whose value is the lower left vertex of the smaller green polytope. This also
justifies the fact that learning an optimal adversary is as difficult as learning an optimal policy in an
RL problem.

The example MDP ex:
_M_

_|A| = 3, γ = 0.8_
_rˆ = [−0.1, −1., 0.1, 0.4, 1.5, 0.1]_
_Pˆ = [[0.9, 0.1], [0.2, 0.8], [0.7, 0.3], [0.05, 0.95], [0.25, 0.75], [0.3, 0.7]]_

The base/clean policy π:

_π(a1_ _s1) = 0.215, π(a2_ _s1) = 0.429, π(a3_ _s1) = 0.356_
_|_ _|_ _|_
_π(a1_ _s2) = 0.271, π(a2_ _s2) = 0.592, π(a3_ _s2) = 0.137_
_|_ _|_ _|_


-----

C EXTENTIONS AND ADDITIONAL DETAILS OF OUR ALGORITHM

C.1 ATTACKING A DETERMINISTIC VICTIM POLICY

For a deterministic victim πD = argmaxaπ(a _s), we define Deterministic Policy Adversary MDP_
_|_
(D-PAMDP) as below, where a subscript D is added to all components to distinguish them from their
stochastic counterparts. In D-PAMDP, the director proposes a target action _aD_ (=: _D), and_
the actor tries its best to let the victim output this target action. _∈A_ _A_
**Definition 17 (Deterministic Policy Adversary MDP (D-PAMDP)). Given an MDP b** [b] =
_M_
_,_ _, P, R, γ_ _, a fixed and deterministic victim policy πD, we define a Deterministic Policy Ad-_
_⟨S_ _A_ _⟩_
_versarial MDP_ _MD = ⟨S,_ _AD,_ _PD,_ _RD, γ⟩, where the action space is_ _AD =_ _AD, and ∀s, s[′]_ _∈_
_S, ∀a ∈A,_
_PD[c](s[′]_ _s,_ _a) = P[b](s[′]_ _s, π[b]_ _D[b](g(a, s))),_ _RD(s,_ _a) =_ _R(s, πD[b](g(a, s[b])))._
_|_ _|_ _−_
_The actor functionb_ _g is defined as_
b _gD b(a, s) = argmaxs˜_ bϵ(s) _π(a_ _sb˜)_ max b _a_ _,a=a[π][(][a][|]s[˜])_ b (GD)
_∈B_ _|_ _−_ _∈A_ _̸_

The optimal policy of D-PAMDP is an optimal adversary against  _πD as proved in Appendixb_  D.2.2

b b

C.2 IMPLEMENTATION DETAILS OF PA-AD

To address the actor function g (or gD) defined in (G) and (GD), we let the actor maximize objectives
_JD and J within the Bϵ(·) ball around the original state, for a deterministic victim and a stochastic_
victim, respectively. Below we explicitly define JD and J.

**Actor Objective for Deterministic Victim** For the deterministic variant of PA-AD, the actor function (GD) is simple and can be directly solved to identify the optimal adversary. Concretely, we
define the following objective

_JD(˜s;_ _a, s) := π(a_ _s˜)_ maxa _,a=a[π][(][a][|]s[˜]),_ (JD)
_|_ _−_ _∈A_ _̸_

which can be realized with the multi-class classification hinge loss. In practice, a relaxed cross-b
entropy objective can also be used to maximize b b _π(a|s˜)._

**Actor Objective for Stochastic Victim** Different from the deterministic-victim case, the actor
function for a stochastic victim defined in (G) requires solving a more complex optimization prob-b
lem with a non-convex constraint set, which in practice can be relaxed to (J) (a Lagrangian relaxation) to efficiently get an approximation of the optimal adversary.
argmaxs˜∈Bϵ(s)[J][(˜]s; _a, s) := ∥π(·|s˜) −_ _π(·|s)∥_ + λ × CosineSim _π(·|s˜) −_ _π(·|s),_ _a_ (J)
where CosineSim in the second refers to the cosine similarity function; the first term measures how
  
far away the policy is perturbed from the victim policy; b _λ is a hyper-parameter controlling the trade- b_
off between the two terms. Experimental results show that our PA-AD is not sensitive to the value
of λ. In our reported results in Section 6, we set λ as 1. Appendix E.2.2 shows the evaluation of our
algorithm using varying λ’s.

The procedure of learning the optimal adversary is depicted in Algorithm 3, where we simply use
the Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2015) to approximately solve the actor’s
objective, although more advanced solvers such as Projected Gradient Decent (PGD) can be applied
to further improve the performance. Experiment results in Section 6 verify that the above FGSMbased implementation achieves state-of-the-art attack performance.

**What is the Influence of the Relaxation in (J)? First, it is important that the relaxation is only**
needed for a stochastic victim. For a deterministic victim, which is often the case in practice, the
actor solves the original unrelaxed objective.
Second, as we will discuss in the next paragraph, the optimality of both SA-RL and PA-AD is regarding the formulation. That is, SA-RL and PA-AD formulate the optimal attack problem as an
MDP whose optimal policy is the optimal adversary. However, in a large-scale task, deep RL algorithms themselves usually do not converge to the globally optimal policy and exploration becomes
the main challenge. Thus, when the adversary’s MDP is large, the suboptimality caused by the RL
solver due to exploration difficulties could be much more severe than the suboptimality caused by
the relaxation of the formulation. The comparison between SA-RL and PA-AD in our experiments


-----

**Algorithm 3: Policy Adversarial Actor Director (PA-AD) with FGSM**


**1 Input: Initialization of director’s policy ν; victim policy π; budget ϵ; start state s0**

**2 for t = 0, 1, 2, ... do**

|Col1|Director samples a policy perturbing directiona ν( s ) bt ∼ ·| t if Victim is deterministic then # for a deterministic victim, J is defined in Equation (J D) D Actor computes the gradient of its objective ∇δJ D(s + δ; ba t, s t) t else # for a stochastic victim, J is defined in Equation (J) Actor computes the gradient of its objective ∇δJ(s + δ; ba t, s t) t Actor sets s˜ = s + ϵ sign(δ) t t · Victim takes action a π( s˜), proceeds to s, receives r t ∼ ·| t t+1 t Director saves (s,a, r, s ) to its buffer t bt − t t+1 Director updates its policy ν using any RL algorithm|
|---|---|



can justify that the size of the adversary MDP has a larger impact than the relaxation of the problem
on the final solution found by the attackers.
Third, in Appendix F.1, we empirically show that with the relaxed objective, PA-AD can still find
the optimal attacker in 3 example environments.

**Optimality in Formulation v.s. Approximated Optimality in Practice** PA-AD has an optimal
formulation, as the optimal solution to its objective (the optimal policy in PAMDP) is always an optimal adversary (Theorem 7). Similarly, the previous attack method SA-RL has an optimal solution
since the optimal policy in the adversary’s MDP is also an optimal adversary. However, in practice
where the environments are in a large scale and the number of samples is finite, the optimal policy
is not guaranteed to be found by either PA-AD and SA-RL with deep RL algorithms. Therefore, for
practical consideration, our goal is to search for a good solution or approximate the optimal solution
using optimization techniques (e.g. actor-critic learning, one-step FGSM attack, Lagrangian relaxation for the stochastic-victim attack). In experiments (Section 6), we show that our implementation
universally finds stronger attackers than prior methods, which verifies the effectiveness of both our
theoretical framework and our practical implementation.

C.3 VARIANTS FOR ENVIRONMENTS WITH CONTINUOUS ACTION SPACES

Although the analysis in the main paper focuses on an MDP whose action space is discrete, our
algorithm also extends to a continuous action space as justified in our experiments.

C.3.1 FOR A DETERMINISTIC VICTIM

In this case, we can still use the formulation D-PAMDP, but a slightly different actor function
_gD(a, s) = argmins˜∈Bϵ(s)[∥][π][D][(˜]s) −_ _a∥._ (GCD)

C.3.2 FOR A STOCHASTIC VICTIMb b

Different from a stochastic victim in a discrete action space whose actions are sampled from a categorical distribution, a stochastic victim in a continuous action space usually follows a parametrized
probability distribution with a certain family of distributions, usually Gaussian distributions. In
this case, the formulation of PAMDP in Definition 6 is impractical. However, since the mean of
a Gaussian distribution has the largest probability to be selected, one can still use the formulation
in (GCD), while replacing πD(˜s) with the mean of the output distribution. Then, the director and
the actor can collaboratively let the victim output a Gaussian distribution whose mean is the target
action. If higher accuracy is needed, we can use another variant of PAMDP, named Continuous
Policy Adversary MDP (C-PAMDP) that can also control the variance of the Gaussian distribution.

**Definition 18 (Continuous Policy Adversary MDP (C-PAMDP)). Given an MDP M** =
_⟨S, A, P, R, γ⟩_ _where A is continuous, a fixed and stochastic victim policy π, we define a Con-_
_tinuous Policy Adversarial MDP_ _M[c]C = ⟨S,_ _A[b]C,_ _P[b]C,_ _R[b]C, γ⟩, where the action space is_ _A[b]D = A,_


-----

_and ∀s, s[′]_ _∈S, ∀a ∈A,_

_P_ (s[′]|s, _a) =b_ _π(a|g(a, s))P_ (s[′]|s, a) da, _R(s,_ _a) = −_ _π(a|g(a, s))R(s, a)da._
ZA ZA

_The actor function g is defined as_

b b b b b b

_g(a, s) = argmins˜_ _ϵ(s)[KL][(][π][(][·|]s[˜])_ (a, σ[2]I )). (GC)
_∈B_ _||N_ _|A|_
_where σ is a hyper-parameter, and N denotes a multivariate Gaussian distribution._
b b

In short, Equation (GC) encourages the victim to output a distribution that is similar to the target
distribution. The hyperparameter σ controls the standard deviation of the target distribution. One
can set σ to be small in order to let the victim execute the target action ba with higher probabilities.

D CHARACTERIZE OPTIMALITY OF EVASION ATTACKS

In this section, we provide a detailed characterization for the optimality of evasion attacks from the
perspective of policy perturbation, following Definition 5 in Section 4. Section D.1 establishes the
existence of the optimal policy adversary which is defined in Section 3. Section D.2 then provides
a proof for Theorem 7 that the formulation of PA-AD is optimal. We also analyze the optimality of
heuristic attacks in Section D.3.

D.1 EXISTENCE OF AN OPTIMAL POLICY ADVERSARY

**Theorem 19 (Existence of An Optimal Policy Adversary). Given an MDP M = ⟨S, A, P, R, γ⟩,**
_and a fixed stationary policy π on M, let Hϵ be a non-empty set of admissible state adversaries and_
_Bϵ[H]_ [(][π][)][ be the corresponding Adv-policy-set, then there exists an optimal policy adversary][ π][h][∗] _[∈]_
_Bϵ[H]_ [(][π][)][ such that][ π][h][∗] _[∈]_ [argmin]πh∈Bϵ[H] [(][π][)][V][ π]M[h] [(][s][)][,][ ∀][s][ ∈S][.]

_Proof. We prove Theorem 19 by constructing a new MDP corresponding to the original MDP M_
and the victim π.

**Definition 20 (Policy Perturbation MDP). For a given MDP M, a fixed stochastic victim pol-**
_icy π, and an admissible state adversary set Hϵ, define a policy perturbation MDP as MP =_
_⟨S, AP, PP, RP, γ⟩, where AP = ∆(A), and ∀s ∈S, aP ∈AP,_

_RP (s, aP ) :=_ _a∈A_ _[a][P][ (][a][|][s][)][R][(][s, a][)]_ _if ∃h ∈_ _Hϵ s.t. aP (·|s) = π(·|h(s))_ (4)
_{ [−]_ [P] _otherwise_

_−∞_

_PP (s[′]|s, aP ) :=_ _aP (a|s)P_ (s[′]|s, a) (5)

_aX∈A_

Then we can prove Theorem 19 by proving the following lemma.

**Lemma 21. The optimal policy in MP is an optimal policy adversary for π in M.**


Let NP denote the set of deterministic policies in MP . According to the traditional MDP theory (Puterman, 1994), there exists a deterministic policy that is optimal in MP . Note that Hϵ is
non-empty, so there exists at least one policy in MP with value ≥−∞, and then the optimal policy
should have value . Denote this optimal and deterministic policy as νP[∗]
the Bellman equation of ≥−∞ νP[∗] [, i.e.,] _[∈]_ _[N][P][ . Then we write]_

_P_
_VP[ν][∗]_ [(][s][) = max]νP _NP_ _[R][P][ (][s, ν][P][ (][s][)) +][ γ]_ _PP (s[′]|s, νP (s))VP[ν][P]_ [(][s][′][)]
_∈_ _s[′]_
X∈S


_νP (a|s)P_ (s[′]|s, a)VP[ν][P] [(][s][′][)]
_aX∈A_


_νP (a|s)R(s, a) + γ_
_aX∈A_


= max
_νP ∈NP_

= max
_νP ∈NP_


(6)


_s[′]∈S_


_P_ (s[′]|s, a)VP[ν][P] [(][s][′][)]
_sX[′]∈S_


_νP (a|s)_
_aX∈A_


_−R(s, a) + γ_


-----

Note that νP[∗] [(][s][)][ is a distribution on action space,][ ν]P[∗] [(][a][|][s][)][ is the probability of][ a][ given by distribution]
_ν[∗](s)._

Multiply both sides of Equation (6) by −1, and we obtain


_P_
_−VP[ν][∗]_ [(][s][) = min]ν _NP_
_∈_


_P_ (s[′]|s, a) _−_ _VP[ν][P]_ [(][s][′][)]
_sX[′]∈S_  


(7)

(8)


_νP (s)(a|s)_
_aX∈A_


_R(s, a) + γ_


In the original MDP, an optimal policy adversary (if exists) πh∗ for π should satisfy
_M_


_V_ _[π][h][∗]_ (s) = min
_πh∈Bϵ[H]_ [(][π][)]


_P_ (s[′]|s, a)V _[π][h]_ (s[′])
_sX[′]∈S_


_πh(a_ _s)_
_|_
_aX∈A_


_R(s, a) + γ_


By comparing Equation (7) and Equation (8) we get the conclusion that νP[∗] [is an optimal policy]
adversary for π in M.

D.2 PROOF OF THEOREM 7: OPTIMALITY OF OUR PA-AD

In this section, we provide theoretical proof of the optimality of our proposed evasion RL algorithm
PA-AD.

D.2.1 OPTIMALITY OF PA-AD FOR A STOCHASTIC VICTIM

We first build a connection between the PAMDP _M defined in Definition 6 (Section 4) and the_
policy perturbation MDP defined in Definition 20 (Appendix D.1).

[c]

A deterministic policy ν in the PAMDP _M can induce a policy ˆνP in MP in the following way:_
equal because of the formulations of the two MDPs, i.e.,νP (s) = π(·|g(ν(s), s)), ∀s ∈S. More importantly, the values of[c] _V_ _[ν]_ = VPνP ν[, where] and _ν[ b]PV in andM V andP denote the MP are_
value functions inb _M and VP respectively._ [b] b [c]

[b]

Proposition 22 below builds the connection of the optimality between the policies in these two
MDPs. [c]
**Proposition 22. An optimal policy in** _M induces an optimal policy in MP ._

_Proof of Proposition 22. Let ν[∗]_ be an deterministic optimal policy in[c], and it induces a policy in
_M_
_MP, namely_ _νP ._

Let us assumeP bνP _νP is not an optimal policy in MP, hence there exists a policy[c]_ _νP[∗]_ [in][ M][P][ s.t.]
_VP[ν][∗]_ [(][s][)][ > V]P[ b] [(][s][)][ for at least one][ s][ ∈S][. And based on Theorem][ 4][, we are able to find such a][ ν]P[∗]
whose corresponding policy perturbation is on the outermost boundary of b _B(π), i.e., ν[∗]_ _∈_ _∂πBϵ[H]_ [(][π][)][.]

Then we can construct a policy ν[′] in _M such that ν[′](s) = νP[∗]_ [(][s][)][ −] _[π][(][s][)][,][ ∀][s][ ∈S][. And based on]_
Equation (G), π( _g(ν[′](s), s)) is in ∂π_ (π(s)) for all s . According to the definition of ∂π, if

_·|_ _B_ _∈S_
two policy perturbations perturb π in the same direction and are both on the outermost boundary,

[c]
then they are equal. Thus, we can conclude that π(g(ν[′](s), s)) = νP[∗] [(][s][)][,][ ∀][s][ ∈S][. Then we obtain]
_V_ _[ν][′]_ (s) = VP[ν]P[∗] [(][s][)][,][ ∀][s][ ∈S][.]

Now we have conditions:
(1)b _V_ _[ν][∗]_ (s) = VPνP [(][s][)][,][ ∀][s][ ∈S][;]
_P_ _νP_
(2) VP[ν][∗] [(][s][)][ > V]P[ b][b] [(][s][)][ for at least one][ s][ ∈S][;]
(3) ∃[b]ν[′] such that _V_ _[ν][′]_ (s) = VP[ν]P[∗] [(][s][)][,][ ∀][s][ ∈S][.]

From (1), (2) and (3), we can conclude that[b] _V_ _[ν][′]_ (s) > _V_ _[ν][∗]_ (s) for at least one s ∈S, which conflicts
with the assumption that ν[∗] is optimal in _M. Therefore, Proposition 22 is proven._

[b] [b]

[c]


-----

Proposition 22 and Lemma 21 together justifies that the optimal policy of _M, namely ν[∗], induces_
an optimal policy adversary for π in the original M. Then, if the director learns the optimal policy
in, then it collaborates with the actor and generates the optimal state adversary[c] _h[∗]_ by h[∗](s) =
_M_
_g(ν[∗](s), s), ∀s ∈S._

[c]

D.2.2 OPTIMALITY OF OUR PA-AD FOR A DETERMINISTIC VICTIM

In this section, we show that the optimal policy in D-PAMDP (the deterministic variant of PAMDP
defined in Appendix C.1) also induces an optimal policy adversary in the original environment.

Let πD be a deterministic policy reduced from a stochastic policy π, i.e.,
_πD(s) := argmaxa∈Aπ(a|s), ∀s ∈S._
Note that in this case, the Adv-policy-set Bϵ[H] [(][π][)][ is not connected as it contains only deterministic]
policies. Therefore, we re-formulate the policy perturbation MDP introduced in Appendix D.1 with
a deterministic victim as below:

**Definition 23 (Deterministic Policy Perturbation MDP). For a given MDP M, a fixed determin-**
_istic victim policy π, and an admissible adversary set Hϵ, define a deterministic policy perturbation_
_MDP as MDP = ⟨S, ADP, PDP, RDP, γ⟩, where ADP = A, and ∀s ∈S, aDP ∈ADP,_

_if_ _h_ _Hϵ s.t. aDP (s) = πD(h(s))_
_RDP (s, aDP ) :=_ _∃_ _∈_ (9)
_{ [−][R][(][s, a][DP][ )]_ _otherwise_
_−∞_

_PDP (s[′]|s, aDP ) := P_ (s, aDP ) (10)

_MDP can be viewed as a special case of MP where only deterministic policies have ≥−∞_ values.
Therefore Theorem 19 and Lemma 21 also hold for deterministic victims.

Next we will show that an optimal policy in _MD induces an optimal policy in MDP ._

**Proposition 24. An optimal policy in** _D induces an optimal policy in_ _DP ._
_M[c]_ [c] _M_


_Proof of Proposition 24. We will prove Proposition 24 by contradiction. Let ν[∗]_ be an optimal policy
in _MD, and it induces a policy in MDP, namely_ _νDP ._

Let us assume _νDP is not an optimal policy in_ _DP, hence there exists a deterministic policy νDP[∗]_

[c] _DP_ _νDP_ _M b_

_VinDP M[ν]DP[∗]_ _DP[(][s][0] s.t.[)][ > V] V bDP[ b]νDP[ν]DP[∗]_ [(][(][s][s][0][)][)][ > V][.] [ b]DP [(][s][)][ for at least one][ s][ ∈S][. Without loss of generality, suppose]

Next we construct another policy ν[′] in _MD by setting ν[′](s) = νDP[∗]_ [(][s][)][,][ ∀][s][ ∈S][. Given that][ ν]DP[∗]
is deterministic, ν[′] is also a deterministic policy. So we use νDP[∗] [(][s][)][ and][ ν][′][(][s][)][ to denote the action]
selected by νDP[∗] [and][ ν][′][ respectively at state][c] _[ s][.]_

For an arbitrary state si, let ai := νDP[∗] [(][s][i][)][. Since][ ν]DP[∗] [is the optimal policy in][ M][DP][, we get]
that there exists a state adversary h ∈ _Hϵ such that πD(h(si)) = ai, or equivalently, there exists a_
stateproblem ( ˜si ∈BGDϵ() given directionsi) such that argmax ai and statea∈Aπ(˜ ssii) =, denoted as ai. Then, the solution to the actor’s optimization ˜s[∗], satisfies
_s˜[∗]_ = argmaxs′∈Bϵ(s) _π(a|s[′]) −_ argmaxa∈A,a≠ a[π][(][a][|][s][′][)] (11)
and we can get
  b 
_π(a_ _s˜[∗])_ argmaxa _,a=a[π][(][a][|]s[˜][∗])b_ _π(a_ _s˜i)_ argmaxa _,a=a[π][(][a][|]s[˜]i) > 0_ (12)
_|_ _−_ _∈A_ _̸_ _≥_ _|_ _−_ _∈A_ _̸_
Given that argmaxa∈Aπ(ai|s˜i) =b _ai, we obtain argmaxa∈Aπ(ai|s˜b[∗])_ = _ai, and hence_
_πD(gD(ai, si)) =b_ _ai. Since this relation holds for an arbitrary stateb_ _s, we can get_
_πD(gD(ν[′](s), s)) = πD(gD(ν[′](s), s)) = ν[′](s),_ _s_ (13)
_∀_ _∈S_


-----

Also, we have ∀s ∈S
_VD[ν][′]_ [(][s][) =][ b]RD(s, ν[′](s)) + _PD(s[′]|s, ν[′](s))V[b]D[ν][′]_ [(][s][′][)] (14)

_sX[′]∈S_

_VDP[ν]bDP[∗]_ [(][s][) =][ R][DP][ (][s, ν]DP[∗] [(][s][)) +] b _PDP (s[′]|s, νDP[∗]_ [(][s][))][V][ ν]DPDP[∗] [((][s][′][)] (15)

_sX[′]∈S_

Therefore, _VD[ν][′]_ [(][s][) =][ V][ ν]DPDP[∗] [(][s][)][,][ ∀][s][ ∈S][.]

Then we have
_νDP_ _DP_

[b] _VD[ν][′]_ [(][s][0][)][ ≤] _V[b]_ _[ν][∗]_ (s0) = VDP [(][s][0][)][ < V]DP[ ν][∗] [(][s][0][) =][ b]VD[ν][′] [(][s][0][)] (16)

which gives _VD[ν][′]_ [(][s][0][)][ <][ b]VD[ν][′] [(][s][0][)][, so there is a contradiction.][b]

b

[b]

Combining the results of Proposition 24 and Lemma 21, for a deterministic victim, the optimal
policy in D-PAMDP gives an optimal adversary for the victim.

D.3 OPTIMALITY OF HEURISTIC-BASED ATTACKS

There are many existing methods of finding adversarial state perturbations for a fixed RL policy,
most of which are solving some optimization problems defined by heuristics. Although these methods are empirically shown to be effective in many environments, it is not clear how strong these
adversaries are in general. In this section, we carefully summarize and categorize existing heuristic
attack methods into 4 types, and then characterize their optimality in theory.

D.3.1 _TYPE I - MINIMIZE THE BEST (MINBEST)_

A common idea of evasion attacks in supervised learning is to reduce the probability that the learner
selects the “correct answer” Goodfellow et al. (2015). Prior works Huang et al. (2017); Kos & Song
(2017); Korkmaz (2020) apply a similar idea to craft adversarial attacks in RL, where the objective
is to minimize the probability of selecting the “best” action, i.e.,
_h[MinBest]_ _∈_ argminh∈Hϵ _πh(a[+]|s), ∀s ∈S_ (I)
where a[+] is the “best” action to select at state s. Huang et al.Huang et al. (2017) define a[+] as
argmaxa∈AQ[π](s, a) for DQN, or argmaxa∈Aπ(a|s) for TRPO and A3C with a stochastic π. Since
the agent’s policy π is usually well-trained in the original MDP, a[+] can be viewed as (approximately)
the action taken by an optimal deterministic policy π[∗](s).

**Lemma 25 (Optimality of MinBest). Denote the set of optimal solutions to objective (I) as**
_H_ _[MinBest]. There exist an MDP M and an agent policy π, such that H_ _[MinBest]_ _does not contain an_
_optimal adversary h[∗], i.e., H_ _[MinBest]_ _∩_ _Hϵ[∗]_ [=][ ∅][.]

_Proof of Lemma 25. We prove this lemma by constructing the following MDP such that for any_
victim policy, there exists a reward configuration in which MinBest attacker is not optimal.

**Figure 7: A simple MDP where MinBest Attacker cannot find the optimal adversary for a given victim policy.**

Here, let r1 = r(s4|s2, a1), r2 = r(s5|s2, a2), r3 = r(s3|s1, a2). Assuming all the other rewards
are zero, transition dynamics are deterministic, and states s3, s4, s5 are the terminal states. For the


-----

sake of simplicity, we also assume that the discount factor here γ = 1.
Now given a policyr1, r2, r3 such that the following constraints hold: π such that π(a1|s1) = β1 and π(a1|s2) = β2 (β1, β2 ∈ [0, 1]), we could find
_r1 > r2 ⇐⇒_ _Q[π](s1, a1) > Q[π](s1, a2)_ (17)
_β2r1 + (1 −_ _β2)r2 > r3 ⇐⇒_ _Q[π](s2, a1) > Q[π](s2, a2)_ (18)
_r3 > (β2 −_ _ϵ2)r2 + (1 −_ _β2 + ϵ2)r2 ⇐⇒_ _r3 > Q[π](s1, a1) −_ _ϵ2(r1 −_ _r2)_ (19)
Now we consider the Adv-policy-set

_Bϵ[H]_ [(][π][) =] _π[′]_ _∈_ Π _∥π[′](·|s1) −_ _π(·|s1)∥_ _< ϵ1, ∥π[′](·|s2) −_ _π(·|s2)∥_ _< ϵ2_ _._

Under these three linear constraints, the policy given by MinBest attacker satisfies thatn o
_πhMinBest_ (a1 _s1) = β1_ _ϵ1, and πhMinBest(a1_ _s2) = β2_ _ϵ2. On the other hand, we can find an-_
other admissible policy adversary| _−_ _πh∗_ (a1 _s1|) = β1 + ϵ −1, and πh∗_ (a1 _s2) = β2_ _ϵ2. Now we show_
that V _[π][h][∗]_ (s1) < V _[π][h][MinBest]_ (s1), and thus MinBest attacker is not optimal.| _|_ _−_

_V_ _[π][h][MinBest]_ (s1) = (β1 − _ϵ1)_ (β2 − _ϵ2)r1 + (1 −_ _β2 + ϵ2)r2_ + (1 − _β1 + ϵ1)r3_ (20)

= (β1 − _ϵ1)(hβ2 −_ _ϵ2)r1 + (β1 −_ _ϵ1)(1 −_ _β2 +i_ _ϵ2)r2 + (1 −_ _β1 + ϵ1)r3_ (21)

_V_ _[π][h][∗]_ (s1) = (β1 + ϵ1) (β2 − _ϵ2)r1 + (1 −_ _β2 + ϵ2)r2_ + (1 − _β1 −_ _ϵ1)r3_ (22)

= (β1 + ϵ1)(hβ2 − _ϵ2)r1 + (β1 + ϵ1)(1 −_ _β2 +i_ _ϵ2)r2 + (1 −_ _β1 −_ _ϵ1)r3_ (23)
Therefore,
_V_ _[π][h][∗]_ (s1) − _V_ _[π][h][MinBest]_ (s1) = 2ϵ1(β2 − _ϵ2)r2 + 2ϵ1(1 −_ _β2 + ϵ2)r2 −_ 2ϵ1r3 (24)

= 2ϵ1 (β2 − _ϵ2)r2 + (1 −_ _β2 + ϵ2)r2 −_ _r3_ (25)

_< 0 Because of the constraint (h_ 19) i (26)

D.3.2 _TYPE II - MAXIMIZE THE WORST (MAXWORST)_


Pattanaik et al. (Pattanaik et al., 2018) point out that only preventing the agent from selecting the
best action does not necessarily result in a low total reward. Instead, Pattanaik et al. (Pattanaik et al.,
2018) propose another objective function which maximizes the probability of selecting the worst
action, i.e.,
_h[MaxWorst]_ _∈_ argmaxh∈Hϵ _πh(a[−]|s), ∀s ∈S_ (II)

where a[−] refers to the “worst” action at state s. Pattanaik et al.(Pattanaik et al., 2018) define the
“worst” action as the actions with the lowest Q value, which could be ambiguous, since the Q
function is policy-dependent. If a worst policy π[−] argminπV _[π](s),_ _s_ is available, one
_∈_ _∀_ _∈S_
can use a[−] = argminQ[π][−] (s, a). However, in practice, the attacker usually only has access to the
agent’s current policy π, so it can also choose a[−] = argminQ[π](s, a). Note that these two selections
are different, as the agent’s policy π is usually far away from the worst policy.

**Lemma 26 (Optimality of MaxWorst). Denote the set of optimal solutions to objective (II) as**
_H_ _[MaxWorst], which include both versions of MaxWorst attacker formulations as we discussed above._
_Then there exist an MDP M and an agent policy π, such that H_ _[MaxWorst]_ _contains a non-optimal_
_adversary h[∗], i.e., H_ _[MaxWorst]_ _̸⊂_ _Hϵ[∗][.]_

_Proof of Lemma 26._
**Case I: Using current policy to compute the target action**
We prove this lemma by constructing the MDP in Figure 8 such that for any victim policy, there
exists a reward configuration in which MaxWorst attacker is not optimal.
Here, let r1 = r(s11|s1, a1), r2 = r(s12|s1, a2), r3 = r(s21|s2, a1), r4 = r(s22|s2, a2). Assuming
all the other rewards are zero, transition dynamics are deterministic, and states s11, s12, s21, s22 are
the terminal states. For the sake of simplicity, we also assume that the discount factor here γ = 1.
Now given a policy[0, 1]), consider the Adv-policy-set π such that π(a1|s0) = β0, π(a1|s1) = β1, and π(a2|s2) = β2 (β0, β1, β2 ∈

_Bϵ[H]_ [(][π][) =] _π[′]_ _∈_ Π _∥π[′](·|s1)−π(·|s1)∥_ _< ϵ0, ∥π[′](·|s1)−π(·|s1)∥_ _< ϵ1, ∥π[′](·|s2)−π(·|s2)∥_ _< ϵ2,_
n


-----

**Figure 8: A simple MDP where the first version of MaxWorst Attacker cannot find the optimal adversary for a**
given victim policy.

We could find r1, r2, r3, r4 such that the following linear constraints hold:
_β1r1 + (1 −_ _β1)r2 >β2r3 + (1 −_ _β2)r4 ⇐⇒_ _Q[π](s0, a1) > Q[π](s0, a2)_ (27)
_r1 >r2 ⇐⇒_ _Q[π](s1, a1) > Q[π](s1, a2)_ (28)
_r3 >r4 ⇐⇒_ _Q[π](s2, a1) > Q[π](s2, a2)_ (29)
(β1 − _ϵ1)r1 + (1 −_ _β1 + ϵ1)r2 <(β2 −_ _ϵ2)r3 + (1 −_ _β2 + ϵ2)r4_ (30)
Now, given these constraints, the perturbed policy given by MaxWorst attaker satisfies
_πhMaxWorst_ (a1 _s0) = β0_ _ϵ0, πhMaxWorst_ (a1 _s1) = β1_ _ϵ1, and πhMaxWorst_ (a1 _s2) = β2_ _ϵ2. How-_
ever, consider another perturbed policy| _−_ _π|_ _h∗_ in Adv-policy-set such that − _π|_ _h∗_ (a1 _s0) = −_ _β0 + ϵ0,_
_|_
_πh∗_ (a1 _s1) = β1_ _ϵ1, and πh∗_ (a1 _s2) = β2_ _ϵ2. We will prove that V_ _[π][h][∗]_ (s1) < V _[π][h][MaxWorst]_ (s1),
and thus MaxWorst attacker is not optimal.| _−_ _|_ _−_
On the one hand,

_V_ _[π][h][MaxWorst]_ (s1) =(β0 − _ϵ0)_ (β1 − _ϵ1)r1 + (1 −_ _β1 + ϵ1)r2_ + (1 − _β0 + ϵ0)_ (β2 − _ϵ2)r3 + (1 −_ _β2 + ϵ2)r4_
h i h (31)

=(β0 − _ϵ0)(β1 −_ _ϵ1)r1 + (β0 −_ _ϵ0)(1 −_ _β1 + ϵ1)r2_
+ (1 − _β0 + ϵ0)(β2 −_ _ϵ2)r3 + (1 −_ _β0 + ϵ0)(1 −_ _β2 + ϵ2)r4_ (32)
On the other hand,


_V_ _[π][h][∗]_ (s1) =(β0 + ϵ0) (β1 − _ϵ1)r1 + (1 −_ _β1 + ϵ1)r2_ + (1 − _β0 −_ _ϵ0)_ (β2 − _ϵ2)r3 + (1 −_ _β2 + ϵ2)r4_
h i h (33)

=(β0 + ϵ0)(β1 − _ϵ1)r1 + (β0 + ϵ0)(1 −_ _β1 + ϵ1)r2_
+ (1 − _β0 −_ _ϵ0)(β2 −_ _ϵ2)r3 + (1 −_ _β0 −_ _ϵ0)(1 −_ _β2 + ϵ2)r4_ (34)
Therefore,
_V_ _[π][h][∗]_ (s1) − _V_ _[π][h][MaxWorst]_ (s1) =2ϵ0(β1 − _ϵ1)r1 + 2ϵ0(1 −_ _β1 + ϵ1)r2_
_−_ 2ϵ0(β2 − _ϵ2)r3 −_ 2ϵ0(1 − _β2 + ϵ2)r4_ (35)
_< 0 Because of the constraint (30)_ (36)
**Case II: Using worst policy to compute the target action**

**Figure 9: A simple MDP where the second version of MaxWorst Attacker cannot find the optimal adversary**
for a given victim policy.


-----

Same as before, we construct a MDP where H [MaxWorst] contains a non-optimal adversary. Let
_r1 = r(s1|s0, a1), r2 = r(s2|s0, a2), r3 = r(s3|s0, a3). Assuming all the other rewards are zero,_
transition dynamics are deterministic, and states s1, s2, s3 are the terminal states. For the sake of
simplicity, we also assume that the discount factor here γ = 1.
Let pi be the given policy such that π(a1|s0) = β1 and π(a2|s0) = β2. Now without loss of generality, we assume r1 > r2 > r3 (∗). Then the worst policy π[′] satisfies that π[′](a3|s0) = 1. Consider

the Adv-policy-set Bϵ[H] [(][π][) =] _π[′]_ _∈_ Π _∥π[′](·|s0) −_ _π(·|s0)∥1 < ϵ_ . Then H [MaxWorst] = _π[′]_ _∈_

Π _π[′](a3_ _s0) = (1_ _β1_ _β2) +n_ _ϵ_ . o n
_|_ _−_ _−_

Now consider two policies πh1 _, πh2o_ _H_ [MaxWorst], where πh1 (a1 _s0) = β1, πh1_ (a2 _s0) = β2_ _ϵ,_
_πh2_ (a1 _s0) = β1_ _ϵ, πh2_ (a2 _s0) = β ∈2. Then V_ _[π][h][1]_ (s0) _V_ _[π][h][2]_ (|s0) = ϵ(r1 _r2) >|_ 0. Therefore, −
_πh1_ _H|_ [MaxWorst] _−but it’s not optimal.|_ _−_ _−_
_∈_

D.3.3 _TYPE III - MINIMIZE Q VALUE (MINQ)._

Another idea of attacking Pattanaik et al. (2018); Zhang et al. (2020a) is to craft perturbations such
that the agent selects actions with minimized Q values at every step, i.e.,

_h[MinQ]_ _∈_ argminh∈Hϵ _a_ _[π][h][(][a][|][s][) ˆ]Q[π](s, a), ∀s ∈S_ (III)

_∈A_

where _Q[ˆ] is the approximated Q function of the agent’s original policy. For example, Pattanaik etX_
al.Pattanaik et al. (2018) directly use the agent’s Q network (of policy π), while the Robust SARSA
(RS) attack proposed by Zhang et al.Zhang et al. (2020a) learns a more stable Q network for the
agent’s policy π. Note that in practice, this type of attack is usually applied to deterministic agents
(e.g., DQN, DDPG, etc), then the objective becomes argminh _Hϵ_ _Q[ˆ][π](s, πh(s)),_ _s_ Pattanaik
_∈_ _∀_ _∈S_
et al. (2018); Zhang et al. (2020a); Oikarinen et al. (2020). In this case, the MinQ attack is equivalent
to the MaxWorst attack with the current policy as the target.

**Lemma 27 (Optimality of MinQ). Denote the set of optimal solutions to objective (III) as H** _[MinQ],_
_which include both versions of MinQ attacker formulations as we discussed above. Then there exist_
_an MDP M and an agent policy π, such that H_ _[MinQ]_ _contains a non-optimal adversary h[∗], i.e.,_
_H_ _[MinQ]_ _̸⊂_ _Hϵ[∗][.]_

_Proof of Lemma 26._
**Case I: For a deterministic victim**
In the deterministic case
_h[MinQ]_ argminh _Hϵ_ _Q[ˆ][π](s, πh(s)) = argmaxh_ _Hϵ_ _πh(argminaQ[ˆ][π](s, a)_ _s),_ _s_ (IIID)
_∈_ _∈_ _∈_ _|_ _∀_ _∈S_
In this case, the objective is equivalent to objective (II), thus Lemma 27 holds.

**Case II: For a stochastic victim**
In this case, we consider the MDP in Figure 8 and condition (27) to (30). Then the MinQ objective
gives πhMinQ (a1 _s0) = β0_ _ϵ0, πhMinQ_ (a1 _s1) = β1_ _ϵ1, and πhMinQ_ (a1 _s2) = β2_ _ϵ2._
_|_ _−_ _|_ _−_ _|_ _−_

According to the proof of the first case of Lemma 26, πhMinQ = πhMaxWorst is not an optimal adversary.
Thus Lemma 27 holds.

D.3.4 _TYPE IV - MAXIMIZE DIFFERENCE (MAXDIFF)._

The MAD attack proposed by Zhang et al. (Zhang et al., 2020a) is to maximize the distance between
the perturbed policy πh and the clean policy π, i.e.,
_h[MaxDiff]_ _∈_ argmaxh∈Hϵ DTV[πh(·|s)||π(·|s)], ∀s ∈S (IV)
where TV denotes the total variance distance between two distributions. In practical implementations, the TV distance can be replaced by the KL-divergence, as DTV[πh( _s)_ _π(_ _s)]_

_·|_ _||_ _·|_ _≤_
(DKL[πh( _s)_ _π(_ _s)])[2]. This type of attack is inspired by the fact that if two policies select ac-_

_·|_ _||_ _·|_
tions with similar action distributions on all the states, then the value of the two policies is also
small (see Theorem 5 in Zhang et al. (2020a)).


-----

**Lemma 28 (Optimality of MaxDiff). Denote the set of optimal solutions to objective (IV) as**
_H_ _[MaxDiff]. There exist an MDP M and an agent policy π, such that H_ _[MaxDiff]_ _contains a non-optimal_
_adversary h[∗], i.e., H_ _[MaxDiff]_ _̸⊂_ _Hϵ[∗][.]_

_Proof of Lemma 28. The proof follows from the proof of lemma 25. In the MDP we constructed,_
from the victim policy within Adv-policy-set. However, as we proved inπ[′] = β1 − _ϵ1, πhMinBest_ (a1|s2) = β2 − _ϵ2 is one of the policies that has the maximum KL divergence 25, this is not the optimally_
perturbed policy. Therefore, MaxDiff attacker may not be optimal.

E ADDITIONAL EXPERIMENT DETAILS AND RESULTS

In this section, we provide details of our experimental settings and present additional experimental
results. Section E.1 describes our implementation details and hyperparameter settings for Atari and
MuJoCo experiments. Section E.2 provide additional experimental results, including experiments
with varying budgets (ϵ) in Section E.2.1, more comparison between SA-RL and PA-AD in terms
of convergence rate and sensitivity to hyperparameter settings as in Section E.2.3, robust training in
MuJoCo games with fewer training steps in Section E.2.4, attacking performance on robust models
in Atari games in Section E.2.5, as well as robust training results in Atari games in Section E.2.6.

E.1 IMPLEMENTATION DETAILS

E.1.1 ATARI EXPERIMENTS

In this section we report the configurations and hyperparameters we use for DQN, A2C and ACKTR
in Atari environments. We use GeForce RTX 2080 Ti GPUs for all the experiments.

**DQN Victim** We compare PA-AD algorithm with other attacking algorithms on 7 Atari games.
For DQN, we take the softmax of the Q values Q(s, ·) as the victim policy π(·|s) as in prior
works (Huang et al., 2017). For these environments, we use the wrappers provided by stablebaselines (Hill et al., 2018), where we clip the environment rewards to be −1 and 1 during training
and stack the last 4 frames as the input observation to the DQN agent. For the victim agent, we implement Double Q learning (Hado Van Hasselt, 2016) and prioritized experience replay (Tom Schaul
& Silver, 2016). The clean DQN agents are trained for 6 million frames, with a learning rate 0.00001
and the same network architecture and hyperparameters as the ones used in Mnih et al. (2015). In
addition, we use a replay buffer of size 5 × 10[5]. Prioritized replay buffer sampling is used with
_α = 0.6 and β increases from 0.4 to 1 linearly during training. During evaluation, we execute the_
agent’s policy without epsilon greedy exploration for 1000 episodes.

**A2C Victim** For the A2C victim agent, we also use the same preprocessing techniques and convolutional layers as the one used in Mnih et al. (2015). Besides, values and policy network share
the same CNN layers and a fully-connected layer with 512 hidden units. The output layer is a categorical distribution over the discrete action space. We use 0.0007 as the initial learning rate and
apply linear learning rate decay, and we train the victim A2C agent for 10 million frames. During
evaluation, the A2C victim executes a stochastic policy (for every state, the action is sampled from
the categorical distribution generated by the policy network). Our implementation of A2C is mostly
based on an open-source implementation by Kostrikov Kostrikov (2018).

**ACKTR Adversary** To train the director of PA-AD and the adversary in SA-RL, we use
ACKTR (Wu et al., 2017) with the same network architecture as A2C. We train the adversaries
of PA-AD and SA-RL for the same number of steps for a fair comparison. For the DQN victim, we
use a learning rate 0.0001 and train the adversaries for 5 million frames. For the A2C victim, we
use a learning rate 0.0007 and train the adversaries for 10 million frames. Our implementation of
ACKTR is mostly based on an open-source implementation by Kostrikov Kostrikov (2018).

**Heuristic Attackers** For the MinBest attacker, we following the algorithm proposed by Huang
et al. (2017) which uses FGSM to compute adversarial state perturbations. The MinBest + Mo

-----

mentum attacker is implemented according to the algorithm proposed by Korkmaz (2020), and we
set the number of iterations to be 10, the decaying factor µ to be 0.5 (we tested 0.01, 0.1, 0.5, 0.9
and found 0.5 is relatively better while the difference is minor). Our implementation of the MinQ
attacker follows the gradient-based attack by Pattanaik et al. (2018), and we also set the number of
iterations to be 10. For the MaxDiff attacker, we refer to Algorithm 3 in Zhang et al. (2020a) with
the number of iterations equal to 10. In addition, we implement a random attacker which perturbs
state s to ˜s = s + ϵsign(µ), where µ is sampled from a standard multivariate Gaussian distribution
with the same dimension as s.

E.1.2 MUJOCO EXPERIMENTS

For four OpenAI Gym MuJoCo continuous control environments, we use PPO with the original
fully connected (MLP) structure as the policy network to train the victim policy. For robustness
evaluations, the victim and adversary are both trained using PPO with independent value and policy
optimizers. We complete all the experiments on MuJoCo using 32GB Tesla V100.

**PPO Victim** We directly use the well-trained victim model provided by Zhang et al. (2020a).

**PPO Adversary** Our PA-AD adversary is trained by PPO and we use a grid search of a part
of adversary hyperparameters (including learning rates of the adversary policy network and policy
network, the entropy regularization parameter and the ratio clip ϵ for PPO) to train the adversary as
powerful as possible. The reported optimal attack result is from the strongest adversary among all
50 trained adversaries.

**Other Attackers** For Robust Sarsa (RS) attack, we use the implementation and the optimal RS
hyperparameters from Zhang et al. (2020a) to train the robust value function to attack the victim.
The reported RS attack performance is the best one over the 30 trained robust value functions.

For MaxDiff attack, the maximal action difference attacker is implemented referring to Zhang et al.
(2020a).

For SA-RL attacker, following Zhang et al. (2021), the hyperparameters is the same as the optimal
hyperparameters of vanilla PPO from a grid search. And the training steps are set for different
environments. For the strength of SA-PPO regularization κ, we choose from 1 × 10[−][6] to 1 and
report the worst-case reward.

**Robust Training** For ATLA Zhang et al. (2021), the hyperparameters for both victim policy and
adversary remain the same as those in vanilla PPO training. To ensure sufficient exploration, we run
a small-scale grid search for the entropy bonus coefficient for agent and adversary. The experiment
results show that a larger entropy bonus coefficient allows the agent to learn a better policy for the
continual-improving adversary. In robust training experiments, we use larger training steps in all
the MuJoCo environments to guarantee policy convergence. We train 5 million steps in Hopper,
Walker, and HalfCheetah environments and 10 million steps for Ant. For reproducibility, the final
results we reported are the experimental performance of the agent with medium robustness from 21
agents training with the same hyperparameter set.

E.2 ADDITIONAL EXPERIMENT RESULTS

E.2.1 ATTACKING PERFORMANCE WITH VARIOUS BUDGETS

In Table 1, we report the performance of our PA-AD attacker under a chosen epsilon across different
environments. To see how PA-AD algorithm performs across different values of ϵ’s, here we select
three Atari environments each for DQN and A2C victim agents and plot the performance of PA-AD
under various ϵ’s compared with the baseline attackers in Figure 10. We can see from the figures
that our PA-AD universally outperforms baseline attackers concerning various ϵ’s.

In Table 2, we provide the evaluation results of PA-AD under a commonly unused epsilon in four
MuJoCo experiments (Zhang et al. (2020a; 2021)) to show that PA-AD attacker also has the best
attacking performance compared with other attackers under different ϵ’s in Figure 11.


-----

_·10[4]_


100

80

60

40

20


20

10

0

_−10_

_−20_


|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|Mi Mi Mi Ma PA|nBest nBest nQ xDiff -AD|Mome|ntum|||


MinBest
MinBest Momentum
MinQ
MaxDiff

0.2 0.4 0.6 0.8 1 1.2

|Col1|Col2|MinQ|Col4|
|---|---|---|---|
|||MinQ MaxDiff PA-AD||
|||||

|Col1|Min Max PA-|Q Diff AD|Col4|
|---|---|---|---|
|||||
|||||


MinBest
MinBest Momentum
MinQ
MaxDiff
PA-AD


MinBest
MinBest Momentum
MinQ
MaxDiff
PA-AD


0.2 0.4 0.6 0.8

_ϵ_

**(b) DQN Pong**


0.2 0.4 0.6 0.8



_·10[−][3]_

|Col1|Col2|Max PA-A|Diff D|Col5|
|---|---|---|---|---|
||||||


MinBest
MinBest Momentum
MaxDiff
PA-AD


1

_·10[−][3]_



_·10[−][3]_

|Col1|Col2|MaxDiff PA-AD|Col4|
|---|---|---|---|
|||||


MinBest
MinBest Momentum
MaxDiff
PA-AD


1

_·10[−][3]_


_ϵ_ _·10[−][3]_

**(c) DQN RoadRunner**

|Col1|M PA|axDiff -AD|Col4|
|---|---|---|---|
|||||


MinBest
MinBest Momentum
MaxDiff
PA-AD


0 0.2 0.4 0.6 0.8


**(a) DQN Boxing**

MinBest
MinBest Momentum
MaxDiff
PA-AD

0.2 0.4 0.6 0.8


400

300


20


1,500

1,000


10

0

_−10_

_−20_


200

100


500


0.2 0.4 0.6 0.8

_ϵ_

**(e) A2C Breakout**



_·10[−][2]_


**(d) A2C Pong**


**(f) A2C Seaquest**


**Figure 10: Comparison of different attack methods against DQN and A2C victims in Atari w.r.t. different**
budget ϵ’s.


6,000

4,000

2,000

0

_−2,000_

_−4,000_


|Col1|Col2|Col3|Ma Rob|xDiff ust Sars|
|---|---|---|---|---|
||||SA- PA-|RL AD|
||||||
||||||


0.02 0.04 0.06 0.08 0.10 0.12 0.14



5,000

4,000

3,000

2,000

1,000


3,000

2,000


1,000

|Col1|Col2|Col3|Col4|M R|axDiff obust Sars|
|---|---|---|---|---|---|
|||||S PA|A-RL -AD|
|||||||
|||||||

|Col1|Col2|Col3|MaxDiff Robust Sa|
|---|---|---|---|
||||SA-RL PA-AD|
|||||
|||||
|||||


0.02 0.04 0.06 0.08 0.10 0.12 0.14

_ϵ_

**(b) PPO Walker2d**


MaxDiff
Robust Sarsa
SA-RL
PA-AD

0.0500.0750.1000.1250.1500.1750.2000.2250.250

_ϵ_

**(c) PPO Ant**


**(a) PPO Hopper**


**Figure 11: Comparison of different attack methods against PPO victims in MuJoCo w.r.t. different budget ϵ’s.**


-----

E.2.2 HYPERPARAMETER TEST

In our Actor-Director Framework, solving an optimal actor is a constraint optimization problem.
Thus, in our algorithm, we instead use Lagrangian relaxation for the actor’s constraint optimization.
In this section, we report the effects of different choices of the relaxation hyperparameter λ on
the final performance of our algorithm. Although we set λ by default to be 1 and keep it fixed
throughout all of the other experiments, here we find that in fact, difference choice of λ has
only minor impact on the performance of the attacker. This result demonstrates that our PA-AD
algorithm is robust to different choices of relaxation hyperparameters.

**Table 4: Performance of PA-AD across difference choices of the relaxation hyperparameter λ**


|Col1|Pong|Boxing|
|---|---|---|
|Nature Reward|21 0 ±|96 4 ±|
|λ = 0.2|19 2 − ±|16 12 ±|
|λ = 0.4|18 2 − ±|17 12 ±|
|λ = 0.6|20 2 − ±|19 15 ±|
|λ = 0.8|19 2 − ±|14 12 ±|
|λ = 1.0|19 2 − ±|15 12 ±|
|λ = 2.0|20 1 − ±|21 15 ±|
|λ = 5.0|20 1 − ±|19 14 ±|


**(a) Atari**


|Col1|Ant|Walker|
|---|---|---|
|Nature Reward|5687 758 ±|4472 635 ±|
|λ = 0.2|2274 632 − ±|897 157 ±|
|λ = 0.4|2239 716 − ±|923 132 ±|
|λ = 0.6|2456 853 − ±|954 105 ±|
|λ = 0.8|2597 662 − ±|872 162 ±|
|λ = 1.0|2580 872 − ±|804 130 ±|
|λ = 2.0|2378 794 − ±|795 124 ±|
|λ = 5.0|2425 765 − ±|814 140 ±|


**(b) Mujoco**


E.2.3 EMPIRICAL COMPARISON BETWEEN PA-AD AND SA-RL

In this section, we provide more empirical comparison between PA-AD and SA-RL. Note that PAAD and SA-RL are different in terms of their applicable scenarios: SA-RL is a black-box attack
methods, while PA-AD is a white-box attack method. When the victim model is known, we can see
that by a proper exploitation of the victim model, PA-AD demonstrates better attack performance,
higher sample and computational efficiency, as well as higher scalability. Appendix F.2 shows detailed theoretical comparison between SA-RL and PA-AD.

**PA-AD has better convergence property than SA-RL. In Figure 12, we plot the learning curves**
of SA-RL and PA-AD in the CartPole environment and the Ant environment. Compared with SARL attacker, PA-AD has a higher attacking strength in the beginning and converges much faster. In
Figure 12b, we can see that PA-AD has a “warm-start” (the initial reward of the victim is already
significantly reduced) compared with SA-RL attacker which starts from scratch. This is because
PA-AD always tries to maximize the distance between the perturbed policy and the original victim
policy in every step according to the actor function (G). So in the beginning of learning, PA-AD
works similarly to the MaxDiff attacker, while SA-RL works similarly to a random attacker. We also
note that although PA-AD algorithm is proposed particularly for environments that have state spaces
much larger than action spaces, in CartPole where the state dimensions is fewer than the number of
actions, PA-AD still works better than SA-RL because of the distance maximization.


**(a) Learning curve of SA-RL and PA-AD attacker**
against an A2C victim in CartPole.


**(b) Learning curve of SA-RL and PA-AD attacker**
against a PPO victim in Ant.


**Figure 12: Comparison of convergence rate between SA-RL and PA-AD in Ant and Cartpole. Results are**
averaged over 10 random seeds.


-----

**PA-AD is more computationally efficient than SA-RL. Our experiments in Section 6 show that**
PA-AD converges to a better adversary than SA-RL given the same number of training steps, which
verifies the sample efficiency of PA-AD. Another aspect of efficiency is based on the computational
resources, including running time and required memory. For RL algorithms, the computation cost
comes from the interaction with the environment (the same for SA-RL and PA-AD) and the policy/value update. If the state space S is higher-dimensional than the action space A, then SA-RL
requires a larger policy network than PA-AD since SA-RL has a higher-dimensional output, and
thus SA-RL has more network parameters than PA-AD, which require more memory cost and more
computation operations. On the other hand, PA-AD requires to solve an additional optimization
problem defined by the actor objective (G) or (GD). In our implementation, we use FGSM which
only requires one-step gradient computation and is thus efficient. But if more advanced optimization algorithms (e.g. PGD) are used, more computations may be needed. In summary, if S is much
larger than A, PA-AD is more computational efficient than SA-RL; if A is much larger than S, SARL is more efficient than PA-AD; if the sizes of S and A are similar, PA-AD may be slightly more
expensive than SA-RL, depending on the optimization methods selected for the actor.

To verify the above analysis, we compare computational training time for training SA-RL and PAAD attackers, which shows that PA-AD is more computationally efficient. Especially on the environment with high-dimensional states like Ant, PA-AD takes significantly less training time than
SA-RL (and finds a better adversary than SA-RL), which quantifies the efficiency of our algorithm
in empirical experiments.

|Method|Hopper Walker2d HalfCheetah Ant|
|---|---|
|SA-RL PA-AD|1.80 1.92 1.76 4.88 1.43 1.46 1.40 3.76|



**Table 5: Average training time (in hours) of SA-RL and PA-AD in MuJoCo environments, using GeForce**
RTX 2080 Ti GPUs. For Hopper, Walker2d and HalfCheetah, SA-RL and PA-AD are both trained for 2 million
steps; for Ant, SA-RL and PA-AD are both trained for 5 million steps

**PA-AD is less sensitive to hyperparameters settings than SA-RL. In addition to better final**
attacking results and convergence property, we also observe that PA-AD is much less sensitive to
hyerparameter settings compared to SA-RL. On the Walker environment, we run a grid search over
216 different configurations of hyperparameters, including actor learning rate, critic learning rate,
entropy regularization coefficient, and clipping threshold in PPO. Here for comparison we plot two
histograms of the agent’s final attacked results across different hyperparameter configurations.

**(a) SA-RL Attacker** **(b) PA-AD Attacker**

**Figure 13: Histograms of victim rewards under different hyperparameter settings of SA-RL and PA-AD on**
Walker.

The perturbation radius is set to be 0.05, for which the mean reward reported by Zhang et al. (2020a)
is 1086. However, as we can see from this histogram, only one out of the 216 configurations of SARL achieves an attacking reward within the range 1000-2000, while in most hyperparameter settings,
the mean attacked return lies in the range 4000-4500. In contrast, about 10% hyperparameter settings


-----

of PA-AD algorithm are able to reduce the reward to 500-1000, and another 10% settings could
reduce the reward to 1000-2000. Therefore, the performance of PA-AD attacker is generally better
and more robust across different hyperparameter configurations than SA-RL.

E.2.4 ROBUST TRAINING EFFICIENCY ON MUJOCO BY PA-ATLA

In the ATLA process proposed by Zhang et al. (2021), one alternately trains an agent and an adversary. As a result, the agent policy may learn to adapt to the specific type of attacker it encounters
during training. In Table 3, we present the performance of our robust training method PA-ATLAPPO compared with ATLA-PPO under different types of attacks during testing. ATLA-PPO uses
SA-RL to train the adversary, while PA-ATLA-PPO uses PA-AD to train the adversary during alternating training. As a result, we can see that ATLA-PPO models perform better under the SA-RL
attack, and PA-ATLA-PPO performs better under the PA-AD attack. However, the advantage of
ATLA-PPO over PA-ATLA-PPO against SA-RL attack is much smaller than the advantage of PAATLA-PPO over ATLA-PPO against PA-AD attack. In addition, our PA-ATLA-PPO models significantly outperform ATLA-PPO models against other heuristic attack methods, and achieve higher
average rewards across all attack methods. Therefore, PA-ATLA-PPO is generally more robust than
ATLA-PPO.

Furthermore, the efficiency of training an adversary could be the bottleneck in the ATLA Zhang
et al. (2021) process for practical usage. Appendix E.2.3 suggests that our PA-AD generally converges faster than SA-RL. Therefore, when the computation resources are limited, PA-ATLA-PPO
can train robust agents faster than ATLA-PPO. We conduct experiments on continuous control environments to empirically show the efficiency comparison between PA-ATLA-PPO and ATLA-PPO.
In Table 6, we show the robustness performance of two ATLA methods with 2 million training steps
for Hopper, Walker and Halfcheetah and 5 million steps for Ant (Compared with results in Table 3,
we have reduced training steps by half or more). It can be seen that our PA-ATLA-PPO models still
significantly outperform the original ATLA-PPO models under different types of attacks. More importantly, our PA-ATLA-PPO achieves higher robustness under SA-RL attacks in Walker and Ant,
suggesting the efficiency and effectiveness of our method.

**Environment** **_ϵ_** **step(million)** **Model** **RewardNatural** **Zhang et al.RS** **(2020a)** **Zhang et al.SA-RL (2021)** **PA-AD(ours)** **Average rewardacross attacks**


ATLA-PPO 1763 818 1349 174 1172 344 **477** **30** 999.3
**Hopper** 0.075 2 _±_ _±_ _±_ **_±_**

**PA-ATLA-PPO** 2164 ± 121 1720 ± 490 1119 ± 123 **1024 ± 188** 1287.7

ATLA-PPO 3183 842 2405 529 2170 1032 **516** **47** 1697.0
**Walker** 0.05 2 _±_ _±_ _±_ **_±_**

**PA-ATLA-PPO** 3206 ± 445 2749 ± 106 2332 ± 198 **1072 ± 247** 2051.0

ATLA-PPO 4871 112 3781 645 3493 372 **856** **118** 2710.0
**Halfcheetah** 0.15 2 _±_ _±_ _±_ **_±_**

**PA-ATLA-PPO** 5257 ± 94 4012 ± 290 3329 ± 183 **1670 ± 149** 3003.7

ATLA-PPO 3267 51 3062 149 2208 56 **18** **100** 1750.7
**Ant** 0.15 5 _±_ _±_ _±_ **_−_** **_±_**

**PA-ATLA-PPO** 3991 ± 71 3364 ± 254 2685 ± 41 **2403 ± 82** 2817.3

**Table 6: Average episode rewards ± standard deviation of robust models with fewer training steps under**
different evasion attack methods. Results are averaged over 50 episodes. We bold the strongest attack in each
row. The gray cells are the most robust agents with the highest average rewards across all attacks.

E.2.5 ATTACKING ROBUSTLY TRAINED AGENTS ON ATARI

In this section, we show the attack performance of our proposed algorithm PA-AD against DRL
agents that are trained to be robust by prior works (Zhang et al., 2020a; Oikarinen et al., 2020) in
Atari games.

Zhang et al. (2020a) propose SA-DQN, which minimizes the action change under possible state
perturbations within ℓp norm ball, i.e., to minimize the extra loss

DQN(θ) := max max _s, a)_ _Qθ (ˆs, a[∗](s)),_ _c_ (37)
_R_ _sˆ_ _B(s)_ _a[max]=a[∗]_ _[Q][θ][(ˆ]_ _−_ _−_

_s_  _∈_ _̸_ 

X

where θ refers to the Q network parameters, a[∗](s) = argmaxaQθ(a _s), and c is a small constant._
_|_
Zhang et al. (2020a) solve the above optimization problem by a convex relaxation of the Q network,
which achieves 100% action certification (i.e. the rate that action changes with a constrained state
perturbation) in Pong and Freeway, over 98% certification in BankHeist and over 47% certification
in RoadRunner under attack budget ϵ = 1/255.


-----

**MinBest +**
**Momentum**

**Korkmaz (2020)**


**Natural** **MinBest**
**Environment** **_ϵ_** **Random**
**Reward** **Huang et al. (2017)**


**MinQ** **MaxDiff** **PA-AD**

**Pattanaik et al. (2018)** **Zhang et al. (2020a)** **(ours)**


**RoadRunner 46440 ± 5797**
**SA-DQN** **BankHeist** 1237 ± 11


2551 [45032][ ±][ 7125] 40422 ± 8301 43856 ± 5445 42790 ± 8456 45946 ± 8499 **38652 ± 6550**

2551 1236 ± 12 1235 ± 15 1233 ± 17 1237 ± 14 1236 ± 13 1237 ± 14

2551 [41584][ ±][ 8351] 41824 ± 7858 42330 ± 8925 40572 ± 9988 42014 ± 8337 **38214 ± 9119**

2553 [23766][ ±][ 6129] 9808 ± 4345 35598 ± 8191 39866 ± 6001 18994 ± 6451 **1366 ± 3354**

2551 1037 ± 103 991 ± 105 **988 ± 102** 1021 ± 96 1042 ± 112 999 ± 100

2553 1011 ± 130 801 ± 114 460 ± 310 842 ± 33 1023 ± 110 **397 ± 172**

2551 [30828][ ±][ 7297] 31296 ± 7095 31132 ± 6861 30838 ± 5743 32038 ± 6898 **30550 ± 7182**

2553 [30690][ ±][ 7006] 30198 ± 6075 29936 ± 5388 29988 ± 6340 31170 ± 7453 **29768 ± 5892**

2551 847 ± 31 847 ± 33 848 ± 31 848 ± 31 848 ± 31 848 ± 31

2553 848 ± 31 644 ± 158 822 ± 11 842 ± 33 834 ± 30 **620 ± 168**


**RoadRunner 39102 ± 13727**

**BankHeist** 1060 ± 95

**RoadRunner 30854 ± 7281**

**BankHeist** 847 ± 31


**RADIAL**
**-DQN**

**RADIAL**
**-A3C**


**Table 7: Average episode rewards ± standard deviation of SA-DQN, RADIAL-DQN, RADIAL-A3C robust**
agents under different evasion attack methods in Atari environments RoadRunner and BankHeist. All attack
methods use 30-step PGD to compute adversarial state perturbations. Results are averaged over 50 episodes.
In each row, we bold the strongest attack, except for the rows where none of the attacker reduces the reward
significantly (which suggests that the corresponding agent is relatively robust).)
Oikarinen et al. (2020) propose another robust training method named RADIAL-RL. By adding a
adversarial loss to the classical loss of the RL agents, and solving the adversarial loss with interval
bound propagation, the proposed RADIAL-DQN and RADIAL-A3C achieve high rewards in Pong,
Freeway, BankHeist and RoadRunner under attack budget ϵ = 1/255 and ϵ = 3/255.

**Implementation of the Robust Agents and Environments.** We directly use the trained SA-DQN
agents provided by Zhang et al. (2020a), as well as RADIAL-DQN and RADIAL-A3C agents provided by Oikarinen et al. (2020). During test time, the agents take actions deterministically. In order
to reproduce the results in these papers, we use the same environment configurations as in Zhang
et al. (2020a) and Oikarinen et al. (2020), respectively. But note that the environment configurations of SA-DQN and RADIAL-RL are simpler versions of the traditional Atari configurations we
use (described in Appendix E.1.1). Both SA-DQN and RADIAL-RL use a single frame instead
of the stacking as 4 frames. Moreover, SA-DQN restricts the number of actions as 6 (4 for Pong)
in each environment, although the original environments have 18 actions (6 for Pong). The above
simplifications in environments can make robust training easier since the dimensionality of the input
space is much smaller, and the number of possible outputs is restricted.

**Attack Methods** In experiments, we find that the robust agents are much harder to attack than
vanilla agents in Atari games, as claimed by the robust training papers (Zhang et al., 2020a; Oikarinen et al., 2020). A reason is that Atari games have discrete action spaces, and leading an agent
to make a different decision at a state with a limited perturbation could be difficult. Therefore, we
use a 30-step Projected Gradient Descent for all attack methods (with step size ϵ/10), including
MinBest (Huang et al., 2017) and our PA-AD which use FGSM for attacking vanilla models. Note
that the PGD attacks used by Zhang et al. (2020a) and Oikarinen et al. (2020) in their experiments
are the same as the MinBest-PGD attack we use. For our PA-AD, we use PPO to train the adversary
since PPO is relatively stable. The learning rate is set to be 5e − 4, and the clip threshold is 0.1.
Note that SA-DQN, RADIAL-DQN and RADIAL-A3C agents all take deterministic actions, so we
use the deterministic formulation of PA-AD as described in Appendix C.1. In our implementation,
we simply use a CrossEntropy loss for the actor as in Equation (38).
_gD(a, s) = argmins′∈Bϵ(s)CrossEntropy(π(s[′]),_ _a)._ (38)

**Experiment Results** In Tableb 7, we reproduce the results reported by b Zhang et al. (2020a)
and Oikarinen et al. (2020), and demonstrate the average rewards gained by these robust agents
under different attacks in RoadRunner and BankHeist. Note that SA-DQN is claimed to be robust to
attacks with budget ϵ = 1/255, and RADIAL-DQN and RADIAL-A3C are claimed to be relatively
robust against up to ϵ = 3/255 attacks. (ℓ is used in both papers.) So we use the same ϵ’s for these
_∞_
agents in our experiments.

It can be seen that compared with vanilla agents in Table 1, SA-DQN, RADIAL-DQN and RADIALA3C are more robust due to the robust training processes. However, in some environments, PA-AD
can still decrease the rewards of the agent significantly. For example, in RoadRunner with ϵ =


-----

3/255, RADIAL-DQN gets 1k+ reward against our PA-AD attack, although RADIAL-DQN under
other attacks can get 10k+ reward as reported by Oikarinen et al. (2020). In contrast, we find that
RADIAL-A3C is relatively robust, although the natural rewards gained by RADIAL-A3C are not as
high as RADIAL-DQN and SA-DQN. Also, as SA-DQN achieves over 98% action certification in
BankHeist, none of the attackers is able to noticeably reduce its reward with ϵ = 1/255.

Therefore, our PA-AD can approximately evaluate the worst-case performance of an RL agent under
attacks with fixed constraints, i.e., PA-AD can serve as a “detector” for the robustness of RL agents.
For agents that perform well under other attacks, PA-AD may still find flaws in the models and
decrease their rewards; for agents that achieve high performance under PA-AD attack, they are very
likely to be robust against other attack methods.

E.2.6 IMPROVING ROBUSTNESS ON ATARI BY PA-ATLA

Note that different from SA-DQN (Zhang et al., 2020a) and RADIAL-RL (Oikarinen et al., 2020)
discussed in Appendix E.2.5, we use the traditional Atari configurations (Mnih et al., 2015) without
any simplification (e.g. disabling frame stacking, or restricting action numbers). We aim to improve the robustness of the agents in original Atari environments, as in real-world applications, the
environments could be complex and unchangeable.

**Baselines** We propose PA-ATLA-A2C by combining our PA-AD and the ATLA framework proposed by Zhang et al. (2021). We implement baselines including vanilla A2C, adversarially trained
A2C (with MinBest (Huang et al., 2017) and MaxDiff (Zhang et al., 2020a) adversaries attacking
50 frames). SA-A2C (Zhang et al., 2020a) is implemented using SGLD and convex relaxations in
Atari environments.

In Table 6, naive adversarial training methods have unreliable performance under most strong attacks
and SA-A2C is ineffective under PA-AD strongest attack. To provide evaluation using different ϵ,
we provide the attack rewards of all robust models with different attack budgets ϵ. Under all attacks with different ϵ value, PA-ATLA-A2C models outperform all other robust models and achieve
consistently better average rewards across attacks. We can observe that our PA-ATLA-A2C training
method can considerably enhance the robustness in Atari environments.

**Model** **RewardNatural** **_ϵ_** **Random** **Huang et al.MinBest (2017)** **Zhang et al.MaxDiff (2020a)** **Zhang et al.SA-RL (2021)** **PA-AD(ours)** **Average rewardacross attacks**


**vanillaA2C** 1228 ± 93 1/2553/255 10641223 ± ± 129 77 697972 ± ± 153 99 1095913 ± ± 164 107 9281132 ± ± 124 30 **284436 ± ± 116 74** 777971..26

**(adv: MinBest Huang et al.A2C** **(2017))** 948 ± 94 3/2551/255 874932 ± ± 69 51 813927 ± ± 32 30 936829 ± ± 11 27 940843 ± ± 103 126 **704521 ± ± 19 72** 887774..82

**(adv: MaxDiff Zhang et al.A2C** **(2020a))** 743 ± 29 3/2551/255 712756 ± ± 109 42 638702 ± ± 133 89 694752 ± ± 115 79 686749 ± ± 110 85 **403529 ± ± 101 45** 626697..66

**SA-A2CZhang et al. (2021)** 1029 ± 152 3/2551/255 1054985 ± ± 47 31 786902 ± ± 52 89 1070923 ± ± 52 42 9721067 ± ± 126 18 **644836 ± ± 153 70** 862985..08

**PA-ATLA-A2C(ours)** 1076 ± 56 1/2553/255 10551026 ± ± 204 78 842957 ± ± 154 78 1069967 ± ± 82 94 1045976 ± ± 159 143 **757862 ± ± 132 106** 997913..66

**Table 8: Average episode rewards ± standard deviation over 50 episodes of A2C, A2C with adv. training,**
SA-A2C and our PA-ATLA-A2C robust models under different evasion attack methods in Atari environment
BankHeist. In each row, we bold the strongest attack. The gray cells are the most robust agents with the
highest average rewards across all attacks.

F ADDITIONAL DISCUSSION OF OUR ALGORITHM

F.1 OPTIMALITY OF OUR RELAXED OBJECTIVE FOR STOCHASTIC VICTIMS

**Proof of Concept: Optimality Evaluation in A Small MDP**

We implemented and tested heuristic attacks and our PA-AD in the 2-state MDP example used in
Appendix B.6, and visualize the results in Figure 14. For simplicity, assume the adversaries can
perturb only perturb π at s1 within a ℓ2 norm ball of radius 0.2. And we let all adversaries perturb
the policy directly based on their objective functions. As shown in Figure 14a, all possible ˜π(s1)’s
form a disk in the policy simplex, and executing above methods, as well as our PA-AD, leads to 4
different policies on this disk. All these computed policy perturbations are on the boundary of the
policy perturbation ball, justifying our Theorem 4.


-----

**(a)** **(b)** **(c)**

**Figure 14: Comparison of the optimality of different adversaries. (a) The policy perturbation generated for s1**
by all attack methods. (b) The values of corresponding policy perturbations. (c) A zoomed in version of (b),
where the values of all possible policy perturbations are rendered. Our method finds the policy perturbation
that achieves the lowest reward among all perturbations.

As our theoretical results suggest, the resulted value vectors lie on a line segment shown in Figure 14b and a zoomed in version Figure 14c, where one can see that MinBest, MaxWorst and MAD
all fail to find the optimal adversary (the policy with lowest value). On the contrary, our PA-AD
finds the optimal adversary that achieves the lowest reward over all policy perturbations.

**For Continuous MDP: Optimality Evaluation in CartPole and MountainCar**

We provided a comparison between SA-RL and PA-AD in the CartPole environment in Figure 15,
where we can see the SA-RL and PA-AD converge to the same result (the learned SA-RL adversary
and PA-AD adversary have the same attacking performance).

**Figure 15: Learning curve of SA-RL and PA-AD attacker against an A2C victim in CartPole.**

CartPole has a 4-dimensional state space, and contains 2 discrete actions. Therefore since SA-RL
has an optimal formulation, we expect SA-RL to converge to the optimal adversary in a small MDP
like CartPole. Then the result in Figure 15 suggests that our PA-AD algorithm, although with a
relaxation in the actor optimization, also converges to the optimal adversary with even a faster rate
than SA-RL (the reason is explained in Appendix E.2.3).

In addition to CartPole, we also run experiments in MountainCar with a 2-dimensional state space
against a DQN victim. The SA-RL attacker reduces the victim reward to -128, and our PA-AD
attacker reduces the victim reward to -199.45 within the same number of training steps. Note that the
lowest reward in MountainCar is -200, so our PA-AD indeed converges to a near-optimal adversary,
while SA-RL fails to converge to a near-optimal adversary. This is because MountainCar is an
environment with relatively spare rewards. The actor in PA-AD utilizes our Theorem 4 and only
**focuses on perturbations in the outermost boundary, which greatly reduces the exploration**
**burden in solving an RL problem. In contrast, SA-RL directly uses RL algorithms to learn the**
perturbation, and thus it has difficulties in converging to the optimal solution.

F.2 MORE COMPARISON BETWEEN SA-RL AND PA-AD

We provide a more detailed comparison between SA-RL and PA-AD from the following multiple
aspects to claim our contribution.


-----

**1. Size of the Adversary MDP**
Suppose the original MDP has size |S|, |A| for its state space and action space, respectively. Both
PA-AD and SA-RL construct an adversary’s MDP and search for the optimal policy in it. But the
adversary’s MDPs for PA-AD and SA-RL have different sizes.

**PA-AD: state space is of size |S|, action space is of size R[|A|−][1]** for a stochastic victim, or |A| for a
deterministic victim.
**SA-RL: state space is of size |S|, action space is of size |S|.**

**2. Learning Complexity and Efficiency**
When the state space is larger than the action space, which is very common in RL environments, PAAD solves a smaller MDP than SA-RL and thus more efficient. In environments with pixel-based
states, SA-RL becomes computationally intractable, while PA-AD still works. It is also important
to note that the actor’s argmax problem in PA-AD further accelerates the convergence, as it rules out
the perturbations that do not push the victim policy to its outermost boundary. Our experiment and
analysis in Appendix E.2.3 verify the efficiency advantage of our PA-AD compared with SA-RL,
even in environments with small state spaces.

**3. Optimality**
**PA-AD: (1) the formulation is optimal for a deterministic victim policy; (2) for a stochastic victim**
policy, the original formulation is optimal, but in practical implementations, a relaxation is used
which may not have optimality guarantees.
**SA-RL: the formulation is optimal.**
Note that both SA-RL and PA-AD require training an RL attacker, but the RL optimization process
may not converge to the optimal solution, especially in deep RL domains. Therefore, SA-RL and
PA-AD are both approximating the optimal adversary in practical implementations.

**4. Knowledge of the Victim**
**PA-AD: needs to know the victim policy (white-box). Note that in a black-box setting, PA-AD can**
still be used based on the transferability of adversarial attacks in RL agents, as verified by Huang
et al. (2017). But the optimality guarantee of PA-AD does not hold in the black-box setting.
**SA-RL: does not need to know the victim policy (black-box).**
It should be noted that the white-box setting is realistic and helps in robust training:
(1) The white-box assumption is common in existing heuristic methods.
(2) It is always a white-box process to evaluate and improve the robustness of a given agent, for
which PA-AD is the SOTA method. As discussed in our Ethics Statement, the ultimate goal of finding the strongest attacker is to better understand and improve the robustness of RL agents. During
the robust training process, the victim is the main actor one wants to train, so it is a white-box setting. The prior robust training art ATLA (Zhang et al., 2021) uses the black-box attacker SA-RL,
despite the fact that it has white-box access to the victim actor. Since SA-RL does not utilize the
knowledge of the victim policy, it usually has to deal with a more complex MDP and face converging difficulties. In contrast, if one replaces SA-RL with our PA-AD, PA-AD can make good use of
the victim policy and find a stronger attacker with the same training steps as SA-RL, as verified in
our Section 6 and Appendix E.2.6.

**5. Applicable Scenarios**
**SA-RL is a good choice if (1) the action space is much larger than the state space in the original**
MDP, or the state space is small and discrete; (2) the attacker wants to conduct black-box attacks.
**PA-AD is a good choice if (1) the state space is much larger than the state space in the original**
MDP; (2) the victim policy is known to the attacker; (3) the goal is to improve the robustness of
one’s own agent via adversarial training.

In summary, as we discussed in Section 4, there is a trade-off between efficiency and optimality
in evasion attacks in RL. SA-RL has an optimal RL formulation, but empirical results show that
_SA-RL usually do not converge to the optimal adversary in a continuous state space, even in a low-_
_dimensional state space (e.g. see Appendix F.1 for an experiment in MountainCar). Therefore, the_
difficulty of solving an adversary’s MDP is the bottleneck for finding the optimal adversary. Our
PA-AD, although may sacrifice the theoretical optimality in some cases, greatly reduces the size and
the exploration burden of the attacker’s RL problem (can also be regarded as trading some estimation
bias off for lower variance). Empirical evaluation shows our PA-AD significantly outperforms SARL in a wide range of environments.


-----

Though PA-AD requires to have access to the victim policy, PA-AD solves a smaller-sized RL
problem than SA-RL by utilizing the victim’s policy and can be applied on evaluating/improving
the robustness of RL policy. It is possible to let PA-AD work in a black-box setting based on the
transferability of adversarial attacks. For example, in a black-box setting, the attacker can train a
proxy agent in the same environment, and use PA-AD to compute a state perturbation for the proxy
agent, then apply the state perturbation to attack the real victim agent. This is out of the scope of
this paper, and will be a part of our future work.

F.3 VULNERABILITY OF RL AGENTS

It is commonly known that neural networks are vulnerable to adversarial attacks (Goodfellow et al.,
2015). Therefore, it is natural that deep RL policies, which are modeled by neural networks, are
also vulnerable to adversarial attacks (Huang et al., 2017). However, there are few works discussing
the difference between deep supervised classifiers and DRL policies in terms of their vulnerabilities.
In this section, we take a step further and investigate the vulnerability of DRL agents, through a
comparison with standard adversarial attacks on supervised classifiers. Our main conclusion is
that commonly used deep RL policies can be instrinsically much more vulnerable to small-radius
adversarial attacks. The reasons are explained below.

**1. Optimization process**
Due to the different loss functions that RL and supervised learning agents are trained on, the size of
robustness radius of an RL policy is much smaller than that of a vision-based classifier.
On the one hand, computer vision-based image classifiers are trained with cross-entropy loss. Therefore, the classifier is encouraged to make the output logit of the correct label to be larger than the
logits of other labels to maximize the log probability of choosing the correct label. On the other
hand, RL agents, in particular DQN agents, are trained to minimize the Bellman Error instead. Thus
the agent is not encouraged to maximize the absolute difference between the values of different actions. Therefore, if we assume the two networks are lipschitz continuous and their lipschitz constants
do not differ too much, it is clear that a supervised learning agent has a much larger perturbation
radius than an RL agent.

To prove our claim empirically, we carried out a simple experiment, we compare the success rate
of target attacks of a well-trained DQN agent on Pong with an image classifier trained on the
CIFAR-10 dataset with similar network architecture. For a fair comparison, we use the same image
preprocessing technique, which is to divide the pixel values by 255 and no further normalization
is applied. On both the image-classifier and DQN model, we randomly sample a target label other
than the model predicted label and run the same 100-step projected gradient descent (PGD) attack
to minimize the cross-entropy loss between the model output and the predicted label. We observe
that for a perturbation radius of 0.005 (l norm), the success rate of a targeted attack for the
_∞_
image classifier is only 15%, whereas the success rate of a targeted attack for the DQN model is
100%. This verifies our claim that a common RL policy is much more vulnerable to small-radius
adversarial attacks than image classifiers.

**2. Network Complexity**
In addition, we also want to point out that the restricted network complexity of those commonly
used deep RL policies could play an important role here. Based on the claim by Madry et al. (2018),
a neural network with greater capacity could have much better robustness, even when trained with
only clean examples. But for the neural network architectures commonly used in RL applications,
the capacity of the networks is very limited compared to SOTA computer vision applications. For
example, the commonly used DQN architecture proposed in Mnih et al. (2015) only has 3 convolutional layers and 2 fully connected layers. But in vision tasks, a more advanced and deeper structure
(e.g. ResNet has 100 layers) is used. Therefore, it is natural that the perturbation radius need for
attacking an RL agent is much smaller than the common radius studied in the supervised evasion
attack and adversarial learning literature.


-----

