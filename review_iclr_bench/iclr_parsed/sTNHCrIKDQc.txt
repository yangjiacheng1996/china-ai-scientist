# GRAPHON BASED CLUSTERING AND TESTING OF NET## WORKS: ALGORITHMS AND THEORY


**Mahalakshmi Sabanayagam**
Technical University of Munich
maha.sabanayagam@tum.de

**Debarghya Ghoshdastidar**
Technical University of Munich
Munich Data Science Institute
ghoshdas@in.tum.de


**Leena C. Vankadara**
University of T¨ubingen
leena.chennuru-vankadara@uni-tuebingen.de

ABSTRACT


Network-valued data are encountered in a wide range of applications, and pose
challenges in learning due to their complex structure and absence of vertex correspondence. Typical examples of such problems include classification or grouping
of protein structures and social networks. Various methods, ranging from graph
kernels to graph neural networks, have been proposed that achieve some success
in graph classification problems. However, most methods have limited theoretical justification, and their applicability beyond classification remains unexplored.
In this work, we propose methods for clustering multiple graphs, without vertex
correspondence, that are inspired by the recent literature on estimating graphons—
symmetric functions corresponding to infinite vertex limit of graphs. We propose
a novel graph distance based on sorting-and-smoothing graphon estimators. Using
the proposed graph distance, we present two clustering algorithms and show that
they achieve state-of-the-art results. We prove the statistical consistency of both
algorithms under Lipschitz assumptions on the graph degrees. We further study
the applicability of the proposed distance for graph two-sample testing problems.

1 INTRODUCTION

Machine learning on graphs has evolved considerably over the past two decades. The traditional
view towards network analysis is limited to modelling interactions among entities of interest, for instance social networks or world wide web, and learning algorithms based on graph theory have been
commonly used to solve these problems (Von Luxburg, 2007; Yan et al., 2006). However, recent applications in bioinformatics and other disciplines require a different perspective, where the networks
are the quantities of interest. For instance, it is of practical interest to classify protein structures
as enzyme or non-enzyme (Dobson & Doig, 2003) or detect topological changes in brain networks
caused by Alzheimer’s disease (Stam et al., 2007). In this paper, learning from network-valued data
refers to clustering where each network is treated as an entity, as opposed to the traditional network
analysis problems that involve a single network of interactions (Newman, 2003).

Machine learning on network-valued data has been an active area of research in recent years, although most works focus on the network classification problem. The generic approach is to convert
the network-valued data into a standard representation. Graph neural networks are commonly used
for network embedding, that is, finding Euclidean representations of each network that can be further used in standard machine learning models (Narayanan et al., 2017; Xu et al., 2019). In contrast,
graph kernels capture similarities between pairs of networks that can be used in kernel based learning
algorithms (Shervashidze et al., 2011; Kondor & Pan, 2016; Togninalli et al., 2019). In particular,
the graph neural tangent kernel defines a graph kernel that corresponds to infinitely wide graph neural networks, and typically outperforms neural networks in classification tasks (Du et al., 2019). A
more classical equivalent for graph kernels is to define metrics that characterise the distances be

-----

tween pairs of graphs (Bunke & Shearer, 1998), but there has been limited research on designing
efficient graph distances and developing algorithms for clustering network-valued data.

The motivation for this paper stems from two shortcomings in the literature on network-valued data
analysis: first, the efficacy of existing kernels or embeddings have not been studied beyond network classification, and second is the lack of theoretical analysis of these methods, particularly in
the small sample setting. Generalisation error bounds for graph kernel based learning exist (Du
et al., 2019), but these bounds, based on learning theory, are meaningful only when many networks
are available. However, in many applications, one needs to learn from a small population of large
networks and, in such cases, an informative statistical analysis should consider the small sample,
large graph regime. To address this issue, we take inspiration from the recent statistics literature
on graph two-sample testing—given two (populations of) large graphs, the goal is to decide if they
are from same statistical model or not. Although most theoretical studies in graph two-sample testing focus on graph with vertex correspondence (Tang et al., 2017a; Ghoshdastidar & von Luxburg,
2018), some works address the problem of testing graphs on different vertex sets either by defining
distances between graphs (Tang et al., 2017b; Agterberg et al., 2020) or by representing networks
in terms of pre-specified network statistics (Ghoshdastidar et al., 2017). The use of network statistics for clustering network-valued data is studied in Mukherjee et al. (2017). Another fundamental
approach for dealing with graphs of different sizes is graph matching, where the objective is to
determine the vertex correspondence. Graph matching is often solved by formulating it as an optimization problem (Zaslavskiy et al., 2008; Guo et al., 2019) or defining graph edit distance between
the graphs (Riesen & Bunke, 2009; Gao et al., 2010). Although there is extensive research on graph
matching, the efficacy of these methods in clustering network-valued data remains unexplored.

**Contribution and organisation. In this work, we follow the approach of defining meaningful graph**
distances based on statistical models, and use the proposed graph distance in the context of learning
from networks without vertex correspondence. In particular, we propose graph distances based on
graphons. Graphons are symmetric bivariate functions that represent the limiting structure for a
sequence of graphs with increasing number of nodes (Lov´asz & Szegedy, 2006), but can be also
viewed as a nonparametric statistical model for exchangeable random graphs (Diaconis & Janson,
2007; Bickel & Chen, 2009). The latter perspective is useful for the purpose of machine learning
since it allows us to view the multiple graphs as random samples drawn from one or more graphon
models. This perspective forms the basis of our contributions, which are listed below:

**1) In Section 2, we propose a distance between two networks, that do not have vertex correspondence**
and could have different number of vertices. We view the networks as random samples from (unknown) graphons, and propose a graph distance that estimates the L2-distance between the graphons.
The distance is inspired by the sorting-and-smoothing graphon estimator (Chan & Airoldi, 2014).

**2) In Section 3, we present two algorithms for clustering network-valued data based on the proposed**
graph distance: a distance-based spectral clustering algorithm, and a similarity based semi-definite
programming (SDP) approach. We derive performance guarantees for both algorithms under the
assumption that the networks are sampled from graphons satisfying certain smoothness conditions.

**3) We empirically compare the performance of our algorithms with other clustering strategies based**
on graph kernels, graph matching, network statistics etc. and show that, on both simulated and
real data, our graph distance-based spectral clustering algorithm outperforms others while the SDP
approach also shows reasonable performance, and they also scale to large networks (Section 3.3).

**4) Inspired by the success of the proposed graph distance in clustering, we use the distance for graph**
two-sample testing. In Section 4, we show that the proposed two-sample test is statistically consistent for large graphs, and also demonstrate the efficacy of the test through numerical simulation.

We provide further discussion in Section 5 and present the proofs of theoretical results in Appendix.

2 GRAPH DISTANCE BASED ON GRAPHONS

Clustering or testing of multiple networks requires a notion of distance between the networks. In
this section, we present a transformation that converts graphs of different sizes into a fixed size
representation, and subsequently, propose a graph distance inspired by the theory of graphons. We
first provide some background on graphons and graphon estimation. Graphon has been studied in


-----

the literature from two perspectives: as limiting structure for infinite sequence of growing graphs
(Lov´asz & Szegedy, 2006), or as exchangeable random graph model. In this paper, we follow the
latter perspective. A random graph is said to be exchangeable if its distribution is invariant under
permutation of nodes. Diaconis & Janson (2007) showed that any statistical model that generates
exchangeable random graphs can be characterised by graphons, as introduced by Lov´asz & Szegedy
(2006). Formally, a graphon is a symmetric measurable continuous function w : [0, 1][2] _→_ [0, 1]
where w(x, y) can be interpreted as the link probability between two nodes of the graph that are assigned values x and y, respectively. This interpretation propounds the following two stage sampling
procedure for graphons. To sample a random graph G with n nodes from a graphon w, in the first
stage, one samples n variables U1, . . ., Un uniformly from [0, 1] and constructs a latent mapping between the sampled points and the node labels. In the second stage, edges between any two nodes i, j
are randomly added based on the link probability w(Ui, Uj). Mathematically, if we abuse notation
to denote the adjacency matrix by G ∈{0, 1}[n][×][n], we have

iid
_U1, . . ., Un_ _Uniform[0, 1]_ and _Gij_ _Ui, Uj_ _Bernoulli_ (w(Ui, Uj)) for all i < j.
_∼_ _|_ _∼_

We consider problems involving multiple networks sampled independently from the same (or different) graphons. We make the following smoothness assumptions on the graphons.

**Assumption 1 (Lipschitz continuous) A graphon w is Lipschitz continuous with constant L if**


(u − _u[′])[2]_ + (v − _v[′])[2]_ _for every u, v, u[′], v[′]_ _∈_ [0, 1].


_|w(u, v) −_ _w(u[′], v[′])| ≤_ _L_


**Assumption 2 (Two-sided Lipschitz degree) A graphon w has two-sided Lipschitz degree with**
1
_constants λ1, λ2 > 0 if its expected degree function g, defined by g(u) =_ 0 _[w][(][u, v][)d][v][, satisfies]_

_λ2_ _u_ _u[′]_ _g(u)_ _g(u[′])_ _λ1_ _u_ _u[′]_ _for every u, uR_ _[′]_ [0, 1].
_|_ _−_ _| ≤|_ _−_ _| ≤_ _|_ _−_ _|_ _∈_

One of the challenges in graphon estimation is due to the issue of non-identifiability, that is, different graphon functions w can generate the same random graph model. In particular, two graphons w
and w[′] generate the same random graph model if they are weakly isomorphic—there exist two measure preserving transformations φ, φ[′] : [0, 1] → [0, 1] such that w(φ(u), φ(v)) = w[′](φ[′](u), φ[′](v)).
Moreover, the converse also holds meaning that such transformations are known to be the only
source of non-identifiability (Diaconis & Janson, 2007). This weak isomorphism induces equivalence classes on the space of graphons. Since our goal is only to cluster graphs belonging to random
graph models, we simply make the following assumption on our graphons.

**Assumption 3 (Equivalence classes) Any reference to K graphons, w1, . . ., wK, assumes that, for**
_every i, j, either wi = wj or wi and wj belong to different equivalence classes. Furthermore, with-_
_out loss of generality, we assume that every graphon wi is represented such that the corresponding_
_degree function gi is non-decreasing._

**Remark on the necessity of Assumptions 1–3. Assumption 1 is standard in graphon estimation**
literature (Klopp et al., 2017) since it avoids graphons corresponding to inhomogeneous random
graph models. It is known that two graphs from widely separated inhomogeneous models (in L2distance) are statistically indistinguishable (Ghoshdastidar et al., 2020), and hence, it is essential to
ignore such models to derive meaningful guarantees. Assumption 2 ensures that, under a measurepreserving transformation, the graphon has strictly increasing degree function, which is a canonical
representation of an equivalence class of graphons (Bickel & Chen, 2009). Assumption 3 is needed
since graphons can only be estimated up to measure-preserving transformation. As noted above, it
is inconsequential for all practical purposes but simplifies the theoretical exposition.

**Graph transformation. In order to deal with multiple graphs and measure distances among pairs**
of graphs, we require a transformation that maps all graphs into a common metric space—the space
of allprovides several consistent estimators (Klopp et al., 2017; Zhang et al., 2017), only the histogram n0 × n0 symmetric matrices for some integer n0. While the graphon estimation literature
based sorting-and-smoothing graphon estimator of Chan & Airoldi (2014) can be adapted to meet
the above requirement. We use the following graph transformation, inspired by Chan & Airoldi
(2014). The adjacency matrix G of size n × n is first reordered based on a non-unique permutation
_σ, such that the empirical degree based on this permutation is monotonically increasing. The degree_


-----

sorted adjacency matrix is denoted by G[σ]. It is then transformed to a ‘histogram’ A ∈ R[n][0][×][n][0] as

_h_ _h_

_n_

_Aij = [1]_ _G[σ](i_ 1)h+i1,(j 1)h+j1 _[,][ where][ h][ =]_ and is the floor function. (1)

_h[2]_ _−_ _−_ _n0_ _⌊·⌋_

_i1=1_ _j1=1_  

X X

**Proposed graph distance. Given two directed or undirected graphs G1 and G2 with n1 and n2**
nodes, respectively, we apply the transformation (1) to both the graphs with n0 min _n1, n2_ . We
propose to use the graph distance _≤_ _{_ _}_

_d(G1, G2) = [1]_ _A1_ _A2_ _F,_ (2)

_n0_ _∥_ _−_ _∥_


where A1 and A2 denote the transformed matrices and ∥· ∥F denotes the matrix Frobenius norm.
Proposition 1 shows that, if G1 and G2 are sampled from two graphons, then the graph distance (2) consistently estimates the L2-distance between the two graphons, which is defined as
1 1
_∥w1 −_ _w2∥L[2]_ 2 [=] 0 0 [(][w][1][(][x, y][)][ −] _[w][2][(][x, y][))][2][ d][x][ d][y][.]_
R R

**Proposition 1 (Graph distance is consistent) Let w1 and w2 satisfy Assumptions 1–3. Let G1 and**
_G2 be random graphs with at least n nodes sampled from the graphons w1 and w2, respectively. If_
_n_ _and n0 is chosen such that_ _[n]0[2]_ [log]n _[ n]_ 0, then with high probability (w.h.p.),
_→∞_ _→_

_w1_ _w2_ _L2_ _d(G1, G2)_ = _n10_ _._ (3)
_∥_ _−_ _∥_ _−_ _O_
 

_Proof sketch._ We define a novel technique for approximating the graphon. The proof in Appendix A.1 first establishes that the approximation error is bounded using Assumption 1. Consequently, a relation between approximated graphons and transformed graphs is derived using lemmas
from Chan & Airoldi (2014). Proposition 1 is subsequently proved using the above two results. □

**Remark on Proposition 1 for sparse graphs. The defined sampling procedure for graphon gen-**
erates dense graphs, deviating from the real world sparse networks. To adapt it to sparse graphs,
one may modify the sampling procedure to Gij _Ui, Uj_ _Bernoulli_ (ρ w(Ui, Uj)) where ρ depends
only on n (Olhede & Wolfe, 2014). Under this process, the consistency result in Proposition 1 re-| _∼_
mains unchanged for ρ = Ω( log n/n) (proof in Appendix A.2). This bound cannot be improved

to the expected real world measure where ρ = Ω(log n/n), because of the degree sorting step in (1).

p

Nevertheless, our analysis allows for relatively sparser graphs with strong consistency result.

**Notation. For ease of exposition, Proposition 1 as well as main results are stated asymptotically**
using the standard O(·) and Ω(·) notations, which subsume absolute and Lipschitz constants. We
use “with high probability” (w.h.p.) to state that the probability of an event converges to 1 as n →∞.

3 GRAPH CLUSTERING

We now present the first application of the proposed graph distance (2) in the context of clustering
network-valued data. We are particularly interested in the setting where one needs to cluster a small
population of large graphs, that is, minimum graph size n grows faster than the sample size m. This
scenario is relevant in practice as bioinformatics or neuroscience application often deals with very
few graphs (see real datasets in Section 3.3). Theoretically, this perspective complements guarantees
for (graph) kernels that are applicable only in supervised setting and large sample regime, m →∞.
In contrast, our guarantees are more conclusive for bounded m and large graph size, n →∞.

3.1 DISTANCE BASED SPECTRAL CLUSTERING (DSC)

Given m graphs with adjacency matrices G1, ..., Gm, we propose a distance based clustering algorithm where we apply spectral clustering to an estimated distance matrix. The distance matrix
_D ∈_ R[m][×][m] is computed on all pairs of graphs using the defined estimator function (2), that is
_Dij = d(Gi, Gj). Unlike the standard Laplacian based spectral clustering, which is applicable_
for adjacency or similarity matrices, we use the method suggested by Mukherjee et al. (2017) thatb
b


-----

computes the K leading eigenvectors of _D (corresponding to the K smallest eigenvalues in magni-_
tude) and applies k-means clustering to the rows of the eigenvector matrix resulting in K number
of clusters. We refer to this distance based clustering algorithm as DSC, described in Algorithm 1

[b]
of Appendix. To derive the statistical consistency of DSC, we consider the problem of clustering m
random graphs of potentially different sizes, each sampled from one of K graphons. We establish the
consistency in Theorem 1 by proving that the number of misclustered graphs → 0 asymptotically.

**Theorem 1 (Consistency of DSC) Consider K graphons satisfying Assumptions 1–3, and m ran-**
_dom graphs G1, . . ., Gm, each sampled from one of the K graphons (assume there is at least one_
_wheregraph from each graphon). Define the distance matrix wi and wj are the graphons from which Gi and D G ∈j are generated. LetR[m][×][m]_ _such that D nij be the size of the = ∥wi −_ _wj∥L2_
_smallest graph, andchosen such that_ _[m][2] γ[n]0[2] be then[log][ n]_ _→ K0-th smallest eigenvalue value of, then DSC misclusters at most D O in magnitude. Asγm[2]n[3][2]0_ _graphs w.h.p. n →∞, if n0 is_

 

_Proof sketch. The proof, given in Appendix A.3, uses the Davis-Kahan spectral perturbation theorem_
to bound the error in terms of ∥D[b] − _D∥F, which is further bounded using Proposition 1._ □

While the number of misclustered graphs seem to depend on m[3], we note that there is an inverse
dependence on γ[2] which has dependence on m (see Corollary 1 that illustrates it for a specific case).
Moreover, our focus is on the setting where m = (1) and n, n0, in which case, the error
asymptotically vanishes. It is natural to wonder whether the dependence on O _→∞_ _m and n0 is tight in_
the above bounds. Currently, we do not know the optimal rates, but deriving this would be difficult
due to the strong dependency of entries in _D and slow rate of convergence of the graph distance_
in Proposition 1. The presence of γ in the above clustering error bound makes Theorem 1 less
interpretable. Hence, we also consider the specific case of K = 2 (two graphons) in the following

[b]
result, along with the assumption that equal number of graphs are generated from both graphons.

**Corollary 1 Let w ̸= w[′]** _be two graphons satisfying Assumptions 1–3, and m is a bounded even_
_number. Assume that equal number of graphs are generated from w and w[′]. For any n0 and large_
_enough constantgraphs misclustered by Algorithm 1 goes to zero w.h.p. C such that ∥w −_ _w[′]∥L2 ≥_ _C_ _n[m]0_ _[and][ m][2][n]0[2]n[log][ n]_ _→_ 0 as n →∞, the number of

The corollary implies that given the observed graphs are large enough, and if the choice of n0
is relatively small, n0 ≪ _n/ log n, and the graphons are Ω(_ _n[1]0_ [)][ apart in][ L][2][-distance, then the]

clustering is consistent. Intuitively, it can be understood that if we condense large graphs to a small

p

representation (small n0), then the clusters can be identified only if the models are quite dissimilar.

3.2 SIMILARITY BASED SEMI-DEFINITE PROGRAMMING (SSDP)


We propose another algorithm for clustering m graphs based on similarity between pairs of graphs.
The pairwise similarity matrix _S ∈_ R[m][×][m] is computed by applying Gaussian kernel on the distance
between the graphs, that is _Sij = exp_ _σiσj_, where σ1, . . ., σn are parameters. For theo_−_ _[d][(][G][i][,G][j]_ [)]

retical analysis, we assume σ1 =[b] _. . . = σn is fixed, but in experiments, the parameters are chosen_
adaptively. We use the following semi-definite program (SDP) (Yan et al., 2018; Perrot et al., 2020)[b]
to find membership of the observed graphs. Let X ∈ R[m][×][m] be the normalised clustering matrix,
that is Xij = 1/|C| if i and j belong to cluster C, and 0 otherwise. Then, SDP for estimating X is:

max trace(SX[b] ) s.t. X 0, X 0, X1 = 1, trace(X) = K, (4)
_X_ _≥_ _⪰_

where X ≥ 0, X ⪰ 0 ensure that X is a non-negative, positive semi-definite matrix, and 1 denotes
the vector of all ones. We denote the optimal X from the SDP as _X. Once we have_ _X, we apply_
standard spectral clustering on _X to obtain a clustering of the graphs. We refer to this algorithm as_
SSDP, described in Algorithm 2 of Appendix. We present strong consistency result for SSDP below.[b] [b]

[b]

**Theorem 2 (Consistency of SSDP) Consider K graphons, w1, . . ., wK, satisfying Assumptions 1–**
_3, and m random graphs, each sampled from one of the K graphons. Let n be the size of the smallest_
_graph. As n →∞, if n0 is chosen such that_ _[m][2][n]0[2]n[log][ n]_ _→_ 0 and minl=l[′][ ∥][w][l][ −] _[w][l][′]_ _[∥][L][2][ = Ω]_ _nm0_ _, then_

_̸_

_the number of graphs misclustered by SSDP is zero w.h.p._  


-----

_Proof sketch. The proof in Appendix A.4 adapts Perrot et al. (2020, Proposition 1) to the present_
setting and combines it with Proposition 1 to derive the stated condition for zero error. □

Theorem 2 is slightly stronger than Theorem 1, or Corollary 1, since SSDP achieve a zero clustering
error for large enough graphs. This theoretical merit of SDP over spectral clustering is known in the
statistics literature. Similar to Corollary 1, the choice of n0 is important such that it does not violate
the minimum L2-distance condition in the theorem to ensure consistency.

**Remark on the knowledge of K. Above discussions assume that the number of clusters K is**
known, which is not necessarily the case in practice. To tackle this issue, one can estimate K using
Elbow method (Thorndike, 1953) or approach from Perrot et al. (2020),and then use it as input in
our algorithms, DSC and SSDP. One can modify the SDP (4) and Theorem 2 to the case where K
is adaptively estimated. However, we found the corresponding algorithm, adapted from Perrot et al.
(2020), to be empirically unstable in the present context. Hence, the knowledge of K is assumed
in the following experiments, which also allows the efficacy of the proposed algorithms and graph
distance to be evaluated without the error induced by incorrect estimation of K.

3.3 EXPERIMENTAL ANALYSIS

In this section, we evaluate the performance of our algorithms DSC and SSDP, both in terms of
accuracy and computational efficacy. We measure the performance of the algorithms in terms of
error rate, that is, the fraction of misclustered graphs by using the source of the graphs as labels.
Since clustering provides labels up to permutation, we use the Hungarian method (Kuhn, 1955) to
match the labels. The performance can also be measured in terms of Adjusted Rand Index (results
in Appendix C.3). We use simulated and real datasets for evaluation and obtain all our experimental
[results using Tesla K80 GPU instance with 12GB memory from Google. Code available in Github.](https://github.com/mahalakshmi-sabanayagam/Clustering-Testing-Networks)

**Simulated data.** We generate graphs of varied sizes from four graphons, W1(u, v) = uv,
_W2(u, v) = exp_ max(u, v)[0][.][75], W3(u, v) = exp 0.5 (min(u, v) + u[0][.][5] + v[0][.][5]) and
_−_ _−_ _∗_
_W4(u, v) =_ _u_ _v_ . The simulated graphs are dense and the graph sizes are controlled to study
_|_ _−_ | 
how algorithms scale. Their corresponding L2 distances between pairs of graphons is shown later
in Figure 3 and the heatmap of the graphons are visualised in Figure 4 in Appendix.

**Real data. We use datasets from two contrasting domains: small molecule datasets from Bioin-**
formatics and large network datasets from Social Networks. We use Proteins (Borgwardt et al.,
2005), KKI (Pan et al., 2016), OHSU (Pan et al., 2016) and Peking 1 (Pan et al., 2016) datasets from
Bioinformatics, and Facebook Ct1 (Oettershagen et al., 2020), Github Stargazers (Rozemberczki
et al., 2020), Deezer Ego Nets (Rozemberczki et al., 2020) and Reddit Binary (Yanardag & Vishwanathan, 2015) datasets from Social Networks. We sub-sample a few graphs from each dataset by
setting a minimum number of nodes to validate clustering small number of large graphs (small m,
large n). The number (#graphs) and size (#nodes) of the graphs are listed in Figure 1 tables. We
consider all combinations of the datasets for three and four clusters in both the domains separately.

**Choice of n0 and σi. As noted in our algorithms DSC and SSDP, n0 is an input parameter. Theo-**
rems 1 and 2 show the choice of n0 = O _n/log n_ . Hence, we set n0 = _n/ log n where n is the_

minimum number of nodes. In Appendix C.2, we use simulated data to show that the above choice

 p  p

of n0 is reasonable (if not the best) for both DSC and SSDP. Furthermore, the similarity matrix _S_
in SSDP is computed using parameters σi and we set σi = d(Gi, G5nn) where G5nn is the fifth
nearest neighbour of Gi. Hence, apart from knowledge of K, our algorithms are parameter-free.

[b]

**Performance comparison with existing methods. We compare our algorithms with a range of**
approaches for measuring similarity or distance among multiple networks. Most methods discussed
below provide a kernel or distance matrix to which we apply spectral clustering to obtain the clusters:

1) Network Clustering based on Log-Moments (NCLM) is an embedding based clustering strategy
for different sized graphs (Mukherjee et al., 2017) that is based on network statistics called log
moments. Log moments for a graph with adjacency matrix A and number of nodes n is obtained by
(log (m1(A)), log (m2(A)), . . ., log (mJ (A))) where mi(A) = trace(A/n)[i] and J is a parameter.

2) Wasserstein Weisfeiler-Lehman Graph Kernels (WWLGK) is a recent graph kernel that is based
on the Wasserstein distance between the node feature vector distributions of two graphs proposed
by Togninalli et al. (2019).


-----

0.6

0.4

0.2

0.0

0.6

0.4

0.2

0.0

0.6

0.4

0.2

0.0


DSC (ours)
SSDP (ours)
NCLM
NCMMD


WWLGK
GNTK
NCGMM
Time Limit Exceeded


TLE


W1, W2, W3 W2, W3, W4 W1, W3, W4 W1, W2, W4 W1, W2, W3, W4

B1, B2, B3 B1, B3, B4 B1, B2, B4 B2, B3, B4 B1, B2, B3, B4

TLE TLE TLE TLE TLE


S1, S2, S4 S1, S2, S3 S1, S3, S4 S2, S3, S4 S1, S2, S3, S4


Bioinformatics #graphs #nodes

B1 Proteins 7 [273, 620]

B2 KKI 7 [62, 90]

B3 OHSU 9 [140, 171]

B4 Peking_1 7 [86, 134]

Social Networks #graphs #nodes

S1 Facebook_Ct1 10 [100, 100]

S2 Github_Stargazers 8 [951, 957]

S3 Deezer_Ego_Nets 10 [208, 363]

S4 Reddit_Binary 12 [3219, 3782]


Figure 1: Evaluation of DSC and SSDP with other methods. (row 1) Results on simulated data.
**(rows 2 and 3) Results on real data from Bioinformatics and Social Networks, respectively. DSC**
outperforms in majority of the cases. Tables in rows 2 and 3 show details of the considered datasets.

3) Graph Neural Tangent Kernel (GNTK) is another graph kernel that describes infinitely wide graph
neural networks derived by Du et al. (2019). Both WWLGK and GNTK provide state-of-the-art
performance in graph classification with GNTK outperforming most graph neural networks.

4) Network Clustering algorithm based on Maximum Mean Discrepancy (NCMMD) considers a
graph metric (MMD) to cluster the graphs. MMD distance between random graphs is proposed
as an efficient test statistic for random dot product graphs (Agterberg et al., 2020). We compute
MMD between the graphs that are represented by latent finite dimensional embedding called spectral
adjacency embedding with the dimension r as a parameter.

5) In Network Clustering algorithm based on Graph Matching Metric (NCGMM), we match two
graphs of different sizes by appending null nodes to the small graph (Guo et al., 2019) and compute
Frobenius norm between the matched graphs as their distance. Although the graph metrics (MMD
and graph matching) are for different purposes, we evaluate their efficacy in the context of clustering.

The parameters in the algorithms are n0 and σi in DSC and SSDP, J in NCLM, number of iterations
(#itr) to perform in WWLGK, number of layers (#layer) in graph neural networks for GNTK, r
in NCMMD and none in NCGMM. We fix n0 in our algorithms using the theoretical bound and σi
is set adaptively as discussed, whereas use grid search to tune the parameters for other algorithms.

**Evaluation on simulated data. We sample 10 graphs of varied sizes between 50 and 100 nodes**
from each of the four graphons in Figure 4, and perform the experiments by considering all combinations of three and four clusters of the graphons. Based on the theoretical bound, n0 is fixed
to 5 as n = 50. We report the performance for J = 8, r = 3, #itr = 1 and #layer = 2 as
these produce the best results. The first row of Figure 1 shows the average performance of the algorithms computed over 5 independent runs. We observe that our algorithm DSC outperforms all
the other algorithms, achieving nearly zero error in all cases, and SSDP also performs competitively
by standing second or third best. The graph kernels, WWLGK and GNTK, and the graph metric
based method NCGMM typically do not perform well. NCMMD either performs very well or quite
poorly. We sample small graphs since otherwise GNTK cannot run due to memory requirement for
dense large graphs and NCGMM has high computation time. Appendix C.4 includes evaluation of
the algorithms except GNTK and NCGMM on larger graphs, where we observe similar behaviour.

**Evaluation on real data. We consider all combinations of three and four clusters of both Bioinfor-**
matics and Social Networks separately. The second and third rows of Figure 1 show the performance
with n0 = 30, J = 8, r = 3, #itr = 1 and #layer = 2, and the upper limit of 7200 seconds (2
hours) as running time of algorithms. We observe DSC outperforms other algorithms by a large margin in majority of the combinations, while in the other combinations like {Proteins,KKI,Peking 1},
DSC performs well with a very small margin to the best performing one. Although NCLM and
GNTK compare favorably in Social Networks datasets, they typically have high error rate in Bioinformatics or simulated datasets, suggesting that they could be well suited for large networks, whereas


-----

DSC is more versatile and suitable for all networks. SSDP performs moderately on real data, but it
achieves the smallest error in some cases, implying that it is suited for certain types of networks.

**Computation time comparison. Figure 2 shows the time (measured in seconds) taken by each al-**
gorithm for four clusters case, plotted in log scale. Appendix C.5 illustrates similar behavior in three
clusters case as well. Our algorithms, DSC and SSDP, perform competitively with respect to time as
well. In addition, it scales effectively for large graphs unlike other algorithms. It is worth noting that

although NCLM takes lesser time than DSC and SSDP for small
graphs, it takes longer for large social networks datasets, thus SSDP (ours) WWLGK TLE Time Limit
favoring our methods in terms of both accuracy and scalability. 8 NCLM GNTK Exceeded
Graph matching based algorithm, NCGMM, has severe scalabil- 6
ity issue showing the inapplicability of such methods to learning 4 TLE
problems. We also evaluate the scalability of all algorithms by Log Time (s) 2
measuring the time for clustering different sets of varied sized 0
graphs from graphons W1, W2, W3 and W4. Detailed discussion W1, W2, W3, W4 B1, B2, B3, B4 S1, S2, S3, S4

DSC (ours) NCMMD NCGMM
SSDP (ours) WWLGK TLE Time Limit
NCLM GNTK Exceeded

TLE

on high scalability of DSC and SSDP is given in Appendix C.6.

Figure 2: Computation time

4 GRAPH TWO-SAMPLE TESTING


Inspired by the remarkable performance of our graph distance (2) in clustering, we analyse its applicability in graph two-sample testing. Two-sample testing is usually studied in the large sample case
_m →∞, and several nonparametric tests are known that could also be applied to graphs. However,_
it is also relevant to study the small sample setting in graphs, particularly m = 2, that is, whether
two large graphs are statistically identical or not (Ghoshdastidar et al., 2020; Agterberg et al., 2020).

We consider the following formulation of the graph two-sample problem, stated under the assumption that the graphs are sampled from graphons. Given two random graphs, G1 sampled from some
model (here, graphon w1), and G2 from another model w2, the goal is to determine which of the
following hypothesis is true: H0 : _w1 = w2_ or Ha : _w1_ = w2 : _w1_ _w2_ _L2_ _φ_ for some
_φ > 0. Existing works consider alternative random graph model, such as inhomogeneous Erd˝ {_ _}_ _̸_ _∥_ _−_ _∥_ _≥_ osR´enyi models or random dot product graph models, which are more restrictive. The condition _φ > 0_
is necessary if one only has access to finitely many independent samples (Ghoshdastidar et al., 2020).
A two-sample test T is a binary function of the given samples such that T = 1 denotes that the test
rejects the null hypothesis H0 and T = 0 implies that the test rejects the alternate hypothesis Ha.
The goodness of a two-sample test is measured in terms of the Type-I and Type-II errors, which
denote the probabilities of incorrectly rejecting the null and alternate hypotheses, respectively. The
goal of this section is to show that one can construct a test T that has arbitrarily small Type-I and
Type-II errors. For this purpose, we consider the test

_T : I {d(G1, G2) ≥_ _ξ}_ (5)

for some ξ > 0, where I{·} is the indicator function and d(G1, G2) is the proposed graph distance
for some choice of integer n0. We state the following theoretical guarantee for the test T .

_andTheorem 3Type-II errors of the test G2 ∼_ _Assume that the graphonsw2 have at least T in n (5) nodes. As go to 0 if w1 n, w[n] →∞0[2]2[log] satisfy Assumptions 1–3, and let the graphsn_ _[ n]_ _, there is a choice of→_ 0 and φ ≥ _n[C]0_ _[, where the constant] ξ such that the Type-I and[ C] G[ depends]1 ∼_ _w1_

_only on the Lipschitz constants._

Theorem 3 shows that the test T in (5) can distinguish between any pair of graphons that have
separation ∥w1 − _w2∥L2 = Ω(1/n0) with arbitrarily small error, if the graphs are large enough._

**Empirical analysis. We validate the consistency result in Theorem 3 by computing power of the**
proposed test T, which measures the probability of rejecting the null hypothesis H0. Intuitively,
power of the test for graphs sampled from same graphons should be small (close to a pre-specified
significance level) since H0 must not be rejected, whereas, it should be close to 1 for graphs sampled
from different graphons. As known in the testing literature, theoretical threshold, ξ in (5), is typically conservative in practice and the rejection/acceptance is decided based on p-values, computed
using bootstrap samples. To this end, we follow the bootstrapping strategy in Ghoshdastidar & von
Luxburg (2018, Boot-ASE algorithm). We also compare the test T by replacing d(G1, G2) in 5


-----

w1

w2

w3

w4


w1

w2

w3

w4


w1

w2

w3

w4


w1

w2

w3

w4


0.00 0.25 0.22 0.38

0.25 0.00 0.10 0.34

0.22 0.10 0.00 0.26

0.38 0.34 0.26 0.00

0.06 1.00 1.00 1.00

1.00 0.06 0.97 1.00

1.00 0.97 0.03 1.00

1.00 1.00 1.00 0.13

0.04 0.99 0.90 0.05

0.99 0.03 0.26 1.00

0.90 0.26 0.05 1.00

0.05 1.00 1.00 0.03

0.57 1.00 1.00 1.00

1.00 0.64 0.47 1.00

1.00 0.47 0.51 1.00

1.00 1.00 1.00 0.77

w1 w2 w3 w4 w1 w2 w3 w4 w1 w2 w3 w4 w1 w2 w3 w4

L2 distance Proposed Distance Log Moments MMD

Figure 3: (left) L2 distance between the graphons W1, W2, W3 and W4. (other plots) Average
power of the test (5) for a graph pair of sizes 100 and 200, sampled from every pair of graphons.

with two other statistics, log moments (Mukherjee et al., 2017) and MMD, an efficient test statistics
for random dot product graphs (Agterberg et al., 2020). We perform the experiment by sampling
graphonsgraphs G W1 ∼1, Ww12 and, W3 G, W2 ∼4. The power ofw2 of size n T and is computed for 2n, respectively, where n = 100 (thus w1 and n0 = 10 w2 are chosen from from the theoretical bound) and the significance level 0.05, averaged over 500 trials of bootstrapping 100 samples
generated from all pairs of graphons. The plots in Figure 3 show the average power of T with our
proposed distance, log moments and MMD as d(G1, G2), respectively. From the results, it is clear
that T using our distance can distinguish between graphons that are quite close too (smallest L2
distance for W2 and W3), whereas, other test statistics are weak as log moments statistic accepts H0
even when it is wrong (see W1 and W4) and MMD based test rejects it strongly almost always (see
diagonal). Appendix C shows similar result for small and large n, and evaluation on real datasets.

5 CONCLUSION

There has been significant progress in learning on complex data, including network-valued data.
However, much of the theoretical and algorithmic development have been in large sample problems,
where one has access to m →∞ independent samples. Practical applications of network-valued
data analysis often leads to small sample, large graph problems—a setting where the machine learning literature is quite limited. Inspired by graph limits and high-dimensional statistics, this paper
proposes a simple graph distance (2) based on non-parametric graph models (graphons).

Sections 3–4 demonstrate that the proposed graph distance leads to provable and practically effective
algorithms for clustering (DSC and SSDP) as well as two-sample testing (5). Extensive empirical
studies on simulated and real data show that the clustering based on the graph distance (2) outperforms methods based on more complex graph similarities or metrics, both in terms of accuracy
and scalability. Figures 1–2 show that DSC achieves best performance for both small dense graphs
(simulated graphons) as well as large sparse graphs (social networks). On the other hand, popular
machine learning approaches—graph kernels or graph matching—can be computationally expensive
in large graphs and their performance may not improve as n →∞, see WWLGK in Figure 7.

Statistical approaches, such as the proposed clustering algorithms and two-sample test, show better
performance on large graphs (Figures 1, 3 and Appendix C.4). Theorems 1–3 theoretically support
this observation by showing consistency of the clustering and testing methods in the limit of n →∞.
The theoretical results, however, hinge on Assumptions 1–3. We remark that such smoothness and
equivalence assumptions could be necessary for meaningful non-parametric approaches, which is
also supported by the graph testing and graphon estimation literature. Further insights about the
necessity of smoothness assumptions would aid in theoretical and algorithmic development.

The poor performance of graph kernels and graph matching in clustering and small sample problems
calls for further studies on these methods, which have shown success in network classification.
Fundamental research, combining graphon based approaches and kernels, could lead to improved
techniques. Instead of sorting-and-smoothing graphon estimator, other histogram based methods
that reduce graphons to a fixed size by identifying block structures and averaging (Olhede & Wolfe,
2014) can be explored. However, the distance defined on such reduced graphons require alignment
of the blocks between the graphons. While such methods do not require Assumption 2 and allow
sparse graphs, the computationally inefficient alignment step pose a practical challenge. Algorithmic
modifications, such as estimation of K, would be also useful in practice.


-----

6 ACKNOWLEDGEMENTS

This work has been supported by the German Research Foundation (Research Training Group GRK
2428) and the Baden-W¨urttemberg Stiftung (Eliteprogram for Postdocs project “Clustering large
evolving networks”). The authors thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Leena Chennuru Vankadara.

7 ETHICS STATEMENT

The proposed clustering algorithms for network-valued data can naturally be evaluated on realworld data, but no social interpretation about the results will be drawn. Moreover, development of
fair clustering algorithms is not the focus of this project.

8 REPRODUCIBILITY STATEMENT

The assumptions for the theory are stated clearly in Assumptions 1–3 and all the theoretical results, Proposition 1, Theorems 1-3, Corollary 1, are proved in detail in Appendix A. The imple[mentation of the considered algorithms with steps to reproduce the results is available in Github -](https://github.com/mahalakshmi-sabanayagam/Clustering-Testing-Networks)
[https://tinyurl.com/3ycmmj4u. Datasets used in the experiments are public and the links are pro-](https://github.com/mahalakshmi-sabanayagam/Clustering-Testing-Networks)
vided in download datasets function of utils.py. The experimental results can be reproduced by following graph clustering.ipynb.

REFERENCES

Joshua Agterberg, Minh Tang, and Carey Priebe. Nonparametric two-sample hypothesis testing for
random graphs with negative and repeated eigenvalues. arXiv preprint arXiv:2012.09828, 2020.

Peter J Bickel and Aiyou Chen. A nonparametric view of network models and newman–girvan and
other modularities. Proceedings of the National Academy of Sciences, 106(50):21068–21073,
2009.

Karsten M Borgwardt, Cheng Soon Ong, Stefan Sch¨onauer, SVN Vishwanathan, Alex J Smola, and
Hans-Peter Kriegel. Protein function prediction via graph kernels. Bioinformatics, 21(Suppl 1):
i47–i56, 2005.

Horst Bunke and Kim Shearer. A graph distance metric based on the maximal common subgraph.
_Pattern recognition letters, 19(3-4):255–259, 1998._

Stanley Chan and Edoardo Airoldi. A consistent histogram estimator for exchangeable graph models. In International Conference on Machine Learning, pp. 208–216, 2014.

Persi Diaconis and Svante Janson. Graph limits and exchangeable random graphs. arXiv preprint
_arXiv:0712.2749, 2007._

Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without
alignments. Journal of molecular biology, 330(4):771–783, 2003.

Simon S Du, Kangcheng Hou, Russ R Salakhutdinov, Barnabas Poczos, Ruosong Wang, and Keyulu
Xu. Graph neural tangent kernel: Fusing graph neural networks with graph kernels. In Advances
_in Neural Information Processing Systems, pp. 5724–5734, 2019._

Xinbo Gao, Bing Xiao, Dacheng Tao, and Xuelong Li. A survey of graph edit distance. Pattern
_Analysis and applications, 13(1):113–129, 2010._

Debarghya Ghoshdastidar and Ulrike von Luxburg. Practical methods for graph two-sample testing.
In Advances in Neural Information Processing Systems, pp. 3019–3028, 2018.

Debarghya Ghoshdastidar, Maurilio Gutzeit, Alexandra Carpentier, and Ulrike von Luxburg. Twosample tests for large random graphs using network statistics. In Conference on Learning Theory,
pp. 954–977, 2017.


-----

Debarghya Ghoshdastidar, Maurilio Gutzeit, Alexandra Carpentier, Ulrike von Luxburg, et al. Twosample hypothesis testing for inhomogeneous random graphs. Annals of Statistics, 48(4):2208–
2229, 2020.

Xiaoyang Guo, Anuj Srivastava, and Sudeep Sarkar. A quotient space formulation for generative
statistical analysis of graphical data. Journal of Mathematical Imaging and Vision, pp. 735–752,
2019.

Olga Klopp, Alexandre B Tsybakov, Nicolas Verzelen, et al. Oracle inequalities for network models
and sparse graphon estimation. The Annals of Statistics, 45(1):316–354, 2017.

Risi Kondor and Horace Pan. The multiscale laplacian graph kernel. In Advances in Neural Infor_mation Processing Systems, pp. 2990–2998, 2016._

Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics
_quarterly, 2(1-2):83–97, 1955._

L´aszl´o Lov´asz and Bal´azs Szegedy. Limits of dense graph sequences. Journal of Combinatorial
_Theory, Series B, 96(6):933–957, 2006._

Soumendu Sundar Mukherjee, Purnamrita Sarkar, and Lizhen Lin. On clustering network-valued
data. In Advances in neural information processing systems, pp. 7071–7081, 2017.

Annamalai Narayanan, Mahinthan Chandramohan, Rajasekar Venkatesan, Lihui Chen, Yang Liu,
and Shantanu Jaiswal. graph2vec: Learning distributed representations of graphs. arXiv preprint
_arXiv:1707.05005, 2017._

Mark EJ Newman. The structure and function of complex networks. SIAM review, 45(2):167–256,
2003.

Lutz Oettershagen, Nils M Kriege, Christopher Morris, and Petra Mutzel. Temporal graph kernels
for classifying dissemination processes. In Proceedings of the 2020 SIAM International Confer_ence on Data Mining, pp. 496–504, 2020._

Sofia C Olhede and Patrick J Wolfe. Network histograms and universality of blockmodel approximation. Proceedings of the National Academy of Sciences, 111(41):14722–14727, 2014.

Shirui Pan, Jia Wu, Xingquan Zhu, Guodong Long, and Chengqi Zhang. Task sensitive feature
exploration and learning for multitask graph classification. IEEE transactions on cybernetics, 47
(3):744–758, 2016.

Micha¨el Perrot, Pascal Esser, and Debarghya Ghoshdastidar. Near-optimal comparison based clustering. In Advances in Neural Information Processing Systems, pp. 19388–19399, 2020.

Kaspar Riesen and Horst Bunke. Approximate graph edit distance computation by means of bipartite
graph matching. Image and Vision computing, 27(7):950–959, 2009.

Benedek Rozemberczki, Oliver Kiss, and Rik Sarkar. Karate club: An api oriented open-source
python framework for unsupervised learning on graphs. In Proceedings of the 29th ACM Inter_national Conference on Information & Knowledge Management, pp. 3125–3132, 2020._

Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn, and Karsten M
Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(9):
2539–2561, 2011.

Cornelis J Stam, BF Jones, G Nolte, M Breakspear, and Ph Scheltens. Small-world networks and
functional connectivity in alzheimer’s disease. Cerebral cortex, 17(1):92–99, 2007.

Minh Tang, Avanti Athreya, Daniel L Sussman, Vince Lyzinski, Youngser Park, and Carey E Priebe.
A semiparametric two-sample hypothesis testing problem for random graphs. Journal of Compu_tational and Graphical Statistics, 26(2):344–354, 2017a._

Minh Tang, Avanti Athreya, Daniel L Sussman, Vince Lyzinski, and Carey E Priebe. A nonparametric two-sample hypothesis testing problem for random graphs. Bernoulli, 23(3):1599–1630,
2017b.


-----

Robert L Thorndike. Who belongs in the family? Psychometrika, 18(4):267–276, 1953.

Matteo Togninalli, Elisabetta Ghisu, Felipe Llinares-L´opez, Bastian Rieck, and Karsten Borgwardt.
Wasserstein weisfeiler-lehman graph kernels. In Advances in Neural Information Processing
_Systems, pp. 6439–6449, 2019._

Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17(4):395–416,
2007.

Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In International Conference on Learning Representations, 2019.

Bowei Yan, Purnamrita Sarkar, and Xiuyuan Cheng. Provable estimation of the number of blocks
in block models. In International Conference on Artificial Intelligence and Statistics, pp. 1185–
1194, 2018.

Shuicheng Yan, Dong Xu, Benyu Zhang, Hong-Jiang Zhang, Qiang Yang, and Stephen Lin. Graph
embedding and extensions: A general framework for dimensionality reduction. IEEE transactions
_on pattern analysis and machine intelligence, 29(1):40–51, 2006._

Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In ACM SIGKDD international
_conference on knowledge discovery and data mining, pp. 1365–1374, 2015._

Mikhail Zaslavskiy, Francis Bach, and Jean-Philippe Vert. A path following algorithm for the graph
matching problem. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(12):
2227–2242, 2008.

Yuan Zhang, Elizaveta Levina, and Ji Zhu. Estimating network edge probabilities by neighbourhood
smoothing. Biometrika, 104(4):771–783, 2017.


-----

A PROOFS OF THEORETICAL RESULTS

We discuss the proofs of Proposition 1 and Theorems 1–3 with supporting lemmas in this section.

A.1 PROPOSITION 1

The distance function defined in (2) estimates the L2-distance between graphons that are continuous.
To prove this, we introduce a method to discretize the continuous graphons in the following so that
it is comparable with the transformed graphs described in the graph distance estimator (2).

**Graphon discretization. We discretize the graphon by applying piece-wise constant function ap-**
proximation that is inspired from Chan & Airoldi (2014), similar to the graph transformation. More
precisely, any continuous graphon w is discretized to a matrix W of size n0 × n0 with


_w(x +_ _[i]_ _, y +_ _[j]_ ) dx dy (6)

_n0_ _n0_


_n0_


_n0_


_Wij =_


1/n[2]0


We make Assumptions 1–3 to derive the concentration bound stated in Proposition 1. The proof
structure is as follows:

1. We bound the point-wise deviation of graphon with its discretized correspondence using
Lipschitzness assumption (Lemma 1).

2. We derive the error bound between the L2-distance and Frobenius norm of the discretized
graphons using Lemma 1 (Lemma 2).

3. We establish a relation between Frobenius norm of the histograms of graphons and graphs
(Lemma 3).

4. Finally, we prove Proposition 1 by combining Lemmas 2 and 3.

**Lemma 1 (Lipschitz condition) For any graphon w and corresponding discretization W** _, define a_
_piecewise constant function w(x, y) = Wij where x ∈_ [ _n[i]0_ _[,]_ _ni0_ [+][ 1]n0 []][ and][ y][ ∈] [[][ j]n0 _[,][ j]n0_ [+][ 1]n0 []][. Using]

_the Lipschitz continuous assumption, we have_
_√2L_
_w(x, y)_ _w(x, y)_ _,_ (7)
_−_ _≤_ [2] _n0_

_where L is the Lipschitz constant in Assumption 1._

**Proof of Lemma 1 (Lipschitz condition). We use Assumption 1 on Lipschitzness to prove this**
lemma. The following holds for a graphon w with Lipschitz constant L,

_i[2]_ _√2_

_, y +_ _[j]_ ) _w(x, y)_ + _[j][2]_ (8)
_n0_ _n0_ _−_ _[≤]_ _[L]s_ _n[2]0_ _n[2]0_ _≤_ _[L]n0_

We prove Lemma 1 using (8) and the definition ofy ∈ [ _n[j]0_ _[,][ j]n0_ [+] _n10_ []][,] _[w][(][x][ +][ i]_ _w(x, y) = Wij where x ∈_ [ _n[i]0_ _[,]_ _ni0_ [+] _n10_ []][ and]

_w(x, y)_ _w(x, y)_ = _, y +_ _[j]_ )
_−_ _n0_ _n0_

_√2L_

_[w][(][x, y][)][ −]_ _[w][(][x, y][)][ ±][ w], y[(] +[x][ +][j][ i])_ [+] _, y +_ _[j]_ ) _w(x, y)_ [= 2]

_≤_ _n0_ _n0_ _n0_ _n0_ _−_ _n0_

**Lemma 2 (Error bound of discretization)[w][(][x, y][)][ −]** _[w][(][x][ +][ i] For two graphons[w][(] w[x][ +]1 and[ i]_ _w2, the error bound between_
_the L2-distance and the Frobenius norm of the corresponding discretized graphons W1 and W2_
_satisfies_

_√2L_

_W1_ _W2_ _F_ _._ (9)

_[−]_ _n[1]0_ _∥_ _−_ _∥_ _[≤]_ [4] _n0_

_[∥]_ _[w][1][ −]_ _[w][2][ ∥][L][2]_


-----

**Proof of Lemma 2 (Error bound of the approximation). Lemma 1 is used to prove this lemma.**
Let L1 and L2 be the Lipschitz constants of w1 and w2.

1 1
_∥_ _w1 −_ _w2 ∥L[2]_ 2 [=] 0 0 (w1(x, y) − _w2(x, y))[2]_ dx dy
Z Z

_w1(x, y) and_ _w2(x, y) within the square, expand and apply Lipschitzness condition from Lemma 1_
_±_ _±_

1 2 _n0_ _n0_ 1 2

+ [8][L][2] + [16][L][1][L][2] + (W1)kl (W2)kl

_≤_ [8]n[L][2]0[2] _n[2]0_ _n[2]0_ _k=0_ _l=0_ _n[2]0_ _−_

X Xn0 _n0_  

_√2(L1 + L2)_ 1
+ [4] (W1)kl (W2)kl

_n0_ _k=0_ _l=0_ _n[2]0_ _−_

X X

2√2(L1 + L2) 2 _√2(L1 + L2)_

+ [1] _W1_ _W2_ _F_ [+ 4] _W1_ _W2_ 1

_≤_ _n0_ _n[2]0_ _∥_ _−_ _∥[2]_ _n[3]0_ _∥_ _−_ _∥_
 

(a) 2√2(L1 + L2) + [1] _W1_ _W2_ _F_ 2 (10)
_≤_ _n0_ _n0_ _∥_ _−_ _∥_
 

(a) : _x_ 1 _n_ _x_ _F_
_∥_ _∥_ _≤_ _[√]_ _∥_ _∥_

Similarly, by _w1(x, y) and_ _w2(x, y) to_ [1] _W1_ _W2_ _F_ [and applying Lipschitzness condition]
_±_ _±_ _n[2]0_ _∥_ _−_ _∥[2]_

from Lemma 1 we get,

1 2√2(L1 + L2) 2

_n[2]0_ _∥W1 −_ _W2∥F[2]_ _[≤]_ _n0_ + ∥w1 − _w2∥L2_ (11)
 

Combining (10) and (11), we prove

_√2(L1 + L2)_ (b) _√2L_

_W1_ _W2_ _F_ _,_ (b): L = max _L1, L2_ □

_[−]_ _n[1]0_ _∥_ _−_ _∥_ _[≤]_ [2] _n0_ _≤_ [4] _n0_ _{_ _}_

We derive the following relation between histogram of graphs and graphons by adapting lemmas[∥] _[w][1][ −]_ _[w][2][ ∥][L][2]_
from Chan & Airoldi (2014) for our problem and the error bound of discretization (9).

_andLemma 3 W2 be the corresponding discretized graphons of Let G1 ∼_ _w1 and G2 ∼_ _w2 have respective graph transformations w1 and w2, respectively. As A1 and n A →∞2. Let Wand1_
_n[2]0_ [log][ n]

0, then for any ϵ > 0,
_n_ _→_

_A1_ _A2_ _F_ _W1_ _W2_ _F_ 4 ϵ (12)
_|∥_ _−_ _∥_ _−∥_ _−_ _∥_ _| ≤_

_with probability converging to 1._

**Proof of Lemma 3. The proof of this lemma is inspired from Chan & Airoldi (2014). For i = {1, 2},**
let matrixanother transformation of graph Wi of size n0 × n0 be the discretized graphon Gi based on the true permutation wi and let matrixσi, that is, _σAi denotes the orderingi of size n0 × n0 be_
of the graph Gi based on the corresponding graphon wi. In other words, the discretized graphon Wi

[b]
is the expectation of _Ai. The reordered graph is denoted by Gσi[b]_ [and then] b [ b]Ai is obtained by, b

_h_ _h_

_σi_

(Ai)kl = [1] [b] (G[b] )kh+k1,lh+l1 where h = and is the floor function.

_h[2]_ _⌊_ _n[n]0_ _⌋_ _⌊·⌋_

_k1=0_ _l1=0_

X X

[b]

We bound ∥A1 − _A2∥F using_ _A1,_ _A2, W1 and W2 as,_

_∥A1 −_ _A2∥F ≤∥A1 −[b]_ _A[b]1[b]∥F + ∥A2 −_ _A[b]2∥F + ∥A[b]1 −_ _A[b]2∥F_

2 max _Ai_ _F +_ _A1_ _A2_ _F_
_≤_ _i={1,2}_ _[∥][A][i][ −]_ [b] _∥_ _∥_ [b] _−_ [b] _∥_

2 max _Ai_ _F + 2 max_ _Ai_ _Wi_ _F +_ _W1_ _W2_ _F_ (13)
_≤_ _i={1,2}_ _[∥][A][i][ −]_ [b] _∥_ _i={1,2}_ _[∥]_ [b] _−_ _∥_ _∥_ _−_ _∥_


-----

We have the following for all i using Assumption 2 on Lipschitzness of the expected degree function
_g(u)i of graphon wi and Lemma 3 of Chan & Airoldi (2014),_


E[ _Ai_ _Ai_ _F_ []][ ≤] _[n]0[4]_ 2 + 4Ci[2][L]i[2] log ni
_∥_ _−_ [b] _∥[2]_ _n[2]i_ _ni_



2 _[n]0[4]_ 0 _i_ _[L]i[2]_ log n
_≤_ _n[2][ +][ n]n[4][2][ 4][C]_ [2] _n_


log ni
4Ci[2][L]i[2]

_ni_


+ n[2]0


; Ci depends on Lipschitz constants of g(u)i


log n
+ n[2]0[4][C]i[2][L]i[2]

_n_


log n
= O _n[2]0_ _n_



(14)

using (14),


Applying Markov’s inequality to bound the probability P max _Ai_ _F_ 1 using (14),
i={1,2} _[∥][A][i][ −]_ [b] _∥[2]_ _[≥]_ _[ϵ][2]_

P max _Ai_ _F_ 1 (a) E _∥Ai −_ _A[b]i∥F[2]_ (14)= _n20_ [log][ n] ; (a) : Union bound
i={1,2} _[∥][A][i][ −]_ [b] _∥[2]_ _[≥]_ _[ϵ][2]_ _≤_ _i=X{1,2}_ h _ϵ[2]1_ i _O_  _ϵ[2]1[n]_ 

(15)


0 [log][ n]
Thus asymptotically, as n and _[n][2]_ 0, for all i and any ϵ1 > 0, _Ai_ _Ai_ _F < ϵ1 with_
_→∞_ _n_ _→_ _∥_ _−_ [b] _∥_

probability converging to 1.

From Lemma 4 of Chan & Airoldi (2014), we have the following for all i,

E[ _Ai_ _Wi_ _F_ []][ ≤] _[n]0[4]_ 0 (16)
_∥_ [b] _−_ _∥[2]_ _n[2]i_ _≤_ _n[n][2][4]_

Applying Markov’s inequality to bound the probability P max _Ai_ _Wi_ _F_ 2 using (16),
i={1,2} _[∥]_ [b] _−_ _∥[2]_ _[≥]_ _[ϵ][2]_

P max _Ai_ _Wi_ _F_ 2 (a) E _∥A[b]i −_ _Wi∥F[2]_ (16) 2n[4]0 ; (a) : Union bound
i={1,2} _[∥]_ [b] _−_ _∥[2]_ _[≥]_ _[ϵ][2]_ _≤_ _i=X{1,2}_ h _ϵ[2]2_ i _≤_ _ϵ[2]2[n][2]_

(17)

Again asymptotically asconverging to 1. Lets assume n →∞ ϵ2 =, for all ϵ1 = ϵ. Substituting (15) and (17) in (13), i and any ϵ2 > 0, ∥A[b]i − _Wi∥F < ϵ2 with probability_

_A1_ _A2_ _F_ 4ϵ + _W1_ _W2_ _F_ (18)
_∥_ _−_ _∥_ _≤_ _∥_ _−_ _∥_

0 [log][ n]
with probability converging to 1 as n and _[n][2]_ 0.
_→∞_ _n_ _→_

The lower bound can similarly be obtained,


_A1_ _A2_ _F_ _W1_ _W2_ _F_ 2 max _Ai_ _Wi_ _F_ 2 max _Ai_ _Ai_ _F_
_∥_ _−_ _∥_ _≥∥_ _−_ _∥_ _−_ _i={1,2}_ _[∥]_ [b] _−_ _∥_ _−_ _i={1,2}_ _[∥]_ [b] _−_ _∥_

_W1_ _W2_ _F_ 4ϵ (19)
_≥∥_ _−_ _∥_ _−_

0 [log][ n]
with probability converging to 1 as n and _[n][2]_ 0.
_→∞_ _n_ _→_

Thus, _A1_ _A2_ _F_ _W1_ _W2_ _F_ 4ϵ satisfy for any ϵ > 0 with probability converging to 1
_|∥_ _−_ _∥_ _−∥_ _−_ _∥_ _| ≤_

0 [log][ n]
as n and _[n][2]_ 0 from equations (18) and (19). □
_→∞_ _n_ _→_

**Proof of Proposition 1 (Graph distance is consistent). Proposition 1 immediately follows from**
Lemmas 2 and 3 after a simple decomposition step as shown below.

_|∥_ _w1 −_ _w2 ∥L2 −_ _d(G1, G2)| =_ _[−]_ _[d][(][G][1][, G][2][)][ ±][ 1]n0_ _∥W1 −_ _W2∥F_

1

_[∥]_ _[w][1][ −]_ _[w][2][ ∥][L][2]_ _W1_ _W2_ _F_ [+] _W1_ _W2_ _F_ _d(G1, G2)_

_≤_ _n0_ _∥_ _−_ _∥_ _n0_ _∥_ _−_ _∥_ _−_

2,3 _[∥]√[w]2[1][ −]L_ + [w][2][4][ ∥][ϵ][L][2]=[ −] [1] 1 holds for any ϵ > 0 as n and _[n]0[2]_ [log][ n] 0 .□
_≤_ [4] _n0_ _n0_ _O_ _n0_ _→∞_ _n_ _→_

 


-----

A.2 PROPOSITION 1 FOR SPARSE GRAPHS

To adapt the dense graph model, graphon, to sparse graphs, the sampling procedure is modified
to Gij _Ui, Uj_ _Bernoulli (ρ w(Ui, Uj)) where ρ depends only on n. With this modification,_
equation 14 will be changed to| _∼_

E[ _Ai_ _Ai_ _F_ []][ ≤] [2] _[n]0[4]_ 0 _i_ _[L]i[2][ρ][2][ log][ n]_ + n[2]0[4][C]i[2][L]i[2][ρ] [log][ n] = _n[2]0[ρ]_ [log][ n] (20)
_∥_ _−_ [b] _∥[2]_ _n[2][ +][ n]n[4][2][ 4][C]_ [2] _n_ _n_ _O_ _n_

 

and the proof involves modification to Lemma 2 of Chan & Airoldi (2014). We state the modified
lemma and briefly sketch the proof below.

**Modified Lemma 2 of Chan & Airoldi (2014). Let σ(i) be the oracle permutation such that**


_σ(i)_
_Uσ(1) < Uσ(2) < . . . < Uσ(n). Then, if_


_−_ _[σ][(]n[j][)]_


log n


then


6L1ρ

log n


_dσ(i)_ _dσ(j)_
_−_

1 log n


(21)


with probability at least 1 − 8 exp


. Conversely, if (21) holds with probability at


18L[2]1


_ρ[2]_

log n

_n_

log n

_ρ[2]_


log n

_ρ[2]_


least 1 − 8 exp


then


18L[2]1


1

3L1




_σ(i)_


_−_ _[σ][(]n[j][)]_



_[<][ 1]_

_ρ_

r

1
_−_ 18L[2]1



1 + [1]

3L1L2 _L2_


(22)


with probability at least 1 − 40 exp


**Changes to Proof in Chan & Airoldi (2014). Suppose** _[σ]n[(][i][)]_ _n_

_[−]_ _[σ][(][j][)]_

Then, P _Uσ(i)_ _Uσ(j)_ _> 3_ _ρ[δ]_ 4 exp 2n _ρ[δ][2][2]_ . Consequently,
_−_ _≤_ _−_
   

P _g(Uσ(i))_ _g(Uσ(j))_ _> 3L1δ_ P _Uσ(i)_ _Uσ(j)_ _> 3_ _[δ]_
_−_ _≤_ _−_ _ρ_
   

Following their proof, we have


_<_ _ρ[δ]_ [for][ δ >][ 0][ and][ δ < ρ][.]

4 exp 2n _[δ][2]_ _._
_≤_ _−_ _ρ[2]_
  


P _dσ(i) −_ _dσ(j)_ _> 6L1δ_ _Uσ(i), Uσ(j)_
 

4 exp 1[δ][2] + 4 exp 2n _[δ][2]_
_≤_ _−_ 2[9] _[nL][2]_ _−_ _ρ[2]_
  


8 exp 2n _[δ][2]_
_≤_ _−_ _ρ[2]_



(23)


when ρ > 3L2 1 [. Note that there is a small mistake in their proof when Hoeffding’s inequality is]

applied, where a factor of n[2] is written in place of n. Putting δ = 6L1 1 logn n and considering

_ρ = Ω_ logn n since ρ depends on n and δ < ρ, we get q
q 

log n 1 log n

P _dσ(i)_ _dσ(j)_ _>_ _Uσ(i), Uσ(j)_ 8 exp

_−_ r _n_ ! _≤_ − 18L[2]1 _ρ[2]_ 

Converse can similarly be proved.

(20) can be derived by substituting the above changed lemma in their proof. Consequently, Proposition 1 still holds under this formulation, with slight change to the condition as n →∞ and
_n[2]0[ρ][ log][ n]_ log n

0 for ρ = Ω . We will not consider sparsity ρ for analysing consistency of
_n_ _→_ r _n_ !


our algorithms in the next sections.


-----

A.3 DISTANCE BASED SPECTRAL CLUSTERING (DSC)

We make Assumptions 1–3 on the K graphons to analyse the Algorithm 1. We establish the consistency of this algorithm by deriving the number of misclustered graphs |M| through the following
steps.

1. We establish deviation bound between the estimated distance matrix _D and the ideal dis-_
tance matrix D (Lemma 4).

2. We formulate Davis-Kahan theorem in terms of the deviation bound using the result from[b]
Mukherjee et al. (2017) (Lemma 5).

3. We derive the number of misclustered graphs from Lemma 5.

As stated previously, _D in Algorithm 1 is an estimate of D ∈_ R[m][×][m], where we define Dij =
from same graphon, and equals the distance between the graphons∥wi − _wj∥L2_ . Note that D is a block matrix with rank K, since D i andij = 0 j otherwise. for all Gi, Gj generated

[b]

We derive the deviation bound for the distance matrix using Lemma 3 and the result is as follows.


0 [log][ n]
**Lemma 4 (Distance deviation bound) As n** _and_ _[n][2]_ 0, we establish
_→∞_ _n_ _→_

_m_
_D_ _D_
_−_ _F_ [=][ O] _n0_
 

_with probability converging to 1._

[b]


(24)


**Proof of Lemma 4 (Distance deviation bound). From Proposition 1 and the definitions of** _Dij and_

1
_Dij, it is easy to see that_ _Dij_ _Dij_ = with probability converging to 1 as n and
_−_ _O_ _n0_ _→∞[b]_
 

_n[2]0_ [log][ n]

0. [b]
_n_ _→_

Using Lemmas 2 and 3, and the definitions of _Dij and Dij, we have_
1
_Dij_ _Dij_ = _Ai_ _Aj_ _F_ _wi_ _wj_ _L2_

[b] _−_ _n0_ _∥_ _−_ _∥_ _−∥_ _−[b]_ _∥_


1
= _n0_ _∥Ai −_ _Aj∥F ±_ _n[1]0_ _∥Wi −_ _Wj∥F −∥wi −_ _wj∥L2_

_Ai_ _Aj_ _F_ _Wi_ _Wj_ _F_ + _Wi_ _Wj_ _F_

_≤_ _n[1]0_ _|∥_ _−_ _∥_ _−∥_ _−_ _∥_ _|_ _n0_ _∥_ _−_ _∥_

(12) + [4]√2L with prob. 1 asymptotically[∥][w][i][ −] _[w][j][∥][L][2][ −]_ [1]
_≤_ _n[4][ϵ]0_ _n0_ _→_

1
Thus asymptotically, _Dij_ _Dij_ = with probability converging to 1.
_−_ _O_ _n0_
 

[b] _m_ 0 [log][ n]

Hence, _D_ _D_ _F =_ with probability converging to 1 as n and _[m][2][n][2]_ 0.
_∥_ [b] − _∥_ _O_ _n0_ _→∞_ _n_ _→_
 

A variant of Davis-Kahan theorem (Mukherjee et al., 2017) and the derived deviation bound (24)
are used to prove the following lemma.

**Lemma 5 (Davis-Kahan theorem) Let V and** _V be the m×K matrices whose columns correspond_
_to the leading K eigenvectors of D and_ _D, respectively. Let γ be the K-th smallest eigenvalue value_

0 [log][ n] [b]
_of D in magnitude. As n_ _and_ _[n][2]_ 0, there exists an orthogonal matrix _O such that,_
_→∞_ _n[b]_ _→_

_m_
_V_ _O_ _V_ [b] (25)
_−_ _F_ [=][ O] _γn0_
 

_with probability converging to 1._

[b] [b]


-----

**Proof of Lemma 5 (Davis-Kahan theorem). A variant of Davis Kahan theorem from Proposition**
A.2 of Mukherjee et al. (2017) states the following for matrix D of rank K. Let _V and V be m ∗_ _K_
matrices whose columns correspond to the leading K eigenvectors of _D and D, respectively, and γ_
be the K-th smallest eignenvalue of D in magnitude, then there exists an orthogonal matrix[b] _O of_
size K _K such that,_ [b]
_∗_

4 _D_ _D_ [b]
_−_ _F_ 4 _m_ 0 [log][ n]
_V_ _O_ _V_ = as n and _[m][2][n][2]_ 0.□
_−_ _F_ _γ_ _O_ _γn0_ _→∞_ _n_ _→_

_[≤]_ [b]  

The number of misclustered graphs isof graphs generated from a single graphon (Mukherjee et al., 2017). Since3 [b] [b] _|M| ≤_ 8mT ∥V[b] _O −_ _V ∥F[2]_ [where][ m][T][ is the maximum number] mT = O(m), |M| =

_m_

by substituting (25) in . Hence proving Theorem 1.[b]

_O_ _γ[2]n[2]0_ _|M|_
 

2
**Proof of Theorem 1.** The number of misclustered graphs 8mT _V_ _O_ _V_
_|M| ≤_ _−_ _F_ [from]

Mukherjee et al. (2017). Thus, we prove the theorem using Lemma 5. That is, as n →∞ and
_m[2]n[2]0_ [log][ n] [b] [b]

0,
_n_ _→_

2 3

5 _m_

8mT _V_ _O_ _V_ = □
_|M| ≤_ _−_ _F_ _O_ _γ[2]n[2]0_

 

**Proof of Corollary 1. This corollary deals with a special case where[b]** [b] _K = 2 and equal number of_
graphs are generated from the two graphons w and w[′]. Therefore, mT in the number of misclustered
graphs |M| is m/2. The ideal distance matrix D will be of size m × m with 0 and ∥w − _w[′]∥L2 as_
entries depending on whether the samples are generated from the same graphon or not. For such a
block matrix D, the two non zero eigenvalues are
_±_ _[m]2_ 2

Corollary 1 can be derived by substituting the derived γ[∥][w] in the number of misclustered graphs[ −] _[w][′][∥][L][2]_ [. Therefore,][ γ][ is][ m] _[∥][w][ −]_ _[w] |M|[′][∥][L][2]_ [.]
in Theorem 1 as shown below.


=
_|M|_ _O_ _w_ _w[′]_ _L2_ _[n]0[2]_
 _∥_ _−_ _∥[2]_ 

0 [log][ n]

Let us assume ∥w − _w[′]∥L2 ≥_ _C n[m]0_ where C is a large constant, then as n →∞, _[m][2][n][2]n_

_|M| →_ 0.


_→_ 0,


A.4 SIMILARITY BASED SEMI-DEFINITE PROGRAMMING (SSDP)

We make Assumptions 1–3 on the K graphons to study the recovery of clusters from Algorithm 2.
The proof structure for cluster recovery stated in Theorem 2 is as follows:

1. We establish deviation bound between the estimated similarity matrix _S and the ideal sim-_
ilarity matrix S (Lemma 6).

2. We derive the recoverability condition by adapting Proposition 1 of Perrot et al. (2020) and[b]
the obtained deviation bound (Lemma 7).


The ideal similarity matrix S ∈ R[m][×][m] is symmetric with K × K block structure, and S = ZΣZ _[T]_
where Z 0, 1 be the clustering membership matrix and Σ R[K][×][K] such that Σll′ repre_∈{_ _}[m][×][K]_ _∈_
sents ideal pairwise similarity between graphs from clusters Cl and Cl′ . From the definition of Sij,

Σll′ = exp where wl and wl′ are graphons corresponding to clusters _l and_ _l′_,
_−_ _[∥][w][l][ −]σlσ[w]l′[l][∥][L][2]_ _C_ _C_
 

respectively. _S is the estimated similarity matrix of S as mentioned earlier. Since_ _X ∈1R[K][×][K]_ is the1

normalised clustering matrix, _X = ZN_ _[−][1]Z_ _[T]_ where N is a diagonal matrix with

1 _K_

[b] [b]

_|C_ _|_ _[, . . .,]_ _|C_ _|_ [.]

We derive the deviation bound for the similarity matrices using Lemma 3 and the result is as follows.

[b]


-----

0 [log][ n]
**Lemma 6 (Similarity deviation bound) As n** _,_ _[n][2]_ 0, we establish
_→∞_ _n_ _→_

1
_Sij_ _Sij_ =
_|_ [b] _−_ _|_ _O_ _n0_
 

_m_
_with probability converging to 1. Hence, from the result_ _S_ _S_ _F =_
_∥_ [b] − _∥_ _O_ _n0_
 

_converging to 1._


(26)

_with probability_


**Proof of Lemma 6 (Similarity deviation bound). We derive the bound using Lemmas 2 and 3, and**
the definitions of _Sij and Sij._

_Sij = exp_ [b] Consider σi = σj = σ
_−_ _[∥][A]n[i][ −]0σi[A]σ[j]j[∥][F]_
 
b


3
exp
_≥_ _−_ _[∥][W][i][ −]n[W]0σ[j][∥][2][F][ + 4][ϵ]_



with probability → 1 asymptotically

exp(−x) ≥ 1 − 2x for x > 0


2
exp
_≥_ _−_ _[∥][w][i][ −]σ[w][2]_ _[j][∥][L][2]_



_√_
_−_ [4][ϵ][ + 4]n0σ[2]


2L


exp


_√2L)_
1
_−_ [8(][ϵ][ +]n0σ[2]


_Sij_
_≥_


8(ϵ + _√2L)_
_Sij_ _Sij_ _Sij_ [0, 1]
_≥_ _−_ _n0σ[2]_ _∈_

_√2L)_
_Sij_ (27)
_≥_ _−_ [8(][ϵ][ +]n0σ[2]


_Sij = exp_
_−_ _[∥][A][i]n[ −]0σ[A][2][j][∥][F]_
 

3

b

exp
_≤_ _−_ _[∥][W][i][ −]n[W]0σ[j][∥][2][F][ −]_ [4][ϵ]



with probability → 1 asymptotically

exp(x) ≤ 1 + 2x for x > 0


2
exp
_≤_ _−_ _[∥][w][i][ −]σ[w][2]_ _[j][∥][L][2]_



4ϵ + 4√

_n0σ[2]_


2L


exp


1 + [8(][ϵ][ +] _√2L)_

_n0σ[2]_


_Sij_
_≤_


8(ϵ + _√2L)_
_Sij + Sij_ _Sij_ [0, 1]
_≤_ _n0σ[2]_ _∈_


_√2L)_
_Sij + [8(][ϵ][ +]_ (28)
_≤_ _n0σ[2]_

_√2L)_ 1
Thus, from (27) and (28), we get _Sij_ _Sij_ = for any ϵ, with probability
_|_ [b] _−_ _| ≤_ [8(][ϵ][ +]n0σ[2] _O_ _n0_

 

0 [log][ n] _√2L)_ _m_
converging to 1 as n and _[n][2]_ 0. Hence, _S_ _S_ _F_ =,
_→∞_ _n_ _→_ _∥_ [b] − _∥_ _≤_ [8][m][(][ϵ]n[ +]0σ[2] _O_ _n0_

 

0 [log][ n]
with probability converging to 1 as n and _[m][2][n][2]_ 0. □
_→∞_ _n_ _→_

The condition for exact recovery of clusters is derived by adapting Proposition 1 of Perrot et al.
(2020). The proposition states the recoverability condition for such an SDP defined in (4) in terms of
the similarity deviation bound. Thus, we use the derived bound in Lemma 6 and establish condition
on the L2-distance to satisfy the proposition from Perrot et al. (2020). First, we state the adapted
proposition.


-----

We define ∆1 and ∆2 as,

∆1 = min and ∆2 = max _Sij_ _Sij_ _._
_l≠_ _l[′][ (1][ −]_ [Σ][ll][′] [)] _ij_ _|_ [b] _−_ _|_

Then, the following should be satisfied for _X to be the unique optimal solution of the SDP in (4):_

∆1
_S_ _S_ _F_ min[b] _l_ min _._
_∥_ [b] − _∥_ _≤_ _l_ _|C_ _|_ 2 _[,][ ∆][1][ −]_ [6∆][2]
 

The minimum cluster size min _l_ in our case is 1. Consequently, the recoverability condition is
_l_ _|C_ _|_

derived and is as follows.

0 [log][ n]
**Lemma 7 (Recoverability of clusters) As n** _,_ _[m][2][n][2]_ 0, the min _l[∥][L]2_ _[should]_
_→∞_ _n_ _→_ _l=l[′][ ∥][w][l][ −]_ _[w][′]_

_m_ _̸_
_be Ω_ _so that_ _X is the unique optimal solution of the SDP (4)._

_n0_

 

**Proof of Lemma 7 (Recoverability of clusters).[b]** We derive the condition to satisfy the stated
proposition.

∆1 = 1 maxl=l′ Σll′ and ∆2 = min _Sij_ _Sij_ . The minimum cluster size in our case can be 1.
_−_ _̸_ _ij_ _[|]_ [b] _−_ _|_

The analyses of the two cases of the Proposition is as follows.

**Case 1. Let us assume ∆2** ∆21 _[,][ ∆][1][ −]_ [6∆][2] will be [∆]2[1] [. Therefore,]
_≤_ [∆]12[1] [, then][ min]



8m(ϵ + _√2L)_

_n0σ[2]_ _≤_ 2[1] _[−]_ [1]2 [max]l≠ _l[′][ exp]_ − _[∥][w][l][ −]σ[w][2]_ _[l][′]_ _[∥][L][2]_ 

_wl_ _wl′_ _L2_ _√2L)_
exp min _∥_ _−_ _∥_ 1
− _l≠_ _l[′]_ _σ[2]_  _≤_ _−_ [16][m][(]n[ϵ][ +]0σ[2]


_wl_ _wl′_ _L2_
min _∥_ _−_ _∥_ log
_l=l[′]_ _σ[2]_ _≥−_
_̸_

_∞_
min
_l=l[′][ ∥][w][l][ −]_ _[w][l][′]_ _[∥][L][2][ ≥]_ _[σ][2]_
_̸_ _k=1_
X

_m_
min
_l≠_ _l[′][ ∥][w][l][ −]_ _[w][l][′]_ _[∥][L][2][ = Ω]_  _n0_


1
_−_ [16][m][(]n[ϵ][ +]0σ[2]


2L)


_k_
!


16m(ϵ +

_n0σ[2]_


2L)


(29)


**Case 2. Let us assume ∆2 >** [∆][1] ∆21 _[,][ ∆][1][ −]_ [6∆][2] will be ∆1 6∆2. Therefore,

12 [, then][ min] _−_


8m(ϵ + _√2L)_ _√2L)_

1 max with probability 1 aymptotically
_n0σ[2]_ _≤_ _−_ _l≠_ _l[′][ exp]_ − _[∥][w][l][ −]σ[w][2]_ _[l][′]_ _[∥][L][2]_  _−_ [6][ ∗] [8(]n[ϵ]0[ +]σ[2] _→_


_wl_ _wl′_ _L2_
min _∥_ _−_ _∥_ log
_l=l[′]_ _σ[2]_ _≥−_
_̸_


1
_−_ [8(][m][ + 6)(]n0σ[ϵ][ +][2]


2L)


_k_
!


8(m + 6)(ϵ +

_n0σ[2]_


2L)


8(m + 6)(ϵ + 2L) 1

min
_l≠_ _l[′][ ∥][w][l][ −]_ _[w][l][′]_ _[∥][L][2][ ≥]_ _[σ][2]_ _k=1_ _n0σ[2]_ ! _k_
X

_m_
min
_l≠_ _l[′][ ∥][w][l][ −]_ _[w][l][′]_ _[∥][L][2][ = Ω]_  _n0_ 

_m_
Thus, from (29) and (30), we must satisfy minl=l′ _wl_ _wl′_ _L2 = Ω_
_̸_ _∥_ _−_ _∥_ _n0_


hold. Consequently, Theorem 2 is the direct reflection of this lemma.


(30)

for the Proposition to


-----

A.5 GRAPH TWO-SAMPLE TESTING

Theorem 3 of two-sample testing is proved by deriving the probability of Type-1 and Type-2 errors.
We make Assumptions 1–3 for this case. Letand w2, respectively, obtained using (6). Then, the alternate hypothesis W1 and W2 be the n0 × n0 discretized graphons of Ha can be rewritten using w1
Lemma 2 in the following way,


1

_W1_ _W2_ _F + [4]_
_n0_ _∥_ _−_ _∥_


_√2L_

_n0_


(9)
_≥_ _φ_


2L = ρ (31)


_W1_ _W2_ _F_ _n0φ_ 4
_∥_ _−_ _∥_ _≥_ _−_


We derive the probability of the errors using Lemma 3 and is stated in the following lemmas.

**Lemma 8 (Probability of Type-1 error) The probability of Type-1 error, i.e. rejecting the null hy-**
_pothesis when it is actually true, is_


log n

P(T = 1 _H0 : True)_
_|_ _≤_ _ξ[C][2]_ _n_

_where C depends only on the Lipschitz constants._


(32)


**Proof of Lemma 8 (Probability of Type-1 error). The Type-1 error is rejecting H0 when it is**
true. Therefore, in this scenario, ∥W1 − _W2∥F = 0. Thus, from Lemma 3, (15) and (17), we have_

_n[2]0_ [log][ n]

_∥A1 −_ _A2∥F ≤_ 4ϵ with 1 − _ϵ[C][2]_ _n_ probability. Therefore, the probability of Type-1 error is,

P(T = 1|H0 : True) = P(d(G1, G2) ≥ _ξ)_
= P( _A1_ _A2_ _F_ _n0ξ)_ err only when n0ξ 4ϵ

(15) _∥C_ _−n[2]0_ [log]∥[ n] _≥_ _≤_

□

_≤_ _ξ[2]n[2]0_ _n_

**Lemma 9 (Probability of Type-2 error) The probability of Type-2 error, i.e. accepting the null**
_hypothesis when the alternate hypothesis is actually true, is_


_C1_
P(T = 0 _Ha : True)_
_|_ _≤_ _φ_ _√n20C2_
_−_ [4]



log n
2 _n_



(33)


_where C1 and C2 depend only on the Lipschitz constants._


**Proof of Lemma 9 (Probability of Type-2 error). The Type-2 error is evaluating to null hypothesis**
when the alternate hypothesis is true. Therefore, from (31) _W1_ _W2_ _F_ _ρ. From Lemma 3, (15)_
and (17), _∥_ _−_ _∥_ _≥_

_n[2]0_ [log][ n]

_∥A1 −_ _A2∥F ≥∥W1 −_ _W2∥F −_ 4ϵ with probability 1 − _[C]ϵ[2][1]_ _n_

_≥_ _ρ −_ 4ϵ

The probability of Type-2 error is,


P(T = 0|Ha : True) = P(d(G1, G2) < ξ)
= P(∥A1 − _A2∥F < n0ξ)_ err only when n0ξ ≤ _ρ −_ 4ϵ; let n0ξ = 4ϵ

P( _A1_ _A2_ _F < [ρ]_
_≤_ _∥_ _−_ _∥_ 2 [)]

_n[2]0_ [log][ n]

_≤_ _[C]ρ[2][1]_ _n_

We get the probability by substituting _[ρ]_ = φ _√2L_ from (31) in the above equation. Theorem

_n0_ _−_ [4] _n0_

3 can be proved by asymptotic analysis of Lemmas 8 and 9. □


-----

B DSC AND SSDP ALGORITHMS

The proposed algorithms DSC and SSDP are described as follows:


**Algorithm 1: Distance based Spectral Clus-**
tering (DSC)
**input : Adjacency matrices G1, ..., Gm,**
histogram size n0
**output: K clusters C1, ..., CK**
**Construct distance matrix Compute**
_Db ∈_ R[m][×][m], where _D[b]ij = d(Gi, Gj)_

**Clustering Apply spectral clustering to** _D_
with K number of clusters resulting in
1, ..., _K_
_C_ _C_ [b]

C EXPERIMENTAL DETAILS


**Algorithm 2: Similarity based Spectral Clus-**
tering (SSDP)
**input : Adjacency matrices G1, ..., Gm,**
histogram size n0
**output: K clusters C1, ..., CK**
**Construct similarity matrix Compute**
_S_ R[m][×][m], where _Sij = exp_ _σiσj_
_∈_ _−_ _[d][(][G][i][,G][j]_ [)]

with σ1 = . . . = σn  
**Clusteringb** Find _X using (4) and apply[b]_
standard spectral clustering to _X resulting in_
1, ..., _K._ [b]
_C_ _C_

[b]


In this section, we present experimental details and additional experiments.

C.1 SIMULATED DATA - HEATMAP OF GRAPHONS

Figure 4 shows the heatmap of the considered four graphons W1, W2, W3 and W4. We sample
graphs from these graphons for the experiments.

Figure 4: Heatmaps of graphons W1, W2, W3 and W4.

C.2 CHOICE OF n0

We validate the theoretically deduced bound for n0 = O( _n/ log n) by sampling 5 graphs with_

a fixed number of nodes n from each of the four graphons, in total 20 graphs, and measuring the

p

performance of DSC and SSDP for different n0 = {5, 10, 15, 20, 25, 30}. We perform three simulations with n = {50, 100, 500} and fix neighbourhood of one in SSDP. Figure 5 shows the average
error of both the algorithms over 5 independent trials. Based on the theoretical considerations for
_n0 (≪_ _n/ log n), we evaluate n0 = {5, 7, 15} for n = {50, 100, 500}, respectively. The experi-_

mental results show that the derived bound for n0 serves as a reasonable choice (if not the best) for

p

both DSC and SSDP irrespective of n. Hence, the choice of n0 can be deterministic and adaptive
with respect to n, thus making our algorithms parameter-free.

C.3 EXPERIMENTAL RESULTS USING ADJUSTED RAND INDEX (ARI)

In this section, we provide the results for evaluation of algorithms on simulated and real data under
the same setting as described in Section 3.3. Figure 6 shows the evaluation of all the discussed
algorithms using ARI, where the observations made from error rate hold.


-----

|Col1|n0 = 5 n = 50|
|---|---|


5 10 15 20 25 30

Histogram size n0


|Col1|n0 = 7 n = 100|
|---|---|


5 10 15 20 25 30

Histogram size n0


|n0 = 15|n = 500|
|---|---|


5 10 15 20 25 30

Histogram size n0


0.4

0.2


DSC (ours)
SSDP (ours)


0.0


Figure 5: Validation of the bound for n0. The plot shows the average error rate (percentage of
misclustered graphs) of the proposed algorithms DSC and SSDP for different n = {50, 100, 500}.


1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0


DSC (ours)
SSDP (ours)
NCLM
NCMMD
WWLGK
GNTK
NCGMM
Time Limit Exceeded


W1, W2, W3 W2, W3, W4 W1, W3, W4 W1, W2, W4 W1, W2, W3, W4

B1, B2, B3 B1, B3, B4 B1, B2, B4 B2, B3, B4 B1, B2, B3, B4


TLE


TLE TLE TLE TLE TLE

S1, S2, S4 S1, S2, S3 S1, S3, S4 S2, S3, S4 S1, S2, S3, S4


Figure 6: Evaluation of DSC and SSDP with other methods. (row 1) Results on simulated data using
ARI. (rows 2 and 3) Results on real data from Bioinformatics and Social Networks, respectively.

C.4 EVALUATION ON LARGE SIMULATED DATA


As mentioned in Section 3.3, we evaluate algorithms except GNTK and NCGMM on large graphs
sampled from the four graphons W1, W2, W3 and W4 with nodes between 100 and 1000. Figure 7
shows the results measured using average error rate and average ARI, respectively. The proposed
algorithm DSC outperforms the others in all the case and SSDP stands second or third best, as
observed in simulated data with small graphs.


1.0

0.5

0.0


0.6

0.4

0.2

0.0


W1, W2, W3 W2, W3, W4 W1, W3, W4 W1, W2, W4 W1, W2, W3, W4


W1, W2, W3 W2, W3, W4 W1, W3, W4 W1, W2, W4 W1, W2, W3, W4


DSC (ours) SSDP (ours) NCLM NCMMD WWLGK

Figure 7: Evaluation of all algorithms except GNTK and NCGMM using average error rate and
average ARI for large simulated data.


-----

C.5 COMPUTATION TIME OF ALGORITHMS

Table 1 shows the time (measured in seconds) taken for the algorithms on the considered dataset
combinations. Three clusters also show similar behavior to four clusters.


Table 1: Time measured in seconds for algorithms on different datasets combinations

**Dataset** **DSC** **SSDP** **NCLM** **NCMMD** **WWLGK** **GNTK** **NCGMP**


_W1, W2, W3_ 0.13 0.22 0.17 11.66 0.66 225.46 15.43
_W2, W3, W4_ 0.14 0.23 0.18 12.98 0.70 199.98 16.27
_W1, W3, W4_ 0.13 0.25 0.19 12.93 0.71 200.85 16.98
_W1, W2, W4_ 0.13 0.24 0.18 12.55 0.68 198.38 16.54
_W1, W2, W3, W4_ 0.20 0.38 0.28 22.05 1.19 390.07 30.18
_B1, B2, B3_ 1.10 1.25 0.17 20.72 2.54 28.21 219.58
_B1, B3, B4_ 0.99 1.18 0.16 21.51 2.85 32.22 225.02
_B1, B2, B4_ 1.01 1.08 0.14 15.61 1.92 19.49 174.25
_B2, B3, B4_ 1.04 1.13 0.13 11.99 0.78 13.38 21.47
_B1, B2, B3, B4_ 1.33 1.46 0.183 28.66 3.19 40.18 278.80
_S1, S2, S4_ 1.50 1.65 8.21 1125.71 454.19 1609.78 TLE
_S1, S2, S3_ 1.25 1.34 0.36 77.67 15.32 294.53 TLE
_S1, S3, S4_ 1.52 1.64 8.07 1001.52 348.25 1485.90 TLE
_S2, S3, S4_ 1.48 1.69 8.88 1035.98 440.87 1757.06 TLE
_S1, S2, S3, S4_ 1.97 2.22 9.47 1069.28 437.21 2060.68 TLE

C.6 SCALABILITY EXPERIMENT


We evaluate the scalability of the considered algorithms using simulated data by measuring the time
taken for clustering 40 random graphs, 10 sampled from each of the graphons W1, W2, W3 and
_W4. We did 7 experiments in which the size of the sampled graphs are varied as [50, max size]_
where max size = {100, 200, 300, 400, 500, 600, 700}. Figure 8 shows the experimental results
which illustrates high scalability of DSC, SSDP and NCLM over other algorithms. Note that the
experiment shows NCLM as scalable as DSC and SSDP since the sampled graphs are small.






DSC (ours) NCLM WWLGK NCGMP
SSDP (ours) NCMMD GNTK TLETLE Time Limit Exceeded


100 200 300 400 500 600 700

TLE

TLE

Max graph size

Figure 8: Computation time of algorithms on different sets of simulated data for four clusters case
demonstrating the scalability of each algorithm. Computation time is plotted in log scale.


C.7 TWO-SAMPLE TESTING

In this section, we evaluate the efficacy of the proposed test T with different d(G1, G2) by varying
the graph sizes n. We consider n = {50, 100, 150} and fix n0 = 10 from the theoretical bound
for evaluating the test T . The power is computed using the test T for the significance level 0.05,
and the plots in Figure 9 show the average power computed over 500 trials of bootstrapping 100
samples generated from all pairs of graphons for d(G1, G2) as our proposed distance, log moments


-----

graph sizes (50, 100) graph sizes (100, 200) graph sizes (150, 300)


w1

w2

w3

w4

w1

w2

w3

w4


w1

w2

w3

w4

w1

w2

w3

w4


w1

w2

w3

w4

w1

w2

w3

w4


0.07 1.00 1.00 1.00

1.00 0.06 0.39 1.00

1.00 0.39 0.05 0.91

1.00 1.00 0.91 0.17

w1 w2 w3 w4

0.02 0.85 0.69 0.03

0.85 0.04 0.12 1.00

0.69 0.12 0.04 0.96

0.03 1.00 0.96 0.03


w1 w2 w3 w4


0.06 1.00 1.00 1.00

1.00 0.06 0.97 1.00

1.00 0.97 0.03 1.00

1.00 1.00 1.00 0.13

w1 w2 w3 w4

0.04 0.99 0.90 0.05

0.99 0.03 0.26 1.00

0.90 0.26 0.05 1.00

0.05 1.00 1.00 0.03


w1 w2 w3 w4


0.04 1.00 1.00 1.00

1.00 0.08 1.00 1.00

1.00 1.00 0.05 1.00

1.00 1.00 1.00 0.13

w1 w2 w3 w4

0.05 1.00 0.99 0.05

1.00 0.03 0.46 1.00

0.99 0.46 0.04 1.00

0.05 1.00 1.00 0.04


w1 w2 w3 w4


w1

w2

w3

w4


w1

w2

w3

w4


w1

w2

w3

w4


0.52 1.00 1.00 0.93

1.00 0.45 0.54 0.96

1.00 0.54 0.51 0.87

0.93 0.96 0.87 0.61

w1 w2 w3 w4


0.57 1.00 1.00 1.00

1.00 0.64 0.47 1.00

1.00 0.47 0.51 1.00

1.00 1.00 1.00 0.77

w1 w2 w3 w4


0.70 1.00 1.00 1.00

1.00 0.87 0.85 1.00

1.00 0.85 0.98 1.00

1.00 1.00 1.00 0.99

w1 w2 w3 w4


Figure 9: Illustration of two-sample testing with the proposed distance vs log moments and MMD
on varying n in graph pairs of size (n, 2n). The plots show the average power of the test T . Test
based on the proposed distance is consistent for sufficiently large graphs and efficient compared to
other methods in distinguishing even closer graphons.

and MMD, respectively. From the result for graph sizes (50, 100), we observe that the graphon
pair (W2, W3) is not easily distinguishable (low H0 rejection probability), which can be explained
by their respective L2 distance that is shown in the left plot of Figure 3. This issue does not arise
in testing larger graphs as the result shows for graph sizes (100, 200) and (150, 300). Therefore,
test T with the proposed distance can distinguish between pairs of graphons that are quite close
provided that the observed graphs are sufficiently large, thus proving to be consistent. On the other
hand, log moments and MMD based tests show weakness in distinguishing the graphons, where
log moments based test T accepts the null hypothesis in most cases even when the graphons are
different for all graph sizes. For instance, the result for graphon pair W1 and W4 is indistinguishable
using log moments statistic for any graph size. On the contrary, MMD based test T rejects the null
hypothesis almost always for larger graphs (diagonal values in all graph cases). Thus, we conclude
that the proposed test T in 5 is consistent and this experiment illustrates the efficiency of the test T
compared to other plausible test statistics.

Subsequently, we evaluate the efficacy of the above tests on the discussed real datasets – Bioinformatics and Social Networks. We consider graphs from a dataset to belong to a population and hence
the objective of the test statistic is to distinguish graphs from different populations, that is, graphs
from two different datasets. Since the populations are not known and the real graphs are treated as
representatives of the population, we compute p-value of the test instead of power to measure the
efficacy. The p-value is the evidence for rejecting the null hypothesis which implies that the smaller
the p-value, the stronger the evidence that the null hypothesis should be rejected. Therefore, the pvalue should be high (greater than the significance level) for graphs from same population and low
(≃ 0) for graphs from different populations. Figure 10 shows the result for both the dataset cases
and different tests. From the results, it is clear that the log moments and MMD based tests are poor


-----

Proposed Distance Log Moments MMD


1.00

0.75

0.50

0.25

1.00

0.75

0.50

0.25


B1

B2

B3

B4

S1

S2

S3

S4


B1

B2

B3


B1

B2

B3


B4


B4


B1 B2 B3 B4 B1 B2 B3 B4 B1 B2 B3 B4


S1

S2

S3

S4


S1

S2

S3

S4


S1 S2 S3 S4 S1 S2 S3 S4 S1 S2 S3 S4

Figure 10: Illustration of two-sample testing with the proposed distance vs log moments and MMD
on real datasets - Bioinformatics and Social Networks. The plots show the p-value of the test T .
Test based on the proposed distance is better than the other two tests.


and inefficient on real datasets as log moments based test has high acceptance of null hypothesis for
almost all the pair of graphs from any population and MMD based test rejects the null hypothesis
always except when the graphs are the same. Whereas, the test using our proposed distance perform
well on large graphs from Social Networks datasets, for instance, S4 with other datasets and within
itself. This test performs well even for small graphs when compared to the other two tests.


-----

