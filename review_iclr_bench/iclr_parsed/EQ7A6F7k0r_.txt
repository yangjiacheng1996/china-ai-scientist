## QTN-VQC: AN END-TO-END LEARNING FRAME### WORK FOR QUANTUM NEURAL NETWORKS

**Anonymous authors**
Paper under double-blind review

ABSTRACT

The advent of noisy intermediate-scale quantum (NISQ) computers raises a crucial challenge to design quantum neural networks for fully quantum learning tasks.
To bridge the gap, this work proposes an end-to-end learning framework named
QTN-VQC, by introducing a trainable quantum tensor network (QTN) for quantum embedding on a variational quantum circuit (VQC). The architecture of QTN
is composed of a parametric tensor-train network for feature extraction and a tensor product encoding for quantum embedding. We highlight the QTN for quantum embedding in terms of two perspectives: (1) we theoretically characterize
QTN by analyzing its representation power of input features; (2) QTN enables an
end-to-end parametric model pipeline, namely QTN-VQC, from the generation of
quantum embedding to the output measurement. Our experiments on the MNIST
dataset demonstrate the advantages of QTN for quantum embedding over other
quantum embedding approaches.

1 INTRODUCTION

The state-of-the-art machine learning (ML), particularly based on deep neural networks (DNN),
has enabled a wide spectrum of successful applications ranging from the everyday deployment of
speech recognition (Deng et al., 2013) and computer vision (Sermanet et al., 2014) through to the
frontier of scientific research in synthetic biology (Jumper et al., 2021). Despite rapid theoretical
and empirical progress in DNN based regression and classification (Goodfellow et al., 2016), DNN
training algorithms are computationally expensive for many new scientific applications, such as
new drug discovery (Smalley, 2017), which requires computational resources that are beyond the
computational limits of classical hardwares (Freedman, 2019). Fortunately, the imminent advent
of quantum computing devices opens up new possibilities of exploiting quantum machine learning
(QML) (Biamonte et al., 2017; Schuld et al., 2015; Schuld & Petruccione, 2018; Schuld & Killoran,
2019; Saggio et al., 2021; Dunjko, 2021) to improve the computational efficiency of ML algorithms
in the new scientific domains.

Although the exploitation of quantum computing devices to carry out QML is still in its initial
exploratory stages, the rapid development in quantum hardware has motivated advances in quantum
neural networks (QNN) to run in noisy intermediate-scale quantum (NISQ) devices (Preskill, 2018;
Huggins et al., 2019; Huang et al., 2021; Kandala et al., 2017). A NISQ device means that not
enough qubits could be spared for quantum error correction, and the imperfect qubits have to be
directly used at the physical layer. Even though, a compromised QNN approach is proposed by
employing hybrid quantum-classical models that rely on the optimization of variational quantum
circuits (VQC) (Benedetti et al., 2019; Mitarai et al., 2018). The resilience of the VQC based
models to certain types of quantum noise errors and high flexibility concerning coherence time and
gate requirements (McClean et al., 2018) admit many practical implementations of QNN on NISQ
devices (Chen et al., 2020b; Yang et al., 2021; Du et al., 2020; 2021; Skolik et al., 2021; Dunjko
et al., 2016; Jerbi et al., 2021; Ostaszewski et al., 2021). One notable limitation in the current QNN
training pipeline is that the quantum embedding is not fully realizable in a quantum computer, which
may impede the learning of the QNN. Hence, this work proposes QTN-VQC to enable an end-toend trainable QNN, including data embedding to quantum measurements, that are easily realizable
in quantum devices, where QTN stands for the quantum tensor network (Or´us, 2019; Huckle et al.,
2013; Biamonte et al., 2017; Murg et al., 2010) for generating quantum embedding.


-----

|H |ψx⟩ = |g (x)⟩ θ θ|z 1 z 2 z S|
|---|---|


As shown in Figure 1, our QNN builds a unitary linear operator that consists of three main com
Classical data x

| 0⟩ _z1_

| 0⟩ _z2_

### Ux | 0⟩[⨂] [S] = | ψx⟩ Hθ | ψx⟩= | gθ(x)⟩

| 0⟩ _zS_

Quantum Embedding Generation Variational Quantum Circuit Measurement

_Hx_ _Hθ_ ℳ( | _gθ(x)⟩)_

Figure 1: An illustration of QNN based on VQC.

Input y Input y

Dense TTN
x x

| 0⟩ | 0⟩

| 0⟩ _Ux_ | 0⟩[⨂] _[S]_ = | _ψx⟩_ EmbeddingQuantum | 0⟩ _Ux_ | 0⟩[⨂] _[S]_ = | _ψx⟩_ EmbeddingQuantum

| 0⟩ | 0⟩

(a) A dense layer for dimension reduction (b) A Tensor-Train layer for dimension reduction

Figure 2: Different paradigms for quantum embedding. (a) a dense layer is used to generate low_dimensional vector x from a high-dimensional one y; (b) a TTN is used for dimension reduction._

ponents: (1) quantum embedding generation; (2) variational quantum circuit; (3) measurement.
Quantum embedding generation, also known as quantum encoding, applies a fixed unitary linear
operator Hx transforming classical vectors x to quantum states |ψx⟩ in a Hilbert space. This step
is an important aspect of designing quantum algorithms that directly impact the entire computation
cost of VQC and owns a characteristic of quantum superposition. Moreover, the VQC comprises
two types of quantum gates: (1) Controlled-NOT (CNOT) gates; (2) learnable parametric quantum
gates. The CNOT gates ensure the property of quantum entanglement through mutually connecting
the qubits, and the parametric quantum gates can be adjustable to best fit the quantum input states.
The model parameters of VQC should be optimized by employing variants of gradient descent algorithms during the training process. Those parametric quantum gates of VQC are similar to the
weights assigned to DNN, and such quantum circuits have been justified to be resilient to quantum
noises (Farhi et al., 2014; Kandala et al., 2017; McClean et al., 2016). Besides, the measurement
_M(|gθ(x)⟩) aims at projecting the quantum output states |gθ(x)⟩_ to one classical output zi.

This work focuses on quantum embedding generation because it is quite related to the practical usage
in machine learning applications in terms of computational cost and representation capability of
classical input features. In particular, we design a novel quantum tensor network (QTN) for quantum
embedding generation. More specifically, the QTN consists of a tensor-train network (TTN) for
dimension reduction and a quantum tensor encoding framework for outputting quantum embeddings.
The dimension reduction is a necessary procedure before the quantum encoding because only a
small number of qubits could be supported on available NISQ computers at this moment. A typical
approach for dimension reduction relies on a classical fully-connected layer, also known as a dense
layer, to convert high-dimensional input vectors y into low-dimensional ones x. However, since a
dense layer cannot be physically mapped on a quantum computer, much overhead has to be incurred
by frequently communicating between classical and quantum devices during the end-to-end training
pipeline.


-----

As shown in Figure 2 (b), one of our contribution is to leverage a tensor train network (TTN)
to replace the dense layer in Figure 2 (a). The benefits of applying TTN arise from two aspects:
(1) TTN can maintain the representation power of the dense layer, which will be justified in our
theorems; (2) TTN is a tensor network and can be flexibly placed in quantum computers, which
enables an end-to-end training process fully conducted in a quantum computer. Moreover, in this
work, a tensor product encoding (TPE) is delicately designed for generating quantum embedding,
which builds the relationship between a classical vector x and the corresponding quantum state |x⟩;
Besides, we further investigate the representation of QTN-VQC in terms of model size and nonlinear activation function used in TTN. We denote a QTN as the combination of TTN and TPE and
utilize QTN-VQC as a genuine end-to-end learning framework for QNN.

2 RELATED WORK

The work (Schuld & Petruccione, 2018; Biamonte et al., 2017; Dunjko & Briegel, 2018) demonstrate
that VQC shows great promise in surpassing the performance of classical ML. Prominent examples
of VQC based models include quantum approximate optimization algorithm (QAOA) (Farhi et al.,
2014), and quantum circuit learning (QCL) (Mitarai et al., 2018). Various architectures and geometries of VQC have been shown in tasks ranging from image classification (Henderson et al., 2020;
Chen et al., 2020a; Kerenidis et al., 2020) to reinforcement learning (Chen et al., 2020b).

As for quantum embedding, basis encoding is the process of associating classical input data in the
form of binary strings with the computational basis state of a quantum system (Leymann & Barzen,
2020). Similarly, amplitude encoding is a technique involving encoding data into the amplitudes of
a quantum state (Soklakov & Schack, 2006). Unfortunately, the computational cost of both quantum
embedding and amplitude encoding becomes exponentially expensive with the increasing number of
qubits (Schuld & Killoran, 2019). A new technique of angle embedding makes use of the quantum
gates to generate quantum states (Fu et al., 2011), but it cannot deal with the high-dimensional
feature inputs. Therefore, this work exploits the use of TTN for dimension reduction followed by a
TPE for generating quantum embedding.

In particular, this work employs the TTN for dimensionality reduction. The TTN model based
on TT decomposition in neural networks was first proposed in (Oseledets, 2011), and it could be
flexibly extended the convolutional neural network (CNN) (Garipov et al., 2016) and recurrent neural
network (RNN) (Tjandra et al., 2017). The empirical study of TTN on machine learning tasks
shows that TTN is capable of maintaining the DNN baseline results (Qi et al., 2020a; Yu et al.,
2017; Yang et al., 2017; Jin et al., 2020). However, to our best knowledge, no existing works have
applied TTN to QML. Besides, since the tensor network-based machine learning model like TTN is
closely related to quantum machine learning in terms of their model structures (Liu & Wang, 2018;
Gao et al., 2017), the QTN-VQC model can be directly regarded as the classical simulation of the
corresponding quantum machine learning. In addition to a classical dense layer, more complicated
architectures like AlexNet (Lloyd et al., 2020) could be used for dimension reduction, and we also
compare the performance between TTN and AlexNet-based models.

3 NOTATIONS

We denote R[I] as a I-dimensional real coordinate space, and R[I][1][×][I][2][×···][I][K] refers to a space of Korder tensors. The symbol W ∈ R[I][1][×][I][2][×···×][I][K] represents a K-order multi-dimensional tensor in
R[I][1][×][I][2][×···][I][K], and the symbols v ∈ R[I] and W ∈ R[I][×][J] represent a vector and a matrix, respectively.

For the notations of quantum computing, ∀ **v ∈** R[I], the symbol |v⟩ denotes a quantum state associated with a 2[I] -dimensional vector in a Hilbert space. Particularly, |0⟩ = [1 0][T] and |1⟩ = [0 1][T] .

The quantum gate RY (θ) means a Pauli-Y gate with a unitary operator as defined in Eq. (1), which
implies a qubit rotates the Bloch sphere along the Y-axis by a given angle θ.


_RY (θ) =_ cos _[θ]2_

_i sin_ _[θ]2_

−


_i sin_ _[θ]2_
_−_

cos _[θ]2_


(1)


Moreover, the operatorvectors is defined as ⊗i[I] ⊗=1[v]is a tensor product. Given the vectors[i][, which is a][ 2][I] [-dimensional vector and can provide a compact represen-] vi ∈ R[I], the tensor product of I


-----

_J1_ _J2_ _J3_ _JK−2_ _JK−1_ _JK_

_I1_ _I2_ _I3_ _IK−2_ _IK−1_ _IK_

_r0_ _r1_ _r2_ _rK−2_ _rK−1_ _rK_

(a) Tensor-Train Network: given a set of TT-ranks {r1, r2, . . ., rK}, a circle

represents a core tensor and each line is related to a dimension.


| 0⟩

| 0⟩

| 0⟩


|R (πx) Y 1|Col2|
|---|---|
|||

|R (πx) Y 2|Col2|
|---|---|
|||

|Col1|Col2|
|---|---|

|R (πx ) Y d|Col2|
|---|---|
|||


(b)  Tensor Product Encoding: RY(πx) denotes the Pauli-Y rotation gate


Figure 3: A demonstration of quantum tensor network for quantum embedding.

states oftation for | v0⟩1. Furthermore, for a scalar ⊗ **v2 ⊗· · · ⊗** **vI** . Similarly, the symbol v, the quantum state |0⟩[⊗] |[S]vmeans a tensor product of⟩ can be written as: _S quantum_


cos v
_v_ = cos v 0 + sin v 1 = _._
_|_ _⟩_ _|_ _⟩_ _|_ _⟩_ sin v
 

QTN-VQC: OUR PROPOSED END-TO-END LEARNING FRAMEWORK


(2)


This section introduces our proposed end-to-end learning framework, namely QTN-VQC in this
work. As shown in Figure 3, the QTN model includes two components (a) TTN and (b) TPE, which
will be separately introduced in Section 4.1 and Section 4.2. Moreover, Figure 4 illustrates the
framework of VQC and Section 4.3 is devoted to discussing the details of VQC.

4.1 TENSOR TRAIN NETWORK FOR DIMENSION REDUCTION

We leverage TTN (Novikov et al., 2015) for the dimension reduction of input features. TTN relies on the TT decomposition (Oseledets, 2011) and has been commonly employed in machine
learning tasks like speech processing (Qi et al., 2020b) and computer vision (Yang et al., 2017).
The TT decomposition assumes that given a set of TT-ranks _r0, r1, ..., rK_, a K-order tensor
_{_ _}_
_W ∈In more detail, given a set of indicesR[I][1][×][I][2][×···×][I][K]_ is factorized into the multiplication ofi1, i2, ..., iK, (i1, i2, ..., i 3-order tensorsK) is decomposed as: Xk ∈ R[r][k][−][1][×][I][k][×][r][k] .
_{_ _}_ _X_


_k(ik),_ (3)
_X_
_k=1_

Y


(i1, i2, ..., iK) =
_X_


where _ik_ [Ik], _k(ik)_ R[r][k][−][1][×][r][k] . Since r0 = r1 = 1, the term _k=1_
_∀_ _∈_ _X_ _∈_ _[X][k][(][i][k][)][ is a scalar value.]_

TTN employs the TT decomposition in a dense layer and is explicitly demonstrated in Figure 3 (a).
In more detail, for an input tensor X ∈ R[I][1][×][I][2][×···×][I][K] and an output tensor[Q][K] _Y ∈_ R[J][1][×][J][2][×···×][J][K],
we achieve


_I1_

_i1=1_

X

_I1_


_I2_

_· · ·_
_i2=1_

X

_I2_


_IK_

((i1, j1), (i2, j2), ..., (iK, jK)) (i1, i2, ..., iK)
_W_ _X_
_iK_ =1

X

_IK_ _K_ _K_


(j1, j2, ..., jK) =
_Y_


_k(ik, jk)_

_· · ·_ _W_

_i1=1_ _i2=1_ _iK_ =1 _k=1_

X X X Y

_K_ _Ik_

_k(ik, jk)_ _k(ik)_

_k=1_ _ik=1_ _W_ _X_ !

Y X

_K_

_k(jk),_
_Y_
_k=1_

Y


_k(ik)_
_X_
_k=1_

Y


(4)


-----

where _k(ik)_ R[r][k][−][1][×][r][k], and _k(jk)_ R[r][k][−][1][×][r][k] which results in a scalar _k=1_
cause of the ranks X _∈_ _r0 = r1 = 1 Y_ ; W(( ∈i1, j1), (i2, j2), ..., (iK, jK)) is closely associated with[Y][k][(][j][k][)][ be-]
(m1, m2, ..., mK) as defined in Eq. (3), if each index mk = ik _jk is set. The multi-dimensional_
_Wlinear activation function, e.g., Sigmoid, Tanh, and ReLU, is imposed upon the tensortensor W is decomposed into the multiplication of 4-order tensors × Wk ∈_ R[r][k][−][1][×][I][Q][k][×][K][J] Y[k][×]. Compared[r][k] . A nonwith a dense layer with _k=1_ _[I][k][J][k][ parameters, a TTN owns as few as][ P]k[K]=1_ _[r][k][r][k][−][1][I][k][J][k][ trainable]_
parameters.

When a TTN is utilized for the dimension reduction, the high-dimensional input vector[Q][K] **x ∈** R[I] is
first reshaped into a tensor X ∈ R[I][1][×][I][2][×···×][I][K], and then we can represent X as a TT format that
goes through TTN. The outputs of TTN can be converted back to a tensor Y ∈ R[J][1][×][J][2][×···×][J][K],
which is further reshaped to a lower dimensional vectorK **y ∈** R[J] . Here, we define _k=1_ _[I][k][ =][ I][ and]_
_k=1_ _[J][k][ =][ J][. Moreover, the computational complexities of TTN and the related dense layer are in]_
the same scale, which is discussed in (Yang et al., 2017). [Q][K]
Q

Eq. (4) suggests that TTN is a multi-dimensional extension of a dense layer, where the trainable
weight matrix of a dense layer is changed to the learnable core tensors. Additionally, many empirical
studies demonstrate that a TTN is capable of maintaining the baseline results of the dense layer (Qi
et al., 2020b; Yang et al., 2017; Novikov et al., 2015; Qi et al., 2020a). More significantly, since
TTN can be flexibly mapped into a quantum circuit, the quantumness inherent in TTN brings great
advantages over other architectures like the dense layer. In other words, although TTN is treated
classically, it is possible to substitute equivalent quantum circuits for TTN when more qubits become
available (Du et al., 2020), which implies that QTN-VQC stands for a genuine end-to-end QNN
learning architecture on a quantum computer.

Furthermore, since the gradient exploding and diminishing problems are serious issues in the TTN
training. To avoid those training problems, we only consider 3-order core tensors and small TT-ranks
to configure a simple TTN in our experimental simulations. Our theoretical analysis of QTN-VQC
based on Theorem 3 in Section 5 suggests that the representation power is not related to TT-ranks
and the tensor order K, thus small TT-ranks and the tensor order K are preferred. In particular, a
lower K can significantly reduce the computational cost and speed up the convergence rate.

4.2 TENSOR PRODUCT ENCODING

In this subsection, we first introduce Theorem 1, and then we derive our TPE associated with the
circuits in Figure 3 (b).
**Theorem 1. Given the classical vector x = [x1, x2, ..., xI** ][T] _∈_ R[I] _, a TPE as shown in Figure 3 (b)_
_can result in a quantum state |x⟩_ _with the following complete vector representation as:_
_⊗i[I]=1[R][Y]_ [(2][x][i][)] _|0⟩[⊗][I]_ = cossin x x11 _⊗_ cossin x x22 _⊗· · · ⊗_ cossin x xII = |x⟩. (5)
     
  

_Proof. Since each element xi in the vector x can be written as |xi⟩_ = cos xi|0⟩ + sin xi|1⟩, the
quantum state |x⟩ can be written as:

**x** = cos x1 cos x2 cos xI _._ (6)
_|_ _⟩_ sin x1 _⊗_ sin x2 _⊗· · · ⊗_ sin xI
     

When the vector x goes through the quantum tensor network, which implies the following as:
_RY (2xi)|0⟩_ = cosxi|0⟩ + sinxi|1⟩ = |xi⟩. (7)
The preceding equation, in turn, implies that Eq. (5).

Theorem 1 builds a connection between the vector x and the quantum state |x⟩, and the resulting
_|x⟩_ is taken as the quantum embedding as the inputs to VQC. Since ⊗i[I]=1[R][Y][ (2][x][i][)][ is a reversely]
unitary linear operator, there is no information loss incurred during the stage of quantum encoding.
Furthermore, if the input is multiplied with a constant _[π]2_ [, we obtain the following term as:]

_⊗i[I]=1[R][Y]_ [(][πx][i][)] _|0⟩[⊗][I]_ = cos(sin(πxπx11)) _⊗_ cos(sin(πxπx22)) _⊗· · · ⊗_ cos(sin(πxπxII)) _,_ (8)
     
  

which corresponds to Figure 3 (b).


-----

|Col1|· R (α) R (β) R(γ) · ⨁ X 1 Y 1 Z 1 R (α) R (β) R(γ) ⨁ · X 2 Y 2 Z 2 R (α) R (β) R(γ) ⨁ · X 3 Y 3 Z 3 R (α) R (β) R(γ) ⨁ X 4 Y 4 Z 4|Col3|
|---|---|---|
||||
||· ⨁||
||· ⨁||
||||


| 0⟩ ⨁ _RX(α1)_ _RY(β1)_ _RZ(γ1)_ _Z1_

| 0⟩ ⨁ _RX(α2)_ _RY(β2)_ _RZ(γ2)_ _Z2_

| 0⟩ ⨁ _RX(α3)_ _RY(β3)_ _RZ(γ3)_ _Z3_

| 0⟩ ⨁ _RX(α4)_ _RY(β4)_ _RZ(γ4)_ _Z4_

(a) Variational quantum circuit: the dashed square indicates repeated model with CNOT gates for entangling

# ··· ·

quantum states, and RX(α), RY(β), RZ(γ) represent Pauli-X, Y, Z gates with free parameters α, β, γ

_RY(β) =_ cossin _β2β2_ −sincos _β2β2_ ⨁· = 1000 0100 0001 0010 _RX(α) = [−cosi sin[α]2_ _[α]2_ −cosi sin[α]2 _[α]2_ []] _RZ(γ) =_ exp(−0 _i_ 2γ [)] exp(0i 2γ [)]

Pauli-Y CNOT gate Pauli-X Pauli-Z

(b) Matrix representation for the Quantum gates applied in the VQC

Figure 4: A framework of variational quantum circuit.


4.3 THE FRAMEWORK OF VARIATIONAL QUANTUM CIRCUIT

The framework of VQC is shown in Figure 4 (a), where 4 qubit wires are taken into account, and
the CNOT gates aim at mutually entangling the channels such that _x1_, _x2_, _x3_ and _x4_ lie
_|_ _⟩_ _|_ _⟩_ _|_ _⟩_ _|_ _⟩_
in the same entanglement state. The Pauli-X, Y, Z gates RX (·), RY (·) and RZ(·) with learnable
parameters (α1, β1, γ1), (α2, β2, γ2), (α3, β3, γ3), (α4, β4, γ4) are built to set up the learnable part.
Being similar to the unitary operators of RY (α), RX (β) and RZ(γ), which are defined in Figure 4
(b), are separately associated with the rotations along X-axis and Z-axis by the given angles of β and
_γ. Besides, the quantum circuits in the dash square can be repeatedly copied to compose a deeper_
architecture. The outputs of VQC are connected to the measurement which projects the quantum
states into a certain quantum basis that becomes a classical scalar zi.

As for the end-to-end training paradigm for QTN-VQC, the learnable parameters come from the
VQC and TTN models, and they should be updated by applying the back-propagation algorithm
based on the Adam optimizer. Given D qubits and H depths, there are totally 3DH trainable
parameters for VQC. Consequently, there are _k=1_ _[r][k][−][1][r][k][I][k][J][k][ + 3][DH][ parameters for QTN-]_
VQC. On the other hand, the Dense-VQC model possesses more model parameters than QTN-VQC
([Q][K]k=1 _[r][k][−][1][r][k][I][k][J][k][ + 3][DH][ vs.][ P]k[K]=1_ _[r][k][−][1][r][k][I][k][P][J][k][K][ + 3][DH][).]_

5 CHARACTERIZING REPRESENTATION POWER OF QTN-VQC

This section focuses on analyzing the representation power of QTN-VQC. As shown in Figure 5,
given d qubits and a target quantum state |z⟩ = ⊗d[D]=1[|][z][d][⟩][, since][ H][θ][ is known as a linear operator and]
_Tx is defined as a definite mapping from input x to the unitary matrix Ux, the representation power_
of QTN-VQC is determined by how TTN can approximate the classical vectorunderstand the expressiveness of TTN, we first start with the discussion on the expressive capability Tx[−][1](Hθ[−][1][|][z][⟩][)][. To]
of Dense-VQC (a dense layer is taken for dimension reduction) and then generalize it to QTN-VQC.
Based on the universal approximation theorem (Cybenko, 1989; Barron, 1994) for a feed-forward
neural network, we derive the following theorem as:
_with a dense layer connecting toTheorem 2. Given a target vector D T qubits, thenx[−][1](Hθ[−][1][|][z][⟩][)][, there exists a feed-forward neural network][ f][dense]_


_|| Tx[−][1](Hθ[−][1][|][z][⟩][)][ −]_ _[f][dense][(][y][)][ ||]1_ _[≤]_


(9)


_where the activation function tanh(·) is imposed upon the dense layer, and C is a constant associ-_
_ated with the target vector Tx[−][1](Hθ[−][1][|][z][⟩][)][.]_


-----

|Col1||0⟩|0⟩|0⟩|0⟩|0⟩|0⟩ Tensor Product Encoder Hx|Variational Quantum Circuit H θ|
|---|---|---|
||||


Dimension Reduction

(TTN / Dense)


Figure 5: An illustration of analyzing the representation power of QTN-VQC.

Since TTN is a compact TT representation of a dense layer, by modifying Theorem 2 for TTN, we
can also derive the upper bound on the approximation error as follows:

_layer connecting toTheorem 3. Given a target vector D qubits, then_ _Tx[−][1](Hθ[−][1][|][z][⟩][)][, there exists a TTN, denoted as][ f][T T N]_ _[, with a TT]_


_|| Tx[−][1](Hθ[−][1][|][z][⟩][)][ −]_ _[f][T T N]_ [(][y][)][ ||]1 _[≤]_


_C_
_√Dk_ _,_ (10)


_k=1_


_where_ _k=1_ _[D][k][ =][ D][, the Sigmoid activation function is imposed upon the TTN model,][ K][ denotes]_
_the multi-dimensional order, C is a constant associated with the target vector Tx[−][1](Hθ[−][1][|][z][⟩][)][.]_

[Q][K]

Comparing the two upper bounds, it is observed that TTN can attain an identical upper bound as
the dense layer on the approximation error because _k=1_ _[D][k][ =][ D][. That implies that TTN can]_
at least maintain the representation power of a dense layer. Besides, the number of qubits D is a
key factor determining the upper bound on the approximation error. However, D is a small fixed

[Q][K]
number on a NISQ device, and a larger number of qubits D is expected to further improve the
representation power of QTN-VQC. However, the computational costs of classical simulation may
grow exponentially with the increasing number of qubits, and a small number of qubits have to be
considered in practice.

6 EXPERIMENTS AND RESULTS

6.1 EXPERIMENTAL SETUPS

We assess our QTN-VQC based end-to-end learning system on the standard MNIST. MNIST is a
dataset for the task of 10 digit classification, where there are 50000 and 10000 28 × 28 image data
assigned for training and testing, respectively. The full MNIST dataset is challenging for quantum
machine learning algorithms, and many works only consider 2-digit classification on the MNIST
task (Wang et al., 2021; Chen et al., 2020a). Moreover, the image data are separately reshaped
into 784 dimensional input vectors. Dense-VQC and PCA-VQC are taken as our experimental
baselines to compare with our QTN-VQC model. Dense-VQC denotes that a dense layer is used for
dimension reduction, and PCA-VQC refers to using principal component analysis (PCA) to extract
low-dimensional features before training the VQC parameters.

As for the experiments of QTN-VQC, the image data are reshaped into 3-order 7 × 16 × 7 tensors.
We set small TT-ranks as {1, 2, 2, 1} to reduce the computational cost of TTN. the image data are
represented as the TT format according to Eq. (3) before going through the TTN model. Since 8
qubits are used for the quantum encoding, the output of TTN needs to configure the tensor format
as 2 × 2 × 2, which results in 8 dimensional output vectors. Besides, the model parameters of
QTN-VQC are randomly initialized based on the Gaussian distribution, and the back-propagation
algorithm is applied to train the models. The Sigmoid function is utilized for the hidden layers of
TTN.

To be consistent with QTN-VQC, the weight of the dense layer for Dense-VQC is configured as the
shape of 784×8. Although Dense-VQC is a hybrid classical-quantum model, the training process of
Dense-VQC can also be set as an end-to-end pipeline and the weights of the dense layer are updated
during the training stage. The Sigmoid function is used for the dense layer. On the other hand, PCA


-----

is employed to reduce the feature dimension to 8, and the resulting low-dimensional features are
further encoded into quantum states. Consequently, PCA-VQC admits the VQC parameters solely
to be updated during the training stage. A standard AlexNet (Iandola et al., 2016) is employed to
constitute an AlexNet-VQC to compare the performance.

Moreover, 6 VQC layers are constructed to form a deep model, and the outputs of the VQC model
are connected to 10 classes with a non-trainable matrix. The back-propagation algorithm based on
the Adam optimizer with a learning rate of 0.001 is employed for the model training. The loss of
cross-entropy (CE) is utilized as the objective function during the training stage, and it is also taken
as the metric to evaluate the model performance. We leverage the tools of Pennylane (Bergholm
et al., 2018) and PyTorch (Paszke et al., 2019) to simulate the model performance. In particular,
we separately simulate the model performance with noiseless quantum circuits and noisy quantum
circuits corrupted by quantum noises from IBM quantum machines.

6.2 EXPERIMENTAL RESULTS OF NOISELESS QUANTUM CIRCUIT

Table 1 shows the final results of the models on the test dataset. QTN-VQC owns much fewer model
parameters than Dense-VQC (328 vs. 6416) and attains even higher classification accuracy than
Dense-VQC (91.43% vs. 88.54%) and lower loss values than Dense-VQC (0.3090 vs. 0.4132).
However, PCA-VQC with 144 trainable VQC parameters attains the worst performance by all metrics, which implies that a trainable quantum embedding is of significance to boost experimental
performance. Although our empirical results cannot reach the state-of-the-art classification performance of classical ML algorithms, our empirical results demonstrate the advantages of QTN-VQC
over the PCA-VQC and Dense-VQC counterparts. With the development of more powerful quantum devices supporting more qubits, the representation power of QTN-VQC can be improved and
better experimental results could be attained. Moreover, AlexNet-VQC achieves better results than
QTN-VQC (92.81%vs.91.43%), but it involves more model parameters than QTN-VQC.

Table 1: Empirical results on the MNIST test dataset under the noiseless quantum circuit setting.

|Models|Params|CE|Acc (%).|
|---|---|---|---|
|PCA-VQC|144|0.5877|82.48 1.02 ±|
|Dense-VQC|6416|0.4132|88.54 0.73 ±|
|AlexNet-VQC|3.25 106 ×|0.2562|92.81 0.47 ±|
|QTN-VQC|328|0.3090|91.43 ± 0.51|


Models Params CE Acc (%).

PCA-VQC 144 0.5877 82.48 ± 1.02

Dense-VQC 6416 0.4132 88.54 ± 0.73

AlexNet-VQC 3.25 × 10[6] 0.2562 92.81 ± 0.47

QTN-VQC 328 **0.3090** **91.43 ± 0.51**


6.3 EXPERIMENTAL RESULTS OF NOISY QUANTUM CIRCUIT

To empirically validate the effectiveness of our proposed VQC algorithm, we proceed with the
simulation of the practical experiments with noisy quantum circuits. More specifically, we follow
an established noisy circuit experiment with the NISQ device suggested by (Chen et al., 2020b). One
major advantage of the setups is to observe the robustness and preserve the quantum advantages of
a deployed VQC with physical settings being close to quantum processing unit (QPU) experiments
without an executive queuing time. As for the detailed setup, we first use an IBM Q 20-qubit
machine to collect channel noise in the real scenario for a deployed VQC and upload the machine
noise into our Pennylane-Qiskit simulator (denoted as Accq20. We provide a depolarizing noisy
circuit simulation (denoted as Accdepo) based on a depolarizing channel attained from (Nielsen &
Chuang, 2010) with a noise level of 0.1. As shown in Table 2, the quantum noise brings about
the performance degradation of all models, but our proposed QTN-VQC consistently outperforms
PCA-VQC and Dense-VQC in the condition of noisy quantum circuits. In particular, QTN-VQC
can even outperform the AlexNet-VQC counterpart in noisy circuit conditions.

6.4 FURTHER DISCUSSIONS

The above experimental results show the advantages of QTN-VQC over Dense-VQC and PCAVQC in the scenarios with noiseless and noisy quantum circuits. Next, we will further discuss the
representation power of QTN-VQC based on two factors: (1) the activation function used in TTN;
(2) the number of qubits.


-----

Table 2: Empirical results on the MNIST test dataset under the noisy quantum circuit setting.

|Models|Params|Acc (%) q20|Acc (%) depo|
|---|---|---|---|
|PCA-VQC|144|81.23 1.34 ±|83.12 1.17 ±|
|Dense-VQC|6416|84.55 1.22 ±|86.09 1.04 ±|
|AlexNet-VQC|3.25 106 ×|87.46 1.34 ±|87.86 1.08 ±|
|QTN-VQC|328|88.12 ± 1.09|89.32 ± 1.07|



6.4.1 THE ACTIVATION FUNCTION USED IN TTN

Table 3 compares the results of QTN-VQC based on different activation functions. Our simulation on noiseless quantum circuits shows that the non-linear activation functions can bring more
performance gain than a linear one, but the Sigmoid function attains a better performance than the
Tanh and ReLU counterparts in our experiments. Our experiments also correspond to the universal
approximation theory for QTN-VQC in Theorem 3.

Table 3: Comparing performance of QTN-VQC with and without activation function.

|Models|CE|Acc (%).|
|---|---|---|
|QTN-VQC (Linear)|0.4958|86.16 0.65 ±|
|QTN-VQC (Tanh)|0.4792|87.12 0.51 ±|
|QTN-VQC (ReLU)|0.3764|89.56 0.49 ±|
|QTN-VQC (Sigmoid)|0.3090|91.43 ± 0.54|


Models CE Acc (%).

QTN-VQC (Linear) 0.4958 86.16 ± 0.65

QTN-VQC (Tanh) 0.4792 87.12 ± 0.51

QTN-VQC (ReLU) 0.3764 89.56 ± 0.49

QTN-VQC (Sigmoid) **0.3090** **91.43 ± 0.54**


6.4.2 THE NUMBER OF QUBITS

Finally, we investigate the effects of the number of qubits on the performance of QTN-VQC by
increasing the qubits from 8 to 12 and 16. Accordingly, the output of TTN is configured as a
tensor format of 2 × 3 × 2, and the model size is increased from 328 to 464 and 600, respectively.
Our experiments show that the baseline performance of QTN-VQC can be further improved by
increasing the number of qubits, which implies that more qubits are likely to possess higher accuracy.

Table 4: Comparing performance of QTN-VQC with fewer qubits.

|Models|Params|CE|Acc (%)|
|---|---|---|---|
|QTN-VQC (8 qubits)|328|0.3090|91.43 0.51 ±|
|QTN-VQC (12 qubits)|464|0.2679|92.36 0.62 ±|
|QTN-VQC (16 qubits)|600|0.2355|92.98 ± 0.52|


Models Params CE Acc (%)

QTN-VQC (8 qubits) 328 0.3090 91.43 ± 0.51

QTN-VQC (12 qubits) 464 0.2679 92.36 ± 0.62

QTN-VQC (16 qubits) 600 **0.2355** **92.98 ± 0.52**


7 CONCLUSIONS

This work proposes a genuine end-to-end learning framework for quantum neural networks based on
QTN-VQC. QTN consists of a TTN for dimension reduction and a TPE framework for generating
quantum embedding. The TTN model is a compact representation of a dense layer to classically
simulate quantum machine learning algorithms. Our theorem on the representation of QTN-VQC
shows that the number of qubits is inversely related to the approximation error of QTN-VQC and the
non-linear activation plays an important role. Our experiments compare our proposed QTN-VQC
with Res-VQC, Dense-VQC, and PCA-VQC. Our simulated results demonstrate that QTN-VQC
obtains better experimental performance than Dense-VQC and PCA-VQC with both noiseless and
noisy quantum circuits, and it achieves marginally worse performance than AlexNet-VQC. Besides,
our results justify our theorem on the representation power of QTN-VQC.


-----

REFERENCES

Andrew R Barron. Approximation and Estimation Bounds for Artificial Neural Networks. Machine
_Learning, 14(1):115–133, 1994._

Marcello Benedetti, Erika Lloyd, Stefan Sack, and Mattia Fiorentini. Parameterized Quantum Circuits as Machine Learning Models. Quantum Science and Technology, 4(4):043001, 2019.

Ville Bergholm, Josh Izaac, Maria Schuld, Christian Gogolin, M Sohaib Alam, Shahnawaz
Ahmed, Juan Miguel Arrazola, Carsten Blank, Alain Delgado, Soran Jahangiri, et al. Pennylane: Automatic Differentiation of Hybrid Quantum-Classical Computations. arXiv preprint
_arXiv:1811.04968, 2018._

Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth Lloyd.
Quantum Machine Learning. Nature, 549(7671):195–202, 2017.

Samuel Yen-Chi Chen, Chih-Min Huang, Chia-Wei Hsing, and Ying-Jer Kao. Hybrid QuantumClassical Classifier Based on Tensor Network and Variational Quantum Circuit. arXiv preprint
_arXiv:2011.14651, 2020a._

Samuel Yen-Chi Chen, Chao-Han Huck Yang, Jun Qi, Pin-Yu Chen, Xiaoli Ma, and Hsi-Sheng
Goan. Variational Quantum Circuits for Deep Reinforcement Learning. IEEE Access, 8:141007–
141024, 2020b.

George Cybenko. Approximation by Superpositions of A Sigmoidal Function. Mathematics of
_Control, Signals and Systems, 2(4):303–314, 1989._

Li Deng, Jinyu Li, Jui-Ting Huang, Kaisheng Yao, Dong Yu, Frank Seide, Michael Seltzer, Geoff
Zweig, Xiaodong He, Jason Williams, et al. Recent Advances in Deep Learning for Speech
Research at Microsoft. In Proc. IEEE International Conference on Acoustics, Speech and Signal
_Processing, pp. 8604–8608, 2013._

Yuxuan Du, Min-Hsiu Hsieh, Tongliang Liu, and Dacheng Tao. Expressive Power of Parametrized
Quantum Circuits. Physical Review Research, 2(3):033125, 2020.

Yuxuan Du, Min-Hsiu Hsieh, Tongliang Liu, Dacheng Tao, and Nana Liu. Quantum Noise Protects
Quantum Classifiers Against Adversaries. Physical Review Research, 3(2):023153, 2021.

Vedran Dunjko. Inside Quantum Black Boxes. Nature Physics, pp. 1–2, 2021.

Vedran Dunjko and Hans J Briegel. Machine Learning & Artificial Intelligence in the Quantum
Domain: A Review of Recent Progress. Reports on Progress in Physics, 81(7):074001, 2018.

Vedran Dunjko, Jacob M Taylor, and Hans J Briegel. Quantum-Enhanced Machine Learning. Phys_ical Review Letters, 117(13):130501, 2016._

Edward Farhi, Jeffrey Goldstone, and Sam Gutmann. A Quantum Approximate Optimization Algorithm. arXiv preprint arXiv:1411.4028, 2014.

David H Freedman. Hunting for New Drugs with AI. Nature, 576(7787):S49–S53, 2019.

Yangguang Fu, Mingyue Ding, and Chengping Zhou. Phase Angle-Encoded and Quantum-Behaved
Particle Swarm Optimization Applied to Three-Dimensional Route Planning for UAV. _IEEE_
_Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans, 42(2):511–526,_
2011.

Xun Gao, Zhengyu Zhang, and Luming Duan. An Efficient Quantum Algorithm for Generative
Machine Learning. arXiv preprint arXiv:1711.02038, 2017.

Timur Garipov, Dmitry Podoprikhin, Alexander Novikov, and Dmitry Vetrov. Ultimate Tensorization: Compressing Convolutional and FC Layers Alike. arXiv preprint arXiv:1611.03214, 2016.

Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep Learning, volume 1.
MIT Press, 2016.


-----

Maxwell Henderson, Samriddhi Shakya, Shashindra Pradhan, and Tristan Cook. Quanvolutional
Neural Networks: Powering Image Recognition with Quantum Circuits. Quantum Machine In_telligence, 2(1):1–9, 2020._

Hsin-Yuan Huang, Michael Broughton, Masoud Mohseni, Ryan Babbush, Sergio Boixo, Hartmut
Neven, and Jarrod R McClean. Power of Data in Quantum Machine Learning. Nature Communi_cations, 12(1):1–9, 2021._

Thomas Huckle, Konrad Waldherr, and Thomas Schulte-Herbr¨uggen. Computations in Quantum
Tensor Networks. Linear Algebra and its Applications, 438(2):750–781, 2013.

William Huggins, Piyush Patil, Bradley Mitchell, K Birgitta Whaley, and E Miles Stoudenmire.
Towards Quantum Machine Learning with Tensor Networks. Quantum Science and Technology,
4(2):024001, 2019.

Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt
Keutzer. SqueezeNet: AlexNet-level Accuracy with 50x Fewer Parameters and¡ 0.5 MB Model
Size. arXiv preprint arXiv:1602.07360, 2016.

Sofiene Jerbi, Casper Gyurik, Simon Marshall, Hans J Briegel, and Vedran Dunjko. Variational
Quantum Policies for Reinforcement Learning. arXiv preprint arXiv:2103.05577, 2021.

Xuanyu Jin, Jiajia Tang, Xianghao Kong, Yong Peng, Jianting Cao, Qibin Zhao, and Wanzeng Kong.
CTNN: A Convolutional Tensor-Train Neural Network for Multi-Task Brainprint Recognition.
_Proc. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 29:103–112, 2020._

John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger,
Kathryn Tunyasuvunakool, Russ Bates, Augustin Z´[ˇ] ıdek, Anna Potapenko, et al. Highly Accurate
Protein Structure Prediction with AlphaFold. Nature, 596(7873):583–589, 2021.

Abhinav Kandala, Antonio Mezzacapo, Kristan Temme, Maika Takita, Markus Brink, Jerry M
Chow, and Jay M Gambetta. Hardware-Efficient Variational Quantum Eigensolver for Small
Molecules and Quantum Magnets. Nature, 549(7671):242–246, 2017.

Iordanis Kerenidis, Jonas Landman, and Anupam Prakash. Quantum Algorithms for Deep Convolutional Neural Networks. In Proc. International Conference on Learning Representations, 2020.

Frank Leymann and Johanna Barzen. The Bitter Truth About Gate-Based Quantum Algorithms In
the NISQ Era. Quantum Science and Technology, 5(4):044007, 2020.

Jin-Guo Liu and Lei Wang. Differentiable Learning of Quantum Circuit Born Machines. Physical
_Review A, 98(6):062324, 2018._

Seth Lloyd, Maria Schuld, Aroosa Ijaz, Josh Izaac, and Nathan Killoran. Quantum Embeddings for
Machine Learning. arXiv preprint arXiv:2001.03622, 2020.

Jarrod R McClean, Jonathan Romero, Ryan Babbush, and Al´an Aspuru-Guzik. The Theory of
Variational Hybrid Quantum-Classical Algorithms. New Journal of Physics, 18(2):023023, 2016.

Jarrod R McClean, Sergio Boixo, Vadim N Smelyanskiy, Ryan Babbush, and Hartmut Neven. Barren Plateaus in Quantum Neural Network Training Landscapes. Nature Communications, 9(1):
1–6, 2018.

Kosuke Mitarai, Makoto Negoro, Masahiro Kitagawa, and Keisuke Fujii. Quantum Circuit Learning.
_Physical Review A, 98(3):032309, 2018._

Valentin Murg, Frank Verstraete, Ors Legeza, and Reinhard M Noack. Simulating Strongly Corre-[¨]
lated Quantum Systems with Tree Tensor Networks. Physical Review B, 82(20):205105, 2010.

Michael A Nielsen and Isaac Chuang. Quantum Computation and Quantum Information. Cambridge, 2010.

Alexander Novikov, Dmitry Podoprikhin, Anton Osokin, and Dmitry Vetrov. Tensorizing Neural
Networks. In Proc. Advances in Neural Information Processing Systems, 2015.


-----

Rom´an Or´us. Tensor Networks for Complex Quantum Systems. Nature Reviews Physics, 1(9):
538–550, 2019.

Ivan V Oseledets. Tensor-Train Decomposition. SIAM Journal on Scientific Computing, 33(5):
2295–2317, 2011.

Mateusz Ostaszewski, Lea M Trenkwalder, Wojciech Masarczyk, Eleanor Scerri, and Vedran Dunjko. Reinforcement Learning for Optimization of Variational Quantum Circuit Architectures.
_arXiv preprint arXiv:2103.16089, 2021._

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An Imperative Style,
High-Performance Deep Learning Library. In Proc. Advances in Neural Information Processing
_Systems, volume 32, pp. 8026–8037, 2019._

John Preskill. Quantum Computing in the NISQ Era and Beyond. Quantum, 2:79, August 2018.
ISSN 2521-327X.

Jun Qi, Hu Hu, Yannan Wang, Chao-Han Huck Yang, Sabato Marco Siniscalchi, and Chin-Hui
Lee. Exploring deep hybrid tensor-to-vector network architectures for regression based speech
enhancement. arXiv preprint arXiv:2007.13024, 2020a.

Jun Qi, Hu Hu, Yannan Wang, Chao-Han Huck Yang, Sabato Marco Siniscalchi, and Chin-Hui
Lee. Tensor-to-Vector Regression for Multi-Channel Speech Enhancement Based on Tensor-Train
Network. In Proc. IEEE International Conference on Acoustics, Speech and Signal Processing,
pp. 7504–7508, 2020b.

Valeria Saggio, Beate E Asenbeck, Arne Hamann, Teodor Str¨omberg, Peter Schiansky, Vedran Dunjko, Nicolai Friis, Nicholas C Harris, Michael Hochberg, Dirk Englund, et al. Quantum Speed-ups
in Reinforcement Learning. In Quantum Nanophotonic Materials, Devices, and Systems 2021,
volume 11806, pp. 118060N. International Society for Optics and Photonics, 2021.

Maria Schuld and Nathan Killoran. Quantum Machine Learning in Feature Hilbert Spaces. Physical
_Review Letters, 122(4):040504, 2019._

Maria Schuld and Francesco Petruccione. _Supervised Learning with Quantum Computers, vol-_
ume 17. Springer, 2018.

Maria Schuld, Ilya Sinayskiy, and Francesco Petruccione. An Introduction to Quantum Machine
Learning. Contemporary Physics, 56(2):172–185, 2015.

Pierre Sermanet, David Eigen, Xiang Zhang, Micha¨el Mathieu, Rob Fergus, and Yann LeCun. Overfeat: Integrated Recognition, Localization and Detection Using Convolutional Networks. In Proc.
_International Conference on Learning Representations, 2014._

Andrea Skolik, Sofiene Jerbi, and Vedran Dunjko. Quantum Agents in The Gym: A Variational
Quantum Algorithm for Deep Q-Learning. arXiv preprint arXiv:2103.15084, 2021.

Eric Smalley. AI-Powered Drug Discovery Captures Pharma Interest. Nature Biotechnology, 35(7):
604–606, 2017.

Andrei N Soklakov and R¨udiger Schack. Efficient State Preparation for A Register of Quantum Bits.
_Physical review A, 73(1):012307, 2006._

Andros Tjandra, Sakriani Sakti, and Satoshi Nakamura. Compressing Recurrent Neural Network
with Tensor-Train. In Proc. International Joint Conference on Neural Networks, pp. 4451–4458,
2017.

Hanrui Wang, Yongshan Ding, Jiaqi Gu, Yujun Lin, David Z Pan, Frederic T Chong, and Song
Han. Quantumnas: Noise-Adaptive Search for Robust Quantum Circuits. _arXiv preprint_
_arXiv:2107.10845, 2021._


-----

Chao-Han Huck Yang, Jun Qi, Samuel Yen-Chi Chen, Pin-Yu Chen, Sabato Marco Siniscalchi,
Xiaoli Ma, and Chin-Hui Lee. Decentralizing Feature Extraction with Quantum Convolutional
Neural Network for Automatic Speech Recognition. In Proc. IEEE International Conference on
_Acoustics, Speech and Signal Processing, pp. 6523–6527, 2021._

Yinchong Yang, Denis Krompass, and Volker Tresp. Tensor-Train Recurrent Neural Networks for
Video Classification. In Proc. International Conference on Machine Learning, pp. 3891–3900,
2017.

R Yu, S Zheng, A Anandkumar, and Y Yue. Long-term Forecasting Using Tensor-Train RNNs.
_arXiv preprint arXiv:1711.00073, 31, 2017._


-----

A APPENDIX

The section of appendix includes the proofs for Theorem 2 and Theorem 3.

A.1 PROOF FOR THEOREM 2

_Proof. Theorem 2 is derived from the modification of the universal approximation theory proposed_
by Barron (1994); Cybenko (1989). The universal approximation theory is shown in Lemma 1,
which suggests that a feed-forward neural network with d neurons can approximate any continuous
function with arbitrarily small ϵ.

**Lemma 1. Given a continuous target function** _f[ˆ] : R[I]_ _→_ R, we can employ a 2-layer neural network
_with a non-linear activation f : R[I]_ _→_ R, such that


_|f[ˆ] −_ _f_ _| ≤_


_C ˆf_ _[,]_ (11)


_where J denotes the number of neurons, and C ˆf_ _[is a constant associated with][ ˆ]f_ _. In particular, for_
_r ≥_ 1, Cf satisfies the following condition as:

_f[ˆ]_ + _f_ _C ˆf_ _[,]_ (12)
_||_ _||∞_ _||D[k][ ˆ]||∞_ _≤_

_k,1≤_ _[k]X[(][k]2[−][1)]_ _≤r_

_T_
_where D[k][ ˆ]f =_ _∇f,[ˆ]_ _∇[2][ ˆ]f, ..., ∇[k][ ˆ]f_ _._
h i

To associate Lemma 1 with our Theorem 2, the target function is replaced with the target vector
_HX[−][1][H]θ[−][1][|][z][⟩][, then there is a neural network with a dense layer connected to][ D][ qubits such that]_


_||HX[−][1][H]θ[−][1][|][z][⟩−]_ _[f][dense][(][y][)][||][1][ ≤]_

where C is related to the target vector HX[−][1][H]θ[−][1][|][z][⟩][.]

A.2 PROOF FOR THEOREM 3


_C,_ (13)


_Proof. Assume that X = fT T N_ (y), _X[ˆ] = HX[−][1][H]θ[−][1][|][z][⟩]_ [and the TT decomposition of target vector is]

[ˆ]1, [ˆ]2, ..., [ˆ]K, then we obtain
_{X_ _X_ _X_ _}_


_||HX[−][1][H]θ[−][1][|][z][⟩−]_ _[f][T T N]_ [(][y][)][||][1][ =][ ||][ ˆ]X −X||1 ≤



[ˆ]k _k_ 1 (14)
_k=1_ _||X_ _−X_ _||_

Y


On the other hand, we denote vec(Yk) and vec(Xk) as the vectorization of the tensors Yk and Xk,
respectively. We also define _k=1_ _[I][k][ =][ I][,][ W][k][ ∈]_ [R][D][k][×][I][k][×][r][k][−][1][×][r][k][ as the TTN parameters, and]
also defineactivation function. Wk ∈ R[I][k][×][r][k][−][1][r][k][D][k] as the matricization of Wk. Moreover, σ refers to a non-linear

[Q][K]

Since vec(Xk) = σ(W[T] vec(Yk)) that corresponds to a dense layer, we can obtain that

1

[ˆ]k _k_ 1 _C._ (15)
_||X_ _−X_ _||_ _≤_ _√Dk_


In sum, we can further obtain

_||HX[−][1][H]θ[−][1][|][z][⟩−]_ _[f][T T N]_ [(][y][)][||][1][ ≤]

where _k=1_ _[D][k][ =][ D][.]_

[Q][K]



[ˆ]k _k_ 1
_k=1_ _||X_ _−X_ _||_ _≤_

Y


1
_√Dk_ _C,_ (16)


_k=1_


-----

Figure 6: A comparison of convergence rates for different models.

B APPENDIX

This section includes additional experimental simulations. First, we assess the settings of TT-ranks,
and then we compare the convergence rates of QTN-VQC and Dense-VQC in the experiments.

B.1 EXPERIMENTS ON TT-RANKS FOR QTN-VQC

Table 5 corresponds to the experiments of QTN-VQC with 8 qubits and the Sigmoid function. The
empirical results suggest that the larger TT-ranks cannot result in better results than the smaller ones.
The main reason is that the TT-ranks can correspond to a manifold, and there may potentially exist
an optimal manifold with smaller TT-ranks that corresponds to the best performance.

Table 5: Comparing performance of different TT-ranks for QTN-VQC

|TT-ranks|Params|CE|Acc (%)|
|---|---|---|---|
|1, 2, 2, 1 { }|328|0.3090|91.43 0.51 ±|
|1, 4, 4, 1 { }|768|0.3082|91.46 0.53 ±|
|1, 6, 6, 1 { }|1464|0.3079|91.47 0.52 ±|


TT-ranks Params CE Acc (%)

_{1, 2, 2, 1}_ 328 0.3090 91.43 ± 0.51

_{1, 4, 4, 1}_ 768 0.3082 91.46 ± 0.53

_{1, 6, 6, 1}_ 1464 0.3079 91.47 ± 0.52


B.2 A COMPARISON OF CONVERGENCE RATES

Next, we analyze the computational complexity for TTN for QTN-VQC. In more detail, given the
TT-ranks _r1, r2, ..., rK_, a multi-dimensional tensor is factorized into several K-order tensors
_{_ _}_ _W_
_WO(kK ∈ maxR[r][k]k[−] I[1][×]k max[I][k][×][J][k]k[×] J[r]k[k](max, the computational complexity of the feed-forward process is in the scale ofk rk)[K]). In contrast, the computational overhead for a dense layer is_
in the scale of O([Q]k _[I][k]_ _k_ _[J][k][)][. It means that smaller TT-ranks can reduce the computational cost]_

for QTN-VQC, which explains that smaller TT-ranksof QTN-VQC. Q _{1, 2, 2, 1} is configured in our experiments_

Empirically, we compare the convergence rates of different models on the test data in our experiments. In our experimental settings with the Tanh activation function and 8 qubits, the QTN-VQC
model consistently attains a faster convergence rate than the Dense-VQC and PCA-VQC counterparts. Moreover, Table 6 compares the absolute running time of QTN-VQC with Dense-VQC and
AlexNet-VQC. Since our experiments are conducted on the same GPUs and CPUs, the training time
of all models can be comparable. Our evaluation shows that QTN-VQC is marginally slower than
Dense-VQC, but it is much faster than AlexNet-VQC.


-----

Table 6: Comparing performance of different TT-ranks for QTN-VQC

|Models|Dense-VQC|AlexNet-VQC|QTN-VQC|
|---|---|---|---|
|Time/epochs (mins)|58|75|61|


Models Dense-VQC AlexNet-VQC QTN-VQC

Time/epochs (mins) 58 75 61


C EXPERIMENTS OF LABELED FACES IN THE WILD (LFW)

C.1 EXPERIMENTAL SETUPS

The LFW is a dataset for the task of unconstrained face recognition, which is composed of 13000
images with the shape of [154, 154, 3]. The shape of We randomly split all the datasets into 11000
training data, 2000 test data. 16 qubits are used for VQC, and the shape of the input tensor is set as
22 × 147 × 22. The other settings are kept the same as the configurations for the MNIST task.

C.2 EXPERIMENTAL RESULTS

Table 6 presents the simulation results under the noiseless quantum circuit condition, while Table
7 demonstrates the empirical results in the setting of noisy quantum circuits. The QTN-VQC outperforms the Dense-VQC counterpart (92.15 vs. 91.27), and it owns much fewer model parameters
(2816 vs. 1.1 × 10[6]). Although

The experimental results on the LFW dataset also highlight the advantages of QTN-VQC in terms
of fewer model parameters and better empirical performance.

Table 7: Simulation results on the LFW test dataset under a noiseless circuit condition.

|Models|Params|CE|Acc (%)|
|---|---|---|---|
|Dense-VQC|1.1 106 ×|0.3011|91.27 0.25 ±|
|AlexNet-VQC|2.3 107 ×|0.2875|93.21 0.36 ±|
|QTN-VQC|2816|0.2910|92.15 ± 0.43|


Models Params CE Acc (%)

Dense-VQC 1.1 × 10[6] 0.3011 91.27 ± 0.25

AlexNet-VQC 2.3 × 10[7] 0.2875 93.21 ± 0.36

QTN-VQC 2816 **0.2910** **92.15 ± 0.43**


Table 8: Empirical results on the LFW test dataset under the noisy quantum circuit setting.

|Models|Params|Acc (%) q20|Acc (%) depo|
|---|---|---|---|
|Dense-VQC|1.1 106 ×|88.65 1.22 ±|87.23 1.04 ±|
|AlexNet-VQC|2.3 107 ×|89.76 1.34 ±|88.66 1.08 ±|
|QTN-VQC|2816|89.93 ± 1.09|89.64 ± 1.07|


Models Params Accq20 (%) Accdepo (%)

Dense-VQC 1.1 × 10[6] 88.65 ± 1.22 87.23 ± 1.04

AlexNet-VQC 2.3 × 10[7] 89.76 ± 1.34 88.66 ± 1.08

QTN-VQC 2816 **89.93 ± 1.09** **89.64 ± 1.07**


-----

