# SAMPLING WITH MIRRORED STEIN OPERATORS

**Jiaxin Shi[1]** **Chang Liu[2]** **Lester Mackey[1]**

1
Microsoft Research New England
2
Microsoft Research Asia
_{jiaxinshi,chang.liu,lmackey}@microsoft.com_

ABSTRACT

We introduce a new family of particle evolution samplers suitable for constrained
domains and non-Euclidean geometries. Stein Variational Mirror Descent and
Mirrored Stein Variational Gradient Descent minimize the Kullback-Leibler (KL)
divergence to constrained target distributions by evolving particles in a dual space
defined by a mirror map. Stein Variational Natural Gradient exploits non-Euclidean
geometry to more efficiently minimize the KL divergence to unconstrained targets.
We derive these samplers from a new class of mirrored Stein operators and adaptive
kernels developed in this work. We demonstrate that these new samplers yield
accurate approximations to distributions on the simplex, deliver valid confidence intervals in post-selection inference, and converge more rapidly than prior methods in
large-scale unconstrained posterior inference. Finally, we establish the convergence
of our new procedures under verifiable conditions on the target distribution.

1 INTRODUCTION

Accurately approximating an unnormalized distribution with a discrete sample is a fundamental
challenge in machine learning, probabilistic inference, and Bayesian inference. Particle evolution
methods like Stein variational gradient descent (SVGD, Liu & Wang, 2016) tackle this challenge
by applying deterministic updates to particles using operators based on Stein’s method (Stein, 1972;
Gorham & Mackey, 2015; Oates et al., 2017; Liu et al., 2016; Chwialkowski et al., 2016; Gorham &
Mackey, 2017) and reproducing kernels (Berlinet & Thomas-Agnan, 2011) to sequentially minimize
Kullback-Leibler (KL) divergence. SVGD has found great success in approximating unconstrained
distributions for probabilistic learning (Feng et al., 2017; Haarnoja et al., 2017; Kim et al., 2018) but
breaks down for constrained targets, like distributions on the simplex (Patterson & Teh, 2013) or the
targets of post-selection inference (Taylor & Tibshirani, 2015; Lee et al., 2016; Tian et al., 2016), and
fails to exploit informative non-Euclidean geometry (Amari, 1998).

In this work, we derive a family of particle evolution samplers suitable for target distributions with
constrained domains and non-Euclidean geometries. Our development draws inspiration from mirror
descent (MD) (Nemirovskij & Yudin, 1983), a first-order optimization method that generalizes
gradient descent with non-Euclidean geometry. To sample from a distribution with constrained
support, our method first maps particles to a dual space. There, we update particle locations using
a new class of mirrored Stein operators and adaptive reproducing kernels introduced in this work.
Finally, the dual particles are mapped back to sample points in the original space, ensuring that all
constraints are satisfied. We illustrate this procedure in Fig. 1. In Sec. 3, we develop two algorithms –
Mirrored SVGD (MSVGD) and Stein Variational Mirror Descent (SVMD) – with different updates
in the dual space; when only a single particle is used, MSVGD reduces to gradient ascent on the
log dual space density, and SVMD reduces to mirror ascent on the log target density. In addition,
by exploiting the connection between MD and natural gradient descent (Amari, 1998; Raskutti &
Mukherjee, 2015), we develop a third algorithm – Stein Variational Natural Gradient (SVNG) – that
extends SVMD to unconstrained targets with non-Euclidean geometry.

In Sec. 5, we demonstrate the advantages of our algorithms on benchmark simplex-constrained
problems from the literature, constrained sampling problems in post-selection inference (Taylor &
Tibshirani, 2015; Lee et al., 2016; Tian et al., 2016), and unconstrained large-scale posterior inference
with the Fisher information metric. Finally, we analyze the convergence of our mirrored algorithms
in Sec. 6 and discuss our results in Sec. 7.


-----

_∇ψ[∗]_



_ϵtEθ∼qt_ [Mp,ψK(θt, θ)]

_ηt+1_


_∇ψ_

_ηt_

_θt+1_ _θt_

Θ


Figure 1: Updating particle approximations in constrained domains Θ. Standard updates like SVGD
(dashed arrow) can push particles outside of the support. Our mirrored Stein updates in Alg. 1 (solid
arrows) preserve the support by updating particles in a dual space and mapping back to Θ.

**Related work** Our mirrored Stein operators (6) are instances of diffusion Stein operators in the
sense of Gorham & Mackey (2017), but their specific properties have not been studied, nor have
they been used to develop sampling algorithms. There is now a large body of work on transferring
algorithmic ideas from optimization to MCMC (e.g., Welling & Teh, 2011; Simsekli et al., 2016;
Dalalyan, 2017; Durmus et al., 2018; Ma et al., 2019) and SVGD-like sampling methods (e.g., Liu
et al., 2019a;b; Zhu et al., 2020; Zhang et al., 2020a). The closest to our work in this space is the
recent marriage of mirror descent and MCMC. For example, Hsieh et al. (2018) propose to run
Langevin Monte Carlo (LMC, an Euler discretization of the Langevin diffusion) in a mirror space.
Zhang et al. (2020b) analyze the convergence properties of the mirror-Langevin diffusion, Chewi et al.
(2020) demonstrate its advantages over the Langevin diffusion when using a Newton-type metric, and
Ahn & Chewi (2020) study its discretization for MCMC sampling in constrained domains. Relatedly,
Patterson & Teh (2013) proposed stochastic Riemannian LMC for sampling on the simplex.

Several modifications of SVGD have been proposed to incorporate geometric information. Riemannian SVGD (RSVGD, Liu & Zhu, 2018) generalizes SVGD to Riemannian manifolds, but, even with
the same metric tensor, their updates are more complex than ours: notably they require higher-order
kernel derivatives, do not operate in a mirror space, and do not reduce to natural gradient descent
when a single particle is used. They also reportedly do not perform well when with scalable stochastic
estimates of ∇ log p. Stein Variational Newton (SVN, Detommaso et al., 2018; Chen et al., 2019)
introduces second-order information into SVGD. Their algorithm requires an often expensive Hessian
computation and need not lead to descent directions, so inexact approximations are employed in
practice. Our SVNG can be seen as an instance of matrix SVGD (MatSVGD, Wang et al., 2019) with
an adaptive time-dependent kernel discussed in Sec. 4.4, a choice that is not explored in Wang et al.
(2019) and which recovers natural gradient descent when n = 1 unlike the heuristic kernel constructions of Wang et al. (2019). None of the aforementioned works provide convergence guarantees, and
neither SVN nor matrix SVGD deals with constrained domains.

2 BACKGROUND: MIRROR DESCENT AND NON-EUCLIDEAN GEOMETRY

Standard gradient descent can be viewed as optimizing a local quadratic approximation to the target
function f : θt+1 = argminθ Θ _f_ (θt)[⊤]θ + 21ϵt 2[.][ When][ Θ][ ⊆] [R][d][ is constrained, it can be]

advantageous to replace ∥· ∥2∈ with a function ∇ Ψ that reflects the geometry of a problem (Nemirovskij[∥][θ][ −] _[θ][t][∥][2]_
& Yudin, 1983; Beck & Teboulle, 2003):

_θt+1 = argminθ∈Θ ∇f_ (θt)[⊤]θ + _ϵ[1]t_ [Ψ(][θ, θ][t][)][.] (1)

We consider the mirror descent algorithm (Nemirovskij & Yudin, 1983; Beck & Teboulle, 2003)
which chooses Ψ to be the Bregman divergence induced by a strongly convex, essentially smooth[1]
function ψ : Θ → R ∪{∞}: Ψ(θ, θ[′]) = ψ(θ) − _ψ(θ[′]) −∇ψ(θ[′])[⊤](θ −_ _θ[′]). When Θ is a (d + 1)-_
simplex {θ : _i=1_ _[θ][i][ ≤]_ [1][ and][ θ][i][ ≥] [0][ for][ i][ ∈] [[][d][]][}][, a common choice of][ ψ][ is the negative entropy]
_ψ(θ) =_ _i=1_ _[θ][i][ log][ θ][i][, for][ θ][d][+1][ ≜]_ [1][ −] [P]i[d]=1 _[θ][d][. The solution of (1) is given by]_

[P][d] _θt+1 = ∇ψ[∗](∇ψ(θt) −_ _ϵt∇f_ (θt)), (2)

[P][d][+1]

where ψ[∗](η) ≜ supθ Θ η[⊤]θ _ψ(θ) is the convex conjugate of ψ and_ _ψ is a bijection from Θ to_
_∈_ _−_ _∇_
dom(ψ[∗]) with inverse map (∇ψ)[−][1] = ∇ψ[∗]. We can view the update in (2) as first mapping θt to ηt
by ∇ψ, applying the update ηt+1 = ηt − _ϵt∇f_ (θt), and mapping back through θt+1 = ∇ψ[∗](ηt+1).

1ψ is continuously differentiable on the interior of Θ with ∥∇ψ(θt)∥→∞ whenever θt → _θ ∈_ _∂Θ._


-----

Mirror descent can also be viewed as a discretization of the continuous-time dynamics dηt =
_−∇f_ (θt)dt, θt = ∇ψ[∗](ηt), which is equivalent to the Riemannian gradient flow (see App. A):

_dθt = −∇[2]ψ(θt)[−][1]∇f_ (θt)dt, or equivalently, _dηt = −∇[2]ψ[∗](ηt)[−][1]∇ηt_ _f_ (∇ψ[∗](ηt))dt, (3)

where ∇[2]ψ(θ) and ∇[2]ψ[∗](η) are Riemannian metric tensors. In information geometry, the discretization of (3) is known as natural gradient descent (Amari, 1998). There is considerable theoretical and
practical evidence (Martens, 2014) showing that natural gradient works efficiently in learning.

3 STEIN’S IDENTITY AND MIRRORED STEIN OPERATORS

Stein’s identity (Stein, 1972) is a tool for characterizing a target distribution P using a so-called Stein
_operator. We assume P has a differentiable density p with a closed convex support Θ ⊆_ R[d]. A Stein
operator Sp takes as input functions g from a Stein set G and outputs mean-zero functions under p:

Eθ _p[(_ _pg)(θ)] = 0,_ for all g _._ (4)
_∼_ _S_ _∈G_

Gorham & Mackey (2015) proposed the Langevin Stein operator given by

( _pg)(θ) = g(θ)[⊤]_ log p(θ) + _g(θ),_ (5)
_S_ _∇_ _∇·_

where g is a vector-valued function and ∇· g is its divergence. For an unconstrained domain with
Ep[ log p(θ) 2] <, Stein’s identity (4) holds for this operator whenever g _C_ [1] is bounded and
_∥∇_ _∥_ _∞_ _∈_
Lipschitz by (Gorham et al., 2019, proof of Prop. 3). However, on constrained domains Θ, Stein’s
identity fails to hold for many reasonable inputs g if p is non-vanishing or explosive at the boundary.

Motivated by this deficiency and by a desire to exploit non-Euclidean geometry, we propose an
alternative mirrored Stein operator,

( _p,ψg)(θ) = g(θ)[⊤]_ _ψ(θ)[−][1]_ log p(θ) + ( _ψ(θ)[−][1]g(θ)),_ (6)
_M_ _∇[2]_ _∇_ _∇·_ _∇[2]_

where ψ is a strongly convex, essentially smooth function as in Sec. 2 with (∇[2]ψ)[−][1] differentiable
and Lipschitz on Θ. We derive this operator from the (infinitesimal) generator of the mirror-Langevin
diffusion (19) in App. C. The following result, proved in App. I.1, shows that Mp,ψ generates
mean-zero functions under p whenever ∇[2]ψ[−][1] suitably cancels the growth of p at the boundary.
**Proposition 1. Suppose that ∇[2]ψ(θ)[−][1]∇** log p(θ) and ∇· ∇[2]ψ(θ)[−][1] _are p-integrable._ _If_
limr→∞ _∂Θr_ _[p][(][θ][)][∥∇][2][ψ][(][θ][)][−][1][n][r][(][θ][)][∥][2][dθ][ = 0][ for][ Θ][r][ ≜]_ _[{][θ][ ∈]_ [Θ :][ ∥][θ][∥][∞] _[≤]_ _[r][}][ and][ n][r][(][θ][)][ the]_

_outward unit normal vector[2]_ _to ∂Θr at θ, then Ep[(_ _p,ψg)(θ)] = 0 if g_ _C_ [1] _is bounded Lipschitz._

R _M_ _∈_

**Example 1 (Dirichlet p, Negative entropy ψ). When θ1:d+1** Dir(α) for α R[d]+[+1], even
setting g(θ) = 1 in (5) need not cause the identity to hold when some ∼ _αj_ 1. However, when ∈
_≤_
_ψ(θ) =_ _j=1_ _[θ][j][ log][ θ][j][, we show in App. B that the conditions of Prop. 1 are met for any][ α][.]_
Remarkably, the mirror-Langevin diffusion for our choice of ψ is the Wright-Fisher diffusion (Ethier,
1976) which Gan et al. (2017) recently used to bound distances to Dirichlet distributions.

[P][d][+1]

4 SAMPLING WITH MIRRORED STEIN OPERATORS

Liu & Wang (2016) pioneered the idea of using Stein operators to approximate a target distribution
with particles. Their popular SVGD algorithm updates each particle in its approximation by applying
the update rule θt+1 = θt + ϵtgt(θt) for a chosen mapping gt : R[d] _→_ R[d]. Specifically, SVGD
chooses the mapping gt[∗] [that leads to the largest decrease in KL divergence to][ p][ in the limit as][ ϵ][t]
The following theorem summarizes their main findings. _[→]_ [0][.]
**Theorem 2 (Liu & Wang, 2016, Thm. 3.1). Suppose (θt)t** 0 satisfies dθt = gt(θt)dt for bounded
_≥_
_LipschitzEqt_ [log(q gt/pt ∈)]C exists then, for the Langevin Stein operator[1] : R[d] _→_ R[d] _and that θt has density qt with S Ep (5)qt_ [∥∇, log qt∥2] < ∞. If KL(qt∥p) ≜

_dtd_ [KL][(][q][t][∥][p][) =][ −][E][θ][t][∼][q][t] [[(][S][p][g][t][)(][θ][t][)]][.] (7)

2For a closed convex set whose boundary ∂Θ can be locally represented as F (θ1, . . ., θd) = 0, its unit
normal vector is defined as n(θ) = ± _∥∇∇FF ( (θθ11,...,θ,...,θdd))∥2_ _[.]_


-----

**Algorithm 1 Mirrored Stein Variational Gradient Descent & Stein Variational Mirror Descent**

**Input: density p on Θ, kernel k, mirror function ψ, particles (θ0[i]** [)][n]i=1 _t=1_

**forInit: t = 0 : η0[i]** _[←∇] T do[ψ][(][θ]0[i]_ [)][ for][ i][ ∈] [[][n][]] _[⊂]_ [Θ][, step sizes][ (][ϵ][t][)][T]

**if SVMD then Kt** _Kψ,t (13) else Kt_ _kI (MSVGD)_
_←_ _n_ _←_
for i [n], ηt[i]+1 _t_ [+][ ϵ][t][ 1]n _j=1_ _t[, θ]t[j][)]_ (for _p,ψKt(_ _, θ) defined in Thm. 3)_
_∈_ _[←]_ _[η][i]_ _[M][p,ψ][K][t][(][θ][i]_ _M_ _·_

for i [n], θt[i]+1 _t+1[)]_
_∈_ _[←∇][ψ][∗][(][η][i]_ P

**return {θT[i]** +1[}]i[n]=1[.]


To improve its current particle approximation, SVGD finds the choice of gt that most quickly
decreases KL(qt∥p) at time t, i.e., it minimizes _dtd_ [KL][(][q][t][∥][p][)][ over a set of candidate directions][ g][t][.]

SVGD finds gt in a reproducing kernel Hilbert space (RKHS, Berlinet & Thomas-Agnan, 2011) norm
ball _d =_ _g :_ _g_ _d_ 1, where is the product RKHS containing vector-valued functions
with each component in the RKHS BH _{_ _∥_ _∥H_ _≤_ _}_ _H H of[d] k. Then the optimal gt[∗]_ _[∈B]H[d][ that minimizes (7) is]_

_gt[∗]_ _qt,k_ [≜] [E][θ]t[∼][q]t [[][k][(][θ][t][,][ ·][)][∇] [log][ p][(][θ][t][) +][ ∇][θ]t _[k][(][θ][t][,][ ·][)] =][ E][θ]t[∼][q]t_ [[][S][p][K][k][(][·][, θ][t][)]][,]

_[∝]_ _[g][∗]_

where we let Kk(θ, θ[′]) = k(θ, θ[′])I, and SpKk(·, θ) denotes applying Sp to each row of Kk(·, θ).
SVGD has found great success in approximating unconstrained target distributions p but breaks
down for constrained targets and fails to exploit non-Euclidean geometry. Our goal is to develop new
particle evolution samplers suitable for constrained domains and non-Euclidean geometries.

4.1 MIRRORED DYNAMICS


SVGD encounters two difficulties when faced with a constrained support. First, the SVGD updates can
push the random variable θt outside of its support Θ, rendering all future updates undefined. Second,
Stein’s identity (4) often fails to hold for candidate directions in _d (cf. Ex. 1). When this occurs,_
_BH_
SVGD need not converge to p as p is not a stationary point of its dynamics (i.e., _dt[d]_ [KL][(][q][t][∥][p][)][|][q][t][=][p][ ̸][= 0]

when qt = p). Inspired by mirror descent (Nemirovskij & Yudin, 1983), we consider the following
_mirrored dynamics_

_θt = ∇ψ[∗](ηt)_ for _dηt = gt(θt)dt,_ or, equivalently, _dθt = ∇[2]ψ(θt)[−][1]gt(θt)dt,_ (8)

where gt : Θ → R[d] now represents the update direction in η space. The inverse mirror map ∇ψ[∗]
automatically guarantees that θt belongs to the constrained domain Θ. Since ψ is strongly convex
and ∇[2]ψ[−][1] is bounded Lipschitz, from Thm. 2 it follows for any bounded Lipschitz gt that

_dtd_ [KL][(][q][t][∥][p][) =][ −][E][θ][t][∼][q][t] [[(][M][p,ψ][g][t][)(][θ][t][)]][,] (9)

where Mp,ψ is the mirrored Stein operator (6). In the following sections, we propose three new
deterministic sampling algorithms by seeking the optimal direction gt that minimizes (9) over different
function classes. Thm. 3 (proved in App. I.2) forms the basis of our analysis.
**Theorem 3 (Optimal mirror updates in RKHS). Suppose (θt)t** 0 follows the mirrored dynamics (8).
_≥_
_Let HK denote the RKHS of a matrix-valued kernel K : Θ × Θ →_ S[d][×][d] _(Micchelli & Pontil, 2005)._
_Then, the optimal direction of gt that minimizes (9) in the norm ball BHK ≜_ _{g : ∥g∥HK_ 1} is
_≤_

_gt[∗]_ _qt,K_ [≜] [E][θ]t[∼][q]t [[][M][p,ψ][K][(][·][, θ][t][)]][,] (10)

_[∝]_ _[g][∗]_

_where Mp,ψK(·, θ) applies Mp,ψ (6) to each row of the matrix-valued function Kθ = K(·, θ)._

4.2 MIRRORED STEIN VARIATIONAL GRADIENT DESCENT


Following the pattern of SVGD, one can choose the K of Thm. 3 to be Kk(θ, θ[′]) = k(θ, θ[′])I, where
_k is any scalar-valued kernel. In this case, the resulting update gq[∗]t,Kk_ [(][·][) =][ E][θ]t[∼][q]t [[][M][p,ψ][K][k][(][·][, θ][t][)]]
is equivalent to running SVGD in the dual η space before mapping back to Θ.
**Theorem 4 (Mirrored SVGD updates).** _In the setting of Thm. 3,_ _let kψ(η, η[′])_ =
_k(_ _ψ[∗](η),_ _ψ[∗](η[′])), pH_ (η) = p( _ψ[∗](η))_ det _ψ[∗](η)_ _denote the density of η =_ _ψ(θ)_
_∇_ _∇_ _∇_ _· |_ _∇[2]_ _|_ _∇_
_when θ ∼_ _p, and qt,H denote the distribution of ηt under the mirrored dynamics (8). If Kk = kI,_

_gq[∗]t,Kk_ [(][θ][′][) =][ E][η]t[∼][q]t,H [[][k][ψ][(][η][t][, η][′][)][∇] [log][ p][H] [(][η][t][) +][ ∇][η]t _[k][ψ][(][η][t][, η][′][)]]_ _θ[′]_ Θ, η[′] = _ψ(θ[′])._ (11)
_∀_ _∈_ _∇_


-----

The proof is in App. I.3. By discretizing the dynamics dηt = gq[∗]t,Kk [(][θ][t][)][dt][ and initializing with any]
particle approximation q0 = _n[1]_ _ni=1_ _[δ][θ]0[i]_ [, we obtain][ Mirrored SVGD (MSVGD)][, our first algorithm]

for sampling in constrained domains. The details are summarized in Alg. 1.
P

When only a single particle is used (n = 1) and the differentiable input kernel satisfies ∇k(θ, θ) = 0,
the MSVGD update (11) reduces to gradient descent on log pH (η). Note however that the modes
_−_
of the mirrored density pH (η) need not match those of the target density p(θ) (see the examples in
App. E). Since we are primarily interested in the θ-space density, it is natural to ask whether there
exists a mirrored dynamics that reduces to finding the mode of p(θ) in this limiting case. In the next
section, we give an answer to this question by designing an adaptive reproducing kernel that yields a
mirror descent-like update.

4.3 STEIN VARIATIONAL MIRROR DESCENT


Our second sampling algorithm for constrained problems is called Stein Variational Mirror De_scent (SVMD). We start by introducing a new matrix-valued kernel that incorporates the metric ∇[2]ψ_
and evolves with the distribution qt.
**Definition 1 (Kernels for SVMD). Given a continuous scalar-valued kernel k, consider the Mercer**
_representation[3]_ _k(θ, θ[′]) =_ _i_ 1 _[λ][t,i][u][t,i][(][θ][)][u][t,i][(][θ][′][)][ w.r.t.][ q][t][, where][ u][t,i][ is an eigenfunction satisfying]_

_≥_

Eθt _qt_ [k(θ, θt)ut,i(θt)] = λt,iut,i(θ). (12)
_∼_

[P]

_For kt[1][/][2](θ, θ[′]) ≜_ [P]i 1 _[λ]t,i[1][/][2][u][t,i][(][θ][)][u][t,i][(][θ][′][)][, we define the adaptive SVMD kernel at time][ t][,]_

_≥_

_Kψ,t(θ, θ[′]) ≜_ Eθt _qt_ [kt[1][/][2](θ, θt) _ψ(θt)kt[1][/][2](θt, θ[′])]._ (13)
_∼_ _∇[2]_

By Thm. 3, the optimal update direction for the SVMD kernel ball is gq[∗]t,Kψ,t [=] [E][q]t [[][M][p,ψ][K][ψ,t][(][·][, θ][t][)]][.]
We obtain the SVMD algorithm (summarized in Alg. 1) by discretizing dηt = gq[∗]t,Kψ,t [(][θ][t][)][dt][ and]
initializing with q0 = _n[1]_ _ni=1_ _[δ][θ]0[i]_ [. Because of the discrete representation of][ q][t][,][ K][ψ,t][ takes the form]

_n_
_Kψ,tP(θ, θ[′]) =_ _i=1_ _j=1_ _[λ]t,i[1][/][2][λ]t,j[1][/][2][u][t,i][(][θ][)][u][t,j][(][θ][′][)Γ][t,ij][,]_

Γt,ij = _n[1]_ _nℓ=1P[u][t,i][(][θ]t[ℓ][)][u][t,j][(][θ]t[ℓ][)][∇][2][ψ][(][θ]t[ℓ][)][.]_

[P][n]

Here both λt,j and ut,j(θt[i][)][ can be computed by solving a matrix eigenvalue problem involving the]P
particle set _θt[i][}][n]i=1[:][ B][t][v][t,j][ =][ nλ][t,j][v][t,j][,][ where][ B][t][ = (][k][(][θ]t[i][, θ]t[j][))][n]i,j=1_
_{_ _[∈]_ [R][n][×][n][ is the Gram matrix of]
pairwise kernel evaluations at particle locations, and the i-th element of vt,j is ut,j(θt[i][)][. To compute]
_θKψ,t(θ, θ[′]), we differentiate both sides of (12) to find that_ _ut,j(θ) =_ _λt,j1_ _ni=1_ _[v][t,j,i][∇][θ][k][(][θ, θ]t[i][)][.]_
_∇_ _∇_

This technique was used in Shi et al. (2018) to estimate gradients of eigenfunctions w.r.t. a continuous
P
_q. Following their recommendations, we truncate the sum at the J-th largest eigenvalues according to_
a threshold (τ ≥ [P]j[J]=1 _[λ][t,j][/][ P]j[n]=1_ _[λ][t,j][) to ensure numerical stability.]_

Notably, SVMD differs from MSVGD only in its choice of kernel, but, whenever ∇k(θ, θ) = 0, this
change is sufficient to exactly recover mirror descent when n = 1.
**Proposition 5 (Single-particle SVMD is mirror descent). If n = 1, then one step of SVMD becomes**

_ηt+1 = ηt + ϵt(k(θt, θt)∇_ log p(θt) + ∇k(θt, θt)), _θt+1 = ∇ψ[∗](ηt+1)._

4.4 STEIN VARIATIONAL NATURAL GRADIENT

The fact that SVMD recovers mirror descent as a special case is not only of relevance in constrained
problems. We next exploit the connection between MD and natural gradient descent discussed in
Sec. 2 to design a new sampler – Stein Variational Natural Gradient (SVNG) – that more efficiently
approximates unconstrained targets. The idea is to replace the Hessian ∇[2]ψ(·) in the SVMD dynamics
_dθt = ∇[2]ψ(θt)[−][1]gq[∗]t,Kψ,t_ [(][θ][t][)][ with a general metric tensor][ G][(][·][)][. The result is the Riemannian]
gradient flow

_dθt = G(θt)[−][1]gq[∗]t,KG,t_ [(][θ][t][)][dt] with _KG,t(θ, θ[′]) ≜_ Eθt _qt_ [k[1][/][2](θ, θt)G(θt)k[1][/][2](θt, θ[′])]. (14)
_∼_

3See App. F for background on Mercer representations in non-compact domains.


-----

Sparse Dirichlet Quadratic

1.5 0.3 Projected SVGD

SVMD
MSVGD, k

1.0 0.2 MSVGD, k2

Projected SVGD

Energy distance 0.5 SVMD 0.1

MSVGD, k
MSVGD, k2

0.0 0.0

0 200 400 0 200 400

Number of particle updates, T Number of particle updates, T


Figure 2: Quality of 50-particle approximations to 20-dimensional distributions on the simplex after
_T particle updates. (Left) Sparse Dirichlet posterior of Patterson & Teh (2013). (Right) Quadratic_
simplex target of Ahn & Chewi (2020). Details of the target distributions are in App. G.1.

Given any initial particle approximation q0 = _n1_ _ni=1_ _[δ][θ]0[i]_ [, we discretize these dynamics to]
obtain the unconstrained SVNG sampler of Alg. 2 in the appendix. SVNG can be seen
P
as an instance of MatSVGD (Wang et al., 2019) with a new adaptive time-dependent kernel
_G[−][1](θ)KG,t(θ, θ[′])G[−][1](θ[′]). However, similar to Prop. 5 and unlike the heuristic kernels of Wang_
et al. (2019), SVNG reduces to natural gradient ascent for finding the mode of p(θ) when n = 1.
SVNG is well-suited to Bayesian inference problems where the target is a posterior distribution
_p(θ) ∝_ _π(θ)π(y|θ). There, the metric tensor G(θ) can be set to the Fisher information matrix_
Eπ(y _θ)[_ log π(y _θ)_ log π(y _θ)[⊤]] of the data likelihood π(y_ _θ). Ample precedent from natu-_
_|_ _∇_ _|_ _∇_ _|_ _|_
ral gradient variational inference (Hoffman et al., 2013; Khan & Nielsen, 2018) and Riemannian
MCMC (Patterson & Teh, 2013) suggests that encoding problem geometry in this manner often leads
to more rapid convergence.

5 EXPERIMENTS

We next conduct a series of simulated and real-data experiments to assess (1) distributional approximation on the simplex, (2) frequentist confidence interval construction for (constrained) post-selection
inference, and (3) large-scale posterior inference with non-Euclidean geometry. To compare with
standard SVGD on constrained domains and to prevent its particles from exiting the domain Θ, we
introduce a Euclidean projection onto Θ following each SVGD update. For SVMD, we need to solve
an eigenvalue problem, which costs O(n[3]) time. In practice the number of particles used for particle
evolution algorithms is relatively small, even for SVGD, due to the O(n[2]) cost of updates. We have
produced a practical SVMD implementation that is computationally competitive with MSVGD and
SVGD for standard particle counts like n = 50 (used in all experiments).

5.1 APPROXIMATION QUALITY ON THE SIMPLEX

We first measure distributional approximation quality using two 20-dimensional simplex-constrained
targets: the sparse Dirchlet posterior of Patterson & Teh (2013) extended to 20 dimensions and the
quadratic simplex target of Ahn & Chewi (2020). The Dirichlet target mimics the multimodal sparse
conditionals that arise in latent Dirichlet allocation (Blei et al., 2003) but induces a log concave
density in η space, while the quadratic is log-concave in θ space. In Fig. 2, we compare the quality of
MSVGD, SVMD, and projected SVGD with 50 particles and inverse multiquadric kernel k (Gorham &
Mackey, 2017) by computing the energy distance (Szekely & Rizzo, 2013) to a surrogate ground truth´
sample of size 1000 (drawn i.i.d. or, in the quadratic case, from the No-U-Turn Sampler (Hoffman &
Gelman, 2014)). We also compare to MSVGD with k2(θ, θ[′]) = k( _ψ(θ),_ _ψ(θ[′])), a choice which_
_∇_ _∇_
corresponds to running SVGD in the dual space with kernel k by Thm. 4 and which ensures the
convergence of MSVGD to p by the upcoming Thms. 6 to 8.

In the quadratic case, SVMD is favored over MSVGD as it is able to exploit the log-concavity of p(θ).
In contrast, for the multimodal sparse Dirichlet with p(θ) unbounded near the boundary, MSVGD
converges slightly more rapidly than SVMD by exploiting the log concave structure in η space. This
parallels the observation of Hsieh et al. (2018) that LMC in the mirror space outperforms Riemannian
LMC for sparse Dirichlet distributions. Projected SVGD fails to converge to the target in both cases
and has particular difficulty in approximating the sparse Dirichlet target with unbounded density.


-----

0.94

0.92


1.00

0.95


0.90

0.88


0.90

0.85


0.86


0.80

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||Standar MSVGD|d|

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
|||||Standard MSVGD|


Standard
MSVGD
SVMD


Standard
MSVGD
SVMD


1000 1500 2000 2500 3000

Number of sample points, N


0.80 0.85 0.90 0.95 1.00

Nominal coverage


(a) Nominal coverage: 0.9


(b) N = 5000 sample points


Figure 3: Coverage of post-selection CIs across (a) 500 / (b) 200 replications of simulation of Sepehri
& Markovic (2017).

MSVGD with k and k2 perform very similarly, but we observe that k yields better approximation
quality upon convergence. Therefore, we employ k in the remaining MSVGD experiments.


5.2 CONFIDENCE INTERVALS FOR POST-SELECTION INFERENCE

We next apply our algorithms to the constrained sampling problems that arise in post-selection
inference (Taylor & Tibshirani, 2015; Lee et al., 2016). Specifically, we consider the task of forming
valid confidence intervals (CIs) for regression parameters selected using the randomized Lasso (Tian
et al., 2016) with data X ∈ Rn[˜]×p and y ∈ Rn˜ and user-generated randomness w ∈ Rp from a
log-concave distribution with density g. The randomized Lasso returns _β[ˆ] ∈_ R[p] with non-zero
coefficients denoted by _β[ˆ]E and their signs by sE. It is common practice to report least squares CIs_
for βE by running a linear regression on the selected features E. However, since E is chosen based
on the same data, the resulting CIs are often invalid.

Post-selection inference solves this problem by conditioning the inference on the knowledge of E
and sE. To construct valid CIs, it suffices to approximate the selective distribution with support
_βE, u_ _E : sE_ _βE > 0, u_ _E_ [ 1, 1][p][−|][E][|] and density
_{_ [ˆ] _−_ _⊙_ [ˆ] _−_ _∈_ _−_ _}_

_gˆ(β[ˆ]E, u−E) ∝_ _g_ _X_ _[⊤]y −_ _XE[⊤]X[X]−[⊤][E]E[+][X][ϵI][E][|][E][|]_ _βˆE + λ_ _su−EE_ _._
       

In our experiments, we integrate out u _E analytically, following Tian et al. (2016), and reparameterize_
_−_
_βˆE as sE ⊙|β[ˆ]E| to obtain a log-concave density of |β[ˆ]E| supported on the nonnegative orthant with_
mirror function ψ(θ) = _j=1[(][θ][j][ log][ θ][j][ −]_ _[θ][j][)][. In Fig. 4a we show the example of a 2D selective]_
distribution using samples drawn by NUTS (Hoffman & Gelman, 2014). We also plot the results by
projected SVGD, SVMD, and MSVGD in this example. Projected SVGD fails to approximate the

[P][d]
target with many samples gathering at the truncation boundary, while the samples by MSVGD and
SVMD closely resemble the truth.

We then compare our methods with the standard norejection MCMC approach of the
selectiveInference R package (Tibshirani et al., 2019) using the example simulation setting
described in Sepehri & Markovic (2017) and a penalty factor 0.7. To generate N total sample
points we run MCMC for N iterations after burn-in or aggregate the particles from N/n independent
runs of MSVGD or SVMD with n = 50 particles. As N ranges from 1000 to 3000 in Fig. 3a,
the MSVGD and SVMD CIs consistently yield higher coverage than the standard 90% CIs. This
increased coverage is of particular value for smaller sample sizes, for which the standard CIs tend
to undercover. For a much larger sample size of N = 5000 in Fig. 3b, the SVMD and standard CIs
closely track one another across confidence levels, while MSVGD consistently yields longer CIs with
high coverage. The higher coverage of MSVGD is only of value for larger confidence levels at which
the other methods begin to undercover.

We next apply our samplers to a post-selection inference task on the HIV-1 drug resistance
dataset (Rhee et al., 2006), where we run randomized Lasso (Tian et al., 2016) to find statistically significant mutations associated with drug resistance using susceptibility data on virus isolates.


-----

We take the vitro measurement of log-fold change under the 3TC drug as response and include
mutations that had appeared at least 11 times in the dataset as regressors. In Fig. 4b we plot the
CIs of selected mutations obtained with N = 5000 sample points. We see that the invalid unadjusted least squares CIs can lead to premature conclusions, e.g., declaring mutation 215Y significant
when there is insufficient support after conditioning on the selection event. In contrast, mutation
184V, which has known association with drug resistance, is declared significant by all methods even
after post-selection adjustment. The MSVGD and SVMD CIs mostly track those of the standard
selectiveInference method, but their conclusions sometimes differ: e.g., 62Y is flagged as
significant by MSVGD and SVMD but not by selectiveInference.

|Col1|Unadjusted Standard SVMD MSVGD|
|---|---|


Truth Projected SVGD

Unadjusted
2.4
Standard

2.2 SVMD

MSVGD

2.0

SVMD MSVGD 0.4

0.2

0.0

_−0.2_

P41L P62V P65R P67N P69i P70R P83KP151MP181CP184VP210WP215Y


(a)


(b)


Figure 4: (a) Sampling from a 2D selective density; (b) Unadjusted and post-selection CIs for the
mutations selected by the randomized Lasso as candidates for HIV-1 drug resistance (see Sec. 5.2).

5.3 LARGE-SCALE POSTERIOR INFERENCE WITH NON-EUCLIDEAN GEOMETRY

Finally, we demonstrate the advantages of exploiting non-Euclidean geometry by recreating the
real-data large-scale Bayesian logistic regression experiment of Liu & Wang (2016) with 581,012
datapoints and d = 54 feature dimensions. Here, the target p is the posterior distribution over logistic
regression parameters. We adopt the Fisher information metric tensor G, compare 20-particle SVNG
to SVGD and its prior geometry-aware variants RSVGD (Liu & Zhu, 2018) and MatSVGD with
average and mixture kernels (Wang et al., 2019), and for all methods use stochastic minibatches of
size 256 to scalably approximate each log likelihood query. In Fig. 5, all geometry-aware methods
substantially improve the log predictive probability of SVGD. SVNG also strongly outperforms
RSVGD and converges to its maximum test probability in half as many steps as MatSVGD (Avg) and
more rapidly than MatSVGD (Mixture).

6 CONVERGENCE GUARANTEES

We next turn our attention to the convergence properties of our proposed methods. For Kt and ϵt
as in Alg. 1, let (qt[∞][, q]t,H[∞] [)][ represent the distributions of the mirrored Stein updates][ (][θ][t][, η][t][)][ when]
_θ0_ _q0[∞]_ [and][ η][t][+1] [=][ η][t] [+][ ϵ][t][g]q[∗]t,Kt [(][θ][t][)][ for][ t][ ≥] [0][. Our first result, proved in App. I.5, shows that if]
_∼_ _n_
the Alg. 1 initialization q0[n],H [=][ 1]n _i=1_ _[δ][η]0[i]_ [converges in Wasserstein distance to a distribution][ q]0[∞],H

as n →∞, then, on each round t >P 0, the output of Alg. 1, qt[n] [=][ 1]n _ni=1_ _[δ][θ]t[i]_ [, converges to][ q]t[∞][.]

**Theorem 6 (Convergence of mirrored updates asn** _n →∞). Suppose Alg. 1 is initialized withP_
_q0[n],H_ [=][ 1]n _i=1_ _[δ][η]0[i]_ _[satisfying][ W][1][(][q]0[n],H_ _[, q]0[∞],H_ [)][ →] [0][ for][ W][1][ the][ L][1][ Wasserstein distance. Define the]

_η-induced kernelP_ _K∇ψ∗,t(η, η[′]) ≜_ _Kt(∇ψ[∗](η), ∇ψ[∗](η[′])). If, for some c1, c2 > 0,_

_∥∇(K∇ψ[∗],t(·, η)∇_ log pH (η) + ∇· K∇ψ[∗],t(·, η))∥op ≤ _c1(1 + ∥η∥2),_
_∥∇(K∇ψ∗,t(η[′], ·)∇_ log pH (·) + ∇· K∇ψ∗,t(η[′], ·))∥op ≤ _c2(1 + ∥η[′]∥2),_

_then, W1(qt,H[n]_ _[, q]t,H[∞]_ [)][ →] [0][ and][ q]t[n] _[⇒]_ _[q]t[∞]_ _for each round t._


-----

_−0.515_

_−0.520_

_−0.525_

_−0.530_

_−0.535_

_−0.540_

_−0.545_

_−0.550_


SVNG
Matrix SVGD (Avg)

Matrix SVGD (Mixture)


17.5 SVNG

Matrix SVGD (Avg)

15.0 Matrix SVGD (Mixture)

12.5

10.0

Time (s) 7.5

5.0

2.5

0.0

0 50 100 150 200 250 300 350 400

Number of particle updates, T


_−0.51_

_−0.52_

_−0.53_

_−0.54_

_−0.55_

Test log predictive probability _−0.56_ SVGD

_−0.57_ SVNGRSVGD

_−0.58_ 0 500 1000 1500 2000 2500 3000

Number of particle updates, T

**Remark**
convex, and Kt = kI for k

Given a mirrored Stein operator (6), an arbitrary Stein set
we define the mirrored Stein discrepancy

example of a diffusion kernel Stein discrepancy (Barp et al., 2019). Since the MKSD optimiza
|SVNG Matrix SVGD (Avg) Matrix SVGD (Mixture)|Col2|
|---|---|
|0 10 n geom or exa ith bou arbitra cy and g)(θ) ψ on Stei|0 200 300 400 Number of particle updates, T etry in large-scale B mple, whenever l ∇ nded derivatives. ry Stein set, and a G mirrored kernel Ste ] and MKSD ( K n discrepancy (Gor|

tion problem (15) matches that in Thm. 3, we have that MKSDK(q, p) = _gq,K[∗]_ _[∥][H]K_ [. Our next]
_∥_
result, proved in App. I.6, shows that the infinite-particle mirrored Stein updates reduce the KL
divergence to p whenever the step size is sufficiently small and drive MKSD to 0 if, for example,
_ϵt = Ω(MKSDKt_ (qt[∞][, p][)][α][)][ for any][ α >][ 0][. We also provide two conditions in App. H that generalize]
the Stein Log-Sobolev and Stein Poincare inequalities in Duncan et al. (2019); Korba et al. (2020)´
and which imply exponential convergence rates of our algorithms in continuous time.
**Theorem 7 (Infinite-particle mirrored Stein updates decrease KL and MKSD). Assume κ1 ≜**
supθ∥Kt(θ, θ)∥op < ∞, κ2 ≜ [P]i[d]=1 [sup]θ _[∥∇]i,d[2]_ +i[K][t][(][θ, θ][)][∥][op][ <][ ∞][,][ ∇] [log][ p][H][ is][ L][-Lipschitz, and]
_ψ is α-strongly convex. If ϵt < 1/(2 supθ ∥∇[2]ψ(θ)[−][1]∇gq[∗]t[∞][,K][t]_ [(][θ][) +][ ∇][g]q[∗]t[∞][,K][t] [(][θ][)][⊤][∇][2][ψ][(][θ][)][−][1][∥][op][)][,]

KL(qt[∞]+1[∥][p][)][ −] [KL][(][q]t[∞] _ϵt_ _Lκ2_ 1 + [2]α[κ][2][2] _ϵ[2]t_ MKSDKt (qt[∞][, p][)][2][.]

_[∥][p][)][ ≤−]_ _−_

Our last result, proved in App. I.7, shows that  qt[∞] _⇒ _ _p if MKSDKk(qt[∞][, p][)][ →]_ [0][. Hence, by Thms. 6]
and 7, n-particle MSVGD converges weakly to p if ϵt decays at a suitable rate.
**Theorem 8 (MKSDKk determines weak convergence). Assume pH is distantly dissipative (Eberle,**
_2016) with ∇_ log pH Lipschitz, ψ is strongly convex with continuous ∇ψ[∗], and k(θ, θ[′]) =
_κ(∇ψ(θ), ∇ψ(θ[′])) for κ(x, y) = (c[2]_ + ∥x − _y∥2[2][)][β][ with][ β][ ∈]_ [(][−][1][,][ 0)][.] _Then, qt[∞]_ _⇒_ _p if_
MKSDKk (qt[∞][, p][)][ →] [0][.]

**Remark** The pre-conditions hold, for example, for any Dirichlet target with negative entropy ψ.


7 DISCUSSION

This paper introduced the mirrored Stein operator along with three new particle evolution algorithms
for sampling with constrained domains and non-Euclidean geometries. The first algorithm MSVGD
performs SVGD updates in a mirrored space before mapping to the target domain. The other two
algorithms are different discretizations of the same continuous dynamics for exploiting non-Euclidean
geometry. SVMD is a multi-particle generalization of mirror descent for constrained domains, while
SVNG is designed for unconstrained problems with informative metric tensors.

We highlight three limitations. First, like SVGD, our MSVGD require O(n[2]) time per update.
Second, SVMD and SVNG are more costly than MSVGD due to the adaptive kernel construction.
Low-rank kernel approximation may be needed to reduce their complexity. Third, we leave open
the question of convergence when stochastic gradient estimates are employed, but we suspect
the results of Gorham et al. (2020, Thm. 7) can be extended to our setting. In the future, we
hope to deploy our mirrored Stein operators for other inferential tasks on constrained domains
including sample quality measurement (Gorham & Mackey, 2015; Huggins & Mackey, 2018),
goodness-of-fit testing (Chwialkowski et al., 2016; Liu et al., 2016; Jitkrittum et al., 2017), graphical
model inference (Zhuo et al., 2018; Wang et al., 2018), parameter estimation (Barp et al., 2019),
thinning (Riabiz et al., 2020), and de novo sampling (Chen et al., 2018; Futami et al., 2019).


-----

REPRODUCIBILITY STATEMENT

See App. G for experimental details and

[https://github.com/thjashin/mirror-stein-samplers](https://github.com/thjashin/mirror-stein-samplers)

for Python and R code replicating all experiments.

REFERENCES

Kwangjun Ahn and Sinho Chewi. Efficient constrained sampling via the mirror-Langevin algorithm.
_arXiv preprint arXiv:2010.16212, 2020._

Shun-Ichi Amari. Natural gradient works efficiently in learning. Neural Computation, 10(2):251–276,
1998.

Andrew D Barbour. Stein’s method and Poisson process convergence. Journal of Applied Probability,
pp. 175–184, 1988.

Alessandro Barp, Francois-Xavier Briol, Andrew Duncan, Mark Girolami, and Lester Mackey.
Minimum Stein discrepancy estimators. In Advances in Neural Information Processing Systems,
pp. 12964–12976, 2019.

Amir Beck and Marc Teboulle. Mirror descent and nonlinear projected subgradient methods for
convex optimization. Operations Research Letters, 31(3):167–175, 2003.

Alain Berlinet and Christine Thomas-Agnan. Reproducing kernel Hilbert spaces in probability and
_statistics. Springer Science & Business Media, 2011._

Rabi N Bhattacharya and Edward C Waymire. Stochastic processes with applications. SIAM, 2009.

David M Blei, Andrew Y Ng, and Michael I Jordan. Latent Dirichlet allocation. Journal of Machine
_Learning Research, 3:993–1022, 2003._

Peng Chen, Keyi Wu, Joshua Chen, Tom O'Leary-Roseberry, and Omar Ghattas. Projected Stein
variational Newton: A fast and scalable Bayesian inference method in high dimensions. In
_Advances in Neural Information Processing Systems, volume 32, 2019._

Wilson Ye Chen, Lester Mackey, Jackson Gorham, Franc¸ois-Xavier Briol, and Chris Oates. Stein
points. In International Conference on Machine Learning, pp. 844–853, 2018.

Sinho Chewi, Thibaut Le Gouic, Chen Lu, Tyler Maunu, Philippe Rigollet, and Austin Stromme.
Exponential ergodicity of mirror-Langevin diffusions. arXiv preprint arXiv:2005.09669, 2020.

Kacper Chwialkowski, Heiko Strathmann, and Arthur Gretton. A kernel test of goodness of fit. In
_International Conference on Machine Learning, pp. 2606–2615, 2016._

Arnak Dalalyan. Further and stronger analogy between sampling and optimization: Langevin Monte
Carlo and gradient descent. In Conference on Learning Theory, pp. 678–689, 2017.

Gianluca Detommaso, Tiangang Cui, Youssef Marzouk, Alessio Spantini, and Robert Scheichl.
A Stein variational Newton method. In Advances in Neural Information Processing Systems,
volume 31, pp. 9187–9197, 2018.

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(7), 2011.

Andrew Duncan, Nikolas Nusken, and Lukasz Szpruch. On the geometry of Stein variational gradient¨
descent. arXiv preprint arXiv:1912.00894, 2019.

Alain Durmus, Eric Moulines, and Marcelo Pereyra. Efficient Bayesian computation by proximal
Markov chain Monte Carlo: when Langevin meets Moreau. SIAM Journal on Imaging Sciences,
11(1):473–506, 2018.


-----

Andreas Eberle. Reflection couplings and contraction rates for diffusions. Probability Theory and
_Related Fields, 166(3):851–886, 2016._

Stewart N Ethier. A class of degenerate diffusion processes occurring in population genetics.
_Communications on Pure and Applied Mathematics, 29(5):483–493, 1976._

Yihao Feng, Dilin Wang, and Qiang Liu. Learning to draw samples with amortized Stein variational
gradient descent. Uncertainty in Artificial Intelligence, 2017.

JC Ferreira and VA Menegatto. Eigenvalues of integral operators defined by smooth positive definite
kernels. Integral Equations and Operator Theory, 64(1):61–81, 2009.

Futoshi Futami, Zhenghang Cui, Issei Sato, and Masashi Sugiyama. Bayesian posterior approximation
via greedy particle optimization. In Proceedings of the AAAI Conference on Artificial Intelligence,
pp. 3606–3613, 2019.

Han L Gan, Adrian Rollin, and Nathan Ross. Dirichlet approximation of equilibrium distributions in¨
Cannings models with mutation. Advances in Applied Probability, 49(3):927–959, 2017.

Damien Garreau, Wittawat Jitkrittum, and Motonobu Kanagawa. Large sample analysis of the median
heuristic. arXiv preprint arXiv:1707.07269, 2017.

Jackson Gorham and Lester Mackey. Measuring sample quality with Stein’s method. In Advances in
_Neural Information Processing Systems, pp. 226–234, 2015._

Jackson Gorham and Lester Mackey. Measuring sample quality with kernels. In International
_Conference on Machine Learning, pp. 1292–1301, 2017._

Jackson Gorham, Andrew B Duncan, Sebastian J Vollmer, and Lester Mackey. Measuring sample
quality with diffusions. The Annals of Applied Probability, 29(5):2884–2928, 2019.

Jackson Gorham, Anant Raj, and Lester Mackey. Stochastic stein discrepancies. arXiv preprint
_arXiv:2007.02857, 2020._

Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. In International Conference on Machine Learning, pp. 1352–1361,
2017.

Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning lecture
6a: overview of mini-batch gradient descent. 2012.

Matthew D Hoffman and Andrew Gelman. The No-U-Turn sampler: adaptively setting path lengths
in Hamiltonian Monte Carlo. Journal of Machine Learning Research, 15(1):1593–1623, 2014.

Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational inference.
_Journal of Machine Learning Research, 14(5), 2013._

Ya-Ping Hsieh, Ali Kavis, Paul Rolland, and Volkan Cevher. Mirrored Langevin dynamics. In
_Advances in Neural Information Processing Systems, pp. 2878–2887, 2018._

Jonathan Huggins and Lester Mackey. Random feature Stein discrepancies. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural
_Information Processing Systems, pp. 1903–1913. 2018._

Wittawat Jitkrittum, Wenkai Xu, Zoltan Szab´ o, K. Fukumizu, and A. Gretton. A Linear-Time Kernel´
Goodness-of-Fit Test. In Advances in Neural Information Processing Systems, 2017.

Sham Kakade, Shai Shalev-Shwartz, Ambuj Tewari, et al. On the duality of strong convexity and
strong smoothness: Learning applications and matrix regularization. Unpublished Manuscript, 2
(1), 2009.

Mohammad Emtiyaz Khan and Didrik Nielsen. Fast yet simple natural-gradient descent for variational
inference in complex models. In 2018 International Symposium on Information Theory and Its
_Applications (ISITA), pp. 31–35. IEEE, 2018._


-----

Taesup Kim, Jaesik Yoon, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn.
Bayesian model-agnostic meta-learning. Advances in Neural Information Processing Systems,
2018.

Anna Korba, Adil Salim, Michael Arbel, Giulia Luise, and Arthur Gretton. A non-asymptotic analysis
for Stein variational gradient descent. Advances in Neural Information Processing Systems, 33,
2020.

Jason D Lee, Dennis L Sun, Yuekai Sun, and Jonathan E Taylor. Exact post-selection inference, with
application to the lasso. Annals of Statistics, 44(3):907–927, 2016.

Chang Liu and Jun Zhu. Riemannian Stein variational gradient descent for Bayesian inference. In
_Proceedings of the AAAI Conference on Artificial Intelligence, pp. 3627–3634, 2018._

Chang Liu, Jingwei Zhuo, Pengyu Cheng, Ruiyi Zhang, and Jun Zhu. Understanding and accelerating
particle-based variational inference. In International Conference on Machine Learning, pp. 4082–
4092, 2019a.

Chang Liu, Jingwei Zhuo, and Jun Zhu. Understanding MCMC dynamics as flows on the Wasserstein
space. In Proceedings of the 36th International Conference on Machine Learning, pp. 4093–4103,
2019b.

Qiang Liu. Stein variational gradient descent as gradient flow. In Advances in Neural Information
_Processing Systems, pp. 3115–3123, 2017._

Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose Bayesian inference
algorithm. Advances in Neural Information Processing Systems, 29:2378–2386, 2016.

Qiang Liu, Jason Lee, and Michael Jordan. A kernelized Stein discrepancy for goodness-of-fit tests.
In International Conference on Machine Learning, pp. 276–284, 2016.

Yi-An Ma, Tianqi Chen, and Emily Fox. A complete recipe for stochastic gradient MCMC. In
_Advances in Neural Information Processing Systems, pp. 2917–2925, 2015._

Yi-An Ma, Niladri Chatterji, Xiang Cheng, Nicolas Flammarion, Peter Bartlett, and Michael I Jordan.
Is there an analog of Nesterov acceleration for MCMC? arXiv preprint arXiv:1902.00996, 2019.

James Martens. New insights and perspectives on the natural gradient method. arXiv preprint
_arXiv:1412.1193, 2014._

Charles A Micchelli and Massimiliano Pontil. On learning vector-valued functions. Neural Computa_tion, 17(1):177–204, 2005._

Arkadij Semenovic Nemirovskij and David Borisovich Yudin. Problem complexity and method
efficiency in optimization. 1983.

Chris J Oates, Mark Girolami, and Nicolas Chopin. Control functionals for Monte Carlo integration.
_Journal of the Royal Statistical Society: Series B (Methodological), 79(3):695–718, 2017._

Bernt Øksendal. Stochastic Differential Equations: An Introduction with Applications. Springer
Science & Business Media, 2003.

Sam Patterson and Yee Whye Teh. Stochastic gradient Riemannian Langevin dynamics on the
probability simplex. In Advances in Neural Information Processing Systems, pp. 3102–3110, 2013.

Garvesh Raskutti and Sayan Mukherjee. The information geometry of mirror descent. IEEE
_Transactions on Information Theory, 61(3):1451–1457, 2015._

Soo-Yon Rhee, Jonathan Taylor, Gauhar Wadhera, Asa Ben-Hur, Douglas L Brutlag, and Robert W
Shafer. Genotypic predictors of human immunodeficiency virus type 1 drug resistance. Proceedings
_of the National Academy of Sciences, 103(46):17355–17360, 2006._

Marina Riabiz, Wilson Chen, Jon Cockayne, Pawel Swietach, Steven A Niederer, Lester Mackey,
Chris Oates, et al. Optimal thinning of MCMC output. arXiv preprint arXiv:2005.03952, 2020.


-----

Amir Sepehri and Jelena Markovic. Non-reversible, tuning-and rejection-free Markov chain Monte
Carlo via iterated random functions. arXiv preprint arXiv:1711.07177, 2017.

Jiaxin Shi, Shengyang Sun, and Jun Zhu. A spectral approach to gradient estimation for implicit
distributions. In International Conference on Machine Learning, pp. 4644–4653, 2018.

Umut Simsekli, Roland Badeau, Taylan Cemgil, and Gael Richard. Stochastic quasi-Newton Langevin¨
Monte Carlo. In International Conference on Machine Learning, pp. 642–651, 2016.

Charles Stein. A bound for the error in the normal approximation to the distribution of a sum of
dependent random variables. In Proceedings of the Sixth Berkeley Symposium on Mathemati_cal Statistics and Probability, Volume 2: Probability Theory. The Regents of the University of_
California, 1972.

Gabor J Sz´ ekely and Maria L Rizzo. Energy statistics: A class of statistics based on distances.´
_Journal of Statistical Planning and Inference, 143(8):1249–1272, 2013._

Jonathan Taylor and Robert J Tibshirani. Statistical learning and selective inference. Proceedings of
_the National Academy of Sciences, 112(25):7629–7634, 2015._

Xiaoying Tian, Nan Bi, and Jonathan Taylor. MAGIC: a general, powerful and tractable method for
selective inference. arXiv preprint arXiv:1607.02630, 2016.

Ryan Tibshirani, Rob Tibshirani, Jonatha Taylor, Joshua Loftus, Stephen Reid, and Jelena
[Markovic. selectiveInference: Tools for Post-Selection Inference, 2019. URL https://CRAN.](https://CRAN.R-project.org/package=selectiveInference)
[R-project.org/package=selectiveInference. R package version 1.2.5.](https://CRAN.R-project.org/package=selectiveInference)

Dilin Wang, Zhe Zeng, and Qiang Liu. Stein variational message passing for continuous graphical
models. In International Conference on Machine Learning, pp. 5219–5227, 2018.

Dilin Wang, Ziyang Tang, Chandrajit Bajaj, and Qiang Liu. Stein variational gradient descent with
matrix-valued kernels. In Advances in Neural Information Processing Systems, pp. 7836–7846,
2019.

Max Welling and Yee W Teh. Bayesian learning via stochastic gradient Langevin dynamics. In
_International Conference on Machine Learning, pp. 681–688, 2011._

Edwin B Wilson. Probable inference, the law of succession, and statistical inference. Journal of the
_American Statistical Association, 22(158):209–212, 1927._

Tatiana Xifara, Chris Sherlock, Samuel Livingstone, Simon Byrne, and Mark Girolami. Langevin
diffusions and the Metropolis-adjusted Langevin algorithm. Statistics & Probability Letters, 91:
14–19, 2014.

Jianyi Zhang, Yang Zhao, and Changyou Chen. Variance reduction in stochastic particle-optimization
sampling. In Proceedings of the 37th International Conference on Machine Learning, pp. 11307–
11316, 2020a.

Kelvin Shuangjian Zhang, Gabriel Peyre, Jalal Fadili, and Marcelo Pereyra. Wasserstein control of´
mirror Langevin Monte Carlo. arXiv preprint arXiv:2002.04363, 2020b.

Michael Zhu, Chang Liu, and Jun Zhu. Variance reduction and quasi-Newton for particle-based
variational inference. In Proceedings of the 37th International Conference on Machine Learning,
pp. 11576–11587, 2020.

Jingwei Zhuo, Chang Liu, Jiaxin Shi, Jun Zhu, Ning Chen, and Bo Zhang. Message passing Stein
variational gradient descent. In International Conference on Machine Learning, pp. 6018–6027,
2018.


-----

**Algorithm 2 Stein Variational Natural Gradient (SVNG)**

**Input: density p(θ) on R[d], kernel k, metric tensor G(θ), particles (θ0[i]** [)][n]i=1[, step sizes][ (][ϵ][t][)]t[T]=1

**for t = 0 : T do**

for i ∈ [n], θt[i]+1n[←] _[θ]t[i]_ [+][ ϵ][t][G][(][θ]t[i][)][−][1][g]G,t[∗] [(][θ]t[i][)][, where]

_gG,t[∗]_ [(][θ][) =][ 1]n _j=1[[][K][G,t][(][θ, θ]t[j][)][G][(][θ]t[j][)][−][1][∇]_ [log][ p][(][θ]t[j][)+][∇]θt[j] _t_ [)][G][(][θ]t[j][)][−][1][)]][ (see (14))]

_[·][(][K][G,t][(][θ, θ][j]_

**return** _θT[i]_ +1[}]i[n]=1P[.]
_{_


MIRROR DESCENT, RIEMANNIAN GRADIENT FLOW, AND NATURAL
GRADIENT


The equivalence between the mirror flow dηt = −∇f (θt)dt, θt = ∇ψ[∗](ηt)dt and the Riemannian
gradient flow in (3) is a direct result of the chain rule:
_dθt_ _dηt_ (16)

_dt_ [=][ −∇][η][t] _[θ][t]_ _dt_ [=][ −][(][∇][θ][t] _[η][t][)][−][1][ dη]dt[t]_ [=][ −∇][2][ψ][(][θ][t][)][−][1][∇][f] [(][θ][t][)][,]

_dηt_ (17)

_dt_ [=][ −∇][f] [(][θ][t][) =][ −∇][θ][t] _[η][t][∇][η][t]_ _[f]_ [(][∇][ψ][∗][(][η][t][)) =][ −∇][2][ψ][∗][(][η][t][)][−][1][∇][η][t] _[f]_ [(][∇][ψ][∗][(][η][t][))][.]

Depending on discretizing (16) or (17), there are two natural gradient descent (NGD) updates that
can arise from the same gradient flow:
NGD (a): _θt+1 = θt −_ _ϵt∇[2]ψ(θt)[−][1]∇f_ (θt),

NGD (b): _ηt+1 = ηt −_ _ϵt∇[2]ψ[∗](ηt)[−][1]∇ηt_ _f_ (∇ψ[∗](ηt)).
With finite step sizes ϵt, their updates need not be the same and can lead to different optimization
paths. Since _f_ (θt) = _ψ[∗](ηt)[−][1]_ _ηt_ _f_ ( _ψ[∗](ηt)), NGD (b) is equivalent to the dual-space update_
_∇_ _∇[2]_ _∇_ _∇_
by mirror descent. This relationship was pointed out in Raskutti & Mukherjee (2015) and has been
used for developing natural gradient variational inference algorithms (Khan & Nielsen, 2018). We
emphasize, however, our SVNG algorithm developed in Sec. 4.4 corresponds to the discretization
in the primal space as in NGD (a). Therefore, it does not require an explicit dual space, and allows
replacing ∇[2]ψ with more general information metric tensors.

B DETAILS OF EXAMPLE 1


For the entropic mirror map ψ(θ) = _j=1_ _[θ][j][ log][ θ][j][, we have][ ∇][2][ψ][(][θ][)][−][1][ = diag(][θ][)][ −]_ _[θθ][⊤][. Note]_

here θ denotes a d-dimensional vector and does not include θd+1 = 1 − [P]j[d]=1 _[θ][d][. Since][ Θ][ is a]_
(d + 1)-simplex, ∂Θ is composed of d[P] + 1[d][+1] faces with θ in the j-th face satisfies θj = 0. The outward
unit normal vector n(θ) for the first d faces are −ej for 1 ≤ _j ≤_ _d, where ej denotes the j-th standard_
basis of R[d]. The outward unit normal vector for the (d + 1)-st face is a vector with 1/√d in all

coordinates. Therefore, we have


_∂Θ_ _p(θ)g(θ)[⊤]∇[2]ψ(θ)[−][1]n(θ)dθ =_


_∂Θ_ _p(θ)g(θ)[⊤](diag(θ) −_ _θθ[⊤])n(θ)dθ_

_∂Θ_ _p(θ)(θ ⊙_ _g(θ) −_ _θθ[⊤]g(θ))[⊤]n(θ)dθ_


_θj_ =0 _p(θ)(θ[⊤]g(θ) −_ _gj(θ))θjdθ−j_

_p(θ)θ[⊤]g(θ)θd+1dθ_

_d_ _θd+1=0_

Z


_j=1_

+ [1]
_√_

=0,


where in the second to last identity we used θ[⊤]1 = 1 _θd+1. Finally, we can verify the condition in_
_−_
Prop. 1 as


_p(θ)_ _ψ(θ)[−][1]nr(θ)_ 2dθ = sup
_∂Θr_ _∥∇[2]_ _∥_ _∥g∥∞≤1_


_∂Θ_ _p(θ)g(θ)[⊤]∇[2]ψ(θ)[−][1]n(θ)dθ = 0._


lim
_r→∞_


-----

C DERIVATION OF THE MIRRORED STEIN OPERATOR

We first review the (overdamped) Langevin diffusion – a Markov process that underlies many recent
advances in Stein’s method – along with its recent mirrored generalization. The Langevin diffusion
with equilibrium density p on R[d] is a Markov process (θt)t 0 R[d] satisfying the stochastic
differential equation (SDE) _≥_ _⊂_
_dθt = ∇_ log p(θt)dt + _√2dBt_ (18)

with (Bt)t 0 a standard Brownian motion (Bhattacharya & Waymire, 2009, Sec. 4.5).
_≥_

To identify Stein operators that satisfy (4) for broad classes of targets p, Gorham & Mackey (2015)
proposed to build upon the generator method of Barbour (1988): First, identify a Markov process
(θt)t 0 that has p as the equilibrium density; they chose the Langevin diffusion of (18). Next, build a
_≥_
Stein operator based on the (infinitesimal) generator A of the process (Øksendal, 2003, Def. 7.3.1):

(Af )(θ) = limt→0 [1]t [(][E][f] [(][θ][t][)][ −] [E][f] [(][θ][0][))] for f : R[d] _→_ R,

as the generator satisfies Eθ _p[(Af_ )(θ)] = 0 under relatively mild conditions. We use the following
_∼_
theorem to derive the generator of the processes described by SDEs like (18):
**Theorem 9 (Generator of Ito diffusion; Øksendal, 2003, Thm 7.3.3)ˆ** **. Let (xt)t** 0 be the Ito diffusionˆ
_≥_
_in X ⊆_ R[d] _satisfying dxt = b(xt)dt + σ(xt)dBt. For any f ∈_ _Cc[2][(][X]_ [)][, the (infinitesimal) generator]
_A of (xt)t_ 0 is
_≥_

(Af )(x) = b(x)[⊤]∇f (x) + [1]2 [Tr(][σ][(][x][)][σ][(][x][)][⊤][∇][2][f] [(][x][))][.]

For the Langevin diffusion (18), substituting ∇ log p(·) for b(·) and _√2I for σ(·) in Thm. 9, we_

obtain Af = (∇ log p)[⊤]∇f + ∇· ∇f . Replacing ∇f with a vector-valued function g gives the
Langevin Stein operator in (5).

To derive a Stein operator that works well for constrained domains, we consider the Riemannian
Langevin diffusion (Patterson & Teh, 2013; Xifara et al., 2014; Ma et al., 2015) that extends the
Langevin diffusion to non-Euclidean geometries encoded in a positive definite metric tensor G(θ):


_dθt = (G(θt)[−][1]∇_ log p(θt) + ∇· G(θt)[−][1])dt +


2G(θt)[−][1][/][2]dBt.[4]


We show in App. D that the choice G = ∇[2]ψ yields the recent mirror-Langevin diffusion (Zhang
et al., 2020b; Chewi et al., 2020)


_θt = ∇ψ[∗](ηt),_ _dηt = ∇_ log p(θt)dt +


2 _ψ(θt)[1][/][2]dBt._ (19)
_∇[2]_


According to Thm. 9, the generator of the mirror-Langevin diffusion described by (20) is

(Ap,ψf )(θ) = ( _ψ(θ)[−][1]_ log p(θ) + _ψ(θ)[−][1])[⊤]_ _f_ (θ) + Tr( _ψ(θ)[−][1]_ _f_ (θ))
_∇[2]_ _∇_ _∇· ∇[2]_ _∇_ _∇[2]_ _∇[2]_

= ∇f (θ)[⊤]∇[2]ψ(θ)[−][1]∇ log p(θ) + ∇· (∇[2]ψ(θ)[−][1]∇f (θ)).

Now substituting g(θ) for ∇f (θ), we obtain the associated mirrored Stein operator:

( _p,ψg)(θ) = g(θ)[⊤]_ _ψ(θ)[−][1]_ log p(θ) + ( _ψ(θ)[−][1]g(θ))._
_M_ _∇[2]_ _∇_ _∇·_ _∇[2]_


D RIEMANNIAN LANGEVIN DIFFUSIONS AND MIRROR-LANGEVIN
DIFFUSIONS

Zhang et al. (2020b) pointed out (19) is a particular case of the Riemannian LD. However, they did
not give an explicit derivation. The Riemannian LD (Patterson & Teh, 2013; Xifara et al., 2014; Ma
et al., 2015) with ∇[2]ψ(·) as the metric tensor is

_dθt = (∇[2]ψ(θt)[−][1]∇_ log p(θt) + ∇· ∇[2]ψ(θt)[−][1])dt + _√2∇[2]ψ(θt)[−][1][/][2]dBt._ (20)

To see the connection with mirror-Langevin diffusion, we would like to obtain the SDE that describes
the evolution of ηt = ∇ψ(θt) under the diffusion. This requires the following theorem that provides
the analog of the “chain rule” in SDEs.

4A matrix divergence ∇· G(θ) is the vector obtained by computing the divergence of each row of G(θ).


-----

**Theorem 10 (Ito formula; Øksendal, 2003, Thm 4.2.1)ˆ** **. Let (xt)t** 0 be an Ito process inˆ R[d]
_≥_ _X ⊂_

_satisfying dxt = b(xt)dt + σ(xt)dBt. Let f_ (x) ∈ _C_ [2] : R[d] _→_ R[d][′] _. Then yt = f_ (xt) is again an Itoˆ
_process, and its i-th dimension satisfies_

_dyt,i = (∇fi(xt)[⊤]b(xt) + [1]2 [Tr(][∇][2][f][i][(][x][t][)][σ][(][x][t][)][σ][(][x][t][)][⊤][)][dt][ +][ ∇][f][i][(][x][t][)][⊤][σ][(][x][t][)][dB][t][.]_


Substituting ∇ψ for f in Thm. 10, we have the SDE of ηt = ∇ψ(θt) as

_dηt = (∇_ log p(θt) + ∇[2]ψ(θt)∇· ∇[2]ψ(θt)[−][1] + h(θt))dt + _√_

where h(θt)i = Tr( _θt_ [(][∇][θ]t,i _[ψ][(][θ][t][))][∇][2][ψ][(][θ][t][)][−][1][)][. Moreover, we have]_
_∇[2]_


2 _ψ(θt)[1][/][2]dBt,_
_∇[2]_



[ _ψ(θt)_ _ψ(θt)[−][1]]i + Tr(_ _θt_ [(][∇][θ]t,i _[ψ][(][θ][t][))][∇][2][ψ][(][θ][t][)][−][1][)]_
_∇[2]_ _∇· ∇[2]_ _∇[2]_


_ψ(θt)ij_ _θt,ℓ_ [ _ψ(θt)[−][1]]jℓ_ +
_∇[2]_ _∇_ _∇[2]_
_j=1_

X


_θt,ℓ_ _ψ(θt)ij[_ _ψ(θt)[−][1]]jℓ_
_∇_ _∇[2]_ _∇[2]_
_j=1_

X

_d_

_θt,ℓ_ _Iiℓ_ = 0.
_∇_
_ℓ=1_

X


_ℓ=1_


_ℓ=1_


_ψ(θt)ij[_ _ψ(θt)[−][1]]jℓ_
_∇[2]_ _∇[2]_
_j=1_

X


=






_θt,ℓ_
_∇_
_ℓ=1_

X


Therefore, the ηt diffusion is described by the SDE:


2∇[2]ψ(θt)[1][/][2]dBt, _θt = ∇ψ[∗](ηt)._


_dηt = ∇_ log p(θt)dt +


E MODE MISMATCH UNDER TRANSFORMATIONS

0.16

12.5

0.12

10.0

( )p 7.5 ( )pH0.08

5.0

0.04

2.5

0.0 0.00

0.0 0.2 0.4 0.6 0.8 1.0 10 5 0 5 10

8

6

( )p 4

2

0

0.0 0.2 0.4 0.6 0.8 1.0

0.3

( )H0.2
p

0.1

0.0

10 5 0 5 10


Figure 6: The density functions of the same distribution in θ (left) and η (right) space under
the transformation η = ∇ψ(θ). Each θ follows a Beta distributions on [0, 1]. We choose the
negative entropy ψ(θ) = θ log θ + (1 − _θ) log(1 −_ _θ). Then, the transformation is the logit function_
_η = log(θ/(1−θ)) and its reverse is the sigmoid function θ = 1/(1+e[−][η]). Top: θ ∼_ Beta(0.5, 0.5).
Dashed lines mark the mode of the transformed density pH (η) and the corresponding θ, which gives
the lowest value of p(θ); Bottom: θ ∼ Beta(1.1, 10). Dashed lines mark the mode of the target
density p(θ) and the corresponding η, which clearly does not match the mode of pH (η).


-----

F BACKGROUND ON REPRODUCING KERNEL HILBERT SPACES

Let H be a Hilbert space of functions defined on X and taking their values in R. We say k is a
reproducing kernel (or kernel) of if _x_ _, k(x,_ ) and _f_ _,_ _f, k(x,_ ) = f (x). is
_H_ _∀_ _∈X_ _·_ _∈H_ _∀_ _∈H_ _⟨_ _·_ _⟩H_ _H_
called a reproducing kernel Hilbert space (RKHS) if it has a kernel. Kernels are positive definite (p.d.)
functions, which means that matrices with the form (k(xi, xj))ij are positive semidefinite. For any
p.d. function k, there is a unique RKHS with k as the reproducing kernel, which can be constructed
by the completion of {[P]i[n]=1 _[a][i][k][(][x][i][,][ ·][)][, x][i][ ∈X]_ _[, a][i][ ∈]_ [R][, i][ ∈] [N][}][.]

Now we assume X is a metric space, k is a bounded continuous kernel with the RKHS H, and ν is a
positive measure on X . L[2](ν) denote the space of all square-integrable functions w.r.t. ν. Then the
kernel integral operator Tk : L[2](ν) → _L[2](ν) defined by_


_g(x)k(x, ·)dν_


_Tkg =_


is compact and self-adjoint. Therefore, according to the spectral theorem, there exists an at most
countable set of positive eigenvalues _λj_ _j_ _J_ R with λ1 _λ2_ _. . . converging to zero and_
orthonormal eigenfunctions _uj_ _j_ _J such that {_ _}_ _∈_ _⊂_ _≥_ _≥_
_{_ _}_ _∈_

_Tkuj = λjuj,_

and k has the representation k(x, x[′]) = _j_ _J_ _[λ][j][u][j][(][x][)][u][j][(][x][′][)][ (Mercer’s theorem on non-compact]_

_∈_
domains), where the convergence of the sum is absolute and uniform on compact subsets of X ×
(Ferreira & Menegatto, 2009).

[P]

_X_

G SUPPLEMENTARY EXPERIMENTAL DETAILS AND ADDITIONAL RESULTS

In this section, we report supplementary details and additional results from the experiments of Sec. 5.
In Secs. 5.1 and 5.2, we use the inverse multiquadric input kernel k(θ, θ[′]) = (1 + ∥θ − _θ[′]∥2[2][/ℓ][2][)][−][1][/][2]_
due to its convergence control properties (Gorham & Mackey, 2017). In the unconstrained experiments
of Sec. 5.3, we use the Gaussian kernel k(θ, θ[′]) = exp(−∥θ − _θ[′]∥2[2][/ℓ][2][)][ for consistency with past]_
work. The bandwidth ℓ is determined by the median heuristic (Garreau et al., 2017). We select τ
from {0.98, 0.99} for all SVMD experiments. For unconstrained targets, we report, for each method,
results from the best fixed step size ϵ ∈{0.01, 0.05, 0.1, 0.5, 1} selected on a separate validation
set. For constrained targets, we select step sizes adaptively to accommodate rapid density growth
near the boundary; specifically, we use RMSProp (Hinton et al., 2012), an extension of the AdaGrad
algorithm (Duchi et al., 2011) used in Liu & Wang (2016), and report performance with the best
learning rate. Results were recorded on an Intel(R) Xeon(R) CPU E5-2690 v4 @ 2.60GHz and an
NVIDIA Tesla P100 PCIe 16GB.

G.1 APPROXIMATION QUALITY ON THE SIMPLEX

The sparse Dirichlet posterior of Patterson & Teh (2013) extended to 20 dimensions features a sparse,
symmetric Dir(α) prior with αk = 0.1 for k ∈{1, . . ., 20} and sparse count data n1 = 90, n2 =
_n3 = 5, nj = 0 (j > 3), modeled via a multinomial likelihood. The quadratic target satisfies_
log p(θ) = − 2σ1[2][ θ][⊤][Aθ][ + const][, where we slightly modify the target density of Ahn & Chewi (2020)]

to make it less flat by introducing a scale parameter σ = 0.01. A ∈ R[19][×][19] is a positive definite
matrix generated by normalizing products of random matrices with i.i.d. elements drawn from
Unif[−1, 1].

We initialize all methods with i.i.d samples from Dirichlet(5) to prevent any of the initial particles
being too close to the boundary. For each method and each learning rate we apply 500 particle updates.
For SVMD we set τ = 0.98. We search the base learning rates of RMSProp in {0.1, 0.01, 0.001} for
SVMD and MSVGD. Since projected SVGD applies updates in the θ space, the appropriate learning
rate range is smaller than those of SVMD and MSVGD. There we search the base learning rate of
RMSProp in {0.01, 0.001, 0.0001}. For all methods the results under each base learning rate are
plotted in Fig. 7.


-----

Projected SVGD SVMD MSVGD, k MSVGD, k2

1.5 1.5 1.5 1.5

1.0 1.0 1.0 LR=0.1 1.0 LR=0.1

LR=0.01 LR=0.01
LR=0.001 LR=0.001

Energy distance 0.5 LR=0.01 0.5 LR=0.1 0.5 0.5

LR=0.001 LR=0.01
LR=0.0001 LR=0.001

0.0 0.0 0.0 0.0

0 200 400 0 200 400 0 200 400 0 200 400

Number of particle updates, T Number of particle updates, T Number of particle updates, T Number of particle updates, T


Figure 7: Sampling from a Dirichlet target on a 20-simplex. We plot the energy distance to a ground
truth sample of size 1000.

Projected SVGD SVMD MSVGD, k MSVGD, k2

LR=0.01 LR=0.1

0.3 0.3 0.3 0.3

LR=0.001 LR=0.01
LR=0.0001 LR=0.001

0.2 0.2 0.2 0.2

Energy distance 0.1 0.1 0.1 LR=0.1 0.1 LR=0.1

LR=0.01 LR=0.01
LR=0.001 LR=0.001

0.0 0.0 0.0 0.0

0 200 400 0 200 400 0 200 400 0 200 400

Number of particle updates, T Number of particle updates, T Number of particle updates, T Number of particle updates, T


Figure 8: Sampling from a quadratic target on a 20-simplex. We plot the energy distance to a ground
truth sample of size 1000 drawn by NUTS (Hoffman & Gelman, 2014).

G.2 CONFIDENCE INTERVALS FOR POST-SELECTION INFERENCE

Given a dataset X ∈ Rn[˜]×p, y ∈ Rn˜, the randomized Lasso (Tian et al., 2016) solves the following
problem:

argminβ∈Rp [1]2 _[∥][y][ −]_ _[Xβ][∥]2[2]_ [+][ λ][∥][β][∥]1 _[−]_ _[w][⊤][β][ +][ ϵ]2_ _[∥][β][∥]2[2][,]_ _w ∼_ G.

where G is a user-specified log-concave distribution with density g. We choose G to be zeromean independent Gaussian distributions while leaving its scale and the ridge parameter ϵ to be
automatically determined by the randomizedLasso function of the selectiveInference
package. We initialize the particles of our SVMD and MSVGD in the following way: First, we map
the solution _β[ˆ]E to the dual space by ∇ψ. Next, we add i.i.d. standard Gaussian noise to n copies of_
the image in the dual space. Finally, we map the n particles back to the primal space by ∇ψ[∗] and use
them as the initial locations. Below we discuss the remaining settings and additional results of the
simulation and the HIV-1 drug resistance experiment separately.

**Simulation** In our simulation we mostly follow the settings of Sepehri & Markovic (2017) except
using a different penalty level λ recommended in the selectiveInference R package. We
set ˜n = 100 and p = 40. The design matrix X is generated from an equi-correlated model, i.e.,
each datapointnormalized to have almost unit length. The normalization is done by first centering each dimension xi ∈ R[p] is generated i.i.d. from N (0, Σ) with Σii = 1, Σij = 0.3 (i ̸= j) and then
by subtracting the mean and dividing the standard deviation of that column of X, then additionally
multiplying 1/n˜[1][/][2]. y is generated from a standard Gaussian which is independent of X, i.e., we
assume the global null setting where the true value of β is zero. We set λ to be the value returned
by theoretical.lambda of the selectiveInference R package multiplied a coefficient
0.7˜n, where the 0.7 adjustment is introduced in the test examples of the R package to reduce the
regularization effect so that we have a reasonably large set of selected features when p = 40. The
base learning rates for SVMD and MSVGD are set to 0.01 and we run them for T = 1000 particle
updates. τ is set to 0.98 for SVMD.

Our 2D example in Fig. 4a is grabbed from one run of the simulation where there happen to be
only 2 features selected by the randomized Lasso. The selective distribution in this case has logdensityconst, θ log1,2 _p(θ0) =._ _−8.07193((2.39859θ1 + 1.90816θ2 + 2.39751)[2]_ + (1.18099θ2 − 1.46104)[2]) +
_≥_


-----

The error bars for actual coverage levels in Fig. 3a and Fig. 3b are 95% Wilson intervals (Wilson,
1927), which is known to be more accurate than ±2 standard deviation intervals for binomial
proportions like the coverage. In Fig. 9a and Fig. 9b we additionally plot the average length of the
confidence intervals w.r.t. different sample size N and nominal coverage levels. For all three methods
the CI widths are very close, although MSVGD consistently has wider intervals than SVMD and
selectiveInference. This indicates that SVMD can be preferred over MSVGD when both
methods produce coverage above the nominal level.

**HIV-1 drug resistance** We take the vitro measurement of log-fold change under the 3TC drug
as response and include mutations that had appeared 11 times in the dataset as regressors. This
results in ˜n = 663 datapoints with p = 91 features. We choose λ to be the value returned by
theoretical.lambda of the selectiveInference R package multiplied by ˜n. The base
learning rates for SVMD and MSVGD are set to 0.01 and we run them for T = 2000 particle updates.
_τ is set to 0.99 for SVMD._


8

7

6

5

4

Width

3

2

Standard

1 MSVGD

SVMD

0

1000 1500 2000 2500 3000

Number of sample points, N


8

7

6

5

4

Width

3

2

Standard

1 MSVGD

SVMD

0

0.80 0.85 0.90 0.95 1.00

Nominal coverage


(a) Nominal coverage: 0.9


(b) N = 5000 sample points


Figure 9: Width of post-selection CIs across (a) 500 / (b) 200 replications of simulation of Sepehri &
Markovic (2017).

G.3 LARGE-SCALE POSTERIOR INFERENCE WITH NON-EUCLIDEAN GEOMETRY

The Bayesian logistic regression model we consider is _ni=1_ _[p][(][y][i][|][x][i][, w][)][p][(][w][)][, where][ p][(][w][) =]_
(w 0, I), p(yi _xi, w) = Bernoulli(σ(w[⊤]xi)). The bias parameter is absorbed into into w by_
_N_ _|_ _|_
adding an additional feature 1 to each xi. The gradient of the log density of the posterior distribution[Q][˜]
of w is ∇w log p(w|{yi, xi}i[N]=1[) =][ P][N]i=1 _[x][i][(][y][i][ −]_ _[σ][(][w][⊤][x][i][))][ −]_ _[w.][ We choose the metric tensor]_
_∇[2]ψ(w) to be the Fisher information matrix (FIM) of the likelihood:_

_n˜_

_F = [1]_ Ep(yi _w,xi)[_ _w log p(yi_ _xi, w)_ _w log p(yi_ _xi, w)[⊤]]_

_n˜_ _|_ _∇_ _|_ _∇_ _|_

_i=1_

X

_n˜_

= [1] _σ(w[⊤]xi)(1_ _σ(w[⊤]xi))xix[⊤]i_ _[.]_

_n˜_ _−_

_i=1_

X

Following Wang et al. (2019), for each iterationminibatch Br of size 256: _F[ˆ]Br =_ _|Bn˜r|_ _i∈Br_ _[σ][(] r[w] ([⊤]r[x] ≥[i][)(1]1), we estimate the sum with a stochastic[ −]_ _[σ][(][w][⊤][x][i][))][x][i][x]i[⊤]_ [and approximate the]

FIM with a moving average across iterations:

P

_Fˆr = ρrF[ˆ]r_ 1 + (1 _ρr) F[ˆ]_ _r_ _,_ where ρr = min(1 1/r, 0.95).
_−_ _−_ _B_ _−_

To ensure the positive definiteness of the FIM, a damping term 0.01I is added before taking the
inverse. For RSVGD and SVNG, the gradient of the inverse of FIM is estimated with ∇wj _F_ _[−][1]_ _≈_
_−F[ˆ]r[−][1]( ∇[ˆ]_ _w[r]_ _j_ _[F]_ [) ˆ]Fr[−][1], where _∇[ˆ]_ _w[r]_ _j_ _[F][ =][ ρ][r]∇[ˆ]_ _w[r][−]j_ [1][F][ + (1][ −] _[ρ][r][)][∇][w]j_ _F[ˆ]Br_ _._

We run each method for T = 3000 particle updates with learning rates in {0.01, 0.05, 0.1, 0.5, 1}
and average the results for 5 random trials. τ is set to 0.98 for SVNG. For each run, we randomly


-----

keep 20% of the dataset as test data, 20% of the remaining points as the validation set, and all the rest
as the training set. The results of each method on validation sets with all choices of learning rates are
plotted in Fig. 10. We see that the SVNG updates are very robust to the change in learning rates and
is able to accommodate very large learning rates (up to 1) without a significant loss in performance.
The results in Fig. 5 are reported with the learning rate that performs best on the validation set.

1}. Running RSVGD with learning rates 0.5 and 1 produces numerical errors. Therefore, we did not

SVGD SVNG

_−0.52_ _−0.52_

_−0.54_ _−0.54_

_−0.56_ _−0.56_

0.01 0.01

_−0.58_ 0.05 _−0.58_ 0.05

_−0.60_ 0.1 _−0.60_ 0.1

0.5 0.5

_−0.62_ 1.0 _−0.62_ 1.0

Validation log predictive probability Validation log predictive probability

0 1000 2000 3000 0 1000 2000 3000

Number of particle updates, T Number of particle updates, T

RSVGD Matrix SVGD (Avg)

_−0.52_ _−0.52_

_−0.54_ _−0.54_

_−0.56_ _−0.56_

0.01

_−0.58_ _−0.58_ 0.05

_−0.60_ 0.01 _−0.60_ 0.1

0.05 0.5

_−0.62_ 0.1 _−0.62_ 1.0

Validation log predictive probability Validation log predictive probability

0 1000 2000 3000 0 1000 2000 3000

Number of particle updates, T Number of particle updates, T

Matrix SVGD (Mixture)

_−0.52_

_−0.54_

_−0.56_

0.01

_−0.58_ 0.05

_−0.60_ 0.1

0.5

_−0.62_ 1.0

Validation log predictive probability

0 1000 2000 3000

Number of particle updates, T

Figure 10: Logistic regression results on validation sets with learning rates in {0.01, 0.05, 0.1, 0.5,
. Running RSVGD with learning rates 0.5 and 1 produces numerical errors. Therefore, we did not

include them in the plot.

H EXPONENTIAL CONVERGENCE OF CONTINUOUS-TIME ALGORITHMS

We derive a time-inhomogeneous generalization of the Stein Log-Sobolev inequality of Duncan et al.
(2019) and Korba et al. (2020) which ensures the exponential convergence of our continuous-time
algorithms and time-inhomogeneous generalization of the Stein Poincare inequality of Duncan et al.´
(2019) which guarantees exponential convergence near equilibrium (i.e., when qt is sufficiently close
to p). As the results hold for a generic sequence of kernels (Kt)t≥0, the implications apply to both
MSVGD and SVMD.


-----

**Definition 2 (Mirror Stein Log-Sobolev inequality). We define the Mirror Stein Log-Sobolev inequal-**
_ity (cf., Korba et al., 2020, Def. 2) as_

KL(qt _p)_ _Kt_ [(][q][t][, p][) = 1]
_∥_ _≤_ 2[1]λ [MKSD][2] 2λ [E][q][t] [[(][∇][2][ψ][−][1][∇] [log][ q]p[t] [)][⊤][P][K][t][,q][t] _[∇][2][ψ][−][1][∇]_ [log][ q]p[t] []][,]


_where MKSDKt is defined in Eq. (15); PKt,qt : L[2](qt) →_ _L[2](qt) is the kernel integral operator:_
(PKt,qt _ϕ)(·) ≜_ Eqt(θ)[Kt(·, θ)ϕ(θ)] for a general kernel Kt and vector-valued function ϕ on Θ, and
_the stated equality holds whenever integration-by-parts is applicable._
**Proposition 11. Suppose (θt)t≥0 follows the mirrored dynamics (8) with gt chosen to be gq[∗]t,Kt** _[as]_
_in (10). Then, the dissipation of KL(qt_ _p) is_
_∥_
_d_

_dt_ [KL(][q][t][∥][p][) =][ −][MKSD][K][t] [(][q][t][, p][)][2][.]

**Proof** The proof directly follows from Thm. 3 since the optimization problem there matches the
definition of MKSD in (15).

Therefore, when the Mirror Stein Log-Sobolev inequality holds, we have
d

dt [KL(][q][t][∥][p][)][ ≤−][2][λ][KL(][q][t][∥][p][)][,]

and the exponential convergence KL(qt _p)_ KL(q0 _p)e[−][2][λt]_ follows by Gronwall’s lemma (Gron_∥_ _≤_ _∥_
wall, 1919).
**Definition 3 (Mirror Stein Poincare inequality)´** **. We say that the distribution p satisfies the Mirror**
_Stein Poincare inequality (cf., Duncan et al. 2019, Eq. (57)) with strongly convex´_ _ψ and constant λ if_

Varp[φ]
_≤_ _λ[1]_ [E][p][[][∇][φ][⊤][P][K][t][,p][∇][2][ψ][−][1][∇][φ][]]

_for all φ ∈_ _L[2](p) ∩_ _C_ _[∞](Θ) that is locally Lipschitz, where PKt,p is the kernel integral operator_
_under p defined similarly as in Definition 2._

This inequality can also be viewed as a kernelized generalization of the mirror Poincare inequality´
introduced in Chewi et al. (2020, Def. 1) for proving exponential convergence of mirror-Langevin
diffusion. In a manner analogous to Thm. 1 of Chewi et al. (2020), the following proposition relates
the Mirror Stein Poincar´e inequality to chi-squared divergence.

**Proposition 12. Suppose (θt)t≥0 follows the mirrored dynamics (8) with gt chosen to be gq[∗]t,Kt** _[as]_
_in (10). Then, the dissipation of chi-square divergence χ[2](qt_ _p) is_
_∥_
d _⊤_

dt _[χ][2][(][q][t][∥][p][) =][ −][2][E][q][t]_ _∇_ _[q]p[t]_ _PKt,p∇[2]ψ[−][1]∇_ _[q]p[t]_

 

_whenever integration-by-parts is applicable._

**Proof** We first note that by applying integration-by-parts, gq[∗]t,Kt [as in][ (10)][ can be equivalently]
written as
_gq[∗]t,Kt_ [=][ −][P][K]t[,q]t _[∇][2][ψ][−][1][∇]_ [log][ q][t]

_p [.]_

Then using the Fokker-Planck equation of qt under the dynamics, we have

2

_dtd_ _[χ][2][(][q][t][∥][p][) =][ d]dt_ _qpt_ _dp = 2_ _qpt_ _dt_ [=][ −][2][E][q][t] _gt,K[∗]_ _t_ _⊤∇_ _[q]pt_

Z   Z _[·][ dq][t]_  

_⊤_

_⊤_

= 2Eqt _PKt,qt_ _ψ[−][1]_ log _[q][t]_ = 2Eqt _PKt,p_ _ψ[−][1]_ _._
_−_ " _∇[2]_ _∇_ _p_  _∇_ _[q]p[t]_ # _−_ ∇ _[q]p[t]_ _∇[2]_ _∇_ _[q]p[t]_ 

Note that the right hand side of the equation differs from the Mirror Stein Poincare inequality´
only in the base measure of the expectation. Duncan et al. (2019) proposes to replace qt with p
to study the convergence near equilibrium (See their Sec. 6, where Eq. (46) is replaced with Eq.
(51)). If we do the same and combine this identity with the Mirror Stein Poincare inequality, we´
obtain d[d]t _[χ][2][(][q][t][∥][p][)][ ≤−][2][λ][Var][p][[][ q]p[t]_ [] =][ −][2][λχ][2][(][q][t][∥][p][)][, which implies exponential convergence in KL]

KL(qt _p)_ _χ[2](qt_ _p)_ _χ[2](q0_ _p)e[−][2][λt]_ by Gronwall’s lemma (Gronwall, 1919).
_∥_ _≤_ _∥_ _≤_ _∥_


-----

I PROOFS

I.1 PROOF OF PROP. 1

**Proof** Fix any g ∈Gψ. Since g and ∇g are bounded and ∇[2]ψ(θ)[−][1]∇ log p(θ) and ∇· ∇[2]ψ(θ)[−][1]
are p-integrable, the expectation Eθ _p[(_ _p,ψg)(θ)] exists. Because Θ is convex, Θr is bounded and_
_∼_ _M_
convex with Lipschitz boundary. Since p∇[2]ψ[−][1]g ∈ _C_ [1], we have
_|Ep[(Mp,ψg)(θ)]| = |Ep[g(θ)[⊤]∇[2]ψ(θ)[−][1]∇_ log p(θ) + ∇· (∇[2]ψ(θ)[−][1]g(θ))]|

= Θ _∇p(θ)[⊤]∇[2]ψ(θ)[−][1]g(θ) + p(θ)∇· (∇[2]ψ(θ)[−][1]g(θ))dθ_

Z

= Θ _∇· (p(θ)∇[2]ψ(θ)[−][1]g(θ))dθ_

Z


Θr _∇· (p(θ)∇[2]ψ(θ)[−][1]g(θ))dθ_ (by dominated convergence)

(p(θ) _ψ(θ)[−][1]g(θ))[⊤]nr(θ)dθ_ (by the divergence theorem)
_∂Θr_ _∇[2]_


= _r[lim]→∞_

= _r[lim]→∞_

lim
_≤_ _r_
_→∞_


_≤_ _rlim→∞_ _∂Θr_ _p(θ)∥g(θ)∥2_ _∇[2]ψ(θ)[−][1]nr(θ)_ 2[dθ] (by Cauchy-Schwarz)

Z

_≤∥g∥∞_ _rlim→∞_ _∂Θr_ _p(θ)_ _∇[2]ψ(θ)[−][1]nr(θ)_ 2[dθ][ = 0] (by assumption).

Z


I.2 PROOF OF THM. 3: OPTIMAL MIRROR UPDATES IN RKHS

**Proof** Let ei denote the standard basis vector of R[d] with the i-th element being 1 and others being
zeros. Since m _K, we have_
_∈H_
_m(θ)[⊤]∇[2]ψ(θ)[−][1]∇_ log p(θ) = ⟨m, K(·, θ)∇[2]ψ(θ)[−][1]∇ log p(θ)⟩HK


_∇· (∇[2]ψ(θ)[−][1]m(θ)) =_


_θi_ (m(θ)[⊤] _ψ(θ)[−][1]ei)_
_∇_ _∇[2]_
_i=1_

X

_d_

_⟨m, ∇θi_ (K(·, θ)∇[2]ψ(θ)[−][1]ei)⟩HK
_i=1_

X


= ⟨m, ∇θ · (K(·, θ)∇[2]ψ(θ)[−][1])⟩HK _,_
where we define the divergence of a matrix as a vector whose elements are the divergences of each
row of the matrix. Then, we write (9) as
_−_ Eqt [m(θ)[⊤]∇[2]ψ(θ)[−][1]∇ log p(θ) + ∇· (∇[2]ψ(θ)[−][1]m(θ))]

= Eqt [ _m, K(_ _, θ)_ _ψ(θ)[−][1]_ log p(θ) + _θ_ (K( _, θ)_ _ψ(θ)[−][1])_ _K_ ]
_−_ _⟨_ _·_ _∇[2]_ _∇_ _∇_ _·_ _·_ _∇[2]_ _⟩H_

= _m, Eqt_ [K( _, θ)_ _ψ(θ)[−][1]_ log p(θ) + _θ_ (K( _, θ)_ _ψ(θ)[−][1])]_ _K_
_−⟨_ _·_ _∇[2]_ _∇_ _∇_ _·_ _·_ _∇[2]_ _⟩H_
= _m, Eqt_ [ _p,ψK(_ _, θ)]_ _K_ _._
_−⟨_ _M_ _·_ _⟩H_
Therefore, the optimal direction in the HK norm ball BHK = {g : ∥g∥HK 1} that minimizes (9)
is gt[∗] _qt,K_ [=][ E][q]t [[][M][p,ψ][K][(][·][, θ][)]][.] _≤_

_[∝]_ _[g][∗]_

I.3 PROOF OF THM. 4: MIRRORED SVGD UPDATES


**Proof** A p.d. kernel k composed with any map φ is still a p.d. kernel. To prove this, let
_x1, . . ., xp_ = _φ(η1), . . ., φ(ηn)_ _, p_ _n. Then_
_{_ _}_ _{_ _}_ _≤_

_αiαjk(φ(ηi), φ(ηj)) =_ _βℓβmk(xℓ, xm)_ 0,

_≥_

_i,j_ _ℓ,m_

X X


-----

where βℓ = _i_ _Sℓ_ _[α][i][,][ S][ℓ]_ [=][ {][i][ :][ φ][(][η][i][) =][ x][ℓ][}][. Therefore,][ k][ψ][(][η, η][′][) =][ k][(][∇][ψ][∗][(][η][)][,][ ∇][ψ][∗][(][η][′][))][ is a]

_∈_
p.d. kernel. Plugging K = kI into Lem. 13, for any θ[′] _∈_ Θ and η[′] = ∇ψ(θ[′]), we have

_gq[∗]t,Kk_ [(][θ][′][) =][P][ E][η]t[∼][q]t,H [[][K][∇][ψ][∗] [(][∇][ψ][(][θ][′][)][, η][t][)][∇] [log][ p][H] [(][η][t][) +][ ∇][η]t

_[·][ K][∇][ψ][∗]_ [(][∇][ψ][(][θ][′][)][, η][t][)]]


= Eηt _qt,H_ [k( _ψ[∗](η[′]),_ _ψ[∗](ηt))_ log pH (ηt) +
_∼_ _∇_ _∇_ _∇_


_ηt,j_ _k(_ _ψ[∗](η[′]),_ _ψ[∗](ηt))ej]_
_∇_ _∇_ _∇_
_j=1_

X


= Eηt _qt,H_ [kψ(η[′], ηt) log pH (ηt) + _ηt_ _kψ(η[′], ηt)]._
_∼_ _∇_ _∇_

I.4 PROOF OF PROP. 5: SINGLE-PARTICLE SVMD IS MIRROR DESCENT

**Proof** When n = 1, λ1 = k(θt, θt), u1 = 1, and thus Kψ,t(θt, θt) = k(θt, θt)∇[2]ψ(θt).

I.5 PROOF OF THM. 6: CONVERGENCE OF MIRRORED UPDATES AS n →∞

**Proof** The idea is to reinterpret our mirrored updates as one step of a matrix SVGD in η space based
on Lem. 13 and then follow the path of Gorham et al. (2020, Thm. 7). Assume that qt,H[n] [and][ q]t,H[∞] [have]
integrable means. Let η[n], η[∞] be an optimal Wasserstein-1 coupling of qt,H[n] [and][ q]t,H[∞] [. Let][ Φ][q]t[,K]t
denote the transform through one step of mirrored update: θt = ∇ψ[⋆](ηt), ηt+1 = ηt + ϵtgq[∗]t,Kt [(][θ][t][)][.]
Then, with Lem. 13, we have

Φqt,Kt (η) Φqt,Kt (η[′]) 2
_∥_ _−_ _∥_
= ∥η + ϵtgq[∗]t[n][,K][t] [(][θ][)][ −] _[η][′][ −]_ _[ϵ][t][g]q[∗]t[∞][,K][t]_ [(][θ][′][)][∥][2]

_≤∥η −_ _η[′]∥2 + ϵt∥gq[∗]t[n][,K][t]_ [(][θ][)][ −] _[g]q[∗]t[∞][,K][t]_ [(][θ][′][)][∥][2]

_η_ _η[′]_ 2
_≤∥_ _−_ _∥_
+ ϵt∥Eηn [K∇ψ∗,t(η, η[n])∇ log pH (η[n]) + ∇ηn · K∇ψ∗,t(η, η[n])

_−_ (K∇ψ∗,t(η[′], η[n])∇ log pH (η[n]) + ∇ηn · K∇ψ∗,t(η[′], η[n]))]∥2
+ ϵt∥Eηn,η∞ [K∇ψ∗,t(η[′], η[n])∇ log pH (η[n]) + ∇ηn · K∇ψ∗,t(η, η[n])

_−_ (K∇ψ∗,t(η[′], η[∞])∇ log pH (η[∞]) + ∇η∞ _· K∇ψ∗,t(η[′], η[∞]))]∥2_
_η_ _η[′]_ 2 + ϵtc1(1 + E[ _η[n]_ 2)] _η_ _η[′]_ 2 + ϵtc2(1 + _η[′]_ 2)Eη[n],η[∞] [ _η[n]_ _η[∞]_ 2]
_≤∥_ _−_ _∥_ _∥_ _∥_ _∥_ _−_ _∥_ _∥_ _∥_ _∥_ _−_ _∥_

= ∥η − _η[′]∥2 + ϵtc1(1 + Eqt,H[n]_ [[][∥·∥][2][])][∥][η][ −] _[η][′][∥][2][ +][ ϵ][t][c][2][(1 +][ ∥][η][′][∥][2][)][W][1][(][q]t,H[n]_ _[, q]t,H[∞]_ [)][.]

Since Φqt,Kt (η[n]) ∼ _qt[n]+1,H_ [,][ Φ][q]t[,K][(][η][∞][)][ ∼] _[q]t[∞]+1,H_ [, we conclude]

_W1(qt[n]+1,H_ _[, q]t[∞]+1,H_ [)]


_≤_ E[∥Φqt,K(η[n]) − Φqt,K(η[∞])∥2]

_≤_ (1 + ϵtc1(1 + Eqt,H[n] [[][∥·∥][2][]))][E][[][∥][η][n][ −] _[η][∞][∥][2][] +][ ϵ][t][c][2][(1 +][ ∥][η][′][∥][2][)][W][1][(][q]t,H[n]_ _[, q]t,H[∞]_ [)]]

_≤_ (1 + ϵtc1(1 + Eqt,H[n] [[][∥·∥][2][]) +][ ϵ][t][c][2][(1 +][ E][q]t,H[∞] [[][∥·∥][2][]))][W][1][(][q]t,H[n] _[, q]t,H[∞]_ [)][.]

The final claim qt[n] _t_ now follows by the continuous mapping theorem as _ψ[∗]_ is continuous.

_[⇒]_ _[q][∞]_ _∇_

I.6 PROOF OF THM. 7: INFINITE-PARTICLE MIRRORED STEIN UPDATES DECREASE KL AND
MKSD


**Proof** Let Tqt[∞][,K][t][ denote transform of the density function through one step of mirrored update:]
_θt = ∇ψ[⋆](ηt), ηt+1 = ηt + ϵtgq[∗]t[∞][,K][t]_ [(][θ][t][)][. Then]

KL(qt[∞]+1[∥][p][)][ −] [KL][(][q]t[∞]

_[∥][p][)]_

= KL(qt[∞][∥][T][ −]qt[∞][1][,K][t] _[p][)][ −]_ [KL][(][q]t[∞][∥][p][)]

= Eηt∼qt,H[∞] [[log][ p][H] [(][η][t][)][ −] [log][ p][H] [(][η][t][ +][ ϵ][t][g]q[∗]t[∞][,K][t] [(][θ][t][))][ −] [log][ |][ det(][I][ +][ ϵ][t][∇][η][t] _[g]q[∗]t[∞][,K][t]_ [(][θ][t][))][|][]][,]


-----

where we have used the invariance of KL divergence under reparameterization: KL(qt _p) =_
_∥_
KL(qt,H _pH_ ) . Following Liu (2017), we bound the difference of the first two terms as
_∥_

log pH (ηt) − log pH (ηt + ϵtgq[∗]t[∞][,K][t] [(][θ][t][))]

1
= − 0 _∇s log pH_ (ηt(s)) ds, where ηt(s) ≜ _ηt + sϵtgq[∗]t[∞][,K][t]_ [(][θ][t][)]
Z

1
= − 0 _∇_ log pH (ηt(s))[⊤](ϵtgq[∗]t[∞][,K][t] [(][θ][t][))][ ds]
Z

1
= −ϵt∇ log pH (ηt)[⊤]gq[∗]t[∞][,K][t] [(][θ][t][) +] 0 (∇ log pH (ηt) −∇ log pH (ηt(s)))[⊤](ϵtgq[∗]t[∞][,K][t] [(][θ][t][))][ ds]
Z

1

_≤−ϵt∇_ log pH (ηt)[⊤]gq[∗]t[∞][,K][t] [(][θ][t][) +][ ϵ][t] 0 _∥∇_ log pH (η) −∇ log pH (ηt(s))∥2 · ∥gq[∗]t[∞][,K][t] [(][θ][t][)][∥][2][ ds]

Z

_ϵt_ log pH (ηt)[⊤]gq[∗]t[∞][,K][t] [(][θ][t][) +][ Lϵ]t[2] _qt[∞][,K][t]_ [(][θ][t][)][∥]2[2][,]
_≤−_ _∇_ 2 _[∥][g][∗]_

and bound the log determinant term using Lem. 15:

_−_ log | det(I + ϵt∇ηt _gq[∗]t[∞][,K][t]_ [(][θ][t][))][ ≤−][ϵ][t][ Tr(][∇][η][t] _[g]q[∗]t[∞][,K][t]_ [(][θ][t][)) + 2][ϵ]t[2][∥∇][η]t _[g]q[∗]t[∞][,K][t]_ [(][θ][t][)][∥]F[2] _[.]_

The next thing to notice is that Eηt∼qt,H[∞] [[][∇] [log][ p][H] [(][η][t][)][⊤][g]q[∗]t[∞][,K][t] [(][θ][t][) + Tr(][∇][η][t] _[g]q[∗]t[∞][,K][t]_ [(][θ][t][))]][ is the]
square of the MKSD in (15). We can show this equivalence using the identity proved in Lem. 14:

Eηt∼qt,H[∞] [[][g]q[∗]t[∞][,K][t] [(][θ][t][)][⊤][∇] [log][ p][H] [(][η][t][) + Tr(][∇][η][t] _[g]q[∗]t[∞][,K][t]_ [(][θ][t][))]]

= Eθt∼qt[∞] [[][g]q[∗]t[∞][,K][t] [(][θ][t][)][⊤][∇][2][ψ][(][θ][t][)][−][1][∇][θ][t] [(log][ p][(][θ][t][)][ −] [log det][ ∇][2][ψ][(][θ][t][))]

+ Tr(∇[2]ψ(θt)[−][1]∇gq[∗]t[∞][,K][t] [(][θ][t][))]]

= Eθt∼qt[∞] [[][g]q[∗]t[∞][,K][t] [(][θ][t][)][⊤][∇][2][ψ][(][θ][t][)][−][1][∇] [log][ p][(][θ][t][) +][ ∇·][ (][∇][2][ψ][(][θ][t][)][−][1][g]q[∗]t[∞][,K][t] [(][θ][t][))]] (Lem. 14)

= Eθt∼qt[∞] [[(][M][p,ψ][g]q[∗]t[∞][,K][t] [)(][θ][t][)]]

= MKSDKt (qt[∞][, p][)][2][.]


Finally, we are going to bound ∥gq[∗]t[∞][,K][t] [(][θ][t][)][∥]2[2] [and][ ∥∇][η]t _[g]q[∗]t[∞][,K][t]_ [(][θ][t][)][∥]F[2] [. From the assumptions we]
have ψ is α-strongly convex and thus ψ[∗] is _α[1]_ [-strongly smooth (Kakade et al., 2009), therefore]

_∥∇[2]ψ[∗](·)∥2 ≤_ _α[1]_ [. By Lem. 16, we know]

_∥gq[∗]t[∞][,K][t]_ [(][θ][t][)][∥]2[2] _[≤∥][g]q[∗]t[∞][,K][t]_ _[∥][2]HKt_ _[∥][K][(][θ][t][, θ][t][)][∥][op]_ [=][ MKSD][K]t [(][q]t[∞][, p][)][2][∥][K][t][(][θ][t][, θ][t][)][∥][op][,]

_∥∇ηt_ _gq[∗]t[∞][,K][t]_ [(][θ][t][)][∥]F[2] [=][ ∥∇][2][ψ][∗][(][η][t][)][∇][g]q[∗]t[∞][,K][t] [(][θ][t][)][∥]F[2]
_≤∥∇[2]ψ[∗](ηt)∥2[2][∥∇][g]q[∗]t[∞][,K][t]_ [(][θ][t][)][∥]F[2]


_≤_ _α[1][2][ ∥][g]q[∗]t[∞][,K][t]_ _[∥][2]HKt_


_i,d+i[K][t][(][θ][t][, θ][t][)][∥][op]_
_∥∇[2]_
_i=1_

X


= [1] _t_ _[, p][)][2]_

_α[2][ MKSD][K][t]_ [(][q][∞]


_∥∇i,d[2]_ +i[K][t][(][θ][t][, θ][t][)][∥][op][,]
_i=1_

X


where ∇i,d[2] +i[K][(][θ, θ][)][ denotes][ ∇][2]θi,θi[′] _[K][(][θ, θ][′][)][|][θ][′][=][θ][. Combining all of the above, we have]_


KL(qt[∞]+1[∥][p][)][ −] [KL][(][q]t[∞]

_[∥][p][)]_

_ϵt_ _t_ sup _Kt(θ, θ)_ op _t_

_≤−_ _−_ _[Lϵ]2[2]_ _θ_ _∥_ _∥_ _−_ [2]α[ϵ][2][2]


sup _i,d+i[K][t][(][θ, θ][)][∥][op]_
_θ_ _∥∇[2]_
_i=1_

X


MKSDKt (qt[∞][, p][)][2][.]


Plugging in the definition of κ1 and κ2 finishes the proof.


-----

I.7 PROOF OF THM. 8: MKSDKk DETERMINES WEAK CONVERGENCE

**Proof** According to Thm. 4,

_gq,K[∗]_ _k_ [=][ E][q]H [[][k][(][·][,][ ∇][ψ][∗][(][η][))][∇] [log][ p][H] [(][η][) +][ ∇][η][k][(][∇][ψ][∗][(][η][)][,][ ·][)]][,]

where qH (η) denotes the density of η = _ψ(θ) under the distribution θ_ _q. From the assumptions_
_∇_ _∼_
we have k(θ, θ[′]) = κ(∇ψ(θ), ∇ψ(θ[′])). With this specific choice of k, the squared MKSD is

MKSDKk (q, p)[2] = ∥gq,K[∗] _k_ _[∥][2]HKk_


1

_pH_ (η)pH (η[′]) _[∇][η][∇][η][′]_ [(][p][H] [(][η][)][k][(][∇][ψ][∗][(][η][)][,][ ∇][ψ][∗][(][η][′][))][p][H] [(][η][′][))]


1

_._ (21)
_pH_ (η)pH (η[′]) _[∇][η][∇][η][′]_ [(][p][H] [(][η][)][κ][(][η, η][′][)][p][H] [(][η][′][))]



= Eη,η′∼qH

= Eη,η′∼qH


The final expression in (21) is the squared kernel Stein discrepancy (KSD) (Liu et al., 2016;
Chwialkowski et al., 2016; Gorham & Mackey, 2017) between qH and pH with the kernel κ:
KSDκ(qH _, pH_ )[2]. Recall that it is proved in Gorham & Mackey (2017, Theorem 8) that, for
_κ(x, y) = (c[2]_ + ∥x − _y∥2[2][)][β][ with][ β][ ∈]_ [(][−][1][,][ 0)][ and distantly dissipative][ p][H] [with Lipschitz score]
functions,now follows by the continuous mapping theorem as qH ⇒ _pH if KSDκ(qH_ _, pH_ ) → 0. The advertised result ( ∇ψ[∗] is continuous.q ⇒ _p if MKSDKk_ (q, p) → 0)

J LEMMAS

**Lemma 13. Let K∇ψ∗** (η, η[′]) ≜ _K(∇ψ[∗](η), ∇ψ[∗](η[′])). The mirrored updates gq[∗]t,K_ _[in][ (10)][ can be]_
_equivalently expressed as_

_gq[∗]t,K_ [=][ E][q]t,H [[][K][∇][ψ][∗] [(][∇][ψ][(][·][)][, η][)][∇] [log][ p][H] [(][η][) +][ ∇][η][ ·][ K][∇][ψ][∗] [(][∇][ψ][(][·][)][, η][)]][.]

**Proof** We will use the identity proved in Lem. 14.

_gq[∗]t,K_ [=][ E][q]t [[][M][p,ψ][K][(][·][, θ][)]]

= Eqt [K(·, θ)∇[2]ψ(θ)[−][1]∇ log p(θ) + ∇θ · (K(·, θ)∇[2]ψ(θ)[−][1])]

= Eqt [K(·, θ)∇[2]ψ(θ)[−][1]∇θ(log pH (∇ψ(θ)) + log det ∇[2]ψ(θ)) +(by change-of-variable formula) ∇θ · (K(·, θ)∇[2]ψ(θ)[−][1])]


= Eqt [K(·, θ)∇[2]ψ(θ)[−][1]∇θ log pH (∇ψ(θ)) +

= Eqt [K(·, θ)∇[2]ψ(θ)[−][1]∇θ log pH (∇ψ(θ)) +



[ _ψ(θ)[−][1]]ij_ _θi_ _K(_ _, θ):,j]_
_∇[2]_ _∇_ _·_
_i,j=1_

X

(by applying Lem. 14 to each row of K(·, θ))

_d_

_ηj_ _K(_ _, θ):,j]_
_∇_ _·_
_j=1_

X


= Eqt,H [K(·, ∇ψ[∗](η))∇ log pH (η) +


_ηj_ _K(_ _,_ _ψ[∗](η)):,j]_
_∇_ _·_ _∇_
_j=1_

X


= Eqt,H [K∇ψ∗ (∇ψ(·), η)∇ log pH (η) + ∇η · K∇ψ∗ (∇ψ(·), η)],

where A:,j denotes the j-th column of a matrix A.

**Lemma 14. For a strictly convex function ψ ∈** _C_ [2] : R[d] _→_ R and any vector-valued g ∈ _C_ [1] : R[d] _→_
R[d], the following relation holds:

_∇· (∇[2]ψ(θ)[−][1]g(θ)) = Tr(∇[2]ψ(θ)[−][1]∇g(θ)) −_ _g(θ)[⊤]∇[2]ψ(θ)[−][1]∇θ log det ∇[2]ψ(θ)._

**Proof** By the product rule of differentiation:

_∇· (∇[2]ψ(θ)[−][1]g(θ)) = Tr(∇[2]ψ(θ)[−][1]∇g(θ)) + g(θ)[⊤]∇· (∇[2]ψ(θ)[−][1])._ (22)


-----

This already gives us the first term on the right side. Next, we have

[ _ψ(θ)[−][1]_ log det _ψ(θ)]i_
_∇[2]_ _∇_ _∇[2]_

_d_

= [∇[2]ψ(θ)[−][1]]ij Tr(∇[2]ψ(θ)[−][1]∇θj _∇[2]ψ(θ))_

_j=1_

X



[ _ψ(θ)[−][1]]ij_
_∇[2]_
_j=1_

X



[ _ψ(θ)[−][1]]ℓm[_ _θj_ _ψ(θ)]mℓ_
_∇[2]_ _∇_ _∇[2]_
_ℓ,m=1_

X



[ _ψ(θ)[−][1]]ij[_ _ψ(θ)[−][1]]ℓm_ _θj_ _ψ(θ)mℓ_
_∇[2]_ _∇[2]_ _∇_ _∇[2]_
_j,ℓ,m=1_

X

_d_

[ _ψ(θ)[−][1]]ij_ _θm_ _ψ(θ)jℓ[_ _ψ(θ)[−][1]]ℓm_
_∇[2]_ _∇_ _∇[2]_ _∇[2]_
_j,ℓ,m=1_

X


_θm(_ _ψ(θ)[−][1])im_
_∇_ _∇[2]_
_m=1_

X


= −


= [ _ψ(θ)[−][1]]i._
_−_ _∇· ∇[2]_
Plugging the above relation into (22) proves the claimed result.

**Lemma 15 (Liu, 2017, Lemma A.1). Let A be a square matrix, and 0 < ϵ <** 2∥A+1A[⊤]∥op _[. Then,]_

log | det(I + ϵA)| ≥ _ϵ Tr(A) −_ 2ϵ[2]∥A∥F[2] _[,]_
_where ∥· ∥F denotes the Frobenius norm of a matrix._
**Lemma 16. Let K be a matrix-valued kernel and HK be the corresponding RKHS. Then, for any**
_f ∈HK (f is vector-valued), we have_


_∥f_ (x)∥2 ≤∥f _∥HK_ _∥K(x, x)∥op[1][/][2][,]_ _∥∇f_ (x)∥F[2] _[≤∥][f]_ _[∥]H[2]_ _K_


_∥∇x[2]_ _i,x[′]i_ _[K][(][x, x][′][)][|][x][′][=][x][∥][op][,]_
_i=1_

X


_where ∥· ∥op denotes the operator norm of a matrix induced by the vector 2-norm._

**Proof** We first bound the ∥f (x)∥2 as

_∥f_ (x)∥2 = _ysup2=1_ _f_ (x)[⊤]y = _ysup2=1⟨f, K(·, x)y⟩HK ≤∥f_ _∥HK_ _ysup2=1∥K(·, x)y∥HK_
_∥_ _∥_ _∥_ _∥_ _∥_ _∥_

= _f_ _K_ sup (y[⊤]K(x, x)y)[1][/][2] _f_ _K_ sup sup (u[⊤]K(x, x)y)[1][/][2]
_∥_ _∥H_ _y_ 2=1 _≤∥_ _∥H_ _y_ 2=1 _u_ 2=1
_∥_ _∥_ _∥_ _∥_ _∥_ _∥_

= ∥f _∥HK_ _ysup2=1∥K(x, x)y∥2[1][/][2]_ = ∥f _∥HK_ _∥K(x, x)∥op[1][/][2][.]_
_∥_ _∥_

The second result follows similarly,


sup ( _xi_ _f_ (x)[⊤]y)[2] =
_i=1_ _∥y∥2=1_ _∇_

X


_∥∇f_ (x)∥F[2] [=]


_∥∇xi_ _f_ (x)∥2[2] [=]
_i=1_

X


_i=1_ _∥ysup∥2=1(⟨f, ∇xi_ _K(·, x)y⟩HK_ )[2]

X

_d_

_i=1_ _∥ysup∥2=1(y[⊤]∇x[2]_ _i,x[′]i_ _[K][(][x, x][′][)][|][x][=][x][′]_ _[y][)]_

X


_f_ _K_
_≤∥_ _∥[2]H_

_f_ _K_
_≤∥_ _∥[2]H_

= _f_ _K_
_∥_ _∥[2]H_


_i=1_ _∥ysup∥2=1∥∇xi_ _K(·, x)y∥[2]HK_ [=][ ∥][f] _[∥][2]HK_

X


_i=1_ _∥ysup∥2=1_ _∥usup∥2=1(u[⊤]∇x[2]_ _i,x[′]i_ _[K][(][x, x][′][)][|][x][=][x][′]_ _[y][)]_

X

_d_

_∥∇x[2]_ _i,x[′]i_ _[K][(][x, x][′][)][|][x][′][=][x][∥][op][.]_
_i=1_

X


-----

