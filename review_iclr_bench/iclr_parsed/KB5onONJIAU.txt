# COMPARING DISTRIBUTIONS BY MEASURING DIF## FERENCES THAT AFFECT DECISION MAKING

**Shengjia Zhao[∗], Abhishek Sinha[∗], Yutong He[∗], Aidan Perreault, Jiaming Song, Stefano Ermon**
Department of Computer Science
Stanford University
_{sjzhao,a7b23,kellyyhe,aperr,tsong,ermon}@stanford.edu_

ABSTRACT

Measuring the discrepancy between two probability distributions is a fundamental
problem in machine learning and statistics. We propose a new class of discrepancies based on the optimal loss for a decision task – two distributions are different
if the optimal decision loss is higher on their mixture than on each individual
distribution. By suitably choosing the decision task, this generalizes the JensenShannon divergence and the maximum mean discrepancy family. We apply our
approach to two-sample tests, and on various benchmarks, we achieve superior
test power compared to competing methods. In addition, a modeler can directly
specify their preferences when comparing distributions through the decision loss.
We apply this property to understanding the effects of climate change on different
economic activities and selecting features targeting different decision tasks.

1 INTRODUCTION

Quantifying the difference between two probability distributions is a fundamental problem in machine learning. Modelers choose different types of discrepancies (or probability divergences) to
encode their prior knowledge about which aspects are relevant to evaluate the difference. Integral probability metrics (IPMs, M¨uller (1997)) and f -divergences (Csisz´ar, 1964) are widely used
discrepancies in machine learning. IPMs, such as the Wasserstein distance, maximum mean discrepancy (MMD) (Rao, 1982; Burbea & Rao, 1984; Gretton et al., 2012), are based on the idea that if two
distributions are identical, any function should have the same expectation under both distributions.
IPMs are used to define training objectives for generative models (Arjovsky et al., 2017), perform
independence tests (Doran et al., 2014), robust optimization (Esfahani & Kuhn, 2018) among many
other applications. f -divergences, such as the KL divergence and the Jensen Shannon divergence,
are based on the idea that if two distributions are identical, they assign the same likelihood to every
point. One can then define a discrepancy based on how different the likelihood ratio is from one.
KL divergence underlies some of the most commonly used training objectives for both supervised
and unsupervised machine learning algorithms, such as cross entropy loss.

We propose a third category of divergences called H-divergences that overlaps with but also extends
the set of integral probability metrics or the set f -divergences. Intuitively, H-divergence compares
two distributions in terms of the optimal loss for a certain decision task. This optimal loss corresponds to a generalized notion of entropy (DeGroot et al., 1962). Instead of measuring the best
average code length of any encoding scheme (Shannon entropy), the generalized entropy uses arbitrary loss function (rather than code length) and set of actions (rather than encoding schemes), and
is defined as the best expected loss among the set of actions. In particular, given two distribution p
and q, we compare the generalized entropy of the mixture distribution (p + _q)/2 and the generalized_
entropy of p and q individually. Intuitively, if p and q are different, it is more difficult to minimize
expected loss under the mixture distribution (p + q)/2, and hence the mixture distribution should
have higher generalized entropy; if p and q are identical, then the mixture distribution is identical to
_p or q, and hence should have the same generalized entropy._

Our divergence strictly generalizes the maximum mean discrepancy family and the Jensen Shannon
divergence, which can be obtained with specific choices of the loss function. We illustrate this via

_∗Co-first author_


-----

Figure 1: Relationship between H-divergence (this paper) and existing divergences. The Jensen
Shannon divergence is an f -divergence but not an IPM; the MMD is an IPM but not always an
_f_ -divergence; both are H-divergences. There are H-divergences that are not f -divergences or IPMs.

the Venn diagram in Figure 1. Our formulation allows us to choose alternative losses to leverage
inductive biases and machine learning models from different problem domains. For example, if we
choose the generalized entropy as the maximum log likelihood of deep generative models, we are
able to leverage recent progress in modeling high dimensional images.

We demonstrate the effectiveness of H-divergence in two sample tests, i.e. to decide whether two
sets of samples come from the same distribution or not. A test based on a probability discrepancy
declares two sets of samples different if their discrepancy exceeds some threshold. We use Hdivergences based on generalized entropy defined by the log likelihood of off-the-shelf generative
models. Compared to state-of-the-art tests based on MMD with deep kernels (Liu et al., 2020), tests
based on the H-divergence achieve better test power (given identical type I error) on a large set of
benchmarks.

More importantly, scientists and policy makers are often interested not only in if two distributions are
different, but how two distributions are different and whether the differences affect decision making.
Typical divergence measures (such as KL) or two sample tests only quantify if two distributions are
different, while we show that H-divergence is a useful tool for quantifying how distributions are
different with three application examples: studying the effect of climate change, feature selection,
and sample quality evaluation. In each of these examples, we compare different aspects of the
distributions by choosing specific decision loss functions. For example, climate change (Figure 3)
might impact agriculture in a region but not energy production, or vice versa. By choosing suitable
loss functions (related to agriculture, energy, etc) we can quantify and test if the change in climate
distribution impact different economic activities.

2 BACKGROUND

2.1 PROBABILITY DIVERGENCES

Let X denote a finite set or a finite dimensional vector space, and P(X ) denote the set of probability
distributions on X that have a density. We consider the problem of defining a probability divergence
between any two distributions in P(X ), where a probability divergence is any function D : P(X ) ×
_P(X_ ) → R that satisfies D(p∥q) ≥ 0, D(p∥p) = 0, ∀p, q ∈P(X ). We call the divergence D
“strict” if D(p∥q) > 0 ∀p ̸= q, and “non-strict“ otherwise. In this paper we consider both types of
divergences.

**Integral Probability Metrics** Let F denote a set of functions X → R. An integral probability
metric is defined as IPM (p _q) = supf_ Ep[f (X)] Eq[f (X)] . Several important divergences
belong to integral probability metrics. Examples include the Wasserstein distance, whereF _∥_ _∈F |_ _−_ _|_ _F is the set_
of 1-Lipschitz functions; the total variation distance, where F is the set of functions X → [−1, 1].
The maximum mean discrepancy (MMD) (Rao, 1982; Burbea & Rao, 1984; Gretton et al., 2012)
chooses a kernel function k : X × X → R+ and is defined by

MMD[2](p∥q) = Ep,pk(X, Y ) + Eq,qk(X, Y ) − 2Ep,qk(X, Y )

MMD is an IPM where F is the unit norm functions in the reproducing kernel Hilbert space (RKHS)
associated with the kernel k.


-----

_ff_ **-Divergences-Divergence is defined as (assuming densities exist)Given any convex continuous function Df f(p :∥ Rq) =+ → Eq[fR( such thatp(X)/q(X f))](1) = 0. Examples, the**
_tinclude the KL divergence, where(t + 1) log_ _t+12_ + t log t. _f : t 7→_ _t log t and the Jensen Shannon divergence, where f :_
_7→_
 

2.2 H-ENTROPY

For any action space A and any loss function ℓ : X × A → R, the H-entropy (DeGroot et al., 1962;
DeGroot, 2005; Gr¨unwald et al., 2004) is defined as

_Hℓ(p) = inf_
_a∈A_ [E][p][[][ℓ][(][X, a][)]]

In words, H-entropy is the Bayes optimal loss of a decision maker who must select some action a
not for a particular x, but in expectation for a random x drawn from p(x). H-entropy generalizes
several important notions of uncertainty. Examples include: Shannon Entropy, where A as the set of
probabilities P(X ), and ℓ(x, a) = − log a(x); Variance where A = X, and ℓ(x, a) = ∥x−a∥2[2][;][ Pre-]
_dictive V-entropy, where A ⊂P(X_ ) is some subset of distributions, and ℓ(x, a) = − log a(x) (Xu
et al., 2020).

A key property we will use is that H-entropy is concave (DeGroot et al., 1962).

**Lemma 1. For any choice of ℓ** : X × A → R, Hℓ _is a concave function._

This Lemma can be proved by observing that inf is a concave function: it is always better to pick an
optimal action for p and q separately rather than a single one for both.

_Hℓ_ _αp + (1_ _α)q_ = inf
_−_ _a_ [(][α][E][p][[][ℓ][(][X, a][)] + (1][ −] _[α][)][E][q][[][ℓ][(][X, a][)])]_
 α inf 

_≥_ _a_ [E][p][[][ℓ][(][X, a][)] + (1][ −] _[α][) inf]a_ [E][q][[][ℓ][(][X, a][)] =][ αH][ℓ][(][p][) + (1][ −] _[α][)][H][ℓ][(][q][)]_

This Lemma reflects why Hℓ can be thought of as a measurement of entropy or uncertainty. If the
distribution is more uncertain (e.g. a mixture of p and q, rather than p or q separately) then decisions
made under higher uncertainty will suffer a higher loss.

3 DEFINITION AND THEORETICAL PROPERTIES

3.1 H-JENSEN SHANNON DIVERGENCE

As a warm up, we present a special case of our divergence.

**Definition 1 (H-Jensen Shannon divergence).**

_p + q_

_Dℓ[JS][(][p, q][) =][ H][ℓ]_ _Hℓ(p) + Hℓ(q)_ (1)

2 _−_ 2[1]

 

  

_Dℓ[JS]_ is always non-negative because H-entropy is concave (Lemma 1), and clearly Dℓ[JS][(][p, q][) = 0]
whenever p = q. Therefore, Dℓ[JS] is a valid probability divergence. In particular, if we choose Hℓ
as the Shannon entropy, Definition 1 recovers the Jensen Shannon divergence. Other special loss
function choices can recover definitions in (Burbea & Rao, 1982).

3.2 GENERAL H-DIVERGENCE

In addition to the H-Jensen Shannon divergence, there are other functions based on the H-entropy
that satisfy the requirements of a divergence. For example,

_p + q_

_Dℓ[Min]_ = Hℓ 2 _−_ min(Hℓ(p), Hℓ(q)) (2)

 

is also a valid divergence (this will be proved later as a special case of Lemma 2). We can define a
general set of divergences that includes the above two divergences with the following definition:


-----

**Definition 2 (H-divergence). For two distributions p, q on X** _, given any continuous function φ :_
R[2] _→_ R such that φ(θ, λ) > 0 whenever θ + λ > 0 and φ(0, 0) = 0, define

_p + q_ _p + q_
_Dℓ[φ][(][p][∥][q][) =][ φ]_ _Hℓ_ _Hℓ(p), Hℓ_ _Hℓ(q)_

2 _−_ 2 _−_

     

Intuitively Hℓ _p+2_ _q_ _Hℓ(p) and Hℓ_ _p+2_ _q_ _Hℓ(q) measure how much more difficult it is to_

_−_ _−_

minimize loss on the mixture distribution (p + _q)/2 than on p and q respectively. φ is a general class_

     

of functions that map these differences into a scalar divergence, while satisfying some desirable
properties described in the next section.

The following proposition shows that the H-divergence generalizes the previous definitions (1) and
(2). Therefore, any property of H-divergence is inherited by e.g. the H-Jensen Shannon divergence.

**Proposition 1. If φ(θ, λ) =** _[θ][+]2_ _[λ]_ _then Dℓ[φ][(][p, q][)][ is the H-Jensen Shannon divergence in Eq.(1). If]_

_φ(θ, λ) = max(θ, λ) then Dℓ[φ][(][p, q][)][ is the H-Min divergence in Eq.(2).]_

3.3 PROPERTIES OF THE H-DIVERGENCE

We first verify that Dℓ[φ] [is indeed a (strict or non-strict) probability divergence.]

**Lemma 2. For any choice of ℓ** _and for any choice of φ that satisfy Definition 2, Dℓ[φ]_ _[is non-negative]_
_and Dℓ[φ][(][p, q][) = 0][ whenever][ p][ =][ q][. Furthermore,][ D]ℓ[φ]_ _[is symmetric whenever][ φ][ is symmetric.]_

Depending on the choice of ℓ, H-divergence may or may not be strict (i.e. whenever p ̸= q,
_D(p∥q) > 0). The following proposition characterizes conditions for a strict divergence._

**Proposition 2 (Strict Divergence). For any choice of φ the following are equivalent 1) ∀p ̸= q,**
_Dℓ(p∥q) > 0. 2) The H-entropy Hℓ(p) := inf_ _a Ep[ℓ(X, a)] is strictly convex in p. 3) ∀p ̸= q,_
arg inf _a Ep[ℓ(X, a)] ∩_ arg inf _a Eq[ℓ(X, a)] = ∅._

In particular, this proposition can be used to characterize all strict H-divergences, because the set of
all losses ℓ that induces strict H-entropy functions Hℓ can be characterized by Fenchel duality (Duchi
et al., 2018).

One important property of the H-divergence is that two distributions have non-zero divergence if and
only if they have different optimal actions, i.e. the optimal solutions for their respective H-entropy
are different. This is shown in the following proposition (proof in Appendix A).

**Proposition 3. Dℓ[φ][(][p][∥][q][)][ >][ 0][ if and only if][ arg inf]** _[a][ E][p][[][ℓ][(][X, a][)]][ ∩]_ [arg inf] _[a][ E][q][[][ℓ][(][X, a][)] =][ ∅][.]_

Intuitively, Dℓ[φ] [only takes into account differences between distributions that lead to different opti-]
mal action choices. This property allows us to incorporate prior domain knowledge. By choosing A
and ℓ we can specify which differences between distributions lead to different optimal actions, and
which differences do not. For example, we can choose A as a set of generative models (e.g., mixture
of Gaussians) and ℓ(x, a) as the negative log likelihood of x under generative model a. If under
two distributions we end up learning the same generative model (by maximizing log likelihood), the
H-divergence between them is zero.

3.4 RELATIONSHIP TO MMD

An important special case of the H-divergence is the set of squared Maximum Mean Discrepencies
(MMD), as shown by the following theorem:

**Theorem 1. The set of H-Jensen Shannon Divergences is strictly larger than the MMD[2]** _distances._

To prove this theorem, we show that for each choice of kernel k : X × X → R, there exists an
action space A and loss ℓ such that the corresponding squared MMD distance and H-divergence are
the same (see proof in Appendix A). In particular, this equivalence can be achieved by choosing A
to be the RKHS of k( _,_ ), and ℓ(x, a) = 4 _k(x,_ ) _a_
_H_ _·_ _·_ _∥_ _·_ _−_ _∥H[2]_ [. Inclusion is strict because the Jensen]
Shannon divergence is a H-Jensen Shannon Divergence but not a squared MMD distance.


-----

3.5 ESTIMATION AND CONVERGENCE

Many machine learning tasks can be reduced to the problem of estimating the divergence between
two distributions given samples. Specifically, suppose we are provided with a set of m i.i.d. samples
_pˆm = (x1, · · ·, xm) drawn from distribution p and ˆqm = (x[′]1[,][ · · ·][, x]m[′]_ [)][ drawn from distribution][ q][,]
and would like to obtain an estimate of Dℓ[φ][(][p][∥][q][)][ based on the samples. Here,][ ˆ]pm and ˆqm denote
empirical distributions drawn from p and q respectively. In this section we propose an empirical
estimator for the H-divergence and show that it has favorable convergence properties.

Let _D[ˆ]_ _ℓ[φ][(ˆ]pm∥qˆm) be the empirical (random) estimator for Dℓ[φ][(][p][∥][q][)][ defined by]_


_Dˆ_ _ℓ[φ][(ˆ]pm∥qˆm) =φ_


_ℓ(x[′′]i_ _[, a][)][ −]_ [inf]a
_i=1_

X


_ℓ(x[′′]i_ _[, a][)][ −]_ [inf]a
_i=1_

X


_ℓ(x[′]i[, a][)]_
_i=1_

X


inf


_ℓ(xi, a), inf_
_a_
_i=1_

X


where x[′′]i [=][ x][i][b][i][ +][ x]i[′] [(1][ −] _[b][i][)][ and][ b][i][ are i.i.d uniformly sampled from][ {][0][,][ 1][}][, so that][ x][′′]i_ [is a sample]
from the mixture distribution (p + q)/2 of size m.

Using x[′′]i [as defined above is crucial for the convergence properties we will prove in Theorem 2. It]
might be tempting to replace the term _m1_ _mi=1_ _[ℓ][(][x]i[′′][, a][)][ with]_ 21m _mi=1[(][ℓ][(][x][i][, a][) +][ ℓ][(][x]i[′]_ _[, a][))][ to use]_

all the available samples. However, optimizing the action based on a finite set of samples (instead of
in expectation) is prone to overfitting, and introduces bias. Intuitively, usingP P _m samples (x[′′]i_ [) ensures]
the bias for the mixture is comparable to that of p and q. Without this, Theorem 2 is no longer true,
and empirical performance also degrades.

Before presenting the convergence results, we first must define several assumptions that make convergence possible. In particular, we are going to assume that the loss function ℓ is C-bounded, i.e.
there exists some C such that 0 ≤ _ℓ(x, a) ≤_ _C, ∀a, x. This assumption seemingly exclude important_
special cases such as the Jensen-Shannon divergence (which is associated with the unbounded log
loss). However, we show in the appendix that the Jensen-Shannon divergence cannot be consistently
estimated in general, hence correctly excluded by our theorem. One practical solution is to clip the
log likelihood, which is the approach adopted in (Song & Ermon, 2019) for improved divergence
estimation (for a similar KL divergence estimation problem).

In addition, we assume that φ is 1-Lipschitz under the ∞-norm, i.e. |φ(θ + _dθ, λ_ + _dλ)_ _−_ _φ(θ, λ)| ≤_
max(dθ, dλ), _θ, λ, dθ, dλ_ R . Both φ(θ, λ) = _[θ][+]2_ _[λ]_ and φ(θ, λ) = max _θ + λ_ are 1-Lipschitz
_∀_ _∈_ _{_ _}_

under the ∞-norm. This is a mild assumption because if φ is not 1-Lipschitz we can rescale φ to
make it 1-Lipschitz. Finally, define the Radamacher complexity


_m[(][ℓ][) =][ E]Xi_ _p,ϵi_ Uniform( 1,1 )
_R[p]_ _∼_ _∼_ _{−_ _}_


sup
_a∈A_


_ϵiℓ(Xi, a)_
_i=1_

X


We define Rm[q] [(][ℓ][)][ analogously. Based on these assumptions and definitions we can bound the dif-]
ference between _D[ˆ]_ _ℓ[φ][(ˆ]pm∥qˆm) and Dℓ[φ][(][p][∥][q][)][.]_

**Theorem 2. If ℓ** _is C-bounded, and φ is 1-Lipschitz under the ∞-norm, for any choice of distribution_
_p, q ∈P(X_ ) and t > 0 we have

_1. Pr[ D[ˆ]_ _ℓ[φ][(ˆ]pm∥qˆm) ≥_ _t] ≤_ 4e[−] _[t]2[2]C[m][2]_ _if p = q._


_2. Pr_ _D[ˆ]_ _ℓ[φ][(ˆ]pm∥qˆm) −_ _Dℓ[φ][(][p][∥][q][)]_ _≥_ 4 max(Rm[p] [(][ℓ][)][,][ R][q]m[(][ℓ][)) +][ t] _≤_ 4e[−] _[t]2[2]C[m][2]_
h i

**Corollary 1.** Var[ D[ˆ] _ℓ[φ][(ˆ]pm∥qˆm)] ≤_ 4 max(Rm[p] [(][ℓ][)][,][ R][q]m[(][ℓ][)) +] 2C [2]/m

q

p

For proof see Appendix A. Note that when p = q, the convergence of _D[ˆ]_ _ℓ[φ][(ˆ]pm∥qˆm) does not depend_
on the Radamacher complexity of ℓ, and converges to 0 very quickly. When p ̸= q the estimator
_Dˆ_ _ℓ[φ][(ˆ]pm∥qˆm) is still consistent (under regularity assumptions)_

**Corollary 2. [Consistency] Under the condition of Theorem 2, if additionally either 1. A is a finite**
_set 2. A is a bounded subset of R[d]_ _for some d ∈_ N and ℓ _is Lipschitz w.r.t. a, then almost surely_
limm _Dℓ[φ][(ˆ]pm_ _qˆm) = Dℓ[φ][(][p][∥][q][)][.]_
_→∞_ [ˆ] _∥_


-----

For both cases in Corollary 2 the Radamacher complexity Rm[p] [(][ℓ][)][ goes to zero (as sample size]
_m →∞) at a rate of O(1/[√]m). In other words we can conclude that the estimation error in_
Theorem 2 is bounded by O(1/[√]m) and the variance of the estimator is also bounded by O(1/[√]m)
when the sample size m →∞.

4 EXPERIMENT: TWO SAMPLE TEST

The first application is to design more powerful two sample tests. We aim to show that H-divergence
allow us to leverage inductive biases for each data type (e.g. image, bio, text) by choosing suitable
actions A and loss ℓ, which leads to improved test power. [1]

4.1 TWO SAMPLE TEST

For the task of two sample test, we would like to decide if two sets of samples are drawn from
i.i.d.
the same distribution or not. Specifically, given two sets of samplesi.i.d. ˆpm := (x1, · · ·, xm) _∼_ _p_
and ˆqm := (x[′]1[,][ · · ·][, x]m[′] [)] _∼_ _q we would like to decide if p = q. Typical approaches estimate a_
divergence _D[ˆ]_ (ˆpm _qˆm) and output p_ = q if the divergence exceeds some threshold.
_∥_ _̸_

There are two types of errors: Type I error happens when the algorithm incorrectly outputs p ̸= q;
the probability of type I errors is called the significance level. Type II error happens when the
algorithm incorrectly outputs p = q; the probability of not making a Type II error is called the
_test power (higher is better). Note that both the significance level and the test power are relative to_
distributions p and q.

We follow the typical setup where we guarantee a certain significance level while empirically measuring the test power. In particular, the significance level can be guaranteed with a permutation
test (Ernst et al., 2004). In a permutation test, in addition to the original set of samples ˆpm and ˆqm,
we also uniformly randomly swap elements between ˆpm and ˆqm, and sample multiple randomly
swapped datasets (ˆp[1]m[,][ ˆ]qm[1] [)][,][ (ˆ]p[2]m[,][ ˆ]qm[2] [)][,][ · · ·][ . The testing algorithm outputs][ p][ ̸][=][ q][ if][ ˆ]D(ˆpm∥qˆm) is
in the top α-quantile among {D[ˆ] (ˆp[1]m[∥]q[ˆ]m[1] [)][,][ ˆ]D(ˆp[2]m[∥]q[ˆ]m[2] [)][,][ · · · }][. Permutation test guarantees the sig-]
nificance level (i.e. low Type I error) because if p = q then swapping elements between ˆpm and
_qˆm should not change its distribution, so each pair (ˆpm, ˆqm), (ˆp[1]m[,][ ˆ]qm[1]_ [)][,][ · · ·][ should have the same]
distribution. Therefore, _D[ˆ]_ (ˆpm _qˆm) happens to be in the top α-quantile with at most α probability._
_∥_
Note that the significance level guarantee does not rely on accurate estimation of H-divergence in
Theorem 2 (accurate H-divergence estimation is still important because the test power does depends
on it).

When the choice of D[ℓ] is not a strict divergence (See Proposition 2) we may falsely conclude p = q
(D(p∥q) = 0) when in reality p ̸= q. This is true but inconsequential in finite data scenarios.
With finite data, it is generally impossible to guarantee the test power (i.e. bounding the probability
of concluding p = q when in reality p ̸= q for any p, q) and prior literature do not provide such
guarantees. Hence our guarantee is no weaker than prior two sample test literature.

4.2 EXPERIMENT SETUP

**Baselines** We compare our proposed approach with six other divergences. All methods are
based on the permutation test explained in Section 4.1. MMD-D (Liu et al., 2020) measures
the MMD distance with a deep kernel, while MMD-O (Gretton et al., 2012) measures the MMD
distance with a Gaussian kernel. Mean embedding (ME) and smoothed characteristic functions
(SCF) (Chwialkowski et al., 2015; Jitkrittum et al., 2016) are distances based on the difference in
Gaussian kernel mean embedding at a set of optimized points, or a set of optimized frequencies.
C2STS-S & C2ST-L (Lopez-Paz & Oquab, 2017; Cheng & Cloninger, 2019) use a classifier’s accuracy distinguishing between the two distributions.

**Comparison Metrics and Setup** All methods have the same significance level (which is provably
equal to α = 0.05 because of the permutation test), therefore we only consider the test power. We
follow Liu et al. (2020) and consider four datasets: Blob (Liu et al., 2020), HDGM (Liu et al.,

[1The code to reproduce our experiments can be found here.](https://github.com/a7b23/H-Divergence)


-----

2020), HIGGS (Adam-Bourdarios et al., 2014) and MNIST (LeCun & Cortes, 2010). Our method
and all the baseline methods have hyper-parameters. To ensure fair comparison, we follow the same
evaluation setup as (Liu et al., 2020) for all methods. We split each dataset into two equal partitions:
a training set to tune hyper-parameters, and a validation set to compute the final test output.

**Implementation Details** We choose φ(θ, λ) = _θs+2_ _λs_ 1/s for s > 1 (which includes the H
Jensen Shannon divergence when s = 1 and the H-Min divergence when s = ). We define l(x, a)

   _∞_

as the negative log likelihood of x under distribution a, where a is in a certain model family A. We
experiment with mixture of Gaussian distributions, Parzen density estimtor and Variational Autoencoder (Kingma & Welling, 2013). Our hyper-parameters consist of the best parameter s and also the
best generative model family. Choosing these hyper-parameters might seem cumbersome, but compared to the second best baseline (MMD-D which chooses thousands of deep kernel parameters),
we have much fewer hyper-parameters.

We use α = 0.05 in all two-sample test experiments. Each permutation test uses 100 permutations,
and we run each test 100 times to compute the test power (i.e. the percent of times it correctly
outputs p ̸= q). Finally we plot and report the performance standard deviation by repeating the
entire experiment 10 times.

4.3 EXPERIMENT RESULTS

The average test powers are reported in Figure 4, Figure 2, Table 1 and Table 3. Our approach
_achieves superior test power across the board. Notably on Higgs we achieve the same test power_
with 2x fewer samples than the second best test, and on MNIST we can achieve perfect test power
even on the smallest sample size evaluated in (Liu et al., 2020).

Following (Liu et al., 2020) we also evaluate the test power as the dimension of the problem increases
(Figure 2). Our test power decreases gracefully as the dimension of the problem increases. We
hypothesize that the test power improvements come from leveraging progress in generative model
research: for each type of data (e.g. bio, image, text) there has been decades of research finding
suitable generative models; we use commonly used generative models (in modern literature) for
each data type (e.g. KDE for low dimensional physics/bio data, VAE for simple images).

Figure 2: Average test power on HDGM dataset. Left: results with the same sample size (4000)
and different data dimensions. Right: results with the same sample dimension (10) and different
sample sizes. Our method (H-Div, dashed line) achieve better test power for almost every setup.
All tests have high test power for low data dimensions, but our method scales better for higher data
dimensions.

|N|ME SCF C2ST-S C2ST-L MMD-O MMD-D H-Div|
|---|---|
|1000 2000 3000 5000 8000 10000|0.120±0.007 0.095±0.007 0.082±0.015 0.097±0.014 0.132±0.005 0.113±0.013 0.240±0.020 0.165±0.019 0.130±0.019 0.183±0.026 0.232±0.032 0.291±0.017 0.304±0.012 0.380±0.040 0.197±0.012 0.142±0.025 0.257±0.049 0.399±0.058 0.376±0.022 0.403±0.050 0.685±0.015 0.410±0.041 0.261±0.044 0.592±0.037 0.447±0.045 0.659±0.018 0.699±0.047 0.930±0.010 0.691±0.067 0.467±0.038 0.892±0.029 0.878±0.020 0.923±0.013 0.952±0.024 1.000±0.000 0.786±0.041 0.603±0.066 0.974±0.007 0.985±0.005 1.000±0.000 1.000±0.000 1.000±0.000|
|Avg.|0.395 0.283 0.497 0.506 0.564 0.579 0.847|



Table 1: Average test power ± standard error for N samples over the HIGGS dataset. The results
on MNIST is similar and presented in Table 3, Appendix B.1.


-----

Figure 3: Example plots of H-divergence across different geographical locations for losses ℓ related
to agriculture (left) and energy production (right). Darker color indicates larger H-divergence. Compared to divergences such as KL, H-divergence measures changes relevant to different social and
economic activities (by selecting appropriate loss functions ℓ). For example, even though climate
change significantly impact the high latitude or high altitude areas, this change has less relevance to
agriculture (because few agriculture activities are possible in these areas).

5 EXPERIMENT: DECISION DEPENDENT DISCREPANCY MEASUREMENT

5.1 ASSESSING CLIMATE CHANGE

As an illustrative example of how H-divergence can facilitate decision making, we use climate data
and study how climate change affects decision making through the lens of H-divergence. Scientists and policy makers are often interested in how climate change disparately affect different geographical locations. Existing methods (Preston et al., 2011) focus on one aspect of climate change
(such as the expected economic loss (Burke et al., 2018)) using tailor-designed analysis, while Hdivergence provides a general tool for hypothesis testing and visualization for different aspects of
climate change. In our example, we choose suitable loss functions to quantitatively measure aspects of climate change that are relevant to decision making in agriculture and renewable energy
production.[2]

**Setup** We use the NOAA database which contains daily weather from thousands of weather stations at different geographical locations. For each location, we summarize the weather sequence of
each year into a few summary statistics (average yearly temperature, humidity, wind speed and rainy
days). We are interested in assessing changes in weather over this period at each location, from the
perspective of agriculture and renewable energy activities. Further details of these experiments are
in Appendix C.2.

**Example: Agriculture** It is known that climate changes affect crop suitability (Lobell et al.,
2008). Let A denote the set of possible crops to plant at each location (e.g. wheat/barley/rice),
and ℓ(x, a) denote the loss of planting crop a if the yearly weather is x. We estimate the function ℓ
by matching geographical locations in the FAO crop yield dataset (FAOSTAT et al., 2006) to weather
stations in the NOAA database, and learn a function to predict crop yield from weather data with
kernel ridge regression.

The H-divergence has a natural interpretation: a geographical location could either (1) plant the
same crop for the entire period 1981-2019 that is optimal for the local climate (i.e. choose
_a[∗]_ = arg mina∈A E(p+q)/2[ℓ(X, a)]); (2) plant the optimal crops for 1981-1999 and for 2000-2019
respectively. H divergence measures the additional loss of option (1) compared to option (2). In
other words, it is the excess loss of not adapting crop type to climate change. For each geographical
location we can compute the H-divergence Dℓ[JS] for the estimated ℓ (plotted in Figure 3 left).

**Example: Energy production** Changes in weather also affect electricity generation, since climate change could affect the amount of wind/solar energy available. Let A denote the number
of wind/solar/fossil fuel power plants built, and ℓ(x, a) denote the loss (negative utility) when the
weather is x. We obtain the function ℓ using empirical formulas for energy production (Npower,
2012). The H-divergence for this loss function is shown in Figure 3 (right). Intuitively the H di
2Designing loss functions ℓ that capture the effect of climate on human activities is a well studied topic in
economics, and beyond the scope of this work. Our results should be taken as an illustrative example of how
domain experts might use H-divergence with more realistic loss functions.


-----

|Loss Selection|Selected Features|
|---|---|


|Neutral Upweight low income Upweight high income|education, cap-gain, sex, age, occupation education, cap-gain, relationship, marital-status, sex education, cap-gain, sex, age, race|
|---|---|


Table 2: Features selected by different approaches. With H-Divergence we can select different
features that are important in different decision problems. For example, if we assign a high / low
penalty to making incorrect prediction for higher income groups, we select a different set of features.

vergence measures the excess loss of using the same energy generation infrastructure for the entire
time period vs. using different infrastructure that adapts to climate change. While this is only an
illustrative example, comparing the two maps we see that regions and industries are affected by
climate change in different ways – H divergence provides a quantitative framework for this kind of
assessments.

5.2 FEATURE SELECTION

In a feature selection task, we wish to know which input features are most predictive of the label.
Feature selection provides information on which features have the biggest influence on the label,
and can be used in scientific discovery (Jovi´c et al., 2015; Zhang et al., 2015).

Off-the-shelf feature selection algorithms often do not take into account problem specific requirements. For example, denote the input features as X1, · · ·, XK and label as Y, the mutual information feature selection algorithms estimate the Shannon mutual information I(Xi, Y ) :=
KL(p(Xi, Y ) _p(Xi)p(Y )) and select features with largest mutual information. However, scien-_
_∥_
tists and policies makers often need fine-grained control to answer their specific scientific or policy
questions. For example, social scientists might want to know which features are more important for
high-income as compared to low-income groups (e.g. to understand potential glass ceilings).

With H-Divergence we can select features with large Dℓ[φ][(][p][(][X][i][, Y][ )][∥][p][(][X][i][)][p][(][Y][ ))][ (i.e. the optimal]
action is different under the joint p(Xi, Y ) and the product of marginals p(Xi)p(Y )). By choosing
different loss functions ℓ we can get different feature selection results, each reflecting important
features for that decision problem. For example, in Table 2 we show the features selected for the
UCI income prediction dataset (Blake, 1998). For this dataset, we choose A as the set of logistic
regression functions, l(x, a) as the cross entropy loss for regression function a on the sample x and
_φ(θ, λ) = max(θ, λ). If we want to focus on high income groups, we can assign a higher weight to_
the loss of high income samples, and vice versa. We observe that gender/race is more predictive of
income for high-income groups, while relationship or marital status is more predictive of income for
lower-income groups. This can help us identify potential inequality or suggest further investigation
into the cause of low income and poverty. For example, our results suggest a connection between
family and relationship status and poverty, and a connection between gender/race and high income.
These connections merit further investigation into the cause and policy remedy.

6 ACKNOWLEDGEMENTS

SE acknowledges support by NSF(#1651565, #1522054, #1733686), ONR (N000141912145),
AFOSR (FA95501910024), ARO (W911NF-21-1-0125) and Sloan Fellowship.

REFERENCES

Claire Adam-Bourdarios, Glen Cowan, C´ecile Germain, Isabelle Guyon, Bal´azs K´egl, and David
Rousseau. The higgs boson machine learning challenge. In Proceedings of the 2014 International
_Conference on High-Energy Physics and Machine Learning - Volume 42, HEPML’14, pp. 19–55._
JMLR.org, 2014.

Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein generative adversarial networks.
In International conference on machine learning, pp. 214–223. PMLR, 2017.

Peter Bartlett. Theoretical statistics, lecture 13. [https://www.stat.berkeley.edu/](https://www.stat.berkeley.edu/~bartlett/courses/2013spring-stat210b/notes/13notes.pdf)
[˜[bartlett/courses/2013spring-stat210b/notes/13notes.pdf][, 2013.]](https://www.stat.berkeley.edu/~bartlett/courses/2013spring-stat210b/notes/13notes.pdf)


-----

Catherine Blake. Uci repository of machine learning databases. _http://www. ics. uci. edu/˜_
_mlearn/MLRepository. html, 1998._

Jacob Burbea and C Radhakrishna Rao. Entropy differential metric, distance and divergence measures in probability spaces: A unified approach. Journal of Multivariate Analysis, 12(4):575–596,
1982.

Jacob Burbea and C Radhakrishna Rao. Differential metrics in probability spaces. Probab. Math.
_Stat, 3:241–258, 1984._

Marshall Burke, W Matthew Davis, and Noah S Diffenbaugh. Large potential reduction in economic
damages under un mitigation targets. Nature, 557(7706):549–553, 2018.

X. Cheng and A. Cloninger. Classification logit two-sample testing by neural networks. ArXiv,
abs/1909.11298, 2019.

Kacper P Chwialkowski, Aaditya Ramdas, Dino Sejdinovic, and Arthur Gretton. Fast two-sample
testing with analytic representations of probability measures. In C. Cortes, N. D. Lawrence, D. D.
Lee, M. Sugiyama, and R. Garnett (eds.), Advances in Neural Information Processing Systems
_28, pp. 1981–1989. Curran Associates, Inc., 2015._

Imre Csisz´ar. Eine informationstheoretische ungleichung und ihre anwendung auf beweis der ergodizitaet von markoffschen ketten. Magyer Tud. Akad. Mat. Kutato Int. Koezl., 8:85–108, 1964.

Morris H DeGroot. Optimal statistical decisions, volume 82. John Wiley & Sons, 2005.

Morris H DeGroot et al. Uncertainty, information, and sequential experiments. _The Annals of_
_Mathematical Statistics, 33(2):404–419, 1962._

Gary Doran, Krikamol Muandet, Kun Zhang, and Bernhard Sch¨olkopf. A permutation-based kernel
conditional independence test. In UAI, pp. 132–141, 2014.

John Duchi, Khashayar Khosravi, Feng Ruan, et al. Multiclass classification, information, divergence and surrogate risk. Annals of Statistics, 46(6B):3246–3275, 2018.

Michael D Ernst et al. Permutation methods: a basis for exact inference. Statistical Science, 19(4):
676–685, 2004.

Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust optimization
using the wasserstein metric: Performance guarantees and tractable reformulations. Mathematical
_Programming, 171(1-2):115–166, 2018._

FAO FAOSTAT et al. Fao statistical databases. Rome: Food and Agriculture Organization of the
_United Nations, 2006._

Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Sch¨olkopf, and Alexander Smola.
A kernel two-sample test. The Journal of Machine Learning Research, 13(1):723–773, 2012.

Peter D Gr¨unwald, A Philip Dawid, et al. Game theory, maximum entropy, minimum discrepancy
and robust bayesian decision theory. the Annals of Statistics, 32(4):1367–1433, 2004.

Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. arXiv preprint arXiv:1903.12261, 2019.

Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
_in neural information processing systems, pp. 6626–6637, 2017._

Wittawat Jitkrittum, Zolt´an Szab´o, Kacper P Chwialkowski, and Arthur Gretton. Interpretable distribution features with maximum testing power. In D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp.
181–189. Curran Associates, Inc., 2016.


-----

Alan Jovi´c, Karla Brki´c, and Nikola Bogunovi´c. A review of feature selection methods with applications. In 2015 38th international convention on information and communication technology,
_electronics and microelectronics (MIPRO), pp. 1200–1205. Ieee, 2015._

Diederik P Kingma and Max Welling. Auto-Encoding variational bayes. _arXiv preprint_
_arXiv:1312.6114v10, December 2013._

[Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.](http://yann.lecun.com/exdb/mnist/)
[lecun.com/exdb/mnist/.](http://yann.lecun.com/exdb/mnist/)

Feng Liu, Wenkai Xu, Jie Lu, Guangquan Zhang, A. Gretton, and D. Sutherland. Learning deep
kernels for non-parametric two-sample tests. ArXiv, abs/2002.09116, 2020.

David B Lobell, Marshall B Burke, Claudia Tebaldi, Michael D Mastrandrea, Walter P Falcon, and
Rosamond L Naylor. Prioritizing climate change adaptation needs for food security in 2030.
_Science, 319(5863):607–610, 2008._

David Lopez-Paz and M. Oquab. Revisiting classifier two-sample tests. arXiv: Machine Learning,
2017.

[Tengyu Ma. Machine learning theory. https://github.com/tengyuma/cs229m_notes/](https://github.com/tengyuma/cs229m_notes/blob/main/Winter2021/pdf/02-08-2021.pdf)
[blob/main/Winter2021/pdf/02-08-2021.pdf, 2021.](https://github.com/tengyuma/cs229m_notes/blob/main/Winter2021/pdf/02-08-2021.pdf)

Alfred M¨uller. Integral probability metrics and their generating classes of functions. Advances in
_Applied Probability, pp. 429–443, 1997._

Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. In Proceedings of the 30th International Conference
_on Neural Information Processing Systems, pp. 271–279, 2016._

Npower. Wind turbine power calculations. Mechanical and Electrical Engineering Power Industry,
_The Royal Academy of Engineering, 2012._

Benjamin L Preston, Emma J Yuen, and Richard M Westaway. Putting vulnerability to climate
change on the map: a review of approaches, benefits, and risks. Sustainability science, 6(2):
177–202, 2011.

C Radhakrishna Rao. Diversity and dissimilarity coefficients: a unified approach. _Theoretical_
_population biology, 21(1):24–43, 1982._

Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo_rithms. Cambridge university press, 2014._

Jiaming Song and Stefano Ermon. Understanding the limitations of variational mutual information
estimators. arXiv preprint arXiv:1910.06222, 2019.

Bharath K Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Sch¨olkopf, and Gert RG
Lanckriet. On integral probability metrics,\phi-divergences and binary classification. _arXiv_
_preprint arXiv:0901.2698, 2009._

Yilun Xu, Shengjia Zhao, Jiaming Song, Russell Stewart, and Stefano Ermon. A theory of usable
information under computational constraints. arXiv preprint arXiv:2002.10689, 2020.

Yudong Zhang, Zhengchao Dong, Preetha Phillips, Shuihua Wang, Genlin Ji, Jiquan Yang, and TiFei Yuan. Detection of subjects and brain regions related to alzheimer’s disease using 3d mri
scans based on eigenbrain and machine learning. Frontiers in computational neuroscience, 9:66,
2015.


-----

A PROOFS

**Lemma 2. For any choice of ℓ** _and for any choice of φ that satisfy Definition 2, Dℓ[φ]_ _[is non-negative and][ D]ℓ[φ][(][p, q][) = 0]_
_whenever p = q. Furthermore, Dℓ[φ]_ _[is symmetric whenever][ φ][ is symmetric.]_

_Proof of Lemma 2. For any choice of p, q by the concavity of the H-entropy in Lemma 1 we have_

_p + q_

_Hℓ_ _Hℓ(p)_

2 _−_ _≥_ 2[1] [(][H][ℓ][(][q][)][ −] _[H][ℓ][(][p][))]_

 

_p + q_

_Hℓ_ _Hℓ(q)_

2 _−_ _≥_ 2[1] [(][H][ℓ][(][p][)][ −] _[H][ℓ][(][q][))]_

 

Therefore by summing the two inequalities we have

_p + q_ _p + q_

_Hℓ_ _Hℓ(p)_ + _Hℓ_ _Hℓ(q)_ 0

2 _−_ 2 _−_ _≥_

       

By the requirement on φ we know that Dℓ[φ][(][p][∥][q][)][ ≥] [0][. In addition when][ p][ =][ q][ since][ (][p][ +][ q][)][/][2 =][ p][ =][ q][ we have]
_Dℓ[φ][(][p][∥][q][) =][ φ][(0][,][ 0) = 0][.]_

To show it is symmetric, note that

_p + q_ _p + q_ _p + q_ _p + q_

_Dℓ[φ][(][p][∥][q][) =][ φ]_ _Hℓ_ _Hℓ(p), Hℓ_ _Hℓ(q)_ = φ _Hℓ_ _Hℓ(q), Hℓ_ _Hℓ(p)_

2 _−_ 2 _−_ 2 _−_ 2 _−_

           

= Dℓ[φ][(][q][∥][p][)]

whenever φ is symmetric.

**Proposition 3. Dℓ[φ][(][p][∥][q][)][ >][ 0][ if and only if][ arg inf]** _[a][ E][p][[][ℓ][(][X, a][)]][ ∩]_ [arg inf] _[a][ E][q][[][ℓ][(][X, a][)] =][ ∅][.]_


_Proof of Proposition 3. Denote Ap[∗]_ [= arg inf] _[a]_ [E][p][[][ℓ][(][X, a][)]][ and][ A]q[∗] [= arg inf] _[a]_ [E][q][[][ℓ][(][X, a][)]][. Also compute]

_p + q_ 1

_Hℓ_ = inf (3)

2 _a_ [E][ p][+]2 _[q]_ [[][ℓ][(][X, a][)] = inf]a 2 [E][p][[][ℓ][(][X, a][)] + 1]2 [E][q][[][ℓ][(][X, a][)]]

   

IfH Aℓ(qp[∗])[∩A]. Similar if we chooseq[∗] [=][ ∅][, for any action] a[′′][ a]such that[′][ such that] E[ E]q[ℓ[p]([[][ℓ]X, a[(][X, a][′′])] =[′][)] =] H[ H]ℓ([ℓ]q[(][p]) we have similarly have[)][, we must have][ a][′][ ∈A] Ep[∗] [so]p[ℓ[ a](X, a[′][ ̸∈A][′′])]q[∗] _> H[and][ E]ℓ[q]([[]p[ℓ])[(]. In other[X, a][′][)]][ >]_
words, for any choice of action a ∈A either a /∈Ap[∗] [and][ E][p][[][l][(][X, a][)]][ > H][ℓ][(][p][)][ or][ a][ ∈A]p[∗] [and][ E][q][[][l][(][X, a][)]][ > H][ℓ][(][q][)][.]
Therefore

1

inf _>_ [1] (4)
_a_ 2 [E][p][[][ℓ][(][X, a][)] + 1]2 [E][q][[][ℓ][(][X, a][)]] 2 _[H][ℓ][(][p][) + 1]2_ _[H][ℓ][(][q][)]_

 

Combining Eq.(3) and Eq.(4) we have

1 _p + q_ _p + q_

_Hℓ_ _Hℓ(p)_ + [1] _Hℓ_ _Hℓ(q)_ _> 0_

2 2 _−_ 2 2 _−_

       

By Definition 2 this implies (for any choice of φ that satisfies the requirements in Definition 2) that Dℓ[φ][(][p][∥][q][)][ >][ 0][.]

To prove the converse simply obverse that if Ap[∗] _[∩A]q[∗]_ _[̸][=][ φ][, let][ a][∗]_ _[∈A]p[∗]_ _[∩A]q[∗]_ [we have][ a][∗] [= arg inf] _[a][∈A]_ [E][ p][+]2 _[q]_ [[][l][(][X, a][)]][. This]

implies that

_p + q_

2Hℓ 2 _−_ _Hℓ(q) −_ _Hℓ(p) = 2E p+2_ _q_ [[][l][(][X, a][∗][)]][ −] [E][q][[][l][(][X, a][∗][)]][ −] [E][p][[][l][(][X, a][∗][)] = 0]

 

By Definition 2 we can conclude that Dℓ[φ][(][p][∥][q][) = 0][.]

**Theorem 1. The set of H-Jensen Shannon Divergences is strictly larger than the MMD[2]** _distances._


-----

_Proof of Theorem 1. Let k(x, y) be some kernel on an input space X_, and let H be the RKHS induced by the kernel. The
(squared) MMD distance is defined by

MMD[2](p, q) = EX _p,Y_ _pk(X, Y ) + EX_ _q,Y_ _qk(X, Y )_ 2EX _p,Y_ _qk(X, Y )_
_∼_ _∼_ _∼_ _∼_ _−_ _∼_ _∼_

which we write more compactly as MMD[2](p, q) = Ep,pk(X, Y ) + Eq,qk(X, Y ) − 2Ep,qk(X, Y ).

Define φ(x, y) = _k(x,_ ) _k(y,_ )
_∥_ _·_ _−_ _·_ _∥H[2]_ [. We can rewrite this in the following form:]


MMD[2](p, q) = Ep,qφ(X, Y ) (5)
_−_ [1]2 [E][p,p][φ][(][X, Y][ )][ −] [1]2 [E][q,q][φ][(][X, Y][ )]

= Ep,q _k(X,_ ) [+][ ∥][k][(][Y,][ ·][)][∥][2] [+][ ∥][k][(][Y,][ ·][)][∥][2]
_∥_ _·_ _∥H[2]_ _H_ _[−]_ [2][k][(][X, Y][ )][ −] [1]2 [E][p,p][∥][k][(][X,][ ·][)][∥]H[2] _H_ _[−]_ [2][k][(][X, Y][ )]

[+][ ∥][k][(][Y,][ ·][)][∥][2]

_−_ [1]2 [E][q,q][∥][k][(][X,][ ·][)][∥]H[2] _H_ _[−]_ [2][k][(][X, Y][ ) =][ E][p,p][k][(][X, Y][ ) +][ E][q,q][k][(][X, Y][ )][ −] [2][E][p,q][k][(][X, Y][ )]

We also observe an algebraic relationship for any function f (x, y) such that f (x, y) = f (y, x) for all x, y:


_,_ _[p][+]2_ _[q]_ _[f]_ [(][X, Y][ ) = 1]4 [E][p,p][f] [(][X, Y][ ) + 1]4 [E][p,q][f] [(][X, Y][ ) + 1]4 [E][q,p][f] [(][X, Y][ ) + 1]4 [E][q,q][f] [(][X, Y][ )]

= [1]

4 [E][p,p][f] [(][X, Y][ ) + 1]4 [E][p,q][f] [(][X, Y][ ) + 1]4 [E][q,p][f] [(][Y, X][) + 1]4 [E][q,q][f] [(][X, Y][ )]

= [1] (6)

4 [E][p,p][f] [(][X, Y][ ) + 1]4 [E][q,q][f] [(][X, Y][ ) + 1]2 [E][p,q][f] [(][X, Y][ )]


E p+q


Furthermore, we have that

Ep,p _k(X,_ ) _k(Y,_ ) [= 2][E][p][∥][k][(][X,][ ·][)][ −] [E][p][k][(][Y,][ ·][)][∥][2] (7)
_∥_ _·_ _−_ _·_ _∥H[2]_ _H_

Based on the above, noting that φ(x, y) = φ(y, x), we can derive

MMD[2](p, q) = Ep,q _k(X,_ ) _k(Y,_ ) Eq (5)
_∥_ _·_ _−_ _·_ _∥H[2]_ _[−]_ [1]2 [E][p,p][∥][k][(][X,][ ·][)][ −] _[k][(][Y,][ ·][)][∥]H[2]_ _[−]_ [1]2 [E][q,q][∥][k][(][X,][ ·][)][ −] _[k][(][Y,][ ·][)][∥]H[2]_

= 2E p+2 _q_ _,_ _[p][+]2_ _[q]_ _[∥][k][(][X,][ ·][)][ −]_ _[k][(][Y,][ ·][)][∥]H[2]_ _[−]_ [E][p,p][∥][k][(][X,][ ·][)][ −] _[k][(][Y,][ ·][)][∥]H[2]_ _[−]_ [E][q,q][∥][k][(][X,][ ·][)][ −] _[k][(][Y,][ ·][)][∥]H[2]_ Eq (6)

= 4E p+2 _q_ _[∥][k][(][X,][ ·][)][ −]_ [E][ p][+]2 _[q]_ _[k][(][Y,][ ·][)][∥][2]H_ _[−]_ [2][E][p][∥][k][(][X,][ ·][)][ −] [E][p][k][(][Y,][ ·][)][∥]H[2] _[−]_ [2][E][q][∥][k][(][X,][ ·][)][ −] [E][q][k][(][Y,][ ·][)][∥]H[2] Eq (7)

= 4 inf mean def.
_a∈H_ [E][ p][+]2 _[q]_ _[∥][k][(][X,][ ·][)][ −]_ _[a][∥]H[2]_ _[−]_ [2 inf]a∈H [E][p][∥][k][(][X,][ ·][)][ −] _[a][∥]H[2]_ _[−]_ [2 inf]a∈H [E][q][∥][k][(][X,][ ·][)][ −] _[a][∥]H[2]_ _[.]_


Therefore we can define a loss ℓ : X × H → R where

_ℓ(x, a) = 4_ _k(x,_ ) _a_
_∥_ _·_ _−_ _∥H[2]_

Under the new notation we have

MMD[2](p, q) = inf inf
_a∈H_ [E][ p][+]2 _[q]_ _[l][(][X, a][)][ −]_ [1]2 a∈H [E][p][l][(][X, a][) + inf]a∈H [E][q][l][(][X, a][)]

_p + q_

= Hℓ _ℓ_ [(][p][∥][q][)]

2 _−_ 2[1] [(][H][ℓ][(][p][) +][ H][ℓ][(][q][)) =][ D][JS]

 

Conversely we want to show that not every H-Jensen Shannon divergence is a MMD. For example, take Hℓ to be the Shannon entropy, then the corresponding Dℓ[JS] is the Jensen-Shannon divergence, which is not a MMD. This is because the JS
divergence is a type of f -divergence, and the only f -divergence that is also an IPM is total variation distance Sriperumbudur
et al. (2009). Therefore, the set of H-Jensen Shannon Divergences is strictly larger than the set of MMDs.

**Theorem 2. If ℓ** _is C-bounded, and φ is 1-Lipschitz under the ∞-norm, for any choice of distribution p, q ∈P(X_ ) and t > 0
_we have_


_1. Pr[ D[ˆ]_ _ℓ[φ][(ˆ]pm∥qˆm) ≥_ _t] ≤_ 4e[−] _[t]2[2]C[m][2]_ _if p = q._

_2. Pr_ _D[ˆ]_ _ℓ[φ][(ˆ]pm∥qˆm) −_ _Dℓ[φ][(][p][∥][q][)]_ _≥_ 4 max(Rm[p] [(][ℓ][)][,][ R][q]m[(][ℓ][)) +][ t] _≤_ 4e[−] _[t]2[2]C[m][2]_
h i


-----

_Proof of Theorem 2. Let ˆpm be a sequence of n samples (x1, · · ·, xm) drawn from p, and ˆqm a sequence of n sam-_
ples (x[′]1[,][ · · ·][, x]m[′] [)][ drawn from][ q][.] Let ˆrm the sub-sampling mixture (x[′′]1 _[,][ · · ·][, x]m[′′]_ [)][ defined in Section 3.5 (i.e.] _x[′′]i_ =
_xibi + x[′]i[(1][ −]_ _[b][i][)][ where][ b][i][ is uniformly sampled from][ {][0][,][ 1][}][). We also overload the notation][ H][ℓ]_ [by defining][ H][ℓ][(ˆ]pm) =
inf _a∈A_ _m[1]_ _mi=1_ _[l][(][x][i][, a][)][, and define][ H][ℓ][(ˆ]qm), Hℓ(ˆrm) similarly._

Before proving this theorem we need the following LemmasP

**Lemma 3. Under the assumptions of Theorem 2**


Pr [Hℓ(ˆpm) − E[Hℓ(ˆpm)] ≥ _t] ≤_ _e[−]_ [2][t]C[2][2][m]

Pr [Hℓ(ˆpm) − E[Hℓ(ˆpm)] ≤−t] ≤ _e[−]_ [2][t]C[2][2][m]

**Lemma 4. Under the assumptions of Theorem 2**


Pr [ _Hℓ(p)_ _Hℓ(ˆpm)_ 2 _m(ℓ) + t]_ _e[−]_ [2][t]C[2][2][m]
_|_ _−_ _| ≥_ _R_ _≤_

To prove the first statement of the Theorem, when p = q we can denote µ = E[Hℓ(ˆpm)] = E[Hℓ(ˆqm)] = E[Hℓ(ˆrm)], and
we have

Pr _Dˆ_ _ℓ[φ][(ˆ]pm∥qˆm) ≥_ _t_

= Pr[h _φ(Hℓ(ˆrm)_ _Hiℓ(ˆpm), Hℓ(ˆrm)_ _Hℓ(ˆqm))_ _t]_ Def 2
_−_ _−_ _≥_
Pr [max(Hℓ(ˆrm) _Hℓ(ˆpm), Hℓ(ˆrm)_ _Hℓ(ˆqm))_ _t]_ _φ 1-Lipschitz_
_≤_ _−_ _−_ _≥_
Pr [Hℓ(ˆrm) _Hℓ(ˆpm)_ _t] + Pr [Hℓ(ˆrm)_ _Hℓ(ˆqm)_ _t]_ Union bound
_≤_ _−_ _≥_ _−_ _≥_
Pr [Hℓ(ˆpm) _µ_ _t/2] + 2 Pr [Hℓ(ˆrm)_ _µ_ _t/2] + Pr [Hℓ(ˆqm)_ _µ_ _t/2]_ Union bound
_≤_ _−_ _≤−_ _−_ _≥_ _−_ _≤−_

_t[2]_
_≤_ 4e− 2C[2] _/m_ Lemma 3

The third inequality is because if Hℓ(ˆrm) _Hℓ(ˆpm)_ _t then it must be either Hℓ(ˆpm)_ _µ_ _t/2 or Hℓ(ˆrm)_ _µ_ _t/2._
_−_ _≥_ _−_ _≤−_ _−_ _≥_
Similarly if Hℓ(ˆrm) _Hℓ(ˆqm)_ _t then it must be either Hℓ(ˆqm)_ _µ_ _t/2 and Hℓ(ˆrm)_ _µ_ _t/2._
_−_ _≥_ _−_ _≤−_ _−_ _≥_

To prove the second statement of the Theorem, we observe that

_|D[ˆ]_ _ℓ[φ][(][p][m][∥][q][m][)][ −]_ _[D]ℓ[φ][(][p][∥][q][)][|]_

_p + q_ _p + q_
= _φ (Hℓ(ˆrm) −_ _Hℓ(ˆpm), Hℓ(ˆrm) −_ _Hℓ(ˆqm)) −_ _φ_ _Hℓ_ 2 _−_ _Hℓ(p), Hℓ_ 2 _−_ _Hℓ(q)_ Def 2

_p + q_     _p + q_ 
max _Hℓ(ˆrm)_ _Hℓ(ˆpm)_ _Hℓ_ + Hℓ(p) _,_ _Hℓ(ˆrm)_ _Hℓ(ˆqm)_ _Hℓ_ + Hℓ(q) _φ 1-Lip_
_≤_ _−_ _−_ 2 _−_ _−_ 2
 _p + q_   _p + q_   

max _Hℓ(ˆrm)_ _Hℓ_ + _Hℓ(ˆpm)_ _Hℓ(p)_ _,_ _Hℓ(ˆrm)_ _Hℓ_ + _Hℓ(ˆqm)_ _Hℓ(q)_ Jensen
_≤_ _−_ 2 _|_ _−_ _|_ _−_ 2 _|_ _−_ _|_
     

Therefore, the event |D[ˆ] _ℓ[φ][(][p][m][∥][q][m][)][ −]_ _[D]ℓ[φ][(][p][∥][q][)][| ≥]_ [4 max(][R]m[p] [(][ℓ][)][,][ R][q]m[(][ℓ][)) +][ t][ happens only if at least one of the following]
events happen

_p + q_

_rm) −_ _Hℓ_  2  _[≥R]m[p]_ [(][ℓ][) +][ R][q]m[(][ℓ][) +][ t/][2][ ≥] [2][R][(]m[p][+][q][)][/][2](ℓ) + t/2 _R convex_

_Hℓ(ˆpm)_ _Hℓ(p)_ 2 _m(ℓ) + t/2_

_[H][ℓ][(ˆ]_ _|_ _−_ _| ≥_ _R_

_Hℓ(ˆqm)_ _Hℓ(q)_ 2 _m(ℓ) + t/2_
_|_ _−_ _| ≥_ _R_

Based on Lemma 4 each of these events only happen with probability at most e[−] _[t]2[2]C[m][2]_ . Therefore we can conclude by union

bound that

Pr[|D[ˆ] _ℓ[φ][(][p][∥][q][)][ −]_ _[D]ℓ[φ][(][p][∥][q][)][| ≥]_ [4 max(][R]m[p] [(][ℓ][)][,][ R][q]m[(][ℓ][)) +][ t][]][ ≤] [4][e][−] _[t]2[2]C[m][2]_

Finally we prove the two Lemmas used in the theorem. Lemma 4 is a standard result in the Radamacher complexity literature.
For a proof, see e.g. Section 26.1 in (Shalev-Shwartz & Ben-David, 2014). Lemma 3 can also be proved by standard
techniques. We provide the proof here.


-----

_Proof of Lemma 3. Consider two sets of samples x1, · · ·, xj, · · ·, xm and x[′]1[,][ · · ·][, x][′]j[,][ · · ·][, x]m[′]_ [where][ x][i] [=][ x][′]i [for every]
index i = 1, · · ·, m except index j.
1 1 1

_ℓ(xi, a)_ inf _ℓ(x[′]i[, a][)]_ _ℓ(xi, a)_ _ℓ(x[′]i[, a][)]_

_a_ _m_ _−_ _a_ _m_ _a_ _m_ _−_ _m[1]_

_i_ _i_ _[≤]_ [sup] _i_ _i_

X X X X

[inf] = [1] _ℓ(xj, a)_ _ℓ(x[′]j[, a][)]_

_m_ [sup]a _−_ _≤_ _m[C]_

Then we can conclude by Mcdiarmid inequality that


2t[2]
_e−_ _C[2]_ _/m = e[−]_ [2][t]C[2][2][m]
_≤_


_ℓ(Xi, a) −_ E


Pr


inf


inf


_ℓ(Xi, a)_


_≥_ _t_


Var[ D[ˆ] _ℓ[φ][(ˆ]pm∥qˆm)] ≤_ 4 max(Rm[p] [(][ℓ][)][,][ R][q]m[(][ℓ][)) +]


**Corollary 1.**


2C [2]/m


_Proof of Corollary 1. For notation convenience denote B = 4 max(Rm[p]_ [(][ℓ][)][,][ R][q]m[(][ℓ][))]

Var[ D[ˆ] _ℓ[φ][(ˆ]pm∥qˆm)]_

2[]
= E _Dˆ_ _ℓ[φ][(ˆ]pm∥qˆm) −_ _Dℓ[φ][(][p][∥][q][)]_

_∞_  2
= _t=0_ Pr _Dˆ_ _ℓ[φ][(ˆ]pm∥qˆm) −_ _Dℓ[φ][(][p][∥][q][)]_ _≥_ _t_ _dt_
Z   

= _∞t=0_ Pr _D[ˆ]_ _ℓ[φ][(ˆ]pm∥qˆm) −_ _Dℓ[φ][(][p][∥][q][)]_ _≥_ _√t_ _dt_
Z h i

= _∞s=0_ Pr _D[ˆ]_ _ℓ[φ][(ˆ]pm∥qˆm) −_ _Dℓ[φ][(][p][∥][q][)]_ _≥_ _s_ 2sds _s =_ _√_
Z B h _∞_ i

_≤_ _s=0_ 2sds + _s=0_ Pr _D[ˆ]_ _ℓ[φ][(ˆ]pm∥qˆm) −_ _Dℓ[φ][(][p][∥][q][)]_ _≥_ _B + s_ 2(B + s)ds
Z Z h i

_∞_
_≤_ _B[2]_ + _s=0_ 2(B + s)e[−] _[s]2[2]C[m][2]_ _ds_
Z

_∞_ 2C [2] 2C [2] _m_
_B[2]_ + 2(B + t _t = s_
_≤_ Zt=0 r _m_ [)][e][−][t][2] r _m [dt]_ r 2C [2]

2C [2]

= B[2] + 2B _e[−][t][2]_ _dt + [4][C]_ [2] _te[−][t][2]_ _dt_

_m_ _m_

r Z Z

2πC [2]

= B[2] + B + [2][C] [2] (B + 2C [2]/m)[2]

_m_ _m_ _≤_

r

p


**Corollary 2. [Consistency] Under the condition of Theorem 2, if additionally either 1. A is a finite set 2. A is a bounded**
_subset of R[d]_ _for some d_ N and ℓ _is Lipschitz w.r.t. a, then almost surely limm_ _Dℓ[φ][(ˆ]pm_ _qˆm) = Dℓ[φ][(][p][∥][q][)][.]_
_∈_ _→∞_ [ˆ] _∥_

_Proof of Corollary 2. We can prove the consistency results from Theorem 2 by observing that the expected Radamacher_
complexity goes to 0 when m →∞.

The first statement is a simple consequence of Massart’s Lemma, (e.g. see Eq.(8.44) in (Ma, 2021)). In particular, because A
is finite we have


_Rm[p]_ [(][ℓ][)][ ≤]


2 log |A|/m →m→∞ 0


-----

To prove the second statement, first observe that because A is bounded, there must exist some r ∈ R such that A ⊂ _Br :=_
_{a, ∥a∥2 ≤_ _r}. In addition, without loss of generality we can assume that there exists L ∈_ R such that ∀x ∈X and a, a[′] _∈A_

_ℓ(x, a)_ _ℓ(x, a[′])_ _L_ _a_ _a[′]_ 2
_|_ _−_ _| ≤_ _∥_ _−_ _∥_

There is no loss of generality because in finite dimensions all norms are equivalent, so if f is Lipschitz under any norm, then ℓ
is Lipschitz under the 2-norm. We can apply the results on Radamacher complexity for smoothly parameterized class proved
in (Bartlett, 2013), and conclude that limm _m[(][ℓ][) = 0][.]_
_→∞_ _R[p]_

B ADDITIONAL EXPERIMENTAL RESULTS

B.1 BLOB DATASET

Figure 4: Left: Average test power on the Blob dataset for different sample sizes and significance
level α = 0.05. Our method (H-Div, dashed line) has significantly better test power, especially for
setups with small sample sizes. Right: The same plot with significance level α = 0.01.

B.2 EVALUATING SAMPLE QUALITY

Figure 5: The divergence between corrupted image and original image measured by H-divergence
vs. FID. For better comparison we normalize each distance to between [0, 1] by a linear transformation. For “speckle” and “impulse” corruption, both divergences are monotonically increasing with
more corruption. For “snow” corruption H-divergence is monotonic while FID is not. Other types
of corruptions are provided in Appendix B.2.

The gold standard for evaluating image generative models requires human decision, which is nevertheless expensive. Several surrogate measurements are commonly used, such as the Frechet Inception Distance (FID) (Heusel et al., 2017) or the
inception score. Here by formulating such evaluation as an estimation of the discrepancy between the generated and the real
images, we can quantify the quality of image samples by calculating the corresponding H-Divergences.

We choose A as the set of Gaussian mixture distributions on the inception feature space, l(x, a) as the negative log likelihood
of x under distribution a and φ(θ, λ) = max(θ, λ). To evaluate the performance, we use the same setup as (Heusel et al.,
2017), where we add corruption from (Hendrycks & Dietterich, 2019) to a set of images. Intuitively, adding more corruption
degrades the sample quality, so a good measurement of sample quality should assign a lower quality score (higher divergence
from clean images). The results are plotted in Figure 5. The remaining plots of other perturbations are in Appendix B.2.
Both FID and H-divergence are generally monotonically increasing as we increase the amount of corruption. Our method is
slightly better on some perturbations (such as “snow”), where the FID fails to be monotonically increasing, but our method
is still monotonic, better aligning with human intuition.


-----

Figure 6: Additional plots that extend Figure 5.

C ADDITIONAL THEORY AND EXPERIMENT DETAILS

C.1 CONNECTION TO OPTIMAL TRANSPORT

We first show that H-divergence can also have a transportation interpretation. For all the intuitive interpretations we avoid
technical difficulty by assuming X is a finite set, even though all the formulas are applicable when X has infinite cardinality.


-----

|N|ME SCF C2ST-S C2ST-L MMD-O MMD-D H-Div|
|---|---|
|200 400 600 800 1000|0.414±0.050 0.107±0.018 0.193±0.037 0.234±0.031 0.188±0.010 0.555±0.044 1.000±0.000 0.921±0.032 0.152±0.021 0.646±0.039 0.706±0.047 0.363±0.017 0.996±0.004 1.000±0.000 1.000±0.000 0.294±0.008 1.000±0.000 0.977±0.012 0.619±0.021 1.000±0.000 1.000±0.000 1.000±0.000 0.317±0.017 1.000±0.000 1.000±0.000 0.797±0.015 1.000±0.000 1.000±0.000 1.000±0.000 0.346±0.019 1.000±0.000 1.000±0.000 0.894±0.016 1.000±0.000 1.000±0.000|
|Avg.|0.867 0.243 0.768 0.783 0.572 0.910 1.000|


Table 3: Average test power ± standard error for N samples over the MNIST dataset.

**Setup** Choose A = X, and let l(x, a) be a symmetric function (l(x, a) = l(a, x)) that denotes the cost of transporting a
unit of goods from x to a. When we say that a unit of goods is located according to p, we mean that there is 1 unit of goods
dispersed in X locations, where p(x) is the amount of goods at location x.

**Optimal Transport Distance** The optimal transport distance is defined by

_Oℓ(p, q) =_ inf
_rXY,rX_ =pX _,rY =qY_ [E][r][XY][ [][l][(][X, Y][ )]]

Intuitively the optimal transport distance measures the following cost: initially the goods are located according to p, we would
like to move them to locate according to q; O(p, q) denote the minimum cost to accomplish this transportation task.

**H-Divergence as Optimal Storage** We first consider the intuitive interpretation to the H-entropy

_Hℓ(p) = inf_ _a[∗]_ = arg inf
_a_ [E][p][[][ℓ][(][X, a][)]] _a∈X_ [E][p][[][ℓ][(][X, a][)]]

Suppose we want to move goods located according to p to a storage location (for example, we want to collect all the mail in a
city to a package center), then a[∗] is the optimal location to build the storage location, and H-entropy measures the minimum
cost to transport all goods to the storage location. Similarly 2Hℓ _p+2_ _q_ measures the minimum cost to transport both goods

located according to p and goods located according to q to the same storage location. The H-divergence

  

_p + q_

2Dℓ[JS][(][p][∥][q][) := 2][H][ℓ] (Hℓ(q) + Hℓ(p))

2 _−_

 

measures the reduction of transportation cost with two storage locations (one for p and one for q) rather than a single storage
location (for both p and q).

The H-Divergence is related to the optimal transport distance by the following inequality.
**Proposition 4. If ℓ** _satisfies the triangle inequality ∀x, y, z ∈X_ _, l(x, y) + l(y, z) ≥_ _l(x, z) then Dℓ[JS][(][p][∥][q][)][ ≤]_ [1]2 _[O][(][p, q][)]_


_Proof of Proposition 4. Let a[∗]q_ [= arg inf][ E][q][[][l][(][X, a][)]][ then we have]

_p + q_

2Hℓ = inf _q[)] +][ E][q][[][ℓ][(][X, a]q[∗][)]]_

2 _a_ [(][E][p][[][ℓ][(][X, a][)] +][ E][q][[][ℓ][(][X, a][)])][ ≤] [E][p][[][ℓ][(][X, a][∗]

 

inf _q[)] +][ E][q][[][ℓ][(][X, a][∗]q[)]]_
_≤_ _rXY,rX_ =pX _,rY =qY_ [E][r][XY][ [][ℓ][(][X, a][∗]

inf _q[)] +][ E][q][[][ℓ][(][X, a][∗]q[)]]_
_≤_ _rXY,rX_ =pX _,rY =qY_ [E][r][XY][ [][ℓ][(][X, Y][ ) +][ ℓ][(][Y, a][∗]

= Oℓ(p, q) + 2Hℓ(q)

Intuitively, to move goods located according to p and goods located according to q to some storage location, one option is to
first transport all goods from p to q (so that the goods at location x will be 2q(x)), then move the goods located according to
2q to the optimal storage location. Similarly we have

_p + q_

2Hℓ _O(q, p) + 2Hℓ(p)_

2 _≤_

 

which combined we have

_p + q_

2Dℓ[JS][(][p][∥][q][) = 2][H][ℓ] (Hℓ(q) + Hℓ(p)) _O(p, q)_

2 _−_ _≤_

 


-----

C.2 IMPOSSIBILITY OF JENSEN-SHANNON DIVERGENCE ESTIMATION

Suppose we have a consistent estimator for the Jenson-Shannon divergence, i.e. a function JS[ˆ] such that for any pair of
contradiction by the probabilistic method.distribution p, q and given N i.i.d. samples pN ∼ _p and qN ∼_ _q we have limN_ _→∞_ JS([ˆ] _pN_ _∥qN_ ) = JS([ˆ] _p∥q), then we prove a_

Let p be a standard Gaussian distribution, let Q[M] be a uniform distribution on a set of M i.i.d. samples from p (hence Q[M]
is itself a random variable that depends on the i.i.d. samples). Let Q[∗] be the limit of Q[M] when M →∞ (i.e. it is the
uniform distribution on an infinite set of samples). Let q[∗] denote a value that Q[∗] can take. Because q[∗] is always supported
on countably many points, hence JS(p∥q[∗]) = 1. Note that for any N the following two sampling process leads to identical
distribution on pN _, qN_ :

_pN_ _p, qN_ _p_ _Q[∗]_ _p, pN_ _p, qN_ _Q[∗]_
_∼_ _∼_ _∼_ _∼_ _∼_

Hence, the expectation of any function (including JS[ˆ] ) is also identical.

EQ∗∼p EpN _∼p,qN_ _∼Q∗_ [ JS([ˆ] _pN_ _, qN_ )] = EpN _∼p,qN_ _∼p[ JS([ˆ]_ _pN_ _, qN_ )]
h i


Hence the limit when N →∞ must be identical

_Nlim→∞_ [E][Q][∗][∼][p] EpN _∼p,qN_ _∼Q∗_ [ JS([ˆ] _pN_ _, qN_ )] = limN _→∞_ [E][p][N] _[∼][p,q][N]_ _[∼][p][[ ˆ]JS(pN_ _, qN_ )]
h i

Because the Jenson Shannon divergence is always bounded, any consistent estimator must also be bounded for sufficiently
large N . By the dominated convergence theorem we can exchange the expectation and the limit.

EQ∗∼p _Nlim→∞_ [E][p][N] _[∼][p,q][N]_ _[∼][Q][∗]_ [[ ˆ]JS(pN _, qN_ )] = limN _→∞_ [E][p][N] _[∼][p,q][N]_ _[∼][p][[ ˆ]JS(pN_ _, qN_ )]
h i

By the probabilistic method (i.e. for any function f if EQ[∗] _p[f_ (Q[∗])] = 0 there must exist some q[∗] such that f (q[∗]) 0)
_∼_ _≤_
there must exist some q[∗] such that

lim JS(pN _, qN_ )] lim JS(pN _, qN_ )] = 0 = JS(p _q[∗]) = 1_
_N_ _≤_ _N_ _̸_ _∥_
_→∞_ [E][p][N] _[∼][p,q][N]_ _[∼][q][∗]_ [[ ˆ] _→∞_ [E][p][N] _[∼][p,q][N]_ _[∼][p][[ ˆ]_

Therefore JS[ˆ] cannot be consistent.


C.3 CLIMATE CHANGE EXPERIMENT DETAILS

**Setup Details** In this experiment, we extract the statistics of yearly weather for each year from 1981-2019. We use the
NOAA dataset, which contains daily weather data from thousands of weather stations across the globe. For each year we
compute the following summary statistics: average yearly temperature, average yearly humidity, average yearly wind speed
and average number of rainy days in an year. For example x1990 is a 4 dimensional vector where each dimension correspond
to one of the summary statistics above.

Let p denote the uniform distribution over _x1981,_ _, x1999_ and q denote the uniform distribution over _x2000,_ _, x2019_ .
_{_ _· · ·_ _}_ _{_ _· · ·_ _}_
For example Ep[ℓ(X, a)] denote the expected loss of action a for a random year sampled from 1981-1999. Note that for many
decision problems, it is possible to make yearly decisions (e.g. decide the best crop to plant for each year). However, because
we want to measure the difference between two time periods 1981-1999 vs. 2000-2019, we choose the action space A to
be a single crop selection that will be used for the entire time period (rather than a different crop selection for each year).
Similarly for energy production we choose the action space A to be the proportion of different energy production methods
that will be used for the entire time period.

**Crop yield** We obtain the crop yield loss function ℓ(x, a) with the following procedure

1. We obtain the crop yield dataset from (FAOSTAT et al., 2006), each entry we extract is the following tuple: (country
code, year, crop type, yield per hectare (kg/ha))

2. We associate each country code with the central coordinate (i.e. the average latitude and longitude) of the country. For
each central coordinate we find the nearest weather station in the NOAA database. We use data for the nearest weather
station as the weather data for the country.

3. Based on step 2 for each (country code, year) pair we can associate a weather statistics (i.e. the 4 dimensional vector
described in Setup Details). We update each entry in step 1 to be (weather statistics, crop type, yield per hectare).

4. Based on the data entries we obtain in step 3 we train a Kernel Ridge regression model to learn the function ℓ(x, a)
where x is the weather statistics, a is the crop type, and ℓ(x, a) is learned to predict the yield (normalized by market
price) of the weather x for crop type a.


-----

**Energy production** We consider three types of energy production methods: solar, wind and traditional (such as fossil
fuel). Solar energy and wind energy both depend heavily on weather, while traditional energy does not. In particular, we use
empirical formulas for solar and wind energy calculation:

solar ∝ number of sunny days ∗ daylight hour

wind ∝ wind velocity[3]


D DISCUSSIONS

**Future Work** In this paper we explored the applications of H-divergence to two sample testing. Future work can explore
other applications of divergences. Potential applications include

_• Generative model training. Many generative models learning algorithm minimize divergences (Nowozin et al., 2016;_
Arjovsky et al., 2017), and future work can explore if the new divergence family leads to new generative model learning
algorithms.

_• Independence tests. Independence tests are two sample tests between the joint distribution pXY and the product of_
marginal distributions pX _pY, therefore the two sample test results from this paper are applicable to independence tests._

_• Robustness. Many robust optimization, estimation or prediction methods aim to achieve good performance even when_
the data distribution is perturbed. Typically perturbation is measured by e.g. the KL divergence or the Lp distances.
Future work can measure perturbation with the H-divergence Hℓ by choosing loss functions ℓ that are tailored for the
problem.

**Computation Issues** There are several situations where estimating the H-divergence is (provably) computationally feasible:

_• When A is a small finite set, in which case we can enumerate all possible values of a ∈A._

_• When the loss function ℓ(y, a) is convex in a, in which case we can accurately estimate the H-divergence in polynomial_
time by solving the optimization problem inf _a E[ℓ(Y, a)] by gradient descent._

In general, while it is difficult to guarantee computational feasibility, we use a practical technique that works well in our
experiments: we use the same number of gradient descent steps for evaluating Hℓ _p+2_ _q_ and Hℓ(p), Hℓ(q). Intuitively, the

“sub-optimality” when estimating the three terms is approximately the same in expectation and cancels out.

  


-----

