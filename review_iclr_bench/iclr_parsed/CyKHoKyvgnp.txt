# TRANSITION TO LINEARITY OF WIDE NEURAL NET## WORKS IS AN EMERGING PROPERTY OF ASSEMBLING
# WEAK MODELS


**Chaoyue Liu**
Depart. of Computer Science
Ohio State University
liu.2656@osu.edu


**Libin Zhu**
Depart. of Computer Science
UC, San Diego
l5zhu@ucsd.edu

ABSTRACT


**Mikhail Belkin**
HDSI
UC, San Diego
mbelkin@ucsd.edu


Wide neural networks with linear output layer have been shown to be near-linear,
and to have near-constant neural tangent kernel (NTK), in a region containing
the optimization path of gradient descent. These findings seem counter-intuitive
since in general neural networks are highly complex models. Why does a linear
structure emerge when the networks become wide? In this work, we provide a
new perspective on this “transition to linearity” by considering a neural network
as an assembly model recursively built from a set of sub-models corresponding
to individual neurons. In this view, we show that the linearity of wide neural
networks is, in fact, an emerging property of assembling a large number of diverse
“weak” sub-models, none of which dominate the assembly.

1 INTRODUCTION

Success of gradient descent methods for optimizing neural networks, which generally correspond
to highly non-convex loss functions, has long been a challenge to theoretical analysis. A series
of recent works including Du et al. (2018; 2019); Allen-Zhu et al. (2019); Zou et al. (2018); Oymak & Soltanolkotabi (2020) showed that convergence can indeed be shown for certain types of
wide networks. Remarkably, Jacot et al. (2018) demonstrated that, when the network width goes
to infinity, the Neural Tangent Kernel (NTK) of the network becomes constant during training with
gradient flow, a continuous time limit of gradient descent. Based on that Lee et al. (2019) showed
that the training dynamics of gradient flow in the space of parameters for the infinite width network
is equivalent to that of a linear model.

As discussed in (Liu et al., 2020), the constancy of the Neural Tangent Kernel stems from the fact
that wide neural networks with linear output layer “transition to linearity” as their width increases to
infinity. The network becomes progressively more linear in O(1)-neighborhoods around the network
initialization, as the network width grows. Specifically, consider the neural network as a function
_f_ (θ) of its parameters θ and write the Taylor expansion with the Lagrange remainder term:

_f_ (θ) = f (θ0) + _f_ (θ0)[T] (θ _θ0) + [1]_ (1)
_∇_ _−_ 2 [(][θ][ −] _[θ][0][)][T][ H][(][ξ][)(][θ][ −]_ _[θ][0][)][,]_

where H is the Hessian, the second derivative matrix of f, and ξ is a point between θ and θ0. The
“transition to linearity” is the phenomenon when the quadratic term [1]2 [(][θ][ −] _[θ][0][)][T][ H][(][ξ][)(][θ][ −]_ _[θ][0][)][ tends]_

to zero in an O(1) ball around θ0 as the network width increases to infinity.

In particular, transition to linearity provides a method for showing that the square loss function of
wide neural networks satisfy the Polyak-Lojasiewicz (PL) inequality which guarantees convergence
of (Stochastic) Gradient Descent to a global minimum (Liu et al., 2022).

While the existing analyses demonstrate transition to linearity mathematically, the underlying structure of that phenomenon does not appear to be fully clear. What structural properties account for it?
Are they specific to neural networks, or also apply to more general models?

In this paper, we aim to explain this phenomenon from a more structural point of view and to reveal
some underlying mechanisms. We provide a new perspective in which a neural network, as well as


-----

each of its hidden layer neurons before activation, can be viewed as an “assembly model”, built up
by linearly assembling a set of sub-models, corresponding to the neurons from the previous layer.
Specifically, a (l + 1)-layer pre-activation neuron


1
_α˜[(][l][+1)]_ = _√m_


_wiαi[(][l][)][,]_
_i=1_

X


as a linear combination of the l-th layer neurons αi[(][l][)][, is considered as an assembly model, whereas]
the l-layer neurons αi[(][l][)] are considered as sub-models. Furthermore, each pre-activation ˜αi[(][l][)] is
also an assembly model constructed from (l 1)-th layer neurons αi[(][l][−][1)]. In this sense, the neural
_−_
network is considered as a multi-level assembly model. To show O(1)-neighborhood linearity, we
prove that the quadratic term in the Taylor expansion Eq.(1) vanishes in O(1)-neighborhoods of
network initialization, as a consequence of assembling sufficiently many sub-models .

To illustrate the idea of assembling, we start with a simple case: assembling independent sub-models
in Section 2. The key finding is that, as long as the assembly model is not dominated by one
or a few sub-models, the quadratic term in the Taylor expansion Eq.(1) becomes small in O(1)neighborhoods of the parameter space when the number of sub-models m is tends to infinity. This
means that as m increases to infinity, the assembly model becomes a linear function of parameters.
Since we put almost no requirements on the form of sub-models, it is the assembling process that
results in the linearity of the assembly model. This case includes two-layer neural networks as
examples.

For deep neural networks (Section 3), we consider a neural network as a multi-level assembly model
each neuron is considered as an assembly model constructed iteratively from all the neurons from
the previous layer. We then follow an inductive argument: using near-linearity of previous-layer
pre-activation neurons to show the linearity of next-layer pre-activation neurons. Our key finding
is that, when the network width is large, the neurons within the same layer become essentially
independent to each other in the sense that their gradient directions are orthogonal in the parameter
space. This orthogonality allows for the existence of a new coordinate system such that these neuron
gradients are parallel to the new coordinate axes. Within the new coordinate system, one can apply
the argument of assembling independent sub-models, and obtain the O(1)-neighborhood linearity
of the pre-activation neurons, as well as the output of the neural network.

We further point out that this assembling viewpoint and the O(1)-neighborhood linearity can be
extended to more general neural network architectures, e.g., DenseNet.

We end this section by commenting on a few closely related concepts and point out some of the
differences.

**Boosting.** Boosting (Schapire, 1990; Freund, 1995), which is a popular method that combines
multiple “weak” models to produce a powerful ensembe model, has a similar form with the assembly
model, Eq.(4), see, e.g., Friedman et al. (2000). However, we note a few key differences between the
two. First, in boosting, each “weak” model is trained separately on the dataset and the coefficients
(i.e., vi in Eq.(4)) of the weak models are determined by the model performance. In training an
assembly model, the “weak” sub-models (i.e., hidden layer neurons), are never directly evaluated on
the training dataset, and the coefficients vi are considered as parameters of the assembly model and
trained by gradient descent. Second, in boosting, different data samples may have different sample
weights. In assembling, the data samples always have a uniform weight.

**Bagging and Reservoir computing.** Bagging (Breiman, 1996) and Reservoir computing (Jaeger,
2001) are two other ways of combining multiple sub-models to build a single model. In bagging,
each sub-model is individually trained and the ensemble model is simply the average or the max of
the sub-models. In reservoir computing, each sub-model is fixed, and only the coefficients of the
linear combination are trainable.

**Notation.** We use bold lowercase letters, e.g., v, to denote vectors, capital letters, e.g., W, to
denote matrices. We use ∥· ∥ to denote the Euclidean norm of vectors and spectral norm (i.e.,
matrix 2-norm) for matrices. We denote the set 1, 2, _, n_ as [n]. We use **wf to denote the**
_{_ _· · ·_ _}_ _∇_


-----

partial derivative of f with respect to w. When w represents all of the parameters of f, we omit the
subscript, i.e., ∇f .

2 ASSEMBLING INDEPENDENT MODELS

In this section, we consider assembling a sufficiently large number m of independent sub-models
and show that the resulting assembly model is (approximately) linear in O(1)-neighborhood of the
model parameter space.

**Ingredients: sub-models. Let’s consider a set of m (sub-)models {gi}i[m]=1[, where each model][ g][i][,]**
with a set of model parametersscalar prediction gi(wi; x). Here w Pi ∈Di is the parameter space of sub-modeli ⊂Pi = R[p][i], takes an input x ∈D gi andx ⊂ DRx[d] is the domainand outputs a
of the input. By independence, we mean that any two distinct models gi and gj share no common
model parameters:
_Pi ∩Pj = {0},_ _∀i ̸= j ∈_ [m]. (2)
In this sense, the change of gi’s output, as a result of change of it parameters wi, does not affect the
outputs of the other models gj where j ̸= i.

In addition, we require that there is no dominating sub-models over the others. Specifically, we
assume the following in this section:
**Assumption 1 (No dominating sub-models). There exists a constant c ∈** (0, 1) independent of m
such that, for any parameter setting {wi : wi ∈Di}i[m]=1 [and input][ x][ ∈D][x][,]

median[ _g1(w1; x)_ _,_ _,_ _gm(wm; x)_ ]
_|_ _|_ _· · ·_ _|_ _|_ _c._ (3)

max[ _g1(w1; x)_ _,_ _,_ _gm(wm; x)_ ] _≥_
_|_ _|_ _· · ·_ _|_ _|_

Furthermore, we assume that max[|g1(w1; x)|, · · ·, |gm(wm; x)|] ∈ [a, b] ⊂ R for some constants
_a > 0 and b > 0._
**Remark 1. This assumption guarantees that most of the outputs of the sub-models are at the same**
order, typically O(1). It makes sure that the resulting assembly model is not dominated by one or a
minor portion of the sub-models. This is typically seen for the neurons of neural networks.

We further make the following technical assumption, which is common in the literature.
**Assumption 2 (Twice differentiablity and smoothness). Each sub-model gi is twice differentiable**
with respect to the parameters wi, and is β-smooth in the parameter space: there exists a positive
number β such that, for any i ∈ [m],

_gi(wi; x)_ _gi(wi[′][;][ x][)][∥≤]_ _[β][∥][w][i]_ _i[∥][.]_
_∥∇_ _−∇_ _[−]_ **[w][′]**

This assumption makes sure that for each gi, the gradient is well-defined and its Hessian has a
bounded spectral norm.

**Assembly model. Based on these sub-models {gi}i[m]=1[, we construct an assembly model (or super-]**
model) f, using linear combination as follows:


_vigi(wi; x),_ (4)
_i=1_

X


_f_ (θ; x) :=


_s(m)_


where vi is the weight of the sub-model gi, and 1/s(m), as a function of m, is the scaling factor.
Here, we denote θ as the set of parameters of the assembly model, θ := (w1, . . ., wm). The total
number of parameters that f has is p = _i=1_ _[p][i][. Typical choices for the weights][ v][i][ are setting]_
_vi = 1 for all i ∈_ [m], or random i.i.d. drawing vi from some zero-mean probability distribution.
**Remark 2. For the ease of analysis and simplicity of notation, we assumed that the outputs of[P][m]**
assembly model f and sub-models gi are scalars. It is not difficult to see that our analysis below
also applies to the scenarios of finite dimensional model outputs.

**The scaling factor 1/s(m). The presence of the scaling factor 1/s(m) is to keep the output of**
assembly model f at the order O(1) w.r.t. m. In general, we expect that s(m) grows with m. In
particular, when vi are chosen i.i.d. from a probability distribution with mean zero, e.g., {−1, 1},


-----

the sum in Eq.(4) is expected to be of the order _m, under the Assumption 1. In this case, the_

_[√]_
scaling factor 1/s(m) = O(1/[√]m).

Now, we will show that the assembly model f has a small quadratic term in its Taylor expansion,
when the size of the set of sub-models i.e. m, is sufficiently large.
**Theorem 1. Consider the assembly model f constructed in Eq.(4) with each vi either set to be 1**
_or randomly drawn from {−1, 1}, and suppose Assumption 1 and 2 hold. Given a positive number_
_R >of the quadratic term in Taylor expansion Eq.(1) is bounded by: 0 and a parameter setting θ0 ∈_ R[p], for any θ ∈ R[p] _such that ∥θ −_ _θ0∥≤_ _R, the absolute value_

1

(5)

2 [(][θ][ −] _[θ][0][)][T][ H][(][ξ][)(][θ][ −]_ _[θ][0][)]_ _[≤]_ 2[βR]s(m[2]) _[.]_

_Proof. In what follows, we don’t explicitly write out the dependence on θ and x in the Hessian_
_H :=_ _[∂]∂θ[2][f][2][, for simplicity of notation. We further denote][ H][g][i]_ [:=][ ∂]∂[2]w[g]i[2][i] [as the Hessian of][ g][i][.]

We decompose the assembly model Hessian H as a linear combination of Hgi . By the definition
of the assembly model in Eq.(4), an arbitrary entry Hjk of the Hessian can be written as: Hjk =

1 _m_ _∂[2]gi_

_s(m)_ _i=1_ _[v][i]_ _∂θj_ _∂θk_ _[.][ Here][ θ][j][, θ][k][ are two individual parameters of the assembly model][ f]_ [. Note that]

_∂[2]gi_

_∂θj_ _∂θPk_ [is non-zero, only if both][ θ][j][ and][ θ][k][ are parameters of sub-model][ g][i][, i.e.,][ θ][j][, θ][k][ ∈P][i][. Hence,]
the assembly model Hessian can be decomposed as a linear combination of sub-model Hessians:
_H =_ _s(1m)_ _mi=1_ _[v][i][H][g]i_ [. Therefore, the quadratic term becomes]

_m_ _m_

1 P 1 1 1

_vi_ = _vi_

_s(m)_ 2 [(][θ][ −] _[θ][0][)][T][ H][g][i]_ [(][ξ][)(][θ][ −] _[θ][0][)]_ _s(m)_ 2 [(][w][i][ −] **[w][i,][0][)][T][ H][g][i]** [(][ξ][)(][w][i][ −] **[w][i,][0][)]**

_i=1_   _i=1_  

X X

and its absolute value is bounded by


2 [(][θ][ −] _[θ][0][)][T][ H][(][ξ][)(][θ][ −]_ _[θ][0][)]_

[1]


_vi_ _Hgi_ **wi** **wi,0**
_i=1_ _|_ _| · ∥_ _∥· ∥_ _−_ _∥[2]_ _≤_

X


**wi** **wi,0**
_i=1_ _∥_ _−_ _∥[2]_

X


2s(m)


2s(m)


In the second inequality, we used that |vi| = 1 and that gi is β-smooth. Because of the independence
of the sub-models as seen in Eq.(2), the summation in the above equation becomes _θ_ _θ0_, which
_∥_ _−_ _∥[2]_
is bounded by R[2], as stated in the theorem condition. Therefore, we conclude the theorem.

It is important to note that β is a constant and 1/s(m) = O(1/[√]m). Then we have the following
corollary in the limiting case.
**Corollary 1. Consider the assembly model f under the same setting as in Theorem 1. If the number**
_of sub-models m increases to infinity, then for all parameter θ0 and input x, the quadratic term_

0, (6)
2 [(][θ][ −] _[θ][0][)][T][ H][(][ξ][)(][θ][ −]_ _[θ][0][)]_ _→_

_as long as θ is within an O(1)-neighborhood of θ0, i.e.,_ _θ_ _θ0_ _R for some constant R > 0._

[1] _∥_ _−_ _∥[2]_ _≤_

**Example: Two-layer neural networks.** A good example of this kind of assembly model is the
two-layer neural network. A two-layer neural network is mathematically defined as:


_uiσ(wi[T]_ **[x][)][,]** (7)
_i=1_

X


_f_ (W, v; x) = _√m_


where m is the number of hidden layer neurons, σ(·) is the activation function, x ∈ R[d] is the network
input, and W ∈ R[m][×][d] and u ∈ R[m] are the parameters for the first and second layer, respectively.
Here, we can view the i-th hidden neuron and all the parameters wi and ui that connect to it as the
_i-th sub-model: gi = uiσ(wi[T]_ **[x][)][. We see that these sub-models do not share parameters, and each]**
sub-model has d + 1 parameters. In addition, the weights of the sub-models are all 1. By Corollary
1, the two-layer neural network becomes a linear model in any O(1)-neighborhoods, as the network
width m increases to infinity. This is consistent with the previous observation that a two-layer neural
network transitions to linearity Liu et al. (2020).


-----

**Gaussian distributed weights vi.** Another common way to set the weights vi of the sub-models is
to independently draw each vi from N (0, 1). In this case, vi is unbounded, but with high probability
the quadratic term is still O(log(m)/[√]m). Please see the detailed analysis in Appendix A.

**Hierarchy of assembly models.** In principle, we can consider the assembly model f, together
with multiple similar models independent to each other, as sub-models, and construct a higher-level
assembly model. Repeating this procedures, we can have a hierarchy of assembly models. Our
analysis above also applies to this case and each assembly model is also O(1)-neighborhood linear,
when the number of its sub-models is sufficiently large. However, one drawback of this hierarchy
is that the total number of parameters of the highest level assembly model increase exponentially
with the number of levels. We will see shortly that wide neural networks, as a hierarchy of assembly
models, allows overlapping between sub-models, but still keeping the O(1)-neighborhood linearity.

3 DEEP NEURAL NETWORKS AS ASSEMBLING MODELS

In this section, we view deep neural networks as multi-level assembly models and show how the
_O(1)-neighborhood linearity arises naturally as a consequence of assembling._

**Setup. We start with the definition of multi-layer neural networks. A L-layer full-connected neural**
network is defined as follows:

_α[(0)]_ = x,

1
_α[(][l][)]_ = σ(˜α[(][l][)]), _α˜[(][l][)]_ = _W_ [(][l][)]α[(][l][−][1)], _l = 1, 2,_ _, L,_ (8)
_√ml_ 1 _∀_ _· · ·_
_−_

_f = ˜α[(][L][)],_

where σ(·) is the activation function and is applied entry-wise above. We assume σ(·) is twice
differentiable to make sure that the network f is twice differentiable and its Hessian is well-defined.
We also assume that σ(·) is an injective function, which includes the commonly used sigmoid, tanh,
_softplus, etc. In the network, with ml being the width of l-th hidden layer, ˜α[(][l][)]_ _∈_ R[m][l] (called
_pre-activations) and α[(][l][)]_ _∈_ R[m][l] (called post-activations) represent the vectors of the l-th hidden
layer neurons before and after the activation function, respectively. Denote θ := (W [(1)], . . ., W [(][L][)]),
with W [(][l][)] _∈_ R[m][l][−][1][×][m][l], as all the parameters of the network, and θ[(][l][)] := (W [(1)], . . ., W [(][l][)]) as the
parameters before layer l. We also denote p and p[(][l][)] as the dimension of θ and θ[(][l][)], respectively.
Denote θ(αi[(][l][)][)][ as the set of the parameters that neuron][ α]i[(][l][)] depends on.

This neural network is typically initialized following Gaussian random initialization: each parameter is independently drawn from normal distribution, i.e., (W0[(][l][)][)][ij][ ∼N] [(0][,][ 1)][. In this paper, we]
focus on the O(1)-neighborhood of the initialization θ0. As pointed out by Liu et al. (2020), the
whole gradient descent trajectory is contained in a O(1)-neighborhood of the initialization θ0 (more
precisely, a Euclidean ball B(θ0, R) with finite R).

**Overview of the methodology. As it is defined recursively in Eq.(8), we view the deep neural**
network as a multi-level assembly model. Specifically, a pre-activation neuron at a certain layer l is
considered as an assembly model constructed from the post-activation neurons at layer l − 1, at the
same time its post-activation also serves as a sub-model for the neurons in layer l + 1. Hence, a preactivation ˜αi[(][l][)] is an l-level assembly model, and the post-activation αi[(][l][)] is a (l +1)-level sub-model.
To prove the O(1)-neighborhood linearity of the neural network, we start from the linearity at the
first layer, and then follow an inductive argument from layer to layer, up to the output. The main
argument lies in the inductive steps, from layer l to layer l +1. Our key finding is that, in the infinite
width limit, the neurons within the same layer become independent to each other, which allows us
using the arguments in Section 2 to prove the O(1)-neighborhood linearity. Since the exact linearity
happens in the infinite width limit, in the following analysis we takesequentially, the same setting as in Jacot et al. (2018). Note that for neural networks that finite but m1, m2, . . ., mL−1 →∞
large network width, the linearity will be approximate; the larger the width, the closer to linear.


-----

3.1 BASE CASE: FIRST AND SECOND HIDDEN LAYER.

The first hidden layer pre-activations are defined as: ˜α[(1)] = _√m1_ 0 W [(1)]x, where m0 = d. It is

obvious that each element of ˜α[(1)] is linear in its parameters. Each second layer pre-activation
_α˜i[(1)][, i][ ∈]_ [[][m][2][]][ can be considered as a two-layer neural network with parameters][ {][W][ (1)][,][ w]i[(2)]},
where wi[(2)] is the i-th row of W [(2)]. As we have seen in Section 2, the sub-models of a two-layer
neural network share no common parameters, and the assembly model, which is the network itself,
is O(1)-neighborhood linear in the limit of m1 .
_→∞_

3.2 FROM LAYER l TO LAYER l + 1.

First, we make the induction hypothesis at layer l.

**Assumption 3 (Induction hypothesis). Assume that all the l-th layer pre-activation neurons ˜α[(][l][)]** are
_O(1)-neighborhood linear in the limit of m1, . . ., ml−1 →∞._

Under this assumption, we will first show two key properties about the sub-models, i.e., the l-th layer
post-activation neurons α[(][l][)]: (1) level sets of these neurons are hyper-planes with co-dimension 1 in
_O(1)-neighborhoods; (2) normal vectors of these hyper-planes are orthogonal with probability one_
in the infinite width limit.

**Setup. Note that l-th layer neurons only depend on the parameters θ[(][l][)]. Without ambiguity, denote**
the parameter space spanned by θ[(][l][)] as P. We can write θ[(][l][)] as

_θ[(][l][)]_ = θ[(][l][−][1)] _∪_ **w1[(][l][)]** _[∪]_ _[. . .][ ∪]_ **[w]m[(][l][)]l** _[,]_

where wi[(][l][)] is the i-th row of the weight matrix W [(][l][)]. One observation is that the set of parameters
of neuron αi[(][l][)] is θ(αi[(][l][)][) =][ θ][(][l][−][1)][ ∪] **[w]i[(][l][)][. Namely,][ θ][(][l][−][1)][ are shared by all][ l][-layer neurons and]**
each parameter in wi[(][l][)] is privately owned by only one neuron αi[(][l][)][. Hence, we can decompose the]
parameter space as: = _i=0_
_P_ _P_ _[P][i][, where common space][ P][0][ is spanned by][ θ][(][l][−][1)][ and the private]_
spaces Pi, i ̸= 0, are spanned by wi[(][l][)][. Define a set of projection operators][ {][π][i][}]i[m]=0[l] [such that, for]
any vector z ∈P, πi(z) ∈P[L]i, and[m][l] _i=0_ _[π][i][(][z][) =][ z][.]_

**Property 1: Level sets of neurons are hyper-planes. The first key observation is that each of the**
_l-th layer post-activations α[(][l][)]_ has linear level sets in the[P][m][l] _O(1)-neighborhoods._
**Theorem 2 (Linear level sets of neurons). Assume the induction hypothesis Assumption 3 holds.**
_Given a fix input x, for each post-activation αi[(][l][)][,][ i][ ∈]_ [[][m][l][]][, the level set]

_Sc := {θ[(][l][)]_ : αi[(][l][)][(][θ][(][l][)][;][ x][) =][ c][} ∩N] [(][θ][0][)][,] (9)

_is linear or an empty set, for all c ∈_ R, where N (θ0) is an O(1)-neighborhood of θ0. Moreover,
_these level sets are parallel to each other: for anySc2 are parallel to each other._ _c1, c2 ∈_ R, if Sc1 ̸= ∅ _and Sc2 ̸= ∅, then Sc1 and_
**Remark 3. With abuse of notation, we did not explicitly write out the dependence of the level set**
on the neuron αi[(][l][)][, the input][ x][, and the network initialization][ θ][0][.]

_Proof. First, note that all the pre-activation neurons ˜α[(][l][)]_ are linear in (θ0) in the limit of
_N_
_m1, . . ., ml−1 →∞, as assumed in the induction hypothesis. This linearity of these function_
guarantees that all the level sets of the pre-activations {θ[(][l][)] : ˜αi[(][l][)][(][θ][(][l][)][;][ x][) =][ c][} ∩N] [(][θ][0][)][, for all]
_c ∈_ R and i ∈ [ml] are either linear or empty sets, and they are parallel to each other. Since the
activation function σ(·) is element-wisely applied to the neurons, it does not change the shape and
direction of the level sets. Specifically, if {θ[(][l][)] : αi[(][l][)][(][θ][(][l][)][;][ x][) =][ c][}][ is non-empty, then]

_{θ[(][l][)]_ : αi[(][l][)][(][θ][(][l][)][;][ x][) =][ c][}][ =][ {][θ][(][l][)][ : ˜]αi[(][l][)][(][θ][(][l][)][;][ x][) =][ σ][−][1][(][c][)][}][.]

Therefore, {θ[(][l][)] : αi[(][l][)][(][θ][;][ x][) =][ c][} ∩N] [(][θ][0][)][ is linear or an empty set, and the activation function][ σ][(][·][)]
preserves the parallelism.


-----

This means that, although each of the neurons α[(][l][)]
is constructed by a large number of neurons in previous layers containing tremendously many parameters and is transformed by non-linear functions, it
actually has a very simple geometric structure. A
geometric view of the post-activation neurons is illustrated in Figure 1.

As the neurons have scalar outputs and σ(·) is injective by assumption, each level set is a piece of
a co-dimension 1 hyper-plane in the p-dimensional
parameter space . Hence, at each point of the
_P_ Figure 1: A geometric view of the posthyper-plane there exists a unique (up to a negative

activation neurons. Level sets are parallel

sign) unit-length normal vector n, which is perpen
hyper-planes.

dicular to the hyper-plane. A direct corollary of the
linearity of the level sets, Theorem 2, is that the normal vector n is identical everywhere in the O(1)neighborhood (θ0).
_N_

**Corollary 2. Assume the induction hypothesis Assumption 3 holds. Given a specific neuron αi[(][l][)]**
_and an input x, for any θ1[(][l][)][, θ]2[(][l][)]_ _∈P ∩N_ (θ0), n(θ1[(][l][)][) =][ n][(][θ]2[(][l][)][)][.]

Hence, we can define ni as the normal vector for each neuron αi[(][l][)][. The next property is about the]
set of normal vectors {ni}i[m]=1[l] [.]

**Property 2: Orthogonality of normal vectors ni. Now, let’s look at the directions of the normal**
vectors ni. Note that the neuron αi[(][l][)] does not depend on the parameters wj[(][l][)] for any j = i, hence
_̸_
_πj(ni) = 0 for all j /_ _i, 0_ . That means:
_∈{_ _}_

_decomposed asProposition 1. For all ni = π ii( ∈ni) +[ml π], the normal vector0(ni)._ **ni resides in the sub-space Pi ⊕P0, and can be**

other:As πi(ni) ∈Pi, and Pi ∩Pj = {0} for i ̸= j, the components {πi(ni)}i[m]=1[l] [are orthogonal to each]
_πi(ni)_ _πj(nj),_ _for all i_ = j [ml]. (10)
_⊥_ _̸_ _∈_
By Proposition 1, to show the orthogonality of the normal vectors {ni}i[m]=1[l] [, it suffices to show the]
orthogonality of {π0(ni)}i[m]=1[l] [.]

Since it is perpendicular to the corresponding level sets, the normal vector ni is parallel to the
gradient ∇αi[(][l][)][, up to a potential negative sign. Similarly, the projection][ π][0][(][n][i][)][ is parallel to the]
partial gradient ∇θ(l−1) ˜αi[(][l][)][.]

By the definition of neurons in Eq.(8) and the constancy of ni in the neighborhood N (θ0), we have

1
( _θ(l−1) ˜αi[(][l][)][)][T][ =]_ (wi,[(][l]0[)][)][T][ ∇][α][(][l][−][1)][.] (11)
_∇_ _√ml_ 1
_−_

Here, because α[(][l][−][1)] is an ml-dimensional vector, _α[(][l][−][1)]_ is an ml _p[(][l][−][1)]_ matrix, where p[(][l][−][1)]
denotes the size of θ[(][l][−][1)]. It is important to note that the matrix ∇ _∇α[(][l][−] ×[1)]_ is shared by all the neurons
in layer l and is independent of the index i, while the vector wi,[(][l]0[)][, which is][ w]i[(][l][)] at initialization, is
totally private to the l-th layer neurons.

Recall that the vectors {wi,[(][l]0[)][}][m]i=1[l] [are independently drawn from][ N] [(][0][, I][m]l−1[×][m]l−1 [)][. As is well-]

known, when the vector dimension ml 1 is large, these independent random vectors **wi,[(][l]0[)][}][ are]**
_−_ _{_
nearly orthogonal. When in the infinite width limit, the orthogonality holds with probability 1:


1

(wi,[(][l]0[)][)][T][ w]j,[(][l]0[)] _[| ≥]_ _[ϵ]_ = 0, for all i = j [ml]. (12)
_ml−1_ _|_  _̸_ _∈_


_ϵ > 0,_ lim
_∀_ _ml−1→∞_ [P]


From Eq.(11), we see that the partial gradientsa universal linear transform, i.e., ∇α[(][l][−][1)], onto a set of nearly orthogonal vectors. The following {∇θ(l−1) ˜αi[(][l][)][}]i[m]=1[l] [are in fact the result of applying]
lemma shows that the vectors remain orthogonal even after this linear transformation.


-----

**Lemma 1.**

_∀ϵ > 0,_ _ml−lim1→∞_ [P] (∇θ(l−1) ˜αi[(][l][)][)][T][ ∇]θ[(][l][−][1)][ ˜]αj[(][l][)] _≥_ _ϵ_ = 0, for all i ̸= j ∈ [ml]. (13)
 

See the proof in Appendix C.

Recalling Eq.(10) and the fact that the normal vectors π0(ni) are parallel to the gradients
_θ(l−1) ˜α[(][l][)], we immediately have that these normal vectors are orthogonal with probability 1, in_
_∇_
the limit of ml−1 →∞, as stated in the following theorem.
**Theorem 3 (Orthogonality of normal vectors).**

_∀ϵ > 0,_ _ml−lim1→∞_ [P] **n[T]i** **[n][j]** _≥_ _ϵ_ = 0, for all i ̸= j ∈ [ml]. (14)
  

**Remark 4. As seen in Section 2, a two-layer neural network Eq.(7) is an assembly model with**
independent sub-models. The normal vectors {ni}i[m]=1[l] [are exactly orthogonal to each other, even for]
finite hidden layer width.

See Appendix B for an numerical verification of
this orthogonality. This orthogonality allows the
existence of a new coordinate system such that all
the normal vectors are along the axes. Specifically,
in the old coordinate system O, each axis is along
an individual neural network parameter θi _θ with_
_i ∈_ [p]. After an appropriate rotation, O ∈ can be
transformed to a new coordinate system O[′] such
that each normal vector ni is parallel to one of the
new axes. See Figure 2 for an illustration. Denote
_θ[′]_ as the set of new parameters that are long the
axes of new coordinate system O[′]: θ[′] = Rθ, where
is a rotation operator.
_R_ Figure 2: Coordinate systems: (new) vs.

_O[′]_ _O_

The interesting observation is that each neuron αi[(][l][)] (old). normal vectors ni and gradients ∇αi are
essentially depends on only one new parameter along an axis of O[′].
_θi[′]_
axis in the new coordinate system[∈] _[θ][′][, because its gradient direction is parallel with one normal vector] O[′]. Moreover, different neurons depends on different new pa-[ n][i][ and][ n][i][ is along one]_
rameters, as normal vectors are never parallel. Hence, these neurons are essentially independent to
each other. This view actually allows us to use the analysis for assembling independent models in
Section 2 to show the O(1)-neighborhood linearity at layer l + 1, as follows.

**Linearity of the pre-activations in layer l+1. Having the properties for neurons in layer l discussed**
above, we are now ready to analyze the pre-activation neurons ˜α[(][l][+1)] in layer l + 1 as assembly
models. Recall that each pre-activation ˜α[(][l][+1)] is defined as:


_ml_

_wj[(][l][+1)]αj[(][l][)][.]_ (15)
_i=1_

X


1
_α˜[(][l][+1)]_ =
_√ml_


Without ambiguity, we omitted the index i for the pre-activation ˜α[(][l][+1)] and the weights w[(][l][+1)].

We want to derive the quadratic term (i.e., the Lagrange remainder term) of ˜α[(][l][+1)] in its Taylor expansion and to show it is arbitrarily small, in the O(1)-neighborhood (θ0). First, let’s consider the
_N_
special case where the parameters w[(][l][+1)] are fixed, i.e., w[(][l][+1)] = w0[(][l][+1)], and ˜α[(][l][+1)] only depends
on θ[(][l][)]. This case conveys the key concepts of the O(1)-neighborhood linearity after assembling.
We will relax this constraint in Appendix D.

Consider an arbitrary parameter setting θ (θ0), and let R := _θ_ _θ0_ = O(1). By the
_∈N_ _∥_ _−_ _∥_
definition of assembly model ˜α[(][l][+1)] in Eq.(15), the quadratic term in its Taylor expansion Eq.(1)
can be decomposed as:


1

_._ (16)
2 [(][θ][ −] _[θ][0][)][T][ H][α]i[(][l][)]_ [(][ξ][)(][θ][ −] _[θ][0][)]_

 


_ml_

_wi,[(][l]0[+1)]_
_i=1_

X


1 1
_α[(][l][+1)][(][ξ][)(][θ][ −]_ _[θ][0][) =]_
2 [(][θ][ −] _[θ][0][)][T][ H]_ [˜] _√ml_


-----

_α[(][l][+1)]_ _i_
where Hα˜[(][l][+1)][ =][ ∂][2][ ˜]∂θ[2] and Hα(il) = _[∂][2]∂θ[α][2][(][l][)]_ are the Hessians of ˜α[(][l][+1)] and αi[(][l][)][, respectively, and]

_ξ ∈P is on the line segment between θ[(][l][)]_ and θ0[(][l][)][.]

We bounded the term in the square bracket using a treatment analogous to Theorem 1. First, as seen
in Property 1, the level sets of αi[(][l][)] are hyper-planes with co-dimension 1, perpendicular to ni. Then
the value of αi[(][l][)] only depends on the component (θ − _θ0)[T]_ **ni, and Hessian Hα(il)** is rank 1 and can

be written as Hα(il) = cnin[T]i [, with some constant][ c][ ≤] _[β][. Hence, we have]_

2 2
(θ − _θ0)[T]_ _Hα(il)_ [(][ξ][)(][θ][ −] _[θ][0][)]_ = c (θ − _θ0)[T]_ **ni** _≤_ _β_ (θ − _θ0)[T]_ **ni** _._
     

Here β is the smoothness of the sub-model, i.e., post-activation α[(][l][)]. By Eq.(16), we have:


1 _β_ _ml_
2 (θ − _θ0)[T]_ _Hα˜[(][l][+1)][(][ξ][)(][θ][ −]_ _[θ][0][)]_ _≤_ 2[√]ml max(|wi,[(][l]0[+1)]|)

_i=1_

X

Second, by the orthogonality of normal vectors as in Theorem 3, we have


2
(θ _θ0)[T]_ **ni** _._ (17)
_−_



_ml_

(θ _θ0)[T]_ **ni** 2 _θ_ _θ0_ 2 = R2. (18)
_−_ _≤∥_ _−_ _∥_

_i=1_

X   

Combining the above two equations, we obtain the following theorem which upper bound the magnitude of the quadratic term of ˜α[(][l][+1)]:

**Theorem 4 (Bounding the quadratic term). Assume that the parameters in layer l + 1 are fixed to**
_the initialization. For any parameter setting θ_ (θ0),
_∈N_

1 _βR_
2 (θ − _θ0)[T]_ _Hα˜[(][l][+1)][(][ξ][)(][θ][ −]_ _[θ][0][)]_ _≤_ 2[√]ml max(|wi,[(][l]0[+1)]|). (19)

For the Gaussian random initialization, the above upper bound is of the order O(log(ml)/[√]ml).
Hence, as ml, the quadratic term of pre-activation ˜α[(][l][+1)] in the (l + 1)-th layer vanishes, and
_→∞_
the function ˜α[(][l][+1)] becomes linear.


**Concluding the induction analysis.** Applying the analysis in Section 3.2, Theorem 2 - 4, with
a standard induction argument, we conclude that the neural network, as well as each of its
hidden pre-activation neurons, is O(1)-neighborhood linear, in the infinite network width limit,
_m1, . . ., mL−1 →∞._

**Extension to other architectures.** In Appendix E, we further show that the assembly analysis and
the O(1)-neighborhood linearity also hold for DenseNet (Huang et al., 2017).

4 CONCLUSION AND FUTURE DIRECTIONS

In this work, we viewed a wide fully-connected neural network as a hierarchy of assembly models.
Each assembly model corresponds to a pre-activation neuron and is linearly assembled from a set of
sub-models, the post-activation neurons from the previous layer. When the network width increases
to infinity, we observed that the neurons within the same hidden layer become essentially independent. With this property, we shown that the network is linear in an O(1)-neighborhood around the
network initialization.

We believe the assembly analysis and the principles we identified, especially the essential independence of sub-models, and their iterative construction, are significantly more general than the specific
structures we considered in this work, and, for example, apply to a broad range of neural architectures. In future work, we aim to apply the analysis to general feed-forward neural networks, such as
architectures with arbitrary connections that form an acyclic graph.


-----

ACKNOWLEDGEMENTS

We are grateful for the support of the NSF and the Simons Foundation for the Collaboration on the
Theoretical Foundations of Deep Learning[1] through awards DMS-2031883 and #814639. We also
acknowledge NSF support through IIS-1815697 and the TILOS institute (NSF CCF-2112665).

REFERENCES

Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via overparameterization. In International Conference on Machine Learning, pp. 242–252, 2019.

Leo Breiman. Bagging predictors. Machine learning, 24(2):123–140, 1996.

Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675–
1685, 2019.

Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2018.

Yoav Freund. Boosting a weak learning algorithm by majority. Information and computation, 121
(2):256–285, 1995.

Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic regression: a statistical
view of boosting (with discussion and a rejoinder by the authors). The annals of statistics, 28(2):
337–407, 2000.

Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
_recognition, pp. 4700–4708, 2017._

Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in neural information processing systems, pp. 8571–
8580, 2018.

Herbert Jaeger. The “echo state” approach to analysing and training recurrent neural networks-with
an erratum note. Bonn, Germany: German National Research Center for Information Technology
_GMD Technical Report, 148(34):13, 2001._

Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selection. Annals of Statistics, pp. 1302–1338, 2000.

Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha SohlDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in neural information processing systems, pp. 8570–8581,
2019.

Chaoyue Liu, Libin Zhu, and Mikhail Belkin. On the linearity of large non-linear models: when
and why the tangent kernel is constant. Advances in Neural Information Processing Systems, 33,
2020.

Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Loss landscapes and optimization in overparameterized non-linear systems and neural networks. Applied and Computational Harmonic
_Analysis, 2022._

Samet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks. IEEE Journal on Selected Areas in
_Information Theory, 1(1):84–105, 2020._

Robert E Schapire. The strength of weak learnability. Machine learning, 5(2):197–227, 1990.

1https://deepfoundations.ai/


-----

Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
_arXiv:1011.3027, 2010._

Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018.

Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018.


-----

A RANDOM SUB-MODEL WEIGHTS WITH GAUSSIAN DISTRIBUTION

Let’s consider the case where the weights vi of the sub-models are randomly drawing from Gaussian
distributions, for example the normal distribution N (0, 1). That is


_vigi(wi; x),_ (20)
_i=1_

X


_f_ (θ; x) =


_s(m)_


where vi’s are randomly drawn from (0, 1).
_N_

In this case, there is no strict upper bounded on the absolute values of weights _vi_ . But with high
_|_ _|_
probability, we can still bound them using the following lemma.
**Lemma 2. Let v1, ..., vm be i.i.d.** _Gaussian variables._ _Then with probability at least 1 −_
2e[−][0][.][5 log][2][ m][+log][ m],

max
_i∈[m]_ _[|][v][i][| ≤]_ [log][ m.]

The above lemma can be obtained by letting t = log m in Eq.(2.10) in Vershynin (2018) and using
union bound.

Using the same analysis as in the proof of Theorem 1, we have, with probability at least
1 − 2e[−][0][.][5 log][2][ m][+log][ m],


_m_

_i=1_ _|vi| · ∥Hgi_ _∥· ∥wi −_ **wi,0∥[2]** _≤_ _[β]2[ log]s(m[ m])_

X


2 [(][θ][ −] _[θ][0][)][T][ H][(][ξ][)(][θ][ −]_ _[θ][0][)]_

[1]


**wi** **wi,0**
_i=1_ _∥_ _−_ _∥[2]_

X


2s(m)


Under Assumption 2, β is a constant. Note that as the number of m goes to infinity, the probability
1 − 2e[−][0][.][5 log][2][ m][+log][ m] converges to 1. Since 1/s(m) is O(1/[√]m), hence, we get

log m
2 [(][θ][ −] _[θ][0][)][T][ H][(][ξ][)(][θ][ −]_ _[θ][0][)]_ = O _√m_ _._ (21)
 

When m, the quadratic term of the Taylor expansion converges to 0, with probability 1.

[1]

_→∞_

B EXPERIMENTAL VERIFICATION OF THE ORTHOGONALITY IN THEOREM 3

In this section, we run experiments to verify the theoretical finding in Theorem 3, that the normal
vectors ni within the same hidden layer tends to become more and more orthogonal, as the layer
width increases.

Specifically, we run two experiments. In the first one, we use full-batch gradient descent to train
4-layer fully-connected neural networks on a simple dataset D: 20 images randomly selected from
CIFAR-10 of the classes “airplane” or “bird”. In the network, all the hidden layers have the same
width m. Given a training sample and any two neurons ˜αi and ˜αj in the same layer, we compute the
cosine cos θij between the gradients ∇α˜i and ∇α˜j as

_αi,_ _α˜j_
cos θij = _∇_ _⟩_ (22)

_[⟨∇]α˜[˜]i_ _α˜i_

_|∇_ _| · |∇_ _|_ _[.]_

As a metric to measure the orthogonality of the gradients, the average absolute cosine | cos θ| is
calculated by averaging cos θij over all pairs of (i, j) with i = j and over all data samples. Left
_|_ _|_ _̸_
panel of Figure 3 shows the numerical results of the average absolute cosine | cos θ| as a function
of network width m. We see that, as m increases, | cos θ| monotonically decreases towards zero,
verifying that the neuron gradients becomes more and more orthogonal.

In the second experiment, we consider a bottleneck network, which has three hidden layers, with
the 1st and 3rd layers with large width m = 1000 and 2nd layer with width mb as the bottleneck
layer. We train this bottleneck network using full-batch gradient descent on the same dataset D as in
the first experiment. Given a training sample and any two neurons ˜αi and ˜αj in the bottleneck layer
(i.e., 2nd hidden layer), we compute the cosine of the angle between their gradients using Eq. (22),


-----

Figure 3: Absolute cosine of the angle between two neurons, averaged over all neuron pairs and data
samples. Left: 4-layer fully-connected neural network, all hidden layers of which have equal width
_m; Right: Bottleneck network with three hidden layers, m1 = m3 = 1000 and m2 = mb. In both_
cases, the average absolute cosine | cos θ| monotonically decreases towards zero, consistent with the
theoretical prediction of orthogonality between neuron gradient in Theorem 3.

and then take the average of its absolute value over all pairs of different neurons and over all data
samples. We use this averaged absolute cosine | cos θ| as the metric to measure the orthogonality
of the gradients. Right panel of Figure 3 shows the relation between | cos θ| and the bottleneck
width mb. It is clear that | cos θ| monotonically decreases towards zero as mb increase, verifying the
orthogonality of gradients of bottleneck neurons.

C PROOF OF LEMMA 1

As in Eq. (11), the partial gradient√m1l−1 (wi,[(][l]0[)][)][T][ ∇][α][(][l][−][1)][.][ Hence, we have] _∇θ(l−1) ˜αi[(][l][)]_ can be written as: (∇θ(l−1) ˜αi[(][l][)][)][T] =


( _θ(l−1) ˜αi[(][l][)][)][T][ ∇]θ[(][l][−][1)][ ˜]αj[(][l][)]_
_∇_


1

(wi,[(][l]0[)][)][T][ ∇][α][(][l][−][1)][(][∇][α][(][l][−][1)][)][T][ (][w]j,[(][l]0[)] [)][.]
_ml−1_


Note that wi,[(][l]0[)] [and][ w]j,[(][l]0[)] [are independent to each other when][ i][ ̸][=][ j][, and][ ∇][α][(][l][−][1)][ is a fixed matrix]

independent of wi,[(][l]0[)] [and][ w]j,[(][l]0[)] [. Denote][ a][j][ :=][ ∇][α][(][l][−][1)][(][∇][α][(][l][−][1)][)][T][ w]j,[(][l]0[)] _[∈]_ [R][m][l][−][1] [. Then,]


( _θ(l−1) ˜αi[(][l][)][)][T][ ∇]θ[(][l][−][1)][ ˜]αj[(][l][)]_
_∇_


1

(wi,[(][l]0[)][)][T][ a][j][,] (23)
_ml−1_


the inner product of two independent vectors wi,[(][l]0[)] [and][ a][j][ with a scaling factor][ 1][/m][l][−][1][.]

When conditioned on the vector aj, the quantity in Eq.(23) is a Gaussian variable:


_j_ **[a][j]**
0, **[a][T]**

_m[2]l−1_


1

(wi,[(][l]0[)][)][T][ a][j][ ∼N]
_ml−1_


because wi[(][l][)] is initialized following N (0, I). In the limit of ml−1 →∞, if the variance _ma[T]j[2]l−[a][j]1_

converges to 0, then this Gaussian variable should also converges to zero with probability 1, from

which we can conclude the lemma. In the following, we will show that the variance _ma[T]j[2]l−[a][j]1_ [converges]

to 0 with probability 1 in this limit.

First, we need the following lemma to upper bound the spectral norm of ∇α[(][l][)]. The proof is deferred
to Section C.1.


-----

**Lemma 3. Consider the neural networks defined in Eq.(8) with random Gaussian initialized pa-**
_rameters. There exist a constant C > 0, such that, with probability at least 1 −_ 4(l + 1)e[−][m/][2], we
_have_

_∥∇α[(][l][)](∇α[(][l][)])[T]_ _∥≤_ _C,_

_with m = min{m1, m2, . . ., ml−1}._

Since aj = ∇α[(][l][−][1)](∇α[(][l][−][1)])[T] **wj,[(][l]0[)]** [, using Lemma 3, we can bound][ ∥][a]j[T] **[a][j][∥]** [by]

_∥a[T]j_ **[a][j][∥≤∥∇][α][(][l][−][1)][(][∇][α][(][l][−][1)][)][T][ ∥][2][∥][w]j,[(][l]0[)]** _[∥][2][ ≤]_ _[C]_ [2][∥][w]j,[(][l]0[)] _[∥][2][.]_

Recall that wj,[(][l]0[)] _[∈]_ [R][m][l][−][1][ follows the Gaussian distribution][ N] [(][0][, I][)][. Then][ ∥][w]j,[(][l]0[)] _[∥][2][ ∼]_ _[χ][2][(][m][l][−][1][)][.]_
By Lemma 1 in Laurent & Massart (2000), we have with probability at least 1 − _e[−][m][l][−][1]_,

_∥wj,[(][l]0[)]_ _[∥][2][ ≤]_ [5][m][l][−][1][.]

By union bound, we have with probability 1 − 2(L + 1)e[−][m/][2] _−_ _e[−][m][l][−][1]_, the variance

**a[T]j** **[a][j]**

_m[2]l−1_ _≤_ _m[5][C]l−[2]1_


In the infinite network width limitthe probability converges to 1. _m1, . . ., ml−1 →∞, we see that the variance converges to 0 and_

C.1 PROOF OF LEMMA 3

_Proof. First, note that ∇α[(][l][−][1)](∇α[(][l][−][1)])[T]_ can be decomposed as


_l−1_

_α[(][l][−][1)](_ _α[(][l][−][1)])[T]_ = _W (l[′]_ ) _α[(][l][−][1)](_ _W (l[′]_ ) _α[(][l][−][1)])[T]_ _._
_∇_ _∇_ _∇_ _∇_

_l[′]=1_

X

Then, its spectral norm can be bounded by


_l−1_

_W (l[′]_ ) _α[(][l][−][1)](_ _W (l[′]_ ) _α[(][l][−][1)])[T]_
_∇_ _∇_ _∥_
_l[′]=1_

X


_∥∇α[(][l][−][1)](∇α[(][l][−][1)])[T]_ _∥_ = ∥


_l−1_

_W (l[′]_ ) _α[(][l][−][1)](_ _W (l[′]_ ) _α[(][l][−][1)])[T]_
_∥∇_ _∇_ _∥_
_l[′]=1_

X


_T_
!

(24)


_l−1_

_l[′]=1_

X

_l−1_

_l[′]=1_

X


_l−1_

_l[′′]=l[′]+1_

Y

2 _l−1_


_l−1_

_l[′′]=l[′]+1_

Y


_∂α[(][l][′][)]_

_∂W_ [(][l][′][)]

_∂α[(][l][′][)]_

_∂W_ [(][l][′][)]


_∂α[(][l][′′][)]_

_∂α[(][l][′′][−][1)]_

_∂α[(][l][′′][)]_


_∂α[(][l][′][)]_

_∂W_ [(][l][′][)]


_∂α[(][l][′′][)]_

_∂α[(][l][′′][−][1)]_


_∂α[(][l][′′][−][1)]_


_l[′′]=l[′]+1_


In the following, we need to bound the terms ∥∂α[(][l][′][)]/∂W [(][l][′][)]∥ and ∥∂α[(][l][′′][)]/∂α[(][l][′′][−][1)]∥.

To simplify the presentation of the proof, we assume in the following that each network hidden layer
has the same width m. We leave the general analysis for the readers.

We use the following lemma to bound the spectral norm of the weight matrices at initialization.


**Lemma 4. If each component of W0[(][l][)][,][ l][ ∈]** [[][L][]][, is i.i.d. drawn from][ N] [(0][,][ 1)][, then with probability]
_at least 1 −_ 2e[−][m/][2],

_W0[(][l][)]_
_∥_ _[∥≤]_ [3][√][m.]


-----

See the proof in Section C.2.

There is also a lemma that bounds the Euclidean norm of each hidden layer neuron vector α[(][l][)].

**Lemma 5 (Modified Lemma F.3 of (Liu et al., 2020)). There exists constants Cα, Bα > 0 such that,**
_with probability at least 1 −_ 2(L + 1)e[−][m/][2], for all l ∈ [L],

_α[(][l][)]_ _Cα√m + Bα._ (25)
_∥_ _∥≤_

Using the above two lemmas, we can bounded those two terms. At initialization, we have, with
probability at least 1 − 2e[−][m/][2], 2 _m_
_∂α[(][l][)]_ 1 2

= sup _σ[′](˜αi[(][l][)][)][W][ (]0,ij[l][)]_ _[v][j]_

_∂α[(][l][−][1)]_ _∥v∥=1_ _m_ Xi=1  

1

= sup 0 **[v][∥][2]**
_∥v∥=1_ _m_ _[∥][Σ][′][(][l][)][W][ (][l][)]_

_≤_ _m[1]_ _[∥][Σ][′][(][l][)][∥][2][∥][W][ (]0[l][)][∥][2]_

_≤_ 9L[2]σ[,]

and with probability at least 1 − 2(L + 1)e[−][m/][2], for all l ∈ [L],

2 _m_

_∂α[(][l][)]_ 1 2

= sup _σ[′](˜αi[(][l][)][)][α]j[(][l][′][−][1)]Ii=jVjj[′]_

_∂W_ [(][l][)] _∥V ∥F =1_ _m_ Xi=1  Xj,j[′] 


1

_m_ _[∥][Σ][′][(][l][)][V α][(][l][−][1)][∥][2]_


sup
_∥V ∥F =1_


1
_≤_ _m_ _[∥][Σ][′][(][l][)][∥][2][∥][α][(][l][−][1)][∥][2]_

L[2]σ[C]α[2] [+ 1] _σ[B]α[2]_ _[.]_ (26)
_≤_ _m_ [L][2]

Here Σ[′][(][l][)] is a diagonal matrix, with the diagonal entry Σ[′][(]ii[l][)] [=][ σ][′][(˜]αi[(][l][)][)][ and][ L][σ][ is the degree of]
Lipschitz continuity of the activation function σ(·).

Now, using the above results, we can upper bound the spectral norm of the matrix
_∇α[(][l][−][1)](∇α[(][l][−][1)])[T]_ . With probability 1 − 4(L + 1)e[−][m/][2], we have

_l−1_ _∂α[(][l][′][)]_ 2 _l−1_ _∂α[(][l][′′][)]_ 2

_α[(][l][−][1)](_ _α[(][l][−][1)])[T]_
_∥∇_ _∇_ _∥≤_ _∂W_ [(][l][′][)] _∂α[(][l][′′][−][1)]_

_l[′]=1_ _l[′′]=l[′]+1_

X Y

(l 1)9[l][−][1]L[2]σ[l][(][C]α[2] [+ 1] _α[)][.]_ (27)
_≤_ _−_ _m_ _[B][2]_

Since l _≤_ _L, we can see that, for sufficient large network width m, the spectral norm_
_∥∇α[(][l][−][1)](∇α[(][l][−][1)])[T]_ _∥_ is bounded by a constant.

C.2 PROOF OF LEMMA 4


_Proof. Consider an arbitrary random matrix A ∈_ R[m][a][×][m][b] with each entry Ai,j ∼N (0, 1). By
Corollary 5.35 of Vershynin (2010), for any t > 0, we have with probability at least 1 − 2exp(− _[t]2[2]_ [)][,]

_A_ _ma +_ _mb + t._ (28)
_∥_ _∥≤_ _[√]_ _[√]_
For the initial neural network weight matrices, we have

_W0[(1)]_ 2 _√d +_ _m + t,_
_∥_ _∥_ _≤_ _[√]_

_W0[(][l][)]_ _l_ 2, 3, ..., L _,_
_∥_ _[∥][2][ ≤]_ [2][√][m][ +][ t,] _∈{_ _}_

_W0[(][L][+1)]_ 2 _m + 1 + t._
_∥_ _∥_ _≤_ _[√]_

Letting t = _m and noting that m > d, we finish the proof._

_[√]_


-----

D INCLUDING w[(][l][+1)] AS PARAMETERS

Now, we consider the more general case where parameters in layer l + 1 are not fixed. Then, the
quadratic term of the pre-activation ˜α[(][l][+1)] is written as:


1
_α[(][l][+1)]_ [(][ξ][)(][θ][ −] _[θ][0][)]_
2 [(][θ][ −] _[θ][0][)][T][ H]_ [˜]

_T_ _∂[2]α˜[(][l][+1)]_ _∂[2]α˜[(][l][+1)]_

1 **w[(][l][+1)]** **w0[(][l][+1)]** (∂w[(][l][+1)])[2][ (][ξ][)] _∂θ[(][l][)]∂w[(][l][+1)][ (][ξ][)]_
2 _θ[(][l][)]_ _−−_ _θ0[(][l][)]_ ! " _∂w∂[(][2][l]α[+1)]˜[(][l][+1)]∂θ[(][l][)][ (][ξ][)]_ _∂([2]∂θα˜[(][(][l][l][)][+1)])[2][ (][ξ][)]_

1 _α[(][l][+1)]_
2 [(][w][(][l][+1)][ −] **[w]0[(][l][+1)])[T][ ∂](∂w[2]** [˜][(][l][+1)])[2][ (][ξ][)(][w][(][l][+1)][ −] **[w]0[(][l][+1)])** (=: A)

_α[(][l][+1)]_

+ [1] 0 [)][T][ ∂][2] [˜] 0 [)] (=: )

2 [(][θ][(][l][)][ −] _[θ][(][l][)]_ (∂θ[(][l][)])[2][ (][ξ][)(][θ][(][l][)][ −] _[θ][(][l][)]_ _B_


**w[(][l][+1)]** **w0[(][l][+1)]**
_−_
_θ[(][l][)]_ _θ0[(][l][)]_
_−_


_∂[2]α˜[(][l][+1)]_
+(w[(][l][+1)] **w0[(][l][+1)])[T]** 0 [)] (=: )
_−_ _∂w[(][l][+1)]∂θ[(][l][)][ (][ξ][)(][θ][(][l][)][ −]_ _[θ][(][l][)]_ _C_

where ξ is some point between θ0 and θ.


_∂[2]α˜[(][l][+1)]_
Note that ˜α[(][l][+1)] is linear in w[(][l][+1)], hence, (∂w[(][l][+1)])[2][ is always a zero matrix. Hence, term][ A ≡] [0][.]

Moreover, by Theorem 4, the term B becomes zero, as the width ml increases to infinity. For the
term C, we note that
_∂[2]α˜[(][l][+1)]_ 1

_α[(][l][)]._
_∂w[(][l][+1)]∂θ[(][l][)][ =]_ _√ml_ _∇_

Here, α[(][l][)] = (α1[(][l][)][, . . ., α]m[(][l][)]l [)][ is the vector of the neurons in layer][ l][ and][ ∇][α][(][l][)][ is the matrix of the]
first-order derivative of the vector α[(][l][)]. Using Lemma 3, we have an upper bound on the spectral
norm: ∥∇α[(][l][)]∥≤ _√C, with C > 0 being a constant. Therefore, we have_

1 _C_
_|C| ≤_ _√ml_ _∥w[(][l][+1)]_ _−_ **w0[(][l][+1)]∥∥∇α[(][l][)]∥∥θ[(][l][)]** _−_ _θ0[(][l][)][∥≤]_ _[R]√[2][√]ml_ _._ (29)

In the limit of ml, term converges to 0. Therefore, we have proved that each (l + 1)-th
_→∞_ _C_
layer pre-activation ˜α[(][l][+1)] has a vanishing quadratic term in its Taylor expansion Eq.(1), and hence
becomes linear, in the limit of ml .
_→∞_

E DENSENET

In a DenseNet (Huang et al., 2017), a l-th layer neuron takes all previous layer neurons as inputs:

1
_α[(][l][)]_ = σ(˜α[(][l][)]), _α˜[(][l][)]_ = _√mW_ [(][l][)] _∗_ Concat[α[(][l][−][1)], . . ., α[(1)], x], (30)

where the “Concat” function concatenates all the vector arguments into a single long vector with
_mof l being the size, and linear functions, ˜α ∗[(][l][)]is the matrix multiplication. The pre-activation can be viewed as the sum=_ _l[′]=1_ _α[˜]U[(][l][′][)][, where each][ ˜]αU[(][l][′][)]_ := _√1m_ _U_ [(][l][′][)]α[(][l][′][)] has a similar form as

the pre-activation in full-connected neural network Eq.(8), but with parameters U [(][l][′][)] being a subset
of W [(][l][)]. Hence, using the analysis in Section 3, in the infinite network width limit and in[P][l] _O(1)-_
neighborhoods of the initialization, each of the pre-activations of the DenseNet is a summation
of l ≤ _L linear functions, and hence is linear. Therefore, the DenseNet is also linear in O(1)-_
neighborhoods of the initialization.


-----

