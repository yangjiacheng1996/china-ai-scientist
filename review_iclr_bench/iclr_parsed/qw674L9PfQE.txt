# CLOOB: MODERN HOPFIELD NETWORKS WITH INFOLOOB OUTPERFORM CLIP

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Contrastive learning with the InfoNCE objective is exceptionally successful in
various self-supervised learning tasks. Recently, the CLIP model yielded impressive results on zero-shot transfer learning when using InfoNCE for learning
visual representations from natural language supervision. However, InfoNCE as
a lower bound on the mutual information has been shown to perform poorly for
high mutual information. In contrast, the InfoLOOB upper bound (leave one out
bound) works well for high mutual information but suffers from large variance and
instabilities. We introduce “Contrastive Leave One Out Boost” (CLOOB), where
modern Hopfield networks boost learning with the InfoLOOB objective. Modern
Hopfield networks replace the original embeddings by retrieved embeddings in the
InfoLOOB objective. The retrieved embeddings give InfoLOOB two assets. Firstly,
the retrieved embeddings stabilize InfoLOOB, since they are less noisy and more
similar to one another than the original embeddings. Secondly, they are enriched
by correlations, since the covariance structure of embeddings is reinforced through
retrievals. We compare CLOOB to CLIP after learning on the Conceptual Captions
and the YFCC dataset with respect to their zero-shot transfer learning performance
on other datasets. CLOOB consistently outperforms CLIP at zero-shot transfer
learning across all considered architectures and datasets.

1 INTRODUCTION

With the advent of large corpora of unlabeled data in vision and language, self-supervised learning
via contrastive learning has become highly successful. Some contrastive learning objectives, such as
those of BYOL (Grill et al., 2020) and SimSiam (Chen & He, 2021), do not require negative samples.
However, the most popular objective for contrastive learning is InfoNCE (van den Oord et al., 2018),
in which for an anchor sample, a positive sample is contrasted with negative samples.

The idea to use objectives with negative samples is well known in deep learning (Gutmann & Hyvärinen, 2010; Chen et al., 2017; Mikolov et al., 2013). For contrastive learning, the most successful
objective is InfoNCE, which has been introduced as Contrastive Predictive Coding (CPC) (van den
Oord et al., 2018). InfoNCE has been applied to transfer learning (Hénaff et al., 2019), to natural
language response suggestion (Henderson et al., 2017), to learning sentence representations from
unlabelled data (Logeswaran & Lee, 2018), and to unsupervised feature learning by maximizing
distinctions between instances (Wu et al., 2018). InfoNCE has been used for learning visual representations in Pretext-Invariant Representation Learning (PIRL) (Misra & vanDerMaaten, 2020), in
Momentum Contrast (MoCo) (He et al., 2020), and in SimCLR (Chen et al., 2020). SimCLR became
well known as is was highly effective for transfer learning. Zero-shot transfer learning (Lampert
et al., 2009) is one of the most ambitious goals in vision, since it would improve various real-world
downstream applications. Current models in natural language processing and vision perform very
well on standard benchmarks, but they fail at new data, new applications, deployments in the wild,
and stress tests (D’Amour et al., 2020; Recht et al., 2019; Taori et al., 2020; Lapuschkin et al., 2019;
Geirhos et al., 2020). A model with high zero-shot transfer learning performance will not fail on such
data, therefore will be trusted by practitioners.

Contrastive Language-Image Pre-training (CLIP) based on the InfoNCE objective yielded very
impressive results at zero-shot transfer learning (Radford et al., 2021). CLIP learns expressive image
embeddings directly from raw text, thereby leverages a much richer source of supervision than just


-----

labels. A plethora of CLIP follow-up work has already been published (see Appendix Section A.5).
The CLIP model is considered as an important foundation model (Bommasani et al., 2021). Though
CLIP excels at zero-shot transfer learning, it can be improved.

CLIP training suffers from an “explaining away” problem (Wellman & Henrion, 1993), which leads
to “shortcut learning” (Geirhos et al., 2020) or the Clever Hans phenomenon (Lapuschkin et al., 2019).
Explaining away impedes the increase of the similarity between a text and a corresponding image,
since learning focuses on only one common aspect and does not exploit the full covariance structure
of the data. If one common aspect is sufficient for high similarity, the InfoNCE objective saturates,
since it has the form a/(a + b) with a giving the similarity of a matched pair and b giving the average
similarity of unmatched pairs. For a large similarity a, the objective saturates and increasing a has a
small effect. Contrary to InfoNCE, the leave-one-out (“InfoLOOB”) bound (Poole et al., 2019) is
of the form a/b which does not saturate. However, so far the InfoLOOB bound was not used as an
objective in contrastive learning. We justify the maximization of the InfoLOOB bound for contrastive
learning in Appendix Section A.1.3. We show that maximizing the InfoLOOB bound leads to a good
approximation of the mutual information, in particular for high mutual information. A problem of
InfoLOOB is that it has high variance for small b.

Even when InfoLOOB avoids saturation, CLIP insufficiently extracts the covariance structure in the
data. The covariance originates from co-occurrences of related words in text or from co-occurrences
of objects, textures, or colors in images. CLIP’s problem of insufficiently extracting the covariance
structure of the data is tackled by modern Hopfield networks. Hopfield networks are energy-based,
binary associative memories, which popularized artificial neural networks in the 1980s (Hopfield,

1982; 1984). Associative memory networks have been designed to store and retrieve samples. Their
storage capacity can be considerably increased by polynomial terms in the energy function (Chen
et al., 1986; Psaltis & Cheol, 1986; Baldi & Venkatesh, 1987; Gardner, 1987; Abbott & Arian, 1987;
Horn & Usher, 1988; Caputo & Niemann, 2002; Krotov & Hopfield, 2016). In contrast to these
binary memory networks, we use continuous associative memory networks with very high storage
capacity. These modern Hopfield networks for deep learning architectures have an energy function
with continuous states and can retrieve samples with only one update (Ramsauer et al., 2021; 2020).
Modern Hopfield Networks have already been successfully applied to immune repertoire classification
(Widrich et al., 2020) and chemical reaction prediction (Seidl et al., 2021). Modern Hopfield networks
reinforce the covariance structure in the data and stabilize the InfoLOOB objective by increasing b.
The covariance structure of retrieved embeddings is amplified through co-occurrences of embedding
features in the memory. Additionally, the retrieved embeddings are less noisy and more similar to
one another which leads to a larger b. We introduce “Contrastive Leave One Out Boost” (CLOOB)
which overcomes CLIP’s problems of (i) “explaining away” with saturation and (ii) insufficiently
extracting the covariance structure of the data. CLOOB uses the leave-one-out (“InfoLOOB”) bound
(Poole et al., 2019) as the objective in combination with modern Hopfield networks.

Our contributions are:

(a) we introduce a new contrastive learning method called CLOOB,

(b) we propose InfoLOOB as an objective for contrastive learning,

(c) we propose to use modern Hopfield networks to reinforce covariance structures,

(d) we show theoretical properties of the InfoLOOB objective and loss function.

2 INFOLOOB VS. INFONCE

We discuss and analyse known bounds on the mutual information I(X ; Y ) between random variables
_X and Y, which are distributed according to p(x, y):_


ln _[p][(][x][ |][ y][)]_

_p(x)_


ln _[p][(][y][ |][ x][)]_

_p(y)_


_p(x, y)_
ln

_p(x) p(y)_


_. (1)_


I(X ; Y ) = Ep(x,y)


= Ep(x,y)


= Ep(x,y)


We consider the multi-sample lower bound “InfoNCE” (van den Oord et al., 2018). A pair of an
anchor sample y and a positive sample x1 is drawn via the joint distribution p(x1, y). The negative
samples _X[˜] =_ **_x2, . . ., xN_** are drawn iid according to the marginal distribution p(x). Using
_{_ _}_
_X = {x1, x2, . . ., xN_ _}, the probabilities of the datasets are p( X[˜]_ ) = _i=2_ _[p][(][x][i][)][,][ p][(][X][ |][ y][) =]_

[Q][N]


-----

_p(x1 | y)_ _i=2_ _[p][(][x][i][)][, and][ p][(][X][) =][ Q]i[N]=1_ _[p][(][x][i][)][. The InfoNCE with score function][ f]_ [(][x][,][ y][)][ is]

_f_ (x1, y)

[Q][N] IInfoNCE(X1 ; Y ) = Ep(y) Ep(X|y) ln 1 _N_ _,_ (2)

" " _N_ _i=1_ _[f]_ [(][x][i][,][ y][)] !##

using the factor 1/N as in Poole et al. (2019); Tschannen et al.P (2019); Cheng et al. (2020); Chen
et al. (2021). For f (x, y) = p(y | x), we obtain the InfoNCE with probabilities. The InfoNCE is a
lower bound on the mutual information (Poole et al., 2019), which is stated in the next theorem.
**Theorem 1 (InfoNCE lower bound). InfoNCE with score function f** (x, y) is a lower bound on the
_mutual information:_

_f_ (x1, y)

I(X1 ; Y ) ≥ Ep(y) "Ep(X|y) "ln _N1_ _Ni=1_ _[f]_ [(][x][i][,][ y][)] !## = IInfoNCE(X1 ; Y ) . (3)

_In particular, the bound holds for InfoNCE with probabilities, i.e. forP_ _f_ (x, y) = p(y **_x)._**
_|_

For a proof see Poole et al. (2019) and the proof of Theorem A1 in the Appendix.

The “Leave one out upper bound” (Poole et al., 2019) on the mutual information was called “L1Out”
in Cheng et al. (2020), while we call it “InfoLOOB” (LOOB for “Leave One Out Bound”). InfoLOOB
is the same as InfoNCE (Eq. (3)), but without the positive sample x1 in the denominator. Contrastive
Log-ratio Upper Bound (CLUB), another upper bound on the mutual information, was only used for
minimizing it (Cheng et al., 2020). Maximizing CLUB failed in experiments, because the embedding
distribution was not uniform as known for similar objectives (Wang & Liu, 2021). Uniform embedding
distributions are required for successful contrastive learning (Wang & Isola, 2020).

We use InfoLOOB as an objective, since it approximates high mutual information better than InfoNCE.
Maximizing an upper bound on the mutual information might be counter-intuitive. Therefore, we
justify the maximization of the InfoLOOB bound for contrastive learning in Appendix Section A.1.3.
We show that maximizing the InfoLOOB bound approximates the mutual information, the better
the higher it is. Recently, InfoLOOB was independently introduced for and successfully applied to
image-to-image contrastive learning (Yeh et al., 2021).

The InfoLOOB with score function f (x, y) is defined in the following, where we obtain the InfoLOOB with probabilities for f (x, y) = p(y | x):

_f_ (x1, y)

IInfoLOOB(X1 ; Y ) = Ep(y) Ep˜(X|y) ln 1 _N_ _._ (4)

" " _N_ 1 _i=2_ _[f]_ [(][x][i][,][ y][)] !##

_−_

Before we show that InfoLOOB with a score function is an upper bound on the mutual information,P
we need some definitions. ˜p(x | y) draws the positives for y with lower probability than p(x), that
is, the positives are under-sampled. Z(y) = Ep˜(x **_y)_** [[][f] [(][x][,][ y][)]][ gives the average score][ f] [(][x][,][ y][)][, if]
_|_
under-sampling via ˜p(x | y), while Z _[∗](y) = Ep(x) [f_ (x, y)] average score f (x, y) if sampling
from p(x). We define the variational distribution q(x **_y) =_** _[p][(][x]Z[)][∗][f]([(]y[x])[,][y][)]_ . Our main assumption is
_|_

expressed by the log-ratio of the averages Z(y) and Z _[∗](y):_

Ep(y) [KL(p(x | y) ∥ _q(x | y))] ⩽_ Ep(y) [ln Z _[∗](y) −_ ln Z(y)], (5)

which ensures that the positives x are sufficiently under-sampled via p(x | y). The Kullback-Leibler
divergence gives the minimal difference between averaging f (x, y) via p(x) and via ˜p(x | y). The
next theorem shows that InfoLOOB is an upper bound on the mutual information.

**Theorem 2 (InfoLOOB upper bound). If** _X[˜] = {x2, . . ., xN_ _} are drawn iid according to ˜p(x | y)_
_and if the main assumption Eq. (5) holds, then InfoLOOB with score function f_ (x, y) is an upper
_bound on the mutual information:_

_f_ (x1, y)

I(X1 ; Y ) ⩽ Ep(y) Ep˜(X|y) ln 1 _N_ = IInfoLOOB(X1 ; Y ) . (6)

" " _N_ 1 _i=2_ _[f]_ [(][x][i][,][ y][)] !##

_−_

_The bound is valid for InfoLOOB with probabilities (without under-sampling), where the negativeP_
_samples_ _X[˜] =_ **_x2, . . ., xN_** _are drawn iid according to p(x) and f_ (x, y) = p(y **_x)._**
_{_ _}_ _|_

The proof for this theorem is given as proof for Theorem A2 in the Appendix.


-----

**Loss functions and their gradients. The training set {(x1, y1), (x2, y2), . . ., (xN** _, yN_ )} consists
of N samples that are drawn iid from p(x, y). InfoNCE uses the matrix X = (x1, . . ., xN ), while
InfoLOOB uses **_X[˜] = (x2, . . ., xN_** ). The matrices differ by the positive sample x1. For the score
function f (x, y), we use f (x, y) = exp(τ _[−][1]sim(x, y)) with the similarity sim(x, y) = y[T]_ **_x and_**
_τ as the temperature. We have the InfoNCE and InfoLOOB loss functions:_

_N_ _N_

LInfoNCE = − _N[1]_ ln _Nexp(τ_ _[−][1]_ **_x[T]i_** **_[y][i][)]_** _−_ _N[1]_ ln _Nexp(τ_ _[−][1]_ **_x[T]i_** **_[y][i][)]_** _,_ (7)

_i=1_ _j=1_ [exp(][τ][ −][1][ x]i[T] **_[y][j][)]_** _i=1_ _j=1_ [exp(][τ][ −][1][ x]j[T] **_[y][i][)]_**

X X

_N_ _N_

LInfoLOOB = − _N[1]_ ln PNexp(τ _[−][1]_ **_x[T]i_** **_[y][i][)]_** _−_ _N[1]_ ln PNexp(τ _[−][1]_ **_x[T]i_** **_[y][i][)]_** _._ (8)

_i=1_ _j=i_ [exp(][τ][ −][1][ x]i[T] **_[y][j][)]_** _i=1_ _j=i_ [exp(][τ][ −][1][ x]j[T] **_[y][i][)]_**

X _̸_ X _̸_

In the second sum of the losses in Eq.P 7 and Eq. 8, we consider only the first term. For simplicity, weP
abbreviate y = y1 leading to the pair (x1, y) and the negatives **_X[˜] = (x2, . . ., xN_** ).

LInfoNCE(y) = ln _Nexp(τ_ _[−][1]_ **_x[T]1_** **_[y][)]_** _,_ LInfoLOOB(y) = ln _Nexp(τ_ _[−][1]_ **_x[T]1_** **_[y][)]_** _._
_−_ _−_
_j=1_ [exp(][τ][ −][1][ x]j[T] **_[y][)]_** _j=2_ [exp(][τ][ −][1][ x]j[T] **_[y][)]_**

These loss terms can be simplified toP LInfoNCE(y) = −τ _[−][1]y[T]_ **_x1 + τP[−][1]lse(τ_** _[−][1], X_ _[T]_ **_y) and_**
LInfoLOOB(y) = −τ _[−][1]y[T]_ **_x1 + τ_** _[−][1]lse(τ_ _[−][1],_ **_X[˜]_** _[T]_ **_y), where lse is the log-sum-exp function_**
(see Eq. (A103) in the Appendix). The gradient of the InfoNCE loss with respect to y is
_−τ_ _[−][1]x1 + τ_ _[−][1]Xsoftmax(τ_ _[−][1]X_ _[T]_ **_y) and the gradient of the InfoLOOB loss is −τ_** _[−][1]x1 +_
_τ_ **_Xsoftmax(τ_** **_X_** _[T]_ **_y). Using p = (p1, . . ., pN_** )[T] = softmax(τ **_X_** _[T]_ **_y), the gradient of_**

_[−][1][ ˜]_ _[−][1][ ˜]_ _[−][1]_
InfoNCE with respect to y is _τ_ (1 _p1)(x1_ **_Xsoftmax(τ_** **_X_** _[T]_ **_y)) and its gradient with_**
respect to x1 is −τ _[−][1](1 −_ _p1) −y (see Appendix Subsection[−][1]_ _−_ _−_ [˜] A.1.4). _[−][1][ ˜]_

By and large, the gradient of InfoNCE is scaled by (1 _p1) compared to the gradient of InfoLOOB,_
_−_
where p1 is softmax similarity between the anchor y and positive sample x1. Consequently, InfoNCE
saturates and learning stalls when anchor and positive sample become similar to each other.

|x1 x2 · · · xN x1 x2 · · · xN|Col2|Col3|Col4|
|---|---|---|---|
||x2|· · · xN||


|x2|· · · xN|
|---|---|



Hopfield retrieval with U Hopfield retrieval with V

|1|Vx2|· · · VxN|
|---|---|---|
||||


Hopfield retrieval with V



|≃|≃|
|---|---|
|Uy2|· · · UyN|


**legend**

**_≃_** similarity to anchor

positive sample

negative sample


CLOOB: INFOLOOB WITH M

image
encoder

**_x1_** **_x2_** _· · ·_ **_xN_**

Hopfield retrieval with U

**_Ux1 Ux2_** _· · · UxN_

**_≃_** **_≃_** **_≃_**

**_Uy1 Uy2_** _· · · UyN_


**_≃_**

**_y1 V_**


**_≃_**

2


**_≃_**

_N_


|y2|· · · yN|
|---|---|


|Hopfield retrieval with U Hopfield retrieval with|opfield|retrieval with|Col4|
|---|---|---|---|
|y1 y2 · · · yN y1 y2 · · · yN|y2|· · · yN||
|||||


Our dog is
playing in text
the snow. encoder

Figure 1: The CLOOB architecture for image-text pairs. The image embedding xi and the text embedding yi retrieve the embeddings Uxi and Uyi, respectively, from a modern Hopfield network that
stores image embeddings U = (u1, . . ., uM ) (green boxes at the left). The image-retrieved image
embedding Uxi serves as anchor in order to contrast the positive text-retrieved image embedding
**_Uyi with the negative text-retrieved image embedding Uyj for j ̸= i. Analog, for the second modern_**
Hopfield network that stores text embeddings V = (v1, . . ., vK) (green boxes at the right).


-----

**CLOOB for contrastive learning. Our novel Contrastive Leave One Out Boost (CLOOB) combines**
the InfoLOOB objective with modern Hopfield networks. Modern Hopfield networks substitute
the original by retrieved embeddings, thereby reduce the variance of InfoLOOB and reinforce the
covariance structure in the data. Figure 1 sketches the CLOOB architecture for image-text pairs.

The training set consists of N pairs of embeddings (x1, y1), . . ., (xN _, yN_ ), M stored embeddings
_{_ _}_
**_U = (u1, . . ., uM_** ), and K stored embeddings V = (v1, . . ., vK). The state or query embeddings
**_xi and yi retrieve Uxi and Uyi, respectively, from U — analogous notation for retrievals from V ._**
All samples are normalized: **_xi_** = **_yi_** = **_ui_** = **_vi_** = 1. The following vectors are retrieved
_∥_ _∥_ _∥_ _∥_ _∥_ _∥_ _∥_ _∥_
from modern Hopfield networks (Ramsauer et al., 2021):

**_Uxi = U softmax(β U_** _[T]_ **_xi),_** **_Uyi = U softmax(β U_** _[T]_ **_yi),_** (9)

**_Vxi = V softmax(β V_** _[T]_ **_xi),_** **_Vyi = V softmax(β V_** _[T]_ **_yi)_** (10)


where Uxi denotes an image-retrieved image embedding, Uyi a text-retrieved image embedding,
**_Vxi an image-retrieved text embedding and Vyi a text-retrieved text embedding. The hyperparameter_**
_β corresponds to the inverse temperature: β = 0 retrieves the average of the stored pattern, while_
large β retrieves the stored pattern that is most similar to the state pattern (query).

In InfoLOOB, CLOOB substitutes the embedded samples xi and yi by the retrieved embedded
samples. In the first term, xi and yi are substituted by Uxi and Uyi, respectively, while in the second
term by Vxi and Vyi . All retrieved samples are normalized, ∥Uxi _∥_ = ∥Uyi _∥_ = ∥Vxi _∥_ = ∥Vyi _∥_ = 1.
We obtain the InfoLOOB loss function that is used by CLOOB:


_N_ exp(τ _[−][1]_ **_Ux[T]i_** **_[U][y]i_** [)]

_i=1_ ln _Nj=i_ [exp(][τ][ −][1][ U][ T]xi **_[U][y]j_** [)] _−_ _N[1]_

X _̸_

P


_N_ exp(τ _[−][1]_ **_Vx[T]i_** **_[V][y]i_** [)]

ln _N_ _._
_i=1_ _j=i_ [exp(][τ][ −][1][ V][ T]xj **_[V][y]i_** [)]

X _̸_

(11)

P


LInfoLOOB =
_−_ _N[1]_


**Modern Hopfield Networks reduce high variance of InfoLOOB. CLOOB uses InfoLOOB as**
objective, since it estimates the mutual information (MI) better than InfoNCE, in particular, for large
MI. Cheng et al. (2020, Fig. 1 and Fig. 2) show that InfoLOOB is a better estimator for the MI than
InfoNCE (van den Oord et al., 2018), MINE (Belghazi et al., 2018), and NWJ (Nguyen et al., 2010).
We experimentally confirmed that InfoLOOB better estimates the mutual information than InfoNCE.


14

12

10

8

6

4

2

0

_−2_

_−4_


14

12

10

8

6

4

2

0

_−2_

_−4_


14

12

10

8

6

4

2

0

_−2_

_−4_


14

12

10

_−_

_−_


**with Hopfield**
MI 10

True MI


**with Hopfield**
MI 14

True MI



1000 2000 3000 4000

steps



1000 2000 3000 4000 5000 6000

steps


**without Hopfield**
MI 10

True MI

0 1000 2000 3000 4000

steps


**without Hopfield**
MI 14

True MI

0 1000 2000 3000 4000 5000 6000

steps


Figure 2: Variance reduction of InfoLOOB by modern Hopfield networks. From left to right: without
Hopfield for MI 10, with Hopfield for MI 10, without Hopfield for MI 14, with Hopfield for MI 14.
Modern Hopfield networks reduce the variance of the InfoLOOB loss.

However, InfoLOOB has higher variance than lower bounds on MI like InfoNCE, which considerably
hampers learning (Cheng et al., 2020, Fig. 1 and Fig. 2), see also Appendix Section A.2. The
InfoNCE objective has the form a/(a + b) while InfoLOOB has the form a/b with a giving the
anchor-to-positive similarity and b the average anchor-to-negative similarity. For small b, we observe
high variance and instability of InfoLOOB. Modern Hopfield networks (Ramsauer et al., 2021)
are a remedy for the high variance. Modern Hopfield networks substitute the original patterns by
retrieved patterns, which are an average over the stored patterns. We tested the variance of MI
estimators/bounds on toy tasks, with samples drawn from Gaussian distributions following (Belghazi
et al., 2018; Poole et al., 2019; Cheng et al., 2020). With the InfoLOOB objective, we train deep
learning architectures with and without modern Hopfield networks on top, where the current learning
batch is stored in the modern Hopfield networks. We used training data with mutual information of


-----

10 and 14, where the parameters were optimized for the best performance on a validation set. We
test the final model on different levels of mutual information. Figure 2 shows that modern Hopfield
networks reduce the variance of the model. The average variances are reduced from 0.67 to 0.33 for
MI 10 and from 1.00 to 0.48 for MI 14 (more details in Appendix A.2).

**Modern Hopfield Networks amplify the covariance structure in the data. The covariance struc-**
ture is extracted by the retrieved embeddings Ux[T]i **_[U][y]i_** [and][ V][ T]xi **_[V][y]i[. The Jacobian][ J][ of the soft-]_**
max p = softmax(βa) is J(βa) = β diag(p) − **_pp[T][ ]. We define the weighted covariance_**
Cov(U ), where sample ui is drawn with probability  _pi, as [Cov(U_ )]kl = **_U_** J(βa)U _[T][ ]kl_ [=]

_β([P][M]i=1_ _[p][i][u][ik][u][il][ −]_ [P]i[M]=1 _[p][i][u][ik]_ _Mi=1_ _[p][i][u][il][)][. The formula of the weighted covariance differs from]_
the standard empirical covariance, since the factor 1/M is replaced by pi. Thus ui is sampled with
probability pi instead of being sampled uniformly with probabilityP 1/M .

We apply the mean value theorem to the softmax function with mean Jacobian matrix J[m](βa) =
1
0 [J(][λβ][a][) d][λ][. The mean Jacobian][ J][m][(][β][a][)][ is a symmetric, diagonally dominant, positive semi-]
Rdefinite matrix with one eigenvalue of zero for eigenvector 1 and spectral norm bounded by ∥J[m]∥2 ⩽
0.5β (see Appendix Lemma A1). We can express Ux[T]i **_[U][y]i_** [as (see Appendix Theorem][ A3][):]

**_Ux[T]i_** **_[U][y]i_** [= (¯]u + Cov(U _, xi) xi)[T]_ (¯u + Cov(U _, yi) yi),_ (12)

where the mean is ¯u = 1/M **_U_** **1 and the weighted covariances are Cov(U** _, xi) = U_ J[m](βU _[T]_ **_xi)U_** _[T]_
and Cov(U _, yi) = U_ J[m](βU _[T]_ **_yi)U_** _[T]_ . The weighted covariance Cov(U _, .) is the covariance if the_
stored pattern ui is drawn according to an averaged pi given by J[m](.). When maximizing the dot
product Ux[T]i **_[U][y]i_** [, the normalized vectors][ x][i] [and][ y][i] [are encouraged to agree on drawing the patterns]
**_ui with the same probability pi in order to generate similar weighted covariance matrices Cov(U_** _, .)._
If subsets of U have a strong covariance structure, then it can be exploited to produce large weighted
covariances and, in turn, large dot products of Ux[T]i **_[U][y]i_** [. Furthermore, for a large dot product][ U][ T]xi **_[U][y]i_** [,]
**_xi and yi have to be similar to each other to extract the same direction from the covariance matrices._**
Above considerations for Ux[T]i **_[U][y]i_** [analogously apply to][ V][ T]xi **_[V][y]i_** [.]

We did not use a loss function that contains dot products like Ux[T]i **_[V][y]i[, because these dot products]_**
have higher variance than the ones we have used. The dot product Ux[T]i **_[V][y]i_** [has higher variance, since]
it uses M + K stored patterns, whereas Ux[T]i **_[U][y]i_** [and][ V][ T]xi **_[V][y]i_** [use][ M][ and][ K][, respectively.]

**Modern Hopfield Networks can reuse training samples as stored patterns. We use the training**
samples as the stored patterns in the modern Hopfield network. Hence, we set ui = xi and vi = yi,
that is, U = X and V = Y . Consequently, we store the learning batch in the modern Hopfield
networks as U and V . In particular this means that xi can retrieve itself from U = X but not from
**_V = Y . Analogously, yi can retrieve itself from V = Y but not from U = X._**

**Modern Hopfield networks allow the usage of retrieved embeddings. After learning, both the**
model embeddings x and y as well as the retrieved embeddings Ux, Uy, Vx, and Vy may serve for
the downstream tasks, e.g. for zero-shot transfer learning. When using the retrieved embeddings, the
modern Hopfield networks can store random samples, prototypes, templates, or proprietary samples.
Therefore, particular embedding features can be amplified according to the task at hand.

**Modern Hopfield networks is a new concept for contrastive learning. In bioinformatics the**
covariance structure in a sequence is reinforced by first retrieving similar sequences from a database
and then aligning them. Conserved regions are characterized by high local covariance in the alignment
(Dickson & Gloor, 2012; Kreth & Fodor, 2014). Modern Hopfield networks detect high covariances
of embedded features, which is conveyed by the retrieved sample that corresponds to an alignment.

4 EXPERIMENTS

On two pretraining datasets, we compare our new CLOOB to CLIP (Radford et al., 2021) with respect
to their capability of zero-shot transfer learning. The first dataset, Conceptual Captions (CC) (Sharma
et al., 2018), has a very rich textual description of images but only three million image-text pairs. The
second dataset, a subset of YFCC100M (Thomee et al., 2016), has 15 million image-text pairs but
the textual description is less rich than for CC and often vacuous. For both pretraining datasets, the
downstream zero-shot transfer learning performance is tested on seven image classification datasets.


-----

Table 1: Zero-shot results for models trained on CC with ResNet-50 vision encoders for two different
checkpoints. Results are given as mean accuracy over 5 runs. Statistically significant results are
shown in bold. CLIP and CLOOB were trained for 31 epochs while CLIP* and CLOOB* were
trained for 128 epochs. In the majority of tasks CLOOB significantly outperforms CLIP.



|Dataset|CLIP CLOOB RN-50 RN-50|CLIP* CLOOB* RN-50 RN-50|
|---|---|---|


|Birdsnap Country211 Flowers102 GTSRB UCF101 Stanford Cars ImageNet ImageNet V2|2.26 ± 0.20 3.06 ± 0.30 0.67 0.11 0.67 0.05 ± ± 12.56 0.38 13.45 1.19 ± ± 7.66 1.07 6.38 2.11 ± ± 20.98 1.55 22.26 0.72 ± ± 0.91 ± 0.10 1.23 ± 0.10 20.33 ± 0.28 23.97 ± 0.15 20.24 ± 0.50 23.59 ± 0.15|2.8 ± 0.16 3.24 ± 0.31 0.7 0.04 0.73 0.05 ± ± 13.32 0.43 14.36 1.17 ± ± 8.96 1.70 7.03 1.22 ± ± 21.63 ± 0.65 23.03 ± 0.85 0.99 ± 0.16 1.41 ± 0.32 21.3 ± 0.42 25.67 ± 0.22 21.24 ± 0.22 25.49 ± 0.11|
|---|---|---|


4.1 CONCEPTUAL CAPTIONS PRETRAINING

**Pretraining dataset. The Conceptual Captions (CC) (Sharma et al., 2018) dataset consists of 2.9**
million images with high-quality captions. Images and their captions have been gathered via an
automated process from the web and therefore represent a wide variety of content. Raw descriptions
of images are collected from the alt-text HTML attribute. Both images and texts are filtered for high
quality image-text pairs.

**Methods compared. We compare our new CLOOB to CLIP (Radford et al., 2021). The CLOOB**
implementation is based on OpenCLIP (Ilharco et al., 2021), which achieves results equivalent to
CLIP on the YFCC dataset (see Section 4.2). OpenCLIP also reports results on the CC dataset.
As CLIP does not train models on CC we report results from this reimplementation as baseline.
Analogously to Radford et al. (2021, Section 2.4), we use the modified ResNet (He et al., 2016) and
BERT (Devlin et al., 2018; 2019) architectures to encode image and text input. We use the ResNet
encoders ResNet-50, ResNet-101, and ResNet-50x4.

**Hyperparameter selection and learning schedule. We use the hyperparameter values of OpenCLIP,**
concretely, a learning rate of 1 × 10[−][3] and a weight decay of 0.1 for the Adam optimizer (Kingma
et al., 2014) with decoupled weight decay regularization (Loshchilov & Hutter, 2019). Deviating
from OpenCLIP, we use a batch size of 512 due to computational restraints, which did not change the
performance. The learning rate scheduler for all experiments is cosine annealing with warmup and
hard restarts (Loshchilov & Hutter, 2017). We report the hyperparameter τ (default 0.07) from CLIP
as τ _[−][1]_ of 14.3 to be in the same regime as the hyperparameter β for the modern Hopfield networks.
The main hyperparameter search for CLOOB (also for YFCC pretraining in the next section) was
done with ResNet-50 as the vision encoder. Learnable τ _[−][1]_ in combination with the InfoLOOB loss
results in undesired learning behavior (see Appendix Section A.1.4). Therefore, we set τ _[−][1]_ to a fixed
value of 30, which was determined via hyperparameter search (see Appendix Section A.3.2). For
modern Hopfield networks,the hyperparameter β was set to 8. Further we scale the loss in Eq. (11)
with τ to remove the factor τ _[−][1]_ from the gradients (see Appendix Section A.1.4) resulting in the loss
function τ LInfoLOOB.

**Evaluation metrics: Zero-shot transfer learning. We evaluate and compare both CLIP and**
CLOOB on their zero-shot transfer learning capabilities on the following downstream image classification tasks. Birdsnap (Berg et al., 2014) contains images of 500 different North American bird
species. The Country211 (Radford et al., 2021) dataset consists of photos across 211 countries and
is designed to test the geolocalization capability of visual representations. Flowers102 (Nilsback &
Zisserman, 2008) is a dataset containing images of 102 flower species. GTSRB (Stallkamp et al.,
2011) contains images for classification of German traffic signs. UCF101 (Soomro et al., 2012) is a
video dataset with short clips for action recognition. For UCF101 we follow the procedure reported
in CLIP and extract the middle frame of every video to assemble the dataset. Stanford Cars (Krause
et al., 2013) contains images of 196 types of cars. ImageNet (Deng et al., 2009) is a large scale image
classification dataset with images across 1,000 classes. ImageNetv2 (Recht et al., 2019) consists of


-----

Table 2: Performance with InfoLOOB vs. InfoNCE objective and with vs. without Hopfield retrieval.
InfoLOOB increases the performance of CLIP in most of the tasks. Hopfield with InfoLOOB strongly
improves the performance in 7 out of 8 datasets compared to both CLIP models.

|Dataset|CLIP InfoNCE InfoLOOB|Hopfield InfoNCE InfoLOOB|
|---|---|---|


|Birdsnap Country211 Flowers102 GTSRB UCF101 Stanford Cars ImageNet ImageNetV2|1.94 2.37 0.62 0.63 13.04 13.03 7.28 4.39 21.00 19.14 0.90 1.33 20.31 22.13 20.63 21.65|1.67 2.53 0.54 0.76 11.53 14.24 5.76 5.86 20.56 22.29 1.24 1.37 19.04 24.21 18.97 23.80|
|---|---|---|



three new test sets with 10,000 images each for the ImageNet benchmark. For further details see
Appendix Section A.3.3.

**Results. We employ the same evaluation strategy and use the prompt templates as published in CLIP**
(see Appendix Section A.3.3). We report zero-shot results from two checkpoints in Table 1. CLIP and
CLOOB were trained for a comparable number of epochs used in CLIP (see Appendix Section A.3.2)
while CLIP* and CLOOB* were trained until evaluation performance plateaued (epoch 128). In both
cases CLOOB significantly outperforms CLIP on the majority of tasks or matches its performance.
Statistical significance of these results was assessed by an unpaired Wilcoxon test on a 5% level.

**Ablation studies. CLOOB has two new major components compared to CLIP: (1) the InfoLOOB**
objective instead of the InfoNCE objective and (2) the modern Hopfield networks. To assess which of
the new major components of CLOOB has led to the performance increase over CLIP, we performed
ablation studies on CC. First, we enhanced CLIP by replacing the InfoNCE objective with InfoLOOB.
Table 2 shows that the InfoLOOB objective increases the performance of CLIP in the majority of
the datasets. The reason is that InfoLOOB suffers less than InfoNCE from the “explaining away”
problem. However, InfoLOOB is more effective for higher mutual information, that is, for a richer
covariance structure. Hopfield networks amplify the covariance structure by retrieved embeddings.
For InfoLOOB, however, this amplification is disadvantageous as the saturation effect is increased
by higher similarity between anchor and positive. Thus, combining modern Hopfield networks with
InfoNCE leads to a performance drop. Combining Hopfield and InfoLOOB into CLOOB strongly
improves the performance on 7 out of 8 zero-shot transfer learning tasks. An additional ablation
considers the learning rate scheduler. For more details see in Appendix Section A.3.1.

4.2 YFCC PRETRAINING

**Pretraining dataset. To be comparable to the CLIP results, we use the same subset of 15 million**
samples from the YFCC100M dataset (Thomee et al., 2016) as in Radford et al. (2021), which we
refer to as YFCC. YFCC was created by filtering YFCC100M for images which contain natural
language descriptions and/or titles in English. It was not filtered by quality of the captions, therefore
the textual descriptions are less rich and contain superfluous information. The dataset with 400
million samples used to train the CLIP models in Radford et al. (2021) has not been released and, thus,
is not available for comparison. Due to limited computational resources we are unable to compare
CLOOB to CLIP on other datasets of this size.

**Methods compared and evaluation. In addition to the comparison of CLOOB and CLIP based on**
the OpenCLIP reimplementation (Ilharco et al., 2021), we include the original CLIP results (Radford
et al., 2021, Table 12).

**Hyperparameter selection. We use the hyperparameters selected at the Conceptual Captions dataset,**
except learning rate, batch size, and β. For modern Hopfield networks, the hyperparameter β is set
to 14.3, which is the default parameter of τ _[−][1]_ for the InfoNCE objective in Radford et al. (2021).
Furthermore, the learning rate is set to 5 × 10[−][4] and a batch size of 1024 as in OpenCLIP of Ilharco
et al. (2021). For further details see Appendix Section A.3.2.


-----

**Evaluation metrics. As in the previous experiment, methods are again evaluated at their zero-shot**
transfer learning capabilities on downstream tasks.

**Results. Table 3 provides results of the original CLIP and CLOOB trained on YFCC. The results**
on zero-shot downstream tasks show that CLOOB outperforms the results of CLIP on all 7 tasks
(ImageNet V2 results have not been reported in Radford et al. (2021)). Similarly, CLOOB outperforms
CLIP on 6 out of 7 tasks for linear probing. Results of the comparison of CLOOB an the CLIP
reimplementation of OpenCLIP are given in Table 4. CLOOB exceeds the CLIP reimplementation in
7 out of 8 tasks for zero-shot classification using ResNet-50 encoders. With larger ResNet encoders,
CLOOB outperforms CLIP on all tasks. Furthermore, the experiments with larger vision encoder
networks show that CLOOB performance increases with network size. Visualizations of predictions
of CLOOB zero-shot classifiers from all datasets are shown in Appendix Section A.3.4.

Table 3: Results of CLIP and CLOOB trained on YFCC with ResNet-50 encoder. Except for one
linear probing dataset, CLOOB consistently outperforms CLIP across all tasks.

|Dataset|Linear Probing CLIP CLOOB (OpenAI) (ours)|Zero-Shot CLIP CLOOB (OpenAI) (ours)|
|---|---|---|


|Birdsnap Country211 Flowers102 GTSRB UCF101 Stanford Cars ImageNet|47.4 56.2 23.1 20.6 94.4 96.1 66.8 78.9 69.2 72.3 31.4 37.7 62.0 65.7|19.9 28.9 5.2 7.9 48.6 55.1 6.9 8.1 22.9 25.3 3.8 4.1 31.3 35.7|
|---|---|---|


|ImageNet V2|- 58.7|- 34.6|
|---|---|---|



Table 4: Zero-shot results for the CLIP reimplementation and CLOOB using different ResNet
architectures trained on YFCC. CLOOB outperforms CLIP in 7 out of 8 tasks using ResNet-50
encoders. With larger ResNet encoders CLOOB outperforms CLIP on all tasks. The performance of
CLOOB scales with increased encoder size.




|Dataset|CLIP CLOOB RN-50 RN-50|CLIP CLOOB RN-101 RN-101|CLIP CLOOB RN-50x4 RN-50x4|
|---|---|---|---|


|Birdsnap Country211 Flowers102 GTSRB UCF101 Stanford Cars ImageNet ImageNet V2|21.8 28.9 6.9 7.9 48.0 55.1 7.9 8.1 27.2 25.3 3.7 4.1 34.6 35.7 33.4 34.6|22.6 30.3 7.8 8.5 48.0 55.3 7.4 11.6 28.6 28.8 3.8 5.5 35.3 37.1 34.1 35.6|20.8 32.0 8.1 9.3 50.1 54.3 9.4 11.8 31.0 31.9 3.5 6.1 37.7 39.0 35.9 37.3|
|---|---|---|---|


5 CONCLUSION

For constrastive learning, we have introduced “Contrastive Leave One Out Boost” (CLOOB), for
which modern Hopfield networks boost learning with the InfoLOOB objective. Modern Hopfield
networks both increase the stability of InfoLOOB and reinforce the covariance structure of the data.
We have shown theoretical properties of the InfoLOOB bound and objective. Our results suggest
InfoLOOB as an alternative to InfoNCE in contrastive learning. An ablation study shows that both,
the InfoLOOB objective and modern Hopfield networks, are necessary to yield high performance. At
seven zero-shot transfer learning tasks, the novel CLOOB is compared to CLIP after pretraining on
Conceptual Captions and the YFCC dataset. CLOOB consistently outperforms CLIP at zero-shot
transfer learning across all considered architectures and datasets.


-----

REPRODUCIBILITY STATEMENT

We will publish the source code after the reviewing period. This will ensure that the results are
reproducible in their entirety. The datasets used for training our models as well as for the downstream
tasks are publicly available.

ETHICAL CONSIDERATIONS

**Impact on ML and related scientific fields.** Our research has the potential to positively impact a
wide variety of fields of life due to its general applicability. Most importantly, it has the potential to
reduce the cost for training other AI systems, which could lead to a reduction of compute costs and
carbon dioxide emissions.

However, any new development in machine learning can be applied for good or for bad. Our system
can be used for medical applications where it could save lives but might also be used for surveillance
and malevolent systems.

**Impact on society.** A potential danger could arise from an application of our approach in which
users rely overly on the outcomes. For example, in a medical setting, physicians might rely on the
technical system and shift the liability towards the machine. This might also happen in the domain of
self-driving cars, when drivers start paying less attention to the traffic because of an AI-based driving
system. Finally, our method may also be deployed in companies to automate various simple tasks,
which might lead to a reduced need for particular jobs in production systems.

**Consequences of failures of the method.** Depending on the application area, a failure of this
method might be of lesser concern, such as a failed execution of a computer program. If our method is
employed within a larger automation system, a failure could result in damages such as a car accident
or errors of a production system. However, this holds for almost all machine learning methods, and
their usage and testing depends on the application area.

**Leveraging of biases in the data and potential discrimination.** Our proposed method relies
on human-annotated data and thereby human decisions, which are usually strongly biased. The
undesirable biases contained in dataset are learned and may propagate to downstream applications.
Therefore, the responsible use of our method depends on a careful selection of the training data and
awareness of the potential biases within those.

REFERENCES

L. F. Abbott and Y. Arian. Storage capacity of generalized networks. Phys. Rev. A, 36:5091–5094,
1987. doi: 10.1103/PhysRevA.36.5091.

S. Agarwal, G. Krueger, J. Clark, A. Radford, J. W. Kim, and M. Brundage. Evaluating CLIP: towards
characterization of broader capabilities and downstream implications. ArXiv, 2108.02818, 2021.

P. Baldi and S. S. Venkatesh. Number of stable points for spin-glasses and neural networks of higher
orders. Phys. Rev. Lett., 58:913–916, 1987. doi: 10.1103/PhysRevLett.58.913.

D. Bau, A. Andonian, A. Cui, Y Park, A. Jahanian, A. Oliva, and A. Torralba. Paint by word. arXiv
_preprint arXiv:2103.10951, 2021._

M. I. Belghazi, A. Baratin, S. Rajeswar, S. Ozair, Y. Bengio, A. Courville, and R. D. Hjelm. Mutual
information neural estimation. In J. Dy and A. Krause (eds.), Proceedings of the 35th International
_Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp._
531–540. PMLR, 2018.

T. Berg, J. Liu, S. W. Lee, M. L. Alexander, D. W. Jacobs, and P. N. Belhumeur. Birdsnap: Large-scale
fine-grained visual categorization of birds. In Proc. Conf. Computer Vision and Pattern Recognition
_(CVPR), pp. 2019–2026, 2014. doi: 10.1109/CVPR.2014.259._

R. Bommasani et al. On the opportunities and risks of foundation models. ArXiv, 2108.07258, 2021.


-----

Q. Cai, Y. Wang, Y. Pan, T. Yao, and T. Mei. Joint contrastive learning with infinite possibilities.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural
_Information Processing Systems, volume 33, pp. 12638–12648. Curran Associates, Inc., 2020._

B. Caputo and H. Niemann. Storage capacity of kernel associative memories. In Proceedings of the
_International Conference on Artificial Neural Networks (ICANN), pp. 51–56, Berlin, Heidelberg,_
2002. Springer-Verlag.

N. Carlini and A. Terzis. Poisoning and backdooring contrastive learning. ArXiv, 2106.09667, 2021.

H. H. Chen, Y. C. Lee, G. Z. Sun, H. Y. Lee, T. Maxwell, and C. Lee Giles. High order correlation
model for associative memory. AIP Conference Proceedings, 151(1):86–99, 1986. doi: 10.1063/1.
36224.

J. Chen, Z. Gan, X. Li, Q. Guo, L. Chen, S. Gao, T. Chung, Y. Xu, B. Zeng, W. Lu, F. Li, L. Carin, and
C. Tao. Simpler, faster, stronger: Breaking the log-K curse on contrastive learners with FlatNCE.
_arXiv, 2107.01152, 2021._

T. Chen, Y. Sun, Y. Shi, and L. Hong. On sampling strategies for neural network-based collaborative
filtering. In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge
_Discovery and Data Mining, pp. 767–776, New York, NY, USA, 2017. Association for Computing_
Machinery. doi: 10.1145/3097983.3098202.

T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning of
visual representations. In H. Daumé and A. Singh (eds.), Proceedings of the 37th International
_Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp._
1597–1607. PMLR, 2020.

X. Chen and K. He. Exploring simple siamese representation learning. In Proceedings of the
_IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 15750–15758,_
2021.

P. Cheng, W. Hao, S. Dai, J. Liu, Z. Gan, and L. Carin. CLUB: A contrastive log-ratio upper bound
of mutual information. In H. Daume and A. Singh (eds.), Proceedings of the 37th International
_Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp._
1779–1788. PMLR, 2020.

A. D’Amour et al. Underspecification presents challenges for credibility in modern machine learning.
_ArXiv, 2011.03395, 2020._

J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255.
Ieee, 2009.

B. Devillers, R. Bielawski, B. Choski, and R. VanRullen. Does language help generalization in vision
models? ArXiv, 2104.08313, 2021.

J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: pre-training of deep bidirectional
transformers for language understanding. ArXiv, 2018.

J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American
_Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume_
_1 (Long and Short Papers), pp. 4171–4186. Association for Computational Linguistics, 2019. doi:_
10.18653/v1/N19-1423.

R. J. Dickson and G. B. Gloor. Protein sequence alignment analysis by local covariation: coevolution
statistics detect benchmark alignment errors. PLoS One, 7(6):e37645, 2012. doi: 10.1371/journal.
pone.0037645.

H. Fang, P. Xiong, L. Xu, and Y. Chen. CLIP2Video: mastering video-text retrieval via image CLIP.
_ArXiv, 2106.11097, 2021._


-----

K. Frans, L. B. Soros, and O. Witkowski. CLIPDraw: exploring text-to-drawing synthesis through
language-image encoders. ArXiv, 2106.14843, 2021.

F. A. Galatolo, M. G. C. A. Cimino, and G. Vaglini. Generating images from caption and vice versa
via CLIP-guided generative latent space search. ArXiv, 2102.01645, 2021.

B. Gao and L. Pavel. On the properties of the softmax function with application in game theory and
reinforcement learning. ArXiv, 2017.

T. Gao, X. Yao, and D. Chen. SimCSE: simple contrastive learning of sentence embeddings. ArXiv,
2104.08821, 2021.

E. Gardner. Multiconnected neural network models. Journal of Physics A, 20(11):3453–3464, 1987.
doi: 10.1088/0305-4470/20/11/046.

R. Geirhos, J.-H. Jacobsen, C. Michaelis, R. S. Zemel, W. Brendel, M. Bethge, and F. A. Wichmann.
Shortcut learning in deep neural networks. ArXiv, 2004.07780, 2020.

J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. H. Richemond, E. Buchatskaya, C. Doersch, B. Ávila Pires,
Z. D. Guo, M. Gheshlaghi Azar, B. Piot, K. Kavukcuoglu, R. Munos, and M. Valko. Bootstrap your
own latent - a new approach to self-supervised learning. In H. Larochelle, M. Ranzato, R. Hadsell,
M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33,
pp. 21271–21284. Curran Associates, Inc., 2020.

M. Gutmann and A. Hyvärinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Y. W. Teh and M. Titterington (eds.), Proceedings of the Thirteenth
_International Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings of_
_Machine Learning Research, pp. 297–304. JMLR Workshop and Conference Proceedings, 2010._

T. Han, W. Xie, and A. Zisserman. Self-supervised co-training for video representation learning.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural
_Information Processing Systems, volume 33, pp. 5679–5690. Curran Associates, Inc., 2020._

K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings
_of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016._

K. He, H. Fan, Y. Wu, S. Xie, and R. B. Girshick. Momentum contrast for unsupervised visual
representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and
_Pattern Recognition (CVPR), 2020._

O. J. Hénaff, A. Srinivas, J. DeFauw, A. Razavi, C. Doersch, S. M. A. Eslami, and A. vanDenOord.
Data-efficient image recognition with contrastive predictive coding. ArXiv, 1905.09272, 2019.

M. L. Henderson, R. Al-Rfou, B. Strope, Y.-H. Sung, L. Lukács, R. Guo, S. Kumar, B. Miklos, and
R. Kurzweil. Efficient natural language response suggestion for smart reply. ArXiv, 1705.00652,
2017.

J. J. Hopfield. Neural networks and physical systems with emergent collective computational abilities.
_Proceedings of the National Academy of Sciences, 79(8):2554–2558, 1982._

J. J. Hopfield. Neurons with graded response have collective computational properties like those of
two-state neurons. Proceedings of the National Academy of Sciences, 81(10):3088–3092, 1984.
doi: 10.1073/pnas.81.10.3088.

D. Horn and M. Usher. Capacities of multiconnected memory models. J. Phys. France, 49(3):
389–395, 1988. doi: 10.1051/jphys:01988004903038900.

G. Ilharco, M. Wortsman, N. Carlini, R. Taori, A. Dave, V. Shankar, H. Namkoong, J. Miller,
H. Hajishirzi, A. Farhadi, and L. Schmidt. OpenCLIP, 2021.

D. P. Kingma, S. Mohamed, D .J. Rezende, and M. Welling. Semi-supervised learning with deep generative models. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger
(eds.), Advances in Neural Information Processing Systems 27, pp. 3581–3589. Curran Associates,
Inc., 2014.


-----

J. Krause, M. Stark, J. Deng, and L. Fei-Fei. 3D object representations for fine-grained categorization.
In 4th International IEEE Workshop on 3D Representation and Recognition (3dRR-13), 2013.

K. E. Kreth and A. A. Fodor. Covariance in protein multiple sequence alignments using groups of
columns. ArXiv, 1401.1141, 2014.

D. Krotov and J. J. Hopfield. Dense associative memory for pattern recognition. In D. D. Lee,
M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information
_Processing Systems, pp. 1172–1180. Curran Associates, Inc., 2016._

C. H. Lampert, H. Nickisch, and S. Harmeling. Learning to detect unseen object classes by betweenclass attribute transfer. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 951–958. IEEE, 2009.

S. Lapuschkin, S. Wäldchen, A. Binder, G. Montavon, W. Samek, and K.-R. Müller. Unmasking
Clever Hans predictors and assessing what machines really learn. Nature Communications, 10,
2019. doi: 10.1038/s41467-019-08987-4.

J. Li, P. Zhou, C. Xiong, R. Socher, and S. C. H. Hoi. Prototypical contrastive learning of unsupervised
representations. In International Conference on Learning Representations (ICLR), 2021. URL
[https://openreview.net/forum?id=KmykpuSrjcq. ArXiv 2005.04966.](https://openreview.net/forum?id=KmykpuSrjcq)

L. Logeswaran and H. Lee. An efficient framework for learning sentence representations. In
_[Sixth International Conference on Learning Representations (ICLR), 2018. URL https://](https://openreview.net/forum?id=rJvJXZb0W)_
[openreview.net/forum?id=rJvJXZb0W. ArXiv 1803.02893.](https://openreview.net/forum?id=rJvJXZb0W)

I. Loshchilov and F. Hutter. SGDR: stochastic gradient descent with warm restarts. In 5th
_International Conference on Learning Representations ICLR. OpenReview.net, 2017._ URL

[https://openreview.net/forum?id=Skq89Scxx.](https://openreview.net/forum?id=Skq89Scxx)

I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In International Conference
_[on Learning Representations (ICLR), 2019. URL https://openreview.net/forum?id=](https://openreview.net/forum?id=Bkg6RiCqY7)_
[Bkg6RiCqY7.](https://openreview.net/forum?id=Bkg6RiCqY7)

H. Luo, L. Ji, M. Zhong, Y. Chen, W. Lei, N. Duan, and T. Li. CLIP4Clip: an empirical study of
CLIP for end to end video clip retrieval. ArXiv, 2104.08860, 2021.

D. McAllester and K. Stratos. Formal limitations on the measurement of mutual information. ArXiv,
1811.04251, 2018.

D. McAllester and K. Stratos. Formal limitations on the measurement of mutual information. In Silvia
Chiappa and Roberto Calandra (eds.), Proceedings of the Twenty Third International Conference
_on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning Research,_
[pp. 875–884. PMLR, 26–28 Aug 2020. URL https://proceedings.mlr.press/v108/](https://proceedings.mlr.press/v108/mcallester20a.html)
[mcallester20a.html.](https://proceedings.mlr.press/v108/mcallester20a.html)

T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words
and phrases and their compositionality. In C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani,
and K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems, volume 26, pp.
3111–3119. Curran Associates, Inc., 2013.

T. Milbich, K. Roth, S. Sinha, L. Schmidt, M. Ghassemi, and B. Ommer. Characterizing generalization
under out-of-distribution shifts in deep metric learning. ArXiv, 2107.09562, 2021.

J. Miller, R. Taori, A. Raghunathan, S. Sagawa, P. W. Koh, V. Shankar, P. Liang, Y. Carmon, and
L. Schmidt. Accuracy on the line: On the strong correlation between out-of-distribution and
in-distribution generalization. ArXiv, 2107.04649, 2021.

I. Misra and L. vanDerMaaten. Self-supervised learning of pretext-invariant representations. In
_Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),_
2020.

M. Narasimhan, A. Rohrbach, and T. Darrell. CLIP-It! language-guided video summarization. ArXiv,
2107.00650, 2021.


-----

X. Nguyen, M. J. Wainwright, and M. Jordan. Estimating divergence functionals and the likelihood
ratio by penalized convex risk minimization. IEEE Transactions on Information Theory, 56(11):
5847–5861, 2010. doi: 10.1109/tit.2010.2068870.

M.-E. Nilsback and A. Zisserman. Automated flower classification over a large number of classes.
In Proceedings of the 2008 Sixth Indian Conference on Computer Vision, Graphics and Image
_Processing, pp. 722–729. IEEE Computer Society, 2008. doi: 10.1109/ICVGIP.2008.47._

F. W. J. Olver, D. W. Lozier, R. F. Boisvert, and C. W. Clark. NIST handbook of mathematical
_functions. Cambridge University Press, 1 pap/cdr edition, 2010. ISBN 9780521192255._

D. Pakhomov, S. Hira, N. Wagle, K. E. Green, and N. Navab. Segmentation in style: Unsupervised
semantic image segmentation with stylegan and CLIP. ArXiv, 2107.12518, 2021.

B. Poole, S. Ozair, A. vanDenOord, A. A. Alemi, and G. Tucker. On variational bounds of mutual
information. In K. Chaudhuri and R. Salakhutdinov (eds.), Proceedings of the 36th International
_Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp._
5171–5180. PMLR, 2019.

D. Psaltis and H. P. Cheol. Nonlinear discriminant functions and associative memories. AIP
_Conference Proceedings, 151(1):370–375, 1986. doi: 10.1063/1.36241._

A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,
J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from natural language
supervision. In Proceedings of the 38th International Conference on Machine Learning (ICML),
2021.

H. Ramsauer, B. Schäfl, J. Lehner, P. Seidl, M. Widrich, L. Gruber, M. Holzleitner, M. Pavlovi´c, G. K.
Sandve, V. Greiff, D. Kreil, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter. Hopfield
networks is all you need. ArXiv, 2008.02217, 2020.

H. Ramsauer, B. Schäfl, J. Lehner, P. Seidl, M. Widrich, L. Gruber, M. Holzleitner, M. Pavlovi´c, G. K.
Sandve, V. Greiff, D. Kreil, M. Kopp, G. Klambauer, J. Brandstetter, and S. Hochreiter. Hopfield
networks is all you need. In 9th International Conference on Learning Representations (ICLR),
[2021. URL https://openreview.net/forum?id=tL89RnzIiCd.](https://openreview.net/forum?id=tL89RnzIiCd)

B. Recht, R. Roelofs, L. Schmidt, and V. Shankar. Do ImageNet classifiers generalize to ImageNet?
In K. Chaudhuri and R. Salakhutdinov (eds.), Proceedings of the 36th International Conference
_on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 5389–5400._
PMLR, 2019.

P. Seidl, P. Renz, N. Dyubankova, P. Neves, J. Verhoeven, J. K. Wegner, S. Hochreiter, and G. Klambauer. Modern hopfield networks for few- and zero-shot reaction prediction. ArXiv, 2104.03279,
2021.

P. Sharma, N. Ding, S. Goodman, and R. Soricut. Conceptual captions: A cleaned, hypernymed,
image alt-text dataset for automatic image captioning. In Proceedings of ACL, 2018.

S. Shen, L. H. Li, H. Tan, M. Bansal, A. Rohrbach, K.-W. Chang, Z. Yao, and K. Keutzer. How much
can CLIP benefit vision-and-language tasks? ArXiv, 2107.06383, 2021.

K. Soomro, A. R. Zamir, and M. Shah. A dataset of 101 human action classes from videos in the
wild. Center for Research in Computer Vision, 2(11), 2012.

J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. The German traffic sign recognition benchmark:
A multi-class classification competition. The 2011 International Joint Conference on Neural
_Networks, pp. 1453–1460, 2011._

R. Taori, A. Dave, V. Shankar, N. Carlini, B. Recht, and L. Schmidt. Measuring robustness to
natural distribution shifts in image classification. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
18583–18599. Curran Associates, Inc., 2020.


-----

B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth, and L.-J. Li.
YFCC100M: The new data in multimedia research. Commun. ACM, 59(2):64–73, 2016. doi:
10.1145/2812802.

Y.-H. H. Tsai, M. Q. Ma, H. Zhao, K. Zhang, L.-P. Morency, and R. Salakhutdinov. Conditional
contrastive learning: Removing undesirable information in self-supervised representations. ArXiv,
2106.02866, 2021.

M. Tschannen, J. Djolonga, P. K. Rubenstein, S. Gelly, and M. Lucic. On mutual information maximization for representation learning. _arXiv, 1907.13625, 2019._ [URL https:](https://openreview.net/forum?id=rkxoh24FPH)
[//openreview.net/forum?id=rkxoh24FPH. 8th International Conference on Learning](https://openreview.net/forum?id=rkxoh24FPH)
Representations (ICLR).

A. van den Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding.
_ArXiv, 1807.03748, 2018._

M. J. Wainwright. Basic tail and concentration bounds, pp. 21–57. Cambridge Series in Statistical and
Probabilistic Mathematics. Cambridge University Press, 2019. doi: 10.1017/9781108627771.002.

F. Wang and H. Liu. Understanding the behaviour of contrastive loss. In Proceedings of the IEEE/CVF
_Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2495–2504, 2021._

T. Wang and P. Isola. Understanding contrastive representation learning through alignment and
uniformity on the hypersphere. In Proceedings of the 37th International Conference on Machine
_Learning (ICML), 2020._

M. P. Wellman and M. Henrion. Explaining ’explaining away’. IEEE Trans. Pattern Anal. Mach.
_Intell., 15(3):287–292, 1993. doi: 10.1109/34.204911._

M. Widrich, B. Schäfl, M. Pavlovi´c, H. Ramsauer, L. Gruber, M. Holzleitner, J. Brandstetter, G. K.
Sandve, V. Greiff, S. Hochreiter, and G. Klambauer. Modern Hopfield networks and attention for
immune repertoire classification. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and
H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 18832–18845.
Curran Associates, Inc., 2020.

M. Wortsman, G. Ilharco, M. Li, J. W. Kim, H. Hajishirzi, A. Farhadi, H. Namkoong, and L. Schmidt.
Robust fine-tuning of zero-shot models. ArXiv, 2109.01903, 2021.

M. Wu, M. Mosse, C. Zhuang, D. Yamins, and N. Goodman. Conditional negative sampling for
contrastive learning of visual representations. In International Conference on Learning Represen_[tations (ICLR), 2021. URL https://openreview.net/forum?id=v8b3e5jN66j.](https://openreview.net/forum?id=v8b3e5jN66j)_

Z. Wu, Y. Xiong, S. X. Yu, and D. Lin. Unsupervised feature learning via non-parametric instance
discrimination. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition
_(CVPR), pp. 3733–3742, Los Alamitos, CA, USA, 2018. IEEE Computer Society. doi: 10.1109/_
CVPR.2018.00393.

C.-H. Yeh, C.-Y. Hong, Y.-C. Hsu, T.-L. Liu, Y. Chen, and Y. LeCun. Decoupled contrastive learning.
_ArXiv, 2110.06848, 2021._

K. Zhou, J. Yang, C. C. Loy, and Z. Liu. Learning to prompt for vision-language models. ArXiv,
2109.01134, 2021.


-----

A APPENDIX

This appendix consists of four sections (A.1–A.4). Section A.1 provides the theoretical properties
of the InfoLOOB and InfoNCE. It is shown how to derive that InfoNCE is a lower bound on
mutual information. Further it is shown how to derive that InfoLOOB is an upper bound on mutual
information. The proposed loss function LInfoLOOB and its gradients are discussed. In Section A.2
we discuss the estimation of mutual information for a toy example. Section A.3 provides details on
the experiments for Section 4. Section A.4 briefly reviews continuous modern Hopfield networks.
Section A.5 discusses further related work.

CONTENTS OF THE APPENDIX

A Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

A.1 InfoLOOB vs. InfoNCE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

A.1.1 InfoNCE: Lower Bound on Mutual Information . . . . . . . . . . . . . . . . 17

A.1.2 InfoLOOB: Upper Bound on Mutual Information . . . . . . . . . . . . . . . . 21

A.1.3 InfoLOOB: Analysis of the Objective . . . . . . . . . . . . . . . . . . . . . . 25

A.1.4 InfoNCE and InfoLOOB: Gradients . . . . . . . . . . . . . . . . . . . . . . 32

A.1.5 InfoLOOB and InfoNCE: Probability Estimators . . . . . . . . . . . . . . . . 34

A.1.6 InfoLOOB and InfoNCE: Losses . . . . . . . . . . . . . . . . . . . . . . . . 36

A.2 Mutual Information Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

A.3 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

A.3.1 Ablation studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39

A.3.2 Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41

A.3.3 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41

A.3.4 Zero-shot evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

A.3.5 Linear probing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42

A.4 Review of Modern Hopfield Networks . . . . . . . . . . . . . . . . . . . . . . . . . 43

A.5 Further Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46

## LIST OF THEOREMS


A1 Theorem (InfoNCE lower bound) . . . . . . . . . . . . . . . . . . . . . . . . . . . 19

A2 Theorem (InfoLOOB upper bound) . . . . . . . . . . . . . . . . . . . . . . . . . . 23

A3 Theorem (Weighted Covariances) . . . . . . . . . . . . . . . . . . . . . . . . . . 38

A4 Theorem (Modern Hopfield Networks: Retrieval with One Update) . . . . . . . . . 45

A5 Theorem (Modern Hopfield Networks: Exponential Storage Capacity) . . . . . . . 45


## LIST OF DEFINITIONS

A1 Definition (Pattern Stored and Retrieved) . . . . . . . . . . . . . . . . . . . . . . . 45

## LIST OF FIGURES

A1 Estimated mutual information of different objectives . . . . . . . . . . . . . . . . . 40

A2 Visualization of zero-shot classification of three examples from each dataset . . . . . 44

## LIST OF TABLES

A1 Influence of loss functions and Hopfield retrieval . . . . . . . . . . . . . . . . . . . 40

A2 Influence of learning rate scheduler . . . . . . . . . . . . . . . . . . . . . . . . . . . 41

A3 Datasets used for zero-shot and linear probing . . . . . . . . . . . . . . . . . . . . . . 41

A4 Linear probing for CLIP (reimplementation) and CLOOB trained on YFCC . . . . . 43


-----

A.1 INFOLOOB VS. INFONCE

A.1.1 INFONCE: LOWER BOUND ON MUTUAL INFORMATION

We derive a lower bound on the mutual information between random variables X and Y distributed
according to p(x, y). The mutual information I(X ; Y ) between random variables X and Y is


_p(x, y)_
ln

_p(x) p(y)_


ln _[p][(][x][ |][ y][)]_

_p(x)_




ln _[p][(][y][ |][ x][)]_

_p(y)_


I(X ; Y ) = Ep(x,y)


= Ep(x,y)


= Ep(x,y)


(A1)


“InfoNCE” has been introduced in van den Oord et al. (2018) and is a multi-sample bound. In
the setting introduced in van den Oord et al. (2018), we have an anchor sample y given. For the
anchor sampleX˜ = **_x2, . . ., x yN we draw a positive sample according to p( X[˜]_** ), which are x1 according to n 1 negative samples drawn iid according to p(x1 | y). Next, we draw a set
_{_ _}_ _−_
_p(x). We have drawn a set X =_ **_x1, x2, . . ., xN_** according to p(X **_y), which is one positive_**
_{_ _}_ _|_
samplep(x). **_x1 drawn by p(x1 | y) and N −_** 1 negative samples {x2, . . ., xN _} drawn iid according to_

The InfoNCE with probabilities is

_p(y_ **_x1)_**

IInfoNCE(X1 ; Y ) = Ep(y) Ep(X|y) ln 1 _N_ _|_ _,_ (A2)

" " _N_ _i=1_ _[p][(][y][ |][ x][i][)]_ !##

P

where we inserted the factor _N[1]_ [in contrast to the original version in][ van den Oord et al.][ (][2018][), where]

we followed Poole et al. (2019); Tschannen et al. (2019); Cheng et al. (2020); Chen et al. (2021).


IInfoNCE(X1 ; Y ) = Ep(y)


Ep(X|y)


ln


The InfoNCE with score function f (x, y) is

IInfoNCE(X1 ; Y ) = Ep(y) Ep(X **_y)_**

_|_

"


!##


_f_ (x1, y)

_N_
_i=1_ _[f]_ [(][x][i][,][ y][)]
P


(A3)

(A4)


ln

ln

"


The InfoNCE with probabilities can be rewritten as:


!##


_p(y_ **_x1)_**

_N_ _|_
_i=1_ _[p][(][y][ |][ x][i][)]_
P




IInfoNCE(X1 ; Y ) = Ep(y)


Ep(X|y)

"


_p(y|x1)_

_p(y)_

= Ep(y) Ep(X|y) ln  _N1_ _Ni=1_ _p(py(|yx)i)_ 

   Pp(x1|y) 

= Ep(y) Ep(X|y) ln  _N1_ _pNi=1(x1)pp(x(xi|iy))_  _._

   

P

This is the InfoNCE with f (x, y) = p(y | x).

**Set of pairs. The InfoNCE can be written in a different setting Poole et al. (2019), which is**
used in most implementations. We sample N pairs independently from p(x, y), which gives Z =
(x1, y1), (x2, y2), . . ., (xN _, yN_ ) . The InfoNCE is then
_{_ _}_


_f_ (xi, yi)

_N_
_j=1_ _[f]_ [(][x][j][,][ y][i][)] !#
P


(A5)


IInfoNCE(X ; Y ) = Ep(X|y)


ln

_i=1_

X


-----

Following van den Oord et al. (2018) we have

_p(y|x1)_

_p(y)_

IInfoNCE(X1 ; Y ) = Ep(y) Ep(X|y) ln  _N1_ _Ni=1_ _p(py(|yx)i)_  (A6)

   Pp(x1|y) 

= Ep(y) Ep(X|y) ln  _N1_ _pNi=1(x1)pp(x(xi|iy))_ 

   

_pP(x1_ **_y)_** _l=2_ _[p][(][x][l][)]_

= Ep(y) Ep(X|y) ln _N_ _|_ + ln(N )

" " _i=1_ _[p][(][x][i][ |][ y][)][ Q]l=i_ _[p][(][x][l][)]_ !##

_̸_

[Q][N]

= Ep(y) Ep(X **_y) [ln p(i = 1P_** _X, y)]_ + ln(N ),
_|_ _|_

where p(i = 1 | X, y) is the probability that sample **_x1 is the positive sample if we know there exists_**
exactly one positive sample in X.

The InfoNCE is a lower bound on the mutual information. The following inequality is from van den
Oord et al. (2018):

_p(x1_ **_y)_**

I(X1 ; Y ) = Ep(y) Ep(x1 **_y)_** ln _|_ (A7)

_|_ _p(x1)_
   

_p(x1)_

= Ep(y) Ep(x1|y) _−_ ln _p(x1_ **_y)_**

   _|_ 

1 _p(x1)_

_≥_ Ep(y) Ep(x1|y) _−_ ln _N_ + _p(x1_ **_y)_**

   _|_ 

_N_

1 _p(x1)_ _p(xi_ **_y)_**

Ep(y) Ep(X **_y)_** ln _|_
_≈_ " _|_ "− _N_ [+ 1]N _p(x1_ **_y)_** _i=2_ _p(xi)_ !##

_|_ X

_p(x1|y)_

= Ep(y) Ep(X|y) ln  _N1_ _pp(x(x11|y))_ +p(Nx1 1) _Ni=2_ _pp(x(xi|iy))_ 

= IInfoNCE (X1 ; Y ),  P 

where the "tight, since a≥ =" is obtained by boundingpp(x(x11y)) [can become small. However for the "] ln(1/N + a) by ln(a)[≈], which gives a bound that is not very["][ van den Oord et al.][ (][2018][) have to]

_|_
assume


_p(xi_ **_y)_**
_|_

_p(xi)_


_p(y_ **_xi)_**
_|_ 1, (A8)

_p(y)_ _≥_


_i=2_

which is unclear how to ensure.


_i=2_


For a proof of this bound see Poole et al. (2019).

We assumed that for the anchor sample y a positive sample x1 has been drawn according to p(x1 | y).
A set _X[˜] =_ **_x2, . . ., xN_** of negative samples is drawn according to p(x). Therefore, we have a
_{_ _}_
set X = {x1, x2, . . ., xN _} that is drawn with one positive sample x1 and N −_ 1 negative samples
_X˜ =_ **_x2, . . ., xN_** . We have
_{_ _}_


_p( X[˜]_ ) =


_p(xi),_ (A9)
_i=2_

Y


_p(xi),_ (A10)
_i=2_

Y


_p(X_ **_y) = p(x1_** **_y)_**
_|_ _|_


_p(xi) ._ (A11)
_i=1_

Y


_p(X) =_


-----

Next, we present a theorem that shows this bound, where we largely follow Poole et al. (2019) in the
proof. In contrast to Poole et al. (2019), we do not use the NWJ bound Nguyen et al. (2010). The
mutual information is


_p(x1_ **_y)_**
ln _|_

_p(x1)_




_._ (A12)



I(X1 ; Y ) = Ep(x1,y)


**Theorem A1 (InfoNCE lower bound). InfoNCE with score function f** (x, y) according to Eq. (A3)
_is a lower bound on the mutual information._


!#


_f_ (x1, y)

I(X1 ; Y ) ≥ Ep(y)p(X|y) "ln _N1_ _Ni=1_ _[f]_ [(][x][i][,][ y][)] !# = IInfoNCE(X1 ; Y ) . (A13)

P

_InfoNCE with probabilities according to Eq. (A2) is a lower bound on the mutual information._

_p(y_ **_x1)_**

I(X1 ; Y ) ≥ Ep(y)p(X|y) "ln _N1_ _Ni=1 |[p][(][y][ |][ x][i][)]_ !# = IInfoNCE(X1 ; Y ) . (A14)

P

_The second bound Eq. (A14) is a special case of the first bound Eq. (A13)._


I(X1 ; Y ) Ep(y)p(X **_y)_**
_≥_ _|_


ln


_Proof. Part (I): Lower bound with score function f_ (x, y).

For each set _X[˜] =_ **_x2, . . ., xN_**, we define as data-dependent (depending on _X[˜]_ ) score function
_{_ _}_
_g(x1, y,_ _X[˜]_ ) that is based on the score function f (x, y). Therefore we have for each _X[˜] a different_
data-dependent score function g based on f . We will derive a bound on the InfoNCE, which is the
expectation of a lower bond on the mutual information over the score functions. For score function
_g(x1, y,_ _X[˜]_ ), we define a variational distribution q(x1 **_y,_** _X[˜]_ ) over x1:
_|_

_X)_
_q(x1_ **_y,_** _X[˜]_ ) = _[p][(][x][1][)][ g][(][x][1][,][ y][,][ ˜]_ _,_ (A15)
_|_ _Z(y,_ _X[˜]_ )

_Z(y,_ _X[˜]_ ) = Ep(x1) _g(x1, y,_ _X[˜]_ ) _,_ (A16)
h i

which ensures

_q(x1 | y,_ _X[˜]_ ) dx1 = 1 . (A17)
Z

We have

_q(x1_ **_y,_** _X[˜]_ ) _X)_
_|_ = _[g][(][x][1][,][ y][,][ ˜]_ _._ (A18)

_p(x1)_ _Z(y,_ _X[˜]_ )


For the function g, we set

For the function f we use


_f_ (x1, y)
_g(x1, y,_ _X[˜]_ ) = 1 _N_ _,_ (A19)

_N_ _i=1_ _[f]_ [(][x][i][,][ y][)]
P


_f_ (x1, y) = exp(τ sim(x1, y)), (A20)

_[−][1]_

where sim(x, y) is typically the cosine similarity.

We next show that InfoNCE is a lower bound on the mutual information.


-----

(A21)



I(X1 ; Y ) = Ep( ˜X) [[I(][X][1][ ;][ Y][ )] = E]p( X[˜] ) Ep(x1,y) ln _[p][(][x][1][ |][ y][)]_ (A21)

_p(x1)_

  

_p(x1_ **_y)_** _q(x1_ **_y,_** _X[˜]_ )

= Ep( ˜X) "Ep(x1,y) "ln _q(x1_ **_y |_** _,_ _X[˜]_ ) _p( |x1)_ !##

_|_

_X)_

= Ep( ˜X) "Ep(x1,y) "ln _[q][(][x]p[1]([ |]x[ y]1)[,][ ˜]_ # + Ep(y) KL(p(x1 | y) ∥ _q(x1 | y,_ _X[˜]_ ))

h i[#]

_X)_ _X)_

_≥_ Ep( ˜X) "Ep(x1,y) "ln _[q][(][x]p[1]([ |]x[ y]1)[,][ ˜]_ ## = Ep( ˜X) "Ep(x1,y) "ln _[g][(]Z[x]([1]y[,][ y],_ _X[,][˜][ ˜])_ ##

= Ep( ˜X) Ep(x1,y) ln g(x1, y, _X[˜]_ ) ln Ep(x1) _g(x1, y,_ _X[˜]_ )
_−_
h h  h iii

= Ep( ˜X) Ep(y) Ep(x1|y) ln g(x1, y, _X[˜]_ ) _−_ ln Ep(x1) _g(x1, y,_ _X[˜]_ )
h h h i  h iii

= Ep( ˜X) Ep(y) Ep(x1|y) ln g(x1, y, _X[˜]_ ) _−_ Ep( ˜X) Ep(y) ln Ep(x1) _g(x1, y,_ _X[˜]_ )
h h h iii h h  h iii

_≥_ Ep(y)p(X|y) ln g(x1, y, _X[˜]_ ) _−_ Ep( ˜X) Ep(y) Ep(x1) _g(x1, y,_ _X[˜]_ ) _−_ 1
h i h h h i ii

_f_ (x1, y) _f_ (x1, y)

= Ep(y)p(X|y) "ln _N1_ _Ni=1_ _[f]_ [(][x][i][,][ y][)] # _−_ Ep(y) "Ep(X) " _N1_ _Ni=1_ _[f]_ [(][x][i][,][ y][)] # _−_ 1#

_N_

Pf (x1, y) 1 P _f_ (xi, y)

= Ep(y)p(X|y) "ln _N1_ _Ni=1_ _[f]_ [(][x][i][,][ y][)] # _−_ Ep(y) " _N_ _i=1_ Ep(X) " _N1_ _Ni=1_ _[f]_ [(][x][i][,][ y][)] # _−_ 1#

X

Pf (x1, y) _N1_ _Ni=1_ _[f]_ [(]P[x][i][,][ y][)]

= Ep(y)p(X|y) "ln _N1_ _Ni=1_ _[f]_ [(][x][i][,][ y][)] # _−_ Ep(y) "Ep(X) " _N1_ PNi=1 _[f]_ [(][x][i][,][ y][)] # _−_ 1#

Pf (x1, y) P

= Ep(y)p(X|y) ln 1 _N_

" _N_ _i=1_ _[f]_ [(][x][i][,][ y][)] #

= IInfoNCE(X1 ; Y ) . P


I(X1 ; Y ) = Ep( ˜X) [[I(][X][1][ ;][ Y][ )] = E]p( X[˜] )


Ep(x1,y)


For the first "≥" we used that the Kullback-Leibler divergence is non-negative. For the second "≥"
we used the inequality ln a ⩽ _a −_ 1 for a > 0.

**Part (II): Lower bound with probabilities.**


If the score function f is

then the bound is

I(X1 ; Y ) Ep(y)p(X **_y)_**
_≥_ _|_

= Ep(y)p(X|y)


_f_ (x, y) = p(y | x), (A22)


!#


_p(y_ **_x1)_**

_N_ _|_
_i=1_ _[p][(][y][ |][ x][i][)]_ !#

(A23)

P


_f_ (x1, y)

_N_
_i=1_ _[f]_ [(][x][i][,][ y][)]
P


ln

"

ln






= Ep(y)p(X|y)


ln


_p(y|x1)_

_p(y)_

= Ep(y)p(X|y) ln  _N1_ _Ni=1_ _p(py(|yx)i)_

 

P

This is the bound with probabilities in the theorem.


= IInfoNCE(X1 ; Y ) .






-----

A.1.2 INFOLOOB: UPPER BOUND ON MUTUAL INFORMATION

We derive an upper bound on the mutual information between random variables X and Y distributed
according to p(x, y). The mutual information I(X ; Y ) between random variables X and Y is


_p(x, y)_
ln

_p(x) p(y)_


ln _[p][(][x][ |][ y][)]_

_p(x)_




ln _[p][(][y][ |][ x][)]_

_p(y)_


I(X ; Y ) = Ep(x,y)


= Ep(x,y)


= Ep(x,y)


(A24)


In Poole et al. (2019) Eq. (13) introduces a variational upper bound on the mutual information,
which has been called "Leave one out upper bound" (called "L1Out" in Cheng et al. (2020)). For
simplicity, we call this bound "InfoLOOB", where LOOB is an acronym for "Leave One Out Bound".
In contrast to InfoNCE, InfoLOOB is an upper bound on the mutual information. InfoLOOB is analog
to InfoNCE except that the negative samples do not contain a positive sample. Fig. 1 and Fig. 2 in
Cheng et al. (2020) both show that InfoLOOB is a better estimator for the mutual information than
InfoNCE (van den Oord et al., 2018), MINE (Belghazi et al., 2018), and NWJ (Nguyen et al., 2010).


The InfoLOOB with score function f (x, y) is defined as


!##

!##


_f_ (x1, y)

1 _N_

_N_ 1 _i=2_ _[f]_ [(][x][i][,][ y][)]
_−_
P

_p(y_ **_x1)_**

1 _N |_

_N_ 1 _i=2_ _[p][(][y][ |][ x][i][)]_
_−_
P


(A25)

(A26)

(A27)


IInfoLOOB(X1 ; Y ) = Ep(y)


Ep˜(X|y)


ln

ln


The InfoLOOB with probabilities is defined as


IInfoLOOB(X1 ; Y ) = Ep(y)


Ep(X|y)


This is the InfoLOOB with f (x, y) = p(y | x).

The InfoLOOB with probabilities can be written in different forms:


_p(y_ **_x1)_**

IInfoLOOB(X1 ; Y ) = Ep(y) Ep(X|y) ln 1 _N |_ (A27)

" " _N_ 1 _i=2_ _[p][(][y][ |][ x][i][)]_ !##

_−_

= Ep(y) Ep(X|y) ln  _N1−1_ _p(pyNi(=2|yx)1)p(py(|yx)i)_  P= Ep(y) Ep(X|y) ln  _N1−1_ _pp(x(Nix=211|y))pp(x(xi|iy))_

      

P P

**Set of pairs. The InfoLOOB can we written in a different setting (Poole et al., 2019), which will**
be used in our implementations. We sample N pairs independently from p(x, y), which gives
_X =_ (x1, y1), (x2, y2), . . ., (xN _, yN_ ) . The InfoLOOB is then
_{_ _}_

_N_

1 _f_ (xi, yi)

IInfoLOOB(X ; Y ) = Ep(X|y) _N_ ln 1 _N_ _._ (A28)

" _i=1_ _N_ 1 _j=1,j=i_ _[f]_ [(][x][j][,][ y][i][)] !#

X _−_ _̸_

We assume that an anchor sample y is given. For the anchor sampleP **_y we draw a positive sample_**
**_xto1 ˜p according to(x | y). For a given p(x1 | y y). Next, we draw a set, the x that have a largeX[˜] = p {(xx2 |, . . ., y) are drawn with a lower probability xN_** _} of negative samples according_
_p˜(x | y) compared to random drawing via p(x). The negatives are indeed negatives. We have_
drawn first anchor sample y and then X = {x1, . . ., xN _}, where x1 is drawn according to p(x1 | y)_
and _X[˜] =_ **_x2, . . ., xN_** are drawn iid according to ˜p(x **_y). We have_**
_{_ _}_ _|_


IInfoLOOB(X1 ; Y ) = Ep(y)


Ep(X|y)


ln


_._






_p˜( X[˜] | y) =_


_p˜(xi_ **_y),_** (A29)
_i=2_ _|_

Y


_p˜(xi_ **_y),_** (A30)
_i=2_ _|_

Y


_p˜(X_ **_y) = p(x1_** **_y)_**
_|_ _|_


_p˜( X[˜]_ **_y) p(x1) = p(x1)_**
_|_


_p˜(xi_ **_y) ._** (A31)
_i=2_ _|_

Y


-----

We assume for score function f (x, y)

_∀y∀x : 0 < f_ (x, y) . (A32)

We ensure this by using for score function f

_f_ (x, y) = exp(τ _[−][1]_ sim(x, y)), (A33)

where sim(x, y) is typically the cosine similarity.


InfoLOOB with score function f (x, y) is

IInfoLOOB(X ; Y ) = Ep(y)


_f_ (x1, y)

IInfoLOOB(X ; Y ) = Ep(y) Ep(X|y) ln 1 _N_ _._ (A34)

" " _N_ 1 _i=2_ _[f]_ [(][x][i][,][ y][)] !##

_−_
P

The reference constant Z(y) gives the average score f (x, y), if the negatives for y are selected with
lower probability via ˜p(x | y) than with random drawing according to p(x).

_Z(y) = Ep˜(x_ **_y)_** [[][f] [(][x][,][ y][)]][ .] (A35)
_|_


Ep(X|y)


ln


We define the variational distribution

_q(x_ **_y) =_** _[p][(][x][)][ f]_ [(][x][,][ y][)] _,_ _Z_ _[∗](y) = Ep(x) [f_ (x, y)] . (A36)
_|_ _Z_ _[∗](y)_

With the variational distribution q(x | y), we express our main assumption. The main assumption
**for the bound is:**

Ep(y) [KL(p(x | y) ∥ _q(x | y))] ⩽_ Ep(y) [ln Z _[∗](y) −_ ln Z(y)] . (A37)

This assumption can be written as

_p(y_ **_x) Z(y)_**

Ep(y) Ep(x **_y)_** ln _|_ ⩽ 0 . (A38)

_|_ _p(y) f_ (x, y)
   

This assumption ensures that the x with large p(x | y)) are selected with lower probability via
_p˜(x | y) than with random drawing according to p(x). The negatives are ensured to be real negatives,_
that is, p(x | y) is small and so is f (x, y). Consequently, we make sure that we draw x with
sufficient small f (x, y). The Kullback-Leibler gives the minimal required gap between drawing
_f_ (x, y) via p(x) and drawing f (x, y) via ˜p(x | y).

**EXAMPLE. With h(y) > 0, we consider the setting**

_f_ (x, y) = _[p][(][y][ |][ x][)][ h][(][y][)]_ _,_ (A39)

_p(y)_

1[#]

_p(x) p(y)_ _p(y_ **_x) h(y)_** _−_
_p˜(x | y) =_ _h(y) p(y_ **_x) C(y)_** _[,]_ _C(y) = Ep(x)_ _|p(y)_ _._ (A40)

_|_ " 

The main assumption becomes

Ep(y) Ep(x **_y)_** ln _[Z][(][y][)]_ ⩽ 0 . (A41)

_|_ _h(y)_
  


The main assumption holds since

_p(y_ **_x) h(y)_** _p(x) p(y)_ _p(y_ **_x) h(y)_**

_Z(y) = Ep˜(x|y)_ _|p(y)_ = _h(y) p(y_ **_x) C(y)_** _|p(y)_ dx (A42)

  Z _|_

_p(y_ **_x) h(y)_** _−1[#!][−][1]_

= _p(x) C(y)[−][1]_ dx = C(y)[−][1] = Ep(x) _|p(y)_
Z " 

_p(y_ **_x) h(y)_** _−1[!][−][1]_ _p(y_ **_x) h(y)_**

⩽ Ep(x) _|_ = Ep(x) _|_

_p(y)_ _p(y)_

   

_p(y, x) h(y)_
= dx = h(y),

_p(y)_

Z


-----

where we used for the ⩽ Jensen’s inequality with the function f (a) = 1/a, which is convex for
_a > 0._

For score function f (x, y) and distribution ˜p(x | y) for sampling the negative samples, we have
defined:

_Z(y) = Ep˜(x_ **_y)_** [[][f] [(][x][,][ y][)]][,] (A43)
_|_

_Z_ _[∗](y) = Ep(x) [f_ (x, y)], (A44)


_q(x_ **_y) =_** _[p][(][x][)][ f]_ [(][x][,][ y][)] _._ (A45)
_|_ _Z_ _[∗](y)_

Next theorem gives the upper bound of the InfoLOOB on the mutual information, which is


ln _[p][(][x][1][ |][ y][)]_

_p(x1)_


(A46)


I(X1 ; Y ) = Ep(x1,y)


**Theorem A2 (InfoLOOB upper bound). If** _X[˜] = {x2, . . ., xN_ _} are drawn iid according to ˜p(x | y)_
_and if the main assumption holds:_

Ep(y) [KL(p(x | y) ∥ _q(x | y))] ⩽_ Ep(y) [ln Z _[∗](y) −_ ln Z(y)] . (A47)

_Then InfoLOOB with score function f_ (x, y) as in Eq. (A25) is an upper bound on the mutual
_information:_


_f_ (x1, y)

1 _N_

_N_ 1 _i=2_ _[f]_ [(][x][i][,][ y][)] !##
_−_
P


I(X1 ; Y ) ⩽ Ep(y)


= IInfoLOOB(X1 ; Y ) . (A48)


Ep˜(X|y)


ln


_If the negative samples_ _X[˜] =_ **_x2, . . ., xN_** _are drawn iid according to p(x), then InfoLOOB with_
_{_ _}_
_probabilities according to Eq. (A26) is an upper bound on the mutual information:_


_p(y_ **_x1)_**

1 _N |_

_N_ 1 _i=2_ _[p][(][y][ |][ X][i][)]_ !##
_−_
P


I(X1 ; Y ) ⩽ Ep(y)


= IInfoLOOB(X1 ; Y ) . (A49)


Ep(X|y)


ln


_The second bound Eq. (A49) is a special case of the first bound Eq. (A48)._

_Proof. Part (I): Upper bound with score function f_ (x, y).


-----

_p(x1,y)_ ln _[p][(]p[x]([1]x[ |]1[ y])_ [)]

 

_p(x1_ **_y)_** _q(x1_ **_y)_**
ln _|_ _|_

_q(x1_ **_y)_** _p(x1)_

 _|_ 

ln _[q][(][x][1][ |][ y][)]_ + Ep(y) [KL(p(x1 **_y)_** _q(x1_ **_y))]_**

_p(x1)_ _|_ _∥_ _|_



ln _[q][(][x][1][ |][ y][)]_ + Ep(y) ln Ep(x1) [f (x1, y)] ln Z(y)

_p(x1)_ _−_





(A50)


I(X1 ; Y ) = Ep(x1,y)


= Ep(x1,y)

= Ep(x1,y)

⩽ Ep(x1,y)

= Ep(x1,y)

= Ep(x1,y)

= Ep(x1,y)

= Ep(x1,y)


ln _[q][(][x][1][ |][ y][)]_ + ln [E][p][(][x][1][)][ [][f] [(][x][1][,][ y][)]]

_p(x1)_ _Z(y)_



_f_ (x1, y) Ep(x1) [f (x1, y)]
ln

Ep(x1) [f (x1, y)] _Z(y)_







ln _[f]_ [(][x][1][,][ y][)]

_Z(y)_


_f_ (x1, y)

= Ep(x1,y) ln  1 _N_

Ep˜(X|y) _N_ _−1_ _i=2_ _[f]_ [(][x][i][,][ y][)]

  h

P

= Ep(x1,y) [ln f (x1, y)] Ep(y) ln Ep˜(X **_y)_**
_−_ " _|_ "


_N_

_f_ (xi, y)
_i=2_ #!#

X

_N_

_f_ (xi, y)
_i=2_ !##

X


_N −_ 1


⩽ Ep(x1,y) [ln f (x1, y)] − Ep(y)


Ep˜(X|y)


ln


_N −_ 1

!##


_f_ (x1, y)

1 _N_

_N_ 1 _i=2_ _[f]_ [(][x][i][,][ y][)]
_−_
P


= Ep(y)


Ep˜(X|y)


ln


= IInfoLOOB(X1 ; Y ),


where the first "⩽" uses assumption Eq. (A37), while Jensens’s inequality was used for the second
"⩽" by exchanging the expectation and the "ln". We also used


Ep˜(X|y)


_f_ (xi, y)
_i=2_

X


Ep˜(xi **_y)_** [[][f] [(][x]i[,][ y][)] =]
_|_
_i=2_

X


_Z(y) = Z(y) ._

_i=2_

X

(A51)


_N −_ 1


_N −_ 1


_N −_ 1


**Part (II): Upper bound with probabilities.**

If the score function f is


_f_ (x, y) = p(y | x) (A52)

_p˜(x | y) = p(x),_ (A53)


and

then


_p˜(X | y) = p(X | y),_ (A54)
_Z(y) = Ep(x) [p(y | x)] = p(y),_ (A55)

_Z_ _[∗](y) = Ep(x) [p(y | x)] = p(y),_ (A56)

_q(x_ **_y) =_** _[p][(][x][)][ p][(][y][ |][ x][)]_ = p(x **_y),_** (A57)
_|_ _p(y)_ _|_

KL(p(x | y) ∥ _q(x | y)) = KL(p(x | y) ∥_ _p(x | y)) = 0 ._ (A58)


-----

Therefore, the main assumption holds, since

0 = Ep(y) [KL(p(x | y) ∥ _q(x | y))] = Ep(y) [ln Z_ _[∗](y) −_ ln Z(y)] . (A59)


The bound becomes

I(X1 ; Y ) ⩽ Ep(y)

= Ep(y)


!##


_p(y_ **_x1)_**

1 _N |_

_N_ 1 _i=2_ _[p][(][y][ |][ x][i][)]_
_−_
Pp(py(|yx)1)

_N1−1_ _Ni=2_ _p(py(|yx)i)_ 



P


(A60)


I(X1 ; Y ) ⩽ Ep(y) Ep(X **_y)_** ln

_|_

" "

= Ep(y) Ep(X|y) ln

 

An alternative proof is as follows:

I(X1 ; Y ) = I(X1 ; Y ) − Ep(y) "


= IInfoLOOB(X1 ; Y ) .





(A61)


_N_

1 _p(y)_

I(X1 ; Y ) = I(X1 ; Y ) Ep(y) ln
_−_ " _N −_ 1 _i=2_ _p(y)_ !#

X

_N_

1 _p(y_ **_xi)_**

= I(X1 ; Y ) Ep(y) ln Ep(X **_y)_** _|_
_−_ " _|_ " _N −_ 1 _i=2_ _p(y)_ #!#

X

_N_

1 _p(y_ **_xi)_**

⩽ I(X1 ; Y ) Ep(y) Ep(X **_y)_** ln _|_
_−_ " _|_ " _N −_ 1 _i=2_ _p(y)_ !##

X

_p(x1_ **_y)_** 1

= Ep(y) Ep(x1 **_y)_** ln _|_ Ep(y) Ep(X **_y)_** ln

 _|_   _p(x1)_  _−_ " _|_ " _N −_ 1

_p(x1|y)_

= Ep(y) Ep(X|y) ln  _N1−1_ _p(Nix=21)_ _pp(x(xi|iy))_ 

= IInfoLOOB (X1 ; Y ) . P 


_p(y)_
_p(y)_


ln


!##


_p(xi_ **_y)_**
_|_

_p(xi)_


_i=2_


where we applied Jensens’s inequality for the exchanging the expectation and the "ln" to obtain the
"⩽" inequality.

Experiments that compare upper and lower bounds as mutual information estimates are provided
in Cheng et al. (2020) and in Poole et al. (2019). In Fig. 2 in Cheng et al. (2020) it is shown that
InfoLOOB is a good estimator of the mutual information.

A.1.3 INFOLOOB: ANALYSIS OF THE OBJECTIVE

This subsection justifies the maximization of the InfoLOOB bound for contrastive learning. Maximizing the InfoLOOB bound is not intuitive as it was introduced as an upper bound on the mutual
information in the previous subsection. Still maximizing the InfoLOOB bound leads to a good
approximation of the mutual information, in particular for high mutual information.

InfoLOOB with a neural network as a scoring function is not an upper bound on the mutual information when not under-sampling. As we use InfoLOOB on training data for which we do not
know the sampling procedure, we cannot assume under-sampling. Therefore, we elaborate more on
the rationale behind the maximization of the InfoLOOB bound. (I) We show that InfoLOOB with
neural networks as scoring function is bounded from above. Therefore, there exists a maximum
and the optimization problem is well defined. (II) We show that InfoLOOB with neural networks as
scoring function differs by two terms the mutual information. The first term is the Kullback-Leibler
divergence between the variational q(x | y) and the posterior p(x | y). This divergence is minimal
for q(x | y) = p(x | y), which implies f (y | x) = p(y | x). The second term is governed by the
difference between the mean E[f (x, y)] and the empirical mean 1/(N − 1) _i_ _[f]_ [(][x][,][ y][)][. Hoeffding’s]

inequality bounds this difference as we demonstrate in this subsection. Therefore, the second term

[P]


-----

is negligible for large N . In contrast, the KL term is dominant and the relevant term, therefore
maximizing InfoLOOB leads tof (y | x) ≈ _p(y | x)._

We assume that an anchor sample y is given. For the anchor sample y, we draw a positive sample x1
according to p(x1 **_y). We define the set_** _X[˜] =_ **_x2, . . ., xN_** of negative samples, which are drawn
iid according to p( |x). We define the set X = _{x1, . . ., xN_ .}
_{_ _}_

We have


_p( X[˜]_ ) =


_p(xi),_ (A62)
_i=2_

Y


_p(xi) = p(x1_ **_y) p( X[˜]_** ), (A63)
_i=2_ _|_

Y


_p(X_ **_y) = p(x1_** **_y)_**
_|_ _|_


_p(xi) = p(x1) p( X[˜]_ ) . (A64)
_i=1_

Y


_p(X) =_

We use the score function


_f_ (x, y) = exp(τ _[−][1]_ sim(x, y)), (A65)

where sim(x, y) is typically the cosine similarity.

The InfoLOOB with score function f (x, y) is defined as


!##


_f_ (x1, y)

1 _N_

_N_ 1 _i=2_ _[f]_ [(][x][i][,][ y][)]
_−_
P


(A66)


IInfoLOOB(X1 ; Y ) = Ep(y)

We define the variational distribution


Ep(X|y)


ln


_q(x_ **_y) =_** _[p][(][x][)][ f]_ [(][x][,][ y][)] _,_ (A67)
_|_ _Z(y)_

_Z(y) = Ep(x) [f_ (x, y)] . (A68)


-----

The next inequality shows the relation between I(X1 ; Y ) and IInfoLOOB(X1 ; Y ) for random
variables X1 and Y .


_p(x1,y)_ ln _[p][(]p[x]([1]x[ |]1[ y])_ [)]

 

_p(x1_ **_y)_** _q(x1_ **_y)_**
ln _|_ _|_

_q(x1_ **_y)_** _p(x1)_

 _|_ 

ln _[q][(][x][1][ |][ y][)]_ + Ep(y) [KL(p(x1 **_y)_** _q(x1_ **_y))]_**

_p(x1)_ _|_ _∥_ _|_



ln _[f]_ [(][x][1][,][ y][)] + Ep(y) [KL(p(x1 **_y)_** _q(x1_ **_y))]_**

_Z(y)_ _|_ _∥_ _|_




(A69)


I(X1 ; Y ) = Ep(x1,y)


= Ep(x1,y)

= Ep(x1,y)

= Ep(x1,y)

= Ep(x1,y)


_f_ (x1, y)

= Ep(x1,y) ln  1 _N_

Ep(X|y) _N_ _−1_ _i=2_ _[f]_ [(][x][i][,][ y][)]

  h

P

= Ep(x1,y) [ln f (x1, y)] Ep(y) ln Ep(X **_y)_**
_−_ " _|_


_f_ (x1, y)

= Ep(x1,y) ln  Ep(X|y) _N1−1_ _Ni=2_ _[f]_ [(][x][i][,][ y][)]  + Ep(y) [KL(p(x1 | y) ∥ _q(x1 | y))]_

  h i  _N_

P 1

= Ep(x1,y) [ln f (x1, y)] Ep(y) ln Ep(X **_y)_** _f_ (xi, y)
_−_ " _|_ " _N −_ 1 _i=2_ #!#

X

+ Ep(y) [KL(p(x1 | y) ∥ _q(x1 | y))]_

_N_

1

= Ep(x1,y) [ln f (x1, y)] Ep(y) Ep(X **_y)_** ln _f_ (xi, y)
_−_ " _|_ " _N −_ 1 _i=2_ !##

X

_N_ _N_

1 1

+ Ep(y) "Ep(X|y) "ln _N −_ 1 _i=2_ _f_ (xi, y)!## _−_ Ep(y) "ln Ep(X|y) " _N −_ 1 _i=2_ _f_ (xi, y)#!#

X X

+ Ep(y) [KL(p(x1 | y) ∥ _q(x1 | y))]_

_N_

_f_ (x1, y) 1

= Ep(y) Ep(X|y) ln 1 _N_ + Ep(y) Ep(X|y) ln _N_ 1 _f_ (xi, y)

" " _N_ _−1_ _i=2_ _[f]_ [(][x][i][,][ y][)] !## " " _−_ Xi=2 !##

_N_

1 P

Ep(y) ln Ep(X **_y)_** _f_ (xi, y) + Ep(y) [KL(p(x1 **_y)_** _q(x1_ **_y))]_**
_−_ " _|_ " _N −_ 1 _i=2_ #!# _|_ _∥_ _|_

X

= IInfoLOOB(X1 ; Y )

_N_ _N_

1 1

+ Ep(y) "Ep(X|y) "ln _N −_ 1 _i=2_ _f_ (xi, y)!## _−_ Ep(y) "ln Ep(X|y) " _N −_ 1 _i=2_ _f_ (xi, y)#!#

X X

+ Ep(y) [KL(p(x1 | y) ∥ _q(x1 | y))]_

= IInfoLOOB(X1 ; Y )

_N_

1

+ Ep(y) Ep(X **_y)_** ln _f_ (xi, y) Ep(y) ln Ep(x1) [f (x1, y)]

" _|_ " _N −_ 1 _i=2_ !## _−_

X    

+ Ep(y) [KL(p(x1 | y) ∥ _q(x1 | y))]_

= IInfoLOOB(X1 ; Y )

Ep(x1) [f (x1, y)]

Ep(y) Ep( ˜X) ln 1 _N_
_−_ " " _N_ 1 _i=2_ _[f]_ [(][x][i][,][ y][)] !##

_−_

+ Ep(y) [KL(p(x1 **_y)_** _q(xP1_ **_y))]_**
_|_ _∥_ _|_

= IInfoLOOB(X1 ; Y ) − DE + Ep(y) [KL(p(x1 | y) ∥ _q(x1 | y))],_

where we used

Ep(x1) [f (x1, y)]

DE = Ep(y) Ep( ˜X) ln 1 _N_ (A70)

" " _N_ 1 _i=2_ _[f]_ [(][x][i][,][ y][)] !##

_−_
P


-----

and


(A71)


_Z(y) = Ep(x1) [f_ (x1, y)] = Ep( ˜X)


_f_ (xi, y)
_i=2_

X


_N −_ 1


= Ep(X|y)


_f_ (xi, y)
_i=2_

X


_N −_ 1


Since both KL and DE are non-negative (for DE see below), to increase InfoLOOB we have either
to decrease KL or to increase DE.

**Bounding DE. Next we bound DE. We define**


L = z[T] **_x −_** _β[−][1]_


_zi ln zi ._ (A72)
_i=1_

X


The log-sum-exponential (lse) is the maximum of L on the N -dimensional simplex D with D = {z |

_i_ _[z][i][ = 1][,][ 0][ ⩽]_ _[z][i][}][ (][Gao & Pavel][,][ 2017][):]_

P


lse(β, x) = max
**_z_** _D_ **_[z][T][ x][ −]_** _[β][−][1]_
_∈_


_zi ln zi ._ (A73)
_i=1_

X


For some z ∈ _D we have_

Ea [lse(β, a)] ≥ Ea

therefore


**_z[T]_** **_a −_** _β[−][1]_


= z[T] Ea [a] − _β[−][1]_


_zi ln zi,_ (A74)
_i=1_

X


_zi ln zi_
_i=1_

X


Ea [lse(β, a)] max
_≥_ **_z_** _D_ **_[z][T][ E][a][ [][a][]][ −]_** _[β][−][1]_
_∈_


_zi ln zi = lse(β, Ea [a]) ._ (A75)
_i=1_

X


We obtain


#!##


exp(τ sim(x1, y))

Ep(y) Ep( ˜X) ln Ep(x1) 1 _N_ _[−][1]_ (A76)

" " " _N_ 1 _i=2_ [exp(][τ][ −][1][ sim(][x][i][,][ y][))] #!##

_−_

_N_

P 1

⩽ Ep(y) "ln Ep(x1) exp(τ _[−][1]_ sim(x1, y)) _−_ ln _N −_ 1 _i=2_ exp(τ _[−][1]_ Ep(xi) [sim(xi, y)])!#

  X

= Ep(y) ln Ep(x1) exp(τ _[−][1]_ sim(x1, y)) _−_ _τ_ _[−][1]_ Ep(x1) [sim(x1, y)] _._
   


Ep(y)


Ep( ˜X)


ln


Ep(x1)


We obtain via Jensen’s inequality

exp(τ sim(x1, y))

Ep(y) Ep( ˜X) ln Ep(x1) 1 _N_ _[−][1]_ (A77)

" " " _N_ 1 _i=2_ [exp(][τ][ −][1][ sim(][x][i][,][ y][))] #!##

_−_

_N_

P 1

_≥_ Ep(y) "ln Ep(x1) exp(τ _[−][1]_ sim(x1, y)) _−_ ln _N −_ 1 _i=2_ Ep(xi) exp(τ _[−][1]_ sim(x1, y)) !#

  X  

= 0 .


If we combine both previous inequalities, we obtain

0 ⩽ DE ⩽ Ep(y) ln Ep(x1) exp(τ _[−][1]_ sim(x1, y)) _−_ _τ_ _[−][1]_ Ep(x1) [sim(x1, y)] _._ (A78)
   


-----

In particular, for bounded sim(x1, y), we get

0 ⩽ DE ⩽ _τ_ _[−][1]_ maxy,x1 [sim(][x][1][,][ y][)][ −] [min]y,x1 [sim(][x][1][,][ y][)] _,_ (A79)
 

while Hoeffding’s lemma gives

2

0 ⩽ DE ⩽ [1] max _._ (A80)

8 _[τ][ −][2]_ **_y,x1_** [sim(][x][1][,][ y][)][ −] [min]y,x1 [sim(][x][1][,][ y][)]
 

Thus, for bounded sim(x1, y), DE is bounded, therefore also InfoLOOB. For sub-exponential
distributions with variance σ[2], for which Bernstein’s condition with τ > b holds (Eq. (2.16) in
Wainwright (2019)), we get (Proposition 2.3 in Wainwright (2019)):

_σ[2]_
0 ⩽ DE ⩽ (A81)

2 (τ [2] _b τ_ ) _[.]_
_−_


Next, we show that DE is small. Hoeffding’s inequality states that if f (x, y) ∈ [a, b] then


⩽ 2 exp
_−_ [2 (]([N]b _[ −]a[1)])[2][ ϵ][2]_
 _−_


(A82)



[E][p][(][x][1][)][ [][f] [(][x][1][,][ y][)]][ −]


_f_ (xi, y)
_i=2_ _[≥]_ _[ϵ]_

X


_N −_ 1


For


_f_ (xi, y) ⩽ _ϵ_ (A83)
_i=2_

X


Ep(x1) [f (x1, y)] −


_N −_ 1

⩽ ln


we have


Ep(x1) [f (x1, y)] _N1_ 1 _Ni=2_ _[f]_ [(][x][i][,][ y][) +][ ϵ]

ln 1 _N_ ⩽ ln _−_ 1 _N_

_N_ 1 _i=2_ _[f]_ [(][x][i][,][ y][)] ! _N_ P1 _i=2_ _[f]_ [(][x][i][,][ y][)]
_−_ _−_

_ϵ_ _ϵ_
⩽ 1 PN ⩽ _Z_ _ϵ [,]_ P

_N_ _−1_ _i=2_ _[f]_ [(][x][i][,][ y][)] _−_

where we used ln a ⩽ _a −P1 for 0 < a. Analog for_


(A84)


_f_ (xi, y) − Ep(x1) [f (x1, y)] ⩽ _ϵ_ (A85)
_i=2_

X


_N −_ 1


we have

Ep(x1) [f (x1, y)] Ep(x1) [f (x1, y)]

ln _N1_ 1 _Ni=2_ _[f]_ [(][x][i][,][ y][)] ! _≥_ ln  Ep(x1) [f (x1, y)] + ϵ  (A86)

_−_
PEp(x1) [f (x1, y)] + ϵ _ϵ_

= ln
_−_ Ep(x1) [f (x1, y)] _≥−_ Ep(x1) [f (x1, y)] [=][ −] _Z [ϵ]_ _[,]_
 

where we used − ln a ≥ 1 − _a for 0 < a._

In summary, for

_N_

1

_f_ (xi, y) (A87)

_N −_ 1 _i=2_ [⩽] _[ϵ]_

X

we have

[E][p][(][x][1][)][ [][f] [(][x][1][,][ y][)]][ −]


_−_ _Z[ϵ]_ [⩽] [ln]


Ep(x1) [f (x1, y)]

1 _N_

_N_ 1 _i=2_ _[f]_ [(][x][i][,][ y][)]
_−_
P


_ϵ_

(A88)
_Z_ _ϵ [.]_
_−_


-----

It follows that


_−_ _Z[ϵ]_ [⩽] [DE][ ⩽]


_ϵ_

(A89)
_Z_ _ϵ [.]_
_−_


DE averages the ln-term over y and _X[˜]_, therefore it has an even smaller bound than the bound above
on the ln-term. Consequently, for small b − _a and large N_, the term DE is small.

KL is decreased by making the variation distribution q(x1 **_y) more similar to the posterior p(x1_** **_y)._**
_|_ _|_
The value DE only depends on the marginal distributions p(y) and p(x), since p( X[˜] ) = _i=2_ _[p][(][x][i][)][.]_
The value DE can be changed by adding an offset to f (x, y). However, scaling f (x, y) by a factor
does not change DE. Consequently, DE is difficult to change.

[Q][N]

Therefore, increasing InfoLOOB is most effective by making q(x1 **_y) more similar to the posterior_**
_p(x1_ **_y)._** _|_
_|_

**Gradient of InfoLOOB expressed by gradients of KL and DE. Assume that the similarity is**
parametrized by w giving sim(x, y; w).


_p(x1_ **_y)_**
KL(p(x1 **_y)_** _q(x1_ **_y)) =_** _p(x1_ **_y) ln_** _|_
_|_ _∥_ _|_ _|_ _q(x1_ **_y)_**
Z  _|_

= − _τ_ _[−][1]_ _p(x1 | y) sim(x1, y; w) dx1 + ln Z + C,_
Z

where C is independent of w.

Next, we compute the derivative of KL with respect to parameters w.


dx (A90)


_∂KL_

(A91)
_∂w_

_∂sim(x1, y; w)_

= − _τ_ _[−][1]_ _p(x1 | y)_ _[∂][sim(]∂[x]w[1][,][ y][;][ w][)]_ dx1 + Z[1] _p(x1) [exp(]∂[τ][ −]sim([1][ sim(]x1, y[x][1]; w[,][ y])[;][ w][))]_ _∂w_ dx1
Z Z

_∂sim(x1, y; w)_

= − _τ_ _[−][1]_ _p(x1 | y)_ _[∂][sim(]∂[x]w[1][,][ y][;][ w][)]_ dx1 + τ _[−][1]_ _p(x1) [exp(][τ][ −][1][ sim(]Z_ **_[x][1][,][ y][;][ w][))]_** _∂w_ dx1
Z Z

= − _τ_ _[−][1]_ _p(x1 | y)_ _[∂][sim(]∂[x]w[1][,][ y][;][ w][)]_ dx1 + τ _[−][1]_ _q(x1 | y)_ _[∂][sim(]∂[x]w[1][,][ y][;][ w][)]_ dx1
Z Z


= τ _[−][1]_ (q(x1 | y) − _p(x1 | y))_ _[∂][sim(]∂[x]w[1][,][ y][;][ w][)]_ dx1 .
Z

The derivative is the average difference between the posterior distribution p(x1 **_y) and the variational_**
distribution q(x1 **_y) multiplied by the derivative of the similarity function. If both distribution |_**
match, then the derivative vanishes. |


-----

Next, we compute the derivative of DE with respect to parameters w.

_∂DE_

(A92)
_∂w_

1 _N_

_∂_ ln Z _N_ 1 _i=2_ _[τ][ −][1][ exp(][τ][ −][1][ sim(][x][i][,][ y][;][ w][))][ ∂][sim(]∂[x]w[i][,][y][;][w][)]_

= Ep(y)  _∂w_  _−_ Ep(y) "Ep( ˜X) " _−_ P _N1_ 1 _Nj=2_ _[f]_ [(][x][j][,][ y][)] ##

_−_
P

= Ep(y) _τ_ _q(x1_ **_y)_** _[∂][sim(][x][1][,][ y][;][ w][)]_ dx1

_[−][1]_ _|_ _∂w_
 Z 

1 _N_

_N_ 1 _i=2_ _[τ][ −][1][ exp(][τ][ −][1][ sim(][x][i][,][ y][;][ w][))][ ∂][sim(]∂[x]w[i][,][y][;][w][)]_

Ep(y) Ep( ˜X) _−_ 1 _N_
_−_ " " P _N_ 1 _j=2_ _[f]_ [(][x][j][,][ y][)] ##

_−_
P

= τ Ep(y) _q(x1_ **_y)_** _[∂][sim(][x][1][,][ y][;][ w][)]_ dx1

_[−][1]_ _|_ _∂w_

Z 

_N_

1 _f_ (xi, y) _∂sim(xi, y; w)_

_−_ _τ_ _[−][1]_ Ep(y) "Ep( ˜X) " _N −_ 1 Xi=2 _N1−1_ _Nj=2_ _[f]_ [(][x][j][,][ y][)] _∂w_ ##

_p(x1) f_ (x1, y) _∂sim(xP1, y; w)_

= τ Ep(y) dx1

_[−][1]_ Ep(x) [f (x, y)] _∂w_

Z 

_N_

1 _f_ (xi, y) _∂sim(xi, y; w)_

_−_ _τ_ _[−][1]_ Ep(y) "Ep( ˜X) " _N −_ 1 Xi=2 _N1−1_ _Nj=2_ _[f]_ [(][x][j][,][ y][)] _∂w_ ##

_f_ (x1, y) _∂sim(P_ **_x1, y; w)_**

= τ _[−][1]_ Ep(y) Ep(x1) Ep(x) [f (x, y)] _∂w_

  

_N_

1 _f_ (xi, y) _∂sim(xi, y; w)_

_−_ _τ_ _[−][1]_ Ep(y) "Ep( ˜X) " _N −_ 1 Xi=2 _N1−1_ _Nj=2_ _[f]_ [(][x][j][,][ y][)] _∂w_ ##

_N_

1 _f_ (xPi, y) _∂sim(xi, y; w)_

= τ _[−][1]_ Ep(y) " _N −_ 1 _i=2_ Ep(xi)  Ep(x) [f (x, y)] _∂w_ [#]

X

_N_

1 _f_ (xi, y) _∂sim(xi, y; w)_

_−_ _τ_ _[−][1]_ Ep(y) "Ep( ˜X) " _N −_ 1 Xi=2 _N1−1_ _Nj=2_ _[f]_ [(][x][j][,][ y][)] _∂w_ ##

_N_

1 _f_ (xPi, y) _∂sim(xi, y; w)_

= τ _[−][1]_ Ep(y) "Ep( ˜X) " _N −_ 1 _i=2_ Ep(x) [f (x, y)] _∂w_ ##

X

_N_

1 _f_ (xi, y) _∂sim(xi, y; w)_

_−_ _τ_ _[−][1]_ Ep(y) "Ep( ˜X) " _N −_ 1 Xi=2 _N1−1_ _Nj=2_ _[f]_ [(][x][j][,][ y][)] _∂w_ ##

_N_

1 P1 1

= τ _[−][1]_ Ep(y) "Ep( ˜X) " _N −_ 1 Xi=2 Ep(x) [f (x, y)] _[−]_ _N1−1_ _Nj=2_ _[f]_ [(][x][j][,][ y][)] ! _f_ (xi, y) _[∂][sim(]∂[x]w[i][,][ y][;][ w][)]_

_N_

1 1 1 P

= τ _[−][1]_ Ep(y) Ep( ˜X) _N_ 1 _Z_ 1 _N_ _f_ (xi, y) _[∂][sim(]∂[x]w[i][,][ y][;][ w][)]_ _._

" " _−_ Xi=2 _[−]_ _N_ _−1_ _j=2_ _[f]_ [(][x][j][,][ y][)] ! ##

The derivative is the average of _Z[1]_ _N1_ 1 _Nj=21_ _[f]_ [(][x]P[j] _[,][y][)][ multiplied by the score function and the]_

derivative of the similarity function. The average is over[−] _−_ P **_y and_** _X[˜]_, therefore the whole derivative
becomes even smaller. Consequently, for small b − _a and large N_, the derivative of DE is small.

Note that for

_N_

1

_f_ (xi, y) (A93)

_N −_ 1 _i=2_ [⩽] _[ϵ]_

X

[E][p][(][x][1][)][ [][f] [(][x][1][,][ y][)]][ −]


##


-----

we have

therefore


1

1 _N_ ⩽ _Z[1]_

_N_ _−1_ _j=2_ _[f]_ [(][x][j][,][ y][)] _[−]_

1

P

1 _N_ _≥_ _Z[1]_

_N_ _−1_ _j=2_ _[f]_ [(][x][j][,][ y][)] _[−]_
P


_ϵ_

(A94)
_Z(Z + ϵ)_ _[,]_


_Z_

_[−]_

1

_Z_

_[−]_


_Z + ϵ_ [=]


_ϵ_

(A95)
_Z(Z_ _ϵ)_ _[,]_
_−_


_Z_ _ϵ_ [=][ −]
_−_


_ϵ_

(A96)
_Z(Z_ _ϵ)_ _[.]_
_−_


_Z_

_[−]_


_Z_ _[−]_ _N1−1_ _Nj=2_ _[f]_ [(][x][j][,][ y][)] [⩽] _Z(Z −_ _ϵ)_ _[.]_ (A96)

If the expectation Z is well approximated by the averageP _N1_ 1 _Nj=2_ _[f]_ [(][x][j][,][ y][)][, then both][ DE][ and its]

_−_
gradient are small.
P

Derivative of InfoLOOB via KL and DE:


_∂IInfoLOOB(X1 ; Y )_

= _[∂][DE]_ (A97)
_∂w_ _∂w_ _−_ _[∂]∂[KL]w_ _[.]_

In this gradient, the KL term is dominating, therefore f (x, y) is pushed to approximate the conditional
probability p(y | x). Modern Hopfield networks lead to larger values of p(y | x) as the mutual
information becomes larger, therefore modern Hopfield networks help to push f (x, y) to large values.
Furthermore, modern Hopfield networks increase Z, which is in the denominator of the bound on
DE and its derivative.

A.1.4 INFONCE AND INFOLOOB: GRADIENTS

We consider the InfoNCE and the InfoLOOB loss function. For computing the loss
function, we sample N pairs independently from p(x, y), which gives the training set
(x1, y1), (x2, y2), . . ., (xN _, yN_ ) . InfoNCE and InfoLOOB only differ in using the positive
_{_ _}_
example in the negatives. More precisely, InfoNCE uses for the matrix of negative samples
**_X = (x1, . . ., xN_** ), while InfoLOOB uses **_X[˜] = (x2, . . ., xN_** ).

**InfoNCE.**

The InfoNCE loss is


LInfoNCE =
_−_ _N[1]_

where we used


_f_ (xi, yi)

_N_
_j=1_ _[f]_ [(][x][j][,][ y][i][)]
P


LInfoNCE(yi), (A98)
_i=1_

X


ln

_i=1_

X


_f_ (xi, yi)

_N_
_j=1_ _[f]_ [(][x][j][,][ y][i][)]
P


(A99)


LInfoNCE(yi) = ln
_−_

For the score function f (x, y), we use


_f_ (x, y) = exp(τ _[−][1]_ sim(x, y)), (A100)

sim(x, y) = y[T] **_x_** (A101)

with τ as the temperature.

The loss function for this score function is


LInfoNCE(y) = − _τ_ _[−][1]_ **_y[T]_** **_x1 + τ_** _[−][1]_ lse _τ_ _[−][1], X_ _[T]_ **_y_** _,_ (A102)

where lse is the log-sum-exp function (lse):   


lse(β, a) = β[−][1] log


(A103)


exp(βai)
_i=1_

X


-----

for β > 0 and vector a = (a1, . . ., aN ).

The gradient with respect to y is

_∂LInfoNCE(y)_

_∂y_ = − _τ_ _[−][1]_ **_x1 + τ_** _[−][1]_ **_X softmax_** _τ_ _[−][1]X_ _[T]_ **_y_** _,_ (A104)
  

which is the positive example x1 that fits to the anchor example y minus the Hopfield network update
with state pattern y and stored patterns X and then this difference multiplied by τ _[−][1]._

This gradient can be simplified, since the positive example x1 is also in the negative examples. Using
**_p = (p1, . . ., pN_** )[T] = softmax _τ_ **_X_** _[T]_ **_y_**, we obtain

_[−][1]_

_∂LInfoNCE(y)_    (A105)

_∂y_

1
= _τ_ (1 _p1)_ **_x1_** **_X_** softmax _τ_ **_X_** _[T]_ **_y_** (p1, 0, . . ., 0)[T][ ]
_−_ _[−][1]_ _−_ _−_ 1 _p1_ _[−][1]_ _−_
 _−_

    

= _τ_ (1 _p1)_ **_x1_** **_X˜ softmax_** _τ_ **_X_** _[T]_ **_y_** = (1 _p1)_ _[∂][L][InfoLOOB][(][y][)]_ _._
_−_ _[−][1]_ _−_ _−_ _[−][1][ ˜]_ _−_ _∂y_
  

where


1

**_X_** softmax _τ_ **_X_** _[T]_ **_y_** (p1, 0, . . ., 0)[T][ ] (A106)
1 _p1_ _[−][1]_ _−_
_−_

1     
= **_X_** (p1, p2, . . ., pN )[T] (p1, 0, . . ., 0)[T][ ]

1 _p1_ _−_
_−_
  _N_

1 1
= **_X(0, p2, . . ., pN_** )[T] = _pi xi_

1 _p1_ 1 _p1_
_−_ _−_ _i=2_

X

is the softmax average over the negatives xi for 2 ⩽ _i ⩽_ _N without x1. It can be easily seen that_
1 _N_

1 _p1_ _i=2_ _[p][i][ =][ 1]1[−]p[p][1]1_ [= 1][. For the derivative of the InfoLOOB see below.]
_−_ _−_

The gradient with respect toP **_x1 is_**

_∂LInfoNCE∂x1_ (y) = − _τ_ _[−][1]_ **_y + τ_** _[−][1]_ _Nexp(τ_ _[−][1]_ **_x[T]1_** **_[y][)]_** **_y_** (A107)

_i=1_ [exp(][τ][ −][1][x]i[T] **_[y][)]_**

= _τ_ (1 _p1) y ._ (A108)
_−_ _[−][1]_ _−_ P

Consequently, the learning rate is scaled by (1 _p1)._
_−_

The sum of gradients with respect to x1 and xi is


_∂LInfoNCE(y)_

_∂x1_


_∂LInfoNCE(y)_

_∂xi_ = − _τ_ _[−][1]_ **_y + τ_** _[−][1]_ **_y 1[T]_** softmax _τ_ _[−][1]X_ _[T]_ **_y_** (A109)
  


1 _i_

_i=1_

= − _τ_ _[−][1]_ **_y + τ_** _[−][1]_ **_y = 0,_**

where 1 is the vector with ones. However, the derivatives with respect to the weights are not zero
since the xi are differently computed.


**InfoLOOB.**

The InfoLOOB loss is

LInfoLOOB =
_−_ _N[1]_

where we used


_f_ (xi, yi)

1 _N_

_N_ 1 _j=1,j=i_ _[f]_ [(][x][j][,][ y][i][)]
_−_ _̸_
P


LInfoLOOB(yi), (A110)
_i=1_

X


ln

_i=1_

X


_f_ (xi, yi)

1 _N_

_N_ 1 _j=1,j=i_ _[f]_ [(][x][j][,][ y][i][)]
_−_ _̸_
P


(A111)


LInfoLOOB(yi) = ln
_−_


-----

For the score function f (x, y), we use

_f_ (x, y) = exp(τ _[−][1]_ sim(x, y)), (A112)

sim(x, y) = y[T] **_x_** (A113)


with τ as the temperature.

The loss function for this score function is

LInfoLOOB(y) = − _τ_ _[−][1]_ **_y[T]_** **_x1 + τ_** _[−][1]_ lse _τ_ _[−][1],_ **_X[˜]_** _[T]_ **_y_** _,_ (A114)
 

where lse is the log-sum-exponential function.

The gradient with respect to y is

_∂LInfoLOOB(y)_

_∂y_ = − _τ_ _[−][1]_ **_x1 + τ_** _[−][1][ ˜]X softmax_ _τ_ _[−][1][ ˜]X_ _[T]_ **_y_** _,_ (A115)
 

which is the positive example x1 that fits to the anchor example y minus the Hopfield network update
with state pattern y and stored patterns **_X[˜] and then this difference multiplied by τ_** _[−][1]._

The gradient with respect to x1 is

_∂LInfoLOOB(y)_

_∂x1_ = − _τ_ _[−][1]_ **_y ._** (A116)

The sum of gradients with respect to x1 and xi is


_∂LInfoLOOB(y)_

_∂x1_


_∂LInfoLOOB(y)_

_∂xi_ = − _τ_ _[−][1]_ **_y + τ_** _[−][1]_ **_y 1[T]_** softmax _τ_ _[−][1][ ˜]X_ _[T]_ **_y_**



(A117)

= − _τ_ _[−][1]_ **_y + τ_** _[−][1]_ **_y = 0,_**

where 1 is the vector with ones. However, the derivatives with respect to the weights are not zero
since the xi are differently computed.


**Gradients with respect to τ** _[−][1]._

The gradient of the InfoNCE loss Eq. (A98) using the similarity Eq. (A100) with respect to τ _[−][1]_ is

_∂LInfoNCE(y)_

_∂τ_ = − **_y[T]_** **_x1 + y[T]_** **_X softmax_** _τ_ _[−][1]X_ _[T]_ **_y_** (A118)

_[−][1]_

= **_y[T][  ]x1_** **_X softmax_** _τ _ **_X_** _[T]_ **_y_** , (A119)
_−_ _−_ _[−][1]_

which is the similarity of the anchor y with the difference of the positive example   x1 and the Hopfield
network update with state pattern y and stored patterns X. The gradient of the InfoLOOB loss
Eq. (A110) using the similarity Eq. (A112) with respect to τ _[−][1]_ is

_∂LInfoLOOB(y)_

_∂τ_ = − **_y[T]_** **_x1 + y[T][ ˜]X softmax_** _τ_ _[−][1][ ˜]X_ _[T]_ **_y_** (A120)

_[−][1]_
 

= **_y[T]_** **_x1_** **_X˜ softmax_** _τ_ **_X_** _[T]_ **_y_** _._ (A121)
_−_ _−_ _[−][1][ ˜]_
  

with the difference that the Hopfield network update is done with stored patterns **_X[˜] instead of X._**

Without the positive example x1 in the stored patterns **_X[˜]_**, the term x1 **_X softmax_** _τ_ _[−][1][ ˜]X_ _[T]_ **_y_**
_−_ [˜]

in Eq. (A120) will not decrease like the term x1 **_X softmax_** _τ_ **_X_** _[T]_ **_y_** in Eq. (A118 ) but grow
even larger with better separation of the positive and negative examples. − _[−][1]_
  

A.1.5 INFOLOOB AND INFONCE: PROBABILITY ESTIMATORS

In McAllester & Stratos (2018; 2020) it was shown that estimators of the mutual information by lower
bounds have problems as they come with serious statistical limitations. Statistically more justified for


-----

representing the mutual information is a difference of entropies, which are estimated by minimizing
the cross-entropy loss. Both InfoNCE and InfoLOOB losses can be viewed as cross-entropy losses.

We sample _N_ pairs independently from _p(x, y),_ which gives _Z_ =
(x1, y1), (x2, y2), . . ., (xN _, yN_ ) . We set X = **_x1, x2, . . ., xN_** and Y = **_y1, y2, . . ., yN_**,
_{_ _}_ _{_ _}_ _{_ _}_
so that, Z = X × Y . The score function f (x, y) is an estimator for p(x, y). Then we obtain
estimators ˆq for the conditional probabilities. ˆq(yi **_xi, Y_** **_yi_** ) is an estimator for p(yi **_xi) and_**
_qˆ(xi_ **_yi, X_** **_xi_** ) an estimator for p(xi **_yi). Each estimator |_** _\ {_ _} ˆq uses beyond (xi, yi) additional |_
samples to estimate the normalizing constant. For InfoNCE these estimators are | _\ {_ _}_ _|_

_f_ (xi, yi) _f_ (xi, yi)
_qˆ[1](yi | xi, Y \ {yi}) =_ _N1_ _Nj=1_ _[f]_ [(][x][i][,][ y][j][)] _≈_ Ep(y) [f (xi, y)] _[,]_ (A122)

Pf (xi, yi) _f_ (xi, yi)

_qˆ[2](xi | yi, X \ {xi}) =_ _N1_ _Nj=1_ _[f]_ [(][x][j][,][ y][i][)] _≈_ Ep(x) [f (x, yi)] _[.]_ (A123)

The cross-entropy losses for the InfoNCE estimators areP


L[1]InfoNCE [=][ −] [1]

_N_

L[2]InfoNCE [=][ −] [1]

_N_

For InfoLOOB these estimators are


_f_ (xi, yi)

_N_
_j=1_ _[f]_ [(][x][i][,][ y][j][)]
Pf (xi, yi)

_N_
_j=1_ _[f]_ [(][x][j][,][ y][i][)]
P


(A124)

(A125)


ln

_i=1_

X

_N_

ln

_i=1_

X


_f_ (xi, yi) _f_ (xi, yi)
_qˆ[1](yi | xi, Y \ {yi}) =_ _N1_ 1 _Nj=1,j=i_ _[f]_ [(][x][i][,][ y][j][)] _≈_ Ep(y) [f (xi, y)] _[,]_ (A126)

_−_ _̸_
Pf (xi, yi) _f_ (xi, yi)

_qˆ[2](xi | yi, X \ {xi}) =_ _N1_ 1 _Nj=1,j=i_ _[f]_ [(][x][j][,][ y][i][)] _≈_ Ep(x) [f (x, yi)] _[.]_ (A127)

_−_ _̸_

The cross-entropy losses for the InfoLOOB estimators areP


L[1]InfoLOOB [=][ −] [1]

_N_

L[2]InfoLOOB [=][ −] [1]

_N_


_f_ (xi, yi)

1 _N_

_N_ 1 _j=1,j=i_ _[f]_ [(][x][i][,][ y][j][)]
_−_ _̸_
Pf (xi, yi)

1 _N_

_N_ 1 _j=1,j=i_ _[f]_ [(][x][j][,][ y][i][)]
_−_ _̸_
P


(A128)

(A129)


ln

_i=1_

X

_N_

ln

_i=1_

X


The InfoLOOB estimator uses for normalization


_f_ (xj, yi), (A130)
_j=1X,j≠_ _i_

_N_

_f_ (xi, yj), (A131)
_j=1X,j≠_ _i_

_N_

_f_ (xj, yi), (A132)
_j=1_

X

_N_

_f_ (xi, yj) . (A133)
_j=1_

X


Ep(x) [f (x, yi)] ≈

Ep(y) [f (xi, y)] ≈

in contrast to InfoNCE, which uses


_N −_ 1

1

_N −_ 1


Ep(x) [f (x, yi)] ≈

Ep(y) [f (xi, y)] ≈


If InfoNCE estimates the normalizing constant separately, then it would be biased. (xi, yi) is drawn
according to p(xi, yi) instead of p(xi)p(yi). In contrast, if InfoLOOB estimated the normalizing
constant separately, then it would be unbiased.


-----

A.1.6 INFOLOOB AND INFONCE: LOSSES

We have N pairs drawn iid from p(x, y), where we assume that a pair (xi, yi) is already
an embedding of the original drawn pair. These build up the embedding training set Z =
(x1, y1), (x2, y2), . . ., (xN _, yN_ ) that allows to construct the matrices X = (x1, x2, . . ., xN )
_{_ _}_
of N embedding samples xi and Y = (y1, y2, . . ., yN ) of N embedding samples yi. We also have
_M stored patterns U = (u1, . . ., uM_ ) and K stored patterns V = (v1, . . ., vK).

The state vectors xi and yi are the queries for the Hopfield networks, which retrieve some vectors
from U or V . We normalize vectors **_xi_** = **_yi_** = **_ui_** = **_vi_** = 1. The following vectors are
_∥_ _∥_ _∥_ _∥_ _∥_ _∥_ _∥_ _∥_
retrieved from modern Hopfield networks (Ramsauer et al., 2021):

**_Uxi = U softmax(β U_** _[T]_ **_xi),_** **_Uyi = U softmax(β U_** _[T]_ **_yi),_** (A134)

**_Vxi = V softmax(β V_** _[T]_ **_xi),_** **_Vyi = V softmax(β V_** _[T]_ **_yi)_** (A135)

where Uxi denotes an image-retrieved image embedding, Uyi a text-retrieved image embedding,
**_Vxi an image-retrieved text embedding and Vyi a text-retrieved text embedding. The hyperparameter_**
_β corresponds to the inverse temperature: β = 0 retrieves the average of the stored pattern, while_
large β retrieve the stored pattern that is most similar to the state pattern (query).

We consider the loss functions

_N_ _N_

LInfoNCE = − _N[1]_ log _Nexp(τ_ _[−][1]_ **_x[T]i_** **_[y][i][)]_** _−_ _N[1]_ log _Nexp(τ_ _[−][1]_ **_x[T]i_** **_[y][i][)]_** _,_

_i=1_ _j=1_ [exp(][τ][ −][1][ x]i[T] **_[y][j][)]_** _i=1_ _j=1_ [exp(][τ][ −][1][ x]j[T] **_[y][i][)]_**

X X

(A136)

P P

_N_ _N_

LInfoLOOB = − _N[1]_ log _Nexp(τ_ _[−][1]_ **_x[T]i_** **_[y][i][)]_** _−_ _N[1]_ log _Nexp(τ_ _[−][1]_ **_x[T]i_** **_[y][i][)]_** _,_

_i=1_ _j=i_ [exp(][τ][ −][1][ x]i[T] **_[y][j][)]_** _i=1_ _j=i_ [exp(][τ][ −][1][ x]j[T] **_[y][i][)]_**

X _̸_ X _̸_

(A137)

P P

_N_ exp(τ _[−][1]_ **_Ux[T]i_** **_[V][y]i_** [)] _N_ exp(τ _[−][1]_ **_Ux[T]i_** **_[V][y]i_** [)]

LInfoLOOB[H][−][UVUV] [=][ −] [1] log _N_ log _N_ _,_

_N_ _i=1_ _j=i_ [exp(][τ][ −][1][ U][ T]xi **_[V][y]j_** [)] _−_ _N[1]_ _i=1_ _j=i_ [exp(][τ][ −][1][ U][ T]xj **_[V][y]i_** [)]

X _̸_ X _̸_

(A138)

P P

_N_ exp(τ _[−][1]_ **_Ux[T]i_** **_[U][y]i_** [)] _N_ exp(τ _[−][1]_ **_Vx[T]i_** **_[V][y]i_** [)]

LInfoLOOB[H][−][UUVV] [=][ −] [1] log _N_ log _N_ _,_

_N_ _i=1_ _j=i_ [exp(][τ][ −][1][ U][ T]xi **_[U][y]j_** [)] _−_ _N[1]_ _i=1_ _j=i_ [exp(][τ][ −][1][ V][ T]xj **_[V][y]i_** [)]

X _̸_ X _̸_

(A139)

P P

where for InfoLOOB the sum _j=i_ [in the denominator contains only negative examples][ j][. We do]

_̸_
not consider the loss function L[H]InfoLOOB[−][UVUV] [because of the high variance in the dot product][ U][ T]xi **_[V][y]i_** [as]
elaborated in the following. [P]

Let us consider the dot product between the anchor retrieval with the positive pattern retrieval for the
loss functions with Hopfield. In the first term of the loss function Eq. (A138), Uxi is the anchor with
**_Vyi as the positive sample and Vyi with Uxi as the positive sample for the second term, since the_**
anchor also appears in each term of the denominator. Equivalently the same is valid for Eq. (A139),
but with positive samples Vxi and Uyi respectively. These dot products can be written as

**_Ux[T]i_** **_[V][y]i_** [= softmax(][β][ U][ T][ x][i][)][T][ U][ T][ V][ softmax(][β][ V][ T][ y][i][)][,] (A140)

**_Ux[T]i_** **_[U][y]i_** [= softmax(][β][ U][ T][ x][i][)][T][ U][ T][ U][ softmax(][β][ U][ T][ y][i][)][,] (A141)

**_Vx[T]i_** **_[V][y]i_** [= softmax(][β][ V][ T][ x][i][)][T][ V][ T][ V][ softmax(][β][ V][ T][ y][i][)][ .] (A142)

**High variance of Ux[T]i** **_[V][y]i_** **[.][ To compute the dot product][ U][ T]xi** **_[V][y]i_** [,][ M][ +][ K][ stored patterns are required]
(M of the uj and K of the vj). In contrast, the dot products Ux[T]i **_[U][y]i_** [and][ V][ T]xi **_[V][y]i_** [require only][ M]
or respectively K stored patterns. Therefore, Ux[T]i **_[V][y]i_** [has higher variance than both][ U][ T]xi **_[U][y]i_** [and]
**_Vx[T]i_** **_[V][y]i_** [.]

**Covariance structure extracted by Ux[T]i** **_[U][y]i_** **[and][ V][ T]xi** **_[V][y]i_** **[.]**


-----

The Jacobian J of the softmax p = softmax(βa) is

J(βa) = _[∂][softmax(][β][a][)]_ = β diag(p) **_pp[T][ ]_** _,_ (A143)

_∂a_ _−_
 

which is a symmetric, positive semi-definite matrix with one eigenvalue of zero for eigenvector 1.
J(βa) is diagonally dominant since |pi(1 − _pi)| −_ [P]j≠ _i_ _[|][p][i][p][j][|][ =][ p][i][ −]_ [P]j _[p][i][p][j][ =][ p][i][ −]_ _[p][i][ = 0][.]_

Next we give upper bounds on the norm of J.

**Lemma A1. For a softmax p = softmax(βx) with m = maxi pi(1 −** _pi), the spectral norm of the_
_Jacobian J of the softmax is bounded:_

_∥J∥2 ⩽_ 2 m β, (A144)
_∥J∥1 ⩽_ 2 m β, (A145)
_∥J∥∞_ ⩽ 2 m β . (A146)

_In particular everywhere holds_


_∥J∥2 ⩽_ [1]2 _[β .]_ (A147)

_If pmax = maxi pi ≥_ 1 − _ϵ ≥_ 0.5, then for the spectral norm of the Jacobian holds

_∥J∥2 ⩽_ 2 ϵ β − 2 ϵ[2] _β_ _< 2 ϵ β ._ (A148)

_Proof. We consider the maximum absolute column sum norm_


_aij_ (A149)
_|_ _|_

_aij_ _._ (A150)
_|_ _|_


_∥A∥1 = maxj_

and the maximum absolute row sum norm

**_A_** = max
_∥_ _∥∞_ _i_


We have for A = J = β diag(p) − **_pp[T][ ]_**
 

_aij_ = β _pi(1_ _pi) +_ _pipj_ = β pi (1 2pi +
_|_ _|_  _−_  _−_

Xj _j,jX≠_ _i_

= 2 β p _i (1 −_ _pi) ⩽_ 2 m β, 

_aij_ = β _pj (1_ _pj) +_ _pjpi_ = β pj (1 2pj +
_|_ _|_  _−_  _−_

Xi _i,iX≠_ _j_

= 2 β p _j (1 −_ _pj) ⩽_ 2 m β . 

Therefore, we have


_pj)_ (A151)

_pi)_ (A152)


_∥J∥1 ⩽_ 2 m β, (A153)
_∥J∥∞_ ⩽ 2 m β, (A154)

_∥J∥2 ⩽_ _∥J∥1∥J∥∞_ ⩽ 2 m β . (A155)
q

The last inequality is a direct consequence of Hölder’s inequality.


For 0 ⩽ _pi ⩽_ 1, we have pi(1 − _pi) ⩽_ 0.25. Therefore, m ⩽ 0.25 for all values of pi.

If∂x p(1max − ≥x)/∂x1 − = 1ϵ ≥ −0.25x > (ϵ ⩽ 0 for0.5 x <), then 0. 15, therefore − _pmax ⩽ xϵ(1 and for −_ _x) increases with pi ̸= pmax pi x ⩽ forϵ. The derivative x < 0.5. Using_
we havex = 1 − mpmax ⩽ and forϵ(1 − _ϵ) p._ _i ̸= pmax x = pi, we obtain pi(1 −_ _pi) ⩽_ _ϵ(1 −_ _ϵ) for all i. Consequently,_


-----

For the softmax p = softmax(βa) with Jacobian ∂J/∂a = J(βa) = β diag(p) − **_pp[T][ ]_** and for
arbitrary N -dimensional vectors b and c, we have
 

**_b[T]_** J(βa) c = β b[T][  ]diag(p) − **_p p[T][ ]_** **_c = β_** _i_ _pi bi ci −_ _i_ _pi bi_ _i_ _pi ci!!_ _._

X X ! X

(A156)

Therefore, b[T] J(βa)c is β times the covariance between b and c if component i is drawn with
probability pi of the multinomial distribution p. In our case the component i is sample i.

Using the mean ˆu = 1/M _i=1_ **_[u][i][, the empirical covariance of data][ U][ is]_**

Cov(U ) = 1/M U U _[T]_ **_uˆ ˆu[T]_** _,_ (A157)

[P][M] _−_

_M_ _M_ _M_

[Cov(U )]kl = _i=1_ 1/M uik uil − _i=1_ 1/M uik! _i=1_ 1/M uil! _._ (A158)

X X X

The weighted covariance (samples ui are drawn according to pi)


Cov(U ) = U J(β a) U _[T]_ _,_ (A159)

_M_ _M_ _M_

[Cov(U )]kl = β _i=1_ _pi uik uil −_ _i=1_ _pi uik!_ _i=1_ _pi uil!!_ _,_ (A160)

X X X

which replaces 1/M from equal sampling by the pi, that is, ui is sampled with probability pi.

The next theorem states how to express the dot product Ux[T]i **_[U][y]i_** [by weighted covariances of the data]
**_U_** .

**Theorem A3 (Weighted Covariances). Using the weighted covariances**

Cov(U _, yi) = U J[m](β U_ _[T]_ **_yi) U_** _[T]_ _,_ Cov(U _, xi) = U J[m](β U_ _[T]_ **_xi) U_** _[T]_ _,_ (A161)

1
J[m](β a) = J(λ β a) dλ, (A162)

0

Z

_where the mean Jacobian J[m]_ _is symmetric, diagonally dominant, and positive semi-definite with_
_spectral norm bounded by ∥J[m]∥2 ⩽_ 0.5β.

_The dot product Ux[T]i_ **_[U][y]i_** _[can be expressed by the weighted covariances]_

**_Ux[T]i_** **_[U][y]i_** [= (¯]u + Cov(U _, xi) xi)[T]_ (¯u + Cov(U _, yi) yi),_ (A163)

_where the mean is ¯u = 1/M_ **_U_** **1.**


_Proof. We apply the mean value theorem to the softmax with the symmetric, diagonally dominant,_
1
positive semi-definite Jacobian matrix J[m] = 0 [J(][λ][a][ + (1][ −] _[λ][)][a][′][) d][λ][:]_

softmax(a) − softmax(R **_a[′]) = J[m]_** (a − **_a[′]) ._** (A164)

We set a[′] = 0 and use βa instead of a, which gives:


1
softmax(β a) = 1/M 1 + J[m](β a) a, J[m](β a) =

0

Z

which is exact. We obtain


J(λ β a) dλ, (A165)


softmax(β U _[T]_ **_xi) = 1/M 1 + J[m](β U_** _[T]_ **_xi) U_** _[T]_ **_xi,_** (A166)

softmax(β U _[T]_ **_yi) = 1/M 1 + J[m](β U_** _[T]_ **_yi) U_** _[T]_ **_yi ._** (A167)

The spectral norm of 1 J[m] is bounded by ∥J[m]∥2 ⩽ 0.5β, since this bound holds for every J(λβa) in
J[m](β a) = 0 [J(][λβ][a][) d][λ][ according to Lemma][ A1][.]
R


-----

The dot product between the anchor retrieval and the positive sample is:

**_Ux[T]i_** **_[U][y]i_** [= softmax(][β][ U][ T][ x][i][)][T][ U][ T][ U][ softmax(][β][ U][ T][ y][i][)] (A168)

= 1/M 1 + J[m](β U _[T]_ **_xi) U_** _[T]_ **_xi_** _T U T U_ 1/M 1 + J[m](β U _[T]_ **_yi) U_** _[T]_ **_yi_**

=  1/M U 1 + U J[m](β U _[T]_ **_xi) U_** _[T]_ **_xi_** _T_ 1 /M U **1 + U J[m](β U** _[T]_ **_yi) U_** _[T]_ **_yi_**

= (¯ u + Cov(U _, xi) xi)[T]_ (¯u + Cov(U _, y  i) yi),_ 


where we used the mean ¯u = 1/M **_U_** **1 and the weighted covariances**

Cov(U _, yi) = U J[m](β U_ _[T]_ **_yi) U_** _[T]_ _,_ Cov(U _, xi) = U J[m](β U_ _[T]_ **_xi) U_** _[T]_ _._ (A169)

The Jacobian J[m] is symmetric, diagonally dominant, and positive semi-definite. The weighted
covariance Cov(U _, .) is the covariance if the stored pattern ui is drawn according to an averaged_
_pi given by J[m](.). Analog for weighted covariance Cov(V, .). When maximizing the dot product_
**_Ux[T]i_** **_[U][y]i_** [, the normalized vectors][ x][i] [and][ y][i] [are encouraged to agree on drawing the patterns][ u][i] [with]
the same probability pi to generate similar weighted covariance matrices Cov(U _, .). If subsets of U_
have a strong covariance structure, then it can be exploited to produce large weighted covariances
and, in turn, large dot products of Ux[T]i **_[U][y]i[. Furthermore, for a large dot product][ U][ T]xi_** **_[U][y]i[,][ x][i]_** [and]
**_yi have to be similar to one another to extract the same direction from the covariance matrices. All_**
considerations are analog for Vx[T]i **_[V][y]i_** [.]

A.2 MUTUAL INFORMATION ESTIMATION

We follow the toy experiment discussed in Poole et al. (2019), Belghazi et al. (2018) and Cheng
et al. (2020) and experimentally confirm the superior quality of InfoLOOB for mutual information
than InfoNCE. The dataset consists of samples (xi, yi) drawn jointly from a multivariate Gaussian
distribution with correlation ρ where the dimension of the samples x and y is set to d = 20. We
examine the performance of InfoLoob with and without Hopfield and InfoNCE at estimating mutual
information of these samples. Due to the Gaussian distribution, the true value of mutual information
can be calculated as I(x, y) = − _[d]2_ [log(1][ −] _[ρ][2][)][. We set the mutual information true value to the]_

values (2.0, 4.0, 6.0, 8.0, 10.0, 14.0) by varying the value of ρ. At each MI true value, we sample
data batches 1024 times, with batch size equal to 64, for the training of variational MI estimators.
Figure 2 shows that modern Hopfield networks reduce the variance of the model. For models trained
on data with mutual information of 10 we observe an average variance of approx. 0.67 for a model
without Hopfield and an average variance of approx. 0.33 for a model with Hopfield. For models
trained on data with mutual information of 14 we observe an average variance of approx. 1.00 for a
model without Hopfield and an average variance of approx. 0.48 for a model with Hopfield.

In Figure A1 we show the performance of our method InfoLOOB with and without Hopfield at
estimating mutual information as well as InfoNCE. As expected estimates of InfoNCE have estimates
that saturate at log(batch size). InfoLOOB without Hopfield exhibits good estimates of high mutual
information while InfoLOOB with Hopfield accomplishes both - good estimates of high mutual
information with a decreased variance.

A.3 EXPERIMENTS

A.3.1 ABLATION STUDIES

As mentioned in the main paper, CLOOB has two new main components compared to CLIP: (1) the
InfoLOOB objective instead of the InfoNCE objective and (2) the modern Hopfield networks. To
assess which of the new main components of CLOOB have led to the performance increase over CLIP,
we performed ablation studies on the CC dataset. The results are reported in Table A1. First, we
enhanced CLIP by replacing the InfoNCE objective with InfoLOOB (see column CLIP InfoLOOB).
Next, we added modern Hopfield networks to the CLIP architecture and used retrieved embeddings
instead of the original embeddings, while keeping the InfoNCE objective (see column Hopfield
InfoNCE). Finally, we add modern Hopfield networks to CLIP and replace the InfoNCE objective


-----

infoNCE


infoLOOB without Hopfield


infoLOOB with Hopfield


14

12

10

8

6

4

2

0

_−2_


14

12

10

8

6

4

2

0

_−2_


14

12

10

8

6

4

2

0

_−2_


True MI
log(64)


True MI


1000 2000 3000 4000 5000 6000 7000

steps


1000 2000 3000 4000 5000 6000 7000

steps


True MI

0 1000 2000 3000 4000 5000 6000 7000

steps


Figure A1: The estimated mutual information of the InfoNCE objective saturates at the batch size
induced bound. The InfoLOOB objective trained with the same batch size with samples from the
same correlated Gaussian distributions following (Belghazi et al., 2018; Poole et al., 2019; Cheng
et al., 2020) is not limited by that bound and better estimates higher mutual information but suffers
from higher variance. This is remedied by incorporating the modern Hopfield network.

Table A1: Influence of loss functions and Hopfield retrieval. InfoLOOB increases the performance of
CLIP in most of the tasks. The InfoNCE loss is not suited for the Hopfield approach as it saturates
leading to a worse performance. Hopfield with InfoLOOB strongly improves the performance in 7
out of 8 datasets compared to both CLIP models.


CLIP Hopfield
Dataset InfoNCE InfoLOOB InfoNCE InfoLOOB

Birdsnap 1.94 2.37 1.67 **2.53**
Country211 0.62 0.63 0.54 **0.76**
Flowers102 13.04 13.03 11.53 **14.24**
GTSRB **7.28** 4.39 5.76 5.86
UCF101 21.00 19.14 20.56 **22.29**
Stanford Cars 0.90 1.33 1.24 **1.37**
ImageNet 20.31 22.13 19.04 **24.21**
ImageNetV2 20.63 21.65 18.97 **23.80**

with InfoLOOB (see column Hopfield InfoLOOB). As shown in Table A1 the InfoLOOB objective
increases the performance of CLIP in the majority of the datasets. We attribute this increase to the fact
that InfoLOOB suffers less than InfoNCE from the “explaining away” problem. However, InfoLOOB
is even more effective for higher mutual information, that is, a richer covariance structure. Hopfield
networks amplify the covariance structure in their retrieved embeddings. Though, this amplified
covariance structure is disadvantageous for InfoNCE, as the saturation effect is stronger. The stronger
saturation effect is caused by a richer covariance structure through Hopfield networks, which in turn
leads to higher similarity between anchor and positive. Therefore, we see a performance drop when
combining modern Hopfield networks with InfoNCE. Concluding, modern Hopfield networks are a
perfect match for InfoLOOB as they yield higher mutual information. Therefore, CLOOB strongly
improves the performance on 7 out of 8 zero-shot transfer learning tasks compared to CLIP.

For CLIP with InfoNCE, the hyperparameter τ _[−][1]_ is a learnable parameter. For the other experiments,
we use a fixed τ _[−][1]_ of 30. The value for τ _[−][1]_ was determined via hyperparameter search (see
Section A.3.2).

In contrast to CLIP, we use a learning rate scheduler with restarts (Loshchilov & Hutter, 2017) to be
more flexible regarding the number of total training epochs and enable training up to a plateau. To
investigate the influence of the learning rate scheduler, we performed experiments with and without
restarts. Table A2 shows the zero-shot performance for the different downstream tasks for CLIP and
CLOOB respectively. For both CLIP and CLOOB, the performance at the majority of the tasks either
increases or remains roughly the same with restarts.


-----

Table A2: Influence of learning rate scheduler. For most of the tasks the performance either increases
or remains roughly the same with restarts for both CLIP and CLOOB.

|Dataset|CLIP w/o restarts w/ restarts|CLOOB w/o restarts w/ restarts|
|---|---|---|


|Birdsnap Country211 Flowers102 GTSRB UCF101 Stanford Cars ImageNet|2.10 1.94 0.71 0.62 11.00 13.04 6.16 7.28 19.05 21.00 1.29 0.90 20.19 20.31|2.64 2.53 0.63 0.76 11.50 14.24 5.05 5.86 21.97 22.29 1.22 1.37 23.29 24.21|
|---|---|---|


|ImageNet V2|20.53 20.63|22.97 23.80|
|---|---|---|



Table A3: Datasets used for zero-shot and linear probing. In the case of several train or test sets per
dataset we report the total number of samples. It should be noted that at the time of this work some
Birdsnap images were not accessible anymore.

Dataset Classes Train size Test size Evaluation metric

Birdsnap 500 38,411 1,855 accuracy
Country211 211 42,200 21,100 accuracy
Flowers102 102 2,040 6,149 class-weighted accuracy
GTSRB 43 26,640 12,630 accuracy
ImageNet 1,000 1,281,167 50,000 accuracy
ImageNet V2 1,000 1,281,167 30,000 accuracy
Stanford Cars 196 8,144 8,041 accuracy
UCF101 101 28,747 11,213 accuracy

A.3.2 HYPERPARAMETERS

The hyperparameter search was done on a validation split of CC with about 15,000 samples. For the
hyperparameter τ _[−][1]_ several values were considered (14.3, 30, 50, 70), where 30 leads to the best
results for both YFCC and CC. Analogously to CLIP, we use the Adam optimizer (Kingma et al.,
2014) with decoupled weight decay regularization (Loshchilov & Hutter, 2019). The weight decay is
only applied to weights that are not gains or biases. As proposed in OpenCLIP (Ilharco et al., 2021)
weight decay was set to 0.1. Different choices of weight decay (0.2 or 0.05), did not lead to a relevant
performance change. We use the same learning rate of 1 × 10[−][3] for CC and 5 × 10[−][4] for YFCC as
used in OpenCLIP. For the hyperparameter β we considered values in the range of 5 to 20. A value
of 8 resulted in the best performance for CC and 14.3 for YFCC. The batch size for CC was reduced
to 512 due to computational restraints which did not result in performance losses. The batch size for
YFCC was kept at 1024 as reported by OpenCLIP since a reduction resulted in a significant drop in
performance. The learning rate scheduler for all experiments is cosine annealing with warmup and
hard restarts (Loshchilov & Hutter, 2017) with a cycle length of 7 epochs. For models trained on
YFCC the warmup was set to 10000 steps and for models trained on CC to 20000 steps.

A.3.3 DATASETS

For pretraining we consider two datasets, Conceptual Captions (CC) (Sharma et al., 2018) and
YFCC100M (Thomee et al., 2016). The CC dataset consists of 2.9 million images and corresponding
high-quality captions. Images and their corresponding notations for CC have been gathered via an
automated process from the web and therefore represent a wide variety of styles. Raw descriptions
of images are collected from the alt-text HTML attribute. Both images and texts are filtered such
that only image-text pairs above a certain quality threshold are part of this dataset. The dataset we
refer to as YFCC is a subset of the Yahoo Flickr Creative Commons 100 Million (YFCC100M)
dataset. It was created by filtering for images which contain natural language descriptions and/or
titles in English resulting in 15 million image-caption pairs. The textual descriptions contain less


-----

useful information than CC because they are not filtered by quality. Occasionally they also contain
metadata like camera settings or web addresses.

We evaluate and compare our method on several downstream classification tasks. We evaluate
on the same set of datasets as CLIP reported for a model trained on YFCC. This set contains
Birdsnap (Berg et al., 2014), Country211 (Radford et al., 2021), Flowers102 (Nilsback & Zisserman,
2008), GTSRB (Stallkamp et al., 2011), UCF101 (Soomro et al., 2012), Stanford Cars (Krause et al.,
2013) and ImageNet (Deng et al., 2009). Additionally, we include ImageNet V2 in our analysis
(Recht et al., 2019). Table A3 shows an overview of training and test set sizes, number of classes and
the applied evaluation metric. In the case of several test sets per dataset the metric is calculated for
every set individually and the average performance is reported. The set size in Table A3 corresponds
to the total number of samples across all test and training sets of a dataset respectively.

**Birdsnap contains images of North American bird species, however our dataset is smaller than**
reported in CLIP as some samples are no longer available. The Country211 dataset was published
in CLIP and is a small subset of the YFCC100m dataset. It consists of photos that can be assigned
to 211 countries via GPS coordinates. For each country 200 photos are sampled for the training set
and 100 for testing. For the Flowers102 images of 102 flower categories commonly occuring in the
United Kingdom were collected. Several classes are very similar and there is a large variation in scale,
pose and lighting. The German Traffic Sign Recognition Benchmark (GTSRB) was a challenge held
at the IJCNN 2011. The dataset contains images of german traffic signs from more than 40 classes.
Note that two versions of this dataset exist, one used for the challenge and an official dataset released
after the competition. For CLIP the linear probing classifiers were trained using the competition
training set but tested on the official test set. Stanford Cars contains images of 196 car models at
the level of make, model and year (e.g. Tesla Model S Sedan 2012). UCF101 (Soomro et al., 2012)
is a video dataset with short clips for action recognition consisting of three training sets and three
test sets. We follow the procedure reported in CLIP and extract the middle frame of every video to
assemble the dataset. The ImageNet Large Scale Visual Recognition Challenge was held from 2012
through 2017 and is one of the most widely used benchmarks for object detection and localization.
Several years later ImageNet V2 assembled three new test sets with images from the same 1,000
classes to test for generalization of models optimized for the original ImageNet benchmark. Every
test set comprises 10,000 samples.

A.3.4 ZERO-SHOT EVALUATION

Class names for all downstream tasks were adopted from CLIP, that is, among other changes special
characters like hyphens or apostrophes were removed. Furthermore, some class names of the datasets
were slightly changed (e.g. “kite” to “kite (bird of prey)” in ImageNet). For zero-shot
evaluation, we use the same prompt templates as published in CLIP. Depending on the dataset the
number of prompts can vary from one prompt (e.g. “a photo of a {label}, a type of
bird.” for Birdsnap) up to 80 prompts for ImageNet covering various settings (e.g. “a cropped
photo of a {label}.”, “a origami {label}.”). In case of several prompts an average
embedding over all prompt embeddings is calculated. Figure A2 shows the zero-shot results for all
evaluation tasks with the ResNet-50x4 model reported in Table 4.

A.3.5 LINEAR PROBING

We try to follow the evaluation procedure in Radford et al. (2021) as closely as possible. We note one
difference with respect to the implementation: Instead of scikit-learn’s logistic regression using the
L-BFGS solver, we use cuML’s logistic regression classifier with L-BFGS algorithm to utilize GPUs
for efficiency. All hyperparameters are the same as described in Radford et al. (2021), the maximum
number of iterations was set to 1000, and the L2 regularization strength λ was determined by using a
parametric binary search.

We tried to reproduce the CLIP results with the correspondingly published models, however, failed to
produce the exact numbers. This could be due to several factors:

-  The train and validation split. Same as in Radford et al. (2021), we use the provided
validation set to perform the hyperparameter search. When there is none provided, we use a
random half of the training dataset for validation.


-----

Table A4: Linear probing results for the reimplementation of CLIP and CLOOB using different
ResNet architectures trained on YFCC. The performance of CLOOB scales with increased encoder
size



|Dataset|CLIP CLOOB RN-50 RN-50|CLOOB CLOOB RN-101 RN-50x4|
|---|---|---|


|Birdsnap Country211 Flowers102 GTSRB UCF101 Stanford Cars ImageNet ImageNet V2|50.9 56.2 19.5 20.6 94.8 96.1 82.5 78.9 75.2 72.3 36.2 37.7 66.9 65.7 60.2 58.7|58.1 62.2 21.8 24.2 96.1 96.2 77.9 80.6 72.8 75.3 39.0 44.3 67.0 69.7 60.3 62.2|
|---|---|---|



-  In case of a tie in the validation score, we use the maximal λ for the strongest regularization.
We note though that we came closer to reproducing the results published in CLIP when
using the mean λ over all ties when these exist.

-  For the Birdsnap dataset, the resources that we have got online at the time of this writing
could be different from the resources that CLIP’s authors obtained at the time.

Linear probing evaluation of YFCC-pretrained models is shown in Table A4. Comparing our
reimplementation of CLIP and CLOOB with ResNet-50 encoders, we observe mixed results. The
reason for this effect might be attributed to the observed task-dependence of multimodal models
(Devillers et al., 2021). Another potential reason is that the benefit of the restrictions to more reliable
patterns that occur in both modalities does not directly translate to an evaluation of just the encoding
part of one modality. Again, as expected in self-supervised training, increasing the capacity of the
CLOOB models benefits accuracy.

A.4 REVIEW OF MODERN HOPFIELD NETWORKS

We briefly review continuous modern Hopfield networks that are used for deep learning architectures.
They are continuous and differentiable, therefore they a work with gradient descent in deep architectures. They retrieve with one update only, therefore they can be activated like other deep learning
layers. They have exponential storage capacity, therefore they can tackle large problems. Hopfield
networks are energy-based, binary associative memories, which popularized artificial neural networks
in the 1980s (Hopfield, 1982; 1984). Associative memory networks have been designed to store and
retrieve samples. Their storage capacity can be considerably increased by polynomial terms in the
energy function (Chen et al., 1986; Psaltis & Cheol, 1986; Baldi & Venkatesh, 1987; Gardner, 1987;
Abbott & Arian, 1987; Horn & Usher, 1988; Caputo & Niemann, 2002; Krotov & Hopfield, 2016).
In contrast to these binary memory networks, we use continuous associative memory networks with
very high storage capacity. These modern Hopfield networks for deep learning architectures have an
energy function with continuous states and can retrieve samples with only one update (Ramsauer
et al., 2021; 2020). Modern Hopfield Networks have been successfully applied to immune repertoire
classification (Widrich et al., 2020) and chemical reaction prediction (Seidl et al., 2021).

We assume a set of patterns {u1, . . ., uN _} ⊂_ R[d] that are stacked as columns to the matrix U =
(u1, . . ., uN ) and a state pattern (query) ξ ∈ R[d] that represents the current state. The largest norm
of a stored pattern is M = maxi **_ui_** . Continuous modern Hopfield networks with state ξ have the
energy _∥_ _∥_


+ β[−][1] log N + [1] (A170)

2 **_[ξ][T][ ξ][ + 1]2_** _[M][ 2][ .]_


E = − _β[−][1]_ log


exp(βu[T]i **_[ξ][)]_**
_i=1_

X


For energy E and state ξ, the update rule

**_ξ[new]_** = f (ξ; U _, β) = U p = U softmax(βU_ _[T]_ **_ξ)_** (A171)


-----

cottontail rabbit correct rank: 1/1000


longhorn beetle correct rank: 1/1000


collie correct rank: 2/1000


cottontail rabbit longhorn beetle car mirror

hare cricket insect collie

Angora rabbit weevil Border Collie

Scottish Terrier tiger beetle Great Pyrenees dog

Yorkshire Terrier cockroach American Staffordshire Terrier

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

mosque correct rank: 1/1000 threshing machine correct rank: 13/1000 lipstick correct rank: 7/1000

mosque combine harvester red wine

Black and Tan Coonhound hay lighter

rhinoceros beetle corn perfume

gossamer-winged butterfly thatched roof wine bottle

dragonfly farm plow bell pepper

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

Osprey correct rank: 1/500 Great Blue Heron correct rank: 1/500 Brant correct rank: 3/500

Osprey Great Blue Heron Northern Pintail

Bald Eagle Tricolored Heron Long tailed Duck

Semipalmated Plover Little Blue Heron Brant

Swallow tailed Kite Reddish Egret Greater Scaup

Rough legged Hawk Northern Harrier Gadwall

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

Croatia correct rank: 1/211 Greece correct rank: 1/211 Spain correct rank: 24/211

Croatia Greece Algeria

Greece Croatia Palestine

Montenegro Malta Malta

Turkey Monaco Croatia

Albania Bermuda Barbados

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

azalea correct rank: 1/102 magnolia correct rank: 1/102 rose correct rank: 1/102

azalea magnolia rose

geranium frangipani desert-rose

pelargonium gaura bougainvillea

sweet william cyclamen osteospermum

garden phlox sweet pea camellia

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

red circle with white red and white circle red red and white triangle with
horizontal stripe no entry correct rank: 4/43 truck and black car no passing correct rank: 36/43 black curve approaching warning correct rank: 11/43

red and white triangle withtraffic light approaching warning stop red and white triangle withexclamation mark warning

empty red and white circle red circle with whitehorizonal stripe no entry red and white triangle carskidding / slipping warning

stop blue circle with whitekeep right arrow mandatory red and white triangle withperson digging / construction

red circle with white red and white triangle car red and white triangle with
horizonal stripe no entry skidding / slipping warning deer warning

red and white triangleroad intersection warning empty red and white circle red and white triangle withsnowflake / ice warning

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

Dodge Caliber Wagon 2012 correct rank: 10/196 Chrysler Aspen SUV 2009 correct rank: 19/196 Chevrolet Traverse SUV 2012 correct rank: 33/196

Chevrolet HHR SS 2010 Ford Expedition EL SUV 2009 Honda Odyssey Minivan 2007

Dodge Caliber Wagon 2007 Chevrolet Express Van 2007 Volvo XC90 SUV 2007

Audi RS 4 Convertible 2008 Chevrolet Silverado 1500Classic Extended Cab 2007 Ford Expedition EL SUV 2009

Dodge Journey SUV 2012 Ford Freestar Minivan 2007 Daewoo Nubira Wagon 2002

Dodge Magnum Wagon 2008 Honda Odyssey Minivan 2007 Ford Freestar Minivan 2007

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

Horse Riding correct rank: 3/101 Handstand Walking correct rank: 11/101 Field Hockey Penalty correct rank: 1/101

Military Parade Body Weight Squats Field Hockey Penalty

Pommel Horse Basketball Soccer Penalty

Horse Riding Table Tennis Shot Tennis Swing

Typing Mopping Floor Table Tennis Shot

Nunchucks Bowling Basketball

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0


Figure A2: Visualization of zero-shot classification of three examples from each dataset. The following datasets are used (top to bottom): ImageNet, ImageNet V2, Birdsnap, Country211, Flowers102,
GTSRB, Stanford Cars and UCF101. The ground truth label is displayed above the picture. The bar
plots show the softmax values of the top 5 classes.

has been proven to converge globally to stationary points of the energy E, which are almost always
local minima (Ramsauer et al., 2021). The update rule Eq. (A171) is also the formula of the wellknown transformer attention mechanism (Ramsauer et al., 2021), therefore Hopfield retrieval and
transformer attention coincide.


-----

The separation ∆i of a pattern ui is defined as its minimal dot product difference to any of the
∆other patterns:i ≥ _βN2_ [+][ 1]β [log] ∆i = min2(N − 1)j,jNβM≠ _i_  u[2][T]i [][u]. If the patterns[i][ −] **_[u]i[T]_** **_[u][j]. A pattern is ui are well separated, the iterate Eq. well-separated from the data if (A171)_**

converges to a fixed point close to a stored pattern. If some patterns are similar to one another and,
 
therefore, not well separated, the update rule Eq. (A171) converges to a fixed point close to the mean
of the similar patterns. This fixed point is a metastable state of the energy function and averages over
similar patterns.

The next theorem states that the update rule Eq. (A171) typically converges after one update if the
patterns are well separated. Furthermore, it states that the retrieval error is exponentially small in the
separation ∆i.

**Theorem A4 (Modern Hopfield Networks: Retrieval with One Update). With query ξ, after one**
_update the distance of the new point f_ (ξ) to the fixed point u[∗]i _[is exponentially small in the separation]_
∆i. The precise bounds using the Jacobian J = _[∂f]∂[(]ξ[ξ][)]_ _and its value J[m]_ _in the mean value theorem_

_are:_

_f_ (ξ) **_u[∗]i_** 2 _i_ (A172)
_∥_ _−_ _[∥]_ [⩽] _[∥][J][m][∥]_ _[∥][ξ][ −]_ **_[u][∗][∥]_** _[,]_

_∥J[m]∥2 ⩽_ 2 β N M [2] (N − 1) exp(− _β (∆i −_ 2 max{∥ξ − **_ui∥, ∥u[∗]i_** _[−]_ **_[u][i][∥}][ M]_** [))][ .][ (A173)]

_For given ϵ and sufficient large ∆i, we have_ _f_ (ξ) **_u[∗]i_**
_The retrieval error ∥f_ (ξ) − **_ui∥_** _of pattern u ∥i is bounded by −_ _[∥]_ _[< ϵ][, that is, retrieval with one update.]_

_∥f_ (ξ) − **_ui∥_** ⩽ 2 (N − 1) exp(− _β (∆i −_ 2 max{∥ξ − **_ui∥, ∥u[∗]i_** _[−]_ **_[u][i][∥}][ M]_** [))][ M .](A174)

For a proof see (Ramsauer et al., 2021).

The main requirement of modern Hopfield networks to be suited for contrastive learning is that they
can store and retrieve enough embeddings if the batch size is large. We want to store a potentially
large set of embeddings. We first define what we mean by storing and retrieving patterns from a
modern Hopfield network.

**Definition A1 (Pattern Stored and Retrieved). We assume that around every pattern ui a sphere**
Si is given. We say ui is stored if there is a single fixed point u[∗]i
_Eq.converge, and (A171) gives a point Si ∩_ Sj = ˜ ∅xifor that is at least i ̸= j. We say ϵ-close to the single fixed point ui is retrieved for a given[∈] [S][i][ to which all points] ϵ if iteration (update rule) u[∗]i **_[ ξ][ ∈]_** [S][i]
_error is_ **_x˜i_** **_ui_** _._ _[∈]_ [S][i][. The retrieval]
_∥_ _−_ _∥_

As with classical Hopfield networks, we consider patterns on the sphere, i.e. patterns with a fixed
norm. For randomly chosen patterns, the number of patterns that can be stored is exponential in the
dimension d of the space of the patterns (ui ∈ R[d]).

**Theorem A5 (Modern Hopfield Networks: Exponential Storage Capacity). We assume a failure**

_We defineprobability a 0 := < pd−2 ⩽1_ [(1 + ln(2]1 and randomly chosen patterns on the sphere with radius[βK] [2][p][(][d][ −] [1)))][,][ b][ :=][ 2][K]5[2][β] _, and c :=_ _W0(exp(ba+ln( Mb)) :=[, where] K√[ W]d −[0]4[ is]1._

2 _d−1_
_[the upper branch of the Lambert W function (Olver et al., 2010, (4.13)), and ensure c ≥](http://dlmf.nist.gov/4.13)_ _√p_ _._

_Then with probability 1_ _p, the number of random patterns that can be stored is_  
_−_

_N ≥_ _[√]p c_ _d−4_ 1 _._ (A175)

_Therefore it is proven for c ≥_ 3.1546 with β = 1, K = 3, d = 20 and p = 0.001 (a + ln(b) > 1.27)
_and proven for c ≥_ 1.3718 with β = 1, K = 1, d = 75, and p = 0.001 (a + ln(b) < −0.94).

For a proof see (Ramsauer et al., 2021).

This theorem justifies to use continuous modern Hopfield networks for using retrieved embeddings
instead of the original embeddings for large batch sizes. Even for hundreds of thousands of embeddings, the continuous modern Hopfield network is able to retrieve the embeddings if the dimension of
the embeddings is large enough.


-----

A.5 FURTHER RELATED WORK

Multiple works have proposed improvements to InfoNCE. Joint Contrastive Learning (JCL) studies
the effect of sampling multiple positives for each anchor. (Cai et al., 2020). Sampling negatives
around each positive leads to higher bias but lower variance than InfoNCE (Wu et al., 2021). InfoNCE
has been generalized to C-InfoNCE and WeaC-InfoNCE, which are conditional contrastive learning
approaches to remove undesirable information in self-supervised representations (Tsai et al., 2021).
ProtoNCE is a generalized version of the InfoNCE, which pushes representations to be closer to
their assigned prototypes (Li et al., 2021). ProtoNCE combines contrastive learning with clustering.
SimCSE employs InfoNCE for contrastive learning to learn sentence embeddings (Gao et al., 2021).
InfoNCE has been extended to video representation learning (Han et al., 2020).

Many follow up works have been based on the CLIP model. The CLIP model is used in Vision-andLanguage tasks (Shen et al., 2021). The CLIP model guided generative models via an additional
training objective (Bau et al., 2021; Galatolo et al., 2021; Frans et al., 2021) and improved clustering of
latent representations (Pakhomov et al., 2021). It is used in studies of out of distribution performance
(Devillers et al., 2021; Milbich et al., 2021; Miller et al., 2021), of fine-tuning robustness (Wortsman
et al., 2021), of zero-shot prompts (Zhou et al., 2021) and of adversarial attacks to uncurated datasets
(Carlini & Terzis, 2021). It stirred discussions about more holistic evaluation schemes in computer
vision (Agarwal et al., 2021). Multiple methods utilize the CLIP model in a straightforward way to
perform text-to-video retrieval (Fang et al., 2021; Luo et al., 2021; Narasimhan et al., 2021).


-----

