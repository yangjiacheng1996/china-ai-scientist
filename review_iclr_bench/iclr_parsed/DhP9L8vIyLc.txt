# PAC PREDICTION SETS UNDER COVARIATE SHIFT


**Sangdon Park**
Dept. of Computer & Info. Science
PRECISE Center
University of Pennsylvania
sangdonp@seas.upenn.edu

**Insup Lee**
Dept. of Computer & Info. Science
PRECISE Center
University of Pennsylvania
lee@cis.upenn.edu


**Edgar Dobriban**
Dept. of Statistics & Data Science
The Wharton School
University of Pennsylvania
dobriban@wharton.upenn.edu

**Osbert Bastani**
Dept. of Computer & Info. Science
PRECISE Center
University of Pennsylvania
obastani@seas.upenn.edu


ABSTRACT

An important challenge facing modern machine learning is how to rigorously
quantify the uncertainty of model predictions. Conveying uncertainty is especially important when there are changes to the underlying data distribution that
might invalidate the predictive model. Yet, most existing uncertainty quantification algorithms break down in the presence of such shifts. We propose a novel
approach that addresses this challenge by constructing probably approximately
_correct (PAC) prediction sets in the presence of covariate shift. Our approach fo-_
cuses on the setting where there is a covariate shift from the source distribution
(where we have labeled training examples) to the target distribution (for which we
want to quantify uncertainty). Our algorithm assumes given importance weights
that encode how the probabilities of the training examples change under the covariate shift. In practice, importance weights typically need to be estimated; thus,
we extend our algorithm to the setting where we are given confidence intervals
for the importance weights. We demonstrate the effectiveness of our approach on
covariate shifts based on DomainNet and ImageNet. Our algorithm satisfies the
PAC constraint, and gives prediction sets with the smallest average normalized
size among approaches that always satisfy the PAC constraint.

1 INTRODUCTION

A key challenge in machine learning is quantifying prediction uncertainty. A promising approach is
via prediction sets, where the model predicts a set of labels instead of a single label. For example,
prediction sets can be used by a robot to navigate to avoid regions where the prediction set includes
an obstacle, or in healthcare to notify a doctor if the prediction set includes a problematic diagnosis.

Various methods based on these approaches can provide probabilistic correctness guarantees (i.e.,
that the predicted set contains the true label with high probability) when the training and test distributions are equal—formally, assuming the observations are exchangeable (Vovk et al., 2005; Papadopoulos et al., 2002; Lei et al., 2015) or i.i.d. (Vovk, 2013; Park et al., 2020a; Bates et al., 2021).
However, this assumption often fails to hold in practice due to covariate shift—i.e., where the input distribution changes but the conditional label distribution remains the same (Sugiyama et al.,
2007; Qui˜nonero-Candela et al., 2009). These shifts can be both natural (e.g., changes in color and
lighting) (Hendrycks & Dietterich, 2019) or adversarial (e.g., ℓ attacks) (Szegedy et al., 2014).
_∞_

We consider unsupervised domain adaptation (Ben-David et al., 2007), where we are given labeled
examples from the source domain, but only unlabeled examples from the target (covariate shifted)
domain. We propose an algorithm that constructs probably approximately correct (PAC) prediction
sets under bounded covariate shifts (Wilks, 1941; Valiant, 1984)—i.e., with high probability over


-----

0.40

0.35

0.30

0.25

0.20

0.15

0.10

0.05

0.00


0.25

0.20

0.15

0.10

0.05

0.00


n o

[brain,

  lollipop,

 sea turtle

(a) prediction set example


|Col1|Col2|Col3|Col4|Col5|tra = PS|de-off 0.10|
|---|---|---|---|---|---|---|
||||||WS PS-|CI W|
||||||||
||||||||


0 50 100 150 200 250 300 350

trade-off

= 0.10

PS
WSCI
PS-W

size

(b) size and error tradeoff

|Col1|Col2|Col3|= 0 PS WSC|.10 I|
|---|---|---|---|---|
||||PS-W||
||||||
||||||
||||||


0 50 100 150 200

= 0.10

PS
WSCI
PS-W

size

(c) results across many shifts


Figure 1: (a) An example of a covariate shifted image where an existing approach, PS (Park et al.,
2020a), constructs prediction sets that do not contain the true label (i.e., “sea turtle”); in contrast,
our proposed approach PS-W does. (b) The red curve shows the tradeoff between size and error
achieved by different choices of τ on a single shift; the goal is to be as far to the left as possible
without crossing the desired error bound ε = 0.1 shown as the black dashed line. The existing
approaches PS and WSCI (Tibshirani et al., 2019) fail to satisfy the desired error bound due to
covariate shift, whereas our approach satisfies it. (c) Our approach satisfies the error bound over
nine covariate shifts on DomainNet and ImageNet, whereas existing approaches do not.

the training data (“probably”), the prediction set contains the true label for test instances (“approximately correct”).

Our algorithm uses importance weights to capture the likelihood of a source example under the target
domain. When the importance weights are known, it uses rejection sampling (von Neumann, 1951)
to construct the prediction sets. Often, the importance weights must be estimated, in which case we
need to account for estimation error. When only given confidence intervals around the importance
weights, our algorithm constructs prediction sets that are robust to this uncertainty.

We evaluate our approach in two settings. First, we consider rate shift, where the model is trained on
a broad domain, but deployed on a narrow domain—e.g., an autonomous car may be trained on both
daytime and nighttime images but operate at night. Even if the model continues to perform well,
this covariate shift may invalidate the prediction sets. We show that our approach constructs valid
prediction sets under rate shifts constructed from DomainNet (Peng et al., 2019), whereas several
existing approaches do not. Second, we consider support shift, where the model is trained on one
domain, but evaluated on another domain with shifted support. We train ResNet101 (He et al., 2016)
using unsupervised domain adaptation (Ganin et al., 2016) on both ImageNet-C synthetic perturbations (Hendrycks & Dietterich, 2019) and PGD adversarial perturbations (Madry et al., 2017) to
ImageNet (Russakovsky et al., 2015). Our PS-W algorithm satisfies the PAC constraint, and gives
prediction sets with the smallest average normalized size among approaches that always satisfy the
PAC constraint. See Figure 1 for a summary of our approach and results.

**Related work. Our work is related to conformal prediction (Vovk et al., 2005; Balasubramanian**
et al., 2014) without a shift, where the goal is to construct models that predict sets of labels designed
to include the true label with high probability (instead of predicting individual labels) under the
assumption that the source and target distributions are the same. In particular, our setting is related
to inductive (or split) conformal prediction (Papadopoulos et al., 2002; Vovk, 2013; Lei et al., 2015),
where the training set is split into a proper training set used to train a traditional predictive model,
and a calibration set used to construct the prediction sets. The general approach is to train a model
_f : X × Y →_ R that outputs conformity scores, and then to choose a threshold τ ∈ R that satisfies a
correctness guarantee, where the corresponding prediction sets are C(x) = {y ∈Y | f (x, y) ≥ _τ_ _}_
(Park et al., 2020a; Gupta et al., 2021); this notation is defined in Section 2.

Several kinds of correctness guarantees under no shift have been considered. One possibility is input
_conditional validity (Vovk, 2013; Barber et al., 2020), which ensures correctness for all future co-_
variates x, with high probability only over the conditional label distribution p(y | x). This guarantee
is very strong, and hard to ensure in practice. A weaker notion is approximate (Lei & Wasserman,
2014; Barber et al., 2020) or group (Romano et al., 2019) conditional validity, which ensures correctness with high probability over p(y | x) as well as some distribution p(x) over a subgroup.
Finally, unconditional validity ensures only correctness over the joint distribution p(x, y). We focus
on unconditional validity, though our approach extends readily to group conditional validity.


-----

A separate issue, arising in the no shift setting, is how to condition on the calibration set Z. A
conventional goal is unconditional validity, over the distribution p(x, y, Z). We refer to this strategy
as fully unconditional validity. However, the guarantee we consider uses a separate confidence level
for the training data, which is called a training conditional guarantee (Vovk, 2013); this correctness
notion is equivalent to a PAC correctness guarantee (Park et al., 2020a), and is also equivalent to
the standard “content” guarantee with a certain confidence level for tolerance regions (Wilks, 1941;
Fraser, 1956). We build on Park et al. (2020a), which formulates the problem of choosing τ as
learning a binary classifier where the input and parameter spaces are both one-dimensional; thus,
the correctness guarantee corresponds to a PAC generalization bound. This approach is widely
applicable since it can work with a variety of objectives (Bates et al., 2021).

Recent work has extended inductive conformal prediction to a setting with covariate shift (Tibshirani
et al., 2019; Lei & Cand`es, 2020); similarly, Podkopaev & Ramdas (2021) considers conformal
prediction under label shift (Lipton et al., 2018), i.e., assuming the conditional probabilities p(x |
_y) do not change. These approaches start from the assumption that the true importance weights_
(IWs) are known (Tibshirani et al., 2019; Podkopaev & Ramdas, 2021), or assume some properties
of the estimated IWs (Lei & Cand`es, 2020), whereas our approach considers a special form of
“ambiguity” in the estimated IWs. Furthermore, they are focused on fully unconditional validity,
whereas we obtain PAC prediction sets. In addition, Cauchois et al. (2020) designs confidence sets
that are robust to all distribution shifts with bounded f -divergence; in contrast, we consider the
unsupervised learning setting where we have unlabeled examples from the target distribution. We
provide additional related work in Appendix A.

2 BACKGROUND ON PAC PREDICTION SETS

We give background on PAC prediction sets (Park et al., 2020a); we also re-interpret this approach
using Clopper-Pearson confidence intervals (Clopper & Pearson, 1934), which aids our analysis.

2.1 PAC PREDICTION SETS ALGORITHM

Let x ∈X be covariates and y ∈Y be labels. We consider a source distribution P over X × Y with
probability density function (PDF) p(x, y).[1] A prediction set[2] is a set-valued function C : X → 2[Y] .

**Inputs. We are given a held-out calibration set Sm of i.i.d. samples (xi, yi) ∼** _P for i ∈_ [m] :=
1, . . ., m, written as Sm _P_ _[m], and a score function f :_ R 0. For example, f (x, y)
_{can be a prediction for the probability that}_ _∼_ _y is the label for x X × Y →. The score function can be arbitrary,≥_
but score functions assigning higher scores to likely labels yield smaller prediction sets.

**PAC prediction set. A PAC prediction set is a set-valued function C : X →** 2[Y] satisfying the
following two conditions. First, C is approximately correct in the sense that its expected error
(failing to contain the true label) is bounded by a given ε ∈ (0, 1), i.e.,

_LP (C) := E(x,y)_ _P [1(y /_ _C(x))] = P(x,y)_ _P [y /_ _C(x)]_ _ε._
_∼_ _∈_ _∼_ _∈_ _≤_

is approximately correct must be satisfied with high probability—i.e., for givenSecond, noting that CSm is constructed based on a calibration set Sm ∼ _P_ _[m], the condition that δ ∈_ (0, 1), we have CSm

PSm _P_ _[m] [LP (CSm)_ _ε]_ 1 _δ._
_∼_ _≤_ _≥_ _−_

Our goal is to devise an algorithm for constructing a PAC prediction set C. Letting C(x) = Y for
used as a prediction set. Therefore, we additionally want to minimize the expected sizeall x ∈X always satisfies both conditions above, but this extreme case would be uninformative if E[S(C(x))]
of the prediction sets C(x), where S : 2[Y] R 0 is a size measure, which is application specific
_→_ _≥_
(e.g., the cardinality of a set in classification); however, we only rely on the monotonicity of the size
measure with respect to the prediction set parameterization in construction.

1All quantities that we define are measurable with respect to a fixed σ-algebra on X × Y; for instance, p
is the density induced by a fixed σ-finite measure. To be precise, we consider a probability measure P defined
with respect to the base measure M on X ×Y; then, p = dP/dM is the Radon-Nykodym derivative of P with
respect to2We use the term “prediction set” to denote both the set-valued function and a set output by this function. M . For classification, M is the product of a Lebesgue measure on X and a counting measure on Y.


-----

**Algorithm. To construct C, we first define the search space of possible prediction sets along with**
the size measure S. We parameterize C by a scalar τ := R 0 as
_∈T_ _≥_

_Cτ_ (x) = _y_ _f_ (x, y) _τ_ _,_
_{_ _∈Y |_ _≥_ _}_

i.e., τ is the threshold on f (x, y) above which we include y in C(x). Importantly, τ controls the
tradeoff between size and expected error. The reason is that if τ1 _τ2, then Cτ2_ (x) _Cτ1_ (x)
for all x . Thus, size is monotonically decreasing in τ —i.e., S ≤(Cτ2 (x)) _S(Cτ1 ⊆(x)) for all_
_∈X_ _≤_
_x ∈X_, and error is monotonically increasing in τ —i.e., LP (Cτ1 ) ≤ _LP (Cτ2_ ). See Figure 1b for
an illustration, and Park et al. (2020a) and Gupta et al. (2021) for details.

As a consequence, a typical goal is to construct Cτ that provably contains the true label with high
probability, while empirically minimizing size (Vovk et al., 2005; Gupta et al., 2021). In our setting, we want to maximize τ (equivalently, minimize expected size) subject to the constraint that
_Cτ is PAC. Let_ _L[¯]Sm(Cτ_ ) := (x,y) _Sm_ [1][(][y /] _Cτ_ (x)) be the empirical error count on Sm, and

_∈_ _∈_

_F_ (k; m, ε) = _i=0_ _mk_ _ε[i](1_ _ε)[m][−][i]_ be the cumulative distribution function (CDF) of the binomial
_−_
distribution Binom(m, ε) with m[P] trials and success probability ε. In prior work, Park et al. (2020a)
  
constructs Cτˆ [P][by solving the following problem:][k]

_τˆ = maxτ_ _[τ]_ subj. to _L¯Sm_ (Cτ ) ≤ _k(m, ε, δ),_ (1)
_∈T_

where k(m, ε, δ) = maxk∈N∪{0} k subj. to F (k; m, ε) ≤ _δ. Intuitively, the PAC guarantee via this_
construction is related to the Binomial distribution; _L[¯]Sm(C) has distribution Binom(m, LP (C))_
(given a fixed C), since 1(y ̸∈ _C(x)) has a Bernoulli(LP (C)) distribution when (x, y) ∼_ _P_ . Thus,
_k(m, ε, δ) defines a “confidence interval” such that if_ _L[¯]Sm_ (C) ≤ _k(m, ε, δ), then LP (C) ≤_ _ε_
with probability at least 1 − _δ. Below, we formalize this intuition by drawing a connection to the_
Clopper-Pearson confidence interval.

2.2 CLOPPER-PEARSON INTERPRETATION

We interpret (1) using the Clopper-Pearson (CP) upper bound θ(k; m, δ) ∈ [0, 1] (Clopper &
Pearson, 1934; Park et al., 2021). This is an upper bound on the true success probability µ,
constructed from a samplePk∼Binom(m,µ)[µ ≤ _θ(k; m, δ k)] ∼ ≥_ Binom1 − _δ, where(m, µ), which holds with probability at least 1 −_ _δ, i.e.,_

_θ(k; m, δ) := inf {θ ∈_ [0, 1] | F (k; m, θ) ≤ _δ} ∪{1}._ (2)

Intuitively, for a fixed C, _L[¯]Sm_ (C) ∼ Binom(m, LP (C)), so the true error LP (C) is contained in
the CP upper bound θ(LSm (C); m; δ) with probability at least 1 _δ. Intuitively, we can therefore_
_−_
choose τ so that this upper bound is ≤ _ε. However, we need to account for generalization error of_
our estimator. To this end, we have the following (see Appendix D.1 for a proof and Algorithm 2 in
Appendix E for implementation details on the corresponding algorithm):

_of the following problemTheorem 1 Let UCP(C, S[3]:m, δ) := θ(L[¯]Sm(C); m; δ), where θ is defined in (2). Let ˆτ be the solution_

_τˆ = max_ _subj. to_ _UCP(Cτ_ _, Sm, δ)_ _ε._ (3)
_τ_ _[τ]_ _≤_
_∈T_

_Then, we have PSm_ _P m_ [LP (Cτ ) _ε]_ 1 _δ for any τ_ _τˆ._
_∼_ _≤_ _≥_ _−_ _≤_

3 PAC PREDICTION SETS UNDER COVARIATE SHIFT

We extend the PAC prediction set algorithm described in Section 2 to the covariate shift setting. Our
novel approach combines rejection sampling (von Neumann, 1951) with Theorem 1.

3We consider a trivial solution τ = 0 when (3) is not feasible, but omitting here to avoid clutter.


-----

3.1 PROBLEM FORMULATION

We assume labeled training examples from the source distribution P are given, but want to construct
prediction sets that satisfy the PAC property with respect to a (possibly) shifted target distribution Q.
Both of these are distributions over X × Y; denote their PDFs by p(x, y) and q(x, y), respectively.
The challenge is that we are only given unlabeled examples from Q.

**Preliminaries and assumptions.** We denote the likelihood ratio of covariate distributions by
_w[∗](x) := q(x)/p(x), also called the importance weight (IW) of x. Our main assumption is the_
_covariate shift condition, which says the label distributions are equal (i.e., p(y | x) = q(y | x)),_
while the covariate distributions may differ (i.e., we can have p(x) ̸= q(x)).

**Inputs. We assume a labeled calibration set Sm consisting of i.i.d. samples (xi, yi) ∼** _P (for_
_i_ [m]) is given, and a score function f : R 0. For now, we also assume we have
_∈_ _X × Y →_ _≥_
the true importance weights wi[∗] [:=][ w][∗][(][x][i][)][ for each][ (][x][i][, y][i][)][ ∼] _[P]_ [, as well as an upper bound]
_b_ maxx _w[∗](x) on the IWs. In Sections 3.3 and Appendix B, we describe how to estimate these_
_≥_ _∈X_
quantities in a way that provides guarantees under smoothness assumptions on the distributions.[4]

**Problem. Our goal is to construct CSm that is a PAC prediction set under Q—i.e.,**

PSm∼P m [LQ(CSm) ≤ _ε] ≥_ 1 − _δ,_

where LQ(C) := P(x,y)∼Q[y ̸∈ _C(x)] is the error of C on Q, while empirically controlling its size._

3.2 REJECTION SAMPLING STRATEGY

Our strategy is to use rejection sampling to convert Sm consisting of i.i.d. samples from P into a
labeled calibration set consisting of i.i.d. samples from Q.

**Rejection sampling. Rejection sampling (von Neumann, 1951; Owen, 2013; Rubinstein & Kroese,**
2016) is a technique for generating samples from a target distribution q(x) based on samples from
a proposal distribution p(x). Given importance weights (IWs) w[∗](x) and an upper bound b ≥
maxx _w[∗](x), it constructs a set of i.i.d. samples from q(x). Typically, rejection sampling is used_
_∈X_
when the proposal distribution is easy to sample compared to the target distribution. In contrast, we
use it to convert samples from the source distribution into samples from the target distribution.

In particular, our algorithm takes the proposal distribution to be the source covariate distribution PX,
and the target distribution to be our target covariate distribution QX . Let Vi _U := Uniform([0, 1])_
be i.i.d., V := (V1, . . ., Vm), and ⃗w[∗] := (w1[∗][, . . ., w]m[∗] [)][ with][ w]i[∗] [=][ w][∗][(][x][i][)][ as defined before. Then,] ∼
given Sm, it uses rejection sampling to construct a randomly chosen set of N samples

_TN_ (Sm, V, ⃗w[∗], b) := {(xi, yi) ∈ _Sm | Vi ≤_ _wi[∗][/b][}]_ (4)

from QX . The expected number of samples is E[N ] = m/b; thus, rejection sampling is more
effective when the proposal distribution is similar to the target.

**Rejection sampling Clopper-Pearson (RSCP) bound. Given TN**, our algorithm uses the CP
bound to construct a PAC prediction setexample|TN (Sm (, V, ⃗xiw, y[∗]i, b) ∈)| =Sm is accepted—i.e.,i=1 _[σ][i][. Then, it follows that] C T forN_ (S Qm, V, ⃗. Let[ U]w[RSCP][∗] σ, b[ bounds the error on]i) = := { 1(x(Vi, yi ≤i) ∈wi[∗]S[/b]m[ Q][)] |[ indicate whether][, where] σi = 1},where

_URSCP(C, Sm, V, ⃗w[∗], b, δ) := UCP(C, TN_ (Sm, V, ⃗w[∗], b), δ). (5)

[P][m]

Thus, we have the following (see Appendix D.3 for the proof, and Algorithm 4 in Appendix E for a
detailed description of the PS-R algorithm that leverages this bound):

**Theorem 2 Define URSCP as in (5). Let ˆτ be the solution of the following problem:**


_τˆ = max_ _subj. to_ _URSCP(Cτ_ _, Sm, V, ⃗w[∗], b, δ)_ _ε._ (6)
_τ_ _[τ]_ _≤_
_∈T_

_Then, we have PSm∼P m,V ∼U m [LQ(Cτ_ ) ≤ _ε] ≥_ 1 − _δ for any τ ≤_ _τˆ._

Note that V is sampled only once for Algorithm 4, so the randomness in V can contribute to the
failure of the correctness guarantee; however, the failure rate is controlled by δ.

4In practice, we can improve stability of importance weights by considering source and target distributions
induced by a feature mapping, e.g., learned using unsupervised domain-adaptation (Park et al., 2020b).


-----

3.3 APPROXIMATE IMPORTANCE WEIGHTS

So far, we have assumed that the true importance weight w[∗](x) is known. Since in practice, it needs
to be estimated, we relax this assumption to only needing an uncertainty set of possible importance
weights. This allows us to handle estimation error in the weights.

**Problem. Let Sm[X]** [be unlabeled calibration examples from the source distribution (i.e., the covariates]
in Sm), Tn[X] [be][ n][ unlabeled calibration examples from the target distribution (denoted by][ T][ X]n _∼_
_Q[n]X_ [), and][ ⃗]w[∗] := (w1[∗][, ..., w]m[∗] [)][ ∈] [R][m][ be the vector of true importance weights][ w]i[∗] [:=][ w][∗][(][x][i][)][, for]
(xi, yi) ∈ _Sm. Then, we assume an uncertainty set W ⊆_ R[m] that has ⃗w[∗] with high probability, i.e.,

PSmX _[∼][P][ m]X_ _[,T][ X]n_ _[∼][Q]X[n]_ [[]w[⃗] _[∗]_ _∈W] ≥_ 1 − _δw,_ (7)

where δw (0, 1). We assume has the form
_∈_ _W_

:= _w_ R[m] _i_ [m], wi _wi_ _wi_ _,_
_W_ _{_ _∈_ _| ∀_ _∈_ _≤_ _≤_ _}_

for some wi and wi. See the discussion on our choice of W in Appendix C.1.

**Robust Clopper-Pearson bound. To construct a PAC prediction set C for Q, it suffices to bound**
the worst-case error over w ∈W, i.e., we have the following (see Appendix D.4 for the proof):

**Theorem 3 Suppose W satisfies (7). Define URSCP as in (5). Let ˆτ be the solution of the following:**

_τˆ = max_ _subj. to_ max (8)
_τ_ _[τ]_ _w_ _[U][RSCP][(][C][τ]_ _[, S][m][, V, w, b, δ][C][)][ ≤]_ _[ε.]_
_∈T_ _∈W_

_Then, we have PSm_ _P m,V_ _U m,T Xn_ _X_ [[][L][Q][(][C][τ] [)][ ≤] _[ε][]][ ≥]_ [1][ −] _[δ][C][ −]_ _[δ][w][ for any][ τ][ ≤]_ _τ[ˆ]._
_∼_ _∼_ _[∼][Q][n]_

A key challenge applying Theorem 3 is solving the maximum over w ∈W. We propose a simple
greedy algorithm that achieves the maximum.

**Greedy algorithm for robust URSCP. The RSCP bound URSCP satisfies certain monotonicity prop-**
erties that enable us to efficiently compute an upper bound to the maximum in (8). In particular,
ifw Ci[∗] makes an error on[=][ w][∗][(][x][i][)][; intuitively, this holds since a larger] (xi, yi) (i.e., yi ̸∈ _C(xi)), then[ w] Ui[∗]_ RSCP[increases the probability that] is monotonically non-decreasing in[ (][x][i][, y][i][)][ is]
included in TN (Sm, V, w[∗], b), which in turn increases the empirical error _L[¯]TN_ (Sm,V,w∗,b)(C). ConMore formally, we have the following result (see Appendix D.5 for a proof):versely, if C does not make an error on (xi, yi) (i.e., yi ∈ _C(xi)), URSCP is non-increasing in wi[∗][.]_

**Lemma 1 For any i ∈** [m], URSCP(C, Sm, V, w[∗], b, δ) is monotonically non-decreasing in wi[∗] _[if]_
_yi /∈_ _C(xi), and monotonically non-increasing in wi[∗]_ _[if][ y][i][ ∈]_ _[C][(][x][i][)][.]_

Our greedy algorithm leverages the monotonicity of URSCP. In particular, given W and C, the choice

_wˆ := ( ˆw1, . . ., ˆwm),_ where _wˆi =_ _wi_ if yi ̸∈ _C(xi)_ ( _i_ [m]) (9)
_wi_ if yi _C(xi)_ _∀_ _∈_
 _∈_

is the maximum value over w ∈W of the constraint URSCP(Cτ _, Sm, V, w, b, δC) in (8), i.e.,_

max _w, b, δ)._ (10)
_w_ _[U][RSCP][(][C, S][m][, V, w, b, δ][) =][ U][RSCP][(][C, S][m][, V,][ ˆ]_
_∈W_

Thus, we have the following, which follows by (10) and the same argument as the Theorem 3:

**Theorem 4 Suppose W satisfies (7). Define ˆwτ,SmX** _[,T][ X]n_ _[as in (9), making the dependency on][ τ]_ _[,][ S]m[X]_ _[,]_
_and Tn[X]_ _[explicit, and][ U][RSCP]_ _[as in (5). Let][ ˆ]τ be the solution of the following problem:_

_τˆ = maxτ_ _[τ]_ _subj. to_ _URSCP(Cτ_ _, Sm, V, ˆwτ,SmX_ _[,T][ X]n_ _[, b, δ][C][)][ ≤]_ _[ε.]_ (11)
_∈T_

_Then, we have PSm_ _P m,V_ _U m,T Xn_ _X_ [[][L][Q][(][C][τ] [)][ ≤] _[ε][]][ ≥]_ [1][ −] _[δ][C][ −]_ _[δ][w][ for any][ τ][ ≤]_ _τ[ˆ]._ [5]
_∼_ _∼_ _[∼][Q][n]_

**Importance weight estimation. In general, to estimate the importance weights (IWs), some as-**
sumptions on their structure are required (Kanamori et al., 2009; Cortes et al., 2008; Nguyen et al.,

5We assume b is given for simplicity, but our approach estimates it; see Appendix C.2 for details.


-----

**Algorithm 1 PS-W: an algorithm using the robust RSCP bound in (20)**

1: procedure PS-W(Sm, Tn[X] _[, f, g,][ T][, ε, δ]w[, δ]C_ _[, K, E][)]_

2: _W ←_ ESTIMATEIWS(Sm[X] _[, T][ X]n_ _[, g, δ]w[, K, E][)]_ (▷) Compute an uncertainty set W

3: _b_ maxi [K] wi (▷) Compute the maximum IW (see (19) in Appendix C.2)
_←_ _∈_

4: **return PS-ROBUST(Sm, f, T, W, b, ε, δC** )

5: procedure ESTIMATEIWS(Sm[X] _[, T][ X]n_ _[, g, δ]w[, K, E][)]_

6: Construct bins B1, . . ., BK using g as described in Appendix B.2

7: Construct W using B1, . . ., BK, Sm[X] _[, T][ X]n_ _[, g, δ]w_ [and,][ E][ as described in Appendix B.1]

8: **return W**

9: procedure PS-ROBUST(Sm, f, g, T, W, b, ε, δC )

10: _V ∼_ Uniform([0, 1])[m]

11: _τˆ ←_ 0

12: **for τ ∈T do** (▷) Grid search in ascending order

13: Construct ˆw using (9) given τ, Sm, and W

14: **if URSCP(Cτ** _, Sm, V, ˆw, b, δC_ ) ≤ _ε then_

15: _τˆ ←_ max(ˆτ, τ )

16: **else break**

17: **return ˆτ**


2010; Lipton et al., 2018). We use a cluster-based approach (Cortes et al., 2008). In particular, we
heuristically partition the feature space of the score function f into K bins by using a probabilistic
classifier g that separates source and target examples, and then estimate the source and target covariate distributions p(x) and q(x) based on the smoothness assumption over the distributions, where
the degree of the smoothness is parameterized by E. Then, we can construct the uncertainty set W
that satisfies the specified guarantee in (7). See Appendix B for details.

**Algorithm. Our algorithm, called PS-W, is detailed in Algorithm 1; it solves (20) and also performs**
importance weight estimation according to (17) in Appendix B. In particular, Algorithm 1 constructs
a prediction set that satisfies the PAC guarantee in Theorem 4. See Section 4.1 for our choice of
parameters (e.g., K, E, and grid search parameters).

4 EXPERIMENTS

We show the efficacy of our approach on rate and support shifts on DomainNet and ImageNet.

4.1 EXPERIMENTAL SETUP
**Models. For each source-target distribution pair, we split each the labeled source data and unlabeled**
target data into train and calibration sets. We use a separate labeled test set from the target for
evaluation. For each shift from source to target, we use a deep neural network score function f
based on ResNet101 (He et al., 2016), trained using unsupervised domain adaptation based on the
source and target training sets. See Appendix F for details, including data split.

**Prediction set construction. To construct our prediction sets, we first estimate IWs by training**
a probabilistic classifier g using the source and target training sets. Next, we use g to construct
heuristic IWs w(x) = 1/g(s = 1 | x) − 11, where s = 1 if x is from source. Then, we estimate the
lower and upper bound of the true IWs using Theorem 5 with E = 0.001 and K = 10 bins (chosen
to contain equal numbers of heuristic IWs), where we compute the lower and upper Clopper-Pearson
interval using the source and target calibration sets. Furthermore, given a confidence level δ, we use
_δC = δw = δ/2. For the grid search in line 12 of Algorithm 1, we increase τ by 10[−][7]_ until the
bound URSCP exceeds 1.5ε. Finally, we evaluate the prediction set error on the labeled target test set.

**Baselines. We compare the proposed method in Algorithm 1 (PS-W) with the following:**

-  PS: The prediction set using Algorithm 2 that satisfies the PAC guarantee from Theorem 1, based
on Park et al. (2020a), which makes the i.i.d. assumption.

-  PS-C: A Clopper-Pearson method addressing covariate shift by a conservative upper bound on the
empirical loss (see Appendix E.2 for details), resulting in Algorithm 3 that solves the following:

_τˆ = arg maxτ_ _∈T_ _τ_ subj. to _UCP(Cτ_ _, Sm, δ) ≤_ _ε/b._

-  WSCI: Weighted split conformal inference, proposed in (Tibshirani et al., 2019). Under the exchangeability assumption (which is somewhat weaker than our i.i.d. assumption), this approach


-----

error

PS WSCI PS-C PS-W


error

PS-R PS-M PS-W




0.10

0.08

0.06

0.04

0.02

0.00


0.12

0.10

0.08

0.06

0.04

0.02

0.00

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
||=|0.10|||

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
||= 0.1|0||


= 0.10


= 0.10


size

1000

800

600

400

prediction set size 200

0
PS WSCI PS-C PS-W

size

1000

800

600

size

400

200

0
PS-R PS-M PS-W

(a) Comparison (b) Ablation study

Figure 2: The prediction set error and size over 100 random trials under synthetic shift from ImageNet to ImageNet-C13. Parameters are m = 20, 000, ε = 0.1, and δ = 10[−][5]. (a) and (b) shows
the box plots for comparison and ablation study, respectively.

provides correctness guarantees only for a single future test instance, i.e., it includes an ε confidence level (denoted α in their paper) but not δ. Also, it assumes the true IWs are known;
following their experiments, we use the heuristic IWs constructed using g.

-  PS-R: The prediction set using Algorithm 4 that satisfies the PAC guarantee from Theorem 2 with
the heuristic IWs constructed using a probabilistic classifier g.

-  PS-M: The prediction set using Algorithm 5, which is identical to PS-R except that PS-M estimates IWs heuristically using histogram density estimation and a calibration set. In particular, we
partition the IW values into bins, project source and target calibration examples into bins based
on the value of the probabilistic classifier g, and estimate the source density ˆpB and target density
_qˆB for each bin. The estimated importance weight is ˆw(x) = ˆqB(x)/pˆB(x), where ˆpB and ˆqB are_
defined in (14) and (15), respectively.

**Metrics. We measure performance via the prediction set error and size on a held-out test set—i.e.,**
error is the fraction of (x, y) such that y ̸∈ _C(x) and size is the average of |C(x)| over x._

**Rate shifts on DomainNet. We consider settings where the model is trained on data from a variety**
of domains, but is then deployed on a specific domain; we call such a shift rate shift. For instance,
a self-driving car may be trained on both day and night images, but tested during the night time.
While the model should still perform well since the target is a subset of the source, the covariate shift
can nevertheless invalidate prediction set guarantees. We consider rate shifts on DomainNet (Peng
et al., 2019), which consists of images of 345 classes from six domains (sketch, clipart, painting,
quickdraw, real, and infograph); we use all domains as the source and each domain as a target.

**Support shifts on ImageNet. Next, we consider support shifts, where the support of the target is**
different from the support of the source, but unsupervised domain adaptation is used to learn feature representations that align the two distributions (Ganin et al., 2016); then, the score function f
is trained on this representation. First, we consider ImageNet-C (Hendrycks & Dietterich, 2019),
which modifies the original ImageNet dataset (Russakovsky et al., 2015) using 15 synthetic perturbations with 5 severity levels. We use 13 perturbations, omitting “snow” and “glass blur”, which
are computationally expensive to run. We consider the original ImageNet dataset as the source,
and the all synthetic perturbations on all of ImageNet (denoted ImageNet-C13) as the target. See
Appendix F.3 for data split. Second, we consider adversarial shifts, where we generate adversarial
examples for ImageNet using the PGD attack (Madry et al., 2017) with 0.01 ℓ -norm perturbations
_∞_
with respect to a pretrained ResNet101. We consider the original ImageNet as the source and the
adversarially perturbed ImageNet as the target.


4.2 EXPERIMENTAL RESULTS

We summarize our results in Table 1, and provide more details in Figure 4 in Appendix G.2. Our
PS-W algorithm satisfies the PAC constraint, and gives prediction sets with the smallest average
normalized size among approaches that always satisfy the PAC constraint.

**Rate shifts on DomainNet. We use ε = 0.1 and δ = 10[−][5]. As can be seen in Table 1, the**
prediction set error of our approach (PS-W) does not violate the error constraint ε = 0.1, while
all other comparing approaches (except for PS-C) violate it for at least one of the shifts. While
PS-C satisfies the desired bound, it is overly conservative; its prediction sets are significantly larger
than necessary, making it less useful for uncertainty quantification. For the shift to Infograph in
Table 1, PS-W achieves relatively larger average prediction set size compared to other shifts; this is
because the classification error of the score function f over Infograph is large—71.28%, whereas


-----

Table 1: Average prediction set error and sizes over 100 random trials under rate shifts on DomainNet (first six shifts) and support shifts on ImageNet (last two shifts). We denote an approach
satisfying the ε error constraint by , and  otherwise. The “normalized size” is the size divided
by the total number of classes (i.e., 345 for DomainNet and 1000 for ImageNet). Parameters are
_m = 50, 000 for DomainNet, m = 20, 000 for ImageNet, ε = 0.1, and δ = 10[−][5]. Our approach_
PS-W satisfies the ε constraint, while producing prediction sets with the smallest average normalized
size among approaches that always satisfy the error constraint. See Appendix G.2 for box plots.

|Shift|Baselines|Col3|Col4|Ablations|Col6|Ours|
|---|---|---|---|---|---|---|
||PS|WSCI|PS-C|PS-R|PS-M|PS-W|
||error size|error size|error size|error size|error size|error size|
|All| 10.5 (0.094)| 9.5 (0.099)| 10.7 (0.093)| 10.6 (0.094)| 10.8 (0.094)| 17.0 (0.070)|
|Sketch| 13.1 (0.142)| 18.6 (0.116)| 141.7 (0.020)| 28.2 (0.097)| 26.1 (0.105)| 40.3 (0.078)|
|Painting| 15.4 (0.159)| 30.0 (0.113)| 125.4 (0.025)| 37.7 (0.096)| 34.5 (0.103)| 52.8 (0.076)|
|Quickdraw| 5.9 (0.069)| 3.8 (0.097)| 23.8 (0.021)| 4.3 (0.088)| 4.2 (0.087)| 6.1 (0.067)|
|Real| 8.7 (0.079)| 7.2 (0.087)| 47.8 (0.032)| 8.7 (0.080)| 7.1 (0.087)| 11.8 (0.068)|
|Clipart| 10.2 (0.105)| 10.9 (0.101)| 345.0 (0.000)| 19.4 (0.080)| 14.8 (0.086)| 25.7 (0.060)|
|Infograph| 36.4 (0.363)| 165.1 (0.114)| 345.0 (0.000)| 202.6 (0.085)| 177.4 (0.107)| 216.4 (0.078)|
|ImageNet-PGD| 5.5 (0.090)| 4.7 (0.096)| 1000.0 (0.000)| 1000.0 (0.000)| 7.8 (0.074)| 13.9 (0.049)|
|ImageNet-C13| 9.3 (0.127)| 67.0 (0.111)| 1000.0 (0.000)| 1000.0 (0.000)| 15.9 (0.095)| 35.8 (0.061)|
|mean normalized size|–|–|0.0338|0.0257|–|0.0047|



that over Sketch is 37.16%. Even with the poor score function, our proposed approach still satisfies
the ε = 0.1 error constraint. Finally, while our approach PS-W generally performs best subject
to satisfying the error constraint, we note that our ablations PS-R and PS-M also perform well,
providing different tradeoffs. First, the performance of PS-W is significantly more reliable than
PS-R, but in some cases PS-R performs better (e.g., it produces slightly smaller prediction sets on
rate shifts but significantly larger sets on support shifts). Alternatively, PS-M consistently produces
smaller prediction sets, though it sometimes violates the ε error constraints.

**Support shifts on ImageNet. We show results for synthetic and adversarial shifts in Table 1 (and**
Figure 4 in Appendix G.2). As can be seen, the error of our approach (PS-W) is below the desired
level. PS-R performs poorly, likely due to the uncalibrated point IW estimation—we find that calibrated importance weights mitigate these issues, though accounting for uncertainty in the IWs is
necessary for achieving the desired error rate; see Appendix G.3 for details.

For adversarial shifts, the target classification error of the source-trained ResNet101 and the domainadapted ResNet101 is 99.97% and 28.05%, respectively. Domain adaptation can significantly decrease average error rate, but label predictions still do not have guarantees. However, our prediction
set controls the prediction set error rate as specified by ε. As shown in Figure 4, our prediction set
function outputs a prediction set for a given example that includes the true label at least 90% of the
time. Thus, downstream modules can rely on this guarantee for further decision-making.

**Ablation and sensitivity study. We conduct an ablation study on the effect of IW calibration and**
the smoothness parameter E for PS-W. We observe that the IW calibration considering uncertainty
intervals around calibrated IWs is required for the PAC guarantee (e.g., Figure 2b). Also, we find
that a broad range of E-s can be used to satisfy the PAC guarantee; we believe this result is due to
the use of a domain adapted score function. See Appendices G.3 & G.5 for details.

5 CONCLUSION

We propose a novel algorithm for building PAC prediction sets under covariate shift; it leverages rejection sampling and the Clopper-Pearson interval, and accounts for uncertain IWs. We demonstrate
the efficacy of our approach on natural, synthetic, and adversarial covariate shifts on DomainNet and
ImageNet. Future work includes providing optimality guarantees on the prediction set size, rigorous
estimation of the hyperparameter E, and incorporating probabilistic IW uncertainty estimates.


-----

**Ethics statement. We do not foresee any significant ethical issues with our work. One possible**
issue is that end-users may overly trust the prediction sets if they do not understand the limitations
of our approach—e.g., it is only a high probability guarantee.

**Reproducibility statement. Algorithms used for evaluation, including ours, are stated in Algorithm**
1, Algorithm 2, Algorithm 3, Algorithm 4, and Algorithm 5, along with hyperparameters for algorithms in Section 4.1 and Appendix F. Related code is released[6]. The proof of our theorems are
stated in Appendix D; the required assumption is stated in Assumption 1.

ACKNOWLEDGMENTS

This work was supported in part by ARO W911NF-20-1-0080, AFRL and DARPA FA8750-18-C-0090, NSF
award CCF 1910769, and NSF award 2031895 on the Mathematical and Scientific Foundations of Deep Learning (MoDL). Any opinions, findings and conclusions or recommendations expressed in this material are those
of the authors and do not necessarily reflect the views of the Air Force Research Laboratory (AFRL), the Army
Research Office (ARO), the Defense Advanced Research Projects Agency (DARPA), or the Department of
Defense, or the United States Government. The authors are grateful to Art Owen for helpful comments.

REFERENCES

Anastasios N Angelopoulos, Stephen Bates, Emmanuel J Cand`es, Michael I Jordan, and Lihua
Lei. Learn then test: Calibrating predictive algorithms to achieve risk control. arXiv preprint
_arXiv:2110.01052, 2021._

Vineeth Balasubramanian, Shen-Shyang Ho, and Vladimir Vovk. Conformal prediction for reliable
_machine learning: theory, adaptations and applications. Newnes, 2014._

Rina Foygel Barber, Emmanuel J. Cand`es, Aaditya Ramdas, and Ryan J. Tibshirani. The limits of
distribution-free conditional predictive inference, 2020.

Stephen Bates, Anastasios Angelopoulos, Lihua Lei, Jitendra Malik, and Michael I Jordan.
Distribution-free, risk-controlling prediction sets. arXiv preprint arXiv:2101.02703, 2021.

Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations
for domain adaptation. In Advances in neural information processing systems, pp. 137–144, 2007.

Steffen Bickel, Michael Br¨uckner, and Tobias Scheffer. Discriminative learning for differing training
and test distributions. In Proceedings of the 24th international conference on Machine learning,
pp. 81–88. ACM, 2007.

Glenn W Brier. Verification of forecasts expressed in terms of probability. Monthly weather review,
78(1):1–3, 1950.

Emmanuel J. Cand`es, Lihua Lei, and Zhimei Ren. Conformalized survival analysis, 2021.

Maxime Cauchois, Suyash Gupta, Alnur Ali, and John C. Duchi. Robust validation: Confident
predictions even when distributions shift, 2020.

Victor Chernozhukov, Kaspar W¨uthrich, and Zhu Yinchu. Exact and robust conformal inference
methods for predictive machine learning with dependent data. In Conference On Learning Theory,
pp. 732–749. PMLR, 2018.

Charles J Clopper and Egon S Pearson. The use of confidence or fiducial limits illustrated in the
case of the binomial. Biometrika, 26(4):404–413, 1934.

Corinna Cortes, Mehryar Mohri, Michael Riley, and Afshin Rostamizadeh. Sample selection
bias correction theory. In International conference on algorithmic learning theory, pp. 38–53.
Springer, 2008.

David R Cox. Two further applications of a model for binary regression. Biometrika, 45(3/4):
562–565, 1958.

[6https://github.com/sangdon/pac-ps-w](https://github.com/sangdon/pac-ps-w)


-----

Morris H DeGroot and Stephen E Fienberg. The comparison and evaluation of forecasters. Journal
_of the Royal Statistical Society: Series D (The Statistician), 32(1-2):12–22, 1983._

Adam Fisch, Tal Schuster, Tommi Jaakkola, and Regina Barzilay. Few-shot conformal prediction
with auxiliary tasks, 2021.

Donald Alexander Stuart Fraser. Nonparametric methods in statistics. John Wiley & Sons Inc, 1956.

Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural net[works. Journal of Machine Learning Research, 17(59):1–35, 2016. URL http://jmlr.org/](http://jmlr.org/papers/v17/15-239.html)
[papers/v17/15-239.html.](http://jmlr.org/papers/v17/15-239.html)

Isaac Gibbs and Emmanuel Cand`es. Adaptive conformal inference under distribution shift, 2021.

Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. arXiv preprint arXiv:1706.04599, 2017.

Chirag Gupta, Arun K. Kuchibhotla, and Aaditya K. Ramdas. Nested conformal prediction and
quantile out-of-bag ensemble methods, 2021.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
_(CVPR), pp. 770–778, 2016._

Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. Proceedings of the International Conference on Learning Represen_tations, 2019._

Ying Jin, Zhimei Ren, and Emmanuel J Cand`es. Sensitivity analysis of individual treatment effects:
A robust conformal inference approach. arXiv preprint arXiv:2111.12161, 2021.

Takafumi Kanamori, Shohei Hido, and Masashi Sugiyama. A least-squares approach to direct importance estimation. Journal of Machine Learning Research, 10(Jul):1391–1445, 2009.

Danijel Kivaranovic, Kory D Johnson, and Hannes Leeb. Adaptive, distribution-free prediction
intervals for deep networks. In International Conference on Artificial Intelligence and Statistics,
pp. 4346–4356. PMLR, 2020.

Volodymyr Kuleshov and Percy S Liang. Calibrated structured prediction. In Advances in Neural
_Information Processing Systems, pp. 3474–3482, 2015._

Volodymyr Kuleshov, Nathan Fenner, and Stefano Ermon. Accurate uncertainties for deep learning
using calibrated regression. arXiv preprint arXiv:1807.00263, 2018.

Ananya Kumar, Percy S Liang, and Tengyu Ma. Verified uncertainty calibration. In Advances in
_Neural Information Processing Systems, pp. 3792–3803, 2019._

Donghwan Lee, Xinmeng Huang, Hamed Hassani, and Edgar Dobriban. T-cal: An optimal test for
the calibration of predictive models. arXiv preprint arXiv:2203.01850, 2022.

Jing Lei and Larry Wasserman. Distribution-free prediction bands for non-parametric regression.
_Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(1):71–96, 2014._

Jing Lei, Alessandro Rinaldo, and Larry Wasserman. A conformal prediction approach to explore
functional data. Annals of Mathematics and Artificial Intelligence, 74(1):29–43, 2015.

Lihua Lei and Emmanuel J Cand`es. Conformal inference of counterfactuals and individual treatment
effects. arXiv preprint arXiv:2006.06138, 2020.

Sarah Lichtenstein, Baruch Fischhoff, and Lawrence D Phillips. Calibration of probabilities: The
state of the art. Decision making and change in human affairs, pp. 275–324, 1977.


-----

Zachary Lipton, Yu-Xiang Wang, and Alexander Smola. Detecting and correcting for label shift with
black box predictors. In International conference on machine learning, pp. 3122–3130. PMLR,
2018.

Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.

Ali Malik, Volodymyr Kuleshov, Jiaming Song, Danny Nemer, Harlan Seymour, and Stefano Ermon. Calibrated model-based deep reinforcement learning. In International Conference on Ma_chine Learning, pp. 4314–4323, 2019._

Robert G Miller. Statistical prediction by discriminant analysis. In Statistical Prediction by Dis_criminant Analysis, pp. 1–54. Springer, 1962._

Allan H Murphy. Scalar and vector partitions of the probability score: Part i. two-state situation.
_Journal of Applied Meteorology, 11(2):273–282, 1972._

XuanLong Nguyen, Martin J. Wainwright, and Michael I. Jordan. Estimating divergence functionals
and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory,
[56(11):5847–5861, Nov 2010. ISSN 1557-9654. doi: 10.1109/tit.2010.2068870. URL http:](http://dx.doi.org/10.1109/TIT.2010.2068870)
[//dx.doi.org/10.1109/TIT.2010.2068870.](http://dx.doi.org/10.1109/TIT.2010.2068870)

Art B. Owen. Monte Carlo theory, methods and examples. 2013.

Artidoro Pagnoni, Stefan Gramatovici, and Samuel Liu. Pac learning guarantees under covariate
shift. arXiv preprint arXiv:1812.06393, 2018.

Harris Papadopoulos, Kostas Proedrou, Volodya Vovk, and Alex Gammerman. Inductive confidence
machines for regression. In European Conference on Machine Learning, pp. 345–356. Springer,
2002.

Sangdon Park, Osbert Bastani, Nikolai Matni, and Insup Lee. Pac confidence sets for deep neural
networks via calibrated prediction. In International Conference on Learning Representations,
[2020a. URL https://openreview.net/forum?id=BJxVI04YvB.](https://openreview.net/forum?id=BJxVI04YvB)

Sangdon Park, Osbert Bastani, James Weimer, and Insup Lee. Calibrated prediction with covariate
shift via unsupervised domain adaptation. In The 23rd International Conference on Artificial
_Intelligence and Statistics, 2020b._

Sangdon Park, Shuo Li, Insup Lee, and Osbert Bastani. PAC confidence predictions for deep neural network classifiers. In International Conference on Learning Representations, 2021. URL
[https://openreview.net/forum?id=Qk-Wq5AIjpq.](https://openreview.net/forum?id=Qk-Wq5AIjpq)

Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching
for multi-source domain adaptation. In Proceedings of the IEEE International Conference on
_Computer Vision, pp. 1406–1415, 2019._

John Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in large margin classifiers, 1999.

Aleksandr Podkopaev and Aaditya Ramdas. Distribution-free uncertainty quantification for classification under label shift. arXiv preprint arXiv:2103.03323, 2021.

Dimitris N Politis. Model-free prediction in regression. In Model-Free Prediction and Regression,
pp. 57–80. Springer, 2015.

Hongxiang Qiu, Edgar Dobriban, and Eric Tchetgen Tchetgen. Distribution-free prediction sets
adaptive to unknown covariate shift. arXiv preprint arXiv:2203.06126, 2022.

Joaquin Qui˜nonero-Candela, Masashi Sugiyama, Neil D Lawrence, and Anton Schwaighofer.
_Dataset shift in machine learning. Mit Press, 2009._

Yaniv Romano, Rina Foygel Barber, Chiara Sabatti, and Emmanuel J. Cand`es. With malice towards
none: Assessing uncertainty via equalized coverage, 2019.


-----

Reuven Y Rubinstein and Dirk P Kroese. Simulation and the Monte Carlo method, volume 10. John
Wiley & Sons, 2016.

Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International journal of computer vision, 115(3):211–252, 2015.

Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert Muller. Covariate shift adaptation by
importance weighted cross validation. Journal of Machine Learning Research, 8(May):985–1005,
2007.

Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In International Conference on
_Learning Representations (ICLR), 2014._

Ryan J Tibshirani, Rina Foygel Barber, Emmanuel Candes, and Aaditya Ramdas. Conformal prediction under covariate shift. Advances in Neural Information Processing Systems, 32:2530–2540,
2019.

Leslie G Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134–1142, 1984.

John von Neumann. Various techniques used in connection with random digits. In A. S. Householder, G. E. Forsythe, and H. H. Germond (eds.), Monte Carlo Method, volume 12 of National
_Bureau of Standards Applied Mathematics Series, chapter 13, pp. 36–38. US Government Printing_
Office, Washington, DC, 1951.

Vladimir Vovk. Conditional validity of inductive conformal predictors. Machine learning, 92(2-3):
349–376, 2013.

Vladimir Vovk, Alex Gammerman, and Glenn Shafer. Algorithmic learning in a random world.
Springer Science & Business Media, 2005.

Ximei Wang, Mingsheng Long, Jianmin Wang, and Michael Jordan. Transferable calibration with
lower bias and variance in domain adaptation. In H. Larochelle, M. Ranzato, R. Hadsell, M. F.
Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
19212–19223. Curran Associates, Inc., 2020. [URL https://proceedings.neurips.](https://proceedings.neurips.cc/paper/2020/file/df12ecd077efc8c23881028604dbb8cc-Paper.pdf)
[cc/paper/2020/file/df12ecd077efc8c23881028604dbb8cc-Paper.pdf.](https://proceedings.neurips.cc/paper/2020/file/df12ecd077efc8c23881028604dbb8cc-Paper.pdf)

Samuel S Wilks. Determination of sample sizes for setting tolerance limits. The Annals of Mathe_matical Statistics, 12(1):91–96, 1941._

Chen Xu and Yao Xie. Conformal prediction interval for dynamic time-series. In International
_Conference on Machine Learning, pp. 11559–11569. PMLR, 2021._

Yachong Yang, Arun Kumar Kuchibhotla, and Eric Tchetgen Tchetgen. Doubly robust calibration
of prediction sets under covariate shift. arXiv preprint arXiv:2203.01761, 2022.

Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision trees
and naive bayesian classifiers. In In Proceedings of the Eighteenth International Conference on
_Machine Learning. Citeseer, 2001._

Bianca Zadrozny and Charles Elkan. Transforming classifier scores into accurate multiclass probability estimates. In Proceedings of the eighth ACM SIGKDD international conference on Knowl_edge discovery and data mining, pp. 694–699. ACM, 2002._


-----

A ADDITIONAL RELATED WORK

**Prediction sets under i.i.d. assumption. We built our proposed approach on the known PAC**
prediction set approach (Wilks, 1941; Park et al., 2020a) due to its simplicity and sample efficiency.
As other candidates, nested conformal prediction (Vovk et al., 2005; Gupta et al., 2021) reinterprets
multiple known conformal prediction approaches using a scalar parameteriztion of prediction sets;
but different from Wilks (1941) and Park et al. (2020a), it is not known to provide PAC guarantees.
Kivaranovic et al. (2020) provides a PAC style guarantee, but their approach is limited to regression
and is sample-inefficient, i.e., it requires a sample of size O(1/ϵ[2]), while Park et al. (2020a) has a
better sample complexity, as demonstrated in their paper.

**Prediction sets under various settings. Prior prediction set algorithms have been considered in**
several settings. First, traditional conformal prediction (Vovk et al., 2005) often considers the setting
where labeled examples arrive sequentially from the same distribution; there has also been work
extending conformal prediction to the setting where the distribution is time-varying (Politis, 2015;
Chernozhukov et al., 2018; Xu & Xie, 2021; Gibbs & Cand`es, 2021). Alternatively, there has been
work on constructing risk-controlling prediction sets for supervised learning setting (Vovk, 2013;
Park et al., 2020a; Bates et al., 2021; Angelopoulos et al., 2021). Finally, there has been recent work
on conformal prediction in the meta-learning setting (Fisch et al., 2021); in particular, given a few
labeled examples drawn from the new task, their approach leverages labeled examples from previous
tasks to construct a conformal predictor for the new task, assuming the tasks are exchangeable.

**Developments after our work. After our work was made publicly available, Jin et al. (2021)**
has developed a different, robust conformal inference approach to constructing prediction sets with
estimated weights under covariate shift. Their algorithm assumes given upper and lower bounds on
the importance weights, and uses the worst-case quantile over all weights that satisfy the constraint
to set the critical values. Further, Yang et al. (2022) have developed a doubly robust approach to
construct prediction sets satisfying approximate marginal coverage under covariate shift (which can
be robust to estimating the weights and per-covariate prediction error), leveraging semiparametric
efficiency theory. Qiu et al. (2022) have developed a parallel approach for the PAC case.

**Calibration. An alternative way to quantify uncertainty is calibrated prediction (e.g., Brier, 1950;**
Cox, 1958; Miller, 1962; Murphy, 1972; Lichtenstein et al., 1977; DeGroot & Fienberg, 1983; Guo
et al., 2017, etc), which aims to ensure that among instances with a predicted confidence p, the
model is correct a fraction p of the time. Techniques have been proposed to re-scale predicted
confidences to improve calibration (Platt, 1999; Guo et al., 2017; Zadrozny & Elkan, 2001; 2002;
Kuleshov & Liang, 2015; Kuleshov et al., 2018; Malik et al., 2019); including ones with theoretical
guarantees (Kumar et al., 2019; Park et al., 2021) and ones that handle covariate shift (Park et al.,
2020b; Wang et al., 2020). There are also methods to rigorously test calibration, dating back to Cox
(1958); Miller (1962), see e.g., Lee et al. (2022) for a recent approach. These approaches provide a
qualitatively different form of uncertainty quantification compared to the one we consider.

**Rejection sampling. Rejection sampling (sometimes accept-reject sampling) is a well-known tech-**
nique (Owen, 2013; Rubinstein & Kroese, 2016) dating back at least to von Neumann (1951). We
highlight that most work on covariate shift relies on importance weighting; our rejection sampling
approach is relatively less common and more novel. In fact, to the best of our knowledge, only
Pagnoni et al. (2018) has used rejection sampling for a different problem in this area.

**IW estimation. There has been a long line of work studying the problem of estimating impor-**
tance weights (IWs), also called likelihood ratios, in a way that provides theoretical guarantees.
For instance, (Nguyen et al., 2010) provides a convergence rate analysis of IW estimators—under
smoothness assumptions, they provide a finite sample bound on the Hellinger distance between the
true IW and estimated IW. Next, (Kanamori et al., 2009) shows a similar finite sample guarantee,
assuming the true IW can be represented as a linear combination of kernels. Finally, (Cortes et al.,
2008) proposes non-parametric IW estimators, modeling the source and target distribution by histograms over clusters in sample space.

The IW estimation approaches can be used in conjuction with prediction set construction. Compared
to Cand`es et al. (2021), which only guarantees asymptotic validity under certain assumptions on
the estimated IWs, Theorem 4 provides a finite-sample correctness guarantee. Furthermore, we
explicitly describe an algorithm for approximate IWs, required by Theorem 4, in Appendix B.


-----

B IMPORTANCE WEIGHT ESTIMATION

In general, to estimate the importance weights (IWs), some assumptions on their structure are required. A number of approaches have been proposed, with varying guarantees under different assumptions (Kanamori et al., 2009; Cortes et al., 2008; Nguyen et al., 2010; Lipton et al., 2018). We
use a cluster-based approach (Cortes et al., 2008); our approach is compatible with any of these
strategies if they can be modified to provide uncertainty estimates of the IWs.

In our approach, given a partition X = _j=1_ _[B][j][ into bins, we can estimate the IWs based on the]_
fractions of source and target samples in each bin. If the partition is sufficiently fine, then we can
obtain confidence intervals around the estimated IWs with finite-sample guarantees. However, this

[S][K]
strategy requires the number of bins in the partition to be exponential in the dimension of X . Thus,
in practice, we use a heuristic to construct the partition. We describe the cluster-based approach and
our partition construction heuristic below.

B.1 CLUSTER-BASED APPROACH

We assume given unlabeled calibration sets Sm[X] [and][ T][ X]n [, where][ S]m[X] [consists of i.i.d. samples][ x][i]
_p(x) for i_ [m], and Tn[X] [consists of i.i.d. samples][ x][i] _[∼]_
speaking, the cluster-based strategy estimates the average IW in each bin ∈ _[∼]_ _[q][(][x][)][ for][ i][ ∈]_ [[][n] B[]][, respectively]j; assuming p[7][. Roughly] and q are
roughly constant in each bin, these accurately estimate the true IWs. Let j(x) be the bin containing
_x (i.e., x_ _Bj(x)), and let_
_∈_


_p(x[′]) dx[′]_ and _qB(x) := qj(x) s.t. qj =_
_Bj_


_q(x[′]) dx[′]_
_Bj_


_pB(x) := pj(x) s.t. pj =_


be the (unnormalized) approximations of the densities p and q, respectively, that are constant on
each bin. We assume that pB and qB are accurate approximations:

**Assumption 1 Given E** R 0, the partition satisfies
_∈_ _≥_


_Bj_ _|p(x) −_ _p(x[′])|dx[′]_ _≤_ _E_ and


_q(x)_ _q(x[′])_ dx[′] _E_ (j [K], _x_ _Bj)._ (12)
_Bj_ _|_ _−_ _|_ _≤_ _∈_ _∀_ _∈_


Thus, p and q are roughly constant on the partitions. In general, (12) can hold for any E ∈ R>0 if p
and q are Lipschitz continuous and each Bj is sufficiently small (see Appendix B.3 for discussion).
Then, under Assumption 1, it can be verified that

_v(x)_ _p(x)_ _pB(x)_ _E_ and _v(x)_ _q(x)_ _qB(x)_ _E_ ( _x_ ), (13)
_|_ _·_ _−_ _| ≤_ _|_ _·_ _−_ _| ≤_ _∀_ _∈X_

where v(x) = vj(x), and vj = _Bj_ [d][x][′][ is the volume of bin][ B][j][ (see the proof of Theorem 5 for the]

validity of (13)). Next, we have the following empirical estimates of pB and qB, respectively:

R

_pˆB(x) := ˆpj(x) s.t. ˆpj = [1]_ 1 (x[′] _Bj) and_ (14)

_m_ _∈_

_x[′]_ _Sm[X]_

X∈

_qˆB(x) := ˆqj(x) s.t. ˆqj = [1]_ 1 (x[′] _Bj) ._ (15)

_n_ _∈_

_x[′]_ _Tn[X]_

X∈


Now, 1(x[′] _∈_ _Bj) has distribution Bernoulli(pj) when x ∼_ _P_, thus m · ˆpj has distribution
Binom(m, pj), and pj is contained in a Clopper-Pearson interval around ˆpj with high probability; in particular, let θ be the Clopper-Pearson lower bound corresponding to the Clopper-Pearson
upper bound defined in Section 2.2, i.e., Pk∼Binom(m,µ)[µ ≥ _θ(k; m, δ)] ≥_ 1 − _δ. Then, we have_

_θ(m_ ˆpj; m, δ[′]) _pj_ _θ(m_ ˆpj; m, δ[′]) (16)
_·_ _≤_ _≤_ _·_

with probability at least 1 − _δ[′]_ with respect to the samples Sm[X] [. Combining (13) and (16), we have]
the following result (and see Appendix D.6 for a proof):

7We can use the same calibration set to construct IWs and prediction sets due to the union bound in Theorem 3.


-----

**Theorem 5 Letting δ[′]** = δw/(2K) and [v][+] := max{0, v} for all v ∈ R, we have

_qB(x); n, δ[′])_ _E][+]_ _θ (n_ ˆqB(x), n, δ[′]) + E
_w(x) := [[][θ][ (][n][ ·][ ˆ]_ _−_ _·_

_θ (m_ ˆpB(x); m, δ[′]) + E [θ (m ˆpB(x), m, δ[′]) _E][+][ (][∀][x][ ∈X]_ [)]
_·_ _[≤]_ _[w][∗][(][x][)][ ≤]_ _[w][(][x][) :=]_ _·_ _−_

(17)


_with probability at least 1 −_ _δw over Sm[X]_ _[and][ T][ X]n_ _[.]_

We use these upper and lower bounds on the IWs as the inputs w and w to our algorithm—i.e.,

_W = {w : X →_ R | ∀x ∈X _, w(x) ≤_ _w(x) ≤_ _w(x)},_

and use b = maxx _w(x) = maxj_ [K] wj as the maximum IW. If b is known, we need importance
_∈X_ _∈_
weights associated with source calibration samples Sm[X] [; thus we use a simpler form of][ W][ as follows:]

= _w_ R[m] _i_ [m], wi _wi_ _wi_ _,_
_W_ _{_ _∈_ _| ∀_ _∈_ _≤_ _≤_ _}_

where wi := w(xi) and wi := w(xi) for xi ∈ _Sm[X]_ [.]

Theorem 5 under Assumption 1 is one way to construct uncertainty set W that contains the true IWs.

**Corollary 1 Given K** N, E R 0, and δw (0, 1), suppose Assumption 1 is satisfied and _is_
_constructed using Theorem 5. Then, we have ∈_ _∈_ _≥_ _∈_ _W_

PSmX _[∼][P][ m]X_ _[,T][ X]n_ _X_ [[][w][∗] _[∈W][]][ ≥]_ [1][ −] _[δ][w][.]_

_[∼][Q][n]_

B.2 PARTITION CONSTRUCTION HEURISTIC

In general, exponentially many bins are needed to guarantee Assumption 1. Instead, we consider
an intuitive heuristic for constructing these bins, so that the importance weights w(x)—rather than
the density functions p(x) and q(x) individually—are roughly constant on each bin, inspired by
(Park et al., 2021; 2020b); a standard heuristic for estimating IWs is to train a probabilistic classifier
_g(s | x) to distinguish source and target training examples, and then use these probabilities to_
construct the IWs. In particular, define the distribution


_g[∗](x, y) = [1]_

2 _[p][(][x][)][ ·][ 1][(][s][ = 1) + 1]2_ _[q][(][x][)][ ·][ 1][(][s][ = 0)][.]_

Then, letting g[∗](y | x) be the conditional distribution, we have w[∗](x) = 1/g[∗](s = 1 | x)−1 (Bickel
et al., 2007). Thus, we train g(s | x) ≈ _g[∗](s | x) and construct bins according to w(x) = 1/g(s =_
1 | x) − 1, i.e.,

_Bj = {x ∈X | w(x) ∈_ [wj, wj+1)},

whereunlabeled training examples from a source and target, respectively; then, the set 0 = w1 ≤ _w2 ≤_ _... ≤_ _wK+1 = ∞. Finally, we describe how to train g. Let Sm[X][′][ and][ T][ X]n[′][ be]_

_Rm[X][′],n[′][ =][ {][(][x,][ 1)][ |][ x][ ∈]_ _[S]m[X][′]_ _[} ∪{][(][x,][ 0)][ |][ x][ ∈]_ _[T][ X]n[′][ }]_

consists of i.i.d. samples (x, y) ∼ _s[∗](x, y). Thus, we can train s on Rm[X][′],n[′][ using supervised]_
learning. In practice, the corresponding IW estimates w(x) can be inaccurate partly since w is likely
overfit to Rm[X][′],n[′] [, which is why re-estimating the IWs in each bin according to Theorem 5 remains]
necessary.

B.3 DENSITY ESTIMATION SATISFYING ASSUMPTION 1

We consider the following binning strategy that satisfies Assumption 1 for Lipschitz continuous
PDFs. In particular, assume the PDFs p(x) and q(x) are L-Lipschitz continuous for some norm ∥·∥.
Then, for any given E, construct each bin Bj such that


_E_

_x, x[′]_ _Bj,_
_L_ _vj_ _∀_ _∈_
_·_


_∥x −_ _x[′]∥≤_


-----

where vj is the volume of bin Bj. Then, we know that


_p(x)_ _p(x[′])_ _L_ _x_ _x[′]_
_|_ _−_ _| ≤_ _∥_ _−_ _∥≤_ _v[E]j_

_Bj_ _|p(x) −_ _p(x[′])|dx[′]_ _≤_ _E._

Z

_Bj_ _|q(x) −_ _q(x[′])|dx[′]_ _≤_ _E._

Z


so

Similarly, we have


Thus, the bins satisfy Assumption 1. Finally, assuming is the L norm, we describe one way
_∥· ∥_ _∞_
to construct bins. In particular, construct bins by taking an ε-net with ε = (E/L)[1][/][(][d][+1)], where d
is the dimension of X . Then, we have

_E_

_x_ _x[′]_ _ε =_ _[ε][d][+1]_ = _,_
_∥_ _−_ _∥≤_ _ε[d]_ _L_ _vj_

_·_

as desired.

C ADDITIONAL DISCUSSION ON IMPORTANCE WEIGHTS

C.1 APPROXIMATE IMPORTANCE WEIGHTS


In Section 3.3, we assume W has the following form:

:= _w_ R[m] _wi_ _wi_ _wi_ _._
_W_ _{_ _∈_ _|_ _≤_ _≤_ _}_

However, considering the fact that the expected importance weight is one, i.e., Ex _P [w[∗](x)] = 1,_
_∼_
the uncertainty set W that contains the true importance weights with high probability can be further
constrained as follows:

_W_ _[′]_ := {w ∈ R[m] _| ∀i ∈_ [m], wi ≤ _wi ≤_ _wi, c ≤_ [P]i[m]=1 _[w][i][ ≤]_ _[c][}]_

for some c and c. In particular, using the Hoeffding’s inequality for example, we can estimate c
and c such that w1, . . ., wm can be the part of the true importance weight w[∗] that satisfies the mass
constraint Ex _P [w[∗](x)] = 1._
_∼_

Recall that we have to find a maximizer in the uncertainty set W _[′]_ as in (8); however, due to the
additional constraint on _i=1_ _[w][i][ in][ W]_ _[′][, it is challenging to solve the maximization problem exactly.]_

C.2 MAXIMUM IMPORTANCE[P][m] WEIGHT ESTIMATION

To generalize our approach to estimate the maximum importance weight b, we redefine the uncertainty set W over importance weights as follows:

_W := {w : X →_ R | ∀x ∈X _, w(x) ≤_ _w(x) ≤_ _w(x)}_

for some w : X → R and w : X → R such that it contains the true importance weight w[∗] with high
probability—i.e.,

PSmX _[∼][P][ m]X_ _[,T][ X]n_ _X_ [[][w][∗] _[∈W][]][ ≥]_ [1][ −] _[δ][w][.]_ (18)

_[∼][Q][n]_

Given this, the maximum importance weight is obtained as follows:

ˆb = max
_x_ _[w][(][x][)][.]_
_∈X_

Considering that W can be estimated using binning as in Appendix B, the maximum importance
weight is rewritten as follows:

ˆb = max (19)
_x_ _[w][(][x][) = max]j_ [K] _[w][j][,]_
_∈X_ _∈_


-----

where


_θ (n_ ˆqj, n, δ[′]) + E
_wj :=_ _·_

[θ (m ˆpj, m, δ[′]) _E][+][ .]_
_·_ _−_

Here, θ, θ, m, n, δ[′], E, ˆpj, and ˆqj are defined in Theorem 5 and Appendix B.

The guarantee (18) implies the guarantee (7). Thus, for the maximization over the uncertainty set in
(8), we use the original definition W and the greedy algorithm for ˆw in (9). Then, Theorem 4 with
the estimated b using (19) still holds as follows:

**Theorem 6 Suppose Assumption 1 is satisfied and W is estimated using Theorem 5.** _Define_
_wˆτ,SmX_ _[,T][ X]n_ _[as in (9) and][ ˆ]bSmX_ _[,T][ X]n_ _[as in (19), making the dependency on][ τ]_ _[,][ S]m[X]_ _[, and][ T][ X]n_ _[explicit,]_
_and URSCP as in (5). Let ˆτ be the solution of the following problem:_

_τˆ = maxτ_ _[τ]_ _subj. to_ _URSCP(Cτ_ _, Sm, V, ˆwτ,SmX_ _[,T][ X]n_ _[,]_ [ˆ]bSmX _[,T][ X]n_ _[, δ][C][)][ ≤]_ _[ε.]_ (20)
_∈T_

_Then, we have PSm_ _P m,V_ _U m,T Xn_ _X_ [[][L][Q][(][C][τ] [)][ ≤] _[ε][]][ ≥]_ [1][ −] _[δ][C][ −]_ _[δ][w][ for any][ τ][ ≤]_ _τ[ˆ]._
_∼_ _∼_ _[∼][Q][n]_

C.3 CHOOSING HYPERPARAMETERS

In general, there is no systematic way to choose the smoothness parameter E and the number of bins
_K; we briefly discuss strategies for doing so._

**Number of bins K. The bins are defined in one dimensional space as described in Appendix B.2, so**
we follow the standard practice in the calibration literature for binning (Guo et al., 2017; Park et al.,
2021), where K is between 10 and 20. As we are using equal-mass binning, we choose the number
of bins so that each bin contains sufficiently many source examples (in our case, 5000 examples) for
the length of the Clopper-Pearson interval over IWs of each bin to be below some threshold (in our
case, 10[−][3]), which leads us to K = 10. We provide a sensitivity analysis in Appendix G.6.

**Smoothness parameter E. Estimating E (i.e., computing the integral of the difference of source**
and target probabilities) is intractable in general as it requires that we perform density estimation
in a high-dimensional space (i.e., 2048 in our case), and then integrate this density over each bin.
To avoid this hyperparameter selection, we choose E equal to zero or close to zero (in our case,
0.001). Intuitively, we find that binning based on the source-discriminator scores is an effective way
to group examples with similar IWs; thus, the main contribution to the uncertainty is the error of
the point estimate. We provide a sensitivity analysis on E in Appendix G.5. Importantly, note that
PS-W even with E = 0 satisfies PAC criterion. One direction for future work is devising better
strategies for choosing E.

C.4 COMPARISON WITH WSCI

**Guarantee. The main difference between WSCI and PS-W lies in the guarantee provided (rather**
than prediction set size); PS-W provides a stronger guarantee than WSCI. In particular, WSCI does
not provide a PAC guarantee. Instead, to satisfy the coverage probability guarantee, it requires a
new calibration set for every new test example. However, in practice, we usually have a single
held-out calibration set. Thus, our approach PS-W provides a guarantee that holds conditioned on
this set. This difference is illustrated in Figure 3a, which compares WSCI and PS-W given the
true importance weight. PS-W produces a larger set size than WSCI, but strictly satisfies the error
constraint. The shortcoming of WSCI can also be observed in Figure 2 in Tibshirani et al. (2019),
which empirically shows that the guarantee only holds on average over examples.

**Usage of IWs. WSCI requires the true IWs, whereas our method can use approximate IWs. Also,**
IWs are used differently. Given the true importance weights, WSCI uses the importance weights to
reweight examples in the calibration set, and PS-W uses the importance weights to generate a target
labeled calibration set using rejection sampling.


-----

D PROOFS

D.1 PROOF OF THEOREM 1

First, note that the constraint in (1) implies F (L[¯]Sm(Cτ ); m, ε) _δ; conversely, any value of τ_
_≤_
satisfying F (L[¯]Sm (Cτ ); m, ε) _δ also satisfies_ _L[¯]Sm(Cτ_ ) _k(m, ε, δ). Thus, we can rewrite (1) as_
_≤_ _≤_

_τˆ = arg max_ _τ_ subj. to _F_ (L[¯]Sm (Cτ ); m, ε) _δ._
_τ_ _∈T_ _≤_

On the other hand, if τ satisfies (3), then by definition of UCP(Cτ _, Sm, δ) = θ(k; m, δ), we have_

inf _θ_ [0, 1] _F_ (L[¯]Sm(Cτ ); m, θ) _δ_ 1 _ε,_
_{_ _∈_ _|_ _≤_ _} ∪{_ _} ≤_

which implies that F (L[¯]Sm (Cτ ); m, ε) _δ (since the infimum is obtained within the set since the_
_≤_
binomial CDF F is continuous in ε). Conversely, any value of τ satisfying F (L[¯]Sm(Cτ ); m, ε) _δ_
_≤_
also satisfies UCP(Cτ _, Sm, δ) ≤_ _ε. Thus, we can rewrite (3) as_

_τˆ = arg max_ _τ_ subj. to _F_ (L[¯]Sm (Cτ ); m, ε) _δ,_
_τ_ _∈T_ _≤_

implying (1) and (3) are equal. Thus, the claim follows from Theorem 1 of (Park et al., 2020a) and
the fact that the error LP (Cτ ) is monotonically increasing in τ . □

D.2 MONOTONICITY OF THE CLOPPER-PEARSON BOUND

The CP bound UCP enjoys certain monotonicity properties that we will need. Intuitively, the CDF
decreases as the number of observations m increases while holding the number of successes k fixed,
but increases if both m and k are increased by the same amount (i.e., holding the number of failures
_m −_ _k fixed). In particular, we have the following:_

**Lemma 2 We have θ(k; m −** 1, δ) ≥ _θ(k; m, δ) and θ(k −_ 1; m − 1, δ) ≤ _θ(k; m, δ)._

_Proof._ Recall that F (k; m, θ) is the cumulative distribution function of a binomial distribution
Binom (m, θ), or equivalently of the random variable _i=1_ _[X][i][, where][ X][i][ ∼]_ [Bernoulli][(][θ][)][ are i.i.d.]

**Decreasing case. If k ≤** _m −_ 1, then we have [P][m]

_m_ _m−1_

_Xi_ _k_ _Xi_ _k,_
_i=1_ _≤_ _⇒_ _i=1_ _≤_

X X


hence


_m_

_Xi_ _k_

" _i=1_ _≤_
X


_m−1_

P _Xi_ _k_
_⊆_ " _i=1_ _≤_

X


so F (k; m, θ) ≤ _F_ (k; m − 1, θ).

Then, we have

_θ(k; m, δ) := inf {θ ∈_ [0, 1] | F (k; m, θ) ≤ _δ} ∪{1}_
_≤_ inf {θ ∈ [0, 1] | F (k; m − 1, θ) ≤ _δ} ∪{1}_

=: θ(k; m − 1, δ),


thus θ is monotonically non-increasing in m.

**Increasing case. We have**
_m−1_

_Xi_ _k_ 1
_i=1_ _≤_ _−_ _⇒_

X


_Xi_ _k,_
_i=1_ _≤_

X


hence


_m−1_

_Xi_ _k_ 1

" _i=1_ _≤_ _−_
X


_m_

_Xi_ _k_

" _i=1_ _≤_
X


_⊆_ P


-----

so F (k − 1; m − 1, θ) ≤ _F_ (k; m, θ).

Then, we have

_θ(k; m, δ) := inf {θ ∈_ [0, 1] | F (k; m, θ) ≤ _δ} ∪{1}_
_≥_ inf {θ ∈ [0, 1] | F (k − 1; m − 1, θ) ≤ _δ} ∪{1}_

=: θ(k − 1; m − 1, δ),

thus θ is monotonically jointly non-decreasing in (m, k).

D.3 PROOF OF THEOREM 2


The rejection sampling prediction set consists of two steps: (i) generate target samples, using source
samples Sm, importance weights w, and an upper bound on their maximum value b, and (ii) construct
the Clopper-Pearson prediction set using the generated target samples.

From rejection sampling, we choose N := _i=1_ _[σ][i][ samples from][ S][m][, denoting them by][ T][N]_ [; here,]
_N ∼_ Binom (m, 1/b), and 1/b is the acceptance probability (von Neumann, 1951)—i.e.,

[P][m]

P _V_ = [1]

_[′]_ _≤_ _[w][(]b[X][)]_ _b [,]_
 

where V _[′]_ _∼_ Uniform([0, 1]). The samples in TN are independent and identically distributed, conditionally on the random number N of samples being equal to any fixed value n. The reason is that
one can view the rejection sampling algorithm proceeding in stages, iterating through the samples
one by one. The first stage starts at the very beginning, and then each stage ends when a datapoint
is accepted, followed by starting a new stage at the next datapoint. The last stage ends at the last
datapoint.

Based only on the source samples observed in one stage, rejection sampling produces a sample from
the target distribution. Thus, within each stage, we produce one sample from the target distribution, and because each stage is independent of all the other ones, conditionally on any number of
stages reached, our produced target samples are iid. Thus, we can use the Clopper-Pearson bound
conditionally on each N = n.

To this end, let ˆτ (Sm, V ) = ˆτ to explicitly denote the dependence on Sm and V, and let

_τ˜(Tn) = arg maxτ_ _∈T_ _τ_ subj. to _URSCP(Cτ_ _, Tn, δ) ≤_ _ε._

Note that conditioned on obtaining n samples using rejection sampling (i.e., _Tn(Sm, V, w, b)_ = n),
_D_ _D_ _|_ _|_
we have ˆτ (Sm, V ) = ˜τ (Tn), where = denotes equality in distribution. Then, we have

PSm _P m,V_ _U m_ _LQ(Cτˆ(Sm,V )[)][ ≤]_ _[ε]_
_∼_ _∼_

_m_

[]= PSm _P m,V_ _U_ _m[LQ(Cτˆ(Sm,V )[)][ ≤]_ _[ε][ |][ N][ =][ n][]][ ·][ P][[][N][ =][ n][]]_

_∼_ _∼_
_n=0_

Xm

= PTn _Qn_ [LQ(Cτ˜(Tn)[)][ ≤] _[ε][]][ ·][ P][[][N][ =][ n][]]_

_∼_
_n=0_

Xm

_≥_ (1 − _δ) · P[N = n]_

_n=0_

X


= 1 − _δ,_

where the inequality follows by Theorem 1. The claim follows. □

D.4 PROOF OF THEOREM 3

First, let

_τ˜ = arg maxτ_ _∈T_ _τ_ subj. to _URSCP(Cτ_ _, Sm, V, ⃗w[∗], b, δC) ≤_ _ε,_ (21)


-----

which satisfies PSm _P m,V_ _U m [LQ(Cτ˜[)][ ≤]_ _[ε][]][ ≥]_ [1][ −] _[δ]C_ [by Theorem 2. Now, with probability at]
_∼_ _∼_
least 1 _δw, we have ⃗w[∗]_ . Under this event, we have
_−_ _∈W_
_URSCP(Cτ_ _, Sm, V, ⃗w[∗], b, δC)_ max
_≤_ _w_ _[U][RSCP][(][C][τ]_ _[, S][m][, V, w, b, δ][C][)][,]_
_∈W_

so ˆτ satisfies the constraint in (21). Thus, we must have ˆτ _τ˜. By monotonicity of LQ(Cτ_ ) in τ,
_≤_
we have LQ(Cτˆ[)][ ≤] _[L]Q[(][C]τ˜[)][, which implies that]_
PSm∼P m,V ∼U m,T Xn _[∼][Q]X[n]_ [[][L][Q][(][C]τ[ˆ][)][ ≤] _[ε][]][ ≥]_ [P]Sm∼P _[m],V ∼U_ _[m][[][L]Q[(][C]τ˜[)][ ≤]_ _[ε][]][ ≥]_ [1][ −] _[δ]C[,]_

where the last step follows by Theorem 2. The claim follows by a union bound, since ⃗w[∗] _∈W with_
probability at least 1 − _δw. □_

D.5 PROOF OF LEMMA 1

Let w and v be IWs where w(xi) _v(xi) and w(xj) = v(xj) for j_ = i. Additionally, we use the
_≥_ _̸_
following shorthands:


_m_

1 _Vi_
_i=1_  _≤_ _[w][(]b[x][i][)]_

X


_nw :=_


_Tnw :=_ (xi, yi) _Sm_ _[V][i][ ≤]_ _[w][(][x][i][)]_
_∈_ _b_


_kw :=_ 1 (y /∈ _C(x)),_

(x,yX)∈Tnw

_m_

_nv :=_ 1 _Vi_ _,_

_i=1_  _≤_ _[v][(]b[x][i][)]_ 

X


_Tnv :=_ (xi, yi) _Sm_ _[V][i][ ≤]_ _[v][(][x][i][)]_ _, and_
_∈_ _b_
 

_kv :=_ 1 (y /∈ _C(x)) ._

(x,yX)∈Tnv

function of a binomial random variableHere, nw ≥ _nv since w(xi) ≥_ _v(xi). Finally, recall thati=1_ _[X][i][, where][ X] F[i]([ ∼]k; m, θ[Bern])[(] be the cumulative distribution[θ][)][.]_

**Non-decreasing case. If yi /∈** _C(xi), there are two cases to consider:[P][m]_

1. If _[v][(]b[x][i][)]_ _< Vi_ _b_, then we can verify that nw = nv + 1 and kw = kv + 1.

_≤_ _[w][(][x][i][)]_

2. Otherwise, we can verify that nw = nv and kw = kv.

in Lemma 2, we haveIn both cases, kw ≥ _nv and nw ≥_ _nv. Since θ is monotonically jointly non-decreasing in (m, k) as_
_URSCP(C, Sm, V, w, b, δ) := UCP(L[¯]Tnw (C), δ)_

:= θ(kw; nw, δ)

_θ(kv; nv, δ)_
_≥_

=: UCP(L[¯]Tnv (C), δ)

=: URSCP(C, Sm, V, v, b, δ),
thus URSCP is monotonically non-decreasing in w(xi).

**Non-increasing case.as in Lemma 2, we have If yi ∈** _C(xi), then kw = kv. Since θ is monotonically non-increasing in m_
_URSCP(C, Sm, V, w, b, δ) := UCP(L[¯]Tnw (C), δ)_

:= θ(kw; nw, δ)

_θ(kv; nv, δ)_
_≤_

=: UCP(L[¯]Tnv (C), δ)

=: URSCP(C, Sm, V, v, b, δ),


-----

thus URSCP is monotonically non-increasing in w(xi).

D.6 PROOF OF THEOREM 5

Recall that


_K_

1 (x _Bj)_ 1 (x[′] _Bj)_ _,_
_∈_  _m_ _∈_ 
_j=1_ _x[′]_ _Sm[X]_

X X∈

_K_  [1] 

1 (x _Bj)_ 1 (xi _Bj)_ _,_
_j=1_ _∈_  _n_ _x[′]_ _Tn[X]_ _∈_ 

X X∈

_K_  [1] 

1 (x _Bj)_ _p(x[′]) dx[′],_
_j=1_ _∈_ ZBj

X

_K_

1 (x _Bj)_ _q(x[′]) dx[′], and_
_j=1_ _∈_ ZBj

X


_pˆB(x) :=_

_qˆB(x) :=_

_pB(x) :=_

_qB(x) :=_


dx[′].
_Bj(x)_


_v(x) := vj(x) =_


Due to the assumption of (12), |v(x) · p(x) − _pK(x)| is bounded for any x ∈_ _Bj as follows:_


_Bj_ _p(x) dx[′]_ _−_


_p(x[′]) dx[′]_
_Bj_



[=] _Bj_ _p(x) −_ _p(x[′])dx[′]_

Z

_≤_ _Bj_ _|p(x) −_ _p(x[′])| dx[′]_
Z

= E. (22)


_v(x)_ _p(x)_ _pB(x)_ =
_|_ _·_ _−_ _|_

Similarly,


_v(x)_ _q(x)_ _qB(x)_ _E._ (23)
_|_ _·_ _−_ _| ≤_

Observe that mpˆ(x) ∼ Binom _m,_ _Bj_ _[p][(][x][′][)d][x][′][]_ for any x ∈ _Bj; thus pK is bounded with proba-_

bility at least 1 _δ[′]_ as follows due to the Clopper-Pearson interval R (θ, θ):
_−_

_θ(mpˆ(x); m, δ[′])_ _pK(x)_ _θ(mpˆ(x); m, δ[′])._ (24)
_≤_ _≤_

Similarly,

_θ(nqˆ(x); n, δ[′])_ _qK(x)_ _θ(nqˆ(x); n, δ[′])._ (25)
_≤_ _≤_


From (22), (23), (24), and (25), the following holds:

_θ(mpˆ(x); m, δ[′]) −_ _E ≤_ _v(x) · p(x) ≤_ _θ(mpˆ(x); m, δ[′]) + E and_

_θ(nqˆ(x); n, δ[′]) −_ _E ≤_ _v(x) · q(x) ≤_ _θ(nqˆ(x); n, δ[′]) + E._

Therefore, for any x _Bj, w[∗](x) is bounded as follows:_
_∈_


_θ(nqˆ(x); n, δ[′]) −_ _E_ _q(x); n, δ[′]) + E_

_θ(mpˆ(x); m, δ[′]) + E_ _p(x)_ _θ(mpˆ(x); m, δ[′])_ _E [.]_

_[≤]_ _[w][∗][(][x][) =][ q][(][x][)]_ _[≤]_ _[θ][(][n][ˆ]_ _−_

Since we apply the Clopper-Pearson interval for K partitions for both source and target, the claim
holds due to the union bound.


-----

E ADDITIONAL ALGORITHMS

E.1 PS ALGORITHM

**Algorithm 2 PS: an algorithm using the CP bound in (3)**

**procedure PS(Sm, f, T, ε, δ)**

_τˆ ←_ 0
**for τ ∈T do** (▷) Grid search in ascending order

**if UCP(Cτ** _, Sm, δ) ≤_ _ε then_

_τˆ ←_ max(ˆτ, τ )

**else**

**break**

**return ˆτ**


E.2 PS-C ALGORITHM

**Algorithm 3 PS-C: an algorithm using the CP bound in (3) with ε/b**

**procedure PS-C(Sm, f, T, b, ε, δ)**

**return PS(Sm, f, T, ε/b, δ)**

We describe the PS-C algorithm, which uses a conservative upper bound on the CP interval. Let


_LP (C) :=_ E _C(x))]_
(x,y) _P_ [[][1][ (][y /]∈
_∼_

_LQ(C) :=_ E _C(x))]_
(x,y) _Q_ [[][1][ (][y /]∈
_∼_

_w[∗](x) :=_ _[q][(][x][)]_

_p(x)_
_b := max_
_x_ _[w][∗][(][x][)][.]_
_∈X_


Then, we have

_LQ(C) =_ E _C(x))]_
(x,y) _Q_ [[][1][ (][y /]∈
_∼_

= E _C(x))]_
(x,y) _P_ [[][w][∗][(][x][)][1][ (][y /]∈
_∼_

E _C(x))]_
_≤_ (x,y) _P_ [[][b][ ·][ 1][ (][y /]∈
_∼_

= b E _C(x))]_
(x,y) _P_ [[][1][ (][y /]∈
_∼_

= b · LP (C).

Thus, LQ(C) ≤ _ε if b · LP (C) ≤_ _ε. Equivalently, LQ(C) ≤_ _ε if LP (C) ≤_ _ε/b. As a consequence,_
we can choose C based on the CP bound for the i.i.d. case (i.e., Algorithm 2), except using the
desired error of ε/b (instead of ε). The algorithm is described in Algorithm 3.


-----

E.3 PS-R ALGORITHM

**Algorithm 4 PS-R: an algorithm using the RSCP bound in (6)**

**procedure PS-R(Sm, f, T, w, b, ε, δ)**

_V ∼_ Uniform([0, 1])[m]

_τˆ ←_ 0
**for τ ∈T do** (▷) Grid search in ascending order

**if URSCP(Cτ** _, Sm, V, w, b, δ) ≤_ _ε then_

_τˆ ←_ max(ˆτ, τ )

**else**

**break**

**return ˆτ**


E.4 PS-M ALGORITHM

**Algorithm 5 PS-M: an algorithm using the RSCP bound in (6) along with IWs rescaling**

**procedure PS-M(Sm, Tn[X]** _[, f,][ T][, w, b, ε, δ][)]_

_wˆ(x) ←_ _pqˆ[ˆ]BB((xx))_ [for][ x][ ∈X] [, where][ ˆ]pB and ˆqB are defined in (14) and (15), respectively

**return PS-R(Sm, f, T, ˆw, b, ε, δ)**


F EXPERIMENT DETAILS

F.1 DOMAIN ADAPTATION

We use a fully-connected network (with two hidden layers, where each layers has 500 neurons
followed by ReLU activations and a 0.5-dropout layer) as the domain classifier (recall that the input
of this domain classifier is the last hidden layer of ResNet101). We use the last hidden layer of
the model as example space X, where its dimension is 2048. For neural network training, we run
stochastic gradient descent (SGD) for 100 epochs with an initial learning rate of 0.1, decaying it by
half once every 20 epochs. The domain adaptation regularizer is gradually increased as in (Ganin
et al., 2016). We use the same hyperparameters for all experiments.

F.2 DOMAINNET

We split the dataset into 409,832 training, 88,371 calibration, and 88,372 test images.

F.3 IMAGENETC-13

We split ImageNet into 1.2M training, 25K calibration, and 25K test images, and ImageNet-C13
into 83M training, 1.6M calibration, and 1.6M test images.

To train a model using domain adaptation, due to the large size of the target training set, we subsample the target training set to be the same size as the source training set on for each random
trials.

G ADDITIONAL RESULTS

G.1 SYNTHETIC RATE SHIFT BY TWO GAUSSIANS

We demonstrate the efficacy of the proposed approaches (i.e., PS-R with the known IWs and PS-W
with the estimated IWs) using a synthetic dataset consisting of samples from two Gaussian distributions.

**Dataset. We consider two Gaussian distributions N** (µ, Σ) and N (µ, Σ[′]) over 2048-dimensional
covariate space X . Here, µ = 0; Σ and Σ[′] are diagonal where Σ1,1 = 5[2], Σi,i = 10[−][1], Σ[′]1,1 [= 1][,]


-----

error


error




0.04

0.03

0.02

0.01

0.00


0.04

0.03

0.02

0.01

0.00


PS WSCI PS-C PS-W PS WSCI

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||


= 0.01

size

2.00

1.75

1.50

1.25

1.00

0.75

prediction set size0.50

0.25

0.00
PS WSCI PS-C PS-W

(a) With the known true IW


PS WSCI PS-C PS-R PS-W PS

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||


= 0.01

size

2.00

1.75

1.50

1.25

1.00

0.75

prediction set size0.50

0.25

0.00
PS WSCI PS-C PS-R PS-W

(b) With an estimated IW


Figure 3: Error under the rate shift by Two Gaussians (over 100 random trials). Parameters are
_m = 50, 000, ε = 0.01, and δ = 10[−][5]._

and Σ[′]i,i [= 10][−][1][ for][ i][ ∈{][2][, . . .,][ 2048][}][. We consider the “flat” Gaussian][ N] [(][µ,][ Σ)][ as the source]
and the “tall” Gaussian N (µ, Σ[′]) as the target. Intuitively, there is a rate shift from the source to the
target—i.e., the target examples are a subset of the source, but occur with higher frequencies. We
use the following labeling function: p(y _x) = σ(5x1), where σ is the sigmoid function. Finally,_
_|_
we generate 50,000 labeled examples for each training, calibration, and test.

**Results. We consider two different setups: 1) the true IW is known, and 2) the true IW is unknown.**
In Figure 3a, we demonstrate the prediction set errors given the true IW. As expected PS-R satisfies
the PAC guarantee—i.e., the error is below ε. However, as shown in Figure 3b, when we need to
estimate IWs, using just the point-estimate of the IW results in PS-R performing poorly in terms
of prediction set error; it still satisfies the ε constraint, but the error is close to zero, indicating that
the prediction set size is too large to be useful for an uncertainty quantifier. In contrast, PS-W (i.e.,
rejection sampling based on interval estimates of IWs) produces a larger, more reasonable error rate
while still satisfying the PAC condition. These experiments demonstrate that PS-R works well when
given the true IW, but accounting for IW uncertainty is important when using estimated IWs.


-----

G.2 PREDICTION SET SIZE AND ERROR


error


error




0.10

0.08

0.06

0.04

0.02

0.00

0.10

0.08

0.06

0.04

0.02

0.00


0.14

0.12

0.10

0.08

0.06

0.04

0.02

0.00

= 0.10


0.150

0.125

0.100

0.075

0.050

0.025

0.000

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
|||= 0.1|0||

|Col1|Col2|Col3|= 0.10|
|---|---|---|---|
||||= 0.1|
|||||
|||||
|||||


PS WSCI PS-C PS-R PS-W

= 0.10

size

100

80

60

40

prediction set size

20

0
PS WSCI PS-C PS-R PS-W

(a) All


PS WSCI PS-C PS-W

size

350

300

250

200

150

100

prediction set size

50

0
PS WSCI PS-C PS-W

(b) Sketch


error


|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
||=|0.10||

|Col1|Col2|error|Col4|
|---|---|---|---|
||||= 0.1|
|||||
|||||


PS WSCI PS-C PS-W

= 0.10

size

350

300

250

200

150

100

prediction set size

50

0
PS WSCI PS-C PS-W

(c) Clipart


PS WSCI PS-C PS-W

= 0.10

size

350

300

250

200

150

100

prediction set size

50

0
PS WSCI PS-C PS-W

(d) Painting


error


error




0.10

0.08

0.06

0.04

0.02

0.00


0.10

0.08

0.06

0.04

0.02

0.00

0.35

0.30

0.25

0.20

0.15

0.10

0.05

0.00

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
||=|0.10||

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
||=|0.10||


PS WSCI PS-C PS-W

= 0.10

size

175

150

125

100

75

50

prediction set size

25

0
PS WSCI PS-C PS-W

(e) Quickdraw


PS WSCI PS-C PS-W

= 0.10

size

175

150

125

100

75

prediction set size 50

25

0
PS WSCI PS-C PS-W

(f) Real


error

|Col1|Col2|Col3|Col4|
|---|---|---|---|


PS WSCI PS-C PS-W

= 0.10

size

350

300

250

200

150

100

prediction set size

50

0
PS WSCI PS-C PS-W

(g) Infograph


error size

1000

0.12

0.10 800

0.08 600

0.06 400

0.04

prediction set error0.02 = 0.10 prediction set size 200

0.00 0

PS WSCI PS-C PS-W PS WSCI PS-C PS-W


(h) ImageNet-C13



PS WSCI PS-C PS-W


0.10

0.08

0.06

0.04

0.02

0.00

|Col1|error|Col3|
|---|---|---|
||||
||||
||||
||||
|=|0.10||


= 0.10


size

1000

800

600

400

prediction set size 200

0
PS WSCI PS-C PS-W


(i) ImageNet + PGD

Figure 4: Prediction set error and size under rate shifts on DomainNet (a-g) and under support shifts
on ImageNet (h, i) (over 100 random trials for error and over a held-out test set for size). Parameters
are m = 50, 000 for DomainNet and m = 20, 000 for ImageNet, ε = 0.1, and δ = 10[−][5].


-----

G.3 ABLATION STUDY ON CALIBRATION


error


error




0.10

0.08

0.06

0.04

0.02

0.00

0.10

0.08

0.06

0.04

0.02

0.00


0.10

0.08

0.06

0.04

0.02

0.00

0.10

0.08

0.06

0.04

0.02

0.00

|Col1|Col2|Col3|Col4|
|---|---|---|---|
||= 0.1|0||

|Col1|Col2|Col3|Col4|
|---|---|---|---|
||= 0.1|0||


PS-R PS-M PS-W PS-R

= 0.10

size

300

250

200

size150

100

50

0
PS-R PS-M PS-W

(a) DomainNet-Sketch


PS-R PS-M PS-W PS-R

= 0.10

size

200

175

150

125

size100

75

50

25

0
PS-R PS-M PS-W

(b) DomainNet-Clipart


error


error

|Col1|Col2|Col3|Col4|
|---|---|---|---|
||= 0.1|0||

|Col1|Col2|Col3|Col4|
|---|---|---|---|
||= 0.1|0||


PS-R PS-M PS-W PS-R

= 0.10

size

250

200

150

size

100

50

0
PS-R PS-M PS-W

(c) DomainNet-Painting


PS-R PS-M PS-W PS-R

= 0.10

size

40

35

30

25

size20

15

10

5

0
PS-R PS-M PS-W

(d) DomainNet-Quickdraw



error


error



0.12

0.10

0.08

0.06

0.04

0.02

0.00

0.10

0.08

0.06

0.04

0.02

0.00


0.10

0.08

0.06

0.04

0.02

0.00

0.10

0.08

0.06

0.04

0.02

0.00

|Col1|Col2|Col3|Col4|
|---|---|---|---|
||= 0.1|0||

|Col1|Col2|Col3|Col4|
|---|---|---|---|
||= 0.1|0||


PS-R PS-M PS-W PS-R

= 0.10

size

80

70

60

50

size40

30

20

10

0
PS-R PS-M PS-W

(e) DomainNet-Real


PS-R PS-M PS-W PS-R

= 0.10

size

350

300

250

200

size

150

100

50

0
PS-R PS-M PS-W

(f) DomainNet-Infograph


error

PS-R PS-M PS-W


error

PS-R PS-M PS-W


size

PS-R PS-M PS-W


size

PS-R PS-M PS-W


1000

800

600

400

200


1000

800

600

400

200

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
|||||
||= 0.1|0||

|0|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
|||||
|||||
|||||

|Col1|Col2|= 0.|.10|
|---|---|---|---|
|||= 0|.10|
|||||
|||||
|||||
|||||

|0|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
|||||
|||||
|||||


= 0.10


= 0.10


(g) ImageNet-C13


(h) ImageNet-PGD


Figure 5: Ablation study on calibration (over 100 random trial for error and over a held-out test set
for size). PS-R does not use rescaled IWs, PS-M uses point estimates of rescaled IWs, and PS-W
uses intervals around rescaled IWs. Parameters are m = 50, 000 for DomainNet shifts, m = 20, 000
for ImageNet shifts, ε = 0.1, and δ = 10[−][5]. In particular, we consider a variant PS-M of PS-W that
ignores the worst-case IWs as in Theorem 3; instead, it rescales the importance weights in each bin
via a point estimate, i.e., Theorem 5 without the Clopper-Pearson interval (i.e., m = n = ∞) and
with E = 0. As can be seen in the results on the shift to various domains, our approach satisfies the
PAC guarantee but this version does not—in fact, its error is even worse than PS-R, which uses the
non-rescaled importance weights from the probabilistic classifier s.


-----

G.4 COMPARISON WITH VARIOUS TOP-K PREDICTION SETS


error


error




0.35

0.30

0.25

0.20

0.15

0.10

0.05

0.00

0.30

0.25

0.20

0.15

0.10

0.05

0.00


0.35

0.30

0.25

0.20

0.15

0.10

0.05

0.00

0.4

0.3

0.2

0.1

0.0

|Col1|Col2|Col3|Col4|= 0.1|0|
|---|---|---|---|---|---|
|||||||
|||||||

|Col1|Col2|Col3|Col4|= 0.1|
|---|---|---|---|---|
||||||
||||||


Top1 Top5 Top10 Top50 PS-W

= 0.10

size

100

80

60

40

prediction set size 20

0 Top1 Top5 Top10 Top50 PS-W


(a) All


Top1 Top5 Top10 Top50 PS-W 0

= 0.10

size

300

250

200

150

100

prediction set size

50

0 Top1 Top5 Top10 Top50 PS-W


(b) Sketch


error


error

|Col1|Col2|Col3|Col4|= 0.1|0|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|||||||

|Col1|Col2|Col3|Col4|= 0.10|
|---|---|---|---|---|
|||||= 0.10|
||||||
||||||
||||||


Top1 Top5 Top10 Top50 PS-W 0

= 0.10

size

200

175

150

125

100

75

prediction set size 50

25

0 Top1 Top5 Top10 Top50 PS-W


(c) Clipart


Top1 Top5 Top10 Top50 PS-W 0

= 0.10

size

250

200

150

100

prediction set size 50

0 Top1 Top5 Top10 Top50 PS-W


(d) Painting




error


error


0.30

0.25

0.20

0.15

0.10

0.05

0.00

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.0


0.25

0.20

0.15

0.10

0.05

0.00

0.4

0.3

0.2

0.1

0.0

|Col1|Col2|Col3|Col4|= 0.1|0|
|---|---|---|---|---|---|
|||||||

|Col1|Col2|Col3|Col4|= 0.1|
|---|---|---|---|---|


Top1 Top5 Top10 Top50 PS-W 0 Top1

= 0.10

size

50

40

30

20

prediction set size10

0 Top1 Top5 Top10 Top50 PS-W


(e) Quickdraw


Top1 Top5 Top10 Top50 PS-W 0

= 0.10

size

80

70

60

50

40

30

prediction set size20

10

0 Top1 Top5 Top10 Top50 PS-W


(f) Real


error


error

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|||||||
|||||||

|Col1|Col2|Col3|=|= 0.10|
|---|---|---|---|---|
|||||= 0.10|
||||||
||||||
||||||


Top1 Top5 Top10 Top50 PS-W 0

= 0.10

size

350

300

250

200

150

100

prediction set size

50

0 Top1 Top5 Top10 Top50 PS-W


(g) Infograph


Top1 Top5 Top10 Top50 PS-W 0 Top1

= 0.10

size

250

200

150

100

prediction set size 50

0 Top1 Top5 Top10 Top50 PS-W


(h) ImageNet-C13


error

|Col1|Col2|Col3|Col4|= 0.1|0|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|||||||



Top1 Top5 Top10 Top50 PS-W



0.25

0.20

0.15

0.10

0.05

0.00


= 0.10


size

250

200

150

100

prediction set size 50

0 Top1 Top5 Top10 Top50 PS-W


(i) ImageNet + PGD

Figure 6: Prediction set error size size under rate shifts on DomainNet (a-g) and under support shifts
on ImageNet (h, i) (over 100 random trial). Default parameters are m = 50, 000 for DomainNet
and m = 20, 000 for ImageNet, ε = 0.1, and δ = 10[−][5]. A Top-K prediction set is a prediction
set that contains top K labels based on a domain-adapted score function; thus the set size is always
_K. As can be seen in the prediction set error plots, Top-K prediction sets do not consistently satisfy_
the desired ϵ guarantee. Moreover, the prediction set size is worse than PS-W. For example, when
the Top-50 prediction set error rate almost achieves the desired error rate, e.g., (6d), the mean and
medial of the corresponding prediction set size is larger than PS-W.


-----

G.5 VARYING A SMOOTHNESS PARAMETER


0.10

0.08

0.06

0.04

0.02

0.00

0.10

0.08

0.06

0.04

0.02

0.00


0.10

0.08

0.06

0.04

0.02

0.00

0.10

0.08

0.06

0.04

0.02

0.00

|Col1|Col2|Col3|= 0.1|0|
|---|---|---|---|---|
||||||
||||||

|Col1|Col2|Col3|= 0.1|
|---|---|---|---|
|||||
|||||


0.001 0.005 0.01

= 0.10

E

175

150

125

size100

75

50

25

0
0 0.001 0.005 0.01

(a) All


0.001 0.005 0.01 0

= 0.10

E

250

200

size150

100

50

0
0 0.001 0.005 0.01

(b) Sketch

|Col1|Col2|Col3|Col4|(a|
|---|---|---|---|---|
||||= 0.1|0|
||||||

|Col1|Col2|Col3|(b)|
|---|---|---|---|
||||= 0.1|
|||||


0.001 0.005 0.01 0

= 0.10

E

200

150

size

100

50

0
0 0.001 0.005 0.01

(c) Clipart


0.001 0.005 0.01 0

= 0.10

E

250

200

size150

100

50

0
0 0.001 0.005 0.01

(d) Painting


0.10

0.08

0.06

0.04

0.02

0.00

0.10

0.08

0.06

0.04

0.02

0.00


0.10

0.08

0.06

0.04

0.02

0.00

0.10

0.08

0.06

0.04

0.02

0.00



0.001 0.005 0.01 0

|Col1|Col2|Col3|= 0.1|0|
|---|---|---|---|---|
||||||
||||||
||||||
||||||


= 0.10

E

80

70

60

50

size40

30

20

10

0
0 0.001 0.005 0.01

(e) Quickdraw


0.001 0.005 0.01

|Col1|Col2|Col3|(d)|
|---|---|---|---|
||||= 0.1|
|||||
|||||
|||||
|||||


E

250

200

150

size

100

50

0
0 0.001 0.005 0.01

(f) Real



0.001 0.005 0.01 0

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||


= 0.10

E

350

300

250

size200

150

100

50

0
0 0.001 0.005 0.01

(g) Infograph


0.001 0.005 0.01 0

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||


= 0.10

E 0

500

400

size300

200

100

0
0 0.001 0.005 0.01

(h) ImageNet-C13


0.10

0.08

0.06

0.04

0.02

0.00


0.001 0.005 0.01

|Col1|Col2|= 0.1|
|---|---|---|
||||
||||
||||
||||


= 0.10


200

175

150

125

size100

75

50

25

0
0 0.001 0.005 0.01


(i) ImageNet + PGD

Figure 7: Prediction set error size size under rate shifts on DomainNet (a-g) and under support
shifts on ImageNet (h, i) (over 100 random trial) for varying E. Parameters are m = 50, 000 for
DomainNet and m = 20, 000 for ImageNet, ε = 0.1, and δ = 10[−][5]. In particular, the parameter
_E in Assumption 1 bounds the quality of our estimates of p(x) and q(x); since these errors cannot_
be conveniently measured, we have chosen it heuristically as a hyperparameter. In this figure, we
show the error of PS-W as a function of E. As E becomes smaller, the prediction sets become more
optimistic while still satisfying the PAC guarantee. Note that the optimal case is E = 0, since PS-W
still satisfies the PAC guarantee; this result suggests that our IW estimates are reasonably accurate.


-----

G.6 VARYING A NUMBER OF BINS


0.10

= 0.10 200

0.08 175

0.06 150

125

0.04 size100

75

prediction set error0.02 50

0.00 25

5 10 15 20 0

K 5 10 15 20


0.10 = 0.10

250

0.08

200

0.06

150

0.04 size

100

prediction set error0.02

50

0.00

5 10 15 20 0

K 5 10 15 20


(a) All


(b) Sketch


0.10

= 0.10 300

0.08

250

0.06 200

0.04 size150

prediction set error0.02 100

50

0.00

5 10 15 20 0

K 5 10 15 20

0.10

0.08

0.06

0.04

0.02

0.00


0.10

= 0.10

0.08 250

0.06 200

0.04 size150

100

prediction set error0.02

50

0.00

5 10 15 20 0

K 5 10 15 20


(c) Clipart

10 15 20 0


(d) Painting


0.10

0.08

0.06

0.04

0.02

0.00


10 15 20

|Col1|Col2|Col3|(d)|
|---|---|---|---|
||||= 0.1|
|||||
|||||
|||||


K

200

150

size

100

50

0
5 10 15 20

(f) Real

|Col1|Col2|Col3|= 0.1|0|
|---|---|---|---|---|
||||||
||||||
||||||


= 0.10


50

40

size30

20

10

0
5 10 15 20


(e) Quickdraw

0.12 = 0.10 350

0.10 300

0.08 250

0.06 size200

0.04 150

prediction set error0.02 100

50

0.00

5 10 15 20 0

K 5 10 15 20


(g) Infograph


0.10

= 0.10 500

0.08

400

0.06

size300

0.04

200

prediction set error0.02

100

0.00

5 10 15 20 0

K 5 10 15 20


(h) ImageNet-C13


0.10

0.08

0.06

0.04

0.02

0.00


10 15 20

|Col1|Col2|= 0.1|
|---|---|---|
||||
||||
||||
||||


= 0.10


400

350

300

250

size200

150

100

50

0
5 10 15 20


(i) ImageNet + PGD

Figure 8: Prediction set error size size under rate shifts on DomainNet (a-g) and under support
shifts on ImageNet (h, i) (over 100 random trial) for varying K. Parameters are m = 50, 000
for DomainNet and m = 20, 000 for ImageNet, ε = 0.1, and δ = 10[−][5]. In general, K must be
chosen to be small enough so each bin contains sufficiently many source examples to achieve a small
Clopper-Perason interval size (e.g., 10[−][3]), though it also needs to be sufficiently large to satisfy the
smoothness assumption.


-----

G.7 PREDICTION SET VISUALIZATION

|Example x|Cˆ (x) PS|Cˆ (x) PS-W|Example x|Cˆ (x) PS|Cˆ (x) PS-W|
|---|---|---|---|---|---|


|Col1|ra\ccoon| owl,  ra\ccoon|Col4|angel, hdarp| angel,   c hde al rl po,,  m pi vc ir io op lh ioo nn,e,  a n|
|---|---|---|---|---|---|


|Col1| w\ine bottle| bread,   grapes,  w\ine bottle, wine glass|Col4| s[hark, snorkel| dolphin,    s[hark, snorkel, sub wm ha ar li ene,|
|---|---|---|---|---|---|


|Col1|n o ca\mpfire|  ca\mpfire,   ocean, star,   tent|Col4|coffee cup, cup c|coffee cup,  cup,  c mug,   teapot|
|---|---|---|---|---|---|


|Col1|o[cean|hurricane,  o[cean,  square,   tornado|Col4|n o b[rain| b[rain,    fish, lion, sl eo all i tp uo rtp le,|
|---|---|---|---|---|---|


|Col1|n o p\enguin| hydrant, fire   foot, p\enguin, telephone|Col4| hat,  h\ot tub| bed,  birth gdb hae atyl tat,,ake, rc u i, h\ot tub, tiny,can, p ala boi on et p ti l lew s h,|
|---|---|---|---|---|---|


|Col1|as\paragus|as\paragus,  b ba rs ek ae dt,,  carrot, tol ooh ha sr btp r, sh tb e ur,|Col4| b\aseball, onion| b\aseball,  bas be rb ea al dl,bat, light bulb,   o pn oi to an to,|
|---|---|---|---|---|---|



Figure 9: Prediction sets of the DomainNet shift from All to Paint. Parameters are m = 50, 000,
_ε = 0.1, and δ = 10[−][5]. The green label is the true label and the label with the hat is the predicted_
label. We choose examples where the two approaches differ; in particular, if PS-W is incorrect, then
PS is incorrect as well since the prediction set sizes are monotone in τ .


-----

|Example x|Cˆ (x) PS|Cˆ (x) PS-W|Example x|Cˆ (x) PS|Cˆ (x) PS-W|
|---|---|---|---|---|---|


|Col1|n o pho\tocopier| printer,  pho\tocopier|Col4|  tim\ber wolf,  red wolf,  dingo | t wim h\b ite er ww oo ll ff,,  red wolf,  c do iy no gt oe, |
|---|---|---|---|---|---|


|Col1|airliner,   airship, w\arplane|aircraft carrier, co ta oai pr irl wsi en arie cr s,,,ip, ta i h tp sn n hh  p wa \ar wra pc nh gu nt ee,,  t a n k, il a|Col4| forklift,   hg ao rl vf ec sa tr et r, , law\n mower, sn r, to raw cp kl to ow| forklift,   ag ro vk eca sar trt et,  hg o l f r,, law\n mower, sn tp ri awc ck khu ep orw, r,, ho sp l to t r e|
|---|---|---|---|---|---|


|Col1|  le\opard,   snow leopard,  cheetah |  le\opard,  leopard, snow jaguar,   cheetah|Col4| dia\mondback, sidewinder|hognose snake,   diam\ondback,  sidewinder |
|---|---|---|---|---|---|


|Col1|n o tel\evision|ent. center,   monitor, screen, tel\evision|Col4|analog clock,  b oa dr oo mm ee tt ee rr,,   s wto a\p llw ca lt oc ch k, |analog clock, mdi ab ga .r ao clm me at pe acr,, g i t w t sh s, o  so wtd a\o llm cae ot ce chr k,  o p w lt,|
|---|---|---|---|---|---|


|Col1|chameleon, gre\en lizard|chameleon, gre\en lizard, green snake, wal min ag s tit sick, n|Col4|  b\arbell, dumbbell, lens cap,   puck| b\arbell,  b da ur mo wm be et ee lllr,,, c a r bh e lens cap, p e, sto ew he or kd c,r oi pll tp u c s|
|---|---|---|---|---|---|


|Col1| Pe\mbroke, Cardigan| Pe\mbroke, Cardigan|Col4| face powder,    li\pstick, paintbrush|ballpoint, fac l\ie sp to icw kd,e, p paintbrush, sp  ue nr sfu crm eee n,|
|---|---|---|---|---|---|


Figure 10: Prediction sets of the shift from ImageNet to ImageNet-C13. Parameters are m =
20, 000, ε = 0.1, and δ = 10[−][5]. The green label is the true label and the label with the hat is
the predicted label. We choose examples where the two approaches differ; in particular, if PS-W is
incorrect, then PS is incorrect as well since the prediction set sizes are monotone in τ .


-----

|Example x|Cˆ (x) PS|Cˆ (x) PS-W|Example x|Cˆ (x) PS|Cˆ (x) PS-W|
|---|---|---|---|---|---|


|Col1|n o bo\x turtle|terrapin, bo\x turtle|Col4|brain coral,   s\tarfish, sea urchin| brain coral,   s\c th arit fio sn h, , sea urchin, sea tc iru nac u hrm rb n,er, c o kl e oe f s|
|---|---|---|---|---|---|


|Col1|  w\hite terrier,  kuvasz, komondor|Maltese dog, wh\ite terrier, kuvasz, k So am mo on yd eo dr,|Col4|n o k\it fox|red fox, k\it fox|
|---|---|---|---|---|---|


|Col1|n o la\dybug|leaf beetle, la\dybug|Col4|n o t\usker| t\usker,  Afri. elephant|
|---|---|---|---|---|---|


|Col1| banjo,    elec. guitar,  s[tage |aco. guitar,  banjo,  elec. guitar,   s[tage|Col4| beaker,  po\p bottle, ww ia nte er b bo ottl te t, le| beaker,  b pee er fb uo mtt el,e, r po\p bottle, ww le ia nte er b bo ottl te t,|
|---|---|---|---|---|---|


|Col1|n o c\uirass|breastplate, c\uirass|Col4|convertible, sp\orts car|car wheel,   convertible,  sp\orts car |
|---|---|---|---|---|---|


|Col1|n o con\fectionery|n o con\fectionery|Col4|n o cras\h helmet| bonnet,    cras\h helmet, football helmet|
|---|---|---|---|---|---|


Figure 11: Prediction sets of the shift from ImageNet to ImageNet-PGD. Parameters are m =
20, 000, ε = 0.1, and δ = 10[−][5]. The green label is the true label and the label with the hat is
the predicted label. We choose examples where the two approaches differ; in particular, if PS-W is
incorrect, then PS is incorrect as well since the prediction set sizes are monotone in τ .


-----

