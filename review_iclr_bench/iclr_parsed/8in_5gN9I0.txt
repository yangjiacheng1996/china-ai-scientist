# TRIANGLE AND FOUR CYCLE COUNTING WITH PRE## DICTIONS IN GRAPH STREAMS

**Justin Y. Chen, Piotr Indyk, Shyam Narayanan, Ronitt Rubinfeld, and Sandeep Silwal** _[∗]_
Computer Science and Artificial Intelligence Laboratory
Massachusetts Institute of Technology
Cambridge, MA 02139, USA
_{justc, indyk, shyamsn, ronitt, silwal}@mit.edu_

**Honghao Lin, David P. Woodruff, Michael Zhang**
Computer Science Department
Carnegie Mellon University
Pittsburgh, PA 15213, USA
_{honghaol, dwoodruf, jinyaoz}@andrew.cmu.edu_


**Tal Wagner**
Microsoft Research
Redmond, WA 98052, USA
tal.wagner@gmail.com


**Talya Eden**
Massachusetts Institute of Technology and Boston University
Cambridge, MA 02139, USA
teden@mit.edu

ABSTRACT


We propose data-driven one-pass streaming algorithms for estimating the number
of triangles and four cycles, two fundamental problems in graph analytics that are
widely studied in the graph data stream literature. Recently, Hsu et al. (2019a)
and Jiang et al. (2020) applied machine learning techniques in other data stream
problems, using a trained oracle that can predict certain properties of the stream
elements to improve on prior “classical” algorithms that did not use oracles. In
this paper, we explore the power of a “heavy edge” oracle in multiple graph edge
streaming models. In the adjacency list model, we present a one-pass triangle
counting algorithm improving upon the previous space upper bounds without such
an oracle. In the arbitrary order model, we present algorithms for both triangle and
four cycle estimation with fewer passes and the same space complexity as in previous algorithms, and we show several of these bounds are optimal. We analyze our
algorithms under several noise models, showing that the algorithms perform well
even when the oracle errs. Our methodology expands upon prior work on “classical” streaming algorithms, as previous multi-pass and random order streaming
algorithms can be seen as special cases of our algorithms, where the first pass or
random order was used to implement the heavy edge oracle. Lastly, our experiments demonstrate advantages of the proposed method compared to state-of-theart streaming algorithms.

1 INTRODUCTION

Counting the number of cycles in a graph is a fundamental problem in the graph stream model
(e.g., Atserias et al. (2008); Bera & Chakrabarti (2017); Seshadhri et al. (2013); Kolountzakis et al.
(2010); Bar-Yossef et al. (2002); Kallaugher et al. (2019)). The special case of counting triangles
is widely studied, as it has a vast range of applications. In particular, it provides important insights
into the structural properties of networks (Prat-P´erez et al., 2012; Farkas et al., 2011), and is used
to discover motifs in protein interaction networks (Milo et al., 2002), understand social networks
(Foucault Welles et al., 2010), and evaluate large graph models (Leskovec et al., 2008). See Al Hasan
& Dave (2018) for a survey of these and other applications.

_∗All authors contributed equally._


-----

Because of its importance, a large body of research has been devoted to space-efficient streaming
algorithms for (1 + ϵ)-approximate triangle counting. Such algorithms perform computation in one
or few passes over the data using only a sub-linear amount of space. A common difficulty which
arises in all previous works is the existence of heavy edges, i.e., edges that are incident to many
triangles (four cycles). As sublinear space algorithms often rely on sampling of edges, and since a
single heavy edge can greatly affect the number of triangles (four cycles) in a graph, sampling and
storing these edges are often the key to an accurate estimation. Therefore, multiple techniques have
been developed to determine whether a given edge is heavy or not.

Recently, based on the observation that many underlying patterns in real-world data sets do not
change quickly over time, machine learning techniques have been incorporated into the data stream
model via the training of heavy-hitter oracles. Given access to such a learning-based oracle, a wide
range of significant problems in data stream processing — including frequency estimation, estimating the number of distinct elements, Fp-Moments or (k, p)-Cascaded Norms — can all achieve
space bounds that are better than those provided by “classical” algorithms, see, e.g., Hsu et al.
(2019a); Cohen et al. (2020); Jiang et al. (2020); Eden et al. (2021); Du et al. (2021). More generally, learning-based approaches have had wide success in other algorithmic tasks, such as data
structures (Kraska et al., 2018; Ferragina et al., 2020; Mitzenmacher, 2018; Rae et al., 2019; Vaidya
et al., 2021), online algorithms (Lykouris & Vassilvtiskii, 2018; Purohit et al., 2018; Gollapudi &
Panigrahi, 2019; Rohatgi, 2020; Wei, 2020; Mitzenmacher, 2020; Lattanzi et al., 2020; Bamas et al.,
2020), similarity search (Wang et al., 2016; Dong et al., 2020) and combinatorial optimization (Dai
et al., 2017; Balcan et al., 2017; 2018a;b; 2019). See the survey and references therein for additional
works (Mitzenmacher & Vassilvitskii, 2020).

Inspired by these recent advancements, we ask: is it possible to utilize a learned heavy edge oracle
_to improve the space complexity of subgraph counting in the graph stream model? Our results_
demonstrate that the answer is yes.

1.1 OUR RESULTS AND COMPARISON TO PREVIOUS THEORETICAL WORKS

We present theoretical and empirical results in several graph streaming models, and with several
notions of prediction oracles. Conceptually, it is useful to begin by studying perfect oracles that
provide exact predictions. While instructive theoretically, such oracles are typically not available in
practice. We then extend our theoretical results to noisy oracles that can provide inaccurate or wrong
predictions. We validate the practicality of such oracles in two ways: by directly showing they can
be constructed for multiple real datasets, and by showing that on those datasets, our algorithms attain
significant empirical improvements over baselines, when given access to these oracles.

We proceed to a precise account of our results. Let G = (V, E) denote the input graph, and let n,
_m, and T denote the number of vertices, edges, and triangles (or four-cycles) in G, respectively._
There are two major graph edge streaming models: the adjacency list model and the arbitrary order
model. We show that training heavy edge oracles is possible in practice in both models, and that such
oracles make it possible to design new algorithms that significantly improve the space complexity of
triangle and four-cycle counting, both in theory and in practice. Furthermore, our formalization of
a heavy edge prediction framework makes it possible to show provable lower bounds as well. Our
results are summarized in Table 1.

In our algorithms, we assume that we know a large-constant approximation of T for the purposes of
setting various parameters. This is standard practice in the subgraph counting streaming literature
(e.g., see (Braverman et al., 2013, Section 1), (McGregor et al., 2016, Section 1.2) for an extensive
discussions on this assumption). Moreover, when this assumption cannot be directly carried over in
practice, in Subsection F.3 we discuss how to adapt our algorithms to overcome this issue.

1.1.1 PERFECT ORACLE

Our first results apply for the case that the algorithms are given access to a perfect heavy edge oracle.
That is, for some threshold ρ, the oracle perfectly predicts whether or not a given edge is incident to
at least ρ triangles (four cycles). We describe how to relax this assumption in Section 1.1.2.

**Adjacency List Model** All edges incident to the same node arrive together. We show:


-----

Table 1: Our results compared to existing theoretical algorithms. ∆E (∆V ) denotes the maximum
number of triangles incident to any edge (vertex), and κ denotes the arboricity of the graph.

e

|Problem|Previous Results (no oracle)|Our Results|
|---|---|---|
|Triangle, Adjacency|√ Oe(ϵ−2m/ T), 1-pass (McGregor et al., 2016)|Oe(min(ϵ−2m2/3/T 1/3, ϵ−1m1/2)), 1-pass|
|Triangle, Arbitrary|Oe(ϵ−2m3/2/T), 3-pass (McGregor et al., 2016)|√ O(ϵ−1(m/ T +m1/2)), 1-pass|
||√ Oe(ϵ−2m/ T), 2-pass (McGregor et al., 2016)||
||√ Oe(ϵ−2(m/ T + m∆ E/T)), 1-pass (Pagh & Tsourakakis, 2012)||
||m√ Oe(ϵ−2(m/T 2/3 + m∆ E/T + ∆ /T)), 2-pass V (Kallaugher & Price, 2017)||
||√ Oe(poly(ϵ−1)(m∆ E/T + m ∆ /T)), 1-pass V (Jayaram & Kallaugher, 2021)||
||Oe(poly(ϵ−1)mκ/T), multi-pass (Bera & Sheshadhri, 2020)||
|4-cycle, Arbitrary|Oe(ϵ−2m/T 1/4), 3-pass (McGregor & Vorotnikova, 2020)|Oe(T 1/3 + ϵ−2m/T 1/3), 1-pass|
||Oe(ϵ−2m/T 1/3), 3-pass (Vorotnikova, 2020)||


**Theorem 1.1.** _There exists a one-pass algorithm,_ _Algorithm 1,_ _with space complexity[1]_

_O(min(ϵ[−][2]m[2][/][3]/T_ [1][/][3], ϵ[−][1]m[1][/][2])) in the adjacency list model that, using a learning-based ora_cle, returns a (1 ± ϵ)-approximation to the number T of triangles with probability at least[2]_ 7/10.
e

An overview of Algorithm 1 is given in Section 2, and the full analysis is provided in Appendix B.

**Arbitrary Order Model** In this model, the edges arrive in the stream in an arbitrary order. We
present a one-pass algorithm for triangle counting and another one-pass algorithm for four cycle
counting in this model, both reducing the number of passes compared to the currently best known
space complexity algorithms. Our next result is as follows:

**Theorem 1.2.** _There exists a one-pass algorithm,_ _Algorithm 4,_ _with space complexity_
_O(ϵ[−][1](m/√T +_ _m)) in the arbitrary order model that, using a learning-based oracle, returns a_

_[√]_
(1 ± ϵ)-approximation to the number T of triangles with probability at least 7/10.

e

An overview of Algorithm 4 is given in Section 3, and full details are provided in Appendix C.

We also show non-trivial space lower bounds that hold even if appropriate predictors are available.
In Theorem C.2 in Appendix C.3, we provide a lower bound for this setting by giving a construction
that requires Ω(min(m/√T, m[3][/][2]/T )) space even with the help of an oracle, proving that our result

is nearly tight in some regimes. Therefore, the triangle counting problem remains non-trivial even
when extra information is available.

**Four Cycle Counting.** For four cycle counting in the arbitrary order model, we give Theorem 1.3

which is proven in Appendix D.

**Theorem 1.3. There exists a one-pass algorithm, Algorithm 5, with space complexity** _O(T_ [1][/][3] +
_ϵ[−][2]m/T_ [1][/][3]) in the arbitrary order model that, using a learning-based oracle, returns a (1 ± ϵ)_approximation to the number T of four cycles with probability at least 7/10._

[e]

To summarize our theoretical contributions, for the first set of results of counting triangles in the
adjacency list model, our bounds always improve on the previous state of the art due to McGregor
et al. (2016) for all values of m and T . For a concrete example, consider the case that T = Θ([√]m).

21The success probability can beWe use _O(f_ ) to denote O(f · polylog( 1 − _δ by runningf_ )). log(1/δ) copies of the algorithm and taking the median.
e


-----

In this setting previous bounds result in an _O(m[3][/][4])-space algorithm, while our algorithm only_
requires _O([√]m) space (for constant ϵ)._

[e]

For the other two problems of counting triangles and 4-cycles in the arbitrary arrival model, our
space bounds have an additional additive term compared to[e] McGregor et al. (2016) (for triangles)
and Vorotnikova (2020) (for 4-cycles) but importantly run in a single pass rather than multiple
passes. In the case where the input graph has high triangles density, T = Ω(m/ϵ[2]), our space
bound is worse due to the additive factor. When T = O(m/ϵ[2]), our results achieve the same
dependence on m and T as that of the previous algorithms with an improved dependency in ϵ.
Moreover, the case T ≤ _m/ϵ[2]_ is natural for many real world datasets: for ϵ = 0.05, this condition
holds for all of the datasets in our experimental results (see Table 2). Regardless of the triangle
density, a key benefit of our results is that they are achieved in a single pass rather than multiple
passes. Finally, our results are for general graphs, and make no assumptions on the input graph
(unlike Pagh & Tsourakakis (2012); Kallaugher & Price (2017); Bera & Sheshadhri (2020)). Most
of our algorithms are relatively simple and easy to implement and deploy. At the same time, some of
our results require the use of novel techniques in this context, such as the use of exponential random
variables (see Section 2).

1.1.2 NOISY ORACLES

The aforementioned triangle counting results are stated under the assumption that the algorithms are
given access to perfect heavy edge oracles. In practice, this assumption is sometimes unrealistic.
Hence, we consider several types of noisy oracles. The first such oracle, which we refer to as a
_K-noisy oracle, is defined below (see Figure 3 in the Supplementary Section C.2)._

**Definition 1.1. For an edge e = xy in the stream, define Ne as the number of triangles that contain**
_Kboth-noisy x and oracle if for every edge y. For a fixed constant e, 1 K − ≥K ·1N and for a thresholdρe_ _[≤]_ _[Pr][[][O][ρ][(][e][) =][ HEAVY] ρ we say that an oracle[]][ ≤]_ _[K][ ·][ N]ρ[e]_ _[.]_ _Oρ is a_

This oracle ensures that if an edge is extremely heavy or extremely light, it is classified correctly
with high probability, but if the edge is close to the threshold, the oracle may be inaccurate. We
further discuss the properties of this oracle in Section G.

For this oracle, we prove the following two theorems. First, in the adjacency list model, we prove:

**Theorem 1.4. Suppose that the oracle given to Algorithm 1 is a K-noisy oracle as defined in**
_Definition 1.1. Then with probability 2/3, Algorithm 1 returns a value in (1 ±_ _√K · ϵ)T_ _, and_

_uses space at most_ _O(min(ϵ[−][2]m[2][/][3]/T_ [1][/][3], K · ϵ[−][1]m[1][/][2])).

Hence, even if our oracle is inaccurate for edges near the threshold, our algorithm still obtains an

[e]

effective approximation with low space in the adjacency list model. Likewise, for the arbitrary order
model, we prove in Theorem C.1 that the O(ϵ[−][1](m/√T + _m)) 1-pass algorithm of Theorem 1.2_

_[√]_

also works when Algorithm 4 is only given access to a K-noisy oracle.

The proof of Theorem 1.4 is provided in Appendix B.1, and the proof of Theorem C.1 is provided
in Appendix C.2. We remark that Theorems 1.4 and C.1 automatically imply Theorems 1.1 and 1.2,
since the perfect oracle is automatically a K-noisy oracle.

**(Noisy) Value Oracles** In the adjacency list model, when we see an edge xy, we also have access
to all the neighbors of either x or y, which makes it possible for the oracle to give a more accurate
prediction. For an edge xy, let Rxy denote the number of triangles {x, z, y} so that x precedes z
and z precedes y in the stream arrival order. Formally, Rxy = |z : {x, y, z} ∈ ∆ and x <s z <s y|
where x <s y denotes that the adjacency list of x arrives before that of y in the stream.

Motivated by our empirical results in Section F.7, it is reasonable in some settings to assume we
have access to oracles that can predict a good approximation to Rxy. We refer to such oracles as
_value oracles._

In the first version of this oracle, we assume that the probability of approximation error decays
linearly with the error from above but exponentially with the error from below.

**Definition 1.2. Given an edge e, an (α, β) value-prediction oracle outputs a random value p(e)**
_where E[p(e)]_ _αRe + β, and Pr[p(e) <_ _[R]λ[e]_
_≤_ _[−]_ _[β][]][ ≤]_ _[Ke][−][λ][ for some constant][ K][ and any][ λ][ ≥]_ [1][.]


-----

For this variant, we prove the following theorem.
**Theorem 1.5. Given an oracle with parameters (α, β), there exists a one-pass algorithm, Algo-**
_rithm 2, with space complexity O(ϵ[−][2]_ log[2](K/ϵ)(α + mβ/T )) in the adjacency list model that
_returns a (1 ± ϵ)-approximation to the number of triangles T with probability at least 7/10._

In the second version of this noisy oracle, we assume that the probability of approximation error
decays linearly with the error from both above and below. For this variant, we prove that we can
achieve the same guarantees as Theorem 1.5 up to logarithmic factors (see Theorem B.1). The
algorithms and proofs for both Theorem 1.5 and Theorem B.1 appear in Appendix B.2.

**Experiments** We conduct experiments to verify our results for triangle counting on a variety of
real world networks (see Table 2) in both the arbitrary and adjacency list models. Our algorithms
use additional information through predictors to improve empirical performance. The predictors
are data dependent and include: memorizing heavy edges in a small portion of the first graph in a
sequence of graphs, linear regression, and graph neural networks (GNNs). Our experimental results
show that we can achieve up to 5x decrease in estimation error while keeping the same amount of
edges as other state of the art empirical algorithms. For more details, see Section 4. In Section F.7,
we show that our noisy oracle models are realistic for real datasets.

**Related Empirical Works** On the empirical side, most of the focus has been on triangle counting
in the arbitrary order model for which there are several algorithms that work well in practice. We
primarily focus on two state-of-the-art baselines, ThinkD (Shin et al., 2018) and WRS (Shin, 2017).
In these works, the authors compare to previous empirical benchmarks such as the ones given in
Stefani et al. (2017); Han & Sethu (2017); Lim & Kang (2015) and demonstrate that their algorithms
achieve superior estimates over these benchmarks. There are also other empirical works such as
Ahmed et al. (2017) and Ahmed & Duffield (2020) studying this model but they do not compare
to either ThinkD or WRS. While these empirical papers demonstrate that their algorithm returns
unbiased estimates, their theoretical guarantees on space is incomparable to the previously stated
space bounds for theoretical algorithms in Table 1. Nevertheless, we use ThinkD and WRS as part
of our benchmarks due to their strong practical performance and code accessibility.

**Implicit Predictors in Prior Works** The idea of using a predictor is implicit in many prior works.
The optimal two pass triangle counting algorithm of McGregor et al. (2016) can be viewed as an
implementation of a heavy edge oracle after the first pass. This oracle is even stronger than the Knoisy oracle as it is equivalent to an oracle that is always correct on an edge e if Ne either exceeds
or is under the threshold ρ by a constant multiplicative factor. This further supports our choice of
oracles in our theoretical results, as a stronger version of our oracle can be implemented using one
additional pass through the data stream (see Section G). Similarly, the optimal triangle counting
streaming algorithm (assuming a random order) given in McGregor & Vorotnikova (2020) also
implicitly defines a heavy edge oracle using a small initial portion of the random stream (see Lemma
2.2 in McGregor & Vorotnikova (2020)). The random order assumption allows for the creation of
such an oracle since heavy edges are likely to have many of their incident triangle edges appearing
in an initial portion of the stream. We view these two prior works as theoretical justification for our
oracle definitions. Lastly, the WRS algorithm also shares the feature of defining an implicit oracle:
some space is reserved for keeping the most recent edges while the rest is used to keep a random
sample of edges. This can be viewed as a specific variant of our model, where the oracle predicts
recent edges as heavy.

**Preliminaries.** _G = (V, E) denotes the input graph, and n, m and T denote the number of ver-_
tices, edges and triangles (or four-cycles) in G, respectively. We use N (v) to denote the set of neighbors of a node v, and ∆ to denote the set of triangles. In triangle counting, for each xy ∈ _E(G),_
we recall that Nxy = |z : {x, y, z} ∈ ∆| is the number of triangles incident to edge xy, and
_Rxy = |z : {x, y, z} ∈_ ∆, x <s z <s y| is the number of triangles adjacent to xy with the third
vertex z of the triangle between x and y in the adjacency list order. Table A summarizes the notation.

2 TRIANGLE COUNTING IN THE ADJACENCY LIST MODEL

We describe an algorithm with a heavy edge oracle, and one with a value oracle.

**Heavy Edge Oracle.** We present an overview of our one-pass algorithm, Algorithm 1, with a space
complexity of _O[e](min(ϵ[−][2]m[2][/][3]/T_ [1][/][3], ϵ[−][1]m[1][/][2])), given in Theorem 1.1. We defer the pseudocode


-----

and proof of the theorem to Appendix B. The adaptations and proofs for Theorems 1.4, 1.5 and B.1
for the various noisy oracles appear in Appendix B.1 and Appendix B.2.

Our algorithm works differently depending on the value of T . We first consider the case that
_T ≥_ (m/ϵ)[1][/][2]. Assume that for each sampled edge xy in the stream, we can exactly know the
number of triangles Rxy this edge contributes to T . Then the rate at which we would need to sample
each edge would be proportional toangles incident to any edge. Hence, our first idea is to separately consider light and non-light edges pnaive ≈ _ϵ[−][2]∆E/T_, where ∆E is the maximum number of triusing the heavy edge oracle. This allows us to sample edges that are deemed light by the oracle
at a lower rate, p1, and compute their contribution by keeping track of Rxy for each such sampled
edge. Intuitively, light edges offer us more flexibility and thus we can sample them with a lower
probability while ensuring the estimator’s error does not drastically increase. In order to estimate
the contribution due to non-light edges, we again partition them into two types: medium and heavy,
according to some threshold ρ. We then use an observation from McGregor et al. (2016), that since
order to both detect if some edgefor heavy edges Rxy > ρ, it is sufficient to sample from the entire stream at rate xy is heavy and if so to estimate Rxy. _p3 ≈_ _ϵ[−][2]/ρ, in_

Therefore, it remains to estimate the contribution to T due to medium edges (these are the edges
that are deemed non-light by the oracle, and also non-heavy according to the sub-sampling above).
Since the number of triangles incident to medium edges is higher than that of light ones, we have
to sample them at some higher rate p2 > p1. However, since their number is bounded, this is still
space efficient. We get the desired bounds by correctly setting the thresholds between light, medium
and heavy edges.

When T < (m/ϵ)[1][/][2] our algorithm becomes much simpler. We only consider two types of edges,
light and heavy, according to some threshold T/ρ. To estimate the contribution due to heavy edges
we simply store them and keep track of their Rxy values. To estimate the contribution due to light
edges we sub-sample them with rate ϵ[−][2] ∆E/T = ϵ[−][2]/ρ. The total space we use is _O(ϵ[−][1][√]m),_

_·_
which is optimal in this case. See Algorithm 1 for more details of the implementation.
**Value-Based Oracle.** We also consider the setting where the predictor returns an estimate[e] _p(e) of_
_Re, and we assume Re_ _p(e)_ _α_ _Re, where α_ 1 is an approximation factor. We relax this
assumption to also handle additive error as well as noise, but for intuition we focus on this case. ≤ _≤_ _·_ _≥_
The value-based oracle setting requires the use of novel techniques, such as the use of exponential
random variables (ERVs). Given this oracle, for an edge e, we compute p(e)/ue, were ue is a
standard ERV. We then store the O(α log(1/ϵ)) edges e for which p(e)/ue is largest. Since we
are in the adjacency list model, once we start tracking edge e = xy, we can also compute the
true value Re of triangles that the edge e participates in (since for each future vertex z we see,
we can check if x and y are both neighbors of z). Note that we track this quantity only for the
_O(α log(1/ϵ)) edges that we store. Using the max-stability property of ERVs, maxe Re/ue is equal_
in distribution to T/u, where u is another ERV. Importantly, using the density function of an ERV,
one can show that the edge e for which Re/ue is largest is, with probability 1 − _O(ϵ[3]), in our list_
of the O(α log(1/ϵ)) largest p(e)/ue values that we store. Repeating this scheme r = O(1/ϵ[2])
times, we obtain independent estimates T/u[1], . . ., T/u[r], where u[1], . . ., u[r] are independent ERVs.
Taking the median of these then gives a (1 ± ϵ)-approximation to the total number T of triangles.
We note that ERVs are often used in data stream applications (see, e.g., Andoni (2017)), though to
the best of our knowledge they have not previously been used in the context of triangle estimation.
We also give an alternative algorithm, based on subsampling at O(log n) scales, which has worse
logarithmic factors in theory but performs well empirically.

3 TRIANGLE COUNTING IN THE ARBITRARY ORDER MODEL

In this section we discuss Algorithm 4 for estimating the number of triangles in an arbitrary order
stream of edges. The pseudo-code of the algorithm as well as omitted proofs for the different
oracles are given in Supplementary Section C. Here we give the intuition behind the algorithm. Our
approach relies on sampling the edges of the stream as they arrive and checking if every new edge
forms a triangle with the previously sampled edges. However, as previously discussed, this approach
alone fails if some edges have a large number of triangles incident to them as “overlooking” such
edges might lead to an underestimation of the number of triangles. Therefore, we utilize a heavy
edge oracle, and refer to edges that are not heavy as light. Whenever a new edge arrives, we first


-----

query the oracle to determine if the edge is heavy. If the edge is heavy we keep it, and otherwise
we sample it with some probability. As in the adjacency list arrival model case, this strategy allows
us to reduce the variance of our estimator. By balancing the sampling rate and our threshold for
heaviness, we ensure that the space requirement is not too high, while simultaneously guaranteeing
that our estimate is accurate.

In more detail, our algorithm works as follows. First, we set a heaviness threshold ρ, so that if we
predict an edge e to be part of ρ or more triangles, we label it as heavy. We also set a sampling
parameter p. We let H be the set of edges predicted to be heavy and let SL be a random sample of
edges predicted to be light. Then, we count three types of triangles. The first counter, ℓ1, counts the
triangles ∆= (e1, e2, e), where the first two edges seen in this triangle by the algorithm, represented
by e1 and e2, are both in SL. Note that we only count the triangle if e1 and e2,were both in SL at
the time e arrives in the stream. Similarly, ℓ2 counts triangles whose first two edges are in SL and
_H (in either order), and ℓ3 counts triangles whose first two edges are in H. Finally, we return the_
estimate ℓ = ℓ1/p[2] + ℓ2/p + ℓ3. Note that if the first two edges in any triangle are light, they will
both be in SL with probability p[2], and if exactly one of the first two edges is light, it will be in SL
with probability p. Therefore, we divide ℓ1 by p[2] and ℓ2 by p so that ℓ is an unbiased estimator.

4 EXPERIMENTS

We now evaluate our algorithm on real and synthetic data whose properties are summarized in
Table 2 (see Appendix F for more details).

Table 2: Datasets used in our experiments. Snapshot graphs are a sequence of graphs over time (the
length of the sequence is given in parentheses) and temporal graphs are formed by edges appearing
over time. The listed values for n (number of vertices), m (number of edges), and T (number of
triangles) for Oregon and CAIDA are approximated across all graphs. The Oregon and CAIDA
datasets come from Leskovec & Krevl (2014); Leskovec et al. (2005), the Wikibooks dataset comes
from Rossi & Ahmed (2015), the Reddit dataset comes from Leskovec & Krevl (2014); Kumar et al.
(2018), the Twitch dataset comes from Rozemberczki et al. (2019), the Wikipedia dataset comes
from Rossi & Ahmed (2015), and the Powerlaw graphs are sampled from the Chung-Lu-Vu random
graph model with expected degree of the i-th vertex proportional to 1/i[2] (Chung et al., 2003).

**Name** **Type** **Predictor** _n_ _m_ _T_

Oregon Snapshot (9) 1st graph _∼_ 10[4] _∼_ 2.2 · 10[4] _∼_ 1.8 · 10[4]

CAIDA 2006 Snapshot (52) 1st graph _∼_ 2.2 · 10[4] _∼_ 4.5 · 10[4] _∼_ 3.4 · 10[4]

CAIDA 2007 Snapshot (46) 1st graph _∼_ 2.5 · 10[4] _∼_ 5.1 · 10[4] _∼_ 3.9 · 10[4]

Wikibooks Temporal Prefix _∼_ 1.3 · 10[5] _∼_ 3.9 · 10[5] _∼_ 1.8 · 10[5]

Reddit Temporal Regression _∼_ 3.6 · 10[4] _∼_ 1.2 · 10[5] _∼_ 4.1 · 10[5]

Twitch -  GNN _∼_ 6.5 · 10[3] _∼_ 5.7 · 10[4] _∼_ 5.4 · 10[4]

Wikipedia -  GNN _∼_ 4.8 · 10[3] _∼_ 4.6 · 10[4] _∼_ 9.5 · 10[4]

Powerlaw Synthetic EV _∼_ 1.7 · 10[5] _∼_ 10[6] _∼_ 3.9 · 10[7]

We now describe the edge heaviness predictors that we use (see also Table 2). Our predictors adapt
to the type of dataset and information available for each dataset. Some datasets we use contain
only the graph structure (nodes and edges) without semantic features, thus not enabling us to train a
classical machine learning predictor for edge heaviness. In those cases we use the true counts on a
small prefix of the data (either 10% of the first graph in a sequence of graphs, or a prefix of edges
in a temporal graph) as predicted counts for subsequent data. However, we are able to create more
sophisticated predictors on three of our datasets, using feature vectors in linear regression or a Graph
Neural Network. Precise details of the predictors follow.

_• Snapshot: For Oregon / CAIDA graph datasets, which contain a sequence of graphs, we use_
exact counting on a small fraction of the first graph as the predictor for all the subsequent graphs.
Specifically, we count the number of triangles per edge, Ne, on the first graph for each snapshot
dataset. We then only store 10% of the top heaviest edges and use these values as estimates for edge
heaviness in all later graphs. If a queried edge is not stored, its predicted Ne value is 0.


-----

_• Prefix: In the WikiBooks temporal graph, we use the exact Ne counts on the first half of the graph_
edges (when sorted by their timestamps) as the predicted values for the second half.

_• Linear Regression:_ In the Reddit Hyperlinks temporal graph, we use a separate dataset
(Kumar et al., 2019) that contains 300-dimensional feature embeddings of subreddits (graph
nodes). Two embeddings are close in the feature space if their associated subreddits have
similar sub-communities. To produce an edge f (e) embedding for an edge e = uv from
the node embedding of its endpoints, f (u) and f (v), we use the 602-dimensional embedding
(f (u), f (v), ∥(f (u) − _f_ (v)∥1, ∥(f (u) − _f_ (v)∥2). We then train a linear regressor to predict Ne
given the edge embedding f (e). Training is done on a prefix of the first half of the edges.

_• Link Prediction (GNN): For each of the two networks, we start with a graph that has twice as_
many edges as listed in Table 2, (∼ 1.1 · 10[5] edges for Twitch and 9.2 · 10[4] edges for Wikipedia).
We then randomly remove 50% of the total edges to be the training data set, and use the remaining
edges as the graph we test on. We use the method proposed in Zhang & Chen (2018) to train a link
prediction oracle using a Graph Neural Network (GNN) that will be used to predict the heaviness
of the testing edges. For each edge that arrives in the stream of the test edges, we use the predicted
likelihood of forming an edge given by the the neural network to the other vertices as our estimate
for Nuv, the number of triangles on edge uv. See Section F.2 for details of training methodology.

_• Expected Value (EV): In the Powerlaw graph, the predicted number of triangles incident to each_
_Ne is its expected value, which can be computed analytically in the CLV random graph model._

**Baselines.** We compare our algorithms with the following baselines.

_• ThinkD and WRS (Arbitrary Order): These are the state of the art empirical one-pass algorithms_
from Shin et al. (2018) and Shin (2017) respectively. The ThinkD paper presents two versions of
the algorithm, called ‘fast’ and ‘accurate’. We use the ‘accurate’ version since it provides better
estimates. We use the authors’ code for our experiments (Shin et al., 2020; Shin, 2020).

_• MVV (Arbitrary Order and Adjacency List): We use the one pass streaming algorithms given in_
McGregor et al. (2016) for the arbitrary order model and the adjacency list model.

**Error measurement. We measure accuracy using the relative error |1 −** _T/T[e]_ _|, where T is the true_
triangle count and _T is the estimate returned by an algorithm. Our plots show the space used by an_
algorithm (in terms of the number of edges) versus the relative error. We report median errors over
50 independent executions of each experiment, one standard deviation.

[e] _±_

4.1 RESULTS FOR ARBITRARY ORDER TRIANGLE COUNTING EXPERIMENTS

In this section, we give experimental results for Algorithm 4 which approximates the triangle count
in arbitrary order streams. Note that we need to set two parameters for Algorithm 4: p, which is the
edge sampling probability, and ρ, which is the heaviness threshold. In our theoretical analysis, we
assume knowledge of a lower bound on T in order to set p and ρ, as is standard in the theoretical
streaming literature. However, in practice, such an estimate may not be available; in most cases,
the only parameter we are given is a space bound for the number of edges that can be stored. To
remedy this discrepancy, we modify our algorithm slightly by setting a fixed fraction of space to use
for heavy edges (10% of space for all of our experiments) and setting p correspondingly to use up
the rest of the space bound given as input. See details in Supplementary Section F.3.

**Oregon and CAIDA** In Figures 1(a) and 1(b), we display the relative error as a function of increasing space for graph #4 in the dataset for Oregon and graph #30 for CAIDA 2006. These
figures show that our algorithm outperforms the other baselines by as much as a factor of 5. We do
not display the error bars for MVV and WRS for the sake of visual clarity, but they are comparable
to or larger than both ThinkD and our algorithm. A similar remark applies to all figures in Figure
1. As shown in Figure 2, these specific examples are reflective of the performance of our algorithm
across the whole sequence of graphs for Oregon and CAIDA. We also show qualitatively similar
results for CAIDA 2007 in Figure 4(a) in Supplementary Section F.4.

We also present accuracy results over the various graphs of the Oregon and CAIDA datasets. We
fix the space to be 10% of the number of edges (which varies across graphs). Our results for the
Oregon dataset are plotted in Figure 2(a) and the results for CAIDA 2006 and 2007 are plotted in


-----

(d) Wikibooks


(e) Wikipedia


(f) Powerlaw


0.25 Oregon, Graph#4 Our Alg 0.30 Caida-2006, Graph#30 Our Alg 0.40 Reddit Hyperlinks Graph Our Alg

ThinkD 0.25 ThinkD 0.35 ThinkD

0.20 MVV MVV 0.30 MVV

WRS 0.20 WRS 0.25 WRS

0.15

0.15 0.20

Relative Error0.10 Relative Error0.10 Relative Error0.15

0.10

0.05 0.05 0.05

0.000 5 10[3] 1 10[4] 1.5 10[4] 2 10[4] 0.00 0 1 10[4] 2 10[4] 3 10[4] 4 10[4] 0.00 0 0.5 10[4] 1 10[4] 1.5 10[4] 2 10[4] 2.5 10[4]

Space Space Space

(a) Oregon (b) CAIDA 2006 (c) Reddit

0.40 Wikibooks Graph Wikipedia Graph Powerlaw Graph

Our Alg 0.14 Our Alg 0.16 Our Alg

0.350.30 ThinkDMVV 0.12 ThinkDMVV 0.14 ThinkDMVV

0.25 WRS 0.10 WRS 0.120.10 WRS

0.20 0.08 0.08

Relative Error0.15 Relative Error0.06 Relative Error0.06

0.10 0.04 0.04

0.05 0.02 0.02

0.00 1 10[4] 2 10[4] 3 10[4] 4 10[4] 5 10[4] 6 10[4] 0.00 0 1 10[4] 2 10[4] 3 10[4] 4 10[4] 0.00 1 10[4] 2 10[4] 3 10[4] 4 10[4] 5 10[4] 6 10[4]

Space Space Space


Figure 1: Error as a function of space in the arbitrary order model.





0.25 Oregon, Space = 0.1m 0.25 Caida-2006, Space = 0.1m Caida-2007, Space = 0.1m

Our Alg Our Alg 0.16 Our Alg

0.20 ThinkDWRS 0.20 ThinkDWRS 0.14 ThinkDWRS

0.12

0.15 0.15 0.10

0.08

Relative Error0.10 Relative Error0.10 Relative Error0.06

0.05 0.05 0.04

0.02

0.00 2 3 4 5 6 7 8 9 0.00 0 10 20 30 40 50 0.00 0 10 20 30 40

Graph # Graph # Graph #

(a) Oregon


Graph #

(b) CAIDA 2006


Graph #

(c) CAIDA 2007


Figure 2: Error across snapshot graphs with space 0.1m in the arbitrary order model.

Figures 2(b) and 2(c), respectively. These figures illustrate that the quality of the predictor remains
consistent over time even after a year has elapsed between when the first and the last graphs were
created as our algorithm outperforms the baselines on average by up to a factor of 2.

**Reddit Our results are displayed in Figure 1(c). All four algorithms are comparable as we vary**
space. While we do not improve over baselines in this case, this dataset serves to highlight the fact
that predictors can be trained using node or edge semantic information (i.e., features).

**Wikibooks and Powerlaw For Wikibooks, we see in Figure 1(d) that our algorithm is outperform-**
ing ThinkD and WRS by at least a factor of 2, and these algorithms heavily outperform the MVV
algorithm. Nonetheless, it is important to note that our algorithm uses the exact counts on the first
half of the edges (encoded in the predictor), which encode a lot information not available to the
baselines. Thus the takeaway is that in temporal graphs, where edges arrive continuously over time
(e.g., citation networks, road networks, etc.), using a prefix of the edges to form noisy predictors can
lead to a significant advantage in handling future data on the same graph. Our results for Powerlaw
are presented in Figure 1(f). Here too we see that as the space allocation increases, our algorithm
outperforms ThinkD, MVV, and WRS.

**Twitch and Wikipedia** In Figure 1(e), we see that three algorithms, ours, MVV, and WRS, are
all comparable as we vary space. The qualitatively similar result for Twitch is given in Figure 4(b).
These datasets serve to highlight that predictors can be trained using modern ML techniques such as
Graph Neural Networks. Nevertheless, these predictors help our algorithms improve over baselines
for experiments in the adjacency list model (see Section below).

**Results for Adjacency List Experiments** Our experimental results for adjacency list experiments,
which are qualitatively similar to the arbitrary order experiments, are given in full detail in Sections
F.5 and F.6.


-----

ACKNOWLEDGMENTS

Justin is supported by the NSF Graduate Research Fellowship under Grant No. 1745302 and MathWorks Engineering Fellowship. Sandeep and Shyam are supported by the NSF Graduate Research
Fellowship under Grant No. 1745302. Ronitt was supported by NSF awards CCF-2006664, DMS
2022448, and CCF-1740751. Piotr was supported by the NSF TRIPODS program (awards CCF1740751 and DMS-2022448), NSF award CCF-2006798 and Simons Investigator Award. Talya is
supported in part by the NSF TRIPODS program, award CCF-1740751 and Ben Gurion University
Postdoctoral Scholarship. Honghao Lin and David Woodruff would like to thank for partial support
from the National Science Foundation (NSF) under Grant No. CCF-1815840.

REFERENCES

Nesreen Ahmed and Nick Duffield. Adaptive shrinkage estimation for streaming graphs.
In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances
_in Neural Information Processing Systems, volume 33, pp. 10595–10606. Curran Asso-_
[ciates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/](https://proceedings.neurips.cc/paper/2020/file/780261c4b9a55cd803080619d0cc3e11-Paper.pdf)
[780261c4b9a55cd803080619d0cc3e11-Paper.pdf.](https://proceedings.neurips.cc/paper/2020/file/780261c4b9a55cd803080619d0cc3e11-Paper.pdf)

Nesreen K. Ahmed, Nick Duffield, Theodore L. Willke, and Ryan A. Rossi. On sampling from
massive graph streams. _Proc. VLDB Endow., 10(11):1430–1441, August 2017._ ISSN 2150[8097. doi: 10.14778/3137628.3137651. URL https://doi.org/10.14778/3137628.](https://doi.org/10.14778/3137628.3137651)
[3137651.](https://doi.org/10.14778/3137628.3137651)

Mohammad Al Hasan and Vachik S Dave. Triangle counting in large networks: a review. Wiley
_Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 8(2):e1226, 2018._

Alexandr Andoni. High frequency moments via max-stability. In 2017 IEEE International Confer_ence on Acoustics, Speech and Signal Processing, ICASSP 2017, New Orleans, LA, USA, March_
_5-9, 2017, pp. 6364–6368. IEEE, 2017._

Alexandr Andoni, Collin Burns, Ying Li, Sepideh Mahabadi, and David P. Woodruff. Streaming
complexity of svms. In APPROX-RANDOM, 2020.

Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations. Cambridge University Press, 1999. doi: 10.1017/CBO9780511624216.

Albert Atserias, Martin Grohe, and Daniel Marx. Size bounds and query plans for relational joins.
In 49th Annual IEEE Symposium on Foundations of Computer Science, 2008.

Maria-Florina Balcan, Vaishnavh Nagarajan, Ellen Vitercik, and Colin White. Learning-theoretic
foundations of algorithm configuration for combinatorial partitioning problems. In Conference
_on Learning Theory, pp. 213–274. PMLR, 2017._

Maria-Florina Balcan, Travis Dick, Tuomas Sandholm, and Ellen Vitercik. Learning to branch. In
_International Conference on Machine Learning, 2018a._

Maria-Florina Balcan, Travis Dick, and Ellen Vitercik. Dispersion for data-driven algorithm design,
online learning, and private optimization. In 2018 IEEE 59th Annual Symposium on Foundations
_of Computer Science (FOCS), pp. 603–614. IEEE, 2018b._

Maria-Florina Balcan, Travis Dick, and Manuel Lang. Learning to link. _arXiv preprint_
_arXiv:1907.00533, 2019._

Etienne Bamas, Andreas Maggiori, and Ola Svensson. The primal-dual method for learning augmented algorithms. In Advances in Neural Information Processing Systems, 2020.

Ziv Bar-Yossef, Ravi Kumar, and D. Sivakumar. Reductions in streaming algorithms, with an application to counting triangles in graphs. In 13th Annual ACM-SIAM Symposium on Discrete
_Algorithms, 2002._

Suman K. Bera and Amit Chakrabarti. Towards tighter space bounds for counting triangles and
other substructures in graph streams. In Symposium on Theoretical Aspects of Computer Science
_(STACS 2017), 2017._


-----

Suman K. Bera and C. Sheshadhri. How the degeneracy helps for traingle counting in graph streams.
In ACM Symposium on Principles of Database Systems, June 2020.

Vladimir Braverman, Rafail Ostrovsky, and Dan Vilenchik. How hard is counting triangles in the
streaming model? In International Colloquium on Automata, Languages, and Programming, pp.
244–254. Springer, 2013.

Fan Chung, Linyuan Lu, and Van Vu. The spectra of random graphs with given expected degrees. In_[ternet Math., 1(3):257–275, 2003. URL https://projecteuclid.org:443/euclid.](https://projecteuclid.org:443/euclid.im/1109190962)_
[im/1109190962.](https://projecteuclid.org:443/euclid.im/1109190962)

Edith Cohen, Ofir Geri, and Rasmus Pagh. Composable sketches for functions of frequencies:
Beyond the worst case. In International Conference on Machine Learning. PMLR, 2020.

Hanjun Dai, Elias Khalil, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial optimization algorithms over graphs. In Advances in Neural Information Processing Systems, pp.
6351–6361, 2017.

Michael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Faster
matchings via learned duals. In Advances in Neural Information Processing Systems, 2021. URL
[https://arxiv.org/abs/2107.09770.](https://arxiv.org/abs/2107.09770)

Yihe Dong, Piotr Indyk, Ilya P Razenshteyn, and Tal Wagner. Learning space partitions for nearest
neighbor search. ICLR, 2020.

Elbert Du, Franklyn Wang, and Michael Mitzenmacher. Putting the “learning” into learningaugmented algorithms for frequency estimation. In International Conference on Machine Learn_ing, 2021._

Talya Eden, Piotr Indyk, Shyam Narayanan, Ronitt Rubinfeld, Sandeep Silwal, and Tal Wagner.
Learning-based support estimation in sublinear time. In International Conference on Learning
_[Representations, 2021. URL https://openreview.net/forum?id=tilovEHA3YS.](https://openreview.net/forum?id=tilovEHA3YS)_

Illes J Farkas, Imre Der´enyi, A-L Barab´asi, and Tamas Vicsek. Spectra of” real-world” graphs:
Beyond the semicircle law. In The Structure and Dynamics of Networks, pp. 372–383. Princeton
University Press, 2011.

Paolo Ferragina, Fabrizio Lillo, and Giorgio Vinciguerra. Why are learned indexes so effective? In
_International Conference on Machine Learning, pp. 3123–3132. PMLR, 2020._

Brooke Foucault Welles, Anne Van Devender, and Noshir Contractor. Is a ”friend” a friend? investigating the structure of friendship networks in virtual worlds. In CHI 2010 - The 28th Annual CHI
_Conference on Human Factors in Computing Systems, Conference Proceedings and Extended Ab-_
_stracts, pp. 4027–4032, June 2010. ISBN 9781605589312. doi: 10.1145/1753846.1754097. 28th_
Annual CHI Conference on Human Factors in Computing Systems, CHI 2010 ; Conference date:
10-04-2010 Through 15-04-2010.

Sreenivas Gollapudi and Debmalya Panigrahi. Online algorithms for rent-or-buy with expert advice.
In International Conference on Machine Learning, pp. 2319–2327. PMLR, 2019.

Heitor Murilo Gomes, Jesse Read, Albert Bifet, Jean Paul Barddal, and Jo˜ao Gama. Machine
learning for streaming data: state of the art, challenges, and opportunities. SIGKDD Explor.,
21:6–22, 2019.

Guyue Han and Harish Sethu. Edge sample and discard: A new algorithm for counting triangles
in large dynamic graphs. In Proceedings of the 2017 IEEE/ACM International Conference on
_Advances in Social Networks Analysis and Mining 2017, ASONAM ’17, pp. 44–49, New York,_
NY, USA, 2017. Association for Computing Machinery. ISBN 9781450349932. doi: 10.1145/
[3110025.3110061. URL https://doi.org/10.1145/3110025.3110061.](https://doi.org/10.1145/3110025.3110061)

Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estimation
[algorithms. In International Conference on Learning Representations, 2019a. URL https:](https://openreview.net/forum?id=r1lohoCqY7)
[//openreview.net/forum?id=r1lohoCqY7.](https://openreview.net/forum?id=r1lohoCqY7)


-----

Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estimation
algorithms. In International Conference on Learning Representations, 2019b.

Zachary Izzo, Sandeep Silwal, and Samson Zhou. Dimensionality reduction for wasserstein
barycenter. In Advances in Neural Information Processing Systems, 2021.

Rajesh Jayaram and John Kallaugher. An optimal algorithm for triangle counting in the stream.
In Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques
_(APPROX/RANDOM 2021). Schloss Dagstuhl-Leibniz-Zentrum f¨ur Informatik, 2021._

Tanqiu Jiang, Yi Li, Honghao Lin, Yisong Ruan, and David P. Woodruff. Learning-augmented
data stream algorithms. In International Conference on Learning Representations, 2020. URL
[https://openreview.net/forum?id=HyxJ1xBYDH.](https://openreview.net/forum?id=HyxJ1xBYDH)

John Kallaugher and Eric Price. A hybrid sampling scheme for triangle counting. In Philip N. Klein
(ed.), Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms,
_SODA 2017, Barcelona, Spain, Hotel Porta Fira, January 16-19, pp. 1778–1797. SIAM, 2017._

John Kallaugher, Andrew McGregor, Eric Price, and Sofya Vorotnikova. The complexity of counting cycles in the adjacency list streaming model. In Proceedings of the 38th ACM SIGMOD_SIGACT-SIGAI Symposium on Principles of Database Systems, PODS ’19, pp. 119–133, New_
York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450362276. doi:
[10.1145/3294052.3319706. URL https://doi.org/10.1145/3294052.3319706.](https://doi.org/10.1145/3294052.3319706)

Mihail N. Kolountzakis, Gary L. Miller, Richard Peng, and Charalampos E. Tsourakakis. Efficient
triangle counting in large graphs via degree-based vertex partitioning. Lecture Notes in Computer
_[Science, pp. 15–24, 2010. ISSN 1611-3349. doi: 10.1007/978-3-642-18009-5 3. URL http:](http://dx.doi.org/10.1007/978-3-642-18009-5_3)_
[//dx.doi.org/10.1007/978-3-642-18009-5_3.](http://dx.doi.org/10.1007/978-3-642-18009-5_3)

Tim Kraska, Alex Beutel, Ed H Chi, Jeffrey Dean, and Neoklis Polyzotis. The case for learned
index structures. In Proceedings of the 2018 International Conference on Management of Data
_(SIGMOD), pp. 489–504. ACM, 2018._

Srijan Kumar, William L Hamilton, Jure Leskovec, and Dan Jurafsky. Community interaction and
conflict on the web. In Proceedings of the 2018 World Wide Web Conference on World Wide Web,
pp. 933–943. International World Wide Web Conferences Steering Committee, 2018.

Srijan Kumar, Xikun Zhang, and Jure Leskovec. Predicting dynamic embedding trajectory in temporal interaction networks. In Proceedings of the 25th ACM SIGKDD International Conference
_on Knowledge Discovery & Data Mining, pp. 1269–1278. ACM, 2019._

Silvio Lattanzi, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Online scheduling via learned weights. In Proceedings of the 31st Annual ACM-SIAM Symposium on Discrete
_Algorithms, pp. 1859–1877. SIAM, 2020._

[Jure Leskovec and Andrej Krevl. SNAP Datasets: Stanford large network dataset collection. http:](http://snap.stanford.edu/data)
[//snap.stanford.edu/data, June 2014.](http://snap.stanford.edu/data)

Jure Leskovec, Jon Kleinberg, and Christos Faloutsos. Graphs over time: Densification laws,
shrinking diameters and possible explanations. In Proceedings of the Eleventh ACM SIGKDD
_International Conference on Knowledge Discovery in Data Mining, KDD ’05, pp. 177–187,_
New York, NY, USA, 2005. Association for Computing Machinery. ISBN 159593135X. doi:
[10.1145/1081870.1081893. URL https://doi.org/10.1145/1081870.1081893.](https://doi.org/10.1145/1081870.1081893)

Jure Leskovec, Lars Backstrom, Ravi Kumar, and Andrew Tomkins. Microscopic evolution of social
networks. In Proceedings of the 14th ACM SIGKDD International Conference on Knowledge
_Discovery and Data Mining, KDD ’08, pp. 462–470, New York, NY, USA, 2008. Association for_
[Computing Machinery. ISBN 9781605581934. doi: 10.1145/1401890.1401948. URL https:](https://doi.org/10.1145/1401890.1401948)
[//doi.org/10.1145/1401890.1401948.](https://doi.org/10.1145/1401890.1401948)

Yongsub Lim and U Kang. Mascot: Memory-efficient and accurate sampling for counting local
triangles in graph streams. In Proceedings of the 21th ACM SIGKDD International Conference
_on Knowledge Discovery and Data Mining, KDD ’15, pp. 685–694, New York, NY, USA, 2015._
Association for Computing Machinery. ISBN 9781450336642. doi: 10.1145/2783258.2783285.
[URL https://doi.org/10.1145/2783258.2783285.](https://doi.org/10.1145/2783258.2783285)


-----

Mario Lucic, Matthew Faulkner, Andreas Krause, and Dan Feldman. Training gaussian mixture
models at scale via coresets. Journal of Machine Learning Research, 18(160):1–25, 2018. URL
[http://jmlr.org/papers/v18/15-506.html.](http://jmlr.org/papers/v18/15-506.html)

Thodoris Lykouris and Sergei Vassilvtiskii. Competitive caching with machine learned advice. In
_International Conference on Machine Learning, pp. 3296–3305. PMLR, 2018._

M. Mahoney. Large text compression benchmark., 2011.

Andrew McGregor and Sofya Vorotnikova. Triangle and four cycle counting in the data stream
model. PODS’20, pp. 445–456, New York, NY, USA, 2020. Association for Computing Machin[ery. ISBN 9781450371087. doi: 10.1145/3375395.3387652. URL https://doi.org/10.](https://doi.org/10.1145/3375395.3387652)
[1145/3375395.3387652.](https://doi.org/10.1145/3375395.3387652)

Andrew McGregor, Sofya Vorotnikova, and Hoa T. Vu. Better algorithms for counting triangles in
data streams. In ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems,
June 2016.

R. Milo, S. Shen-Orr, S. Itzkovitz, N. Kashtan, D. Chklovskii, and U. Alon. Network motifs: Simple
building blocks of complex networks. Science, 298(5594):824–827, 2002. ISSN 0036-8075. doi:
[10.1126/science.298.5594.824. URL https://science.sciencemag.org/content/](https://science.sciencemag.org/content/298/5594/824)
[298/5594/824.](https://science.sciencemag.org/content/298/5594/824)

Michael Mitzenmacher. A model for learned bloom filters and optimizing by sandwiching. In
_Advances in Neural Information Processing Systems, 2018._

Michael Mitzenmacher. Scheduling with predictions and the price of misprediction. In ITCS, 2020.

Michael Mitzenmacher and Sergei Vassilvitskii. Algorithms with predictions. _arXiv preprint_
_arXiv:2006.09123, 2020._

Rasmus Pagh and Charalampos E. Tsourakakis. Colorful triangle counting and a mapreduce implementation. Inf. Process. Lett., 112(7):277–281, 2012. doi: 10.1016/j.ipl.2011.12.007. URL
[https://doi.org/10.1016/j.ipl.2011.12.007.](https://doi.org/10.1016/j.ipl.2011.12.007)

Arnau Prat-P´erez, David Dominguez-Sal, Josep M Brunat, and Josep-Lluis Larriba-Pey. Shaping
communities out of triangles. In Proceedings of the 21st ACM international conference on Infor_mation and knowledge management, pp. 1677–1681, 2012._

Manish Purohit, Zoya Svitkina, and Ravi Kumar. Improving online algorithms via ml predictions.
In Advances in Neural Information Processing Systems, pp. 9661–9670, 2018.

Jack Rae, Sergey Bartunov, and Timothy Lillicrap. Meta-learning neural bloom filters. In Interna_tional Conference on Machine Learning, pp. 5271–5280, 2019._

Piyush Rai, Hal Daum´e, and Suresh Venkatasubramanian. Streamed learning: One-pass svms.
_ArXiv, abs/0908.0572, 2009._

Dhruv Rohatgi. Near-optimal bounds for online caching with machine learned advice. In Proceed_ings of the 31st Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 1834–1845. SIAM,_
2020.

Ryan A. Rossi and Nesreen K. Ahmed. The network data repository with interactive graph analytics
[and visualization. In AAAI, 2015. URL http://networkrepository.com.](http://networkrepository.com)

Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding, 2019.

C. Seshadhri, Ali Pinar, and Tamara G. Kolda. Fast triangle counting through wedge sampling. In
_the International Conference on Data Mining (ICDM), 2013._

Kijung Shin. Wrs: Waiting room sampling for accurate triangle counting in real graph streams.
_2017 IEEE International Conference on Data Mining (ICDM), pp. 1087–1092, 2017._

Kijung Shin. Wrs: Waiting room sampling for accurate triangle counting in real graph streams.

[https://github.com/kijungs/waiting_room, 2020.](https://github.com/kijungs/waiting_room)


-----

Kijung Shin, Jisu Kim, Bryan Hooi, and Christos Faloutsos. Think before you discard: Accurate
triangle counting in graph streams with deletions. In Joint European Conference on Machine
_Learning and Knowledge Discovery in Databases, pp. 141–157. Springer, 2018._

Kijung Shin, Jisu Kim, Bryan Hooi, and Christos Faloutsos. Think before you discard: Accu[rate triangle counting in graph streams with deletions. https://github.com/kijungs/](https://github.com/kijungs/thinkd)
[thinkd, 2020.](https://github.com/kijungs/thinkd)

Lorenzo De Stefani, Alessandro Epasto, Matteo Riondato, and Eli Upfal. TriEst: Counting local and[`]
global triangles in fully dynamic streams with fixed memory size. ACM Trans. Knowl. Discov.
_[Data, 11(4), June 2017. ISSN 1556-4681. doi: 10.1145/3059194. URL https://doi.org/](https://doi.org/10.1145/3059194)_
[10.1145/3059194.](https://doi.org/10.1145/3059194)

Kapil Vaidya, Eric Knorr, Tim Kraska, and Michael Mitzenmacher. Partitioned learned bloom filter.
In International Conference on Learning Representations, 2021.

Sofya Vorotnikova. Improved 3-pass algorithm for counting 4-cycles in arbitrary order streaming.
_[CoRR, abs/2007.13466, 2020. URL https://arxiv.org/abs/2007.13466.](https://arxiv.org/abs/2007.13466)_

Jun Wang, Wei Liu, Sanjiv Kumar, and Shih-Fu Chang. Learning to hash for indexing big data - a
survey. Proceedings of the IEEE, 104(1):34–57, 2016.

Alexander Wei. Better and simpler learning-augmented online caching. In Approximation, Ran_domization, and Combinatorial Optimization. Algorithms and Techniques (APPROX/RANDOM_
_2020). Schloss Dagstuhl-Leibniz-Zentrum f¨ur Informatik, 2020._

David P. Woodruff. Sketching as a tool for numerical linear algebra. Found. Trends Theor. Comput.
_Sci., 10:1–157, 2014._

Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. In Samy Bengio,
Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol`o Cesa-Bianchi, and Roman Garnett
(eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural
_Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr´eal, Canada,_
pp. 5171–5181, 2018.


-----

|TIONS TAB|BLE|
|---|---|
|Notation|Definition|
|G = (V, E)|a graph G with vertex set V and edge set E|
|n|number of triangles|
|m|number of edges|
|T|number of triangles (or 4-cycles)|
|ϵ|approximation parameter|
|< s|total ordering on the vertices according to their arrival in the adjacency list arrival model|
|∆|set of triangles in G|
|□|set of 4-cycles in G|
|N(v)|set of neighbors of node v|
|N xy|number of triangles (4-cycles) incident to edge xy, i.e., |{z |(x, y, z) ∈∆ }| ( |{w, z |(x, y, w.z) ∈□}| )|
|R xy|number of triangles (x, z, y) where x precedes z and z precedes y in the adjacency list arrival model, i.e., |{z |(x, z, y) ∈∆, x < z < y s s }||
|∆ E|maximum number of triangles incident to any edge|
|∆ V|maximum number of triangles incident to any vertex|
|O ρ|heavy edge oracle with threshold ρ|


B OMITTED PROOFS FROM SECTION 2

In this section, we prove correctness and bound the space of Algorithm 1, thus proving Theorem 1.1.
In the adjacency list model, recall that we have a total ordering on nodes <s based on the stream
ordering, where x <s y if and only if the adjacency list of x arrives before the adjacency list of y in
the stream. For each xy ∈ _E(G), we define Rxy = |z : {x, y, z} ∈_ ∆ and x <s z <s y|. Note that
if y <s x, then Rxy = 0. It holds that _xy_ _E_ _[R][xy][ =][ T]_ [.]

_∈_

We first give a detailed overview of the algorithm. We first consider the case when T (m/ϵ)[1][/][2].

[P] _≥_

LetT _ρ = (mT_ )[1][/][3]. We define the edge xy to be heavy if Rxy ≥ _ρ, light if Rxy ≤_ _[T]ρ_ [, and][ medium][ if]

_ρ_ _[< R][xy][ < ρ][. We train the oracle to predict, upon seeing][ xy][, whether][ R][xy][ is light or not. We define]_
_H, M_, and L as the set of heavy, medium, and light edges, respectively. Define TL = _xy_ _L_ _[R][xy][.]_
_∈_
We define TH and TM similarly. Since _xy_ _E_ _[R][xy][ =][ T][ =][ T][L][ +][ T][M][ +][ T][H]_ [, it suffices to estimate]

_∈_
each of these three terms. [P]

For the light edges, as shown in Lemma[P] B.2, if we sample edges with rate p1 _ϵ[−][2]/ρ, with_
high probability we obtain an estimate of TL within error ±ϵT . For the heavy edges, we recall an ≈
observation in McGregor et al. (2016): for an edge xy where Rxy is large, even if we do not sample
_xy directly, if we sample from the stream (at rate p3_ _ϵ[−][2]/ρ), we are likely to sample some edges_
in the set |xz : {x, y, z} ∈ ∆ and x <s z <s y|. We refer to the sampled set of these edges as ≈ _Saux._

Further, we will show that not only can we use Saux to recognize whether Rxy is large, but also to
estimate Rxy (Lemma B.3). What remains to handle is the medium edges. Since medium edges have
higher values of Rxy than light edges, we sample with a larger rate, p2, to reduce variance. However,
we show there cannot be too many medium edges, so we do not pay too much extra storage space.
The overall space we use is _O(ϵ[−][2]m[2][/][3]/T_ [1][/][3]).

When T < (m/ϵ)[1][/][2] our algorithm becomes much simpler. Let ρ = _m/ϵ. We define the edge xy_
to be heavy if Rxy _T/ρ, and[e]_ _light otherwise (so in this case all the medium edges become heavy[√]_
_≥_


-----

edges). We directly store all the heavy edges and sub-sample with rate ϵ[−][2]/ρ to estimate TL. The
total space we use is _O(ϵ[−][1][√]m), which is optimal in this case. See Algorithm 1 for more details of_
the implementation.

[e]

**Algorithm 1 Counting Triangles in the Adjacency List Model**

1: Input:LIGHT otherwise. Adjacency list edge stream and an oracle O that outputs HEAVY if Rxy ≥ _T/ρ and_

2: Output: An estimate of the triangle count T .
3: Initialize Al, Am, Ah = 0, SL, SM _, Saux = ∅_
4: if T ≥ (m/ϵ)[1][/][2] **then**
5: Set ρ = (mT )[1][/][3], p1 = αϵ[−][2]/ρ, p2 = min(βϵ[−][2]ρ/T, 1), p3 = γϵ[−][2] log n/ρ.

6: else
7: Set ρ = m[1][/][2]/ϵ, p1 = 0, p2 = 1, p3 = γϵ[−][2]/ρ. {α, β, and γ are large enough constants.}

8: while seeing edges adjacent to v do

10:9: **for allif a, b ab ∈ ∈** _NSL(v ∪) thenSM do Cab = Cab + 1. {Update the counter for all the sampled light and_
medium edges}

11: **for all av ∈** _Saux do_

12: _Bav = 1 {seen av twice}._

13: **for each incident edge vu do**

14: W.p. p3: Saux = {vu} ∪ _Saux, Bvu = 0_

15: **if O(uv) outputs LIGHT then**

16: **if uv ∈** _SL then Al = Al + Cuv. {Finished counting Ruv}_

17: **else w.p. p1, SL = {vu} ∪** _SL_

18: **else {uv is either MEDIUM or HEAVY}**

19: # := |{z : uz ∈ _Saux, Buz = 1, z ∈_ _N_ (v)}| .

20: **if # ≥** _p3ρ then {uv is HEAVY}_

21: _Ah = Ah + # ._

22: **else if uv ∈** _SM then_

23: _Am = Am + Cuv. {Finished counting Ruv}_

24: **else**

24: With probability p2, SM = {vu} ∪ _SM ._

25: return: Al/p1 + Am/p2 + Ah/p3.

**Remark B.1. In the adjacency list model, we assume the oracle can predict whether Rxy** _Tc_

_or not, where Rxy is dependent on the node order. This may sometimes be impractical. However, ≥_
_observe that Nxy ≥_ _Rxy and_ _Nxy = 3T_ _. Hence, our proof still holds if we assume what the_
_oracle can predict is whether Nxy ≥_ _[T]c_ _[, which could be a more practical assumption.]_

[P]

Recall the following notation from Section 2. We define the edge xy to be heavy if Rxy _ρ, light if_
_≥_
_Rxy ≤_ _[T]ρ_ [, and][ medium][ if][ T]ρ _[< R][xy][ < ρ][. We define][ H, M]_ [, and][ L][ as the set of heavy, medium, and]

light edges, respectively. Define TL = _xy_ _L_ _[R][xy][. We define][ T][H][ and][ T][M][ similarly. Also, recall]_

_∈_
that since _xy_ _E_ _[R][xy][ =][ T][ =][ T][L][ +][ T][M][ +][ T][H]_ [, it suffices to estimate of these three terms.]

_∈_

[P]

**Lemma B.2.[P] Let A be an edge set such that for each edge xy ∈** _A, Rxy ≤_ _[T]c_ _[, for some parameter]_

_c, and let S be a subset of A such that every element in A is included in S with probability p =_ _τ[1]_

_for τ = ϵ[2]_ _· c/α, so that 1/τ = αϵ[−][2 1]c_ _[independently, for a sufficient large constant][ α][. Then, with]_

_probability at least 9/10, we have_


_Rxy_ _ϵT_
_xyX∈A_ _±_


_Rxy =_
_xyX∈S_


-----

_Proof. We let Y = τ_ _i_ _S_ _[R][i][ =][ τ][ P]i_ _A_ _[f][i][R][i][, where][ f][i][ = 1][ when][ i][ ∈]_ _[S][ and][ f][i][ = 0][ otherwise. It]_

_∈_ _∈_

follows that E[Y ] = TA. Then,

[P]

**Var[Y ] = E[Y** [2]] − E[Y ][2]


= τ [2]

_≤_ _τ_ [2]


E[ _fiRi[2]_ [+] _fifjRiRj]_ _TL[2]_

  _−_

Xi∈A Xi≠ _j_
 



[2] E[ _fiRi[2][] +]_



Xi∈A


_Ri[2]_ _[.]_

Xi∈A


_TL[2]_

 _−_


_τ_ [2][ R][i][R][j]


_i,j_


= τ


To bound this, we notice that Ri _c_ [and][ P]i _[R][i][ =][ T][A][ ≤]_ _[T]_ [, so]
_≤_ _[T]_

_τ_ _Ri[2]_ _Ri_ _τ_ _[T]_
Xi∈A _[≤]_ _[τ][ ·][ max]i∈A_ _[R][i][ ·]_ Xi∈A _≤_ _·_ _c_ _[·][ T][ =][ τ]c [T][ 2][ = 1]α_ _[ϵ][2][T][ 2][ .]_


By Chebyshev’s inequality, we have that

1

_α_ _[ϵ][2][T][ 2]_

Pr[|Y − _TA| > ϵT_ ] ≤ _ϵ[2]T_ [2] _≤_ _α [1]_ _[.]_


The following lemma shows that for the heavy edges, we can use the edges we have sampled to
recognize and estimate them.

**Lemma B.3. Let xy be an edge in E. Assume we sample the edges with rate p3 = βϵ[−][2]** log n/ρ in
_the stream for a sufficiently large constant β, and consider the set_

# = |xz has been sampled : {x, y, z} ∈ ∆ and x <s z <s y| .

_Then we have the following: if Rxy ≥_ _ρ2_ _[, with probability at least]1_ [ 1][ −] _n1[5][, we have][ p]3[−][1]_ _· # =_

(1 ± ϵ)Rxy. If Rxy < _[ρ]2_ _[, with probability at least][ 1][ −]_ _n[5][, we have][ p]3[−][1]_ _· # < ρ._

_Proof. We first consider the case when Rxy ≥_ _[ρ]2_ [.]

For each edge i ∈|xz : {x, y, z} ∈ ∆ and x <s z <s y|, let Xi = 1 if i has been sampled, or
_Xi = 0 otherwise. We can see the Xi are i.i.d. Bernoulli random variables and E[Xi] = p3. By a_
Chernoff bound we have


2 exp _ϵ[2]p3_ _Rxy/3_
_≤_ _−_ _·_ _≤_ _n[1][5][ .]_
  


_Xi_ _p3Rxy_ _ϵp3_ _Rxy_
_−_ _| ≥_ _·_


**Pr** "|

Therefore, we have


_≤_ _n[1][5][ .]_


**Pr** _p[−]3_ [1] # _Rxy_ _ϵRxy_ =Pr
_|_ _·_ _−_ _| ≥_
 


_p[−]3_ [1]
_|_


_Xi_ _Rxy_ _ϵRxy_
_−_ _| ≥_


Alternatively, when Rxy < _[ρ]2_ [, we have]

**Pr** _|p[−]3_ [1] _· # −_ _Rxy| ≥_ 2[1] _[ρ]_ _≤_ _n[1][5][,]_
 


which means p[−]3 [1] # < ρ with probability at least 1

_·_ _−_


1

_n[5][ .]_


We are now ready to prove Theorem 1.1, restated here for the sake of convenience.


-----

**Theorem 1.1.** _There exists a one-pass algorithm,_ _Algorithm 1,_ _with space complexity[3]_

_O(min(ϵ[−][2]m[2][/][3]/T_ [1][/][3], ϵ[−][1]m[1][/][2])) in the adjacency list model that, using a learning-based ora_cle, returns a (1 ± ϵ)-approximation to the number T of triangles with probability at least[4]_ 7/10.
e

_Proof. We first consider the case when T ≥_ (m/ϵ)[1][/][2].

For each edge(1 ± ϵ)Rxy in Algorithm xy ∈ _H, from Lemma 1. If xy /∈_ _H B.3, then with probability we have that with probability at least 1 −_ _n1[5][,][ xy][ will not contribute to] 1 −_ _n1[5][,][ #][/p][ A][3][ =][h][.]_

Taking a union bound, we have that with probability at least 1 − 1/n[3],

_Ah/p3 = (1 ± ϵ)_ _Rxy = (1 ± ϵ)TH ._

_xyX∈H_

We now turn to deal with the light and medium edges. For a light edge xy, it satisfies Rxy _T/ρ_
and therefore we can invoke Lemma B.2 with the parameter c set to ρ. For a medium edge ≤ xy, it
satisfies Rxy _ρ = T/(T/ρ), and therefore we can invoke Lemma B.2 with the parameter c set to_
_T/ρ. Hence, we have that with probability ≤_ 4/5,
_Am/p2 = TM ± ϵT, Al/p1 = TL ± ϵT ._
Putting everything together, we have that with probability 7/10,
_|Ah/p3 + Am/p2 + Al/p1 −_ _T_ _| ≤_ 3ϵT.
Now we analyze the space complexity. We note that there are at most ρ medium edges. Hence, the
expected total space we use is O(mp3 + ρp2 + mp1) = O(ϵ[−][2]m[2][/][3]/T [1][/][3] log n)[5].

When T < (m/ϵ)[1][/][2], as described in Section 2, we only have two classes of edges: heavy and light.
We will save all the heavy edges and use sub-sampling to estimate TL. The analysis is almost the
same.

B.1 NOISY ORACLES FOR THE ADJACENCY LIST MODEL

_Proof of Theorem 1.4. We first consider the case that T < (m/ϵ)[1][/][2]. Recall that at this time, our_
algorithms becomes that we save all the heavy edges xy such that p(e) ≥ _ϵT/[√]m and sampling the_
light edges with rate pl = ϵ[−][1]/[√]m. We define L as the set of deemed light edges, H as the set of
deemed heavy edges and L = |L|, H = |H|. Let ρ = ϵT/[√]m, then we have the condition that for
every edge e,

1 _K_ _[ρ]_ _Pr[Oρ(e) = HEAVY]_ _K_ _[R][e]_
_−_ _·_ _Re_ _≤_ _≤_ _·_ _ρ [.]_

We will show that, (i) E[H] ≤ (K + 1) _√ϵm_ [,][ (][ii][)][ Var][[][A][L][/p][1][]][ ≤] [(4][K][ + 2)][ϵ][2][T][ 2][. Under the above]

conditions we can get that with probability at least 9/10 we can get a (1±O(√K)·ϵ)-approximation

of T by Chebyshev’s inequality.

Forin H (li are indeed), we divide light H =edges. Then, it is easy to see that Hh ∪ _Hl, where the edges in H |hH are indeedh| ≤_ _T/ρ heavy. For every light edges, theedges, and the edges_
probability that it is included in Hl is at most K · _[R]ρ[e]_ [, hence we have]

_√m_

E[ _Hh_ ] _K_ ( _[R][e]_
_|_ _|_ _≤_ _·_ _ρ_ [)][ ≤] _[K][ ·][ T]ρ_ [=][ K] _ϵ_

_eX∈H_

which implies E[H] ≤ (K + 1) _√ϵm_ [.]

whereFor (ii X), we also divide = _e_ _Ll,e has been sampled L = Ll ∪_ _L[R]h similarly. Then we have[e][,][ Y][ =][ P]e_ _Lh,e has been sampled Var[A[R]L[e]] ≤[. Similar to the proof in]2(Var[X] + Var[Y ]),_

_∈_ _∈_

Lemma B.2, we have

_T_

[P] **Var[X]** _p1_ _Re[2]_

_≤_ _eX∈L_ _[≤]_ _[p][1]_ _ρ [ρ][2][ =][ p][1][Tρ.]_

453The success probability can beWe useUsing Markov’s inequality, we have that with probability at least eO(f ) to denote O(f · polylog( 1 − _δ by runningf_ )). log(1/δ) copies of the algorithm and taking the median. 9/10, the total space we use is less than
10 times the expected total space.


-----

Now we bound Var[Y ]. For every heavy edge we have that Pr[e ∈ _Lh] ≤_ _K ·_ _Rρe_ [. Then, condition]

on the oracle Oρ, we have

_ρ_

E[Var[Y |Oρ]] ≤ _p1K_ _eX∈H_ _Re_ _Re[2]_ _[≤]_ _[p][1][K]_ _eX∈H_ _ρRe ≤_ _p1KρT ._

Also, we have

_ρ_

**Var[E[Y |Oρ]] < p1K** _eX∈H_ _Re_ _Re[2]_ _[≤]_ _[p][1][KρT .]_

**VarFrom[A Varl/p1[]Y ≤ ] =p11 E[(4][K][Var[ + 2)][Y |[ρT]O[ = (4]ρ]] + Var[K][ + 2)][E[[ϵ]Y[2] |[T]O[ 2]ρ[.]** ]] we know Var[Y ] ≤ 2p1KρT, which means

When T ≥ (m/ϵ)[1][/][2], recall that the oracle Oρ only needs to predict the edges that are medium
edges or light edges. So, we can use the same way to bound the expected space for the deemed
medium edges and the variance of the deemed light edges.

B.2 A VALUE PREDICTION ORACLE FOR THE ADJACENCY LIST MODEL

In this section, we consider two types of value-prediction oracles, and variants of Algorithm 1 that
take advantage of these oracles. Recall that given an edge xy, Rxy = |z : {x, y, z} ∈ ∆ and x <s
_z <s y| ._
**Definition B.4. A value-prediction oracle with parameter (α, β) is an oracle that, for any edge e,**
_outputs a value p(e) for which E[p(e)]_ _αRe + β, and Pr[p(e) <_ _[R]λ[e]_

_constant K and any λ ≥_ 1. _≤_ _[−]_ _[β][]][ ≤]_ _[Ke][−][λ][ for some]_

Our algorithm is based on the following stability property of exponential random variables (see
e.g. Andoni (2017))
**Lemma B.5. Let x1, x2, ..., xn > 0 be real numbers, and let u1, u2, ...un be i.i.d.** _standard_
_exponential random variables with parameter λ = 1. Then we have that the random variable_
max( _u[x][1]1_ _[, ...,][ x]u[n]n_ [)][ ∼] _u[x]_ _[, where][ x][ =][ P]i_ _[x][i][ and][ u][ ∼]_ [Exp(1)][.]

**Lemma B.6. Let r = O(ln(1/δ)/ϵ[2]) and ϵ ≤** 1/3. If we take r independent samples X1, X2, ..., Xr
_from Exp(1) and let X = median(X1, X2, ..., Xr), then with probability 1_ _δ, X_ [ln 2(1
_−_ _∈_ _−_
_ϵ), ln 2(1 + 5ϵ)]._

_Proof. We first prove the following statement: with probability 1 −_ _δ, F_ (X) ∈ [1/2 − _ϵ, 1/2 + ϵ],_
where F is the cumulative density function of a standard exponential random variable.

Consider the case when F (X) ≤ 1/2 − _ϵ. We use the indicator variable Zi where Zi = 1 if_
_F_ (Xi) ≤ 1/2 − _ϵ or Zi = 0 otherwise. Notice that when F_ (X) ≤ 1/2 − _ϵ, at least half of the Zi_
are 1, so Z = _i_ _[Z][i][ ≥]_ _[r/][2][. On the other hand, we have][ E][[][Z][] =][ r/][2][ −]_ _[rϵ][, by a Chernoff bound.]_

We thus have
**Pr[** _Z_ E[Z] _ϵr]_ 2e[−][ϵ][2][r/][3] _δ/2 ._

[P] _|_ _−_ _| ≥_ _≤_ _≤_

Therefore, we have with probability at most δ/2, F (X) ≤ 1/2 − _ϵ. Similarly, we have that with_
probability at most δ/2, F (X) ≥ 1/2 + ϵ. Taking a union bound, we have that with probability at
least 1 − _δ, F_ (X) ∈ [1/2 − _ϵ, 1/2 + ϵ]._

Now, condition on F (X) ∈ [1/2 − _ϵ, 1/2 + ϵ]. Note that F_ _[−][1](x) = −_ ln(1 − _x), so if we consider_
the two endpoints of F (X), we have

_F_ _[−][1](1/2 −_ _ϵ) = −_ ln(1/2 + ϵ) ≥ (1 − _ϵ) ln 2,_

and
_F_ _[−][1](1/2 + ϵ) = −_ ln(1/2 − _ϵ) ≤_ (1 + 5ϵ) ln 2.
which indicates that X ∈ [ln 2(1 − _ϵ), ln 2(1 + 5ϵ)]._


The algorithm utilizing this oracle is shown in Algorithm 2. As described in Section 2, here we runs
_O(_ _ϵ[1][2][ )][ copies of the subroutine and takes a medium of them. For each subroutine, we aim to output]_

the maximum value maxe Re/ue. We will show that with high probability for each subroutine,


-----

this maximum will be at least T/ log(K/ϵ). Hence, we only need to save the edge e, such that
(Re + β)/ue is larger than T/ log(K/ϵ). However, one issue here is we need the information of T .
We can remove this assumption when β = 0 using the following way: we will show that with high
probability, the total number of edges we save is less than H = O( _ϵ[1][2][ log][2][(][K/ϵ][)(][α][ +][ mβ/T]_ [))][, so]

every time when a new edge comes, we can search the minimum value M such that the total number
of edges e that in at least one subroutine (p(e) + β)/ue > M is less than H(we count an edge
multiple times if it occurs in multiple subroutines), then use M as the threshold. We can see M will
increase as the new edge arriving and it will be always less than H.

**Algorithm 2 Counting Triangles in the Adjacency List Model with a Value Prediction Oracle**

1: Input: Adjacency list edge stream and an value prediction oracle with parameter(α, β).
2: Output: An estimate of the number T of triangles.
3: Initialize X ←∅ and H = O( _ϵ[1][2][ log][2][(][K/ϵ][)(][α][ +][ mβ/T]_ [))][, and let][ c][ be some large constant.]

4: for i = 0 to cϵ[−][2] **do {Initialize sets for cϵ[−][2]** copies of samples, where Si stores edges, Qi stores
all the exponential random variables for each sampled edge, and Ai stores the current maximum
of each sample}

5: _S[i]_ _←∅, Q[i]_ _←∅, and A[i]_ _←_ 0 .

6: while seeing edges adjacent to y in the stream do
7: **for i = 0 to cϵ[−][2]** **do {Update the counters for each sample}**

9:8: **forFor all each incident edge ab ∈** _S[i]: if a, b yx ∈ doN_ (y) then Cab[i] _[←]_ _[C]ab[i]_ [+ 1]

10: **for i = 0 to cϵ[−][2]** **do**

11: **if xy ∈** _S[i]_ **then {Finished counting Cxy, update the current maximum}**

12: _A[i]_ _←_ max(A[i], Cxy[i] _[/u][i]xy[)][ .]_

13: **else {Put yx into the sample sets temporarily}**

14: Let _R[ˆ]yx be the predicted value of Ryx._

15: _Rˆyx ←_ _R[ˆ]yx + β._

16: Generate u[i]yx _[∼]_ [Exp(1)][. Set][ Q][i][ ←] _[Q][i][ ∪{][u]yx[i]_ _[}][, S][i][ ←]_ _[S][i][ ∪{][yx][}][.]_

17: Set M to be the minimal integer such that _i_ _[s][i][ ≤]_ _[H][, where][ s][i][ =][ |{][e][ ∈]_ _[E][ |][ ˆ]Re/u[i]e_ _[≥]_ _[M]_ _[}|][.]_

18: **for i = 0 to cϵ[−][2]** **do** Adjust the samples to be within the space limit
_{_ [P] _}_

19: Let Q[i] _←_ _Q[i]_ _\ {u[i]ab[}][, S][i][ ←]_ _[S][i][ \ {][ab][}][ if][ ˆ]Rab/u[i]ab_ _[< M][ for all][ ab][ ∈]_ _[E][.]_

20: for i = 0 to cϵ[−][2] **do**
21: _X ←_ _X ∪{A[i]}_

22: return: ln 2 · median(X) .

We are now ready to prove Theorem 1.5. We first recall the theorem.

**Theorem 1.5. Given an oracle with parameters (α, β), there exists a one-pass algorithm, Algo-**
_rithm 2, with space complexity O(ϵ[−][2]_ log[2](K/ϵ)(α + mβ/T )) in the adjacency list model that
_returns a (1 ± ϵ)-approximation to the number of triangles T with probability at least 7/10._

_Proof. It is easy to see that for the i-th subroutine, the value f_ (i) = maxe Re/u[i]e [is the sample from]
the distribution T/u, where u ∼ Exp(1). And if the edge ei, for which the true value Rei _/u[i]ei_ [is the]
maximum value among E, is included in S[i], then the output of the i-th subroutine will be a sample
from T/u. Intuitively, the prediction value p(e) will help us find this edge.

We will first show that with probability at least 9/10 the following events will happen:

-  (i) f (i) ≥ _c1T/ log(1/ϵ) for all i, where c1 is a constant._

-  (cii[2]1[T/]) Let[ log] s[2]i[(] be the number of the edge[K/ϵ][)][, then][ P]i _[s][i][ ≤]_ _[c][2][(][ 1]ϵ[2][ log] e in the[2][(][K/ϵ] i[)(]-th subroutine such that[α][ +][ mβ/T]_ [))][ for some constant] (p(e) + β[ c])[2]/u[.] _[i]e_ _[≥]_

-  (iii) p(ei) + β _c1f_ (i)/ log(K/ϵ) for all i.
_≥_


-----

For (i), recall that f (i) ∼ _T/u, where u ∼_ Exp(1). Hence, we have

1
**Pr[f** (i) < c1T/ log(1/ϵ)] = e[−] [log(1][/ϵ][)][/c][1]
_≤_ 100c


_ϵ[2][ .]_


Taking an union bound we get that with probability at least 99/100, (i) will happen. For each edge
_e, we have_
**Pr[p(e) + β < Re/λ] < Ke[−][λ],**
similarly we can get that with probability at least 99/100, (iii) will happen.

For (ii), we first bound the expectation of si. For each edge e, we have

**Pr** (p(e) + β)/u[i]e _[≥]_ _[c]1[2][T/][ log][2][(][K/ϵ][)]_ = Pr[ue ≤ _c[2]1[(][p][(][e][) +][ β][) log][2][(][K/ϵ][)][/T]_ []]
  _c[2]1[(][p][(][e][) +][ β][) log][2][(][K/ϵ][)][/T]_

_≤_

_c[2]1[(][αR][e]_ [+ 2][β][) log][2][(][K/ϵ][)][/T .]
_≤_

Hence, we have

(αRe + 2β) log[2](K/ϵ)

E[si] = _c[2]1_ = O(log[2](K/ϵ)(α + mβ/T )).
X


Using Markov’s inequality, we get that with probability at least 99/100, (ii) will happen. Finally,
taking a union bound, we can get with probability at least 9/10, all the three events will happen.

Now, conditioned on the above three events, we will show that all the subroutine i will output f (i).
For the subroutine i, from (ii) we know that M will be always smaller than O(T/ log[2](K/ϵ)), and
from (i) and (iii) we get that (p(ei)+β)/u[i]ei [will be saved]
in the subroutine i. From Lemma B.6 we get that with probability at least[≥] [Ω(][T/][ log][2][(][K/ϵ][))][. Hence, the edge] 7/10, we can finally get a[ e][i]
(1 ± ϵ)-approximation of T .

We note that when β = 0, the space complexity will become O( _ϵ[1][2][ log][2][(][K/ϵ][)][α][)][, and in this case]_

we will not need a lower bound of T .

We will also consider the following noise model.
**Definition B.7. A value-prediction oracle with parameter (α, β) is an oracle that, for any edge e,**
_outputs a valueconstant K and any p(e) λ for which ≥_ 1. **Pr[p(e) > λαRe +** _β] ≤_ _[K]λ_ _[, and][ Pr][[][p][(][e][)][ <][ R]λ[e]_ _[−]_ _[β][]][ ≤]_ _[K]λ_ _[for some]_

The algorithm for this oracle is shown in Algorithm 3.


We now state our theorem.
**Theorem B.1. Given an oracle with parameter (α, β), there exists a one-pass algorithm, Algo-**
_rithmthat returns a 3, with space complexity (1_ _√K_ _ϵ)-approximation to O(_ _ϵ[K][2]_ _α(log n T) with probability at least[3]_ log log n + mβ/T ) in the adjacency list model 7/10.

_±_ _·_   

_Proof. Define q(e) = p(e) + β, it is easy to see that from the condition we can get that_


**Pr[q(e) > 2λαRe]** _K_ [1] _q(e) < [R][e]_
_≤_ _λ_ [when][ R][e][ >][ 2][β,][ Pr] _λ_




_K_ [1]
_≤_ _λ_


We define the set Ei such that e ∈ _Ei if and only if q(e) ∈_ _Ii. We can see E[Ai/pi] =_ _e∈Ei_ _[R][e][.]_

Now, we bound Var[Ai/pi] when i 1.
_≥_

Let X = _e_ _Ei,Re_ 2[i][+1]β _[R][e][ and][ Y][ =][ P]e_ _Ei,Re>2[i][+1]β_ _[R][e][, then we have][ Var][[][A][i][]][ ≤]_ [2(][P][Var][[][X][]+]

_∈_ _≤_ _∈_

**Var[Y ]).**

[P]

Similar to the proof in Lemma B.2, we can get that


_Re[2]_
_e∈Ei,RXe≤2[i][+1]β_ _[≤]_ _[p][i][(2][i][+1][β][)][2]_


_T_

2[i][+1]β [=][ p][i][2][i][+1][βT.]


**Var[X]** _pi_
_≤_


-----

**Algorithm 3 Counting Triangles in the Adjacency List Model with a Value Prediction Oracle**

1: Input: Adjacency list edge stream and an value prediction oracle with parameter(α, β).
2: Output: An estimate of the number T of triangles.
3: Initialize Si[j] _i_
and pi = cϵ[−][←∅][2]2[i]β(log[and][ A] n)[j][2]/T[←] for some constant[0][ where][ i][ =][ O][(log] c. Define[ n][)][ and][ j] I[ =]0 = [0[ O][(log log], 2β) and[ n][)][.] I[ p]i = [2[0][ =][ cϵ][i]β,[−] 2[2][2][i][+1][i][β/T]β),
_H ←_ 0.

4: while seeing edges adjacent to y in the stream do
5: For all ab ∈ _Si[j][: if][ a, b][ ∈]_ _[N]_ [(][y][)][ then][ C][ab][ ←] _[C][ab][ + 1]_

6: **for each incident edge yx do**

7: **if xy ∈** _Si[j]_ **[then]**

9:8: **elseA[j]i** _[←]_ _[A]i[j]_ [+][ C][yx][ .]

10: Let _R[ˆ]yx be the predicted value of Ryx._

11: Search i such that ( R[ˆ]yx + β) ∈ _Ii_

12: For each j let Si[j] _i_

13: for i = 0 to O(log n) do[←] _[S][j]_ _[∪{][yx][}][ with probability][ p][i][.]_
14: _H_ _H + median(A[j]i_ [)][/p][i]
_←_

15: return: H .


For Var[Y ], recall that for an edge e such that Re 2[i][+1]β, we have Pr[e _Ei]_ _K_ _[q]R[(][e]e[)]_
_≥_ _∈_ _≤_

_K_ [2][i]R[+1]e _[β]_ [. Then, condition on the oracle][ O][θ][, we have]


_K_ [2][i][+1][β] _Re[2]_ [=][ p][i][K][2][i][+1][β]

_Re_

_Re≥X2[i][+1]β_


E[Var[Y |Oθ]] ≤ _pi_


_Re = piK2[i][+1]βT._
_Re≥X2[i][+1]β_


And


**Var[E[Y** _Oθ]] < pi_ _K_ [2][i][+1][β] _Re[2]_ [=][ p][i][K][2][i][+1][βT.]
_|_ _Re_

_Re≥X2[i][+1]β_

From Var[Y ] = E[Var[Y _Oθ]] + Var[E[Y_ _Oθ]] we get that Var[Y ]_ 2piK2[i][+1]βT, from which
_|_ 1 _|_ _≤_
we can get that Var[Ai/pi] = _p[2]i_ **[Var][[][A][i][] =][ O][(][K][(][ϵ/][ log][ n][)][2][T][ 2][)][. Using Chebyshev’s inequality]**

and taking a median over O(log log n) independent trials, we can get Xi = median(A[j]i [)][ satisfies]
_|Xi −_ [P]e∈Ei _[R][e][| ≤]_ _√K(ϵ/ log n)T with probability 1 −_ _O(1/ log n). A similar argument also_

algorithmholds for I 30 is a, which means that after taking a union bound, with probability (1 ± _√K · ϵ)- approximation of T_ . 9/10, the output of the

Now we analyze the space complexity of Algorithm 3.


**Var[E[Y |Oθ]] < pi**


For i ≥ 1, we have

_Tα_

E [ _Ei_ ] = E [e _Ei] +_ [e _Ei]_ E [e _Ei]_ +
_|_ _|_  _∈_ _∈_  _≤_  _∈_  2[i][−][1]β

_Re≤X2[i][−][1]β/α_ _Re>X2[i][−][1]β/α_ _Re≤X2[i][−][1]β/α_

 _Tα_  _Re_  _Tα_ 

2K [R][e] _K_

_≤_ _p(e) [+]_ 2[i][−][1]β 2[i][−][1]β [+] 2[i][−][1]β 2[i][−][1]β [,]

_Re≤X2[i][−][1]β/α_ _[≤]_ _Re≤X2[i][−][1]β/α_ _[≤]_ [(][K][ + 1)][ Tα]

and |E0| _≤_ _m, hence we have that the total expectation of the space is_ _i_ _[p][i][E][[][|][E][i][|][]][ =]_

_O(_ _ϵ[K][2]_ _α(log n)[3]_ log log n + mβ/T ).

[P]

  

C OMITTED PROOFS FROM SECTION 3

We first present Algorithm 4, and then continue to prove Theorem 1.2. In the following algorithm,
for two sets A, B of edges, we let wA,B represent the set of pairs of edges (u, v), where u ∈ _A, v ∈_
_B, and u, v share a common vertex._


-----

**Algorithm 4 Counting Triangles in the Arbitrary Order Model**

1: Input: Arbitrary order edge stream and an oracle Oρ that outputs HEAVY if Nxy > ρ and LIGHT
otherwise.

2: Output: An estimate of the triangle count T .
3: Initialize ρ = max _ϵT√m_ _, 1_, p = C max _ϵ√1T_ _[,]_ _ϵ[2]ρ·T_ for a constant C.

4: Initialize ℓ1, ℓ2, ℓ3 = 0, and SL, H = .

 _∅_ 

5: for every arriving edge e in the stream do
6: **if the oracle Oρ outputs HEAVY then**

7: Add e to H

8: **else**

9: Add e to SL with probability p.

10: **for each w ∈** _wSL,SL do_

11: **if (e, w) is a triangle then**

12: Increment ℓ1 = ℓ1 + 1.

13: **for each w ∈** _wSL,H do_

14: **if (e, w) is a triangle then**

15: Increment ℓ2 = ℓ2 + 1.

16: **for each w ∈** _wH,H do_

17: **if (e, w) is a triangle then**

18: Increment ℓ3 = ℓ3 + 1.

19: return: ℓ = ℓ1/p[2] + ℓ2/p + ℓ3.


C.1 PROOF OF THEOREM 1.2

Recall that we first assume that Algorithm 4 is given access to a perfect oracle Oρ. That is, for every
edge e ∈ _E,_

_Oρ(e) =_ LIGHT if Te ≤ _ρ_

HEAVY if Te > ρ,



where Te as the number of triangles incident to the edge e.

_Theorem 1.2. First, we assume that T ≥_ _ϵ[−][2]. If not, our algorithm can just store all of the edges,_
since m ≤ _ϵ[−][1]_ _· m/√T._

For each integer i ≥ 1, let E(i) denote the set of edges that are part of exactly i triangles, and let
_Ei = |E(i)|. Since each triangle has 3 edges, we have that_ _i≥1_ _[i][ ·][ E][(][i][) = 3][T.]_

LetFor every triangle T1 denote the set of triangles whose first two edges in the stream are light (according to ti in T1, let χi denote the indicator of the event that the first two edges of[P] _O tρ).i_
are sampled to SL, and let ℓ1 = _i_ _[χ][i][. Since each][ χ][i][ is][ 1][ with probability][ p][2][,][ Ex][[][χ][i][] =][ p][2][, and]_

**Var[χi] = p[2]** _−p[4]_ _≤_ _p[2]. For any two variables χi, χj, they must be uncorrelated unless the triangles_
_ti, tj share a light edge that is among the first two edges of[P]_ _ti and among the first two edges of tj._
Moreover, in this case, Cov[χi, χj] ≤ **Ex[χiχj] = P(χi = χj = 1). This event only happens if**
the first two edges of both ti and tj are sampled in SL, but due to the shared edge, this comprises 3
edges in total, so P(χi = χj = 1) = p[3]. Hence, if we define t[12]i as the first two edges of ti in the
stream for each triangle ti,
**Ex[ℓ1] = p[2]|T1|,**
and

**Var[ℓ1]** _p[2]_ + _p[3]_ _p[2]_ 1 + p[3][ X] _Te[2]_
_≤_ _≤_ _|T_ _|_

_tXi∈T1_ _t[12]iti,tXj_ _j∈T1_ _Te≤ρ_

_[∩][t][12][̸][=][∅]_


= p[2] 1 + p[3]
_|T_ _|_ _·_


_t[2]_ _· E(t) ≤_ (p[2] + 3p[3] _· ρ) · T ._
_t=1_

X


The first line follows by adding the covariance terms; the second line follows since each light edge e
has at most Te[2] [pairs][ (][t][i][, t][j][)][ intersecting at][ e][; the third line follows by summing over][ t][ =][ T][e][, ranging]
from 1 to ρ, instead over e; and the last line follows since _t_ 1 _[t][ ·][ E][(][t][) = 3][T]_ [.]

_≥_

[P]


-----

Now, let T2 denote the set of triangles whose first two edges in the stream are light and heavy (in
_peither order). For every triangle. Also all other triangles have no chance to contribute to t = (e1, e2, e3) in T2, e1, e2 are sampled to ℓ2. Hence, Ex SL[ ∪ℓ2] =H with probability p · T2. Also,_
two triangles ti, tj ∈T2 that intersect on an edge e have Cov(χi, χj) ̸= 0 only if e is light and
_e ∈_ _t[12]i_ _[, t]j[12][. In this case.]_
**Cov(χi, χj) ≤** **Ex[χiχj] ≤** _p,_
since if e is added to SL (which happens with probability p) then χi = χj = 1 as the other edges in
_t[12]i_ _[, t]j[12]_ [are heavy and are added to][ H][ with probability 1. Therefore,]


**Var[ℓ2] =**


_p +_
_tXi∈T2_


_p ≤_ (p + 3p · ρ) · T,


_ti,tj_ _∈T2_
_t[12]i_ _[∩][t]j[12][=][e, O][ρ][(][e][)=][LIGHT]_


by the same calculations as was done to compute Var[ℓ1].

Finally, let T3 denote the set of triangles whose first two edges in the stream are heavy. Then
_ℓ3 = |T3|._

Now, using the well known fact that Var[X +Y ] ≤ 2(Var[X]+Var[Y ]) for any (possibly correlated)
random variables X, Y, we have that

**Var[p[−][2]ℓ1 + p[−][1]ℓ2 + ℓ3] ≤** 2 _p[−][4](p[2]_ + 3p[3]ρ)T + p[−][2](p + 3pρ)T

4T (p[−][2] + 3p[−][1]ρ), 
_≤_

since ℓ3 has no variance. Moreover, Ex[p[−][2]ℓ1 + p[−][1]ℓ2 + ℓ3] = |T1| + |T2| + |T3| = T . So, by
Chebyshev’s inequality, since ℓ = ℓ1/p[2] + ℓ2/p + ℓ3,

Pr[ _ℓ_ _T_ _> ϵT_ ] < **[Var][[][ℓ][]]** _._
_|_ _−_ _|_ _ϵ[2]_ _T_ [2][ <][ 4(][p][−][2]ϵ[ + 3][2] _T[p][−][1][ρ][)]_

_·_ _·_

Therefore, setting p = C · max 1/(ϵ · _√T_ ), ρ/(ϵ[2] _· T_ ) for some fixed constant C implies that,

with probability at least 2/3, ℓ is an (1 _ϵ)-multiplicative estimate ofo_ _T_ .
_±_

Furthermore, the expected space complexity is O(mp + H) = O(mp + T/ρ). Setting ρ =
max{ϵT/[√]m, 1} implies that the space complexity is O(ϵ[−][1]m/√T +ϵ[−][2]m/T +ϵ[−][1][√]m·T/T ) =

_O(ϵ[−][1]m/√T + ϵ[−][1][√]m · T/T_ ), since we are assuming that T ≥ _ϵ[−][2]. Hence, assuming that T is a_

constant factor approximation of T, the space complexity is O(ϵ[−][1]m/√T + ϵ[−][1][√]m).


C.2 PROOF OF THEOREM 1.2 FOR THE K-NOISY ORACLE

In this section, we prove Theorem 1.2 for the case that the given oracle is a K-noisy oracle, as
defined in Definition 1.1. That is, we prove the following:
**Theorem C.1. Suppose that the oracle in Algorithm 4 is a K-noisy oracle as defined in Defini-**
_tion 1.1 for a fixed constant K. Then with probability 2/3, Algorithm 4 returns ℓ_ _∈_ (1 ± ϵ)T _, and_

_uses space at most O_ _ϵ[−][1][ ]m/√T +_ _m_ _._

_[√]_

 

Recall that in Definition 1.1, we defined a K-noisy oracle if the following holds. For every edge e,

model, in Figure1 − _K ·_ _Nρe_ _[≤]_ [Pr[] 3[O] we have plotted[ρ][(][e][) =][ HEAVY][]] N[ ≤]e versus the range[K][ ·][ N]ρ[e] [. We first visualize this model. To visualize this error]1 − _K ·_ _Nρe_ _[, K][ ·][ N]ρ[e]_ for K = 2. We set

_Ne to vary on a logarithmic scale for clarity. For example, ifh_ _Ne exceeds the thresholdi_ _ρ by a factor_
of 2, there is no restriction on the oracle output, whereas if Ne exceeds ρ by a factor of 4, then the
oracle must classify the edge as heavy with probability at least 0.5. In contrast, the blue piece-wise
line shows the probability Pr[Oρ(e) = HEAVY] if the oracle Oρ is a perfect oracle.

_Proof of Theorem C.1. We follow the proof of Theorem 4.1 from the main paper. Let T1 be the set_
of triangles such that their first two edges in the stream are light according to the oracle. Let 2
_T_
be the set of triangles such that their first two edges in the stream are determined light and heavy
according to the oracle. Finally, let T3 be the set of triangles for which their first two edges are


-----

Figure 3: Plot of Ne, the number of triangles containing edge e, versus the allowed oracle probability
range Pr[Oρ(e) = HEAVY], shaded in orange. The blue piece-wise line shows the probability
Pr[Oρ(e) = HEAVY] if the oracle Oρ is a perfect oracle.

determined heavy by the oracle. Furthermore, we define χi for each triangle ti, t(e) for each edge
_e, and E(i), Ei for each i ≥_ 1 as done in Theorem 4.1. Finally, we define L(i) as the set of deemed
light edges that are part of exactly i triangles, and L(i) = |L(i)|.

First, note that the expectation (over the randomness of Oρ) of L(i) is at most E(i) · K · _[ρ]i_ [, since]

our assumption on Pr[Oρ(e) = heavy] tells us that Pr[Oρ(e) = light] ≤ _K ·_ _[ρ]i_ [if][ t][(][e][) =][ i][.]

Therefore, by the same computation as in Theorem 4.1, we have that, conditioning on the oracle,
**Ex[ℓ1|Oρ] = p[2]|T1] and**

**Var[ℓ1|Oρ] =** _p[2]_ + _p[3]_

_tXi∈T1_ _t[12]iti,tXj_ _j∈T1_

_[∩][t][12][̸][=][∅]_

_p[2]_ 1 + p[3][ X] _t(e)[2]_
_≤_ _|T_ _|_

_e light_

= p[2] 1 + p[3] _t[2]_ _L(t)_
_|T_ _|_ _·_ _·_
Xt≥1


But since ExOρ [L(t)] _t_
_≤_ _[K][·][ρ]_



_· E(t) for all t, we have that_


_p[2]_ 1 + p[3]

_· |T_ _|_ _·_


**ExOρ** [Var[ℓ1 _Oρ]]_ **ExOρ**
_|_ _≤_


_t[2]L(t)_

Xt≥1


_≥_

= p[2] 1 + Kρ _p[3][ X]_ _t_ _E(t)_
_|T_ _|_ _·_ _·_

_t≥1_

_≤_ (p[2] + 3Kp[3] _· ρ) · T._


A similar analysis to the above shows that Ex[ℓ2] = p · |T2| and that


**ExOρ** [Var[ℓ2 _Oρ]]_ **ExOρ**
_|_ _≤_


_t[2]L(t)_

Xt≥1


_p_ 2 + p
_· |T_ _|_ _·_


= p 2 + Kρ _p_ _t_ _E(t)_
_|T_ _|_ _·_ _·_
Xt≥1

_≤_ (p + 3Kp · ρ) · T.

Hence, as in Theorem 4.1, we have Ex[ℓ _Oρ] = T and ExOρ_ [Var[ℓ _Oρ]]_ 4T (p[−][2] + 3Kp[−][1]ρ).
_|_ _|_ _≤_ _·_
Thus, Ex[ℓ] = Ex[Ex[ℓ _Oρ]] = T and Var[ℓ] = ExOρ_ [Var[ℓ _Oρ]]+_ **VarOρ** [Ex[ℓ _Oρ]]_ 4T (p[−][2] +
_|_ _|_ _|_ _≤_ _·_
3Kp[−][1]ρ) by the laws of total expectation/variance. Therefore, since K is a constant, the variance is


-----

the same as in Theorem 4.1, up to a constant. Therefore, we still have that ℓ _∈_ (1 ± O(ϵ)) · T with
probability at least 2/3.

Finally, the expected space complexity is bounded by


_Pr[Oρ(e) = heavy]_ _mp +_ _E(t)_ _K_ _[t]_
_≤_ _·_ _·_ _ρ_
_eX∈m_ Xt≥1

= mp + _[K]_ _E(t)_ _t_

_ρ_ _·_

_[·]_ Xt≥1

= O(mp + T/ρ),


_mp +_


since _t_ 1 _[E][(][t][)][ ·][ t][ = 3][T][ and][ K][ =][ O][(1)][. Hence, setting][ ρ][ and][ p][ as in the proof of Theorem 4.1]_

_≥_
implies that the returned value is a (1 ± ϵ)-approximation of T, and the the space complexity is
_O(ϵ[−][1][P](m/√T +_ _m)) as before._

_[√]_

C.3 LOWER BOUND


In this section, we prove a lower bound for algorithms in the arbitrary order model that have access
to a perfect heavy edge oracle. Here heavy means Nuv _T/c for a pre-determined threshold c, and_
we assume c = o(m) and T/c > 1, as otherwise the threshold will be too small or too close to the ≥
average to give an accurate prediction. The following theorem shows an Ω(min(m/√T, m[3][/][2]/T ))

lower bound even with such an oracle.
**Theorem C.2. Suppose that the threshold of the oracle c = O(m[q]), where 0 ≤** _q < 1. Then for_
_any T and m, T ≤_ _m, there exists m[′]_ = Θ(m) and T _[′]_ = Θ(T ) such that any one-pass arbi_trary order streaming algorithm that distinguishes between m[′]_ _edge graphs with 0 and T_ _[′]_ _triangles_
_with probability at least 2/3 requires Ω(m/√T_ ) space. For any T and m, T = Ω(m[1+][δ]) where

max(0, q − 2[1] [)][ ≤] _[δ <][ 1]2_ _[, there exists][ m][′][ = Θ(][m][)][ and][ T][ ′][ = Θ(][T]_ [)][ such that any one-pass arbitrary]

_order streaming algorithm that distinguishes between m[′]_ _edge graphs with 0 and T_ _[′]_ _triangles with_
_probability at least 2/3 requires Ω(m[3][/][2]/T_ ) space.

When T ≤ _m, we consider the hubs graph mentioned in Kallaugher & Price (2017)._
**Definition C.3 (Hubs Graph, Kallaugher & Price (2017)). The hubs graph Hr,d consists of a single**
_vertex v with 2rd incident edges, and d edges connecting disjoint pairs of v’s neighbors to form_
_triangles._

It is easy to see that inedges of this kind in the graph. In Hr,d, each edge has at most one triangle. Hence, there will not be any heavy Kallaugher & Price (2017), the authors show an Ω(r√d) lower

bound for the hubs graph.
**Lemma C.4 (Kallaugher & Price (2017)). Given r and d, there exist two distributions G1, G2 on**
_subgraphs G1 and G2 of Hr,d, such that any algorithm which distinguishes them with probability at_
_least 2/3 requires Ω(r√d) space, where Gi has Θ(rd) edges and T_ (G1) = d, T (G2) = 0.

Now, given T and m, where T ≤ _m, we let r = Θ(m/T_ ) and d = T . We can see Hr,d has Θ(m)
edges and T triangles, and we need Ω(r√d) = Ω(m/√T ) space.

When T > m, we consider the following Ω(m[3][/][2]/T ) lower bound in Bera & Chakrabarti (2017).

Let H be a complete bipartite graph with b vertices on each side (we denote the two sides of vertices
by A and B). We add an additional N vertex blocks V1, V2, ...VN with each |Vi| = d. Alice has
an N -dimensional binary vector x. Bob has an N -dimensional binary vector y. Both x and y have
exactly N/3 coordinates that are equal to 1. Then, we define the edge sets


_EAlice =_


_u, v_ _, u_ _A, v_ _Vi_
_{{_ _}_ _∈_ _∈_ _}_
_i:xi=1_

[


and
_EBob =_


_u, v_ _, u_ _B, v_ _Vi_ _._
_{{_ _}_ _∈_ _∈_ _}_
_i:yi=1_

[


-----

Let the final resulting graph be denoted byE = EH _EAlice_ _EBob._ _G = (V, E) where V = VH ∪_ _V1 ∪_ _... ∪_ _VN and_
_∪_ _∪_

In Bera & Chakrabarti (2017), the authors show by a reduction to the DISJ[N/]N [3] problem in communication complexity, that we need Ω(N ) space to distinguish the case when G has 0 triangles from
the case when G has at least b[2]d triangles.

Given m and T, T = Θ(m[1+][δ]) where 1 ≤ _δ < 1/2, as shown in Bera & Chakrabarti (2017),_
we can set b = N _[s]_ and d = N _[s][−][1]_ where s = 1/(1 − 2δ). This will make m = Θ(N [2][s]) and
_T = Θ(N_ [3][s][−][1]), which will make the Ω(m[3][/][2]/T ) lower bound hold. Note that at this time each
edge will have an O( _m[1]_ [)][-fraction of triangles or an][ O][(] _m[1][−]1_ 2[1]s [)][-fraction of triangles. Assume the]

threshold of the oracle c = O(m[q]). When δ ≥ max(0, q − 2[1] [)][, there will be no heavy edges in the]

graph.

Theorem C.2 follows from the above discussion.

D FOUR-CYCLE COUNTING IN THE ARBITRARY ORDER MODEL

In the 4-cycle counting problem, for each xy ∈ _E(G), define Nxy = |z, w : {x, y, z, w} ∈_ □| as
the number of 4-cycles attached to edge xy.

In this section, we present the one-pass _O(T_ [1][/][3] +ϵ[−][2]m/T [1][/][3]) space algorithm behind Theorem 1.3

and the proof of the theorem.[6]

We begin with the following basic idea.[e]

-  Initialize: C ← 0, S ←∅. On the arrival of edge uv:

**– With probability p, S ←{uv} ∪** _S._
**– C ←** _C + |{{w, z} : uw, wz and zv ∈_ _S}|._

-  Return C/p[3].


Following the analysis in Vorotnikova (2020), we have

**Var[C] ≤** _O(Tp[3]_ + T ∆Ep[5] + T ∆W p[4]),

where ∆E and ∆W denote the maximum number of 4-cycles sharing a single edge or a single
wedge.

There are two important insights from the analysis: first, this simple sampling scheme can output
a good estimate to the number of 4-cycles on graphs that do not have too many heavy edges and
heavy wedges (the definitions of heavy will be shown later). Second, assuming that we can store all
the heavy edges, then at the end of the stream we can also estimate the number of 4-cycles that have
exactly one heavy edge but do not have heavy wedges.

For the 4-cycles that have heavy wedges, we use an idea proposed in McGregor & Vorotnikova
(2020); Vorotnikova (2020): for any node pair u and v, if we know their number of common neighbors (denoted by k), we can compute the exact number of 4-cycles with u, v as a diagonal pair (i.e.,
the four cycle contains two wedges with the endpoints u, v), and this counts to _k2_ . Furthermore,
if k is large (in this case we say all the wedges with endpoints u, v are heavy), we can detect it and
obtain a good estimation to k by a similar vertex sampling method mentioned in Section   3. We can
then estimate the total number of 4-cycles that have heavy wedges with our samples.

At this point, we have not yet estimated the number of 4-cycles having more than one heavy edge
but without heavy wedges. However, McGregor & Vorotnikova (2020) shows this class of 4-cycles
only takes up a small fraction of T, and hence we can get a (1 _±_ _ϵ)-approximation to T even without_
counting this class of 4-cycles.

The reader is referred to Algorithm 5 for a detailed version of our one-pass, 4-cycle counting algorithm. Before the stream, we randomly select a node set S, where each vertex is in S with probability


6We assume


1

_T_ [1][/][6][ ≤] _[O][(][ϵ][)][, which is the same assumption as in previous work]_


-----

**Algorithm 5 Counting 4-cycles in the Arbitrary Order Model**

1: Input:otherwise. Arbitrary Order Stream and an oracle that outputs TRUE if Nxy ≥ _T/ρ and FALSE_

2: Output: An estimate of the number T of 4-cycles.
3: Initialize Al 0, Ah 0, Aw 0 EL _EH_, ES, and W . Set ρ _T_ [1][/][3],
_p ←_ _αϵ[−][2]_ log ← n/ρ for a sufficiently large ← _←_ _←∅ α, and Let ←∅ S be a random subset of nodes such that ←∅_ _←∅_ _←_
each node is in S with probability p.

4: while seeing edge uv in the stream do
5: **if u ∈** _S or v ∈_ _S then_

6: _ES_ _uv_ _ES_ .

7: **if the oracle outputs FALSE ←{** _} ∪_ **then**

8: With probability p, EL _uv_ _EL_ .

9: Find pair (w, z) such that ←{ uw, wz,} ∪ and zv _EL, let D_ _D_ (u, w, z, v) .
_∈_ _←_ _∪{_ _}_

10: **else**

11: _EH_ _uv_ _EH_ .

12: for each node pair ←{ _} ∪ (u, v) do_
13: let q(u, v) be the number of wedges with center in S and endpoints u and v.

14:15: **if qA(u, vw** ) ≥AwpT +[1][/][3]q(thenu,v2 ) .

_←_

16: _W_ _W_ (u, v) .
_←_ _∪{_   _}_

17: for each 4-cycle d in D do
18: **if the end points of wedges in d are not in W then**

20:19: for each edgeAl ← _Al uv + 1 in EH do_
21: **for each 4-cycle d formed with uv and e ∈** _EL do_

22: **if the end points of wedges in d are not in W then**

23:24: return:A Ahl ←/p[3]A+h A + 1h/p[3] + Aw .


_p independently, and we later store all edges that are incident to S during the stream. In the mean-_
time, we define the edge uv to be heavy if Nuv _T_ [2][/][3] and train the oracle to predict whether uv
_≥_
is heavy or not when we see the edge uv during the stream. Let p = _O(ϵ[−][2]/T_ [1][/][3]). We store all
the heavy edges and sample each light edge with probability p during the stream. Upon seeing see
a light edge uv, we look for the 4-cycles that are formed by uv and the light edges that have been

[e]
sampled before, and then record them in set D. At the end of the stream, we first find all the node
pairs that share many common neighbors in S and identify them as heavy wedges. Then, for each
4-cycle d ∈ _D, we check if d has heavy wedges, and if so, remove it from D. Finally, for each_
heavy edge uv indicated by the oracle, we compute the number of 4-cycles that are formed by uv
and the sampled light edges, and without heavy wedges. This completes all parts of our estimation
procedure.

We now prove Theorem 1.3, restated here for the sake of convenience.

**Theorem 1.3. There exists a one-pass algorithm, Algorithm 5, with space complexity** _O(T_ [1][/][3] +
_ϵ[−][2]m/T_ [1][/][3]) in the arbitrary order model that, using a learning-based oracle, returns a (1 ± ϵ)_approximation to the number T of four cycles with probability at least 7/10._

[e]

_Proof. From the Lemma 3 in Vorotnikova (2020), we know that with probability at least 9/10, we_
can get an estimate Aw such that
_Aw = Tw ± ϵT._

Where Tw is the number of the 4-cycles that have at least one heavy wedge. We note that if a 4cycle has two heavy wedges, it will be counted twice. However, Vorotnikova (2020) shows that this
double counting is at most O(T [2][/][3]) = O(ϵ)T .

For the edge sampling algorithm mentioned in D, from the analysis in Vorotnikova (2020), we have

**Var[Al/p[3]] ≤** _O(T/p[3]_ + T ∆E/p + T ∆W /p[2]) .


-----

Notice that in our algorithm, the threshold of the heavy edges and heavy wedges are Nuv _T_ [2][/][3]

and Nw _T_ [1][/][3], respectively, which means Var[ _[A]p[3][l][ ] =][ O][(][ϵ][2][T][ 2][)][. Hence, we can get an estimate] ≥_ _[ A][l]_
_≥_

such that with probability at least 9/10,

_Al = Tl ± ϵT,_

where Tl is the number of 4-cycles that have no heavy edges. Similarly, we have with probability at
least 9/10,
_Ah = Th ± ϵT,_

where Th is the number of 4-cycles that have at most one heavy edge.

One issue is that the algorithm has not yet estimated the number of 4-cycles having more than one
heavy edge, but without heavy wedges. However, from Lemma 5.1 in McGregor & Vorotnikova
(2020) we get that the number of this kind of 4-cycles is at most O(T [5][/][6]) = O(ϵ)T . Hence putting
everything together, we have


_Al/p[3]_ + Ah/p[3] + Aw _T_ _O(ϵ)T,_
_|_ _−_ _| ≤_


which is what we need.


Now we analyze the space complexity. The expected number of light edges we sample is
_O(mp) =_ _O(ϵ[−][2]m/T_ [1][/][3]) and the expected number of nodes in S and edges in ES is O(2mp) =
_O(ϵ[−][2]m/T_ [1][/][3]). The expected number of 4-cycles we store in D is O(Tp[3]) = _O(ϵ[−][6]). We store_
all the heavy edges, and the number of heavy edges is at most[e] _O(T_ [1][/][3]). Therefore, the expected
total space ise _O(T_ [1][/][3] + ϵ[−][2]m/T [1][/][3]). [e]

E RUNTIME[e] ANALYSIS

In this section, we verify the runtimes of our Algorithms 1, 2, 3, 4, and 5.


**Proposition E.1. Algorithm 1 runs in expected time at most** _O_ min(ϵ[−][2]m[5][/][3]/T [1][/][3], ϵ[−][1]m[3][/][2]) _in_
_the setting of Theorem 1.1, or_ _O_ min(ϵ[−][2]m[5][/][3]/T [1][/][3], K _ϵ[−][1]m[3][/][2])_ _in the setting of Theorem 1.4._
_·_   

[e]
  

_Proof. For each edge ab_ _SL[e]_ _SM_, we check whether we see both va and vb (lines 9-10) when
looking at edges adjacent to ∈ _v. So, this takes time ∪_ _SL_ + _SM_ per edge in the stream. We similarly
_|_ _|_ _|_ _|_
check for each edge in Saux (lines 11-12), so this takes time |Saux|. Finally, for each edge vu (line
13), we note that lines 14-17 can trivially be implemented in O(1) time, along with 1 call to the
oracle. The remainder of the oracle simply involves looking through the set Saux and SM to search
or count for elements. Thus, the overall runtime is O( _SL_ + _SM_ + _Saux_ ) per stream element, so
_|_ _|_ _|_ _|_ _|_ _|_
the runtime is O (m ( _SL_ + _SM_ + _Saux_ )).
_·_ _|_ _|_ _|_ _|_ _|_ _|_

This is at most O(m · S), where S is the space used by the algorithm. So, the runtime is _O_ min(ϵ[−][2]m[5][/][3]/T [1][/][3], ϵ[−][1]m[3][/][2][] in the setting of Theorem 1.1, and is at most
_O_ min(ϵ[−][2]m[5][/][3]/T [1][/][3], K _ϵ[−][1]m[3][/][2])_ in the setting of Theorem 1.4.
  _·_
e

**Proposition E.2.e**   _Algorithm 2 runs in time_ _O(ϵ[−][2](α + mβ/T_ )m).

_Proof. For each edge ab_ _S[i]_ for each 0 [e] _i_ _cϵ[−][2], we check whether we see both ya and yb_
_∈_ _≤_ _≤_
in the stream when looking at edges adjacent to y. So, lines 7-8 take time O([P] _|S[i]|) for each edge_
we see in the stream. By storing each S[i] (and each Q[i]) in a balanced binary search tree or a similar
data structure, we can search for xy ∈ _S[i]_ in time O(log n) in line 11, and it is clear that lines
12-16 take time O(log n) (since insertion into the data structure for Q[i], S[i] can take time O(log n)).
Since we do this for each i from 0 to cϵ[−][2] and for each incident edge yx we see, in total we spend
_O(ϵ[−][2]_ log n + _|S[i]|) time up to line 16. The remainder of the lines take time O(ϵ[−][2]_ _· m log n),_
since the slowest operation is potentially deleting an edge from each S[i] and a value from each Q[i]
up to m times (for each edge).

[P]

Overall, the runtime is _O(ϵ[−][2]_ _· m + m ·_ _|S[i]|) =_ _O(m · S), where S is the total space used by the_
algorithm. Hence, the runtime is _O(ϵ[−][2](α + mβ/T_ )m).

[e] [P] [e]

[e]


-----

**Proposition E.3. Algorithm 3 runs in time** _O(Kϵ[−][2](α + mβ/T_ )m).

_Proof. For each edge ab ∈_ _Si[j]_ [for each][ 1][ ≤] _[i][e][ ≤]_ _[O][(log][ n][)][,][ 1][ ≤]_ _[j][ ≤]_ _[O][(log log][ n][)][, we check whether]_
we see both ya and yb in the stream when looking at edges adjacent to y. So, line 5 takes time
_Oor a similar data structure, we can search for([P]_ _|Si[j][|][)][ for each edge we see in the stream. By storing each] xy ∈_ _S[i]_ in time O[ S](logi[j] [in a balanced binary search tree] n) in line 7, and it is clear that
lines 8-12 take time O(log n) (since insertion into the data structure for Q[j]i [can take time][ O][(log][ n][)][).]
Since we do this for each i from 0 to cϵ[−][2] and for each incident edge yx we see, in total we spend
poly log n · O([P] _|S[i]|) time up to line 12. Finally, lines 13-15 take time poly log n._

Overall, the runtime is _O(m ·_ _|S[i]|) =_ _O(m · S), where S is the total space used by the algorithm._
Hence, the runtime is _O(Kϵ[−][2](α + mβ/T_ )m).

[e] [P] [e]

**Proposition E.4. Algorithm 4 runs in time** _O_ _ϵ[−][1]_ _m[2]/√T + ϵ[−][1]_ _m[3][/][2][]_ _._

[e]

_·_ _·_


_Proof. For each edge e in the stream, first we check the oracle, and we either add[e]_ _e to H or to SL with_
probability p (lines 6-9), which take O(1) time. The remaining lines (lines 10-18) involve operations
that take O(1) time for each w which represents a pair (e1, e2) of edges where e1, e2 _SL_ _H_
and (e, e1, e2) forms a triangle. So, the runtime is bounded by the amount of time it takes to find all ∈ _∪_
_e1, e2_ _SL_ _H such that (e, e1, e2) forms a triangle. If the edge e = (u, v), we just find all edges_
in SL ∈H adjacent to ∪ _u, and all edges in SL_ _H adjacent to v. Then, we sort these edges based on_
_∪_ _∪_
their other endpoint and match the edges if they form triangles. So, the runtime is _O(_ _SL_ + _H_ )
_|_ _|_ _|_ _|_
per edge e in the stream. Finally, line 19 takes O(1) time.

Overall, the runtime is _O(m_ ( _SL_ + _H_ )) = _O(m_ _S) where S is the total space of the algorithm.[e]_
_·_ _|_ _|_ _|_ _|_ _·_

So, the runtime is _O_ _ϵ[−][1]_ _· m[2]/√T + ϵ[−][1]_ _· m[3][/][2][]_ _._

[e] [e]



**Proposition E.5. Algorithm[e]** _5 runs in time_ _O_ _ϵ[−][2]/T_ [1][/][3] (n[3] + m[2]) _._

_·_
  

_Proof. We note that for each edge uv in the stream (line 4), the code in the loop (lines 5-11) can[e]_
be implemented using 1 oracle call and _O(_ _EL_ ) time. The only nontrivial step here is to find pairs
_|_ _|_
(w, z) such that uw, wz, zv are all in EL. However, by storing EL in a balanced binary search tree
or similar data structure, one can enumerate through each edge wz _EL and determine if uw and_

[e] _∈_
_zv are in EL in O(log |EL|) time. So, lines 4-11 take time_ _O(|EL|) per stream element._

Next, lines 12-16 can easily be implemented in time O(n[2][e]· |S|), lines 17-19 can be easily implemented in time _O(|D|) if the vertices in W are stored properly, and lines 20-23 can be done in_
_O(|EH_ _| · |EL|) time. The last part is true since we check each uv ∈_ _EH and e = (u[′], v[′]) ∈_ _EL,_
and then check if[e] u, u[′], v[′], v form a 4-cycle by determining if u, u[′] and v, v[′] are in EL _EH (which_
takes timee _O(log_ _EL_ + log _EH_ ) time assuming EL, EH are stored in search tree or similar data ∪
_|_ _|_ _|_ _|_
structure).

Overall, we can bound the runtime as _O(m_ _EL_ +n[2] _S_ + _D_ + _EH_ _EL_ ) = _O(m_ _EL_ +n[2] _S_ ).

_·|_ _|_ _·|_ _|_ _|_ _|_ _|_ _|·|_ _|_ _·|_ _|_ _·|_ _|_
As each edge is in EL with probability at most p and each edge is in S with probability at most p,
the total runtime is _O_ (m[2] + n[3]) _p_ [e]= _O_ _ϵ[−][2]/T_ [1][/][3] (n[3] + m[2]) . [e]
_·_ _·_
     

F ADDITIONAL[e] EXPERIMENTAL[e] RESULTS

All of our graph experiments were done on a CPU with i5 2.7 GHz dual core and 8 GB RAM or
a CPU with i7 1.9 GHz 8 core and 16GB RAM. The link prediction training was done on a single
GPU.

F.1 DATASET DESCRIPTIONS

-  Oregon: 9 graphs sampled over 3 months representing a communication network of internet routers Leskovec & Krevl (2014); Leskovec et al. (2005).


-----

-  CAIDA: 98 graphs sampled approximately weekly over 2 years, representing a communication network of internet routers Leskovec & Krevl (2014); Leskovec et al. (2005).

-  Reddit Hyperlinks: Network where nodes represent sub communities (subreddits) and
edges represent posts that link two different sub communities Leskovec & Krevl (2014);
Kumar et al. (2018).

-  WikiBooks: Network representing Wikipedia users and pages, with editing relationships
between them Rossi & Ahmed (2015).

-  Twitch: A user-user social network of gamers who stream in a certain language. Vertices
are the users themselves and the links are mutual friendships between them. Rozemberczki
et al. (2019)

-  Wikipedia: This is a co-occurrence network of words appearing in the first million bytes
of the Wikipedia dump. Mahoney (2011)

-  Synthetic Power law: Power law graph sampled from the Chung-Lu-Vu (CLV) model with
the expected degree of the ith vertex proportional to 1/i[2] Chung et al. (2003). To create this
graph, the vertices are ‘revealed’ in order. When the jth vertex arrives, the probability of
forming an edge between the jth vertex and ith vertex for i < j is proportional to 1/(ij)[2].

F.2 DETAILS ON LINK PREDICTION ORACLE

Our oracle for the Twitch and Wikipedia graphs is based on Link Prediction, where the task is to
estimate the likelihood of the existence of edges or to find missing links in the network. Here we
use the method and code proposed in Zhang & Chen (2018). For each target link, it will extract
a local enclosing subgraph around a node pair, and use a Graph Neural Network to learn general
graph structure features for link prediction. For the Twitch network, we use all the training links
as the training data for the graph neural network, while for the Wikipedia network, we use 30% of
the training links as the training data due to memory limitations. For the Twitch network, our set of
links that we will try to predict are between two nodes that have a common neighbor in the training
network, but do not form a link in the training network. This is about 3.8 million pairs, and we
call this the candidate set. We do this for memory considerations. For the remaining node pairs,
we set the probability they form a link to be 0. For the Wikipedia network, we randomly select a
link candidate set of size 20 times the number of edges (about 1 million pairs) from the entire set of
testing links. These link candidate sets will be used by the oracle to determine heaviness. Then, we
use this network to do our prediction for all links in our candidate sets for the two networks.

Now we are ready to build our heavy edge oracle. For the adjacency list model, when we see the
edge uv in the stream, we know all the neighbors of u, and hence, we only need to predict the
neighbors of v. Let deg(v) be the degree of v in the training graph. The training graph and testing
graph are randomly split. Hence, we can use the training set to provide a good estimation to the
degree of v. Next, we choose the largest deg(v) edges incident to v from the candidate set, in terms
of their predicted likelihood given by the the neural network, as our prediction of N (v), which leads
to an estimate of Ruv. For the arbitrary order model, we use the same technique as above to predict
_N_ (u) and N (v), and estimate Nuv.

F.3 PARAMETER SELECTION FOR ALGORITHM 2

We need to set two parameters for Algorithm 2: p, which is the edge sampling probability, and
_θ, which is the heaviness threshold. In our theoretical analysis, we assume knowledge of a lower_
bound on T in order to set p and θ, as is standard in the theoretical streaming literature. However, in
practice, such an estimate may not be available; in most cases, the only parameter we are given is a
space bound for the number of edges that can be stored. To remedy this discrepancy, we modify our
algorithm in experiments as follows.

First, we assume we only have access to the stream, a space parameter Z indicating the maximum
number of edges that we are allowed to store, and an estimate of m, the number of edges in the
stream. Given Z, we need to designate a portion of it for storing heavy edges, and the rest for
storing light edges. The trade-off is that the more heavy edges we store, the smaller our sampling
probability of light edges would be. We manage this trade off in our implementation by reserving
0.3 · Z of the edge ‘slots’ for heavy edges. The constant 0.3 is fixed throughout all our experiments.


-----

We then set p = 0.7 · Z/m. To improve the performance of our algorithm, we optimize for space
usage by always insuring that we are storing exactly Z space (after observing the first Z edges of the
stream). To do so and still maintain our theoretical guarantees, we perform the following procedure.
Call the first Z edges of the stream the early phase and the rest of the edges the late phase.

We always keep edges in the early phase to use our space allocation Z and also keep track of the
0.3-fraction of the heaviest edges. After the early phase is over, i.e., more than Z edges have passed
in the stream, if a new incoming edge is heavier than the lightest of the stored heavy edges, we
replace the least heavy stored edge with the new arriving edge and re-sample the replaced edge with
probability p. Otherwise, if the new edge is not heavier than the lightest stored edge, we sample the
new incoming edge with probability p. If we exceed Z, the space threshold, we replace one of the
light edges sampled in the early phase. Then similarly as before, we re-sample this replaced edge
with probability p and continue this procedure until one of the early light edges has been evicted. In
the case that there are no longer any of the light edges sampled in the early phase stored, we replace
a late light edge and then any arbitrary edge (again performing the re-sampling procedure for the
evicted edge).

Note that in our modification, we only require our predictor be able to compare the heaviness between two edges, i.e., we do not need the predictor to output an estimate of the number of triangles
on an edge. This potentially allows for more flexibility in the choice of predictors.

If the given space bound meets the space requirements of Theorem C.1, then the theoretical guarantees of this modification simply carry over from Theorem 1.2: we always keep the heaviest edges
and always sample light edges with probability at least p. In case the space requirements are not
met, the algorithm stores the most heavy edges as to reduce the overall variance.

F.4 ADDITIONAL FIGURES FROM ARBITRARY ORDER TRIANGLE COUNTING EXPERIMENTS

Additional figures for our arbitrary order triangle counting experiments are given in Figures 4 and 5.


|Col1|Caida-2007, Graph#25|Col3|
|---|---|---|
||Our Alg ThinkD MVV WRS|Our Alg ThinkD MVV WRS|
|4 4 4 4 4|||


Space


0.25

0.20

0.15

0.10

0.05

0.00


0.10

0.08

0.06

0.04

0.02

0.00


|Col1|Twitch Graph|Col3|
|---|---|---|
||Our Alg ThinkD MVV WRS|Our Alg ThinkD MVV WRS|
|4 4 4 4 4|||


(a) CAIDA 2007


_Z_

. Otherwise, if the new edge is not heavier than the lightest stored edge, we sample the
, the space threshold, we replace one of the

and continue this procedure until one of the early light edges has been evicted. In

C.1
1.2
_p. In case the space requirements are not_

RDER TRIANGLE COUNTING

Twitch Graph

Our Alg
ThinkD
MVV
WRS

1 10[4] 2 10[4] 3 10[4] 4 10[4] 5 10

Space

(b) Twitch


Figure 4: Error as a function of space for various graph datasets.

F.5 EXPERIMENTAL DESIGN FOR ADJACENCY LIST EXPERIMENTS


We now present our adjacencly list experiments. At a high level overview, similarly to the arbitrary
order experiments, for our learning-based algorithm, we reserve the top 10% of the total space for
storing the heavy edges. To do this in practice, we can maintain the heaviest edges currently seen so
far and evict the smallest edge in the set when a new heavy edge is predicted by the oracle and we no
longer have sufficient space. We also consider a multi-layer sub-sampling version of the algorithm
in Section B.2. Here we use more information from the oracle by adapting the sub-sampling rates
of edges based on their predicted value. For more details, see Section F.5. Our results are presented
in Figure 6 (with additional plots given in Figure 7). Our algorithms soundly outperform the MVV
baseline for most graph datasets. We only show the error bars for the multi-layer algorithm and
MVV for clarity. Additional details follow.

We use the same predictor for Nxy in Section 4 as a prediction for Rxy. The experiment is done
under a random vertex arrival order. For the learning-based algorithm, suppose Z is the maximum
number of edges that we are allowed to store. we set the k = Z/10 edges with the highest predicted
_Ruv values to be the heavy edges, and store them during the stream (i.e., we use 10% of the total_


-----

0.20

0.15

0.10

0.05

Caida-2006, Graph#20

Our Alg
ThinkD
MVV
WRS

0 1 10[4] 2 10[4] 3 10[4] 4 10[4]

Space


(a) CAIDA 2006, Graph #20


0.30

0.25

0.20

0.15

0.10

0.05

0.00


Caida-2006, Graph#40

Our Alg
ThinkD
MVV
WRS

0 1 10[4] 2 10[4] 3 10[4] 4 10[4]

Space

(b) CAIDA 2006, Graph #40


Oregon, Graph#3


0.25

0.20

0.15

0.10

0.05

0.00


0.25

0.20

0.15

0.10

0.05

0.00


Our Alg
ThinkD
MVV
WRS

0 5 10[3] 1 10[4] 1.5 10[4] 2 10[4]

Space

(c) Oregon, Graph #3


Oregon, Graph#7

Our Alg
ThinkD
MVV
WRS

0 5 10[3] 1 10[4] 1.5 10[4] 2 10[4]

Space

(d) Oregon, Graph #7


Figure 5: Error as a function of space for various graph datasets.

space for storing the heavy edges). For the remaining edges, we use a similar sampling-based
method. Note that it is impossible to know the k-heaviest edges before we see all the edges in the


are qualitatively similar to the results presented in Figure 6 as the multi-layer sampling algorithm is

stream. However, in the implementation we can maintain the k heaviest edges we currently have
seen so far, and evict the smallest edge in the set when a new heavy edge is predicted by the oracle.

We also consider the multi-layer sub-sampling algorithm mentioned in Section B.2, which uses more
information from the oracle. We notice that in many graph datasets, most of the edges having very
few number of triangles attached to. Taking an example of the Oregon and CAIDA graph, only
about 3%-5% of edges will satisfy Re 5 under a random vertex arrival order. Hence, for this
edges, intuitively we can estimate them using a slightly smaller space. ≥

For the implementation of this algorithm(we call it multi-layer version), we use 10%
space for storing the top k = Z/10 edges, and 70% of the space for sub-sampling the edges that the
oracle predict value is very tiny(the threshold may be slightly different for different datasets, like
for the Oregon and CAIDA graph, we set the threshold to be 5). Then, we use 20% of the space for
sub-sampling the remaining edges, for which we call them the medium edges.

FIGURES FROM ADJACENCY LIST TRIANGLE COUNTING EXPERIMENTS

Caida-2006 #30 0.10 Wikipedia 0.30 Powerlaw

0.14 Our Alg Our Alg Our Alg

0.12 Our Alg(multi-layer)MVV 0.08 Our Alg(multi-layer)MVV 0.25 Our Alg(multi-layer)MVV

0.10 0.20

0.06

0.08 0.15

Relative Error0.06 Relative Error0.04 Relative Error0.10

0.04

0.02 0.02 0.05

0.00 1 10[4] 2 10[4] 3 10[4] 4 10[4] 0.00 0.5 10[4] 1 10[4] 1.5 10[4] 2 10[4] 2.5 10[4] 3 10[4] 0.00 1 10[4] 2 10[4] 3 10[4] 4 10[4] 5 10[4] 6 10[4]

Space Space Space

(a) CAIDA 2006 (b) Wikipedia (c) Powerlaw

Figure 6: Error as a function of space in the adjacency list model.

Additional figures from the adjacency list triangle counting experiments are shown in Figure 7

superior over the MVV baseline for all of our datasets.


-----

0.6

0.5

0.4

0.3

0.2

0.1

0.0


0.14

0.12

0.10

0.08

0.06

0.04

0.02

0.00

0.10

0.08

0.06

0.04

0.02

0.00



Oregon #4

0.14 Our Alg

Our Alg(multi-layer)

0.12 MVV

0.10

0.08

Relative Error0.06

0.04

0.02

0.00 4 10[3] 8 10[3] 1.2 10[4] 1.6 10[4] 2 10[4]

Space

(a) Oregon

|Col1|Wikibooks|Col3|
|---|---|---|
||Our Alg Our Alg(multi-layer MVV|)|
||||
|4|4 4 4 4||
||2 10 3 10 4 10 5 10 6 Space Wikibooks||


|Col1|Caida-2007 #25|Col3|
|---|---|---|
||Our Alg Our Alg(multi-layer) MVV|Our Alg Our Alg(multi-layer) MVV|
|1 (|104 2 104 3 104 4 104 5 1 Space b) CAIDA 2007 Twitch||
||Our Alg Our Alg(multi-layer) MVV|Our Alg Our Alg(multi-layer) MVV|
|4 4 4 4 4|||


Space

(d) Twitch


Figure 7: Error as a function of space for various graph datasets.

F.7 ACCURACY OF THE ORACLE


In this section, we evaluate the accuracy of the predictions the oracle gives in our experiments.

**Value Prediction Oracle** : We use the prediction of Nxy in Section 4 as a value prediction for
_Rxy(x ≤s y), under a fixed random vertex arrival order. The results are shown in Figure 8. For a_
fixed approximation factor k, we compute the failure probability δ of the value prediction oracle as
follows: δ = #/m, where m is the number of total edges and # equals to the number of edges e
that p(e) ≥ _kαRe + β or p(e) ≤_ _k[1]_ _[R][e][ −]_ _[β][, respectively. Here we set][ α][ = 1][, β][ = 10][ for all graph]_

datasets.

We can see for the smaller side, there are very few edges e such that p(e) ≤ _k[1]_ _[R][e][ −]_ _[β][. This meets]_

the assumption of the exponential decay tail bound of the error. For the larger side, it also meets the
assumption of the linear decay tail bound on most of the graph datasets.


F.8 DETAILS ON ORACLE TRAINING OVERHEAD

The overhead of the oracles used in our experiments vary from task to task. For the important
use case illustrated by the Oregon and CAIDA datasets in which we are interested in repeatedly
counting triangles over many related streams, we can pay a small upfront cost to create the oracle
which can be reused over and over again. Thus, the time complexity of building the oracle can be
amortized over many problem instances, and the space complexity of the oracle is relatively small
as we only need to store the top 10% of heavy edges from the first graph (a similar strategy is used
in prior work on learning-augmented algorithms in Hsu et al. (2019b)). To give more details, for
this snapshot oracle, we simply calculate the Ne values for all edges only in the first graph. We then
keep the heaviest edges to form our oracle. Note that this takes polynomial time to train. The time
complexity of using this oracle in the stream is as follows: when an edge in the stream comes, we
simply check if it’s among the predicted heavy edges in our oracle. This is a simple lookup which
can even be done in constant time using hashing.

For learning-based oracles like the linear regression predictor, we similarly need to pay an upfront
cost to train the model, but the space required to store the trained model depends on the dimension
of the edge features. For the Reddit dataset with ∼ 300 features, this means that the storage cost
for the oracle is a small fraction of the space of the streaming algorithm. Training of this oracle
can be done in polynomial time and can even be computed in a stream via sketching and sampling


-----

0.030 Oregon 0.040 CAIDA-2006 0.040 CAIDA-2007

larger side larger side larger side

0.025 smaller side 0.035 smaller side 0.035 smaller side

0.030 0.030

0.020

0.025 0.025

0.015 0.020 0.020

0.015 0.015

Failure Probability0.010 Failure Probability Failure Probability

0.010 0.010

0.005

0.005 0.005

0.000 2 4 6 8 10 0.000 2 4 6 8 10 0.000 2 4 6 8 10

Approximation Factor k Approximation Factor k Approximation Factor k


(a) Oregon


(b) CAIDA 2006


(c) CAIDA 2007


0.010 Wikibooks 0.200 Twitch 0.05 Wikipedia

larger side larger side larger side
smaller side 0.175 smaller side smaller side

0.008 0.04

0.150

0.006 0.125 0.03

0.100

0.004 0.075 0.02

Failure Probability Failure Probability Failure Probability

0.050

0.002 0.01

0.025

0.000 2 4 6 8 10 0.000 2 4 6 8 10 0.00 2 4 6 8 10

Approximation Factor k Approximation Factor k Approximation Factor k

(d) Wikibooks


Approximation Factor k


Approximation Factor k

(f) Wikipedia


(e) Twitch




0.35 Random

0.30

0.25

0.20

0.15

Failure Probability0.10

0.05 larger side

smaller side

0.00 2 4 6 8 10

Approximation Factor k


(g) Powerlaw

Figure 8: Failure probability as a function of approximation factor k for various graph datasets.


techniques which reduce the number of constraints from m (number of edges) to roughly linear in
the number of features.

Our expected value oracle exploits the fact that our input graph is sampled from the CLV random
graph model. Given this, we can explicitly calculate the expected value of Ne for every edge which
requires no training time and nearly constant space. For training details for our link prediction
model, see Section F.2. Note that in general, there is a wide and rich family of predictors which can
be trained using sublinear space or in a stream such as regression Woodruff (2014), classification for
example using SVMs Andoni et al. (2020); Rai et al. (2009) and even deep learning modelsGomes
et al. (2019).

G IMPLICIT PREDICTOR IN PRIOR WORKS


We prove that the first pass of the two pass triangle counting Algorithm given in Section 3.1 of
McGregor et al. (2016), satisfies the conditions of the K-noisy oracle in Definition 1.1. Therefore,
our work can be seen as a generalization of their approach when handling multiple related data sets,
where instead of performing two passes on each data set and using the first of which to train the
heavy edge oracle, we perform the training once according to the first related dataset, and we get a
one pass algorithm for all remaining sets.

We first recall the first pass of (McGregor et al., 2016, Section 3.1):


1. Sample each node z of the graph with probability p = Cϵ[−][2] log m/ρ for some large constant C > 0. Let Z be the set of sampled nodes.


-----

2. Collect all edges incident on the set Z.

3. For any edge e = {u, v}, let _t(e) = |{z ∈_ _Z : u, v ∈_ _N_ (z)}| and define the oracle as

LIGHT if _t(e) < p_ _ρ_
oracle[e] (e) = _·_

HEAVY if _t(e)_ _p_ _ρ._

 _≥_ _·_

**Lemma G.1.HEAVY implies For any edge Ne > ρ/√2 with failure probability at most e = (u, v), oracle(e) = LIGHT 1[e][e]/n implies[10].** _Ne ≤_ 2ρ and oracle(e) =


_Proof. For any edge e, it follows that_ _t(e) ∼_ Bin(Ne, p). Therefore if Ne > 2ρ,

Pr[[e]t(e) < p _ρ]_ exp( Ω(p _ρ))_ 1/n[10]

[e] · _≤_ _−_ _·_ _≤_

by picking C large enough in the definition of p. The other case follows similarly.

**Lemma G.2. The expected space used by the above oracle is O(pm).**

_Proof. Each vertex is sampled in Z with probability p and we keep all of its incident edges. There-_
fore, the expected number of edges saved is O(p _v_ _[d][v][) =][ O][(][pm][)][.]_

Note that the oracle satisfies the conditions of the[P] K-noisy oracle in Definition 1.1. For example, if
_Ne_ _C_ _[′]ρ for C_ _[′]_ 1, Definition 1.1 only assumes that we incorrectly classify e with probability
1/C ≥[′], whereas the oracle presented above incorrectly classifies≫ _e with probability exp(−C_ _[′])/n[10]_
which is much smaller than the required 1/C _[′]._


H LEARNABILITY RESULTS

In this section, we give formal learning bounds for efficient predictor learning for edge heaviness.
In particular, we wish to say that a good oracle or predictor for edge heaviness and related graph
parameters can be learned efficiently using few samples if we observe graph instances drawn from
a distribution. We can view the result of this section, which will be derived via the PAC learning
framework, as one formalization of data driven algorithm design. Our results are quite general but
we state simple examples throughout the exposition for clarity. Our setting is formally the following.

Suppose there is an underlying distribution which generates graph instances H1, H2, all on
_D_ _· · ·_
_n vertices. Note that this mirrors some of our experimental setting, in particular our graph datasets_
which are similar snapshots of a dynamic graph across time.

Our goal is to efficiently learn a good predictor f among some family of functions F. The input
of each f is a graph instance H and the output is a feature vector in k dimensions. The feature
vector represents the prediction of the oracle and can encapsulate a variety of different meanings.
One example is when k = |E| and f outputs an estimate of edge heaviness for all edges. Another
is when k << _E_ and f outputs the id’s of the k heaviest edges. We also think of each input
_|_ _|_ _n_
instance H as encoded in a vector in R[p] for p 2 (for example, each instance is represented as an
adjacency matrix). Note that we allow for p > ≥ _n2_ if for example, each edge or vertex for H
   _∼D_
also has an endowed feature vector.
  

To select the ‘best’ f, we need to precisely define the meaning of best. Note that in many settings,
this involves a loss function which captures the quality of a solution. Indeed, suppose we have a loss
function L : f × H → R which represents how well a predictor f performs on some input H. An
example of L could be squared error from the output of f to the true edge heaviness values of edges
in H. Note that such a loss function clearly optimizes for predicting the heavy edges well.

Our goal is to learn the best function f ∈F which minimizes the following objective:

EH _D[L(f, H)]._ (1)
_∼_

Let f _[∗]_ be such the optimal f ∈F, and assume that for each instance H and each f ∈ _F_, f (H) can
be computed in time T (p, k). For example, suppose graphs drawn from D possess edge features in
R[d], and that our family F is parameterized by a single vector θ ∈ R[d] and represents linear functions


-----

which outputs the dot product of each edge feature with θ. Then it is clear that T (p, k) is a (small)
polynomial in the relevant parameters.

Our main result is the following.

**Theorem H.1. There is an algorithm which after poly(T** (p, k), 1/ϵ) samples, returns a function _f[ˆ]_
_that satisfies_
EH∼D[L( f, H[ˆ] )] ≤ EH∼D[L(f _[∗], H)] + ϵ_
_with probability at least 9/10._

We remark that one can boost the probability of success to 1 − _δ by taking additional log(1/δ)_
multiplicative samples.

The above theorem is a PAC-style bound which shows that only a small number of samples are
needed in order to ensure a good probability of learning an approximately-optimal function _f[ˆ]. The_
algorithm to compute _f[ˆ] is the following: we simply minimize the empirical loss after an appropriate_
number of samples are drawn, i.e., we perform empirical risk minimization. This result is proven
by Theorem H.3. Before introducing it, we need to define the concept of pseudo-dimension for a
function class which is the more familiar VC dimension, generalized to real functions.
**Definition H.2 (Pseudo-Dimension, Definition 9 Lucic et al. (2018)). Let X be a ground set and F**
_be a set of functions from_ _to the interval [0, 1]. Fix a set S =_ _x1,_ _, xn_ _, a set of real_
_X_ _{_ _· · ·_ _} ⊂X_
_numbersri_ _is called the induced subset of R = {r1, · · ·, rn} with ri ∈ S formed by[0, 1] and a function f and R f. The set ∈F. The set S with associated values Sf = {xi ∈_ _S | f_ (x Ri) ≥ is
_}_
_shattered by_ _if_ _Sf_ _f_ = 2[n]. The pseudo-dimension of _is the cardinality of the largest_
_shattered subset of F_ _|{ X (or |_ _∞ ∈F}|)._ _F_

The following theorem relates the performance of empirical risk minimization and the number of
samples needed, to the notion of pseudo-dimension. We specialize the theorem statement to our
situation at hand. For notational simplicity, we define A be the class of functions in f composed
with L:
_A := {L ◦_ _f : f ∈F}._
Furthermore, by normalizing, we can assume that the range of L is equal to [0, 1].
**Theorem H.3 (Anthony & Bartlett (1999)). Let D be a distribution over graph instances and A**
_be a class of functions a : H_ [0, 1] with pseudo-dimension d _. Consider t i.i.d. samples_
_→_ _A_
_H1, H2, . . ., Ht from_ _. There is a universal constant c0, such that for any ϵ > 0, if t_ _c0_ _d_ _/ϵ[2],_
_then we have_ _D_ _≥_ _·_ _A_

_t_

1

_a (Hi)_ EH _a(H)_

_t_ _−_ _∼D_

_i=1_ _[≤]_ _[ϵ]_

X

_for all a ∈A with probability at least 9/10._

The following corollary follows from the triangle inequality.
**Corollary H.4. Consider a set of t independent samples H1, . . ., Ht from** _and let ˆa be a function_

_t_ _D_

_in A which minimizes_ [1]t _i=1_ _[a][(][H][i][)][. If the number of samples][ t][ is chosen as in Theorem][ H.3][, then]_

P EH∼D[ˆa(H)] ≤ EH∼D[a[∗](H)] + 2ϵ

_holds with probability at least 9/10._

The main challenge is to bound the pseudo-dimension of our given function class A. To do so, we
first relate the pseudo-dimension to the VC dimension of a related class of threshold functions. This
relationship has been fruitful in obtaining learning bounds in a variety of works such as Lucic et al.
(2018); Izzo et al. (2021).
**Lemma H.5 (Pseudo-dimension to VC dimension, Lemma 10 in Lucic et al. (2018)). For any**
_a ∈A, let Ba be the indicator function of the region on or below the graph of a, i.e., Ba(x, y) =_
_sgn(a(x) −_ _y). The pseudo-dimension of A is equivalent to the VC-dimension of the subgraph class_
_B_ = _Ba_ _a_ _._
_A_ _{_ _|_ _∈A}_

Finally, the following theorem relates the VC dimension of a given function class to its computational complexity, i.e., the complexity of computing a function in the class in terms of the number
of operations needed.


-----

**Lemma H.6 (Theorem 8.14 in Anthony & Bartlett (1999)). Let w : R[α]** _×R[β]_ _→{0, 1}, determining_
_the class_
_W = {x →_ _w(θ, x) : θ ∈_ R[α]}.

_Suppose that any w can be computed by an algorithm that takes as input the pair (θ, x) ∈_ R[α] _× R[β]_
_and returns w(θ, x) after no more than r of the following operations:_

-  arithmetic operations +, −, ×, and / on real numbers,

-  jumps conditioned on >, ≥, <, ≤, =, and = comparisons of real numbers, and

-  output 0, 1,

_then the VC dimension of W is O(α[2]r[2]_ + r[2]α log α).

Combining the previous results allows us prove Theorem H.1. At a high level, we are instantiating
Lemma H.6 with the complexity of computing any function in the function class A.

_Proof of Theorem H.1. First by Theorem H.3 and Corollary H.4, it suffices to bound the pseudo-_
dimension of the class A = L ◦F. Then from Lemmas H.5, the pseudo-dimension of A is the VC
dimension of threshold functions defined by A. Finally from Lemma H.6, the VC dimension of the
appropriate class of threshold functions is polynomial in the complexity of computing a member of
the function class. In other words, Lemma H.6 tells us that the VC dimension of B as defined in
_A_
Lemma H.5 is polynomial in the number of arithmetic operations needed to compute the threshold
function associated to some a ∈A. By our definition, this quantity is polynomial in T (p, k). Hence,
the pseudo-dimension of G is also polynomial in T (p, k) and the result follows.

Note that we can consider initializing Theorem H.1 with specific predictions. If the family of oracles
we are interested in is efficient to compute (which is the case of the predictors we employ in our
experiments), then Theorem H.1 assures us that only polynomially many samples are required (in
terms of the computational complexity of our function class), to be able to learn a nearly optimal
oracle. Furthermore, computing the empirical risk minimizer needed in Theorem H.1 is also efficient
for a wide verity of function classes. For example in practice, we can simply use gradient descent
or stochastic gradient descent for a range of predictor models, such as regression or even general
neural networks.

We remark that our learnability result is in similar in spirit to one given in the recent learningaugmented paper Dinitz et al. (2021). There, they derive sample complexity learning bounds for
the different algorithmic problem of computing matchings in a graph (not in a stream). Since they
specialize their analysis to a specific function class and loss function, their bounds are tighter compared to the possibly loose polynomial bounds we have stated. However, our analysis above is
more general as it allows for a variety of predictors and loss functions to measure the quality of the
predictions.


-----

