# FAIR NODE REPRESENTATION LEARNING
## VIA ADAPTIVE DATA AUGMENTATION

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Node representation learning has demonstrated its efficacy for various applications on graphs, which leads to increasing attention towards the area. However,
fairness is a largely under-explored territory within the field, which may lead to
biased results towards underrepresented groups in ensuing tasks. To this end, this
work theoretically explains the sources of bias in node representations obtained
via Graph Neural Networks (GNNs). Our analysis reveals that both nodal features and graph structure lead to bias in the obtained representations. Building
upon the analysis, fairness-aware data augmentation frameworks on nodal features and graph structure are developed to reduce the intrinsic bias. Our analysis
and proposed schemes can be readily employed to enhance the fairness of various
GNN-based learning mechanisms. Extensive experiments on node classification
and link prediction are carried out over real networks in the context of graph contrastive learning. Comparison with multiple benchmarks demonstrates that the
proposed augmentation strategies can improve fairness in terms of statistical parity and equal opportunity, while providing comparable utility to state-of-the-art
contrastive methods.

1 INTRODUCTION

Graphs are widely used in modeling and analyzing complex systems such as biological networks or
financial markets, which leads to a rise in attention towards various machine learning (ML) tasks
over graphs. Specifically, node representation learning is a field with growing popularity. Node
representations are mappings from nodes to vector embeddings containing both structural and attributive information. Their applicability on ensuing tasks has enabled various applications such as
traffic forecasting (Opolka et al., 2019), and crime forecasting (Jin et al., 2020). Graph neural networks (GNNs) have been prevalently used for representation learning, where node embeddings are
created by repeatedly aggregating information from neighbors for both supervised and unsupervised
learning tasks (Kipf & Welling, 2017; Veliˇckovi´c et al., 2018; Garc´ıa-Dur´an & Niepert, 2017).

It has been shown that ML models propagate pre-existing bias in training data, which may lead to
discriminative results for ensuing applications (Dwork et al., 2012; Beutel et al., 2017). Particular
to ML over graphs, while GNN-based methods achieve state-of-the-art results for graph representation learning, they also amplify already existing biases in training data (Dai & Wang, 2020). For
example, nodes in social networks tend to connect to other nodes with similar attributes, leading
to denser connectivity between nodes with same sensitive attributes (e.g., gender) (Hofstra et al.,
2017). Thus, by aggregating information from the neighbors, the representations obtained by GNNs
may be highly correlated with the sensitive attributes. This causes discrimination in ensuing tasks
even when the sensitive attributes are not directly used in training (Hajian & Domingo-Ferrer, 2013).

Data augmentation has been widely utilized to improve generalizability in trained models, as well
as enable learning in unsupervised methods such as contrastive or self-supervised learning. Augmentation schemes have been extensively studied in vision (Shorten & Khoshgoftaar, 2019; Hjelm
et al., 2018) and natural language processing (Zhang et al., 2015; Kafle et al., 2017). However,
there are comparatively limited work in the graph domain due to the complex, non-Euclidean structure of graphs. To the best of our knowledge, (Agarwal et al., 2021) is the only study that designs
fairness-aware graph data augmentation in the contrastive learning framework to reduce bias.


-----

This study theoretically investigates the sources of bias in GNN-based learning and in turn improves
fairness in node representations by employing fairness-aware graph data augmentation schemes.
Proposed schemes corrupt both input graph topology and nodal features adaptively, in order to reduce the corresponding terms in the analysis that lead to bias. Although the proposed schemes are
presented over their applications using contrastive learning, the introduced augmentation strategies
can be flexibly utilized in several GNN-based learning approaches together with other fairnessenhancement methods. Our contributions in this paper can be summarized as follows:
**c1) We theoretically analyze the sources of bias that is propagated towards node representations in**
a GNN-based learning framework.
**c2) Based on the analysis, we develop novel fairness-aware graph data augmentations that can re-**
duce potential bias in learning node representations. Our approach is adaptive to both input graph
and sensitive attributes, and to the best of our knowledge, is the first study that tackles fairness enhancement through adaptive graph augmentation design.
**c3) The proposed strategies incur low additional computation complexity compared to non-adaptive**
counterparts, and are compatible to operate in conjunction with various GNN-based learning frameworks, including other fairness enhancement methods.
**c4) Theoretical analysis is provided to corroborate the effectiveness of the proposed feature masking**
and node sampling augmentation schemes.
**c5) Performance of the proposed graph data augmentation schemes is evaluated on real networks**
for both node classification and link prediction tasks. It is shown that compared to state-of-the-art
graph contrastive learning methods, the novel augmentation schemes improve fairness metrics while
providing comparable utility measures.

2 RELATED WORK

**Representation learning on graphs. Conventional graph representation learning approaches can**
be summarized under two categories: factorization-based and random walk-based approaches.
Factorization-based methods aim to minimize the difference between the inner product of node
representations and a deterministic similarity metric between them (Ahmed et al., 2013; Cao et al.,
2015; Ou et al., 2016). Random walk-based approaches, on the other hand, employ stochastic measures of similarity between nodes (Perozzi et al., 2014; Grover & Leskovec, 2016; Tang et al., 2015;
Chen et al., 2018). GNNs have gained popularity in representation learning, for both supervised
(Kipf & Welling, 2017; Veliˇckovi´c et al., 2018; Hu et al., 2019; Wu et al., 2019), and unsupervised
tasks, e.g., (Garc´ıa-Dur´an & Niepert, 2017; Hamilton et al., 2017). Specifically, recent success of
contrastive learning on visual representation learning (Wu et al., 2018; Ye et al., 2019; Ji et al., 2019)
has paved the way for contrastive learning for unsupervised graph representation learning.

**Graph data augmentation. Augmentation strategies have been extensively investigated in vision**
(Shorten & Khoshgoftaar, 2019; Hjelm et al., 2018) and natural language processing (Zhang et al.,
2015; Kafle et al., 2017) domains. However, the area is comparatively under-explored in the graph
domain due to the complex, non-Euclidean topology of graphs. Graph augmentation based on graph
structure modification has been developed to improve the utility of ensuing tasks (Rong et al., 2019;
Zhao et al., 2020; Chen et al., 2020a). Meanwhile, graph data augmentation has been used to generate graph views for unsupervised graph contrastive learning, see, e.g., (Veliˇckovi´c et al., 2019;
Opolka et al., 2019; Zhu et al., 2020; 2021), which achieves state-of-the-art results in various learning tasks over graphs such as node classification, regression, and link prediction (Opolka et al., 2019;
Veliˇckovi´c et al., 2019; You et al., 2020; Zhu et al., 2020; Peng et al., 2020; Hassani & Khasahmadi,
2020). Among which, (Zhu et al., 2020) is the first study that aims to maximize the agreement of
node-level embeddings across two corrupted graph views. Building upon (Zhu et al., 2020), (Zhu
et al., 2021) develops adaptive augmentation schemes with respect to various node centrality measures and achieves better results. However, none of these studies are fairness-aware.

**Fairness-aware learning on graphs. A pioneering study tackling the fairness problem in graph**
representation learning based on random walks is developed in (Rahman et al., 2019). In addition, adversarial regularization is employed to account for fairness of node representations (Dai &
Wang, 2020; Bose & Hamilton, 2019; Fisher et al., 2020) where (Dai & Wang, 2020) is presented
specifically for node classification, and (Fisher et al., 2020) works on knowledge graphs. (Buyl &
De Bie, 2020) also aims to create fair node representations by utilizing a Bayesian approach where
sensitive information is modeled in the prior distribution. Contrary to these aforementioned works,


-----

our framework is built on a theoretical analysis (developed within this paper). Similar to the works
mentioned above, the proposed methods can be utilized within the learning process to mitigate bias
by modifying the learned model (i.e., in-processing fairness strategy). In addition, the proposed
schemes can also be regarded as “pre-processing” tools, implying their compatibility to a wide array
of GNN-based learning schemes in a versatile manner. Furthermore, (Ma et al., 2021) carries out a
PAC-Bayesian analysis and connects the concept of subgroup generalization to accuracy disparity,
and (Zeng et al., 2021) introduces several methods including GNN-based ones to decrease the bias
for the representations of heterogeneous information networks. While (Li et al., 2021; Laclau et al.,
2021) modify adjacency to improve different fairness measures specifically for link prediction, (Buyl
& De Bie, 2021) designs a regularizer for the same purpose. With a specific interest on individual
fairness over graphs, (Dong et al., 2021) employs a ranking-based strategy. A biased edge dropout
scheme is proposed in (Spinelli et al., 2021) to improve fairness. However, the scheme therein is not
adaptive to the graph structure (the parameters of the framework are independent of the input graph
topology). Fairness-aware graph contrastive learning is first studied in (Agarwal et al., 2021), where
a layer-wise weight normalization scheme along with graph augmentations is introduced. However,
the fairness-aware augmentation utilized therein is designed primarily for counterfactual fairness.

3 FAIRNESS IN GNN-BASED REPRESENTATION LEARNING

GNN-based approaches are the state-of-the-art for node representation learning. However, it has
been demonstrated that the utilization of graph structure in the learning process not just propagates
but also amplifies a possible bias towards certain sensitive groups (Dai & Wang, 2020). To this
end, this section investigates the sources of bias in the generated representations via GNN-based
learning. It carries out an analysis revealing that both nodal features and graph structure lead to bias,
for which several graph data augmentation frameworks are introduced.

3.1 PRELIMINARIES

This study aims to learn fairness-aware nodal representations for a given graph G := (V, E) where
_V := {v1, v1, · · ·, vN_ _} denotes the node set, and E ⊆V × V represents the edge set. X ∈_ R[N] _[×][F]_
and A ∈{0, 1}[N] _[×][N]_ are used to denote the feature and adjacency matrices, respectively, with
the (i, j)-th entry aij = 1 if and only if eij := (vi, vj) ∈E. Degree matrix D ∈ R[N] _[×][N]_ is
defined to be a diagonal matrix with the nth diagonal entry dn denoting the degree of vn. For the
fairness examination, sensitive attributes of the nodes are represented with s ∈{0, 1}[N] _[×][1], where_
the existence of a single, binary sensitive attribute is considered. In this work, unsupervised methods
are chosen as enabling schemes for the representation generation where given the inputs A, X, and
**s, the main purpose is to learn a mapping f : R[N]** _[×][N]_ _× R[N]_ _[×][F]_ _× R[N]_ _[×][1]_ _→_ R[N] _[×][F][ L]_ that generates
_F_ _[L]_ dimensional (generally F _[L]_ _≪_ _F_ ) unbiased nodal representations H[L] = f (A, X, s) ∈ R[N] _[×][F][ L]_

through an L-layer GNN, which can be used in an ensuing task such as node classification. xi ∈ R[F],
**h[l]i**
attribute of node[∈] [R][F][ l] [, and][ s] v[i]i[ ∈{]. Furthermore,[0][,][ 1][}][ denote the feature vector, representation at layer] S0 and S1 denote the set of nodes whose sensitive attributes are[ l][ and the sensitive]
0 and 1, respectively. Define inter-edge set := _eij_ _vi_ _a, vj_ _b, a_ = b, while intra-edge
set is defined as := _eij_ _vi_ _a, vj_ _E_ _[χ]b, a = { b_ . Similarly, the set of nodes having at least| _∈S_ _∈S_ _̸_ _}_
one inter edge is denoted by E _[ω]_ _{_ _| S ∈S[χ], while S ∈S[ω]_ defines the set of nodes that have no inter-edges. The}
intersection of the sets S0, S _[χ]_ is denoted as S0[χ][. Additionally,][ d]i[χ] [and][ d]i[ω] [denote the numbers of]
inter-edges and intra-edges adjacent to vi, respectively. Finally, denotes the entry-wise absolute
_|[˙]|_
value for scalar or vector inputs, while it is used for the cardinality when the input is a set.

3.2 ANALYSIS FOR BIAS IN GNN REPRESENTATIONS

This subsection presents an analysis to find out the sources of bias in node representations generated
by GNNs. Analysis is developed for the mean aggregation scheme in which aggregated representations at layer l, Z[l] R[N] _[×][F][ l]_, are generated such that z[l]i [=] _d1i_ _vj_ (i) **[h]j[l][−][1]** for i = 1, _, N_,
_∈_ _∈N_ _{_ _· · ·_ _}_

where z[l]i [is the][ i][th row of][ Z][l][ corresponding to node][ v][i][,][ d][i][ denotes the degree of node]P _[ v][i][,][ N]_ [(][i][)][ refers]
to the neighbor set of node vi (including itself). The recursive relation in a GNN layer in which left
normalization is applied for feature smoothing is H[l] = σ(D[−][1](A + IN )H[l][−][1]W[l]) where W[l] is
the weight matrix in layer l, and IN ∈ R[N] _[×][N]_ denotes an identity matrix. With these definitions, the


-----

relation between the aggregated information Z[l] and node representations H[l] becomes equivalent
to H[l] = σ(Z[l]W[l]) at the lth GNN layer. As the provided analysis is applicable to every layer, l
superscript is dropped in the following to keep the notation simple.

It has been demonstrated that features that are correlated with the sensitive attribute result in bias
even when the sensitive attribute is not utilized in the learning process (Hajian & Domingo-Ferrer,
2013). This work provides an analysis on the correlation of s with Z, and aims to reduce it. Note
that, the reduction of correlation can still allow the generation of discriminable representations for
different class labels, if the discriminability is provided by non-sensitive attributes. The (sample)
correlation between the sensitive attributes s and aggregated representations can be written as

_N_ _N_ _N_
_j=1[(][z][ji][ −]_ _N[1]_ _k=1_ _[z][ki][)(][ s][j][ −]_ _N[1]_ _k=1_ [s][k][)]

_ρi = Corr(˜z:,i, s) =_

_N_ _N_ _N_ _N_
_j=1P[(][z][ji][ −]_ _N[1]_ _k=1P[z][ki][)][2]_ _j=1[(][ s][j][ −]P_ _N[1]_ _k=1_ [s][k][)][2]

where ˜z:,i, i = 1 _F is the ith column ofqP_ **Z. In the analysis, following assumptions are made:P** qP P
_· · ·_

**A1: Node representations have sample means µ0 and µ1 respectively across each group, where**
**_µA2:∀Based on these assumptions, the following theorem shows thatvi = mean(j ∈ Node representations have finite maximal deviationsSi withh ij | ∈{ vj ∈S0, 1}i.). Throughout the paper, mean(· ∆) denotes the sample mean operation.0 and ∥ ∆ρ∥11: That is, can be bounded from above, ∥hj −_** **_µi∥∞_** _≤_ ∆i,
which will serve as a guideline to design a fairness-aware graph data augmentation scheme.
**Theorem 1. The total correlation between the sensitive attributes s and representations Z that are**
_obtained after a mean aggregation over graph_ _,_ **_ρ_** 1, can be bounded above by
_G_ _∥_ _∥_

_∥ρ∥1 ≤∥c∥1(∥δ∥1 max(γ1, γ2) + 2N_ ∆) (1)

_where ci :=_ _√Nσ|S0˜z||S:,i_ 1| _, with σθ :=_ _N1_ _Nn=1[(][θ][n][ −]_ _N[1]_ _Ni=1_ _[θ][i][)][2][,][ ∀][θ][ ∈]_ [R][N] _[,][ δ][ :=][ µ][0][ −]_ **_[µ][1][,]_**

∆= max(∆γ1 := 1 − _[|]|S0[S],0[χ]0 ∆|[|]_ _[−]1).[|S]|S1[χ]1|[|]_ _, γ2 =_ 1 − q2 minPmean _d[χ]md[+][χ]m[d][ω]mP[|][v][m][ ∈S][0]_ _, mean_ _d[χ]nd[+][χ]n[d][ω]n_ _[|][v][n][ ∈S][1]_ _,_

       

The proof is given in Appendix A.1. The upper bound in equation 1 can be lowered by i) utilizing
can change the value offeature masking which has an effect on the term γ1, iii) edge augmentations that can reduce the value of ∥µ0 − **_µ1∥1 at the first layer, ii) node sampling that γ2._**

3.3 FAIR GRAPH DATA AUGMENTATIONS

Data augmentation has been studied extensively in order to enable certain unsupervised learning
schemes such as contrastive learning, self-supervised learning or as a general framework to improve
the generalizability of the trained models over unseen data. However, the design of graph data
augmentations is still a developing research area due to the challenges introduced by complex, nonEuclidean graph structure. Several augmentation schemes over the graph structure are proposed in
order to enhance the generalizability of GNNs (Rong et al., 2019; Zhao et al., 2020), while both
topological (e.g., edge/node deletion) and attributive (e.g., feature shuffling/masking) corruption
schemes have been developed in the context of contrastive learning (Veliˇckovi´c et al., 2019; You
et al., 2020; Zhu et al., 2021; 2020). However, none of these works are fairness-aware. Hence, in
this work, novel data augmentation schemes that are adaptive to the sensitive attributes, as well as
the input graph structure are introduced with Theorem 1 as guidelines.

3.3.1 FEATURE MASKING

In this subsection, an augmentation framework on nodal features H[0] = X is presented in order
to mitigate possible intrinsic bias propagated by them. Note that ∥δ∥1 in equation 1 is minimized
when all nodal features are the same (i.e., all nodal features masked/zeroed out). However, this
would result in the loss of all information in nodal features. Motivated by this, the proposed scheme
aims to improve uniform feature masking in terms of reducing ∥δ∥1 for a given masking budget
(a total amount of nodal features to be masked in expectation). Specifically, the random feature
masking scheme used in (You et al., 2020; Zhu et al., 2020) where each feature has the same masking
probability is modified to assign higher masking probabilities to the features varying more across


-----

different sensitive groups. Thus, masking probabilities are generated based on the term|δ|−min(|δ|) _|δ|. Let_ **_δ[¯] :=_**

max( **_δ_** ) min( **_δ_** ) [denote the normalized][ |][δ][|][, the feature masking probability can then be designed as]
_|_ _|_ _−_ _|_ _|_

_αδ[¯]_

**p[(][m][)]** = min _F1_ _Fi=1_ _δ[¯]i_ _, 1!_ (2)

where α is a hyperparameter. The feature mask m P 0, 1 is then generated as a random binary
_∈{_ _}[F]_
vector, with the i-th entry of m drawn independently from Bernoulli distribution with pi = (1 −
_p[(]i[m][)]) for i = 1, . . ., F_ . The augmented feature matrix is obtained via

**X˜** = [m **x1; . . . ; m** **xN** ][⊤], (3)
_◦_ _◦_

where [·; ·] is the concatenation operator, and ◦ is the Hadamard product. Since the proposed feature
masking scheme is probabilistic in nature, the resulting **_δ[˜] is a random vector with entry_** _δ[˜]i having_

_|δ[˜]i| =_ _|0δ,i|,_ with probabilitywith probability p 1i, _pi_ (4)
 _−_

where pi := 1 _p[(]i[m][)]_ is the probability that the i[th] feature is not masked in the graph view. The
_−_
following proposition shows that the novel, adaptive feature masking approach can decrease **_ρ_** 1
_∥_ _∥_
compared to random feature masking, the proof of which can be found in Appendix A.2.
**Proposition 1. In expectation, the proposed adaptive feature masking scheme results in a lower**
_∥δ[˜]∥1 value compared to uniform feature masking, meaning_

_Ep[_ **_δ_** 1] _Eq[_ **_δ_** 1] (5)
_∥[˜]∥_ _≤_ _∥[˜]∥_

_where q corresponds to uniform masking with masking probability 1 −_ _qi, and qi =_ _F[1]_ _Fj=1_ _[p][j][.]_

P

3.3.2 NODE SAMPLING

In this subsection, an adaptive node sampling framework is introduced to decrease the term γ1 :=

0 1 0 1
1− _|S|S[χ]0|[|]_ [+][ |S]|S[χ]1|[|] = 1− _|S0[χ]|[|][S][+][χ][|S][|]_ 0[ω][|][ +] _|S1[χ]|S[|][+][χ][|S][|]_ 1[ω][|] in equation 1 of Theorem 1, and hence to reduce

the intrinsic bias that the graph topology can create. A small γ1 suggests a more balanced population

     

distribution with respect to |S _[χ]| and |S_ _[ω]|. Specifically, a subset of nodes is selected at every epoch_
and the training is carried over the subgraph induced by the sampled nodes. This augmentation
mainly aims at reducing the bias by selecting a subset of more balanced groups, meanwhile it also
helps reduce the computational and memory complexity in training.

The proposed node sampling is adaptive to the input graph, that is, it depends on the cardinalities of
the sets S0[χ][,][ S]1[χ][,][ S]0[ω][, and][ S]1[ω][. The developed scheme copes with the case][ |S] _[χ][| ≤|S]_ _[ω][|][. In algorithm]_
design, it is assumed that if then 0 0 1 1
holds for all real graphs in our experiments, but our design principles can be readily extended to |S _[χ]| ≤|S_ _[ω]|_ _|S_ _[χ][| ≤|S]_ _[ω][|][ and][ |S]_ _[χ][| ≤|S]_ _[ω][|][ (same for][ S]_ _[ω][), which]_
different settings as well.

Given input graph G, the augmented graph _G[˜] can be obtained as an induced subgraph from a subset_
of nodes _V[˜]. All nodes in S_ _[χ]_ are retained, (S[˜]0[χ] [=][ S]0[χ] [and][ ˜]S1[χ] [=][ S]1[χ][), while subsets of nodes][ ¯]S0[ω]
andrespectively . See also Algorithm 1 in Appendix A.3. The cardinalities of node sets in the resultingS[¯]1[ω] [are randomly sampled from][ S]0[ω] [and][ S]1[ω] [with sample sizes][ |][ ¯]S0[ω][|][ =][ |S]0[χ][|][ and][ |][ ¯]S1[ω][|][ =][ |S]1[χ][|][,]
graph augmentation _G[˜] satisfy γ1 = 0. (See Appendix A.3 for details.)_

**Remark 1.** Note that the resulting graph [˜] yields γ1 = 0 as long as [˜]0 0 = 1/2 + ϕ and
_G_ _|S_ _[χ][|][/][|][ ˜]S_ _|_

[˜]1 1 = 1/2 _ϕ is satisfied for any_ 1/2 < ϕ < 1/2. The presented scheme here simply sets
_ϕ|S = 0[χ][|][/][|], which results in a balanced ratio across groups, but the performance can be improved if[ ˜]S_ _|_ _−_ _−_ _ϕ is_
selected carefully for specific datasets.

3.3.3 AUGMENTATION ON GRAPH CONNECTIVITY

Minimizing γ2 to zero in Theorem 1 suggests a graph topology where all nodes in the network have
the same number of neighbors from each sensitive group, i.e., d[ω]i [=][ d]i[χ][,][ ∀][v][i][ ∈V][. Since, for this]


-----

scenario,finding is parallel to the main design idea of Fairwalk (Rahman et al., 2019) in which the transition γ2 = 1 − 2 min mean _d[χ]md[+][χ]m[d][ω]m_ _[|][v][m][ ∈S][0]_ _, mean_ _d[χ]nd[+][χ]n[d][ω]n_ _[|][v][n][ ∈S][1]_ = 0. Note that this
       

probabilities are equalized for different sensitive groups in random walks in order to reduce bias in
random walk-based representations.

This finding suggests that an ideal augmented graph _G[˜] could be generated by deleting edges from_
or adding edges to G such that each node has exactly the same number of neighbors from each
sensitive group. However, such a per-node sampling scheme is computationally complex and may
not be ideal for large-scale graphs. To this end, we propose global probabilistic edge augmentation
schemes such that in the augmented graph,

E[|E[˜][ω]|] = E[|E[˜][χ]|]. (6)

Here the expectation is taken with respect to the randomness in the augmentation design. It is shown
in our experiments that the global approach can indeed help to reduce the value of γ2 (see Appendix
A.8). Note that even though the strategy is presented for the case where |E _[χ]| ≤|E_ _[ω]| (which holds for_
all datasets considered herein), the scheme can be easily generalized to the case where |E _[χ]| > |E_ _[ω]|._

In social networks, users connect to other users that are similar to themselves with higher probabilities (Mislove et al., 2010), hence the graph connectivity naturally inherits bias towards potential
minority groups. Motivated by the reduction of γ2, the present subsection introduces augmentation
schemes over edges to provide a balanced graph structure that can mitigate such bias.

**Adaptive Edge Deletion.** To obtain a balanced graph structure, we first develop an adaptive edge
deletion scheme where edges are removed with certain deletion probabilities. Based on the graph
structure and sensitive attributes, the probabilities are assigned as


1 _π,_ if si = sj
_−_ _̸_
1 − 2|E|ES[χ][ω]0| _[|]_ _[π,]_ if si = sj = 0 (7)

1 2|E _[χ]|_ if si = sj = 1
_−_ _|ES[ω]1_ _[|]_ _[π,]_


_p[(][e][)](eij) =_


where p[(][e][)](eij) is the removal probability of the edge connecting nodes vi and vj, π is a hyperparameter, and ES[ω]i [denotes the set of intra-edges connecting the nodes in][ S][i][. Note that][ π][ is chosen]
to be 1 in this work, but it can be selected by schemes such as grid search to improve performance.
While this graph-level edge deletion scheme does not not directly minimize γ2, it provides a balanced global structure such that Ep(e) [ [˜] 0 _[|][] =][ E]p[(][e][)]_ [[][|][ ˜] 1 _[|][] =][ 1]2_ [E][p][(][e][)] [[][|][ ˜] ], henceforth equation 6
_|ES[ω]_ _ES[ω]_ _E_ _[χ]|_

holds in the augmented graph _G[˜], see Appendix A.4 for more details._

**Adaptive Edge Addition.** For graphs that are very sparse, edge deletion may not be an ideal graph
augmentation as it may lead to unstable results. In this case, an adaptive edge addition framework
is developed to obtain a more balanced graph structure. Specifically, for graphs where |E _[χ]| ≤|E_ _[ω]|_
holds, |E _[ω]| −|E_ _[χ]| pairs of the nodes are sampled uniformly from S0[χ]_ [and][ S]1[χ] [with replacement.]
Then, a new edge is created to connect each sampled pair of nodes, in order to obtain an augmented
graph _G[˜] for which equation 6 holds. Note that experimental results in Section 4 also show that edge_
addition may become a better alternative over edge deletion for graphs that are sparse in inter-edges.

**Remark 2. Overall, while a subset of these augmentation schemes can be employed based on the**
input graph properties (sparse/dense, large/small), all schemes can also be employed on the input
graph sequentially. The framework where node sampling, edge deletion, edge addition are employed
sequentially together with feature masking is called ’FairAug’. It is worth emphasizing that edge
augmentation schemes should always follow node sampling, as performing node sampling changes
the distribution of edges. Since the cardinalities of different sets will be calculated only once (in
pre-processing), we note that the proposed augmentations will not incur significant additional cost.

4 EXPERIMENTS

In this section, experiments are carried out on 7 real-world datasets for node classification and link
prediction tasks. Performances of our proposed adaptive augmentations are compared with baseline
schemes in terms of utility and fairness metrics.


-----

4.1 DATASETS AND SETTINGS

**Datasets.** Experiments are conducted on real-world social and citation networks consisting of
Pokec-z, Pokec-n (Dai & Wang, 2020), UCSD34, Berkeley13 (Red et al., 2011), Cora, Citeseer,
Pubmed. Pokec-z and Pokec-n sampled from a larger social network, Pokec (Takac & Zabovsky,
2012), are used in node classification experiments where Pokec is a Facebook-like, social network
used in Slovakia. While the original network includes millions of users, Pokec-z and Pokec-n are
generated by collecting the information of users from two major regions (Dai & Wang, 2020). The
region information is treated as the sensitive attribute, while the working field of the users is the
label to be predicted in classification. Both attributes are binarized, see also (Dai & Wang, 2020).
UCSD34 and Berkeley13 are Facebook networks where edges are created based on the friendship
information in social media. Each user (node) has 7 dimensional nodal features including student/faculty status, gender, major, etc. Gender is utilized as the sensitive attribute in UCSD34 and
Berkeley13. Cora, Citeseer, and Pubmed are citation networks that consider articles as nodes and
descriptions of articles as their nodal attributes. In these datasets, the category of the articles is used
as the sensitive attribute. Statistics for datasets are presented in Tables 5 and 6 of Appendix A.5.

**Evaluation Metrics. Performance of the node classification task is evaluated in terms of accuracy.**
Two quantitative group fairness metrics are used to assess the effectiveness of fairness aware strategies in terms of statistical parity: ∆SP = |P (ˆy = 1 | s = 0) − _P_ (ˆy = 1 | s = 1)| and equal
**opportunity: ∆EO = |P** (ˆy = 1 | y = 1, s = 0) − _P_ (ˆy = 1 | y = 1, s = 1)|, where y and
_yˆ denote the ground-truth and the predicted labels, respectively. Lower values for ∆SP and ∆EO_
imply better fairness performance (Dai & Wang, 2020). For the link prediction task, both accuracy and Area Under the Curve (AUC) are employed as utility metrics. As the fairness metrics,
the definitions of statistical parity and equal opportunity are modified for link prediction such that
∆SP = |P (ˆy = 1 | e ∈E _[χ])−P_ (ˆy = 1 | e ∈E _[ω])| and ∆EO = |P_ (ˆy = 1 | y = 1, e ∈E _[χ])−P_ (ˆy =
1 | y = 1, e ∈E _[ω])|, where e denotes the edges and ˆy is the decision for whether the edge exists._

**Implementation details. The proposed augmentation scheme is tested on social networks with node**
representations generated through an unsupervised contrastive learning framework. Node classification and link prediction are employed as ensuing tasks that evaluate the performances of generated
node representations. In addition, we also provide link prediction results obtained via an end-to-end
graph convolutional network (GCN) model on citation networks. Further details on the implementation of the experiments are provided in Appendix A.7. Contrastive learning is utilized to demonstrate the effects of the proposed augmentation schemes, as augmentations are inherently utilized in
its original design (You et al., 2020). Specifically, GRACE (Zhu et al., 2020) is employed as our
baseline framework. GRACE constructs two different graph views using random, non-adaptive augmentation schemes, which we replaced by augmentations obtained via FairAug in the experiments.
For more details on the employed contrastive learning framework, see Appendix A.6.

**Baselines. We present the performances on a total of 7 baseline studies. As we examine our pro-**
posed augmentations in the context of contrastive learning, graph contrastive learning schemes are
employed as the natural baselines. Said schemes are Deep Graph Infomax (DGI) (Veliˇckovi´c et al.,
2019), Deep Graph Contrastive Representation Learning (GRACE) (Zhu et al., 2020), Graph Contrastive Learning with Adaptive Augmentations (GCA) (Zhu et al., 2021), and Fair and Stable Graph
Representation Learning (NIFTY) (Agarwal et al., 2021). We note that the objective function of
NIFTY (Agarwal et al., 2021) consists of both supervised and unsupervised components, and its
results for the unsupervised setting are provided here given the scope of this paper. In addition, as
another family of unsupervised approaches, random walk-based methods for unsupervised node representation generation are also considered. Such schemes include DeepWalk (Perozzi et al., 2014),
Node2Vec (Grover & Leskovec, 2016), and FairWalk (Rahman et al., 2019). Lastly, for end-to-end
link prediction, we present results for the random edge dropout scheme (Rong et al., 2019).

4.2 EXPERIMENTAL RESULTS

The comparison between baselines and our proposed framework FairAug is presented in Table 1.
Note that ’FairAug wo ED’ in Table 1 refers to FairAug where edge deletion (ED) is removed
from the chain, but all node sampling (NS), edge addition (EA), and feature masking (FM) are
still employed. Firstly, the results of Table 1 show that FairAug provides roughly 45% reduction
in fairness metrics over GRACE, the strategy it is built upon, while providing similar accuracy


-----

Table 1: Comparative Results with Baselines on Node Classification

Pokec-z Pokec-n

Accuracy (%) ∆SP (%) ∆EO (%) Accuracy (%) ∆SP (%) ∆EO(%)

DeepWalk 60.82 ± 0.77 9.79 ± 3.43 8.51 ± 1.67 59.66 ± 1.02 20.19 ± 3.38 22.96 ± 3.44
Node2Vec 61.56 ± 0.50 16.18 ± 12.40 14.88 ± 12.31 60.31 ± 0.75 20.76 ± 1.64 20.32 ± 1.36
FairWalk 57.48 ± 0.11 12.03 ± 10.19 10.95 ± 8.82 57.44 ± 0.73 15.00 ± 1.40 15.72 ± 1.80

DGI 65.56 ± 1.29 4.82 ± 1.89 5.81 ± 0.97 65.71 ± 0.24 5.18 ± 2.15 7.16 ± 2.44
GRACE **67.09 ± 0.47** 6.20 ± 2.22 6.18 ± 2.46 **67.90 ± 0.13** 8.85 ± 1.39 10.13 ± 2.11
GCA 66.85 ± 0.71 7.40 ± 4.14 7.46 ± 4.09 67.02 ± 0.43 5.31 ± 0.62 7.75 ± 1.49
NIFTY 66.09 ± 0.40 5.86 ± 1.97 6.19 ± 1.72 65.44 ± 0.17 5.18 ± 2.80 5.78 ± 3.18

FairAug 67.04 ± 0.69 3.29 ± 1.66 **3.04 ± 1.37** 67.88 ± 0.45 4.81 ± 0.59 6.52 ± 0.99
FairAug wo ED 67.38 ± 0.31 **2.66 ± 3.22** 4.26 ± 2.92 67.57 ± 0.18 **3.37 ± 0.62** **5.13 ± 1.12**

Table 2: Ablation Study on Node Classification

Pokec-z Pokec-n

Accuracy (%) ∆SP (%) ∆EO (%) Accuracy (%) ∆SP (%) ∆EO(%)

GRACE 67.09 ± 0.47 6.20 ± 2.22 6.18 ± 2.46 67.90 ± 0.13 8.85 ± 1.39 10.13 ± 2.11

FairAug 67.04 ± 0.69 3.29 ± 1.66 **3.04 ± 1.37** 67.88 ± 0.45 4.81 ± 0.59 6.52 ± 0.99
FairAug wo FM 66.88 ± 0.42 4.64 ± 1.91 5.50 ± 1.80 66.15 ± 0.75 7.61 ± 0.29 9.28 ± 1.06
FairAug wo NS 67.01 ± 0.41 6.05 ± 2.63 6.83 ± 2.88 66.33 ± 0.20 6.26 ± 0.26 9.16 ± 0.63
FairAug wo ED **67.38 ± 0.31** **2.66 ± 3.22** 4.26 ± 2.92 67.57 ± 0.18 **3.37 ± 0.62** **5.13 ± 1.12**
FairAug wo EA 66.96 ± 1.08 2.86 ± 1.79 3.22 ± 1.29 **67.96 ± 0.19** 7.13 ± 0.29 9.40 ± 0.81

values. Second, we note that similar to our framework, GCA is built upon GRACE through adaptive
augmentations as well. However, the adaptive augmentations utilized in GCA are not fairness-aware,
and the results of Table 1 demonstrate that the effect of such augmentations on the fairness metrics
is unpredictable. Third, the results indicate that all contrastive learning methods provide better
fairness performance than random walk-based methods on evaluated datasets, including Fairwalk,
which is a fairness-aware study. Since the sole information source of random walk-based studies is
the graph structure, obtained results confirm that the graph topology indeed propagates bias, which
is consistent with the motivation of our graph data augmentation design. Finally, the results of Table
1 demonstrate that the closest competitor scheme to FairAug is NIFTY (Agarwal et al., 2021) in
terms of fairness measures. For ∆SP, FairAug outperforms NIFTY on both datasets, whereas in
terms of ∆EO, FairAug and NIFTY outperform each other on Pokec-z and Pokec-n, respectively.
Table 1 shows that the ED-removed version of FairAug, ’FairAug wo ED’, outperforms NIFTY on
Pokec-n as well, suggesting that at least one of the proposed strategies outperform all considered
benchmark schemes in both ∆SP and ∆EO. This fairness performance improvement achieved by
ED removal motivated us to conduct an ablation study on the building blocks of FairAug, which we
present in the sequel.

Table 2 lists the results of an ablation study for FairAug, where the last four rows demonstrate
the effects of the removal of FM, NS, ED, and EA, respectively. Pokec-z and Pokec-n are highly
unbalanced datasets (|E _[ω]| ≈_ 20|E _[χ]|) with a considerable sparsity in inter-edges (see Table 5 of_
Appendix A.5). For such networks, the exact probabilities presented in equation 7 cannot be utilized
for edge deletion as it would result in the deletion of a significantly large portion of the edges,
damaging the graph structure. We employ an upper limit on the deletion probabilities to avoid
this (see Appendix A.7). However, with such a limit, the proposed ED framework alone cannot
sufficiently balance |E _[ω]| and |E_ _[χ]|. At this point, we note that since |S_ _[ω]| > |S_ _[χ]|, node sampling_
also provides a way of decreasing |E _[ω]| by excluding nodes from the set S_ _[ω]_ when generating the
subgraph. As the graph was already sparse initially in inter-edges (i.e., small |E _[χ]|), employing_
both NS and ED causes a similar over-deletion of intra-edges and creates unstable results due to the
significantly distorted graph structure. Overall, combining this phenomenon with the results of Table
2, in order to balance the input graphs on Pokec networks consistently (which have |E _[ω]| >> |E_ _[χ]|_
and inter-edge sparsity), node sampling is observed to be a better choice than edge manipulations.

Table 3 presents the obtained results for link prediction in the framework of contrastive learning.
Results demonstrate that ’FairAug wo NS’ consistently improves the fairness metrics of the framework it is built upon (GRACE), together with similar utility measures. In addition, comparing GCA


-----

Table 3: Link prediction results obtained on node representations

UCSD34 Berkeley13

AUC (%) ∆SP (%) ∆EO (%) AUC (%) ∆SP (%) ∆EO(%)

GCA 71.59 ± 0.29 0.70 ± 0.27 1.92 ± 0.45 69.69 ± 0.38 **0.50 ± 0.36** 6.52 ± 0.99
GRACE 71.57 ± 0.28 0.49 ± 0.28 1.48 ± 0.70 69.55 ± 0.46 0.76 ± 0.44 4.34 ± 1.20

FairAug 71.46 ± 0.30 0.71 ± 0.38 1.62 ± 0.62 69.48 ± 0.29 0.70 ± 0.43 4.24 ± 0.90
FairAug wo NS 71.50 ± 0.31 **0.41 ± 0.32** **1.16 ± 0.65** 69.65 ± 0.33 0.68 ± 0.34 **4.22 ± 0.97**

Table 4: Employment of fair edge deletion as an edge dropout method

Accuracy (%) AUC (%) ∆SP (%) ∆EO (%)


**Cora** Edge Dropout 82.79 ± 0.83 90.52 ± 0.52 57.22 ± 2.19 36.18 ± 4.53
Fair ED 80.38 ± 1.14 87.95 ± 1.22 48.78 ± 2.58 27.79 ± 3.89

**Citeseer** Edge Dropout 78.30 ± 1.17 87.93 ± 1.05 43.05 ± 2.39 25.45 ± 3.69
Fair ED 77.26 ± 1.80 86.79 ± 1.39 39.90 ± 3.57 25.02 ± 6.93

**Pubmed** Edge Dropout 88.63 ± 0.34 95.14 ± 0.16 45.59 ± 0.78 15.81 ± 0.92
Fair ED 87.48 ± 0.54 94.15 ± 0.36 40.90 ± 1.10 11.70 ± 0.68

and GRACE, obtained results confirm our previous assessment regarding the unpredictable effect of
GCA’s augmentations on the fairness metrics. Finally, comparing FairAug with and without NS, the
results of Table 3 show that in UCSD34 and Berkeley13, the employment of node sampling can be
ineffective in improving fairness metrics, or can even worsen them. In UCSD34 and Berkeley13, we
have |S _[χ]| >> |S_ _[ω]| (see Table 5 of Appendix A.5). Therefore, for NS on these datasets, half of the_
nodes are sampled randomly from the sets S1[χ] [and][ S]0[χ] [(as the limit on the minimum node sampling]
budget is half of the initial group size to avoid a possible over-sampling, see Appendix A.7). Since
_|S_ _[χ]| >> |S_ _[ω]|, such a sampling framework coincides with random sampling, which makes the ef-_
fects of the proposed node sampling scheme unpredictable on the fairness metrics. Furthermore,
while γ1 suggests that the cardinality of the set |S _[χ]| should be reduced when |S_ _[ω]| ≤|S_ _[χ]|, γ1 ac-_
tually appears in the upper bound and not in the exact ∥ρ∥1 expression. The removal of nodes with
inter-edges is actually counter-intuitive, as inter-edges generally help to reduce bias in graphs where
_|E_ _[ω]| ≥|E_ _[χ]| (which holds for all datasets considered herein). Therefore, the proposed node sam-_
pling becomes effective for graph structures providing |S _[ω]| ≥|S_ _[χ]|. The effect of node sampling on_
Facebook networks is further investigated through γ1 and γ2 in Appendix A.8, which corroborates
the explanations on the random/detrimental effect of node sampling for these datasets.

As also noted in Section 1, even though FairAug is presented through its application on graph constrastive learning in this section, the proposed approach can be fully or partially used in conjunction
to other learning frameworks as well. To exemplify such a use case, Table 4 lists the results for an
end-to-end link prediction task with a two-layer GCN where the proposed fair edge deletion scheme
is employed as an edge dropout method. As the benchmark, random edge dropout (Rong et al.,
2019) is considered, which is a scheme originally proposed to improve the generalizability of the
model on unseen data (Rong et al., 2019). In the experiments, the number of deleted edges is the
same for both strategies. The results demonstrate that our method ’Fair ED’ can indeed enhance the
fairness of the learning framework it is employed in while it slightly reduces the utility measures.

5 CONCLUSIONS

In this study, the source of bias in aggregated representations in a GNN-based framework has been
theoretically analyzed. Based on the analysis, several fairness-aware augmentation schemes have
been introduced on both graph structure and nodal features. The proposed augmentations can be
flexibly utilized together with several GNN-based learning methods. In addition, they can readily be
employed in unsupervised node representation learning schemes such as graph contrastive learning.
Experimental results on real-world graphs demonstrate that the proposed adaptive augmentations
can improve fairness metrics with comparable utilities to state-of-the-art in node classification and
link prediction.


-----

6 REPRODUCIBILITY STATEMENT

The proofs for Theorem 1 and Proposition 1 are presented together with all assumptions made for
them in Appendix A.1 and Appendix A.2, respectively. Implementation details and hyperparameter
values are presented in Appendix A.7 for the clarification of experimental settings. All codes that
are needed to regenerate the presented results in Tables 1, 2, 3 and 4 are provided within the supplementary material together with a README file. Finally, the details on the utilized data sets are
provided in Appendix A.5 where all datasets are publicly available.

REFERENCES

Chirag Agarwal, Himabindu Lakkaraju*, and Marinka Zitnik*. Towards a unified framework for
fair and stable graph representation learning. arXiv preprint arXiv:2102.13186, February 2021.

Amr Ahmed, Nino Shervashidze, Shravan Narayanamurthy, Vanja Josifovski, and Alexander J
Smola. Distributed large-scale natural graph factorization. In Proc. International Conference
_on World Wide Web (WWW), pp. 37–48, May 2013._

Alex Beutel, Jilin Chen, Zhe Zhao, and Ed H Chi. Data decisions and theoretical implications when
adversarially learning fair representations. arXiv preprint arXiv:1707.00075, June 2017.

Avishek Bose and William Hamilton. Compositional fairness constraints for graph embeddings. In
_International Conference on Machine Learning (ICML, pp. 715–724. PMLR, 2019._

Maarten Buyl and Tijl De Bie. Debayes: a bayesian method for debiasing network embeddings. In
_International Conference on Machine Learning (ICML), pp. 1220–1229. PMLR, 2020._

Maarten Buyl and Tijl De Bie. The kl-divergence between a graph model and its fair i-projection as
a fairness regularizer. arXiv preprint arXiv:2103.01846, 2021.

Shaosheng Cao, Wei Lu, and Qiongkai Xu. Grarep: Learning graph representations with global
structural information. In Proc. ACM International Conference on Information and Knowledge
_Management (CIKM), pp. 891–900, October 2015._

Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the oversmoothing problem for graph neural networks from the topological view. In Proceedings of the
_AAAI Conference on Artificial Intelligence, volume 34, pp. 3438–3445, 2020a._

Haochen Chen, Bryan Perozzi, Yifan Hu, and Steven Skiena. Harp: Hierarchical representation
learning for networks. In Proc. AAAI Conference on Artificial Intelligence, volume 32, February
2018.

Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework
for contrastive learning of visual representations. In Proc. International Conference on Machine
_Learning (ICML), pp. 1597–1607, July 2020b._

Enyan Dai and Suhang Wang. Say no to the discrimination: Learning fair graph neural networks
with limited sensitive attribute information. arXiv preprint arXiv:2009.01454, September 2020.

Yushun Dong, Jian Kang, Hanghang Tong, and Jundong Li. Individual fairness for graph neural
networks: A ranking based approach. In Proc ACM Conference on Knowledge Discovery & Data
_Mining (SIGKDD), pp. 300–310, 2021._

Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness
through awareness. In Proc. Innovations in Theoretical Computer Science (ITCS), pp. 214–226,
January 2012.

Joseph Fisher, Arpit Mittal, Dave Palfrey, and Christos Christodoulopoulos. Debiasing knowledge
graph embeddings. In Proc. Conference on Empirical Methods in Natural Language Processing
_(EMNLP), pp. 7332–7345, 2020._


-----

Alberto Garc´ıa-Dur´an and Mathias Niepert. Learning graph representations with embedding propagation. In Proc. International Conference on Neural Information Processing Systems (NeurIPS),
pp. 5125–5136, December 2017.

Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proc. International Conference on Artificial Intelligence and Statistics (AISTATS),
pp. 249–256, May 2010.

Ignacio S Gomez, Bruno G da Costa, and Maike AF Dos Santos. Majorization and dynamics of
continuous distributions. Entropy, 21(6):590, 2019.

Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proc. ACM
_International Conference on Knowledge Discovery and Data Mining (SIGKDD), August 2016._

Sara Hajian and Josep Domingo-Ferrer. A methodology for direct and indirect discrimination prevention in data mining. IEEE Transactions on Knowledge and Data Engineering, 25(7):1445–
1459, July 2013. doi: 10.1109/TKDE.2012.72.

William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In Proc. International Conference on Neural Information Processing Systems (NeurIPS),
pp. 1025–1035, December 2017.

Kaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view representation learning on
graphs. In Proc. International Conference on Machine Learning (ICML), pp. 4116–4126, July
2020.

R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. In Proc. International Conference on Learning Representations (ICLR), April
2018.

Bas Hofstra, Rense Corten, Frank Van Tubergen, and Nicole B Ellison. Sources of segregation
in social networks: A novel approach using facebook. American Sociological Review, 82(3):
625–656, May 2017.

Fenyu Hu, Yanqiao Zhu, Shu Wu, Liang Wang, and Tieniu Tan. Hierarchical graph convolutional
networks for semi-supervised node classification. In Proc. International Joint Conference on
_Artificial Intelligence, (IJCAI), August 2019._

Xu Ji, Jo˜ao F Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised
image classification and segmentation. In Proc. IEEE International Conference on Computer
_Vision (ICCV), pp. 9865–9874, October 2019._

Guangyin Jin, Qi Wang, Cunchao Zhu, Yanghe Feng, Jincai Huang, and Jiangping Zhou. Addressing crime situation forecasting task with temporal graph convolutional neural network approach. In Proc. International Conference on Measuring Technology and Mechatronics Automa_tion (ICMTMA), pp. 474–478, February 2020. doi: 10.1109/ICMTMA50254.2020.00108._

Kushal Kafle, Mohammed Yousefhussien, and Christopher Kanan. Data augmentation for visual
question answering. In Proceedings of the 10th International Conference on Natural Language
_Generation, pp. 198–202, 2017._

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
_arXiv:1412.6980, 2014._

Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In Proc. International Conference on Learning Representations (ICLR), April 2017.

Charlotte Laclau, Ievgen Redko, Manvi Choudhary, and Christine Largeron. All of the fairness for
edge prediction with optimal transport. In International Conference on Artificial Intelligence and
_Statistics, pp. 1774–1782. PMLR, 2021._


-----

Peizhao Li, Yifei Wang, Han Zhao, Pengyu Hong, and Hongfu Liu. On dyadic fairness: Exploring and mitigating bias in graph connections. In Proc. International Conference on Learning
_Representations (ICLR), April 2021._

Jiaqi Ma, Junwei Deng, and Qiaozhu Mei. Subgroup generalization and fairness of graph neural
networks. arXiv preprint arXiv:2106.15535, 2021.

Albert W Marshall, Ingram Olkin, and Barry C Arnold. Inequalities: theory of majorization and its
_applications, volume 143. Springer, 1979._

Alan Mislove, Bimal Viswanath, Krishna P Gummadi, and Peter Druschel. You are who you know:
inferring user profiles in online social networks. In Proc. ACM International Conference on Web
_Search and Data Mining (WSDM), pp. 251–260, February 2010._

Felix L Opolka, Aaron Solomon, C˘at˘alina Cangea, Petar Veliˇckovi´c, Pietro Li`o, and R Devon
Hj elm. Spatio-temporal deep graph infomax. arXiv preprint arXiv:1904.06316, April 2019.

Mingdong Ou, Peng Cui, Jian Pei, Ziwei Zhang, and Wenwu Zhu. Asymmetric transitivity preserving graph embedding. In Proc. ACM International Conference on Knowledge Discovery and
_Data Mining (SIGKDD), pp. 1105–1114, August 2016._

Zhen Peng, Wenbing Huang, Minnan Luo, Qinghua Zheng, Yu Rong, Tingyang Xu, and Junzhou
Huang. Graph representation learning via graphical mutual information maximization. In Proc.
_Web Conference (WWW), pp. 259–270, April 2020._

Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Proc. ACM International Conference on Knowledge Discovery and Data
_[Mining (SIGKDD), pp. 701–710, August 2014. URL http://doi.acm.org/10.1145/](http://doi.acm.org/10.1145/2623330.2623732)_
[2623330.2623732.](http://doi.acm.org/10.1145/2623330.2623732)

Tahleen A Rahman, Bartlomiej Surma, Michael Backes, and Yang Zhang. Fairwalk: Towards fair
graph embedding. In Proc. International Joint Conference on Artificial Intelligence (IJCAI), pp.
3289–3295, August 2019.

Veronica Red, Eric D Kelsic, Peter J Mucha, and Mason A Porter. Comparing community structure
to characteristics in online collegiate social networks. SIAM review, 53(3):526–543, 2011.

Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph
convolutional networks on node classification. arXiv preprint arXiv:1907.10903, 2019.

Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning.
_Journal of Big Data, 6(1):1–48, 2019._

Indro Spinelli, Simone Scardapane, Amir Hussain, and Aurelio Uncini. Biased edge dropout for
enhancing fairness in graph representation learning. arXiv preprint arXiv:2104.14210, 2021.

Lubos Takac and Michal Zabovsky. Data analysis in public social networks. In International Sci_entific Conference and International Workshop. ’Present Day Trends of Innovations’, volume 1,_
May 2012.

Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale
information network embedding. In Proc. International Conference on World Wide Web (WWW),
pp. 1067–1077, May 2015.

Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua
Bengio. Graph attention networks. Proc. International Conference on Learning Representations
_(ICLR), April 2018._

Petar Veliˇckovi´c, William Fedus, William L. Hamilton, Pietro Li`o, Yoshua Bengio, and R Devon
Hjelm. Deep graph infomax. In Proc. International Conference on Learning Representations
_[(ICLR), May 2019. URL https://openreview.net/forum?id=rklz9iAcKQ.](https://openreview.net/forum?id=rklz9iAcKQ)_

Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying graph convolutional networks. In Proc. International Conference on Machine Learning
_(ICML), pp. 6861–6871, July 2019._


-----

Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via nonparametric instance discrimination. In Proc. IEEE Conference on Computer Vision and Pattern
_Recognition (CVPR), pp. 3733–3742, June 2018._

Mang Ye, Xu Zhang, Pong C Yuen, and Shih-Fu Chang. Unsupervised embedding learning via
invariant and spreading instance feature. In Proc. IEEE Conference on Computer Vision and
_Pattern Recognition (CVPR), pp. 6210–6219, June 2019._

Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph
contrastive learning with augmentations. Proc. International Conference on Neural Information
_Processing Systems (NeurIPS), 33, December 2020._

Ziqian Zeng, Rashidul Islam, Kamrun Naher Keya, James Foulds, Yangqiu Song, and Shimei
Pan. Fair representation learning for heterogeneous information networks. _arXiv preprint_
_arXiv:2104.08769, 2021._

Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. Advances in neural information processing systems, 28:649–657, 2015.

Tong Zhao, Yozen Liu, Leonardo Neves, Oliver Woodford, Meng Jiang, and Neil Shah. Data augmentation for graph neural networks. arXiv preprint arXiv:2006.06830, 2020.

Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep Graph Contrastive Representation Learning. In Proc. International Conference on Machine Learning
_(ICML) Workshop on Graph Representation Learning and Beyond, July 2020._ [URL http:](http://arxiv.org/abs/2006.04131)
[//arxiv.org/abs/2006.04131.](http://arxiv.org/abs/2006.04131)

Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Graph contrastive learning
with adaptive augmentation. In Proc. Web Conference (WWW), April 2021.


-----

A APPENDIX

A.1 PROOF OF THEOREM 1

Let ¯s = _N[1]_ _Ni=1_ _[s][i][ =][ |S]N[1][|]_ [Hence, the elements of centered sensitive attribute vector can be written]

as

P 1

_ρi =_ ( (0 _s¯)(zm,i_ _z¯i) +_ (1 _s¯)(zn,i_ _z¯i)),_ (8)

_Nσsσz˜:,i_ _vmX∈S0_ _−_ _−_ _vnX∈S1_ _−_ _−_

where ¯zi = _N1_ _Nj=1_ _[z][ji][ and][ z][ji][ is the element of matrix][ Z][ at row][ j][, column][ i][. Using][ ¯]s values, the_

equation 8 becomes
P

1 1 0
_ρi =_ ( _−|S_ _|_ (zm,i _z¯i) +_ _|S_ _|_ _zi)),_

_Nσsσz˜:,i_ _vmX∈S0_ _N_ _−_ _vnX∈S1_ _N_ [(][z][n,i][ −] [¯]

1 1 0
= ( _[|S][0][||S][1][|]_ _z¯i +_ _−|S_ _|_ _zm,i_ _z¯i +_ _|S_ _|_

_Nσsσz˜:,i_ _N_ _vmX∈S0_ _N_ _−_ _[|S][0]N[||S][1][|]_ _vnX∈S1_ _N [z][n,i][)][,]_ (9)

1 0 1
= ( _|S_ _|_ _|S_ _|_

_Nσsσz˜:,i_ _vnX∈S1_ _N [z][n,i][ −]_ _vmX∈S0_ _N [z][m,i][)][,]_

0 1 1 1
= _N|S[2]σ||Ssσz˜:|,i_ _|S1|_ _vnX∈S1_ _zn,i −_ _|S0|_ _vmX∈S0_ _zm,i!_ _._

Similarly analysis for σs follows as

_N_ _N_

[1] [1] 1 0

_σs :=_ (sn _s¯)[2]_ = _|S_ _|[2]_ _|S_ _|[2]_

vu _N_ _n=1_ _−_ vu _N_ _n=1_ _vm_ _S0_ _N_ [2][ +] _vn_ _S1_ _N_ [2] !
u X u X X∈ X∈
t t

1 0 1 2

= _|S_ _||S_ _|_ +

s _N_ _N_ [2] _[|S][1]N[||S][2][0][|][2]_ (10)

 


0 1 [(][|S][0][|][ +][ |S][1][|][)]
_|S_ _||S_ _|_ ( 0 + 1 )[2]

_|S_ _|_ _|S_ _|_


0 1

= _|S_ _||S_ _|_

_N_

p

_Nσ|S0˜z||S:,i_ 1| and c ∈ R[F] _′_ denote the vector whose elements are cis. Then ρ equals to


Define ci :=


(11)

(12)

(13)


**_ρ = c_** **zn** **zm**
_◦_ _|S1|_ _vnX∈S1_ _−_ _|S0|_ _vmX∈S0_

where ◦ represents the Hadamard product. Therefore, ∥ρ∥1 follows as


**_ρ = c ◦_**


1
_|S_ _|_


_∥ρ∥1 =_ _|S1|_ _vnX∈S1_ **zn −** _|S0|_ _vmX∈S0_ **zm!1.**

We first consider the terms 11 **[c]v[ ◦]n** _S1_ **[z][n][ and]** 10 _vm_ _S0_ **[z][m][ individually]**

_|S_ _|_ _∈_ _|S_ _|_ _∈_
P P


( [1] **_µ0 + [1]_** **_µ1)_** ∆,

_dn_ _dn_ _±_

_vn_ _S1_ _a_ (n) _S0_ _b_ (n) _S1_

X∈ _∈NX∩_ _∈NX∩_

_d[χ]n[µ][0]_ [+][ d][ω]n[µ][1]

∆,

_vnX∈S1_ _d[χ]n_ [+][ d][ω]n _±_

**_µ1 +_** _d[χ]n_ (µ0 **_µ1)_** ∆.

_vn_ _S1_  _d[χ]n_ [+][ d][ω]n _−_  _±_

X∈


**zn**
_vnX∈S1_ _∈_


1
_|S_ _|_


1
_|S_ _|_

1

1
_|S_ _|_

1

1
_|S_ _|_


-----

Similarly, the expression for the term


_vm_ _S0_ **[z][m][ can also be derived.]**
_∈_


_|S0|_


**_µ0 +_** _d[χ]m_ (µ1 **_µ0)_** ∆. (14)

_d[χ]m_ [+][ d][ω]m _−_ _±_



**zm**

_|S0|_ _vmX∈S0_ _∈_


0
_|S_ _|_


_vm∈S0_


Define ϵ := 11 _vn_ _S1_ **[z][n][ −]** 10 _vm_ _S0_ **[z][m][, the following can be written by equation 13 and]**

_|S_ _|_ _∈_ _|S_ _|_ _∈_

equation 14

P P

**_ϵ_** (µ1 **_µ0)_** 1 1 _d[χ]n_ 1 _d[χ]m_ 2∆. (15)
_∈_ _−_ _−_ _|S1|_ _vnX∈S1_  _d[χ]n_ [+][ d][ω]n  _−_ _|S0|_ _vmX∈S0_  _d[χ]m_ [+][ d][ω]m [!] _±_

**_ϵ is bounded below, which follows as_**

1 1 _d[χ]n_ _d[χ]m_
_−_ _|S1|_ _vnX∈S1_  _d[χ]n_ [+][ d][ω]n  _−_ _|S[1]0|_ _vmX∈S0_  _d[χ]m_ [+][ d][ω]m 

= 1 1 _d[χ]n_ 1 _d[χ]m_ (16)
_−_ 1 _d[χ]n_ [+][ d][ω]n _−_ 0 _d[χ]m_ [+][ d][ω]m

_|S_ _|_ _vnX∈S1[χ]_   _|S_ _|_ _vmX∈S0[χ]_  

1 0 _[|]_ 1 _[|]_ (17)
_≥_ _−_ _[|S]_ _[χ]0_ _S1_

_|S_ _| [−|S]|_ _[χ]|_

equation 16 follows as d[χ]u [= 0][ for][ ∀][v][u] _[∈S]_ _[ω][. The upper bound equation 17 holds, since]_ _d[χ]ud[+][χ]u[d][ω]u_ _≤_

1 for _vu_ .  
_∀_ _∈G_

The upper bound for ϵ follows as


_d[χ]n_

_d[χ]n_ [+][ d][ω]n


_d[χ]m_

_d[χ]m_ [+][ d][ω]m


(18)

(19)

_,_

(20)


1 −


1
_|S_ _|_


_d[χ]n_ [+][ d][ω]n _−_ _|S0|_ _vmX∈S0_ _d[χ]m_ [+]

= 1 mean _d[χ]m_ _vm_ 0
_−_ _d[χ]m_ [+][ d][ω]m _|_ _∈S_



_vn∈S1_


mean _d[χ]n_ _vn_ 1
_−_ _d[χ]n_ [+][ d][ω]n _|_ _∈S_



_n_ _n_

_, mean_ _d[χ]n_ _vn_ 1

_d[χ]n_ [+][ d][ω]n _|_ _∈S_

 


1 2 min mean _d[χ]m_ _vm_ 0
_≤_ _−_ _d[χ]m_ [+][ d][ω]m _|_ _∈S_
 


where mean(·) denotes the sample mean operation. The main purpose is to generate an upper bound
for **_ρ_** 1, for which the first step is to derive the following bound
_∥_ _∥_

**_ρ_** 1 **c** 1 **_ϵ_** 1 (21)
_∥_ _∥_ _≤∥_ _∥_ _∥_ _∥_

equation 21 can be written, as + _xN_ _yN_ ( _x1_ + + ∥xx ◦N **y)(∥1y ≤∥1** +x∥1 +∥y∥y1N for arbitrary vectors) due to additional non-negative cross-terms x and y ∈ R[N] (|x1y1| +

_· · ·_ _|_ _| ≤_ _|_ _|_ _· · ·_ _|_ _|_ _|_ _|_ _· · ·_ _|_ _|_
_|xi||yj|, i ̸= j). The next step is to bound the ℓ1 norm of the difference term ϵ:_

**_ϵ_** 1 **_µ1_** **_µ0_** 1max(γ1, γ2) + 2N ∆ (22)
_∥_ _∥_ _≤∥_ _−_ _∥_

using the derived bounds in equation 17, equation 20 and the equality 2∆ **1** 1 =

_· ∥_ _∥_

2N ∆, where 1 denotes an N 1 vector of ones, γ1 := 00 _[|]_ _S11_ _[|]_ =
_×_ _|S_ _|_ _[−|]|[S]_ _[χ]|_

1 − 2 min mean _d[χ]md[+][χ]m[d][ω]m_ _, mean_ _d[χ]nd[+][χ]n[d][ω]n_ . Using equation 21 and equa-[1][ −|][S] _[χ]_ [, and][ γ][2]

tion 22, the final upper bound can be derived:  _[|][v][m][ ∈S][0]_  _[|][v][n][ ∈S][1]_

_∥ρ∥1 ≤∥c∥1 (∥µ1 −_ **_µ0∥1max(γ1, γ2) + 2N_** ∆) . (23)


-----

A.2 PROOF OF PROPOSITION 1

The expected ∥δ[˜]∥1 can be written as:

_Ep[_ **_δ_** 1] = E
_∥[˜]∥_


_F_

_δi_

" _i=1_ _|[˜]_ _|_
X


(24)


_E[_ _δi_ ] =
_|[˜]_ _|_
_i=1_

X


_pi_ _δi_ _._
_|_ _|_
_i=1_

X


Similarly, the expected ∥δ[˜]∥1 for the uniform masking scheme is:[1]


_Eq[_ **_δ_** 1] =
_∥[˜]∥_


_qi_ _δi_ _._ (25)
_|_ _|_
_i=1_

X


For fair comparison, set uniform keeping probability of the nodal features qi = ¯p = _F1_ _Fi=1_ _[p][i][.]_

Without loss of generality, assume, _δi_ s are ordered such that _δ1_ _δF_ . With the ordered
_δi_ s, assigned probabilities to keep the nodal features by our method will also be ordered such that | _|_ _|_ _| ≤· · · ≤|_ _|_ P
_|_ _|_
_p1 ≥· · · ≥FpF . Defining a dummy variable |δ0| := 0, equation 24 can be rewritten as:_

_pi|δi| = (|δ1| −|δ0|)(p1 + · · · + pF ) + (|δ2| −|δ1|)(p2 + · · · + pF ) + . . ._
_i=1_

X


(26)


+ ( _δF_ 1 _δF_ 2 )(pF 1 + pF ) + ( _δF_ _δF_ 1 )(pF )

_· · ·_ _|_ _−_ _| −|_ _−_ _|_ _−_ _|_ _| −|_ _−_ _|_


(|δl| −|δl−1|)
_l=1_

X


_pi._

_i=l_

X


Following the definition in (Marshall et al., 1979), a sequence x = _x1, x2, . . ., xF_ majorizes
another sequence y = _y1, y2, . . ., yF_, if the following holds: _{_ _}_
_{_ _k_ _}_ _k_


_yi_
_i=1_ _≤_

X

_F_

_yi =_
_i=1_

X


_xi_ _k = 1, . . ., F_ 1,
_−_
_i=1_

X

_F_

_xi._
_i=1_

X


(27)


Since the uniform sequence is majorized by any other non-increasing ordered sequence with the
same sum (see (Gomez et al., 2019, Equation 3)), the sequence p = _p1, p2,_ _, pF_ majorizes
_{_ _· · ·_ _}_
equation 26 can be re-written asthe sequence q = {q1, q2, . . ., qF } where qi = ¯p, ∀i ∈{1, . . ., F _}. Defining_ _i=1_ _[p][i][ := 0][,]_

[P][0]


_Ep[_ **_δ_** 1] =
_∥[˜]∥_

=

(a)


(|δl| −|δl−1|)
_l=1_

X


_pi_

_i=l_

X


_F_ _l−1_

(|δl| −|δl−1|)(F ¯p − _pi)_
_l=1_ _i=1_

X X

_F_ _l−1_

(|δl| −|δl−1|)(F ¯p − _qi)_
_l=1_ _i=1_

X X


(28)


(|δl| −|δl−1|)
_l=1_

X


_qi_

_i=l_

X


= Eq[ **_δ_** 1],
_∥[˜]∥_

ities1 qFrom equation 25, it can be obtained thati ∈ [0, 1]. Here, ∥δ∥1 corresponds to the distribution difference with no feature masking ( Eq[∥δ˜∥1] = _i=1_ _[q][i][|][δ][i][|≤]_ [P]i[F]=1 _[|][δ][i][|][ =][ ∥][δ][∥][1][, as the probabil-]qi = 1, ∀i)._
This implies that no feature masking upper bounds E[ **_δ_** 1], which is obtained via a stochastic feature masking.[P][F]
_∥[˜]∥_


-----

where inequality (a) follows from the definition of majorization in equation 27.

A.3 NODE SAMPLING AND ITS EFFECTS

**Algorithm 1: Node Sampling**
**Data: G := (V, E), X, s, β**
**Result:** _G[˜],_ **X[˜]** _,_ ˜s
Splitif |S _[ω] V| ≥|S into sets[χ]| then {S0[χ][,][ S]1[χ][,][ S]0[ω][,][ S]1[ω][}]_

Sample |S0[χ][|][ nodes from][ S]0[ω] [uniformly to obtain][ ¯]S0[ω][;]
SampleV˜ := {S |S[χ],1[χ]S[¯][|][ nodes from]0[ω][,][ ¯]S1[ω][}][;] _[ S]1[ω]_ [uniformly to obtain][ ¯]S1[ω][;]
**end**
**if |S** _[χ]| ≥|S_ _[ω]| then_

Sample |S0[ω][|][ nodes from][ S]0[χ] [uniformly to obtain][ ¯]S0[χ][;]
_VSample˜ := {S |S[¯]0[χ][,]1[ω][ ¯]S[|][ nodes from]1[χ][,][ S]_ _[ω][}][;]_ _[ S]1[χ]_ [uniformly to obtain][ ¯]S1[χ][;]
**end**
Obtain subgraph _G[˜] induced by_ _V[˜] along with the nodal features_ **X[˜]** and sensitive attributes ˜s


The overall node sampling scheme is presented in Algorithm 1.

Assuming |S _[ω]| ≥|S_ _[χ]|, all nodes in S_ _[χ]_ are retained, meaning _S[˜]0[χ]_ [=][ S]0[χ] [and][ ˜]S1[χ] [=][ S]1[χ][. While]
subsets of nodes _S[¯]0[ω]_ [and][ ¯]S1[ω] [are randomly sampled from][ S]0[ω] [and][ S]1[ω] [respectively with sample size]

[¯]0 0 1 1
_|S_ _[ω][|][ =][ |S]_ _[χ][|][ and][ |][ ¯]S_ _[ω][|][ =][ |S]_ _[χ][|][.]_

Therefore, in the resulting graph _G[˜], we have_

[˜]0 0 0 (29)
_|S_ _[ω][|][ =][|][ ¯]S_ _[ω][|][ =][ |S]_ _[χ][|]_

[˜]1 1 1 (30)
_|S_ _[ω][|][ =][|][ ¯]S_ _[ω][|][ =][ |S]_ _[χ][|]_

therefore

[˜]0
_S_ _[χ]_ 0

= _|S_ _[χ][|]_ (31)

[˜]0 0 0 0 2
_|S_ _[χ][|][ +][ |][ ˜]S_ _[ω][|]_ _|S_ _[χ][|][ +][ |S]_ _[χ][|][ = 1]_

[˜]1
_S_ _[χ]_ 1

= _|S_ _[χ][|]_ (32)

[˜]1 1 1 1 2
_|S_ _[χ][|][ +][ |][ ˜]S_ _[ω][|]_ _|S_ _[χ][|][ +][ |S]_ _[χ][|][ = 1]_

which leads to γ1 = 0 for the obtained _G[˜]._

A.4 EFFECTS OF GLOBAL EDGE DELETION SCHEME IN EQUATION 7

Given the edge deletion scheme in equation 7, we have

Ep(e) _|E[˜][χ]|_ =π|E _[χ]|_ (33)
h i

Ep(e) h|E[˜]S[ω]0 _[|]i_ = 2[|E]E[χ]S[ω][|]0 _ES[ω]0_ = _[π][|E]2_ _[χ][|]_ (34)

Ep(e) [˜] 1 _[|]_ = _[π][ ×]_ 1 = _[π][|E]_ _[χ][|]_ _._ (35)
h|ES[ω] i 2[|E]E[χ]S[ω][|]1 _ES[ω]_ 2

Therefore, Ep(e) _|E[˜]S[ω]0_ _[|]_ = Ep(e) _|E[˜]S[ω]1_ _[|]_ = [1]2 [E][p][(][e][)] _[π][ ×]|E[˜][χ]|_ and equation 6 holds.
h i h i h i


-----

A.5 DATASET STATISTICS

Dataset statistics are provided in Tables 5 and 6.

Table 5: Dataset statistics for social networks

Dataset _|S0[χ][|]_ _|S0[ω][|]_ _|S1[χ][|]_ _|S1[ω][|]_ _|E_ _[χ]|_ _|ES[ω]0_ _[|]_ _|ES[ω]1_ _[|]_

Pokec-z 622 4229 582 2226 1730 23428 15942
Pokec-n 423 3617 479 1666 1422 18548 10672
UCSD34 2246 118 1697 71 51607 36787 19989
Berkeley13 1619 80 1488 77 27542 19550 13582

Table 6: Dataset statistics for citation networks

Dataset _|V|_ # sensitive attr. _|E_ _[χ]|_ _|E_ _[ω]|_

Cora 2708 7 1428 5964
Citeseer 3327 6 1628 4746
Pubmed 19717 3 12254 49802

A.6 CONTRASTIVE LEARNING OVER GRAPHS

The main goal of contrastive learning is to learn discriminable representations by contrasting the embeddings of positive and negative examples, through minimizing a specific contrastive loss (Opolka
et al., 2019; Veliˇckovi´c et al., 2019; Zhu et al., 2021; 2020). The contrastive loss in the present work
is designed to maximize node-level agreement, meaning that the representations of the same node
generated from different graph views can be discriminated from the embeddings of other nodes. Let
**H[1]** = f (A[1], **X[1]) and H[2]** = f (A[2], **X[2]) denote the nodal embeddings generated with graph views**
_G[1]_ and _G[2], where_ **A[i], and** **X[i]** are the adjacency and feature matrices of _G[i], which are corrupted_
versions of the matrices[e] [e] **A and X[e]** . Let[e] **h[1]i** [and][ h]i[2] [denote the embeddings for][ v][i][: They should be]
more similar to each other than to the embeddings of all other nodes. Hence, the representations ofe [e] [e] [e] [e]
all other nodes are used as negative samples. The contrastive loss for generating embeddings h[1]i [and]
**h[2]i** [(considering][ h]i[1] [as the anchor representation) can be written as]


_i_ _[,][h]i[2][)][/τ]_
_e[s][(][h][1]_
_ℓ_ **h[1]i** _[,][ h]i[2]_ = log (36)
_−_ _e[s][(][h]i[1][,][h]i[2][)][/τ]+[P][N]k=1_ [1][[][k][̸][=][i][]][e][s][(][h]i[1][,][h]k[2][)][/τ]+[P][N]k=1[1][[][k][̸][=][i][]][e][s][(][h]i[1][,][h]k[1][)][/τ]
  

where s **h[1]i** _[,][ h]i[2]_ := c _g(h[1]i_ [)][, g][(][h]i[2][)], with c(·, ·) denoting the cosine similarity between the input vectors, and g( ) representing a nonlinear learnable transform executed by utilizing a 2-layer
   _·_   
multi-layer perceptron (MLP), see also, (Zhu et al., 2021; 2020; Chen et al., 2020b). τ denotes the
temperature parameter, andk ̸= i. This objective can also be written in the symmetric form, when the generated representation 1[k≠ _i] ∈{0, 1} is the indicator function which takes the value 1 when_
**h[2]i** [is considered as the anchor example. Therefore, considering all nodes in the given graph, the]
final loss can be expressed as


**h[1]i** _[,][ h]i[2]_ + ℓ **h[2]i** _[,][ h]i[1]_ _._ (37)
   


_J =_


2N


_i=1_


A.7 HYPERPARAMETERS AND IMPLEMENTATION DETAILS

**Contrastive Learning:** All experimental settings are kept unaltered as presented in our baseline
study GRACE (Zhu et al., 2020) for fair comparison, where in the GNN-based encoder, weights are
initialized utilizing Glorot initialization (Glorot & Bengio, 2010) and ReLU activation is used after
each GCN layer. All models are trained for 400 epochs by employing Adam optimizer (Kingma &
Ba, 2014) together with a learning rate of 5×10[−][4] and ℓ2 weight decay factor of 10[−][5]. The temperature parameter τ in contrastive loss is chosen to be 0.4 and the dimension of the node representations
is selected as 256 for all datasets.


-----

**End-to-end training: All models are trained for 100 epochs by employing Adam optimizer**
(Kingma & Ba, 2014) together with a learning rate of 5 × 10[−][3]. The dimension of the node representations is selected as 128 for all datasets.

**Implementation details: Node representations are obtained using a two-layer GCN (Kipf &**
Welling, 2017) for all contrastive learning baselines, which is kept the same as the one used in
(Zhu et al., 2021; 2020) to ensure fair comparison. After obtaining node embeddings from different schemes, a linear classifier based on ℓ2-regularized logistic regression is applied for the node
classification and link prediction tasks, which is again the same as the scheme used in (Veliˇckovi´c
et al., 2019; Zhu et al., 2021; 2020). In node classification, the classifier is trained on 90% of the
nodes selected through a random split, and the remaining nodes constitute the test set. For each
experiment, results for three random data splits are obtained, and the average together with standard
deviations are presented. In link prediction, 90% of the edges are utilized in the training of GCN
encoder and linear classifier. For each edge, the input to the linear classifier is the concatenated
representations of the nodes that the edge connects. Results for link prediction are obtained for five
different edge splits, the resulting average metrics and standard deviations are provided. Finally, in
the end-to-end link prediction experiments, a two-layer GCN is trained, and results are obtained for
6 different random data splits.

Note that if the sizes of the sets _S[ˆ] and_ _S[˜] are highly unbalanced, we put a limit on the minimum_
of sampling budgets in node sampling to prevent any excessive corruption of the input graph. If
_S_ _[χ]_ _≥S_ _[ω], the minimum limit for each set is the 50% of the initial group size, otherwise the limit is_
the 25% of the initial group size. Furthermore, to avoid overly down sampling the edges, a minimum
value for p[(][e][)](eij), p[(]min[e][)] [, is set for the edge deletion scheme. In the feature masking scheme,][ α][ in]
equation 2 is set to 0 and 0.1 for different graph views used in the contrastive loss (i.e., default
parameter values provided for GCA are used directly). In the edge deletion framework, π = 1, and
_p[(]min[e][)]_ [=][ π]2 [= 0][.][5][ are used in the experiments of both Pokec datasets and citation networks. Such]

a selection is utilized to present the scheme with the most “natural” hyperparameter value, in the
sense that only intra-edges are deleted, where the already sparse inter-edges are left unchanged. We
note that this selection is naturally a good choice for unbalanced data sets in terms of inter- and
intra-edges. For Facebook networks where intra- and inter-edges are balanced for certain sensitive
groups, π = 0.8, and p[(]min[e][)] [is set as][ π]2 [for all experiments.]

Proposed augmentation frameworks have only two hyperparameters: α in feature masking and π in
adaptive edge deletion. While we note the value of α is kept the same for all baselines that utilize a
feature masking scheme (default hyperparameters provided by GCA (Zhu et al., 2021)), and 1 is the
most natural choice for π in Pokec datasets and citation networks, we carry out a sensitivity analysis
for these two hyperparameters in this appendix. In the analysis, for α, the values [0.1, 0.2, 0.3] are
examined for the first graph view, where the α value for the second view is assigned to be higher
than the value of the first one by 0.1. Overall, sensitivity analyses for Pokec and Facebook networks
on the FairAug algorithm are presented in Tables 7 and 8, respectively. In Tables 7 and 8, the first
column shows the α values of the first and second view, respectively.

The sensitivity analysis for α on Pokec networks in Table 7 demonstrates that increased α values
can both improve or degrade the fairness performance together with similar classification accuracies.
However, the resulted fairness performances are observed to be consistently better than our baseline,
GRACE. Furthermore, the results of Table 8 show that fairness performances for the link prediction
task can generally be improved with increased α values, and the performance is quite stable over
different α selections.

Table 7: Sensitivity analysis for α on Pokec networks for FairAug.

Pokec-z Pokec-n

_α_ Accuracy (%) ∆SP (%) ∆EO (%) Accuracy (%) ∆SP (%) ∆EO(%)

0.0/0.1 67.04 ± 0.69 3.29 ± 1.66 3.04 ± 1.37 67.88 ± 0.45 4.81 ± 0.59 6.52 ± 0.99
0.1/0.2 67.90 ± 0.69 3.95 ± 2.09 4.20 ± 2.41 67.60 ± 0.48 4.38 ± 0.45 5.82 ± 1.44
0.2/0.3 67.68 ± 0.74 4.42 ± 1.48 4.46 ± 2.25 68.01 ± 0.36 4.95 ± 0.39 6.44 ± 1.36
0.3/0.4 67.71 ± 0.64 2.99 ± 1.89 3.20 ± 2.57 67.50 ± 0.11 3.38 ± 0.90 4.41 ± 1.12


-----

Table 8: Sensitivity analysis for α on Facebook networks for FairAug

UCSD34 Berkeley13

_α_ AUC (%) ∆SP (%) ∆EO (%) AUC (%) ∆SP (%) ∆EO(%)

0.0/0.1 71.46 ± 0.30 0.71 ± 0.38 1.62 ± 0.62 69.48 ± 0.29 0.70 ± 0.43 4.24 ± 0.90
0.1/0.2 71.54 ± 0.33 0.65 ± 0.13 1.76 ± 0.38 69.50 ± 0.42 0.55 ± 0.22 4.31 ± 0.76
0.2/0.3 71.51 ± 0.30 0.58 ± 0.26 1.50 ± 0.66 69.57 ± 0.46 0.55 ± 0.35 4.15 ± 0.79
0.3/0.4 71.55 ± 0.41 0.49 ± 0.14 1.65 ± 0.50 69.41 ± 0.35 0.97 ± 0.19 4.47 ± 1.10

The sensitivity analysis for _π_ is carried out by examining the values

[0.75, 0.80, 0.85, 0.90, 0.95, 1.00] for FairAug algorithm for both views. The sensitivity analyses for Pokec and Facebook networks on FairAug algorithm are presented in Tables 9 and 10,
respectively. In Tables 9 and 10, π = 1 is the parameter choice utilized to generate the results in
Table 2 and π = 0.8 is the selection for the results in Table 3.

Table 9: Sensitivity analysis for π on Pokec networks for FairAug.

Pokec-z Pokec-n

_π_ Accuracy (%) ∆SP (%) ∆EO (%) Accuracy (%) ∆SP (%) ∆EO(%)

1.00 67.04 ± 0.69 3.29 ± 1.66 3.04 ± 1.37 67.88 ± 0.45 4.81 ± 0.59 6.52 ± 0.99
0.95 67.66 ± 0.57 4.00 ± 1.76 4.37 ± 1.82 68.01 ± 0.53 5.66 ± 1.27 7.00 ± 0.72
0.90 67.14 ± 0.73 3.28 ± 1.97 3.22 ± 2.20 67.81 ± 0.33 5.22 ± 0.82 6.87 ± 1.22
0.85 66.96 ± 0.77 3.49 ± 2.28 3.52 ± 2.15 68.00 ± 0.29 5.66 ± 0.29 7.77 ± 1.01
0.80 67.87 ± 0.35 3.59 ± 2.33 4.30 ± 1.95 68.17 ± 0.28 5.61 ± 0.73 7.82 ± 1.24
0.75 67.74 ± 0.28 3.94 ± 2.48 4.58 ± 1.91 67.95 ± 0.10 5.69 ± 0.27 7.44 ± 1.08

Table 10: Sensitivity analysis for π on Facebook networks for FairAug

UCSD34 Berkeley13

_π_ AUC (%) ∆SP (%) ∆EO (%) AUC (%) ∆SP (%) ∆EO(%)

0.80 71.46 ± 0.30 0.71 ± 0.38 1.62 ± 0.62 69.48 ± 0.29 0.70 ± 0.43 4.24 ± 0.90
1.00 71.30 ± 0.41 0.56 ± 0.47 1.41 ± 0.57 69.70 ± 0.32 0.74 ± 0.36 4.35 ± 1.11
0.95 71.37 ± 0.41 0.60 ± 0.30 1.48 ± 0.69 69.52 ± 0.42 0.66 ± 0.16 3.99 ± 0.84
0.90 71.40 ± 0.38 0.59 ± 0.34 1.45 ± 0.44 69.48 ± 0.31 0.86 ± 0.72 4.64 ± 1.37
0.85 71.39 ± 0.36 0.65 ± 0.28 1.38 ± 0.54 69.49 ± 0.41 0.70 ± 0.40 4.19 ± 0.84
0.75 71.47 ± 0.23 0.63 ± 0.37 1.55 ± 0.65 69.52 ± 0.43 0.74 ± 0.48 4.31 ± 1.19

The sensitivity analysis for π on Pokec networks shows that π = 1 results in the best performances
in terms of fairness. However, we note that the fairness results for the remaining π values also
outperform our baseline, GRACE, together with similar node classification accuracies. Furthermore,
the results of Table 10 demonstrate that the presented results for link prediction in Table 3 can
indeed be improved with a grid search, as better fairness performances can be obtained with π ̸=
0.8. Moreover, the results for different π values vary less for link prediction compared to node
classification.

A.8 EFFECTS OF PROPOSED AUGMENTATIONS ON γ1 AND γ2

Table 11: Effects of proposed augmentations on γ2

Original Node Sampling Edge Deletion Edge Addition
Pokec-z 0.90 0.59 0.87 0.77
Pokec-n 0.91 0.62 0.89 0.81
UCSD34 0.18 0.45 0.03 0.11
Berkeley13 0.18 0.43 0.03 0.06

Tables 11 and 12 present the effects of the proposed framework on γ values for different datasets. In
Table 11, the effect of each augmentation step on γ2 is considered independently. Table 12 demonstrates the effect of augmentations sequentially in the overall algorithm in a cumulative manner
(e.g., the “Edge Deletion” column implies both Node Sampling and Edge Deletion are applied).
Both Tables 11 and 12 demonstrate that even though the edge deletion/addition schemes are not


-----

Table 12: Effects of each step in FairAug

Original Graph Node Sampling Edge Deletion Edge Addition

Pokec-z _γ1_ 0.66 0.11 0.11 0.11
_γ2_ 0.90 0.59 0.50 0.43

Pokec-n _γ1_ 0.67 0.15 0.15 0.15
_γ2_ 0.91 0.62 0.55 0.50

UCSD34 _γ1_ 0.91 0.86 0.86 0.86
_γ2_ 0.18 0.45 0.17 0.17

Berkeley13 _γ1_ 0.90 0.83 0.83 0.83
_γ2_ 0.18 0.43 0.17 0.17

directly designed based on γ2, the proposed approaches indeed reduce the values of it. In addition,
presented γ values can also help to explain the ineffectiveness of edge deletion on Pokec networks.
Due to the inter-edge sparsity and overly unbalanced structure of these datasets in terms of |E _[χ]|_
and |E _[ω]| (presented in Table 5 of Appendix A.5), node sampling becomes a better choice than_
edge deletion/addition to reduce the bias via an augmentation on the graph topology. Table 11 also
shows the better effectiveness of node sampling strategy in reducing γ2 compared to edge deletion/addition frameworks, which also supports the findings of Table 2, that is, node sampling is an
essential step of FairAug on the Pokec networks. Furthermore, γ values can also help to explain the
random/detrimental effect of node sampling for Facebook networks, which is presented in Table 3.
Tables 11 and 12 show that while node sampling is not very effective in reducing γ1, it is observed to
increase γ2 for these networks. This effect results from the graph topologies of Facebook networks
having |S _[χ]| >> |S_ _[ω]|. Note that γ1 values are not 0 after node sampling due to the limit on the_
minimum of sampling budgets, see Appendix A.7.

A.9 CASE STUDIES FOR THE EFFECTS OF PROPOSED AUGMENTATIONS

To demonstrate the effects of the proposed augmentation schemes in a more intuitive manner on
different graph structures, we consider two small toy examples. The topology in Figure 1a is inspired
by the topology of the Pokec networks (few inter-edges, many more intra-edges, i.e., |E _[ω]| > |E_ _[χ]|),_
whereas Figure 1b is motivated by the Facebook networks (few nodes with no inter-edges, many
more that have at least one, i.e., |S _[χ]| > |S_ _[ω]|). For demonstrative purposes, assume the nodes in_
both graphs share the same nodal features, with the corresponding feature matrix X equal to


0.0 0.1 _−0.3_ 0.1 _−0.1_
0.2 _−0.2_ _−0.2_ 0.1 0.1
0.1 0.2 0.0 0.2 0.0
0.2 0.1 _−0.2_ 0.1 0.2
0.1 _−0.1_ _−0.1_ 0.1 _−0.2_
_−0.1_ _−0.1_ 0.3 _−0.2_ 0.3
_−0.2_ 0.1 0.4 _−0.1_ _−0.1_
_−0.3_ _−0.1_ 0.1 _−0.3_ _−0.2_


**X =**


**Feature masking:** For the given X, **_δ¯ utilized in adaptive feature masking equals to_**

[0.74, 0.12, 1.00, 0.74, 0.00]. Therefore, in a feature masking scheme where 40% of the features
are masked, the most probable augmented **X[˜]** becomes[2]

0.0 0.2 0.0 0.0 _−0.1_
0.0 0.1 0.0 0.1 _−0.1_

0.0 _−0.2_ 0.0 0.1 0.1 

0.0 0.2 0.0 0.2 0.0

**X˜** = 0.0 0.1 0.0 0.1 0.2  _._

 
0.0 0.1 0.0 0.1 0.2
 _−_ _−_ 
0.0 0.1 0.0 0.2 0.3 
0.0 _−0.1_ 0.0 _−0.1_ 0.1
 
0.0 0.1 0.0 _−0.3_ _−0.2_
 _−_ _−_ _−_ 
 

2 Note that the feature masking process is stochastic. We present the mask with the largest probability of
occurrence, for demonstrative purposes.


-----

7 G2

## G1 7

4

8

1

3 6

2

5

## G2 7

4

8

1

3 6

2

5


intra-edges
inter-edges

(a) Case one.

Figure 1: Toy examples


s=0
s=1


s=0 intra-edges
s=1 inter-edges

(b) Case two.


7

4

8

2

5


7

4

8

1

2

5


s=0(a) Case one after NS.intra-edges s=0(b) Case two after NS.intra-edges
s=1 inter-edges s=1 inter-edges

Figure 2: Augmented graphs via node sampling

For **X˜**, (µ1 **_µ0)_** in equation 15 becomes [0, 0.05, 0, 0.32, 0] where it was

[−0.32, −0.05, − 0.43, −0.32, 0] for the original graph. Note that this is the best case scenario − _−_
given the 40% masking budget, with the maximal decrease in **_µ1_** **_µ0_** 1. As also mentioned
in Footnote 2, the actual feature masking process is stochastic. ∥ _−By assigning larger masking∥_
probabilities to the features with high deviations betweeen S0 and S1, the non-uniform masking
strategy proposed herein increases the chances of obtaining the “better” cases with larger reductions
in (µ1 **_µ0)._**
_−_

**Node sampling: The results in Table 3 demonstrate that node sampling becomes effective in graphs**
where |S _[ω]| > |S_ _[χ]|. Here, we visualize this finding through the toy examples presented in Figures_
1a and 1b. Note that the two toy graphs differ in their sets S _[ω]_ and S _[χ]. Specifically, we have_
_|S_ _[ω]| > |S_ _[χ]| for Figure 1a and |S_ _[ω]| < |S_ _[χ]| for Figure 1b._

after node sampling for two different graph examplesHere, the exact ∥ϵ∥1 = ∥ _|S11|_ Pvn∈S1 **[z][n][ −]** _|S10|_ Pv Gm∈1S and0 **[z][m] G[∥][1]2[ values are calculated before and]. Note that zi =** _d1i_ _vj_ _∈N (i)_ **[x][j][,]**

and the original nodal features X are utilized to calculate the zi vectors.

P

_Case 1: In the first example graph G1,_ 0 0 1 1
the proposed node sampling scheme selects |S _[χ] 2[|][ = 2] nodes from the set[,][ |S]_ _[ω][|][ = 3][,][ |S] S[χ][|]0[ω][ = 1][, and][, and][ 1][ node from the set][ |S]_ _[ω][|][ = 2][. Therefore,][ S]1[ω][. A]_
possible augmented graph is presented in Figure 2a on which the examination is carried over. While
the original ∥ϵ∥1 equals to 0.865 for G1, ∥ϵ∥1 becomes 0.771 for the output topology of the node
sampling provided in Figure 2a, which shows the efficacy of the proposed framework.

_Case 2: For G2,_ 0 0 1 1
sampling scheme selects |S _[χ][|][ = 4] 2 nodes from the set[,][ |S]_ _[ω][|][ = 1][,][ |S]_ _[χ][|][ = 2] S0[χ]_ [, and][(minimum limit for each set is the][ |S] _[ω][|][ = 1][. Therefore, the proposed node][ 50%][ of the]_


-----

7

4

8

1

3 6

2

5


7

4

8

1

3 6

2

5


s=0
s=1


(a) Case one.intra-edges s=0 (b) Case two.intra-edges

inter-edges s=1 inter-edges

Figure 3: Augmented graphs via edge deletion


initial group size), and 1 node from the set S1[χ][. Therefore, the presented example in Figure 2b is a]
potential augmented topology obtained after the proposed node sampling framework. For this case,
the original ∥ϵ∥1 = 0.558 which becomes 0.892 for the augmented graph presented in Figure 2b.
This case study clearly illustrates that the proposed node sampling scheme is mainly effective for
graphs where |S _[ω]| > |S_ _[χ]|._

**Edge deletion: Results in Table 2 show that node sampling is a better choice than edge manipu-**
lations for graphs with inter-edge sparsity and |E _[ω]| ≫|E_ _[χ]|. Again, two case studies based on the_
graphs provided in Figure 1 are utilized to demonstrate the effects of the proposed adaptive edge
deletion scheme. Specifically, the resulted ∥ϵ∥1 = ∥ _|S11|_ _vn∈S1_ **[z][n][ −]** _|S10|_ _vm∈S0_ **[z][m][∥][1][ value is]**

examined before and after the application of edge deletion.

P P

_Case 1: In the first example graph G1,_ = 2, 0 _[|][ = 6][, and][ |E]_ _[ω]1_ _[|][ = 3][. Therefore, edge deletion]_
_|E_ _[χ]|_ _|ES[ω]_ _S_
probability of the adaptive edge deletion framework is 0.5 for the edges in ES[ω]0 [, and][ E]S[ω]0 [with the]

hyperparameter selections π = 1, and p[(]min[e][)] [=][ π]2 [= 0][.][5][. For such a scheme, a possible augmented]

graph is presented in Figure 3a for demonstrative purposes. While the value of ∥ϵ∥1 is reduced from
0.865 to 0.792 with the application of adaptive edge deletion, the graph in Figure 3a also becomes
disconnected. For certain real networks, such as the Pokec networks considered in this paper, the
imbalance between intra- and inter-edges is considerably larger than the imbalance in the graph in
Figure 1a. Therefore, the sparsity resulting from the adaptive edge deletion can be even more severe
for such graphs than this toy example. Thus, this case study also confirms that node sampling can
indeed be a better choice than edge deletion for graphs with inter-edge sparsity and |E _[ω]| >> |E_ _[χ]|._

_Case 2: For G2, we have_ = 4, 0 _[|][ = 4][, and][ |E]_ _[ω]1_ _[|][ = 2][, which shows a more balanced structure]_
_|E_ _[χ]|_ _|ES[ω]_ _S_
than the graph utilized in Case 1. For the given graph connectivity, edge deletion probability of
adaptive edge deletion scheme is 0.5 for the edges in ES[ω]0 [, and][ 0][ for the edges in][ E]S[ω]0 [together with]

the hyperparameter selections π = 1, and p[(]min[e][)] [=][ π]2 [= 0][.][5][. The presented example in Figure 3b]

is a potential resulting augmented topology obtained with the proposed edge deletion framework,
presented for demonstrative purposes. For this outcome, the original ∥ϵ∥1 is decreased from 0.558
to 0.497 for the augmented graph presented in Figure 3b, which demonstrates the effectiveness of
the proposed augmentation for the input graph in Case 2.

**Edge Addition: We also present the effects of the proposed edge addition scheme on the toy ex-**
amples for a better illustration of the augmentation. Again, the exact1 _∥ϵ∥1 = ∥_ _|S11|_ _vn∈S1_ **[z][n][ −]**

_|S0|_ _vm∈S0_ **[z][m][∥][1][ values are calculated before and after edge addition for two different graph ex-]P**

amples G1 and G2.

P

_Case 1: For G1, as_ = 9 and = 2, new seven artificial inter-edges are generated for the
_|E_ _[ω]|_ _|E_ _[χ]|_
proposed edge addition scheme. For such a scheme, a possible augmented graph is presented in
Figure 4a, where ∥ϵ∥1 is reduced from 0.865 to 0.111 with the application of edge addition.

_Case 2: For G2, as_ = 6 and = 4, therefore two artificial inter-edges should be created for
_|E_ _[ω]|_ _|E_ _[χ]|_
the proposed edge addition scheme. A possible augmented graph resulting from the proposed edge


-----

7

4

8

1

3 6

2

5


7
4

8

1

3 6

2

5


s=0
s=1


(a) Case one.intra-edges s=0 (b) Case two.intra-edges

Figure 4: Augmented graphs via edge addition


addition is presented in Figure 4b. For the presented example in Figure 4b, edge addition can lower
the value of ∥ϵ∥1 from 0.558 to 0.293.

A.10 OPTIMAL TOPOLOGY AUGMENTATION SCHEMES FOR γ1 AND γ2

While Algorithm 1 can make γ1 = 0 for graphs satisfying |S _[ω]| ≥|S_ _[χ]|, as Table 12 demonstrates,_
the obtained γ1 values after node sampling are not exactly zero due to the limit on the number of
nodes that can be sampled. If |S _[χ]| ≥|S_ _[ω]|, the minimum number of nodes that will be sampled_
for each set is 50% of the initial group size, otherwise the limit is 25% of the initial group size.
Such limits are employed to prevent the generation of excessively small subgraphs, which can prevent proper learning and create stability issues. Furthermore, proposed global edge deletion/addition
frameworks cannot directly make γ2 zero, as the reduction of γ2 to zero requires a node-wise consideration. Since such an independent consideration of each node can incur a significant computational
complexity for large graphs, we stay within the realm of global schemes that can still help to reduce
the value of γ2. However, the examination of the optimal schemes that can make γ1 γ2 zero helps
provide important insights and justification for the proposed augmentations.

Motivated by this, we examined “optimal” strategies which can drive γ1 or γ2 to zero. The corresponding node sampling strategy copes with each node independently by deleting/adding edges so
that d[ω]i [=][ d]i[χ][,][ ∀][v][i][ ∈V][. Different from the proposed global edge manipulation scheme in Section]
3.3.3, such node-level edge deletion/addition frameworks can change the value of γ1 and henceforth
interfere with the effects of the node sampling step. Additionally, the employment of edge deletion
for the graph makes the utilization of edge addition unnecessary and vice versa, as both schemes
seek to achieve the same goal, i.e., d[ω]i [=][ d]i[χ][,][ ∀][v][i][ ∈V][. Therefore, only the “optimal” edge dele-]
tion or addition scheme is employed. We would like to note that the investigation is carried out
for Pokec networks, as |S _[ω]| ≤|S_ _[χ]| for Facebook networks, which makes the design of an optimal_
node sampling scheme challenging due to the unpredictable increase in S _[ω]_ and the unpredictable
decrease in S _[χ], when sampling from the set S_ _[χ]_ (some of the excluded nodes and edges may be the
only inter-edges for the nodes in the sampled graph, resulting in a further increase in S _[ω]_ and further
decrease in S _[χ])._

Table 13: Effects of optimal augmentations on γ1

Original Node Sampling Edge Deletion Edge Addition
Pokec-z 0.66 0.00 0.67 1.00
Pokec-n 0.67 0.00 0.67 1.00

Table 14: Effects of optimal augmentations on γ2

Original Node Sampling Edge Deletion Edge Addition
Pokec-z 0.90 0.48 0.00 0.00
Pokec-n 0.91 0.46 0.00 0.00


-----

Table 15: Effects of each step in Node Sampling + Edge Deletion

Original Graph Node Sampling Edge Deletion

Pokec-z _γ1_ 0.66 0.00 0.08
_γ2_ 0.90 0.48 0.00

Pokec-n _γ1_ 0.67 0.00 0.11
_γ2_ 0.91 0.46 0.00

Table 16: Effects of each step in Node Sampling + Edge Addition

Original Graph Node Sampling Edge Addition

Pokec-z _γ1_ 0.66 0.00 0.73
_γ2_ 0.90 0.48 0.00

Pokec-n _γ1_ 0.67 0.00 0.70
_γ2_ 0.91 0.46 0.00

Tables 13-16 demonstrate that the utilized node sampling, edge deletion/addition frameworks are
optimal in the sense that they reduce γ1 and γ2 to zero, respectively. However, presented γ1 and
_γ2 values also show the interference of optimal edge manipulations on γ1, as γ1 increases after the_
application of edge deletion/addition following node sampling. Specifically, edge addition has a
great impact on it.


-----

