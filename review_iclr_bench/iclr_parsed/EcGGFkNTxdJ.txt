## TRUST REGION POLICY OPTIMISATION IN MULTI-AGENT REINFORCEMENT LEARNING

**Jakub Grudzien Kuba[1][,][2][∗], Ruiqing Chen[3][,][∗], Muning Wen[4], Ying Wen[4],**
**Fanglei Sun[3], Jun Wang[5], Yaodong Yang[6][,][†]**

1University of Oxford, 2Huawei R&D UK, 3ShanghaiTech University,
4Shanghai Jiao Tong University 5University College London
6Institute for AI, Peking University & BIGAI
_†Corresponding to: yaodong.yang@pku.edu.cn_

ABSTRACT

Trust region methods rigorously enabled reinforcement learning (RL) agents to
learn monotonically improving policies, leading to superior performance on a variety of tasks. Unfortunately, when it comes to multi-agent reinforcement learning
(MARL), the property of monotonic improvement may not simply apply; this is
because agents, even in cooperative games, could have conflicting directions of
policy updates. As a result, achieving a guaranteed improvement on the joint
policy where each agent acts individually remains an open challenge. In this paper, we extend the theory of trust region learning to cooperative MARL. Central
to our findings are the multi-agent advantage decomposition lemma and the se_quential policy update scheme. Based on these, we develop Heterogeneous-Agent_
_Trust Region Policy Optimisation (HATPRO) and Heterogeneous-Agent Proxi-_
_mal Policy Optimisation (HAPPO) algorithms. Unlike many existing MARL al-_
gorithms, HATRPO/HAPPO do not need agents to share parameters, nor do they
need any restrictive assumptions on decomposibility of the joint value function.
Most importantly, we justify in theory the monotonic improvement property of
HATRPO/HAPPO. We evaluate the proposed methods on a series of Multi-Agent
MuJoCo and StarCraftII tasks. Results show that HATRPO and HAPPO significantly outperform strong baselines such as IPPO, MAPPO and MADDPG on all
tested tasks, thereby establishing a new state of the art.

1 INTRODUCTION

Policy gradient (PG) methods have played a major role in recent developments of reinforcement
learning (RL) algorithms (Silver et al., 2014; Schulman et al., 2015a; Haarnoja et al., 2018). Among
the many PG variants, trust region learning (Kakade & Langford, 2002), with two typical embodiments of Trust Region Policy Optimisation (TRPO) (Schulman et al., 2015a) and Proximal Policy
_Optimisation (PPO) (Schulman et al., 2017) algorithms, offer supreme empirical performance in_
both discrete and continuous RL problems (Duan et al., 2016; Mahmood et al., 2018). The effectiveness of trust region methods largely stems from their theoretically-justified policy iteration
procedure. By optimising the policy within a trustable neighbourhood of the current policy, thus
avoiding making aggressive updates towards risky directions, trust region learning enjoys the guarantee of monotonic performance improvement at every iteration.

In multi-agent reinforcement learning (MARL) settings (Yang & Wang, 2020), naively applying
policy gradient methods by considering other agents as a part of the environment can lose its effectiveness. This is intuitively clear: once a learning agent updates its policy, so do its opponents;
this however changes the loss landscape of the learning agent, thus harming the improvement effect
from the PG update. As a result, applying independent PG updates in MARL offers poor convergence property (Claus & Boutilier, 1998). To address this, a learning paradigm named centralised
_training with decentralised execution (CTDE) (Lowe et al., 2017b; Foerster et al., 2018; Zhou et al.,_
2021) was developed. In CTDE, each agent is equipped with a joint value function which, during

_∗First two authors contribute equally._ [Code is available at https://github.com/PKU-MARL/](https://github.com/PKU-MARL/TRPO-PPO-in-MARL)
[TRPO-PPO-in-MARL.](https://github.com/PKU-MARL/TRPO-PPO-in-MARL)


-----

training, has access to the global state and opponents’ actions. With the help of the centralised value
function that accounts for the non-stationarity caused by others, each agent adapts its policy parameters accordingly. As such, the CTDE paradigm allows a straightforward extension of single-agent
PG theorems (Sutton et al., 2000; Silver et al., 2014) to multi-agent scenarios (Lowe et al., 2017b;
Kuba et al., 2021; Mguni et al., 2021). Consequently, fruitful multi-agent policy gradient algorithms
have been developed (Foerster et al., 2018; Peng et al., 2017; Zhang et al., 2020; Wen et al., 2018;
2020; Yang et al., 2018).

Unfortunately, existing CTDE methods offer no solution of how to perform trust region learning in
MARL. Lack of such an extension impedes agents from learning monotonically improving policies
in a stable manner. Recent attempts such as IPPO (de Witt et al., 2020a) and MAPPO (Yu et al.,
2021) have been proposed to fill such a gap; however, these methods are designed for agents that are
_homogeneous (i.e., sharing the same action space and policy parameters), which largely limits their_
applicability and potentially harm the performance. As we show in Proposition 1 later, parameter
sharing could suffer from an exponentially-worse suboptimal outcome. On the other hand, although
IPPO/MAPPO can be practically applied in a non-parameter sharing way, it still lacks the essential
theoretical property of trust region learning, which is the monotonic improvement guarantee.

In this paper, we propose the first theoretically-justified trust region learning framework in MARL.
The key to our findings are the multi-agent advantage decomposition lemma and the sequential pol_icy update scheme. With the advantage decomposition lemma, we introduce a multi-agent policy_
iteration procedure that enjoys the monotonic improvement guarantee. To implement such a procedure, we propose two practical algorithms: Heterogeneous-Agent Trust Region Policy Optimisation
(HATRPO) and Heterogeneous-Agent Proximal Policy Optimisation (HAPPO). HATRPO/HAPPO
adopts the sequential update scheme, which saves the cost of maintaining a centralised critic for
each agent in CTDE. Importantly, HATRPO/HAPPO does not require homogeneity of agents, nor
any other restrictive assumptions on the decomposibility of the joint Q-function (Rashid et al., 2018).
We evaluate HATRPO and HAPPO on benchmarks of StarCraftII and Multi-Agent MuJoCo against
strong baselines such as MADDPG (Lowe et al., 2017a), IPPO (de Witt et al., 2020b) and MAPPO
(Yu et al., 2021); results clearly demonstrate its state-of-the-art performance across all tested tasks.

2 PRELIMINARIES

In this section, we first introduce problem formulation and notations for MARL, and then briefly
review trust region learning in RL and discuss the difficulty of extending it to MARL. We end by
surveying existing MARL work that relates to trust region methods and show their limitations.

2.1 COOPERATIVE MARL PROBLEM FORMULATION AND NOTATIONS

We consider a Markov game (Littman, 1994), which is defined by a tuple ⟨N _, S, A, P, r, γ⟩. Here,_
= 1, . . ., n denotes the set of agents, is the finite state space, = _i=1_
_Nof finite action spaces of all agents, known as the joint action space, {_ _}_ _S_ _P A : S × A × S →[A][i][ is the product][0, 1] is the_
transition probability function, r : R is the reward function, and γ [0, 1) is the discount
_S × A →_ _∈[Q][n]_
factor. The agents interact with the environment according to the following protocol: at time step t ∈
N, the agents are at state st ; every agent i takes an action a[i]t
which together with other agents’ actions gives a joint action ∈S **at = ([∈A]a[i][1]t[, drawn from its policy][, . . .,][ a][n]t** [)][ ∈] **_[A][, drawn from the][ π][i][(][·|][s][t][)][,]_**
joint policy π(·|st) = _i=1_ _[π][i][(][·][i][|][s][t][)][; the agents receive a joint reward r][t][ =][ r][(][s][t][,][ a][t][)][ ∈]_ [R][, and move]
to a state st+1 with probability P (st+1|st, at). The joint policy π, the transition probabililty function
_P_, and the initial state distribution ρ[0], induce a marginal state distribution at time t, denoted by ρ[t]π[.]

[Q][n]
We define an (improper) marginal state distribution ρπ ≜ [P]t[∞]=0 _[γ][t][ρ]π[t]_ [. The state value function]
and the state-action value function are defined: Vπ(s) ≜ Ea0:∞∼π,s1:∞∼P _∞t=0_ _[γ][t][r][t]_ s0 = s and
_Qπ(s, a) ≜_ Es1:∞∼P,a1:∞∼π _∞t=0_ _[γ][t][r][t]_ s0 = s, a0 = a . The advantage function is written P 
as Aπ(s, a) ≜ _Qπ(s, a)_ _Vπ(s). In this paper, we consider a fully-cooperative setting where all_
_−_  P 
agents share the same reward function, aiming to maximise the expected total reward:


_∞_

_γ[t]rt_

" _t=0_
X


_J(π) ≜_ Es0:∞∼ρ0:π _∞,a0:∞∼π_


Throughout this paper, we pay close attention to the contribution to performance from different
subsets of agents. Before proceeding to our methods, we introduce following novel definitions.


-----

**Definition 1. Let i1:m denote an ordered subset {i1, . . ., im} of N** _, and let −i1:m refer to its com-_
_plement. We write ik when we refer to the k[th]_ _agent in the ordered subset. Correspondingly, the_
_multi-agent state-action value function is defined as_

_Q[i]π[1:][m]_ _s, a[i][1:][m]_ [] ≜ Ea−i1:m ∼π−i1:m _Qπ_ _s, a[i][1:][m]_ _, a[−][i][1:][m]_ [i] _,_
 h 

_and for disjoint sets j1:k and i1:m, the multi-agent advantage function is_

_A[i]π[1:][m]_ _s, a[j][1:][k]_ _, a[i][1:][m]_ [] ≜ _Q[j]π[1:][k][,i][1:][m]_ _s, a[j][1:][k]_ _, a[i][1:][m]_ [] _−_ _Q[j]π[1:][k]_ _s, a[j][1:][k]_ [] _._ (1)
  

Hereafter, the joint policies π = (π[1], . . ., π[n]) and ¯π = (¯π[1], . . ., ¯π[n]) shall be thought of as the
“current”, and the “new” joint policy that agents update towards, respectively.

2.2 TRUST REGION ALGORITHMS IN REINFORCEMENT LEARNING

Trust region methods such as TRPO (Schulman et al., 2015a) were proposed in single-agent RL
with an aim of achieving a monotonic improvement of J(π) at each iteration. Formally, it can be
described by the following theorem.
**Theorem 1. (Schulman et al., 2015a, Theorem 1) Let π be the current policy and ¯π be**
_the next candidate policy._ _We define Lπ(¯π) = J(π) + Es_ _ρπ,a_ _π¯_ [[][A]π[(][s, a][)]][,][ D][max]KL [(][π,][ ¯]π) =
_∼_ _∼_
maxs DKL (π(·|s), ¯π(·|s)) . Then the inequality of

_J(¯π)_ _Lπ(¯π)_ _CD[max]KL_ _π, ¯π_ (2)
_≥_ _−_

_holds, where C =_ [4][γ][ max](1[s,a]−[ |]γ[A])[2][π][(][s,a][)][|] _._   

The above theorem states that as the distance between the current policy π and a candidate policy ¯π
decreases, the surrogate Lπ(¯π), which involves only the current policy’s state distribution, becomes
an increasingly accurate estimate of the actual performance metric J(¯π). Based on this theorem, an
iterative trust region algorithm is derived; at iteration k + 1, the agent updates its policy by


_Lπk_ (π) − _CD[max]KL_ [(][π][k][, π][)]


_πk+1 = arg max_


Such an update guarantees a monotonic improvement of the policy, i.e., J(πk+1) _J(πk). To im-_
_≥_
plement this procedure in practical settings with parameterised policies πθ, Schulman et al. (2015a)
approximated the KL-penalty with a KL-constraint, which gave rise to the TRPO update of

_θk+1 = arg maxθ_ _Lπθk (πθ),_ subject to Es∼ρπθk [DKL(πθk _, πθ)] ≤_ _δ._ (3)

At each iteration k + 1, TRPO constructs a KL-ball Bδ(πθk ) around the policy πθk, and optimises
_πθk+1 ∈_ Bδ(πθk ) to maximise Lπθk (πθ). By Theorem 1, we know that the surrogate objective
_Lπθk (πθ) is close to the true reward J(πθ) within Bδ(πθk_ ); therefore, πθk leads to improvement.
Furthermore, to save the cost on Es _ρ_ DKL(πθk _, πθ)] when computing Equation (3), Schulman_
_∼_ _πθk [_
et al. (2017) proposed an approximation solution to TRPO that uses only first order derivatives,
known as PPO. PPO optimises the policy parameter θk+1 by maximising the PPO-clip objective of

_L[PPO]πθk_ [(][π]θ[) =][ E]s∼ρπθk,a∼πθk min _ππθθk((aa|ss))_ _[A][π][θk][ (][s][,][ a][)][,][ clip]_ _ππθθk((aa|ss))_ _[,][ 1][ ±][ ϵ]_ _Aπθk (s, a)_ _._ (4)

  _|_  _|_  

The clip operator replaces the ratio _ππθkθ_ ( (aa||ss)) [with][ 1+] _[ϵ][ or][ 1]_ _[−]_ _[ϵ][, depending on whether or not the ratio]_

is beyond the threshold interval. This effectively enables PPO to control the size of policy updates.

2.3 LIMITATIONS OF EXISTING TRUST REGION METHODS IN MARL

Extending trust region methods to MARL is highly non-trivial. One naive approach is to equip all
agents with one shared set of parameters and use agents’ aggregated trajectories to conduct policy
optimisation at every iteration. This approach was adopted by MAPPO (Yu et al., 2021) in which
the policy parameter θ is optimised by maximising the objective of


_i_ _i_
min _ππθθk((aa[i]|ss))_ _[A][π][θk][ (][s][,][ a][)][,][ clip]_ _ππθθk((aa[i]|ss))_ _[,][ 1][ ±][ ϵ]_ _Aπθk (s, a)_ _._
 _|_  _|_  

(5)


_L[MAPPO]πθk_ (πθ) =


Es∼ρπθk,a∼πθk
_i=1_

X


-----

Unfortunately, this simple approach has significant drawbacks. An obvious demerit is that parameter
sharing requires that all agents have identical action spaces, i.e., A[i] = A[j], ∀i, j ∈N, which limits
the class of MARL problems to solve. Importantly, enforcing parameter sharing is equivalent to
putting a constraint θ[i] = θ[j], ∀i, j ∈N on the joint policy space. In principle, this can lead to a
suboptimal solution. To elaborate, we demonstrate through an example in the following proposition.
**Proposition 1. Let’s consider a fully-cooperative game with an even number of agents n, one state,**
_and the joint action space {0, 1}[n], where the reward is given by r(0[n/][2], 1[n/][2]) = r(1[n/][2], 0[n/][2]) = 1,_
_and r(a[1:][n]) = 0 for all other joint actions. Let J_ _[∗]_ _be the optimal joint reward, and Jshare[∗]_ _[be the]_
_optimal joint reward under the shared policy constraint. Then_
_Jshare[∗]_ = [2]

_J_ _[∗]_ 2[n][ .]

For proof see Appendix B. In the above example, we show that parameter sharing can lead to a
suboptimal outcome that is exponentially worse with the increasing number of agents. We also
provide an empirical verification of this proposition in Appendix F.

Apart from parameter sharing, a more general approach to extend trust region methods in
MARL is to endow all agents with their own parameters, and at each iteration k + 1, agents
construct trust regions of {Bδ(πθ[i] _k[i]_ [)][}][i][∈N] [, and optimise their objectives][ {][L][π][θ][k][ (][π]θ[i] _[i]_ **_[π]θ[−]k[−][i]_** _[i]_ )}i∈N .

Admittedly, this approach can still be supported by the current

MAPPO & IPPO & HAPPOHATRPO MAPPO implementation (Yu et al., 2021) if one turns off pa
rameter sharing, thus distributing the summation in Equation
(5) to all agents. However, such an approach cannot offer a
rigorous guarantee of monotonic improvement during training.
In fact, agents’ local improvements in performance can jointly
lead to a worse outcome. For example, in Figure 1, we design a
single-state differential game where two agents draw their actions from Gaussian distributions with learnable means µ[1], µ[2]

MAPPO & IPPO & HAPPOHATRPO

Figure 1: Example of a two- and unit variance, and the reward function is r(a[1], a[2]) = a[1]a[2].
player differentiable game with

The failure of MAPPO-style approach comes from the fact

_r(a[1], a[2]) = a[1]a[2]._ We initialise

that, although the reward function increases in each of the

two Gaussian policies with µ[1] =

agents’ (one-dimensional) update directions, it decreases in the

0.25, µ[2] = 0.25. The purple
_−_ joint (two-dimensional) update direction.
intervals represent the KL-ball of
_δ = 0.5. Individual trust region_ Having seen the limitations of existing trust region methods in
updates (red) decrease the joint re- MARL, in the following sections, we first introduce a multiturn, whereas our sequential up- agent policy iteration procedure that enjoys theoreticallydate (blue) leads to improvement. justified monotonic improvement guarantee. To implement

this procedure, we propose HATRPO and HAPPO algorithms,
which offer practical solutions to apply trust region learning in MARL without the necessity of assuming homogeneous agents while still maintaining the monotonic improvement property.

3 MULTI-AGENT TRUST REGION LEARNING

The purpose of this section is to develop a theoretically-justified trust region learning procedure in
the context of multi-agent learning. In Subsection 3.1, we present the policy iteration procedure with
monotonic improvement guarantee, and in Subsection 3.2, we analyse its properties during training
and at convergence. Throughout the work, we make the following regularity assumptions.
**Assumption 1. There exists η ∈** R, such that 0 < η ≪ 1, and for every agent i ∈N _, the policy_
_space Π[i]_ _is η-soft; that means that for every π[i]_ _∈_ Π[i], s ∈S, and a[i] _∈A[i], we have π[i](a[i]|s) ≥_ _η._

3.1 TRUST REGION LEARNING IN MARL WITH MONOTONIC IMPROVEMENT GUARANTEE

We start by introducing a pivotal lemma which shows that the joint advantage function can be decomposed into a summation of each agent’s local advantages. Importantly, this lemma offers a
critical intuition behind the sequential policy-update scheme that our algorithms later apply.
**Lemma 1 (Multi-Agent Advantage Decomposition). In any cooperative Markov games, given a**
_joint policy π, for any state s, and any agent subset i1:m, the below equations holds._

_m_

_A[i]π[1:][m]_ _s, a[i][1:][m]_ [] = _A[i]π[j]_ _s, a[i][1:][j][−][1]_ _, a[i][j]_ [] _._

_j=1_

  X  


-----

For proof see Appendix C.2. Notably, Lemma 1 holds in general for cooperative Markov games,
with no need for any assumptions on the decomposibility of the joint value function such as those in
VDN (Sunehag et al., 2018), QMIX (Rashid et al., 2018) or Q-DPP (Yang et al., 2020).

Lemma 1 indicates an effective approach to search for the direction of performance improvement
(i.e., joint actions with positive advantage values) in multi-agent learning. Specifically, let agents
take actions sequentially by following an arbitrary order i1:n, assuming agent i1 takes an action ¯a[i][1]
such that A[i][1] (s, ¯a[i][1] ) > 0, and then, for the rest m = 2, . . ., n, the agent im takes an action ¯a[i][m]
such that A[i][m] (s, ¯a[i][1:][m][−][1] _, ¯a[i][m]_ ) > 0. For the induced joint action ¯a, Lemma 1 assures that Aπθ (s, ¯a)
is positive, thus the performance is guaranteed to improve. To formally extend the above process
into a policy iteration procedure with monotonic improvement guarantee, we need the following
definitions.
**Definition 2. Let π be a joint policy, ¯π[i][1:][m][−][1]** = _j=1_ _π[¯][i][j]_ _be some other joint policy of agents_
_i1:m−1, and ˆπ[i][m]_ _be some other policy of agent im. Then_

[Q][m][−][1]

_L[i]π[1:][m]_ **_π¯_** _[i][1:][m][−][1]_ _, ˆπ[i][m][]_ ≜ Es∼ρπ,ai1:m−1 _∼π¯_ _[i][1:][m][−][1]_ _,a[im]_ _∼πˆ[im]_ _A[i]π[m]_ s, a[i][1:][m][−][1] _, a[i][m][]_ _._

Note that, for any  **¯π[i][1:][m][−][1]**, we have   

_L[i]π[1:][m]_ **_π¯_** _[i][1:][m][−][1]_ _, π[i][m]_ [] = Es∼ρπ,ai1:m−1 _∼π¯_ _[i][1:][m][−][1]_ _,a[im]_ _∼π[im]_ _A[i]π[m]_ s, a[i][1:][m][−][1] _, a[i][m]_ []
  = Es∼ρπ,ai1:m−1 _∼π¯_ _[i][1:][m][−][1]_ Eaim _∼πim_ _A[i]π [m]_ s, a[i][1:][m][−][1] _, a[i][m][]_ = 0. (6)

   

Building on Lemma 1 and Definition 2, we can finally generalise Theorem 1 of TRPO to MARL.
**Lemma 2. Let π be a joint policy. Then, for any joint policy ¯π, we have**

_n_

_J(π¯) ≥_ _J(π) +_ _L[i]π[1:][m]_ **_π¯_** _[i][1:][m][−][1]_ _, ¯π[i][m][]_ _−_ _CD[max]KL_ [(][π][i][m] _[,][ ¯]π[i][m])_ _._

_m=1_

X    

For proof see Appendix C.2. This lemma provides an idea about how a joint policy can be improved.
Namely, by Equation (6), we know that if any agents were to set the values of the above summands
_L[i]π[1:][m]_ (π¯ _[i][1:][m][−][1]_ _, ¯π[i][m]) −_ _CD[max]KL_ [(][π][i][m][,][ ¯]π[i][m] ) by sequentially updating their policies, each of them can
always make its summand be zero by making no policy update (i.e., ¯π[i][m] = π[i][m] ). This implies
that any positive update will lead to an increment in summation. Moreover, as there are n agents
making policy updates, the compound increment can be large, leading to a substantial improvement.
Lastly, note that this property holds with no requirement on the specific order by which agents
make their updates; this allows for flexible scheduling on the update order at each iteration. To
summarise, we propose the following Algorithm 1. We want to highlight that the algorithm is

**Algorithm 1 Multi-Agent Policy Iteration with Monotonic Improvement Guarantee**


1: Initialise the joint policy π0 = (π0[1][, . . ., π]0[n][)][.]

2: for k = 0, 1, . . . do
3: Compute the advantage function Aπk (s, a) for all state-(joint)action pairs (s, a).

4: Compute ϵ = maxs,a |Aπk (s, a)| and C = (14−γϵγ)[2][ .]

5: Draw a permutaion i1:n of agents at random.

6: **for m = 1 : n do**

7: Make an update πk[i][m]+1 [= arg max]π[im] _L[i]π[1:]k[m]_ **_πk[i][1:]+1[m][−][1]_** _, π[i][m]_ _−_ _CD[max]KL_ [(][π]k[i][m] _[, π][i][m]_ [)] .

8: **end for** h   i

9: end for

markedly different from naively applying the TRPO update, i.e., Equation (3), on the joint policy of
all agents. Firstly, our Algorithm 1 does not update the entire joint policy at once, but rather update
each agent’s individual policy sequentially. Secondly, during the sequential update, each agent has a
unique optimisation objective that takes into account all previous agents’ updates, which is also the
key for the monotonic improvement property to hold.

3.2 THEORETICAL ANALYSIS

Now we justify by the following theorm that Algorithm 1 enjoys monotonic improvement property.


-----

**Theorem 2. A sequence (πk)[∞]k=0** _[of joint policies updated by Algorithm][ 1][ has the monotonic im-]_
_provement property, i.e., J(πk+1) ≥_ _J(πk) for all k ∈_ N.
For proof see Appendix C.2. With the above theorem, we finally claim a successful introduction of
trust region learning to MARL, as this generalises the monotonic improvement property of TRPO.
Moreover, we take a step further to study the convergence property of Algorithm 1. Before stating
the result, we introduce the following solution concept.
**Definition 3. In a fully-cooperative game, a joint policy π** = (π[1]
_∗_ _∗[, . . ., π]∗[n][)][ is a Nash equilibrium]_
_(NE) if for every i_ _, π[i]_ Π[i] _implies J (π_ ) _J_ _π[i], π[−][i]_ _._
_∈N_ _∈_ _∗_ _≥_ _∗_

NE (Nash, 1951) is a well-established game-theoretic solution concept. Definition   3 characterises
the equilibrium point at convergence for cooperative MARL tasks. Based on this, we have the
following result that describes Algorithm 1’s asymptotic convergence behaviour towards NE.
**Theorem 3. Supposing in Algorithm 1 any permutation of agents has a fixed non-zero probability to**
_begin the update, a sequence (πk)[∞]k=0_ _[of joint policies generated by the algorithm, in a cooperative]_
_Markov game, has a non-empty set of limit points, each of which is a Nash equilibrium._

For proof see Appendix C.3. In deriving this result, the novel details introduced by Algorithm 1
played an important role. The monotonic improvement property (Theorem 2), achieved through the
multi-agent advantage and the sequential update scheme, provided us with a guarantee on the convergence of the return. Furthermore, randomisation of the update order assured that, at convergence,
none of the agents is incentified to make an update. The proof is finalised by excluding a possibility
that the algorithm converges at non-equilibrium points.

4 PRACTICAL ALGORITHMS
When implementing Algorithm 1 in practice, large state and action spaces could prevent agents from
designating policies π[i](·|s) for each state s separately. To handle this, we parameterise each agent’s
policy πθ[i] _[i][ by][ θ][i][, which, together with other agents’ policies, forms a joint policy][ π][θ][ parametrised]_
by θ = (θ[1], . . ., θ[n]). In this section, we develop two deep MARL algorithms to optimise the θ.

4.1 HATRPO

Computing D[max]KL _πθ[i][m]k[im]_ _[, π]θ[i][m][im]_ in Algorithm 1 is challenging; it requires evaluating the KL
divergence for all states at each iteration. Similar to TRPO, one can ease this maximal KL
  

divergence penalty D[max]KL _πθ[i][m]k[im]_ _[, π]θ[i][m][im]_ by replacing it with the expected KL-divergence constraint

Es∼ρπθk DKL _πθ[i][m]k[im]_ (·|s), π θ[i][m][im] [(][·|][s][)] _≤δ where δ is a threshold hyperparameter, and the expectation_

can be easily approximated by stochastic sampling. With the above amendment, we propose practi-h   [i]
cal HATRPO algorithm in which, at every iteration k + 1, given a permutation of agents i1:n, agent
_im∈{1,...,n} sequentially optimises its policy parameter θk[i][m]+1_ [by maximising a constrained objective:]

_θk[i][m]+1_ [= arg max]θ[im] Es∼ρπθk,ai1:m−1 ∼πθi1:ki1:+1mm−−11 _,a[im]_ _∼πθ[im][im]_ _A[i]π[m]θk_ [(][s][,][ a][i][1:][m][−][1] _[,][ a][i][m]_ [)] _,_

 

subject to Es∼ρπθk DKL _πθ[i][m]k[im]_ [(][·|][s][)][, π]θ[i][m][im] [(][·|][s][)] _≤_ _δ._ (7)

To compute the above equation, similar to TRPO, one can apply a linear approximation to the   
objective function and a quadratic approximation to the KL constraint; the optimisation problem in
Equation (7) can be solved by a closed-form update rule as


2δ

_θk[i][m]+1_ [=][ θ]k[i][m] + α[j] (Hk[i][m] [)][−][1][g]k[i][m] _,_ (8)
s **_gk[i][m]_** [(][H]k[i][m] [)][−][1][g]k[i][m]

where Hk[i][m] = ∇θ[2][im] [E][s][∼][ρ][π]θk DKL _πθ[i][m]k[im]_ (·|s), πθ[i][m][im] [(][·|][s][)] _θ[im]_ =θk[im] is the Hessian of the expected KL
divergence, gk[i][m] is the gradient of the objective in Equation (    7), α[j] _< 1 is a positive coefficient that_
is found via backtracking line search, and the product of (Hk[i][m] [)][−][1][g]k[i][m] can be efficiently computed
with conjugate gradient algorithm.

The last missing piece for HATRPO is to estimate Eai1:m−1 _∼πθi1:k+1m−1_ _,a[im]_ _∼πθ[im][im]_ _A[i]π[m]θk_ s, a[i][1:][m][−][1] _, a[i][m][i]_

which poses new challenges because each agent’s objective has to take into account all previoush  


-----

agents’ updates, and the size of input vaires. Fortunately, with the following proposition, we can
efficiently estimate this objective by employing a joint advantage estimator.
**Proposition 2. Let π =** _j=1_ _[π][i][j][ be a joint policy, and][ A][π][(][s][,][ a][)][ be its joint advantage function.]_
_Let ¯π[i][1:][m][−][1]_ = _j=1_ _π[¯][i][j]_ _be some other joint policy of agents i1:m−1, and ˆπ[i][m]_ _be some other policy_
_of agent im. Then, for every state[Q][n]_ _s,_

Eai1:m−1 _∼π¯[Q][i][1:][m][m][−][−][1][1]_ _,a[im]_ _∼πˆ[im]_ _A[i]π[m]_ _s, a[i][1:][m][−][1]_ _, a[i][m]_ []
   ˆπ[i][m] (a[i][m] _s)_ **¯π[i][1:][m][−][1]** (a[i][1:][m][−][1] _s)_

= Ea **_π_** _|_ _|_ _._ (9)
_∼_ h _π[i][m]_ (a[i][m] _|s)_ _[−]_ [1] **_π[i][1:][m][−][1]_** (a[i][1:][m][−][1] _|s)_ _[A][π][(][s,][ a][)]i_

For proof see Appendix D.1. One benefit of applying Equation (9) is that agents only need to maintain a joint advantage estimator Aπ(s, a) rather than one centralised critic for each individual agent
(e.g., unlike CTDE methods such as MADDPG). Another practical benefit one can draw is that,
given an estimator _A[ˆ](s, a) of the advantage function Aπθk (s, a), for example GAE (Schulman et al.,_

2015b), we can estimate Eai1:m−1 ∼πθi1:ki1:+1mm−−11 _,a[im]_ _∼πθ[im][im]_ hA[i]π[m]θk  s, a[i][1:][m][−][1] _, a[i][m]_ [i] with an estimator of

_πθim_ [(][a][i][m] _[|][s][)]_ _M_ _[i][1:][m]_ [ ]s, a _,_ where M _[i][1:][m]_ = **_π[¯]_** _[i][1:][m][−][1]_ (a[i][1:][m][−][1] _|s)_ _Aˆ_ _s, a_ _._ (10)
 _πθ[i][m]k_ [(][a][i][m] _[|][s][)][ −]_ [1]  **_π[i][1:][m][−][1]_** (a[i][1:][m][−][1] _|s)_   

Notably, Equation (10) aligns nicely with the sequential update scheme in HATRPO. For agent im,
since previous agents i1:m 1 have already made their updates, the compound policy ratio for M _[i][1:][m]_
_−_
in Equation (10) is easy to compute. Given a batch B of trajectories with length T, we can estimate
the gradient with respect to policy parameters (derived in Appendix D.2) as follows,


**_gˆk[i][m]_** = |B|[1]


_M_ _[i][1:][m]_ (st, at)∇θim log πθ[i][m][im] [(][a]t[i][|][s]t[)] _θ[im]_ =θk[im] _[.]_
_t=0_

X


_τ_ _∈B_


The term −1 · M _[i][1:][m]_ (s, a) of Equation (10) is not reflected in ˆgk[i][m][, as it only introduces a constant]
with zero gradient. Along with the Hessian of the expected KL-divergence, i.e., Hk[i][m][, we can update]
_θk[i][m]+1_ [by following Equation (][8][). The detailed pseudocode of HATRPO is listed in Appendix][ D.3][.]

4.2 HAPPO

To further alleviate the computation burden from Hk[i][m] in HATRPO, one can follow the idea of PPO
in Equation (4) by considering only using first order derivatives. This is achieved by making agent
_im choose a policy parameter θk[i][m]+1_ [which maximises the clipping objective of]

_πθ[i][m][im]_ [(][a][i][|][s][)] _πθim[im]_ [(][a][i][|][s][)]

Es∼ρπθk,a[∼]πθk " min _πθ[i][m]k[im]_ (a[i]|s) _[M][ i][1:][m][ (][s][,][ a][)][,][ clip]_ _πθ[i][m]k[im]_ (a[i]|s) _[,][ 1][ ±][ ϵ]M_ _[i][1:][m]_ (s, a) !#. (11)

The optimisation process can be performed by stochastic gradient methods such as Adam (Kingma
& Ba, 2014). We refer to the above procedure as HAPPO and Appendix D.4 for its full pseudocode.

4.3 RELATED WORK

We are fully aware of previous attempts that tried to extend TRPO/PPO into MARL. Despite empirical successes, none of them managed to propose a theoretically-justified trust region protocol
in multi-agent learning, or maintain the monotonic improvement property. Instead, they tend to
impose certain assumptions to enable direct implementations of TRPO/PPO in MARL problems.
For example, IPPO (de Witt et al., 2020a) assume homogeneity of action spaces for all agents and
enforce parameter sharing. Yu et al. (2021) proposed MAPPO which enhances IPPO by considering a joint critic function and finer implementation techniques for on-policy methods. Yet, it still
suffers similar drawbacks of IPPO due to the lack of monotonic improvement guarantee especially
when the parameter-sharing condition is switched off. Wen et al. (2021) adjusted PPO for MARL
by considering a game-theoretical approach at the meta-game level among agents. Unfortunately, it
can only deal with two-agent cases due to the intractability of Nash equilibrium. Recently, Li & He
(2020) tried to implement TRPO for MARL through distributed consensus optimisation; however,
they enforced the same ratio ¯π[i](a[i]|s)/π[i](a[i]|s) for all agents (see their Equation (7)), which, similar


-----

corridor

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
||||||
||||||
|||||HATRPO|
|||||HAPPO MAPPO|



0.0 0.5 Environment Steps1.0 1.5 2.01e7


1.0

0.8

0.6

0.4

0.2

0.0


HATRPO
HAPPO
MAPPO


2c_vs_64zg 3s5z

1.0 1.0

0.8 0.8

0.6 0.6

0.4 0.4

Evaluate Winning Rate0.2 Evaluate Winning Rate0.2

0.0 0.0

0.0 0.5 Environment Steps1.0 1.5 2.01e7 0.0 0.5 Environment Steps1.0 1.5 2.01e7

(a) 2c-vs-64zg (hard)


Environment Steps 1e7

(b) 3s5z (hard)


(c) corridor (super hard)


Figure 2: Performance comparisons between HATRPO/HAPPO and MAPPO on three SMAC tasks.
Since all methods achieve 100% win rate, we believe SMAC is not sufficiently difficult to discriminate the capabilities of these algorithms, especially when non-parameter sharing is not required.

to parameter sharing, largely limits the policy space for optimisation. Moreover, their method comes
with a δ/n KL-constraint threshold that fails to consider scenarios with large agent number.

One of the key ideas behind our HATRPO/HAPPO is the sequential update scheme. A similar
idea of multi-agent sequential update was also discussed in the context of dynamic programming
(Bertsekas, 2019) where artificial “in-between” states have to be considered. On the contrary, our
sequential update sceheme is developed based on Lemma 1, which does not require any artificial assumptions and hold for any cooperative games. Furthermore, Bertsekas (2019) requires to maintain
a fixed order of updates that is pre-defined for the task, whereas the order in HATRPO/MAPPO can
be randomised at each iteration, which also offers desirable convergence property, as stated in Proposition 3 and also verified through ablation studies in Appendix F. The idea of sequential update also
appeared in principal component analysis; in EigenGame (Gemp et al., 2020) eigenvectors, represented as players, maximise their own utility functions one-by-one. Although EigenGame provably
solves the PCA problem, it is of little use in MARL, where a single iteration of sequential updates
is insufficient to learn complex policies. Furthermore, its design and analysis rely on closed-form
matrix calculus, which has no extension to MARL.

Lastly, we would like to highlight the importance of the decomposition result in Lemma 1. This
result could serve as an effective solution to value-based methods in MARL where tremendous
efforts have been made to decompose the joint Q-function into individual Q-functions when the
joint Q-function are decomposable (Rashid et al., 2018). Lemma 1, in contrast, is a general result
that holds for any cooperative MARL problems regardless of decomposibility. As such, we think of
it as an appealing contribution to future developments on value-based MARL methods.

5 EXPERIMENTS AND RESULTS
We consider two most common benchmarks—StarCraftII Multi-Agent Challenge (SMAC)
(Samvelyan et al., 2019) and Multi-Agent MuJoCo (de Witt et al., 2020b)—for evaluating MARL
algorithms. All hyperparameter settings and implementations details can be found in Appendix E.

**StarCraftII Multi-Agent Challenge (SMAC). SMAC contains a set of StarCraft maps in which**
a team of ally units aims to defeat the opponent team. IPPO (de Witt et al., 2020a) and MAPPO
(Yu et al., 2021) are known to achieve supreme results on this benchmark. By adopting parameter
sharing, these methods achieve a winning rate of 100% on most maps, even including the maps that
have heterogeneous agents. Therefore, we hypothesise that non-parameter sharing is not necessarily
required and the trick of sharing policies is sufficient to solve SMAC tasks. We test our methods
on two hard maps and one super-hard; results on Figure 2 confirm that SMAC is not sufficiently
difficult to show off the capabilities of HATRPO/HAPPO when compared against existing methods.

**Multi-Agent MuJoCo. In comparison to SMAC, we believe Mujoco enviornment provides a more**
suitable testing case for our methods. MuJoCo tasks challenge a robot to learn an optimal way
of motion; Multi-Agent MuJoCo models each part of a robot as an independent agent, for example, a leg for a spider or an arm for a swimmer. With the increasing variety of the body parts,
modelling heterogeneous policies becomes necessary. Figure 3 demonstrate that, in all scenarios,
HATRPO and HAPPO enjoy superior performance over those of parameter-sharing methods: IPPO
and MAPPO, and also outperform non-parameter sharing MADDPG (Lowe et al., 2017b) both in
terms of reward values and variance. It is also worth noting that the performance gap between HATRPO and its rivals enlarges with the increasing number of agents. Meanwhile, we can observe that
HATRPO outperforms HAPPO in almost all tasks; we believe it is because the hard KL constraint


-----

Ant 2x4 Ant 4x2

4000 4000

3000

3000

2000

2000

1000

Average Episode Reward1000 Average Episode Reward 0

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

Environment steps 1e7 Environment steps 1e7


Ant 8x1

5000 HATRPO

4000 HAPPOMAPPO

IPPO

3000 MADDPG

2000

1000

Average Episode Reward 0

0.0 0.2 0.4 0.6 0.8 1.0

Environment steps 1e7


(a) 2x4-Agent Ant


(b) 4x2-Agent Ant


(c) 8x1-Agent Ant


HalfCheetah 2x3 HalfCheetah 3x2

6000

6000

5000

5000

4000

4000

3000 3000

2000 2000

1000 1000

Average Episode Reward 0 Average Episode Reward 0

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

Environment steps 1e7 Environment steps 1e7


HalfCheetah 6x1

6000 HATRPO

HAPPO

5000 MAPPO

4000 IPPOMADDPG

3000

2000

1000

Average Episode Reward 0

0.0 0.2 0.4 0.6 0.8 1.0

Environment steps 1e7


(d) 2x3-Agent HalfCheetah


(e) 3x2-Agent HalfCheetah


(f) 6x1-Agent HalfCheetah


Walker 2x3 Walker 3x2

5000

4000

4000

3000

3000

2000 2000

1000 1000

Average Episode Reward Average Episode Reward

0 0

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

Environment steps 1e7 Environment steps 1e7


Walker 6x1

5000 HATRPO

HAPPO

4000 MAPPO

IPPO

3000 MADDPG

2000

1000

Average Episode Reward

0

0.0 0.2 0.4 0.6 0.8 1.0

Environment steps 1e7


(g) 2x3-Agent Walker


(h) 3x2-Agent Walker


(i) 6x1-Agent Walker


Humanoid 17x1 HumanoidStandup 17x1

160

800 1e3

140

700

120

600

500 100

400 80

300 60

200 40

Average Episode Reward100 Average Episode Reward

20

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

Environment steps 1e7 Environment steps 1e7

(j) 17x1-Agent Humanoid


Environment steps 1e7

(k) 17x1-Agent HumanoidStandup


Figure 3: Performance comparison on multiple Multi-Agent MuJoCo tasks. HAPPO and HATRPO
consistently outperform their rivals, thus establishing a new state-of-the-art algorithm for MARL.

ManyAgentSwimmer 10x2

250

200

150

100

50

0

50

Average Episode Reward 100

150

0.0 0.2 0.4 0.6 0.8 1.0

Environment steps 1e7

(l) 10x2-Agent Swimmer

The performance gap enlarges with increasing number of agents.

in HATRPO, compared to the clipping version in HAPPO, relates more closely to Algorithm 1 that
attains monotonic improvement guarantee.

6 CONCLUSION


In this paper, we successfully apply trust region learning to multi-agent settings by proposing the first
MARL algorithm that attains theoretically-justified monotonical improvement property. The key to
our development is the multi-agent advantage decomposition lemma that holds in general with no
need for any assumptions on agents sharing parameters or the joint value function being decomposable. Based on this, we introduced two practical deep MARL algorithms: HATRPO and HAPPO.
Experimental results on both discrete and continuous control tasks (i.e., SMAC and Multi-Agent
Mujoco) confirm their state-of-the-art performance. For future work, we will consider incorporating
the safety constraint into HATRPO/HAPPO and propose rigorous safety-aware MARL solutions.


-----

REFERENCES

Dimitri Bertsekas. Multiagent rollout algorithms and reinforcement learning. _arXiv preprint_
_arXiv:1910.00120, 2019._

Caroline Claus and Craig Boutilier. The dynamics of reinforcement learning in cooperative multiagent systems. AAAI/IAAI, 1998(746-752):2, 1998.

Christian Schroeder de Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip HS
Torr, Mingfei Sun, and Shimon Whiteson. Is independent learning all you need in the starcraft
multi-agent challenge? arXiv preprint arXiv:2011.09533, 2020a.

Christian Schr¨oder de Witt, Bei Peng, Pierre-Alexandre Kamienny, Philip H. S. Torr, Wendelin
B¨ohmer, and Shimon Whiteson. Deep multi-agent reinforcement learning for decentralized continuous cooperative control. CoRR, abs/2003.06709, 2020b.

Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep
reinforcement learning for continuous control. In International conference on machine learning,
pp. 1329–1338. PMLR, 2016.

Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In Proceedings of the AAAI Conference on Artificial
_Intelligence, volume 32, 2018._

Ian Gemp, Brian McWilliams, Claire Vernade, and Thore Graepel. Eigengame: Pca as a nash
equilibrium. arXiv preprint arXiv:2010.00554, 2020.

Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Confer_ence on Machine Learning, pp. 1861–1870. PMLR, 2018._

Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
_In Proc. 19th International Conference on Machine Learning. Citeseer, 2002._

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
_arXiv:1412.6980, 2014._

Jakub Grudzien Kuba, Muning Wen, Yaodong Yang, Linghui Meng, Shangding Gu, Haifeng Zhang,
David Henry Mguni, and Jun Wang. Settling the variance of multi-agent policy gradients. arXiv
_preprint arXiv:2108.08612, 2021._

Hepeng Li and Haibo He. Multi-agent trust region policy optimization. arXiv e-prints, pp. arXiv–
2010, 2020.

Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
_Machine learning proceedings 1994, pp. 157–163. Elsevier, 1994._

Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actorcritic for mixed cooperative-competitive environments. In Proceedings of the 31st International
_Conference on Neural Information Processing Systems, pp. 6382–6393, 2017a._

Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, and Igor Mordatch. Multi-agent actorcritic for mixed cooperative-competitive environments. In Proceedings of the 31st International
_Conference on Neural Information Processing Systems, pp. 6382–6393, 2017b._

A Rupam Mahmood, Dmytro Korenkevych, Gautham Vasan, William Ma, and James Bergstra.
Benchmarking reinforcement learning algorithms on real-world robots. In Conference on robot
_learning, pp. 561–591. PMLR, 2018._

David H Mguni, Yutong Wu, Yali Du, Yaodong Yang, Ziyi Wang, Minne Li, Ying Wen, Joel Jennings, and Jun Wang. Learning in nonzero-sum stochastic games with potentials. In Marina Meila
and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning,
volume 139 of Proceedings of Machine Learning Research, pp. 7688–7699. PMLR, 18–24 Jul
2021.


-----

John Nash. Non-cooperative games. Annals of mathematics, pp. 286–295, 1951.

P Peng, Q Yuan, Y Wen, Y Yang, Z Tang, H Long, and J Wang. Multiagent bidirectionallycoordinated nets for learning to play starcraft combat games. arxiv 2017. _arXiv preprint_
_arXiv:1703.10069, 2017._

Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and
Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning, pp. 4295–4304. PMLR, 2018.

Mikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nantas
Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson.
The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.

John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889–1897. PMLR,
2015a.

John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. Highdimensional continuous control using generalized advantage estimation. _arXiv preprint_
_arXiv:1506.02438, 2015b._

John Schulman, F. Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. ArXiv, abs/1707.06347, 2017.

David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In International conference on machine learning, pp.
387–395. PMLR, 2014.

Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max
Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-decomposition
networks for cooperative multi-agent learning based on team reward. In Proceedings of the 17th
_International Conference on Autonomous Agents and MultiAgent Systems, pp. 2085–2087, 2018._

R. S. Sutton, D. Mcallester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement
learning with function approximation. In Advances in Neural Information Processing Systems 12,
volume 12, pp. 1057–1063. MIT Press, 2000.

Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. 2018.

Ying Wen, Yaodong Yang, Rui Luo, Jun Wang, and Wei Pan. Probabilistic recursive reasoning for
multi-agent reinforcement learning. In International Conference on Learning Representations,
2018.

Ying Wen, Yaodong Yang, and Jun Wang. Modelling bounded rationality in multi-agent interactions
by generalized recursive reasoning. In Christian Bessiere (ed.), Proceedings of the Twenty-Ninth
_International Joint Conference on Artificial Intelligence, IJCAI-20, pp. 414–421. International_
Joint Conferences on Artificial Intelligence Organization, 7 2020. Main track.

Ying Wen, Hui Chen, Yaodong Yang, Zheng Tian, Minne Li, Xu Chen, and Jun Wang. A gametheoretic approach to multi-agent trust region optimization. arXiv preprint arXiv:2106.06828,
2021.

Jiayi Weng, Huayu Chen, Dong Yan, Kaichao You, Alexis Duburcq, Minghao Zhang, Hang Su, and
Jun Zhu. Tianshou: a highly modularized deep reinforcement learning library. arXiv preprint
_arXiv:2107.14171, 2021._

Yaodong Yang and Jun Wang. An overview of multi-agent reinforcement learning from game theoretical perspective. arXiv preprint arXiv:2011.00583, 2020.

Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. Mean field multiagent reinforcement learning. In International Conference on Machine Learning, pp. 5571–5580.
PMLR, 2018.


-----

Yaodong Yang, Ying Wen, Jun Wang, Liheng Chen, Kun Shao, David Mguni, and Weinan Zhang.
Multi-agent determinantal q-learning. In International Conference on Machine Learning, pp.
10757–10766. PMLR, 2020.

Chao Yu, A. Velu, Eugene Vinitsky, Yu Wang, A. Bayen, and Yi Wu. The surprising effectiveness
of mappo in cooperative, multi-agent games. ArXiv, abs/2103.01955, 2021.

Haifeng Zhang, Weizhe Chen, Zeren Huang, Minne Li, Yaodong Yang, Weinan Zhang, and Jun
Wang. Bi-level actor-critic for multi-agent coordination. In Proceedings of the AAAI Conference
_on Artificial Intelligence, volume 34, pp. 7325–7332, 2020._

Ming Zhou, Ziyu Wan, Hanjing Wang, Muning Wen, Runzhe Wu, Ying Wen, Yaodong Yang,
Weinan Zhang, and Jun Wang. Malib: A parallel framework for population-based multi-agent
reinforcement learning. arXiv preprint arXiv:2106.07551, 2021.


-----

# Appendices

**A Preliminaries** **14**

A.1 Definitions and Assumptions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14

A.2 Proofs of Preliminary Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14

**B** **Proof of Proposition 1** **16**

**C Derivation and Analysis of Algorithm 1** **17**

C.1 Recap of Existing Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

C.2 Analysis of Training of Algorithm 1 . . . . . . . . . . . . . . . . . . . . . . . . . 17

C.3 Analysis of Convergence of Algorithm 1 . . . . . . . . . . . . . . . . . . . . . . . 19

**D HATRPO and HAPPO** **22**

D.1 Proof of Proposition 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22

D.2 Derivation of the gradient estimator for HATRPO . . . . . . . . . . . . . . . . . . 22

D.3 Pseudocode of HATRPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23

D.4 Pseudocode of HAPPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

**E** **Hyper-parameter Settings for Experiments** **24**

**F** **Ablation Experiments** **26**

**G Ablation Study on Non-Parameter Sharing MAPPO/IPPO** **26**

**H Multi-Agent Particle Environment Experiments** **27**


-----

A PRELIMINARIES

A.1 DEFINITIONS AND ASSUMPTIONS

**Assumption 1. There exists η ∈** R, such that 0 < η ≪ 1, and for every agent i ∈N _, the policy_
_space Π[i]_ _is η-soft; that means that for every π[i]_ _∈_ Π[i], s ∈S, and a[i] _∈A[i], we have π[i](a[i]|s) ≥_ _η._

**Definition 3. In a fully-cooperative game, a joint policy π** = (π[1]
_∗_ _∗[, . . ., π]∗[n][)][ is a Nash equilibrium]_
_(NE) if for every i_ _, π[i]_ Π[i] _implies J (π_ ) _J_ _π[i], π[−][i]_ _._
_∈N_ _∈_ _∗_ _≥_ _∗_

**Definition 4. Let X be a finite set and p : X →** R, q  : X →R be two maps. Then, the notion of
**_distance between p and q that we adopt is given by_** _p_ _q_ ≜ maxx _X_ _p(x)_ _q(x)_ _._
_||_ _−_ _||_ _∈_ _|_ _−_ _|_

A.2 PROOFS OF PRELIMINARY RESULTS

**Lemma 3. Every agent i’s policy space Π[i]** _is convex and compact under the maximum norm._

_Proof. We start from proving convexity od the policy space: we prove that, for any two policies_
_π[i], ¯π[i]_ _∈_ Π[i], for every α ∈ [0, 1], απ[i] + (1 − _α)¯π[i]_ _∈_ Π[i]. Clearly, for all s ∈S and a[i] _∈A[i], we_
have

_απ[i](a[i]|s) + (1 −_ _α)¯π[i](a[i]|s) ≥_ _αη + (1 −_ _α)η = η._

Also, for every state s,


_απ[i](a[i]|s) + (1 −_ _α)¯π[i](a[i]|s)_ = α
i


_π[i](a[i]|s) + (1 −_ _α)_
_a[i]_

X


_π¯[i](a[i]|s) = α + 1 −_ _α = 1,_
_a[i]_

X


_a[i]_


which establishes convexity.

For compactness, we first prove that Π[i] is closed.

Let _πk[i]_ _∞k=0_ [be a convergent sequence of policies of agent][ i][. Let][ π][i][ be the limit. We will prove that]
_π[i]_ is a policy. First, by Assumption 1, for any k N, s, and a[i], we have πk[i] _a[i]_ _s_ _η._
   _∈_ _∈S_ _∈A[i]_ _|_ _≥_
Hence,
  
_π[i][  ]a[i]_ _s_ = lim _k_ _a[i]_ _s_ lim
_|_ _k_ _|_ _≥_ _k_
_→∞_ _[π][i]_ _→∞_ _[η][ ≥]_ _[η.]_

Furtheremore, for any k and s, we have _a[i][ π]k[i]_  a[i]|s = 1. Hence

  

_π[i][  ]a[i]_ _s_ = lim _a[i]_ _s_ = lim _π[i][  ]a[i]_ _s_ = lim
_|_ _k_ [P] _|_ _k_ _|_ _k_
_a[i]_ _a[i]_ _→∞_ _[π][i][  ]_ _→∞_ _a[i]_ _→∞_ [1 = 1][.]

X  X  X 

With these two conditions satisfied, π[i] is a policy, which proves the closure.

which proves boundedness. Hence,Further, for all policies π[i], states s Π and actions[i] is compact. a, |π[i](a[i]|s)| ≤ 1. This means that ||π[i]||max ≤ 1,

**Lemma 4 (Continuity of ρπ). The improper state distribution ρπ is continuous in π.**

_Proof. First, let us show that for any t ∈_ N, the distribution ρ[t]π [is continous in][ π][. We will do it]
by induction. This obviously holds when t = 0, as ρ[0] does not depend on policy. Hence, we can
assume that for some t ∈ N, the distribution ρ[t]π [is continuous in][ π][. Let us now consider two policies]
_π and ˆπ. Let s[′]_ _∈S. We have_


_ρ[t]π[+1](s[′])_ _ρ[t]πˆ[+1](s[′])_ = _ρ[t]π[(][s][)]_ _π(a_ _s)P_ (s[′] _s, a)_
_−_ _|_ _|_ _−_

_s_ _a_

X X

= _ρ[t]π[(][s][)][π][(][a][|][s][)][ −]_ _[ρ][t]πˆ_ [(][s][)ˆ]π(a|s) _P_ (s[′]|s, a)

_s_ _a_

X X  

_≤_ _ρ[t]π[(][s][)][π][(][a][|][s][)][ −]_ _[ρ][t]πˆ_ [(][s][)ˆ]π(a|s) _P_ (s[′]|s, a)

_s_ _a_

X X


_ρ[t]πˆ_ [(][s][)]


_πˆ(a|s)P_ (s[′]|s, a)


-----

_ρ[t]π[(][s][)][π][(][a][|][s][)][ −]_ _[ρ]π[t]_ [(][s][)ˆ]π(a|s) + ρ[t]π[(][s][)ˆ]π(a|s) − _ρ[t]πˆ_ [(][s][)ˆ]π(a|s) _P_ (s[′]|s, a)

_ρ[t]π[(][s][)][ |][π][(][a][|][s][)][ −]_ _π[ˆ](a|s)| + ˆπ(a|s)_ _ρ[t]π[(][s][)][ −]_ _[ρ]π[t]ˆ_ [(][s][)]
 ρ[t]π[(][s][)][||][π][ −] _π[ˆ]|| + ˆπ(a|s)||ρ[t]π_ _[−]_ _[ρ]π[t]ˆ_ _[||]_ []

  


_ρ[t]π[(][s][)]_


_||ρ[t]π_ _[−]_ _[ρ]π[t]ˆ_ _[||]_


_||π −_ _πˆ|| +_


_πˆ(a|s)_


= _π_ _πˆ_ + _ρ[t]π_ _πˆ_ _[||][.]_
_|A| · ||_ _−_ _||_ _|S| · ||_ _[−]_ _[ρ][t]_
Hence, we obtain
_ρ[t]π[+1]_ _ρ[t]πˆ[+1]_ _π_ _π¯_ + _ρ[t]π_ _πˆ[||][.]_ (12)
_||_ _−_ _|| ≤|A| · ||_ _−_ _||_ _|S| · ||_ _[−]_ _[ρ][t]_
Using the base case, taking the limit as ¯π → _π, we get that the right-hand-side of Equation (12)_
converges to 0, which proves that every ρ[t]π[+1] is continuous in π, and finishes the inductive step.

We can now prove that the total marginal state distribution ρπ is continous in π. To do that, let’s

take an arbitrary ϵ > 0, and a natural T such that T > log[logϵ(1 γ4−γ) ] . Equivalently, we choose T such

that 1[2][γ][T]γ _[<][ ϵ]2_ [. Let][ π][ and][ ˆ]π be two policies. We have

_−_

_∞_

_|ρπ(s) −_ _ρπˆ_ [(][s][)][|][ =] _γ[t][  ]ρ[t]π[(][s][)][ −]_ _[ρ][t]πˆ_ [(][s][)]

_t=0_

_T −1_ X _∞_ 

= _γ[t][  ]ρ[t]π[(][s][)][ −]_ _[ρ][t]πˆ_ [(][s][)] + γ[T] _γ[t][−][T][  ]ρ[t]π[(][s][)][ −]_ _[ρ][t]πˆ_ [(][s][)]

_t=0_ _t=T_

_TX −1_  _∞X_ 

_≤_ _γ[t]_ _ρ[t]π[(][s][)][ −]_ _[ρ][t]πˆ_ [(][s][)] + γ[T] _γ[t][−][T]_ _ρ[t]π[(][s][)][ −]_ _[ρ][t]πˆ_ [(][s][)]

_t=0_ _t=T_

X X

_T −1_ _∞_

_≤_ _γ[t]_ _ρ[t]π[(][s][)][ −]_ _[ρ][t]πˆ_ [(][s][)] + γ[T] 2γ[t][−][T]

_t=0_ _t=T_

X X

_T −1_ _T −1_

= _γ[t]_ _ρ[t]π[(][s][)][ −]_ _[ρ][t]πˆ_ [(][s][)] + [2][γ][T] _γ[t]_ _ρ[t]π[(][s][)][ −]_ _[ρ][t]πˆ_ [(][s][)] + _[ϵ]_

1 _γ [<]_ 2

_t=0_ _−_ _t=0_

X X

_T −1_ _T −1_

_ρ[t]π[(][s][)][ −]_ _[ρ][t]πˆ_ [(][s][)] + _[ϵ]_ _ρ[t]π_ _πˆ_ _[||][ +][ ϵ]_ (13)

_≤_ _t=0_ 2 _[≤]_ _t=0_ _||_ _[−]_ _[ρ][t]_ 2 _[.]_

X X

Now, by continuity of each of ρ[t]π[, for][ t][ = 0][,][ 1][, . . ., T][ −] [1][, we have that there exists a][ δ >][ 0][ such that]
_π_ _πˆ_ _< δ implies_ _ρ[t]π_ _πˆ[||][ <]_ 2ϵT [. Taking such][ δ][, by Equation (][13][), we get][ ||][ρ][π][ −] _[ρ]π[ˆ][|| ≤]_ _[ϵ][,]_
_||which finishes the proof. −_ _||_ _||_ _[−]_ _[ρ][t]_

**Lemma 5 (Continuity of Qπ). Let π be a policy. Then Qπ(s, a) is Lipschitz-continuous in π.**

_Proof. Let π and ˆπ be two policies. Then we have_
_|Qπ(s, a) −_ _Qπˆ[(][s, a][)][|]_


_P_ (s[′] _s, a)π(a_ _s)Qπ(s, a)_
_|_ _|_


_P_ (s[′]|s, a)ˆπ(a|s)Qπˆ[(][s, a][)]


_r(s, a) + γ_


_r(s, a) + γ_


_s[′]_


_s[′]_


_P_ (s[′]|s, a) [π(a|s)Qπ(s, a) − _πˆ(a|s)Qπˆ[(][s, a][)]]_

_P_ (s[′]|s, a) |π(a|s)Qπ(s, a) − _πˆ(a|s)Qπˆ[(][s, a][)][|]_

_P_ (s[′]|s, a) |π(a|s)Qπ(s, a) − _πˆ(a|s)Qπ(s, a) + ˆπ(a|s)Qπ(s, a) −_ _πˆ(a|s)Qπˆ[(][s, a][)][|]_

_P_ (s[′]|s, a) (|π(a|s)Qπ(s, a) − _πˆ(a|s)Qπ(s, a)| + |πˆ(a|s)Qπ(s, a) −_ _πˆ(a|s)Qπˆ[(][s, a][)][|][)]_


= γ

_≤_ _γ_

= γ

_≤_ _γ_


_s[′]_

_s[′]_

X

_s[′]_

X

_s[′]_

X


-----

_P_ (s[′] _s, a)_ _π(a_ _s)_ _πˆ(a_ _s)_ _Qπ(s, a)_
_|_ _|_ _|_ _−_ _|_ _| · |_ _|_

_P_ (s[′]|s, a)ˆπ(a|s) |Qπ(s, a) − _Qπˆ[(][s, a][)][|]_
_a_

X

_P_ (s[′]|s, a)||π − _πˆ|| · Qmax_

_a_ _P_ (s[′]|s, a)ˆπ(a|s)||Qπ − _Qπˆ[||]_

X


= γ

_≤_ _γ_


_s[′]_

+ γ

_s[′]_

X

+ γ


_s[′]_

_s[′]_

X


= γ Qmax · |A| · ||π − _πˆ|| + γ||Qπ −_ _Qπˆ[||]_

_||Qπ −_ _Qπˆ[|| ≤]_ _[γ Q]max_ _[· |A| · ||][π][ −]_ _π[ˆ]|| + γ||Qπ −_ _Qπˆ[||][,]_


Hence, we get

which implies


_π_
_Qπ_ _Qπˆ[|| ≤]_ _[γ Q][max][ · |A| · ||][π][ −]_ [ˆ]|| _._ (14)
_||_ _−_ 1 _γ_

_−_

Equation (14) establishes Lipschitz-continuity with a constant _[γ Q]1[max]−γ[·|A|]_ .

**Corollary 1. From Lemma 5 we obtain that the following functions are Lipschitz-continuous in π:**

_the state value function Vπ =_ _a_ _[π][(][a][|][s][)][Q][π][(][s, a][)][,]_

_the advantage function Aπ(s, a) = Qπ(s, a)_ _Vπ(s),_

[P] _−_

_and the expected total reward J(π) = Es_ _ρ0 [Vπ(s)]._
_∼_


**Lemma 6. Let π and ˆπ be policies. The quantity Es** _ρπ,a_ _πˆ_ [[][A]π[(][s][,][ a][)]][ is continuous in][ π][.]
_∼_ _∼_

_Proof. We have_


Es _ρπ,a_ _πˆ_ [[][A]π[(][s][,][ a][)] =]
_∼_ _∼_


_ρπ(s)ˆπ(a_ _s)Aπ(s, a)._
_|_


Continuity follows from continuity of each ρπ(s) (Lemma 4) and Aπ(s, a) (Corollary 1).

**Corollary 2 (Continuity in MARL). All the results about continuity in π extend to MARL. Policy π**
_can be replaced with joint policy π; as π is Lipschitz-continuous in agent i’s policy π[i], the above_
_continuity results extend to conitnuity in π[i]. Thus, we will quote them in our proofs for MARL._

B PROOF OF PROPOSITION 1

**Proposition 1. Let’s consider a fully-cooperative game with an even number of agents n, one state,**
_and the joint action space {0, 1}[n], where the reward is given by r(0[n/][2], 1[n/][2]) = r(1[n/][2], 0[n/][2]) = 1,_
_and r(a[1:][n]) = 0 for all other joint actions. Let J_ _[∗]_ _be the optimal joint reward, and Jshare[∗]_ _[be the]_
_optimal joint reward under the shared policy constraint. Then_

_Jshare[∗]_ = [2]

_J_ _[∗]_ 2[n][ .]


_Proof. Clearly J_ _[∗]_ = 1. An optimal joint policy in this case is, for example, the deterministic policy
with joint action (0[n/][2], 1[n/][2]).

Now, let the shared policy be (θ, 1−θ), where θ determines the probability that an agent takes action
0. Then, the expected reward is

_J(θ) = Pr_ **_a[1:][n]_** = (0[n/][2], 1[n/][2]) _· 1 + Pr_ **_a[1:][n]_** = (1[n/][2], 0[n/][2]) _· 1 = 2 · θ[n/][2](1 −_ _θ)[n/][2]._
   


-----

In order to maximise J(θ), we must maximise θ(1 − _θ), or equivalently,_ _θ(1 −_ _θ)._ By the

artithmetic-geometric means inequality, we have

p


_θ(1_ _θ)_
_−_ _≤_ _[θ][ + (1]2[ −]_ _[θ][)]_


= [1]

2 _[,]_


where the equality holds if and only if θ = 1 − _θ, that is θ =_ 2[1] [. In such case we have]

1
_Jshare[∗]_ [=][ J] = 2 2[−][n/][2] 2[−][n/][2] = [2]

2 _·_ _·_ 2[n][,]

 

which finishes the proof.

C DERIVATION AND ANALYSIS OF ALGORITHM 1

C.1 RECAP OF EXISTING RESULTS

**Lemma 7 (Performance Difference). Let ¯π and π be two policies. Then, the following identity**
_holds,_

_J(¯π)_ _J(π) = Es_ _ρπ¯_ _[,][a][∼]π[¯]_ [[][A]π[(][s][,][ a][)]][ .]
_−_ _∼_

_Proof. See Kakade & Langford (2002) (Lemma 6.1) or Schulman et al. (2015a) (Appendix A)._

**Theorem 1. (Schulman et al., 2015a, Theorem 1)** _Let π be the current policy and ¯π be_
_the next candidate policy._ _We define Lπ(¯π) = J(π) + Es_ _ρπ,a_ _π¯_ [[][A]π[(][s, a][)]][,][ D][max]KL [(][π,][ ¯]π) =
_∼_ _∼_
maxs DKL (π(·|s), ¯π(·|s)) . Then the inequality of

_J(¯π)_ _Lπ(¯π)_ _CD[max]KL_ _π, ¯π_ (2)
_≥_ _−_

_holds, where C =_ [4][γ][ max](1[s,a]−[ |]γ[A])[2][π][(][s,a][)][|] _._   

_Proof. See Schulman et al. (2015a) (Appendix A and Equation (9) of the paper)._

C.2 ANALYSIS OF TRAINING OF ALGORITHM 1

**Lemma 1 (Multi-Agent Advantage Decomposition). In any cooperative Markov games, given a**
_joint policy π, for any state s, and any agent subset i1:m, the below equations holds._


_A[i]π[1:][m]_ _s, a[i][1:][m]_ []
 


_m_

_A[i]π[j]_ _s, a[i][1:][j][−][1]_ _, a[i][j]_ []

_j=1_

X  


_Proof. By the definition of multi-agent advantage function,_

_A[i]π[1:]θ[m][(][s,][ a][i][1:][m]_ [) =][ Q]π[i][1:]θ[m][(][s,][ a][i][1:][m][)][ −] _[V][π]θ_ [(][s][)]


_Q[i]π[1:]θ[k]_ [(][s,][ a][i][1:][k] [)][ −] _[Q]π[i][1:]θ[k][−][1]_ (s, a[i][1:][k][−][1] )


_k=1_


= _A[i]π[k]θ_ [(][s,][ a][i][1:][k][−][1] _[, a][i][k]_ [)][,]

_k=1_

X

which finishes the proof.
Note that a similar finding has been shown in Kuba et al. (2021).

**Lemma 8. Let π =** _i=1_ _[π][i][ and][ ¯]π =_ _i=1_ _π[¯][i]_ _be joint policies. Then_

_n_

[Q][n] D[max]KL [(][Q][π][,][n][ ¯]π) ≤ D[max]KL _π[i], ¯π[i][]_

_i=1_

X  


-----

_Proof. For any state s, we have_

DKL (π( _s), ¯π(_ _s)) = Ea_ **_π [log π(a_** _s)_ log ¯π(a _s)]_

_·|_ _·|_ _∼_ _|_ _−_ _|_

_n_ _n_

= Ea **_π_** log _π[i](a[i]_ _s)_ log _π¯[i](a[i]_ _s)_
_∼_ " _i=1_ _|_ ! _−_ _i=1_ _|_ !#

Y Y

_n_ _n_

= Ea **_π_** log π[i](a[i] _s)_ log ¯π[i](a[i] _s)_
_∼_ " _i=1_ _|_ _−_ _i=1_ _|_ #

X X


_n_

= Eai∼πi,a−i∼π−i log π[i](a[i]|s) − log ¯π[i](a[i]|s)

_i=1_

Now, taking maximum overX _s on both sides yields[]_


DKL _π[i](·|s), ¯π[i](·|s)_
_i=1_

X  


_n_

D[max]KL _π[i], ¯π[i][]_
_i=1_

X  


D[max]KL [(][π][,][ ¯]π) ≤


as required.


**Lemma 2. Let π be a joint policy. Then, for any joint policy ¯π, we have**


_L[i]π[1:][m]_ **_π¯_** _[i][1:][m][−][1]_ _, ¯π[i][m][]_ _−_ _CD[max]KL_ [(][π][i][m] _[,][ ¯]π[i][m])_
 


_J(π¯) ≥_ _J(π) +_

_m=1_

X

_Proof. By Theorem 1_


_J(π¯) ≥_ _Lπ(π¯) −_ _CD[max]KL_ [(][π][,][ ¯]π)
= J(π) + Es _ρπ,a_ **_π¯_** [[][A]π[(][s][,][ a][)]][ −] _[C][D][max]KL_ [(][π][,][ ¯]π)
_∼_ _∼_
which by Lemma 1 equals


_n_

_A[i]π[m]_ s, a[i][1:][m][−][1] _, a[i][m]_ []
_m=1_

X  


_−_ _CD[max]KL_ [(][π][,][ ¯]π)


= J(π) + Es _ρπ,a_ **_π¯_**
_∼_ _∼_

and by Lemma 8 this is at least


_n_ _n_

_J(π) + Es_ _ρπ,a_ **_π¯_** _A[i]π[m]_ s, a[i][1:][m][−][1] _, a[i][m]_ [] _CD[max]KL_ [(][π][i][m][,][ ¯]π[i][m])
_≥_ _∼_ _∼_ "m=1 # _−_ _m=1_

_n_ X   X _n_

= J(π) + Es _ρπ,ai1:m−1_ **_π¯_** _[i][1:][m][−][1]_ _,a[im]_ _π¯[im]_ _A[i]π[m]_ s, a[i][1:][m][−][1] _, a[i][m][]_ _CD[max]KL_ [(][π][i][m][,][ ¯]π[i][m])

_∼_ _∼_ _∼_ _−_
_m=1_ _m=1_

Xn    X

= J(π) + _L[i]π[1:][m]_ **_π¯_** _[i][1:][m][−][1]_ _, ¯π[i][m][]_ _−_ _CD[max]KL_ [(][π][i][m][,][ ¯]π[i][m] ) _._

_m=1_

X     

**Theorem 2. A sequence (πk)[∞]k=0** _[of joint policies updated by Algorithm][ 1][ has the monotonic im-]_
_provement property, i.e., J(πk+1) ≥_ _J(πk) for all k ∈_ N.

_Proof. Let π0 be any joint policy. For every k ≥_ 0, the joint policy πk+1 is obtained from πk by
Algorithm 1 update; for m = 1, . . ., n,


_L[i]π[1:]k[m]_ **_πk[i][1:]+1[m][−][1]_** _, π[i][m][]_ _CD[max]KL_ _πk[i][m]_ _[, π][i][m][i]_
_−_
  


_πk[i][m]+1_ [= arg max]
_π[im]_


-----

By Theorem 1, we have
_J(πk+1) ≥_ _Lπk_ (πk+1) − _CD[max]KL_ [(][π][k][,][ π][k][+1][)][,]
which by Lemma 8 is lower-bounded by


_CD[max]KL_ [(][π]k[i][m] _[, π]k[i][m]+1[)]_
_m=1_

X

_L[i]π[1:]k[m]_ [(][π]k[i][1:]+1[m][−][1] _, πk[i][m]+1[)][ −]_ _[C][D]KL[max][(][π]k[i][m]_ _[, π]k[i][m]+1[)]_ _,_ (15)
 


_Lπk_ (πk+1)
_≥_ _−_

_n_

= J(πk) +

_m=1_

X


and as for every m, πk[i][m]+1 [is the argmax, this is lower-bounded by]


_L[i]π[1:]k[m]_ [(][π]k[i][1:]+1[m][−][1] _, πk[i][m][)][ −]_ _[C][D]KL[max][(][π]k[i][m][, π]k[i][m]_ [)]


_J(πk) +_
_≥_


_m=1_


which, as mentioned in Definition 2, equals


= J(πk) +


0 = J(πk),
_m=1_

X


where the last inequality follows from Equation (6). This proves that Algorithm 1 achieves monotonic improvement.

C.3 ANALYSIS OF CONVERGENCE OF ALGORITHM 1

**Theorem 3. Supposing in Algorithm 1 any permutation of agents has a fixed non-zero probability to**
_begin the update, a sequence (πk)[∞]k=0_ _[of joint policies generated by the algorithm, in a cooperative]_
_Markov game, has a non-empty set of limit points, each of which is a Nash equilibrium._

_Proof. Step 1 (convergence)._ Firstly, it is clear that the sequence (J(πk))[∞]k=0 [converges as, by]
Theorem 2, it is non-decreasing and bounded above by _[R]1_ [max]γ [. Let us denote the limit by][ ¯]J. For every

_−_
_k, we denote the tuple of agents, according to whose order the agents perform the sequential updates,_
by i[k]1:n[, and we note that] _i[k]1:n_ _k∈N_ [is a random process. Furthermore, we know that the sequence]

of policies (πk) is bounded, so by Bolzano-Weierstrass Theorem, it has at least one convergent

  

subsequence. Let ¯π be any limit point of the sequence (note that the set of limit points is a random
set), and **_πkj_** _∞j=0_ [be a subsequence converging to][ ¯]π (which is a random subsequence as well). By
continuity of J in π (Corollary 1), we have
  


= limj→∞ _[J]_ **_πkj_** = J.[¯] (16)
  


_J(¯π) = J_ lim
j→∞ **_[π][k][j]_**

For now, we introduce an auxiliary definition.


**Definition 5 (TR-Stationarity). A joint policy ¯π is trust-region-stationary (TR-stationary) if, for**
_every agent i,_
_π¯[i]_ = arg maxπ[i] Es∼ρπ¯ _[,][a][i][∼][π][i][ ]A[i]π¯_ [(][s][,][ a][i][)] _−_ _Cπ¯_ [D][max]KL _π¯[i], π[i][]_ _,_

4γϵ    
_where Cπ¯_ [=] (1−γ)[2][, and][ ϵ][ = max][s,][a][ |][A]π[¯] [(][s,][ a][)][|][.]

We will now establish the TR-stationarity of any limit point joint policy ¯π (which, as stated above,
is a random variable). Let Ei0:1:∞n [[][·][]][ denote the expected value operator under the random process]
(i[0:]1:[∞]n [)][. Let also][ ϵ][k] [= max][s,][a] _k_ [(][s,][ a][)][|][, and][ C][k] [=] (14γϵγk)[2][ . We have]

_[|][A][π]_ _−_

0 = lim
_k→∞_ [E][i]1:[0:]n[∞] [[][J][(][π][k][+1][)][ −] _[J][(][π][k][)]]_

_≥_ _klim_ 1:n [[][L][π][k] [(][π][k][+1][)][ −] _[C][k][D]KL[max][(][π][k][,][ π][k][+1][)]][ by Theorem][ 1]_
_→∞_ [E][i][0:][∞]

_≥_ _klim_ 1:n _L[i]π1[k]k_ _πk[i]1[k]+1_ _−_ _CkD[max]KL_ _πk[i]1[k]_ _[, π]k[i]1[k]+1_
_→∞_ [E][i][0:][∞]

h    i

by Equation (15) and the fact that each of its summands is non-negative


-----

Now, we consider an arbitrary limit point ¯π from the (random) limit set, and a (random) subsequence
**_πkj_** _∞j=0_ [that converges to][ ¯]π. We get

_kj_ _kj_ _kj_ _kj_

   _i1_ _i1_ _i1_ _i1_

0 ≥ _jlim_ 1:n _Lπkj_ _πkj_ +1 _−_ _Ckj_ D[max]KL _πkj_ _[, π]kj_ +1 _._
_→∞_ [E][i][0:][∞]     

As the expectation is taken of non-negative random variables, and for every i ∈N and k ∈ N, with
some positive probability pi, we have i[k]1[j] [=][ i][ (because every permutation has non-zero probability),]
the above is bounded from below by

_pi limj→∞_ [max]π[i] _L[i]πkj_ [(][π][i][)][ −] _[C][k]j_ [D][max]KL _πk[i]_ _j_ _[, π][i][i]_ _,_

h 

which, as πkj converges to ¯π, equals to

_pi max_ _L[i]π¯_ [(][π][i][)][ −] _[C]π[¯]_ [D][max]KL _π¯[i], π[i][]_ 0, by Equation (6).
_π[i]_ _≥_

For convergence of L[i]πkj [(][π][i][)][ we used Definition]  [ 2][ combined with Lemma][ 6][, for convergence]
of Ckj we used Corollary 1, and the convergence of D[max]KL [follows from continuity of D][KL] [and]
max. This proves that, for any limit point ¯π of the random process (πk) induced by Algorithm 1,
maxπi _L[i]π¯_ [(][π][i][)][ −] _[C]π[¯]_ [D][max]KL _π¯[i], π[i][]_ = 0, which is equivalent with Definition 5.

**Step 2 (dropping the penalty term).cooperative Markov games. The main step is to prove the following statement:[]**   Now, we have to prove that TR-stationary points are NEs of a TR-stationary joint
_policy ¯π, for every state s ∈S, satisfies_
_π¯[i]_ = arg max Eai _πi_ []A[i]π¯ [(][s,][ a][i][)] _._ (17)
_π[i]_ _∼_

We will use the technique of the proof by contradiction. Suppose that there is a state _s0 such that_
there exists a policy ˆπ[i] with
Eai _πˆ[i]_ []A[i]π¯ [(][s][0][,][ a][i][)] _> Eai_ _π¯[i]_ []A[i]π¯ [(][s][0][,][ a][i][)] _._ (18)
_∼_ _∼_
Let us parametrise the policies π[i] according to the template
 

_d[i]−1_

_π[i](_ _s0) =_ _x[i]1[, . . ., x][i]d[i]_ 1[,][ 1][ −] _x[i]j_

_·|_ _−_

_j=1_

  X 

where the values of x[i]j [(][j][ = 1][, . . ., d][i][ −] [1)][ are such that][ π][i][(][·|][s][0][)][ is a valid probability distribution.]
Then we can rewrite our quantity of interest (the objective of Equation (17) as


_d[i]−1_

_x[i]j_ _π¯_ _s0, a[i]j_ + (1
_j=1_ _[·][ A][i]_ _−_

X   


_d[i]−1_

_x[i]h[)][A]π[i]¯_ _s0, a[i]d[i]_
_h=1_

X  


Eai _πi_ []A[i]π¯ [(][s][0][,][ a][i][)]
_∼_


_d[i]−1_

= _x[i]j_ _A[i]π¯_ _s0, a[i]j_ _A[i]π¯_ _s0, a[i]d[i]_ + A[i]π¯ _s0, a[i]d[i]_ _,_

_−_
_j=1_

X          

which is an affine function of the policy parameterisation. It follows that its gradient (with respect to
_x[i]) and directional derivatives are constant in the space of policies at state s0. The existance of policy_
_πˆ[i](_ _s0), for which Inequality (18) holds, implies that the directional derivative in the direction from_

_·|_
_π¯[i](_ _s0) to ˆπ[i](_ _s0) is strictly positive. We also have_

_·|_ _·|_
_∂DKL(¯π[i](_ _s0), π[i](_ _s0))_ _∂_

_·|_ _·|_ = (¯π[i]( _s0))[T]_ (log ¯π[i]( _s0)_ log π[i]( _s0))_

_∂x[i]j_ _∂x[i]j_ _·|_ _·|_ _−_ _·|_

 


_∂_
= (¯π[i])[T] log π[i][] (omitting state s0 for brevity)

_∂x[i]j_ _−_

 _di−1_ _∂_ _di−1_

= _π¯k[i]_ [log][ x]k[i] _π¯d[i]_ _i_ [log] 1 _x[i]k_
_−_ _∂x[∂]_ _[i]j_ _k=1_ _[−]_ _∂x[i]j_ _−_ _k=1_ !

X X

= _πj[i]_ + _π¯d[i]_ _i_
_−_ _x[¯][i]j_ 1 − [P]k[d][i]=1[−][1] _[x]k[i]_

= _πj[i]_ + π[¯]d[i] _i_ = 0, when evaluated at π[i] = ¯π[i], (19)
_−_ _π[¯]j[i]_ _πd[i]_ _i_


-----

which means that the KL-penalty has zero gradient at ¯π[i]( _s0). Hence, when evaluated at π[i](_ _s0) =_

_·|_ _·|_
_π¯[i](_ _s0), the objective_

_·|_

_ρπ¯_ [(][s]0[)][E]a[i] _π[i]_ []A[i]π¯ [(][s][0][,][ a][i][)] _Cπ¯_ [D]KL _π¯[i](_ _s0), π[i](_ _s0)_
_∼_ _−_ _·|_ _·|_

has a strictly positive directional derivative in the direction of   ˆπ[i]( _s0). Thus, there exists a policy_

_·|_
_π[i](_ _s0), sufficiently close to ¯π[i](_ _s0) on the path joining it with ˆπ[i](_ _s0), for which_

_·|_ _·|_ _·|_

_ρπ¯_ [(][s]0[)][E]a[i] _π[i]_ []A[i]π¯ [(][s][0][,][ a][i][)] _Cπ¯_ [D]KL _π¯[i](_ _s0),_ _π[i](_ _s0)_ _> 0._

e _∼_ _−_ _·|_ _·|_

Let π∗[i] [be a policy such that][ π]e∗[i] [(][·|][s][0][) =][ e]π[i](·|s0), and π ∗[i] [(][·|][s][) = ¯] eπ[i](·|s) for states _s ̸= s0. As for_
these states we have

_ρπ¯_ [(][s][)][E]a[i] _π[i]_ _A[i]π¯_ [(][s,][ a][i][)] = ρπ¯ [(][s][)][E]a[i] _π¯[i]_ []A[i]π¯ [(][s,][ a][i][)] = 0, and DKL(¯π[i]( _s), π[i]_
_∼_ _∗_ _∼_ _·|_ _∗[(][·|][s][)) = 0][,]_

it follows that   

_L[i]π¯_ [(][π][i] **_π[D][max]KL_** [(¯]π[i], π[i] **_π[(][s]0[)][E]a[i]_** _π[i]_ []A[i]π¯ [(][s][0][,][ a][i][)] _Cπ¯_ [D]KL _π¯[i](_ _s0),_ _π[i](_ _s0)_
_∗[)][ −]_ _[C][¯]_ _∗[) =][ ρ][¯]_ _∼_ _−_ _·|_ _·|_

_> 0 = L[i]π¯_ [(¯]π[i])e _Cπ¯_ [D][max]KL [(¯]π[i], ¯π[i]),   
_−_

e

which is a contradiction with TR-stationarity of ¯π. Hence, the claim of Equation (17) is proved.
**Step 3 (optimality). Now, for a fixed joint policy ¯π[−][i]** of other agents, ¯π[i] satisfies

_π¯[i]_ = arg max Eai _πi_ []A[i]π¯ [(][s,][ a][i][)] = arg max Eai _πi_ []Q[i]π¯ [(][s,][ a][i][)] _,_ _s_ _,_
_π[i]_ _∼_ _π[i]_ _∼_ _∀_ _∈S_
 

which is the Bellman optimality equation (Sutton & Barto, 2018). Hence, for a fixed joint policy
**_π¯_** _[−][i], the policy ¯π[i]_ is optimal:

_π¯[i]_ = arg max _J(π[i], ¯π[−][i])._
_π[i]_

As agent i was chosen arbitrarily, ¯π is a Nash equilibrium.


-----

D HATRPO AND HAPPO

D.1 PROOF OF PROPOSITION 2

**Proposition 2. Let π =** _j=1_ _[π][i][j][ be a joint policy, and][ A][π][(][s][,][ a][)][ be its joint advantage function.]_
_Let ¯π[i][1:][m][−][1]_ = _j=1_ _π[¯][i][j]_ _be some other joint policy of agents i1:m−1, and ˆπ[i][m]_ _be some other policy_
_of agent im. Then, for every state[Q][n]_ _s,_

Eai1:m−1 _∼π¯[Q][i][1:][m][m][−][−][1][1]_ _,a[im]_ _∼πˆ[im]_ _A[i]π[m]_ _s, a[i][1:][m][−][1]_ _, a[i][m]_ []
   ˆπ[i][m] (a[i][m] _s)_ **¯π[i][1:][m][−][1]** (a[i][1:][m][−][1] _s)_

= Ea **_π_** _|_ _|_ _._ (9)
_∼_ h _π[i][m]_ (a[i][m] _|s)_ _[−]_ [1] **_π[i][1:][m][−][1]_** (a[i][1:][m][−][1] _|s)_ _[A][π][(][s,][ a][)]i_


_Proof. We have_

**¯π[i][1:][m]** (a[i][1:][m] _s)_ **_π[i][1:][m][−][1]_** (a[i][1:][m][−][1] _s)_

= Ea **_π_** _|_ _|_
_∼_ **_π[i][1:][m]_** (a[i][1:][m] _s)_ _[A][π][(][s,][ a][)][ −]_ **_π[¯]_** _[i][1:][m][−][1]_ (a[i][1:][m][−][1] _s)_ _[A][π][(][s,][ a][)]_

 _|_ _|_ 

**¯π[i][1:][m](a[i][1:][m]** _s)_

= Eai1:m _∼πi1:m_ _,a−i1:m_ _∼π−i1:m_ **_π[i][1:][m](a[i][1:][m]_** _|s)_ _[A][π][(][s,][ a][i][1:][m][,][ a][−][i][1:][m]_ [)]

 _|_ 

**¯π[i][1:][m][−][1]** (a[i][1:][m][−][1] _s)_

_−_ Eai1:m−1 _∼πi1:m−1_ _,a−i1:m−1_ _∼π−i1:m−1_ **_π[i][1:][m][−][1]_** (a[i][1:][m][−][1] _|s)_ _[A][π][(][s,][ a][i][1:][m][−][1]_ _[,][ a][−][i][1:][m][−][1]_ [)]

 _|_

= Eai1:m **_π¯_** _[i][1:][m]_ _,a[−][i][1:][m]_ **_π[−][i][1:][m]_** _Aπ(s, a[i][1:][m]_ _, a[−][i][1:][m])_
_∼_ _∼_

Eai1:m−1 **_π¯_** _[i][1:][m][−][1]_ _,a[−][i][1:][m][−][1]_ π[−][i][1:][m][−][1] _Aπ(s, a[i][1:][m]_ _[−][1]_ _, a[−][i][1:][m][−][1]_ )
_−_ _∼_ _∼_

= Eai1:m _∼π¯_ _[i][1:][m]_ Ea−i1:m _∼π−i1:m_ _Aπ(s, a_ _[i][1:][m], a[−][i][1:][m]_ ) 

Eai1:m−1 **_π¯_** _[i][1:][m][−][1]_ Ea−i1:m−1 π−i1:m−1 _Aπ(s, a[i][1:][m][−][1]_ _, a[−][i][1:][m][−][1]_ )
_−_ _∼_ _∼_
  

= Eai1:m _∼π¯_ _[i][1:][m]_ _A[i]π[1:][m]_ (s, a[i][1:][m])

Eai1:m−1 **_π¯_** _[i][1:][m][−][1]_ _A[i]π[1:][m][−][1]_ (s, **a[i][1:][m][−][1]** ) _,_
_−_ _∼_

which, by Lemma 1, equals

 

= Eai1:m _∼π¯_ _[i][1:][m]_ _A[i]π[1:][m]_ (s, a[i][1:][m]) − _A[i]π[1:][m][−][1]_ (s, a[i][1:][m][−][1] )

= Eai1:m **_π¯_** _[i][1:][m]_ A[i]π[m][(][s,][ a][i][1:][m][−][1] _[,][ a][i][m][)]_ _._ 
_∼_
 


D.2 DERIVATION OF THE GRADIENT ESTIMATOR FOR HATRPO


_πθ[i][m][im]_ [(][a][i][m][|][s][)]

_π[i][m]_
_θk[im]_ [(][a][i][m] _[|][s][)][ −]_ [1]


_∇θim Es∼ρπθk,a∼πθk_ "

= ∇θim Es∼ρπθk,a∼πθk


_M_ _[i][1:][m](s, a)_


_πθ[i][m][im]_ [(][a][i][m] _[|][s][)]_

_π[i][m]_
_θk[im]_ [(][a][i][m] _[|][s][)]_ _[M][ i][1:][m][(][s][,][ a][)]_


_−∇θim Es∼ρπθk,a∼πθk_


_M_ _[i][1:][m](s, a)_


_∇θim_ _πθ[i][m][im]_ [(][a][i][m] _[|][s][)]_ _M_ _[i][1:][m]_ (s, a)

_πθ[i][m]k[im]_ [(][a][i][m][|][s][)] #

_πθ[i][m][im][ (][a][i][m]_ _[|][s][)]_

_θ[im]_ [(][a][i][m][|][s][)][M][ i][1:][m] [(][s][,][ a][)]
_π[i][m]_
_θk[im]_ [(][a][i][m] _[|][s][)]_ _[∇][θ][im][ log][ π][i][m]_


= Es∼ρπθk,a∼πθk

= Es∼ρπθk,a∼πθk


Evaluated at θ[i][m] = θk[i][m][, the above expression equals]


Es∼ρπθk,a∼πθk

which finishes the derivation.


_M_ _[i][1:][m](s, a)∇θim log πθ[i][m][im]_ [(][a][i][m][|][s][)] _θ[im]_ =θk[im]


-----

D.3 PSEUDOCODE OF HATRPO

**Algorithm 2 HATRPO**

1: Input: Stepsize α, batch size B, number of: agents n, episodes K, steps per episode T, possible
steps in line search L, line search acceptance threshold κ.

2: Initialize: Actor networks {θ0[i] _[,][ ∀][i][ ∈N}][, Global V-value network][ {][φ][0][}][, Replay buffer][ B]_

3: for k = 0, 1, . . ., K − 1 do
4: Collect a set of trajectories by running the joint policy πθk = (πθ[1]k[1] _[, . . ., π]θ[n]k[n]_ [)][.]

5: Push transitions {(o[i]t[, a][i]t[, o][i]t+1[, r][t][)][,][ ∀][i][ ∈N] _[, t][ ∈]_ _[T]_ _[}][ into][ B][.]_

6: Sample a random minibatch of B transitions from B.

7: Compute advantage function _A[ˆ](s, a) based on global V-value network with GAE._

8: Draw a random permutation of agents i1:n.

9: Set M _[i][1]_ (s, a) = A[ˆ](s, a).

10: **for agent im = i1, . . ., in do**

11: Estimate the gradient of the agent’s maximisation objective


**_gˆk[i][m]_** = _B[1]_ _b=1_ _t=1_ _∇θkim_ log πθ[i][m]k[im] _a[i]t[m]_ _| o[i]t[m]_ _M_ _[i][1:][m]_ (st, at).

Use the conjugate gradient algorithm to compute the update directionP P   

**_x[i]k[m]_** _≈_ ( H[ˆ] _k[i][m][)][−][1][g]k[i][m]_ [,]
where **_H[ˆ]_** _k[i][m]_ is the Hessian of the average KL-divergence


_BT1_ _b=1_ _t=1_ DKL _πθ[i][m]k[im]_ [(][·|][o]t[i][m][)][, π]θ[i][m][im] [(][·|][o]t[i][m] [)] .

12: Estimate the maximal step size allowing for meeting the KL-constraintP P  


2δ

(ˆx[i]k[m][)][T][ ˆ]Hk[i][m] **_x[ˆ][i]k[m]_**


_βˆk[i][m]_


13: Update agent im’s policy by

_θk[i][m]+1_ [=][ θ]k[i][m] + α[j][ ˆ]βk[i][m] **_x[ˆ][i]k[m]_** [,]
where j ∈{0, 1, . . ., L} is the smallest such j which improves the sample loss by at least
_κα[j][ ˆ]βk[i][m]_ **_x[ˆ][i]k[m]_** **_gk[i][m]_** [, found by the backtracking line search.]

_[·][ ˆ]_ _π[im]_

_θk[im]+1_ [(][a][im] _[|][o][im]_ [)]

14: Compute M _[i][1:][m][+1]_ (s, a) =

_π[im]_ (a[im] o[im] ) _[M][ i][1:][m][(][s][t][,][ a][t][)][. //][Unless][ m][ =][ n][.]_
_θk[im]_ _|_

15: **end for**

16: Update V-value network by following formula:

_B_ _T_ 2

_φk+1 = arg minφ_ _BT1_ _Vφ(st)_ _Rt_

_b=1_ _t=0_ _−_ [ˆ]

17: end for P P  


-----

D.4 PSEUDOCODE OF HAPPO

**Algorithm 3 HAPPO**

1: Input: Stepsize α, batch size B, number of: agents n, episodes K, steps per episode T .
2: Initialize: Actor networks {θ0[i] _[,][ ∀][i][ ∈N}][, Global V-value network][ {][φ][0][}][, Replay buffer][ B]_

3: for k = 0, 1, . . ., K − 1 do
4: Collect a set of trajectories by running the joint policy πθk = (πθ[1]k[1] _[, . . ., π]θ[n]k[n]_ [)][.]

5: Push transitions {(o[i]t[, a][i]t[, o][i]t+1[, r][t][)][,][ ∀][i][ ∈N] _[, t][ ∈]_ _[T]_ _[}][ into][ B][.]_

6: Sample a random minibatch of B transitions from B.

7: Compute advantage function _A[ˆ](s, a) based on global V-value network with GAE._

8: Draw a random permutation of agents i1:n.

9: Set M _[i][1]_ (s, a) = A[ˆ](s, a).

10: **for agent im = i1, . . ., in do**

11: Update actor i[m] with θk[i][m]+1[, the argmax of the PPO-Clip objective]


_BT1_ _b=1B_ _t=0T_ min _ππθθ[im][im]k[im][im]_ [(][(][a][a]t[im]t[im] _||oo[im]t[im]t_ [)][)] _[M][ i][1:][m][(][s][t][,][ a][t][)][,][ clip]_ _ππθθ[im][im]k[im][im]_ [(][(][a][a]t[im]t[im] _||oo[im]t[im]t_ [)][)] _[,][ 1][ ±][ ϵ]M_ _[i][1:][m]_ (st, at)

P P _π[im]_

_θk[im]+1_ [(][a][im] _[|][o][im]_ [)]

12: Compute M _[i][1:][m][+1]_ (s, a) =

_π[im]_ (a[im] o[im] ) _[M][ i][1:][m][(][s][,][ a][)][. //][Unless][ m][ =][ n][.]_
_θk[im]_ _|_

13: **end for**

14: Update V-value network by following formula:

_B_ _T_ 2

_φk+1 = arg minφ_ _BT1_ _Vφ(st)_ _Rt_

_b=1_ _t=0_ _−_ [ˆ]

15: end for P P  

E HYPER-PARAMETER SETTINGS FOR EXPERIMENTS

|hyperparameters value|hyperparameters value|hyperparameters value|
|---|---|---|


|critic lr 5e-4 gamma 0.95 gain 0.01 actor network mlp hypernet embed 64 activation ReLU|optimizer Adam optim eps 1e-5 hidden layer 1 num mini-batch 1 max grad norm 10 hidden layer dim 64|stacked-frames 1 batch size 3200 training threads 32 rollout threads 20 episode length 160 use huber loss True|
|---|---|---|



Table 1: Common hyperparameters used in the SMAC domain.

|Algorithms|MAPPO HAPPO HATRPO|
|---|---|


|actor lr ppo epoch kl-threshold ppo-clip accept ratio|5e-4 5e-4 / 5 5 / / / 0.06 0.2 0.2 / / / 0.5|
|---|---|



Table 2: Different hyperparameters used for MAPPO, HAPPO and HATRPO in the SMAC

The implementation of MADDPG is adopted from the Tianshou framework (Weng et al., 2021), all
hyperparameters left unchanged at the origin best-performing status.


-----

|hyperparameters value|hyperparameters value|hyperparameters value|
|---|---|---|


|critic lr 5e-3 gamma 0.99 gain 0.01 std y coef 0.5 std x coef 1 activation ReLU|optimizer Adam optim eps 1e-5 hidden layer 1 actor network mlp max grad norm 10 hidden layer dim 64|num mini-batch 40 batch size 4000 training threads 8 rollout threads 4 episode length 1000 eval episode 32|
|---|---|---|


Table 3: Common hyperparameters used for IPPO, MAPPO, HAPPO, HATRPO in the Multi-Agent
MuJoCo domain

|Algorithms|IPPO MAPPO HAPPO HATRPO|
|---|---|


|actor lr ppo epoch kl-threshold ppo-clip accept ratio|5e-6 5e-6 5e-6 / 5 5 5 / / / / [1e-4,1.5e-4,7e-4,1e-3] 0.2 0.2 0.2 / / / / 0.5|
|---|---|



Table 4: Different hyperparameters used for IPPO, MAPPO, HAPPO and HATRPO in the MultiAgent MuJoCo domain.

|hyperparameters value|hyperparameters value|hyperparameters value|
|---|---|---|


|actor lr 1e-3 critic lr 1e-3 gamma 0.99 tau 5e-2 start-timesteps 25000 epoch 200|optimizer Adam exploration noise 0.1 step-per-epoch 50000 step-per-collector 2000 update-per-step 0.05 hidden-sizes [256,256]|buffer size 1e6 batch size 200 training num 16 test num 10 n-step 1 episode length 1000|
|---|---|---|



Table 5: Hyper-parameter used for MADDPG in the Multi-Agent MuJoCo domain

|task value|task value|task value|
|---|---|---|


|Ant(2x4) 1e-4 HalfCheetah(2x3) 1e-4 Walker(2x3) 1e-3 Humanoid(17x1) 7e-4|Ant(4x2) 1e-4 HalfCheetah(3x2) 1e-4 Walker(3x2) 1e-4 Humanoid-Standup(17x1) 1e-4|Ant(8x1) 1e-4 HalfCheetah(6x1) 1e-4 Walker(6x1) 1e-4 Swimmer(10x2) 1.5e-4|
|---|---|---|



Table 6: Parameter kl-threshold used for HATRPO in the Multi-Agent MuJoCo domain

|hyperparameters value|hyperparameters value|hyperparameters value|
|---|---|---|


|lr 7e-4 gamma 0.99 gain 0.01 max grad norm 10 hidden layer dim 64|optimizer Adam optim eps 1e-5 hidden layer 1 actor network rnn activation ReLU|num mini-batch 1 eval episode 32 training threads 1 rollout threads 128 episode length 25|
|---|---|---|



Table 7: Common hyperparameters used for MAPPO, HAPPO in the Multi-Agent Particle Environment


-----

ABLATION EXPERIMENTS


In this section, we conduct ablation study to investigate the importance of two key novelties that our
HATRPO introduced; they are heterogeneity of agents’ parameters and the randomisation of order
of agents in the sequential update scheme. We compare the performance of original HATRPO with
a version that shares parameters, and with a version where the order in sequential update scheme is
fixed throughout training. We run the experiments on two MAMuJoCo tasks (2-agent & 6-agent).

Walker 2x3 Walker 6x1


5000

4000

3000

2000

1000

|HATRPO (original) HATRPO (shared parameter HATRPO (no random order) MAPPO|)|
|---|---|
|||
|||
|||
|||


0.0 0.2 0.4 0.6 0.8 1.0

HATRPO (original)
HATRPO (shared parameter)
HATRPO (no random order)
MAPPO

Environment steps 1e7

(b) 6-Agent Walker


Walker 2x3

5000

4000

3000

2000

1000

Average Episode Reward

0

0.0 0.2 0.4 0.6 0.8 1.0

Environment steps 1e7

(a) 2-Agent Walker


Figure 4: Performance comparison between original HATRPO, and its modified versions: HATRPO
with parameter sharing, and HATRPO without randomisation of the sequential update scheme.

The experiments reveal that, although the modified versions of HATRPO still outperform baselines
(represented by MAPPO), their deviation from the theory harms performance. In particular, pa_rameter sharing introduces extra variance to training, harms the monotonic improvement property_
(Theorem 2 assumes heterogeneity), and causes HATRPO to converge to suboptimal policies. The
suboptimality is more severe in the task with more agents, as suggested by Proposition 1. Similarly,
_fixed order in the sequential update scheme negatively affected the performance at convergence_
(especially in the task with 6 agents), as suggested by Proposition 3. We conclude that the fine performance of HATRPO relies strongly on the close connection between theory an implementation.
The connection becomes increasingly important with the growing number of agents.


ABLATION STUDY ON NON-PARAMETER SHARING MAPPO/IPPO


We verify that heterogeneous-agent trust region algorithms (represented by HATRPO) achieve superior performance to, originally homogeneous, IPPO/MAPPO algorithms with the parameter-sharing
function switched off.

Halfcheetah 3x2 Walker 3x2


4000

3000


2000

1000

|MAPPO (No share param.)|Col2|
|---|---|
|IPPO (No share param.) HATRPO||
|||
|||
|||
|||


0.0 0.2 0.4 0.6 0.8 1.0

MAPPO (No share param.)
IPPO (No share param.)
HATRPO

Environment steps 1e7

(b) 3-Agent Walker


Halfcheetah 3x2

5000

4000

3000

2000

1000

Average Episode Reward

0

0.0 0.2 0.4 0.6 0.8 1.0

Environment steps 1e7

(a) 3-Agent HalfCheetah


Figure 5: Performance comparison between HATRPO and MAPPO/IPPO without parameter sharing. HATRPO significantly outperforms its counterparts.


-----

MULTI-AGENT PARTICLE ENVIRONMENT EXPERIMENTS


We verify that heterogeneous-agent trust region algorithms (represented here by HAPPO) quickly
solve cooperative MPE tasks.

Spread Reference


50

100

150

200

250

|Col1|MAPPO|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
||HAPPO|||||||
|||||||||
|||||||||
|||||||||
|||||||||
|||||||||


0.0 0.5 1.0 1.5 2.0 2.5 3.0

MAPPO
HAPPO

Environment steps 1e6

(b) Simple Reference


Spread

60

80

100

120

140

160

180

200

Average Episode Reward

220

0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00

Environment steps 1e7

(a) Simple Spread


Figure 6: Performance comparison between MAPPO and HAPPO on MPE.


-----

