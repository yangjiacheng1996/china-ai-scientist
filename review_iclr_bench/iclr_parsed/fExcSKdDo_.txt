# LEARNING TO DEQUANTISE WITH TRUNCATED FLOWS

**Shawn Tan & Chin-Wei Huang**
Mila, University of Montreal
_{jing.shan.shawn.tan,chin-wei.huang}@umontreal.ca_


**Alessandro Sordoni**
Microsoft Research
Montreal
alsordon@microsoft.com


**Aaron Courville**
Mila, University of Montreal
Canada CIFAR AI Chair
courvila@iro.umontreal.ca

ABSTRACT


Dequantisation is a general technique used for transforming data described by a
discrete random variable x into a continuous (latent) random variable z, for the purpose of it being modeled by likelihood-based density models. Dequantisation was
first introduced in the context of ordinal data, such as image pixel values. However,
when the data is categorical, the dequantisation scheme is not obvious. We learn
such a dequantisation scheme q(z|x), using variational inference with TRUncated
FLows (TRUFL) — a novel flow-based model that allows the dequantiser to have
a learnable truncated support. Unlike previous work, the TRUFL dequantiser is
(i) capable of embedding the data losslessly in certain cases, since the truncation
allows the conditional distributions q(z|x) to have non-overlapping bounded supports, while being (ii) trainable with back-propagation. Addtionally, since the
support of the marginal q(z) is bounded and the support of prior p(z) is not, we
propose to renormalise the prior distribution over the support of q(z). We derive
a lower bound for training, and propose a rejection sampling scheme to account
for the invalid samples. Experimentally, we benchmark TRUFL on constrained
generation tasks, and find that it outperforms prior approaches. In addition, we find
that rejection sampling results in higher validity for the constrained problems.

1 INTRODUCTION

Deep generative models aim to model a distribution of high-dimensional natural data. Many of these
methods assume that the data is continuous, despite it being digitally stored in bits and therefore
intrinsically discrete. This discrepancy has led to recent interest in dequantising discrete data types
to avoid some of the degeneracies of fitting continuous models to discrete data (Theis et al., 2015).
When data is ordinal (such as pixel intensities) a naive dequantisation scheme can be obtained by
adding uniform noise to the discrete values (Theis et al., 2015). More recently, a generalisation of
this approach where dequantisation is seen as inference in a latent variable model has also been
proposed (Ho et al., 2019; Hoogeboom et al., 2020; Nielsen et al., 2020). However, these methods
may not be directly applied in cases where the data is categorical (Hoogeboom et al., 2021), because
the data is not naturally represented in a vector space.

Attempts at devising dequantisation schemes for categorical data by building upon the variational
dequantisation scheme have been recently proposed in Hoogeboom et al. (2021) and Lippe &
Gavves (2020). These approaches dequantise a categorical input into a latent continuous space.
Ideally, a dequantisation scheme for categorical data should be: (i) easily learnable by standard
optimization techniques and (ii) possibly lossless, in the sense that quantisation should recover the
input category. Argmax Flow (Hoogeboom et al., 2021) offer lossless dequantisation but the support
of the stochastic embedding is chosen arbitrarily and not optimised, and the dimensionality of the
continuous (dequantised) variable is required to be at least logarithmic in the number of categories
of the input data. Moreover, the method makes minimal assumptions about the topology of the
categorical data, disregarding the possible relationships between categories, which can occur for
example between word indices in natural language (Bengio et al., 2003) or the atomic representations


-----

_q(zt_ _xt)_
_|_

_p(xt_ _zt)_
_|_

CatNF Argmax flow (thresh.) Argmax flow (binary) TRUFL

Figure 1: Decoder for CNFs (left) is the posterior of the encoder and the posteriors have unbounded
support resulting in probability mass in all categories in p(xt _zt). Argmax flow (center) have bounded_
_|_
supports that partitions the full latent space into regions where the posterior for each category is
deterministic, but (1) the naive thresholding (thresh.) requires the dimensionality of the space to be
equal to the number of categories, and (2) the binary encoding partitions data into separate octants,
sometimes leaving octants unsupported. TRUFL (right) allows bounded support for each category,
and for latent dimensions to be different from the category size.

of a molecule’s constituents. On the other hand, Categorical Normalizing Flows (CatNF; Lippe &
Gavves 2020) can learn a more compact representation of the input category but the dequantisation
might be lossy given that the posteriors over the continuous variables have overlapping support.

Is there a trade-off between these two schemes? In this paper, we propose TRUFL, which builds
upon the aforementioned variational dequantisation techniques. We achieve that by using truncated
posterior distributions over the continuous variables with potentially bounded and disjoint support. In
addition, we present a parametrisation of truncated distributions that can be optimised with standard
stochastic reparametrisation techniques. Overall, our method inherits strengths of both CatNF and
Argmax flow. Our experimental results highlight the effectiveness of our approach.

2 BACKGROUND: VARIATIONAL DEQUANTISATION

Dequantisation refers to the process of embedding discrete-valued data into a continuous space, which
allows us to employ density-based models to capture the distribution of the continuous representation.
Concretely, let z = _z1, . . ., zT_ denote this continuous representation, and x = _x1, . . ., xT_
describe the observed data, where each { _}_ _xt represent, e.g. a node in a graph or a token in a sentence. {_ _}_
Eachinterpreted as a latent variable, which follows a prior distribution xt is assumed to be categorical, i.e. xt ∈{0, · · ·, K − 1} for some integer p(z). We refer to K > q(z 1t .x zt) can be as the
_|_
_dequantiser and p(xt_ _zt) as the quantiser. Training can be achieved by maximizing a variational_
_|_
lower bound on the marginal likelihood of the data, i.e.:


log _[p][(][x][|][z][)][ p][(][z][)]_

_q(z|x)_


log p(x) Eq(z **_x)_**
_≥_ _|_


=: L(x) (1)


We are interested in the case where the representation zt can be inferred from xt alone, so we choose
the factorisation p(x|z) = _t_ _[p][(][x][t][|][z][t][)][ and][ q][(][z][|][x][) =][ Q]t_ _[q][(][z][t][|][x][t][)][, following][ Lippe & Gavves]_

(2020). In this case, the “optimal” quantiser p(xt _zt) can be conveniently computed as:_
_|_

[Q] _q(zt_ _xt) ˜p(xt)_

arg max Eq(x)[ (x)] = _K_ 1 _|_ = q(xt _zt) =: p(xt_ _zt)_ (2)
_p(xt|zt)_ _L_ _x[′]t−[=0][ q][(][z][t][|][x]t[′]_ [) ˜]p(x[′]t[)] _|_ _|_

where q(x) denotes the (empirical) data distribution, andP ˜p(xt) denotes the estimate of the marginal
distribution of each category (which can be obtained by counting and, in the case of textual data, this
corresponds to the unigram distribution over words). This equation shows that the optimal quantiser
can be obtained implicitly by applying Bayes’ rule with the parametric dequantiser q(zt _xt). The_
_|_
factorisation we chose for p(x|z) and q(z|x) is crucial for the arg max above to be represented in this


-----

simple form. Without this assumption, the solution will involve a combinatorial sum or an integral,
which results in the choice of a parametric quantiser in Ziegler & Rush (2019) for computational
tractability. Plugging the optimal decoder into Eq. 1 yields:

_p(z)_

_L(x) = Eq(z|x)_ _t_ log ˜p(xt) + log _Kx[′]t−[=0]1[ q][(][z][t][|][x]t[′]_ [)˜]p(x[′]t[)] # (3)

"X

P

We note that the first term is a constant. Therefore, the expression above implies that accurately
modelling the dependencies in x boils down to learning an expressive prior p(z) and regularising the
dequantiser q(zt|xt). q(xt|zt) is deterministic when q(zt|xt) does not overlap with other q(zt|x[′]t[)][,]
in which case q(zt _xt) is encouraged to be expanded to maximize the entropy. If there is certain_
_|_
amount of overlapping, the denominator in the second term will push down the density of other
_q(zt|x[′]t[)][, therefore resulting in a spikier aggregate posterior distribution (see more discussion on this]_
in Section 5.3). With this general framework that also accounts for lossy quantisation, we briefly
present some of the previously proposed strategies for dequantisation.

**Ordinal dequantisation** In the case where the data is ordinal, such as the case of image pixel
values (e.g. for an 8-bit representation, K = 256), a dequantisation scheme can be obtained by
setting q(zt|xt) = Uniform(xt, xt + 1). The resulting quantisation process is simply ⌊zt⌋, and is
deterministic. More generally, q(zt|xt) can be any distribution on [xt, xt +1]. See Nielsen & Winther
(2020); Hoogeboom et al. (2019) for extensions of the uniform dequantisation scheme.

**Argmax Flow** For categorical data, uniform dequantisation is not applicable, as there is no intrinsic
ordering between the categories. Argmax Flow (Hoogeboom et al., 2021) dequantise categorical
data by lettingthe support over the latent space is disjoint, zt ∈ R[K] be distributed by q(z pt|(xxtt) with supportzt) = q(xt _z {t) =zt : arg max 1[xt = arg maxk(zt)k =k x(ztt}). Whenk][1]; we_
_|_ _|_
depict this in Figure 1, Argmax flow (thresh.). Argmax Flow makes minimal assumptions on the
topology of the data: the support of the dequantiser partitions the continuous space evenly and the
representations are equally far from each other. As an example, synonyms in text may still have
very distinct dequantised representations despite having similar functions and meaning in a language
modelling setting. In the naive formulation, Argmax Flow requires the dimensionality to the latent
space to be the same as the number of the input categories K. To accomodate for larger categorical
spaces, the authors suggest a binary factorisation, reducing the required latent space dimension to
_⌈log2 K⌉; See Figure 1, Argmax flow (binary)._

**Categorical Normalising Flows (CatNF)** In the previous cases, the quantisation is deterministic,
and there is no loss of information. This is because in a cleanly partitioned latent space like in the
ordinal setting or argmax flow, the dequantising distributions q(zt|xt = k) for all 0 ≤ _k ≤_ _K −_ 1
have non-overlapping support. CatNF learns a dequantiser that can “softly” partition the space. Lippe
& Gavves (2020) propose using a conditional logistic distribution as q(zt _xt). In this case, the optimal_
_|_
quantiser q(xt _zt) is nearly-deterministic if the locations of the dequantisers are far away from each_
_|_
other and they have sufficiently small scale. For this reason, and unlike the first two approaches,
CatNF is not capable of losslessly dequantising the data (we provide a formal discussion on this in
Appendix A.2, which is based on an data-processing inequality argument, using the dequantiser as a
transitional kernel). It can approximate the lossless limit by pushing the bulk of the mass of q(zt _xt)_
_|_
away from each other, but that could potentially lead to a highly complex and multi-modal empirical
distribution over the representation space for p(z) to approximate.

Next, we consider the case where q(zt _xt) is a truncated distribution, and as such has the ability to_
_|_
encode the data losslessly while learning a meaningful latent topology of the different categories.

3 TRUNCATED DEQUANTISER AND PRIOR

The general approach to optimising the variational lower bound proposed in Kingma & Welling
(2014) involves sampling from the proposal distribution q(zt _xt) to estimate the expectation (Eq. 1)._
_|_
In our case, we want to parameterise this using a TRUncated FLow, which we will refer to as TRUFL.

1We use 1[·] to denote an indicator function.


-----

For simplicity, we will drop the dependency on t and xt, but all of the variational distribution is
conditioned on the categorical value xt.

If we want to bound a scalar distribution between (a, b), and we have a density function f where its
cumulative distribution function F (CDF) and the inverse of its CDF F _[−][1]_ are tractable, we can easily
sample from f by sampling u from Uniform(F (a), F (b)), and then evaluating F _[−][1](u). Note that_
this method is differentiable, and we use it to sample from our dequantiser.

However, multi-variate distributions may not simply be truncated at the tails, but rather have a
support which is a strict subset of its base distribution. One general approach to sampling from
such a distribution is via rejection sampling (Murphy, 2012). This approach has been used in prior
work for sampling from bounded-support distributions (Polykovskiy & Vetrov, 2020; Xu & Durrett,
2018; Davidson et al., 2018). Computing gradients for this method is possible via implicit gradients
(Figurnov et al., 2018), but we do not need gradients in our case, as we use rejection sampling for
generating samples from the generative model (See Section 3.2).

3.1 TRUNCATED LOGISTIC DISTRIBUTION

We choose the truncated logistic distribution to parameterise the dequantiser, because the density, the
CDF and the inverse CDF of the logistic distribution can all be easily computed:

_u_
_f_ (z) = σ(z) · σ(1 − _z),_ _F_ (z) = σ(z), _F_ _[−][1](u) = log_ 1 _u_ _[,]_ (4)

_−_

for z ∈ R and u ∈ (0, 1), where σ is the logistic sigmoid function. Additionally, we can parameterise
the width of Uniform(F (a), F (b)) directly by s, where 0 < s ≤ 1, and the mean of this distribution
as m, such that m + 2[s] _[<][ 1][ and][ m][ −]_ 2[s] _[>][ 0][. Given a set of statistical parameters][ ˆ]s ∈_ R and ˆm ∈ R

for each category, we can impose these constraints by the following reparameterisation:

_m = σ( ˆm),_ _s = 2 · min(m, 1 −_ _m) · σ(ˆs)._ (5)

Then, to sample from Uniform(F (a), F (b)),


_u = m +_ _u0_
_−_ [1]2



_s,_ _u0_ Uniform(0, 1). (6)

_·_ _∼_


This allows for a simple implementation for a truncated logistic distribution that is differentiable (see
Appendix A.1 for the derivation of the gradient). We can write the pdf as


1s _[σ][(][z][)][ ·][ σ][(1][ −]_ _[z][)][,]_ if 0 < _[F][ (][z]s[)][−][m]_ + [1]2 (7)

0, otherwise. _[≤]_ [1]


_f˜(z; m, s) =_


Like in Lippe & Gavves (2020), the resulting approximate posterior q(zt _xt) requires category_
_|_
specific parameters for the truncation (m(xt) and s(xt)) and any flow applied on top of the truncated
logistic (we denote this flow as g). During training, conditioned on a given category xt, we can
sample zt with the method described above, and compute the probability q(zt|xt). Since decoding
requires computing g[−][1](zt, ˆxt) for all K categories, it limits the choice of flows applicable. In CatNF
implementations, a linear flow is used (Kingma & Dhariwal, 2018), while empirically the authors find
that a category conditional scale and shift suffices. Algorithm 1 details the steps taken for computing
_zt, log q(zt_ _xt), and log p(xt_ _zt)._
_|_ _|_

With this parameterisation of the decoder and the truncated approximate posterior, some q(zt _xˆt)_
_|_
wherefor all possible assignments of xt ̸= ˆxt may not have support over the sample xt had mutually exclusive support, then decoding will always be zt. In the extreme case, when q(zt|xt)
deterministic, similar to Argmax flow. This is when TRUFL has the flexibility to embed discrete
data losslessly in a continuous space. Empirically we find that q(xt _zt) is sparse, but not always_
_|_
deterministic.

**Recovering Argmax Flow and CatNF** Specific choices of m and s in Eq. 6 can recover CatNF
and Argmax Flow support for each category. By setting m = 0.5 and s = 1, we perform no truncation
on the posterior distribution, where we recover CatNF. For Argmax Flow, we can achieve orthant
support by setting m to 0.25 or 0.75 for each dimension according to the orthant it was assigned
based on the binary encoding, and s = 0.25. This will ensure that the support covers only half of the
real line in that dimension, thus matching Argmax Flow support for that category.


-----

**Algorithm 1 Truncated Categorical Encoding for a timestep t**

**Input: Categorical data xt, Flow g(·, ·)**
**Output: zt, log q(zt|xt), log p(xt|zt)**
_uu0 ∼mUniform(0(xt) +_ _u,0 1)_ 2 _s(xt)_ _▷_ Begin encoding

_←_ _−_ [1] _·_

_zt[′]_

_zfort ←[←] ˆxt = 0g[F]([ −]zt[1][′][, x] to[(][u]t[)] K[)]  −_ 1 do _▷_ Compute probability of zt given all possible▷ End Encoding ˆxt

_zˆt[′]_ _t[,][ ˆ]xt)_

log[←] q(z[g]t[−]xˆ[1][(]t[z]) log f[˜](ˆzt[′][;][ m][(ˆ]xt), s(ˆxt)) + log dˆdzztt[′]
_|_ _←_

**end for**
log p(xt|zt) ← log q(zt|xt) ˜p(xt) − log _xˆt_ _[q][(][z][t][|]x[ˆ]t) ˜p(ˆxt)_ _▷_ log computation of the q posterior

[P]


**Algorithm 2 Rejection sampling**

**Output: x**
**_z ∼_** _p(z)_
**while ∀xˆ : f[˜](z; m(ˆx), s(ˆx)) = 0 do**

**_z ∼_** _p(z)_

**end while**
**_x ∼_** _p(x|z)_


_p(z)_ _ptrunc(z)_


Figure 2: Left: Pseudo-code for rejection sampling. Centre: The untruncated prior p(z). Right: The
prior truncated according to the unioned support of q. We denote this as ptrunc(z).

3.2 REJECTION SAMPLING

When sampling from the model, the standard process is ancestral sampling: z ∼ _p(z), and then_
**_x ∼_** _p(x|z). However, in our model, the sample zt may not have support under any of the q(zt|xt),_
resulting in an undefined optimal p(xt _zt). As mentioned, we reject the samples that have no support_
_|_
for any of the mixture components. This is equivalent to redefining a new prior ptrunc(z) proportional
to p(z) such that its support matches the unioned support of q(z|x) and renormalising. The existing
ELBO is still a valid lower-bound of such a distribution:


log _[p][(][x][|][z][)][ p][(]Z[z][)]_

_q(z|x)_


log _[p][(][x][|][z][)][ p][(][z][)]_

_q(z|x)_


= Eq(z **_x)_**
_L_ _|_


Eq(z **_x)_**
_≤_ _|_


_p(x_ **_z) ptrunc(z) dz,_** (8)
_|_


where _Z :=_ _p(z) · 1_ _∃xˆ : f[˜](z; m(ˆx), s(ˆx)) > 0_ dz ≤ 1. (9)
Z h i

Eq. 9 in principle can be estimated by counting the frequency of accepted samples. For one of our
experiments, the overall rejection rate at the end of training is about 0.14, suggesting that rejection
sampling from a trained prior is not inefficient (see Section 5.1). Using the decoder as an accepting
heuristic, we find empirically that this results in more valid samples in constrained tasks that require
sampling from the model.

4 RELATED WORK

**Flows on Discrete Data** Discrete flows (Tran et al., 2019; Hoogeboom et al., 2019) deal with flows
in the discrete space, but because of this they resort to the straight-through method (Bengio et al.,
2013) for estimating gradient. Using conditional permutations, Lindt & Hoogeboom (2021) develop
a way to use discrete flows that do not have the same gradient bias that Discrete flows introduce due
to its gradient estimator. Our approach maps the discrete data into a continuous space, allowing the
prior to take on most of the modelling complexity.


-----

Table 1: Results on the graph coloring problem (Lippe & Gavves, 2020). SMALL are graphs of
size 10 ≤|V | ≤ 20, and LARGE 25 ≤|V | ≤ 50. All results are attained using the CatNF
codebase, and averaged across 3 random seeds. Results in the rounded box are using a different set of
hyperparameters than the ones used in CatNF.

SMALL LARGE

**Method** **Validity** **bpd** **Validity** **bpd**

RNN+Largest first 93.41% ±0.42% 0.68 ±0.01 71.32% ±0.77% 0.43 ±0.01

|CatNF 94.56% ±0.55% 0.67 ±0.00|66.80% ±1.14% 0.45 ±0.01|
|---|---|
|± ± Argmax (thresh.) 94.81% ±0.37% 0.66±0.00|68.06% ±0.04% 0.45 ±0.00|
||63.65% ±0.24% 0.46 ±0.00|
|RN eo ere cj te ioc nti o sn sa pm linp gling 99 55 .. 94 00 %% ± ±0 0. .3 25 9% 0.65 ±0.01 j am %|76 48 .. 21 00 %% ± ±0 0. .0 00 1% 0.45 ±0.00 %|
|||



**Learning the Prior** Since most of the modelling happens in the prior after dequantisation, the prior
has to have the expressibility to model the resulting continuous data. Learnable priors are not new in
the VAE literature (Tomczak & Welling, 2018; Huang et al., 2017). One common way for learning
sequential data is to use an autoregressive prior (Ziegler & Rush, 2019). Liu et al. (2019) introduces
a graph-structured flows which can be used for modelling such data, and there have been flow-based
models developed for molecule generation (Madhawa et al., 2019; Shi et al., 2020). Ziegler & Rush
(2019) and Huang et al. (2018) also introduce normalising flows that can learn multi-modal target
distributions, which Ziegler & Rush (2019) show is important in modelling text data.

**Optimal decoder** In the case of categorical data with a sufficiently small K, computing the
posterior of the encoder is tractable during training. The optimal decoder in Eq. 3 was used in
practice in Lippe & Gavves (2020). Argmax flow (Hoogeboom et al., 2021) are a special case in
which the encoding of the categorical variable results in a latent variable that has a deterministic
decoding, which Nielsen et al. (2020) generalises and calls surjectivity which can be parameterised
in either inference or generation. The optimal decoder also has the added effect of alleviating the
posterior collapse problem commonly faced when modelling discrete data such as text (Yang et al.,
2017; Bowman et al., 2015; Ziegler & Rush, 2019; Chen et al., 2016).

**Bounded-support distributions in latent variable models** While normalising flows (Papamakarios et al., 2019) allow for learning a more flexible posterior distribution, Jaini et al. (2020) and Verine
et al. (2021) discuss the current limitations of flows due to their bi-lipschitz property which result
in nearly no transformation at the tails of distributions. Truncated distributions are an extreme case
of a light-tailed distribution, achieved by performing a shift and scaling of the base uniform distribution. There have also been efforts on spherical latent variables, modelled by a von Mises-Fisher
(vMF) distribution (Xu & Durrett, 2018; Davidson et al., 2018). Polykovskiy & Vetrov (2020) uses
bounded-support kernels as the basis of their posterior distributions, and modify the decoder so that
the results are deterministic, but introduce gradient estimations in order to train the model. In the
latter two cases, rejection sampling was used in order to attain samples for inference.

5 EXPERIMENTS

In this section, we present experiments on TRUFL. See Appendix C for implementation details.

5.1 GRAPH COLORING

We first consider the graph colouring problem (Bondy et al., 1976) introduced in Lippe & Gavves
(2020). The task is to colour the nodes in a graph with 3 different colours such that any two nodes
connected by an edge do not have the same colour, which provides a meaningful benchmark for
evaluating how well a model learns unknown constraints from the data. We evaluate the Validity of
the generated samples conditioned a given graph.


-----

Table 2: Performance on molecule generation trained on Zinc250k (Irwin et al., 2012), calculated on
10k samples and averaged over 4 random seeds.

**Method** **Validity** **Uniqueness** **Novelty**

_Constrained generation_

JT-VAE (Jin et al., 2018) 100% 100% 100%

GraphAF (Shi et al., 2020) 68% 99.10% 100%
R-VAE (Ma et al., 2018) 34.90% 100% —
GraphNVP (Madhawa et al., 2019) 42.60% 94.80% 100%
Argmax flow (thresh.) 78.36% ±4.93% 100% 99.99%
CNF (Lippe & Gavves, 2020) 83.41% ±2.34% 99.99% 100%
_,→_ Sub-graphs 96.35% ±2.58% 99.98% 99.98%

No rejection sampling 84.46%±4.24% 100% 100%
_,→_ Sub-graphs 97.51%±2.37% 99.98% 99.99%
_,→_ Rejection sampling 85.01%±1.05% 100% 100%
_,→_ Sub-graphs **97.87%±0.06%** 100% 100%

For baselines, we present results from CatNF (Lippe & Gavves, 2020), and implemented the thresholding version of Argmax flow. We implement the truncated flows using the CatNF codebase, and
compare results with and without rejection sampling for TRUFL. Additionally, we find a better
set of hyperparameters work better for the LARGE setting, and report the results base on those
hyperparameters for the baseline as well. All graph-structured flows used in this task are the same as
the ones detailed in CatNF, we simply replace the categorical embedding module. For results without
rejection sampling, we also compute the probability over the categories using Eq. 9, but disregard the
truncation of the densities. We find we perform marginally better than the baselines in the small graph
regime. However, with rejection sampling and better hyperparameters, we significantly outperform
the flow-based benchmarks, and perform better than the RNN baseline used in Lippe & Gavves
(2020).

**Rejection Rate of TRUFL** Since the
prior will be optimised to fit the aggregate
posterior during training, we expect the rejection rate of sampling from the truncated
prior to drop as training progresses. We
measure the rejection rate when sampling
from the model and report its average rate
of rejection as training progresses. While
the model is conditioned on graphs of different sizes during sampling, we look at
the average rate of rejection go obtain a

Figure 3: Rejection rate vs training iterations. The re
measure of the efficiency of our rejection

jection rate when sampling for evaluation for the graph

sampling scheme. Figure 3 suggests that

colouring task reduces during iteration, indicating (1)

the prior learns to fit the support of the ag
the rejection rate is not intractably high, and (2) the

gregate posterior better during training, as

prior tries to match the support of TRUFL over time.

the rejection rate reduces from about 0.45
to about 0.14 at the end of training. This
corresponds to roughly losing log Z ≈ 0.15 nats (or 0.22 bits) per molecule. Note that we did not
account for this gap while computing the bpd in Table 1 since we would have to estimate log Z
conditioned on each graph size, and this would give us slightly lower value in theory.

5.2 MOLECULE GENERATION

Molecule generation is another task commonly used as a generative modelling benchmark (Jin et al.,
2018; Shi et al., 2020; Ma et al., 2018; Madhawa et al., 2019). The atoms constituting the molecules
are naturally represented by tokens that are usually interpreted categorically, but the intrinsic structure
of the atoms, such as the electron configurations, is often neglected. For this reason, we believe
TRUFL and CatNF will learn a more useful representation than Argmax Flow. Moreover, given the


-----

Table 3: Results on character-level and word-level language modelling, an average across 3 different
random seeds. Results reported with † indicates results attained with the Argmax flow codebase. ∗
indicates results attained with the CatNF codebase. All other results are from the CatNF paper.

CHARACTER WORD

**Model** **PTB (char.)** **Text8** **Wikitext103**

LSTM 1.28 ±0.01 1.44 ±0.01 4.81 ±0.05
Latent NF (Ziegler & Rush, 2019) 1.30 ±0.01 1.61 ±0.02 6.39 ±0.19
Categorical NF (Lippe & Gavves, 2020) 1.27 ±0.01 1.45 ±0.01 5.43 ±0.09
Argmax Flow (Hoogeboom et al., 2021) _∗1.26 ±0.01_ _†1.39 ±0.00_ _∗5.42 ±0.01_

Ours _∗1.26 ±0.02_ _†1.40 ±0.01_ _∗5.35 ±0.01_

Table 4: Results on word-level language modelling, each sentence is modelled as a separate data
point. CatNF, Argmax Flow, and TRUFL results are averaged across 4 random seeds. Setting and
baseline LSTM provided by Kim et al. (2019).

**Model** **Dim.** **PPL** **NLL** **Recon.** **KL**

LSTM — 86.2 4.46 — —
CatNF 12 139.7 _± 3.0_ 4.93 _± 0.02_ 1.01 4.18
Argmax Flow (binary) 14 242.7 _± 2.7_ 5.49 _± 0.01_ 0.00 5.75

Ours 12 143.6 _± 4.9_ 4.97 _± 0.03_ 1.47 3.71

results in graph colouring, we believe that rejection sampling can aid in creating more valid samples
than prior unconstrained generation methods. Molecule generation also requires the generation of
edge categories in addition to the node categories.

Our benchmarks were implemented with the Lippe & Gavves (2020) codebase, and the Argmax flow
benchmark uses the same thresholding scheme. Rejection sampling in this case requires rejecting
invalid edges and vertices both, and so resulted in longer sampling times. The results are in Table
2 Again, we find that TRUFL consistently improves upon CatNF in terms of the validity of the
generated samples, which can be further increased by truncating the prior via rejection sampling, and
taking the largest sub-graph in cases where multiple disconnected graphs are generated (as in done in
Lippe & Gavves (2020)).

5.3 LANGUAGE MODELLING

One can view these variational dequantisation methods when applied to language modelling as a
method of using stochastic embeddings: each token is represented by a distribution over the embedding space, and language models then operate over samples from these distributions. Probabilistic
embeddings have been suggested in prior work (Li et al., 2018; Dasgupta et al., 2020; Chen et al.,
2021), but here, distribution parameters are trained with a language modelling objective. We perform experiments on character-level and word-level language modelling. For word-level language
modelling, many tasks require a large number of categories (∼10k). The naive thresholding method
will result in a large latent space, no different from using a one-hot representation, so we use the
previously mentioned binary encoding of each category in our implementation of the Argmax flow.

For the character level experiments on the Penn Treebank (PTB; Marcus et al. 1993), we use the setup
in CatNF, replacing the categorical encoding with TRUFL. For text8 (Mikolov et al., 2014), we use
the Argmax flow codebase, modifying the Argmax flow module and replacing it with TRUFL. For
word level experiments on Wikitext103 (Merity et al., 2016), we use the setting provided by CatNF
as well, which evaluates chunks of 256 words, and initialises the word embeddings using GloVe
(Pennington et al., 2014). While this is not the standard setting for language modelling, we include
these results for comparison with Argmax Flow and CatNF. We report the negative log-likelihood in
Table 3. For word level experiments on PTB, we use the setting specified by Kim et al. (2019), which
also provides an LSTM baseline. Unlike standard PTB results, this benchmark treats each sentence


-----

Figure 4: Scatter plot of t-SNE embeddings of samples from Armgax Flows (left) and TRUFL (right).
Since t-SNE reveals the relative proximities of the embeddings, we should note that the visualisation
here reveals what clusters of embeddings are close to others relative to the other words in the plot.

as i.i.d., instead of treating the entire dataset as a continuous string of text. We provide further details
of the architecture in Appendix C.3.

The reconstruction loss for TRUFL is higher than that of CatNF, suggesting that CatNF may have
optimised the variance of each dequantising distribution to be fairly small in order to approach
deterministic decoding. To verify this, we compute the Within-Group Standard Deviation (WGSD)
across all q(zt _xt). Sampling 200 samples from each category, we first normalise the mean and_
_|_
standard deviation across all 10, 000 × 200 categories and samples. We then compute the standard
deviation for each category across the 200 samples, and average across all 12 dimensions. CatNF
has an WGSD of 0.52, while TRUFL has an WGSD of 0.66. A smaller WGSD would indicate
peakier disributions in the latent space, given that each quantising distribution has a lower dispersion
on average. Since the prior is modelling the correlations between the categories in the sequence, a
peakier distribution will require more capacity to model. In this scenario, we find that both models
are comparable, with CatNF performing slightly better. However, recall that even with importance
sampling, we are not accounting for the unsupported regions of the prior. This means this is an
estimate of the upper bound in Eq. 8, and that estimating Z will result in a lower perplexity score. In
general, there has been a gap between autoregressive models (e.g. RNNs, Transformers) that model
the discrete distribution directly, and latent variable models, and we believe this gap will close as
dequantising techniques improve.

We perform a qualitative analysis on the learned stochastic embeddings, comparing the t-SNE plots
of samples of 10 chosen words. We took 200 samples from q(zt _xt) for each word, and performed_
_|_
t-SNE over all 200 × 10 vectors. As expected, Argmax flow partitions the latent space into separate
orthants, resulting in word embeddings that are arbitrarily chosen. N and three are an exception,
but this is perhaps due to a chance occurrence of their orthants being dissimilar in only one dimension.
On the other hand, TRUFL learning the topology of these embeddings results in clusters that retain
some properties of the words they represent. Interestingly, while flew:fly and walked:walk
are different tenses of the same word, the learned distribution for each word appears to group them
by tense instead. Since this would inform predictions of the tense of future words, this is perhaps a
functional representation of these words.

6 CONCLUSION

In this paper, we propose TRUFL, a flow based dequantiser with truncated support for stochastically
embedding categorical data in a continuous space. This allows us to employ density-based models
like normalising flows to fit the continuous representation of the data. To deal with the unsupported
regions in the decoder, we further propose truncating the prior distribution to account for the unioned
support of the dequantiser, which proves to be more effective at generating samples with constraints,
such as for graph coloring and molecule generation. In language modelling, we find that learned
stochastic embeddings has better performance than a strict orthant partitioning of the space, and
qualitative analysis reveals that the proximity between similar words under such a partitioning is poor.
We believe that further work on variational dequantisation will close the gap between auto-regressive
models that directly model the categorical space and latent variable models.


-----

ACKNOWLEDGEMENTS

We would like to thank Yikang Shen and Christos Tsirigotis for their input and suggestions during
the course of this work. Phillip Lippe was also gracious in helping us understand the specifics of the
CatNF codebase and running the experiments.

REFERENCES

Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic´
language model. The journal of machine learning research, 3:1137–1155, 2003.

Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through´
stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.

John Adrian Bondy, Uppaluri Siva Ramachandra Murty, et al. Graph theory with applications,
volume 290. Macmillan London, 1976.

Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Bengio.
Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349, 2015.

Xi Chen, Diederik P Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya
Sutskever, and Pieter Abbeel. Variational lossy autoencoder. arXiv preprint arXiv:1611.02731,
2016.

Xuelu Chen, Michael Boratko, Muhao Chen, Shib Sankar Dasgupta, Xiang Lorraine Li, and Andrew
McCallum. Probabilistic box embeddings for uncertain knowledge graph reasoning. arXiv preprint
_arXiv:2104.04597, 2021._

Shib Dasgupta, Michael Boratko, Dongxu Zhang, Luke Vilnis, Xiang Li, and Andrew McCallum.
Improving local identifiability in probabilistic box embeddings. Advances in Neural Information
_Processing Systems, 33:182–192, 2020._

Tim R Davidson, Luca Falorsi, Nicola De Cao, Thomas Kipf, and Jakub M Tomczak. Hyperspherical
variational auto-encoders. arXiv preprint arXiv:1804.00891, 2018.

Michael Figurnov, Shakir Mohamed, and Andriy Mnih. Implicit reparameterization gradients. arXiv
_preprint arXiv:1805.08498, 2018._

Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle. Made: Masked autoencoder for
distribution estimation. In International Conference on Machine Learning, pp. 881–889. PMLR,
2015.

Joshua Goodman. Classes for fast maximum entropy training. In 2001 IEEE International Conference
_on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No. 01CH37221), volume 1, pp._
561–564. IEEE, 2001.

Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving flowbased generative models with variational dequantization and architecture design. In International
_Conference on Machine Learning, pp. 2722–2730. PMLR, 2019._

Emiel Hoogeboom, Jorn Peters, Rianne van den Berg, and Max Welling. Integer discrete flows and
lossless compression. Advances in Neural Information Processing Systems, 32:12134–12144,
2019.

Emiel Hoogeboom, Taco Cohen, and Jakub Mikolaj Tomczak. Learning discrete distributions by
dequantization. In Third Symposium on Advances in Approximate Bayesian Inference, 2020.

Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forre, and Max Welling.´ Argmax
flows and multinomial diffusion: Towards non-autoregressive language models. arXiv preprint
_arXiv:2102.05379, 2021._

Chin-Wei Huang, Ahmed Touati, Laurent Dinh, Michal Drozdzal, Mohammad Havaei, Laurent
Charlin, and Aaron Courville. Learnable explicit density for continuous latent space and variational
inference. arXiv preprint arXiv:1710.02248, 2017.


-----

Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron Courville. Neural autoregressive
flows. In International Conference on Machine Learning, pp. 2078–2087. PMLR, 2018.

John J Irwin, Teague Sterling, Michael M Mysinger, Erin S Bolstad, and Ryan G Coleman. Zinc: a
free tool to discover chemistry for biology. Journal of chemical information and modeling, 52(7):
1757–1768, 2012.

Priyank Jaini, Ivan Kobyzev, Yaoliang Yu, and Marcus Brubaker. Tails of lipschitz triangular flows.
In International Conference on Machine Learning, pp. 4673–4681. PMLR, 2020.

Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for
molecular graph generation. In International conference on machine learning, pp. 2323–2332.
PMLR, 2018.

Yoon Kim, Chris Dyer, and Alexander M Rush. Compound probabilistic context-free grammars for
grammar induction. arXiv preprint arXiv:1906.10225, 2019.

Diederik P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions.
_arXiv preprint arXiv:1807.03039, 2018._

Diederik P Kingma and Max Welling. Stochastic gradient vb and the variational auto-encoder. In
_Second International Conference on Learning Representations, ICLR, volume 19, pp. 121, 2014._

Xiang Li, Luke Vilnis, Dongxu Zhang, Michael Boratko, and Andrew McCallum. Smoothing the geometry of probabilistic box embeddings. In International Conference on Learning Representations,
2018.

Alexandra Lindt and Emiel Hoogeboom. Discrete denoising flows. In ICML Workshop on Invertible
_Neural Networks, Normalizing Flows, and Explicit Likelihood Models, 2021._

Phillip Lippe and Efstratios Gavves. Categorical normalizing flows via continuous transformations.
_arXiv preprint arXiv:2006.09790, 2020._

Jenny Liu, Aviral Kumar, Jimmy Ba, Jamie Kiros, and Kevin Swersky. Graph normalizing flows.
_arXiv preprint arXiv:1905.13177, 2019._

Tengfei Ma, Jie Chen, and Cao Xiao. Constrained generation of semantically valid graphs via
regularizing variational autoencoders. arXiv preprint arXiv:1809.02630, 2018.

Kaushalya Madhawa, Katushiko Ishiguro, Kosuke Nakago, and Motoki Abe. Graphnvp: An invertible
flow model for generating molecular graphs. arXiv preprint arXiv:1905.11600, 2019.

Mitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated
corpus of english: The penn treebank. 1993.

Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. arXiv preprint arXiv:1609.07843, 2016.

Tomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu, and Marc’Aurelio Ranzato.
Learning longer memory in recurrent neural networks. arXiv preprint arXiv:1412.7753, 2014.

Andriy Mnih and Geoffrey E Hinton. A scalable hierarchical distributed language model. Advances
_in neural information processing systems, 21:1081–1088, 2008._

Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model. In
_International workshop on artificial intelligence and statistics, pp. 246–252. PMLR, 2005._

Kevin P Murphy. Machine learning: a probabilistic perspective. MIT press, 2012.

Didrik Nielsen and Ole Winther. Closing the dequantization gap: Pixelcnn as a single-layer flow.
_arXiv preprint arXiv:2002.02547, 2020._

Didrik Nielsen, Priyank Jaini, Emiel Hoogeboom, Ole Winther, and Max Welling. Survae flows:
Surjections to bridge the gap between vaes and flows. Advances in Neural Information Processing
_Systems, 33, 2020._


-----

George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji
Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. arXiv preprint
_arXiv:1912.02762, 2019._

Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 conference on empirical methods in natural language
_processing (EMNLP), pp. 1532–1543, 2014._

Daniil Polykovskiy and Dmitry Vetrov. Deterministic decoding for discrete data in variational
autoencoders. In International Conference on Artificial Intelligence and Statistics, pp. 3046–3056.
PMLR, 2020.

Yikang Shen, Shawn Tan, Chrisopher Pal, and Aaron Courville. Self-organized hierarchical softmax.
_arXiv preprint arXiv:1707.08588, 2017._

Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. Graphaf: a
flow-based autoregressive model for molecular graph generation. arXiv preprint arXiv:2001.09382,
2020.

Lucas Theis, Aaron van den Oord, and Matthias Bethge. A note on the evaluation of generative¨
models. arXiv preprint arXiv:1511.01844, 2015.

Jakub Tomczak and Max Welling. Vae with a vampprior. In International Conference on Artificial
_Intelligence and Statistics, pp. 1214–1223. PMLR, 2018._

Dustin Tran, Keyon Vafa, Kumar Agrawal, Laurent Dinh, and Ben Poole. Discrete flows: Invertible
generative models of discrete data. Advances in Neural Information Processing Systems, 32:
14719–14728, 2019.

Alexandre Verine, Benjamin Negrevergne, Fabrice Rossi, and Yann Chevaleyre. On the expressivity
of bi-lipschitz normalizing flows. arXiv preprint arXiv:2107.07232, 2021.

Jiacheng Xu and Greg Durrett. Spherical latent spaces for stable variational autoencoders. arXiv
_preprint arXiv:1808.10805, 2018._

Zichao Yang, Zhiting Hu, Ruslan Salakhutdinov, and Taylor Berg-Kirkpatrick. Improved variational
autoencoders for text modeling using dilated convolutions. In International conference on machine
_learning, pp. 3881–3890. PMLR, 2017._

Zachary Ziegler and Alexander Rush. Latent normalizing flows for discrete sequences. In Interna_tional Conference on Machine Learning, pp. 7673–7682. PMLR, 2019._


-----

A APPENDIX

A.1 GRADIENT ESTIMATES OF a AND b

_p(z; a, b) =_

Let


_b_ 1 _a_ _[,]_ if a < z < b,
_−_ (10)
0, otherwise


_z = a + (b −_ _a) · u_ (11)
_u ∼_ Uniform(0, 1) (12)

_u =_ _[z][ −]_ _[a]_ (13)

_b −_ _a_


_aEz[f_ (z)] = Eu Uniform(0,1) [ _af_ (a + (b _a)_ _u)]_ (14)
_∇_ _∼_ _∇_ _−_ _·_

= Eu Uniform(0,1) [(1 _u)(_ _zf_ (z))] (15)
_∼_ _−_ _∇_

_bEz[f_ (z)] = Eu Uniform(0,1) [u( _zf_ (z))] (16)
_∇_ _∼_ _∇_

A.2 SOFT DEQUANTIZATION

Let x denote the data (e.g. a sentence) and L denote the dimensionality of the data; i.e. x ∈C[L] and
_cL ∈_ Z+ has the same support, we call it. x can be dequantized by sampling soft dequantization. We write zj ∼ _q(zj|xj) for j ∈ p1(, ..., Lxj_ _zj. When) and p( {z)q to denote the(zj|xj = c) :_
_∈C}_ _|_
quantizer and the prior over the dequantized data. We consider the following ELBO


_L_

log _[p][(][x][j][|][z][j][)]_

_q(zj_ _xj) [+ log][ p][(][z][)]_

_j=1_ _|_

X


(x, L) = E[Q] _q(zj_ _xj_ )
_L_ _|_


(17)


**Theorem 1 (Maximizer of ELBO, dequantization). Assume q(x, L) > 0 for all x ∈C[L]** _and L ∈_ Z+,
_and that x1, ..., xL are not conditionally independent given L. Then_

Eq(x,L)[log q(x|L)] > Eq(x,L)[L] (18)

_as long as the densities q(zj|xj = c) for different categories c ∈C have the same support. That is,_
_soft dequantization with shared support is suboptimal._

_Proof. The optimal quantizer is_


_q(xj_ _zj)_ _q(xj)q(zj_ _xj)_ where _q(xj)_
_|_ _∝_ _|_ _∝_

Plugging it into the ELBO gives


_q(x[′]j_ [=][ x][j][, L][)] (19)
_j[′]=1_

X


_L=1_


_L_

log _[q][(][x][j][|][z][j][)]_ = E[Q] _q(zj_ _xj_ )

_q(zj_ _xj) [+ log][ p][(][z][)]_ _|_

_j=1_ _|_

X




_L_

_p(z)_
log q(xj) + log
_j=1_ _j_ _[q][(][z][j][)]_

X

Q (20)


(x, L) = E[Q] _q(zj_ _xj_ )
_L_ _|_


where q(zj) = _x[′]j_ _[q][(][x]j[′]_ [)][q][(][z][j][|][x][′]j[)][.]

On the other hand, we can rewrite the negentropy of the data as

[P]


_L_

_q(x_ _L)_
log q(xj) + log _L_ _|_
_j=1_ _j=1_ _[q][(][x][j][)]_

X

Q


Eq(x,L)[log q(x|L)] = Eq(x,L)


(21)


-----

Now to compare the second term with that of the ELBO we let q(z|L) = _q(x|L)_ _j=1_ _[q][(][z][j][|][x][j][)][.]_

For simplicity, we denote by T (z|x) = _j=1_ _[q][(][z][j][|][x][j][)][ the transition kernel applied to either][ q][(][x][|][L][)]_

[P] [Q][L]

or _j=1_ _[q][(][x][j][)][. Then]_

[Q][L]

[Q][L] _q(x_ _L)_ _q(x_ _L)T_ (z _x)_

Eq(x,L) "log _Lj=1_ _|[q][(][x][j][)]_ # = Eq(x,L)T (z|x) log _Lj=1_ _|[q][(][x][j][)]_ _|T_ (z|x)  (22)

Q  Q _Lj=1_ _[q][(][x][j][)]_ _T_ (z|x)

= Eq(z,L)q(x _z,L)_ log (23)
_|_ − Q _q(x_ _L)T_ (z _x)_ 

_|_ _|_

 

_∗_ _j_ _[q][(][x][j][)]_ _T_ (z|x)
_> Eq(z,L)_ log Eq(x _z,L)_ (24)

−  _|_  Qq(x _L)T(z_ _x)_ 

_|_ _|_

   

_j_ _[q][(][z][j][)]_

= Eq(z,L) log = Eq(z,L) log _[q][(][z][|][L][)]_ (25)

− Qq(z|L)  " _j_ _[q][(][z][j][)]_ #

which is greater or equal to the second term of the ELBO by Gibb’s inequality. The inequality markedQ
by ∗ is due to Jensen and the convexity of − log, and is strict since the dependency of x1,...,L implies
there exist some x[1] and x[2] s.t. _j_ _[q][(][x]j[i]_ [)][ ̸][=][ q][(][x][i][|][L][)][ for][ i][ ∈{][1][,][ 2][}][. Now, since][ T] [(][z][|][x][)][ >][ 0][ over the]

shared support by assumption, for almost all z and L, q(x[i]|z, L) > 0 for i ∈{1, 2}. That is, with
non-zero probability the argument to the convex function[Q] log has a different value from its mean.
_−_
This implies the inequality is strict and concludes the proof.

B FURTHER RELATED WORK

**Consequences of a binary encoding** The binary encoding scheme of Argmax flow bears some
resemblance to prior work on hierachical softmax schemes to reduce the computational footprint
of the large softmax layer (Goodman, 2001; Morin & Bengio, 2005; Mnih & Hinton, 2008; Shen
et al., 2017). Specifically, Mnih & Hinton (2008) proposed an analogous scheme for scaling up the
softmax layer by representing words as leaves in a tree, and binary encodings as a traversal of such a
tree. However, the partitioning of the ⌈log2 K⌉-dimensional space into K equal categories will only
be exact if K is a power of 2. Otherwise, K − 2[⌈][log][2][ K][⌉] orthants of the embedding space will not
be utilised by the dequantiser. TRUFL has unsupported regions in the decoder as well, but we take
the approach of modifying the prior during generation to account for these regions. Furthermore,
because the binary vectors are arbitrary, the similarity between words/categories is not reflected in
the embedding space as it is in a learned distributed representation.

C EXPERIMENTS & IMPLEMENTATION DETAILS

We incorporated the codebases from the following sources:

[1. Lippe & Gavves (2020): https://github.com/phlippe/CategoricalNF](https://github.com/phlippe/CategoricalNF)

[2. Hoogeboom et al. (2021): https://github.com/didriknielsen/argmax_flows](https://github.com/didriknielsen/argmax_flows)

[3. Kim et al. (2019): https://github.com/harvardnlp/compound-pcfg](https://github.com/harvardnlp/compound-pcfg)

C.1 TOY EXAMPLE: TWO CATEGORY JOINT DISTRIBUTION

In this simple synthetic example, we demonstrate a case in which TRUFL provides an advantage over
CatNF and Argmax flow. Our goal is to model a joint probability over 2 categorical random variables,
each with 4 categories. This joint distribution is described in Table 5.


-----

(a) CatNF (b) TRUFL (c) Argmax Flow

Figure 5: Embeddings (q(zi _xi)) of toy example for each method._
_|_

Table 5: Left: The joint distribution for x1 and x2 for the toy example. Right: Log probability over
the data of the model, and the breakdown of the ELBO.



**Method** log p(x) E [log p(x _z)]_ E log _[q]p[(][z](z[|][x])[)]_
_|_
h

CatNF -2.000 -1.064 0.967
Argmax -1.915 -0.687 1.263
TRUFL -1.983 0.000 2.098


_x2_
Cat. 0 1 2 3

0 18 0 18 0

_x1_ 21 018 018 018 018

3 0 0 0 14


Notice that under this distribution, p(x1|x2 = 0) = p(x1|x2 = 2) and p(x2|x1 = 0) = p(x2|x1 = 2).
For the purposes of this subsection we will say categories 0 and 2 have equal functionality, while
categories 1 and 3 are similar in functionality, but not equal.

To model this distribution, we perform variational dequantisation over the discrete variables x1, x2
using the above methods, and learn a prior over the latent variables. Conditioned on xi, the dequantisation method produces a distribution over a 2 dimensional space — q(zi _xi). It is this distribution_
_|_
we plot in Figure 5.

In both CatNF and TRUFL, the distribution q(zi|xi = 0) and q(zi|xi = 2) are similar. Intuitively,
this aligns with the motivation behind the construction of the joint distribution: that the categories are
functionally equivalent. However, the dequantisation distribution may not always have this property,
as evidenced by the Argmax Flow, due to the way supports are constrained by the orthants. This
bimodality in the distributions of these two categories will have to be modelled by the prior, which
evidently resulted in a higher log probability in this particular case. In our implementation of Argmax
flow for this task, the training eventually resulted in numerical instability. Here we report the best log
probability attained before numerical errors occured.

As mentioned in Lippe & Gavves (2020), CatNF can approach deterministic decoding if all other
classes are sufficiently far from the target class. However, if several tokens have similar ‘function’ in
the dataset, the corresponding components can be forced to be close together in latent space. When
this happens, the closeness in the components again result in decoding becoming non-deterministic.
TRUFL allows such components to have minimal and sparse overlapping support but yet have the
modes of these components be relatively close. If we consider Figure 5b, we can see that the support
and distribution for xi = 0 and xi = 2 overlap the most, while the overlap with xi = 1 and xi = 3 is
not as great. Since TRUFL minimises the support for the classes that do not overlap in functionality
the TRUFL reconstruction loss is lower in comparison to CatNF.

C.2 HYPERPARAMETERS FOR LARGE GRAPH COLOURING TASK

C.3 ARCHITECTURE FOR THE PRIOR IN LANGUAGE MODELLING TASK

We develop our own architecture for this benchmark, replacing only the categorical encoding scheme
for each of the benchmarks. The prior is autoregressive both ‘in time’ and ‘in hidden’, to use


-----

Param. Value

batch size 128
encoding dim 2
coupling num flows 10
optimizer 4
learning rate 3e-4

Table 6: New arguments for the LARGE graph colouring task we use in our experiments.

nomenclature from Ziegler & Rush (2019). Autoregression in time is governed by an LSTM, and
autoregression in hidden is governed by a MADE model (Germain et al., 2015), conditioned on
said LSTM. We use a mixture of 4 logistics per-dimension (Ho et al., 2019) in order to model the
multi-modality of the distribution over z, which Ziegler & Rush (2019) suggests is prevalent in text
data for characters.


-----

