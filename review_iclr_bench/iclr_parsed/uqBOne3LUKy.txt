# IS IMPORTANCE WEIGHTING INCOMPATIBLE WITH IN## TERPOLATING CLASSIFIERS?

**Ke Alexander Wang[∗], Niladri S. Chatterji[∗], Saminul Haque, Tatsunori Hashimoto**
Department of Computer Science
Stanford University
_{alxwang,niladri}@cs.stanford.edu,{saminulh,thashim}@stanford.edu_

ABSTRACT

Importance weighting is a classic technique to handle distribution shifts. However, prior work has presented strong empirical and theoretical evidence demonstrating that importance weights can have little to no effect on overparameterized
neural networks. Is importance weighting truly incompatible with the training of
_overparameterized neural networks? Our paper answers this in the negative. We_
show that importance weighting fails not because of the overparameterization, but
instead, as a result of using exponentially-tailed losses like the logistic or crossentropy loss. As a remedy, we show that polynomially-tailed losses restore the
effects of importance reweighting in correcting distribution shift in overparameterized models. We characterize the behavior of gradient descent on importance
weighted polynomially-tailed losses with overparameterized linear models, and
theoretically demonstrate the advantage of using polynomially-tailed losses in a
label shift setting. Surprisingly, our theory shows that using weights that are obtained by exponentiating the classical unbiased importance weights can improve
performance. Finally, we demonstrate the practical value of our analysis with
neural network experiments on a subpopulation shift and a label shift dataset.
When reweighted, our loss function can outperform reweighted cross-entropy by
as much as 9% in test accuracy. Our loss function also gives test accuracies comparable to, or even exceeding, well-tuned state-of-the-art methods for correcting
distribution shifts.

1 INTRODUCTION

Machine learning models are often evaluated on test data which differs from the data that they were
trained on. A classic statistical technique to combat such distribution shift is to importance weight
the loss function during training (Shimodaira, 2000). This procedure upweights points in the training
data that are more likely to appear in the test data and downweights ones that are less likely. The
reweighted training loss is an unbiased estimator of the test loss and can be minimized by standard
algorithms, resulting in a simple and general procedure to address distribution shift.

Surprisingly, recent papers (Byrd & Lipton, 2019; Xu et al., 2020) have found that importance
weighting is ineffective in the current deep learning paradigm, where overparameterized models interpolate the training data or have vanishingly small train loss. In particular, Byrd & Lipton (2019)
empirically showed that when no regularization is used, overparameterized linear and nonlinear
models trained with the importance weighted cross-entropy loss ignore the importance weights. Xu
et al. (2020) followed up and provided a theoretical justification for this observation in overparameterized linear and non-linear models.

To build intuition about why importance weighting fails, consider linear classifiers as an example. Given linearly separable data (x1, y1), . . ., (xn, yn) ∈ R[d] _× {−1, 1}, Soudry et al. (2018)_
showed that if gradient descent is applied to minimize an exponentially-tailed classification loss
([P]i [n] _[ℓ][exp][(][y][i][x][i][))][ then the iterates converge in direction to the maximum margin classifier][ b]θMM :=_

_∈_
arg min∥θ∥=1 {γ : yixi · θ ≥ _γ, for all i ∈_ [n]}. Xu et al. (2020) showed that in this same setting,
minimizing the importance weighted loss ([P]i [n] _[w][i][ℓ][exp][(][y][i][x][i][))][ with gradient descent also results]_

_∈_

_∗Equal contribution._


-----

|tropy: No IW Cross E|ntropy: IW Poly-taile|d Loss: No IW Poly-tail|
|---|---|---|
|Learned Boundary Predicted Majority Majority Class True Boundary Predicted Minority Minority Class|||


Figure 1: Models trained with gradient descent, with and without importance weights (IW), in
the label shift setting where classes are imbalanced in the training set. All models interpolate the
training points with 100% accuracy. (Left) Importance weights provably fails to correct for the
distribution shift for the cross-entropy loss. The learned boundary is asymptotically the maximummargin classifier even with reweighting. (Right) Our polynomially-tailed loss restores the effects of
importance weights, correctly adjusting for the distribution shift.

in convergence to the maximum margin classifier, regardless of the weights. To see why, consider
the special case where the weights (w1, . . ., wn) are positive integers. This reweighting is equivalent to simply repeating each datapoint wi times, and the maximum margin classifier over this “new

Majority Class
Minority Class

dataset” remains unchanged. Thus, invoking the original result by Soudry et al. (2018) proves that
the importance weights has no effect in correcting the distribution shift. This result can be seen in
Figure 1 where we demonstrate this phenomenon in a simple toy problem.

Such evidence has led some to wonder if importance weighting is fundamentally incompatible with
overparameterized interpolating models. In this paper, we show that this is not the case. We find
that the culprit behind the ineffectiveness of importance weighting is the exponential tail of popular
losses such as the cross-entropy or the logistic. We propose altering the structure of the loss to have
fatter, polynomially decaying tails instead. We theoretically and empirically show that importance
weights do correct for distribution shift under such losses even for overparameterized classifiers.

Our first contribution is to characterize the limiting direction of the iterates of gradient descent
(its implicit bias) when minimizing reweighted polynomially-tailed losses with linear classifiers.
We show that this limiting direction is a function of both the datapoints as well as the importance
weights, unlike the maximum margin classifier that only depends on the data (see the right half of
Figure 1). Next, we analyze the generalization behavior of this classifier in a label shift setting. We
prove that when the weights are an exponentiation of the unbiased importance weights, the test error
decays to zero in the large sample limit, regardless of the level of imbalance in the data. In contrast,
we prove that the maximum margin classifier test error in this same setting must be at least 1/8.

Finally, we demonstrate the practical benefits of our framework by applying this approach to experiments with neural networks. In both a label shift dataset (imbalanced binary CIFAR10), and a
subpopulation shift dataset with spurious correlations (CelebA (Sagawa et al., 2019)), we find that
reweighting polynomially-tailed losses consistently outperforms reweighted cross-entropy loss, as
our linear theory suggests. Additionally, poly-tailed loss with biased importance weights can perform comparably to, or better than, state-of-the-art methods distribution shift (Cao et al., 2019; Ye
et al., 2020; Menon et al., 2020; Kini et al., 2021)[1].

2 RELATED WORK

Early work (Shimodaira, 2000; Wen et al., 2014) already warned against the potential ineffectiveness of importance weights on interpolating overparameterized models. Shimodaira (2000) showed
that when the model is well-specified, importance weights can fail to have an effect, and that the
ordinary maximum likelihood estimate is asymptotically optimal. Wen et al. (2014) showed that
when there is a zero-loss minimizer of an unweighted convex loss minimization problem, then it is
also a minimizer of the (adversarially) reweighted loss as well. Recent work (Byrd & Lipton, 2019;
Xu et al., 2020) has shown that importance weighting fails to have an effect on neural networks

[1Code is available at https://github.com/KeAWang/importance-weighting-interpolating-classifiers](https://github.com/KeAWang/importance-weighting-interpolating-classifiers)


-----

trained with gradient descent, though always in the setting of exponentially-tailed losses. Sagawa
et al. (2019) demonstrated that reweighting can fail to have the desired effect when unregularized
distributionally robust optimization (DRO) methods are used in conjunction with the cross-entropy
loss. They empirically showed that regularization is necessary to reap the benefits of reweighting,
also observed by Byrd & Lipton (2019).

A recent line of work (Cao et al., 2019; Ye et al., 2020; Menon et al., 2020; Kini et al., 2021) has
introduced modifications to the logistic and cross-entropy losses to correct for distribution shift. Cao
et al. (2019) and Menon et al. (2020) proposed using additive corrections to the logits. However,
without regularization or early-stopping these corrections are ineffective since the additive corrections to the logits is analogous to importance weighting exponential-tailed losses. Multiplicative
logit corrections (Ye et al., 2020), possibly combined with additive corrections (Kini et al., 2021),
have also been proposed. Unlike the additive corrections, these methods do not converge to the
max-margin classifier, but they also do not correspond to importance weighting algorithms. In our
work, we focus on the question of whether importance weighting alone can correct for distribution
shift, and show practical empirical benefits relative to multiplicative logit corrections.

Our work also connects to literature that has studied the implicit bias of gradient descent (Soudry
et al., 2018; Ji & Telgarsky, 2019; Nacson et al., 2019). Especially relevant is the work by Ji et al.
(2020) who relate the implicit bias of gradient descent with exponentially and polynomially-tailed
losses for linear classifiers to a solution of a regularized loss minimization problem. Finally, our
generalization analysis draws from the growing literature focused on finite sample bounds on the
test error of the maximum margin classifier in the overparameterized regime (Chatterji & Long,
2021; Muthukumar et al., 2021; Wang & Thrampoulidis, 2021; Cao et al., 2021).

3 SETTING

We consider a distribution shift setting where the training samples {(x1, y1), . . ., (xn, yn)} ∈ R[d] _×_
1, 1 are drawn i.i.d. from Ptrain, and the test samples are drawn from a different distribution
_{−_ _}_
Ptest that is absolutely continuous with respect to Ptrain. Let fθ denote a classifier parameterized by
_θ. Given a feature x, a classifier maps this feature to fθ(x) ∈_ R. In this paper we shall consider
cases where the classifier is either linear (for our theory) or a neural network (for our experiments).

Our goal is to find a classifier fθ that minimizes the 0-1 loss with respect to the test distribution:
TestError[fθ] = P(x,y) Ptest [sign(fθ(x)) = y] .
_∼_ _̸_

To handle the mismatch between Ptrain and Ptest, we shall study importance weighting algorithms.
Given a datapoint (x, y) ∈ R[d] _×{−1, 1}, the classical unbiased importance weight at (x, y) is given_
by the ratio of densities between the test and the train distributions P[P]train[test][(]([x,y]x,y[)]) [. Using these unbiased]

importance weights ensures that the reweighted training loss is an unbiased estimate of the test loss.

However, as noted above, past work has shown that interpolating classifiers trained with gradient
descent on importance weighted exponentially-tailed losses (such as the logistic loss ℓlog(z) :=
log (1 + exp(−z)), the exponential loss ℓexp(z) := exp(−z), and the cross-entropy loss) ignore the
importance weights. For example, consider the case when the classifier is linear fθ(x) = x _θ, the_
_·_
weights are w1, . . ., wn > 0, and the reweighted loss function is _L(θ) =_ _i=1_ _[w][i][ℓ][log][(][y][i][x][i][ ·][ θ][)][.]_
Xu et al. (2020) showed that if the data is linearly separable then the iterates of gradient descent
converge in direction to the ℓ2-maximum margin classifier,

[b] [P][n]
_θMM := arg maxθ:∥θ2∥=1_ _{γ : yixi · θ ≥_ _γ for all i ∈_ [n]} . (1)

Observe that the maximum margin classifier does not depend on the importance weightsb
(w1, . . ., wn) and hence may suffer large test error when there is distribution shift. Xu et al. (2020)
further showed that when separability assumptions hold, non-linear classifiers trained with gradient
descent on exponentially-tailed losses are also unaffected by importance weights.

We initiate a study of polynomially-tailed losses in the distribution shift setting and show that they
have improved behavior with respect to importance weighting even when the model is overparameterized. Given parameters α > 0 and β ∈ R define the polynomially-tailed loss as follows:

_ℓleft(z)_ if z < β

_ℓα,β(z) :=_ ( [z (β1 1)][α] if z _β,_

_−_ _−_ _≥_


-----

where ℓleft is any loss function such that the overall loss function ℓα,β is convex, differentiable
and strictly decreasing. Several natural choices for ℓleft include the scaled logistic (c1 log(1 +
exp(−c2z))), exponential (c1 exp(−c2z)) or linear (−c1z + c2) losses.

Given a training dataset (x1, y1), . . ., (xn, yn) and a set of weights w1, . . ., wn 0 we let
_{_ _}_ _≥_


_Lα,β(fθ) :=_


_wiℓα,β(yifθ(xi))_
_i=1_

X


be the reweighted empirical loss on this dataset.

**Notation.** Given a vector v, let ∥v∥ denote its Euclidean norm. For any j ∈ N, we denote the set
_{1, . . ., j} by [j]. A random variable ξ is 1-sub-Gaussian if for any λ ∈_ R, E _e[λξ][]_ _≤_ _e[λ][2][/][2]._


4 THEORETICAL RESULTS

In this section, we present several theoretical results that justify the use of polynomially-tailed losses
in conjunction with importance weights to handle distribution shifts. Our final result shows a strict
separation between the performance of reweighted polynomially- and explonentially-tailed models
under label shift. We restrict our theoretical analysis to linear classifiers, fθ(x) = x _θ, for some_
_·_
_θ ∈_ R[d].

First, in Section 4.1 we shall characterize the limiting direction of gradient descent on reweighted
polynomially-tailed losses and show that this direction depends on both the weights as well as the
datapoints. Next, in Section 4.2, we upper bound the test error of this limiting solution in a label
shift setting. We also show that choosing weights that are obtained by exponentiating the unbiased
importance weights helps in reducing the test error. Finally, in this label shift setting, we show that
the maximum margin classifier suffers an error that is at least 1/8.

4.1 IMPLICIT BIAS OF GRADIENT DESCENT ON POLYNOMIALLY-TAILED LOSSES

We begin by presenting a result that characterizes the implicit bias of gradient descent on reweighted
polynomially-tailed losses for linearly separable classification. Understanding this situation is a key
first step, as gradient descent for separable linear classification is often used as a simplified model to
theoretically characterize the behavior of overparameterized neural networks (Soudry et al., 2018;
Ji & Telgarsky, 2019; Nacson et al., 2019; Lyu & Li, 2019; Ji & Telgarsky, 2020).

Given a linearly separable dataset (x1, y1), . . ., (xn, yn), we let zi := yixi. We shall analyze the
iterates of gradient descent with step-size η > 0 and initial iterate θ[(0)] _∈_ R[d], for all t ∈{0, 1, . . .}:
_θ[(][t][+1)]_ = θ[(][t][)] _η_ _Lα,β(θ[(][t][)]). Define the direction_
_−_ _∇[b]_

_n_

_wi_

_θα := arg min_ _zi_ _θ > 0,_ for all i [n] _._ (2)
_θ:∥θ∥=1_ (Xi=1 (zi · θ)[α][,][ s.t.] _·_ _∈_ )

The following proposition characterizes the limiting direction of gradient descent iterates.b
**Proposition 4.1. Suppose that the data is linearly separable. For any α > 0, β ∈** R, any initial
_point θ[(0)]_ _∈_ R[d], and for all small enough step-sizesθ[(][t][)] _η the direction of the gradient descent iterates_
_satisfy the following: limt→∞_ _∥θ[(][t][)]∥_ _[→]_ _θ[b]α._

The proof is presented in Appendix A. This proof relies recent result by Ji et al. (2020) that relates
the limiting direction of gradient descent on the unregularized loss to the limiting solution of a normconstrained loss minimization problem, where the limit is taken with respect to the norm constraint.

Note that, unlike the maximum margin classifier, it is immediately clear that this limiting direction
_θα depends on the weights w1, . . ., wn. As one would intuitively expect, the direction_ _θα tries to_
achieve a larger margin zi _θ on points with larger weights wi. This behavior is also apparent in the_
simulation in the rightmost panel in Figure 1, where upweighting points in the minority class helpsb _·_ [b]
to learn a classifier that is similar in direction to the Bayes optimal classifier for the problem.

Our limiting direction _θα has the interesting property that it does not depend on several quantities:_
the initial point θ[(0)], the properties of ℓleft, and the “switchover” point β. Linear separability ensures

[b]


-----

that in the limit, the margin on each point is much larger than β, and so the loss of each point is in
the polynomial tail of part of ℓα,β.

4.2 GENERALIZATION ANALYSIS

The result in the previous subsection shows that the asymptotic classifier learnt by gradient descent,
_θα respects importance weights. However, this does not demonstrate that using polynomially-tailed_
losses leads to classifiers with lower test error compared to using exponentially-tailed losses.
b

To answer this more fine-grained question, we will need to perform a more refined analysis. To do
this we will make sub-Gaussian cluster assumptions on the data, similar to ones considered in the
generalization analysis of the overparameterized maximum margin classifiers (Chatterji & Long,
2021; Wang & Thrampoulidis, 2021; Liang & Recht, 2021; Cao et al., 2021). In our setting, the
features associated with each label are drawn from two different sub-Gaussian clusters. We shall
consider a label shift problem where the training data is such that the number of data points from
the positive (majority) cluster will be much larger than the number of datapoints from the negative
(minority) cluster. The test datapoints will be drawn uniformly from either cluster.

Under these assumptions, we derive upper bounds on the error incurred by _θ1, the limiting direction_
of gradient descent of polynomially tailed losses with α = 1 (Section 4.2.2). We will find that its
test error is small when the reweighting weights are set to cubic powers of the unbiased importance

[b]
weights. (A similar analysis can also be conducted for other α.) In the same setting in Section 4.2.3,
we will show that the maximum margin classifier must suffer large test errors.

4.2.1 SETTING FOR GENERALIZATION ANALYSIS

Here we formally describe the setting of our generalization analysis. We let C ≥ 1 and 0 < _C ≤_ 1
denote positive absolute constants, whose value is fixed throughout the remainder of the paper. We
will use c, c[′], c1, . . . to denote “local” positive constants, which may take different values in different

[e]
contexts.

The training dataset := (x1, y1), . . ., (xn, yn) has n independently drawn samples. The condi_S_ _{_ _}_
tional distribution of the features given the label is
_x | {y = 1} = µ1 + Uq_
_x | {y = −1} = µ2 + Uq,_
where µ1 · µ2 = 0, ∥µ1∥ = ∥µ2∥, U is an arbitrary orthogonal matrix, and q is a random variable
such that: its entries are 1-sub-Gaussian and independent, and E _∥q∥[2][]_ _≥_ _Cd[e]_ .

We note that in past work that studied this setting (Chatterji & Long, 2021; Wang & Thrampoulidis,
2021; Cao et al., 2021), the cluster centers were chosen to be opposite one another µ1 = −µ2. Here,
since our goal here is to study a label shift setting we consider the cluster centers µ1 and µ2 to be
orthogonal. This ensures that learning the direction of one of the centers reveals no information
about the other center, which makes this problem more challenging in this setting.

Define P := {i ∈ [n] : yi = 1} to be the set of indices corresponding to the positive labels and
_N := {i ∈_ [n] : yi = −1} to be the set of indices corresponding to the negative labels. As stated
above, we will focus on the case wherethe number of positive and negative samples. The test distribution |P| ≫|N| ≥ 1. Let τ := P|N ||P|test[≥] is balanced. That is, if[1][ be the ratio between]

(x, y) ∼ Ptest, then P [y = 1] = P [y = −1] = 1/2 and x | y follows the distribution as described
above. We shall study the case where negative examples (which are in the minority) are upweighted.
Specifically, set the importance weights as follows: wi = 1 for i ∈P and wi = w for i ∈N .

**Assumptions.** Given a failure probability 0 < δ < 1/C, we make the following assumptions on
the parameters of the problem:

1. Number of samples n ≥ _C log(1/δ)._
2. Norm of the means _µ_ := _µ1_ = _µ2_ _Cn[2]_ log(n/δ).
_∥_ _∥[2]_ _∥_ _∥[2]_ _∥_ _∥[2]_ _≥_
3. Dimension d ≥ _Cn∥µ∥[2]._

Our assumptions allow for large overparameterization, where the dimension d scales polynomially
with the number of samples n.


-----

4.2.2 UPPER BOUND FOR THE REWEIGHTED POLYNOMIALLY-TAILED LOSS CLASSIFIER

First, we shall prove an upper bound on the test error of the solution learnt by gradient descent on the
reweighted polynomially-tailed loss. Under the choice of weights described above, by equation (2)


_w_

s.t. _zi_ _θ > 0,_ for all i [n],
_zi_ _θ_ _[,]_ _·_ _∈_
_·_


_θ1 = arg min_
_θ:∥θ∥=1_
b


_zi_ _θ_ [+]
_·_


_i∈P_


_i∈N_


where zi = yixi as defined previously. The following theorem provides an upper bound on the test
error of _θ1 and specifies a range of values for the weight w._

**Theorem 4.2. For any 0 <** _C ≤_ 1, there is a constant c such that, for all large enough C and
_for any 0[b] < δ < 1/C, under the assumptions of this subsection the following holds. If the weight_
_τ2[3]_ [e] _θ1 satisfying:_

_[≤]_ _[w][ ≤]_ [2][τ][ 3][,][ then with probability at least][ 1] _[−]_ _[δ][, training on][ S][ produces a classifier][ b]_

_µ_

TestError[θ[b]1] = P(x,y) Ptest sign _θ1_ _x_ = y exp _∥_ _∥[4]_ _._
_∼_ _·_ _̸_ _≤_ _−_ _[cn]τ_ _d_
h   i  
b

This theorem is proved in Appendix B. To prove this theorem, we first show that the data is linearly
separable under our assumptions and use the implicit bias results of Proposition 4.1. Then, our
upper bound is equivalent to bounding the test error of the limiting iterate of gradient descent on
our reweighted loss. Next, we show that the sum of the gradients across each of the two groups
remains roughly balanced throughout training under our choice of w. This ensures that the classifier
aligns well with both cluster centers µ1 and −µ2 which then proves our bound on the test error via
a standard Hoeffding bound.

If we consider a regime where d and n are growing simultaneously, then the test error goes down to
zero if the norm of the cluster means ∥µ∥ grows faster than d[1][/][4], regardless of the level of imbalance
in the training data as τ < n. Briefly note that in setting with balanced data (τ = 1), the bound on
the test error here matches the bound obtained for the maximum margin classifier in (Chatterji &
Long, 2021; Wang & Thrampoulidis, 2021; Cao et al., 2021) (although these bounds are not directly
comparable as the setting here is slightly modified for a label shift). The impossibility result by Jin
(2009) shows that these previous results guarantee that learning occurs right up until the information
theoretic frontier (∥µ∥[2] = ω(√d)).

Finally, it is interesting to note that the theorem requires the weight w to scale with τ [3] instead of
_τ (the unbiased importance weight), which would ensure that the reweighted training loss is an_
unbiased estimate of the test loss. In our proof, we find that in order to suffer low test error it is
important to guarantee that the norm of the gradients across the two groups is roughly balanced
throughout training. We show that at any training iteration, the ratio of the sum of the derivative of
the weighted losses in the majority and minority clusters scales as _w[1]τ[/][3][ . Thus choosing][ w][ to scale]_

with τ [3] ensures that the gradients are balanced across both groups. We verify this in our simulations
(Figure 2), and find that w = τ [3] ensures equal test error across both classes, reducing overall error.

4.2.3 LOWER BOUND FOR THE MAXIMUM MARGIN CLASSIFIER

In the previous section we derived an upper bound to demonstrate that classifiers trained with
polynomially-tailed losses achieve low test error. We will now show that classifiers trained with
exponentially-tailed losses have their error lower bounded by 1/8 in the same setting.

**Theorem 4.3. Let q** N(0, Id _d). There exist constants c and c[′]_ _such that, for all large enough_
_∼_ _×_
_C and for any 0 < δ < 1/C, under the assumptions of this subsection the following holds. With_
_probability at least 1 −_ _δ, training on S produces a maximum margin classifier_ _θMM satisfying:_

TestError[θ[b]MM] = P(x,y) Ptest sign _θMM_ _x_ = y [b] _,_
_∼_ _·_ _̸_ _≥_ 2[1] _−_ _[c][√]τ_ _[n]_ _· [∥]√[µ][∥]d[2]_
h   i _[·][ Φ]_  

2

b _√n_ _µ_

_where Φ is the Gaussian cdf. Furthermore, if the imbalance ratio τ_ _c[′]_ _√∥d_ _∥_ _then with probabil-_
_≥_

_ity at least 1 −_ _δ, TestError[θ[b]MM] ≥_ [1]8 _[.]_

This theorem is proved in Appendix C. Intuitively, the lower bound holds because when the imbalance is severe, the max-margin classifier essentially ignores the samples in the minority class and


-----

Test Error vs. Importance Weight (w = τ _[ρ])_


Test Error vs. Imbalance Ratio (τ )


10


25

20

15

10

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||w = 1 w = τ||||||
||w = τ 3 Max-m|argi|n||||
||||||||
||||||||
||||||||

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
|||Majority Minority|Class Class||||
|||Overall|||||
||||||||
||||||||
||||||||


_w = 1_
_w = τ_
_w = τ_ [3]

Max-margin


Majority Class
Minority Class
Overall


2 4 6 8 10 12 0 1 2 3 4 5

Imbalance Ratio (τ ) _ρ_

Figure 2: A simulation study with class-conditional Gaussians, with d = 10[6], ∥µ∥[2] = d[0][.][502] and
_n = 100. The mean µ1 =_ _µ_ _e1 and µ2 =_ _µ_ _e2, and q_ N(0, Id _d). (Left) We plot the test_
_∥_ _∥_ _∥_ _∥_ _∼_ _×_
error of _θ1 as τ varies for different choices of w, and also for the maximum margin classifier. The_
choice w = τ [3] leads to the lowest error throughout, while w = τ also does well for small τ . Both
the polynomially-tailed classifier with no importance weighting and the maximum margin classifier

[b]
suffer large test error. (Right) Here we fix the imbalance ratio τ = 10.1, and study how the error of
_θ1 varies with w. When w = τ_ [3], the error on both the majority and minority classes is almost equal,
resulting in low overall test error.
b


overfits to the majority class. Ignoring these negative samples results in a relatively small alignment
between the maximum margin classifier and the negative of the minority class center, which leads
to high error on this class.

Together, these two theorems demonstrate that there is a strict gap between the performance of
exponentially tailed and polynomially tailed losses under distribution shifts. As a concrete example,
1 1 3
consider the scaling where ∥µ∥[2] = d 2 [+][ 1]40, n = d 5 and τ = d 20 ≤ _n. For all large enough d, all_

our assumptions are satisfied. Now since, _n_ _µ_ _>_ _√τd, Theorem 4.2 guarantees that the test_

_[√]_ _∥3_ _∥[2]_ 5

error of _θ1_ 0 as d . However, as τ = d 20 _c[′]_ _µ_ _n/d = c[′]d_ 40, for all large enough d,

the test error of the maximum margin classifier is guaranteed to be at least → _→∞_ _≥_ _∥_ _∥[2][p]_ 1/8. This degradation of
the test error with τ is also apparent in our simulations in Figure 2.

[b]

5 EMPIRICAL EVALUATION ON DEEP INTERPOLATING CLASSIFIERS


Inspired by our theoretical results, we use a polynomially-tailed loss with importance weights to
train interpolating deep neural networks under distribution shift. We use a polynomially-tailed loss
with β = 1 and ℓleft(z) = log(1 + e[−][z])/ log(1 + e[−][1]), ensuring that ℓα,β is continuous at transition
_z = β when α = 1. We train models on two image classification datasets, one with label shift and_
one with subpopulation shift, and include additional experiment details in Appendix E. Though these
nonlinear networks violate the assumptions of our theory, polynomially-tailed loss with importance
weights consistently improves test accuracy for interpolating neural networks under distribution shift
compared to importance-weighted cross-entropy loss.

**Imbalanced binary CIFAR10.** We construct our label shift dataset from the full CIFAR10
dataset. Similar to Byrd & Lipton (2019), we create a binary classification dataset out of the “cat”
and “dog” classes. We use the official test examples as our label-balanced test set of 1000 cats and
1000 dogs. To form the train and validation sets, we use all 5000 cat examples but only 500 dog
examples from the official train set, corresponding to a 10:1 label imbalance. We then use 80%
of those examples for training and the rest for validation. We use the same convolutional neural
network architecture as Byrd & Lipton (2019) with random initializations for this dataset.

**Subsampled CelebA.** For our subpopulation shift dataset, we use the CelebA with spurious correlations dataset constructed by Sagawa et al. (2019). This dataset has two class labels, “blonde
hair” and “dark hair”. Distinguishing examples by the “male” versus “female” attribute results in
four total subpopulations, or groups. The distribution shift in the dataset comes from the change
in relative proportions among the groups between the train and test sets. To reduce computation,
we train on 2% of the full CelebA training set, resulting in group sizes of 1446, 1308, 468, and 33.
We construct our test set by downsampling the original test to get group-balanced representation in


-----

Trained to Interpolation


0.70

0.65

0.60

0.55

0.50

0.45

0.40




Figure 3: Polynomially-tailed loss versus cross-entropy loss on a label shift dataset and a subpopulation shift dataset for neural networks optimized past 100% train accuracy without regularization.
_∗_ and ∗∗ indicate p < 0.05 and p < 0.005 statistical significance, respectively. Importance weights
(IW) consistently leads to gains when used with the polynomially-tailed loss across both datasets.
Exponentiating the weights further amplifies these gains for the polynomially-tailed loss. IW has

the test set, resulting in 180 examples for each group. Following Sagawa et al. (2019), we use a

|Imbalanced Binary CIFAR10 Trained to|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|
|---|---|---|---|---|---|---|---|---|---|
|∗ ∗∗ ∗∗||||||||||
|||||||||||
|||||||||||
|||||||||||
|||||||||||
|||||||||||
|||||||||||
||No : Pol ift da indic nsiste tiatin ect fo set, r 50 wi|IW ynom taset ate p ntly l g the r cros esulti th Im||IW y-tail neur 0.05 s to eights ntrop in 18 eNet i|(w) ed lo al net and p gains furth y.2 0 exa nitial||IW ( versus rks o 0.00 en us ampl les f tion f|w3/2) cros ptimi 5 stat ed w ifies t or eac or thi||



5.1 ISOLATING THE EFFECTS OF IMPORTANCE WEIGHTS


**Interpolating regime.** To understand the impact of polynomial losses in the interpolating regime,
we train unregularized neural networks with SGD past 100% train accuracy and report their final accuracies on the test sets. We compare models trained with our polynomially-tailed loss against those
trained with the cross-entropy loss which has exponential tails. Let w correspond to the unbiased
importance weights. We consider three weighting scenarios for both loss functions: no importance
weighting at all (No IW) the classical unbiased importance weighting (IW (w)), and biased weighting where we exponentiate the weights, (IW (w[c])), increasing the ratio of different weights. The
third setting is inspired by our theory in Section 4 that shows biased importance weights can improve performance for polynomially-tailed losses. For these experiments, we fixed α = 1 and set
the exponents to be the largest value that still allowed for stable optimization. For CelebA, we exponentiate by 2 and for CIFAR10 we exponentiate by 3/2. We run 10 seeds for each experiment
setting. We compare the two losses via paired one-sided Welch’s t-tests, pairing runs with the same
random seed. We report the exact numbers in Appendix E.

Figure 3 shows the mean accuracy and the standard error for each of the three settings. As indicated
by our theory, we find that importance weighting with polynomially-tailed losses leads to statisti
|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|r|po|lat|io|n|||||||||||
||||||||||||||||
|.9|0||||||||||||||
||||||P C|ol ro|y-tailed Loss ss Entropy||||||||
|||∗∗ ∗|||||||||||||
||||||||||||||||
||||||||||||||||
||||||||||||||||
|||No ss on 0% tra ficanc nomia for th ollowi TS nomia % trai our po l tails. os for e weig asing hows ese ex stable 2. We Welch’ . or eac||||IW a labe in ac e, res lly-ta e poly ng S l loss n acc lyno Let both hting the ra biased perim opti run s t-tes h of t||l s cur pec ile no aga es ura mia w los (I tio i en miz 10 ts, he|IW hift d acy w tively d loss miall wa et in the cy an lly-ta corres s fun W (w of d mport ts, we ation seeds pairi three|(w) ataset ithou . Imp acro y-tail al. ( inter d rep iled l pond ctions )), an iffere ance fixe . For for ng ru settin|an t r or ss ed 20 pol ort oss to : n d b nt we d α Ce eac ns gs.|IW d a s egula tance both loss. 19), w ating their again the u o imp iased weigh ights = 1 lebA h exp with t As i|(w2) ubpop rizati weig datase IW h e us regim final st tho nbias ortan weig ts. T can i and, we e erim he sa ndicat|u- on. hts ts. as e a e, ac- se ed ce ht- he m- set x- ent me ed|

cally significant gains over cross-entropy in all cases. Further we find that exponentiating weights
boosts the performance of polynomially tailed losses, confirming our claims in Theorem 4.2. However, exponentiating the weights leaves the performance of cross-entropy loss largely unchanged. In
the case of CelebA, we find that exponentiated reweighting with poly-tailed losses outperforms all
other methods. While in the case of Imbalanced Binary CIFAR10, we find a smaller gap between
polynomially-tailed losses and cross-entropy, partially due to a substantially higher run-to-run variability in training. This variability does not affect our main conclusion: exponentiated reweighting
with poly-tailed losses still outperforms cross-entropy with significance p = 0.01.

In Appendix D, we also compare the performance of polynomially-tailed losses with the crossentropy loss when we regularize training via early-stopping. We find that even with regularization,
poly-tailed losses give test accuracies better than or similar to cross-entropy across scenarios.

5.2 COMPARING AGAINST PRIOR DISTRIBUTION SHIFT METHODS


To place our method’s empirical performance in the context of prior works, we extensively compare
our method to state-of-the-art distribution shift correction methods. On binary CIFAR10, we com
2The mean test accuracy for No IW with cross-entropy on CIFAR10 appears to deviate from IW, but this
is due to the high variance across multiple runs of the model. The high variance arises due to 10:1 imbalanced
training data, combined with the fact that we train the models until they interpolate the training data. These
results for No IW differ slightly from those in Byrd & Lipton (2019) which used balanced training data and
imbalanced test data.


-----

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|
|---|---|---|---|---|---|---|---|---|---|
|||||||||||
|||||||||||
|||||||||||
|||||||||||
||CE+US ynom te of t e val ware ctor-s distri||LDAM lly-tai e art ation argin ling l tion-a||CDT d loss ethod et. W oss (L s (VS are m||LA with e that a comp AM), and t rgin (||VS ponen dress d e to cr class-d e use o DAM)|


(58.6% test accuracy). More thorough grid search for VS loss gave optimal hyperparameters that

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|
|---|---|---|---|---|---|---|---|---|---|
|||||||||||
|||||||||||
|||||||||||
|||||||||||
|n if w p al a e l f d i er o ti ie p e t s l e e|CE+US ce we t. We ith un eratu ly rob l., 20 non et oss o distri loss v nitiali s we ur pol on wi rs. erly t n exce ropy ( ses to y to L inter t al. (|ig d re u 1 nl b e za fo y th un e C f A va 20|VS hts is tune a ersam s loss st opt 9), cla al., 20 y sinc utiona rsus V tion s und. -tailed out e ed, o ding t E) lo orm a loss ls. At 21), a|b ll p ( i s 2 e ll e U l ar ur h ss a n|Poly+DRO etter meth ling ( CDT) mizati s-depe 0), an it en y robu S loss. eds. S nlike oss. F ly sto poly ose b with balanc nd is first, w d it pe|th o C, o n d ca s e i o p -t y u e o e r|CE+DRO an or ds ext E+US logit- n (DR dent t vecto psula t opti We gri e Ap n Sec r all ping ailed l these nders d trai nly 2 grid forme|c e ) a O e r- te m d pe ti m or o r a ni % se d|VS+DRO ompet nsively, label djusted ). mpera scaling s CDT ization search ndix E on 5.1 ethods strong ss with ecentl mpling ng set wors arched poorly|


Imbalanced Binary CIFAR10 Subsampled CelebA

0.90

0.7

0.85

0.6

0.80

Test Accuracy 0.5 Test Accuracy 0.75

0.4 Poly+IW CE+US LDAM CDT LA VS 0.70 Poly+IW CE+US VS Poly+DRO CE+DRO VS+DRO

Figure 4: Polynomially-tailed loss with exponentiated importance weights is better than or competitive with state of the art methods that address distribution shift. We tune all methods extensively
using the same validation set. We compare to cross entropy with undersampling (CE+US), labeldistribution-aware margin loss (LDAM), class-dependent temperatures loss (CDT), logit-adjusted
loss (LA), vector-scaling loss (VS), and the use of distributionally robust optimization (DRO).

pare to label-distribution-aware margin (LDAM) loss (Cao et al., 2019), class-dependent temperatures (CDT) loss (Ye et al., 2020), logit-adjusted (LA) loss (Menon et al., 2020), and vector-scaling
(VS) loss (Kini et al., 2021). On CelebA, we compare to VS loss only since it encapsulates CDT
loss, LA loss, and LDAM loss. We also evaluate the effects of distributionally robust optimization
(DRO) by Sagawa et al. (2019) when combined with poly-tailed loss versus VS loss. We grid search
each method exhaustively, and report results over 10 random initialization seeds. See Appendix E
for more details on our procedure and the best hyperparameters we found. Unlike in Section 5.1,
here we tune both the importance weight exponent and α for our poly-tailed loss. For all methods,
including ones that use DRO, we train all models to interpolation without early stopping or strong
regularizations, since we are interested in interpolating classifiers.

Figure 4 shows that when α and the weights’ exponent are properly tuned, our poly-tailed loss with
_exponentiated weights gives accuracies comparable to or even exceeding those by these recently_
_proposed methods. For both datasets, we found that cross entropy (CE) loss with undersampling_
(US), which discards data from overrepresented groups or classes to form a balanced training set,
was a strong baseline.

On binary CIFAR10, our poly-tailed loss performs comparably to LA loss and is only 2% worse
than VS loss in mean test accuracy with overlapping confidence intervals. At first, we grid searched
VS loss across the published hyperparameter ranges from Kini et al. (2021), and it performed poorly

were similar to those of LA loss, increasing test accuracy by 10%. In sum, our poly-tailed loss
matched or exceeded existing data imbalance adjustment methods (under published hyperparameters) and was beaten by 2% only after more extensively tuning the baselines.

On CelebA, our poly-tailed loss with IW achieves comparable test accuracy to VS loss. When
we use both loss functions with DRO, poly loss with DRO is better than VS loss with DRO by
3.5% on test accuracy and 8% better on worst group accuracy. Poly-tailed loss with IW gives the
best worst group accuracy out of all loss functions when not using DRO (see Appendix E). Such
improved performance is surprising, as we designed polynomially tailed losses to optimize for static
importance weights, rather than the worst case weights in DRO. The finding that poly-tailed losses
with IW performs well on worst-group accuracy may be an interesting direction for future work

6 CONCLUSION

Contrary to popular belief, importance weights are in fact compatible with the current paradigm
of overparameterized classifiers, provided that the loss function does not decay exponentially. The
limiting behavior of an interpolating linear classifier trained with a weighted polynomially-tailed
loss provably depends on the importance weights, unlike the maximum-margin classifier recovered by an exponentially decaying loss function. Our theoretical analysis of generalization error
further suggested that exponentiating the classic importance weights can be key to accurate distribution shift correction. We empirically corroborated these theoretical intuitions, finding that weighted
polynomially-tailed losses are also able to address distribution shift for interpolating neural networks. Our work suggests that heavy-tailed losses together with importance weighting serve as a
simple and general candidate for addressing distribution shift in deep learning.


-----

ETHICS STATEMENT

We use publicly available datasets in our paper, and do not foresee any potential harm arising from
the use of our methods.

REPRODUCIBILITY STATEMENT

The full proofs of our theoretical results are presented in the appendix. All experimental details and
hyperparameters are presented in Appendix E.

REFERENCES

Brian Beavis and Ian Dobbs. Optimisation and stability theory for economic analysis. Cambridge
University Press, 1990.

Jonathon Byrd and Zachary Lipton. What is the effect of importance weighting in deep learning?
In International Conference on Machine Learning, pp. 872–881, 2019.

Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced
datasets with label-distribution-aware margin loss. Advances in Neural Information Processing
_Systems, pp. 1567–1578, 2019._

Yuan Cao, Quanquan Gu, and Mikhail Belkin. Risk bounds for over-parameterized maximum margin classification on sub-Gaussian mixtures. arXiv preprint arXiv:2104.13628, 2021.

Niladri S Chatterji and Philip M Long. Finite-sample analysis of interpolating linear classifiers in
the overparameterized regime. Journal of Machine Learning Research, 22(129):1–30, 2021.

Ziwei Ji and Matus Telgarsky. The implicit bias of gradient descent on nonseparable data. In
_Conference on Learning Theory, pp. 1772–1798, 2019._

Ziwei Ji and Matus Telgarsky. Directional convergence and alignment in deep learning. In Advances
_in Neural Information Processing Systems, pp. 17176–17186, 2020._

Ziwei Ji, Miroslav Dudik, Robert E Schapire, and Matus Telgarsky. Gradient descent follows the
regularization path for general losses. In Conference on Learning Theory, pp. 2109–2136, 2020.

Jiashun Jin. Impossibility of successful classification when useful features are rare and weak. Pro_ceedings of the National Academy of Sciences, 106(22):8859–8864, 2009._

Ganesh Ramachandra Kini, Orestis Paraskevas, Samet Oymak, and Christos Thrampoulidis.
Label-imbalanced and group-sensitive classification under overparameterization. arXiv preprint
_arXiv:2103.01550, 2021._

Tengyuan Liang and Benjamin Recht. Interpolating classifiers make few mistakes. arXiv preprint
_arXiv:2101.11815, 2021._

Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks.
In International Conference on Learning Representations, 2019.

Aditya Krishna Menon, Sadeep Jayasumana, Ankit Singh Rawat, Himanshu Jain, Andreas Veit, and
Sanjiv Kumar. Long-tail learning via logit adjustment. In International Conference on Learning
_Representations, 2020._

Vidya Muthukumar, Adhyyan Narang, Vignesh Subramanian, Mikhail Belkin, Daniel Hsu, and
Anant Sahai. Classification vs regression in overparameterized regimes: Does the loss function
matter? Journal of Machine Learning Research, 22(222):1–69, 2021.

Mor Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique Pamplona Savarese, Nathan
Srebro, and Daniel Soudry. Convergence of gradient descent on separable data. In Conference on
_Artificial Intelligence and Statistics, pp. 3420–3428, 2019._

Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust
neural networks for group shifts: On the importance of regularization for worst-case generalization. arXiv preprint arXiv:1911.08731, 2019.


-----

Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the loglikelihood function. Journal of Statistical Planning and Inference, 90(2):227–244, 2000.

Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. Journal of Machine Learning Research, 19(1):
2822–2878, 2018.

Roman Vershynin. High-dimensional probability: An introduction with applications in data science.
Cambridge University Press, 2018.

Ke Wang and Christos Thrampoulidis. Benign overfitting in binary classification of Gaussian mixtures. In IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 4030–
4034, 2021.

Junfeng Wen, Chun-Nam Yu, and Russell Greiner. Robust learning under uncertain test distributions: Relating covariate shift to model misspecification. In International Conference on Machine
_Learning, pp. 631–639, 2014._

Da Xu, Yuting Ye, and Chuanwei Ruan. Understanding the role of importance weighting for deep
learning. In International Conference on Learning Representations, 2020.

Han-Jia Ye, Hong-You Chen, De-Chuan Zhan, and Wei-Lun Chao. Identifying and compensating
for feature deviation in imbalanced deep learning. arXiv preprint arXiv:2001.01385, 2020.


-----

A PROOF OF PROPOSITION 4.1

First, we restate the statement of the proposition.
**Proposition 4.1. Suppose that the data is linearly separable. For any α > 0, β ∈** R, any initial
_point θ[(0)]_ _∈_ R[d], and for all small enough step-sizesθ[(][t][)] _η the direction of the gradient descent iterates_
_satisfy the following: limt→∞_ _∥θ[(][t][)]∥_ _[→]_ _θ[b]α._

**Proof Recall that the loss ℓα,β is assumed to be convex, strictly decreasing to zero and is differen-**
tiable. Define the minimizer of the loss over a ball of radius R as follows


_θR = arg min_ _Lα,β(θ)._
_θ:∥θ∥≤R_

Under the assumptions of this proposition, we can invoke Theorem 1 by Ji et al. (2020) to get thatb

_θ[(][t][)]_ _θR_
lim
_t→∞_ _∥θ[(][t][)]∥_ [= lim]R→∞ _R [.]_

We will therefore instead demonstrate that limR→∞ _θRR_ [=][ b]θα to establish our claim.

First, note that the classifier θR is the minimizer of loss in a ball of radius R, so there exists a R0
with
_Lα,β(θR0_ ) = _wiℓα,β(zi · θR0_ ) ≤ (mini _wi)ℓα,β(β)_

_iX∈[n]_

such that for allexample is at least R > R β. Therefore,b0, each example is classified correctly by θR and the margin (zi · _θR) on each_


_wi_

[zi _θR_ (β 1)][α][ .]
_·_ _−_ _−_


_Lα,β(θR) =_


_wiℓα,β(zi_ _θR) =_
_i=1_ _·_

X


_i=1_


Thus, for any radius R > R0 we have


_θR_

_Lα,β(θ)_

_R_ [= 1]R [arg min]θ:∥θ∥≤R

_n_

b _wi_

= [1] s.t., zi _θ_ _β_

_R_ [arg min]θ:∥θ∥≤R (Xi=1 [zi · θ − (β − 1)][α][,] _·_ _≥_ )

_n_

= R[1] [arg min]θ:∥θ∥≤R  _R1[α]_ Xi=1 _zi ·_ _θR_ _w−i_ _[β]R[−][1]_ _α,_ s.t., zi ·   θR  _≥_ _R[β]_

hn    i

 _wi_

= [1] _R_ arg min _α,_ s.t., zi _θ_ _R_

_R_ _[×]_  _·_ _θ:∥θ∥≤1_ Xi=1 _zi · θ −_ _[β]R[−][1]_ _·_ _≥_ _[β]_ 

 h i

Now taking the limit R we get that 
_→∞_

_n_

_θR_ _wi_
_Rlim→∞_ _R_ [= lim]R→∞ [arg min]θ:∥θ∥≤1 Xi=1 _zi · θ −_ _[β]R[−][1]_ _α,_ s.t., zi · θ ≥ _R[β]_

_n_ h i

(i)  _wi_
= arg minθ:∥θ∥≤1 R→∞ Xi=1 _zi · θ −_ _[β]R[−][1]_ _α,_ s.t., zi · θ ≥ _R[β]_

_n_ h i

 [lim] _wi_

= arg min s.t., zi _θ_ 0
_θ:∥θ∥≤1_ (Xi=1 (zi · θ)[α][,] _·_ _≥_ )

_n_

(ii) _wi_
= arg min s.t., zi _θ > 0_
_θ:∥θ∥≤1_ (Xi=1 (zi · θ)[α][,] _·_ )

= _θα._

[b]


_._






-----

where (i) follows since for every R the function


_wi_

_zi_ _θ_ _R_
_·_ _−_ _[β][−][1]_


_α_
i


_i=1_

is convex and continuous, and therefore


_wi_

_zi_ _θ_ _R_
_·_ _−_ _[β][−][1]_


_wi_

arg min _α,_ s.t., zi _θ_ _R_
_θ:∥θ∥≤1_ Xi=1 _zi · θ −_ _[β]R[−][1]_ _·_ _≥_ _[β]_ 

h i

has a unique minimizer (since the objective function is strictly convex, and the constraint set is 
convex and compact), and is a continuous map by Berge’s maximum theorem (see, e.g., Beavis &
Dobbs, 1990, Theorem 3.6). Thus, it is possible to switch the order of the limit and the arg min.
Equation (ii) follows since the data is linearly separable, so there exists a θ such that zi _θ > 0 for_
all i which has finite objective value. _·_


arg min
_θ:∥θ∥≤1_


_i=1_


Therefore,

which finishes our proof.


_θ[(][t][)]_ _θR_
lim _θα,_
_t→∞_ _∥θ[(][t][)]∥_ [= lim]R→∞ _R_ [=][ b]


B PROOF OF THEOREM 4.2

In this section, we will prove an upper bound on the test error _θ1, which is the classifier learnt_
by gradient descent with the importance weighted Polynomially-tailed loss that decays as 1/z. By
Proposition 4.1, we know the choice of ℓleft and β does not affect the implicit bias of gradient

[b]
descent, hence here, for the sake of convenience, we shall use the specific loss function


_−z_ ((zβ1 −1)β) + 1 ifif z < β z _β,_

_−_ _−_ _≥_


_ℓ1,β(z) :=_


with β = 1 − _wn < 0. It can be easily checked that this loss function is convex, differentiable and_
monotonically decreasing.

We want to bound the test error of _θ1, however, in light of Proposition 4.1 we will instead bound_
the test error of the limiting iterate of gradient descent starting from θ[(0)] = 8w[2][/][3]n _µ1_ _w[1][/][3]µ2_

with step-size η > 0 (again by the implicit bias of gradient descent is not affected by the choice of −

[b]
  

the initial point):


_θ[(][t][+1)]_ = θ[(][t][)] _−_ _η∇L[b](θ[(][t][)]),_


where


_L(θ) =_


_ℓ1,β(zi_ _θ) + w_
_iX∈P_ _·_


_ℓ1,β(zi_ _θ)._
_iX∈N_ _·_


_θ[(][t][)]_
Recall that if the step-size is small enough then Proposition 4.1 guarantees that limt→∞ _∥θ[(][t][)]∥_ [=][ b]θ1.

In this section we will simply let ℓ denote ℓ1,β and also use the shorthands ℓ[(]i[t][)] := ℓ(zi _θ[(][t][)]) and_
_·_
_ℓ[′]i[(][t][)]_ := ℓ[′](zi _θ[(][t][)])._
_·_

With this setup in place let us begin the proof. All of the assumptions made in Section 4.2.1 are in
scope here. First we have a lemma that upper bounds the test error using Hoeffding’s inequality.

**Lemma B.1. There is a positive absolute constant c such that**





exp _c_ [(][θ][ ·][ µ][1][)][2]
_−_ _θ_
 _∥_ _∥[2]_


+ exp _c_ [(][θ][ ·][ µ][2][)][2]
_−_ _θ_
 _∥_ _∥[2]_


P(x,y) Ptest [sign (θ _x)_ = y]
_∼_ _·_ _̸_ _≤_ 2[1]


-----

This lemma follows by mirroring the proof of (Chatterji & Long, 2021, Lemma 9) which in turn
follows by a simple application of Hoeffding’s inequality (Vershynin, 2018, Theorem 2.6.3).

Next we have a lemma that proves bounds on the norms of the samples, bounds the inner products
between the samples and also shows that with high probability the data is linearly separable.

Recall that the constants C ≥ 1 and 0 < _C ≤_ 1 were defined above in Section 4.2.

**Lemma B.2. For all 0 <** _C ≤_ 1, there is a c ≥ 1 such that, for all large enough C, with probability
1 _δ over the draw of the samples the following events simultaneously occur:[e]_
_−_

[e]

_1. For all k ∈_ [n]
_d_
_c_

_[≤∥][z][k][∥][2][ ≤]_ _[cd.]_

_2. For all i ∈P and j ∈N_ _,_

_zi_ _zj_ _c_ _d log(n/δ)._
_|_ _·_ _| ≤_
p


_3. For all i ̸= j ∈_ [n],

_4. For all k ∈P,_

_5. For all k ∈N_ _,_

_6. For all k ∈N_ _,_

_7. For all k ∈P,_


_zi_ _zj_ _< c(_ _µ_ +
_|_ _·_ _|_ _∥_ _∥[2]_


_d log(n/δ))._


_µ1_ _zk_ _µ_ _<_ _µ_ _/2._
_|_ _·_ _−∥_ _∥[2]|_ _∥_ _∥[2]_

( _µ2)_ _zk_ _µ_ _<_ _µ_ _/2._
_|_ _−_ _·_ _−∥_ _∥[2]|_ _∥_ _∥[2]_


_µ1_ _zk_ _< c_ _µ_
_|_ _·_ _|_ _∥_ _∥_

_µ2_ _zk_ _< c_ _µ_
_|_ _·_ _|_ _∥_ _∥_


log(n/δ).

log(n/δ).


_8. The samples are linearly separable._

**Proof We shall prove that each of the ten parts hold with probability at least 1 −** _δ/10 and take a_
union bound to prove our lemma. Under the assumptions of this lemma, Parts (1), (3)-(5) and (8) can
be shown to hold with the required probability by invoking (Chatterji & Long, 2021, Lemma 13).
We will show that Parts (2), (6) and (7) also hold with the required probability to complete the proof.

_Proof of Part (2): Note that for all i ∈P, zi = µ1 + Uqi, and for all i ∈N_, zi = µ2 + Uq2.
Therefore, for any i ∈P and j ∈N, since µ1 · µ2 = 0,

_zi_ _zj_ = µ1 (Uqj) + µ2 (Uqi) + qi _qj_
_|_ _·_ _|_ _·_ _·_ _·_
_µ1_ (Uqj) + _µ2_ (Uqi) + _qi_ _qj_
_≤|_ _·_ _|_ _|_ _·_ _|_ _|_ _·_ _|_

= (U _µ1)_ _qj_ + (U _µ2)_ _qi_ + _qi_ _qj_ _._ (3)
_|_ _[⊤]_ _·_ _|_ _|_ _[⊤]_ _·_ _|_ _|_ _·_ _|_

We will show that each of these terms is small with high probability for all pairs i ∈P and j ∈N .

For the first two terms, note that by Hoeffding’s inequality (Vershynin, 2018, Theorem 2.6.3)


_∥µ∥[2]_ log(n/δ)

_∥µ∥[2]_


_< 2 exp_
_−_ _[c][1]9[c][3]_


= 2 exp
_−_ _[c][1]9[c][3]_



(U _µ1)_ _qj_ _> [c][∥][µ][∥]_
_|_ _[⊤]_ _·_ _|_


log(n/δ)


log(n/δ)


_δ_
= 2

_n_



_δ_
_≤_ 30n _[,]_


_c1c[3]/9_



-----

where the last inequality follows since the constant c ≥ 1 can be chosen to be large enough and
because δ < 1/C for a large enough constant C. Therefore, by taking a union bound over all j ∈N


P "∃j ∈N _, |(U_ _[⊤]µ1) · qj| > [c][∥][µ][∥]_

Using an analogous argument we can also show that

P "∃i ∈P, |(U _[⊤]µ2) · qi| > [c][∥][µ][∥]_


log(n/δ)

3

log(n/δ)


_< [δ]_ (4)

30 _[.]_


_< [δ]_ (5)

30 _[.]_

#

(6)

_≤_ 30[δ] _[.]_


Next, using (Chatterji & Long, 2021, inequality (15)) we get that


_i_ = j [n], _qi_ _qj_ _> [c]_
_∃_ _̸_ _∈_ _|_ _·_ _|_


_d log(n/δ)_


Combining inequalities (3)-(6) we get that for all i ∈P and j ∈N with probability at least 1 _−_ _δ/10_

log(n/δ) _d log(n/δ)_

_zi_ _zj_ + _[c]_ _c_ _d log(n/δ),_
_|_ _·_ _| ≤_ [2][c][∥][µ][∥]p3 p 3 _≤_

p

where the last inequality follows since by assumption d ≥ _Cn∥µ∥[2]_ and because C is large enough.
This completes the proof of this part.

_Proof of Part (6): For any k ∈N_, zk = µ2 + Uqk. Recall that µ1 · µ2 = 0, thus

_µ1_ _zk_ = _µ1_ (Uqk) _._
_|_ _·_ _|_ _|_ _·_ _|_

Invoking inequality (4) proves that this part holds with probability at least 1 − _δ/10._

_Proof of Part (7): This follows by an analogous argument as in the previous part._


We continue by defining a good event that we will work under for the rest of the proof.
**Definition B.3. If the training dataset S satisfies all the conditions specified in Lemma B.2 then we**
_call it a good run._

Going forward in this section we shall assume that a good run occurs.

The following lemma provides some useful bound on the loss ratio at initialization and guarantees
that the loss of example remains in the polynomially-tailed part throughout training.

**Lemma B.4.i ∈N** _. On a good run, for all Recall that θ[(0)] i= 8 ̸= jw ∈[2][/][3][nn](µ1 −_ _w[1][/][3]µ2), and that wi = 1 if i ∈P and wi = w if_

1/3

_ℓ[(0)]i_ 8 _wj_ _._

_ℓ[(0)]j_ _≤_  _wi_ 

_Furthermore, if the step-size η is sufficiently small then on a good run, for all t ∈{0, 1, . . .} and for_
_all i ∈_ [n]


_ℓ[(]i[t][)]_


_zi_ _θ[(][t][)]_ (β 1) _[.]_
_·_ _−_ _−_


**Proof Consider an i ∈P then**

(i) 3 _µ_ 2
_zi_ _θ[(0)]_ = 8w[2][/][3]n _zi_ _µ1_ _w[1][/][3]zi_ _µ2_ 8w[2][/][3]n _∥_ _∥_ + c1w[1][/][3] _µ_ log(n/δ)
_·_ _·_ _−_ _·_ _≤_ 2 _∥_ _∥_
   p

log(n/δ)

12w[2][/][3]n _µ_ 1 + [2][c][1][w][1][/][3][p]
_≤_ _∥_ _∥[2]_ " 3 _µ_ #

_∥_ _∥_

(ii)
_≤_ 16w[2][/][3]n∥µ∥[2],


-----

whereC (i) follows by Lemma B.2 and (ii) follows since ∥µ∥[2] _≥_ _Cn[2]_ log(n/δ) ≥ _Cτ_ [2] log(n/δ) ≥

2[2][/][3][ w][2][/][3][ log(][n/δ][)][ for a large enough constant][ C][. Similarly, we also have the lower bound]

_zi_ _θ[(0)]_ = 8w[2][/][3]n _zi_ _µ1_ _w[1][/][3]zi_ _µ2_
_·_ _·_ _−_ _·_
 _µ_ 2  log(n/δ)

8w[2][/][3]n _∥_ _∥_ _c1w[1][/][3]_ _µ_ log(n/δ) 4w[2][/][3]n _µ_ 1
_≥_  2 _−_ _∥_ _∥_  _≥_ _∥_ _∥[2]_ " _−_ [2][c][1][w][1][/][3]∥[p]µ∥ #

p

Recall from the definition of the loss that we setC _β = 1 −_ _nw._ Now again since ∥µ∥[2] _≥_

2[2][/][3][ w][2][/][3][ log(][n/δ][)][, and because we choose][ β][ = 1][ −] _[nw][, we infer that]_

_zi_ _θ[(0)]_ (β 1) _nw_ 2w[2][/][3]n _µ_ _._
_·_ _−_ _−_ _≥_ [8][w][2][/][3]3[n][∥][µ][∥][2] _−_ _≥_ _∥_ _∥[2]_

Since the margin of the point is larger than (β − 1) therefore by the definition of the loss function
above we have that

1
_ℓ[(0)]i_ = _zi_ _θ[(0)]_ (β 1)

_·_ _−_ _−_
and hence


1 1

16w[2][/][3]n _µ_ _i_ _≤_ 2w[2][/][3]n _µ_ (7)
_∥_ _∥[2][ ≤]_ _[ℓ][(0)]_ _∥_ _∥[2][ .]_

By mirroring the logic we can also prove that for all j ∈N
1 1

16wn _µ_ _j_ _≤_ 2wn _µ_ (8)
_∥_ _∥[2][ ≤]_ _[ℓ][(0)]_ _∥_ _∥[2][ .]_

By combining equations (7) and (8) we immediately get that

1/3

_ℓ[(0)]i_ 8 _wj_ _._

_ℓ[(0)]j_ _≤_  _wi_ 

This proves the first part of the lemma.

To prove the second part we shall prove that the loss on each example remains smaller than 1/2,
which ensures ℓ[(]i[t][)] is equal to the polynomial loss function. Note that

_w_ _wn_

_L(θ[(0)]) =_ _ℓ[(0)]i_ + w _ℓ[(0)]i_ _≤_ 2w[2][/]|P|[3]n _µ_ 2wn|N|µ 2wn _µ_ (9)

_iX∈P_ _iX∈N_ _∥_ _∥[2][ +]_ _∥_ _∥[2][ ≤]_ _∥_ _∥[2][ ≤]_ [1][/][2][.]

b

This proves that ℓ[(0)]i 1/2 for all i [n]. By (Ji et al., 2020, Lemma 2) we know that if the step_≤_ _∈_
size is small enough then the sequence _L(θ[(][t][)])_ is non-increasing. Hence, we have that ℓ[(]i[t][)] 1/2
_{[b]_ _}_ _≤_
for all t ∈{0, 1, . . .} and for all samples. This wraps up our proof.

The next lemma lower bounds the inner product between the normalized gradient descent iterates,
and µ1 and µ2.
**Lemma B.5. Let c and c[′]** _be absolute constants. Then, on a good run, for any t ∈{0, 1, . . .}_

_µ1_ _θ[(][t][+1)]_ 1

_∥θ ·[(][t][+1)]∥_ _[≥]_ _∥[µ]θ[1][(][ ·][t][+1)][ θ][(0)]∥_ [+][ ∥][µ][∥]2[2]c[p]√d|N|  1 + _cη_ _|N |d_ _ts=0∥θ[(0)]∥i∈[n]_ _[−][w][i][ℓ]i[′][(][s][)]_  _×_

 q P P 

_t_ _c[′][√]log(n/δ)_
_s=0_ _i_ _i_ _µ_ _i_ _i_

_∈P_ _[−][w][i][ℓ][′][(][s][)]_ _−_ _∥_ _∥_ _∈N_ _[−][w][i][ℓ][′][(][s][)]_

P P _t_ P

_s=0_ _i∈[n]_ _[−][w][i][ℓ][′]i[(][s][)]_

_and_

P P

_µ2_ _θ[(][t][+1)]_ 1

_∥θ ·[(][t][+1)]∥_ _[≥]_ _∥[µ]θ[2][(][ ·][t][+1)][ θ][(0)]∥_ [+][ ∥][µ][∥]2[2]c[p]√d|N|  1 + _cη_ _|N |d_ _ts=0∥θ[(0)]∥i∈[n]_ _[−][w][i][ℓ]i[′][(][s][)]_  _×_

 q P P 

_t_ _c[′][√]log(n/δ)_
_s=0_ _i_ _i_ _µ_ _i_ _i_

_∈N_ _[−][w][i][ℓ][′][(][s][)]_ _−_ _∥_ _∥_ _∈P_ _[−][w][i][ℓ][′][(][s][)]_ _._

P P _t_ P

_s=0_ _i∈[n]_ _[−][w][i][ℓ]i[′][(][s][)]_
P P


-----

**Proof Let us prove the first claim for the inner product with µ1. The second claim regarding µ2**
shall follow analogously. For any t ∈{0, 1, . . .}, by the definition of the gradient descent step, we
have that


_µ1_ _θ[(][t][+1)]_ = µ1 _θ[(][t][)]_ + η
_·_ _·_


_wiµ1_ _zi_ _ℓ[′]i[(][t][)]_ + η
_i_ _·_ _−_

X∈P  


_wiµ1_ _zi_ _ℓ[′]i[(][t][)]_
_i_ _·_ _−_

X∈N 


Note that by the definition of the loss function we have that _ℓ[′]i[(][t][)]_ 0 for all t and all i. Thus by
_−_ _≥_
Parts (1) and (6) of Lemma B.2 we get that


_µ1_ _θ[(][t][+1)]_ _µ1_ _θ[(][t][)]_ + _[η][∥][µ][∥][2]_
_·_ _≥_ _·_ 2

= µ1 _θ[(][t][)]_ + _[η][∥][µ][∥][2]_
_·_ 2

= µ1 _θ[(][t][)]_ + _[η][∥][µ][∥][2]_
_·_ 2


_wiℓ[′]i[(][t][)]_ _c1η_ _µ_
_−_ _−_ _∥_ _∥_
_iX∈P_


_wiℓ[′]i[(][t][)]_
_−_
_iX∈N_

log(n/δ)

_µ_ _−wiℓ[′]i[(][t][)]_
_∥_ _∥_ ! Xi∈N


log(n/δ)

1 + _[c][1]_
p


log(n/δ)

= µ1 · θ[(][t][)] + _[η][∥][µ]2[∥][2]_ _iX∈[n]_ _−wiℓ[′]i[(][t][)]_ _−_ _[η][∥][µ]2[∥][2]_ 1 + _[c][1]p∥µ∥_ ! Xi∈N _−wiℓ_

log(n/δ)

= µ1 · θ[(][t][)] + _[η][∥][µ]2[∥][2]_ "Xi∈P _−wiℓ[′]i[(][t][)]_ _−_ _[c][1]p∥µ∥_ _iX∈N_ _−wiℓ[′]i[(][t][)]#_ _,_

where c1 is the constant from Lemma B.2. Unrolling this inequality over t steps we have that

_t_

log(n/δ)

_µ1 · θ[(][t][+1)]_ = µ1 · θ[(0)] + _[η][∥][µ]2[∥][2]_ Xs=0 "Xi∈P _−wiℓ[′]i[(][s][)]_ _−_ _[c][1]p∥µ∥_ _iX∈N_ _−wiℓ[′]i[(][s][)]#_ _._


(10)

(11)


On the other hand, by the triangle inequality we know that

_∥θ[(][t][+1)]∥≤∥θ[(][t][)]∥_ + η∥∇L[b](θ[(][t][)])∥

= _θ[(][t][)]_ + η _zi(_ _wiℓ[′]i[(][t][)])_
_∥_ _∥_ _−_

_iX∈[n]_


_zi(_ _ℓ[′]i[(][t][)]_
_−_
_iX∈P_


_zi(_ _ℓ[′]i[(][t][)]_
_−_
_iX∈N_


_≤∥θ[(][t][)]∥_ + η



[+][ ηw]


Now note that

_zi(_ _ℓ[′]i[(][t][)]_
_−_
_iX∈P_


(ℓ[′]i[(][t][)])[2] _zi_ +
_∥_ _∥[2]_
_iX∈P_


_ℓ[′]i[(][t][)]ℓ[′]j[(][t][)](zi_ _zj)_
_i≠_ Xj∈P _·_


(i)
_c1_ ( _ℓ[′]i[(][t][)])[2]d +_
_≤_  _−_

_i∈P_

(ii) [X]
_c1_ max _j_ _d_
_≤_ _j_

 _∈P_ _[−][ℓ][′][(][t][)] ["]_


_ℓ[′]i[(][t][)]ℓ[′]j[(][t][)](_ _µ_ +
_∥_ _∥[2]_
_i≠_ Xj∈P p

_ℓ[′]i[(][t][)]_ + ( _µ_ +
_−_ _|P|_ _∥_ _∥[2]_
_iX∈P_


_d log(n/δ))_

_d log(n/δ))_ _ℓ[′]i[(][t][)]_

_−_

p _iX∈P_


(d + |P|∥µ∥[2] + |P|


max _j_
_j∈P_ _[−][ℓ][′][(][t][)]_


_ℓ[′]i[(][t][)]_
_−_
_iX∈P_


= c1



(iii)
_c2_
_≤_


_d log(n/δ))_


2
!


_ℓ[′]i[(][t][)]_
_−_

Xi∈P


_|P|_


where (i) follows by using Part (1) and Part (3) of Lemma B.2, (ii) follows since _ℓ[′]j[(][t][)]_ is positive
_−_
for all j ∈ [n], and (iii) follows since by assumption d ≥ _Cn∥µ∥[2]_ and ∥µ∥[2] _≥_ _Cn[2]_ log(n/δ) for a
large enough constant C. Hence we have that


_zi(_ _ℓ[′]i[(][t][)]_
_−_
_iX∈P_


_ℓ[′]i[(][t][)]_
_−_
_iX∈P_


_c2_


_|P|_


-----

Similarly, one can also show that

_zi(_ _ℓ[′]i[(][t][)]_
_−_
_iX∈N_


_ℓ[′]i[(][t][)]_
_−_
_iX∈N_


_c2_


_|N|_


Applying these bounds in inequality (11) we get that


_c2_

"s

_c2_

"s


_d_

_ℓ[′]i[(][t][)]_ + w
_−_

_|P|_ _iX∈P_

_d_

_ℓ[′]i[(][t][)]_ + w
_−_

_|N|_ _iX∈P_


_ℓ[′]i[(][t][)]_
_−_
_iX∈N_

_ℓ[′]i[(][t][)]_
_−_
_iX∈N_


_∥θ[(][t][+1)]∥≤∥θ[(][t][)]∥_ + η

_≤∥θ[(][t][)]∥_ + η


_c2_ _|N|_

_d_
_c2_ _|N|_


_wiℓ[′]i[(][t][)]_
_−_
_iX∈[n]_


= _θ[(][t][)]_ + c3η
_∥_ _∥_


_|N|_


Therefore, unrolling this bound over t steps we find that


_wiℓ[′]i[(][s][)]_ (12)
_−_
_iX∈[n]_


_θ[(][t][+1)]_ _θ[(0)]_ + c3η
_∥_ _∥≤∥_ _∥_


_|N|_


_s=0_


_t_

_d_ _θ[(0)]_

= c3ηs _|N|_ Xs=0 _iX∈[n]_ _−wiℓ[′]i[(][s][)]_ 1 + _c3η_ _|N |d_ _ts∥=0_ _∥i∈[n]_ _[−][w][i][ℓ][′]i[(][s][)]_

 q

Thus, combined with inequality (10) we get that, P P


_._






_µ1_ _θ[(][t][+1)]_ 1

_∥θ ·[(][t][+1)]∥_ _[≥]_ _∥[µ]θ[1][(][ ·][t][+1)][ θ][(0)]∥_ [+][ ∥][µ]2[∥]c[2]3[p]√|N|d  1 + _c3η_ _|N |d_ _ts∥=0θ[(0)]∥i∈[n]_ _[−][w][i][ℓ][′]i[(][s][)]_  _×_

 q P P 

_t_ _c1[√]log(n/δ)_
_s=0_ _i_ _i_ _µ_ _i_ _i_

_∈P_ _[−][w][i][ℓ][′][(][s][)]_ _−_ _∥_ _∥_ _∈N_ _[−][w][i][ℓ][′][(][s][)]_ _,_

P P _t_ P

_s=0_ _i∈[n]_ _[−][w][i][ℓ]i[′][(][s][)]_

which completes the proof of the first part of the lemma. The second part follows by an identicalP P
argument.


Next, we prove a lemma that shows that throughout training the ratio between the losses between
any two samples remains bounded.

**Lemma B.6. There is a positive absolute constant c ≥** 1 such that the following holds for all large
_enough C, and all small enough step-sizes η and for any_ _[τ]2[ 3]_

_t ∈{1, 2, . . .} and all i ̸= j ∈_ [n] _[≤]_ _[w][ ≤]_ [2][τ][ 3][. On a good run, for all]

1/3

_ℓ[(]i[t][)]_ _c_ _wj_ _._

_ℓ[(]j[t][)]_ _≤_  _wi_ 

maxProof Let8, 2(4 cc1[2]1 ≥[)][1][/][3]1 be the constantsuffices. _c from Lemma B.2 above. We shall show that the choice c =_

We shall prove this via an inductive argument. For the base case, at step _t = 0, we know that by_
Lemma B.4 that the ratio between the losses of sample i and j is upper bounded by 8 (wj/wi)[1][/][3].
Now, we shall assume that the inductive hypothesis holds at an arbitrary step t > 0 and prove that it
holds at step t + 1.

Without loss of generality, we shall analyze the ratio between the losses of the samples with indices
_i = 1 and j = 2. A similar analysis shall hold for any other pair. Define Gt := ℓ[(]1[t][)][,][ H][t][ :=][ ℓ][(]2[t][)][,]_


-----

_At := Ht/Gt. Note that,_

_Gt+1 =_

=

(i)


_θ[(][t][+1)]_ _z1_ (β 1)

_·_ _−_ _−_


=

_θ[(][t][)]_ _· z1 + η_ _k∈[n]_ _[w][k][(][−][ℓ]k[′][(][t][)][)(][z][k][ ·][ z][1][)][ −]_ [(][β][ −] [1)]

(i) 1
= [P] 2

_θ[(][t][)]_ _· z1 −_ (β − 1) + η _k∈[n]_ _[w][k][(][z][k][ ·][ z][1][)]_ _ℓ[(]k[t][)]_

 

_ℓ[(]1[t][)]_
= [P] 2

1 + ηℓ[(]1[t][)] _k_ [n] _[w][k][(][z][k][ ·][ z][1][)]_ _ℓ[(]k[t][)]_

_∈_

P 1  

= Gt 2
_·_

1 + ηℓ[(]1[t][)] _k_ [n] _[w][k][(][z][k][ ·][ z][1][)]_ _ℓ[(]k[t][)]_

_∈_
 

where (i) follows since _ℓ[′]k[(][t][)]_ = (ℓ[(]k[t][)][)][2][ as the loss of each example is always in the polynomial tail]P
_−_
of the loss by Lemma B.4. Therefore, we have that

2
1 + ηℓ[(]1[t][)] _k_ [n] _[w][k][(][z][k][ ·][ z][1][)]_ _ℓ[(]k[t][)]_
_∈_

_At+1 =_ _[H]Gt[t]+1[+1]_ = _[H]Gt[t]_ _·_ P  2

1 + ηℓ[(]2[t][)] _k_ [n] _[w][k][(][z][k][ ·][ z][2][)]_ _ℓ[(]k[t][)]_

_∈_
 2

P

1 + ηℓ[(]1[t][)] _k_ [n] _[w][k][(][z][k][ ·][ z][1][)]_ _ℓ[(]k[t][)]_

_∈_

= At · P  2 _[.]_

1 + ηℓ[(]2[t][)] _k_ [n] _[w][k][(][z][k][ ·][ z][2][)]_ _ℓ[(]k[t][)]_

_∈_

Now since the step-sizeand by the assumption on η is chosen to be small enough, d) and because the losses are all smaller than a constant by Lemma B.4,P _|zi · zj| ≤_ _c1d (by Part (_  **??) of Lemma B.2**
the following approximations hold


2
_wk(zk_ _z1)_ _ℓ[(]k[t][)]_ exp
_·_ _≤_
_k_ [n]

X∈  

2
_wk(zk_ _z2)_ _ℓ[(]k[t][)]_ exp
_·_ _≥_
_k_ [n]

X∈  


2
_wk(zk_ _z1)_ _ℓ[(]k[t][)]_
_·_ 
_k_ [n]

X∈  



2
_wk(zk_ _z2)_ _ℓ[(]k[t][)]_ _,_
_·_ 
_k_ [n]

X∈  




1 + ηℓ[(]1[t][)]

1 + ηℓ[(]2[t][)]


_ηℓ[(]1[t][)]_





2

 2

 _[ηℓ][(][t][)]_


and thus,


2

_At+1 ≤_ _At exp_ ηℓ[(]1[t][)] _wk(zk · z1)_ _ℓ[(]k[t][)]_ _−_ _[ηℓ]22[(][t][)]_

_k_ [n]

X∈  

Let us further upper bound the RHS as follows


2
_wk(zk_ _z2)_ _ℓ[(]k[t][)]_ _._ (13)
_·_ 
_k_ [n]

X∈  




2 2

_At exp_ ηℓ[(]1[t][)] _wk(zk · z1)_ _ℓ[(]k[t][)]_ _−_ _[ηℓ]22[(][t][)]_ _wk(zk · z2)_ _ℓ[(]k[t][)]_ 

_k_ [n] _k_ [n]

X∈   X∈  
 3 3 

= At exp _ηw1_ _ℓ[(]1[t][)]_ _∥z1∥[2]_ _−_ 2[1] _[ηw][2]_ _ℓ[(]2[t][)]_ _∥z2∥[2]_
     

2 2

_× exp_ ηℓ[(]1[t][)] _k=1_ _wk(zk · z1)_ _ℓ[(]k[t][)]_ _−_ 2[1] _[ηℓ][(]2[t][)]_ _k=2_ _wk(zk · z2)_ _ℓ[(]k[t][)]_

X̸   X̸  

(i)  3 3[]
_≤_ _At exp_ _c1ηdw1_ _ℓ[(]1[t][)]_ _−_ 2[1]c1 _ηdw2_ _ℓ[(]2[t][)]_
    

2

_× exp_ c2η _ℓ[(]1[t][)]_ + _[ℓ][(]22[t][)]_ _∥µ∥[2]_ + _d log(n/δ)_ _wk_ _ℓ[(]k[t][)]_ 

_k_ [n]

!  p  X∈  

 


-----

where (i) follows since for all i [n], d/c1 _zi_ _c1d and for any j_ = k, _zj_ _zk_
_∈_ _≤∥_ _∥[2]_ _≤_ _̸_ _|_ _·_ _| ≤_

_c1_ _µ_ + _d log(n/δ)_ by Lemma B.2. Continuing we get that,
_∥_ _∥[2]_
 p 

_At+t ≤_ _At exp_ _c1ηdw1G[3]t_ _[−]_ 2[1]c1 _ηdw2Ht[3]_
 


2

exp _c2η (Gt + Ht)_ _µ_ + _d log(n/δ)_ _wk_ _ℓ[(]k[t][)]_
_×_  _∥_ _∥[2]_ 

_k_ [n]

 p  X∈  
 

1[w][1]

= At exp _−_ 2[1]c1 _ηdw2G[3]t_ _A[3]t_ _[−]_ [2][c]w[2] 2
  

2

exp _c2η (Gt + Ht)_ _µ_ + _d log(n/δ)_ _wk_ _ℓ[(]k[t][)]_ _,_ (14)
_×_  _∥_ _∥[2]_ 

_k_ [n]

 p  X∈  
 

With this upper bound in place, consider two cases.


**Case 1 (A[3]t** _[≤]_ [4][c]w1[2][w]2 [1] **[):][ Using inequality (14) we know that]**



[1]



2

_d log(n/δ)_ _wk_ _ℓ[(]k[t][)]_

_k_ [n]

 X∈  


1[w][1]
_A[3]t_ _[−]_ [2][c]w[2] 2



_At+1 ≤_ _At exp_ _−_ 2[1]c1 _ηdw2G[3]t_



_c2η (Gt + Ht)_ _µ_ +

 _∥_ _∥[2]_





_× exp_


2

_d log(n/δ)_ _wk_ _ℓ[(]k[t][)]_

_k_ [n]

 X∈  


_c2η (Gt + Ht)_ _µ_ +

 _∥_ _∥[2]_





_._






_At exp_ _c1ηdw1G[3]t_ exp
_≤_
  


Now the loss on each example is less than the total initial loss 1/2 (see equation (9)) and therefore,


_At+1 ≤_ _At exp (c3ηdw) exp_ _c4η_ _∥µ∥[2]_ +

(i) 4c21[w][1] 1/3   4c21[w]p[1]

exp(1/8) 2

_≤_ _w2_ _≤_ _w2_
  


_d log(n/δ)_ _wn_

1/3 (ii) w1
_c_
_≤_ _w2_
 


1/3



1/3
in



where (i) follows by choosing the step-size η to be small enough and because At 4cw[2]1[w]2 [1]
_≤_

this case, and (ii) follows by the choice of constant c 2(4c[2]1[)][1][/][3][ from above.] 
_≥_

**Case 2 (** [4][c]w1[2][w]2 [1] _< A[3]t_ _[≤]_ _[c][3]w[w]2[1]_ **[):][ In this case again by inequality (14)]**


1[w][1]

_≤_ _At exp_ _−_ 2[1]c1 _ηdw2G[3]t_ _A[3]t_ _[−]_ [2][c]w[2] 2
  

2

exp _c2η (Gt + Ht)_ _µ_ + _d log(n/δ)_ _wk_ _ℓ[(]k[t][)]_
_×_  _∥_ _∥[2]_

_k_ [n]

 p  X∈  


1[w][1]

= At exp _−_ 2[1]c1 _ηdw2G[3]t_ _A[3]t_ _[−]_ [2][c]w[2] 2
  

2
_ℓ[(]k[t][)]_

_× exp_ c2ηG[3]t [(][A][t] [+ 1)] _∥µ∥[2]_ + _d log(n/δ)_ _wk_  _G[2]t_

_k_ [n]

 p  X∈




1[w][1]
_A[3]t_ _[−]_ [2][c]w[2] 2


_At+1 ≤_ _At exp_ _−_ 2[1]c1 _ηdw2G[3]t_



_._




-----

Continuing we find that,

(i)

1[w][1]

_At+1_ _≤_ _At exp_ _−_ 2[1]c1 _ηdw2G[3]t_ _A[3]t_ _[−]_ [2][c]w[2] 2

  

1/3 2/3

_w1_ _w1_

exp _c6ηG[3]t_ [max] _, 1_ _µ_ + _d log(n/δ)_ _wk_
_×_  _w2_ _∥_ _∥[2]_ _wk_ 

(  )  p  Xk∈[n]  

 

1[w][1]

= At exp _−_ 2[1]c1 _ηdw2G[3]t_ _A[3]t_ _[−]_ [2][c]w[2] 2
  

1/3

_wk_

_× exp_ c6ηw1G[3]t _∥µ∥[2]_ + _d log(n/δ)_ min _w1, w2_ 

 p  Xk∈[n]  _{_ _}_ 

(ii)  

1[w][1]

_≤_ _At exp_ _−_ 2[1]c1 _ηdw2G[3]t_ _A[3]t_ _[−]_ [2][c]w[2] 2
  

exp _c6ηw1G[3]t_ _µ_ + _d log(n/δ)_ + w[1][/][3]
_×_ _∥_ _∥[2]_ _|P|_ _|N|_
  p 1[w][1]   

= At exp _−_ 2[1]c1 _ηdw2G[3]t_ _A[3]t_ _[−]_ [2][c]w[2] 2
  

_× exp_ _c6ηw1G[3]t_ _∥µ∥[2]_ + _d log(n/δ)_ _|P|_ 1 + _[w]τ[1][/][3]_

(iii)   p   

1[w][1]

_≤_ _At exp_ _−_ 2[1]c1 _ηdw2G[3]t_ _A[3]t_ _[−]_ [2][c]w[2] 2 exp 2c6ηw1G[3]t _∥µ∥[2]_ + _d log(n/δ)_ _n_

(iv)      p  

1[w][1]

_≤_ _At exp_ _−_ 2[1]c1 _ηdw2G[3]t_ _[·][ 2][c]w[2]_ 2 exp 2c6ηw1G[3]t _∥µ∥[2]_ + _d log(n/δ)_ _n_
    p  

= At exp _c1w1G[3]t_ _d_ _c7n_ _µ_ + _d log(n/δ)_ _,_
_−_ _−_ _∥_ _∥[2]_
h   p i

_k_
where (i) follows by the inductive hypothesis which guarantees that _[ℓ]ℓ[(][(]1[t][t][)][)]_ _≤_ _c(_ _w[w]k[1]_ [)][1][/][3][ ≤] _[cw][1][/][3][,][ (][ii][)]_

follows since wk = 1 if k ∈P and wk = w if k ∈N . Inequality (iii) follows since w < 2τ [3] and
since |P| ≤ _n, and finally (iv) follows since in this case A[3]t_ _[>][ 4][c]w1[2][w]2_ [1] [. Now since by assumption the]

dimension

_d ≥_ _Cn∥µ∥[2]_ _≥_ _C_ [2]n[3] log(n/δ)

1/3

for a large enough constant C. Hence, we have that At+1 _At_ _c_ _ww12_ in this case.
_≤_ _≤_
 

Recall that since we assumed the inductive hypothesis to hold, the two cases analyzed above are
exhaustive. This completes our proof.


The next lemma uses the loss ratio bound that we established to show that the difference between
the gradient of the losses over the positive cluster and the negative cluster is small at any iteration.

**Lemma B.7. For any positive ˜c, there exists a 0 < c < 1 such that, for all large enough C, if the**
_step-size η is sufficiently small and_ _[τ]2[ 3]_

_[≤]_ _[w][ ≤]_ [2][τ][ 3][ then on a good run, for any][ t][ ∈{][0][,][ 1][, . . .][}]


_c_
_wiℓ[′]i[(][t][)]_
_−_ _−_ [˜]
_iX∈P_

_c_
_wiℓ[′]i[(][t][)]_
_−_ _−_ [˜]
_iX∈N_


log(n/δ)

_∥µ∥_

log(n/δ)

_∥µ∥_


_wiℓ[′]i[(][t][)]_ _c_ _wiℓ[′]i[(][t][)]_
_−_ _≥_ _−_
_iX∈N_ _iX∈[n]_

_wiℓ[′]i[(][t][)]_ _c_ _wiℓ[′]i[(][t][)]_
_−_ _≥_ _−_
_iX∈P_ _iX∈[n]_


_and_


-----

**Proof We begin by proving the first part of the lemma. Note that since [n] = P ∪N to prove the**
first part it suffices to instead show that


_wiℓ[′]i[(][t][)]_
_−_

! Xi∈N


_c_
_c + [˜]_


log(n/δ)

_∥µ∥_


_wiℓ[′]i[(][t][)]_
_−_
_iX∈P_


(1 − _c)_


_c_
_c + [˜]_

_c_
_c + [˜]_


log(n/δ)

_∥µ∥_

log(n/δ)


(1 _c)_ _ℓ[′]i[(][t][)]_
_⇒_ _−_ _−_ _≥_

_iX∈P_

(1 _c)_ (ℓ[(]i[t][)][)][2][ ≥]
_⇒_ _−_

_iX∈P_


_ℓ[′]i[(][t][)]_
_−_
_iX∈N_


log(n/δ)

_w_ (ℓ[(]i[t][)][)][2]

_µ_

p _∥_ _∥_ ! _iX∈N_

(since _ℓ[′]i[(][t][)]_ = (ℓ[(]i[t][)][)][2][ for the polynomial loss)]
_−_


_c_
_c + [˜]_


log(n/δ)

p _∥µ∥_ ! _w|N| maxi∈N_ [(][ℓ][(]i[t][)][)][2]

_i_ [)][2]
_w_ [max][i][∈N][ (][ℓ][(][t][)]

mini∈P (ℓ[(]i[t][)][)][2]


_⇐_ (1 − _c)|P| mini_ [(][ℓ]i[(][t][)][)][2][ ≥]
_∈P_


_c_
_c + [˜]_

_c_
_c + [˜]_


log(n/δ)

_∥µ∥_

log(n/δ)

_∥µ∥_


_⇐_ (1 − _c)τ ≥_

_⇐_ (1 − _c)τ ≥_

(1 − _c)_
_⇐_ _c˜_

_c1_ _c +_ _√_



_wc1_
_·_


_w[2][/][3]_


(by invoking Lemma B.6; note that c1 1)
_≥_


 _τ ≥_ _w[1][/][3]_

(since ∥µ∥≥


_Cn[2]_ log(n/δ), where C is a large enough constant).


Since C is large enough, we now choose the constant 0 < c < 1 to be such that (1 _c)/(c1(c +_
_−_
_c/˜_ _√C)) is at least (2)[1][/][3]. This proves the first part of the lemma._

To prove the second part of the lemma, note that again since [n] = P ∪N it suffices to show that

_c_ log(n/δ)

(1 − _c)_ _−wiℓ[′]i[(][t][)]_ _≥_ _c + [˜]_ _µ_ _−wiℓ[′]i[(][t][)]_

_iX∈N_ p _∥_ _∥_ ! Xi∈P


_c_ log(n/δ)

_⇐_ (1 − _c)|N|w mini_ _i_ [)][2][ ≥] _c + [˜]_ _µ_
_∈N_ [(][ℓ][(][t][)] p _∥_ _∥_

_c˜[√]log(n/δ)_

_w_ c + _∥µ∥_  maxi∈P (ℓ[(]i[t][)][)][2]
_⇐_ _≥_ (1 − _c)_ mini∈N (ℓ[(]i[t][)][)][2][ ·][ τ]

_c˜[√]log(n/δ)_
_c +_ _∥µ∥_

_w_   _c1w[2][/][3]τ_
_⇐_ _≥_ (1 _c)_

_−_

_c1_ _c +_ _√c˜C_
_w[1][/][3]_ _τ_
_⇐_ _≥_ (1 _c)_  _·_

_−_

_τ_
_w[1][/][3]_
_⇐_ _≥_ 2[1][/][3]


_|P| maxi_ [(][ℓ][(]i[t][)][)][2]
_∈P_


where the last implication follows by the choice of c from above. This completes the proof.

We now have all the pieces required to prove our main theorem. Recall its statement.

**Theorem 4.2. For any 0 <** _C ≤_ 1, there is a constant c such that, for all large enough C and
_for any 0 < δ < 1/C, under the assumptions of this subsection the following holds. If the weight_

[e]


-----

_τ2[3]_ _θ1 satisfying:_

_[≤]_ _[w][ ≤]_ [2][τ][ 3][,][ then with probability at least][ 1] _[−]_ _[δ][, training on][ S][ produces a classifier][ b]_

_µ_

TestError[θ[b]1] = P(x,y) Ptest sign _θ1_ _x_ = y exp _∥_ _∥[4]_ _._
_∼_ _·_ _̸_ _≤_ _−_ _[cn]τ_ _d_
h   i  
b

**Proof First, by Part (8) of Lemma B.2 we know that the data is linearly separable. Thus, by**
Proposition 4.1 we know that

_θ[(][t][)]_
_θ1 = lim_

Given this equivalence, by Lemma B.1 we know thatb _t→∞_ _∥θ[(][t][)]∥_ _[.]_


P(x,y) Ptest sign _θ1_ _x_ = y
_∼_ _·_ _̸_
h   i

_θ[(][t][)]_

= P(x,y) bPtest sign lim = y
_∼_ _t_ _θ[(][t][)][∥]_ _[·][ x]_ _̸_

  _→∞_ _∥_  

2[!]

_θ[(][t][)]_ _µ1_

exp _c[′]_ lim _·_ + exp

_≤_ 2[1] " _−_ t→∞ _∥θ[(][t][)]∥_ 


2[!#]

_θ[(][t][)]_ _µ2_
_c[′]_ lim _·_
_−_ _t_ _θ[(][t][)]_
 _→∞_ _∥_ _∥_ 


(15)


We shall now establish lower bounds on limt→∞ _θ∥[(]θ[t][)][(][t]·[)]µ∥1_ [and][ lim][t][→∞] _θ∥[(]θ[t][)][(][t]·[)]µ∥2_ [to obtain the desired]

bound on the test error. Let us lower bound limt→∞ _θ∥[(]θ[t][)][(][t]·[)]µ∥1_ [. The bound on][ lim][t][→∞] _θ∥[(]θ[t][)][(][t]·[)]µ∥2_ [shall]

follow by exactly the same logic.

By Lemma B.5 we know that


_µ1_ _θ[(][t][+1)]_
lim _·_
_t→∞_ _∥θ[(][t][+1)]∥_

_µ1_ _θ[(0)]_ _µ_
lim _·_ _∥_ _∥[2][p]|N|_
_≥_ _t→∞_ _∥θ[(][t][+1)]∥_ [+ lim]t→∞ 2c1√d


_×_




2c1√d  1 + _c1η_ _|N |d_ _ts∥=0θ[(0)]∥i∈[n]_ _[−][w][i][ℓ]i[′][(][s][)]_ 

_t_  q P P 

log(n/δ)

_−wiℓ[′]i[(][s][)]_ _−_ _[c][2]_ _µ_ _−wiℓ[′]i[(][s][)]_

Xs=0 "Xi∈P p∥ _∥_ _iX∈N_

1

_θ[(0)]_ 
1 + _c1η_ _|N |d_ _ts∥=0_ _∥i∈[n]_ _[−][w][i][ℓ]i[′][(][s][)]_  _×_

_t_ q P P 

log(n/δ)

_−wiℓ[′]i[(][s][)]_ _−_ _[c][2]_ _µ_ _−wiℓ[′]i[(][s][)]_

Xs=0 "Xi∈P p∥ _∥_ _iX∈N_


(i) _|N|_
_≥_ _[∥][µ]2[∥]c[2]1[p]√d_


lim

_·_ _t_
_→∞_

lim

_·_ _t_
_→∞_


= _|N|_

_[∥][µ]2[∥]c[2]1[p]√d_


_×_



log(n/δ)

_µ_

p∥ _∥_


2c1√d _·_ _t→∞_  1 + _c1η_ _|N |d_ _ts∥=0θ[(0)]∥i∈[n]_ _[−][w][i][ℓ][′]i[(][s][)]_  _×_

 q _t_ P P 

log(n/δ)

lim _wiℓ[′]i[(][s][)]_ _wiℓ[′]i[(][s][)]_ _,_
_t→∞_ Xs=0 "Xi∈P _−_ _−_ _[c][2]p∥µ∥_ _iX∈N_ _−_ #

where (i) follows since µ1 _θ[(0)]_ is bounded and _θ[(][t][+1)]_ by (Ji et al., 2020, Lemma 2).
_·_ _∥_ _∥→∞_

We will now show that the first limit in RHS equals 1. To do this, first note that


(16)

_._







1 lim
_≥_ _t_
_→∞_


=




1 + _c1η_ _|N |d_ _ts∥=0θ[(0)]∥i∈[n]_ _[−][w][i][ℓ][′]i[(][s][)]_

q P P


1 + limt→∞ _c1η_ _|N |d_ _ts∥=0θ[(0)]∥i∈[n]_ _[−][w][i][ℓ]i[′][(][s][)]_

q P P


-----

Now we will show that

_θ[(0)]_
lim _∥_ _∥_ = 0.
_t→∞_ _c1η_ _|N |d_ _ts=0_ _i∈[n]_ _[−][w][i][ℓ]i[′][(][s][)]_

q

First note that _c1η√d_ _s=0∥θ[(0)]i∈∥[n]_ _[−][w][i][ℓ][′]i[(][s][)]_ _≥P0, sinceP −wiℓ[′]i[(][s][)]_ _≥_ 0, so to show that this limit equals

0, it suffices to show that[P][t] _cP1η√d_ _s=0_ _i_ [n] _i_ grows unboundedly as t . Using

inequality (12) from above we get that, _∈_ _[−][w][i][ℓ][′][(][s][)]_ _→∞_

[P][t] P _t_


_wiℓ[′]i[(][s][)]_
_−_
_iX∈[n]_


_θ[(][t][+1)]_ _θ[(0)]_ + c3η
_∥_ _∥≤∥_ _∥_


_|N|_


_s=0_


The norm ∥θ[(0)]∥ is finite and we know that ∥θ[(][t][)]∥→∞ by (Ji et al., 2020, Lemma 2), therefore
_c1η√d_ _s=0_ _i∈[n]_ _[−][w][i][ℓ]i[′][(][s][)]_ must grow unboundedly. This proves that

[P][t] P

1

lim = 1
_t→∞_  1 + _∥θ[(0)]∥_ 

 _c1η√d_ _s=0_ _i∈[n]_ _[−][w][i][ℓ]i[′][(][s][)]_ 
 P 

as claimed. This combined with inequality (16) yields the bound[P][t]

_t_ _c2[√]log(n/δ)_

_tlim→∞_ _µ∥1θ ·[(] θ[t][+1)][(][t][+1)]∥_ _[≥∥][µ]2[∥]c[2]1[p]√|N|d_ _tlim→∞_ Ps=0 Pi∈P _[−][w][i]ts[ℓ]=0i[′][(][s][)]_ _−i∈[n]_ _[−]∥µ[w]∥_ _[i][ℓ]i[′][(][s][)]Pi∈N_ _[−][w][i][ℓ][′]i[(][s][)]_

_t_

(≥i) _[c][3][∥][µ][∥]√[2]d[p]|N|_ _tlim→∞_ Psts=0=0 Pii∈∈[[nn]] P[−][−][w][w][i][i][ℓ][ℓ]i[′]i[′][(][(]P[s][s][)][)]

P P

= _[c][3][∥][µ][∥][2][p]|N|_

_√d_

= _[c][3][∥][µ][∥][2][p]|P|_

_√τd_

(ii)
_≥_ _[c][4][∥]√[µ][∥]τd[2][√][n]_


where (i) follows by invoking Lemma B.7 and (ii) follows sinceµ1 _θ[(][t][+1)]_ _|P| ≥c4_ _µ_ _n/n2. As stated above, using_
a similar argument we can also show that limt→∞ _∥θ·[(][t][+1)]∥_ _[≥]_ _∥√∥τd[2][√]_ . Plugging these lower

bounds into inequality (15) completes our proof.

C PROOF OF THEOREM 4.3

In this section we will prove a lower bound on the test error of the maximum margin linear classifier
_θMM. Here we will work with the exponential loss_

_ℓ(z) = exp(_ _z)._

b _−_

Define the loss _L(θ) =_ _i_ [n] _[ℓ][(][y][i][x]i[⊤][θ][) =][ P]i_ [n] _[ℓ][(][z]i[⊤][θ][)][. For a step-size][ η][ and initial iterate]_

_∈_ _∈_

_θ[(0)]_ = 0 (this choice of initial point is for the sake of convenience, it does not affect the implicit
bias of gradient descent) for any[b] [P] _t_ 0, 1, . . .
_∈{_ _}_

_θ[(][t][+1)]_ = θ[(][t][)] _−_ _η∇L[b](θ[(][t][)])_


-----

be the iterates of gradient descent. We let ℓ[(]i[t][)] be shorthand for ℓ(zi _θ[(][t][)]) and therefore,_
_·_

_θ[(][t][+1)]_ = θ[(][t][)] _−_ _η∇L[b](θ[(][t][)]) = θ[(][t][)]_ + η _ziℓ[(]i[t][)][.]_

_iX∈[n]_

The results by Soudry et al. (2018) guarantee that for a small enough step-size η the direction of the
_θ[(][t][)]_
iterates of gradient descent limt→∞ _∥θ[(][t][)]∥_ [=][ b]θMM. Therefore, we will instead prove a lower bound

on the asymptotic iterates of gradient descent.

As we did in the proof of Theorem 4.2, going forward we will assume that a good run occurs (see
Definition B.3), which guarantees that all of the conditions specified in Lemma B.2 are satisfied by
the training dataset S.

With the setup in place, we shall now prove this theorem in stages. Throughout this section the
assumptions stated in Section 4.2.1 shall remain in force. We begin with a lemma that shows that
the margin of the maximum margin classifier scales with _d/n._

**Lemma C.1. There is an absolute constant c such that, on a good run, for all large enough C, for**

p

_all i ∈_ [n]


_θMM_ _zi_ _c_
b _·_ _≥_


_n_ _[.]_


**Proof We will prove this result by constructing a unit vector ϕ with a margin that scales with** _d/n._

This immediately implies that the maximum margin classifier must also attain this margin on all of

p

the points.

Define ϕ to be as follows


_ϕ :=_ _i∈[n]_ _[z][i]_

_∥[P]Pj∈[n]_ _[z][j][∥]_ _[.]_

We will first bound the norm of the denominator as follows


= _zj_ + _zj_ _zk_

_jX∈[n]∥_ _∥[2]_ Xj≠ _k_ _·_

_zj_ + _zj_ _zk_

_≤_ _jX∈[n]∥_ _∥[2]_ Xj≠ _k_ _|_ _·_ _|_

(i)
_c1nd + c1n[2][ ]_ _µ_ +
_≤_ _∥_ _∥[2]_
p

= c1nd 1 + _[c][1][∥][µ][∥][2]_ + _[n]_


_zj_
_jX∈[n]_


_d log(n/δ)_

log(n/δ)


(ii)
2c1nd (17)
_≤_

where (i) follows by Lemma B.2 and (ii) follows since d ≥ _Cn∥µ∥[2]_ _≥_ _C_ [2]n[3] log(n/δ) where
recall C is sufficiently large.

Now we lower bound the margin between numerator of v and zk for any k ∈ [n]


_zi_ _zk =_ _zi_ +

  _·_ _∥_ _∥[2]_

_i∈[n]_

 [X] 


_zi_ _zk_ _zi_

Xi≠ _k_ _·_ _≥∥_ _∥[2]_ _−_


_zi_ _zk_

Xk≠ _i_ _|_ _·_ _|_


(i)

_c1n(_ _µ_ +

_≥_ _c[d]1_ _−_ _∥_ _∥[2]_


_d log(n/δ))_


1[n][∥][µ][∥][2]
1
_−_ _[c][2]_ _d_


1[n]
_−_ _[c][2]_


= _[d]_

_c1_


log(n/δ)


_d_

_,_ (18)
2c1


-----

where (i) again follows by invoking Lemma B.2 and by the assumption on d,

Combining inequalities (17) and (18) yields that for any k ∈ [n]


2c1√2c1nd = c


_ϕ_ _zk_
_·_ _≥_


_n_ _[.]_


This proves the result.

The next lemma provides control over the rate at which the norm of iterates θ[(][t][)] grows late in
training.
**Lemma C.2. There is an absolute constant c such that, for all large enough C, if the step-size η is**
_sufficiently small then on a good run, there exists a t0 such that for all t ≥_ _t0_


_ℓ[(]i[t][)][.]_
_iX∈[n]_


_∥θ[(][t][+1)]∥≥∥θ[(][t][)]∥_ + cη

**Proof By the definition of gradient descent**


_∥θ[(][t][+1)]∥[2]_ = ∥θ[(][t][)] + η _ℓ[(]i[t][)][z][i][∥][2]_

_iX∈[n]_

= ∥θ[(][t][)]∥[2] + 2η _ℓ[(]i[t][)][z][i][ ·][ θ][(][t][)][ +][ η][2][∥]_ _ℓ[(]i[t][)][z][i][∥][2]_

_kX∈[n]_ _iX∈[n]_

_≥∥θ[(][t][)]∥[2]_ + 2η _ℓ[(]i[t][)][z][i][ ·][ θ][(][t][)][.]_ (19)

_iX∈[n]_


_θ[(][t][)]_
Now we know that _∥θ[(][t][)]∥_ _[→]_ _θ[b]MM. Also note that in the previous lemma (Lemma C.1) we showed_

that for all i ∈ [n]


_θMM_ _zi_ _c1_
b _·_ _≥_


_n_ _[.]_


Therefore, there exists a iteration t1 such that for all t ≥ _t1 and all i ∈_ [n]

_θ[(][t][)]_ _zi_ _d_

_·_

_θ[(][t][)]_ 2 _n_ _[.]_
_∥_ _∥_ _[≥]_ _[c][1]_ r

Continuing from (19), for any t _t1_
_≥_

_∥θ[(][t][+1)]∥[2]_ _≥∥θ[(][t][)]∥[2]_ + 2η _ℓ[(]i[t][)][z][i][ ·][ θ][(][t][)]_

_iX∈[n]_


_θ[(][t][)]_ + c1η _θ[(][t][)]_
_≥∥_ _∥[2]_ _∥_ _∥_

_c1η_

= _θ[(][t][)]_ 1 +
_∥_ _∥[2]_  q

Taking square roots we get that for any t _t1_
_≥_


_d_
_n_ _ℓ[(]i[t][)]_

r _iX∈[n]_

_d_
_n_ _i_ [n] _[ℓ][(]i[t][)]_

_∈_

_θP[(][t][)]_
_∥_ _∥_


_._






_c1η_ _nd_ _i_ [n] _[ℓ]i[(][t][)]_

_θ[(][t][+1)]_ _θ[(][t][)]_ v1 + _∈_ _._ (20)
_∥_ _∥≥∥_ _∥u_ q _θP[(][t][)]_

u _∥_ _∥_
t

_tFurther by (Ji et al., 2020, Lemma 2) we know thatt2,_ _θ[(][t][)]_ 8√nd . Thus, for t _t2, we have ∥θ[(][t][)]∥→∞, so there exists a t2 such that for all_
_≥_ _∥_ _∥≥_ _[c][1][η]_ _≥_

1 _d_ _i_ [n] _[ℓ][(]i[t][)]_

_θ[(][t][)]_ _n_ _ℓ[(]i[t][)]_ _≤_ 8 · _∈n_ _≤_ 8, (21)
_∥_ _∥_ _[·][ c][1][η]r_ _iX∈[n]_ P


-----

where the last inequality follows since the initial loss is equal to n, as θ[(0)] = 0, and the total loss is
decreasing again by (Ji et al., 2020, Lemma 2) if the step-size is small enough. It is easy to check
that for any 0 ≤ _x ≤_ 8
_√1 + x_ 1 + _[x]_

_≥_ 4 _[.]_

Thus, combining inequalities (20) and (21) we get that for all t ≥ _t0 = max{t1, t2}_


_nd_ _i_ [n] _[ℓ][(]i[t][)]_ _c1η_

_∈_ _θ[(][t][)]_ 1 +

_θP[(][t][)]_ _≥∥_ _∥_ 
_∥_ _∥_



= _θ[(][t][)]_ + _[c][1]_
_∥_ _∥_ 4 _[η]_

r


_d_
_n_ _i_ [n] _[ℓ][(]i[t][)]_

_∈_

4 Pθ[(][t][)]
_∥_ _∥_

_ℓ[(]i[t][)][,]_
_iX∈[n]_


_c1η_

_θ[(][t][+1)]_ _θ[(][t][)]_ v1 +
_∥_ _∥≥∥_ _∥u_

u
t

which completes our proof.


Continuing we will show that throughout training the ratio of the losses between the different examples are bounded by a constant. This ensures that each example roughly “influences” the gradient
update by the same amount in each step. However, since the number of points from the positive
cluster is larger, the gradient update shall overall be more highly correlated with the mean of the
majority positive center µ1 than the mean of the minority negative center µ2.

The proof is identical to the proof of Lemma 11 by Chatterji & Long (2021). However, since our
setting is slightly different to the setting studied in that paper we reprove the result here.
**Lemma C.3. There is an absolute constant c such that, for all large enough C, and all small enough**
_step sizes η, on a good run, for all iterations t ∈{0, 1, . . .} and all i, j ∈_ [n]

_ℓℓ[(][(]ij[t][t][)][)]_ _≤_ _c._

**Proof First note that** _L(θ[(0)]) =_ _i_ [n] _[ℓ]i[(0)]_ = n and since step-size η is small enough training loss

_∈_
is non-increasing by (Ji et al., 2020, Lemma 2).

Let c1 be the constant c[b] ≥ 1 from Lemma B.2. We will show that[P] _c = 4c[2]1_ [suffices.]

We shall prove this via an inductive argument. For the base case, at step t = 0, we know that
_θ[(0)]_ = 0, therefore the loss on all of the samples is equal to 1. Now, we shall assume that the
inductive hypothesis holds at an arbitrary step t > 0 and prove that it holds at step t + 1.

Without loss of generality, we shall analyze the ratio between the losses of the samples with indices
_i = 1 and j = 2. A similar analysis shall hold for any other pair. Define Gt := ℓ[(]1[t][)][,][ H][t][ :=][ ℓ][(]2[t][)][,]_
_At := Ht/Gt. The ratio between these losses at step t + 1 is_

_At+1 = [exp(][−][θ][(][t][+1)][ ·][ z][2][)]_

exp( _θ[(][t][+1)]_ _z1)_
_−_ _·_

= exp(−(θ[(][t][)] + η _k∈[n]_ _[ℓ][(]k[t][)][z][k][)][ ·][ z][2][)]_

exp(−(θ[(][t][)] + η [P]k∈[n] _[ℓ][(]k[t][)][z][k][)][ ·][ z][1][)]_

= At exp _η_ [P] _ℓ[(]k[t][)]_ [(][z][k][ ·][ z][2][ −] _[z][k][ ·][ z][1][)]_
_·_ − 

_kX∈[n]_

 

= At exp _η_ _ℓ[(]2[t][)]_ 1 _ℓ[(]k[t][)][z][k][ ·][ z][2][ +]_ _ℓ[(]k[t][)][z][k][ ·][ z][1]_
_·_ −  _[∥][z][2][∥][2][ −]_ _[ℓ][(][t][)][∥][z][1][∥][2][ −]_ _kX=2̸_ _kX=1̸_ 

  

= At exp _η_ _Ht_ _z2_ _Gt_ _z1_ _ℓ[(]k[t][)][z][k][ ·][ z][2][ +]_ _ℓ[(]k[t][)][z][k][ ·][ z][1]_
_·_ −  _∥_ _∥[2]_ _−_ _∥_ _∥[2]_ _−_ _kX=2̸_ _kX=1̸_ 

  

_At_ exp _η_ _Ht_ _z2_ _Gt_ _z1_ _ℓ[(]k[t][)]_ _ℓ[(]k[t][)]_ _._
_≤_ _·_ −  _∥_ _∥[2]_ _−_ _∥_ _∥[2]_ _−_ _kX=2̸_ _[|][z][k][ ·][ z][2][| −]_ _kX=1̸_ _[|][z][k][ ·][ z][1][|]_

  


-----

Now note that by Lemma B.2, for all i = j [n], d/c1 _zi_ _c1d and_ _zi_ _zj_
_̸_ _∈_ _≤∥_ _∥[2]_ _≤_ _|_ _·_ _| ≤_

_c1_ _µ_ + _d log(n/δ)_, and therefore,
_∥_ _∥[2]_
 p 


_At+1 ≤_ _At · exp_ −η  _c1_ _−_ _Gtc1d −_ 2c1 _∥µ∥[2]_ + _d log(n/δ)_ _ℓ[(]k[t][)]_

_k_ [n]

 p  X∈

  _[H][t][d]_ 

= At · exp −η  _c1_ _At −_ _c[2]1_ _−_ 2c1 _∥µ∥[2]_ + _d log(n/δ)_ _Gt_ _ℓG[(]k[t]t[)]_ 

_k_ [n]

    p  X∈

(i)  G[G]t[t]d[d] 
_≤_ _At · exp_ _−η_ _c1_ _At −_ _c[2]1_ _−_ 8c[3]1 _∥µ∥[2]_ + _d log(n/δ)_ _Gtn_
      p  

_µ_ _n_ log(n/δ)

= At exp _At_ _c[2]1_ 1 _∥_ _∥[2]_ + _[n]_
_·_ _−_ _[ηG]c1[t][d]_ _−_ _[−]_ [8][c][4] _d_ p _√d_ !!!

(ii)

1

_≤_ _At · exp_ _−_ _[ηG]c1[t][d]_ _At −_ _c[2]1_ _[−]_ [8][c]1[4] _C_ [+ 1]C
   

(iii)
_≤_ _At · exp_ _−_ _[ηG]c1[t][d]_ _At −_ 2c[2]1 _,_ (22)


  []


where (i) follows since by the inductive hypothesis ℓ[(]k[t][)][/G][t][ ≤] _[c][ = 4][c]1[2][,][ (][ii][)][ follows since by]_
assumption d ≥ _Cn∥µ∥[2]_ _≥_ _C_ [2]n[3] log(n/δ), and (iii) follows since C is sufficiently large.

Now consider two cases.

**Case 1 (At ≤** 2c[2]1[):][ By inequality (22)]


_ηGtd_
_At −_ 2c[2]1 = At exp _c1_

[]


2c[2]1 _[−]_ _[A][t]_ _≤_ 2c[2]1 [exp (2][ηc][1][G][t][d][)]
[]

_≤_ 2c[2]1 [exp (2][ηc][1][nd][)]

_≤_ 4c[2]1[,]


_At+1_ _At_ exp
_≤_ _·_ _−_ _[ηG]c1[t][d]_



where the last inequality follows if the step-size η is sufficiently small.

**Case 2 (2c[2]1** _[≤]_ _[A][t]_ _[≤]_ [4][c]1[2] [=][ c][):][ Again by inequality (22)]


_At −_ 2c[2]1 _≤_ _At ≤_ 4c[2]1[.]
[]


_At+1_ _At_ exp
_≤_ _·_ _−_ _[ηG]c1[t][d]_



Thus, we have shown thatto hold at step t, these two cases are exhaustive, and hence the induction is complete. At+1 ≤ 4c[2]1 [=][ c][ in both cases. Since we assumed the induction hypothesis]

The next lemma proves an upper bound on the difference between the inner product between θ[(][t][+1)] _·_
_µ2 and the corresponding inner product at iteration t. Since we start at θ[(0)], unrolling this over t_
steps gives us an upper bound on inner product θ[(][t][)] _· µ2 for any t ≥_ 0.

**Lemma C.4. There is an absolute constant c such that, for all large enough C, if the step-size η is**
_sufficiently small then on a good run, for all t ∈{0, 1, . . .}_


_θ[(][t][+1)]_ _θ[(][t][)][]_ ( _µ2)_
_−_ _·_ _−_ _≤_ _[cη][∥]τ[µ][∥][2]_


_ℓ[(]i[t][)][.]_
_iX∈[n]_


-----

**Proof By the definition of a gradient descent step**


(θ[(][t][+1)] _−_ _θ[(][t][)]) · (−µ2) = η_ _ℓ[(]i[t][)][z][i][ ·][ (][−][µ][2][)]_

_iX∈[n]_

(i)
_≤_ [3][η][∥]2[µ][∥][2] _ℓ[(]i[t][)]_ + c1η∥µ∥ log(n/δ) _ℓ[(]i[t][)]_

_iX∈N_ p _iX∈P_

(≤ii) [3][η][∥]2[µ][∥][2] _·_ _[c][2][|N|]n_ _ℓ[(]i[t][)]_ + c1η∥µ∥ log(n/δ) · _[c][2]n[|P|]_

_iX∈[n]_ p

= [3][c][2][η]2[∥][µ][∥][2] _· [|P|]n_ _|N|_ [+] log(µn/δ) _ℓ[(]i[t][)]_

_|P|_ p _∥_ _∥_ ! Xi∈[n]

log(n/δ)

_≤_ 3c2η∥µ∥[2] max ( _|N||P|_ _[,]_ p _∥µ∥_ ) Xi∈[n] _ℓ[(]i[t][)][,]_


_ℓ[(]i[t][)]_
_iX∈[n]_


where (i) follows by Lemma B.2 and (ii) follows by the loss ratio bound in Lemma C.3. Since by
assumption ∥µ∥[2] _≥_ _Cn[2]_ log(n/δ), we infer that


log(n/δ)

_µ_ _≤_ _n[1]_ [= 1]τ [.]
_∥_ _∥_ _[≤|N|]|P|_


Thus,


_ℓ[(]i[t][)]_ = _[cη][∥]τ[µ][∥][2]_

) Xi∈[n]


_|N|_

_[,]_
_|P|_


log(n/δ)

_∥µ∥_


(θ[(][t][+1)] _θ[(][t][)])_ ( _µ2)_ 3c2η _µ_ max
_−_ _·_ _−_ _≤_ _∥_ _∥[2]_

wrapping up our proof.


_ℓ[(]i[t][)][,]_
_iX∈[n]_


With these lemmas in place we are now ready to prove our result. Let us restate it here.
**Theorem 4.3. Let q** N(0, Id _d). There exist constants c and c[′]_ _such that, for all large enough_
_∼_ _×_
_C and for any 0 < δ < 1/C, under the assumptions of this subsection the following holds. With_
_probability at least 1 −_ _δ, training on S produces a maximum margin classifier_ _θMM satisfying:_

TestError[θ[b]MM] = P(x,y)∼Ptest sign _θMM · x_ ≠ _y_ _≥_ 2[1] _−_ _[c][√]τ_ _[n][b]· [∥]√[µ][∥]d[2]_ _,_
h   i _[·][ Φ]_  2 

_where Φ is the Gaussian cdf. Furthermore, if the imbalance ratiob_ _τ_ _c[′]_ _√n√∥dµ∥_ _then with probabil-_
_≥_

_ity at least 1 −_ _δ, TestError[θ[b]MM] ≥_ [1]8 _[.]_

**Proof The test error for** _θMM is_

TestError[θ[b]MM]

[b]

= P(x,y) Ptest sign _θMM_ _x_ = y
_∼_ _·_ _̸_
h   i

= 2[1] [P][q][∼][N][(0][,][ 1]2 _[I][p][×][p][)]_ signb _θMM · (µ1 + q)_ = 1̸ + 2[1] [P][q][∼][N][(0][,][ 1]2 _[I][p][×][p][)]_ sign _θMM · (µ2 + q)_ ≠ _−1_

h   i h  

_≥_ [1]2 [P][q][∼][N][(0][,][ 1]2 _[I][p][×][p][)]_ sign _θbMM · (µ2 + q)_ ≠ _−1_ b

h   i

= 2[1] [P][q][∼][N][(0][,][ 1]2 _[I][p][×][p][)]_ _−θ[b]MM ·b µ2 −_ _θ[b]MM · q < 0_

h i

= [1]2 [P][q][∼][N][(0][,][ 1]2 _[I][p][×][p][)]_ _−θ[b]MM · q < −θ[b]MM · (−µ2)_

h i

= [1] _ξ <_ _θMM_ ( _µ2)_ (since _θMM_ _q_ N(0, 1))

2 [P][ξ][∼][N][(0][,][1)] _−[b]_ _·_ _−_ _−[b]_ _·_ _∼_
h i

_θMM_ ( _µ2))_
= [Φ(][−][b] _·_ _−_ _,_ (23)

2


-----

where Φ is the Gaussian cumulative distribution function.

2
With this inequality in place, we want to prove an upper bound on _θMM_ ( _µ2)_ . Now, since
_·2_ _−_

_∥θθ[(][(][t][t][)][)]∥_ _[→]_ _θ[b]MM, it suffices to prove an upper bound on_ limt→∞ _θ[(][t]∥[)]θ·([(]−[t][)]µ∥b2)_ instead.
 

To do this, going forward let us assume that a good run (see Definition B.3) occurs. Lemma B.2
guarantees that this happens with probability at least 1 − _δ._

Let t0 be the constant from Lemma C.2, then by Lemma C.4 we have that for any t > t0


_θ[(][s][)]_ _θ[(][s][−][1)][]_ ( _µ2)_
_−_ _·_ _−_


_t0_

(θ[(][s][)] _θ[(][s][−][1)])_ ( _µ2) +_
_−_ _·_ _−_
_s=1_

X


_θ[(][t][)]_ ( _µ2) = θ[(0)]_ ( _µ2) +_

_·_ _−_ _·_ _−_


_s=t0+1_


_t−1_

_s=t0_

X


_ψ +_ _[c][1][η][∥][µ][∥][2]_
_≤_ _τ_


_ψ +_ _[c][1][η][∥][µ][∥]_ _ℓ[(]i[s][)][,]_ (24)
_≤_ _τ_

_sX=t0_ _iX∈[n]_

where we define ψ := θ[(0)] ( _µ2) +_ _s=1_ _θ[(][s][)]_ _θ[(][s][−][1)][]_ ( _µ2)._

_·_ _−_ _−_ _·_ _−_

On the other hand, by repeatedly applying Lemma C.2 we get that[P][t][0]  

_d_ _t−1_

_θ[(][t][)]_ _θ[(][t][0][)]_ + c2η _ℓ[(]i[s][)][.]_ (25)
_∥_ _∥≥∥_ _∥_ _n_

r _sX=t0_ _iX∈[n]_


Furthermore, since ∥θ[(][t][)]∥→∞ (by Ji et al., 2020, Lemma 2) and


_θ[(0)]_ + η


_ℓ[(]i[s][)][z][i]_
_iX∈[n]_


_ℓ[(]i[s][)]_

Xs=0 _iX∈[n]_ _[∥][z][i][∥]_


_∥θ[(][t][+1)]∥_ =


_≤_ _η_


_s=0_


_ℓ[(]i[s][)]_ (by Part 1 of Lemma B.2)

Xs=0 _iX∈[n]_

_t0−1_ _t_

_ℓ[(]i[s][)]_ + c3ηd _ℓ[(]i[s][)]_

Xs=0 _iX∈[n]_ _sX=t0_ _iX∈[n]_


_c3ηd_ _ℓ[(]i[s][)]_
_≤_

Xs=0 _iX∈[n]_

_t0−1_ _t_

= c3ηd _ℓ[(]i[s][)]_ + c3ηd

Xs=0 _iX∈[n]_ _sX=t0_ _iX∈[_

we can conclude that _s=t0_ _i∈[n]_ _[ℓ][(]i[s][)]_ _→∞._

Thus combining inequalities (24) and (25) we get that[P][t] P

_θ[(][t][)]_ ( _µ2)_ _ψ +_ _[c][1][η][∥]τ[µ][∥][2]_ _ts−=1t0_ _i_ [n] _[ℓ][(]i[s][)]_
lim _·_ _−_ lim _∈_ = _[c][4]_
_t_ _θ[(][t][)]_ _≤_ _t_ _d_ _t_ 1 _τ_
_→∞_ _∥_ _∥_ _→∞_ _∥θ[(][t][0][)]∥_ + c2η Pn _s−=Pt0_ _i∈[n]_ _[ℓ]i[(][s][)]_

q

Plugging this upper bound into inequality (23) completes the proof.P P


_n_

_d_

r _[∥][µ][∥][2][.]_


D EARLY-STOPPING BEFORE MODELS INTERPOLATE

Prior works (Sagawa et al., 2019; Byrd & Lipton, 2019) found that adding regularization such as
strong L2 regularization and early stopping can partially restore the effects of importance weights
at the cost of not interpolating the training set. As such, we compare the performance of our
polynomially-tailed loss against cross-entropy when models are early-stopped, using the same settings as before. For each run here, we select the model checkpoint with the best weighted validation
accuracy and evaluate the checkpoint on the test set. Figure 5 compares the test accuracies of the
early-stopped models (IW-ES (w), IW-ES (w[c])) against reweighted models trained past interpolation
(IW (w)).

Consistent with prior works, we see that early stopping as a form of regularization improves the test
accuracies of both loss functions when used with importance weights. Our polynomially-tailed loss


-----

Early-stopped



0.70

0.65

0.60

0.55

0.50

0.45

0.40




Figure 5: Polynomially-tailed loss versus cross-entropy loss on a label shift dataset and a subpopulation shift dataset for neural networks early-stopped according to the best weighted validation
accuracy. ∗ and ∗∗ indicate p < 0.05 and p < 0.005 statistical significance, respectively. While
early stopping is effective for both losses, our polynomially-tailed loss performs even better on im
gives test accuracies that are better than or similar to cross-entropy in all weighted loss scenarios.
The gain over cross-entropy is statistically significant in the binary CIFAR10 runs. On subsampled

|Imbalanced Binary CIFAR10 Early|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|
|---|---|---|---|---|---|---|---|---|---|
|∗ ∗∗ ∗∗||||||||||
|||||||||||
|||||||||||
|||||||||||
|||||||||||
|||||||||||
||IW : Pol hift . a ∗ pping bina t acc over|(w) ynom datase nd ∗∗ is ef ry CI uracie cross||IW-E ly-tail or ne dicat tive f R10. hat ar ntrop|S (w) ed lo ural n e p < or bo e bet y is st||IW-ES versu works .05 a osses than sticall|(w3/2) s cros earl nd p, our or si y sig||

CelebA, the polynomially-tailed loss with squared weights attains the highest mean test accuracy
out of all settings.


E EXPERIMENTAL DETAILS

E.1 HYPERPARAMETERS

E.1.1 FOR SECTION 5.1


**Imbalanced binary CIFAR10 experiments.** We use the same convolutional neural network architecture with random initialization as (Byrd & Lipton, 2019). We train for 400 epochs with SGD
with a batch size of 64. We chose hyperparameters that resulted in stable training for each setting.
We use a constant 0.001 learning rate with 0.9 momentum for “No IW”, “IW (w)”, and “IW-ES (w)”.
We use a constant 0.008 learning rate with no momentum for “IW (w[3][/][2])”, and “IW-ES (w[3][/][2])”.

**Subsampled CelebA experiments.** We use a ResNet50 architecture with ImageNet initialization
as done in (Sagawa et al., 2019). Due to the large dataset size, we use only 2% of the full training

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|Col12|Col13|Col14|Col15|
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
|p|ed||||||||||||||
||||||||||||||||
|.9|0||||||||||||||
||||||||y-tailed Loss ss Entropy||||||||
||||||P C|ol ro|||||||||
|||∗|||||||||||||
||||||||||||||||
||||||||||||||||
||||||||||||||||
||||||||||||||||
|||IW oss on ccordi atistica ly-tail ss-entr he bin ts atta he sam 019). esulte for “N for “I archit||||(w) a lab ng to l sig ed los opy i ary C ins th e con We tr d in st o IW” W (w ectur||el th nif s p n a IFA e vo ai ab, “ 3/2 e w|IW-E shift e bes icance erfor ll we R10 highe lution n for le trai IW (w )”, an ith I|S (w) datase t wei, resp ms ev ighte runs. st me al ne 400 e ning )”, a d “IW mage|t a ght ec en d l O an ura poc for nd -E Net|IW-E nd a ed va tively bette oss sc n sub test a l net hs wi each “IW- S (w initia|S (w2) subpo lidati . Wh r on i enari sampl ccura work th SG setti ES (w 3/2)”. lizati|p- on ile m- os. ed cy ar- D ng. )”. on|

dataset as mentioned in the main text. Reducing the computation requirements allows us to perform
statistical evaluations of the results over sufficiently many seeds. We train for 100 epochs with SGD
with a batch size of 64. We chose hyperparameters that resulted in stable training for each setting.
We use a constant 0.0004 learning rate with 0.9 momentum for all settings.

E.1.2 FOR SECTION 5.2


To fairly evaluate every method, we grid search the hyperparamters for each method extensively. We
kept the same architectural setup as before, but adjust the optimization settings such that the models
train until they interpolate the training data.

**Imbalanced binary CIFAR10 experiments.**



-  Cross entropy + Class Undersampling: We use the same hyperparameters as Cross entropy
on the full dataset and average results over random undersamplings plus initializations

-  LDAM: Following the original paper Cao et al. (2019), we grid search the margin hyperparameter over [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]. We found 1.0 to be the best.

-  CDT Loss, LA Loss, VS Loss: Following Kini et al. (2021), we tune τ and γ with
_τ = 0 being CDT loss and γ = 0 being LA Loss. We grid search across (τ, γ) ∈_

[0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0] × [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]. We found τ = 3 to be the
best for LA loss, γ = 0.5 to be the best for CDT loss, and τ = 3, γ = 0.3 to be the best


-----

for VS Loss. Unlike the original paper (Kini et al., 2021), we found that the best hyperparameters of VS loss are similar to those of LA Loss. We initially followed the recommendations of Kini et al. (2021) and grid searched over (τ, γ) ∈ [0.5, 0.75, 1.0, 1.25, 1.5] ×

[0.05, 0.1, 0.15, 0.2], but it gave VS loss poor performance (mean test acc was only 58.6%
using the best hyperparameters τ = 1, γ = 0.05). Hence, we did a more extensive tuning
afterwards.

-  Poly-tailed Loss: We grid search over (α, c) _∈_ [0.25, 0.5, 1.0, 2.0, 4.0] ×

[1.0, 1.5, 2.0, 2.5, 3.0]. We found α = 2 and exponent c = 3 to be the best.

**Subsampled CelebA.**

-  Cross entropy + Group Undersampling: We use the same hyperparameters as Cross entropy
on the full dataset and average results over random undersamplings plus initializations.

-  VS Loss: We follow the tuning procedure in Kini et al. (2021) for group-sensitive VS Loss
and grid search over γ ∈ [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]. We found γ = 0.4 to be
the best.

-  Poly Loss: We grid search over (α, c) ∈ [1, 2, 3] × [1.0, 1.5, 2.0, 2.5, 3.0]. We found α = 2
and exponent c = 2.5 to be the best.

-  Cross entropy + DRO: We tune the adversarial step size _ηq_ over

[0.0025, 0.005, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1]. We found ηq = 0.05 to be the best.

-  VS Loss + DRO: We used the best γ from VS Loss alone and ηq = 0.05 for DRO.

-  Poly Loss + DRO: We used the best α from Poly Loss alone and ηq = 0.05 for DRO.

E.2 IMPORTANCE WEIGHT COMPUTATION.

Here we explain in greater detail how we compute the importance weights. Note that the scale of the
importance weights is important in ensuring stable training. Let there be G groups of subpopulations
in the training and testing set. G = 2 for binary CIFAR10 and G = 4 for CelebA. Let n1, . . ., nG
be the counts of each group in the training set. Let n be the total number of training examples. In
our case, the test set is balanced across the groups. Our procedure for computing weights is:

1. For each example i that belongs to group g, compute the unbiased weight: wi 1/ng.
_←_

2. Exponentiate the weights bysponds to c = 0. Unbiased importance weighting corresponds to c if necessary: wi ← _w[c]i_ [. No importance weighting corre-] c = 1.

3. To adjust for the scale of weights, normalize by the average exponentiated weight across
the full training set: wi ← _wi/_ _j=1_ _[w][i][. Only normalizing across each minibatch can]_
result in unstable training, especially when a minibatch contains no representatives from a
group.

[P][n]

E.3 EARLY-STOPPING METRIC.

We use the checkpoint with the highest importance weighted validation accuracy when early stopping. We compute separate importance weights of the validation set with respect to the test set, and
reweight the validation accuracy using unbiased weights, even when the training weights are biased.
Our procedure allows for the situation where the validation set is not exactly the same distribution
as the training set.


-----

E.4 EXACT NUMERICAL RESULTS OF OUR EXPERIMENTS

**Cross Entropy** **Poly-tailed Loss**
mean std. err. mean std. err. _p-value_
**Dataset** **Setting**

Subsampled CelebA Early-stopped IW 79.3% 0.4% 80.7% 0.6% 0.014
IW-ES 84.9% 0.7% 84.0% 0.9% 0.762
IW-Exp-ES 85.1% 0.7% 85.9% 0.8% 0.202
Interpolating IW 79.3% 0.4% 80.7% 0.6% 0.014
IW-Exp 78.7% 0.4% 82.7% 0.4% 0.000
No IW 79.6% 0.4% 78.5% 0.4% 0.999
Binary CIFAR10 Early-stopped IW 57.8% 0.4% 60.4% 0.3% 0.000
IW-ES 62.5% 0.7% 63.4% 0.5% 0.022
IW-Exp-ES 61.2% 0.8% 63.5% 0.6% 0.001
Interpolating IW 57.8% 0.4% 60.4% 0.3% 0.000
IW-Exp 57.4% 0.6% 63.0% 0.6% 0.000
No IW 60.5% 0.8% 56.4% 0.4% 1.000

Table 1: Numerical results corresponding to Figure 3 and 5. Here “Exp” corresponds to exponentiated weights. We use 3/2 and 2 as the exponents in Binary CIFAR10 and CelebA respectively. We
use α = 1 for all of these experiments without tuning α.


**Setting** **Summary of method** **Best settings** **Mean**
**test acc**


**Std**
**err**


Cross Entropy N/A 60.5% 0.8%


Cross Entropy + IW Classical importance weighting


N/A 57.8% 0.4%

N/A 58.5% 1.4%

largest margin=1.0 58.7% 1.6%

_γ = 0.5_ 60.2% 1.6%

_τ = 3_ 66.9% 2.1%

_τ = 1, γ = 0.05_ 58.6% 1.4%

_τ = 3, γ = 0.3_ **69.3%** 0.9%


Cross Entropy + Class
undersampling


Balance the classes by throwing away majority data


LDAM Add class dependent constants to logits

CDT Multiply class dependent constants by logits

LA Loss Add class dependent constants to logits


VS Loss (best setting
around range suggested
by paper)

VS Loss (our own tuning)


Multiply by and add class dependent constants to logits

Multiply by and add class dependent constants to logits


Poly-tailed Loss Change loss function and _α = 2, w[3]_ 67.3% 0.8%
exponentiate importance
weights


Table 2: Results on imbalanced binary CIFAR10, corresponding to Figure 4 (left).


-----

**Setting** **Method summary** **Best set-**
**tings**


**Mean**
**test acc**


**Std**
**err**


**Worst**
**group**
**test acc**


**Std**
**err**


Cross Entropy N/A 79.6% 1.2% 43.1% 2.9%


Cross Entropy + Group
undersampling


Balance the groups by
throwing away overrepresented data


N/A 82.3% 3.2% **72.1%** 8.6%

N/A 79.3% 0.4% 43.1% 2.9%

_γ = 0.4_ **82.6%** 0.7% 51.3% 1.5%


Cross Entropy + IW Classical importance
weighting

VS Loss Multiply by and add
class dependent constants to logits


Poly-tailed Loss Change loss function _α_ = 2, 82.4% 0.8% 53.3% 1.5%
and exponentiate impor- _w[2][.][5]_
tance weights


Cross Entropy + DRO N/A 84.8% 0.9% 60.4% 2.4%


VS Loss + DRO Change loss function
and use DRO

Poly-tailed Loss + DRO Change loss function
and use DRO


_γ = 0.4_ 83.2% 0.9% 58.2% 2.0%

_α = 2_ **86.7%** 1.1% **66.5%** 1.9%


Table 3: Results on subsampled CelebA, corresponding to Figure 4 (right)


-----

