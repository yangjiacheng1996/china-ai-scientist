# NEW PERSPECTIVE ON THE GLOBAL CONVERGENCE
## OF FINITE-SUM OPTIMIZATION

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Deep neural networks (DNNs) have shown great success in many machine learning tasks. Their training is challenging since the loss surface of the network architecture is generally non-convex, or even non-smooth. How and under what assumptions is guaranteed convergence to a global minimum possible? We propose
a reformulation of the minimization problem allowing for a new recursive algorithmic framework. By using bounded style assumptions, we prove convergence
to an ε-(global) minimum using _O[˜](1/ε[3]) gradient computations. Our theoretical_
foundation motivates further study, implementation, and optimization of the new
algorithmic framework and further investigation of its non-standard bounded style
assumptions. This new direction broadens our understanding of why and under
what circumstances training of a DNN converges to a global minimum.

1 INTRODUCTION

In recent years, deep neural networks (DNNs) have shown a great success in many machine learning tasks. However, training these neural networks is challenging since the loss surface of network
architecture is generally non-convex, or even non-smooth. Thus, there have been a long-standing
question on how optimization algorithms may converge to a global minimum. Many previous work
have investigated Gradient Descent algorithm and its stochastic version for over-parameterized setting (Arora et al., 2018; Soudry et al., 2018; Allen-Zhu et al., 2019; Du et al., 2019a; Zou & Gu,
2019). Although these works have shown promising convergence results under certain assumptions,
there is still a lack of new efficient methods that can guarantee global convergence for machine
learning optimization. In this paper, we address this problem using a different perspective. Instead
of analyzing the traditional finite-sum formulation, we adopt a new composite formulation that exactly depicts the structure of machine learning where a data set is used to learn a common classifier.

_n_
**Representation. Let** (x[(][i][)], y[(][i][)]) _i=1_ [be a given training set with][ x][(][i][)][ ∈] [R][m][, y][(][i][)][ ∈] [R][c][, we]
investigate the following novel representation for deep learning tasks:



_F_ (w) = [1]


(1)


min
_w∈R[d]_


_φi(h(w; i))_
_i=1_

X


where h(·; i) : R[d] _→_ R[c], i ∈ [n] = {1, . . ., n}, is the classifier for each input data x[(][i][)]; and
_φi : R[c]_ R, i [n], is the loss function corresponding to each output data y[(][i][)]. Our composite for_→_ _∈_ _n_
_mulation (1) is a special case of the finite-sum problem minw∈Rd_ _F_ (w) = _n[1]_ _i=1_ _[f]_ [(][w][;][ i][)] where

each individual function f ( ; i) is a composition of the loss function φi and the classifier h( ; i). This

_·_  P _·_
problem covers various important applications in machine learning, including logistic regression and
neural networks. The most common approach for the finite-sum problem is using first-order methods such as (stochastic) gradient algorithms and making assumptions on the component functions
_f_ (·; i). As an alternative, we further investigate the structure of the loss function φi and narrow
our assumption on the classifier h(·; i). For the purpose of this work, we first consider convex and
Lipschitz-smooth loss functions while the classifiers can be non-convex. Using this representation,
we propose a new framework followed by two algorithms that guarantee global convergence for the
minimization problem.

**Algorithmic Framework. Representation (1) admits a new perspective. Our key insight is to (A)**
define zi[(][t][)] = h(w[(][t][)]; i), where t is an iteration count of the outer loop in our algorithmic framework.


-----

Next (B), we want to approximate the change zi[(][t][+1)] _zi[(][t][)]_ in terms of a step size times the gradient
_−_

_∇φi(zi[(][t][)][) = (][∂φ][i][(][z][)][/∂z][a][)][a][∈][[][c][]]_ _z=zi[(][t][)]_ _[,]_

and (C) we approximate the change h(w[(][t][+1)]; i) − _h(w[(][t][)]; i) in terms of the first order derivative_

_Hi[(][t][)]_ = (∂ha(w; i)/∂wb)a∈[c],b∈[d] _w=w[(][t][)]_ _[.]_

Finally, we combine (A), (B), and (C) to equate the approximations of zi[(][t][+1)] _zi[(][t][)]_ and
_−_
_h(w[(][t][+1)]; i) −_ _h(w[(][t][)]; i). This leads to a recurrence on w[(][t][)]_ of the form w[(][t][+1)] = w[(][t][)] _−_ _η[(][t][)]v[(][t][)],_
where η[(][t][)] is a step size and which involves computing v[(][t][)] by solving a convex quadratic subproblem, see the details in Section 4. We explain two methods for approximating a solution for the
derived subproblem. We show how to approximate the subproblem by transforming it into a strongly
convex problem by adding a regularizer which can be solved in closed form. And we show how to
use Gradient Descent (GD) on the subproblem to find an approximation v[(][t][)] of its solution.

**Convergence Analysis. Our analysis introduces non-standard bounded style assumptions. Intu-**
itively, we assume that our convex and quadratic subproblem has a bounded solution. This allows
us to prove a total complexity of _O[˜]_ ( _ε[1][3][ )][ to find an][ ε][-(global) solution that satisfies][ F]_ [( ˆ]w) − _F∗_ _≤_ _ε,_

where F is the global minimizer of F . Our analysis applies to a wide range of applications in
_∗_
machine learning: Our results hold for squared loss and softmax cross-entropy loss and applicable
for a range of activation functions in DNN as we only assume that the h(·; i) are twice continuously
differentiable and their Hessian matrices (second order derivatives) as well as their gradients (first
order derivatives) are bounded.

**Contributions and Outline. Our contributions in this paper can be summarized as follows.**

-  We propose a new representation (1) for analyzing the machine learning minimization problem. Our formulation utilizes the structure of machine learning tasks where a training data
set of inputs and outputs is used to learn a common classifier. Related work in Section 2
shows how (1) is different from the classical finite-sum problem.

-  Based on the new representation we propose a novel algorithm framework. The algorithmic framework approximates a solution to a subproblem for which we show two distinct
approaches.

-  For general DNNs and based on bounded style assumptions, we prove a total complexity
of _O[˜]_ ( _ε[1][3][ )][ to find an][ ε][-(global) solution that satisfies][ F]_ [( ˆ]w) _−_ _F∗_ _≤_ _ε, where F∗_ is the global

minimizer of F .

We emphasize that our focus is on developing a new theoretical foundation and that a translation
to a practical implementation with empirical results is for future work. Our theoretical foundation
motivates further study, implementation, and optimization of the new algorithmic framework and
further investigation of its non-standard bounded style assumptions. This new direction broadens
our understanding of why and under what circumstances training of a DNN converges to a global
minimum.

The rest of this paper is organized as follows. Section 2 discusses related work. Section 3 describes
our setting and deep learning representation. Section 4 explains our key insight and derives our
Framework 1. Section 5 presents our algorithms and their global convergence. All technical proofs
are deferred to the Appendix.

2 RELATED WORK

**Formulation for Machine Learning Problems. The finite-sum problem is one of the most im-**
portant and fundamental problems in machine learning. Analyzing this model is the most popular
approach in the machine learning literature and it has been studied intensively throughout the years
(Bottou et al., 2018; Reddi et al., 2016; Duchi et al., 2011b). Our new formulation (1) is a special case of the finite-sum problem, however, it is much more complicated than the previous model
since it involves the data index i both inside the classifiers h( ; i) and the loss functions φi. For a

_·_
comparison, previous works only consider a common loss function l(ˆy, y) for the predicted value


-----

_yˆ and output data y (Zou et al., 2018; Soudry et al., 2018). Our modified version of loss function_
_φi is a natural setting for machine learning. We note that when h(w; i) is the output produced by_
a model, our goal is to match this output with the corresponding target y[(][i][)]. For that reason, the
loss function for each output has a dependence on the output data y[(][i][)], and is denoted by φi. This
fact reflects the natural setting of machine learning where the outputs are designed to fit different
targets, and the optimization process depends on both outer function φi and inner functions h(·; i).
This complication may potentially bring a challenge to theoretical analysis. However, with separate
loss functions, we believe this model will help to exploit better the structure of machine learning
problems and gain more insights on the neural network architecture.

Other related composite optimization models are also investigated thoroughly in (Lewis & Wright,
2016; Zhang & Xiao, 2019; Tran-Dinh et al., 2020). Our model is different from these works as
it does not have a common function wrapping outside the finite-sum term, as in (Lewis & Wright,
2016). Note that a broad class of variance reduction algorithms (e.g. SAG (Le Roux et al., 2012),
SAGA (Defazio et al., 2014), SVRG (Johnson & Zhang, 2013), SARAH (Nguyen et al., 2017)) is
designed specifically for the finite-sum formulation and is known to have certain benefits over Gradient Descent. In addition, the multilevel composite problem considered in (Zhang & Xiao, 2021)
also covers empirical risk minimization problem. However our formulation does not match their
work since our inner function h(w; i) is not an independent expectation over some data distribution,
but a specific function that depends on the current data.

**Global Convergence for Neural Networks. A recent popular line of research is studying the dy-**
namics of optimization methods on some specific neural network architectures. There are some early
works that show the global convergence of Gradient Descent (GD) for simple linear network and
two-layer network (Brutzkus et al., 2018; Soudry et al., 2018; Arora et al., 2019; Du et al., 2019b).
Some further works extend these results to deep learning architectures (Allen-Zhu et al., 2019; Du
et al., 2019a; Zou & Gu, 2019). These theoretical guarantees are generally proved for the case when
the last output layer is fixed, which is not standard in practice. A recent work (Nguyen & Mondelli,
2020) prove the global convergence for GD when all layers are trained with some initial conditions.
However, these results are for neural networks without bias neurons and it is unclear how these analyses can be extended to handle the bias terms of deep networks with different activations. Our novel
framework and algorithms do not exclude learning bias layers as in (Nguyen & Mondelli, 2020).

Using a different algorithm, Brutzkus et al. (2018) investigate Stochastic Gradient Descent (SGD)
for two-layer networks in a restricted linearly separable data setting. This line of research continues
with the works from Allen-Zhu et al. (2019); Zou et al. (2018) and later with Zou & Gu (2019). They
justify the global convergence of SGD for deep neural networks for some probability depending on
the number of input data and the initialization process.

**Over-Paramaterized Settings and other Assumptions for Machine Learning. Most of the mod-**
ern learning architectures are over-parameterized, which means that the number of parameters are
very large and often far more than the number of input data. Some recent works prove the global
convergence of Gradient Descent when the number of neurons are extensively large, e.g. (Zou &
Gu, 2019) requires Ω(n[8]) neurons for every hidden layer, and (Nguyen & Mondelli, 2020) improves this number to Ω(n[3]). If the initial point satisfies some special conditions, then they can
show a better dependence of Ω(n). In Allen-Zhu et al. (2019), the authors initialize the weights
using a random Gaussian distribution where the variance depends on the dimension of the problem.
In non-convex setting, they prove the convergence of SGD using the assumption that the dimension
depends inversely on the tolerance ϵ. We will discuss how these over-paramaterized settings might
be a necessary condition to develop our theory.

Other standard assumptions for machine learning include the bounded gradient assumption (Nemirovski et al., 2009; Shalev-Shwartz et al., 2007; Reddi et al., 2016; Tran et al., 2021). It is also
common to assume all the iterations of an algorithm stays in a bounded domain (Duchi et al., 2011a;
Levy et al., 2018; G¨urb¨uzbalaban et al., 2019; Reddi et al., 2018; Vaswani et al., 2021). Since we
are analyzing a new composite formulation, it is understandable that our assumptions may also not
be standard. However, we believe that there is a strong connection between our assumptions and the
traditional setting of machine learning. We will discuss this point more clearly in Section 4.


-----

3 BACKGROUND

In this section, we discuss our formulation and notations in detail. Although this paper focuses on
deep neural networks, our framework and theoretical analysis are general and applicable for other
learning architectures.

**Deep Learning Representation. Let** (x[(][i][)], y[(][i][)]) _i=1_ [be a training data set where][ x][(][i][)][ ∈] [R][m][ is a]
_{_ _}[n]_
training input and y[(][i][)] _∈_ R[c] is a training output. We consider a fully-connected neural network with
_L layers, where the l-th layer, l ∈{0, 1, . . ., L}, has nl neurons. We represent layer 0-th and L-th_
layer as input and output layers, respectively, that is, n0 = d and nL = c. For l ∈{1, . . ., L}, let
_W_ [(][l][)] _∈_ R[n][l][−][1][×][n][l] and b[(][l][)] _∈_ R[n][l], where {(W [(][l][)], b[(][l][)])[L]l=1[}][ represent the parameters of the neural]
network. A classifier h(w; i) is formulated as

_h(w; i) = W_ [(][L][)][⊤]σL−1(W [(][L][−][1)][⊤]σL−2(. . . σ1(W [(1)][⊤]x[(][i][)] + b[(1)]) . . . ) + b[(][L][−][1)]) + b[(][L][)],

where w = vec({W [(1)], b[(1)], . . ., W [(][L][)], b[(][L][)]}) ∈ R[d] is the vectorized weight and {σl}l[L]=1[−][1] [are some]
activation functions. The most common choices for machine learning are ReLU, sigmoid, hyperbolic
tangent and softplus. For j ∈ [c], hj(·; i) : R[d] _→_ R denotes the component function of the output
_h(·; i), for each data i ∈_ [n] respectively. Moreover, we define h[∗]i [= arg min][z][∈][R][c][ φ][i][(][z][)][, i][ ∈] [[][n][]][.]

**Loss Functions.** The well-known loss functions in neural networks for solving classification and
regression problems are softmax cross-entropy loss and square loss, respectively:

_(Softmax) Cross-Entropy Loss: F_ (w) = _n[1]_ _ni=1_ _[f]_ [(][w][;][ i][)][ with]

_f_ (w; i) = −y[(]P[i][)][⊤] log(softmax(h(w; i))). (2)


_n_
_i=1_ _[f]_ [(][w][;][ i][)][ with]
P

_f_ (w; i) = [1] (3)

2 _[∥][h][(][w][;][ i][)][ −]_ _[y][(][i][)][∥][2][.]_


_Squared Loss: F_ (w) = _n[1]_


We provide some basic definitions in optimization theory to support our theory.

**Definition 1 (L-smooth). Function φ : R[c]** _→_ R is Lφ-smooth if there exists a constant Lφ > 0
_such that, ∀x1, x2 ∈_ R[c],

_φ(x1)_ _φ(x2)_ _Lφ_ _x1_ _x2_ _._ (4)
_∥∇_ _−∇_ _∥≤_ _∥_ _−_ _∥_

**Definition 2 (Convex). Function φ : R[c]** _→_ R is convex if ∀x1, x2 ∈ R[c],

_φ(x1)_ _φ(x2)_ _φ(x2), x1_ _x2_ _._ (5)
_−_ _≥⟨∇_ _−_ _⟩_

The following corollary shows the properties of softmax cross-entropy loss (2) and squared loss (3).

**Corollary 1. For softmax cross-entropy loss (2) and squared loss (3), there exist functions h(·; i) :**
R[d] _→_ R[c] _and φi : R[c]_ _→_ R such that, for i ∈ [n], φi(z) is convex and Lφ-smooth with Lφ = 1, and

_f_ (w; i) = φi(h(w; i)) = φi(z) _z=h(w;i)[.]_ (6)

4 NEW ALGORITHM FRAMEWORK

4.1 KEY INSIGHT

We assume f (w; i) = φi(h(w; i)) with φi convex and Lφ-smooth. Our goal is to utilize the convexity of the outer function φi. In order to simplify notation, we write _zφi(h(w[(][t][)]; i)) instead of_
_∇_
_∇zφi(z)_ _z=h(w[(][t][)];i)_ [and denote][ z]i[(][t][)] = h(w[(][t][)]; i). Starting from the current weight w[(][t][)], we would

like to find the next point w[(][t][+1)] that satisfies the following approximation for all i ∈ [n]:

_h(w[(][t][+1)]; i) = zi[(][t][+1)]_ _≈_ _zi[(][t][)]_ _−_ _αi[(][t][)][∇][z][φ][i][(][z]i[(][t][)][) =][ h][(][w][(][t][)][;][ i][)][ −]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][.]_ (7)


-----

We can see that this approximation is a “noisy” version of a gradient descent update for every
function φi, simultaneously for all i [n]. In order to do this, we use the following update
_∈_

_w[(][t][+1)]_ = w[(][t][)] _−_ _η[(][t][)]v[(][t][)],_ (8)

where η[(][t][)] _> 0 is a learning rate and v[(][t][)]_ is a search direction that helps us approximate equation
(7). If the update term η[(][t][)]v[(][t][)] is small enough, and if h(·; i) has some nice smooth properties, then
from basic calculus we have the following approximation:

_h(w[(][t][+1)]; i) = h(w[(][t][)]_ _η[(][t][)]v[(][t][)]; i)_ _h(w[(][t][)]; i)_ _Hi[(][t][)]_ _η[(][t][)]v[(][t][)][],_ (9)
_−_ _≈_ _−_
 

where Hi[(][t][)] is a matrix in R[c][×][d] with first-order derivatives. Motivated by approximations (7) and
(9), we consider the following optimization problem:


_v[(][t][)]_ = arg min
_∗_ _v∈R[d]_


_Hi[(][t][)]_ _η[(][t][)]v_ _αi[(][t][)]_ (10)
_i=1_ _∥_ _−_ _[∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2][.]_

X   


Hence, by solving for the solution v[(][t][)] of problem (10) we are able to find a search direction for the
_∗_
key approximation (7). This yields our new algorithmic Framework 1, see below.

**Framework 1 New Algorithm Framework**

**Initialization: Choose an initial point w[(0)]** _∈_ R[d];
**for t = 0, 1, · · ·, T −** 1 do

Solve for an approximation v[(][t][)] of the solution v[(][t][)] of the problem in (10)
_∗_


_v[(][t][)]_ = arg min
_∗_ _v∈R[d]_


_i=1_ _∥η[(][t][)]Hi[(][t][)][v][ −]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_

X


Update w[(][t][+1)] = w[(][t][)] _−_ _η[(][t][)]v[(][t][)]_

**end for**

4.2 TECHNICAL ASSUMPTIONS

**Assumption 1.that it is lower bounded, i.e. The loss function inf** _z∈ φRc φi is convex andi(z) > −∞_ _for L iφ ∈-smooth for[n]._ _i ∈_ [n]. Moreover, we assume

We have shown the convexity and smoothness of squared loss and softmax cross-entropy loss in
Section 3. The bounded property of φi is required in any algorithm for the well-definedness of (1).
Now, in order to use the Taylor series approximation, we need the following assumption on the
neural network architecture h:

**Assumption 2. We assume that h(·; i) is twice continuously differentiable for all i ∈** [n] (i.e. the
_second-order partial derivatives of all scalars hj(_ ; i) are continuous for all j [c] and i [n]),

_·_ _∈_ _∈_
_and that their Hessian matrices are bounded, that is, there exists a G > 0 such that for all w ∈_ R[d],
_i ∈_ [n] and j ∈ [c],

_∥Mi,j(w)∥_ = ∥Jw (∇whj(w; i))∥≤ _G,_ (11)

_where Jw denotes the Jacobian[1]._

**Remark 1 (Relation to second-order methods). Although our analysis requires an assumption on**
_the Hessian matrices of h(w; i), our algorithms do not use any second order information or try to_
_approximate this information. Our theoretical analysis focused on the approximation of the clas-_
_sifier and the gradient information, therefore is not related to the second order type algorithms._
_It is currently unclear how to apply second order methods into our problem, however, this is an_
_interesting research question to expand the scope of this work._

1For a continuously differentiable function g(w) : Rd → Rc we define the Jacobian Jw(g(w)) as the matrix
(∂ga(w)/∂wb)a∈[c],b∈[d].


-----

Assumption 2 allows us to apply a Taylor approximation of each function hj( ; i) with which we

_·_
prove the following Lemma that bounds the error in equation (9):

**Lemma 1. Suppose that Assumption 2 holds for the classifier h. Then for all i ∈** [n] and 0 ≤ _t < T_ _,_

_h(w[(][t][+1)]; i) = h(w[(][t][)]_ _−_ _η[(][t][)]v[(][t][)]; i) = h(w[(][t][)]; i) −_ _η[(][t][)]Hi[(][t][)][v][(][t][)][ +][ ϵ][(]i[t][)][,]_ (12)

_where_
_Hi[(][t][)]_ = Jw(h(w; i)) _w=w(t)_ R[c][×][d] (13)
_|_ _∈_

_is defined as the Jacobian matrix of h(w; i) at w[(][t][)]_ _and entries ϵ[(]i,j[t][)][,][ j][ ∈]_ [[][c][]][, of vector][ ϵ][(]i[t][)] _satisfy_

_ϵ[(]i,j[t][)][| ≤]_ [1] (14)
_|_ 2 [(][η][(][t][)][)][2][∥][v][(][t][)][∥][2][G.]

In order to approximate (7) combined with (9), that is, to make sure the right hand sides of (7) and
(9) are close to one another, we consider the optimization problem (10):


_v[(][t][)]_ = arg min
_∗_ _v∈R[d]_


_i=1_ _∥η[(][t][)]Hi[(][t][)][v][ −]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2][.]_

X


The optimal value of problem (10) is equal to 0 if there exists a vector v∗[(][t][)] satisfying η[(][t][)]Hi[(][t][)][v]∗[(][t][)] =
_αRi[(][c][t], this condition is equivalent to a linear system with[)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][ for every][ i][ ∈]_ [[][n][]][. Since the solution] n · c[ v] constraints and∗[(][t][)] is in R[d] and d ∇ variables. In the over-zφi(h(w[(][t][)]; i)) is in
parameterized setting where dimension d is sufficiently large (d ≫ _n · c) and there are no identical_
data, there exists almost surely a vector v[(][t][)] that interpolates all the training set, see the Appendix
_∗_
for details.

Let us note that an approximation of v[(][t][)] serves as the search direction for Framework 1. For this
_∗_
reason, the solution v[(][t][)] of problem (10) plays a similar role as a gradient in the search direction
_∗_
of (stochastic) gradient descent method. It is standard to assume a bounded gradient in the machine learning literature (Nemirovski et al., 2009; Shalev-Shwartz et al., 2007; Reddi et al., 2016).
Motivated by these facts, we assume the following Assumption 3, which implies the existence of a
near-optimal bounded solution of (10):

**Assumption 3. We consider an over-parameterized setting where dimension d is sufficiently large**
_enough to interpolate all the data and the tolerance ε. We assume that there exists a bound V > 0_
_such that for ε > 0 and 0_ _t < T as in Framework 1, there exists a vector ˆv[(][t]ε[)]_ _[with][ ∥]v[ˆ][(][t]ε[)]_
_so that_ _≤_ _∗_ _∗_ _[∥][2][ ≤]_ _[V]_

_n_

1 1
2 _n_ _i=1_ _∥η[(][t][)]Hi[(][t][)]vˆ∗[(][t]ε[)]_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2][ ≤]_ _[ε][2][.]_

X

Our Assumption 3 requires a nice dependency on the tolerance ε for the gradient matrices Hi[(][t][)] and
_zφi(h(w[(][t][)]; i)). We note that at the starting point t = 0, these matrices may depend on ε due to_
_∇_
the initialization process and the dependence of d on ε. This setting is similar to previous works,
e.g. Allen-Zhu et al. (2019).

5 NEW ALGORITHMS AND CONVERGENCE RESULTS

5.1 APPROXIMATING THE SOLUTION USING REGULARIZER

Since problem (10) is convex and quadratic, we consider the following regularized problem:


_n_

_η[(][t][)]Hi[(][t][)][v][ −]_ _[α]i[(][t][)]_
_i=1_ _∥_ _[∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2][ +][ ε]2[2]_ _[∥][v][∥][2]_

X


Ψ(v) = [1]


(15)


min
_v∈R[d]_


-----

for some small ε > 0 and t ≥ 0. It is widely known that problem (15) is strongly convex, and has a
unique minimizer v[(][t][)]reg[. The global minimizer satisfies][ ∇]v[Ψ(][v][(][t][)]reg[) = 0][. We have]
_∗_ _∗_


_vΨ(v) = [1]_
_∇_ _n_

=

Therefore,

1

_v[(][t][)]reg_ [=]
_∗_ _n_



[η[(][t][)]Hi[(][t][)]⊤Hi[(][t][)][η][(][t][)][v][ −] _[α]i[(][t][)][η][(][t][)][H]i[(][t][)]⊤∇zφi(h(w(t); i))] + ε2 · v_
_i=1_

X


_η[(][t][)]Hi[(][t][)]⊤Hi[(][t][)][η][(][t][)][ +][ ε][2][I]_
_i=1_

X


1 _αi[(][t][)][η][(][t][)][H]i[(][t][)]⊤_ _zφi(h(w(t); i))_
_n_ _∇_

_i=1_

X

_n_

_i=1_ _αi[(][t][)][η][(][t][)][H]i[(][t][)]⊤∇zφi(h(w(t); i))!_ _._

X


_v −_


_−1_
!


_η[(][t][)]Hi[(][t][)]⊤Hi[(][t][)][η][(][t][)][ +][ ε][2][I]_
_i=1_

X


(16)


If ε[2] is small enough, then v[(][t][)]reg [is a close approximation of the solution][ v][(][t][)] for problem (10). Our
_∗_ _∗_
first algorithm updates Framework 1 based on this approximation.

**Algorithm 1 Solve for the exact solution of the regularized problem**

**Initialization: Choose an initial point w[(0)]** _∈_ R[d], tolerance ε > 0;
**for t = 0, 1, · · ·, T −** 1 do

Update the search direction v[(][t][)] as the solution v[(][t][)]reg [of problem in (15):]
_∗_

_n_ _−1_ _n_

_v[(][t][)]_ = v∗[(][t][)]reg [=] _n1_ _i=1_ _η[(][t][)]Hi[(][t][)]⊤Hi[(][t][)][η][(][t][)][ +][ ε][2][I]!_ _n1_ _i=1_ _αi[(][t][)][η][(][t][)][H]i[(][t][)]⊤∇zφi(h(w(t); i))_

X X


Update w[(][t][+1)] = w[(][t][)] _−_ _η[(][t][)]v[(][t][)]_

**end for**


The following Lemma shows the relation between the regularized solution v[(][t][)]reg [and the optimal]
_∗_
solution of the original convex problem ˆv[(][t]ε[)][.]
_∗_

**Lemma 2. For given ε > 0, suppose that Assumption 3 holds for bound V > 0. Then, for iteration**
0 _t < T_ _, the optimal solution v[(][t][)]reg_ _[of problem (15) satisfies][ ∥][v][(][t][)]reg[∥][2][ ≤]_ [2 +][ V][ and]
_≤_ _∗_ _∗_

_n_

1 1
2 _n_ _i=1_ _∥η[(][t][)]Hi[(][t][)][v]∗[(][t][)]reg_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2][ ≤]_ [(1 +][ V]2 [)][ε][2][.] (17)

X


Based on Lemma 2, we guarantee the global convergence of Algorithm 1 and prove our first theorem.
Since it is currently expensive to solve for the exact solution of problem (15), our algorithm serves
as a theoretical method to obtain the global convergence for the finite-sum minimization.

**Theorem 1. Let w[(][t][)]** _be generated by Algorithm 1 where we use the closed form solution for the_
_search direction. We execute Algorithm 1 for T =_ _βε_ _[outer loops for some constant][ β >][ 0][. We]_

_assume Assumption 1 holds. Suppose that Assumption 2 holds for G > 0 and Assumption 3 holds_
_for V > 0. We set the step size equal to η[(][t][)]_ = D[√]ε for some D > 0 and choose a learning rate
_αi[(][t][)]_ = (1 + ε)αi[(][t][−][1)] = (1 + ε)[t]αi[(0)][. Based on][ β][, we define][ α]i[(0)] = _e[β]αLφ_ _[with][ α][ ∈]_ [(0][,][ 1]3 [)][. Let][ F][∗]

_be the global minimizer of F_ _, and h[∗]i_ [= arg min][z][∈][R][c][ φ][i][(][z][)][, i][ ∈] [[][n][]][. Then]

1 _T −1_ _n_

_T_ Xt=0 [F (w[(][t][)]) − _F∗] ≤_ 2(1[e][β][L] −[φ][(1 +]3α)αβ[ ε][)] _[·][ 1]n_ Xi=1 _∥h(w[(0)]; i) −_ _h[∗]i_ _[∥][2][ ·][ ε]_

+ _[e][β][L][φ][(3][ε][ + 2)]_ _c(4 + (V + 2)GD[2])[2]_ + 8 + 4V _ε._ (18)

8α(1 3α) _·_
_−_
 

We note that β is a constant for the purpose of choosing the number of iterations T . The analysis
can be simplified by choosing β = 1 with T = [1]ε [. Notice that the common convergence criteria for]


-----

finding a stationary point for non-convex problems is _T[1]_ _Tt=1_

has been widely used in the existing literature for non-convex optimization problems. Our conver-[||∇][F] [(][w][t][)][||][2][ ≤] _[O][(][ε][)][. This criteria]_

_T_ P

gence criteria _T[1]_ _t=1[[][F]_ [(][w][t][)][ −] _[F][∗][]][ ≤]_ _[O][(][ε][)][ is slightly different, in order to find a global solution]_

for non-convex problems.
P

Our proof for Theorem 1 is novel and insightful. It is originally motivated by the Gradient Descent
update (7) and the convexity of the loss functions φi 1. For this reason it may not be a surprise that
Algorithm 1 can find an ε-global solution after _ε_ iterations. However, computing the exact
_O_

solution in every iteration might be extremely challenging, especially when the number of samples
_n is large. Therefore, we present a different approach to this problem in the following section. _ 

5.2 APPROXIMATION USING GRADIENT DESCENT


In this section, we use Gradient Descent (GD) algorithm to solve the strongly convex problem (15).
It is well-known that if ψ(x) 2

e.g. Nesterov (2004)). Hence − Ψ([µ]·[∥]) is[x][∥] ε[2][ is convex for][2]-strongly convex. For each iteration[ ∀][x][ ∈] [R][c][, then][ ψ][(][x][)][ is][ µ] t, we use GD to find a[-strongly convex (see]
search direction v[(][t][)] which is sufficiently close to the optimal solution v[(][t][)]reg [in that]
_∗_

_v[(][t][)]_ _v[(][t][)]reg[∥≤]_ _[ε.]_ (19)
_∥_ _−_ _∗_

Our Algorithm 2 is described as follows.

**Algorithm 2 Solve the regularized problem using Gradient Descent**

**Initialization: Choose an initial point w[(0)]** _∈_ R[d], tolerance ε > 0;
**for t = 0, 1, · · ·, T −** 1 do

Use Gradient Descent algorithm to solve Problem (15) and find a solution v[(][t][)] that satisfies

_v[(][t][)]_ _v[(][t][)]reg[∥≤]_ _[ε]_
_∥_ _−_ _∗_


Update w[(][t][+1)] = w[(][t][)] _−_ _η[(][t][)]v[(][t][)]_

**end for**

Since Algorithm 2 can only approximate a solution within some ε-preciseness, we need a supplemental assumption for the analysis of our next Theorem 2:

**Assumption 4. Let Hi[(][t][)]** _be the Jacobian matrix defined in Lemma 1. We assume that there exists_
_some constant H > 0 such that, for i ∈_ [n], ε > 0, and 0 ≤ _t < T as in Algorithm 2,_

_Hi[(][t][)]_ (20)
_∥_ _[∥≤]_ _√[H]ε._

Assumption 4 requires a mild condition on the bounded Jacobian of h(w; i), and the upper bound
may depend on ε. This flexibility allows us to accommodate a good dependence of ε for the theoretical analysis. We are now ready to present our convergence theorem for Algorithm 2.

**Theorem 2. Let w[(][t][)]** _be generated by Algorithm 2 where v[(][t][)]_ _satisfies (19). We execute Algorithm_
_2 for T =_ _βε_ _[outer loops for some constant][ β >][ 0][. We assume Assumption 1 holds. Suppose]_

_that Assumption 2 holds for G > 0, Assumption 3 holds for V > 0 and Assumption 4 holds for_
_H > 0. We set the step size equal to η[(][t][)]_ = D[√]ε for some D > 0 and choose a learning rate
_αi[(][t][)]_ = (1 + ε)αi[(][t][−][1)] = (1 + ε)[t]αi[(0)][. Based on][ β][, we define][ α]i[(0)] = _e[β]αLφ_ _[with][ α][ ∈]_ [(0][,][ 1]4 [)][. Let][ F][∗]

_be the global minimizer of F_ _, and h[∗]i_ [= arg min][z][∈][R][c][ φ][i][(][z][)][, i][ ∈] [[][n][]][. Then]

1 _T −1_ _n_

_T_ Xt=0 [F (w[(][t][)]) − _F∗] ≤_ 2(1[e][β][L] −[φ][(1 +]4α)αβ[ ε][)] _[·][ 1]n_ Xi=1 _∥h(w[(0)]; i) −_ _h[∗]i_ _[∥][2][ ·][ ε]_

+ _[e][β][L][φ][(4][ε][ + 3)]_ _D[2]H_ [2] + c(2 + (V + ε[2] + 2)GD[2])[2] + 2 + V _ε._

2α(1 4α) _·_
_−_
 


-----

Theorem 2 implies Corollary 2 which provides the computational complexity for Algorithm 2. Note
that for (Stochastic) Gradient Descent, we derive the complexity in terms of component gradient
calculations for the finite-sum problem (1). As an alternative, for Algorithm 2 we compare the
number of component gradients in problem (15). Such individual gradient has the following form:

_∇vψi(v) = η[(][t][)]Hi[(][t][)]⊤Hi[(][t][)][η][(][t][)][v][ −]_ _[α]i[(][t][)][η][(][t][)][H]i[(][t][)]⊤∇zφi(h(w(t); i))._

In machine learning applications, the gradient of f (·; i) is calculated using automatic differentiation
(i.e. backpropagation). Since f (·; i) is the composition of the network structure h(·; i) and loss function φi( ), this process also computes the Jacobian matrix Hi[(][t][)] and the gradient _zφi(h(w[(][t][)]; i)) at_

_·_ _∇_
a specific weight w[(][t][)]. Since matrix-vector multiplication computation is not expensive, the cost for
computing the component gradient of problem (15) is similar to problem (1).

**Corollary 2. Suppose that the conditions in Theorem 2 hold with η[(][t][)]** = _[D]√√Nεˆ_ _[for some][ D >][ 0][ and]_

0 < ˆε ≤ _N (that is, we set ε = ˆε/N_ _), where_

_ni=1_ _i_ 7e[β] _Lφ[D[2]H_ [2]+c(2+(V +3)GD[2])[2]+2+V ]
_N =_ _[e][β]_ _[L][φ]_ _n(1[∥][h]4[(]α[w])[(0)]αβ[;][i][)][−][h][∗][∥][2]_ + 2α(1 4α) _._
P _−_ _−_

_Then, the total complexity to guarantee min0≤t≤T −1[F_ (w[(][t][)]) _−_ _F∗] ≤_ _T[1]_ _Tt=0 −1[[][F]_ [(][w][(][t][)][)] _[−]_ _[F][∗][]][ ≤]_ _ε[ˆ]_

_is_ _n_ _[N]εˆ[ 3][3][β][ (][D][2][H]_ [2][ + (ˆ]ε[2]/N )) log( _[N]εˆ_ [)] _._ P
_O_

**Remark 2.** _Corollary 2 shows that O (1_ _/εˆ) outer loop iterations are needed in order to reach an n_
_εˆ-global solution, and it proves that each iteration needs the equivalent of O_ _εˆ[2][ log(][ 1]εˆ[)]_ _gradient_

_computations for computing an approximate solution. In total, Algorithm 2 has total complexity_
_O_ _nεˆ[3][ log(][ 1]εˆ[)]_ _for finding an ˆε-global solution._   

_For a comparison, Stochastic Gradient Descent uses a total of _  _O(_ _ε[1][2][ )][ gradient computations to]_

_find a stationary point satisfying E[∥∇F_ ( ˆw)∥[2]] ≤ _ε for non-convex problems (Ghadimi & Lan,_
_2013). Gradient Descent has a better complexity in terms of ε, i.e. O(_ _[n]ε_ [)][ such that][ ∥∇][F] [( ˆ]w)∥[2] _≤_ _ε_

_(Nesterov, 2004). However, both methods may not be able to reach a global solution of (1). In order_
_to guarantee global convergence for nonconvex settings, one may resort to use Polyak-Lojasiewicz_
_(PL) inequality (Karimi et al., 2016; Gower et al., 2021). This assumption is widely known to be_
_strong, which implies that every stationary point is also a global minimizer._

6 FURTHER DISCUSSION AND CONCLUSIONS

This paper presents an alternative composite formulation for solving the finite-sum optimization
problem. Our formulation allows a new way of exploiting the structure of machine learning problems and the convexity of squared loss and softmax cross entropy loss, and leads to a novel algorithmic framework that guarantees global convergence (when the outer loss functions are convex and
Lipschitz-smooth). Our analysis is general and can be applied to various different learning architectures, in particular, our analysis and assumptions match practical neural networks; in recent years,
there has been a great interest in the structure of deep learning architectures for over-parameterized
settings (Arora et al., 2018; Allen-Zhu et al., 2019; Nguyen & Mondelli, 2020). Algorithm 2 demonstrates a gradient method to solve the regularized problem, however, other methods can be applied
to our framework (e.g. conjugate gradient descent).

Our theoretical foundation motivates further study, implementation, and optimization of the new
algorithmic framework and further investigation of its non-standard bounded style assumptions.
Possible research directions include more practical algorithm designs based on our Framework 1,
and different related methods to solve the regularized problem and approximate the solution. This
potentially leads to a new class of efficient algorithms for machine learning problems. This paper
presents a new perspective to the research community.


-----

ETHICS STATEMENT

This paper does not contain ethics concerns.

REFERENCES

Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via overparameterization. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the
_36th International Conference on Machine Learning, volume 97 of Proceedings of Machine_
_[Learning Research, pp. 242–252. PMLR, 09–15 Jun 2019. URL http://proceedings.](http://proceedings.mlr.press/v97/allen-zhu19a.html)_
[mlr.press/v97/allen-zhu19a.html.](http://proceedings.mlr.press/v97/allen-zhu19a.html)

Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In Jennifer Dy and Andreas Krause (eds.), Proceedings of
_the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine_
_[Learning Research, pp. 244–253. PMLR, 10–15 Jul 2018. URL http://proceedings.](http://proceedings.mlr.press/v80/arora18a.html)_
[mlr.press/v80/arora18a.html.](http://proceedings.mlr.press/v80/arora18a.html)

Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In Kamalika
Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference
_on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 322–332._
PMLR, 09–15 Jun 2019. [URL http://proceedings.mlr.press/v97/arora19a.](http://proceedings.mlr.press/v97/arora19a.html)
[html.](http://proceedings.mlr.press/v97/arora19a.html)

L´eon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. SIAM Review, 60(2):223–311, 2018. doi: 10.1137/16M1080173.

Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. SGD learns overparameterized networks that provably generalize on linearly separable data. In International
_[Conference on Learning Representations, 2018. URL https://openreview.net/forum?](https://openreview.net/forum?id=rJ33wwxRb)_
[id=rJ33wwxRb.](https://openreview.net/forum?id=rJ33wwxRb)

Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in Neural Information
_Processing Systems, pp. 1646–1654, 2014._

Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),
_Proceedings of the 36th International Conference on Machine Learning, volume 97 of Pro-_
_ceedings of Machine Learning Research, pp. 1675–1685. PMLR, 09–15 Jun 2019a._ URL
[http://proceedings.mlr.press/v97/du19c.html.](http://proceedings.mlr.press/v97/du19c.html)

Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
[2019b. URL https://openreview.net/forum?id=S1eK3i09YQ.](https://openreview.net/forum?id=S1eK3i09YQ)

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(61):2121–2159, 2011a. URL
[http://jmlr.org/papers/v12/duchi11a.html.](http://jmlr.org/papers/v12/duchi11a.html)

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12:2121–2159, 2011b.

S. Ghadimi and G. Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM J. Optim., 23(4):2341–2368, 2013.

Robert Gower, Othmane Sebbouh, and Nicolas Loizou. Sgd for structured nonconvex functions:
Learning rates, minibatching and interpolation. In Arindam Banerjee and Kenji Fukumizu (eds.),
_Proceedings of The 24th International Conference on Artificial Intelligence and Statistics, volume_
130 of Proceedings of Machine Learning Research, pp. 1315–1323. PMLR, 13–15 Apr 2021.
[URL https://proceedings.mlr.press/v130/gower21a.html.](https://proceedings.mlr.press/v130/gower21a.html)


-----

M. G¨urb¨uzbalaban, A. Ozdaglar, and P. A. Parrilo. Convergence rate of incremental gradient and
incremental newton methods. SIAM Journal on Optimization, 29(4):2542–2565, 2019. doi: 10.
[1137/17M1147846. URL https://doi.org/10.1137/17M1147846.](https://doi.org/10.1137/17M1147846)

Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in Neural Information Processing Systems, pp. 315–323, 2013.

Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximalgradient methods under the Polyak-Łojasiewicz condition. In Paolo Frasconi, Niels Landwehr,
Giuseppe Manco, and Jilles Vreeken (eds.), Machine Learning and Knowledge Discovery in
_Databases, pp. 795–811, Cham, 2016. Springer International Publishing._

Nicolas Le Roux, Mark Schmidt, and Francis Bach. A stochastic gradient method with an exponential convergence rate for finite training sets. In NIPS, pp. 2663–2671, 2012.

Kfir Y. Levy, Alp Yurtsever, and Volkan Cevher. Online adaptive methods, universality and
acceleration. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran As[sociates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/](https://proceedings.neurips.cc/paper/2018/file/b0169350cd35566c47ba83c6ec1d6f82-Paper.pdf)
[b0169350cd35566c47ba83c6ec1d6f82-Paper.pdf.](https://proceedings.neurips.cc/paper/2018/file/b0169350cd35566c47ba83c6ec1d6f82-Paper.pdf)

Adrian S. Lewis and Stephen J. Wright. A proximal method for composite minimization. Mathe_matical Programming, 158:501–546, 2016._

A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to
stochastic programming. SIAM J. on Optimization, 19(4):1574–1609, 2009.

Yurii Nesterov. Introductory lectures on convex optimization : a basic course. Applied optimization.
Kluwer Academic Publ., Boston, Dordrecht, London, 2004. ISBN 1-4020-7553-7.

Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak´aˇc. Sarah: A novel method for machine
learning problems using stochastic recursive gradient. In Proceedings of the 34th International
_Conference on Machine Learning-Volume 70, pp. 2613–2621. JMLR. org, 2017._

Quynh N Nguyen and Marco Mondelli. Global convergence of deep networks with one wide
layer followed by pyramidal topology. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp.
11961–11972. Curran Associates, Inc., 2020. [URL https://proceedings.neurips.](https://proceedings.neurips.cc/paper/2020/file/8abfe8ac9ec214d68541fcb888c0b4c3-Paper.pdf)
[cc/paper/2020/file/8abfe8ac9ec214d68541fcb888c0b4c3-Paper.pdf.](https://proceedings.neurips.cc/paper/2020/file/8abfe8ac9ec214d68541fcb888c0b4c3-Paper.pdf)

Sashank J. Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochastic variance
reduction for nonconvex optimization. In Maria Florina Balcan and Kilian Q. Weinberger (eds.),
_Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceed-_
_ings of Machine Learning Research, pp. 314–323, New York, New York, USA, 20–22 Jun 2016._
[PMLR. URL https://proceedings.mlr.press/v48/reddi16.html.](https://proceedings.mlr.press/v48/reddi16.html)

Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In
_[International Conference on Learning Representations, 2018. URL https://openreview.](https://openreview.net/forum?id=ryQu7f-RZ)_
[net/forum?id=ryQu7f-RZ.](https://openreview.net/forum?id=ryQu7f-RZ)

Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pegasos: Primal estimated sub-gradient
solver for svm. Association for Computing Machinery, 2007. doi: 10.1145/1273496.1273598.
[URL https://doi.org/10.1145/1273496.1273598.](https://doi.org/10.1145/1273496.1273598)

Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit bias of gradient descent on separable data. J. Mach. Learn. Res., 19(1):2822–2878, January
2018. ISSN 1532-4435.

Trang H Tran, Lam M Nguyen, and Quoc Tran-Dinh. Smg: A shuffling gradient-based method
with momentum. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International
_Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research,_
[pp. 10379–10389. PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/](https://proceedings.mlr.press/v139/tran21b.html)
[v139/tran21b.html.](https://proceedings.mlr.press/v139/tran21b.html)


-----

Quoc Tran-Dinh, Nhan Pham, and Lam Nguyen. Stochastic Gauss-Newton algorithms for nonconvex compositional optimization. In Hal Daum´e III and Aarti Singh (eds.), Proceedings of the 37th
_International Conference on Machine Learning, volume 119 of Proceedings of Machine Learn-_
_[ing Research, pp. 9572–9582. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.](https://proceedings.mlr.press/v119/tran-dinh20a.html)_
[press/v119/tran-dinh20a.html.](https://proceedings.mlr.press/v119/tran-dinh20a.html)

Sharan Vaswani, Issam Laradji, Frederik Kunstner, Si Yi Meng, Mark Schmidt, and Simon LacosteJulien. Adaptive gradient methods converge faster with over-parameterization (but you should do
a line-search), 2021.

Junyu Zhang and Lin Xiao. A stochastic composite gradient method with incremental variance reduction. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran As[sociates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/file/](https://proceedings.neurips.cc/paper/2019/file/a68259547f3d25ab3c0a5c0adb4e3498-Paper.pdf)
[a68259547f3d25ab3c0a5c0adb4e3498-Paper.pdf.](https://proceedings.neurips.cc/paper/2019/file/a68259547f3d25ab3c0a5c0adb4e3498-Paper.pdf)

Junyu Zhang and Lin Xiao. Multilevel composite stochastic optimization via nested variance reduction. SIAM Journal on Optimization, 31(2):1131–1157, 2021. doi: 10.1137/19M1285457. URL
[https://doi.org/10.1137/19M1285457.](https://doi.org/10.1137/19M1285457)

Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep
neural networks. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d’Alch´e-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural In_formation Processing Systems 32:_ _Annual Conference on Neural Information Process-_
_ing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp._
[2053–2062, 2019. URL https://proceedings.neurips.cc/paper/2019/hash/](https://proceedings.neurips.cc/paper/2019/hash/6a61d423d02a1c56250dc23ae7ff12f3-Abstract.html)
[6a61d423d02a1c56250dc23ae7ff12f3-Abstract.html.](https://proceedings.neurips.cc/paper/2019/hash/6a61d423d02a1c56250dc23ae7ff12f3-Abstract.html)

Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks, 2018.


-----

## APPENDIX

A TABLE OF NOTATIONS

Notation Meaning

_F_ Global minimization function of F in (1)
_∗_

_F∗_ = minw∈Rd F (w)

_h[∗]i_ _h[∗]i_ [= arg min][z][∈][R][c][ φ][i][(][z][)][,][ i][ ∈] [[][n][]]

_v[(][t][)]_ Solution of the convex problem in (10)
_∗_

1 _n_

minv∈Rd [1]2 _n_ _i=1_ _i_ _[v][ −]_ _[α]i[(][t][)]_

_[∥][η][(][t][)][H]_ [(][t][)] _[∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_

_v[(][t][)]_ An approximation ofP _v[(][t][)]_ which is used as the search direction in Framework 1
_∗_

_vˆ∗[(][t]ε[)]_ A vector that satisfies

1 1 _n_
2 _n_ _i=1_ _[∥][η][(][t][)][H]i[(][t][)][v][ −]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2][ ≤]_ _[ε][2]_

for someP _ε > 0 and ∥vˆ∗[(][t]ε[)][∥][2][ ≤]_ _[V][, for some][ V >][ 0][.]_

_v∗[(][t][)]reg_ Solution of the strongly convex problem in (15)

1 1 _n_
minv∈Rd 2 _n_ _i=1_ _i_ _[v][ −]_ _[α]i[(][t][)]_ 2
n _[∥][η][(][t][)][H]_ [(][t][)] _[∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2][ +][ ε][2]_ _[∥][v][∥][2][o]_
P


B USEFUL RESULTS


The following lemmas provide key tools for our results.
**Lemma 3 (Squared loss). Let b ∈** R[c] _and define φ(z) =_ 2[1] _[∥][z][ −]_ _[b][∥][2][ for][ z][ ∈]_ [R][c][. Then][ φ][ is convex]

_and Lφ-smooth with Lφ = 1._
**Lemma 4 (Softmax cross-entropy loss). Let index a ∈** [c] and define

_c_ _c_

_φ(z) = log_ "k=1 exp(zk − _za)#_ = log "k=1 exp(wk[⊤][z][)]# _,_

X X

_for z = (z1, . . ., zc)[⊤]_ _∈_ R[c], where wk = ek−ea with ei representing the i-th unit vector (containing
1 at the i-th position and 0 elsewhere). Then φ is convex and Lφ-smooth with Lφ = 1.

The following lemma is a standard result in (Nesterov, 2004).
**Lemma 5 ((Nesterov, 2004)). If φ is Lφ-smooth and convex, then for ∀z ∈** R[c],

_∥∇φ(z)∥[2]_ _≤_ 2Lφ(φ(z) − _φ(z∗)),_ (21)

_where z_ = arg minz φ(z).
_∗_

The following useful derivations could be used later in our theoretical analysis. Since φi is convex,
by Definition 2 we have

_φi(h(w; i))_ _φi(h(w[′]; i)) +_ _zφi(z)_ _._ (22)
_≥_ _∇_ _z=h(w[′];i)[, h][(][w][;][ i][)][ −]_ _[h][(][w][′][;][ i][)]_
 

If φi is convex and Lφ-smooth, then by Lemma 5

2
2Lφ [φi(h(w; i)) _φi(h[∗]i_ [)]][,] (23)

_z=h(w;i)_ _≤_ _−_

where h[∗]i [= arg min][z][∈][R][c][ φ][i][(][z][)][.]

_[∇][z][φ][i][(][z][)]_

We compute gradients of f (w; i) in term of φi(h(w; i)).


-----

-  Gradient of softmax cross-entropy loss:

_∂φi(z)_
_∇φi(z)_ _z=h(w;i)_ [=] _∂z1_


where for j ∈ [c],


_⊤_



_z=h(w;i)[, . . ., ∂φ]∂z[i][(]c[z][)]_


_z=h(w;i)_


exp [h(w;i)]j _−[h(w;i)]I(y(i))_

_∂φi(z)_ _ck=1_ [exp] [h(w;i)]k−[h(w;i)]I(y(i))

_∂zj_ _z=h(w;i)_ [=] −PPk≠ _ckI(=1y[(][i][exp][)]_ ) [exp][h([wh(;iw)];ki)]−k[h−([wh(;iw)];Ii()]yI((iy))(i))

-  Gradient of squared loss: P  


_, j ̸= I(y[(][i][)])_

_._ (24)
_, j = I(y[(][i][)])_


_∇φi(z)_ _z=h(w;i)_ [=][ h][(][w][;][ i][)][ −] _[y][(][i][)][.]_ (25)

C ADDITIONAL DISCUSSION

C.1 ABOUT ASSUMPTION 2

We make a formal assumption for the case h(·; i) is closely approximated by k(·; i).
**Assumption 5. We assume that for all i ∈** [n] there exists some approximations k(w; i) : R[d] _→_ R[c]
_such that_

_|kj(w; i) −_ _hj(w; i)| ≤_ _ε, ∀w ∈_ R[d], i ∈ [n] and j ∈ [c], (26)

_where k(·; i) are twice continuously differentiable (i.e. the second-order partial derivatives of all_
_scalars kj(_ ; i) are continuous for all i [n]), and that their Hessian matrices are bounded:

_·_ _∈_

_∥Mi,j(w)∥_ = ∥Jw (∇wkj(w; i))∥≤ _G, ∀w ∈_ R[d], i ∈ [n] and j ∈ [c]. (27)

Assumption 5 allows us to prove the following Lemma that bound the error in equation (9):
**Lemma 6. Suppose that Assumption 5 holds for the classifier h. Then for all i ∈** [n] and 0 ≤ _t < T_ _,_
_we have:_

_h(w[(][t][+1)]; i) = h(w[(][t][)]_ _−_ _η[(][t][)]v[(][t][)]; i) = h(w[(][t][)]; i) −_ _η[(][t][)]Hi[(][t][)][v][(][t][)][ +][ ϵ][(]i[t][)][,]_ (28)

_where Hi[(][t][)]_ _is defined to be the Jacobian matrix of the approximation k(w; i) at w[(][t][)]:_


_∂k∂w1(w1;i)_ _. . ._ _∂k∂w1(wd;i)_

_. . ._ _. . ._ _. . ._

_∂k∂wc(w1;i)_ _. . ._ _∂k∂wc(wd;i)_


_Hi[(][t][)]_ := Jwk(w; i) _w=w(t) =_
_|_

_Additionally we have,_


_∈_ R[c][×][d]. (29)
_w=w[(][t][)]_


_ϵ[(]i,j[t][)][| ≤]_ [1] (30)
_|_ 2 [(][η][(][t][)][)][2][∥][v][(][t][)][∥][2][G][ + 2][ε, j][ ∈] [[][c][]][.]

Note that these result recover the case when h(·; i) is itself smooth. Hence we analyze our algorithms
using the result of Lemma 6, which generalizes the result from Lemma 1.

C.2 ABOUT ASSUMPTION 3

In this section, we justify the existence of the search direction in Assumption 3 (almost surely). We
argue that there exists a vector ˆv[(][t]ε[)] [satisfying]
_∗_

_n_

1 1
2 _n_ _i=1_ _∥η[(][t][)]Hi[(][t][)]vˆ∗[(][t]ε[)]_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2][ ≤]_ _[ε][2][.]_

X


-----

It is sufficient to find a vector v satisfying that

_η[(][t][)]Hi[(][t][)][v][ =][ α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][ for every][ i][ ∈]_ [[][n][]][.]

Since the solution v is in R[d] and ∇zφi(h(w[(][t][)]; i)) is in R[c], this condition is equivalent to a linear
system with n·c constraints and d variables. Let A and b be the following stacked matrix and vector:

_H1[(][t][)][η][(][t][)]_ _α1[(][t][)][∇][z][φ][1][(][h][(][w][(][t][)][;][ i][))]_

_A =_  _. . ._   _. . ._ 

Hn[(][t][)][η][(][t][)] αn[(][t][)] _z[φ]n[(][h][(][w][(][t][)][;][ i][))]_
   _[∇]_ 
  _[∈]_ [R][n][·][c][×][d][,][ and][ b][ =]   _[∈]_ [R][n][·][c][,]

then the problem reduce to finding the solution of the equation Av = b. In the over-parameterized
setting where dimension d is sufficiently large (d ≫ _n · c), then rank A = n · c almost surely and_
there exists almost surely a vector v that interpolates all the training set.

To demonstrate this fact easier, we consider a simple neural network where the classifier h(w; i) is
formulated as

_h(w; i) = W_ [(2)][⊤]σ(W [(1)][⊤]x[(][i][)]),

where c = 1, W [(1)] _∈_ R[m][×][l] and W [(2)] _∈_ R[l][×][1], w = vec({W [(1)], W [(2)]}) ∈ R[d] is the vectorized
weight where d = l(m + 1) and σ is sigmoid activation function.

_Hi[(][t][)]_ is defined to be the Jacobian matrix of h(w; i) at w[(][t][)]:


_∂h(w;i)_ _∂h(w;i)_
_Hi[(][t][)]_ := Jwh(w; i) _w=w(t) =_ _∂w1_ _. . ._ _∂wd_
_|_
h


_∈_ R[1][×][d],
_w=w[(][t][)]_


then

_H1[(][t][)]_ _∂h∂w(w1;1)_ _. . ._ _∂h∂w(wd;1)_

_A = η[(][t][)]_  _. . ._   _. . ._ _. . ._ _. . ._ 

Hn[(][t][)]  _∂h∂w(w1;n)_ _. . ._ _∂h∂w(wd;n)_ 
   
  [=][ η][(][t][)]   _[∈]_ [R][n][×][d][.]

We want to show that A has full rank, almost surely. We consider the over-parameterized setting
where the last layer has at least n neuron (i.e. l = n and the simple version when c = 1. We argue
that rank of matrix A is greater than or equal to rank of the submatrix B created by the weights of
the last layer W [(2)] _∈_ R[n]:

_∂h(w;1)_ _. . ._ _∂h(w;1)_

_∂W1[(2)]_ _∂Wn[(2)]_

_B =_  _. . ._ _. . ._ _. . ._ 

_∂h(w;n)_ _∂h1(w;n)_

 _. . ._ 
 _∂W1[(2)]_ _∂Wn[(2)]_ 
 
  _[∈]_ [R][n][×][n][.]

Note that h(·, i) is a linear function of the last weight layers (in this simple case W [(2)] _∈_ R[n] and
_σ(W_ [(1)][⊤]x[(][i][)]) ∈ R[n]), we can compute the partial derivatives as follows:

_∂h(w; i)_

= σ(W [(1)][⊤]x[(][i][)]); i [n].
_∂W_ [(2)] _∈_

Hence

_σ(W_ [(1)][⊤]x[(1)])

_B =_  _. . ._ 

σ(W [(1)][⊤]x[(][n][)])
 
  _[∈]_ [R][n][×][n][.]

Assuming that there are no identical data, and σ is the sigmoid activation, the set of weights W [(1)]
that make matrix B degenerate has measure zero. Hence B has full rank almost surely, and we have
the same conclusion for A. Therefore we are able to prove the almost surely existence of a solution
_v of the linear equation Av = b for simple two layers network. Using the same argument, this result_
can be generalized for larger neural networks where the dimension d is sufficiently large (d ≫ _nc)._


-----

C.3 INITIALIZATION EXAMPLE

Our Assumption 3 requires a nice dependency on the tolerance ε for the gradient matrices Hi[(0)] and
_zφi(h(w[(0)]; i)). We note that at the starting point t = 0, these matrices may depend on ε due_
_∇_
to the initialization process and the dependence of d on ε. In order to accommodate the choice of
learning rate η[(0)] = D[√]ε in our theorems, in this section we describe a network initialization that
satisfies _Hi[(0)]_ = Θ _√1ε_ where the gradient norm _zφi(h(w[(0)]; i))_ is at most constant order
_∥_ _∥_ _∥∇_ _∥_
with respect to ε. To simplify the problem, we only consider small-dimension data and networks 
without activation.

**About the target vector: We choose φi to be the softmax cross-entropy loss. By Lemma 7 (see**
below), we have that the gradient norm is upper bounded by a constant c, where c is the output
dimension of the problem and is not dependent on ε. Note that when we stack all gradients for n
data points, then the size of new vector is still not dependent on ε.

**About the network architecture: For simplicity, we consider the following classification problem**
where

-  The input data is in R[2]. There are only two data points {x[(1)], x[(2)]}. Input data is bounded
and non-degenerate (we will clarify this property later).

-  The output data is (categorical) in R[2]: {y[(1)] = (1, 0), y[(2)] = (0, 1)}.

We want to have an over-parameterized setting where the dimension of weight vector is at least
_nc = 4. We consider a simple network with two layers, no biases and no activation functions. Let the_
number of neurons in the hidden layer be m. The flow of this network is (in) R[2] _→_ R[m] _→_ R[2] (out).
First, we consider the case where m = 1.

-  The first layer has 2 parameters (w1, w2) and only 1 neuron that outputs z[(][i][)] = w1x[(]1[i][)] +
_w2x2[(][i][)]_ (the subscript is for the coordinate of input data x[(][i][)]).

-  The second layer has 2 parameters (w3, w4). The final output is

_h(w, i) = [w3(w1x[(]1[i][)]_ [+][ w][2][x]2[(][i][)][)][, w][4][(][w][1][x]1[(][i][)] [+][ w][2][x]2[(][i][)][)]][⊤] _[∈]_ [R][2][,]

with w = [w1, w2, w3, w4][⊤] _∈_ R[4]. This network satisfies that the Hessian matrices of h(w; i) are
bounded. Let Q and b be the following stacked matrix and vector:


_H1[(0)]_
_H2[(0)]_


_zφ1(h(w[(0)]; 1))_
_∇_

_zφ2(h(w[(0)]; 2))_
_∇_


_∈_ R[4][×][4], and b =


_∈_ R[4],


_Q =_


Then we have the following:

_∇w[w3(w1x[(1)]1_ + w2x[(1)]2 [)]]

_Q = Q(w) =_ _H1[(0)]_ = ∇w[w4(w1x[(1)]1 + w2x[(1)]2 [)]]

"H2[(0)]#  _w[w3(w1x[(2)]1_ + w2x[(2)]2 [)]]

∇ 
 
∇w[w4(w1x[(2)]1 + w2x[(2)]2 [)]]
 

_w3x[(1)]1_ _w3x[(1)]2_ _w1x[(1)]1_ + w2x[(1)]2 0

= w4x[(1)]1 _w4x[(1)]2_ 0 _w1x[(1)]1_ + w2x[(1)]2  _._

w3x[(2)]1 _w3x[(2)]2_ _w1x[(2)]1_ + w2x[(2)]2 0 
 
 
w4x[(2)]1 _w4x[(2)]2_ 0 _w1x[(2)]1_ + w2x[(2)]2 
 
 

The determinant of this matrix is a polynomial of the weight w and the input data. Under some mild
non-degenerate condition of the input data, we can choose some base point w[′] that made this matrix
invertible (note that if this condition is not satisfied, we can rescale/add a very small noise to the
data - which is the common procedure in machine learning).


-----

Hence the system Qu = b always has a solution. Now we consider the following two initializations:

1. We choose to initialize the starting point at w[(0)] = _√1ε_ _w[′]_ and note that Q(w) is a linear function

of w and Q(w[′]) is independent of ε. Then the norm of matrix Q(w[(0)]) has the same scale with _√1ε_ .

2. Instead of choosing m = 1, we consider an over-parameterized network where m = [1]ε [(recall]

that m is the number of neurons in the hidden layer). The hidden layer in this case is:


_z1[(][i][)]_ = w1[(1)],1[x][(]1[i][)] [+][ w]2[(1)],1[x][(]2[i][)]
_. . ._
_zm[(][i][)]_ = w1[(1)],m[x][(]1[i][)] [+][ w]2[(1)],m[x][(]2[i][)]


_z =_


The output layer is:

_y1[(][i][)]_ = z1[(][i][)][w]1[(2)],1 [+][ · · ·][ +][ z]m[(][i][)][w]m,[(2)]1 [= (][w]1[(1)],1[x][(]1[i][)] [+][ w]2[(1)],1[x][(]2[i][)][)][w]1[(2)],1 [+][ · · ·][ + (][w]1[(1)],m[x][(]1[i][)] [+][ w]2[(1)],m[x][(]2[i][)][)][w]m,[(2)]1
(y2[(][i][)] = z1[(][i][)][w]1[(2)],2 [+][ · · ·][ +][ z]m[(][i][)][w]m,[(2)]2 [= (][w]1[(1)],1[x][(]1[i][)] [+][ w]2[(1)],1[x][(]2[i][)][)][w]1[(2)],2 [+][ · · ·][ + (][w]1[(1)],m[x][(]1[i][)] [+][ w]2[(1)],m[x][(]2[i][)][)][w]m,[(2)]2

with w = [w1[(1)],1[, . . ., w]1[(1)],m[, w]2[(1)],1[, . . ., w]2[(1)],m[, w]1[(2)],1[, w]1[(2)],2[, . . ., w]m,[(2)]1[, w]m,[(2)]2[]][⊤] _[∈]_ [R][4][m][.]

Hence,

_w1[(2)],1[x][(1)]1_ _. . . wm,[(2)]1[x][(1)]1_ _w1[(2)],1[x][(1)]2_ _. . ._ _wm,[(2)]1[x][(1)]2_ _z1[(1)]_ 0 _. . . zm[(1)]_ 0

 


_w1[(2)],2[x][(1)]1_ _. . . wm,[(2)]2[x][(1)]1_ _w1[(2)],2[x][(1)]2_ _. . ._ _wm,[(2)]2[x][(1)]2_ 0 _z1[(1)]_ _. . . 0_ _zm[(1)]_

_w1[(2)],1[x][(2)]1_ _. . . wm,[(2)]1[x][(2)]1_ _w1[(2)],1[x][(2)]2_ _. . ._ _wm,[(2)]1[x][(2)]2_ _z1[(2)]_ 0 _. . . zm[(2)]_ 0

_w1[(2)],2[x][(2)]1_ _. . . wm,[(2)]2[x][(2)]1_ _w1[(2)],2[x][(2)]2_ _. . ._ _wm,[(2)]2[x][(2)]2_ 0 _z1[(2)]_ _. . . 0_ _zm[(2)]_


_Q(w) =_


Hence, the number of (possibly) non-zero elements in each row is 3m = [3]ε [.]

For matrix A of rank r, we have _A_ 2 _A_ _F_ _r_ _A_ 2. Since the rank of Q(w) is at most 4
(nc = 4, independent of ε), we only need to find the Frobenius norm of ∥ _∥_ _≤∥_ _∥_ _≤_ _[√]_ _∥_ _∥_ _Q(w). We have_


4m

_qij_ _._
_|_ _|[2]_
_j=1_

X


_∥Q(w)∥F =_


_i=1_


Let(0, 0) qmin and choose and qmax w be the element with smallest/largest magnitude of= 0 such that z = 0, qmin > 0 and independent of Q(w ε). Hence,. Suppose that√√8ε _qmin x[(][i][)]_ ≠
_̸_ _̸_ _|_ _| ≤_

_Q(w)_ _F_ _√√12ε_ _qmax_ .
_∥_ _∥_ _≤_ _|_ _|_

Hence, _Q(w)_ = Θ _√1ε_ . Therefore this simple network initialization supports the dependence
_∥_ _∥_
on ε for our Assumption 3. We note that a similar setting is found in (Allen-Zhu et al., 2019), where 
the authors initialize the weights using a random Gaussian distribution with a variance depending
on the dimension of the problem. In non-convex setting, they prove the convergence of SGD using
the assumption that the number of neurons m depends inversely on the tolerance ε.

**Lemma 7. For softmax cross-entropy loss, and x = h(w; i) ∈** R[c], for ∀w ∈ R[d] _and i ∈_ [n], we
_have_

2
_c._ (31)

_[∇][z][φ][i][(][x][)]_ _x=h(w;i)_ _≤_

_Proof. By (24), we have for i = 1, . . ., n,_


-----

-  For j ̸= I(y[(][i][)]):


2

exp [h(w; i)]j [h(w; i)]I(y(i))

_c_ _−_
_k=1_ [exp]  [h(w; i)]k [h(w; i)]I(y(i)) !
_−_
P exp  [h(w; i)]j [h(w; i)]I(y(i))

_−_

1 + _k=I(y [(][i][)])_ [exp] [h(w; i)]k [h(w; i)]I(y(i))

_̸_ _−_

[P]  


_∂φi(x)_

_∂xj_




2



_x=h(w;i)_


2
!

2
!


_≤_ 1.

_≤_ 1



-  For j = I(y[(][i][)]):


2 2

_∂φi(x)_ _k=I(y[(][i][)])_ [exp] [h(w; i)]k [h(w; i)]I(y(i))

= _̸_ _c_ _−_

 _∂xj_ _x=h(w;i)_ P _k=1_ [exp] [h (w; i)]k − [h(w; i)]I(y(i))  !

Pk=I(y[(][i][)]) [exp] [h(w; i)]k [h(w; i)]I(y(i))

= _̸_ _−_

1 +P _k=I(y[(][i][)])_ [exp]  [h(w; i)]k [h(w; i)]I(y(i))

_̸_ _−_

Hence, for i = 1, . . ., n, [P]  

2 _c_ 2

_∂φi(x)_

= _c._

_x=h(w;i)_ _∂xj_ _x=h(w;i)_ _≤_

_j=1_  

X

This completes the proof.[∇][z][φ][i][(][x][)]


_∂φi(x)_

_∂xj_




2

_x=h(w;i)_




D PROOFS OF LEMMAS AND COROLLARY 1

PROOF OF LEMMA 1

_Proof. Since h(·; i) are twice continuously differentiable for all i ∈_ [n], we have the following
Taylor approximation for each component outputs hj( ; i) where j [c] and i [n]:

_·_ _∈_ _∈_

_hj(w[(][t][+1)]; i) = hj(w[(][t][)]_ _η[(][t][)]v[(][t][)]; i)_
_−_

= hj(w[(][t][)]; i) − **Jwhj(w; i)|w=w(t)** _η[(][t][)]v[(][t][)]_ + 2[1] [(][η][(][t][)][v][(][t][)][)][⊤][M][i,j][( ˜]w[(][t][)])(η[(][t][)]v[(][t][)]),

(32)


where Mi,j( ˜w[(][t][)]) is the Hessian matrices of hj( ; i)at ˜w[(][t][)] and ˜w[(][t][)] = αw[(][t][)] + (1 _α)w[(][t][+1)]_ for

_·_ _−_
some α ∈ [0, 1]. This leads to our desired statement:

_h(w[(][t][+1)]; i) = h(w[(][t][)]_ _−_ _η[(][t][)]v[(][t][)]; i) = h(w[(][t][)]; i) −_ _η[(][t][)]Hi[(][t][)][v][(][t][)][ +][ ϵ][(]i[t][)][,]_

where

_ϵ[(]i,j[t][)]_ [= 1] _w[(][t][)])(η[(][t][)]v[(][t][)]), j_ [c],

2 [(][η][(][t][)][v][(][t][)][)][⊤][M][i,j][( ˜] _∈_

Hence we get the final bound:


_ϵ[(]i,j[t][)][| ≤]_ [1] (η[(][t][)]v[(][t][)])[⊤]Mi,j( ˜w[(][t][)])(η[(][t][)]v[(][t][)])
_|_ 2

_w[(][t][)])_

_≤_ 2[1] [(][η][(][t][)][)][2][∥][v][(][t][)][∥][2][ · ∥][M][i,j][( ˜] _∥_

(11)
_≤_ 2[1] [(][η][(][t][)][)][2][∥][v][(][t][)][∥][2][G, j][ ∈] [[][c][]][.]


-----

PROOF OF LEMMA 2

_Proof. From Assumption 3, we know that there exists ˆv[(][t]ε[)]_ [so that]
_∗_

_n_

1 1
2 _n_ _i=1_ _∥η[(][t][)]Hi[(][t][)]vˆ∗[(][t]ε[)]_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2][ ≤]_ _[ε][2][,]_

X

and ∥vˆ∗[(][t]ε[)][∥][2][ ≤] _[V][, for some][ V >][ 0][. Hence,]_

_n_

1 1
2 _n_ _i=1_ _∥η[(][t][)]Hi[(][t][)]vˆ∗[(][t]ε[)]_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2][ +][ ε]2[2]_ _[∥]v[ˆ]∗[(][t]ε[)][∥][2][ ≤]_ _[ε][2][ +][ ε]2[2]_ _[V][ = (1 +][ V]2 [)][ε][2][.]_

X


Since v[(][t][)]reg [is the optimal solution of the problem in (15) for][ 0][ ≤] _[t < T]_ [, we have]
_∗_

_n_

1 1
2 _n_ _i=1_ _∥η[(][t][)]Hi[(][t][)][v]∗[(][t][)]reg_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2][ +][ ε]2[2]_ _[∥][v]∗[(][t][)]reg[∥][2][ ≤]_ [(1 +][ V]2 [)][ε][2][.]

X


Therefore, we have (17) and _v[(][t][)]reg[∥][2][ ≤]_ [2 +][ V][ for][ 0][ ≤] _[t < T]_ [.]
_∥_ _∗_

PROOF OF LEMMA 3

_Proof. 1. We want to show that for any α ∈_ [0, 1]

_φ(αz1 + (1 −_ _α)z2) ≤_ _αφ(z1) + (1 −_ _α)φ(z2), ∀z1, z2 ∈_ R[c], (33)

in order to have the convexity of φ with respect to z (see (Nesterov, 2004)).

For any α ∈ [0, 1], we have for ∀z1, z2 ∈ R[c],

_α_ _z1_ _b_ + (1 _α)_ _z2_ _b_ _α(z1_ _b) + (1_ _α)(z2_ _b)_
_∥_ _−_ _∥[2]_ _−_ _∥_ _−_ _∥[2]_ _−∥_ _−_ _−_ _−_ _∥[2]_

= α _z1_ _b_ + (1 _α)_ _z2_ _b_ _α[2]_ _z1_ _b_ (1 _α)[2]_ _z2_ _b_
_∥_ _−_ _∥[2]_ _−_ _∥_ _−_ _∥[2]_ _−_ _∥_ _−_ _∥[2]_ _−_ _−_ _∥_ _−_ _∥[2]_

2α(1 _α)_ _z1_ _b, z2_ _b_
_−_ _−_ _⟨_ _−_ _−_ _⟩_

_α(1_ _α)_ _z1_ _b_ + (1 _α)α_ _z2_ _b_ 2α(1 _α)_ _z1_ _b_ _z2_ _b_
_≥_ _−_ _∥_ _−_ _∥[2]_ _−_ _∥_ _−_ _∥[2]_ _−_ _−_ _∥_ _−_ _∥· ∥_ _−_ _∥_

= α(1 _α) (_ _z1_ _b_ _z2_ _b_ )[2] 0,
_−_ _∥_ _−_ _∥−∥_ _−_ _∥_ _≥_

where the first inequality follows according to Cauchy-Schwarz inequality ⟨a, b⟩≤∥a∥·∥b∥. Hence,

1

_z2_ _b_ _._

2 _[∥][αz][1][ + (1][ −]_ _[α][)][z][2][ −]_ _[b][∥][2][ ≤]_ _[α]2_ _[∥][z][1][ −]_ _[b][∥][2][ + (1][ −]2_ _[α][)]_ _∥_ _−_ _∥[2]_


Therefore, (33) implies the convexity of φ with respect to z.

2. We want to show that ∃Lφ > 0 such that

_∥∇φ(z1) −∇φ(z2)∥≤_ _Lφ∥z1 −_ _z2∥, ∀z1, z2 ∈_ R[c]. (34)

Notice that ∇φ(z) = z − _b, then clearly ∀z1, z2 ∈_ R[c],

_φ(z1)_ _φ(z2)_ = _z1_ _z2_ _._
_∥∇_ _−∇_ _∥_ _∥_ _−_ _∥_

Therefore, (34) implies the Lφ-smoothness of φ with respect to z with Lφ = 1.

PROOF OF LEMMA 4

_Proof.using Holder inequality 1. For ∀z1, z2 ∈_ R[c] and 1 ≤ _k ≤_ _c, denote uk,1 = exp(wk[⊤][z][1][)][ and][ u][k,][2][ = exp(][w]k[⊤][z][2][)][ and]_



[1] [1]

_c_ _p_ _c_ _q_

_ak_ _bk_
_k=1_ _|_ _|[p]!_ _k=1_ _|_ _|[q]!_

X X


_, where_ [1] (35)

_p_ [+ 1]q [= 1][,]


_ak_ _bk_
_k=1_ _·_ _≤_

X


-----

we have

_c_ _c_

_φ(αz1 + (1_ _α)z2) = log_ exp(wk[⊤][(][αz][1] [+ (1][ −] _[α][)][z][2][))]_ = log _u[α]k,1_ _k,2_
_−_ "k=1 # "k=1 _[·][ u][(1][−][α][)]_

X X

(35) log _c_ _uk,α·1α[1]_ _α_ _c_ _u(1k,2−α)·_ (1−1α) 1−α[]
_≤_  _k=1_ ! _k=1_ !

X X

 _c_ _c_ 

= α log "k=1 exp(wk[⊤][z][1][)]# + (1 − _α) log_ "k=1 exp(wk[⊤][z][2][)]#

X X


= αφ(z1) + (1 _α)φ(z2),_
_−_

where the first inequality since log(x) is an increasing function for ∀x > 0 and exp(v) > 0 for
_∀v ∈_ R. Therefore, (33) implies the convexity of φ with respect to z.

2. Note that ∥∇[2]φ(z)∥≤ _Lφ if and only if φ(z) is Lφ-smooth (see (Nesterov, 2004)). First, we_
compute gradient of φ(z):

-  For i ̸= a:


_∂φ(z)_ exp(zi _za)_

= _c_ _−_
_∂zi_ Pk=1 [exp(][z][k][ −] _[z][a][)]_ _[.]_



-  For i = a:


_∂φ∂z(zi_ ) = _−_ [P]ck=1k≠ _a[exp(][exp(][z][k][z][ −][k][ −][z][a][z][)][a][)]_ = _[−]_ [P]k[c]ck=1=1[exp(][exp(][z][z][k][k][ −][ −][z][z][a][a][) + 1][)]

1 exp(zi _za)_
= P1 + _c_ P _c_ _−_
_−_ Pk=1 [exp(][z][k][ −] _[z][a][) =][ −][1 +]_ Pk=1 [exp(][z][k][ −] _[z][a][)]_ _[.]_

We then calculate _∂z[∂][2]j[φ]∂z[(][z][)]i_ [=] _∂z∂j_ _∂φ∂z(iz)_

 



-  For i = j:

_∂[2]φ(z)_ = [exp(][z][i][ −] _[z][a][)[][P]k[c]_ =1 [exp(][z][k][ −] _[z][a][)]][ −]_ [exp(][z][i][ −] _[z][a][) exp(][z][i][ −]_ _[z][a][)]_
_∂zj∂zi_ [[P][c]k=1 [exp(][z][k][ −] _[z][a][)]][2]_

= [exp(][z][i][ −] _[z][a][)[][P]k[c]_ =1 [exp(][z][k][ −] _[z][a][)][ −]_ [exp(][z][i][ −] _[z][a][)]]_ _._

[[P][c]k=1 [exp(][z][k][ −] _[z][a][)]][2]_

-  For i ̸= j:

_∂[2]φ(z)_
= _._
_∂zj∂zi_ _[−]_ [exp(][[P][c]k[z]=1[j][ −][exp(][z][a][) exp(][z][k][ −] _[z][z][i][a][ −][)]][2][z][a][)]_

Denote that yi = exp(zi − _za) ≥_ 0, i ∈ [c], we have:



-  For i = j:

_∂[2]φ(z)_

[=]

_∂zj∂zi_

-  For i ̸= j:
_∂[2]φ(z)_
_∂zj∂zi_


_yi([P][c]k=1_ _[y][k][ −]_ _[y][i][)]_

([P][c]k=1 _[y][k][)][2]_

_yiyj_
_|_ _|_

[=]

([P][c]k=1 _[y][k][)][2][ .]_


-----

Recall that for matrix A = (aij) ∈ R[c][×][c]: ∥A∥[2] _≤∥A∥F[2]_ [=][ P]i[c]=1 _cj=1_ _[|][a][ij][|][2][. We have:]_

_c_ 2 _c_ P

_∂[2]φ(z)_ 1

_yi[2][(]_ _yk_ _yi)[2]_ + (yiyj)[2]

Xj=1 _∂zj∂zi_ _≤_ ([P][c]k=1 _[y][k][)][4]_  _kX=1_ _−_ Xj≠ _i_ 

 _c_ _c_ 

1
= _yi[2][(]_ _yk)[2]_ 2yi[2] _yk.yi + yi[4]_ [+] (yiyj)[2]

([P][c]k=1 _[y][k][)][4]_  _kX=1_ _−_ _kX=1_ Xj≠ _i_

 _c_ _c_ _c_

1
= _yi[2][(]_ _yk)[2]_ 2yi[3] _yk + yi[2]_ _yk[2]_

([P][c]k=1 _[y][k][)][4]_ " _k=1_ _−_ _k=1_ _k=1_ #

X X X

Therefore,


_∂[2]φ(z)_
_∂zj∂zi_


_∥∇[2]φ(z)∥[2]_ _≤_


_i=1_


_j=1_


1

( _yi[2][)(]_ _yk)[2]_ 2( _yi[3][)(]_ _yk) + (_

_≤_ ([P][c]k=1 _[y][k][)][4]_ " _i=1_ _k=1_ _−_ _i=1_ _k=1_

X X X X

_i=1_ _[y]i[2][)(][P]k[c]_ =1 _[y][k][)][2]_ _k=1_ _[y][k][)][4]_
_≤_ [(][P][c] ([P][c]k=1 _[y][k][)][4]_ _≤_ [(]([P][P][c][c]k=1 _[y][k][)][4][ = 1][,]_

where the last inequality holds since


_yi[2][)(]_ _yk[2][)]_
_i=1_ _k=1_

X X


( _yi[2][)(]_ _yk[2][)][ ≤]_ [(] _yi[3][)(]_ _yk)_ ( _yk[2][)][ ≤]_ ( _yi[3][)(]_ _yk),_

_⇔_ v

_i=1_ _k=1_ _i=1_ _k=1_ _k=1_ u _i=1_ _k=1_

X X X X X u X X

t

which follows by the application of Holder inequality (35) with p = 2, q = 2, ak = yk[3][/][2], and
_bk = yk[1][/][2]_ (Note that yk 0, k [c]). Hence, _φ(z)_ _Lφ with Lφ = 1 which is equivalent to_
_Lφ-smoothness of φ._ _≥_ _∈_ _∥∇[2]_ _∥≤_

PROOF OF LEMMA 6


_Proof. Since k(·; i) are twice continuously differentiable for all i ∈_ [n], we have the following
Taylor approximation for each component outputs kj( ; i) where j [c] and i [n]:

_·_ _∈_ _∈_

_kj(w[(][t][+1)]; i) = kj(w[(][t][)]_ _η[(][t][)]v[(][t][)]; i)_
_−_

= kj(w[(][t][)]; i) − **Jwkj(w; i)|w=w(t)** _η[(][t][)]v[(][t][)]_ + 2[1] [(][η][(][t][)][v][(][t][)][)][⊤][M][i,j][( ˜]w[(][t][)])(η[(][t][)]v[(][t][)]),

(36)

where Mi,j( ˜w[(][t][)]) is the Hessian matrices of kj( ; i)at ˜w[(][t][)] and ˜w[(][t][)] = αw[(][t][)] + (1 _α)w[(][t][+1)]_ for

_·_ _−_
some α ∈ [0, 1].

Shifting this back to the original function hj( ; i) we have:

_·_

_hj(w[(][t][+1)]; i) = kj(w[(][t][+1)]; i) + (hj(w[(][t][+1)]; i)_ _kj(w[(][t][+1)]; i))_
_−_

(36)
= kj(w[(][t][)]; i) − **Jwkj(w; i)|w=w(t)** _η[(][t][)]v[(][t][)]_ + 2[1] [(][η][(][t][)][v][(][t][)][)][⊤][M][i,j][( ˜]w[(][t][)])(η[(][t][)]v[(][t][)])

+ (hj(w[(][t][+1)]; i) _kj(w[(][t][+1)]; i)),_
_−_

= hj(w[(][t][)]; i) − **Jwkj(w; i)|w=w(t)** _η[(][t][)]v[(][t][)]_ + 2[1] [(][η][(][t][)][v][(][t][)][)][⊤][M][i,j][( ˜]w[(][t][)])(η[(][t][)]v[(][t][)])


+ (hj(w[(][t][+1)]; i) _kj(w[(][t][+1)]; i)) + (kj(w[(][t][)]; i)_ _hj(w[(][t][)]; i)),_
_−_ _−_

which leads to our desired statement:

_h(w[(][t][+1)]; i) = h(w[(][t][)]_ _−_ _η[(][t][)]v[(][t][)]; i) = h(w[(][t][)]; i) −_ _η[(][t][)]Hi[(][t][)][v][(][t][)][ +][ ϵ][(]i[t][)][,]_


-----

where


_ϵ[(]i,j[t][)]_ [= 1] _w[(][t][)])(η[(][t][)]v[(][t][)])_

2 [(][η][(][t][)][v][(][t][)][)][⊤][M][i,j][( ˜]

+ (hj(w[(][t][+1)]; i) _kj(w[(][t][+1)]; i)) + (kj(w[(][t][)]; i)_ _hj(w[(][t][)]; i)), j_ [c],
_−_ _−_ _∈_

Hence we get the final bound:


_ϵ[(]i,j[t][)][| ≤]_ [1] (η[(][t][)]v[(][t][)])[⊤]Mi,j( ˜w[(][t][)])(η[(][t][)]v[(][t][)])
_|_ 2

+ _hj(w[(][t][+1)]; i)_ _kj(w[(][t][+1)]; i)_ + _kj(w[(][t][)]; i)_ _hj(w[(][t][)]; i)_
_|_ _−_ _|_ _|_ _−_ _|_

(26)

(η[(][t][)]v[(][t][)])[⊤]Mi,j( ˜w[(][t][)])(η[(][t][)]v[(][t][)]) + 2ε,

_≤_ 2[1]

_w[(][t][)])_ + 2ε

_≤_ [1]2 [(][η][(][t][)][)][2][∥][v][(][t][)][∥][2][ · ∥][M][i,j][( ˜] _∥_


(11)
_≤_ 2[1] [(][η][(][t][)][)][2][∥][v][(][t][)][∥][2][G][ + 2][ε, j][ ∈] [[][c][]][.]

PROOF OF COROLLARY 1

_Proof. The proof of this corollary follows directly by the applications of Lemmas 3 and 4._

E TECHNICAL PROOFS FOR THEOREM 1

**Lemma 8. Suppose that Assumption 2 holds for G > 0 and Assumption 3 holds for V > 0, and**
_v[(][t][)]_ = v[(][t][)]reg[. Consider][ η][(][t][)][ =][ D][√][ε][ for some][ D >][ 0][ and][ ε >][ 0][. For][ i][ ∈] [[][n][]][ and][ 0][ ≤] _[t < T]_ _[, we]_
_∗_
_have_

_∥ϵ[(]i[t][)][∥][2][ ≤]_ 4[1] _[c][(4 + (][V][ + 2)][GD][2][)][2][ε][2][.]_ (37)

_Proof. From (14), for i ∈_ [n], j ∈ [c], and for 0 ≤ _t < T_, by Lemma 1 and Lemma 6 we have

_ϵ[(]i,j[t][)][| ≤]_ [1]
_|_ 2 [(][η][(][t][)][)][2][∥][v][(][t][)][∥][2][G][ + 2][ε][ ≤] [1]2 [(][V][ + 2)][GD][2][ε][ + 2][ε][ = 1]2 _[ε][(4 + (][V][ + 2)][GD][2][)][,]_

where the last inequality follows by the fact _v[(][t][)]_ = _v[(][t][)]reg[∥][2][ ≤]_ [2 +][ V][ of Lemma 2 and][ η][(][t][)][ =]
_∥_ _∥[2]_ _∥_ _∗_
_D[√]ε. Hence,_


_c_

_ϵ[(]i,j[t][)][|][2][ ≤]_ [1]
_|_ 4 _[c][(4 + (][V][ + 2)][GD][2][)][2][ε][2][.]_
_j=1_

X


_ϵ[(]i[t][)]_
_∥_ _[∥][2][ =]_


**Lemma 9. Let w[(][t][)]** _be generated by Algorithm 1 where we use the closed form solution for the_
_search direction. We execute Algorithm 1 for T =_ _βε_ _[outer loops for some constant][ β >][ 0][. We]_

_assume Assumption 1 holds. Suppose that Assumption 2 holds for G > 0 and Assumption 3 holds_
_for V > 0. We set the step size equal to η[(][t][)]_ = D[√]ε for some D > 0 and choose a learning rate
_αi[(][t][)]_ _≤_ _Lαφ_ _[, for some][ α][ ∈]_ [(0][,][ 1]3 [)][. For][ i][ ∈] [[][n][]][ and][ 0][ ≤] _[t < T]_ _[, we have]_

_∥h(w[(][t][+1)]; i) −_ _h[∗]i_ _[∥][2][ ≤]_ [(1 +][ ε][)][∥][h][(][w][(][t][)][;][ i][)][ −] _[h]i[∗][∥][2][ −]_ [2(1][ −] [3][α][)][α]i[(][t][)][[][φ][i][(][h][(][w][(][t][)][;][ i][))][ −] _[φ][i][(][h]i[∗][)]]_

+ [(3][ε][ + 2)] _c(4 + (V + 2)GD[2])[2]_ _ε_

4 _·_

+ [3][ε][ + 2]ε _∥η[(][t][)]Hi[(][t][)][v]∗[(][t][)]reg_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_ (38)


-----

_Proof. Note that we have the optimal solution v[(][t][)]reg_ [for the optimization problem (15) for][ 0][ ≤] _[t <]_
_∗_
_T_ . From (12), we have, for i ∈ [n],

_h(w[(][t][+1)]; i) = h(w[(][t][)]_ _η[(][t][)]v[(][t][)]reg[;][ i][)]_
_−_ _∗_

= h(w[(][t][)]; i) − _η[(][t][)]Hi[(][t][)][v]∗[(][t][)]reg_ [+][ ϵ][(]i[t][)]
= h(w[(][t][)]; i) − _αi[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][)) +][ ϵ][(]i[t][)]_ _−_ [η[(][t][)]Hi[(][t][)][v]∗[(][t][)]reg _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))]][.]_

Hence, we have


_h(w[(][t][+1)]; i)_ _h[∗]i_
_∥_ _−_ _[∥][2]_

= ∥h(w[(][t][)]; i) − _h[∗]i_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][)) +][ ϵ][(]i[t][)]_ _−_ [η[(][t][)]Hi[(][t][)][v]∗[(][t][)]reg _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))]][∥][2]_

= ∥h(w[(][t][)]; i) − _h[∗]i_ _[∥][2][ + (][α]i[(][t][)][)][2][∥∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_

+ ∥ϵ[(]i[t][)][∥][2][ +][ ∥][η][(][t][)][H]i[(][t][)][v]∗[(][t][)]reg _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_

_−_ 2 · ⟨h(w[(][t][)]; i) − _h[∗]i_ _[, α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][⟩]_

+ 2 · ⟨h(w[(][t][)]; i) − _h[∗]i_ _[, ϵ]i[(][t][)][⟩]_

_−_ 2 · ⟨h(w[(][t][)]; i) − _h[∗]i_ _[, η][(][t][)][H]i[(][t][)][v]∗[(][t][)]reg_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][⟩]_

2 _αi[(][t][)]_ _i_
_−_ _· ⟨_ _[∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][, ϵ][(][t][)][⟩]_

+ 2 · ⟨αi[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][, η][(][t][)][H]i[(][t][)][v]∗[(][t][)]reg _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][⟩]_

_−_ 2 · ⟨ϵ[(]i[t][)][, η][(][t][)][H]i[(][t][)][v]∗[(][t][)]reg _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][⟩][,]_

where we expand the square term. Now applying Young’s inequalities: 2|⟨u, v⟩| ≤ _[∥]ε/[u][∥]2[2]_ [+(][ε/][2)][∥][v][∥][2]

for ε > 0 and 2|⟨u, v⟩| ≤∥u∥[2] + ∥v∥[2] we have:

_h(w[(][t][+1)]; i)_ _h[∗]i_
_∥_ _−_ _[∥][2]_

= ∥h(w[(][t][)]; i) − _h[∗]i_ _[∥][2][ + (][α]i[(][t][)][)][2][∥∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_

+ ∥ϵ[(]i[t][)][∥][2][ +][ ∥][η][(][t][)][H]i[(][t][)][v]∗[(][t][)]reg _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_

_−_ 2αi[(][t][)][⟨][h][(][w][(][t][)][;][ i][)][ −] _[h]i[∗][,][ ∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][⟩]_

+ 2[ε] _[∥][h][(][w][(][t][)][;][ i][)][ −]_ _[h]i[∗][∥][2][ + 2]ε_ _[∥][ϵ][(]i[t][)][∥][2]_

+ 2[ε] _[∥][h][(][w][(][t][)][;][ i][)][ −]_ _[h]i[∗][∥][2][ + 2]ε_ _[∥][η][(][t][)][H]i[(][t][)][v]∗[(][t][)]reg_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_

+ 2(αi[(][t][)][)][2][∥∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2][ + 2][∥][ϵ][(]i[t][)][∥][2]

(22)+ 2∥η[(][t][)]Hi[(][t][)][v]∗[(][t][)]reg _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_
_≤_ (1 + ε)∥h(w[(][t][)]; i) − _h[∗]i_ _[∥][2][ + 3(][α]i[(][t][)][)][2][∥∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_

+ 3 + [2]ε _∥ϵ[(]i[t][)][∥][2][ +]_ 3 + [2]ε _∥η[(][t][)]Hi[(][t][)][v]∗[(][t][)]reg_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_
   

_−_ 2αi[(][t][)][[][φ][i][(][h][(][w][(][t][)][;][ i][))][ −] _[φ][i][(][h]i[∗][)]]_


and using the fact thatNote that from (23) we get that αi[(][t][)] _≤_ _L ∥∇αφ_ [, for some]zφi(h(w[ α][(][t][)][ ∈]; i))[(0]∥[,][2][ 1]3≤[)][, we are able to derive:]2Lφ[φi(h(w[(][t][)]; i)) _−_ _φi(h[∗]i_ [)]][. Applying this]

_h(w[(][t][+1)]; i)_ _h[∗]i_
_∥_ _−_ _[∥][2]_

_≤_ (1 + ε)∥h(w[(][t][)]; i) − _h[∗]i_ _[∥][2][ −]_ [2(1][ −] [3][α][)][α]i[(][t][)][[][φ][i][(][h][(][w][(][t][)][;][ i][))][ −] _[φ][i][(][h]i[∗][)]]_

+ [3][ε][ + 2]ε _∥ϵ[(]i[t][)][∥][2][ + 3][ε][ + 2]ε_ _∥η[(][t][)]Hi[(][t][)][v]∗[(][t][)]reg_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_

_≤_ (1 + ε)∥h(w[(][t][)]; i) − _h[∗]i_ _[∥][2][ −]_ [2(1][ −] [3][α][)][α]i[(][t][)][[][φ][i][(][h][(][w][(][t][)][;][ i][))][ −] _[φ][i][(][h]i[∗][)]]_


-----

+ [3][ε][ + 2]


1
4 _[c][(4 + (][V][ + 2)][GD][2][)][2][ε][2][ + 3][ε][ + 2]ε_


_∥η[(][t][)]Hi[(][t][)][v]∗[(][t][)]reg_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_


where the last inequality follows by Lemma 8.

**Lemma 10. Let w[(][t][)]** _be generated by Algorithm 1 where we use the closed form solution for the_
_search direction. We execute Algorithm 1 for T =_ _βε_ _[outer loops for some constant][ β >][ 0][. We]_

_assume Assumption 1 holds. Suppose that Assumption 2 holds for G > 0 and Assumption 3 holds_
_for V > 0. We set the step size equal to η[(][t][)]_ = D[√]ε for some D > 0 and choose a learning rate
_αi[(][t][)]_ = (1 + ε)αi[(][t][−][1)] = (1 + ε)[t]αi[(0)][. Based on][ β][, we define][ α]i[(0)] = _e[β]αLφ_ _[with][ α][ ∈]_ [(0][,][ 1]3 [)][.]


_We have_

1 _T −1_

_T_

_t=0_

X


_n_ _n_

[f (w[(][t][)]; i) _φi(h[∗]i_ [)]][ ≤] _[e][β][L][φ][(1 +][ ε][)]_ _h(w[(0)]; i)_ _h[∗]i_

Xi=1 _−_ 2(1 − 3α)αβ _[·][ 1]n_ Xi=1 _∥_ _−_ _[∥][2][ ·][ ε]_

_e[β]Lφ_
+ _c(4 + (V + 2)GD[2])[2]_ + 8 + 4V _ε._

8α(1 3α) [(3][ε][ + 2)] _·_
_−_
 (39)


_Proof. Rearranging the terms in Lemma 9, we have_


(1 + ε)

_h(w[(][t][)]; i)_ _h[∗]i_
_αi[(][t][)]_ _∥_ _−_ _[∥][2][ −]_


_φi(h(w[(][t][)]; i)) −_ _φi(h[∗]i_ [)][ ≤]


1

_h(w[(][t][+1)]; i)_ _h[∗]i_
_αi[(][t][)]_ _∥_ _−_ _[∥][2]_


2(1 − 3α)


1

_αi[(][t][)]_ _· ε(3ε + 2)c(4 + (V + 2)GD[2])[2]_


8(1 − 3α) _[·]_

1

2(1 − 3α) _[·]_


1 [3][ε][ + 2]

_αi[(][t][)]_ _·_ _ε_


_∥η[(][t][)]Hi[(][t][)][v]∗[(][t][)]reg_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_


1 (1 + ε)

2(1 − 3α) _αi[(][t][)]_ _∥h(w[(][t][)]; i) −_ _h[∗]i_ _[∥][2][ −]_ [(1 +]αi[(][t][+1)][ ε][)] _∥h(w[(][t][+1)]; i) −_ _h[∗]i_ _[∥][2]_

_e[β]Lφ_
+

8α(1 − 3α) _[·][ ε][(3][ε][ + 2)][c][(4 + (][V][ + 2)][GD][2][)][2]_

_e[β]Lφ_
+ 2α(1 3α) _ε_ _∥η[(][t][)]Hi[(][t][)][v]∗[(][t][)]reg_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2][.]_

_−_ _[·][ 3][ε][ + 2]_


The last inequality follows because the learning rate satisfies αi[(0)]

1, . . ., T = _[β]ε_ [for some][ β >][ 0]


_α_

_Lφ_ [and for][ t][ =]


_α_

_e[β]_ _Lφ_

_[≤]_


_α_

_e[β]Lφ_ _≤_ _L[α]φ_


_αi[(][t][)]_ = (1 + ε)αi[(][t][−][1)] = (1 + ε)[t]αi[(0)] (1 + ε)[T] _αi[(0)]_ = (1 + ε)[β/ε]
_≤_


since (1 + x)[1][/x] _≤_ _e, x > 0. Moreover, we have_


_α[(0)]i1_ = _[e][β]α[L][φ]_ [,][ t][ = 0][, . . ., T][ −] [1][.]


_α[(]i[t][)]_


Taking the average sum from t = 0, . . ., T − 1, we have

1 _T −1_ 1

[φi(h(w[(][t][)]; i)) _φi(h[∗]i_ [)]][ ≤] _h(w[(0)]; i)_ _h[∗]i_

_T_ Xt=0 _−_ 2(1 − 3α)T _[·][ (1 +]αi[(0)][ ε][)]_ _∥_ _−_ _[∥][2]_

_e[β]Lφ_
+

8α(1 − 3α) _[·][ ε][(3][ε][ + 2)][c][(4 + (][V][ + 2)][GD][2][)][2]_

_e[β]Lφ_ 1 _T −1_
+ 2α(1 3α) _ε_ _T_ _t=0_ _∥η[(][t][)]Hi[(][t][)][v]∗[(][t][)]reg_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_

_−_ _[·][ 3][ε][ + 2]_ X

= 2(1[e][β][L][φ][(1 +]3α)αβ [ ε][)] _[ε][ · ∥][h][(][w][(0)][;][ i][)][ −]_ _[h]i[∗][∥][2]_

_−_


-----

_e[β]Lφ_
+

8α(1 − 3α) _[·][ ε][(3][ε][ + 2)][c][(4 + (][V][ + 2)][GD][2][)][2]_

_e[β]Lφ_ 1 _T −1_
+ 2α(1 3α) _ε_ _T_ _t=0_ _∥η[(][t][)]Hi[(][t][)][v]∗[(][t][)]reg_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2][.]_

_−_ _[·][ 3][ε][ + 2]_ X

Taking the average sum from i = 1, . . ., n, we have


_T −1_

_t=0_

X



[φi(h(w[(][t][)]; i)) − _φi(h[∗]i_ [)]]
_i=1_

X


_n_

_≤_ 2(1[e][β][L] −[φ][(1 +]3α)αβ [ ε][)] _[ε][ ·][ 1]n_ _i=1_ _∥h(w[(0)]; i) −_ _h[∗]i_ _[∥][2]_

X

_e[β]Lφ_
+

8α(1 − 3α) _[·][ ε][(3][ε][ + 2)][c][(4 + (][V][ + 2)][GD][2][)][2]_

_e[β]Lφ_ 1 _T −1_ 1 _n_
+ 2α(1 − 3α) _[·][ 3][ε][ + 2]ε_ _T_ Xt=0 _n_ Xi=1 _∥η[(][t][)]Hi[(][t][)][v]∗[(][t][)]reg_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_


_n_

(17) _e[β]Lφ(1 + ε)_
_≤_ 2(1 − 3α)αβ [ε][ ·][ 1]n _i=1_ _∥h(w[(0)]; i) −_ _h[∗]i_ _[∥][2]_

X

_e[β]Lφ_
+

8α(1 − 3α) _[·][ ε][(3][ε][ + 2)][c][(4 + (][V][ + 2)][GD][2][)][2]_

_e[β]Lφ_
+ (2 + V )ε[2]. (40)

2α(1 − 3α) _[·][ 3][ε][ + 2]ε_

Note that


_T −1_

_t=0_

X


_T −1_

_t=0_

X


_n_

[φi(h(w[(][t][)]; i)) _φi(h[∗]i_ [)] = 1]
_−_ _T_
_i=1_

X



[f (w[(][t][)]; i) − _φi(h[∗]i_ [)]][.] (41)
_i=1_

X


Therefore, applying (41) to (40), we have


_T −1_

_t=0_

X



[f (w[(][t][)]; i) − _φi(h[∗]i_ [)]]
_i=1_

X


_n_

_≤_ 2(1[e][β][L] −[φ][(1 +]3α)αβ[ ε][)] _[·][ 1]n_ Xi=1 _∥h(w[(0)]; i) −_ _h[∗]i_ _[∥][2][ ·][ ε]_

_e[β]Lφ_
+ _c(4 + (V + 2)GD[2])[2]_ + 8 + 4V _ε._

8α(1 3α) [(3][ε][ + 2)] _·_
_−_
 

which is our desired result.


PROOF OF THEOREM 1

_Proof. We have_

_F_ = min
_∗_ _w∈R[d][ F]_ [(][w][) = min]w∈R[d]


= [1]

_n_ _w[min]∈R[d]_


_fi(w)_
_i=1_

X


_fi(w)_
_i=1_

X


_n_ _n_ _n_

_≥_ _n[1]_ _i=1_ _wmin∈R[d][ (][f][i][(][w][)) = 1]n_ _i=1_ _fi[∗]_ _[≥]_ _n[1]_ _i=1_ _φi(h[∗]i_ [)][.] (42)

X X X

Hence F∗ _−_ _n[1]_ _ni=1_ _[φ][i][(][h]i[∗][)][ ≥]_ [0][. Therefore]

_T_ _T_ _n_ _n_

1 P 1

[F (w[(][t][)]) _F_ ] = [1] [f (w[(][t][)]; i) _φi(h[∗]i_ [)]][ −] _F_ _φi(h[∗]i_ [)]

_T_ _t=1_ _−_ _∗_ _T_ _t=1_ _n_ _i=1_ _−_ " _∗_ _−_ _n[1]_ _i=1_ #!

X X X X


-----

_≤_ _T[1]_



[f (w[(][t][)]; i) − _φi(h[∗]i_ [)]]
_i=1_

X


_t=1_


_n_

(39) _e[β]Lφ(1 + ε)_
_≤_ 2(1 − 3α)αβ _[·][ 1]n_ Xi=1 _∥h(w[(0)]; i) −_ _h[∗]i_ _[∥][2][ ·][ ε]_

+ _[e][β][L][φ][(3][ε][ + 2)]_ _c(4 + (V + 2)GD[2])[2]_ + 8 + 4V _ε._

8α(1 3α) _·_
_−_
 

F TECHNICAL PROOFS FOR THEOREM 2

**Lemma 11. For 0 ≤** _t < T_ _, suppose that Assumption 3 holds for V ≥_ 0 and v[(][t][)] _satisfies (19)._
_Then_

_∥v[(][t][)]∥[2]_ _≤_ 2(ε[2] + V + 2).

_Proof. From_ _v[(][t][)]_ _v[(][t][)]reg[∥≤]_ _[ε][. Using][ ∥][a][∥][2][ ≤]_ [2][∥][a][ −] _[b][∥][2][ + 2][∥][b][∥][2][, we have]_
_∥_ _−_ _∗_

_v[(][t][)]_ 2 _v[(][t][)]_ _v[(][t][)]reg[∥][2][ + 2][∥][v][(][t][)]reg[∥][2 (19)]_ 2ε[2] + 4 + 2V.
_∥_ _∥[2]_ _≤_ _∥_ _−_ _∗_ _∗_ _≤_

where the last inequality follows since _v[(][t][)]reg[∥][2][ ≤]_ [2 +][ V][ for some][ V >][ 0][ in Lemma 2.]
_∥_ _∗_

**Lemma 12. Suppose that Assumption 2 holds for G > 0 and Assumption 3 holds for V > 0.**
_Consider η[(][t][)]_ = D[√]ε for some D > 0 and ε > 0. For i ∈ [n] and 0 ≤ _t < T_ _, we have_

_ϵ[(]i[t][)]_ (43)
_∥_ _[∥][2][ ≤]_ _[c][(2 + (][V][ +][ ε][2][ + 2)][GD][2][)][2][ε][2][.]_

_Proof. From (14), for i ∈_ [n], j ∈ [c], and for 0 ≤ _t < T_, by Lemma 1 and Lemma 6 we have


_ϵi,j[(][t][)][| ≤]_ [1]
_|_ 2 [(][η][(][t][)][)][2][∥][v][(][t][)][∥][2][G][ + 2][ε][ ≤] [1]2 [2(][ε][2][ +][ V][ + 2)][GD][2][ε][ + 2][ε][ =][ ε][(2 + (][V][ +][ ε][2][ + 2)][GD][2][)][,]

where the last inequality follows by the application of Lemma 11 and η[(][t][)] = D[√]ε. Hence,


_ϵ[(]i[t][)]_
_∥_ _[∥][2][ =]_


_|ϵ[(]i,j[t][)][|][2][ ≤]_ _[c][(2 + (][V][ +][ ε][2][ + 2)][GD][2][)][2][ε][2][.]_
_j=1_

X


**Lemma 13. Let w[(][t][)]** _be generated by Algorithm 2 where v[(][t][)]_ _satisfies (19). We execute Algorithm_
_2 for T =_ _[β]ε_ _[outer loops for some constant][ β >][ 0][. We assume Assumption 1 holds. Suppose that]_

_Assumption 2 holds for G > 0, Assumption 3 holds for V > 0 and Assumption 4 holds for H > 0._
_We set the step size equal to η[(][t][)]_ = D[√]ε for some D > 0 and choose a learning rate αi[(][t][)] _≤_ _Lαφ_ _[,]_

_for some α ∈_ (0, [1]4 [)][. For][ i][ ∈] [[][n][]][ and][ 0][ ≤] _[t < T]_ _[, we have]_

_∥h(w[(][t][+1)]; i) −_ _h[∗]i_ _[∥][2][ ≤]_ [(1 +][ ε][)][∥][h][(][w][(][t][)][;][ i][)][ −] _[h]i[∗][∥][2][ −]_ [2(1][ −] [4][α][)][α]i[(][t][)][[][φ][i][(][h][(][w][(][t][)][;][ i][))][ −] _[φ][i][(][h]i[∗][)]]_

++ ε[4](4[ε][ + 3]εε + 3)∥η[(][t]D[)]H[2]iH[(][t][)][2][v]∗[(]+[t][)]reg c(2 + ([−] _[α]i[(][t]V[)][∇] +[z][φ] ε[i][2][(][h]+ 2)[(][w][(][t]GD[)][;][ i][))][2])[∥][2][2][]_ (44)


-----

_Proof. Note that v[(][t][)]_ is obtained from the optimization problem (15) for 0 ≤ _t < T_ . From (9), we
have, for i ∈ [n],

_h(w[(][t][+1)]; i) = h(w[(][t][)]_ _−_ _η[(][t][)]v[(][t][)]; i)_

= h(w[(][t][)]; i) − _η[(][t][)]Hi[(][t][)][v][(][t][)][ +][ ϵ][(]i[t][)]_
= h(w[(][t][)]; i) − _η[(][t][)]Hi[(][t][)][(][v][(][t][)][ −]_ _[v]∗[(][t][)]reg[)][ −]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][)) +][ ϵ][(]i[t][)]_
_−_ [η[(][t][)]Hi[(][t][)][v]∗[(][t][)]reg _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))]][.]_

Hence, we have


_h(w[(][t][+1)]; i)_ _h[∗]i_
_∥_ _−_ _[∥][2]_

= ∥h(w[(][t][)]; i) − _h[∗]i_ _[−]_ _[η][(][t][)][H]i[(][t][)][(][v][(][t][)][ −]_ _[v]∗[(][t][)]reg[)][ −]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))]_

+ ϵ[(]i[t][)] _−_ [η[(][t][)]Hi[(][t][)][v]∗[(][t][)]reg _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))]][∥][2]_

= ∥h(w[(][t][)]; i) − _h[∗]i_ _[∥][2][ +][ ∥][η][(][t][)][H]i[(][t][)][(][v][(][t][)][ −]_ _[v]∗[(][t][)]reg[)][∥][2][ + (][α]i[(][t][)][)][2][∥∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_

+ ∥ϵ[(]i[t][)][∥][2][ +][ ∥][η][(][t][)][H]i[(][t][)][v]∗[(][t][)]reg _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_

_−_ 2 · ⟨h(w[(][t][)]; i) − _h[∗]i_ _[, η][(][t][)][H]i[(][t][)][(][v][(][t][)][ −]_ _[v]∗[(][t][)]reg[)][⟩]_

_−_ 2 · ⟨h(w[(][t][)]; i) − _h[∗]i_ _[, α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][⟩]_

+ 2 · ⟨h(w[(][t][)]; i) − _h[∗]i_ _[, ϵ]i[(][t][)][⟩]_

_−_ 2 · ⟨h(w[(][t][)]; i) − _h[∗]i_ _[, η][(][t][)][H]i[(][t][)][v]∗[(][t][)]reg_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][⟩]_

+ 2 · ⟨η[(][t][)]Hi[(][t][)][(][v][(][t][)][ −] _[v]∗[(][t][)]reg[)][, α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][⟩]_

_−_ 2 · ⟨η[(][t][)]Hi[(][t][)][(][v][(][t][)][ −] _[v]∗[(][t][)]reg[)][, ϵ][(]i[t][)][⟩]_

+ 2 · ⟨η[(][t][)]Hi[(][t][)][(][v][(][t][)][ −] _[v]∗[(][t][)]reg[)][, η][(][t][)][H]i[(][t][)][v]∗[(][t][)]reg_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][⟩]_

2 _αi[(][t][)]_ _i_
_−_ _· ⟨_ _[∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][, ϵ][(][t][)][⟩]_

+ 2 · ⟨αi[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][, η][(][t][)][H]i[(][t][)][v]∗[(][t][)]reg _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][⟩]_

_−_ 2 · ⟨ϵ[(]i[t][)][, η][(][t][)][H]i[(][t][)][v]∗[(][t][)]reg _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][⟩][,]_

where we expand the square term. Now applying Young’s inequalities: 2|⟨u, v⟩| ≤ _[∥]ε/[u][∥]3[2]_ [+(][ε/][3)][∥][v][∥][2]

for ε > 0 and 2|⟨u, v⟩| ≤∥u∥[2] + ∥v∥[2] we have:

_h(w[(][t][+1)]; i)_ _h[∗]i_
_∥_ _−_ _[∥][2]_

= ∥h(w[(][t][)]; i) − _h[∗]i_ _[∥][2][ +][ ∥][η][(][t][)][H]i[(][t][)][(][v][(][t][)][ −]_ _[v]∗[(][t][)]reg[)][∥][2][ + (][α]i[(][t][)][)][2][∥∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_

+ ∥ϵ[(]i[t][)][∥][2][ +][ ∥][η][(][t][)][H]i[(][t][)][v]∗[(][t][)]reg _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_

+ 3[ε] _[∥][h][(][w][(][t][)][;][ i][)][ −]_ _[h]i[∗][∥][2][ + 3]ε_ _[∥][η][(][t][)][H]i[(][t][)][(][v][(][t][)][ −]_ _[v]∗[(][t][)]reg[)][∥][2]_

_−_ 2αi[(][t][)][⟨][h][(][w][(][t][)][;][ i][)][ −] _[h]i[∗][,][ ∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][⟩]_

+ 3[ε] _[∥][h][(][w][(][t][)][;][ i][)][ −]_ _[h]i[∗][∥][2][ + 3]ε_ _[∥][ϵ][(]i[t][)][∥][2]_

+ 3[ε] _[∥][h][(][w][(][t][)][;][ i][)][ −]_ _[h]i[∗][∥][2][ + 3]ε_ _[∥][η][(][t][)][H]i[(][t][)][v]∗[(][t][)]reg_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_

+ 3(η[(][t][)])[2]∥Hi[(][t][)][(][v][(][t][)][ −] _[v]∗[(][t][)]reg[)][∥][2][ + 3(][α]i[(][t][)][)][2][∥∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2][ + 3][∥][ϵ][(]i[t][)][∥][2]_

(22)+ 3∥η[(][t][)]Hi[(][t][)][v]∗[(][t][)]reg _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_
_≤_ (1 + ε)∥h(w[(][t][)]; i) − _h[∗]i_ _[∥][2][ + 4(][α]i[(][t][)][)][2][∥∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_

+ 4 + [3]ε _∥η[(][t][)]Hi[(][t][)][(][v][(][t][)][ −]_ _[v]∗[(][t][)]reg[)][∥][2][ +]_ 4 + [3]ε _∥ϵ[(]i[t][)][∥][2]_
   


-----

4 + [3]


_∥η[(][t][)]Hi[(][t][)][v]∗[(][t][)]reg_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_


_−_ 2αi[(][t][)][[][φ][i][(][h][(][w][(][t][)][;][ i][))][ −] _[φ][i][(][h]i[∗][)]]_

and using the fact thatNote that from (23) we get that αi[(][t][)] _≤_ _L ∥∇αφ_ [, for some]zφi(h(w[ α][(][t][)][ ∈]; i))[(0]∥[,][2][ 1]4≤[)][, we are able to derive:]2Lφ[φi(h(w[(][t][)]; i)) _−_ _φi(h[∗]i_ [)]][. Applying this]

_h(w[(][t][+1)]; i)_ _h[∗]i_
_∥_ _−_ _[∥][2]_

_≤_ (1 + ε)∥h(w[(][t][)]; i) − _h[∗]i_ _[∥][2][ −]_ [2(1][ −] [4][α][)][α]i[(][t][)][[][φ][i][(][h][(][w][(][t][)][;][ i][))][ −] _[φ][i][(][h]i[∗][)]]_

+ [4][ε][ + 3]ε _∥η[(][t][)]Hi[(][t][)][(][v][(][t][)][ −]_ _[v]∗[(][t][)]reg[)][∥][2][ + 4][ε][ + 3]ε_ _∥ϵ[(]i[t][)][∥][2]_

+ [4][ε][ + 3]ε _∥η[(][t][)]Hi[(][t][)][v]∗[(][t][)]reg_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_

(a)
_≤_ (1 + ε)∥h(w[(][t][)]; i) − _h[∗]i_ _[∥][2][ −]_ [2(1][ −] [4][α][)][α]i[(][t][)][[][φ][i][(][h][(][w][(][t][)][;][ i][))][ −] _[φ][i][(][h]i[∗][)]]_

+ [4][ε][ + 3]ε _D[2]ε_ _[H]ε[2]_ _[∥][v][(][t][)][ −]_ _[v]∗[(][t][)]reg[∥][2][ + 4][ε][ + 3]ε_ _∥ϵ[(]i[t][)][∥][2]_

+ [4][ε][ + 3]ε _∥η[(][t][)]Hi[(][t][)][v]∗[(][t][)]reg_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_

(b)
_≤_ (1 + ε)∥h(w[(][t][)]; i) − _h[∗]i_ _[∥][2][ −]_ [2(1][ −] [4][α][)][α]i[(][t][)][[][φ][i][(][h][(][w][(][t][)][;][ i][))][ −] _[φ][i][(][h]i[∗][)]]_

+ [4][ε][ + 3] _D[2]H_ [2] _ε[2]_ + [4][ε][ + 3] _c(2 + (V + ε[2]_ + 2)GD[2])[2]ε[2]

_ε_ _·_ _ε_ _·_


+ [4][ε][ + 3]ε _∥η[(][t][)]Hi[(][t][)][v]∗[(][t][)]reg_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_

= (1 + ε)∥h(w[(][t][)]; i) − _h[∗]i_ _[∥][2][ −]_ [2(1][ −] [4][α][)][α]i[(][t][)][[][φ][i][(][h][(][w][(][t][)][;][ i][))][ −] _[φ][i][(][h]i[∗][)]]_

++ ε[4](4[ε][ + 3]εε + 3)∥η[(][t]D[)]H[2]iH[(][t][)][2][v]∗[(]+[t][)]reg c(2 + ([−] _[α]i[(][t]V[)][∇] +[z][φ] ε[i][2][(][h]+ 2)[(][w][(][t]GD[)][;][ i][))][2])[∥][2][2][]_


where (a) follows by using matrix vector inequality ∥Hv∥≤∥H∥∥v∥, where H ∈ R[c][×][d] and
_v ∈_ R[d] and Assumption 4 in (20) and η[(][t][)] = D[√]ε for some D > 0 and ε > 0; (b) follows by the
fact that _v[(][t][)]_ _v[(][t][)]reg[∥][2][ ≤]_ _[ε][2][ in (19) and Lemma 12.]_
_∥_ _−_ _∗_

**Lemma 14. Let w[(][t][)]** _be generated by Algorithm 2 where v[(][t][)]_ _satisfies (19). We execute Algorithm_
_2 for T =_ _βε_ _[outer loops for some constant][ β >][ 0][. We assume Assumption 1 holds. Suppose]_

_that Assumption 2 holds for G > 0, Assumption 3 holds for V > 0 and Assumption 4 holds for_
_H > 0.We set the step size equal to η[(][t][)]_ = D[√]ε for some D > 0 and choose a learning rate
_αi[(][t][)]_ = (1 + ε)αi[(][t][−][1)] = (1 + ε)[t]αi[(0)][. Based on][ β][, we define][ α]i[(0)] = _e[β]αLφ_ _[with][ α][ ∈]_ [(0][,][ 1]4 [)][.]

_We have_


_T −1_

_t=0_

X



[f (w[(][t][)]; i) − _φi(h[∗]i_ [)]]
_i=1_

X


_n_

_≤_ 2(1[e][β][L] −[φ][(1 +]4α)αβ[ ε][)] _[·][ 1]n_ Xi=1 _∥h(w[(0)]; i) −_ _h[∗]i_ _[∥][2][ ·][ ε]_

+ _[e][β][L][φ][(4][ε][ + 3)]_ _D[2]H_ [2] + c(2 + (V + ε[2] + 2)GD[2])[2] + 2 + V _ε._ (45)

2α(1 4α) _·_
_−_
 

_Proof. Rearranging the terms in Lemma 13, we have_


(1 + ε)

_h(w[(][t][)]; i)_ _h[∗]i_
_αi[(][t][)]_ _∥_ _−_ _[∥][2][ −]_


_φi(h(w[(][t][)]; i)) −_ _φi(h[∗]i_ [)][ ≤]


1

_h(w[(][t][+1)]; i)_ _h[∗]i_
_αi[(][t][)]_ _∥_ _−_ _[∥][2]_


2(1 − 4α)


-----

1

_ε(4ε + 3)_ _D[2]H_ [2] + c(2 + (V + ε[2] + 2)GD[2])[2][]
_αi[(][t][)]_ _·_


1

_αi[(][t][)]_ _·_ [4][ε][ + 3]ε _∥η[(][t][)]Hi[(][t][)][v]∗[(][t][)]reg_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_


2(1 − 4α) _[·]_

1

2(1 − 4α) _[·]_


(1 + ε)

_h(w[(][t][)]; i)_ _h[∗]i_ _h(w[(][t][+1)]; i)_ _h[∗]i_
_αi[(][t][)]_ _∥_ _−_ _[∥][2][ −]_ [(1 +]αi[(][t][+1)][ ε][)] _∥_ _−_ _[∥][2]_



_[ −]_

2(1 − 4α) _αi[(][t][)]_ _∥_ _−_ _i_ _[∥]_ _αi[(][t][+1)]_ _∥_ _−_

_e[β]Lφ_
+ _D[2]H_ [2] + c(2 + (V + ε[2] + 2)GD[2])[2][]

2α(1 − 4α) _[·][ ε][(4][ε][ + 3)]_

_e[β]Lφ_ 
+ 2α(1 4α) _ε_ _∥η[(][t][)]Hi[(][t][)][v]∗[(][t][)]reg_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2][.]_

_−_ _[·][ 4][ε][ + 3]_


(46)


The last inequality follows because the learning rate satisfies αi[(0)]

1, . . ., T = _[β]ε_ [for some][ β >][ 0]


_α_

_Lφ_ [and for][ t][ =]


_α_

_e[β]_ _Lφ_

_[≤]_


_α_

_e[β]Lφ_ _≤_ _L[α]φ_


_αi[(][t][)]_ = (1 + ε)αi[(][t][−][1)] = (1 + ε)[t]αi[(0)] (1 + ε)[T] _αi[(0)]_ = (1 + ε)[β/ε]
_≤_


since (1 + x)[1][/x] _≤_ _e, x > 0. Moreover, we have_


_α[(0)]i1_ = _[e][β]α[L][φ]_ [,][ t][ = 0][, . . ., T][ −] [1][.]


_α[(]i[t][)]_


Taking the average sum from t = 0, . . ., T − 1, we have


_T −1_ 1

[φi(h(w[(][t][)]; i)) _φi(h[∗]i_ [)]][ ≤] _h(w[(0)]; i)_ _h[∗]i_

Xt=0 _−_ 2(1 − 4α)T _[·][ (1 +]αi[(0)][ ε][)]_ _∥_ _−_ _[∥][2]_

_e[β]Lφ_
+ _D[2]H_ [2] + c(2 + (V + ε[2] + 2)GD[2])[2][]

2α(1 − 4α) _[·][ ε][(4][ε][ + 3)]_

_T_ 1

_e[β]Lφ_ 1 _−_
+ 2α(1 4α) _ε_ _T_ _t=0_ _∥η[(][t][)]Hi[(][t][)][v]∗[(][t][)]reg_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_

_−_ _[·][ 4][ε][ + 3]_ X

= 2(1[e][β][L][φ][(1 +]4α)αβ [ ε][)] _[ε][ · ∥][h][(][w][(0)][;][ i][)][ −]_ _[h]i[∗][∥][2]_

_−_

_e[β]Lφ_
+ _D[2]H_ [2] + c(2 + (V + ε[2] + 2)GD[2])[2][]

2α(1 − 4α) _[·][ ε][(4][ε][ + 3)]_

_T_ 1

_e[β]Lφ_ 1 _−_
+ 2α(1 4α) _ε_ _T_ _t=0_ _∥η[(][t][)]Hi[(][t][)][v]∗[(][t][)]reg_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2][.]_

_−_ _[·][ 4][ε][ + 3]_ X


Taking the average sum from i = 1, . . ., n, we have


_T −1_

_t=0_

X



[φi(h(w[(][t][)]; i)) − _φi(h[∗]i_ [)]]
_i=1_

X


_n_

_≤_ 2(1[e][β][L] −[φ][(1 +]4α)αβ [ ε][)] _[ε][ ·][ 1]n_ _i=1_ _∥h(w[(0)]; i) −_ _h[∗]i_ _[∥][2]_

X

_e[β]Lφ_
+ _D[2]H_ [2] + c(2 + (V + ε[2] + 2)GD[2])[2][]

2α(1 − 4α) _[·][ ε][(4][ε][ + 3)]_

_T_ 1 _n_

_e[β]Lφ_ 1 _−_ 1
+ 2α(1 − 4α) _[·][ 4][ε][ + 3]ε_ _T_ Xt=0 _n_ Xi=1 _∥η[(][t][)]Hi[(][t][)][v]∗[(][t][)]reg_ _[−]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2]_


(17) _e[β]Lφ(1 + ε)_
_≤_ 2(1 4α)αβ [ε][ ·][ 1]n

_−_


_h(w[(0)]; i)_ _h[∗]i_
_i=1_ _∥_ _−_ _[∥][2]_

X


-----

_e[β]Lφ_

_D[2]H_ [2] + c(2 + (V + ε[2] + 2)GD[2])[2][]
2α(1 − 4α) _[·][ ε][(4][ε][ + 3)]_

_e[β]Lφ_ 

(2 + V )ε[2]. (47)

2α(1 − 4α) _[·][ 4][ε][ + 3]ε_


Note that


_T −1_

_t=0_

X


_T −1_

_t=0_

X


_n_

[φi(h(w[(][t][)]; i)) _φi(h[∗]i_ [)] = 1]
_−_ _T_
_i=1_

X



[f (w[(][t][)]; i) − _φi(h[∗]i_ [)]][.] (48)
_i=1_

X


Therefore, applying (48) to (47), we have


_T −1_

_t=0_

X



[f (w[(][t][)]; i) − _φi(h[∗]i_ [)]]
_i=1_

X


_n_

_≤_ 2(1[e][β][L] −[φ][(1 +]4α)αβ[ ε][)] _[·][ 1]n_ Xi=1 _∥h(w[(0)]; i) −_ _h[∗]i_ _[∥][2][ ·][ ε]_

_e[β]Lφ_
+ _D[2]H_ [2] + c(2 + (V + ε[2] + 2)GD[2])[2] + 2 + V _ε._

2α(1 4α) [(4][ε][ + 3)] _·_
_−_
 


PROOF OF THEOREM 2

_Proof. From (42) we have F∗_ _−_ _n[1]_


_n_

_−_ _n[1]_ _i=1_ _[φ][i][(][h]i[∗][)][ ≥]_ [0][. This leads to]

_T_ _n_

P

1

[f (w[(][t][)]; i) _φi(h[∗]i_ [)]][ −]

_n_ _−_

_t=1_ _i=1_

X X

_T_ _n_

1

[f (w[(][t][)]; i) _φi(h[∗]i_ [)]]

_n_ _−_

_t=1_ _i=1_

X X


_n_

_φi(h[∗]i_ [)]
_i=1_ #!

X


_T_

[F (w[(][t][)]) _F_ ] = [1]
_−_ _∗_ _T_
_t=1_

X

(42)


_F_
_∗_ _−_ _n[1]_


_n_

(45) _e[β]Lφ(1 + ε)_
_≤_ 2(1 − 4α)αβ _[·][ 1]n_ Xi=1 _∥h(w[(0)]; i) −_ _h[∗]i_ _[∥][2][ ·][ ε]_

+ _[e][β][L][φ][(4][ε][ + 3)]_ _D[2]H_ [2] + c(2 + (V + ε[2] + 2)GD[2])[2] + 2 + V _ε._

2α(1 4α) _·_
_−_
  (49)

PROOF OF COROLLARY 2


_Proof. For each iteration 0 ≤_ _t < T_, we need to find v[(][t][)] satisfying the following criteria:

_v[(][t][)]_ _v[(][t][)]reg[∥][2][ ≤]_ _[ε][2][,]_
_∥_ _−_ _∗_

for some ε > 0. Using Gradient Descent we need O(n _[L]µ_ [log(][ 1]ε[2][ )) =][ O][(2][n][ L]µ [log(][ 1]ε [))][ number of]

gradient evaluations (Nesterov, 2004), where L and µ = ε[2] are the smooth and strongly convex
constants, respectively, of Ψ. Let

_ψi(v) = [1]_ _i_ _[v][ −]_ _[α]i[(][t][)]_ (50)

2 _[∥][η][(][t][)][H]_ [(][t][)] _[∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))][∥][2][, i][ ∈]_ [[][n][]][.]

Then, for any v ∈ R[c]

_∇vψi(v) = η[(][t][)]Hi[(][t][)]⊤[η(t)Hi[(][t][)][v][ −]_ _[α]i[(][t][)][∇][z][φ][i][(][h][(][w][(][t][)][;][ i][))]][, i][ ∈]_ [[][n][]][.] (51)


-----

Consider η[(][t][)] = D[√]ε for some D > 0 and ε > 0, we have for i ∈ [n] and 0 ≤ _t < T_

(20)
_v[ψ][i][(][v][)][∥]_ [= (][η][(][t][)][)][2][∥][H]i[(][t][)]⊤Hi[(][t][)] _i_ _i_ _D[2]H_ [2].
_∥∇[2]_ _[∥≤]_ [(][η][(][t][)][)][2][∥][H] [(][t][)][∥· ∥][H] [(][t][)][∥] _≤_

Hence, _v[Φ(][v][)][∥≤∥∇][2]v[ψ][i][(][v][)][∥]_ [+][ ε][2][ for any][ v][ ∈] [R][c][ which implies that][ L][ =][ D][2][H] [2][ +][ ε][2][ (Nes-]
terov (2004)) and ∥∇[2] _[L]µ_ [=] _D[2]Hε[2][2]+ε[2]_ . Therefore, the complexity to find v[(][t][)] for each iteration t is

_O(2n_ _[D][2][H]ε[2][2][+][ε][2]_ log( [1]ε [))][.]

Let us choose 0 < ε ≤ 1. From (49), we have

1 _T −1_ _e[β]Lφ_ _n_

_T_ Xt=0 [F (w[(][t][)]) − _F∗] ≤_ (1 − 4α)αβ _[·][ 1]n_ Xi=1 _∥h(w[(0)]; i) −_ _h[∗]i_ _[∥][2][ ·][ ε]_

7e[β]Lφ
+ _D[2]H_ [2] + c(2 + (V + 3)GD[2])[2] + 2 + V _ε = Nε,_

2α(1 4α) _·_
_−_
 

where


_N =_ (1−e[β]4Lαφ)αβ


_h(w[(0)]; i)_ _h[∗]i_ 2α7(1e[β] _L4φα)_ _D[2]H_ [2] + c(2 + (V + 3)GD[2])[2] + 2 + V
_i=1_ _∥_ _−_ _[∥][2][ +]_ _−_

X 


Let ˆε = Nε with 0 < ˆε _N_ . Then, we need T = _Nβεˆ_ for some β > 0 to guarantee

min0≤t≤T −1[F (w[(][t][)]) − _F∗ ≤] ≤_ _T1_ _tT=0 −1[[][F]_ [(][w][(][t][)][)][ −] _[F][∗][]][ ≤]_ _ε[ˆ]. Hence, the total complexity is_

_n_ _[N]εˆ[ 3][3][β][ (][D][2][H]_ [2][ + (ˆ]ε[2]/N )) log( _[N]εˆ_ [)]P.
_O_
 


-----

