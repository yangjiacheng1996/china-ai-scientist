# TOWARDS UNDERSTANDING DISTRIBUTIONAL REIN## FORCEMENT LEARNING: REGULARIZATION, OPTI- MIZATION, ACCELERATION AND SINKHORN ALGO- RITHM

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Distributional reinforcement learning (RL) is a class of state-of-the-art algorithms
that estimate the whole distribution of the total return rather than only its expectation. Despite the remarkable performance of distributional RL, a theoretical understanding of its advantages over expectation-based RL remains elusive. In this
paper, we interpret distributional RL as entropy-regularized maximum likelihood
estimation in the neural Z-fitted iteration framework, and establish the connection
of the resulting risk-aware regularization with maximum entropy RL. In addition, We shed light on the stability-promoting distributional loss with desirable
smoothness properties in distributional RL, which can yield stable optimization
and guaranteed generalization. We also analyze the acceleration behavior while
optimizing distributional RL algorithms and show that an appropriate approximation to the true target distribution can speed up the convergence. Finally, we
propose a class of Sinkhorn distributional RL algorithm that interpolates between
the Wasserstein distance and maximum mean discrepancy (MMD). Experiments
on a suite of Atari games reveal the competitive performance of our algorithm
relative to existing state-of-the-art distributional RL algorithms.

1 INTRODUCTION

The intrinsic characteristics of classical reinforcement learning (RL) algorithms, such as temporaldifference (TD) learning (Sutton & Barto, 2018) and Q-learning (Watkins & Dayan, 1992), are based
on the expectation of discounted cumulative rewards that an agent observes while interacting with
the environment. In stark contrast to the classical expectation-based RL, a new branch of algorithms
called distributional RL estimates the full distribution of total returns and has demonstrated stateof-the-art performance in a wide range of environments (Bellemare et al., 2017a; Dabney et al.,
2018b;b; Yang et al., 2019; Zhou et al., 2020; Nguyen et al., 2020). Meanwhile, distributional RL
has also enjoyed further benefits in risk-sensitive control, policy exploration settings (Mavrin et al.,
2019; Rowland et al., 2019) and robsutness (Sun et al., 2021).

Despite the existence of numerous algorithmic variants of distributional RL with remarkable empirical success, theoretical studies of advantages of distributional RL over expectation-based RL are
less established. Existing works include (Lyle et al., 2019) that proved in many tabular and linear
approximation settings, distributional RL behaves exactly the same as expectation-based RL. Lyle
et al. (2021) investigated the impact of distributional RL from the perspective of representation dynamics. Martin et al. (2020) recently mapped distributional RL problems to a Wasserstein gradient
flow problem, treating the distributional Bellman residual as a potential energy functional. Offline
distributional RL (Ma et al., 2021) has also been proposed to investigate the efficacy of distributional
RL in both risk-neutral and risk-averse domains. Recent works have tended towards closing the gap
between theory and practice in distributional RL.

From an algorithmic perspective, the Sinkhorn loss (Sinkhorn, 1967) can be used to tractably approximate the Wasserstein distance and has been successfully applied in numerous crucial machine
learning developments, including the Sinkhorn-GAN (Genevay et al., 2018) and Sinkhorn-based
adversarial training (Wong et al., 2019). Inspired by the distributional RL literature, Martin et al.


-----

(2020) argues for the use of a second-order stochastic dominance relation to select among a multiplicity of competing solutions via Sinkhorn iteration (Sinkhorn, 1967), which can be useful to
manage stochastic uncertainty in RL paradigms. However, a Sinkhorn-based distributional RL algorithm has not yet to be formally proposed and investigated.

In this paper, we theoretically illuminate the superiority of distributional RL over expectation-based
RL from the perspectives of regularization, optimization, acceleration, and representation. Specifically, we simplify distributional RL as a neural Z-fitted iteration, within which we establish an
equivalence between distributional RL and a form of entropy-regularized maximum likelihood estimation (MLE). We also demonstrate that the resulting novel cross entropy regularization correlates
strongly with the behavior of maximum entropy RL. By incorporating a histogram distributional
loss, we further achieve stable optimization and guaranteed generalization of distributional RL, attributable to desirable smoothness properties of the distribution loss. We further characterize the
effect of acceleration on distributional RL and discuss when distributional RL algorithms are effective in various environments. After gaining insights from the theoretical advantages of distributional
RL, we further propose a novel distributional RL algorithm based on Sinkhorn loss that interpolates
between the Wasserstein distance and maximum mean discrepancy (MMD). Our approach allows
us to find a trade-off that simultaneously leverages the geometry of the Wasserstein distance and the
favorable high-dimensional sample complexity and unbiased gradient estimates of MMD. In summary, our analysis opens the door to a deeper understanding of theoretical advantage of distributional
RL.

2 PRELIMINARY KNOWLEDGE

In the standard RL setting, an agent interacts with an environment via a Markov decision process (MDP), a 5-tuple (S, A, R, P, γ), where S and A are the state and action spaces, respectively.
_P is the environment transition dynamics, R is the reward function and γ ∈_ (0, 1) is the discount
factor.

**State-value function vs.** **state-value distribution.** Given a policy π, the discounted sum of
future rewards is a random variable Z _[π](s, a) =_ _t=0_ _[γ][t][R][(][s][t][, a][t][)][, where][ s][0][ =][ s][,][ a][0][ =][ a][,]_
_st+1_ _P_ ( _st, at), and at_ _π(_ _st). In the control setting, expectation-based RL focuses on the_
state-value function ∼ _·|_ _Q[π](s, a ∼), which is the expectation of·|_ _Z_ _[π](s, a), i.e., Q[π](s, a) = E [Z_ _[π](s, a)]._

[P][∞]
Distributional RL, on the other hand, focuses on the state-value distribution, the full distribution
of Z _[π](s, a). Leveraging knowledge on the entire distribution can better capture the uncertainty of_
returns in the MDP beyond the expectation of return (Dabney et al., 2018a; Mavrin et al., 2019).

**Bellman operators vs.** **distributional Bellman operators.** For the policy evaluation in
expectation-based RL, the value function is updated via the Bellman operator T _[π]Q(s, a) =_
E[R(s, a)] + γEs′∼p,π [Q (s[′], a[′])]. In distributional RL, the state-value distribution of Z _[π](s, a) is_
updated via the distributional Bellman operator T[π]

T[π]Z(s, a) = R(s, a) + γZ (s[′], a[′]), (1)

where s[′] _∼_ _P_ (·|s, a) and a[′] _∼_ _π (·|s[′])._

From a theoretical perspective, both the Bellman operator T _[π]_ in the policy evaluation setting and the
Bellman optimality operator T in the control setting are contractive in the stationary policy case. In
contrast, the distributional Bellman operator T[π] is contractive under certain distribution divergence
metrics, but the distributional Bellman optimality operator T can only converge to a set of optimal
non-stationary value distributions in a weak sense (Elie & Arthur, 2020).

3 EFFECT OF REGULARIZATION ON DISTRIBUTIONAL RL

Although the theoretical framework of distributional RL in the tabular setting has been basically
established mentioned in Section 2, the theoretical understanding of its advantages over expectationbased RL has been less studied. In this section, we attribute the superiority of distributional RL into
its regularization effect.


-----

3.1 DISTRIBUTIONAL RL: NEURAL Z-FITTED ITERATION

**Neural Q-Fitted Iteration. In the function approximation setting, Deep Q Learning (Mnih et al.,**
2015) can be simplified into Neural Q-Fitted Iteration (Fan et al., 2020) under tricks of experience
replay and the target network Qθ[∗], where we update parameterized Qθ(s, a) in each iteration k:

_n_

1 2

_Q[k]θ[+1]_ = argmin _yi_ _Q[k]θ_ [(][s][i][, a][i][)] _,_ (2)
_Qθ_ _n_ _i=1_ _−_

X  

where the target yi = r(si, ai) + γ maxa _Q[k]θ[∗]_ [(][s]i[′] _[, a][)][ is fixed within every][ T][target][ steps to up-]_
_∈A_
date target network Qθ∗ by letting θ[∗] = θ and the experience buffer induces independent samples
_{(si, ai, ri, s[′]i[)][}]i∈[n][. In an ideal case that neglects the non-convexity and TD approximation errors,]_
we have Q[k]θ[+1] = _Q[k]θ_ [, which is exactly the updating under Bellman optimality operator. Under the]
_T_
two conditions, the optimization problem in Eq. 2 can be viewed as Least Square Estimation (LSE)
in a neural network parametric regression problem between the updating of target network Qθ∗ .

**Neural Z-Fitted Iteration. Analogous to neural Q-fitted iteration, we can also simplify value-based**
distributional RL methods based on a parameterized Zθ into a Neural Z-fitted Iteration as


_Q[k]θ[+1]_ = argmin
_Qθ_


_i=1_


_Zθ[k][+1]_ = argmin
_Zθ_


_dp(Yi, Zθ[k]_ [(][s][i][, a][i][))][,] (3)
_i=1_

X


where the target Yi = R(si, ai) + γZθ[k][∗] [(][s]i[′] _[, π][Z][(][s][′][))][ with][ π][Z][(][s][′][) = argmax]a[′][ E]_ _Zθ[k][∗]_ [(][s][′][, a][′][)] is
fixed within every Ttarget steps to update target network Zθ∗, and dp is a divergence metric between
two distributions. Notably, the options of representation manner on Zθ and the metric _dp are pivotal_
for the empirical success of distributional RL algorithms. For instance, QR-DQN (Dabney et al.,
2018b) approximates Wasserstein distance Wp, which leverages quantiles to represent the distribution of Zθ. C51 (Bellemare et al., 2017a) represents Zθ via a categorical distribution under the
convergence of Cram´er distance (Bellemare et al., 2017b; Rowland et al., 2018), a special case with
_p = 2 of the ℓp distance (Elie & Arthur, 2020), while Moment Matching (Nguyen et al., 2020)_
learns deterministic samples to represent the distribution of Zθ based on Maximum Mean Discrepancy (MMD). Contractive properties under typical metrics dp can be summarized as follows

_[π]_ is γ-contractive under the supreme form of Wassertein distance Wp.

_• T_

_• T_ _[π]_ is γ[1][/p]-contractive under the supreme form of ℓp distance.

_[π]_ is γ[α/][2]-contractive under MMD with the kernel kα(x, y) = _x_ _y_ _,_ _α_ R.

_• T_ _∞_ _−∥_ _−_ _∥[α]_ _∀_ _∈_


For the completeness, the definition of mentioned distances and the proof of contraction are provided in Appendix A. Although the widely used Kullback–Leibler (KL) divergence is not a contraction (Morimura et al., 2012), we show in Proposition 1 that the KL divergence still enjoys desirable
properties in distributional RL context, which can be reasonable for the theoretical analysis. We
assume Zθ is absolutely continuous and has joint supports, under which the KL divergence is welldefined. Proof of Proposition 1 and the definition of supreme DKL are provided in Appendix B.

**Proposition 1. Denote the supreme of DKL as DKL[∞][, we have: (1)][ T][π][ is a non-expansive operator]**
_under DKL[∞][, i.e.,][ D]KL[∞][(][T][π][Z][1][,][ T][π][Z][2][)][ ≤]_ _[D]KL[∞][(][Z][1][, Z][2][)][, (2)][ D]KL[∞][(][Z][n][, Z][)][ →]_ [0][ implies][ W][p][(][Z][n][, Z][)][ →] [0][,]
_(3) the expectation of Z_ _[π]_ _is still γ-contractive, i.e., ∥ET[π]Z1 −_ ET[π]Z2∥∞ _≤_ _γ ∥EZ1 −_ EZ2∥∞.

3.2 DISTRIBUTIONAL RL: A NOVEL ENTROPY-REGULARIZED MLE

The reasonable properties of KL divergence in Proposition 1 allows us to leverage it to conduct the
theoretical analysis. To separate the impact of additional distribution information from the expectation of Z _[π], we leverage the variant technique of gross error model from robust statistics (Huber,_
2004), similar to the technique to analyze Label Smoothing (M¨uller et al., 2019) and Knowledge
Distillation (Hinton et al., 2015). Specifically, we denote the one-dimensional full distribution of
_Z_ _[π]_ as F, and the distribution on the remaining support getting rid of E [Z _[π]] as Fµ. Hence, we can_
obtain the distribution decomposition for Z _[π](s, a) as_

_F_ _[s,a](x) = (1 −_ _ϵ)1{x≥E[Zπ(s,a)]}(x) + ϵFµ[s,a][(][x][)][,]_ (4)


-----

where ϵ controls the proportion of Fµ[s,a][(][x][)][ and the indicator function][ 1]{x≥E[Z[π](s,a)]} [= 1][ if][ x][ ≥]
E [Z _[π](s, a)], otherwise 0. After taking derivatives on both sides, we attain the relationship of their_
density functions as p[s,a](x) = (1 − _ϵ)δ{x=E[Zπ(s,a)]}(x) + ϵµ[s,a](x), where µ[s,a](x) is the density_
function related to Z _[π](s, a) on remaining supports removing E [Z_ _[π](s, a)]. It is worth noting that_
the existence of(1 − _ϵ)δ{x=E[Zπ µ(s,a[s,a])](}x/ϵ) can be simply guaranteed by directly computing as long as p[s,a](x) and the expectation of Z_ _[π](s, a µ[s,a]) exist. Next, we use(x) = p[s,a](x)/ϵ −_
_p[s,a](x) and qθ[s,a][(][x][)][ to denote the density distributions behind][ {][Y][i][}][i][∈][[][n][]][ and][ Z]θ[k][(][s, a][)][ in neural Z-]_
fitted iteration via Eq. 3, respectively. Therefore, we can derive the following result in Proposition 2.
**Proposition 2. Let H(P, Q) as the cross entropy, i.e., H(P, Q) = −** _x∈X_ _[P]_ [(][x][) log][ Q][(][x][) d][x][. Let]

_α be a positive constant, and based on the decomposition in Eq. 4 and DKL as dp, Neural Z-fitted_

R

_iteration in Eq. 3 can be reformulated as_


_Zθ[k][+1]_ = argmin
_Zθ_


_H(δ{x=E[Zπ(si,ai)]}, qθ[s][i][,a][i]_ ) + αH(µ[s][i][,a][i] _, qθ[s][i][,a][i]_ ). (5)
_i=1_

X


We provide the proof in Appendix C. For the uniformity of notation, we still use s, a in the following
analysis instead of si, ai in Eq. 5. Importantly, the first term in Eq. 5 can be further simplified as
_−_ _x∈X_ [log][ q]θ[s,a][(][E][ [][Z][(][s, a][)])][. Minimizing this first term can be viewed as a variant of Maximum]

Likelihood Estimation (MLE) on the expectation E [Z(s, a)] rather than the traditional MLE directly

R

on observed samples. The cross entropy regularization in the second term pushes qθ[s,a] to approximate
the distribution µ[s,a] in order to fully utilize the additional distributional information while learning,
serving as the key to the superiority of distributional RL. This novel cross entropy regularization
regarding µ[s,a] and qθ[s,a] is different from the classical entropy regularization used in RL, which we
further analyze their connection and discrepancy in Section 3.3. In summary, distributional RL can
be simplified as a novel entropy-regularized MLE within neural Z-fitted iteration framework in stark
contrast to the Least-Square estimation of expectation-based RL in the neural Q-fitted iteration.

3.3 CONNECTION WITH MAXIMUM ENTROPY RL

We establish the connection between the derived novel cross entropy regularization in Eq. 5 in
distributional RL with the classical maximum entropy RL (Williams & Peng, 1991). Maximum
entropy RL, including Soft Q-Learning (Haarnoja et al., 2017), greedily maximizes the entropy of
the policy π(·|s) in each state:


E(st,at) _ρπ [r (st, at) + β_ (π( _st))],_ (6)
_∼_ _H_ _·|_
_t=0_

X


_J(π) =_


where H (πθ (·|st)) = − [P]a _[π][θ][ (][a][|][s][t][) log][ π][θ][ (][a][|][s][t][)][ and][ ρ][π][ is the generated distribution following]_

_π. The temperature parameter β determines the relative importance of the entropy term against the_
cumulative rewards, and thus controls the stochasticity of the optimal policy. This maximum entropy regularization has various conceptual and practical advantages. Firstly, the policy encourages
exploration, avoiding situations in which the agent might fall into a local optimum. Secondly, it
considerably improves learning speed over classical methods and therefore are widely used in stateof-the-art algorithms, e.g., Soft Actor-Critic (SAC) (Haarnoja et al., 2018) and PPO (Schulman et al.,
2017). Lastly, maximum entropy can yield more robustness to abnormal or rare events while developing a task. Similar robustness superiority of distributional RL against noisy state observations has
been investigated in (Sun et al., 2021).

Similar benefits of both distributional RL and maximum entropy RL motivate us to explore
their mathematical connection, and we refocus on the Bellman updating based on these two
kinds of entropy-based regularization. In particular, Soft Policy Evaluation proposed in SAC
has shown that by alternately iterating the following equations, the algorithm is equivalent to
maximize the maximum entropy RL objective function in Eq. 6: T _[π]Q (st, at) ≜_ _r (st, at) +_
_γEst+1_ _ρ[π] [V (st+1)], where V (st+1) = Eat+1_ _π [Q (st+1, at+1)_ log π (at+1 _st+1)] is the soft_
_∼_ _∼_ _−_ _|_
value function (Haarnoja et al., 2018). Interestingly, we still have the similar convergence result
related to our novel cross entropy regularization in Eq. 5, which is shown in Theorem 1:
**Theorem 1. Consider the soft Bellman operator Td[π]** _[in Eq. 7 and a mapping][ Q][0][ :][ S × A →]_ [R]
_with |A| ≤∞, and define Q[k][+1]_ = Td[π][Q][k][. Given the true distribution][ µ][s][t][,a][t][ for each][ t][, then the]


-----

_sequence Q[k]_ _will converge to a soft Q-value of π as k →∞_ _with the new entropy objective function_
_as J_ _[′](θ) =_ _t=0_ [E][(][s]t[,a]t[)][∼][ρ][π][ [][r][ (][s][t][, a][t][)][ −] _[γ][H][ (][µ][s][t][,a][t]_ _[, q]θ[s][t][,a][t]_ )]:

_d[π][Q][ (][s][t][, a][t][)][ ≜]_ _[r][ (][s][t][, a][t][) +][ γ][E][s]t+1[∼][ρ][π][ [][V][ (][s][t][+1][)]][,]_ (7)

[P][T] _T_

_where V (st+1) = Eat+1∼π,x∼µ[s]t+1_ _[,a]t+1_ _Q (st+1, at+1) + log qθ[s][t][+1][,a][t][+1]_ (x) _and the policy π fol-_
_lows the classical greedy policy rule, i.e., π(·|s) = arg maxa E [Zθ(s, a)], determined by_ _θ._

Please refer to Appendix D for the proof. The new entropy augmented objective function J _[′](θ) in Theorem 1_
reveals that different from adding the entropy regarding
the policy π( _st) in maximum entropy RL, distributional_

_·|_
RL can be viewed as subtracting a cross entropy term between µ and qθ. Moreover, the entropy in maximum entropy RL is state-wise, while our cross entropy regularization is state-action-wise, which is a more fine-grained
characterization on the variability of the full distribution
of return Z.


Notably, we highlight that our derived cross entropy reg- Figure 1: Impact of the risk-aware regularization is risk-aware. If the true state-action distri- ularization in distributional RL. The
bution Fµ has higher degree of dispersion, e.g., a larger entropy-based regularization will push
variance, than the current qθ, minimizing our entropy au- _qθ[s,a]_ to match the true density functomatically encourages the policy to explore the uncer- tion µ[s,a] determined by the environmap in Figure 1 is provided to illustrate the impact of ourtainty of the environment through changing Zθ. A sketch ment, and thus let the learned policy befully aware of the uncertainty of envirisk-aware entropy regularization. We remark that this ronment.
risk-aware regularization may not necessarily encourage exploration under the greedy action rule
_πZ(s) = argmaxa E [Zθ(s, a)] even though qθ changes towards µ as exhibited in Figure 1. A_
risk-aware action rule on the current greedy version might be preferred in the future.

4 STABLE OPTIMIZATION AND GUARANTEED GENERALIZATION


In this section, we further characterize qθ[s,a] in order to attain stable optimization and guaranteed
generalization properties of distributional RL. Concretely, we leverage the histogram function f _[s,a]_
to parameterize the density function of Zθ(s, a), i.e., qθ[s,a][, still based on the KL divergence as][ d][p][,]
yielding the histogram distributional loss (Imani & White, 2018) within each update in the neural
Z-fitted iteration. Denote x(s) as the state feature on each state s, and we let the support of x(s) be
uniformly partitioned into k bins. We let the function f _[s,a]_ : X → [0, 1][k] provide a k-dimensional
vector f _[s,a](x(s)) of the coefficients indicating the probability the target is in that bin given the state_
_s and action a pair, and x(s). We use softmax based on the linear approximation x(s)[⊤]θi to express_
_f_ _[s,a], i.e., fi[s,a,θ](x(s)) = exp_ **x(s)[⊤]θi** _/_ _j=1_ [exp] **x(s)[⊤]θj** . For simplicity, we use fi[θ][(][x][(][s][))]

to replace fi[s,a,θ](x(s)). Therefore, the resulting   _histogram distributional loss _  _θ is formulated as:_
_L_

[P][k] _k_

_Lθ(s, a) = DKL(p[s,a], qθ[s,a][) =]_ Zx∈X _p[s,a](x) log (p[s,a](x)/qθ[s,a][(][x][))][ dx][ =][ −]_ _i=1_ _p[s,a]i_ log fi[θ][(][x][(][s][))]

X

(8)
where θ = _θ1, ..., θk_ and p[s,a]i is the histogram probability of p[s,a](x) defined in Eq. 4. The deriva_{_ _}_
tion of the histogram distributional loss is given in Appendix E. To attain the stable optimization
property of distributional RL, we firstly derive Lemma 1 as follows
**Lemma 1. (Properties of Histogram Distributional Loss) Assume each state feature is bounded,**
_i.e., ∥x(s)∥≤_ _l, then Lθ is kl[2]-smooth, convex and kl-Lipschitz continuous w.r.t. θ._

Please refer to Appendix F for the proof. The derived Lipschitz properties of dp under histogram
distributional loss plays an integral part in the stable optimization of distributional RL, but the classical Least-Squared estimation in neural-Q-fitted iteration for expectation-based RL does not enjoy
these smoothness properties. In particular, for histogram distributional loss we have _θ_ _θ_ _kl,_
_∥∇_ _L_ _∥≤_
while the counterpart in expectation-based is |yi − _Q[k]θ_ [(][s, a][)][|∥][x][(][s][)][∥][, which might be arbitrarily large]


-----

while runningdue to the unbounded stochastic gradient descend |yi − _Q[k]θ_ [(][s, a][)][|][. To further derive the uniform stability of distributional RL] (SGD), we introduce its definition in Definition 1.

**Definition 1. (Uniform Stability) (Hardt et al., 2016) Consider a function g, a randomized algorithm**
_M is uniformly stable if for all data sets S, S[′]_ _such that S, S[′]_ _differ in at most one example, we have_

sup (9)
_x_ [E][M][ [][g][(][M][(][S][);][ x][)][ −] _[g][ (][M][ (][S][′][) ;][ x][)]][ ≤]_ _[ϵ][stab][ .]_

Given the definition of uniform stability, we show that distributional RL under histogram distributional loss while running SGD is ϵstab-uniformly stable and generalization guaranteed in Theorem 2.

**Theorem 2. (Stable Optimization and Guaranteed Generalization) Suppose that we run SGD under**
_Lθ in Eq. 8 with step sizes λt ≤_ 2/kl[2] _for T steps, and ∥x(s)∥≤_ _l for each state s, we have:_

_(1) SGD under Lθ satisfies the uniform stability in Definition 1 with ϵstab ≤_ [4][kT]n _[,]_

_(2) Suppose that the data distribution is stable within each neural Z-fitted iteration, the generaliza-_
_tion gap for solving the entropy regularized MLE in Eq. 5 is ϵstab -bounded._

Please refer to the proof of Theorem 2 in Appendix G. In summary, the optimization to solve the
entropy regularized MLE of distributional RL within the update of neural Z-fitted iteration is stable
with the stability errors shrinking at the rate of O(n[−][1]). This stability optimization properties is
ascribed to the smooth and convex properties from histogram distributional loss (non-convex case
still holds in (Hardt et al., 2016)) and can further control the generalization gap. By contrast, the
least-estimation in neural Q-fitted iteration for expectation-based RL, which is without these smooth
properties, can not yield the stable optimization and guaranteed generalization directly.

5 ACCELERATION EFFECT

Based on the setting in Section 4, we further incorporate the decomposition of F _[s,a](x) proposed_
in Eq. 4 into the analysis, where p[s,a](x) = (1 − _ϵ)δ{x=E[Zπ(s,a)]}(x) + ϵµ[s,a](x), in order to_
derive the acceleration effect of distributional RL. Within each update in the neural Z-fitted iteration, the target is to minimize _n[1]_ _ni=1_

_G[k](θ) = E(s,a)_ _ρπ [_ _θ(s, a)]. As such, the convex and smooth properties in Lemma 1 still hold[L][θ][(][s][i][, a][i][)][. We denote][ G][k][(][θ][)][ as the expectation of][ L][θ][, i.e.,]_
_∼_ _L_ P
for G[k](θ). We use G(θ) for G[k](θ) for simplicity. As the KL divergence enjoys the property of
unbiased gradient estimates, we let the variance of its stochastic gradient over the expectation be
bounded, i.e., E(s,a)∼ρπ _∥∇Lθ(δ{x=E[Zπ(s,a)]}, qθ[s,a][))][ −∇][G][(][θ][)][∥][2][]_ = σ[2]. Next, we characterize
the approximation degree of qθ[s,a] to the ground-truth distribution µ[s,a] defined in Eq. 5 by measuring
its variance as κσ[2]: []

E(s,a)∼ρπ _∥∇Lθ(µ[s,a], qθ[s,a][))][ −∇][G][(][θ][)][∥][2][]_ = κσ[2]. (10)

Notably, a favorable approximation of qθ[s,a] to µ[s,a]θ would lead to a small κ, in which case an ac
[]

celeration effect of distributional RL can be derive as shown in Theorem 3. Based on Eq. 10, we
immediately have the following lemma.

**Lemma 2. As p[s,a](x) = (1 −** _ϵ)δ{x=E[Zπ(s,a)]}(x) + ϵµ[s,a](x), we have:_

E(s,a)∼ρπ _∥∇Lθ(p[s,a], qθ[s,a][))][ −∇][G][(][θ][)][∥][2][]_ _≤_ (1 − _ϵ)σ[2]_ + ϵκσ[2]. (11)

Please refer to Appendix H for the proof. To measure the convergence of the entropy regularized[]
MLE in distributional RL or least-square estimation in expectation-based RL, we need the following
definition of the first-order τ -stationary point.

**Definition 2. (First-order τ** _-Stationary Point) While solving minθ G(θ), the updated parameters θT_
_after T steps is a first-order τ_ _-stationary point if ∥∇G(θT )∥≤_ _τ_ _, where the small τ is in (0, 1)._

We formally show our Theorem 3 to characterize the acceleration effect of distributional RL determined by the variance magnitude τ .

**Theorem 3. (Acceleration Effect) While running SGD to solve the entropy-regularized MLE within**
_neural Z-fitted iteration in Eq. 5 with the step size λ = 1/kl[2]_ _and ϵ = 1/(1 + τ_ ), we have:


-----

_(1) The sample complexity is O(_ _τ[1][4][ )][ if we only consider the expectation in][ p][s,a][, i.e.,][ δ][{][x][=][E][[][Z][π][(][s,a][)]][}][.]_

_(2) When κ_ 4τσ[2][2][ and let][ T][ =][ 4][G]λτ[(][θ][2][0][, the regularized MLE of distributional RL within each neural][)]
_≤_

_Z-fitted iteration converges to a τ_ _-stationary point in expectation with sample complexity O(_ _τ[1][2][ )][.]_

_(3) When κ >_ 4τσ[2][2][ and let][ T][ =][ G]λκτ[(][θ][0][2][)][, the regularized MLE of distributional RL does not converge]

_to a τ_ _-stationary point, but we have E_ _∥∇G(θ)∥[2][]_ _≤_ _O(κ)._

The proof is provided in Appendix I. Theorem 3 is inspired by (Xu et al., 2020) to analyze the
convergence of label smoothing, which can be similarly utilized to analyze the acceleration effect
of distributional RL. Importantly, we need to emphasize that Theorem 3 in fact reveals the reason
why distributional RL algorithms can achieve inconsistent superiority across different Atari games.
This phenomenon is mainly owing to the different degree of TD target approximation to p[s,a], especially µ[s,a] for various environments. In particular, a small approximation error of qθ to p (or µ)
corresponds to a small κ, This can be normally ≤ _τ_ [2]/4σ[2], yielding better sample efficiency and
accelerating the algorithm relative to expectation-based RL. Conversely, an unsatisfactory approximation may not lead to the acceleration effect, but the bounded gradient normally corresponds to a
reasonable performance, coinciding with previous empirical observations on various environments.

**Extension of Representation Effect.** Apart from the acceleration effect, we also conduct some
empirical analysis of distributional RL from the perspective of representation. In particular, distributional RL encourages state representation from the same action class classified by the policy in
tighter clusters. Please refer to Appendix K for more details.

6 ALGORITHM: SINKHORN DISTRIBUTIONAL RL

Through the theoretical analysis, we can gain insights into the advantages of distribution RL over
expectation-based RL, attributing to entropy-based regularization, stable optimization and acceleration effect determined by the distributional approximation error (measured by κ). From the algorithmic perspective, the choice of representation manner on Zθ and the distributional divergence metric
_dp mentioned in Section 3.1 are pivotal for the final performance of distributional RL algorithms._

we further design Sinkhorn distributional RL algorithm. Sinkhorn loss (Sinkhorn, 1967) is a
tractable loss to approximate optimal transport problem by leveraging an entropic regularization to
turn the original Wasserstein distance into a differentiable and more robust quantity. The resulting
loss can be computed using Sinkhorn fixed point iterations, which is naturally suitable for modern
deep learning frameworks. In particular, the entropic smoothing generates a family of losses interpolating between Wasserstein distance and Maximum Mean Discrepancy (MMD). Thus it allows us
to find a sweet trade-off that simultaneously leverages the geometry of Wasserstein distance on the
one hand, and the favorable high-dimensional sample complexity and unbiased gradient estimates
of MMD. We introduce the entropic regularized Wassertein distance as


Π(x, y)
_c(x, y)dΠ(x, y) + ε_ log

du(x)dv(y)

Z 


dΠ(x, y), (12)


min
Π∈Π(u,v)


where c is the cost function in optimal transport. This objective function associated with cost function c can be rewritten as _c,ε(µ, ν) =_ _c(x, y)dΠε(x, y). Therefore, the sinkhorn loss between_
_W_
two measures u and v is defined as
R

_c,ε(u, v) = 2_ _c,ε(u, v)_ _c,ε(u, u)_ _c,ε(v, v)_ (13)
_W_ _W_ _−W_ _−W_

**Theorem 4. If we leverage Sinkhorn loss as the metric in distributional RL and choose c as unrec-**
_tified kernel kα, i.e., kα(x, y) := −∥x −_ _y∥[α], for ∀α ∈_ R, we have:

_(1) As ϵ_ 0, _c,ε(u, v)_ 2 _α(u, v), and thus T[π]_ _is a γ-contraction._
_→_ _W_ _→_ _W_

_(2) As ϵ →∞, W_ _c,ε(u, v) →_ _MMD−kα_ (u, v), and thus T[π] _is a γ[α/][2]-contraction._

Proof is provided in Appendix J. Theorem 4 indicates that if we choose c as the unrectified kernel, the
limiting behaviors of distributional Bellman operator T[π] are both contractive under Sinkhorn loss.
Note that without the limitation, the entropic regularization restricts the search space on the optimal


-----

transport that eventually corresponds to a Gibbs kernel, based on which T[π] may not be a contraction.
However, similar to MMD method, we show that our approach can still achieve empirical success
and is very competitive across a wide range of Atari games in Section 7.


In the algorithm design, similar to in MMD distributional RL (Nguyen et al., 2020), we apply particle representation to represent Zθ(s, a) by directly generating multiple deterministic samples.
Finally, we minimize the Sinkhorn loss between
the approximate distribution via multiple samples and its distributional Bellman target. A detailed description of algorithm is provided in Algorithm 1.

**Remark.** MMD enjoys a convergence rate of
_O(n[−][1][/][2]) 1 regardless of the underlying dimen-_
sion while 1-Wasserstein distance has a convergence rate of O(n[−][1][/d]) if d > 2, which is slower
for large d. Thus, Sinkhorn loss has the potential
to enjoy the faster convergence of MMD.

7 EXPERIMENTS


**Algorithm 1 Generic Sinkhorn Algorithm**
**Require: Number of particles N**, number of
Sinkhorn iteration L and hyperparameter ε.
**Input: Sample transition (s, a, r[′], s[′])**

1: if Policy evaluation then
2: _a[∗]_ _∼_ _π(·|s[′])._

3: else
4: _a[∗]_ _←_ arg maxa′∈A _N[1]_ _Ni=1_ _[Z][θ][ (][s][′][, a][′][)]i_

5: end if
6: TZi ← _r + γZθ−_ (s[′], a[∗])Pi, ∀1 ≤ _i ≤_ _N_

**Output: W** _k2,ε_ _{Zθ(s, a)i}i[N]=1_ _[,][ {][T][Z][i][}]i[N]=1_
 


We demonstrate the effectiveness of Sinkhorn distributinal RL (SinkhornDRL) as described in Algorithm 1 on a wide range of Atari 2600 games. Specifically, we leverage the same architecture
as QR-DQN (Dabney et al., 2018b) for a fair comparison. More advanced techniques that can expand the model expressiveness from IQN (Dabney et al., 2018a) and FQF (Yang et al., 2019) can be
naturally incorporated into our framework.

**Baselines.** Due to interpolation characteristic of SinkhornDRL, we choose 3 typical distributional
RL algorithms as classic baselines, including QR-DQN (Dabney et al., 2018b), C51 (Bellemare
et al., 2017a) and MMD (Nguyen et al., 2020), as well as DQN (Mnih et al., 2015). MMD algorithm
is implemented with the same architecture as QRDQN, and leverages Gaussian kernels kh(x, y) =


500 DQNC51 Breakout 3500 DQNC51 Enduro 20 DQNC51 Pong

400300 QRDQNMMDSinkhorn 30002500 QRDQNMMDSinkhorn 10 QRDQNMMDSinkhorn

2000 0

200 1500

1000 10

Average Return100 Average Return Average Return

500

0 0 20

0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8

Time Steps (1e7) Time Steps (1e7) Time Steps (1e7)


20001750 DQNC51QRDQNSpaceInvaders 1000 DQNC51QRDQN Berzerk 3500030000 DQNC51QRDQN BattleZone

15001250 MMDSinkhorn 800600 MMDSinkhorn 2500020000 MMDSinkhorn

1000

15000

750 400

Average Return 500 Average Return Average Return10000

250 200 5000

0.0 0.2 Time Steps (1e7)0.4 0.6 0.8 1.0 0.0 0.2 Time Steps (1e7)0.4 0.6 0.8 1.0 00.0 0.2 Time Steps (1e7)0.4 0.6 0.8 1.0


Figure 2: Performance of SinkhornDRL on six Atari games.


-----

exp(−(x − _y)[2]/h) with the kernel mixture trick covering a range of bandwidths h, which is same_
as the basic setting in the original MMD paper (Nguyen et al., 2020).

**Hyperparameter settings.** For a fair comparison with QR-DQN, C51 and MMD, we used the
same hyperparamters: the number of generated samples N = 200, Adam optimizer with lr =
0.00005, ϵAdam = 0.01/32. We used a target network to compute the distributional Bellman target,
which fits well in the neural Z-fitted iteration framework. In addition, we choose number of Sinkhorn
iterations L = 10 and smoothing hyperparameter ε = 10.0 in Section 7.1 as they are not sensitive
within a proper interval demonstrated in Section 7.2. We choose α = 2 in kα.

7.1 PERFORMANCE OF SINKHORNDRL


Figure 2 illustrates that SinkhornDR can achieve the state-of-the-art or competitive performance
on typical Atari games compared with various baseline algorithms (DQN, QR-DQN, C51, MMD)
in different metrics dp and representation manners on Zθ. Even though C51 outperforms others on
breakout game, it is significantly inferior on other games, e.g., Enduro, Pong and Berzerk. Moreover,
SinkhornDRL significantly outperforms MMD on breakout and SpaceInvader, and this superiority
can be owing to the theoretical advantage of Sinkhorn loss over MMD. This coincides with the
theoretical results as demonstrated in Theorem 4 that Sinkhorn loss interpolates between Wasserstein
distance and MMD, which can simultaneously make full use of the data geometry from Wasserstein
distance and the faster convergence, unbiased gradient estimates from Maximum Mean Discrepancy.
We provide the competitive performance of SinkhornDRL on other Atari games in Appendix L.

7.2 SENSITIVITY ANALYSIS


We further examine the impact of different hyperpa-rameters in SinkhornDRL 500 =1 Breakout 600 Samples=50Breakout

400

including the smoothing 300
hyperparameter ε in Eq. 12 200 300
the number of determin-istic samples N in Algo- Average Return100 Average Return200100

|Col1|=1 =10|Col3|Col4|Col5|
|---|---|---|---|---|
||=100||||
||||||
||||||
||||||
||||||
||||||


|S|amples=5|Col3|0|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
|S S|amples=2 amples=5||00 00||||
||||||||
||||||||
||||||||
||||||||
||||||||


agram in Figure 3, we can Time Steps (1e7) Time Steps (1e7)
observe that our algorithm

(a) Hyper-parameter ε (b) Number of Samples

is not sensitive to the magnitude of ε as long as ε is

Figure 3: Sensitivity analysis of SinkhornDRL on Breakout.

within an appropriate interval, e.g., [1, 100]. Meanwhile, it turns out that an overly large number of samples N can even slightly worsen the performance of SinkhornDRL.

8 DISCUSSIONS AND CONCLUSION


Implicit generative models can be further incorporated into SinkhornDRL, including parameterizing
the cost function in Sinkhorn loss, which can be naturally expected to achieve more promising
performance in the future. Moreover, a direct analysis on Wasserstein or ℓp distance can be more
effective than KL divergence presented in this paper, albeit being theoretically tricky.

In this paper, we illuminate the superiority of distributional RL over expectation-based RL from the
perspectives of regularization, optimization, acceleration and representation. In addition, a novel
family of distributional RL algorithms based on Sinkhorn loss is designed that accomplishes the
promising performance on Atari games. Our analysis paves the way towards deeper understanding
of distributional RL, and further promote its deployment in real applications.

**Ethics Statement.** Revealing the advantage of distributional RL would promote the application of
distributional RL algorithms in real scenarios. As distributional RL enjoys the robustness against


-----

abnormal events, e.g., noisy state observations, it can also be beneficial for the privacy of algorithms.
Besides, the deeper insights into distributional RL plays a key role into the research integrity issue.
Based on our knowledge, it is not related to harmful applications or fairness issues.

**Reproducibility Statement.** For the theoretical part, we clearly state the related assumption and
detailed proof process in the appendix. In terms of the algorithm implementation, our Sinkhorn
algorithm is directly adapted from the public distributional RL algorithms, such as MMD.

REFERENCES

Martin Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein generative adversarial networks.
In International conference on machine learning, pp. 214–223. PMLR, 2017.

Marc G Bellemare, Will Dabney, and R´emi Munos. A distributional perspective on reinforcement
learning. International Conference on Machine Learning (ICML), 2017a.

Marc G Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshminarayanan,
Stephan Hoyer, and R´emi Munos. The cramer distance as a solution to biased wasserstein gradients. arXiv preprint arXiv:1705.10743, 2017b.

Will Dabney, Georg Ostrovski, David Silver, and R´emi Munos. Implicit quantile networks for
distributional reinforcement learning. International Conference on Machine Learning (ICML),
2018a.

Will Dabney, Mark Rowland, Marc G Bellemare, and R´emi Munos. Distributional reinforcement
learning with quantile regression. _Association for the Advancement of Artificial Intelligence_
_(AAAI), 2018b._

Odin Elie and Charpentier Arthur. Dynamic Programming in Distributional Reinforcement Learn_ing. PhD thesis, Universit´e du Qu´ebec `a Montr´eal, 2020._

Jianqing Fan, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang. A theoretical analysis of deep qlearning. In Learning for Dynamics and Control, pp. 486–489. PMLR, 2020.

Aude Genevay, Gabriel Peyr´e, and Marco Cuturi. Learning generative models with sinkhorn divergences. In International Conference on Artificial Intelligence and Statistics, pp. 1608–1617.
PMLR, 2018.

Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. In International Conference on Machine Learning, pp. 1352–1361.
PMLR, 2017.

Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018.

Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In International Conference on Machine Learning, pp. 1225–1234. PMLR,
2016.

Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
_preprint arXiv:1503.02531, 2015._

Peter J Huber. Robust Statistics, volume 523. John Wiley & Sons, 2004.

Ehsan Imani and Martha White. Improving regression performance with distributional losses. In
_International Conference on Machine Learning, pp. 2157–2166. PMLR, 2018._

Clare Lyle, Marc G Bellemare, and Pablo Samuel Castro. A comparative analysis of expected
and distributional reinforcement learning. In Proceedings of the AAAI Conference on Artificial
_Intelligence, volume 33, pp. 4504–4511, 2019._


-----

Clare Lyle, Mark Rowland, Georg Ostrovski, and Will Dabney. On the effect of auxiliary tasks on
representation dynamics. In International Conference on Artificial Intelligence and Statistics, pp.
1–9. PMLR, 2021.

Yecheng Jason Ma, Dinesh Jayaraman, and Osbert Bastani. Conservative offline distributional reinforcement learning. arXiv preprint arXiv:2107.06106, 2021.

John Martin, Michal Lyskawinski, Xiaohu Li, and Brendan Englot. Stochastically dominant distributional reinforcement learning. In International Conference on Machine Learning, pp. 6745–6754.
PMLR, 2020.

Borislav Mavrin, Shangtong Zhang, Hengshuai Yao, Linglong Kong, Kaiwen Wu, and Yaoliang
Yu. Distributional reinforcement learning for efficient exploration. International Conference on
_Machine Learning (ICML), 2019._

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. nature, 518(7540):529–533, 2015.

Tetsuro Morimura, Masashi Sugiyama, Hisashi Kashima, Hirotaka Hachiya, and Toshiyuki Tanaka.
Parametric return density estimation for reinforcement learning. arXiv preprint arXiv:1203.3497,
2012.

Rafael M¨uller, Simon Kornblith, and Geoffrey Hinton. When does label smoothing help? _arXiv_
_preprint arXiv:1906.02629, 2019._

Thanh Tang Nguyen, Sunil Gupta, and Svetha Venkatesh. Distributional reinforcement learning with
maximum mean discrepancy. Association for the Advancement of Artificial Intelligence (AAAI),
2020.

Mark Rowland, Marc Bellemare, Will Dabney, R´emi Munos, and Yee Whye Teh. An analysis
of categorical distributional reinforcement learning. In International Conference on Artificial
_Intelligence and Statistics, pp. 29–37. PMLR, 2018._

Mark Rowland, Robert Dadashi, Saurabh Kumar, R´emi Munos, Marc G Bellemare, and Will Dabney. Statistics and samples in distributional reinforcement learning. International Conference on
_Machine Learning (ICML), 2019._

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

Richard Sinkhorn. Diagonal equivalence to matrices with prescribed row and column sums. The
_American Mathematical Monthly, 74(4):402–405, 1967._

Ke Sun, Yi Liu, Yingnan Zhao, Hengshuai Yao, Shangling Jui, and Linglong Kong. Exploring
the robustness of distributional reinforcement learning against noisy state observations. arXiv
_preprint arXiv:2109.08776, 2021._

Richard S Sutton and Andrew G Barto. Reinforcement learning: An Introduction. MIT press, 2018.

G´abor J Sz´ekely. E-statistics: The energy of statistical samples. Bowling Green State University,
_Department of Mathematics and Statistics Technical Report, 3(05):1–18, 2003._

Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279–292, 1992.

Ronald J Williams and Jing Peng. Function optimization using connectionist reinforcement learning
algorithms. Connection Science, 3(3):241–268, 1991.

Eric Wong, Frank Schmidt, and Zico Kolter. Wasserstein adversarial examples via projected
sinkhorn iterations. In International Conference on Machine Learning, pp. 6808–6817. PMLR,
2019.

Yi Xu, Yuanhong Xu, Qi Qian, Hao Li, and Rong Jin. Towards understanding label smoothing.
_arXiv preprint arXiv:2006.11653, 2020._


-----

Derek Yang, Li Zhao, Zichuan Lin, Tao Qin, Jiang Bian, and Tie-Yan Liu. Fully parameterized quantile function for distributional reinforcement learning. Advances in neural information processing
_systems, 32:6193–6202, 2019._

Fan Zhou, Jianing Wang, and Xingdong Feng. Non-crossing quantile regression for distributional
reinforcement learning. Advances in Neural Information Processing Systems, 33, 2020.

Florian Ziel. The energy distance for ensemble and scenario reduction. _arXiv preprint_
_arXiv:2005.14670, 2020._


-----

A DEFINITION OF DISTANCES AND CONTRACTION

**Definition of distances. Given two random variables X and Y, p-Wasserstein metric Wp between**
the distributions of X and Y is defined as

1 1/p
_Wp(X, Y ) =_ Z0 _FX[−][1][(][ω][)][ −]_ _[F][ −]Y_ [1][(][ω][)] _dω_ = ∥FX[−][1] _[−]_ _[F][ −]Y_ [1][∥][p][,] (14)

which F _[−][1]_ is the inverse cumulative distribution function of a random variable with the cumulative[p]
distribution function as F . Further, ℓp distance (Elie & Arthur, 2020) is defined as

_∞_ 1/p
_ℓp(X, Y ) :=_ _|FX_ (ω) − _FY (ω)|[p]_ dω = ∥FX − _FY ∥p_ (15)
Z−∞ 

The ℓp distance and Wassertein metric are identical at p = 1, but are otherwise distinct. Note that
when p = 2, ℓp distance is also called Cram´er distance (Bellemare et al., 2017b) dC(X, Y ). Also,
the Cram´er distance has a different representation given by

_dC(X, Y ) = E_ _X_ _Y_ (16)
_|_ _−_ _| −_ [1]2 [E][ |][X][ −] _[X]_ _[′][| −]_ 2[1] [E][ |][Y][ −] _[Y][ ′][|][,]_

where X _[′]_ and Y _[′]_ are the i.i.d. copies of X and Y . Energy distance (Sz´ekely, 2003; Ziel, 2020) is a
natural extension of Cram´er distance to the multivariate case, which is defined as

_dE(X, Y) = E_ **X** **Y** 2 (17)
_∥_ _−_ _∥_ _−_ 2[1] [E][∥][X][ −] **[X][′][∥−]** [1]2 [E][∥][Y][ −] **[Y][′][∥][,]**


where X and Y are multivariate. Moreover, the energy distance is a special case of the maximum
mean discrepancy (MMD), which is formulated as

MMD(X, Y; k) = (E [k (X, X[′])] + E [k (Y, Y[′])] − 2E[k(X, Y)])[1][/][2] (18)

where k(·, ·) is a continuous kernel on X . In particular, if k is a trivial kernel, MMD degenerates to energy distance. Additionally, we further define the supreme MMD, which is a functional
_P(X_ )[S×A] _× P(X_ )[S×A] _→_ R defined as

MMD (µ, ν) = sup MMD (µ(x, a), ν(x, a))
_∞_ (x,a)∈S×A _∞_ (19)

**Proof of Contraction.**

_• Contraction under supreme form of Wasserstein diatance is provided in Lemma 3 (Belle-_
mare et al., 2017a).

_• Contraction under supreme form of ℓp distance can refer to Theorem 3.4 (Elie & Arthur,_
2020).

Contraction under MMD is provided in Lemma 6 (Nguyen et al., 2020).

_•_ _∞_

B PROOF OF PROPOSITION 1

_Proof. (1) We recap three crucial properties of a divergence metric. The first is scale sensitive (S)_
(of order β, beta > 0), i.e., dp(cX, cY ) ≤|c|[β]dp(X, Y ). The second property is shift invariant (I),
i.e., dp(A + X, A + Y ) ≤ _dp(X, Y ). The last one is unbiased gradient (U). We use p and q to_
denote the density function of two random variables X and Y, and thus DKL(X, Y ) is defined as
_DKL(X, Y ) =_ _∞_ _q(x)_ [d][x][. Firstly, we show that][ D][KL][(][X, Y][ )][ is NOT scale sensitive:]
_−∞_ _[p][(][x][)][ p][(][x][)]_
R _∞_ 1 _a1_ _[p][(][ x]a_ [)]

_DKL(cX, cY ) =_ _a_ _[p][(]_ _[x]a_ [)] 1

_a_ _[q][(][ x]a_ [) d][x]

Z−∞

_∞_ (20)
= _p(y)_ _[p][(][y][)]_

_q(y) [d][y]_

Z−∞

= DKL(X, Y ), with β = 0


-----

We further show that DKL(X, Y ) is shift invariant:

_∞_
_DKL(A + X, A + Y ) =_ _p(x_ _A)_ _[p][(][x][ −]_ _[A][)]_

_−_ _q(x_ _A) [d][x]_

Z−∞ _−_

_∞_
= _p(y)_ _[p][(][y][)]_

_q(y) [d][y]_

Z−∞

= DKL(X, Y )


(21)


Moreover, it is well-known that KL divergence has unbiased sample gradients (Bellemare et al.,
2017b). The supreme DKL is a functional P(X )[S×A] _× P(X_ )[S×A] _→_ R defined as

_DKL[∞]_ [(][µ, ν][) =] sup _DKL(µ(x, a), ν(x, a))_ (22)
(x,a)∈S×A

Therefore, we prove T[π] is at best a non-expansive operator under the supreme form of DKL:


_DKL[∞]_ [(][T][π][Z][1][,][ T][π][Z][2][)]
= sup
_s,a_ _[D][KL][(][T][π][Z][1][(][s, a][)][,][ T][π][Z][2][(][s, a][))]_

= DKL(R(s, a) + γZ1(s[′], a[′]), R(s, a) + γZ2(s[′], a[′]))

= DKL(R(s, a) + Z1(s[′], a[′]), Z2(s[′], a[′]))

_≤_ _ssup[′],a[′][ D][KL][(][R][(][s, a][) +][ Z][1][(][s][′][, a][′][)][, Z][2][(][s][′][, a][′][))]_

= DKL[∞] [(][Z][1][, Z][2][)]


(23)


There we have DKL[∞] [(][T][π][Z][1][,][ T][π][Z][2][)][ ≤] _[D]KL[∞]_ [(][Z][1][, Z][2][)][, implying that][ T][π][ is a non-expansive operator]
under DKL[∞] [.]

(2) By the definition of DKL[∞] [, we have][ sup]s,a _[D][KL][(][Z][n][(][s, a][)][, Z][(][s, a][))][ →]_ [0][ implies][ D][KL][(][Z][n][, Z][)][ →]
0. DKL(Zn, Z) → 0 implies the total variation distance δ(Zn, Z) → 0 according to a straightforward application of Pinsker’s inequality


_δ (Zn, Z)_
_≤_

_δ (Z, Zn)_
_≤_


2 _[D][KL][ (][Z][n][, Z][)][ →]_ [0]

1
2 _[D][KL][ (][Z, Z][n][)][ →]_ [0]


(24)


Based on Theorem 2 in WGAN (Arjovsky et al., 2017), δ(Zn, Z) 0 implies Wp(Zn, Z) 0.
_→_ _→_
This is trivial by recalling the fact that δ and W give the strong an weak topologies on the dual of
(C( ), ) when restricted to Prob( ).
_X_ _∥· ∥∞_ _X_

(3) The conclusion holds because the T[π] degenerates to _[π]_ regardless of the metric dp. Specifically,
_T_
due to the linearity of expectation, we obtain that
ET[π]Z1 ET[π]Z2 = _[π]EZ1_ _[π]EZ2_ _γ_ EZ1 EZ2 _._ (25)
_∥_ _−_ _∥∞_ _∥T_ _−T_ _∥∞_ _≤_ _∥_ _−_ _∥∞_
This implies that the expectation of Z under DKL exponentially converges to the expectation of Z _[∗],_
i.e., γ-contraction.

C PROOF OF PROPOSITION 2


_Proof. Firstly, given a fixed p(x) we know that minimizing DKL(p, qθ) is equivalent to minimizing_
_H(p, q) by following_

+∞
_DKL(p, qθ) =_ _p(x) log_ _[p][(][x][)]_
Z−∞ _qθ(x) [d][x]_

+∞ +∞
= _p(x) log qθ(x) dx_ ( _p(x) log p(x) dx)_ (26)
_−_ _−_ _−_
Z−∞ Z−∞

= (p, qθ) (p)
_H_ _−H_
(p, qθ)
_∝H_


-----

Based on (p, qθ), we can derive the objective function within each neural Z-fitted iteration as
_H_

_n_

1
_n_ _H(p[s][i][,a][i]_ (x), qθ[s][i][,a][i] (x))

_i=1_

X _n_ +∞

= n[1] _−_ _p[s][i][,a][i]_ (x) log qθ[s][i][,a][i] (x) dx

_i=1_  Z−∞ 

Xn +∞

= n[1] _−_ (1 − _ϵ)δ{x=E[Zπ(s,a)]}(x) + ϵµ[s][i][,a][i]_ (x) log qθ[s][i][,a][i] (x) dx

_i=1_  Z−∞ 

Xn   +∞ 

= n[1] (1 − _ϵ)_ _−_ _δ{x=E[Zπ(s,a)]}(x) log qθ[s][i][,a][i]_ (x) dx + ϵH(µ[s][i][,a][i] _, qθ[s][i][,a][i]_ )

_i=1_   Z−∞  

Xn

= n[1] (1 − _ϵ)H(δ{x=E[Zπ(si,ai)]}, qθ[s][i][,a][i]_ ) + ϵH(µ[s][i][,a][i] _, qθ[s][i][,a][i]_ )

_i=1_

Xn  

_ϵ_

_∝_ _n[1]_ _H(δ{x=E[Zπ(si,ai)]}, qθ[s][i][,a][i]_ ) + αH(µ[s][i][,a][i] _, qθ[s][i][,a][i]_ ), where α = 1 _ϵ [>][ 0]_

_i=1_ _−_

= n[1] Xn _−_ +∞ log qθ[s][i][,a][i] (E [Z _[π](s, a)]) dx + αH(µ[s][i][,a][i]_ _, qθ[s][i][,a][i]_ )

_i=1_  Z−∞ 

X


(27)


D PROOF OF THEOREM 1

_Proof. Firstly, we have:_

_V (st+1) = Eat+1∼π,x∼µ[s]t+1_ _[,a]t+1_ _Q (st+1, at+1) + log qθ[s][t][+1][,a][t][+1]_ (x)

= Eat+1∼π [Q (st+1, at+1 )] + Eat+1∼π,x∼µ[s]t+1 _[,a]t+1_ log qθ[s][t][+1][,a][t][+1] (x)

_∞_

 

= Eat+1∼π [Q (st+1, at+1)] − Eat+1∼π _−_ _µ[s][t][+1][,a][t][+1]_ (x) log qθ[s][t][+1][,a][t][+1] (x) dx

 Z−∞ 

= Eat+1∼π [Q (st+1, at+1)] − Eat+1∼π _H_ _µ[s][t][+1][,a][t][+1]_ _, qθ[s][t][+1][,a][t][+1]_

(28)

   

Further, we plug in V (st+1) into RHS of the iteration in Eq. 7, then we obtain

_Td[π][Q][ (][s][t][, a][t][)]_
= r (st, at) + γEst+1 _ρπ [V (st+1)]_
_∼_

= r (st, at) _γE(st+1,at+1)_ _ρπ_ _µ[s][t][+1][,a][t][+1]_ _, qθ[s][t][+1][,a][t][+1]_ + γE(st+1,at+1) _ρπ [Q (st+1, at+1)]_
_−_ _∼_ _H_ _∼_

≜ _rπ (st, at) + γE(st+1,at+1)_ _ρπ [Q  (st+1, at+1)],_ 
_∼_
(29)
where rπ (st, at) ≜ _r (st, at) −_ _γE(st+1,at+1)∼ρπ_ _H_ _µ[s][t][+1][,a][t][+1]_ _, qθ[s][t][+1][,a][t][+1]_ is the entropy augmented reward we redefine. Thus, repeatedly updating via the above equations is equivalent to
   
maximizing the new objective function J _[′](θ) =_ _t=0_ [E][(][s]t[,a]t[)][∼][ρ][π][ [][r][ (][s][t][, a][t][)][ −] _[γ][H][ (][µ][s][t][,a][t]_ _[, q]θ[s][t][,a][t]_ )].
Applying the standard convergence results for policy evaluation (Sutton & Barto, 2018), we can
attain that this Bellman updating under Td[π] [is convergent under the assumption of][P][T] _[ |A|][ <][ ∞]_ [and]
bounded entropy augmented rewards rπ.


E DERIVATION OF HISTOGRAM DISTRIBUTIONAL LOSS

We show the derivation details of the Histogram distribution loss starting from KL divergence between p and qθ. pi is the cumulative probability increment of target distribution _Yi_ _i_ [n] within the
_{_ _}_ _∈_

_i-th bin, and qθ corresponds to a (normalized) histogram, and has density values_ _[f][ θ]i_ [(]w[x]i[(][s][))] per bin.


-----

Thus, we have:


_b_
_DKL (p[s,a], qθ[s,a][) =][ −]_

_a_

Z


_p[s,a](y) log qθ[s,a][(][y][)][dy]_

_li+wi_

_i_ [(][x][(][s][))]
_p[s,a](y) log_ _[f][ θ]_ _dy_
_li_ _wi_

Z


= −

= −

_∝−_


_i=1_


(30)


_k_

_i_ [(][x][(][s][))]
log _[f][ θ]_ (F _[s,a]_ (li + wi) _F_ _[s,a]_ (li))

_wi_ _−_

_i=1_

X _pi_

_k_

| {z }

_p[s,a]i_ log fi[θ][(][x][(][s][))]
_i=1_

X


where the last equality holds because the width parameter wi can be ignored for this minimization
problem.

F PROOF OF LEMMA 1

_Proof. For the histogram distributional loss below,_


_k_ _p[s,a]i_ log fi[θ][(][x][(][s][))][,][ where][ f][ θ]i [(][x][(][s][)) =] _kexp_ **x(s)[⊤]θi**

_i=1_ _j=1_ [exp (]  **[x][(][s][)][⊤][θ][j][)]**

X

P


_θ(s, a) =_
_L_ _−_


exp(x(s)[⊤]θi)
we firstly prove its convexity. Note that − log _kj=1_ [exp(][x][(][s][)][⊤][θ][j] [)][ = log][ P]j[k]=1 [exp] **x(s)[⊤]θj** _−_

**x(s)[⊤]θi, the first term is Log-sum-exp, which is convex (see Convex optimization by Boyd andP**   
Vandenberghe), and the second term is affine function. Thus, _θ(s, a) is convex._
_L_

Secondly, we show that _θ(s, a) is kl-Lipschitz. We compute the gradient of the Histogram distri-_
_L_
butional loss regarding θi:


_p[s,a]j_ log fj[θ][(][x][(][s][))]
_j=1_

X


_∂θi_


_p[s,a]j_
_j=1_

X

_k_

_p[s,a]j_
_j=1_

X


1

_j_ [(][x][(][s][))]
_fj[θ][(][x][(][s][))]_ _[∇][θ][i]_ _[f][ θ]_

1

_i_ [(][x][(][s][))(][δ][ij] _j_ [(][x][(][s][)))][x][(][s][)]
_fj[θ][(][x][(][s][))]_ _[f][ θ]_ _[−]_ _[f][ θ]_


(31)


_p[s,a]i_ (1 _fi[θ][(][x][(][s][)))][ −]_ _p[s,a]j_ _[f][ θ]i_ [(][x][(][s][))] **x(s)**

 _−_ 

Xj≠ _i_

p[s,a]i _−_ _p[s,a]i_ _fi[θ][(][x][(][s][))][ −]_ [(1][ −] _[p]i[s,a])fi[θ][(][x][(][s][))]_ **x(s)**

 p[s,a]i _fi[θ][(][x][(][s][))]_ **x(s)** 

_−_
  


-----

where δij = 1 if i = j, otherwise 0. Then, as we have ∥x(s)∥≤ _l, we bound the norm of its gradient_


_∥_ _∂θ[∂]_


_pj log fj[θ][(][x][(][s][))][∥]_
_j=1_

X


_k_

_∥_ _∂θ[∂]_ _i_
_i=1_

X


_pj log fj[θ][(][x][(][s][))][∥]_
_j=1_

X


(32)


= _∥_ _p[s,a]i_ _−_ _fi[θ][(][x][(][s][))]_ **x(s)∥**

_i=1_

Xk   

_≤_ _|p[s,a]i_ _−_ _fi[θ][(][x][(][s][))][|∥][x][(][s][)][∥]_

_i=1_

X

_≤_ _kl_


obtain thatThe last equality satisfies because Lθ is kl-Lipschitz. _|pi −_ _fi[θ][(][x][(][s][))][|][ is less than 1 and even smaller. Therefore, we]_

Lastly, we show that Lθ is kl[2]-Lipschitz smooth. A lemma is that log(1 + exp(x)) is 4[1] [-smooth]

as its second-order gradient is bounded by [1]4 [, and if][ g][(][w][)][ is][ β][-smooth w.r.t.][ w][, then][ g][(][⟨][x, w][⟩][)][ is]

_βlog∥x f∥j[θ][2][(]-smooth. Based on this knowledge, we firstly focus on the 1-dimensional case of function[z][)][. As we have derived, we know that]_ _∂θ∂_ _i_ [log][ f][ θ]j [(][z][) =][ δ][ij][ −] _[f][ θ]i_ [(][z][)][. Then the second-]

order gradient is _∂[2]∂θlogi∂θ fj[θ]k[(][z][)]_ = (δik _fk[θ][(][z][)) =][ f][ θ]k_ [(][z][)][ −] [1][ if][ i][ =][ k][, otherwise][ f][ θ]k [(][z][)][. Clearly,]

_∂[2]∂θlogi∂θ fj[θ]k[(][z][)]_ 1, which implies that − log − fj[θ][(][z][)][ is 1-smooth. Thus,][ log][ f][ θ]j [(][⟨][x, θ][i][⟩][)][ is][ ∥][x][∥][2][-smooth,]
_|_ _| ≤_

or l[2]-smooth. Further, _j=1_ _[p]j[s,a]_ log fj[θ][(][x][(][s][))][ is also][ l][2][-smooth as we have]

_k_ _k_

[P][k]

_θi_ _p[s,a]j_ log fj[θ][(][µ][)][ −∇][θ]i _p[s,a]j_ log fj[θ][(][ν][)][∥]
_∥∇_

_j=1_ _j=1_

X X


_≤_ _j=1_ _p[s,a]j_ _[∥∇][θ]i_ [log][ f][ θ]j [(][µ][)][ −∇][θ]i [log][ f][ θ]j [(][ν][)][∥]

X

_k_

_p[s,a]j_ _l[2]_ _µ_ _ν_

_≤_ _·_ _∥_ _−_ _∥_

_j=1_

X

= l[2]∥µ − _ν∥_

for each µ and ν. Therefore, we further have


(33)

(34)


_p[s,a]j_ log fj[θ][(][µ][)][ −∇][θ]
_j=1_

X


_p[s,a]j_ log fj[θ][(][ν][)][∥]
_j=1_

X


_θ_
_∥∇_


_p[s,a]j_ log fj[θ][(][µ][)][ −∇][θ]i
_j=1_

X


_p[s,a]j_ log fj[θ][(][ν][)][∥]
_j=1_

X


_θi_
_∥∇_
_i=1_

X


_≤_ _l[2]∥µ −_ _ν∥_

_i=1_

X

= kl[2]∥µ − _ν∥_

Finally, we conclude that _θ(s, a) is kl[2]-smooth._
_L_


-----

G PROOF OF THEOREM 2

_Proof. Consider the stochastic gradient descent rule as Gλ,L(θ) = θ−λ∇θL(θ). Firstly, we provide_
two definitions about Lθ for the following proof.

**Definition 3. (σ-bounded) An update rule is σ-bounded if supθ** _θ_ _λ_ _θ_ (θ) _σ._
_∥_ _−_ _∇_ _L_ _∥≤_

**Definition 4. (η-expansive) An update rule is η-expansive if supv,w** _∥Gλ,L(vu)−Gwλ,L(w)∥_ _η._

_∥_ _−_ _∥_ _≤_

**Lemma 3. (Grow Recursion, Lemma 2.5 (Hardt et al., 2016)) Fix an arbitrary sequence of updates**
_G1, ..., GT and another sequence G[′]1[, ..., G][′]T_ _[. Let][ θ][0][ =][ θ]0[′]_ _[be the starting point and define][ δ][t]_ [=]
_∥θi[′]_ _[−]_ _[θ][t][∥][, where][ θ][t][ and][ θ]t[′]_ _[are defined recursively through]_

_θt+1 = Gλ,_ (θt), θt[′]+1 [=][ G]λ,[′] _t[)]_
_L_ _L[(][θ][′]_

_Then we have the recurrence relation:_

_ηδt_ _Gt = G[′]t_ _[is][ η][-expansive]_
_δt+1 ≤_ min(η, 1)δt + 2σt _Gt and G[′]t_ _[are][ σ][-bounded][, G][t]_ _[is][ η][ expansive]_


**Lemma 4. (Lipschitz Continuity) Assume** _θ is L-Lipschitz, the gradient update Gλ,_ _is (λL)-_
_L_ _L_
_bounded._

_Proof. ∥θ −_ _Gλ,L(θ)∥_ = ∥λ∇θL(θ)∥≤ _λL_

**Lemma 5. (Lipschitz Smoothness) Assume Lθ is β-smooth, then for any λ ≤** _β[2]_ _[, the gradient update]_

_Gλ,_ _is 1-expansive._
_L_

_Proof. Please refer to Lemma 3.7 in (Hardt et al., 2016) for the proof._


Based on all the results above, we start to prove Theorem 2. Our proof is largely based on (Hardt
et al., 2016), but it is applicable in distributional RL setting as well as considering desirable properties of histogram distributional loss. According Lemma 1, we attain that _θ is kl-Lipschitz as well_
_L_ 2
as kl[2]-smooth, and thus based on Lemma 4, Gλ,L is (λkl)-bounded, and 1-expansive if λ ≤ _kl[2][ .]_

In the step t, SGD selects samples that are both in S and S[′], with probability 1 − _n[1]_ [. In this case,]

ples selected are different with probabilityGt = G[′]t[, and thus][ δ][t][+1] _[≤]_ _[δ][t]_ [as][ G][t] [is 1-expansive based on Lemma 3. The other case is that sam-]n[1] [, where][ δ][t][+1][ ≤] _[δ][t][ + 2][λ][t][kl][ based on Lemma 3. Thus, if]_

_λt ≤_ _kl2[2][ we have:]_

E (θT ; x) (θT[′] [;][ x][)][| ≤] _[kl][E][ [][δ][T]_ []][,][ where][ δ][T] [=][ ∥][θ][T] _T_
_|L_ _−L_ _[−]_ _[θ][′]_ _[∥]_

_kl_ (1
_≤_ _−_ _n[1]_ [)][E][ [][δ][T][ −][1][] + 1]n [E][ [][δ][T][ −][1][] + 2][λ][T]n[ −][1][kl]
 

= kl E [δT 1] + [2][λ][T][ −][1][kl]
_−_ _n_
 


_T −1_

_t=0_

X


(35)


2λtkl


E [δ0] +


= kl


_T −1_

_t=0_

X


_≤_ [2][k]n[2][l][2]

= [4][kT]


_kl[2]_


Since this bounds hold for all S, S[′] and x, we attain the uniform stability in Definition 1 for our
histogram distributional loss applied in distributional RL.

Define the population risk as:
_R [θ] = ExL(θ; x)_


-----

and the empirical risk as:


_RS [θ] = [1]_


(θ; xi)
_L_
_i=1_

X


According to Theorem 2.2 in (Hardt et al., 2016), if an algorithm M is ϵstab-uniformly stable, then
the generalization gap is ϵstab-bounded, i.e.,

_|ES,A [RS[M(S)] −_ _R[M(S)]]| ≤_ _ϵstab_

H PROOF OF LEMMA 2

E(s,a)∼ρπ _∥∇Lθ(p[s,a], qθ[s,a][))][ −∇][G][(][θ][)][∥][2][]_ _≤_ (1 − _ϵ)σ[2]_ + ϵκσ[2]. (36)

_Proof. As we know that p[s,a][]_ (x) = (1 − _ϵ)δ{x=E[Zπ(s,a)]}(x) + ϵµ[s,a](x), then we have:_

_∇Lθ(p[s,a], qθ[s,a][) = (1][ −]_ _[ϵ][)][∇L][θ][(][δ][{][x][=][E][[][Z][π][(][s,a][)]][}][, q]θ[s,a][) +][ ϵ][∇L][θ][(][µ][s,a][, q]θ[s,a][)]_

Therefore,

E(s,a)∼ρπ _∥∇Lθ(p[s,a], qθ[s,a][))][ −∇][G][(][θ][)][∥][2][]_

_≤_ E(s,a)∼ρπ (1 − _ϵ)∥∇Lθ(δ{x=E[Zπ(s,a)]}, qθ[s,a][))][ −∇][G][(][θ][)][∥][2][ +][ ϵ][∥∇L][θ][(][µ][s,a][, q]θ[s,a][))][ −∇][G][(][θ][)][∥][2][]_

[]

= (1 − _ϵ)σ[2]_ + ϵκσ[2],

[] (37)

where the first inequality uses the triangle inequality of norm, i.e.,ϵ)∥a∥[2] + ϵ∥b∥[2], and the last equality uses the definition of the variance of ∥(1 L −θ(ϵδ){ax +=E ϵ[Zbπ∥(s,a[2] )]≤}, q(1θ[s,a] −[)]
and Lθ(µ[s,a], qθ[s,a][)][.]

I PROOF OF THEOREM 3


_Proof. (1) If we only consider the expectation of Z_ _[π](s, a), the entropy-regularized MLE would_
degenerate to the pure MLE regarding δ{x=E[Zπ(s,a)]}. As Lθ(δ{x=E[Zπ(s,a)]}, qθ[s,a][)][ is][ kl][2][-smooth,]
we have
_G(θt+1)_ _G(θt)_
_−_

_≤⟨∇G(θt), θt+1 −_ _θt⟩_ + _[kl]2[2]_ _[∥][θ][t][+1][ −]_ _[θ][t][∥][2]_ (38)

= −λ _∇G(θt), ∇Lθ(δ{x=E[Zπ(s,a)]}, qθ[s,a][)]_ + _[kl][2]2[λ][2]_ _∥∇Lθ(δ{x=E[Zπ(s,a)]}, qθ[s,a][)][∥][2]_

where the last first equation is according to the definition of Lipschitz-smoothness, and the last
second one is based on the updating rule of θ. Next, we take the expectation on both sides,

E [G(θt+1) − _G(θt)]_

_≤−λE_ _∥∇G(θt)∥[2][]_ + _[kl][2]2[λ][2]_ E _∥∇Lθ(δ{x=E[Zπ(s,a)]}, qθ[s,a][)][ −∇][G][(][θ][t][) +][ ∇][G][(][θ][t][)][∥][2][]_
 

_≤−λE_ ∥∇G(θt)∥[2][] + _[kl][2]2[λ][2]_ E ∥∇Lθ(δ{x=E[Zπ(s,a)]}, qθ[s,a][)][ −∇][G][(][θ][t][)][∥][2][] + _[kl][2]2[λ][2]_ E ∥∇G(θt)∥[2][]

= _[λ][(][kl][2][λ][ −]_ [2)] E _G(θt)_ + _[kl][2][λ][2]_ _σ[2]_

2 ∥∇ _∥[2][]_ 2

_G(θt)_ + _[kl][2][λ][2]_ _σ[2]_

_≤−_ _[λ]2_ [E] _∥∇_ _∥[2][]_ 2

(39)



where the first two equation hold because _G(θ) = E [_ _θ] and the last inequality comes from_
1 _∇_ _∇L_
_λ ≤_ _kl[2][ . Through the summation, we obtain that]_

_T −1_

E [G(θT ) _G(θ0)]_ E _G(θt)_ + _[kl][2][λ][2][T]_ _σ[2]_
_−_ _≤−_ _[λ]2_ _∥∇_ _∥[2][]_ 2

_t=0_

X 


-----

We let E [G(θT )] = 0, we have

1 _T −1_

E _G(θt)_ + kl[2]λσ[2]

_T_ _∥∇_ _∥[2][]_ _≤_ [2][G]λT[(][θ][0][)]

_t=0_

X 

By setting λ ≤ 2klτ [2][2]σ [and][ T][ =][ 4][G]λτ[(][θ][2][0][, we can have][)] [ 1]T _Tt=0 −1_ [E] _∥∇G(θt)∥[2][]_ _≤_ _τ_ [2], implying that

the degenerated MLE can achieve τ -station point if the sample complexity T = O( _τ[1][4][ )][.]_
P 

(2) and (3) We are still based on the kl[2]-smoothness of L(p[s,a], qθ[s,a][)][.]

_G(θt+1)_ _G(θt)_
_−_

_G(θt), θt+1_ _θt_ + _[kl][2]_
_≤⟨∇_ _−_ _⟩_ 2 _[∥][θ][t][+1][ −]_ _[θ][t][∥][2]_

= _λ_ _G(θt),_ _θ(p[s,a], qθ[s,a][)][⟩]_ [+][ kl][2][λ][2] _θ(p[s,a], qθ[s,a][)][∥][2]_ (40)
_−_ _⟨∇_ _∇L_ 2 _∥∇L_

= _θ_ [)][∥][2][ +][ λ][(][kl][2][λ][ −] [1)] _θ(p[s,a], qθ[s,a][)][∥][2]_
_−_ _[λ]2_ _[∥∇][G][(][θ][t][)][∥][2][ +][ λ]2_ _[∥∇][G][(][θ][t][)][ −∇L][θ][(][p][s,a][, q][s,a]_ 2 _∥∇L_

_θ_ [)][∥][2]

_≤−_ _[λ]2_ 2

_[∥∇][G][(][θ][t][)][∥][2][ +][ λ]_ _[∥∇][G][(][θ][t][)][ −∇L][θ][(][p][s,a][, q][s,a]_

where the second equation is based on **a,** **b** = [1]2 **a** **b** **a** **b**, and the last in
equality is according to λ _kl1[2][ . After taking the expectation, we have] ⟨_ _−_ _⟩_ _∥_ _−_ _∥[2]_ _−∥_ _∥[2]_ _−∥_ _∥[2][]_
_≤_  

E [G(θt+1) − _G(θt)]_

_≤−_ _[λ]2_ [E] _∥∇G(θt)∥[2][]_ + _[λ]2_ [E] _∥∇G(θt) −∇Lθ(p[s,a], qθ[s,a][)][∥][2][]_ (41)

 

_G(θt)_ + _[λ]_ (1 _ϵ)σ[2]_ + ϵκσ[2][]

_≤−_ _[λ]2_ [E] _∥∇_ _∥[2][]_ 2 _−_

where the last inequality is based on Lemma 2. We take the summation, and therefore,  


_T −1_

E [G(θT ) _G(θ0)]_ E _G(θt)_ + _[Tλ]_
_−_ _≤−_ _[λ]2_ _∥∇_ _∥[2][]_ 2

_t=0_

X 

We let E [G(θT )] = 0 and ϵ = 1+1 _κ_ [, then,]

1 _T −1_

E _G(θt)_

_T_ _∥∇_ _∥[2][]_

_t=0_

X 

+ (1 _ϵ)σ[2]_ + ϵκσ[2]

_≤_ [2][G]λT[(][θ][0][)] _−_


(1 _ϵ)σ[2]_ + ϵκσ[2][]
_−_


(42)


= [2][G][(][θ][0][)] + 2κ

_λT_ 1 + κ _[σ][2]_

+ 2κσ[2]

_≤_ [2][G]λT[(][θ][0][)]

with the sample complexity asIf κ ≤ 4τσ[2][2][ and let][ T][ =][ 4][G]λτ[(][θ][2][0][, this leads to][)] O( _τ[1][2][ )][. Thus, (2) has been proved. On the other hand, if][ 1]T_ PTt=0 −1 [E] ∥∇G(θt)∥[2][] _≤_ _τ_ [2], i.e., τ -stationary point,[ κ >] 4τσ[2][2][, we]

set T = _[G]λκσ[(][θ][0][2][)][ . This implies that][ 1]T_ _tT=0 −1_ [E] _∥∇G(θt)∥[2][]_ _≤_ 4κσ[2] = O(κ). Therefore, the degree

of stationary point is determined the degree of distribution approximation measured by κ. Thus, we
obtain (3). P 


J PROOF OF THEOREM 4

_Proof. 1. As ε →_ 0, it is obvious to observe that Sinkhorn loss degenerates to the wasserstein
distance.


-----

2. As ε, Sinkhorn loss turns to be log duΠ((x)dx,yv)(y) dΠ(x, y). We can consider its dual
_→∞_

problem by introducing Lagrange multipliers u and _v. By additionally considering the primal-dual_

R

relation, we can solve the dual problem, which gives u = v = 0, and thus the optimal coupling
is simply the product of the marginals, ie., Π = u ⊗ _v. Please refer to more detailed proof in_
Sinkhorn-GAN (Genevay et al., 2018). Since the cost function is the rectified kernel kα, under with
MMD (Nguyen et al., 2020) can be γ[α/][2]-contractive.

K REPRESENTATION EFFECT OF DISTRIBUTION RL

From the perspective of representation, we find that distributional RL encourages state representation from the same action class classified by the policy in tighter clusters.

The intrinsic characteristic of distributional RL is that it enables to learn
a richer and more faithful representation of the environment, leading to
more stable and efficient learning. To
investigate the more informative representation resulting from distribu
(a) DQN (b) C51 (c) QRDQN

tional RL, we visualize how distributional RL changes the representation

Figure 4: Visualization on the penultimate layer of the value

learned in the penultimate layer of the

network on Breakout. Each color denotes one action class.

value network. We collect state features in the penultimate layer classified by the policy π in different action classes, and perform t-SNE
to reduce them to the two-dimensional space. In Figure 4, points in different colors represent state
features classified into different action classes. It illustrates that both QR-DQN and C51 encourage
the representation of state observations from the same action class to group in tighter clusters relative to expectation-based DQN. Interestingly, this phenomenon is similar to label smoothing (M¨uller
et al., 2019), which leverages additional distributional knowledge in soft labels relative to hard labels. Therefore, we conclude that similar to the benefit of label smoothing, distributional RL also
_encourages the activations of the penultimate layer to be close to the template of the correct action_
_class and distant to the templates of the incorrect action classes._

L MORE EXPERIMENTAL RESULTS


-----

YarsRevenge

### Alien

DQN 2250

20000 C51QRDQN 2000 DQNC51

MMD 1750 QRDQN

15000 Sinkhorn 1500 MMD

1250 Sinkhorn

10000 1000

750

Average Return 5000 Average Return 500

250

0

0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8 1.0 1.2

Time Steps (1e7) Time Steps (1e7)

UpNDown

### Qbert 25000 DQN

16000 C51

14000 DQNC51 20000 QRDQNMMD

12000 QRDQN Sinkhorn

10000 MMD 15000

Sinkhorn

8000

6000 10000

4000 Average Return

5000

Average Return 2000

0 0

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8

Time Steps (1e7) Time Steps (1e7)


Figure 5: Performance of SinkhornDRL on YarRevenge, Aline, Qbert and UpNDown.


-----

