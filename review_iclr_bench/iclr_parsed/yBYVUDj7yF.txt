# THE POWER OF CONTRAST FOR FEATURE LEARNING: A THEORETICAL ANALYSIS

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Contrastive learning has achieved state-of-the-art performance in various selfsupervised learning tasks and even outperforms its supervised counterpart. Despite its empirical success, theoretical understanding of why contrastive learning
works is still limited. In this paper, under linear representation settings, (i) we
provably show that contrastive learning outperforms autoencoder, a classical unsupervised learning method, for both feature recovery and downstream tasks; (ii)
we also illustrate the role of labeled data in supervised contrastive learning. This
provides theoretical support for recent findings that contrastive learning with labels improves the performance of learned representations in the in-domain downstream task, but it can harm the performance in transfer learning. We verify our
theory with numerical experiments.

1 INTRODUCTION

Deep supervised learning has achieved great success in various applications, including computer
vision (Krizhevsky et al., 2012), natural language processing (Devlin et al., 2018), and scientific
computing (Han et al., 2018). However, its dependence on manually assigned labels, which is
usually difficult and costly, has motivated research into alternative approaches to exploit unlabeled
data. Self-supervised learning is a promising approach that leverages the unlabeled data itself as
supervision and learns representations that are beneficial to potential downstream tasks.

At a high level, there are two common approaches for feature extraction in self-supervised learning:
generative and contrastive (Liu et al., 2021). Both approaches aim to learn latent representations
of the original data, while the difference is that the generative approach focused on minimizing the
reconstruction error from latent representations, and the contrastive approach targets to decrease the
similarity between the representations of contrastive pairs. Recent works have shown the benefits of
contrastive learning in practice (Chen et al., 2020a; He et al., 2020; Chen et al., 2020b;c). However,
why the contrastive approach outperforms the generative approach remains mysterious.

Additionally, recent works aim to further improve contrastive learning by introducing the label information. Specifically, Khosla et al. (2020) proposed the supervised contrastive learning, where the
contrasting procedures are performed across different classes rather than different instances. With
the help of label information, their proposed method outperforms self-supervised contrastive learning and classical cross entropy based supervised learning. However, despite this improvement on
in-domain downstream tasks, Islam et al. (2021) found that such improvement in transfer learning
is limited and even negative for such supervised contrastive learning. This phenomenon motivates
us to rethink the role of labeled data in the contrastive learning framework.

In this paper, we first compare contrastive learning with a representative method in the generative
approach – the autoencoders. Specifically, we initialize the investigation in the linear representation setting, which has been widely adopted in theory to shed light upon complex machine learning phenomena such as in Du et al. (2020); Tripuraneni et al. (2021). We provide a theoretical
analysis of their feature learning performances on the spiked covariance model (Bai & Yao, 2012;
Yao et al., 2015; Zhang et al., 2018) and theoretically justify why contrastive learning outperforms
autoencoders—contrastive learning is able to remove more noises by constructing contrastive samples. Then we investigate the role of label information in the contrastive learning framework and
provide a theoretical justification of why labeled data help to gain accuracy in same-domain classification while can hurt multi-task transfer learning.


-----

**Related works** The idea of contrastive learning was first proposed in Hadsell et al. (2006) as
an effective method to perform dimension reduction. Following this line of research, Dosovitskiy
et al. (2014) proposed to perform instance discrimination by creating surrogate classes for each
instance and Wu et al. (2018) further proposed to preserve a memory bank as a dictionary of negative
samples. Other extensions based on this memory bank approach include He et al. (2020); Misra &
Maaten (2020); Tian et al. (2020); Chen et al. (2020c). Rather than keeping a costly memory bank,
another line of works exploits the benefit of mini-batch training where different samples are treated
as negative to each other Ye et al. (2019); Chen et al. (2020a). Moreover, Khosla et al. (2020)
explores the supervised version of contrastive learning where pairs are generated based on label
information.

Despite its success in practice, theoretical understanding of contrastive learning is still limited. Previous works provide provable guarantees for contrastive learning under conditional independence
assumption (or its variants) (Arora et al., 2019; Lee et al., 2020; Tosh et al., 2021; Tsai et al., 2020).
Specifically, they assume the two contrastive views are independent conditioned on the label and
show that contrastive learning can provably learn representations beneficial for downstream tasks.
In addition to this line of research, Wang & Isola (2020); Graf et al. (2021) investigated the representation geometry of supervised contrastive loss, and HaoChen et al. (2021) provided analysis via a
novel concept of augmentation graph with a new loss function that performs spectral decomposition
on such graph. Moreover, Wen & Li (2021) considered the representation learning under the sparse
coding model and studied the optimization properties on shallow ReLU neural networks. Different
from all previous works, which aim to show that contrastive learning can learn useful representation, our paper aim to explain why contrastive learning outperforms other representation learning
methods and also shed light on the role of labeled data in contrastive learning framework, which is
under-explored in prior works.

2 PRELIMINARIES

**Notations** In this paper, we use O, Ω, Θ to hide universal constants and we write ak ≲ _bk for_
two sequences of positive numbers _ak_ and _bk_ if and only if there exists an universal constant
_{_ _}_ _{_ _}_
_C > 0 such that ak < Cbk for any k. We use ∥· ∥, ∥· ∥2, ∥· ∥F to represent the ℓ2 norm of_
vectors, spectral norm of matrices and Frobenius norm of matrices respectively. Let Od,r be a set
of d × r orthogonal matrices. i.e., Od,r ≜ _{U ∈_ R[d][×][r] : U _[⊤]U = Ir}. We use |A| to denote_
the cardinality of a set A. For any n ∈ N[+], let [n] = {1, 2, · · ·, n}. We use ∥ sin Θ(U1, U2)∥F
to refer to the sine distance between two orthogonal matrices U1, U2 ∈ Od,r, which is defined by:
_∥sin Θ (U1, U2)∥F ≜_ _U1[⊤]⊥[U][2]_ _F_ [. More properties of sine distance can be found in Section A.1.]
We use {ei}i[d]=1 [to denote the canonical basis in][ d][-dimensional Euclidean space][ R][d][, that is,][ e][i][ is]
the vector whose i-th coordinate is 1 and all the other coordinates are 0. Let I{A} be an indicator
function that takes 1 when A is true, otherwise takes 0. We write a _∨_ _b and a_ _∧_ _b to denote max(a, b)_
and min(a, b), respectively.

2.1 SETUP

Given an input x ∈ R[d], contrastive learning aims to learn a low dimensional representation h =
_f_ (x; θ) ∈ R[r] by contrasting different samples, i.e., maximizing the agreement between positive
pairs, and minimizing the agreement between negative pairs. Suppose we have n data points X =

[x1, x2, · · ·, xn] ∈ R[d][×][n] from the population distribution D. The contrastive learning task can be
formulated to be an optimization problem:


_ℓ(xi,_ _i_ _,_ _i_ ; f ( _, θ)) + λR(θ),_ (2.1)
_B[P os]_ _B[Neg]_ _·_
_i=1_

X


min (θ) = min
_θ_ _L_ _θ_


where ℓ( ) is a contrastive loss and λR(θ) is a regularization term; _i_ _,_ _i_ are the sets of

_·_ _B[P os]_ _B[Neg]_
positive samples and negative samples corresponding to xi, which we will describe in detail below.

**Losses and Models.** We then present the model setup considered in this paper.

**_(a). Linear representation and regularization term. We consider the linear representation function_**
_f_ (x, W ) = Wx, where the parameter θ is a matrix W ∈ R[r][×][d]. Since regularization techniques


-----

have been widely adapted in contrastive learning practice (Chen et al., 2020a; He et al., 2020; Grill
et al., 2020), we further consider penalizing the representation by a regularization term R(W ) =
_∥WW_ _[⊤]∥F[2]_ _[/][2][ to encourage the orthogonality of][ W][ and therefore promote the diversity of][ w][i][ to]_
learn different representations.

**_(b). Triplet Contrastive loss. The contrastive loss is set to be the average similarity between positive_**
pairs minus that between negative pairs:

_f_ (x, θ), f (x[P os], θ) _f_ (x, θ), f (x[Neg], θ)

_ℓ(x,_ _,_ _, f_ ( _, θ)) =_ _⟨_ _⟩_ + _⟨_ _⟩_ _,_
_B[P os]_ _B[Neg]_ _·_ _−_

_x[P os]X∈B[P os]_ _|B[P os]|_ _x[Neg]X∈B[Neg]_ _|B[Neg]|_

(2.2)
where B[P os], B[neg] are sets of positive samples and negative samples corresponding to x. This loss
has been commonly used in constrastive learning (Hadsell et al., 2006) and metric learning (Schroff
et al., 2015; He et al., 2018). In Khosla et al. (2020), the authors show that it is an approximation
of the NT-Xent contrastive loss, which has been highlighted in recent contrastive learning practice
(Sohn, 2016; Wu et al., 2018; Oord et al., 2018; Chen et al., 2020a).

**_(c). Generation of positive and negative pairs. There are two common approaches to generate such_**
pairs, depending on whether or not label information is available. When the label information is
not available, the typical strategy is to generate different views of the original data via augmentation
(Hadsell et al., 2006; Chen et al., 2020a). Two views of the same data point serve as positive pair
for each other while those of different data serve as negative pairs.
**Definition 2.1 (Augmented pairs generation). Given two augmentation functions g1, g2 : R[d]** _→_ R[d]
and n training samples B = {xi}i∈[n], the augmented views are given by: {(g1(xi), g2(xi))}i∈[n].
Then for each view gv(xi), v = 1, 2, the corresponding positive samples and negative samples are
defined by: _i,v_ = _gs(xi) : s_ [2] _v_ and _i,v_ = _gs(xj) : s_ [2], j [n] _i_ .
_B[P os]_ _{_ _∈_ _\ {_ _}}_ _B[Neg]_ _{_ _∈_ _∈_ _\ {_ _}}_

The loss function of self-supervised contrastive learning problem can be written as:


_n_ 2 2

SelfCon(W ) = _Wgv(xi), Wg[2]_ _v_ (xi) _⟨Wgv(xi), Wgs(xj)⟩_ + _[λ]_ _F_ _[.]_
_L_ _−_ 2[1]n _⟨_ _\{_ _}_ _⟩−_ 2n 2 2

Xi=1 Xv=1 Xj≠ _i_ Xs=1 _−_  _[∥][WW][ ⊤][∥][2]_

(2.3)
In particular, we adopt the following augmentation in our analysis.

**Definition 2.2 (Random masking augmentation). The two views of the original data are generated**
by randomly dividing its dimensions to two sets, that is, g1(xi) = Axi, and g2(xi) = (I _A)xi,_
_−_
where A = diag(a1, · · ·, ad) ∈ R[d][×][d] is the diagonal masking matrix with {ai}i[d]=1 [being][ i.i.d.]
random variables sampled from a Bernoulli distribution with mean 1/2.

A similar augmentation was considered in Wen & Li (2021). However, our primary interest lies in
comparing the performance of contrastive learning against autoencoders and analyzing the role of
labeled data, while their work focuses on understanding the training process of neural networks in
contrastive learning.

When the label information is available, Khosla et al. (2020) proposed the following approach to
generate pairs.
**Definition 2.3 (Supervised pairs generation). In a K-class classification problem, given nk samples**
for each class k ∈ [K]: {x[k]i [:][ i][ ∈] [[][n][k][]][}]k[K]=1 [and let][ n][ =][ P]k[K]=1 _[n][k][, the corresponding positive]_
samples and negative samples for x[k]i [are defined by][ B]i,k[P os] = {x[k]j [:][ j][ ∈] [[][n][k][]][ \][ i][}][ and][ B]i,k[Neg] = {x[s]j [:]
_s_ [K] _k, j_ [ns] . That is, the positive samples are the remaining ones in the same class with
_∈_ _\_ _∈_ _}_
_x[k]i_ [and the negative samples are the samples from different classes.]

Correspondingly, the loss function of the supervised contrastive learning problem can be written as:


Xj≠ _i_


_⟨Wx[k]i_ _[, Wx]j[k][⟩]_

_n −_ 1


_⟨Wx[k]i_ _[, Wx]j[s][⟩]_

_n(K −_ 1)


SupCon(W ) =
_L_ _−_ _nK[1]_


+ _[λ]_ _F_ _[.][ (2.4)]_

2

_[∥][WW][ ⊤][∥][2]_


_k=1_


_i=1_


_j=1_


_s≠_ _k_


**_(d). Spiked Covariance Model. We consider the following spiked covariance model (Bai & Yao,_**
2012; Yao et al., 2015; Zhang et al., 2018) to study the power of contrastive learning:
_x = U_ _[⋆]z + ξ,_ Cov(z) = ν[2]Ir, Cov(ξ) = Σ, (2.5)


-----

where z ∈ R[r] and ξ ∈ R[d] are both zero mean sub-Gaussian random variables. In particular,
_U_ _[⋆]_ _∈_ Od,r and Σ = diag(σ1[2][,][ · · ·][, σ]d[2][)][. The first term][ U][ ⋆][z][ represents the signal of interest residing]
in a low-dimensional subspace spanned by the columns of U _[⋆]. The second term ξ is the dense noise_
with heteroskedastic noise. Given that, the ideal low-dimensional representation is to compress the
observed x into a low-dimensional representation spanned by the columns of U _[⋆]._

In this paper, we aim to learn a good projection W ∈ R[r][×][d] **_onto a lower-dimensional subspace_**
**_from observation x. Since the information of W is invariant with the transformation W ←_** _OW_
for any invertible matrix O ∈ Rr,r, the essential information of W is contained in the right
eigenvector of W . Thus we quantify the goodness of the representation W by the sine distance
_∥_ sin Θ(U, U _[⋆])∥F, where U is the top-r right eigenspace of W_ .

3 SELF-SUPERVISED CONTRASTIVE LEARNING VERSUS AUTOENCODER

Autoencoder and contrastive learning are two popular approaches for self-supervised learning. Recent experiments have highlighted the improved performance of contrastive learning compared with
autoencoders. In this section, we rigorously demonstrate the advantage of contrastive learning over
autoencoder by investigating the linear representation settings under the spiked covariance model
Eq.(2.5). The investigation is conducted for both feature recovery and downstream tasks.

3.1 RECOVER FEATURES FROM NOISY DATA

Here we focus on the analysis of feature recovery to understand the benefit of contrastive learning over autoencoders. As mentioned above, our target is to recover the subspace spanned by the
columns of U _[⋆], which can further help us obtain information on the unobserved z that is important_
for downstream tasks. However, the observed data has covariance matrix of ν[2]U _[⋆]U_ _[⋆][⊤]_ + Σ rather
than the desired ν[2]U _[⋆]U_ _[⋆][⊤], which brings difficulty to representation learning. We demonstrate that_
contrastive learning can better exploit the structures of core features and obtain better estimation
than an autoencoder in this setting.

We start with autoencoders. Formally, an autoencoder consists of an encoder f _[AE]_ : R[d] _→_ R[r] and
a decoder g[DE] : R[r] _→_ R[d]. While the encoder compresses the original data into low dimensional
features, and the decoder recovers the original data from those features. It can be formalized as the
following optimization problem for samples {xi}i[n]=1 [(Ballard, 1987; Fan et al., 2019):]


_i=1_ _∥xi −_ _g[DE](f_ _[AE](xi))∥2[2][.]_

X


min
_f_ _[AE]_ _,g[AE]_


In the linear representation setting, where f _[AE](x) = WAEx + bAE and g[DE](y) = WDEy + bDE,_
previous works (Bourlard & Kamp, 1988; Plaut, 2018) have shown that the autoencoder can be
reduced to principal component analysis (PCA). That is, the optimal WAE is given by:

_WAE = (UAEΣAEVAE[⊤]_ [)][⊤][,] (3.1)

where UAE is the top-r eigenvectors of matrix M := X(I 1n1[⊤]n _[/n][)][X]_ _[⊤][,][ Σ][AE]_ [is a diagonal matrix]
_−_
consists of eigenvalues of M and VAE = [v1, · · ·, vn] ∈ R[r][×][r] can be any orthonormal matrix. In
the noiseless case, the covariance matrix is ν[2]U _[⋆]U_ _[⋆][⊤]_ and autoencoder can perfectly recover the core
features. However, in noisy cases, the random noises sometimes perturb the core features, which
make autoencoders fail to learn core features. Such noisy cases are very common in real applications
such as measurement errors and backgrounds in images such as grasses and sky. Interestingly, we
will later show that contrastive learning can better recover U _[⋆]_ despite the presence of large noise.

To provide rigorous analysis, we first introduce the incoherent constant (Cand`es & Recht, 2009).

**Definition 3.1 (Incoherent constant). We define the incoherence constant of U ∈** Od,r as

_I(U_ ) = max _e[⊤]i_ _[U]_ _._ (3.2)
_i∈[d]_

[2]

Intuitively, the incoherent constant measures the degree of the incoherence of the distribution of
entries among different coordinates, or loosely speaking, the similarity between U and canonical


-----

basis {ei}i[d]=1[. For uncorrelated random noise, the covariance matrix is diagonal and its eigenspace]
is exactly spanned by the canonical basis {ei}i[d]=1 [(if the diagonal entries in][ Σ][ are all different),]
which attains the maximum value of incoherent constant. On the contrary, core features usually
exhibit certain correlation structures and the corresponding eigenspace of the covariance matrix is
expected to have a lower incoherent constant.

We then introduce a few assumptions where our theoretical results are built on. Recall that in the
spiked covariance model Eq.(2.5), x = U _[⋆]z + ξ, Cov(z) = ν[2]Ir and Cov(ξ) = diag(σ1[2][,][ · · ·][, σ]d[2][)][.]_

_Assumption 3.1 (Regular covariance condition). The condition number of covariance matrix Σ =_
diag(σ1[2][,][ · · ·][, σ]d[2][)][ satisfies][ κ][ :=][ σ](1)[2] _[/σ]([2]d)_ _[< C,][ where][ σ]([2]j)_ [represents the][ j][-th largest number]
among σ1[2][,][ · · ·][, σ]d[2] [and][ C >][ 0][ is a universal constant.]

_Assumption 3.2 (Signal to noise ratio condition). Define the signal to noise ratio ρ := v/σ(1), we_
assume ρ = Θ(1), implying that the covariance of noise is of the same order as that of core features.
_Assumption 3.3 (Incoherent condition). The incoherent constant of the core feature matrix U_ _[⋆]_ _∈_
Od,r satisfies I(U _[⋆]) = O(r log d/d)._ [1]

_Remark 3.1. Assumption 3.1 implies that the variances of all dimensions are of the same order._
For Assumption 3.2, we focus on a large-noise regime where the noise may hurt the estimation
significantly. Here we assume the ratio lies in a constant range, but our theory can easily adapt
to the case where ρ has a decreasing order. Assumption 3.3 implies a stronger correlation among
coordinates of core features, which is the essential property to distinguish them from random noise.

Now we are ready to present our first result, showing that the autoencoder is unable to recover the
core features in the large-noise regime.

**Theorem 3.1 (Recovery ability of autoencoder, lower bound). Consider the spiked covariance**
_model Eq.(2.5), under Assumption 3.1-3.3 and n > d ≫_ _r, let WAE be the learned representa-_
_tion of autoencoder with singular value decomposition WAE = (UAEΣAEVAE[⊤]_ [)][⊤] _[(as in Eq.(3.1)).]_
_If we further assume {σi[2][}]i[d]=1_ _[are different from each other and][ σ](1)[2]_ _[/][(][σ]([2]r)_ _[−]_ _[σ]([2]r+1)[)][ < C][σ][ for some]_
_universal constant Cσ. Then there exist two universal constants Cρ > 0, c ∈_ (0, 1), such that when
_ρ < Cρ, we have_
E sin Θ (U _[⋆], UAE)_ _F_ _c[√]r._ (3.3)
_∥_ _∥_ _≥_

_Remark 3.2. The additional assumptions {σi[2][}]i[d]=1_ [are different from each other and][ σ](1)[2] _[/][(][σ]([2]r)_ _[−]_
_σ([2]r+1)[)][ < C][σ][ for some universal constant][ C][σ][ are for technical consideration. We need these condi-]_
tions to guarantee the uniqueness of UAE. As an extreme example, the top-r eigenspace of identity
matrix can be any r-dimensional subspace and thus not unique. To avoid discussing such arbitrariness of output, we make these assumptions to guarantee the separability of eigenspace.

Then we investigate the feature recovery ability of the self-supervised contrastive learning approach.

**Theorem 3.2 (Recovery ability of contrastive learning, upper bound). Under the spiked covariance**
_model Eq.(2.5), random masking augmentation in Definition 2.2, Assumption 3.1-3.3 and n > d ≫_
_r, let WCL be any solution that minimizes Eq.(2.3), and denote its singular value decomposition as_
_WCL = (UCLΣCLVCL[⊤]_ [)][⊤][, then we have]


E ∥sin Θ (U _[⋆], UCL)∥F ≲_ _[r][3]d[/][2]_


_dr_

_n [.]_ (3.4)


log d +


_Remark 3.3. In Eq.(3.4), the first term is due to the shift between the distributions of the augmented_
data and the original data. Specifically, the random masking augmentation generates two views with
disjoint non-zero coordinates and thus can mitigate the influence of random noise on the diagonal
entries in the covariance matrix. However, such augmentation slightly hurts the estimation of core
features. This bias, appearing as the first term in Eq.(3.4), is measured by the incoherent constant
Eq.(3.2). The second term corresponds to the estimation error of the population covariance matrix.

Theorem 3.1 and 3.2 characterize the difference of feature recovery ability between autoencoder and
contrastive learning. The autoencoder fails to recover most of the core features in the large-noise

1The order of I(U ⋆) can be chosen to be any function that decreasing to 0 when d →∞ and one can easily
adapt the later results to this setting. Here we set it to O(r log d/d), the order when U is drawn from a uniform
distribution on Od,r (see the proof in Lemma B.1) for simplicity to obtain an exact order in later results.


-----

regime, since ∥ sin Θ(U, U _[⋆])∥F has a trivial upper bound_ _[√]r. In contrast, with the help of data_
augmentation, the contrastive learning approach mitigates the corruption of random noise while
preserving core features. As n and d increase, it yields a consistent estimator of core features and
further leads to better performance in the downstream tasks, as shown in the next section.

3.2 PERFORMANCE ON THE DOWNSTREAM TASK

In the previous section, we have seen that contrastive learning can recover the core feature effectively. In practice, we are interested in using the learned features to downstream tasks. He et al.
(2020) experimentally showed the overwhelming performance of linear classifiers trained on representations learned with contrastive learning against several supervised learning methods in downstream tasks.

Following the recent empirical studies, here we evaluate the downstream performance of simple
predictors, which take a linear transformation of representation as an input. Specifically, we consider
the regression setting with a class of predictors δW,w(x) = w[⊤]Wx constructed upon the learned
representations W = WCL and WAE respectively. Given the observation ˇx = U _[⋆]zˇ + ξ[ˇ] independent_
of unsupervised data X, our prediction target is ˇy generated from a linear model ˇy = ⟨z, wˇ _[⋆]⟩/ν + ˇϵ,_
where ˇz _N_ (0, ν[2]Ir) is the low-dimensional core feature, ν is the scale of ˇz and ˇϵ is the error
_∼_
term independent of ˇz with zero mean and finite variance. w[⋆] _∈_ R[r] is a unit vector of coefficients.
We can interpret this model as a principal component regression model (PCR) (Jolliffe, 1982) under
standard error-in-variables settings[2], where we assume that coefficients lie in a low-dimensional
subspace spanned by column vectors of U _[⋆]. We either estimate or predict the signal based on_
observed samples contaminated by the measurement error _ξ[ˇ]. For details of PCR in error-in-variables_
settings, see, for example, Cevid et al. (2020); Agarwal et al. (2020); Bing et al. (2021).[´]

Now we state our result on the downstream performance in prediction task. For any linear representation W ∈ R[r][×][d], let R(W ) := EX [inf _w∈Rr Ey,ˇ_ _xˇ[[][ℓ][(][δ]W,w[)]][ be the risk of the best linear predictor,]_
where ℓ(δ) = (ˇy − _δ(ˇx))[2]._

**Theorem 3.3 (Upper Bound for Downstream Excess Risk of WCL). Suppose conditions in Theorem**
_3.2 are satisfied. Then we have_


(WCL) (U _[⋆][⊤]) ≲_ _[r][3][/][2]_
_R_ _−R_ _d_


_dr_

_n [.]_


log d +


This result shows that the price of estimating U _[⋆]_ by contrastive learning on a downstream prediction
task can be made small in a case where the core feature lies in a relatively low-dimensional subspace,
and the number of samples is relatively large compared to the ostensible dimension of data.

However, the downstream performance of autoencoders is not as good as contrastive learning. We
obtain the following lower bound for the downstream prediction risk with the autoencoder.
**Theorem 3.4 (Lower Bound for Downstream Excess Risk of WAE). Suppose the conditions in**
_Theorem 3.1 hold. Assume r ≤_ _rc holds for some constant rc > 0. Additionally assume that_
_ρ = Θ(1) is sufficiently small and n ≫_ _d ≫_ _r. Then,_

(WAE) (U _[⋆][⊤])_ _c[′],_
_R_ _−R_ _≥_

_where c[′]_ _> 0 is a constant independent of n and d._

Comparing theorems 3.3 and 3.4, we find that even when r/d and _d/n are small, the downstream_

performance of autoencoders is not satisfying and has a much larger downstream task error rate than

p

that of contrastive learning. We note that the constant lower bound of Theorem 3.4 can be obtained
without the assumption r ≤ _rc by assuming a slightly stronger conditions ρ = O(1/ log d) and_
_n ≫_ _dr. We also illustrate this phenomenon via numerical simulation in Fig.1. As predicted by_
Theorem 3.1 and 3.2, the downstream task risk of contrastive learning decreases as d increases (Fig.
1: Left) and as n increases (Fig. 1: Center) while that of autoencoder remains large when n and d
increase.

2In error-in-variables settings, the bias term from the measurement error appears in prediction and estimation risk. Since our focus lies in proving a better performance of contrastive learning against autoencoders, we
ignore the unavoidable bias term here by considering the excess risk.


-----

Figure 1: The vertical axes indicate the prediction risk. Left: Comparison of downstreak task
performance between contrastive learning and autoencoders the dimension d. The sample size n
is set as n = 20000. Center: Comparison of downstreak task performance between contrastive
learning and autoencoders the dimension n. The dimension d is set as d = 40. Right: Downstream
task performance in transfer learning against penalty parameter α in log scale. T is the number of
source tasks and r is the dimension of the representation function. We set the number of labeled
data and unlabeled data as m = 1000 and n = 1000 respectively.

_Remark 3.4. A similar results of upper and lower bound hold for linear predictors δ(x) =_
I{F (w[⊤]Wx) > 1/2} under binary classification settings, where label is generated from a binary
response model ˇy|zˇ = Ber(F (⟨z, wˇ _[⋆]⟩/ν)) with a known function F : R →_ [0, 1]. Our results imply
that the learned representations by contrastive learning are also useful in downstream classification
tasks, compared to an autoencoder, thus supporting the empirical success of contrastive learning. A
detailed results and proofs are deferred to Appendix B.2.

4 THE IMPACT OF LABELED DATA IN SUPERVISED CONTRASTIVE LEARNING

Recent works have explored adding label information to improve contrastive learning (Khosla et al.,
2020). Empirical results show that label information can significantly improve the accuracy of the
in-domain downstream tasks. However, when domain shift (multiple sources) is considered, the
label information hardly improves and even hurts transferability (Islam et al., 2021). Motivated
by those empirical observations, in this section, we aim to investigate the role of labeled data in
contrastive learning and provide a theoretical foundation for such phenomena.

4.1 FEATURE MINING IN MULTI-CLASS CLASSIFICATION

We first demonstrate the role of label in the single-sourced case in contrastive learning. Suppose
our samples are drawn fromr+1 _r + 1 different classes with probability pk for class k ∈_ [r + 1], and
_k=1_ _[p][k][ = 1][. For each class, samples are generated from a class-specific Gaussian distribution:]_
P _x[k]_ = µ[k] + ξ[k], _ξ[k]_ (0, Σ[k]), _k = 1, 2,_ _, r + 1._ (4.1)

_∼N_ _∀_ _· · ·_

To be consistent with the spiked covariance model Eq.(2.5), we assume ∥µ[k]∥ = _[√]rν, ∀k ∈_ [r + 1].
We further assume Σ[k] = diag(σ1[2],k[,][ · · ·][, σ]d,k[2] [)][, denote][ σ](1)[2] [= max][1][≤][i][≤][d,][1][≤][j][≤][r][+1][ σ]i,j[2] [and assume]
_r+1_
_k=1_ _[p][k][µ][k][ = 0][, where the last assumption is added to ensure identifiable since the classification]_
problem (4.1) is invariant under translation. DenoteP Λ = _k=1_ _[p][k][µ][k][µ]k[⊤][, we assume][ rank(Λ) =][ r]_
and C1ν[2] _< λ(r)(Λ) < λ(1)(Λ) < C2ν[2]_ for two universal constants C1 and C2. We remark that
this model is a labeled version of the spiked covariance model Eq.(2.5) since the core features and

[P][r][+1]
random noise are both sub-Gaussian. We use r +1 classes to ensure that µk’s span an r-dimensional
space, and denote its orthonormal basis as U _[⋆]. Recall that our target is to recover U_ _[⋆]._

As introduced in Definition 2.3, Khosla et al. (2020) proposed a novel approach named supervised
contrastive learning, which allows us to discriminate instances across classes. When we have both
labeled data and unlabeled data, we can perform contrastive learning based on pairs that are generated separately for the two types of data.

**_Data Generation Process. Formally, let us consider the case in which we draw n samples as unla-_**
beled data X = [x1, · · ·, xn] ∈ R[d][×][n] from the Gaussian mixture model Eq.(4.1) with p1 = p2 =
= pr+1. For the labeled data, we draw (r + 1)m samples, i.e., m samples for each of the r + 1

_· · ·_
classes in the Gaussian mixture model, and denote them as _X[ˆ] = [ˆx1, · · ·, ˆx(r+1)m] ∈_ R[d][×][(][r][+1)][m].
We discuss the above case for simplicity. More general versions are considered in Theorem C.2 (in


-----

the appendix). We study the following hybrid loss to illustrate how label information helps promote
the performance over the self-supervised contrastive learning:
min min (4.2)
_W_ R[r][×][d][ L][(][W] [) :=] _W_ R[r][×][d][ L][SelfCon][(][W] [) +][ α][L][SupCon][(][W] [)][,]
_∈_ _∈_

where α > 0 is the ratio between supervised loss and self-supervised contrastive loss.

We first provide a high-level explanation of why label information help learn core features. When
the label information is unavailable, no matter how much (unlabeled) data we have, we can only
take themselves (and their augmented views) as positive samples. In such a scenario, performing
augmentation leads to an unavoidable trade-off between estimation bias and accuracy. However, if
we have additional class information, we can contrast between data in the same class to extract more
beneficial features that help distinguish a particular class from others and therefore reduce the bias.
**Theorem 4.1. Suppose the labeled and unlabeled samples are generated as the process mentioned**
_above. If Assumption 3.1-3.3 hold, n > d ≫_ _r and let WCL be any solution that minimizes the_
_supervised contrastive learning problem in Eq.(4.2), and denote its singular value decomposition as_
_WCL = (UCLΣCLVCL[⊤]_ [)][⊤][, then we have]

1 _r3/2_ _dr_ _α_ _dr_
E sin Θ(UCL, U _[⋆])_ _F ≲_ log d + +
_∥_ _∥_ 1 + α _d_ _n_ 1 + α _m [.]_

r r

_Moreover, if we have m labeled data for each class and no unlabeled data, then_ 


_dr_

_m [.]_


E∥ sin Θ(UCL, U )∥F ≲


The first bound in Theorem 4.1 demonstrates how the effect of labeled data changes against the
ratio α in the hybrid loss in Eq.(4.2). In addition, compared with Theorem 3.2, when we only have
labeled data (α →∞), the second bound in Theorem 4.1 indicates that with labeled data being
available, the supervised contrastive learning can yield consistent estimation as m →∞ while the
self-supervised contrastive learning consists of an irreducible bias term O(r[3][/][2] log d/d). At a high
level, label information help gain accuracy by creating more positive samples for a single anchor
and therefore extract more decisive features. One should notice a caveat that when labeled data
is extremely rare compared with unlabeled data, the estimation of supervised contrastive learning
suffers from high variance. In comparison, self-supervised contrastive learning, which can exploit a
much larger number of samples, may outperform it.

4.2 INFORMATION FILTERING IN MULTI-TASK TRANSFER LEARNING

Label information can tell us the beneficial information for the downstream task, and learning with
labeled data will filter out useless information and preserve the decisive parts of core features. However, in transfer learning, the label information is sometimes found to hurt the performance of contrastive learning (Khosla et al., 2020; Islam et al., 2021). In this section, we consider two regimes
of transfer learning – tasks are insufficient/abundant. In both regimes, we provide theories to support the empirical observations and further demonstrate how to wisely combine the supervised and
self-supervised contrastive learning to avoid those harms and reach better performance. Specifically,
we consider a transfer learning problem with regression settings. Suppose we have T source tasks
which share a common data generating model Eq.(2.5). In order to study the case of transfer learning, the labels are generated in a different way, that is, for the t-th task, the labels are generated by
_y[t]_ = ⟨wt, z⟩/ν, where wt ∈ R[r] is a unit vector and different across tasks.

To incorporate label information, we maximize the Hilbert-Schmidt Independence Criteria (HSIC)
(Gretton et al., 2005; Barshan et al., 2011), which has been widely used in literature (Song et al.,
2007a;b;c; Barshan et al., 2011). HSIC is defined as HSIC(X, y; W ) = X _[⊤]W_ _[⊤]WXHyy[⊤]H/(n_ _−_
the centering matrix.1)[2], where W ∈ R[r][×]A detailed discussion about its background, motivation and its connec-[d] is linear representation to be learned and H = In − (1/n)1n1[⊤]n [is]
tion to the mean squared loss is presented in Appendix A.3. Suppose we have n unlabeled data
_X = [x1, · · ·, xn] ∈_ R[d][×][n] and m labeled data for each source task _X[ˆ]_ _[t]_ = [ˆx[t]1[,][ · · ·][,][ ˆ]x[t]m[]][, y][t][ =]

[y1[t] _[,][ · · ·][, y]m[t]_ []][,][ ∀][t][ = 1][, . . ., T][ where][ x][i][’s and][ ˆ]x[t]j[’s are independently drawn from spiked covariance]
model Eq. (2.5), we learn the linear representation via the joint optimization:


HSIC( X[ˆ] _[t], y[t]; W_ ), (4.3)

_t=1_

X


min min
_W ∈R[r][×][d][ L][(][W]_ [) :=] _W ∈R[r][×][d][ L][SelfCon][(][W]_ [)][ −] _[α]_


-----

where α > 0 is a pre-specified ratio between the self-supervised contrastive loss and HSIC. (A
more general setting is considered in the appendix, see Section C.2 for details.) We now present a
theorem showing the recoverability of W by minimizing the hybrid loss function (4.3).
**Theorem 4.2.r, if we further assume that Suppose Assumption 3.1-3.3 hold for spiked covariance model Eq.(2.5) and α > C for some constant C, T < r and wt’s are orthogonal to each n > d ≫**
_other, and let W_ _[CL]_ _be any solution that optimizes the problem in Eq.(4.3), and denote its singular_
_value decomposition as WCL = (UCLΣCLVCL[⊤]_ [)][⊤][, then we have:]

E sin Θ(UCL, U _[⋆])_ _F ≲√r_ _T_ _r log d_ + _d_ _d_ + _√T_ _r log d_ + [1] _d_ _d_ _._
_∥_ _∥_ _−_ _d_ _n_ [+][ αT] _m_ _αd_ _α_ _n_ [+][ T] _m_

 r r _[∧]_ [1]  r r (4.4)

In Theorem 4.2, as α goes to infinity (corresponding to the case where we only use the supervised
loss), Eq.(4.4) is reduced to _√r −_ _T + T_ [3][/][2][p]d/m, which is worse than the r[3][/][2] log d/d rate ob
tained by self-supervised contrastive learning (Theorem 3.2). This implies that when the model
focuses mainly on the supervised loss, the algorithm will extract the information only beneficial for
the source tasks and fail to estimate other parts of core features. As a result, when the target task has
a very different distribution, labeled data will bring extra bias and therefore hurt the transferability.
Additionally, one can minimize the right-hand side of Eq.(4.4) to obtain a sharper rate. Specifically,
we can choose an appropriate α such that the upper bound becomes _r[2](r −_ _T_ ) log d/d (when

_n, m →∞), obtaining a smaller rate than that of the self-supervised contrastive learning. Thesep_
facts provide theoretical foundations for the recent empirical observations that smartly combining
supervised and self-supervised contrastive learning achieves significant improvement on transferability compared with performing each of them individually (Islam et al., 2021).

When the tasks are abundant enough then estimation via labeled data can recover core features
completely. Similar to Theorem 4.2, we have the following result.
**Theorem 4.3. Suppose Assumptions 3.1-3.3 hold for spiked covariance model Eq.(2.5) and n >**
_d ≫_ _r, if we further assume that T > r and λ(r)([P][T]i=1_ _[w][i][w]i[⊤][)][ > c][ for some constant][ c >][ 0][,]_
_suppose W_ _[CL]_ _is the optimal solution of optimization problem eq.(4.3), and denote its singular_
_value decomposition as WCL = (UCLΣCLVCL[⊤]_ [)][⊤][, then we have:]

E sin Θ(UCL, U _[⋆])_ _F ≲_ _√r_ _r_ _d_ + T _dr_ (4.5)
_∥_ _∥_ _α + 1_ _d_ [log][ d][ +] _n_ _m [.]_

r r

 

Similar to Theorem 4.2, Theorem 4.3 shows that in the case where tasks are abundant, as α goes
to infinity (corresponding to the case where we use the supervised loss only), Eq.(4.5) is reduced to
_T_ _rd/m. This rate can be worse than the_ _√r[3]_ log d/d + _rd/n rate obtained by self-supervised_

contrastive learning when m is small. Recall that when the number of tasks is small, labeled data

p p

introduce extra bias term _√r −_ _T (Theorem 4.2). We note that when the tasks are abundant enough,_

the harm of labeled data is mainly due to the variance brought by the labeled data. When m is sufficiently large, supervised learning on source tasks can yield consistent estimation of core features,
whereas self-supervised contrastive learning can not. We also illustrate the different behaviors of
the two regimes via numerical simulations in Fig. 1 right panel. Consistent with our theory, it is
observed that when tasks are not abundant, the transfer performance exhibit a U -shaped curve, and
the best result is achieved by choosing an appropriate α. When tasks are abundant and labeled data
are sufficient, the error remains small when we take large α.

5 CONCLUSION

In this work, we theoretically prove that contrastive learning, compared with autoencoders, can
obtain a better low-rank representation under the spiked covariance model, which further leads to
better performance in downstream tasks. We also highlight the role of labeled data in supervised
contrastive learning and multi-task transfer learning: labeled data can reduce the domain shift bias in
contrastive learning, but it harms the learned representation in transfer learning. To our knowledge,
our result is the first theoretical result to guarantee the success of contrastive learning by comparing it
with existing representation learning methods. However, in order to get tractable analysis, like many
other theoretical works in representation learning (Du et al., 2020; Lee et al., 2020; Tripuraneni et al.,
2021), our work starts with linear representations, which still provides important insights. Extending
the results to more complex models is an interesting direction of future work.


-----

REFERENCES

Anish Agarwal, Devavrat Shah, and Dennis Shen. On principal component regression in a highdimensional error-in-variables setting. arXiv preprint arXiv:2010.14449, 2020.

Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saunshi. A theoretical analysis of contrastive unsupervised representation learning. arXiv preprint
_arXiv:1902.09229, 2019._

Zhidong Bai and Jianfeng Yao. On sample eigenvalues in a generalized spiked population model.
_Journal of Multivariate Analysis, 106:167–177, 2012._

Dana H Ballard. Modular learning in neural networks. In AAAI, volume 647, pp. 279–284, 1987.

Elnaz Barshan, Ali Ghodsi, Zohreh Azimifar, and Mansoor Zolghadri Jahromi. Supervised principal
component analysis: Visualization, classification and regression on subspaces and submanifolds.
_Pattern Recognition, 44(7):1357–1371, 2011._

Xin Bing, Florentina Bunea, Seth Strimas-Mackey, and Marten Wegkamp. Prediction under latent
factor regression: Adaptive pcr, interpolating predictors and beyond. Journal of Machine Learn_ing Research, 22(177):1–50, 2021._

Herv´e Bourlard and Yves Kamp. Auto-association by multilayer perceptrons and singular value
decomposition. Biological cybernetics, 59(4):291–294, 1988.

Herv´e Bourlard and Yves Kamp. Auto-association by multilayer perceptrons and singular value
decomposition. Biological Cybernetics, 59:291–294, 2004.

T Tony Cai and Anru Zhang. Rate-optimal perturbation bounds for singular subspaces with applications to high-dimensional statistics. The Annals of Statistics, 46(1):60–89, 2018.

T Tony Cai, Rungang Han, and Anru R Zhang. On the non-asymptotic concentration of heteroskedastic wishart-type matrix. arXiv preprint arXiv:2008.12434, 2020.

Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foun_dations of Computational mathematics, 9(6):717–772, 2009._

Domagoj Cevid, Peter B¨[´] uhlmann, and Nicolai Meinshausen. Spectral deconfounding via perturbed
sparse linear models. Journal of Machine Learning Research, 21:232, 2020.

Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning,
pp. 1597–1607. PMLR, 2020a.

Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big selfsupervised models are strong semi-supervised learners. arXiv preprint arXiv:2006.10029, 2020b.

Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum
contrastive learning. arXiv preprint arXiv:2003.04297, 2020c.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative unsupervised feature learning with convolutional neural networks. Advances in neural
_information processing systems, 27:766–774, 2014._

Simon S Du, Wei Hu, Sham M Kakade, Jason D Lee, and Qi Lei. Few-shot learning via learning
the representation, provably. arXiv preprint arXiv:2002.09434, 2020.

Carl Eckart and G. Marion Young. The approximation of one matrix by another of lower rank.
_Psychometrika, 1:211–218, 1936._

Jianqing Fan, Cong Ma, and Yiqiao Zhong. A selective overview of deep learning. arXiv preprint
_arXiv:1904.05526, 2019._


-----

Gene H. Golub and Charles Van Loan. Matrix computations (3rd ed.). 1996.

Florian Graf, Christoph Hofer, Marc Niethammer, and Roland Kwitt. Dissecting supervised constrastive learning. In International Conference on Machine Learning, pp. 3821–3830. PMLR,
2021.

Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Sch¨olkopf. Measuring statistical dependence with hilbert-schmidt norms. In International conference on algorithmic learning theory,
pp. 63–77. Springer, 2005.

Jean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin Tallec, Pierre H Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi
Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint
_arXiv:2006.07733, 2020._

Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant
mapping. In 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recogni_tion (CVPR’06), volume 2, pp. 1735–1742. IEEE, 2006._

Jiequn Han, Arnulf Jentzen, and E Weinan. Solving high-dimensional partial differential equations
using deep learning. _Proceedings of the National Academy of Sciences, 115(34):8505–8510,_
2018.

Jeff Z HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma. Provable guarantees for self-supervised
deep learning with spectral contrastive loss. arXiv preprint arXiv:2106.04156, 2021.

Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
_Computer Vision and Pattern Recognition, pp. 9729–9738, 2020._

Xinwei He, Yang Zhou, Zhichao Zhou, Song Bai, and Xiang Bai. Triplet-center loss for multi-view
3d object retrieval. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 1945–1954, 2018.

Ashraful Islam, Chun-Fu Chen, Rameswar Panda, Leonid Karlinsky, Richard J. Radke, and
Rog´erio Schmidt Feris. A broad study on the transferability of visual representations with contrastive learning. ArXiv, abs/2103.13517, 2021.

Ian T Jolliffe. A note on the use of principal components in regression. _Journal of the Royal_
_Statistical Society: Series C (Applied Statistics), 31(3):300–303, 1982._

Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron
Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. _arXiv preprint_
_arXiv:2004.11362, 2020._

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25:1097–1105,
2012.

Jason D Lee, Qi Lei, Nikunj Saunshi, and Jiacheng Zhuo. Predicting what you already know helps:
Provable self-supervised learning. arXiv preprint arXiv:2008.01064, 2020.

Xiao Liu, Fanjin Zhang, Zhenyu Hou, Li Mian, Zhaoyu Wang, Jing Zhang, and Jie Tang. Selfsupervised learning: Generative or contrastive. IEEE Transactions on Knowledge and Data En_gineering, 2021._

George Marsaglia. Choosing a point from the surface of a sphere. Annals of Mathematical Statistics,
43:645–646, 1972.

Ishan Misra and Laurens van der Maaten. Self-supervised learning of pretext-invariant representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 6707–6717, 2020.


-----

Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.

Dan Pirjol. The logistic-normal integral and its generalizations. Journal of Computational and
_Applied Mathematics, 237(1):460–469, 2013._

Elad Plaut. From principal subspaces to principal components with linear autoencoders. arXiv
_preprint arXiv:1804.10253, 2018._

Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face
recognition and clustering. 2015 IEEE Conference on Computer Vision and Pattern Recognition
_(CVPR), pp. 815–823, 2015._

Kihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In NIPS, 2016.

Le Song, Arthur Gretton, Karsten Borgwardt, and Alex Smola. Colored maximum variance unfolding. Advances in Neural Information Processing Systems, 20:1385–1392, 2007a.

Le Song, Alex Smola, Arthur Gretton, and Karsten M Borgwardt. A dependence maximization
view of clustering. In Proceedings of the 24th international conference on Machine learning, pp.
815–822, 2007b.

Le Song, Alex Smola, Arthur Gretton, Karsten M Borgwardt, and Justin Bedo. Supervised feature
selection via dependence estimation. In Proceedings of the 24th international conference on
_Machine learning, pp. 823–830, 2007c._

Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In Computer
_Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,_
_Part XI 16, pp. 776–794. Springer, 2020._

Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive learning, multi-view redundancy, and linear models. In Algorithmic Learning Theory, pp. 1179–1206. PMLR, 2021.

Nilesh Tripuraneni, Chi Jin, and Michael Jordan. Provable meta-learning of linear representations.
In International Conference on Machine Learning, pp. 10434–10443. PMLR, 2021.

Yao-Hung Hubert Tsai, Yue Wu, Ruslan Salakhutdinov, and Louis-Philippe Morency. Selfsupervised learning from a multi-view perspective. arXiv preprint arXiv:2006.05576, 2020.

Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Machine Learning, pp.
9929–9939. PMLR, 2020.

Zixin Wen and Yuanzhi Li. Toward understanding the feature learning process of self-supervised
contrastive learning. arXiv preprint arXiv:2105.15134, 2021.

Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via nonparametric instance discrimination. In Proceedings of the IEEE conference on computer vision
_and pattern recognition, pp. 3733–3742, 2018._

Jianfeng Yao, Shurong Zheng, and ZD Bai. Sample covariance matrices and high-dimensional data
_analysis. Cambridge University Press Cambridge, 2015._

Mang Ye, Xu Zhang, Pong C Yuen, and Shih-Fu Chang. Unsupervised embedding learning via invariant and spreading instance feature. In Proceedings of the IEEE/CVF Conference on Computer
_Vision and Pattern Recognition, pp. 6210–6219, 2019._

Yi Yu, Tengyao Wang, and Richard J Samworth. A useful variant of the davis–kahan theorem for
statisticians. Biometrika, 102(2):315–323, 2015.

Anru R Zhang, T Tony Cai, and Yihong Wu. Heteroskedastic pca: Algorithm, optimality, and
applications. arXiv preprint arXiv:1810.08316, 2018.


-----

A BACKGROUND

A.1 DISTANCE BETWEEN SUBSPACES

In this section, we will provide some basic properties of sin Θ distance between subspaces. Recall
that the definition is:
_∥sin Θ (U1, U2)∥F ≜_ _U1[⊤]⊥[U][2]_ _F_ [=] _U2[⊤]⊥[U][1]_ _F_ _[.]_ (A.1)
where U1, U2 ∈ Od,r are two orthogonal matrices. Similarly, we can also define:

_∥sin Θ (U1, U2)∥2 ≜_ _U1[⊤]⊥[U][2]_ 2 [=] _U2[⊤]⊥[U][1]_ 2 _[.]_
We first give two equivalent definition of this distance:
**Proposition A.1.**


_Proof. Write U = [U1, U1_ ] _∥Osin Θ (d,d we have :U1, U2)∥F[2]_ [=][ r][ −] _U1[⊤][U][2]_ _F[2]_
_⊥_ _∈_

_r = ∥U2∥F[2]_ [=][ ∥][U][ ⊤][U][2][∥]F[2] [=] _U1[⊤]⊥[U][2]_ _F_ [+] _U1[⊤][U][2]_ _F_ _[,]_

then by definition of sin Θ distance we can obtain the desired equation.

[2] [2]

**Proposition A.2.**

sin Θ (U1, U2) _F_ [= 1] 1 2 _F_
_∥_ _∥[2]_ 2 _[∥][U][1][U][ ⊤]_ _[−]_ _[U][2][U][ ⊤][∥][2]_

_Proof. Expand the right hand and use Proposition A.1 we have:_
1
2 _[∥][U][1][U][ ⊤]1_ _[−]_ _[U][2][U][ ⊤]2_ _[∥]F[2]_ [=1]2 [(][∥][U][1][U][ ⊤]1 _[∥]F[2]_ [+][ ∥][U][2][U][ ⊤]2 _[∥]F[2]_ _[−]_ [2 tr] _U1U1[⊤][U][2][U][ ⊤]2_

 

= 2[1] [(][r][ +][ r][ −] [2 tr] _U1[⊤][U][2][U][ ⊤]2_ _[U][1]_ )

=r −∥U1[⊤][U][2][∥]F[2]  [=][ ∥][sin Θ (][U][1][, U][2][)][∥]F[2] _[.]_


With Proposition A.1 and A.2, it’s easy to verify its properties to be a distance function. Obviously,
Moreover, we have the following results:we have 0 ≤∥sin Θ (U1, U2)∥F ≤ _[√]r and ∥sin Θ (U1, U2)∥F = ∥sin Θ (U2, U1)∥F by definition._
**Lemma A.1 (Lemma 1 in Cai & Zhang (2018)). For any U, V ∈** Od,r,

sin Θ(U, V ) 2 inf _√2_ sin Θ(U, V ) 2, (A.2)
_∥_ _∥_ _≤_ _O∈Or,r_ _∥_ _∥_

_[∥][UO][ −]_ _[V][ ∥][2][ ≤]_

_and_
sin Θ(U, V ) _F_ inf _√2_ sin Θ(U, V ) _F ._ (A.3)
_∥_ _∥_ _≤_ _O∈Or,r_ _∥_ _∥_

_[∥][UO][ −]_ _[V][ ∥][F][ ≤]_

**Proposition A.3 (Identity of indiscernibles).**

_∥sin Θ (U1, U2)∥F = 0 ⇔∃O ∈_ O[r][×][r], s.t. U1O = U2

_Proof. It’s a straightforward corollary by definition:_
_∥sin Θ (U1, U2)∥F = 0 ⇔_ _U1[⊤]⊥[U][2]_ _F_ [= 0][ ⇔] _[U][2][⊥]_ _[⊥]_ _[U][1]_
_⇔∃O ∈_ O[r][×][r], s.t. U1O = U2.

**Proposition A.4 (Triangular inequality).**

_∥sin Θ (U1, U2)∥F ≤∥sin Θ (U1, U3)∥F + ∥sin Θ (U2, U3)∥F_

_Proof. By the triangular inequality for Frobenius norm we have:_
_∥U1U1[⊤]_ _[−]_ _[U][2][U][ ⊤]2_ _[∥][F]_ _[≤∥][U][1][U][ ⊤]1_ _[−]_ _[U][3][U][ ⊤]3_ _[∥][F]_ [+][ ∥][U][2][U][ ⊤]2 _[−]_ _[U][3][U][ ⊤]3_ _[∥][F]_ _[,]_
then apply Proposition A.2 to replace the Frobenius norm with sin Θ distance we can finish the
proof.


-----

A.2 PRINCIPAL COMPONENT ANALYSIS AND AUTOENCODERS

Autoencoders are popular unsupervised learning methods to perform dimension reduction. Its basic
idea is to learn low dimensional representations for the original data while largely preserving its
salient features. To achieve this goal, autoencoders learn two functions: encoder f : R[d] _→_ R[r]
and decoder g : R[r] _→_ R[d]. While the encoder f compresses the high dimensional data to the low
dimensional space, we try to recover the original data based on this low dimensional representation
via the decoder g. Formally, it can be formulated to be the following optimization problem:

min (A.4)
_f,g_ [E][x][L][(][x, g][(][f] [(][x][)))][.]

Intuitively speaking, optimization problem (A.4) is trying to preserve the most essential features to
recover the original data in the low dimensional representation. In practice, a commonly used form
is to optimize the empirical loss function and choose the loss function to be mean squared error, that
is:
1
min 2[,] _i_ [n], (A.5)
_f_ _[AE]_ _,g[DE]_ _n_ _[∥][x][i][ −]_ _[g][DE][(][f][ AE][(][x][i][))][∥][2]_ _∀_ _∈_

where f _[AE], g[DE]_ are usually two neural networks. Now if we take f _[AE], g[DE]_ to be both linear
transformations, i.e.,

_f_ _[AE](x) = WAEx + bAE, g[DE](x) = WDEx + bDE,_

whereR[d][×][n], the optimization problem (A.5) can be reduced to be: WAE ∈ R[r][×][d], bAE ∈ R[r], WDE ∈ R[d][×][r],, bAE ∈ R[d], and denote X = [x1, · · ·, xn] ∈

1
min _n_ [) +][ b][DE][1]n[⊤][)][∥]F[2] _[.]_
_WAE_ _,bAE_ _,WDE_ _,bDE_ _n_ _[∥][X][ −]_ [(][W][DE][(][W][AE][X][ +][ b][AE][1][⊤]

It’s shown in Bourlard & Kamp (2004) that the bias term can be reduced by centeralize the data
matrix X, that is, denote ¯x = _n[1]_ _ni=1_ _[x][i][ as the sample mean and][ X][0][ =][ X][ −]_ _x[¯]1[⊤]n_ [as the centered]

sample matrix, then the optimization problem correspond to WAE and WDE can be transfromed to
be: P
1
min _F_ _[.]_
_WAE_ _,WDE_ _n_ _[∥][X][0][ −]_ _[W][DE][W][AE][X][0][∥][2]_

By Theorem 2.4.8 in (Golub & Loan, 1996), the optimal solution is given by the singular value
decomposition of matrixPCA does. This fact indicates that PCA is actually a linear case of autoencoders, which is often X0, i.e., the eigenspace of X(In − _n[1]_ [1][n][1]n[⊤][)][X] _[⊤][, which is actually what]_

known as undercomplete linear autoencoders. (Bourlard & Kamp, 1988; Plaut, 2018; Fan et al.,
2019)

A.3 HILBERT-SCHMIDT INDEPENDENT CRITERIA

In Gretton et al. (2005), the Hilbert Schmidt Independent Criteria (HSIC) is proposed to measure
the dependence between two random variables by computing the Hilbert-Schmidt norm of the crosscovariance operator associated with their Reproducing Kernel Hilbert Spaces (RKHSs). Such measurement has been widely used as supervised loss function in feature selection(Song et al., 2007c),
feature extraction(Song et al., 2007a), clustering(Song et al., 2007b) and supervised PCA (Barshan
et al., 2011).

The basic idea behind HSIC is that two random variables, named X and Y, are independent if and
only if any bounded continuous function of the two random variables are uncorrelated. Let F be
a separable RKHS containing all continuous bounded real-valued functions of x from X to R and
_G be that of Y, likewise. To each point x ∈X_, there corresponds an element φ(x) ∈F such that
_⟨φ(x), φ (x[′])⟩F = k (x, x[′]), where k : X × X →_ R is a unique positive definite kernel. Likewise,
define the kernel l( _,_ ) and feature map ψ for . Denote the joint measure of px,y, then the empirical

_·_ _·_ _G_
HSIC is defined to be:
**Definition A.1 (Empirical HSIC (Gretton et al., 2005)). Let Z := {(x1, y1), . . ., (xm, ym)} ⊆**
be a series of m independent observations drawn from pxy. An estimator of HSIC, written
_X × Y_
HSIC(Z, F, G), is given by

HSIC(Z, F, G) := (m − 1)[−][2] tr(KHLH),


-----

where H, K, L ∈ R[m][×][m], Kij := k (xi, xj), Lij := l (yi, yj) and H = Im − _m[1]_ [1][m][1]m[⊤] [.]

In our problems, we hope to maximize the dependency between learned features WX ∈ R[r][×][n]
and label y ∈ R[n] via HSIC, equivalently maximize tr(KHLH) where K is a kernel of WX (e.g.
_X_ _[⊤]W_ _[⊤]WX) and L is a kernel of y (e.g. yy[⊤]). Then we obtain our supervised loss corresponding_
to parameter W :


1

(n 1)[2][ tr] _X_ _[⊤]W_ _[⊤]WXHyy[⊤]H_ _._ (A.6)
_−_
  


HSIC(X, y; W ) =


A.3.1 CONNECTION WITH MEAN SQUARED ERROR

In regression tasks, a more commonly used loss function is mean squared error:


MSE(θ) = [1]
_L_ _n_


_∥f_ (xi, θ) − _yi∥F[2]_ _[,]_ (A.7)
_i=1_

X


where θ is model parameter. In our contrastive learning framework, where we first learn the representation via a linear transformation and then perform linear regression to learn a classifier w ∈ R[r]
with learned representation. Assuming that both of X and y have been centered and we ignore the
bias term, the model can be viewed as a two layer linear network:

_f_ (x, θ) = w[⊤]Wx.

Plug this formula back into mean squared error, we have:

MSE(θ) = [1] _F_ _[.]_
_L_ _n_ _[∥][w][⊤][WX][ −]_ _[y][⊤][∥][2]_

Since the label is a scalar, the optimal W will be singular since we perform a 1-dimensional projection via w. That is, the feature filtering discussed in the main body and if we jointly optimize this
loss with unsupervised contrastive loss, the optimal W should be full rank in general.

On the other hand, the classifier w only involves in the supervised loss function and does not affect
the contrastive loss, thus we can find the optimal solution for w and W sequentially. For any fixed
_W such that rank(WX) = d, which can be achieved via joint optimization, the optimal solution of_
_w is:_
_w[⋆]_ = (WXX _[⊤]W_ _[⊤])[−][1]WXy._
And the optimal error is:

_L(W_ ) = n[1] _[∥][X]_ _[⊤][W][ ⊤][(][WXX]_ _[⊤][W][ ⊤][)][−][1][WXy][ −]_ _[y][∥]F[2]_

= n[1] [(][y][⊤][y][ −] [tr] _y[⊤]X_ _[⊤]W_ _[⊤](WXX_ _[⊤]W_ _[⊤])[−][1]WXy_ ).

  

Ignoring the constant term y[⊤]y and scalar, we can find that the only difference between this loss
function and HSIC (A.6) is the inverse matrix (WXX _[⊤]W_ _[⊤])[−][1]_ which can be viewed as normalization of W . In our contrastive learning framework, this normalization can be achieved by the
regularization term ∥WW _[⊤]∥F[2]_ [. Thus we can use the HSIC to replace the standard regression error]
which helps us to avoid dealing with singularity and additional parameters in optimization.

B OMITTED PROOFS FOR SECTION 3

B.1 PROOFS FOR SECTION 3.1

In this section, we will prove Theorem 3.1 and 3.2 in Section 3.1. The restatement and proof of them
can be found in B.1 and B.3. Before starting the proof, we first provide a lemma to justify the order
of I(U _[⋆]) in the Assumption 3.3._
**Lemma B.1 (Expectation of incoherent constant over a uniform distribution).**

_r_
EU _∼Uniform(Od,r)I(U_ _[⋆]) = O_ _d_ [log][ d] _._ (B.1)
 


-----

Before starting the proof, we give two technical lemmas to help the proof.

**Lemma B.2 (Uniform distribution on the unit shpere (Marsaglia, 1972)). If x1, x2,** _, xn i.i.d._
_n_ _n_ _· · ·_
_∼N_ (0, 1), then (x1/ _i=1_ _[x]i[2][,][ · · ·][, x][n][/]_ _i=1_ _[x]i[2][)][ is uniformly distributed on the unit sphere]_
S[d] = (x1, _, xn)_ R[n] : _i=1_ _[x]i[2]_ [= 1][}][.]
_{_ _· · ·_ _∈pP_ pP

**Lemma B.3. If x1, x2, · · ·, xn i.i.d. ∼N** (0, 1), then:

[P][n]

E max _i_
1≤i≤n _[x][2]_ _[≤]_ [2 log(][n][)][.]


_Proof. Denote Y = max1≤i≤n x[2]i_ [, then we have:]


exp(tEY ) E exp(tY ) E exp _tx[2]i_ = nE exp _tx[2]i_
_≤_ _≤_

_i=1_

X     


Note that the moment-generating function of chi square distribution with v degrees of freedom is:

_MX_ (t) = (1 2t)[−][v/][2].
_−_

Then combine this fact with equation B.1 we have:

exp(tEY ) ≤ _n(1 −_ 2t)[−] [1]2,


which implies:


EY
_≤_ [log(]t _[n][)]_


_,_ _t <_ [1]

_−_ [1][ −]2t[2][t] _∀_ 2 _[.]_


In particular, take t → 2[1] [yields:]

EY ≤ 2 log(n)

as desired.

_Proof of Lemma B.1. Denote the columns of U as U = [u1, · · ·, ur] ∈_ Od,r, we have:


_|e[⊤]i_ _[u][j][|][2]_
_j=1_

X


EU _∼Uniform(Od,r)I(U_ ) =EU _∼Uniform(Od,r) maxi∈[d]_


_≤EU_ _∼Uniform(Od,r)_


max _i_ _[u][j][|][2]_
_i_ [d]
_j=1_ _∈_ _[|][e][⊤]_

X


=rEu Uniform(Sd) max _i_ _[u][|][2][.]_
_∼_ _i_ [d]
_∈_ _[|][e][⊤]_

By Lemma B.2 we can transform this expectation on the uniform sphere distribution into normalized
multivariate Gaussian variables:

EU _∼Uniform(Od,r)I(U_ ) = rEx1,···,xd maxdi∈[d] x[2]i _._ (B.2)
_j=1_ _[x]j[2]_

where x1, x2, · · ·, xd are i.i.d. standard normal random variables. Apply Chebyshev’s inequalityP
we know that:


_|_ _d[1]_


_x[2]j_
_i=1_ _[−]_ [1][|][ > ϵ]

X

_d_

_x[2]j_ _[< d]_

2

_i=1_ !

X


_dϵ[2][ .]_


In particular, take ϵ = 1 we have:


_≤_ _d[8]_ _[.]_


-----

Then take it back into equation B.2 and apply Lemma B.3 we obtain:

EU _∼Uniform(Od,r)I(U_ ) =rEx1,···,xd maxdi∈[d] x[2]i I{ _d_ _x[2]j_ _[< d]2_ _[}]_
_j=1_ _[x]j[2]_ _i=1_

X

+ rEx1,···,xPd maxdji=1∈[d[x]] xj[2] [2]i I{ _i=1d_ _x[2]j_ _[≥]_ _[d]2_ _[}]_

X

_d_

P

_rP_ _x[2]j_ _[< d]_ + [2][r] _i_
_≤_ _i=1_ 2 ! _d_ [E][x][1][,][···][,x][d][ max]i∈[d] _[x][2]_

X


_≤_ [8]d[r] [+ 4][r][ log]d _[ d]_


as desired.

Lemma B.1 demonstrates the expectation of incoherent constant over a uniform distribution on Od,r
takes the order of O( _d[r]_ [log][ d][)][, thus in the main body we take the order to be the same as it. Again we]

need to mention that the order can be chosen to be other functions that decrease to 0 when d →∞,
and one can easily prove our results under more general assumptions.

Now, let’s start proving our main results. In the mainbody and section A.2, we have shown that in
the linear representation setting, the autoencoder can be deduced to PCA. Here we briefly review the
results again, for the autoencoder with an encoder f _[AE]_ : R[d] _→_ R[r] and a decoder g[DE] : R[r] _→_ R[d],
it can be formalized as solving the following optimization problem for samples _xi_ _i=1_
_{_ _}[n]_


_i=1_ _∥xi −_ _g[DE](f_ _[AE](xi))∥2[2][.]_

X


min
_f_ _[AE]_ _,g[AE]_


In the linear representation setting, where f _[AE](x) = WAEx + bAE and g[DE](y) = WDEy + bDE,_
it has been shown that the optimal WAE is given by:

_⊤_
_WAE =_ _UAEΣAEVAE[⊤]_ _,_ (B.3)
 

where UAE is top-r eigenspace of matrix M := X(I 1n1[⊤]n _[/n][)][X]_ _[⊤][,][ Σ][AE]_ [is a diagonal matrix]
_−_
consists of eigenvalues of M and VAE = [v1, · · ·, vn] ∈ R[r][×][r] can be any orthonormal matrix.
Note that UAE is the top-r left eigenspace of the observed covariance matrix and U _[⋆]_ is that of
core feature covariance matrix, and by Assumption 3.2 the observed covariance matrix is dominated
by the covariance of random noise. The Davis-Kahan theorem provides a technique to estimate the
eigenspace distance via estimate the difference between target matrices. We will adopt this technique
to prove the lower bound of feature recovery ability of autoencoder in Theorem 3.1.

**Theorem B.1 (Restatement of Theorem 3.1). Consider the spiked covariance model Eq.(2.5), under**
_Assumption 3.1-3.3 and n > d ≫_ _r, let WAE be the learned representation of autoencoder with_
_singular value decomposition WAE = (UAEΣAEVAE[⊤]_ [)][⊤] _[(as in Eq.(3.1)). If we further assume]_
_{σi[2][}]i[d]=1_ _[are different from each other and][ σ](1)[2]_ _[/][(][σ]([2]r)_ _[−]_ _[σ]([2]r+1)[)][ < C][σ][ for some universal constant]_
_Cσ. Then there exist two universal constants Cρ > 0, c ∈_ (0, 1), such that when ρ < Cρ, we have

E sin Θ (U _[⋆], UAE)_ _F_ _c[√]r._ (B.4)
_∥_ _∥_ _≥_

_Proof. Denote M = ν[2]U_ _[⋆]U_ _[⋆][⊤]_ to be the target matrix, xi = U _[⋆]zi + ξi,_ _i = 1, 2, · · · n to be_
the samples generated from model 2.5 and let X = [x1, · · ·, xn] ∈ R[d][×][n], Z = [z1, · · ·, zn] ∈
R[r][×][n], E = [ξ1, · · ·, ξn] ∈ R[d][×][n] to be the corresponding matrices. In addition, we write the
column mean matrix _X[¯] ∈_ R[n][×][d] of a matrix X ∈ R[n][×][d] to be _X[¯] =_ _n[1]_ _[X][1][n][1]n[⊤][, that is, each column]_

of _X[¯] is the column mean of X. We denote the sum of variance σi[2]_ [as][ σ]sum[2] [=][ P]i[d]=1 _[σ]i[2][. As shown]_
in B.3, autoencoder finds the top-r eigenspace of the following matrix:

_Mˆ_ 1 = [1] _n_ [)][X] _[⊤]_ [= 1] _Z + E[¯])(U_ _[⋆]Z[¯] + E[¯])[⊤]._

_n_ _[X][(][I][n][ −]_ _n[1]_ [1][n][1][⊤] _n_ [(][U][ ⋆][Z][ +][ E][)(][U][ ⋆][Z][ +][ E][)][⊤] _[−]_ _n[1]_ [(][U][ ⋆] [¯]


-----

The rest of the proof is divided into three steps for the sake of presentation.

Step 1, bound the difference between _M[ˆ]_ 1 and Σ. In this step, we aim to show that the data recovery
of autoencoder is dominated by the random noise term. Note that Σ = Cov(ξ) = Eξξ[⊤], we just
need to bound the norm of the following matrix:

_Mˆ_ 1 Σ = [1] _Z_ + E[¯])(U _[⋆]Z[¯]_ + E[¯])[⊤],
_−_ _n_ _[U][ ⋆][ZZ]_ _[⊤][U][ ⋆][⊤]_ [+ 1]n [(][U][ ⋆][ZE][⊤] [+][EZ] _[⊤][U][ ⋆][⊤][)+( 1]n_ _[EE][⊤]_ _[−][Σ)][−]_ _n[1]_ [(][U][ ⋆] [¯]

(B.5)
and we will deal with these four terms separately.

1. For the first term, note that Ezz[⊤] = ν[2]Ir, the first term can then be divided into two terms


1

(B.6)

_n_ _[U][ ⋆][ZZ]_ _[⊤][U][ ⋆][⊤]_ [=][ M][ +][ U][ ⋆][( 1]n _[ZZ]_ _[⊤]_ _[−]_ [E][zz][⊤][)][U][ ⋆][⊤][.]

Then apply the concentration inequality of Wishart-type matrices (Lemma D.3) we have:

_r_

E
_∥_ _n[1]_ _[ZZ]_ _[⊤]_ _[−]_ [E][zz][⊤][∥][2][ ≤] [(] _n_ [+][ r]n [)][ν][2][.]

r


Plug it back into (B.6) we obtain the bound for the first term:

_r_
1 +

_∥_ _n[1]_ _[UZZ]_ _[⊤][U][ ⊤][∥][2][ ≤∥][M]_ _[∥][2][ +][ ∥][U]_ _[∥][2][∥]_ _n[1]_ _[ZZ]_ _[⊤]_ _[−]_ [E][zz][⊤][∥][2][∥][U] _[∥][2][ ≤]_ _n_ [+][ r]n

 r


_ν[2]._

(B.7)


2. For the second term, since Z and E are independent, we must have EU _[⋆]ZE[⊤]_ = 0, so
apply Lemma D.2 twice we have:


1
_n_ [E][∥][EZ] _[⊤][U][ ⋆][∥][2][ = 1]n_ [E][Z][[][E][E][[][∥][EZ] _[⊤][U][ ⋆][∥][2][|][Z][]]]_

≲ [1]

_n_ [E][Z][[][∥][Z][∥][2][(][σ][sum][ +][ r][1][/][4][√][σ][sum][σ][(1)][ +][ √][rσ][(1)][)]]

≲ [1] _√dσ(1)_

_n_ [E][Z][[][∥][Z][∥][2][]]

≲ [1] _√dσ(1)(r[1][/][2]ν + (nr)[1][/][4]ν + n[1][/][2]ν)_


(B.8)


≲ _√nσ(1)ν._

3. For the third term, apply Lemma D.3 again yields:

E
_∥_ _n[1]_ _[EE][⊤]_ _[−]_ [Σ][∥][2][ ≤]

r


_d_
_n_ [+][ d]n


_σ(1)[2]_ _[.]_ (B.9)


4. For the last term, note that each columns of _Z[¯] and_ _E[¯] are the same, so we can rewrite is as:_

1
_Z + E[¯])(U_ _[⋆]Z[¯] + E[¯])[⊤]_ = (U _[⋆]z¯ + ξ[¯])(U_ _[⋆]z¯ + ξ[¯])[⊤],_
_n_ [(][U][ ⋆] [¯]

where ¯z = _n1_ _ni=1_ _[z][i][ and][ ¯]ξ =_ _n1_ _ni=1_ _[ξ][i][. Since][ z][ and][ ξ][ are independent zero mean]_
sub-Gaussian random variables and Cov(z) = ν[2]Ir, Cov(ξ) = Σ, we can conclude that:
P P

E _Z + E[¯])(U_ _[⋆]Z[¯] + E[¯])[⊤]_ 2 E _z¯z¯[⊤]_ 2 + 2E _z¯ξ[¯][⊤]_ 2 + E _ξξ[¯][⊤]_ 2
_∥_ _n[1]_ [(][U][ ⋆] [¯] _∥_ _≤_ _∥_ _∥_ _∥_ _∥_ _∥_ [¯] _∥_

_√d_ _dσ(1)[2]_

≲ _[rν][2]_ (1)ν + _._

_n_ [+] _√nσ_ _n_


-----

To sum up, combine equations (B.7)(B.8)(B.9)(4) together we obtain the upper bound for the 2 norm
expectation of matrix _M[ˆ] −_ Σ:

_r_ _d_ _d_
E _M1_ Σ 2 ≲ _ν[2]_ 1 + + σ(1)[2] + (B.10)
_∥_ [ˆ] _−_ _∥_  r _n_ [+][ r]n  r _n_ [+][ d]n ! r _n_ _[σ][(1)][ν.]_

Step 2, bound the sin Θ distance between eigenspaces. As we have shown in step 1, the target matrix
of autoencoder is close to the covariance matrix of random noise, i.e., Σ. Note that Σ is assumed to
be diagonal matrix with different elements, hence its eigenspace only consists of canonical basis ei.
Denote UΣ to be the top-r eigenspace of Σ and _ei_ _i_ _C to be its corresponding basis vectors, apply_
_{_ _}_ _∈_
the Davis-Kahan Theorem D.1 we can conclude that:

_M1_ Σ 2
E sin Θ(UAE, UΣ) _F_ _−_ _∥_
_∥_ _∥_ _≤_ [2][√]σ[r]([2]r[E])[∥] [ˆ] (r+1)

_[−]_ _[σ][2]_

_r_ _d_ _d_

≲[√]r [1] _ν[2]_ 1 + + σ(1)[2] +

_σ(1)[2]_  r _n_ [+][ r]n  r _n_ [+][ d]n ! r _n_ _[σ][(1)][ν]!_


≲[√]r


_ρ[2]_ +


_n_ [+][ ρ]


Step 3, obtain the final result by triangular inequality. By Assumption 3.3 we know that the distance
between canonical basis and the eigenspace of core features can be large:

sin Θ(U _[⋆], UΣ)_ _F_ [=][ ∥][U][ ⊤]Σ _F_ [=] _e[⊤]i_ _[U][ ⋆][∥]F[2]_ [=][ ∥][U][ ⋆][∥]F[2] _e[⊤]i_ _[U][ ⋆][∥]F[2]_
_∥_ _∥[2]_ _⊥[U][ ⋆][∥][2]_ _i∈X[d]/C_ _∥_ _[−]_ _iX∈C_ _∥_

_r2_
_r_ _rI(U_ _[⋆]) = r_ _O_ _._
_≥_ _−_ _−_ _d_ [log][ d]
 

Then apply the triangular inequality of sin Θ distance (Proposition A.4) we can obtain the lower
bound of autoencoder.
E∥ sin Θ(UAE, U _[⋆])∥F ≥_ E∥ sin Θ(U _[⋆], UΣ)∥F −_ E∥ sin Θ(UAE, UΣ)∥F

_≥_ _[√]r −_ _O r√d_ log d _−_ _O_ _√r_ _ρ[2]_ + r _nd_ [+][ ρ]r _nd_ !!.

p

By Assumption 3.2, it implies that when n and d is sufficient large and ρ is sufficient small (smaller
than a given constant Cρ > 0), there exist a universal constant c ∈ (0, 1) such that:

E∥ sin Θ(UAE, U _[⋆])∥F ≥_ _c[√]r._

On the other hand, recall that the optimization problem for self-supervised contrastive learning has
been formulated to be:


_n_

min _⟨f_ (xi, W ), f (x[P os]i _, W_ )⟩ _⟨f_ (xi, θ), f (x[Neg]i _, W_ )⟩ +λR(W ),
_W ∈R[d][×][r][ −]_ _n[1]_ Xi=1 x[P os]i X∈Bi[P os] _|Bi[P os]|_ _−_ _x[Neg]i_ X∈Bi[Neg] _|Bi[Neg]|_ 

 (B.11)

where f (x, W ) = Wx, R(W ) = ∥WW _[⊤]∥F[2]_ _[/][2][. To compare contrastive learning with autoencoder,]_
we now derive the optimal solution of the optimization problem B.11. Let’s start with the general
result for self-supervised contrastive learning with augmented pairs generation (Definition 2.1), and
then turn to the special case for random masking augmentation (Definition 2.2).
**Theorem B.2. For two fixed augmentation function g1, g2 : R[d]** _→_ R[d], denote the augmented data
_matrices as X1 = [g1(x1), · · ·, g1(xn)] ∈_ R[d][×][n] _and X2 = [g2(x1), · · ·, g2(xn)] ∈_ R[d][×][n], when
_the augmented pairs are generated as in Definition 2.1, the optimal solution of contrastive learning_
_problem (B.11) is given by:_

_r_

_⊤_

_WCL = C_ _uiσivi[⊤]_ _,_

_i=1_

 X 


-----

_where C > 0 is a positive constant, σi is the i-th largest eigenvalue of the following matrix:_


1
_X1X2[⊤]_ [+][ X][2][X]1[⊤] _r_ [+][ X][2][)][⊤][,] (B.12)

_[−]_ 2(n − 1) [(][X][1][ +][ X][2][)(1][r][1][⊤] _[−]_ _[I][r][)(][X][1]_

_ui is the corresponding eigenvector and V = [v1, · · ·, vn] ∈_ R[r][×][r] _can be any orthonormal matrix._

_Proof of Theorem B.2. When augmented pairs generation 2.1 is applied, the contrastive loss can be_
written as:


_L(W_ ) = _[λ]2_ _F_ _[−]_ _n[1]_

_[∥][WW][ ⊤][∥][2]_



[ _Wt1(xi), Wt2(xi)_
_⟨_ _⟩_
_i=1_

X


_Wt1(xi) + Wt2(xi), Wt1(xj) + Wt2(xi)_ ]
_⟨_ _⟩_

Xj≠ _i_


4(n − 1)


= _[λ]2_ _F_ _[−]_ _n[1]_

_[∥][WW][ ⊤][∥][2]_


= 2 _[∥][WW][ ⊤][∥]F[2]_ _[−]_ _n_ _i=1⟨Wt1(xi), Wt2(xi)⟩_

_n_ X

1
+ _Wt1(xi) + Wt2(xi), Wt1(xj) + Wt2(xi)_

4n(n 1) _⟨_ _⟩_
_−_ Xi=1 Xj≠ _i_

= _[λ]_ _F_ _X1[⊤][W][ ⊤][WX][2]_ [+][ X]2[⊤][W][ ⊤][WX][1]

2 _[−]_ 2[1]n [tr]

_[∥][WW][ ⊤][∥][2]_

1   
+ 4n(n − 1) [tr] (1n1[⊤]n _[−]_ _[I][n][)(][X][1]_ [+][ X][2][)][⊤][W][ ⊤][W] [(][X][1] [+][ X][2][)]

  

= _[λ]2_ _F_

_[∥][WW][ ⊤][∥][2]_

1
(X2X1[⊤] [+][ X][1][X]2[⊤] _n_ [+][ X][2][)][⊤][)][W][ ⊤][W]

_−_ 2[1]n [tr] _[−]_ 2(n 1) [(][X][1][ +][ X][2][)(1][n][1][⊤] _[−]_ _[I][n][)(][X][1]_

 _−_ 

1 1

= [1] _X2X1[⊤]_ [+][ X][1][X]2[⊤] _n_ [+][ X][2][)][⊤]

2 2nλ _[−]_ 2(n 1) [(][X][1][ +][ X][2][)(1][n][1][⊤] _[−]_ _[I][n][)(][X][1]_

 _−_

2

1 1

_[λW][ ⊤][W]X[ −]2X1[⊤]_ [+][ X][1][X]2[⊤] _n_ [+][ X][2][)][⊤] _._

_−_ 2nλ _[−]_ 2(n 1) [(][X][1][ +][ X][2][)(1][n][1][⊤] _[−]_ _[I][n][)(][X][1]_ _F_

 _−_ 

Note that the last term only depends on X, and the first term implies that when WCL is the optimal
solution, λWCLWCL[⊤] [is the best rank-][r][ approximation of] (n 11)λ _[XHX]_ _[⊤][. Then apply Lemma D.4]_

_−_
we can conclude that WCL satisfy the desired conditions.

Theorem B.2 shows a general result for augmented pairs generation with any augmentation. Specifically, if we apply the random masking augmentation 2.2, we can obtain a more precise result to
characterize the optimal solution. To formally state the result, we need additional notations: for any
square matrix A ∈ R[d][×][d], we denote D(A) to be A with all off-diagonal entries set to be zero and
∆(A) = A − _D(A) to be A with all diagonal entries set to be zero. Then we have the following_
corollary for random masking augmentation.

**Corollary B.1. Under the same conditions as in Theorem B.2, if we use random masking (Definition**
_2.2) as our augmentation function, then in expectation, the optimal solution of contrastive learning_
_problem (B.11) is given by:_

_r_

_⊤_

_WCL = C_ _uiσivi[⊤]_ _,_

_i=1_

 X 

_where C > 0 is a positive constant, σi is the i-th largest eigenvalue of the following matrix:_

1
∆(XX _[⊤]) −_ _n_ 1 _[X][(1][n][1]n[⊤]_ _[−]_ _[I][n][)][X]_ _[⊤][,]_ (B.13)

_−_

_ui is the corresponding eigenvector and V = [v1, · · ·, vn] ∈_ R[r][×][r] _can be any orthonormal matrix._


4n(n − 1)


_i=1_


∆(XX _[⊤]) −_


-----

_Proof of Corollary B.1. Following the proof of Theorem B.2, now we only need to compute the_
expectation over the augmentation distribution defined in 2.2:


(W ) = _[λ]_ _F_ (t1,t2) [[ 1]
_L_ 2 _[∥][WW][ ⊤][∥][2]_ _[−]_ [E] _∼T_ _n_



[ _Wt1(xi), Wt2(xi)_
_⟨_ _⟩_
_i=1_

X


_Wt1(xi) + Wt2(xi), Wt1(xj) + Wt2(xi)_ ]]

_−_ 4(n 1) _⟨_ _⟩_

_−_ Xj≠ _i_ (B.14)

= _[λ]_ _F_ (t1,t2) [[ 1] 1 [+][ X][1][X]2[⊤]

2 _[∥][WW][ ⊤][∥][2]_ _[−]_ [E] _∼T_ 2n [tr((][X][2][X] _[⊤]_

1
_−_ 2(n − 1) [(][X][1][ +][ X][2][)(1][n][1]n[⊤] _[−]_ _[I][n][)(][X][1]_ [+][ X][2][)][⊤][)][W][ ⊤][W] [)]][.]

Note that by the definition of random masking augmentation, we have X1 = AX, X2 = (I − _A)X,_
which implies X1 + X2 = X. On the other hand, X1 and X2 has disjoint nonzero dimensions,
hence the matrix X1X2[⊤] [+][ X][2][X]1[⊤] [only consists of off-diagonal entries and each of the off-diagonal]
entry, let’s say xij, appears if and only if ai + aj = 1. Moreover, once it appears, we must have
_xij equals to the (i, j) element of XX_ _[⊤]. With this result, we can then compute the expectation in_
equation (B.14):

(W ) = _[λ]_ _F_ (t1,t2) [[ 1] 1 [+][ X][1][X]2[⊤]
_L_ 2 _[∥][WW][ ⊤][∥][2]_ _[−]_ [E] _∼T_ 2n [tr((][X][2][X] _[⊤]_

1
_−_ 2(n − 1) [(][X][1][ +][ X][2][)(1][n][1]n[⊤] _[−]_ _[I][n][)(][X][1]_ [+][ X][2][)][⊤][)][W][ ⊤][W] [)]]

1

= _[λ]2_ _F_ _[−]_ 2[1]n [tr] ( [1]2 [∆(][XX] _[⊤][)][ −]_ 2(n 1) _[X][(1][n][1]n[⊤]_ _[−]_ _[I][n][)][X]_ _[⊤][)][W][ ⊤][W]_

_[∥][WW][ ⊤][∥][2]_  _−_ 

1 1

= [1]2 _[∥][λW][ ⊤][W][ −]_ 4nλ [(∆(][XX] _[⊤][)][ −]_ _n_ 1 _[X][(1][n][1]n[⊤]_ _[−]_ _[I][n][)][X]_ _[⊤][∥]F[2]_

_−_

1

_n_ _F_ _[.]_

_−∥_ 4nλ[1] [(∆(][XX] _[⊤][)][ −]_ _n_ 1 _[X][(1][n][1][⊤]_ _[−]_ _[I][n][)][X]_ _[⊤][∥][2]_

_−_

By similar argument as in the proof of Theorem B.2, we can conclude that WCL satisfy the desired
conditions.

_Remark B.1. Note that the two views generated by random masking augmentation have disjoint_
non-zero dimensions, hence contrasting such positive pairs yields correlation between different dimensions only. That’s why the first term in equation (B.13) appears to be ∆(XX _[⊤]) where the_
diagonal entries are eliminated.

With Theorem B.2 and Corollary B.1 established, we can find that the self-supervised contrastive
learning equipped with augmented pairs generation and random masking augmentation can eliminate the effect of random noise on the diagonal entries of the observed covariance matrix. Since
Cov(ξ) = Σ is a diagonal matrix, and by Assumption 3.3 we know that the diagonal entries
Cov(U _[⋆]z) = ν[2]U_ _[⋆]U_ _[⋆][⊤]_ only take a small proportion of the total Frobenius norm. Thus contrasting
augmented pairs will preserve the core features while eliminating most of the random noise, and
give a more accurate estimation of core features. To start the proof, we introduce a technical lemma
first.
**Lemma B.4 (Lemma 4 in Zhang et al. (2018)). If M ∈** R[p][×][p] _is any square matrix and ∆(M_ ) is
_the matrix M with diagonal entries set to 0, then_
∆(M ) 2 2 _M_ 2.
_∥_ _∥_ _≤_ _∥_ _∥_
_Here, the factor ” 2 ” in the statement above cannot be improved._

Then we turn to prove Theorem 3.2.
**Theorem B.3 (Restatement of Theorem 3.2). Under the spiked covariance model Eq.(2.5), ran-**
_dom masking augmentation in Definition 2.2, Assumption 3.1-3.3 and n > d_ _r, let WCL_
_≫_
_be any solution that minimizes Eq.(2.3), and denote its singular value decomposition as WCL =_
(UCLΣCLVCL[⊤] [)][⊤][, then we have]


E ∥sin Θ (U _[⋆], UCL)∥F ≲_ _[r][3]d[/][2]_


_dr_

_n [.]_ (B.15)


log d +


-----

_Proof. The proof strategy is quite similar to that of Theorem 3.2 and we follow the notation de-_
fined in the first paragraph of that proof. As we have shown in Corollary B.1, under our linear
representation setting, the contrastive learning algorithm finds the top-r eigenspace of the following
matrix:

1

_Mˆ_ 2 = n[1] ∆(XX _[⊤]) −_ _n_ 1 _[X][(1][n][1]n[⊤]_ _[−]_ _[I][n][)][X]_ _[⊤]_

 _−_ 

1

= [1] _Z + E[¯])(U_ _[⋆]Z[¯] + E[¯])[⊤]_

_n_ [∆((][U][ ⋆][Z][ +][ E][)(][U][ ⋆][Z][ +][ E][)][⊤][)][ −] _n_ 1 [(][U][ ⋆] [¯]

_−_

1
+

_n(n_ 1) [(][U][ ⋆][Z][ +][ E][)(][U][ ⋆][Z][ +][ E][)][⊤][.]
_−_

To prove the theorem, first we need to bound the difference between _M[ˆ]_ 2 and M . We aim to show that
the contrastive learning algorithm is dominated by the core feature term. Note that Σ = EUzz[⊤]U _[⊤],_
we just need to bound the norm of the following matrix:

_Mˆ_ 2 _M =( [1]_
_−_ _n_ [∆(][U][ ⋆][ZZ] _[⊤][U][ ⋆][⊤][)][ −]_ _[M]_ [) + 1]n [∆(][U][ ⋆][ZE][⊤] [+][ EZ] _[⊤][U][ ⋆][⊤][) + 1]n_ [∆(][EE][⊤][)]

(B.16)

1 1

_Z + E[¯])(U_ _[⋆]Z[¯] + E[¯])[⊤]_ +

_−_ _n_ 1 [(][U][ ⋆] [¯] _n(n_ 1) [(][U][ ⋆][Z][ +][ E][)(][U][ ⋆][Z][ +][ E][)][⊤][.]

_−_ _−_

and we will also deal with these five terms separately.

1. For the first term, we can divide it into two parts:


1

(B.17)

_n_ [∆(][U][ ⋆][ZZ] _[⊤][U][ ⋆][⊤][)][ −]_ _[M][ = ∆( 1]n_ _[U][ ⋆][ZZ]_ _[⊤][U][ ⋆T][ −]_ _[M]_ [) + ∆(][M] [)][ −] _[M.]_

Then apply Lemma B.4 and Lemma D.3 we have:

_r_

E ∆( [1]
_∥_ _n_ _[U][ ⋆][ZZ]_ _[⊤][U][ ⋆][⊤]_ _[−]_ _[M]_ [)][∥][2][ ≤] [2][E][∥] _n[1]_ _[U][ ⋆][ZZ]_ _[⊤][U][ ⋆][⊤]_ _[−]_ _[M]_ _[∥][2][ ≤]_ [2(] _n_ [+][ r]n [)][ν][2][.]

r

Using the incoherent condition I(U ) = O( _d[r]_ [log][ d][)][, we know that:]

_M_ ∆(M ) 2 _ν[2]_ max _i_ _[U][ ⋆][∥]2[2]_ [=][ ν][2][I][(][U][ ⋆][)][ ≲] _[r]_
_∥_ _−_ _∥_ _≤_ _i∈[d]_ _[∥][e][⊤]_ _d_ [log][ dν][2][.]


Combine the two equations above together we obtain the bound for the first term:


E
_∥_ _n[1]_ [∆(][U][ ⋆][ZZ] _[⊤][U][ ⋆][⊤][)][ −]_ _[M]_ _[∥][2][ ≤]_ [E][∥][∆( 1]n _[U][ ⋆][ZZ]_ _[⊤][U][ ⋆][⊤]_ _[−]_ _[M]_ [)][∥][2][ +][ ∥][M][ −] [∆(][M] [)][∥][2]

(B.18)

_r_

≲ _ν[2](_ _[r]_ (B.19)

_d_ [log][ d][ +][ r]n [+] _n_ [)][.]

r

2. For the second term, apply equation (B.8) yields:


1
_n_ [E][∥][∆(][U][ ⋆][ZE][⊤] [+][ EZ] _[⊤][U][ ⋆][⊤][)][∥][2][ ≤]_ _n[4]_ [E][∥][EZ] _[⊤][U][ ⋆][⊤][∥][2][ ≲]_

3. For the third term, apply equation (B.9) yields:


_d_
(1)ν. (B.20)
_√nσ_


E
_∥_ _n[1]_ [∆(][EE][⊤][)][∥][2][ =][ E][∥][∆( 1]n _[EE][⊤]_ _[−]_ [Σ)][∥][2][ ≤] [2][∥] _n[1]_ _[EE][⊤]_ _[−]_ [Σ][∥][2][ ≲] [(]

4. For the fourth term, apply equation (4) yields:


_d_

(1)[.][ (B.21)]

_n_ [+][ d]n [)][σ][2]


1

_Z + E[¯])(U_ _Z[¯] + E[¯])[⊤]_ 2 ≲E _Z + E[¯])(U_ _Z[¯] + E[¯])[⊤]_ 2
_n_ 1 [(][U][ ⋆] [¯] _∥_ _∥_ _n[1]_ [(][U][ ¯] _∥_
_−_

_√d_ _dσ(1)[2]_

≲ _[rν][2]_ (1)ν + _._

_n_ [+] _√nσ_ _n_


E∥


(B.22)


-----

5. For the last term, by equations (B.7)(B.8)(B.9) we know:


E
_∥_ _n[1]_ [(][U][ ⋆][Z][ +][ E][)(][U][ ⋆][Z][ +][ E][)][⊤][∥][2]

_r_
≲ Σ 2 + 1 + _ν[2]_ +
_∥_ _∥_ _n_ [+][ r]n
 r 

Thus we can conclude that:


_d_
_n_ [+][ d]n


_σ(1)[2]_ _[.]_


_n_ _[σ][(1)][ν][ +]_


1
E (1) [+][ r] (B.23)
_∥_ _n(n_ 1) [(][U][ ⋆][Z][ +][ E][)(][U][ ⋆][Z][ +][ E][)][⊤][∥][2][ ≲] _n[d]_ _[σ][2]_ _n_ _[ν][2][.]_

_−_


To sum up, combine equations (B.18)(B.20)(B.21)(B.22)(B.23) together we obtain the upper bound
for the 2 norm expectation of matrix _M[ˆ]_ 2 _M_ :
_−_

_r_ _r_ _d_ _d_
E _M2_ _M_ 2 ≲ _ν[2]_ + σ(1)[2] + σ(1)ν (B.24)
_∥_ [ˆ] _−_ _∥_  _d_ [log][ d][ +] r _n_ [+][ r]n  r _n_ [+][ d]n ! r _n_ _[.]_

With the upper bound for _M2_ _M_ 2, simply apply Lemma D.1 we can obtain the desired bound
for sin Θ distance: _∥_ [ˆ] _−_ _∥_

E sin Θ(UCL, U _[⋆])_ _F_ _M2 −_ _M_ _∥2_
_∥_ _∥_ _≤_ [2][√][r][E][∥] _ν[ˆ][2]_

_r_ _r_ _d_ _d_

≲[√]r [1] _ν[2]_ + σ(1)[2] + σ(1)ν

_ν[2]_ _d_ [log][ d][ +] _n_ [+][ r]n _n_ [+][ d]n _n_

 r  r ! r !

_r_ _r_ _d_ _d_

=[√]r + ρ[−][2] + ρ[−][1]

_d_ [log][ d][ +] _n_ [+][ r]n _n_ [+][ d]n _n_

 r  r ! r !


≲ _[r][3][/][2]_


_dr_

_n [.]_


log d +


Moreover, there exists an orthogonal matrix _O[ˆ] ∈_ O[r][×][r] depending on UCL such that:

_M2_ _M_ 2
E∥U _[⊤]UCLO[ˆ] −_ _Ir∥F = E∥UCLO[ˆ] −_ _U_ _∥F ≤_ [2][√][r][E][∥] _ν[ˆ][2] −_ _∥_ ≲ _[r][3]d[/][2]_ log d +


_dr_

_n [.]_


which finishes the proof.

In the following, we will show that our results do not change if we applied the same augmentation
(2.2) for autoencoders, which indicates that our comparison is fair. As discussed in Section A.2, we
can ignore the bias term in autoencoders for simplicity, which only serves as centralization of the
data matrix. In that case, we applied random augmentation t1(x) = Ax and t2(x) = (I _A)x to_
_−_
the original data {xi}i[n]=1[, and the optimization problem can be formulated as follows:]


1

_F_ [+][ ∥][(][I][ −] _[A][)][X][ −]_ _[W][DE][W][AE][(][I][ −]_ _[A][)][X][∥]F[2]_ []][.] (B.25)
2n [E][A][[][∥][AX][ −] _[W][DE][W][AE][AX][∥][2]_


min
_WAE_ _,WDE_


Then, similar to Theorem B.2 for contrastive learning, we can also obtain an explicit solution for
this optimization problem.
**Theorem B.4. The optimal solution of autoencoders with random masking augmentation (B.25) is**
_given by:_

_r_

_⊤_

_WAE = WDE[⊤]_ [=][ C] _uiσivi[⊤]_ _,_

_i=1_

 X 

_where C > 0 is a positive constant, σi is the i-th largest eigenvalue of the following matrix:_

1
(B.26)
2 [∆(][XX] _[⊤][) +][ D][(][XX]_ _[⊤][)][,]_

_ui is the corresponding eigenvector and V = [v1, · · ·, vn] ∈_ R[r][×][r] _can be any orthonormal matrix._


-----

_Proof. We first derive the equivalent form for this objective function:_

1

_F_ [+][ ∥][(][I][ −] _[A][)][X][ −]_ _[W][DE][W][AE][(][I][ −]_ _[A][)][X][∥]F[2]_ []]
2n [E][A][[][∥][AX][ −] _[W][DE][W][AE][AX][∥][2]_

= 2[1]n [E][A][[tr] _X_ _[⊤]A[⊤]AX_ + tr _X_ _[⊤]A[⊤]WDEWAEAX_ + tr _X_ _[⊤]A[⊤]WAE[⊤]_ _[W][ ⊤]DE[W][DE][W][AE][AX]_

       


+ tr _X_ _[⊤](I_ _A)[⊤](I_ _A)X_ + tr _X_ _[⊤](I_ _A)[⊤]WDEWAE(I_ _A)X_
_−_ _−_ _−_ _−_

+ tr X _[⊤](I −_ _A)[⊤]WAE[⊤]_ _[W][ ⊤]DE[W][DE] [W][AE][(][I][ −]_ _[A][)][X]_ ] 

= 2[1]n [E] [A][[tr] _X_ _[⊤]AX_ + tr _AXX_ _[⊤]A[⊤]WDEWAE_ + tr _AXX_ _[⊤]A[⊤]WAE[⊤]_ _[W][ ⊤]DE[W][DE][W][AE]_

       


+ tr _X_ _[⊤](I_ _A)X_ + tr (I _A)XX_ _[⊤](I_ _A)[⊤]WDEWAE_
_−_ _−_ _−_

+ tr (I − _A)XX_ _[⊤](I −_ _A )[⊤]WAE[⊤]_ _[W][ ⊤]DE[W][DE][W][AE]_ ] 

= 2[1]n [E] [A][[tr] _X_ _[⊤]X_ + tr(MWDEWAE) + tr _MWAE[⊤]_ [W][ ⊤]DE[W][DE][W][AE] ],

(B.27)
     
where M := AXX _[⊤]A[⊤]_ + (I − _A)XX_ _[⊤](I −_ _A)[⊤]. Note that by Definition 2.2 we have A =_
diag(a1, · · ·, ad) and ai follows the Bernoulli distribution, so we have:

EAM = [1] (B.28)

2 [∆(][XX] _[⊤][) +][ D][(][XX]_ _[⊤][)]_


Again, by Theorem 2.4.8 in (Golub & Loan, 1996), the optimal solution of B.25 is given by the
eigenvalue decomposition of EAM = [1]2 [∆(][XX] _[⊤][)+]_ _[D][(][XX]_ _[T][ )][, up to an orthogonal transformation,]_

which finishes the proof.

With Theorem B.4 established, we can now derive the space distance for autoencoders with random
masking augmentation.

**Theorem B.5. Consider the spiked covariance model Eq.(2.5), under Assumption 3.1-3.3 and n >**
_d ≫_ _r, let WAE be the learned representation of augmented autoencoder with singular value_
_decomposition WAE = (UAEΣAEVAE[⊤]_ [)][⊤] _[(i.e., the optimal solution of optimization problem B.25).]_
_If we further assume {σi[2][}]i[d]=1_ _[are different from each other and][ σ](1)[2]_ _[/][(][σ]([2]r)_ _[−]_ _[σ]([2]r+1)[)][ < C][σ][ for some]_
_universal constant Cσ. Then there exist two universal constants Cρ > 0, c ∈_ (0, 1), such that when
_ρ < Cρ, we have_

E sin Θ (U _[⋆], UAE)_ _F_ _c[√]r._ (B.29)
_∥_ _∥_ _≥_

_Proof. Step1, similar to the proof of Theorem B.1, we first bound the difference between_ _M[ˆ] :=_
∆(XX _[⊤]) + 2D(XX_ _[⊤]) and Σ := Cov(ξξ[⊤]). Note that:_

_M_ Σ 2 = _XX_ _[⊤]_ Σ
_∥_ [ˆ] − _∥_ _∥_ _−_ _−_ 2[1] [∆(][XX] _[⊤][)][∥][2][ ≤∥][XX]_ _[⊤]_ _[−]_ [Σ][∥][2][ + 1]2 _[∥][∆(][XX]_ _[⊤]_ _[−]_ [Σ)][∥][2][ + 1]2 _[∥][∆(Σ)][∥][2]_

(B.30)
Since Σ is a diagonal matrix, then by Lemma B.4 we have:

_M_ Σ 2 2 _XX_ _[⊤]_ Σ 2 (B.31)
_∥_ [ˆ] − _∥_ _≤_ _∥_ _−_ _∥_

Now, directly apply equation (B.7)(B.8)(B.9) we can obtain that:


_r_
E _M_ Σ 2 ≲ _ν[2]_ 1 +
_∥_ [ˆ] − _∥_ _n_ [+][ r]n
 r


_d_
_n_ [+][ d]n


+ σ(1)[2]


_d_
(B.32)
_n_ _[σ][(1)][ν.]_


Step 2, bound the sin Θ distance between eigenspaces. As we have shown in step 1, the target matrix
of autoencoder is close to the covariance matrix of random noise, i.e., Σ. Note that Σ is assumed to
be diagonal matrix with different elements, hence its eigenspace only consists of canonical basis ei.
Denote UΣ to be the top-r eigenspace of Σ and _ei_ _i_ _C to be its corresponding basis vectors, apply_
_{_ _}_ _∈_


-----

the Davis-Kahan Theorem D.1 we can conclude that:

_M_ Σ 2
E sin Θ(UAE, UΣ) _F_ _−_ _∥_
_∥_ _∥_ _≤_ [2][√]σ([2][r]r[E]) _[∥]_ [ˆ] (r+1)

_[−]_ _[σ][2]_

_r_ _d_

≲[√]r [1] _ν[2]_ 1 + + σ(1)[2]

_σ(1)[2]_  r _n_ [+][ r]n  r _n_ [+][ d]n


_n_ _[σ][(1)][ν]_


≲[√]r


_ρ[2]_ +


_n_ [+][ ρ]


Step 3, obtain the final result by triangular inequality. By Assumption 3.3 we know that the distance
between canonical basis and the eigenspace of core features can be large:

sin Θ(U _[⋆], UΣ)_ _F_ [=][ ∥][U][ ⊤]Σ _F_ [=] _e[⊤]i_ _[U][ ⋆][∥]F[2]_ [=][ ∥][U][ ⋆][∥]F[2] _e[⊤]i_ _[U][ ⋆][∥]F[2]_
_∥_ _∥[2]_ _⊥[U][ ⋆][∥][2]_ _i∈X[d]/C_ _∥_ _[−]_ _iX∈C_ _∥_

_r2_
_r_ _rI(U_ _[⋆]) = r_ _O_ _._
_≥_ _−_ _−_ _d_ [log][ d]
 

Then apply the triangular inequality of sin Θ distance (Proposition A.4) we can obtain the lower
bound of autoencoder.


E∥ sin Θ(UAE, U _[⋆])∥F ≥_ E∥ sin Θ(U _[⋆], UΣ)∥F −_ E∥ sin Θ(UAE, UΣ)∥F

_≥_ _[√]r −_ _O r√d_ log d _−_ _O_ _√r_ _ρ[2]_ + r _nd_ [+][ ρ]

p


!!


By Assumption 3.2, it implies that when n and d is sufficient large and ρ is sufficient small (smaller
than a given constant Cρ > 0), there exist a universal constant c ∈ (0, 1) such that:

E∥ sin Θ(UAE, U _[⋆])∥F ≥_ _c[√]r._

Compared with Theorem 3.1, we can find that random masking augmentation makes no difference
to autoencoders, which justifies the fairness of our comparison between contrastive learning and
autoencoders.

B.2 PROOFS FOR SECTION 3.2

In this section, we will provide the proof of Theorem 3.3 and 3.4 with both regression and classification settings. The statement and detailed proof can be found in Theorem B.6 and B.7.

Before going onto the proof, we clarify our models and assumptions. Let WCL and WAE be the
learned representations based on train data X ∈ R[n][×][d]. We observe a new signal ˇx = U _[⋆]zˇ + ξ[ˇ] in-_
dependent of X following the spiked covariance model 2.5. For simplicity, assume ˇz _N_ (0, ν[2]Ir)
_∼_
and _ξ[ˇ] ⊥_ _zˇ. We consider the two major types of downstream tasks: classification and regression._
For binary classification task, we observe a new supervised sample ˇy following the binary response
model:

_yˇ|zˇ = Ber(F_ (⟨z, wˇ _[⋆]⟩/ν)),_ (B.33)

where F : R → [0, 1] is a known monotone increasing function satisfying 1 − _F_ (u) = F (−u) for
any u ∈ R. Notice that our model B.33 includes logistic models (when F (u) = 1/(1 + e[−][u])) and
probit models (when F (u) = Φ(u), where Φ is the cumulative distribution function of standard normal distribution.) We can also interpret model B.33 as a shallow neural network model with width
_r for binary classification. For regression task, we observe a new supervised sample ˇy following the_
linear regression model:

_yˇ = ⟨z, wˇ_ _[⋆]⟩/ν + ˇϵ,_ (B.34)

where ˇϵ ∼ (0, σϵ[2][)][ is independent of][ ˇ]z.


-----

In classification setting, we specify 0-1 loss, i.e., ℓc(δ) ≜ I{yˇ ̸= δ(ˇx)} for some predictor δ taking values in {0, 1}. For regression task, we employ squared error loss ℓr(δ) ≜ (ˇy − _δ(ˇx))[2]._
Based on some learned representation W, we consider a class of linear predictors, i.e., δW,w(ˇx) ≜
I{F (w[⊤]W ˇx) ≥ 1/2} for classification task and δW,w(ˇx) ≜ _w[⊤]W ˇx for regression task, where_
_w ∈_ R[r] is a weight vector w ∈ R[r]. Note that the learned representation depends only on unsupervised samples X. Let E [ ] and E [ ] the expectations with respect to (X, Z) and (ˇy, ˇx, ˇz),
_D_ _·_ _E_ _·_
respectively.

For notational simplicity, define the prediction risk of predictor δ for classification and regression
tasks as _c(δ) := E_ [ℓc(δ)] and _r(δ) := E_ [ℓr(δ)], respectively. Define Σx := ν[2]U _[⋆]U_ _[⋆][⊤]_ + Σ.
_R_ _D_ _R_ _D_
Since any representation W can be decomposed into W = V ΣW U _[⊤]_ by singular value decomposition, where U ∈ Od,r, {δW,w : w ∈ R[r]} = {δU ⊤,w : w ∈ R[r]}. Thus we write δU,w for δW,w
with a slight abuse of notation. Our goal as stated above is to bound the prediction risk of predictors
_{infδW,ww∈R :r E wE ∈[ℓ(δRW[r]}CL constructed upon the learned representations,w)] and inf_ _w∈Rr EE_ [ℓ(δWAE _,w)]._ _WCL and WAE, i.e., the quantity_

Now we state our result on the downstream task.

**Theorem B.6 (Excess Risk for Downstream Task: Upper Bound). Suppose the conditions in Theo-**
_rem 3.2 hold. Then, for classification task, we have_


_r[3][/][2]_


_dr_

_n_

!

_dr_

r


E [ inf
_D_ _w_ R[r][ E][E] [[][ℓ][c][(][δ][W][CL][,w][)]][ −] _w[inf]R[r][ E][E]_ [[][ℓ][c][(][δ][U][ ⋆][,w][)] =][ O]
_∈_ _∈_

_and for regression task,_

E [ inf
_D_ _w_ R[r][ E][E] [[][ℓ][r][(][δ][W][CL][,w][)]][ −] _w[inf]R[r][ E][E]_ [[][ℓ][r][(][δ][U][ ⋆][,w][)] =][ O]
_∈_ _∈_


log d +


_∧_ 1,


_r[3][/][2]_


log d +


We obtain the lower bound for the downstream predction risk with autoencoders.

**Theorem B.7. Suppose the conditions in Theorem 3.1 hold. Suppose r ≤** _rc for some constant_
_rc > 0. Additionally assume that ρ = Θ(1) is sufficiently small and n ≫_ _d ≫_ _r. For classification_
_task, assume F is differentiable at 0 and F_ _[′](0) > 0. Then,_

E [ inf
_D_ _w_ R[r][ E][E] [[][ℓ][c][(][δ][U][AE] _[,w][)]][ −]_ _w[inf]R[r][ E][E]_ [[][ℓ][c][(][δ][U][ ⋆][,w][)]][ ≳] [1][.]
_∈_ _∈_

_For regression task,_

E [ inf
_D_ _w_ R[r][ E][E] [[][ℓ][r][(][δ][U][AE] _[,w][)]][ −]_ _w[inf]R[r][ E][E]_ [[][ℓ][r][(][δ][U][ ⋆][,w][)]][ ≳] [1][.]
_∈_ _∈_

For two matrices A and B of the same order, we define A ⪰ _B when A−B is positive semi-definite._

The proofs of Theorem B.6 and B.7 relies on Lemma B.8, B.9, B.10, B.11 and B.12 which are
proved later in this section.

_Proof of Theorem B.6: Classification Task Part. Lemma B.10 gives for any U ∈_ Od,r,

E [ inf
_D_ _w_ R[r][ R][c][(][δ][U,w][)][ −] _w[inf]R[r][ R][c][(][δ][U][ ⋆][,w][)]]_
_∈_ _∈_

((κ(1 + ρ[2]))[3] + κρ[2](1 + ρ[−][2])[2] + (κρ[2] 1)[−][1])E [ sin Θ(U, U _[⋆])_ 2].
_≤_ _∨_ _D_ _∥_ _∥_

Substituting U ← _UAE combined with Assumption 3.2 and κ = O(1) concludes the proof._

_Proof of Theorem B.6: Regression Part. Note that under Assumption 3.2 and κ = O(1), (1 +_
_ρ[−][2])/(1 + κ[−][1]ρ[−][2])[2]_ = O(1). Lemma B.12 gives for any U ∈ Od,r,

E [ inf (1 + ρ[−][2])E [ sin Θ(U, U _[⋆])_ 2] _w[⋆]_ _._
_D_ _w_ R[r][ R][r][(][δ][U,w][)][ −] _w[inf]R[r][ R][r][(][δ][U][ ⋆][,w][)] =][ O]_ _D_ _∥_ _∥_ _∥_ _∥[2][]_
_∈_ _∈_
 

Theorem 3.2 with substitution U ← _UAE gives the desired result._


-----

_Proof of Theorem B.7: Classification Part. Lemma B.9 gives that for c1 := 1 −_ 1/(2κrc) ∈ (0, 1),
we can take n ≫ _d ≫_ _r and sufficiently small ρ > 0 so that ED[∥_ sin Θ(UAE, U _[⋆])∥F[2]_ []][ ≥] _[c][1][r][ holds.]_
By Lemma B.11,

E [ inf
_D_ _w_ R[r][ R][c][(][δ][U][AE] _[,w][)][ −]_ _w[inf]R[r][ R][c][(][δ][U][ ⋆][,w][)]]_
_∈_ _∈_

1

≳ [(1 +][ ρ][2][)][3][/][2] _F_ [)] (B.35)

(1 + κρ[2])[3][/][2][ ρ][2] 1 + ρ[2][ −] _[κ][(][r][ −∥]_ [sin Θ(][U][AE][, U][ ⋆][)][∥][2]
 

1

(B.36)

_≥_ (1 +[(1 +] κρ[ ρ][2][2][)])[3][3][/][/][2][2][ ρ][2] 1 + ρ[2][ −] _[κ][(1][ −]_ _[c][1][)][r]_

 

1

_,_ (B.37)

_≥_ (1 +[(1 +] κρ[ ρ][2][2][)])[3][3][/][/][2][2][ ρ][2] 1 + ρ[2][ −] [1]2

 


where the last inequality follows since r _rc. If we further take ρ = Θ(1) < 1/2, the right hand_
_≤_
becomes a positive constant. This concludes the proof.

_Proof of Theorem B.7: Regression Part. From proposition B.1, we have_

inf
_w_ R[r][ R][r][(][δ][U][AE] _[,w][)][ −]_ _w[inf]R[r][ R][r][(][δ][U][ ⋆][,w][)]_
_∈_ _∈_

= w[⋆][⊤]((I + (1/ν[2])U _[⋆][⊤]ΣU_ _[⋆])[−][1]_

_−_ _U_ _[⋆][⊤]UAE(UAE[⊤]_ _[U][ ⋆][U][ ⋆][⊤][U][AE]_ [+ (1][/ν][2][)][U][ ⊤]AE[Σ][U][AE][)][−][1][U][ ⊤]AE[U][ ⋆][)][w][⋆][.]

Thus from Lemma B.8,


inf
_w_ R[r][ R][r][(][δ][U][AE] _[,w][)][ −]_ _w[inf]R[r][ R][r][(][δ][U][ ⋆][,w][)]_
_∈_ _∈_

1
_≥_ 1 + ρ[−][2][ +][ ρ][2][κ] _∥_ sin Θ(UAE, U _[⋆])∥F[2]_ _[−]_ _[r]_ _∥w[⋆]∥[2]._


  []

Using Lemma B.9 and by the same argument in the proof of Theorem B.7: Classification Part, we
conclude the proof.

**Lemma B.5. For any U ∈** Od,r,


_ν[2]_
_λmin(ν[2]U_ _[⋆][⊤]U_ (U _[⊤]ΣxU_ )[−][1]U _[⊤]U_ _[⋆]) ≥_ _ν[2]_ + σ(1)[2] (1 −∥ sin Θ(U, U _[⋆])∥2[2][)][.]_

_Proof. Since λmin(AC)_ _λmin(A)λmin(C) for symmetric positive semi-definite matrices A and_
_≥_
_C,_

_λmin(ν[2]U_ _[⋆][⊤]U_ (U ΣxU )[−][1]U _U_ _[⋆])_

_[⊤]_ _[⊤]_

_λmin(U_ _U_ _[⋆]U_ _[⋆][⊤]U_ )λmin(ν[2](U ΣxU )[−][1])
_≥_ _[⊤]_ _[⊤]_

_ν[2]_
_λmin(I_ (I _U_ _U_ _[⋆]U_ _[⋆][⊤]U_ ))
_≥_ _−_ _−_ _[⊤]_ _λmax(ν[2]U_ _U_ _[⋆]U_ _[⋆][⊤]U + U_ ΣU )

_[⊤]_ _[⊤]_

_ν[2]_

(1 sin Θ(U, U _[⋆])_ 2[)][,]

_≥_ _ν[2]_ + σ(1)[2] _−∥_ _∥[2]_

where we used Weyl’s inequality λmin(A + C) ≥ _λmin(A) −∥C∥2 in the second inequality._

**Lemma B.6. For any U ∈** Od,r,


_ν[2]_
_λmax(ν[2]U_ _[⋆][⊤]U_ (U ΣxU )[−][1]U _U_ _[⋆])_

_[⊤]_ _[⊤]_ _≤_ _ν[2](1_ sin Θ(U, U _[⋆])_ 2) + σ([2]d)

_−∥_ _∥_


-----

_Proof. Since_ _AC_ 2 _A_ 2 _C_ 2,
_∥_ _∥_ _≤∥_ _∥_ _∥_ _∥_

_λmax(ν[2]U_ _[⋆][⊤]U_ (U ΣxU )[−][1]U _U_ _[⋆])_ _λmax(ν[2](U_ ΣxU )[−][1])

_[⊤]_ _[⊤]_ _≤_ _[⊤]_

_ν[2]_
_≤_ _λmin(ν[2]U_ _U_ _[⋆]U_ _[⋆][⊤]U + U_ ΣU )

_[⊤]_ _[⊤]_

_ν[2]_


_≤_ _λmin(ν[2]I_ _ν[2](I_ _U_ _U_ _[⋆]U_ _[⋆][⊤]U_ ) + U ΣU )

_−_ _−_ _[⊤]_ _[⊤]_

_ν[2]_

_,_

_≤_ _ν[2](1_ sin Θ(U, U _[⋆])_ 2) + σ([2]d)

_−∥_ _∥_

where we used Weyl’s inequality λmin(A + C) ≥ _λmin(A) −∥C∥2 and λmin(ν[2]I + U_ _[⊤]ΣU_ ) ≥
_ν[2]_ + σ([2]d)[.]


**Lemma B.7. For any U ∈** Od,r,

_ν[2](U_ _[⋆][⊤]ΣxU_ _[⋆])[−][1]_ _ν[2]U_ _[⋆][⊤]U_ (U ΣxU )[−][1]U _U_ _[⋆]_ 2
_∥_ _−_ _[⊤]_ _[⊤]_ _∥_

1 1 + ρ[−][2]
= O _._

1 sin Θ(U, U _[⋆])_ 2 [+][ κ][−][1][ρ][−][2] 1 + κ[−][1]ρ[−][2][ ∥] [sin Θ(][U, U][ ⋆][)][∥][2]

 _−∥_ _∥[2]_ 

_Proof. Observe that_

(U _[⋆][⊤]ΣxU_ _[⋆])[−][1]_ _U_ _[⋆][⊤]U_ (U ΣxU )[−][1]U _U_ _[⋆]_ 2
_∥_ _−_ _[⊤]_ _[⊤]_ _∥_
_≤∥(U_ _[⋆][⊤]ΣxU_ _[⋆])[−][1]_ _−_ (U _[⊤]ΣxU_ )[−][1]∥2 + ∥(U _[⊤]ΣxU_ )[−][1] _−_ _U_ _[⋆][⊤]U_ (U _[⊤]ΣxU_ )[−][1]U _[⊤]U_ _[⋆]∥2_
:= (T 1) + (T 2).

For the term (T 1),


(T 1) = (U ΣxU )[−][1](U ΣxU )(U _[⋆][⊤]ΣxU_ _[⋆])[−][1]_ (U ΣxU )[−][1](U _[⋆][⊤]ΣxU_ _[⋆])(U_ _[⋆][⊤]ΣxU_ _[⋆])[−][1]_ 2
_∥_ _[⊤]_ _[⊤]_ _−_ _[⊤]_ _∥_
(U ΣxU )[−][1] 2 _U_ ΣxU _U_ _[⋆][⊤]ΣxU_ _[⋆]_ 2 (U _[⋆][⊤]ΣxU_ _[⋆])[−][1]_ 2.
_≤∥_ _[⊤]_ _∥_ _∥_ _[⊤]_ _−_ _∥_ _∥_ _∥_

Note

_∥U_ _[⊤]ΣxU −_ _U_ _[⋆][⊤]ΣxU_ _[⋆]∥2 = ∥ν[2]U_ _[⊤]U_ _[⋆]U_ _[⋆][⊤]U −_ _ν[2]I + U_ _[⊤]ΣU −_ _U_ _[⋆][⊤]ΣU_ _[⋆]∥2_
_ν[2]_ sin Θ(U, U _[⋆])_ 2 [+][ ∥][U][ ⊤][Σ(][U][ −] _[U][ ⋆][) + (][U][ −]_ _[U][ ⋆][)][⊤][Σ][U][ ⋆][∥][2]_
_≤_ _∥_ _∥[2]_
_≤_ _ν[2]∥_ sin Θ(U, U _[⋆])∥2[2]_ [+ 2][σ](1)[2] _[∥][U][ −]_ _[U][ ⋆][∥][2][.]_

Also we have λmin(U _[⊤]ΣxU_ ) ≥ _ν[2](1_ _−∥_ sin Θ(U, U _[⋆])∥2[2][)+]_ _[σ]([2]d)_ [from the proof of Lemma B.6 and]
_λmin(U_ _[⋆][⊤]ΣxU_ _[⋆]) ≥_ _ν[2]_ + σ([2]d)[. Therefore]


1

2 [+ 2][σ](1)[2] _[∥][U][ −]_ _[U][ ⋆][∥][2][)][.]_
(ν[2] + σ([2]d)[)(][ν][2][(1][ −∥] [sin Θ(][U, U][ ⋆][)][∥]2[2][) +][ σ]([2]d)[)(][ν][2][∥] [sin Θ(][U, U][ ⋆][)][∥][2]


(T 1) ≤


For the term (T 2),

(T 2) = (U ΣxU )[−][1] _U_ _[⋆][⊤](U_ _[⋆]_ + (U _U_ _[⋆]))(U_ ΣxU )[−][1](U _[⋆]_ + (U _U_ _[⋆]))[⊤]U_ _[⋆]_ 2
_∥_ _[⊤]_ _−_ _−_ _[⊤]_ _−_ _∥_
= _U_ _[⋆][⊤](U_ _U_ _[⋆])(U_ ΣxU )[−][1] (U ΣxU )[−][1](U _U_ _[⋆])[⊤]U_ _[⋆]_
_∥−_ _−_ _[⊤]_ _−_ _[⊤]_ _−_

_U_ _[⋆][⊤](U_ _U_ _[⋆])(U_ ΣxU )[−][1](U _U_ _[⋆])[⊤]U_ _[⋆]_ 2
_−_ _−_ _[⊤]_ _−_ _∥_

1

(2 _U_ _U_ _[⋆]_ 2 + _U_ _U_ _[⋆]_ 2[)][.]

_≤_ _ν[2](1_ sin Θ(U, U _[⋆])_ 2[) +][ σ]([2]d) _∥_ _−_ _∥_ _∥_ _−_ _∥[2]_

_−∥_ _∥[2]_

From Lemma A.1, sin Θ(U, U _[⋆])_ 2 _U_ _U_ _[⋆]_ 2. Finally from these results and _U_ _U_ _[⋆]_ 2
2 _U_ _U_ _[⋆]_ 2, _∥_ _∥_ _≤∥_ _−_ _∥_ _∥_ _−_ _∥[2]_ _[≤]_
_∥_ _−_ _∥_

_ν[2](U_ _[⋆][⊤]ΣxU_ _[⋆])[−][1]_ _ν[2]U_ _[⋆][⊤]U_ (U ΣxU )[−][1]U _U_ _[⋆]_ 2
_∥_ _−_ _[⊤]_ _[⊤]_ _∥_

= O _ν[2](1_ sin Θ(νU, U[2] _[⋆])_ 2[) +][ σ]([2]d) _νν[2][2]_ ++ σ σ((1)[2][2]d) _∥U −_ _U_ _[⋆]∥2!._

_−∥_ _∥[2]_


-----

Since LHS does not depend on the orthogonal transformation U ← _UO where O ∈_ Or,r, we obtain

_ν[2](U_ _[⋆][⊤]ΣxU_ _[⋆])[−][1]_ _ν[2]U_ _[⋆][⊤]U_ (U ΣxU )[−][1]U _U_ _[⋆]_ 2
_∥_ _−_ _[⊤]_ _[⊤]_ _∥_

_ν[2]_ _ν[2]_ + σ(1)[2]

= O inf _._

_ν[2](1 −∥_ sin Θ(U, U _[⋆])∥2[2][) +][ σ]([2]d)_ _ν[2]_ + σ([2]d) _O∈Or,r_ _[∥][UO][ −]_ _[U][ ⋆][∥][2]!_

Combined again with Lemma A.1, we obtain the desired result.


**Lemma B.8. For any U ∈** Od,r,

_λmin(ν[2](U_ _[⋆][⊤]ΣxU_ _[⋆])[−][1]_ _ν[2]U_ _[⋆][⊤]U_ (U ΣxU )[−][1]U _U_ _[⋆])_
_−_ _[⊤]_ _[⊤]_

_ν[2]_

(r sin Θ(U, U _[⋆])_ _F_ [)][.]

_≥_ _ν[2]_ + σ(1)[2] _−_ _σ[ν]([2]d[2])_ _−∥_ _∥[2]_

_Proof. Observe_


_λmin(ν[2](U_ _[⋆][⊤]ΣxU_ _[⋆])[−][1]_ _ν[2]U_ _[⋆][⊤]U_ (U ΣxU )[−][1]U _U_ _[⋆])_
_−_ _[⊤]_ _[⊤]_

_λmin((I + (1/ν[2])U_ _[⋆][⊤]ΣU_ _[⋆])[−][1])_ _U_ _[⋆][⊤]U_ (U _U_ _[⋆]U_ _[⋆][⊤]U + (1/ν[2])U_ ΣU )[−][1]U _U_ _[⋆]_ 2.
_≥_ _−∥_ _[⊤]_ _[⊤]_ _[⊤]_ _∥_

Since U _[⊤]U_ _[⋆]U_ _[⋆][⊤]U ⪰_ 0, it follows that (U _[⊤]U_ _[⋆]U_ _[⋆][⊤]U + (1/ν[2])U_ _[⊤]ΣU_ )[−][1] _⪯_ _ν[2](U_ _[⊤]ΣU_ )[−][1]. Thus

_U_ _[⋆][⊤]U_ (U _U_ _[⋆]U_ _[⋆][⊤]U + (1/ν[2])U_ ΣU )[−][1]U _U_ _[⋆]_ 2
_∥_ _[⊤]_ _[⊤]_ _[⊤]_ _∥_
_≤_ _ν[2]λmax((U_ _[⊤]ΣU_ )[−][1])∥U _[⋆][⊤]U_ _∥2[2]_

_≤_ _σ[ν]([2]d[2])_ _∥U_ _[⋆][⊤]U_ _∥F[2]_


= _[ν][2]_ (r sin Θ(U, U _[⋆])_ _F_ [)][,]

_σ([2]d)_ _−∥_ _∥[2]_

where we used λmax((U _[⊤]ΣU_ )[−][1]) ≤ 1/λmin(U _[⊤]ΣU_ ) ≤ 1/σ([2]d) [and][ ∥][sin Θ (][U][1][, U][2][)][∥]F[2] [=][ r][ −]
_U1[⊤][U][2]_ _F_ [from Proposition A.1. Combined with Lemma B.6, we obtain]

[2] _λmin(ν[2](U_ _[⋆][⊤]ΣxU_ _[⋆])[−][1]_ _−_ _ν[2]U_ _[⋆][⊤]U_ (U _[⊤]ΣxU_ )[−][1]U _[⊤]U_ _[⋆])_

_ν[2]_

(r sin Θ(U, U _[⋆])_ _F_ [)][.]

_≥_ _ν[2]_ + σ(1)[2] _−_ _σ[ν]([2]d[2])_ _−∥_ _∥[2]_


**Lemma B.9. Suppose the conditions in Theorem 3.1 hold. Fix c1 ∈** (0, 1). There exists a constant
_c2 > 0such that if_ _r log d/d ∨_ _ρ[2]_ _∨_ _d/n < c2, then_
p ED∥ sin Θ(UAE, U _[⋆])∥F[2]_ _[≥]_ _[c][1][r,]_

_where c1_ (0, 1) is a universal constant.
_∈_


_Proof. By Cauchy-Schwartz inequality,_

ED∥ sin Θ(UAE, U _[⋆])∥F[2]_ _[−]_ _[r]_

(E sin Θ(UAE, U _[⋆])_ _F )[2]_ _r_
_≥_ _D∥_ _∥_ _−_

= (E sin Θ(UAE, U _[⋆])_ _F_ _r)_ E sin Θ(UAE, U _[⋆])_ _F +_ _r_
_D∥_ _∥_ _−_ _[√]_ _D∥_ _∥_ _[√]_

From Theorem 3.1, there exists a constant c3 > 0 such that 


ED ∥sin Θ (U _[⋆], UAE)∥F ≥_ _[√]r −_ _c3_


log d _c3√r_
_−_


_ρ[2]_ +


_n_ [+][ ρ]


-----

Therefore combined with a trivial bound sin Θ(UAE, U _[⋆])_ _F_ _r,_
_∥_ _∥_ _≤_ _[√]_


_r[1][/][2]_

_√d_

2 _[r][1][/][2]_

_√_


log d + ρ[2] +


ED∥ sin Θ(UAE, U _[⋆])∥F[2]_ _[−]_ _[r][ ≥−][rc][3]_

_rc3_
_≥−_


_n_ [+][ ρ]


log d ∨ 6ρ[2] _∨_ 6


where we used ρ _d/n ≤_ _ρ[2]_ _∨d/n ≤_ _ρ[2]_ _∨_ _d/n since d < n. Thus we can take c2 = 6(1−c1)/c3._

This concludes the proof.

p p

**Lemma B.10. For any U ∈** Od,r,

E [ inf
_D_ _w_ R[r][ R][c][(][δ][U,w][)][ −] _w[inf]R[r][ R][c][(][δ][U][ ⋆][,w][)]]_
_∈_ _∈_

((κ(1 + ρ[2]))[3] + κρ[2](1 + ρ[−][2])[2] + (κρ[2] 1)[−][1])E [ sin Θ(U, U _[⋆])_ 2].
_≤_ _∨_ _D_ _∥_ _∥_

_Proof. Recall that we are considering the class of linear classifiers {δU,w : w ∈_ R[r]}, where
_δU,w(ˇx) = I{F_ (ˇx[⊤]Uw) > 1/2}. For notational simplicity, write β := Uw and β[⋆] := U _[⋆]w[⋆]._

_c(δU,w) = P_ (δU,w(ˇx) = ˇy) = P (ˇy = 0, F (ˇx[⊤]β) > 1/2) + P (ˇy = 1, F (ˇx[⊤]β) 1/2).
_R_ _E_ _̸_ _E_ _E_ _≤_

Since F (0) = 1/2 and F is monotone increasing, the false positive probability becomes

P (ˇy = 0, F (ˇx[⊤]β) > 1/2) = P (ˇy = 0, ˇx[⊤]β > 0)
_E_ _E_

= E [E [I _yˇ = 0_ _x,ˇ_ ˇz]I _xˇ[⊤]β > 0_ ]
_E_ _E_ _{_ _}|_ _{_ _}_

= E [(1 _F_ (ν[−][1]zˇ[⊤]U _[⋆][⊤]β[⋆]))I_ _xˇ[⊤]β > 0_ ].
_E_ _−_ _{_ _}_


Write ω := ˇx[⊤]β and ω[⋆] := ν[−][1]zˇ[⊤]U _[⋆][⊤]β[⋆]. From assumption, (ω[⋆], ω) jointly follows a normal_
distribution with mean 0. Write v[⋆][2] := Var(ω[⋆]) = w[⋆][⊤]w[⋆], v[2] := Var(ω) = β[⊤]Σxβ, where
Σx := ν[2]U _[⋆]U_ _[⋆][⊤]_ + Σ. Let τ := Cor(ω[⋆], ω) = νw[⋆][⊤]U _[⋆][⊤]β/(v[⋆]v). By a formula for conditional_
normal distribution, we have ω|ω[⋆] _∼_ _N_ (τvω[⋆]/v[⋆], v[2](1 − _τ_ [2])). This gives

P (ˇy = 0, F (ˇx[⊤]β) > 1/2)
_E_
= E [(1 _F_ (ω[⋆]))I _ω > 0_ ]
_E_ _−_ _{_ _}_
= E [(1 _F_ (ω[⋆]))E [I _ω > 0_ _ω[⋆]]]_
_E_ _−_ _E_ _{_ _}|_
= E [(1 _F_ (ω[⋆]))P (ω > 0 _ω[⋆])]_
_E_ _−_ _E_ _|_

_ω_ _τvω⋆/v⋆_ _τvω[⋆]/v[⋆]_

= E (1 _F_ (ω[⋆]))P _−_
_E_ _−_ _E_ _v(1_ _τ_ [2])[1][/][2][ >][ −] _v(1_ _τ_ [2])[1][/][2]

  _−_ _−_ 

= E [(1 _F_ (ω[⋆]))Φ(αω[⋆]/v[⋆])]
_E_ _−_ _[ω][⋆]_
= E [(1 _F_ (ω[⋆]))Φ(αω[⋆]/v[⋆])I _ω[⋆]_ _> 0_ ] + E [(1 _F_ (ω[⋆]))Φ(αω[⋆]/v[⋆])I _ω[⋆]_ _< 0_ ],
_E_ _−_ _{_ _}_ _E_ _−_ _{_ _}_


ΨwhereF (s[2] Φ) := 2 is cumulative distribution function ofEu _N_ (0,s2)[F (u)I _u > 0_ ]. When N F(0(u, 1)) = 1 and/ α(1+ :=e τ/[−][u])(1, Ψ −F (τs[2][2]))[1] is called the logistic-[/][2]. We define ΨF as
_∼_ _{_ _}_
normal integral, whose analytical form is not known (Pirjol (2013)). Since a random variable ω[⋆] is
symmetric about mean 0 and F (u) = 1 − _F_ (−u),

E [(1 _F_ (ω[⋆]))Φ(αω[⋆]/v[⋆])I _ω[⋆]_ _< 0_ ] = E [(1 _F_ ( _ω[⋆]))(1_ Φ(αω[⋆]/v[⋆]))I _ω[⋆]_ _> 0_ ]
_E_ _−_ _{_ _}_ _E_ _−_ _−_ _−_ _{_ _}_
= E [F (ω[⋆])(1 Φ(αω[⋆]/v[⋆]))I _ω[⋆]_ _> 0_ ].
_E_ _−_ _{_ _}_

Hence

P (ˇy = 0, F (ˇx[⊤]β) > 1/2)
_E_
= E [(Φ(αω[⋆]/v[⋆]) + F (ω[⋆]) 2F (ω[⋆])Φ(αω[⋆]/v[⋆]))I _ω[⋆]_ _> 0_ ]
_E_ _−_ _{_ _}_

= [1]

2 [Ψ][F][ (][v][⋆][2][)][ −] [E][E] [[(2][F] [(][ω][⋆][)][ −] [1)Φ(][αω][⋆][/v][⋆][)][I][{][ω][⋆] _[>][ 0][}][]][.]_


-----

Note that the true negative probability is exactly the same as the false positive probability under our
settings:


P (ˇy = 1, F (ˇx[⊤]β) 1/2) = E [F (ˇx[⊤]β[⋆])I _xˇ[⊤]β_ 0 ]
_E_ _≤_ _E_ _{_ _≤_ _}_

= E [F ( _xˇ[⊤]β[⋆])I_ _xˇ[⊤]β_ 0 ]
_E_ _−_ _{_ _≥_ _}_

= E [(1 _F_ (ˇx[⊤]β[⋆]))I _xˇ[⊤]β_ 0 ]
_E_ _−_ _{_ _≥_ _}_

= P (ˇy = 0, F (ˇx[⊤]β) > 1/2).
_E_

_c(δU,w) = ΨF (v[⋆][2])_ 2E [(2F (ω[⋆]) 1)Φ(αω[⋆]/v[⋆])I _ω[⋆]_ _> 0_ ].
_R_ _−_ _E_ _−_ _{_ _}_


Therefore

Let


_τmax,U := sup_
_w_ R[r][ νw][⋆][⊤][U][ ⋆][⊤][Uw/][(][w][⋆][⊤][w][⋆][w][⊤][U][ ⊤][Σ][x][Uw][)][1][/][2][,]
_∈_

_τmax,U ⋆_ := sup
_w_ R[r][ νw][⋆][⊤][w/][(][w][⋆][⊤][w][⋆][w][⊤][U][ ⋆][⊤][Σ][x][U][ ⋆][w][)][1][/][2][.]
_∈_

From Cauchy-Schwartz inequality,

_τmax[2]_ _,U_ [=][ ν][2][w][⋆][⊤][U][ ⋆][⊤][U] [(][U][ ⊤][Σ][x][U] [)][−][1][U][ ⊤][U][ ⋆][w][⋆] _,_

_w[⋆][⊤]w[⋆]_


_τmax[2]_ _,U_ _[⋆]_ [=][ ν][2][w][⋆][⊤][(][U]w[ ⋆][⋆][⊤][⊤][Σ]w[x][⋆][U][ ⋆][)][−][1][w][⋆] _._

Define αmax,U := τmax,U _/(1 −_ _τmax[2]_ _,U_ [)][1][/][2][ and][ α][max][,U][ ⋆] [:=][ τ][max][,U][ ⋆] _[/][(1][ −]_ _[τ][ 2]max,U_ _[⋆]_ [)][1][/][2][. Then,]
since on the event where ω[⋆] _> 0, α 7→_ Φ(αω[⋆]/v[⋆]) is monotone increasing and 2F (w[⋆]) − 1 is
non-negative, we have


inf
_w_ R[r][ R][c][(][δ][U,w][) = Ψ][F][ (][v][⋆][2][)][ −] [2][E][E] [[(2][F] [(][ω][⋆][)][ −] [1)Φ(][α][max][,U] _[ω][⋆][/v][⋆][)][I][{][ω][⋆]_ _[>][ 0][}][]]_
_∈_

inf
_w_ R[r][ R][c][(][δ][U][ ⋆][,w][) = Ψ][F][ (][v][⋆][2][)][ −] [2][E][E] [[(2][F] [(][ω][⋆][)][ −] [1)Φ(][α][max][,U][ ⋆] _[ω][⋆][/v][⋆][)][I][{][ω][⋆]_ _[>][ 0][}][]][.]_
_∈_

This yields

inf
_w_ R[r][ R][c][(][δ][U,w][)][ −] _w[inf]R[r][ R][c][(][δ][U][ ⋆][,w][)]_
_∈_ _∈_

= 2E [(2F (ω[⋆]) 1)(Φ(αmax,U ⋆ _ω[⋆]/v[⋆])_ Φ(αmax,U _ω[⋆]/v[⋆]))I_ _ω[⋆]_ _> 0_ ].
_E_ _−_ _−_ _{_ _}_

Note that for any a, b ≥ 0,

_|Φ(b) −_ Φ(a)| ≤ _φ(a ∧_ _b)|b −_ _a|,_

where φ is a density function of standard normal distribution. Observe


inf
_w_ R[r][ R][c][(][δ][U,w][)][ −] _w[inf]R[r][ R][c][(][δ][U][ ⋆][,w][)]_
_∈_ _∈_

2E [(2F (ω[⋆]) 1) Φ(αmax,U _[⋆]_ _ω[⋆]/v[⋆])_ Φ(αmax,U _ω[⋆]/v[⋆])_ I _ω[⋆]_ _> 0_ ]
_≤_ _E_ _−_ _|_ _−_ _|_ _{_ _}_

_∞_

≲ [2] (2F (ω[⋆]) 1) _αmax,U ⋆_ _αmax,U_ _ω[⋆]φ((αmax,U ⋆_ _αmax,U_ )ω[⋆]/v[⋆]) _[φ][(][ω][⋆][/v][⋆][)]_ dω[⋆]

_v[⋆]_ 0 _−_ _|_ _−_ _|_ _∧_ _v[⋆]_

Z

_∞_

≲ _[|][α][max][,U][ ⋆]_ _[−]_ _[α][max][,U]_ _[|]_ (2F (ω[⋆]) 1)φ((αmax,U ⋆ _αmax,U_ )ω[⋆]/v[⋆]) dω[⋆]

_v[⋆]_ 0 _−_ _∧_

Z

_∞_ 1/(2((αmax,U ⋆ _αmax,U_ )[−][2]v[⋆][2]))ω[⋆][2][]

= (2F (ω[⋆]) 1) [exp] _−_ _∧_ dω[⋆]

_[|][α]α[max]max[,U],U[ ⋆] ⋆_ _[−]∧_ _α[α]max[max],U[,U]_ _[|]_ Z0 _−_   2π((αmax,U ⋆ _∧_ _αmax,U_ )[−][2]v[⋆][2])

= (ΨF (((αmax,U ⋆ _αmax,Up ⋆_ )[−][2]v[⋆][2])) 1/2),

_[|][α]α[max]max[,U],U[ ⋆][⋆]_ _[−]_ _α[α]max[max],U[,U]_ _[|]_ _∧_ _−_

_∧_

where we used supu>0 uφ(u) < ∞. Since (a − _b) = (a[2]_ _−_ _b[2])/(a + b) ≤_ (a[2] _−_ _b[2])/(a ∧_ _b) for_
_a, b > 0, and ΨF_ 1, we obtain
_≤_

inf _|αmax[2]_ _,U_ _[⋆]_ _[−]_ _[α]max[2]_ _,U_ _[|]_ _._
_w∈R[r][ R][c][(][δ][U,w][)][ −]_ _w[inf]∈R[r][ R][c][(][δ][U][ ⋆][,w][)][ ≲]_ _αmax[2]_ _,U_ _[⋆]_ _[∧]_ _[α]max[2]_ _,U_


-----

When τmax,U ⋆ _τmax,U_, since τ _τ_ [2]/(1 _τ_ [2]) is increasing in τ > 0,
_≥_ _7→_ _−_

_αmax[2]_ _,U_ _[⋆]_ _[−]_ _[α]max[2]_ _,U_
inf
_w∈R[r][ R][c][(][δ][U,w][)][ −]_ _w[inf]∈R[r][ R][c][(][δ][U][ ⋆][,w][)][ ≲]_ _αmax[2]_ _,U_

= _τmax[2]_ _,U_ _[⋆]_ _[−]_ _[τ][ 2]max,U_ _._ (B.38)

(1 _τmax[2]_ _,U_ _[⋆]_ [)][τ][ 2]max,U
_−_


From Lemma B.5 and B.6, we have

_ν[2]_ _ν[2]_

_ν[2]_ + σ(1)[2] (1 −∥ sin Θ(U, U _[⋆])∥2[2][)][ ≤]_ _[τ][ 2]max,U_ _[≤]_ _ν[2](1_ sin Θ(U, U _[⋆])_ 2[) +][ σ]([2]d) _,_

_−∥_ _∥[2]_

_ν[2]_ _ν[2]_

_ν[2]_ + σ(1)[2] _≤_ _τmax[2]_ _,U_ _[⋆]_ _[≤]_ _ν[2]_ + σ([2]d) _._ (B.39)


Then, equation B.38 becomes

inf
_w_ R[r][ R][c][(][δ][U,w][)][ −] _w[inf]R[r][ R][c][(][δ][U][ ⋆][,w][)]_
_∈_ _∈_

_ν[2]_ + σ([2]d) _ν[2]_ + σ(1)[2]
≲ max,U _[⋆]_ _[−]_ _[τ][ 2]max,U_ [)]

_σ([2]d)_ _ν[2](1_ sin Θ(U, U _[⋆])_ 2[)(][τ][ 2]

_−∥_ _∥[2]_

_ν[2]_ + σ([2]d) _ν[2]_ + σ(1)[2]
_≤_ _σ([2]d)_ _ν[2](1_ sin Θ(U, U _[⋆])_ 2[)] _[∥][ν][2][(][U][ ⋆][⊤][Σ][x][U][ ⋆][)][−][1][ −]_ _[ν][2][U][ ⋆][⊤][U]_ [(][U][ ⊤][Σ][x][U] [)][−][1][U][ ⊤][U][ ⋆][∥][2]

_−∥_ _∥[2]_

(κρ[2] + 1)(ρ[−][2] + 1)[2]
_≤_ (1 + κ[−][1]ρ[−][2])(1 −∥ sin Θ(U, U _[⋆])∥2[2][)][2][ ∥]_ [sin Θ(][U, U][ ⋆][)][∥][2]

_κρ[2](ρ[−][2]_ + 1)[2]
=

(1 sin Θ(U, U _[⋆])_ 2[)][2][ ∥] [sin Θ(][U, U][ ⋆][)][∥][2][.]
_−∥_ _∥[2]_


where the last inequality follows from Lemma B.7.

On the event where sin Θ(U, U _[⋆])_ 2
_∥_ _∥[2]_ _[≤]_ [1][/][2][,]

inf
_w_ R[r][ R][c][(][δ][U,w][)][ −] _w[inf]R[r][ R][c][(][δ][U][ ⋆][,w][)][ ≲]_ _[κρ][2][(1 +][ ρ][−][2][)][2][∥]_ [sin Θ(][U, U][ ⋆][)][∥][2][.]
_∈_ _∈_

When τmax,U ⋆ _< τmax,U_, on the event where sin Θ(U, U _[⋆])_ 2 _κ[−][1]ρ[−][2]/2,_
_∥_ _∥_ _≤_


inf
_w_ R[r][ R][c][(][δ][U,w][)][ −] _w[inf]R[r][ R][c][(][δ][U][ ⋆][,w][)]_
_∈_ _∈_

_ν[2]_ + σ(1)[2] _ν[2](1_ sin Θ(U, U _[⋆])_ 2[) +][ σ]([2]d)
≲ _−∥_ _∥[2]_ (τmax[2] _,U_ max,U _[⋆]_ [)]

_ν[2]_ _ν[2]_ sin Θ(U, U _[⋆])_ 2 [+][ σ]([2]d) _[−]_ _[τ][ 2]_

_−_ _∥_ _∥[2]_

(ν[2] + σ(1)[2] [)][2] 1
_≤_ _ν[2]_ _−ν[2]∥_ sin Θ(U, U _[⋆])∥2[2]_ [+][ σ]([2]d)

_ν[2](U_ _[⋆][⊤]ΣxU_ _[⋆])[−][1]_ _ν[2]U_ _[⋆][⊤]U_ (U ΣxU )[−][1]U _U_ _[⋆]_ 2
_× ∥_ _−_ _[⊤]_ _[⊤]_ _∥_

(1 + ρ[−][2])[3]
_≤_ (−∥ sin Θ(U, U _[⋆])∥2[2]_ [+][ κ][−][1][ρ][−][2][)][3][ ∥] [sin Θ(][U, U][ ⋆][)][∥][2]

≲ (κ(1 + ρ[2]))[3]∥ sin Θ(U, U _[⋆])∥2,_


where we used Lemma B.7 again.

In summary, on the event where sin Θ(U, U _[⋆])_ 2 _κ[−][1]ρ[−][2]/2_ 1/2,
_∥_ _∥_ _≤_ _∧_

inf
_w_ R[r][ R][c][(][δ][U,w][)][ −] _w[inf]R[r][ R][c][(][δ][U][ ⋆][,w][)]_
_∈_ _∈_

≲ ((κ(1 + ρ[2]))[3] + κρ[2](1 + ρ[−][2])[2])∥ sin Θ(U, U _[⋆])∥2._


-----

inequalityOn the other hand, on the event where inf _w∈Rr Rc(δU,w) −_ inf _w∈Rr R ∥_ sin Θ(c(δU ⋆,wU, U) ≤[⋆])1∥. This gives2 > κ[−][1]ρ[−][2]/2 ∧ 1/2, we have a trivial

E [ inf
_D_ _w_ R[r][ R][c][(][δ][U,w][)][ −] _w[inf]R[r][ R][c][(][δ][U][ ⋆][,w][)]]_
_∈_ _∈_

≲ ((κ(1 + ρ[2]))[3] + κρ[2](1 + ρ[−][2])[2])E [ sin Θ(U, U _[⋆])_ 2]
_D_ _∥_ _∥_

+ P ( sin Θ(U, U _[⋆])_ 2 > κ[−][1]ρ[−][2]/2 1/2)
_D_ _∥_ _∥_ _∧_

≲ ((κ(1 + ρ[2]))[3] + κρ[2](1 + ρ[−][2])[2] + (κρ[2] 1))E [ sin Θ(U, U _[⋆])_ 2],
_∨_ _D_ _∥_ _∥_

where the last inequality follows from Markov’s inequality.

**Lemma B.11. Suppose U ∈** Od,r satisfies 1/(1 + ρ[2]) − _κ(r −∥_ sin Θ(U, U _[⋆])∥F[2]_ [)][ ≥] [0][. Then,]

inf
_w_ R[r][ R][c][(][δ][U,w][)][ −] _w[inf]R[r][ R][c][(][δ][U][ ⋆][,w][)]_
_∈_ _∈_

1

≳ [(1 +][ ρ][2][)][3][/][2] _F_ [)] _._

(1 + κρ[2])[3][/][2][ ρ][2] 1 + ρ[2][ −] _[κ][(][r][ −∥]_ [sin Θ(][U, U][ ⋆][)][∥][2]
 


_Proof. We firstly bound the term τmax[2]_ _,U_ _[⋆]_ _[−]_ _[τ][ 2]max,U_ [. From Lemma B.8,]

_τmax[2]_ _,U_ _[⋆]_ _[−]_ _[τ][ 2]max,U_

_[≥]_ _[λ][min][(][ν][2][(][U][ ⋆][⊤][Σ][x][U][ ⋆][)][−][1][ −]_ _[ν][2][U][ ⋆][⊤][U]_ [(][U][ ⊤][Σ][x][U] [)][−][1][U][ ⊤][U][ ⋆][)]

_ν[2]_

(r sin Θ(U, U _[⋆])_ _F_ [)][.] (B.40)

_≥_ _ν[2]_ + σ(1)[2] _−_ _σ[ν]([2]d[2])_ _−∥_ _∥[2]_

From assumption, RHS of equation B.40 is non-negative. Then using the inequality a − _b = (a[2]_ _−_
_b[2])/(a + b) ≥_ (a[2] _−_ _b[2])/(2a) for a ≥_ _b ≥_ 0,


_αmax,U_ _[⋆]_ _−_ _αmax,U ≳_


1

_αmax,U ⋆_ [(][α]max[2] _,U_ _[⋆]_ _[−]_ _[α]max[2]_ _,U_ _[⋆]_ [)]


(1 _τmax[2]_ _,U_ _[⋆]_ [)][1][/][2] _τmax[2]_ _,U_ _[⋆]_ _[−]_ _[τ][ 2]max,U_
_−_
_≥_ _τmax,U ⋆_ (1 _τmax[2]_ _,U_ _[⋆]_ [)(1][ −] _[τ][ 2]max,U_ [)] _[.]_

_−_

From equation B.39 and equation B.40,


_αmax,U ⋆_ _αmax,U_
_−_

1/2

≳ _ν[2]_ + σ([2]d) _ν[2]_ + σ(1)[2]

_ν[2]_ ! _σ(1)[2]_


3/2
_ν[2]_

(r sin Θ(U, U _[⋆])_ _F_ [)]

! _ν[2]_ + σ(1)[2] _−_ _σ[ν]([2]d[2])_ _−∥_ _∥[2]_


_ν[2]_
= (1 + κ[−][1]ρ[−][2])[1][/][2](1 + ρ[2])[3][/][2] (r sin Θ(U, U _[⋆])_ _F_ [)]

_ν[2]_ + σ(1)[2] _−_ _σ[ν]([2]d[2])_ _−∥_ _∥[2]_

From the proof of Lemma B.10,


(B.41)


inf
_w_ R[r][ R][c][(][δ][U,w][)][ −] _w[inf]R[r][ R][c][(][δ][U][ ⋆][,w][)]_
_∈_ _∈_

= 2E [(2F (ω[⋆]) 1)(Φ(αmax,U ⋆ _ω[⋆]/v[⋆])_ Φ(αmax,U _ω[⋆]/v[⋆]))I_ _ω[⋆]_ _> 0_ ].
_E_ _−_ _−_ _{_ _}_

Note that for any b ≥ _a ≥_ 0, Φ(b) − Φ(a) ≥ _φ(b)(b −_ _a). Since we assume RHS of equation B.40_
is positive, αmax,U ⋆ _αmax,U_ . Thus on the event where ω[⋆] _> 0, αmax,U ⋆_ _ω[⋆]/v[⋆]_ _αmax,U_ _ω[⋆]/v[⋆]._
_≥_ _≥_
Observe

inf
_w_ R[r][ R][c][(][δ][U,w][)][ −] _w[inf]R[r][ R][c][(][δ][U][ ⋆][,w][)]_
_∈_ _∈_

2E [(2F (ω[⋆]) 1)φ(αmax,U ⋆ _ω[⋆]/v[⋆])(αmax,U ⋆_ _ω[⋆]/v[⋆]_ _αmax,U_ _ω[⋆]/v[⋆])I_ _ω[⋆]_ _> 0_ ]
_≥_ _E_ _−_ _−_ _{_ _}_

_∞_

= [2] (2F (ω[⋆]) 1)ω[⋆] _[φ][(][ω][⋆][/v][⋆][)]_ _φ(αmax,U ⋆_ _ω[⋆]/v[⋆]) dω[⋆]_

_v[⋆]_ [(][α][max][,U][ ⋆] _[−]_ _[α][max][,U]_ [)] 0 _−_ _v[⋆]_
Z

_∞_

_≃_ _[α][max][,U][ ⋆]v[−][⋆]_ _[α][max][,U]_ 0 (2F (ω[⋆]) − 1)ω[⋆] exp _−(1/2)(1 + αmax[2]_ _,U_ _[⋆]_ [)][ω][⋆][2][/v][⋆][2][] dω[⋆]

Z 

_∞_

_≃_ _[α][max]1 +[,U] α[ ⋆]_ max[2][−] _[α],U[max][⋆]_ _[,U]_ 0 (2F ((1 + αmax[2] _,U_ _[⋆]_ [)][−][1][/][2][v][⋆][ω][⋆][)][ −] [1)][ω][⋆] [exp] _−(1/2)ω[⋆][2][]_ dω[⋆] _,_

Z 


-----

where in the last equality we transformed w[⋆] (1 + αmax[2] _,U_ _[⋆]_ [)][1][/][2][w][⋆][/v][⋆][. Since][ F] [(][u][)][ is differen-]
_→_
tiable at 0 and F (0) = 1/2,

_F_ (u) − 1/2 = F _[′](0)u + o(u)._

Thus there exists a constant ϵ > 0 only depending on F such that 2(F (u) − 1/2) ≥ _F_ _[′](0)u for all_
_u ∈_ [0, ϵ] since F _[′](0) > 0. This gives_

inf
_w_ R[r][ R][c][(][δ][U,w][)][ −] _w[inf]R[r][ R][c][(][δ][U][ ⋆][,w][)]_
_∈_ _∈_

≳ _[α][max]1 +[,U] α[ ⋆]_ max[2][−] _[α],U[max][⋆]_ _[,U]_ _F_ _[′](0)(1 + αmax[2]_ _,U_ _[⋆]_ [)][−][1][/][2][v][⋆]

_ϵ(1+α[2]max,U_ _[⋆]_ [)][1][/][2][v][⋆]
_×_ 0 _ω[⋆][2]_ exp _−(1/2)ω[⋆][2][]_ dω[⋆]
Z  _ϵv[⋆]_

≳ _[α][max][,U][ ⋆]_ _[−]_ _[α][max][,U]_ (1 + αmax[2] _,U_ _[⋆]_ [)][−][1][/][2][v][⋆] _ω[⋆][2]_ exp (1/2)ω[⋆][2][] dω[⋆]

1 + αmax[2] _,U_ _[⋆]_ 0 _−_
Z 

≳ _[α][max][,U][ ⋆]_ _[−]_ _[α][max][,U]_ (1 + αmax[2] _,U_ _[⋆]_ [)][−][1][/][2][.]

1 + αmax[2] _,U_ _[⋆]_


The last inequality follows since v[⋆] = _w[⋆]_ = 1 by assumption. It is noted that αmax[2] _,U_ _[⋆]_ _[≤]_ _[ν][2][/σ]([2]d)_
_∥_ _∥_
from equation B.39. Therefore with equation B.41,

inf
_w_ R[r][ R][c][(][δ][U,w][)][ −] _w[inf]R[r][ R][c][(][δ][U][ ⋆][,w][)]_
_∈_ _∈_


1

(1 + κρ[2])[3][/][2][ (1 +][ κ][−][1][ρ][−][2][)][1][/][2][(1 +][ ρ][2][)][3][/][2]


1

_F_ [)]
1 + ρ[−][2][ −] _[κρ][2][(][r][ −∥]_ [sin Θ(][U, U][ ⋆][)][∥][2]


≳ [(1 +][ ρ][2][)][3][/][2]

(1 + κρ[2])[3][/][2][ ρ][2]


1

_F_ [)]
1 + ρ[2][ −] _[κ][(][r][ −∥]_ [sin Θ(][U, U][ ⋆][)][∥][2]


**Proposition B.1. For any U ∈** Od,r,

inf _ϵ_ _[.]_
_w_ R[r][ R][r][(][δ][U,w][) =][ ν][2][w][⋆][⊤][(][I][ −] _[ν][2][U][ ⋆][⊤][U]_ [(][ν][2][U][ ⊤][U][ ⋆][U][ ⋆][⊤][U][ +][ U][ ⊤][Σ][U] [)][−][1][U][ ⊤][U][ ⋆][)][w][⋆] [+][ σ][2]
_∈_

_Proof of Proposition B.1. Generate random variables (ˇx, ˇz,_ _ξ,[ˇ]_ ˇϵ) following the model equation B.34.
We calculate the prediction risk of δU,w as:


_r(δU,w) := E_ (ˇy _xˇ[⊤]Uw)[2]_
_R_ _E_ _−_

= Var (ν[−][1]zˇ[⊤]w[⋆] + ˇϵ)[2] 2Cov (ν[−][1]zˇ[⊤]w[⋆] + ˇϵ, U _[⋆]zˇ + ξ[ˇ])Uw_
_E_ _−_ _E_

+ w[⊤]U _[⊤]VarE_ (U _[⋆]zˇ + ξ[ˇ])Uw_

= _w[⋆]_ + σϵ[2]
_∥_ _∥[2]_ _[−]_ [2][νw][⋆][⊤][U][ ⋆][⊤][Uw][ +][ w][⊤][(][ν][2][U][ ⊤][U][ ⋆][U][ ⋆][⊤][U][ +][ U][ ⊤][Σ][U] [)][w]

= (w − _A[−][1]b)[⊤]A(w −_ _A[−][1]b) −_ _b[⊤]A[−][1]b + ∥w[⋆]∥[2]_ + σϵ[2][,]

where A := ν[2]U _[⊤]U_ _[⋆]U_ _[⋆][⊤]U + U_ _[⊤]ΣU and b := νU_ _[⊤]U_ _[⋆]w[⋆]. From this, we obtain_

_winfR[r][ R][r][(][δ][U,w][) =][ w][⋆][⊤][ ]I −_ _U_ _[⋆][⊤]U_ (U _[⊤]U_ _[⋆]U_ _[⋆][⊤]U + (1/ν[2])U_ _[⊤]ΣU_ )[−][1]U _[⊤]U_ _[⋆][]w[⋆]_ + σϵ[2][.]
_∈_

**Lemma B.12. For any U ∈** Od,r,

E [ inf (1 + ρ[−][2])E [ sin Θ(U, U _[⋆])_ 2] _w[⋆]_ _._
_D_ _w_ R[r][ R][r][(][δ][U,w][)][ −] _w[inf]R[r][ R][r][(][δ][U][ ⋆][,w][)] =][ O]_ _D_ _∥_ _∥_ _∥_ _∥[2][]_
_∈_ _∈_
 

_Proof of Lemma B.12. From proposition B.1, we have_


inf
_w_ R[r][ R][r][(][δ][U,w][)][ −] _w[inf]R[r][ R][r][(][δ][U][ ⋆][,w][)]_
_∈_ _∈_

= w[⋆][⊤][ ](I + (1/ν[2])U _[⋆][⊤]ΣU_ _[⋆])[−][1]_ _−_ _U_ _[⋆][⊤]U_ (U _[⊤]U_ _[⋆]U_ _[⋆][⊤]U + (1/ν[2])U_ _[⊤]ΣU_ )[−][1]U _[⊤]U_ _[⋆][]w[⋆]._


-----

Note that inf _w∈Rr Rr(δU,w) −_ inf _w∈Rr Rr(δU ⋆,w) ≡_ inf _w∈Rr Rr(δUO,w) −_ inf _w∈Rr Rr(δU ⋆,w)_
for any orthogonal matrix O ∈ Or,r. Take _O[˜] ∈_ Or,r such that ∥U _O[˜] −_ _U_ _[⋆]∥2 ≤_ _√2∥_ sin Θ(U, O)∥2

without loss of generality, since we can always take a sequence ( O[˜]m)m 1 such that _UOm_ _U_ _[⋆]_ 2
_≥_ _∥_ _−_ _∥_ _≤_
_√2∥_ sin Θ(U, O)∥2 + 1/m from Lemma A.1.

Lemma B.7 gives

inf
_w_ R[r][ R][r][(][δ][U,w][)][ −] _w[inf]R[r][ R][r][(][δ][U][ ⋆][,w][)]_
_∈_ _∈_

1 1 + ρ[−][2]
= O _._

1 sin Θ(U, U _[⋆])_ 2 [+][ κ][−][1][ρ][−][2] 1 + κ[−][1]ρ[−][2][ ∥] [sin Θ(][U, U][ ⋆][)][∥][2][∥][w][⋆][∥][2]

 _−∥_ _∥[2]_ 

On the event where ∥ sin Θ(U, U _[⋆])∥2[2]_ _[<][ 1][/][2][,]_

1 + ρ[−][2]
inf _._
_w_ R[r][ R][r][(][δ][U,w][)][ −] _w[inf]R[r][ R][r][(][δ][U][ ⋆][,w][) =][ O]_ (1 + κ[−][1]ρ[−][2])[2][ ∥] [sin Θ(][U, U][ ⋆][)][∥][2][∥][w][⋆][∥][2]
_∈_ _∈_  

On the event where sin Θ(U, U _[⋆])_ 2
_∥_ _∥[2]_ _[≥]_ [1][/][2][, we utlize the trivial upper bound]

_ν[2]_
inf _w[⋆]_ _._
_w∈R[r][ R][r][(][δ][U,w][)][ −]_ _w[inf]∈R[r][ R][r][(][δ][U][ ⋆][,w][)][ ≤∥][(][I][ +][ ν][−][2][U][ ⋆][⊤][Σ][U][ ⋆][)][−][1][∥][2][∥][w][⋆][∥][2][ ≤]_ _ν[2]_ + σ([2]d) _∥_ _∥[2]_


Combining these results, we have

E [ inf
_D_ _w_ R[r][ R][r][(][δ][U,w][)][ −] _w[inf]R[r][ R][r][(][δ][U][ ⋆][,w][)]]_
_∈_ _∈_

1 + ρ[−][2]
≲

(1 + κ[−][1]ρ[−][2])[2][ E][D][[][∥] [sin Θ(][U, U][ ⋆][)][∥][2][]][∥][w][⋆][∥][2]


1
+

1 + κ[−][1]ρ[−][2][ ∥][w][⋆][∥][2][P][D][(][∥] [sin Θ(][U, U][ ⋆][)][∥][2][ ≥] [1][/]

1 + ρ[−][2]

(1 + κ[−][1]ρ[−][2])[2][ E][D][[][∥] [sin Θ(][U, U][ ⋆][)][∥][2][]][∥][w][⋆][∥][2][,]


2)


where the last inequality follows from Markov’s inequality.

C OMITTED PROOFS FOR SECTION 4

C.1 PROOFS FOR SECTION 4.1

In this section, we will provide the proof of a generalized version of Theorem 4.1 to cover the
imbalanced setting, the statement and the detailed proof can be found in Theorem C.2. In the main
body, we assume the unlabeled data and labeled data are both balanced for the sake of clarity and
simplicity. Now we allow them to be imbalanced and provide a more general analysis. Suppose
we have n unlabeled data X = [x1, · · ·, xn] ∈ R[d][×][n] and nk labeled data Xk = [x[1]k[,][ · · ·][, x][n]k _[k]_ []][ ∈]
R[d][×][n][k] for class k, the contrastive learning task can be formulated as:

min min (C.1)
_W_ R[r][×][d][ L][(][W] [) :=] _W_ R[r][×][d][ L][SelfCon][(][W] [) +][ L][SupCon][(][W] [)][.]
_∈_ _∈_

In addition, we write a generalized version of supervised contrastive loss function to cover the imbalanced cases:

1 _r+1_ _αk_ _nk_ _Wx[k]i_ _[, Wx]j[k]_ _nj=1_ _s=k[⟨][Wx]i[k][, Wx]j[s][⟩]_
SupCon(W ) = [ _⟨_ _[⟩]_ _̸_ ]+ _[λ]_ _F_ _[,]_
_L_ _−_ _r + 1_ _kX=1_ _nk_ Xi=1 Xj≠ _i_ _nk −_ 1 _−_ P P _s≠_ _k_ _[n][s]_ 2 _[∥][WW][ ⊤][∥][2]_

(C.2)

P

where αk > 0 is the weight for supervised loss of class k. Again we first provide a theorem to give
the optimal solution of contrastive learning problem.
**Theorem C.1. The optimal solution of supervised contrastive learning problem (C.1) is given by :**

_r_ _⊤_

_WCL = C_ _uiσivi[⊤]_ _,_

_i=1_ !

X


-----

_where C > 0 is a positive constant, σi is the i-th largest eigenvalue of the following matrix:_


∆(XX _[⊤]) −_


1

_n_ 1 _[X][(1][n][1]n[⊤]_ _[−]_ _[I][n][)][X]_ _[⊤]_
_−_


4n


_r+1_

_k=1_

X


_αk_
_nk_


1 _αk_ 1 1
+ _nk_ _k_ [)][X]k[⊤] _Xk1k1[⊤]s_ _[X]s[⊤]_ _,_

_r + 1_ _kX=1_ _nk_ " _nk −_ 1 _[X][k][(1][n][k]_ [1][⊤] _[−]_ _[I][n]_ _[−]_ _t≠_ _k_ _[n][t]_ #

P

_ui is the corresponding eigenvector and V = [v1, · · ·, vn] ∈_ R[r][×][r] _can be any orthonormal matrix._

_Proof. Under this setting, combine with the result obtained in Corollary B.1, the contrastive loss_
can be rewritten as:

1 1

_L(W_ ) = _[λ]2_ _F_ _[−]_ 2[1]n [tr] 2 [∆(][XX] _[⊤][)][ −]_ 2(n 1) _[X][(1][n][1]n[⊤]_ _[−]_ _[I][n][)][X]_ _[⊤]_ _W_ _[⊤]W_

_[∥][WW][ ⊤][∥][2]_  _−_  

1 _r+1_ 1 _nk_ 1 1 _ns_

_αk_ _Wx[k]i_ _[, Wx]j[k]_ _Wx[k]i_ _[, Wx]j[s][⟩]_ _._

_−_ _r + 1_ _kX=1_ _nk_ Xi=1  _nk −_ 1 Xj≠ _i⟨_ _[⟩−]_ _t≠_ _k_ _[n][t]_ Xs≠ _k_ Xj=1⟨ 

 P 

Then we deal with the last term independently, note that:

_nk_ 1 1 _ns_

_Wx[k]i_ _[, Wx]j[k]_ _Wx[k]i_ _[, Wx]j[s][⟩]_

Xi=1  _nk −_ 1 Xj≠ _i⟨_ _[⟩−]_ _t≠_ _k_ _[n][t]_ Xs≠ _k_ Xj=1⟨ 

 _nk_ P _nk_ _ns_ 


_r + 1_


Xj≠ _i⟨Wx[k]i_ _[, Wx]j[k][⟩−]_


_⟨Wx[k]i_ _[, Wx]j[s][⟩]_
_j=1_

X


_nk_ 1
_−_


_t≠_ _k_ _[n][t]_


_i=1_


_i=1_


_s≠_ _k_


1 1
= _Xk(1nk_ 1[⊤]nk _k_ [)][X]k[⊤][W][ ⊤][W]

_nk_ 1 [tr] _[−]_ _[I][n]_ _−_ _t=k_ _[n][t]_
_−_ _̸_
  
P

Thus we have:


tr _Xk1k1[⊤]s_ _[X]s[⊤][W][ ⊤][W]_

Xs≠ _k_  


1

_L(W_ ) = _[λ]2_ _F_ _[−]_ 4[1]n [tr] (∆(XX _[⊤]) −_ _n_ 1 _[X][(1][n][1]n[⊤]_ _[−]_ _[I][n][)][X]_ _[⊤][)][W][ ⊤][W]_

_[∥][WW][ ⊤][∥][2]_  _−_

_r+1_

1 _αk_ 1

[ _Xk(1nk_ 1[⊤]nk _k_ [)][X]k[⊤][W][ ⊤][W]

_−_ _r + 1_ _nk_ _nk_ 1 [tr]

_k=1_ _[−]_ _[I][n]_

X _−_   


tr _Xk1k1[⊤]s_ _[X]s[⊤][W][ ⊤][W]_ ].

Xs≠ _k_   


_t≠_ _k_ _[n][t]_


Then by similar argument as in the proof of Theorem B.2, we can conclude that the optimal solution
_WCL must satisfy the desired conditions._

With optimal solution obtained in Theorem C.1, we can provide a generalized version of Theorem
4.1 to cover the imbalance cases.

**Theorem C.2 (Generalized version of Theorem 4.1). If Assumption 3.1-3.3 hold, n > d ≫** _r and_
_let WCL be any solution that minimizes the supervised contrastive learning problem in Eq.(C.1),_
_and denote its singular value decomposition as WCL = (UCLΣCLVCL[⊤]_ [)][⊤][, then we have]

_ν[2]_ _r3/2_ _dr_
E sin Θ(UCL, U ) _F ≲_ log d +
_∥_ _∥_ _λr(T_ ) _d_ _n_

 r

_r+1_

1 _√nsd_ _d_ _dr_
+ _αk_ ( + _r) +_ _,_

_r + 1_ _kX=1_ Xs≠ _k_ _t≠_ _k_ _[n][t]_ r _nk_ _[√]_ r _nk_ 

_where T ≜_ [1]4 _rk+1=1_ _[p][i][µ][k][µ]k[⊤]_ [+] _r+11_ _rk+1=1_ _[α][k][(][µ][k][µ]k[⊤]_ _[−]P[P]s≠_ _k_ _tn≠_ _sk_ _[n][t]_ 12 [(][µ][k][µ]s[⊤] [+][ µ][s][µ]k[⊤][))][.]

P

P P


-----

_Proof. For labeled data X = [x1,_ _, xn], we write it to be X = M +_ _E, where M = [µ1,_ _, µn]_
_· · ·_ _· · ·_
and E = [ξ1, _, ξn] are two matrices consisting of class mean and random noise. To be more_
_· · ·_
randomly drawn from each class,specific, if xi subject to the k-th cluster, then µi follows the multinomial distribution over µi = µ[k] and ξi ∼N (0, Σ[k]). Since the data is µ[1], · · ·, µ[r] with
probability p1, · · ·, pr+1. Thus µi follows a subgaussian distribution with covariance matrix
_N =_ _k=1_ _[p][k][µ][k][µ]k[⊤][.]_

As shown in Theorem C.1, the optimal solution of contrastive learning is equivalent to PCA of the[P][r][+1]
following matrix:


_Tˆ ≜_ [1]

4n [(∆(][XX] _[⊤][)][ −]_


1

_n_ 1 _[X][(1][n][1]n[⊤]_ _[−]_ _[I][n][)][X]_ _[⊤][)]_
_−_

1

[ _nk_ 1 _[X][k][(1][n][k]_ [1]n[⊤]k _k_ [)][X]k[⊤]

_−_ _[−]_ _[I][n]_

1
_s_ _[X]s[⊤]_ [+][ X][s][1][s][1]k[⊤][X]k[⊤][)]][.]
2 [(][X][k][1][k][1][⊤]


_r+1_

_k=1_

X


_αk_
_nk_


_r + 1_


_t≠_ _k_ _[n][t]_


_t≠_ _k_ _[t]_ _s≠_ _k_

Again we will deal with these terms separately,P


1. For the first term, as we have discussed, X can be divided into two matrices M and E, each
of them consists of subgaussian columns. Again we can obtain the result as in (B.24) (the
proof is totally same):

1 _r_ _d_

E _n_ [≲] _[ν][2][(]_ _[r]_ (1)
_∥_ _n[1]_ [(∆(][XX] _[⊤][)][ −]_ _n −_ 1 _[X][(1][n][1][⊤]_ _[−]_ _[I][n][)][X]_ _[⊤][)][ −]_ _[N]_ _[∥][2]_ _d_ [log][ d][ +] r _n_ [) +][ σ][2] r _n_ _[.]_

(C.3)

2. For the second term, notice that:

_nk_


_Xk(1nk_ 1[⊤]nk _[−]_ _[I][n]k_ [)][X]k[⊤] [=]


(µk + ξi[k][)(][µ][k] [+][ ξ]j[k][)][⊤]

Xi=1 Xj≠ _i_

_nk_ _nk_


_nk_


=nk(nk 1)µkµ[⊤]k [+ (][n][k] _ξi[k][)][⊤]_ [+ (][n][k] _ξi[k][)][µ]k[⊤]_ [+] _ξi[k][ξ]j[kT]_ _[,]_
_−_ _[−]_ [1)][µ][k][(]Xi=1 _[−]_ [1)(]Xi=1 Xi=1 Xj≠ _i_

(C.4)
and that:


_nk_

(µk + ξi[k][)]
_i=1_

X

_nk_


_ns_

(µs + ξj[s][)][⊤]
_j=1_

X


_Xk1k1[⊤]s_ _[X]s[⊤]_ [=]

Xs≠ _k_


_t≠_ _k_ _[n][t]_

1

_t≠_ _k_ _[n][t]_


_t≠_ _k_ _[n][t]_

_ns_


_s≠_ _k_


(C.5)


1 _ns_ _nk_ _nk_ _ns_
= [nknsµkµ[⊤]s [+][ n][k][µ][k][(] _ξj[s][)][⊤]_ [+][ n][s] _ξi[k][µ]s[⊤]_ [+] _ξi[k]_ _ξj[sT]_ []][.]

_t≠_ _k_ _[n][t]_ Xs≠ _k_ Xj=1 Xi=1 Xi=1 Xj=1

SinceP ξi[k]

_[∼N]_ [(0][,][ Σ][k][)][, we can conclude that:]

_nk_ _nk_ _d_

E _ξi[k]_ E _ξi[k]_ 2 [=] _σ(1)._ (C.6)
_∥_ _n[1]k_ _i=1_ _[∥][2]_ _[≤]_ vu _∥_ _n[1]k_ _i=1_ _[∥][2]_ r _nk_

X u X

Moreover, we have t

1 _nk_ 1 _nk_

_ξi[k][ξ]j[kT]_ _k_ [+] _ξ[k][ ¯]ξ[k][⊤]_ 2

_nk(nk −_ 1) [E][∥] Xi=1 Xj≠ _i_ _[∥][2]_ _[≤]_ _nk(nk −_ 1) [E][∥][E][k][E][⊤][∥][2] _nk −_ 1 [E][∥] [¯] _∥_ (C.7)

≲ _[d]_ _σ(1)[2]_ _[.]_

_nk_


Take equation (C.6) and (C.7) back into (C.4) we can conclude:


_d_ _σ(1)√rν + d_ _σ(1)[2]_ _[.]_ (C.8)

_nk_ _nk_


1

_nk_ _k_ [)][X]k[⊤] _k_ [≲]
_nk(nk −_ 1) _[X][k][(1][n][k]_ [1][⊤] _[−]_ _[I][n]_ _[−]_ _[µ][k][µ][⊤][∥][2]_


E∥


-----

On the other hand, by equation (C.6) we know:


1 _ns_ _ns_ _ns_ _ns_ _d_
E _ξj[s][∥][2]_ E _ξi[s]_ [≲] _σ(1)._
_∥_ _t≠_ _k_ _[n][t]_ Xs≠ _k_ Xj=1 _[≤]_ Xs≠ _k_ _t≠_ _k_ _[n][t]_ _∥_ _n[1]s_ Xi=1 _[∥][2]_ Xs≠ _k_ _t≠_ _k_ _[n][t]_ r _ns_

(C.9)

P P P

Notice that:


_nk_

_ξi[k]_
_i=1_

X


_ns_


1 1 _k_ _ns_ _ns_ ¯

_t≠_ _k_ _[n][t]_ _nk_ Xs≠ _k_ Xi=1 _ξi[k]_ Xj=1 _ξj[sT]_ _[∥][2]_ _[≤]_ [E][∥] Xs≠ _k_ _t≠_ _k_ _[n][t]_ _ξ[k][ ¯]ξ[s][⊤]∥2_ (C.10)

_ns_ _ns_ _d_ P

E _ξ[k][ ¯]ξ[s][⊤]_ 2 ≲ _σ(1)[2]_ _[.]_
_t≠_ _k_ _[n][t]_ _∥_ [¯] _∥_ Xs≠ _k_ _t≠_ _k_ _[n][t]_ _√nkns_

P P


E∥

Xs≠ _k_


Thus take equations (C.9) and (C.10) back into equation (C.5) we have:


E
_∥_ _n[1]k_

≲
Xs≠ _k_


1 _ns_

_Xk1k1[⊤]s_ _[X]s[⊤]_ _µkµ[⊤]s_ (C.11)

_t≠_ _k_ _[n][t]_ Xs≠ _k_ _[−]_ Xs≠ _k_ _t≠_ _k_ _[n][t]_ _[∥][2]_

_√nsd_ _d_ P

_t≠_ _k_ _[n][t]_ (r _nk_ _σ(1)[2]_ [+][ σ][(1)]√rν). (C.12)


Then combine equations (C.3)(C.8)(C.11) together, we can obtain the following result:


_r+1_

_αk(µkµ[⊤]k_
_kX=1_ _[−]_ Xs≠ _k_

_d_

+ σ(1)[2]

r


E _T_
_∥_ [ˆ] − [1]4 _[N][ −]_


_ns_

_t≠_ _k_ _[n][t]_


1
_s_ [+][ µ][s][µ]k[⊤][))][∥][2]
2 [(][µ][k][µ][⊤]


_r + 1_


_r_ _r_
≲ν[2]

_d_ [log][ d][ +] _n_

 r


_√nsd_
_t≠_ _k_ _[n][t]_


_r+1_

1 _√nsd_ _d_ _d_
+ _αk_ _σ(1)[2]_ [+][ √][rσ][(1)][ν] + _σ(1)√rν + d_ _σ(1)[2]_ _._

_r + 1_ _kX=1_ s≠ _k_ _t≠_ _k_ _[n][t]_ r _nk_ ! r _nk_ _nk_ 

[X] P 

Since we have assumed that rank([P][r]k[+1]=1 _[p][k][µ][k][µ]k[⊤][) =][ r][ we can find that the top-r eigenspace of]_
matrix:

_r+1_ _r+1_

1 _ns_ 1

_T = [1]_ _piµkµ[⊤]k_ [+] _αk_ _µkµ[⊤]k_ _s_ [+][ µ][s][µ]k[⊤][)]

4 _kX=1_ _r + 1_ _kX=1_  _[−]_ Xs≠ _k_ _t≠_ _k_ _[n][t]_ 2 [(][µ][k][µ][⊤] 

 P 

is spanned by U _[⋆], then apply Lemma D.1 again we have:_


_r+1_

_αk_

_k=1_

X


_N_ _N_ 2
E sin Θ(USCL, U ) _F_ _−_ _∥_
_∥_ _∥_ _≤_ [2][√][r][E]λ[∥]r([ˆ]N )

_√r_ _r_ _r_ _d_

_λr(T_ ) "ν[2] _d_ [log][ d][ +] r _n_  + σ(1)[2] r _n_

_r+1_

1 _√nsd_ _d_
+ _αk_ _σ(1)[2]_ [+][ √][rσ][(1)][ν] +

_r + 1_ _kX=1_ "Xs≠ _k_ _t≠_ _k_ _[n][t]_ r _nk_ !

P _r+1_

_ν[2]_ _dr_ 1 _√nsd_

log d + _αk_

_λr(T_ )  _d_ r _n_ [+] _r + 1_ _kX=1_ s≠ _k_ _t≠_ _k_ _[n][t]_

 _[r][3][/][2]_ [X] P


##

_._






_√rσ(1)ν + dnk_ _σ(1)[2]_


_nk_


_d_

+ _r_
_nk_ _[√]_


_dr_
_nk_


_ν[2]_
Roughly speaking, since _µk_ = _rν and_ _k=1_ _[p][k][µ][k][ = 0][, approximately we have]_ _λr(N_ )

1 _∥_ _∥_ _[√]_ _[≈]_

mink∈[r][1+αk] [. Although we can not obtain the closed-form eigenvalue in general, in a special case,]

[P][r][+1]


-----

where α = α1 = · · · = αr+1, m = n1 = n2 = · · · = nr+1 and _r+11_ [=][ p][1][ =][ p][2][ =][ · · ·][ =][ p][r][+1][, it’s]

easy to find that:


1
_s_ [+][ µ][s][µ]k[⊤][) =][ −][µ][k][µ]k[⊤][,]
2 [(][µ][k][µ][⊤]

_r+1_

_α(1 + [1]_ _k_ _[,]_ _λr(T_ ) = [ [1]

_r_ [)][µ][k][µ][⊤] 4 [+][ α][(1 + 1]r [)]][λ][(][N] [)][.]

_k=1_

X


_s≠_ _k_

1

_r + 1_


which further implies that:


_r+1_

_pkµkµ[⊤]k_ [+]
_k=1_

X


_T = [1]_


and we can obtain the result in Theorem 4.1.

C.2 PROOFS FOR SECTION 4.2

In this section, we will provide the proof of generalized version of Theorem 4.2 and 4.3 to cover the
imbalanced setting, the statement and detailed proof can be found in Theorem C.4 and C.5. First
we prove a useful lemma to illustrate that supervised loss function only yields estimation along a
1-dimensional space. Consider a single source task, where the data x = U _[⋆]z + ξ is generated by_
spiked covariance model and the label is generated by

_y = ⟨w[⋆], z⟩_

suppose we have collect n labeled data from this task, denote the data as X = [x1, x2, _, xn]_
_· · ·_ _∈_
R[d][×][n] and the label y = [y1, y2, · · ·, yn] ∈ R[n], then we have the following result.
**Lemma C.1. Under the conditions similar to Theorem 3.2, we can find an event A such that**
P(A[C]) = O( _d/n) and:_
p


1

I _A_
(n 1)[2][ XHyy][⊤][HX] _[⊤]_ _[−]_ _[ν][2][U][ ⋆][w][⋆][w][⋆][⊤][U][ ⋆][⊤]_ _F_ _{_ _}_
_−_


_d_
(C.13)
_n_ _[σ][(1)][ν.]_


The proof strategy is to estimate the difference between the two rank-1 matrices via bounding the
difference of the corresponding the vector component. We first provide a simple lemma to illustrate
the technique:
**Lemma C.2. Suppose α, β ∈** R[d] _are two vectors, then we have:_

_∥αα[⊤]_ _−_ _ββ[⊤]∥F ≤_ _√2(∥α∥2 + ∥β∥2)∥α −_ _β∥2._

_Proof. Denote α = (α1,_ _, αd), β = (β1,_ _, βd), then we have:_
_· · ·_ _· · ·_


_αiαj_ _βiβj_ 2

_i=1_ _j=1_ _|_ _−_ _|[2]_ _≤_

X X


_αiαj_ _αiβj_ + _αiβj_ _βiβj_
_j=1_ _|_ _−_ _|[2]_ _|_ _−_ _|[2]_

X


_αα[⊤]_ _ββ[⊤]_ _F_
_∥_ _−_ _∥[2]_ _[≤]_


_i=1_


_j=1_ _|αi|[2]|αj −_ _βj|[2]_ + |βj|[2]|αi − _βi|[2]_ _≤_ 2(∥α∥2[2] [+][ ∥][β][∥]2[2][)][∥][α][ −] _[β][∥][2]2_

X


_≤2_


_i=1_


_≤2(∥α∥2 + ∥β∥2)[2]∥α −_ _β∥2[2][.]_

Take square root on both side we can finish the proof.

Now we can prove the Lemma C.1.

_Proof of Lemma C.1. Clearly, we have:_


1
_∥_ (n 1)[2][ XHyy][⊤][HX] _[⊤]_ _[−]_ _[ν][2][U][ ⋆][w][⋆][w][⋆][⊤][U][ ⋆][⊤][∥][F]_

_−_

_n[2]_
_≤_ (n 1)[2][ ∥] _n[1][2][ XHyy][⊤][HX]_ _[⊤]_ _[−]_ _[ν][2][U][ ⋆][w][⋆][w][⋆][⊤][U][ ⋆][⊤][∥][F][ + 2](n[n][ + 1]1)[2][ ∥][ν][2][U][ ⋆][w][⋆][w][⋆][⊤][U][ ⋆][⊤][∥][F]_

_−_ _−_

≲
_∥_ _n[1][2][ XHyy][⊤][HX]_ _[⊤]_ _[−]_ _[ν][2][U][ ⋆][w][⋆][w][⋆][⊤][U][ ⋆][⊤][∥][F][ +][ r]n_ _[ν][2][,]_


-----

thus we can replace the (n 11)[2][ with][ 1]n [in equation (C.13) and conclude the proof. Denote][ ˆ]N ≜

_−_

_n1[2][ XHyy][⊤][HX]_ _[⊤][, note that both of][ ˆ]N and Uw[⋆]w[⋆][⊤]U_ _[⊤]_ are rank-1 matrices. We first bound the
difference between _n[1]_ _[XHy][ and][ Uw][⋆][:]_

_∥_ _n[1]_ _[XHy][ −]_ _[νU][ ⋆][w][⋆][∥]_ [=][∥] _nν[1]_ [(][U][ ⋆][Z][ +][ E][)][HZ] _[⊤][w][⋆]_ _[−]_ _[νU][ ⋆][w][⋆][∥]_

_≤∥_ _nν[1]_ [(][U][ ⋆][Z][ +][ E][)][HZ] _[⊤]_ _[−]_ _[νU][ ⋆][∥][2]_

_Z_ _[⊤]_ 2 + [1] _Z_ _[⊤]_ 2).

_≤_ _ν[1]_ [(][∥] _n[1]_ _[U][ ⋆][ZZ]_ _[⊤]_ _[−]_ _[ν][2][U][ ⋆][∥][2][ + 1]n_ _[∥][EZ]_ _[⊤][∥][2][ + 1]n_ _[∥][U][ ⋆][Z][ ¯]_ _∥_ _n_ _[∥][E][ ¯]_ _∥_

(C.14)
We deal with the four terms in (C.14) separately:


1. For the first term, apply Lemma D.3 we have:

_r_ _r_

E
_∥_ _n[1]_ _[U][ ⋆][ZZ]_ _[⊤]_ _[−]_ _[ν][2][U][ ⋆][∥][2][ ≤]_ [E][∥] _n[1]_ _[ZZ]_ _[⊤]_ _[−]_ _[ν][2][I][r][∥][2][ ≤]_ _n_ [+] _n_

 r


_ν[2]._ (C.15)


2. For the second term, apply Lemma D.2 twice we have:


1
_n_ [E][∥][EZ] _[⊤][∥][2][ = 1]n_ [E][Z][[][E][E][[][∥][EZ] _[⊤][∥][2][|][Z][]]]_

≲ [1]

_n_ [E][Z][[][∥][Z][∥][2][(][σ][sum][ +][ r][1][/][4][√][σ][sum][σ][(1)][ +][ √][rσ][(1)][)]]

≲ [1] _√dσ(1)_ (C.16)

_n_ [E][Z][[][∥][Z][∥][2][]]

≲ [1] _√dσ(1)(r[1][/][2]ν + (nr)[1][/][4]ν + n[1][/][2]ν)_

_n_

_√d_

≲ _√nσ(1)ν._

3. For the third term and fourth term, from equation (4) we know:

E [1] _Z_ _[⊤]_ 2 + E [1] _Z_ _[⊤]_ 2 E _z¯z¯[⊤]_ 2 + E _ξz¯[⊤]_ 2 _d_ (C.17)

_n_ _[∥][U][ ⋆][Z][ ¯]_ _∥_ _n_ _[∥][E][ ¯]_ _∥_ _≤_ _∥_ _∥_ _∥_ [¯] _∥_ _≤_ _n[r]_ _[ν][2][ +]_ r _n_ _[νσ][(1)][.]_

Combine these three equations (C.15)(C.16)(C.17) together we have:


E
_∥_ _n[1]_ _[XHy][ −]_ _[νU][ ⋆][w][⋆][∥]_ [≲]


_d_
(C.18)
_n_ _[σ][(1)][.]_


With equation (C.18), we can now turn to the difference between _N[ˆ] and Uw[⋆]w[⋆][⊤]U_ _[⊤]. By Lemma_
C.2 we know that:

_N_ _ν[2]U_ _[⋆]w[⋆]w[⋆][⊤]U_ _[⋆][⊤]_ _F ≲_ (
_∥_ [ˆ] − _∥_ _∥_ _n[1]_ _[XHy][∥]_ [+][ ∥][νU][ ⋆][w][⋆][∥][)][∥] _n[1]_ _[XHy][ −]_ _[νU][ ⋆][w][⋆][|∥][.]_


Using Markov’s inequality, we can conclude from (C.18) that:

_n_ _[XHy][ −]_ _[νU][ ⋆][w][⋆][∥]_

P(
_∥_ _n[1]_ _[XHy][ −]_ _[νU][ ⋆][w][⋆][∥≥]_ _[ν][)][ ≤]_ [E][∥] [1] _ν_


_n_ _[.]_


Then denote A = {ω : ∥ _n[1]_ _[XHy][ −]_ _[ν][2][U][ ⋆][w][⋆][∥][2][ < ν][}][ we have:]_

E _N_ _ν[2]U_ _[⋆]w[⋆]w[⋆][⊤]U_ _[⋆][⊤]_ _F I_ _A_ ≲E(
_∥_ [ˆ] − _∥_ _{_ _}_ _∥_ _n[1]_ _[XHy][∥]_ [+][ ∥][νU][ ⋆][w][⋆][∥][)][∥] _n[1]_ _[XHy][ −]_ _[νU][ ⋆][w][⋆][|∥][I][{][A][}]_


≲νE
_∥_ _n[1]_ _[XHy][ −]_ _[νU][ ⋆][w][⋆][∥][2][ ≲]_


_n_ _[σ][(1)][ν.]_


which finished the proof.


-----

In the main body, we assume the number of labeled data and the ratio of loss function is both
balanced. Now we will provide a more general result to cover the imbalance occasions. Formally, suppose we have n unlabeled data X = [x1, · · ·, xn] ∈ R[d][×][n] and ni labeled data Si
_Xi = [x[1]i_ _[,][ · · ·][, x]i[n][i]_ []][, y][i][ = [][y]i[1][,][ · · ·][, y]i[n][1] []][,][ ∀][i][ = 1][,][ · · ·][ T][ for source task, we learn the linear rep-]
resentation via joint optimization:

_T_

min min _αi HSIC( X[ˆ]_ _[t], y[t]; W_ ), (C.19)
_W ∈R[r][×][d][ L][(][W]_ [) :=] _W ∈R[r][×][d][ L][SelfCon][(][W]_ [)][ −] _t=1_

X

To investigate its feature recovery ability, we first give the following result.
**Theorem C.3. For optimization problem C.19, if we apply augmented pairs generation 2.1 with**
_random masking augmentation 2.2 for unlabeled data, then the optimal solution is given by:_

_r_ _⊤_

_WCL = C_ _uiσivi[⊤]_ _,_

_i=1_ !

X

_where C > 0 is a constant, σi is the i-largest eigenvalue of the following matrix:_

_T_

1 1 _αi_

∆(XX _[⊤])_ _n_ + _i_ _[H][n]i_ _[X]i[⊤][)][,]_

4n  _−_ _n −_ 1 _[X][(1][n][1][⊤]_ _[−]_ _[I][n][)][X]_ _[⊤]_ Xi=1 (ni − 1)[2][ X][i][H][n][i] _[y][i][y][⊤]_

_ui is the corresponding eigenvector, V = [v1, · · ·, vr] ∈_ R[r][×][r] _can be any orthogonal matrix and_
_Hni = Ini −_ _n[1]i_ [1][n][i] [1]n[⊤]i _[is the centering matrix.]_

_Proof. Under this setting, combine with the result obtained in B.1, the loss function can be rewritten_
as:

1 1

_L(W_ ) = _[λ]2_ _F_ _[−]_ 2[1]n [tr] 2 [∆(][XX] _[⊤][)][ −]_ 2(n 1) _[X][(1][n][1]n[⊤]_ _[−]_ _[I][n][)][X]_ _[⊤]_ _W_ _[⊤]W_

_[∥][WW][ ⊤][∥][2]_  _−_  


1

_Xi[⊤][W][ ⊤][WX][i][Hy][i][y]i[⊤][H]_
(ni 1)[2][ tr]
_−_
 


_αi_
_t=1_

X


= _[λ]_


1 1

= _[λ]2_ 4nλ ∆(XX _[⊤]) −_ _n_ 1 _[X][(1][n][1]n[⊤]_ _[−]_ _[I][n][)][X]_ _[⊤]_

 _−_ 

_T_ 2

_αi_

_[WW][ ⊤]_ _[−]_ _i_ _[H][n]i_ _[X]i[⊤][)]_

_−_ _i=1_ _λ(ni_ 1)[2][ X][i][H][n][i] _[y][i][y][⊤]_ _F_

X _−_

1 1

_−_ _[λ]2_ 4nλ ∆(XX _[⊤]) −_ _n_ 1 _[X][(1][n][1]n[⊤]_ _[−]_ _[I][n][)][X]_ _[⊤]_

 _−_ 

_T_ 2

_αi_

+ _i_ _[H][n]i_ _[X]i[⊤]_ _._

_i=1_ _λ(ni_ 1)[2][ X][i][H][n][i] _[y][i][y][⊤]_ _F_

X _−_

Then by similar argument as in the proof of Theorem B.2, we can conclude that the optimal solution
_WCL must satisfy the desired conditions._

Then we can give the proofs of Theorem 4.2 and Theorem 4.3 under our generalized setting, one can
easily obtain those under balanced setting by simply setting α = α1 = · · · = αT and m = n1 =

_· · · = nT, which is consistent with Theorem 4.2 and Theorem 4.3 in the mainbody._
**Theorem C.4 (Generalized version of Theorem 4.2). Suppose Assumption 3.1-3.3 hold for spiked**
_covariance model Eq.(2.5) and n > d_ _r, if we further assume that T < r and wt’s are orthogonal_
_≫_
_to each other, and let W_ _[CL]_ _be any solution that optimizes the problem in Eq.(C.19), and denote its_
_singular value decomposition as WCL = (UCLΣCLVCL[⊤]_ [)][⊤][, then we have:]


_√r −_ _T_

mini∈[T ]{αi, 1} [+]


_√T_

)( _[r]_
mini [T ] αi _d_ [log][ d][ +]
_∈_


E∥ sin(Θ(UCL, U _[⋆]))∥F ≲(_


_n_ [)]


_r_ _T [α][i][ + min][i][∈][[][T][ ]][{][α][i][,][ 1][}]_
_−_ mini∈[T ]{αi, 1}


_T [α][i][ + min][i][∈][[][T][ ]][ α][i]_

mini [T ] αi
_∈_


_ni_


_i=1_


-----

_Proof. As shown in Theorem C.3, optimizing loss function (C.19) is equivalent to find the top-r_
eigenspace of matrix


_αi_

_i_ _[H][n]i_ _[X]i[⊤][.]_
(ni 1)[2][ X][i][H][n][i] _[y][i][y][⊤]_
_−_


∆(XX _[⊤]) −_


1

_n_ 1 _[X][(1][n][1]n[⊤]_ _[−]_ _[I][n][)][X]_ _[⊤]_
_−_


4n


_i=1_


Again denote _M[ˆ]_ 2 ≜ _n[1]_ [(∆(][XX] _[⊤][)][ −]_ _n1_ 1 _[X][(1][n][1]n[⊤]_ _Ni ≜_ (ni 1 1)[2][ X][i][Hy][i][y]i[⊤][HX]i[⊤][.]

_−_ _[−]_ _[I][n][)][X]_ _[⊤][)][ and][ ˆ]_ _−_

By equation (B.24) we know that:


_r_ _r_
E _M2_ _M_ 2 ≲ _ν[2]_
_∥_ [ˆ] _−_ _∥_ _d_ [log][ d][ +] _n_ [+][ r]n
 r


_d_
_n_ [+][ d]n


+ σ(1)[2]


+ σ(1)ν


_n_ _[.]_


By Theorem C.1 we know that for each task Si, we can find an event Ai such that P(Ai) = O(


_d_
_n_ [)][:]


_d_

E _Ni_ _ν[2]U_ _[⋆]wiwi[⊤][U][ ⋆][⊤][∥][F]_ [I][{][A][i][}][ ≲] _σ(1)ν._
_∥_ [ˆ] _−_ _ni_

r

The target matrix is N = ν[2]U _[⋆]U_ _[⋆][⊤]_ + [P][T]i=1 _[α][i][ν][2][U][ ⋆][w][i][w]i[T]_ _[U][ ⋆][⊤][, and we can obtain the upper bound]_
for the difference between N and _N[ˆ]_ :


E _N_ _N_ 2I _i=1[A][i][} ≤]_ [1] _M2_ _M_ 2 +
_∥_ [ˆ] − _∥_ _{∩[T]_ 4 [E][∥] [ˆ] _−_ _∥_


_i=1_ _αiE∥N[ˆ]i −_ _ν[2]Uwiwi[⊤][U][ ⊤][∥][F]_ [I][{][A][i][}]

X


(C.20)


_r_

≲ν[2]( _[r]_ (1)

_d_ [log][ d][ +] _n_ [+][ r]n [) +][ σ][2]
r


_d_
_n_ [+][ d]n


_d_

_ni_ _σ(1)ν[2]_


+ σ(1)ν


_n_ [+]


_αi_


_i=1_


We divide the top-r eigenspace UCL of WCLWCL[⊤] [into two parts: the top-][T][ eigenspace][ U][ (1)]CL [and]
top-(T + 1) to top-r eigenspace UCL[(2)][. Similarly, we also divide the top-r eigenspace][ U][ ⋆] [of][ N][ into]
two parts: U _[⋆][(1)]_ and U _[⋆][(2)]. Then apply Lemma D.1 we have we can bound the sin Θ distance for_
each parts: on the one hand,

E∥ sin Θ(UCL[(1)][, U][ ⋆][(1)][)] _∥F_
 

=E∥ sin Θ(UCL[(1)][, U][ ⋆][(1)][)] _∥F I{∩i[T]=1[A][i][}][ +][ E][∥]_ [sin] Θ(UCL[(1)][, U][ ⋆][(1)][)] _∥F I{∪i[T]=1[A]i[C][}]_

_√T_ E∥N[ˆ] − _N_ _∥2I{∩i[T]=1[A][i][}]_ + _√T_ P( _i=1[A]i[C][)]_  

_≤_ _λ(T )(N_ ) _λ(T +1)(N_ ) _∪[T]_

_−_


_ν[2][ r]d_ [log][ d][ +][ σ](1)[2]

_r_ _d_

+

_d_ [log][ d][ +] _n_

r !


_αi_

_n_ [+]

_i=1_

X


_ni_ _σ(1)ν_


mini [T ] αiν[2]
_∈_

_√T_

≲

mini [T ] αi
_∈_

On the other hand,


_ni_


_i=1_


_αi + mini_ [T ] αi
_∈_

mini [T ] αi
_∈_


_ni_


_i=1_


E∥ sin Θ(UCL[(2)][, U][ ⋆][(2)][)] _∥F_
 

= E∥ sin Θ(UCL[(2)][, U][ ⋆][(2)][)] _∥F I{∩i[T]=1[A][i][}][ +][ E][∥]_ [sin] Θ(UCL[(2)][, U][ ⋆][(2)][)] _∥F I{∪i[T]=1[A][C]i_ _[}]_
   

_√r −_ _T_ E∥N[ˆ] − _N_ _∥2I{∩i[T]=1[A][i][}]_ _√r_ _T_ P( _i=1[A][C]i_ [)]

min _λ(T )(N_ ) _λ(T +1)(N_ ), λ(r)(N ) [+] _−_ _∪[T]_
_{_ _−_ _}_


_√r −_ _T_

mini∈[T ]{αi, 1}ν[2]


_ν[2][ r]d_ [log][ d][ +][ σ](1)[2]


_n_ [+]


_αi_
_i=1_

X


_ni_ _σ(1)ν_


_r −_ _T_


_ni_


_i_

_i=1_

_αi_ _d_

mini∈[T ]{αi, 1} [+ 1][r] _ni_


_√r −_ _T_

mini∈[T ]{αi, 1}


_d_ [log][ d][ +]


_r −_ _T_


_i=1_


-----

Note that:
sin(Θ(UCL, U _[⋆]))_ _F_
_∥_ _∥[2]_
= r −∥UCL[⊤] _[U][ ⋆][∥][2]F_

_≤_ _r −∥UCL[(1)][⊤][U][ ⋆][(1)][∥]F[2]_ _[−∥][U][ (2)]CL[T]_ _[U][ ⋆][(2)][∥]F[2]_

_≤_ _T −∥UCL[(1)][⊤][U][ ⋆][(1)][∥]F[2]_ [+ (][r][ −] _[T]_ [)][ −∥][U][ (2)]CL[⊤][U][ ⋆][(2)][∥]F[2]

_≤∥_ sin Θ(UCL[(1)][, U][ ⋆][(1)][)] _∥F[2]_ [+][ ∥] [sin] Θ(UCL[(1)][, U][ ⋆][(1)][)] _∥F[2]_ _[,]_

and the sin Θ distance has trivial upper bounds:   

_∥_ sin Θ(UCL[(1)][, U][ ⋆][(1)][)] _∥F[2]_ _[≤]_ _[T,]_ _∥_ sin Θ(UCL[(2)][, U][ ⋆][(2)][)] _∥F[2]_ _[≤]_ _[r][ −]_ _[T]_
   

Thus we can conclude:
E∥ sin(Θ(UCL, U _[⋆]))∥F_

_≤_ E∥ sin Θ(UCL[(1)][, U][ ⋆][(1)][)] _∥F + E∥_ sin Θ(UCL[(2)][, U][ ⋆][(2)][)] _∥F_
   


_r_ _T [α][i][ + min][i][∈][[][T][ ]][{][α][i][,][ 1][}]_
_−_ mini∈[T ]{αi, 1}


_√r −_ _T_

mini∈[T ]{αi, 1}

_√T_

mini [T ] αi
_∈_


_r −_ _T_


_d_ [log][ d][ +]

_r_
_d_ [log][ d][ +]

r


_ni_

_T._


_i=1_

_T_ _√_

_i=1_

X


_T [α][i][ + min][i][∈][[][T][ ]][ α][i]_

mini [T ] αi
_∈_


_ni_


**Theorem C.5 (Generalized version of Theorem 4.3). Suppose Assumptions 3.1-3.3 hold for spiked**
_covariance model Eq.(2.5) and n > d ≫_ _r, if we further assume that T ≥_ _r and_ _i=1_ _[α][i][w][i][w]i[⊤]_
_is full rank, suppose W_ _[CL]_ _is the optimal solution of optimization problem eq.(C.19), and denote its_
_singular value decomposition as WCL = (UCLΣCLVCL[⊤]_ [)][⊤][, then we have:] [P][T]

_√r_ _r_ _d_
E sin(Θ(UCL, U )) _F ≲_
_∥_ _∥_ 1 + ν[2]λ(r)([P][T]i=1 _[α][i][w][i][w]i[⊤][)]_ _d_ [log][ d][ +] r _n_ !

_T_

_αi_ _d_

+ _r_ + 1 _._

_[√]_ _i=1_ 1 + ν[2]λ(r)([P][T]i=1 _[α][i][w][i][w]i[⊤][)]_ !r _ni_

X

_Proof. The proof strategy is similar to that of Theorem 4.2, here the difference is that each direc-_
tion can be accurately estimated by the labeled data and we don’t need to separate the eigenspace.
Directly applying Lemma D.1 and equation (C.20) we have:

E∥ sin(Θ(UCL, U _[⋆]))∥F_
= E∥ sin(Θ(UCL, U _[⋆]))∥F I{∩i[T]=1[A][i][}][ +][ E][∥]_ [sin(Θ(][U][CL][, U][ ⋆][))][∥][F] [I][{∪]i[T]=1[A][C]i _[}]_

_√rE_ _Nˆ_ _N_ 2I _i=1[A][i][}]_
≲ _∥_ _−_ _∥_ _{∩[T]_ + _rP(_ _i=1[A][C]i_ [)]

_λ(r)(N_ ) _[√]_ _∪[T]_


_√r_

_ν[2]_ + ν[2]λ(r)([P][T]i=1 _[α][i][w][i][w]i[⊤][)]_ _ν[2][ r]d_ [log][ d][ +][ σ](1)[2] r

_√r_ _r_ _d_

+ _r_

1 + λ(r)([P][T]i=1 _[α][i][w][i][w]i[⊤][)]_ _d_ [log][ d][ +] r _n_ ! _[√]_


+ _r_

_[√]_


_n_ [+]

_T_

_i=1_

X


_αi_
_i=1_

X


_ni_ _σ(1)ν_


_i_ _ni_ (1) _ni_
=1 _i=1_

X X

_αi_

+ 1
1 + λ(r)([P][T]i=1 _[α][i][w][i][w]i[⊤][)]_ !r


_ni_


D USEFUL LEMMAS

In this section, we list some of the main techniques that have been used in the proof of the main
results.


-----

**Lemma D.1 (Theorem 2 in Yu et al. (2015)). Let Σ,** Σ[ˆ] _∈_ R[p][×][p] _be symmetric, with eigenval-_
_uesmin ( λλ1 ≥r_ 1 _. . .λ ≥r, λλs_ _p andλs+1λ[ˆ])1 > ≥ 0. . . where ≥_ _λλ[ˆ]p0 respectively. Fix :=_ _and λp+1 := 1 ≤_ _r ≤. Lets d ≤ := sp and assume thatr + 1, and let_
_−_ _−_ _−_ _∞_ _−∞_ _−_
_V = (vr, vr+1, . . ., vs) ∈_ R[p][×][d] _and_ _V[ˆ] = (ˆvr, ˆvr+1, . . ., ˆvs) ∈_ R[p][×][d] _have orthonormal columns_
_satisfying Σvj = λjvj and_ Σˆ[ˆ] _vj = λ[ˆ]jvˆj for j = r, r + 1, . . ., s. Then_

2 min _d[1][/][2]_ Σ Σ 2, Σ Σ F
_∥_ [ˆ] _−_ _∥_ _∥_ [ˆ] _−_ _∥_
sin Θ( V, V[ˆ] ) F _._
_∥_ _∥_ _≤_ min ( _λr_ 1 _λr, λs_ _λs+1)_ 

_−_ _−_ _−_

_Moreover, there exists an orthogonal matrix_ _O[ˆ] ∈_ R[d][×][d] _such that_


2[3][/][2] min _d[1][/][2]_ Σ Σ 2, Σ Σ F
_∥_ [ˆ] _−_ _∥_ _∥_ [ˆ] _−_ _∥_
_V_ _O[ˆ]_ _V_ F _._
_∥_ [ˆ] _−_ _∥_ _≤_ min (λr 1 _λr, λs_ _λs+1)_ 

_−_ _−_ _−_

**Lemma D.2 (Lemma 2 in Zhang et al. (2018)). Assume that E ∈** R[p][1][×][p][2] _has independent sub-_
_Gaussian entries, Var (Eij) = σij[2]_ _[, σ]C[2]_ [= max][j] _i_ _[σ]ij[2]_ _[, σ]R[2]_ [= max][i] _j_ _[σ]ij[2]_ _[, σ](1)[2]_ [= max][i,j][ σ]ij[2] _[.]_

_Assume that_

P P

_∥Eij/σij∥ψ2 := maxq_ 1 _[q][−][1][/][2][ {][E][ (][|][E][ij][|][ /σ][ij][)][q][}][1][/q][ ≤]_ _[κ.]_
_≥_

_Let V ∈_ Op2,r be a fixed orthogonal matrix. Then


)!


_x[4]_ _x[2]_

_,_
_κ[4]σ(1)[2]_ _[σ]C[2]_ _κ[2]σ(1)[2]_


P (∥EV ∥2 ≥ 2 (σC + x)) ≤ 2 exp


5r − min


E∥EV ∥2 ≲ _σC + κr[1][/][4][  ]σ(1)σC_ 1/2 + κr1/2σ(1).

**Lemma D.3 (Theorem 6 in Cai et al. (2020)). Suppose Z is a p1-by- p2 random matrix with inde-**
_pendent mean-zero sub-Gaussian entries. If there exist σ1, . . ., σp_ 0 such that _Zij/σi_ _ψ2_ _CK_
_for constant CK > 0, then_ _≥_ _∥_ _∥_ _≤_

E _ZZ_ _[⊤]_ _−_ EZZ _[⊤]_ 2 [≲] _i_ _σi[2]_ [+] sp2 _i_ _σi[2]_ _[·][ max]i_ _σi._
X X

**Lemma D.4 (The Eckart-Young-Mirsky Theorem (Eckart & Young, 1936)). Suppose that A =**
_U_ ΣV _[T]_ _is the singular value decomposition of A. Then the best rank- k approximation of the matrix_
_A w.r.t the Frobenius norm, ∥· ∥F, is given by_


_σiuivi[T]_ _[.]_
_i=1_

X


_Ak =_

_that is, for any matrix B of rank at most k_


_∥A −_ _Ak∥F ≤∥A −_ _B∥F ._

E NUMERICAL EXPERIMENTS

E.1 SUPPORTING EMPIRICAL RESULTS IN RELATED WORKS

In this section, we list some recent empirical evidence that provides sound support to our theory:

**Contrastive learning outperforms generative self-supervised learning.** In Figure 1 of Chen
et al. (2020a) and Figure 5 of Liu et al. (2021), it’s observed that contrastive learning has superior
performance compared to the generative approach. These results are consistent with our theory
in Theorem 3.1-3.4, where we show that contrastive learning achieves better performance on both
feature recovery and downstream tasks compared with autoencoder, a representative generative selfsupervised learning method.


-----

**Supervised contrastive learning improves downstream accuracy.** In Table 2 of Khosla et al.
(2020) and the first column in Table 4 of Islam et al. (2021), supervised contrastive learning shows
significant improvement with 7%-8% accuracy increase on ImageNet and Mini-ImageNet. This observation is consistent with our finding in Theorem 4.1, where we prove that supervised contrastive
learning can achieve a better upper bound in feature recovery compared with self-supervised contrastive learning.

**Label information may hurt transferability in contrastive learning.** In Table 4 of Khosla et al.
(2020) and Table 4 of Islam et al. (2021), where the supervised contrastive learning hardly increases
the predictive accuracy compared to the self-supervised contrastive learning (the difference of mean
accuracy is less than 1%) and can harm significantly on some datasets (e.g. 5.5% lower for SUN
397 in Table 3 of Khosla et al. (2020)). These results indicate that some mechanisms in supervised
contrastive learning hurt model transferability since the improvement on source tasks is significant.
Moreover, in Table 4 of Islam et al. (2021), it is observed that combining supervised learning and
self-supervised contrastive learning together achieves the best transfer learning performance compared with each of them individually. These findings are consistent with our theory in Theorem
4.2, where we show that the error can increase as α (the ratio between supervised learning loss and
self-supervised learning loss) grows and the optimal error is achieved when choosing a moderate α,
i.e., combining self-supervised learning and supervised learning together.

E.2 SIMULATION WITH SYNTHETIC DATA

To verify our theory, we conducted numerical experiments on the spiked covariance model (2.5)
under a linear representation setting. As we have explicitly formulated the loss function and derive
its equivalent form in the main body and appendix, we simply minimize the corresponding loss
by gradient descent to find the optimal linear representation W . For self-supervised contrastive
learning with random masking augmentation, we independently draw the augmentation function by
Definition 2.2 and apply them to all of the samples in each iteration. To ensure the convergence, we
set the maximum number of iteration for it (typically 10000 or 50000 depends on dimension d).

We report two criteria to evaluate the quality of the representation, downstream error and sine distance. To obtain the sine distance for a learned representation W, we perform singular value decomposition to get W = (U ΣV _[⊤])[⊤]_ and then compute ∥ sin Θ(U, U _[⋆])∥F . To obtain the downstream_
task performance, in the comparison between autoencoder and contrastive learning, we first draw
_n labeled data from spiked covariance model (2.5) with labels generated as in Section 3.2, then we_
train the model by using the data without labels to obtain the linear representation W, and learn
a linear predictor w using the data with labels and compute the regression error. In the transfer
learning setting, we draw some labeled data on the source tasks and additional unlabeled data. The
number of labeled data is set m = 1000 and the number of unlabeled data is set n = 1000. Then
train with them to obtain the linear representation W, and draw labeled data from a new source task
to learn a linear predictor w to compute the regression error. In particular, we subtract the optimal
regression error obtained by the best representation U _[⋆][⊤]_ for each regression error and report the
difference, or more precisely, the excess risk as downstream performance.

The results are reported in Fig. 1, 2 and Table 1, 2. As predicted by Theorem 3.1 and 3.2, the feature
recovery error and downstream task risk of contrastive learning decreases as d increases (Fig. 1:
**Left) and as n increases (Fig. 1: Center) while that of autoencoder is insensible to the changes in d**
and n. The performance of transfer learning exhibits a U-shape type curve when the number of tasks
is insufficient, which implies that the supervised training may hurt the transferability and we need
to choose an appropriate ratio α to obtain the best performance. When tasks are abundant enough,
the performance of transfer learning becomes better as we increase the weight of supervised loss.

|log (α) e|-5 -4 -3 -2 -1 0 1 2 3 4 5|
|---|---|
|T = 8, r = 10|0.0242 0.0231 0.0199 0.0141 0.0122 0.0125 0.0184 0.0345 0.0499 0.0535 0.0587|
|T = 20, r = 10|0.0223 0.0163 0.0156 0.0096 0.0079 0.0055 0.0064 0.0064 0.0067 0.0070 0.0079|



Table 1: Downstream performance in transfer learning against the penalty parameter α. T is the
number of source tasks.


-----

|log (α) e|-5 -4 -3 -2 -1 0 1 2 3 4 5|
|---|---|
|T = 8, r = 10|2.0373 2.0371 2.0228 1.9908 2.0021 2.0055 2.0010 2.0362 2.0699 2.0705 2.0813|
|T = 20, r = 10|2.0352 2.0292 2.0030 1.9871 1.9740 1.9690 1.9766 1.9702 1.9790 1.9714 1.9672|


Table 2: Feature recovery performance in transfer learning against the penalty parameter α. T is the
number of source tasks.

Figure 2: Left: Comparison of learned feature between contrastive learning and autoencoders
against the dimension d. The sample size n is set as n = 20000. Center: Comparison of feature recovery performance between contrastive learning and autoencoders against the dimension n.
The dimension d is set as d = 40. Right: Feature recovery performance in transfer learning against
penalty parameter α in log scale. T is the number of source tasks. We set the number of labeled data
and unlabeled data as m = 1000 and n = 1000 respectively.


-----

