# A GLOBAL CONVERGENCE THEORY FOR DEEP RELU IMPLICIT NETWORKS VIA OVER-PARAMETERIZATION


**Tianxiang Gao**
Department of Computer Science
Iowa State University
gaotx@iastate.edu

**Jia Liu**
Dept. of Electrical and Computer Engineering
The Ohio State University
liu@ece.osu.edu

**Hongyang Gao**
Department of Computer Science
Iowa State University
hygao@iastate.edu

ABSTRACT


**Hailiang Liu**
Department of Mathematics
Iowa State University
hliu@iastate.edu

**Hridesh Rajan**
Department of Computer Science
Iowa State University
hridesh@iastate.edu


Implicit deep learning has received increasing attention recently, since it generalizes the recursive prediction rules of many commonly used neural network architectures. Its prediction rule is provided implicitly based on the solution of an
equilibrium equation. Although many recent studies have experimentally demonstrates its superior performances, the theoretical understanding of implicit neural
networks is limited. In general, the equilibrium equation may not be well-posed
during the training. As a result, there is no guarantee that a vanilla (stochastic)
gradient descent (SGD) training nonlinear implicit neural networks can converge.
This paper fills the gap by analyzing the gradient flow of Rectified Linear Unit
(ReLU) activated implicit neural networks. For an m-width implicit neural network with ReLU activation and n training samples, we show that a randomly
initialized gradient descent converges to a global minimum at a linear rate for the
square loss function if the implicit neural network is over-parameterized. It is
worth noting that, unlike existing works on the convergence of (S)GD on finitelayer over-parameterized neural networks, our convergence results hold for implicit neural networks, where the number of layers is infinite.

1 INTRODUCTION

**1) Background and Motivation: In the last decade, implicit deep learning (El Ghaoui et al., 2019)**
have attracted more and more attention. Its popularity is mainly because it generalizes the recursive
rules of many widely used neural network architectures. A line of recent works (Bai et al., 2019;
El Ghaoui et al., 2019; Bai et al., 2020) have shown that the implicit neural network architecture
is a wider class that includes most current neural network architectures as special cases, such as
feed-forward neural networks, convolution neural networks, residual networks, and recurrent neural networks. Moreover, implicit deep learning is also well known for its competitive performance
compared to other regular deep neural networks but using significantly fewer computational resources (Dabre & Fujita, 2019; Dehghani et al., 2018; Bai et al., 2018).

Although a line of literature has been shown the superior performance of implicit neural networks
experimentally, the theoretical understanding is still limited. To date, it is still unknown if a simple
first-order optimization method such as (stochastic) gradient descent can converge on an implicit
neural network activated by a nonlinear function. Unlike a regular deep neural network, an implicit
neural network could have infinitely many layers, resulting in the possibility of divergence of the


-----

forward propagation (El Ghaoui et al., 2019; Kawaguchi, 2021). The main challenge in establishing
the convergence of implicit neural network training lies in the fact that, in general, the equilibrium
equation of implicit neural networks cannot be solved in closed-form. What exacerbates the problem
is the well-posedness of the forward propagation. In other words, the equilibrium equation may
have zero or multiple solutions. A line of recent studies have suggested a number of strategies to
handle this well-posedness challenge, but they all involved reformulation or solving subproblems
in each iteration. For example, El Ghaoui et al. (2019) suggested to reformulate the training as
the Fenchel divergence formulation and solve the reformulated optimization problem by projected
gradient descent method. However, this requires solving a projection subproblem in each iteration
and convergence was only demonstrated numerically. By using an extra softmax layer, Kawaguchi
(2021) established global convergence result of gradient descent for a linear implicit neural network.
Unfortunately, their result cannot be extended to nonlinear activations, which are critical to the
learnability of deep neural networks.

This paper proposes a global convergence theory of gradient descent for implicit neural networks
activated by nonlinear Rectified Linear Unit (ReLU) activation function by using overparameterization. Specifically, we show that random initialized gradient descent with fixed stepsize converges
to a global minimum of a ReLU implicit neural network at a linear rate as long as the implicit
neural network is overparameterized. Recently, over-parameterization has been shown to be effective in optimizing finite-depth neural networks (Zou et al., 2020; Nguyen & Mondelli, 2020; Arora
et al., 2019; Oymak & Soltanolkotabi, 2020). Although the objective function in the training is nonsmooth and non-convex, it can be shown that GD or SGD converge to a global minimum linearly
if the width m of each layer is a polynomial of the number of training sample n and the number of
layers h, i.e., m = poly(n, h). However, these results cannot be directly applied to implicit neural
networks, since implicit neural networks have infinitely many hidden layers, i.e., h →∞, and the
well-posedness problem surfaces during the training process. In fact, Chen et al. (2018); Bai et al.
(2019; 2020) have all observed that the time and number of iterations spent on forward propagation
are gradually increased with the the training epochs. Thus, we have to ensure the unique equilibrium
point always exists throughout the training given that the width m is only polynomial of n.

**2) Preliminaries of Implicit Deep Learning: In this work, we consider an implicit neural network**
with the transition at the ℓ-th layer in the following form (El Ghaoui et al., 2019; Bai et al., 2019):

_γ_
**_z[ℓ]_** = σ _√m_ **_Az[ℓ][−][1]_** + φ(x) _,_ (1)
 

where φ : R[d] _→_ R[m] is a feature mapping function that transforms an input vector x ∈ R[d] to a
desired feature vector φ ≜ _φ(x), z[ℓ]_ _∈_ R[m] is the output of the ℓ-th layer, A ∈ R[m][×][m] is a trainable
weight matrix, σ(u) = max{0, u} is the ReLU activation function, and γ ∈ (0, 1) is a fixed scalar to
scale A. As will be shown later in Section 2.1, γ plays the role of ensuring the existence of the limit
**_z[∗]_** = limℓ→∞ **_z[ℓ]. In general, the feature mapping function φ is a nonlinear function, which extracts_**
features from the low-dimensional input vector x. In this paper, we consider a simple nonlinear
feature mapping function φ given by

1
_φ(x) ≜_ _√mσ(W x),_ (2)


where W ∈ R[m][×][d] is a trainable parameter matrix. As ℓ _→∞, an implicit neural network can be_
considered an infinitely deep neural network. Consequently, z[∗] is not only the limit of the sequence
_{z[ℓ]}ℓ[∞]=0_ [with][ z][0][ =][ 0][, but it is also the][ equilibrium point][ (or][ fixed point][) of the equilibrium equation:]

**_z[∗]_** = σ(˜γAz[∗] + φ), (3)

where ˜γ ≜ _γ/[√]m. In implicit neural networks, the prediction ˆy for the input vector x is the_
combination of the fixed point z[∗] and the feature vector φ, i.e.,

_yˆ = u[T]_ **_z[∗]_** + v[T] **_φ,_** (4)

where u, v ∈ R[m] are trainable weight vectors. For simplicity, we use θ ≜ vec (A, W, u, v) to
group all training parameters. Given a training data set {(xi, yi)}i[n]=1[, we want to minimize]

_n_

1

_L(θ) =_ _yi_ _yi)[2]_ = [1] **_y_** **_y_** _,_ (5)

_i=1_ 2 [(ˆ] _−_ 2 _[∥]_ [ˆ] − _∥[2]_

X


-----

where ˆy and y are the vectors formed by stacking all the prediction and labels.

**3) Main Results: Our results are based on the following observations. We first analyze the for-**
ward propagation and find that the unique equilibrium point always exists if the scaled matrix ˜γA in
Eq. (3) has an operator norm less than one. Thus, the well-posedness problem is reduced to finding
a sequence of scalars {γk}k[∞]=1 [such that][ ˜]γkA(k) is appropriately scaled. To achieve this goal, we
show that the operator norm A(k) is uniformly upper bounded by a constant over all iterations.
Consequently, a fixed scalar γ is enough to ensure the well-posedness of Eq. (3). Our second observation is from the analysis of the gradient descent method with infinitesimal step-size (gradient
flow). By applying the chain rule with the gradient flow, we derive the dynamics of prediction ˆy(t)
which is governed by the spectral property of a Gram matrix. In particular, if the smallest eigenvalue of the Gram matrix is lower bounded throughout the training, the gradient descent method
enjoys a linear convergence rate. Along with some basic functional analysis results, it can be shown
that the smallest eigenvalue of the Gram matrix at initialization is lower bounded if no two data
samples are parallel. Although the Gram matrix varies in each iteration, the spectral property is
preserved if the Gram matrix is close to its initialization. Thus, the convergence problem is reduced
to showing the Gram matrix in latter iterations is close to its initialization. Our third observation is
that we find random initialization, over-parameterization, and linear convergence jointly enforce the
(operator) norms of parameters upper bounded by some constants and close to their initialization.
Accordingly, we can use this property to show that the operator norm of A is upper bounded and the
spectral property of the Gram matrix is preserved throughout the training. Combining all these insights together, we can conclude that the random initialized gradient descent method with a constant
step-size converges to a global minimum of the implicit neural network with ReLU activation.

The main contributions of this paper are summarized as follows:

(i) By scaling the weight matrix A with a fixed scalar γ, we show that the unique equilibrium
point z[∗] for each x always exists during the training if the parameters are randomly initialized,
even for the nonlinear ReLU activation function.

(ii) We analyze the gradient flow of implicit neural networks. Despite the non-smooth and nonconvexity of the objective function, the convergence to a global minimum at a linear rate is
guaranteed if the implicit neural network is over-parameterized and the data is non-degenerate.

(iii) Since gradient descent is discretized version of gradient flow, we can show gradient descent
with fixed stepsize converges to a global minimum of implicit neural networks at a linear rate
under the same assumptions made by the gradient flow analysis, as long as the stepsize is
chosen small enough.

**Notation: For a vector x, ∥x∥** is the Euclidean norm of x. For a matrix A, ∥A∥ is the operator
norm of A. If A is a square matrix, then λmin(A) and λmax(A) denote the smallest and largest
eigenvalue of A, respectively, and λmax(A) ≤∥A∥. We denote [n] ≜ _{1, 2, · · ·, n}._

2 WELL-POSEDNESS AND GRADIENT COMPUTATION

In this section, we provide a simple condition for the equilibrium equation (3) to be well-posed in the
sense that the unique equilibrium point exists. Instead of backpropagating through all the intermediate iterations of a forward pass, we derive the gradients of trainable parameters by using the implicit
function theorem. In this work, we make the following assumption on parameter initialization.
**Assumption 1 (Random Initialization). The entries Aij and Wij are randomly initialized by the**
standard Gaussian distribution N (0, 1), and ui and vi are randomly initialized by the symmetric
Bernoulli or Rademacher distribution.
**Remark 2.1. This initialization is similar to the approaches widely used in practice (Glorot &**
Bengio, 2010; He et al., 2015). The result obtained in this work can be easily extended to the case
where the distributions for Aij, Wij, ui, and vi are replaced by sub-Gaussian random variables.

2.1 FORWARD PROPAGATION AND WELL-POSEDNESS

In a general implicit neural network, Eq. (3) is not necessarily well-posed, since it may admit zero
or multiple solutions. In this work, we show that scaling the matrix A with ˜γ = γ/[√]m guarantees


-----

the existence and uniqueness of the equilibrium point z[∗] with random initialization. This follows
from a foundational result in random matrix theory as restated in the following lemma.
**Lemma 2.1 (Vershynin (2018), Theorem 4.4.5). Let A be an m × n random matrix whose entries**
**_Aij are independent, zero-mean, and sub-Gaussian random variables. Then, for any t > 0, we have_**
_∥A∥≤_ _CK([√]m +_ _[√]n + t) with probability at least 1 −_ 2e[−][t][2] . Here C > 0 is a fixed constant,
and K = maxi,j **_Aij_** _ψ2_ .
_∥_ _∥_

Under Assumption 1, Lemma 2.1 implies that, with exponentially high probability, ∥A∥≤ _c[√]m_
for some constant c > 0. By scaling A by a positive scalar ˜γ, we show that the transition Eq. (1) is a
contraction mapping. Thus, the unique equilibrium point exists with detailed proof in Appendix A.1.
**Lemma 2.2. If** **_A_** _c[√]m for some c > 0, then for any γ0_ (0, 1), the scalar γ ≜ min _γ0, γ0/c_
uniquely determines the existence of the equilibrium ∥ _∥≤_ **_z[∗]_** for every ∈ **_x, and_** **_z[ℓ]_** 1 1γ0 _{_ _}_
_∥_ _∥≤_ _−_

_[∥][φ][∥]_ [for all][ ℓ][.]

Lemma 2.2 indicates that equilibria always exist if we can maintain the operator norm of the scaled
matrix (γ/[√]m)A less than 1 during the training. However, the operator norms of matrix A are
changed by the update of the gradient descent. It is hard to use a fixed scalar γ to scale the matrix
**_A over all iterations, unless the operator norm of A is bounded. In Section 3, we will show ∥A∥≤_**
2c[√]m always holds throughout the training, provided that ∥A(0)∥≤ _c[√]m at initialization and the_
width m is sufficiently large. Thus, by using the scalar γ = min _γ0, γ0/(2c)_ for any γ0 (0, 1),
equilibria always exist and the equilibrium equation Eq. (3) is well-posed.{ _}_ _∈_

2.2 BACKWARD GRADIENT COMPUTING


For a regular network with finite layers, one needs to store all intermediate parameters and apply
backpropagation to compute the gradients. In contrast, for an implicit neural network, we can derive
the formula of the gradients by using the implicit function theorem. Here, the equilibrium point
**_z[∗]_** is a root of the function f given by f (z, A, W ) ≜ **_z −_** _σ (˜γAz + φ). The essential challenge_
is to ensure the partial derivative ∂f/∂z at z[∗] is always invertible throughout the training. The
following lemma shows that the partial derivative ∂f/∂z at z[∗] always exists with the scalar γ,
and the gradient derived in the lemma always exists. The gradient derivations for each trainable
parameters are provided in the following lemma and the proof is provided in Appendix A.2.
**Lemma 2.3 (Gradients of an Implicit Neural Network). Define the scalar γ ≜** min{γ0, γ0/c}. If
**_A_** _c[√]m for some constant c > 0, then for any γ0_ (0, 1), the following results hold:
_∥_ _∥≤_ _∈_

(i) The partial derivatives of f with respect to z, A, and W are
_∂f_
_γ diag(σ[′](˜γAz + φ))A][T]_ _,_ (6)
_∂z_ [= [][I][m][ −] [˜]

_∂f_ _T_

_γ_ **_z[T]_** **diag(σ[′](˜γAz + φ))** _,_ (7)
_∂A_ [=][ −][˜] _⊗_

_∂f_  _T_

_∂W_ [=][ −] _√[1]m_ **_x[T]_** _⊗_ **diag(σ[′](W x))** **diag(σ′(˜γAz + φ))[T]** _,_ (8)

 

where ˜γ ≜ _γ/[√]m._

(ii) For any vector v, the following inequality holds
_λmin {Im −_ _γ˜ diag(σ[′](v))A} > 1 −_ _γ0 > 0,_ (9)
which further implies ∂f/∂z is invertible at the equilibrium point z[∗].

(iii) The gradient of the objective function L with respect to A and W are given by


(ˆyi _yi)φi,_ (10)
_i=1_ _−_

X


**_uL =_**
_∇_

**_AL =_**
_∇_


(ˆyi _yi)zi,_ **_vL =_**
_i=1_ _−_ _∇_

X


_i=1_ _γ˜(ˆyi −_ _yi)Di[T]_ [[][I][m] _[−]_ _γ[˜]DiA][−][T]_ **_uzi[T]_** _[,]_ (11)

X


1
_√m_ (ˆyi − _yi)Ei[T]_ **_Di[T]_** [[][I][m] _[−]_ _γ[˜]DiA][−][T]_ **_u + v_** **_x[T]i_** _[,]_ (12)
n o


_∇W L =_


_i=1_


-----

where zi is the equilibrium point for the training data (xi, yi), Di ≜ **diag(σ[′](˜γAzi + φi)),**
and Ei ≜ **diag(σ[′](W xi)).**

3 GLOBAL CONVERGENCE OF THE GRADIENT DESCENT METHOD

In this section, we establish the global convergence results of the gradient descent method in implicit neural networks. In Section 3.1, we first study the dynamics of the prediction induced by the
gradient flow, that is, the gradient descent with infinitesimal step-size. We show that the dynamics
of the prediction is controlled by a Gram matrix whose smallest eigenvalue is strictly positive over
iterations with high probability. Based on the findings in gradient flow, we will show that the random initialized gradient descent method with a constant step-size converges to a global minimum at
a linear rate in Section 3.2.

3.1 CONTINUOUS TIME ANALYSIS

The gradient flow is equivalent to the gradient descent method with an infinitesimal step-size. Thus,
the analysis of gradient flow can serve as a stepping stone towards understanding discrete-time
gradient-based algorithms. Following previous works on gradient flow of different machine learning
models (Saxe et al., 2013; Du et al., 2019; Arora et al., 2018; Kawaguchi, 2021) the gradient flow of
implicit neural networks is given by the following ordinary differential equations:
_dvec (A)_ _dvec (W )_ _du_ _dv_

= = (13)
_dt_ _−_ _[∂L]∂A[(][t][)]_ _[,]_ _dt_ _−_ _[∂L]∂W[(][t][)]_ _[,]_ _dt_ [=][ −] _[∂L]∂u[(][t][)]_ _[,]_ _dt_ [=][ −] _[∂L]∂v[(][t][)]_ _[,]_

where L(t) represents the value of the objective function L(θ) at time t.

Our results relies on the analysis of the dynamics of prediction ˆy(t). In particular, Lemma 3.1 shows
that the dynamics of prediction ˆy(t) is governed by the spectral property of a Gram matrix H(t).

**Lemma 3.1 (Dynamics of Prediction ˆy(t)). Suppose ∥A(s)∥≤** _c[√]m for all 0 ≤_ _s ≤_ _t. Let_
**_X ∈_** R[n][×][d], Φ(s) ∈ R[n][×][m], and Z(s) ∈ R[n][×][m] be the matrices whose rows are the training data xi,
feature vectors φi, and equilibrium points zi at time s, respectively. With scalar γ ≜ min{γ0, γ0/c}
for any γ0 (0, 1), the dynamics of prediction ˆy(t) is given by
_∈_
_dyˆ_

(γ[2]M (t) + In) **_Z(t)Z(t)[T]_** + Q(t) **_XX_** _[T]_ + Φ(t)Φ(t)[T][ ] (ˆy **_y),_**
_dt_ [=][ −] _◦_ _◦_ _−_

≜ _−_ **_H_** (t)(ˆy − **_y),_** (14)

where ◦ is the Hadamard product (i.e., element-wise product), and matrices M (t) ∈ R[n][×][n] and
**_Q(t) ∈_** R[n][×][n] are defined as follows:

**_M_** (t)ij ≜ [1] _γDiA)[−][1]DiDj[T]_ [(][I][m] _γDjA)[−][T]_ **_u,_** (15)

_m_ **_[u][T][ (][I][m][ −]_** [˜] _[−]_ [˜]

**_Q(t)ij ≜_** [1] **_Di[T]_** [(][I][m] _γDiA)[−][1]u + v_ _T EiEjT_ **_Dj[T]_** [(][I][m] _γDjA)[−][T]_ **_u + v_** _._ (16)

_m_ _[−]_ [˜] _[−]_ [˜]
     

Note that the matrix H(t) is clearly positive semidefinite since it is the sum of three positive semidefinite matrices. If there exists a constant λ > 0 such that λmin **_H(t)_** _λ > 0 for all t, i.e., H(t)_
_{_ _} ≥_
is positive definite, then the dynamics of the loss function L(t) satisfies the following inequality
_L(t) ≤_ exp{−λt}L(0),
which immediately indicates that the objective value L(t) is consistently decreasing to zero at a
geometric rate. With random initialization, we will show that H(t) is positive definite as long as
the number of parameters m is sufficiently large and no two data points are parallel to each other. In
particular, by using the nonlinearity of the feature map φ, we will show that the smallest eigenvalue
of the Gram matrix G(t) ≜ **Φ(t)Φ(t)[T]** is strictly positive over all time t with high probability. As
a result, the smallest eigenvalue of H(t) is always strictly positive.

Clearly, G(t) is a time-varying matrix. We first analyze its spectral property at its initialization.
When t = 0, it follows from the definition of the feature vector φ in Eq. (2) that


**_G(0) = Φ(0)Φ(0)[T]_** = [1]

_m_ _[σ][(][XW][ (0)][T][ )][σ][(][XW][ (0)][T][ )][T][ = 1]m_


_σ(Xwr(0))σ(Xwr(0))[T]_ _._
_r=1_

X


-----

By Assumption 1, each vector wr(0) follows the standard multivariate normal distribution, i.e.,
**_wr(0) ∼_** _N_ (0, Id). By letting m →∞, we obtain the covariance matrix G[∞] _∈_ R[n][×][n] as follows:

**_G[∞]_** ≜ Ew _N_ (0,Id)[σ(Xw)σ(Xw)[T] ]. (17)
_∼_

Here G[∞] is a Gram matrix induced by the ReLU activation function and the random initialization.
The following lemma shows that the smallest eigenvalue of G[∞] is strictly positive as long as no
two data points are parallel. Moreover, later in Lemmas 3.3 and 3.4, we conclude that the spectral
property of G[∞] is preserved in the Gram matrices G(0) and G(t) during the training, as long as the
number of parameter m is sufficiently large.

**Lemma 3.2. Assume ∥xi∥** = 1 for all i ∈ [n]. If xi ̸∥ **_xj for all i ̸= j, then λ0 ≜_** _λmin{G[∞]} > 0._

_Proof. The proof follows from the Hermite Expansions of the matrix G[∞]_ and the complete proof is
provided in Appendix A.4.

**Assumption 2 (Training Data). Without loss of generality, we can assume that each xi is normalized**
to have a unit norm, i.e., ∥xi∥ = 1, for all i ∈ [n]. Moreover, we assume xi ̸∥ **_xj for all i ̸= j._**

In most real-world datasets, it is extremely rare that two data samples are parallel. If this happens,
by adding some random noise perturbation to the data samples, Assumption 2 can still be satisfied.
Next, we show that at the initialization, the spectral property of G[∞] is preserved in G(0) if m
is sufficiently large. Specifically, the following lemma shows that if m = Ω([˜] _n[2]), then G(0) has_
a strictly positive smallest eigenvalue with high probability. The proof follows from the standard
concentration bound for Gaussian random variables, and we relegate the proof to Appendix A.5.

_n[2]_ _n_
**Lemma 3.3. Let λ0 = λmin(G[∞]) > 0. If m = Ω** _λ[2]0_ [log] _δ_, then with probability of at least

1 − _δ, it holds that ∥G(0) −_ **_G[∞]∥2 ≤_** _[λ]4[0]_ [, and hence][ λ][min][(][G] [(0))][][ ≥] 4[3] _[λ][0][.]_

During training, G(t) is time-varying, but it can be shown to be close to G(0) and preserve the
spectral property of G[∞], if the matrices W (t) has a bounded operator norm and it is not far away
from W (0). This result is formally stated below and its proof is provided in Appendix A.6.

**Lemma 3.4.that satisfies SupposeW** _∥2cW[√] (0)m and∥≤_ _cW[√]m, andW (0) λmin{G16(0)√cmλ} ≥X0_ 4[3] _[λ][0][. For any matrix][ W][ ∈]_ [R][m][×][d]

1 _∥_ _∥≤_ _∥_ _−_ _∥≤_ _∥_ _∥[2][ ≜]_ _[R][, the matrix defined by][ G][ ≜]_

_m_ _[σ][(][XW][ T][ )][σ][(][XW][ T][ )][T][ satisfies][ ∥][G][ −]_ **_[G][(0)][∥≤]_** _[λ]4[0]_ [and][ λ][min][(][G][)][ ≥] _[λ]2[0]_ [.]

The next lemma shows three facts: (1) The smallest eigenvalue of G(t) is strictly positive for all
_t ≥_ 0; (2) The objective value L(t) converges to zero at a linear convergence rate; (3) The (operator)
norms of all trainable parameters are upper bounded by some constants, which further implies that
unique equilibrium points in matrices Z(t) always exist. We prove this lemma by induction, which
is provided in Appendix A.7.
**Lemma 3.5.λmin{G(0)} ≥ Suppose that[3]4** _[λ][0][ >][ 0][. If] ∥u[ m](0)[ = Ω]∥_ = _[√]mc[2]n, ∥λ∥X[3]0v(0)∥[2]_ _∥∥yˆ(0)=_ _[√] −my, ∥∥[2]W[]_ (0)and∥≤ 0 < γc[√] ≤m, ∥minA(0){ 2[1]∥≤[,][ 1]4c _[}]c[, then for][√]m, and_

any t 0, the following results hold: 
_≥_

(i) λmin(G(t)) ≥ _[λ]2[0]_ [,]

(ii) **_u(t)_** _λ0_ **_yˆ(0)_** **_y_**,
_∥_ _∥≤_ [16][c][√][n] _∥_ _−_ _∥_

(iii) **_v(t)_** _λ0_ **_y(0)_** **_y_**,
_∥_ _∥≤_ [8][c][√][n] _[∥]_ [ˆ] _−_ _∥_

(iv) ∥W (t)∥≤ 2c[√]m,

(v) ∥A(t)∥≤ 2c[√]m,

(vi) **_yˆ(t)_** **_y_** exp _λ0t_ **_yˆ(0)_** **_y_** .
_∥_ _−_ _∥[2]_ _≤_ _{−_ _}∥_ _−_ _∥[2]_


By using simple union bounds in Lemma 2.1 and 3.5, we immediately obtain the global convergence
result for the gradent flow as follows.


-----

**Theorem 3.1 (Convergence Rate of Gradient Flow). Suppose that Assumptions 1 and 2 hold. If**
_n[2]_ _n_
we set the number of parameter m = Ω _λ[2]0_ [log] _δ_ and choose 0 < γ ≤ min{ 2[1] _[,][ 1]4c_ _[}][, then with]_

probability at least 1 − _δ over the initialization, we have_   []
**_yˆ(t)_** **_y_** exp _λ0t_ **_yˆ(0)_** **_y_** _,_ _t_ 0.
_∥_ _−_ _∥[2]_ _≤_ _{−_ _}∥_ _−_ _∥[2]_ _∀_ _≥_

This theorem establishes the global convergence of the gradient flow. Despite the nonconvexity of
the objective function L(θ), Theorem 3.1 shows that if m is sufficiently large, then the objective
value is consistently decreasing to zero at a geometric rate. In particular, Theorem 3.1 requires
_m = Ω([˜]_ _n[2]), which is similar or even better than recent results for the neural network with finite_
layers (Nguyen & Mondelli, 2020; Oymak & Soltanolkotabi, 2020; Allen-Zhu et al., 2019; Zou &
Gu, 2019; Du et al., 2019). In particular, previous results showed that m is a polynomial of the
number of training sample n and the number of layers h, i.e., m = Ω(n[α]h[β]) with α ≥ 2 and
_β ≥_ 12 (Nguyen & Mondelli, 2020, Table 1). These results do not apply in our case since we have
_infinitely many layers. By taking advantage of the nonlinear feature mapping function, we establish_
the global convergence for the gradient flow with m independent of depth h.

3.2 DISCRETE TIME ANALYSIS

In this section, we show that the randomly initialized gradient descent method with a fixed stepsize converges to a global minimum at a linear rate. With similar argument used in the analysis
of gradient flow, we can show the (operator) norms of the training parameters are upper bounded
by some constants. It is worth noting that, unlike the analysis of gradient flow, we do not have an
explicit formula for the dynamics of prediction ˆy(t) in the discrete time analysis. Instead, we have to
show the difference between the equilibrium points Z(k) in two consecutive iterations are bounded.
Based on this, we can further bound the changes in the predictions ˆy(k) between two consecutive
iterations. Another challenge is to show the objective value consistently decreases over iterations.
A general strategy is to show the objective function is (semi-)smooth with respect to parameter θ,
and apply the descent lemma to the objective function (Nguyen & Mondelli, 2020; Allen-Zhu et al.,
2019; Zou & Gu, 2019). In this section, we take advantage of the nonlinear feature mapping function
in Eq. (4). Consequently, we are able to obtain a Polyak-Łojasiewicz-like condition (Karimi et al.,
2016; Nguyen, 2021), which allows us to provide a much simpler proof.

The following lemma establishes the global convergence for the gradient descent method with a
fixed step-size when the operator norms of A(0) and W (0) are bounded and λmin(G(0)) > 0. The
proof is proved in Appendix A.8.
**Lemma 3.6 (Gradient Descent Convergence Rate). Suppose** **_u(0)_** = _m,_ **_v(0)_** = _m,_
3 _∥_ _∥_ _[√]_ _∥_ _∥_ _[√]_
_∥W (0)∥≤_ _c[√]m, ∥A(0)∥≤_ _c[√]m, and λmin{G(0)} ≥_ 4 _[λ][0][ >][ 0][. If][ 0][ < γ][ ≤]_ [min][{][ 1]2 _[,][ 1]4c_ _[}][,]_

_m = Ω_ _c[2]nλ∥X[3]0_ _∥[2]_ _∥yˆ(0) −_ **_y∥[2][], and stepsize α = O_** _λ0/n[2][], then for any k ≥_ 0, we have
  

(i) λmin(G(k)) ≥ _[λ]2[0]_ [,]

(ii) **_u(k)_** _λ0_ **_yˆ(0)_** **_y_**,
_∥_ _∥≤_ [32][c][√][n] _∥_ _−_ _∥_

(iii) **_v(k)_** _λ0_ **_yˆ(0)_** **_y_**,
_∥_ _∥≤_ [16][c][√][n] _∥_ _−_ _∥_

(iv) ∥W (k)∥≤ 2c[√]m,

(v) ∥A(k)∥≤ 2c[√]m,

(vi) **_yˆ(k)_** **_y_** (1 _αλ0/2)[k]_ **_yˆ(0)_** **_y_** .
_∥_ _−_ _∥[2]_ _≤_ _−_ _∥_ _−_ _∥[2]_

By using simple union bounds to combine Lemma 2.1 and 3.6, we obtain the global convergence
result for the gradient descent.
**Theorem 3.2 (Convergence Rate of Gradient Descent). Suppose that Assumption 1 and 2 hold. If**
_n[2]_ _n_
we set m = Ω _λ0_ [log] _δ_, 0 < γ ≤ min{ 2[1] _[,][ 1]4c_ _[}][, and choose step-size][ α][ =][ O]_ _λ0/n[2][], then_

with probability at least 1  _−[]δ over the initialization, we have_  

**_yˆ(k)_** **_y_** (1 _αλ0/2)[k]_ **_yˆ(0)_** **_y_** _,_ _k_ 0.
_∥_ _−_ _∥[2]_ _≤_ _−_ _∥_ _−_ _∥[2]_ _∀_ _≥_


-----

train loss test loss operator norm of layer

0.5 width 500 1.0 width 500 width 500

1.0 width 1000width 2000 width 1000width 2000 0.1992 width 1000width 2000

width 4000 1.5 width 4000 width 4000

1.5 0.1990

2.0 2.0 0.1988

log(loss) 2.5 log(loss) 2.5 0.1986

3.0 operator norm

3.0 0.1984

3.5

3.5 0.1982

4.0

4.5 4.0 0.1980

0 200 400 600 800 1000 0 200 400 600 800 1000 0 200 400 600 800 1000

epoch epoch epoch


(a) Training loss

train loss test loss operator norm of layer

0.5 width 500 width 500 width 500

1.0 width 1000width 2000width 4000 1.0 width 1000width 2000width 4000 0.19920.1990 width 1000width 2000width 4000

1.5 1.5 0.1988

log(loss) 2.0 log(loss) 2.0 operator norm 0.1986

0.1984

2.5 2.5 0.1982

3.0 3.0 0.1980

0 200 400 600 800 1000 0 200 400 600 800 1000 0 200 400 600 800 1000

epoch epoch epoch


(d) Training loss

0.4 train loss test loss operator norm of layer

width 500 width 500 width 500

0.6 width 1000width 2000 0.6 width 1000width 2000 0.1992 width 1000width 2000

0.8 width 4000 0.8 width 4000 0.1990 width 4000

1.0 1.0 0.1988

log(loss) 1.2 log(loss) 1.2 0.1986

operator norm

1.4 1.4 0.1984

1.6 1.6 0.1982

1.8 1.8 0.1980

0 200 400 epoch 600 800 1000 0 200 400 epoch 600 800 1000 0 200 400 epoch 600 800 1000


(g) Training loss

train loss test loss operator norm of layer

0.4 width 500 width 500 width 500

0.6 width 1000width 2000 0.8 width 1000width 2000 0.1992 width 1000width 2000

width 4000 width 4000 width 4000

0.8 0.1990

1.0 1.0 0.1988

log(loss) 1.2 log(loss) 1.2 operator norm 0.1986

1.4 0.1984

1.6 1.4 0.1982

1.8 1.6 0.1980

0 200 400 600 800 1000 0 200 400 600 800 1000 0 200 400 600 800 1000

epoch epoch epoch


(j) Training loss


(b) Test loss

test loss

400 600

epoch

(e) Test loss

test loss

400 600

epoch

(h) Test loss

test loss

400 600

epoch

(k) Test loss


(c) Operator norms

operator norm of layer

200 400 600 800

epoch

(f) Operator norms

operator norm of layer

200 400 600 800

epoch

(i) Operator norms

operator norm of layer

200 400 600 800

epoch

(l) Operator norms


Figure 1: Results on MNIST, FashionMNIST, CIFAR10, and SVHN. We evaluate the impact of the
width m on the training loss, test loss, and operator norm of the scaled matrix (γ/[√]m)A(k) on four
real datasets.

4 EXPERIMENTAL RESULTS

In this section, we use real-world datasets MNST, FashionMNST, CIFAR10, and SVHN to evaluate our theoretical findings. We initialize the entries of parameters A, W, u, and v by standard
Gaussian or symmetric Bernoulli distribution independently as suggested in Assumption 1. For
each dataset, we only use classes 0 and 1, and 500 samples are randomly drawn from each class
to generate the training dataset with n = 1000 samples. All data samples are converted to gray
scale and resized into a 28 × 28 pixel image. We also normalize each data to have unit norm. If
two parallel samples are observed, we add a random Gaussian noise perturbation to one of them.
Thus, Assumption 2 is also satisfied. We run 1000 epochs of gradient descent with a fixed step-size.
We test three metrics with different width m. We first test how the extent of over-parameterization


-----

affects the convergence rates. Then, we test the relation between the extent of over-parameterization
and the operator norms between matrix A(k) and its initialization. Note that the “operator norm” in
the plots denotes the operator norm of the scaled matrix (γ/[√]m)∥A(k)∥. Third, we test the extent
of over-parameterization and the performance of the trained neural network on the unseen test data.
Similar to the training dataset, We randomly select 500 samples from each class as the test dataset.

From Figure 1, the figures in the first column show that as m becomes larger, we have better convergence rates. The figures in the second column show that as m becomes larger, the neural networks
achieve lower test loss. The figures in the third column show that the operator norms are slightly
larger for larger m but overall the operator norms are approximately equal to its initialization. The
bell curve from the classical bias-variance trade-off does not appear. This opens the door to a new research direction in implicit neural networks in the analyses of generalization error and bias-variance
trade-off.

5 RELATED WORKS

Implicit models has been explored explored by the deep learning community for decades. For example, Pineda (1987) and ALMEIDA (1987) studied implicit differentiation techniques for training
recurrent dynamics, also known as recurrent back-propagation (Liao et al., 2018). Recently, there
has been renewed interested in the implicit models in the deep learning community (El Ghaoui
et al., 2019; Gould et al., 2019). For example, Bai et al. (2019) introduces an implicit neural network called deep equilibrium model for the for the task of sequence modeling. By using implicit
ODE solvers, Chen et al. (2018) proposed neural ordinary differential equation as an implicit residual network with continuous-depth. Other instantiations of implicit modeling include optimization
layers (Djolonga & Krause, 2017; Amos & Kolter, 2017), differentiable physics engines (de Avila
Belbute-Peres et al., 2018; Qiao et al., 2020), logical structure learning (Wang et al., 2019), and
continuous generative models (Grathwohl et al., 2019).

The theoretical study of training finite-layer neural networks via over-parameterization has been an
active research area. Jacot et al. (2018) showed the trajectory of the gradient descent method can
be characterized by a kernel called neural tangent kernel for smooth activation and infinitely width
neural networks. For a finite-width neural network with smooth or ReLU activation, Arora et al.
(2019); Du et al. (2019); Li & Liang (2018) showed that the dynamics of the neural network is governed by a Gram matrix. Consequently, Zou et al. (2020); Du et al. (2019); Allen-Zhu et al. (2019);
Nguyen & Mondelli (2020); Arora et al. (2019); Zou & Gu (2019); Oymak & Soltanolkotabi (2020)
showed that (stochastic) gradient descent can attain global convergence for training a finite-layer
neural network when the width m is a polynomial of the sample size n and the depth h. However,
their results cannot be applied directly to implicit neural networks since implicit neural networks
have infinite layers and the equilibrium equation may not be well-posed. Our work establishes the
well-posedness of the equilibrium equation even if the width m is only square of the sample size n.

6 CONCLUSION AND FUTURE WORK

In this paper, we provided a convergence theory for implicit neural networks with ReLU activation
in the over-parameterization regime. We showed that the random initialized gradient descent method
with fixed step-size converges to a global minimum of the loss function at a linear rate if the width
_m = Ω([˜]_ _n[2]). In particular, by using a fixed scalar γ ∈_ (0, 1) to scale the random initialized weight
matrix A, we proved that the equilibrium equation is always well-posed throughout the training. By
analyzing the gradient flow, we observe that the dynamics of the prediction vector is controlled by a
Gram matrix whose smallest eigenvalue is lower bounded by a strictly positive constant as long as
_m = Ω([˜]_ _n[2]). We envision several potential future directions based on the observations made in the_
experiments. First, we believe that our analysis can be generalized to implicit neural networks with
other scaling techniques and initialization. Here, we use a scalar γ/[√]m to ensure the existence of
the equilibrium point z[∗] with random initialization. With an appropriate normalization, the global
convergence for identity initialization can be obtained. Second, we believe that the width m not
only improves the convergence rates but also the generalization performance. In particular, our
experimental results showed that with a larger m value, the test loss is reduced while the classical
bell curve of bias-variance trade-off is not observed.


-----

ACKNOWLEDGMENTS

This work has been supported in part by National Science Foundation grants III-2104797,
DMS1812666, CAREER CNS-2110259, CNS-2112471, CNS-2102233, CCF-2110252, CNS-2120448, CCF-19-34884 and a Google Faculty Research Award.

REFERENCES

Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via overparameterization. In International Conference on Machine Learning, pp. 242–252. PMLR, 2019.

LB ALMEIDA. A learning rule for asynchronous perceptrons with feedback in a combinatorial
environment. In Proceedings, 1st First International Conference on Neural Networks, volume 2,
pp. 609–618. IEEE, 1987.

Brandon Amos and J Zico Kolter. Optnet: Differentiable optimization as a layer in neural networks.
In International Conference on Machine Learning, pp. 136–145. PMLR, 2017.

Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient
descent for deep linear neural networks. arXiv preprint arXiv:1810.02281, 2018.

Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. arXiv preprint arXiv:1904.11955, 2019.

Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Trellis networks for sequence modeling. arXiv
_preprint arXiv:1810.06682, 2018._

Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. _arXiv preprint_
_arXiv:1909.01377, 2019._

Shaojie Bai, Vladlen Koltun, and J Zico Kolter. Multiscale deep equilibrium models. arXiv preprint
_arXiv:2006.08656, 2020._

Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. arXiv preprint arXiv:1806.07366, 2018.

Raj Dabre and Atsushi Fujita. Recurrent stacking of layers for compact neural machine translation
models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 6292–
6299, 2019.

Filipe de Avila Belbute-Peres, Kevin Smith, Kelsey Allen, Josh Tenenbaum, and J Zico Kolter. Endto-end differentiable physics for learning and control. Advances in neural information processing
_systems, 31:7178–7189, 2018._

Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser. Universal
transformers. arXiv preprint arXiv:1807.03819, 2018.

Josip Djolonga and Andreas Krause. Differentiable learning of submodular models. Advances in
_Neural Information Processing Systems, 30:1013–1023, 2017._

Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675–
1685. PMLR, 2019.

Laurent El Ghaoui, Fangda Gu, Bertrand Travacca, Armin Askari, and Alicia Y Tsai. Implicit deep
learning. arXiv preprint arXiv:1908.06315, 2, 2019.

Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
_statistics, pp. 249–256. JMLR Workshop and Conference Proceedings, 2010._

Stephen Gould, Richard Hartley, and Dylan Campbell. Deep declarative networks: A new hope.
_arXiv preprint arXiv:1909.04866, 2019._


-----

Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord:
Free-form continuous dynamics for scalable reversible generative models. International Confer_ence on Learning Representations, 2019._

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
_conference on computer vision, pp. 1026–1034, 2015._

Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. arXiv preprint arXiv:1806.07572, 2018.

Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximalgradient methods under the polyak-łojasiewicz condition. In Joint European Conference on Ma_chine Learning and Knowledge Discovery in Databases, pp. 795–811. Springer, 2016._

Kenji Kawaguchi. On the theory of implicit deep learning: Global convergence with implicit layers.
_arXiv preprint arXiv:2102.07346, 2021._

Erwin Kreyszig. Introductory functional analysis with applications, volume 1. wiley New York,
1978.

Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. arXiv preprint arXiv:1808.01204, 2018.

Renjie Liao, Yuwen Xiong, Ethan Fetaya, Lisa Zhang, KiJung Yoon, Xaq Pitkow, Raquel Urtasun, and Richard Zemel. Reviving and improving recurrent back-propagation. In International
_Conference on Machine Learning, pp. 3082–3091. PMLR, 2018._

Barbara MacCluer. Elementary functional analysis, volume 253. Springer Science & Business
Media, 2008.

Quynh Nguyen. On the proof of global convergence of gradient descent for deep relu networks with
linear widths. arXiv preprint arXiv:2101.09612, 2021.

Quynh Nguyen and Marco Mondelli. Global convergence of deep networks with one wide layer
followed by pyramidal topology. arXiv preprint arXiv:2002.07867, 2020.

Samet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks. IEEE Journal on Selected Areas in
_Information Theory, 1(1):84–105, 2020._

Fernando Pineda. Generalization of back propagation to recurrent and higher order neural networks.
In Neural information processing systems, pp. 602–611, 1987.

Yi-Ling Qiao, Junbang Liang, Vladlen Koltun, and Ming C Lin. Scalable differentiable physics for
learning and control. arXiv preprint arXiv:2007.02168, 2020.

Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.

Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Sci_ence. Number 47 in Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge_
University Press, 2018. ISBN 978-1-108-41519-4.

Po-Wei Wang, Priya Donti, Bryan Wilder, and Zico Kolter. Satnet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver. In International Conference on Machine
_Learning, pp. 6545–6554. PMLR, 2019._

Difan Zou and Quanquan Gu. An improved analysis of training over-parameterized deep neural
networks. arXiv preprint arXiv:1906.04688, 2019.

Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes overparameterized deep relu networks. Machine Learning, 109(3):467–492, 2020.


-----

A APPENDIX

A.1 PROOF OF LEMMA 2.2

_Proof. Let γ0 ∈_ (0, 1). Set γ ≜ min{γ0, γ0/c}. Denote ˜γ = γ/[√]m. Then
**_z[ℓ][+1]_** _−_ **_z[ℓ]_** = _σ_ _γ˜Az[ℓ]_ + φ _−_ _σ_ _γ˜Az[ℓ][−][1]_ + φ

_γ˜_ **_Az _** _[ℓ]_ **_Az[ℓ][−][1]_** _,_   _σ is 1-Lipschitz continuous_
_≤_ _−_

=˜γ **_A(z[ℓ]_** _−_ **_z[ℓ][−][1])_**

_≤γ˜∥A∥∥z[ℓ]_ _−_ **_z[ℓ][−][1]∥,_**

_γc˜_ _[√]m_ **_z[ℓ]_** **_z[ℓ][−][1]_** _,_ **_A_** _c[√]m_
_≤_ _∥_ _−_ _∥_ _∥_ _∥≤_

=γ0 **_z[ℓ]_** **_z[ℓ][−][1]_** _._
_∥_ _−_ _∥_

Applying the above argument ℓ times, we obtain

_∥z[ℓ][+1]_ _−_ **_z[ℓ]∥≤_** _γ0[ℓ][∥][z][1][ −]_ **_[z][0][∥]_** [=][ γ]0[ℓ][∥][z][1][∥] [=][ γ]0[ℓ][∥][σ][(][φ][)][∥≤] _[γ]0[ℓ][∥][φ][∥][,]_

where we use the fact z[0] = 0. For any positive integers p, q with p ≤ _q, we have_

_∥z[p]_ _−_ **_z[q]∥≤∥z[p]_** _−_ **_z[p][+1]∥_** + · · · + ∥z[q][−][1] _−_ **_z[q]∥_**

_≤γ0[p][∥][φ][∥]_ [+][ · · ·][ +][ γ]0[q][∥][φ][∥]

_≤γ0[p][∥][φ][∥]_ 1 + γ0 + γ0[2] [+][ · · ·]

_γ0[p]_   
= **_φ_** _._

1 _γ0_ _∥_ _∥_
_−_

Since γ0 ∈ (0, 1), we have ∥z[p] _−_ **_z[q]∥→_** 0 as p →∞. Hence, {z[ℓ]}ℓ[∞]=1 [is a Cauchy sequence.]
Since R[m] is complete, the equilibrium point z[∗] is the limit of the sequenceγ[p] _{z[ℓ]}ℓ[∞]=1[, so that][ z][ exists]_
and is unique. Moreover, let q, then we obtain **_z[p]_** **_z[∗]_** 1 _γ_

iteration converges to z linearly. →∞ _∥_ _−_ _∥≤_ _−_ _[∥][φ][∥][, so that the fixed-point]_


Let p = 0 and q = ℓ, then we obtain ∥z[ℓ]∥≤

A.2 PROOF OF LEMMA 2.3


1

1−γ0

_[∥][φ][∥][.]_


_Proof._ (i) To simplify the notations, we denote D ≜ **diag(σ[′](˜γAz + φ)), and E**
**diag(σ[′](W x)). The differential of f is given by**

_df =d(z −_ _γσ˜_ (˜γAz + φ))
=dz − **_Dd(˜γAz + φ)_**
= [Im _γ˜DA] dz_ _γ˜D(dA)z_ **_Ddφ._**
_−_ _−_ _−_

Taking vectorization on both sides yields


vec (df ) = [Im _γ˜DA] vec (dz)_ vec (˜γDdAz) **_Dvec (dφ)_**
_−_ _−_ _−_

= [Im _γ˜DA] vec (dz)_ _γ˜[z[T]_ **_D]vec (dA)_** **_Dvec (dφ) ._**
_−_ _−_ _⊗_ _−_

Therefore, the partial derivative of f with respect to z, A, and φ are given by

_∂f_
_γDA][T]_ _,_
_∂z_ [= [][I][m][ −] [˜]

_∂f_ _T_

_γ_ **_z[T]_** **_D_** _,_
_∂A_ [=][ −][˜] _⊗_

_∂f_  

_∂φ_ [=][ −][D][T][ .]


-----

It follows from the definition of the feature vector φ in Eq. (2) that


**_x[T]_** _⊗_ **_E_** vec (dW ) .



_dφ =_ _√mdσ(W x) =_ _√m_ **_E(dW )x =_** _√m_


Thus, the partial derivative of φ with respect to W is given by
_∂φ_ 1 _T_

_∂W_ [=] _√m_ **_x[T]_** _⊗_ **_E_** _._ (18)

By using the chain rule, we obtain the partial derivative of  _f with respect to W as follows_
_∂f_ _∂f_ **_x[T]_** **_E_** _T DT ._

_∂W_ [=][ ∂]∂W[φ] _∂φ_ [=][ −] _√[1]m_ _⊗_

 

(ii) Let v be an arbitrary vector, and u be an arbitrary unit vector. The reverse triangle inequality
implies that

(Im _γ˜ diag(σ[′](v))A) u_ **_u_** _γ˜ diag(σ[′](v))Au_
_∥_ _−_ _∥≥∥_ _∥−∥_ _∥_

_≥∥u∥−_ _γ˜∥_ **diag(σ[′](v))∥∥A∥∥u∥**

(a)
(1 _γ0)_ **_u_**
_≥_ _−_ _∥_ _∥_
=1 − _γ0 > 0,_

left-hand side over all unit vectorwhere (a) is due to |σ[′](v)| ≤ 1 u and yields the desired result. ∥A∥op ≤ _c[√]m. Therefore, taking infimum on the_


(iii) Since f (z[∗], A, W ) = 0, taking implicit differentiation of f with respect to A at z[∗] gives us
_∂z_ _∂f_ _∂f_

+ = 0.

_∂A_ _z=z[∗]_ _∂z_ _z=z[∗]_ _∂A_ **_z=z[∗]_**

     

The results in part (i)-(ii) imply the smallest eigenvalue of _∂[∂f]z_ **_z[∗]_** [is strictly positive, so that it]

is invertible. Therefore, we have

_∂z[∗]_ _∂f_ _∂f_ _−1_ _T_

= ˜γ **_z[T]_** **_D_** [Im _γ˜DA][−][T]_ _._ (19)

_∂A_ [=][ −] _∂A_ **_z=z[∗]_** _∂z_ _z=z[∗]_ _⊗_ _−_
   

 

Similarly, we obtain the partial derivative of z[∗] with respect to W as follows

_∂z[∗]_ _∂f_ _∂f_ _−1_ 1 _T_ _T_

= **_x[T]_** **_E_** **_D_** [Im _γ˜DA][−][T]_ _. (20)_

_∂W_ [=][ −] _∂W_ **_z=z[∗]_** _∂z_ _z=z[∗]_ _√m_ _⊗_ _−_
   

 

To further simplify the notation, we denote z to be the equilibrium point z[∗] by omitting the
superscribe, i.e., z = z[∗]. Let ˆy = u[T] **_z + v[T]_** **_φ be the prediction for the training data (x, y)._**
The differential of ˆy is given by

_dyˆ = d_ **_u[T]_** **_z + v[T]_** **_φ_** = u[T] _dz + zdu + v[T]_ _dφ + φ[T]_ _dv._


The partial derivative of ˆy with respect to u, v, z, and φ are given by
_∂yˆ_ _∂yˆ_ _∂yˆ_ _∂yˆ_

(21)

_∂z_ [=][ u][,] _∂u_ [=][ z][,] _∂v_ [=][ φ][,] _∂φ_ [=][ v][.]

Let ℓ = [1]2 [(ˆ]y − _y)[2]. Then ∂ℓ/∂yˆ = (ˆy −_ _y). By chain rule, we have_

_∂ℓ_ _y_ _∂ℓ_

_y_ _y),_ (22)

_∂u_ [=][ ∂]∂u[ˆ] _∂yˆ_ [=][ z][(ˆ] −

_∂ℓ_ _y_ _∂ℓ_

_y_ _y)._ (23)

_∂φ_ [=][ ∂]∂v[ˆ] _∂yˆ_ [=][ φ][(ˆ] −


By using (19)-(20) and chain rule, we obtain
_∂ℓ_ _∂ℓ_

_∂A_ [=][ ∂]∂A[z] _∂z_

_∂yˆ_ _∂ℓ_ _T_

= _[∂][z]_ _γ(ˆy_ _y)_ **_z[T]_** **_D_** [Im _γ˜DA][−][T]_ **_u,_** (24)

_∂A_ _∂z_ _∂yˆ_ [= ˜] _−_ _⊗_ _−_

 


-----

and


_∂ℓ_ _∂yˆ_ _∂ℓ_ _∂yˆ_ _∂ℓ_

_∂W_ [=][ ∂]∂W[z] _∂z_ _∂yˆ_ [+][ ∂]∂W[φ] _∂φ_ _∂yˆ_


= [1] (ˆy _y)[x[T]_ **_E][T][ ]D[T]_** (Im _γ˜DA)[−][T]_ **_u + v_** _._ (25)
_√m_ _−_ _⊗_ _−_


Since L = _i=1_ _[ℓ][i][ with][ ℓ][i][ =][ ℓ][(ˆ]yi, yi), we have dL =_ _i=1_ _[dℓ][i][ and][ ∂L/∂ℓ][i][ = 1][. Therefore,]_
we obtain

_n_ _n_

_∂L_ [P][n]∂ℓi _T_ [P][n]

_∂A_ [=] _i=1_ _∂A_ [=] _i=1_ _γ˜(ˆyi −_ _yi)_ **_zi[T]_** _[⊗]_ **_[D][i]_** [Im − _γ˜DiA][−][T]_ **_u,_** (26)

Xn Xn  

_∂L_ _∂ℓi_ 1

(ˆyi _yi)[x[T]i_ **_Di[T]_** [(][I][m] _γDiA)[−][T]_ **_u + v_** _,_ (27)

_∂W_ [=] _i=1_ _∂W_ [=] _i=1_ _√m_ _−_ _[⊗]_ **_[E][i][]][T][ ]_** _[−]_ [˜]

_nX_ _n_ X 

_∂L_ _∂ℓi_

(ˆyi _yi)zi,_ (28)

_∂u_ [=] _i=1_ _∂u_ [=] _i=1_ _−_

Xn Xn

_∂L_ _∂ℓi_

(ˆyi _yi)φi._ (29)

_∂v_ [=] _i=1_ _∂v_ [=] _i=1_ _−_

X X

A.3 PROOF OF LEMMA 3.1

_Proof. Let zi denote the i-th equilibrium point for the i-the data sample xi. By using (19), (20),_
(26) and (27), we obtain the dynamics of the equilibrium point zi as follows


_T_ _T_

_dzi_ _∂zi_ _dvec (A)_ _∂zi_ _dvec (W )_

+

_dt_ [=] _∂A_ _dt_ _∂W_ _dt_
   

_T_ _T_

_∂zi_ _∂zi_
= +

_∂A_ _−_ _∂[∂L]A_ _∂W_ _−_ _∂[∂L]W_

 n      

= _γ˜[2]_ (ˆyj _yj) [Im_ _γ˜DiA][−][1]_ [zi[T] **_zj[T]_** _T [Im_ _γ˜DjA][−][T]_ **_u_**
_−_ _j=1_ _−_ _−_ _[⊗]_ **_[D][i][]]_** _[⊗]_ **_[D][j]_** _−_
Xn  

(ˆyj _yj) [Im_ _γ˜DiA][−][1]_ **_Di_** **_x[T]i_** [x[T]j **_Dj[T]_** [(][I][m] _γDjA)[−][T]_ **_u + v_**

_−_ _m[1]_ _j=1_ _−_ _−_ _[⊗]_ **_[E][i]_** _[⊗]_ **_[E][j][]][T][ ]_** _[−]_ [˜]

Xn  

= _γ˜[2]_ (ˆyj _yj) [Im_ _γ˜DiA][−][1]_ **_DiDj[T]_** [[][I][m] _γDjA][−][T]_ **_uzi[T]_** **_[z][j]_**
_−_ _j=1_ _−_ _−_ _[−]_ [˜]
X

_n_

(ˆyj _yj) [Im_ _γ˜DiA][−][1]_ **_DiEiEj[T]_** **_Dj[T]_** [(][I][m] _γDjA)[−][T]_ **_u + v_** **_x[T]i_** **_[x][j][.]_**

_−_ _m[1]_ _j=1_ _−_ _−_ _[−]_ [˜]

X  

By using (18) and 27, we obtain the dynamics of the feature vector φi


_T_

_dφi_ _∂φi_ _dvec (W )_

_dt_ [=] _∂W_ _dt_
 

_T_

_∂φi_
=

_∂W_ _−_ _∂W[∂L]_

 n  

= (ˆyi _yi)EiEj[T]_ [[][D]j[T] [(][I][m] _γDjA)[−][T]_ **_u + v]x[T]i_** **_[x][j][.]_**
_−_ _m[1]_ _j=1_ _−_ _[−]_ [˜]

X


-----

By chain rule, the dynamics of the prediction ˆyi is given by


_T_

_dyˆi_ _∂yˆi_ _dzi_ _∂yˆi_

_dt_ [=] _∂zi_ _dt_ [+] _∂φi_
  


_T dφi_ _∂yˆi_

_dt_ [+] _∂u_

 


_T du_ _∂yˆi_ _T dv_

_dt_ [+] _∂v_ _dt_

  


= − _γ˜[2]_ _j=1(ˆyj −_ _yj)_ **_u[T]_** (Im − _γ˜DiA)[−][1]DiDj[T]_ [(][I][m] _[−]_ _γ[˜]DjA)[−][T]_ **_u_** (zi[T] **_[z][j][)]_**
Xn  

(ˆyj _yj)_ **_Di[T]_** [(][I][m] _γDiA)[−][1]u + v_ _T EiEjT_ **_Dj[T]_** [(][I][m] _γDjA)[−][T]_ **_u + v_** (x[T]i **_[x][j][)]_**

_−_ _m[1]_ _j=1_ _−_ _[−]_ [˜] _[−]_ [˜]

_n_ X h     [i]


_−_ _j=1(ˆyj −_ _yj)(zi[T]_ **_[z][j][)]_**

X

_n_

_−_ _j=1(ˆyj −_ _yj)(φ[T]i_ **_[φ][j][)][.]_**

X

Define the matrices M (t) ∈ R[n][×][n] and Q(t) ∈ R[n][×][n] as follows

**_M_** (t)ij ≜ [1] _γDiA)[−][1]DiDj[T]_ [(][I][m] _γDjA)[−][T]_ **_u,_**

_m_ **_[u][T][ (][I][m][ −]_** [˜] _[−]_ [˜]

**_Q(t)ij ≜_** [1] **_Di[T]_** [(][I][m] _γDiA)[−][1]u + v_ _T EiEjT_ **_Dj[T]_** [(][I][m] _γDjA)[−][T]_ **_u + v_** _._

_m_ _[−]_ [˜] _[−]_ [˜]
     

Let X ∈ R[n][×][d], Φ(t) ∈ R[n][×][m], and Z(t) ∈ R[n][×][m] be the matrices whose rows are the training
data xi, feature vectors φi, and equilibrium points zi at time t, respectively. The dynamics of the
prediction vector ˆy is given by


_dyˆ_

_γ[2]M_ (t) + In **_Z(t)Z(t)[T]_** + Q(t) **_XX_** _[T]_ + Φ(t)Φ(t)[T][ ] (ˆy(t) **_y)._**
_dt_ [=][ −] _◦_ _◦_ _−_
  

A.4 PROOF OF LEMMA 3.2

A.4.1 REVIEW OF HERMITE EXPANSIONS

To make the paper self-contained, we review the necessary background about the Hermite polynomials in this section. One can find each result in this section from any standard textbooks about
functional analysis such as MacCluer (2008); Kreyszig (1978), or most recent literature (Nguyen &
Mondelli, 2020, Appendix D) and (Oymak & Soltanolkotabi, 2020, Appendix H).

We consider an L[2]-space defined by L[2](R, dP ), where dP is the Gaussian measure, that is,


1

2 .

2π [e][−] _[x][2]_


_dP = p(x)dx,_ where _p(x) =_


Thus, L[2](R, dP ) is a collection of functions f for which
_∞_ _∞_

_f_ (x) _dP_ (x) = _f_ (x) _p(x)dx = Ex_ _N_ (0,1) _f_ (x) _<_ _._
_|_ _|[2]_ _|_ _|[2]_ _∼_ _|_ _|[2]_ _∞_

Z−∞ Z−∞

**Lemma A.1. The ReLU activation σ ∈** _L[2](R, dP_ ).


_Proof. Note that_
_∞_ _∞_

_σ(x)_ _p(x)dx_ _x_ _p(x)dx = Ex_ _N_ (0,1) _x_ = Var(x) = 1.
_|_ _|[2]_ _≤_ _|_ _|[2]_ _∼_ _|_ _|[2]_

Z−∞ Z−∞


-----

For any functions f, g ∈ _L[2](R, dP_ ), we define an inner product

_∞_ _∞_
_f, g_ := _f_ (x)g(x)dP (x) = _f_ (x)g(x)p(x)dx = Ex _N_ (0,1)[f (x)g(x)].
_⟨_ _⟩_ _∼_
Z−∞ Z−∞

Furthermore, the induced norm ∥· ∥ is given by

_∞_
_f_ = _f, f_ = _f_ (x) _dP_ (x) = Ex _N_ (0,1) _f_ (x) _._
_∥_ _∥[2]_ _⟨_ _⟩_ _|_ _|[2]_ _∼_ _|_ _|[2]_
Z−∞


This L[2] space has an orthonormal basis with respect to the inner product defined above, called
_normalized probabilist’s Hermite polynomials {hn(x)}n[∞]=0_ [that are given by]


1

( 1)[n]e[x][2][/][2]D[n](e[−][x][2][/][2]), where _D[n](e[−][x][2][/][2]) =_ _[d][n]_
_n!_ _−_ _dx[n][ e][−][x][2][/][2][.]_


_hn(x) =_


**Lemma A.2. The normalized probabilist’s Hermite polynomials is an orthonormal basis of**
_L[2](R, dP_ ): ⟨hm, hn⟩ = δmn.

_Proof. Note that D[n](e[−][x][2][/][2]) = e[−][x][2][/][2]Pn(x) for a polynomial with degree of n and leading term_
is (−1)[n]x[n]. Thus, we can consider hn(x) = _√1n!_ [(][−][1)][n][P][n][(][x][)][.]

Assume m < n


_hn, hm_ =Ex _N_ (0,1)[hn(x)hm(x)]
_⟨_ _⟩_ _∼_

_∞_ 1
= _hn(x)hm(x)_

_√2π [e][−][x][2][/][2][dx,]_

Z−∞

1 _∞_
= ( 1)[n] _D[n](e[−][x][2][/][2])hm(x)dx,_ rewrite hn(x) by its definition
_√2π√n!_ _−_ Z−∞

1 _∞_
= _√2π√n!√m!_ (−1)[n][+][m] Z−∞ _D[n](e[−][x][2][/][2])Pm(x)dx,_ rewrite hm by the polynomial form

1 _∞_
= ( 1)[2][n][+][m] _e[−][x][2][/][2]Dn[Pm(x)]dx,_ integration by parts n times
_√2π√n!√m!_ _−_ Z−∞

There is no boundary terms because the super exponential decay of e[−][x][2][/][2] at infinity. Since m < n,
then Dn(Pm) = 0 so that _hm, hn_ =0. If m = n, then Dn(Pm) = ( 1)[n]n!. Thus, _hn, hn_ =
_⟨_ _⟩_ _−_ _⟨_ _⟩_
1.

**Remark: Since {hn} is an orthonormal basis, for every f ∈** _L[2](R, dP_ ), we have


_f, hn_ _hn(x)_
_⟨_ _⟩_
_n=0_

X


_f_ (x) =


in the sense that


lim _f, hn_ _hn(x)_ = lim
_N_ _⟨_ _⟩_ _N_
_→∞_ _n=0_ _→∞_ [E][x][∼][N] [(0][,][1)]

X

**Lemma A.3.[f] f[(][x][)][ −]L[2](R, dP** ) if and only if _n=0_ _[f]_ [(][x][)][ −]
_∈_ _[|⟨][f, h][n][⟩|][2][ <][ ∞][.]_

[P][∞]


_f, hn_ _hn(x)_
_⟨_ _⟩_
_n=0_

X


= 0


-----

_Proof. Note that_


_∞_
_⟨f, f_ _⟩_ = _|f_ (x)|[2] _dP_ (x)
Z−∞

_∞_ _∞_ _∞_
= _f, hi_ _hi(x)_ _f, hj_ _hj(x)_ _dP_ (x)
Z−∞ _i=0_ _⟨_ _⟩_ ! []j=0 _⟨_ _⟩_ 

X X

_∞_ _∞_  

= _f, hi_ _f, hj_ _hi(x)hj(x)dP_ (x)

_⟨_ _⟩⟨_ _⟩_
_i,j=0_ Z−∞

X

_∞_

= _f, hi_ _._

_|⟨_ _⟩|[2]_
_i=1_

X

**Lemma A.4. Consider a Hilbert space H with inner product ⟨·, ·⟩. If ∥fn** _−f_ _∥→_ 0 and ∥gn _−g∥→_
0, then ⟨f, g⟩ = limn→∞ _⟨fn, gn⟩._

_Proof. Observe that_

_f, g_ _fn, gn_ _f, g_ _fn, g_ + _fn, g_ _fn, gn_
_|⟨_ _⟩−⟨_ _⟩| ≤|⟨_ _⟩−⟨_ _⟩|_ _|⟨_ _⟩−⟨_ _⟩|_
_f_ _g_ _gn_ + _fn_ _g_ _gn_ _._
_≤∥_ _∥∥_ _−_ _∥_ _∥_ _∥∥_ _−_ _∥_

Let n →∞, then the continuity of ∥· ∥ implies the desired result.

**Lemma A.5. Let {hn(x)} be the normalized probabilist’s Hermite polynomials. For any fixed**
number t, we have


_t[n]_

_hn(x)._ (30)

_√n!_


_e[xt][−][t][2][/][2]_ =


_n=0_


_Proof. First, we show f_ (x) = e[xt][−][t][2][/][2] _∈_ _H ≜_ _L[2](R, dP_ ).

_f, f_ =Ex _N_ (0,1) _f_ (x)
_⟨_ _⟩_ _∼_ _|_ _|[2]_

_∞_ 1
= _e[2][xt][−][t][2]_ _√2π [e][−][x][2][/][2][dx]_
Z−∞

1

=e[t][2][ Z][ ∞] _√2π_ [exp] _−_ [(][x][ −]2[2][t][)][2] _dx,_ _x ∼_ _N_ (2t, 1)

_−∞_  

=e[t][2] _<_ _._
_∞_

Thus f (x) _H. Then f_ (x) = _n=0_
_∈_ _[⟨][f, h][n][⟩]_ _[h][n][(][x][)][. Note that]_

_f, hn_ =Ex _N_ (0,1)[f (x)hn(x)]
_⟨_ _⟩_ _∼_ [P][∞]

_∞_ 1 1
= _e[xt][−][t][2][/][2]_ ( 1)[n]e[x][2][/][2]Dn(e[−][x][2][/][2])
Z−∞ _·_ _√n!_ _−_ _·_ _√2π [e][−][x][2][/][2][dx]_

1 _∞_

= [1] ( 1)[n] _e[xt][−][t][2][/][2]_ _Dn(e[−][x][2][/][2])dx,_ integration by parts n times
_√n!_ _−_ _√2π_ Z−∞ _·_

= √[1]n! (−1)[2][n] _√12π_ Z ∞−∞ _e[xt][−][t][2][/][2]t[n]_ _· e[−][x][2][/][2]dx_

_∞_ 1

= _[t][n]_ _x_ _N_ (t, 1)

_√n!_ Z−∞ _√2π [e][−][(][x][−][t][)][2][/][2][dx,]_ _∼_

= _√[t][n]n!_ _._


-----

**Lemma A.6. Let a, b ∈** R[d] with ∥a∥ = ∥b∥ = 1, then

Ew _N_ (0,Id)[hn( **_a, w_** )hm( **_b, w_** )] = **_a, b_** _δmn._
_∼_ _⟨_ _⟩_ _⟨_ _⟩_ _⟨_ _⟩[n]_

_Proof. Given fixed numbers s and t, we define two functions f_ (w) = e[⟨][a][,][w][⟩][t][−][t][2][/][2] and g(w) =
_e[⟨][b][,][w][⟩][s][−][s][2][/][2]. Let x = ⟨a, w⟩_ and y = ⟨b, w⟩. Then we have


_t[n]_

_hn(x) =_

_√n!_

_s[n]_

_hn(y) =_

_√n!_


_t[n]_

_hn(_ **_a, w_** ),

_√n!_ _⟨_ _⟩_

_s[n]_

_hn(_ **_b, w_** ).

_√n!_ _⟨_ _⟩_


_f_ (w) =e[⟨][a][,][w][⟩][t][−][t][2][/][2] = e[xt][−][t][2][/][2] =

_g(w) =e[⟨][b][,][w][⟩][s][−][s][2][/][2]_ = e[ys][−][s][2][/][2] =


_n=0_

_∞_

_n=0_

X


_n=0_

_∞_

_n=0_

X


Define a Hilbert space Hd = L[2](R[d], dP ), where dP is the multivariate Gaussian measure,
equipped with inner product _f, g_ ≜ Ew _N_ (0,Id)[f (w)g(w)]. Clearly, f, g _Hd. Define se-_
_⟨_ _⟩_ _∼_ _∈_
quences _fN_ and _gN_ as follows
_{_ _}_ _{_ _}_


_t[n]_

_hn(_ **_a, w_** ) and _gN_ (w) =

_√n!_ _⟨_ _⟩_


_s[n]_

_hn(_ **_b, w_** ).

_√n!_ _⟨_ _⟩_


_fN_ (w) =


_n=0_


_n=0_


Since _f_ _fN_ 0 and _g_ _gN_ 0, we have
_∥_ _−_ _∥→_ _∥_ _−_ _∥→_

Ew _N_ (0,Id)[f (w)g(w)] = _f, g_
_∼_ _⟨_ _⟩_

= lim
_N_ _→∞_ _[⟨][f][N]_ _[, g][N]_ _[⟩]_

= lim
_N_ _→∞_ [E][w][∼][N] [(][0][,][I][d][)][[][f][N] [(][w][)][g][N] [(][w][)]]


_t[n]s[m]_

Ew _N_ (0,Id)[hn( **_a, w_** )gn( **_b, w_** )]

_n!√m!_ _∼_ _⟨_ _⟩_ _⟨_ _⟩_


= lim
_N_ _→∞_ _n,m=0_

X

Note that the LHS is also given by


Ew _N_ (0,Id)[f (w)g(w)] =e[−][t][2][/][2][−][s][2][/][2]Ew _N_ (0,Id)[e[⟨][a][,][w][⟩][t][+][⟨][b][,][w][⟩][s]]
_∼_ _∼_

_d_
=e[−][t][2][/][2][−][s][2][/][2]Ew _N_ (0,Id)[e _i=1_ **_[w][i][(][a][i][t][+][b][i][s][)]]_**
_∼_ P


=e[−][t][2][/][2][−][s][2][/][2]

=e[−][t][2][/][2][−][s][2][/][2]

=e[⟨][a][,][b][⟩][st]


Ewi _N_ (0,1)[e[w][i][(][a][i][t][+][b][i][s][)]]
_∼_
_i=1_

Y

_d_

_Mwi_ (ait + bis)
_i=1_

Y


_⟨a, b⟩[n]_ (st)[n]

_n!_


_n=0_


Since s and t are arbitrary numbers, matching the coefficients yields

Ew _N_ (0,Id)[hn( _a, w_ )hm( _b, w_ )] = **_a, b_** _δmn._
_∼_ _⟨_ _⟩_ _⟨_ _⟩_ _⟨_ _⟩[n]_

A.4.2 LOWER BOUND THE SMALLEST EIGENVALUES OF G[∞]

The result in this subsection is similar to the results in (Nguyen & Mondelli, 2020, Appendix D) and
(Oymak & Soltanolkotabi, 2020, Appendix H). The key difference is the assumptions made on the
training data. In particular, Oymak & Soltanolkotabi (2020) assumes the training data is δ-separable,
_i.e.the data, min x{∥i follows some sub-Gaussian random variable, while we assume no two data are parallelxi −_ **_xj∥, ∥xi + xj∥} ≥_** _δ > 0 for all i ̸= j, and Nguyen & Mondelli (2020) assumes_
to each other, i.e., xi ̸∥ **_xj for all i ̸= j._**


-----

**Lemma A.7. Given an activation function σ, if σ ∈** _L[2](R, dP_ ) and ∥xi∥ = 1 for all i ∈ [n], then


_∞_

_σ, hk_ **_XX_** _[T]_ **_XX_** _[T][ ],_
_|⟨_ _⟩|[2][  ]_ _◦· · · ◦_
_k=0_

X _k times_

| {z }


**_G[∞]_** =

where ◦ is elementwise product.

_Proof. Observe_


(31)


**_G[∞]ij_** [=][E]w _N_ (0,Id) [[][σ][(][⟨][w][,][ x][i][⟩][)][σ][(][⟨][w][,][ x][j][⟩][)]]
_∼_

_∞_

= _σ, hk_ _σ, hℓ_ Ew _N_ (0,Id) [hk( **_w, xi_** )hℓ( **_w, xj_** )]

_⟨_ _⟩⟨_ _⟩_ _∼_ _⟨_ _⟩_ _⟨_ _⟩_
_k,ℓ=0_

X

_∞_

= _σ, hk_ _σ, hℓ_ **_xi, xj_** _δkℓ_

_⟨_ _⟩⟨_ _⟩· ⟨_ _⟩[k]_
_k,ℓ=0_

X

_∞_

= _σ, hk_ **_xi, xj_**

_⟨_ _⟩[2]_ _⟨_ _⟩[k]_
_k=0_

X

Note that the tensor product of xi and xi is xi ⊗ **_xi ∈_** R[d][2][×][1], so that


**_xi, xj_** = **_xi_** **_xi, xj_** **_xj_**
_⟨_ _⟩[k]_ -  _⊗· · · ⊗_ _⊗· · · ⊗_ +

_k times_ _k times_

Here we introduce the (row-wise) Khatri–Rao product| {z } of two matrices| {z } **_A ∈_** R[k][×][m], B ∈ R[k][×][n].
Then

**_A1∗_** _⊗_ **_B1∗_**
.

**_A_** **_B =_**  ..  R[k][×][mn]
_∗_ _∈_

Ak∗ _⊗_ **_Bk∗_**
 

where Ai indicates the i-th row of matrix A. Therefore, the i-th row of X **_X ≜_** **_X_** _[∗][n]_ is
_∗_ _∗· · · ∗_
**_xi_** **_xi. As a result, we obtain a more compact form of (31) as follows_**
_⊗· · · ⊗_


_∞_

**_G[∞]_** = _σ, hk_ (X _[∗][k])(X_ _[∗][k])[T]_ _._ (32)

_|⟨_ _⟩|[2]_
_k=0_

X


**Lemma A.8. If σ(x) is a nonlinear function and |σ(x)| ≤|x| and, then**

sup _n :_ _σ, hn_ _> 0_ = _._
_{_ _⟨_ _⟩_ _}_ _∞_

_Proof. It is equivalent to show σ(x) is not a finite linear combination of polynomials. We prove_
by contradiction. Suppose σ(x) = a0 + a1x + · · · + anx[n]. Since σ(0) = 0 = a0, then σ(x) =
_a1x +_ + anx[n]. Observe that
_· · ·_

_σ(x)_ _a1x +_ + anx[n]
lim _|_ _|_ = lim _|_ _· · ·_ _|_
_x→∞_ _|x|_ _x→∞_ _|x|_

= lim _a1 +_ + anx[n][−][1] _,_
_x_ _· · ·_
_→∞_

=∞

which contradicts _[|][σ]|[(]x[x]|[)][|]_ _≤_ 1 for all x ̸= 0.

**Lemma A.9. If xi ̸∥** **_xj for all i ̸= j, then there exists k0 > 0 such that λmin_** (X _[∗][k])(X_ _[∗][k])[T][ ]_ _> 0_
for all k _k0. Therefore, λmin(G[∞]) > 0._
_≥_ 


-----

_Proof. To simplify the notation, denote K = (X_ _[∗][k])[T]_ _∈_ R[kd][×][n]. Since xi ̸∥ **_xj and ∥xi∥_** = 1, then
let δ ≜ max{|⟨xi, xj⟩|} = max{|cos θij|} and δ ∈ (0, 1), where θij is the angle between xi and
**_xj. For any unit vector v ∈_** R[n], we have


**_v[T]_** (X _[∗][k])(X_ _[∗][k])[T]_ **_v =∥Kv∥[2]_** =


_viK∗i_
_i=1_

X


_j=1_ _vivj ⟨K∗i, K∗j⟩_

X

_n_

_vivj_ **_xi, xj_**
_j=1_ _⟨_ _⟩[k]_

X


_i=1_

_n_

_i=1_

X


_vi[2]_
_i=1_ _[∥][x][i][∥][2][k][ +]_

X


_vi[2]_ _vivj_ **_xi, xj_**

_[∥][x][i][∥][2][k][ +]_ Xi≠ _j_ _⟨_ _⟩[k]_

_vivj_ **_xi, xj_** _,_

Xi≠ _j_ _⟨_ _⟩[k]_


=1 +


where the last equality is because **_xi_** = 1 and **_v_** = 1. Note that
_∥_ _∥_ _∥_ _∥_


_vivj_ **_xi, xj_**

Xi≠ _j_ _⟨_ _⟩[k]_

By inverse triangle inequality, we have


_vi_ _vj_ **_xi, xj_**

_≤_ _|_ _| |_ _| |⟨_ _⟩|[k]_
Xi≠ _j_

_δ[k][ X]_ _vi_ _vj_ _,_ by **_xi, xj_** _δ_
_≤_ _|_ _| |_ _|_ _|⟨_ _⟩| ≤_

_i≠_ _j_

_n_ 2
_δ[k]_ _vi_
_≤_ _i=1_ _|_ _|!_
X

_≤nδ[k],_ by Cauchy-Schwart’s inequlity.


_∥Kv∥[2]_ _≥_ 1 − _nδ[k]._

Choose k0 log n/ log(1/δ), then λmin (X _[∗][k])(X_ _[∗][k])[T]_ _> 0 for all k_ _k0._
_≥_ _{_ _}_ _≥_

A.5 PROOF OF LEMMA 3.3

_Proof. Since ∥xi∥_ = 1 and wr(0) ∼N (0, Id), we have x[T]i **_[w][r][(0)][ ∼]_** _[N]_ [(0][,][ 1)][ for all][ i][ ∈] [[][n][]][. Let]
_Xir ≜_ _σ_ **_x[T]i_** **_[w][r][(0)]_** and Z ∼ _N_ (0, 1), then for any |λ| ≤ 1/√2, we have
  2 2 2 2

E exp{Xir[2] _[λ][2][}][ =][ E][ exp][{][σ]_ **_x[T]i_** **_[w][r][(0)]_** _λ_ _} ≤_ E exp{Z _λ_ _} = 1/_ 1 − 2λ[2] _≤_ _e[2][t][2]_ _,_

where the first inequality is due tot _|σ(x)| ≤|_ _x|, and the last inequality is by using the numerical1/2_ p2

inequality 1/(1 − _x) ≤_ _e[2][x]. Choose λ ≤_ log _√2_, we obtain E{Xir[λ][2][} ≤] [2][. By using]

Markov’s inequality, we have for any t ≥ 0   

P {|Xir| ≥ _t} = P_ _|Xir|[2]_ _/ log_ _√2 ≥_ _t[2]/ log_ _√2_ _≤_ 2 exp _−t[2]_ log _√2_ _≤_ 2 exp _−t[2]/4_ _._
n o n o 

2018, Proposition 2.5.2). ThenTherefore Xir is a sub-Gaussian random variable with sub-Gaussian norm XirXjr is a sub-exponential random variable with sub-exponential ∥Xi∥ψ2 ≤ 2 (Vershynin,
norm _XirXjr_ _ψ1_ 4 (Vershynin, 2018, Lemma 2.7.7). Observe that
_∥_ _∥_ _≤_


**_Gij(0) = φi(0)[T]_** **_φj(0) = [1]_**


_m_

_σ_ **_x[T]i_** **_[w][r][(0)]_** _σ_ **_x[T]j_** **_[w][r][(0)]_** = [1]

_m_

_r=1_

X    


_XirXjr._
_r=1_

X


-----

Since G[∞]ij [=][ E][ [][G][ij][(0)]][, (Vershynin, 2018, Exercise 2.7.10) implies that][ G][ij][(0)][ −] **_[G]ij[∞]_** [is also a]
zero-mean sub-exponential random variable. It follows from the Bernstein’s inequality that

P **_G(0)_** **_G[∞]_** 2 P **_G(0)_** **_G[∞]_** _F_
_∥_ _−_ _∥_ _≥_ _[λ]4[0]_ _≤_ _∥_ _−_ _∥_ _≥_ _[λ]4[0]_
   

2[)]

_λ0_

=P (∥G(0) − **_G[∞]∥F[2]_** _[≥]_  4 

_n_ 2

_λ0_

=P **_Gij(0)_** **_G[∞]ij_**

 _−_ _≥_ 4 
i,jX=1   

_n_ [2] 2[)]

 _λ0_ 

_≤_ _i,j=1_ P ( **_Gij(0) −_** **_G[∞]ij_** _≥_  4n 

X

_n_

[2]

= P **_Gij(0) −_** **_G[∞]ij_** _≥_ 4[λ]n[0]

_i,j=1_ 

X 

_≤n[2]_ _· 2 exp_ _−cλ[2]0[m/n][2]_

_δ,_



_≤_

where c > 0 is some constant, and we use the facts **_X_** 2 **_X_** _F, and P_ _i=1_ _[x][i][ ≥]_ _[ε][} ≤]_
_n_ _∥_ _∥_ _≤∥_ _∥_ _{[P][n]_
_i=1_ [P][{][x][i][ ≥] _[ε/n][}][.]_
P

A.6 PROOF OF LEMMA 3.4

_Proof. By using the 1-Lipschitz continuity of σ(x), we have_


**_G_** **_G(0)_** = [1]
_∥_ _−_ _∥_ _m_ _[∥][σ][(][XW][ T][ )][σ][(][XW][ T][ )][T][ −]_ _[σ][(][XW][ (0)][T][ )][σ][(][XW][ (0)][T][ )][T][ ∥]_

_≤_ _m[1]_ _[∥][σ][(][XW][ T][ )][σ][(][XW][ T][ )][T][ −]_ _[σ][(][XW][ T][ )][σ][(][XW][ (0)][T][ )][T][ ∥]_

+ [1]

_m_ _[∥][σ][(][XW][ T][ )][σ][(][XW][ (0)][T][ )][T][ −]_ _[σ][(][XW][ (0)][T][ )][σ][(][XW][ (0)][T][ )][T][ ∥]_

= [1]

_m_ _[∥][σ][(][XW][ T][ )][∥∥][σ][(][XW][ T][ )][ −]_ _[σ][(][XW][ (0)][T][ )][∥]_

+ [1]

_m_ _[∥][σ][(][XW][ T][ )][ −]_ _[σ][(][XW][ (0)][T][ )][∥∥][σ][(][XW][ (0)][T][ )][∥]_

_≤_ _m[1]_ _[∥][X][∥∥][W][ ∥∥][X][∥∥][W][ −]_ **_[W][ (0)][∥]_** [+ 1]m _[∥][X][∥∥][W][ −]_ **_[W][ (0)][∥∥][X][∥∥][W][ (0)][∥]_**


_≤_ _√[4]m[c]_ _∥X∥[2]∥W −_ **_W (0)∥_**

_≤_ _[λ]4[0]_ _[.]_

A.7 PROOF OF LEMMA 3.5

_Proof. It suffices to show the result holds for γ = min{γ0, γ0/2c}, where γ0 = 1/2. Note that_
Lemma 2.1 still holds if one chooses a larger c. Thus, we choose c ≿ _[√]λ0/_ _X_ . We prove by the
_∥_ _∥_
induction. Suppose that for 0 ≤ _s ≤_ _t, the followings hold_


(i) λmin(G(s)) ≥ _[λ]2[0]_ [,]

(ii) **_u(s)_** _λ0_ **_yˆ(0)_** **_y_**,
_∥_ _∥≤_ [16][c][√][n] _∥_ _−_ _∥_

(iii) **_v(s)_** _λ0_ **_y(0)_** **_y_**,
_∥_ _∥≤_ [8][c][√][n] _[∥]_ [ˆ] _−_ _∥_


-----

(iv) ∥W (s)∥≤ 2c[√]m,

(v) ∥A(s)∥≤ 2c[√]m,

(vi) **_yˆ(s)_** **_y_** exp _λ0s_ **_yˆ(0)_** **_y_**,
_∥_ _−_ _∥[2]_ _≤_ _{−_ _}∥_ _−_ _∥[2]_


Since λmin(G(s)) ≥ _[λ]2[0]_ [, we have]

_d_

**_y(t)_** **_y_** = 2(ˆy(t) **_y)[T]_** **_H(t)(ˆy(t)_** **_y)_**
_dt_ _[∥]_ [ˆ] _−_ _∥[2]_ _−_ _−_ _−_


_λ0_ **_yˆ(t)_** **_y_**
_≤−_ _∥_ _−_ _∥[2]_

Solving the ordinary differential equation yields

**_yˆ(t)_** **_y_** exp _λ0t_ **_yˆ(0)_** **_y_** _._
_∥_ _−_ _∥[2]_ _≤_ _{−_ _}∥_ _−_ _∥[2]_

By using the inductive hypothesis ∥W (s)∥≤ 2c[√]m, we have

1 1
**_φi(s)_** = (W (s)xi) **_W (s)_** **_xi_** 2c.
_∥_ _∥_ _√mσ_ _[≤]_ _√m_ _∥_ _∥∥_ _∥≤_

It follows from Lemma 2.2 with γ0 = 1/2 that

_∥zi[∗][(][s][)][∥≤]_ [2][∥][φ][i][(][s][)][∥≤] [4][c.]

Note that


**_vL(s)_**
_∥∇_ _∥≤_


_yˆi(s)_ _yi_ **_φi(s)_**
_|_ _−_ _| ∥_ _∥_
_i=1_

X


2c _yˆi(s)_ _yi_
_≤_ _|_ _−_ _|_

_i=1_

X

_≤2c[√]n∥yˆ(s) −_ **_y∥_**

2c[√]n exp _λ0s/2_ **_y(0)_** **_y_**
_≤_ _{−_ _}∥_ _−_ _∥_


and so


_t_
**_v(t)_** **_v(0)_** **_vL(s)_** _ds_
_∥_ _−_ _∥≤_ 0 _∥∇_ _∥_
Z

_t_
_≤2c[√]n∥y(0) −_ **_y∥_** 0
Z

**_yˆ(0)_** **_y_** _,_

_≤_ [4][c]λ[√]0 _[n]_ _∥_ _−_ _∥_


exp _λ0s/2_ _ds_
_{−_ _}_


Since vi(0) follows symmetric Bernoulli distribution, then **_v(0)_** = _m and we obtain_
_∥_ _∥_ _[√]_

**_v(t)_** **_v(t)_** **_v(0)_** + **_v(0)_** **_yˆ(0)_** **_y_** _,_
_∥_ _∥≤∥_ _−_ _∥_ _∥_ _∥≤_ [8][c]λ[√]0 _[n]_ _∥_ _−_ _∥_

where the last inequality is due to m = Ω  _c[2]nλ∥X[3]0_ _∥[2]_ _∥yˆ(0) −_ **_y∥[2][]_** and c ≿ _[√]λ0/∥X∥._

Similarly, we have

_n_

**_uL(s)_** _yˆi(s)_ _yi_ **_zi[∗]_**
_∥∇_ _∥≤_ _i=1_ _|_ _−_ _| ∥_ _[∥]_

X

_≤4c[√]n∥yˆ(s) −_ **_y∥_**

4c[√]n exp _λ0s/2_ **_yˆ(0)_** **_y_** _,_
_≤_ _{−_ _}∥_ _−_ _∥_


-----

so that

_t_
**_u(t)_** **_u(0)_** **_uL(s)_** _ds_ **_yˆ(0)_** **_y_** _._
_∥_ _−_ _∥≤_ 0 _∥∇_ _∥_ _≤_ [8][c]λ[√]0 _[n]_ _∥_ _−_ _∥_
Z

Since ui(0) follows symmetric Bernoulli distribution, then **_u(0)_** = _m and we obtain_
_∥_ _∥_ _[√]_

**_u(t)_** **_u(t)_** **_u(0)_** + **_u(0)_** **_yˆ(0)_** **_y_** _._
_∥_ _∥≤∥_ _−_ _∥_ _∥_ _∥≤_ [16]λ[c][√]0 _[n]_ _∥_ _−_ _∥_


Note that

so that


1

_∥∇W L(s)∥≤_ _√m_ _yˆi(s) −_ _yi| ∥Ei(s)∥_ _∥Ui(s)[−][1]u(s)∥_ + ∥v(s)∥ _∥xi∥_

_i=1_ _|_

X _n_   

**_yˆ(0)_** **_y_** _._ _yˆi(s)_ _yi_

_≤_ [64]λ[c][√]0 _[n]_ _∥_ _−_ _∥_ _|_ _−_ _|_

_i=1_

X


**_yˆ(0)_** **_y_** **_yˆ(s)_** **_y_**

_≤_ [64]λ[cn]0 _∥_ _−_ _∥· ∥_ _−_ _∥_

**_yˆ(0)_** **_y_** exp _λ0s/2_ _,_

_≤_ [64]λ[cn]0 _∥_ _−_ _∥[2]_ _·_ _{−_ _}_


_t_
_∥W (t) −_ **_W (0)∥≤_** 0 _∥∇W L(s)∥ds_
Z

_≤_ _λ[128][2]0√[cn]√mm∥yˆ(0) −_ **_y∥[2]_**

_≤_ 16[λ]c[0] **_X_**

_∥_ _∥[2]_

_≤R._


Therefore, we obtain


_∥W (t)∥≤∥W (t) −_ **_W (0)∥_** + ∥W (0)∥≤ 2c[√]m,

Moreover, it follows from Lemma 3.4 that λmin{G(t)} ≥ _[λ]2[0]_ [.]

Note that


_γ_

**_AL(s)_** _yˆi(s)_ _yi_ **_Di_** **_Ui(s)[−][1]_** **_u(s)_** **_zi[∗]_**
_∥∇_ _∥≤_ _i=1_ _√m |_ _−_ _| ∥_ _∥∥_ _∥∥_ _∥∥_ _[∥]_

X _n_

**_yˆ(0)_** **_y_** _yˆi(s)_ _yi_

_≤_ [32]λ0[c]√[√]m[n] _−_ _∥·_ _|_ _−_ _|_

_∥_ _i=1_

X

_≤_ _λ[32]0√[cn]m_ _∥yˆ(0) −_ **_y∥· ∥yˆ(s) −_** **_y∥_**


**_yˆ(0)_** **_y_** exp _λ0s/2_ _,_

_≤_ _λ[32]0√[cn]m_ _∥_ _−_ _∥[2]_ _·_ _{−_ _}_


so that

Then


_t_
**_A(t)_** **_A(0)_** **_AL(s)_** _ds_
_∥_ _−_ _∥≤_ 0 _∥∇_ _∥_
Z

_≤_ _λ[64][2]0√[cn]m_ _∥yˆ(0) −_ **_y∥[2]._**

_∥A(t)∥≤∥A(t) −_ **_A(0)∥_** + ∥A(0)∥≤ 2c[√]m.


-----

A.8 PROOF OF LEMMA 3.6

In this section, we prove the result for discrete time analysis or result for gradient descent. Assume
_∥A(0)∥≤_ _c[√]m and ∥W (0)∥≤_ _c[√]m. Further, we assume λmin(G(0)) ≥_ 4[3] _[λ][0][ and we assume]_

_m = Ω_ _c[2]nλ∥X[3]0_ _∥[2]_ _∥yˆ(0) −_ **_y∥[2][]_** and choose 0 < γ ≤ min{1/2, 1/4c}. Moreover, we assume the

stepsize α = _λ0/n[2][]. We make the inductive hypothesis as follows for all 0_ _s_ _k_
_O_ _≤_ _≤_
 

(i) λmin(G(s)) ≥ _[λ]2[0]_ [,]

(ii) **_u(s)_** _λ0_ **_yˆ(0)_** **_y_**,
_∥_ _∥≤_ [32][c][√][n] _∥_ _−_ _∥_

(iii) **_v(s)_** _λ0_ **_yˆ(0)_** **_y_**,
_∥_ _∥≤_ [16][c][√][n] _∥_ _−_ _∥_

(iv) ∥W (s)∥≤ 2c[√]m,

(v) ∥A(s)∥≤ 2c[√]m,

(vi) **_yˆ(s)_** **_y_** (1 _αλ0/2)[s]_ **_yˆ(0)_** **_y_** .
_∥_ _−_ _∥[2]_ _≤_ _−_ _∥_ _−_ _∥[2]_

_Proof. Note that Lemma 2.1 still holds if one chooses a larger c. Thus, we choose c ≿_ _[√]λ0/_ _X_ .
_∥_ _∥_
By using the inductive hypothesis, we have for any 0 ≤ _s ≤_ _k_

1
**_φi(s)_** = (W (s)xi) **_W (s)_** 2c
_∥_ _∥_ _∥_ _√[1]mσ_ _∥≤_ _√m_ _∥_ _∥≤_

and


_n_ 1/2

**_φi(s)_**
_i=1_ _∥_ _∥[2]!_

X


_≤_ 2c[√]n. (33)


_∥Φ(s)∥≤∥Φ(s)∥F =_


By using Lemma 2.2, we obtain the upper bound for the equilibrium point zi(s) for any 0 _s_ _k_
_≤_ _≤_
as follows


**_zi(s)_**
_∥_ _∥≤_


**_φi(s)_** = 2 **_φi(s)_** 4c,
1 _γ0_ _∥_ _∥_ _∥_ _∥≤_
_−_


where the last inequality is because we choose γ0 = 1/2, and


_n_ 1/2

**_zi(s)_**
_i=1_ _∥_ _∥[2]!_

X


= 4c[√]n. (34)


_∥Z(s)∥≤∥Z(s)∥F =_


By using the upper bound of φi(s), we obtain for any 0 _s_ _k_
_≤_ _≤_


**_vL(s)_**
_∥∇_ _∥≤_


_yˆi(s)_ _yi_ **_φi(s)_**
_|_ _−_ _| ∥_ _∥_
_i=1_

X


2c _yˆi(s)_ _yi_
_≤_ _|_ _−_ _|_

_i=1_

X

_≤2c[√]n∥yˆ(s) −_ **_y∥_**

2c[√]n(1 _αλ0/2)[s/][2]_ **_yˆ(0)_** **_y_** _._
_≤_ _−_ _∥_ _−_ _∥_


Let β ≜


1 _αλ0/2. Then the upper bound of_ **_vL(s)_** can be written as
_−_ _∥∇_ _∥_

**_vL(s)_** 2c[√]nβ[s] **_yˆ(0)_** **_y_** _,_ (35)
_∥∇_ _∥≤_ _∥_ _−_ _∥_


-----

and


_∥v(k + 1) −_ **_v(0)∥≤_**


_∥v(s + 1) −_ **_v(s)∥_** = α
_s=0_

X


**_vL(s)_**
_∥∇_ _∥_
_s=0_

X


_α_ 2c[√]n **_yˆ(0)_** **_y_**
_≤_ _·_ _∥_ _−_ _∥·_


_β[s]_

_s=0_

X


= [2(1][ −] _[β][2][)]_ 2c[√]n **_yˆ(0)_** **_y_**

_λ0_ _·_ _∥_ _−_ _∥_ [1][ −]1 _[β][k]β[+1]_

_−_

**_yˆ(0)_** **_y_** _,_

_≤_ [8][c]λ[√]0 _[n]_ _∥_ _−_ _∥_

where the last inequality we use the facts β < 1. By triangle inequality, we obtain

**_v(k + 1)_** **_v(k + 1)_** **_v(0)_** + **_v(0)_** **_yˆ(0)_** **_y_** _,_
_∥_ _∥≤∥_ _−_ _∥_ _∥_ _∥≤_ [16]λ[c][√]0 _[n]_ _∥_ _−_ _∥_

which proves the result (iii). Similarly, we can upper bound the gradient of u


_n_

_yˆi(s)_ _yi_ **_zi_** 4c[√]n **_yˆ(s)_** **_y_** 4c[√]nβ[s] **_yˆ(0)_** **_y_** (36)
_|_ _−_ _| ∥_ _∥≤_ _∥_ _−_ _∥≤_ _∥_ _−_ _∥_
_i=1_

X


**_uL(s)_**
_∥∇_ _∥≤_


so that

and


**_u(k + 1)_** **_u(0)_** **_yˆ_** **_y_** _,_
_∥_ _−_ _∥≤_ [16]λ[c][√]0 _[n]_ _∥_ _−_ _∥_

**_u(k)_** **_u(k)_** **_u(0)_** + **_u(0)_** **_yˆ(0)_** **_y_** _._
_∥_ _∥≤∥_ _−_ _∥_ _∥_ _∥≤_ [32]λ[c][√]0 _[n]_ _∥_ _−_ _∥_


The result (ii) is also obtained.

By using the inductive hypothesis, we can upper bound the gradient of W as follows


1

_∥∇W L(s)∥≤_ _√m_ _yˆi(s) −_ _yi| ∥Ei(s)∥_ _∥Ui(s)[−][1]u(s)∥_ + ∥v(s)∥ _∥xi∥_

_i=1_ _|_

X _n_   

**_yˆ(0)_** **_y_** _yˆi(s)_ _yi_

_≤_ [128]λ0√[c][√]m[n] _−_ _∥_ _|_ _−_ _|_

_∥_ _i=1_

X

_≤_ _λ[128]0√[cn]m_ _∥yˆ(0) −_ **_y∥· ∥yˆ(s) −_** **_y∥_**


_≤_ _λ[128]0√[cn]m_ _∥yˆ(0) −_ **_y∥[2]_** _· β[s],_ (37)


so that


_∥W (k + 1) −_ **_W (0)∥≤α_**


_∥∇W L(s)∥_
_s=0_

X


_≤α ·_ _λ[128][2]0√[cn]m_ _∥yˆ(0) −_ **_y∥[2]_** _·_

_≤_ _λ[512]√[2]0√mλ[cn]m_ _∥0yˆ(0) −_ **_y∥[2]_**


_β[s]_

_s=0_

X


_≤_ 16c **_X_**

_∥_ _∥[2]_

_≤R,_


-----

where the third inequality holds is because m is large, i.e., m = Θ _c[2]nλ∥X[3]0_ _∥[2]_ _∥yˆ(0) −_ **_y∥[2][]. To_**

simplify the notation, we assume 

_m =_ _[Cc][2][n][∥][X][∥][2]_ **_yˆ(0)_** **_y_** (38)

_λ[3]0_ _∥_ _−_ _∥[2]_

for some large number C > 0. Moreover, we obtain

_∥W (k + 1)∥≤∥W (k + 1) −_ **_W (0)∥_** + ∥W (0)∥≤ 2c[√]m,

Therefore, it follows from Lemma 3.4 that λmin{G(k + 1)} ≥ _[λ]2[0]_ [. Thus, the results (i) and (iv) are]

established.

By using similar argument, we can upper bound the gradient of A as follows Note that


_γ_

**_AL(s)_** _yˆi(s)_ _yi_ **_Di_** **_Ui(s)[−][1]_** **_u(s)_** **_zi[∗]_**
_∥∇_ _∥≤_ _i=1_ _√m |_ _−_ _| ∥_ _∥∥_ _∥∥_ _∥∥_ _[∥]_

X _n_

**_yˆ(0)_** **_y_** _yˆi(s)_ _yi_

_≤_ [64]λ0[c]√[√]m[n] _−_ _∥·_ _|_ _−_ _|_

_∥_ _i=1_

X

_≤_ _λ[64]0√[cn]m_ _∥yˆ(0) −_ **_y∥· ∥yˆ(s) −_** **_y∥_**


_≤_ _λ[64]0√[cn]m_ _∥yˆ(0) −_ **_y∥[2]_** _· β[s],_


so that


_∥A(k + 1) −_ **_A(0)∥≤α_**


**_AL(s)_**
_∥∇_ _∥_
_s=0_

X


_≤α ·_ _λ[64]0√[cn]m_ _∥yˆ(0) −_ **_y∥[2]_** _·_


_β[s]_

_s=0_

X


_≤_ _λ[256][2]0√[cn]m_ _∥yˆ(0) −_ **_y∥[2]._**

Since m = _[Cc][2][n]λ[∥][3]0[X][∥][2]_ _∥yˆ(0) −_ **_y∥[2]_** and c, C > 0 are large enough, we have

_∥A(k + 1)∥≤∥A(k + 1) −_ **_A(0)∥_** + ∥A(0)∥≤ 2c[√]m.


Therefore, the result (v) is obtained and the equilibrium points zi(k + 1) exists for all i [n].
_∈_

To establish the result (vi), we need to derive the bounds between equilibrium points Z(k) and
feature vectors Φ(k). We firstly bound the difference between equilibrium points zi(k + 1) and
**_zi(k). For any ℓ_** 1, we have
_≥_

**_zi[ℓ][+1](k + 1)_** **_zi[ℓ][+1](k)_** = _σ_ _γ˜A(k + 1)zi[ℓ][(][k][ + 1) +][ φ][i][(][k][ + 1)]_ _σ_ _γ˜A(k)zi[ℓ][(][k][) +][ φ][i][(][k][)]_
_∥_ _−_ _∥_ _∥_ _−_ _∥_

_γ˜A_ (k + 1)zi[ℓ][(][k][ + 1) +][ φ][i][(][k][ + 1)][ −] γ[˜]A(k)zi[ℓ][(][k][)][ −] **_[φ][i][(][k][)][∥]_** 
_≤∥_

_≤γ˜∥A(k + 1)zi[ℓ][(][k][ + 1)][ −]_ **_[A][(][k][)][z]i[ℓ][(][k][)][∥]_** [+][ ∥][φ][i][(][k][ + 1)][ −] **_[φ][i][(][k][)][∥][,]_**


where the first term can be bounded as follows

_γ˜∥A(k + 1)zi[ℓ][(][k][ + 1)][ −]_ **_[A][(][k][)][z]i[ℓ][(][k][)][∥]_**

_≤γ˜∥A(k + 1) −_ **_A(k)∥∥zi[ℓ][(][k][ + 1)][∥]_** [+ ˜]γ∥A(k)∥∥zi[ℓ][(][k][ + 1)][ −] **_[z]i[ℓ][(][k][)][∥]_**

_≤γα˜_ _∥∇AL(k)∥(4c) + ˜γ∥A(k)∥∥zi[ℓ][(][k][ + 1)][ −]_ **_[z]i[ℓ][(][k][)][∥]_**

**_y(0)_** **_y_** _β[k]_ + (1/2) **_zi[ℓ][(][k][ + 1)][ −]_** **_[z]i[ℓ][(][k][)][∥][,]_**

_≤_ [64]λ0[αcn]m _−_ _∥[2]_ _∥_

_[∥]_ [ˆ]


-----

and the second term is bounded as follows

**_φi(k + 1)_** **_φi(k)_**
_∥_ _−_ _∥_

= [1] _σ[W (k + 1)xi]_ _σ[W (k)xi]_
_√m_ _∥_ _−_ _∥_

**_W (k + 1)_** **_W (k)_** **_xi_**
_≤_ _√[1]m_ _∥_ _−_ _∥∥_ _∥_

_≤_ _√[α]m_ _∥∇W L(k)∥_


**_y(0)_** **_y_** _β[k]._

_≤_ [128]λ0[αcn]m _−_ _∥[2]_ _·_

_[∥]_ [ˆ]


Thus, we obtain


_∥zi[ℓ][+1](k + 1) −_ **_zi[ℓ][+1](k)∥≤(1/2)∥zi[ℓ][(][k][ + 1)][ −]_** **_[z]i[ℓ][(][k][)][∥]_** [+ 256]λ0[αcn]m **_y(0) −_** **_y∥[2]_** _· β[k]_

_[∥]_ [ˆ]

(1/2)[ℓ] **_zi[1][(][k][ + 1)][ −]_** **_[z]i[1][(][k][)][∥]_** [+ 256][αcn] **_y(0)_** **_y_** _β[k]_
_≤_ _∥_ _λ0m_ _−_ _∥[2]_ _·_ _·_

_[∥]_ [ˆ]

(1/2)[ℓ] **_zi[1][(][k][ + 1)][ −]_** **_[z]i[1][(][k][)][∥]_** [+ 512][αcn] **_y(0)_** **_y_** _β[k]._
_≤_ _∥_ _λ0m_ _−_ _∥[2]_ _·_

_[∥]_ [ˆ]

By letting ℓ _→∞, we obtain_

**_zi(k + 1)_** **_zi(k)_** **_y(0)_** **_y_** _β[k]._
_∥_ _−_ _∥≤_ [512]λ0[αcn]m _−_ _∥[2]_ _·_

_[∥]_ [ˆ]

By using the Cauchy-Schwartz’s inequality, we have


2[−][j]

_j=0_

X


**_Z(k + 1)_** **_Z(k)_** **_Z(k + 1)_** **_Z(k)_** _F_ **_yˆ(0)_** **_y_** _β[k]._ (39)
_∥_ _−_ _∥≤∥_ _−_ _∥_ _≤_ [512]λ[αcn]0m[3][/][2] _∥_ _−_ _∥[2]_ _·_

In addition, we will also bound the difference in φi(k + 1) and φi(k). Note that


1
**_φi(k + 1)_** **_φi(k)_** = _σ[W (k + 1)xi]_ _σ[W (k)xi]_ **_y(0)_** **_y_** _β[k],_
_∥_ _−_ _∥_ _√m_ _∥_ _−_ _∥≤_ [128]λ0[αcn]m _−_ _∥[2]_ _·_

_[∥]_ [ˆ]

so that

_∥Φ(k + 1) −_ **Φ(k)∥≤∥Φ(k + 1) −** **Φ(k)∥F ≤** [128]λ[αcn]0m[3][/][2] _∥yˆ(0) −_ **_y∥[2]_** _· β[k]_ (40)

Now, we are ready to establish the result (vi). Note that

_∥yˆ(k + 1) −_ **_y∥[2]_** =∥yˆ(k + 1) − **_yˆ(k) + ˆy(k) −_** **_y∥[2]_**

=∥yˆ(k + 1) − **_yˆ(k)∥[2]_** + 2 ⟨yˆ(k + 1) − **_yˆ(k), ˆy(k) −_** **_y⟩_** + ∥yˆ(k) − **_y∥[2]._**

In the rest of this proof, we will bound each term in the above inequality. By the prediction rule of
**_yˆ, we can bound the difference between ˆy(k + 1) and ˆy(k) as follows_**

_∥yˆ(k + 1) −_ **_yˆ(k)∥_** =∥Z(k + 1)u(k + 1) + Φ(k + 1)v(k + 1) − **_Z(k)u(k) −_** **Φ(k)v(k)∥**
_≤∥Z(k + 1)u(k + 1) −_ **_Z(k)u(k)∥_** + ∥Φ(k + 1)v(k + 1) − **Φ(k)v(k)∥,**

where the first term can be bounded as follows by using (34), 36, 38, 39, hypothesis (ii), and a large
constant C0 > 0


_∥Z(k + 1)∥∥u(k + 1) −_ **_u(k)∥_** + ∥Z(k + 1) − **_Z(k)∥∥u(k)∥_**
=α **_Z(k + 1)_** **_uL(k)_** + **_Z(k + 1)_** **_Z(k)_** **_u(k)_**
_∥_ _∥∥∇_ _∥_ _∥_ _−_ _∥∥_ _∥_

_αC0c[2]n_ **_yˆ(0)_** **_y_** _β[k],_
_≤_ _∥_ _−_ _∥·_


-----

and the second term is bounded as follows by using (33), 35, 40, 38, hypothesis (iii), and a large
constant C0 > 0

_∥Φ(k + 1)∥∥v(k + 1) −_ **_v(k)∥_** + ∥Φ(k + 1) − **Φ(k)∥∥v(k)∥**
=α **Φ(k + 1)** **_vL(k)_** + **Φ(k + 1)** **Φ(k)** **_v(k)_**
_∥_ _∥∥∇_ _∥_ _∥_ _−_ _∥∥_ _∥_

_αC0c[2]n_ **_yˆ(0)_** **_y_** _β[k]._
_≤_ _∥_ _−_ _∥·_

Therefore, we have


**_yˆ(k + 1)_** **_yˆ(k)_** _αC0c[2]n_ **_yˆ(0)_** **_y_** _β[k],_ (41)
_∥_ _−_ _∥≤_ _∥_ _−_ _∥·_

where the scalar 2 is absorbed in C0 and the constant C0 is difference from C.

Let g ≜ **_Z(k)u(k + 1) + Φ(k)v(k + 1). Then we have_**


_⟨yˆ(k + 1) −_ **_yˆ(k), ˆy(k) −_** **_y⟩_** = ⟨yˆ(k + 1) − **_g, ˆy(k) −_** **_y⟩_** + ⟨g − **_yˆ(k), ˆy(k) −_** **_y⟩_** _._

Let us bound each term individually. By using the Cauchy-Schwartz inequality, we have

_⟨yˆ(k + 1) −_ **_g, ˆy(k) −_** **_y⟩_**
= ⟨(Z(k + 1) − **_Z(k))u(k + 1), ˆy(k) −_** **_y⟩_** + ⟨(Φ(k + 1) − **Φ(k))v(k + 1), ˆy(k) −** **_y⟩_**
_≤_ (∥Z(k + 1) − **_Z(k)∥∥u(k + 1)∥_** + ∥Φ(k + 1) − **Φ(k)∥∥v(k + 1)∥) ∥yˆ(k) −** **_y∥_**

_αC0c[2]n_ **_yˆ(0)_** **_y_** _β[k]_ **_yˆ(k)_** **_y_** _,_ by (34), 36, 38, 39
_≤_ _∥_ _−_ _∥·_ _∥_ _−_ _∥_

_αC0c[2]n_ _β[2][k]_ **_yˆ(0)_** **_y_** _._ (42)
_≤_ _·_ _∥_ _−_ _∥[2]_

By using **_uL(k) = Z(k)[T]_** (ˆy(k) **_y),_** **_vL(k) = Φ(k)[T]_** (ˆy(k) **_y) and λmin(G(k))_** _λ0/2,_
_∇_ _−_ _∇_ _−_ _≥_
we get

_⟨g −_ **_yˆ(k), ˆy(k) −_** **_y⟩_** = −α(ˆy(k) − **_y)[T][ ]Z(k)Z(k)[T]_** + Φ(k)Φ(k)[T][ ] (ˆy(k) − **_y)_**

**_y(k)_** **_y_** _._ (43)

_≤−_ _[αλ]2_ [0] _[∥]_ [ˆ] _−_ _∥[2]_

By combining the inequalities (41), 42, 43, we obtain


**_yˆ(k + 1)_** **_y_** 1 _α_ _λ0_ _C0c[2]n_ _αC0[2][c][4][n][2][]_ _β[2][k]_ **_yˆ(0)_** **_y_**
_∥_ _−_ _∥[2]_ _≤_ _−_ _−_ _−_ _∥_ _−_ _∥[2]_
  1  _β[2][k]_ **_yˆ(0)_** **_y_**

_≤_ _−_ _[αλ]2_ [0] _·_ _∥_ _−_ _∥[2]_
 

_k+1_

= 1 **_yˆ(0)_** **_y_** _,_
_−_ _[αλ]2_ [0] _∥_ _−_ _∥[2]_
 

where the second inequality is by α = _λn0[2]_ . This proves the result (vi) and complete the proof.
_O_
  


-----

