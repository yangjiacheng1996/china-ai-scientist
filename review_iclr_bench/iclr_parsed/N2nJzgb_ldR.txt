# FASTRPB: A SCALABLE RELATIVE POSITIONAL EN## CODING FOR LONG SEQUENCE TASKS

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Transformers achieve remarkable performance in various domains, including
NLP, CV, audio processing, and graph analysis. However, they do not scale well
on long sequence tasks due to their quadratic complexity w.r.t. the input’s length.
Linear Transformers were proposed to address this limitation. However, these
models have shown weaker performance on the long sequence tasks comparing to
the original one. In this paper, we explore Linear Transformer models, rethinking their two core components. Firstly, we improved Linear Transformer with
**Shift-Invariant Kernel Function SIKF, which achieve higher accuracy without**
loss in speed. Secondly, we introduce FastRPB which stands for Fast Relative
**Positional Bias, which efficiently adds positional information to self-attention us-**
ing Fast Fourier Transformation. FastRPB is independent of the self-attention
mechanism and can be combined with an original self-attention and all its efficient variants. FastRPB has O(N log N ) computational complexity, requiring
_O(N_ ) memory w.r.t. input sequence length N .
We compared introduced modifications with recent Linear Transformers in different settings: text classification, document retrieval, and image classification.
Extensive experiments with FastRPB and SIKF demonstrate that our model significantly outperforms another efficient positional encodings method in accuracy,
having up to x1.5 times higher speed and requiring up to x10 times less memory
than the original Transformer.

1 INTRODUCTION

Transformer architecture (Vaswani et al., 2017) originally proposed for machine translation tasks
has shown impressive results in a wide range of domains, including natural language processing,
image recognition, audio captioning, graph analysis, and bioinformatics (Lin et al., 2021). However, in applications that require processing long sequences, the benefits of transformers are often
accompanied by high consumption of computational and memory resources. The main bottleneck
is the transformer’s core component, the self-attention mechanism. Self-attention computes similarity scores for all pairs of tokens in the input sequence, and therefore, it has a quadratic complexity
_O(N_ [2]) in computations and memory relative to the length of the input sequence N [1].

Recently, several approaches have been introduced to reduce the computational complexity and
memory footprint of self-attention. Some works utilize the sparsity of the attention map (Beltagy et al., 2020), others express self-attention as a linear dot-product of kernel feature maps
_φ(·) (Katharopoulos et al., 2020), or utilize random feature vectors (Choromanski et al., 2020). Pro-_
posed approaches reduce the computational complexity to O(N )[2]. One of the promising variants of
a transformer is the Linear Transformer (Katharopoulos et al., 2020) since, along with linear complexity, it requires constant O(1) memory in auto-regressive language modeling. Experiments with
the long sequence benchmark Long Range Arena (LRA) (Tay et al., 2020)[3] have indeed shown that

1The full complexity of self-attention also depends on attention head size D. For the original self-attention,
complexity is2In contrast, for O(N Linear Transformer[2]D) (Katharopoulos et al., 2020; Choromanski et al., 2020), the complexity of linear self-attention is O(ND[2]). In long sentences, N is assumed to be around thousands of tokens.
Therefore, switching to linear self-attention appears beneficial.
3In benchmark sequences ranging from 1K to 16K tokens


-----

_wi_ AAN -3600 -2800 -2000 -1200 -400 400 1200 2000 2800 3600 00..0201

ListOps 0.00

-1800 -1400 -1000 -600 -200 200 600 1000 1400 1800 _−0.01_

Assigned weight, TC _−0.02_

-900 -700 -500 -300 -100 100 300 500 700 900

Relative distance between tokens, i


10[−][4]

0 _×_

1.4

2

4 1.2

6

8 1.0

10

12 0.8

14

16 0.6

Vertical coordinate18

20 0.4

22

24 0.2

26

0 2 4 6 8 10 12 14 16 18 20 22 24 26

Horizontal coordinate


Figure 1: Learned weights wi assigned to pairwise distances between tokens i in FastRPB 1D
for different text LRA tasks.


Figure 2: Learned FastRPB 2D weights assigned
to distances from pixel (12, 10) to each other
pixel in MNIST 28 × 28 image classification.


the Linear Transformer is 5x times faster than the vanilla Transformer in training speed. However,
the drawback of this architecture is lower performance compared to the original Transformer.

One way to reduce the performance gap between the Linear Transformer and the original one is
to select a more suitable kernel function φ(·) in linear attention (Choromanski et al., 2020; Schlag
et al., 2021). The poor performance of efficient transformers on LRA can also be attributed to
the model’s ability to capture positional information. The original Transformer model utilizes only
absolute positional information, which is added through positional embeddings to contextual embeddings of the tokens. Other approaches, which enrich self-attention with additional information
about relative distances between tokens, have recently shown visible improvements in performance.
Some of them directly add a matrix of relative distances to the attention map (Shaw et al., 2018),
others compute separate attention scores between positional embeddings (He et al., 2020). We hypothesize that adding relative positional information could improve efficient transformers. However,
most of the current implementations possess quadratic computational complexity, which neutralizes
all efficiency of the Linear Transformer. To deal with this problem, a linear complexity stochastic
positional encoding (SPE) was proposed (Liutkus et al., 2021). Despite linear asymptotic, SPE remains relatively inefficient in training time due to its stochastic nature, while the improvement in
accuracy it brings is relatively small on several LRA tasks.

The contribution of this paper is two-fold. At first, we propose the Shift-Invariant Kernel Function
(SIKF). It could be used as a kernel for the Linear Transformer model and holds the shift-invariance
property of softmax in the original attention. Second, we propose Fast Relative Positional Bias
(FastRPB) — Fast Fourier Transform-based bias for self-attention that represents relative positional information within sequences, has O(N log N ) complexity and requires only O(N ) memory.
FastRPB is orthogonal to the self-attention mechanism and can be combined with both efficient and
original implementations.

We observed that SIKF is comparable to more complex kernels (Choromanski et al., 2020; Schlag
et al., 2021) while being as fast as the original one (Katharopoulos et al., 2020). We also evaluated
FastRPB under different long-context scenarios, such as image classification and Long Range Arena
tasks. Through a comprehensive study, we showed that the proposed technique outperforms the
prior fast positional encoding method (Liutkus et al., 2021) by a significant margin without adding
a substantial computational footprint.

2 RECENT WORKS


2.1 ATTENTION MECHANISM

The core component of Transformer (Vaswani et al., 2017) is the attention layer, which computes
attention weights Am,n, measuring how important the role of n-th key word is in shaping the meaning of m-th output word. Using Am,n we can construct attention matrix A ∈ R[M] _[×][N]_, and rewrite


-----

50

20


800

600


400

200


500 1000 1500 2000 2500

|Origi Linea Linea Perfo|nal r (SIKF/ELU) r (DPFP) rmer|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
|Origi Linea Linea|nal, FastRPB r (SIKF/ELU), Fast r (DPFP), FastRPB|RPB|||||
|Perfo|rmer, FastRPB||||||
||||||||
||||||||
||||||||


Original
Linear (SIKF/ELU)
Linear (DPFP)
Performer
Original, FastRPB
Linear (SIKF/ELU), FastRPB
Linear (DPFP), FastRPB
Performer, FastRPB

Num pixels

(a) Evaluation memory consumption.


256 576 1024 1600 2304 2500

|Col1|Col2|Col3|Col4|Col5|Original Linear (SIKF)|Col7|Col8|
|---|---|---|---|---|---|---|---|
||||||Linear (ELU) Linear (DPFP) Performer Original, Fast|RPB||
||||||Linear (SIKF), Linear (ELU), Linear (DPFP) Performer, Fa|FastRP FastRP, FastR stRPB|B B PB|
|||||||||
|||||||||


Original
Linear (SIKF)
Linear (ELU)
Linear (DPFP)
Performer
Original, FastRPB
Linear (SIKF), FastRPB
Linear (ELU), FastRPB
Linear (DPFP), FastRPB
Performer, FastRPB

Num pixels

(b) Evaluation time.


Figure 3: Evaluation time and memory for various type of transformer on Nvidia A100 with respect
to number of pixels in the input image. For memory consumption y-axis is log-scaled.

the equation using matrix notation. The output of the attention layer Y is defined based on three
matrices Q ∈ R[M] _[×][D], K ∈_ R[N] _[×][D]_ and V ∈ R[N] _[×][D]_ (Queries, Keys, and Values) as follows:

**_Y = AV = softmax(A)V = softmax(QK[T]_** _/√D)V_ (1)

In vanilla Transformer, the attention matrix A is computed explicitly, which leads to a O(MND)
complexity, and O(MN ) memory to store the matrix[4].


2.2 EFFICIENT ATTENTION MECHANISM

_Linear Transformer variants (Katharopoulos et al., 2020; Choromanski et al., 2020) are a way to_
reduce the complexity of attention from quadratic to linear using the associative property of matrix
products and kernel reformulation of attention.


By substituting the softmax function in Equation 1, we obtain m-th row ym of matrix Y :


_√_
_n_ [exp(][q]m[T] **_[k][n][/]_**

_n_ [exp(][q]m[T] **_[k][n][/]_**

P


_n_ [sim][(][q][m][,][ k][n][)][v][n] (2)

_n_ [sim][(][q][m][,][ k][n][)]

P


_√DD)v)n_


**_ym =_**


where exp(qm[T] **_[k][n][/]_**


_D) is generalized by any arbitrary defined similarity function sim(qm, kn)._


The core idea of Linear Transformer is to replace sim(qm, kn) with a dot-product using kernel
function φ(·) and then use an associative property of matrix products as follows:

_n_ _[φ][(][q][m][)][T][ φ][(][k][n][)][v][n]_ _n_ _[φ][(][k][n][)][v][n]_

**_ym =_** = _[φ][(][q][m][)][T][ P]_ (3)
P _n_ _[φ][(][q][m][)][T][ φ][(][k][n][)]_ _φ(qm)[T][ P]n_ _[φ][(][k][n][)]_

Original attention has (N [2]DP) time complexity, where N represents the sequence length, and
_O_
_O(N_ [2]) for the memory footprint, while linear attention has time and memory complexity O(ND[2]),
which scales linearly with sequence length N .

2.3 KERNEL FUNCTION VARIANTS


An open question is selecting an appropriate kernel function for Linear Transformer since different
kernel functions dramatically affect trained model accuracy and speed.

**ELU + 1. Originaly proposed kernel is an element-wise ELU(·) + 1 (Katharopoulos et al., 2020):**

_x + 1,_ _x > 0_
_φ(x) = ELU(x) + 1 =_ (4)
exp(x), _x_ 0
 _≤_

The choice of ELU(·)+1 over ReLU(·) was prompted by its non-zero gradients for negative values.

is O4(In case of self-attention,N [2]). _M equals to N_, and thus the complexity is O(N 2D) and memory requirements


-----

**Performer. The core idea is to approximate the softmax on average using random features (Choro-**
manski et al., 2020). The kernel function is evaluated as:


_φ(x) =_ _[h][(][x][)]_
_√m_


(5)

2 [exp] _−_ 2[1] _[∥][x][∥]_
 


exp(Rx)
exp(−Rx)


_, where h(x) =_


exp(Rx)
Here states for concatenation of vectors exp(Rx) and exp( **Rx) along feature di-**
exp( **Rx)** _−_
 _−_ 

mension, each row r ∈ R[D] of matrix R ∈ R[R][×][D] is sampled from normal distribution N (0, ID),
and dimension size R is a hyperparameter.

The main drawback of Performer is that sampling of matrix R requires extra computations and
introduces variance into the model’s output.

**DPFP. Deterministic parameter-free projection is an alternative approach (Schlag et al., 2021). The**
kernel function, designed to facilitate orthogonality in the projected space R[D][proj], is described as:

**_x_** **_x_**
_φi_ _ν(x) = ReLU_ ReLU _, where φ : R[D]_ R[D][proj] (6)

_·_ **_x_** _i_ **_x_** _i+ν_ _→_
−  − 

here i _·_ _ν indicates the index of vector φ(x), i ∈{1, 2, ..., 2D} is an index and ν ∈{1, 2, ..., 2D_ _−_ 1}
is a hyperparameter, controlling the capacity of kernel function φ(·). Linear Transformer with
DPFP model outperforms model with default kernel and Performer, even if Dproj is relatively small.
Also, DPFP showed to be faster than models utilizing random features, but still slightly slower than
ELU + 1.

2.4 POSITIONAL INFORMATION

Attention is permutation-invariant, which means that the attention layer does not make use of the
order of the sequence. There exist different ways to encode positional information in the attention:

**Absolute Positional Encoding (APE) proposed in original Transformer architecture uses real-**
valued vectorformer (Vaswani et al., 2017), use predefined vectors, while others employ learnable vectors, e.g., pi ∈ R[D] assigned to each positions i. Some approaches, such as vanilla Transin BERT (Devlin et al., 2018).

**Relative Positional Encoding (RPE) is complement to the absolute positional encoding, which**
explicitly adds relative positional information between vectors (Shaw et al., 2018) to the model.
Raffel et al. (2019) proposed to directly embed positional information to the matrix A (see the
Equation 1). This approach was then improved by separating semantic correlation of words and
their positions correlation by Ke et al. (2020). The component Am,n of matrix A then calculated as:


(UQpm)[T] (UKpn) (7)


**_qm[T]_** **_[k][n]_** [+]


_Am,n =_


wherelearnable projection matrices for the positional embedding. pn and pm are embeddings of corresponding positions n and m, and UQ, UK ∈ R[D][×][D] are

By design, these approaches have quadratic computational complexity. Thus their usage with Linear
_Transformer is challenging since the naive application will neutralize all effectiveness of linear_
computation time.

**Stochastic Positional Encoding (SPE) proposed by Liutkus et al. (2021) is, to the best of our**
knowledge, currently the only positional encoding method compatible with Linear Transformer
variants due to its linear complexity. The key idea for SPE is to represent the attention relative
distances matrix as a covariance. Following the notation from equation 1, we can express Am,n as:


_D, where Pd(m, n) = E_


qd(m) kd(n) (8)
_·_



_Am,n =_


_Qm,d_ _d(m, n)_ _Kn,d/_
_d=1_ _· P_ _·_

X


here Qm,d and Kn,d are components of matrices Q and K respectively. qd(m) and kd(n) are
two real and zero-mean random variables such that their covariance function matches _d. Varying_
_P_
the structure of matrices Pd authors designed two variants of SPE: sinSPE and convSPE. The first


-----

Positional Encoding

None FastRPB sinSPE convSPE RPE

|AAN|Original Linear, DPFP Linear, SIKF Linear, ReLU Performer|OOM OOM OOM OOM OOM 61.01 ± 0.79 64.79 ± 1.52 61.53 ± 0.75 63.52 ± 0.71 N/A 59.51 ± 0.3 67.19 ± 1.64 62.0 ± 0.36 58.93 ± 1.65 N/A 58.78 ± 0.93 64.94 ± 1.6 62.39 ± 0.59 61.00 ± 1.34 N/A 59.84 ± 1.46 66.65 ± 0.91 60.00 ± 1.20 57.22 N/A|
|---|---|---|


|ListOps|Original Linear, DPFP Linear, SIKF Linear, ReLU Performer|14.43 ± 4.73 14.6 ± 4.14 – – OOM 20.67 ± 3.95 17.97 ± 11.68 17.57 ± 0.18 16.17 ± 5.89 N/A 12.55 ± 3.8 11.47 ± 4.79 15.25 ± 8.97 17.8 ± 0.0 N/A 17.58 ± 1.01 17.67 ± 0.59 17.80 ± 0.00 9.50 ± 1.17 N/A 17.80 ± 0.00 17.75 ± 0.39 17.43 ± 0.32 17.80 N/A|
|---|---|---|


|CIFAR|Original Linear, DPFP Linear, SIKF Linear, ReLU Performer|41.88 ± 0.48 39.02 ± 0.22 – – N/A 41.79 ± 0.27 38.73 ± 0.09 41.97 ± 1.24 41.33 ± 0.84 N/A 41.96 ± 0.47 38.89 ± 0.15 40.73 ± 0.58 42.94 ± 0.51 N/A 42.25 ± 0.01 38.44 ± 0.38 41.21 ± 1.18 39.96 ± 1.31 N/A 41.81 ± 1.16 32.26 ± 9.53 41.12 ± 1.70 40.06 N/A|
|---|---|---|


|TC|Original Linear, DPFP Linear, SIKF Linear, ReLU Performer|62.27 ± 0.8 62.02 ± 2.02 – – 55.7 ± 1.94 62.78 ± 0.48 63.05 ± 0.62 62.76 ± 0.21 62.78 ± 0.48 N/A 61.64 ± 0.82 62.35 ± 0.24 63.37 ± 1.4 62.24 ± 0.56 N/A 58.78 ± 0.93 63.95 ± 0.16 62.39 ± 0.59 61.00 ± 1.34 N/A 59.84 ± 1.46 62.66 ± 0.11 60.00 ± 1.20 57.22 N/A|
|---|---|---|



Table 1: Experiments on Long Range Arena benchmark, the best model is boldface, the double
underline is a top-2 result. Results for Performer and Linear Transformer (ReLU) are copied from
SPE (Liutkus et al., 2021), except experiments with FastRPB. We mark experiments that failed due
to memory limitations as OOM (Out of Memory). Since RPE is compatible only with the Original
Transformer, we marked other experiments as Not Applicable (N/A). RPE is N/A for CIFAR since
plain RPE is designed for 1D sequences. We marked experiments that were too long to train as ”–”.

one yields periodic covariance functions, which showed to be beneficial in such tasks as music
generation. The second utilizes vanishing covariance functions, a promising concept introduced in
Wang et al. (2020), which yields notably smaller validation losses in some SPE experiments.

Although SPE was beneficial in some music generation tasks, it still requires many computations due
to its stochastic nature. In practice, it could be dozens of times slower than the original Transformer,
as we will show feather.

3 APPROACH

3.1 SHIFT-INVARIANT KERNEL FUNCTION (SIKF)

We hypothesize that the shift-invariance property of softmax function (i.e., the fact that
softmaxi(x + c) = softmaxi(x), where x is some vector, c is a constant, which is added to every component of x) is an important property which makes original Transformer perform better
than Linear Transformer with an arbitrary kernel. Based on this assumption, we propose SIKF as
_φ(x) = exp (x), which satisfies the property of shift-invariance. If we substitute this function in the_
linear attention from Equation 3, then for every real-valued constants c and d we will get:

_φ(qm + c)[T][ P]n_ _[φ][(][k][n][ +][ d][)][v][n]_ _n_ _[e][d][φ][(][k][n][)][v][n]_ _n_ _[φ][(][k][n][)][v][n]_

= _[e][c][φ][(][q][m][)][T][ P]_ = _[φ][(][q][m][)][T][ P]_ (9)

_φ(qm + c)[T][ P]m_ _[φ][(][k][n][ +][ d][)]_ _e[c]φ(qm)[T][ P]n_ _[e][d][φ][(][k][n][)]_ _φ(qm)[T][ P]n_ _[φ][(][k][n][)]_

Thus, attention in Linear Transformer with exp(·) kernel function holds the same shift-invariance
property as plain softmax.

Based on our experiments, we conclude that SIKF is faster than Performer and DPFP, simultaneously
has comparable accuracy, and does not provide an extra memory footprint, which is essential for
scaling Linear Transformer on extremely long sequences.


-----

|Model|w/o FastRPB|w/ FastRPB|
|---|---|---|


|Original Linear, DPFP Linear, SIKF Linear, ELU + 1 Performer|97.34 ± 0.23 97.09 ± 0.19 96.49 ± 0.20 94.01 ± 0.31 96.6 ± 0.29|98.27 ± 0.19 97.66 ± 0.22 97.37 ± 0.35 96.71 ± 0.40 97.52 ± 0.26|
|---|---|---|


Table 2: MNIST F1 score. All the experiments run on 4 Nvidia Tesla T4.

3.2 FAST RELATIVE POSITIONAL BIAS (FASTRPB)

Although adding positional information in the attention mechanism is beneficial for model accuracy, current approaches are relatively inefficient in long sequences in terms of speed and memory
footprint. In this context, there is a desire to design an approach that will add relative positional information to attention efficiently and simultaneously be compatible with various efficient attention
modifications. To achieve this goal, we propose FastRPB[5] as a separate term for attention.

More formally, output matrix Y of attention layer with FastRPB is defined as:

**_Y = AttentionVariant(Q, K, V ) + W V_** (10)

here matrix W ∈ R[M] _[×][N]_ consists of learnable weights Wm,n representing relative distances between m and n embedding vectors from matrix V . Note that Equation 10 is invariant of choosing
a specific attention mechanism and could be used with both vanilla attention and its linear variants
(Equations 1 and 3 respectively).

One can think of the matrix W as a bias term to the usual attention matrix A from equation 1,
correcting the attention weights according to the relative distance between corresponding tokens.
However, adding positional bias term in the Equation 10 still requires O(NMD) computations due
to the matrix product and O(NM ) memory to store the bias matrix W [6]. In this regard, in the
following two subsections, we will construct FastRPB positional bias terms matrices W1d and W2d
for different types of sequences, that can be efficiently multiplied by V . W1d utilized in the case of
1D sequences (e.g., natural language texts), and its coefficients will correspond to distances between
words in 1D sequences. For 2D sequences we will utilize W2d, which coefficients will represent
distances between elements of 2D sequences (i.e., pixels). We will show that these specific matrices
**_W1d and W2d could be multiplied with V using only O(DN log N_** ) computations, and requiring
only O(N ) memory.

Further, we work with self-attention — a variant of attention mechanism, where input and output
sequences lengths are the same, i.e. N = M . In the general case of attention, when we have an
input sequence of length N and an output sequence of length M, we can pad the longer one to make
the input and output lengths match.

3.2.1 1D SEQUENCE CASE

Suppose we have a 1D sequence with N tokens. In such a sequence, there are exactly 2N − 1
distancerelative distances between tokens i ∈{−N + 1, ..., −1, 0, 1[7]. Let’s assign a learnable parameter, ..., N − 1}. We then will obtain a set of parameters: wi ∈ R for each relative

_{w−N_ +1, ..., w−1, w0, w1, ..., wN _−1}_ (11)

Next we will construct matrix W1d using parameters _wi_ _i=_ _N_ +1[. The basic intuition is to make]
_{_ _}[N]_ _[−]−[1]_
(n, m)-th element of matrix W1d to be assigned to the relative distance between from m-th token to

5There was a desire to name FastRPB as FastRPE to represent that it is like a faster RPE. However, we
change one letter to emphasize that FastRPB is orthogonal to the selection of an attention algorithm and could
be seen as a separate bias term to the attention map.
67ORelative distance from(N 2D) and O(N 2) respectively in the case of self-attention m-th token to n-th token is m − _n, which can have both positive and negative_
values, in this regard we have exactly 2N − 1 learnable parameters


-----

_n-th token, i.e., wm_ _n. Therefore matrix W1d will have the following structure:_
_−_

_w0_ _w1_ _w2_ _· · ·_ _wN_ _−1_

**_W1d =_**  _w−._ 1 _w.0_ _w.1_ _· · ·_ _wN_ _−2_ (12)

.. .. .. ...

w _N_ +1 _w_ _N_ +2 _w_ _N_ +3 _w0_ 
 _−_ _−_ _−_ _· · ·_ 
 

requiresBy definition, O(N [2] WD)1 computations in case of self-attentiond is a Toeplitz matrix (Gray, 2001). A naive way to calculate product[8]. It turns out that it can be efficiently W1d · V
multiplied by a matrix V according to the following proposition:

**Proposition 3.1product W1d** **_V For every Toeplitz matrix requires_** (DN log N ) operations and W1d ∈ R[N] _[×][N]_ _and for every matrix(N_ ) memory. Here V N ∈ is length of theR[N] _[×][D], matrix_
_input sequence. ·_ _O_ _O_

Using proposition 3.1 we can claim that FastRPB for 1D sequence will require O(DN log N ) computational operations. Moreover, we only need (N ) memory for storing parameters _wi_ _i=_ _N_ +1[,]
_O_ _{_ _}[N]_ _[−]−[1]_
generating Toeplitz matrix W1d. For the proof and a more detailed explanation of proposed properties see Appendix B.2.

3.2.2 2D SEQUENCE CASE

In the case of 2D sequences (e.g., images), a similar matrix to W1d could be defined. We will call
this matrix W2d, and it will consist of learnable weights assigned to pairwise distances from each
pixel of the image to the rest of the pixels. Here we will consider only the case of square images of
size N × N [9].

The natural way to process images of the size N ×N in the Transformer model is to flatten them into
a vector of size N [2]. In this regard, matrix of pairwise distances W2d needs to be of size N [2] _× N_ [2].
For simplicity we will assume images to be presented as a N ×N matrix, and W2d will be expressed
as a tensor W2d of size (N × _N_ ) _×_ (N × _N_ ), which (n, m, l, k) component represents distance from
pixel (l, k) to pixel (n, m).

We will assume the distance between two pixels to be a sum of two distances: the vertical and
horizontal[10]. In this regard, tensor W2d can be decomposed on vertical and horizontal tensor terms
**X and Y, respectively, as W2d = X + Y. Similar to the 1D case, we will assign shared learnable**
parameters _wi_ _i=_ _N_ +1 [for horizontal and vertical distances. Then to compute a matrix product of]
_{_ _}[N]_ _[−]−[1]_
tensor W2d of size N [2] _× N_ [2] with matrix V of size N [2] _× D we will simply flatten tensors X and Y,_
and obtain matrices Xflat and Yflat of shape N [2] _× N_ [2], and compute W2dV as XflatV + YflatV .

It turns out that the structure of matrices Xflat and Yflat is very similar to Toeplitz matrices from
the Section 3.2.1. In this regard, W2d can be multiplied by V efficiently, according to the following
proposition:

**Proposition 3.2and Yflat requires Product of matrix O(DN log N** ) and W O2d( ∈N )R memory.[N][ 2][×][N][ 2] _with matrix V ∈_ R[N][ 2][×][D] _using matrices Xflat_

Using the above proposition 3.2 we conclude that FastRPB 2D will require O(DN log N ) computational operations. Moreover, it is not essential to store whole tensors X and Y to compute the
product, we only need (N ) of memory for parameters _wi_ _i=_ _N_ +1 [generating this tensors. (See]
_O_ _{_ _}[N]_ _[−]−[1]_
Appendix B.3 for the proof).

4 EXPERIMENTS

**Long Range Arena. We evaluate proposed methods in the Long Range Arena (Tay et al., 2020),**
a benchmark for efficient Transformers with several text and image long sequence tasks. The

1089MatrixIf we work with non-square images of sizeIf we consider two pixels V has size N × D p, where1 = (3 D, is a hidden size 2) and N p ×2 = (0 M, we can simply pad them with zeros to make it square., 1) of image of size 4 × 4, the horizontal relative
distance from pixel p2 to pixel p1 then will be 2 − 1, and the vertical will be 3 − 0


-----

|AAN|Original Linear, DPFP Linear, SIKF Linear, ReLU Performer|OOM OOM OOM OOM OOM 0.36 0.43 2.45 15.41 N/A 0.36 0.45 1.22 9.12 N/A 0.36 0.45 1.26 9.13 N/A 0.6 0.79 1.6 10.52 N/A|5.81 6.03 – – 9.69 0.31 0.57 0.78 0.87 N/A 0.31 0.57 0.78 0.83 N/A 0.31 0.57 0.78 0.83 N/A 0.54 0.68 0.77 0.87 N/A|
|---|---|---|---|


|ListOps|Original Linear, DPFP Linear, SIKF Linear, ReLU Performer|0.74 0.85 – – OOM 0.26 0.36 2.3 13.8 N/A 0.24 0.34 0.95 6.85 N/A 0.24 0.34 0.98 6.87 N/A 0.38 0.48 1.06 8.7 N/A|3.25 3.49 – – 3.66 0.68 0.85 1.32 1.33 N/A 0.68 0.85 1.32 1.33 N/A 0.68 0.85 1.32 1.33 N/A 0.67 0.9 1.03 1.32 N/A|
|---|---|---|---|


|CIFAR|Original Linear, DPFP Linear, SIKF Linear, ReLU Performer|1.94 1.97 – – N/A 1.94 1.97 2.07 2.44 N/A 1.94 1.96 2.06 2.43 N/A 1.94 1.97 2.06 2.44 N/A 1.94 1.96 2.07 2.44 N/A|12.36 12.39 – – N/A 12.36 12.39 12.57 12.58 N/A 12.36 12.39 12.57 12.58 N/A 12.36 12.39 12.57 12.58 N/A 12.36 12.39 12.57 12.58 N/A|
|---|---|---|---|


|TC|Original Linear, DPFP Linear, SIKF Linear, ReLU Performer|1.81 2.24 – – 6.58 1.48 1.58 4.25 13.56 N/A 1.56 1.62 3.52 9.85 N/A 1.46 1.62 3.54 9.85 N/A 1.93 2.38 5.81 40.09 N/A|0.52 0.52 – – 0.78 0.23 0.23 0.33 0.43 N/A 0.23 0.23 0.33 0.43 N/A 0.23 0.23 0.33 0.43 N/A 0.33 0.41 0.33 0.47 N/A|
|---|---|---|---|


Training time (hours) Peak Memory Usage (GB)

None FastRPB sinSPE convSPE RPE None FastRPB sinSPE convSPE RPE

Table 3: Benchmark results on LRA with experiment setup proposed in SPE (Liutkus et al., 2021).
All the above experiments were conducted using a single Nvidia A100 GPU. The best model is
boldface, the double underline is a top-2 result, and the single underline is a top-3 result. We mark
experiments that failed due to limited memory as OOM (Out of Memory). We did not run Original
Transformer with sinSPE and convSPE since they require too much time to train.

main challenge of these tasks is dictated by the large sequence lengths, which average varies from
1K to 16K token[11]. In our experiments, we used the following tasks from this benchmark: (1)
ListOps, which test if a model is capable of parsing hierarchical expressions (Nangia & Bowman,
2018); (2) TC, which consist of movie review sentiment analysis on the IMDB corpus (Maas et al.,
2011); (3) All About NLP (AAN), which evaluates the model performance in matching and retrieval
tasks (Radev et al., 2013); and (4) CIFAR10, image classification dataset (Krizhevsky, 2009).

We compared Vanilla Transformer, Linear Transformer with all kernels observed in section 2.3 with
SIKF, combined with different positional encodings, namely: sineSPE, convSPE, FastRPB. We also
report results of experiments w/o adding any relative positional information. All models also used
trainable Absolute Positional Encodings.

All experiments and hyperparameters were conducted following instructions for the LRA dataset.
We also used LRA tasks to measure memory during the evaluation and computational footprints
during the training.

**MNIST. Due to the fact that in LRA CIFAR10 experiment only single layer transformer is used, we**
conducted another image recognition experiment with larger networks. We evaluated all the above
models w/ and w/o FastRPB on classical image classification dataset MNIST (Lecun et al., 1998). In
this experiment, we did not compare FastPRE with other positional encoding methods since, as we
observed in LRA, they require dozens of times more training time in experiments with multi-layer
transformers with large size of hidden states.

For all experiments, we used a model with 8 layers, 8 attention heads, a hidden size equal to 256,
and batch equal to 160. We trained models using AdamW optimizer and made 20 runs of Bayesian
hyperparameter search to find the optimal learning rate and than trained all models for 25 epochs.
Parameters are presented in Appendix A. We linearly decayed the learning rate to 0 during the
training. Final results are averaged over 10 runs with different values of random seed.

11We do not include another synthetic image classification task, Pathfinder, since we were unable to reproduce the results, obtained in the original paper (Tay et al., 2020).


-----

5 RESULTS

**Long Range Arena. See Table 1 for the evaluation results. Linear Transformer with SIKF kernel**
comes out in the top two results for every dataset, except CIFAR10, which we will discuss separately.
Memory footprint (see Table 3) of SIKF is indeed very tight to the ReLU, DPFP, and Performer,
while DPFP and Performer constantly performed slower (up to 1.4x times).

_Linear Transformer equipped with FastRPB showed significantly higher results on ANN and TC_
both in memory, speed, and accuracy, achieving even better results than Original Transformer. Moreover, architectures with FastRPB, confirming the above propositions 3.1 and 3.2, proved to have
memory and computation consumption comparable with the default architectures. For the ListOps
dataset, the best performance was obtained by the model without any relative positional encoding.
We attribute this result to the fact that relative distances can be confusing in parse hierarchical structure, such as expressions for ListOps or source code (e.g., the distance between IF and ELSE in
source code can be pretty large, however, these statements are inwardly connected). We measured
memory and computational footprint of models (see Table 3), according to which FastRPB requires
up to 30x less time to train than convSPE, and up to 3x less then sinSPE. Simultaneously, FastRPB
has 1.5x smaller memory footprint on evaluation then sinSPE and convSPE. Thus we conclude that
FastRPB is the fastest and the most accurate method among others.

The learned weights of FastRPB for different text tasks sequence is presented in Figure 1, where
_x-axis represents relative distance m_ _−_ _n from n-th token to m-th token, and color denotes the value_
of (W1d)n,m = wm _n. In the AAN task FastRPB forces the model to attend more to the very end of_
_−_
the text. Such observation can be attributed to the fact that AAN mainly consists of scientific texts,
which final part usually contains a conclusion. As for ListOps, learned FastRPB weights are usually
relatively small, which supports the hypothesis that relative positional encodings in such tasks should
be designed using the text hierarchy information. In the TC task, learned weights mainly draw the
model’s focus forward and backward to enable the model to capture long-range dependencies.

In experiments with CIFAR, usage of FastRPB was shown to decrease the model’s performance, and
the best result was obtained with convSPE, which outperformed others by a significant margin. We
attributed this to the experiment setup, where a single-layer network is utilized. In this regard, we
conduct the experiments with a more extensive network on the MNIST dataset.

**MNIST. In this task, each of the above models equipped with FastRPB showed superior perfor-**
mance (see Table 2), requiring quite a small amount of additional memory and computational time
(see Figures 3a and 3b respectively). As can be observed from the plots, of Linear Transformer
with FastRPB is up to 10x times less than the original ones. Moreover, in terms of speed Linear
_Transformer with FastRPB is 5x time faster in evaluation time compared to original Transformer._

The detailed view of trained FastRPB could be found in Figure 2, where we learned slice
(W2d):,:,12,10, which represents pairwise distances from pixel (12, 10) to every other pixel of 28 28
_×_
MNIST image. We observed that FastRPB enforced the model to look at more distinct pixels rather
than to close ones.

6 CONCLUSION

We presented two novel approaches aimed to increase the accuracy of the Linear Transformer model
without additional memory footprint and significant loss in speed. Contribution is two-fold: we first
make linear attention shift-invariance and then add a bias term to attention scores, representing
pairwise distances between tokens of the sequence. We computed this bias term efficiently and
achieved O(N log N ) complexity and O(N ) memory w.r.t. sequence length.

We demonstrate the superiority of our approach among others using four long sequence tasks from
the Long Range Arena benchmark and on the MNIST dataset. Our model performs significantly
better than previous approaches, obtaining the best accuracy on several tasks while being almost as
efficient in terms of speed and memory as plain Linear Transformer.

We believe that the principles presented in this work can serve as a basis for future research on the
role of positional information encoding in transformer architectures. To this end, we open-source all
the code and trained models.


-----

REFERENCES

Bassam Bamieh. Discovering transforms: A tutorial on circulant matrices, circular convolution, and
the discrete fourier transform, 2020.

Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.
_arXiv preprint arXiv:2004.05150, 2020._

Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas
Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention
with performers. arXiv preprint arXiv:2009.14794, 2020.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.

Robert Gray. Toeplitz and circulant matrices: A review. Foundations and Trends® in Communica_tions and Information Theory, 2, 10 2001. doi: 10.1561/0100000006._

Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert
with disentangled attention. arXiv preprint arXiv:2006.03654, 2020.

Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Franc¸ois Fleuret. Transformers are
rnns: Fast autoregressive transformers with linear attention. In International Conference on Ma_chine Learning, pp. 5156–5165. PMLR, 2020._

Guolin Ke, Di He, and Tie-Yan Liu. Rethinking positional encoding in language pre-training. arXiv
_preprint arXiv:2006.15595, 2020._

Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.

Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. doi: 10.1109/5.726791.

Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. A survey of transformers, 2021.

Antoine Liutkus, Ondˇrej C´ıfka, Shih-Lun Wu, Umut Simsekli, Yi-Hsuan Yang, and Gael Richard.
Relative positional encoding for transformers with linear complexity. In International Conference
_on Machine Learning, pp. 7067–7079. PMLR, 2021._

Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher
Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting
_of the Association for Computational Linguistics: Human Language Technologies, pp. 142–150,_
[Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http:](http://www.aclweb.org/anthology/P11-1015)
[//www.aclweb.org/anthology/P11-1015.](http://www.aclweb.org/anthology/P11-1015)

Nikita Nangia and Samuel R Bowman. Listops: A diagnostic dataset for latent tree learning. arXiv
_preprint arXiv:1804.06028, 2018._

Dragomir R Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. The acl
anthology network corpus. Language Resources and Evaluation, 47(4):919–944, 2013.

Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. arXiv preprint arXiv:1910.10683, 2019.

Andreas Rosowski. On fast computation of a circulant matrix-vector product. _arXiv preprint_
_arXiv:2103.02605, 2021._

Imanol Schlag, Kazuki Irie, and J¨urgen Schmidhuber. Linear transformers are secretly fast weight
memory systems. arXiv preprint arXiv:2102.11174, 2021.

Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155, 2018.


-----

Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,
Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient
transformers. arXiv preprint arXiv:2011.04006, 2020.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
_processing systems, pp. 5998–6008, 2017._

Benyou Wang, Donghao Zhao, Christina Lioma, Qiuchi Li, Peng Zhang, and Jakob Grue Simonsen.
Encoding word order in complex embeddings, 2020.

A MNIST HYPERPARAMTERTS SEARCH

|Model|w/o FastRPB|w/ FastRPB|
|---|---|---|


|Original Linear, DPFP Linear, SIKF Linear, ELU + 1 Performer|1.05 · 10−4 1.3 · 10−4 1.25 · 10−4 1.3 · 10−4 1.2 · 10−4|1.35 · 10−4 0.7 · 10−4 1.45 · 10−4 1.25 · 10−4 1.0 · 10−4|
|---|---|---|



Table 4: Best Learning Rate values obtained from 20 runs of Bayesian hyperparameters search

B PROPOSITION PROOFS

B.1 CIRCULANT MATRICES

To design a more efficient positional encoding method, we leveraged circulant matrices – a subclass
of matrices with some special properties due to their relation to the Fast Fourier Transform (FFT)
and circular convolution Bamieh (2020). Here we will only focus on the property that allows performing matrix-vector product fast and efficient in terms of speed and memory. Given an vector
**_c = (c0, c1, ..., cn−1) we will define the associated n × n circulant matrix C = circ(c) which first_**
column is exactly c, and each subsequent column is obtained by a circular shift of the previous
column:

_c0_ _cn−1_ _cn−2_ _· · ·_ _c1_
_c1_ _c0_ _cn−1_ _c2_

**_C =_**  _c2_ _c1_ _c0_ _c3_ (13)

. .

 .. ... ... .. 
 
cn 1 _cn_ 2 _cn_ 3 
 _−_ _−_ _−_ _· · ·_ 
 

It can be shown that for every vector x of size n matrix-vector product Cx requires only O(n log n)
computation Rosowski (2021). Moreover, to compute the above product, it is not necessary to
store the whole matrix C in memory; it is enough to only keep in memory O(n) parameters of
vector c. Further, we will prove that FastRPB and FastRPB 2D can be expressed through circular
matrices, and hence relative positional information can be embedded efficiently in the self-attention
mechanism.

B.2 PROPOSITION 1

In 3.2.1 we introduced a Toeplitz matrix W1d of shape N × N :


_w0_ _w1_ _w2_ _· · ·_ _wN_ _−1_
_w−1_ _w0_ _w1_ _· · ·_ _wN_ _−2_
_w−2_ _w−1_ _w0_ _wN_ _−3_
. .
.. ... ... ..
_w−N_ +1 _w−N_ +2 _w−N_ +3 _· · ·_ _w0_


(14)


**_W1d =_**


-----

AAN

Relative distance between tokens, i


0.06

0.04


0.02

0.00


_−0.02_


(a) Learned in AAN task weights wi assigned to pairwise distances between tokens i in
FastRPB 1D.

ListOps


0.00

_−0.02_


_−0.04_


Relative distance between tokens, i


(b) Learned in ListOps task weights wi assigned to pairwise distances between tokens i
in FastRPB 1D.

TC


0.01

0.00


_−0.01_


Relative distance between tokens, i

(c) Learned in TC task weights wi assigned to pairwise distances between tokens i in
FastRPB 1D.


-----

Our goal is to efficiently multiply W1d by arbitrary matrix V of shape N × D. As was stated in
section Circulant matrices, a special class of matrices, namely circulant matrices, can be multiplied
by a vector efficiently in O(N log N ) operations and requires O(N ) memory. We will extend matrix W1d with additional rows and columns and thus obtain circulant matrix W1[ext]d [. Then we will]
introduce V [ext], a modified version of matrix V, which W1[ext]d [will be multiplied by. Finally, we will]
select a slice from the product W1[ext]d

_[·][ V][ ext][, which will be exactly][ W][1][d][ ·][ V][ .]_

The first step is to define W1[ext]d [:]


_w−N_ +1 _w−N_ +2 _· · ·_ _w0_ _w1_ _w2_ _· · ·_ _wN_ _−1_
_wN_ _−1_ _w−N_ +1 _· · ·_ _w−1_ _w0_ _w1_ _· · ·_ _wN_ _−2_
_wN_ _−2_ _wN_ _−1_ _· · ·_ _w−2_ _w−1_ _w0_ _wN_ _−3_
. .
.. ... ... ... ..
_w1_ _w2_ _· · ·_ _w−N_ +1 _w−N_ +2 _· · ·_ _w−1_ _w0_
. .
.. ... ... ... ... ..
_w−N_ +2 _w−N_ +3 _· · ·_ _· · ·_ _· · ·_ _· · ·_ _wN_ _−1_ _w−N_ +1


**_W1[ext]d_** [=]


(15)


As can be seen, constructed matrix W1[ext]d [is indeed circulant. Moreover, the right upper corner]
of W1[ext]d [is essentially][ W][1][d][. Hence,][ W][1][d][ can be expressed as a slice of][ W][ ext]1d [following numpy]
notation: W1d = **_W1[ext]d_** _N_ :, :N +1[.]

Now we want to calculate matrix productDue to this fact we will need to multiply   **_W W11[ext]dd · V[with appropriate matrix] using matrix W1[ext]d_** [of size][ V][ ext][ (2][ of size][N][ −] [1)][ 2][ ×][N][ −][ (2][N][1][ ×][ −][ D][1)][.][.]
Since, as we seen, W1d is a slice of W1[ext]d [, we only will need first][ N][ rows of the resulting product]
**_W1[ext]d_** _[·]_ **_[V][ ext][. More formally we need to find such matrix][ V][ ext][ that:][ W][1][d][ ·]_** **_[V][ =]_** **_W1[ext]d_** _[·]_ **_[V][ ext][]:N_** +1, :[.]

To achieve with we will basically pad V with N 1 additional rows filled with zeros as follows:
_−_  

0 0 _· · ·_ 0

 0. 0. _· · ·_ 0. 

.. .. ... ..

 0 0 0 

**_V_** [ext] =  _v0,0_ _v0,1_ _· · ·_ _v0,D_ 1  (16)

 _· · ·_ _−_ 
 _v1,0_ _v1,1_ _v1,D_ 1 
 _· · ·_ _−_ 
 . . . 
 .. .. ... .. 
 
vN _−1,0_ _vN_ _−1,1_ _· · ·_ _vN_ _−1,D−1_
 

To complete the proof lets explicitly show that W1d · V = **_W1[ext]d_** _[·][ V][ ext][]:N_ +1, :[. Lets assume]

_D = 1, generalization for bigger dimensions can be done using similar operations:_

 


_w−N_ +1 _· · ·_ _w0_ _w1_ _· · ·_ _wN_ _−1_
_wN_ _−1_ _· · ·_ _w−1_ _w0_ _· · ·_ _wN_ _−2_
_wN_ _−2_ _· · ·_ _w−2_ _w−1_ _wN_ _−3_
. .
.. ... ... ..
_w1_ _· · ·_ _w−N_ +1 _w−N_ +2 _· · ·_ _w0_


**_W1[ext]d_** _[·][ V][ ext][]:N_ +1, : [=]


(17)


_v0_
_v1_
.
.
.
_vN_ _−1_


_w0_ _w1_ _· · ·_ _wN_ _−1_ _v0_

 _ww−−12_ _ww−01_ _· · ·_ _wwNN_ _−−23_  _v.1_  (18)

 ... ... ... ...  ..
  vN 1
w _N_ +1 _w_ _N_ +2 _w0_   _−_ 
 _−_ _−_ _· · ·_    [=][ W][1][d][ ·][ V]
 

The last thing we have to do is to calculate the complexity of matrix product of circulant matrix
**_W1[ext]d_** [with][ V][ ext][. Since matrix][ W][ ext]1d [is circulant of size][ (2][N][ −] [1)][ ×][ (2][N][ −] [1)][, according to the]


-----

section Circulant matrices, it requires O(N ) memory to it, and O(N log N ) operations to perform
a matrix-vector product with vector if size 2N − 1. In this regard to compute matrix product with
matrix V of size N × D, we will need to perform D times operations more, i.e. O(DN log N )
operations.

B.3 PROPOSITION 2

In the following sections, we will be considering an example of 3 × 3 image. In section Structure
of pairwise distances tensors we will study the general structure of tensors X and Y, which were
introduced in section . In section Flattening of the tensors we will reshape these tensors and obtain a
new pair of tensors Xflat and Yflat, which will be then efficiently multiplied by V matrix in the final
section Efficient matrix product.

B.3.1 STRUCTURE OF PAIRWISE DISTANCES TENSORS

First of all, to gain a deeper understanding of structure of tensors X and Y, we will explicitly write
down components of this tensors for an image of size 3 × 3. Consider X1,1,:,: and Y1,1,:,: which
contains weights assigned to vertical and horizontal relative distances from pixel (1, 1) to all other
pixels:

_w−1_ _w−1_ _w−1_ _w−1_ _w0_ _w1_

**_X1,1,:,: =_** _w0_ _w0_ _w0_ _, Y1,1,:,: =_ _w_ 1 _w0_ _w1_ (19)

_−_

_w1_ _w1_ _w1_ ! _w−1_ _w0_ _w1!_

One can observe, that Xn,m,:,: and Yn,m,:,: have the property of symmetry, through which it can be
proven that:

**Proposition B.1 Xn,i,:,: = Xn,j,:,: and Yi,n,:,: = Yj,n,:,: for every n, i, j ∈{0, ..., N −** 1}, and
_this property holds for image of every size._

Taking advantage of the proposition B.1 we can write out explicit form of tensors X and Y in case
of 3 × 3 images. We will introduce the following notation:

_w−2_ _w−2_ _w−2_ _w−1_ _w−1_ _w−1_ _w0_ _w0_ _w0_

**_A =_** _w−1_ _w−1_ _w−1_ _, B =_ _w0_ _w0_ _w0_ _, C =_ _w1_ _w1_ _w1_ (20)

_w0_ _w0_ _w0_ ! _w1_ _w1_ _w1_ ! _w2_ _w2_ _w2!_

It can be seen that different slices of tensor X can be expressed using matrices A, B, C:
**_X0,0,:,: = X0,1,:,: = X0,2,:,: = C_** (21)
**_X1,0,:,: = X1,1,:,: = X1,2,:,: = B_** (22)
**_X2,0,:,: = X2,1,:,: = X2,2,:,: = A_** (23)
Moreover, are matrices A, B, C also applicable for tensor Y:

**_Y0,0,:,: = Y0,1,:,: = Y0,2,:,: = C_** _[T]_ (24)

**_Y1,0,:,: = Y1,1,:,: = Y1,2,:,: = B[T]_** (25)

**_Y2,0,:,: = Y2,1,:,: = Y2,2,:,: = A[T]_** (26)

B.3.2 FLATTENING OF THE TENSORS

In transformer architecture before processing the image of size N × N, it is usually flattened into
1-dimensional vector of size N [2]. To define flatten operation, consider an arbitrary matrix M of
shape 3 × 3, than its flattened version mflat will have the following structure:

_m0,0_
_m0,1_

m0,2

_m0,0_ _m0,1_ _m0,2_ flattening m1,0

**_M =_** _m1,0_ _m1,1_ _m1,2_ **_mflat =_** m1,1 (27)

_m2,0_ _m2,1_ _m2,2!_ _−−−−−→_ m1,2

 
m2,0
m2,1
 
m2,2
 
 


-----

Due to the flatten of images in transformer, matrix V in will have shape N [2] _× D and hence it is_
essential to reshape tensors X and Y from size (N × N ) × (N × N ) to size N [2] _× N_ [2]. We will
denoted reshaped versions of tensors X and Y as Xflat and Yflat respectively. Reshaping of the above
tensors can be decomposed into two consecutive flatten operations firstly applied to last two dims
of tensors X and Y and then to the first two ones. Flattening of the last dimensions is equivalent to
flattening of each of the matrix A, B, C, after this operation in case of 3 × 3 images we will obtain
the following three vectors:


_w0_
_w0_
_w0_
_w1_
_w1_
_w1_
_w2_
_w2_
_w2_


_w−1_
_w−1_
_w−1_
_w0_
_w0_
_w0_
_w1_
_w1_
_w1_


_w−2_
_w−2_
_w−2_
_w−1_
_w−1_
_w−1_
_w0_
_w0_
_w0_


(28)

(29)

(30)


**_aflat =_**


_, bflat =_


_, cflat =_


Than after next flattening operation we will get:


_w0_ _w0_ _w0_ _w1_ _w1_ _w1_ _w2_ _w2_ _w2_
_w0_ _w0_ _w0_ _w1_ _w1_ _w1_ _w2_ _w2_ _w2_
_w0_ _w0_ _w0_ _w1_ _w1_ _w1_ _w2_ _w2_ _w2_
_w−1_ _w−1_ _w−1_ _w0_ _w0_ _w0_ _w1_ _w1_ _w1_
_w−1_ _w−1_ _w−1_ _w0_ _w0_ _w0_ _w1_ _w1_ _w1_
_w−1_ _w−1_ _w−1_ _w0_ _w0_ _w0_ _w1_ _w1_ _w1_
_w−2_ _w−2_ _w−2_ _w−1_ _w−1_ _w−1_ _w0_ _w0_ _w0_
_w−2_ _w−2_ _w−2_ _w−1_ _w−1_ _w−1_ _w0_ _w0_ _w0_
_w−2_ _w−2_ _w−2_ _w−1_ _w−1_ _w−1_ _w0_ _w0_ _w0_

_w0_ _w1_ _w2_ _w0_ _w1_ _w2_ _w0_ _w1_ _w2_
_w−1_ _w0_ _w1_ _w−1_ _w0_ _w1_ _w−1_ _w0_ _w1_
_w−2_ _w−1_ _w0_ _w−2_ _w−1_ _w0_ _w−2_ _w−1_ _w0_
_w0_ _w1_ _w2_ _w0_ _w1_ _w2_ _w0_ _w1_ _w2_
_w−1_ _w0_ _w1_ _w−1_ _w0_ _w1_ _w−1_ _w0_ _w1_
_w−2_ _w−1_ _w0_ _w−2_ _w−1_ _w0_ _w−2_ _w−1_ _w0_
_w0_ _w1_ _w2_ _w0_ _w1_ _w2_ _w0_ _w1_ _w2_
_w−1_ _w0_ _w1_ _w−1_ _w0_ _w1_ _w−1_ _w0_ _w1_
_w−2_ _w−1_ _w0_ _w−2_ _w−1_ _w0_ _w−2_ _w−1_ _w0_


**_Xflat = X.reshape(N_** [2], N [2]) =

**_Yflat = Y.reshape(N_** [2], N [2]) =


Note that rows of the Xflat and Yflat are aflat, bflat and cflat.

Now we have flattened representation Xflat and Yflat and the last step we have to take is to compute
matrix product Xflat · V and Yflat · V .

B.3.3 EFFICIENT MATRIX PRODUCT

**Calculation of Yflat · V**

In case of 3 × 3 images it is easy to see that the matrix Yflat, presented in formula 30 consists of 9
identical blocks, we will denote this blocks as R:


_w0_ _w1_ _w2_
_w−1_ _w0_ _w1_
_w−2_ _w−1_ _w0_


_, where R =_


(31)


**_Yflat =_**


Note that matrix R is just a block of Y, not its element. Also a very important fact is that can
matrix R is a Toeplitz, which means that it can be efficiently multiplied by a vector, according
to Proposition 1. Our goal is to efficiently multiply Yflat with a matrix V of shape N [2] _×_ _D. Since V_
is basically D times stacked vectors of size N [2] it will be enough to consider matrix-vector product


-----

of Yflat with a vector v of shape N [2]. The product can be expressed as:


_v0_
_v1_
_v2_
_v3_



_v4_



_v5_



_v6_
_v7_



_v8_




**_R ·_**


_v0_
_v1_
_v2_


_v3_
_v4_
_v5_


_v6_
_v7_
_v8_


**_R ·_**


+ R ·


+ R ·


(32)

(33)


**_Yflat · V =_**


_v0 + v3 + v6_
_v1 + v4 + v7_
_v2 + v5 + v8_


Lets introduce an additional notation, we will denote as Vm a tensor of size N × N × D, which was
obtained as a reshape of matrix V of size N [2] _× D. Vm will have the following structure:_

_v0_ _v1_ _v2_

**_Vm =_** _v3_ _v4_ _v5_ = V .reshape(N, N, D) (34)

_v6_ _v7_ _v8!_


Then:


_v0 + v3 + v6_
_v1 + v4 + v7_
_v2 + v5 + v8_


**_Vm[T]_** _[.][sum][(1) =]_


(35)

(36)


Finally using notation 34 and result of product 33, we can conclude that:

**_R ·_** **_Vm[T]_** _[.][sum][(1)]_

**_Yflat · V =_**   ... 

 
 


Note that the summation by dim = 1, not by dim = −1, since in general case, when D is not 1, the
latter dimension refers to hidden size.

**_R is a Topleitz matrix we worked with in the previous paragraph B.2, that is, the product of matrix_**
**_R with Vm[T]_** _[.][sum][(1)][ can be efficiently computed using the properties of Toeplitz matrix. Finally]_
we can conclude that forcomputations and O(N ) memory to store a vector, generating matrix N × N image computation of product Yflat R ·. V require O(DN log N )

**Calculation of Xflat · V**

Here we will apply the following trick: by rearranging the columns of matrix Xflat we will construct
matrix Xflat[′] [, and accordingly construct matrix][ U][ by rearranging the rows of matrix][ V][ in such a way]
that the equality holds Xflat **_V = Xflat[′]_**
noted previously, it can be easily generalized for higher dimensions. · _[·][ U]_ [. We still will work with the][ D][ = 1][ case since, as was]


_w0_ _w0_ _w0_ _w1_ _w1_ _w1_ _w2_ _w2_ _w2_
_w0_ _w0_ _w0_ _w1_ _w1_ _w1_ _w2_ _w2_ _w2_
_w0_ _w0_ _w0_ _w1_ _w1_ _w1_ _w2_ _w2_ _w2_
_w−1_ _w−1_ _w−1_ _w0_ _w0_ _w0_ _w1_ _w1_ _w1_
_w−1_ _w−1_ _w−1_ _w0_ _w0_ _w0_ _w1_ _w1_ _w1_
_w−1_ _w−1_ _w−1_ _w0_ _w0_ _w0_ _w1_ _w1_ _w1_
_w−2_ _w−2_ _w−2_ _w−1_ _w−1_ _w−1_ _w0_ _w0_ _w0_
_w−2_ _w−2_ _w−2_ _w−1_ _w−1_ _w−1_ _w0_ _w0_ _w0_
_w−2_ _w−2_ _w−2_ _w−1_ _w−1_ _w−1_ _w0_ _w0_ _w0_


_v0_
_v1_
_v2_
_v3_
_v4_
_v5_
_v6_
_v7_
_v8_


(37)


**_Xflat · V =_**


-----

_w0_ _w1_ _w2_ _w0_ _w1_ _w2_ _w0_ _w1_ _w2_
_w0_ _w1_ _w2_ _w0_ _w1_ _w2_ _w0_ _w1_ _w2_

 _w0_ _w1_ _w2_ _w0_ _w1_ _w2_ _w0_ _w1_ _w2_

w−1 _w0_ _w1_ _w−1_ _w0_ _w1_ _w−1_ _w0_ _w1_
w 1 _w0_ _w1_ _w_ 1 _w0_ _w1_ _w_ 1 _w0_ _w1_
 _−_ _−_ _−_
w 1 _w0_ _w1_ _w_ 1 _w0_ _w1_ _w_ 1 _w0_ _w1_
 _−_ _−_ _−_
w−2 _w−1_ _w0_ _w−2_ _w−1_ _w0_ _w−2_ _w−1_ _w0_
w 2 _w_ 1 _w0_ _w_ 2 _w_ 1 _w0_ _w_ 2 _w_ 1 _w0_
 _−_ _−_ _−_ _−_ _−_ _−_
w 2 _w_ 1 _w0_ _w_ 2 _w_ 1 _w0_ _w_ 2 _w_ 1 _w0_
 _−_ _−_ _−_ _−_ _−_ _−_


Now lets use already introduced matrix Vm:


_v0_
_v3_
_v6_
_v1_
_v4_
_v7_
_v2_
_v5_
_v8_


= Xflat[′]

_[·][ U][ =]_


(38)

(39)

(40)


_w0_ _w1_ _w2_ _v0 + v1 + v2_
_w_ 1 _w0_ _w1_ _v3 + v4 + v5_
_−_
_w_ 2 _w_ 1 _w0! v6 + v7 + v8_
_−_ _−_


**_R_** **_Vm.sum(1)_**
_·_
 


_v0_
_v3_
_v6_


_v1_
_v4_
_v7_


_v2_
_v5_
_v8_


**_R ·_**


+ R ·


+ R ·


And finally:

B.3.4 CONCLUSION


**_Xflat[′]_**

_[·][ U][ =]_


_.reshape(N_ [2]) = (41)


**_R_** **_Vm.sum(1)_** _.reshape(N_ [2]) (42)
_·_ _· · ·_ _· · ·_
   


As we proved in the previous section, products Xflat · V and Yflat · V can be computed using just
two products: R · **_Vm.sum(1)_** and R · **_Vm[T]_** _[.][sum][(1)]_ respectively. Moreover, matrix R utilized
in the above products is a Toeplitz, and in this regard this products can be computed efficiently ac-   
cording to Proposition 1. Summarizing it all we get that FastRBP 2D will require be O(DN log N )
computations and O(N ) memory.


-----

