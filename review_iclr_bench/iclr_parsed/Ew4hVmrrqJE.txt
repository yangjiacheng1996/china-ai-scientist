## SAMPLE AND COMMUNICATION-EFFICIENT DECEN### TRALIZED ACTOR-CRITIC ALGORITHMS WITH FINITE## TIME ANALYSIS

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Actor-critic (AC) algorithms have been widely adopted in decentralized multi-agent
systems to learn the optimal joint control policy. However, existing decentralized
AC algorithms either need to share agents’ sensitive information, e.g., local actions
and policies, or are not sample and communication-efficient. In this work, we
develop two decentralized AC and natural AC (NAC) algorithms that are sample and
communication-efficient and avoid sharing agents’ local actions and policies . In
both algorithms, agents share only noisy rewards and adopt mini-batch local policy
gradient updates to improve sample and communication efficiency. Particularly
for decentralized NAC, we develop a decentralized Markovian SGD algorithm
with an adaptive mini-batch size to efficiently compute the natural policy gradient.
Under Markovian sampling and linear function approximation, we prove that
the proposed decentralized AC and NAC algorithms achieve the state-of-the-art
sample complexities O(ϵ[−][2] ln ϵ[−][1]) and O(ϵ[−][3] ln ϵ[−][1]), respectively, and achieve
an improved communication complexity O(ϵ[−][1] ln ϵ[−][1]). Numerical experiments
demonstrate that the proposed algorithms achieve lower sample and communication
complexities than the existing decentralized AC algorithm.

1 INTRODUCTION

Multi-agent reinforcement learning (MARL) has achieved great success in various application
domains, including control (66; 10; 51), robotics (64), wireless sensor networks (24; 67), intelligent
systems (71), etc. In MARL, a set of fully decentralized agents interact with a dynamic environment
following their own policies and collect local rewards, and their goal is to collaboratively learn the
optimal joint policy that achieves the maximum expected accumulated reward.

Classical policy optimization algorithms have been well developed and studied, e.g., policy gradient
(PG) (49), actor-critic (AC) (23) and natural actor-critic (NAC) (37; 7). In particular, AC-type
algorithms are more computationally tractable and efficient as they take advantages of both policy
gradient and value-based updates. However, in the multi-agent setting, decentralized AC is more
challenging to design compared with the centralized AC, as the algorithm updates involve sensitive
agent information, e.g., local actions, rewards and policies, which must be kept locally in the
decentralized learning process. In the existing designs of decentralized AC, the agents need to share
either their local actions (70; 69; 8; 36; 72; 27; 19; 26; 11) or local rewards (15; 33; 32) with their
neighbors, and hence are not desired. This issue is addressed by Algorithm 2 of (70) at the cost of
learning a parameterized model to estimate the averaged reward, yet this approach requires extra
learning effort and the reward estimation can be inaccurate. Moreover, existing decentralized AC
algorithms are not sample and communication-efficient, and do not have finite-time convergence
guarantee, especially under the practical Markovian sampling setting. Therefore, we aim to address
the following important question.

-  Q1: Can we develop a decentralized AC algorithm that is convergent, sample and communication_efficient, and does not require sharing agents’ local actions and policies ?_

On the other hand, as an important variant of the decentralized AC, decentralized NAC algorithm has
not been formally developed and rigorously analyzed in the existing literature. In particular, a major
challenge is that we need to develop a fully decentralized and computationally tractable scheme to


-----

compute the inverse of the high dimensional Fisher information matrix, and this scheme must be both
sample and communication efficient. Hence, we want to ask:

-  Q2: Can we develop a computationally tractable and communication-efficient decentralized NAC
_algorithm that has a low finite-time sample and communication complexity?_

In this study, we provide affirmative answers to the above two questions by developing fully decentralized AC and NAC algorithms that are sample and communication-efficient, and do not reveal
agents’ local actions and policies. We also develop rigorous finite-time analysis of these algorithms
under Markovian sampling. Our contributions are summarized as follows.

Table 1: List of complexities of the existing AC and NAC algorithms for achieving
E[∥∇J(ω)∥[2]] ≤ _ϵ and E[J(ω[∗]) −_ _J(ω))] ≤_ _ϵ, respectively._

Share local Sampling Sample Communication
Algorithm Papers

action/policy scheme complexity complexity


(54) – i.i.d. _O(ϵ[−][36])_ –

(38) – i.i.d. _O(ϵ[−][4])_ –

(25) – i.i.d. _O(ϵ[−][2][.][5])_ –

(61) – Markovian (ϵ[−]e[2][.][5] ln[3] _ϵ[−][1])_ –
_O_

(56) – Markovian _O(ϵ[−][2][.][5])_ –

(60) – Markovian _O(ϵ[−][2]_ ln ϵ[−][1]) –

e

(70; 69; 15)
(72; 27; 26) _×_ Markovian – –

(70; 47; 33) ✓ Markovian – –

This work ✓ Markovian _O(ϵ[−][2]_ ln ϵ[−][1]) _O(ϵ[−][1]_ ln ϵ[−][1])


Centralized AC

Decentralized AC


(54) – i.i.d. _O(ϵ[−][36])_ –
Centralized NAC (61) – Markovian _O(ϵ[−][4]_ ln[2] _ϵ[−][1])_ –

(60) – Markovian _O(ϵ[−][3]_ ln ϵ[−][1]) –

Decentralized NAC This work ✓ Markovian _O(ϵ[−][3]_ ln ϵ[−][1]) _O(ϵ[−][1]_ ln ϵ[−][1])

1.1 OUR CONTRIBUTIONS

We develop fully decentralized AC and NAC algorithms and analyze their finite-time sample and
communication complexities under Markovian sampling. Our results and comparisons to existing
works are summarized in Table 1 [1]. Our decentralized AC and NAC algorithms adopt the following
novel designs to accurately estimate the policy gradient in an efficient way.

-  Noisy Rewards: In a decentralized setting, local policy gradients (estimated locally by the agents)
involve the average of all agents’ local rewards. To help agents estimate this averaged reward
without revealing the raw local rewards, we let them share Gaussian-corrupted local rewards with
their neighbor, and the variance of the Gaussian noise can be adjusted by each agent to reach its
desired level.

-  Mini-batch Updates: We apply mini-batch Markovian sampling to both the decentralized actor
and critic updates. This approach 1) helps the agents obtain accurate estimations of the corrupted
averaged reward; 2) significantly reduces the variance of policy gradient caused by Markovian
sampling; and 3) significantly reduces the communication frequency and complexity.

Moreover, for our decentralized NAC algorithm, we additionally adopt the following design to
compute the inverse of the Fisher information matrix in an efficient and decentralized way.

-  Decentralized Natural Policy Gradient: By reformulating the natural policy gradient as the minimizer of a quadratic program, we develop a decentralized SGD with Markovian sampling that
allows the agents to estimate the corresponding local natural gradients by communicating only

1In this table, _O(·) hides all logarithm factors. In (25), the sample complexity has been established for_
various AC-type algorithms, and we compare with the best one. In (70), the Algorithm 1 needs to share local
actions while the Algorithm 2 does not.
e


-----

scalar variables with their neighbors. In particular, in order to minimize the sample complexity of
the decentralized SGD, we set the batch size to be exponentially increasing.

Theoretically, for the first time, we provide finite-time convergence analysis of decentralized AC
and NAC algorithms under Markovian sampling. Specifically, we prove that our decentralized AC
and NAC algorithms achieve the overall sample complexities O(ϵ[−][2] ln ϵ[−][1]) and O(ϵ[−][3] ln ϵ[−][1]),
respectively, and both match the state-of-the-art complexities of their centralized versions (60).
Moreover, both decentralized algorithms achieve a significantly reduced overall communication
complexity O(ϵ[−][1] ln ϵ[−][1]). In particular, our analysis involves new technical developments. First, we
need to characterize the bias and variance of (natural) policy gradient and stochastic gradient caused
by the noisy rewards and the inexact local averaging steps, and control them with proper choices of
batch sizes and number of local averaging steps. Second, when using decentralized Markovian SGD
to compute the inverse Fisher information matrix, we need to use an exponentially increasing batch
size to achieve an optimized sample complexity bound. Such a Markovian SGD with adaptive batch
size has not been studied before and can be of independent interest.

1.2 RELATED WORK

**Convergence analysis of AC and NAC. In the centralized setting, the AC algorithm was firstly**
proposed by (23) and later developed into the natural actor-critic (NAC) algorithm (37; 7). Specifically,
(37) does not provide any convergence result, while (22; 5) and (20; 6; 7) establish the asymptotic
convergence rate of centralized AC and NAC, respectively, which is weaker than our finite-time
convergence results. Furthermore, (54; 25; 38; 61; 56) and (54) establish the finite-time convergence
rate of centralized AC and NAC, respectively. Please refer to Table 1 for their sample complexities.
Moreover, (60) improve the finite-time sample complexities of the above works to the state-of-theart result for both centralized AC and NAC by leveraging mini batch sampling, and our sample
complexities match these state-of-the-art results .

In the decentralized setting, a few works have established the almost sure convergence result of
AC (15; 27; 47; 33), but they do not characterize the finite-time convergence rate and the sample
complexity. To the best of our knowledge, there is no formally developed decentralized NAC
algorithm.

**Decentralized TD-type algorithms. The finite-time convergence of decentralized TD(0) has been**
obtained using i.i.d samples (52; 14; 53; 28) and Markovian samples (46; 53), respectively, without
revealing the agents’ local actions, policies and rewards . Decentralized off-policy TD-type algorithms
have been studied in (34; 45; 9; 12).

**Decentralized AC in other MARL settings. Some works apply decentralized AC to other MARL**
settings that are very different from ours. For example, (44; 36; 17; 11; 57) studied adversarial game.
(30) studied a mixed cooperative-competitive environment where each agent maximizes its own Q
function (30). (11) proposed Delay-Aware Markov Game which considers delay in Markov game.
(68; 31) studied linear control system and linear quadratic regulators instead of an MDP. (55) studied
sequential prisoner’s dilemmas.

**Policy gradient algorithms. Policy gradient (PG) and natural policy gradient (NPG) are popular**
policy optimization algorithms. (1) characterizes the iteration complexity (i.e., number of episodes)
of centralized PG and NPG algorithms by assuming access to exact policy gradient. They also
established a sample complexity result O(ϵ[−][6]) in the i.i.d. setting for NPG, which is worse than the
state-of-the-art result O(ϵ[−][3] ln ϵ[−][1]) of both centralized NAC (60) and our decentralized NAC with
Markovian samples. (3) proposes decentralized PG in a simple cooperative MARL setting, where all
the agents share one action and the same policy, and they establish a iteration complexity in the order
of O(ϵ[−][4]). (13; 73) apply decentralized PG to Markov games. (2) applies decentralized NPG to a
different cooperative MARL setting where each agent observes its own state, takes its own action and
has access to these information of its neighbors.

**Value-based algorithms. Value-based algorithms have also been develop for MARL. Specifically,**
(21; 18) develop distributed Q-learning in a simplified cooperative MARL setting, where the agents
share a joint action. In particular, (18) characterizes the convergence rate of a value function-based
convergence error, which is a different optimality measure from that of AC-type algorithms. (35)
applies distributed Q-learning to another cooperative MARL setting, where each agent observes


-----

its own state and takes its own action. It establishes an asymptotic convergence guarantee, and no
convergence rate is given. (40) develops a value propagation algorithm that uses primal-dual method
to minimize a soft Bellman error in the MARL setting. Under an assumption that the variance of
the stochastic gradient is uniformly bounded, it establishes a non-asymptotic convergence rate to an
approximate stationary point.

2 REVIEW OF MULTI-AGENT REINFORCEMENT LEARNING

In this section, we first introduce some standard settings of RL. Consider an agent that starts from
an initial stateby interacting with an underlying environment (with transition kernel s0 ∼ _ξ and collects a trajectory of Markovian samples P {) following a parameterizedst, at, Rt}t ⊂S × A × R_
policy πω with induced stationary state distribution µω. The agent aims to learn an optimal policy
that maximizes the expected accumulated reward J(ω) = (1 _γ)E_ _∞t=0_ _[γ][t][R][t]_, where γ (0, 1)
_−_ _∈_
is a discount factor. The marginal state distribution is denoted as Pω(st) and the visitation measure
 P 
is defined as νω(s) := (1 − _γ)_ _t=0_ _[γ][t][P][ω][(][s][t][ =][ s][)][, both of which depend on the policy parameter]_
_ω_ Ω and the transition kernel . We also define the mixed transition kernel _ξ(_ _s, a) :=_
_∈_ _P_ _P_ _·|_
_γ_ ( _s, a) + (1_ _γ)ξ(_ ), whose stationary state distribution is known to be νω.
_P_ _·|_ _−_ _·_ [P][∞]

In the multi-agent RL (MARL) setting, M agents are connected via a fully decentralized network
and interact with a shared environment. The network topology is specified by a doubly stochastic
communication matrix W ∈ R[M] _[×][M]_ . At any time t, all the agents share a common state st. Then,
every agent m takes an action a[(]t[m][)] following its own current policy πt[(][m][)]( _st) parameterized by_

_·|_
_ωt[(][m][)]. After all the actions at :=_ _a[(]t[m][)]_ _m=1_ [are taken, the global state][ s][t] [transfers to a new state]
_{_ _}[M]_
_st+1 and every agent m receives a local reward Rt[(][m][)]. In this MARL setting, each agent m can only_
access the global state {st}t, its own actions {a[(]t[m][)]}t and rewards {Rt[(][m][)]}t and policy πt[(][m][)]. Next,
define the joint policyand define the average reward πt(at|st R) :=t := _mM1=1_ _[π]Mmt[(][m]=1[)]([R]a[(]tt[(][m][m][)][)]|. The goal of the agents is to collaborativelyst) parameterized by ωt = [ωt[(1)]; . . . ; ωt[(][M]_ [)]],

learn the optimal joint policy that maximizes the expected accumulated average reward[Q][M] _J(ω) :=_
P

(1 _γ)E_ _∞t=0_ _[γ][t][R][t]_ _s0_ _ξ_ _. Throughout, we consider the setting that the agents keep interacting_
_−_ _∼_
with the environment and observing a trajectory of MDP transition samples, which are further used
 P 
to learn the optimal joint policy.

3 SAMPLE AND COMMUNICATION-EFFICIENT DECENTRALIZED AC

In this section, we propose a decentralized actor-critic (AC) algorithm that is sample and
communication-efficient and avoids revealing agents’ local actions, policies and raw rewards .

We first consider a direct extension of the centralized AC to the decentralized case. As each agent m
has its own policy π[(][m][)], it aims to update the policy parameter ω[(][m][)] using the local policy gradient
_ω(m)_ _J(ω). Under linear approximation of the value function Vθ(s)_ _φ(s)[⊤]θ where φ(s) is the_
_∇_ _≈_
feature vector, the local policy gradient has the following stochastic approximation.


_Rt + γφ(s[′]t+1[)][⊤][θ]t[(][m][)]_ _−_ _φ(st)[⊤]θt[(][m][)]_ _ψt[(][m][)](a[(]t[m][)]|st),_ (1)
i


_ω(m)_ _J(ωt)_
_∇_ _≈_


where a[(]t[m][)] _πt[(][m][)](_ _st), st+1_ _ξ(_ _st, at), s[′]t+1_ (2)
_∼_ _·|_ _∼P_ _·|_ _[∼P][(][·|][s][t][, a][t][)][.]_

Here, θt[(][m][)] is agent m’s critic parameter and ψt[(][m][)](a[(]t[m][)] _st) =_ _ω(m) ln πt[(][m][)](a[(]t[m][)]_ _st) is the local_
_|_ _∇_ _|_
score function. It is clear that both θt[(][m][)] and ψt[(][m][)](a[(]t[m][)] _st) can be obtained/computed by agent_
_|_
_m using the local information. However, the average reward Rt requires agent m aggregating_
the local rewards from all the other agents, which raises concerns. In the existing literature on
decentralized AC, this issue is avoided by either 1) sharing the agents’ actions with each other instead
(70; 69; 8; 36; 72; 27; 19; 26; 11), yet the action information is also highly sensitive; or 2) learning a
parameterized model to estimate the average reward (70), which requires extra learning effort and
does not provide an accurate estimation. Hence, we are motivated to develop a simpler approach that
provides accurate estimation of the average reward while avoids sharing raw local rewards .


-----

**1. Efficient Policy Gradient Estimation. We propose a decentralized policy gradient estimation**
scheme that improves the sample and communication efficiency and avoids revealing the agents’ local
actions, policies and raw rewards . First, in order for each agent to estimate the average reward Rt in
eq. (1), we let each agent m generate a noisy local reward _Rt[(][m][)]_ = Rt[(][m][)](1 + e[(]t[m][)]) and share with
other agents, where e[(]t[m][)] _∼N_ (0, σm[2] [)][ 2][. The noise variance is determined by the agent based on its]
desired level. Specifically, every agent m first initializes its local estimation of the averaged reward[e]
(m) (m)
_Rt_ using its own noisy reward, i.e., Rt,0 [=][ e]Rt[(][m][)]. Then, each agent m performs decentralized
local averaging with its neighbors Nm for T _[′]_ iterations, i.e.,

(m) (m)
_Rt,ℓ+1_ [=][ P]m[′]∈Nm _[W][m,m][′][ R]t,ℓ_ _[,]_ _ℓ_ = 0, 1, . . ., T _[′]_ _−_ 1. (3)

(m) (m) (m)
After that, agent m obtains the final estimate Rt := Rt,T _[′][. It can be shown that][ R]t_ converges

to the averaged noisy reward _M1_ _Mm=1_ _Rt[(][m][)]_ exponentially fast. Ideally, by averaging these noisy

local rewards over the M agents, the variance of the noise in the final estimation will be scaled by a
factor of _M1_ [. Therefore, to obtain an accurate estimation, the network needs to have a sufficiently]P

[e]

large number of agents, which does not always hold in practice.

To address this issue, we let each agent m collect a mini-batch of N Markovian samples in each
iteration t to estimate the local policy gradient, which then takes the following form.


(t+1)N _−1_

_i=tN_

X


_∇b_ _ω(m)_ _J(ωt) = N[1]_


(m)
_i_ + γφ(s[′]i+1[)][⊤][θ]t[(][m][)] _φ(si)[⊤]θt[(][m][)]_ _ψt[(][m][)](a[(]i[m][)]_ _si),_ (4)
_−_ _|_
i


(m)
where Ri is an estimation of Ri obtained by agent m following the process described in eq. (3).

Intuitively, eachthe agents. Then, the mini-batch samples further help scale the noise variance by a factor of R(im) is corrupted by a zero-mean noise with variance O( _M[1]_ [)][ due to averaging over]N1 [.]

Consequently, with a sufficiently large batch size N, we can obtain an accurate estimation of the
averaged reward and hence the policy gradient. To summarize, our decentralized policy gradient
**estimation scheme has the following advantages.**

-  Avoid sharing raw rewards: The agents share only noisy rewards _Rt[(][m][)]_ with their neighbors, and
the noise variance can be adjusted based on the desired level such that Rt[(][m][)] is unknown to the other
agents. This is in contrast to other decentralized AC algorithms where the agents need to either[e]
share local actions, rewards or collaboratively learn an additional parameterized reward model.

-  Sample-efficient: The mini-batch updates help greatly suppress the noise variance of the local
policy gradient in (4) and improve its estimation accuracy. On the other hand, mini-batch policy
gradient also helps reduce the optimization variance caused by Markovian sampling and leads to a
good finite-time sample complexity as we prove later. We note that there is no trade-off between
noise variance and sample efficiency here, because for highly noisy local rewards we can choose a
large batch size to suppress the overall estimation error to the desired level.

-  Communication-efficient: The mini-batch updates also significantly reduce the communication
frequency as well as the complexity as we prove later. In comparison, the existing decentralized
AC requires to perform one communication round per Markovian sample.

**Remark. We note that the local mini-batch policy gradient update in eq. (4) can be computed in an**
_accumulative way by the agent when observing the mini-batch of transition samples on the fly. There_
_is no need to store all these samples and perform a large batch computation._

**2. Fully Decentralized Critic Update. The critic parameters of the agents are updated following**
the standard decentralized TD-type algorithm. Specifically, consider the t-th local critic update of
each agent m. It first collects a mini-batch of Nc Markovian samples. Then, starting from a fixed
initialization θt,[(][m]0 [)] [=][ θ][−][1][, agent][ m][ performs][ T][c][ iterations of decentralized TD updates as follows,]

where {st}t∈N follows the transition kernel P and a[(]t[m][)] _∼_ _πt[(][m][)](·|st): for t[′]_ = 0, 1, ..., Tc − 1,

(t+1)Nc−1

_θt,t[(][m][′][)]+1_ [=] _Wm,m′ θt,t[(][m][′][′][)]_ + _[β]_ _Ri[(][m][)]_ + γφ(si+1)[⊤]θt,t[(][m][′][)][ −] _[φ][(][s][i][)][⊤][θ]t,t[(][m][′][)]_ _φ(si). (5)_

_Nc_

_m[′]_ _m_ _i=tNc_

X∈N X h i

2More generally, any noise with zero mean and variance σm2 [will work.]


-----

Then, the updated critic parameter is set to be θt[(][m][)] := θt,T[(][m]c[)][. To further reduce the consensus error,]
we perform additional Tc[′] [steps of local model averaging, as also adopted in (][12][). The pseudo code of]
the entire decentralized AC algorithm is summarized in Algorithms 1 and 2 below.

**Algorithm 2 Decentralized TD (critic update)**

**Algorithm 1 Decentralized Actor-Critic**

**Initialize: Critic parameter θt,0 = θ** 1.

**Initialize: Actor-critic parameters ω0, θ** 1. _−_
_−_ **for critic iterations t[′]** = 0, 1, . . ., Tc 1 do
**for actor iterations t = 0, 1, . . ., T −** 1 do ▶ Collect Nc Markovian samples following −

▶ **Critic update on θt: by Algorithm 2.** policy πt and transition kernel .
▶ Collect N Markovian samples by eq. (2). **for agents m = 1, ..., M in parallel P** **do**
**for agents m = 1, ..., M in parallel do**

▶ Send local critic parameters.

▶ Send noisy local rewards and perform ▶ Decentralized TD update in eq. (5).
_T_ _[′]_ local average steps following eq. (3). **end**
▶ Compute the estimated local policy

**end**

gradient _∇ω(m)_ _J(ωt) following eq. (4)._ **for iterations t[′]** = Tc, ..., Tc + Tc[′]
▶ **Actor update on ωt:** **for agents m = 1, ..., M in parallel[−]** [1][ do] do
_ωt[(]+1[m][)]_ [=][ ω][b]t[(][m][)] + α∇[b] _ω(m)_ _J(ωt)._ ▶ _θt,t[(][m][′][)]+1_ [=][ P]m[′] _m_ _[W][m,m][′][ θ]t,t[(][m][′][′][ .][)]_
**end** _∈N_

**end**

**end**

**end**

uniform
**Output: ω eT** [with][ e]T _∼_ _{1, 2, . . ., T_ _}._ **Output: θt = θt,Tc+T ′c** [.]

4 FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED AC

In this section, we analyze the finite-time convergence of Algorithm 1 and characterize the sample
and communication complexities. All the notations and universal constants are summarized in
Appendices A & F respectively. We first introduce the following standard assumptions that have been
widely adopted in the existing literature.
**Assumption 1. Regarding the transition kernels P, Pξ, denote µω, νω respectively as their stationary**
_state distributions under policy πω and denote P, Pξ respectively as their marginal state distributions._
_Then, there exist constants κ > 0 and ρ ∈_ (0, 1) such that for all t ≥ 0,

sups _dT V_ P (st | s0 = s), µω _≤_ _κρ[t],_ sups _dT V_ Pξ (st | s0 = s), νω _≤_ _κρ[t]_ (6)
_∈S_ _∈S_

_where dT V (P, Q) denotes the total-variation distance between probability measures _     _P and Q._
**Assumption 2. There exist constants Cψ, Lψ, Lπ > 0 such that for all ω,** _ω ∈_ Ω, s ∈S and a ∈A,
_∥ψω(a|s)∥≤_ _Cψ, ∥ψω[(][a][|][s][)][ −]_ _[ψ]ω[(][a][|][s][)][∥≤]_ _[L]ψ[∥]ω[e] −_ _ω∥_ _and dTV_ _πω[(][·|][s][)][, π]ω[(][·|][s][)]_ _≤_ _Lπ∥ω −_ _ω∥._

**Assumption 3. There exists Rmax > 0 such that for any agent m and any Markovian sample e**

e   e 

(s, a, s[′]), we have 0 _R[(][m][)](s, a, s[′])_ _Rmax._ e
_≤_ _≤_

**Assumption 4. The feature vectors satisfy ∥φ(s)∥≤** 1 for all s ∈S. There exists a constant λφ > 0
_such that λmin_ Es _µω_ [φ(s)φ(s)[⊤]] _λφ for all ω._
_∼_ _≥_

**Assumption 5.  The communication matrix** _W_ R[M] _[×][M]_ _of the decentralized network is doubly_
_∈_
_stochastic, and its second largest singular value satisfies σW_ [0, 1).
_∈_

Assumption 1 has been widely considered in the existing literature (4; 38; 63; 58; 42; 60; 12) and it
holds for any time-homogeneous Markov chains with finite-state space and any uniformly ergodic
Markov chains. Assumption 2 introduces boundedness and Lipschitzness to the policy and its
associated score function (65; 60), and holds for many parameterized policies such as Gaussian policy
(25) and Boltzman policy (16). Assumption 4 can always hold by normalizing the feature vector φ(s)
Assumption 5 is widely used in decentralized optimization (43; 41) and multi-agent reinforcement
learning (46; 53; 12), which ensures that all the decentralized agents can reach a global consensus.

With the above assumptions, we obtain the following finite-time convergence result of the decentralized AC algorithm. Throughout, we follow (60; 56) and define the critic approximation error as
_ζapprox[critic]_ [:= sup]ω [E][s][∼][ν]ω [(][V][ω][(][s][)][−][φ][(][s][)][⊤][θ]ω[∗] [)][2][ where][ θ]ω[∗] [is the optimal critic parameter (see its definition]
right before Lemma D.3 in Appendix D). We also define sample complexity as the total number of
Markovian samples required for achieving E[∥∇J(ω)∥[2]] ≤ _ϵ. All the universal constants are listed_
in Appendix F.


-----

**Theorem 1. Let Assumptions 1–5 hold and adopt the hyperparameters of the decentralized TD**
_in Algorithm 2 following Lemma D.4. Choose α ≤_ 4L1J _[,][ T][ ′][ ≥]_ 2 lnln σ MW[−][1] _[. Then, the output of the]_

_decentralized AC in Algorithm 1 has the following convergence rate._

_Tc_

E _J(ωT_ [)] +4(c4σW[2][T][ ′] [+][c][5][β][2][σ]W[2][T][ ′]c [)+4][c][6] 1 + [4][c][7] +64Cψ[2] _[ζ]approx[critic]_ _[.]_
_∇_ _≤_ [4][R]Tα[max] _−_ _[λ]8[B]_ _[β]_ _N_ [+ 4]N[c]c[8]
h  
e

_Moreover, to achieve[2][i]_ E _∇J(ωT_ [)] _≤_ _ϵ for any ϵ ≥_ 128Cψ[2] _[ζ]approx[critic]_ _[, we can choose][ T, N, N][c]_ [=]
_O(ϵ[−][1]) and Tc, Tc[′][, T][ ′][ =][ O][(ln][ ϵ][−] e[1][)][. Consequently, the overall sample complexity is][ T]_ [(][T][c][N][c] [+][N] [) =]
(ϵ[−][2] ln ϵ[−][1]), and the communication complexities for synchronizing linear model parameters and[2][]
_O_
_rewards are T_ (Tc + Tc[′][) =][ O][(][ϵ][−][1][ ln][ ϵ][−][1][)][ and][ TT][ ′][ =][ O][(][ϵ][−][1][ ln][ ϵ][−][1][)][, respectively.]

**Remark. We note that the constraint on ϵ is naturally induced by the critic approximation error. In**
_particular, if this approximation errors vanish, for example, when the dimension of features equals the_
_number of states and the parameterized policy space is sufficiently expressive, then we can achieve_
_arbitrarily small target accuracy._

To the best of our knowledge, Theorem 1 provides the first finite-time analysis of decentralized AC
under Markovian sampling. To elaborate, under any pre-specified variance σm[2] [of the reward noise, our]
result shows that the gradient norm asymptotically converges to the order O(N _[−][1]_ + Nc[−][1] + ζapprox[critic] [)][,]
which can be made arbitrarily close to the linear model approximation error ζapprox[critic] [by choosing]
sufficiently large batch sizes N, Nc. In particular, exact gradient convergence can be achieved when
there is no model approximation error. The overall sample complexity of our decentralized AC is
_O(ϵ[−][2]_ ln ϵ[−][1]), matching the state-of-the-art complexity result for centralized AC (60). Moreover,
with proper choices of the batch sizes N, Nc = O(ϵ[−][1]), the overall communication complexity is
significantly reduced to O(ϵ[−][1] ln ϵ[−][1]).

The proof of Theorem 1 relies on developing several new algorithmic and technical developments
to reduce the communication complexity of both the decentralized actor and critic updates while
establishing tight convergence error bounds for both components. We further elaborate on these novel
technical developments below.

-  To achieve an overall reduced communication complexity, we adopt mini-batch updates in both the
actor and critic steps to reduce the communication frequency, as opposed to the single sample-based
update adopted in the existing work on decentralized TD learning (46). Specifically, in the analysis
of the decentralized TD described in Algorithm 2 (see Lemma D.4), the mini-batch updates with
batch size O(ϵ[−][1]) substantially improve the communication complexity from O(ϵ[−][1] ln ϵ[−][1]) to
_O(ln ϵ[−][1]) while help achieve the state-of-the-art sample complexity. Eventually, this together_
with the mini-batch updates in the decentralized actor steps help achieve the desired overall low
communication complexity.

-  To achieve the state-of-the-art overall sample complexity, it is critical that the policy gradient
vanishes fast, which further requires a fast convergence of the decentralized TD learning. However,
although the standard Tc decentralized mini-batch TD updates can yield a small convergence error
for the global critic model (i.e., the average of all local critic models), it still suffers from a relatively
large consensus error. To resolve this issue, we introduce an additional Tc[′] [global consensus steps]
in Algorithm 2 to reduce the consensus error. It is proved that a small number O(ln ϵ[−][1]) of such
steps suffices to yield a desired TD error.

-  We inject random noises into the local raw rewards Rt[(][m][)] to protect the information. These noises
introduce additional Markovian bias and variance to the local policy gradients in (4). Fortunately,
as proved in Lemma D.6, by applying mini-batch policy gradient updates, we are able to control
the bias and variance induced by the noisy rewards to an acceptable level that does not affect the
overall sample and communication complexities.

5 DECENTRALIZED NATURAL AC

Natural actor-critic (NAC) is a popular variant of the AC algorithm. It utilizes a Fisher information
matrix to perform a natural policy gradient update, which helps attain the globally optimal solution in
terms of the function value convergence. In this section, we develop a fully decentralized version of
the NAC algorithm that is sample and communication-efficient.


-----

**Algorithm 3 Decentralized Natural Actor-Critic**
**Initialize: Actor-critic parameters ω0, θ** 1, natural policy gradient h 1.
_−_ _−_
**for actor iterations t = 0, 1, . . ., T −** 1 do

▶ **Critic update on θt: by Algorithm 2.**
**for agents m = 1, ..., M in parallel do**

**for iterations k = 0, 1, . . ., K −** 1 do

▶ Collect Nk Markovian samples following eq. (2).
▶ Send _Ri[(][m][)]_ and zi,ℓ[(][m][)] and perform T _[′]_ and Tz local average steps, respectively.

▶ Estimate local gradient _∇ω(m)_ _fωt_ (ht,k) following eqs. (8) and (4).
▶ Perform SGD update in eq. ([e] 9).
**end**

[b]
▶ **Actor update on ωt: ωt[(]+1[m][)]** [=][ ω]t[(][m][)] + αh[(]t[m][)].
**end**
**end**

uniform
**Output: ω eT** [with][ e]T _∼_ _{1, 2, . . ., T_ _}._

A major challenge of developing fully decentralized NAC algorithm is computing the inverse Fisher
information matrix-vector product involved in the natural policy gradient update. To explain, first
recall the exact natural policy gradient update of the centralized NAC algorithm, i.e., ωt+1 =
_ωt + αF_ (ωt)[−][1]∇J(ωt), where F (ωt) := Est∼νωt _,at∼πt(·|st)_ _ψt(at|st)ψt(at|st)[⊤][]_ is the Fisher
information matrix. However, in the multi-agent case, it is challenging to perform the natural policy
gradient update in a decentralized manner. This is because the Fisher information matrix _F_ (ωt) is
based on the concatenated multi-agent score vector ψt(at _st) = [ψt[(1)](a[(1)]t_ _t_ (a[(]t[M] [)] _st)]_
and the inverse matrix-vector product F (ωt)[−][1] _J(ωt) is not separable with regard to each agent’s|_ _[|][s][t][);][ ...][;][ ψ][(][M]_ [)] _|_
_∇_
policy parameter dimensions. Next, we develop a fully decentralized scheme to implement the
natural policy gradient update in the multi-agent setting.

First, note that the natural policy gradient update h(ωt) := F (ωt)[−][1] _J(ωt) is equivalent to the_
_∇_
solution of a quadratic program, i.e.,

_h(ωt) = arg min_ (7)
_h_ _[f][ω][t]_ [(][h][) := 1]2 _[h][⊤][F]_ [(][ω][t][)][h][ −∇][J][(][ω][t][)][⊤][h.]


Therefore, we can apply K steps of SGD with Markovian sampling to solve this problem and obtain
an estimated natural policy gradient update. Specifically, starting from the initialization ht,0 = ht 1
_−_
(obtained in the previous iteration), in theMarkovian samples to estimate ∇fωt (h) as k-th SGD step, we sample a mini-batchN1k _i∈Bt,k_ _[ψ][t][(][a][i][|][s][i][)][ψ][t][(][a][i][|][s][i][)][⊤][h][t,k][ −]_ _B∇[b]_ _Jt,k(ω[3]t;of B Nt,k)k,_

where _J(ωt;_ _t,k) is estimated in the same decentralized way as eq. (4) using the mini-batch_

P

_∇_ _B_
of samples _t,k. In particular, each agent m needs to compute the corresponding local gradi-_
1 _B_
ent _Nk_ [b] _i∈Bt,k_ _[ψ]t[(][m][)](a[(]i[m][)]|si)_ _ψt(ai|si)[⊤]ht,k_ _−_ _∇[b]_ _ω(m)_ _J(ωt; Bt,k), in which ψt[(][m][)](a[(]i[m][)]|si) and_

_ω(m)_ _JP(ωt;_ _t,k) can be computed/estimated by the agent_  _m. Then, it suffices to obtain an estimate_
_∇_ _B_
of the scalar ψt(ai|si)[⊤]ht,k, which can be rewritten as _m=1_ _[ψ]t[(][m][)](a[(]i[m][)]|si)[⊤]h[(]t,k[m][)][. This summa-]_
tion can be easily estimated by the decentralized agents through local averaging. Specifically, eachb
agent m locally computes zi,[(][m]0 [)] = ψt[(][m][)](a[(]i[m][)]|si)[⊤]h[(]t,k[m][)][P][and performs][M] _[ T][z][ steps of local averaging,]_

i.e., zi,ℓ[(][m]+1[)] [=][ P]m[′]∈Nm _[W][m,m][′][ z]i,ℓ[(][m][′][)],_ _ℓ_ = 0, 1, . . ., Tz − 1. After that, the quantity Mzi,T[(][m]z[)] [can be]

proven to converge to the desired summation _m=1_ _[ψ]t[(][m][)](a[(]i[m][)]|si)[⊤]h[(]t,k[m][)]_ [exponentially fast. Finally,]
the local gradient for agent m is approximated as

[P][M]

_∇ω(m)_ _fωt_ (ht,k) = _N[M]k_ _ψt[(][m][)](a[(]i[m][)]|si)zi,T[(][m]z[)]_ _∇ω(m)_ _J(ωt; Bt,k)._ (8)

_i∈BXt,k_ _[−]_ [b]

b

Then, the agent m performs the following SGD updates to obtain h[(]t[m][)] := h[(]t,K[m][)][.]

_h[(]t,k[m]+1[)]_ [=][ h]t,k[(][m][)] _[−]_ _[η][ b]∇ω(m)_ _fωt_ (ht,k), _k = 0, ..., K −_ 1. (9)

3Specifically, the mini-batch Bt,k contains sample indices _tN +_ _k[′]=0_ _[N][k][′]_ _[, . . ., tN][ +][ P]k[k][′]=0_ _[N][k][′][ −]_ [1] .
 [P][k][−][1]


-----

We emphasize that the above mini-batch SGD updates use Markovian samples. In particular, as
shown in Section 6, we need to develop an adaptive batch size scheduling scheme for this SGD in
order to reduce its sample complexity. We summarize the decentralized NAC in Algorithm 3.

6 FINITE-TIME CONVERGENCE ANALYSIS OF DECENTRALIZED NAC

To analyze the decentralized NAC, we introduce the following additional standard assumptions.
**Assumption 6. There exists a constant λF > 0 such that λmin** _F_ (ω) _≥_ _λF > 0, ∀ω ∈_ Ω.

**Assumption 7. There exists C∗** _> 0 such that for ω[∗]_ = arg max  _ω∈Ω_ J(ω) and any ω ∈ Ω,

_νω∗_ (s)πω∗ (a _s)_ 2
Es _νω,a_ _πω(_ _s)_ _|_ _C_ [2]
_∼_ _∼_ _·|_ _νω(s)πω(a_ _s)_ _≤_ _∗_ _[.]_
h _|_  i


Assumption 6 ensures that the Fisher information matrix F (ω) is uniformly positive definite, and
is also considered in (65; 29; 62). Assumption 7 regularizes the discrepancy between the stationary
state-action distributions νω∗ (s)πω∗ (a _s) and νω(s)πω(a_ _s) (54; 59)._
_|_ _|_

We obtain the following finite-time convergence result of the decentralized NAC algorithm.
Throughout, we follow (54; 60; 62) and define the actor approximation error as ζapprox[actor] [:=]

2
supω minhEs _νω,a_ _πω(_ _s)_ _ψω(a_ _s)[⊤]h_ _Aω(s, a)_ . All universal constants are listed in Ap_∼_ _∼_ _·|_ _|_ _−_
pendix F.
   
**Theorem 2. Let Assumptions 1–7 hold and adopt the hyperparameters of the decentralized TD in**

_Algorithm 2 following Lemma D.4. Choose hyperparameters α ≤_ min 1, 4LλJ[2]FCψ[2] _[,]_ 2CLψ[2]J _, β ≤_ 1,

ln M 1 ln(3DJ _Cψ[2]_ [)] ln 3 2304Cψ[4] [(][κ][+1][−][ρ][)]
_T_ _[′]_ _≥_ 2 ln σW[−][1] _[,][ η][ ≤]_ 2Cψ[2] _[,][ T][z][ ≥]_ ln σW[−][1] _, K ≥_ ln(1−ηλF /2)[−][1][,][ N][ ≥] _ηλ _ [5]F [(1][−][ρ][)(1][−][ηλ][F][ /][2)][(][K][−][1)][/][2]

_and Nk ∝_ (1 − _ηλF /2)[−][k/][2]. Then, the output of Algorithm 3 satisfies_

_J(ω[∗]) −_ E _J(ωT_ [)] _≤_ _Tα[c][17]_ [+][ c][18] 1 − _[ηλ]2[F]_ (K−1)/4 + c19σW[T][z] [+][ c][20][σ]W[T][ ′] [+][ c][21][βσ]W[T][ ′]c [+][ c]√[23]Nc
 e   _Tc/2_

+ c22 1 + Cψ _c16ζapprox[critic]_ [+][ c][24][ζ]approx[critic] [+][ C] _[∗][q]ζapprox[actor]_ _[.]_
_−_ _[λ]8[B]_ _[β]_
  q

_Moreover, to achieve J(ω[∗]) −_ E _J(ωT_ [)] _≤_ _ϵ for any ϵ ≥_ 2Cψ _c16ζapprox[critic]_ [+ 2][c][24][ζ]approx[critic] [+]

2C _[∗][p]ζapprox[actor]_ _[, we can choose][ T][ =][ O]_ [(][ϵ][−] b[1][)][,][ N, N][c] [=][ O][(][ϵ][−][2][)][,][ T][c][, T]c[ ′][, T]q[ ′][, T][z][, K][ =][ O][(ln][ ϵ][−][1][)][. Con-]
_sequently, the overall sample complexity is T_ (TcNc + N ) = O(ϵ[−][3] ln ϵ[−][1]), and the communication
_complexities for synchronizing linear model parameters and rewards are T_ (Tc +Tc[′][) =][ O][(][ϵ][−][1][ ln][ ϵ][−][1][)]
_and TT_ _[′]_ = O(ϵ[−][1] ln ϵ[−][1]), respectively.

Theorem 2 provides the first finite-time analysis of fully decentralized natural AC algorithm. Our
result proves that the function value optimality gap converges to the order O _Nc[−][1][/][2]_ + _ζapprox[critic]_ [+]

actor
_ζapprox_, which can be made arbitrarily close to the actor and critic approximation error by choosing  q
a sufficiently large batch size Nc. In particular, exact global optimum can be achieved when there
pis no model approximation error. We note that the overall sample complexity of our decentralized
NAC is O(ϵ[−][3] ln ϵ[−][1]), matching the state-of-the-art complexity result for centralized NAC (60).
Moreover, with the mini-batch updates, the overall communication complexity is significantly reduced
to O(ϵ[−][1] ln ϵ[−][1]).

Similar to that of Theorem 1, our analysis of Theorem 2 also leverages the mini-batch decentralized
TD updates to reduce the communication complexity and deal with the bias and variance of the
local policy gradient introduced by noisy rewards. In addition, decentralized NAC uses mini-batch
SGD with Markovian sampling to solve the quadratic problem in eq. (7). Here, we use a special
geometrically increasing batch size scheduling scheme, i.e., Nk ∝ (1 − _ηλF /2)[−][k/][2], to achieve_
the best possible convergence rate under the total sample budget that _k=1_ _[N][k][ =][ N][ and obtain]_
the desired overall sample complexity result. Such an analysis of SGD with Markovian sampling
under adaptive batch size scheduling has not been studied in the literature and can be of independent

[P][K]
interests.


-----

Figure 1: Comparison of accumulated discounted reward J(ωt) among decentralized AC-type
algorithms in a ring network with sparse connections .

7 EXPERIMENTS

We simulate a fully decentralized ring network with 6 agents. Please refer to Appendix E for
detailed environment setup. We implement four decentralized AC-type algorithms and compare their
performance, namely, our Algorithms 1 and 3, an existing decentralized AC algorithm (Algorithm 2 of
(70)) that uses a linear model to parameterize the agents’ averaged reward (we name it DAC-RP1 for
decentralized AC with reward parameterization), and our proposed modified version of DAC-RP1 to
incorporate minibatch, which we refer to as DAC-RP100 with batch size N = 100. For our Algorithm

1, we choose T = 500, Tc = 50, Tc[′] [= 10][,][ N][c] [= 10][,][ T][ ′][ =][ T][z] [= 5][,][ β][ = 0][.][5][,][ {][σ][m][}][6]m=1 [= 0][.][1][, and]
consider batch size choices N = 100, 500, 2000. Algorithm 3 uses the same hyperparameters as
those of Algorithm 1 except that T = 2000 in Algorithm 3. For DAC-RP1, we set learning rates
_βθ = 2(t + 1)[−][0][.][9], βv = 5(t + 1)[−][0][.][8]_ and batch size N = 1 as mentioned in (70). The modified
DAC-RP100 adopts the same learning rates as Algorithm 1 with N = 100.

Figure 1 plots the accumulated reward J(ωt) v.s. communication and sample complexity. Each
curve includes 10 repeated experiments, and its upper and lower envelopes denote the 95% and 5%
percentiles of the 10 repetitions, respectively. For our decentralized AC algorithm (left two figures),
its communication and sample complexities for achieving a high accumulated reward are significantly
reduced under a larger batch size N . This matches our theoretical understanding in Theorem 1 that
a large N helps reduce the communication frequency and policy gradient variance. In comparison,
DAC-RP1 (with N = 1) has almost no improvement on the accumulated reward. Moreover, although
the modified DAC-RP100 (with N = 100) outperforms DAC-RP1, its performance is much worse
than our Algorithm 1 with N = 100. This performance gap is due to two reasons: (i) Both DAC-RP
algorithms suffer from an inaccurate parameterized estimation of the averaged reward, and their mean
relative reward errors are over 100%. In contrast, our noisy averaged reward estimation achieves a
mean relative error in the range of 10[−][5] _∼_ 10[−][4];(ii) Both DAC-RP algorithms apply only a single
TD update per-round, and hence suffers from a large mean relative TD error (about 2% and 1%
for DAC-RP1 and DAC-RP100, respectively)whereas our algorithms perform multiple TD learning
updates per-round and achieve a smaller mean relative TD error (about 0.3%). For our decentralized
NAC algorithm (right two figures), one can make similar observations and conclusions.

8 CONCLUSION

We developed fully-decentralized AC and NAC algorithms that are efficient and do not reveal agents’
local actions and policies . The agents share noisy reward information and adopt mini-batch updates
to improve sample and communication efficiency. Under Markovian sampling and linear function
approximation, we proved that our decentralized AC and NAC algorithms achieve the state-of-the-art
sample complexities O(ϵ[−][2] ln ϵ[−][1]) and O(ϵ[−][3] ln ϵ[−][1]), respectively, and they both achieve a small
communication complexity O(ϵ[−][1] ln ϵ[−][1]). Numerical experiments demonstrate that our algorithms
achieve better sample and communication complexity than the existing decentralized AC algorithm
that adopts reward parameterization.


-----

REFERENCES

[1] A. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan. On the theory of policy gradient methods:
Optimality, approximation, and distribution shift. ArXiv:1908.00261, 2019.

[2] C. Alfano and P. Rebeschini. Dimension-free rates for natural policy gradient in multi-agent
reinforcement learning. ArXiv:2109.11692, 2021.

[3] Q. Bai, M. Agarwal, and V. Aggarwal. Joint optimization of multi-objective reinforcement
learning with policy gradient based algorithm. ArXiv:2105.14125, 2021.

[4] J. Bhandari, D. Russo, and R. Singal. A finite time analysis of temporal difference learning with
linear function approximation. In Proc. Conference on Learning Theory (COLT), volume 75,
pages 1691–1692, 2018.

[5] S. Bhatnagar. An actor–critic algorithm with function approximation for discounted cost
constrained markov decision processes. Systems & Control Letters, 59(12):760–766, 2010.

[6] S. Bhatnagar, M. Ghavamzadeh, M. Lee, and R. S. Sutton. Incremental natural actor-critic
algorithms. In Proc. Advances in Neural Information Processing Systems (NeurIPS), volume 20,
pages 105–112, 2007.

[7] S. Bhatnagar, R. S. Sutton, M. Ghavamzadeh, and M. Lee. Natural actor–critic algorithms.
_Automatica, 45(11):2471–2482, 2009._

[8] G. Bono, J. S. Dibangoye, L. Matignon, F. Pereyron, and O. Simonin. Cooperative multi-agent
policy gradient. In Proc. Joint European Conference on Machine Learning and Knowledge
_Discovery in Databases (ECML PKDD), pages 459–476, 2018._

[9] L. Cassano, K. Yuan, and A. H. Sayed. Multi-agent fully decentralized value function learning
with linear convergence rates. IEEE Transactions on Automatic Control, 2020.

[10] B. Chalaki and A. A. Malikopoulos. A hysteretic q-learning coordination framework for
emerging mobility systems in smart cities. ArXiv:2011.03137, 2020.

[11] B. Chen, M. Xu, Z. Liu, L. Li, and D. Zhao. Delay-aware multi-agent reinforcement learning.
_ArXiv:2005.05441, 2020._

[12] Z. Chen, Y. Zhou, and R. Chen. Multi-agent off-policy td learning: Finite-time analysis with
near-optimal sample complexity and communication complexity. ArXiv:2103.13147, 2021.

[13] C. Daskalakis, D. J. Foster, and N. Golowich. Independent policy gradient methods for
competitive reinforcement learning. ArXiv:2101.04233, 2021.

[14] T. Doan, S. Maguluri, and J. Romberg. Finite-time analysis of distributed TD(0) with linear function approximation on multi-agent reinforcement learning. In Proc. International Conference
_on Machine Learning (ICML), volume 97, pages 1626–1635, 09–15 Jun 2019._

[15] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson. Counterfactual multi-agent
policy gradients. In Proc. Association for the Advancement of Artificial Intelligence (AAAI),
volume 32, 2018.

[16] A. Ghosh and V. Aggarwal. Model free reinforcement learning algorithm for stationary mean
field equilibrium for multiple types of agents. ArXiv:2012.15377, 2020.

[17] D. Hennes, D. Morrill, S. Omidshafiei, R. Munos, J. Perolat, M. Lanctot, A. Gruslys, J.-B.
Lespiau, P. Parmas, E. Duéñez-Guzmán, et al. Neural replicator dynamics: Multiagent learning
via hedging policy gradients. In Proc. International Conference on Autonomous Agents and
_MultiAgent Systems (AAMAS), pages 492–501, 2020._

[18] P. Heredia, H. Ghadialy, and S. Mou. Finite-sample analysis of distributed q-learning for
multi-agent networks. In 2020 American Control Conference (ACC), pages 3511–3516, 2020.

[19] P. C. Heredia and S. Mou. Distributed multi-agent reinforcement learning by actor-critic method.
_IFAC-PapersOnLine, 52(20):363–368, 2019._


-----

[20] S. M. Kakade. A natural policy gradient. In Proc. Advances in Neural Information Processing
_Systems (NeurIPS), volume 14, 2001._

[21] S. Kar, J. M. F. Moura, and H. V. Poor. Qd-learning: A collaborative distributed strategy for
multi-agent reinforcement learning through consensus + innovations. IEEE Transactions on
_Signal Processing, 61(7):1848–1862, 2013._

[22] V. Konda. Actor-critic algorithms (ph.d. thesis). Department of Electrical Engineering and
_Computer Science, Massachusetts Institute of Technology, 2002._

[23] V. R. Konda and J. N. Tsitsiklis. Actor-critic algorithms. In Proc. Advances in Neural
_Information Processing Systems (NeurIPS), pages 1008–1014, 2000._

[24] V. Krishnamurthy, M. Maskery, and G. Yin. Decentralized adaptive filtering algorithms for
sensor activation in an unattended ground sensor network. IEEE Transactions on Signal
_Processing, 56(12):6086–6101, 2008._

[25] H. Kumar, A. Koppel, and A. Ribeiro. On the sample complexity of actor-critic method for
reinforcement learning with function approximation. ArXiv:1910.08412, 2019.

[26] Y. Lin, Y. Luo, K. Zhang, Z. Yang, Z. Wang, T. Basar, R. Sandhu, and J. Liu. An asynchronous multi-agent actor-critic algorithm for distributed reinforcement learning. In NeurIPS
_Optimization Foundations for Reinforcement Learning Workshop, 2019._

[27] Y. Lin, K. Zhang, Z. Yang, Z. Wang, T. Ba¸sar, R. Sandhu, and J. Liu. A communication-efficient
multi-agent actor-critic algorithm for distributed reinforcement learning. In 2019 IEEE 58th
_Conference on Decision and Control (CDC), pages 5562–5567, 2019._

[28] R. Liu and A. Olshevsky. Distributed td (0) with almost no communication. ArXiv:2104.07855,
2021.

[29] Y. Liu, K. Zhang, T. Basar, and W. Yin. An improved analysis of (variance-reduced) policy gradient and natural policy gradient methods. In Proc. Advances in Neural Information Processing
_Systems (NeurIPS), volume 33, pages 7624–7636, 2020._

[30] R. Lowe, Y. Wu, A. Tamar, J. Harb, P. Abbeel, and I. Mordatch. Multi-agent actor-critic for
mixed cooperative-competitive environments. ArXiv:1706.02275, 2017.

[31] Y. Luo, Z. Yang, Z. Wang, and M. Kolar. Natural actor-critic converges globally for hierarchical
linear quadratic regulator. ArXiv:1912.06875, 2019.

[32] X. Lyu, Y. Xiao, B. Daley, and C. Amato. Contrasting centralized and decentralized critics in
multi-agent reinforcement learning. ArXiv:2102.04402, 2021.

[33] X. Ma, Y. Yang, C. Li, Y. Lu, Q. Zhao, and Y. Jun. Modeling the interaction between agents in
cooperative multi-agent reinforcement learning. ArXiv:2102.06042, 2021.

[34] S. V. Macua, J. Chen, S. Zazo, and A. H. Sayed. Distributed policy evaluation under multiple
behavior strategies. IEEE Transactions on Automatic Control, 60(5):1260–1274, 2014.

[35] D. J. Ornia and M. Mazo Jr. Event-based communication in multi-agent distributed q-learning.
_ArXiv:2109.01417, 2021._

[36] J. Perolat, B. Piot, and O. Pietquin. Actor-critic fictitious play in simultaneous move multistage
games. In Proc. International Conference on Artificial Intelligence and Statistics, pages 919–
928, 2018.

[37] J. Peters and S. Schaal. Natural actor-critic. Neurocomputing, 71(7-9):1180–1190, 2008.

[38] S. Qiu, Z. Yang, J. Ye, and Z. Wang. On the finite-time convergence of actor-critic algorithm.
In NeurIPS Optimization Foundations for Reinforcement Learning Workshop, 2019.

[39] W. Qiu, X. Wang, R. Yu, R. Wang, X. He, B. An, S. Obraztsova, and Z. Rabinovich. Rmix:
Learning risk-sensitive policies forcooperative reinforcement learning agents. In Proc. Advances
_in Neural Information Processing Systems (NeurIPS), 2021._


-----

[40] C. Qu, S. Mannor, H. Xu, Y. Qi, L. Song, and J. Xiong. Value propagation for decentralized
networked deep multi-agent reinforcement learning. In Proc. Advances in Neural Information
_Processing Systems (NeurIPS)), pages 1184–1193, 2019._

[41] R. Saha, S. Rini, M. Rao, and A. Goldsmith. Decentralized optimization over noisy, rateconstrained networks: How to agree by talking about how we disagree. ArXiv:2010.11292,
2020.

[42] M. Shaocong, Z. Yi, and Z. Shaofeng. Variance-reduced off-policy tdc learning: Non-asymptotic
convergence analysis. In Proc. Advances in Neural Information Processing Systems (NeurIPS),
2020.

[43] N. Singh, D. Data, J. George, and S. Diggavi. Squarm-sgd: Communication-efficient momentum
sgd for decentralized optimization. ArXiv:2005.07041, 2020.

[44] S. Srinivasan, M. Lanctot, V. Zambaldi, J. Pérolat, K. Tuyls, R. Munos, and M. Bowling. Actorcritic policy optimization in partially observable multiagent environments. ArXiv:1810.09026,
2018.

[45] M. S. Stankovi´c and S. S. Stankovi´c. Multi-agent temporal-difference learning with linear
function approximation: Weak convergence under time-varying network topologies. In Proc.
_American Control Conference (ACC), pages 167–172, 2016._

[46] J. Sun, G. Wang, G. B. Giannakis, Q. Yang, and Z. Yang. Finite-sample analysis of decentralized temporal-difference learning with linear function approximation. In Proc. International
_Conference on Artificial Intelligence and Statistics (AISTATS), pages 4485–4495, 2020._

[47] W. Suttle, Z. Yang, K. Zhang, Z. Wang, T. Basar, and J. Liu. A multi-agent off-policy actor-critic
algorithm for distributed reinforcement learning. ArXiv:1903.06372, 2019.

[48] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction (Second Edition). 2018.

[49] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In Proc. Advances in Neural Information
_Processing Systems (NeurIPS), volume 12, 2000._

[50] R. S. Sutton, D. A. McAllester, S. P. Singh, Y. Mansour, et al. Policy gradient methods for
reinforcement learning with function approximation. In Proc. Advances in Neural Information
_Processing Systems (NeurIPS), volume 99, pages 1057–1063, 1999._

[51] F. Venturini, F. Mason, F. Pase, F. Chiariotti, A. Testolin, A. Zanella, and M. Zorzi. Distributed
reinforcement learning for flexible and efficient uav swarm control. ArXiv:2103.04666, 2021.

[52] H.-T. Wai, Z. Yang, Z. Wang, and M. Hong. Multi-agent reinforcement learning via double
averaging primal-dual optimization. In Proc. Advances in Neural Information Processing
_Systems (NeurIPS), pages 9672–9683, 2018._

[53] G. Wang, S. Lu, G. Giannakis, G. Tesauro, and J. Sun. Decentralized td tracking with linear
function approximation and its finite-time analysis. In Proc. Advances in Neural Information
_Processing Systems (NeurIPS), volume 33, 2020._

[54] L. Wang, Q. Cai, Z. Yang, and Z. Wang. Neural policy gradient methods: Global optimality and
rates of convergence. ArXiv:1909.01150, 2019.

[55] W. Wang, J. Hao, Y. Wang, and M. Taylor. Achieving cooperation through deep multiagent
reinforcement learning in sequential prisoner’s dilemmas. In Proc. of International Conference
_on Distributed Artificial Intelligence (DAI), pages 1–7, 2019._

[56] Y. F. Wu, W. ZHANG, P. Xu, and Q. Gu. A finite-time analysis of two time-scale actor-critic
methods. In Proc. Advances in Neural Information Processing Systems (NeurIPS), volume 33,
pages 17617–17628, 2020.

[57] B. Xiao, B. Ramasubramanian, and R. Poovendran. Shaping advice in deep multi-agent
reinforcement learning. ArXiv:2103.15941, 2021.


-----

[58] T. Xu and Y. Liang. Sample complexity bounds for two timescale value-based reinforcement
learning algorithms. ArXiv:2011.05053, 2020.

[59] T. Xu, Y. Liang, and G. Lan. A primal approach to constrained policy optimization: Global
optimality and finite-time analysis. ArXiv:2011.05869, 2020.

[60] T. Xu, Z. Wang, and Y. Liang. Improving sample complexity bounds for (natural) actor-critic
algorithms. In Proc. Advances in Neural Information Processing Systems (NeurIPS), volume 33,
2020.

[61] T. Xu, Z. Wang, and Y. Liang. Non-asymptotic convergence analysis of two time-scale (natural)
actor-critic algorithms. ArXiv:2005.03557, 2020.

[62] T. Xu, Z. Yang, Z. Wang, and Y. Liang. Doubly robust off-policy actor-critic: Convergence and
optimality. ArXiv:2102.11866, 2021.

[63] T. Xu, S. Zou, and Y. Liang. Two time-scale off-policy td learning: Non-asymptotic analysis over
markovian samples. In Proc. Advances in Neural Information Processing Systems (NeurIPS),
pages 10634–10644, 2019.

[64] Z. Yan, N. Jouandeau, and A. A. Cherif. A survey and analysis of multi-robot coordination.
_International Journal of Advanced Robotic Systems, 10(12):399, 2013._

[65] L. Yang, Q. Zheng, and G. Pan. Sample complexity of policy gradient finding second-order
stationary points. ArXiv:2012.01491, 2020.

[66] E. Yanmaz, M. Quaritsch, S. Yahyanejad, B. Rinner, H. Hellwagner, and C. Bettstetter. Communication and coordination for drone networks. In Proc. International Conference on Ad Hoc
_Networks, pages 79–91, 2017._

[67] M. Yuan, Q. Cao, M.-o. Pun, and Y. Chen. Towards user scheduling for 6g: A fairness-oriented
scheduler using multi-agent reinforcement learning. ArXiv:2012.15081, 2020.

[68] H. Zhang, H. Jiang, Y. Luo, and G. Xiao. Data-driven optimal consensus control for discretetime multi-agent systems with unknown dynamics using reinforcement learning method. IEEE
_Transactions on Industrial Electronics, 64(5):4091–4100, 2016._

[69] K. Zhang, Z. Yang, and T. Basar. Networked multi-agent reinforcement learning in continuous
spaces. In Proc. 2018 IEEE Conference on Decision and Control (CDC), pages 2771–2776.
IEEE, 2018.

[70] K. Zhang, Z. Yang, H. Liu, T. Zhang, and T. Basar. Fully decentralized multi-agent reinforcement
learning with networked agents. In Proc. International Conference on Machine Learning
_(ICML), pages 5872–5881, 2018._

[71] W. Zhang, H. Liu, F. Wang, T. Xu, H. Xin, D. Dou, and H. Xiong. Intelligent electric vehicle
charging recommendation based on multi-agent reinforcement learning. ArXiv:2102.07359,
2021.

[72] Y. Zhang and M. M. Zavlanos. Distributed off-policy actor-critic reinforcement learning with
policy consensus. In Proc, Conference on Decision and Control (CDC), pages 4674–4679,
2019.

[73] Y. Zhao, Y. Tian, J. D. Lee, and S. S. Du. Provably efficient policy gradient methods for
two-player zero-sum markov games. ArXiv:2102.08903, 2021.


-----

# Appendix

### Table of Contents

**A Notations** **15**

**B** **Proof of Theorem 1** **16**

**C Proof of Theorem 2** **17**

**D Supporting Lemmas** **20**

**E** **Experiment Setup and Additional Results** **37**

E.1 Experiment Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

E.2 Gradient Norm Convergence Results in Ring Network . . . . . . . . . . . . . . 38

E.3 Additional Experiments in Fully Connected Network . . . . . . . . . . . . . . . 39

E.4 Two-agent Cliff Navigation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

**F** **Constant scalars** **41**

A NOTATIONS

**Norms: For any vector x, we denote ∥x∥** as its ℓ2 norm. For any matrix X, we denote ∥X∥, ∥X∥F
as its spectral norm and Frobenius norm, respectively.

**Difference matrix: ∆:= I −** _M1_ **[11][⊤][, where][ 1][ denotes a column vector that consists of 1s.]**

**Moments of random vectors: For a random vector X, we define its variance and covariance matrix**
as Var(X) := E∥X − EX∥[2] and Cov(X) := E [X − EX][X − EX][⊤][], respectively. It is well
known that E _X_ = Var(X) + EX and that Var(X) = tr[Cov(X)].
_∥_ _∥[2]_ _∥_ _∥[2]_  

**Score function: At any time t, The joint score function ψt(at|st) := ∇ω ln πt(at|st) can be de-**
composed into individual score functions ψt[(][m][)](a[(]t[m][)] _st) :=_ _ω(m) ln πt[(][m][)](a[(]t[m][)]_ _st) as ψt(at_ _st) =_
_|_ _∇_ _|_ _|_

[ψt[(1)](a[(1)]t _t_ (a[(]t[M] [)] _st)]._

_[|][s][t][)][, . . ., ψ][(][M]_ [)] _|_

**Reward functions: At any time t, we denote Rt[(][m][)]** := R[(][m][)](st, at, st+1) and Rt := R(st, at, st+1),
where R(s, a, s[′]) = _M1_ _Mm=1_ _[R][(][m][)][(][s, a, s][′][)][.]_

**Policy gradient: The policy gradient theorem (P** 50) shows that

_∇J(ω) = Eνω_ _Aω(s, a)ψω(s, a)_ _._ (10)

where Aω(s, a) := Qω(s, a) _Vω(s) denotes the advantage function. In the decentralized case,_ 
_−_
we have the approximations Vω(st) _φ(st)[⊤]θ, Qω(st, at)_ _Rt + γφ(s[′]t+1[)][⊤][θ][ where][ s][′]t+1_
( _st, at). Therefore, we can stochastically approximate the partial policy gradient as eq. ≈_ _≈_ (1), i.e.,[∼]
_P_ _·|_
for m = 1, ..., M,


_Rt + γφ(s[′]t+1[)][⊤][θ]t[(][m][)]_ _−_ _φ(st)[⊤]θt[(][m][)]_ _ψt[(][m][)](a[(]t[m][)]|st)._
i


_ω(m)_ _J(ωt)_
_∇_ _≈_


We also define the following mini-batch stochastic (partial) policy gradient.

_ω(m)_ _J(ωt) :=_ _N[1]_ (i=t+1)tN _N_ _−1_ _Ri + γφ(s[′]i+1[)][⊤][θ]t[(][m][)]_ _φ(si)[⊤]θt[(][m][)]_ _ψt[(][m][)](a[(]i[m][)]_ _si)._
_∇_ _−_ _|_

h i

e _J(ωt) :=_ _ω(1)JP(ωt); . . . ;_ _ω(M_ ) _J(ωt)_ .

_∇_ _∇_ _∇_

**Filtrations: We define the following filtrations for Algorithms** 1 & 3.

e e [e]


-----

_Ft := σ_ _{θt[(][′][m][)]}m∈M,0≤t′≤t ∪{si, ai, s[′]i+1[,][ {][e][(]i[m][)]}m∈M}i[tN]=0[−][1]_ _∪{stN_ _}_ .

_Ft[′]_ [:=][ σ] Ft ∪ _σ_ _{si, ai, s[′]i+1[}][(]i=[t][+1)]tN_ +1[N] _[−][1]_ . 

_Ft,k = σ_ _Ft ∪_ _σ _ _{si, ai, si+1, s[′]i+1[,][ {][e][(]i[m][)]}m∈M}i∈∪kk−[′]_ =01 _[B][t,k][′]_ .
   

B PROOF OF THEOREM 1

**Theorem 1. Let Assumptions 1–5 hold and adopt the hyperparameters of the decentralized TD**
_in Algorithm 2 following Lemma D.4. Choose α ≤_ 4L1J _[,][ T][ ′][ ≥]_ 2 lnln σ MW[−][1] _[. Then, the output of the]_

_decentralized AC in Algorithm 1 has the following convergence rate._

_Tc_

_c_

E _J(ωT_ [)] +4(c4σW[2][T][ ′] [+][c][5][β][2][σ]W[2][T][ ′] [)+4][c][6] 1 + [4][c][7] +64Cψ[2] _[ζ]approx[critic]_ _[.]_
_∇_ _≤_ [4][R]Tα[max] _−_ _[λ]8[B]_ _[β]_ _N_ [+ 4]N[c]c[8]
h  
e

_Moreover, to achieve[2][i]_ E _∇J(ωT_ [)] _≤_ _ϵ for any ϵ ≥_ 128Cψ[2] _[ζ]approx[critic]_ _[, we can choose][ T, N, N][c]_ [=]
_O(ϵ[−][1]) and Tc, Tc[′][, T][ ′][ =][ O][(ln][ ϵ][−] e[1][)][. Consequently, the overall sample complexity is][ T]_ [(][T][c][N][c] [+][N] [) =]
(ϵ[−][2] ln ϵ[−][1]), and the communication complexities for synchronizing linear model parameters and[2][]
_O_
_rewards are T_ (Tc + Tc[′][) =][ O][(][ϵ][−][1][ ln][ ϵ][−][1][)][ and][ TT][ ′][ =][ O][(][ϵ][−][1][ ln][ ϵ][−][1][)][, respectively.]

_Proof. Concatenating all the agents’ actor updates in Algorithm 1, we obtain the joint actor update_
_ωt+1 = ωt + α∇[b]_ _J(ωt). Then, the item 7 of Lemma D.5 implies that_

_J(ωt+1)_ _J(ωt) +_ _J(ωt)[⊤](ωt+1_ _ωt)_ _ωt+1_ _ωt_
_≥_ _∇_ _−_ _−_ _[L]2[J]_ _−_

= J(ωt) + α _J(ωt)[⊤]_ [b] _J(ωt)_ _J(ωt)_ [2]
_∇_ _∇_ _−_ _[L][J]2[α][2]_ _∇_

(i)
_J(ωt) + α_ _J(ωt)_ + α _J(ωt)[⊤][ b]bJ(ωt)_ [2] _J(ωt)_
_≥_ _∥∇_ _∥[2]_ _∇_ _∇_ _−∇_

_LJ_ _α[2]_ _J(ωt)_ _J(ωt)_ _LJ_ _α[2]_ _J(ωt)_ 
_−_ _∇_ _−∇_ _−_ _∇_

(ii) _α_ _α_
_J(ωt) +b_ _J([2]ωt)_ [2] _J(ωt)_ _J(ωt)_

(≥iii)  2 _[−]_ _[L][J]_ _[α][2][]∥∇_ _∥[2]_ _−_  2 [+][ L][J] _[α][2][]∇_ _−∇_
_J(ωt) +_ _[α]_ _J(ωt)_ _J(ωt)_ b [2]
_≥_ 4 _[∥∇][J][(][ω][t][)][∥][2][ −]_ _[α]_ _∇_ _−∇_

where (i) and (ii) use the inequalitiesfor any x, y ∈ R[d], respectively, and (iii) uses the condition that ∥x∥[2] _≤_ b2∥x − _y∥[2]_ + 2∥y α∥[2] ≤and[2] 4L1 xJ _[⊤][. Then, summing up the]y ≥−_ 2[1] _[∥][x][∥][2][ −]_ [1]2 _[∥][y][∥][2]_

inequality above over t = 0, 1, . . ., T − 1 yields that

_T −1_ _T −1_

_J(ωT )_ _J(ω0) +_ _[α]_ _J(ωt)_ _α_ _J(ωt)_ _J(ωt)_ _._
_≥_ 4 _∥∇_ _∥[2]_ _−_ _∇_ _−∇_

_t=0_ _t=0_

X X

Rearranging the equation above and taking expectation on both sides yields thatb [2]


_T −1_

E∥∇J(ωt)∥[2]
_t=0_

X


_T −1_

_J(ωT_ [)] = [1] E _J(ωt)_
_∇_ _T_ _∥∇_ _∥[2]_

_t=0_

e X

[2]

4
_≤_ _Tα_ [E][[][J][(][ω][T][ )][ −] _[J][(][ω][0][)] + 4]T_


4 _T −1_

E _J(ωt)_ _J(ωt)_

_≤_ _Tα_ [E][[][J][(][ω][T][ )][ −] _[J][(][ω][0][)] + 4]T_ _∇_ _−∇_

_t=0_

X h

(≤i) [4][R]Tα[max] + 4c4σW[2][T][ ′] + 4c5β[2]σW[2][T][ ′]c + 4bc6 1 − _[λ]4[B]_ _[β]_ _Tc_ [2][i]

 

+ [4][c][7] + 64Cψ[2] _[ζ]approx[critic]_ _[,]_ (11)

_N_ [+ 4]N[c]c[8]


where (i) uses the item 4 of Lemma D.5 and eq. (39) of Lemma D.6 (The condition of Lemma D.6
that T _[′]_ _≥_ 2 ln(ln Mσ[−][1]) [holds). This proves the error bound of Theorem][ 1][.]


-----

Finally, for any ϵ ≥ 128Cψ[2] _[ζ]approx[critic]_ [, it can be easily verified that the following hyperparameter choices]
make the error bound in (11) smaller than ϵ and also satisfy the conditions of this Theorem and those
in Lemma D.4 that β ≤ min _λ8CBB[2]_ _[,]_ _λ4B_ _[,][ 1]2C[−]B[σ]_, Nc ≥ 2λB [+ 2][β] 192C(1B2 _−[[1+(]ρ)λ[κ]B[−][1)][ρ][]]_ .
  1    

_α = min_ 1, = (1)

4LJ _O_

 

_λB_
_β = min_ _,_ [4] _,_ [1][ −] _[σ]_ = (1)

8CB[2] _λB_ 2CB _O_

48R max 
_T =_ = (ϵ[−][1])

_αϵ_ _O_

l 1 m

_T_ = ln(48c4ϵ[−][1]), ln M = ln(ϵ[−][1])

_[′]_ 2 ln(σ[−][1]) [max] _O_
l ln(48c5β2ϵ−1)  [m]   

_Tc[′]_ [=] 2 ln(σ[−][1]) = O ln(ϵ[−][1])
l ln(48c6ϵ[−][1]m)   

_Tc =_ = ln(ϵ[−][1])

2 ln[(1 _λBβ/4)[−][1]]_ _O_

l 48c7 _−_ m   

_N =_ = (ϵ[−][1])

_ϵ_ _O_

l m 48c7 2 192CB[2] [[1 + (][κ][ −] [1)][ρ][]]

_Nc =_ max _,_ + 2β = (ϵ[−][1]) (12)

_ϵ_ _λB_ (1 _ρ)λB_ _O_

l h    _−_ im

C PROOF OF THEOREM 2


**Theorem 2. Let Assumptions 1–7 hold and adopt the hyperparameters of the decentralized TD in**

_Algorithm 2 following Lemma D.4. Choose hyperparameters α ≤_ min 1, 4LλJ[2]FCψ[2] _[,]_ 2CLψ[2]J _, β ≤_ 1,

ln M 1 ln(3DJ _Cψ[2]_ [)] ln 3 2304Cψ[4] [(][κ][+1][−][ρ][)]
_T_ _[′]_ _≥_ 2 ln σW[−][1] _[,][ η][ ≤]_ 2Cψ[2] _[,][ T][z][ ≥]_ ln σW[−][1] _, K ≥_ ln(1−ηλF /2)[−][1][,][ N][ ≥] _ηλ _ [5]F [(1][−][ρ][)(1][−][ηλ][F][ /][2)][(][K][−][1)][/][2]

_and Nk ∝_ (1 − _ηλF /2)[−][k/][2]. Then, the output of Algorithm 3 satisfies_

_J(ω[∗]) −_ E _J(ωT_ [)] _≤_ _Tα[c][17]_ [+][ c][18] 1 − _[ηλ]2[F]_ (K−1)/4 + c19σW[T][z] [+][ c][20][σ]W[T][ ′] [+][ c][21][βσ]W[T][ ′]c [+][ c]√[23]Nc
 e   _Tc/2_

+ c22 1 + Cψ _c16ζapprox[critic]_ [+][ c][24][ζ]approx[critic] [+][ C] _[∗][q]ζapprox[actor]_ _[.]_
_−_ _[λ]8[B]_ _[β]_
  q

_Moreover, to achieve J(ω[∗]) −_ E _J(ωT_ [)] _≤_ _ϵ for any ϵ ≥_ 2Cψ _c16ζapprox[critic]_ [+ 2][c][24][ζ]approx[critic] [+]

2C _[∗][p]ζapprox[actor]_ _[, we can choose][ T][ =][ O]_ [(][ϵ][−] b[1][)][,][ N, N][c] [=][ O][(][ϵ][−][2][)][,][ T][c][, T]c[ ′][, T]q[ ′][, T][z][, K][ =][ O][(ln][ ϵ][−][1][)][. Con-]
_sequently, the overall sample complexity is T_ (TcNc + N ) = O(ϵ[−][3] ln ϵ[−][1]), and the communication
_complexities for synchronizing linear model parameters and rewards are T_ (Tc +Tc[′][) =][ O][(][ϵ][−][1][ ln][ ϵ][−][1][)]
_and TT_ _[′]_ = O(ϵ[−][1] ln ϵ[−][1]), respectively.

_Proof. Concatenating all the agents’ actor updates in Algorithm 3, we obtain the joint actor update_
_ωt+1 = ωt + αht. Then, the item 7 of Lemma D.5 implies that_

_J(ωt+1)_ _J(ωt) +_ _J(ωt)[⊤](ωt+1_ _ωt)_ _ωt+1_ _ωt_
_≥_ _∇_ _−_ _−_ _[L]2[J]_ _−_

= J(ωt) + α _J(ωt)[⊤]ht_ _ht_ [2]
_∇_ _−_ _[L][J]2[α][2]_

(i)
_J(ωt) + α_ _J(ωt)[⊤]F_ (ωt)[−][1] _J(ωt) +[2]_ _α_ _J(ωt)[⊤][ht_ _h(ωt)]_
_≥_ _∇_ _∇_ _∇_ _−_

_LJ_ _α[2]_ _ht_ _h(ωt)_ _LJ_ _α[2]_ _F_ (ωt)[−][1] _J(ωt)_
_−_ _−_ _−_ _∇_

[2] [2]


-----

(ii) _J(ωt) +_ _α_ _α_ _J(ωt)_ _αCψ2_ + LJ _α[2][]ht_ _h(ωt)_
_≥_ _Cψ[2]_ _−_ 2Cψ[2] _−_ _[L]λ[J][2]F[α][2]_ _∥∇_ _∥[2]_ _−_ 2 _−_
  

(iii) _α_ [2]
_≥_ _J(ωt) +_ 4Cψ[2] _∥∇J(ωt)∥[2]_ _−_ _αCψ[2]_ _ht −_ _h(ωt)_

[2]

where (i) uses the notation that h(ωt) =△ F (ωt)[−][1] _J(ωt) and the inequality that_ _x_ 2 _x_
_∇_ _∥_ _∥[2]_ _≤_ _∥_ _−_
_y∥[2]1+ 2∥y∥[2]_ for anyCψ[2] _x, y ∈_ R[d], (ii) uses the item 3 of Lemma D.7 and the inequality thatλ[2]F _x[⊤]Cyψ[2] ≥_

Taking expectation on both sides of the above inequality, summing over− 2Cψ[2] _[∥][x][∥][2][ −]_ 2 _[∥][y][∥][2][ for any][ x, y][ ∈]_ [R][d][, and (iii) uses the condition that][ α] t = 0[ ≤] [min], 1, . . ., T 4LJ _Cψ[2]_ _[,]_ 12L andJ 
_−_
rearranging, we obtain that

1 _T −1_ 4Cψ[2] 4Cψ[4] _T −1_

E _J(ωt)_ E _ht_ _h(ωt)_

_T_ _t=0_ _∥∇_ _∥[2]_ _≤_ _Tα_ [E][[][J][(][ω][T][ )][ −] _[J][(][ω][0][)] +]_ _T_ _t=0_ _−_

X X

(i) 4Cψ[2] _[R][max]_ (K−1)/2 [2]
_≤_ _Tα_ + 4Cψ[4] _c10_ 1 − _[ηλ]2[F]_ + c11σ[2][T][z] + c12σ[2][T][ ′]

h  _Tc_ 

+ c13β[2]σ[2][T][ ′]c + c14 1 + _[c][15]_ + c16ζapprox[critic] _,_ (13)
_−_ _[λ]4[B]_ _[β]_ _Nc_
  i

where (i) uses the item 4 of Lemma D.5 and the item 8 of Lemma D.7.

By Assumption 2, ln πω(s, a) is an Lψ-smooth function of ω. Denote ω[∗]:= arg minω∈Ω _J(ω) and_
denote Eω∗ as the unconditional expectation over s _νω∗_ _, a_ _πω∗_ ( _s). We obtain that_
_∼_ _∼_ _·|_

Eω∗ [] ln πt+1(a _s)_ ln πt(a _s)_
_|_ _−_ _|_

Eω∗ _ωt ln πt(a_ _s)_ _⊤(ωt+1_ _ωt)_
_≥_ _∇_ _|_ _−_ _−_ _[L]2[ψ]_ [E][∥][ω][t][+1][ −] _[ω][t][∥][2]_
h   i

= αEω[∗] []ψt(a _s)[⊤]ht_ E _ht_
_|_ _−_ _[L][ψ]2[α][2]_ _∥_ _∥[2][]_

(i)  
_αEω[∗]_ []ψt(a _s)[⊤][ ]ht_ _h(ωt)_ + αEω∗ []ψt(a _s)[⊤]h(ωt)_ _Aωt_ (s, a) + αEω∗ []Aωt (s, a)
_≥_ _|_ _−_ _|_ _−_

_−_ _Lψα[2]E_ _ht −_ _h(ωt)_ _−Lψα[2]E_ _F_ (ωt)[−][1]∇J(ωt)  

(ii)

 

_≥−αCψ_ E _ht −_ _h(ωt)[2][]_ _−_ _αC∗_ _ζapprox[actor]_ [2][]

+ αE _Jq(ω[∗]) −_ _J(ωt)_ _−_ _L[2]ψ[]α[2]E_ _hqt −_ _h(ωt)_ _−_ _Lψα[2]λ[−]F_ [2][E] _∇J(ωt)_ _,_

where (i) uses the inequality that  _x_  2 _x_ _y_ + 2 _y_ for any x, y R[d] and the notation
_∥_ _∥[2]_ _≤_ _∥_ _−_ _∥[2]_ [2][] _∥_ _∥[2]_ _∈_ [2][]

that h(ωt) =△ F (ωt)[−][1] _J(ωt), (ii) uses Cauchy-Schwarz inequality, the items 3 & 6 of Lemma_
_∇_

D.7, the inequality that E∥X∥≤ E _∥X∥[2][]_ for any random vector X and the equality that

Eω∗ []Aωt (s, a) = E _J(ω[∗])_ _J(ωt)q(See its proof in Lemma 3.2 of (_ 1).). Averaging the inequality
_−_
above over t = 0 _, 1, . . ., T_ _−_ 1 and rearranging it yields that

_T −1_

_J(ω[∗])_ E _J(ωT_ [)] = [1] E _J(ωt)_
_−_ _T_

_t=0_

1  e  X   _T −1_

ln πT (a _s)_ ln π0(a _s)_ + C _ζapprox[actor]_ [+][ C][ψ] E _ht_ _h(ωt)_

_≤_ _Tα_ [E][ω][∗] [] _|_ _−_ _|_ _∗_ _T_ _t=0_ _−_

_T −1_  _Tq −1_ X q  [2][]

+ _[L][ψ][α]_ E _ht_ _h(ωt)_ + _[L][ψ][α]_ E _J(ωt)_

_T_ _t=0_ _−_ _Tλ[2]F_ _t=0_ _∇_

X  X 

(i) 1 [2][] [2][]

KL _πω∗_ ( _s)_ _π0(_ _s)_ KL _πω∗_ ( _s)_ _πT (_ _s)_ + C _[∗][q]ζapprox[actor]_

_≤_ _Tα_ [E][s][∼][ν][ω][∗] _·|_ _||_ _·|_ _−_ _·|_ _||_ _·|_

(K 1)/2

+ Cψ _c10_ 1   _−_ + c11σ[2][T][z]  + c12σ[2][T][ ′] + c13β[2]σ[2][T][ ′]c
_−_ _[ηλ]2[F]_
h  


-----

_Tc_ 1/2

+ c14 1 + _[c][15]_ + c16ζapprox[critic]
_−_ _[λ]4[B]_ _[β]_ _Nc_

+ Lψα _c10_ 1  (K−1)/2 + c11σi[2][T][z] + c12σ[2][T][ ′] + c13β[2]σ[2][T][ ′]c
_−_ _[ηλ]2[F]_
h  _Tc_ 

+ c14 1 + _[c][15]_ + c16ζapprox[critic]
_−_ _[λ]4[B]_ _[β]_ _Nc_

+ _[L]λ[ψ][2]F[α]_ 4Cψ2Tα[R][max] + 4Cψ[4] _c10_ 1 − _[ηλ]i2[F]_ (K−1)/2 + c11σ[2][T][z] + c12σ[2][T][ ′] + c13β[2]σ[2][T][ ′]c

n _Tc_ h  

+ c14 1 + _[c][15]_ + c16ζapprox[critic]
_−_ _[λ]4[B]_ _[β]_ _Nc_

(ii) 1   io

KL _πω∗_ ( _s)_ _π0(_ _s)_ + C _[∗][q]ζapprox[actor]_

_≤_ _Tα_ [E][s][∼][ν][ω][∗] _·|_ _||_ _·|_

(K 1)/4

+ Cψ _√c10_ 1   _−_ + _c11σ[T][z]_ + _c12σ[T][ ′]_ + _c13βσ[T][ ′]c_
_−_ _[ηλ]2[F]_ _[√]_ _[√]_ _[√]_
h  _Tc/2_ _c15_

+ _c14_ 1 + + _c16ζapprox[critic]_

_[√]_ _−_ _[λ]4[B]_ _[β]_ _Nc_
  r q i

+ Lψ 1 + 4Cψ[4] _c10_ 1 (K−1)/4 + c11σ[T][z] + c12σ[T][ ′] + c13βσ[T][ ′]c

_λ[2]F_ _−_ _[ηλ]2[F]_

 h  

_Tc/2_ 4LψCψ[2] _[R][max]_

+ c14 1 + _[c][15]_ + c16ζapprox[critic] +
_−_ _[λ]4[B]_ _[β]_ _√Nc_ _Tαλ[2]F_

(iii=) _[c][17]_ 1  (K−1)/4 + c19σ[T][z] +i _c20σ[T][ ′]_ + c21βσ[T][ ′]c + c22 1 _Tc/2_

_Tα_ [+][ c][18] _−_ _[ηλ]2[F]_ _−_ _[λ]4[B]_ _[β]_
   

+ _√[c][23]Nc_ + Cψ _c16ζapprox[critic]_ [+][ c][24][ζ]approx[critic] [+][ C] _[∗][q]ζapprox[actor]_ _[,]_ (14)
q

where (i) uses the definition of KL divergence that KL _πω∗_ ( _s)_ _πω(_ _s)_ =

_·|_ _||_ _·|_
Ethe inequality thattions thata∼πω∗ (·| cs17):=ln πEωs∼∗pP(νaω∗|s)niKL=1 − ln[x]π[i] π[ ≤]ω∗ω(([P]·|as|i[n]s)=1||) _πs√0(and eqs.x·|is for any)_ + (413L nψ) ∈C &λψ[2][2]FN[R] ([max]54+ and), (ii) uses the condition that, c18 x :=1, . . ., x Cψ [√]nc ≥10 +0, (iii) uses the nota-c10Lψ _α1+ ≤_ 41λC and[2]Fψ[4],

_c19 := Cψ[√]c11 + c11Lψ_  1 + 4λC[2]Fψ[4], c20 := Cψ[√]c12 + c12Lψ 1 + 4λC[2]Fψ[4], c21 := C _ψ[√]c13 +_

4Cψ[4]   4Cψ[4]   4Cψ[4]
_c13Lψ_ 1 + _λ[2]F_, c22 := Cψ[√]c14 + c14Lψ 1 + _λ[2]F_, c23 := Cψ[√]c15 + c15Lψ 1 + _λ[2]F_,
     


_c24 := c16Lψ_ 1 + 4λC[2]ψ[4]



. This proves the error bound of Theorem 2.


Finally, for any ϵ ≥ 2Cψ _c16ζapprox[critic]_ [+ 2][c][24][ζ]approx[critic] [+ 2][C] _[∗][p][ζ]approx[actor]_ [, it can be verified that the]

following hyperparameter choices make the error bound inq (14) smaller than ϵ and satisfy all
the conditions of this Theorem and those in Lemma 2λB [+ 2][β] 192C(1B2 [[1+(]ρ)λ[κ]B[−][1)][ρ][]] . D.4 that β ≤ min   λ8CBB[2] _[,]_ _λ4B_ _[,][ 1]2C[−]B[σ]_ , Nc ≥

_−_

  


_α = min_ 1, _λ[2]F_ _,_ _Cψ[2]_

4LJ _Cψ[2]_ 2LJ




= O(1)


_β = min_ 1, [λ][B] _,_ [4] _,_ [1][ −] _[σ]_

8CB[2] _λB_ 2CB

 

_η = [1]_ = (1)

2Cψ[2] _O_

14c17
_T =_ = (ϵ[−][1])

_αϵ_ _O_

l m


= O(1)


-----

ln 3 4 ln(14c18ϵ[−][1])
_K =_ max = ln(ϵ[−][1])
l h ln[(1 − _ηλ2F /2)[−][1]]_ _[,]_ ln (1 − _ηλF /2)[−][1][][ + 1]im_ _O_ 

ln(3DJ _Cψ[)]_ 
_Tz =_ max _,_ [ln(14][c][19][ϵ][−][1][)] = ln(ϵ[−][1])

ln(σ[−][1]) ln(σ[−][1]) _O_

l h ln M i m  

_T_ _[′]_ = max 2 ln(σ[−][1]) _[,][ ln(14]ln([c]σ[20][−][1][ϵ][−])_ [1][)] = O ln(ϵ[−][1])
l ln(14hc21ϵ−1) im  

_Tc[′]_ [=] ln(σ[−][1]) = O ln(ϵ[−][1])
l 2 ln(14c22ϵ[−]m [1])  

_Tc =_ = ln(ϵ[−][1])

ln[(1 _λBβ/4)[−][1]]_ _O_

l _−2304Cψ[4]_ [(][κ][ + 1]m[ −] _[ρ][)]_ 

_N =_ = (ϵ[−][2])

_ηλ[5]F_ [(1][ −] _[ρ][)(1][ −]_ _[ηλ][F][ /][2)][(][K][−][1)][/][2]_ _O_

l 2 192CB2 [[1 + (][κ]m[ −] [1)][ρ][]]

_Nc =_ max + 2β _, 196c[2]23[ϵ][−][2][im]_ = (ϵ[−][2]) (15)

_λB_ (1 _ρ)λB_ _O_

l h  _−_

SUPPORTING LEMMAS


First, we extend the Lemma F.3 of (12) to the Lemma D.1 below. The item 1 of Lemma D.1
generalizes the case n = 1 to any n ∈ N[+], the items 2 & 3 remain unchanged, and the item 4 is
added for convenience of our convergence analysis.
**Lemma D.1. The doubly stochastic matrix W and the difference matrix ∆= I −** _M1_ **[11][⊤]** _[have the]_

_following properties:_

_1. ∆W_ _[n]_ = W _[n]∆= W_ _[n]_ _−_ _M1_ **[11][⊤]** _[for any][ n][ ∈]_ [N][+][.]

_2. The spectral norm of W satisfies ∥W_ _∥_ = 1.

_3. For any x_ R[M] _and n_ N[+], _W_ _[n]∆x_ _σW[n]_
_of W_ _). Hence, for any ∈_ _H ∈_ R[M] ∥[×][M] _,_ _W∥≤[n]∆H_ _F[∥][∆][x]σ[∥]W[n][(][σ][W][ is the second largest singular value]_
_∈_ _∥_ _∥_ _≤_ _[∥][∆][H][∥][F][ .]_

1 1

_4._ _W_ _[n]_ _−_ _M_ **[11][⊤]** _≤_ _σW[n]_ _[,]_ _W_ _[n]_ _−_ _M_ **[11][⊤]** _F_ _[≤]_ _[σ]W[n]_ _√M for any n ∈_ N[+].

_Proof. The proof of items 2 & 3 can be found in (12). We prove the item 1 and item 4._

We prove item 1 by induction. The case n = 1 of the item 1 can be proved by the following two
equalities, as shown in (12).

∆W = _I_ _W = W_
_−_ _M[1]_ **[11][⊤][]** _−_ _M[1]_ **[11][⊤][W][ =][ W][ −]** _M[1]_ **[11][⊤]**


_W_ ∆= W _I_ = W
_−_ _M[1]_ **[11][⊤][]** _−_ _M [1]_ _[W]_ **[11][⊤]** [=][ W][ −] _M[1]_ **[11][⊤]**



Suppose the case of n = k holds for a certain k ∈ N[+], then the following two equalities proves the
case of n = k + 1 and thus proves the item 1.

∆W _[k][+1]_ = (∆W _[k])W =_ _W_ _[k]_ _W = W_ _[k][+1]_
_−_ _M[1]_ **[11][⊤][]** _−_ _M[1]_ **[11][⊤]**


_W_ _[k][+1]∆= W_ (W _[k]∆) = W_ _W_ _[k]_ = W _[k][+1]_
_−_ _M[1]_ **[11][⊤][]** _−_ _M[1]_ **[11][⊤]**


The item 4 can be proved by the following two inequalities.
_W_ _[n]_ (=i) _W_ _[n]∆_ = sup _W_ _[n]∆x_ (ii) sup _σW[n]_ (=iv) σW[n] _[,]_ (16)
_−_ _M[1]_ **[11][⊤]** _x:_ _x_ 1 _∥_ _∥_ _≤_ _x:_ _x_ 1 _[∥][∆][∥∥][x][∥]_

_∥_ _∥≤_ _∥_ _∥≤_


-----

_W_ _[n]_ _−_ _M[1]_ **[11][⊤]** _F_ (=i) _W_ _[n]∆_ _F_ (iii≤) _σW[n]_ _[∥][∆][∥][F]_

(iv) 2 2
= σW[n] _M_ 1 − _M[1]_ + M (M − 1) _−_ _M[1]_ _≤_ _σW[n]_ _√M,_ (17)

r

   

where (i) uses the item 1, (ii) and (iii) use the item 3 (H = I in (iii)), and (iv) uses the fact that
∆ has M diagnoal entries 1 − _M1_ [and][ M] [(][M][ −] [1)][ off-diagnoal entries][ −] _M[1]_ [, which implies that]

_∥∆∥_ = 1.


Next, we extend the Lemma F.2. of (12) to the Lemma D.2 below.

**Lemma D.2. Suppose the Markovian samples** _si, ai_ _i_ 0 are generated following the policy πω and
_{_ _}_ _≥_
_transition kernel_ _(can be_ _or_ _ξ), and s[′]i+1_
_Xany : s, s S × A × S × S →[′],_ _s ∈S, a P ∈A[′]_ _, we haveR[p][×] P[q]_ _(p, q P ∈_ N[+] _are arbitrary.) such that[∼P][(][·|][s][i][, a][i][)][. Then, for any deterministic mapping] ∥X(s, a, s[′],_ _s)∥F ≤_ _Cx and for_

_n+n[′]_ 1 e
_−_ 2

E e [1] _X(si, ai, si+1, s[′]i+1[)][ −]_ _[X]_ _sn[′]_ _x[(][κ][ + 1][ −]_ _[ρ][)]_ _,_ _n, n[′]_ N[+] (18)

_n_ _F_ _≤_ [9][C] [2]n(1 _ρ)_ _∀_ _∈_

h _iX=n[′]_ i _−_

_where X = E_ _X(si, ai, si+1, s[′]i+1[)]_ _si_ _with si ∼_ _µω (or νω) when P_ _[′]_ = P (or Pξ).
 

_Proof. Denote Y (s, a, s[′]) := Es_ ( _s,a)_ _X(s, a, s[′],_ _s)_ _s, a, s[′][]_ which satisfies _Y (s, a, s[′])_ _Cx_
_∼P_ _[′]_ _·|_ _∥_ _∥≤_
and Esi _νω_ _Y (si, ai, si+1)_ = X. Hence, Lemma F.2 of (12) can be applied to Y (s, a, s[′]) and
_∼_ e 
obtain the following inequality e
 


_n+n[′]−1_

E [1] _Y (si, ai, si+1)_ _X_

_n_ _−_

_i=n[′]_

h X

Therefore, we obtain that


_x[(][κ][ + 1][ −]_ _[ρ][)]_
_sn[′]_ _._ (19)
_≤_ [8][C] [2]n(1 _ρ)_
i _−_


_n+n[′]_ 1
_−_ 2

E [1] _X(si, ai, si+1, s[′]i+1[)][ −]_ _[X]_ _si, ai, si+1_ _i=n[′]_

_n_ _F_ _{_ _}[n][+][n][′][−][1]_

_i=n[′]_

h X

1 _n+n[′]_
= E _n_ _X(si, ai, si+1, s[′]i+1[)][ −]_ _[X]_ _{si, ai, si+1}i[n]=[+]n[n][′][′][−][1]_

_i=n[′]_

h X i

1 _n+n[′]−1_
+ Var _n_ _X(si, ai, si+1, s[′]i+1[)]_ _{si, ai, si+1}i[n]=[+]n[n][′][′][−][1]_

_i=n[′]_

h X i

_n+n[′]_ 1

(i) _−_ 2
= [1] _Y (si, ai, si+1)_ _X_

_n_ _−_ _F_

_i=n[′]_

X


_n+n[′]−1_

+ n[1][2] Var _X(si, ai, si+1, s[′]i+1[)]_ _{si, ai, si+1}i[n]=[+]n[n][′][′][−][1]_

_i=n[′]_

X 

_n+n[′]_ 1

(ii) _−_ 2

[1] _Y (si, ai, si+1)_ _X_ _x_
_≤_ _n_ _−_ _F_ [+][ C]n[2]

_i=n[′]_

X


(20)


where (i) uses the conditional independency among _s[′]i+1[}][(]i=[t][+1)]tN_ _[N]_ _[−][1]_ on _si, ai, si+1_ _i=n[′]_ and (ii)
_{_ _{_ _}[n][+][n][′][−][1]_
uses the fact that ∥X(si, ai, si+1, s[′]i+1[)][∥][F][ ≤] _[C][x][.]_

Finally, eq. (18) can be proved via the following inequality.


_n+n[′]_

_X(si, ai, si+1, s[′]i+1[)][ −]_ _[X]_
_i=n[′]_

X


_sn[′]_


-----

_n+n[′]_ 1

(i) _−_ 2
E [1] _Y (si, ai, si+1)_ _X_ _sn[′]_ + _[C]x[2]_
_≤_ _n_ _−_ _F_ _n_

_i=n[′]_

h X i

(ii)
_x[(][κ][ + 1][ −]_ _[ρ][)]_ _x_ _x[(][κ][ + 1][ −]_ _[ρ][)]_

+ _[C]_ [2] _,_

_≤_ [8][C] [2]n(1 _ρ)_ _n_ _n(1_ _ρ)_

_−_ _[≤]_ [9][C] [2] _−_

where (i) takes the conditional expectation of eq. (20) on s[′]n [and (ii) uses eq. (][19][).]

Next, we prove the following Lemmas D.3 & D.4 on the decentralized TD in Algorithm 2. We first
define the following useful notations.


_λφ := λmin_ Es _µω_ [φ(s)φ(s)[⊤]] _> 0, see Assumption 4._
_∼_

_B(s, s[′]) := φ _ (s) _γφ(s[′]) −_ _φ(s)⊤._

_Bt :=_ _N1c_ (i=t+1)tNcNc−1 _B(si, si+1_ ).

_Bω := Es∼Pµω,a∼πω(·|s),s′∼P(·|s,a)_ _B(s, s[′])_ .

_b[(][m][)](s, a, s[′]) := R[(][m][)](s, a, s[′])φ(s)._ 


_b(s, a, s[′]) :=_ _M1_ _Mm=1_ _[b][(][m][)][(][s, a, s][′][)][.]_

1 (t+1)Nc 1
_b[(]t[m][)]_ := _Nc_ _i=PtNc_ _−_ _b[(][m][)](si, ai, si+1)._

_bt :=_ _M1_ _MmP=1_ _[b][(]t[m][)]._

_bω := EsP∼µω,a∼πω(·|s),s′∼P(·|s,a)_ _b(s, a, s[′])_ .

_θω[∗]_ [:=][ B]ω[−][1][b][ω][, which is the optimal critic parameter under policy]  _[ π][ω][.]_

**Lemma D.3. The following bounds hold for Algorithm 2.**


_1. ∥B(s, s[′])∥F, ∥Bt∥F, ∥Bω∥F ≤_ _CB := 1 + γ,_
_b[(][m][)](s, a, s[′])_ _,_ _b(s, a, s[′])_ _,_ _b[(]t[m][)]_ _,_ _bt_ _,_ _bω_ _Cb := Rmax._
_∥_ _∥_ _∥_ _∥_ _∥_ _∥_ _∥_ _∥_ _∥_ _∥≤_

_2. θ[⊤]Bωθ_ 2
_≤−_ _[λ][B]_ _[∥][θ][∥][2][ uniformly for all][ ω][, where][ λ][B][ := 2(1][ −]_ _[γ][)][λ][φ][ >][ 0][.]_

_3. ∥θω[∗]_ _[∥≤]_ _[R][θ]_ [:=][ 2]λ[C]B[b] _[uniformly for all][ ω][.]_

_Proof. We first prove the item 1. Notice that for any vectors x, y ∈_ R[d],


_∥xy[⊤]∥F =_

Hence, we obtain that


(xiyj)[2] =
_j=1_

X


_x[2]i_
_i=1_

X


_yj[2]_ [=][ ∥][x][∥∥][y][∥][.]
_j=1_

X


_i=1_


_∥B(s, s[′])∥F =_ _φ(s)_ _γφ(s[′]) −_ _φ(s)_ _⊤_ _F_ [=][ ∥][φ][(][s][)][∥∥][γφ][(][s][′][)][ −] _[φ][(][s][)][∥≤]_ [1 +][ γ][ :=][ C][B][,] (21)
  

_∥b(s, a, s[′])∥_ = R(s, a, s[′])∥φ(s)∥≤ _Rmax := Cb._ (22)

The other terms listed in the item 1 can be proved by applying the Jensen’s inequality to the convex
function ∥· ∥.

Next, we prove the item 2, where we use the underlying distribution that s _µω, a_ _πω(_ _s),_
_∼_ _∼_ _·|_
_s[′]_ _∼P(·|s, a). We obtain that_

_θ[⊤]Bωθ = Eω_ _θ[⊤]φ(s)_ _γφ(s[′]) −_ _φ(s)_ _⊤θ_

= γEω _θ[⊤]φ(s)_ _θ[⊤]φ(s[′])_ − Eω _θ[⊤]φ(s)_ 2[i]
h    [i] h  


-----

2[i] 2[i] 2[i]
Eω _θ[⊤]φ(s)_ + Eω _θ[⊤]φ(s[′])_ Eω _θ[⊤]φ(s)_

_≤_ _[γ]2_ _−_

(i)  h   2[i] h   h  
= (γ − 1)Eω _θ[⊤]φ(s)_

= −(1 − _γ)θ[⊤]h Eω[φ(s)φ_ (s)[⊤]]θ

(ii)

(23)

_≤−_ _[λ]2[B]_ _[∥][θ][∥][2][,]_

where (i) uses the fact that s, s[′] _∼_ _µω which is the stationary state distribution with the transition_
kernel P and the policy πω, and (ii) uses Assumption 4 and we denote λB := 2(1 − _γ)λφ > 0._

Finally, the item 3 can be proved via the following inequality.

_θω[∗]_ _[∥][2 (][i][)]_ (θω[∗] [)][⊤][B][ω][θ]ω[∗] _θω[∗]_ _[∥∥][B][ω][θ]ω[∗]_ _[∥]_ [= 2] _θω[∗]_ _[∥∥][b][ω][∥≤]_ [2][C][b] _θω[∗]_ _[∥][,]_ (24)
_∥_ _≤−_ _λ[2]B_ _[≤]_ _λ[2]B_ _∥_ _λB_ _∥_ _λB_ _∥_

where (i) uses the item 2.

2Lemma D.4.β 192C(1B2 _−[[1+(]ρ) Under Assumptionsλ[κ]B[−][1)][ρ][]]_ _, Algorithm 2 has the following convergence rate. 1–5 and choosing β ≤_ min   λ8CBB[2] _[,]_ _λ4B_ _[,][ 1]2[−]C[σ]B[W]_ , Nc ≥   2λB [+]
 _M_ _Tc_

E _θT[(][m]c+[)]_ _Tc[′]_ _ωt_ _ωt_ _≤_ _σW[2][T][ ′]c_ _[β][2][c][2][ + 2][M]_ _c3_ 1 − _[λ]8[B]_ _[β]_ + _N[c][1]c_ _._ (25)
_m=1_ _[−]_ _[θ][∗]_

X   h   i

[2]

_Moreover, to achieveand Nc =_ (ϵ[−][1]). Consequently, the sample complexity ism=1 [E] _θT[(][m]c+[)]_ _Tc[′]_ _[−]_ _[θ]ω[∗]_ _t_ _ωt_ _≤_ _ϵ, we can choose TcNc =_ _Tϵc[−], T[1]_ ln(c[′] [=]ϵ[−][ O][1]) ln(and theϵ[−][1])

_O_   _O_  
_communication complexity is[P][M]_ _Tc + Tc[′]_ [=][ O] ln(ϵ[−][2][1]) _._  
 

_Proof.the averaged critic parameter In Algorithm 2, by averaging the TD update rule θt,t[′] :=_ _M1_ _Mm=1_ _[θ](t,tm[′])[ follows the following update rule] (26) over the agents m ∈M, we obtain that_

_M_ _M_

P

_θt,t′+1 = M[1]_ _Wm,m′_ _θt,t[(][m][′][′][)]_ + β _Bt′_ _θt,t[(][m][′][)][ +][ b][(]t[′][m][)]_

_m=1_ _m[′]=1_

XM h X _M_   [i]

= M[1] _θt,t[(][m][′][′][)]_ + β _M[1]_ _Bt′_ _θt,t[(][m][′][)][ +][ b][(]t[′][m][)]_

_m[′]=1_ _m=1_

X X   

= θt,t[′] + β _Bt[′]_ _θt,t[′] + bt[′]_ [] (26)

which can be viewed as a centralized TD update using the Markovian samples  _{si, ai}i from the_
transition kernel and the joint policy πt. Therefore, Theorem 4 in (60) can be directly applied to
_P_
analyze this centralized TD update and obtain the following convergence rate of θt,t′, since all the
conditions of that theorem are met [4].

_Tc_

E _θt,Tc −_ _θω[∗]_ _t_ _ωt_ _≤_ 1 − _[λ]4[B]_ _[β]_ E _θt,0 −_ _θω[∗]_ _t_ _ωt_
 
 [2]  + 2 + 2β 192 _CB[2]_ _[R]θ[2]_ [+][ C][2]b[2] [1 + ( _κ −_ 1)ρ]

_λB_ (1 _ρ)λBNc_

(i)  _T c_ _−_ 
_≤_ 2 1 − _[λ]4[B]_ _[β]_ _θ−1_ + Rθ[2] + _N[c][1]c_
 

(ii) _T  c_ 

[2]

_c3_ 1 + _[c][1]_ _._ (27)
_≤_ _−_ _[λ]4[B]_ _[β]_ _Nc_

 

where (i) uses the condition that β 4/λB, the item 3 of Lemma D.3 and the constant that

1920(CB[2] _[R]θ[2][+][C]b[2][)][[1+(][κ][−][1)][ρ][]]_ _≤_
_c1 :=_ (1−ρ)λ[2]B, (ii) uses the constant that c3 := 2 _θ−1_ + Rθ[2] .

4 _λB_   
We corrected the typo 1 − 8 _[β][, which should be][ 1][ −]_ _[λ]4[B]_ _[β][.]_ [2]


-----

Next, we consider the consensus error ∥∆Θt,t′ _∥F[2]_ [=][ P]m[M]=1 _θt,t[(][m][′][)][ −]_ _[θ][t,t][′]_ where we define

Θt,t′ := [θt,t[(1)][′] _[, . . ., θ]t,t[(][M][′][ ]][)]_ _[⊤][. Note that the critic-step][ (][26][)][ can be rewritten into the following matrix]_

[2]

form

Θt,t′+1 = W Θt,t′ + β Θt,t′ _Bt[⊤][′][ + [][b][(1)]t[′][ ;][ . . .][ ;][ b][(]t[′][M]_ [)]][⊤][]; t[′] = 0, 1, . . ., Tc 1, (28)
_−_

which further implies that for any t [′] = 0, 1, . . ., Tc 1,
_−_

(i)

∆Θt,t′+1 _F_ _≤_ _W_ ∆Θt,t′ _F_ [+][ β] ∆Θt,t′ _Bt[⊤][′]_ _F_ [+][ β] ∆[b[(1)]t[′][ ;][ . . .][ ;][ b][(]t[′][M] [)]][⊤] _F_

_M_

(ii)
_≤_ (σW + βCB) ∆Θt,t′ _F_ [+][ β]vM _∥b[(]t[′][m][)]∥[2]_

u _m=1_
u X

(iii) t
_≤_ [1 +]2[ σ][W] ∆Θt,t′ _F_ [+][ βMC][b][,]

where (i) uses the item 1 of Lemma D.1, (ii) uses the item 3 of Lemma D.1 and the item 1 of Lemma
D.3, (iii) uses the condition that β ≤ [1]2[−]C[σ]B[W] [and the item 1 of Lemma][ D.3][. Telescoping the inequality]

above yields that

1 + σW _Tc_ (i)
∆Θt,Tc _F_ 2 ∆Θt,0 _F_ [+ 2]1[βMC]σW[b] = [2]1[βMC]σW[b] _,_ (29)

_[≤]_   _−_ _−_

where (i) uses the equality that ∆Θ0 = O due to the initial condition that Θt,0 = [θ 1; . . . ; θ 1][⊤].
_−_ _−_

On the other hand, the final Tc[′] [local average steps in Algorithm][ 2][ can be rewritten into the following]
matrix form

Θt,t′+1 = W Θt,t′ ; t = Tc, Tc + 1, . . ., Tc + Tc[′] _[−]_ [1][.]

Hence, the average critic parameter θt,t′ does not change in these local average steps, i.e.,


_θt,Tc+T ′c_ [= 1]M [Θ]t,T[⊤] _c+Tc[′]_ **[1][ = 1]M** [Θ]t,T[⊤] _c_ [(][W][ T][ ′]c )[⊤]1 = M[1] [Θ]t,T[⊤] _c_ **[1][ =][ θ][t,T]c** _[.]_ (30)

Therefore, we obtain that


_θt,T[(][m]c[)]+Tc[′]_ = _θt,T[(][m]c[)]+Tc[′]_ _c_ = ∥∆Θt,Tc+T ′c _[∥]F[2]_ [=][ ∥][∆][W][ T][ ′]c Θt,Tc _∥F[2]_

_m=1_ _[−]_ _[θ][t,T][c]_ _m=1_ _[−]_ _[θ][t,T][c][+][T][ ′]_

X X

[2] (=i) _W_ _[T][ ′]c_ ∆Θt,Tc _F_ (ii) _σW[2][T][ ′]c_ [2] _c_ _[∥][2]F_

(iii ∥) _σW[2][T][ ′]c_ 2βMC∥[2]b _≤2 (=iv) σW[2][∥][T][∆Θ][ ′]c_ _[β][2][t,T][c][2][/][2]_ (31)
_≤_ 1 _σW_
 _−_ 

where (i) and (ii) use the items 1 and 3 of Lemma D.1 respectively, (iii) uses eq. (29), (iv) denotes
that c2 := 2 21−MCσWb 2. Combining eqs. (27) & (31) yields that

_M_    _M_

_m=1_ E _θt,T[(][m]c[)]+Tc[′]_ _[−]_ _[θ]ω[∗]_ _t_ _ωt_ _≤_ 2 _m=1_ E _θt,T[(][m]c[)]+Tc[′]_ _[−]_ _[θ][t,T][c]_ _ωt_ + 2M E _θt,Tc −_ _θω[∗]_ _t_ _ωt_

X   X  Tc  

[2] _σW[2][T][ ′]c_ _[β][2][c][2][ + 2][M]_ _c3_ 1 [2] + _[c][1]_ _._ [2]

_≤_ _−_ _[λ]4[B]_ _[β]_ _Nc_
h   i

In the inequality above, replacing θt,T[(][m]c[)]+Tc[′] [from Algorithm][ 2][ by its corresponding variable][ θ]t[(][m][)] from
Algorithm 1 proves eq. (25). Finally, it can be easily verified that the following hyperparameter
choices make the error bound in (25) smaller than ϵ and also satisfy the conditions of Lemma D.4.

_λB_
_β = min_ _,_ [4] _,_ [1][ −] _[σ][W]_ = (1)

8CB[2] _λB_ 2CB _O_

  2 192CB[2] [[1 + (][κ][ −] [1)][ρ][]]

_Nc = max_ + 2β _, 6Mc1ϵ[−][1][i]_ = (ϵ[−][1])

_λB_ (1 _ρ)λB_ _O_
_−_

h  


-----

ln(6Mc3ϵ[−][1])
_Tc =_ 1 = ln(ϵ[−][1])

ln 1 _λBβ/4_ _−_ _O_

l ln(3  _β −2c2ϵ−1)_   m  

_Tc[′]_ [= 2] ln(σW[−][1][)] = O ln(ϵ[−][1])
l m  

**Lemma D.5. For any ω,** _ω ∈_ Ω, s ∈S and a[(][m][)] _∈Am (Am denotes the action space for the agent_
_m), the following properties hold._

_1._ _ψω[(][m][)](a[(][m][)]_ _s)_ _Cψ e, where ψω[(][m][)](a[(][m][)]_ _s) :=_ _ω(m) ln πω[(][m][)](a[(][m][)]_ _s)._
_∥_ _|_ _∥≤_ _|_ _∇_ _|_


_2._ _ψω[(][m][)](a[(][m][)]_ _s)_ _ψω[(][m][)](a[(][m][)]_ _s)_ _Lψ_ _ω[(][m][)]_ _ω[(][m][)]_ _._
_∥_ _|_ _−_ _|_ _∥≤_ _∥_ _−_ _∥_

_3. dTVe_ _πω[(][m][(][m][)][)]_ [(][·|][s][)][, π]ω[(][m][(][m][)][)] [(][·|][s][)] _≤_ _Lπ∥ω[(][m][)]_ _−e_ _ω[(][m][)]∥._

_4. 0_ Veω(s), Qω(s, a) (1 _γ)Rmax, 0_ _J(ω)_ _Rmax._
_≤_ _≤_ _−_ e _≤_ _≤_

_5. dTV_ _νω(_ _s), νω[(][·|][s][)]_ _Lν_ _ω[′]_ _ω_ _where Lν := Lπ[1 + logρ(κ[−][1]) + (1_ _ρ)[−][1]]._

_·|_ _≤_ _∥_ _−_ _∥_ _−_

_6. dTVQω[(][s, a][)][, Q]e_ _ω[(][s, a]_ [)] _LQ_ _ω_ _ω_ _where LQ :=_ [2][R]1[max]γ[L][ν] _._
_≤_ _∥_ _−_ _∥_ _−_

_7. J(ω) ise LJ_ _-smooth where_ _LJ := Rmax(4Lν + Lψ)/(1_ _γ)._

e _−_

_8._ _J(ω)_ _DJ :=_ _[C][ψ]1[R][max]γ_ _._
_∥∇_ _∥≤_ _−_


_9. F_ (ω) is LF -Lipschitz where LF := 2Cψ(LπCψ + LνCψ + Lψ).

_10. h(ω) is Lh-Lipschitz where Lh := 2λ[−]F_ [1][(][D][J] _[λ]F[−][1][L][F][ +][ L][J]_ [)][.]

_Proof. For any ω[(][m][)],_ _ω[(][m][)]_ Ωm, s and a[(][m][)] _m, arbitrarily select ω[(][m][′][)]_ = _ω[(][m][′][)]_ Ωm′,
_∈_ _∈S_ _∈A_ _∈_
_a[(][m][′][)]_ _m′ for every m[′]_ 1, ..., M _/_ _m_ . Denote ω = [ω[(1)]; . . . ; ω[(][M] [)]], _ω = [ω[(1)]; . . . ;_ _ω[(][M]_ [)]],
_∈A_ _∈{_ _}_ _{_ _}_
_a = [a[(1)], . . ., a[(][M]_ [)]]. Notice that the joint score vector has the following decomposition e e
e e e

_ψω(a|s) = [ψω[(1)][(][a][(1)][|][s][);][ . . .][ ;][ ψ]ω[(][M]_ [)](a[(][M] [)]|s)]. (32)

Hence, the items 1 & 2 can be proved via the following two inequalities, respectively.


_M_

(ii)
_ψω[(][m][′][)](a[(][m][′][)]_ _s)_ = _ψω(a_ _s)_ _Cψ._
_∥_ _|_ _∥[2 (][i][)] ∥_ _|_ _∥_ _≤_
_m[′]=1_

X


_ψω[(][m][)](a[(][m][)]_ _s)_
_∥_ _|_ _∥≤_


_ψω[(][m][)](a[(][m][)]_ _s)_ _ψω[(][m][)](a[(][m][)]_ _s)_ = _ψω[(][a][|][s][)][ −]_ _[ψ]ω[(][a][|][s][)][∥]_
_∥_ _|_ _−_ _|_ _∥_ _∥_

(i)

e _Lψe_ _ω_ _ω_ = Lψ _ω[(][m][)]_ _ω[(][m][)]_

_≤_ _∥_ _−_ _∥_ _∥_ _−_ _∥_

where (i) uses Assumption 2.

e e

Next, we prove the item 3. Notice that

_dTV_ _πω[(][·|][s][)][, π]ω[(][·|][s][)]_

(i)
= supA _⊂Ae_ _|πω[(][A][|][s][)][ −][π]ω[(][A][|][s][)][|]_

(ii) e _M_ _M_
sup _πω[(][m][′]_ [)] [(][A][m][′] _[|][s][)][ −]_ _πω(m[′]_ ) (Am′ _s)_
_≥_ _A1⊂A1,...,AM_ _⊂AM_ _m[′]=1_ _m[′]=1_ _|_

Y Y

_Me_

(iii)
= sup _πω(m[′]_ ) (Am′ _s)_ _πω[(][m][)]_ [(][A][m][|][s][)][ −] _[π]ω[(][m][)]_ [(][A][m][|][s][)]
_A1⊂A1,...,AM_ _⊂AM_ _m[′]=1,m[′]=m_ _|_

Y _̸_ e


-----

(iv)
= sup _πω[(][m][)]_ [(][A][m][|][s][)][ −] _[π]ω[(][m][)]_ [(][A][m][|][s][)] = dTV _πω[(][m][(][m][)][)]_ [(][·|][s][)][, π]ω[(][m][(][m][)][)] [(][·|][s][)] _,_
_Am⊂Am_

e  e 

where (i) denotes that πω(A|s) = _A_ _[π][ω][(][a][|][s][)][da][, (ii) uses the relation that][ ×][m][∈M][A][m][ ⊂A][, (iii) uses]_

our construction that ω[(][m][′][)] = _ω[(][m][′][)]_ Ωm′ _,_ _m[′]_ 1, ..., M _/_ _m_, and (iv) uses Am′ = _m′ to_

R _∈_ _∀_ _∈{_ _}_ _{_ _}_ _A_

achieve the supremum. Therefore, the item 2 can be proved via the following inequality.
e

_≤_
_dTV_ _πω[(][m][(][m][)][)]_ [(][·|][s][)][, π]ω[(][m][(][m][)][)] [(][·|][s][)] = dTV _πω[(][·|][s][)][, π]ω[(][·|][s][)]_ _Lπ ∥ω −_ _ω∥_ = Lπ∥ω[(][m][)] _−_ _ω[(][m][)]∥,_

where (i) uses Assumption e 2.   e 

e e

The item 4 can be proved by the following three inequalities that use Assumption 3.



_[∞]_ _∞_
0 _Vω(s) = Eω_ _γ[t]Rt_ _s0 = s_ _γ[t]Rmax =_ _[R][max]_
_≤_ _≤_ 1 _γ [,]_

_t=0_ _t=0_

h X i X _−_

0 ≤ _Qω(s, a) = Es′∼P(·|s,a)[R(s, a, s[′]) + γVω(s[′])] ≤_ _Rmax + γ 1[R][max]γ_ [=][ R]1 [max]γ [,]

_−_ _−_



_[∞]_
0 ≤ _J(ω) = (1 −_ _γ)Eω_ _γ[t]Rt_ _≤_ (1 − _γ)_

_t=0_

h X i


_γ[t]Rmax = Rmax._
_t=0_

X


The proof of the items 5 – 7 can be found in the proof of Lemma 3, Lemma 4 and Proposition 1 of
(60), respectively.

Next, the item 8 is proved by the following inequality.
_J(ω)_ = Es _νω,a_ _πω(_ _s)_ _Qω(s, a)ψω(a_ _s)_
_∇_ _∼_ _∼_ _·|_ _|_

(i)

 

Es _νω,a_ _πω(_ _s)_ _Qω(s, a)_ _ψω(a_ _s)_ _,_
_≤_ _∼_ _∼_ _·|_ _|_ _|_ _|_ _≤_ _[C]1[ψ][R][max]γ_

_−_



where (i) applies Jensen’s inequality, (ii) uses Assumption 2 and the item[][ (][ii][)] 4.


Next, the item 9 is proved by the following inequality.
_F_ (ω) − _F_ (ω)

= Es∼νπω _[,a][∼][π]ω[e]_ [(][·|][s][)] _ψω[(][a][|][s][)][ψ]ω[(][a][|][s][)][⊤][]_ _−_ Es∼νπω,a∼πω(·|s) _ψω(a|s)ψω(a|s)[⊤][]_

e

(i) e  e e 
_≤_ Es∼νπω _[,a][∼][π]ω[e]_ [(][·|][s][)] _ψω[(][a][|][s][)][ψ]ω[(][a][|][s][)][⊤][]_ _−_ Es∼νπω,a∼πω(·|s) _ψω[(][a][|][s][)][ψ]ω[(][a][|][s][)][⊤][]_

+ Es∼νπωe _,a∼πω(·|s)_ e[ψω[(][a][|][s][)]e[ −] _[ψ]ω[(][a][|][s][)]][ψ]ω[(][a][|][s][)][⊤]_  e e

+ Es∼νπω,a∼πω(·|s)ψωe(a|s)[ψω[(][a][|][s][)][ −] _[ψ]ωe[(][a][|][s][)]][⊤]_ []

(ii)

 e

[νω[(][s][)][π]ω[(][a][|][s][)][ −] _[ν]ω[(][s][)][π]ω[(][a][|][s][)]]_ _ψω[(][a][|][s][)][ψ]ω[(][][a][|][s][)][⊤][]dsda_ + 2CψLψ _ω_ _ω_

_≤_ _∥_ _−_ _∥_
ZS×A

e e  e e

_Cψ[2]_ _νω[(][s][)][π]ω[(][a][|][s][)][ −]_ _[ν]ω[(][s][)][π]ω[(][a][|][s][)][|][dsda][ + 2][C]ψ[L]ψ[∥]ω[e]_ _ω_ e
_≤_ _|_ _−_ _∥_

ZS×A

e e

_Cψ[2]_ _νω[(][s][)][|][π]ω[(][a][|][s][)][ −]_ _[π]ω[(][a][|][s][)][|][dsda]_
_≤_

ZS×A

e e

+ Cψ[2] _πω(a_ _s)_ _νω[(][s][)][ −]_ _[ν]ω[(][s][)][|][dsda][ + 2][C]ψ[L]ψ_ _ω_ _ω_

_|_ _|_ _−_

ZS×A

(iii) e
2LπCψ[2] _ω_ _ω_ + 2LνCψ[2] _ω_ _ω_ + 2CψLψ _ω_ eω := LF _ω_ _ω_
_≤_ _−_ _−_ _−_ _−_

where (i) applies triangle inequality and then Jensen’s inequality to the norm2, (iii) uses the equality thate _[ν][ω]e[(][s][)][ds][ =]_ _[π][ω][(][a][|][s]e[)][da][ = 1][ as well as the inequlities that]e ∥·∥, (ii) uses Assumption_

_S_ _A_

_ω[(][a][|][s][)][ −]_ _[π]ω[(][a][|][s][)][|][da][ = 2][d]TV_ _πω[(][·|][s][)][, π]ω[(][·|][s][)]_ 2Lπ _ω_ _ω_ (based on Assumption 2) and
thatA _[|][π][e]_ _ω[(][s][)][ −]_ _[ν]ω[(][s][)][|][ds][ = 2][d]RTV_ _νω(_ _s), νω[(][·|]R[s][)]_ _≤2Lν_ _∥ω[′]_ _−_ _ω_ _∥_ (based on the item 5).

R _S_ _[|][ν][e]_  e _·|_ ≤ _∥_ _−_ _∥_

e
R  e 


-----

Finally, the item 10 is proved by the following inequality
_h(ω) −_ _h(ω)_

= _F_ (ω)[−][1]∇J(ω) − _F_ (ω)[−][1]∇J(ω)

e

_≤_ 2 [F (ω)[−][1] _−_ _F_ (ω)[−][1]]∇J(ω) + 2 _F_ (ω)[−][1][∇J(ω) −∇J(ω)]∥

(i) e e
2DJ _F_ (ω)[−][1][F (ω) _F_ (ω)]F (ω)[−][1] + 2LJ _F_ (ω)[−][1] _ω_ _ω_
_≤_ e _−_ e e _−_

(ii)
_≤_ 2DJ _λ[−]F_ [2][L][F] _ω −_ _ω_ + 2LeJ _λ[−]F_ [1]e _ω −_ _ω_ := Lh _ω −_ _ω_ e,

where (i) uses the items 7 & 8, and (ii) uses the inequality that _F_ (ω)[−][1] = λmax(F (ω)[−][1]) =
_λmin[F_ (ω)][−][1] _λ[−]F_ [1] for all ω (sincee _F_ (ω) and F (ωe)[−][1] are positive definite) and the item ∥e _∥_ 9.
_≤_

Next, we bound the approximation error of the following stochastic (partial) policy gradients.


(t+1)N _−1_


_∇b_ _ω(m)_ _J(ωt) := N[1]_


(m)

_∇ω(m)_ _J(ωt) := N[1]_ _Ri_ + γφ(s[′]i+1[)][⊤][θ]t[(][m][)] _−_ _φ(si)[⊤]θt[(][m][)]_ _ψt[(][m][)](a[(]i[m][)]|si),_ (33)

_i=tN_

X  

b _J(ωt) :=_ _ω(1)J(ωt); . . . ;_ _ω(M_ ) _J(ωt)_ _,_ (34)

_∇_ _∇_ _∇_

(m)

_∇ω(m)_ _J(ωbt; Bt,k) := bN[1]k_ _Ri_ +[b] γφ(s[′]i+1[)][⊤][θ]t[(][m][)] _−_ _φ(si)[⊤]θt[(][m][)]_ _ψt[(][m][)](a[(]i[m][)]|si),_ (35)

_i∈BXt,k_  

b

_J(ωt;_ _t,k) :=_ _ω(1)J(ωt); . . . ;_ _ω(M_ ) _J(ωt)_ _._ (36)
_∇_ _B_ _∇_ _∇_

_AlgorithmLemma D.6.b_ _2 following Lemma Let Assumptionsb_ _D.4 1-5. Choose hold and adopt the hyperparameters of the decentralized TD in[b] T_ ln M

_[′]_ _≥_ 2 ln(σW[−][1][)] _[. Then, the following properties hold.]_


(m)
_1. The estimated average reward Ri_ _has the following bias and variance bound._

_M_

E _R(im)_ _−_ _Ri_ _Ri_ 2 ≤MσW2T ′ _[R]max[2]_ _[,]_ (37)
_m=1_

X _M_ 

(m)

_Var_ _Ri_ _Ri_ 4Rmax[2] _[σ][2][,]_ (38)

_≤_

_m=1_

X  

_where Ri := [Ri[(1)][;][ . . .][ ;][ R]i[(][M]_ [)]] denotes the joint reward.


_2. The stochastic policy gradients have the following error bound._

_Tc_

E _∇J(ωt) −∇J(ωt)_ _≤c4σW[2][T][ ′]_ + c5β[2]σW[2][T][ ′]c + c6 1 − _[λ]8[B]_ _[β]_
b [2][] + _[c][7]_ + 16Cψ[2] _[ζ]approx[critic]_  (39)

_N_ [+][ c]N[8]c


E _J(ωt;_ _t,k)_ _J(ωt)_ _t,k_ _c4σW[2][T][ ′]_ + 16Cψ[2] _θt[(][m][)]_ _θω[∗]_ _t_
_∇_ _B_ _−∇_ _F_ _≤_ _−_

_m=1_

  X
b [2] [2]

+ _[c][7]_ + 16Cψ[2] _[ζ]approx[critic]_ _[,]_ (40)

_Nk_

_where Ft,k := σ_ _Ft ∪_ _σ_ _{si, ai, si+1, s[′]i+1[,][ {][e][(]i[m][)]}m∈M}i∈∪kk−[′]_ =01 _[B][t,k][′]_ _._
   

_Proof. We will first prove the item 1._

When Ri := [Ri[(1)][;][ . . .][ ;][ R]i[(][M] [)]] is given and fixed, the randomness of _Ri[(][m][)]_ := Ri[(][m][)](1 + e[(]i[m][)]
and _ω(m)_ _J(ωt) defined in eq. (4) only comes from the noises_ _e[(]i[m][)]_ _m=1[. Since][ {][e][(]i[m][)]_ _m=1_
_∇_ _{_ _}[M]_ _}[M]_
are independent noises with zero mean and variances σ1[2][, . . ., σ]M[2] [,][ e]Ri := [[e] _Ri[(1)][;][ . . .][ ;][ e]Ri[(][M]_ [)]] has the
following moments[b]

E _Ri|Ri_ = Ri, [e]
 e 


-----

cov _Ri|Ri_ = diag (Ri[(1)][)][2][σ]1[2][, . . .,][ (][R]i[(][M] [)])[2]σM[2] := Σi.

(1) (m)  
Hence, _Ri := [Ri_ _[, . . ., R] e_ _i_ ][⊤] = W _[T][ ′][ e]Ri (the second “=” comes from eq. (3) and the nota-_
tions that _Ri[(][m][)]_ := _Ri,[(][m]0_ [)] [and that][ b]Ri[(][m][)] := _Ri,T[(][m][)][′][) has the moment that][ E]_ _Ri|Ri_ = W _[T][ ′]_ _Ri and_

Cov _Ri_ [b]Ri = W _[T][ ′]_ Σi(W _[T][ ′]_ )[⊤]. Therefore, eq. (37) can be proved as follows



_|_ [e] [b] [b]  b
 b M (m) 2 2 2

_m=1_ E _Ri_ _−_ _Ri_ _Ri_ = E _Ri −_ _Ri1_ _Ri_ = _W_ _[T][ ′]_ _Ri −_ _M[1]_ **[11][⊤][R][i]**

X   

 b 2

_W_ _[T][ ′]_ _Ri_ _MσW[2][T][ ′]_ _[R]max[2]_ _[,]_
_≤_ _−_ _M[1]_ **[11][⊤]** _∥_ _∥[2 (]≤[i][)]_

where 1 is a M -dim vector of 1’s, (i) uses the inequality that ∥Ri∥[2] = _m=1[(][R]i[(][m][)])[2]_ _≤_ _MRmax[2]_
(based on Assumption 3) and the item 4 of Lemma D.1. Then, eq. (38) can be proved as follows

_M_ [P][M]

var _R(im)_ _Ri_ = Var _Ri_ _Ri_ = tr (W _[T][ ′]_ )[⊤]ΣiW _[T][ ′]_ []

_|_

_m=1_

X    

 b 1

= tr _W_ _[T][ ′]_ Σi _W_ _[T][ ′]_ + tr (W _[T][ ′]_ )Σi
_−_ _M[1]_ **[11][⊤][]** _−_ _M[1]_ **[11][⊤][][⊤][i]** _M_ **[11][⊤][i]**
h 1  1 1h 

+ tr Σi(W _[T][ ′]_ )[⊤][i] + tr Σi

(i) h _M_ **[11][⊤][]** 2 h _M_ **[11][⊤][]**  _M_ **[11][⊤][i]1**
_MRmax[2]_ _[σ][2]_ _W_ _[T][ ′]_ + [2] _W_ _[T][ ′]_ Σi11[⊤][] +
_≤_ _−_ _M[1]_ **[11][⊤]** _M_ [tr] _M_ [2][ tr][[][1][(][1][⊤][Σ][i][1][)][1][⊤][]]

(ii) 1
_≤_ _MRmax[2]_ _[σ][2][σ]W[2][T][ ′]_ + M[2] **[1][⊤][Σ][i][W][ T][ ′]** **[1][ +]** _M_ [2][ (][1][⊤][Σ][i][1][)][tr][[][1][⊤][1][]]

(iii)
_Rmax[2]_ _[σ][2][ + 3]_
_≤_ _M_ **[1][⊤][Σ][i][1]**


= Rmax[2] _[σ][2][ + 3]_

_M_

(iv)
_≤_ 4Rmax[2] _[σ][2][,]_


(Ri[(][m][)])[2]σm[2]
_m=1_

X


where (i) uses the equality that tr(Y _[⊤]) = tr(Y ) and the inequality (41) below in which X =_
_W_ _[T][ ′]_ _M1_ **[11][⊤]** [and the][ m][-th entry of][ v][m][ ∈] [R][M][ is 1 while its other entries are 0, (ii) uses the item 4]
_−_

of Lemma D.1 and the equality that tr(xy[⊤]) = y[⊤]x for any x, y ∈ R[M], (iii) uses the condition that
_T_ _[′]_ _≥_ [ln M ]/[2 ln(σW[−][1][)]][ and the item 1 of Lemma][ D.1][, (iv) uses Assumption][ 3][.]


tr(XΣiX _[⊤]) =tr(X_ _[⊤]XΣi) =_


_vm[⊤][X]_ _[⊤][X][Σ][i][v][m]_
_m=1_ _[≤]_

X


_vm_ _X_ Σivm
_∥_ _∥∥_ _∥[2]∥_ _∥_
_m=1_

X


(Ri[(][m][)])[2]σm[2] _[∥][X][∥][2][ ≤]_ _[MR]max[2]_ _[σ][2][∥][X][∥][2][.]_ (41)
_m=1_

X


Next, we will prove eq. (39) in the item 2, where the error term can be decomposed as follows

_J(ωt)_ _J(ωt)_ 4 _J(ωt)_ _gt_ +4 _gt_ _gt[∗]_
_∇_ _−∇_ _≤_ _∇_ _−_ _−_
(I) (II)
b [2] b [2] [2]

+ 4| _gt[∗]_ {z _t_ +4} _g[∗]t|_ {z } _,_

_[−]_ _[g][∗]_ _[−∇][J][(][ω][t][)]_
(III) (IV )

[2] [2]

where we use the following notations that | {z } | {z }


(42)


_gt := [gt[(1)]; . . . ; gt[(][M]_ [)]], (43)


-----

(t+1)N _−1_

_i=tN_

X


_gt[(][m][)]_ := [1]


_Ri + γφ(s[′]i+1[)][⊤][θ]t[(][m][)]_ _φ(si)[⊤]θt[(][m][)]_ _ψt[(][m][)](a[(]i[m][)]_ _si),_ (44)
_−_ _|_



(t+1)N _−1_

_i=tN_

X


_gt[∗]_ [:= 1]


_Ri + γφ(s[′]i+1[)][⊤][θ]ω[∗]_ _t_ _ωt_ _ψt(ai_ _si),_ (45)

_[−]_ _[φ][(][s][i][)][⊤][θ][∗]_ _|_



_g[∗]t_ [:=][ E]s∼νωt _,a∼πt(·|s),s[′]∼P(·|s,a)_ _R(s, a, s[′]) + γφ(s[′])[⊤]θω[∗]_ _t_ _[−]_ _[φ][(][s][)][⊤][θ]ω[∗]_ _t_ _ψt(a|s)_ _ωt_ _._ (46)
  

Conditioned on the following filtration

_Ft[′]_ [:=][σ] _Ft ∪_ _σ_ _{si, ai, s[′]i+1[}][(]i=[t][+1)]tN_ +1[N] _[−][1]_

=σ{θt[(][′][m][)]} m∈M,0≤t′≤t ∪{si, ai, s[′]i+1[}][(]i=0[t][+1)][N] _[−][1]_ _∪{s(t+1)N_ _} ∪{{e[(]i[m][)]}m∈M}i[tN]=0[−][1]_ _,_

the error term (I) can be bounded as follows.  

E _J(ωt)_ _gt,k_ _t_
_∇_ _−_ _F_ _[′]_
h _M_ i
b [2]

= E _ω(m)_ _J(ωt)_ _gt,k[(][m][)]_ _t_

_∇_ _−_ _F_ _[′]_

_m=1_

h X i

(i) _M_ b (t+1)N _−1_ (m) [2] 2
= E _N[1]_ _Ri_ _−_ _Ri_ _ψt[(][m][)](a[(]i[m][)]|si)_ _Ft[′]_

_m=1_ _i=tN_

X h X    i

_M_ (t+1)N 1

(ii) 1 _−_ (m) 2
_≤_ E _N_ _Ri_ _−_ _Ri_ _ψt[(][m][)](a[(]i[m][)]|si)_ _Ft[′]_

_m=1_ _i=tN_

X h X    i

_M_ 1 (t+1)N _−1_ (m)

+ Var _N_ _Ri_ _−_ _Ri_ _ψt[(][m][)](a[(]i[m][)]|si)_ _Ft[′]_

_m=1_ _i=tN_

X h X    i

_M_ (t+1)N 1

(iii) 1 _−_ (m) 2
_≤_ E _N_ _Ri_ _−_ _Ri_ _Ft[′]_ _ψt[(][m][)](a[(]i[m][)]|si)_

_m=1_ _i=tN_

X h X    i

_M_ (t+1)N _−1_ (m)

+ N[1][2] Var _Ri_ _−_ _Ri_ _ψt[(][m][)](a[(]i[m][)]|si)_ _Ft[′]_

_m=1_ _i=tN_

X X    

_M_ (t+1)N 1

(iv) 1 _−_ (m)
_≤_ _N_ E _Ri_ _−_ _Ri_ _Ft[′]_ _ψt[(][m][)](a[(]i[m][)]|si)_

_m=1_ _i=tN_

X h X   [i][2]

+ N[1][2] _M_ (t+1)N _−1_ _ψt[(][m][)](a[(]i[m][)]|si)_ var _R(im)_ _−_ _Ri_ _Ft[′]_ [2]

_m=1_ _i=tN_

X X  

(≤v) _CNψ[2]_ _M_ (t+1)N _−1_ E _R(im)_ _−_ _Ri_ _Ft[′]_ [2] 2 + _NCψ[2][2]_ (t+1)N _−1_ _M_ var _R(im)_ _Ft[′]_

_m=1_ _i=tN_ _i=tN_ _m=1_

X X     X X  

(vi) _Cψ[2]_ [(][Mσ]W[2][T][ ′] _[R]max[2]_ [) +] _Cψ[2]_ max[σ][2][)]
_≤_ _N_ [(4][R][2]

= Cψ[2] _[R]max[2]_ _MσW[2][T][ ′]_ + [4] _,_ (47)
 _N [σ][2][]_

where (i) uses the definitions of _ω(m)_ _J(ωt) and gt[(][m][)]_ defined in eqs. (33) & (44) respectively, (ii)
_∇_
uses the relation that E∥X∥[2] = Var(X) + ∥(mE)X∥[2] for any random vector X, (iii) uses the facts that

_ψt[(][m][)](a[(]i[m][)]|si), Ri ∈Ft[′]_ [are fixed while][b] _[ {][R]i_ _}[(]i=[t][+1)]tN_ _[N]_ _[−][1]_ are random and independent given Ft[′][, (iv)]
uses the equality that Var(xY ) = _j=1_ [var][(][xy][j][) =][ P]j[d]=1 _[y]j[2][var][(][x][) =][ ∥][y][∥][2][var][(][x][)][ for any random]_

scalar x and fixed vector Y = [y1, . . ., yd] R[d] (Here we denote y = ψt[(][m][)](a[(]i[m][)] _si)_ _t[), (v)]_
_∈_ _|_ _∈F_ _[′]_
applies Jensen’s inequality to the convex function[P][d] (·)[2] and uses the item 1 of Lemma D.5 as well as


-----

the fact that Ri _t_ [is fixed, (vi) uses eqs.][ (][37][)][ &][ (][38][)][ and the fact that the conditional distribution]
(m) _∈F_ _[′]_
variables.of Ri on Ri ∈Ft[′] [is the same as that on][ F]t[′] [since the noise][ e]i[(][m][)] is independent from any other

Then we bound the error term (II) of eq. (42) as follows.


(t+1)N _−1_

_i=tN_

X


_gt −_ _gt[∗]_ = _m=1_ _N[1]_ _i_

X

[2] (i) (t+1)N _−1_

_≤_ _N[1]_

_i=tN_

X



[γφ(s[′]i+1[)][ −] _[φ][(][s][i][)]][⊤][(][θ]t[(][m][)]_ _−_ _θω[∗]_ _t_ [)] _ψt[(][m][)](a[(]i[m][)]|si)_



(i)
_≤_ _N[1]_ _γφ(s[′]i+1[)][ −]_ _[φ][(][s][i][)]_ _θt[(][m][)]_ _−_ _θω[∗]_ _t_ _ψt[(][m][)](a[(]i[m][)]|si)_

_i=tN_ _m=1_

X X

(ii) _Cψ[2]_ [(1 +][ γ][)][2] (t+1)N _−1_ _M_ [2] [2] [2]
_≤_ _N_ _θt[(][m][)]_ _−_ _θω[∗]_ _t_

_i=tN_ _m=1_

X X

_M_ [2]

= 4Cψ[2] _θt[(][m][)]_ _θω[∗]_ _t_ _,_ (48)

_−_

_m=1_

X

[2]

where (i) applies Jensen’s inequality to the convex function ∥· ∥[2], (ii) uses Assumption 4 and the
item 1 of Lemma D.5.


To bound the error term (III) of eq. (42), denote that

_X(s, a, s[′],_ _s) =_ _R(s, a,_ _s) + γφ(s)[⊤]θω[∗]_ _t_ _ωt_ _ψt(a_ _s),_ (49)

_[−]_ _[φ][(][s][)][⊤][θ][∗]_ _|_

which satisfies _X(s, a, s[′],_ _s)_  _R(s, a,_ _s)_ + _γφ(s) + φ(s)_ _θω[∗]_ _t_  _ψt(a_ _s)_ _Cψ(Rmax +_
_∥_ e∥≤ _|_ e _|_ e _|_ _≤_
2Rθ) (the second uses the item 3 of Lemma D.3) and X = Esi _νt_ _X(si, ai, si+1, s[′]i+1[)]_ _t_ = g[∗]t
_≤_  _∼_ _F_
whereHence, Lemma sN _, ωt ∈F D.2t := yields that σ_ _{θ et[(][′][m][)]}m∈M,0≤t′≤ et ∪{si, ai, se[′]i+1[,][ {][e][(]i[m][)]}m∈M[]}i[tN]=0[−][1]_ _∪{stN_ _}_ are fixed.
  


(t+1)N 1
_−_ 2

E _gt[∗]_ _t_ _t_ = E [1] _X(si, ai, si+1, s[′]i+1[)][ −]_ _[X]_ _t_

_[−]_ _[g][∗]_ _F_ _N_ _i=tN_ _F_
  h X i

[2] 9Cψ[2] [(][R][max][ + 2][R][θ][)][2][(][κ][ + 1][ −] _[ρ][)]_

_._ (50)

_≤_ _N_ (1 _ρ)_

_−_

Next, we bound the error term (IV) of eq. (42). Notice that


_g[∗]t_

_[−∇][J][(][ω][t][)]_

= Eωt _R(s, a,_ _s) + [γφ(s)_ _φ(s)][⊤]θω[∗]_ _t_ _R(s, a,_ _s) + γVωt_ (s) _Vωt_ (s) _ψt(a_ _s)_ _ωt_

_−_ _[−]_ _−_ _|_

= Eωt hγ _φ(s)[⊤] eθω[∗]_ _t_ _[−]_ _[V][ω]et_ [(]s[e]) _−_ _φ(s)[⊤]θω[∗]_ _t_ _[−]_ _[V][ω]t_ [(] e[s][)] _ψt(a|s)e_ _ωt_ _._ [] i(51)

Hence, h    [] i

e

2
_g[∗]t_ Eωt _γ_ _φ(s)[⊤]θω[∗]_ _t_ _t_ [(]s[e]) _φ(s)[⊤]θω[∗]_ _t_ _t_ [(][s][)] _ψt(a_ _s)_ _ωt_
_∥_ _[−∇][J][(][ω][t][)][∥][2][ =]_ _[−]_ _[V][ω]_ _−_ _[−]_ _[V][ω]_ _|_

(i) h    [] 2 i
Eωt _γ_ _φ(es)[⊤]θω[∗]_ _t_ _t_ [(]s[e]) _φ(s)[⊤]θω[∗]_ _t_ _t_ [(][s][)] _ψt(a_ _s)_ _ωt_
_≤_ _[−]_ _[V][ω]_ _−_ _[−]_ _[V][ω]_ _|_

(ii) h   2 [] 2 i
2Cψ[2] [E][ω]t _γ[2]eφ(s)[⊤]θω[∗]_ _t_ _t_ [(]s[e]) + _φ(s)[⊤]θω[∗]_ _t_ _t_ [(][s][)] _ωt_
_≤_ _[−]_ _[V][ω]_ _[−]_ _[V][ω]_
h 2 i

= 2Cψ[2] _[γ][2]_ eφ(s)[⊤]θω[∗] _t_ _t_ [(]s[e]) _νt(s)πt(a_ _s)_ (s _s, a)dsdads_
ZS×A×S _[−]_ _[V][ω]_ _|_ _P_ _|_

2
+ 2Cψ[2] [E][ω]t _φ(s)[⊤]θω[∗]et_ _t_ [(][s][)] _ωt_ e e

_[−]_ _[V][ω]_

(iii) h i2
2Cψ[2] _[γ]_ _φ(s)[⊤]θω[∗]_ _t_ _t_ [(]s[e]) _νt(s)πt(a_ _s)_ _ξ(s_ _s, a)dsdads_
_≤_ ZS×A×S _[−]_ _[V][ω]_ _|_ _P_ _|_

e e e


-----

+ 2Cψ[2] [E][ω]t _φ(s)[⊤]θω[∗]_ _t_ _t_ [(][s][)] _ωt_

_[−]_ _[V][ω]_

(iv) h i 2
= 2Cψ[2] [(][γ][ + 1)][E][ω]t _φ(s)[⊤]θω[∗]_ _t_ _t_ [(][s][)] _ωt_

_[−]_ _[V][ω]_

(v) h i
_≤_ 4Cψ[2] _[ζ]approx[critic]_ _[,]_ (52)

where (i) applies Jensen’s inequality to the convex function ∥· ∥[2], (ii) uses the inequality that
_∥x + y∥[2]_ _≤_ 2∥x∥[2] + 2∥y∥[2] for any x, y ∈ R[d], (iii) uses the inequality that P(s[′]|s, a) ≤
_γ[−][1]Pξ(s[′]|s, a); ∀s, s[′]_ _∈S, a ∈A, (iv) uses the equality that_ _S×A_ _[ν][t][(][s][)][π][t][(][a][|][s][)][P][ξ][(]s[e]|s, a)dsda =_

_νt(s), and (v) uses the notation that ζapprox[critic]_ [:= sup]ω [E][s][∼][ν]ω _VωR(s) −_ _φ(s)[⊤]θω[∗]_ . Substituting eqs.
(47),(48),(50)&(52) into eq. (42) yields that

e E _∇J(ωt) −∇J(ωt)_ _Ft_ [2][]

_M_

 
b4Cψ[2] _[R]max[2]_ _MσW[2][T][ ′]_ + [2] [4] + 16Cψ[2] _θt[(][m][)]_ _θω[∗]_ _t_

_≤_ _N [σ][2][]_ _−_

_m=1_

 X

36Cψ[2] [(][R][max][ + 2][R][θ][)][2][(][κ][ + 1][ −] _[ρ][)]_ [2]
+ + 16Cψ[2] _[ζ]approx[critic]_

_N_ (1 − _ρ)_

_M_

= c4σW[2][T][ ′] + _N[c][7]_ [+ 16][C]ψ[2] _θt[(][m][)]_ _−_ _θω[∗]_ _t_ + 16Cψ[2] _[ζ]approx[critic]_ _[,]_ (53)

_m=1_

X

[2]

where θt[(][m][)], ωt ∈Ft are fixed, and we take the conditional expectation of eq.36Cψ[2] [(][R][max][+2][R][θ][)][2][(][κ][+1] ([−]47[ρ][)]) on Ft ⊂Ft[′] [and]

denote that c4 := 4MCψ[2] _[R]max[2]_ [,][ c][7] [:= 16][C]ψ[2] _[R]max[2]_ _[σ][2][ +]_ 1−ρ . Substituting eq.

(25) into the unconditional expectation of eq. (53) yields that

E _∇J(ωt) −∇J(ωt)_

_Tc_

bc4σW[2][T][ ′] + _[c][7]_ _ψ[2][]σW[2][T][ ′]c_ _[β][2][c][2][ + 2][M]_ _c3_ 1 + _[c][1]_ + 16Cψ[2] _[ζ]approx[critic]_

_≤_ _N_ [+ 16][C] [2] _−_ _[λ]8[B]_ _[β]_ _Nc_

 _Tc_ h   i

= c4σW[2][T][ ′] + c5β[2]σW[2][T][ ′]c + c6 1 − _[λ]8[B]_ _[β]_ + _N[c][7]_ [+][ c]N[8]c + 16Cψ[2] _[ζ]approx[critic]_ _[,]_
 

where we denote that c5 := 16c2Cψ[2] [,][ c][6][ := 32][Mc][3][C]ψ[2] [,][ c][8][ := 32][Mc][1][C]ψ[2] [. This proves eq. (][39][).]

Equation (40) can be proved in the same way as that of proving eq. (53). There are two differences.
First, _∇J(ωt; Bt,k) uses the minibatch Bt,k of size Nk while_ _∇J(ωt) uses batchsize N_ . Second, eq.
(40) is conditioned on the filtration Ft,k := σ _Ft∪σ_ _si, ai, si+1, s[′]i+1[,][ {][e][(]i[m][)]}m∈M_ _i∈∪k[k][′][−]=0[1]_ _[B][t,k][′]_

[b] [b]

which includes not only the filtration Ft use by eq. ( 53) but also the minibatches ∪k[k][′][−]=0[1] _[B][t,k][′][ used by]_
the previous (k − 1) SGD steps.

**Lemma D.7. Implementing Algorithm 3 with η ≤** 2C1ψ[2] _[,][ T][ ′][ ≥]_ 2 ln(ln MσW[−][1][)] _[,][ T][z][ ≥]_ ln(3ln(DσJW[−]C[1][)]ψ[2][,][)]

_K ≥_ ln[(1−ηλln 3F /2)[−][1]] _[,][ N][ ≥]_ _ηλ[5]F_ [(1][−]2304[ρ][)(1]C[−]ψ[4] _[ηλ][(][κ][+1][F][ /][−][2)][ρ][(][)][K][−][1)][/][2][ and][ N][k][ ∝]_ [(1][ −] _[ηλ][F][ /][2)][−][k/][2][, the involved]_

_quantities have the following properties, where Eω denotes the expectation under the underlying_
_distributions that s_ _νω, a_ _πω(_ _s)._
_∼_ _∼_ _·|_

_1. λF ≤_ _λmax[F_ (ω)] = ∥F (ω)∥≤ _Cψ[2]_ _[,][ ∀][ω][.]_

_2._ 21 _[≤]_ [1][ −] _[ηC]ψ[2]_ _[≤]_ _I −_ _ηF_ (ω) _≤_ 1 − _ηλF, so η ≤_ 2λ1F _[.]_

_3. Cψ[−][2]_ _≤∥F_ (ω)[−][1]∥≤ _λ[−]F_ [1][. For any][ ω, x][ ∈] [R][d][ω] _[,][ x][⊤][F]_ [(][ω][)][−][1][x][ ≥] _[C]ψ[−][2][∥][x][∥][2][.]_

_4._ _h(ω)_ _≤_ _λ1F_ _∇J(ω)_ _≤_ _[D]λF[J]_ _[.]_

2
_5. h(ω) = arg minh_ Eω _ψω(a|s)[⊤]h −_ _Aω(s, a)_ _, so_

2
Eω _ψω(a|s)[⊤]h(ω)  −_ _Aω(s, a)_ _≤_ _ζapprox[actor]_ [where] _[ s][ ∼]_ _[ν][ω][,][ a][ ∼]_ _[π][ω][(][·|][s][)][.]_
   


-----

_6. Eω∗_ []ψω(a _s)[⊤]h(ω)_ _Aω(s, a)_ _C_ _ζapproxactor_ _[,][ ∀][ω][.]_
_|_ _−_ _≥−_ _∗_

_7. Nk =_ _N_ (1−ηλF /12)−[(][K](1[−]−[1]ηλ[−][k]F[)][/] /[2]2)(1[K/]−[√][2]1−ηλF /2)p≥ 576λC[4]Fψ[4][(1][(][κ][−][+1][ρ][)][−][ρ][)] _._

_8. ht approximates the natural gradient h(ωt) with the following error bound._


E _ht −_ _h(ωt)_ _≤_ _c10_ 1 − _[ηλ]2[F]_ (K−1)/2 + c11σW[2][T][z] + c12σW[2][T][ ′] + c13β[2]σW[2][T][ ′]c
   _Tc_

[2][] + c14 1 + _[c][15]_ + c16ζapprox[critic] _[.]_ (54)

_−_ _[λ]8[B]_ _[β]_ _Nc_
 

_Proof. The item 1 is proved by the following inequality._


(i) (ii)

_λF_ _λmin[F_ (ω)] _λmax[F_ (ω)] = _F_ (ω)

_≤_ _≤_ _∥_ _∥_

= Eω _ψ(a|s)ψ(a|s)[⊤][] ≤_ Eω _ψ(a|s)_ _ψ(a|s)[⊤]_ _≤_ _Cψ[2]_ _[,]_

where (i) uses Assumption 6, (ii) uses the fact that F (ω) is positive definite implied by Assumption 6,

[][ (][iii][)]

(iii) applies Jensen’s inequality to the convex function ∥· ∥ and (iv) uses Assumption 2.

Next we will prove the item 2. On one hand,

[(][i][)]
_λmin_ _I −_ _ηF_ (ω) = 1 − _ηλmax_ _F_ (ω) _≥_ 1 − _ηCψ[2]_ _[≥]_ 2[1] _[,]_ (55)
   1

where (i) uses the item 1, (ii) uses the condition that η ≤ 2Cψ[2] [. On the other hand,]

(i)
_λmin_ _I −_ _ηF_ (ω) _≤_ _λmax_ _I −_ _ηF_ (ω) = ∥I − _ηF_ (ω)∥ = I − _ηλmin_ _F_ (ω) _≤_ 1 − _ηλF, (56)_

where (i) uses the fact that  _I_  _ηF_ (ω) is positive definite based on eq. (55). Hence, eqs. (55) & (56)
_−_
prove the item 2.

The item 3 can be proved by the fact that F (ω)[−][1] is positive definite with minimum eigenvalue
_λmax[F_ (ω)][−][1] _Cψ[−][2]_ and maximum eigenvalue λmin[F (ω)][−][1] _λ[−]F_ [1] implied by the item 1.
_≥_ _≤_

The item 4 can be proved by the following inequality.

(i) (ii)
_∥h(ω)∥_ = _F_ (ω)[−][1]∇J(ω) _≤_ _F_ (ω[−][1]) _∇J(ω)_ _≤_ _λ[−]F_ [1] _∇J(ω)_ _≤_ _λ[−]F_ [1][D][J] _[,]_

where (i) uses the item 3 and (ii) uses the item 8 of Lemma D.5.

Next we will prove item 5.


Consider the following function of x ∈ R[d][ω] .

2

_fω(x) = [1]_ _ψω(a_ _s)[⊤]x_ _Aω(s, a)_

2 [E][ω] _|_ _−_
   

= [1] _ψω(a_ _s)ψω(a_ _s)[⊤][]x_ Eω _Aω(s, a)ψω(a_ _s)_ _⊤x + 1_ _Aω(s, a)[2][]_

2 _[x][⊤][E][ω]_ _|_ _|_ _−_ _|_ 2 [E][ω]
   

= [1] _Aω(s, a)[2][]_

2 _[x][⊤][F]_ [(][ω][)][x][ −∇][J][(][ω][)][⊤][x][ + 1]2 [E][ω]



Since ∇[2]f (ω) = F (ω) is positive definite, f is strongly convex quardratic and thus it has unique
minimizer h(ω) = F (ω)[−][1] _J(ω) obtained by solving h from the equation_ _fω(h) = F_ (ω)h
_∇_ _∇_ _−_
_∇J(ω) = 0. Hence,_

Eω _ψω(a|s)[⊤]h(ω) −_ _Aω(s, a)_

2
= min _ψω(a_ _s)[⊤]h_ _Aω(s, a)_
_h_ [E][ω] _|_ _−_ [2][]

2

   

sup _ψω(a_ _s)[⊤]h_ _Aω(s, a)_ := ζapprox[actor] _[,]_ (57)
_≤_ _ω_ [min]h [E][ω] _|_ _−_
   


-----

which proves the item 5.

The item 6 can be proved by the following inequality.

Eω[∗] []Aω(s, a) − _ψω(a|s)[⊤]h(ω)_

= _νω∗_ (s)πω∗ (a _s)_ _Aω(s, a)_ _ψω(a_ _s)[⊤]h(ω)_ _dsda_
_|_ _−_ _|_
Z
 


= _νω(s)πω(a_ _s)_ _[ν][ω][∗]_ [(][s][)][π][ω][∗] [(][a][|][s][)] _Aω(s, a)_ _ψω(a_ _s)[⊤]h(ω)_ _dsda_
_|_ _νω(s)πω(a_ _s)_ _−_ _|_
Z _|_

_νω∗_ (s)πω∗ (a _s)_  
= Eω _|_ _Aω(s, a)_ _ψω(a_ _s)[⊤]h(ω)_

_νω(s)πω(a_ _s)_ _−_ _|_

h _|_  [i]

_νω∗_ (s)πω∗ (a _s)_ 2 2 [(][i][)]
Eω _|_ Eω _Aω(s, a)_ _ψω(a_ _s)[⊤]h(ω)_ _C_ _ζapprox[actor]_ _[,]_ (58)

_≤_ s _νω(s)πω(a_ _s)_ _−_ _|_ _≤_ _∗_

h _|_  i[q]     q

where (i) uses Assumption 7 and the item 5. Multiplying −1 to the above inequality proves the item
6.

Next, the item 7 can be proved as follows.


(i) (1 _ηλF /2)[−][k/][2]_
_Nk_ =N _K−1 −_

_k[′]=0[(1][ −]_ _[ηλ][F][ /][2)][−][k][′][/][2]_
P 1 _ηλF /2)_

= _[N]_ [(1][ −] _[ηλ][F][ /][2)][(][K][−][1][−][k][)][/][2][(1][ −]_ _−_

1 (1 _ηλF /2)[K/][2]_
_−_ _−_ p

(ii) 2304Cψ[4] [(][κ][ + 1][ −] _[ρ][)]_ (1 − _ηλF /2)[(][K][−][1)][/][2](ηλF /2)_
_≥_ _ηλ[5]F_ [(1][ −] _[ρ][)(1][ −]_ _[ηλ][F][ /][2)][(][K][−][1)][/][2]_ 1 + 1 − _ηλF /2_

576Cψ[4] [(][κ][ + 1][ −] _[ρ][)]_ p

_,_

_≥_ _λ[4]F_ [(1][ −] _[ρ][)]_

where (i) uses the conditions that2304Cψ[4] [(][κ] N[+1]k[−] ∝[ρ][)] (1 − _ηλF /2)[−][k/][2]_ and _k=0_ _[N][k][ =][ N][ and (ii) uses the]_

condition that N ≥ _ηλ[5]F_ [(1][−][ρ][)(1][−][ηλ][F][ /][2)][(][K][−][1)][/][2]

[P][K][−][1]

Finally, we will prove the item 8. Until the end of this proof, we use the underlying distribution that
_ai_ _πt(_ _si),si+1_ _ξ(_ _si, ai) for tN_ _i_ (t + 1)N 1 in the t-th iteration of the multi-agent
NAC algorithm (Algorithm ∼ _·|_ _∼P_ _·|_ 1). _≤_ _≤_ _−_

The local averaging steps of zi,ℓ := [zi,ℓ[(1)][, . . ., z]i,ℓ[(][M] [)][]][⊤] [yield the following consensus error bound.]

_M_

(zT[(][m]z [)] _zTz_ )[2] = ∆zi,Tz = ∆W _[T][z]_ _zi,0_ = _W_ _[T][z]_ ∆zi,0 _σW[2][T][z]_
_m=1_ _−_ _∥_ _∥[2]_ _∥_ _∥[2 (][i][)] ∥_ _∥[2 (]≤[ii][)]_ _[∥][∆][z][i,][0][∥][2]_

X


_M_ _M_

(iii)

2

_≤_ _σW[2][T][z]_ (zi,[(][m]0 [)][)][2][ =][ σ]W[2][T][z] _ψt[(][m][)](a[(]i[m][)]|si)[⊤]h[(]t,k[m][)]_

_m=1_ _m=1_

XM X  

(iv)
_Cψ[2]_ _[σ]W[2][T][z]_ _h[(]t,k[m][)]_ _Cψ[2]_ _[σ]W[2][T][z]_ _ht,k_ _,_
_≤_ _≤_

_m=1_

X

where zTz := _M1_ _Mm=1_ _[z]i,T[(][m]z[)][, (i) and (ii) use the items 1 and 3 of Lemma][2]_ [2] [ D.1][ respectively, (iii) uses]

the equality that ∆ = 1, and (iv) uses the item 1 of Lemma D.5.
_∥P∥_

Then, we define the following stochastic gradients of function fω.


_∇ω(m)_ _fωt_ (ht,k) := N[1]k
e

_∇fωt_ (ht,k) := N[1]k
e


_ψt[(][m][)](a[(]i[m][)]_ _si)ψt(ai_ _si)[⊤]ht,k_ _ω(m)_ _J(ωt;_ _t,k)_
_i∈BXt,k_ _|_ _|_ _−_ _∇[b]_ _B_

_ψt(ai_ _si)ψt(ai_ _si)[⊤]ht,k_ _J(ωt;_ _t,k)_
_i∈BXt,k_ _|_ _|_ _−_ _∇[b]_ _B_


-----

= _ω(1)fωt_ (ht,k); . . . ; _ω(M_ ) _fωt_ (ht,k) _,_
_∇_ _∇_

_∇ω(m)_ _fωt_ (ht,k) :=eN[M]k _ψt[(][m][)](a[(]i[m][)]|s[e]i)zi,T[(][m]z[)]_ _∇ω(m)_ _J(ωt; Bt,k),_

_i∈BXt,k_ _[−]_ [b]

b

_fωt_ (ht,k) := _ω(1)fωt_ (ht,k); . . . ; _ω(M_ ) _fωt_ (ht,k) _⊤,_
_∇_ _∇_ _∇_

where _ω(m)_ _J(ωt;_ _t,k) and_ _J(ωt;_ _t,k) are defined in eqs. (35) & (_ 36) respectively. Hence,
_∇_ b _B_ b∇ _B_ [b]
_fωt_ (ht,k) _fωt_ (ht,k)

[b] _∇_ [b] _−_ _∇[e]_

_M_

b= _ω(m)_ _fωt_ (ht,k) [2] _ω(m)_ _fωt_ (ht,k)

_∇_ _−_ _∇[e]_

_m=1_

X

_M_ b [2] 2

= _N[1]k_ _Mzi,T[(][m]z[)]_ _ψt[(][m][)](ai|si)_

_mX=1_ _i∈BXt,k_  _[−]_ _[ψ][t][(][a][i][|][s][i][)][⊤][h][t,k]_

_M_

(i) 1
_≤_ _Nk_ _M_ _zi,T[(][m]z[)]_ _z_ _ψt[(][m][)](ai|si)_

_i∈BXt,k_ _mX=1_   _[−]_ _[z][T]_ 

[2]


_M_

(ii) _M_ [2]Cψ[2]
_≤_ _Nk_ (zi,T[(][m]z[)] _z_ [)][2][ ≤] _[M][ 2][C]ψ[4]_ _[σ]W[2][T][z]_ _ht,k_ _._ (59)

_i∈BXt,k_ _mX=1_ _[−]_ _[z][T]_

[2]

where (i) uses the equality that ψt(ai _si)[⊤]ht,k =_ _m_ _[z]i,T[(][m]z[)]_ [=][ Mz][T]z [, (ii) uses the item][ 1][ of]
_|_ _∈M_

Lemma D.5.

Since, ωt, ht,k ∈Ft,k while {si, ai}i∈Bt,k are random. Hence,[P]

E _∇fωt_ (ht,k) −∇fωt (ht,k) _Ft,k_

2

= Eb [1] _ψt(ai_ _si)ψt([2]ai_ _si)[⊤]_ []ht,k _J(ωt;_ _t,k)_ _F_ (ωt)ht,k + _J(ωt)_ _t,k_

_Nk_ _i_ _t,k_ _|_ _|_ _−_ _∇[b]_ _B_ _−_ _∇_ _F_

h _∈BX_  i

(i) 2
2E [1] _ψt(ai_ _si)ψt(ai_ _si)[⊤][]_ _F_ (ωt) _ht,k_ _t,k_
_≤_ _Nk_ _|_ _|_ _−_ _∥_ _∥[2]_ _F_

_i_ _t,k_

h _∈BX_  i

+ 2E _∇J(ωt; Bt,k) −∇J(ωt)_ _Ft,k_

(ii) 2
= 2E b[1] _ψt(ai_ _si)ψt(ai_ [2]si)[⊤][]  _F_ (ωt) _t,k_ _ht,k_

_Nk_ _|_ _|_ _−_ _F_ _∥_ _∥[2]_

_i_ _t,k_

h _∈BX_  i

+ 2E _∇J(ωt; Bt,k) −∇J(ωt)_ _Ft,k_

_M_

(iii) 18Cψ[4] [(][κ][ + 1][ −] _[ρ][)]_ 
_≤_ _Nbk(1_ _ρ)_ _∥ht,k∥[2]_ + 2c4[2]σW[2][T][ ′] + [2]N[c]k[7] + 32Cψ[2] _θt[(][m][)]_ _−_ _θω[∗]_ _t_ + 32Cψ[2] _[ζ]approx[critic]_ _[,]_

_m=1_

_−_

X

[2] (60)

where (i) uses the inequalities that ∥x + y∥[2] _≤_ 2∥x∥[2] + 2∥y∥[2] for any x, y ∈ R[d], (ii) uses
the fact that ht,k _t,k, and (iii) uses eq. (40) and applies Lemma D.2 to the quantity that_
_∈F_
_X(s, a, s[′],_ _s) = ψt(a|s)ψt(a|s)[⊤]_ in which ωt ∈Ft,k is fixed and ∥X(s, a, s[′], _s)∥F ≤_ _Cψ[2]_ [.]

Combining eqs. (59) & (60) yields that

e e

E _∇fωt_ (ht,k) −∇fωt (ht,k) _Ft,k_

2E _fωt_ (ht,k) _fωt_ (ht,k) _t,k_ + 2E _fωt_ (ht,k) _fωt_ (ht,k) _t,k_
_≤_ b∇ _−_ _∇[e]_ [2] _F_ _∇_ _−∇_ _F_

_Cψ[4]b2M_ [2]σW[2][T][z] + [36(][κ][ + 1][ −] _[ρ][)]_ [2] _ht,k_ + 4c4eσW[2][T][ ′] [2] 
_≤_ _Nk(1_ _ρ)_ _∥_ _∥[2]_
h _−_ i


+ [4]N[c]k[7] + 64Cψ[2]


_M_

_θt[(][m][)]_ _θω[∗]_ _t_ + 64Cψ[2] _[ζ]approx[critic]_ _[.]_ (61)
_−_

_m=1_

X

[2]


-----

Therefore,

E _ht,k+1 −_ _h(ωt)_ _Ft,k_

= E _ht,k_ _η_ _fωt_ (ht,k) _h(ωt)_ _t,k_
_−_ _∇_ [2] _−_ _F_

(i)

 

_≤_ (1 + ηλF )E b _ht,k −_ _η∇fωt_ (ht,k)[2] − _h(ωt)_ _Ft,k_

+ 1 + (ηλF )[−][1][]E _η_ _fωt_ (ht,k) _fωt_ (ht,k)  _t,k_
_∇_ _−∇_ [2] _F_

(ii)
= (1 + _ηλF )_ _ht,k −ηFb(ωt)_ _ht,k −_ _h(ωt)_ _−_ _h(ωt)_ [2] 

+ η _η + λ[−]F_ [1] E _fωt_ (ht,k) _fωt_ (ht,k ) _t,k_
_∇_ _−∇_ _F_ [2]

= (1 +  ηλF ) _I_ ηF (ωt) _ht,k_ _h(ωt)_ 
_−b_ _−_ [2]

+ η _η + λ[−]F_ [1] E [ _fωt_ (ht,k) _fωt_ (ht,k)] _t,k_

[] _∇_ _−∇_ [2] _F_

(iii)

    

(1 + ηλF )(1 _ηλbF )[2]_ _ht,k_ _h(ωt)_ [2]
_≤_ _−_ _−_

+ [2][η] _Cψ[4]_ 2M [2]σW[2][T][z] + [36(][κ][ + 1][ −] _[ρ][)]_ [2] _ht,k_ + 4c4σW[2][T][ ′]

_λF_ _Nk(1_ _ρ)_ _∥_ _∥[2]_

 h _−_ i


_M_

+ [4]N[c]k[7] + 64Cψ[2] _θt[(][m][)]_ _−_ _θω[∗]_ _t_ + 64Cψ[2] _[ζ]approx[critic]_

_m=1_

X 

_≤_ (1 − _ηλF )_ _ht,k −_ _h(ωt)_ [2]

+ [2][η] 2Cψ[4] 2M [2]σW[2][T][z] + [2][36(][κ][ + 1][ −] _[ρ][)]_ ( _ht,k_ _h(ωt)_ + _h(ωt)_ )

_λF_ _Nk(1_ _ρ)_ _∥_ _−_ _∥[2]_ _∥_ _∥[2]_

 h _−_ i


_M_

+ 4c4σW[2][T][ ′] + [4]N[c]k[7] + 64Cψ[2] _θt[(][m][)]_ _−_ _θω[∗]_ _t_ + 64Cψ[2] _[ζ]approx[critic]_

_m=1_

X 

(≤iv) 1 − _[ηλ]2[F]_ _ht,k −_ _h(ωt)_ + λ[2]F[η] 2Cψ[4] 2[2]M [2]σW[2][T][z] + [36(]N[κ]k[ + 1](1 _[ −]ρ)[ρ][)]_ _Dλ[2]FJ2_
  _M_  h _−_ i

[2]

+ 4c4σW[2][T][ ′] + [4]N[c]k[7] + 64Cψ[2] _θt[(][m][)]_ _−_ _θω[∗]_ _t_ + 64Cψ[2] _[ζ]approx[critic]_

_m=1_

X 

(v) [2]
_≤_ 1 − _[ηλ]2[F]_ _ht,k −_ _h(ωt)_ + λ[8]F[η] _Cψ[4]_ _[M][ 2][σ]W[2][T][z]_ + _N[c][9]k_
  _M_ 

[2]

+ c4σW[2][T][ ′] + 16Cψ[2] _θt[(][m][)]_ _θω[∗]_ _t_ + 16Cψ[2] _[ζ]approx[critic]_

_−_

_m=1_

X 

where (i) uses the inequality that ∥x + _y∥[2]_ _≤_ (1+ _ηλ[2]_ _F )∥x∥[2]_ +[1+(ηλF )[−][1]]∥y∥[2] for any x, y ∈ R[d],
(ii) uses the notation that _fωt_ (h) = F (ωt)h _J(ωt) = F_ (ωt)[h _h(ωt)] and the fact that_
_∇_ _−∇_ _−_
_ωt, ht,k_ _t,k, (iii) uses eq. (61) and the item 2 of this Lemma, (iv) uses the conditions that Tz_
ln(3DJ _C ∈Fψ[2]_ [)] 18Cψ[4] _[D]J[2]_ [(][κ][+1][−][ρ][)] _≥_

ln(σW[−][1][)] and the item 7 of this Lemma, and (v) uses the notation that c9 := _λ[2]F_ [(1][−][ρ][)] + c7.

Then, taking unconditional expectation of the above inequality and iterating it over k = 0, 1, . . ., K−1
yield that

E _ht −_ _h(ωt)_ = E _ht,K −_ _h(ωt)_

_K_ 1

 _K_  _−_ _K−1−k_

1 [2]E[] _ht,0_ _h(ωt)_ + [2][8][][η] 1
_≤_ _−_ _[ηλ]2[F]_ _−_ _λF_ _k=0_ _−_ _[ηλ]2[F]_
   X  

[2][] _M_

_Cψ[4]_ _[M][ 2][σ]W[2][T][z]_ + _N[c][9]k_ + c4σW[2][T][ ′] + 16Cψ[2] E _θt[(][m][)]_ _−_ _θω[∗]_ _t_ + 16Cψ[2] _[ζ]approx[critic]_

_m=1_

 X  

(i) _K_ [2][]
1 E _ht_ 1 _h(ωt)_
_≤_ _−_ _[ηλ]2[F]_ _−_ _−_
  

[2][]


-----

_M_

+ λ[16][2]F _Cψ[4]_ _[M][ 2][σ]W[2][T][z]_ + c4σW[2][T][ ′] + 16Cψ[2] _m=1_ E _θt[(][m][)]_ _−_ _θω[∗]_ _t_ + 16Cψ[2] _[ζ]approx[critic]_

 _K−1_ X  (K 1 _k)/2_ [2][] 

_−_ _−_

+ [8][ηc][9][[1][ −] [(1][ −] _[ηλ][F][ /][2)][K/][2][]]_ 1

_NλF (1_ 1 _ηλF /2)_ _k=0_ _−_ _[ηλ]2[F]_
_−_ _−_ X  

(ii) _K_

p

_≤_ 1 − _[ηλ]2[F]_ E _ht−1 −_ _h(ωt)_ + λ[16][2]F _Cψ[4]_ _[M][ 2][σ]W[2][T][z]_ + c4σW[2][T][ ′] + 16Cψ[2] _[ζ]approx[critic]_

+256Cψ[2] _σW[2][T][ ′]c_ _[β][2][c][2][ + 2][M]_ _c3_ 1 [2][]  Tc + _[c][1]_ + 8ηc9 

_λ[2]F_  h  _−_ _[λ]8[B]_ _[β]_ _Nc_ i _NλF (1 −_ 1 − _ηλF /2)[2]_

(iii) _K_

p

_≤_ 1 − _[ηλ]2[F]_ E _ht−1 −_ _h(ωt)_ + λ[16][2]F _Cψ[4]_ _[M][ 2][σ]W[2][T][z]_ + c4σW[2][T][ ′] + 16Cψ[2] _[ζ]approx[critic]_

+256Cψ[2] _σW[2][T][ ′]c_ _[β][2][c][2][ + 2][M]_ _c3_ 1 [2][] _T c_ + _[c][1]_ + [128][c][9]  (62)

_λ[2]F_ _−_ _[λ]8[B]_ _[β]_ _Nc_ _Nηλ[3]F_

 h   i

(iv) _K_
3 1 E _ht_ 1 _h(ωt_ 1) + _h(ωt_ 1) + _h(ωt)_
_≤_ _−_ _[ηλ]2[F]_ _−_ _−_ _−_ _−_ _−_
  

+ λ[16][2]F _Cψ[4]_ _[M][ 2][σ]W[2][T][z]_ + c4σW[2][T][ ′] + 16Cψ[2] _[ζ][2]approx[critic]_ [2] [2][]

  


+ 256Cψ[2] _σW[2][T][ ′]c_ _[β][2][c][2][ + 2][M]_ _c3_ 1 _Tc_ + _[c][1]_ + [128][c][9]

_λ[2]F_ _−_ _[λ]8[B]_ _[β]_ _Nc_ _Nηλ[3]F_

 h   i

(v) _K_ _K_
3 1 E _ht_ 1 _h(ωt_ 1) + [6][D]J[2] 1
_≤_ _−_ _[ηλ]2[F]_ _−_ _−_ _−_ _λ[2]F_ _−_ _[ηλ]2[F]_
    

+ λ[16][2]F _Cψ[4]_ _[M][ 2][σ]W[2][T][z]_ + c4σW[2][T][ ′] + 16Cψ[2] _[ζ][2]approx[][critic]_

+ 256 Cψ[2] _σW[2][T][ ′]c_ _[β][2][c][2][ + 2][M]_ _c3_ 1 _Tc_ + _[c][1]_ + [128][c][9] _,_

_λ[2]F_ _−_ _[λ]8[B]_ _[β]_ _Nc_ _Nηλ[3]F_

 h   i

where (i) uses the notation that ht,0 = ht, the item 7 of this Lemma and the inequality
that _k=0_ 1 − _[ηλ]2[F]_ _K−1−k_ _≤_ _ηλ2F_ [, (ii) uses Lemma][ D.4][, (iii) uses the inequality that]

(1−[√][P]1−1[K]ηλ[−]F[1] / 2)[2][ =] (1+[√](ηλ1−F /ηλ2)F[2] /2)[2] _≤_ (ηλ16F )[2][ implied by the item][ 2][ of this Lemma, (iv) uses]

the inequality that ∥x + y + z∥[2] _≤_ 3∥x∥[2] + 3∥y∥[2] + 3∥z∥[2], ∀x, y, z ∈ R[d], and (v) uses the items 4
of this Lemma. Taking unconditional expectation of the above inequality and iterating it over t yield
that
E _ht −_ _h(ωt)_

(i) _K_ _t_ _K_

 _J_

3 1 [2][] E _h0_ _h(ω0)_ + [12][D][2] 1
_≤_ _−_ _[ηλ]2[F]_ _−_ _λ[2]F_ _−_ _[ηλ]2[F]_
h   i   

+ λ[32][2]F _Cψ[4]_ _[M][ 2][σ]W[2][T][z]_ + c4σW[2][T][ ′] + 16C[2]ψ[2][][ζ]approx[critic]

  


+ 512Cψ[2] _σW[2][T][ ′]c_ _[β][2][c][2][ + 2][M]_ _c3_ 1 _Tc_ + _[c][1]_ + [256][c][9]

_λ[2]F_ _−_ _[λ]8[B]_ _[β]_ _Nc_ _Nηλ[3]F_

 h   i

(ii) _K_ _t_ _K_
3 1 1 E _h_ 1 _h(ω0)_
_≤_ _−_ _[ηλ]2[F]_ _−_ _[ηλ]2[F]_ _−_ _−_
h   i h  

+ λ[16][2]F _Cψ[4]_ _[M][ 2][σ]W[2][T][z]_ + c4σW[2][T][ ′] + 16Cψ[2] _[ζ]approx[critic]_ [2][]

  


_Tc_

_σW[2][T][ ′]c_ _[β][2][c][2][ + 2][M]_ _c3_ 1 + _[c][1]_ + [128][c][9]
_−_ _[λ]8[B]_ _[β]_ _Nc_ _Nηλ[3]F_
 _K_ h   i i

1 − _[ηλ]2[F]_ + λ[32][2]F _Cψ[4]_ _[M][ 2][σ]W[2][T][z]_ + c4σW[2][T][ ′] + 16Cψ[2] _[ζ]approx[critic]_

  


+ 256Cψ[2]

_λ[2]F_

_J_
+ [12][D][2]

_λ[2]F_


-----

+ 512Cψ[2] _σW[2][T][ ′]c_ _[β][2][c][2][ + 2][M]_ _c3_ 1 _Tc_ + _[c][1]_ + [256][c][9]

_λ[2]F_ _−_ _[λ]8[B]_ _[β]_ _Nc_ _Nηλ[3]F_

 h   i

(iii) _K_
_≤_ 2 1 − _[ηλ]2[F]_ _h−1_ + _[D]λ[2]FJ[2]_
   _K_ 

_J_ [2]
+ [12]λ[D][2]F [2] 1 − _[ηλ]2[F]_ + λ[48][2]F _Cψ[4]_ _[M][ 2][σ]W[2][T][z]_ + c4σW[2][T][ ′] + 16Cψ[2] _[ζ]approx[critic]_

+ 768Cψ[2] _σW[2][T][ ′]c_ _[β][2][c][2][ + 2][M]_ _c 3_ 1 _Tc_ + _[c][1]_ 

_λ[2]F_ _−_ _[λ]8[B]_ _[β]_ _Nc_

 h   i

+ [384][c][9] _ηλ[5]F_ [(1][ −] _[ρ][)(1][ −]_ _[ηλ][F][ /][2)][(][K][−][1)][/][2]_

_ηλ[3]F_ 2304Cψ[4] [(][κ][ + 1][ −] _[ρ][)]_

(≤iv) _c10_ 1 − _[ηλ]2[F]_ (K−1)/2 + c11σW[2][T][z] + c12σW[2][T][ ′] + c13β[2]σW[2][T][ ′]c
  _Tc_

+ c14 1 + _[c][15]_ + c16ζapprox[critic]
_−_ _[λ]8[B]_ _[β]_ _Nc_
 

where (i) uses the inequality thatK ln[(1 _ηλln 3F /2)[−][1]]_ [, (ii) uses eq.] 3(1(62 −) withηλF / t2) = 0[K] _≤, (iii) uses the condition that1 implied by the condition that N_
_≥_ _−_ _≥_

2304Cψ[4] [(][κ][+1][−][ρ][)]

_ηλ[5]F_ [(1][−][ρ][)(1][−][ηλ][F][ /][2)][(][K][−][1)][/][2][ as well as the inequalities that] _h−1 −_ _h(ω0)_ _≤_ 2 _h−1_ +

2 _h(ω0)_ _≤_ 2 _h−1_ + 2DJ[2] _[λ][−]F_ [2] (* uses the itemJ _F_ 4 of this Lemma) and that48Cψ[4] _[M][ 2]_ 3(1[2] _−_ _ηλF /2)768[K]c2≤[2]Cψ[2]1,_

(iv) denotes that[2][ ∗] _c10 := 2[2]_ _∥h−1∥[2]_ + [14]λ[D][2]F [2] + _[c]C[9][λ]ψ[4][2]_ [,][ c][11][ :=] _λ[2]F_, c12 := [48]λ[2]F[c][4] [,][ c][13][ :=] _λ[2]F_,

_c14 :=_ 1536Mcλ[2]F 3Cψ[2], c15 := 1536Mcλ[2]F 1Cψ[2], c16 := 768λ[2]FCψ[2] . This proves the item 8 of this Lemma.

E EXPERIMENT SETUP AND ADDITIONAL RESULTS

In this section, we present all the details of experiment setup and additional experiment results.

E.1 EXPERIMENT SETUP

We simulate a fully decentralized ring network with 6 fully decentralized agents, using communication
matrix with diagonal entries 0.4 and off-diagnonal entries 0.3. The shared state space contains 5
states and each agent can take 2 actions. We adopt the softmax policy πω(a _s)_ _e[ω][s,a]_ . The entries
_|_ _∝_
of the transition kernel and the reward functions are independently generated from the standard
Gaussian distribution (with proper normalization of the absolute value for the transition kernel). We
use the rows of a 5-dimensional identity matrix as state features. We set the discount factor γ = 0.95.

We implement and compare four decentralized AC-type algorithms in this multi-agent MDP: our
decentralized AC in Algorithm 1, our decentralized NAC in Algorithm 3, an existing decentralized
AC algorithm (Algorithm 2 of (70)) that uses a linear model to parameterize the agents’ averaged
reward R(s, a, s[′]) = _i_ _[λ][i][f][i][(][s, a, s][′][)][ (we name it DAC-RP1 for decentralized AC with reward]_

parameterization) [5], and our proposed modified version of DAC-RP1 to incorporate minibatch,
which we refer to as DAC-RP100 with batch size N = 100. For our Algorithm 1, we choose

[P]

_T = 500, Tc = 50, Tc[′]_ [= 10][,][ N][c] [= 10][,][ T][ ′][ =][ T][z] [= 5][,][ β][ = 0][.][5][,][ {][σ][m][}][6]m=1 [= 0][.][1][, and consider]
batch size choices N = 100, 500, 2000. Algorithm 3 uses the same hyperparameters as those of
Algorithm 1 except that T = 2000 in Algorithm 3. We select α = 10, 50, 200 for Algorithm 1 with
_N = 100, 500, 2000 respectively, and Tz = 5, α = 0.1, 0.5, 2, η = 0.04, 0.2, 0.8, K = 50, 100, 200,_
_Nk_ 2, 5, 10 for Algorithm 3 with N = 100, 500, 2000, respectively. For DAC-RP1 that was
originally designed for discount factor ≡ _γ = 1, we slightly adjust it to fit our setting where 0 < γ < 1[6]._

5The original algorithm in (70) uses the parameterization R(s, a) = _i_ _[λ][i][f][i][(][s, a][)][, and we extend to our]_

setting where the rewards also depend on the next state s[′].
6(70) defined the Q-function Qθ(s, a) = E _rt+1_ _J(θ)_ for policy parameter[P] _θ and used the temporal_

_−_
differences δt[i] [=][ r]t[i]+1 _t_ [+][ V]t+1[(][v]t[i][)][ −] _[V]t[(][v]t[i][)][ and][ e]δt[i]_ [=][ R]t[(][λ][i]t[)][ −] _[µ][i]t_ [+][ V]t+1[(][v]t[i][)][ −] _[V]t[(][v]t[i][)][ for critic]_

_[−]_ _[µ][i]_


-----

Figure 2: Comparison of _J(ωt)_ among decentralized AC-type algorithms for ring network .
_∥∇_ _∥[2]_

For this adjusted DAC-RP1, we select diminishing stepsizes βθ = 2(t + 1)[−][0][.][9], βv = 5(t + 1)[−][0][.][8]
as recommended in (70) and use the rows of a 1600-dimensional identity matrix as the reward
features _fi(s, a, s[′]) : s, s[′]_ _, a_ (i = 1, 2, . . ., 1600) to fully express R(s, a, s[′]) over all the
_{_ _∈S_ _∈A}_
5 × 2[6] _× 5 = 1600 triplets (s, a, s[′]). DAC-RP100 has batchsizes 100 and 10 for actor and critic_
updates respectively, and selects constant stepsizes βv = 0.5, βθ = 10. This setting is similar to
Algorithm 1 with N = 100 to inspect the reason of performance difference between Algorithm 1 and
DAC-RP1. All the algorithms are repeated 10 times using initial state 0 and the same initial actor
parameter ω0 generated from standard Gaussian distribution.

E.2 GRADIENT NORM CONVERGENCE RESULTS IN RING NETWORK

Figure 2 plots ∥∇J(ωt)∥[2] v.s. communication complexity (t(Tc + Tc + T _[′]) = 65t, t(Tc + Tc +_
_T_ + Tz) = 70t and 2t for Algorithms 1 & 3, and both DAC-RP algorithms, respectively)[7] and

_[′]_
sample complexity (t(TcNc + N ), 2t and 110t for both of our AC-type algorithms, DAC-RP1 and
DAC-RP100, respectively).[8] For each curve, its upper and lower envelopes denote the 95% and 5%
percentiles of the 10 repetitions, respectively.

Similar to the result of accumulative reward J(ωt) shown in Figure 1, it can be seen from Figure 2 that
the communication and sample efficiency of both our decentralized AC and NAC algorithms improve
with larger batchsize due to reduced gradient variance, which matches our understanding in Theorems
1 & 2. Our decentralized AC and NAC algorithms significantly outperform DAC-RP1 which has
batchsize 1. Using mini-batch, DAC-RP100 outperforms a lot than DAC-RP1, and converges to
critical points earlier than Algorithm 1. However, it can be seen from Figure 1 that such early
convergence turns out to have much lower J(ωt) than Algorithm 1 with N = 100 and Nc = 10.
Such a performance gap is caused by two reasons: (i) Both DAC-RP1 and DAC-RP100 suffer from
an inaccurate parameterized estimation of the averaged reward, and the mean relative estimation
errors of both DAC-RP1 and DAC-RP100 are over 100% [9]. In contrast, our noisy averaged reward
estimation achieves a mean relative error in the range of 10[−][5] _∼_ 10[−][4]. [10] ; (ii) Both DAC-RP1

update and actor update respectively. To fit 0 < γ < 1, we use δt[i] [=][ r]t[i]+1 [+][ γV]t+1[(][v]t[i][)][ −] _[V]t[(][v]t[i][)][ and]_
_δused two different chains generated from transition kernelst[i]_ [=][ R]t[(][λ][i]t[) +][ γV]t+1[(][v]t[i][)][ −] _[V]t[(][v]t[i][)][ where][ µ][i]t_ _[≈]_ _[J][(][θ]t[)][ is removed since] P, Pξ respectively for critic update and actor update[ Q]θ[(][s, a][) =][ E][(][r]t+1[)][. In addition, we]_
as in our Algorithme 1.
7Each update of our decentralized AC uses Tc + T ′c [and][ T][ ′][ communication rounds for synchronizing critic]
model and rewards, respectively. Each update of our decentralized NAC uses Tc + Tc[′][,][ T][ ′][,][ T][ ′]z [communication]
rounds for synchronizing critic model, rewards and scalar z, respectively. Each update of both DAC-RP1 and
DAC-RP100 uses 1 communication round for synchronizing v and λ respectively.
8DAC-RP1 uses 1 sample for actor and critic updates respectively. DAC-RP100 uses 100 and 10 samples for
actor and critic updates respectively.
9The relative reward estimation error at the t-th iteration of both DAC-RP1 and DAC-RP100 is defined as A/B where A = _M_ 1 _Mm=1_ _s,s[′]_ _a_ _i_ _[λ]i[(][m][)]fi(s, a, s[′])][2]_ and B =

1 _|S|[2]|A|_ _∈S_ _∈A[[][R][(][s, a, s][′][)][ −]_ [P]

_|S|10[2]|A|At thePs,s t-th iteration of Algorithms[′]∈S_ Pa∈A _[R][(][s, a, s][′][)][2][.]_ P 1 & 3, we focus onP P _rt[(][m][)]_ = _N1_ (i=t+1)tN _N_ _−1_ _Ri(m)_ as the estimation of

the batch-averaged reward rt = _N1_ (i=t+1)tN _N_ _−1_ _Ri since its estimation error affects the accuracy of the policyP_

gradient (4). The relative estimation error is defined asP _Mr1_ [2]t _Mm=1[(][r]t[(][m][)]_ _−_ _rt)[2]._

P


-----

and DAC-RP100 apply only a single TD update per-round, and hence suffers from a larger mean
TD learning error (about 2% and 1% for DAC-RP1 and DAC-RP100, respectively), whereas our
algorithms perform multiple TD learning updates per-round and achieve a smaller mean relative error
(about 0.3% and 0.07% for our decentralized AC and NAC respectively) [11]. All these relative errors
are averaged over iterations.

E.3 ADDITIONAL EXPERIMENTS IN FULLY CONNECTED NETWORK

To investigate the effect of network topology on the performance of our algorithms, we also conduct
the above experiments on a fully connected network with 6 fully decentralized agents, using communication matrix with diagonal entries 0.4 and all the other entries 0.12. The MDP environment and all
the hyperparameters are the same as the above experiments for ring network. Figures 3 & 4 plot the
learning curves of the optimality gap J _[∗]_ _J(ωt) and_ _J(ωt)_ respectively for fully connected
_−_ _∥∇_ _∥[2]_
network. To make comparison, we plot J _[∗]_ _J(ωt) and_ _J(ωt)_ in Figures 5 & 2 respectively for
_−_ _∥∇_ _∥_
the above experiments with ring network. It can be seen by comparing these figures that network
topology does not much affect the performance of these algorithms, so the conclusions for ring
network that we summarized right before this subsection also holds for fully connected network.

Figure 3: Comparison of optimality gap J(ω[∗]) _J(ωt) among decentralized AC-type algorithms in_
_−_
fully connected network.

Figure 4: Comparison of _J(ωt)_ among decentralized AC-type algorithms in fully connected
_∥∇_ _∥[2]_
network.

Figure 5: Comparison of optimality gap J(ω[∗]) _J(ωt) among decentralized AC-type algorithms in_
_−_
ring network.


_M_ _θ1[∗]_ _Mm=1_ _t_ _θω[∗]_ _t_ _[∥][2][.]_
_∥_ _ωt_ _[∥][2]_ _[∥][θ][(][m][)]_ _−_
P


11The TD error at the t-th iteration is defined as


-----

E.4 TWO-AGENT CLIFF NAVIGATION

In this subsection, we test our algorithms in solving a two-agent Cliff Navigation problem (39)
in a grid-world environment. This problem is
adapted from its single-agent version (see Example 6.6 of (48)). As illustrated in Figure 6,
two agents start from the starting point “S” on
a 3 × 4 grid and aim to reach the destination
“D”. Here, global state is defined as the joint location of the two agents, and there are in total
(3 × 4)[2] = 144 global states. In most states,
an agent can choose to move up, down, left or
right by one step and receives −1 reward. However, once an agent falls into the cliff “X”, it

Figure 6: Two-agent cliff navigation. (“S”, “X”,

will return to the starting point “S” and receives

“D” denote starting point, cliff and destination re
100 reward. When an agent reaches “D”, it
_−_ spectively. The optimal path is shown in red.)
will always stay at “D”, and receives 0 reward
if the other agent also reaches/stays at “D”, or
receives −0.5 reward otherwise. If an agent is not at “X” or “D” and selects a direction that points
outside the grid, then it stays in the previous location and receives −1 reward. The optimal path
for both agents is the red path shown in Figure 6, which has the minimum accumulative reward
_J_ _[∗]_ = −0.1855 under the discount factor γ = 0.95.

For our Algorithm 1, we choose T = 500, Tc = 50, Tc[′] [= 10][,][ N][c] [= 10][,][ T][ ′][ =][ T][z] [= 5][,][ β][ = 0][.][5][,]
_{σm}m[6]_ =1 [= 0][.][1][, and consider batch size choices][ N][ = 100][,][ 500][,][ 2000][. Our Algorithm 3 uses the]
same hyperparameters as those of Algorithm 1 except that we choose T = 2000. We select α =
1, 5, 20 for Algorithm 1 with N = 100, 500, 2000 respectively, and Tz = 5, α = 0.002, 0.01, 0.04,
_η = 0.002, 0.01, 0.04, K = 50, 100, 200, Nk_ 2, 5, 10 for Algorithm 3 with N = 100, 500, 2000,
respectively. For DAC-RP1, we select T = 10000 ≡, βv = 10(t + 1)[−][0][.][6] and βθ = 5(t + 1)[−][0][.][6]. For
DAC-RP100, we use T = 2000 and batchsizes 100 and 10 for actor and critic updates respectively,
and selects constant stepsizes βv = 0.5, βθ = 1. This setting is similar to Algorithm 1 with N = 100
to inspect performance difference between Algorithm 1 and DAC-RP1.


Figure 7: Comparison of optimality gap J(ω[∗]) _J(ωt) among decentralized AC-type algorithms on_
_−_
cliff navigation.

Figure 8: Comparison of optimality gap J(ω[∗]) _J(ωt) among decentralized AC-type algorithms on_
_−_
cliff navigation.


-----

We plot J _[∗]_ _J(ωt) and_ _J(ωt)_ in Figures 7 & 8 respectively. It can be seen from these figures
_−_ _∥∇_ _∥_
that both our Algorithm 1 & Algorithm 3 significantly reduce the function value gap J _[∗]_ _J(ωt), and_
_−_
their convergence is faster with a larger batchsize. In contrast, the function value gaps of DAC-RP1
and DAC-RP100 do not decrease sufficiently and converge to a high value. In particular, since
DAC-RP100 achieves a larger function value gap than our Algorithm 1 with N = 100 while their
hyperparameter choices are similar, we attribute this performance gap to the inaccurate average
reward estimation and TD error, as we analyzed in Appendix E.2.

F CONSTANT SCALARS

The following global constants are frequently used.

_M_ : The number of agents.

_γ: Discount rate._

(AssumptionRmax: The reward bound such that 3). Hence, 0 _R(m)(s, a, s 0 ≤′), RR[(]i([m]m[)])(, Rs, a, si_ _[′])R ≤maxR._ max for any s, s[′] _∈S and a ∈A_
_≤_ _≤_

_σW_ [0, 1): The second largest singular value of W .
_∈_

_ω[∗]_ := maxω J(ω) denotes the optimal policy parameter.

The following constants are defined in Lemma D.3.

_CB := 1 + γ._

_Cb := Rmax._

_λφ := λmin_ Es _µω_ [φ(s)φ(s)[⊤]] _> 0 satisfies Assumption 4._
_∼_

_λB := 2(1 − _ _γ)λφ > 0. (Assumption_ 4 implies that λφ > 0.)

_Rθ :=_ [2]λ[C]B[b] [.]

The policy-related norm bounds and Lipschitz parameters are defined as follows.


_Cψ, Lψ, Lπ > 0 defined in Assumption 2: For all s ∈S, a ∈A and ω,_ _ω, ∥ψω(a|s)∥≤_ _Cψ,_
_∥ψω[(][a][|][s][)][ −]_ _[ψ]ω[(][a][|][s][)][∥≤]_ _[L]ψ[∥]ω[e] −_ _ω∥_ and dTV _πω[(][·|][s][)][, π]ω[(][·|][s][)]_ _≤_ _Lπ∥ω −_ _ω∥._
e

_Lν :=e_ _Lπ[1 + logρ(κ[−][1]) + (1_ _ρ)[−][1]]._   e 
_−_ e

_LQ :=_ [2][R]1[max]−γ[L][ν] .

_LJ := Rmax(4Lν + Lψ)/(1 −_ _γ)._

_DJ :=_ _[C][ψ]1[R]−[max]γ_ .

_LF := 2Cψ(LπCψ + LνCψ + Lψ)._

_Lh := 2λ[−]F_ [1][(][D][J] _[λ]F[−][1][L][F][ +][ L][J]_ [)][ where][ λ][F][ := inf] _[ω][∈][Ω]_ _[λ][min][[][F]_ [(][ω][)]][ >][ 0][ (][λ][min][ denotes the minimum]
eigenvalue) which satisfies Assumption 6.

The following constants are defined to simplify the notations in the proof.

1920(CB[2] _[R]θ[2][+][C]b[2][)][[1+(][κ][−][1)][ρ][]]_
_c1 :=_ (1 _ρ)λ[2]B_ .

_−_

_c2 := 2_ 21−MCσWb 2.

_c3 := 2 _ _θ−1_  + Rθ[2] where θ−1 is the initial parameter of decentralized TD (Algorithm 2).

_c4 := 4 MCψ[2]_ _[R]max[2]_ [.] 

[2]


-----

_c5 := 16c2Cψ[2]_ [.]

_c6 := 32Mc3Cψ[2]_ [.]

36Cψ[2] [(][R][max][+2][R][θ][)][2][(][κ][+1][−][ρ][)]
_c7 := 16Cψ[2]_ _[R]max[2]_ _[σ][2][ +]_ 1−ρ

_c8 := 32Mc1Cψ[2]_ [.]


18Cψ[4] _[D]J[2]_ [(][κ][+1][−][ρ][)]
_c9 :=_ _λ[2]F_ [(1][−][ρ][)] + c7.

_J_ _F_
_c10 := 2∥h−1∥[2]_ + [14]λ[D][2]F [2] + _[c]C[9][λ]ψ[4][2]_ [where][ h][−][1][ is the initial natural gradient of Algorithm][ 3][.]


48Cψ[4] _[M][ 2]_
_c11 :=_ _λ[2]F_

_c12 :=_ [48]λ[2]F[c][4] [.]


_c13 :=_ 768λc[2]F2Cψ[2] .

_c14 :=_ 1536Mcλ[2]F 3Cψ[2]

_c15 :=_ 1536Mcλ[2]F 1Cψ[2]


_c16 :=_ 768λ[2]FCψ[2] .

_c17 := Es∼νω∗_ KL _πω∗_ (·|s)||π0(·|s) + 4LψCλψ[2][2]F[R][max]

_c18 := Cψ[√]c10 + c 10Lψ_ 1 + 4λC[2]Fψ[4] .
 


_c19 := Cψ[√]c11 + c11Lψ_ 1 + 4λC[2]Fψ[4]

_c20 := Cψ[√]c12 + c12Lψ1 +_ 4λC[2]Fψ[4]

_c21 := Cψ[√]c13 + c13Lψ1 +_ 4λC[2]Fψ[4]

_c22 := Cψ[√]c14 + c14Lψ1 +_ 4λC[2]Fψ[4]

_c23 := Cψ[√]c15 + c15Lψ1 +_ 4λC[2]Fψ[4]

_c24 := c16Lψ_ 1 + 4λC[2]ψ[4] .
 


-----

