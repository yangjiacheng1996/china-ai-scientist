# SPP-RL: STATE PLANNING POLICY REINFORCEMENT LEARNING

**Anonymous authors**
Paper under double-blind review

ABSTRACT

We introduce an algorithm for reinforcement learning, in which the actor plans
for the next state provided the current state. To communicate the actor output to
the environment we incorporate an inverse dynamics control model and train it
using supervised learning. We train the RL agent using off-policy state-of-the-art
reinforcement learning algorithms: DDPG, TD3, and SAC. To guarantee that the
target states are physically relevant, the overall learning procedure is formulated
as a constrained optimization problem, solved via the classical Lagrangian optimization method. We benchmark the state planning RL approach using a varied
set of continuous environments, including standard MuJoCo tasks, safety-gym
level 0 environments, and AntPush. In SPP approach the optimal policy is being searched for in the space of state-state mappings, a considerably larger space
than the traditional space of state-action mappings. We report that quite surprisingly SPP implementations attain superior performance to vanilla state-of-the-art
off-policy RL algorithms in a wide class of robotic locomotion environments.

1 INTRODUCTION

Research on reinforcement learning (RL) has brought a tremendous number of successful applications in diverse fields of science and technology. Application areas of RL can be split into two
classes: discrete and continuous. Here, we are interested in continuous simulation environments,
mostly in the robotics domain. Despite the magnitude of successful applications of RL in this domain, two of the main issues: sample efficiency and interpretability of RL trained agents, persist.
These problems have utmost practical importance, especially for mission-critical applications. The
current methods often require a vast amount of experience for training. The decision-making process of trained agents is not interpretable, sometimes resulting in finding proxy solutions. Thus, it is
vital to research new RL algorithms that may partially solve the mentioned problems.

Traditionally, RL is based on the principle of searching for the optimal policy within the space of
state-action mappings. We propose a new algorithm based on the principle of training an actor (a
policy) operating entirely in the state space (state-state mappings). We call such policies the state
planning policies (SPP), whose actions determine desired trajectories in the state-space. The task
of training SPP may initially seem infeasible due to a significantly larger dimension of states than
of actions. Nonetheless, quite surprisingly, we show that the approach is feasible and often leads to
significant improvements in average performance for a class of robotic locomotion tasks.

We call our approach State Planning Policy Reinforcement Learning (SPP-RL). It is a generic approach for problems specified using continuous environments. The main building block of SPP-RL
– the RL agent can be implemented using virtually any RL algorithm. We chose to develop our
approach using the state-of-the-art off-policy DDPG (Lillicrap et al., 2016), TD3 (Fujimoto et al.,
2018), and SAC (Haarnoja et al., 2018a) algorithms. Note that, in SPP-RL we need another trainable model to communicate the policy output to the environment; as such, we incorporate a learnable
inverse dynamics control model (IDM), see Fig. 1. The overall algorithm optimizes the policy simultaneously with CM. To ensure that the policy target states satisfy physical and under-actuation
constraints, we formulate a constrained optimization objective for policy training. Our work lies
within the category of RL methods that already implemented state-state policies, including work on
hierarchical RL like Nachum et al. (2018), and the D3G algorithm from Edwards et al. (2020).


-----

**Summary of results.** Although the SPP-RL algorithm searches for the optimal policy within a
much larger space, our performance benchmarks revealed that SPP-RL implementations often outperform their vanilla RL counterparts. Experiments performed in Ant & Humanoid tasks from MuJoCo suite (Todorov et al., 2012; Brockman et al., 2016) reveal that SPP-DDPG outperforms DDPG.
Experiments in Safety-Gym Level 0 environments (Ray et al., 2019) demonstrate that SPP-TD3 and
SPP-SAC outperform by a great margin TD3 and SAC, respectively. Experiments in AntPush task
(Nachum et al., 2018) show that SPP-TD3 outperforms hierarchical RL method HIRO (Nachum
et al., 2018) and provides some interpretability of the agent behavior.

We hypothesize that the superior performance of SPP-RL in the tested continuous environments
originates in more efficient state-space exploration by state-state policies than traditional state-action
policies; here noise is being added to target states rather than actions. To argue this, we performed
series of experiments, including evaluation of a shadow agent utilizing experience from SPP and
vanilla replay buffers (Sec. 5.4) and a study of the distributions of states gathered in different replay
buffers (App. E.2). To analyze which features of the algorithm are crucial for superior performance,
we report on a thorough ablation study of SPP-TD3 (App. E.1).

We implemented SPP-RL methods as a modular PyTorch library shared as open-source. SPP-RL
algorithms are derived from their vanilla RL counterparts, making extending the library with new
RL algorithms straightforward. We also share videos with test episodes of the trained agents to
accompany benchmark plots (web, 2021).

Last but not least, we demonstrate theoretical convergence of clipped Double Q-learning implemented in SPP-TD3 algorithm within the classical stochastic processes, finite-state/action spaces
setting and in environments based on rigid body dynamics model, refer to App. B.3.

1.1 RELATED WORK

We present a (non-exhaustive) list of related works; refer to Tab. 1 for a perspective on related work.
The closest approach to ours is the D3G algorithm introduced by Edwards et al. (2020), which
includes state planning policies, and introduces a novel form of the value function defined on statenext state pairs. There are two main ways our method is distinct. First, SPP employs the classical
formulation of the value function. Also, we do not include a forward dynamics model nor the cycle
loss. Instead, to guarantee consistency of the policy target-states in SPP, we formulate a constrained
optimization problem (compare Fig. 2) solved via Lagrangian optimization.

Our work builds on the classical RL algorithms going back to REINFORCE (Williams, 1992), Asynchronous Actor-Critic (Mnih et al., 2016), and especially the off-policy actor-critic algorithms including Q-Prop (Gu et al., 2017), DDPG (Lillicrap et al., 2016), SAC (Haarnoja et al., 2018a;b), and
TD3 (Fujimoto et al., 2018). State planning policies have been used in hierarchical RL (HRL) methods like HIRO (Nachum et al., 2018) and FuN (Vezhnevets et al., 2017). Contrary to HRL SPP-RL
approach does not employ a hierarchy of multiple policies nor state conditioned value functions.

Training predictive models (like IDMs) is fundamental for the model-based RL approach including
algorithms: a locally linear latent dynamics model (Watter et al., 2015), model-based planning for
discrete and continuous actions (Henaff et al., 2017), model-predictive control (Chua et al., 2018),
and model based policy optimization (Janner et al., 2019). We deployed IDMs for mapping currenttarget states to actions; other applications of IDMs in RL include the context of planning: search on
replay buffer (Eysenbach et al., 2019), episodic memory graph (Yang et al., 2020), and topological
memory for navigation (Savinov et al., 2018). Existing many other applications of IDM in context
of RL including: policy adaptation during deployment (Hansen et al., 2021), sim to real transfer
(Christiano et al., 2016), adversarial exploration (Hong et al., 2020), and curiosity-driven exploration
(Pathak et al., 2017).

1.2 BACKGROUND

Following the standard setting used in RL literature, we work with infinite horizon Markov decision
_process (MDP) formalism (_ _,_ _, P, r, ρ0, γ), where_ is a state space, is a action space, P :
_S_ _A_ _S_ _A_ _S ×_
_A × S →_ [0, 1] is a transition probability distribution, r : S × A → R is a reward function, ρ0 is an
_initial state distribution, and γ ∈_ (0, 1) is a discount factor. From now on we assume that the MDP
is fixed. In RL the agent interacts with E in discrete steps by selecting an action at for the state st


-----

Table 1: A Perspective on Related Work

**Technique** **State-state policy** **Inverse/forward model** **State cond. Q funct.** **Policy Hierarchy** **Planning horizon**

SPP (ours) yes inverse no single policy single step
D3G yes inverse yes single policy single step
HRL yes(upper level) inverse yes(upper level) multiple policies multiple steps
Planning yes inverse yes N/A multiple steps
Model based no forward N/A single policy single step

at time t, causing the state transition st+1 = E(st, at), as a result the agent collects a scalar reward
_rt+1(st, at), the return is defined as the sum of discounted future reward Rt =_ _i=t_ _[γ][(][i][−][t][)][r][(][s][i][, a][i][)][.]_
The goal in RL is to learn a policy that maximizes the expected return from the start distribution.

[P][T]

2 STATE PLANNING POLICY REINFORCEMENT LEARNING APPROACH

Our SPP approach is rooted in state-state reinforcement learning, by which we mean setting in which
RL agent is trained to plan goals in the state-space, the approach already employed e.g., in HRL,
planning, D3G RL algorithms (see Tab. 1). In SPP a state planning policy π given the current state
_st outputs zt – the desired target state to be reached by the environment in the next step. Forcing_
the environment to reach the desired state requires translating the target state to a suitable action at.
Hence, we employ an additional model capable of mapping the current state-target state pair (st, zt)
to the action at – a (trainable) IDM model. Ideally, we like to have consistency zt(st) ≈ _st+1._
The consistency cannot be guaranteed a-priori, is rather achieved in SPP setting by employing a
constrained optimization approach. A diagram illustrating SPP approach is presented in Fig. 1.

We have freedom of choice of the particular RL algorithm (RL agent) and IDMs implementations.
Currently, we use feed-forward neural networks, and RL Agent using implementations of the stateof-the-art off-policy RL algorithms: DDPG (Lillicrap et al., 2016), TD3 (Fujimoto et al., 2018)
and SAC (Haarnoja et al., 2018a). We present details of SPP-RL implementation in Sec. 4 using
as the example SPP-DDPG. The encountered experiences during the execution of an off-policy RL
algorithm are stored in replay buffer D.

The main building block of SPP-RL are the state planning policies, intuitively a state planning policy
selects a desired trajectory in the state-space of the environment.
**Definition 1. We call a state planning policy a map πθ : S →P(S) parametrized using a vector of**
parameters θ ∈ R[n][(][θ][)], and we denote πθ(z|s) a probability of the desired target state z ∈S for the
given current state s ∈S.

We call a deterministic state planning policy a parametrized map πθ : S →S, and we denote
_πθ(s) = z._

We assume that π has continuous and bounded derivatives with respect to θ. We will call state
planning policy whenever it is clear from the context deterministic/stochastic and omit the parameter
subscript π = πθ.

Besides the state planning policy (Def. 1) the second main building block of the overall SPP agent
is a model for mapping the current state-target pair (st, zt) to suitable action at. Following the
existing literature, we call such model the inverse dynamics control model (IDM), or simply the
_control model._
**Definition 2. For a given MDP (S, A, P, r, ρ0, γ). Let s, z ∈S. We define the control model:**

CM : S × S →A, CM(s, z) = a,

i.e. for the given two states CM computes the action a. We call s, z the initial state and the target
state respectively, where a informally satisfies arg maxb _P_ (s, a, b) _z for stochastic E, or z_
_∈S_ _∼_ _≈_
_E(s, a) for deterministic E._

Obviously, in order to work, SPP requires consistency of the target states generated by the policy
with the actual next-states of the environment. We call this property the state consistency property
(or simply consistency) of π, refer to Fig. 2. As it may be intractable to verify SPP for all possible


-----

interactions in continuous environments, we are interested in guaranteeing the state consistency for
the experiences stored in the replay buffer.
**Property 1. Let D be a replay buffer, CM be an IDM and π be a (SPP) policy. We say that π has**
the state consistency property with threshold d > 0 if it holds that


(st,stE+1) _∥st+1 −_ _zt∥2[2]_ _≤_ _d,_
_∈D_
_zt_ _π(st)_ h i
_∼_

_consistency distancefor deterministic π we have (refer Fig. 2). We will often assume that zt = π(st). Given (zt, st+1), we call distance d is known from context and omit ∥zt −_ _st+1∥2[2]_ [the][ state-]
’with threshold d > 0’.


control model stateconsistency

|RL Agent Control model Environment|RL Agent|Col3|Control model|
|---|---|---|---|
|||||


consistency
distance


Figure 2: Diagram presenting the idea

Figure 1: Diagram presenting

of state consistency property, ultimately

our SPP method

we want to achieve zt _st+1._
_≈_

3 ENSURING STATE CONSISTENCY PROPERTY BY CONSTRAINED
OPTIMIZATION

Our SPP algorithm is utilizing three parametrized models: IDM CMψ, policy model πθ, and Qfunction model(s) Qϕ. In our current implementation, all of the models are feed-forward neural
networks. One way of ensuring the state consistency property (Prop. 1) is to modify the policy
training loss, such that the expected values are maximized under fixed state consistency distance
penalty. However, such an approach has many disadvantages, e.g., choosing appropriate learning
temperature (λ) for the state consistency penalty is a very delicate issue, see the ablation study in
Appendix E.1. It is easy to notice that setting its value too low would result in π biased towards
nonphysical target-states. On the other hand, setting its value too high would make π overly conservative for off-policy states. Hence, we find a solution relying on constrained optimization more
appealing for the studied problem. Namely, the objective for policy training is to maximize the sum
of discounted rewards assuming a fixed threshold for the state consistency distance.
**Definition 3. Let π be a state planning policy (Def. 1), CM be a control model (Def. 2), D be the**
replay buffer with experience generated by executing an off-policy RL algorithm. In particular zt’s
are target states evaluated on-the-fly by π (susceptible to be changed during the course of algorithm),
and st+1’s are E next-states. Let d > 0 be a fixed hyperparameter.


We define the constrained objective for state planning policy π as follows

_T_

maxπ _τ_ _π,ECM_ _γ[i]r(si, ai)_ _,_
_ai=∼CM(si,zi)_ "Xi=0 #


(1a)


_st+1_ _zt_ 2 _d,_ (1b)
_∥_ _−_ _∥[2]_ _≤_
i


s.t. E
(st,st+1)∈D
_zt∼π(st)_


where d is a hyperparameter for determining the allowed threshold for the expected divergence of
predictions from actual next-states, equation 1a is an expectation over the policy trajectories generated by both of the state planning policy and IDM (trajectory is composed out of tuples (st, zt, at)).
The whole optimization process of equation 1a is being performed off-policy, see Sec. 4.

The constrained objective in equation 1 is being solved using the standard Lagrange multiplier
method. The max-min Lagrangian objective L(π, λ) for the constrained optimization problem takes

the form maxπ minλ 0 (π, λ) = E E _st+1_ _zt_ 2 _d_ . For more
_≥_ _L_ _τ_ _∼π[[][R][0][(][π][)]][ −]_ _[λ]_  _D_  _∥_ _−_ _∥[2]_ _zt∼π(st)_ _−_ 

details refer to App. C.1.


-----

4 ALGORITHM IMPLEMENTATION

We briefly present here details of the SPP Algorithm implementation, more detailed discussion
can be found in Appendix C. We implemented SPP-DDPG, SPP-TD3, and SPP-SAC as a modular
Python library within PyTorch framework (Paszke et al., 2019). All gradient optimization steps were
performed using the Adam optimizer by Kingma & Ba (2014). Many of the algorithmic choices
were motivated by the Spinning Up RL on-line resource (Achiam, 2018). We publish the modular
SPP-RL software package as open-source (web, 2021). For illustrative purposes, we present the
pseudo-code of the full SPP-DDPG algorithm in Algorithm 1. SPP-SAC and SPP-TD3 algorithms
are presented in Appendix C. We provide theoretical derivations concerning SPP-RL algorithms,
including convergence in the classical finite setting and policy gradient analogues to vanilla RL in
Appendix B. We emphasize that SPP algorithms are not using any extra samples, i.e. the samples
utilized for ICM training are added to the buffer and then reutilized for RL training, and if the buffer
is full new samples are not being added anymore. An important caveat of our policy implementation in SPP-DDPG, not present in the vanilla DDPG, is that the output of π is being normalized
in order to reflect the physical bounds. Contrary to vanilla DDPG where π outputs actions within
well-defined uniform bounds, in SPP a suitable normalization of target state π output is being computed online – depends on the past E observations. Implementation of π is a feed-forward neural
network (refer to Appendix D for details) with tangential outputs bounded within [−1, 1]. Hence, we
normalize the output of neural network π by utilizing the current mean and min/max values of the
past observations in replay buffer D. We recompute mean and min/max values after each episode
of the algorithm. Our base implementation of DDPG algorithm is parametrized by the usual hyper
**Algorithm 1: SPP-DDPG Algorithm**
**input : environment E; initial model parameters θ, ϕ, ψ; state planning distance threshold d; empty**
replay buffer D; the DDPG algorithm hyperparameters
**output: trained model parameters θ, ϕ, ψ; total return**
**repeat**

Sample random action a ∼U;
Store experience (st, at, zt = st+1, rt+1, st+1) in replay buffer D; (use next-state as the initial actor
actions)
**until random exploration is done;**
**repeat**

**if buffer D is not full then**

Compute actor prediction zt = π(st) + ε, where ε ∼N ;
Compute action at = CM(st, zt) and observe reward rt+1 and next state st+1;
Store experience (st, zt, at, st+1, rt+1) in D;
**end**
**if it’s time to update CM then**

Sample {bi = {((st, st+1), a)}}i[b]=1 _[b][ batches of samples from replay buffer][ D][;]_
SGD train CM using the batches and MSE loss;
**end**
**if it’s time to update actor and critic then**

**for update steps do**

Randomly sample B = {(st, zt, at, st+1, rt+1)} set of batches from D;
Compute ˜ak+1 = CM(st+1, π(st+1));
Compute targets y = rt+1 + γQ[π,]ϕtarg[CM] [(][s][t][+1][,][ ˜]ak+1) (using target parameters ϕtarg);

Update ϕ = ϕ − _|B|lϕ_ _[· ∇][ϕ]_ _B_ _y −_ _Q[π,]ϕ_ [CM](st, at) 2;

Update policy parameters (ascent w.r.t _θ of max-min Lagrangian obj.)_ _θ =_

P


_θ + lθ_  _|B|1_ _[· ∇][θ]_ _B_ _[Q]ϕ[π,][CM](st, at)_ _at=CM(st,πθ_ (st)) _[−]_ _|B|λ_ _[· ∇][θ]_ _B_ _[∥][s][t][+1][ −]_ _[π][θ][(][s][t][)][∥]2[2]_

Update (descent w.r.t.P _λ of max-min Lagrangian obj.)_ P
_λ = λ + lλ_ _|B|1_ _B_ _[∥][s][t][+1][ −]_ _[π][θ][(][s][t][)][∥]2[2]_ _[−]_ _[d]_ ;

Update actor & critic _ϕtarg = (1_ _τ_ )ϕtarg + τϕ; θtarg = (1 _τ_ )θtarg + τθ;

P _−_ _−_

**end**
**end**
**until convergence;**


-----

parameters including episode length, update batch size, Polyak averaging parameter (τ ), actor and
critic learning rate l, maximal episode length, number of test episodes, γ. To ensure that we perform
minimization equation 6 within the domain of positive λ values, we optimize the parameter of the
softplus function. All relevant hyper-parameters are provided in Appendix D.

We performed a thorough ablation study of the different features implemented in SPP-TD3 algorithm including the Lagrangian multipliers method (and different values of fixed λ), target state
normalization, state-state critic (Q(s, s[′])); we report results in Appendix E.1.

5 EXPERIMENTAL EVALUATION

To show the feasibility of our method, we performed experiments on a set of benchmarks using
continuous environments, most of them having large space dimensions. We performed all of the
reported experiments using the default vector state input. We show that SPP-RL implementations
compare favorably to their vanilla RL counterparts. As our SPP approach differs considerably from
the vanilla off-policy RL, we performed a thorough hyper-parameter sweep from scratch. We provide the hyper-parameter values from the actual SPP implementations in Appendix D. For the sake
of presentation we share videos with example test episodes rendered using the trained actors and
high-resolution benchmark plots. All of our experiments are reproducible, we share the sources,
training, evaluation, and trained models online (web, 2021). All of the reported experiments were
run using CPU only, and a single experiment was always run on a single CPU core, i.e., we have not
performed collecting experience in parallel. The experiments were performed on an example machine: AMD Ryzen Tr. 1920X, 64 Gb RAM, Ubuntu OS 18.04. Example average time of execution
of 10[6] steps stands at SPP-DDPG 5hrs 58[′], DDPG 3hrs, 7[′], (SPP-)SAC 19hrs 20[′], SAC 11hrs.

5.1 CLASSICAL MUJOCO

First, to measure the performance of SPP-RL we run benchmarks in the MuJoCo control tasks
(Todorov et al., 2012) from OpenAI gym (Brockman et al., 2016): Ant and Humanoid. The experiments were performed using solely the vector state input. We present the results in Fig. 3. We
compare the performance of SPP implementations with vanilla RL implementations in terms of average test episodes return. We emphasize that SPP algorithms do not increase the overall sample
efficiency of the RL procedure. The samples utilized for ICM training are added to the buffer and
then reutilized for RL training, and if the buffer is full new samples are not added anymore (refer
to Algorithm 1). The algorithm does not take advantage of a pre-trained ICM before the actual RL
training, rather optimizes π and trains ICM simultaneously. However, as reported in Fig. 3, quite surprisingly, SPP based RL algorithms perform on par (TD3 & SAC case) or significantly better (DDPG
case) than corresponding vanilla RL implementations in the MuJoCo environments. Analyzing the
case of (SPP-)DDPG Figs.3a, 3d reveals that SPP significantly outperforms the vanilla implementation, as the vanilla implementation does not converge on those envs (compare with Achiam (2018)).
Analyzing the case of (SPP)-TD3 reveals that for Ant env (Fig. 3b) SPP-TD3 performs on par with
vanilla TD3 and outperforms by far D3G – the most related method. For Humanoid env (Fig. 3e)
we see that SPP-TD3 outperformed vanilla TD3, the visible high variance of vanilla TD3 caused by
a single run, which did not converge, D3G do not converge in this env. There is a visible divergence
of the results we obtained for D3G in Ant from those reported by Edwards et al. (2020). We used
MuJoCo v.1.5 library version that is known to be compatible with OpenAI gym; it is known that
using v.2.0 results in zeroing out a part of observation space (in Ant & Humanoid) (gym, 2019),
which may alter the results.

5.2 SAFETY-GYM (LOCOMOTION TASKS)

In the next set of benchmarks, we use environments from the safety-gym suite by Ray et al. (2019).
Currently, we employed only Level 0 environments (which does not involve the cost function for violating the safety). We find Level 0 tasks from the safety-gym suite as the perfect ground to study the
performance of SPP-RL in robotic locomotion environments, the goal being to steer agents (robots)
to solve planar goal-reaching tasks. Moreover, we concentrate on difficulties arising from higher
dimensionality of the state (and actions) space rather than maximizing returns under safety constraints. The experiments were performed using solely the vector state input. We leave investigating


-----

5000

4000

3000

2000

1000

1000

2000

6000

5000

4000

3000

2000

1000


3000

2000

1000

0

1000

2000

|SPP-DDPG|Col2|
|---|---|
|vanilla DDPG||
|||
|||
|||
|||

|Col1|SPP-TD3|
|---|---|
||vanilla TD3 D3G|
|||
|||
|||
|||
|||
|||
|||


SPP-DDPG
vanilla DDPG


0.0 0.5 1.0 steps1.5 2.0 2.5 3.0

(a) Ant, (SPP-)DDPG


SPP-TD3vanilla TD3 SPP-SACvanilla SAC
D3G 6000

4000

average return 2000

0

0.0 0.5 1.0 steps1.5 2.0 2.5 3.0 0.0 0.5 1.0 steps1.5 2.0 2.5 3.0


(b) Ant, (SPP-)TD3


(c) Ant, (SPP-)SAC

SPP with Lagrangian optvanillaD3G 5000 SPP-SACvanilla SAC

4000

3000

average return

2000

1000

0.00 0.25 0.50 0.75 steps1.00 1.25 1.50 1.75 2.00 0 0.0 0.2 0.4 steps 0.6 0.8 1.0


(f) Humanoid, (SPP-)SAC


3000

2500

2000

1500

average return

1000

500

|SPP-DDPG|Col2|
|---|---|
|vanilla DDPG||
|||
|||
|||
|||
|||

|Col1|SPP with Lagrangian opt|
|---|---|
||vDa3nGilla|
|||
|||
|||
|||
|||


SPP-DDPG
vanilla DDPG

0.0 0.5 1.0 steps1.5 2.0 2.5 3.0

(d) Humanoid, (SPP-)DDPG


(e) Humanoid, (SPP-)TD3


Figure 3: Experimental comparison of SPP implementations with corresponding vanilla off-policy
RL on the two most complex MuJoCo benchmarks Ant (upper row) & Humanoid (bottom row).
Figures show test return computed every 5k frames averaged over 10 different seeds. The continuous
curve is the mean, whereas the faded color regions represent std. deviation. SPP RL algorithms in
blue, corresponding vanilla RL in orange, only case of TD3 Fig. 3b, 3e include results obtained
using D3G implementation in green. Refer to Appendix D for exact hyperparameters. Dimensions:
Ant: dim(S) = 111, dim(A) = 8; Humanoid: dim(S) = 376, dim(A) = 17.

the higher-level environments considering the cost function as a topic of future research. We chose
a subset of the most challenging Level 0 tasks, including Car-Push, Doggo-Goal, Doggo-Button environments. We also create a custom environment (termed Doggo-Columns) based on Doggo-Goal
with additional 10 fixed pillars placed in the arena, obscuring the paths toward the goal.

We evaluate our SPP-TD3 implementation against state-of-the-art off-policy algorithms like TD3
and SAC. We do not include on-policy algorithms like PPO (Schulman et al., 2017) or TRPO (Schulman et al., 2015), as their performance in the studied environments is inferior to off-policy, refer to
Ray et al. (2019) and the project web-page.

The results presented in Fig. 4 clearly show that SPP-TD3 is superior to vanilla off-policy algorithms
within the studied safety-gym environments. Also, there is a noticeable difference in the learned
behavior of the trained agents. The agents trained using the SPP-RL approach show smarter and
more efficient behavior; for instance, the trained using SPP doggo robot learned an efficient gait of
moving backward to mark a goal or press a button. For comparison, we publish videos of the trained
agents online (web, 2021).

We argue that the performance boost exhibited by SPP-RL algorithms over vanilla counterparts is
due to improved exploration in some sense. In Sec. 5.4 we show results from evaluating a TD3
shadow agent, i.e. vanilla TD3 agent utilizing for training some portion of experience from SPPTD3 replay buffer. In Appendix E.2 we investigate differences in distribution of states collected by
both of the methods.

5.3 HARDER EXPLORATORY TASK ANTPUSH


In order to demonstrate interpretability aspects of the SPP approach, we describe an experiment in
AntPush environment from Nachum et al. (2018). The experiments were performed using solely
the vector state input. The task is to control the ant such that it reaches the goal. The goal is
hidden within a chamber behind a block. Therefore, Ant needs to learn to walk around the block
and push it to the right first before eventually reaching the goal. Success is defined as finishing
the episode within a radius 5 from the goal. We benchmark SPP-TD3 against the state-of-the-art


-----

(d) Doggo-Columns

10 SPP-TD3TD3 vanillaSAC vanillaSPP-SAC 80 SPP-TD3TD3 vanillaSAC vanillaSPP-SACD3G 10080 SPP-TD3TD3 vanillaSAC vanillaSPP-SACD3G 605040 SPP-TD3TD3 vanillaSAC vanillaSPP-SAC

60

0 60 30

average return 10 average return 40 average return 40 average return 20

20 20 10

20 0 0 0

0.0 0.5 1.0 steps1.5 2.0 2.5 3.0 0.0 0.5 1.0 steps1.5 2.0 2.5 3.0 0.0 0.5 1.0 steps1.5 2.0 2.5 3.0 0.0 0.5 1.0 steps1.5 2.0 2.5 3.0

(custom), (SPP)TD3 &
SAC


(a) Car-Push,
(SPP)TD3 & SAC


(b) Doggo-Goal,
(SPP)TD3 & SAC


(c) Doggo-Button,
(SPP)TD3 & SAC


Figure 4: Experimental comparison of SPP-TD3 with corresponding vanilla off-policy RL on set of
safety-gym level 0 environments. Figures show test return computed every 5k frames averaged over
10 different seeds. The continuous curve is the mean, whereas the faded color regions std. deviation.
SPP-TD3 algorithm blue, vanilla TD3 orange, SPP-SAC red, vanilla SAC green, D3G purple. D3G
did not converge (return oscillated around zero, or it diverged in CarPush to a large negative score removed from the plot for clarity). Refer to Appendix D for exact hyperparameters that we used to
perform those experiments.

hierarchical RL HIRO method by Nachum et al. (2018). Specifically, we used the implementation
by Qin (2021). Instead of reporting the achieved success rate of a single training run like in Nachum
et al. (2018), which may be spurious if a lucky seed is chosen, we report the mean and std.dev.
of the AntPush success rate using 10 random seeded training runs. Our experiments revealed that
HIRO is highly susceptible to the random seed used. Only a single HIRO agent out of 10 trained
using random seeds in total achieved a positive success rate, comparing to 7 out of 10 SPP-TD3
agents successfully learned to solve the task. The performance reported in Fig. 5a shows SPP-TD3
is eventually superior to HIRO. Example two solution paths are marked on Fig. 5b (blue curves).
The right path is suboptimal as Ant blocks the entrance to the chamber where the goal is. The left
path is optimal, Ant traverses to the left to push the red block away and open the passage towards
the goal (green arrow).

Finally, Figs 5c, 5d show the obtained paths in the state space (blue dashed), and the policy target
states zt’s (orange solid), only the first two coordinates corresponding to the position of the Ant
body in x, y coordinates are illustrated. Observe that in the case of the suboptimal path in Fig. 5c,
the planned path diverts to the left from the actual path (blue dashed), which indicates that the agent
learned and attempted the correct behavior of pushing the red brick away and successfully open the
entrance to the goal. In this case, however, the block is not movable; it is stuck as it was pushed
forward before, hence as we see, the actual path in the state-space diverts in the middle. Figs 5c,5d
show that apparently, the policy target states path being more erratic than the actual path in the
state space. Erratic behavior can be mitigated by adjusting the hyperparameter d in equation 1b (the
smaller d, the closer the paths will be). Nonetheless, the policy target paths (orange) in Figs 5c,5d
could be potentially used to cluster agent behavior, qualitatively differentiating two example agents
executing (sub)optimal path. This information could then be used to pick appropriate agents for
deployment.

5.4 MORE EFFICIENT EXPLORATION IN SPP-RL

Our experimental evaluation using the safety-gym environments show that SPP-RL implementations
outperform by a great margin their vanilla off-policy RL counterparts (TD3 and SAC) in terms of
the average returns (See Fig.4). The performance boost is also visible in case of DDPG in MuJoCo
Ant (Figs. 3a, 3d).

In this section, we argue the performance boost of SPP-RL compared to the vanilla RL counterparts. Our intuition is that exploration performed in the target-state space rather than in the action
space may be more efficient in some cases. Exploration using SPP policies results in more viable
experience being collected in the replay buffer, leading to more efficient Actor & Critic training. It
is also possible that the constrained optimization induces some kind of curriculum. To confirm the
mentioned intuition, we evaluated the performance of a TD3 shadow agent i.e., a vanilla TD3 Actor&Critic trained using (partially) experience collected by the SPP-TD3 agent. Both of the agents


-----

wrong path


correct path


20.0

17.5

15.0

12.5

10.0

7.5

5.0

2.5

0.0



12 actual state-space path

10 path of planned target-states

8

6

4

2

0

2


actual state-space path
path of planned target-states

6 5 4 3 2 1 0


(b) Goal in
AntPush env (green
arrow), and two
possible paths (blue
curves)


(c) A suboptimal Ant path
example in the state space
(blue dashed) and target
states (orange solid)


(d) Example of an optimal
path in the state space (blue
dashed) and target states
(orange solid)


0.4 SPP-TD3HIRO

0.3

0.2

average return 0.1

0.0

0.1

0.2 0.0 0.5 1.0 steps1.5 2.0 2.5 3.0


(a) AntPush,
SPPTD3 & HIRO


Figure 5: Experiment in AntPush environment from Nachum et al. (2018). Fig. 5a shows success
rate for SPP-TD3 & HIRO computed every 5k frames averaged over 10 different seeds (10 independent training runs were used). The continuous curve is the mean, whereas the faded color regions
std. dev. Fig. 5b show two possible solution Ant paths. Figs 5c,5d show the paths in the state space
(blue dashed) and target states (orange solid), the coordinates describing the position of the Ant
body (x, y) are used.

were trained in parallel. The TD3 shadow agent updates were performed using samples drawn from
two of the replay buffers. The replay buffers of the SPP-TD3 and TD3 agent were used according
to a 50/50 ratio. The results are presented in Fig. 6. Such TD3 shadow agent outperforms vanilla
TD3, and eventually, its performance matches SPP-TD3 agent’s in all of the studied safety-gym
environments, excluding Doggo-Button.

Additionally, to address the question of how the distribution of experiences gathered by vanilla
RL/SPP-RL compares to each other, we performed an additional experiment in Appendix E.2. We
present plots of the discretized distributions of states encoded using a random encoder and crossentropy of two distributions w.r.t. the quantity of gathered experience.


(d) Doggo-Columns

10 SPP-TD3vanilla TD3TD3 shadow agent 80 SPP-TD3vanilla TD3TD3 shadow agent 100 SPP-TD3vanilla TD3TD3 shadow agent 6050 SPP-TD3vanilla TD3TD3 shadow agent

80 40

0 60 60 30

average return

average return 10 average return 40 average return 40 20

20 20 20 10

0

30 0.0 0.5 1.0 steps1.5 2.0 2.5 3.0 0 0.0 0.5 1.0 steps1.5 2.0 2.5 3.0 0 0.0 0.5 1.0 steps1.5 2.0 2.5 3.0 0.0 0.5 1.0 steps1.5 2.0 2.5 3.0

(custom), (SPP)TD3 &
SAC


(a) Car-Push,
(SPP)TD3 & SAC


(b) Doggo-Goal,
(SPP)TD3 & SAC


(c) Doggo-Button,
(SPP)TD3 & SAC


Figure 6: Experimental evaluation of TD3 shadow agent, i.e. vanilla TD3 trained utilizing experience collected by a SPP-TD3 agent, on a set of safety-gym level 0 environments. Presented metrics
are same as in Fig. 4. Shadow agent marked with the darkest color, vanilla TD3 and SPP-TD3 using
lighter colors respectively.

6 CONCLUSIONS

We presented the State Planning Policy Reinforcement Learning (SPP-RL) – an approach for reinforcement learning, where the policy is selecting target states. Experiments performed on continuous
benchmark environments often show the superior performance of SPP-RL compared to state-of-theart off-policy RL algorithms. There are various avenues for future work pertaining to this research.
One path is to include in our approach physically informed control models. Another, important path
of work is to implement a long-term policy planning method scheme and application in the safety
RL setting of SPP approach.


-----

7 REPRODUCIBILITY STATEMENT

To ensure reproducibility of our results we published a supplementary material web page available
[at https://sites.google.com/view/spprl, there a link to a public github repository. The repository](https://sites.google.com/view/spprl)
contains the sources, training & evaluation scripts, and trained models. The webpage also contains
the benchmark plots in higher resolution, and also include videos with agents trained using different
methods in the considered environments.

REFERENCES

Openai gym issue related to newer mujoco lib versions. [https://github.com/openai/](https://github.com/openai/gym/issues/1541)
[gym/issues/1541, 2019.](https://github.com/openai/gym/issues/1541)

Spp-rl supplementary material webpage. [https://sites.google.com/view/spprl,](https://sites.google.com/view/spprl)
2021.

Joshua Achiam. Spinning Up in Deep Reinforcement Learning. 2018.

Dimitri P. Bertsekas. Dynamic Programming and Optimal Control, volume I. Athena Scientific,
Belmont, MA, USA, 3rd edition, 2005.

V.S. Borkar. An actor-critic algorithm for constrained markov decision processes. Systems Con_trol Letters, 54(3):207–213, 2005._ ISSN 0167-6911. doi: https://doi.org/10.1016/j.sysconle.
2004.08.007. [URL https://www.sciencedirect.com/science/article/pii/](https://www.sciencedirect.com/science/article/pii/S0167691104001276)
[S0167691104001276.](https://www.sciencedirect.com/science/article/pii/S0167691104001276)

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
[Wojciech Zaremba. Openai gym. CoRR, abs/1606.01540, 2016. URL http://arxiv.org/](http://arxiv.org/abs/1606.01540)
[abs/1606.01540.](http://arxiv.org/abs/1606.01540)

Paul Christiano, Zain Shah, Igor Mordatch, Jonas Schneider, Trevor Blackwell, Joshua Tobin, Pieter
Abbeel, and Wojciech Zaremba. Transfer from simulation to real world through learning deep
inverse dynamics model, 2016.

Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. In
S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett
(eds.), Advances in Neural Information Processing Systems, volume 31. Curran Asso[ciates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/](https://proceedings.neurips.cc/paper/2018/file/3de568f8597b94bda53149c7d7f5958c-Paper.pdf)
[3de568f8597b94bda53149c7d7f5958c-Paper.pdf.](https://proceedings.neurips.cc/paper/2018/file/3de568f8597b94bda53149c7d7f5958c-Paper.pdf)

Ashley Edwards, Himanshu Sahni, Rosanne Liu, Jane Hung, Ankit Jain, Rui Wang, Adrien Ecoffet,
Thomas Miconi, Charles Isbell, and Jason Yosinski. Estimating q(s,s’) with deep deterministic dynamics gradients. In Hal Daum´e III and Aarti Singh (eds.), Proceedings of the 37th International
_Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research,_
[pp. 2825–2835. PMLR, 13–18 Jul 2020. URL http://proceedings.mlr.press/v119/](http://proceedings.mlr.press/v119/edwards20a.html)
[edwards20a.html.](http://proceedings.mlr.press/v119/edwards20a.html)

Ben Eysenbach, Russ R Salakhutdinov, and Sergey Levine. Search on the replay buffer: Bridging
planning and reinforcement learning. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´eBuc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. [URL https://proceedings.neurips.cc/](https://proceedings.neurips.cc/paper/2019/file/5c48ff18e0a47baaf81d8b8ea51eec92-Paper.pdf)
[paper/2019/file/5c48ff18e0a47baaf81d8b8ea51eec92-Paper.pdf.](https://proceedings.neurips.cc/paper/2019/file/5c48ff18e0a47baaf81d8b8ea51eec92-Paper.pdf)

Scott Fujimoto, Herke van Hoof, and David Meger. Addressing Function Approximation Error in
Actor-Critic Methods. arXiv e-prints, art. arXiv:1802.09477, February 2018.

Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard E. Turner, and Sergey Levine. Q[prop: Sample-efficient policy gradient with an off-policy critic. 2017. URL https://arxiv.](https://arxiv.org/abs/1611.02247)
[org/abs/1611.02247.](https://arxiv.org/abs/1611.02247)


-----

Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th
_International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning_
_Research, pp. 1352–1361, International Convention Centre, Sydney, Australia, 06–11 Aug 2017._
[PMLR. URL http://proceedings.mlr.press/v70/haarnoja17a.html.](http://proceedings.mlr.press/v70/haarnoja17a.html)

Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning,
volume 80 of Proceedings of Machine Learning Research, pp. 1861–1870, Stockholmsm¨assan,
[Stockholm Sweden, 10–15 Jul 2018a. PMLR. URL http://proceedings.mlr.press/](http://proceedings.mlr.press/v80/haarnoja18b.html)
[v80/haarnoja18b.html.](http://proceedings.mlr.press/v80/haarnoja18b.html)

Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018b.

Nicklas Hansen, Rishabh Jangir, Yu Sun, Guillem Aleny`a, Pieter Abbeel, Alexei A Efros, Lerrel
Pinto, and Xiaolong Wang. Self-supervised policy adaptation during deployment. In International
_[Conference on Learning Representations, 2021. URL https://openreview.net/forum?](https://openreview.net/forum?id=o_V-MjyyGV_)_
[id=o_V-MjyyGV_.](https://openreview.net/forum?id=o_V-MjyyGV_)

Mikael Henaff, William F. Whitney, and Yann LeCun. Model-based planning with discrete and
continuous actions, 2017.

Zhang-Wei Hong, Tsu-Jui Fu, Tzu-Yun Shann, and Chun-Yi Lee. Adversarial active exploration for inverse dynamics model learning. In Leslie Pack Kaelbling, Danica Kragic, and
Komei Sugiura (eds.), Proceedings of the Conference on Robot Learning, volume 100 of Pro_ceedings of Machine Learning Research, pp. 552–565. PMLR, 30 Oct–01 Nov 2020._ URL
[http://proceedings.mlr.press/v100/hong20a.html.](http://proceedings.mlr.press/v100/hong20a.html)

Tommi Jaakkola, Michael Jordan, and Satinder Singh. Convergence of stochastic iterative dynamic programming algorithms. In J. Cowan, G. Tesauro, and J. Alspector (eds.), _Advances in Neural Information Processing Systems,_ volume 6. MorganKaufmann, 1994. [URL https://proceedings.neurips.cc/paper/1993/file/](https://proceedings.neurips.cc/paper/1993/file/5807a685d1a9ab3b599035bc566ce2b9-Paper.pdf)
[5807a685d1a9ab3b599035bc566ce2b9-Paper.pdf.](https://proceedings.neurips.cc/paper/1993/file/5807a685d1a9ab3b599035bc566ce2b9-Paper.pdf)

Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Modelbased policy optimization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox,
and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Cur[ran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper/2019/](https://proceedings.neurips.cc/paper/2019/file/5faf461eff3099671ad63c6f3f094f7f-Paper.pdf)
[file/5faf461eff3099671ad63c6f3f094f7f-Paper.pdf.](https://proceedings.neurips.cc/paper/2019/file/5faf461eff3099671ad63c6f3f094f7f-Paper.pdf)

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014. URL
[http://arxiv.org/abs/1412.6980. cite arxiv:1412.6980Comment: Published as a con-](http://arxiv.org/abs/1412.6980)
ference paper at the 3rd International Conference for Learning Representations, San Diego, 2015.

Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Yoshua
Bengio and Yann LeCun (eds.), ICLR, 2016. [URL http://dblp.uni-trier.de/db/](http://dblp.uni-trier.de/db/conf/iclr/iclr2016.html#LillicrapHPHETS15)
[conf/iclr/iclr2016.html#LillicrapHPHETS15.](http://dblp.uni-trier.de/db/conf/iclr/iclr2016.html#LillicrapHPHETS15)

Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. _CoRR,_
[abs/1312.5602, 2013. URL http://arxiv.org/abs/1312.5602.](http://arxiv.org/abs/1312.5602)

Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd
_International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning_
_[Research, pp. 1928–1937, New York, New York, USA, 20–22 Jun 2016. PMLR. URL http:](http://proceedings.mlr.press/v48/mniha16.html)_
[//proceedings.mlr.press/v48/mniha16.html.](http://proceedings.mlr.press/v48/mniha16.html)


-----

Ofir Nachum, Shixiang (Shane) Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical reinforcement learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 31. Curran As[sociates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/](https://proceedings.neurips.cc/paper/2018/file/e6384711491713d29bc63fc5eeb5ba4f-Paper.pdf)
[e6384711491713d29bc63fc5eeb5ba4f-Paper.pdf.](https://proceedings.neurips.cc/paper/2018/file/e6384711491713d29bc63fc5eeb5ba4f-Paper.pdf)

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d’ Alch´e-Buc,
E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp.
[8024–8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/](http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf)
[9015-pytorch-an-imperative-style-high-performance-deep-learning-library.](http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf)
[pdf.](http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf)

Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, and Trevor Darrell. Curiosity-driven exploration
by self-supervised prediction. In ICML, 2017.

Friedrich Pfeiffer and Christoph Glocker. Multibody Dynamics with Unilateral Contacts. Wiley.

Ziang Qin. Repository implementing hiro algorithm. [https://github.com/](https://github.com/ziangqin-stu/rl_hiro)
[ziangqin-stu/rl_hiro, 2021.](https://github.com/ziangqin-stu/rl_hiro)

Alex Ray, Joshua Achiam, and Dario Amodei. Benchmarking safe exploration in deepreinforcement
learning, 2019.

Nikolay Savinov, Alexey Dosovitskiy, and Vladlen Koltun. Semi-parametric topological memory
[for navigation. In International Conference on Learning Representations, 2018. URL https:](https://openreview.net/forum?id=SygwwGbRW)
[//openreview.net/forum?id=SygwwGbRW.](https://openreview.net/forum?id=SygwwGbRW)

John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International
_Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research,_
[pp. 1889–1897, Lille, France, 07–09 Jul 2015. PMLR. URL http://proceedings.mlr.](http://proceedings.mlr.press/v37/schulman15.html)
[press/v37/schulman15.html.](http://proceedings.mlr.press/v37/schulman15.html)

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. ArXiv, abs/1707.06347, 2017.

David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In Proceedings of the 31st International Conference
_on International Conference on Machine Learning - Volume 32, ICML’14, pp. I–387–I–395._
JMLR.org, 2014.

Satinder P. Singh, Tommi S. Jaakkola, Michael L. Littman, and Csaba Szepesv´ari. Convergence results for single-step on-policy reinforcement-learning algorithms. Mach. Learn., 38(3):
[287–308, 2000. URL http://dblp.uni-trier.de/db/journals/ml/ml38.html#](http://dblp.uni-trier.de/db/journals/ml/ml38.html#SinghJLS00)
[SinghJLS00.](http://dblp.uni-trier.de/db/journals/ml/ml38.html#SinghJLS00)

Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based
control. In IROS, pp. 5026–5033. IEEE, 2012. ISBN 978-1-4673-1737-5. [URL http:](http://dblp.uni-trier.de/db/conf/iros/iros2012.html#TodorovET12)
[//dblp.uni-trier.de/db/conf/iros/iros2012.html#TodorovET12.](http://dblp.uni-trier.de/db/conf/iros/iros2012.html#TodorovET12)

Lung-Wen Tsai. Robot Analysis: the Mechanics of serial and parallel manipulators. John Wiley &
Sons, New York, 1999. ISBN 0-471-32593-7.

Alexander Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max Jaderberg, David
Silver, and Koray Kavukcuoglu. Feudal networks for hierarchical reinforcement learning. In
_Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17,_
pp. 3540–3549. JMLR.org, 2017.


-----

Manuel Watter, Jost Springenberg, Joschka Boedecker, and Martin Riedmiller. Embed to control: A locally linear latent dynamics model for control from raw images. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), _Advances_ _in_ _Neural_ _Information_ _Processing_ _Systems_ _28,_ pp. 2746–
2754. Curran Associates, Inc., 2015. [URL http://papers.nips.cc/paper/](http://papers.nips.cc/paper/5964-embed-to-control-a-locally-linear-latent-dynamics-model-for-control-from-raw-images.pdf)
[5964-embed-to-control-a-locally-linear-latent-dynamics-model-for-control-from-raw-](http://papers.nips.cc/paper/5964-embed-to-control-a-locally-linear-latent-dynamics-model-for-control-from-raw-images.pdf)
[pdf.](http://papers.nips.cc/paper/5964-embed-to-control-a-locally-linear-latent-dynamics-model-for-control-from-raw-images.pdf)

Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Mach. Learn., 8(3–4):229–256, May 1992. ISSN 0885-6125. doi: 10.1007/
[BF00992696. URL https://doi.org/10.1007/BF00992696.](https://doi.org/10.1007/BF00992696)

Ge Yang, Amy Zhang, Ari S Morcos, Joelle Pineau, Pieter Abbeel, and Roberto Calandra. Think,
act and learn on an episodic memory graph. In ICLR 2020 workshop: Beyond tabula rasa in RL
_(BeTR-RL), 2020._


-----

A APPENDIX

Videos visualizing the trained agents, and link to the git software repository with SPP-RL algorithms
[implementations are available online https://sites.google.com/view/spprl.](https://sites.google.com/view/spprl)

B THEORY OF STATE PLANNING POLICY RL ALGORITHMS

In this section we establish foundations of SPP-RL theory.

B.1 STATE-VALUE AND ACTION-VALUE FUNCTIONS

In order to employ in SPP RL agents based on the state-of-the-art RL actor-critic algorithms, we
start with defining the classical state-value and action-value functions in SPP setting.

Recall Rt = _i=t_ _[γ][(][i][−][t][)][r][(][s][i][, a][i][)][ is the sum of discounted future rewards. The classical][ Q][-function]_
is
_Q[π](st, at) = Eri_ _t,si>t_ _E, ai>t_ _π[Rt_ _st, at],_

[P][T] _≥_ _∼_ _∼_ _|_

however, in our setting we use state planning policies (Def. 1) and we do not directly sample an
being computedaction using the policy ai = CM ai ∼(siπ, z. Instead we sample a target statei). _zi ∼_ _π. Then the actual action ai is_

Hence, the Q-function in SPP setting can take the form

_Q[π,][CM](st, zt) = Eri≥t,si>t∼E, zi>t∼π, ai=CM(si,zi)[Rt|st, zt],_ (2)

i.e. the expected value of following the trajectory planned by the policy π in the state-space, and
using IDM CM.

As we use deterministic IDM map CM : (st, zt) _at, we further reformulate equation 2 into_
_7→_

_Q[π,][CM](st, at) = Eri≥t,si>t∼E, zi>t∼π, ai=CM(si,zi)[Rt|st, at]._ (3)

Analogously we define the state-value function V _[π,][CM]_


_V_ _[π,][CM](st) =_ E
_zt∼π_


_Q[π,][CM](st, CM(st, zt))_ _._ (4)



Observe that if the parameters vector ψ of CM is fixed, both of the formulations equation 2 and
equation 3 are equivalent as CM maps (st, zt) _at. However, in SPP CM is being trained, and its_
_7→_
parameters are being changed during the execution of the full SPP algorithm. Therefore, we find the
formulation equation 3 more suitable, as the established theory proves convergence of the Bellman
_iterates applied to equation 3 towards the optimal Q._

We evaluated the first variant of Q-function equation 2 in the ablation study presented in Appendix E.1 and it does not perform well. For the Q-function equation 3 we show theoretical convergence within the established framework in App. B.3.

B.2 POLICY GRADIENT THEOREM

In this section, for the sake of completeness we include the fundamental Policy Gradient Theorem
in the setting of SPP policies. We provide a formula for the gradient of the standard reward function
in the deterministic policy gradient setting (Silver et al., 2014), which is applied in DDPG off-policy
RL algorithm (Lillicrap et al., 2016), and also generalized in SAC off-policy algorithm (Haarnoja
et al., 2018a) for stochastic policies.

Observe that the standard reward function in the deterministic policy gradient setting (π and CM are
deterministic) is given by

_J(θ) =_ E _Q[π,][CM](s, CM(s, π(s)))_ _,_ (5)
_s∼d[β]_
 

where d[β](s) is the stationary discounted state distribution.


-----

Let the reward in the deterministic setting be given by equation 5. Assume CM is deterministic and
differentiable, rest of assumptions like in (Silver et al., 2014, Thm. 1). Then, the formula for the
standard policy gradient follows from (Silver et al., 2014, Thm. 1)

_θJ(θ) =_ E _θπθ(s)_ _zCM(s, z)_ _aQ(s, a)_ _z=πθ(s), a=CM(s,z)_ _._
_∇_ _s∼d[β]_ _∇_ _· ∇_ _· ∇_ _|_

h i

It is a straightforward extension of the formula given in (Silver et al., 2014, Thm. 1), applying one
additional step of the chain rule.

Observe that in our algorithm we compute the policy gradient of the Lagrangian objective L(π, λ)
in the deterministic setting, i.e.,


_s[′]_ _πθ(s)_ 2 _d_
_∥_ _−_ _∥[2]_ _−_
i


(πθ, λ) = J(θ) _λ_
_L_ _−_


(s,s[′])∈D


where D is an experience replay buffer (given), and J is the standard reward function in equation 5.

The gradient w.r.t. θ of the second term satisfies


_s[′]_ _πθ(s)_ 2 _d_
_∥_ _−_ _∥[2]_ _−_
i


_θπθ(s)_ _z_ _s[′]_ _z_ 2
_∇_ _· ∇_ _∥_ _−_ _∥[2]_ _z=π(s)_


_λ_ _θ_
_∇_


= λ E
(s,s[′])∈D


(s,s[′])∈D


by applying the Leibniz integral rule to exchange order of derivative and integration, and continuity
of πθ and its derivative. In particular, note that the next state s[′] is sampled from the buffer and thus
does not depend on the policy.

Therefore, it holds that


_∇θL(πθ, λ) =_ _s∼Ed[β]_


_θπθ(s)_ _zCM(s, z)_ _aQ(s, a)_ _z=πθ(s), a=CM(s,z)_
_∇_ _· ∇_ _· ∇_ _|_


_θπθ(s)_ _z_ _s[′]_ _z_ 2
_∇_ _· ∇_ _∥_ _−_ _∥[2]_ _z=π(s)_


(s,s[′])∈D

B.3 CONVERGENCE OF Q-LEARNING IN SPP SETTING


In this section we argue convergence of Q-learning in SPP-TD3 algorithm within a finite setting
using the existing framework of convergent stochastic processes based on the classic lemma by
Bertsekas (2005). Used in Singh et al. (2000) to prove convergence of SARSA algorithm, and
subsequently in Fujimoto et al. (2018) to prove convergence of TD3 algorithm Fujimoto et al. (2018).
We provide the lemma here for completeness.
**Lemma B.1. Consider a stochastic process (αt, ∆t, Ft), t ≥** 0, where αt, ∆t, Ft : X → R, which
satisfies the equations

∆t+1(x) = (1 _αt(x))∆t(x) + αt(x)Ft(x),_ _x_ _X, t = 0, 1, 2, ..._
_−_ _∈_

Let Pt be a sequence of increasing σ-fields such that α0, ∆0 are P0-measurable and αt, ∆t and
_Ft_ 1 are Pt-measurable, t = 1, 2, . . . . Assume that the following hold
_−_

1. the set of possible states X is finite.

2. 0 ≤ _αt(x) ≤_ 1, _t_ _[α][t][(][x][) =][ ∞][,][ P]t_ _[α]t[2][(][x][)][ <][ ∞]_ [w.p.][ 1][.]

3. ∥E{Ft(·)|Pt∥W ≤[P]κ ∥∆t∥W + ct, where κ ∈ [0, 1) and ct converges to zero w.p. 1.

4. Var{Ft(x)|Pt} ≤ _K(1 + ∥∆t∥W )[2], where K is some constant._


Then, ∆t converges to zero with probability one.

The proof can be found e.g. in Jaakkola et al. (1994); Singh et al. (2000).

Based on Lemma B.1 Fujimoto et al. (2018) show the convergence of Clipped Double Q-learning to
the optimal value function Q[∗], as defined by the Bellman optimality equation w.p. 1.


-----

**Theorem B.2 (Thm. 1 in Fujimoto et al. (2018)). Given the following conditions:**

1. Each state action pair is sampled an infinite number of times.

2. The MDP is finite.

3. γ ∈ [0, 1).

4. Q values are stored in a lookup table.

5. Both Q[A] and Q[B] receive an infinite number of updates.

6. The learning rates satisfy αt(s, a) ∈ [0, 1], _t_ _[α][t][(][s, a][) =][ ∞][,][ P]t_ [(][α][t][(][s, a][))][2][ <][ ∞] [with]

probability 1 and αt(s, a) = 0, (s, a) = (st, at).
_∀_ _̸_

7. Var[r(s, a)] < ∞, ∀s, a. [P]


Then Clipped Double Q-learning will converge to the optimal value function Q[∗], as defined by the
Bellman optimality equation with probability 1.

We can apply Thm. B.2 to show convergence of Q learning in SPP-TD3 algorithm. First, observe
that SPP-TD3 utilizes the same Q-functions, hence, the assumptions 2, . . ., 7 can be transferred to
SPP-TD3 setting. However, the policy part is different.

Let us focus our attention to the assumption 1 of Thm. B.2. Observe that in the classical setting the
assumption 1 is easily realizable by employing a policy performing either fully random exploration
or ε-greedy exploration. In the studied SPP setting the situation is much more subtle. We do not have
a-priori guarantees that the such an assumption is satisfied. Observe that even a randomly exploring
policy may not realize such assumption. It will not be satisfied if for example the IDM is degenerate
and maps each pair of states to the same action.

However, if we show

-  for each pair of reachable states there is a unique action that results in the transition of the
previous state to the next state,

-  the SPP policy outputs only realizable target states.

then assumption 1 of Thm. B.2 can also be satisfied in SPP-RL setting by employing a fully random
or ε-greedy SPP policy. The first condition above may not be true for general environments, as we
can easily imagine an environment, where a particular state may be reached from another state by
applying different actions. However, in the case of rigid body environments we can demonstrate that
the desired property is true. We formalize this in the following lemma.
**Lemma B.3. Let E be a (deterministic) environment based on the rigid body dynamics model, i.e.**
the state transitions are computed using a time-wise discretization of a continuous and differentiable
dynamics model. Let s, s[′] _∈S be a pair of reachable states in environment E._

If there exists a, a[′] _∈A, such that E(s, a) = E(s, a[′]) = s[′], i.e. actions a and a[′]_ result in the same
state transition, then it holds that a = a[′].

_Proof. It is known that the dynamics of rigid bodies is governed by the laws of kinematics and by the_
application of Newton’s second law (Tsai, 1999). Namely, the equations governing the dynamics
of rigid bodies in (time-dependent) generalized position-velocity coordinates (q, v), q : R → R[n],
_v_ : R → R[n] take the following form

_M_ (q) _[dv]_

_dt_ [=][ k][(][q, v][) +][ ρ][(][q, v][) +][ τ,]

where M : R[n] _→_ R[n][×][n] is the positive definite, symmetric inertia matrix, k : R[n] _× R[n]_ _→_ R[n] is
the vector of external and contact forces, ρ : : R[n] _× R[n]_ _→_ R[n] is the sum of friction components,
_τ : R →_ R[n] is the applied external force (Pfeiffer & Glocker; Todorov et al., 2012).

Hence, there is a one-to-one mapping of state tuple (q, v, _[dv]dt_ _[, ρ][)][ and the external force][ τ][ (torque).]_

The one-to-one mapping still holds for realizable environment E based on simulation, where infinitesimal acceleration dv is approximated by the finite difference vt+1 _vt, dt is a fixed time-step,_
_−_


-----

_ρ is calculated from the Cartesian body position and the fixed domain constraints, external force τ_
is calculated from input action a by a linear transformation.

We conclude with the following corollary demonstrating the convergence of Q learning in SPP-TD3
algorithm within the finite setting.

**Corollary B.3.1. Let**

-  E be a (deterministic) environment based on the rigid body dynamics model, i.e. the state
transitions are being generated by a time-wise discretization of a continuous and differentiable dynamics model,

-  the experiences utilized for Q updates be gathered using a random/ε-greedy SPP (deterministic) policy π,

-  the target states form π be realizable, i.e. if z = π(s) for s ∈S, then it holds that z ∈S,

-  IDM CM be implemented as a lookup table and receive infinite updates.

Moreover, assume 2, . . ., 7 of Thm. B.2.

Then, clipped Double Q-learning in SPP-TD3 will converge to the optimal value function Q[∗], as
defined by the Bellman optimality equation, with probability 1.

_Proof. From Lemma B.3 it follows that for any pair of reachable states s, s[′]_ of environment E there
exists a unique action a resulting in transition E(s, a) = s[′]. Hence, as CM receives infinite updates
and is implemented as a lookup table it will learn to map any of the policy target states (realizable
states) to the unique action. It follows that each state action will be sampled infinite number of times
in the limit. As the rest of Thm. B.2 assumptions are assumed we obtain the claim.

C SPP-RL ALGORITHM DETAILS

C.1 LAGRANGIAN OPTIMIZATION

To solve the constrained optimization problem we use the idea of Lagrange multipliers going back
to the classical calculus. Lagrange multipliers were already applied in the RL context in Ray et al.
(2019), where authors developed on-policy RL algorithms for safety RL. This method uses an adaptive penalty coefficient to enforce constraints. We find this method plausible, as it translates a constrained objective into solving a classical max-min problem that can be incorporated within any RL
algorithm. The max-min Lagrangian objective L(π, λ) for the constrained optimization problem
equation 1 takes the form


_st+1_ _zt_ 2
_∥_ _−_ _∥[2]_ _zt∼π(st)_


max min
_π_ _λ_ 0 _τ_ _π[[][R][0][(][π][)]][ −]_ _[λ]_
_≥_ _[L][(][π, λ][) =][ E]∼_


_−_ _d_ _._ (6)



Intuitively, when the constraint equation 1b is satisfied, then the min w.r.t. λ is attained for λ = 0,
and causes λ to decrease. On the other hand, if the constraint is not satisfied, then the min is attained
for λ = ∞, and hence causing λ to increase, the constraint satisfaction receives more weight in the
overall max-min objective, eventually forcing the constraint to be fulfilled.

We solve the max-min Lagrangian objective for SPP policy using simultaneous gradient ascent
w.r.t. the policy parameters θ, descent w.r.t. the Lagrange multiplier λ, updates. Such an approach
can be viewed as an instance of a primal-dual algorithm for constrained MDP. It is known in the
literature that time-steps of Actor, Critic & dual variable gradient updates should be adjusted separately to guarantee convergence, refer to Borkar (2005) and references therein. The SPP algorithms
use three learning rates: actor (lθ), critic (lϕ), and dual variable (lλ), which were adjusted using a
hyper-parameter optimization. The actor and critic learning rates were equal in all of the studied
algorithms, whereas the dual variable was fixed in all algorithms to lλ = 0.0001.


-----

C.2 SPP-SAC ALGORITHM

The original DDPG algorithm, which is the base of SPP-DDPG presented in Algorithm 1 aims
at training a deterministic policy, as contrary to for example SAC algorithm by Haarnoja et al.
(2018a;b) that trains a stochastic policy. Its main principle is rooted in the very successful deep
_Q-learning method, originally applied for solving the Atari benchmark problem (Mnih et al., 2013)._
However, DDPG is suitable for continuous action spaces like in the MuJoCo tasks. SAC is an
algorithm that combines ideas of DDPG and soft Q-learning Haarnoja et al. (2017) utilizing the
advantages of both approaches, SPP-SAC is presented in Algorithm 2.

**Algorithm 2: SPP-SAC Algorithm**
**repeat**

Sample random action a ∼U;
Store experience (st, zt, at, st+1, rt) in ; (use next-state as the initial actor actions)
_D_
**until random exploration is done;**
**repeat**

**if buffer D is not full then**

Sample actor prediction zt _π(st);_
Compute action at = CM( ∼st, zt);
Observe reward rt and next state st+1;
Store experience (st, zt, at, st+1, rt) in ;
_D_
If st+1 is terminal, reset environment state;
**end**
**if it’s time to update CM then**

Randomly sample {bi = {(st, st+1), at}}i[n]=1 _[n][ batches of samples from replay buffer]_
_D;_
SGD train CM using the batches and MSE loss;
**end**
**if it’s time to update actor and critic then**

**for update steps do**

Randomly sample = (st, zt, at, st+1, rt) set of batches from ;
_B_ _{_ _}_ _D_
Compute actions ˆat+1 = CM(st+1, ˆzt+1), using samples ˆzt+1 ∼ _π(zt+1|st+1);_
Compute targets y = rt + γ mini=1,2 _[Q]ϕ[π,]targ,i[CM]_ [(][s][t][+1][,][ ˆ]at+1) ;

2

Compute ϕ = ϕ _lϕ_ _ϕ_ [1] _y_ _Q[π,]ϕ_ [CM](st, at) ;
_−_ _· ∇_ _|B|_ _B_ _−_

Compute θ = θ + lθ _θ_ [1] P  _ϕ_ (st, ˜aθ,t(st)) _α log πθ(˜zθ,t(st)_ _st),_
_· ∇_ _|B|_ _B[(min]i=1,2_ _[Q][π,][CM]_ _−_ _|_

where ˜zθ,t(st) _π(zt_ _st) is differentiable wrtP_ _θ;_
_∼_ _|_

UpdateCompute λ θ = = λ θ + − lλ[l]|B|[θ][λ] 1[· ∇][θ] PB _[∥][s][t][+1][ −]zθ,tz[˜](θ,tst()st2)∥2[2][;]_ ;

_|B|_ _B_ _[∥][s][t][+1][ −]_ [˜] _∥[2]_ _[−]_ _[d]_

Update target networks _ϕtarg,i = (1_ _τ_ )ϕtarg,i + τϕ _i, for i = 1,2;_

P _−_

**end**
**end**
**until convergence;**


C.3 SPP-TD3 ALGORITHM

Twin Delayed Policy Gradient (TD3) (Fujimoto et al., 2018) is an improved DDPG algorithm using
several tricks, which can be summarized as Clipped Double-Q Learning, Delayed Policy Updates,
Target Policy Smoothing. SPP-TD3 algorithm is presented in Algorithm 3.


-----

**Algorithm 3: SPP-TD3 Algorithm**
**input : environment E; initial model parameters θ, ϕ1, ϕ2, ψ; state planning distance threshold d; empty**
replay buffer D; TD3 algorithm hyperparameters
**output: trained model parameters θ, ϕ1, ϕ2, ψ; total return**
**repeat**

Sample random action a ∼U;
Store experience (st, at, zt, rt+1, st+1) in replay buffer D; (use next-state as the initial actor actions)
**until random exploration is done;**
**repeat**

**if buffer D is not full then**

Compute actor prediction zt = clip (π(st) + ε, zlow, zhigh), where ε ∼N ;
Compute action at = CM(st, zt) and observe reward rt+1 and next state st+1;
Store experience (st, zt, at, st+1, rt+1) in D;
**end**
**if it’s time to update CM then**

Sample {bi = {((st, st+1), a)}}i[b]=1 _[b][ batches of samples from replay buffer][ D][;]_
SGD train CM using the batches and MSE loss;
**end**
**if it’s time to update actor and critic then**

**for update steps do**

Randomly sample B = {(st, zt, at, st+1, rt+1)} set of batches from D;
Compute target states _zt+1(st+1) = clip_ _πθtarg_ (st+1) + clip(ε, _c, c), zlow, zhigh_, where
_−_
Compute target actionsε ∼N (0, σ); _at+1(st+1) = CM (st+1,_ _zt+1(st+1));_ 
e
Compute targets y(r, st+1) = r + γ mini=1,2 Q[π,]ϕtarg,i[CM] [(][s][t][+1][,][ e]at+1(st+1) (using target
parameters ϕtarg,1, ϕtarg, e 2); e

Update ϕ1 = ϕ1 − _|B|llϕϕ_ _[· ∇][ϕ][1]_ _B_ y(r, st+1) − _Q[π,]ϕ1[CM](st, at)22;_

Update ϕ2 = ϕ2 − _|B|_ _[· ∇][ϕ][2]_ PB _y(r, st+1) −_ _Q[π,]ϕ2[CM](st, at)_ ;

**if current step mod policy delay = 0P**  **then** 

Update θ = θ +

Updatelθ  _|B|1 λ =[· ∇] λ[θ]P + lBλ[Q]ϕ[π,]|B|1[CM](sBt, a[∥][s]t[t])[+1]at[ −]=CM[π][θ](s[(]t[s],π[t][)]θ[∥](s2[2]t[−]))_ _[−][d]_ _|B|;λ_ _[· ∇][θ]_ PB _[∥][s][t][+1][ −]_ _[π][θ][(][s][t][)][∥]2[2];_

Update actor & critics _ϕtarg,i = (1_ _τ_ )ϕtarg,i + τϕi; θtarg = (1 _τ_ )θtarg + τθ;

P _−_ _−_

**end**
**end**
**end**
**until convergence;**


D HYPERPARAMETERS USED IN EXPERIMENTS

We present hyper-parameters used in our experiments in Table 2 for the SPP-DDPG, in Table 6 for
the SPP-SAC. SPP-TD3 hyper-parameters were dependent on the set of benchmarks, and hyperparameters specific for MuJoCo environments are in Tab. 3, for SafetyGym in Tab. 4, and for AntPush in Tab. 5.

In SPP-DDPG implementation we used the following architectures:

-  Actor: dim(S) −→ 256 ReLU −→ 256 ReLU −→ dim(A) tanh;

-  Critic: dim(S) + dim(A) −→ 256 ReLU −→ 256 ReLU → 1;

-  CM: 2 · dim(S) −→ 100 tanh −→ 50 tanh −→ dim(A) tanh;


In SPP-TD3 implementation we used the following architectures:

-  Actor: dim(S) −→ 256 ReLU −→ 256 ReLU −→ dim(A) tanh;

-  Critic: dim(S) + dim(A) −→ 256 ReLU −→ 256 ReLU → 1;

-  CM: 2 · dim(S) −→ 100 tanh −→ 50 tanh −→ dim(A) tanh;


-----

parameters specific for SPP-DDPG

|Hyperparameter|Value|
|---|---|
|γ τ actor/critic learning rate l = l ϕ θ λ learning rate l λ episode length batch size test episodes update freq. & grad.steps. exploration noise param. replay buffer size|0.99 0.005 0.0005 0.0001 5000 100 10 50 0.05 same as number of env. interacts|



CM hyper-parameters

|d state consistency|0.2|
|---|---|


|init. rand. samples learning rate batch size update frequency update epochs update batches|20000 0.005 128 500 steps 1 100|
|---|---|



Table 2: SPP-DDPG Algorithm hyperparameters, DDPG used the same hyperparameters, except
exploration noise (set to 0.1).


parameters specific for SPP-TD3

|Hyperparameter|Value|
|---|---|
|γ τ actor/critic learning rate l = l ϕ θ λ learning rate l λ episode length batch size test episodes update freq. & grad.steps. exploration noise param. policy noise noise clip. replay buffer size|0.99 0.005 0.0001 0.0001 5000 100 10 50 0.2 0.2 0.5 same as number of env. interacts|



CM hyper-parameters

|d state consistency|0.2|
|---|---|


|init. rand. samples learning rate batch size update frequency update epochs update batches|25000 0.001 128 500 steps 1 200|
|---|---|



Table 3: SPP-TD3 Algorithm hyperparameters for MuJoCo environments, TD3 used the same (common) hyperparameters.

In SPP-SAC implementation we used the following architectures:

-  Actor: dim(S) −→ 256 ReLU −→ 256 ReLU −→ dim(A) tanh;

-  Critic: dim(S) + dim(A) −→ 256 ReLU −→ 256 ReLU → 1;

-  CM: 2 · dim(S) −→ 64 tanh −→ 32 tanh −→ dim(A) tanh;


-----

parameters specific for SPP-TD3

|Hyperparameter|Value|
|---|---|
|γ τ actor/critic learning rate l = l ϕ θ λ learning rate l λ episode length batch size test episodes update freq. & grad.steps. exploration noise param. policy noise noise clip. replay buffer size|0.99 0.005 0.0002 0.0001 5000 100 10 50 0.1 0.2 0.5 same as number of env. interacts|



CM hyper-parameters

|d state consistency|0.2|
|---|---|


|init. rand. samples learning rate batch size update frequency update epochs update batches|400000 0.001 128 500 steps 1 200|
|---|---|



Table 4: SPP-TD3 Algorithm hyperparameters for SafetyGym environments. Observe that for this
environment specifically we used larger number of CM pretrain samples


parameters specific for SPP-TD3

|Hyperparameter|Value|
|---|---|
|γ τ actor/critic learning rate l = l ϕ θ λ learning rate l λ episode length batch size test episodes update freq. & grad.steps. exploration noise param. policy noise noise clip. replay buffer size|0.99 0.005 0.0001 0.0001 5000 100 10 50 1 0.2 0.5 250000|



CM hyper-parameters

|d state consistency|0.2|
|---|---|


|init. rand. samples learning rate batch size update frequency update epochs update batches|100000 0.001 128 500 steps 1 200|
|---|---|



Table 5: SPP-TD3 Algorithm hyperparameters for AntPush environment. Observe that for this environment specifically we used fixed buffer size which is significantly smaller than the total number
of env interacts, and a much larger exploration noise param equal to 1.


-----

parameters specific for SPP-SAC

|Hyperparameter|Value|
|---|---|
|γ τ actor/critic learning rate l = l ϕ θ λ learning rate l λ batch size test episodes update freq. & grad.steps. α α learning rate replay buffer size|0.99 0.005 0.001 0.0001 100 10 each 1000 frames 50 0.2 0.001 1e6|



CM hyper-parameters

|d state consistency|0.2|
|---|---|


|init. rand. samples learning rate batch size update frequency update batches|10000 0.001 100 1000 steps 100|
|---|---|



Table 6: SPP-SAC Algorithm hyperparameters

E ADDITIONAL EXPERIMENTS

E.1 ABLATION STUDY

We performed a thorough ablation study of the features that we implemented in the SPP-TD3 algorithm presented in Sec. C. Ablation study was performed in Ant (Fig. 7a) and Doggo-Goal (Fig. 7b).
We explain the meaning of particular ablations visible in Fig. 7.

-  SPP-TD3 less init. sampl. means that less randomly generated experience the algorithm
added to the replay buffer at the beginning of executing Alg. 3 (the first repeat until block).
In some cases, like SPP-TD3 for Doggo-Goal, see Table 4 we choose to include a large
number of random samples in the buffer (400000). However, this has no considerable
effect on performance. In fact, the SPP-RL agent utilizing the number of random samples
the same as vanilla RL performs better initially. Only in the long term, slightly losing to
the agent utilizing more experience. This shows that the performance boost of SPP-RL still
persists when feeded the same number of initial random samples as in the corresponding
vanilla RL method.

-  SPP-TD3 w/o state consistency the state consistency defined in Property 1 is not introduced
in the algorithm; therefore, there is no incentive on the policy to choose as targets realizable states. Therefore it demonstrates that the SPP approach is more than just a policy
reparametrization.

-  SPP-TD3 Q(s, s[′]) critic (state-state critic) replaced the traditional target Q function computation using state-action pairs with state-next state pairs. Hence the step of computing
target actions is omitted. As a result, the algorithm did not converge in several cases and
exhibited a significant variation of the obtained returns between different runs. This is consistent with the observed performance of D3G algorithm in Ant (Fig. 3b). We do not show
this ablation in Doggo-Goal, as the algorithm did not converge in this environment.



-  SPP-TD3 w/o state normalization π is not being normalized using the current mean and
min/max values of the past observations in replay buffer D.

-  SPP-TD3 const. λ = 1 the Lagrangian multipliers method is not used. The algorithm
uses the constant value λ = 1 when performing policy updates. We chose this particular
_λ value, as it tends to work well in Ant, whereas in Doggo-Goal results in lack of conver-_
gence; this shows how delicate the matter of choosing appropriate λ (when fixed) per given
environment is.


-----

-  SPP-TD3 const. λ = 0.1 or λ = 0.5 the Lagrangian multipliers method is not used, the
algorithm uses the constant value λ = 0.1 or = 0.5 respectively when performing policy
updates. The particular λ values resulted in much better performance in Doggo-Goal than
_λ = 1 (which works in Ant). Observe that λ = 0.1 results in even better performance than_
the base SPP-TD3 implementation utilizing Lagrangian multipliers. However, employing
the Lagrange multipliers provides a natural way of solving the constrained objective optimization, and the hyper-parameter d is adjusted once for all the environments.


5000

4000

3000

2000

1000

0

1000

2000


100

80


|SPP-TD3 SPP-TD3 less init. sampl. SPP-TD3 const. = 1 SPP-TD3 const. = 0.5 SPP-TD3 const. = 0.1|Col2|
|---|---|
|||
|||
|||
|||
|||


0.0 0.5 1.0 1.5 2.0 2.5 3.0

steps


60

40


20

|SPP-TD3 (base) SPP-TD3 less init. sampl. SPP-TD3 w/o state consistency|Col2|
|---|---|
|SPP-TD3 Q(s,s') SPP-TD3 w/o targ. state normalize||
|SPP-TD3 const. = 1||
|||
|||
|||
|||
|||


SPP-TD3 (base)
SPP-TD3 less init. sampl.
SPP-TD3 w/o state consistency
SPP-TD3 Q(s,s')
SPP-TD3 w/o targ. state normalize
SPP-TD3 const. = 1


0.0 0.5 1.0 1.5 2.0 2.5 3.0

steps

(a) Ablation study using Ant


(b) Ablation study using Doggo-Goal


Figure 7: Ablation study of various SPP-TD3 algorithm features, performed using Ant and DoggoGoal environments. For the exact features of the algorithm that were tested refer to the discussion in
Appendix E.1. Figures show test return computed every 5k frames averaged over 5 different seeds.
The continuous curve is the mean, whereas the faded color regions std. deviation.

E.2 SPP-RL VS VANILLA RL REPLAY BUFFER


As argued in Sec. 5.4 the performance boost visible in SPP-RL vs. vanilla RL approaches is presumably due to more efficient exploration performed by SPP-RL algorithms vs. vanilla RL approaches.
One empirical argument given concerns the performance of the so-called TD3 shadow agent, i.e.,
vanilla TD3 agent implementation utilizing (partially) the experience gathered by SPP-TD3 agent
to perform its Actor&Critic updates. Here we provide empirical evidence that the distribution of
observations in replay buffers gathered by SPP-RL and vanilla RL implementations differs considerably. In Fig. 8 we present an empirical study of distributions of states gathered in SPP-SAC and
vanilla SAC replay buffers for the Doggo-Goal task. The state-space in this task has 72 dimensions.
Hence to make it amenable to visual investigation and entropy computation, we encode the state
vectors using a random encoder. The encoder architecture that we used for this task is just initialized (default PyTorch ini) and not trained architecture: 72 → 20 tanh → 10 tanh → 2. Fig. 8
show plots of discretized state distributions gathered by example run of vanilla SAC and SPP-SAC
respectively using the Doggo-Goal environment and encoded using the random encoder. We also
compute cross-entropy to quantify the difference between those distributions as the training of both
algorithms progresses and the replay buffer is being filled up. Observe that the cross-entropy is
clearly increasing as the replay buffer is being filled up, which suggests that vanilla RL and SPP-RL
algorithms gather different observation distributions in the replay buffer.


-----

9.6

9.4

9.2

9.0

8.8

8.6

8.4


0.0 0.5 1.0 1.5 2.0 2.5 3.0

samples 1e6

(c) Cross-entropy of the two
encoded states distributions (vanilla
SAC vs SPP-SAC)


(a) Encoded state density from
vanilla SAC buffer

(d) Encoded state density from
vanilla TD3 buffer


(b) Encoded state density from
SPP-SAC buffer

(e) Encoded state density from
SPP-TD3 buffer


9.3

9.2

9.1

9.0

8.9

8.8

8.7

8.6


0.0 0.5 1.0 1.5 2.0 2.5 3.0

samples 1e6

(f) Cross-entropy of the two
encoded states distributions (vanilla
TD3 vs SPP-TD3)


Figure 8: Visualizations of states distribution (density histogram plot) in the replay buffer at the end
of training taken from single vanilla SAC, TD3, SPP-SAC, and SPP-TD3 runs. States are encoded in
2D using the random encoder. Figs. 8c,8f is the cross-entropy of the two discrete state distributions
(vanilla vs SPP) with respect to the quantity of experience in the replay buffer (excluding the initial
random initialization).


-----

