# LEARN LOCALLY, CORRECT GLOBALLY: A DISTRIBUTED ALGORITHM FOR TRAINING GRAPH NEURAL NETWORKS

**Morteza Ramezani[∗], Weilin Cong[∗], Mehrdad Mahdavi**
**Mahmut T. Kandemir, Anand Sivasubramaniam**
Pennsylvania State University, University Park, PA 16802, USA
morteza@cse.psu.edu, wxc272@psu.edu, mzm616@psu.edu
anand@cse.psu.edu, kandemir@cse.psu.edu


ABSTRACT

Despite the recent success of Graph Neural Networks (GNNs), training GNNs on
large graphs remains challenging. The limited resource capacities of the existing
servers, the dependency between nodes in a graph, and the privacy concern due
to the centralized storage and model learning have spurred the need to design an
effective distributed algorithm for GNN training. However, existing distributed
GNN training methods impose either excessive communication costs or large
memory overheads that hinders their scalability. To overcome these issues, we
propose a communication-efficient distributed GNN training technique named
_Learn Locally, Correct Globally (LLCG). To reduce the communication and mem-_
ory overhead, each local machine in LLCG first trains a GNN on its local data by
ignoring the dependency between nodes among different machines, then sends the
locally trained model to the server for periodic model averaging. However, ignoring
node dependency could result in significant performance degradation. To solve the
performance degradation, we propose to apply Global Server Corrections on the
server to refine the locally learned models. We rigorously analyze the convergence
of distributed methods with periodic model averaging for training GNNs and show
that naively applying periodic model averaging but ignoring the dependency between nodes will suffer from an irreducible residual error. However, this residual
error can be eliminated by utilizing the proposed global corrections to entail fast
convergence rate. Extensive experiments on real-world datasets show that LLCG
can significantly improve the efficiency without hurting the performance.


1 INTRODUCTION

In recent years, Graph Neural Networks (GNNs) have achieved impressive results across numerous
graph-based applications, including social networks (Hamilton et al., 2017; Deng et al., 2019),

recommendation systems (Ying et al., 2018; Wang et al., 2018),
and drug discovery (Fout et al., 2017; Do et al., 2019; Ghorbani 4 SpeedupMemory 10
et al., 2022; Faez et al., 2021). Despite their recent success, effective
training of GNNs on large-scale real-world graphs, such as Face- Speedup2
book social network (Boldi & Vigna, 2004), remains challenging. 0
Although several attempts have been made to scale GNN training by No. Machines
sampling techniques (Hamilton et al., 2017; Zou et al., 2019; Zeng

Speedup 10

4 Memory

5

Speedup2

Memory (GB)

0

1 2 4

No. Machines

Figure 1: Comparison of the

et al., 2020; Chiang et al., 2019; Chen et al., 2018; Zhang et al.,

speedup and the memory con
2021; Ramezani et al., 2020), they are still inefficient for training

sumption of distributed multi
on extremely large graphs, due to the unique structure of GNNs

machine training and central
and the limited memory capacity/bandwidth of current servers. One

ized single machine training

potential solution to tackle these limitations is employing distributed
training with data parallelism, which have become almost a de facto on the Reddit dataset.
standard for fast and accurate training for natural language processing (Lin et al., 2021; Hard et al.,


_∗Equal Contribution._


-----

2018) and computer vision (Bonawitz et al., 2019; Koneˇcny et al.`, 2018). For example, as shown in
Figure 1, moving from single machine to multiple machines reduces the training time and alleviates
the memory burden on each machine. Besides, scaling the training of GNNs with sampling techniques
can result in privacy concerns: existing sampling-based methods require centralized data storage and
model learning, which could result in privacy concerns in real-world scenarios (Shin et al., 2018; Wu
et al., 2021). Fortunately, the privacy in distributed learning can be preserved by avoiding mutual
access to data between different local machines, and using only a trusted third party server to access
the entire data.

Nonetheless, generalizing the existing data parallelism techniques of classical distributed training to the graph domain is non-trivial, which is mainly due to the dependency between nodes
in a graph. For example, unlike solving image classification problems where images are mutually independent, such that we can divide the image dataset into several partitions without
worrying about the dependency between images; GNNs are heavily relying on the information
inherent to a node and its neighboring nodes. As a result, partitioning the graph leads to subgraphs with edges spanning subgraphs (cut-edges), which will cause information loss and hinder the performance of the model (Angerd et al., 2020). To cope with this problem, (Md et al.,
2021; Jiang & Rumi, 2021; Angerd et al., 2020) propose to transfer node features and (Zheng
et al., 2020; Tripathy et al., 2020; Scardapane et al., 2020) propose to transfer both the node
feature and its hidden embeddings between local machines, both of which can cause significant storage/communication overhead and privacy concerns (Shin et al., 2018; Wu et al., 2021).


To better understand the challenge of distributed
GNN training, we compare the validation F1score in Figure 2 (a) and the average data communicated per round in Figure 2 (b) for two different distributed GNN training methods on the
Reddit dataset. On the one hand, we can observe that when ignoring the cut-edges, Parallel
_SGD with Periodic Averaging (PSGD-PA (Dean_
et al., 2012; Li et al., 2020b)) suffers from significant accuracy drop and cannot achieve the
same accuracy as the single machine training,
even by increasing the number of communication. However, Global Graph Sampling (GGS)
can successfully reach the baseline by considering the cut-edges and allowing feature transfer, at
the cost of significant communication overhead,
and potential violation of privacy.


0.50

0.00

0 20 40 60 PSGD-PA GGS

|Col1|Col2|Col3|
|---|---|---|
||||
|||PSGA-PA|
|||GGS Baseline Score|
||||


Number of Communication foo

(a) (b)

10[9]

10[8]

Average Number of Bytes Communicated

PSGD-PA GGS

foo

Figure 2: Comparison of (a) the validation F1score and (b) the average data communicated
per round (in bytes and log-scale) for two different distributed GNN training settings, including
_Parallel SGD with Periodic Averaging (PSGD-_
PA) where the cut-edges are ignored and only
the model parameters are transferred and Global
_Graph Sampling (GGS), where the cut-edges are_
considered and the node features of the cut-edges
are transferred to the corresponding local machine, on the Reddit dataset using 8 machines.


In this paper, we propose a communication-efficient distributed GNN training method, called Learn
_Locally, Correct Globally (LLCG). To reduce the communication overhead, inspired by the recent_
success of the distributed optimization with periodic averaging (Stich, 2019; Yu et al., 2019), we
propose Local Training with Periodic Averaging: where each local machine first locally trains a
GNN model by ignoring the cut-edges, then sends the trained model to the server for periodic model
averaging, and receive the averaged model from server to continue the training. By doing so we
eliminate the features exchange phase between server and local machines, but it can result in a
significant performance degradation due to the lack of the global graph structure and the dependency
between nodes among different machines. To compensate for this error, we propose a Global Server
_Correction scheme to take advantage of the available global graph structure on the server and refine_
the averaged locally learned models before sending it back to each local machine. Notice that without
_Global Server Correction, LLCG is similar to PSGD-PA as introduced in Figure 2._

To get a deeper understanding on the necessity of Global Server Correction, we provide the first
theoretical analysis on the convergence of distributed training for GNNs with periodic averaging. In
particular, we show that solely averaging the local machine models and ignoring the global graph
structure will suffer from an irreducible residual error, which provides sufficient explanation on
why Parallel SGD with Periodic Averaging can never achieve the same performance as the model
trained on a single machine in Figure 2 (a). Then, we theoretically analyze the convergence of
our proposal LLCG . We show that by carefully choosing the number of global correction steps,


-----

LLCG can overcome the aforementioned residual error and enjoys O 1/√PT convergence rate

with P local machines and T iterations of gradient updates, which matches the rate of (Yu et al.,

  

2019) on a general (not specific for GNN training) non-convex optimization setting. Finally, we
conduct comprehensive evaluations on real-world graph datasets with ablation study to validate the
effectiveness of LLCG and its improvements over the existing distributed methods.

**Related works. Recently, several attempts have been made on distributed GNN training. According**
to how they deal with the input/hidden feature of nodes that are associated with the cut-edges (i.e.,
the edges spanning subgraphs of each local machine), existing methods can be classified into two
main categories: (1) Input feature only communication-based methods: In these methods, each local
machine receives the input features of all nodes required for the gradient computation from other
machines, and trains individually. However, since the number of required nodes grows exponentially
with the number of layers, these methods suffer from a significant communication and storage
overhead. To alleviate these issues, (Md et al., 2021) proposes to split the original graph using a
min-cut graph partition algorithm that can minimize the number of cut-edges. (Jiang & Rumi, 2021)
proposes to use importance sampling to assign nodes on the local machine with a higher probability.
(Angerd et al., 2020) proposes to sample and save a small subgraph from other local machines as
an approximation of the original graph structure. Nonetheless, these methods are limited to a very
shallow GNN structure and suffer from significant performance degradation when the original graph
is dense. (2) Input and hidden feature communication-based methods: These methods propose to
communicate hidden features in addition to the input node features. Although these methods reduce
the number of transferred bytes during each communication round (due to the smaller size of hidden
embedding and less required nodes features), the number of communication rounds grows linearly
as the number of layers, and are prone to more communication delay. To address these issues, in
addition to optimal partitioning of the graph, (Zheng et al., 2020) proposes to use sparse embedding
to reduce the number of bytes to communicate and (Tripathy et al., 2020) proposes several graph
partitioning techniques to diminish the communication overhead.

2 BACKGROUND AND PROBLEM FORMULATION

In this section, we start by describing Graph Convolutional Network (GCN) and its training algorithm
on a single machine, then formulate the problem of distributed GCN training. Note that we use
GCN with mean aggregation for simplicity, however, our discussion is also applicable to other GNN
architectures, such as SAGE (Hamilton et al., 2017), GAT (Velickovic et al., 2018), ResGCN (Li
et al., 2019) and APPNP (Klicpera et al., 2019).

**Training GCN on a single machine. Here, we consider the semi-supervised node classification in**
an undirected graph ( _,_ ) with N = nodes and edges. Each node vi is associated with
_G_ _V_ _E_ _|V|_ _|E|_ _∈V_
a pairthe candidate labels in the multi-class classifications. Besides, let (xi, yi), where xi ∈ R[d] is the input feature vector, yi ∈ R X[|C|] = [is the ground truth label, andx1, . . ., xN ] ∈ R[N] _[×][d]_ denote C is
the input node feature matrix. Our goal is to find a set of parameters θ = {W[(][ℓ][)]}ℓ[L]=1 [by minimizing]
the empirical loss L(θ) over all nodes in the training set, i.e.,1

(θ) = [1] _i_ _, yi),_ **h[(]i[ℓ][)]** = σ _j_ **W[(][ℓ][)][],** (1)

where φ(L·, ·) is the loss function (e.g., cross entropy loss),N Xi∈V _[φ][(][h][(][L][)]_  _|N σ((·)v is the activation function (e.g., ReLU),i)|_ Xj∈N (vi) **[h][(][ℓ][−][1)]**
and (vi) is the neighborhood of node vi. In practice, we can update the model parameters by the
_N_
stochastic gradient computed on a sampled mini-batch (using full-neighbors) by
˜ (θ, ξ) = [1] _i_ _, yi),_ (2)
_∇L_ _B_ _i_ _ξ_

_∈_ _[∇][φ][(][h][(][L][)]_

where ξ denotes an i.i.d. sampled mini-batch of sizeX _B and we have E[ ∇L[˜]_ (θ, ξ)] = ∇L(θ).

**Distributed GCN training with periodic averaging.** In this paper, we consider the distributed
learning setting with P local machines and a single parameter server. The original input graph G
is partitioned into P subgraphs, where _p(_ _p,_ _p) denotes the subgraph on the p-th local machine_
_G_ _V_ _E_
with Np = |Vp| nodes, and Xp ∈ R[N][p][×][d] as the input feature of all nodes in Vp located on the p-th
machine. Then, the full-batch local gradient _p_ (θp) is computed as
_∇L[local]_

1

_p_ (θp) = [1] _i_ _, yi), h[(]i[ℓ][)]_ = σ _j_ **Wp[(][ℓ][)]** _,_ (3)
_∇L[local]_ _Np_ Xi∈Vp _[∇][φ][(][h][(][L][)]_  _|Np(vi)|_ Xj∈Np(vi) **[h][(][ℓ][−][1)]** 


-----

**Algorithm 1 Distributed GCN training with “Parallel SGD with Periodic Averaging”**

**Input: Global parameters** **_θ[¯][0], local parameters θp[0]_** [= ¯]θ[0], time-step t = 0, learning rate η.

1: for r ← 1 to R do
2: **for p ←** 1 to P do in parallel _▷_ Parallel training on local machines

3: Local machine p receives the global parameters θp[t] **_θ[t]._** _▷_ Communication

4: **for k ←** 1 to K do _[←]_ [¯]

5: _t ←_ _t + 1._

6: Local machine p constructs the mini-batch ξp[t] [with neighbor sampling.]

7: Local machine p computes the stochastic gradients _∇L[˜]_ _p[local](θp[t]_ _[, ξ]p[t]_ [)][.]

8:9: **end forLocal machine p updates the local parameter by θp[t][+1]** = θp[t] _[−]_ _[η][ ˜]∇Lp[local](θp[t]_ _[, ξ]p[t]_ [)][.]

10: Local machine p sends the local parameters θp[t][+1] to the server. ▷ Communication

11: **end for**

12: Server updates the global parameters by parameter averaging **_θ[¯][t][+1]_** = _P[1]_ _Pp=1_ **_[θ]p[t][+1]._**

13: end for

P

**Output: Server returns trained GCN model with mint E[∥∇L(θ[¯][t])∥[2]].**

whereEp} is the local neighbors of node θp = {Wp[(][ℓ][)][}]ℓ[L]=1 [is the model parameters on the] vi on the p-th local machine. When the graph is large, the[ p][-th local machine,][ N][p][(][v][i][) =][ {][v][j][|][(][v][i][, v][j][)][ ∈]
computational complexity of forward and backward propagation could be very high. One practical
solution is to compute the stochastic gradient on a sampled mini-batch with neighbor sampling, i.e.,

1

˜ _p_ (θp, ξp) = [1] **h[(]i[L][)], yi),** **h[˜][(]i[ℓ][)]** = σ **h˜[(]j[ℓ][−][1)]Wp[(][ℓ][)]** _, (4)_
_∇L[local]_ _Bp_ _i∈ξp_ [˜]p(vi) _j∈N[˜]p(vi)_

X _[∇][φ][(˜]_  _|N_ _|_ X 

where ξp is an i.i.d. sampled mini-batch of Bp nodes, _N[˜]p(vi) ⊂N_ (vi) is the sampled neighbors.

An illustration of distributed GCN training with Parallel SGD with Periodic Averaging (PSGD-PA) is
summarized in Algorithm 1. Before training, the server maintains a global model **_θ[¯][0]_** and each local
machine keeps a local copy of the same model θp[0][. During training, the local machine first updates]
the local model θp[t] [using the stochastic gradient][ ˜]∇Lp[local](θp[t] _[, ξ]p[t]_ [)][ computed by Eq.][ 4][ for][ K][ iterations]
(line 8), then sends the local model θp[t] [to the server (line][ 10][). At each communication step, the server]
collects and averages the model parameters from the local machines (line 12) and send the averaged
model θp[t][+1] back to each local machine.

**Limitations. Although PSGD-PA can significantly reduce the communication overhead by transfer-**
ring the locally trained models instead of node feature/embeddings (refer to Figure 2 (b)), it suffers
from performance degeneration due to ignorance of the cut-edges (refer to Figure 2 (a)). In the next
section, we introduce a communication-efficient algorithm LLCG that does not suffer from this
issue, and can achieve almost the same performance as training the model on a single machine.

3 PROPOSED ALGORITHM: LEARN LOCALLY CORRECT GLOBALLY

In this section, we describe Learn Locally, Correct Globally (LLCG) for distributed GNN training.
LLCG includes two main phases, local training with periodic model averaging and global server
_correction, to help reduce both the number of required communications and size of transferred data,_
without compromising the predictive accuracy. We summarize the details of LLCG in Algorithm 2.

3.1 LOCAL TRAINING WITH PERIODIC MODEL AVERAGING

At the beginning of a local epoch, each local machine receives the latest global model parameters
from the server (line 3). Next, each local machine runs Kρ[r] iterations to update the local model
(line 4 to 9), where K and ρ are the hyper-parameters that control the local epoch size. Note that
instead of using a fixed local epoch size as Algorithm 1, we choose to use exponentially increasing
local epoch size in LLCG with ρ > 1. The reasons are as follows.

At the beginning of the training phase, all local models θp[t] [are far from the optimal solution and]
will receive a gradient _∇L[˜]_ _p[local](θp[t]_ _[, ξ]p[t]_ [)][ computed by Eq.][ 4][. Using a smaller local update step at]
the early stage guarantees each local model does not diverge too much from each other before the
model averaging step at the server side (line 12). However, towards the end of the training, all local


-----

**Algorithm 2 Distributed GCN training by “Learn Locally, Correct Globally”**


**Input: Global parameters** **_θ[¯][0], local parameters θp[0][, time-step][ t][ = 0][, local step size hyper-]_**
parameters K, ρ, and learning rate γ, η

1: for r ← 1 to R do
2: **for p ←** 1 to P do in parallel _▷_ Parallel training on local machine

3: Local machine p receives the global parameters θp[t] **_θ[t]_** _▷_ Communication

4: **for k ←** 1 to Kρ[r] **do** _[←]_ [¯]

5: _t ←_ _t + 1_

6: Local machine p constructs the mini-batch ξp[t] [with neighbor sampling]

7: Local machine p computes stochastic gradients [˜] _p_ (θp[t] _[, ξ]p[t]_ [)]
_∇L[local]_

8: Local machine p updates model parameter by θp[t][+1] = θp[t] _p_ (θp[t] _[, ξ]p[t]_ [)]

9: **end for** _[−]_ _[η][ ˜]∇L[local]_

10: Local machine p sends the local parameters θp[t][+1] to the server _▷_ Communication

11: **end for**

12: Server updates the global parameters using parameter averaging **_θ[¯][t][+1]_** = _P[1]_ _Pp=1_ **_[θ]p[t][+1]_**

13: **for s ←** 1 to S do _▷_ Server CorrectionP

14: _t ←_ _t + 1_

15: Server constructs a mini-batch ξ[t] with full-neighbors

16: Server computes the stochastic gradient _∇L[˜]_ (θ[¯][t], ξ[t])

17: Server updates the global parameters by **_θ[¯][t][+1]_** = θ[¯][t] _−_ _γ∇L[˜]_ (θ[¯][t], ξ[t])

18: **end for**

19: end for
**Output: Server return GCN model with trained mint E[∥∇L(θ[¯][t])∥[2]]**

models θp[t] [will receive relatively smaller gradient][ ˜]∇Lp[local](θp[t] _[, ξ]p[t]_ [)][, such that we can chose a larger]
local epoch size to reduce the number of communications, without worrying about the divergence of
local models. By doing so, after total number of T = _r=1_ _[Kρ][r][ iterations, LLCG only requires]_
_R = logρ_ _KT_ [rounds of communications. Therefore, compared to the fully-synchronous method, we]

can significantly reduce the total number of communications from[P][R] (T ) to (logρ _KT_ [)][.]
_O_ _O_

3.2 GLOBAL SERVER CORRECTION

The design of the global server correction is to ensure that the trained model not only learns from
the data on each local machine, but also learns the global structure of the graph, thus reducing
the information loss caused by graph partitioning and avoiding cut-edges. Before the correction,
the server receives the locally trained models from all local machines (line 10) and applies model
parameter averaging (line 12). Next, S server correction steps are applied on top of the averaged
model (line 13 to 18). During the correction, the server first constructs a mini-batch ξ[t] using fullneighbors[1](line 15), compute the stochastic gradient _∇L[˜]_ (θ[¯][t], ξ[t]) on the constructed mini-batch by
Eq. 2 (line 16) and update the averaged model **_θ[¯][t]_** for S iterations (line 17). The number of correction
steps S [2] depends on the heterogeneity among the subgraphs on each local machine: the more
heterogeneous the subgraphs are, the more correction steps are required to better refine the averaged
model and reduce the divergence across the local models. Note that, the heterogeneity is minimized
when employing GGS (Figure 2) with the local machines having access to the full graph, as a result.
However, GGS requires sampling from the global graph and communication at every iteration, which
results in additional overhead and lower efficiency. Instead, in LLCG we are trading computation on
the server for the costly feature communication, and only requires periodic communication.

4 THEORETICAL ANALYSIS

In this section, we provide the convergence analysis on the distributed training of GCN under two
different settings, i.e., with and without server correction. We first introduce the notations and
assumptions for the analysis (Section 4.1). Then, we show that periodic averaging of local machine
models alone and ignoring the global graph structure will suffer from an irreducible residual error
(Section 4.2). Finally, we show that this residual error can be eliminated by running server correction
steps after each periodic averaging step on the server (Section 4.3).

1Note that using full neighbors is required for the server correction but not the local machines
2In practice, we found S = 1 or S = 2 works well on most datasets.


-----

(a)

With cross-device communication
Without neighbor sampling


(b)

Without cross-device communication
Without neighbor sampling


(c)

Without cross-device communication
With neighbor sampling


_∇L1[full][(][θ][)]_ _∇L2[full][(][θ][)]_ _∇L1[local](θ)_ _∇L2[local](θ)_ _∇L˜_ 1[local](θ, ξ1) _∇L˜_ 2[local](θ, ξ2)


ℓ=2

ℓ=1

ℓ=0


_∇L(θ) =_ _[∇L]1[full][(][θ][) +]2_ _[ ∇L]2[full][(][θ][)]_ _∇L(θ) ̸=_ _[∇L]1[local](θ) +2_ _∇L2[local](θ)_ EE! ˜ ˜∇L∇L1[local]1[local]((θθ, ξ, ξ12))" ≠≠ _∇L ∇L1[local]2[local]((θθ)),_

! "

where the blue node and green circles represent nodes on different local machines.Figure 3: Comparison of notations∇ℒ𝜃 ≠0.5× ∇ℒ!"#$%" 𝜃+ ∇ℒ &"#$%" ∇L(𝜃) _p[local](θ),_ _∇L[˜]_ _p[local](θ, ξp), and ∇Lp[full][(][θ][)][ on two local machines,]_

Central

4.1 NOTATIONS AND ASSUMPTIONS

\mathcal{N}(v_i)

Let us first recall the notations defined in Section 2, where L(θ) denotes the global objective function
computed using the all node featuresfunction computed using the local node features X and the original graph Xp and local graph𝔼𝔼 _G∇ℒ∇ℒ[-][-]_ !&"#$%""#$%", L(𝜃) ≠∇ℒ(𝜃) ≠∇ℒp(θ)! G&"#$%""#$%" denotes the local objectiveP 𝜃(𝜃)p, θp[t] [denotes the model]
parameters on the p-th local machine at the t-th step, and **_θ[¯][t]_** = _P[1]_ _p=1_ **_[θ]p[t]_** [denotes the virtual]

averaged model at the t-th step. In the non-convex optimization, our goal is to show the expected
gradient of the global objective on the virtual averaged model parametersP E[∥∇L(θ[¯][t])∥[2]] decreases as
the number of local machines P and the number of training steps T increase. Besides, we introduce
_∇Lp[full][(][θ][)][ as the gradient computed on the][ p][-th local machine but have access the full node features]_
**X and the original graph structure1** _G as_ 1

Please refer to Figure∇Lp[full][(][θ][) =] _|Vp|_ X 3 for an illustration of different gradient computations. Besides, we introducei∈Vp _[∇][φ][(][h]i[(][L][)], yi),_ **h[(]i[ℓ][)]** = σ _|N_ (vi)| Xj∈N (vi) **[h]j[(][ℓ][−][1)]Wp[(][ℓ][)].** (5)
_local-global gradient discrepancy as κ[2]_ = κ[2]A [+][ κ]X[2] [, where][ κ][2]A [= max][p][∈][[][P][ ]][{∥∇L]p[local](θ) −
_∇Lp[full][(][θ][)][∥][2][}][ is the maximum difference between the gradient computed on the local machine with]_
and without having access to the global graph structure, which is mainly due to fact that the local
machines are oblivious to the full graph information; and κ[2]X [= max][p][∈][[][P][ ]][{∥∇L]p[full][(][θ][)][ −∇L][(][θ][)][∥][2][}]
is the maximum difference between the gradient computed using the local node and all nodes, which
is mainly due to the heterogeneity of the node features on each local machine, and we have κ[2]X [= 0]
if the nodes are i.i.d. sampled to each local machine. Notice that local-global gradient discrepancy
_κ[2]_ plays an important role in our theoretical results.

For the convergence analysis, we make the following standard assumptions.

**Assumption 1 The stochastic gradient on the p-th local machine (with neighbor sampling) has**
_stochastic gradient variance bounded by σvar[2]_ _[and stochastic gradient bias bounded by][ σ]bias[2]_ _[, i.e.,]_
E[∥∇L[˜] _p[local](θ; ξ) −_ E[ ∇L[˜] _p[local](θ; ξ)]∥[2]] ≤_ _σvar[2]_ _[,][ E][[][∥][E][[ ˜]∇Lp[local](θ; ξ)] −∇Lp[local](θ)∥[2]] ≤_ _σbias[2]_ _[.]_

**Assumption 2 The stochastic gradient for global server correction (with full neighbors) has stochas-**
_tic gradient variance bounded by σglobal[2]_ _[, i.e.,][ E][[][∥]∇L[˜]_ _p[full][(][θ][;][ ξ][)][ −∇L]p[full][(][θ][)]][∥][2][]][ ≤]_ _[σ]global[2]_ _[.]_

The existence of stochastic gradient bias and variance in sampling-based GNN training have been
studied in (Cong et al., 2020; 2021), where (Cong et al., 2021) further quantify the stochastic gradient
bias and variance as a function of the number of GCN layers. In particular, they show that the
existence of σbias[2] [is due to neighbor sampling and non-linear activation, and we have][ σ]bias[2] [= 0][ if all]
neighbors are used or the non-linear activation is removed. The existence of σvar[2] [is because we are]
sampling mini-batches to compute the stochastic gradient on each local machine during training. As
the mini-batch size increases, σvar[2] [will be decreasing, and we have][ σ]var[2] [= 0][ when using full-batch.]

4.2 DISTRIBUTED GNN VIA PARAMETER AVERAGING

In the following, we provide the first convergence analysis on distributed training of GCN. We show
that solely periodic averaging of the local machine models and ignoring the global graph structure
suffers from an upper bound that is irreducible with the number of training steps. Comparing to the
traditional distributed training (e.g., distributed training Convolutional Neural Network for image
classification (Dean et al., 2012; Li et al., 2020b)), the key challenges in the distributed GCN training
is the two different types of gradient bias: (1) The expectation of the local full-batch gradient is


(θ) = 1 [(][θ][) +][ ∇L]2[full][(][θ][)]
_∇L_ _[∇L][full]_ 2


(θ) = 1 (θ) + ∇L2[local]
_∇L_ _̸_ _[∇L][local]_ 2


(θ)


-----

a biased estimation of the global full-batch gradient, i.e., _P1_ _Pp=1_ _p_ (θ) = (θ). This is

because each local machine does not have access to the original input graph and full node feature[∇L][local] _̸_ _∇L_
matrix. Note that the aforementioned equivalence is important for the classifcal distributed trainingP
analysis Dean et al. (2012); Yu et al. (2019). (2) The expectation of the local stochastic gradient is a
biased estimation of the local full-batch gradient i.e., E[ [˜] _p_ (θ, ξp)] = _p_ (θ). This is because
_∇L[local]_ _̸_ _∇L[local]_
the stochastic gradient on each local machine is computed by using neighbor sampling, which has
been studied in (Cong et al., 2021).

**Theorem 1 (Distributed GCN via Parameter Averaging) Consider applying model averaging for**
_GNN training under Assumption 1 and 2. If we choose learning rate η =_ _√√PT_ _[and the local step size]_

_K_ _√8LP2T_ [3][1][/][/][4][4][, then for any][ T][ ≥] _[L][2][P][ steps of gradient updates we have]_
_≤_ 1 _T −1_ 1

**_θ[t])_** ] = + (κ[2] + σbias[2] [)][.]

_T_ _t=0_ [E][[][∥∇L][(¯] _∥[2]_ _O_ _√PT_ _O_

 

X

Theorem 1 implies that, by carefully choosing the learning rate η and the local step size K, the gradient
norm computed on the virtual averaged model is bounded by (1/√PT ) after R = T/K = ( _[P]T[ 3][3][/][/][4][4][ )]_
_O_ _O_

communication rounds, but suffers from an irreducible residual error upper bound O(κ[2] + σbias[2] [)][. In]
the next section, we show that this residual error can be eliminated by applying server correction.

4.3 DISTRIBUTED GCN VIA SERVER CORRECTION


Before proceeding to our result, in order to simplify the presentation, let us first define the notationas the minimum gradient computed at thelocal G(r[r]global) are the number of iteration run after the[= min][t][∈T]global[(][r][)] [E][[][∥∇L][(¯]θ[t])∥[2]] and r-th round global and local step, where G r[r]local-th communication round on server and local[= min][t][∈T]local[(][r][)] [E] _P[1]_ PPp=1 _[∇L] Tglobalp[local]((θrp[t])[)] and[2][]_

_T_
machine, respectively. Please refer to Eq. 42 in Appendix C.2 for a formal definition.

**Theorem 2 Consider applying model averaging for GCN training under Assumption 1 and 2. If we**
_choose learning rate γ = η =_ _√√PT_ _[, the local step size][ K, ρ][ such that][ P]r[R]=1_ _[K]_ [2][ρ][2][r][ ≤] 32RTL[2]P[1][/][3][2][/][2][, and]

_server correction step size S = maxr∈[R]_ 1 _κL[2]+2([√]σP/Tbias[2]_ ) _local_ _KρG[r]localr_ _[, then for any][ T][ ≥]_ _[L][2][P][ steps]_

_of gradient updates we have:_ _T[1]_ _Tt=1_ [E][[][∥∇L]  _−[(¯]θ[t])∥[2]] =[−] O[G][r]√P T1_  _._

Theorem 2 implies that, by carefully choosing the learning ratesP   _γ_ and η, the local step size hyperparameters K, ρ, and the number of global correction steps S, after T steps (R rounds of communication), employing parameter averaging with Global Server Correction, we have the norm of gradient
bounded by O(1/√PT ), without suffering the residual error that exists in the naive parameter

averaging (in Theorem 1). Besides, the server correction step size is proportional to the scale of κ[2]
and local stochastic gradient bias σbias[2] [. The larger][ κ][2][ and][ σ]bias[2] [, the more corrections are required to]
eliminate the residual error. However, in practice, we observe that a very small number of correction
steps (e.g., S = 1) performs well, which minimizes the computation overhead on the server.

5 EXPERIMENTS

**Real-world simulation. In a real-world distributed setting, the server and local machines are located**
on different machines, connected through the network (Li et al., 2020a). However, for our experiments,
we only have access to a single machine with multiple GPUs. As a result, we simulate a real-world
distributed learning scenario, such that each GPU is responsible for the computation of two local
machines (8 in total) and the CPU acts as the server. For these reasons, in our evaluations, we opted
to report the communication size and number of communication rounds, instead of the wall-clock
time, which can show the benefit of distributed training. We argue that these are acceptable measures
in real-world scenarios as well since the two main factors in distributed training are initializing
connection overhead and bandwidth (Tripathy et al., 2020).
**Baselines. To illustrate the effectiveness of LLCG, we setup two general synchronized distributed**
training techniques as the our baseline methods, namely “Parallel SGD with Parameter Averaging”
(PSGD-PA) and “Global Graph Sampling” (GGS), as introduced in Figure 2, where the cut-edges
in PSGD-PA are ignored and only the model parameters are transferred, but the cut-edges in GGS
are considered and the node features of the cut-edges are transferred to the corresponding machine.


-----

Proteins



Reddit

(d)


0.50

0.45

0.40

1.4

1.2

1.0

0.8


0.70

0.65

0.60

0.7

0.6

0.5

0.4


0.8

0.6

0.4

0.2


0.7


|Col1|Col2|Col3|
|---|---|---|
||PSG|D-PA|
||GGS LLC|G|


|0 .6|20 40 No. Communications PSGD-PA GGS LLCG (b)|Col3|Col4|
|---|---|---|---|
||Reddit|||
|||PSG GGS|D-PA|
|||LLC|G|
|||||


20 40 60

No. Communications

(f)

|Col1|Flickr|Col3|
|---|---|---|
||||
||||
|||PSGD-PA GGS LLCG|
|10 20 3 No. Communications|||

|Col1|Col2|
|---|---|
|||
||PSGD-PA|
||GGS LLCG|
|0 20 No. Com|40 60 munications|

|Col1|Arxiv|Col3|
|---|---|---|
||||
||||
||P G L|SGD-PA GS LCG|
|0|20 40 No. Communications (c)||


|Col1|Arxiv|Col3|
|---|---|---|
||||
||PSGD-PA GGS||
||LLCG||


10[8] 10[10]

Size of Comm. in Bytes (Log Scale)

(g)

|No. Communications (a)|Col2|Col3|
|---|---|---|
|Arxiv|||
||PSG GG|D-PA S|
||LLC|G|
||||


20 40

PSGD-PA
GGS
LLCG

No. Communications

(e)



Reddit

0.5

Global Val. F1 Score0.0

10[8] 10[10]

Size of Comm. in Bytes (Log Scale)

(h)


Figure 4: Comparing LLCG against PSGD-PA and GGS on real-world datasets. We show the global
validation score in terms of the number of communications in (a,b,c,d), the training loss per round of
communications in (e,f ), and the global validation score per bytes of exchanged data in (g,h).

Table 1: Comparison of performance and the average Megabytes of node representation/feature
communicated per round on various datasets.

Method No. GCN / SAGE GAT APPNP
Comm. Performance Avg. MB Performance Avg. MB Performance Avg. MB


PSGD-PA 49.08±0.27 12.57 51.56±0.28 4.24 50.81±0.48 8.40
GGS 50 51.22±0.13 1849.32 52.41±0.29 1895.61 51.33±0.33 1897.82
**LLCG** 50.38±0.20 12.57 52.01±0.33 4.24 51.15±0.25 8.40

PSGD-PA 72.85±0.70 6.20 64.95±1.01 3.14 71.10±0.79 7.31
GGS 100 74.78±0.36 922.42 68.11±0.60 912.79 71.29±0.31 917.20
**LLCG** 73.92±0.45 6.20 67.62±0.58 3.14 71.18±0.43 7.31

PSGD-PA 69.43±0.21 3.55 69.88±0.18 3.59 68.48±0.17 7.71
GGS 100 70.51±0.26 3391.03 70.82±0.23 3396.79 69.01±0.10 3394.33
**LLCG** 70.21±0.13 3.55 70.58±0.37 3.59 68.73±0.29 7.71

PSGD-PA 71.17±1.06 14.83 70.57±1.24 7.48 83.48±0.81 11.63
GGS 75 94.77±0.20 3798.81 95.03±0.48 3805.28 95.23±0.22 3770.46
**LLCG** 94.67±0.15 14.83 94.73±0.23 7.48 94.64±0.17 11.63


Flickr
(F1-score)

OGB-Proteins
(ROC-AUC)


OGB-Arxiv
(F1-score)

Reddit
(F1-score)


Note that we choose GGS as a reasonable representation for most existing proposals (Md et al., 2021;
Zheng et al., 2020; Tripathy et al., 2020) for distributed GNN training, since these methods have very
close communication cost and also require a large cluster of machines to truly show their performance
improvement. We also use PSGD-PA as a lower bound for communication size, which is widely
used in traditional distributed training and similar to the one used in (Angerd et al., 2020; Jiang &
Rumi, 2021). However, we did not specifically include these methods in our results since we could
not reproduce their results in our settings. Please refer to Appendix A for a detailed description of
implementation, hardware specification and link to our source code.
**Datasets and evaluation metric.** We compare LLCG and other baselines on real-world semisupervised node classification datasets, details of which are summarized in Table 2 in the Appendix.
The input graphs are splitted into multiple subgraphs using METIS before training, then the same
set of subgraphs are used for all baselines. For training, we use neighborhood sampling (Hamilton
et al., 2017) with 10 neighbors sampled per node and ρ = 1.1 for LLCG. For a fair comparison, we
chose the base local update step K such that LLCG has the same number of local update steps as
PSGD-PA. During evaluation, we use full-batch without sampling, and report the performance on
the full graph using AUC ROC and F1 Micro as the evaluation metric. Unless otherwise stated, we
conduct each experiment five times and report the mean and standard deviation.

5.1 PRIMARY RESULTS


In this section, we compare our proposed LLCG algorithm with baselines on four datasets. Due to
space limitations we defer the detailed discussion on additional datasets to the Appendix A.4.

**LLCG requires same number of communications.** Figure 4 (a) through 4 (d) illustrate the
validation accuracy per communication rounds on four different datasets. We run a fixed number of
communication rounds and plot the global validation score (the validation score computed using the
full-graph on the server) at the end of each communication step. For PSGD-PA and GGS, the score is
calculated on the averaged model, whereas for LLCG the validation is calculated after the correction


-----

Arxiv


Reddit

20 40 60

|Col1|Col2|e|Col4|Col5|Col6|
|---|---|---|---|---|---|
|.5 .0||0.7 Score F1 S=1 0.6 Val. %,S=1, S=1 Global, S=2 0.5|||S=1 %,S=1, S=1, S=4|
||w/o sampling, w/ sampling 20 w/ sampling 5% w/ sampling 5%|||w/o sampling, w/ sampling 20 w/ sampling 5% w/ sampling 5%||
|||||||


Number of Communications


Arxiv

w/o sampling ,S=1
w/ sampling 20%,S=1
w/ sampling 5%, S=1
w/ sampling 5%, S=4

20 40 60

Number of Communications


0.8

0.7

0.6

0.5




0.7


0.5

0.0


Figure 6: Effect of sampling on local machine and number of correction steps on the server


Figure 5: Effect of local

|K=1 K=4|K=16 K=128 K=64|
|---|---|
|||
|||
|0 20 Number of|40 Communication|

epoch size (K)


step. It can be seen that PSGD-PA suffers from performance drop compared to other two methods,
due to the residual error we discussed in Section 4, while both GGS and LLCG perform well and
can achieve the expected accuracy. Note that the performance drop of PSGD-PA can vary across
different datasets; in some cases such as Reddit, PSGD-PA can significantly hurt the accuracy,
while on other datasets the gap is smaller. Nevertheless, LLCG can always close the gap between
PSGD-PA and GGS with minimal overhead.
**LLCG convergences as fast as GGS. To represent the effect of communication on the real-time**
convergence, in Figure 4 (e) and 4 (f), we plot the global training loss (training loss computed on the
full-graph on the server) after each communication round. Similar to the accuracy score, the training
loss is also computed on the server averaged (and corrected, in case of LLCG) global model. These
results clearly indicate that LLCG can improve the convergence over PSGD-PA, while it shows a
similar convergence speed to GGS.
**LLCG exchanges data as little as PSGD-PA. Figure 4 (g) and 4 (h) show the relation between**
global validation accuracy with the average size (volume) of communication in bytes. As expected,
this figure clearly shows the effectiveness of LLCG . On the one hand, LLCG has a similar amount
of communication volume as PSGD-PA but can achieve a higher accuracy. On the other hand,
LLCG requires significantly less amount of communication volume than GGS to achieve the same
accuracy, which leads to slower training time in real world settings.
**LLCG works with various GNN models and aggregations.** We evaluate four popular GNN
models, used in recent graph learning literature: GCN Kipf & Welling (2017), SAGE Hamilton et al.
(2017), GAT Velickovic et al. (2018) and APPNP Klicpera et al. (2019). In Table 1, we summarize
the test score and average communication size (in MB) on different datasets for a fixed number of
communication rounds. Note that we only include the results for the aggregation methods (GCN
or SAGE) that have higher accuracy for the specific datasets, details of which can be found in
Appendix A.2. As shown here, LLCG can consistently improve the test accuracy for all different
models compared to PSGD-PA, while the communication size is significantly lower than GGS, since
LLCG only needs to exchange the model parameters.
**Effect of local epoch size.** Figure 5 compares the effect of various values of local epoch size
_K ∈{1, 4, 16, 64, 128} for fixed ρ and S on the OGB-Arxiv dataset. When using fully synchronous_
with K = 1, the model suffers from very slow convergence and needs more communications. Further
increasing the K to larger values can speed up the training; however, we found a diminishing return
point for K > 128 in this dataset and extremely large K in general.
**Effect of sampling in local machines. In Figure 6, we report the validation scores per round of**
communication to compare the effect of neighborhood sampling at local machines. We can observe
that when the neighborhood sampling size is reasonably large (i.e., 20%), the performance is very
similar to full neighborhood training. However, reducing the neighbor sampling ratio to 5% could
result in a larger local stochastic gradient bias σbias[2] [, which requires using more correction steps (][S][).]

6 CONCLUDING REMARKS


In this paper, we propose a novel distributed algorithm for training Graph Neural Networks (GNNs).
We theoretically analyze various GNN models and discover that, unlike the traditional deep neural
networks, due to inherent data samples dependency in GNNs, naively applying periodic parameter
averaging leads to a residual error and current solutions to this issue impose huge communication
overheads. Instead, our proposal tackles these problems by applying correction on top of locally
learned models, to infuse the global structure of the graph back into the network and avoid any costly
communication. In addition, through extensive empirical analysis, we support our theoretical findings
and demonstrate that LLCG can achieve high accuracy without additional communication costs.


-----

ACKNOWLEDGEMENTS

This work was supported in part by CRISP, one of six centers in JUMP, a Semiconductor Research
Corporation (SRC) program sponsored by DARPA and NSF grants 1909004, 1714389, 1912495,
1629915, 1629129, 1763681, 2008398.

REPRODUCIBILITY STATEMENT

We provide a GitHub repository in Appendix A including all code and scripts used in our experimental studies. This repository includes a README.md file, explaining how to install and
prepare the code and required packages. Detailed instruction on how to use the partitioning
scripts is provided for various datasets. In addition, we provide several configuration files (under scripts/configs) folder for different hyper-parameters on each individual dataset, and a
general script (scripts/run-config.py) to run and reproduce the results with these configurations. Details of various models and parameters used in our evaluation studies can also be found in
Appendix A.

REFERENCES

Alexandra Angerd, Keshav Balasubramanian, and Murali Annavaram. Distributed training of graph
convolutional networks using subgraph approximation. arXiv preprint arXiv:2012.04930, 2020.

Paolo Boldi and Sebastiano Vigna. The webgraph framework I: compression techniques. In Proceed_ings of the 13th international conference on World Wide Web, WWW 2004, New York, NY, USA,_
_May 17-20, 2004, pp. 595–602. ACM, 2004. doi: 10.1145/988672.988752._

Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir
Ivanov, Chloé Kiddon, Jakub Konecný, Stefano Mazzocchi, Brendan McMahan, Timon Van
Overveldt, David Petrou, Daniel Ramage, and Jason Roselander. Towards federated learning
at scale: System design. In Proceedings of Machine Learning and Systems 2019, MLSys 2019,
_Stanford, CA, USA, March 31 - April 2, 2019. mlsys.org, 2019._

Jianfei Chen, Jun Zhu, and Le Song. Stochastic training of graph convolutional networks with
variance reduction. In Proceedings of the 35th International Conference on Machine Learning,
_ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings_
_of Machine Learning Research, pp. 941–949. PMLR, 2018._

Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An
efficient algorithm for training deep and large graph convolutional networks. In Proceedings of the
_25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2019,_
_Anchorage, AK, USA, August 4-8, 2019, pp. 257–266. ACM, 2019. doi: 10.1145/3292500.3330925._

Weilin Cong, Rana Forsati, Mahmut T. Kandemir, and Mehrdad Mahdavi. Minimal variance sampling
with provable guarantees for fast training of graph neural networks. In KDD ’20: The 26th ACM
_SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August_
_23-27, 2020, pp. 1393–1403. ACM, 2020._

Weilin Cong, Morteza Ramezani, and Mehrdad Mahdavi. On the importance of sampling in learning
graph convolutional networks. arXiv preprint arXiv:2103.02696, 2021.

Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao,
Marc’Aurelio Ranzato, Andrew W. Senior, Paul A. Tucker, Ke Yang, and Andrew Y. Ng. Large
scale distributed deep networks. In Advances in Neural Information Processing Systems 25: 26th
_Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting_
_held December 3-6, 2012, Lake Tahoe, Nevada, United States, pp. 1232–1240, 2012._

Songgaojun Deng, Huzefa Rangwala, and Yue Ning. Learning dynamic context graphs for predicting
social events. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge
_Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019, pp. 1007–1016._
ACM, 2019. doi: 10.1145/3292500.3330919.


-----

Kien Do, Truyen Tran, and Svetha Venkatesh. Graph transformation policy network for chemical
reaction prediction. In Proceedings of the 25th ACM SIGKDD International Conference on
_Knowledge Discovery & Data Mining, KDD 2019, Anchorage, AK, USA, August 4-8, 2019, pp._
750–760. ACM, 2019. doi: 10.1145/3292500.3330958.

Faezeh Faez, Yassaman Ommi, Mahdieh Soleymani Baghshah, and Hamid R Rabiee. Deep graph
generators: A survey. IEEE Access, 9:106675–106702, 2021.

Alex Fout, Jonathon Byrd, Basir Shariat, and Asa Ben-Hur. Protein interface prediction using graph
convolutional networks. In Advances in Neural Information Processing Systems 30: Annual
_Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach,_
_CA, USA, pp. 6530–6539, 2017._

Mahsa Ghorbani, Anees Kazi, Mahdieh Soleymani Baghshah, Hamid R Rabiee, and Nassir Navab.
Ra-gcn: Graph convolutional network for disease prediction problems with imbalanced data.
_Medical Image Analysis, 75:102272, 2022._

William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural
_Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp. 1024–1034,_
2017.

Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Françoise Beaufays, Sean
Augenstein, Hubert Eichner, Chloé Kiddon, and Daniel Ramage. Federated learning for mobile
keyboard prediction. arXiv preprint arXiv:1811.03604, 2018.

Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,
and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In
_Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information_
_Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020._

Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec. Ogb-lsc: A
large-scale challenge for machine learning on graphs. arXiv preprint arXiv:2103.09430, 2021.

Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In Proceedings of the 32nd International Conference on
_Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and_
_Conference Proceedings, pp. 448–456. JMLR.org, 2015._

Peng Jiang and Masuma Akter Rumi. Communication-efficient sampling for distributed training of
graph convolutional networks. arXiv preprint arXiv:2101.07706, 2021.

Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April
_24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017._

Johannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann. Predict then propagate:
Graph neural networks meet personalized pagerank. In 7th International Conference on Learning
_Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019._

Jakub Koneˇcny, H Brendan McMahan, X Yu Felix, Ananda Theertha Suresh, Dave Bacon, and Peter`
Richtárik. Federated learning: Strategies for improving communication efficiency. 2018.

Guohao Li, Matthias Müller, Ali K. Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep
as cnns? In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul,
_Korea (South), October 27 - November 2, 2019, pp. 9266–9275. IEEE, 2019. doi: 10.1109/ICCV._
2019.00936.

Shen Li, Yanli Zhao, Rohan Varma, Omkar Salpekar, Pieter Noordhuis, Teng Li, Adam Paszke, Jeff
Smith, Brian Vaughan, Pritam Damania, et al. Pytorch distributed: Experiences on accelerating
data parallel training. arXiv preprint arXiv:2006.15704, 2020a.


-----

Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of
fedavg on non-iid data. In 8th International Conference on Learning Representations, ICLR 2020,
_Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020b._

Bill Yuchen Lin, Chaoyang He, Zihang Zeng, Hulin Wang, Yufen Huang, Mahdi Soltanolkotabi,
Xiang Ren, and Salman Avestimehr. Fednlp: A research platform for federated learning in natural
language processing. arXiv preprint arXiv:2104.08815, 2021.

Vasimuddin Md, Sanchit Misra, Guixiang Ma, Ramanarayan Mohanty, Evangelos Georganas, Alexander Heinecke, Dhiraj Kalamkar, Nesreen K Ahmed, and Sasikanth Avancha. Distgnn: Scalable
distributed training for large-scale graph neural networks. arXiv preprint arXiv:2104.06700, 2021.

Morteza Ramezani, Weilin Cong, Mehrdad Mahdavi, Anand Sivasubramaniam, and Mahmut Kandemir. Gcn meets gpu: Decoupling “when to sample” from “how to sample”. Advances in Neural
_Information Processing Systems, 33, 2020._

Simone Scardapane, Indro Spinelli, and Paolo Di Lorenzo. Distributed graph convolutional networks.
_arXiv preprint arXiv:2007.06281, 2020._

Hyejin Shin, Sungwook Kim, Junbum Shin, and Xiaokui Xiao. Privacy enhanced matrix factorization
for recommendation with local differential privacy. IEEE Transactions on Knowledge and Data
_Engineering, 30(9):1770–1782, 2018._

Sebastian U. Stich. Local SGD converges fast and communicates little. In 7th International
_Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019._
OpenReview.net, 2019.

Chuxiong Sun and Guoshi Wu. Adaptive graph diffusion networks with hop-wise attention. arXiv
_preprint arXiv:2012.15024, 2020._

Chuxiong Sun and Guoshi Wu. Scalable and adaptive graph neural networks with self-label-enhanced
training. arXiv preprint arXiv:2104.09376, 2021.

Alok Tripathy, Katherine Yelick, and Aydin Buluc. Reducing communication in graph neural network
training. arXiv preprint arXiv:2005.03300, 2020.

Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua
Bengio. Graph attention networks. In 6th International Conference on Learning Representations,
_ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings._
OpenReview.net, 2018.

Jizhe Wang, Pipei Huang, Huan Zhao, Zhibo Zhang, Binqiang Zhao, and Dik Lun Lee. Billion-scale
commodity embedding for e-commerce recommendation in alibaba. In Proceedings of the 24th
_ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2018,_
_London, UK, August 19-23, 2018, pp. 839–848. ACM, 2018. doi: 10.1145/3219819.3219869._

Chuhan Wu, Fangzhao Wu, Yang Cao, Yongfeng Huang, and Xing Xie. Fedgnn: Federated graph
neural network for privacy-preserving recommendation. arXiv preprint arXiv:2102.04925, 2021.

Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L. Hamilton, and Jure Leskovec.
Graph convolutional neural networks for web-scale recommender systems. In Proceedings of the
_24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD 2018,_
_London, UK, August 19-23, 2018, pp. 974–983. ACM, 2018. doi: 10.1145/3219819.3219890._

Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted sgd with faster convergence and less
communication: Demystifying why model averaging works for deep learning. In Proceedings of
_the AAAI Conference on Artificial Intelligence, volume 33, pp. 5693–5700, 2019._

Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor K. Prasanna.
Graphsaint: Graph sampling based inductive learning method. In 8th International Conference on
_Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net,_
2020.


-----

Qingru Zhang, David Wipf, Quan Gan, and Le Song. A biased graph neural network sampler with
near-optimal regret. arXiv preprint arXiv:2103.01089, 2021.

Da Zheng, Chao Ma, Minjie Wang, Jinjing Zhou, Qidong Su, Xiang Song, Quan Gan, Zheng Zhang,
and George Karypis. Distdgl: Distributed graph neural network training for billion-scale graphs.
_arXiv preprint arXiv:2010.05337, 2020._

Difan Zou, Ziniu Hu, Yewen Wang, Song Jiang, Yizhou Sun, and Quanquan Gu. Layer-dependent
importance sampling for training deep and large graph convolutional networks. In Advances in
_Neural Information Processing Systems 32: Annual Conference on Neural Information Processing_
_Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 11247–11256,_
2019.


-----

