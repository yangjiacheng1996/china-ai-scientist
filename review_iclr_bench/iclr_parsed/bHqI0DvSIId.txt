# NEURAL SIMULATED ANNEALING

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Simulated annealing (SA) is a stochastic global optimisation technique applicable
to a wide range of discrete and continuous variable problems. Despite its simplicity, the development of an effective SA optimiser for a given problem hinges on
a handful of carefully handpicked components; namely, neighbour proposal distribution and temperature annealing schedule. In this work, we view SA from a
reinforcement learning perspective and frame the proposal distribution as a policy,
which can be optimised for higher solution quality given a fixed computational
budget. We demonstrate that this Neural SA with such a learnt proposal distribution outperforms SA baselines with hand-selected parameters on a number of
problems: Rosenbrock’s function, the Knapsack problem, the Bin Packing problem, and the Travelling Salesperson problem. We also show that Neural SA scales
well to large problems while again outperforming popular off-the-shelf solvers in
terms of solution quality and wall clock time.

1 INTRODUCTION

There are many different kinds of combinatorial optimisation (CO) problem, spanning bin packing,
routing, assignment, scheduling, constraint satisfaction, and more. Solving these problems while
sidestepping their inherent computational intractability has great importance and impact for the real
world, where poor bin packing or routing lead to wasted profit or excess greenhouse emissions
(Salimifard et al., 2012). General solving frameworks or metaheuristics for all these problems are
desirable, due to their conceptual simplicity and ease-of-deployment, but require manual tailoring
to each individual problem. One such metaheuristic is Simulated Annealing (SA) (Kirkpatrick et al.,
1987), a simple, and equally very popular, iterative global optimisation technique for numerically approximating the global minimum of both continuous- and discrete-variable problems. While SA has
wide applicability, this is also its Achilles’ Heel, leaving many design choices to the user. Namely, a
user has to design 1) neighbourhood proposal distributions, which define the space of possible transitions from a solution xk at time k to solutions xk+1 at time k + 1, and 2) a temperature schedule,
which determines the balance of exploration to exploitation. In this work, we mitigate the need for
extensive finetuning of SA’s parameters by designing a learnable proposal distribution speeding up
convergence while limiting computational overhead to O(N ) per step for problem size N .

CO algorithms fall into two categories: exact and approximate. Exact methods such as dynamic
programming (Bellman, 1952) and branch-and-bound (Land & Doig, 1960) find global minimisers
**x[∗], but are slow due to the NP-hardness of many CO problems. Approximate algorithms therefore**
seek a good enough solution under practical computational constraints. SA is one such approximate
algorithm. In recent years there has been an explosion of works (Bengio et al., 2018) in machine
learning for CO (ML4CO). In ML4CO, a lot of work has focused on end-to-end neural architectures
(Bello et al., 2016; Vinyals et al., 2017; Dai et al., 2017; Kool et al., 2018; Emami & Ranka, 2018;
Bresson & Laurent, 2021). These work by brute force learning the instance to solution mapping—in
CO these are sometimes referred to as construction heuristics. Other works focus on learning good
parameters for classical algorithms, whether they be parameters of the original algorithm (Kruber
et al., 2017; Bonami et al., 2018) or extra neural parameters introduced into the computational graph
of classical algorithms (Gasse et al., 2019; Gupta et al., 2020; Kool et al., 2021; de O. da Costa
et al., 2020; Wu et al., 2019b; Chen & Tian, 2019; Fu et al., 2021). Our method, neural simulated
_annealing (Neural SA) can be viewed as sitting firmly within this last category._

SA is an improvement heuristic; it navigates the search space of feasible solutions by iteratively applying (small) perturbations to previously found solutions. Figure 1 illustrates this for the Travelling


-----

or

initial solution pick action form proposal accept/reject proposal


Figure 1: Neural SA pipeline for the TSP. Starting with a solution (tour) xk, we sample an action
**a = (i, j) from our learnable policy/proposal distribution, defining start i and end j points of a 2-opt**
move (replacing two old with two new edges). Each pane shows both the linear and graph-based
representations for a tour. From xk and a we form a proposal x[′] which is either accepted or rejected
according to the traditional MH step used in SA. Accepted moves assign xk+1 = x[′]; whereas,
rejected moves assign xk+1 = xk.

Salesperson Problem (TSP), perhaps the most classic of NP-hard problems. In this work, we pose
this as a Reinforcement Learning (RL) agent navigating an environment, searching for better solutions. In this light the proposal distribution is an optimisable quantity. Conveniently, our method
inherits convergence guarantees from SA. We are able to directly optimise the proposal distribution using policy optimisation for both faster convergence and better solution quality under a fixed
computation budget. We demonstrate Neural SA on four tasks: Rosenbrock’s function, a toy 2D
optimisation problem, where we can easily visualise and analyse what is being learnt; the Knapsack
and Bin Packing problems, which are classical NP-hard resource allocation problems; and the TSP.

Our contributions are:

-  We pose simulated annealing as a Markov decision process, bringing it into the realm of
reinforcement learning. This allows us to optimise the proposal distribution in a principled
manner. Our method inherits all the convergence guarantees of vanilla simulated annealing.

-  We demonstrate superior performance to off-the-shelf CO tools on the Knapsack, Bin Packing, and Travelling Salesperson problems, in terms of solution quality and wall-clock time.

-  We show our methods transfer to problems of different sizes, and also perform well on
problems up to 40× larger than the ones used for training.

-  Our method is competitive within the ML4CO space, only using a very lightweight neural
architecture, with number of learnable parameters of the order of 100s or fewer.

2 BACKGROUND AND RELATED WORK

Here we outline the basic simulated annealing algorithm and its main components. Then we provide
an overview of prior works in the machine learning literature which have sought to learn parts of the
algorithm or where SA has found uses in machine learning.

**Combinatorial optimisation** A combinatorial optimisation problem is defined by a triple
(Ψ, X _, E) where ψ ∈_ **Ψ are problem instances (city locations in the TSP), X is the set of feasible**
solutions given ψ (Hamiltonian cycles in the TSP) and E : X × Ψ → R is an energy function (tour
length in the TSP). Without loss of generality, the task is to minimise the energy minx _E(x; ψ)._
_∈X_
CO problems are in general NP-hard, meaning that there is no known algorithm to solve them in
time polynomial in the number of bits that represents a problem instance.

**Simulated Annealing** Simulated annealing (Kirkpatrick et al., 1987) is a metaheuristic for CO
problems. It builds an inhomogeneous Markov chain x0 **x1** **x2** for xk, asymptotically converging to a minimizer ofquantities: 1) a proposal distribution, and 2) a temperature schedule. The proposal distribution E. The stochastic transitions → _→_ **x →· · ·k →** **xk+1 depend on two ∈X**
_π : X →_ P(X ), for P(X ) the space of probability distributions on X, suggests new states in the
chain. It perturbs current solutions to new ones, potentially leading to lower energies immediately
or later on. After perturbing a solution xk **x[′], a Metropolis–Hastings (MH) step (Metropo-**
lis et al., 1953; Hastings, 1970) is executed. This either accepts the perturbation ( → **xk+1 = x[′]) or**


-----

rejects it (xk+1 = xk)—see Algorithm 1 for details. The target distribution of the MH step has
this distribution tends to a sum of Dirac deltas on the minimisers of the energy. The temperatureform p(x|Tk) ∝ exp{−E(x)/Tk}, where Tk is the temperature at time k. In the limit Tk → 0,
is annealed, according to the temperature schedule, T1, T2, ..., from high to low, to steer the target
distribution smoothly from broad to peaked around the global optima. The algorithm is outlined in
Algorithm 1. Under certain regularity conditions and provided the chain is long enough, it will visit
the minimisers almost surely (Geman & Geman, 1984). More concretely,

lim **xk** arg min _E(x; ψ)_ = 1. (1)
_k→∞_ _[P]_  _∈_ **x∈X** 

Despite this guarantee, practical convergence speed is determined by π and the temperature schedule, which are hard to fine-tune. There exist problem-specific heuristics for setting these (Pereira &
Fernandes, 2004; Cicirello, 2007), but in this paper we propose to learn the proposal distribution.

2.1 SIMULATED ANNEALING AND MACHINE LEARNING

A natural way to combine machine learning and simulated annealing is to design local improvement
heuristics that feed off each other. Notable examples are Cai et al. (2019) and Vashisht et al. (2020)
where RL is used to find good initial solutions later refined by standard SA. The two methods only
interact via shared solutions, fundamentally different to our approach, as we focus on augmenting
SA with RL optimisable components. In fact, our method is perfectly compatible with theirs and
any other SA application. Other works proposed the optimising components of SA. In particular,
Blum et al. (2020) study the design of optimal cooling schedules using data, deriving theoretical
guarantees on sample complexity. However, the authors do not provide empirical evaluations and
focus exclusively on the cooling schedule while we concentrate on the proposal distribution. Similarly, Wauters et al. (2020) and Khairy et al. (2020) use RL to find optimal parameters for quantum
variants of SA but their methods are not readily applicable to standard SA.

More closely to our method, other approaches improve the proposal distribution. In Adaptive Simulated Annealing (ASA) (Ingber, 1996) the proposal distribution is not fixed but evolves throughout
annealing process as a function of the variance of observed solution fitness. ASA improves the
convergence of standard SA but is not learnable like Neural SA. To the best of our knowledge, Marcos Alvarez et al. (2012) are the only others to learn the proposal distribution for SA, but they rely
on supervised learning, requiring high quality solutions or good search strategies to imitate, both
expensive to compute. In contrast, Neural SA is fully unsupervised, thus easier to train and extend
to different CO tasks. Finally, many Monte Carlo methods use a a proposal distribution. No´e et al.
(2019) and Albergo et al. (2019) recently studied how to learn a proposal distribution of an MCMC
chain for sampling the Boltzmann distribution of a physical system. While their results serve as
motivation for our methods, we investigate a completely different context and set of applications.

Our work falls under bi-level optimisation methods, where an outer optimisation loop finds the best
parameters of an inner optimisation. This encompasses situations such as learning the parameters (Rere et al., 2015) or hyperparameters of a neural network optimiser (Maclaurin et al., 2015;
Andrychowicz et al., 2016) and meta-learning (Finn et al., 2017). However, most recent approaches
assume differentiable losses on continuous state spaces Likhosherstov et al. (2021); Ji et al. (2021);
Vicol et al. (2021), while we focus on the more challenging CO setting. We note, however, methods
in Vicol et al. (2021) are based on evolution strategies and could be used in the discrete setting.

2.2 MARKOV DECISION PROCESSES

Simulated annealing naturally fits into the Markov Decision Process (MDP) framework as we explain below. An MDP M = (S, A, R, P, γ) consists of states s ∈S, actions a ∈A, an immediate
_reward function R : S × A × S →_ R, a transition kernel P : S × A → P(S), and a discount factor
_γ ∈_ [0, 1]. On top of this MDP we add a stochastic policy π : S → P(A). The policy and transition
kernel together define a length-K trajectory τ = (s0, a0, s1, a1, ..., sK), which is a sample from the
distribution P (τ _|π) = ρ0(s0)_ _k=0_ _[P]_ [(][s][k][+1][|][s][k][, a][k][)][π][(][a][k][|][s][k][)][ and where][ s][0][ ∼] _[ρ][0][ is sampled from]_
the start-state distribution ρ0. One can then define the discounted return R(τ ) = _k=0_ _[γ][t][r][k][ over]_
a trajectory, where rk = R(sk[Q], a[K]k, s[−][1]k+1). We say that we have solved an MDP if we have found a
policy that maximises the expected return Eτ _P (τ_ _π)[R(τ_ )].
_∼_ _|_ [P][K][−][1]


-----

**Algorithm 1 Neural simulated annealing. To get back to vanilla SA, replace the parametrised pro-**
posal distribution πθ with a uniform distribution π over neighbourhoods N (•).

**Require: Initial state s0 = (x0, ψ, T0), proposal distribution π, transition function P**, temperature
schedule T1 _T2_ _T3_ _..., energy function E(_ ; ψ)
**for k = 1 : K ≥ do** _≥_ _≥_ _•_

**a** _πθ(sk)_ _▷_ Sample action
_∼_

_u ∼_ Uniform(u; 0, 1) _▷_ Metropolis–Hastings step

**if u < exp {−(E(x[′]; ψ) −** _E(xk; ψ))/Tk} then_

**sk+1** (x[′], ψ, Tk+1) _▷_ Accept
_←_

**else**

**sk+1** (xk, ψ, Tk+1) _▷_ Reject
_←_

**end if**

**end for**

3 METHOD

Here we outline our approach to learn the proposal distribution. First we define an MDP corresponding to SA. We then show how the proposal distribution can be optimised and provide a justification
that this does not affect convergence guarantees of the classical algorithm.

3.1 MDP FORMULATION

We formalise SA as an MDP, defining states s = (x, ψ, T ) ∈S for ψ a parametric description
of the problem instance as in Section 2, and T the instantaneous temperature. Examples are in
Section 4. Our actions a ∈A perturb (x, ψ, T ) 7→ (x[′], ψ, T ), where x[′] _∈N_ (x) is a solution in the
neighbourhood of x. It is common to define small neighbourhoods, to limit energy variation from
one state to the next. This heuristic discards exceptionally good and exceptionally bad moves, but
given that bad moves are commoner than good ones, it generally leads to faster convergence.

The MH step in SA can be viewed as a stochastic transition kernel, governed by the current temperature of the system, with transition probabilities following a Gibbs distribution and dynamics

**x[′],** with probability p
**xk+1 =** where p = min 1, e[−] _Tk[1]_ [(][E][(][x][′][;][ψ][)][−][E][(][x][k][;][ψ][))][o] _._ (2)
**xk,** with probability 1 _p,_
 _−_ n

This defines a transition kernel P (sk+1|sk, a), where sk+1 = (x[′], ψ, T ) or sk+1 = (xk, ψ, T ),
the two outcomes of the MH step. For rewards, we use either the immediate gain E(xk; ψ)
_−_
_E(xk+1; ψ) or the primal reward_ _δk=K_ 1 minx **x1:k E(x; ψ). We explored training with two dif-**
_−_ _−_ _∈_
ferent methods: Proximal Policy Optimisation (PPO) (Schulman et al., 2017) and Evolution Strategies (ES) Salimans et al. (2017). The immediate gain works best with PPO, where at each iteration
of the rollout, the immediate gain gives fine-grained feedback on whether the previous action helped
or not. The primal reward works best with ES, because it is non-local, returning the minimum along
an entire rollout τ at the very end. We explored using the acceptance count but found that this
sometimes led to pathological behaviours. We also tried the primal integral (Berthold, 2013), which
encourages finding a good solution fast, but found we could not get training dynamics to converge.

3.2 POLICY NETWORK ARCHITECTURE

SA chains are long. It is because of this that we need as lightweight a policy architecture as possible.
Furthermore, this architecture should have the capacity to scale to varying numbers of inputs, so that
we can transfer experience across problems of different size N . We opt for a very simple network,
shown in Figure 2. For each dimension of the problem we map the state (x, ψ, T ) into a set of
features. For all problems we try, there is a natural way to do this. Each feature is fed into an
MLP, embedding it into a logit space, followed by a softmax function to yield probabilities. The
complexity of this architecture scales linearly with N, which is important since we plan to evaluate
it many times. A notable property of this architecture is that is is permutation equivariant (Zaheer
et al., 2017), an important requirement for CO problems. Note that our model is a permutation
equivariant set-to-set mapping, but we have not used attention or other kinds of pairwise interaction
to keep the computational complexity linear in the number of items.


-----

**Convergence** Convergence of SA to the optimum in the infinite time limit requires the
Markov chain of the proposal distribution to be
irreducible (van Laarhoven & Aarts, 1987, Thm
6, Chap 3), meaning that for any temperature,
any two states are reachable through a sequence
of transitions with positive conditional probability under π. Our neural network policy satisfies this condition as long as the softmax layer
does not assign zero probability to any state, a
condition which is met in practice. Thus Neural
SA inherits convergence guarantees from SA.

4 EXPERIMENTS

|softmax|Col2|Col3|
|---|---|---|
||||


N items **softmax**

inputs MLP logits policy


Figure 2: (a) Policy network: The same MLP is
applied to all inputs pointwise. We use this network in all CO problems.


We evaluate our method on 4 tasks—Rosenbrock’s function, the Knapsack, Bin Packing, and Travelling Salesperson problems—fixing the architecture and overall hyperparameters of Neural SA.
This shows the wide applicability and ease of use of our method. For each task (except for Rosenbrock’s function) we test Neural SA on problems of different size N, training only on the smallest.
Similarly, we consider rollouts of different lengths, training only on short ones. This accelerates
training, showing Neural SA’s generalisation capabilities. This type of transfer learning is rare in
ML4CO, and is a merit of our lightweight, equivariant architecture. In all experiments, we adopt
an exponential multiplicative cooling schedule as originally proposed by Kirkpatrick et al. (1987),
with Tk = α[k]T0. α is computed for set T0 and TK. This allows us to vary the rollout length while
maintaining the same range of temperatures for every run. Exact training details are in the Appendix.

4.1 THE ROSENBROCK FUNCTION


The Rosenbrock function is non-convex over Euclidean space. Of course, gradient-based optimisers
are more suited to this problem, but we use it as toy example to showcase the properties of Neural
SA. Our policy is an axis-aligned Gaussian πθ(a|s) = N (a; 0, σθ[2][(][s][i][))][, with MLP-parameterised]
variance σθ[2] [of shape][ 2][ →] [16][ →] [2][ and a ReLU in the middle. Proposals are][ x][′][ =][ x][ +][ a][, and the]
state is sk = (xk, a, b, Ti). An example rollout is in Figure 3a. Mathematically the function is

_E(x0, x1; a, b) = (a −_ _x0)[2]_ + b(x1 − _x[2]0[)][2][,]_ (3)

which has global minimum at x = (a, a[2]). We contrast Neural SA against vanilla SA with fixed
proposal distribution, i.e. σ(si) = σ, for different σ averaged over 2[17] problem instances. This
shows in Figure 3d that no constant variance policy can outperform an adaptive policy on this problem. Plots of acceptance ratio in Figure 3b show Neural SA has higher acceptance probability early
in the rollout, a trend we observed in all experiments, suggesting its proposals are skewed towards
lower energy solutions than standard SA. Figure 3c shows the variance network σθ[2][(][s][i][)][ as a function]
of time. It has learnt to make large steps until hitting the basin, whereupon large moves will be
rejected with high probability, so variance must be reduced.


1.0

0.8

0.6

0.4

0.2

0.0


10

10

10

10

10


10 3 10 2 10 1 10[0] 10[1]

|2|Col2|Col3|Col4|Col5|Vanilla SA|
|---|---|---|---|---|---|
|02 01 00|||||Neural S|
|||||||
|||||||
|1||||||
|2||||||


Vanilla SA
Neural SA

3 2 1 [0] [1]

Standard deviation

(d) Baseline sweep


200 400Iteration600 800 1000

|Col1|Col2|Col3|N V|eural SA anilla SA|
|---|---|---|---|---|
||||||
||||||
||||||


Neural SA
Vanilla SA


(a) 2D rollout


(b) Acceptance ratios (c) σ by iteration


Figure 3: Results on Rosenbrock’s function: (a) Example trajectory, moving from red to blue, showing convergence around the minimiser at (1,1) (b) Neural SA has higher acceptance ratio than the
baseline, a trend observed in all experiments, (c) Standard deviation of the learned policy as a function of iteration. Large initial steps offer great gains followed by small exploitative steps, (d) A
non-adaptive vanilla SA baseline cannot match an adaptive one, no matter the standard deviation.


-----

|Col1|6 4 2 0|
|---|---|
|||
|||
|||


0

10

20

30

Knapsack state

40

0 200 400 600 800

Iteration


1.0 xi = 0 xi = 1

6

0.8 4

2

0.6

0

Value

0.4

2

0.2 4

6

0.0

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

Weights Weights


(a) Solution per annealing step.


(b) KnapNet Policy (logit space)


Figure 4: (a) Exemplar knapsack rollout on KNAP50: Each row is an item and each column is a
subsequent iteration. Items absent from the knapsack are in black and those present are in white.
Items are ordered by value-to-weight ratio in ascending order top to bottom. We see that light, valuable objects are held in the knapsack longer than heavy, valueless ones. (b) Knapsack logits: Policy
logits for xi = 0 and xi = 1 are shown in each pane. Light valuable objects are favoured to insert.
Once inserted the policy downweights an object’s probably of flipping state again. Interestingly, the
ejection probability of heavy, valueless objects is low, perhaps because this only makes sense close
to overflowing, but the policy does not receive free capacity as a feature.

4.2 KNAPSACK PROBLEM


The Knapsack problem is a classic CO problem in resource allocation. Given a set of N items, each
of a different value vi > 0 and weight wi > 0, the goal is to find a subset that maximises the sum of
values while respecting a limit W on its total weight. This has corresponding integer linear program


_N_ _−1_

_vixi,_ subject to
_i=0_

X


_N_ _−1_

_wixi_ _W,_ _xi_ 0, 1 _._ (4)
_i=0_ _≤_ _∈{_ _}_

X


minimise E(x; ψ) = −


Solutions are represented as a binary vector x, with xi = 0 for ‘out of the bin’ and xi = 1 for
‘in the bin’. Our proposal distribution flips individual bits, one at a time, with the constraint that
we cannot flip 0 7→ 1 if the bin capacity will be exceeded. The neighbourhood of xk is thus all
feasible solutions at a Hamming distance of 1 from xk. We use the proposal distribution described
in Section 3.2 and illustrated in Figure 2, consisting of a pointwise embedding of each item—its
weight, value, occupancy bit, the knapsack’s overall capacity, and global temperature—into a logitspace, followed by a softmax. Mathematically the policy and state–action to proposal mapping are

_πθ(i|s) = softmax (z)i,_ _zi = fθ([xi, wi, vi, W, T_ ]) (5)

**x[′]** = x + onehot(i) mod 2. (6)


where fθ is a very small two-layer neural network 5 → 16 → 1 with a ReLU nonlinearity between
the two layers. Actions are sampled from the categorical distribution induced by the softmax, and
cast to one-hot vectors onehot(i).

Neural networks have been used to solve the Knapsack Problem in Vinyals et al. (2017), Nomer
et al. (2020), and Bello et al. (2016). We follow the setup of Bello et al. (2016), honing in on 3

Table 1: Average cost of solutions for the Knapsack Problem across five random seeds and, in
parentheses, optimality gap to best solution found among solvers. Bigger is better. *Values as
reported by Bello et al. (2016) for reference.

|Col1|Random Search Bello RL Bello AS|SA Ours (PPO) Ours (ES)|Greedy OR-Tools|
|---|---|---|---|
|KNAP50 KNAP100 KNAP200 KNAP500 KNAP1K KNAP2K|17.91∗ 19.86∗ 20.07∗ 33.23∗ 40.27∗ 40.50∗ 35.95∗ 57.10∗ 57.45∗ - - - - - - - -|18.90 (5.82%) 19.87 (0.99%) 19.95 (0.65%) 36.75 (9.26%) 39.92 (1.43%) 39.90 (1.48%) 48.88 (14.91%) 55.65 (3.13%) 55.58 (3.25%) 126.94 (11.93%) 139.66 (3.10%) 141.01 (2.17%) 254.44 (11.96%) 280.39 (2.98%) 282.46 (2.26%) 507.83 (12.03%) 560.40 (2.92%) 563.75(2.34%)|19.94 (0.60%) 20.12 (0.00%) 40.17 (0.81%) 40.41 (0.00%) 57.30 (0.26%) 57.65 (0.00%) 143.77 (0.25%) 144.14 (0.00%) 288.64 (0.13%) 289.01 (0.00%) 576.89 (0.06%) 577.28 (0.00%)|


-----

50

45

40

35

30

25

20

|Col1|Col2|Col3|Col4|Neural SA Vanilla SA|Col6|
|---|---|---|---|---|---|
|||||||
|||||Greedy N|eural SA|
|||||||
|||||||

|Col1|3 2 1 0|
|---|---|
|||
|||


0 100 200 300 400 500

Neural SA
Vanilla SA
Greedy Neural SA

Iteration

(a) Bin Packing primal objective


Item selector Bin selector

1.0

30

0.8 20

10

0.6 0

10

0.4

20

Source used capacity0.2 arget used capacityT 30

40

0.0

0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0

Item weight Item weight


(b) Bin-Packing policy (logits)


Figure 5: (a) Plot of the BIN50 primal objective for vanilla, Neural, and Greedy Neural SA. 25[th],
50[th], and 75[th] percentiles shown: Greedy and Neural SA have faster convergence than vanilla SA.
Prolonged sampling improves over the greedy version. (b) Policy for the Bin Packing Problem: The
bin packing policy consists of two networks, an item selector and a bin selector. The item selector
uses item weight and bin used capacities to select an item to move. The bin selector then places this
item in a bin, based on target bin fullness and selected item weight. The learnt policy is very sensible.
The item selector looks for a light item in an under-full bin. The bin selector then place sthis in an
almost-full bin. We mask bins with insufficient free capacity, hence the triangular logit-spaces.

self-generated datasets: KNAP50, KNAP100 and KNAP200. KNAPN consists of N items with
weights and values generated uniformly at random in (0, 1] and capacities C50 = 12.5, C100 = 25,
and C200 = 25. We use OR-Tools (Perron & Furnon) to compute groundtruth solutions. Results in
Table 1, show that Neural SA improves over vanilla SA by up to 10% optimality gap, and heuristic
methods (Random Search) by much more. Neural SA falls slightly behind two methods by Bello
et al. (2016), which use (1) a large attention-based pointer network with several orders of magnitude more parameters in Bello RL, and (2) this coupled with 5000 iterations of their Active Search
method. It also falls behind a greedy heuristic for packing a knapsack based on the value-to-weight
ratio. In Figure 4 we analyse the policy network and a typical rollout. It has learnt a mostly greedy
policy to fill its knapsack with light, valuable objects, only ejecting them when full. This is line with
the value-to-weight greedy heuristic. Despite not coming top among methods, we note Neural SA
is typically within 1 - 3 % of the minimum energy, while not designed for this problem in particular.


4.3 BIN PACKING PROBLEM

The Bin Packing problem is similar to the Knapsack problem in nature. Here, one wants to pack all
of N items into the smallest number of bins possible, where each item i ∈{1, · · ·, N _} has weight_
_wi, and we assume, without loss of generality, N bins of equal capacity W_ maxi(wi)—there
_≥_
would be no valid solution otherwise. If xij denotes item i occupying bin j, then the problem can
be written as minimising an energy


minimise E(x; ψ) =


_yj,_ (7)
_j=1_

X


_N_ _−1_ _N_ _−1_ _N_ _−1_

subject to _i=0_ _wixij ≤_ _W_ _,_ _j=0_ _xij = 1,_ _yj = min_ 1, _i=0_ _xij,_ !, _xij ∈{0, 1}_ (8)

X X X

bin capacity constraint 1 bin per item bin occupancy indicator

where the constraints apply for all| {z } _i| and{z j. We define the policy in two steps: we first pick an item}_ | {z }
_i, and then select a bin j to place it into. We can then write πθ,φ(a = (i, j)_ **s) = πφ(i** **s)πθ(j** **s, i),**
_|_ _|_ _|_
which we parametrise as

_πθ(i|s) = softmax_ **z[item][]i** _[,]_ _zi[item]_ = fθ([wi, cb(i), T ]), (9)

_πφ(j|s, i) = softmax _ **z[bin][]j** _[,]_ _zj[bin]_ = fφ([wi, cj, T ]), (10)
 


-----

Table 2: Average cost of solutions for the Bin Packing Problem across five random seeds and, in
parentheses, optimality gap to best solution found among solvers. Lower is better. We set a time out
of 1 minute per problem for Or-Tools; * indicates only the trivial solution was found in this time.

|SA Ours (PPO) Ours (ES)|OR-Tools (SCIP) FFD|
|---|---|
|BIN50 29.48 (10.37%) 27.29 (2.17%) 27.24 (1.98%) BIN100 60.03 (13.46%) 53.47 (1.06%) 53.38 (1.29%) BIN200 121.21 (16.27%) 105.63 (1.32%) 105.43 (1.13%) BIN500 302.79 (17.81%) 259.08 (0.80%) 260.27 (1.26%) BIN1000 605.09 (18.77%) 512.66 (0.63%) 516.84 (1.45%) BIN2000 1209.48 (18.82%) 1017.88 (0.00%) 1028.67 (1.06%)|26.71 (0.00%) 27.10 (1.46%) 53.91 (1.89%) 52.91 (0.00%) 109.19 (4.74%) 104.25 (0.00%) 267.63 (4.13%) 257.02 (0.00%) 1000∗ 509.46 (0.00%) 2000∗ 1028.67 (1.06%)|



where b(i) is the bin item i is in before the action (in terms of xij, we have xib(i) = 1), cj is the free
capacity of bin j (cj = W − [P]i[N]=1 _[w][i][x][ij][), and both][ f][θ][ and][ f][φ][ are lightweight architectures][ 3][ →]_
16 → 1 with a ReLU nonlinearity between the two layers. We sample from the policy ancestrally,
sampling first an item from πθ(i **s), followed by a bin from πφ(j** **s, i). Results in Table 2 show that**
_|_ _|_
our lightweight model is able to find a solution to about 1% higher energy than the minimum found
by FFD Johnson (1973), a very strong heuristic for this problem (Rieck, 2021). We even see that
we very often beat the SCIP (Gamrath et al., 2020a;b) optimizer in OR-Tools, which timed out on
most problems. Figure 5a compared convergence speed of Neural SA with vanilla SA and a third
option, Greedy Neural SA, which uses argmax samples from the policy. The learnt policy, visualised
in Figure 5b has much faster convergence than the vanilla version. Again, we see that our method,
although simple, is competitive with hand-design alternatives.

4.4 TRAVELLING SALESPERSON PROBLEM

Imagine you will make a round road-trip through N cities and want to plan the shortest route visiting each city once; this is the Travelling Salesperson Problem (TSP) (Applegate et al., 2006).
The TSP has been a long time favourite of computer scientists due to its easy description and
NP-hardness. Here we use it as an example of a difficult CO problem. We compare with Concorde (Applegate et al., 2006) and LKH-3 (Helsgaun, 2000), two custom solvers for TSP. Given
cities i 0, 1, ..., N 1 with spatial coordinates ci [0, 1][2], we wish to find a linear ordering of the cities, called a ∈{ _−_ _} tour, denoted by the permutation vector ∈_ **x = (x0, x1, ..., xN** _−1) for_
_xi_ 0, 1, ..., N 1 such that
_∈{_ _−_ _}_


_N_ _−1_

**cxi+1** **cxi** 2 (11)
_i=0_ _∥_ _−_ _∥_

X


minimise E(x; ψ) =


subject to xi ̸= xj for all i ̸= j and xi ∈{0, 1, ..., N − 1}, (12)

where we have defined xN = x0 for convenience of notation. Our action space consists of so-called
_2-opt moves (Croes, 1958), which reverse contiguous segments of a tour. An example of a 2-opt_
move is shown in Figure 1. We have a two-stage architecture, like in Bin Packing, which selects the

Table 3: Comparison of Neural SA against different solvers and deep learning models on TSP.
Extended version of this table is provided in the appendix. Lower is better.

|Col1|TSP20 Cost Gap Time|TSP50 Cost Gap Time|TSP100 Cost Gap Time|TSP200 Cost Gap Time|TSP500 Cost Gap Time|
|---|---|---|---|---|---|
|CONCORDE LKH-3|3.836 0.00% 48s 3.836 0.00% 1m|5.696 0.00% 2m 5.696 0.00% 14m|7.764 0.00% 7m 7.764 0.00% 1h|10.70 0.00% 38m 10.70 0.00% 21m|16.54 0.00% 7h58m 16.54 0.00% 1h15m|
|SA OURS (PPO) OURS (ES)|3.881 1.17% 10s 3.852 0.42% 17s 3.840 0.10% 17s|5.943 4.34% 1m 5.762 1.16% 2m 5.828 2.32% 2m|8.341 7.43% 6m 7.907 1.85% 14m 8.191 5.50% 14m|11.98 11.96% 30m 11.04 3.27% 32m 11.74 9.72% 32m|20.22 22.25% 2h35m 17.87 8.04% 5h13m 20.27 22.55% 5h13m|
|GAT-T{1000} Costa{500}|3.84 0.03% 12m 3.84 0.01% 5m|5.75 0.83% 16m 5.72 0.36% 7m|8.01 3.24% 25m 7.91 1.84% 10m|- - - - - -|- - - - - -|


-----

start and end cities of the segment to reverse. Denoting i as the start and j as the end

_πθ,φ(a = (i, j)_ **s) = πφ(i** **s)πθ(j** **s, i)** (13)
_|_ _|_ _|_
_πθ(i|s) = softmax (z)i,_ _zi = fθ([cx[i−1:i+1]_ _, T_ ]) (14)

_πφ(j|s, i) = softmax (z)j,_ _zj = fφ([cx[i−1:i+1]_ _, cx[j−1:j+1]_ _, T_ ]), (15)

where x[i 1:i+1] are the indices of city i and its tour neighbours i 1 and i + 1. WE use the
_−_ _−_
standard policy: fθ has architecture 7 → 16 → 1 and fφ, 13 → 16 → 1. We test on publically
available TSP20/50/100 (Kool et al., 2018) with 10K problems each and generate TSP200/500 with
1K tours each. Results, in Table 11, show Neural SA improves on vanilla SA. Neural improvement
heuristics methods, GAT-T{1000} (Wu et al., 2019b) and Costa{500} (de O. da Costa et al., 2020),
designed for routing problems, outperform us for small TSPs, but are neck-and-neck for TSP100.
Given Neural SA is not custom designed for TSPs, we view this as surprisingly good.


5 DISCUSSION

Neural SA is a general, plug-and-play method, requiring only definition of neighbourhoods and
training problem instances. Our lightweight architecture shared for all problems we consider and,
with a maximum of 384 parameters on TSP, 160 for Bin Packing, and 112 for Knapsack can achieve
within a few percentage points or less of global minimia. It can be trained with any policy optimisation method making it highly extendable. We found no winner between PPO and ES, apart from
on the TSP, where PPO excelled. We also observed PPO to converge ∼ 10× faster than ES, but ES
policies were more robust, such as when we switched to greedy sampling. Here the PPO policy was
usually hurt, but ES was not. An interesting observation we made was that the acceptance rate over
trajectories was problem dependent, always higher in Neural SA than vanilla SA. This contradicts
conventional wisdom that it should be held at 0.44 throughout a rollout (Lam & Delosme, 1988).

While never achieving SOTA, this was never to be expected, given SA is an approximation metaheuristic, but if left running long enough, results are guaranteed to improve, following from the
convergence guarantees of SA. It is also worth pointing out we did little to no fine-tuning of the
hyperparameters of SA, and yet achieved positive results even with an unsophisticated temperature
cooling schedule. This demonstrates that besides improving convergence speeds, Neural SA is also
more robust to variations in the optimisation dynamics of SA. In fact, our experiments show Neural
SA generalises across different problem sizes and rollout lengths; a truly remarkable feat, as transfer learning is notoriously difficult in reinforcement learning and combinatorial optimisation. That
makes Neural SA an attractive alternative in many applications where problem sizes and time constraints fluctuate, especially when computational power or energy consumption are limiting factors.

6 CONCLUSION

We presented Neural SA, neurally augmented simulated annealing, where the SA chain is a trajectory from an MDP. In this light, the proposal distribution could be interpreted as a policy, which
could be optmised. This has numerous benefits: 1) accelerated convergence of the chain, 2) ability to condition the proposal distribution on side-information 3) we do not need groundtruth data
to learn the proposal distribution, 4) our architectures are very lightweight and can be run on CPU
unlike many contemporary ML4CO methods, 5) the method scales well due to its lightweight computational overhead, 6) we can train on small problems and generalise to much larger ones. Of
these contributions, we believe 6 is the most impressive. Many ML4CO solutions currently use
huge transformer-based models, with computation scaling quadratically per step with problem size;
whereas, neural SA instead scales linearly. This enabled us to yield results on larger problem sizes.

Removing pairwise interactions in the model, however, could be its limiting factor. In all experiments we were not able to achieve the minimum energy, but could usually get within a percentage
point. The model also has no in-built termination condition, neither can it provide a certificate on
the quality of solutions found. There is still also the question of how to tune the temperature schedule, which we did not attempt in this work. These shortcomings are all points to be addressed in
upcoming research. We are also interested in extending the framework to multiple trajectories, such
as in parallel tempering Swendsen & Wang (1986) or genetic algorithms Holland (1992). For these,
we would maintain a population of chains, which could exchange information.


-----

7 REPRODUCIBILITY STATEMENT

Our method is outlined in Algorithm 1. In sections 4.1, 4.2, 4.3, and 4.4 we provide details of
the exact models we use and the datasets. Each dataset was generated synthetically, apart from the
TSP20, 50, and 100 datasets, which are publically available. We have reported all our numbers, with
standard deviations, and provide precise training details in the appendix. As a precaution, we plan
to release our code upon acceptance of this paper, for complete transparency.

8 ETHICS STATEMENT

Solving real world combinatorial optimisation problems can reduce waste, allocating scarse resources near optimally. This paper in particular introduces a lightweight method to improve upon
simulated annealing, a popular combinatorial optimisation method, such that it can be targeted at a
broad set of problems with ease. Where simulated annealing itself is deployed, for moral or likewise
immoral purposes, is beyond the scope of this paper, but based on typical examples of combinatorial
optimisation problems, our advances probably cause little downstream harm. The improvements in
convergence speed we observed also reduce total FLOPs and therefore carbon footprint, a common
problem with large-scale optimisations and deep learning models.

REFERENCES

MS Albergo, G Kanwar, and PE Shanahan. Flow-based generative models for markov chain monte
carlo in lattice field theory. Physical Review D, 100(3):034515, 2019.

Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,
Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient
descent. In Advances in neural information processing systems, pp. 3981–3989, 2016.

David L Applegate, Robert E Bixby, Vasek Chv´atal, and William J Cook. The Traveling Salesman
_Problem: A Computational Study. Princeton University Press, 2006._

Richard Bellman. On the theory of dynamic programming. Proceedings of the National Academy
_[of Sciences, 38(8):716–719, 1952. ISSN 0027-8424. doi: 10.1073/pnas.38.8.716. URL https:](https://www.pnas.org/content/38/8/716)_
[//www.pnas.org/content/38/8/716.](https://www.pnas.org/content/38/8/716)

Irwan Bello, Hieu Pham, Quoc V. Le, Mohammad Norouzi, and Samy Bengio. Neural combina[torial optimization with reinforcement learning. CoRR, abs/1611.09940, 2016. URL http:](http://arxiv.org/abs/1611.09940)
[//arxiv.org/abs/1611.09940.](http://arxiv.org/abs/1611.09940)

Yoshua Bengio, Andrea Lodi, and Antoine Prouvost. Machine learning for combinatorial optimization: a methodological tour d’horizon. _CoRR, abs/1811.06128, 2018._ [URL http:](http://arxiv.org/abs/1811.06128)
[//arxiv.org/abs/1811.06128.](http://arxiv.org/abs/1811.06128)

Timo Berthold. Measuring the impact of primal heuristics. Operations Research Letters, 41(6):
[611–614, 2013. ISSN 0167-6377. doi: https://doi.org/10.1016/j.orl.2013.08.007. URL https:](https://www.sciencedirect.com/science/article/pii/S0167637713001181)
[//www.sciencedirect.com/science/article/pii/S0167637713001181.](https://www.sciencedirect.com/science/article/pii/S0167637713001181)

Avrim Blum, Chen Dan, and Saeed Seddighin. Learning complexity of simulated annealing, 2020.

Pierre Bonami, Andrea Lodi, and Giulia Zarpellon. Learning a classification of mixed-integer
quadratic programming problems. In Willem-Jan van Hoeve (ed.), Integration of Constraint Pro_gramming, Artificial Intelligence, and Operations Research, pp. 595–604, Cham, 2018. Springer_
International Publishing. ISBN 978-3-319-93031-2.

Xavier Bresson and Thomas Laurent. The transformer network for the traveling salesman problem.
_arXiv preprint arXiv:2103.03012, 2021._

Qingpeng Cai, Will Hang, Azalia Mirhoseini, George Tucker, Jingtao Wang, and Wei Wei. Reinforcement learning driven heuristic optimization, 2019.


-----

Xinyun Chen and Yuandong Tian. Learning to perform local rewriting for combinatorial optimization. Advances in Neural Information Processing Systems, 32:6281–6292, 2019.

Vincent A Cicirello. On the design of an adaptive simulated annealing algorithm. In Proceedings of
_the international conference on principles and practice of constraint programming first workshop_
_on autonomous search, 2007._

G. A. Croes. A method for solving traveling-salesman problems. Operations Research, 6:791–812,
1958.

Hanjun Dai, Elias B. Khalil, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning combinatorial
[optimization algorithms over graphs. CoRR, abs/1704.01665, 2017. URL http://arxiv.](http://arxiv.org/abs/1704.01665)
[org/abs/1704.01665.](http://arxiv.org/abs/1704.01665)

Paulo R. de O. da Costa, Jason Rhuggenaath, Yingqian Zhang, and Alp Akcay. Learning 2opt heuristics for the traveling salesman problem via deep reinforcement learning. _CoRR,_
[abs/2004.01608, 2020. URL https://arxiv.org/abs/2004.01608.](https://arxiv.org/abs/2004.01608)

Patrick Emami and Sanjay Ranka. Learning permutations with sinkhorn policy gradient. CoRR,
[abs/1805.07010, 2018. URL http://arxiv.org/abs/1805.07010.](http://arxiv.org/abs/1805.07010)

Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In International Conference on Machine Learning, pp. 1126–1135. PMLR,
2017.

Zhang-Hua Fu, Kai-Bin Qiu, and Hongyuan Zha. Generalize a small pre-trained model to arbitrarily
large tsp instances. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35,
pp. 7474–7482, 2021.

Gerald Gamrath, Daniel Anderson, Ksenia Bestuzheva, Wei-Kun Chen, Leon Eifler, Maxime Gasse,
Patrick Gemander, Ambros Gleixner, Leona Gottwald, Katrin Halbig, Gregor Hendel, Christopher Hojny, Thorsten Koch, Pierre Le Bodic, Stephen J. Maher, Frederic Matter, Matthias Miltenberger, Erik M¨uhmer, Benjamin M¨uller, Marc E. Pfetsch, Franziska Schl¨osser, Felipe Serrano,
Yuji Shinano, Christine Tawfik, Stefan Vigerske, Fabian Wegscheider, Dieter Weninger, and Jakob
Witzig. The SCIP Optimization Suite 7.0. Technical report, Optimization Online, March 2020a.
[URL http://www.optimization-online.org/DB_HTML/2020/03/7705.html.](http://www.optimization-online.org/DB_HTML/2020/03/7705.html)

Gerald Gamrath, Daniel Anderson, Ksenia Bestuzheva, Wei-Kun Chen, Leon Eifler, Maxime Gasse,
Patrick Gemander, Ambros Gleixner, Leona Gottwald, Katrin Halbig, Gregor Hendel, Christopher Hojny, Thorsten Koch, Pierre Le Bodic, Stephen J. Maher, Frederic Matter, Matthias Miltenberger, Erik M¨uhmer, Benjamin M¨uller, Marc E. Pfetsch, Franziska Schl¨osser, Felipe Serrano,
Yuji Shinano, Christine Tawfik, Stefan Vigerske, Fabian Wegscheider, Dieter Weninger, and Jakob
Witzig. The SCIP Optimization Suite 7.0. ZIB-Report 20-10, Zuse Institute Berlin, March 2020b.
[URL http://nbn-resolving.de/urn:nbn:de:0297-zib-78023.](http://nbn-resolving.de/urn:nbn:de:0297-zib-78023)

Maxime Gasse, Didier Chetelat, Nicola Ferroni, Laurent Charlin, and Andrea Lodi. Exact combinatorial optimization with graph convolutional neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch´e-Buc, E. Fox, and R. Garnett (eds.),
_Advances in Neural Information Processing Systems,_ volume 32. Curran Associates,
Inc., 2019. URL [https://proceedings.neurips.cc/paper/2019/file/](https://proceedings.neurips.cc/paper/2019/file/d14c2267d848abeb81fd590f371d39bd-Paper.pdf)
[d14c2267d848abeb81fd590f371d39bd-Paper.pdf.](https://proceedings.neurips.cc/paper/2019/file/d14c2267d848abeb81fd590f371d39bd-Paper.pdf)

Stuart Geman and Donald Geman. Stochastic relaxation, gibbs distributions, and the bayesian
restoration of images. _IEEE Transactions on pattern analysis and machine intelligence, (6):_
721–741, 1984.

Prateek Gupta, Maxime Gasse, Elias B. Khalil, M. Pawan Kumar, Andrea Lodi, and Yoshua Bengio.
[Hybrid models for learning to branch. CoRR, abs/2006.15212, 2020. URL https://arxiv.](https://arxiv.org/abs/2006.15212)
[org/abs/2006.15212.](https://arxiv.org/abs/2006.15212)

W. K. Hastings. Monte Carlo sampling methods using Markov chains and their applications.
_Biometrika, 57(1):97–109, 04 1970._ ISSN 0006-3444. doi: 10.1093/biomet/57.1.97. URL
[https://doi.org/10.1093/biomet/57.1.97.](https://doi.org/10.1093/biomet/57.1.97)


-----

Keld Helsgaun. An effective implementation of the lin–kernighan traveling salesman heuristic.
_European Journal of Operational Research, 126(1):106–130, 2000._

John H. Holland. _Adaptation in Natural and Artificial Systems: An Introductory Analysis with_
_Applications to Biology, Control and Artificial Intelligence. MIT Press, Cambridge, MA, USA,_
1992. ISBN 0262082136.

Lester Ingber. Adaptive simulated annealing (asa): lessons learned. Control and Cybernetics, 25(1),
1996.

Kaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and enhanced
design. In International Conference on Machine Learning, pp. 4882–4892. PMLR, 2021.

David S Johnson. Near-optimal bin packing algorithms. PhD thesis, Massachusetts Institute of
Technology, 1973.

Chaitanya K Joshi, Thomas Laurent, and Xavier Bresson. On learning paradigms for the travelling
salesman problem. arXiv preprint arXiv:1910.07210, 2019.

Sami Khairy, Ruslan Shaydulin, Lukasz Cincio, Yuri Alexeev, and Prasanna Balaprakash. Learning
to optimize variational quantum circuits to solve combinatorial problems. Proceedings of the
_AAAI Conference on Artificial Intelligence, 34(03):2367–2375, Apr 2020. ISSN 2159-5399. doi:_
[10.1609/aaai.v34i03.5616. URL http://dx.doi.org/10.1609/aaai.v34i03.5616.](http://dx.doi.org/10.1609/aaai.v34i03.5616)

Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua
Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR
_[2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:](http://arxiv.org/abs/1412.6980)_
[//arxiv.org/abs/1412.6980.](http://arxiv.org/abs/1412.6980)

Scott Kirkpatrick, C Daniel Gelatt Jr, and Mario P Vecchi. Optimization by simulated annealing. In
_Readings in Computer Vision, pp. 606–615. Elsevier, 1987._

Wouter Kool, Herke van Hoof, and Max Welling. Attention, learn to solve routing problems! In
_International Conference on Learning Representations, 2018._

Wouter Kool, Herke van Hoof, Joaquim Gromicho, and Max Welling. Deep policy dynamic programming for vehicle routing problems. arXiv preprint arXiv:2102.11756, 2021.

Markus Kruber, Marco L¨ubbecke, and Axel Parmentier. Learning when to use a decomposition. pp.
202–210, 05 2017. ISBN 978-3-319-59775-1. doi: 10.1007/978-3-319-59776-8 16.

Jimmy Lam and Jean-Marc Delosme. Performance of a new annealing schedule. In Proceedings of
_the 25th ACM/IEEE Design Automation Conference, pp. 306–311, 1988._

A. H. Land and A. G. Doig. An automatic method of solving discrete programming problems.
_Econometrica, 28(3):pp. 497–520, 1960. ISSN 00129682._

Valerii Likhosherstov, Xingyou Song, Krzysztof Choromanski, Jared Davis, and Adrian Weller.
Debiasing a first-order heuristic for approximate bi-level optimization. _arXiv preprint_
_arXiv:2106.02487, 2021._

Dougal Maclaurin, David Duvenaud, and Ryan P. Adams. Gradient-based hyperparameter optimization through reversible learning, 2015.

Alejandro Marcos Alvarez, Francis Maes, and Louis Wehenkel. Supervised learning to tune simulated annealing for in silico protein structure prediction. In ESANN 2012 proceedings, 20th
_European Symposium on Artificial Neural Networks, Computational Intelligence and Machine_
_Learning, pp. 49–54. Ciaco, 2012._

Nicholas Metropolis, Arianna W. Rosenbluth, Marshall N. Rosenbluth, Augusta H. Teller, and Edward Teller. Equation of state calculations by fast computing machines. The Journal of Chemical
_[Physics, 21(6):1087–1092, 1953. doi: 10.1063/1.1699114. URL https://doi.org/10.](https://doi.org/10.1063/1.1699114)_
[1063/1.1699114.](https://doi.org/10.1063/1.1699114)


-----

Frank No´e, Simon Olsson, Jonas K¨ohler, and Hao Wu. Boltzmann generators: Sampling equilibrium
states of many-body systems with deep learning. Science, 365(6457), 2019.

Hazem AA Nomer, Khalid Abdulaziz Alnowibet, Ashraf Elsayed, and Ali Wagdy Mohamed. Neural
knapsack: A neural network based solver for the knapsack problem. IEEE Access, 8:224200–
224210, 2020.

Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.

Ana I. Pereira and Edite M. G. P. Fernandes. A study of simulated annealing variants. In Proceedings
_of XXVIII Congreso de Estad´ıstica e Investigaci´on Operativa, 2004._

[Laurent Perron and Vincent Furnon. Or-tools. URL https://developers.google.com/](https://developers.google.com/optimization/)
[optimization/.](https://developers.google.com/optimization/)

L.M. Rasdi Rere, Mohamad Ivan Fanany, and Aniati Murni Arymurthy. Simulated annealing algorithm for deep learning. Procedia Computer Science, 72:137–144, 2015. ISSN 1877-0509. doi:
[https://doi.org/10.1016/j.procs.2015.12.114. URL https://www.sciencedirect.com/](https://www.sciencedirect.com/science/article/pii/S1877050915035759)
[science/article/pii/S1877050915035759. The Third Information Systems Interna-](https://www.sciencedirect.com/science/article/pii/S1877050915035759)
tional Conference 2015.

Bastian Rieck. Basic analysis of bin-packing heuristics. arXiv preprint arXiv:2104.12235, 2021.

Tim Salimans, Jonathan Ho, Xi Chen, and Ilya Sutskever. Evolution strategies as a scalable alternative to reinforcement learning. ArXiv, abs/1703.03864, 2017.

Khodakaram Salimifard, Hamid Shahbandarzadeh, and Ramin Raeesi. Green transportation and
the role of operations research. In 2012 International Conference on Traffic and Transportation
_Engineering (ICTTE 2012), pp. 74–79, 2012._

John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.

Robert H. Swendsen and Jian-Sheng Wang. Replica monte carlo simulation of spin-glasses. Phys.
_Rev. Lett., 57:2607–2609, Nov 1986._ doi: 10.1103/PhysRevLett.57.2607. [URL https://](https://link.aps.org/doi/10.1103/PhysRevLett.57.2607)
[link.aps.org/doi/10.1103/PhysRevLett.57.2607.](https://link.aps.org/doi/10.1103/PhysRevLett.57.2607)

P.J. van Laarhoven and E.H. Aarts. Simulated Annealing: Theory and Applications. Mathematics
[and Its Applications. Springer Netherlands, 1987. ISBN 9789027725134. URL https://](https://books.google.co.in/books?id=-IgUab6Dp_IC)
[books.google.co.in/books?id=-IgUab6Dp_IC.](https://books.google.co.in/books?id=-IgUab6Dp_IC)

Dhruv Vashisht, Harshit Rampal, Haiguang Liao, Yang Lu, Devika Shanbhag, Elias Fallon, and
Levent Burak Kara. Placement in integrated circuits using cyclic reinforcement learning and
simulated annealing, 2020.

Paul Vicol, Luke Metz, and Jascha Sohl-Dickstein. Unbiased gradient estimation in unrolled computation graphs with persistent evolution strategies. In International Conference on Machine
_Learning, pp. 10553–10563. PMLR, 2021._

Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer networks, 2017.

Matteo M. Wauters, Emanuele Panizon, Glen B. Mbeng, and Giuseppe E. Santoro. Reinforcementlearning-assisted quantum optimization. Physical Review Research, 2(3), Sep 2020. ISSN 26431564. doi: 10.1103/physrevresearch.2.033446. [URL http://dx.doi.org/10.1103/](http://dx.doi.org/10.1103/PhysRevResearch.2.033446)
[PhysRevResearch.2.033446.](http://dx.doi.org/10.1103/PhysRevResearch.2.033446)

Yaoxin Wu, Wen Song, Zhiguang Cao, Jie Zhang, and Andrew Lim. Learning improvement heuristics for solving the travelling salesman problem. 2019a.

Yaoxin Wu, Wen Song, Zhiguang Cao, Jie Zhang, and Andrew Lim. Learning improvement
heuristics for solving the travelling salesman problem. CoRR, abs/1912.05784, 2019b. URL
[http://arxiv.org/abs/1912.05784.](http://arxiv.org/abs/1912.05784)


-----

Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnab´as P´oczos, Ruslan Salakhutdinov, and
[Alexander J. Smola. Deep sets. abs/1703.06114, 2017. URL http://arxiv.org/abs/](http://arxiv.org/abs/1703.06114)
[1703.06114.](http://arxiv.org/abs/1703.06114)


-----

A ADDITIONAL EXPERIMENTAL INFORMATION AND RESULTS

Our code was implemented in Pytorch 1.9 (Paszke et al., 2017) and run in a standard machine
equipped with a single GPU RTX2080. The randomly generated datasets used for testing can be
recreated by setting the seed of Pytorch’s random number generator to 0.

Table 4: Running Times

|Col1|Knapsack Ours OR-Tools|Bin Packing Ours OR-Tools|
|---|---|---|


|50N 100N 200N 500N 1000N 2000N|0.74s < 0.01s 1.13s < 0.01s 3.01s < 0.01s 9.40s 0.01s 38.68s 0.02s 130.86s 0.08s|1.14s 54s 2.27s 56s 4.79s 1m ≥ 14.18s 1m ≥ 37.67s 1m ≥ 121.74s 1m ≥|
|---|---|---|



A.1 KNAPSACK PROBLEM

**Data** We consider problems of different sizes, with KNAPN consisting of N items, each with a
also an associated capacity, that is, the maximum weight the knapsack can comport. Here we followweight wi and value vi sampled from a uniform distribution, wi, vi ∼U(0:1). Each problem has
(Bello et al., 2016) and set C50 = 12.5, C100 = 25 and C200 = 25. However, for larger problems
we set CN = N/8.

**Training** We train only on KNAP50 with short rollouts of length K = 100 steps. The model
is trained for 1000 epochs each of which is run on 256 random problems generated on the fly as
described in the previous section. We set initial and final temperatures to T0 and TK, and compute

1

the temperature decay as α = (TK/T0) _K ._

-  PPO: Both actor and critic networks were optimised with Adam (Kingma & Ba, 2015) with
learning rate of 1e−3, weight decay of 1e−2 and β = (0.9, 0.999). T0 = 1 and TK = 0.1.

-  ES: We use a population of 16 perturbations sampled from a Gaussian of standard deviation
0.05. Updates are fed into an SGD optimizer with learning rate 1e-3 and momentum 0.9.
_T0 = 1 and TK = 0.1._


**Testing** We evaluate Neural SA on test sets of 1000 randomly generated Knapsack problems, while
varying the length of the rollout. For each problem size N, we consider rollouts of length K = N,
_K = 2N_, K = 5N and K = 10N . The initial and final temperatures are kept fixed to T0 = 1 and

1

_TK = 0.1, respectively, and the temperature decay varies as function of K, α = (TK/T0)_ _K ._

We compare our methods against one of the dedicated solvers for knapsack in OR-Tools (Perron &
Furnon) (KNAPSACK MULTIDIMENSION BRANCH AND BOUND SOLVER). We also compare sampled and greedy variants of Neural SA. The former naturally samples actions from the
proposal distribution while the latter always selects the most likely action.


-----

Table 5: ES results on the Knapsack benchmark. Bigger is better. Comparison among rollouts of
different lengths: 1, 2, 5 or 10 times the dimension of the problem.

|Col1|Greedy ×1|Sampled ×1 ×2 ×5 ×10|OR-Tools|
|---|---|---|---|
|KNAP50 KNAP100 KNAP200 KNAP500 KNAP1K KNAP2K|16.59 ± .00 31.15 ± .00 55.96 ± .00 135.92 ± .00 259.20 ± .00 489.02 ± .00|19.45 ± .01 19.70 ± .00 19.86 ± .00 19.95 ± .00 39.07 ± .01 39.49 ± .01 39.76 ± .01 39.90 ± .01 53.72 ± .02 55.21 ± .02 56.22 ± .02 56.58 ± .01 134.20 ± .05 137.89 ± .03 140.20 ± .02 141.01 ± .03 269.21 ± .04 276.48 ± .05 280.94 ± .02 282.46 ± .03 537.53 ± .08 551.92 ± .07 560.75 ± .07 563.75 ± .02|20.07 40.50 57.45 144.14 289.01 577.28|



Table 6: PPO results on the Knapsack benchmark. Bigger is better. Comparison among rollouts of
different lengths: 1, 2, 5 or 10 times the dimension of the problem.

|Col1|Greedy ×1|Sampled ×1 ×2 ×5 ×10|OR-Tools|
|---|---|---|---|
|KNAP50 KNAP100 KNAP200 KNAP500 KNAP1K KNAP2K|19.46 ± .00 38.79 ± .00 47.90 ± .00 117.51 ± .00 234.34 ± .00 464.92 ± .00|19.31 ± .01 19.43 ± .01 19.70 ± .01 19.87 ± .01 38.53 ± .02 38.92 ± .02 39.56 ± .02 39.92 ± .01 48.76 ± .04 51.13 ± .03 54.05 ± .02 55.65 ± .01 121.88 ± .05 128.78 ± .03 136.12 ± .03 139.66 ± .02 245.48 ± .12 259.80 ± .05 273.96 ± .05 280.39 ± .01 491.48 ± .10 520.61 ± .06 548.15 ± .06 560.40 ± .02|20.07 40.50 57.45 144.14 289.01 577.28|



A.2 BIN PACKING PROBLEM

**Data** We consider problems of different sizes, with BINN consisting of N items, each with a
weight (size) sampled from a uniform distribution, wi (0:1). Without loss of generality, we also
assume N bins, all with unitary capacity. Each dataset B ∼UINN in Tables 7 and 8 contains 1000 such
random Bin Packing problems used to evaluate the methods at test time.

**Training** We train only on BIN50 with short rollouts of length K = 100 steps. The model is
trained for 1000 epochs each of which is ran on 256 random problems generated on the fly as
described in the previous section. We set initial and final temperatures to T0 and TK, and compute

1

the temperature decay as α = (TK/T0) _K ._

-  PPO: Both actor and critic networks were optimised with Adam (Kingma & Ba, 2015) with
learning rate of 2e−4, weight decay of 1e−2 and β = (0.9, 0.999). T0 = 1 and TK = 0.1.

-  ES: We use a population of 16 perturbations sampled from a Gaussian of standard deviation
0.1. Updates are fed into an SGD optimizer with learning rate 1e-3 and momentum 0.9.
_T0 = 0.1 and TK = 1e −_ 4.


**Testing** We evaluate Neural SA on test sets of 1000 randomly generated Bin Packing problems,
while varying the length of the rollout. For each problem size N, we consider rollouts of length K =
_N_, K = 2N, K = 5N and K = 10N . The initial and final temperatures are kept fixed to T0 = 1

1

and TK = 0.1, respectively, and the temperature decay varies as function of K, α = (TK/T0) _K ._

We compare Neural SA against First-Fit-Decreasing (FFD) (Johnson, 1973), a powerful heuristic for
the Bin Packing problem, and against OR-Tools (Perron & Furnon) MIP solver powered by SCIP
(Gamrath et al., 2020a). The OR-Tools solver can be quite slow on Bin Packing so we set a time out
of one minute per problem so that we could complete

We also compare sampled and greedy variants of Neural SA. The former naturally samples actions
from the proposal distribution while the latter always selects the most likely action.


-----

Table 7: ES results on the Bin Packing benchmark. Lower is better.

|Col1|Greedy ×1|Sampled ×1 ×2 ×5 ×10|OR-Tools FFD|
|---|---|---|---|
|BIN50 BIN100 BIN200 BIN500 BIN1K BIN2K|27.62±.00 53.80±.00 105.63±.00 259.09±.00 512.66±.00 1017.88 ± .00|27.43±.01 27.36±.01 27.29±.00 27.24±.01 53.63±.00 53.54±.01 53.44±.01 53.38±.01 105.78±.02 105.64±.01 105.51±.01 105.43±.01 260.86±.03 260.65±.01 260.42±.02 260.27±.02 517.87±.02 517.46±.02 517.08±.02 516.84±.01 1030.66±.01 1029.89±.01 1029.11±.02 1028.67±.02|26.71 27.10 53.91 52.91 109.19 104.25 267.63 257.02 1000∗ 509.46 2000∗ 1028.67|



Table 8: PPO results on the Bin Packing benchmark. Lower is better.

|Col1|Greedy ×1|Sampled ×1 ×2 ×5 ×10|OR-Tools FFD|
|---|---|---|---|
|BIN50 BIN100 BIN200 BIN500 BIN1K BIN2K|27.62 ± .00 53.80 ± .00 105.63 ± .00 259.08 ± .00 512.66 ± .00 1017.88 ± .00|27.83 ± .01 27.65 ± .01 27.41 ± .01 27.29 ± .01 54.63 ± .02 54.15 ± .02 53.68 ± .01 53.47 ± .01 108.04 ± .02 106.93 ± .02 106.08 ± .01 105.75 ± .01 267.19 ± .05 264.14 ± .01 262.26 ± .02 261.61 ± .01 531.17 ± .05 524.78 ± .03 521.32 ± .03 520.32 ± .02 1058.74 ± .04 1045.36 ± .03 1038.77 ± .02 1037.15 ± .03|26.71 27.10 53.91 52.91 109.19 104.25 267.63 257.02 1000∗ 509.46 2000∗ 1028.67|



A.3 TRAVELLING SALESPERSON PROBLEM (TSP)

**Data** We generate random instances for 2D Euclidean TSP by sampling coordinates uniformly in
a unit square, as done in previous research (Kool et al., 2018; Chen & Tian, 2019; de O. da Costa
et al., 2020). We assume complete graphs (fully-connected TSP), which means every pair of cities
is connected by a valid route (an edge).

**Training** We train only on TSP20 with very short rollouts of length K = 40. Just like in the other
problems we consider, we train using 256 random problems generated on the fly for each. We also

1

maintain the same initial temperature and cooling schedule with T0 = 1 and α = (TK/T0) _K, but_

use lower final temperatures for the TSP. We set TK = 1e−2 for PPO and TK = 1e−4 for ES,
which we empirically found to work best with the training dynamics of each of these methods.

-  PPO: Both actor and critic networks were optimised with Adam (Kingma & Ba, 2015)
with learning rate of 2e−4, weight decay of 1e−2 and β = (0.9, 0.999). We trained PPO
for 1000 epochs.

-  ES: We use a population of 16 perturbations sampled from a Gaussian of standard deviation
0.1. Updates are fed into an SGD optimizer with learning rate 1e-3 and momentum 0.9,
and trained for 10 000 epochs.

**Testing** We evaluate Neural SA on TSP20, TSP50 and TSP100 using the 10K problem instances
made available in Kool et al. (2018). This allows us to directly compare our methods to previous research on the TSP. We also consider larger problem sizes, namely TSP200 and TSP500 to showcase
the scalability of Neural SA. For each of these, we randomly generate 1000 instances by uniformly
sampling coordinates in a 2D unit square. As for the other CO problems we study, we also compare
sampled and greedy variants of Neural SA. The former naturally samples actions from the proposal
distribution while the latter always selects the most likely action.

We compare Neural SA against standard solvers LKH-3 (Helsgaun, 2000) and Concorde (Applegate
et al., 2006), which we have run ourselves. We also compare against the self-reported results of
other Deep Learning models that have targeted TSP and relied on the test data provided by Kool
et al. (2018): GCN (Joshi et al., 2019), GAT (Kool et al., 2018), GAT-T (Wu et al., 2019a), and the
works of de O. da Costa et al. (2020) and Fu et al. (2021).

Note that Fu et al. (2021) also provide results for TSP200 and TSP500, but given that there is no
public dataset for these problem sizes, it is hard to make a direct comparison to our results, especially
regarding running times. They use a dataset of 128 instances, while we use 1000.


-----

Table 9: PPO results on the TSP benchmark. Lower is better

|Col1|Greedy ×1|Sampled ×1 ×2 ×5 ×10|LKH-3 Concorde|
|---|---|---|---|
|TSP20 TSP50 TSP100|4.937 ± .001 8.104 ± .006 11.764 ± .012|3.941 ± .001 3.898 ± .001 3.865 ± .000 3.852 ± .000 5.916 ± .002 5.847 ± .001 5.789 ± .000 5.762 ± .000 8.167 ± .000 8.052 ± .001 7.956 ± .001 7.907 ± .001|3.836 3.836 5.696 5.696 7.764 7.764|



Table 10: ES results on the TSP benchmark. Lower is better

|Col1|Greedy ×1|Sampled ×1 ×2 ×5 ×10|LKH-3 Concorde|
|---|---|---|---|
|TSP20 TSP50 TSP100|3.868 ± .000 6.020 ± .002 8.659 ± .003|3.868 ± .001 3.854 ± .000 3.844 ± .000 3.840 ± .000 6.022 ± .002 5.947 ± .001 5.871 ± .000 5.828 ± .001 8.660 ± .002 8.477 ± .001 8.298 ± .002 8.191 ± .002|3.836 3.836 5.696 5.696 7.764 7.764|


First city selector Second city selector

11 4 11 3

15 0 17 15 0 17

16 16 4

3

12 4 13 12 4 13 5

10 3 2 10 3 6

6 6 7

19 1 19 1

9 18 7 1 9 18 7 8

14 2 8 14 2 8 9

5 0 5

10

11

15 0 17

16

12 4 13

10

3

6

19 1

9 18 7

14 2 8

5

Resulting tour if action is accepted


Figure 6: Policy for the Travelling Salesperson Problem. At each step, an action consists of selecting
a pair of cities (i, j), one after the other. The figure depicts a TSP problem layed out in the 2D plane,
with the learnt proposal distribution over the first city i in the left, and in the right, the distribution
over the second city j, given i = 12. We mask out and exclude the neighbours of i (0 and 14) as
candidates for j because selecting those would lead to no changes in the tour. It is clear the model
has a strong preference towards a few cities, but otherwise the probability mass is spread almost
uniformly among the other nodes. However, once i is fixed, Neural SA strongly favours nodes j
that are close to i. That is a desirable behaviour and even features in popular algorithms like LKH-3
(Helsgaun, 2000). That is because a 2-opt move (i, j) actually adds edge (i, j) to the tour, so leaning
towards pairs of cities that are close to each other is more likely to lead to shorter tours.


-----

Time

Gap

TSP500

Cost

Time

TSP200Gap

Cost

Time

TSP100Gap

Cost

Time

TSP50Gap

Cost

Time

TSP20Gap

Cost


7h58m1h15m
0.00%0.00%

16.5416.54

38m21m

0.00%0.00%

10.7010.70

7m1h

0.00%0.00%

7.7647.764

2m14m

0.00%0.00%

5.6965.696

48s1m

0.00%0.00%

3.8363.836

ONCORDE
CLKH-3


2h35m5h13m5h13m
22.25%8.04%22.55%

20.2217.8720.27

30m32m32m
11.96%3.27%9.72%

11.9811.0411.74

6m14m14m

7.43%1.85%5.50%

8.3417.9078.191

1m2m2m

4.34%1.16%2.32%

5.9435.7625.828

10s17s17s

1.17%0.42%0.10%

3.8813.8523.840

SA PPO SA ES
EURAL EURAL
SANN


---

---

---

---

---

---

6m40 m1 h
8.38%1.39%2.26%

8.417.877.94

55s18m24m

3.10%0.01%0.52%

5.875.705.73

6s12m5 m

0.60%0.01%0.08%

3.863.843.84
GCNGCN + Beam SearchGAT


------6m

------2.96%

------16.96

------3m

------0.88%

------10.81

25m1 h2 h10m21 m41m15m
3.24%1.85%1.42%1.84%1.26%0.87%0.03%

8.017.917.877.917.867.837.76

16m45 m1 h7m13m29m9m
0.83%0.34%0.20%0.36%0.21%0.12%0.01%

5.755.725.715.725.715.705.69

12m39m1 h5m10m15m2m

0.03%0.00%0.00%0.01%0.00%0.00%0.00%

3.843.843.843.843.843.843.84

_}}_
_}_
50010002000
_}}} { { {_
100030005000
_{ { {_
GAT-TGAT-TGAT-TDa Costa et al.Da Costa et al.Da Costa et al.Fu et al.


-----

