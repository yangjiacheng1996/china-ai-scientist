# EXPLAINING SCALING LAWS OF NEURAL NETWORK GENERALIZATION

**Anonymous authors**
Paper under double-blind review

ABSTRACT

The test loss of well-trained neural networks often follows precise power-law
scaling relations with either the size of the training dataset or the number of
parameters in the network. We propose a theory that explains and connects these
scaling laws. We identify variance-limited and resolution-limited scaling behavior
for both dataset and model size, for a total of four scaling regimes. The variancelimited scaling follows simply from the existence of a well-behaved infinite data
or infinite width limit, while the resolution-limited regime can be explained by
positing that models are effectively resolving a smooth data manifold. In the
large width limit, this can be equivalently obtained from the spectrum of certain
kernels, and we present evidence that large width and large dataset resolutionlimited scaling exponents are related by a duality. We exhibit all four scaling
regimes in the controlled setting of large random feature and pretrained models and
test the predictions empirically on a range of standard architectures and datasets.
We also observe several empirical relationships between datasets and scaling
exponents: super-classing image tasks does not change exponents, while changing
input distribution (via changing datasets or adding noise) has a strong effect. We
further explore the effect of architecture aspect ratio on scaling exponents.

1 SCALING LAWS FOR NEURAL NETWORKS

For a large variety of models and datasets, neural network performance has been empirically observed
to scale as a power-law with model size and dataset size (Hestness et al., 2017; Kaplan et al., 2020;
Rosenfeld et al., 2020b; Henighan et al., 2020). These exponents determine how quickly performance
improves with more data and larger models. We would like to understand why these power-laws
emerge, and what features of the data and models determine the values of the power law exponents.

In this work, we present a theoretical framework for understanding scaling laws in trained neural
networks. We identify four related scaling regimes with respect to the number of model parameters
_P and the dataset size D. With respect to each of D, P_, there is both a variance-limited regime and a
_resolution-limited regime._

**Variance-Limited Regime** In the limit of infinite data or an arbitrarily wide model, some aspects
of neural network training simplify. Specifically, if we fix one of D, P and study scaling with respect
to the other parameter as it becomes arbitrarily large, then the difference between the finite test loss
and its limiting value scales as 1/x, i.e. as a power-law with exponent 1, with x = D or _√P ∝_ width

in deep networks and x = D or P in linear models.

**Resolution-Limited Regime** In this regime, one of D or P is effectively infinite, and we study
scaling as the other parameter increases. In this case, a variety of works have empirically observed
power-law scalings 1/x[α], typically with 0 < α < 1 for both x = P or D. We derive exponents in
this regime precisely in the setting of random feature models (c.f. next section). Empirically, we find
that our theoretical predictions for exponents hold in pretrained, fine-tuned models even though these
lie outside our theoretical setting.


-----

|V|Variance-lim|mited : Theo|Col4|ory D = 1|1|Col7|Col8|
|---|---|---|---|---|---|---|---|
||||||||10 Loss 10|
|||||||||
|||||||||
|||||||||
||D: 1.02 : 1.01|D||: 1.02 : 1.10||||
||D D: 0.98 (C D: 1.01|D NN) D D||: 1.01 : 1.11 (SGD|)|||


10[1] 10[2] 10[3] 10[4]

D[: 1.02] D[: 1.02]

D[: 1.01] D[: 1.10]

D[: 0.98 (CNN)] D[: 1.01]

D[: 1.01] D[: 1.11 (SGD)]

Dataset size (D)

Resolution-limited


Resolution-limited

|Col1|Col2|
|---|---|
||D: 0.40 D: 0.58|
|D: 0.26 D: 0.37|D: 0.40 D: 0.58|



10[3] 10[4]

D[: 0.26] D[: 0.40]

D[: 0.37] D[: 0.58]

Dataset size (D)

Variance-limited : Theory W[ = 1]


10[0]

10 2

10 4

10 6

10 8


12.7

12.6

12.5

12.4


10[3]

10[2]


10[0]

10 1

|Col1|Col2|
|---|---|
|||
|||
||Teacher|


0.0 0.2 0.4 0.6 0.8 1.0

Teacher

Interpolating Between Training Points in 4-dimensions

25

20

D15
4/10


10[0]

10 1

10 2

10 3

10 4

10 5


10[0]

10 1


10[1] 10[2]

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|: 0.46|: 0.62|||
|W W: 0.34|W W: 0.40|||


W[: 0.46] W[: 0.62]

W[: 0.34] W[: 0.40]

Width


10[2] 10[3] 10[4]

|Va|ariance-limited :|Col3|Col4|Col5|: Theory W = 1|Col7|Col8|Col9|Col10|1|Col12|
|---|---|---|---|---|---|---|---|---|---|---|---|
|||||||||||||
|||||||||||||
|||||||||||||
|||||||||||||
|||||||||||||
||W: 0.98 (MSE) : 1.03||||W: 1.01 : 1.00|||||||
||W W: 1.02 (ERF)||||W W: 1.03 (ERF|||||)||


W[: 0.98 (MSE)] W[: 1.01]

W[: 1.03] W[: 1.00]

W[: 1.02 (ERF)] W[: 1.03 (ERF)]

Width


8 10 12 14 16 18 20 22 24 26

4/ D

2/ D

Dimension


(a) (b)


Teacher-Student CIFAR-10 CIFAR-100 SVHN FashionMNIST MNIST

Figure 1: (a) Four scaling regimes Here we exhibit the four regimes we focus on in this work. (top-left,
**bottom-right) Variance-limited scaling of under-parameterized models with dataset size and over-parameterized**
models with number of parameters (width) exhibit universal scaling (αD = αW = 1) independent of the
architecture or underlying dataset. (top-right, bottom-left) Resolution-limited over-parameterized models with
dataset or under-parameterized models with model size exhibit scaling with exponents that depend on the details
of the data distribution. These four regimes are also found in random feature (Figure 2a) and pretrained models
(see supplement). (b) Resolution-limited models interpolate the data manifold Linear interpolation between
two training points in a four-dimensional input space (top). We show a teacher model and four student models,
each trained on different sized datasets. In all cases teacher and student approximately agree on the training
endpoints, but as the training set size increases they increasingly match everywhere. (bottom) We show 4/αD
versus the data manifold dimension (input dimension for teacher-student models, intrinsic dimension for standard
datasets). We find that the teacher-student models follow the 4/αD (dark dashed line), while the relationship for
a four layer CNN (solid) and WRN (hollow) on standard datasets is less clear.


For more general nonlinear models, we propose a refinement of naive bounds into estimates via
expansions that hold asymptotically. These rely on the idea that additional data (in the infinite
model-size limit) or added model parameters (in the infinite data limit) are used by the model to
carve up the data manifold into smaller components. For smooth manifolds, loss, and network, the
test loss will depend on the linear size of a sub-region, while it is the d-dimensional sub-region
volume that scales inversely with P or D, giving rise to α ∝ 1/d.[1] To test this empirically, we make
measurements of the resolution-limited exponents in neural networks and intrinsic dimension of the
data manifold, shown in Figure 1b.

**Explicit Derivation** We derive the scaling laws for these four regimes explicitly in the setting of
random feature teacher-student models, which also applies to neural networks in the large width limit.
This setting allows us to solve for the test error directly in terms of the feature covariance (kernel).
The scaling of the test loss then follows from the asymptotic decay of the spectrum of the covariance
matrix. For generic continuous kernels on a d-dimensional manifold, we can further relate this to the
dimension of the data manifold.

**Summary of Contributions:**


1. We propose four scaling regimes for neural networks. The variance-limited and resolutionlimited regimes originate from different mechanisms, which we identify. To our knowledge,

1A visualization of this successively better approximation with dataset size is shown in Figure 1b for models
trained to predict data generated by a random fully-connected network.


-----

this categorization has not been previously exhibited. We provide empirical support for all four
regimes in deep networks on standard datasets.

2. We derive the variance-limited regime under simple yet general assumptions (Theorem 1).

3. We present a hypothesis for resolution-limited scaling through refinement of naive bounds (Theorems 2, 3), for general nonlinear models. We empirically test the dependence of the estimates on
intrinsic dimension of the data manifold for deep networks on standard datasets (Figure 1b).

4. In the setting of random feature teacher-student networks, we derive both variance-limited and
resolution-limited scaling exponents exactly. In the latter case, we relate this to the spectral decay
of kernels. We identify a novel duality that exists between model and dataset size scaling.

5. We empirically investigate predictions from the random features setting in pretrained, fine-tuned
models on standard datasets and find they give excellent agreement.

6. We study the dependence of the scaling exponent on changes in architecture and data, finding that
(i) changing the input distribution via switching datasets and (ii) the addition of noise have strong
effects on the exponent, while (iii) changing the target task via superclassing does not.

**Related Works: There have been a number of recent works demonstrating empirical scaling laws**
(Hestness et al., 2017; Kaplan et al., 2020; Rosenfeld et al., 2020b; Henighan et al., 2020; Rosenfeld
et al., 2020a) in deep neural networks, including scaling laws with model size, dataset size, compute,
and other observables such as mutual information and pruning. Some precursors (Ahmad & Tesauro,
1989; Cohn & Tesauro, 1991) can be found in earlier literature. Recently, scaling laws have also
played a significant role in motivating work on the largest models that have yet been developed
(Brown et al., 2020; Fedus et al., 2021).

There has been comparatively little work on theoretical ideas (Sharma & Kaplan, 2020; Bisla et al.,
2021) that match and explain empirical findings in generic deep neural networks. In the particular
case of large width, deep neural networks behave as random feature models (Neal, 1994; Lee et al.,
2018; Matthews et al., 2018; Jacot et al., 2018; Lee et al., 2019; Dyer & Gur-Ari, 2020), and known
results on the loss scaling of kernel methods can be applied (Spigler et al., 2020; Bordelon et al.,
2020). Though not in the original, Bordelon et al. (2020) analyze resolution-limited dataset size
scaling for power-law spectra in later versions.

During the completion of this work, Hutter (2021) presented a specific solvable model of learning
exhibiting non-trivial power-law scaling for power-law (Zipf) distributed features. This does not
directly relate to the setups studied in this work, or present bounds that supersede our results.
Concurrent to our work, Bisla et al. (2021) presented a derivation of the resolution-limited scaling
with dataset size, also stemming from nearest neighbor distance scaling on data manifolds. However,
they do not discuss requirements on model versus dataset size or how this scaling behavior fits into
other asymptotic scaling regimes.

In the variance-limited regime, scaling laws in the context of random feature models (Rahimi &
Recht, 2008; Hastie et al., 2019; d’Ascoli et al., 2020), deep linear models (Advani & Saxe, 2017;
Advani et al., 2020), one-hidden-layer networks (Mei & Montanari, 2019; Adlam & Pennington,
2020a;b), and wide neural networks treated as Gaussian processes or trained in the NTK regime
(Lee et al., 2019; Dyer & Gur-Ari, 2020; Andreassen & Dyer, 2020; Geiger et al., 2020) have been
studied. In particular, this behavior was used in (Kaplan et al., 2020) to motivate a particular ansatz
for simultaneous scaling with data and model size. The resolution-limited analysis can perhaps be
viewed as an attempt to quantify the ideal-world generalization error of Nakkiran et al. (2021).

This work makes use of classic results connecting the spectrum of a smooth kernel to the geometry
it is defined over (Weyl, 1912; Reade, 1983; Kuhn, 1987; Ferreira & Menegatto, 2009) and on the¨
scaling of iteratively refined approximations to smooth manifolds (Stein, 1999; Bickel et al., 2007;
de Laat, 2011).


-----

2 FOUR SCALING REGIMES

Throughout this work we will be interested in how the average test loss L(D, P ) depends on the
dataset size D and the number of model parameters P . Unless otherwise noted, L denotes the test
loss averaged over initialization of the parameters and draws of a size D training set. Some of our
results only pertain directly to the scaling with width w ∝ _√P_, but we expect many of the intuitions

apply more generally. We use the notation αD, αP, and αW to indicate scaling exponents with
respect to dataset size, parameter count, and width. All proofs appear in the supplement.

2.1 VARIANCE-LIMITED EXPONENTS

In the limit of large D the outputs of an appropriately trained network approach a limiting form with
corrections which scale as D[−][1]. Similarly, recent work shows that wide networks have a smooth
large P limit (Jacot et al., 2018), where fluctuations scale as 1/√P . If the loss is sufficiently smooth

then its value will approach the asymptotic loss with corrections proportional to the variance, (1/D or
1/√P ). In Theorem 1 we present sufficient conditions on the loss to ensure this variance dominated

scaling. We note, these conditions are satisfied by mean squared error and cross entropy loss, though
we conjecture the result holds even more generally.

**Theorem 1. Let ℓ(f** ) be the test loss as a function of network output, (L = E [ℓ(f )]), and let
_fT be the network output after T training steps, thought of as a random variable over weight_
_initialization, draws of the training dataset, and optimization seed. Further let fT be concentrating_
_with E[(fT −_ E[fT ])[k]] = O (ϵ) ∀k ≥ 2. If ℓ _is a finite degree polynomial, or has bounded second_
_derivative, or is 2-H¨older, then E [ℓ(fT )] −_ _ℓ_ (E [fT ]) = O(ϵ).

**Dataset scaling** Consider a neural network, and its associated training loss Ltrain(θ). For every
value of the weights, the training loss, thought of as a random variable over draws of a training set
of size D, concentrates around the population loss, with a variance which scales as O _D[−][1][]. If_
the optimization procedure is sufficiently smooth, the trained weights, network output, and higher
 
moments, will approach their infinite D values, ED (fT − ED [fT ])[k][i] = O _D[−][1][]. Here, the_
subscript D on the expectation indicates an average over draws of the training set. This scalingh  
together with Theorem 1 gives the variance limited scaling of loss with dataset size.

This concentration result with respect to dataset size has appeared for linear models in Rahimi
& Recht (2008) and for single hidden layer networks with high-dimensional input data in Mei &
Montanari (2019); Adlam & Pennington (2020a;b). In the supplement we prove this for GD and SGD
with polynomial loss as well as present informal arguments more generally. Additionally, we present
examples violating the smoothness assumption and exhibiting different scaling.


**Large Width Scaling** We can make a very similar argument in the w →∞ limit. It has been
shown that the predictions from an infinitely wide network, either under Bayesian inference (Neal,
1994; Lee et al., 2018), or when trained via gradient descent (Jacot et al., 2018; Lee et al., 2019)
approach a limiting distribution at large width equivalent to a linear model. Furthermore, corrections
to the infinite width behavior are controlled by the variance of the full model around the linear model
predictions. This variance (and higher moments) have been shown to scale as 1/w (Dyer & Gur-Ari,

2020; Yaida, 2020; Andreassen & Dyer, 2020), Ew (fT − Ew [fT ])[k][i] = O _w[−][1][]. Theorem 1 then_

implies the loss will differ from its w = limit by a term proportional toh 1 /w.
_∞_

We note that there has also been work studying the combined large depth and large width limit, where
Hanin & Nica (2020) found a well-defined infinite size limit with controlled fluctuations. In any such
context where the model predictions concentrate, we expect the loss to scale with the variance of
the model output. In the case of linear models, studied below, the variance is O(P _[−][1]) rather than_
_O(√P_ ), and we see the associated variance scaling in this case.


-----

2.2 RESOLUTION-LIMITED EXPONENTS

In this section we consider training and test data drawn uniformly from a compact d-dimensional
manifold, x _d, and targets given by some smooth function y =_ (x) on this manifold.
_∈M_ _F_

**Over-Parameterized Dataset Scaling**

Consider the double limit of an over-parameterized model with large training set size, P ≫ _D ≫_ 1.
We further consider well-trained models, i.e. models that interpolate all training data. The goal is to
understand L(D). If we assume that the learned model f is sufficiently smooth, then the dependence
of the loss on D can be bounded in terms of the dimension of the data manifold _d._
_M_

Informally, if our train and test data are drawn i.i.d. from the same manifold, then the distance from a
test point to the closest training data point decreases as we add more and more training data points. In
particular, this distance scales as O(D[−][1][/d]) (Levina & Bickel, 2005). Furthermore, if f, F are both
sufficiently smooth, they cannot differ too much over this distance. If in addition the loss function, L,
is a smooth function vanishing when f = F, we have L = O(D[−][1][/d]). This is summarized in the
following theorem.

**Theorem 2. Let L(f** ), f and _be Lipschitz with constants KL, Kf_ _, and K_ _. Further let_
_F_ _F_ _D_
_be a training dataset of size D sampled i.i.d from Md and let f_ (x) = F(x), ∀x ∈D then
_L(D) = O_ _KLmax(Kf_ _, KF_ )D[−][1][/d][].
 

**Under-Parameterized Parameter Scaling**

We will again assume that F varies smoothly on an underlying compact d-dimensional manifold
_d. We can obtain a bound on L(P_ ) if we imagine that f approximates as a piecewise function
_M_ _F_
with roughly P regions (see Sharma & Kaplan (2020)). Here, we instead make use of the argument
from the over-parameterized, resolution-limited regime above. If we construct a sufficiently smooth
estimator for F by interpolating among P randomly chosen points from the (arbitrarily large) training
set, then by the argument above the loss will be bounded by O(P _[−][1][/d])._

**Theorem 3. Let L(f** ), f and _be Lipschitz with constants KL, Kf_ _, and K_ _. Further let f_ (x) =
_F_ _F_
_F(x) for P points sampled i.i.d from Md then L(P_ ) = O _KLmax(Kf_ _, KF_ )P _[−][1][/d][]._
 

**From Bounds to Estimates**

Theorems 2 and 3 are phrased as bounds, but we expect the stronger statement that these bounds also
generically serve as estimates, so that eg L(D) = Ω(D[−][c/d]) for c ≥ 2, and similarly for parameter
scaling. If we assume that F and f are analytic functions on Md and that the loss function L(f, F) is
analytic in f −F and minimized at f = F, then the loss at a given test input, xtest, can be expanded
around the nearest training point, ˆxtrain, L(xtest) = _m=n_ 2 _[a][m][(ˆ]xtrain)(xtest_ _xˆtrain)[m],[2]_ where the
_≥_ _−_
first term is of finite order n ≥ 2 because the loss vanishes at the training point. As the typical
distance between nearest neighbor points scales as[P] D[∞][−][1][/d] on a d-dimensional manifold, the loss
will be dominated by the leading term, L ∝ _D[−][n/d], at large D. Note that if the model provides an_
accurate piecewise linear approximation, we will generically find n ≥ 4.

2.3 EXPLICIT REALIZATION IN LINEAR MODELS

In the proceeding sections we have conjectured typical case scaling relations for a model’s test loss.
We have further given intuitive arguments for this behavior which relied on smoothness assumptions
on the loss and training procedure. In this section, we provide a concrete realization of all four scaling
regimes within the context of linear models. Of particular interest is the resolution-limited regime,
where the scaling of the loss is a consequence of the linear model kernel spectrum – the scaling of
over-parameterized models with dataset size and under-parameterized models with parameters is a
consequence of a classic result, originally due to Weyl (1912), bounding the spectrum of sufficiently
smooth kernel functions by the dimension of the manifold they act on.

2For simplicity we have used a very compressed notation for multi-tensor contractions in higher order terms.


-----

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|: 1.01||: 1.01||
|D D: 1.01||D D: 1.01||
|D: 1.01 D: 1.01||D: 1.00 D: 1.00||

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|
|---|---|---|---|---|---|---|---|---|---|---|
||||||||||||
||||||||||||
||||||||||||
|: 0.34||||: 0||||.67|||
|D D: 0.43||||D D: 0||||.76|||
|D: 0.52||||D: 0||||.85|||
|D: 0.61||||D: 1||||.22|||

|Col1|Col2|Col3|Col4|Col5|
|---|---|---|---|---|
||||||
|: 0.3|4|: 0.55|||
|K K: 0.4|2|K K: 0.60|||
|K: 0.5 K: 0.5|0 1|K: 0.71 K: 1.25|||

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|Col9|Col10|Col11|
|---|---|---|---|---|---|---|---|---|---|---|
||||||||||||
||||||||||||
||||||||||||
||P: 0.34 P: 0.44 P: 0.52||||P: 0 P: 0 P: 0||||.70 .79 .92||
||P: 0.63||||P: 1||||.31||

|Col1|Col2|Col3|Col4|
|---|---|---|---|
|||||
|||||
||P: 1.14 P: 1.14 P: 1.15|P: 1.15 P: 1.15 P: 1.14||
||P: 1.14|P: 1.15||

|Col1|P|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||K||||||
||||||||
||||||||
||||||||
||||||||


Variance-limited Resolution-limited Kernel spectrum

10[0] 10[2] 10[2]

10[1] 14

)10 1 10[0] 10[0]
Loss - Loss(1010 23 DD[: 1.01][: 1.01] DD[: 1.01][: 1.01] Loss10101010 1234 DD[: 0.34][: 0.43] DD[: 0.67][: 0.76] i1010 24 KK[: 0.34][: 0.42] KK[: 0.55][: 0.60] 12

10 4 DD[: 1.01][: 1.01] DD[: 1.00][: 1.00] 1010 56 DD[: 0.52][: 0.61] DD[: 0.85][: 1.22] 10 6 KK[: 0.50][: 0.51] KK[: 0.71][: 1.25] 10

10 5 10 7

10[3] 10[4] 10[5] 10[1] 10[2] 10[3] 10[4] 10[0] 10[1] 10[2] 10[3] 8

Dataset size (D) Dataset size (D) i

10[2] Resolution-limited 10[2] Variance-limited Fit exponents pool size

P 6

10[1] 10[1] 1.2 K
10[0] )

Loss101010101010 123456 PPPP[: 0.34][: 0.44][: 0.52][: 0.63] PPPP[: 0.70][: 0.79][: 0.92][: 1.31] Loss - Loss(10101010123[0] PPPP[: 1.14][: 1.14][: 1.15][: 1.14] PPPP[: 1.15][: 1.15][: 1.14][: 1.15] 1.00.80.60.4 42

10 7 10 4

10[1] 10[2] 10[3] 10[4] 10[2] 10[3] 10[4] 0.4 0.6 0.8 1.0 1.2

Parameter count (P) Parameter count (P) D

(a) (b)


Figure 2: (a) Random feature models exhibit all four scaling regimes Here we consider linear teacherstudent models with random features trained with MSE loss to convergence. We see both variance-limited
scaling (top-left, bottom-right) and resolution-limited scaling (top-right, bottom-left). Data is varied by
downsampling MNIST by the specified pool size. (b) Duality and spectra in random feature models Here
we show the relation between the decay of the kernel spectra, αK, and the scaling of the loss with number of
data points, αD, and with number of parameters, αP (top) The spectra of random FC kernels on pooled MNIST
**(bottom) appear well described by a power law decay. The theoretical relation αD = αP = αK is given by the**
black dashed line.

Linear predictors serve as a model system for learning. Such models are used frequently in practice
when more expressive models are unnecessary or infeasible (McCullagh & Nelder, 1989; Rifkin &
Lippert, 2007; Hastie et al., 2009) and also serve as an instructive test bed to study training dynamics (Advani et al., 2020; Goh, 2017; Hastie et al., 2019; Nakkiran, 2019; Grosse, 2021). Furthermore,
in the large width limit, randomly initialized neural networks become Gaussian Processes (Neal,
1994; Lee et al., 2018; Matthews et al., 2018; Novak et al., 2019; Garriga-Alonso et al., 2019; Yang,
2019), and in the low-learning rate regime (Lee et al., 2019; Lewkowycz et al., 2020; Huang et al.,
2020) neural networks train as linear models at infinite width (Jacot et al., 2018; Lee et al., 2019;
Chizat et al., 2019).

Here we discuss linear models in general terms, though the results immediately hold for the special
cases of wide neural networks. In this section we focus on teacher-student models with weights
initialized to zero and trained with mean squared error (MSE) loss to their global optimum.

We consider a linear teacher, F, and student f, F (x) = _M_ =1 _[ω][M]_ _[F][M]_ [(][x][)][,][ f] [(][x][) =][ P]µ[P]=1 _[θ][µ][f][µ][(][x][)][ .]_
Here {FM _} are a (potentially infinite) pool of features and the teacher weights, ωM are taken to be_
normal distributed, ω (0, 1/S). The student model is built out of a subset of the teacher features.[P][S]
_∼N_
To vary the number of parameters in this simple model, we construct P features, fµ=1,...,P, by
introducing a projector onto a P -dimensional subspace of the teacher features, fµ = _M_
_P_ _[P][µM]_ _[F][M]_ [.]

We train by sampling a training set of size D and minimizing the MSE loss, Ltrain =
1 _D_ [P]

2D _a=1_ [(][f] [(][x][a][)][ −] _[F]_ [(][x][a][))][2][. We are interested in the test loss averaged over draws of our teacher]
and training dataset. The infinite data test loss,P _L(P_ ) := limD→∞ _L(D, P_ ), takes the form.

_L(P_ ) = [1] _._ (1)

2S [Tr] _C −CP_ _[T][  ]PCP_ _[T][ ][−][1]_ _PC_
h i

Here we have introduced the feature-feature second moment-matrix, C = Ex _F_ (x)F _[T]_ (x) .

If the teacher and student features had the same span, this would vanish, but due to the mismatch 
the loss is non-zero. On the other hand, if we keep a finite number of training points, but allow the


-----

student to use all of the teacher features, the test loss, L(D) := limP _S L(D, P_ ), takes the form,
_→_

_L(D) = [1]_ (x, x) (x) [¯] (x) _._ (2)

2 [E][x] _K_ _−_ _K[⃗]_ _K[−][1][ ⃗]K_
h i

Here, K(x, x[′]) is the data-data second moment matrix, _K[⃗]_ indicates restricting one argument to the D
training points, while _K[¯] indicates restricting both. This test loss vanishes as the number of training_
points becomes infinite but is non-zero for finite training size.

We present a full derivation of these expressions in the supplement. In the remainder of this section,
we explore the scaling of the test loss with dataset and model size.

2.3.1 VARIANCE-LIMITED EXPONENTS

To derive the limiting expressions (1) and (2) for the loss one makes use of the fact that the sample
expectation of the second moment matrix over the finite dataset, and finite feature set is close to the
full covariance, _D[1]_ _Da=1_ _[F]_ [(][x][a][)][F][ T][ (][x][a][) =][ C][ +][ δ][C][,][ 1]P _[f][ T][ (][x][)][f]_ [(][x][′][)][,][ =][ K][ +][ δ][K][, with the fluctuations]

satisfying ED _δCP[2][]_ = O(D[−][1]) and EP _δK_ [2][] = O(P _[−][1]), where expectations are taken over_
draws of a dataset of size D and over feature sets. Using these expansions yields the variance-limited
 
scaling, L(D, P ) − _L(P_ ) = O(D[−][1]), L(D, P ) − _L(D) = O(P_ _[−][1]) in the under-parameterized and_
over-parameterized settings respectively.

In Figure 2a we see evidence of these scaling relations for features built from randomly initialized
ReLU networks on pooled MNIST independent of the pool size. In the supplement we provide an
in-depth derivation of this behavior and expressions for the leading contributions to L(D, P ) − _L(P_ )
and L(D, P ) − _L(D)._

2.3.2 RESOLUTION-LIMITED EXPONENTS

We now would like to analyze the scaling behavior of our linear model in the resolution-limited
regimes, that is the scaling with P when 1 ≪ _P ≪_ _D and the scaling with D when 1 ≪_ _D ≪_ _P_ .
well described by a power-law, where eigenvaluesIn these cases, the scaling is controlled by the shared spectrum of λi satisfy λi = _i[1+] C1[αK] or[. See Figure 2b for example] K. This spectrum is often_

spectra on pooled MNIST. In this case, we will argue that the losses also obey a power law scaling,
with the exponents controlled by the spectral decay factor, 1 + αK.

_L(D) ∝_ _D[−][α][K]_ _,_ _L(P_ ) ∝ _P_ _[−][α][K]_ _._ (3)

In other words, in this setting, αP = αD = αK. This is supported empirically in Figure 2b. We
then argue that when the kernel function, K is sufficiently smooth on a manifold of dimension d,
_αK_ _d[−][1], thus realizing the more general resolution-limited picture described above._
_∝_

**From spectra to scaling laws for the loss** To be concrete let us focus on the over-parameterized
loss. If we introduce the notation ei for the eigenvectors of and ¯ei for the eignvectors of
1 _D_ _C_

_D_ _a=1_ _[F]_ [(][x][a][)][F][ T][ (][x][a][)][, the loss becomes,]

_S_ _D_

P

_L(D) = [1]_ _λi(1_ (ei ¯ej)[2]) . (4)

2 _i=1_ _−_ _j=1_ _·_

X X

Before discussing the general asymptotic behavior of (4), we can gain some intuition by considering
the case of large αK. In this case, ¯ej ≈ _ej (see e.g. Loukas (2017)), we can simplify (4) to,_


1

(5)
_i[1+][α][K][ =][ α][K][D][−][α][K][ +][ O][(][D][−][α][K]_ _[−][1][)][ .]_


_L(D) ∝_


_D+1_


More generally in the supplement, following Bordelon et al. (2020); Canatar et al. (2021), we use
replica theory methods to derive, L(D) ∝ _D[−][α][K]_ and L(P ) ∝ _P_ _[−][α][K]_, without requiring the large
_αK limit._


-----

|Col1|Col2|
|---|---|
||.41 .42 .40|
|D=0.39 D=0 D=0.39 D=0 D=0.39 D=0 D=0.38|.41 .42 .40|


|Col1|Col2|
|---|---|
||.37 .33 .29|
|D=0.58 D=0 D=0.46 D=0 D=0.41 D=0|.37 .33 .29|


Nclass stddev

Super-classed CIFAR-100 Corrupted CIFAR-10

100 0.200

0.175

0.150

10[0] 10[0] 0.125

Loss 50 Loss 0.100

0.075

D[=0.39] D[=0.41]

D[=0.39] D[=0.42] D[=0.58] D[=0.37] 0.050

D[=0.39] D[=0.40] 20 D[=0.46] D[=0.33] 0.025

10 1 D[=0.38] 105 10 1 D[=0.41] D[=0.29]

0.000

10[3] 10[4] 10[3] 10[4]

Dataset size (D) Dataset size (D)

Figure 3: Effect of data distribution on scaling exponents For CIFAR-100 superclassed to N classes (left),
we find that the number of target classes does not have a visible effect on the scaling exponent. (right) For
CIFAR-10 with the addition of Gaussian noise to inputs, we find the strength of the noise has a strong effect on
performance scaling with dataset size. All models are WRN-28-10.

**Data Manifolds and Kernels** In Section 2.2, we discussed a simple argument that resolutionlimited exponents α ∝ 1/d, where d is the dimension of the data manifold. Our goal now is to
explain how this connects with the linearized models and kernels discussed above: how does the
spectrum of eigenvalues of a kernel relate to the dimension of the data manifold?

The key point is that sufficiently smooth kernels must have an eigenvalue spectrum with a bounded
tail. Specifically, a C _[t]_ kernel on a d-dimensional space must have eigenvalues λn ≲ _n[1+]1[t/d][ (K]uhn,[¨]_

1987). In the generic case where the covariance matrices we have discussed can be interpreted as
kernels on a manifold, and they have spectra saturating the bound, linearized models will inherit
scaling exponents given by the dimension of the manifold.

As a simple example, consider a d-torus. In this case we can study the Fourier series decomposition, and examine the case of a kernel K(x − _y)._ This must take the form K =

_nI_ [[][a][n]I [sin(][n][I][ ·][ (][x][ −] _[y][)) +][ b][n]I_ [cos(][n][I][ ·][ (][x][ −] _[y][))]][, where][ n][I][ = (][n][1][,][ · · ·][, n][d][)][ are integer indices,]_
and anI, bnI are the overall Fourier coefficients. To guarantee that K is a C _[t]_ function, we must have

P 1

_ain this simple case, the tail eigenvalues of the kernel must be bounded bynI_ _, bnI ≲_ _n[d][+][t][ where][ n][d][ =][ N][ indexes the number of][ a][n][I][ in decreasing order. But this means that]N_ [1+]1[t/d][ as][ N][ →∞][.]

2.4 DUALITY

We argued above that for kernels with pure power law spectra, the asymptotic scaling of the underparameterized loss with respect to model size and the over-parameterized loss with respect to
dataset size share a common exponent. In the linear setup at hand, the relation between the underparameterized parameter dependence and over-parameterized dataset dependence is even stronger.
The under-parameterized and over-parameterized losses are directly related by exchanging the
projection onto random features with the projection onto random training points. Note, sample-wise
double descent observed in Nakkiran (2019) is a concrete realization of this duality for a simple data
distribution. In the supplement, we present examples exhibiting the duality of the loss dependence on
model and dataset size outside of the asymptotic regime.

3 EXPERIMENTS

3.1 DEEP TEACHER-STUDENT MODELS

Our theory can be tested very directly in the teacher-student framework, in which a teacher deep
neural network generates synthetic data used to train a student network. Here, it is possible to generate
unlimited training samples and, crucially, controllably tune the dimension of the data manifold. We


-----

accomplish the latter by scanning over the dimension of the inputs to the teacher. We have found that
when scanning over both model size and dataset size, the interpolation exponents closely match the
prediction of 4/d. The dataset size scaling is shown in Figure 1, while model size scaling experiments
appear in the supplement and have previously been observed in Sharma & Kaplan (2020).

3.2 VARIANCE-LIMITED SCALING IN THE WILD

Variance-limited scaling, (Section 2.1), can be universally observed in real datasets. Figure 1a
(top-left, bottom-right) measures the variance-limited dataset scaling exponent αD and width scaling
exponent αW . In both cases, we find striking agreement with the theoretically predicted values
_αD, αW = 1 across a variety of dataset, network architecture, stochastic batch size and loss type._
Our testbed includes deep fully-connected and convolutional networks with Relu or Erf nonlinearities
and MSE or softmax-cross-entropy losses. The supplement contains experimental details.

3.3 RESOLUTION-LIMITED SCALING IN THE WILD

In addition to teacher-student models, we explored resolution-limited scaling behavior in the context
of standard classification datasets. Wide ResNet (WRN) models (Zagoruyko & Komodakis, 2016)
were trained for a fixed number of steps with cosine decay. In Figure 1b we also include data from
a four hidden layer CNN detailed in the supplement. As detailed above, we find dataset dependent
scaling behavior in this context.

We further investigated the effect of the data distribution on the resolution-limited exponent, αD, by
tuning the number of target classes and input noise (Figure 3). To probe the effect of the number of
classes, we constructed tasks derived from CIFAR-100 by grouping classes into broader semantic
categories. We found that performance depends on the number of categories, but αD is insensitive
to this number. In contrast, the addition of Gaussian noise had a more pronounced effect on αD.
This suggest a picture in which the network learns to model the input data manifold, independent of
the classification task, consistent with observations in Nakkiran & Bansal (2020); Grathwohl et al.
(2020).

We also explored the effect of aspect ratio on dataset scaling, finding that the exponent magnitude
increases with width up to a critical width, while the dependence on depth is milder (see supplement).

4 DISCUSSION

We have presented a framework for categorizing neural scaling laws, along with derivations that
help to explain their very general origins. Crucially, our predictions agree with empirical findings in
settings which have often proven challenging for theory – deep neural networks on real datasets. The
variance-scaling regime yields, for smooth test losses, a universal prediction of αD = 1 (for D ≫ _P_ )
and αW = 1 (for w ≫ _D). The resolution-limited regime yields exponents whose numerical value is_
variable and data and model dependent.

There are many intriguing directions for future work; amongst these, we highlight one in particular.
The invariance of the dataset scaling exponent to superclassing (Figure 3) suggests that deep networks
may be largely learning properties of the input data manifold – akin to unsupervised learning –
rather than significant task-specific structure, which may shed light on the versatility of learned deep
network representations for different downstream tasks. This begs to be explored further.

**Limitations** One limitation is that our theoretical results are asymptotic, while experiments are
performed with finite models and datasets. This is apparent in the resolution-limited regime which
requires a hierarchy (D ≫ _P or P ≫_ _D). In Figures 1a and 2a top-right (bottom-left), we see_
a breakdown of the predicted scaling behavior as D (P ) become large and the hierarchy is lost.
Furthermore in the resolution-limited regime for deep networks, our theoretical tools rely on positing
the existence of a data manifold. A precise definition of the data manifold, however, is lacking forcing
us to use imperfect proxies, such as nearest neighbor distances of final embedding layers.


-----

**Ethics Statement** Work on scaling laws provides an opportunity for discussion on how to define and
measure progress in machine learning. The values of exponents allow us to estimate expected gains
that come from increases in scale of dataset, model, and compute. Applying similar considerations
to other metrics (i.e. transfer, bias, robustness) in principle allows one to quantify whether and how
models are improving or degrading with scale and at what environmental or computational cost. On
the other hand, one may require that truly non-trivial progress in machine learning be progress that
occurs modulo scale: namely, improvements in performance across different tasks that are not simple
extrapolations of existing behavior. And perhaps the right combinations of algorithmic, model, and
dataset improvements can lead to emergent behavior at new scales. Large language models such as
GPT-3 (Fig. 1.2 in Brown et al. (2020)) have exhibited this in the context of few-shot learning. We
hope our work spurs further research in understanding and controlling neural scaling laws.

REFERENCES

Ben Adlam and Jeffrey Pennington. The Neural Tangent Kernel in high dimensions: Triple descent
and a multi-scale theory of generalization. In International Conference on Machine Learning, pp.
74–84. PMLR, 2020a.

Ben Adlam and Jeffrey Pennington. Understanding double descent requires a fine-grained biasvariance decomposition. Advances in Neural Information Processing Systems, 33, 2020b.

Madhu S Advani and Andrew M Saxe. High-dimensional dynamics of generalization error in neural
networks. arXiv preprint arXiv:1710.03667, 2017.

Madhu S Advani, Andrew M Saxe, and Haim Sompolinsky. High-dimensional dynamics of generalization error in neural networks. Neural Networks, 132:428–446, 2020.

Subutai Ahmad and Gerald Tesauro. Scaling and generalization in neural networks: a case study. In
_Advances in neural information processing systems, pp. 160–168, 1989._

Alnur Ali, J Zico Kolter, and Ryan J Tibshirani. A continuous-time view of early stopping for least
squares regression. In The 22nd International Conference on Artificial Intelligence and Statistics,
pp. 1370–1378, 2019.

Anders Andreassen and Ethan Dyer. Asymptotics of wide convolutional neural networks. arxiv
_preprint arXiv:2008.08675, 2020._

Peter J Bickel, Bo Li, et al. Local polynomial regression on unknown manifolds. In Complex datasets
_and inverse problems, pp. 177–186. Institute of Mathematical Statistics, 2007._

Devansh Bisla, Apoorva Nandini Saridena, and Anna Choromanska. A theoretical-empirical approach
to estimating sample complexity of dnns. arXiv preprint arXiv:2105.01867, 2021.

Blake Bordelon, Abdulkadir Canatar, and Cengiz Pehlevan. Spectrum dependent learning curves in
kernel regression and wide neural networks. In International Conference on Machine Learning,
pp. 1024–1034. PMLR, 2020.

James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and
Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL
[http://github.com/google/jax.](http://github.com/google/jax)

Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020.

Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan. Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks. Nature
_communications, 12(1):1–12, 2021._


-----

Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
In Advances in Neural Information Processing Systems, pp. 2937–2947, 2019.

Omry Cohen, Or Malka, and Zohar Ringel. Learning curves for deep neural networks: a gaussian
field theory perspective. arXiv preprint arXiv:1906.05301, 2019.

David Cohn and Gerald Tesauro. Can neural networks do better than the vapnik-chervonenkis
bounds? In Advances in Neural Information Processing Systems, pp. 911–917, 1991.

David de Laat. Approximating manifolds by meshes: asymptotic bounds in higher codimension.
_Master’s Thesis, University of Groningen, Groningen, 2011._

Ethan Dyer and Guy Gur-Ari. Asymptotics of wide networks from feynman diagrams. In International
_[Conference on Learning Representations, 2020. URL https://openreview.net/forum?](https://openreview.net/forum?id=S1gFvANKDS)_
[id=S1gFvANKDS.](https://openreview.net/forum?id=S1gFvANKDS)

Stephane d’Ascoli, Maria Refinetti, Giulio Biroli, and Florent Krzakala. Double trouble in double´
descent: Bias and variance (s) in the lazy regime. In International Conference on Machine
_Learning, pp. 2280–2290. PMLR, 2020._

William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter
models with simple and efficient sparsity, 2021.

JC Ferreira and VA Menegatto. Eigenvalues of integral operators defined by smooth positive definite
kernels. Integral Equations and Operator Theory, 64(1):61–81, 2009.

Adria Garriga-Alonso, Laurence Aitchison, and Carl Edward Rasmussen. Deep convolutional`
networks as shallow gaussian processes. In International Conference on Learning Representations,
2019.

Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun, Stephane d’Ascoli,´
Giulio Biroli, Clement Hongler, and Matthieu Wyart. Scaling description of generalization with´
number of parameters in deep learning. Journal of Statistical Mechanics: Theory and Experiment,
2020(2):023401, 2020.

Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc Mezard, and Lenka Zdeborov´ a. General-´
isation error in learning with random features and the hidden manifold model. In International
_Conference on Machine Learning, pp. 3452–3462. PMLR, 2020._

Gabriel Goh. Why momentum really works. Distill, 2017. doi: 10.23915/distill.00006. URL
[http://distill.pub/2017/momentum.](http://distill.pub/2017/momentum)

Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi,
and Kevin Swersky. Your classifier is secretly an energy based model and you should treat
[it like one. In International Conference on Learning Representations, 2020. URL https:](https://openreview.net/forum?id=Hkxzx0NtDB)
[//openreview.net/forum?id=Hkxzx0NtDB.](https://openreview.net/forum?id=Hkxzx0NtDB)

Roger Grosse. University of Toronto CSC2541 winter 2021 neural net training dynamics, lecture
[notes, 2021. URL https://www.cs.toronto.edu/˜rgrosse/courses/csc2541_](https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2021)
[2021.](https://www.cs.toronto.edu/~rgrosse/courses/csc2541_2021)

Boris Hanin and Mihai Nica. Finite depth and width corrections to the neural tangent kernel. In
_[International Conference on Learning Representations, 2020. URL https://openreview.](https://openreview.net/forum?id=SJgndT4KwB)_
[net/forum?id=SJgndT4KwB.](https://openreview.net/forum?id=SJgndT4KwB)

Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning: data
_mining, inference, and prediction. Springer Science & Business Media, 2009._

Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in highdimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019.


-----

Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas
Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2020. URL
[http://github.com/google/flax.](http://github.com/google/flax)

Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo
Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec Radford,
Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, and Sam McCandlish. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701,
2020.

Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad,
Md Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable,
empirically. arXiv preprint arXiv:1712.00409, 2017.

Wei Huang, Weitao Du, Richard Yi Da Xu, and Chunrui Liu. Implicit bias of deep linear networks in
the large learning rate phase. arXiv preprint arXiv:2011.12547, 2020.

Marcus Hutter. Learning curve theory. arXiv preprint arXiv:2102.04074, 2021.

Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural Tangent Kernel: Convergence and
generalization in neural networks. In Advances in Neural Information Processing Systems, 2018.

Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott
Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.
_arXiv preprint arXiv:2001.08361, 2020._

Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,
and Neil Houlsby. Big transfer (bit): General visual representation learning. arXiv preprint
_arXiv:1912.11370, 6(2):8, 2019._

Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do better imagenet models transfer better?
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
2661–2671, 2019.

Thomas Kuhn. Eigenvalues of integral operators with smooth positive definite kernels.¨ _Archiv der_
_Mathematik, 49(6):525–534, 1987._

Jaehoon Lee, Yasaman Bahri, Roman Novak, Sam Schoenholz, Jeffrey Pennington, and Jascha
Sohl-dickstein. Deep neural networks as Gaussian processes. In International Conference on
_Learning Representations, 2018._

Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha SohlDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in Neural Information Processing Systems, 2019.

Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, and
Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. Advances in
_Neural Information Processing Systems, 33, 2020._

Elizaveta Levina and Peter J Bickel. Maximum likelihood estimation of intrinsic dimension. In
_Advances in neural information processing systems, pp. 777–784, 2005._

Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large
learning rate phase of deep learning: the catapult mechanism. arXiv preprint arXiv:2003.02218,
2020.

Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv
_preprint arXiv:1608.03983, 2016._


-----

Andreas Loukas. How close are the eigenvectors of the sample and actual covariance matrices? In
_International Conference on Machine Learning, pp. 2228–2237. PMLR, 2017._

Dorthe Malzahn and Manfred Opper. Learning curves for gaussian processes regression: A framework¨
for good approximations. Advances in neural information processing systems, pp. 273–279, 2001.

Dorthe Malzahn and Manfred Opper. A variational approach to learning curves. In T. Dietterich,¨
S. Becker, and Z. Ghahramani (eds.), Advances in Neural Information Processing Systems, vol[ume 14, pp. 463–469. MIT Press, 2002. URL https://proceedings.neurips.cc/](https://proceedings.neurips.cc/paper/2001/file/26f5bd4aa64fdadf96152ca6e6408068-Paper.pdf)
[paper/2001/file/26f5bd4aa64fdadf96152ca6e6408068-Paper.pdf.](https://proceedings.neurips.cc/paper/2001/file/26f5bd4aa64fdadf96152ca6e6408068-Paper.pdf)

Dorthe Malzahn and Manfred Opper. Learning curves and bootstrap estimates for inference with¨
gaussian processes: A statistical mechanics study. Complexity, 8(4):57–63, 2003.

Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahramani.
Gaussian process behaviour in wide deep neural networks. In International Conference on Learning
_Representations, 2018._

P McCullagh and John A Nelder. Generalized Linear Models, volume 37. CRC Press, 1989.

Song Mei and Andrea Montanari. The generalization error of random features regression: Precise
asymptotics and double descent curve. arXiv preprint arXiv:1908.05355, 2019.

Preetum Nakkiran. More data can hurt for linear regression: Sample-wise double descent. arXiv
_preprint arXiv:1912.07242, 2019._

Preetum Nakkiran and Yamini Bansal. Distributional generalization: A new kind of generalization.
_arXiv preprint arXiv:2009.08092, 2020._

Preetum Nakkiran, Behnam Neyshabur, and Hanie Sedghi. The deep bootstrap framework: Good
online learners are good offline generalizers. In International Conference on Learning Representa_[tions, 2021. URL https://openreview.net/forum?id=guetrIHLFGI.](https://openreview.net/forum?id=guetrIHLFGI)_

Radford M. Neal. Bayesian Learning for Neural Networks. PhD thesis, University of Toronto, Dept.
of Computer Science, 1994.

Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A. Abolafia,
Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with many
channels are gaussian processes. In International Conference on Learning Representations, 2019.

Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A. Alemi, Jascha Sohl-Dickstein,
and Samuel S. Schoenholz. Neural Tangents: Fast and easy infinite neural networks in python. In
_[International Conference on Learning Representations, 2020. URL https://github.com/](https://github.com/google/neural-tangents)_
[google/neural-tangents.](https://github.com/google/neural-tangents)

Giorgio Parisi. A sequence of approximated solutions to the SK model for spin glasses. Journal of
_Physics A: Mathematical and General, 13(4):L115, 1980._

Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. Advances in Neural Information Processing Systems, 32:
8026–8037, 2019.

Ali Rahimi and Benjamin Recht. Weighted sums of random kitchen sinks: replacing minimization
with randomization in learning. In Nips, pp. 1313–1320. Citeseer, 2008.

JB Reade. Eigenvalues of positive definite kernels. SIAM Journal on Mathematical Analysis, 14(1):
152–157, 1983.

Ryan M Rifkin and Ross A Lippert. Notes on regularized least squares, 2007.


-----

Sam Ritchie, Ambrose Slone, and Vinay Ramasesh. Caliban: Docker-based job manager for
reproducible workflows. Journal of Open Source Software, 5(53):2403, 2020. doi: 10.21105/joss.
[02403. URL https://doi.org/10.21105/joss.02403.](https://doi.org/10.21105/joss.02403)

Jonathan S. Rosenfeld, Jonathan Frankle, Michael Carbin, and Nir Shavit. On the predictability of
pruning across scales. arXiv preprint arXiv:2006.10621, 2020a.

Jonathan S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. A constructive prediction
of the generalization error across scales. In International Conference on Learning Representations,
2020b.

Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information
propagation. International Conference on Learning Representations, 2017.

Vaishaal Shankar, Alex Chengyu Fang, Wenshuo Guo, Sara Fridovich-Keil, Ludwig Schmidt,
Jonathan Ragan-Kelley, and Benjamin Recht. Neural kernels without tangents. In International
_Conference on Machine Learning, 2020._

Utkarsh Sharma and Jared Kaplan. A neural scaling law from the dimension of the data manifold.
_arXiv preprint arXiv:2004.10802, 2020._

Peter Sollich. Learning curves for gaussian processes. In Proceedings of the 11th International
_Conference on Neural Information Processing Systems, pp. 344–350, 1998._

Peter Sollich and Anason Halees. Learning curves for gaussian process regression: Approximations
and bounds. Neural computation, 14(6):1393–1428, 2002.

Stefano Spigler, Mario Geiger, and Matthieu Wyart. Asymptotic learning curves of kernel methods:
empirical data versus teacher–student paradigm. Journal of Statistical Mechanics: Theory and
_Experiment, 2020(12):124001, 2020._

Michael L Stein. Interpolation of Spatial Data: Some Theory for Kriging. Springer Science &
Business Media, 1999.

Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks.
In International Conference on Machine Learning, pp. 6105–6114. PMLR, 2019.

Matthew J Urry and Peter Sollich. Replica theory for learning curves for gaussian processes on
random graphs. Journal of Physics A: Mathematical and Theoretical, 45(42):425005, 2012.

Hermann Weyl. Das asymptotische verteilungsgesetz der eigenwerte linearer partieller differentialgleichungen (mit einer anwendung auf die theorie der hohlraumstrahlung). Mathematische Annalen,
71(4):441–479, 1912.

Christopher KI Williams and Francesco Vivarelli. Upper and lower bounds on the learning curve for
gaussian processes. Machine Learning, 40(1):77–102, 2000.

Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel Schoenholz, and Jeffrey Pennington. Dynamical isometry and a mean field theory of CNNs: How to train 10,000-layer vanilla
convolutional neural networks. In International Conference on Machine Learning, 2018.

Sho Yaida. Non-Gaussian processes and neural networks at finite widths. In Mathematical and
_Scientific Machine Learning Conference, 2020._

Greg Yang. Scaling limits of wide neural networks with weight sharing: Gaussian process behavior,
gradient independence, and neural tangent kernel derivation. arXiv preprint arXiv:1902.04760,
2019.

Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision
_Conference, 2016._


-----

A EXPERIMENTAL SETUP

**Figure 1 (top-left)**

Experiments utilize relatively small models, with the number of trainable parameteters P ∼O(1000),
trained with full-batch gradient descent (GD) and small learning rate on datasets of size D ≫ _P_ . Each
data point in the figure represents an average over subsets of size D sampled from the full dataset.
Experiments are done using Neural Tangents (Novak et al., 2020) based on JAX (Bradbury et al.,
2018). All experiment except denoted as (CNN), use 3-layer, width-8 fully-connected networks. CNN
architecture used is Myrtle-5 network (Shankar et al., 2020) with 8 channels. Relu activation function
with critical initialization (Schoenholz et al., 2017; Lee et al., 2018; Xiao et al., 2018) was used.
Unless specified softmax-cross-entropy loss was used. We performed full-batch gradient descent
update for all dataset sizes without L2 regularization. 20 different training data sampling seeds were
averaged for each point. For fully-connected networks, input pooling of size 4 was performed for
CIFAR-10/100 dataset and pooling of size 2 was performed for MNIST and Fashion-MNIST dataset.
This was to reduce number of parameters in the input layer (# of pixels × width) which can be quite
large even for small width networks.

**Figure 1 (top-right) All experiments were performed using a Flax (Heek et al., 2020) implementation**
of Wide ResNet 28-10 (Zagoruyko & Komodakis, 2016), and performed using the Caliban experiment
manager (Ritchie et al., 2020). Models were trained for 78125 total steps with a cosine learning rate
decay (Loshchilov & Hutter, 2016) and an augmentation policy consisting of random flips and crops.
We report final loss, though we found no qualitative difference between using final loss, best loss,
final accuracy or best accuracy (see Figure S1).

**Figure 1 (bottom-left) The setup was identical to Figure 1 (top-right) except that the model consid-**
ered was a depth 10 residual network with varying width.

**Figure 1 (bottom-right)**

Experiments are done using Neural Tangents. All experiments use 100 training samples and twohidden layer fully-connected networks of varying width (ranging from w = 64 to W = 11, 585) with
Relu nonlinearities unless specified as Erf. Full-batch gradient descent and cross-entropy loss were
used unless specified as MSE, and the figure shows curves from a random assortment of training
times ranging from 100 to 500 steps (equivalently, epochs). Training was done with learning rates
small enough so as to avoid catapult dynamics (Lewkowycz et al., 2020) and no L2 regularization; in
such a setting, the infinite-width learning dynamics is known to be equivalent to that of linearized
models (Lee et al., 2019). Consequently, for each random initialization of the parameters, the test
loss of the finite-width linearized model was additionally computed in the identical training setting.
This value approximates the limiting behavior L(∞) known theoretically and is subtracted off from
the final test loss of the (nonlinear) neural network before averaging over 50 random initializations to
yield each of the individual data points in the figure.

A.1 DEEP TEACHER-STUDENT MODELS

The teacher-student scaling with dataset size (figure S2) was performed with fully-connected teacher
and student networks with two hidden layers and widths 96 and 192, respectively, using PyTorch
(Paszke et al., 2019). The inputs were random vectors sampled uniformly from a hypercube of
dimension d = 2, 3, · · ·, 9. To mitigate noise, we ran the experiment on eight different random seeds,
fixing the random seed for the teacher and student as we scanned over dataset sizes. We also used a
fixed test dataset, and a fixed training set, which was sub-sampled for the experiments with smaller D.
The student networks were trained using MSE loss and Adam optimizer with a maximum learning
rate of 3 × 10[−][3], a cosine learning rate decay, and a batch size of 64, and 40, 000 steps of training.
The test losses were measured with early stopping. We combine test losses from different random
seeds by averaging the logarithm of the loss from each seed.


-----

|Col1|CIFAR-10|Col3|
|---|---|---|
|||final loss best loss final error|
|||final l best final|
|||best error|
||||

|Col1|CIFAR-100|Col3|
|---|---|---|
||fi b fi b|nal loss est loss nal error est error|
||||

|Col1|Col2|SVHN|Col4|
|---|---|---|---|
||||final loss best loss final error best error|
|||||

|Col1|Col2|final loss best loss final error best error|
|---|---|---|
||||


CIFAR-10 CIFAR-100

final loss final loss
best loss best loss
final error final error

10[0] best error best error

10[0]

10 1

10[2] 10[3] 10[4] 10[2] 10[3] 10[4]

SVHN FashionMNIST

final loss final loss
best loss best loss
final error final error
best error best error

10 1

10 1

10[3] 10[4] 10[5] 10[3] 10[4]


Figure S1: Alternate metrics and stopping conditions We find similar scaling behavior for both the loss and
error, and for final and best (early stopped) metrics.

In our experiments, we always use inputs that are uniformly sampled from a d-dimensional hypercube,
following the setup of Sharma & Kaplan (2020). They also utilized several intrisic dimension (ID)
estimation methods and found the estimates were close to the input dimension, so we simply use the
latter for comparisons. For the dataset size scans we used randomly initialized teachers with width
96, and students with width 192. We found similar results with other network sizes.

The final scaling exponents and input dimensions are show in the bottom of Figure 1b. We used the
same experiments for the top of that figure, interpolating the behavior of both teacher and a set of
students between two fixed training points. The students only differed by the size of their training
sets, but had the same random seeds and were trained in the same way. In that figure the input space
dimension was four.

Finally, we also used a similar setup to study variance-limited exponents and scaling. In that case we
used much smaller models, with 16-dimensional hidden layers, and a correspondingly larger learning
rate. We then studied scaling with D again, with results pictured in Figure 1a.

A.2 CNN ARCHITECTURE FOR RESOLUTION-LIMITED SCALING

Figure 1b includes data from CNN architectures trained on image datasets. The architectures are
summarized in Table 1. We used Adam optimizer for training, with cross-entropy loss. Each network
was trained for long enough to achieve either a clear minimum or a plateau in test loss. Specifically,
CIFAR10, MNIST and Fashion MNIST were trained for 50 epochs, CIFAR100 was trained for 100
epochs and SVHN was trained for 10 epochs. The default Keras training parameters were used. In
case of SVHN we included the additional images as training data. We averaged (in log space) over


-----

Teacher/Student Dataset Size

10 4 9

10 5 8

7

10 6

6

Loss 10 7 5

Input Dimension

10 8 4

10 9 3

2

2[6] 2[7] 2[8] 2[9] 2[10] 2[11] 2[12] 2[13]

Dataset Size


Figure S2: This figure shows scaling trends of MSE loss with dataset size for teacher/student models. The
exponents extracted from these fits and their associated input-space dimensionalities are shown in Figure 1.



|Layer|Col2|Width|Col4|Layer|Col6|Col7|Width|
|---|---|---|---|---|---|---|---|
|CNN window (3, 3)||50||CNN window (3, 3)|||50|
|2D Max Pooling (2, 2)||||2D Max Pooling (2, 2)||||
|CNN window (3, 3)||100||CNN window (3,3)|||100|
|2D Max Pooling (2, 2)||||2D Max Pooling (2, 2)||||
|CNN window (3, 3)||100||CNN window (3, 3)|||200|
|Dense||64||Dense|||256|
|Dense||10||Dense|||100|
||Layer||||Width|||
||CNN window (3, 3)||||64|||
||2D Max Pooling (2, 2)|||||||
||CNN window (3, 3)||||64|||
||2D Max Pooling (2, 2)|||||||
||Dense||||128|||
||Dense||||10|||


Table 1: CNN architectures for CIFAR10, MNIST, Fashion MNIST (left), CIFAR100 (center) and SVHN
(right)

20 runs for CIFAR100 and CIFAR10, 16 runs for MNIST, 12 runs for Fashion MNIST, and 5 runs
for SVHN. The results of these experiments are shown in Figure S3.

The measurement of input-space dimensionality for these experiments was done using the nearestneighbour algorithm, described in detail in Appendix B and C in Sharma & Kaplan (2020). We used
2, 3 and 4 nearest neighbors and averaged over the three.

A.3 TEACHER-STUDENT EXPERIMENT FOR SCALING OF LOSS WITH MODEL SIZE

We replicated the teacher-student setup in Sharma & Kaplan (2020) to demonstrate the scaling of
loss with model size. The resulting variation of −4/αP with input-space dimensionality is shown in
figure S4. In our implementation we averaged (in log space) over 15 iterations, with a fixed, randomly
generated teacher.

B EFFECT OF ASPECT RATIO ON SCALING EXPONENTS

We trained Wide ResNet architectures of various widths and depths on CIFAR-10 accross dataset
sizes. We found that the effect of depth on dataset scaling was mild for the range studied, while the
effect of width impacted the scaling behavior up until a saturating width, after which the scaling
behavior fixed. See Figure S5.


-----

CIFAR10: Loss scaling with Dataset Size FashionMNIST: Loss scaling with Dataset Size

D[ : 0.198] D[ : 0.207]

3 × 10[0] 6 × 10 1

2 × 10[0]

4 × 10 1

Test Loss Test Loss

3 × 10 1

10[0]

10[2] 10[3] 10[4] 10[3] 10[4]

Dataset Size Dataset Size

MNIST: Loss scaling with Dataset Size CIFAR100: Loss scaling with Dataset Size

D[ : 0.397] D[ : 0.164]

5 × 10[0]

4 × 10[0]

Test Loss 10 1 Test Loss

3 × 10[0]

10[3] 10[4] 10[3] 10[4]

Dataset Size Dataset Size

SVHN: Loss scaling with Dataset Size

D[ : 0.242]

10[0]

Test Loss

10[3] 10[4] 10[5]

Dataset Size


Figure S3: This figure shows scaling trends of CE loss with dataset size for various image datasets. The
exponents extracted from these fits and their associated input-space dimensionalities are shown in Figure 1.

Teacher/Student Model Size Exponents

d = 4/ P

14

12

P 10

4/

8

6

4

4 6 8 10 12

Dimension


Figure S4: This figure shows the variation of αP with the input-space dimension. The exponent αP is the
scaling exponent of loss with model size for teacher-student setup.


-----

|Col1|Col2|Col3|
|---|---|---|
||D: 0.42 D: 0.50 D: 0.54 D: 0.58 D: 0.58||

|Col1|Col2|Col3|
|---|---|---|
||D: 0.48 D: 0.55 D: 0.58 D: 0.58||


Width factor Depth

CIFAR-10 varying width (d=28) CIFAR-10 varying depth (k=10)

12 40

10[0] 10

10[0]

28

Loss Loss

D[: 0.42]

D[: 0.50] D[: 0.48]

4

D[: 0.54] D[: 0.55] 16

D[: 0.58] D[: 0.58]

D[: 0.58] 2 D[: 0.58]

1 10

10[3] 10[4] 10[3] 10[4]

Dataset size (D) Dataset size (D)


Figure S5: Effect of aspect ratio on dataset scaling We find that for WRN-d-k trained on CIFAR-10, varying
depth from 10 to 40 has a relatively mild effect on scaling behavior, while varying the width multiplier, k, from 1
to 12 has a more noticeable effect, up until a saturating width.

C PROOF OF THEOREM 1

We now prove Theorem 1 repeated below for convenience.

**Theorem 1. Let ℓ(f** ) be the test loss as a function of network output, (L = E [ℓ(f )]), and let
_fT be the network output after T training steps, thought of as a random variable over weight_
_initialization, draws of the training dataset, and optimization seed. Further let fT be concentrating_
_with E[(fT −_ E[fT ])[k]] = O (ϵ) ∀k ≥ 2. If ℓ _is a finite degree polynomial, or has bounded second_
_derivative, or is 2-H¨older, then E [ℓ(fT )] −_ _ℓ_ (E [fT ]) = O(ϵ).

_Proof. Case 1 – finite degree polynomial: In this case, we can write,_


_ℓ[(][k][)]_ (E [fT ])

(fT E [fT ])[k] _,_ (S1)
_k!_ _−_


_ℓ(fT ) −_ _ℓ(E [fT ]) =_


_k=1_


where K is the polynomial degree and ℓ[(][k][)] is the k-th derivative of ℓ. Taking the expectation of (S1)
and using the moment scaling proves the result.

_Case 2 – bounded second derivative: The quadratic mean value theorem states that for any fT, there_
exists a c such that,

_ℓ(fT )_ _ℓ(E [fT ]) = (fT_ E [fT ]) ℓ[′](E [fT ]) + [1] (S2)
_−_ _−_ 2 _[ℓ][′′][(][c][) (][f][T][ −]_ [E][ [][f][T][ ])][2][ .]

Taking the expectation of (S2) and using the fact that f _[′′](c) is bounded yields the desired result._

_Case 3 – 2-H¨older: Lastly, the loss being 2-H¨older means we may write,_

_ℓ(fT ) −_ _ℓ(E [fT ]) ≤|ℓ(fT ) −_ _ℓ(E [fT ])| ≤_ _Kℓ_ (fT − E [fT ])[2] _._ (S3)

Again, taking the expectation of this inequality completes the proof.

**A note on loss variance** Theorem 1 concerns the mean loss, however we would also like to
understand if this scaling holds for typical instances. This can be understood by examining how the
variance of the loss or altetnatively how E [|ℓ (fT ) − _ℓ_ (E [fT ])|] scales.

For Case 3 – 2-Holder loss¨, we can rerun the argument of Theorem 1, using (S3) to yield
E [|ℓ (fT ) − _ℓ_ (E [fT ])|] = O (ϵ).

For Cases 1 and 2, we can attempt to apply the same argument as in the proof. This almost works. In
particular, using Holder’s inequality,¨ E[(fT − E[fT ])[k]] = O (ϵ) ∀k ≥ 2 implies E[|fT − E[fT ]|[k]] =


-----

_O (ϵ) ∀k ≥_ 2. Taking the absolute value and expectation of (S1) or (S2) then gives

E [|ℓ (fT ) − _ℓ_ (E [fT ])|] ≤|ℓ[′] (E [fT ])| E [|fT − E [fT ]|] + O (ϵ) . (S4)

In general, the above assumptions on ℓ and fT imply only that E [|fT − E [fT ]|] = O ([√]ϵ) and thus
typical instances of the loss will exhibit a less dramatic scaling with ϵ than the mean. If we further
assume, however, that fT on average has been trained such as to be sufficiently close to a local
minimum of the loss, such that |ℓ[′] (E [fT ])| = O ([√]ϵ), then typical instances will also obey the O (ϵ)
scaling.

D VARIANCE-LIMITED DATASET SCALING

In this section, we expand on our discussion of the variance-limited dataset scaling, L(D) −
limD→∞ _L(D) = O_ _D[−][1][]. We first explain some intuition for why this behavior might be_
expected for sufficiently smooth loss. We then derive it explicitly for losses that are polynomial in the
 
weights. Finally, we present non-smooth examples where the scaling can be violated either by having
unbounded loss, or first derivative.

D.1 INTUITION

At a high level, the intuition is as follows. For any fixed value of weights, θ, the training loss with
_D training points (thought of as a random variable over draws of the dataset), Ltrain[θ] concentrates_
around the population loss Lpop[θ], with variance that scales as O _D[−][1][]._

Our optimization procedure can be thought of as a map from initial weights and training loss to 
final weights Op : (θ0, Ltrain[θ]) → _θT . If this map is sufficiently smooth – for instance satisfying_
the assumptions of Theorem 1 or well approximated by a Taylor series about all ED [Ltrain[θt]]
– then the output, θT, will also concentrate around its infinite D limit with variance scaling as
_O_ _D[−][1][]. Finally, if the population loss is also sufficiently smooth, the test loss for a model_
trained on D data points averaged over draws of the dataset, L(D) = ED [Lpop[θT ]], satisfies
 
_L(D) −_ limD→∞ _L(D) = O_ _D[−][1][]. We now walk through this in a little more detail._
 

**Early time** We can follow this intuition a bit more explicitly for the first few steps of gradient
descent. As the training loss at initialization, Ltrain[θ0], is a sample average over D i.i.d draws, it
concentrates around the population loss Lpop[θ0] with variance O _D[−][1][]. As a result, the initial_
gradient, g0 = _[∂L]∂θ[train]0_ [will also concentrate with][ O] _D[−][1][]_ variance and so will the weights at time  1,

_θ1 = θ0_ _ηg0. The training loss at time step 1, is then given by_
_−_  

_Ltrain[θ1] = Ltrain[θ0 −_ _g0] ._ (S5)

If Ltrain is sufficiently smooth around θ0 − ED [g0], then we get that Ltrain[θ1] concentrates around
_Ltrain[θ1] with O_ _D[−][1][]_ variance. We can keep bouncing back and forth between gradient (or
equivalently weights) and training loss for any number of steps T which does not scale with D.
 
Plugging this final θT into the population loss and taking the expectation over draws of the training
set, L(D) = ED [Lpop[θT ]]. If Lpop is also sufficiently smooth, this yields L(D) limD _L(D) =_
_−_ _→∞_
_O_ _D[−][1][]._

Here we have used the term sufficiently smooth. A sufficient set of criteria are given in Theorem 1; 
however this is likely too restrictive. Indeed, any set of train and population loss for which a Taylor
series (or asymptomatic series with optimal truncation) give an O _D[−][1][]_ error around the training
points ED [θt=0...T ] will have this behavior.
 

**Local minimum** The above intuition relied on training for a number of steps that was fixed as D is
taken large. Here we present some alternative intuition for the variance-limited scaling at late times,
as training approaches a local minimum in the loss. For simplicity we discuss a one-dimensional loss.

Consider a local minimum, θ[∗], of the population loss. As D is taken large, with high probability, the
training loss will have a local minimum, _θ[¯][∗], such that |θ[∗]_ _−_ _θ[¯][∗]| = O_ _D[−][1][]. One way to see this,_
 


-----

is to note that for a generic local minimum the first derivative changes sign, i.e. we can find θ1, θ2
such that θ1 < θ[∗] _< θ2 and either L[′]pop[[][θ][1][]][ <][ 0][, L][′]pop[[][θ][2][]][ >][ 0][ or][ L][′]pop[[][θ][2][]][ <][ 0][, L][′]pop[[][θ][1][]][ >][ 0][. To be]_
concrete let’s focus on the first case (the argument will be identical in either case). As D becomes
large, the probability that the training loss at θ1 and θ2 differs significantly from the population loss
approaches zero. This can be seen from Markov’s inequality, where, P _L[′]train[[][θ][]][ −]_ _[L]pop[′]_ [[][θ][]] _> a_ _≤_

VarD(L[′]train[[][θ][]][)]

_a[2]_, or more dramatically from Hoeffding’s inequality (assuming bounded  _Ltrain_ _Lpop_
_−_
lying in an interval of size I)

_P_ _L[′]train[[][θ][]][ −]_ _[L][′]pop[[][θ][]]_ _> a_ _≤_ 2e[−] _I[2]_ _[D][2][a][2]_ _._ (S6)
  

Here to have non-vanishing probability as we take D large, L[′]train[[][θ][1][]][ and][ L][′]train[[][θ][2][]][ must be closer]
than O _D[−][1][]. If θ1 and θ2 are taken to be O_ _D[0][], then L[′]train_ [must change sign, indicating an]
extremum of Ltrain; however we can do even better. If we assume Ltrain is Lipshitz about θ[∗] then we
   
can still ensure a sign change even if |θ1 − _θ[∗]|, |θ2 −_ _θ[∗]| = O_ _D[−][1][]. Using concentration of L[′′]train[[][θ][]]_
ensures the extremum is a local minimum. For non-generic minimum (i.e. vanishing first derivatives)
 
we can apply the same arguments to higher order derivatives (assuming they exist) of Lpop. Thus for
a local minimum of Lpop, with high probability Ltrain will have a corresponding minimum within a
distance O _D[−][1][]_

If we now consider an initialization procedure,  _θ0, and training procedure such that training converges_
to the local minimum of the training loss, _θ[¯][∗], and that the population loss is sufficiently smooth about_
_θ[∗]_ (e.g. Lipshitz), then ED[Ltrain[θ[¯][∗]]−Lpop[θ[∗]]] = ED[Ltrain[θ[¯][∗]]−Lpop[θ[¯][∗]]]+ED[Lpop[θ[¯][∗]]−Lpop[θ[∗]]].
The first term vanishes, while the second is O(D[−][1]). If we further assume that this happens on
average over choices of θ0 then we expect L(D) limD _L(D) =_ _D[−][1][]._
_−_ _→∞_ _O_
 

**SGD** At first blush it may be surprising that the variance-limited scaling holds even for mini-batch
training. Indeed in this case, there is batch noise that comes in at a much higher scale than any
variance due to the finite training set size. Indeed, the effect of mini-batching changes the final test
loss, however if we fix the SGD procedure or average over SGD seeds, as we take D large, we can
still ask how the training loss for a model trained under SGD on a training set of size D differs from
that for a model trained under SGD on an infinite training set.

To see this, we fist consider averaging over minibatches of size B, but where points are drawn i.i.d.
with replacement. If we denote the batch at step t by Bt and the average over independent draws of
this batch by EB [•], then note we can translate moments with respect to batch draws with empirical
averages over the entire training set. Explicitly, consider ca and da potentially correlated, but each
drawn i.i.d. within a batch. We have that,


_D_

= [1] _ca_

_D_

_a=1_

X

= 1
_−_ _B[1]_



EB


_ca_

_B_

_aX∈Bt_ #

_da′_
_aX[′]∈Bt_ !#


+ [1]


EB


_ca_
_aX∈Bt_


_ca_
_a=1_

X


_da′_
_a[′]=1_

X


_cada ._
_a=1_

X

(S7)


This procedure means, after taking an average over draws of SGD batch, rather than thinking about
a function of mini-batch averages, we can equivalently consider a modified function, with explicit
dependence on the batch size, but that is only a function of empirical means over the training set. We
can thus recycle the above intuition for the scaling of smooth functions of empirical means.

The above relied on independently drawing every sample from every batch. At the other extreme, we
can consider drawing batches without shuffling and increasing training set size by B datapoints at
a time, so as to keep the initial set of batches in an epoch fixed. In this case, the first deviation in
training between a dataset of size D and one of size D + B happens at the last batch in the first epoch
after processing D datapoints.


-----

As an extreme example, consider the case where D > BT . In this case, as we only take T
steps, the loss is constant for all D > BT and so limD→∞ _L(D; T_ ; B) = L(BT ; T ; B) and thus
_L(D > BT_ ) − limD→∞ _L(D) = 0 (and in particular is trivially O_ _D[−][1][])._
 

D.2 POLYNOMIAL LOSS

Before discussing neural network training we review the concentration behavior of polynomials of
sample means.

**Lemma 1. Let ¯c[(][i][)]** = _D1_ _Da=1_ _[c]a[(][i][)]_ _for i = 0 . . . J be empirical means, over D i.i.d. draws of_

_c[(]a[i][)]_ _[and let][ c][(][i][)][ denote the distributional mean. Further, let][ X][ = (¯]c[(0)])[k][0]_ (¯c[(1)])[k][1] (¯c[(][J][)])[k][J] _be a_
P _· · ·_
_monomial in the sample means. Then X concentrates with moments O_ _D[−][1][],_

ED _X −_ (c[(0)])[k][0] (c[(1)])[k][1] _· · · (c[(][J][)])[k][J]_ [][n][i] = O  D[−][1][] _._ (S8)

_Here, ED [•] denotes the average over independent draws ofh_ _D samples. _

_Proof. To establish this we can proceed by direct computation._

ED _X −_ (c[(0)])[k][0] (c[(1)])[k][1] _· · · (c[(][J][)])[k][J]_ [][n][i]
h _n_ _n_ (S9)

= ( 1)[n][−][p] ED [X _[p]]_ (c[(0)])[k][0] (c[(1)])[k][1] (c[(][J][)])[k][J] [][n][−][p]

_−_ _p_ _· · ·_

Xp=0   

Each term in the sum can be computed using

ED [X _[p]] = ED_ (¯c[(0)])[pk][0] (¯c[(1)])[pk][1] _· · · (¯c[(][J][)])[pk][J]_ [i]
h 1

= ED _c[(0)]_ _c[(0)]_ _c[(1)]_ _c[(1)]_ _c[(][J][)]_ _c[(][J][)]_

_D[(][p][ P]i[J]=0_ _[k][i][)]_ _a[(0)]1_ _· · ·_ _a[(0)]pk0_ _a[(1)]1_ _· · ·_ _a[(1)]pk1_ _· · ·_ _a[(]1[J][)]_ _· · ·_ _a[(]pkJ[J][)]_

_a[(]α[i][)]_      
_{X[}]_

1
= ED _c[(0)]_ _c[(0)]_ _c[(1)]_ _c[(1)]_ _c[(][J][)]_ _c[(][J][)]_

_D[(][p][ P]i[J]=0_ _[k][i][)]_ _a[(0)]1_ _· · ·_ _a[(0)]pk0_ _a[(1)]1_ _· · ·_ _a[(1)]pk1_ _· · ·_ _a[(]1[J][)]_ _· · ·_ _a[(]pkJ[J][)]_

_{a[(]α[i]X[)][̸][=][a]β[(][j][)][}]_      

+ _D[−][1][]_
_O_
  _i=0_ _[k][i][ −]_ [1))]

= _[D][(][D][ −]_ [1)][ · · ·][ (][D][ −] [(][p][ P][J] _c[(0)][][pk][0][ ]c[(1)][][pk][1]_ _c[(][J][)][][pk][J]_ + _D[−][1][]_

_D[(][p][ P]i[J]=0_ _[k][i][)]_ _· · ·_ _O_

   

= _c[(0)][][k][0][ ]c[(1)][][k][1]_ _· · ·_ _c[(][J][)][][k][J]_ [][p] + O _D[−][1][]_ _._
   

Plugging this into (S9) establishes the lemma.

In the above, we use the multi-index notation _a[(]α[i][)]_
_{_ _[}][ for the collection of indices on the][ c][i][ and the]_
notation _aα[(][i][)]_ _β_
_{_ _[̸][=][ a][(][j][)][}][ for the subset of terms in the sum where all indices take different values.]_

Lemma 1 immediately implies that the mean of polynomials of ¯c[(][i][)] concentrate around their infinite
data limit.

ED _g_ _c¯[(0)], ¯c[(1)], . . ., ¯c[(][K][)][]_ _−_ _g_ _c[(0)], c[(1)], . . ., c[(][K][)][][n][i]_ = O _D[−][1][]_ _,_ (S10)

for g _PK_ _c¯[(0)]h, ¯c[(1)], . . ., ¯c[(][K][)][]._   
_∈_

With this out of the way, we can proceed to analysing the scaling of trained neural networks. Here
we consider the simplified setting where the network map, f, and loss ℓ evaluated on each training
example, xa = (xa, ya), are polynomial of degree J and K in the weights, θµ,


_bµ[(][i]1[)]µ2...µi_ [(][x][)][θ][µ]1 _[θ][µ]2_ _i_ _ℓ(xa) =_
_i=1_ _[· · ·][ θ][µ]_

X


_i=1_ _c[(]µ[i]1[)]µ2...µi_ [(][x][a][)][θ][µ]1 _[θ][µ]2_ _[· · ·][ θ][µ]i_ _[.]_ (S11)

X


_f_ (x) =


-----

The training loss can then be written as,


_K_

_c¯µ[(][i]1[)]µ2...µi_ _[θ][µ]1_ _[θ][µ]2_ _i_ _[,]_ _c¯[(][i][)]_ = [1]
_i=1_ _[· · ·][ θ][µ]_ _D_

X


_c[(][i][)](xa) ._ (S12)
_a=1_

X


_Ltrain =_


Here we have used the convention that the repeated weight indices µj are summed over.

**Gradient Descent** As a result of the gradient descent weight update, θt+1 = θt − _η_ _[∂L]∂θ[train]_ [, the]

weights at time T are a polynomial of degree (K − 1)[T] in the ¯c[(][i][)].

_θT ∈_ _P(K−1)T_ _c¯[(0)], ¯c[(1)], . . ., ¯c[(][K][)][i]_ _._ (S13)
h

The coefficients of this polynomial depend on the initial weights, θ0. Plugging these weights back
into the network output, we have that the network function at time T is again a polynomial in ¯c[(][i][)],
now with degree J (K − 1)[T] .

_fT (x) ∈_ _PJ(K−1)T_ _c¯[(0)], ¯c[(1)], . . ., ¯c[(][K][)][i]_ _._ (S14)
h

Thus, again using Lemma 1, fT concentrates with variance O(D[−][1]).

ED (fT − ED [fT ])[2][i] = O _D[−][1][]_ _._ (S15)
h  

and by Theorem 1 the loss will obey they variance-limited scaling.

**Stochastic Gradient Descent** We now consider the same setup of polynomial loss, but now trained
via stochastic gradient descent (SGD). We consider SGD batches drawn i.i.d. with replacement and
are interested in the test loss averaged over SGD draws, with fixed batch size, B.

We proceed by proving the following lemma, which allows us to reuse a similar argument to the GD
case.

**Lemma 2. Let ˜c[(][i][;][t][)]** = _B[1]_ _a_ _t_ _[c]a[(][i][)]_ _[for][ i][ = 0][ . . . J][ be mini-batch averages, over][ B][ i.i.d. draws of]_

_∈B_

_c[(]a[i][)][. Further, let][ X][ = (˜]c[(0;]P[t][0][)])[k][0]_ (˜c[(1;][t][1][)])[k][1] _· · · (˜c[(][J][;][t][J]_ [)])[k][J] _be a monomial in the mini-batch means._

_Then EB [X] ∈_ _P[P]Ji=0_ _[k][i]_ _d¯[(0)],_ _d[¯][(1)], . . .,_ _d[¯][(][Q]i[J]=0[(][k][i][+1)][−][1)][i], where_ _d[¯][(][i][)]_ _are empirical means over the_
_full training set of i.i.d. random variables as in Lemma 1 andh_ EB [•] denotes the expectation over
_draws of SGD batches of size B._


_Proof. Expectations over draws of batches at different time steps are independent. Thus, WLOG, we_
can consider t := t0 = t1 = · · · = tJ . We can again proceed by direct computation, expanding the
mini-batch sums.

1
EB [X] = _J_ _c[(0)]_ _c[(0)]_ _c[(1)]_ _c[(1)]_ _c[(][J][)]_ _c[(][J][)]_ _._

_B_ _i=0_ _[k][i][ E][B]_  _a[(0)]1_ _· · ·_ _a[(0)]k0_ _a[(1)]1_ _· · ·_ _a[(1)]k1_ _· · ·_ _a[(]1[J][)]_ _· · ·_ _a[(]kJ[J][)]_

_a[(]α[i][)]_ _t_      []

P _{_ X[}∈B]

 (S16)

To proceed, we must keep track of terms in the sum where the a[(]α[i][)] [take the same or different values.]
If all a[(]α[i][)] [are different, the expectation over batch draws fully factorizes. More generally][ (S16)][ can]
be decomposed as a sum over products.

One way of keeping track of the index combinatorics is to introduce a set of graphs, Γ, where each
graph γ ∈ Γ has k0 vertices of type 0, k1 vertices of type 1, . . ., and kJ vertices of type J (one vertex
for each a[(]α[i][)] [index). Any pair of vertices may have zero or one edge between them. For any set of]
three vertices, v1, v2, and v3 with edges (v1, v2) and (v2, v3) there must also be an edge (v1, v3). The
set Γ consists of all possible ways of connecting these vertices consistent with these rules.


-----

For each graph, γ, we denote connected components by σ and denote the number of vertices of type i
within the connected component σ by m[(]σ[i][)][. With this we can write the sum, (S16) as]


1 _m[(0)]σ_ _m[(1)]σ_ _m[(]σ[J][)]_

_σ_ _γ_ EB " _B_ _a_ _t_ _c[(0)]a_ _c[(1)]a_ _· · ·_ _c[(]a[J][)]_

Y∈ X∈B   []   

_D_

1 _m[(0)]σ_ _m[(1)]σ_ _m[(]σ[J][)]_

_D_ _c[(0)]a_ _c[(1)]a_ _· · ·_ _c[(]a[J][)]_

_σY∈γ_ _aX=1_   []   

_d¯[(][{][m]σ[(0)][,m]σ[(1)][,...,m]σ[(][J][)]}) ._
_σY∈γ_


EB [X] =


_Sγ(B)_
_γX∈Γ_

_Sγ(B)_
_γX∈Γ_

_Sγ(B)_
_γX∈Γ_


(S17)


Here Sγ(B) is a combinatoric factor associated to each graph, not relevant for the argument. The
_m[(]σ[i][)]_ [take on values][ 0][ to][ k]i[, so the multi-index, can take on][ Q][J]i=1[(][k][i][ + 1)][ different values, which we]
re-index to _d[¯][(0)],_ _d[¯][(1)], . . .,_ _d[¯][(][Q]i[J]=1[(][k][i][+1)][−][1)]. Meanwhile, the degree of (S17) in_ _d[¯][(][i][)]_ is bounded by the
number of total vertices in each graph, i.e. _i=0_ _[k][i][. This establishes the lemma.]_

For a polynomial loss of degree K, the mini-batch training loss at each time step takes the form[P][J]


_K_

_c˜[(]µ[i]1[;][t]µ[)]2...µi_ _[θ][µ]1_ _[θ][µ]2_ _i_ _[,]_ _c˜[(][i][;][t][)]_ = [1]
_i=1_ _[· · ·][ θ][µ]_ _B_

X


_L[(]train[t][)]_ [=]


_c[(][i][)](xa) ._ (S18)
_a=X∈Bt_


The update rule, θt+1 = θt _η_ _[∂L]∂θtrain[(][t][+1)]_ ensures that θT is a polynomial of degree (K 1)[T] in the
_−_ _−_

_c˜[(][i][;0)], ˜c[(][i][;1)], · · ·, ˜c[(][i][;][T][ )]_

_θT ∈_ _P(K−1)T_ _c˜[(0;0)], ˜c[(0;1)], . . ., ˜c[(0;][T][ )], ˜c[(1;0)], ˜c[(1;1)], . . ., ˜c[(1;][T][ )], . . ., ˜c[(][K][;0)], ˜c[(][K][;1)], . . ., ˜c[(][K][;][T][ )][i]_ _,_
h (S19)

and consequently, denoting the test loss evaluated at θT by L[θT ],

_L[θT ]_ _PK(K_ 1)T _c˜[(0;0)], ˜c[(0;1)], . . ., ˜c[(0;][T][ )], ˜c[(1;0)], ˜c[(1;1)], . . ., ˜c[(1;][T][ )], . . ., ˜c[(][K][;0)], ˜c[(][K][;1)], . . ., ˜c[(][K][;][T][ )][i]_
_∈_ _−_
h (S20)


Using Lemma 2, the expectation of L[θT ] over draws of SGD batches is given by

EB [L[θT ]] _PK(K_ 1)T _d¯[(0)], . . .,_ _d[¯][(][K][K]_ [(][K][−][1)][T K] [)][i] _._ (S21)
_∈_ _−_
h

Finally, denoting ED [EB [L[θT ]]] by L(D; B) and applying Lemma 1 gives

_L(D; B)_ lim _D[−][1][]_ _._ (S22)
_−_ _D_
_→∞_ _[L][(][D][;][ B][) =][ O]_
 

D.3 NON-SMOOTH EXAMPLES

Here we present two worked examples where non-bounded or non-smooth loss leads to violations of
the variance dominated scaling. In example one, the system obeys the variance dominated scaling
at early times, but exhibits different behavior for times larger than the dataset size. In the second
example, the system violates the variance dominated scaling even for two gradient descent steps, as a
result of an unbounded derivative in the loss.[3]

**Example 1 – unbounded loss at late times** Consider a dataset with two varieties of data points,
drawn with probabilities α and 1 − _α, and one-dimensional quadratic losses, ℓ1 (concave up) and ℓ2_
(concave down), on these two varieties.

_ℓ1(θ) = [1]_ _ℓ2(θ) =_ (S23)

2 _[θ][2][,]_ _−_ 2[1] _[θ][2][ .]_


3We thank Anonymous for suggesting these two types of examples.


-----

If, in a slight abuse of notation, we further denote the training loss on a sample with n1 points of type
1 and D − _n1 points of type two by ℓn1 and the population loss at a given value of the weight by Lpop,_
we have

_n1_
_ℓn1 =_ _θ[2]_ _,_ _Lpop =_ _α_ _θ[2]_ _._ (S24)

_D_ 2 _−_ 2[1]

 _[−]_ [1]   

For this example we take α > 1/2. In this case, the minimum of the population loss is at zero, while
the minimum of the training loss can be at zero, or at ±∞ depending on whether the training sample
has n1 greater than or less than D/2. We can thus create a situation where at late training times, θT
does not concentrate around the minimum of the population loss.

As we work through this example explicitly, we will see the following. (i) A mismatch larger than
_O_ _D[−][1][]_ between the population minimum and the minimum found by training on a sample set of
size D requires times T larger than a constant multiple of D. (ii) The quantity we study throughout
 
this work is the difference between the infinite data limit of the test loss, and the finite data value,
_L(D) −_ limD→∞ _L(D). The minimum of the infinite data limit of the test loss is not the same as_
the minimum of the population loss, min limD _L(D)_ = minθ Lpop. In this example one diverges,
_→∞_ _̸_
while the other is finite. In particular this example evades the scaling result by L(D) for times larger
than D having a diverging limit.

Explicitly, we study the evolution of the model under gradient flow.

_θ˙ =_ 2 _n1_ _θ,_ _θT = e[−][2][(]_ _nD1_ _[−]_ [1]2 [)][T] _θ0 ._ (S25)
_−_ _D_ 2
 _[−]_ [1] 

The test loss averaged over draws of the dataset is given by

_D_

_L(D; T_ ) = En1 _α −_ 2[1] _θT[2]_ = e[2][T] _α −_ 2[1] 1 − _α_ 1 − _e[−]_ [4]D[T] _θ0[2]_ (S26)

       

If we consider this loss at large D and fixed T we get

2 [)][T]

_L(D; T_ ) = e[−][4][(][α][−] [1] _α −_ 2[1] _θ0[2]_ 1 + [8][T][ 2][α]D[(1][ −] _[α][)]_ + O _D[−][2][]_ _,_ (S27)

  

 

and thus L(D; T ) − limD→∞ _L(D; T_ ) = O _D[−][1][]_ as expected.

If on the other hand we consider taking T  D we have
_≫_

_L(D; T_ _D) = e[2][T]_ _α_ (1 _α)[D]_ _θ0[2]_ _[,]_ (S28)
_≫_ _−_ [1]2 _−_
 

the limit limD,T →∞ _L(D; T ≫_ _D) diverges._

Lastly, we note that if we take T = βD with β < | log(1 − _α)|/2 we can approach the large D limit_
with non-generic, tuneable exponential convergence.

**Example 2 – unbounded derivative** Again, consider a two variety setup, this case with equal
probabilities and per sample losses,

_ℓ1(θ) = [1]_ _ℓ2(θ) = [1]_ (S29)

2 _[θ][2][ + 1]2α_ _[|][θ][|][α][,]_ 2 _[θ][2][ −]_ 2[1]α _[|][θ][|][α][ .]_


We will consider different values of α > 0. The train loss and population loss are then,

_n1_

_ℓn1 = [1]_ _θ_ _,_ _Lpop = [1]_ (S30)

2 _[θ][2][ + 1]α_ _D_ 2 _|_ _|[α]_ 2 _[θ][2][ .]_

 _[−]_ [1] 

We consider a model initialized to θ0 = 1 and trained for two steps of gradient descent with learning
rate 1.

_n1_
_gt = θt +_ _θt_ _θt_ _,_ _θt+1 = θt_ _gt ._ (S31)

_D_ 2 _|_ _|[α][−][2]_ _−_

 _[−]_ [1] 


-----

Two update steps gives

_α_

_n1_
_θ2 =_ _._

_D_ 2

_[−]_ [1]

The test loss is given by the population loss evaluated at θ2 averaged over test set draws.


(S32)

(S33)


_D_ 2α

1 1 _D_ _n1_

_L(D) = En1_ 2 _[θ]2[2]_ = 2[D][+1] _n1_ _D_ 2

  _nX1=0_   _[−]_ [1]

_D_ _∞_ 2 [)]2 2α (S33)

= _e[−][2][D][(][x][−]_ [1] + _D[−][1][]_

2π 2 _O_

r Z−∞

= [Γ] _α +_ [1]2 _D[−][x][1][ −][]_ _.[1]_  

2[1+][α][√]π [D][−][α][ +][ O]

  

 

Here we have approximated the binomial distribution at large D with a normal distribution using
Stirling’s approximation.

Note that if α ≥ 1 then L(D) − limD→∞ _L(D) = O_ _D[−][1][]_ i.e. the finite sample loss approaches
the infinite data loss with the predicted variance-limited scaling. For 0 < α < 1, we get a different
 
scaling controlled by α. Note that the gradient, expression (S31) is singular at the origin for α
precisely in this range.

In summary, this example achieves a different scaling exponent through a diverging gradient.

E PROOF OF THEOREMS 2 AND 3

In this section we detail the proof of Theorems 2 and 3. The key observation is to make use of the
fact that nearest neighbor distances for D points sampled i.i.d. from a d-dimensional manifold have
mean ED,x [|x − _xˆ|] = O_ _D[−][1][/d][], where ˆx is the nearest neighbor of x and the expectation is the_
mean over data-points and draws of the dataset see e.g. (Levina & Bickel, 2005).
 

The theorem statements are copied for convenience. In the main, in an abuse of notation, we used
_L(f_ ) to indicate the value of the test loss as a function of the network f, and L(D) to indicate the
test loss averaged over the population, draws of the dataset, model initializations and training. To be
more explicit below, we will use the notation ℓ(f (x)) to indicate the test loss for a single network
evaluated at single test point.

**Theorem 2. Let ℓ(f** ), f and _be Lipschitz with constants KL, Kf_ _, and K_ _and ℓ(_ ) = 0. Further
_F_ _F_ _F_
_let D be a training dataset of size D sampled i.i.d from Md and let f_ (x) = F(x), ∀x ∈D then
_L(D) = O_ _KLmax(Kf_ _, KF_ )D[−][1][/d][].
 

_Proof. Consider a network trained on a particular draw of the training data. For each training_
point, x, let ˆx denote the neighboring training data point. Then by the above Lipschitz assumptions and the vanishing of the loss on the true target, we have ℓ(f (x)) _KL_ _f_ (x) (x)
_≤_ _|_ _−F_ _| ≤_
_KL (Kf + K_ ) _x_ _xˆ_ . With this, the average test loss is bounded as
_F_ _|_ _−_ _|_

_L(D)_ _KL (Kf + K_ ) ED,x [ _x_ _xˆ_ ] = _KLmax(Kf_ _, K_ )D[−][1][/d][] _._ (S34)
_≤_ _F_ _|_ _−_ _|_ _O_ _F_


In the last equality, we used the above mentioned scaling of nearest neighbor distances.

**Theorem 3. Let ℓ(f** ), f and _be Lipschitz with constants KL, Kf_ _, and K_ _. Further let f_ (x) =
_F_ _F_
_F(x) for P points sampled i.i.d from Md then L(P_ ) = O _KLmax(Kf_ _, KF_ )P _[−][1][/d][]._
 

_Proof. Denote by P the P points, z, for which f_ (z) = F(z). For each test point x let ˆx denote the
closest point in, ˆx = argmin ( _x_ _z_ ). Adopting this notation, the result follows by the same
_P_ _P_ _|_ _−_ _|_
argument as Theorem 2.


-----

F RANDOM FEATURE MODELS

Here we present random feature models in more detail. We begin by reviewing exact expressions
for the loss. We then go onto derive its asymptotic properties. We again consider training a
model f (x) = _µ=1_ _[θ][µ][f][µ][(][x][)][, where][ f][µ][ are drawn from some larger pool of features,][ {][F][M]_ _[}][,]_

_fµ(x) =_ _M_ =1

[P][P][µM][P] _[F][M]_ [(][x][)][.]

Note, if _FM_ (x) form a complete set of functions over the data distribution, than any target function,
_{[P][S]_ _}_
_y(x), can be expressed as y =_ _M_ =1 _[ω][M]_ _[F][M]_ [(][x][)][. The extra constraint in a teacher-student model is]
specifying the distribution of the ωM . The variance-limited scaling goes through with or without the
teacher-student assumption, however it is crucial for analysing the variance-limited behavior.[P][S]

As in Section 2.3 we consider models with weights initialized to zero and trained to convergence with
mean squared error loss.


(f (xa) _ya)[2]_ _._ (S35)
_−_
_a=1_

X


_Ltrain =_


2D


The data and feature second moments play a central role in our analysis. We introduce the notation,


= Ex _F_ (x)F _[T]_ (x) _,_ ¯ = [1]
_C_ _C_ _D_
 


_F_ (xa)F _[T]_ (xa), _C =_ _,_ _C¯ =_ [¯] _._
_PCP_ _[T]_ _PCP_ _[T]_
_a=1_

X


(S36)


(x, x[′]) = [1] ¯ = _K(x, x[′]) = [1]_ _K¯ = K_
_K_ _S [F][ T][ (][x][)][F]_ [(][x][′][)][,] _K_ _K_ train _[,]_ _P [f][ T][ (][x][)][f]_ [(][x][′][)][,] train _[.]_

_D_ _D_

Here the script notation indicates the full feature space while the block letters are restricted to the
student features. The bar represents restriction to the training dataset. We will also indicate kernels
with one index in the training set as _[⃗]_ (x) := (x, xa=1...D) and _K[⃗]_ (x) := K(x, xa=1...D). After
_K_ _K_
this notation spree, the test loss can be written for under-parameterized models, P ≤ _D as_

_L(D, P_ ) = [1] Tr + [¯] _C_ _[−][1]CC[¯][−][1]_ [¯] 2 [¯] _C_ _[−][1]_ _._ (S37)

2S [E][D] _C_ _CP_ _[T][ ¯]_ _PC −_ _CP_ _[T][ ¯]_ _PC_

and for over-parameterized models (at the unique minimum found by GD, SGD, or projected Newton’s   
method),

_L(D, P_ ) = [1] (x, x) + _K[⃗]_ (x)[T][ ¯]K _[−][1][ ¯]K[¯]_ _[−][1][ ⃗]K(x)_ 2K[⃗] (x)[T][ ¯]K _[−][1][ ⃗]_ (x) _._ (S38)

2 [E][x,D] _K_ _K_ _−_ _K_
h i


Here the expectation ED [•] is an expectation with respect to iid draws of a dataset of size D from the
input distribution, while Ex [•] is an ordinary expectation over the input distribution. Note, expression
(S37) is also valid for over-parameterized models and (S38) is valid for under-parameterized models
if the inverses are replaces with the Moore-Penrose pseudo-inverse. Also note, the two expressions
can be related by echanging the projections onto finite features with the projection onto the training
dataset and the sums of teacher features with the expectation over the data manifold. This realizes the
duality between dataset and features discussed above.

F.1 ASYMPTOTIC EXPRESSIONS

We are interested in (S37) and (S38) in the limits of large P and D.

**Variance-limited scaling** We begin with the under-parameterized case. In the limit of lots of data
the sample estimate of the feature feature second moment matrix, _C[¯], approaches the true second_
moment matrix, C. Explicitly, if we define the difference, δC by _C[¯] = C + δC. We have_

ED [δC] = 0

ED [δ _M1N1_ _δ_ _M2N2_ ] = [1] (S39)
_C_ _C_ _D_ [(][E][x][ [][F][M][1] [(][x][)][F][N][1] [(][x][)][F][M][2] [(][x][)][F][N][2] [(][x][)]][ −C][M][1][N][1] _[C][M][2][N][2]_ [)]

ED [δCM1N1 · · · δCMnNn ] = O _D[−][2][]_ _∀n > 2 ._
 


-----

The key takeaway from (S39) is that the dependence on D is manifest.

Using these expressions in (S37) yields.

_L(D, P_ ) = [1] _C_ _[−][1]_

2S [Tr] _C −CP_ _[T]_ _PC_
  


_TM1N1M2N2_ _δM1M2_ _C_ _[−][1]_
_P_ _[T]_ _P_
_M1,2N1,2=1_

X h  


_N1N2_ [+ (][C] _[−][1][PC][2][P]_ _[T][ C]_ _[−][1][)][M][1][M][2]_ _[C]N[−]1[1]N2_


2DS


+ O _D[−][2][]_ _._
  (S40)


_−2_ _CP_ _[T]_ _C_ _[−][1]P_
 


_C_ _[−][1]_
_M1M2_
_P_ _[T]_ _P_
 


_N1N2_


Here we have introduced the notation, TM1N1M2N2 = Ex [FM1 (x)FN1 (x)FM2 (x)FN2 (x)].

As above, defining

_L(P_ ) := lim _C_ _[−][1]_ _._ (S41)
_D_ 2S [Tr] _C −CP_ _[T]_ _PC_

we see that though L(D, P ) − _L(→∞P_ ) is a somewhat cumbersome quantity to compute, involving the[L][(][D, P] [) = 1]   
average of a quartic tensor over the data distribution, its dependence on D is simple.

For the over-parameterized case, we can similarly expand (S38) using K = K+δK. With fluctuations
satisfying,


EP [δK] = 0

EP [δ _a1b1_ _δ_ _a2b2_ ] = [1]
_K_ _K_ _P_ [(][E][P][ [][f][µ][(][x][a][1] [)][f][µ][(][x][b][1] [)][f][µ][(][x][a][2] [)][f][µ][(][x][b][2] [)]][ −K][a][1][b][1] _[K][a][2][b][2]_ [)]

EP [δKa1a1 · · · δKanan ] = O _P_ _[−][2][]_ _∀n > 2 ._

This gives the expansion  


(S42)


_L(D, P_ ) = [1]2 [E][x,D] _K(x, x) −_ _K[⃗]_ (x)[T][ ¯]K[−][1][ ⃗]K(x) + O(P _[−][1]),_ (S43)

h i


and


_L(D) = [1]_ (x, x) (x)[T][ ¯] (x) _._ (S44)

2 [E][x,D] _K_ _−_ _K[⃗]_ _K[−][1][ ⃗]K_
h i

**Resolution-limited scaling** We now move onto studying the parameter scaling of L(P ) and dataset
scaling of L(D). We explicitly analyse the dataset scaling of L(D), with the parameter scaling
following via the dataset parameter duality.

Much work has been devoted to evaluating the expression, (S44) (Williams & Vivarelli, 2000;
Malzahn & Opper, 2002; Sollich & Halees, 2002). One approach is to use the replica trick – a tool
originating in the study of disordered systems which computes the expectation of a logarithm of a
random variable via simpler moment contributions and analyticity assumption (Parisi, 1980). The
replica trick has a long history as a technique to study the generalization properties of kernel methods
(Sollich, 1998; Malzahn & Opper, 2001; 2003; Urry & Sollich, 2012; Cohen et al., 2019; Gerace
et al., 2020; Bordelon et al., 2020). We will most closely follow the work of Canatar et al. (2021)
who use the replica method to derive an expression for the test loss of linear feature models in terms
of the eigenvalues of the kernel C and ¯ω, the coefficient vector of the target labels in terms of the
model features.


_κ[2]_
_L(D) =_

1 − _γ_


_κ[2]_ _λiω¯i[2]_

1 − _γ_ _i_ (κ + Dλi)[2][,]

X (S45)

_κλi_ _,_ _γ =_ _Dλ[2]i_

_κ + Dλi_ _i_ (κ + Dλi)[2][ .]
X


_κ =_


This is the ridge-less, noise-free limit of equation (4) of Canatar et al. (2021). Here we analyze the
asymptotic behavior of these expressions for eigenvalues satisfying a power-law decay, λi = i[−][(1+][α][K] [)]

and for targets coming from a teacher-student setup, w ∼N (0, 1/S).


-----

Fixed Low Regularization


Tuned Regularization


P / D



10[2]

10[1]

10[0]

10 1


10[1] 10[2] 10[3] 10[1] 10[2] 10[3]

|Col1|Fixed Low|Regularization|n|10 1|Col6|Tuned R|Regularization|Col9|Col10|Col11|
|---|---|---|---|---|---|---|---|---|---|---|
|||||10 1 6 × 10 2 4 × 10 2 3 × 10 2 2 × 10 2||||||103 102 101|
||||||||||||
||||||||||||
||||||||||||
||||||||||||


Feature size (P, Solid), Dataset size (D, Dashed) Feature size (P, Solid), Dataset size (D, Dashed)


Figure S6: Duality between dataset size vs feature number in pretrained features Using pretrained embedding features of EfficientNet-B5 (Tan & Le, 2019) for different levels of regularization,
we see that loss as function of dataset size or loss as a function of the feature dimension track each
other both for small regularization (left) and for tuned regularization (right). Note that regularization
strength with trained-feature kernels can be mapped to inverse training time (Ali et al., 2019; Lee
et al., 2020). Thus (left) corresponds to long training time and exhibits double descent behavior,
while (right) corresponds to optimal early stopping.

To begin, we note that for teacher-student models in the limit of many features, the overlap coefficients
_ω¯ are equal to the teacher weights, up to a rotation ¯ωi = OiM_ _wM_ . As we are choosing an isotropic
Gaussian initialization, we are insensitive to this rotation and, in particular, Ew _ω¯i[2]_ = 1/S. See
Figure S8 for empirical support of the average constancy of ¯ωi for the teacher-student setting and
 
contrast with realistic labels.

With this simplification, we now compute the asymptotic scaling of (S45) by approximating the sums
with integrals and expanding the resulting expressions in large D. We use the identities:


_∞_

1

Z


_x[−][n][(1+][α][)]_ Γ _n_ 1+1α 1 _α_

_−_

_dx_ 2F1 _m, n_

_κ + Dx[−][(1+][α][)][][m][ =][ κ][−][m]_ (1 + α)Γ _n +_ 1+αα  _−_ 1 + α _[, n][ +]_ 1 + α _[,][ −]κ[D]_ 
   

2[F]1 [(][a, b, c,][ −][y][)][ ∝] _[y][−][a][ +][ B][y][−][b][ +][ . . .,]_
(S46)


Here 2F1 is the hypergeometric function and the second line gives its asymptotic form at large y. B is
a constant which does not effect the asymptotic scaling.

Using these relations yields


_κ ∝_ _D[−][α][K]_ _,_ _γ ∝_ _D[0],_ and _L(D) ∝_ _D[−][α][K]_ _,_ (S47)

as promised. Here we have dropped sub-leading terms at large D. Scaling behavior for parameter
scaling L(P ) follow via the dataset parameter duality.


F.2 DUALITY BEYOND ASYMPTOTICS

Expressions (S37) and (S38) are related by changing projections onto finite feature set, and finite
dataset even without taking any asymptotic limits. We thus expect the dependence of test loss on
parameter count and dataset size to be related quite generally in linear feature models. See Section G
for further details.


G LEARNED FEATURES

In this section, we consider linear models with features coming from pretrained neural networks.
Such features are useful for transfer learning applications (e.g. Kornblith et al. (2019); Kolesnikov
et al. (2019)). In Figures S6 and S7, we take pretrained embedding features from an EfficientNet-B5


-----

|Variance-limited: Theory  =|Col2|Col3|= 1|
|---|---|---|---|
|Variance-limited: Theory D = 02 P=10, 01 P=25, 00 P=35, 1 2 3 4 5|||1|
|||P=10, P=25,|D=1.13 D=1.25|
|||P=35,|D=1.28|
|||||
|||||
|||||

|Resolution-limited: Theory  =|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|Resolution-limited: Theory D = P=1156, D=0.23 00 P P= =1 25 08 47 8, D D= =0 0. .2 25, 6 1 2|||||P|
||P=1156, P=1587,|D=0.23 D=0.25||||
||P=2048,|D=0.26||||
|||||||

|Col1|Variance-limited:|: Theory D = 1|Col4|
|---|---|---|---|
|||||
|||||
|||||
||P=10, D=1.09 P=25, D=1.04 P=35, D=1.00|||

|R|Resolution-limited|d: Theory D = P|
|---|---|---|
|||9 0 0|
||P=1156, D=0.2 P=1587, D=0.3 P=2048, D=0.3||

|Col1|Resolution-limi|ited: Theory P =|= D|
|---|---|---|---|
|00 1|D=1156, P= D=1587, P= D=2048, P=|0.25 0.27 0.28||
|||||
|||||

|Col1|Variance-limited|d: Theory P = 1|Col4|Col5|
|---|---|---|---|---|
|02 01 00 1 2 3||D=10, P=1 D=25, P=1 D=35, P=1||.27 .35 .41|
||||||
||||||
||||||

|Col1|R|Resolution-limited|ted|d: Theory P = D|
|---|---|---|---|---|
|2 2 2 2||||D=1156, P=0.28 D=1587, P=0.28 D=2048, P=0.29|
||||||

|1 2 3|D=10, P=1.13 D=25, P=1.07 D=35, P=1.05|D=10, P=1.13 D=25, P=1.07 D=35, P=1.05|
|---|---|---|


Variance-limited: Theory D [= 1] Resolution-limited: Theory D [=] P

10[2] P=10, D[=1.13] P=1156, D[=0.23]

) 1010[1][0] P=25, P=35, DD[=1.25][=1.28] 10[0] P=1587, P=2048, DD[=0.25][=0.26]

1010 12 Loss10 1

Loss - Loss (10 3

10 4

10 5 10[1] 10[2] 10[3] 10[4] 10 2 10[1] 10[2] 10[3] 10[4]

Dataset size (D) Dataset size (D)

Resolution-limited: Theory P [=] D Variance-limited: Theory P [= 1]

D=1156, P[=0.25] 10[2] D=10, P[=1.27]

10[0] D=1587, D=2048, PP[=0.27][=0.28] ) 10[1] D=25, D=35, PP[=1.35][=1.41]

10[0]

Loss 10 1

10 1 Loss - Loss (10 2

10 3

10[1] 10[2] 10[3] 10[1] 10[2] 10[3]

Feature size (P) Feature size (P)


Variance-limited: Theory D [= 1] Resolution-limited: Theory D [=] P

10 1

)10 2

10 3 Loss

Loss - Loss (10 4 P=10, D[=1.09] P=1156, D[=0.29]

P=25, D[=1.04] P=1587, D[=0.30]

10 5 P=35, D[=1.00] 10 2 P=2048, D[=0.30]

Dataset size (D) Dataset size (D)

Resolution-limited: Theory P [=] D Variance-limited: Theory P [= 1]

6 × 10 2 D=1156, D=1587, D=2048, PPP[=0.28][=0.28][=0.29] )10 1 D=10, D=25, D=35, PPP[=1.13][=1.07][=1.05]

Loss4 × 10 2 10 2

3 × 10 2 Loss - Loss (

10 3

2 × 10 2

10[1] 10[2] 10[3] 10[1] 10[2] 10[3]

Feature size (P) Feature size (P)


Figure S7: Four scaling regimes exhibited by pretrained embedding features Using pretrained
embedding features of EfficientNet-B5 (Tan & Le, 2019) for fixed low regularization (left) and tuned
regularization (right), we can identify four regimes of scaling using real CIFAR-10 labels.

model (Tan & Le, 2019) using TF hub[4]. The EfficientNet model is pretrained using the ImageNet
dataset with input image size of (456, 456). To extract features for the (32, 32) CIFAR-10 images,
we use bilinear resizing. We then train a linear classifier on top of the penultimate pretrained features.
To explore the effect feature size, P, and dataset size D, we randomly subset the feature dimension
and training dataset size and average over 5 random seeds. Prediction on test points are obtained as a
kernel ridge regression problem with linear kernel. We note that the regularization ridge parameter
can be mapped to an inverse early-stopping time (Ali et al., 2019; Lee et al., 2020) of a corresponding
ridgeless model trained via gradient descent. Inference with low regularization parameter denotes
training for long time while tuned regularization parameter is equivalent to optimal early stopping.

In Figure S7 we see evidence of all four scaling regimes for low regularization (left four) and optimal
regularization (right four). We speculate that the deviation from the predicted variance-limited
exponent αP = αD = 1 for the case of fixed low regularization (late time) is possibly due to the
double descent resonance at D = P which interferes with the power law fit.

In Figure S6, we observe the duality between dataset size D (solid) and feature size P (dashed) – the
loss as a function of the number of features is identical to the loss as function of dataset size for both
the optimal loss (tuned regularization) or late time loss (low regularization).

In Figure S8, we also compare properties of random features (using the infinite-width limit) and
learned features from trained WRN 28-10 models. We note that teacher-student models, where the
feature class matches the target function and ordinary, fully trained models on real data (Figure 1a),
have significantly larger exponents than models with fixed features and realistic targets.

The measured ¯ωi – the coefficient of the task labels under the i-th feature (S45) are approximately
constant as function of index i for all teacher-student settings. However for real targets, ¯ωi are only
constant for the well-performing Myrtle-10 and WRN trained features (last two columns).

4https://www.tensorflow.org/hub


-----

FC 2 × 10 1 CNN-VEC 10 1 Myrtle-10 WRN pretrained

Loss10 1 TS: Real: D[ = 0.20]D[ = 0.05] 6 × 1010 12 TS: Real: D[ = 0.23]D[ = 0.07] 1010 23 TS: Real: D[ = 0.41]D[ = 0.15] 101010 369

4 × 10 2 10 12 TS: D[ = 2.52]

3 × 10 2 10 4 Real: D[ = 0.25]

10[1] 10[2] 10[3] 10[4] 10[1] 10[2] 10[3] 10[4] 10[1] 10[2] 10[3] 10[4] 10[1] 10[2] 10[3] 10[4]

Dataset size (D) Dataset size (D) Dataset size (D) Dataset size (D)

10[0] K[ = 0.26] 10[0] K[ = 0.26] 1010[0]1 K[ = 0.46] 10 2 K[ = 1.31]

/0i 10 1 10 1 10 2 10 6

iC i 10 3 10 10

10 2 10 2 10 4 10 14

10[0] 10[1] 10[2] 10[3] 10[4] 10[0] 10[1] 10[2] 10[3] 10[4] 10[0] 10[1] 10[2] 10[3] 10[4] 10[0] 10[1] 10[2] 10[3] 10[4]

i i i i

10[2]

10[1] 10[1] 10[1]

10 1

Normalized Value1010 25 2TS2Real[ 0.03][ 1.41] 1010 25 2TS2Real[ 0.03][ 1.37] 1010 25 2TS2Real[ -0.07][ -0.45] 1010 47 2TS2Real[ -0.07][ -0.40]

10 8 i 10 8 i 10 8 i 10 10 i

10[0] 10[1] 10[2] 10[3] 10[0] 10[1] 10[2] 10[3] 10[0] 10[1] 10[2] 10[3] 10[0] 10[1] 10[2] 10[3]

i i i i


Figure S8: Loss on the teacher targets scale better than real targets for both untrained and
**trained features The first three columns are infinite width kernels while the last column is a kernel**
built out of features from the penultimate layer of pretrained WRN 28-10 models on CIFAR-10. The
first row is the loss as a function of dataset size D for teacher-student targets vs real targets. The
observed dataset scaling exponent is denoted in the legend. The second row is the normalized partial
sum of kernel eigenvalues. The partial sum’s scaling exponent is measured to capture the effect of the
finite dataset size when empirical αK is close to zero. The third row shows ¯ωi for teacher-student
and real target compared against the kernel eigenvalue decay. We see the teacher-student ¯ωi are
approximately constant.


-----

