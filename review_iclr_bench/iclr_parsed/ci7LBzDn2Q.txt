# DEEP RELU NETWORKS PRESERVE EXPECTED LENGTH


**Boris Hanin** _[∗]_
Dept. of Operations Research
& Financial Engineering
Princeton University
Princeton, NJ 08544 USA
bhanin@princeton.edu


**Ryan Jeong** _[∗]_
Dept. of Mathematics
University of Pennsylvania
Philadelphia, PA 19104 USA
rsjeong@sas.upenn.edu

ABSTRACT


**David Rolnick** _[∗]_
School of Computer Science
McGill University
Montr´eal, QC H3A 0G4 Canada
drolnick@cs.mcgill.ca


Assessing the complexity of functions computed by a neural network helps us
understand how the network will learn and generalize. One natural measure of
complexity is how the network distorts length – if the network takes a unit-length
curve as input, what is the length of the resulting curve of outputs? It has been
widely believed that this length grows exponentially in network depth. We prove
that in fact this is not the case: the expected length distortion does not grow with
depth, and indeed shrinks slightly, for ReLU networks with standard random initialization. We also generalize this result by proving upper bounds both for higher
moments of the length distortion and for the distortion of higher-dimensional volumes. These theoretical results are corroborated by our experiments.

1 INTRODUCTION

The utility of deep neural networks ultimately arises from their ability to learn functions that are
sufficiently complex to fit training data and yet simple enough to generalize well. Understanding
the precise sense in which functions computed by a given network have low or high complexity
is therefore important for studying when the network will perform well. Despite the fundamental
importance of this question, our mathematical understanding of the functions expressed and learned
by different neural network architectures remains limited.

A popular way to measure the complexity of a neural network function is to compute how it distorts
lengths. This may be done by considering a set of inputs lying along a curve and measuring the
length of the corresponding curve of outputs. It has been claimed in prior literature that in a ReLU
network this length distortion grows exponentially with the network’s depth Price & Tanner (2019);
Raghu et al. (2017), and this has been used as a justification of the power of deeper networks.
We prove that, in fact, for networks with the typical initialization used in practice, expected length
distortion does not grow at all with depth.

Our main contributions are:

1. We prove that for ReLU networks initialized with the usual 2/fan-in weight variance, the
expected length distortion does not grow with depth at initialization, actually decreasing
slightly with depth (Thm. 3.1) and exhibiting an interesting width dependency.

2. We prove bounds on higher moments of the length distortion, giving upper bounds that
hold with high probability (Thm. 4.1). We also obtain similar results for the distortion in
the volume of higher-dimensional manifolds of inputs (Thm. 4.2).

3. We empirically verify that our theoretical results accurately predict observed behavior for
networks at initialization, while previous bounds are loose and fail to capture subtle architecture dependencies.

It is worth explaining why our conclusions differ from those of Price & Tanner (2019); Raghu et al.
(2017). First, prior authors prove only lower bounds on the expected length distortion, while we use

_∗Equal contribution_


-----

different methodology to calculate tight upper bounds, allowing us to say that the expected length
distortion does not grow with depth. As we show in Thm. 5.1 and Fig. 1, the prior bounds are in fact
quite loose. Second, Thm. 3(a) in Raghu et al. (2017) has a critical typo[1], which has unfortunately
been perpetuated in Thm. 1 of Price & Tanner (2019). Namely, the “2” was omitted in the following
statement (paraphrased from the original):


depth
! 


_σw√width_
E [length distortion] = Ω 2√width + 1



where σw[2] _[/][width is the variance of the weight distribution. As we prove in Thm. 3.1, leaving out the]_
2 makes the statement false, incorrectly suggesting that length distortion explodes with depth for the
standard He initialization σw = _√2._

Finally, prior authors drew the conclusion that length distortion grows exponentially with depth by
considering the behavior of ReLU networks with unrealistically large weights. If one multiplies by
_C the weights and biases of a ReLU network, one multiplies the length distortion by C_ [depth], so it
should come as no surprise that there exist settings of the weights for which the distortion grows
(or decays) exponentially with depth. The value of our results comes in analyzing the behavior
specifically at He initialization (σw = _√2) He et al. (2015). This initialization is the one used in_

practice, since this is the weight variance that must be used if the outputs Hanin & Rolnick (2018)
and gradients Hanin (2018) are to remain well-controlled at init. In the present work, we show that
this is also the correct initialization for the expected length distortion to remain well-behaved.

2 RELATED WORK

A range of complexity measures for functions computed by deep neural networks have been considered in the literature, dividing the prior work into at least three categories. In the first, the emphasis
is on worst-case (or best-case) scenarios – what is the maximal possible complexity of functions
computed by a given network architecture. These works essentially study the expressive power of
neural networks and often focus on showing that deep networks are able to express functions that
cannot be expressed by shallower networks. For example, it has been shown that it is possible to set
the weights of a deep ReLU network such that the number of linear regions computed by the network
grows exponentially in the depth Daniely (2017); Eldan & Shamir (2016); Mont´ufar et al. (2014);
Telgarsky (2015; 2016). Other works consider the degree of polynomials approximable by networks
of different depths Lin et al. (2017); Rolnick & Tegmark (2018) and the topological invariants of
networks Bianchini & Scarselli (2014).

While such work has sometimes been used to explain the utility of different neural network architectures (especially deeper ones), a second strand of prior work has shown that a significant gap
can exist between the functions expressible by a given architecture and those which may be learned
in practice. Such average-case analyses have indicated that some readily expressible functions are
provably difficult to learn Shalev-Shwartz et al. (2017) or vanishingly unlikely for random networks
Hanin & Nica (2020); Hanin & Rolnick (2019; 2020), and that some functions learned more easily
by deep architectures are nonetheless expressible by shallow ones Ba & Caruana (2014). (While
here we consider neural nets with ReLU activation, it is worth noting that for arithmetic circuits,
worst-case and average-case scenarios may be more similar – in both scenarios, a significant gap
exists between the matricization rank of the functions computed by deep and shallow architectures
Cohen et al. (2016).) As noted earlier, average-case analyses in Price & Tanner (2019); Raghu et al.
(2017) provided lower bounds on expected length distortion, while Poole et al. (2016) presented a
similar analysis for the curvature of output trajectories.

Finally, a number of authors have sought complexity measures that either empirically or theoretically
correlate with generalization Jiang et al. (2019). Such measures have been based on classification
margins Bartlett et al. (2017), network compressibility Arora et al. (2018), and PAC-Bayes considerations Dziugaite & Roy (2017).

1We have confirmed this in personal correspondence with the authors, and it is simple to verify – the typo
arises in the last step of the proof given in the Supplementary Material of Raghu et al. (2017).


-----

3 EXPECTED LENGTH DISTORTION

3.1 MOTIVATION

While neural networks are typically overparameterized and trained with little or no explicit regularization, the functions they learn in practice are often able to generalize to unseen data. This
phenomenon indicates an implicit regularization that causes these learned functions to be surprisingly simple.

Why this occurs is not well-understood theoretically, but there is a high level intuition. In a nutshell,
randomly initialized neural networks will compute functions of low complexity. Moreover, as optimization by first order methods is a kind of greedy local search, network training is attracted to
minima of the loss that are not too far from the initialization and hence will still be well-behaved.

While this intuition is compelling, a key challenge in making it rigorous is to devise appropriate
notions of complexity that are small throughout training. The present work is intended to provide
a piece of the puzzle, making precise the idea that, at the start of training, neural networks compute tame functions. Specifically, we demonstrate that neural networks at initialization have low
distortion of length and volume, as defined in §3.2.

An important aspect of our analysis is that we study networks in typical, average-case scenarios.
A randomly initialized neural network could, in principle, compute any function expressible by a
network of that architecture, since the weights might with low probability take on any set of values.
Some settings of weights will lead to functions of high complexity, but these settings may be unlikely
to occur in practice, depending on the distribution over weights that is under consideration. As prior
work has emphasized Price & Tanner (2019); Raghu et al. (2017), atypical distributions of weights
can lead to exponentially high length distortion. We show that, in contrast, for deep ReLU networks
with the standard initialization, the functions computed have low distortion in expectation and with
high probability.

Figure 1: Mean length distortion as a function of depth, for randomly initialized ReLU networks
of varying architecture. As described in Thm. 3.1, distortion not only fails to grow, but shrinks
with depth, especially for networks of small width. (a) compares the predictions of our Thm. 5.1
(solid lines) to the lower bounds proven in prior work Raghu et al. (2017) (dashed lines) and the
true empirical means (colored dots), which closely track our predictions. (b) zooms in on the upper
part of (a), showing the empirical mean length distortion as a function of depth for different widths.
The horizontal black line is y = Γ(2Γ(3).5)√2.5 [, the mean length distortion we predict when][ n][0][ =][ n][L]

in the limit of infinite width for any fixed depth. In each network, width is constant in all hidden
layers, while input and output dimension are both 5. The input curve is a fixed line segment of unit
length. Length distortion is calculated for 500 different initializations of the weights and biases of
the network (the weight variance is 2/fan-in). For further experimental details, see Appendix B.


3.2 DEFINITIONS

Let L 1 be a positive integer and fix positive integers n0, . . ., nL. We consider a fully connected
_≥_
feed-forward ReLU network with input dimension n0, output dimension nL, and hidden layer
_N_


-----

widths n1, . . ., nL 1. Suppose that the weights and biases of are independent and Gaussian with
(ℓ) _−_ (ℓ) _N_
the weights Wij [between layers][ ℓ] _[−]_ [1][ and][ ℓ] [and biases][ b]j in layer ℓ satisfying:

_Wij[(][ℓ][)]_ _∼_ _G (0, 2/nℓ−1),_ _b[(]j[ℓ][)]_ _∼_ _G(0, Cb)._ (1)

Here, Cb > 0 is any fixed constant and G(µ, σ[2]) denotes a Gaussian with mean 0 and variance
_σ[2]. For any M ⊆_ R[n][0], we denote by N (M ) ⊆ R[n][L] the (random) image of M under the map
_x 7→N_ (x). Our primary object of the study will be the size of the output N (M ) relative to that of
_M_ . Specifically, when M is a 1-dimensional curve, we define

_length distortion = [len(][N]_ [(][M] [))] _._

len(M )

Note that while a priori this random variable depends on M, we will find that its statistics depend
only on the architecture of N (see Thms. 3.1 and 5.1).

3.3 RESULTS

We prove that the expected length distortion does not grow with depth – in fact, it slightly decreases. Our result may be informally stated thus (for a formal statement and proof, see Thm. 5.1
and App. C.):
**Theorem 3.1 (Length distortion: Mean; Informal Statement of Theorem 5.1). Consider a ReLU**
_network of depth L, input dimension n0, output dimension nL, and hidden layer widths nℓ, with_
_weights given by standard He normal initialization He et al. (2015). The expected length distortion_
_is upper bounded by_ _nL/n0. More precisely:_

1/2 _L_ 1 _nL+1_

E [length distortionp ] ≈ _C_  _nnL0_  exp "− 8[5] _ℓ=1−_ _n1ℓ_ # _,_ _C :=_ ΓΓ n 2L 2 _n2L_ _≈_ 1.

X

   p

Our experiments align with the theorem. In Figure 1, we compute the
empirical mean of the length distortion for randomly initialized deep
ReLU networks of various architectures with fixed input and output dimension. The figure confirms
three of our key predictions: (1)
the expected length distortion does
not grow with depth, and actually
decreases slightly; (2) the decrease
happens faster for narrower networks
(since 1/nℓ is larger for smaller nℓ);
and (3) for equal input and output dimension n0 = nL, there is an upper
bound of 1 (in fact, the tighter upper
bound of C is approximately 0.9515

Figure 2: Mean length distortion as a function of the ratio of output to input dimension, for ReLU networks with for nL = 5, as shown in the fig
ure). The figure also shows the prior

various architectures (e.g. [10, 10, 10] denotes three hid
bounds in Raghu et al. (2017) to be

den layers, each of width 10). All networks are randomly

quite loose. For further experimental

initialized as in Figure 1. We sample 100 pairs of input

details, see Appendix B.

dimension n0 and output dimension nL, each at most 50,
such that the ratio of output to input dimension is distinct In Fig. 2, we instead fix the set of hidfor each such pair. For each pair, 200 different network den layer widths, but vary input and
initializations are tested and the resulting length distortion output dimensions. The results conis calculated; the log of the empirical mean is plotted. The firm that indeed the expected length
dashed black line plots log(y) = 12 [log(][x][)][, the approxi-] distortion grows as the square root of
mate prediction by Theorem 3.1. the ratio of output to input dimension.

Note that our theorem also applies for initializations other than He normal; the result in such cases
is simply less interesting. Suppose we initialize the weights in each layer ℓ as i.i.d. normal with


-----

variance 2c[2]/nℓ 1 instead of 2/nℓ 1. This is equivalent to taking a He normal initialization and
_−_ _−_
multiplying the weights by c. Multiplying the weights and biases in a depth-L ReLU network by
_c simply multiplies the output (and hence the length distortion) by c[L], so the expected distortion_
is c[L] times the value given in Thm. 3.1. This emphasizes why an exponentially large distortion
necessarily occurs if one sets the weights too large.

3.4 INTUITIVE EXPLANATION

The purpose of this section is to give an intuitive but technical explanation for why, in Theorem 3.1,
we find that the distortion len(N (M ))/ len(M ) of the length of a 1D curve M under the neural
network N is typically not much larger than 1, even for deep networks.

Our starting point is that, near a given point x ∈ _M_, the length of the image under N of a small
portion of M with length dx is approximately given by ||Jxu|| dx, where Jx is the input-output
Jacobian of N at the input x and u is the unit tangent vector to M at x. Prior work (see Fact 7.2 in
Allen-Zhu et al. (2019)).

In fact, in Lemma C.1 of the Supp. Material, we use a simple argument to give upper bounds
on the moments of len( (M ))/ len(M ) in terms of the moments of the norm _Jxu_ of the
_N_ _||_ _||_
Jacobian-vector product Jxu.


Thus, upper bounds on the length distortion reduce to studying the Jacobian Jx, which in a network with
_L hidden layers can be written as a_
productlayer ℓ _J1x to layer = JL,x ℓ · · ·Jacobians J1,x of the Jℓ,x._
_−_
In a worst-case analysis, the left singular vectors of Jℓ,x would align with
the right singular vectors of Jℓ+1,x,
causing the resulting product Jx to
have a largest singular value that
grows exponentially with L. However, at initialization, this is provably
not the case with high probability. Indeed, the Jacobians Jℓ,x are independent (see Lemma C.3) and their singular vectors are therefore incoherent. We find in particular (Lemma
C.2) the following equality in distribution:


Figure 3: Length distortion as a function of _ℓ=1_ _[n]ℓ[−][1][,]_
showing both mean and standard deviation across initializations. We test several types of network architecture –

[P][L]
with constant width 20, alternating between widths 30 and
10, and with the first (respectively, second) half of the layers of width 30 and the rest width 10. Each architecture
type is tested for several depths. For each such network,
we use n0 = nL = 5 and compute length distortion for
100 initializations on a fixed line segment. As predicted
in Thm. 4.1, the mean length distortion decreases with
the sum of width reciprocals. Empirical standard deviation does not, in general, increase with greater depth, remaining modest throughout, as is consistent with the upper bound on variance in Thm. 4.1.


_Jxu_
_||_ _||_


_Jℓ,xe1_ _,_
_||_ _||_
_ℓ=1_

Y


where e1 is the first standard unit vector and the terms in the product are
independent. On average each term
in this product is close to 1:


E [||Jℓ,xe1||] = 1 + O(n[−]ℓ [1][)][.]

This is a consequence of initializing weights to have variance 2/fan-in. Put together, the two preceding displayed equations suggest writing

_L_

_Jxu_ = exp log( _Jℓ,xe1_ )
_||_ _||_ "ℓ=1 _||_ _||_ #

X

The terms being summed in the exponent are independent and the argument of each logarithm scales
like 1 + O(n[−]ℓ [1][)][. With probability][ 1][ −] _[δ][ we have for all][ ℓ]_ [that][ log(][||][J][ℓ,x][e][1][||][)][ ≤] _[cn][−]ℓ_ [1][, where][ c][ is a]


-----

constant depending only on δ. Thus, all together,

_Jxu_ exp
_||_ _|| ≤_


_n[−]ℓ_ [1]
_ℓ=1_

X


with high probability. In particular, in the simple case where nℓ are proportional to a single large
constant n, we find the typical size of _Jxu_ is exponential in L/n rather than exponential in L,
_||_ _||_
as in the worst case. If N is wider than it is deep, so that L/n is bounded above, the typical size
of _Jxu_ and hence of len( (M ))/ len(M ) remains bounded. Theorem 5.1 makes this argument
_||_ _||_ _N_
precise. Moreover, let us note that articles like Daniely (2017); Giryes et al. (2016); Poole et al.
(2016) show low distortion of angles and volumes in very wide networks. Our results, however,
apply directly to more realistic network widths.

4 FURTHER RESULTS

4.1 HIGHER MOMENTS

We have seen that the mean length distortion is upper-bounded by 1, and indeed by a function of the
architecture that decreases with the depth of the network. However, a small mean is insufficient by
itself to ensure that typical networks have low length distortion. For this, we now show that in fact
all moments of the length distortion are well-controlled. Specifically, the variance is bounded above
by the ratio of output to input dimension, and higher moments are upper-bounded by a function that
grows very slowly with depth. Our results may be informally stated thus:
**Theorem 4.1 (Length distortion: Higher moments). Consider, as before, a ReLU network of depth**
_L, input dimension n0, output dimension nL, and hidden layer widths nℓ, with weights given by He_
_normal initialization. We have the following bounds on higher moments of the length distortion:_


_nL_

Var[length distortion] _and_ E [(length distortion)[m]]
_≤_ _[n]n[L]0_ _≤_ _n0_





_[m]_

2




_cm[2]_


_n[−]ℓ_ [1]
_ℓ=1_

X


exp


_for some universal constant c > 0._

A formal statement and proof of this result are given in Theorem 5.1 and Appendix C.

We consider the mean and variance of length distortion for a wide range of network architectures
in Figure 3. The figure confirms that variance is modest for all architectures and does not increase
with depth, and that mean length distortion is consistently decreasing with the sum of layer width
reciprocals, as predicted by Theorem 4.1.

4.2 HIGHER-DIMENSIONAL VOLUMES

Another natural generalization to consider is how ReLU networks distort higher-dimensional volumes: Given a d-dimensional input M, the output N (M ) will in general also be d-dimensional, and
we can therefore consider the volume distortion vold( (M ))/ vold(M ). The theorems presented
_N_
in §3.3 when d = 1 can be extended to all d, and the results may be informally stated thus:
**Theorem 4.2 (Volume distortion). Consider a ReLU network of depth L, input dimension n0, output**
_dimension nL, and hidden layer widths nℓ, with weights given by He normal initialization. Both the_
_squared mean and the variance of volume distortion are upper-bounded by:_

_d_ _L_

_nL_ _d_
 _n0_  exp "−2 _ℓ=1_ _n[−]ℓ_ [1]# _._

X

A formal statement and proof of this result are given in Theorem 5.2 and Appendix D.

5 FORMAL STATEMENTS

In this section, we provide formal statements of our results. Full proofs are given in the appendices.


-----

5.1 ONE-DIMENSIONAL MANIFOLDS

Fix a smooth one-dimensional manifold M ⊆ R[n][0] (i.e. a curve) with unit length. Each point on
_M represents a possible input to our ReLU network_, which we recall has input dimension n0,
_N_
output dimension nL, and hidden layer widths n1, . . ., nL−1. The function x 7→N (x) computed
by N is continuous and piecewise linear. Thus, the image N (M ) ⊆ R[n][L] of M under this map
is a piecewise smooth curve in R[n][L] with a finite number of pieces, and its length len(N (M )) is a
random variable. Our first result states that, provided N is wider than it is deep, this distortion is
small with high probability at initialization.
**Theorem 5.1. Let N be a fully connected ReLU network with input dimension n0, output dimension**
_nL, hidden layer widths n1, . . ., nL−1, and independent centered Gaussian weights/biases with the_
_weights having variance 2/fan-in (as in (1)). Let M be a 1-dimensional curve of unit length in R[n][0]_ _._
_Then, the mean length E [len(N_ (M ))] equals

1/2 _nL+1_ _L_ 1 _L_ 1

 _nnL0_  Γ _nΓ2L _ 2 n2L 1/2 _[×][ exp]_  _−_ [5]8 _ℓ=1−_ _n[−]ℓ_ [1] + O _ℓ=1−_ _n[−]ℓ_ [2]! , (2)

X X

_where implied constants in O _ ( )   are universal and Γ( ) denotes the Gamma function. Moreover:

_·_ _·_

Var[len( (M ))] _._ (3)
_N_ _≤_ _[n]n[L]0_


_Finally, there exist universal constants c1, c2 > 0 such that if m < c1 min_ _n1, . . ., nL_ 1 _, then_
_{_ _−_ _}_

_m/2_ _L_

_nL_
E [(len(N (M )))[m]] ≤  _n0_  exp "c2m[2] _ℓ=1_ _n[−]ℓ_ [1]# _._

X


(4)


Theorem 5.1 is proved in §C. Several comments are in order. First, we have assumed for simplicity
that M has unit length. For curves of arbitrary length, both the equality (2) and the bounds (3)
and (4) all hold and are derived in the same way provided len(N (M )) is replaced by the distortion
len(N (M ))/ len(M ) per unit length.

Second, in the expression (2), note that the exponent tends to 0 as nℓ for any fixed L. More
_→∞_
precisely, when nℓ = n is large and constant, it scales as 5L/8n. This shows that, for fixed
_−_
_n0, nL, n, the mean E [len(N_ (M ))] is actually decreasing with L. This somewhat surprising phenomenon is borne out in Fig. 1 and is a consequence of the fact that wide fully connected ReLU nets
are strongly contracting (see e.g. Thms. 3 and 4 in Giryes et al. (2016)).

Third, the pre-factor (nL/n0)[1][/][2] encodes the fact that, on average over initialization, for any vector
of inputs x ∈ R[n][0], we have (see e.g. Cor. 1 in Hanin & Rolnick (2018))

E (x) _x_ _,_ (5)
_||N_ _||[2][i]_ _≈_ _[n]n[L]0_ _||_ _||[2]_
h

where the approximate equality becomes exact if the bias variance Cb is set to 0 (see (1)). In other
words, at the start of training, the network rescales inputs by an average factor (nL/n0)[1][/][2]. This
_N_
overall scaling factor also dilates len(N (M )) relative to len(M ).

Fourth, we stated Theorem 5.1 for Gaussian weights and biases (see (1)), but the result holds for any
weight distribution that is symmetric around 0, has variance 2/fan-in, and has finite higher moments.
The only difference is that the constant 5/8 may need to be slightly enlarged (e.g. around (19)).

Finally, all the estimates (2), (3), and (4) are consistent with the statement that, up to the scaling
factor (nL/n0)[1][/][2], the random variable len( (M )) is bounded above by a log-normal distribution:
_N_

1/2 _L_

_nL_ 1 _β_
len(N (M )) ≤ _n0_ exp 2 _[G]_ 2 _[, β]_ _, where β = c_ _n[−]ℓ_ [1] and c is a fixed constant.
     _ℓ=1_

X

5.2 HIGHER-DIMENSIONAL MANIFOLDS

The results in the previous section generalize to the case when M has higher dimension. To consider
this case, fix a positive integer d min _n0, . . ., nL_ and a smooth d dimensional manifold M
_≤_ _{_ _}_ _−_ _⊆_


-----

R[n][0] of unit volume vold(M ) = 1. Note that if d > min {n0, . . ., nL}, then N (M ) is at most
(d _−_ 1)-dimensional and its d-dimensional volume vanishes. Its image N (M ) ⊆ R[n][L] is a piecewise
smooth manifold of dimension at most d. The following result, proved in §D, gives an upper bound
on the typical value of the d−dimensional volume of N (M ).
**Theorem 5.2. Let N be a fully connected ReLU network with input dimension n0, output dimension**
_nL, hidden layer widths n1, . . ., nL_ 1 and independent centered Gaussian weights/biases with the
_−_
_variance of the weights given by 2/fan-in (see (1)). Let M be a d-dimensional smooth submanifold_
_of R[n][0]_ _with unit volume and d ≤_ min {n0, . . ., nL}. Then, both the squared mean and the variance
_of the d_ _dimensional volume vold(_ (M )) of (M ) is bounded above by
_−_ _N_ _N_

_d_ _L_

_nL_ _d_
 _n0_  exp "−2 _ℓ=1_ _n[−]ℓ_ [1]# (6)

X

6 PROOF SKETCH

The purpose of this section is to explain the main steps to obtaining the mean and variance estimates
(2) and (3) from Theorem 5.1. In several places, we will gloss over some mathematical subtleties
that we deal with in the detailed proof of Theorem 5.1 given in Appendix C. We will also content
ourselves here with proving a slightly weaker estimate than (2) and show instead simply that

1/2

_nL_
E [len( (M ))] and Var [len( (M ))] _._ (7)
_N_ _≤_ _n0_ _N_ _≤_ _[n]n[L]0_
 

We refer the reader to Appendix C for a derivation of the more refined result stated in Theorem 5.1.
Since we have E [X][2] _≤_ E _X_ [2][] and Var[X] ≤ E _X_ [2][], both estimates in (7) follow from
 E len( (M )) [2][] _._ (8)

_N_ _≤_ _[n]n[L]0_


To obtain this bound, we begin by relating moments of len(N (M )) to those of the input-output
Jacobian of N at a single point for which we need some notation. Namely, let us choose a unit
speed parameterization of M ; that is, fix a smooth mapping

_γ : R →_ R[n][0] _,_ _γ(t) = (γ1(t), . . ., γn0_ (t))

for which M is the image under γ of the interval [0, 1]. That γ has unit speed means that for every
_t ∈_ [0, 1] we have ||γ[′](t)|| = 1, where γ[′](t) := _γ1[′]_ [(][t][)][, . . ., γ]n[′] 0 [(][t][)] _. Then, the mapping Γ := N ◦_ _γ_
(for Γ : R R[n][L] ) gives a parameterization of the curve (M ). Note that this parameterization is
_→_   _N_ 
not unit speed. Rather the length of N (M ) is given by

1
len(N (M )) = 0 _||Γ[′](t)|| dt._ (9)
Z

Intuitively, the integrand ||Γ[′](t)|| dt computes, at a given t, the length of Γ([t, t + dt]) as dt → 0.
The following Lemma uses (9) to bound the moments of len(N (M )) in terms of the moments of
the norm of the input-output Jacobian Jx, defined for any x ∈ R[n][0] by

_∂_ _i_
_Jx :=_ _N_ (x) _,_ (10)
 _∂xj_ 11≤≤ij≤≤nnL0

where Ni is the i-th component of the network output.
**Lemma 6.1. We have**

1
E len(N (M ))[2][] _≤_ 0 E _Jγ(t)γ[′](t)_ _dt._ (11)
 Z h

[2][i]

_Sketch of Proof. Taking powers in (9) and interchanging the expectation and integrals, we obtain_


1
len(N (M ))[2][] = 0
Z


1

0

Z


E [||Γ[′](t1)|| ||Γ[′](t2)||] dt1dt2. (12)


-----

Applying the inequality ab ≤ 2[1] [(][a][2][ +][ b][2][)][, for][ a, b][ ∈] [R][, to the integrand in (12), we conclude]

1
E len(N (M ))[2][] _≤_ 0 E _||Γ[′](t)||[2][i]_ _dt._ (13)
 Z h

The chain rule yields Γ[′](t) = Jγ(t)γ[′](t). Substituting this into (13) completes the proof.

Lemma 6.1, while elementary, appears to be new, allowing us to bound global quantities such as
length in terms of local quantities such as the moments of _Jxu_ . In particular, having obtained
_||_ _||_
the expression (11), we have reduced bounding the second moment of len(N (M )) to bounding the
second moment of the random variable ||Jxu|| for a fixed x ∈ R[n][0] and unit vector u ∈ R[n][0] . This is
a simple matter since in our setting the distribution of weights and biases is Gaussian and hence, in
distribution, _Jxu_ is equal to a product of independent random variables:
_||_ _||_

**Proposition 6.2. For any x ∈** R[n][0] _and any unit vector u ∈_ R[n][0] _, the random variable ||Jxu||[2]_ _is_
_equal in distribution to a product of independent scaled chi-squared random variables_

_L−1_ 2

_||Jxu||[2][ d]=_ _[n]n[L]0_ _ℓ=1_ _nℓ_ _χ[2]nℓ_ ! _·_ _n[1]L_ _χ[2]nL_ _[,]_

Y

_where the number of degrees of freedom in thed_ _ℓth term of the product (for ℓ_ = 1, . . ., L − 1) is
_given by an independent binomial nℓ_ = Bin (nℓ, 1/2) with nℓ _trials and success probability 1/2._
_The number of degrees of freedom in the final term is deterministic and given by nL._

This Proposition has been derived a number of times in the literature (see Theorem 3 in Hanin (2018)
and Fact 7.2 in Allen-Zhu et al. (2019)). We provide an alternative proof based on Proposition 2 in
Hanin & Nica (2019) in the Supplementary Material. Note that the distribution of _Jxu_ is the same
_||_ _||_
at every x and u. Thus, fixing x ∈ R[n][0] and a unit vector u ∈ R[n][0], we find

To prove (8), we note that E _χ[2]nE=len( n/2N and apply Lemma 6.2 to find, as desired,(M_ ))[2][] _≤_ E h||Jxu||[2][i] _._

_L_ 1 _L_ 1
_−_  2 _−_ 2

E _Jxu_ = _[n][L]_ E _χ[2]nℓ_ E _n[−]L_ [1][χ]n[2] _L_ = _[n][L]_ E [nℓ] = _[n][L]_ _._ □
_||_ _||[2][i]_ _n0_ _nℓ_ _n0_ _nℓ_ _n0_
h _ℓY=1_  [!]   _ℓY=1_  

7 LIMITATIONS AND FUTURE WORK

We show that deep ReLU networks with appropriate initialization do not appreciably distort lengths
and volumes, contrary to prior assumptions of exponential growth. Specifically, we provide an exact
expression for the mean length distortion, which is bounded above by 1 and decreases slightly with
increasing depth. We also prove that higher moments of the length distortion admit well-controlled
upper bounds, and generalize this to distortion of higher dimensional volumes. We show empirically
that our theoretical results closely match observations, unlike previous loose lower bounds.

There are several notable limitations of this paper, which offer promising directions for future work.
First, we prove statements for networks at initialization, and our results do not necessarily hold
after training; analyzing such behavior formally would require consideration of the loss function,
optimizer, and data distribution. Second, our results, as stated, do not apply to convolutional, residual, or recurrent networks. We envision generalizations for networks with skip connections being
straightforward, while formulations for convolutional and recurrent networks will likely be more
complicated. In Appendix A, we provide preliminary results suggesting that expected length distortion decreases modestly with depth in both convolutional networks and those with skip connections.
Third, we believe that various other measures of complexity for neural networks, such as curvature,
likely demonstrate similar behavior to length and volume distortion in average-case scenarios with
appropriate initialization. While somewhat parallel results for counting linear regions were shown
in Hanin & Rolnick (2019; 2020), there remains much more to be understood for other notions of
complexity. Finally, we look forward to work that explicitly ties properties such as low length distortion to improved generalization, as well as new learning algorithms that leverage this line of theory
in order to better control inductive biases to fit real-world tasks.


-----

8 ETHICS STATEMENT

This paper falls within deep learning theory and is intended to advance innovation for more effective
and reliable algorithms. However, insofar as our work shows that deep ReLU networks at initialization are better-behaved than was previously believed, this could encourage the use of deeper neural
networks. While deeper networks may enhance performance in some contexts, larger numbers of parameters are also associated with increased computational cost, which can both increase greenhouse
gas emissions and exacerbate inequities caused by differential access to computational resources.

9 ACKNOWLEDGEMENTS

The authors gratefully acknowledge a range of funding sources that supported this research. BH
would like to acknowledge NSF grants DMS-1855684 DMS-2133806 as well as an NSF CAREER grant DMS-2143754 and an ONR MURI on Foundations of Deep Learning. DR would like
to acknowledge support from the Natural Sciences and Engineering Research Council of Canada
(NSERC) Discovery Grants program and the Canada CIFAR AI Chairs program.

REFERENCES

Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via overparameterization. In International Conference on Machine Learning (ICML), 2019.

Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. In International Conference on Machine Learning (ICML),
2018.

Lei Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In Conference on Neural
_Information Processing Systems (NeurIPS), 2014._

Peter Bartlett, Dylan J Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural
networks. Preprint arXiv:1706.08498, 2017.

Monica Bianchini and Franco Scarselli. On the complexity of neural network classifiers: A comparison between shallow and deep architectures. IEEE Transactions on Neural Networks and
_Learning Systems, 25(8):1553–1565, 2014._

Nadav Cohen, Or Sharir, and Amnon Shashua. On the expressive power of deep learning: A tensor
analysis. In Conference on Learning Theory (COLT), 2016.

Amit Daniely. Depth separation for neural networks. In Conference on Learning Theory (COLT),
2017.

Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds
for deep (stochastic) neural networks with many more parameters than training data. Preprint
_arXiv:1703.11008, 2017._

Ronen Eldan and Ohad Shamir. The power of depth for feedforward neural networks. In Conference
_on Learning Theory (COLT), 2016._

Raja Giryes, Guillermo Sapiro, and Alex M Bronstein. Deep neural networks with random Gaussian
weights: A universal classification strategy? IEEE Transactions on Signal Processing, 64(13):
3444–3457, 2016.

Boris Hanin. Which neural net architectures give rise to exploding and vanishing gradients? In
_Conference on Neural Information Processing Systems (NeurIPS), 2018._

Boris Hanin and Mihai Nica. Products of many large random matrices and gradients in deep neural
networks. Communications in Mathematical Physics, pp. 1–36, 2019.

Boris Hanin and Mihai Nica. Finite depth and width corrections to the neural tangent kernel. In
_International Conference on Learning Representations (ICLR), 2020._


-----

Boris Hanin and David Rolnick. How to start training: The effect of initialization and architecture.
In Conference on Neural Information Processing Systems (NeurIPS), 2018.

Boris Hanin and David Rolnick. Complexity of linear regions in deep networks. In International
_Conference on Machine Learning (ICML), 2019._

Boris Hanin and David Rolnick. Deep ReLU networks have surprisingly few activation patterns. In
_Conference on Neural Information Processing Systems (NeurIPS), 2020._

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification. In IEEE International Conference on
_Computer Vision (ICCV), 2015._

Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic
generalization measures and where to find them. Preprint arXiv:1912.02178, 2019.

Henry W Lin, Max Tegmark, and David Rolnick. Why does deep and cheap learning work so well?
_Journal of Statistical Physics, 168(6):1223–1247, 2017._

Guido Mont´ufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. In Conference on Neural Information Processing Systems
_(NeurIPS), 2014._

Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponential expressivity in deep neural networks through transient chaos. In Conference on Neural
_Information Processing Systems (NeurIPS), 2016._

Ilan Price and Jared Tanner. Trajectory growth lower bounds for random sparse deep ReLU networks. Preprint arXiv:1911.10651, 2019.

Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. On the
expressive power of deep neural networks. In International Conference on Machine Learning
_(ICML), 2017._

David Rolnick and Max Tegmark. The power of deeper networks for expressing natural functions.
In International Conference on Learning Representations, 2018.

Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah. Failures of gradient-based deep learning.
In International Conference on Machine Learning (ICML), 2017.

Matus Telgarsky. Representation benefits of deep feedforward networks. _Preprint_
_arXiv:1509.08101, 2015._

Matus Telgarsky. Benefits of depth in neural networks. In Conference on Learning Theory (COLT),
2016.


-----

A EXPERIMENTS FOR CONVOLUTIONAL AND RESIDUAL NETWORKS

We here provide preliminary experimental results for the mean length distortion in networks with
convolutional layers and skip connections.

In Figure 4(a), we show results for networks with sequential convolutional layers (without pooling
layers), initialized as before with weights i.i.d. normal with variance 2/fan-in, and a final fully
connected layer. We use input dimension n0 = 16 × 16 × 3 = 768 and output dimension 5. Our
results indicate that the mean length distortion is, as expected, approximately equal to _nL/n0_

0.08, and that it decays slightly with depth. _≈_

p

In Figure 4(b), we consider residual networks. Here, the overall network N is defined in terms of
residual modules N1, N2, . . ., NL and scales η1, η2, . . ., ηL according to:

(x) = x + η1 1(x) + η2 2(x + η1 1(x)) +
_N_ _N_ _N_ _N_ _· · ·_
+ ηLNL (x + η1N1(x) + η2N2(x + η1N1(x)) + · · · )) .

We set all residual modules _ℓ_ to be two-layer, fully connected ReLU networks. In keeping with
_N_
Hanin & Rolnick (2018), we initialize all weights i.i.d. normal with variance 2/fan-in, while setting
_η1 = η2 = · · · = ηL = 1/L. Our results suggest that mean length distortion again decays modestly_
overall, with a somewhat sharper decrease for small depths.

Figure 4: Mean length distortion as a function of depth, for networks with convolutional layers and
skip connections, initialized using He normal initialization He et al. (2015). (a) shows results for
networks having convolutional layers, where the input dimension n0 equals 16 × 16 × 3 = 768 and
output dimension nL equals 5. Here width corresponds to the number of 3 × 3 kernels in each layer.
As expected, the mean length distortion is _nL/n0_ 0.08 and decays modestly with depth. (b)

shows results for networks with skip connections occurring between even layers (i.e. each residual ≈

p

block is a fully-connected ReLU network of depth 2), again taking a line segment of unit length
as the input curve. Here, depth corresponds to the total number of layers in the network, so that a
depth-20 network includes 10 residual blocks. Both plots (a) and (b) show the empirical mean over
100 initializations of the weights and biases.

B EXPERIMENTAL DETAILS

For all experiments, weights were initialized from i.i.d. normal distributions with variance 2/fan-in
and bias variance 0.1. We run several experiments that involve computing the length distortion of
a given line segment in R[n][0] . We remark on how this was done, for which we rely on the notion of
linear regions and bent hyperplanes associated with ReLU networks, explored in Hanin & Rolnick
(2020); Telgarsky (2015). Specifically, ReLU networks partition input space into a collection of
convex polytopes, which we call linear regions, as (generically) distinct linear functions are defined
on each region corresponding to a different subset of the hidden neurons being activated (where a
neuron is activated if its pre-activation is nonnegative). The boundaries of these linear regions are
given by sets of points for which a particular neuron has preactivation equal to 0.


-----

distortion. Let[0Say we are given a line segment, 1], given by the set w[ℓ] = p {2 **p1p +1. t(p ℓ2 −with endpointsp1) : t ∈** [0 p, 1]1,} p, for which we aim to compute the length2 ∈ R[n][0] parametrized by unit speed on
_−_

We first approximate the intersections of the line segment with linear region boundaries using a
binary search subroutine. Specifically, initialize the set S = [0, 0.5, 1], which will contain the
parameter values for these approximations. For each iteration of the subroutine, we run the following
procedure: for any three consecutive points t1, t2, t3 ∈S, let x1 = p1 + t1(p2 − **p1), x2 = p1 +**
_t2(p2_ **p1), x3 = p1 + t3(p2** **p1), and consider the values** _[||N][ (][x]x[2]2[)][−N]x1[ (][x][1][)][||]_ and _[||N][ (][x]x[3]3[)][−N]x2[ (][x][2][)][||]_ .
_−_ _−_ _||_ _−_ _||_ _||_ _−_ _||_

These values are equal if the three points are in the same linear region, in which case we eliminate t1
and t3 from S. If not equal, then there exists a linear region boundary in the segment from x1 to x3;
we add the points _[t][1][+]2_ _[t][2]_ and _[t][2][+]2_ _[t][3]_ to . We iterate this procedure (15 times in our implementation);

_S_
for the final iteration, we ensure that only the last point associated with a given linear region is in S.

Now take the set = [t1, t2, . . ., tn] returned by the binary search procedure; we proceed to com_S_
pute the parameter values denoting the exact intersections of the line segment with region boundaries, which we shall store in . For consecutive points ti, ti+1 _, i = 1, . . ., n_ 1, determine
and find the neuron at which these sets differ; we solve a linear equation to determine the valuethe set of activated neurons for both points at S _[∗]_ **xi = p1 + ti(p2 − ∈Sp1), xi+1 = p1 + t −i+1(p2 −** **p1),**
_t[∗]_ _∈_ [0, 1] for which this neuron is 0, and replace ti with the exact value t[∗]i [in][ S] _[∗][. Finally, we]_
append 0 and 1 to the respective ends of S _[∗]_ = [t[∗]0 [= 0][, t]1[∗][, . . ., t]n[∗] [= 1]][.]

Computing the length distortion is reduced to summing the lengths of the output segments corresponding to consecutive pairs ti, ti+1 . Namely, for each such pair, the network is
**xof the corresponding output segment is the product ofgiven by a single linear function on the segment of inputs betweeni+1 = p1 + ti+1(p2 −** **p1). We calculate the weight matrix ∈S** _[∗]_ **Wi with the vector Wi of this linear function – the length xi = p w1 +[ℓ]. The total length ti(p2 −** **p1) and**
distortion is then given by the sum

_n−1_

_i=0_ _||Wiw[ℓ]||(t[∗]i+1_ _[−]_ _[t]i[∗][)]_

X


C PROOF OF THEOREM 5.1

Our first step in proving Theorem 5.1 is to obtain bounds on the moments of len(N (M )) in terms of
the input-output Jacobian of N at a single point, which we recall was defined in (10). To accomplish
them, we recall the notation from §6. Namely, fix a smooth unit speed parameterization of M =
_γ([0, 1]) with_
_γ : R →_ R[n][0] _,_ _γ(t) = (γ1(t), . . ., γn0_ (t)) .

The mapping
Γ := N ◦ _γ,_ Γ : R → R[n][L]

gives a parameterization of the curve N (M ), and we have

1
len(N (M )) = 0 _||Γ[′](t)|| dt._ (14)
Z

Let us note an important but ultimately benign technicality. The Jacobian Jx of the map x 7→N (x)
is not defined at every x (namely those x where some neuron turns from on to off). Thus, a priori,
Γ[′](t) exists only at those t for which Jγ(t) exists. However, the formula (14) is still valid. Indeed,
for any setting of weights and biases of N the map Γ is Lipschitz. Thus, by Rademacher’s theorem,
Γ[′](t) exists for almost every t ∈ [0, 1] and the length of the curve is given by integrating the norm
of this almost-everywhere defined derivative. The following simple Lemma is a generalization of
Lemma 6.1 and allows us to bound all moments of len(N (M )) in terms of the moments of the norm
of the Jacobian vector product _Jxu_ .
_||_ _||_

**Lemma C.1. For any integer m ≥** 1, we have

1
E [len(N (M ))[m]] ≤ 0 E _Jγ(t)γ[′](t)_ _dt._ (15)
Z



_[m][]_


-----

_Proof. Taking powers in (14) and using Tonelli’s theorem to interchange the expectation and inte-_
grals, we obtain


1
E [len(N (M ))[m]] = 0
Z


1

_· · ·_ 0
Z


E [len( (M ))[m]] = E Γ[′](tj) _dt1_ _dtm._ (16)
_N_ Z0 _· · ·_ Z0 j=1 _||_ _||_ _· · ·_

Y
 

To proceed, let us recall a special case of the power mean inequality which says that for any ai 0
we have _≥_


_m_

_ai_
_i=1_ _≤_ _m[1]_

Y

Applying this to the integrand in (16), we conclude


_a[m]i_
_i=1_

X


1
E [len(N (M ))[m]] ≤ 0 E _||Γ[′](t)||[m][]_ _dt._ (17)
Z



Next, fix t ∈ [0, 1]. For any neuron z in N, denote by x 7→ _z(x) its pre-activation. Note that_
P(z(γ(t)) = 0) = 0 since our bias variance Cb is set to some fixed positive constant and hence the
bias of each neuron has a continuous density. Therefore, with probability 1 over the randomness in
the weights and biases of N, there exists a neighborhood U of γ(t) on which z(x) has constant sign
for all x ∈U. The Jacobian Jγ(t) is therefore well-defined and the chain rule yields

Γ[′](t) = Jγ(t)γ[′](t). (18)

Substituting this into (17) completes the proof.

Having obtained the expression (15), we have reduced bounding the moments of len(N (M )) to
bounding the moments of the random variable ||Jxu|| for a fixed x ∈ R[n][0] and unit vector u ∈ R[n][0] .
Prior work (e.g. Thm. 1 in Hanin & Nica (2019)) shows that these moments satisfy


_L−1_

_n[−]ℓ_ [2]
_ℓ=1_

X


!!


_nL_
_Jxu_ =
_||_ _||[2][m][i]_ _n0_



_m_
exp



_m_ _L−1_
2 _n[−]ℓ_ [1] + O
  _ℓ=1_

X


provided
_m_

_<_ min (19)

 2  _ℓ=1,...,L−1_ _[n][ℓ][.]_

Substituting these moment estimates in (15) completes the derivation of (4). However, the results in
Hanin & Nica (2019) are subtle because they apply to any distribution of weights and biases. They
also give the slightly sub-optimal restriction (19) that m[2] must be smaller than a constant times the
minimum of the nℓ’s. In the special case where the distribution of weights and biases is Gaussian, we
can do slightly better by computing the moments of _Jxu_ more directly (note that in the statement
_||_ _||_
of Theorem 5.1, we required only that m is smaller than a constant times the minimum of the nℓ’s).
We will need this anyway to derive the slightly more refined estimates in (2) and (3). Let us therefore
check that, in distribution, _Jxu_ is equal to a product of independent random variables:
_||_ _||_

**Proposition C.2. For any x ∈** R[n][0] _and any unit vector u ∈_ R[n][0] _, the random variable ||Jxu||[2]_ _is_
_equal in distribution to a product of independent scaled chi-squared random variables_


_L−1_

_ℓ=1_

Y


_Jxu_ = _[n][L]_
_||_ _||[2][ d]_ _n0_



[1] _χ[2]nL_ _[,]_

_·_ _nL_


2

_nℓ_ _χ[2]nℓ_


_where the number of degrees of freedom in the ℓth term of the product (for ℓ_ = 1, . . ., L _−_ 1) is given
_by an independent binomial_

_d_
**nℓ** = Bin (nℓ, 1/2)

_with nℓ_ _trials and success probability 1/2. The number of degrees of freedom in the final term is_
_deterministic and given by nL._


-----

_Proof. Consider a ReLU network_ with input dimension 1, output dimension nL, and hidden layer
_N_
widths n1, . . ., nL−1. Suppose the weight matrices W [(][ℓ][)] and bias vectors b[(][ℓ][)] are independent with
i.i.d. Gaussian components:

_Wij[(][ℓ][)]_ _∼_ _G(0, 2/nℓ−1),_ _b[(]j[ℓ][)]_ _∼_ _G(0, Cb),_

where Cb > 0 is any fixed constant. For a fixed network input x ∈ R[n][0], with probability 1, the
input-output Jacobian Jx is well-defined. Moreover, it can be written as

_Jx = W_ [(][L][)]D[(][L][−][1)]W [(][L][−][1)] _· · · D[(1)]W_ [(1)],

where W [(][ℓ][)] is the matrix of weights from layer ℓ 1 to layer ℓ and D[(][ℓ][)] is an nℓ _nℓ_ diagonal
_−_ _×_
matrix:

_D[(][ℓ][)]_ = Diag **1[n]** _, i = 1, . . ., nℓ_
_zi[(][ℓ][)]_ 0
_≥_
 o 

whose diagonal entries are 0 or 1 depending on whether the pre-activation zi[(][ℓ][)] of neuron i in layer
_ℓ_ is positive at our fixed input x. Next, according to Proposition 2 in Hanin & Nica (2019), the
marginal distribution of each D[(][ℓ][)] is that its diagonal entries are independent Bernoulli(1/2) random
variables. Moreover, we have the following equality in distribution:

_Jx_ =d _ηW_ [(][L][)] _W_ [(][L][−][1)] _W_ [(1)]
_D[(][L][−][1)]_ _· · · D[(1)]_

where D[(][ℓ][)] are independent of each other (and of W [(][ℓ][)]) resampled from the marginal distribution
of D[(][ℓ][)] and η is an independent diagonal matrix with independent diagonal entries that are ±1 with
equal probability. In particular, for a fixed unit vector u ∈ R[n][0], we have

_Jxu_ =d _W_ [(][L][)] _W_ [(][L][−][1)] _W_ [(1)]u _._
_||_ _||_ _||_ _D[(][L][−][1)]_ _· · · D[(1)]_ _||_

We may rewrite this as

_||W_ [(][L][)]D[(][L][−][1)]W [(][L][−][1)] _· · · D[(2)]W_ [(2)]u[(1)]|| ||D[(1)]W [(1)]u||, (20)

for u[(1)] := _||DD[(1)][(1)]WW_ [(1)][(1)]uu|| [, where we interpret the expression (20) as equal to][ 0][ if][ D][(1)][ is the zero]

matrix. To complete the proof, we need the following standard observation.

**Lemma C.3. Suppose W is an n × n[′]** _matrix with i.i.d. Gaussian entries and u is a random unit_
_vector in R[n][′]_ _that is independent of W but otherwise has any distribution. Then Wu is independent_
_of u and is equal in distribution to Wv where v is any fixed unit vector in R[n][′]_ _._

_Proof. For any fixed orthogonal matrix O, it is a standard fact that W_ _O is equal in distribution to_
_W_ . Thus, for any measurable sets A ⊆ R[n] and B ⊆ R[n][′], since u, W are independent we have


P (Wu ∈ _A, u ∈_ _B) =_


_S[n][′−][1][ P][ (][Wu][0][ ∈]_ _[A][)][ d][P][u][(][u][0][)][,]_


wherematrix so that Pu(u0) u is the distribution of0 = Oe1 with e1 = (1 u., Fix any 0, . . ., 0) u is the first standard unit vector. Then, since0 ∈ _S[n][′][−][1]_ and let O = O(u0) be any orthogonal W _O is_
equal in distribution to W, we have

P (Wu0 ∈ _A) = P (We1 ∈_ _A),_

which is independent of u0. We therefore find


P (Wu ∈ _A, u ∈_ _B) = P (We1 ∈_ _A) P(u ∈_ _B),_


as desired.


We are now in a position to complete the proof of Proposition C.2. Combining Lemma C.3 with
(20), we find that, in distribution _Jxu_ equals
_||_ _||_

_||W_ [(][L][)]D[(][L][−][1)]W [(][L][−][1)] _· · · D[(2)]W_ [(2)]u|| ||D[(1)]W [(1)]u||. (21)


-----

Note that these two terms are independent. Repeating this argument, we obtain that _Jxu_ is equal
_||_ _||_
in distribution to the product

_||W_ [(][L][)]u|| ||D[(][L][−][1)]W [(][L][−][1)]u|| · · · ||D[(1)]W [(1)]u||. (22)

The terms in this product are independent. To complete the proof note that for ℓ = 1, . . ., L − 1
the number of non-zero entries in is a binomial random variable nℓ with nℓ trials, each with
_D[(][ℓ][)]_
probability of success 1/2 and that
_||D[(][ℓ][)]W_ [(][ℓ][)]u||[2]

is precisely 2/nℓ times a sum of nℓ squares of independent standard Gaussians. Thus, for ℓ =
1, . . ., L − 1,

2
_W_ [(][ℓ][)]u = _χ[2]nℓ_ _[.]_ (23)
_||D[(][ℓ][)]_ _||[2][ d]_ _nℓ−1_

Similarly

2
_W_ [(][L][)]u = _χ[2]nL_ _[.]_ (24)
_||_ _||[2][ d]_ _nL−1_

Substituting (23) and (24) into (22) completes the proof.

Evaluating the moments of len(N (M )) is now a matter of computing the moments of some scaled
chi-squared random variables with a random number of degrees of freedom. For instance, recalling
that
E _χ[2]k_ = k

and applying Lemma C.2 as well as the tower property of the expectation we find 


_L−1_ 2

E _Jxu_ = _[n][L]_ E _χ[2]nℓ_ E _n[−]L_ [1][χ]n[2] _L_
_||_ _||[2][i]_ _n0_ _nℓ_
h _ℓY=1_  [!] 

_L−1_ 2

= _[n][L]_ E E _χ[2]nℓ_ _[|][ n][ℓ]_

_n0_ _nℓ_

_ℓ=1_ 

Y   []

_L−1_ 2

= _[n][L]_ E [nℓ]

_n0_ _nℓ_

_ℓ=1_  

Y

= _[n][L]_ _._

_n0_

Substituting this into (15) with m = 2 yields

Var [len( (M ))] _,_
_N_ _≤_ _[n]n[L]0_


yielding (3). To prove (2), we need to estimate E [len N (M )]. By taking expectations in (14) and
using (18), we find

1
E [len N (M )] = 0 E _Jγ(t)γ[′](t)_ _dt = E [||Jxu||],_
Z

where we’ve used that by Lemma C.2, the distribution of _||J[]xu|| is the same for every x ∈_ R[n][0] and
every unit vector u. Moreover, again using Lemma C.2, we see that E [||Jxu||] equals

1/2 L 1 1/2[#] 1/2[#]

_n0_ _−_ 2 1

_nL_ E _nℓ_ _χ[2]nℓ_ E _nL_ _χ[2]nL_ _._

  _ℓ=1_ "  " 

Y

To simplify this, a direct computation shows that


_k[−][1]χ[2]k_ 1/2[i] = Γ k _k+12 k_ 1/2 _[,]_

Γ 2  2 

h  

     


-----

where Γ(·) is the Gamma function. Next, for any positive random variable X with E [X] = 1 we
have

E _X_ [1][/][2][i] = E (1 + (X − 1))[1][/][2][i]
h h

= 1
_−_ 8[1] [Var[][X][] +][ O][(][|][X][ −] [1][|][3][)][.]

Recall that
Var[χ[2]k[] = 2][k.]

Using this and the law of total variance yields

1/2[#]

2 1

E _nℓ_ _χ[2]nℓ_ = 1 − 2n[2]ℓ Var[χ[2]nℓ [] +][ O][(][n]ℓ[−][2][)]

" 

1
= 1 E Var[χ[2]nℓ _[|][ n][ℓ][]]_
_−_ 2n[2]ℓ


 


+ Var[E _χ[2]nℓ_ _[|][ n][ℓ]_ ] + O(n[−]ℓ [2][)]

 


1

[E [2nℓ] + Var[nℓ]] + O(n[−]ℓ [2][)]
2n[2]ℓ


= 1 −

= 1 −

= 1 −


_nℓ_ + _[n][ℓ]_


+ O(n[−]ℓ [2][)]


2n[2]ℓ


5

+ O(n[−]ℓ [2][)][.]
8nℓ


Thus, we find that E [len(N (M ))] equals

1/2 _nL+1_ _L_ 1 _L_ 1

 _nnL0_  Γ _nΓ2L _ 2 n2L 1/2 _[×][ exp]_ "− 8[5] _ℓ=1−_ _n[−]ℓ_ [1] + O _ℓ=1−_ _n[−]ℓ_ [2]!# _,_

X X

as claimed. Finally, let us check the higher moment estimates (4). By Lemma C.2 we have the     
following estimate


_L−1_ 2

_Jxu_ = _[n][L]_ _χ[2]nℓ_ _n[−]L_ [1][χ]n[2] _L_
_||_ _||[2]_ _n0_ _ℓ=1_ _nℓ_ !

Y

_L−1_ 2

exp _χ[2]nℓ_ _[−]_ [1]

_≤_ _[n]n[L]0_ " _ℓ=1_  _nℓ_ [#]

X


_n[−]L_ [1][χ]n[2] _L_ _[,]_


where we used that x = x − 1 + 1 ≤ _e[x][−][1]. For any m, we have_

_m_

1
E _nL_ _χ[2]nL_ = 1 + n[2]L _· · ·_ 1 + [2][m]n[ −]L [2]
     

_m−1_ 2j _m2_

exp exp
_≤_  _nL_  _≤_ _nL_

_j=0_  

X
 

Therefore, _Jxu_ is bounded above by
_||_ _||[2][m]_
_nL_ _m_ _L−1_ 2

exp _m_ _χ[2]nℓ_ _[−]_ [1] + _[m][2]_ _._

 _n0_  " _ℓ=1_  _nℓ_  _nL_ #

X

Finally, for any fixed positive integer n we may write


(25)


2
_n_ _[χ]n[2]_ _[−]_ [1 = 2]n


_ξkZk[2]_ _[−]_ 2[1]


_k=1_


-----

where ξk are independent Bernoulli(1/2) random variables and Zk are independent standard Gaus
SEsians. A direct computation shows that the centered random variables(ν[2], α) with some universal parameters ν[2], α. Thus, by the stability of sub-exponential random ξkZk[2] _[−]_ 2[1] [are sub-exponential]
variables under summation we have

2 4ν2

_χ[2]nℓ_ _[−]_ [1][ ∈] [SE] _,_ [2][α] _._
_nℓ_ _nℓ_ _nℓ_
 

Again using this property we conclude


_L−1_

_ℓ=1_

X


_L−1_
4ν[2]

_ℓ=1_

X


1

_,_ [2][α]
_nℓ_ _n∗_


2

_χ[2]nℓ_ _[−]_ [1][ ∈] [SE]
_nℓ_


where
_n∗_ = min {n1, . . ., nL−1} .

Therefore, by (25), we find that


_nL_
_Jxu_
_||_ _||[2][m][i]_ _≤_ _n0_
h 


_nL_ _m_ _m2_

E _e[mY][ ]_ exp _,_

[2][m][i] _≤_ _n0_ _nL_
   



. By definition of sub-exponential random variables, we have


where Y SE 4ν[2][ P][L]ℓ=1[−][1]
_∈_



1

_nℓ_ _[,][ 2]n[α]_

_∗_


_L−1_
4m[2]ν[2]

_ℓ=1_

X


_L−1_ 1

E _e[mY][ ]_ exp 4m[2]ν[2] _,_
_≤_ " _ℓ=1_ _nℓ_ #
 X

provided m < n _/2α for some universal constant c > 0. This completes the derivation of (4) and_
_∗_
completes the proof of Theorem 5.1. □


D PROOF OF THEOREM 5.2

The proof of Theorem 5.2 follows closely the argument for Theorem 5.1. For a given input x ∈ R[n][0],
we will continue to write
_J_ (x) := _∂xj_ _i(x)_ 1 _j_ _n0_
_N_ _N_ n1≤≤i≤≤nLo
  

for the input-output Jacobian of the network function, which exists for Lebesgue almost every x ∈
R[n][0] . We will write ΠTxM : R[n][0] _→_ _TxM for the orthogonal projection onto this tangent space. We_
have

1/2

vold( (M )) = det ΠTxM _J_ (x)[T] _J_ (x)ΠTxM vold(dx),
_N_ _M_ _N_ _N_
Z

    

where vold is the d−dimensional Hausdorff measure. Indeed, the integrand measures, at each x ∈
_M_, the volume of the image of an infinitesimal cube on M of volume vold(dx) centered at x under
the map x 7→N (x). Arguing precisely as in the proof of Lemma C.1, we find that for any integer
_m ≥_ 1

_m/2_

E [vold( (M ))[m]] E det ΠTxM _J_ (x)[T] _J_ (x)ΠTxM vold(dx) (26)
_N_ _≤_ _M_ _N_ _N_
Z

   

Fix x ∈ _M and denote by_
_e1(x), . . ., ed(x)_

an orthonormal basis of the tangent space of M . Then, by the Gram identity, we may write

det ΠTxM _J_ (x)[T] _J_ (x)ΠTxM = _J_ (x)e1(x) _J_ (x)ed(x) _,_ (27)
_N_ _N_ _||_ _N_ _∧· · · ∧_ _N_ _||[2]_

where we recall that the wedge product is the anti-symmetrization of the tensor product.   Just
as in the proof of Theorem 5.1, the key observation is that for Gaussian weights we have that
_J_ (x)e1(x) _J_ (x)ed(x) is a product of i.i.d. random variables. The formal statement
_||_ _N_ _∧· · · ∧_ _N_ _||_
is the following:


-----

**Proposition D.1.the random variable For any x ∈** R[n][0] _and collection of orthonormal unit vectors u1, . . ., ud ∈_ R[n][0] _,_
_Jxu1_ _Jxud_
_||_ _∧· · · ∧_ _||[2]_

_is equal in distribution to a product of independent scaled chi-squared random variables:_
_nL_ _d_ _L−1_ _d_ 2 _d_ 1

_n0_  _nℓ_ _χ[2]nℓ−j+1_ _·_ _nL_ _χ[2]nL−j+1[,]_

  _ℓ=1_ _j=1_ _j=1_

Y Y Y

_where all the terms in the product are independent and for_  _ℓ_ = 1, . . ., L 1 we’ve written nℓ _for_
_−_
_independent Binomial random variables:_

_d_
**nℓ** = Bin (nℓ, 1/2)
_with nℓ_ _trials and success probability 1/2._

_Proof. The proof is identical to that of Lemma C.2. The only difference is that we must invoke the_
following amplification of Lemma C.3:

**Lemma D.2.any distribution. Let Let u1 W, . . ., u be an independentk ∈** R[n][′] _be a collection of orthonormal vectors (i.e. a collection) with n × n[′]_ _matrix with i.i.d. Gaussian entries. Then_
_W_ (u1 _uk) = Wu1_ _Wuk_
_∧· · · ∧_ _∧· · · ∧_
_is independent ofany fixed collection of orthonormal vectors. u1 ∧· · · ∧_ _uk and is equal in distribution to Wv1 ∧· · · Wvk where v1, . . ., vk is_

_Proof. The proof is identical to that of Lemma C.3, except that we note that for that, given_
_u1, . . ., uk, there exists an orthogonal matrix_ = (u1, . . ., uk) so that
_O_ _O_
_uj = Oej,_ _j = 1, . . ., k_
where ej are the standard basis vectors.

With Lemma D.1 in hand, we complete the proof of Theorem 5.2 as follows. First, note that for any
random variable X we have

E [X] ≤ E _X_ [2][][1][/][2] _,_ Var[X] ≤ E _X_ [2][] _._
Hence, Theorem 5.2 will follow once we show that
 

_d_ _L_

_nL_ _d_
E vold(N (M ))[2][] _≤_  _n0_  exp "−2 _ℓ=1_ _n[−]ℓ_ [1]# _._
 X

Combining Proposition D.1 with (26) and (27), this estimate follows by showing that

_d_ _L_

_nL_ _d_
E h||Jxu1 ∧· · · ∧ _Jxud||[2][i]_ _≤_  _n0_  exp "−2 Xℓ=1 _n[−]ℓ_ [1]# _._ (28)

To check this, recall that
E _χ[2]k_ = k.
Hence,
 

_nL_ _d_ _L−1_ _d_ 2 _d_ 1
E _J_ (x)e1(x) _J_ (x)ed(x) = E _χ[2]nℓ_ _j+1_ E _χ[2]nL_ _j+1_
_||_ _N_ _∧· · · ∧_ _N_ _||[2][i]_ _n0_  _nℓ_ _−_  _·_ _nL_ _−_
h   _ℓY=1_ _jY=1_   _jY=1_ 

_d_ L _d_ 

_nL_
= 1

_n0_ _−_ _[j][ −]nℓ_ [1]

  _ℓ=1_ _j=1_  

Y Y

_d_ _L_

_nL_ _d_
_≤_  _n0_  exp "−2 _ℓ=1_ _n[−]ℓ_ [1]# _,_

X

where in the last line we used that 1 + x ≤ _e[x]. This completes the proof._ □


-----

