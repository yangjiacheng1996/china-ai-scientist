# GENERALIZED RECTIFIER WAVELET COVARIANCE
## MODELS FOR TEXTURE SYNTHESIS


**Antoine Brochard**
ENS, PSL University, Paris, France
antoine.brochard@ens.fr

**St´ephane Mallat**
Coll`ege de France, Paris, France
Flatiron Institute, New York, USA


**Sixin Zhang**
Universit´e de Toulouse, INP, IRIT, Toulouse, France
sixin.zhang@irit.fr

ABSTRACT


State-of-the-art maximum entropy models for texture synthesis are built from
statistics relying on image representations defined by convolutional neural networks (CNN). Such representations capture rich structures in texture images, outperforming wavelet-based representations in this regard. However, conversely to
neural networks, wavelets offer meaningful representations, as they are known
to detect structures at multiple scales (e.g. edges) in images. In this work, we
propose a family of statistics built upon non-linear wavelet based representations,
that can be viewed as a particular instance of a one-layer CNN, using a generalized
rectifier non-linearity. These statistics significantly improve the visual quality of
previous classical wavelet-based models, and allow one to produce syntheses of
similar quality to state-of-the-art models, on both gray-scale and color textures.
We further provide insights on memorization effects in these models.

1 INTRODUCTION

Textures ares spatially homogeneous images, consisting of similar patterns forming a coherent ensemble. In texture modeling, one of the standard approaches to synthesize textures relies on defining
a maximum entropy model (Jaynes, 1957) using a single observed image (Raad et al., 2018). It consists of computing a set of prescribed statistics from the observed texture image, and then generating
synthetic textures producing the same statistics as the observation. If the statistics correctly describe
the structures present in the observation, then any new image with the same statistics should appear
similar to the observation. A major challenge of such methods resides in finding a suitable set of
statistics, that can generate both high-quality and diverse synthetic samples. This problem is fundamental as it is at the heart of many texture related problems. For example, in patch re-arrangement
methods for texture modeling, these statistics are used to compute high-level similarities of image
patches (Li & Wand, 2016; Raad et al., 2018). Such models are also used for visual perception
(Freeman & Simoncelli, 2011; Wallis et al., 2019; Vacher et al., 2020), style transfer (Gatys et al.,
2016; Deza et al., 2019) and image inpainting (Laube et al., 2018).

A key question along this line of research is to find what it takes to generate natural textures. This
problem was originally posed in Julesz (1962), in which the author looks for a statistical characterization of textures. In the classical work of Portilla & Simoncelli (2000) (noted PS in this work),
the authors presented a model whose statistics are built on the wavelet transform of an input texture
image. These statistics were carefully chosen, by showing that each of them captured a specific
aspect of the structure of the image. This model produces satisfying results for a wide range of
textures, but fails to reproduce complex geometric structures present in some natural texture images. Figure 1 presents a typical example composed of radishes, and synthetic images from three
state-of-the-art models developed over the last few decades. To address this problem, the work of
Gatys et al. (2015) proposes to use statistics built on the correlations between the feature maps of
a deep CNN, pre-trained on the ImageNet classification problem (Deng et al., 2009; Simonyan &
Zisserman, 2014). While this model produces visually appealing images, these statistics are hard to


-----

interpret. The work of Ustyuzhaninov et al. (2017) made a significant simplification of such statistics, by using the feature maps of a one-layer rectifier CNN with random filters (without learning). A
crucial aspect of this simplification relies on using multi-scale filters, which are naturally connected
to the wavelet transform. In this paper, we propose a wavelet-based model, more interpretable than
CNN-based models (with learned or random filters), to synthesize textures with complex geometric
structures. It allows to bridge the gap between the classical work of Portilla & Simoncelli (2000),
and state-of-the-art models.

Observation PS VGG RF

Figure 1: Example of syntheses from three texture models in chronological order. From left to right:
the observed texture in gray-scale, synthesis from PS (Portilla & Simoncelli, 2000), from VGG
(Gatys et al., 2015), and from RF[1](Ustyuzhaninov et al., 2017).

This model is built on the recent development of the phase harmonics for image representations and
non-Gaussian stationary process modeling (Mallat et al., 2020; Zhang & Mallat, 2021). The phase
harmonics are non-linear transformations that adjust the phase of a complex number. In Portilla
& Simoncelli (2000); Zhang & Mallat (2021), the authors illustrate that the phase dependencies
between wavelet coefficients across scales contain important information about the geometric structures in textures and turbulent flows, and that they can be captured by applying the phase harmonics
to complex wavelet coefficients. Remarkably, Mallat et al. (2020) show that the phase harmonics
admit a dual representation, closely related to the rectifier non-linearity in CNNs. Our main contributions are:

-  We develop a family of texture models based on the wavelet transform and a generalized
rectifier non-linearity, that significantly improves the visual quality of the classical waveletbased model of Portilla & Simoncelli (2000) on a wide range of textures. It relies on
introducing spatial shift statistics across scales to capture geometric structures in textures.

-  By changing the number of statistics in our models, we show explicitly the trade-off on
the quality and diversity of the synthesis. When there are too many statistics, our model
tends to memorize image patches. We further investigate such memorization effects on
non-stationary images and find that it sometimes relies on what statistics are chosen, rather
than on how many.

-  Through the modeling of geometric structures in gray-scale textures, our model indicates
the possibility of reducing significantly the number of statistics in the works of Gatys et al.
(2015) and Ustyuzhaninov et al. (2017), to achieve a similar visual quality.

The rest of the paper is organized as follows: Section 2 reviews the framework of microcanonical
maximum-entropy models, build upon a general family of covariance statistics. We then present
our model for both gray-scale and color textures in Section 3. Section 4 shows synthesis results
of our model, compared with state-of-the-art models. Finally, in Section 5, we discuss possible
improvements of our model[2].

**Notations** Throughout the paper, N denotes a positive integer. A gray-scale image x is an element
of R[N] _[×][N]_, i.e. x = x(u), u ∈ ΩN, with ΩN := {0, · · ·, N − 1}[2]. A color image x = {x[c]}c=1,2,3 is
an element of R[3][×][N] _[×][N]_, or equivalently, each x[c] _∈_ R[N] _[×][N]_ . We shall denote ¯x the observed texture
(observation), which is assumed to be a realisation a random vector X. For any complex number
_z ∈_ C, z[∗] is the complex conjugate of z, Real(z) its real part, |z| its modulus, and ϕ(z) its phase.

1As the model from Ustyuzhaninov et al. (2017) uses on random filters, we shall use the abbreviation RF.
[2All calculations can be reproduced by a Python software available at https://github.com/](https://github.com/abrochar/wavelet-texture-synthesis)
[abrochar/wavelet-texture-synthesis.](https://github.com/abrochar/wavelet-texture-synthesis)


-----

2 MICROCANONICAL COVARIANCE MODELS

We briefly review the standard framework of micro-canonical maximum-entropy models for textures. To reliably estimate the statistics in these models, we assume that a texture is a realization of
a stationary and ergodic process X (restricted to ΩN ). We then review a special family of statistics
that are used in the state-of-the-art texture models (mentioned in Figure 1), based on covariance
statistics of an image representation.

2.1 FRAMEWORK

Given a observation texture ¯x, we aim at generating new texture images, similar but different from
_x¯. To that end, a classical method is to define a set of statistics Cx¯, computed on the observation,_
and try to sample from the microcanonical set


_{x : ∥Cx −_ _Cx¯∥≤_ _ϵ},_

where ∥· ∥ denotes the L[2] norm.

Under the stationary and ergodic assumption of X, one can construct Cx as a statistical estimator of
E(CX), from a complex-valued representation Rx.[3] The set of covariance statistics Cx of a model
can then be constructed by computing an averaging over the spatial variable u, i.e.


_Cx(γ, γ[′], τ_ ) :=


_Rx(γ, u)Rx(γ[′], u −_ _τ_ )[∗], (1)
_uX∈ΩN_


ΩN
_|_ _|_


for (γ, γ[′], τ ) Υ Γ Γ ΩN . The statistics Cx(γ, γ[′], τ ) can be interpreted as estimating the
_∈_ _⊆_ _×_ _×_
covariance (resp. correlations) between RX(γ, u) and RX(γ[′], u−τ ) for zero-mean RX (resp. nonzero mean RX). The ergodicity assumption ensures that when N is large enough, the approximation
_Cx¯ ≃_ E(CX) over Υ should hold with high probability. Under these conditions, it makes sense to
sample the microcanonical set in order to generate new texture samples.

This framework encompasses a wide range of state-of-the-art texture models[4]. In particular, the PS
model takes inspiration from the human early visual system to define a multi-scale representation
based on the wavelet transform of the image (Heeger & Bergen, 1995). We next review a family of
covariance model which generalizes the statistics in the PS model. We write C [M] the statistics for a
specific model M that uses the representation R[M].

2.2 WAVELET PHASE HARMONIC COVARIANCE MODELS

We review a family of microcanonical covariance models defined by a representation built upon
the wavelet transform and phase harmonics. It defines a class of covariance statistics that capture
dependencies between wavelet coefficients across scales.

2.2.1 WAVELET TRANSFORM

The wavelet transform is a powerful tool in image processing to analyze signal structures, by defining a sparse representation (Mallat, 2001). For texture modeling, we consider oriented wavelets
to model geometric structures in images at multiple scales. They include the Morlet wavelets and
steerable wavelets, proposed in Goupillaud et al. (1984); Simoncelli & Freeman (1995); Unser &
Chenouard (2013). In particular, the Simoncelli steerable wavelets have been used to model a diverse
variety of textures in Portilla & Simoncelli (2000).

Oriented wavelets are defined by the dilation and rotation of a complex function ψ : R[2] _7→_ C on a
plane. Let rθ denote the rotation by angle θ in R[2]. They are derived from ψ with dilations by factors
2[j], for j ∈{0, 1, · · ·, J − 1}, and rotations rθ over angles θ = ℓπ/L for 0 ≤ _ℓ< L, where L is the_
number of angles in [0, π). The wavelet at scale j and angle θ is defined by

_ψj,θ(u) = 2[−][2][j]ψ(2[−][j]rθu),_ _u ∈_ R[2].

34The complex-valued representatione.g. Portilla & Simoncelli (2000); Gatys et al. (2015); Ustyuzhaninov et al. (2017); Zhang & Mallat (2021) Rx(γ, u) ∈ C is a function of (γ, u) in an index set Γ × ΩN .


-----

Scales equal or larger than J are carried by a low-pass filter φJ .

The wavelet transform of an image x ∈ R[N] _[×][N]_ is a family of functions obtained by the convolution
of x with discrete wavelets.[5] Let Λ := {0, · · ·, J − 1} × _L[π]_ _[{][0][,][ · · ·][, L][ −]_ [1][}][ be an index set. The]

wavelet coefficients are
_x ⋆ψj,θ(u) =_ _x(u_ _v)ψj,θ(v),_ _u_ ΩN _,_ (j, θ) Λ. (2)

_−_ _∈_ _∈_
_vX∈ΩN_

The low-pass coefficients x ⋆φJ are defined similarly.

2.2.2 WAVELET PHASE HARMONICS AND THE PS MODEL

To model natural textures, it has been shown (Portilla & Simoncelli, 2000; Zhang & Mallat, 2021)
that it is crucial to capture statistical dependencies between wavelet coefficients across scales. This
can be achieved by using a wavelet phase harmonic representation, which is defined by the composition of a linear wavelet transform of x, and a non-linear phase harmonic transform.

In Mallat et al. (2020), the authors introduce the phase harmonics to adjust the phase of a complex
number z ∈ C. More precisely, the phase harmonics {[z][k]}k∈Z of a complex number z ∈ C are
defined by multiplying its phase ϕ(z) of z by integers k, while keeping the modulus constant, i.e.
_∀_ _k ∈_ Z, [z][k] := |z|e[ikϕ][(][z][)]. The wavelet phase harmonic representation (WPH) is then defined by

_R[WPH]x(γ, u) = [x ⋆ψj,θ(u)][k]_ _−_ _µγ,_ _γ = (j, θ, k) ∈_ Γ = Λ × Z, (3)

where µγ is defined as the spatial average of [¯x ⋆ψj,θ][k].

It is shown in Zhang & Mallat (2021) that the PS model can be regarded as a low-order wavelet
phase harmonics covariance model, which considers only a restricted number of pairs (k, k[′]) (see
Appendix A for more details). In the next section, we shall use a dual representation of the phase
harmonic operator to define a covariance model to capture high-order phase harmonics.

3 GENERALIZED RECTIFIER WAVELET COVARIANCE MODEL

In the previous section, we presented a class of models, built from the wavelet phase harmonic representation. A dual representation of the phase harmonic operator [·][k] can be defined via a generalized
rectified linear unit, that we review in Section 3.1. We then discuss in Section 3.2 how to define an
appropriate index set of Γ for gray-scale textures. Section 3.3 extends the model to color textures.

3.1 FROM PHASE HARMONICS TO THE GENERALIZED RECTIFIER

The generalized rectified linear unit of a complex number z, with a phase shifted by α ∈ [0, 2π], is
defined by
_ρα(z) = ρ(Real(e[iα]z)),_ (4)

where ρ is a rectified linear unit, i.e. for any t ∈ R, ρ(t) := max(0, t). In Mallat et al. (2020),
it is shown that applying a Fourier transform on ρα(z) along the variable α results in the phase
harmonics of z (up to some normalization constant). This suggests an alternative model, defined by
coefficients of the form
_R[ALPHA]x(γ, u) = ρα(x ⋆ψj,θ(u))_ _µγ,_ _γ = (j, θ, α),_ (5)
_−_
for γ ∈ Γ = Λ × [0, 2π], and µγ is defined as the spatial average of ρα(¯x ⋆ψj,θ(u)) over u ∈ ΩN .

**Relation with high-order phase harmonics** Based on the duality between the phase harmonics
_k ∈_ Z and the phase shift variable α ∈ [0, 2π], we now present the relation between C [ALPHA] and
the high-order phase harmonics in C [WPH], first proved in Mallat et al. (2020).

**Proposition 1 There exists a complex-valued sequence {ck}k∈Z such that for all (j, θ, α) ∈** Γ,
(j[′], θ[′], α[′]) Γ, and all τ ΩN _,_
_∈_ _∈_

_C_ [ALPHA]x((j, θ, α), (j[′], θ[′], α[′]), τ ) = _ckc[∗]k[′]_ _[C]_ [WPH][x][((][j, θ, k][)][,][ (][j][′][, θ][′][, k][′][)][, τ] [)][e][i][(][kα][−][k][′][α][′][)][.]

(k,kX[′])∈Z[2]

5The continuous wavelets are discretized with periodic boundary conditions on the spatial grid ΩN .


-----

The proof is given in Appendix B. We remark that the sequence {ck}k∈Z is uniquely determined
by the rectifier non-linearity ρ, and they are non-zero if k is even (Mallat et al., 2020). This result
shows that for a suitable choice of (α, α[′]), the covariance statistics C [ALPHA]x can implicitly capture
_C_ [WPH]x with a wide range of k and k[′].

**Relation with second order statistics** Using a simple decomposition of wavelet coefficients into
their positive, negative, real and imaginary parts, we can further show that the covariance statistics
_C_ [ALPHA]x capture the classical second order statistics of wavelet coefficients, also used in the PS
model (with phase harmonic coefficients k = k[′] = 1).

_{Proposition 2wα,α′_ _}(α,α′)∈ LetI_ 2 such that for all I = _{0,_ _[π]2 ([, π,]j, θ[ 3]) ∈2[π]_ _[}]Λ[.]_ _, and allThere exists a finite complex-valued sequence τ ∈_ ΩN _,_

_wα,α[′]_ _C_ [ALPHA]x((j, θ, α), (j[′], θ[′], α[′]), τ ) = _x ⋆ψj,θ(u)_ _x ⋆ψj′,θ′_ (u _τ_ ) _∗. (6)_

_−_

(α,αX[′])∈I [2] _uX∈ΩN_     

The proof is given in Appendix C. This shows that using only four α uniformly chosen between

[0, 2π] is sufficient to capture second order statistics. Because the wavelet transform is an invertible
linear operator (on its range space), computing the r.h.s of eq. (6) for all (j, θ, τ ), as well as the
low-pass coefficients carried out by ΦJ, is equivalent to computing the correlation matrix of x.

**Relation with the RF model** Setting aside the subtraction by the spatial mean µγ, the RF model
can be viewed as a particular case of models defined by eq. (5). Indeed, the statistics of the RF
model take the form of eq. (1), with

_R[RF]x(f, u) = ρ(x ⋆ψf_ (u)),

where _ψf_ being a family of multi-scale random filters. By writing ρα(x ⋆ψj,θ(u _τ_ )) = ρ(x ⋆
_{_ _}_ _−_
Real(ψj,θ[τ] _[e][iα][)(][u][))][, with][ ψ]j,θ[τ]_ [denoting the translation of][ ψ][j,θ][ by][ τ] [, we see that the models are similar,]
the difference being that our models use wavelet-based filters instead of random ones.

3.2 DEFINING AN APPROPRIATE Υ

The choice of the covariance set Υ is of central importance in the definition of the model. Intuitively,
a too small set of indices will induce a model that could miss important structural information about
the texture that we want to synthesize. Conversely, if Υ contains too many indices, the syntheses
can have good visual quality, but the statistics of the model may have a large variance, leading to the
memorization of some patterns of the observation. There is a trade-off between these two aspects:
one must capture enough information to get syntheses of good visual quality, but not much, so as not
to reproduce parts of the original image. To illustrate this point, we shall study the model ALPHA
defined with three different sets Υ : A smaller model ALPHAS with a limited amount of elements
in Υ, an intermediate model ALPHAI, and a larger model ALPHAL.

To precisely define these models, let us note2π J := {0, · · ·, J _−1}, Θ :=_ _L[π]_ _[{][0][,][ · · ·][, L][−][1][}][, and][ A][A][ =]_

_A_ _L_

from which the spatial shift shall be selected. Table 1 summarizes the conditions that all parameters[{][0][,][ · · ·][, A][−][1][}][. Let us also define the set][ T][ :=][ {][0][}∪{][2][j][(cos(][θ][)][,][ sin(][θ][))][}][0][≤][j<J, θ][∈] _[π]_ _[{][0][,][···][,][2][L][−][1][}][,]_
have to satisfy to be contained in these sets. Additionally, these models include large scale information through the covariance of a low-pass filter, i.e. the spatial average of x ⋆φJ ( )x ⋆φJ ( _τ_ ), for

_·_ _· −_
_τ ∈_ T. To count the size of Υ without redundancies, Appendix A.3 provides an upper bound on the
non-redundant statistics in our models. This upper bound is used to count the number of statistics in
our models. To keep this number from being too large, instead of taking all shifts in a square box,
such as in Portilla & Simoncelli (2000), we choose to select only shifts of dyadic moduli, and with
the same orientations as the wavelets.

ALPHAS vs. ALPHAI The small model ALPHAS is inspired from the PS model, as it only takes
into account of the interactions between nearby scales (i.e. |j[′] _−_ _j| ≤_ 1), and the spatial shift correlations are only considered for (j, θ) = (j[′], θ[′]). There are two notable differences in the statistics
included in the ALPHAS and ALPHAI models. The first one is the range of scales being correlated. It has been shown in Zhang & Mallat (2021) that constraining correlation between a wider


-----

Table 1: List of indices in Υ for different ALPHA models.

|Scales|Angles|Phase shift|Spatial shift|
|---|---|---|---|
|(j,j′)∈J2 |j′−j|≤1|(θ, θ′) ∈Θ2|(α, α′) ∈A4×A1|τ∈T if (j,θ)=(j′,θ′) τ=0 otherwise.|
|(j, j′) ∈J2|(θ, θ′) ∈Θ2|(α, α′) ∈A4×A1|τ ∈T|


Model Scales Angles Phase shift Spatial shift Size of Υ

ALPHAS (jj,j[′] _[′])j∈J1[2]_ (θ, θ[′]) Θ[2] (α, α[′]) 4 1 _τ_ _∈Tτ if=0 (j,θ otherwise.)=(j[′],θ[′])_ ( J Θ + J Θ T ) 4

_|_ _−_ _|≤_ _∈_ _∈A_ _×A_ _|_ _|[2]_ _|_ _||_ _|_ _|A_ _|_

ALPHAI (j, j[′]) J[2] (θ, θ[′]) Θ[2] (α, α[′]) 4 1 _τ_ T _J_ [2] Θ 4 T
_∈_ _∈_ _∈A_ _×A_ _∈_ _|_ _|[2]|A_ _||_ _|_

ALPHAL (j, j[′]) J[2] (θ, θ[′]) Θ[2] (α, α[′]) 4 _τ_ T _J_ [2] Θ 4 T
_∈_ _∈_ _∈A[2]_ _∈_ _|_ _|[2]|A_ _|[2]|_ _|_


range of scales induces a better model for non-Gaussian stationary processes, and a better estimation
of cosmological parameters from observed data (Allys et al., 2020). The second difference, which
has a significant impact on the number of statistics (it increases the model size by a factor ∼10), is
the number of spatial shifts in the correlations. In the ALPHAI model, spatially shifted correlations
are computed for all pairs of coefficient (γ, γ[′]). For both stationary textures and non-stationary images in gray-scale, shape and contours of salient structures and objects are better reproduced with
ALPHAI, as illustrated in Figure 2. More examples are given in Appendix D.

ALPHAI vs. ALPHAL As we observe in Figure 2, the ALPHAI model, containing 4 times less
coefficients than the ALPHAL, suffers less from memorization effects, while still capturing most of
the geometric information in the images. This small loss of information can be partially explained
by the frequency transposition property of the phase harmonics operator (Mallat et al., 2020), for
compactly supported wavelets in the frequency domain, as detailed in Appendix E. In order to avoid
this memorization effect, we shall, in the rest of the paper, consider only the intermediate model.

3.3 MODELLING COLOR INTERACTIONS

In order to generate color textures, the covariance model ALPHAI defined in Section 3.2 could be
directly applied to each R, G and B color channel independently. However, it would not take into
account the color coherence in the structures of the observation.

To capture color interactions in the observation image, we shall impose the covariance between the
coefficients of eq. (5) for all indices in Υ and all color channels. More precisely, let x = _x[c]_ _c=1,2,3_
_{_ _}_
be a color image, with the parameter c representing the color channel. The ALPHAC color model
is defined by correlations between coefficients of the form:

_R[ALPHA][C]_ _x(γ, u) = ρα(x[c]_ _⋆ψj,θ)(u)_ _µγ,_ _γ = (j, θ, α, c)._ (7)
_−_

The set of indices is defined as Υ[ALPHA][C] := _{(γ, γ[′], τ_ ) : ((j, θ, α), (j[′], θ[′], α[′]), τ ) _∈_
Υ[ALPHA][I] _, (c, c[′]) ∈{1, 2, 3}[2]}._

**Reduced ALPHAC** The model ALPHAC has a large size as it computes correlations between
all coefficients for all color channels. This size can be significantly reduced by computing spatially
shifted coefficients only for the same color channels (to capture their geometries). This reduced
model contains three times less coefficients (∼113k), without significant reduction of the visual
quality of syntheses, as detailed in Appendix F.

4 NUMERICAL RESULTS

In this section, we compare our intermediate model to the state-of-art models (PS, RF and VGG)
on both gray-scale and color textures. We first specify the experimental setup. We then present
the synthesis results of various examples, and discuss their quality through visual inspection. A
quantitative evaluation of the quality of the syntheses, based on the synthesis loss of the VGG model,
and proposed in Ustyuzhaninov et al. (2017), is discussed in Appendix G.


-----

Observation ALPHAS (3.5k) ALPHAI (35k) ALPHAL (142k)

Figure 2: Examples of syntheses from the ALPHA models defined in Table 1. Visually similar
image patches in textures are highlighted by red circles. The number of statistics is given in brackets
next to each model name. From top to bottom: a Julesz counterexample, a stationary texture image,
a stationary turbulent field and a non-stationary image. We see that the model ALPHAI achieves a
balance between the visual quality and diversity on these examples.

4.1 EXPERIMENTAL SETUP

For our experiments, we choose gray-scale and color textures with various visual structures.[6] In the
gray-scale examples, we also include a stationary turbulent field (vorticity), which is simulated from
Navier-Stokes equations in two dimensions (Schneider et al., 2006). These examples all contain
complex geometric structures that are hard to model by the classical PS model.

The texture images presented in this work have a size of N = 256, giving a number of pixels
of ∼65k. For all the ALPHA models, we use Morlet wavelets with a maximal scale J = 5 and
number of orientations L = 4. This choice differs from the PS model, which uses Simoncelli
steerable wavelets. In Appendix H, we discuss the impact of these wavelets on our model. To draw
the samples, we follow gradient-based sampling algorithms, suitable in high-dimensions (Bruna &
Mallat, 2018), to minimize the objective function ∥Cx − _Cx¯∥[2], starting from an initial sample from_
a normal distribution. Similarly to Gatys et al. (2015); Ustyuzhaninov et al. (2017), we use the LBFGS algorithm (Nocedal, 1980) for the optimization of the objective. As in the VGG model (Gatys
et al., 2015), we further apply a histogram matching procedure as post-processing after optimization.
The details of the PS, RF and VGG models, as well as detailed specifications of our models, are given
in Appendix A. More synthesis examples can be found in Appendix I.

4.2 RESULTS

In Figure 3, we present examples of syntheses from the ALPHAI (or ALPHAC), PS, RF and VGG
models, for both gray-scale and color textures, as well as for non-stationary images. We observe that

6The source for the presented textures are given in Appendix A.


-----

our model ALPHAI produces texture syntheses of similar visual quality to the RF and VGG models.
It also significantly outperforms the PS model in terms of the visual quality, without introducing
visible memorization effects. As the model PS uses the statistics closer to ALPHAS compared to
ALPHAI, the performance of PS is somehow expected.

Note that for the tiles example (the fifth row) in Figure 3, the VGG model produces less convincing
textures, because the long-range correlations present in the image (aligned tiles) are not reproduced.
To remedy this issue, it has been proposed in Berger & Memisevic (2017) to add spatial shifts to
the correlations of the network feature maps. These shift statistics are similar to the parameter τ
in our model. We also observe that, in the case of the sixth row example (flowers), all models fail
to reproduce complex structures at object-level. Possible improvements of such models is further
discussed in Section 5.

For non-stationary images, we find that certain image patches can be more or less memorized by
the RF, VGG and ALPHAI models, as illustrated in the seventh row example. Understanding such
memorization effect of non-stationary images is a subtle topic, as we find that in some binary images
(x¯(u) ∈{0, 1}), only the PS model can reproduce the observation, even though it has a much smaller
number of statistics (the last row example). We find that this is related to the spatial correlation
statistics in PS (non-zero τ ). By removing this constraint, ¯x is no longer always reproduced.[7] The
non-stationary nature also appear in some logo near the boundary of some textures (e.g. bottom
left in the observation of the first and fourth rows). Although this logo is reproduced by RF, VGG,
ALPHAI and ALPHAC, it is a very local phenomenon, as we do not find visible copies of the
textures when there is no logo, and it is likely related to the way one addresses the boundary effect
(see more in Appendix A.5).

5 DISCUSSION

In this work, we presented a new wavelet-based model for natural textures. This model incorporates a wide range of statistics, by computing covariances between rectified wavelet coefficients, at
different scales, phases and positions. We showed that this model is able to capture and reproduce
complex geometric structures, present in natural textures or physical processes, producing syntheses
of similar quality to state-of-the-art models that use CNN-based representations.

Being defined with a wavelet family instead of multi-scale random filters, the proposed model uses
less statistics than the RF model. For the gray-scale textures, our model has about 15 times less
statistics, as it focuses on capturing the geometric structures present in images. Although our color
model has a slightly larger number of statistics than VGG, the reduced color model, presented in
Section 3.3 is three times smaller than ALPHAC, while achieving similar visual quality. It shows
the potential to further reduce the size of the color model. In the PS model, a PCA on the color
channels is performed (Vacher & Briand, 2021). The same idea could also be applied to our model.

Furthermore, there are examples where all the models may all fail to produce some geometric structures at object-level, as illustrated in Figure 3 (sixth row). In this situation, we still need to find
more informative statistics. One may for example consider to incorporate a second layer of wavelet
transform as in the wavelet scattering transform (Leonarduzzi et al., 2019; He & Hirn, 2021). Another line of research is to introduce other kinds of losses (such as to encourage image smoothness)
in order to improve the VGG model (Liu et al., 2016; Sendik & Cohen-Or, 2017). These losses
are complementary, and could thus also be added to our models. Integrating these models with
learning-based approaches is another promising direction (Zhou et al., 2018; Zhu et al., 1997; Xie
et al., 2016; 2018).

Finding a minimal set of statistics to define a texture model remains important because a large
number of statistics can result in a high variance of the estimators, and the associated model may
suffer from memorization effects. This is a problem because the aim of the model is to approximate
the underlying distribution of the observation, and therefore produce diverse textures. In this regard,
the mere visual evaluation of the synthetic textures can fail to take this aspect of the model into
account. Defining a quantitative evaluation of quality and diversity, coherent with visual perception,
remains an open problem (Ustyuzhaninov et al., 2017; Yu et al., 2019).

7Set the parameter ∆= 0 in the PS model. See Appendix A for more details. This simple example suggests
that sometimes it is very important to choose the right statistics to capture specific geometric structures.


-----

Observation PS (3.2k/17k) RF (525k) Ours (35k/320k) VGG (177k)

Figure 3: Visual comparison between the gray-scale and color models. ’Ours’ denotes the ALPHAI
model for gray-scale images and the ALPHAC model for color images.

**Acknowledgments** We thank all the reviewers for their insightful feedback. This work was partially supported by a grant from the PRAIRIE 3IA Institute of the French ANR-19-P3IA- 0001 pro

-----

gram. Sixin Zhang acknowledges the support provided by 3IA Artificial and Natural Intelligence
Toulouse Institute, French ”Investing for the Future - PIA3” program under the Grant agreement
ANR-19-PI3A-0004, and by Toulouse INP ETI and AAP unique CNRS 2022 under the Project
LRGMD.

**Reproducibility Statement** In order to reproduce the experiments presented in this work, the main
parameters to define the model can be found in Section 4.1. More details are given in Appendix A.
Additionally, the code is made publicly available.

**Ethics Statement** The authors acknowledge that no potential conflicts of interest, discrimination,
bias, fairness concerns or research integrity issues have been raised during the completion of this
work.

REFERENCES

E Allys, T Marchand, J-F Cardoso, F Villaescusa-Navarro, S Ho, and S Mallat. New interpretable
statistics for large-scale structure analysis and generation. Physical Review D, 102(10):103506,
2020.

Guillaume Berger and Roland Memisevic. Incorporating long-range consistency in cnn-based texture generation. In International Conference on Learning Representations, ICLR 2017. OpenReview.net, 2017.

Joan Bruna and Stephane Mallat. Multiscale Sparse Microcanonical Models. Mathematical Statis_tics and Learning, 1(5):257–315, 2018._

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248–255. Ieee, 2009.

Arturo Deza, Aditya Jonnalagadda, and Miguel P. Eckstein. Towards metamerism via foveated style
transfer. In International Conference on Learning Representations, 2019.

Jeremy Freeman and Eero P Simoncelli. Metamers of the ventral stream. Nature neuroscience, 14
(9):1195–1201, 2011.

Leon Gatys, Alexander S Ecker, and Matthias Bethge. Texture synthesis using convolutional neural
networks. Advances in neural information processing systems, 28:262–270, 2015.

Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Image style transfer using convolutional
neural networks. In Proceedings of the IEEE conference on computer vision and pattern recog_nition, pp. 2414–2423, 2016._

P. Goupillaud, A. Grossmann, and J. Morlet. Cycle-octave and related transforms in seismic signal
analysis. Geoexploration, 23(1):85–102, 1984. ISSN 0016-7142. Seismic Signal Analysis and
Discrimination III.

Jieqian He and Matthew Hirn. Texture synthesis via projection onto multiscale, multilayer statistics.
_arXiv preprint arXiv:2105.10825, 2021._

David J Heeger and James R Bergen. Pyramid-based texture analysis/synthesis. In Proceedings
_of the 22nd annual conference on Computer graphics and interactive techniques, pp. 229–238,_
1995.

Edwin T Jaynes. Information theory and statistical mechanics. Physical review, 106(4):620, 1957.

Bela Julesz. Visual pattern discrimination. IRE transactions on Information Theory, 8(2):84–92,
1962.

P Laube, M Grunwald, MO Franz, and G Umlauf. Image inpainting for high-resolution textures
using cnn texture synthesis. In Proceedings of the Conference on Computer Graphics & Visual
_Computing, pp. 103–107, 2018._


-----

Roberto Leonarduzzi, Gaspar Rochette, Jean-Phillipe Bouchaud, and St´ephane Mallat. Maximumentropy scattering models for financial time series. In ICASSP 2019-2019 IEEE International
_Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5496–5500. IEEE, 2019._

Chuan Li and Michael Wand. Combining markov random fields and convolutional neural networks
for image synthesis. In Proceedings of the IEEE conference on computer vision and pattern
_recognition, pp. 2479–2486, 2016._

Gang Liu, Yann Gousseau, and Gui-Song Xia. Texture synthesis through convolutional neural networks and spectrum constraints. In 2016 23rd International Conference on Pattern Recognition
_(ICPR), pp. 3234–3239. IEEE, 2016._

St´ephane Mallat. A Wavelet Tour of Signal Processing: The Sparse Way, 3rd Edition. Academic
Press, 2001.

St´ephane Mallat, Sixin Zhang, and Gaspar Rochette. Phase harmonic correlations and convolutional
neural networks. Information and Inference: A Journal of the IMA, 9(3):721–747, 2020.

Jorge Nocedal. Updating quasi-newton matrices with limited storage. Mathematics of computation,
35(151):773–782, 1980.

Javier Portilla and Eero P Simoncelli. A parametric texture model based on joint statistics of complex wavelet coefficients. International journal of computer vision, 40(1):49–70, 2000.

Lara Raad, Axel Davy, Agn`es Desolneux, and Jean-Michel Morel. A survey of exemplar-based
texture synthesis. Annals of Mathematical Sciences and Applications, 3(1):89–148, 2018.

Kai Schneider, J¨org Ziuber, Marie Farge, and Alexandre Azzalini. Coherent vortex extraction and
simulation of 2D isotropic turbulence. Journal of Turbulence, 7:N44, 2006.

Omry Sendik and Daniel Cohen-Or. Deep correlations for texture synthesis. ACM Transactions on
_Graphics (ToG), 36(5):1–15, 2017._

E. P. Simoncelli and W. T. Freeman. The steerable pyramid: a flexible architecture for multi-scale
derivative computation. In Proceedings., International Conference on Image Processing, volume 3, pp. 444–447 vol.3, Oct 1995. doi: 10.1109/ICIP.1995.537667.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.

M. Unser and N. Chenouard. A unifying parametric framework for 2d steerable wavelet transforms.
_SIAM Journal on Imaging Sciences, 6(1):102–135, 2013._

Ivan Ustyuzhaninov, Wieland Brendel, Leon A. Gatys, and Matthias Bethge. What does it take to
generate natural textures? In International Conference on Learning Representations, ICLR 2017.
OpenReview.net, 2017.

Jonathan Vacher and Thibaud Briand. The portilla-simoncelli texture model: towards understanding
the early visual cortex. Image Processing On Line, 11:170–211, 2021.

Jonathan Vacher, Aida Davila, Adam Kohn, and Ruben Coen-Cagli. Texture interpolation for probing visual perception. Advances in Neural Information Processing Systems, 33, 2020.

Thomas SA Wallis, Christina M Funke, Alexander S Ecker, Leon A Gatys, Felix A Wichmann, and
Matthias Bethge. Image content is more important than bouma’s law for scene metamers. ELife,
8:e42512, 2019.

Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu. A theory of generative convnet. In
_International Conference on Machine Learning, pp. 2635–2644. PMLR, 2016._

Jianwen Xie, Yang Lu, Ruiqi Gao, Song-Chun Zhu, and Ying Nian Wu. Cooperative training of descriptor and generator networks. IEEE transactions on pattern analysis and machine intelligence,
42(1):27–45, 2018.


-----

Ning Yu, Connelly Barnes, Eli Shechtman, Sohrab Amirghodsi, and Michal Lukac. Texture mixer:
A network for controllable synthesis and interpolation of texture. In Proceedings of the IEEE/CVF
_Conference on Computer Vision and Pattern Recognition, pp. 12164–12173, 2019._

Sixin Zhang and St´ephane Mallat. Maximum entropy models from phase harmonic covariances.
_Applied and Computational Harmonic Analysis, 53:199–230, 2021._

Yang Zhou, Zhen Zhu, Xiang Bai, Dani Lischinski, Daniel Cohen-Or, and Hui Huang. Nonstationary texture synthesis by adversarial expansion. ACM Transactions on Graphics (TOG),
37(4):1–13, 2018.

Song Chun Zhu, Ying Nian Wu, and David Mumford. Minimax entropy principle and its application
to texture modeling. Neural computation, 9(8):1627–1660, 1997.

A MODEL AND ALGORITHMIC SPECIFICATION

We provide additional information needed to reproduce the numerical results of the models (for both
gray-scale and color textures) considered in this paper. First, we detail the models of PS, RF, VGG.
We then give algorithmic parameters to obtain the synthesis images. For natural textures, we also
propose a strategy to synthesize non-periodic images in our models. An image is non-periodic if a
periodic extension of the image to the domain outside ΩN create discontinuities at the contour of
ΩN .

**Sources of textures** Our natural texture examples were obtained from the following three sources:
CNS NYU[8], Textures.com[9], Describable Textures Dataset model[10] and the Github page of Berger
& Memisevic (2017) [11].

A.1 MODEL PARAMETERS

We specify the model parameters to synthesize both gray-scale and color textures. We also discuss
how to extend the RF and the VGG model, originally designed for color textures, to model gray-scale
textures.

-  PS: For both gray-scale and color models, we set the number of scales J = 5, and the
number of orientations L = 8 for the Simoncelli steerable wavelets. The spatial shift
_τ = (τ1, τ2) ∈_ ΩN is chosen to be in the range of max (τ1, τ2) ≤ ∆= 4. Note that this is
different to the model parameters reported for the default PS model (which is J = 4, L =
4, ∆= 3) as we find that it results in a larger set of statistics and better visual quality. The
synthesis results of this model can be reproduced by a Matlab software.[12]

-  RF: For gray-scale textures, we consider J × L random convolutional filters (ψf )1≤f _≤JL._
Let f = (j, ℓ), the index j representing the scale of each filter, whose size is (Wj, Wj) for
1 ≤ _j ≤_ _J. For Γ = {f = (j, ℓ)|j ≤_ _J, ℓ_ _≤_ _L}, the representation is_

_R[RF]x(γ, u) = ρ(x ⋆ψj,ℓ(u)),_ _γ = (j, ℓ)_ Γ.
_∈_

For a color image x = {x[c]}1≤c≤3, we use 3 × J × L random convolutional filters. The
representation of x is


_x[c]_ _⋆ψc,j,ℓ(u)_ _,_ _γ = (j, ℓ)_ Γ.
_∈_
_c=1_

X 


_R[RF]x(γ, u) = ρ_


The correlations C [RF]x are defined for all pairs (γ, γ[′]) ∈ Γ × Γ. In both the gray and color
cases, it results in a correlation matrix C [RF] with J [2]L[2] statistics.

[8http://www.cns.nyu.edu/˜lcv/texture/](http://www.cns.nyu.edu/~lcv/texture/)
[9https://textures.com/](https://textures.com/)
[10https://www.robots.ox.ac.uk/˜vgg/data/dtd/index.html](https://www.robots.ox.ac.uk/~vgg/data/dtd/index.html)
[11https://github.com/guillaumebrg/texture_generation](https://github.com/guillaumebrg/texture_generation)
[12https://www.cns.nyu.edu/˜lcv/texture/](https://www.cns.nyu.edu/~lcv/texture/)


-----

Following the default setting of the RF model, we set J = 8 and L = 128 for filters whose
sizes are W1 = 3, W2 = 5, W3 = 7, W4 = 11, W5 = 15, W6 = 23, W7 = 37, W8 = 55.
Each filter ψj,ℓ or ψc,j,ℓ is generated randomly according to GlorotUniform in the
software Lasagne. [13]

-  VGG. For a color image x = {x[c]}, the VGG model computes a correlation matrix C [VGG]x
between the features maps within different layers of a pre-trained CNN network. To adapt
this model to gray-scale textures, we shall add one input layer which converts a gray-scale
image y into a color image, by setting x[c] = y for each color channel c. This allows one
to use the same C [VGG]x to compute the gradient of the VGG loss with respect to y, and
therefore to synthesise a gray-scale texture. For both gray-scale and color textures, we use
only five layers ’conv1 1’, ’pool1’, ’pool2’, ’pool3’, ’pool4’, as proposed in the original
work.

-  ALPHA. See the main text.

A.2 ALGORITHMIC PARAMETERS

We specify the optimization parameters used to synthesize both gray-scale and color textures.

-  PS: It utilizes iterative projections onto constraint sets to generate textures. We set the
number of iterations to 200.

-  RF: It uses the L-BFGS procedure[14] with a memory size 20, and with a maximal number of
iterations 2000. The initialization for each pixel value of a gray-image is Uniform between

[−1, 1]. For the color image case, each RGB channel is initialized independently with a
Uniform distribution between [−1, 1]. To address non-zero mean textures (i.e. E(X(u)) ̸=
0), the empirical mean of ¯x is subtracted from the input x to compute the representation. It
is added back to the output of the optimization to produce a synthesis.

-  VGG: It uses the L-BFGS procedure[15] with a memory size of 20, and with a maximal
number of iterations of 2000.For both gray-scale and color images, bounds constraints
are used for the optimization. The initialization of each pixel value is the standard normal
distribution (zero mean, unit variance). To address non-zero mean textures (i.e. E(X(u)) ̸=
0), the VGG mean is subtracted from the input x of the representation[16]. It is added back
to the output after the optimization to produce a synthesis (with an additional histogram
matching post-processing).

-  ALPHA: For all the models in ALPHA, we use the L-BFGS optimization algorithm with
restarts. Starting from the standard normal distribution, with mean and standard deviation
estimated from the observation, we use the L-BFGS procedure implemented in Pytorch. It
runs for 500 iterations and then it is restarted with an initialization obtained from the previous L-BFGS result. This is repeated 10 times to obtain the synthesis (with an additional
histogram matching post-processing).

A.3 NUMBER OF STATISTICS IN THE ALPHA MODELS

In this section, we detail the number of statistics in the different ALPHA models. We begin by
giving the formula for each model (note that we do not include the low-pass statistics, which numbers
are negligible). To (partially) avoid redundancy in the coefficients, for all models, we compute only
the correlations for indices in Υ such that j2 _j1. This gives us the following formulas for the_
number of statistics: _≥_

-  #(ALPHAS) = (2J 1) Θ4 4 + J Θ4 4 T
_−_ _|_ _|[2]|A_ _|_ _|_ _||A_ _||_ _|_

-  #(ALPHAI) = 2[1] _[J][(][J][ + 1)][|][Θ][4][|][2][|A][4][||][T][|]_


[13https://lasagne.readthedocs.io/](https://lasagne.readthedocs.io/)
14scipy.optimize.fmin_l_bfgs_b in Python
15scipy.optimize.minimize in Python
16For the color image whose pixel value is between zero and one, the mean of BGR is 0.40760392,
0.45795686, 0.48501961. For the gray-scale, we simply take the average of the BGR mean.


-----

-  #(ALPHAL) = [1]2 _[J][(][J][ + 1)][|][Θ][4][|][2][|A][4][|][2][|][T][|]_

-  #(ALPHAC) = [9]2 _[J][(][J][ + 1)][|][Θ][4][|][2][|A][4][||][T][|]_

Note that, for all ALPHA models, we also compute first order statistics µγ, i.e. the spatial averages
of R[ALPHA]x¯(γ, u). There are J Θ4 4 statistics of this sort in every model, which is negligible
_|_ _||A_ _|_
with respect to the total number of second order statistics.

Note also that there are still some redundancies in these statistics, as for j = j[′], all correlations for
_θ, θ[′], α, α[′], τ are counted twice. The number of such statistics is[17], for all model, superior to the_
number of first order statistics, which shows that our formula is in fact an upper bound for the exact
number of statistics.

A.4 NUMBER OF STATISTICS IN THE PS MODELS

The PS model can be interpreted as a particular case of the WPH covariance model as it contains the
following three key categories of statistics:

_• Raw coefficient correlations (k = k[′]_ = 1): they capture 2nd order statistics of a stationary process
_X, i.e. the correlations between X(u) and X(u[′]) for (u, u[′]) ∈_ Ω[2]N [.]

_• Coefficient magnitude statistics (k = k[′]_ = 0): they capture information of X beyond the 2nd
order statistics. Very often, nearby scales and angles are considered in the model, such as j[′] _≈_ _j_
and θ[′] _≈_ _θ._

_• Cross-scale phase statistics (k = 1, k[′]_ = 2): they capture local phase alignments of the wavelet
coefficients at nearby scales j[′] = j + 1, which are complementary to the magnitude correlations.

The major issue to count the number of statistics of this model is to avoid double counting the
statistics which are the same. This is mostly due to the symmetries of the covariance matrices. We
next detail how obtain the number of statistics for both the gray-scale and color model, by following
the work of Portilla & Simoncelli (2000) and Vacher & Briand (2021).

As before, we assume the number of wavelet scales is J, the number of wavelet orientations is L,
and the spatial shift range is within a square of size (2∆+1)×(2∆+1). Let us denote Na = 2∆+1.
We next describe in detail the number of statistics of each category counted in our paper,

A.4.1 PS IN GRAY-SCALE

_• Marginal statistics of x: 6. They include mean, variance, skewness, etc._

_• Marginal statistics of wavelet coefficients: 2(J + 1) + 1._

_• Auto-correlation of wavelet coefficients (raw coefficient correlations): (J + 1)(Na[2]_ [+ 1)][/][2][.]

_• Auto-correlation of magnitude of wavelet coefficients (coefficient magnitude statistics): JL(Na[2]_ [+]
1)/2 + JL(L − 1)/2 + (J − 1)L[2].

_• Mean of magnitude of wavelet coefficients: JL + 2. This is not counted in the paper of Portilla &_
Simoncelli (2000), but it used in the Matlab software.

_• Cross-correlation of phase of wavelet coefficients (cross-scale phase statistics): 2(J −1)L[2]_ +JL[2].
The extra JL[2] coefficients are cross-correlation of real sub-band cousin coefficients, which are not
counted in the paper, but used in the Matlab software.

To compare with the model size of the original work of Portilla & Simoncelli (2000), one can check
that the sum of these number is 792 when J = 4, L = 4, ∆= 3 (Na = 7). If we do not count the
coefficients which are only counted in the Matlab software (JL + 2 + JL[2] = 82), it results in 710
as reported in the original paper.

A.4.2 PS IN COLOR

_• Marginal statistics of x and PCA transform of x: 6 × 3 + 3 × 4 = 30._

17The number of such moments is of the order of J Θ 2 4 for the small model, J Θ 2 4 2 for the inter_|_ _|_ _|A_ _|_ _|_ _|_ _|A_ _|_
mediate and large models, and 9J|Θ|[2]|A4|[2] for the color model.


-----

_• Marginal statistics of wavelet coefficients: 6(J + 1) + 9._

_• Auto-correlation of wavelet coefficients (raw coefficient correlations): 3(J + 2)(Na[2]_ [+ 1)][/][2][. This]
is called Central autoCorr of the PCA bands in the Matalb software (which includes lowband).

_• Auto-correlation of magnitude of wavelet coefficients (coefficient magnitude statistics):_
3JL(Na[2] [+ 1)][/][2 +][ J][(3][L][)(3][L][ −] [1)][/][2 + (][J][ −] [1)(3][L][)][2][.]

_• Mean of magnitude of wavelet coefficients: 3(JL + 2)._

_• Cross-correlation of phase of wavelet coefficients (cross-scale phase statistics): J(3L)[2]_ + (J −
1)(3L)(6L).

A.5 NON PERIODIC BOUNDARIES IN NATURAL IMAGES

The convolution operation in the wavelet transform (equation 2) is performed using the Fast Fourier
Transform. Additionally, recall from Section 2.1 that spatial shifts are defined with periodic boundary conditions. This implies periodicity of the input image x. However, natural texture images are
not periodic, so one needs to adapt the computation of coefficients to take into account possible
border effects. To that end, instead of averaging over all u ∈ ΩN as in eq. (1), each correlation coefficient is averaged over a sub-window inside ΩN, which size depends on the scales of the coefficients
being correlated. More precisely, let γ = (j, θ, α) and γ[′] = (j[′], θ[′], α[′]). Note jm := max(j, j[′]). We
define Ωjm := {u = (u1, u2) ∈ ΩN : 2[j][m] _≤_ _ui < N −_ 2[j][m] _, i = 1, 2}. Then, for non periodic_
images, we compute


1Ωjm (u)1Ωjm (u − _τ_ )R[ALPHA]x(γ, u)R[ALPHA]x(γ[′], u − _τ_ ),
_uX∈ΩN_

(8)


_C_ [ALPHA]x(γ, γ[′], τ ) =


Ωjm
_|_ _|_


where the spatial shifts are defined periodically. Note that the spatial averages µγ and µγ′ are also
performed on Ωjm.

B PROOF OF PROPOSITION 1

Using the fact that

_ρ(Real(ze[iα])) = ρ(Real(|z|e[i][(][ϕ][(][z][)+][α][)]))_
= |z|ρ(cos(α + ϕ(z)),

and computing the Fourier coefficients of the 2π-periodic function ρα(z) in the variable α, we obtain


(ρα(z))(k) := [1]
_F_ 2π

Z

= _z_ [1]
_|_ _|_ 2π


_ρα(z)e[−][ikα]dα_

[0,2π]

_ρ(cos(α + ϕ(z))e[−][ikα]dα_

[0,2π]

Z


= _z_ _e[ikϕ][(][z][)]ck_
_|_ _|_
= [z][k]ck,

where ck is the Fourier transform of h(.) := ρ(cos(.)) at the frequency k. The function α 7→ _ρα(z)_
being periodic in α, we have its decomposition in Fourier series

_ρα(z) =_ (ρα(z))(k)e[ikα]

_F_
_kX∈Z_

= _ck[z][k]e[ikα]._

_kX∈Z_

We can then write, for any z, z[′] _∈_ C, and α, α[′] _∈_ [0, 2π],

_ρα(z)ρα′_ (z[′])[∗] = _ckc[∗]k[′]_ [[][z][]][k][[][z][′][]][−][k][′] _[e][i][(][kα][−][k][′][α][′][)][.]_

_k,kX[′]∈Z[2]_


-----

Replacing z and z[′] by any two wavelet coefficients x ⋆ψj,θ(u) and x ⋆ψj′,θ′ (u _τ_ ), we thus obtain
_−_
the relation in Proposition 1.

C PROOF OF PROPOSITION 2

Let z ∈ C, and recall from eq. (4), that ρα(z) = ρ(Real(ze[iα])). Note that we have the following
relation
_z = ρ0(z) −_ _ρπ(z) −_ _i(ρ_ _[π]2_ [(][z][)][ −] _[ρ][ 3]2[π]_ [(][z][))][.] (9)


We can then write

_zz[′∗]_ = _ρ0(z) −_ _ρπ(z) −_ _i(ρ_ _[π]2_ [(][z][)][ −] _[ρ][ 3]2[π]_ [(][z][))] _ρ0(z[′]) −_ _ρπ(z[′]) −_ _i(ρ_ _[π]2_ [(][z][′][)][ −] _[ρ][ 3]2[π]_ [(][z][′][))]

=   _wα,α[′]_ _[′]_ _[ρ][α][(][z][)][ρ][α][′]_ [(][z][′][)][,]  

_α,αX[′]∈I_ [2]


withrelation in eq. (1) gives us the desired result, with I = {0, _[π]2_ _[, π,][ 3]2[π]_ _[}][. Replacing][ z][ with][ x ⋆ψ][j,θ] w[(][u]α,α[)][,][ z]′ = Ω[′][ with]N[ x ⋆ψ]wα,α[′]_ _[′][j][.][′][,θ][′]_ [(][u][ −] _[τ]_ [)][,, and injecting this]

D SUPPLEMENTARY RESULTS FOR THE GRAY-SCALE ALPHA MODELS

Here, we present further visual comparison between the different ALPHA models for gray-scale
images, to illustrate the trade-off between quality and diversity.

E RELATION BETWEEN ALPHAL AND ALPHAI

Here, we informally explain why, under some conditions on the wavelet family, setting α[′] _∈{0} in_
the ALPHA models should not lose too much (but still some) information captured by the statistics.

First, remark that the simple linear relation

_ρα(z)e[ikα][]ρ0(z) =_ _ρα(z)ρ0(z)e[ikα]_ (9)

  α[X]∈A4 _αX∈A4_

tells us that computing all correlations for α ∈A4 and α[′] = 0 gives us at least the information
contained in the r.h.s. of eq. (9).

Furthermore, if α 4 = 0, _,_ [3]4[π]
_∈A_ _{_ _· · ·_ _[}][, we make the following approximation]_

_ρα(z)e[ikα]_ _ρα(z)e[−][ikα]dα = 2πck[z][k]._
_αX∈A4_ _≃_ Z[0,2π]


Recall also from the proof of Proposition 2, that (ρα(z))(k) = [z][k]ck, where the Fourier transform
_F_
is taken along the variable α. Therefore,


_ck[z][k]._
_kX∈Z_


_ρ0(z) =_


Therefore,

_ρα(z)e[ikα][]ρ0(z)_ _ρα(z)e[−][ikα]dα_ _ρ0(z)_

  α[X]∈A4 _≃_   [Z][0,2π] 

= 2πck[z][k][  X] _ck′_ [z[′]][k][′] []

_k[′]∈Z_

= 2πck _ck′_ [z][k][z[′]][k][′] _._

_kX[′]∈Z_


-----

Obs ALPHAS (3.5k) ALPHAI (35k) ALPHAL (142k)

Figure 4: Visual comparison of syntheses from the ALPHAS, ALPHAI and ALPHAL models.

Then, replacing z and z[′] with wavelet coefficients, we get


_e[−][ikα]C_ [ALPHA]x((j, θ, α), (j[′], θ[′], 0), τ ) 2πck
_≃_
_αX∈AA_

Using Plancherel’s theorem, we can write that


_ck′_ _C_ [WPH]x((j, θ, k), (j[′], θ[′], k[′]), τ ).
_kX[′]∈Z_


1
_C_ [WPH]x((j, θ, k), (j[′], θ[′], k[′]), τ ) = ([x ⋆ψλ][k])(ω) ([x ⋆tτ _ψλ][−][k][′]_ )(ω),

ΩN _F_ _F_
_|_ _|_ _ω∈X[2]N[π]_ [Ω][N]

where the Fourier transform of an image x is defined by (x)(ω) := _u_ ΩN _[x][(][u][)][e][−][iωu][, and][ t][τ]_
_F_ _∈_

denotes the translation by τ, i.e. tτ _f_ ( ) = f ( _τ_ ).

_·_ _· −_

Now, suppose that the wavelets ψα have disjoint compact frequency support, in balls[P] _Bλ(2[−][j]C_ _[′]),_
where λ = 2[−][j]r _θξ0, and ξ0 is the central frequency of the mother wavelet ψ (cf. ?). Suppose also_
_−_


-----

that frequency transposition property of the phase harmonics operator (cf. Mallat et al. (2020)) is
such that [x ⋆ψλ][k] has (approximately) frequency support in Bkλ(k2[−][j]C _[′]). Then, for all λ, λ[′], and_
all k Z, there exists only one k[∗] such that the frequency supports of [x ⋆ψλ][k] and [x ⋆ψλ′ ][−][k][∗]
_∈_

are not disjoint, i.e. such that C [WPH]x((j, θ, k), (j[′], θ[′], k[′]), τ ) ̸= 0. This tells us that

_e[−][ikα]C_ [ALPHA]x((j, θ, α), (j[′], θ[′], 0), τ ) _ckck∗_ _C_ [WPH]x((j, θ, k), (j[′], θ[′], k[∗]), τ ).
_≃_
_αX∈AA_

Thus, computing all correlations for α 4, and α[′] = 0 gives us (approximately) all the informa_∈A_
tion contained in WPH coefficients for any pair k, k[′].

This result lies on several approximations, and strong assumptions about the wavelets, which are
not fully met in practice. For this reason, setting α[′] = 0 instead of α[′] _∈A4 effectively reduced the_
amount of information captured by the statistics, and therefore increases the diversity of the model.
However, as we observe in Section 3.2, there is not too much information lost, and the resulting
model still captures most of the important geometric structures in texture images.

F REDUCED COLOR MODEL

One can reduce the number of statistics in the color model by selection the spatial shift parameter τ
to be non-zero only for correlations between the same color channels. More precisely, it is defined
by the following index set: Υ := {(γ, γ[′], τ ) : ((j, θ, α), (j[′], θ[′], α[′]), τ ) ∈ Υ[ALPHA][I] _, c = c[′]_ _∈_
_{1, 2, 3}} ∪{(γ, γ[′], 0) : ((j, θ, α), (j[′], θ[′], α[′]), 0) ∈_ Υ[ALPHA][I] _, (c, c[′]) ∈{1, 2, 3}[2]}. This gives a_
model of size ∼ 113k, with little degradation of the visual quality, as shown in Figure 5.

G VGG SCORE

In Ustyuzhaninov et al. (2017), the authors proposed to use the synthesis loss of the VGG model
to evaluate the quality of syntheses from any model. The goal is to define a quantitative, and more
objective evaluation method than mere visual inspection. Since the VGG model produces syntheses
almost indistinguishable from real textures, it is natural to consider its loss to asses the quality of a
synthesis. We computed this loss for the first two examples of Figure 3 (radishes and cherries), and
the frist two examples of Figure 9 (gravel and Turbulence flow). Note however that this loss is not
exactly the same as the one used in Ustyuzhaninov et al. (2017), as the layers selected to compute
the loss are different. In this work, we chose to use the layers suggested in Gatys et al. (2015), (i.e.
’conv1 1’, ’pool1’, ’pool2’, ’pool3’, and ’pool4’ of the VGG-19 network (Simonyan & Zisserman,
2014)), and compute the relative VGG loss[18].

We notice that this score is not always consistent with visual inspection, as there are texture examples
and models for which the syntheses do not look much like the observation image, yet produce a small
VGG loss (see e.g. the first and last rows of Figure 3, the RF model syntheses have the smallest loss).
It should also be noted that the VGG loss reported on the VGG syntheses is not the synthesis loss
after optimization, as a histogram matching (HM) procedure is performed as post-processing after
optimization. We observed that the VGG loss of the syntheses from the VGG model after HM
was considerably higher than the one for syntheses before it, while being visually very similar as
illustrated in Figure 6. These observations suggest that the VGG score suffers from instabilities after
reaching a certain level (that is, if the VGG loss is small enough, small perturbations of the values
of the image pixels might have a strong impact on the loss).

H INFLUENCE OF THE CHOICE OF THE WAVELET TRANSFORM

H.1 INFLUENCE OF THE WAVELET FAMILY

In Section 3.2, we illustrated the importance of the set of indices Υ that define the wavelet coefficients being correlated. Another important role is played by the choice of the wavelets used in

[18Using the code from https://github.com/ivust/random-texture-synthesis/blob/](https://github.com/ivust/random-texture-synthesis/blob/master/vgg_loss.py)
[master/vgg_loss.py (function style loss relative).](https://github.com/ivust/random-texture-synthesis/blob/master/vgg_loss.py)


-----

Obs ALPHAC (320k) Reduced (113k)

Figure 5: Visual comparison of syntheses from ALPHAC model and its reduced version.

Table 2: Relative VGG loss of gray-scale textures for the first two examples of Figure 3, and the first
two examples of Figure 9.

|ALPHAI|VGG|PS|
|---|---|---|
|5.02e-05|1.87e-05|2.37e-04|
|4.86e-05|1.47e-06|6.68e-04|
|5.97e-05|3.08e-06|7.25e-04|


Data / Model ALPHAI VGG PS RF

Radishes 5.02e-05 1.87e-05 2.37e-04 1.13e-05

Cherries 4.86e-05 1.47e-06 6.68e-04 9.65e-06

Gravel 5.97e-05 3.08e-06 7.25e-04 1.29e-05

Turbulence 5.59e-05 5.97e-05 2.42e-04 3.95e-05


-----

Before HM After HM

Figure 6: Visual comparison of syntheses from the VGG model, before and after histogram matching. Before HM, the relative VGG loss is 2.22e-08, while after HM, the loss is 5.38e-05.

equation 2. As illustrated in Figure 7, this choice can have a visible impact on the quality of the
textures. We observe that, while on the first example, the coherence of the structures appear similar
for the three wavelet families, the second example shows that the wavelets used in Portilla & Simoncelli (2000) are less efficient in reproducing the contours of the objects (pebbles). While in our
experiments, we chose to use the classical Morlet wavelets, an optimal choice for the wavelet family
remains an open problem.

Observation Simoncelli Bump Morlet

Figure 7: Comparison between different wavelets families used in the ALPHAI model. Central
zooms of syntheses using the same covariance model, with three different wavelet families. From
left to right: observation, Simoncelli steerable wavelets, bump steerable wavelets (Mallat et al.,
2020), and Morlet wavelets.

H.2 INFLUENCE OF SCALE PARAMETER

Recall from Section 2.2.1, the wavelet transform of an image x is defined by

_{x ⋆ψj,θ, x ⋆φJ_ _}0≤j<J, θ∈_ _L[π]_ _[{][0][,][···][,L][−][1][}][.]_

The maximal scale parameter J also plays an important role in the definition of the wavelet transform. It determines the scales of the structures being captured by the transform. If this parameter
is too small, large structures in the observation image might not be captured and reproduced in
the model syntheses. Conversely, if J is too large, then the large scale statistics may have a high
variance, inducing a memorization effect in the syntheses. Figure 8 illustrates this point on two
examples from Section 4.2. By setting J = 4 (i.e. the maximal range of structures captured by the
wavelets is of size 2[4] = 16), we observe on the first example that the larger structures (bubbles) are
not well reproduced. When J is set to 6, the observation is almost identically reproduced by the
synthesis. Similarly on the second examples, several parts of the synthesis appear very similar to
ones in the observation. We found that a suitable trade-off consists in setting J = 5 for images of
size N = 256.


-----

Observation _J = 4_ _J = 5_ _J = 6_

Figure 8: Syntheses from the ALPHAC model defined with three different maximal scale parameters J ∈{4, 5, 6}. As in Section 4, Morlet wavelets are used.

I SUPPLEMENTARY RESULTS OF ALPHAI/ALPHAC VS. PS, RF, VGG

In Figure 9 we present additional syntheses on various examples, from the PS, ALPHAC, RF and
VGG models. These examples can be viewed as random (Turbulence flow, tree bark, porous stone),
structured (gravel, paisley pattern, tree leaf, school text), or inhomogeneous (crafted pattern of third
row, but also the porous stone).

We see that ALPHAI again significantly improves the visual quality of the PS model on gray-scale
textures such as the gravel and turbulence flow. The visual quality of RF and VGG seems also
worse on Turbulence flow compared to ALPHAI . In some examples such as tree leaf, we find the
synthesis of all the models are similar. On the inhomogeneous porous stone, non of the models give
satisfying visual results.

Similarly, in Figure 10 are presented supplementary syntheses form the color models, for structured
images (radishes, bubbles, flowers), quasi-periodic images (scales, honeycomb, bricks), and nonstationary images (feathers). As previously observed, for highly structured quasi-periodic images
such as the bricks example, the VGG model fails to capture long-range correlation, which can be
solved using the method of Berger & Memisevic (2017). Syntheses of non-stationary images exhibit
memorization effects, as previously observed.


-----

Observation PS (3.2k) RF (525k) ALPHAI (35k) VGG (177k)

Figure 9: Visual comparison between different texture models on gray-scale images.


-----

Observation PS (17k) RF (525k) ALPHAC(320k) VGG (177k)

Figure 10: Visual comparison between different texture models on color images.


-----

