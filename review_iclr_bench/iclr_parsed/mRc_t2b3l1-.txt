# THE LIMITING DYNAMICS OF SGD: MODIFIED LOSS,
## PHASE SPACE OSCILLATIONS, ANOMALOUS DIFFUSION

**Anonymous authors**
**Paper under double-blind review**

ABSTRACT

In this work we explore the limiting dynamics of deep neural networks trained
with stochastic gradient descent (SGD). As observed previously, long after performance has converged, networks continue to move through parameter space by
a process of anomalous diffusion in which distance travelled grows as a power
law in the number of gradient updates with a nontrivial exponent. We reveal an
intricate interaction between the hyperparameters of optimization, the structure in
the gradient noise, and the Hessian matrix at the end of training that explains this
anomalous diffusion. To build this understanding, we first derive a continuoustime model for SGD with finite learning rates and batch sizes as an underdamped
Langevin equation. We study this equation in the setting of linear regression,
where we can derive exact, analytic expressions for the phase space dynamics of
the parameters and their instantaneous velocities from initialization to stationarity. Using the Fokker-Planck equation, we show that the key ingredient driving
these dynamics is not the original training loss, but rather the combination of a
modified loss, which implicitly regularizes the velocity, and probability currents,
which cause oscillations in phase space. We identify qualitative and quantitative
predictions of this theory in the dynamics of a ResNet-18 model trained on ImageNet. Through the lens of statistical physics, we uncover a mechanistic origin
for the anomalous limiting dynamics of deep neural networks trained with SGD.

1 INTRODUCTION
Deep neural networks have demonstrated remarkable generalization across a variety of datasets and
tasks. Essential to their success has been a collection of good practices on how to train these models
with stochastic gradient descent (SGD). Yet, despite their importance, these practices are mainly
based on heuristic arguments and trial and error search. Without a general theory connecting the
hyperparameters of optimization, the architecture of the network, and the geometry of the dataset,
theory-driven design of deep learning systems is impossible. Existing theoretical works studying
this interaction have leveraged the random structure of neural networks at initialization [1, 2, 3]
and in their infinite width limits in order to study their dynamics [4, 5, 6, 7, 8]. Here we take a
different approach and study the training dynamics of pre-trained networks that are ready to be used
for inference. By leveraging the mathematical structures found at the end of training, we uncover an
intricate interaction between the hyperparameters of optimization, the structure in the gradient noise,
and the Hessian matrix that corroborates previously identified empirical behavior such as anomalous
limiting dynamics. Not only is understanding the limiting dynamics of SGD a critical stepping stone
to building a complete theory for the learning dynamics of neural networks, but recently there have
been a series of works demonstrating that the performance of pre-trained networks can be improved
through averaging and ensembling [9, 10, 11]. Combining empirical exploration and theoretical
tools from statistical physics, we identify and uncover a mechanistic explanation for the limiting
dynamics of neural networks trained with SGD.

2 DIFFUSIVE BEHAVIOR IN THE LIMITING DYNAMICS OF SGD
A network that has converged in performance will continue to move through parameter space [12,
13, 14, 15]. To demonstrate this behavior, we resume training of pre-trained convolutional networks
while tracking the network trajectory through parameter space. Let θ R[m] be the parameter vector
_∗_ _∈_
for a pre-trained network and θk ∈ R[m] be the parameter vector after k steps of resumed training.


-----

We track two metrics of the training trajectory, namely the local parameter displacement δk between
consecutive steps, and the global displacement ∆k after k steps from the pre-trained initialization:



As shown in Fig. 1, neither of these differences converge
to zero across a variety of architectures, indicating that
despite performance convergence, the networks continue
to move through parameter space, both locally and globally. The squared norm of the local displacement _δk_ 2
remains near a constant value, indicating the network
is essentially moving at a constant instantaneous speed.

|, both locally and glo cal displacement δ ∥ k indicating the netwo|b- 2 ∥2 rk|
|---|---|

This observation is quite similar to the “equilibrium" phe
**VGG-16**

∆k = θk _θ_ _._ (1)
_−_ _∗_

**VGG-16 with BN**

**…** **VGG-11 with BN**

**ResNet-34**

_δk_ 2
_∥_ _∥[2]_

**ResNet-18**

nomenon or “constant angular update" observed in Li
et al. [17] and Wan et al. [13] respectively. However, these
works only studied the displacement for parameters immediately preceding a normalization layer. The constant
instantaneous speed behavior we observe is for all parameters in the model and is even present in models without
normalization layers.

|Col1|VGG-16 with BN|
|---|---|
||VGG-11 with BN|

|…|VGG-11 with BN ResNet-34|
|---|---|
||ResNet-34 ResNet-18|
|||


Step


While the squared norm of the local displacement is es- 0.5 1 Step 2 3 4 x10[4]

|c=1.085 c c = 1.401 0.5 1|c = 1.075 = 1.370 c = 1.616|
|---|---|
||2 3 4 x10|

sentially constant, the squared norm of the global dis- Figure 1: Despite performance conplacement ∥∆k∥2[2] [is monotonically growing for all net-] **vergence, the network continues to**
works, implying even once trained, the network continues **move through parameter space. We**
to diverge from where it has been. Indeed Fig. 1 indicates plot the squared Euclidean norm for the
a power law relationship between global displacement local and global displacement (δk and
and number of steps, given by ∆k 2 ∆k) of five classic convolutional neusee in section 8, this relationship is indicative of anoma- ∥ _∥[2]_ _[∝]_ _[k][c][. As we’ll]_ ral network architectures. The networks
lous diffusion where c corresponds to the anomalous dif- are standard Pytorch models pre-trained
fusion exponent. Standard Brownian motion corresponds on ImageNet [16]. Their training is reto c = 1. Similar observation were made by Baity-Jesi sumed for 10 additional epochs. We
et al. [14] who noticed distinct phases of the training tra- show the global displacement on a logjectory evident in the dynamics of the global displace- log plot where the slope of the least
ment and Chen et al. [15] who found that the exponent squares line c is the exponent of the
of diffusion changes through the course of training. A power law ∆k 2 _k[c]._ See apparallel observation is given by Hoffer et al. [18] for the pendix H for experimental details. ∥ _∥[2]_ _∝_
beginning of training, where they measure the global displacement from the initialization of an untrained network and observe a rate ∝ log(k), a form of
ultra-slow diffusion. These empirical observations raise the natural questions, where is the network
_moving to and why? To answer these questions we will build a diffusion based theory of SGD, study_
these dynamics in the setting of linear regression, and use lessons learned in this fundamental setting
to understand the limiting dynamics of neural networks.

3 RELATED WORK
There is a long line of literature studying both theoretically and empirically the learning dynamics
of deep neural networks trained with SGD. Our analysis and experiments build upon this literature.

**Continuous models for SGD. Many works consider how to improve the classic gradient flow model**
for SGD to more realistically reflect momentum [19], discretization due to finite learning rates

[20, 21], and stochasticity due to random batches [22, 23]. One line of work has studied the dynamics of networks in their infinite width limits through dynamical mean field theory [24, 25, 26, 27],
while a different approach has used stochastic differential equations (SDEs) to model SGD directly,
the approach we take in this work. However, recently, the validity of this approach has been questioned. The main argument, as nicely explained in Yaida [28], is that most SDE approximations
simultaneously assume that ∆t → 0[+], while maintaining that the learning rate η = ∆t is finite.
The works Simsekli et al. [29] and Li et al. [30] have questioned the correctness of the using the
central limit theorem (CLT) to model the gradient noise as Gaussian, arguing respectively that the
heavy-tailed structure in the gradient noise and the weak dependence between batches leads the
CLT to break down. In our work, we maintain the CLT assumption holds, which we discuss fur

-----

ther in appendix A, but importantly we avoid the pitfalls of many previous SDE approximations by
simultaneously modeling the effect of finite learning rates and stochasticity.

**Limiting dynamics. A series of works have applied SDE models of SGD to study the limiting**
dynamics of neural networks. In the seminal work by Mandt et al. [31], the limiting dynamics were
modeled with a multivariate Ornstein-Uhlenbeck process by combining a first-order SDE model for
SGD with assumptions on the geometry of the loss and covariance matrix for the gradient noise.
This analysis was extended by Jastrz˛ebski et al. [12] through additional assumptions on the covariance matrix to gain tractable insights and applied by Ali et al. [32] to the simpler setting of linear
regression, which has a quadratic loss. A different approach was taken by Chaudhari and Soatto

[33], which did not formulate the dynamics as an OU process, nor assume directly a structure on the
loss or gradient noise. Rather, this analysis studied the same first-order SDE via the Fokker-Planck
equation to propose the existence of a modified loss and probability currents driving the limiting
dynamics, but did not provide explicit expressions. Our analysis deepens and combines ideas from
all these works, where our key insight is to lift the dynamics into phase space. By studying the
dynamics of the parameters and their velocities, and by applying the analysis first in the setting of
linear regression where assumptions are provably true, we are able to identify analytic expressions
and explicit insights which lead to concrete predictions and testable hypothesis.

**Stationary dynamics. A different line of work avoids modeling the limiting dynamics of SGD**
with an SDE and instead chooses to leverage the property of stationarity. These works [28, 34,
35, 36] assume that eventually the probability distribution governing the model parameters reaches
stationarity such that the discrete SGD process is simply sampling from this distribution. Yaida [28]
used this approach to derive fluctuation-dissipation relations that link measurable quantities of the
parameters and hyperparameters of SGD. Liu et al. [35] used this approach to derive properties for
the stationary distribution of SGD with a quadratic loss. Similar to our analysis, this work identifies
that the stationary distribution for the parameters reflects a modified loss function dependent on
the relationship between the covariance matrix of the gradient noise and the Hessian matrix for the
original loss.

**Empirical exploration. Another set of works analyzing the limiting dynamics of SGD has taken**
a purely empirical approach. Building on the intuition that flat minima generalize better than sharp
minima, Keskar et al. [37] demonstrated empirically that the hyperparameters of optimization influence the eigenvalue spectrum of the Hessian matrix at the end of training. Many subsequent
works have studied the Hessian eigenspectrum during and at the end of training. Jastrz˛ebski et al.

[38], Cohen et al. [39] studied the dynamics of the top eigenvalues during training. Sagun et al.

[40], Papyan [41], Ghorbani et al. [42] demonstrated the spectrum has a bulk of values near zero
plus a small number of larger outliers. Gur-Ari et al. [43] demonstrated that the learning dynamics
are constrained to the subspace spanned by the top eigenvectors, but found no special properties of
the dynamics within this subspace. In our work we also determine that the top eigensubspace of the
Hessian plays a crucial role in the limiting dynamics and by projecting the dynamics into this subspace in phase space, we see that the motion is not random, but consists of incoherent oscillations
leading to anomalous diffusion.

4 MODELING SGD AS AN UNDERDAMPED LANGEVIN EQUATION
Following the route of previous works [31, 12, 33] studying the limiting dynamics of neural
networks, we first seek to model SGD as a continuous stochastic process. We consider a network parameterized byL(θ) = _N1_ _Ni=1_ _[ℓ][(][θ, x] θ[i][)][ with corresponding gradient] ∈_ R[m], a training dataset {[ g]x1[(], . . ., x[θ][) =][ ∂]∂θN[L]}[. The state of the network at the] of size N, and a training loss

_k[th]_ step of training is defined by the position vector θk and velocity vector vk of the same dimension.

P

The gradient descent update with learning rate η, momentum β, and weight decayλ is given by
_vk+1 = βvk −_ _g(θk) −_ _λθk,_ _θk+1 = θk + ηvk+1,_ (2)
where we initialize the network such that v0 = 0 and θ0 is the parameter initialization. In order to
understand the dynamics of the network through position and velocity space, which we will refer to
as phase space, we express these discrete recursive equations as the discretization of some unknown
ordinary differential equation (ODE), sometimes referred to as a modified equation as in [44, 20].
While this ODE models the gradient descent process even at finite learning rates, it fails to account
for the stochasticity introduced by choosing a random batch B of size S drawn uniformly from the

To model this effect, we make the following assumption:set of N training points. This sampling yields the stochastic gradient gB(θ) = _S[1]_ _i∈B_ _[∇][ℓ][(][θ, x][i][)][.]_

P


-----

**Assumption 1 (CLT). We assume the batch gradient is a noisy version of the true gradient such**
_that gB(θ) −_ _g(θ) is a Gaussian random variable with mean 0 and covariance_ _S[1]_ [Σ(][θ][)][.]

Incorporating this model of stochastic gradients into the previous finite difference equation and
applying the stochastic counterparts to Euler discretizations, results in the standard drift-diffusion
stochastic differential equation (SDE), referred to as an underdamped Langevin equation,

_θ_ _v_ 0 0
_d_ v = − _η(1+2_ _β)_ [(][g][(][θ][) +][ λθ][ + (1][ −] _[β][)][v][)]_ _dt +_ 0 _√ηS(1+2_ _β)_ Σ(θ) _dWt,_ (3)

where Wt is a standard Wiener process. This is the continuous model we will study in this work:p
**Assumption 2 (SDE). We assume the underdamped Langevin equation (3) accurately models the**
_trajectory of the network driven by SGD through phase space such that θ(ηk) ≈_ _θk and v(ηk) ≈_ _vk._

See appendix A for further discussion on the nuances of modeling SGD with an SDE.

5 LINEAR REGRESSION WITH SGD IS AN ORNSTEIN-UHLENBECK PROCESS
Equipped with a model for SGD, we seek to understand its dynamics in the fundamental setting of
linear regression, one of the few cases where we have a complete model for the interaction of the
dataset, architecture, and optimizer. Let X ∈ R[N] _[×][d]_ be the input data, Y ∈ R[N] be the output labels,

results in the multivariate Ornstein-Uhlenbeck (OU) process,


_θt_ 0 _I_ _θt_ _µ_
_d_ _vt_ = − _η(1+2_ _β)_ [(][H][ +][ λI][)] _η2(1(1+−−ββ))_ _[I]_ _vt_ _−_ 0
      

_A_

where A and| D are the drift and diffusion matrices re-{z }
spectively, κ = S(1 − _β[2]) is an inverse temperature con-_
stant, and µ = (H + λI)[−][1]b is the ridge regression solution. The solution to an OU process is a Gaussian process.
By solving for the temporal dynamics of the first and second moments of the process, we can obtain an analytic
expression for the trajectory at any time t. In particular,
we can decompose the trajectory as the sum of a deterministic and stochastic component defined by the first and
second moments respectively.

**Deterministic component. Using the form of A we can**
decompose the expectation as a sum of harmonic oscillators in the eigenbasis _q1, . . ., qm_ of the Hessian,
_{_ _}_

_m_

E _θt_ = _µ_ + _ai(t)_ _qi_ + bi(t) 0 _. (5)_
_vt_ 0 0 _qi_
    _i=1_     

X

Here the coefficients ai(t) and bi(t) depend on the optimization hyperparameters η, β, λ, S and the respective
eigenvalue of the Hessian ρi as further explained in
appendix F. We verify this expression nearly perfectly
matches empirics on complex datasets under various hyperparameter settings as shown in Fig. 2.

**Stochastic component. The cross-covariance of the pro-**
cess between two points in time t ≤ _s, is_

Cov _θt_ _,_ _θs_ = _κ[−][1][]B_ _e[−][At]Be[−][A][⊺][t][]e[A][⊺][(][t][−][s][)],_
_vt_ _vs_ _−_
   



[) =][ Hθ][−][b][, where][ H][ =][ X]N[⊺][X] and b = _[X]N[⊺][Y]_ [. Plugging]

this expression for the gradient into the underdamped Langevin equation (3), and rearranging terms,

_vθtt_ _−_ _µ0_ _dt +_ _√2κ[−][1]v_ 00 _η2(1(1+−ββ0))_ [Σ(][θ][)] _dWt,_

   u 

u _D_
u
t (4)

| {z }

**Eq. (7)**

Velocity

Position Step

. In particular,

Velocity

_A we can_ Position Step

Figure 2: **Oscillatory dynamics in**
**linear regression.** We train a linear

0 network to perform regression on the

_. (5)_

_qi_ CIFAR-10 dataset by using an MSE



loss on the one-hot encoding of the la-

depend on the op- bels. We compute the hessian of the
and the respective loss, as well as its top eigenvectors.

The position and velocity trajectories
are projected onto the first eigenvector
of the hessian and visualized in black.
The theoretically derived mean, equa-
tion (5), is shown in red. The top and
bottom panels demonstrate the effect of
varying momentum on the oscillation
mode.

_e[A][⊺][(][t][−][s][)],_

(6)


-----

where B solves the Lyapunov equation AB + BA[⊺] = 2D. In order to gain analytic expressions for
_B in terms of the optimization hyperparameters, eigendecomposition of the Hessian, and covariance_
of the gradient noise, we must introduce the following assumption:
**Assumption 3 (Simultaneously Diagonalizable). We assume the covariance of the gradient noise is**
_spatially independent Σ(θ) = Σ and commutes with the Hessian HΣ = ΣH, therefore sharing a_
_common eigenbasis._

6 UNDERSTANDING STATIONARITY VIA THE FOKKER-PLANCK EQUATION
The OU process is unique in that it is one of the few SDEs which we can solve exactly. As shown in
section 5, we were able to derive exact expressions for the dynamics of linear regression trained with
SGD from initialization to stationarity by simply solving for the first and second moments. While the
expression for the first moment provides an understanding of the intricate oscillatory relationship in
the deterministic component of the process, the second moment, driving the stochastic component,
is much more opaque. An alternative route to solving the OU process that potentially provides more
insight is the Fokker-Planck equation.

The Fokker-Planck (FP) equation is a PDE describing the time evolution for the probability distribution of a particle governed by Langevin dynamics. For an arbitrary potential Φ and diffusion matrix
_D, the Fokker-Planck equation (under an Itô integration prescription) is_
_∂tp =_ Φp + _κ[−][1]Dp_ _,_ (7)
_∇·_ _∇_ _∇·_

_J_

  _−_   

where p represents the time-dependent probability distribution, and J is a vector field commonly

| {z }

referred to as the probability current. The FP equation is especially useful for explicitly solving for
the stationary solution, assuming one exists, of the Langevin dynamics. The stationary solution pss
by definition obeys ∂tpss = 0 or equivalently ∇· Jss = 0. From this second definition we see that
there are two distinct settings of stationarity: detailed balance when Jss = 0 everywhere, or broken
_detailed balance when ∇· Jss = 0 and Jss ̸= 0._

For a general OU process, the potential is a convex quadratic function Φ(x) = x[⊺]Ax defined by
the drift matrix A. When the diffusion matrix is isotropic (D ∝ _I) and spatially independent (∇·_
_D = 0) the resulting stationary solution is a Gibbs distribution pss(x)_ _e[−][κ][Φ(][x][)]_ determined
_∝_
by the original loss Φ(x) and is in detailed balance. Lesser known properties of the OU process
arise when the diffusion matrix is anisotropic or spatially dependent [45, 46]. In this setting the
solution is still a Gaussian process, but the stationary solution, if it exists, is no longer defined by
the Gibbs distribution of the original loss Φ(x), but actually a modified loss Ψ(x). Furthermore,
the stationary solution may be in broken detailed balance leading to a non-zero probability current
_Jss(x). Depending on the relationship between the drift matrix A and the diffusion matrix D the_
resulting dynamics of the OU process can have very nontrivial behavior.

In the setting of linear regression, anisotropy in the data distribution will lead to anisotropy in the
gradient noise and thus an anisotropic diffusion matrix. This implies that for most datasets we should
expect that the SGD trajectory is not driven by the original least squares loss, but by a modified loss
and converges to a stationary solution with broken detailed balance, as predicted by Chaudhari and
Soatto [33]. Using the explicit expressions for the drift A and diffusion D matrices we can compute
analytically the modified loss and stationary probability current,

⊺

_θ_ _µ_ _U_ _θ_ _µ_ _θ_ _µ_
Ψ(θ, v) = _v_ _−_ 0 2 _v_ _−_ 0 _,_ _Jss(θ, v) = −QU_ _v_ _−_ 0 _pss,_ (8)
             

where Q is a skew-symmetric matrix and U is a positive definite matrix defined as,


_η(1+2_ _β)_ [Σ(][θ][)][−][1][ (][H][ +][ λI][)] 0
0 Σ(θ)[−][1]


0 Σ(θ)
_Q =_ _−_
Σ(θ) 0



(9)


_U =_


These new fundamental matrices, Q and U, relate to the original drift A and diffusion D matrices
through the unique decomposition A = (D + Q)U, introduced by Ao [47] and Kwon et al. [48].
Using this decomposition we can easily show that B = U _[−][1]_ solves the Lyapunov equation and
indeed the stationary solution pss is the Gibbs distribution defined by the modified loss Ψ(θ, v)
in equation (8). Further, the stationary cross-covariance solved in section 5 reflects the oscillatory
dynamics introduced by the stationary probability currents Jss(θ, v) in equation (8). Taken together,
we gain the intuition that the limiting dynamics of SGD in linear regression are driven by a modified
loss subject to oscillatory probability currents.


-----

7 EVIDENCE OF A MODIFIED LOSS AND OSCILLATIONS IN DEEP LEARNING

Does the theory derived in the linear regression setting (sections 5, 6) help explain the empirical
phenomena observed in the non-linear setting of deep neural networks (section 2)? In order for the
theory built in the previous sections to apply to the limiting dynamics of neural networks, we must
introduce simplifying assumptions on the loss landscape and gradient noise at the end of training:

**Assumption 4 (Quadratic Loss). We assume that at the end of training the loss for a neural network**
_can be approximated by the quadratic loss_ (θ) = (θ _µ)[⊺]_ [ ][ H]2 (θ _µ), where H_ 0 is the
_L_ _−_ _−_ _⪰_

_training loss Hessian and µ is some unknown mean vector, corresponding to a local minimum._


**Assumption 5 (Covariance Structure). We assume the covariance of the gradient noise is propor-**
_tional to the Hessian of the quadratic loss Σ(θ) = σ[2]H where σ ∈_ R[+] _is some unknown scalar._

Under these simplifications, then the expressions derived in the linear regression setting would apply
to the limiting dynamics of deep neural networks and depend only on quantities that we can easily
estimate empirically. Of course, these simplifications are quite strong, but without arguing their theoretical validity, we can empirically test their qualitative implications: (1) a modified isotropic loss
driving the limiting dynamics through parameter space, (2) implicit regularization of the velocity
trajectory, and (3) oscillatory phase space dynamics determined by the Hessian eigen-structure.

**Modified loss. As discussed in section 6, due to the anisotropy of the diffusion matrix, the loss**
landscape driving the dynamics at the end of training is not the original training loss L(θ), but a
modified loss Ψ(θ, v) in phase space. As shown in equation (8), the modified loss decouples into a
term Ψθ that only depends on the parameters θ and a term Ψv that only depends on the velocities v.
Under assumption 5, the parameter dependent component is proportional to the convex quadratic,


_H_ _−1(H + λI)_
Ψθ (θ _µ)[⊺]_
_∝_ _−_ _η(1 + β)_



(θ − _µ) ._ (10)


This quadratic function has the same mean µ as the training loss, but a different curvature. Using
this expression, notice that when λ ≈ 0, the modified loss is isotropic in the column space of H,
regardless of what the nonzero eigenspectrum of H is. This striking prediction suggests that no
matter how anisotropic the original training loss – as reflected by poor conditioning of the Hessian
eigenspectrum – the training trajectory of the network will behave isotropically, since it is driven not
by the original anisotropic loss, but a modified isotropic loss.

We test this prediction by studying the limiting dynamics of a pre-trained ResNet-18 model with
batch normalization that we continue to train on ImageNet according to the last setting of its hyperparameters [49]. Let θ represent the initial pre-trained parameters of the network, depicted with the
_∗_
white dot in figures 3 and 4.

Figure 3: The training trajectory behaves isotropically, regardless of the training loss. We

Training loss train Test loss test Modified loss
_L_ _L_

resume training of a pre-trained ResNet-18 model on ImageNet and project its parameter trajectory (black line) onto the space spanned by the eigenvectors of its pre-trained Hessian q1, q30 (with
eigenvalue ratio ρ1/ρ30 6). We sample the training and test loss within the same 2D subspace
and visualize them as a heatmap in the left and center panels respectively. We visualize the mod- ≃
ified loss computed from the eigenvalues (ρ1, ρ30) and optimization hyperparameters according to
equation (10) in the right plot. Note the projected trajectory is isotropic, despite the anisotropy of
the training and test loss.


-----

We estimate[1] the top thirty eigenvectors q1, . . ., q30 of the
Hessian matrix H evaluated at θ and project the limit_∗_ _∗_

30

ing trajectory for the parameters onto the plane spanned
by the top q1 and bottom q30 eigenvectors to maximize the
illustrated anisotropy with our estimates. We sample the
train and test loss in this subspace for a region around the
projected trajectory. Additionally, using the hyperparameters of the optimization, the eigenvalues ρ1 and ρ30, and
the estimate for the mean µ = θ _H_ _[−][1]_
_∗_ _−_ _∗_ _[g][∗]_ [(][g][∗] [is the gra-]
dient evaluated at θ ), we also sample from the modified
_∗_
loss equation (10) in the same region. Figure 3 shows the
projected parameter trajectory on the sampled train, test
and modified losses. Contour lines of both the train and
test loss exhibit anisotropic structure, with sharper curvature along eigenvector q1 compared to eigenvector q30, as
expected. However, as predicted, the trajectory appears
to cover both directions equally. This striking isotropy
of the trajectory within a highly anisotropic slice of the
loss landscape indicates qualitatively that the trajectory
evolves in a modified isotropic loss landscape.

**Implicit velocity regularization. A second qualitative**
prediction of the theory is that the velocity is regulated
by the inverse Hessian of the training loss. Of course
there are no explicit terms in either the train or test losses
that depend on the velocity. Yet, the modified loss contains a component, Ψv _v[⊺]H_ _[−][1]v, that only depends on_
the velocities This additional term can be understood as a ∝
form of implicit regularization on the velocity trajectory.
Indeed, when we project the velocity trajectory onto the
plane spanned by the q1 and q30 eigenvectors, as shown
in Fig. 4, we see that the trajectory closely resembles the
curvature of the inverse Hessian H _[−][1]. The modified loss_
is effectively penalizing SGD for moving in eigenvectors
of the Hessian with small eigenvalues. A similar qualitative effect was recently proposed by Barrett and Dherin

[21] as a consequence of the discretization error due to
finite learning rates.

**Phase space oscillations. A final implication of the the-**

_q30, as_ **tion defined by the inverse Hessian.The shape of the projected velocity tra-**

jectory closely resembles the contours
of the modified loss Ψv.

A second qualitative Velocity

Step

Position

, that only depends on

Velocity

eigenvectors, as shown

Step

. The modified loss Position

Figure 5: Phase space oscillations are
**determined by the eigendecomposi-**
**tion of the Hessian. We visualize the**
projected position and velocity trajectories in phase space. The top and bottom

ory is that at stationarity the network is in broken detailed
balance leading to non-zero probability currents flowing
through phase space:


Figure 4: Implicit velocity regulariza
**tion defined by the inverse Hessian.**
The shape of the projected velocity trajectory closely resembles the contours
of the modified loss Ψv.

Velocity

Step

Position

Velocity

Step

Position

Figure 5: Phase space oscillations are
**determined by the eigendecomposi-**
**tion of the Hessian. We visualize the**
projected position and velocity trajectories in phase space. The top and bottom
panels show the projections onto q1 and
_q30 respectively. Oscillations at differ-_
ent rates are distinguishable for the different eigenvectors and were verified by
comparing the dominant frequencies in
the fast Fourier transform of the trajectories.


_pss._ (11)


_Jss(θ, v) =_


2

_η(1+β)_ [(][H][ +][ λI][) (][θ][ −] _[µ][)]_


These probability currents encourage oscillatory dynamics in the phase space planes characterized
by the eigenvectors of the Hessian, at rates proportional to their eigenvalues. We consider the same
projected trajectory of the ResNet-18 model visualized in figures 3 and 4, but plot the trajectory in
phase space for the two eigenvectors q1 and q30 separately. Shown in Fig. 5, we see that both trajectories look like noisy clockwise rotations. Qualitatively, the trajectories for the different eigenvectors
appear to be rotating at different rates.

The integral curves of the stationary probability current are one-dimensional paths confined to level
sets of the modified loss. These paths might cross themselves, in which case they are limit cycles, or
they could cover the entire surface of the level sets, in which case they are space-filling curves. This
distinction depends on the relative frequencies of the oscillations, as determined by the pairwise

1To estimate the eigenvectors of H we use subspace iteration, and limit ourselves to 30 eigenvectors to
_∗_
constrain computation time. See appendix H for details.


-----

ratios of the eigenvalues of the Hessian. For real-world datasets, with a large spectrum of incommensurate frequencies, we expect to be in the latter setting, thus contradicting the suggestion that
SGD in deep networks converges to limit cycles, as claimed in Chaudhari and Soatto [33].

8 UNDERSTANDING THE DIFFUSIVE BEHAVIOUR OF THE LIMITING
DYNAMICS
Taken together the empirical results shown in section 7 indicate that many of the same qualitative
behaviors of SGD identified theoretically for linear regression are evident in the limiting dynamics
of neural networks. Can this theory quantitatively explain the results we identified in section 2?

**Constant instantaneous speed. As noted in section 2, we observed that at the end of training,**
across various architectures, the squared norm of the local displacement ∥δt∥2[2] [remains essentially]
constant. Assuming the limiting dynamics are described by the stationary solution the expectation
of the local displacement is

_η[2]_
Ess _δt_ = (12)
_∥_ _∥[2][]_ _S(1_ _β[2])_ _[σ][2][tr (][H][)][,]_

_−_



as derived in appendix G. We cannot test this prediction directly as we do not know σ[2] and computing tr(H) is computationally prohibitive. However, we can estimate σ[2]tr(H) by resuming training
for a model, measuring the average _δt_, and then inverting equation (12). Using this single es_∥_ _∥[2]_
timate, we find that for a sweep of models with varying hyperparameters, equation (12) accurately

eters, namely η = 1e − 4, S = 256, and β = 0.9, we vary each one while keeping the others fixed.

predicts their instantaneous speed. Indeed, Fig. 6 shows an exact match between the empirics and
theory, which strongly suggests that despite changing hyperparameters at the end of training, the
model remains in the same quadratic basin.

**Eq. (11)**

Diffusion c **c = 1.0**

Learning rate Batch size Momentum

**Understanding how the hyperparameters of optimization influence the diffusion.**
resume training of pre-trained ResNet-18 models on ImageNet using a range of learning rates, batch
sizes, and momentum coefficients, tracking _δt_ and ∆t . Starting from the default hyperparam_∥_ _∥[2]_ _∥_ _∥[2]_

The top row shows the measured _δt_ in color, with the default hyperparameter setting highlighted
_∥_ _∥[2]_
in black. The dotted line depicts the predicted value from equation (12). The bottom row shows the
estimated exponent c found by fitting a line to the ∆t trajectories on a log-log plot. The dotted
_∥_ _∥[2]_
line shows c = 1, corresponding to standard diffusion.

**Exponent of anomalous diffusion. The expected value for the global displacement under the sta-**
tionary solution can also be analytically expressed in terms of the optimization hyperparameters and
the eigendecomposition of the Hessian as,

_t_ _m_

_η[2]_
Ess ∆t = tr (H) t + 2t 1 _ρlCl(k)_ _,_ (13)
_∥_ _∥[2][]_ _S(1 −_ _β[2])_ _[σ][2]_ _k=1_  _−_ _[k]t_  _l=1_ !
 X X

where Cl(k) is a trigonometric function describing the velocity of a harmonic oscillator with damping ratio ζl = (1 − _β)/_ 2η(1 + β) (pl + λ), see appendix G for details. As shown empirically in

section 2, the squared norm ∆t monotonically increases as a power law in the number of steps,

p _∥_ _∥[2]_

suggesting its expectation is proportional to t[c] for some unknown, constant c. The exponent c determines the regime of diffusion for the process. When c = 1, the process corresponds to standard
Brownian diffusion. For c > 1 or c < 1 the process corresponds to anomalous super-diffusion
or sub-diffusion respectively. Unfortunately, it is not immediately clear how to extract the explicit


-----

exponent c from equation (13). However, by exploring the functional form of Cl(k) and its relationship to the hyperparameters of optimization through the damping ratio ζl, we can determine overall
trends in the diffusion exponent c.

Akin to how the exponent c determines the regime of diffusion, the damping ratio ζl determines the
regime for the harmonic oscillator describing the stationary velocity-velocity correlation in the l[th]
eigenvector of the Hessian. When ζl = 1, the oscillator is critically damped implying the velocity
correlations converge to zero as quickly as possible. In the extreme setting of Cl(k) = 0 for all
_l, k, then equation (13) simplifies to standard Brownian diffusion, Ess_ _∥∆t∥[2][]_ _∝_ _t. When ζl > 1,_
the oscillator is overdamped implying the velocity correlations dampen slowly and remain positive

even over long temporal lags. Such long lasting temporal correlations in velocity lead to faster
global displacement. Indeed, in the extreme setting of Cl(k) = 1 for all l, k, then equation (13)
simplifies to a form of anomalous super-diffusion, Ess _∥∆t∥[2][]_ _∝_ _t[2]. When ζl < 1, the oscillator is_
underdamped implying the velocity correlations will oscillate quickly between positive and negative
values. Indeed, the only way equation (13) could describe anomalous sub-diffusion is if Cl(k) took
on negative values for certain l, k.

Using the same sweep of models described previously, we can empirically confirm that the optimization hyperparameters each influence the diffusion exponent c. As shown in Fig. 6, the learning
rate, batch size, and momentum can each independently drive the exponent c into different regimes
of anomalous diffusion. Notice how the influence of the learning rate and momentum on the diffusion exponent c closely resembles their respective influences on the damping ratio ζl. Interestingly, a larger learning rate leads to underdamped oscillations, and the resultant temporal velocities’
anti-correlations reduce the exponent of anomalous diffusion. Thus contrary to intuition, a larger
learning rate actually leads to slower global transport in parameter space. The batch size on the other
hand, has no influence on the damping ratio, but leads to an interesting, non-monotonic influence
on the diffusion exponent. Overall, the hyperparameters of optimization and eigenspectrum of the
Hessian all conspire to govern the degree of anomalous diffusion at the end of training.

9 DISCUSSION
Through combined empirics and theory based on statistical physics, we uncovered an intricate interplay between the optimization hyperparameters, structure in the gradient noise, and the Hessian
matrix at the end of training.

**Significance. The significance of our work lies in (1) the identification/verification of multiple**
empirical phenomena (constant instantaneous speed, anomalous diffusion in global displacement,
isotropic parameter exploration despite anisotopic loss, velocity regularization, and slower global
parameter exploration with faster learning rates) present in the limiting dynamics of deep neural
networks, (2) the emphasis on studying the dynamics in velocity space in addition to parameter
space, and (3) concrete quantitative as well as qualitative predictions of an SDE based theory that
we empirically verified in deep networks trained on large scale datasets (indeed some of the above
nontrivial phenomena were predictions of this theory). Of course, these contributions directly build
upon a series of related works studying the immensely complex process of deep learning. To this
end, we further clarify the originality of our contributions with respect to some relevant works.

**Originality. The empirical phenomena we present provide novel insight with respect to the works**
of Wan et al. [13], Hoffer et al. [18], and Chen et al. [15]. We observe that all parameters in the
network (not just those with scale symmetry) move at a constant instantaneous speed at the end
of training and diffuse anomalously at rates determined by the hyperparameters of optimization.
In contrast to the work by Liu et al. [35], we modeled the entire SGD process as an OU process
which allows us to provide insight into the transient dynamics and identify oscillations in parameter
and velocity space. We build on the theoretical framework used by Chaudhari and Soatto [33] and
provide explicit expressions for the limiting dynamics in the simplified linear regression setting and
conclude that the oscillations present in the limiting dynamics are more likely to be space-filling
curves (and not limit cycles) in deep learning due to many incommensurate oscillations.

Overall, by identifying key phenomena, explaining them in a simpler setting, deriving predictions
of new phenomena, and providing evidence for these predictions at scale, we are furthering the
scientific study of deep learning. We hope our newly derived understanding of the limiting dynamics
of SGD, and its dependence on various important hyperparameters like batch size, learning rate, and
momentum, can serve as a basis for future work that can turn these insights into algorithmic gains.


-----

REFERENCES

[1] Radford M Neal. Priors for infinite networks. In Bayesian Learning for Neural Networks,
pages 29–53. Springer, 1996.

[2] Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information propagation. arXiv preprint arXiv:1611.01232, 2016.

[3] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington,
and Jascha Sohl-Dickstein. Deep neural networks as gaussian processes. _arXiv preprint_
_arXiv:1711.00165, 2017._

[4] Arthur Jacot, Franck Gabriel, and Clément Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. arXiv preprint arXiv:1806.07572, 2018.

[5] Jaehoon Lee, Lechao Xiao, Samuel S Schoenholz, Yasaman Bahri, Roman Novak, Jascha
Sohl-Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear
models under gradient descent. arXiv preprint arXiv:1902.06720, 2019.

[6] Mei Song, Andrea Montanari, and P Nguyen. A mean field view of the landscape of two-layers
neural networks. Proceedings of the National Academy of Sciences, 115:E7665–E7671, 2018.

[7] Grant M Rotskoff and Eric Vanden-Eijnden. Parameters as interacting particles: long time
convergence and asymptotic error scaling of neural networks. In Proceedings of the 32nd
_International Conference on Neural Information Processing Systems, pages 7146–7155, 2018._

[8] Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for overparameterized models using optimal transport. arXiv preprint arXiv:1805.09545, 2018.

[9] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, and Andrew Gordon Wilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. In Proceedings of the 32nd
_International Conference on Neural Information Processing Systems, pages 8803–8812, 2018._

[10] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon
Wilson. Averaging weights leads to wider optima and better generalization. arXiv preprint
_arXiv:1803.05407, 2018._

[11] Wesley Maddox, Timur Garipov, Pavel Izmailov, Dmitry Vetrov, and Andrew Gordon Wilson.
A simple baseline for bayesian uncertainty in deep learning. arXiv preprint arXiv:1902.02476,
2019.

[12] Stanisław Jastrz˛ebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua
Bengio, and Amos Storkey. Three factors influencing minima in sgd. _arXiv preprint_
_arXiv:1711.04623, 2017._

[13] Ruosi Wan, Zhanxing Zhu, Xiangyu Zhang, and Jian Sun. Spherical motion dynamics of deep
neural networks with batch normalization and weight decay. arXiv preprint arXiv:2006.08419,
2020.

[14] Marco Baity-Jesi, Levent Sagun, Mario Geiger, Stefano Spigler, Gérard Ben Arous, Chiara
Cammarota, Yann LeCun, Matthieu Wyart, and Giulio Biroli. Comparing dynamics: Deep
neural networks versus glassy systems. In International Conference on Machine Learning,
pages 314–323. PMLR, 2018.

[15] Guozhang Chen, Cheng Kevin Qu, and Pulin Gong. Anomalous diffusion dynamics of learning
in deep neural networks. arXiv preprint arXiv:2009.10588, 2020.

[16] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. Neural Information Processing Systems Workshop, 2017.

[17] Zhiyuan Li, Kaifeng Lyu, and Sanjeev Arora. Reconciling modern deep learning with traditional optimization analyses: The intrinsic learning rate. In Advances in Neural Information
_Processing Systems, volume 33, pages 14544–14555. Curran Associates, Inc., 2020._


-----

[18] Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generalization gap in large batch training of neural networks. In Advances in Neural Information
_Processing Systems, volume 30. Curran Associates, Inc., 2017._

[19] Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks,
12(1):145–151, 1999.

[20] Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel LK Yamins, and Hidenori
Tanaka. Neural mechanics: Symmetry and broken conservation laws in deep learning dynamics. arXiv preprint arXiv:2012.04728, 2020.

[21] David GT Barrett and Benoit Dherin. Implicit gradient regularization. _arXiv preprint_
_arXiv:2009.11162, 2020._

[22] Qianxiao Li, Cheng Tai, and E Weinan. Stochastic modified equations and adaptive stochastic
gradient algorithms. In International Conference on Machine Learning, pages 2101–2110.
PMLR, 2017.

[23] Samuel L Smith, Benoit Dherin, David GT Barrett, and Soham De. On the origin of implicit
regularization in stochastic gradient descent. arXiv preprint arXiv:2101.12176, 2021.

[24] Francesca Mignacco, Florent Krzakala, Pierfrancesco Urbani, and Lenka Zdeborová. Dynamical mean-field theory for stochastic gradient descent in gaussian mixture classification. arXiv
_preprint arXiv:2006.06098, 2020._

[25] Stefano Sarao Mannelli, Eric Vanden-Eijnden, and Lenka Zdeborová. Optimization and generalization of shallow neural networks with quadratic activation functions. _arXiv preprint_
_arXiv:2006.15459, 2020._

[26] Francesca Mignacco, Pierfrancesco Urbani, and Lenka Zdeborová. Stochasticity helps to navigate rough landscapes: comparing gradient-descent-based algorithms in the phase retrieval
problem. Machine Learning: Science and Technology, 2021.

[27] Stefano Sarao Mannelli and Pierfrancesco Urbani. Just a momentum: Analytical study of
momentum-based acceleration methods in paradigmatic high-dimensional non-convex problems. arXiv preprint arXiv:2102.11755, 2021.

[28] Sho Yaida. Fluctuation-dissipation relations for stochastic gradient descent. arXiv preprint
_arXiv:1810.00004, 2018._

[29] Umut Simsekli, Levent Sagun, and Mert Gurbuzbalaban. A tail-index analysis of stochastic
gradient noise in deep neural networks. In International Conference on Machine Learning,
pages 5827–5837. PMLR, 2019.

[30] Zhiyuan Li, Sadhika Malladi, and Sanjeev Arora. On the validity of modeling sgd with stochastic differential equations (sdes). arXiv preprint arXiv:2102.12470, 2021.

[31] Stephan Mandt, Matthew Hoffman, and David Blei. A variational analysis of stochastic gradient algorithms. In International conference on machine learning, pages 354–363. PMLR,
2016.

[32] Alnur Ali, Edgar Dobriban, and Ryan Tibshirani. The implicit regularization of stochastic
gradient flow for least squares. In International Conference on Machine Learning, pages 233–
244. PMLR, 2020.

[33] Pratik Chaudhari and Stefano Soatto. Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks. In 2018 Information Theory and Applica_tions Workshop (ITA), pages 1–10. IEEE, 2018._

[34] Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George Dahl,
Chris Shallue, and Roger B Grosse. Which algorithmic choices matter at which batch sizes?
insights from a noisy quadratic model. In Advances in Neural Information Processing Systems,
volume 32. Curran Associates, Inc., 2019.


-----

[35] Kangqiao Liu, Liu Ziyin, and Masahito Ueda. Noise and fluctuation of finite learning rate
stochastic gradient descent. In International Conference on Machine Learning, pages 7045–
7056. PMLR, 2021.

[36] Liu Ziyin, Kangqiao Liu, Takashi Mori, and Masahito Ueda. On minibatch noise: Discretetime sgd, overparametrization, and bayes. arXiv preprint arXiv:2102.05375, 2021.

[37] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping
Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.

[38] Stanisław Jastrz˛ebski, Zachary Kenton, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and
Amos Storkey. On the relation between the sharpest directions of dnn loss and the sgd step
length. arXiv preprint arXiv:1807.05031, 2018.

[39] Jeremy M Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar. Gradient descent on neural networks typically occurs at the edge of stability. _arXiv preprint_
_arXiv:2103.00065, 2021._

[40] Levent Sagun, Utku Evci, V Ugur Guney, Yann Dauphin, and Leon Bottou. Empirical analysis
of the hessian of over-parametrized neural networks. arXiv preprint arXiv:1706.04454, 2017.

[41] Vardan Papyan. The full spectrum of deepnet hessians at scale: Dynamics with sgd training
and sample size. arXiv preprint arXiv:1811.07062, 2018.

[42] Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net optimization via hessian eigenvalue density. In International Conference on Machine Learning,
pages 2232–2241. PMLR, 2019.

[43] Guy Gur-Ari, Daniel A Roberts, and Ethan Dyer. Gradient descent happens in a tiny subspace.
_arXiv preprint arXiv:1812.04754, 2018._

[44] Nikola B Kovachki and Andrew M Stuart. Analysis of momentum methods. arXiv preprint
_arXiv:1906.04285, 2019._

[45] Crispin W Gardiner et al. Handbook of stochastic methods, volume 3. springer Berlin, 1985.

[46] Hannes Risken. Fokker-planck equation. In The Fokker-Planck Equation, pages 63–95.
Springer, 1996.

[47] Ping Ao. Potential in stochastic differential equations: novel construction. Journal of physics
_A: mathematical and general, 37(3):L25, 2004._

[48] Chulan Kwon, Ping Ao, and David J Thouless. Structure of stochastic dynamics near fixed
points. Proceedings of the National Academy of Sciences, 102(37):13029–13033, 2005.

[49] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recogni_tion, pages 770–778, 2016._

[50] Tomaso Poggio, Kenji Kawaguchi, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Xavier
Boix, Jack Hidary, and Hrushikesh Mhaskar. Theory of deep learning iii: explaining the nonoverfitting puzzle. arXiv preprint arXiv:1801.00173, 2017.

[51] Levent Sagun, Leon Bottou, and Yann LeCun. Eigenvalues of the hessian in deep learning:
Singularity and beyond. arXiv preprint arXiv:1611.07476, 2016.

[52] Valentin Thomas, Fabian Pedregosa, Bart Merriënboer, Pierre-Antoine Manzagol, Yoshua
Bengio, and Nicolas Le Roux. On the interplay between noise and curvature and its effect
on optimization and generalization. In International Conference on Artificial Intelligence and
_Statistics, pages 3503–3513. PMLR, 2020._

[53] Richard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the fokker–
planck equation. SIAM journal on mathematical analysis, 29(1):1–17, 1998.

[54] Richard Jordan, David Kinderlehrer, and Felix Otto. Free energy and the fokker-planck equation. Physica D: Nonlinear Phenomena, 107(2-4):265–271, 1997.


-----

A MODELING SGD WITH AN SDE

As explained in section 4, in order to understand the dynamics of stochastic gradient descent we
build a continuous Langevin equation in phase space modeling the effect of discrete updates and
stochastic batches simultaneously.

A.1 MODELING DISCRETIZATION

To model the discretization effect we assume that the system of update equations (2) is actually a
discretization of some unknown ordinary differential equation. To uncover this ODE, we combine
the two update equations in (2), by incorporating a previous time step θk−1, and rearrange into
the form of a finite difference discretization, as shown in equation (??). Like all discretizations,
the Euler discretizations introduce error terms proportional to the step size, which in this case is
the learning rate η. Taylor expanding θk+1 and θk 1 around θk, its easy to show that both Euler
_−_
discretizations introduce a second-order error term proportional to _[η]2_ _θ[¨]._


_θk+1_ _θk_
_−_


= θ[˙] + _[η]_


_θ¨ + O(η[2]),_ _θk −_ _θk−1_


= θ[˙]
_−_ _[η]2_


_θ¨ + O(η[2])._


Notice how the momentum coefficient β ∈ [0, 1] regulates the amount of backward Euler incorporated into the discretization. When β = 0, we remove all backward Euler discretization leaving
just the forward Euler discretization. When β = 1, we have equal amounts of backward Euler as
forward Euler resulting in a central second-order discretization[2] as noticed in [19].

A.2 MODELING STOCHASTICITY

In order to model the effect of stochastic batches, we first model a batch gradient with the following
assumption:

**Assumption 1 (CLT). We assume the batch gradient is a noisy version of the true gradient such that**
_gB(θ) −_ _g(θ) is a Gaussian random variable with mean 0 and covariance_ _S[1]_ [Σ(][θ][)][.]

The two conditions needed for the CLT to hold are not exactly met in the setting of SGD. Indepen_dent and identically distributed. Generally we perform SGD by making a complete pass through the_
entire dataset before using a sample again which introduces a weak dependence between samples.
While the covariance matrix without replacement more accurately models the dependence between
samples within a batch, it fails to account for the dependence between batches. Finite variance.
A different line of work has questioned the Gaussian assumption entirely because of the need for
finite variance random variables. This work instead suggests using the generalized central limit theorem implying the noise would be a heavy-tailed α-stable random variable [29]. Thus, the previous
assumption is implicitly assuming the i.i.d. and finite variance conditions apply for large enough
datasets and small enough batches.

Under the CLT assumption, we must also replace the Euler discretizations with Euler–Maruyama
discretizations. For a general stochastic process, dXt = µdt + σdWt, the Euler–Maruyama method
extends the Euler method for ODEs to SDEs, resulting in the update equation Xk+1 = Xk + ∆tµ +
_√∆tσξ, where ξ ∼N_ (0, 1). Notice, the key difference is that if the temporal step size is ∆t = η,

then the noise is scaled by the square root _η. In fact, the main argument against modeling SGD_

_[√]_
with an SDE, as nicely explained in Yaida [28], is that most SDE approximations simultaneously
assume that ∆t → 0[+], while maintaining that the square root of the learning rate _[√]η is finite._
However, by modeling the discretization and stochastic effect simultaneously we can avoid this
argument, bringing us to our second assumption:

**Assumption 2 (SDE). We assume the underdamped Langevin equation (3) accurately models the**
_trajectory of the network driven by SGD through phase space such that θ(ηk) ≈_ _θk and v(ηk) ≈_ _vk._

This approach of modeling discretization and stochasticity simultaneously is called stochastic modified equations, as further explained in Li et al. [22].

2The difference between a forward Euler and backward Euler discretization is a second-order central discretization, _θk+1η−θk_ _θk−ηθk−1_ = η _θk+1−2ηθ[2]k+θk−1_ = ηθ[¨] + O(η[2]).

_−_

     


-----

B STRUCTURE IN THE COVARIANCE OF THE GRADIENT NOISE

As we’ve mentioned before, SGD introduces highly structured noise into an optimization process,
often assumed to be an essential ingredient for its ability to avoid local minima.

**Assumption 5 (Covariance Structure). We assume the covariance of the gradient noise is propor-**
_tional to the Hessian of the quadratic loss Σ(θ) = σ[2]H where σ ∈_ R[+] _is some unknown scalar._

In the setting of linear regression, this is a very natural assumption. If we assume the classic generative model for linear regression data yi = x[⊺]i _θ[¯]+σϵ where,_ _θ[¯] ∈_ R[d] is the true model and ϵ ∼N (0, 1),
then provably Σ(θ) ≈ _σ[2]H._


_N_
_i=1_ _[g][i][g]i[⊺]_

_[−]_ _[gg][⊺][. Near stationarity][ gg][⊺]_ _[≪]_
P


_Proof.1_ _N We can estimate the covariance as Σ(θ) ≈_

_N_ _i=1_ _[g][i][g]i[⊺][, and thus,]_
P

Σ(θ)
_≈_ _N[1]_


Σ(θ) _gigi[⊺][.]_
_≈_ _N[1]_

_i=1_

X

Under the generative model yi = x[⊺]i _θ[¯] + σϵ where ϵ ∼N_ (0, 1) and σ ∈ R[+], then the gradient gi is

_gi = (x[⊺]i_ [(][θ][ −] _θ[¯]) −_ _σϵ)xi,_

and the matrix gigi[⊺] [is]
_gigi[⊺]_ [= (][x]i[⊺][(][θ][ −] _θ[¯])_ _σϵ)[2](xix[⊺]i_ [)][.]
_−_

Assuming θ ≈ _θ[¯] at stationarity, then (x[⊺]i_ [(][θ][ −] _θ[¯]) −_ _σϵ)[2]_ _≈_ _σ[2]. Thus,_


Σ(θ)
_≈_ _[σ]N[2]_


_N_

_xix[⊺]i_ [=][ σ][2]

_N [X]_ [⊺][X][ =][ σ][2][H]

_i=1_

X


Also notice that weight decay is independent of the data or batch and thus simply shifts the gradient
distribution, but leaves the covariance of the gradient noise unchanged.

While the above analysis is in the linear regression setting, for deep neural networks it is reasonable
to make the same assumption. See the appendix of Jastrz˛ebski et al. [12] for a discussion on this
assumption in the non-linear setting.

Recent work by Ali et al. [32] also studies the dynamics of SGD (without momentum) in the setting
of linear regression. This work, while studying the classic first-order stochastic differential equation,
made a point to not introduce an assumption on the diffusion matrix. In particular, they make the
point that even in the setting of linear regression, a constant covariance matrix will fail to capture
the actual dynamics. To illustrate this point they consider the univariate responseless least squares
problem,


minimize
_θ∈R_


(xiθ)[2].
_i=1_

X


2n


As they explain, the SGD update for this problem would be


_k_

_θk+1 = θk −_ _S[η]_ Xi∈B _xi!_ _θk =_ _iY=1(1 −_ _η(_ _S[1]_ Xi∈B _xi))θ0,_

from which they conclude for a small enough learning rate η, then with probability one θk
0. They contrast this with the Ornstein-Uhlenbeck process given by a constant covariance matrix →
where while the mean for θk converges to zero its variance converges to a positive constant. So
is this discrepancy evidence that an Ornstein-Uhlenbeck process with a constant covariance matrix
fails to capture the updates of SGD? In many ways this problem is not a simple example, rather a
pathological edge case. Consider the generative model that would give rise to this problem,

_y = 0x + 0ξ = 0._

In otherwords, the true model _θ[¯] = 0 and the standard deviation for the noise σ = 0. This would_
imply by the assumption used in our paper that there would be zero diffusion and the resulting SDE
would simplify to a deterministic ODE that exponentially converges to zero.


-----

C A QUADRATIC LOSS AT THE END OF TRAINING

**Assumption 4 (Quadratic Loss). We assume that at the end of training the loss for a neural network**
_can be approximated by the quadratic loss_ (θ) = (θ _µ)[⊺]_ [ ][ H]2 (θ _µ), where H_ 0 is the
_L_ _−_ _−_ _⪰_

_training loss Hessian and µ is some unknown mean vector, corresponding to a local minimum._


This assumption has been amply used in previous works such as Mandt et al. [31], Jastrz˛ebski et al.

[12], and Poggio et al. [50]. Particularly, Mandt et al. [31] discuss how this assumption makes sense
for smooth loss functions for which the stationary solution to the stochastic process reaches a deep
local minimum from which it is difficult to escape.

It is a well-studied fact, both empirically and theoretically, that the Hessian is low-rank near local
minima as noted by Sagun et al. [51], and Kunin et al. [20]. This degeneracy results in flat directions
of equal loss. Kunin et al. [20] discuss how differentiable symmetries, architectural features that
keep the loss constant under certain weight transformations, give rise to these flat directions. Importantly, the Hessian and the covariance matrix share the same null space, and thus we can always
restrict ourselves to the image space of the Hessian, where the drift and diffusion matrix will be full
rank. Further discussion on the relationship between the Hessian and the covariance matrix can be
found in Thomas et al. [52].

It is also a well known empirical fact that even at the end of training the Hessian can have negative
eigenvalues [41]. This empirical observation is at odds with our assumption that the Hessian is
positive semi-definite H ⪰ 0. Further analysis is needed to alleviate this inconsistency.


-----

D SOLVING AN ORNSTEIN-UHLENBECK PROCESS WITH ANISOTROPIC
NOISE

We will study the multivariate Ornstein-Uhlenbeck process described by the stochastic differential
equation
_dXt = A(µ −_ _Xt)dt +_ _√2κ[−][1]DdWt_ _X0 = x0,_ (14)

where A S[m]++ [is a positive definite drift matrix,][ µ][ ∈] [R][m][ is a mean vector,][ κ][ ∈] [R][+][ is some positive]
_∈_
constant, and D ∈ S[m]++ [is a positive definite diffusion matrix. This OU process is unique in that it]
is one of the few SDEs we can solve explicitly. We can derive an expression for XT as,

_T_
_XT = e[−][AT]_ _x0 +_ _I −_ _e[−][AT][ ]_ _µ +_ 0 _e[A][(][t][−][T][ )][√]2κ[−][1]DdWt._ (15)
Z
 

_Proof. Consider the function f_ (t, x) = e[At]x where e[A] is a matrix exponential. Then by Itô’s
Lemma[3] we can evaluate the derivative of f (t, Xt) as

_df_ (t, Xt) = _Ae[At]Xt + e[At]A(µ −_ _Xt)_ _dt + e[At][√]2κ[−][1]DdWt_

= Ae  _[At]µdt + e[At][√]2κ[−][1]DdW_ _t_

Integrating this expression from t = 0 to t = T gives

_T_ _T_
_f_ (T, XT ) − _f_ (0, X0) = 0 _Ae[At]µdt +_ 0 _e[At][√]2κ[−][1]DdWt_
Z Z

_T_
_e[AT]_ _XT −_ _x0 =_ _e[AT]_ _−_ _I_ _µ +_ 0 _e[At][√]2κ[−][1]DdWt_
Z

which rearranged gives the expression for X  _T ._ 

From this expression it is clear that XT is a Gaussian process. The mean of the process is
E [XT ] = e[−][AT] _x0 +_ _I −_ _e[−][AT][ ]_ _µ,_ (16)
and the covariance and cross-covariance of the process are
 

_T_
Var(XT ) = κ[−][1] _e[A][(][t][−][T][ )]2De[A][⊺][(][t][−][T][ )]dt,_ (17)

0

Z

min(T,S)
Cov(XT, XS) = κ[−][1] _e[A][(][t][−][T][ )]2De[A][⊺][(][t][−][S][)]dt._ (18)

0

Z

These last two expressions are derived by Itô Isometry[4].


D.1 THE LYAPUNOV EQUATION

We can explicitly solve the integral expressions for the covariance and cross-covariance exactly by
solving for the unique matrix B ∈ S[m]++ [that solves the][ Lyapunov equation][,]
_AB + BA[⊺]_ = 2D. (19)
If B solves the Lyapunov equation, notice
_d_

_e[A][(][t][−][T][ )]Be[A][⊺][(][t][−][S][)][]_ = e[A][(][t][−][T][ )]ABe[A][⊺][(][t][−][S][)] + e[A][(][t][−][T][ )]BA[⊺]e[A][⊺][(][t][−][S][)]

_dt_



= e[A][(][t][−][T][ )]2De[A][⊺][(][t][−][S][)]

Using this derivative, the integral expressions for the covariance and cross-covariance simplify as,

Var(XT ) = κ[−][1][ ]B − _e[−][AT]_ _Be[−][A][⊺][T][ ]_ _,_ (20)

Cov(XT, XS) = κ[−][1][ ]B − _e[−][AT]_ _Be[−][A][⊺][T][ ]_ _e[A][⊺][(][T][ −][S][)],_ (21)

where we implicitly assume T ≤ _S._

3Itô’s Lemma states that for any Itô drift-diffusion process dXt = µtdt + σtdWt and twice differentiable

scalar function f (t, x), then df (t, Xt) = _ft + µtfx +_ _σ2t[2]_ _[f][xx]_ _dt + σtfxdWt._

2[]
4Itô Isometry states for any standard Itô process _Xt, then E_  _t0_ _[X][t][dW][t]_ = E _t0_ _[X]t[2][dt]_ .
R  hR i


-----

D.2 DECOMPOSING THE DRIFT MATRIX

While the Lyapunov equation simplifies the expressions for the covariance and cross-covariance, it
does not explain how to actually solve for the unknown matrix B. Following a method proposed by
Kwon et al. [48], we will show how to solve for B explicitly in terms of the drift A and diffusion D.

The drift matrix A can be uniquely decomposed as,
_A = (D + Q)U_ (22)
where D is our symmetric diffusion matrix, Q is a skew-symmetric matrix (i.e. Q = −Q[⊺]), and U is
a positive definite matrix. Using this decomposition, then B = U _[−][1], solves the Lyapunov equation._

_Proof. Plug B = U_ _[−][1]_ into the left-hand side of equation (19),
_AU_ _[−][1]_ + U _[−][1]A[⊺]_ = (D + Q)UU _[−][1]_ + U _[−][1]U_ (D − _Q)_
= (D + Q) + (D − _Q)_
= 2D
Here we used the symmetry of A, D, U and the skew-symmetry of Q.

All that is left is to do is solve for the unknown matrices Q and U . First notice the following identity,
_AD −_ _DA = QA + AQ_ (23)

_Proof. Multiplying A = (D + Q)U on the right by (D −_ _Q) gives,_
_A(D −_ _Q) = (D + Q)U_ (D − _Q)_
= (D + Q)A[⊺],
which rearranged and using A = A[⊺] gives the desired equation.

Let V ΛV [⊺] be the eigendecomposition of A and define the matrices _D = V_ [⊺]DV and _Q = V_ [⊺]QV .
These matrices observe the following relationship,

_Qij =_ _[λ][i][ −]_ _[λ][j]_ _Dij._ [e] [e] (24)

_ρi + λj_

e e

_Proof. Replace A in the previous equality with its eigendecompsoition,_
_V ΛV_ [⊺]D _DV ΛV_ [⊺] = QV ΛV [⊺] + V ΛV [⊺]Q.
_−_
Multiply this equation on the right by V and on the left by V [⊺],


ΛD − _D[e]Λ =_ _QΛ + ΛQ._
Looking at this equality element-wise and using the fact that Λ is diagonal gives the scalar equality
for any i, j, [e] [e] [e]
(λi − _λj)Dij = (λi + λj)Qij,_
which rearranged gives the desired expression.

[e] [e]

Thus, Q and U are given by,

_Q = V_ _QV_ [⊺], _U = (D + Q)[−][1]A._ (25)

This decomposition always holds uniquely when A, D ≻ 0, as _[λ]λ[i]i[−]+λ[λ]j[j]_ [exists and][ (][D][ +][ Q][)][ is invert-]

ible. See [48] for a discussion on the singularities of this decomposition.[e]

D.3 STATIONARY SOLUTION


Using the Lyapunov equation and the drift decomposition, then XT ∼ _pT, where_

_pT = N_ _e[−][AT]_ _x0 +_ _I −_ _e[−][AT][ ]_ _µ, κ[−][1][ ]U_ _[−][1]_ _−_ _e[−][AT]_ _U_ _[−][1]e[−][A][⊺][T][ ]_ _._ (26)

In the limit as T →∞, then e[−][AT]  → 0 and pT → _pss where_
_pss = N_ _µ, κ[−][1]U_ _[−][1][]_ _._ (27)
Similarly, the cross-covariance converges to the stationary cross-covariance,
 
Covss(XT, XS) = κ[−][1]Be[A][⊺][(][T][ −][S][)]. (28)


-----

E A VARIATIONAL FORMULATION OF THE OU PROCESS WITH
ANISOTROPIC NOISE

In this section we will describe an alternative, variational, route towards solving the dynamics of the
OU process studied in appendix D.

Let Φ : R[n] _→_ R be an arbitrary, non-negative potential and consider the stochastic differential
equation describing the Langevin dynamics of a particle in this potential field,

_dXt = −∇Φ(Xt)dt +_ 2κ[−][1]D(Xt)dWt, _X0 = x0,_ (29)

where D(Xt) is an arbitrary, spatially-dependent, diffusion matrix,p _κ is a temperature constant, and_
_xfor the probability distribution0 ∈_ R[m] is the particle’s initial position. The p of the particle’s position such that Fokker-Planck equation p( describes the time evolutionx, t) = P(Xt = x). The FP
equation is the partial differential equation[5],

_∂tp =_ Φ(Xt)p + κ[−][1] (D(Xt)p) _,_ _p(x, 0) = δ(x0),_ (30)
_∇·_ _∇_ _∇·_

where denotes the divergence and  _δ(x0) is a dirac delta distribution centered at the initialization_
_∇·_
_x0. To assist in the exploration of the FP equation we define the vector field,_

_J(x, t) =_ Φ(Xt)p (D(Xt)p), (31)
_−∇_ _−∇·_

which is commonly referred to as the probability current. Notice, that this gives an alternative expression for the FP equation, ∂tp = _J, demonstrating that J(x, t) defines the flow of probability_
_−∇·_
mass through space and time. This interpretation is especially useful for solving for the stationary
_solution pss, which is the unique distribution that satisfies,_

_∂tpss = −∇· Jss = 0,_ (32)

where Jss is the probability current for pss. The stationary condition can be obtained in two distinct
ways:

1. Detailed balance. This is when Jss(x) = 0 for all x Ω. This is analogous to reversibility
_∈_
for discrete Markov chains, which implies that the probability mass flowing from a state i
to any state j is the same as the probability mass flowing from state j to state i.

2. Broken detailed balance. This is when _Jss(x) = 0 but Jss(x)_ = 0 for all x Ω. This
_∇·_ _̸_ _∈_
is analogous to irreversibility for discrete Markov chains, which only implies that the total
probability mass flowing out of state i equals to the total probability mass flowing into state
_i._

The distinction between these two cases is critical for understanding the limiting dynamics of the
process.

E.1 THE VARIATIONAL FORMULATION OF THE FOKKER-PLANCK EQUATION WITH
ISOTROPIC DIFFUSION

We will now consider the restricted setting of standard, isotropic diffusion (D = I). It is easy
enough to check that in this setting the stationary solution is


_pss(x) =_ _[e][−][κ][Φ(][x][)]_


_e[−][κ][Φ(][x][)]dx,_ (33)


_Z =_


where pss is called a Gibbs distribution and Z is the partition function. Under this distribution,
the stationary probability current is zero (Jss(x) = 0) and thus the process is in detailed balance.
Interestingly, the Gibbs distribution pss has another interpretation as the unique minimizer of the the
_Gibbs free energy functional,_
_F_ (p) = E [Φ] − _κ[−][1]H(p),_ (34)
where E [Φ] is the expectation of the potential Φ under the distribution p and H(p) =
_−_ Ω _[p][(][x][)][log][(][p][(][x][))][dx][ is the Shannon entropy of][ p][.]_

5

R This PDE is also known as the Forward Kolmogorov equation.


-----

_Proof. To prove that indeed pss is the unique minimizer of the Gibbs free energy functional, consider_
the following equivalent expression


_p(x)Φ(x)dx + κ[−][1]_


_F_ (p) =


_p(x)log(p(x))dx_


= κ[−][1]


_p(x) (log(p(x))_ _log(pss(x))) dx_ _κ[−][1]_
_−_ _−_


_log(Z)_


= κ[−][1]DKL(p ∥ _pss) −_ _κ[−][1]log(Z)_

From this expressions, it is clear that the Kullback–Leibler divergence is uniquely minimized when
_p = pss._

In other words, with isotropic diffusion the stationary solution pss can be thought of as the limiting
distribution given by the Fokker-Planck equation or the unique minimizer of an energetic-entropic
functional.

Seminal work by Jordan et al. [53] deepened this connection between the Fokker-Planck equation
and the Gibbs free energy functional. In particular, their work demonstrates that the solution p(x, t)
to the Fokker-Planck equation is the Wasserstein gradient flow trajectory on the Gibbs free energy
functional.

Steepest descent is always defined with respect to a distance metric. For example, the update equation,lated as the solution to the minimization problem xk+1 = xk − _η∇Φ(xk), for classic gradient descent on a potential xk+1 = argminxηΦ(x) + Φ(x[1]2)[d], can be formu-[(][x, x][k][)][2][ where]_

_d(x, xk) =_ _x_ _xk_ is the Euclidean distance metric. Gradient flow is the continuous-time limit of
_∥_ _−_ _∥_
gradient descent where we take η → 0[+]. Similarly, Wasserstein gradient flow is the continuous-time
limit of steepest descent optimization defined by the Wasserstein metric. The Wasserstein metric is
a distance metric between probability measures defined as,

_W2[2][(][µ][1][, µ][2][) =]_ inf (35)
_p∈Π(µ1,µ2)_ ZR[n]×R[n][ |][x][ −] _[y][|][2][p][(][dx, dy][)][,]_

where µ1 and µ2 are two probability measures on R[n] with finite second moments and Π(µ1, µ2)
defines the set of joint probability measures with marginals µ1 and µ2. Thus, given an initial distribution and learning rate η, we can use the Wasserstein metric to derive a sequence of distributions
minimizing some functional in the sense of steepest descent. In the continuous-time limit as η → 0[+]
this sequence defines a continuous trajectory of probability distributions minimizing the functional.
Jordan et al. [54] proved, through the following theorem, that this process applied to the Gibbs free
energy functional converges to the solution to the Fokker-Planck equation with the same initialization:
**Theorem 1 (JKO). Given an initial condition p0 with finite second moment and an η > 0, define**
_the iterative scheme pη with iterates defined by_

_pk = argminpη_ E [Φ] − _κ[−][1]H(p)_ + W2[2][(][p, p][k][−][1][)][.]

_As η_ 0[+], then pη _p weakly in L[1] where p is the solution to the Fokker-Planck equation with_
_the same initial condition. →_ _→_

See [54] for further explanation and [53] for a complete derivation.

E.2 EXTENDING THE VARIATIONAL FORMULATION TO THE SETTING OF ANISOTROPIC
DIFFUSION

While the JKO theorem provides a very powerful lens through which to view solutions to the FokkerPlanck equation, and thus distributions for particles governed by Langevin dynamics, it only applies
in the very restricted setting of isotropic diffusion. In this section we will review work by Chaudhari
and Soatto [33] extending the variational interpretation to the setting of anisotropic diffusion.

Consider when D(Xt) is an anisotropic, spatially-dependent diffusion matrix. In this setting, the
original Gibbs distribution given in equation (33) does not necessarily satisfy the stationarity condition equation (32). In fact, it is not immediately clear what the stationary solution is or if the
dynamics even have one. Thus, Chaudhari and Soatto [33] make the following assumption:


-----

**Stationary Assumption. Assume there exists a unique distribution pss that is the stationary solution**
_to the Fokker-Planck equation irregardless of initial conditions._

Under this assumption we can implicitly define the potential Ψ(x) = _κ[−][1]log(pss(x)). Using this_
_−_
modified potential we can express the stationary solution as a Gibbs distribution,

_pss(x)_ _e[−][κ][Ψ(][x][)]._ (36)
_∝_

Under this implicit definition we can define the stationary probability current as Jss(x) =
_j(x)pss(x) where_
_j(x) = −∇Φ(x) −_ _κ[−][1]∇· D(x) + D(x)∇Ψ(x)._ (37)
The vector field j(x) reflects the discrepancy between the original potential Φ and the modified
potential Ψ according to the diffusion D(x). Notice that in the isotropic case, when D(x) = I,
then Φ = Ψ and j(x) = 0. Chaudhari and Soatto [33] introduce another property of j(x) through
assumption,

**Conservative Assumption. Assume that the force j(x) is conservative (i.e. ∇· j(x) = 0).**

Using this assumption, Chaudhari and Soatto [33] extends the variational formulation provided by
the JKO theorem to the anisotropic setting,
**Theorem 2 (CS). Given an initial condition p0 with finite second moment, then the energetic-**
_entropic functional,_
_F_ (p) = Ep [Ψ(x)] − _κ[−][1]H(p)_
_monotonically decreases throughout the trajectory given by the solution to the Fokker-Planck equa-_
_tion with the given initial condition._

In other words, the Fokker-Plank equation (30) with anisotropic diffusion can be interpreted as minimizing the expectation of a modified loss Ψ, while being implicitly regularized towards distributions
that maximize entropy. The derivation requires we assume a stationary solution pss exists and that
the force j(x) implicitly defined by pss is conservative. However, rather than implicitly define
Ψ(x) and j(x) through assumption, if we can explicitly construct a modified loss Ψ(x) such that
the resulting j(x) satisfies certain conditions, then the stationary solution exists and the variational
formulation will apply as well. We formalize this statement with the following theorem,
**Theorem 3 (Explicit Construction). If there exists a potential Ψ(x) such that either j(x) = 0 or**
_∇· j(x) = 0 and ∇Ψ(x) ⊥_ _j(x), then pss is the Gibbs distribution ∝_ _e[−][κ][Ψ(][x][)]_ _and the variational_
_formulation given in Theorem 2 applies._

E.3 APPLYING THE VARIATIONAL FORMULATION TO THE OU PROCESS

Through explicit construction we now seek to find analytic expressions for the modified loss Ψ(x)
and force j(x) hypothesised by Chaudhari and Soatto [33] in the fundamental setting of an OU
process with anisotropic diffusion, as described in section D. We assume the diffusion matrix is
anisotropic, but spatially independent, ∇· D(x) = 0. For the OU process the original potential
generating the drift is
Φ(x) = (x − _µ)[⊺]_ _[A]2_ [(][x][ −] _[µ][)][.]_ (38)

Recall, that in order to extend the variational formulation we must construct some potential Ψ(x)
such that ∇· j(x) = 0 and ∇Ψ ⊥ _j(x). It is possible to construct Ψ(x) using the unique decompo-_
sition of the drift matrix A = (D + Q)U discussed in appendix D. Define the modified potential,

Ψ(x) = (x − _µ)[⊺]_ _[U]2_ [(][x][ −] _[µ][)][.]_ (39)

Using this potential, the force j(x) is

_j(x) = −A(x −_ _µ) + DU_ (x − _µ) = −QU_ (x − _µ)._ (40)

Notice that j(x) is conservative, ∇· j(x) = ∇· −QU (x − _µ) = 0 because Q is skew-symmetric._
Additionally, j(x) is orthogonal, j(x)[⊺] Ψ(x) = (x _µ)[⊺]_ _U_ [⊺]QU (x _µ) = 0, again because Q is_
_∇_ _−_ _−_
skew-symmetric. Thus, we have determined a modified potential Ψ(x) that results in a conservative
orthogonal force j(x) satisfying the conditions for Theorem 3. Indeed the stationary Gibbs distribution given by Theorem 3 agrees with equation (27) derived via the first and second moments in
appendix D,
_e[−][κ][Ψ(][x][)]_ _∝N_ _µ, κ[−][1]U_ _[−][1][]_
 


-----

In addition to the variational formulation, this interpretation further details explicitly the stationary
probability current, Jss(x) = j(x)pss, and whether or not the the stationary solution is in broken
detailed balance.

F EXPLICIT EXPRESSIONS FOR THE OU PROCESS GENERATED BY SGD

We will now consider the specific OU process generated by SGD with linear regression. Here we
repeat the setup as explained in section 5.

Let X R[N] _[×][d], Y_ R[N] be the input data, output labels respectively and θ R[d] be our vector of
_∈_ _∈_ _∈_ 1
regression coefficients. The least squares loss is the convex quadratic loss (θ) = 2N

with gradient g(θ) = Hθ − _b, where H =_ _XN[⊺]X_ and b = _XN[⊺]Y_ [. Plugging this expression for] L _[∥][Y][ −]_ _[Xθ][∥][2]_

the gradient into the underdamped Langevin equation (3), and rearranging terms, results in the
multivariate Ornstein-Uhlenbeck (OU) process,

_d_ _vθtt_ = A _µ0_ _−_ _vθtt_ _dt +_ _√2κ[−][1]DdWt,_ (41)
     

where A and D are the drift and diffusion matrices respectively,

_A =_ _η(1+2_ _β)_ [(]0[H][ +][ λI][)] _η2(1(1+−−Iββ))_ _[I]_ _,_ _D =_ 00 _η2(1(1+−ββ0))_ [Σ(][θ][)] _,_ (42)
   

_κ = S(1 −_ _β[2]) is a temperature constant, and µ = (H + λI)[−][1]b is the ridge regression solution._

F.1 SOLVING FOR THE MODIFIED LOSS AND CONSERVATIVE FORCE

In order to apply the expressions derived for a general OU process in appendix D and E, we must
first decompose the drift as A = (D + Q)U . Under the simplification Σ(θ) = σ[2]H discussed in
appendix B, then the matrices Q and U, as defined below, achieve this,


2

_η(1+β)σ[2][ H]_ _[−][1][ (][H][ +][ λI][)]_


0 _σ[2]H_
_Q =_ _−_
_σ[2]H_ 0



(43)


_U =_


1

_σ[2][ H]_ _[−][1]_


Using these matrices we can now derive explicit expressions for the modified loss Ψ(θ, v) and conservative force j(θ, v). First notice that the least squares loss with L2 regularization is proportional
to the convex quadratic,
Φ(θ) = (θ − _µ)[⊺](H + λI)(θ −_ _µ)._ (44)
The modified loss Ψ is composed of two terms, one that only depends on the position,

_H_ _−1(H + λI)_
Ψθ(θ) = (θ _µ)[⊺]_ (θ _µ),_ (45)
_−_ _η(1 + β)σ[2]_ _−_
 

and another that only depends on the velocity,

_H_ _−1_
Ψv(v) = v[⊺] _v._ (46)

_σ[2]_

 

The conservative force j(θ, v) is


(47)


_j(θ, v) =_ 2 _,_
− _η(1+β)_ [(][H][ +][ λI][) (][θ][ −] _[µ][)]_

and thus the stationary probability current is Jss(θ, v) = j(θ, v)pss.

F.2 DECOMPOSING THE TRAJECTORY INTO THE EIGENBASIS OF THE HESSIAN


As shown in appendix D, the temporal distribution for the OU process at some time T ≥ 0 is,

_pT_ _θ_ = _e[−][AT]_ _θ0_ + _I_ _e[−][AT][  ]µ_ _, κ[−][1][ ]U_ _e[−][AT]_ _U_ _e[−][A][⊺][T][ ]_ _._

_v_ _N_ _v0_ _−_ 0 _[−][1]_ _−_ _[−][1]_
     

 

Here we will now use the eigenbasis _q1, . . ., qm_ of the Hessian with eigenvalues _ρ1, . . ., ρm_ to
_{_ _}_ _{_ _}_
derive explicit expressions for the mean and covariance of the process through time.


-----

**Deterministic component. We can rearrange the expectation as**

E _vθ_ = _µ0_ + e[−][AT] _θ0 −v0_ _µ_ _._
     

Notice that the second, time-dependent term is actually the solution to the system of ODEs

˙
_θ_ _θ_

= _A_

_v_ _−_ _v_
   

with initial condition [θ0 _µ_ _v0][⊺]. This system of ODEs can be block diagonalized by factorizing_
_A = OSO[⊺]_ where O is orthogonal and − _S is block diagonal defined as_


0 _−1_

2 2(1−β) ...

_η(1+β)_ [(][ρ][1][ +][ λ][)] _η(1+β)_
... ...
2 0 2(1−−1β)

_η(1+β)_ [(][ρ][m][ +][ λ][)] _η(1+β)_


_q1_ 0 _. . ._ _qm_ 0
...
0 _q1_ _. . ._ 0 _qm_


_O =_


_S =_


In otherwords in the plane spanned by [qi 0][⊺] and [0 _qi][⊺]_ the system of ODEs decouples into the
2D system
_ab˙ii_ = _η(1+2_ _β)0[(][ρ][i][ +][ λ][)]_ _η(1+1_ _β)_ _abii_
  − _−_ [2(1][−][β][)]   

This system has a simple physical interpretation as a damped harmonic oscillator. If we let bi = ˙ai,
then we can unravel this system into the second order ODE


_a¨i + 2 [1][ −]_ _[β]_ _ai +_

_η(1 + β) [˙]_


_η(1 + β)_ [(][ρ][i][ +][ λ][)][a][i][ = 0]


which is in standard form (i.e. ¨x + 2γ ˙x + ω[2]x = 0) for γ = _η(1+1−ββ)_ [and][ ω][i][ =] _η(1+2_ _β)_ [(][ρ][i][ +][ λ][)][.]

Let ai(0) = ⟨θ0 − _µ, qi⟩_ and bi(0) = ⟨v0, qi⟩, then the solution in terms of γ andq ωi is

_e[−][γt]_ _ai(0) cosh_ _γ[2]_ _−_ _ωi[2][t]_ + _[γa]√[i][(0)+]γ[2]−[b]ω[i]i[2][(0)]_ sinh _γ[2]_ _−_ _ωi[2][t]_ _γ > ωi_

_ai(t) =_ ee[−][−][γt][γt](aai(0) + (i(0) cosγaip(0) +ωi[2] _bi(0))t+)_ _[γa]√[i][(0)+]ωi[2]_ _[b][i][(0)]_ sin pωi[2] [] _γ < ωγ = ωii_

Differentiating these equations gives us solutions for  p _[−]_ _[γ][2][t]_ _bi([−]t)[γ][2]_ p _[−]_ _[γ][2][t][]_

_e[−][γt]_ _bi(0) cosh_ _γ[2]_ _−_ _ωi[2][t]_ _−_ _[ω]i[2][a]√[i][(0)+]γ[2]−[γb]ωi[2][i][(0)]_ sinh _γ[2]_ _−_ _ωi[2][t]_ _γ > ωi_

Combining all these results, we can now analytically decompose the expectation as the sum,bi(t) = ee[−][−][γt][γt][  ]bbii(0)(0) cos −  ωpi[2]p[a][i]ω[(0) +]i[2] _[−]_ _[γ][ γb][2][t][i][(0)]−[ω]ti[2][a]√[i][(0)+]ωi[2][−][γb][γ][2][i][(0)]_ sin ppωi[2] _[−]_ _[γ][2][t][][]_ _γ < ωγ = ωii_








0
+ bi(t) _qi_
 


_ai(t)_ _qi_



_i=1_


Intuitively, this equation describes a damped rotation (spiral) around the OLS solution in the planes
defined by the the eigenvectors of the Hessian at a rate proportional to the respective eigenvalue.


-----

**Stochastic component. Using the previous block diagonal decomposition A = OSO[⊺]** we can
simplify the variance as

_θ_
Var _v_ = κ[−][1][ ]U _[−][1]_ _−_ _e[−][AT]_ _U_ _[−][1]e[−][A][⊺][T][ ]_
 

= κ[−][1][ ]U _e[−][OSO][⊺][T]_ _U_ _e[−][OS][⊺][O][⊺][T][ ]_

_[−][1]_ _−_ _[−][1]_

= κ[−][1]O _O[⊺]U_ _[−][1]O −_ _e[−][ST]_ (O[⊺]U _[−][1]O)e[−][ST][ ⊺][]_ _O[⊺]_


Interestingly, the matrix O[⊺]U _[−][1]O is also block diagonal,_


_η(1+β)σ[2]_


_ρ1_

_ρ1+λ_


_σ[2]ρ1_ ...
... ...

_η(1+β)σ[2]_


2
_η(1+β)σ_
_O[⊺]U_ _[−][1]O = O[⊺]_ 2



(H + λI)[−][1] _H_ 0
0 _σ[2]H_


_O =_


_ρmρm+λ_ 0
_σ[2]ρm_


Thus, similar to the mean, we can simply consider the variance in each of the planes spanned by

[qi 0][⊺] and [0 _qi][⊺]. If we define the block matrices,_


_ησ[2]_ _ρi_

2S(1−β) _ρi+λ_


_Di =_


_Si =_


_Di =_ _−_ _i_ _σ[2]_ _Si =_ 2

0 _S(1−β[2])_ _[ρ][i]_ − _η(1+β)_ [(][ρ][i][ +][ λ][)] _−_ _η[2(1](1+[−][β]β[)])_ 

then the projected variance matrix in this plane simplifies as

_qi[⊺][θ]_
Var _qi[⊺][v]_ = Di − _e[−][S][i][T]_ _Die[−][S][i][T][ ⊺]_
 

Using the solution to a damped harmonic osccilator discussed previously, we can express the matrix
exponential e[−][S][i][T] explicitly in terms of γ = _η(1+1−ββ)_ [and][ ω][i][ =] _η(1+2_ _β)_ [(][ρ][i][ +][ λ][)][. If we let][ α][i][ =]

_γ[2]_ _ωi[2]_ q
_|_ _−_ _[|][, then the matrix exponential is]_

p cosh (αit) + _α[γ]i_ [sinh (][α][i][t][)] _α1i_ [sinh (][α][i][t][)]

e[−][γt] " _−_ _[ω]αi[2]i_ [sinh (][α][i][t][)] cosh (αit) − _α[γ]i_ [sinh (][α][i][t][)]# _γ > ωi_

_e[−][S][i][t]_ = e[−][γt] 1 +−ω γti[2][t] 1 −t _γt_ 1 _γ = ωi_

cos (αit) + _α[γ]i_ [sin (][α][i][t][)] _αi_ [sin (][α][i][t][)]

e[−][γt] " _−_ _[ω]αi[2]i_ [sin (][α][i][t][)] cos (αit) − _α[γ]i_ [sin (][α][i][t][)]# _γ < ωi_


-----

G ANALYZING PROPERTIES OF THE STATIONARY SOLUTION

Assuming the stationary solution is given by equation (??) we can solve for the expected value of
the norm of the local displacement and gain some intuition for the expected value of the norm of
global displacement.

G.1 INSTANTANEOUS SPEED

Ess _∥δk∥[2][]_ = Ess _∥θk+1 −_ _θk∥[2][]_
 = η[2]Ess _vk+1_

_∥_ _∥[2][]_

= η[2]tr Ess _vk+1vk[⊺]+1_

= η[2]tr (Var  _ss_ (vk+1) + Ess [vk+1] Ess [vk+1][⊺])

= η[2]tr _κ[−][1]U_ _[−][1][]_

_η[2]_

 

= _σ[2]H_

_S(1_ _β[2])_ [tr]
_−_
  


Note that this follows directly from the definition of δk in equation (1) and the mean and variance of
the stationary solution in equation ( ??), as well as the follow-up derivation in appendix F.

G.2 ANOMALOUS DIFFUSION

Notice, that the global movement ∆t = θt−θ0 can be broken up into the sum of the local movements
∆t = _i=1_ _[δ][i][, where][ δ][i][ =][ θ][i][ −]_ _[θ][i][−][1][. Applying this decomposition,]_

_t_ 2[]

[P][t] Ess ∆t = Ess _δi_

_∥_ _∥[2][]_ 

_i=1_

 X

_t_   _t_

= Ess _∥δi∥[2][]_ + Ess [⟨δi, δj⟩]

Xi=1  Xi≠ _j_

As we solved for previously,

_η[2]_
Ess _δi_ = η[2]Ess _vi_ = η[2]tr (Varss(vi)) = _σ[2]H_ _._
_∥_ _∥[2][]_ _∥_ _∥[2][]_ _S(1_ _β[2])_ [tr]

_−_

    

By a similar simplification, we can express the second term in terms of the stationary crosscovariance,
Ess [⟨δi, δj⟩] = η[2]Ess [⟨vi, vj⟩] = η[2]tr (Covss(vi, vj)) .
Thus, to simplify this expression we just need to consider the velocity-velocity covariance
Covss(vi, vj). At stationarity, the cross-covariance for the system in phase space, zi = [θi _vi]_
is
Covss(zi, zj) = κ[−][1]U _e[−][A][⊺][|][i][−][j][|]_

_[−][1]_

where κ = S(1 − _β[2]), and_

_U =_ _η(1+2β)σ[2][ H]_ _[−]0[1][ (][H][ +][ λI][)]_ _σ1[2][ H]0_ _[−][1]_ _A =_ _η(1+2_ _β)_ [(]0[H][ +][ λI][)] _η2(1(1+−−Iββ))_ _[I]_
   

As discussed when solving for the mean of the OU trajectory, the drift matrix A can be block
diagonalized as A = OSO[⊺] where O is orthogonal and S is block diagonal defined as


0 _−1_

2 2(1−β) ...

_η(1+β)_ [(][ρ][1][ +][ λ][)] _η(1+β)_
... ...
2 0 2(1−−1β)

_η(1+β)_ [(][ρ][m][ +][ λ][)] _η(1+β)_


_q1_ 0 _. . ._ _qm_ 0
...
0 _q1_ _. . ._ 0 _qm_


_O =_


_S =_


-----

Notice also that O diagonalizes U _[−][1]_ such that,


_η(1+β)σ[2]_


_ρ1_

_ρ1+λ_


_σ[2]ρ1_ ...
... ...

_η(1+β)σ[2]_


Λ = O[⊺]U _[−][1]O =_


_ρmρm+λ_ 0
_σ[2]ρm_


Applying these decompositions, properties of matrix exponentials, and the cyclic invariance of the
trace, allows us to express the trace of the cross-covariance as

tr (Covss(zi, zj)) = κ[−][1]tr _U_ _e[−][A][⊺][|][i][−][j][|][]_

_[−][1]_


= κ[−][1]tr _U_ _Oe[−][S][⊺][|][i][−][j][|]O[⊺][]_

_[−][1]_


= κ[−][1]tr Λe[−][S][⊺][|][i][−][j][|][]

_n_
= κ[−][1] tr Λke[−][S]k[⊺][|][i][−][j][|][]

_k=1_

X 


where Λk and Sk are the blocks associated with each eigenvector of H. As solved for previously in
the variance of the OU process, we can express the matrix exponential e[−][S][k][|][i][−][j][|] explicitly in terms
of γ = _η(1+1−ββ)_ [and][ ω][k][ =] _η(1+2_ _β)_ [(][ρ][k][ +][ λ][)][. If we let][ τ][ =][ |][i][ −] _[j][|][ and][ α][k][ =]_ _|γ[2]_ _−_ _ωk[2][|][, then the]_

matrix exponential is q p


cosh (αkτ ) + _αγk_ [sinh (][α][k][τ] [)] _α1k_ [sinh (][α][k][τ] [)]
_e[−][γτ]_ " _−_ _α[ω]kk[2]_ [sinh (][α][k][τ] [)] cosh (αkτ ) − _αγk_ [sinh (][α][k][τ] [)]

1 + γτ _τ_
_e[−][γτ]_
_ωk[2][τ]_ 1 _γτ_
cos (− _αkτ_ ) + −αγk [sin (] _[α][k][τ]_ [)] _α1k_ [sin (][α][k][τ] [)]

_e[−][γτ]_ " _−_ _α[ω]kk[2]_ [sin (][α][k][τ] [)] cos (αkτ ) − _αγk_ [sin (][α][k][τ] [)]#


_γ > ωk_

_γ = ωk_

_γ < ωk_


_e[−][S][k][|][i][−][j][|]_ =


Plugging in these expressions into previous expression and restricting to just the k[th] velocity component, we see


_κ[−][1]σ[2]ρke[−][γτ][ ]cosh (αkτ_ ) − _αγk_ [sinh (][α][k][τ] [)] _γ > ωk_

_κ[−][1]σ[2]ρke[−][γτ]_ (1 _γτ_ )  _γ = ωk_
_−_
_κ[−][1]σ[2]ρke[−][γτ][ ]cos (αkτ_ ) − _αγk_ [sin (][α][k][τ] [)] _γ < ωk_




Covss(vi,k, vj,k) = κ[−][1][ h]Λke[−][S]k[⊺][|][i][−][j][|][i]

1,1 [=]

Pulling it all together,


_m_

_ρlCl(k)_

 _l=1_
X


_η[2]σ[2]_
Ess ∆t =
_∥_ _∥[2][]_ _S(1_ _β[2])_

_−_



where Cl(k) is defined as


1
_−_ _[k]t_


tr (H) t + 2t


_k=1_


_e[−][γk][ ]cosh (αlk) −_ _α[γ]l_ [sinh (][α][l][k][)] _γ > ωl_

Cl(k) = e[−][γk] (1 _γk)_  _γ = ωl_

e[−][γk][ ]cos ( − _αlk) −_ _α[γ]l_ [sin (][α][l][k][)] _γ < ωl_

1 _β_ 2  
for γ = _η(1+−_ _β)_ [,][ ω][l][ =] _η(1+β)_ [(][ρ][l][ +][ λ][)][, and][ α][l][ =] _|γ[2]_ _−_ _ωl[2][|][.]_

q p


-----

G.3 TRAINING LOSS AND EQUIPARTITION THEOREM

In addition to solving for the expected values of the local and global displacements, we can consider
the expected training loss and find an interesting relationship to the equipartition theorem from
classical statistical mechanics.

The regularized training loss is _λ(θ) =_ [1]2 [(][θ][ −] _[µ][)][⊺][H][(][θ][ −]_ _[µ][) +][ λ]2_

matrix and µ is the true mean. Taking the expectation with respect to the stationary distribution, L _[∥][θ][∥][2][, where][ H][ is the Hessian]_

Ess [ _λ(θ)] = [1]_
_L_ 2 [tr ((][H][ +][ λI][)][E][ss][ [][θθ][⊺][])][ −] _[µ][⊺][H][E][ss][ [][θ][] + 1]2_ _[µ][⊺][Hµ]_


The first and second moments of the stationary solution are

_η_ _σ[2]_
Ess [θ] = µ Ess [θθ[⊺]] =

_S(1_ _β)_ 2 [(][H][ +][ λI][)][−][1][H][ +][ µµ][⊺]
_−_

Plugging these expressions in and canceling terms we get


_η_
Ess [ _λ(θ)] =_ _σ[2]H_ + _[λ]_
_L_ 4S(1 _β)_ [tr] 2

_−_ _[∥][µ][∥][2]_

Define the kinetic energy of the network as K(v) = 12 [m][∥][v][∥][2][, where][ m][ =] _η2_ [(1 +][ β][)][ is the per-]
parameter “mass" of the network according to our previously derived Langevin dynamics. At stationarity,

_η_

Ess [ (v)] = _[η][(1 +][ β][)]_ tr (Ess [vv[⊺]]) = _σ[2]H_
_K_ 4 4S(1 _β)_ [tr]

_−_

1   
where we used the fact that Ess [vv[⊺]] = _S(1_ _β[2])_ _[σ][2][H][. In otherwords, at stationarity,]_

_−_

Ess [ _λ(θ)] = Ess [_ (v)] + _[λ]_
_L_ _K_ 2 _[∥][µ][∥][2][.]_

This relationship between the expected potential and kinetic energy can be understood as a form of
the equipartition theorem.


-----

H EXPERIMENTAL DETAILS

H.1 COMPUTING THE HESSIAN EIGENDECOMPOSITION

Computing the full Hessian of the loss with respect to the parameters is computationally intractable
for large models. However, equipped with an autograd engine, we can compute Hessian-vector products. We use the subspace iteration on Hessian-vector products computed on a variety of datasets.
For Cifar-10 we use the entire train dataset to compute the Hessian-vector products. For Imagenet,
we use a subset 40,000 images sampled from the train dataset to keep the computation within reasonable limits.

For experiments on linear regression, the Hessian is independent of the model (it only depends on
the data) and can be computed using any model checkpoint. For all other experiments, the Hessian
eigenvectors were computed using the model at its initial pre-trained state.

H.2 FIGURE 1

We resumed training for a variety of ImageNet pre-trained models from Torchvision [16] for 10
epochs, with the hyperparameters used at the end of training, shown in table 1.

We kept track of the norms of the local and global displacement, ∥δk∥2[2] [and][ ∥][∆][k][∥]2[2][, every 250]
steps in the training process, to keep the length of the trajectories within reasonable limits. ∥δk∥2[2] [is]
visualized directly, along with its 15 step moving average. We then fit a power law of the form αk[c]
to the ∥∆k∥2[2] [trajectories for each model, using the last 2/3 of the saved trajectories. We visualize]
the ∥∆k∥2[2] [trajectories along with their fits on a log-log plot.]

|Model|Dataset|Opt.|Epochs|Batch size S|LR η|Mom. β|WD λ|
|---|---|---|---|---|---|---|---|

|VGG-16 VGG-11 w/BN VGG-16 w/BN ResNet-18 ResNet-34|ImageNet ImageNet ImageNet ImageNet ImageNet|SGDM SGDM SGDM SGDM SGDM|10 10 10 10 10|256 256 256 256 256|10−5 10−5 10−5 10−4 10−4|0.9 0.9 0.9 0.9 0.9|5 × 10−4 5 × 10−4 5 × 10−4 10−4 10−4|
|---|---|---|---|---|---|---|---|


Table 1: Figure 1 experiments training hyperparameters.

H.3 FIGURE 2

For this figure we trained a linear regression model on Cifar-10, using MSE loss on the one-hot
encoded labels. The hyperparameters used during training are shown in table 2.

At every step in training the full set of model weights and velocities were stored. The top 30 eigenvectors of the hessian were computed as described in appendix H.1, using 10 subspace iterations.

The saved weight and velocity trajectories were then projected onto the top eigenvector of the hessian
and were visualized in black. Using the initial weights and velocities, the red trajectories were
computed according to equation (5).

|Model|Dataset|Opt.|Epochs|Batch size S|LR η|Mom. β|WD λ|
|---|---|---|---|---|---|---|---|


|Linear Regression Linear Regression|Cifar-10 Cifar-10|SGDM SGDM|4 4|512 512|10−5 10−5|0.9 0.99|0 0|
|---|---|---|---|---|---|---|---|



Table 2: Figure 2 experiments training hyperparameters.

H.4 FIGURE 3

For this figure we constructed an arbitrary Ornstein-Uhlenbeck process with anisotropic noise which
would help contrast the original and modified potentials. We sampled from a 2 dimensional OU
process of the form


-----

= A _b −_ _xx12_ _dt + 10[−][4][√]DdWt._
  


_x1_
_x2_


where we set b = [−0.1, 0.05][⊺] and arbitrarily construct A such that it’s eigenvectors are aligned
with q1 = [−1, 1][⊺] and q2 = [1, 1][⊺] and it’s eigenvalues are 4 and 1 as follows:


1
_V =_ _−_
1



_A = V_ _[−][1]DV_


_D =_


The background for the left panel was computed from the convex quadratic potential Φ(x) =
1
2 _[x][⊺][Ax][ −]_ _[bx][. The background for the right panel was computed from the modified quadratic]_
Ψ(x) = 12 _[x][⊺][Ux][ −]_ _[ux][, with][ U][ = (][D][ +][ Q][)][−][1][A][ and][ u][ =][ UA][−][1][b][ (see equation (25)). Both]_
were sampled in a regular 40 × 40 grid in [−0.1, 0.1] × [−0.1, 0.1].

H.5 FIGURES 4, 5, AND 6

Starting with the ImageNet pre-trained ResNet-18 from Torchvision [16], we resumed training for
5 epochs with the hyperparameters used at the end of training, shown in table3. The top 30 Hessian
eigenvectors were computed as described in appendix H.1, using 10 subspace iterations. During
training, we tracked the projection of the weights and velocities onto eigenvectors q1 and q30.

For Figure 3, we show the projection of the position trajectory onto eigenvectors q1 and q30 in 2D in
black. The background for the left and center panels was computed taking the ImageNet pre-trained
ResNet-18 from Torchvision [16] and perturbing its weights in the q1 and q30 directions in a region
close to the projected trajectory. The training and test loss were computed for a grid of 20 × 20
perturbed models. The background for the right panel was computed according to equation (10).

|Model|Dataset|Opt.|Epochs|Batch size S|LR η|Mom. β|WD λ|
|---|---|---|---|---|---|---|---|


|ResNet-18|ImageNet|SGDM|5|256|10−4|0.9|10−4|
|---|---|---|---|---|---|---|---|



Table 3: Figures 4,5,6 experiments training hyperparameters.

H.6 FIGURE 7

We resumed training for an ImageNet pre-trained ResNet-18 from Torchvision [16] for 2 epochs,
using the sweeps of hyperparameters shown in table 4. We indicate a sweep in a particular hyperparameter by [A, B], which denotes 20 evenly spaced numbers between A and B, inclusive.

We kept track of the norms of the local and global displacement, ∥δk∥2[2] [and][ ∥][∆][k][∥]2[2][, every step in]
the training process. The value for ∥δk∥2[2] [at the end of the two epochs is shown in the top row of]
the figure. We fitted power law of the form αk[c] to the ∥∆k∥2[2] [trajectories for each model on the full]
trajectories. The fitted exponent c for each model is plotted in the bottom row of the figure.

|Model|Dataset|Opt.|Epochs|Batch size S|LR η|Mom. β|WD λ|
|---|---|---|---|---|---|---|---|


|ResNet-18 ResNet-18 ResNet-18|ImageNet ImageNet ImageNet|SGDM SGDM SGDM|2 2 2|[32, 1024] 256 256|10−4 [10−3, 10−5] 10−4|0.9 0.9 [0.8, 0.99]|10−4 10−4 10−4|
|---|---|---|---|---|---|---|---|



Table 4: Figure 7 experiments training hyperparameters.

H.7 INCREASING RATE OF ANOMALOUS DIFFUSION

Upon further experimentation with the fitting procedure for the rate of anomalous diffusion explained in Figure 1, we observed an interesting phenomenon. The fitted exponent c for the power
law relationship ∆k 2
_∥_ _∥[2]_ _[∝]_ _[k][c][ increases as a function of the length of the trajectory we fit to. As can]_


-----

be seen in Figure 7, c increases at a diminishing rate with the length of the trajectory. This could
be indicative of ∥∆k∥2[2] [being governed by a sum of power laws where the leading term becomes]
dominant for longer trajectories.

|Col1|50 epochs c = 1.190|
|---|---|


20 epochs 30 epochs 40 epochs

**c = 1.069** **c = 1.117** **c = 1.157**

50 epochs 60 epochs 70 epochs

**c = 1.190** **c = 1.219** **c = 1.245**

Diffusion c

80 epochs 90 epochs 100 epochs

**c = 1.268** **c = 1.288** **c = 1.306**

Step Epochs


Figure 7: The fitted rate of anomalous diffusion increases with the length of trajectory fitted.
The left panel shows the fitted power law on training trajectories of increasing length from the pretrained ResNet-18 model. The right panel shows the fitted exponent c as a function of the length of
the trajectory.


-----

