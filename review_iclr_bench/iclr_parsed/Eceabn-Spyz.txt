# GENERALIZABLE LEARNING TO OPTIMIZE INTO WIDE VALLEYS

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Learning to optimize (L2O) has gained increasing popularity in various optimization tasks, since classical optimizers usually require laborious, problem-specific
design and hyperparameter tuning. However, current L2O approaches are designed for fast minimization of the objective function value (i.e., training error), hence often suffering from poor generalization ability such as in training
deep neural networks (DNNs), including (i) disappointing performance across unseen optimizees (optimizer generalization); (ii) unsatisfactory test-set accuracy of
trained DNNs (optmizee generalization). To overcome the limitations, this paper
introduces flatness-aware regularizers into L2O for shaping the local geometry
of optimizee’s loss landscape. Specifically, it guides optimizee to locate wellgeneralizable minimas in large flat regions of loss surface, while tending to avoid
sharp valleys. Such optimizee generalization abilities of flatness-aware regularizers have been proved theoretically. Extensive experiments consistently validate
the effectiveness of our proposals with substantially improved generalization on
multiple sophisticated L2O models and diverse optimizees. Our theoretical and
empirical results solidify the foundation for L2O’s practically usage. All codes
and pre-trained models will be shared upon acceptance.

1 INTRODUCTION


One cornerstone of deep learning’s success is arguably the stochastic gradient-based optimization
methods, such as SGD (Robbins & Monro, 1951),
Adam (Kingma & Ba, 2014), AdaGrad (Duchi et al.,
2011), RProp (Riedmiller & Braun, 1993), and RMSProp (Tieleman & Hinton, 2012). The performance
of deep neural networks (DNNs) hinges on the choice
of optimization methods and the corresponding parameter settings. Thus, intensive human labor is often
required to empirically select the best optimization
method and its parameters for each specific problem.


**Update**

**Rule**


A seemingly promising data-driven approach, learn
|Col1|Flatness-aware Regularizer|
|---|---|
|Learnable Optimizer ate Tr le Dy Optimizee NNs||


Figure 1: The pipeline illustration of our pro
ing to optimize (L2O), arose from the meta learning
community to alleviate this issue. It aims to replace posed Flatness-aware regularizer in L2O. Loss
traditional optimizers tuned by human hand with neu- plots are credits to Li et al. (2018).
ral network based optimizers that can learn update rules from data. Existing works have demonstrated that a learned optimizer is not only able to decrease the objective function faster, but also
able to tremendously reduce the required human labor. Andrychowicz et al. (2016a) first proposed
to parameterize the update rules using a long short-term memory (LSTM) network. The LSTM
optimizer tries to simulate the behavior of iterative methods by unrolling. By aggregating the loss
of the function to be optimized (optimizee) at each time step, it aims to minimize the overall loss
along the optimization path. Wichrowska et al. (2017) enlarged the optimizer neural network to a
hierarchical recurrent neural network (RNN) architecture, which improves its capability on larger
or unseen optimization problems. Li & Malik (2016) also proposed a reinforcement learning based
approach for this sub-field.


-----

Although L2O methods are able to achieve better performance than analytical optimizers in many
traditional optimization tasks, they are yet mature to serve as practical optimizers for deep neural
networks. The main reason is that all existing L2O methods are designed for solving traditional
optimization problems where the generalization ability is not a concern. Generalization ability, one
of the core problems in machine learning, is neither guaranteed for deep neural networks optimized
by L2O, nor studied by literature in the context of L2O before. It has been shown that for today’s
heavily overparameterized networks, it is easy to memorize the entire training data which lead to
zero training loss (Zhang et al., 2021). Therefore, for a L2O method, fast minimization of the
training error cannot really lead to generalization ability of the optimizee.

The generalization ability of machine learning methods has been extensively studied both theoretically and empirically in the past decades (Keskar et al., 2017; Hochreiter & Schmidhuber, 1997;
1994; Jiang et al., 2020; Chaudhari et al., 2017; Damian et al., 2021). Among them, a prevailing theory that links the generalization ability of models to the geometry of the loss landscape has recently
shown its empirical effectiveness. Most methods taking this idea measure the geometry of the loss
landscape using Hessian spectrum, while some others use the local entropy (Damian et al., 2021;
Chaudhari et al., 2017). Further, Foret et al. (2021) proposes the SAM method, which minimizes the
loss value and the loss sharpness simultaneously. Specifically, an optimization method that prefers
to converge to wide valleys (i.e., flat basin) in loss landscape shows better generalization ability.

Inspired by this, we propose flatness-aware regularizers i.e., Hessian regularizer and Entropy regularizer for learning to optimize. The Hessian regularization is the pseudonorm of the optimizee’s
Hessian matrix and the Entropy regularization measures the local Entropy of the optimizee. Both of
them characterize the geometry of the loss landscape which intends to teach optimizer favor update
rules that lead to wide valleys in loss landscape. To summarize, the contributions of this paper can
be outlined below:

-  We propose to use the flatness-aware regularizers in the training of L2O optimizers. The
update rules learned with such regularizers would favor converging to wide valleys in loss
landscape when minimizing loss functions. Thus, L2O optimizers trained with flatness_aware regularizers can be deemed as a plug-and-play optimizer that favors generalization_
and requires no more time calculating Hessian or Entropy information while in use.

-  We demonstrate theoretically that adopting flatness-aware regularizers in L2O can enhance
the generalization ability of optimizees trained by regularized optimizers. Note that the theoretical result of Entropy regularizer implies Entropy-SGD favors wide valleys in original
loss landscape rather than only in Entropy energy landscape.

-  Comprehensive experiments over various tasks demonstrate the effectiveness of our methods. Empirical results show that our methods significantly improve the generalization ability of existing L2O methods, enabling them to outperform current state-of-the-art by a large
margin.

2 RELATED WORK

**Learning to Optimize (L2O)** As a special case of learning to learn, L2O has been widely investigated in various machine learning problems, including black-box optimization (Chen et al., 2017),
Bayesian swarm optimization (Cao et al., 2019), minmax optimization (Shen et al., 2021), domain
adaptation (Li et al., 2020; Chen et al., 2020b), adversarial training (Jiang et al., 2018; Xiong &
Hsieh, 2020), graph learning (You et al., 2020), and noisy label training (Chen et al., 2020c). The
first L2O framework dates back to Andrychowicz et al. (2016b), in which the gradients and update
rules of optimizee are formulated as the input features and outputs for a RNN optimizer, respectively.
It proposes a coordinate-wise design which enables trained L2O to be applicable for different neural
networks with diverse amount of parameters. Li & Malik (2016) proposes an alternative reinforcement learning frameworks for L2O, leveraging gradient history and objective values as observations
and step vectors as actions. Later on, more advanced variants arise to power up the generalization ability of L2O. For example, (i) regularizers like random scaling, objective convexifying (Lv
et al., 2017), and Jacobian regularization (Li et al., 2020), (ii) enhanced L2O model like hierarchical
RNN architecture (Wichrowska et al., 2017), and (iii) improved training techniques like curriculum
learning and imitation learning (Chen et al., 2020a). A more comprehensive literature is referred to
a recent survey paper of learning to optimize.


-----

**Flatness on Generalization of Neural Network** Generalization analysis of neural networks has
been widely studied by various methods, including VC-dimension (Bartlett et al., 2019), covering
number (Bartlett et al., 2017), stability (Hardt et al., 2016; Zhou et al., 2018a), Rademacher complexity (Golowich et al., 2018; Ji & Liang, 2018; Ji et al., 2021; Arora et al., 2018; 2019), etc. In
particular, the landscape flatness has been known to be associated with better generalization. On
the empirical side, Keskar et al. (2017) and He et al. (2019) showed that minima in wide valleys
often generalize better than those in sharp basins. Further, Wilson et al. (2017) and Keskar & Socher
(2017) showed empirically that SGD favors better generalization solutions than Adam. On the theory side, Zhou et al. (2020) showed that SGD is more unstable at sharp minima than Adam and
explained why SGD generalize better than Adam theoretically and Zou et al. (2021) explained that
the inferior generalization performance of Adam is connected to nonconvex loss landscape. To
improve the generalization performance, Entropy-SGD was introduced in Chaudhari et al. (2017)
which was shown to outperform SGD in terms of the generalization error and the training time.
Meanwhile, the spectral norm regularization has been proposed in Yoshida & Miyato (2017) to
improve the generalization ability of neural networks empirically.

3 METHODOLOGY

In this section, we provide basic notations and the detailed formulations about our flatness-aware
regularizers, i.e., Hessian and Entropy regularizers.

3.1 PRELIMINARY

We define F [(1)](θ; ξ) and F [(2)](θ; ζ) respectively as the non-negative meta-traning and meta-testing
functions, where θ ∈ R[p] is the optimizee parameter, and ξ and ζ respectively denote training and
testing data samples. Suppose there are N training data samples ξ _ξi, i = (1, . . ., N_ ) and M
_∈{_ _}_
testing data samples ζ _ζj, j = (1, . . ., M_ ) . Then we define the empirical meta-training and
_∈{_ _}_
meta-testing functions and their corresponding population risk functions as follows:


(Meta-Training) L[(1)]N [(][θ][) = 1]

_N_

(Meta-Testing) L[(2)]M [(][θ][) = 1]

_M_


_F_ [(1)](θ; ξi), _L[(1)](θ) = EξF_ [(1)](θ; ξ), (1)
_i=1_

X

_M_

_F_ [(2)](θ; ζj), _L[(2)](θ) = EζF_ [(2)](θ; ζ). (2)
_j=1_

X


An L2O algorithm aims to learn an update rule for optimizee θ based on the meta-training function.
An update rule can be expressed as θt[(1)]+1[(][φ][) =][ θ]t[(1)][(][φ][) +][ m][(][z]t[(1)]; φ), where t = 0, 1, . . ., T −
1 denotes the iteration index over one epoch, the variable z captures the information (e.g., loss
values, gradients) that we collect on the optimization path, and the optimizer function m(z; φ) is
parameterized by φ and captures how the update of the optimizee parameter θ depends on the loss
landscape information included in z. In order to find a desirable optimizer parameter φ, L2O solves
the following meta-training problem:

minφ _N_ [(][θ]T[(1)][(][φ][))][}] where _θt[(1)]+1[(][φ][) =][ θ]t[(1)][(][φ][) +][ m][(][z]t[(1)]; φ)._ (3)

_[{][L][(1)]_

A popular L2O meta-training algorithm applies the gradient descent method, which updates φ based
on the gradient of the objective function L[(1)]N [(][θ]T[(1)][(][φ][))][ with respect to][ φ][. As suggested by eq. (3),]
each update of φ requires T iterations of the optimizee parameter θ0[(1)][(][φ][)][ to obtain][ θ]T[(1)][(][φ][)][.]

In meta-testing, we apply the output φ of meta-training and its corresponding optimizer to update
the optimizee as θt[(2)]+1[(][φ][) =][ θ]t[(2)][(][φ][) +][ m][(][z]t[(2)]; φ)(t = 0, 1, . . ., T − 1). Note that we differentiate
the optimizee updates in training and testing by superscripts (1) and (2), respectively.

3.2 HESSIAN REGULARIZER

Motivated by the idea that the optimizee in a flat area of the training objective has superior generalization capability, we propose to incorporate flatness-aware regularizers into L2O meta-training, in
order to learn optimizers that favors to land the optimizee into a flat region. We introduce two such
regularizers in this and next subsection.


-----

The first regularizer we introduce is based on the spectral norm of the Hessian, smaller values of
which corresponds to a flatter landscape. Thus, the new L2O meta-training objective is given by:

minφ _N_ [(][θ]T[(1)][(][φ][)) +][ λ][∥∇]θ[2][L][(1)]N [(][θ]T[(1)][(][φ][))][∥}] where _θt[(1)]+1[(][φ][) =][ θ]t[(1)][(][φ][) +][ m][(][z]t[(1)]; φ),_ (4)

_[{][L][(1)]_

where λ is the regularizer hyperparameter. Note that the Hessian regularizer is adopted for training
the optimizer parameter φ, and its impact on the update rule m(zt[(1)]; φ) is only through φ, i.e., the
information in zt does not include such regularization. We let φ[∗] be the optimal optimizer parameter,
which can be written as

_φ[∗]_ = arg minφ _{L[(1)]N_ [(][θ]T[(1)][(][φ][)) +][ λ][∥∇]θ[2][L][(1)]N [(][θ]T[(1)][(][φ][))][∥}][.] (5)

Due to the computational intractability of directly penalizingapproximation variants in the implementation. x Hessian EV ∇: the eigenvalue of largest module ofθ[2][L][(1)]N [(][θ]T[(1)][(][φ][))][, we investigate three]
Hessian matrix, computed by power iteration (Yao et al., 2020); y Hessian Trace: the trace of Hessian matrix, calculated via Hutchinson method (Yao et al., 2020); z Jacobian Trace: the trace of
Hessian’s Jacobian approximation ∇θL[(1)]N [(][θ]T[(1)][(][φ][))][⊤][∇][θ][L]N[(1)][(][θ]T[(1)][(][φ][))][. Note that such Hessian ap-]
proximation methods do not involve computing Hessian explicitly which helps to reduce the memory
and computational cost. In our case, we perform 10 iterations for Hessian norms’ approximation.

3.3 ENTROPY REGULARIZER

The second flatness-aware regularizer we incorporate to L2O is based on the local entropy function
_G[(1)]N_ [(][θ][;][ γ][) = log] _θ[′][ exp]_ _−L[(1)]N_ [(][θ][′][)][ −] _[γ]2_ _[∥][θ][ −]_ _[θ][′][∥][2][]_ dθ[′] proposed in Chaudhari et al. (2017). Due

to the exponential decay with respect to _θ_ _θ[′]_, the integral mainly captures the value of the loss

R

_∥_ _−_ _∥[2]_
function L[(1)]N [(][θ][′][)][ over the neighborhood of][ θ][. Thus, the value of][ G]N[(1)][(][θ][;][ γ][)][ measures the flatness of]
the local area around θ. Thus, the L2O meta training objective with Entropy regularizer is given by:

minφ _N_ [(][θ]T[(1)][(][φ][))][ −] _[λG]N[(1)][(][θ]T[(1)][(][φ][);][ γ][)][}]_ where _θt[(1)]+1[(][φ][) =][ θ]t[(1)][(][φ][) +][ m][(][z]t[(1)]; φ)._ (6)

_[{][L][(1)]_

We let φ[∗] be the optimal optimizer parameter, which can be written as

_φ[∗]_ = arg min _L[(1)]N_ [(][θ]T[(1)][(][φ][))][ −] _[λG]N[(1)][(][θ]T[(1)][(][φ][);][ γ][)][.]_ (7)
_φ_

In order to implement the gradient descent algorithm for meta-training, the gradient
_−∇φGN_ (θT[(1)][(][φ][);][ γ][)][ can be calculated by the entropy gradient][ −∇][θ][G]N[(1)][(][θ][;][ γ][)][ and the chain rule.]
In particular, as given in Chaudhari et al. (2017), the entropy gradient takes the following form

_−∇θG[(1)]N_ [(][θ][;][ γ][) =][ γ][(][θ][ −] [E][[][θ][′][;][ ξ][])][,] (8)

where ξ _ξi, i = (1, . . ., N_ ) are training samples and the distribution of θ[′] is given by
_∈{_ _}_

_P_ (θ[′]; θ, γ) ∝ exp h−L[(1)]N [(][θ][′][)][ −] _[γ]2_ _[∥][θ][ −]_ _[θ][′][∥][2][i]._

4 THEORETICAL ANALYSIS

In this section, we first introduce several assumptions, and then present the generalization analysis
of L2O with Hessian and Entropy regularizers.

4.1 ASSUMPTIONS

We first define the local basin of θ with the radius d as D[d](θ) = _θ[′]_ : _θ_ _θ[′]_ 2 _d_ . As have been
observed widely in training a variety of machine learning objectives, the convergent point enters into { _∥_ _−_ _∥_ _≤_ _}_
a local neighborhood where the strong convexity (or similar properties such as gradient dominance
condition, reguarity condition, etc) holds (Du et al., 2019; Li & Yuan, 2017; Zhou et al., 2018b;
Safran & Shamir, 2016; Milne, 2019). We thus make the following assumption on the geometry of
the meta-training function.


-----

**Assumption 1. We assume that there exist a a local basin D[d](θT[(1)][(][φ][))(][d >][ 0)][ of the conver-]**
_gence point θT[(1)][(][φ][)][ that in such local basin,][ L][(1)][(][θ][)][ and][ L]N[(1)][(][θ][)][ are][ µ][-strongly convex w.r.t.][ θ][.]_
_Futhermore, there exist a unique optimal point θ[∗][(1)]_ _of function L[(1)](θ) and a optimal point θN[∗][(1)]_
_of function L[(1)]N_ [(][θ][)][ in local basin][ D][d][(][θ]T[(1)][(][φ][∗][))][.]

**Assumption 2. We assume that L[(1)](θ) function is M** _-Lipschitz; ∇θL[(1)](θ) and ∇θL[(2)](θ) are_
_L-Lipschitz; ∇θ[2][L][(1)][(][θ][)][,][ ∇][2]θ[L][(2)][(][θ][)][,][ ∇][2]θ[L][(1)]N_ [(][θ][)][ and][ ∇]θ[2][L][(2)]M [(][θ][)][ are][ ρ][-Lipschitz.]

We further adopt the following assumptions introduced in Mei et al. (2018), in order to guarantee
the similarity between the landscape of the empirical and population objective functions.

**Assumption 3. The gradient of the training loss ∇F** [(1)](θ; ξ) is τ [2]-sub-Gaussian and the Hessian
_of the loss function is τ_ [2]-sub-exponential. See Appendix B.1 for more details.
**Assumption 4. There exist a basin D[r](0) that the meta-training functions L[(1)](θ) is (ϵ, η)-strongly**
_Morse in D[r](0) and the local basins D[d](θT[(][i][)][(][φ][∗][))][ for][ i][ = 1][,][ 2][ of convergence points][ θ]T[(][i][)][(][φ][∗][)(][i][ =]_
1, 2) are in D[r](0). See Appendix B.1 for more details.

4.2 GENERALIZATION ANALYSIS OF HESSIAN-REGULARIZED L2O

In this section, we adopt the optimizer learned by regularized L2O to train a new optimizee, and
analyze the advantages of the Hessian regularizer on the optimizee generalization ability.

We first note that by optimization theory, the regularized optimization problem eq. (5) is equivalent
to the following constrained optimization

minφ _N_ [(][θ]T[(1)][(][φ][))][}] where _θt[(1)]+1_ [=][ θ]t[(1)] + m(zt[(1)]; φ)

_[{][L][(1)]_

subject to ∥∇θ[2][L][(1)]N [(][θ]T[(1)][(][φ][))][∥≤] _[B][Hessian][(][λ][)][,]_ (9)

where BHessian(λ) is the constraint bound on the Hessian determined by λ. Thus, the optimizer parameter φ[∗] learned by the Hessian-regularized L2O meta-training in eq. (5) is also a
solution to eq. (9), i.e., its Hessian satisfy the constraint. Then let θT[(2)][(][φ][∗][)][ denote the opti-]
mizee parameters trained by optimizer φ[∗] in meta-testing, and θ[∗][(2)] denote the optimal point
of the population meta-testing function L[(2)](·). We then characterize the generalization error as
_L[(2)](θT[(2)][(][φ][∗][))][ −]_ _[L][(2)][(][θ][∗][(2)][)][, which capture how well the optimizer][ φ][∗]_ [performs on a testing task]
with respect to the best possible testing loss value.

The following theorem characterizes the generalization performance of the optimizee trained with
Hessian regularized optimizer as defined above.
**Theorem 1 (Generalization Error of Hessian-Regularized L2O). Suppose Assumptions 1, 2, 3, 4**
_hold. We let N_ max 4Cp log N/η[2] [max][{][c][h][,][ 1][,][ log(][ rτ]δ [)][}][,][ η][2]

min _τ[ϵ][2][2][,][ η]τ_ [2][4][,] _ρη[2]τ[4]_ [2] ≥[ }][ and][ C]{[0][ is an universal constant. Then, with probability at least]∗[, Cp][ log][ p][}][ where][ C][ =][ C][0] [ 1][ −] [2][δ]∗[ we][=]
_{_

_have_

_L[(2)](θT[(2)][(][φ][∗][))][ −]_ _[L][(2)][(][θ][∗][(2)][)][ ≤]_ [1] 2[A][1][,] (10)

2 _[A][2]_


_where A1 = BHessian(λ) + ρ∆[∗]T_ [+][ ρ][∆]θ[∗] [+ ∆]H[∗] [+][ O][(][w][T][ −][T][ ′] [) +][ O][(] _C logN_ _N_ ), A2 = ∆[∗]T [+ ∆]θ[∗] [+]

_C log N_ _L_ _µ_ q

_O(w[T][ −][T][ ′]_ ) + O( _N_ ), with w = _L−+µ_ _[,][ ∆]H[∗]_ [=][ ∥∇]θ[2][L][(2)][(][θ][∗][(2)][)][ −∇][2]θ[L][(1)][(][θ][∗][(1)][)][∥][,][ ∆][∗]θ [=]

_θ[∗][(1)]_ _θ[∗][(2)]_ _, ∆q[∗]T_ [=][ ∥][θ]T[(2)][(][φ][∗][)][ −] _[θ]T[(1)][(][φ][∗][)][∥][, and][ T][ ′][ is the minimum gradient descent iterations]_
_∥_ _−_ _∥_
_for θT[(1)][′][ (][GD][)][ to enter into the local basin of][ θ]T[(1)][(][φ][∗][)][.]_

In Theorem 1 the generalization error is bounded by the terms A1 and A2, where the Hessian regularizer affects the generalization error through the constraint BHessian(λ) in A1. Clearly, by choosing the
regularization hyperparameter λ, we control the value of BHessian(λ), which then makes an impact
on the generalization. Specifically, larger λ corresponds to smaller BHessian(λ) and hence smaller
generalization error. This also explains that flatter landscape (i.e., smaller BHessian(λ) on Hessian)
yields better generalization performance (i.e., smaller generalization error).


-----

The generalization error in Theorem 1 also contains other terms which we explain as follows: (a)
_ρ∆[∗]T_ [+][ ρ][∆]θ[∗] [+ ∆]H[∗] [capture the similarity between the training and testing tasks; more similar tasks]
yields better generalization; (b) O(w[T][ −][T][ ′] ) captures the exponential decay rate of the optimizee’s

iteration due to the strong convexity, and vanishes for large T ; and (c) ( _C logN_ _N_ ) arises due to
_O_

the difference between the empirical and population loss functions, and vanishes as the sample sizeq
_N gets large._

4.3 GENERALIZATION ANALYSIS OF ENTROPY-REGULARIZED L2O

In this section, we analyze the generalization error of the Entropy regularizer on the optimizee
generalization ability.

Similarly to the Hessian regularizer, the regularized optimization problem eq. (6) is equivalent to the
following constrained optimization:

minφ _[L]N[(1)][(][θ]T[(1)][(][φ][))]_ where _θt[(1)]+1_ [=][ θ]t[(1)] + m(zt[(1)]; φ)

subject to − _G[(1)]N_ [(][θ]T[(1)][(][φ][);][ γ][)][ ≤] _[B][Entropy][(][λ][)][,]_ (11)

where BEntropy(λ) is the constraint bound on the Entropy determined by λ. Thus, the optimizer φ[∗]
learned by the Entropy-regularized L2O meta-training in eq. (6) is also a solution to eq. (11), i.e.,
the local entropy satisfies the constraint.

In the following, we first establish an important connection between the Entropy constraint bound
_BEntropy(λ) and the Hessian ∥∇[2]L[(1)]N_ [(][θ]T[(1)][(][φ][))][∥] [of the optimizee.]

**Theorem 2 (Connection between Hessian and Entropy Regularizer). Suppose Assumptions 1 and 2**
_hold. We define C(γ, p, m) = log_ _θ[′]:_ _θ[′]_ _θ_ _>m_ [exp(][−] _[γ]2_

_θ_ R[p]. Then, we have _∥_ _−_ _∥_ _[∥][θ][ −]_ _[θ][′][∥][2][)][d][θ][′][ where][ m][ is a constant and]_
_∈_ R

_∥∇[2]L[(1)]N_ [(][θ]T[(1)][(][φ][))][∥≤] _[D][−][1][(][B][Entropy][(][λ][))]_

_where D(x) = L[(1)]N_ [(][θ]T[(1)][(][φ][)) + (][p][ −] [1) log(][γ][ +][ µ][)][ −] _[mM][ −]_ _[p]2_ [log(2][π][)][ −] 2[1] _[ρm][3][ −]_ _[C][(][γ, p, m][) +]_

log(x + γ) and D[−][1](x) denotes the inverse function of D(x).

It can be observed that the function D(x) in monotonically increasing w.r.t. x and so is its inverse
_D[−][1](x), as determined by the only x-dependent term log(x + γ). Hence, Theorem 2 shows that the_
entropy bound constraint BEntropy(λ) implies a corresponding Hessian constraint D[−][1](BEntropy(λ)).
Thus, Theorem 1 can be applied to provide a bound on the generalization error here.
**Corollary 1 (Generalization Error of Entropy-Regularized L2O). Suppose the same conditions of**
_Theorem 1 hold. Then the generalization error of L2O with Entropy regularizer takes the bound in_
_eq. (10) with BHessian(λ) in A1 being replaced by D[−][1](BEntropy(λ))._

Theorem 2 and Corollary 1 establish that the bound D[−][1](BEntropy(λ)) serves the same role as the
Hessian bound in the generalization performance. Thus, by controlling the hyperparameter λ to
be large enough in the L2O training, BEntropy(λ) as well as D[−][1](BEntropy(λ)) and Hessian can be
controlled to be sufficiently small. In this way, the optimizee will be landed into a flat basin (due to
small Hessian) to enjoy better generalization.

To compare with the result in Chaudhari et al. (2017), we note that Chaudhari et al. (2017)
proposed the Entropy-SGD method and showed that Entropy-SGD favors better generalization
solutions in terms of the Entropy energy landscape. As a comparison, Theorem 2 and Corollary 1 establish the generalization error for Entropy-SGD in terms of the un-regularized loss
_L[(2)](θT[(2)][(][φ][∗][))][ −]_ _[L][(2)][(][θ][∗][(2)][)][ in original loss landscape, which is the ultimate goal of generaliza-]_
tion. Such a favorable result is established by exploiting the equivalence between the regularized
optimization and the un-regularized constrained optimization problems.

We further note that Dinh et al. (2017) theoretically shows that sharp minimas can also generalize
well for deep neural networks. However, such a result does not contradict the fact that flat minima
generalizes well, which has strong evidence (He et al., 2019; Keskar et al., 2017), and is the property
that we exploit in the paper.


-----

|Conv-|CIFAR|
|---|---|
|||
|||
|||
|||
|||
|||

|ization.|Col2|
|---|---|
|Conv-Lar|ge-MNIST|
|||
|||
|||
|||
|||
|||


3.0 Conv-MNIST 3.0 Conv-CIFAR 3.0 MLP-ReLU 3.0 Conv-Large-MNIST

SGD

2.5 ADAML2O-Scale 2.5 2.5 2.5

L2O-Scale + Hessian Trace

2.0 2.0 2.0 2.0

1.5 1.5 1.5 1.5

Training Loss1.0 1.0 1.0 1.0

0.5 0.5 0.5 0.5

0.0 0 2000 4000 6000 8000 10000 0.0 0 2000 4000 6000 8000 10000 0.0 0 2000 4000 6000 8000 10000 0.0 0 2000 4000 6000 8000 10000

Training Iterations Training Iterations Training Iterations Training Iterations

1.0

1.0 0.6 1.0

0.8 0.5 0.8 0.8

0.6 0.4 0.6 0.6

0.3

Testing Accuracy0.4 SGD 0.4 0.4

ADAM 0.2

0.2 L2O-Scale 0.2 0.2

L2O-Scale + Hessian Trace 0.1

0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000

Training Iterations Training Iterations Training Iterations Training Iterations

Figure 2: Comparison of the training loss/testing accuracy of optimizees trained using analytical optimizers
and L2O-Scale (Wichrowska et al., 2017) with/without the proposed Hessian regularization.

3.0 Conv-MNIST 3.0 Conv-CIFAR 3.0 MLP-ReLU 3.0 Conv-Large-MNIST

SGD

2.5 ADAML2O-DM-CL 2.5 2.5 2.5

L2O-DM-CL + Hessian Trace

2.0 L2O-DM-CL + Entropy 2.0 2.0 2.0

1.5 1.5 1.5 1.5

Training Loss1.0 1.0 1.0 1.0

0.5 0.5 0.5 0.5

0.0 0 2000 4000 6000 8000 10000 0.0 0 2000 4000 6000 8000 10000 0.0 0 2000 4000 6000 8000 10000 0.0 0 2000 4000 6000 8000 10000

Training Iterations Training Iterations Training Iterations Training Iterations

1.0

0.5 0.8 1.0

0.8

0.8

0.4

0.6 0.6 0.6

0.3

0.4 SGD 0.4 0.4

Testing Accuracy ADAML2O-DM-CL 0.2 0.2

0.2 L2O-DM-CL + Hessian Trace 0.2

L2O-DM-CL + Entropy 0.1 0.0

0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000

Training Iterations Training Iterations Training Iterations Training Iterations

Figure 3: Comparison of the training loss/testing accuracy of optimizees trained using analytical optimizers
and L2O-DM-CL (Chen et al., 2020a) with/without the proposed Hessian regularization.

5 EXPERIMENT


In our experiments, we consider two advanced L2O algorithms, i.e., L2O-DM-CL[1] (Chen et al.,
2020a) and L2O-Scale (Wichrowska et al., 2017) on four diverse meta testing optimizees.

**Meta Training Optimizees.** For training L2O-Scale, we use a three-layer convolutional neural
network (CNN) which has one fully-connected layer, and two convolutional layers with eight 3 × 3
and 5 × 5 kernels respectively. For training L2O-DM, we adopt the same meta training optimizee
from Andrychowicz et al. (2016b), which is a simple Multi-Layer Perceptron (MLP) with one hidden
layer of 20 dimensions and the sigmoid activation function. MNIST (LeCun et al., 1998) dataset is
used for all the meta-training.

**Meta Testing Optimizees.** We select four distinct and representative meta testing optimizees from
Andrychowicz et al. (2016b) and Chen et al. (2020a) to evaluate the generalization ability of the

1It is an enhanced version of the earliest L2O-DM introduced by DeepMind Andrychowicz et al. (2016b).
We choose it as a much stronger baseline instead of the vanilla L2O-DM with “poor-generalization baseline” (Wichrowska et al., 2017; Lv et al., 2017).


-----

learned optimizer. Specifically, x MLP-ReLU: a single layer MLP with 20 neurons and the ReLU
activation function on MNIST. y Conv-MNIST: a CNN has one fully-connected layer, and two
convolutional layers with 16 3 × 3 and 32 5 × 5 kernels on MNIST. z Conv-Large-MNIST:
a large CNN has one fully-connected layer, and four convolutional layers with twotwo 32 5 × 5 kernels on MNIST. { Conv-CIFAR: a CNN has one fully-connected layer, and two 32 3 × 3 and
convolutional layers withOptimizees x, y, and z are for evaluating the generalization of L2O across network architectures. 16 3 _×_ 3 and 32 5 _×_ 5 kernels on CIFAR-10 (Krizhevsky & Hinton, 2009).
Then, { evaluates the generalization of L2O across both network architectures and datasets.

**Training and Evaluation details.** During the meta training stage of L2O, L2O-Scale is trained
with 5 epochs, where the number of each epoch’s iteration is drawing from a heavy tailed distribution (Wichrowska et al., 2017). L2O-DM-CL is trained with a curriculum schedule of training
epochs and iterations, following the default setup in Chen et al. (2020a). RMSprop with the learning rate 1 × 10[−][6] is used to update L2Os. For the {Hessian, Entropy} regularization coefficients
_{λHessian, λEntropy, γ}, we perform a grid search and choose {5 × 10[−][5],-,-}/{1 × 10[−][8],1 × 10[−][6],1 ×_
10[−][4]} for L2O-Scale/L2O-DM-CL.

In the meta testing stage of L2O, we compare our methods with classical optimizers like SGD and
Adam, and state-of-the-art (SOTA) L2Os such as L2O-Scale and L2O-DM-CL. Hyperparameters
of both classical optimizers and L2O baselines are carefully tuned through the grid search and all
other irrelevant variables are strictly controlled for a fair comparison. We run 10, 000 iterations for
the meta testing, and the corresponding training loss and test accuracy on all unseen optimizees
are collected to evaluate the optimizer and optimizee generalization. Note that the training loss
corresponds to meta testing L[(2)]M [and test accuracy corresponds to][ L][(2)][ in Section 3.1][. We conduct]
**ten independent replicates with different random seeds and report the average performance. All of**
our experiments are conducted on a computing facility of NVIDIA GeForce GTX 1080Ti GPUs.

5.1 IMPROVE GENERALIZATION WITH HESSIAN REGULARIZATION

In this section, we conduct extensive evaluations of our proposed Hessian regularization on previous
state-of-the-art L2O methods, i.e., L2O-Scale (Wichrowska et al., 2017) and L2O-DM-CL (Chen
et al., 2020a). Achieved training loss and testing accuracy are collected in Figure 2 and 3 which
also include comparisons with representative analytical optimizers like SGD (Ruder, 2016) and
Adam (Kingma & Ba, 2014). Several consistent observations can be drawn from our results:  Hessian Trace regularizer consistently enhances the generalization abilities of learned L2Os and trained
optimizees. Specifically, L2Os with Hessian Trace enable fast training loss decay and much lower
final loss on all four unseen meta-testing optimizees, demonstrating the improved optimizer gen_eralization ability. Furthermore, all unseen optimizees trained by Hessian regularized L2Os enjoy_
substantial testing accuracy which boosts up to 31%, showing the enhanced optimizee generalization
ability. Such impressive performance gains effective evidence of our proposal, which again suggests
that Hessian regularization potentially leads to well-generalizable minimas in wide valleys in loss
landscape.  Adopting vanilla L2O-DM-CL to train meta-testing optimizees (e.g., Conv-MNIST
and Conv-CIFAR) suffers from instability as shown in Figure 3, and it can be significantly mitigated by introducing our flatness-aware regularization. Conv-Large-MNIST is an exception,
where the L2O-DM-CL fails to train this optimizee and ends up with random guessed accuracy, i.e.,
10%. Although plugging Hessian Trace into L2O-DM-CL greatly improves its test accuracy from
10% to 95%+, it still undergoes a unsatisfactory training loss. Potential reasons may lie in the rough
model architecture and limited input features of L2O-DM-CL, coincided with the findings in Chen
et al. (2020a). We will further investigate this interesting phenomenon in the future.  For advanced
L2O-Scale, Hessian Trace regularization facilitates it to converge a significantly lower minima and
obtain considerable accuracy improvements. It enlarges the advantages of L2O methods compared
to analytical optimizers, SGD and Adam, unleashing the power of parameterized optimizers.

5.2 IMPROVE GENERALIZATION WITH ENTROPY REGULARIZATION

We investigate the generalization ability improvements from the Entropy regularization. Generally,
it boosts both optimizee and optimizer generalizations of L2O in most cases, as shown in Figure 3.
**Hessian v.s. Entropy Regularization.** We compare our two kinds of flatness-aware regularizers
from both computational cost and performance benefits perspectives.  In order to calculate the
local entropy’s gradient in eq. (8), it involves gradients from multiple unroll steps for the estima

-----

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|||||||
|||||||
|||||||
|L2|O-DM-C|L||||
|L2|O-DM-C|L + He|ssian E|V||
|L2 L2|O-DM-C O-DM-C|L + Jac L + He|obian T ssian Tr|race ace||

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||||||||
||||||||
||||||||
||||||||
||||||||
||||||||
||||||||

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||||||||
||||||||
||||||||
||||||||

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|
|---|---|---|---|---|---|---|
||||||||
||||||||
||||||||
||||||||
||||||||

|Col1|Col2|Col3|Col4|Col5|Col6|
|---|---|---|---|---|---|
|||||||
|||||||
|||||||
|||||||
|||||||
|||L2O- L2O-|Scale Scale +|Hessia|n EV|
|||||||
|||L2O- L2O-|Scale + Scale +|Jacobia Hessia|n Trace n Trace|


Figure 4: Comparison of the testing accuracy of optimizees trained using analytical optimizers and SOTA

Conv-MNIST Conv-CIFAR MLP-ReLU Conv-Large-MNIST

1.1 0.45

1.0

1.0 0.40 0.94

0.9 0.35 0.8

0.8 0.93

0.30 0.6

0.7

0.6 0.25 0.92 0.4

Testing Accuracy0.5 L2O-DM-CLL2O-DM-CL + Hessian EV 0.20 0.2

0.4 L2O-DM-CL + Jacobian Trace 0.15 0.91

0.3 L2O-DM-CL + Hessian Trace 0.10 0.0

0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000

Training Iterations Training Iterations Training Iterations Training Iterations

1.1 0.7 1.0

1.0 0.9 1.0

0.9 0.6 0.8

0.8 0.5 0.7 0.8

0.7 0.4 0.6 0.6

0.6

Testing Accuracy0.5 L2O-Scale 0.3 0.5 0.4

L2O-Scale + Hessian EV 0.4

0.4 L2O-Scale + Jacobian Trace 0.2 0.2

0.3 L2O-Scale + Hessian Trace 0.3

0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000

Training Iterations Training Iterations Training Iterations Training Iterations

L2O with/without different Hessian regularization, Hessian EV, Hessian Trace, and Jacobian Trace.

tion (Chaudhari et al., 2017), leading to extra memory and computing outlays. Compared to Hessian
augmented L2O, it costs As for generalization gains, Entropy regularizer performs slightly better on ∼ 2.6x memory and ∼ 3x running time for L2O-DM-CL experiments Conv-MNIST and[2].
Conv-CIFAR, while behaves marginally worse on MLP-ReLU and Conv-Large-MNIST compared to Hessian regularizer. We would like to draw reader’s attention to Conv-Large-MNIST, in
which Entropy regularized L2O-DM-CL is capable of decaying the training loss and finding a much
lower minima than Adam. Note that on this optimizee, both L2O-DM-CL and its Hessian variant
can not decrease the training loss. The possible reason is that multi-layer convolutional neural network without BN cannot be stably trained on MNIST. However, our L2O-DM-CL+Entropy is more
stable in training and improves testing accuracy compared with L2O-DM-CL. This indicates that
L2O-DM-CL + Entropy may also produce a more trainable loss surface for optimizees.

Based on the above experiments as well as the experiment on ResNet20 in Appendix A.1, we observe that L2O+Entropy is preferred when we adopt L2O to train large neural networks, where
L2O+Entropy yields better optimizers and optimizee generalization abilities. On the other hand,
L2O+Hessain optimizer requires less time per iteration to train and achieves lower training loss as
well as higher test accuracy than L2O+Entropy in small networks, e.g. MLP. The possible reason is
that Entropy takes account of the landscape over a large range of loss to measure the flatness, and can
hence capture complex landscape information in large neural networks. On the other hand, Hessian
regularizer captures the flatness information only for the individual point, but in a more accurate
manner, and thus is more suitable to smaller neural networks with a relative simple landscape.

5.3 ABLATION AND VISUALIZATION

In this section, we carefully examine the effect of Hessian regularization’s different approximation
variants, including Hessian EV, Hessian Trace, and Jacobian Trace. Results are presented in Figure 4. We find that Hessian Trace regularizer achieves the most stable and substantial performance
boosts across all optimizees. Jacobian Trace performs the worst which is within expectation since it
provides the roughest estimation of Hessian.

6 CONCLUSION

In this paper, we propose several flatness-aware regularizers to improve both optimizer and op_timizee generalization abilities of current state-of-the-art L2O approaches. Such regularizers are_
capable of shaping the local geometry of optimizee’s loss surface, and leading to well-generalizable
minimas in wide valleys which have been proved theoretically. Our empirical results validate the effectiveness of our proposal, taking a further step for L2O’s practically usage in real-world scenarios.

2We conduct entropy-related experiments on light-weight L2O-DM-CL rather heavy L2O-Scale models,
since RTX TITAN with 24G memory is the largest GPU we can access and afford.


-----

7 REPRODUCIBILITY CHECKLIST

To ensure reproducibility, we use the Machine Learning Reproducibility Checklist v2.0, Apr. 7
2020 (Pineau et al., 2021). An earlier version of this checklist (v1.2) was used for NeurIPS
2019 (Pineau et al., 2021).

-  For all models and algorithms presented,

**– A clear description of the mathematical settings, algorithm, and/or model. We**
clearly describe all of the settings, formulations, and algorithms in Section 3.

**– A clear explanation of any assumptions. All assumptions are stated in Section 4.1**
and details are clearly explained in Appendix B.1.

**– An analysis of the complexity (time, space, sample size) of any algorithm. We do**
not make the analysis.



-  For any theoretical claim,

**– A clear statement of the claim. A clear statement of theoretical claims are made in**
Section 4.2 and Section 4.3.

**– A complete proof of the claim. The complete proofs of all claims are available in**
Appendix B and Appendix C.

-  For all datasets used, check if you include:

**– The relevant statistics, such as number of examples.** We use widely adopted
datasets MNIST and CIFAR-10 in Section 5. The related statistics can be
seen at http://yann.lecun.com/exdb/mnist/ and https://www.cs.
toronto.edu/˜kriz/cifar.html.

**– The details of train/validation/test splits. We give this information in our repository**
in the supplementary material.

**– An explanation of any data that were excluded, and all pre-processing step. We**
did not exclude any data or perform any pre-processing.

**– A link to a downloadable version of the dataset or simulation environment. Our**
repository contains all instructions to download and run experiments on the datasets.

**– For new data collected,a complete description of the data collection process, such**
**as instructions to annotators and methods for quality control. We do not collect**
or release new datasets.

-  For all shared code related to this work, check if you include:

**– Specification of dependencies. We give installation instructions in the README of**
our repository.

**– Training code. The training code is available in our repository.**
**– Evaluation code. The evaluation code is available in our repository.**
**– (Pre-)trained model(s). We do not release any pre-trained models.**
**– README file includes table of results accompanied by precise command to run**
**to produce those results. We include a README with detailed instructions to repro-**
duce all of our experimental results.

-  For all reported experimental results, check if you include:


**– The range of hyper-parameters considered, method to select the best hyper-**
**parameter configuration, and specification of all hyper-parameters used to gen-**
**erate results. We provide all details of the hyper-parameter tuning in Section 5.**

**– The exact number of training and evaluation runs. The details about training and**
evaluation can be seen in Section 5.

**– A clear definition of the specific measure or statistics used to report results. We**
use the classification accuracy on test-set and the loss on the train-set.

**– A description of results with central tendency (e.g. mean) & variation (e.g. error**
**bars). We do not report the mean and standard deviation for experiments.**

**– The average runtime for each result, or estimated energy cost. We do not report**
the running time or energy cost.

**– A description of the computing infrastructure used. All detailed descriptions are**
presented in Section 5.


-----

REFERENCES

Marcin Andrychowicz, Misha Denil, Sergio G´omez, Matthew W Hoffman, David Pfau, Tom Schaul,
Brendan Shillingford, and Nando de Freitas. Learning to learn by gradient descent by gradient
descent. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in
_Neural Information Processing Systems (NeurIPS), volume 29. Curran Associates, Inc., 2016a._

Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,
Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient
descent. In Advances in neural information processing systems (NeurIPS), 2016b.

Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. In International Conference on Machine Learning (ICML),
pp. 254–263, 2018.

Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In International
_Conference on Machine Learning (ICML), pp. 322–332, 2019._

Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems (NeurIPS), volume 30,
pp. 6240–6249, 2017.

Peter L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian. Nearly-tight VC-dimension
and pseudodimension bounds for piecewise linear neural networks. _The Journal of Machine_
_Learning Research (JMLR), 20(1):2285–2301, 2019._

Yue Cao, Tianlong Chen, Zhangyang Wang, and Yang Shen. Learning to optimize in swarms. In
_Advances in Neural Information Processing Systems (NeurIPS), pp. 15018–15028, 2019._

Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian
Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-SGD: Biasing gradient descent into wide valleys. In International Conference on Learning Representations (ICLR),
2017.

Tianlong Chen, Weiyi Zhang, Jingyang Zhou, Shiyu Chang, Sijia Liu, Lisa Amini, and Zhangyang
Wang. Training stronger baselines for learning to optimize. arXiv preprint arXiv:2010.09089,
2020a.

Wuyang Chen, Zhiding Yu, Zhangyang Wang, and Animashree Anandkumar. Automated syntheticto-real generalization. In International Conference on Machine Learning (ICML), pp. 1746–1756,
2020b.

Xuxi Chen, Wuyang Chen, Tianlong Chen, Ye Yuan, Chen Gong, Kewei Chen, and Zhangyang
Wang. Self-pu: Self boosted and calibrated positive-unlabeled training. In International Confer_ence on Machine Learning (ICML), pp. 1510–1519, 2020c._

Yutian Chen, Matthew W Hoffman, Sergio G´omez Colmenarejo, Misha Denil, Timothy P Lillicrap,
Matt Botvinick, and Nando De Freitas. Learning to learn without gradient descent by gradient
descent. In International Conference on Machine Learning (ICML), pp. 748–756, 2017.

Alex Damian, Tengyu Ma, and Jason D. Lee. Label noise SGD provably prefers flat global minimizers. CoRR, abs/2106.06530, 2021.

Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize
for deep nets. In International Conference on Machine Learning (ICML), pp. 1019–1028, 2017.

Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations
_(ICLR), 2019._

John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning
and stochastic optimization. Journal of Machine Learning Research (JMLR), 12(61):2121–2159,
2011.


-----

Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In International Conference on Learning Represen_tations (ICLR), 2021._

Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. In Conference On Learning Theory (COLT), pp. 297–299, 2018.

Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In International Conference on Machine Learning (ICML), pp. 1225–1234,
2016.

Haowei He, Gao Huang, and Yang Yuan. Asymmetric valleys: Beyond sharp and flat local minima.
In Advances in Neural Information Processing Systems (NeurIPS), volume 32, pp. 2553–2564,
2019.

Sepp Hochreiter and J¨urgen Schmidhuber. Simplifying neural nets by discovering flat minima. In
_Advances in neural information processing systems (NeurIPS), pp. 529–536, Cambridge, MA,_
USA, 1994. MIT Press.

Sepp Hochreiter and J¨urgen Schmidhuber. Flat Minima. Neural Computation, 9(1):1–42, 01 1997.
ISSN 0899-7667. doi: 10.1162/neco.1997.9.1.1.

Kaiyi Ji and Yingbin Liang. Minimax estimation of neural net distance. In Advances in Neural
_Information Processing Systems (NeurIPS), pp. 3849–3858, 2018._

Kaiyi Ji, Yi Zhou, and Yingbin Liang. Understanding estimation and generalization error of generative adversarial networks. IEEE Transactions on Information Theory, 67(5):3114–3129, 2021.

Haoming Jiang, Zhehui Chen, Yuyang Shi, Bo Dai, and Tuo Zhao. Learning to defense by learning
to attack. arXiv preprint arXiv:1811.01213, 2018.

Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic generalization measures and where to find them. In International Conference on Learning
_Representations (ICLR), 2020._

Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from
Adam to SGD. arXiv preprint arXiv:1712.07628, 2017.

Nitish Shirish Keskar, Jorge Nocedal, Ping Tak Peter Tang, Dheevatsa Mudigere, and Mikhail
Smelyanskiy. On large-batch training for deep learning: Generalization gap and sharp minima.
In International Conference on Learning Representations (ICLR), 2017.

Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
_arXiv:1412.6980, 2014._

A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Master’s
_thesis, Department of Computer Science, University of Toronto, 2009._

Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.

Chaojian Li, Tianlong Chen, Haoran You, Zhangyang Wang, and Yingyan Lin. Halo: Hardwareaware learning to optimize. In European Conference on Computer Vision (ECCV), pp. 500–518.
Springer, 2020.

Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. In Advances in Neural Information Processing Systems (NeurIPS), 2018.

Ke Li and Jitendra Malik. Learning to optimize. arXiv preprint arXiv:1606.01885, 2016.

Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with ReLU activation. Advances in Neural Information Processing Systems (NeurIPS), 30, 2017.

Kaifeng Lv, Shunhua Jiang, and Jian Li. Learning gradient descent: Better generalization and longer
horizons. In International Conference on Machine Learning (ICML), pp. 2247–2255, 2017.


-----

Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for nonconvex losses.
_The Annals of Statistics, 46(6A):2747–2774, 2018._

Tristan Milne. Piecewise strong convexity of neural networks. In Advances in Neural Information
_Processing Systems (NeurIPS), volume 32, pp. 12973–12983, 2019._

Joelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Lariviere, Alina Beygelzimer,
Florence d’Alch´e Buc, Emily Fox, and Hugo Larochelle. Improving reproducibility in machine
learning research. Journal of Machine Learning Research, 22:1–20, 2021.

M. Riedmiller and H. Braun. A direct adaptive method for faster backpropagation learning: the rprop
algorithm. In IEEE International Conference on Neural Networks, pp. 586–591 vol.1, 1993. doi:
10.1109/ICNN.1993.298623.

Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat_ical Statistics, 22(3):400–407, 1951. ISSN 00034851._

Sebastian Ruder. An overview of gradient descent optimization algorithms. _arXiv preprint_
_arXiv:1609.04747, 2016._

Itay Safran and Ohad Shamir. On the quality of the initial basin in overspecified neural networks.
In International Conference on Machine Learning (ICML), pp. 774–782, 2016.

Jiayi Shen, Xiaohan Chen, Howard Heaton, Tianlong Chen, Jialin Liu, Wotao Yin, and Zhangyang
Wang. Learning a minimax optimizer: A pilot study. In International Conference on Learning
_Representations (ICLR), 2021._

T. Tieleman and G. Hinton. Lecture 6.5—RmsProp: Divide the gradient by a running average of its
recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.

Olga Wichrowska, Niru Maheswaranathan, Matthew W Hoffman, Sergio Gomez Colmenarejo,
Misha Denil, Nando de Freitas, and Jascha Sohl-Dickstein. Learned optimizers that scale and
generalize. In International Conference on Machine Learning (ICML), 2017.

Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, and Benjamin Recht. The
marginal value of adaptive gradient methods in machine learning. In Advances in Neural In_formation Processing Systems (NeurIPS), 2017._

Yuanhao Xiong and Cho-Jui Hsieh. Improved adversarial training via learned optimizer, 2020.

Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W Mahoney. Pyhessian: Neural networks
through the lens of the Hessian. In 2020 IEEE International Conference on Big Data (Big Data),
pp. 581–590. IEEE, 2020.

Yuichi Yoshida and Takeru Miyato. Spectral norm regularization for improving the generalizability
of deep learning. arXiv preprint arXiv:1705.10941, 2017.

Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. L2-gcn: Layer-wise and learned
efficient training of graph convolutional networks. In Proceedings of the IEEE/CVF Conference
_on Computer Vision and Pattern Recognition (CVPR), pp. 2127–2135, 2020._

Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning (still) requires rethinking generalization. Commun. ACM, 64(3):107–115, February
2021. ISSN 0001-0782. doi: 10.1145/3446776.

Pan Zhou, Jiashi Feng, Chao Ma, Caiming Xiong, Steven Chu Hong Hoi, et al. Towards theoretically
understanding why SGD generalizes better than Adam in deep learning. In Advances in Neural
_Information Processing Systems (NeurIPS), volume 33, 2020._

Yi Zhou, Yingbin Liang, and Huishuai Zhang. Generalization error bounds with probabilistic guarantee for SGD in nonconvex optimization. arXiv preprint arXiv:1802.06903, 2018a.

Yi Zhou, Junjie Yang, Huishuai Zhang, Yingbin Liang, and Vahid Tarokh. SGD converges to global
minimum in deep learning via star-convex path. In International Conference on Learning Repre_sentations (ICLR), 2018b._


-----

Difan Zou, Yuan Cao, Yuanzhi Li, and Quanquan Gu. Understanding the generalization of Adam in
learning neural networks with proper regularization. arXiv preprint arXiv:2108.11371, 2021.


-----

## Supplementary Materials

A ADDITIONAL EXPERIMENTAL RESULTS

A.1 RESNET20 EXPERIMENTS

In this section, we evaluate the performance of our trained optimizers on larger neural networks
ResNet-20 on CIFAR-10 dataset. The training loss and testing accuracy are plotted in Figure 5.
We can see that the Entropy regularizer is able to outperform other methods in both training loss
and testing accuracy, demonstrating its generalization ability on large unseen models. Further note
that although the Hessian regularizer may not be preferred in large neural networks, it does perform
better than the Entropy regularizer in small networks as we have shown in Figure 3.

|ResNet-|20-CIFAR|
|---|---|
|L2O-DM-C L2O-DM-C|L L + Hessian Trace|
||L L + Hessian Trace|
|L2O-DM-C|L + Entropy|
|||
|||
|||
|||

|Col1|Col2|Col3|Col4|Col5|Col6|Col7|Col8|
|---|---|---|---|---|---|---|---|
|||||||||
|||||||||
|||||||||
|||||||||
|||||||||
|||L2O-DM|-CL|||||
|||L2O-DM L2O-DM|-CL + -CL +|Hessia Entrop|n Trace y|||
|||||||||


ResNet-20-CIFAR

3.0

0.8

L2O-DM-CL

2.5 L2O-DM-CL + Hessian Trace 0.7

L2O-DM-CL + Entropy

2.0 0.6

1.5 0.5

Training Loss1.0 0.4

Testing Accuracy

L2O-DM-CL

0.3

0.5 L2O-DM-CL + Hessian Trace

L2O-DM-CL + Entropy

0.2

0.0

0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000

Training Iterations Training Iterations


Figure 5: Comparison of the training loss/testing accuracy of ResNet-20 trained using L2O-DM-CL (Chen
et al., 2020a) with/without the proposed Hessian/Entropy regularization.

A.2 WALL CLOCK COMPARISON BETWEEN DIFFERENT ALGORITHMS

We further conduct an optimizee training time comparison between our methods and analytical
optimizers, L2O-DM-CL, and Entropy-SGD (Chaudhari et al., 2017) in Table 1. Note that L2ODM-CL+Hessian and L2O-DM-CL+Entropy share the same time to train optimizee as L2O-DMCL. From Table 1, we can see that trained L2O-DM-CL requires only ∼ 1.5x time than analytical
optimizers in terms of inference time, which is thus time efficient for practical usage. However,
Entropy-SGD requires ∼ 21x time than analytical optimizers to train optimizees. Such cost is
because Entropy-SGD requires multiple Langevin dynamic steps per iteration to estimate the local
entropy.

Table 1: Empirical Time Cost Comparison per Iteration

|Methods SGD|ADAM L2O(L2O+Hessian, L2O+Entropy) Entropy-SGD|
|---|---|
|Time (secs) 0.045|0.045 0.067 0.958|



A.3 ACCURACY COMPARISON BETWEEN DIFFERENT ALGORITHMS

We also compare the testing accuracy (%) of our proposed methods with Entropy-SGD (Chaudhari
et al., 2017) and SGD with Hessian regularization. The Conv-MNIST results shown in Table 2 are
evaluated on L2O-DM-CL and the Conv-CIFAR results shown in Table 3 are evaluated on L2OScale. We adopt the same experimental setting as in Section 5 for training except that the running


-----

epochs are limited to 100 to investigate whether the performance of trained optimizers would persist
in long term.

|Table|2: Additional Testing Accuracy Comparison on Conv-MNIST|
|---|---|
|Methods|L2O L2O+Hessian L2O+Entropy SGD Entropy-SGD SGD+Hessian|
|Testing Accuracy|92.74 97.34 97.87 80.73 97.54 95.37|



Table 3: Additional Testing Accuracy Comparison on Conv-CIFAR

|Methods|L2O+Hessian Entropy-SGD SGD SGD+Hessian|
|---|---|
|Testing Accuracy|59.57 57.73 54.69 51.41|



From these comparisons, we can see that our proposed optimizers (L2O+Hessian, L2O+Entropy)
achieve the best performance compared with regularized analytical optimizers. Specifically, in
Conv-CIFAR setting as shown in Table 3, our algorithm L2O+Hessian outperforms SGD+Hessian
and Entropy-SGD. In Conv-MNIST setting as shown in Table 2, the performances of top three algorithms, i.e. L2O+Entropy, Entropy-SGD and L2O+Hessian, are similar and much better than the
performances of L2O and SGD+Hessian. Among the top three algorithms, the iteration running
time for Entropy-SGD is 0.958 secs while L2O+Hessian and L2O+Entropy only take 0.067 secs
as shown in Table 1. Such wall clock comparison shows that L2O+Hessian and L2O+Entropy are
more time efficient than Entropy-SGD while achieving the high accuracy, which are preferred for
practical usage.

B PROOF OF THEOREM 1

B.1 RESTATEMENT OF ASSUMPTIONS

**Assumption 5 (Restatement of Assumption 2). Lipschitz properties are assumed on functions**
_L[(1)](θ) and L[(2)](θ)._

_a) L[(1)](θ) function is M_ _-Lipschitz, i.e., for any θ1 and θ2, ∥L[(1)](θ1) −_ _L[(1)](θ2)∥≤_ _M_ _∥θ1 −_ _θ2∥._

_b) ∇θL[(1)](θ) and ∇θL[(2)](θ) are L-Lipschitz, i.e., for any θ1 and θ2, ∥∇θL[(][i][)](θ1) −_
_θL[(][i][)](θ2)_ _L_ _θ1_ _θ2_ (i = 1, 2).
_∇_ _∥≤_ _∥_ _−_ _∥_

_c) ∇θ[2][L][(1)][(][θ][)][ and][ ∇][2]θ[L][(2)][(][θ][)][ are][ ρ][-Lipschitz, i.e., for any][ θ][1][ and][ θ][2][,][ ∥∇][2]θ[L][(][i][)][(][θ][1][)][ −]_
_∇θ[2][L][(][i][)][(][θ][2][)][∥≤]_ _[ρ][∥][θ][1][ −]_ _[θ][2][∥][(][i][ = 1][,][ 2)][. This assumption also holds for stochastic][ ∇][2]θ[L][(1)]N_ [(][θ][)]
_and ∇θ[2][L][(2)]M_ [(][θ][)][.]

**Assumption 6 (Restatement of Assumption 3). Similarly as in Mei et al. (2018), we assume the**
_loss gradient ∇F_ [(1)](θ; ξ) is τ [2]-sub-Gaussian, i.e., for any ϱ ∈ R[p], and θ ∈ _D[r](0) where D[r](0) ≡_
_{θ ∈_ R[p], ∥θ∥2 ≤ _r},_

_τ 2_ _ϱ_ 2
E exp( _ϱ,_ _F_ [(1)](θ; ξ) E[ _F_ [(1)](θ; ξ)] ) exp _∥_ _∥_ _._
_{_ _⟨_ _∇_ _−_ _∇_ _⟩_ _} ≤_ 2
 

_Meanwhile, we assume the loss Hessian is τ_ [2]-sub-exponential, i.e., for any ϱ ∈ _D[1](0), and θ ∈_
_D[r](0),_

1
_ξϱ,θ_ _ϱ,_ _F_ [(1)](θ; ξ)ϱ _,_ E exp 2,
_≡⟨_ _∇[2]_ _⟩_ _τ_ [2][ |][ξ][ϱ,θ][ −] [E][ξ][ϱ,θ][|] _≤_
n   o

_and there exists a constant ch such that L ≤_ _τ_ [2]p[c][h] _, ρ ≤_ _τ_ [3]p[c][h] _._
**Assumption 7 (Restatement of Assumption 4). We assume functions L[(1)](θ) is (ϵ, η)-strongly**
_Morse in D[r](0), i.e., if ∥∇L[(1)](θ)∥2 > ϵ for ∥θ∥2 = r and, for any θ ∈_ R[p], ∥θ∥2 < r, the
_following holds:_


_L(θ)_ 2 _ϵ_ min
_∥∇_ _∥_ _≤_ _⇒_ _i∈[p]_ _[|][λ][i][(][∇][2][L][(1)][(][θ][))][| ≥]_ _[η,]_


-----

_where λi(_ _L[(1)](θ)) denotes the i-th eigenvalue of_ _L[(1)](θ). We further make the assumption_
_∇[2]_ _∇[2]_
_that the local basins D[d](θT[(][i][)][(][φ][∗][))(][i][ = 1][,][ 2)][ of convergence points][ θ]T[(][i][)][(][φ][∗][)(][i][ = 1][,][ 2)][ are in][ D][r][(0)][.]_

B.2 PROOF OF SUPPORTING LEMMAS

**Lemma 1 (Restatement of Theorem 1(b) in Mei et al. (2018)). We assume θ[∗]** _corresponding to_
_θN[∗]_ _[in local basin. Based on Assumptions 2 and 3, there exists a universal constant][ C][0][, and we let]_
_C = C0 max{ch, log(rτ/δ), 1}. If N ≥_ _Cp log p, then we have_

_Cp log N_

sup _LN_ (θ) _L(θ)_ _τ_ [2] _,_
_θ_ _D[p](r)_ _∥∇[2]_ _−∇[2]_ _∥≤_ _N_
_∈_ r

_with probability at least 1 −_ _δ._
**Lemma 2 (Restatement of Theorem 2 in Mei et al. (2018)). Based on Assumptions 2, 3 and 4, we**
_set C as in Lemma 1, assume that θ[∗]_ _is corresponding to θN[∗]_ _[, and let][ N][ ≥]_ [4][Cp][ log][ N/η][2]
_∗_ _[where]_
_η[2]_ _N_ _[and][ θ][∗][, we have]_
_∗_ [= min][{][(][ϵ][2][/τ][ 2][)][,][ (][η][2][/τ][ 4][)][,][ (][η][4][/][(][L][2][τ][ 2][))][}][. Then, for each corresponding][ θ][∗]


_∥θN[∗]_ _[−]_ _[θ][∗][∥][2]_ _[≤]_ [2]η[τ]


_Cp log N_


_with probability at least 1 −_ _δ._
**Lemma 3. Suppose Assumptions 1 and 2 hold. Then, we have**


_L_ _L_ _µ_ _T −T_ _[′]_

_∥θT[(1)][(][φ][∗][)][ −]_ _[θ]N[∗][(1)]∥≤_ s _µ_ _L − + µ_ _∥θT[(1)][′][ (][GD][)][ −]_ _[θ]N[∗][(1)]∥,_ (12)

 

_where T_ _[′]_ _is the minimum that after T_ _[′]_ _gradient descent updates, the updated optimizee parameter_
_θT[(1)][′][ (][GD][)][ locates into the local basin of][ θ]T[(1)][(][φ][∗][)][.]_

_Proof. Since the local basin is µ-strongly convex and θN[∗][(1)]_ is the optimal point of smooth function
_L[(1)]N_ [(][θ][)][ in local basin. Then, we have]

_L[(1)]N_ [(][θ]T[(1)][(][φ][∗][))][ −] _[L]N[(1)][(][θ]N[∗][(1)])_ _T_ [(][φ][∗][)][ −] _[θ]N[∗][(1)]_ _._
_≥_ _[µ]2_ _[∥][θ][(1)]_ _∥[2]_

Furthermore, we rearrange the terms and obtain

2
_θT[(1)][(][φ][∗][)][ −]_ _[θ]N[∗][(1)]_ _N_ [(][θ]T[(1)][(][φ][∗][))][ −] _[L]N[(1)][(][θ]N[∗][(1)]))_
_∥_ _∥≤_ _µ_ [(][L][(1)]
r

(i)
2

_N_ [(][θ]T[(1)][(][GD][))][ −] _[L]N[(1)][(][θ]N[∗][(1)]))_

_≤_ _µ_ [(][L][(1)]
r


(ii)
_≤_

_≤_

(iii)


2 _L_

_T_ [(][GD][)][ −] _[θ]N[∗][(1)]_

_µ_ 2 _[∥][θ][(1)]_ _∥[2]_

_L_
_T_ [(][GD][)][ −] _[θ]N[∗][(1)]_
_µ_ _[∥][θ][(1)]_ _∥_

_L_ _L_ _µ_ _T −T_ _[′]_
_µ_ _L − + µ_ _∥θT[(1)][′][ (][GD][)][ −]_ _[θ]N[∗][(1)]∥,_

 


where (i) follows because φ[∗] = arg minφ LN[(1)][(][θ]T[(1)][(][φ][))][ and][ θ]T[(1)][(][GD][)][ locates in the local basin of]

_θN[∗][(1)], (ii) follows from Assumptioin 2 and the fact that θN[∗][(1)]_ = arg minθ LN[(1)][(][θ][)][, and][ (][iii][)][ follows]
if we set step size of GD as _µ+2_ _L_ [.]

**Lemma 4. Based on Assumptions 1, 2, 3 and 4, we let N** max _Cp log p, 4Cp log N/η[2]_
_C = C0 max_ _ch, 1, log(_ _[rτ]δ_ [)][}][,][ η][2] _τ_ [2][,][ η]τ [2][4][,] _ρη[2]τ[4]_ [2][ }][,][ C] ≥[0][ is an universal constant. Then, with]{ _∗[}][ where]_
_{_ _∗_ [= min][{][ ϵ][2]

_probability at least 1 −_ 2δ we have


-----

_L −_ _µ_ _T −T_ _[′]_

_L + µ_

 


2τ


_Cp log N_


_∥θT[(1)][′][ (][GD][)][ −]_ _[θ]N[∗][(1)]_


_∥∇θ[2][L][(2)][(][θ][∗][(2)][)][∥≤][ρ]_


_Cp log N_


+ ∆[∗]H [+][ τ][ 2]


+ BHessian(λ),


_where ∆[∗]H_ [=][ ∥∇]θ[2][L][(2)][(][θ][∗][(2)][)][ −∇][2]θ[L][(1)][(][θ][∗][(1)][)][∥] _[and][ T][ ′][ is defined in Lemma 3.]_

_Proof. Firstly, we bound ∥∇θ[2][L][(2)][(][θ][∗][(2)][)][∥]_ [as following:]

_∥∇θ[2][L][(2)][(][θ][∗][(2)][)][∥≤∥∇][2]θ[L][(2)][(][θ][∗][(2)][)][ −∇][2]θ[L][(1)][(][θ][∗][(1)][)][∥]_ [+][ ∥∇][2]θ[L][(1)][(][θ][∗][(1)][)][ −∇][2]θ[L][(1)]N [(][θ][∗][(1)][)][∥]

+ ∥∇θ[2][L][(1)]N [(][θ][∗][(1)][)][ −∇]θ[2][L][(1)]N [(][θ]N[∗][(1)])∥ + ∥∇θ[2][L][(1)]N [(][θ]N[∗][(1)]) −∇θ[2][L][(1)]N [(][θ]T[(1)][(][φ][∗][))][∥]

+ ∥∇θ[2][L][(1)]N [(][θ]T[(1)][(][φ][∗][))][∥][,]

where θ[∗][(1)] is corresponding to θN[∗][(1)] in the same local basin of θT[(1)][(][φ][∗][)][.]

Based on the constrained problem formulation in eq. (9), the optimal optimizer parameter φ[∗] is
equivalent to the following:

_φ[∗]_ = arg min _L[(1)]N_ [(][θ]T[(1)][(][φ][))][ subject to][ ∇]θ[2][L][(1)]N [(][θ]T[(1)][(][φ][))][ ≤] _[B][Hessian][(][λ][)][.]_
_φ_

Thus, we obtain ∥∇θ[2][L][(1)]N [(][θ]T[(1)][(][φ][∗][))][∥≤] _[B][Hessian][(][λ][)][. Furthermore, if we let][ N][ ≥]_ _[Cp][ log][ p][ where]_
_C = C0 max{ch, 1, log(_ _[rτ]δ_ [)][}][ and][ C][0][ is an universal constant, based on Lemmas 1, 2 and 3, and]

Assumptions 2, we have


_L −_ _µ_ _T −T_ _[′]_

_L + µ_

 


_θ[∗][(1)]_ _θN[∗][(1)]_ +
_∥_ _−_ _∥_


_∥θT[(1)][′][ (][GD][)][ −]_ _[θ]N[∗][(1)]_


_∥∇θ[2][L][(2)][(][θ][∗][(2)][)][∥≤][ρ]_


_Cp log N_

+ ∆[∗]H [+][ τ][ 2] + BHessian(λ),

_N_

r

with probability at least 1 − _δ._

_η[4]_

Furthermore, if we assume N ≥ max{4Cp log N/η∗[2][, Cp][ log][ p][}][ where][ η]∗[2] [= min][{][ ϵ]τ[2][2][,][ η]τ [2][4][,] _ρ[2]τ_ [2][ }][,]

based on Lemma 2, we have


_L −_ _µ_ _T −T_ _[′]_

_L + µ_

 


2τ


_Cp log N_


_∥θT[(1)][′][ (][GD][)][ −]_ _[θ]N[∗][(1)]_


_∥∇θ[2][L][(2)][(][θ][∗][(2)][)][∥≤][ρ]_


_Cp log N_


+ BHessian(λ) + ∆[∗]H [+][ τ][ 2]


with probability at least 1 − 2δ.

**Lemma 5. Based on Assumptions 1, 2, 3, and 4, we let N** 4Cp log N/η[2]
_≥_ _∗_ _[where][ C][ and][ η]∗[2]_ _[are]_
_defined in Lemma 2. Then, with probability at least 1 −_ _δ, we have_

_L_ _L_ _µ_ _T −T_ _[′]_ _Cp log N_

_∥θT[(2)][(][φ][∗][)][ −]_ _[θ][∗][(2)][∥≤]_ [∆]T[∗] [+] s _µ_  _L − + µ_  _∥θT[(1)][′][ (][GD][)][ −]_ _[θ]N[∗][(1)]∥_ + [2]η[τ] r _N_ + ∆[∗]θ[,]

_where ∆[∗]θ_ [=][ ∥][θ][∗][(1)][ −] _[θ][∗][(2)][∥][,][ ∆]T[∗]_ [=][ ∥][θ]T[(2)][(][φ][∗][)][ −] _[θ]T[(1)][(][φ][∗][)][∥]_ _[and][ T][ ′][ is defined in Lemma 3.]_

_Proof. Based on triangle inequality, we obtain_

_∥θT[(2)][(][φ][∗][)][ −]_ _[θ][∗][(2)][∥]_


-----

_≤∥θT[(2)][(][φ][∗][)][ −]_ _[θ]T[(1)][(][φ][∗][)][∥]_ [+][ ∥][θ]T[(1)][(][φ][∗][)][ −] _[θ]N[∗][(1)]∥_ + ∥θN[∗][(1)] _−_ _θ[∗][(1)]∥_ + ∥θ[∗][(1)] _−_ _θ[∗][(2)]∥_

(i)
_≤_ ∆[∗]T [+][ ∥][θ]T[(1)][(][φ][∗][)][ −] _[θ]N[∗][(1)]∥_ + ∥θN[∗][(1)] _−_ _θ[∗][(1)]∥_ + ∥θ[∗][(1)] _−_ _θ[∗][(2)]∥_

(ii) _L_ _L_ _µ_ _T −T_ _[′]_
_≤_ ∆[∗]T [+] s _µ_ _L − + µ_ _∥θT[(1)][′][ (][GD][)][ −]_ _[θ]N[∗][(1)]∥_ + ∥θN[∗][(1)] _−_ _θ[∗][(1)]∥_ + ∆[∗]θ[,]

 

where (i) follows from definition of ∆[∗]T [,][ (][ii][)][ follows from Lemma 3 and definition of][ ∆]θ[∗][. Based]
on Lemma 2, if we let N 4Cp log N/η[2]
_≥_ _∗[. Then, with probability at least][ 1][ −]_ _[δ][, we have]_

_L_ _L_ _µ_ _T −T_ _[′]_ _Cp log N_

_∥θT[(2)][(][φ][∗][)][ −]_ _[θ][∗][(2)][∥≤][∆]T[∗]_ [+] s _µ_  _L − + µ_  _∥θT[(1)][′][ (][GD][)][ −]_ _[θ]N[∗][(1)]∥_ + [2]η[τ] r _N_ + ∆[∗]θ


_C log N_


=∆[∗]T [+ ∆]θ[∗] [+][ O][(][w][T][ −][T][ ′] [) +][ O][(]


),


where w = _[L]L[−]+µ[µ]_ [.]

B.3 PROOF OF THEOREM 1

Generalization loss is defined as below:


_L[(2)](θT[(2)][(][φ][∗][))][ −]_ _[L][(2)][(][θ][∗][(2)][)]_

(i)
= (θT[(2)][(][φ][∗][)][ −] _[θ][∗][(2)][)][T][ ∇][θ][L][(2)][(][θ][∗][(2)][) + 1]_ _T_ [(][φ][∗][)][ −] _[θ][∗][(2)][)][T][ ∇]θ[2][L][(2)][(][θ][′][)(][θ]T[(2)][(][φ][∗][)][ −]_ _[θ][∗][(2)][)]_

2 [(][θ][(2)]

(= ii) [1] _T_ [(][φ][∗][)][ −] _[θ][∗][(2)][)][T][ ∇]θ[2][L][(2)][(][θ][′][)(][θ]T[(2)][(][φ][∗][)][ −]_ _[θ][∗][(2)][)]_

2 [(][θ][(2)]


= [1] _T_ [(][φ][∗][)][ −] _[θ][∗][(2)][)][T][ (][∇]θ[2][L][(2)][(][θ][′][)][ −∇]θ[2][L][(2)][(][θ][∗][(2)][) +][ ∇]θ[2][L][(2)][(][θ][∗][(2)][))(][θ]T[(2)][(][φ][∗][)][ −]_ _[θ][∗][(2)][)]_

2 [(][θ][(2)]

_T_ [(][φ][∗][)][ −] _[θ][∗][(2)][∥][2][(][∥∇]θ[2][L][(2)][(][θ][′][)][ −∇]θ[2][L][(2)][(][θ][∗][(2)][)][∥]_ [+][ ∥∇]θ[2][L][(2)][(][θ][∗][(2)][)][∥][)]

_≤_ 2[1] _[∥][θ][(2)]_


(iii)

_T_ [(][φ][∗][)][ −] _[θ][∗][(2)][∥][3][ + 1]_ _θ[L][(2)][(][θ][∗][(2)][)][∥∥][θ]T[(2)][(][φ][∗][)][ −]_ _[θ][∗][(2)][∥][2]_

_≤_ 2[1] _[ρ][∥][θ][(2)]_ 2 _[∥∇][2]_

_T_ [(][φ][∗][)][ −] _[θ][∗][(2)][∥][2][(][ρ][∥][θ]T[(2)][(][φ][∗][)][ −]_ _[θ][∗][(2)][∥]_ [+][ ∥∇]θ[2][L][(2)][(][θ][∗][(2)][)][∥][)][,]

_≤_ 2[1] _[∥][θ][(2)]_

where (i) follows from Taylor expansion and θ[′] follows from the conditions that ∥θ[′] _−_ _θT[(2)][(][φ][∗][)][∥≤]_
_∥θ[∗][(2)]_ _−_ _θT[(2)][(][φ][∗][)][∥]_ [and][ ∥][θ][′][ −] _[θ][∗][(2)][∥≤∥][θ][∗][(2)][ −]_ _[θ]T[(2)][(][φ][∗][)][∥][,][ (][ii][)][ follows because][ ∇][θ][L][(2)][(][θ][∗][(2)][) = 0][,]_
and (iii) follows from Assumption 2 and the fact that ∥θ[′] _−_ _θ[∗][(2)]∥≤∥θ[∗][(2)]_ _−_ _θT[(2)][(][φ][∗][)][∥][.]_

Based on Lemmas 4 and 5, if we let N max 4Cp log N/η[2]
_≥_ _{_ _∗[, Cp][ log][ p][}][ where][ C][ and][ η]∗[2]_ [are]
defined in Lemma 4. Then, with probability at least 1 − 2δ, we have

_ρ∥θT[(2)][(][φ][∗][)][ −]_ _[θ][∗][(2)][∥]_ [+][ ∥∇]θ[2][L][(2)][(][θ][∗][(2)][)][∥]

_L_ _L_ _µ_ _T −T_ _[′]_ _Cp log N_

_≤ρ∆[∗]T_ [+] s _µ_  _L − + µ_  _∥θT[(1)][′][ (][GD][)][ −]_ _[θ]N[∗][(1)]∥_ + [2]η[τ] r _N_ + ∆[∗]θ

_Cp log N_ 2τ _Cp log N_ _L_ _L_ _µ_ _T −T_ _[′]_

+ τ [2]r _N_ + ρ _η_ r _N_ + s _µ_  _L − + µ_  _∥θT[(1)][′][ (][GD][)][ −]_ _[θ]N[∗][(1)]∥!_

+ ∆[∗]H [+][ B][(][λ][)]

_L_ _L_ _µ_ _T −T_ _[′]_ 4ρτ _Cp log N_

=ρ∆[∗]T [+ 2][ρ]s _µ_ _L − + µ_ _∥θT[(1)][′][ (][GD][)][ −]_ _[θ]N[∗][(1)]∥_ + _η_ + τ [2] _N_

    [r]

+ ρ∆[∗]θ [+ ∆]H[∗] [+][ B][Hessian][(][λ][)]


_C log N_


=BHessian(λ) + ρ∆[∗]T [+][ ρ][∆]θ[∗] [+ ∆]H[∗] [+][ O][(][w][T][ −][T][ ′] [) +][ O][(]


),


-----

where w = _LL−+µµ_ [,][ ∆]H[∗] [=][ ∥∇]θ[2][L][(2)][(][θ][∗][(2)][)][ −∇][2]θ[L][(1)][(][θ][∗][(1)][)][∥][,][ ∆][∗]θ [=][ ∥][θ][∗][(1)][ −] _[θ][∗][(2)][∥][,][ ∆]T[∗]_ [=]

_∥θT[(2)][(][φ][∗][)][ −]_ _[θ]T[(1)][(][φ][∗][)][∥][.]_


Furthermore, We let A1 = BHessian(λ) + ρ∆[∗]T [+][ ρ][∆]θ[∗] [+ ∆]H[∗] [+][ O][(][w][T][ −][T][ ′] [) +][ O][(]

∆[∗]T [+ ∆]θ[∗] [+][ O][(][w][T][ −][T][ ′] [) +][ O][(] _C logN_ _N_ ). Then, we have
q


_C log N_


), A2 =


_L[(2)](θT[(2)][(][φ][∗][))][ −]_ _[L][(2)][(][θ][∗][(2)][)][ ≤]_ [1] _T_ [(][φ][∗][)][ −] _[θ][∗][(2)][∥][2][(][ρ][∥][θ]T[(2)][(][φ][∗][)][ −]_ _[θ][∗][(2)][∥]_ [+][ ∥∇]θ[2][L][(2)][(][θ][∗][(2)][)][∥][)]

2 _[∥][θ][(2)]_

2[A][1][,]

_≤_ 2[1] _[A][2]_


with probability at least 1 − 2δ. Then, the proof is complete.

C PROOF OF THEOREM 2

C.1 PROOF OF SUPPORTING LEMMA

**Lemma 6. Based on Assumptions 1 and 2, in terms of entropy regularizer constraint BEntropy(λ),**
_we have_

_BEntropy(λ) + mM +_ _[p]2 [log(2][π][) + 1]2_ _[ρm][3][ +][ C][(][γ, p, m][)][ ≥]_ [log(det(][∇][2][L]N[(1)][(][θ]T[(1)][(][φ][)) +][ γI][))]


+ L[(1)]N [(][θ]T[(1)][(][φ][))][,]

_where m is a constant, C(γ, p, m) = log_ Rθ[′]:∥θ[′]−θ∥>m [exp]  − _[γ]2_ _[∥][θ][ −]_ _[θ][′][∥][2][]_ _dθ[′]_ _and θ ∈_ R[p].

_Proof. We firstly split the integral area θ[′]_ _∈_ R[p] into two parts: {θ[′] : ∥θ[′] _−_ _θ∥≤_ _m} and {θ[′]_ :
_∥θ[′]_ _−_ _θ∥_ _> m}. Based on the definition of G[(1)]N_ [(][θ][;][ γ][)][, we have]

_G[(1)]N_ [(][θ][;][ γ][)]

= log _L[(1)]N_ [(][θ][′][)][ −] _[γ]_ dθ[′]

_θ[′][ exp]_ _−_ 2

Z  _[∥][θ][ −]_ _[θ][′][∥][2][]_

= log exp _L[(1)]N_ [(][θ][′][)][ −] _[γ]_ dθ[′]
Zθ[′]:∥θ[′]−θ∥≤m − 2 _[∥][θ][ −]_ _[θ][′][∥][2][]_

+ log exp _L[(1)]N_ [(][θ][′][)][ −] _[γ]_ dθ[′]
Zθ[′]:∥θ[′]−θ∥>m − 2 _[∥][θ][ −]_ _[θ][′][∥][2][]_

(i)
log exp _L[(1)]N_ [(][θ][′][)][ −] _[γ]_ dθ[′]
_≤_ Zθ[′]:∥θ[′]−θ∥≤m − 2 _[∥][θ][ −]_ _[θ][′][∥][2][]_

+ log exp dθ[′]
Zθ[′]:∥θ[′]−θ∥>m − _[γ]2_ _[∥][θ][ −]_ _[θ][′][∥][2][]_

(ii)
= log exp _L[(1)]N_ [(][θ][′][)][ −] _[γ]_ dθ[′] + C(γ, p, m)
Zθ[′]:∥θ[′]−θ∥≤m − 2 _[∥][θ][ −]_ _[θ][′][∥][2][]_

(iii)
= log exp _L[(1)]N_ [(][θ][)][ −] [(][θ][′][ −] _[θ][)][T][ ∇][L]N[(1)][(][θ][)][ −]_ [1] _N_ [(][θ][′′][)(][θ][′][ −] _[θ][)]_

_θ[′]:_ _θ[′]_ _θ_ _m_ _−_ 2 [(][θ][′][ −] _[θ][)][T][ ∇][2][L][(1)]_

Z _∥_ _−_ _∥≤_ 

dθ[′] + C(γ, p, m),

_−_ _[γ]2_

_[∥][θ][ −]_ _[θ][′][∥][2]_

where (i) follows from the fact that L[(1)]N [(][θ][′][)][ is non-negative,][ (][ii][)][ follows from the fact that][ θ][ ∈]
R[p] and the definiton of C(γ, p, m), (iii) follows from Taylor expansion. Note that θ[′′] satisfies
_∥θ[′′]_ _−_ _θ∥≤∥θ −_ _θ[′]∥_ and ∥θ[′′] _−_ _θ[′]∥≤∥θ −_ _θ[′]∥._

Based on Assumption 2, −(θ[′] _−_ _θ)[T]_ _∇L[(1)]N_ [(][θ][)][ ≤] _[m][∥∇][L]N[(1)][(][θ][)][∥≤]_ _[mM]_ [. Then, we obtain]

_G[(1)]N_ [(][θ][;][ γ][)]


-----

exp _N_ [(][θ][′′][)(][θ][′][ −] _[θ][)]_
_θ[′]:_ _θ[′]_ _θ_ _m_ _−_ 2[1] [(][θ][′][ −] _[θ][)][T][ ∇][2][L][(1)]_
_∥_ _−_ _∥≤_ 


_≤−_ _L[(1)]N_ [(][θ][) +][ mM][ + log]


dθ[′] + C(γ, p, m)

_−_ _[γ]2_

_[∥][θ][ −]_ _[θ][′][∥][2]_

= _L[(1)]N_ [(][θ][) +][ mM][ + log] exp _N_ [(][θ][′′][) +][ γI][)(][θ][′][ −] _[θ][)]_ dθ[′]
_−_ _θ[′]:_ _θ[′]_ _θ_ _m_ _−_ [1]2 [(][θ][′][ −] _[θ][)][T][ (][∇][2][L][(1)]_
Z _∥_ _−_ _∥≤_  

+ C(γ, p, m)


= − _L[(1)]N_ [(][θ][) +][ mM][ +][ C][(][γ, p, m][)]

+ log exp _L[(1)]N_ [(][θ][′′][)][ −∇][2][L]N[(1)][(][θ][) +][ ∇][2][L]N[(1)][(][θ][)]

_θ[′]:_ _θ[′]_ _θ_ _m_ _−_ 2[1] [(][θ][′][ −] _[θ][)][T][ ]∇[2]_

Z _∥_ _−_ _∥≤_ 

+ γI (θ[′] _−_ _θ)_ dθ[′]

(i)  
_L[(1)]N_ [(][θ][) +][ mM][ +][ C][(][γ, p, m][) + 1]
_≤−_ 2 _[ρm][3]_

+ log exp _L[(1)]N_ [(][θ][) +][ γI] (θ[′] _θ)_ dθ[′]

_θ[′]:_ _θ[′]_ _θ_ _m_ _−_ 2[1] [(][θ][′][ −] _[θ][)][T][ ]∇[2]_ _−_

Z _∥_ _−_ _∥≤_   

(ii)
_L[(1)]N_ [(][θ][) +][ mM][ + 1] _L[(1)]N_ [(][θ][) +][ γI] (θ[′] _θ)_ dθ[′]
_≤−_ 2 _[ρm][3][ + log]_ _θ[′][ exp]_ _−_ 2[1] [(][θ][′][ −] _[θ][)][T][ ]∇[2]_ _−_

Z   

+ C(γ, p, m),


where (i) follows from Assumption 2 and the fact that ∥θ[′′] _−_ _θ∥≤∥θ_ _−_ _θ[′]∥_ and (ii) follows because
exp(− 2[1] [(][θ][′][ −] _[θ][)][T][ (][∇][2][L]N[(1)][(][θ][) +][ γI][)(][θ][′][ −]_ _[θ][))][ ≥]_ [0][.]

Based on Assumption 1, we have the fact that (∇[2]L[(1)]N [(][θ]T[(1)][(][φ][)) +][ γI][)][ is a symmetric and positive-]
definite matrix. Hence, we obtain

_G[(1)]N_ [(][θ]T[(1)][(][φ][);][ γ][)][ ≤−][L]N[(1)][(][θ]T[(1)][(][φ][))][ −] [log(det(][∇][2][L]N[(1)][(][θ]T[(1)][(][φ][)) +][ γI][)) +][ C][(][γ, p, m][)]

+ mM + [1]

2 _[ρm][3][ +][ p]2 [log(2][π][)][.]_


We rearrange the terms and get

_−G[(1)]N_ [(][θ]T[(1)][(][φ][);][ γ][)][ ≥] _[L]N[(1)][(][θ]T[(1)][(][φ][)) + log(det(][∇][2][L]N[(1)][(][θ]T[(1)][(][φ][)) +][ γI][))][ −]_ _[C][(][γ, p, m][)]_

_−_ _mM −_ _[p]2 [log(2][π][)][ −]_ [1]2 _[ρm][3][.]_


Since −G[(1)]N [(][θ]T[(1)][(][φ][);][ γ][)][ ≤] _[B][Entropy][(][λ][)][. Then, we obtain]_

_BEntropy(λ) + mM +_ _[p]2 [log(2][π][) + 1]2_ _[ρm][3][ +][ C][(][γ, p, m][)][ ≥]_ [log(det(][∇][2][L]N[(1)][(][θ]T[(1)][(][φ][)) +][ γI][))]

+ L[(1)]N [(][θ]T[(1)][(][φ][))][.]


C.2 PROOF OF THEOREM 2

Based on Lemma 6, we have

_BEntropy(λ) + mM +_ _[p]2 [log(2][π][) + 1]2_ _[ρm][3][ +][ C][(][γ, p, m][)][ ≥]_ [log(det(][∇][2][L]N[(1)][(][θ]T[(1)][(][φ][)) +][ γI][))]


+ L[(1)]N [(][θ]T[(1)][(][φ][))][.]

Since ∇[2]L[(1)]N [(][θ]T[(1)][(][φ][)) +][ γI][ is positive definite and][ λ][i][(][∇][2][L]N[(1)][(][θ]T[(1)][(][φ][)) +][ γI][)][ ≥] _[γ][ +][ µ][ for]_
any i = 1, . . ., p. Then, based on the definition of Matrix norm ∥∇[2]L[(1)]N [(][θ]T[(1)][(][φ][)) +][ γI][∥] [=]


-----

_λmax(∇[2]L[(1)]N_ [(][θ]T[(1)][(][φ][)) +][ γI][)][, we have]

_∥∇[2]L[(1)]N_ [(][θ]T[(1)][(][φ][)) +][ γI][∥][p][ ≥] [det(][∇][2][L]N[(1)][(][θ]T[(1)][(][φ][)) +][ γI][))][ ≥] [(][γ][ +][ µ][)][p][−][1][∥∇][2][L]N[(1)][(][θ]T[(1)][(][φ][)) +][ γI][∥][.]

Note that we use λi(H) to denote the i-th eigenvalue of matrix H. Then,

log(det(∇[2]L[(1)]N [(][θ]T[(1)][(][φ][)) +][ γI][)))][ ≥][(][p][ −] [1) log(][γ][ +][ µ][) + log][ ∥∇][2][L]N[(1)][(][θ]T[(1)][(][φ][)) +][ γI][∥]

=(p − 1) log(γ + µ) + log(∥∇[2]L[(1)]N [(][θ]T[(1)][(][φ][))][∥] [+][ γ][)][.]

Then, we can obtain


_BEntropy(λ)+mM +_ _[p]2 [log(2][π][) + 1]2_ _[ρm][3][ +][ C][(][γ, p, m][)][ ≥]_ _[L]N[(1)][(][θ]T[(1)][(][φ][)) + (][p][ −]_ [1) log(][γ][ +][ µ][)]

+ log(∥∇[2]L[(1)]N [(][θ]T[(1)][(][φ][))][∥] [+][ γ][)][.]

Hence, we can get a new function D(x) that


_∥∇[2]L[(1)]N_ [(][θ]T[(1)][(][φ][))][∥≤] _[D][−][1][(][B][Entropy][(][λ][))][,]_

where D(x) = L[(1)]N [(][θ]T[(1)][(][φ][)) + (][p][ −] [1) log(][γ][ +][ µ][)][ −] _[mM][ −]_ _[p]2_ [log(2][π][)][ −] 2[1] _[ρm][3][ −]_ _[C][(][γ, p, m][) +]_

log(x + γ). Then, the proof is complete.


-----

