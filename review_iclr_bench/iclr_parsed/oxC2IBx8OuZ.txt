# TOWARDS FEDERATED LEARNING ON TIME- EVOLVING HETEROGENEOUS DATA

**Anonymous authors**
Paper under double-blind review

ABSTRACT

Federated Learning (FL) is an emerging learning paradigm that preserves privacy
by ensuring client data locality on edge devices. The optimization of FL is challenging in practice due to the diversity and heterogeneity of the learning system.
Despite recent research efforts on improving the optimization of heterogeneous
data, the impact of time-varying heterogeneous data in real-world scenarios, such
as changing client data or intermittent clients joining or leaving during training,
has not been well studied. In this work, we propose Continual Federated Learning
(CFL), a flexible framework, to capture the time-varying heterogeneity of FL.
CFL covers complex and realistic scenarios—which are challenging to evaluate
in previous FL formulations—by extracting the information of past local datasets
and approximating the local objective functions. Theoretically, we demonstrate
that CFL methods achieve a faster convergence rate than FedAvg in time-varying
scenarios, with the benefit being dependent on approximation quality. In a series of
experiments, we show that the numerical findings match the convergence analysis,
and CFL methods significantly outperform the other SOTA FL baselines.

1 INTRODUCTION

Federated Learning (FL) has recently emerged as a critical distributed machine learning paradigm to
preserve user/client privacy. Clients engaged in the training process of FL only communicate their
local model parameters, rather than their private local data, with the central server.
As the workhorse algorithm in FL, FedAvg (McMahan et al., 2017) performs multiple local stochastic
gradient descent (SGD) updates on the available clients before communicating with the server.
Despite its success, FedAvg suffers from the large heterogeneity (non-iid-ness) in the data presented
on the different clients, causing drift in each client’s updates and resulting in slow and unstable
convergence (Karimireddy et al., 2020b). To address this issue, a new line of study has been
suggested lately that either simulates the distribution of the whole dataset using preassigned weights
of clients (Wang et al., 2020; Reisizadeh et al., 2020; Mohri et al., 2019; Li et al., 2020a) or adopts
variance reduction methods (Karimireddy et al., 2020b;a; Das et al., 2020; Haddadpour et al., 2021).
However, the FL formulation in these approaches always assumes a fixed data distribution among
clients throughout all training rounds, while in practice this assumption does not always hold because
of the complex, uncontrolled, and unpredictable client behaviors. Local datasets, for example, often
vary over time, and intermittent clients can join or depart during training without prior notice.
The difficulty in addressing the time-varying heterogeneity of FL lies in the stateless nature of the local
datasets, which includes the unpredictable future datasets and the impossibility of retaining all prior
local datasets. To this end, we first provide a novel Continual Federated Learning (CFL) formulation
and then propose a unified CFL framework as our solution. This framework encompasses a variety of
design choices for approximating the local objective functions in the previous training rounds, with
the difference between actual and estimated local object functions described as information loss for
further analysis.
To capture the time-varying data heterogeneity in the CFL framework, we expand the theoretical
assumption on the client drift—which has been extensively utilized recently in Karimireddy et al.
(2020b); Khaled et al. (2020); Li et al. (2020b)—to include both client drift and time drift. This allows
us to quantify the difference between local client objective function and global objective function for
the considered time-varying heterogeneous clients.
We provide convergence rates for our unified CFL framework in conjunction with information loss and
new models of both client and time drifts. Our analysis reveals a faster convergence of CFL framework
than FedAvg, with the benefit dependent on approximation quality. The rate of FedAvg obtained from


-----

our framework is consistent with previous work, while a similarly simplified rate on Continual Learning (CL) (French, 1999; Kirkpatrick et al., 2017) for the convex and strongly convex cases is novel.
Finally, in extensive empirical results, we demonstrate that CFL methods—stemmed from the CFL
framework with different approximation techniques—significantly outperform the SOTA FL competitors on various realistic datasets. These numerical observations corroborate our theoretical findings.

**Contribution.** We summarize our key contributions below.

-  We present a unified framework, termed Continual Federated Learning (CFL), together with a
novel client and time drift modeling approach, to capture complex FL scenarios involving timevarying heterogeneous data. This is the first theoretical study, to our knowledge, that describes the
time-varying nature of FL.

-  We provide rigorous convergence analysis for the CFL methods. Our theoretical analysis explains
the faster and more stabilized optimization of the CFL methods over that of FedAvg, and conjecture
their variance reduction effect. In addition, we provide tight convergence rates for standalone CL
methods on convex and strongly-convex problems: to the best of our knowledge, we are the first
to provide such guarantees for CL on SGD.

-  We examine several approximation techniques for CFL framework: the insights therein offer a
valuable practical guideline. We demonstrate the efficacy and necessity of CFL methods over the
SOTA FL baselines across a range of time-varying heterogeneous data scenarios and datasets.

2 RELATED WORK


**Federated Learning.** FedAvg (McMahan et al., 2017; Lin et al., 2020b) is the de facto standard
FL algorithm, in which multiple local SGD steps are executed on the available clients to alleviate
the communication bottleneck. While communication efficient, heterogeneity, such as system
heterogeneity (Li et al., 2018; Wang et al., 2020; Mitra et al., 2021; Diao et al., 2021) and
statistical/objective heterogeneity (Li et al., 2018; Wang et al., 2020; Mitra et al., 2021; Lin et al.,
2020a; Karimireddy et al., 2020b;a), results in inconsistent optimization objectives and drifted clients
models, impeding federated optimization considerably.
A line of work has been proposed to address the heterogeneity in FL. FedProx (Li et al., 2018)
adds the regularization term on the distance of local and global models when performing local
training—similar formulations can be found in other recent FL works (Hanzely & Richtárik, 2020;
Dinh et al., 2020; Li et al., 2021) for various purposes. To address the issue of objective heterogeneity
e.g. caused by heterogeneous data, works like SCAFFOLD (Karimireddy et al., 2020b; Mitra et al.,
2021) introduce the idea of variance reduction on the client local update steps. FedNova (Wang et al.,
2020) further proposes a general framework for unifying FedAvg and FedProx, and argues that while
averaging, local updates should be normalized to minimize heterogeneity induced by different number
of local update steps. However, most of these prior works focus on the fixed heterogeneity across
clients and throughout the entire optimization procedure; we instead consider the novel scenario with
time-varying data heterogeneity.
The theoretical study on the convergence of FedAvg can date back to the parallel SGD analysis on
the identical functions Zinkevich et al. (2010) and recently is improved by Stich (2019); Stich &
Karimireddy (2020); Patel & Dieuleveut (2019); Khaled et al. (2020); Woodworth et al. (2020b). For
the analysis of heterogeneous data, Li et al. (2020b) first give the convergence rate of FedAvg on
non-iid datasets with random selection, assuming that the client optimum are ϵ-close. Woodworth
et al. (2020a); Khaled et al. (2020) give tighter convergence rates under the assumption of bounded
gradient drift. All above works give a O (1/T ) convergence rate for convex local objective functions.
More recently, Karimireddy et al. (2020b); Koloskova et al. (2020) give the convergence analysis of
local SGD for non-convex objective functions under bounded gradient noise assumptions, and obtain
a O(1/√T ) convergence rate. Our theoretical analysis framework covers more challenging time
varying data heterogeneity in FL, which has not been considered in the community yet—our rate can
be simplified to the standard FL scenario, matching the tight analysis in Karimireddy et al. (2020b).

**Continual Learning.** Continual learning, also known as incremental learning or lifelong learning, aims to learn from (time-varying) sequential data while avoiding the problem of catastrophic
_forgetting (French, 1999; Kirkpatrick et al., 2017). There exists a large amount of works, from_
the perspectives of regularization (Kirkpatrick et al., 2017; Li & Hoiem, 2017; Zenke et al., 2017),
experience replay (Castro et al., 2018; Rebuffi et al., 2017), and dynamic architectures (Maltoni &
Lomonaco, 2019; Rusu et al., 2016). In this paper, we examine both regularization and experience
replay based methods, and compare their empirical performance in depth.


-----

Despite the empirical success, the theoretical analysis of CL is limited: only a very recent preprint (Yin
et al., 2020b) provides a viewpoint of regularization-based continual learning, and only for the single
worker scenario. In this work, we provide tight convergence analysis for both CL and CFL on SGD.

**Continual Federated Learning.** To our knowledge, the scenario of CFL was originally described
in Bui et al. (2018) in order to federated train Bayesian Neural Network and continually learn for
Gaussian Process models—it is orthogonal to our optimization aspect in this paper.
FedCurv (Shoham et al., 2019) blends EWC (Kirkpatrick et al., 2017) regularization with FedAvg,
and empirically shows a faster convergence. FedWeIT (Yoon et al., 2021) extends the regularization
idea, with a focus on selective inter-client knowledge transfer for task-adaptive parameters. In a very
recent parallel (and empirical) work, CDA-FedAvg (Casado et al., 2021) uses a locally maintained
long-term data samples, with asynchronous communication; however, this design choice lacks
sufficient numerical results to justify the performance gain (even over FedAvg) and it cannot be
applied to more general scenarios. Our theoretically sound CFL framework covers the regularization
component of FedCurv and the core set part of CDA-FedAvg, and is orthogonal to the neural
architecture manipulation idea in FedWeIT.

3 CONTINUAL FEDERATED LEARNING FRAMEWORK

3.1 FORMULATION
**Conventional FL Formulation.** The standard FL typically considers a sum-structured distributed
optimization problem as below:

_f_ _[⋆]_ = minω∈Rd _f_ (ω) := _i=1_ _[p][i][f][i][(][ω][)]_ _,_ (1)
h i

where the objective function f (ω) : R[d] R is the weighted sum of the local objective functions
_→_ [P][N]
_fi(ω) := E_ _i [Fi(ω)] of N nodes/clients, and pi is the weight of client i._
_D_
In practice, it may be infeasible to select all clients each round, especially for cross-device
setup (McMahan et al., 2017; Kairouz et al., 2019). The standard FedAvg then randomly selects S
clients to receive the model parameters from the server (S ≤ _N_ ) in each communication round and
performs K local SGD update steps in the form of ωt,i,k = ωt,i,k 1 _ηl(_ _fi(ωt,i,k_ 1) + νt,i,k 1),
with local step-size ηl and gradient noise νt,i,k 1. The selected clients then communicate the updates− _−_ _∇_ _−_ _−_
∆ωt,i = ωt,i,K − **_ωt with the server for the model aggregation:−_** **_ωt+1 = ωt −_** _[η]S[g]_ _Si=1_ [∆][ω][t,i][.]

**Continual FL Formulation.** Despite its wide usage, the standard FL formulation given in EquationP
(1) cannot properly reflect actual time-varying scenarios, such as local datasets changing over time
or intermittent clients joining or leaving during training. To address this problem, we propose the
Continual Federated Learning (CFL) formulation:

_f_ _[⋆]_ = minω∈Rd _f_ (ω) := _t=1_ _i∈St_ _[p][t,i][f][t,i][(][ω][)]_ _,_ (CFL)
h i

where ft,i(ω) represents the local objective function of clientP _i at time t._ _t is a subset of clients_

[P][T] _S_

sampled from all clients set Ω, where _t_ = S. For the time-varying scenarios, client i could have
_|S_ _|_
different local objective functions ft,i(ω) due to the changing local datasets on different t.

3.2 APPROXIMATION OF CFL
The challenge of addressing (CFL) formulation stems from the stateless nature of the local datasets,
which include unpredictable future datasets, as well as the difficulty of keeping all the previous local
datasets. The original definition of (CFL) is theoretically and empirically infeasible. To handle the
second case (while ignoring the intractable former), a straightforward approach is to approximate
the prior local objective functions (Kirkpatrick et al., 2017; Zenke et al., 2017; Li & Hoiem, 2017),
which may be accomplished by retraining information from earlier rounds in accordance with privacy
protection standards. Thus, we can express the approximated CFL formulation as:

_f˜t⋆_ = minω∈Rd _f˜t(ω) :=_ _i∈St_ _[p][t,i][f][t,i][(][ω][) +][ P][t]τ[−]=1[1]_ _i∈St_ _[p][τ,i][ ˜]fτ,i(ω)_ _,_ (2)
h i

where _f[˜]t,i(ω) denotes the approximated local objective function of clientP_ _i at time t. In practice,_

[P]

different approximation methods can be used to calculate _f[˜]τ,i(ω), and we refer the detailed illustration_
and discussions of these approximation algorithms in Section 5.1, Section 5.2, and Appendix C.2.2
We give the formal definition of CFL framework in Algorithm 1. CFL methods are a collection of
methods that make use of different approximation techniques and are based on Algorithm 1. We
retrieve the objective function of conventional Continual Learning (CL), by setting S = 1 in (2).
Take note that in most cases, the previous local object functions cannot be properly approximated.
Due to the fact that such approximation impairs optimization, we define the information loss below.


-----

**Algorithm 1 Approximated CFL Framework**
**Require: initial weights ω0, global learning rate ηg, local learning rate ηl, number of training rounds T**
**Ensure: trained weights ωT**

1: for round t = 1, . . ., T do
2: **communicate ωt to the chosen clients.**

3: **for client i ∈St in parallel do**

4: initialize local model ωt,i,0 = ωt.

5: **for k = 1, . . ., K do**

6: _g˜t,i,k(ωt,i,k−1) =_ _pt,i∇ft,i(ωt,i,k−1) +_ _τ_ =1 _[p][τ,i][∇]f[˜]τ,i(ωt,i,k−1)_ + νt,i,k.

8:7: **communicateωt,i,k ←** **_ω ∆t,i,kωt,i−1 ← −_** **_ωηlt,i,Kg˜t,i,k −(ωωt,i,kt._** _−1)._ [P][t][−][1] 

9: ∆ωt ← _[η]S[g]_ _i∈St_ [∆][ω][t,i][.]

10: **_ωt+1_** **_ωt + ∆ωt._**
_←_ P

**Definition 3.1 (Information Loss). We define the information loss as the difference between the**
_approximated local objective function_ _f[˜]t,i(ω) and the real local objective function ft,i(ω), i.e._

∆t,i(ω) = ∇ft,i(ω) −∇f[˜]t,i(ω) . (3)

In practice, we use ∥∆t,i(ω)∥2 to measure the information loss of different approximation methods.
We show large information loss can impede the convergence theoretically (c.f. Theorem 4.1) and
empirically (e.g. Figure 1 in Section 5.2).

3.3 GRADIENT NOISE MODEL
To analyze Equation (2) and Algorithm 1 in-depth, we propose the Gradient Noise Model (Equation
(5)) to capture the dynamics of performing SGD with data heterogeneity. We first recap the standard
definition of gradient noise in Sammut & Webb (2010); Gower et al. (2019).

**Gradient noise in SGD.** For objective function f (ω), the gradient with stochastic noise can be
defined as ∇f (ω) = g(ω) + ν, where g(ω) is the stochastic gradient, and ν is a zero-mean noise.

**Client drift in FL.** To analyze the impact of data heterogeneity, recent works (Karimireddy et al.,
2020b; Khaled et al., 2020; Li et al., 2020b) similarly use the gradient noise to capture the distribution
drift between local objective function and global objective function:

_∇f_ (ω) = ∇fi(ω) + δi, (4)

where δi is a zero-mean random variable which measures the gradient noise of client i.

**Client drift and time drift in CFL framework.represent client i at time t. Considering the distribution drift in the dimension of client and time, weIn (CFL), we use {t, i} pair instead of i to**
further modify the gradient noise model in FL as,

_∇f_ (ω) = ∇ft,i(ω) + δt,i + ξt,i . (5)

Note that we extend the gradient noise to two terms: δt,i and ξt,i. δt,i is a time independent (of t),
zero-mean random variable that measures the drift of client i. The zero-mean ξt,i measures the drift of
client i at time t: this value of client i may change over time t, due to the time-varying local datasets.
**Remark 3.2. We assume different ξt,i are independent to simplify the analysis. We also empirically**
_examine the performance of CFL methods under overlapped time-varying local data in Section 5.2._

3.4 ASSUMPTIONS
To ease the theoretical analysis of (CFL) framework, we use the following widely used assumptions.
**Assumption 1 (Smoothness and convexity). Assume local objective functions ft,i(ω) are L-smooth**
_and µ-convex, i.e._ _[µ]2_ 2

_[∥][ω][1][ −]_ **_[ω][2][∥][2][ ≤]_** _[f][t,i][(][ω][1][)][ −]_ _[f][t,i][(][ω][2][)][ −⟨∇][f][t,i][(][ω][2][)][,][ ω][1][ −]_ **_[ω][2][⟩≤]_** _[L]_ _[∥][ω][1][ −]_ **_[ω][2][∥][2][.]_**

A widely used corollary is that if a function ft,i is both L-smooth and µ-convex, then it satisfies
1

2L

_[∥∇][f][t,i][(][x][)][ −∇][f][t,i][(][y][)][∥][2][ ≤]_ _[f][t,i][(][x][)][ −]_ _[f][t,i][(][y][)][ −∇][f][t,i][(][x][)][T][ (][x][ −]_ **[y][)][, and][ L][ ≥]** _[µ][.]_

**Assumption 2 (Bounded noise in stochastic gradient). Let gt,i,k(ω) = ∇ft,i,k(ω) + νt,i,k, where**
**_νt,i,k is the stochastic noise of client i on round t at k-th local update step. We assume that_**

E [ν|ω] = 0, and E _∥ν∥[2]_ _|ω_ _≤_ _σ[2]._
h i


-----

Table 1: Number of communication rounds required to reach ε + ϕ accuracy for µ strongly convex and
general convex functions under time-varying FL scenarios (Assumption 1–4). We can recover the rates in
conventional FL setups by setting D = 0 and A = 0. Note that ϕ = 0 in FedAvg. Our convergence rate of
FedAvg matches the results in Karimireddy et al. (2020b). Our SGD rate of CL on strongly-convex case is novel.

**Algorithm** **Strongly Convex** **General Convex**


**SGD**
_σ[2]_

_µNKε_ [+][ 1]µ

**FedAvg**
Li et al. (2020b) _µ[2]σNKε[2]_ [+][ (][G][2][+]µ[2][D]ε[2][)][K]

Khaled et al. (2020) _σ[2]+µNKεG[2]+D[2]_ + _[σ][+]µ[G][√][+]ε_ _[D]_ + _[N]_ [(][A][2]µ[+][B][2][)]


_σ[2]_

_NKε[2][ +][ 1]ε_

- 

_σ[2]+NKεG[2]+[2]D[2]_ + _[σ][+]µε[G][+]23_ _[D]_ + _[N]_ [(][A][2]ε[+][B][2][)]

_KNεσ[2]_ [2][ +][ G][+]3[D] + _cpB (Dε[2]+G[2])_


Karimireddy et al. (2020b) _µNKεσ[2]_ [+][ G]µ[+][√][D]ε [+] _cpBµ_

Woodworth et al. (2020a) _√Kc[√]pBµϵ )_ [+] _σNKϵ[2]cpB[2][ +][ G]µ[+][√][D]ϵ_ [+]


_ε_

_cpB_ _σ[2]cpB_ (G+D)cpB

_Kϵ_ [+] _NKϵ[2][ +]_ 32


+ _√σcKϵpB_

_σ_

_K_ [+][G][+][D]


_Kϵ_


_σKµ[+][√][G]ε[+][D]_ + _cpBµ_


Ours _µNKεσ[2]_ [+][ G][2]µε[+][D][2]


_cpB_


_σ[2]_

_NKε[2][ +][ G][2]ε[+][2][D][2]_


**CLYin et al. (2020b)** _µε_ [(GD)]

Ours 1+µA[2] + _µKϵσ[2]_ [+][ c]µϵ[A] [+]

**CFL**


_σ[2]_

_K_ [+][c][A]

_µ[2]ϵ_


_cB_ _cpB +[√]cB_ + _√[3]_ _cB_

_µ[2]ϵ_ _ε_


_σ[2]_

_K_ [+][c][A]

_ε[3]_

_σ[2]_

+ _K_ [+][c]ε[A][3] [+][G][2]

r


_σ[2]_

_Kε[2][ +][ c]ε[A][2][ +]_

r

_σ[2]_

_NKε[2][ +][ c][A]ε[+][2][G][2]_


_cB_

_µϵ_ [+]


_σ[2]_

_K_ [+][c][A][+][G][2]

_µ[2]ϵ_


_cB_ _cpB +[√]cB_ + _√[3]_ _cB_

_µ[2]ϵ_ _ε_


_σ[2]_

_µNKϵ_ [+][ c][A]µϵ[+][G][2]


Ours _cpBµ_


_cB_

_µϵ_ [+]


_D[2]R[2]_ _D[6]_
_cpB = 1 + B[2]_ + A[2], cA = _R[2]+D[2][,][ c][B][ =]_ (R[2]+D[2])[2][ .][ N][ is the number of chosen clients in each round, and]

_K is the number of local iterations._

Assumption 1 and 2 are common assumptions in FL (Karimireddy et al., 2020b; Li et al., 2020b).
Considering a relaxed assumption like the bounded noise at optimum in Khaled et al. (2020) will be
an interesting future research direction. Besides, we introduce the following assumptions to better
characterize the client and time drifts in CFL framework (i.e. (5)).
**Assumption 3 (Bounded gradient drift of CFL framework). Let f** (ω) = ∇ft,i(ω)+δt,i +ξt,i, where
**_δt,i measures client drift and ξt,i indicates the time drift of {t, i}. We assume E [δ|ω] = 0, E [ξ|ω] =_**

0, and thus E _∥δ∥[2]_ _|ω_ _≤_ _G[2]_ + B[2]E∥∇f (ω)∥[2] _and E_ _∥ξ∥[2]_ _|ω_ _≤_ _D[ˆ]_ [2] + A[ˆ][2]E∥∇fi(ω)∥[2].
h i h i

We can derive the corollary from Assumption 3 that E _∥ξ∥[2]_ _|ω_ _≤_ _D[ˆ]_ [2] + A[ˆ][2]G[2] + A[ˆ][2]B[2]E∥∇f (ω)∥[2],

which can simplified to E _∥ξ∥[2]_ _|ω_ _≤_ _D[2]_ + _A[2]E∥∇h_ _f_ (ω)∥[2]i, by assuming A[2] = A[ˆ][2]B[2], D[2] =
_Dˆ_ [2] + A[ˆ][2]G[2]. h i

Assumption 3 assumes the bounded E _∥δ∥[2]_ _|ω_ and E _∥ξ∥[2]_ _|ω_ in CFL framework, which is

equivalent to the widely used (G, B) gradient drift assumptionh i h [1] (Gower et al., 2019; Karimireddyi

**Assumption 4et al., 2020b) on the bounded (Bounded information loss) E** h∥∇fi(ω).∥ We assume the information loss[2][i]. We prove this claim in Appendix B.1. ∆t,i(ω) can be bounded
_by an arbitrary non-negative value R, i.e._ ∆t,i(ω) _R._
_∥_ _∥≤_
**Remark 3.3. The exact information loss in Assumption 4 may be intractable for some approximation**
_methods. In Lemma B.3 of Appendix B.3, we showcase a tight analysis of information loss for Taylor_
_extension based regularization methods._

4 THEORETICAL RESULTS

In this section, we analyze the theoretical performance of Algorithm 1 under Assumption 1–4. We
prove that CFL methods converges faster than FedAvg, despite that the term (refer to ϕt in Theorem
4.1) induced by information loss concurrently trades off the optimization. Additionally, our results
include the FedAvg rate (and match the prior work) and a novel convergence rate for SGD on CL.
In Table 1, we summarize the convergence rate of different algorithms[2]. The proof details refer to
Appendix B.4 and Appendix B.5.

4.1 CONVERGENCE RATE OF CFL METHODS
We start our analysis of Algorithm 1 from the one round progress of CFL methods in the Theorem
stated below.

12TheTo match the notations of prior works that include all clients in all training rounds, we use the abbreviation (G, B) gradient drift: E _∥∇fi(ω)∥[2][]_ _≤_ _G[2]E_ _∥∇f_ (ω)∥[2][] + B[2], where G[2] _≥_ 1 and B[2] _≥_ 0.
 
_N to denote the number of selected clients in each round in this section._


-----

**Theorem 4.1let η = Kηgη (One round Progress of CFL methods)l, we have the one round progress of CFL methods as,. When {ft,i(ω)} satisfy Assumption 1–4, and**

_f_ (ωt) _f_ (ω[∗]) + c1η + c2η[2] +ϕt, (6)
_−_ _≤_ _η[1]_ [(1][ −] _[µη]2 [)][E][∥][ω][t][ −]_ **_[ω][∗][∥][2][ −]_** _η[1]_ [E][∥][ω][t][+1][ −] **_[ω][∗][∥][2]_**

_A2_

_A1_

| {z }

| _LcpG_ {z _N_ }

_where c1 = 2cpG_ + _NK[σ][2]_ [+2][c][R][,][ c][2][ =] 3ηg[2] [+][ Lσ]6ηg[2][K][2] [ +][ Lc]3η[R]g[2] _[,][ c][p][G][ =][ 1]N_ _i=1_ _[G][2][+][D][2][(][P]τ[t]_ =1 _[p]τ,i[2]_ [)][,][ c][R][ =]

_N1_ _Ni=1[(][P][t]τ[−]=1[1]_ _[p][τ,i][)][2][R][2][.][ ϕ][t][ is a constant satisfying][ ϕ][t][ =][ O]_ _N1P_ _Ni=1_ _tτ−=11_ _[p][t,i][R][ ∥][ω][0][ −]_ **_[ω][∗][∥]_** _._

_N is the number of chosen clients in each round, and K is the number of local iterations._ 

P P P

Theorem 4.1 shows the one round progress of CFL methods. The A1 part of (6) indicates the
linear convergence rate, while the A2 part illustrates the worsened convergence induced by gradient
noise and information loss. ϕt is a constant related to the information loss that causes the drift of
the optimum. When the upper limit on information loss R exceeds 0, we cannot reach the exact
optimum due to the approximation error: the iterates instead reach the neighborhood of ϵ + ϕ, where

_T_

_ϕ =_ _t=1T_ _[q][t][ϕ][t]_ for some sequence qt. We show in Section 5 that in practice, the approximation

P _t=1_ _[q][t]_
methods with small information loss converge much better than methods with larger information loss.P

**Theorem 4.2 (Convergence rate of CFL methods). Assume {ft,i(ω)} satisfy Assumption 1–**
_4, the output of Algorithm 1 has expected error smaller than ϵ + ϕ, for ηg_ = 1, ηl
_≤_
_√3+4(1+6BKL[2]+√A1+[2])−B[√][2]+4(1+A[2]_ _B[2]+A[2])_ _, pτ,i =_ _tD[2]+(Dt−[2]_ 1)R[2][ (][τ < t][), and][ p][t,i][ =][ (]tD[t][−][2]+([1)][R]t−[2][+]1)[D]R[2][2][ on round][ t][.]

_When_ _ft,i(ω)_ _are µ-strongly convex functions, we have,_
_{_ _}_


_σ[2]_

_µNKϵ_ [+][ c]µϵ[A] [+]


_T =_ _LcµO_
_O_



_cB_

_µϵ_ [+]


_cC_

_µ[2]ϵ_ [+]


_µcD[2]ϵ_ _,_ (7)



_and when_ _ft,i(ω)_ _are general convex functions (µ = 0), we have_
_{_ _}_

_T =_ _cO_ _F +[√]cBϵF +_ _√[3]_ _cD_ _F_ [2] + _NKϵσ[2]F[2][ +][ c][A]ϵ[2][F]_ +
_O_



_cC_ _F_ [2]

_ϵ[3]_


_,_ (8)

_Tt=1_ [E] h∥∇f (ωt)∥[2][i]
P


_and when {ft,i(ω)} are non-convex, setting η_ = _Kηgηl =_

_converge to ϵ, we have_


_KN_

_T L_ _[, and when][ 1]T_


_√KNc[2]R1_

_L_




2



_T = O_  _NKc[2]m[ϵ][2]_ + ϵ[1][2] _√KNcL_ [2]R1 + _√σNK[2]_ 2 + _√KNccmϵLR2_ + [(][KN](ϵL[)])31 c23 _R3_ 2  _,_ (9)

 

 _[L][2][(][f][0][ −]_ _[f][∗][)][2]_ 

_D[2]R[2]_ _D[6]_
_where cO = 1 + A[2]_ + B[2], cA = G[2] + _R[2]+D[2][,][ c][B][ =]_ (R[2]+D[2])[2][,][ c][C][ =][ Lσ]ηg[2][K][2][ +][ Lc]ηg[2][A] _[,][ c][D][ =][ Lc]ηg[2][B]_ _[,]_

_R[2]_ _D[4]_
_cR1 =_ _R[2]+D[2][,][ c][R][2][ =]_ (R[2]+D[2])[2][,][ c][m][ is a constant related to][ A][,][ B][ and][ R][,][ and][ F][ =][ ∥][ω][0][ −] **_[ω][∗][∥][2][.][ N]_**

_is the number of chosen clients in each round, and K is the number of local iterations. We elaborate_
_the choice of pt,i in Appendix B.4.3._

**Remark 4.3. When setting N = 1 in Theorem 4.2, we recover the convergence rate of standalone**
_CL methods. To the best of our knowledge, we are the first to provide such theoretical guarantees for_
_SGD: the recent work (Yin et al., 2020a) only gives the convergence rate of GD for regularization_
_based CL methods on general convex case, under the constraint of R = 0._

**Remark 4.4. For stateless FL scenarios (clients only appear once during training), we can derive**
_tighter bounds than that of Theorem 4.2. Applying Lemma B.2 to the considered stateless scenario,_

(D[2]+G[2])[3] _cB_

_cA and cB in Theorem 4.2 become cA′ =_ [(]G[G][2][2]+[+]D[D][2][2]+[)]R[R][2][2][ and][ c][B][′][ =] (R[2]+D[2]+G[2])[2][, where][ c]µϵ[A] [+] _µϵ_ _[≥]_

_cA′_ _cB′_ _cB_ q

_µϵ_ [+] _µϵ_ _[and][ c]µϵ[A]_ [+] _µϵ_ _[corresponds to the rate of strongly convex setting in Theorem 4.2.]_
q q

4.2 CONVERGENCE RATE OF FEDAVG UNDER TIME-VARYING SCENARIOS
To show the faster convergence of CFL methods than FedAvg, we provide the convergence rate of
FedAvg under time-varying scenarios. Results are derived from Theorem 4.1, by setting pt,i = 1.
Note that when pt,i = 1, we can set ϕt to 0.


-----

_Assumption 1–4, the output of FedAvg has expected error smaller thanTheorem 4.5√3+4(1+B[2]+A (Convergence rate of FedAvg under time-varying scenarios)[2])_ 4(1+B[2]+A[2]) **. Assume ϵ, for η {g = 1ft,i(ω and)} satisfy ηl ≤**

6KL√1+−B[√][2]+A[2] _. When_ _ft,i(ω)_ _are µ-strongly convex functions, we have,_

_{_ _}_


_σ[2]_

_µNKϵ_ [+][ c]µϵ[A] [+]


_T =_ _LcµO_
_O_



_µc[2]Cϵ_ _,_ (10)



_and when_ _ft,i(ω)_ _are general convex functions (µ = 0), we have_
_{_ _}_


_σ[2]F_

_NKϵ[2][ +][ c][A]ϵ[2][F]_


_cC_ _F_ [2]

_ϵ[3]_


_T =_ _cOϵF_
_O_



(11)


_where cO = 1 + A[2]_ + B[2], cA = G[2] + D[2], cC = _η[Lσ]g[2][K][2][ +][ Lc]ηg[2][A]_ _[, and][ F][ =][ ∥][ω][0][ −]_ **_[ω][∗][∥][2][.][ N][ is the]_**

_number of chosen clients in each round, and K is the number of local iterations._
**Remark 4.6. CFL methods accelerates the convergence by reducing the variance term. As stated**
_in Theorem 4.2 and Theorem 4.5, FedAvg (under time-varying scenarios) is a special case of CFL_
_methods by setting pt,i = 1, and we can show by proof (in Appendix B.4) that it is equivalent to_
_setting the upper bound of information lossR →∞_ _in pt,i =_ _tD(t−[2]+(1)Rt−[2]+1)DR[2][2][ in Theorem 4.2).] R →∞[ The upper bound](we can also observe this result by setting[ R][ is linearly correlated to]_

_D[2]R[2]_
_gradient noise, where cA = G[2]_ + _R[2]+D[2][ in Theorem 4.2 increases to][ c][A][ =][ G][2][ +]_ _[D][2][ in Theorem 4.5.]_

5 EXPERIMENTS

In our numerical investigation, we first use the Noisy Quadratic Model—a simple convex model—to
verify our theoretical results stated in Section 4. The results (in Appendix C.1) demonstrate that CFL
methods converge much faster than baselines like FedAvg and FedProx, with a smoother convergence
curve. These findings are corroborate Remark 4.6 about the variance reduction effect of CFL methods.
We further provide extensive empirical evaluations below by comparing CFL methods with various
strong FL competitors on different realistic datasets. We explore several approximation techniques
in CFL methods and shed light on achieving practical efficient federated learning with time-varying
data heterogeneity.

5.1 SETUP
**Time-varying heterogeneous local datasets.** We consider federated learn an image classifier using ResNet18 on split-CIFAR10 and split-CIFAR100 datasets, as well as a two layer MLP on the
split-Fashion-MNIST dataset. The “split” follows the idea introduced in Yurochkin et al. (2019); Hsu
et al. (2019); Reddi et al. (2021), where we leverage the Latent Dirichlet Allocation (LDA) to control
the distribution drift with parameter α (See Algorithm 4). Larger α indicates smaller drifts.
Unless specifically mentioned otherwise our studies use the following protocol. All datasets are partitioned to 210 subsets for 7 distinct clients: all clients are selected and trained for 500 communication
rounds, and each client randomly samples one of the corresponding 30 subsets for the local training—
this challenging time-varying scenario mimics the realistic client data sampling scheme (from some
underlying distributions). Notably, the training strategy used in this section is applicable to all FL
baselines and CFL methods, and we assess the model performance using a global test dataset[3]. We
carefully tune the hyper-parameters in all algorithms (details refer to Appendix C.2.1), and report
the optimal results (i.e. mean test accuracy across the past 5 best epochs) after repeated trials.

**Approximation techniques in CFL methods.** Below, we review three representative types of
information approximation techniques in CL, and use them to investigate the effect of various
types of information loss under the (CFL) formulation. We refer to Appendix C.2.2 for a more
comprehensive introduction and discussion.

-  Regularization methods. Instead of keeping datasets from previous rounds, we keep track
the gradients and Hessian matrices of local objective functions, and use Taylor Extension to
approximate the local objective functions in previous rounds. Note that the trade-off between
Hessian estimation and computational overhead constrains the practical feasible of such approach.

-  Core set methods. Another simple yet effective treatment in CL lines in the category of Exemplar
Replay (Rebuffi et al., 2017; Castro et al., 2018). This method selects and regularly saves previous
core-set samples (a.k.a. exemplars), and replays them with the current local datasets.

3The training loss/accuracy on (CFL) formulation are closely aligned with that of global test data, as shown
in Figure 5 of Appendix C.2.3. We here only present the global test results for the common interests in practice.


-----

Table 2: Top-1 accuracy for different choices of approximation techniques in CFL. We train ResNet18 on
split-CIFAR10 dataset (w/ α = 0.2) for 300 communication rounds, and the dataset is partitioned to 300 subsets
for 10 different clients. All examined algorithms use FedAvg as the backbone.

Regularization Methods Core Set Methods Generative Methods
Algorithm

PyHessian Fisher Information Matrix Naive (Small Set) Naive (Large Set) iCaRL (Small Set) iCaRL (Large Set) MCMC

Accuracy 74.15 ± 0.66 73.70 ± 0.39 77.84 ± 0.06 **78.90 ± 0.09** 76.97 ± 0.16 77.09 ± 0.09 74.18 ± 0.08

Table 3: Top-1 accuracy of various CFL methods on diverse datasets for training ResNet18 with 500
communication rounds. In order to observe a noticeable performance difference on Fashion-MNIST, we
use α = 0.1 instead. All examined algorithms use FedAvg as the backbone. Both CFL-Regularization and
CFL-Regularization-Full are regularization based method, and the difference lies on where the regularization
is applied: the full version applies regularization to all layers while the other only considers the top layers. Rr
indicates the accuracy of CFL-Regularization, while Rf denotes the accuracy of FedAvg.

Algorithm Accuracy on Fashion-MNIST (%) Accuracy on CIFAR10 (%) Accuracy on CIFAR100 (%)

CFL-Regularization-FullCFL-Core-SetCFL-RegularizationFedAvg 77878688.32...370275 ± ± ± ± 0 0 0 0....12632114 70337081.48...098651 ± ± ± ± 0 1 0 0....24451931 50144953.17...846997 ± ± ± ± 0 0 0 0....08120619

Metric Improvement on Fashion-MNIST (%) Improvement on CIFAR10 (%) Improvement on CIFAR100 (%)

Ratio (Absolute ((RrR −r −RfR)/Rf ) _f_ ) 0.270.31 0.350.50 01.44.72

-  Generative methods. Maintaining a core set for each client may become impractical when learning scales to millions of clients. To ensure a privacy-preserved federated learning, the generative
models (Goodfellow et al., 2014) could be used locally to capture the local data distribution: fresh
data samples will be generated on the fly and combined with the current local dataset. For the
sake of simplicity, we use Markov Chain Monte Carlo (Nori et al., 2014) in our assessment.

5.2 RESULTS
**Comments on different CFL approximation techniques.** In Table 2, we examine the performance of several approximation techniques using the split-CIFAR10 dataset. We can conclude that
(1) Core set methods outperform other methods by a significant margin. The simple choice of “naive
core set”, i.e. randomly and uniformly sample data from the local dataset, surpasses the sampling
technique described in iCaRL (Rebuffi et al., 2017)[4] for CL, despite their faster convergence in
the initial training phase. (2) The quality of Hessian estimation matters for regularization based
_methods. PyHessian (Yao et al., 2020), as a method to approximate diagonal Hessian matrix, is_
slightly preferable than Fisher Information Matrix, though the latter one involves less computation.
(3) The performance of generative methods is restricted, and we hypothesize that the poor quality of
generated samples contributes to the constraint.
In the subsequent evaluation, we consider naive core set sampling for CFL-Core-Set method, and use
PyHessian for CFL-Regularization. We exclude the results of generative methods, due to the trivial
performance gain and significant computational overhead.

**Understanding various approximated CFL implementations on different datasets.** Table 3
experimentally studies the impacts of various information approximation techniques in CFL methods—
as discussed in Section 5.1—and compares them with the backbone algorithm of CFL methods, i.e.
FedAvg. We have the following consistent findings on different datasets.
1. The improvement of CFL methods over FedAvg becomes larger for more challenging tasks, as
shown in the bottom of the Table 3 for regularization based methods. This might reflect the
fact that the precision of the information approximation is more crucial for complicated tasks.
Similarly, we demonstrate in Figure 6 (in Appendix C.2.3) that CFL methods offer better resistance
_to time-varying non-iid data and the significance of such finding is depending on the task difficulty._

2. For CFL-Regularization method, applying regularization terms on top layers works better than
_that on all layers. This observation matches the recent work on decoupling feature extractor and_
classifier (Collins et al., 2021; Chen & Chao, 2021; Luo et al., 2021): the bottom layers are more
generic across tasks and can serve as a global feature extractor, while the top layers are subject to
task-specific information.

3. The core-set method consistently outperforms FedAvg and other CFL approximation techniques
_by a large margin._
We further investigate the connection between the information loss and the learning performance. We
examine CFL methods with naive core sets, where the degree of information loss[5] can be changed

4The algorithmic details of the sampling method in iCaRL refer to Algorithm 3 in Appendix C.2.2.
5We estimate the information loss by using _tS1_ _Si=1_ _tτ_ =1

Equation (3), S is the number of clients chosen in each round, and t[∥] is the number of communication rounds.[∆][τ,i][(][ω][)][∥][, where][ ∆][τ,i][(][ω][)][ is defined in]
P P


-----

by altering the core set size from 20 to 150, with the same random seed and optimizers. Figure 1
depicts that the performance of models is highly linked to the value of information loss: the lesser the
_information loss, the higher the performance._

**Superior performance of CFL methods over other strong FL baselines.** Alongside the comparison between CFL methods and FedAvg on various datasets (Table 3), in Table 4 we verify the
_efficacy of CFL methods over other strong FL baselines, on split-CIFAR10 dataset. We also examine_
MimeLite (Karimireddy et al., 2020a), a method suggested for stateless FL scenarios. Note that
we exclude the results illustration for methods like SCAFFOLD (Karimireddy et al., 2020b) and
Ditto (Li et al., 2021), due to their infeasibility to be applied in our continual scenariosWe further relax the difficulty of federated 0.03 [6]. 0.85

continual learning, from challenging nonoverlapping time-varying heterogeneous data
(e.g. in Table 2 and Table 3) to a moderate 0.02 0.8
time-varying case (i.e. the local data evolves Information Loss
with the overlapping, while the size of local
datasets stay unchanged). Table 5 illustrates the
performance of FL baselines and CFL methods, 0.0120 40 60 80 100 120 140 1600.75
under different degrees of overlapping (the Core Set Size

Figure 1: Information loss (left y-axis) and accuracy

0.03 . 0.85

0.02 0.8

Accuracy

Information Loss

0.01 0.75

20 40 60 80 100 120 140 160

Core Set Size

construction details refers to Appendix C.2.1):

(right y-axis) for training ResNet18 on split-CIFAR10

_the improvement of CFL methods is consistent_ with different core set sizes (x-axis) and α = 0.2.
_to our previous results, while the overlap_
_parameter has no obvious connection with the final global test performance. We believe that both_
the overlap degree and the new arriving data influence final performance, and we leave future work
on realistic time-varying FL datasets to gain a better understanding.
Table 4: Comparing the SOTA FL baselines with several CFL methods, for training ResNet18 on splitCIFAR10 dataset with different degrees of non-iid-ness α (and with total 500 communication rounds).

FL baselines CFL methods
Accuracy

FedAvg FedProx MimeLite CFL-Core-Set CFL-Regularization CFL-Regularization + FedProx

_αα = 0 = 0..21_ 7770..9851 ± ± 0 0..3619 7871..4057 ± ± 0 0..2217 7869..6555 ± ± 0 0..2636 **8481..4148 ± ± 0 0..1124** 7870..7686 ± ± 0 0..2631 7971..5004 ± ± 0 0..0701

Table 5: Benchmarking FL baselines and CFL methods on different degrees of local dataset overlapping,
for training ResNet18 on split-CIFAR10 dataset. The overlap reduces the degree of non-iid-ness. In order to
observe a noticeable performance difference, we use a larger data distribution gap between rounds (i.e. α = 0.1).

FL baselines CFL methods
Overlap

FedAvg MimeLite CFL-Core-Set CFL-Regularization

0% 80.66 ± 0.40 80.78 ± 0.07 **84.87 ± 0.11** 81.27 ± 0.38
25% 80.16 ± 0.01 80.17 ± 0.11 **84.66 ± 0.11** 80.67 ± 0.27
50% 79.97 ± 0.23 80.35 ± 0.39 **84.59 ± 0.07** 80.91 ± 0.25

**Investigating other time-varying scenarios.** In previous numerical investigations, we examine
the stateful clients and assume that these clients would learn and retain their client states throughout
the training process. The remainder of this section will discuss how to learn with stateless clients (i.e.
each client only appear once). To do this, we change the data partition strategy such that fresh data
partitions are sampled on the fly for each client and communication round.
Table 6 evaluates the above-mentioned scenario. We find that CFL-Regularization methods consis_tently outperform alternative FL baselines, similar to the observations in Table 4. However, due to the_
nature of stateless under the privacy concern, the advancements of core set method in CFL algorithm
cannot be seen in this scenario. Additionally, we discuss the applicability of different algorithms in a
variety of time-varying scenarios in Table 8 of Appendix C.2.3.

Table 6: Learning with stateless clients, regarding training ResNet18 on split-CIFAR10 with α = 0.2. All
examined algorithms use FedAvg as the backbone. The details w.r.t. CFL-Regularization refer to Appendix C.2.2.

FL baselines CFL methods
Algorithm

FedAvg MimeLite FedProx CFL-Regularization CFL-Regularization + FedProx

Accuracy 78.07 ± 0.06 78.47 ± 0.19 78.48 ± 0.05 79.07 ± 0.25 **79.32 ± 0.07**

6We also naively adapted and examined these methods, but we cannot observe significant performance gains.


-----

REFERENCES

Thang D Bui, Cuong V Nguyen, Siddharth Swaroop, and Richard E Turner. Partitioned variational
inference: A unified framework encompassing federated and continual learning. arXiv preprint
_arXiv:1811.11206, 2018._

Fernando E. Casado, Dylan Lema, Marcos F. Criado, Roberto Iglesias, Carlos V. Regueiro, and
Senén Barro. Concept drift detection and adaptation for federated and continual learning. CoRR,
[abs/2105.13309, 2021. URL https://arxiv.org/abs/2105.13309.](https://arxiv.org/abs/2105.13309)

Francisco M Castro, Manuel J Marín-Jiménez, Nicolás Guil, Cordelia Schmid, and Karteek Alahari.
End-to-end incremental learning. In Proceedings of the European conference on computer vision
_(ECCV), pp. 233–248, 2018._

Hong-You Chen and Wei-Lun Chao. On bridging generic and personalized federated learning. arXiv
_preprint arXiv:2107.00778, 2021._

Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared representations for personalized federated learning. In arXiv preprint arXiv:2102.07078, 2021.

Rudrajit Das, Anish Acharya, Abolfazl Hashemi, Sujay Sanghavi, Inderjit S Dhillon, and Ufuk
Topcu. Faster non-convex federated learning via global and local momentum. arXiv preprint
_arXiv:2012.04061, 2020._

Enmao Diao, Jie Ding, and Vahid Tarokh. Heterofl: Computation and communication efficient federated learning for heterogeneous clients. In International Conference on Learning Representations,
[2021. URL https://openreview.net/forum?id=TNkPBBYFkXg.](https://openreview.net/forum?id=TNkPBBYFkXg)

Canh T Dinh, Nguyen H Tran, and Tuan Dung Nguyen. Personalized federated learning with moreau
envelopes. In Advances in Neural Information Processing Systems, 2020.

Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3
(4):128–135, 1999.

Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014.

Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter
Richtárik. Sgd: General analysis and improved rates. In International Conference on Machine
_Learning, pp. 5200–5209. PMLR, 2019._

Farzin Haddadpour, Mohammad Mahdi Kamani, Aryan Mokhtari, and Mehrdad Mahdavi. Federated
learning with compression: Unified analysis and sharp guarantees. In International Conference on
_Artificial Intelligence and Statistics, pp. 2350–2358. PMLR, 2021._

Filip Hanzely and Peter Richtárik. Federated learning of a mixture of global and local models. arXiv
_preprint arXiv:2002.05516, 2020._

Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data
distribution for federated visual classification. arXiv preprint arXiv:1909.06335, 2019.

Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi Bennis, Arjun Nitin
Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances and open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019.

Sai Praneeth Karimireddy, Martin Jaggi, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Sebastian U
Stich, and Ananda Theertha Suresh. Mime: Mimicking centralized stochastic algorithms in
federated learning. arXiv preprint arXiv:2008.03606, 2020a.

Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In
_International Conference on Machine Learning, pp. 5132–5143. PMLR, 2020b._


-----

Ahmed Khaled, Konstantin Mishchenko, and Peter Richtárik. Tighter theory for local sgd on identical
and heterogeneous data. In International Conference on Artificial Intelligence and Statistics, pp.
4519–4529. PMLR, 2020.

James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A
Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. Overcoming
catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114
(13):3521–3526, 2017.

Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian Stich. A unified
theory of decentralized sgd with changing topology and local updates. In International Conference
_on Machine Learning, pp. 5381–5393. PMLR, 2020._

Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2018.

Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. Fair resource allocation in federated
learning. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,
_[Ethiopia, April 26-30, 2020. OpenReview.net, 2020a. URL https://openreview.net/](https://openreview.net/forum?id=ByexElSYDr)_
[forum?id=ByexElSYDr.](https://openreview.net/forum?id=ByexElSYDr)

Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning
through personalization. In International Conference on Machine Learning, pp. 6357–6368. PMLR,
2021.

Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of
fedavg on non-iid data. In International Conference on Learning Representations, 2020b. URL
[https://openreview.net/forum?id=HJxNAnVtDS.](https://openreview.net/forum?id=HJxNAnVtDS)

Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE transactions on pattern analysis
_and machine intelligence, 40(12):2935–2947, 2017._

Tao Lin, Lingjing Kong, Sebastian U Stich, and Martin Jaggi. Ensemble distillation for robust model
fusion in federated learning. In Advances in Neural Information Processing Systems, 2020a.

Tao Lin, Sebastian U. Stich, Kumar Kshitij Patel, and Martin Jaggi. Don’t use large mini-batches,
[use local sgd. In International Conference on Learning Representations, 2020b. URL https:](https://openreview.net/forum?id=B1eyO1BFPr)
[//openreview.net/forum?id=B1eyO1BFPr.](https://openreview.net/forum?id=B1eyO1BFPr)

Mi Luo, Fei Chen, Dapeng Hu, Yifan Zhang, Jian Liang, and Jiashi Feng. No fear of heterogeneity:
Classifier calibration for federated learning with non-iid data. arXiv preprint arXiv:2106.05001,
2021.

Davide Maltoni and Vincenzo Lomonaco. Continuous learning in single-incremental-task scenarios.
_Neural Networks, 116:56–73, 2019._

Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli_gence and Statistics, pp. 1273–1282. PMLR, 2017._

Aritra Mitra, Rayana Jaafar, George J Pappas, and Hamed Hassani. Achieving linear convergence in
federated learning under objective and systems heterogeneity. arXiv preprint arXiv:2102.07053,
2021.

Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In Kamalika
Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference
_on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97_
[of Proceedings of Machine Learning Research, pp. 4615–4625. PMLR, 2019. URL http:](http://proceedings.mlr.press/v97/mohri19a.html)
[//proceedings.mlr.press/v97/mohri19a.html.](http://proceedings.mlr.press/v97/mohri19a.html)

Aditya Nori, Chung-Kil Hur, Sriram Rajamani, and Selva Samuel. R2: An efficient mcmc sampler
for probabilistic programs. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 28, 2014.


-----

Kumar Kshitij Patel and Aymeric Dieuleveut. Communication trade-offs for synchronized distributed
sgd with large step size. In Advances in Neural Information Processing Systems, 2019.

Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl:
Incremental classifier and representation learning. In Proceedings of the IEEE conference on
_Computer Vision and Pattern Recognition, pp. 2001–2010, 2017._

Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Koneˇcný,
Sanjiv Kumar, and Hugh Brendan McMahan. Adaptive federated optimization. In International
_[Conference on Learning Representations, 2021. URL https://openreview.net/forum?](https://openreview.net/forum?id=LkFG3lB13U5)_
[id=LkFG3lB13U5.](https://openreview.net/forum?id=LkFG3lB13U5)

Amirhossein Reisizadeh, Farzan Farnia, Ramtin Pedarsani, and Ali Jadbabaie. Robust federated
learning: The case of affine distribution shifts. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia
Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Pro_cessing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS_
_[2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/](https://proceedings.neurips.cc/paper/2020/hash/f5e536083a438cec5b64a4954abc17f1-Abstract.html)_
[paper/2020/hash/f5e536083a438cec5b64a4954abc17f1-Abstract.html.](https://proceedings.neurips.cc/paper/2020/hash/f5e536083a438cec5b64a4954abc17f1-Abstract.html)

Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick, Koray
Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv preprint
_arXiv:1606.04671, 2016._

Claude Sammut and Geoffrey I. Webb (eds.). Neuro-Dynamic Programming, pp. 716–716. Springer
US, Boston, MA, 2010. ISBN 978-0-387-30164-8. doi: 10.1007/978-0-387-30164-8_588. URL
[https://doi.org/10.1007/978-0-387-30164-8_588.](https://doi.org/10.1007/978-0-387-30164-8_588)

Neta Shoham, Tomer Avidor, Aviv Keren, Nadav Israel, Daniel Benditkis, Liron Mor-Yosef, and Itai
Zeitak. Overcoming forgetting in federated learning on non-iid data. CoRR, abs/1910.07796, 2019.
[URL http://arxiv.org/abs/1910.07796.](http://arxiv.org/abs/1910.07796)

Sebastian U. Stich. Local SGD converges fast and communicates little. In International Confer_[ence on Learning Representations, 2019. URL https://openreview.net/forum?id=](https://openreview.net/forum?id=S1g2JnRcFX)_
[S1g2JnRcFX.](https://openreview.net/forum?id=S1g2JnRcFX)

Sebastian U Stich and Sai Praneeth Karimireddy. The error-feedback framework: Better rates for sgd
with delayed gradients and compressed updates. Journal of Machine Learning Research, 21:1–36,
2020.

Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective
inconsistency problem in heterogeneous federated optimization. arXiv preprint arXiv:2007.07481,
2020.

Blake Woodworth, Kumar Kshitij Patel, and Nathan Srebro. Minibatch vs local sgd for heterogeneous
distributed learning. In NeurIPS 2020 - Thirty-fourth Conference on Neural Information Processing
_Systems, 2020a._

Blake Woodworth, Kumar Kshitij Patel, Sebastian Stich, Zhen Dai, Brian Bullins, Brendan Mcmahan,
Ohad Shamir, and Nathan Srebro. Is local sgd better than minibatch sgd? In International
_Conference on Machine Learning, pp. 10334–10343. PMLR, 2020b._

Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael W. Mahoney. Pyhessian: Neural networks
through the lens of the hessian. In Xintao Wu, Chris Jermaine, Li Xiong, Xiaohua Hu, Olivera
Kotevska, Siyuan Lu, Weija Xu, Srinivas Aluru, Chengxiang Zhai, Eyhab Al-Masri, Zhiyuan Chen,
and Jeff Saltz (eds.), IEEE International Conference on Big Data, Big Data 2020, Atlanta, GA, USA,
_December 10-13, 2020, pp. 581–590. IEEE, 2020. doi: 10.1109/BigData50022.2020.9378171._
[URL https://doi.org/10.1109/BigData50022.2020.9378171.](https://doi.org/10.1109/BigData50022.2020.9378171)

Dong Yin, Mehrdad Farajtabar, and Ang Li. SOLA: continual learning with second-order loss
[approximation. CoRR, abs/2006.10974, 2020a. URL https://arxiv.org/abs/2006.](https://arxiv.org/abs/2006.10974)
[10974.](https://arxiv.org/abs/2006.10974)


-----

Dong Yin, Mehrdad Farajtabar, Ang Li, Nir Levine, and Alex Mott. Optimization and generalization of regularization-based continual learning: a loss approximation viewpoint. arXiv preprint
_arXiv:2006.10974, 2020b._

Jaehong Yoon, Wonyong Jeong, Giwoong Lee, Eunho Yang, and Sung Ju Hwang. Federated continual
learning with weighted inter-client transfer. In International Conference on Machine Learning, pp.
12073–12086. PMLR, 2021.

Mikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan Greenewald, Nghia Hoang, and
Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In International
_Conference on Machine Learning, pp. 7252–7261. PMLR, 2019._

Friedemann Zenke, Ben Poole, and Surya Ganguli. Continual learning through synaptic intelligence.
In International Conference on Machine Learning, pp. 3987–3995. PMLR, 2017.

Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George E Dahl, Christopher J Shallue, and Roger Grosse. Which algorithmic choices matter at which batch sizes? insights
from a noisy quadratic model. In Advances in Neural Information Processing Systems, 2019.

Martin Zinkevich, Markus Weimer, Alexander J Smola, and Lihong Li. Parallelized stochastic
gradient descent. In NIPS, volume 4, pp. 4. Citeseer, 2010.


-----

CONTENTS OF APPENDIX

**A Techniques** **14**

**B** **Convergence rate of CFL** **15**
B.1 Bound of Gradient Noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15

B.2 Bounded Gradient Noise of CFL . . . . . . . . . . . . . . . . . . . . . . . . . . . 15

B.3 Bounded Approximation Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

B.4 Proof of Theorem 4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16

B.4.1 Start up. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

B.4.2 One Step Progress . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17

B.4.3 Weights of rounds. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

B.4.4 Convergence results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

B.5 Proof of convergence rate for non-convex setting . . . . . . . . . . . . . . . . . . 21


**C Experiment details** **25**
C.1 Noisy quadratic model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25

C.2 Realistic datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26

C.2.1 Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26

C.2.2 Approximation Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

C.2.3 Additional Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . 28

C.3 Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28

A TECHNIQUES


Here we show some technical lemmas which are helpful in the theoretical proof.
**Lemma A.1 (Linear convergence rate (Karimireddy et al., 2020b, Lemma 1)). For every non-**
_negative sequence {dr−1}r≥1, and any parameter µ > 0, T ≥_ 2ηmax1 _µ_ _[, there exists a constant]_

_step-size η_ _ηmax and weights ωt = (1_ _µη)[1][−][t]_ _such that for WT =_ _t=1_ _[ω][t]_
_≤_ _−_

_T +1_

ΦT = 1 _ωt−1_ (1 _µη)dt_ 1 = [˜] (µd0exp[P]( _[T][ +1]µηmaxT_ )) . (12)

_WT_ _t=1_  _η_ _−_ _−_ _−_ _[ω]η [t]_ _[d][t]_ _O_ _−_

X

**Lemma A.2 (Sub-linear convergence rate (Karimireddy et al., 2020b, Lemma 2)). For every non-**
_negative sequence_ _dr_ 1 _r_ 1 and any parameters ηmax > 0, c1 0, c2 0, T 0, there exists a
_constant step-size1_ _T +1 η { ≤d−tηmax1}_ _≥ such that_ _d0_ _≥_ _≥_ _≥_ _d0_ [2]3 13

ΦT = _T + 1_ _η−_ _−_ _[d]η[t]_ [+][ c][1][η][ +][ c][2][η][2] _≤_ _ηmax(T + 1) [+ 2]√[√]T[c] + 1 [1][d][0]_ [+ 2] _T + 1_ _c2_ _[.]_

_t=1_    

X

(13)

**Lemma A.3 (Relaxed triangle inequality (Karimireddy et al., 2020b, Lemma 3)). Let {v1, ..., vτ** _}_
_be τ vectors in R[d]. Then the following is true:τ_ _τ_

**vi** **vi** _._ (14)

_∥_ _τ[1]_ _∥[2]_ _≤_ _τ[1]_ _∥_ _∥[2]_

_i=1_ _i=1_

X X

**Lemma A.4 (Separating mean and variance (Stich & Karimireddy, 2020, Lemma 14)). Let**
Ξ1, Ξ2, ..., Ξτ be τ random variables in R[d] _which are not necessarily independent. First sup-_
_pose that their mean isthe following holds_ _E[Ξi] = ξi, and variance is bounded as E∥Ξi −_ _ξi∥[2]_ _≤_ _M_ _∥ξi∥[2]_ + σ[2], then

_τ_ 2 _τ_

E Ξi _≤_ (τ + M ) _∥ξi∥[2]_ + τσ[2] _._ (15)

_i=1_ _i=1_

X X

**Lemma A.5 (Perturbed strongly convexity (Karimireddy et al., 2020b, Lemma 5)). The following**
_holds for any L-smooth and µ-strongly convex function h, and any x, y, z in the domain of h:_

_h(x), z_ **y** _h(z)_ _h(y) +_ _[µ]_ (16)

**Lemma A.6. Define⟨∇ S =** _−t=1⟩≥ptt_ _[where] −[ P]t[T]=1_ _[p][t][ = 1]4_ _[∥][y][, and][ −]_ **[z][∥][ p][2][t][ −][ ≤][L][p][∥][t][z][+1][ −][ then][x][∥][2][ .][ S][ ≤]** _T1_ _Tt=1_ 1t [=]

˜ _lnTT_ _._
_O_ P

[P][T]

  


-----

B CONVERGENCE RATE OF CFL

B.1 BOUND OF GRADIENT NOISE

**Lemma B.1 (Bound of Gradient Noise). Suppose fi(ω) is the local objective function, and f** (ω) =
E [fi(ω)] is the global objective function. Define ∇f (ω) = ∇fi(ω) + ςi, E[ςi] = 0, and assume

E h∥ς∥[2] _|ωi_ _≤_ _A[2]E_ h∥∇f (ω)∥[2][i] + B[2], we have

E h∥∇fi(ω)∥[2][i] _≤_ (A[2] + 1)E h∥∇f (ω)∥[2][i] + B[2] _._ (17)

_Proof._

E h∥∇fi(ω)∥[2][i] = E h∥∇f (ω) + ςi∥[2][i] (18)

= E h∥∇f (ω)∥[2][i] + E h∥ςi∥[2][i] (19)

_≤_ (A + 1)E h∥∇f (ω)∥[2][i] + B . (20)

B.2 BOUNDED GRADIENT NOISE OF CFL

**Lemma B.2 (Bounded Gradient Noise of CFL). For Formulation (CFL), consider CFL and FedAvg,**
_we can bound the gradient drift as_
_(1) FedAvg only optimize current local objective functions, thus,_


E∥∇ft,i(ω)∥[2] _≤_ (1 + A[2] + B[2])E∥∇f (ω)∥[2] + G[2] + D[2] _._

_(2) For CFL, in general (same clients can appear in different rounds), we have_


1 + B[2] + A[2]


_p[2]τ,i_
_τ_ =1

X


E∥∇f (ω)∥[2]


_pτ,i_ _fτ,i(ω)_
_∇_
_τ_ =1

X


+ G[2] + D[2] _p[2]τ,i_ _[.]_

_τ_ =1

X


_(3) For CFL, when clients only participate in training once(δt,i are independent for all t and i), we_
_have_


1 + (B[2] + A[2])


_p[2]τ,i_
_τ_ =1

X


E∥∇f (ω)∥[2]


_pτ,i_ _fτ,i(ω)_
_∇_
_τ_ =1

X


+ (G[2] + D[2])


_p[2]τ,i_ _[.]_
_τ_ =1

X


_Proof. Using Assumption 3, together with the fact that ∇ft,i(ω) = ∇f_ (ω) + δt,i + ξt,i, we have

E∥∇ft,i(ω)∥[2] = E∥∇f (ω) + δt,i + ξt,i∥[2] (21)

= E∥∇f (ω)∥[2] + E∥δt,i∥[2] + E∥ξt,i∥[2] (22)

_≤_ (1 + A[2] + B[2])E∥∇f (ω)∥[2] + G[2] + D[2] _._ (23)

The second inequality is based on the independence of noise, and directly use Assumption 3 we get
the last inequality. Similarly, we have


= E


(24)

(25)

_t_

_p[2]τ,i_ _[.]_ (26)
_τ_ =1

X


_pτ,i_ _fτ,i(ω)_
_∇_
_τ_ =1

X


_pτ,i(∇fτ,i(ω) + δi + ξτ,i)_
_τ_ =1

X


= E∥∇f (ω)∥[2] + E∥δi∥[2] + E


_pτ,iξτ,i_
_τ_ =1

X


E∥∇f (ω)∥[2] + G[2] + D[2]


1 + B[2] + A[2] _p[2]τ,i_

_τ_ =1

X


-----

For the last inequality, when δt1,i and δt2,i are independent for all t1, t2,


= E


(27)

(28)

_t_

_p[2]τ,i_ _[.]_ (29)
_τ_ =1

X


_pτ,i_ _fτ,i(ω)_
_∇_
_τ_ =1

X


_pτ,i(∇fτ,i(ω) + δt,i + ξτ,i)_
_τ_ =1

X


= E∥∇f (ω)∥[2] + E

_≤_ 1 + (A[2] + B[2])

B.3 BOUNDED APPROXIMATION ERROR


+ E


_pτ,iδt,i_
_τ_ =1

X


_pτ,iξτ,i_
_τ_ =1

X


E∥∇f (ω)∥[2] + (G[2] + D[2])


_p[2]τ,i_
_τ_ =1

X


**Lemma B.3 (Bounded Approximation Error). For ∆t,i(ω) = ∇ft,i(ω)−∇f[˜]t,i(ω) (c.f. Definition 3),**

_when use Taylor Extension,_


_ft,i(ω) =_ _ft,i(ωt) +_ _ft,i(ωt)(ω_ **_ωt),_**
_∇_ [˜] _∇_ _∇[2]_ _−_

∆t,i(ω) _ϵ_ **_ω_** **_ωˆ_** _t,i_ _._
_∥_ _∥≤_ _∥_ _−_ _∥_


_we have_

_Proof._


∆t,i(ω) = _ft,i(ω)_ _ft,i(ω)_ (30)
_∥_ _∥_ _∇_ _−∇_ [˜]

= _ft,i(ω)_ _ft,i(ωt,i,K)_ **HtjK(ω** **_ωt,i,K)_** (31)
_∥∇_ _−∇_ _−_ _−_ _∥_

= _ft,i(ωξ,t)(ω_ **_ωt,i,K)_** **HtjK(ω** **_ωt,i,K)_** (32)
_∇[2]_ _−_ _−_ _−_

= ( _ft,i(ωξ,t)_ **HtjK)(ω** **_ωt,i,K)_** (33)
_∇[2]_ _−_ _−_

_ϵ_ **_ω_** **_ωt,i,K_** _._ (34)
_≤_ _∥_ _−_ _∥_

The first two equations come from the mean value theorem which says for continuous function f in
closed intervals [a, b] and differentiable on open interval (a, b), there exists a point c ⊆ (a, b) such
that

_f_ _[′](c) =_ _[f]_ [(][b]b[)][ −] − _a[f]_ [(][a][)] _._ (35)

The last inequality is based on Assumption 4.

B.4 PROOF OF THEOREM 4.2
In this section we will give the complete proof of convergence rate of algorithm 2 when the local
objective functions are convex.

**Algorithm 2 CFL Framework**
**Require: initial weights ω0, global learning rate ηg, local learning rate ηl**

1: for round t = 1, . . ., T do
2: **communicate ωt to the chosen clients.**

3: **for client i ∈St in parallel do**

4: initialize local model ωt,i,0 = ωt.

5: **for k = 1, . . ., K do**

6: _g˜t,i,k(ωt,i,k−1) =_ _pt,i∇ft,i(ωt,i,k−1) +_ _τ_ =1 _[p][τ,i][∇]f[˜]t,i(ωt,i,k−1)_ + νt,i,k.

7:8: **communicateωt,i,k ←** **_ω ∆t,i,kωt,i−1 ← −_** _ηωlgt,i,K˜t,i,k −(ωωt,i,kt._ _−1)._ [P][t][−][1] 

9: ∆ωt _S_ _i_ _t_ [∆][ω][t,i][.]
_←_ _[η][g]_ _∈S_

10: **_ωt+1_** **_ωt + ∆ωt._**

P

_←_


-----

B.4.1 START UP.
The local objective function of round t on client i is

_t−1_

_f¯t,i(ω) = (pt,ift,i(ω) +_ _pτ,if[˜]τ,i(ω) ._ (36)

_τ_ =1

X

Without loss of generality, assume _τ_ =1 _[p][τ,i][ = 1][. Then follow the steps in algorithm 2, we have]_

_t−1_

_ft,i(ω) =[P][t] pt,i_ _ft,i(ω) +_ _pτ,i_ _fτ,i(ω) ._ (37)
_∇_ [¯] _∇_ _∇_ [˜]

_τ_ =1

X


Then the noisy gradient in mini-batch SGD can be described as

_g¯t,i,k(ωt,i,k−1) = ∇f[¯]t,i(ωt,i,k−1) + νt,i,k_ (38)

Then follow the steps in algorithm 2, we have


∆ωt =

_[−][η]N[l][η][g]_

_E[∆ωt] =_

_[−][η]N[l][η][g]_

Then let η = Kηlηg, we have

∆ωt =

_NK[−][η]_


_g¯t,i(ωt,i,k−1),_ (39)
_k=1_

X

_K_

_∇f[¯]t,i(ωt,i,k−1) ._ (40)
_k=1_

X


_i=1_

_N_

_i=1_

X


_g¯t,i(ωt,i,k−1),_ (41)

_i=1_ _k=1_

X X

_N_ _K_

_∇f[¯]t,i(ωt,i,k−1) ._ (42)

_i=1_ _k=1_

X X


_E[∆ωt] =_

_NK[−][η]_

B.4.2 ONE STEP PROGRESS
Consider the one-step progress we have


E∥ωt + ∆ωt − **_ω[∗]∥[2]_** = ∥ωt − **_ω[∗]∥[2]_** _−_ 2E⟨ωt − **_ω[∗], ∆ωt⟩_**
_A1_

Firstly we consider A1 part in equation (43), | {z }


+ E∥∆ωt∥[2]
_A2_
| {z }


(43)


2E **_ωt_** **_ω[∗], ∆ωt_**
_−_ _⟨_ _−_ _⟩_

_N_ _K_

= NK[2][η] _⟨∇f[¯]t,i(ωt,i,k−1), ω[∗]_ _−_ **_ωt⟩_** (44)

_i=1_ _k=1_

X X

_N_ _K_ _t−1_

= [2][η] _pt,i_ _ft,i(ωt,i,k) +_ _pτ,i_ _fτ,i(ωt,i,k), ω[∗]_ **_ωt_** (45)

_NK_ _⟨_ _∇_ _∇_ [˜] _−_ _⟩_

_i=1_ _k=1_ _τ_ =1

X X X

_N_ _K_ _t_ _N_ _K_ _t−1_

= [2][η] _pτ,i_ _fτ,i(ωt,i,k), ω[∗]_ **_ωt_** _pτ,i_ ∆τ,i(ωt,i,k), ω[∗] **_ωt_**

_NK_ _⟨_ _∇_ _−_ _⟩−_ _NK[2][η]_ _⟨_ _−_ _⟩_

_i=1_ _k=1_ _τ_ =1 _i=1_ _k=1_ _τ_ =1

X X X X X X

(46)

_N_ _K_ _t_ _N_ _t−1_

_pτ,i_ _fτ,i(ωt,i,k), ω[∗]_ **_ωt_** + [2][η] _pτ,iR_ **_ω[∗]_** **_ωt_** _._ (47)

_≤_ _NK[2][η]_ _⟨∇_ _−_ _⟩_ _N_ _∥_ _−_ _∥_

_i=1_ _k=1_ _τ_ =1 _i=1_ _τ_ =1

X X X X X


We use equality (42) for first inequality, and directly use Assumption 4 and Cauchy–Schwarz
inequality for last inequality.
Then we can consider the first term of inequality (47). Firstly, by Lemma A.5 we see that

_⟨∇fτ,i(ωt,i,k−1), ω[∗]_ _−_ **_ωt⟩≤_** _fτ,i(ω[∗]) −_ _fτ,i(ωt) −_ _[µ]4_

_[∥][ω][t][ −]_ **_[ω][∗][∥][2][ +][ L][∥][ω][t,i,k][−][1][ −]_** **_[ω][t][∥][2][ .][ (48)]_**


-----

That is, based on (47) and (48), we have,

2E **_ωt_** **_ω[∗], ∆ωt_**
_−_ _⟨_ _−_ _⟩_

2η(ft(ωt) _ft(ω[∗]) +_ _[µ]_
_≤−_ _−_ 4 _[∥][ω][t][ −]_ **_[ω][∗][∥][2][) + 2]NK[ηL]_**


_k=1_ _∥ωt,i,k−1 −_ **_ωt∥[2]_**

X


_i=1_


_t−1_

+ 2η _pτ,iR_ **_ω[∗]_** **_ωt_** _._ (49)

_∥_ _−_ _∥_
_τ_ =1

X


Then consider A2 in (43), and define cpB _,i = 1+B[2]+A[2]([P][t]τ_ =1 _[p]τ,i[2]_ [)][,][ c][p]G[,i][ =][ G][2][+][D][2][(][P][t]τ =1 _[p]τ,i[2]_ [)][,]
_cpB =_ _N[1]_ _Ni=1_ _[c][p]B_ _[,i][, and][ c][p]G_ [=][ 1]N _Ni=1_ _[c][p]G[,i][ we have]_

P P

E∥∆ωt∥[2]


_t−1_

_pt,i∇ft,i(ωt,i,k−1) +_ _pτ,i∇f[˜]τ,i(ωt,i,k−1) + νt,i,k_

_τ_ =1

X


_N_ _K_

_η[2]_
=

_N_ [2]K [2][ E]

_i=1_ _k=1_

X X

_N_ _K_

_η[2]_
=

_N_ [2]K [2][ E]

_i=1_ _k=1_

X X

_N_ _K_

_Lem A.4_ _η[2]_
_≤_ _NK_

_i=1_ _k=1_

X X

_N_ _K_

_Lem A.3_ 2η[2]
_≤_ _NK_

_i=1_ _k=1_

X X

_N_ _K_

_Lem ??_ 2η[2]


(50)

(51)


_t−1_

_pτ,i∆τ,i(ωt,i,k−1) + νt,i,k_
_τ_ =1

X


_pτ,i∇fτ,i(ωt,i,k−1) −_
_τ_ =1

X


_t−1_

_pτ,i∆τ,i(ωt,i,k−1)_
_τ_ =1

X


+ _[η][2][σ][2]_ (52)

_NK_


_pτ,i∇fτ,i(ωt,i,k−1) −_
_τ_ =1

X


2[]

_t,i,k−1)_ + _[η]NK[2][σ][2]_ (53)

t _N_

_G[2]_ + D[2]( _p[2]τ,i[)]_ + [2][η][2]

_τ_ =1 !! _N_ _i=1_

X X

(54)


_t−1_

_pτ,i∆τ,i(ωt,i,k−1)_
_τ_ =1

X


E _pτ,i∇fτ,i(ωt,i,k−1)_ + E _pτ,i∆τ,i_

_τ_ =1 _τ_ =1

X X

 _t_

1 + B[2] + A[2](τ =1 _p[2]τ,i[)]!_ E∥∇f (ωt,i,k−1)∥[2] +

X


_t−1_

_pτ,i)[2]R[2]_ + _[η]NK[2][σ][2]_
_τ_ =1

X


_≤_ _NK_

_i=1_

X

_N_

_Lem A.3_ 4η[2]


_k=1_


_t−1_

_pτ,i)[2]R[2]_ + _[η]NK[2][σ][2]_
_τ_ =1

X

(55)

_t−1_

_pτ,i)[2]R[2]_ + _[η][2][σ][2]_

_NK [.]_

_τ_ =1

X

(56)

(57)


_K_

_cpB_ _,i_ E∥∇f (ωt,i,k−1) −∇f (ωt)∥[2] + E∥∇f (ωt)∥[2][] + 2η[2]cpG + [2]N[η][2]
_k=1_

X  


_NK_


_i=1_


_i=1_

_N_

_i=1_

X


_Ass 1_
_≤_ 16η[2]LcpB (f (ωt) − _f_ (ω[∗])) + [8]NK[η][2][L][2]


_K_

_k=1_ _cpB_ _,iE∥ωt,i,k−1 −_ **_ωt∥[2]_** + 2η[2]cpG + [2]N[η][2]

X


_i=1_


Then consider equation (49) and (56), to solve E **_ωt,i,k_** 1 **_ωt_**, we have
_∥_ _−_ _−_ _∥[2]_


E∥ωt,i,k − **_ωt∥[2]_** = ηl[2][E]


_g¯t,i(ωt,i,τ_ _−1)_
_τ_ =1

X


_Lem A.4_
_kηl[2]_
_≤_


E _f¯t,i(ωt,i,τ_ 1) + kηl[2][σ][2][ .] (58)
_∇_ _−_
_τ_ =1

X

[2]


Here we directly use Lemma A.4 to get inequality (58).


-----

Then for E _∇f¯t,i(ω)_, we have

E _∇f¯t,i(ω)_ [2]

2

_t−1_

= E [2] _pτ,i∇f[˜]τ,i(ω)_

_τ_ =1

X

_t_ _t−1_

_[p][t,i][∇][f][t,i][(][ω][) +]_

= E _pτ,i∇fτ,i(ω) −_ _pτ,i∆τ,i(ω)_

_τ_ =1 _τ_ =1

X X


(59)

(60)

(61)


_t−1_

_pτ,i∆τ,i(ω)_
_τ_ =1

X


_Lem A.3_
_≤_ 2E


+ 2E


_pτ,i_ _fτ,i(ω)_
_∇_
_τ_ =1

X


_t_ 1

_Lem ??_ _−_
_≤_ 2cpB _,iE∥∇f_ (ω)∥[2] + 2cpG,i + 2( _pτ,i)[2]R[2]_ (62)

_τ_ =1

X

_t_ 1

_Ass 1_ _−_
_≤_ 4L[2]cpB _,iE∥ω −_ **_ωt∥[2]_** + 8LcpB _,i (f_ (ωt) − _f_ (ω[∗])) + 2cpG,i + 2( _pτ,i)[2]R[2]_ _._ (63)

_τ_ =1

X

Combine equation (58) and (63) we get,

E∥ωt,i,k − **_ωt∥[2]_**

_t−1_

4k[2]ηl[2][L][2][c][p]B _[,i][E][∥][ω][t,i,k][−][1]_ _l_ _[Lc][p]B_ _[,i][(][f]_ [(][ω][t][)][ −] _[f]_ [(][ω][∗][)) + 2][k][2][η]l[2][c][p]G[,i] [+ 2][k][2][η]l[2][(] _pτ,i)[2]R[2]_ + kηl[2][σ][2][ .]
_≤_ _[−]_ **_[ω][t][∥][2][ + 8][k][2][η][2]_** _τ_ =1

X

(64)

That is,


E∥ωt,i,k − **_ωt∥[2]_**

_k−1_ _t−1_

(8K [2]ηl[2][Lc][p]B _[,i][(][f]_ [(][ω][t][)][ −] _[f]_ [(][ω][∗][)) + 2][K] [2][η]l[2][c][p]G[,i] [+ 2][K] [2][η]l[2][(] _pτ,i)[2]R[2]_ + Kηl[2][σ][2][)(4][K] [2][η]l[2][L][2][c][p]B _[,i][)][τ]_

_≤_

_τ_ =0 _τ_ =1

X X

(65)

_l_ _[Lc][p]B_ _[,i][(][f]_ [(][ω][t][)][ −] _[f]_ [(][ω][∗][)) + 2][K] [2][η]l[2][c][p]G[,i][ + 2][K] [2][η]l[2][(][P]τ[t][−]=1[1] _[p][τ,i][)][2][R][2][ +][ Kη]l[2][σ][2]_

_._ (66)

_≤_ [8][K] [2][η][2] 1 4K [2]ηl[2][L][2][c][p][B] _[,i]_

_−_

Combine the equation (43), (49), (56), and (66), we get


E∥ωt + ∆ωt − **_ω[∗]∥[2]_**

_N_

16K [2]ηηl[2][Lc][p]B _[,i][(1 + 4][ηLc][p]B_ _[,i][)]_

_≤_ (1 − _[µη]2 [)][E][∥][ω][t][ −]_ **_[ω][∗][∥][2][ + (][−][2][η][ + 16][η][2][Lc][p][B][ + 1]N_** _i=1_ 1 − 4K [2]ηl[2][L][2][c][p][B] _[,i]_ )(f (ωt) − _f_ _[∗]) + 2η[2]cpG_

X

_N_ _t−1_ _l_ _[L][(][P]τ[t][−]=1[1]_ _[p][τ,i][)][2][(1 + 4][ηLc][p]B_ _[,i][)]_

+ _[η][2][σ][2]_ 2η[2]( _pτ,i)[2]_ + [4][K] [2][ηη][2] _R[2]_

_NK_ [+ 1]N _i=1_ _τ_ =1 1 − 4K [2]ηl[2][L][2][c][p][B] _[,i]_ !

X X


_N_ 2Kηηl[2][L][(2][Kc][p]G[,i][ +][ σ][2][)(1 + 4][ηLc][p]B _[,i][)]_ _N_ _t−1_

+ [1] + [2][η] _pτ,iR_ **_ωt_** **_ω[∗]_** _._ (67)

_N_ _i=1_ 1 − 4K [2]ηl[2][L][2][c][p][B] _[,i]_ _N_ _i=1_ _τ_ =1 _∥_ _−_ _∥_

X X X

Then we notice that

_N_

16K [2]ηηl[2][L][2][c][p]B _[,i][(1 + 4][ηLc][p]B_ _[,i][)]_

2η + 16η[2]LcpB + [1] 0 . (68)
_−_ _N_ _i=1_ 1 − 4K [2]ηl[2][L][2][c][p][B] _[,i]_ _≤_

X

Use the fact that η = Kηlηg, and solve above inequality, we notice that if η satisfy


3ηg[2] [+ 4][c][p]B _[,i][η]g[4]_ 4cpB _,iηg[4]_

_[−]_ _,_ (69)

6L[√]cpB _,iq_


_η ≤_


-----

for any√3ηg[2][+4ˆ]12c cpBLp[√] ηBg[4]cˆ,i[−]pB, convergence can be promised.[√]4ˆcpB ηg[4], we have 1 − 4K [2]ηl[2][L][2][c][p]BDefine[,i][ ≥] 33 ˆc−p8Bη as the largestg[2]c[ˆ]pB +4[√]363ηg[2]c[ˆ]pB +4 cηpg[4]Bc[ˆ][2]pB,i, let, and then η _≤_

1+4ηLcpB,i 1 7 1

1−4K[2]ηl[2][L][2][c][p]B _[,i][ ≤]_ 12 _[−]_ 33+[√]3ηg[2]c[ˆ]pB +4ηg[4]c[ˆ][2]pB _[−][2]1[η]g[2]c[ˆ]pBN_ _[≤]_ 12 [.] _σ[2]_

Define cR,i = ([P][t]τ[−]=1[1] _[p][τ,i][)][2][R][2][, and][ c][R][ =]_ _N_ _i=1_ _[c][R,i][. Assume][ c][1][ = 2][c][p]G_ [+] _NK_ [+ 2][c][R][,]

_c2 =_ _Lc3ηpGg[2]_ [+] 6Lσηg[2][K][2] [ +][ Lc]3η[R]g[2] [, and][ c][3][ =][ 2]N _Ni=1_ _tτP−=11_ _[p][τ,i][R][, inequality (67) become]_

P P

E∥ωt + ∆ωt − **_ω[∗]∥[2]_** _≤_ (1 − _[µη]2 [)][E][∥][ω][t][ −]_ **_[ω][∗][∥][2][ −]_** _[η][(][f]_ [(][ω][t][)][ −] _[f][ ∗][) +][ c][1][η][2][ +][ c][2][η][3][ +][ c][3][η][ ∥][ω][t][ −]_ **_[ω][∗][∥]_** _[.]_

(70)

Move f (ωt) _f_ to the left side, we get
_−_ _[∗]_


_f_ (ωt) _f_ (ω[∗]) (71)
_−_ _≤_ _η[1]_ [(1][ −] _[µη]2 [)][E][∥][ω][t][ −]_ **_[ω][∗][∥][2][ −]_** _η[1]_ [E][∥][ω][t][+1][ −] **_[ω][∗][∥][2][ +][ c][1][η][ +][ c][2][η][2][ +][ ϕ][t][ .]_**

Herearbitrary ϕt is a constant bounded by ϵ; instead, it can only converge to the neighborhood of c3 ∥ω0 − **_ω[∗]∥. Because of ϕ ϵt +, the model cannot converge to an ϕ, where ϕ is the weighted sum_**
of the sequence ϕt. The following parts give the convergence rates when models converge to ϵ + ϕ
for convenience.

B.4.3 WEIGHTS OF ROUNDS.
We can carefully tune the pτ,i to minimize the noise while speeding up the convergence. Here we
give the lemma of the best choice of pτ,i.

_D[2]_
**Lemma B.4. On round t, when setting pτ,i =** _tD[2]+(t_ 1)R[2][ for any][ τ < t][, and][ p][t,i][ =][ (]tD[t][−][2]+([1)][R]t [2][+]1)[D]R[2][2][,]

_−_ _−_

_we can minimize c1 and c2 to get best convergence._


_Proof. Minimizing c1 and c2 is equal to minimize cpG + cR, that is,_

_N_ _t_

1
min (1 _pt,i)[2]R[2]_ + G[2] + D[2]( _p[2]τ,i[)][,]_
_p_ _N_ _−_

_i=1_ _τ_ =1

X X

_t_

_s.t._ _pτ,i = 1, ∀i = 1, . . ., N ._ (72)

_τ_ =1

X


Solving above optimization problem, we get the results.

Using lemma B.4, we have min _cpG + cR_ = G[2] + D[2][ ] _tDD[2][2]+(+(tt−1)1)RR[2][2]_ . Notice that the value of c1,
_{_ _}_ _−_

_c2, and c3 are changing over t. We use c[t]1[,][ c][t]2[, and][ c][t]3_ [to represent][ c][1][,][ c][2][, and][ c][3] [on round][ t][.]

B.4.4 CONVERGENCE RESULTS
**Strongly convex functions.** By equation (80), when ft,i(ω) are strongly convex functions, using
Lemma A.1, and define qt = (1 − _[µη]2_ [)][1][−][t][, we have]

1

_f_ (ωF ) _f_ (ω[∗]) _µ_ **_ω0_** **_ω[∗]_** exp( + _T_ _qt_ _c1η + c2η[2]_ + ϕt _._ (73)
_−_ _≤O_  _∥_ _−_ _∥[2]_ _−_ _[µηT]2 [)]_ _t=1_ _[q][t]_

  

By Lemma A.6, we have,

P


2 2
_D_ + (t 1)R
_G[2]_ + D[2] _−_

_tD[2]_ + (t 1)R[2]

 _−_





_qt(cR + cpG_ ) = _T_ _qt_ _G[2]_ + D[2] _tD[2]_ + (t − 1)R[2]
_t=1_ _t=1_ _[q][t]_ _t=1_   _−_ 

X X

_T_

P 1 _R[2]_ _D[4]_ 1

= _Tt=1_ _[q][t]_ _t=1_ _qt_ _G[2]_ + D[2] _R[2]_ + D[2][ +] (R[2] + D[2])[2] _t_ _R[2]R+[2]D[2]_ !!

X _−_

_R[2]_ _D[4]_ ln T

P

= _G[2]_ + D[2] _._ (74)
_O_ _R[2]_ + D[2][ +] (R[2] + D[2])[2] _T_
  


_T_
_t=1_ _[q][t]_
P


-----

For ϕt, by Lemma B.4, we have,


_T_

_D[2]_ _D[4]_ ln T
_qtϕt_ _R_
_t=1_ _≤O_   _D[2]_ + R[2][ −] (D[2] + R[2])[2] _T_

X


**_ω0_** **_ω[∗]_** _._ (75)
_∥_ _−_ _∥_



_ϕ =_ _Tt=1_ _[q][t]_ _t=1_ _qtϕt ≤O_ R  _D[2]_ + R[2][ −] (D[2] + R[2])[2] _T_  _∥ω0 −_ **_ω[∗]∥_** _._ (75)

X

Then defineP c[o]1 [=] _Tt=11_ _[q][t]_ _Tt=1_ _[q][t][c]1[t]_ [,][ c][o]2 [=] _Tt=11_ _[q][t]_ _Tt=1_ _[q][t][c]2[t]_ [, we have,]
P P P P

_f_ (ωF ) − _f_ (ω[∗]) − _ϕ ≤O_ _µ ∥ω0 −_ **_ω[∗]∥[2]_** exp(− _[µηT]2 [) +][ c]1[o][η][ +][ c][o]2[η][2]_ (76)
 


_σ[2]_ _cC_ _cD_

= O _µ ∥ω0 −_ **_ω[∗]∥[2]_** exp(− _Lc[µT]O_ ) + _µNKT_ [+][ c]µT[A] [+][ c]µT[B][2][ +] _µ[2]T_ [2][ +] _µ[2]T_ [3]


(77)

_D[2]R[2]_ _D[6]_
where cO = 1 + A[2] + B[2], cA = G[2] + _R[2]+D[2][,][ c][B][ =]_ (R[2]+D[2])[2][,][ c][C][ =][ Lσ]ηg[2][K][2][ +][ Lc]ηg[2][A] [,][ c][D][ =][ Lc]ηg[2][B] [.]


**General convex functions.** When ft,i(ω) are general convex functions (µ = 0), directly use
Lemma A.2 we get,


3
_√cDF 2_


_√cBF_


_σ[2]F_

_NKT_ [+]


_cCF_ [2]

_T_ [2]


_cOF_


_FcA_


_f_ (ωF ) − _f_ (ω[∗]) − _ϕ ≤O_


(78)

_D[2]R[2]_ _D[6]_
where cO = 1 + A[2] + B[2], cA = G[2] + _R[2]+D[2][,][ c][B][ =]_ (R[2]+D[2])[2][,][ c][C][ =][ Lσ]ηg[2][K][2][ +][ Lc]ηg[2][A] [,][ c][D][ =][ Lc]ηg[2][B] [, and]

_F =_ **_ω0_** **_ω[∗]_** .
_∥_ _−_ _∥[2]_

**The drift of optimal point.** Because of information loss, we can’t get the true optimal point when
considering previous rounds’ information. Instead, the drift can be described as


_D[2]_ _D[4]_ ln T
_ϕ = c[o]3_ _[∥][ω][0]_ _[−]_ **_[ω][∗][∥]_** [=][ O] _R_ _D[2]_ + R[2][ −] (D[2] + R[2])[2] _T_
 


**_ω0_** **_ω[∗]_** _._ (79)
_∥_ _−_ _∥_



**Convergence of FedAvg.** Notice that FedAvg is a special case of CFL by setting pt,i = 1 on round
_t. Having this, we derive the one round convergence of FedAvg under our formulation as,_

_f_ (ωt) _f_ (ω[∗]) (80)
_−_ _≤_ _η[1]_ [(1][ −] _[µη]2 [)][E][∥][ω][t][ −]_ **_[ω][∗][∥][2][ −]_** _η[1]_ [E][∥][ω][t][+1][ −] **_[ω][∗][∥][2][ +][ c][1][η][ +][ c][2][η][2][,]_**


_σ[2]_ _Lσ[2]_
where c1 = 2(G[2] + D[2]) + _NK_ [,][ c][2][ =][ L][(][G]3[2]η[+]g[2][D][2][)] + 6ηg[2][K][ . Then using Lemma A.1, we get the]

convergence rate when ft,i(ω) are µ strongly convex.


_cA_ _cˆC_

_f_ (ωt) − _f_ (ω[∗]) ≤O _µ ∥ω0 −_ **_ω[∗]∥[2]_** exp( _Lc[µT]O_ ) + µT[ˆ] [+] _µ[2]T_ [2]



(81)


where cO = 1 + A[2] + B[2], cA = G[2] + D[2], cC = _η[Lσ]g[2][K][2][ +][ Lc]ηg[2][A]_ [. Besides, when][ f][t,i][(][ω][)][ are general]

convex, we have,


_σ[2]F_

_NKT_ [+]


_cCF_ [2]

_T_ [2]


_cOF_


_FcA_


_f_ (ωF ) − _f_ (ω[∗]) ≤O


(82)

(83)


where cO = 1 + A[2] + B[2], cA = G[2] + D[2], cC = _η[Lσ]g[2][K][2][ +][ Lc]ηg[2][A]_ [, and][ F][ =][ ∥][ω][0][ −] **_[ω][∗][∥][2][.]_**

B.5 PROOF OF CONVERGENCE RATE FOR NON-CONVEX SETTING


+ _[L]_ ∆ωt

2 [E] _∥_ _∥[2][i]_
h _A2_

| {z }


E [f (ωt+1)] ≤ E [f (ωt)] + E [⟨∇f (ωt), ∆ωt⟩]
_A1_
| {z }


-----

For A1 part, we have,

E [⟨∇f (ωt), ∆ωt⟩]


=

_NK[−][η]_

=

_NK[−][η]_


E _⟨∇f_ (ωt), ∇f[¯]t,i(ωt,i,k)⟩ (84)
_k=1_

X  


_i=1_

_N_

_i=1_

X

_N_

_i=1_

X


_t−1_

_pτ,i∆τ,i(ωt,i,k)_
_⟩_
_τ_ =1

X


(85)


_f_ (ωt),
_⟨∇_

_f_ (ωt),
_⟨∇_


_pτ,i_ _fτ,i(ωt,i,k)_
_∇_ _−_
_τ_ =1

X

_t_

_pτ,i_ _fτ,i(ωt,i,k)_
_τ_ =1 _∇_ _⟩#_

X


_k=1_

_K_

_k=1_

X


_N_ _K_ _t_ _N_

E _f_ (ωt), _pτ,i_ _fτ,i(ωt,i,k)_ + _[η]_ (1 _pt,i)_ E _f_ (ωt) + R[2][]

_≤_ _NK[−][η]_ _i=1_ _k=1_ "⟨∇ _τ_ =1 _∇_ _⟩#_ 2N _i=1_ _−_ _∥∇_ _∥[2][i]_

X X X X  h

(86)

_N_ _K_ _t_ 2[] _t_

_η_
= E _pτ,i_ _fτ,i(ωt,i,k)_ E _f_ (ωt) E _pτ,i_ _fτ,i(ωt,i,k)_

2NK   _∇_ _−_ _∥∇_ _∥[2][i]_ _−_  _∇_

_i=1_ _k=1_ _τ_ =1 _τ_ =1

X X X h X

_N_   _[∇][f]_ [(][ω][t][)][ −]  

+ _[η]_ (1 _pt,i)_ E _f_ (ωt) + R[2][] (87)

2N _−_ _∥∇_ _∥[2][i]_

_i=1_

X  h


2[]




From equation (53) we have


2[]

+ _[η][2][σ][2]_

_NK_

 (88)


∆ωt
_∥_ _∥[2][i]_ _≤_ _NK[2][η][2]_


_t−1_

_pτ,i∆τ,i(ωt,i,k)_
_τ_ =1

X


E



_k=1_

X




+ E


_pτ,i_ _fτ,i(ωt,i,k)_
_∇_
_τ_ =1

X


_i=1_


2

Then when η ≤ 21L [, the][ E] _τ_ =1 _[p][τ,i][∇][f][τ,i][(][ω][t,i,k][)]_ term can be ignored, since the coefficient

number will less than 0. Then the remaining term in A2 is [2]N[η][2] _Ni=1[(1][ −]_ _[p][t,i][)][2][R][2][ +][ η]NK[2][σ][2]_ [.]

[P][t] 2

Then we are willing to consider the _∇f_ (ωt) − [P]τ[t] =1 _[p][τ,i][∇][f][τ,i]P[(][ω][t,i,k][)]_ . Then we have

_t_ 2[]

E _pτ,i_ _fτ,i(ωt,i,k)_

 _∇_

_τ_ =1

X

  _t_ 2[]

_[∇][f]_ [(][ω][t][)][ −]

= E _pτ,i_ _fτ,i(ωt,i,k)_ (89)

 _∇_

_τ_ =1

X

 _t_  2[]

_[∇][f]_ [(][ω][t][)][ −∇][f] [(][ω][t,i,k][) +][ ∇][f] [(][ω][t,i,k][)][ −]

2E _f_ (ωt) _f_ (ωt,i,k) + 2E _pτ,i_ _fτ,i(ωt,i,k)_ (90)
_≤_ _∥∇_ _−∇_ _∥[2][i]_  _∇_

_τ_ =1

h X

t _[∇][f]_ [(][ω][t,i,k][)][ −]  _t_

2L[2]E **_ωt_** **_ωt,i,k_** + _B[2]_ + A[2] _p[2]τ,i_ E _f_ (ωt,i,k) + _G[2]_ + D[2] _p[2]τ,i_
_≤_ _∥_ _−_ _∥[2][i]_ _τ_ =1 ! _∥∇_ _∥[2][i]_ _τ_ =1 !
h X h X

(91)

_t_

2L[2]E **_ωt_** **_ωt,i,k_** + 2L[2] _B[2]_ + A[2] _p[2]τ,i_ E **_ωt,i,k_** **_ωt_**
_≤_ _∥_ _−_ _∥[2][i]_ _τ_ =1 ! _∥_ _−_ _∥[2][i]_
h X h

_t_ _t_

+ 2 _B[2]_ + A[2] _p[2]τ,i_ E _f_ (ωt) + _G[2]_ + D[2] _p[2]τ,i_ (92)

_τ_ =1 ! _∥∇_ _∥[2][i]_ _τ_ =1 !

X h X

Combine equation (87) and (92) we have


-----

E [⟨∇f (ωt), ∆ωt⟩]

_N_ _K_ _N_

_i=1_ _[p][t,i]_
_cpB_ _,iE_ **_ωt,i,k_** **_ωt_** + η(cpB 1 )E _f_ (ωt)

_≤_ _NK[ηL][2]_ _i=1_ _k=1_ _∥_ _−_ _∥[2][i]_ _−_ _−_ P 2N _∥∇_ _∥[2][i]_

X X h h


+ _[η]_

2

Then we have


_cpG + [1]_

_N_


(1 _pt,i)R[2]_
_−_
_i=1_

X


(93)


_K_ _N_

_i=1_ _[p][t,i]_
_cpB_ _,iE_ **_ωt,i,k_** **_ωt_** + η(cpB 1 )E _f_ (ωt)
_k=1_ _∥_ _−_ _∥[2][i]_ _−_ _−_ P 2N _∥∇_ _∥[2][i]_

X h h


E [f (ωt+1)] E [f (ωt)] + _[ηL][2]_
_≤_ _NK_


_i=1_


+ _[Lη][2]_


_N_

(1 _pt,i)[2]R[2]_ + _[Lη][2][σ][2]_ (94)
_−_ 2NK
_i=1_

X


+ _[η]_

2

Because we know


_cpG + [1]_

_N_


(1 _pt,i)R[2]_
_−_
_i=1_

X


E∥ωt,i,k − **_ωt∥[2]_**

4k[2]ηl[2][L][2][c][p]B _[,i][E][∥][ω][t,i,k][−][1]_ _l_ _[c][p]B_ _[,i][(][E]_ _f_ (ωt) ) + 2k[2]ηl[2][c][p]G[,i] [+ 2][k][2][η]l[2][(]
_≤_ _[−]_ **_[ω][t][∥][2][ + 4][k][2][η][2]_** _∥∇_ _∥[2][i]_
h

That is,


_t−1_

_pτ,i)[2]R[2]_ + kηl[2][σ][2][ .]
_τ_ =1

X

(95)


E∥ωt,i,k − **_ωt∥[2]_**

_k−1_ _t−1_ _r_

4k[2]ηl[2][c][p]B _[,i][(][E]_ _f_ (ωt) ) + 2k[2]ηl[2][c][p]G[,i] [+ 2][k][2][η]l[2][(] _pτ,i)[2]R[2]_ + kηl[2][σ][2] 4k[2]ηl[2][L][2][c][p]B _[,i]_

_≤_ _r=0_ _∥∇_ _∥[2][i]_ _τ_ =1 !

X h X   (96) 

4k[2]ηl[2][c][p]B _[,i][(][E]_ _∥∇f_ (ωt)∥[2][i]) + 2k[2]ηl[2][c][p]G[,i][ + 2][k][2][η]l[2][(][P]τ[t][−]=1[1] _[p][τ,i][)][2][R][2][ +][ kη]l[2][σ][2][]_

(97)

_≤_  h 1 4k[2]ηl[2][L][2][c][p][B] _[,i]_

_−_

Let 8K [2]ηl[2][c][p]B _[,i][ ≤]_ [1][, we have,]


_cpB_ _,iE_ **_ωt,i,k_** 1 **_ωt_**

_i=1_ _k=1_ _∥_ _−_ _−_ _∥[2]_

X X

_N_ 4K [2]ηl[2][c][p]B _[,i][(][E]_ _∥∇f_ (ωt)∥[2][i]) + 2K [2]ηl[2][c][p]G[,i][ + 2][K] [2][η]l[2][(][P]τ[t][−]=1[1] _[p][τ,i][)][2][R][2][ +][ Kη]l[2][σ][2][]_

_cpB_ _,i_
_i=1_  h 1 − 4K [2]ηl[2][L][2][c][p][B] _[,i]_

X

(98)


_NK_

_≤_ _N[1]_

_≤_ _N[1]_


_N_ _t−1_

_cpB_ _,iE_ _f_ (ωt) + [1] _pτ,i)[2]R[2]_ + _[σ][2]_ (99)

_≤_ _N[1]_ _∥∇_ _∥[2][i]_ 2 _[c][p][G][,i][ + 1]2_ [(] 4K

_i=1_ _τ_ =1

X h X

_N_ _t−1_

= cpB E _f_ (ωt) + [1] ( _pτ,i)[2]R[2]_ + _[σ][2]_ (100)
_∥∇_ _∥[2][i]_ 2 _[c][p][G][ + 1]2N_ 4K

_i=1_ _τ_ =1

h X X


Then we have


-----

_N_
_i=1_ _[p][t,i]_
(L[2] + 1)cpB − 1 − 2N
P


_f_ (ωt)
_∥∇_ _∥[2][i]_


E [f (ωt+1)] ≤ E [f (ωt)] + η


+ _[Lη][2]_


_N_

(1 _pt,i)[2]R[2]_ + _[Lη][2][σ][2]_
_−_ 2NK
_i=1_

X


+ _[η]_


_cpG + [1]_


(1 _pt,i)R[2]_
_−_
_i=1_

X


1 _N_ _t−1_
+ ηL[2] ( _pτ,i)[2]R[2]_ + _[σ][2]_ (101)
2 _[c][p][G][ + 1]2N_ 4K

_i=1_ _τ_ =1 !

X X

Then we must to choose the value of pt,i for better convergence. Follow the idea that we want to train
a better model when convergent, we choose pt,i that can minimize the constant term, which is by
solving


_t_ _t−1_

_p[2]τ,i_ [+ (] _pτ,1)[2]R[2]_
_τ_ =1 _τ_ =1

X X


min _G[2]_ + D[2]


_pτ,1 = 1_ (102)
_τ_ =1

X


_s.t._


_D[2]_
We get same results as when the objective function is convex, that is, pτ,i = _tD[2]+(t_ 1)R[2][ for]

_−_

any τ < t, and pt,i = _tD(t−[2]+(1)Rt_ [2]+1)DR[2][2][, and][ min][{][G][2][ +][ D][2][ P]τ[t] =1 _[p]τ,i[2]_ [+ (][P]τ[t][−]=1[1] _[p][τ,][1][)][2][R][2][}][ =][ G][2][ +]_

_−_

_D[2][ ]_ _tDD[2][2]+(+(tt−−1)1)RR[2][2]_ . Then we simplify the equation by using c0(t), c1(t), and c2(t) to denote the

constant terms, we have

Then we have E [f (ωt+1)] ≤ E [f (ωt)] − _ηc0(t)E_ h∥∇f (ωt)∥[2][i] + ηc1(t) + η[2]c2(t) (103)

_T_ _T_

1

_c0(t)E_ _f_ (ωt) + η [1] _c2(t) + ϕ_ (104)

_T_ _∥∇_ _∥[2][i]_ _≤_ [E][ [][f] [(][ω][0][)]][ −]η [E][ [][f] [(][ω][∗][)]] _T_

_t=1_ _t=1_

X h X

Consider c0, because the value of B[2] and A can’t be constrained, the algorithm can’t converge for
very large A and B. Here we first consider when c0 < 0, and use cm to denote the min c0. Then let
_η =_ _√√KNT L_ [, we derive the convergence rate as]

_T_ 2

1 1 _√KN_ _R[2]_ _σ[2]_

_T_ Xt=1 E h∥∇f (ωt)∥[2][i] = O( _√[L][(]TNKc[f][0][ −]_ _[f][∗]m[)]_ + _√T_ _L_  _R[2]_ + D[2]  + _√2_ _NK_ !

_√KN_ _D[4]_ _√KN_ _D[4]_

+ _TL_ _cm(R[2]_ + D[2])[2][ +] _T_ _√TL_ (R[2] + D[2])[2] ) (105)

 

Then we want to analyse when the algorithm won’t converge. When the algorithm won’t converge,
that means c0 0. Because c0 = (L[2] + 1)cpB 1 _Ni=12N[p][t,i]_ = _N1_ _Ni=1[(][L][2][ + 1)(1 +][ A][2][ +]_
_≥_ _−_ _−_ P

_B[2][ P][t]τ_ =1 _[p]τ,i[2]_ [)][ −] [1][ −] _[p][t,i]2_ [. Then the value of][ A][2][, B][2][, L, p][ decide if][ c][0][ ≥] P[0][.]

Notice that in FedAvg, pt,i = 1, and c0,avg = (L[2] + 1)(1 + A[2] + B[2]) − 2[3] [, and in CFL, we can]

setting different p to get better convergence. For example, when B is large, setting pτ,i = [1]t [, we have]

_c0, = (L[2]_ + 1)(1 + A[2] + _[B]t[2]_ [)][ −] [1][ −] 2[1]t [. Thus, we draw the conclusion as]

Then we have following observations:

-  CFL converge faster than FedAvg by reducing the variance terms. For c0 = _N[1]_ _Ni=1[(][L][2][ +]_

1)(1 + A[2] + B[2][ P][t]τ =1 _[p]τ,i[2]_ [)][ −] [1][ −] _[p][t,i]2_ [, in FedAvg, we have][ p][t,i][ = 1][, then]P[ c][0][,avg][ =]

(L[2] + 1)(1 + A[2] + B[2]) − 2[3] [. However, in CFL, we can adjust][ p][, such as when setting]

_pτ,i =_ [1]t [, we have][ c][0][ = (][L][2][ + 1)(1 +][ A][2][ +][ B]t[2] [)][ −] [1][ −] 2[1]t [, then the variance term reduce]

from B[2] to _[B]t[2]_ [.]


-----

-  The convergence rate of CFL become better for larger t, since cpB in c0 become smaller for
_larger t._

-  ImproveHowever, larger N, K will speed up the convergence by reducing the terms about N and K will cause larger information loss and round drift (terms with f0 − _f∗_ and σ R[2].
and D), thus, it should be a trade off in practice.

EXPERIMENT DETAILS


C.1 NOISY QUADRATIC MODEL

4

CFL

3 FedAvg

2 FedProx

loss 1

0

1

2

0 10 20 30 40 50

rounds


(a) Small L, strongly convex, small round drift


4

CFL

3 FedAvg

2 FedProx

loss 1

0

1

2

0 10 20 30 40 50

rounds


(b) Large L, strongly convex, small round drift


4.50 CFL

FedAvg

4.25 FedProx

4.00

loss

3.75

3.50

3.25

0 10 20 30 40 50

rounds


4.2

CFL
FedAvg

4.0 FedProx

3.8

loss

3.6

3.4

3.2

0 10 20 30 40 50

rounds


(c) Small L, general convex, small round drift

CFL
FedAvg

3 FedProx

loss 2

1

0 10 20 30 40 50

rounds


(e) Small L, strongly convex, big round drift


(d) Large L, general convex, small round drift

4 CFL

FedAvg

3 FedProx

loss 2

1

0 10 20 30 40 50

rounds


(f) Large L, strongly convex, big round drift


4.4

CFL

4.2 FedAvg

4.0 FedProx

loss 3.8

3.6

3.4

3.2

0 10 20 30 40 50

rounds

(h) Large L, general convex, big round drift


CFL

4.25 FedAvg

4.00 FedProx

loss 3.75

3.50

3.25

3.00

0 10 20 30 40 50

rounds


(g) Small L, general convex, big round drift


Figure 2: Performance of different algorithms on noisy quadratic model. The curves all evaluate the loss on
global test datasets.

We use the noisy quadratic model introduced in Zhang et al. (2019) to simulate the assumptions
we used in the proof. We use the model f (ω) = ω[T] **Aω + B[T]** **_ω + C, where ω ∈_** R[n] is the
parameter we want to optimize, and A ∈ R[n][×][n], A ⪰ 0, B ∈ R[n], and C ∈ R. We construct A by
firstly constructing a diagonal matrix Λ and then constructing A by let A = U[T] ΛU, where U is
a unitary matrix. Here we control the eigenvalues of A to control the convexity of model, that is,


-----

SRD BRD

Method SL-SC LL-SC SL-GC LL-GC SL-SC LL-SC SL-GC LL-GC

FAVG 0.03 0.08 0.33 0.09 0.02 0.01 0.09 0.02
FPROX 0.04 0.07 0.35 0.09 0.02 0.01 0.26 0.02
CFL 0.2 0.09 0.35 0.09 0.25 0.08 0.3 0.08

Table 7: Best learning rate of different algorithms on noisy quadratic model

_µ_ _λmin(A)_ _λmax(A)_ _L. Because Λ and A have same eigenvalues, it’s simple to control the_
_≤_ _≤_ _≤_
eigenvalues of A by control the eigenvalues of Λ.
To simulate Assumption 2, 3, we formulate the gradient noise model by

_gt,i(ω) = Aω + B + δi + ξt,i + νt,i,k,_ (106)

where δi denotes the client drift of client j, ξt,i denotes the round drift of client j on round i, and
**_νt,i,k denotes the noise of gradient of client j on round i, iteration k. All of above drifts and noise_**
are generated from Normal distribution with zero mean and different variance.
In practice, to show if the numerical results match our theoretical results, we tried eight settings: large
and small L, large and small round drift, and general and strongly convex conditions. For different
types of L, we set L = 20 for large L, and L = 5 for small L. For convexity, we set µ = 1 for
strongly convex, and µ = 0 for general convex. For different round drift, we set E∥ξt,i∥[2] = 100 for
big round drift, and E∥ξt,i∥[2] = 0.01 for small round drift. Besides, we set fixed variance of δi and
**_νt,i,k for E∥δi∥[2]_** = 0.01, and E∥νt,i,k∥[2] = 0.00001.
We do the gradient descent by letbecause for convex functions, when it reaches the global optimal point, we should have ωt,i,k+1 = ωt,i,k − _ηlgt,i(ωt,i,k), and we use ∥Aω ∥ +A Bω +∥_ as loss B∥ =
0, and if it is far away from global optimal, ∥Aω + B∥ will be large.
Besides, we show that CFL algorithms can tolerate a larger learning rate here. Table 7 shows the best
learning rates for different FL algorithms on noisy quadratic model. In Table 7, we denote small
round drift as SRD, big round drift as BRD, small L as SL, large L as LL, strongly convex as SC,
and general convex as GC. We show that the best learning rate of CFL is the largest. The difference
between CFL methods and other FL baselines becomes large when models are strongly convex or
with big round drift.
Figure 2 shows the performance of different algorithms, and CFL outperforms other FL baselines
significantly in all settings. Besides, the loss curves of CFL are smoother than FL methods, which
implies that CFL can reduce the variance of gradients.

C.2 REALISTIC DATASETS
C.2.1 SETUP
We consider federated learning an image classifier on split-CIFAR10 and split-CIFAR100 datasets
with ResNet18, and split-Fashion-MNIST dataset with a two-layer MLP. The “split” follows the idea
introduced in Yurochkin et al. (2019); Hsu et al. (2019); Reddi et al. (2021), where we leverage the
Latent Dirichlet Allocation (LDA) to control the distribution drift with parameter α (See Algorithm
4). Larger α denotes smaller drifts here.
In our experiments, unless specifically mentioned otherwise all datasets are partitioned to 210 subsets
for 7 different clients: all clients are selected and trained for 500 communication rounds, and each
client samples one of the corresponding 30 subsets randomly for the local training. Note that unless
mentioned otherwise the training strategy here applies to all FL baselines and CFL methods, and we
evaluate the performance of models on global test datasets. We carefully tuned the hyper-parameters
in all algorithms, and we report the results under the optimal settings after many trials. For CFLRegularization, we set the weight of regularization term for β = 1 on fc layer, and β = 0.1 for last
block (layer). For FedProx, we set the weight of proximal term µ = 0.1, and for MimeLite, we

improvement on final results.

**Construct local datasets with overlap.** Follow the partition methods of disjoint local
datasets; we first split the whole dataset to


. Besides, for all experiments, we set fixed learning
. We also naively examined warm-up tuning strategies but can not observe significant


-----

_M × S subsets for S different clients: each_
client has M disjoint local subsets, and we put
these M subsets in sequence. For each client,
we use a pointer to point to the start position
of the current local dataset. Figure 3 show the
case whensubsets of client M = 5 i. The pointer point to the start: D1 − _D5 are 5 disjoint local_
point of a local dataset of the current round and the blue lines bound current subsets. At the beginning
of training, pointers belong to each client will point to the beginning of the sequence. At the end of
each round, the pointer moves s steps, and if it moves to the end of all the sequence, it will come
back to the start point.
In practice, the Cifar10 dataset is partitioned to 210 subsets for 7 clients, and set the size of local
dataset Sl = 285. Then when we set s = 285, the local datasets are disjoint, which means the overlap
is 0%. Otherwise, when set s = 213, the overlap is 25%, and when set s = 142, the overlap is 50%.

C.2.2 APPROXIMATION METHODS
**Regularization Methods.** We use Taylor Extension to approximate the objective functions of
previous rounds, that is,

_f˜(ω) = f_ (ˆω) + _f_ (ˆω)[T] (ω **_ωˆ_** ) + [1] **_ω)[T]_** _f_ (ˆω)(ω **_ωˆ_** ), (107)
_∇_ _−_ 2 [(][ω][ −] [ˆ] _∇[2]_ _−_

where ˆω are the parameters of previous rounds. Then the gradient of _f[˜](ω) is,_

_∇f[˜](ω) = ∇f_ (ˆω) + ∇[2]f (ˆω)(ω − **_ωˆ_** ) . (108)

Having this, we can approximate the gradients of objective functions of previous rounds. The problem
is how to approximate ∇[2]f (ˆω). Here we use two kinds of methods to calculate the Hessian matrices.
The first is to use the Fisher Information Matrix as introduced in EWC (Kirkpatrick et al., 2017). The
Fisher Information Matrix is equal to the diagonal Hessian matrix when using cross entropy loss
functions. The Fisher Information Matrix can be calculated by,

_Fij = [∇f_ (ˆω)][2]ij _[,]_ (109)

where Fij are the entries of Fisher Information Matrix. Another method is to calculate the diagonal
Hessian matrix. We use the PyHessian package (Yao et al., 2020) in this experiment.
In addition, if the parameters or the Hessian matrix changes too much, the performance of this method
is constrained. To formulate this problem, assume _∇[2]f_ (ω) _≤_ _ϵ for some arbitrary ϵ, we have_

∆t,i(ω) _ϵ_ **_ω_** **_ωˆ_** _._
_∥_ _∥≤_ _∥_ _−_ _∥[2]_

The corresponding proof refers to B.3.
In each round, the server will collect Hessian matrices from clients, and store them in a buffer.
Then at the beginning of next round, server combine send latest 40 Hessian matrices, gradients, and
parameters, and send them to chosen clients. Each clients use these to calculate regularization terms.
In practice, we only add regularization terms to the top layers (last block and fc layer of ResNet18).
This is based on the assumption that top layers contain more personal information while bottom
layers contain more general information. We verified that this strategy perform better than adding
regularization terms to all layers in experiments.

**Generation Methods.** We use MCMC to generate samples in each round. In practice, we initialize
**x from uniform distribution, and update x by,**

**x[k]** = x[k][−][1] _η_ **xE(x[k][−][1]) + ω,** (110)
_−_ _∇_

where E(x[k] _−_ 1) is the function that can measure the distance between x[k][−][1] and the real local
distribution. ω ∼N (0, σ).
In practice, we generate 50-100 samples of each local dataset and add them to the current local
datasets for training. However, because of the low quality of the generated data, the improvement is
limited. Besides, the generated data will pollute the batch normalization(BN) layer, and we should
use real data to refresh BN layer at the end of each round.


-----

**Algorithm 3 iCaRL Construct Core Set**
**Require: Image set X = {x1, x2, x3, ..., xn} of class y, m: target number of samples, φ : X →R[d]:**
current feature function

1: µ ← _n[1]_ **x∈X** _[φ][(][x][)]_

2: for k = 1, ..., m do

P

3: _pk_ arg minx **X** _µ_ _k_ _φ(x) +_ _j=1_ _[φ][(][p][i][)]_
_←_ _∈_ _−_ [1]

4: P (p1, p2, ..., pm)  
_←_

[P][k][−][1]

**Core Set Methods.** Another simple yet effective treatment in CL lines in the category of Exemplar
Replay (Rebuffi et al., 2017; Castro et al., 2018). This approach stores past core-set samples (a.k.a.
exemplars) selectively and periodically, and replays them together with the current local datasets.
We tried two sample methods. First is so-called Naive method, in which the samples are uniformly
chosen from local datasets. Except of naive select core sets, we also tried another core set sampling
method introduced in iCaRL (Rebuffi et al., 2017). See Algorithm 3 for details.
In practice, we save 100 figures of each local dataset and combine them with the current local datasets.
Saving core sets perform best compared with these three approximation methods; however, only valid
when the number of clients is limited.

C.2.3 ADDITIONAL EXPERIMENTS
**Applicability of different algorithms.** We list the applicability of different algorithms under
various time-varying scenarios in Table 8.

Table 8: The applicability of various algorithms under different time-varying scenarios.

FL baselines CFL methods
Scenarios

FedAvg FedProx SCAFFOLD MimeLite CFL-Regularization CFL-Core-Set

Stateful clientsStateless clients _√√_ _√√_ _××_ _√√_ _√√_ _√×_

**Convergence curves for different settings.** Figure 4 show convergence curves of CFL and FedAvg
on different datasets. The settings are the same as results in Table 3. Models are trained on partitioned
datasets with α = 0.1, and all datasets are partitioned to 210 subsets for 7 clients. To show the
difference between different algorithms more clearly, all curves are smoothed by a 1D-Mean-Filter.
Results show CFL can converge to a better optimum compare with FedAvg.

**Investigating resistance of CFL to time-evolving scenarios.** Figure 6 shows the loss curve of
CFL-Regularization and FedAvg on different datasets. Models are trained on partitioned datasets with
_α = 0.1, and all datasets are partitioned to 210 subsets for 7 clients. The first column shows the loss_
curve of the first 300 rounds, and the second column shows the loss of some chosen details. Because
of the non-iidness of local datasets, the loss will suddenly rise. Notice that CFL-Regularization has
_an apparent mitigation effect on this situation._

**Difference between training and test loss.** Figure 5 show the loss on global test data and past
appeared training data. We evaluate the model with stateful clients, set α = 0.2, and use FedAvg
algorithm. We show that there is no significant difference between loss value on global test data and
_past appeared training data._

C.3 ALGORITHMS


-----

0.9

0.8 0.8

0.7

0.7

0.6

0.6

0.5

Accuracy 0.5 Accuracy 0.4

0.4

0.3

FedAvg FedAvg

0.3 CFL Without Core Set 0.2 CFL Without Core Set

CFL With Core Set CFL With Core Set

0.2 0.1

0 100 200 300 400 500 0 100 200 300 400 500

Communication Round Communication Round

(a) Performance on Fashion-MNIST


Communication Round

(b) Performance on Cifar10


0.6

0.5

0.4

0.3

Accuracy

0.2

FedAvg

0.1 CFL Without Core Set

CFL With Core Set

0

0 100 200 300 400 500

Communication Round

(c) Performance on Cifar100

Figure 4: Models are trained on various datasets with α = 0.1. CFL Without Core Set method use regularization
methods, and CFL With Core Set methods use core set methods. All these two CFL algorithms use FedAvg as
backbone.


3.5 0.7

Loss on Global Test Data
Loss on Past Appeared Data

3 0.6

2.5 0.5

Loss 2 0.4

Accuracy

1.5 0.3

1 0.2

Accuracy on Global Test Data
Accuracy on Past Appeared Data

0.5 0.1

0 10 20 30 40 50 60 70 80 90 0 10 20 30 40 50 60 70 80 90

Communication Round Communication Round


(a) Loss on global test data and past appeared
train data


(b) Accuracy on global test data and past appeared train data


Figure 5: Evaluation on global test data and past appeared training data. We trained ResNet18 on split-Cifar10
dataset with α = 0.2 for 85 rounds.


-----

2.2 1.3

FedAvg FedAvg

2 CFL-Regularization 1.2 CFL-Regularization

1.8 1.1

1.6 1

1.4 0.9

Loss Loss

1.2 0.8

1 0.7

0.8 0.6

0.6 0.5

0.4 0.4

0 50 100 150 200 250 300 100 105 110 115 120 125 130

Communication Round Communication Round


(a) Loss on Fashion-MNIST

2.4 1.6

FedAvg FedAvg

2.2 CFL-Regularization 1.5 CFL-Regularization

2 1.4

1.8 1.3

1.6 1.2

Loss Loss

1.4 1.1

1.2 1

1 0.9

0.8 0.8

0.6 0.7

0 50 100 150 200 250 300 100 105 110 115 120 125 130

Communication Round Communication Round


(c) Loss on Cifar10


(b) Loss on Fashion-MNIST (Local)

FedAvg
CFL-Regularization

105 110 115 120 125

Communication Round

(d) Loss on Cifar10 (Local)


Figure 6: Models are trained on split-Fashion-MNIST and split-Cifar10 datasets with α = 0.1. The loss is
evaluated on global test datasets. Left column is the full curve of 300 rounds, and figures in right column are
partially enlarged curves.

**Algorithm 4 Data splitting**
**Require: S: a list of datasets split by labels, M** : the number of clients, N : the size of local datasets,
_α: the Dirichlet distribution parameter_

**Ensure: D: split datasets**

1: D ← []
2: G ← [0, 1, 2, ..., len(S)]
3: for m = 1, 2, ..., M do
4: _p = [p1, p2, ..., plen(S)], where pt,i denotes the fraction of class i in total dataset._

5: _θ ←_ _Dirichlet(α, p)_

6: _Dm_
_←∅_

7: **while len(Dm) < N do**

8: _i ←_ _multinomial(θ, 1)_

9: _y ←_ _G[i]_

10: _data ←_ _uniform(S[y])_

11: _Dm_ _Dm_ _data_
_←_ _∪{_ _}_

12: _S[y] ←_ _S[y]/data_

13: **if len(S[y]) == 0 then**

14: _G ←_ _G/y_

15: _θ ←_ _renormalize(θ, i)_

16: _D.append(Dm)_


**Algorithm 5 renormalize**
**Require: θ: weights for different classes, i: the class that should be deleted**
**Ensure: θ: renormalized weights**

2:1: for jθ = 1[j] ←, 2θ, ..., len[j]/sum(θ()theta/theta, j ̸= i do [i])


-----

**Algorithm 6 SplitdataMain**
**Require: S: total dataset split by class, T** : rounds, K, N, α, β
**Ensure: Dfinal: split dataset**

1: D ← _splitData(S, K, N, α)_
2: Dfinal []
3: for i = 1 ←, 2, ..., K do
4: _Si_ _splitByClass(D[i])_
_←_

5: _Nlocal_ _len(Dm)/T_
_←_

6: _Di_ _splitData(Si, cluster_num, Nlocal, β)_
_←_

7: _Dfinal_ _Dfinal_ _Di_
_←_ _∪_


-----

