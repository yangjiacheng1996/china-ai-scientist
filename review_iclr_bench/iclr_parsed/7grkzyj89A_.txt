# GENERALIZATION THROUGH THE LENS OF LEAVE## ONE-OUT ERROR

Gregor Bachmann [a], Thomas Hofmann[a], and Aur´elien Lucchi[b]

aDepartment of Computer Science, ETH Z¨urich, Switzerland
bDepartment of Mathematics and Computer Science, University of Basel,
_{gregor.bachmann, thomas.hofmann}@inf.ethz.ch_
aurelien.lucchi@unibas.ch

ABSTRACT

Despite the tremendous empirical success of deep learning models to solve various learning tasks, our theoretical understanding of their generalization ability is
very limited. Classical generalization bounds based on tools such as the VC dimension or Rademacher complexity, are so far unsuitable for deep models and it
is doubtful that these techniques can yield tight bounds even in the most idealistic
settings (Nagarajan & Kolter, 2019). In this work, we instead revisit the concept
of leave-one-out (LOO) error to measure the generalization ability of deep models
in the so-called kernel regime. While popular in statistics, the LOO error has been
largely overlooked in the context of deep learning. By building upon the recently
established connection between neural networks and kernel learning, we leverage
the closed-form expression for the leave-one-out error, giving us access to an efficient proxy for the test error. We show both theoretically and empirically that the
leave-one-out error is capable of capturing various phenomena in generalization
theory, such as double descent, random labels or transfer learning. Our work therefore demonstrates that the leave-one-out error provides a tractable way to estimate
the generalization ability of deep neural networks in the kernel regime, opening
the door to potential, new research directions in the field of generalization.

1 INTRODUCTION

Neural networks have achieved astonishing performance across many learning tasks such as in computer vision (He et al., 2016), natural language processing (Devlin et al., 2019) and graph learning
(Kipf & Welling, 2017) among many others. Despite the large overparametrized nature of these
models, they have shown a surprising ability to generalize well to unseen data. The theoretical understanding of this generalization ability has been an active area of research where contributions have
been made using diverse analytical tools (Arora et al., 2018; Bartlett et al., 2019; 2017; Neyshabur
et al., 2015; 2018). Yet, the theoretical bounds derived by these approaches typically suffer from one
or several of the following limitations: i) they are known to be loose or even vacuous (Jiang et al.,
2019), ii) they only apply to randomized networks (Dziugaite & Roy, 2018; 2017), and iii) they
rely on the concept of uniform convergence which was shown to be non-robust against adversarial
perturbations (Nagarajan & Kolter, 2019).
In this work, we revisit the concept of leave-one-out error (LOO) and demonstrate its surprising
ability to predict generalization for deep neural networks in the so-called kernel regime. This object
is an important statistical estimator of the generalization performance of an algorithm that is theoretically motivated by a connection to the concept of uniform stability (Pontil, 2002). It has also
recently been advocated by Nagarajan & Kolter (2019) as a potential substitute for uniform convergence bounds. Despite being popular in classical statistics (Cawley & Talbot, 2003; Vehtari et al.,
2016; Fukunaga & Hummels, 1989; Zhang, 2003), LOO has largely been overlooked in the context
of deep learning, likely due to its high computational cost. However, recent advances from the neural
tangent kernel perspective (Jacot et al., 2018) render the LOO error all of a sudden tractable thanks
to the availability of a closed-form expression due to Stone (1974, Eq. 3.13). While the role of the
LOO error as an estimator of the generalization performance is debated in the statistics community


-----

(Zhang & Yang, 2015; Bengio & Grandvalet, 2004; Breiman, 1996; Kearns & Ron, 1999), the recent
work by Patil et al. (2021) established a consistency result in the case of ridge regression, ensuring
that LOO converges to the generalization error in the large sample limit, under mild assumptions on
the data distribution.
Inspired by these recent advances, in this work, we investigate the use of LOO as a generalization
measure for deep neural networks in the kernel regime. We find that LOO is a surprisingly rich
descriptor of the generalization error, capturing a wide range of phenomena such as random label
fitting, double descent and transfer learning. Specifically, we make the following contributions:

-  We extend the LOO expression for the multi-class setting to the case of zero regularization
and derive a new closed-form formula for the resulting LOO accuracy.

-  We investigate both the LOO loss and accuracy in the context of deep learning through
the lens of the NTK, and demonstrate empirically that they both capture the generalization
ability of networks in the kernel regime in a variety of settings.

-  We showcase the utility of LOO for practical networks by accurately predicting their transfer learning performance.

-  We build on the mathematically convenient form of the LOO loss to derive some novel
insights into double descent and the role of regularization.

Our work is structured as follows. We give an overview of related works in Section 2. In Section 3
we introduce the setting along with the LOO error and showcase our extensions to the multi-class
case and LOO accuracy. Then, in Section 4 we analyze the predictive power of LOO in various
settings and present our theoretical results on double descent. We discuss our findings and future
work in Section 5. We release the code for our numerical experiments on Github[1].

2 RELATED WORK

Originally introduced by Lachenbruch (1967); Mosteller & Tukey (1968), the leave-one-out error is
a standard tool in the field of statistics that has been used to study and improve a variety of models, ranging from support vector machines (Weston, 1999), Fisher discriminant analysis (Cawley
& Talbot, 2003), linear regression (Hastie et al., 2020; Patil et al., 2021) to non-parametric density
estimation (Fukunaga & Hummels, 1989). Various theoretical perspectives have justified the use
of the LOO error, including for instance the framework of uniform stability (Bousquet & Elisseeff,
2002) or generalization bounds for KNN models (Devroye & Wagner, 1979). Elisseeff et al. (2003)
focused on the stability of a learning algorithm to demonstrate a formal link between LOO error
and generalization. The vanilla definition of the LOO requires access to a large number of trained
models which is typically computationally expensive. Crucially, Stone (1974) derived an elegant
closed-form formula for kernels, completely removing the need for training multiple models. In the
context of deep learning, the LOO error remains largely unexplored. Shekkizhar & Ortega (2020)
propose to fit polytope interpolators to a given neural network and estimate its resulting LOO error.
Alaa & van der Schaar (2020) rely on a Jackknife estimator, applied in a LOO fashion to obtain
better confidence intervals for Bayesian neural networks. Finally, we want to highlight the concurrent work of Zhang & Zhang (2021) who analyze the related concept of influence functions in the
context of deep models.

Recent works have established a connection between kernel regression and deep learning. Surprisingly, it is shown that an infinitely-wide, fully-connected network behaves like a kernel both at
initialization (Neal, 1996; Lee et al., 2018; de G. Matthews et al., 2018) as well as during gradient
flow training (Jacot et al., 2018). Follow-up works have extended this result to the non-asymptotic
setting (Arora et al., 2019a; Lee et al., 2019), proving that wide enough networks trained with small
learning rates are in the so-called kernel regime, i.e. essentially evolving like their corresponding
tangent kernel. Moreover, the analysis has also been adapted to various architectures (Arora et al.,
2019a; Huang et al., 2020; Du et al., 2019; Hron et al., 2020; Yang, 2020) as well as discrete gradient
descent (Lee et al., 2019). The direct connection to the field of kernel regression enables the usage
of the closed-form expression for LOO error in the context of deep learning, which to the best of our
knowledge has not been explored yet.

[1https://github.com/gregorbachmann/LeaveOneOut](https://github.com/gregorbachmann/LeaveOneOut)


-----

3 SETTING AND BACKGROUND

In the following, we establish the notations required to describe the problem of interest and the
setting we consider. We study the standard learning setting, where we are given a dataset of input_i.i.d._
target pairs = (x1, y1), . . ., (xn, yn) where each (xi, yi) for i = 1, . . ., n is distributed
_S_ _{_ _}_ _∼D_
according to some probability measureR[C] as the target. We will often use matrix notations to obtain more compact formulations, thus D. We refer to xi ∈X ⊂ R[d] as the input and to y ∈Y ⊂
summarizing all inputs and targets as matrices X ∈ R[n][×][d] and Y ∈ R[n][×][C]. We will mainly be
concerned with the task of multiclass classification, where targets yi are encoded as one-hot vectors.
We consider a function space F and an associated loss function Lf : X × Y −→ R that measures
how well a given function f ∈F performs at predicting the targets. More concretely, we define the
regularized empirical error as


_Lˆ[λ]_ [:][ F −] R, f
_S_ _→_ _7→_ _n[1]_


_Lf_ (xi, yi) + λΩ(f ),
_i=1_

X


letwhere v[∗] _λ >= argmax 0 is a regularization parameter andk≤C vk. Using this notation, we let Ω: F − a→f_ (xR, is a functional. For any vector y) := 1{f ∗(x)=y∗}. We then define the v ∈ R[C],
**empirical accuracy** _A[ˆ]_ as the average number of instances for which f provides the correct
_S_ _∈F_
prediction, i.e.

_n_

_Aˆ_ : [0, 1], f _af_ (xi, yi)
_S_ _F −→_ _7→_ _n[1]_

_i=1_

X

Finally, we introduce a learning algorithm : ( )[n], that given a function class
_QF_ _X × Y_ _−→F_ _F_
and a training set S ∈ (X × Y)[n], chooses a function f ∈F. We will exclusively be concerned with
learning through empirical risk minimization, i.e.

_f_ _[λ]_ [:= argmin]f _L[ˆ][λ]_
_QF[λ]_ [(][S][) := ˆ]S _∈F_ _S_ [(][f] [)][,]

and use the shortcut _f[ˆ]_ := f[ˆ][λ][=0]. In practice however, the most important measures are given by
_S_ _S_
the generalization loss and accuracy of the model, defined as

_L : F −→_ R, f 7→ E(x,y)∼D [Lf (x, y)], _A : F −→_ [0, 1], f 7→ P(x,y)∼D (f _[∗](x) = y[∗]) ._

A central goal in machine learning is to control the difference between the empirical error _L[ˆ]_ (f )
_S_
and the generalization error L(f ). In this work, we mainly focus on the mean squared loss,


_C_

_Lf_ (x, y) = [1] (fk(x) _yk)[2],_

2 _−_

_k=1_

X


and also treat classification as a regression task, as commonly done in the literature (Lee et al.,
2018; Chen et al., 2020; Arora et al., 2019a; Hui & Belkin, 2021; Bachmann et al., 2021). Finally,
we consider the function spaces induced by kernels and neural networks, presented in the following.

3.1 KERNEL LEARNING

A kernel K : X × X −→ R is a symmetric, positive semi-definite function, i.e. K(x, x[′]) =
_K(x[′], x) ∀_ **_x, x[′]_** _∈X and for any set {x1, . . ., xn} ⊂X_, the matrix K ∈ R[n][×][n] with entries
_Kij = K(xi, xj) is positive semi-definite. It can be shown that a kernel induces a reproducing_
_kernel Hilbert space, a function space equipped with an inner product_ _,_, containing elements
_⟨·_ _·⟩H_

_n_

_H = cl_ _f : fk(·) =_ _i=1_ _αikK(·, xi) for α ∈_ R[n][×][C], xi ∈X
 X []

where cl is the closure operator. Although H is an infinite dimensional space, it turns out that the
minimizer _f[ˆ][λ]_ [can be obtained in closed form for mean squared loss, given by]
_S_

_fˆ[λ]_ **_x_** [(][K][ +][ λ][1][n][)][−][1][ Y] _λ−→0_ _f_ (x) := Kx[T] **_[K][†][Y]_**
_S_ [(][x][) =][ K][T] _−−−→_ [ˆ]S


-----

where the regularization functional is induced by the inner product of the space, Ω(f ) = ||f _||[2]H[,]_
**_K ∈_** R[n][×][n] has entries Kij = K(xi, xj) and Kx ∈ R[n] has entries (Kx)i = K(x, xi). A[†] denotes
the pseudo-inverse of A R[n][×][n]. We will analyze both the regularized (f[ˆ][λ]
_∈_ _S_ [) and unregularized]
model (f[ˆ] ) and their associated LOO errors in the following. For a more in-depth discussion of
_S_
kernels, we refer the reader to Hofmann et al. (2008); Paulsen & Raghupathi (2016).

3.2 NEURAL NETWORKS AND ASSOCIATED KERNELS

Another function class that has seen a rise in popularity is given by the family of fully-connected
neural networks of depth L ∈ N,

_FNN =_ _fθ : fθ(x) = W_ [(][L][)]σ **_W_** [(][L][−][1)] _. . . σ_ **_W_** [(1)]x _. . ._ _, for W_ [(][l][)] _∈_ R[d][l][×][d][l][−][1] _,_
n     o

for dl ∈ N, d0 = d, dL = C and σ : R −→ R is an element-wise non-linearity. We denote
by θ R[m] the concatenation of all parameters **_W_** [(1)], . . ., W [(][L][)][], where m = _i=0_ _[d][i][d][i][+1]_
_∈_
is the number of parameters of the model. The parameters are initialized randomly according to
 
_Wij[(][l][)]_ _w[)][ for][ σ][w]_ _[>][ 0][ and then adjusted to optimize the empirical error][ ˆ]L_, usually through[P][L][−][1]
_S_
an iterative procedure using a variant of gradient descent. Typically, the entire vector[∼N] [(0][, σ][2] **_θ is optimized_**
via gradient descent, leading to a highly non-convex problem which does not admit a closed-form
solution _f[ˆ], in contrast to kernel learning. Next, we will review settings under which the output of a_
deep neural network can be approximated using a kernel formulation.

**Random Features.** Instead of training all the parameters θ, we restrict the function space by only
optimizing over the top layer W [(][L][)] and freezing the other variables W [(1)], . . ., W [(][L][−][1)] to their
value at initialization. This restriction makes the model linear in its parameters and induces a finitedimensional feature map

_φRF : R[d]_ _−→_ R[d][L][−][1] _, x 7→_ _σ_ **_W_** [(][L][−][1)] _. . . σ_ **_W_** [(1)]x _. . ._ _._
   

Originally introduced by Rahimi & Recht (2008) to enable more efficient kernel computations, the
random feature model has recently seen a strong spike in popularity and has been the topic of many
theoretical works (Jacot et al., 2020; Mei & Montanari, 2021).

**Infinite Width.** Another, only recently established correspondence emerges in the infinite-width
scenario. By choosing σw = _√1dl for each layer, Lee et al. (2018) showed that at initialization, the_
network exhibits a Gaussian process behaviour in the infinite-width limit. More precisely, it holds
_i.i.d._
that fk _∼GP_ 0, Σ[(][L][)][], for k = 1, . . ., C, where Σ[(][L][)] : R[d] _×_ R[d] _−→_ R is a kernel, coined NNGP,
obeying the recursive relation
 

Σ[(][l][)](x, x[′]) = Ez∼N (0,Σ(l−1)|x,x′ ) _σ(z1)σ(z2)_

1  
with base Σ[(1)](x, x[′]) = _√d_ **_[x][T][ x][′][ and][ Σ][(][l][−][1)][|][x][,][x][′][ ∈]_** [R][2][×][2][ is][ Σ][(][l][−][1)][ evaluated on the set][ {][x][,][ x][′][}][.]

To reason about networks trained with gradient descent, Jacot et al. (2018) extended this framework
and proved that the empirical neural tangent kernel converges to a constant kernel Θ. The so-called
neural tangent kernel Θ : R[d] _× R[d]_ _−→_ R is also available through a recursive relation involving the
NNGP:
Θ[(][l][+1)](x, x[′]) = Θ[(][l][)](x, x[′]) Σ[˙] [(][l][+1)](x, x[′]) + Σ[(][l][+1)](x, x[′]),

with base Θ[(1)](x, x[′]) = Σ[(1)](x, x[′]) and Σ[˙] [(][l][)](x, x[′]) = Ez∼N (0,Σ(l−1)|x,x′ _σ˙_ (z1) ˙σ(z2) . Training
infinitely-wide networks with gradient descent thus reduces to kernel learning with the NTK Θ[(][L][)].
 
Arora et al. (2019a); Lee et al. (2019) extended this result to a non-asymptotic setting, showing that
very wide networks trained with small learning rates become indistinguishable from kernel learning.

3.3 LEAVE-ONE-OUT ERROR

The goal of learning is to choose a model f ∈F with minimal generalization error L(f ). Since
the data distribution D is usually not known, practitioners obtain an estimate for L(f ) by using an


-----

additional test set Stest = {(x[t]1[,][ y]1[t] [)][, . . .,][ (][x][t]b[,][ y]b[t][)][}][ not used as input to the learning algorithm][ Q][F] [,]
and calculate


_Ltest = [1]_


_b_

_Lf_ (x[t]i[,][ y]i[t][)][,] _Atest = [1]_

_b_

_i=1_

X


_af_ (x[t]i[,][ y]i[t][)] (1)
_i=1_

X


While this is a simple approach, practitioners cannot always afford to put a reasonably sized test set
aside. This observation led the field of statistics to develop the idea of a leave-one-out error (LOO),
allowing one to obtain an estimate of L(f ) without having to rely on additional data (Lachenbruch,
1967; Mosteller & Tukey, 1968). We give a formal definition below.

**Definition 3.1. Consider a learning algorithm** _and a training set_ = (xi, yi) _i=1[. Let][ S][−][i][ =]_
_QF_ _S_ _{_ _}[n]_
_S \ {(xi, yi)} be the training set without the i-th data point (xi, yi) and define_ _f[ˆ][−][i]_ := QF (S−i),
_the corresponding model obtained by training on_ _i. The leave-one-out loss/accuracy of_ _on_
_S−_ _QF_ _S_
_is then defined as the average loss/accuracy of_ _f[ˆ]_ _incurred on the left out point (xi, yi), given by_

_[−][i]_


_LLOO (_ ; ) = [1]
_QF_ _S_ _n_


_n_

_L ˆf_ _[−][i]_ [(][x][i][,][ y][i][)][,] _ALOO(QF_ ; S) = n[1]
_i=1_

X


_a ˆf_ _[−][i][ (][x][i][,][ y][i][)]_
_i=1_

X


Notice that calculating LLOO can indeed be done solely based on the training set. On the other hand,
using the training set to estimate L(f ) comes at a cost. Computing _f[ˆ][−][i]_ for i = 1, . . ., n requires
fitting the given model class n times. While this might be feasible for smaller models such as random
forests, the method clearly reaches its limits for larger models and datasets used in deep learning.
For instance, evaluating LLOO for ResNet-152 on ImageNet requires fitting a model with 6 × 10[7]
parameters 10[7] times, which clearly becomes extremely inefficient. At first glance, the concept of
leave-one-out error therefore seems to be useless in the context of deep learning. However, we will
now show that the recently established connection between deep networks and kernel learning gives
us an efficient way to compute LLOO. Another common technique is K-fold validation, where the
training set is split into K equally sized folds and the model is evaluated in a leave-one-out fashion
on folds. While more efficient than the vanilla leave-one-out error, K-fold does not admit a closedform solution, making the leave-one-out error more attractive in the setting we study.

In the following, we will use the shortcut _f[ˆ][λ]_ [:= ˆ]f _[λ]. Surprisingly, it turns out that the space of_
_S_
kernels using [with mean-squared loss admits a closed-form expression for the leave-one-out]
_QF[λ]_
error, only relying on a single model obtained on the full training set (X, Y ):

**Theorem 3.2. Consider a kernel K : R[d]×R[d]** _−→_ R and the associated objective with regularization
_parameter λ > 0 under mean squared loss. Define A = K (K + λ1n)[−][1]. Then it holds that_

_n_ _C_ _n_

_LLOO_ _QF[λ]_ = n[1] _i=1_ _k=1_ ∆[λ]ik 2, _ALOO_ _QF[λ]_ = n[1] _i=1_ 1[]([y]i[−][∆][λ]i•[)]∗=yi∗ (2)
   X X       X

_where the residual ∆[λ]ik_ _[∈]_ [R][ for][ i][ = 1][, . . ., n][,][ k][ = 1][, . . ., C][ is given by][ ∆]ik[λ] [=][ Y][ik]1[−]−f[ˆ]Ak[λ]ii[(][x][i][)] _._


For the remainder of this text, we will use the shortcut L[λ]LOO [:=][ L][LOO] _Q[λ]F_ . While the expression
for L[λ]LOO [for][ C][ = 1][ has been known in the statistics community for decades (]  [Stone][,][ 1974][) and more]
recently for general C (Tacchetti et al., 2012; Pahikkala & Airola, 2016), the formula for A[λ]LOO [is]
to the best of our knowledge a novel result. We provide a proof of Theorem 3.2 in the Appendix
A.1. Notice that Theorem 3.2 not only allows for an efficient computation of L[λ]LOO [and][ A]LOO[λ] [but]
also provides us with a simple formula to assess the generalization behaviour of the model. It turns
out that L[λ]LOO [captures a great variety of generalization phenomena, typically encountered in deep]
learning. Leveraging the neural tangent kernel together with Theorem 3.2 allows us to investigate
their origin.

As a first step, we consider the zero regularization limit λ −→ 0, i.e. L[0]LOO [since this is the standard]
setting considered in deep learning. A quick look at the formula reveals however that care has to be
taken; as soon as the kernel is invertible, we have a [0]0 [situation.]


-----

**Corollary 3.3. Consider the eigendecomposition K = V diag(ω)V** _[T]_ _for V ∈_ _O(n) and ω ∈_ R[n].
_Denote its rank by r = rank(K). Then it holds that the residuals ∆[λ]ik_

_[∈]_ [R][ can be expressed as]

_n_ _n_ _λ_

_j=r+1_ _[V][ij][V][lj][ +][ P]j[r]=1_ _λ+ωj_ _[V][ij][V][lj]_

∆[λ]ik[(][r][) =] _Ylk_ _n_ _λ_

_l=1_ P _j=r+1_ _[V][ 2]ij_ [+][ P]j[r]=1 _λ+ωj_ _[V][ 2]ij_

X

_Moreover in the zero regularization limit, i.e.P λ −→_ 0, it holds that

_n_ _n_

_j=r+1_ _[V][ij][V][lj]_

_Ylk_ _n_ _if r < n_

 _l=1_ P _j=r+1_ _[V][ 2]ij_

∆[λ]ik[(][r][)][ −]→ ∆ik(r) = Xn _Ylk_ Pnj=1n _ω1j_ 1[V][ij][V][lj] _if r = n_

_l=1_ P _j=1_ _ωj_ _[V][ 2]ij_

X P

**Remark.** At first glance for λ = 0, a distinct phase transition appears to take place when moving
interpolation, i.e.from r = n − 1 ∆ toik r( =n) nωn. Notice however that a small eigenvalue−→0 ∆ik(n 1) for i = 1, . . ., n, k = 1, . . ., C ωn will allow for a smooth.
_−−−−→_ _−_

A similar result for the special case of linear regression and r = n has been derived in Hastie et al.
(2020). We now explore how well these formulas describe the complex behaviour encountered in
deep models in the kernel regime.

4 LEAVE-ONE-OUT AS A GENERALIZATION PROXY

In this section, we study the derived formulas in the context of deep learning, through the lens of
the neural tangent kernel and random feature models. Through empirical studies, we investigate
LOO for varying sample sizes, different amount of noise in the targets as well as different regimes
of overparametrization, in the case of infinitely-wide networks. We evaluate the models on the
benchmark vision datasets MNIST (LeCun & Cortes, 2010) and CIFAR10 (Krizhevsky & Hinton,
2009). To estimate the true generalization error, we employ the test sets as detailed in equation 1.
We provide theoretical insights into double descent, establishing the explosion in loss around the
interpolation threshold. Finally, we demonstrate the utility of LOO for state-of-the-art networks by
predicting their transferability to new tasks. All experiments are performed in Jax (Bradbury et al.,
2018) using the neural-tangents library (Novak et al., 2020).

4.1 FUNCTION OF SAMPLE SIZE

While known to exhibit only a small amount of bias, the LOO error may have a high variance
for small sample sizes (Varoquaux, 2018). Motivated by the consistency results obtained for ridge
regression in Patil et al. (2021), we study how the LOO error (and its variance) of an infinitely-wide
network behaves as a function of the sample size n. In the following we thus interpret LOO as a
function of n. In order to quantify how well LOO captures the generalization error for different
sample regimes, we evaluate a 5-layer fully-connected NTK model for varying sample sizes on
_MNIST and CIFAR10. We display the resulting test and LOO losses in Figure 1. We plot the_
average result of 5 runs along with confidence intervals. We observe that both LLOO and ALOO
closely follow their corresponding test quantity, even for very small sample sizes of around n = 500.
As we increase the sample size, the variance of the estimator decreases and the quality of both LOO
loss and accuracy improves even further, offering a very precise estimate at n = 20000.

4.2 RANDOM LABELS

Next we study the behaviour of the generalization error when a portion of the labels is randomized.
In Zhang et al. (2017), the authors investigated how the performance of deep neural networks is
affected when the functional relationship between inputs x and targets y is destroyed by replacing
**_y by a random label. More specifically, given a dataset X ∈_** R[n][×][d] and one-hot targets y = ek for
_k ∈{1, . . ., C}, we fix a noise level p ∈_ [0, 1] and replace a subset of size pn of the targets with


-----

(a) LLOO, MNIST (b) ALOO, MNIST (c) LLOO, CIFAR10 (d) ALOO, CIFAR10

Figure 1: Test and LOO losses (a,c) and accuracies (b, d) as a function of sample size n. We use a
5-layer fully-connected NTK model on MNIST and CIFAR10.

random ones, i.e. y = ek˜ [for][ ˜]k ∼U({1, . . ., C}). We then train the model using the noisy targets ˜y
while measuring the generalization error with respect to the clean target distribution. As observed in
Zhang et al. (2017); Rolnick et al. (2018), neural networks manage to achieve very small empirical
error even for p = 1 whereas their generalization consistently worsens with increasing p. Random
labels has become a very popular experiment to assess the quality of generalization bounds (Arora
et al., 2019b; 2018; Bartlett et al., 2017). We now show how the leave-one-out error exhibits the
same behaviour as the test error under label noise, thus serving as an ideal test bed to analyze this
phenomenon. In the following, we consider the leave-one-out error as a function of the labels, i.e.
_LLOO = LLOO(y). The phenomenon of noisy labels is of particular interest when the model has_
enough capacity to achieve zero empirical error. For kernels, this is equivalent to rank(K) = n and
we will thus assume in this section that the kernel matrix has full rank. However, a direct application
of Theorem 3.2 is not appropriate, since randomizing some of the training labels also randomizes
the “implicit” test set. In other words, our aim is not to evaluate _f[ˆ][−]˜_ _[i]_ against ˜yi since ˜yi might be
_S_

one of the randomized labels. Instead, we want to compare _f[ˆ][−]˜_ _[i]_ with the true label yi, in order to
_S_
preserve a clean target distribution. To fix this issue, we derive a formula for the leave-one-out error
for a model trained on labels **_Y[˜] but evaluated on Y :_**


**Proposition 4.1. Consider a kernel with spectral decomposition K = V diag(ω)V** _[T]_ _for V ∈_
R[n][×][n] _orthogonal and ω ∈_ R[n]. Assume that rank(K) = n. Then it holds that the leave-one-out
_error LLOO( Y[˜] ; Y ) for a model trained on_ _S[˜] = {(xi, ˜yi)}i[n]=1_ _[but evaluated on][ S][ =][ {][(][x][i][,][ y][i][)][}]i[n]=1_
_is given by_

_n_ _K_ _n_

2

_LLOO( Y[˜] ; Y ) = n[1]_ _i=1_ _k=1_ ∆˜ _ik + Yik −_ _Y[˜]ik_ _,_ _ALOO( Y[˜] ; Y ) = n[1]_ _i=1_ 1[](y[˜]i−∆[˜] _i•)∗=yi∗_

X X   X

_where_ ∆[˜] _ik = ∆ik( Y[˜] ) ∈_ R is defined as in Corollary 3.3.

We defer the proof to the Appendix A.4. Attentive readers will notice that we indeed recover Theorem 3.2 when **_Y[˜] = Y . Using this generalization of LOO, we can study the setup in Zhang et al._**

(2017) to test whether LOO indeed correctly reflects the behaviour of the generalization error. To
this end, we perform a random label experiment on MNIST and CIFAR10 with n = 20000 for a NTK
model of depth 5, where we track test and LOO losses while increasingly perturbing the targets with
noise. We display the results in Figure 2. We see an almost perfect match between test and LOO
loss as well test and LOO accuracy, highlighting how precisely LOO reflects the true error.

4.3 DOUBLE DESCENT

Originally introduced by Belkin et al. (2019), the double descent curve describes the peculiar shape
of the generalization error as a function of the model complexity. The error first follows the classical
_U_ -shape, where an increase in complexity is beneficial until a specific threshold. Then, increasing the model complexity induces overfitting, which leads to worse performance and eventually a
strong spike around the interpolation threshold. However, as found in Belkin et al. (2019), further
increasing the complexity again reduces the error, often leading to the overall best performance. A
large body of works provide insights into the double descent curve but most of them rely on very


-----

(a) LLOO, MNIST (b) ALOO, MNIST (c) LLOO, CIFAR10 (d) ALOO, CIFAR10

Figure 2: Test and LOO losses (a, c) and accuracies (b, d) as a function of noise. We use a 5-layer
fully-connected NTK model on MNIST and CIFAR10 with n = 20000.

restrictive assumptions such as Gaussian input distributions (Harzli et al., 2021; Adlam & Pennington, 2020; d’Ascoli et al., 2020) or teacher models (Bartlett et al., 2020; Harzli et al., 2021; Adlam
& Pennington, 2020; Mei & Montanari, 2021; Hastie et al., 2020). Here we show under arguably
weaker assumptions that the LOO loss also exhibits a spike at the interpolation threshold.

In order to reason about the effect of complexity, we have to consider models with a finite dimensional feature map φm : R[d] _−→_ R[m]. Increasing the dimensionality of the feature space R[m]
naturally overparametrizes the model and induces a dynamic in the rank r(m) := rank(K) =
rank (φm(X)). We restrict our attention to models that admit an interpolation point, i.e. ∃m[∗] _∈_ N
such that r(m[∗]) = n. While the developed theory holds for any kernel, we focus the empirical
evaluations largely to the random feature model due to its aforementioned connection to neural networks. In the following, we will interpret the leave-one-out-error as a function of m and n, i.e.
_LLOO = L[n]LOO[(][m][)][. Notice that][ L][n]LOO[(][m][)][ measures the generalization error of a model trained on]_
_n_ 1 points, instead of n. The underlying interpolation point thus shifts, i.e. we denote m[∗] N
_−_ _n_ _∈_
such that r(m[∗]) = n − 1. In the following we will show that L[n]LOO[(][m][∗][(][n][))] _−−−→−−∞→∞._

**Intuition.** We give a high level explanation for our theoretical insights in this paragraph. For
_m < m[∗]_ the dominating term in L[n]LOO[(][m][)][ is]


_g(r, λ) =_ _n_ _λ_

_i=1_ _l=r+1_ _[V][ 2]il_ [+][ P]l[r]=1 _λ+ωl_ _[V][ 2]il_

X

We observe a blow-up for small λ due to the unit vector nature of bothP **_Vi_** and V _j. For instance,_
considering the interpolation point where r = n − 1 and λ = 0, we get g(•n − 1, 0) =• _i=1_ _V1in[2]_ [,]

the sum over the elements of V•n, which likely has a small entry due to its unit norm nature. On
the other hand, reducing r or increasing λ has a dampening effect as we combine multiple positive[P][n]
terms together, increasing the likelihood to move away from the singularity. A similar dampening
behaviour in L[n]LOO[(][m][)][ can also be observed for][ m > m][∗] [when][ ω][n][ >>][ 0][. In order to formalize this]
argument, we need the following assumption:
**Assumption 4.2. For K = V diag (ω) V** _[T]_ _we impose that the orthogonal matrix V follows_

**_V_** (On)
_∼U_

_where_ (On) is the Haar measure on the orthogonal group On.
_U_


Under Assumption 4.2, we can prove that at the interpolation point, the leave-one-out error exhibits
a spike, which worsens as we increase the amount of data:

**Theorem 4.3. For large enough n ∈** N and λ −→ 0, it holds that

_L[n]LOO[(][m][∗][)][ ⪆]_ [2][nA]

_where A ∼_ Γ( [1]2 _[,][ 1)][ is independent of][ n][.][ L]LOO[n]_ [(][m][∗][)][ hence diverges a.s. with][ n][ −]→∞.


It is surprising that Assumption 4.2 suffices to prove Theorem 4.3, highlighting the universal nature
of double descent. We provide empirical evidence for the double descent behaviour of LOO in
Figure 3. We fit a 1-layer random feature model of varying width to the binary (labeled) MNIST
dataset with a sample size of n = 5000. We observe that the LOO error indeed closely follows the


-----

(a) Loss (b) Accuracy

Figure 3: (a) Train, Test and LOO losses (a) and accuracies (b) as a function of width. We use a
random feature model on binary MNIST with n = 5000.

test error, exhibiting the spike as predicted by Theorem 4.3. The location of the spike indeed exactly
matches the interpolation threshold which is situated around m[∗] _≈_ _n._

4.4 TRANSFER LEARNING

Finally, we study more realistic networks often employed for large-scale computer vision tasks.
We study the task of transfer learning (Yosinski et al., 2014) by considering classifiers trained on
_ImageNet (Krizhevsky et al., 2012) and fine-tuning their top-layer to the CIFAR10 dataset. Notice_
that this corresponds to training a kernel with a data-dependent feature map φdata induced by the nonoutput layers. Our experiments on ResNet18 (He et al., 2016), AlexNet (Krizhevsky et al., 2012),
_VGG (Simonyan & Zisserman, 2015) and DenseNet (Huang et al., 2017) demonstrate that also for_
transferring between tasks we have a very precise estimation of the test accuracy. We display the
results in Table 1. We consider a small data regime where n = 10000, as often encountered in
practice for transfer learning tasks. In order to highlight the role of pre-training, we also evaluate
random feature maps φrand where we use standard initialization for all the parameters in the network.
Indeed we clearly observe the benefits of pre-training, leading to a high increase in performance
for all considered models. For both settings, pre-trained and randomly initialized, we observe an
excellent agreement between test and leave-one-out accuracies across all architectures.

MODEL _ATEST(φDATA)_ _ALOO(φDATA)_ _ATEST(φRAND)_ _ALOO(φRAND)_

RESNET18 67.2 ± 0.1 67.5 ± 0.4 37.7 ± 0.2 37.1 ± 0.1
ALEXNET 57.6 ± 0.2 58.2 ± 0.2 24.2 ± 0.2 23.7 ± 0.2
VGG16 56.6 ± 0.2 56.8 ± 0.5 29.3 ± 0.5 29.2 ± 0.8
DENSENET161 72.5 ± 0.2 71.3 ± 0.3 51.7 ± 0.3 49.9 ± 0.3

Table 1: Test and LOO accuracies for models pre-trained on ImageNet and transferred to CIFAR10
by re-training the top layer. We use 5 runs, each with a different training set of size n = 10000. We
compare the pre-trained networks with random networks to show the benefits of transfer learning.

5 CONCLUSION

We investigated the concept of leave-one-out error as a measure for generalization in the context of
deep learning. Through the use of NTK, we derive new expressions for the LOO loss and accuracy
of deep models in the kernel regime and observe that they capture the generalization behaviour
for a variety of settings. Moreover, we mathematically prove that LOO exhibits a spike at the
interpolation threshold, needing only a minimal set of theoretical tools. Finally, we demonstrated
that LOO accurately predicts the performance of deep models in the setting of transfer learning.

Notably, the simple form of LOO might open the door to new types of theoretical analyses to better
understand generalization, allowing for a disentanglement of the roles of various factors at play such
as overparametrization, noise and architectural design.


-----

REFERENCES

Ben Adlam and Jeffrey Pennington. Understanding double descent requires a fine-grained biasvariance decomposition. 34th Conference on Neural Information Processing Systems (NeurIPS
_2020), 2020._

Ahmed M. Alaa and Mihaela van der Schaar. Discriminative jackknife: Quantifying uncertainty in
deep learning via higher-order influence functions. Proceedings of the 37th International Confer_ence on Machine Learning (ICML), 2020._

Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. Proceedings of the 35th International Conference on
_Machine Learning (ICML), 2018._

Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. 33rd Conference on Neural Information
_Processing Systems (NeurIPS), 2019a._

Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. Proceedings
_of the 36th International Conference on Machine Learning (ICML), 2019b._

Gregor Bachmann, Seyed-Mohsen Moosavi-Dezfooli, and Thomas Hofmann. Uniform convergence, adversarial spheres and a simple remedy. Proceedings of the 38th International Conference
_on Machine Learning (ICML), 2021._

Peter Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural
networks. 31st Conference on Neural Information Processing Systems (Neurips), 2017.

Peter L. Bartlett, Nick Harvey, Chris Liaw, and Abbas Mehrabian. Nearly-tight vc-dimension and
pseudodimension bounds for piecewise linear neural networks. Journal of Machine Learning
_Research 20, pp. 1–17, 2019._

Peter L. Bartlett, Philip M. Long, G´abor Lugosi, and Alexander Tsigler. Benign overfitting in linear
regression. Proceedings of the National Academy of Sciences, 117(48):30063–30070, 2020. ISSN
0027-8424.

Mikhail Belkin, Daniel J. Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machinelearning practice and the classical bias–variance trade-off. Proceedings of the National Academy
_of Sciences, 116:15849 – 15854, 2019._

Yoshua Bengio and Yves Grandvalet. No unbiased estimator of the variance of k-fold crossvalidation. J. Mach. Learn. Res., 5:1089–1105, December 2004. ISSN 1532-4435.

Olivier Bousquet and Andr´e Elisseeff. Stability and generalization. The Journal of Machine Learn_ing Research, 2:499–526, 2002._

K. O. Bowman, L. R. Shenton, and Paul C. Gailey. Distribution of the ratio of gamma variates.
_Communications in Statistics - Simulation and Computation, 27(1):1–19, 1998._

James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao
[Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http:](http://github.com/google/jax)
[//github.com/google/jax.](http://github.com/google/jax)

Leo Breiman. Heuristics of instability and stabilization in model selection. The Annals of Statistics,
24(6):2350 – 2383, 1996.

Gavin C. Cawley and Nicola L.C. Talbot. Efficient leave-one-out cross-validation of kernel fisher
discriminant classifiers. Pattern Recognition, 36(11):2585–2592, 2003. ISSN 0031-3203.

Shuxiao Chen, Hangfeng He, and Weijie J. Su. Label-aware neural tangent kernel: Toward better
generalization and local elasticity. 34th Conference on Neural Information Processing Systems
_(NeurIPS), 2020._


-----

St´ephane d’Ascoli, Maria Refinetti, Giulio Biroli, and Florent Krzakala. Double trouble in double
descent : Bias and variance(s) in the lazy regime. Proceedings of the 37th International Confer_ence on Machine Learning (ICML), 2020._

Alexander G. de G. Matthews, Mark Rowland, Jiri Hron, Richard E. Turner, and Zoubin Ghahramani. Gaussian process behaviour in wide deep neural networks. International Conference on
_Learning Representations (ICLR), 2018._

Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. Proceedings of NAACL-HLT, 2019.

Luc Devroye and Terry Wagner. Distribution-free inequalities for the deleted and holdout error
estimates. IEEE Transactions on Information Theory, 25(2):202–207, 1979.

Simon S. Du, Kangcheng Hou, Barnab´as P´oczos, Ruslan Salakhutdinov, Ruosong Wang, and Keyulu
Xu. Graph neural tangent kernel: Fusing graph neural networks with graph kernels. 34rd Confer_ence on Neural Information Processing Systems (NeurIPS), 2019._

Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. Proceedings of
_the Thirty-Third Conference on Uncertainty in Artificial Intelligence, 2017._

Gintare Karolina Dziugaite and Daniel M Roy. Data-dependent pac-bayes priors via differential
privacy. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett
(eds.), 32nd Conference on Neural Information Processing Systems (NeurIPS 2018), volume 31.
Curran Associates, Inc., 2018.

Andr´e Elisseeff, Massimiliano Pontil, et al. Leave-one-out error and stability of learning algorithms
with applications. NATO science series sub series iii computer and systems sciences, 190:111–
130, 2003.

K. Fukunaga and D.M. Hummels. Leave-one-out procedures for nonparametric error estimates.
_IEEE Transactions on Pattern Analysis and Machine Intelligence, 11(4):421–423, 1989._

Ouns El Harzli, Guillermo Valle-P´erez, and Ard A. Louis. Double-descent curves in neural networks: a new perspective using gaussian processes, 2021.

Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J. Tibshirani. Surprises in highdimensional ridgeless least squares interpolation, 2020.

Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.

Thomas Hofmann, Bernhard Sch¨olkopf, and Alexander J. Smola. Kernel methods in machine learning. The Annals of Statistics, 36(3), Jun 2008. ISSN 0090-5364.

Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak. Infinite attention: Nngp and
ntk for deep attention networks. Proceedings of the 37th International Conference on Machine
_Learning (ICML), 2020._

Gao Huang, Zhuang Liu, and Kilian Q. Weinberger. Densely connected convolutional networks.
_2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2261–2269,_
2017.

Kaixuan Huang, Yuqing Wang, Molei Tao, and Tuo Zhao. Why do deep residual networks generalize
better than deep feedforward networks? – a neural tangent kernel perspective. 34rd Conference
_on Neural Information Processing Systems (NeurIPS), 2020._

Like Hui and Mikhail Belkin. Evaluation of neural architectures trained with square loss vs crossentropy in classification tasks, 2021.

Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. 32rd Conference on Neural Information Processing Systems
_(NeurIPS), 2018._


-----

Arthur Jacot, Berfin S¸ims¸ek, Francesco Spadaro, Cl´ement Hongler, and Franck Gabriel. Implicit
regularization of random feature models. Proceedings of the International Conference on Ma_chine Learning (ICML), 2020._

Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic
generalization measures and where to find them. International Conference on Learning Repre_sentations (ICLR), 2019._

Michael Kearns and Dana Ron. Algorithmic stability and sanity-check bounds for leave-one-out
cross-validation. Neural Computation, 11(6):1427–1453, 1999.

Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. Proceedings of the 5th International Conference on Learning Representations, 2017.

Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical Report 0, University of Toronto, Toronto, Ontario, 2009.

Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger (eds.),
_Advances in Neural Information Processing Systems, volume 25. Curran Associates, Inc., 2012._

PA Lachenbruch. An almost unbiased method of obtaining confidence intervals for the probability
of misclassification in discriminant analysis. Biometrics, 1967.

Yann LeCun and Corinna Cortes. MNIST handwritten digit database, 2010.

Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep neural networks as gaussian processes. International Conference on Learn_ing Representations (ICLR), 2018._

Jaehoon Lee, Lechao Xiao, Samuel S. Schoenholz, Yasaman Bahri, Roman Novak, Jascha SohlDickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. 33rd Conference on Neural Information Processing Systems (NeurIPS),
2019.

Song Mei and Andrea Montanari. The generalization error of random features regression: Precise
asymptotics and the double descent curve. Communications on Pure and Applied Mathematics,
06 2021. doi: 10.1002/cpa.22008.

F. Mosteller and J. Tukey. Data analysis, including statistics. Handbook of Social Psychology, 2,
1968.

Vaishnavh Nagarajan and J. Zico Kolter. Uniform convergence may be unable to explain generalization in deep learning. 33rd Conference on Neural Information Processing Systems (NeurIPS),
2019.

Radford M. Neal. Priors for Infinite Networks, pp. 29–53. Springer New York, New York, NY,
1996. ISBN 978-1-4612-0745-0. doi: 10.1007/978-1-4612-0745-0 2.

Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. Proceedings of The 28th Conference on Learning Theory (PMLR), 2015.

Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to
spectrally-normalized margin bounds for neural networks. International Conference on Learning
_Representations (ICLR), 2018._

Roman Novak, Lechao Xiao, Jiri Hron, Jaehoon Lee, Alexander A. Alemi, Jascha Sohl-Dickstein,
and Samuel S. Schoenholz. Neural tangents: Fast and easy infinite neural networks in python. In
_[International Conference on Learning Representations, 2020. URL https://github.com/](https://github.com/google/neural-tangents)_
[google/neural-tangents.](https://github.com/google/neural-tangents)

Tapio Pahikkala and Antti Airola. Rlscore: Regularized least-squares learners. Journal of Machine
_[Learning Research, 17(220):1–5, 2016. URL http://jmlr.org/papers/v17/16-470.](http://jmlr.org/papers/v17/16-470.html)_
[html.](http://jmlr.org/papers/v17/16-470.html)


-----

Pratik Patil, Yuting Wei, Alessandro Rinaldo, and Ryan Tibshirani. Uniform consistency of crossvalidation estimators for high-dimensional ridge regression. In Arindam Banerjee and Kenji
Fukumizu (eds.), Proceedings of The 24th International Conference on Artificial Intelligence and
_Statistics, volume 130 of Proceedings of Machine Learning Research, pp. 3178–3186. PMLR,_
13–15 Apr 2021.

Vern I. Paulsen and Mrinal Raghupathi. An Introduction to the Theory of Reproducing Kernel Hilbert
_Spaces, pp. i–iv. Cambridge Studies in Advanced Mathematics. Cambridge University Press,_
2016.

M. Pontil. Leave-one-out error and stability of learning algorithms with applications. International
_Journal of Systems Science, 2002._

Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In J. Platt,
D. Koller, Y. Singer, and S. Roweis (eds.), Advances in Neural Information Processing Systems,
volume 20. Curran Associates, Inc., 2008.

David Rolnick, Andreas Veit, Serge Belongie, and Nir Shavit. Deep learning is robust to massive
label noise, 2018.

Sarath Shekkizhar and Antonio Ortega. Deepnnk: Explaining deep models and their generalization
using polytope interpolation, 2020.

Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. International Conference on Learning Representations (ICLR), 2015.

M. Stone. Cross-validatory choice and assessment of statistical predictions. Journal of the Royal
_Statistical Society, 36(2):111–147, 1974._

Andrea Tacchetti, Pavan Mallapragada, Matteo Santoro, and Lorenzo Rosasco. Gurls: a toolbox for
regularized least squares learning. 02 2012.

Ga¨el Varoquaux. Cross-validation failure: Small sample sizes lead to large error bars. NeuroImage,
180:68–77, 2018. ISSN 1053-8119. New advances in encoding and decoding of brain signals.

Aki Vehtari, Andrew Gelman, and Jonah Gabry. Practical bayesian model evaluation using leaveone-out cross-validation and waic. Statistics and Computing, 27(5):1413–1432, Aug 2016. ISSN
1573-1375.

Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Sci_ence. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University Press,_
2018.

Ch. Walck. Hand-book on statistical distributions for experimentalists. 1996.

J. Weston. Leave-one-out support vector machines. In IJCAI, 1999.

Greg Yang. Tensor programs ii: Neural tangent kernel for any architecture, 2020.

Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep
neural networks? Advances in Neural Information Processing Systems, 2014.

Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. International Conference on Learning Repre_sentations (ICLR), 2017._

Rui Zhang and Shihua Zhang. Rethinking influence functions of neural networks in the overparameterized regime, 2021.

Tong Zhang. Leave-one-out bounds for kernel methods. Neural Computation, 15(6):1397–1437,
2003.

Yongli Zhang and Yuhong Yang. Cross-validation for selecting a model selection procedure. Journal
_of Econometrics, 187(1):95–112, 2015. ISSN 0304-4076._


-----

A OMITTED PROOFS

We list all the omitted proofs in the following section.

A.1 PROOF OF THEOREM 3.2

**Theorem 3.2. Consider a kernel K : R[d]** _×R[d]_ _−→_ R and the associated objective with regularization
_parameter λ > 0 under mean squared loss. Define A = K (K + λ1n)[−][1]. Then it holds that_

_n_ _K_ _n_

_LLOO_ _Qλ[ERM]_ = n[1] _i=1_ _k=1_ ∆[λ]ik 2, _ALOO_ _Qλ[ERM]_ = n[1] _i=1_ 1[](yi−∆i•)[∗]=yi[∗]
   X X       X

_where the residuals ∆[λ]ik_ _[∈]_ [R][ for][ i][ = 1][, . . ., n][,][ k][ = 1][, . . ., K][ is given by][ ∆]ik[λ] [=][ Y][ik]1[−]−f[ˆ]Ak[λ]ii[(][x][i][)] _._


_Proof. Recall that_ _f[ˆ][λ]_ [solves the optimization problem]
_S_


(fk(xi) _Yik)[2]_ + λ _f_
_−_ _||_ _||[2]_
_k=1_

X


_fˆ[λ]_ [= argmin]f _[L][λ]_ _f_
_S_ _∈F_ _S_ [(][f] [) := argmin] _∈F_


_i=1_


and predicting on the training data takes the form _f[ˆ]_ (X) = AY for some A R[n][×][n]. Now
_S_ _∈_
consider the model fλ[−][i] := f[ˆ]S[λ]−i [obtained from training on][ S][−][i][. W.L.O.G. assume that][ i][ =][ n][. We]

want to understand the quantity _f[ˆ]λ,k[−][n][(][x][n][)][, i.e. the][ k][-th component of the prediction on][ x][i][. To that]_

end, consider the dataset Z := S−n ∪ **_xn,_** _f[ˆ]λ[−][n](xn)_ . Notice that for any f ∈F, it holds
n   o

_C_ _[n][−][1]_ 2 2

_L[λ]Z_ [( ˆ]fλ[−][n]) = _fˆλ,k[−][n][(][x][i][)][ −]_ _[Y][ik]_ + _fˆλ,k[−][n][(][x][n][)][ −]_ _f[ˆ]λ,k[−][n][(][x][n][)]_ + λ||f[ˆ]λ[−][n]||H[2]

_k=1_ _i=1_

X n X     o

_C_ _n_ 1

_−_ 2

= _fˆλ,k[−][n][(][x][i][)][ −]_ _[Y][ik]_ + λ||f[ˆ]λ[−][n]||H[2]

_k=1_ _i=1_

X X  


= L[λ]S−n _fˆλ[−][n]_


_L[λ]_ _n_ [(][f] [)]
_≤_ _S−_

_L[λ]_ _n_ [(][f] [) +]
_≤_ _S−_


2
_fk(xn) −_ _f[ˆ]λ,k[−][n][(][x][n][)]_
 


_k=1_


= L[λ]
_Z_ [(][f] [)]

where the first inequality follows because _f[ˆ][−][n]_ minimizes L[λ]S−n [by definition. Thus][ ˆ]fλ[−][n] also minimizes L[λ] [and hence also takes the form]
_Z_


_fˆλ[−][n](X) = AY[˜]_

_Yik_ if i = n
_̸_ . Now we care about the n-th prediction
(fˆλ,k[−][n][(][x][n][)] else


where **_Y[˜] ∈_** R[n][×][C] such that _Y[˜]ik =_

which is


_n−1_

_AnjYjk + Annf[ˆ]λ,k[−][n][(][x][n][)]_
_j=1_

X


_fˆλ,k[−][n][(][x][n][) =]_


_AnjY[˜]jk =_
_j=1_

X


= _j=1_ _AnjYjk −_ _AnnYnk + Annf[ˆ]λ,k[−][n][(][x][n][)]_

X

= f[ˆ]S[λ],k[(][x][n][)][ −] _[A][nn][Y][nk]_ [+][ A][nn]f[ˆ]λ,k[−][n][(][x][n][)]


-----

Solving for _f[ˆ]λ,k[−][n][(][x][n][)][ gives]_

Then, subtracting Ynk leads to


_fˆλ,k[−][n][(][x][n][) =]_ _fˆS[λ],k[(][x]1[n][)][ −]Ann[A][nn][Y][nk]_ (3)

_−_


_fˆ[λ],k[(][x][n][)][ −]_ _[A][nn][Y][nk]_ _Ynk_ _YnkAnn_ _f_ _[λ],k[(][x][n][) +][ A][nn][Y][nk]_ _Ynk_ _f_ _[λ],k[(][x][n][)]_
_Ynk −_ _f[ˆ]λ,k[−][n][(][x][n][) =][ Y][nk][ −]_ _S_ 1 _Ann_ = _−_ 1 − [ˆ]AS _nn_ = _−1_ [ˆ]AS _nn_

_−_ _−_ _−_

Squaring and summing the expression over n and k results in the formula.

For the accuracy, we know that we correctly predict if the maximal coordinate of _f[ˆ]λ[−][n](xn) agrees_
with the maximal coordinate of yn, i.e.

argmaxk _f[ˆ]λ,k[−][n][(][x][n][) = argmax]k_ _[Y][nk]_

From equation 3, we notice that


argmaxk _f[ˆ]λ,k[−][n][(][x][n][) = argmax]k_ _fˆS[λ],k[(][x]1[n][)][ −]Ann[A][nn][Y][nk]_ = argmaxk _fˆS[λ],k[(][x][n][)][ −]_ _[Y]1[nk][ +]A[ Y]nn[nk][ −]_ _[A][nn][Y][nk]_

_−_ _−_

= argmaxk ∆[λ]nk [+][ Y][nk]
_−_
= (yn − **∆n•)[∗]**

We thus have to check the indicator 1{(yn−∆n•)[∗]=yn∗ _[}][ and sum it over][ n][ to obtain the result.]_

A.2 BINARY CLASSIFICATION

Here we state the corresponding results in the case of binary classification. The formulation for the
accuracy changes slightly as now the sign of the classifier serves as the prediction.
**Proposition A.1. Consider a kernel K : R[d]** _× R[d]_ _−→_ R and the associated objective with regular_ization parameter λ > 0 under mean squared loss. Define A = K (K + λ1n)[−][1]. Then it holds_
_that_

_n_ _n_

_LLOO_ _Qλ[ERM]_ = n[1] ∆[λ]i 2, _ALOO_ _Qλ[ERM]_ = n[1] 1{yi∆λi _[<][1][}]_

_i=1_ _i=1_

   X       X

_where the residuals ∆[λ]i_ _[∈]_ [R][ for][ i][ = 1][, . . ., n][ is given by][ ∆]i[λ] [=][ y][i][−]1−f[ˆ]A[λ](iixi) _._

_Proof. Notice that the result for LLOO is analogous to the proof for Theorem 3.2 by setting K = 1._
For binary classification we use the sign of the classifier as a decision rule, i.e. the classifier predicts
correctly if yf[ˆ][λ](x) > 0. We can thus calculate that


_f_ _[λ]_ _n_ _f_ _[λ]_ _n_ [+][ y]n[2] _n[A][nn]_
_ynf[ˆ]λ[−][n](xn) =_ _[y][n][ ˆ]S_ [(]1[x][n][)]A[ −]nn[A][nn][y][2] = _[y][n][ ˆ]S_ [(][x][n][)][ −]1 _[y][2]Ann_ _[−]_ _[y][2]_

_−_ _−_

_yn_ _f_ _[λ]_
= yn[2] _−_ [ˆ]S [(][x][n][)]

_[−]_ _[y][n]_ 1 − _Ann_

_yn_ _f_ _[λ]_
= 1 _yn_ _−_ [ˆ]S [(][x][n][)]
_−_ 1 _Ann_

_−_
= 1 _yn∆[λ]n_
_−_
Thus, the n-th sample is correctly classified if and only if

1 − _yn∆[λ]n_ _[>][ 0][ ⇐]⇒_ _yn∆[λ]n_ _[<][ 1]_

We now just count the correct predictions for the accuracy, i.e.


_ALOO_ _Qλ[ERM]_ = n[1]
  


1{yi∆λi _[<][1][}]_
_i=1_

X


-----

A.3 PROOF OF COROLLARY 3.3

**Corollary 3.3. Consider the eigendecomposition K = V diag(ω)V** _[T]_ _for V ∈_ _O(n) and ω ∈_ R[n].
_Denote its rank by r = rank(K). Then it holds that the residuals ∆[λ]ik_

_n_ _n_ _λ_ _[∈]_ [R][ can be expressed as]

_k=r+1_ _[V][ik][V][lk][ +][ P]k[r]_ =1 _λ+ωk_ _[V][ik][V][lk]_

∆[λ]ik[(][r][) =] _Ylk_ _n_ _λ_

_l=1_ P _k=r+1_ _[V][ 2]ik_ [+][ P]k[r] =1 _λ+ωk_ _[V][ 2]ik_

X

_Moreover for zero regularization, i.e. λ_ P0, it holds that
_−→_

_n_ _n_

_j=r+1_ _[V][ij][V][lj]_

_Ylk_ _n_ _if r < n_

 _l=1_ P _j=r+1_ _[V][ 2]ij_

∆[λ]ik[(][r][)][ −]→ ∆ik(r) = Xn _Ylk_ Pnj=1n _ω1j_ 1[V][ij][V][lj] _if r = n_

_l=1_ P _j=1_ _ωj_ _[V][ 2]ij_

X P

_Proof.n_ Define A = K (K + λ1n)[−][1] _∈_ R[n][×][n]. Recall that _f[ˆ]S[λ][(][X][) =][ Ay][ and thus][ ˆ]fk[λ][(][x][i][) =]_
_j=1_ _[A][ij][Y][jk][. Let us first simplify][ A][:]_
P **_A = K (K + λ1n)[−][1]_** = V diag (ω) V _[T][  ]V diag (ω) V_ _[T]_ + λ1n _−1_

**_ω_**
= V diag **_V_** _[T]_ 

**_ω + λ_**

 


We can characterize the off-diagonal elements for i ̸= j as follows:

_n_ _r_ _r_

_ωi_ _ωi_

_Aij =_ _Vik_ _VikVjk_ _VikVjk_

_k=1_ _ωi + λ_ _[V][jk][ =]_ _k=1_ _ωi + λ_ [=] _k=1_ _−_

X X X


_ωk + λ_ _[V][ik][V][jk]_


_k=1_


= −


_VikVjk_
_k=r+1_ _−_

X


_ωk + λ_ _[V][ik][V][jk]_


_k=1_


be written aswhere we made use of the fact that Vi• ⊥ **_Vj•. The diagonal elements i = j on the other hand can_**


_n_

_k=1_ _Vik[2]_ _ωiω +i_ _λ_ [=]

X


_r_

_k=1_ _Vik[2]_ _ωkω +k_ _λ_ [=]

X


_Vik[2]_
_k=1_ _[−]_

X


_Vik[2]_
_k=1_

X


_Aii =_


_ωk + λ_


_Vik[2]_ _Vik[2]_
_k=r+1_ _[−]_ _k=1_

X X


= 1 −


_ωk + λ_


where we have made use of the fact that Vi is a unit vector. Plugging-in the quantities into LLOO

_•_
results in

∆[λ]ik [=][ Y][ik][ −] _f[ˆ]k[λ][(][x][i][)]_ = _[Y][ik][ −]_ [P]l[n]=1 _[A][il][Y][lk]_ = _Yik −_ _Aiiyi −_ [P]l[n]≠ _i_ _[A][il][Y][lk]_

1 _Aii_ 1 _Aii_ 1 _Aii_
_−_ _−_ _−_

_Yik_ _nj=r+1_ _[V][ 2]ij_ [+][ P]k[r] =1 _[V][ 2]ij_ _ωjλ+λ_ + _l=i_ _[Y][lk]_ _nj=r+1_ _[V][ij][V][lj][ +][ P]j[r]=1_ _ωjλ+λ_ _[V][ij][V][lj]_

_̸_

= P _nj=r+1_ _[V][ 2]ij_ [+][ P]j[r]=1P[V][ 2]ij _ωlλ+λ_ 

[P][n]

_n_ _n_ _λ_

= _l=1_ _[Y][lk]_ _n_ _j=r+1_ _[V][ij][V][lj][ +][ P]Pj[r]=1_ _ωλj_ +λ _[V][ij][V][lj]_

P Pj=r+1 _[V][ 2]ij_ [+][ P]j[r]=1 _[V][ 2]ij_ _ωk+λ_

Now crucially, in the full rank caseP _r = n, we have empty sums, i.e._ _l=r+1_ [=][ P]l[n]=n+1 [= 0][ and]
we obtain

_n_ _n_ _λ_ _n_ _n_ 1

∆[λ]ik [=] _l=1_ _[Y][lk]n_ _j=1_ _ωjλ+λ_ _[V][ij][V][lj]_ = _l=1_ _[Y][lk]n_ _j=1[P][n]ωj1+λ_ _[V][ij][V][lj]_

P _jP=1_ _[V][ 2]ij_ _ωj_ +λ P _jP=1_ _[V][ 2]ij_ _ωj_ +λ

_n_ _n_ 1

_λ_ 0 P _j=1_ _ωj_ _[V][ij][V][lj]_ P
_−→_ _Ylk_ _n_ 1
_−−−→_ _l=1_ P _j=1_ _ωj_ _[V][ 2]ij_

X

P


-----

On the other hand, in the rank deficient case r < n we can cancel the regularization term:

_n_ _n_

_λ_ 0 _j=r+1_ _[V][ij][V][lj]_
∆[λ]ik _−→_ _Ylk_ _n_
_−−−→_ _l=1_ P _j=r+1_ _[V][ 2]ij_

X

Plugging this into the formulas for L[λ]LOO [and][ A]LOO[λ] [concludes the proof.]P

A.4 PROOF OF PROPOSITION 4.1

**Proposition 4.1. Consider a kernel with spectral decomposition K = V diag(ω)V** _[T]_ _for V ∈_
R[n][×][n] _orthogonal and ω ∈_ R[n]. Assume that rank(K) = n. Then it holds that the leave-one-out
_error LLOO( Y[˜] ; Y ) for a model trained on_ _S[˜] = {(xi, ˜yi)}i[n]=1_ _[but evaluated on][ S][ =][ {][(][x][i][,][ y][i][)][}]i[n]=1_
_is given by_


_n_ _C_

2

_LLOO( Y[˜] ; Y ) = [1]_ ∆˜ _ik + Yik_ _Yik_ _,_ _ALOO( Y[˜] ; Y ) = [1]_

_n_ _i=1_ _k=1_ _−_ [˜] _n_

X X  

_where_ ∆[˜] _ik = ∆ik( Y[˜] ) ∈_ R is defined as in Corollary 3.3.


1[]
_i=1_ (y[˜]i−∆[˜] _i•)∗=yi∗_

X


_Proof. Denote by_ _S[˜]−i the dataset {(xj, ˜yj)}j[n]≠_ _i[. Denote by][ ˜]fλ[−][i]_ the model trained on _S[˜]−i. Recall_
from the proof of Theorem 3.2 that

_f˜λ,k[−][n][(][x][n][) =]_ _f˜k[λ][(][x]1[n][)][ −]A[A]nn[nn][ ˜]Ynk_

_−_

Instead of subtracting the same label _Y[˜]nk, we now subtract the evaluation label Ynk:_


_fk[λ][(][x][n][) +][ A][nn][ ˜]Ynk_ _Ynk) + Y[˜]nk_ _fk[λ][(][x][n][)]_
_Ynk −_ _f[˜]λ,k[−][n][(][x][n][) =][ Y][nk][ −]_ _[A][nn][Y][nk]1[ −]_ _A[˜]_ _nn_ = [(1][ −] _[A][nn][)(][Y][nk][ −]1_ [˜] _Ann_ _−_ [˜]

_−_ _−_

_Y˜nk_ _fk[λ][(][x][n][)]_
= (Ynk _Ynk) +_ _−_ [˜]
_−_ [˜] 1 _Ann_

_−_
= (Ynk _Ynk) + ∆[˜]_ _[λ]ik_
_−_ [˜]

The second term ∆[˜] _[λ]ik_ [is now the summand of the standard leave-one-out error where we evaluate on]
**_Y˜ . We can hence re-use Theorem 3.3 to decompose it. Squaring and summing over n concludes the_**
LOO loss result. For the accuracy, we notice that a similar derivation as for Theorem 3.2 applies:

_f˜k[λ][(][x][n][)][ −]_ _[A][nn][ ˜]Ynk_ _fˆk[λ][(][x][n][)][ −]_ _Y[˜]nk + Y[˜]nk_ _AnnY[˜]nk_
argmaxk _f[˜]λ,k[−][n][(][x][n][) = argmax]k_ 1 _Ann_ = argmaxk 1 _Ann_ _−_

_−_ _−_

= argmaxk −∆[˜] _[λ]nk_ [+ ˜]Ynk

_∗_
= **_y˜n −_** **∆[˜]** _n•_

We thus have to check the indicator against the true label  **_yn, i.e. 1{(y˜n−∆[˜]_** _n•)∗=yn∗_ _[}][ and sum it over]_
_n to obtain the result._


A.5 PROOF OF THEOREM 4.3

**Theorem 4.3. For large enough n ∈** N, we can estimate as

_L[n]LOO[(][m][∗][)][ ⪆]_ [2][nA]


_where A ∼_ Γ( [1]2 _[,][ 1)][ is independent of][ n][.][ L]LOO[n]_ [(][m][∗][)][ hence diverges a.s. with][ n][ −]→∞.

_Proof. First we notice that for m = m[∗], by definition it holds that r = n −_ 1, which simplifies the
LOO expression to

_n_ _n_ 2

_λ_ 0 1
_L[n]LOO[(][m][∗][)]_ _−→_ _yiVin_
_−−−→_ _n[1]_ _i=1_ _Vin[2]_ ! _i=1_ !

X X


-----

For notational convenience, we will introduce v ∈ R[n] such that vi := Vin. We will now bound the
both factors one-by-one. The first part is a simple application of Proposition B.1 and Proposition
B.5:

_n_ 2 _n_ _n_ 2

1 (d)

_L[n]LOO[(][m][∗][) = 1]_ _vi_ _n[2][ 1]_ _vi_ = n[2]B

_n_ _i=1_ ! _i=1_ _vi[2]_ _≥_ _n_ _i=1_ !

X X X

Now for large enough n, we can use Lemma B.6 to make the following approximation in distribution:


_nB_ 2 _[n][ −]_ [1]
_≈_ 2


(d)
_−−→_ 2A


where A ∼ Γ( 2[1] _[,][ 1)][. Thus for large enough][ n][, it holds that]_

_L[n]LOO[(][m][∗][)][ ⪆]_ [2][nA]

As the approximation becomes exact for larger and larger n, we conclude that

_L[n]LOO[(][m][∗][)]_ _−n−−→−−∞→∞_ a.s.

B ADDITIONAL LEMMAS

In this section we present the additional technical Lemmas needed for the proofs of the main claims
in A.

**Lemma B.1. Consider a unit vector v ∈** S[n][−][1]. Then it holds that


1

_n[2]_
_vi[2]_ _≥_


_i=1_


_Proof. Let’s parametrize each vi as_
_zi_
_vi =_ _n_
_i=1_ _[z]i[2]_
for i = 1, . . ., n and z R[n]. One can easily check thatpP **_v_** 2 = 1 and hence v S[n][−][1]. Plugging
_∈_ _||_ _||_ _∈_
this in, we arrive at

_n_ 1 _n_ _nj=1_ _[z]j[2]_ _n_ _n_ _zj[2]_ _n_ _n_ _zj[2]_

= = = n +

_i=1_ _vi[2]_ _i=1_ P _zi[2]_ _i=1_ _j=1_ _zi[2]_ _i=1_ _j=i_ _zi[2]_

X X X X X X̸

We can re-arrange the sum into pairs


_zj[2]_ + _[z]i[2]_ = a[2] + [1]

_zi[2]_ _zj[2]_ _a[2][ ≥]_ [2]

for a[2] = _zzji[2][2]_ _[>][ 0][ and using the fact that][ x][ +][ 1]x_ _[≥]_ [2][ for][ x][ ≥] [0][. We can find][ n][(][n]2[−][1)] such summands,

and thus


1

_n + 2_ _[n][(][n][ −]_ [1)]
_vi[2]_ _≥_ 2


= n[2]


_i=1_


**Lemma B.2. Consider v** S[n][−][1][] _and any fixed orthogonal matrix U_ _O(n). Then it holds_
_∼U_ _∈_
_that_
  (d)
**_Uv_** = v

_Proof. This is a standard result and can for instance be found in Vershynin (2018)._


-----

**Lemma B.3. Consider w ∼N** (0, 1n). Then it holds that

**_w_**
**_v =_** S[n][−][1][]

**_w_** 2 _∼U_
_||_ _||_
 

_Proof. This is a standard result and can for instance be found in Vershynin (2018)._

**Lemma B.4. Consider two independent Gamma variables X** _∼_ Gamma(α, ν) and Y
Gamma(β, ν). Then it holds that

_X_

_X + Y_

_[∼]_ [Beta (][α, β][)]

_Proof. This is a standard result and can for instance be found in Bowman et al. (1998)._

**Lemma B.5. Consider v** S[n][−][1][]. Then it holds that
_∼U_
  _n_ 2

1 1

_yivi_ Beta

_n_ _i=1_ ! _∼_  2 _[, n][ −]2_ [1] 

X

_Proof. First realize that we can write_


_yivi = 1[˜][T]n_ [(][v][ ⊙] **_[y][)]_** (= d) **1[˜][T]n** **_[v]_**
_i=1_

X


_√n_


where **1[˜]n =** _√1n_ _, . . .,_ _√1n_ with **1n** 2 = 1 and the fact that v **_y_** (=d) v for fixed y 1, 1 .
_||[˜]_ _||_ _⊙_ _∈{−_ _}[n]_

The idea is now to choose **_U ∈_** _O(n) such that U_ _[T][ ˜]1n = e1 = (1, 0, . . ., 0). Then by using Lemma_

B.2, it holds

**1˜[T]n** **_[v]_** (= d) **1[˜][T]n** **_[Uv]_** (=d) **_U_** **1[˜]n** _T v_ (=d) e[T]1 **_[v]_** (=d) v1

Thus, surprisingly, it suffices to understand the distribution of   _v1. By Lemma B.3, we know that_

(d) _z1_
_v1_ =

_z1[2]_ [+][ · · ·][ +][ z]n[2]

where z (0, 1n). We are interested in the square of this expression,p
_∼N_

_n_ 2

_n1_ _i=1_ _vi!_ (=d) v1[2] (=d) _z1[2]_ [+][ · · ·]z1[2] [ +][ z]n[2] (=d) _z1[2]z[+]1[2][ w]_

X

where we defineGamma 12 _[,][ 1]2_ and w w = ∼ Gammai=2 _[z]i[2][, clearly independent of] n−2_ 1 _[,][ 1]2_ . Thus, by Lemma[ z] B.41[2][.] we can conclude thatMoreover, it holds that z1[2] _∼_
   [P][n]   _n_  2

1 1

_vi_ Beta

_n_ _i=1_ ! _∼_  2 _[, n][ −]2_ [1] 

X


**Lemma B.6. Consider the sequence of Beta distributions Xn ∼** Beta(k, n). Then it holds that

(d)
_nXn_ Gamma(k, 1)
_−−→_

_Proof. This is a standard result and can for instance be found in Walck (1996)._

C FURTHER EXPERIMENTS

In this section we present additional experimental results on the leave-one-out error.


-----

(a) Depth 1 (b) Depth 2, m1 (c) Depth 2, m2

Figure 4: Kernel rank rank(K) as a function of complexity. For (a) we use a depth 1 random feature
model σ(W x), W ∈ R[m][×][d], where complexity is measured through width m. In (b) and (c) we
use a random feature model of depth 2, σ(V σ(W x)), W ∈ R[m][1][×][d] and V ∈ R[m][2][×][m][1] . For (b) we
visualize the rank as a function of m1, where m2 = 120 fixed and in (c) as a function of m2, where
_m1 = 10 fixed. We use MNIST with n = 100 samples._

C.1 RANK DYNAMICS FOR DIFFERENT KERNELS

In this section, we illustrate how the rank rank(K) of the kernel evolves as a function of the underlying complexity. As observed in various works, the spike in the double descent curve for neural
networks usually occurs around the interpolation threshold, i.e. the point in complexity where the
model is able to achieve zero training loss. Naturally, for the kernel formulation, interpolation is
achieved when the kernel matrix K has full rank. We illustrate in the following how the dynamics
of the rank change for different architectures, displaying a similar behaviour as finite width neural
networks. In Fig. 4 (a) we show a depth 1 random feature model with feature maps σ(W x) where
**_W ∈_** R[m][×][d] and σ is the ReLU non-linearity. We use MNIST with n = 100. We measure the
complexity through the width m of the model. We observe that in this special case, the critical complexity exactly coincides with the number of samples, i.e. rank(K) = n as soon as m = n. This
seems counter-intuitive at first as an exact match of the complexity with the sample size is usually
not observed for standard neural networks. We show however in Fig. 4 (b) and (c) that the rank dynamics indeed become more involved for larger depth. In Fig. 4 (b) and (c), we use a 2-layer random
feature model with feature map σ(V σ(W x)) for weights W ∈ R[m][1][×][d] and V ∈ R[m][2][×][m][1] . In (b)
we show the change in rank as m1 increases, while m2 = 120 is fixed. We observe that indeed the
dynamics change and the critical complexity is not at m1 = n. Similarly in (c) we fix m1 = 10
and vary m2. Also there we observe that the critical threshold is not located at n but rather a bigger
width is needed to reach interpolation.

Finally we also study the linearizations of networks that also give rise to kernels. More concretely,
we consider feature maps of the form

_φ(x) =_ **_θfθ(x)_**
_∇_

where fθ is a fully-connected network with parameters θ ∈ R[p], as introduced before. Our doubledescent framework also captures this scenario. In Fig. 5, we display the rank dynamics in case of a
2 layer network fθ(x) = w[T] _σ(U_ _σ(V x)) where w ∈_ R[m][2], U ∈ R[m][2][×][m][1], V ∈ R[m][1][×][d] and σ is
the ReLU non-linearity. Again we observe that the rank dynamics change and do not coincide with
the number of samples n.

C.2 LOO AS FUNCTION OF DEPTH L

We study how the depth L of the NTK kernel Θ[(][L][)] affects the performance of LOO loss and accuracy. We use the datasets MNIST and CIFAR10 with n = 5000 and evaluate NTK models with
depth ranging from 3 to 20. We present our findings in Figure 6. Again we see a very close match
between LOO and the corresponding test quantity for CIFAR10. Interestingly the performance is
slightly worse for very shallow models. For MNIST we see a gap between LOO loss and test loss,
which is due to the very zoomed-in nature of the plot (the gap is actually only 0.015) as the loss
values are very small in general. Indeed we observe an excellent match between the test and LOO
accuracy.


-----

Figure 5: Rank dynamics of K for the linearization kernel of fθ(x) = w[T] _σ(U_ _σ(V x)) where_
**_w ∈_** R[m][2], U ∈ R[m][2][×][m][1], V ∈ R[m][1][×][d] and σ is the ReLU non-linearity.

(a) LLOO, MNIST (b) ALOO, MNIST (c) LLOO, CIFAR10 (d) ALOO, CIFAR10

Figure 6: Test and LOO losses (a, c) and accuracies (b, d) as a function of depth L. We use fullyconnected NTK model on MNIST and CIFAR10.

C.3 DOUBLE DESCENT WITH RANDOM LABELS

Here we demonstrate how the spike in double descent is a very universal phenomenon as demonstrated by Theorem 4.3. We consider a random feature model of varying width m on binary MNIST
with n = 2000, where the labels are fully randomized (p = 1), destroying thus any relationship
between the inputs and targets. Of course, there will be no double descent behaviour in the test accuracy as the network has to perform random guessing at any width. We display this in Figure 7. We
observe that indeed the model is randomly guessing throughout all the regimes of overparametrization. Both the test and LOO loss however, exhibit a strong spike around the interpolation threshold.
This underlines the universal nature of the phenomenon, connecting with the fact that Theorem 4.3
does not need any assumptions on the targets.

C.4 TRANSFER LEARNING LOSS

We report the corresponding test and leave-one-out losses, moved to the appendix due to space
constraints. We display the scores in Table 2. Again we observe a very good match between the test
and LOO losses. Moreover, we again find that pre-training on ImageNet is beneficial in terms of the
achieved loss values.


-----

(a) Loss (b) Accuracy

Figure 7: Test and LOO losses (a) and accuracies (b) as a function of sample width m. We use a
random feature model on binary MNIST with random labels.

MODEL _LTEST(φDATA)_ _LLOO(φDATA)_ _LTEST(φRAND)_ _LLOO(φRAND)_

RESNET18 0.602 ± 0.0053 0.619 ± 0.009 0.77 ± 0.003 0.758 ± 0.0051
ALEXNET 0.638 ± 0.0021 0.639 ± 0.0036 0.837 ± 0.002 0.835 ± 0.004
VGG16 0.657 ± 0.0062 0.66 ± 0.0091 0.852 ± 0.0026 0.834 ± 0.0049
DENSENET161 0.599 ± 0.0044 0.613 ± 0.0097 0.718 ± 0.0043 0.731 ± 0.0081

Table 2: Test and LOO losses for models pre-trained on ImageNet and transferred to CIFAR10 by
re-training the top layer. We use 5 runs, each with a different training set of size n = 10000. We
compare the pre-trained networks with random networks to illustrate the benefits of transfer learning.


-----

